Journal of Articial Intelligence Research 18 (2003) 149-181

Submitted 10/02; published 02/03

Wrapper Maintenance: A Machine Learning Approach
Kristina Lerman

lerman@isi.edu

USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Steven N. Minton

minton@fetch.com

Fetch Technologies
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Craig A. Knoblock

knoblock@isi.edu

USC Information Sciences Institute and Fetch Technologies
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Abstract
The proliferation of online information sources has led to an increased use of wrappers
for extracting data from Web sources. While most of the previous research has focused
on quick and ecient generation of wrappers, the development of tools for wrapper maintenance has received less attention. This is an important research problem because Web
sources often change in ways that prevent the wrappers from extracting data correctly. We
present an ecient algorithm that learns structural information about data from positive
examples alone. We describe how this information can be used for two wrapper maintenance applications: wrapper verication and reinduction. The wrapper verication system
detects when a wrapper is not extracting correct data, usually because the Web source has
changed its format. The reinduction algorithm automatically recovers from changes in the
Web source by identifying data on Web pages so that a new wrapper may be generated for
this source. To validate our approach, we monitored 27 wrappers over a period of a year.
The verication algorithm correctly discovered 35 of the 37 wrapper changes, and made
16 mistakes, resulting in precision of 0.73 and recall of 0.95. We validated the reinduction algorithm on ten Web sources. We were able to successfully reinduce the wrappers,
obtaining precision and recall values of 0.90 and 0.80 on the data extraction task.

1. Introduction
There is a tremendous amount of information available online, but much of this information
is formatted to be easily read by human users, not computer applications. Extracting
information from semi-structured Web pages is an increasingly important capability for
Web-based software applications that perform information management functions, such as
shopping agents (Doorenbos, Etzioni, & Weld, 1997) and virtual travel assistants (Knoblock,
Minton, Ambite, Muslea, Oh, & Frank, 2001b; Ambite, Barish, Knoblock, Muslea, Oh, &
Minton, 2002), among others. These applications, often referred to as agents, rely on
Web wrappers that extract information from semi-structured sources and convert it to
a structured format. Semi-structured sources are those that have no explicitly specied
grammar or schema, but have an implicit grammar that can be used to identify relevant
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiLerman, Minton & Knoblock

information on the page. Even text sources such as email messages have some structure in
the heading that can be exploited to extract the date, sender, addressee, title, and body
of the messages. Other sources, such as online catalogs, have a very regular structure that
can be exploited to extract all the data automatically.
Wrappers rely on extraction rules to identify the data eld to be extracted. Semiautomatic creation of extraction rules, or wrapper induction, has been an active area of
research in recent years (Knoblock, Lerman, Minton, & Muslea, 2001a; Kushmerick, Weld,
& Doorenbos, 1997). The most advanced of these wrapper generation systems use machine
learning techniques to learn the extraction rules by example. For instance, the wrapper
induction tool developed at USC (Knoblock et al., 2001a; Muslea, Minton, & Knoblock,
1998) and commercialized by Fetch Technologies, allows the user to mark up data to be
extracted on several example pages from an online source using a graphical user interface.
The system then generates landmark-based extraction rules for these data that rely on
the page layout. The USC wrapper tool is able to eciently create extraction rules from
a small number of examples; moreover, it can extract data from pages that contain lists,
nested structures, and other complicated formatting layouts.
In comparison to wrapper induction, wrapper maintenance has received less attention.
This is an important problem, because even slight changes in the Web page layout can break
a wrapper that uses landmark-based rules and prevent it from extracting data correctly. In
this paper we discuss our approach to the wrapper maintenance problem, which consists of
two parts: wrapper verication and reinduction. A wrapper verication system monitors
the validity of data returned by the wrapper. If the site changes, the wrapper may extract
nothing at all or some data that is not correct. The verication system will detect data
inconsistency and notify the operator or automatically launch a wrapper repair process.
A wrapper reinduction system repairs the extraction rules so that the wrapper works on
changed pages.

Pages to
be labeled

Web
pages

Reinduction System

GUI

Labeled
Web pages

Wrapper
Induction
System

Wrapper
Extracted
data
Change
detected

Automatic
Re-labeling

Wrapper
Verification

Figure 1: Life cycle of a wrapper
Figure 1 graphically illustrates the entire life cycle of a wrapper. As shown in the gure,
the wrapper induction system takes a set of web pages labeled with examples of the data to
be extracted. The output of the wrapper induction system is a wrapper, consisting of a set
150

fiWrapper Maintenance

of extraction rules that describe how to locate the desired information on a Web page. The
wrapper verication system uses the functioning wrapper to collect extracted data. It then
learns patterns describing the structure of data. These patterns are used to verify that the
wrapper is correctly extracting data at a later date. If a change is detected, the system can
automatically repair a wrapper by using this structural information to locate examples of
data on the new pages and re-running the wrapper induction system with these examples.
At the core of these wrapper maintenance applications is a machine learning algorithm that
learns structural information about common data elds. In this paper we introduce the
algorithm, DataProG, and describe its application to the wrapper maintenance tasks in
detail. Though we focus on web applications, the learning technique is not web-specic,
and can be used for data validation in general.
Note that we distinguish two types of extraction rules: landmark-based rules that extract data by exploiting the structure of the Web page, and content-based rules, which we
refer to as content patterns or simply patterns, that exploit the structure of the eld itself.
Our previous work focused on learning landmark rules for information extraction (Muslea,
Minton, & Knoblock, 2001). The current work shows that augmenting these rules with
content-based patterns provides a foundation for sophisticated wrapper maintenance applications.

2. Learning Content Patterns
The goal of our research is to extract information from semi-structured information sources.
This typically involves identifying small chunks of highly informative data on formatted
pages (as opposed to parsing natural language text). Either by convention or design, these
elds are usually structured: phone numbers, prices, dates, street addresses, names, schedules, etc. Several examples of street addresses are given in Fig. 2. Clearly, these strings
are not arbitrary, but share some similarities. The objective of our work is to learn the
structure of such elds.
4676 Admiralty Way
10924 Pico Boulevard
512 Oak Street
2431 Main Street
5257 Adams Boulevard

Figure 2: Examples of a street address eld

2.1 Data Representation
In previous work, researchers described the elds extracted from Web pages by a characterlevel grammar (Goan, Benson, & Etzioni, 1996) or a collection of global features, such as the
number of words and the density of numeric characters (Kushmerick, 1999). We employ an
intermediate word-level representation that balances the descriptive power and specicity
of the character-level representation with the compactness and computational eciency of
the global representation. Words, or more accurately tokens, are strings generated from
151

fiLerman, Minton & Knoblock

an alphabet containing dierent types of characters: alphabetic, numeric, punctuation,
etc. We use the tokens character types to assign it to one or more syntactic categories:
alphabetic, numeric, etc. These categories form a hierarchy depicted in Fig. 3, where the
arrows point from more general to less general categories. A unique specic token type is
created for every string that appears in at least k examples, as determined in a preprocessing
step. The hierarchical representation allows for multi-level generalization. Thus, the token
Boulevard belongs to the general token types Alphanum (alphanumeric strings), Alpha
(alphabetic strings), Upper (capitalized words), as well as to the specic type representing
the string Boulevard. This representation is exible and may be expanded to include
domain specic information. For example, the numeric type is divided into categories
that include range information about the number  Large (larger than 1000), Medium
(medium numbers, between 10 and 1000) and Small (smaller than 10) and number of
digits: 1, 2, and 3digit. Likewise, we may explicitly include knowledge about the type
of information being parsed, e.g., some 5-digit numbers could be represented as zipcode.

TOKEN

PUNCT

ALPHANUM

ALPHA

UPPER

NUMBER

LOWER

SMALL MEDIUM LARGE

ALLCAPS

CA

HTML

1Digit

Boulevard

2Digit

3Digit

310

Figure 3: Portion of the token type syntactic hierarchy
We have found that a sequence of specic and general token types is more useful for
describing the content of information than the character-level nite state representations
used in previous work (Carrasco & Oncina, 1994; Goan et al., 1996). The character-level
description is far too ne grained to compactly describe data and, therefore, leads to poor
generality. The coarse-grained token-level representation is more appropriate for most Web
data types. In addition, the data representation schemes used in previous work attempt
to describe the entire data eld, while we use only the starting and ending sequences,
or patterns, of tokens to capture the structure of the data elds. The reason for this is
similar to the one above: using the starting and ending patterns allows us to generalize
the structural information for many complex elds which have a lot of variability. Such
elds, e.g., addresses, usually have some regularity in how they start and end that we
can exploit. We call the starting and ending patterns collectively a data prototype. As
an example, consider a set of street addresses in Fig. 2. All of the examples start with a
152

fiWrapper Maintenance

pattern <Number Upper> and end with a specic type <Boulevard> or more generally
<Upper>. Note that the pattern language does not allow loops or recursion. We believe
that recursive expressions are not useful representations of the types of data we are trying
to learn, because they are harder to learn and lead to over-generalization.
2.2 Learning from Positive Examples
The problem of learning the data prototype from a set of examples that are labeled as
belonging (or not) to a class may be stated in one of two related ways: as a classication
or as a conservation task. In the classication task, both positive and negative instances of
the class are used to learn a rule that will correctly classify new examples. Classication
algorithms, like FOIL (Quinlan, 1990), use negative examples to guide the specialization
of the rule. They construct discriminating descriptions  those that are satised by the
positive examples and not the negative examples. The conservation task, on the other hand,
attempts to nd a characteristic description (Dietterich & Michalski, 1981) or conserved
patterns (Brazma, Jonassen, Eidhammer, & Gilbert, 1995), in a set of positive examples of a
class. Unlike the discriminating description, the characteristic description will often include
redundant features. For example, when learning a description of street addresses, with city
names serving as negative examples, a classication algorithm will learn that <Number> is
a good description, because all the street addresses start with it and none of the city names
do. The capitalized word that follows the number in addresses is a redundant feature,
because it does not add to the discriminating power of the learned description. However, if
an application using this description encounters a zipcode in the future, it will incorrectly
classify it as a street address. This problem could have been avoided if <Number Upper>
was learned as a description of street addresses. Therefore, when negative examples are
not available to the learning algorithm, the description has to capture all the regularity
of data, including the redundant features, in order to correctly identify new instances of
the class and dierentiate them from other classes. Ideally, the characteristic description
learned from positive examples alone is the same as the discriminating description learned by
the classication algorithm from positive and negative examples, where negative examples
are drawn from innitely many classes. While most of the widely used machine learning
algorithms (e.g., decision trees (Quinlan, 1993), inductive logic programming (Muggleton,
1991)) solve the classication task, there are fewer algorithms that learn characteristic
descriptions.
In our applications, an appropriate source of negative examples is problematic; therefore,
we chose to frame the learning problem as a conservation task. We introduce an algorithm
that learns data prototypes from positive examples of the data eld alone. The algorithm
nds statistically signicant sequences of tokens. A sequence of token types is signicant
if it occurs more frequently than would be expected if the tokens were generated randomly
and independently of one another. In other words, each such sequence constitutes a pattern
that describes many of the positive examples of data and is highly unlikely to have been
generated by chance.
The algorithm estimates the baseline probability of a token types occurrence from the
proportion of all types in the examples of the data eld that are of that type. Suppose we
are learning a description of the set of street addresses in Fig. 2, and have already found
153

fiLerman, Minton & Knoblock

a signicant token sequence  e.g., the pattern consisting of the single token <Number>
 and want to determine whether the more specic pattern, <Number Upper>, is also
a signicant pattern. Knowing the probability of occurrence of the type Upper, we can
compute how many times Upper can be expected to follow Number completely by chance.
If we observe a considerably greater number of these sequences, we conclude that the longer
pattern is also signicant.
We use hypothesis testing (Papoulis, 1990) to decide whether a pattern is signicant.
The null hypothesis is that observed instances of this pattern were generated by chance,
via the random, independent generation of the individual token types. Hypothesis testing
decides, at a given condence level, whether the data supports rejecting the null hypothesis.
Suppose n identical sequences have been generated by a random source. The probability
that a token type T (whose overall probability of occurrence is p) will be the next type in
k of these sequences has a binomial distribution. For a large n, the binomial distribution
approaches a normal distribution P (x, , ) with  = np and  2 = np(1p). The cumulative
probability is the probability of observing at least n1 events:
P (k  n1 ) =

 
n1

P (x, , )dx

(1)

We use polynomial approximation formulas (Abramowitz & Stegun, 1964) to compute the
value of the integral.
The signicance level of the test, , is the probability that the null hypothesis is rejected
even though it is true, and it is given by the cumulative probability above. Suppose we set
 = 0.05. This means that we expect to observe at least n1 events 5% of the time under the
null hypothesis. If the number of observed events is greater, we reject the null hypothesis
(at the given signicance level), i.e., decide that the observation is signicant. Note that
the hypothesis we test is derived from observation (data). This constraint reduces the
number of degrees of freedom of the test; therefore, we must subtract one from the number
of observed events. This also prevents the anomalous case when a single occurrence of a
rare event is judged to be signicant.
2.3 DataProG Algorithm
We now describe DataProG, the algorithm that nds statistically signicant patterns in a
set of token sequences. During the preprocessing step the text is tokenized, and the tokens
are assigned one or more syntactic types (see Figure 3). The patterns are encoded in a
type of prex tree, where each node corresponds to a token type. DataProG relies on
signicance judgements to grow the tree and prune the nodes. Every path through the
resulting tree starting at the root node corresponds to a signicant pattern found by the
algorithm. In this section, we focus the discussion on the version of the algorithm that
learns starting patterns. The algorithm is easily adapted to learn ending patterns.
We present the pseudocode of the DataProG algorithm in Table 1. DataProG grows
the pattern tree incrementally by (1) nding all signicant specializations (i.e., longer patterns) of a pattern and (2) pruning the less signicant of the generalizations (or specializations) among patterns of the same length. As the last step, DataProG extracts all
signicant patterns from the pattern tree, including those generalizations (i.e., shorter patterns) found to be signicant given the more specic (i.e., longer) patterns.
154

fiWrapper Maintenance

DATAPROG MAIN LOOP
Create root node of tree;
For next node Q of tree
Create children of Q;
Prune nodes;
Extract patterns from tree;

CREATE CHILDREN OF Q
For each token type T at next position in examples
Let C = NewNode;
Let C.token = T;
Let C.examples = Q.examples that are followed by T;
Let C.count = |C.examples|;
Let C.pattern = concat(Q.pattern T );
If Signicant(C.count, Q.count, T.probability)
AddChildToTree(C, Q);
End If
End T loop

PRUNE NODES
For each child C of Q
For each sibling S of C s.t. S.pattern  C.pattern
Let N = C.count  S.count
If Not(Signicant(N, Q.count, C.token.probability))
Delete C;
break;
Else
Delete S;
End If
End S loop
End C loop

EXTRACT PATTERNS FROM TREE
Create empty list;
For every node Q of tree
For every child C of Q 
Let N = C.count  i (Si .count|Si  Children(C))
If Signicant( N, Q.count, C.token.probability)
Add C.pattern to the list;
Return (list of patterns);

Table 1: Pseudocode of the DataProG algorithm

155

fiLerman, Minton & Knoblock

The tree is empty initially, and children are added to the root node. The children
represent all tokens that occur in the rst position in the training examples more often
than expected by chance. For example, when learning addresses from the examples in
Fig. 2, the root will have two child nodes: Alphanum and Number. The tree is extended
incrementally at each node Q. A new child is added to Q for every signicant specialization
of the pattern ending at Q. As explained previously, a child node is judged to be signicant
with respect to its parent node if the number of occurrences of the pattern ending at the
child node is suciently large, given the number of occurrences of the pattern ending at the
parent node and the baseline probability of the token type used to extend the pattern. To
illustrate on our addresses example, suppose we have already found that a pattern <Number
Upper> is signicant. There are ve ways to extend the tree (see Fig. 4) given the data:
<Number Upper Alphanum>, <Number Upper Alpha>, <Number Upper Upper>,
<Number Upper Street>, <Number Upper Boulevard>, and <Number Upper Way>.
All but the last of these patterns are judged to be signicant at  = 0.05. For example,
<Number Upper Upper> is signicant, because Upper follows the pattern <Number
Upper> ve out of ve times,1 and the probability of observing at least that many longer
sequences purely by chance is 0.0002.2 Since this probability is less than , we judge this
sequence to be signicant.

ROOT
NUMBER
UPPER

ALPHANUM

ALPHA

UPPER

Boulevard

Street

Figure 4: Pattern tree that describes the structure of addresses. Dashed lines link to nodes
that are deleted during the pruning step.

The next step is to prune the tree. The algorithm examines each pair of sibling nodes,
one of which is more general than the other, and eliminates the less signicant of the pair.
More precisely, the algorithm iterates through the newly created children of Q, from the
most to least general, and for every pair of children Ci and Cj , such that Ci .pattern 
Cj .pattern (i.e., Cj .pattern is strictly more general than Ci .pattern), the algorithm keeps
only Cj if it explains signicantly more data; otherwise, it keeps only Ci . 3
1. Such small numbers are used for illustrative purposes only  the typical data sets from which the
patterns are learned are much larger.
2. The calculation of this cumulative probability depends on the occurrence probability of Upper. We count
the occurrence of each token type independently of the others. In our example, occurrence probability
(relative fraction) of type Upper is 0.18.
3. DataProG is based on an earlier version of the algorithm, DataPro, described in the conference paper (Lerman & Minton, 2000). Note that in the original version of the algorithm, the specic patterns
were always kept, regardless of whether the more general patterns were found to be signicant or not.

156

fiWrapper Maintenance

Let us illustrate the pruning step with the example pattern tree in Fig. 4. We can eliminate the node AlphaNum, because all the examples that match the pattern <Number Upper Alphanum> also match the pattern <Number Upper Alpha>  thus, Alphanum is
not signicant given its specialization Alpha. We can eliminate node Alpha for a similar
reason. Next, we check whether <Number Upper Upper> is signicant given the patterns
<Number Upper Boulevard> and <Number Upper Street>. There are 2 instances of the
address eld that match the pattern <Number Upper Boulevard>, and 2 addresses that
match <Number Upper Street>. If <Number Upper Upper> matches signicantly more
than 4 addresses, it will be retained and the more specic patterns will be pruned from the
tree; otherwise, it will be deleted and the more specic ones kept. Because every example
is described by at most one pattern of a given length, the pruning step ensures that the size
of the tree remains polynomial in the number of tokens, thereby, guaranteeing a reasonable
performance of the algorithm.
Once the entire tree has been expanded, the nal step is to extract all signicant patterns
from the tree. Here, the algorithm judges whether the shorter (more general) pattern, e.g.,
<Number Upper>, is signicant given the longer specializations of it, e.g., <Number
Upper Boulevard> and <Number Upper Street>. This amounts to testing whether the
excess number of examples that are explained by the shorter pattern, and not by the longer
patterns, is signicant. Any pattern that ends at a terminal node of the tree is signicant.
Note that the set of signicant patterns may not cover all the examples in the data set, just
a fraction of them that occur more frequently than expected by chance (at some signicance
level). Tables 24 show examples of several data elds from a yellow pages source (Bigbook)
and a stock quote source (Y ahoo Quote), as well as the starting patterns learned for each
eld.

3. Applications of Pattern Learning
As we explained in the introduction, wrapper induction systems use information from the
layout of Web pages to create data extraction rules and are therefore vulnerable to changes
in the layout, which occur frequently when the site is redesigned. In some cases the wrapper
continues to extract, but the data is no longer correct. The output of the wrapper may also
change because the format of the source data itself has changed: e.g., when $ is dropped
from the price eld (9.95 instead of $9.95), or book availability changes from Ships
immediately to In Stock: ships immediately. Because other applications, such as Web
agents (Ambite et al., 2002; Chalupsky et al., 2001), rely on data extracted by wrappers,
wrapper maintenance is an important research problem. We divide the wrapper maintenance problem into two parts, each described separately in the paper. Wrapper verication
automatically detects when a wrapper is not extracting data correctly from a Web source,
while wrapper reinduction automatically xes broken wrappers. Both applications learn a
description of data, of which patterns learned by DataProG are a signicant part.

This introduced a strong bias for specic patterns into the results, which led to a high proportion of
false positives during the wrapper verication experiments. Eliminating the specicity bias, improved
the performance of the algorithm on the verication task.

157

fiLerman, Minton & Knoblock

BUSINESS NAME
Chado Tea House
Saladang
Information Sciences Institute
Chaya Venice
Acorda Therapeutics
Cajun Kitchen
Advanced Medical Billing Services
Vega 1 Electrical Corporation
21st Century Foundation
TIS the Season Gift Shop
Hide Sushi Japanese Restaurant
Aoat Sushi
Prebica Coee & Cafe
L  Orangerie
Emils Hardware
Natalee Thai Restaurant
Casablanca
Antica Pizzeria
NOBU Photographic Studio
Lotus Eaters
Essex On Coney
National Restaurant
Siam Corner Cafe
Grand Casino French Bakery
Alejo  s Presto Trattoria
Titos Tacos Mexican Restaurant Inc
Killer Shrimp
Manhattan Wonton CO
Starting patterns
<Alpha Upper>
<Alpha Upper Upper Restaurant>
<Alpha >

ADDRESS
8422 West 1st Street
363 South Fair Oaks Avenue
4676 Admiralty Way
110 Navy Street
330 West 58th Street
420 South Fairview Avenue
9478 River Road
1723 East 8th Street
100 East 85th Street
15 Lincoln Road
2040 Sawtelle Boulevard
87 East Colorado Boulevard
4325 Glencoe Avenue
903 North La Cienega Boulevard
2525 South Robertson Boulevard
998 South Robertson Boulevard
220 Lincoln Boulevard
13455 Maxella Avenue
236 West 27th Street
182 5th Avenue
1359 Coney Island Avenue
273 Brighton Beach Avenue
10438 National Boulevard
3826 Main Street
4002 Lincoln Boulevard
11222 Washington Place
523 Washington Boulevard
8475 Melrose Place
<Number Upper Upper>
<Number Upper Upper Avenue>
<Number Upper Upper Boulevard>

Table 2: Examples of the business name and address elds from the Bigbook source, and
the patterns learned from them

158

fiWrapper Maintenance

CITY
Los Angeles
Pasadena
Marina Del Rey
Venice
New York
Goleta
Marcy
Brooklyn
New York
Bualo
Los Angeles
Pasadena
Marina Del Rey
West Hollywood
Los Angeles
Los Angeles
Venice
Marina Del Rey
New York
New York
Brooklyn
Brooklyn
Los Angeles
Culver City
Marina Del Rey
Culver City
Marina Del Rey
West Hollywood
Starting patterns
<Upper Upper>
<Upper Upper Rey>

STATE
CA
CA
CA
CA
NY
CA
NY
NY
NY
NY
CA
CA
CA
CA
CA
CA
CA
CA
NY
NY
NY
NY
CA
CA
CA
CA
CA
CA

PHONE
( 323 ) 655
( 626 ) 793
( 310 ) 822
( 310 ) 396
( 212 ) 376
( 805 ) 683
( 315 ) 793
( 718 ) 998
( 212 ) 249
( 716 ) 839
( 310 ) 477
( 626 ) 792
( 310 ) 823
( 310 ) 652
( 310 ) 839
( 310 ) 855
( 310 ) 392
( 310 ) 577
( 212 ) 924
( 212 ) 929
( 718 ) 253
( 718 ) 646
( 310 ) 559
( 310 ) 202
( 310 ) 822
( 310 ) 391
( 310 ) 578
( 323 ) 655

<AllCaps>

<( 3digit ) 3digit - Large>

-

2056
8123
1511
1179
7552
8864
1871
2550
3612
5090
7242
9779
4446
9770
8571
9380
5751
8182
7840
4800
1002
1225
1357
6969
0095
5780
2293
6030

Table 3: Examples of the city, state and phone number elds from the Bigbook source, and
the patterns learned from them

159

fiLerman, Minton & Knoblock

PRICE CHANGE
+ 0 . 51
+ 1 . 51
+ 4 . 08
+ 0 . 83
+ 2 . 35
- 10 . 84
- 1 . 24
- 1 . 59
- 2 . 94
+ 1 . 04
- 0 . 81
+ 4 . 45
+ 0 . 16
- 3 . 48
+ 0 . 49
- 3 . 38
+ 1 . 15
- 2 . 86
- 6 . 46
- 0 . 82
+ 2 . 00
+ 0 . 13
- 1 . 63
Starting patterns
<Punct 1digit . 2digit>

TICKER
INTC
IBM
AOL
T
LU
ATHM
COMS
CSCO
GTE
AAPL
MOT
HWP
DELL
GM
CIEN
EGRP
HLIT
RIMM
C
GPS
CFLO
DCLK
NT
BFRE
QCOM

VOLUME
17 , 610 , 300
4 , 922 , 400
24 , 257 , 300
8 , 504 , 000
9 , 789 , 300
5 , 646 , 400
15 , 388 , 200
19 , 135 , 900
1 , 414 , 900
2 , 291 , 800
3 , 599 , 600
2 , 147 , 700
40 , 292 , 100
1 , 398 , 100
4 , 120 , 200
7 , 007 , 400
543 , 400
307 , 500
6 , 145 , 400
1 , 023 , 600
157 , 700
1 , 368 , 100
4 , 579 , 900
149 , 000
7 , 928 , 900

<AllCaps>

<Number , 3digit , 3digit>

PRICE
122 3 / 4
109 5 / 16
63 13 / 16
53 1 / 16
68
29 7 / 8
57 11 / 32
134 1 / 2
65 15 / 16
117 3 / 4
169 1 / 4
145 5 / 16
57 3 / 16
77 15 / 16
142
25 7 / 8
128 13 / 16
132 1 / 4
49 15 / 16
44 5 / 8
103 1 / 4
106
124 1 / 8
46 9 / 16
128 1 / 16
<Medium 1digit / Number>
<Medium 15 / 16 >

Table 4: Data examples from the Y ahoo Quote source, and the patterns learned from them

160

fiWrapper Maintenance

3.1 Wrapper Verification
If the data extracted by the wrapper changes signicantly, this is an indication that the
Web source may have changed its format. Our wrapper verication system uses examples
of data extracted by the wrapper in the past that are known to be correct in order to
acquire a description of the data. The learned description contains features of two types:
patterns learned by DataProG and global numeric features, such as the density of tokens
of a particular type. The application then checks that this description still applies to the
new data extracted by the wrapper. Thus, wrapper verication is a specic instance of the
data validation task.
The verication algorithm works in the following way. A set of queries is used to retrieve
HTML pages from which the wrapper extracts (correct) training examples. The algorithm
then computes the values of a vector of features, "k, that describes each eld of the training
examples. These features include the patterns that describe the common beginnings (or
endings) of the eld. During the verication phase, the wrapper generates a set of (new)
test examples from pages retrieved using the same set of queries, and computes the feature
vector "r associated with each eld of the test examples. If the two distributions, "k and "r
(see Fig. 5), are statistically the same (at some signicance level), the wrapper is judged to
be extracting correctly; otherwise, it is judged to have failed.

16
training set
test set

feature value

14
12
10
8
6
4
2
0
1

2

3

4

5

6

7

feature

Figure 5: A hypothetical distribution of features over the training and test examples

Each eld is described by a vector, whose ith component is the value of the ith feature,
such as the number of examples that match pattern j. In addition to patterns, we use the
following numeric features to describe the sets of training and test examples: the average
number of tuples-per-page, mean number of tokens in the examples, mean token length, and
the density of alphabetic, numeric, HTML-tag and punctuation types. We use goodness of
t method (Papoulis 1990) to decide whether the two distributions are the same. To use
the goodness of t method, we must rst compute Pearsons test statistic for the data. The
Pearsons test statistic is dened as:
161

fiLerman, Minton & Knoblock

q=

m

(ti  ei )2
i=1

ei

(2)

where ti is the observed value of the ith feature in the test data, and ei is the expected
value for that feature, and m is the number of features. For the patterns ei = nri /N ,
where ri is the number of training examples explained by the ith patter, N is the number
of examples in the training set and n is the number of examples in the test set. For numeric
features ei is simply the value of that feature for the training set. The test statistic q has
a chi-squared distribution with m  1 independent degrees of freedom. If q < 2 (m  1; ),
we conclude that at signicance level  the two distributions are the same; otherwise, we
conclude that they are dierent. Values of 2 for dierent values of  and m can be looked
up in a statistics table or calculated using an approximation formula.
In order to use the test statistic reliably, it helps to use as many independent features as
possible. In the series of verication experiments reported in (Lerman & Minton, 2000), we
used the starting and ending patterns and the average number of tuples-per-page feature
when computing the value of q. We found that this method tended to overestimate the
test statistic, because the features (starting and ending patterns) were not independent. In
the experiments reported in this paper, we use only the starting patterns, but in order to
increase the number of features, we added numeric features to the description of data.
3.1.1 Results
We monitored 27 wrappers (representing 23 distinct Web sources) over a period of ten
months, from May 1999 to March 2000. The sources are listed in Table 5. For each wrapper,
the results of 1530 queries were stored periodically, every 710 days. We used the same
query set for each source, except for the hotel source, because it accepted dated queries,
and we had to change the dates periodically to get valid results. Each set of new results
(test examples) was compared with the last correct wrapper output (training examples).
The verication algorithm used DataProG to learn the starting patterns and numeric
features for each eld of the training examples and made a decision at a high signicance
level (corresponding to  = 0.001) about whether the test set was statistically similar to the
training set. If none of the starting patterns matched the test examples or if the data was
found to have changed signicantly for any data eld, we concluded that the wrapper failed
to extract correctly from the source; otherwise, if all the data elds returned statistically
similar data, we concluded that the wrapper was working correctly.
A manual check of the 438 comparisons revealed 37 wrapper changes attributable to
changes in the source layout and data format.4 The verication algorithm correctly discovered 35 of these changes and made 15 mistakes. Of these mistakes, 13 were false positives,
which means that the verication program decided that the wrapper failed when in reality
it was working correctly. Only two of the errors were the more important false negatives,
meaning that the algorithm did not detect a change in the data source. The numbers above
4. Seven of these were, in fact, internal to the wrapper itself, as when the wrapper was modied to extract
$22.00 instead of 22.00 for the price eld. Because these actions were mostly outside of our control,
we chose to classify them as wrapper changes.

162

fiWrapper Maintenance

Source
airport
altavista
Amazon
arrowlist

Type
tuple/list
list
tuple
list

Bigbook
Barnes&N oble
borders
cuisinenet

tuple
tuple
list
list

geocoder
hotel
mapquest
northernlight
parking
Quote
Smartpages
showtimes
theatre
W ashington P ost
whitepages
yahoo people
Y ahoo Quote
yahoo weather
cia f actbook

tuple
list
tuple
list
list
tuple
tuple
list
list
tuple
list
list
tuple
tuple
tuple

Data Fields
airport code, name
url, title
book author, title, price, availability, isbn
part number, manufacturer, price, status,
description, url
business name, address, city, state, phone
book author, title, price, availability, isbn
book author, title, price, availability
restaurant name, cuisine, address, city, state,
phone, link
latitude, longitude, street, city, state
name, price, distance, url
hours, minutes, distance, url
url, title
lotname, dailyrate
stock ticker, price, pricechange, volume
name, address, city, state, phone
movie, showtimes
theater name, url, address
taxi price
business name, address, city, state, phone
name, address, city, state, phone
stock ticker, price, pricechange, volume
temperature, forecast
country area, borders, population, etc.

Table 5: List of sources used in the experiments and data elds extracted from them. Source
type refers to how much data a source returns in response to a query  a single
tuple or a list of tuples. For airport source, the type changed from a single tuple
to a list over time.

163

fiLerman, Minton & Knoblock

result in the following precision, recall and accuracy values:
P

=

R =
A =

true positives
= 0.73 ,
true positives + f alse positives
true positives
= 0.95 ,
true positives + f alse negatives
true positives + true negatives
= 0.97 .
positives + negatives

These results are an improvement over those reported in (Lerman & Minton, 2000),
which produced P = 0.47, R = 0.95, A = 0.91. The poor precision value reported in that
work was due to 40 false positives obtained on the same data set. We attribute the improvements both to eliminating the specicity bias in the patterns learned by DataProG
and to changing the feature set to include only the starting patterns and additional numeric
features. Note that this improvement does not result simply from adding numeric features.
To check this, we ran the verication experiments on a subset of data (the last 278 comparisons) using only the global numeric features and obtained P = 0.92 and R = 0.55, whereas
using both patterns and numeric features results in values of P = 0.71 and R = 1.00 for
the same data set.
3.1.2 Discussion of Results
Though we have succeeded in signicantly reducing the number of false positives, we have
not managed to eliminate them altogether. There are a number of reasons for their presence,
some of which point to limitations in our approach.
We can split the types of errors into roughly three not entirely independent classes:
improper tokenization, incomplete data coverage, and data format changes. The URL eld
(Table 6) accounted for a signicant fraction of the false positives, in large part due to
the design of our tokenizer, which splits text strings on punctuation marks. If the URL
contains embedded punctuation (as part of the alphanumeric key associated with the user or
session id), it will be split into a varying number of tokens, so that it is hard to capture the
regularity of the eld. The solution is to rewrite the tokenizer to recognize URLs for which
well dened specications exist. We will address this problem in our ongoing work. Our
algorithm also failed sometimes (e.g., arrowlist, showtimes) when it learned very long and
specic descriptions. It is worth pointing out, however, that it performed correctly in over
two dozen comparisons for these sources. These types of errors are caused by incomplete
data coverage: a larger, more varied training data set would produce more general patterns,
which would perform better on the verication task. A striking example of the data coverage
problem occurred for the stock quotes source: the day the training data was collected, there
were many more down movements in the stock price than up, and the opposite was true on
the day the test data was collected. As a result, the price change elds for those two days
were dissimilar. Finally, because DataProG learns the format of data, false positives will
inevitably result from changes in the data format and do not indicate a problem with the
algorithm. This is the case for the factbook source, where the units of area changed from
km2 to sq km.
164

fiWrapper Maintenance

hotel, mapquest (5 cases): URL eld contains alphanumeric keys, with embedded punctuation symbols. The tokenizer splits the eld into many tokens. The key or its
format changes from:
http://. . .&Stamp=Q4aaiEGSp68*itn/hot%3da11204,itn/agencies/newitn. . . to
http://. . .&Stamp=8bEgGEQrCo*itn/hot%3da11204,itn/agencies/newitn. . .
On one occasion, the server name inside the URL changed: from
http://enterprise.mapquest.com/mqmapgend?MQMapGenRequest=. . . to
http://sitemap.mapquest.com/mqmapgend?MQMapGenRequest=. . .
showtimes, arrowlist (5 cases ): Instance of the showtimes eld and part number and
description elds (arrowlist) are very long. Many long, overly specic patterns are
learned for these elds: e.g.,
<( Number : 2digit AllCaps ) , ( Small : 2digit ) , ( Small : 2digit ) , ( 4 : 2digit
) , 6 : 2digit , 7 : 2digit , 9 : 2digit , 10 : 2digit >

altavista (1 case): Database of the search engine appears to have been updated. A dierent
set of results is returned for each query.
quote (1 case): Data changed  there were many more positive than negative price movements in the test examples
factbook (1 case): Data format changed:
f rom <Number km2 >
to <Number sq km >
Table 6: List of sources of false positive results on the verication task

165

fiLerman, Minton & Knoblock

3.2 Wrapper Reinduction
If the wrapper stops extracting correctly, the next challenge is to rebuild it automatically (Cohen, 1999). The extraction rules for our wrappers (Muslea et al., 2001), as well as
many others (cf. (Kushmerick et al., 1997; Hsu & Dung, 1998)), are generated by a machine
learning algorithm, which takes as input several pages from a source and labeled examples
of data to extract from each page. It is assumed that the user labeled all examples correctly. If we label at least a few pages for which the wrapper fails by correctly identifying
examples of data on them, we can use these examples as input to the induction algorithm,
such as STALKER,5 to generate new extraction rules.6 Note that we do not need to identify the data on every page  depending on how regular the data layout is, Stalker can
learn extraction rules using a small number of correctly labeled pages. Our solution is to
bootstrap the wrapper induction process (which learns landmark-based rules) by learning
content-based rules. We want to re-learn the landmark-based rules, because for the types
of sites we use, these rules tend to be much more accurate and ecient than content-based
rules.
We employ a method that takes a set of training examples, extracted from the source
when the wrapper was known to be working correctly, and a set of pages from the same
source, and uses a mixture of supervised and unsupervised learning techniques to identify
examples of the data eld on new pages. We assume that the format of data did not
change. Patterns learned by DataProG play a signicant role in the reinduction task.
In addition to patterns, other features, such as the length of the training examples and
structural information about pages are used. In fact, because page structure is used during
a critical step of the algorithm, we discuss our approach to learning it in detail in the next
paragraph.
3.2.1 Page Template Algorithm
Many Web sources use templates, or page skeletons, to automatically generate pages and
ll them with results of a database query. This is evident in the example in Fig. 6. The
template consists of the heading RESULTS, followed by the number of results that match
the query, the phrase Click links associated with businesses for more information, then
the heading ALL LISTINGS, followed by the anchors map, driving directions, add
to My Directory and the bolded phrase Appears in the Category. Obviously, data is not
part of the template  rather, it appears in the slots between template elements.
Given two or more example pages from the same source, we can induce the template
used to generate them (Table 7). The template nding algorithm looks for all sequences
of tokens  both HTML tags and text  that appear exactly once on each page. The
algorithm works in the following way: we pick the smallest page in the set as the template
seed. Starting with the rst token on this page, we grow a sequence by appending tokens
5. It does not matter, in fact, matter which wrapper induction system is used. We can easily replace
Stalker with HLRT (Kushmerick et al., 1997) to generate extraction rules.
6. In this paper we will only discuss wrapper reinduction for information sources that return a single tuple
of results per page, or a detail page. In order to create data extraction rules for sources that return lists
of tuples, the Stalker wrapper induction algorithm requires user to specify the rst and last elements
of the list, as well as at least two consecutive elements. Therefore, we need to be able to identify these
data elements with a high degree of certainty.

166

fiWrapper Maintenance

(a)

(b)
Figure 6: Fragments of two Web pages from the same source displaying restaurant information.

167

fiLerman, Minton & Knoblock

to it, subject to the condition that the sequence appears on every page. If we managed to
build a sequence thats at least three tokens long7 , and this sequence appears exactly once
on each page, it becomes part of the page template. Templates play an important role in
helping identify correct data examples on pages.
input:
P = set of N Web pages
output:
T = page template
begin
p = shortest(P )
T = null
s = null
for t = rsttoken(p) to lasttoken(p)
s = concat(s, t)
if ( s appears on every page in P )
s = s
continue
else

n= N
page=1 count(s, page)
if ( n = N AND length(s)  3 )
add-to-template(T, s)
end if
s = null
end if
end for
end
Table 7: Pseudocode of the template nding algorithm

3.2.2 Automatic Labeling Algorithm
Figure 7 is a schematic outline of the reinduction algorithm, which consists of automatic
data labeling and wrapper induction. Because the latter aspect is described in detail in
other work (Muslea et al., 2001), we focus the discussion below on the automatic data
labeling algorithm.
First, DataProG learns the starting and ending patterns that describe the set of training examples. These training examples have been collected during wrappers normal operation, while it was correctly extracting data from the Web source. The patterns are used
to identify possible examples of the data eld on the new pages. In addition to patterns,
we also calculate the mean (and its variance) of the number-of-tokens in the training examples. Each new page is then scanned to identify all text segments that begin with one
of the starting patterns and end with one of the ending patterns. Text segments that con7. The best value for the minimum length for the page template element was determined empirically to be
three.

168

fiWrapper Maintenance

extract

extracted
data

Wrapper

learn
labeled
Web pages

Wrapper
Induction
System

patterns
apply

Web
pages

extracts
score

group

Figure 7: Schematic outline of the reinduction algorithm
tain signicantly more or fewer tokens than expected based on the old number-of-tokens
distribution, are eliminated from the set of candidate extracts. The learned patterns are
often too general and will match many, possibly hundreds, text segments on each page.
Among these spurious text segments is the correct example of the data eld. The rest of
the discussion is concerned with identifying the correct examples of data on pages.
We exploit some simple a priori assumptions about the structure of Web pages to help
us separate interesting extracts from noise. We expect examples of the same data eld to
appear roughly in the same position and in the same context on each page. For example,
Fig. 6 shows fragments of two Web pages from the same source displaying restaurant information. On both pages the relevant information about the restaurant appears after the
heading ALL LISTINGS and before the phrase Appears in the Category:. Thus, we
expect the same eld, e.g., address, to appear in the same place, or slot, within the page
template. Moreover, the information we are trying to extract will not usually be part of
the page template; therefore, candidate extracts that are part of the page template can
be eliminated from consideration. Restaurant address always follows restaurant name (in
bold) and precedes the city and zip code, i.e., it appears in the same context on every page.
A given eld is either visible to the user on every page, or it is invisible (part of an HTML
tag) on every page. In order to use this information to separate extracts, we describe each
candidate extract by a feature vector, which includes positional information, dened by the
(page template) slot number and context. The context is captured by the adjacent tokens:
one token immediately preceding the candidate extract and one token immediately following it. We also use a binary feature which has the value one if the token is visible to the
user, and zero if it is part of an HTML tag. Once the candidate extracts have been assigned
feature vectors, we split them into groups, so that within each group, the candidate extracts
are described by the same feature vector.
The next step is to score groups based on their similarity to the training examples. We
expect the highest scoring group to contain correct examples of the data eld. One scoring
method involves assigning a rank to the groups based on how many extracts they have
in common with the training examples. This technique generally works well, because at
least some of the data usually remains the same when the Web page layout changes. Of
169

fiLerman, Minton & Knoblock

course, this assumption does not apply to data that changes frequently, such as weather
information, ight arrival times, stock quotes, etc. However, we have found that even in
these sources, there is enough overlap in the data that our approach works. If the scoring
algorithm assigns zero to all groups, i.e., there exist no extracts in common with the training
examples, a second scoring algorithm is invoked. This scoring method follows the wrapper
verication procedure and nds the group that is most similar to the training examples
based on the patterns learned from the training examples.
The nal step of the wrapper reinduction process is to provide the extracts in the top
ranking group to the Stalker wrapper induction algorithm (Muslea et al., 2001) along
with the new pages. Stalker learns data extraction rules for the changed pages. Note
that examples provided to Stalker are required to be the correct examples of the eld. If
the set of automatically labeled examples includes false positives, Stalker will not learn
correct extraction rules for that eld. False negatives are not a problem, however. If the
reinduction algorithm could not nd the correct example of data on a page, that page is
simply not used in the wrapper induction stage.
3.2.3 Results
To evaluate the reinduction algorithm we used only the ten sources (listed in Table 5) that
returned a single tuple of results per page, a detail page.8 The method of data collection
was described in Sec. 3.1.1. Over the period between October 1999 and March 2000 there
were eight format changes in these sources. Since this set is much too small for evaluation
purposes, we created an articial test set by considering all ten data sets collected for each
source during this period. We evaluated the algorithm by using it to extract data from
Web pages for which correct output is known. Specically, we took ten tuples from a set
collected on one date, and used this information to extract data from ten pages (randomly
chosen) collected at a later date, regardless of whether the source had actually changed
or not. We reserved the remaining pages collected at a later date for testing the learned
Stalker rules.
The output of the reinduction algorithm is a list of tuples extracted from ten pages, as
well as extraction rules generated by Stalker for these pages. Though in most cases we
were not able to extract every eld on every pages, we can still learn good extraction rules
with Stalker as long as few examples of each eld are correctly labeled. We evaluated the
reinduction algorithm in two stages: rst, we checked how many data elds for each source
were identied successfully; second, we checked the quality of the learned Stalker rules
by using them to extract data from test pages.
Extracting with content-based rules We judged a data eld to be successfully extracted if the automatic labeling algorithm was able to identify it correctly on at least two
of the ten pages. This is the minimum number of examples Stalker needs to create extraction rules. In practice, such a low success rate only occurred for one eld each in two
8. We did not use the geocoder and cia f actbook wrappers in the experiments. The geocoder wrapper
accessed the source through another application; therefore, the pages were not available to us for analysis.
The reason for excluding the f actbook is that it is a plain text source, while our methods apply to Web
pages. Note also that in the verication experiments, we had two wrappers for the mapquest source,
each extracting dierent data. In the experiments described below, we used the one that contained more
data for this time period.

170

fiWrapper Maintenance

of the sources: Quote and Y ahoo Quote. For all other sources, if a eld was successfully
extracted, it was correctly identied in at least three, and in most cases almost all, of the
pages in the set. A false positive occurred when the reinduction algorithm incorrectly identied some text on a page as a correct example of a data eld. In many cases, false positives
consisted of partial elds, e.g., Cloudy rather than Mostly Cloudy (yahoo weather).
A false negative occurred when the algorithm did not identify any examples of a data eld.
We ran the reinduction experiment attempting to extract the elds listed in Table 8. The
second column of the table lists the fractions of data sets for which the eld was successfully
extracted. We were able to correctly identify elds 277 times across all data sets making
61 mistakes, of which 31 were attributed to false positives and 30 to the false negatives.
There are several reasons the reinduction algorithm failed to operate perfectly. In many
cases the reason was the small training set.9 We can achieve better learning for the yellowpages-type sources Bigbook and Smartpages by using more training examples (see Fig. 8).
In two cases, the errors were attributable to changes in the format of data, which resulted
in the failure of patterns to capture the structure of data correctly: e.g., the airport source
changed airport names from capitalized words to allcaps, and in the Quote source in which
the patterns were not able to identify negative price changes because they were learned
for a data set in which most of the stocks had a positive price change. For two sources
the reinduction algorithm could not distinguish between correct examples of the eld and
other examples of the same data type: for the Quote source, in some cases it extracted
opening price or high price for the stock price eld, while for the yahoo weather source, it
extracted high or low temperature, rather than the current temperature. This problem was
also evident in the Smartpages source, where the city name appeared in several places on
the page. In these cases, user intervention or meta-analysis of the elds may be necessary
to improve results of data extraction.
Extracting with landmark-based rules The nal validation experiment consisted of
using the automatically generated wrappers to extract data from test pages. The last three
columns in Table 8 list precision, recall and accuracy for extracting data from test pages.
The performance is very good for most elds, with the notable exception of the STATE eld
of Bigbook source. For that eld, the pattern <Allcaps> was overly general, and a wrong
group received the highest score during the scoring step of the reinduction algorithm. The
average precision and recall values were P = 0.90 and R = 0.80.
Within the data set we studied, ve sources, listed in Table 9, experienced a total of seven
changes. In addition to these sources, the airport source changed the format of the data it
returned, but since it simultaneously changed the presentation of data from a detail page to
a list, we could not use this data to learn Stalker rules. Table 9 shows the performance of
the automatically reinduced wrappers for the changed sources. For most elds precision P ,
the more important of the performance measures, is close to its maximum value, indicating
that there were few false positives. However, small values of recall indicate that not all
examples of these elds were extracted. This result can be traced to a limitation of our
approach: if the same eld appears in a dierent context, more than one rule is necessary
9. Limitations in the data collection procedure prevented us from accumulating large data sets for all
sources; therefore, in order to keep the methodology uniform across all sources, we decided to use
smaller training sets.

171

fiLerman, Minton & Knoblock

source/field
airport code
airport name
Amazon author
Amazon title
Amazon price
Amazon ISBN
Amazon availability
Barnes&N oble author
Barnes&N oble title
Barnes&N oble price
Barnes&N oble ISBN
Barnes&N oble availability
Bigbook name
Bigbook street
Bigbook city
Bigbook state
Bigbook phone
mapquest time
mapquest distance
Quote pricechange
Quote ticker
Quote volume
Quote shareprice
Smartpages name
Smartpages street
Smartpages city
Smartpages state
Smartpages phone
Y ahoo Quote pricechange
Y ahoo Quote ticker
Y ahoo Quote volume
Y ahoo Quote shareprice
W ashington P ost price
W eather temp
W eather outlook
average

ex %
100
90
100
70
100
100
60
100
80
90
100
90
70
90
70
100
90
100
100
50
63
100
38
80
80
0
100
100
100
100
100
80
100
40
90
83

p
1.0
1.0
97.3
98.8
1.0
1.0
1.0
0.93
0.96
1.0
1.0
1.0
1.0
1.0
0.91
0.04
1.0
1.0
1.0
0.38
0.93
1.0
0.46
1.0
1.0
0.68
1.0
0.99
1.0
1.0
1.0
1.0
1.0
0.36
0.83
0.90

r
1.0
1.0
0.92
0.81
0.99
0.91
0.86
0.96
0.62
0.68
0.95
0.92
0.76
0.87
0.98
0.50
0.30
0.98
0.98
0.36
0.87
0.88
0.60
0.82
0.52
0.58
0.70
1.0
0.41
0.98
0.99
0.59
1.0
0.82
1.0
0.80

Table 8: Reinduction results on ten Web sources. The rst column lists the fraction of the
elds for each source that were correctly extracted by the pattern-based algorithm.
We judged the eld to be extracted if the algorithm correctly identied at least
two examples of it. The last two columns list precision and recall on the data
extraction task using the reinduced wrappers.

172

fiWrapper Maintenance

100

extraction accuracy (%)

80

60

40
PHONE
STATE
CITY
NAME
STREET

20

0
0

5

10

15

20

25

number of training examples

Figure 8: Performance of the reinduction algorithm for the elds in the Smartpages source
as the size of the training set is increased

source/field
Amazon author
Amazon title
Amazon price
Amazon ISBN
Amazon availability
Barnes&N oble author
Barnes&N oble title
Barnes&N oble price
Barnes&N oble ISBN
Barnes&N oble availability
Quote pricechange
Quote ticker

P
1.0
1.0
0.9
1.0
1.0
1.0
1.0
1.0
1.0
1.0
0.0
1.0

R
1.0
0.7
0.9
0.9
0.9
0.5
0.8
1.0
1.0
1.0
0.0
1.0

A
1.0
0.7
0.9
0.9
0.9
0.5
0.8
1.0
1.0
1.0
0.0
1.0

source/field
Smartpages name
Smartpages street
Smartpages city
Smartpages state
Smartpages phone
Y ahoo Quote pricechange
Y ahoo Quote ticker
Y ahoo Quote volume
Y ahoo Quote shareprice
Quote volume
Quote shareprice

P
1.0
N/A
0.0
1.0
N/A
1.0
1.0
1.0
1.0

R
0.9
0.0
0.0
0.9
0.0
0.2
0.5
0.7
0.7

A
0.9
0.0
0.0
0.9
0.0
0.2
0.5
0.7
0.7

1.0
0.0

1.0
N/A

1.0
0.0

Table 9: Precision, recall, and accuracy of the learned STALKER rules for the changed
sources

to extract it from a source. In such cases, we extract only a subset of the examples that
share the same context, but ignore the rest of the examples.
As mentioned earlier, we believe we can achieve better performance for the yellow-pagestype sources Bigbook and Smartpages by using more training examples. Figure 8 shows the
eect increasing the size of the training example set on the performance of the automatically
generated wrappers for the Smartpages source. As the number of training examples goes
up, the accuracy of most extracted elds goes up.
173

fiLerman, Minton & Knoblock

3.2.4 Lists
We have also applied the reinduction algorithm to extract data from pages containing lists
of tuples, and, in many cases, have successfully extracted at least several examples of each
eld from several pages. However, in order to learn the correct extraction rules for sources
returning lists of data, Stalker requires that the rst, last and at least two consecutive
list elements be correctly specied. The methods presented here cannot guarantee that
the required list elements are extracted, unless all the list elements are extracted. We are
currently working on new approaches to data extraction from lists (Lerman, Knoblock, &
Minton, 2001) that will enable us to use Stalker to learn the correct data extraction rules.

4. Previous Work
There has been a signicant amount of research activity in the area of pattern learning. In
the section below we discuss two approaches, grammar induction and relational learning,
and compare their performance to DataProG on tasks in the Web wrapper application
domain. In Section 4.2 we review previous work on topics related to wrapper maintenance,
and in Section 4.3 we discuss related work in information extraction and wrapper induction.
4.1 Pattern Learning
4.1.1 Grammar induction
Several researchers have addressed the problem of learning the structure, or patterns, describing text data. In particular, grammar induction algorithms have been used in the past
to learn the common structure of a set of strings. Carrasco and Oncina proposed ALERGIA (Carrasco & Oncina, 1994), a stochastic grammar induction algorithm that learns a
regular language from positive examples of the language. ALERGIA starts with a nite
state automaton (FSA) that is initialized to be a prex tree that represents all the strings
of the language. ALERGIA uses a state-merging approach (Angluin, 1982; Stolcke & Omohundro, 1994) in which the FSA is generalized by merging pairs of statistically similar (at
some signicance level) subtrees. Similarity is based purely on the relative frequencies of
substrings encoded in the subtrees. The end result is a minimum FSA that is consistent
with the grammar.
Goan et al. (Goan et al., 1996) found that when applied to data domains commonly
found on the Web, such as addresses, phone numbers, etc., ALERGIA tended to merge
too many states, resulting in an over-general grammar. They proposed modications to
ALERGIA, resulting in algorithm WIL, aimed at reducing the number of faulty merges. The
modications were motivated by the observation that each symbol in a string belong to one
of the following syntactic categories: NUMBER, LOWER, UPPER and DELIM. When
viewed on the syntactic level, data strings contain additional structural information that can
be eectively exploited to reduce the number of faulty merges. WIL merges two subtrees if
they are similar (in the ALERGIA sense) and also if, at every level, they contain nodes that
are of the same syntactic type. WIL also adds a wildcard generalization step in which the
transitions corresponding to symbols of the same category that are approximately evenly
distributed over the range of that syntactic type (e.g., 09 for numerals) are replaced with
a single transition corresponding to the type (e.g., NUMBER). Goan et al. demonstrated
174

fiWrapper Maintenance

that the grammars learned by WIL were more eective in recognizing new strings in several
relevant Web domains.
We compared the performance of WIL to DataProG on the wrapper verication task.
We used WIL to learn the grammar on the token level using data examples extracted by
the wrappers, not on the character level as was done by Goan et al.Another dierence from
Goan et al. was that, whereas they needed on the order of 100 strings to arrive at a high
accuracy rate, we have on the order of 2030 examples to work with. Note that we can
no longer apply the wildcard generalization step to the FSA because we would need many
more examples to decide whether the token is approximately evenly distributed over that
syntactic type. Instead, we compare DataProG against two versions of WIL: one without
wildcard generalization (WIL1), and one in which every token in the initial FSA is replaced
by its syntactic type (WIL2). In addition to the syntactic types used by Goan et al., we
also had to introduce another type ALNUM to be consistent with the patterns learned by
DataProG. Neither version of WIL allows for multi-level generalization.
The algorithms were tested on data extracted by wrappers from 26 Web sources on ten
dierent occasions over a period of several months (see Sec. 3.1). Results of 2030 queries
were stored every time. For each wrapper, one data set was used as the training examples,
and the data set extracted on the very next date was used as test examples. We used WIL1
and WIL2 to learn the grammar of each eld of the training examples and then used the
grammar to recognize the test examples. If the grammar recognized more than 80% of the
test examples of a data eld, we concluded that it recognized the entire data eld; otherwise,
we concluded that the grammar did not recognize the eld, possibly because the data itself
has changed. This is the same procedure we used in the wrapper verication experiments,
and it is described in greater detail in Section 3.1.1. Over the period of time covered by
the data, there were 21 occasions on which a Web site changed, thereby causing the data
extracted by the wrapper to change as well. The precision and recall values for WIL1
(grammar induction on specic tokens) were P = 0.20, and R = 0.81; for WIL2 (grammar
induction on wildcards representing tokens syntactic categories) the values were P = 0.55
and R = 0.76. WIL1 learned an overly specic grammar, which resulted in a high rate
of false positives on the verication task, while WIL2 learned an overly general grammar,
resulting in slightly more false negatives. The recall and precision value of DataProG for
the same data were P = 0.73 and R = 1.0.
Recently Thollard et al. (Thollard, Dupont, & de la Higuera, 2000) introduced MDI, an
extension to ALERGIA. MDI has been shown to generate better grammars in at least one
domain by reducing the number of faulty merges between states . MDI replaces ALERGIAs
state merging criterion with a more global measure that attempts to minimize the KullbackLeibler divergence between the learned automaton and the training sample while at the same
time keeping the size of the automaton as small as possible. It is not clear whether MDI
(or a combination of MDI/WIL) will lead to better grammars for common Web data types.
We suspect not, because regular grammars capture just a few of the multitude of data types
found on the Web. For example, business names, such as restaurant names shown in Table 2
may not have a well dened structure, yet many of them start with two capitalized words
and end with the word Restaurant  which constitute patterns learned by DataProG.
175

fiLerman, Minton & Knoblock

4.1.2 Relational learning
As a sequence of n tokens, a pattern can also be viewed as a non-recursive n-ary predicate.
Therefore, we can use a relation-learning algorithm like FOIL (Quinlan, 1990) to learn them.
Given a set of positive and negative examples of a class, FOIL learns rst order predicate
logic clauses dening the class. Specically, it nds a discriminating description that covers
many positive and none of the negative examples.
We used Foil.6 with the no-negative-literals option to learn patterns describing several
dierent data elds. In all cases the closed world assumption was used to construct negative
examples from the known objects: thus, for the Bigbook source, names and addresses were
the negative examples for the phone number class. We used the following encoding to
translate the training examples to allow foil.6 to learn logical relations. For each data eld,
FOIL learned clauses of the form
data f ield(A) := P (A) f ollowed by(A, B) P (B) ,

(3)

as a denition of the eld, where A and B are tokens, and the terms on the right hand side
are predicates. The predicate f ollowed by(A, B) expresses the sequential relation between
the tokens. The predicate P (A) allows us to specify the token A as a specic token (e.g.,
John(A)) or a general type (e.g., Upper(A), Alpha(A)), thus, allowing FOIL the same
multi-level generalization capability as DataProG.
We ran Foil.6 on the examples associated with the Bigbook (see Tables 23). The
relational denitions learned by Foil.6 from these examples are shown in Table 10.
In many cases, there were similarities between the denitions learned by FOIL and
the patterns learned by DataProG, though clauses learned by FOIL tended to be overly
general. Another problem was when given examples of a class with little structure, such
as names and book titles, FOIL tended to create clauses that covered single examples, or it
failed to nd any clauses. In general, the description learned by FOIL depended critically
on what we supplied as negative examples of that eld. For example, if we were trying to
learn a denition for book titles in the presence of prices, FOIL would learn that something
that starts with a capitalized word is a title. If author names were supplied as negative
examples as well, the learned denition would have been dierent. Therefore, using FOIL
in situations where the complete set of negative examples is not known or available, is
problematic.
4.2 Wrapper Maintenance
Kushmerick (Kushmerick, 1999) addressed the problem of wrapper verication by proposing
an algorithm Rapture to verify that a wrapper correctly extracts data from a Web page.
In that work, each data eld was described by a collection of global features, such as
word count, average word length, and density of types, i.e., proportion of characters in the
training examples that are of an HTML, alphabetic, or numeric type. Rapture calculated
the mean and variance of each features distribution over the training examples. Given a
set of queries for which the wrapper output is known, Rapture generates a new result for
each query and calculates the probability of generating the observed value for every feature.
Individual feature probabilities are then combined to produce an overall probability that
the wrapper has extracted data correctly. If this probability exceeds a certain threshold,
176

fiWrapper Maintenance

*** Warning:
NAME(A)
NAME(A)
NAME(A)

the following definition does not cover 23 tuples in the relation
:= AllCaps(A), followed by(A,B)
:= Upper(A), followed by(A,B), Number(B)
:= followed by(A,B), Venice(B)

STREET(A) := Large(A), followed by(A,B)
STREET(A) := Medium(A), followed by(A,B), AlphaNum(B)

** Warning:
CITY(A)
CITY(A)
CITY(A)
CITY(A)
CITY(A)

the following definition does not cover 9 tuples in the relation
:= Los(A)
:= Marina(A)
:= New(A)
:= Brooklyn(A)
:= West(A), followed by(A,B), Alpha(B)

STATE(A) := CA(A)
STATE(A) := NY(A)
PHONE(A) := ((A)

Table 10: Denitions learned by foil.6 for the Bigbook source
Rapture decides that the wrapper is correct; otherwise, that it has failed. Kushmerick
found that the HTML density alone can correctly identify almost all of the changes in
the sources he monitored. In fact, adding other features in the probability calculation
signicantly reduced algorithms performance. We compared Raptures performance on
the verication task to our approach, and found that Rapture missed 17 wrapper changes
(false negatives) if it relied solely on the HTML density feature. 10
There has been relatively little prior work on the wrapper reinduction problem. Cohen (Cohen, 1999) adapted WHIRL, a soft logic that incorporates a notion of statistical
text similarity, to recognize page structure of a narrow class of pages: those containing
simple lists and simple hotlists (dened as anchor-URL pairs). Previously extracted data,
combined with page structure recognition heuristics, was used to reconstruct the wrapper
once the page structure changed. Cohen conducted wrapper maintenance experiments using original data and corrupted data as examples for WHIRL. However, his procedure for
corrupting data was neither realistic nor representative of how data on the Web changes.
Although we cannot at present guarantee good performance of our algorithm on the wrapper reinduction for sources containing lists, we handle the realistic data changes in Web
sources returning detail pages.
10. Although we use a dierent statistical test and cannot compare the performance of our algorithm to
Rapture directly, we doubt that it would outperform our algorithm on our data set if it used all global
numeric features, because, as we noted in Section 3.1.1, using patterns as well as global numeric features
in the verication task outperforms using numeric features only.

177

fiLerman, Minton & Knoblock

4.3 Information Extraction
Our system, as used in the reinduction task, is related in spirit to the many information
extraction (IE) systems developed both by our group and others in that it uses a learned
representation of data to extract information from specic texts. Like wrapper induction
systems (see (Muslea et al., 2001; Kushmerick et al., 1997; Freitag & Kushmerick, 2000)),
it is domain independent and works best with semi-structured data, e.g., Web pages. It
does not handle free text as well as other systems, such as AutoSlog (Rilo, 1993) and
Whisk (Soderland, 1999), because free text has fewer non-trivial regularities the algorithm
can exploit. Unlike wrapper induction, it does not extract data based on the features that
appear near it in text, but rather based on the content of data itself. However, unlike Whisk,
which also learns content rules, our reinduction system represents each eld independently
of the other elds, which can be an advantage, for instance, when a web source changes the
order in which data elds appear. Another dierence is that our system is designed to run
automatically, without requiring any user interaction to label informative examples. In the
main part because it is purely automatic, the reinduction system fails to achieve the accuracy
of other IE systems which rely on labeled examples to train the system; however, we do
not see it as a major limitation, since it was designed to complement existing extraction
tools, rather than supersede them. In other words, we consider the reinduction task to be
successful if it can accurately extract a sucient number of examples to use in a wrapper
induction system. The system can then use the resulting wrapper to accurately extract the
rest of the data from the source.
There are many similarities between our approach and that used by the RoadRunner system, developed concurrently with our system and reported recently in (Crescenzi,
Mecca, & Merialdo, 2001b, 2001a). The goal of that system is to automatically extract
data from Web sources by exploiting similarities in page structure across multiple pages.
RoadRunner works by inducing the grammar of Web pages by comparing several pages
containing long lists of data. The grammar is expressed at the HTML tag level, so it is
similar to the extraction rules generated by Stalker. The RoadRunner system has been
shown to successfully extract data from several Web sites. The two signicant dierences
between that work and ours are (i) they do not have a way of detecting changes to know
when the wrapper has to be rebuilt and (ii) our reinduction algorithm works on detail pages
only, while RoadRunner works only on lists. We believe that our data-centric approach is
more exible and will allow us to extract data from more diverse information sources than
the RoadRunner approach that only looks at page structure.

5. Conclusion
In this paper we have described the DataProG algorithm, which learns structural information about a data eld from a set of examples of the eld. We use these patterns in
two Web wrapper maintenance applications: (i) verification  detecting when a wrapper
stops extracting data correctly from a Web source, and (ii) reinduction  identifying new
examples of the data eld in order to rebuild the wrapper if it stops working. The verication algorithm performed with an accuracy of 97%, much better than results reported
in our earlier work (Lerman & Minton, 2000). In the reinduction task, the patterns were
used to identify a large number of data elds on Web pages, which were in turn used to
178

fiWrapper Maintenance

automatically learn Stalker rules for these Web sources. The new extraction rules were
validated by using them to successfully extract data from sets of test pages.
There remains work to be done on wrapper maintenance. Our current algorithms are not
sucient to automatically re-generate Stalker rules for sources that return lists of tuples.
However, preliminary results indicate (Lerman et al., 2001) that it is feasible to combine
information about the structure of data with a priori expectations about the structure of
Web pages containing lists to automatically extract data from lists and assign it to rows and
columns. We believe that these techniques will eventually eliminate the need for the user to
mark up Web pages and enable us to automatically generate wrappers for Web sources. Another exciting direction for future work is using the DataProG algorithm to automatically
create wrappers for new sources in some domain given existing wrappers for other sources
in the same domain. For example, we can learn the author, title and price elds for the
AmazonBooks source, and use them to extract the same elds on the Barnes&N obleBooks
source. Preliminary results show that this is indeed feasible. Automatic wrapper generation
is an important cornerstone of information-based applications, including Web agents.

6. Acknowledgments
We would like to thank Priyanka Pushkarna for carrying out the wrapper verication experiments.
The research reported here was supported in part by the Defense Advanced Research
Projects Agency (DARPA) and Air Force Research Laboratory under contract/agreement
numbers F30602-01-C-0197, F30602-00-1-0504, F30602-98-2-0109, in part by the Air Force
Oce of Scientic Research under grant number F49620-01-1-0053, in part by the Integrated
Media Systems Center, a National Science Foundation (NSF) Engineering Research Center,
cooperative agreement number EEC-9529152 and in part by the NSF under award number
DMI-0090978. The U.S. Government is authorized to reproduce and distribute reports for
Governmental purposes notwithstanding any copy right annotation thereon. The views
and conclusions contained herein are those of the authors and should not be interpreted as
necessarily representing the ocial policies or endorsements, either expressed or implied, of
any of the above organizations or any person connected with them.

References
Abramowitz, M., & Stegun, I. A. (1964). Handbook of mathematical functions with formulas, graphs and mathematical tables. Applied Math. Series 55. National Bureau of
Standards, Washington, D.C.
Ambite, J.-L., Barish, G., Knoblock, C. A., Muslea, M., Oh, J., & Minton, S. (2002).
Getting from here to there: Interactive planning and agent execution for optimizing
travel. In The Fourteenth Innovative Applications of Articial Intelligence Conference
(IAAI-2002), Edmonton, Alberta, Canada, 2002.
Angluin, D. (1982). Inference of reversible languages. Journal of the ACM, 29 (3), 741765.
179

fiLerman, Minton & Knoblock

Brazma, A., Jonassen, I., Eidhammer, I., & Gilbert, D. (1995). Approaches to the automatic discovery of patterns in biosequences. Tech. rep., Department of Informatics,
University of Bergen.
Carrasco, R. C., & Oncina, J. (1994). Learning stochastic regular grammars by means of a
state merging method. Lecture Notes in Computer Science, 862, 139.
Chalupsky, H., et al. (2001). Electric elves: Applying agent technology to support human
organizations. In Proceedings of the Thirteenth Annual Conference on Innovative
Applications of Articial Intelligence (IAAI-2001), Seattle, WA.
Cohen, W. W. (1999). Recognizing structure in web pages using similarity queries. In Proc.
of the 16th National Conference on Articial Intelligence (AAAI-1999), pp. 5966.
Crescenzi, V., Mecca, G., & Merialdo, P. (2001a). Automatic web information extraction
in the roadrunner system. In Proceedings of the International Workshop on Data
Semantics in Web Information Systems (DASWIS-2001).
Crescenzi, V., Mecca, G., & Merialdo, P. (2001b). RoadRunner: Towards automatic data
extraction from large web sites. In Proceedings of the 27th Conference on Very Large
Databases (VLDB) Rome, Italy.
Dietterich, T., & Michalski, R. (1981). Inductive learning of structural descriptions.. Articial Intelligence, 16, 257294.
Doorenbos, R. B., Etzioni, O., & Weld, D. S. (1997). A scalable comparison-shopping
agent for the world-wide webs. In Proceeding of the First International Confence on
Autonomous Agents, Marina del Rey.
Freitag, D., & Kushmerick, N. (2000). Boosted wrapper induction. In Proceedings of the 7th
Conference on Articial Intelligence (AAAI-2000), pp. 577583. AAAI Press, Menlo
Park, CA.
Goan, T., Benson, N., & Etzioni, O. (1996). A grammar inference algorithm for the world
wide web.. In Proceedings of AAAI Spring Symposium on Machine Learning in Information Access, Stanford University, CA.
Hsu, C.-N., & Dung, M.-T. (1998). Generating nite-state transducers for semi-structured
data extraction from the web. Journal of Information Systems, 23, 521538.
Knoblock, C. A., Lerman, K., Minton, S., & Muslea, I. (2001a). Accurately and reliably
extracting data from the web: A machine learning approach. IEEE Data Engineering
Bulletin, 23 (4), 3341.
Knoblock, C. A., Minton, S., Ambite, J. L., Muslea, M., Oh, J., , & Frank, M. (2001b).
Mixed-initiative, multi-source information assistants. In The Tenth International
World Wide Web Conference (WWW10), Hong Kong.
Kushmerick, N. (1999). Regression testing for wrapper maintenance.. In Proceedings of the
14th National Conference on Articial Intelligence (AAAI-1999).
180

fiWrapper Maintenance

Kushmerick, N., Weld, D. S., & Doorenbos, R. B. (1997). Wrapper induction for information extraction. In Proceedings of the Intl. Joint Conference on Articial Intelligence
(IJCAI), pp. 729737.
Lerman, K., Knoblock, C. A., & Minton, S. (2001). Automatic data extraction from lists and
tables in web sources. In Proceedings of the workshop on Advances in Text Extraction
and Mining (IJCAI-2001) Menlo Park. AAAI Press.
Lerman, K., & Minton, S. (2000). Learning the common structure of data. In Proceedings
of the 15th National Conference on Articial Intelligence (AAAI-2000) Menlo Park.
AAAI Press.
Muggleton, S. (1991). Inductive logic programming. New Generation Computing, 8, 295
318.
Muslea, I., Minton, S., & Knoblock, C. (1998). Wrapper induction for semistructured webbased information sources.. In Proceedings of the Conference on Automated Learning
and Discovery (CONALD).
Muslea, I., Minton, S., & Knoblock, C. A. (2001). Hierarchical wrapper induction for
semistructured information sources. Autonomous Agents and Multi-Agent Systems, 4,
93114.
Papoulis, A. (1990). Probability and Statistics. Prentice Hall, Englewood Clis, NJ.
Quinlan, J. R. (1990). Learning logical denitions from relations.. Machine Learning, 5 (3),
239266.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann, San
Mateo, CA.
Rilo, E. (1993). Automatically constructing a dictionary for information extraction tasks.
In Proceedings of the 11th National Conference on Articial Intelligence, pp. 811816
Menlo Park, CA, USA. AAAI Press.
Soderland, S. (1999). Learning information extraction rules for semi-structured and free
text. Machine Learning, 34 (1-3), 233272.
Stolcke, A., & Omohundro, S. (1994). Inference of nite-state probabilistic grammars. In
Proceedings of the 2nd Int. Colloquium on Grammar Induction, (ICGI-94), pp. 106
118.
Thollard, F., Dupont, P., & de la Higuera, C. (2000). Probabilistic DFA inference using
Kullback-Leibler divergence and minimality. In Proceedings of the 17th International
Conf. on Machine Learning, pp. 975982. Morgan Kaufmann, San Francisco, CA.

181

fi
	ff
fi 
			 ! #"$ % 
'&)( *,+.-//021304!(657720

89:;< =?>2@
/-!AB:	%&=C>2@
/0

DFEHGJIKGJLMNOM2GQPSRUTWV$PSXZY,I[Y,PSXZY\PS]<Y)^
_

G)Ea`cbZT2R,dfegRhEaM2RUiZT2YjV$PSXZY\IKY\PSXZY\PS]<YkRUPSX
_

npqso r3w
tu vxqWy?z3{#|

G)EHlJYmN3NHMPSl

}~33HJ.Hff

 \3#s#'J?)f
 sffpa
O$
 $
z uwuy?qrz$u3rq
) 2s'w292
h.
6J.

}~$OJw$$3\~.

ws
ff
 w,32!s
!  <\6


qrrqz$ra
$H$\?ws
fQ
 $ ,6.

~333Q}$$3 ~3$3ff

f)J ws
f 
am  )w2fQa
O$

?H

6sff f[m<s.  
 )	
 ff fiff   9 ff
 [f
99Kf9ffS [ ?2	Jff6Khs$#fC
fif.#
 wf9s9fs2 f
   	C ff.<fJ?.	"!K< f9ff9
f#
 f
fi %ff$ w &H 6f ff  <ff?h ff.<	 'f9s9fs2(m#)

s  !S 
!%$*
   p  !+ <ff.
ff Cf,ff [ Q     
 ? 9
 
-99pf
./K
! h.  2f. # Cff fiff  3  ff01  #.

ffQ
  
2ff
$  9# 3ffi ff [ps hf4     hSf )5 9Uf  f9[  
fff
fi 2s\ sfi 2 /
! ff\ s
Q 2!/.
 
f Jf
 6#
 O# hff

7ff)
9wfff
fi  5 98 9ff fpps9fi :;9U	 2
 ff.'w< f2,
  
 f <H3ff$  9ff (ffi Cf=sfi 9Zff>  !6f 
. 9 ffff) 
$ m?fi U Q
 !
9&s
fi U
ff
7f96

B@ AC;D ffEF+\G ffHIE D
JLKMN.OPOQKMMRTS#KUV&W'XKMYKZ[\MY]ff^K^]#[RTS0_0[RT]ffZ.MP`[a6K4MOQ]#W/K]#b/[a6KW'_0W/KPX`_#Z.c[a.KOQ]ffZ[YXRTd'N6[R]ffZe_#Z.c
[a6K]#Xfff_#Z.RTg_0[RT]ffZh]#b8[a6K?W"_0WiKPXj
.k lmk u$onzu{
p%Z^e_#ZV&MRT[N._0[R]ffZ.M8]#b'KPS#KPXV*c._V2UoRTb1K#`0q:K_0XKOQ]ffZ6brX]ffZ[YKcq4RT[a&[a6K,W.X]#d'UTK^s]#b8tuPvuQwx2y1z'y1z{}|8~ffv
y1wuQuQ00z'vj++MYW/KOPR_#UUTV#`*_#M,_&W.XKURo^eRZ._0XVMY[YKPWh[Y]S0_0XRT]ffN'M:RoZ[YKUUoRTf#KZ[:[_#MY*MrK#jf6jT`.W'Uo_#Z.Z.RZ6f6`c6KQ
OPRMR]ffZ^e_0*RZ6f6`XK_#MY]ffZ.RZ.f `8R[RMZ'_0[N6X_#U=_#Z.cXK_#MY]ffZ._0d"UTK[Y]c.RMOP_0XcKPS#KPXV[a.RZ6fd"N6[q4a._0[&RM
XKUTKPS0_#Z[[Y]_#Oa.RTKPS#K4[a6K^KPOPRTKZ[UTV#j6]#X\RZ'MY[_#Z.OQK#`ffd/KPbr]#XK,MY[_0X[RZ6f3[Y]q4XRT[YK:[a.RM8W'_0W/KPX`#q:Ka._#c
[Y]OQ]ffZ'MRc6KPX[a6KXKUKPS;_#Z[UR[YKPX_0[N6XK?]ffZ[a6K[Y]#W"RO0`"_#Z.c]ffZ.UTVhRT[P"fff_0[a6KPXRZ6f]ffZ]ffN6Xc6KMY*M4_#UU[a6K
W'_0W/KPXM_0di]ffN.[4XKUTKPS0_#Z.OQK}[a._0[4q:K&a'_S#K2_0[a._#Z'cUTKcN.M[Y]MYKP[3_q(_V]ffN6Xb_S#]ffN6XR[YK&OQ]]#hd/]]#*MP`
d/KOP_#N.MYK?[a6KPV_0XK]#b8Z6]ea6KUWh[Y]e[a6K?[_#MY]#bqXRT[RZ.f&[a'RM(W'_0W/KPXja6K_0d'RoURT[V[Y]eb1]*OPN.M,]ffZq4a._0[
RM4XKUTKPS0_#Z[2r]#X3c'N._#UUTV[Y]c.RoMOP_0Xcqa._0[RMZ6]#[ ,OP_#Zd/K}OQ]ffZ.MRc6KPXKc_#M_OQKZ[YX_#UI`iO a._0X_#OQ[YKPXRM[RO
brK_0[N6XK4]#bBRZ[YKUURf#KZ.OQK#RT[=RoMd/KURTKPS#Kc[a._0[+]S#KPX,0]#bB[a.K4Z6KN6X]ffZ'_#U.OQ]ffZ.Z6KOQ[RT]ffZ'M=]#b/]ffN6X\d'X_#RZ.M


"

(

:

\



o 	

-//0 %% !=U
= C2
 H
!fi;
 :!	%&% f		&2%3% 2 =

fi

~33



s~$w

~ww$

_0XK3RZ.a.Rd'RT[Y]#XV2_#Z.cMYKPXS#K[Y]&fffRS#KN6WMYKZ.M]#XR_#U'RoZ6W'N6[M	*N6d.X_#^_#Z.R_#ZB`ff
3XKRZ6KPX`fi9K_0X Um`## j
a.RoMK *W'U_#RoZ.Mq4aVRTXXKUKPS;_#Z.OQK#` N'Z.c6KPX3S;_0X RT]ffN.M3Z._#^KM_#MRZ.c6KPW/KZ.c6KZ'OQK#`iRTXXKc.N.Z.c'_#Z.OQV#`BRZ 'N*
KZ.OQK_0d'RoURT[V#`Z6]S#KU[V#`=MYKPW'_0X _0d'RURT[V#`RMZ.]q(_#c._V*M}OQ]ffZ'MRc6KPXKc_#M}_#ZRo^W/]#X[_#Z[}Z6]#[RT]ffZRZ^e_#ZV
 KUc'M]#b+_0X[R  OPR_#U9RZ[YKUoURTf#KZ.OQKMYKPK_MN.XS#KPVbr]#X3^e]#XK&c.KP[_#RUMP`iK#jf6jT` 
XKRZ.KP
X fi*N6d.X _#^e_#Z.R_#ZB`
#  6N6d.X_#^e_#Z'R_#ZKP[4_#UmjT` # # j
p%Z [a6Kb1]ffUU]q4RZ.f6`+q:K_0XKOQ]ffZ.OQKPXZ6Kc q4RT[a wuQuQ00z u !0wwYu; !#z.y1z{#j p%Z [a.RMb1X_#^eKPq+]#Xi`
[a6K[_#MY RM[VW'ROP_#UoUTV [a._0[]#bc6KP[YKPX^eRoZ.RZ6fqa6KP[a6KPXMY]ff^K W'RTKOQK]#bZ.]q4UTKc.f#K #
_ "N6KPXV.%
 $
OP_#Z d/Kc.KPXRTS#Kc brX]ff^ _Z6]	q4UTKc6f#Kd'_#MY'
K &(
j KPS#KPX_#U4XK_#MY]ffZ.RZ.fMOa.K^KMOP_#Z diK[_0#KZ RZ[Y]
_#OPOQ]ffN.Z[:a.KPXK#`brX]ff^ [a6KOPU_#MMROP_#U.]ffZ.K&RZ6brKPXKZ.OQKRM\OPU_#MMROP_#U6KZ[_#RU^KZ[ =[Y]}^]#XKMY]#W'a.RoMY[ROP_0[YKc
]ffZ6KM?OQ]ff^e^]ffZ6MYKZ.MYKRZ.b1KPXKZ'OQK	 jJ a6KZc6K_#URoZ6fq4R[aMN'OaXK_#MY]ffZ.RoZ6f}[_#MMP`6XKUTKPS;_#Z'OQKRM:]#br[YKZ
K*W'UT]ffRT[YKc&MY]3_#M9[Y]?^e_0#K,RZ.b1KPXKZ'OQK+^]#XK:KPOPRTKZ[8b1X]ff^ _OQ]ff^W'N6[_0[R]ffZ._#UWi]ffRoZ[9]#b.SRKPqjp%Z&]#[a6KPX
XK_#MY]ffZ'RZ6f W.X]#d'UK^eMP`[a6KLW'N6XW/]ffMYK RM[Y] K *W'URoOPRT[UTV c6KPX RTS#KLMY]ff^KRZ[YKZ.MR]ffZ._#UUTVOa._0X _#OQ[YKPXRTgPKc
W'RTKOQKM&]#b*Z6]	q4UTKc6f#KLrK#jf6jT`=[YKUU:^K_#UU\qa._0[}V#]ffN *Z6]q _0d/]ffN6[2,q:KPKP[V. j .]#XeMN.Oa W.X]#d'UTK^eM
r[a._0[_0XKZ.]#[:XKc.N.OPRTd'UK4[Y]ec6KOPRMRT]ffZW.X]#d'UTK^M  `XKUTKPS;_#Z'OQK3_#UoMY]a._#M,_2X]ffUTK3[Y]W'U_V#`*b1]#XRZ'MY[_#Z.OQK
dV_#UUT]	q4RZ6fN.M[Y]Oa._0X _#OQ[YKPXRTgPKe[a6K2W'RKOQKM]#b:RZ6br]#X^e_0[RT]ffZq:K_0XKRZ[YKPXKMY[YKcLRZdVb1]#Xf#KP[Y[RoZ6f
[a6K]#[a.KPX]ffZ6KMPj
9]q4a._0[?K *[YKZ[RM?[a6Kf#]ff_#U\]#b:Ro^W.X]	SRZ.fRZ.b1KPXKZ'OQKXK_#Oa._0d"UT*K ) p%Z ]#Xc6KPX[Y]_#c.c6XKMM3[a.RM
W/]ffRZ[P`"_e#KPVRMMN6K}RM[a6+
K ,!0.x -/6vI#v0y !0z"#1 ,!0.
x -"3u 2ffyr5v 4}]ffZ6K#jp%Z.c6KPKcB`i_#MMN.^eK[a._0[q+K}Z.]qs[a._0[
[a6K+XKMY]ffUN6[R]ffZ?]#b6MY]ff^K\XK_#MY]ffZ.RZ6f4W.X]#d'UTK^M OP_#Zd/K\MWiKcN6W]ffZ.OQK+XKUTKPS0_#Z[RZ6br]#X^e_0[RT]ffZ?a'_#Md/KPKZ
KUROPR[YKcBj+p%Z[a.KMRT[N._0[R]ffZq4a6KPXKRT[4RM4OQ]ff^W'N6[_0[RT]ffZ'_#UUTVha._0Xc6KPX4[Y]W/]ffRZ[]ffN6[4MN'OaRZ6br]#X^e_0[RT]ffZ
brX]ff^ [a.K}RZ6W"N6[[a._#Z[Y]XK_#MY]ffZc'RTXKOQ[UTVbrX]ff^ [a.K}RZ6W"N6[P`'OQ]ff^W'N.[_0[RT]ffZ._#U9diKZ.K  [M4_0XK2a._0Xc[Y]
d/KK W/KOQ[YKcBjpb:MY]6`9_#UT[YKPX Z._0[RTS#KN.MKM?]#b+XKUTKPS;_#Z'OQKeb1]#XXK_#MY]ffZ.RZ.f_0XK[Y]d/KeRZS#KMY[RTfff_0[YKcBj.]#X
RZ.M[_#Z.OQK#`9MYK_0X Oa.RZ.fhb1]#X}XKUTKPS0_#Z.OQKeRZ6br]#X^e_0[RT]ffZLOP_#Zd/KeUR^eRT[YKc dV OQ]ffZ.MRc6KPX RZ6f]ffZ.UTVW'RTKOQKM]#b
*Z6]q4UKc6f#K,[a._0[\OP_#Zd/K,f#KZ6KPX _0[YKcRZ_3[YX_#OQ[_0d"UTKq(_V#jpbiMN'OaeRZ.b1]#X^_0[RT]ffZ2c6KPW/KZ.c2]ffZ.UTV}]ffZ[a6K
*Z6]q4UKc6f#K\d'_#MK#`0_#Z.]#[a6KPX9W/]ffMMRTd'UTK=_0W.W'X]ff_#Oa&RM [Y]r[YKZ[_0[RTS#KUTV.OQ]ff^eWiKZ'M_0[YK\[a6K:OQ]ff^W'N6[_0[R]ffZ._#U
XKMY]ffN.XOQKMMWiKZ[RoZ c6KPXRTS*RZ6fL[a.KXKUTKPS0_#Z.OQK RZ6br]#X^e_0[RT]ffZ [a6X]ffN6fffa ^e_#Z6
V "N6KPXRKMOQ]ff^W'N.[RZ6f
W'RTKOQKM]#bXKUTKPS;_#Z[RZ6br]#X^e_0[R]ffZhOP_#Z[a6KZhd/K?S*RTKPq:Kch_#M4_br]#X^ ]#b8OQ]ff^W'RUo_0[RT]ffZ" j
.k l87:9; u=<\q u>U*?#q  z<qr
@R[Y[UTKRM3Z.]q4ZL_0d/]ffN6[3[a6KeOQ]ff^W"N6[_0[RT]ffZ._#U=OQ]ff^eW'UTK6RT[V]#b+XKUKPS;_#Z.OQK#ja'RMW'_0W/KPX3OQ]ffZ[YXRTd"N6[YKM
[Y]  UU[a.RMfff_0W j&a6K2OQ]ff^W'UTK 6RT[V]#b+MYKPS#KPX _#UUT]#fffROId'_#MYKc XKUTKPS;_#Z'OQK&XKU_0[RT]ffZ.M3RM3Rc6KZ[R  KcRZ _
W.X]#W/]ffMR[RT]ffZ._#U=MYKP[Y[RoZ6f6B
j A:VUT]#fffROId"_#MYKcq:K^eK_#Z [a'_0[}[a6KZ6]#[RT]ffZ.M}]#b,XKUTKPS0_#Z.OQKq+Kb1]*OPN.M}]ffZ
_0XK2Z6]#[4K *[YX_;UT]#fffRoOP_#U9d"N6[d'N.RoUT[RZ.MRoc6K?[a6K}U]#fffRDO C([a6KPV_0XK&c6K  Z6KcN.MRZ6f[a6K}MY[_#Z'c._0XcU]#fffROP_#U
Z6]#[RT]ffZ'M\]#b(OPU_#MMROP_#Urbr]#X^&N.U_*`*^]c.KUm`UT]#fffRoOP_#Uic.Kc.N.OQ[RT]ffZB`KP[O0j\a.KMY[YXKMM,RM+Uo_#Rce]ffZZ6]#[RT]ffZ'M+]#b
XKUTKPS0_#Z.OQK}[a._0[OP_#ZW.X]	S#K}a6KUW.brN'U/b1]#X3R^W.X]S*RZ6feRZ6brKPXKZ.OQK_#Z'cB`"RZW'_0X[ROPN'U_0X`6[a6K&^]ffMY[d'_#MRO
br]#X^ ]#b=R[P`iOPU_#MMROP_#U9KZ[_#RUo^KZ[P.j EKUTKPS0_#Z.OQK2RMOP_0W'[N6XKcdVhXKUo_0[RT]ffZ.M4RoZ[a6K2^KP[_#U_#Z6fffN'_0f#K&]#b
[a6K&UT]#fffRO0`.[a._0[RMP`.q:Kbr]#X^e_#UoRTgPK?XKUTKPS0_#Z.OQK}_#M_eXKUo_0[RT]ffZd/KP[q:KPKZ]#Gd F%KOQ[M]#b8[a6KW'X]#W/]ffMRT[RT]ffZ._#U
U_#Z6fffN'_0f#K2rbr]#X^&N.U_#M+]#X:MYKP[M:]#bUoRT[YKPX_#UIM H	S0_0XR_0d"UTKM 9[aN.M\K *W.XKMMRZ6f}[a6K4b_#OQ[([a._0[:M]ff^K]#d F%KOQ[(RM
XKUTKPS0_#Z[[Y]MY]ff^K]#[a6KPX]ffZ6K#j
,q:] Z6]#[RT]ffZ.MW'U_V _ OQKZ[YX_#UX]ffUTK RZ [a.RoMeW'_0W/KPXj 4a6K  X MY[]ffZ6K#%
` JmPuPxe0z'v0y 0 KLM!0wN
x /TD O
;#w ymQ PQuy1zitRu -6uPz"tuPz  u SRZ.c.KPWiKZ'c6KZ.OQK&b1]#XMa.]#X[ 4[YKUUM[a._0[_W.X]#W/]ffMRT[R]ffZ._#Ubr]#X^&N.UT
_ &sRM
RZ.c.KPWiKZ'c6KZ[,brX]ff^ _fffRTS#KZ MYKPV
[ U ]#b=S;_0X R_0d'UTKMRTb=_#Z.c]ffZ.UTVhRTb=RT[OP_#Zd/K}XKPqXRT[Y[YKZWK "N.RTS0_#UTKZ[UV
_#M&_br]#X^&N.U_RZq4a.RO aZ6]ffZ.K]#b[a6KS;_0X R_0d'UTKM}RX
Z U _0W.W/K_0XMja6KMKOQ]ffZ.c]ffZ.KRM[a6KZ.]#[RT]ffZ
]#Yb M!0wm{uvmvy1z{  {#yr0uQz uPv UZ![;#w ymQ PQuLyrz T
 M!0wN
x /T#
 &j p[hRMhRZ[R^e_0[YKUTV UoRZ6#Kc [Y] [a6K
\^]*_

fif.w~3}33f$3ws

Z6]#[RT]ffZ]#b+br]#X^&N.U_;IS0_0XR_0d'UTKRZ'c6KPW/KZ.c6KZ.OQK&d/KOP_#N.MYK#`9_#Mq+KMa6]q`9[a6KXKMN'UT[3]#b:br]#Xf#KP[Y[RZ.f[a6K
MYKP[?]#b:S;_0X R_0d'UTKM U RZL_b1]#X ^}N.UoT
_ &sOP_#ZdiKc6K  Z6Kc _#M?[a6KeMY[YX]ffZ.f#KMY[OQ]ffZ.MYWK "N6KZ.OQK]#b & d/KRZ6f
RZ.c.KPWiKZ'c6KZ[b1X]ff^ Uej A:]#[a Z6]#[RT]ffZ'M&a'_S#Kh_0W.W/K_0XKc RZ[a.KUR[YKPX_0[N6XKN.Z.c6KPX}S;_0XR]ffN.M}Z'_#^KM
_#Z.cq4RT[aMYKPS#KPX_#U/c.R iKPXKZ[?rd'N6[:WK "N.RS;_#UTKZ[ \c6K  Z.RT[R]ffZ.MP`_#Z.c_0XK?a.Rfffa.UTVN.MYKPbN.U.br]#X,^e_#ZVe[_#MY*M
RZh_#N6[Y]ff^_0[YKcXK_#M]ffZ.RZ6f_#Z.c^_#ZVW.X]#d"UTK^eM(RZh_0X[R  OPR_#UBRZ[YKUURTf#KZ.OQK C
0j}/*v	!#xffvuthtutD/vy0!0z #z"t%I!0z6u	M/'uPz uff
=z"t0yrz{ffj\6]#X^&N.U_;IS0_0XR_0d'UKRZ.c.KPWiKZ'c6KZ.OQKOP_#Zd/K
N'MYKPbrN'U8b1]#X&Oa6KORZ.f N.Z"M_0[RoM  _0d'RUoRT[V]#Xb1]#X&OQ]ffZ.MYKW"N6KZ.OQK  Z.c.RZ.f6j&p%Z.c6KPKc `9MY[YXN'OQ[N6XRZ6f
[a.K?Z6]	q4UTKc6f#K?d"_#MYK?dV  Z.c.RoZ6f&N'MYKPbrN'U"RZ'c6KPW/KZ.c6KZ.OPRTKM:^e_Vd/K3q+]#X[ac6]ffRZ6f2d/KPbr]#XK3XN.Z*
Z'RZ6f_#ZVMYK_0XO a_#UTf#]#XR[a.^hj=a.RoM+W'XRZ.OPRTW"UTKRM(_0[q:]#XRZLfi^RTXY
fi OQpUTX _#RT[aB`0## j+p%Z
]#W'[R^e_#UOP_#MYKMP`9b1]#X?K6 _#^W'UK#`_hM_0[RM  _0d'RURT[VhW.X]#d'UK^ qRUU9diKc6KOQ]ff^eWi]ffMKcRZ[Y]_hM^e_#UU
ZN.^}d/KPX]#b4M_0[RoM  _0d'RUoRT[VW.X]#d'UK^eM?]ffZ K_#MRTKPX&Z.]q4UTKc.f#Kd'_#MYKMrq4RT[a UTKMMS;_0X R_0d'UTKM _#M
Ma6]q4Z dV 8 _0X _#Z.cX

 KUoc6KPX[ #ff jfiM}[Y] R^W.X]	SRoZ6fRZ6brKPXKZ.OQK#`[a.RM}OP_#Zd/KW"_0X[ROPN*
Uo_0XUTVa.KUTW.bN.U8RZ [a6KMR[N._0[RT]ffZ q4a6KPXK[a6KeMYKP[]#b " N6KPXRTKM?N.Z'c6KPXOQ]ffZ.MRc.KPX_0[RT]ffZRM?UoR^eRT[YKc
[Y] br]#X^&N.U_#M $ [a._0[_0XKLMYV*Z[_#OQ[ROP_#UoUTV]#XMYK^e_#Z[ROP_#UUTV.RZ.c6KPW/KZ.c6KZ[?brX]ff^ _MYKP[ U ]#b

S0_0XR_0d"UTKMPj
jW/'uQw 4h#z6 |+uPw yrz{MYKPK2RZW'_0X[ROPN'U_0Xfi^eRTXfiOQpUX_#RT[aB`0## `\t0ym{#z!0 y1&MYKPK2[a6K&q+]#X
d
V 3_0Xq4RO a6K#` # ff j EKZ'c6KPXRZ6f_b1]#X ^}N.Uo_ &RZ'c6KPW/KZ.c6KZ[brX]ff^ _MYKP[ U ]#b(S;_0XRo_0d'UTKM
[a.X]ffN6fffaS;_0XRo_0d'UTK4br]#Xf#KP[Y[RZ.f&fffRS#KM+X RMYK4[Y]_}b1]#X ^}N.Uo_[a._0[(RM "N6KPXVIWK "N.RTS0_#UTKZ[\[Y] & qjXj[Pj
U RZ [a6KMYKZ.MK[a._0[KPS#KPXV UT]#fffROP_#UOQ]ffZ.MWK "N6KZ'OQK $ ]#b & [a._0[RMeRZ.c.KPWiKZ'c6KZ[2brX]ff^ U
_#UoMY]hRM?_UT]#fffROP_#UOQ]ffZ.MYWK "N6KZ.OQK]#b & ]ffZ.OQKe^e_#c6KRZ'c6KPW/KZ.c6KZ[4brX]ff^ U` _#Z'c [a6KOQ]ffZS#KPXMK
a.]ffUc.Me_#Mq+KUoUmj p%Z[YKPXKMY[RZ6fffUV#`,[a6KMYKP[]#b?_#UU[a6Kb1]#X ^}N.Uo_#MeRZ.c6KPW/KZ.c.KZ[brX]ff^ _ MYKP[]#b
S0_0XR_0d"UTKMRM_ MY[_0d"UTK W.X]*c.N.OQ[RT]ffZ  KUc	 *RTKPf#KUm`  }p%Z6]ffN6K#` # ff `_#Z'c br]*OPN.MRZ6f ]ffZ
MN.Oah_2W.X]c.N'OQ[RT]ffZ  KUcRM:S;_#UN'_0d'UTKb1]#XMYKPS#KPX _#U/XK_#MY]ffZ.RZ.f2MOa6K^KMj\6]#X4RZ'MY[_#Z.OQK#`6RZ[a6K
OQ]ffZ'MRMY[YKZ.OQVId'_#MYKc}brX_#^KPq:]#Xbr]#Xc.Ro_0fffZ6]ffMRM\5 E4KRT[YKPX`  # `ff[a.1K "N6KPXRTKM9q+K:_0XK(RZ[YKPXKMY[YKc
RoZ}_0XK([a6K(OQ]ffZ 'RoOQ[M9]#b"[a6K(MYVM[YK^[Y]d/K(c.R_0fffZ6]ffMKcB`#RmjK#jT`#[a6K,OPU_#N.MYKM[a._0[8_0XK,RZ.c.KPWiKZ'c6KZ[
brX]ff^KPS#KPXV&S0_0XR_0d"UTKN.MYKc2[Y]XKPW.XKMYKZ[=[a6KMYV*MY[YK^h`K *OQKPW'[=[a6K4_0d'Z.]#X^e_#URT[VW.X]#Wi]ffMRT[RT]ffZ.M
N'MYKch[Y]eKZ.OQ]*c6K?[a6K}OQ]ff^W/]ffZ6KZ[4br_#RUoN6XKMPj
j ;z !0|,ut{+
u P;uQvIw /5v /wyrz{#`\	v ! -"5y WO P;PutwYu; !0z'y1z{ffje6]#X^&N.U_;IS0_0XR_0d'UK2RoZ.c6KPW/KZ.c6KZ.OQK&RM
 
_#KPV Z6]#[RT]ffZ br]#Xc.KOQ]ff^W/]ffMRZ6f_LW'X]#W/]ffMRT[RT]ffZ._#U(*Z6]	q4UTKc6f#Kd"_#MYK   A br]#XMa6]#X[  `,RmjK#jT`
_  Z'RT[YKMYKP[2]#bW'X]#W/]ffMRT[RT]ffZ._#U=b1]#X^&N.U_#M`=RZ[Y] M^e_#UoUTKPX}MN.d.d'_#MYKMPj *N.O a _c6KOQ]ff^eWi]ffMRT[RT]ffZ
RoM_#UU8[a6K^e]#XKS;_#UoN._0d'UTK_#M[a.KeZN.^}d/KPX?]#b(S;_0XRo_0d'UTKM3[a6KeMN.d.d'_#MYKMc.KPWiKZ'c.M]ffZRMUT]	qj
W.[R^e_#UUV#`	_Z6]	q4UTKc6f#K+d'_#MYK &"!$# $ (&%('''% $*),+(RMbrN'UUTV3c6KOQ]ff^W/]ffM_0d'UTK:RTb.RT[OP_#Z&d/K\q4XRT[Y[YKZ
_#N
M &-! & (/. '('(' . & ) q4a.KPX%
K &10(_#Z.c &/2c.KPWiKZ'c]ffZc.RM FY]ffRZ[3MKP[M]#b:S;_0XRo_0d'UTKM3br]#X_#UU
 fi^eRTX
35!
4 6.j *N'Oa c6KOQ]ff^W/]ffMRT[RT]ffZ'M?q+KPXKOQ]ffZ.MRc6KPXKc RZMYKPS#KPX _#U\W'_0W/KPXM5 8_0XRT*aB` # 7
fi8OQp%UTX_#RT[a 9
` 0##*ff _0,X "N.RM fi9]#IX "N6KP[P/` 0##3qRT[aLMY]ff^KPq4a'_0[c.:R /KPXKZ[^e]#[RTS;_0[R]ffZ.MPj
4a6K^]ffMY[RoZ[N.R[RTS#Kh^]#[RTS0_0[RT]ffZ br]#XMYK_0XO a.RZ6fMN.Oa c.KOQ]ff^W/]ffMRT[RT]ffZ.MRoM[a._0[RT[fffRS#KM_
d/KP[Y[YKPXN.Z.c.KPXMY[_#Z.c.RoZ6f]#b.[a6K:*Z6]q4UKc6f#K+d"_#MYK#`#dV}MY[YXN.OQ[N6X RZ6fR[q4RT[a}XKMYW/KOQ[[Y]erWi]ffMMRTd'UTK
c'RM FY]ffRZ[(d'N.[Z6]#[Z6KOQKMM_0X RUTV.:MYKP[M]#b[Y]#W'ROPM; _0,X "N.RYM fi9]#IX "N6KP[P,` 0## j
<6j P uQym0u LwYuPy1 0y !0z 0z"t yr
z ,!#z6 y1vuPz M4DO%	v !0TuPw%#z'vwYu; !#z.y1z{#=
j KOQ]ff^W/]ffMRZ.f _ W'X]#W/]ffMRT[RT]ffZ._#U
*Z6]	q4UTKc6f#Kd'_#MY'
K & RZ[Y] MN6d.d'_#MK5
M # & ( %('('('>% & ) +W.X]S#KM_#UMY][Y] diKXKUTKPS0_#Z[b1]#Xc6K  Z*
RoZ6f&RoZ.OQ]ffZ.MRM[YKZ.OQVI[Y]ffUKPX_#Z[(XKUo_0[RT]ffZ.M:_#M(q+KUU/_#M(d/KURTKPb/XKPS*RMR]ffZ]#W/KPX_0[Y]#XMj=a6K?_0W.W.X]ff_#O a
W'X]#W/]ffMYKcd@
V ?(a.]#W.X__#Z'X
c 8_0XRa [ ##ff}W.X]*OQKPKc.M2_#M&b1]ffUU]q4MM C[a6K*Z6]	q4UTKc6f#Kd'_#MYK &
\^]^\

fi

~33



s~$w

~ww$

RoM  XMY[2W'_0X[RT[R]ffZ6Kc RoZ[Y]@# & ( %('('('&% & ) +MN.O a [a._0[[a6KhRZ[YKPXMYKOQ[RT]ffZ ]#b[a6KhU_#Z6fffN._0f#KM]#b
[a.KeMN6d.d'_#MKM2RIjK#jT` [a6KMYKP[MU	6	& 0m4]#b:S;_0XRo_0d'UTKM[a6KeMN6d.d"_#MYKM3c6KPW/KZ.c]ffZ"_0XKe_#M
M^e_#UU_#M?W/]ffMMRd'UTK#/[a6KZ $ RM?RZ.b1KPXXKcbrX]ff^ &sqjXj[Pj2[a6KfffRTS#KZLW'_0X[RT[RT]ffZ RTb+_#Z'c ]ffZ.UTVRb
[a.K2OQ]ffDZ FYN.Z'OQ[RT]ffZ ]#b+_#UU &10+MN.Oa [a'_0[
ff U	.	&10fi
ffU.	$8 !
4 RoMOQ]ffZ'MRMY[YKZ[_#Z.c
KZ[_#RU
M $\j\p%Z_eXKPS*RMR]ffZhMRT[N._0[RT]ffZ `6[a.RM_0W.W'X]ff_#Oa_#UMY]eKZ.MN6XKM,[a._0[4[a6K]ffZ.UTV]ffUchd/KURKPbrM
[a'_0[^e_Vd/K[a6X]q4Z_q(_V _0XK2_0di]ffN.[4[a6K}S;_0XRo_0d'UTKMXKUTKPS0_#Z[[Y][a.K}RZ6W"N6[br]#X^}N'U_*j48R 
Z'_#UUTV#`ff_#M=XKOQKZ[UTV2Ma6]	q4Z2dV&MY]ff^eK]#b"[a.K_#N6[a6]#XM]#bi[a6K,W.XKMYKZ[W"_0WiKPX`ffb1]#Xf#KP[Y[RZ6fOP_#Zed/K
_#c.S;_#Z[_0f#KP]ffN.MUTVK W"UT]ffRT[YKc_#M_q:K_0#KZ.RZ6f^KOa'_#Z.RM^ b1]#XXKOQ]	S#KPXRZ6fhOQ]ffZ.MRoMY[YKZ.OQVhbrX]ff^
_#ZRZ.OQ]ffZ.MRMY[YKZ[  A5 @9_#Z6%
f fi_0,X "N.RMP` 0# ff j
j P uQym0u  /^-6tff#vuQff
j fi diKUoRTKPb.N6WBc._0[YK\]#W/KPX_0[Y]#X=^_0W'M9_*Z6]qUTKc6f#K:d'_#MYYK & _#Z.c}_#Z2RZ.W'N6[br]#X^&N.U_
$K *W.XKMMRoZ6fMY]ff^eK:K *W'URoOPRT[ KPS#]ffUN6[R]ffZ}]#b"[a6K:q+]#XUoc[Y]?_3Z6KPq Z.]q4UTKc.f#K+d'_#MK & $\G & _#Z.c
&
 $XKMYW/KOQ[RTS#KUV}XKPW'XKMYKZ[[a.K_0f#KZ[ M+Z.]q4UTKc.f#K P5u M!0wu_#Z.c vuPw[a.K,KPS#]ffUN6[RT]ffZ2]#bi[a6K
q:]#XUcK W.XKMMYKcdV3[a6K+N.W/c'_0[YK#j KPS#KPX_#U_#N6[a6]#XM9_0XfffN6Kc[a._0[9_#Z}N6WBc._0[YK]#W/KPX_0[Y]#XMa6]ffN.Uc
W'XKMYKPXS#K+[a6K+W"_0X[ ]#b.[a6K+Z.]q4UTKc.f#K\d'_#MYK:Z6]#[9OQ]ffZ'OQKPXZ6Kc}dV?[a6K:N6WBc._0[YK#j94a.RMUK_#c.M [Y][a6K
br]ffUUT]	q4RZ6f[a6XKPKQMY[_0f#KW'X]OQKMMP`/W.X]#W/]ffMYKcRoZ.c6KPW/KZ.c6KZ[UTVdV ]ffa.KPX[V#` @9N6;_#MgPKPq4ROQg#`B_#Z.c
_#c._#URoZ.MY0_; A:N.fff*_ F[ # ff\_#Z.c2dVKPXgRTf_#Z.c ER  [ # ff_#Z'ceZ._#^Kch_#Z.c 
!"!$#.`
XKMYW/KOQ[RTS#KUTV.I C,Rrc6KP[YKPX^eRZ.K[a6KS0_0XR_0d'UKM8XKUTKPS0_#Z[+[Y][a6K4N.W/c'_0[YK#`Z._#^KUTV#`%
ff U.	 $8 
RoR14b1]#Xf#KP[[a6KMYKeS0_0XR_0d'UKMRoZ &s[Y]h]#d.[_#RZ_hZ.KPq b1]#X ^}N.Uo_
&'")(%+* U,	.	 & % 
ff U.	 $8Y 
RoRR12K *W'_#Z.c dV $\j p%Z ^e]#XKLOQ]ff^W'_#OQ[[YKPX^eMP`[a.RMN6WBc._0[YK]#W/KPX_0[Y]#XhRMK W.XKMMYKc dV
&- $!.&'")(%+* U,	.	 & % 
ff U.	 $8Y0+
/ $\j\aN.MP`*d/]#[ah]ffN6X,XKMN'UT[M(]ffZh S4RZ'c6KPW/KZ.c6KZ.OQK
_#Z'c}S0_0XR_0d"UTK+br]#Xf#KP[Y[RZ.f3_0XK:XKUTKPS;_#Z[8[Y]?[a6K,OQ]ff^W'N6[_0[R]ffZ._#U*RMMN6KM9W/KPX[_#RZ.RoZ6f4[Y]3[a.RM*RZ.c
]#b8d/KURTKPb9N6WBc._0[YKMPj
jwu0 !0z.yrz{ Q P3!/*v} v0y !0z21}t,u Py1 0y !0z xe ;y1z{31 -" #z.z.yrz{#j @ ]#fffRoOP_#U:Uo_#Z6fffN._0f#KM&b1]#X2XK_#M]ffZ.RZ6f
_0d/]ffN6[_#OQ[RT]ffZeK W.XKMM8[a6K,K /KOQ[Mc6KP[YKPX^RZ.RMY[RoO\]#X=Z6]#[P`OQ]ffZ.c.RT[RT]ffZ'_#U]#X+Z6]#[ 9]#b"_#OQ[RT]ffZ'MdV
^eK_#Z.M\]#bW.X]#W/]ffMRT[RT]ffZ'_#U6b1]#X ^}N.Uo_#M	 
KUTbr]ffZ.+
c fi @9RTbMOa.R[Yg#` #   *_#Z.c.KPq:_#UUI` # .._0XfffRKPX`
@9_#Z6f6` fi _0IX "N.RoMP` 0##*4KPXgRTf6` @9_#Z6f6*` _0IX "N.RoMP` fi ]ffU_#OPMYKPi9` 0#    Z.c.RoZ6fe[a6K2S0_0XR 
_0d"UTKM([a6K?K /KOQ[M4_0XKc6KPW/KZ.c.KZ[(]ffZKZ._0d'UKM([Y]Rc6KZ[RTbrVe[a6K?S0_0XR_0d'UTKM:q4a.]ffMYK?[YXN6[aS0_#UN6K
^_Vd/KOa._#Z6f#KcdVe[a6K3_#OQ[RT]ffZB6^]#XKP]S#KPX`.br]#X^&N.U_;UR[YKPX_#U'RZ'c6KPW/KZ.c6KZ.OQK65_}XK  Z6K^KZ[
]#b SRZ.c6KPW/KZ.c.KZ.OQK2[a._0[q:KRZ[YX]*c.N.OQK5L_#UMY][YKUUoMN.MRZq4a.RO ac.RTXKOQ[R]ffZ rb1X]ff^ b_#UMYK
[Y]h[YX N6K&_#Z'c H	]#X?b1X]ff^ [YX N6K}[Y]hb_#UMYK	4[a6K&W/]ffMMRTd"UTK}O a._#Z6f#K^e_V]*OPOPN6Xja.RoMRMKMYW/KOPR_#UoUTV
N'MYKPbrN'U/b1]#X  UT[YKPXRZ6f2]ffN6[RXXKUTKPS0_#Z[_#OQ[R]ffZ.MRZh_ec.KOPRMRT]ffZh^e_0*RZ6f]#X4W'U_#Z.Z.RoZ6f}W'X]#d'UTK^hj
j -iwY0u PuPwYuPz  u?wYu -"wYuQPuPz'vI#v0y !0z.1
j @]#fffROP_#U'Uo_#Z6fffN._0f#KM=br]#X\XKPW.XKMYKZ[RZ6f?W.XKPb1KPXKZ'OQKqXRT[YK,KUK^KZ*
[_0XVf#]ff_#UM_#M3W'X]#W/]ffMRT[RT]ffZ._#Ub1]#X ^}N.Uo_#M_#Z'cR[RM3OQXN'OPR_#U[Y]Roc6KZ[Rb1V[a.]ffMYK2S;_0XRo_0d'UTKM[a._0[
a'_S#KZ.]3RoZ 'N6KZ.OQK(]ffZ[a.K_0f#KZ[ M+W.XKPbrKPXKZ.OQK#ja6KPXKPb1]#XK#`b1]#X^&N.U_;IS0_0XR_0d"UTK:RoZ.c6KPW/KZ.c6KZ.OQK
a'_#M,_#ZRo^W/]#X[_#Z[:X]ffUTK3[Y]2W'U_V"*br]#XRZ.MY[_#Z'OQK#`[a.KbrX_#^KPq:]#X]#b9MY]0OP_#UUKc uPvuPw y1 -*0w0y PM/
W'XKPb1KPXKZ.OQK,MY[_0[YK^KZ[M=]#b"_#Z_#Z.N
c K_0XU [ # <8RZ[YKPXW.XKP[M_3W'XKPb1KPXKZ.OQK:R[YK^ $ C3798;:<7
dV Cbr]#X_#ZVW'_#RTX3]#b:q+]#X Uc.M2>= % =@?MN.Oa [a._0[eR1=BA !C7`\RR1D=@?EA !:7 _#Z.c RRR1F= _#Z.c
=@?OQ]ffRZ.OPRc6K]ffZ _#UU4S0_0XR_0d"UTK+
M $ 0z"tG7 0wu yrz"tu -6uQz"tuQz'v w !0x2`[a6KZ q:K a'_S#KL_ M[YXROQ[
W'XKPb1KPXKZ.OQK]#b= ]S#KPX=@?1j
.k lIHKJ u{rffmuw{gz3{MLON r|wz3{#>Pz$uw{gu=>J*?q  z=<qsr
a6K4XKMY[\]#bB[a.KW'_0W/KPX+RM\MY[YXN.OQ[N6XKc_#M+b1]ffUU]q4MPj fib1[YKPX(MY]ff^Kb1]#X^_#U.W.XKUoR^eRZ._0X RTKMfffRTS#KZRZLKO
[RT]ffZ `[a6K(#KPV2Z.]#[RT]ffZ2]#b"br]#X^&N.U_;IS0_0XR_0d'UK:RZ'c6KPW/KZ.c6KZ.OQK:RM8W.XKMYKZ[YKcRZ KOQ[R]ffZ  j A:KOP_#N.MK4RT[
\^]+Q

fif.w~3}33f$3ws

OP_0W.[N6XKM=_3S#KPXV2d'_#MRO:br]#X^]#b/UT]#fffROId'_#MKceRZ.c6KPW/KZ.c.KZ.OQK#`0[a.RM8XKU_0[R]ffZa._#M=_#UTXK_#c.V}d/KPKZRZ[YX]0
c.N.OQKcRoZ2[a6KURT[YKPX_0[N.XK,N.Z.c6KPX=MYKPS#KPX _#U.Z._#^KMP`URT#K,RZ "N6KZ.OQK_0d'RUoRT[V5 A:]ffN6[RUoRTKPX` # < `*XKUTKPS0_#Z.OQK
[Y]_MN6Gd FYKOQ[4^e_0[Y[YKPX5 @_0#K^eKPV#KPX` # # `i]#XXKc.N.Z'c._#Z.OQVL ]ffa.KPX[VKP[_#UmjT` # ff j fiUT[a6]ffN.fffaRT[
RMOQ]ffZ.OQKPW'[N._#UUTV&_#Z.c2[YKOa.Z'ROP_#UUTV&MR^W'UK#`;[a'RMZ6]#[RT]ffZa._#MZ.]#[diKPKZM[N.c.RTKc2RZ_?MYV*MY[YK^e_0[RoO(q:_V#`
_#Z.c}]ffN6X8S#KPXV  X MY[OQ]ffZ[YXRTd"N6[RT]ffZ_#Ro^eM9_0[  UoURZ6f,[a.RoMfff_0W `  XMY[dVfffRSRZ.f4MYKPS#KPX_#U*WK "N.RTS0_#UTKZ[9Oa'_0XY
_#OQ[YKPXRTg_0[R]ffZ.M]#b[a.RMZ6]#[R]ffZ r[a.RoM4RMN'MYKPbrN'Um`'_#MMYKPS#KPX_#U9W'_0W/KPXM4RZ[YX]*c.N.OQKc_#Z'cN.MYKc[a6K&M_#^K
OQ]ffZ.OQKPW.[MN'Z.c6KPX}c':R iKPXKZ[2Z._#^KM  `+_#Z.c MKOQ]ffZ.c dVRoZS#KMY[Rfff_0[RZ6f OP_0XKPbrN'UUTVRT[M&OQ]ff^W'N6[_0[R]ffZ._#U
OQ]ff^W'UK *RT[V#j=p%ZW'_0X[ROPN.Uo_0X`q:KMa6]	q [a._0[P`.RZ[a.K?f#KZ6KPX_#UBOP_#MYK#`'O a6KO*RZ6fq4a6KP[a6KPX_2br]#X^&N.U_2RM
RZ.c.KPWiKZ'c6KZ[brX]ff^ _MYKP[:]#b S0_0XR_0d'UKM\RM 'OQ]ff^W'UKP[YK#j+a6KZ `q:Kf#]&d/KPV#]ffZ.ce[a.RoM=S#KPXVMR^W'UTK
Z6]#[RT]ffZdVeRZ[YX]c.N'OPRZ6f?[a6K^]#XK  Z6KQIf#X_#RZ6KcZ6]#[RT]ffZ]#b !#w N
x /* 	 '
yvuQwY0 yrz"tu -6uQzituQz u(RZ]#X c6KPX
[Y]c.RMOQXR^eRZ'_0[YK[a6KMRT[N._0[R]ffZqa6KPXK_b1]#X^&N.UT
_ & OQ]ffZS#KPV*MM]ff^KRZ6br]#X^e_0[RT]ffZ_0d/]ffN6[?_UoRT[YKPX_#U
d'N6[2Z6] RZ6br]#X^e_0[RT]ffZ _0d/]ffN6[&R[M&Z.KPfff_0[RT]ffZBjLa.RM}XK  Z.K^KZ[2RM&a6KUTW.bN.U\q4a6KZ6KPS#KPX2[a6KW/]ffU_0XRT[V
]#b3RZ6br]#X^e_0[RT]ffZ RMMRTfffZ'R  OP_#Z[P`q4a.RO a RM&[a6KOP_#MYKRZ ^e_#ZV fip  KUc.MhRZ.OPUN'c.RZ6fOPU]ffMYKc*Iq:]#XUc
XK_#MY]ffZ'RZ6f_#Z'cXK_#M]ffZ.RZ6f_0d/]ffN6[_#OQ[RT]ffZ'M  j3JLK2_#UoMY]MY[N'c6VMYKPS#KPX_#URZ[YKPXKMY[RoZ6fZ6]#[R]ffZ.Mc.KPXRTS#Kc
brX]ff^b1]#X ^}N.Uo_;IS;_0XRo_0d'UTK(_#Z.cbr]#X^}N'U_;URT[YKPX_#URZ.c.KPWiKZ'c6KZ.OQK#`#MN.O ae_#M[a6KZ6]#[R]ffZ2]#by1.
x -"y 
(ut M!0Iw O
x /* 2	 & RM @9RT[%: S=_0XY+MR^eW'UR  Kc2RTbBRT[:c6KPW/KZ.c.M]ffZKPS#KPXVeUR[YKPX_#U9rS0_0XR_0d'UK	]OPOPN.XXRZ6f}RZRT[ =_#Z.c
N
[a6KOQ]#XXKMYW/]ffZ.c.RoZ6fW.X]*OQKMM]#bMRo^W'URTbrV*RZ6f_b1]#X ^}N.Uo_*j KMYW'R[YK[a.RMOQ]ff^W'UTK 6RT[VL_#Z.cd/KOP_#N.MYK
[a6KMRTgPKe]#b_MR^W'UoR  Kcb1]#X ^}N.Uo_OP_#ZZ6KPS#KPX&d/KU_0Xf#KPX&[a._#Z[a6KMRgPKe]#b,[a6K]#X RTfffRZ._#U8br]#X^&N.U_*`
MR^eW'UR  OP_0[R]ffZ}OP_#ZW.X]S#K_3S;_#UN'_0d'UTK(XKUTKPS0_#Z.OQKQId'_#MYKcW.XKPW.X]OQKMMRZ6f3b1]#X+R^W.X]	SRoZ6f^_#ZV2br]#X^eM
]#bRZ6brKPXKZ.OQK#j
p%%
Z KOQ[RT]ffZ <6`q:K[N6XZ[Y][a6K4MYKOQ]ffZ.ce#KPV2Z.]#[RT]ffZB`Z'_#^KUTYV M!0wm{uvmvy1z{?{#yr0uQzPuv ![;#w ymQ PQu
yrzY !0wN
x /* 5 @RT
Z fi EKRT[YKPX` # < j+a6K3br]#Xf#KP[Y[RZ6fW'X]OQKMM+W"U_V*M,_#ZhR^W/]#X[_#Z[+X]ffUTK?RZ^e_#ZV
fip?[_#MYM&_#Z.c a._#Md/KPKZM[N.c.RTKcRoZ[a6KURT[YKPX_0[N6XKN.Z.c6KPXS;_0XR]ffN.M]#[a6KPX2Z._#^KM`MN'Oa_#M}S0_0XR 
_0d'UTK&KUR^RZ._0[RT]ffZB`i]#X^e_0XfffRoZ._#URTg_0[RT]ffZ  ?]ffa.Uo_#MP*` ]#X _#Um` fi _0KZ.Z.RI` ##ff j KPS#KPX _#UMYK^e_#Z[ROP_#U
Oa'_0X_#OQ[YKPXRTg_0[RT]ffZ'M}_#Z.c^KP[_0[a6KP]#XKP[ROW.X]#W/KPX[RTKM3]#b(br]#Xf#KP[Y[RZ6f_0XKW.XKMYKZ[YKc j A(_#MYKc]ffZ[a.RM
Z6]#[RT]ffZ `.q+K}RZ[YX]c.N'OQK_#Z_#c.c.R[RT]ffZ._#UBZ6]#[RT]ffZ]#b8c.KPWiKZ'c6KZ.OQK?d/KP[q:KPKZ[q+]b1]#X ^}N.Uo_#M,fffRTS#KZ_MYKP[
]#b/S;_0X R_0d'UTKMM C9q+KMY[_0[YK4[a'_0[ & RoMWK "N'RTS;_#UKZ[[Yff
] 
 qjXj[Pj U RTb/_#Z.ce]ffZ'UTV&Rb"d/]#[abr]#X^&N.U_#M=_0XKUT]#f0
ROP_#UUV}WK "N.RTS0_#UTKZ[=]ffZ.OQK^_#c6K4RZ.c.KPWiKZ'c6KZ[8brX]ff^ KPS#KPXVS;_0XRo_0d'UTK,K 6OQKPW.[\[a6]ffMK4]#b Uj<KPXK_0fff_#RZB`
RT[,RM(R^W/]#X[_#Z[([Y]e^e_0#K_2c.RMY[RoZ.OQ[RT]ffZdiKP[q+KPKZ_2S;_0XRo_0d'UTK3_#Z.chRT[M(Z6KPfff_0[RT]ffZrbr]#XRZ.M[_#Z.OQK#`6]ffZ6K
OP_#ZdiKRZ[YKPXKMY[YKcRoZ[a6K4W/]ffMR[RTS#KOQ]ffZ 'ROQ[M+]#bB_MYV*MY[YK^h`]ffZ.UTV. j6]#X:[a.RM=W"N6XW/]ffMYK#`ffq:KRZ[YX]*c.N.OQK
_?Z6]#[RT]ffZ2]#b/URT[YKPX _#Ubr]#Xf#KP[Y[RZ.f[a._0[XK  Z.KM8[a6KOQ]#XXKMYW/]ffZ.c'RZ6f3Z6]#[RT]ffZ2]#biS;_0XRo_0d'UTK:b1]#Xf#KP[Y[RoZ6f6jJLK
Ma6]	q a6]	q OPUT]ffMKc*Iq+]#X UceRZ6brKPXKZ.OQKOP_#Zd/KMR^W"UTV}O a._0X_#OQ[YKPXRTgPKcbrX]ff^[a6KOQ]#XXKMYW/]ffZ.c.RoZ6f3WK "N.RS
_#UTKZ.OQKXKU_0[RT]ffZ j88RZ'_#UUTV#`ffq:KRc.KZ[RTbrV&[a6KOQ]ff^W"UTK *R[V]#bBd/]#[aZ6]#[R]ffZ.M=]#bBWK "N.RTS0_#UTKZ.OQK4_#Z'cMa6]	q
[a6K^ [Y]d/Ka._0Xc  fi - OQ]ff^W"UTKP[YK	 j fiM_OQ]ffZ.MYWK "N6KZ.OQK#`"b1]#Xf#KP[Y[RZ6fS0_0XR_0d"UTKM(]#XUR[YKPX_#UM,q4R[a.RZh_
br]#X^}N'U_OP_#Z.Z.]#[d/K}_#O a.RTKPS#Kc RZWi]ffUVZ6]ff^R_#UB[R^K&RZ[a.K}f#KZ6KPX _#U9OP_#MYKN.Z'UTKMM[a.K}W/]ffUTV*Z6]ff^eR_#U
a.RTKPX _0XOaVOQ]ffUoU_0W'MYKM_0[[a6K  X MY[UTKPS#KU1 j+JLK}_#UMY]Ma6]q [a._0[_W/]ffUTV*MRTgPK3W.X]#Wi]ffMRT[RT]ffZ._#UiXKPW.XKMYKZ*
[_0[RT]ffZ ]#b,b1]#Xf#KP[Y[RZ6fRoMS#KPXVN.Z'URT#KUTV[Y]K 6RMY[}RoZL[a6Kf#KZ.KPX_#U+OP_#MK#jhJLKZ6KPS#KPX[a6KUTKMM}W.XKMYKZ[
MY]ff^KXKMY[YXROQ[YKcMRT[N._0[RT]ffZ'M(q4a6KPXK?br]#Xf#KP[Y[RZ6fRM([YX_#OQ[_0d'UTK#j
KOQ[R]ffX
Z Ma6]	q4M} SRZ.c6KPW/KZ.c.KZ.OQKOPUT]ffMYKUVLXKU_0[YKc [Y]Z6]#[R]ffZ.M]#b4RXXKUTKPS0_#Z.OQK_#UTXK_#c6VRZ*
[YX]*c.N.OQKcLRZ [a.KeURT[YKPX_0[N6XK&dVS;_0XR]ffN.M?_#N6[a6]#XM%
j KOQ[RT]ff
Z hc.RoMOPN.MMYKM]#[a6KPXXKU_0[YKcLq:]#XL_#Z.c
MY#KP[O a6KM3brN.X[a6KPX4K *[YKZ.MR]ffZ.M]#b\M]ff^K}Z6]#[R]ffZ.M_#Z'cXKMN'UT[M4MY[N'c.RTKcRoZ[a6K}W'_0W/KPXj8RZ._#UUTV#` KO
[RT]ffZ OQ]ffZ.OPUN.c.KM[a.K&W"_0WiKPXj =X]]#brM3]#b+[a6K^e_#RZW.X]#W/]ffMRT[R]ffZ.M_0XK&XKPWi]#X[YKcRoZ_#ZL_0W.W/KZ.c'R ij
fi fffUT]ffMM_0XV]#b[a6KZ6]#[_0[R]ffZ.MRM_0[4[a6K?KZ.ch]#b8[a.RM(W"_0WiKPX`X RTfffa[,d/KPbr]#XK?[a6Kd"RTd'URT]#f#X _0W'aV#j

\^]

fi

~33



s~$w

~ww$

8A H H D OH	3
JLK  XMY([ XKOP_#UU M]ff^Kd'_#MRoOZ.]#[RT]ffZ.M,brX]ff^ W.X]#Wi]ffMRT[RT]ffZ._#U/UT]#fffRO0`._#Z'chb1X]ff^ OQ]ff^W'UTK6RT[V[a6KP]#XV#j
7/lmk



=<

ru \u3uw{#z3Cy?u3|

;

@ KPff
[ 
fi d/K_  Z.RT[YKeMYKP[}]#b:W'X]#W/]ffMRT[RT]ffZ._#US0_0XR_0d'UKMPj

RM?[a6KeW'X]#W/]ffMRT[RT]ffZ._#UUo_#Z6fffN._0f#K
d'N.RoUT[N6Wb1X]ff^
fi:`i[a6K2OQ]ffZ.Z6KOQ[RTS#KM3_#Z.c[a6K A+]]ffUTK_#Z OQ]ffZ.MY[_#Z[M*  &_#Z.c "&RZ[a6K}N'MN._#U
q(_V#j\6]#X,KPS#KPX+
V U!"
fi:#` 

%$ c6KZ6]#[YKM:[a6K3MN6d"U_#Z6fffN._0f#K]#b&

f#KZ6KPX_0[YKcbrX]ff^ [a6K
S0_0XR_0d'UTKM3]#b U ]ffZ.UTV#j fi yvuQwY0/]#'b 

%$ RM3KRT[a6KPX_hS0_0XR_0d"UTK2]#b U rW/]ffMRT[RTS#KURT[YKPX _#U14]#X[a6K
Z6KPfff_0[RT]ffZ]#b_}S0_0XR_0d'UTK4]#b U Z6KPfff_0[RTS#K?UR[YKPX_#U1 ja6KRTX:MYKP[(RM+c.KZ6]#[YK)
c (*$(`q4a.RU+K (*,$ rXKMYW -j (/$. 
c6KZ6]#[YKM&[a6KMYKP[&]#b,Wi]ffMRT[RTS#KrXKMW jZ6KPfff_0[RTS#K	URT[YKPX _#UMd'N'RUT[N6W b1X]ff^ Uj fi OPU_#N.MY)
K 0rXKMYWj9_
[YKPX2
^ 1 =]#%b 
3
 $ RM(_hrWi]ffMMRTd'UTVK^eW.[V.  Z.RT[YK3c.RM FN.Z.OQ[R]ffZrXKMYWj*OQ]ffDZ FYN.Z'OQ[RT]ffZ"=]#b9URT[YKPX _#UM\]#b


%$(ffj fi ?54 rXKMYW j*_ 64=\b1]#X ^}N.Uo_]#&b 

%$ RM:_  Z.RT[YKOQ]ffZ FN.Z.OQ[RT]ffZ]#bOPU_#N.MKM3rXKMYW j
c.R5M FYN.Z'OQ[RT]ffZ]#b:[YKPX^eM]#'b 

%$,j fiMN.MN._#Um`/KPS#KPXV  Z.RT[YK2MYKP[]#b:br]#X^}N'U_#M3b1X]ff7
^ 
3
% RM
Rc6KZ[R  Kcq4R[ah[a6K?br]#X^}N'U_2[a._0[4RM([a.KOQ]ffZ FN.Z.OQ[RT]ffZ]#bRT[M(KUTK^eKZ[MPj
6X]ff^ Z6]	q ]ffZBG` & c6KZ6]#[YKM\_W.X]#W/]ffMRT[RT]ffZ'_#Ubr]#X^&N.U_*`RmjK#jT`_^K^}d/KPX=]#8b 
3
%ij U,	.	 &RM
[a6KMYKP[\]#bBW.X]#Wi]ffMRT[RT]ffZ._#U*S0_0XR_0d'UKM=_0W.W/K_0XRZ6f}R%
Z &j8pb (9"(*iG` U,.: ((8RM=[a.KMYKP[+]#bBS;_0XRo_0d'UTKM
brX]ff;
^ 
fi N6W/]ffZ q4a.RoOa URT[YKPX_#UM]#
b ( _0XK d'N'RUT[Pj +UTK^KZ[M]#
b 
fi _0XKLc6KZ6]#[YK=
c <'?
` >?
` @ KP[O0j
\UK^KZ[M:]#&b (  _0XK3c.KZ6]#[YK)
c #`  ( #`  - KP[O0j 6N6d'MYKP[M+]#&b 
fi _0XK?c6KZ6]#[YKc UeB` A#` CKP[O0j=p%Z]#X c6KPX
[Y]MRo^W'URTbrVZ6]#[_0[R]ffZ.MP`q:K4_#MMR^eRU_0[YK,KPS#KPXV2MRZ6fffUTKP[Y]ff%
Z U !$#D< +q4RT[aRT[M=N.Z.8R "N6K(KUTK^eKZ-[ <"ja6K
 Fy Eu\]#b _}b1]#X ^}N.Uo_ &`*c6KZ6]#[YKcdVA &A
`*RM\[a6KZN.^}diKPX+]#bB]OPOPN.XXKZ.OQKM+]#b S0_0XR_0d'UTKM\RT[+OQ]ffZ[_#RZ.MPffj fi
W.X]#W/]ffMR[RT]ffZ._#UBbr]#X^&N.UT
_ & RMM_#Rc[Y]d/K}R
Z 44KPfff_0[R]ffG
Z 44]#X^_#U.]#X^ : 4?4=4Rb=_#Z.c]ffZ.UTVRTb]ffZ.UV
W.X]#W/]ffMR[RT]ffZ._#U8MYV*^d/]ffUM_0XKRZL[a6KMOQ]#W/Ke]#b_#ZL]*OPOPN6XXKZ.OQK]#bD: RZ &jp[}RM?q:KUU I*Z6]q4Z[a._0[
KPS#KPXVW.X]#Wi]ffMRT[RT]ffZ._#U*br]#X^&N.U_ & d'N.RUT[N6Wb1X]ff^ [a6K4OQ]ffZ'Z6KOQ[RTS#KMF/:B` H(` ::#` I ]ffZ.UTV#`OP_#Zd/K[N.XZ6Kc
RZURZ6K_0X:[R^K?RZ[Y]2_#ZWK "N.RS;_#UTKZ5[ 4?4br]#X^&N.U_}dK
V J%W'N.Ma.RoZ6f}c6]	q4MZ L}KPS#KPXV]OPOPN6XXKZ.OQK]#b<: RZ
RT[RmjK#jT`6K *W'UT]ffRT[RoZ6
f K ]#Xfff_#Z  MUo_q:_#Z.cXK^]S*RZ6fc6]ffN6d"UTKZ.KPfff_0[RT]ffZ.MMRoZ.OQK: RM(RZS#]ffUoN6[RTS#K	 j
*URfffa[UTV _0d'N.MRZ6fhq:]#Xc.MP`q+KOP_#UU=[a.Kbr]#X^}N'U_hXKMN'UT[RZ6fhbrX]ff^ [a'RMZ6]#X ^e_#URTg_0[RT]ffZW.X]*OQKMMvr~6u
NON?P]#b & _#Z.chq:KZ6]#[YK ( 3 *P	 &+[a6KMYKP[4]#bUR[YKPX_#UM(brX]ffQ
^ (R]*OPOPN6XX RZ6fRZ[a6
K 4O4 ]#b &j=.]#X
RZ.M[_#Z.OQK#`0[a6'K 4O4]#b & !O:,Y :< T/ SPU H5V RoM: W HFX: SP/FY: V;a.KZ.OQK#`0q+K,a._S#5K ( 3 *P	 &/ !$#  % X: S % Y: V +ffj
4]#[YK4[a._0[=[a.+K 4?4 ]#b/_?b1]#X ^}N.Uo_?c6KPW/KZ.c.M8]ffZeRT[M\MYVZ[_#OQ[ROP_#U"MY[YXN.OQ[N6XK#`RmjK#jT`ff[q+]}b1]#X ^}N.Uo_0K,[a._0[
_0XKMVZ.OQ[_0[RoOP_#UUTVc.:R /KPXKZ[,^e_Vha._S#K&c.:R /KPXKZ/[ 4O4M4KPS#KZRTb9[a6KPVh_0XK?WK "N.RTS0_#UTKZ[Pj
.N'UU\RZ'MY[_#Z[Ro_0[RT]ffZ.M]#b4S;_0XRo_0d'UTKM]#.
b UZ2
fi _0XKOP_#UUTKX
c U| !0wTt; ( \[a6KPV_0XKc6KZ6]#[YKc dV
=%$ _#Z.c[a6KRTX?MYKP[?RMc6KZ6]#[YKG
c [/$,jJ a6K]
Z \ _#Z'_
c ^ _0XK2[q:]c'RM FY]ffRZ[MN6d'MKP[M]#*b 
fi:` = ` / %= a
c6KZ6]#[YKM:[a6O
K \ . ^}Iq:]#XUc[a._0[:OQ]ffRZ.OPRc.KM\q4R[a = `]ffb
Z \ _#Z.cqRT[a %= a]ffb
Z ^/j fiZRoZ[YKPXW'XKP[_0[RT]ffZ
= ]S#KP+X 
3
%RoM FN.MY[(c
_ 
fi9Iq:]#XUcB`*_#Z.c= RM(M_#Rc[Y]2d/K3_2^]*c6KU"]#b & q4a.KZ6KPS#KPX,RT[,^e_0#KYM &
[YXN6K#j\JLKc6KZ6]#[Y
K d f' e"	 &([a6KMYKP[]#b8^]*c6KUM(]#b &j
6]#XKPS#KPXVbr]#X^&N.U_ &s_#Z.cKPS#KPXVS;_0X R_0d'UTff
K >` &5gih / rXKMW j &'gih ( RM3[a6K2b1]#X ^}N.Uo_]#d'[_#RZ6Kc
dVLXKPW'U_#OPRZ.fhKPS#KPXV]*OPOPN6XXKZ.OQK]#Tb > RoX
Z &dVL[a6KOQ]ffZ.MY[_#Zc
[   "rXKMYW j * f  ; j &5j h ( rXKMYW j
&5j h / \RoM,_#Zh_0d.d.XKPS*R_0[RT]ffZbr]#X &'gih ( rXKMYW 1
j &'gih / +qa6K)
Z 9RM,_2W/]ffMR[RTS#KUoRT[YKPX_#8U >_#Z.cbr]#X &'gih /
rXKMYW1j &'gih ( +q4a6Kk
Z 8RM,_Z6KPfff_0[RTS#K}UoRT[YKPX_#UM%: >9j

3RS#KZ_#ZRZ[YKPXW.XKP[_0[RT]ffZG= _#Z.c_hUoRT[YKPX_#X
U `q:KUTKP[&')l V>= % I?c.KZ6]#[YKe[a6KRoZ[YKPXW'XKP[_0[RT]ffZ
[a._0[fffRTS#KMe[a6KM_#^K[YX N6[a S0_#UN6K_#M= [Y] _#UUS;_0XRo_0d'UTKM2K 6OQKPW.[[a.KS;_0X R_0d'UTKh]#3
b `,_#Z.c MN.O a
[a._0[&'"l V>= % I A !!j2p%ZL]#[a6KPXq:]#Xc.M`M&')l V >= % I3RM3[a6KeRZ[YKPXW.XKP[_0[RT]ffZLM_0[RMYbrV*RZ6m
f \[a'_0[RM

nporqtsuXvXwFxty{z}|~{z}u-|yT{||u~Ofz}{|TyD~iu|&yuzvTo
\^]i

fif.w~3}33f$3ws

[a6KOPU]ffMYKMY[?[Y] =,j.]#X}RZ.M[_#Z.OQK#` W.X]	SRoc6Kc [a._0[
fi ! #  % S>+_#Z.cG=4 6 ! =4SP! 0`q:Ka._S#K
&')l V>= % X: SPQ 6!:_#Z'c &')lV >= % :XSPQSP! *j ?,UTK_0XUTV#` RTb6= A !!:[a6KZG&'"lV>= % I!=,j2pb
(@! #i ( %('('('>%  ) +}RoM,_eOQ]ffZ.MRM[YKZ[4MKP[]#bUoRT[YKPX_#UMP`*[a6KZ-&')fV>= % (:,RoM,c6K  Z6Kch_#M
&')l V '('('  &')f V>= %  (  %('('('  %  )  '
@_#M[UTV#`*fffRTS#KZh_#ZhRZ[YKPXW.XKP[_0[RT]ffZ
= _#Z.ch_&S;_0X R_0d'UT?
K >`*q+KUKPT[ fi 3 * V9>= % >/(c.KZ6]#[YK?[a6K?RZ[YKPXW.XKQ
[_0[RT]ffZL[a'_0[fffRTS#KM[a.KeM_#^Ke[YXN.[a S;_#UN.Ke_#M = [Y]_#UU8S0_0XR_0d'UKM3K *OQKPW.
[ >`_#Z.c[a._0[fffRTS#KM?[Y] >
[a6KS0_#UN6K3]#W.W/]ffMR[YK?[Y]e[a._0[fffRTS#KZdV=,j
,q:]br]#X^&N.U_#
M  _#Z.c 
 _0XKM_#Roc [Y]d/K u M/*y100uQz'v}x !Dt /* !_b1]#X^&N.UX
_ & RTb3_#Z.c ]ffZ'UTV Rb
& 
/  &G/ 
j
p%Z[a.RM(W'_0W/KPX,q:KN.MYK?[a.KOQ]ffZ.OQKPW.[M]#bW.XRo^K?R^W'URoOP_0[YKM:_#Z'cW.XR^K?R^eW'UROP_#Z[MPja6K?MYKP[]#b
W.XRo^K?R^W'URoOP_0[YKM(]#b_br]#X^&N.U_ &`.c.KZ6]#[YKchd

V 	 
	 & `'RM4c6K  Z6Kch_#MM C
	 

 	&ff!$#02OPU_#N'MYKKA&OA ! &0 _#Z.c

4ff

0 ? OPU_#N'MYKMPj[Pj O
& A ! 0 ? _#Z.ck0 ? A ! 0&_#Z.c 0 A4! 0 ? +
'

fi^]ffZ6fL_#UU:[a6KR^eW'UROP_0[YKM}]#
b & RmjK#jT`+[a6KhOPU_#N.MYKM2KZ[_#RUTKc dV#& `+[a6KW.XR^eKR^eW'UROP_0[YKM
]#b.&_0XK[a6K^eRZ.R^_#U8]ffZ6KMqjXj[Pj A ! RIjK#jT`[a.KUT]#fffROP_#UoUTVM[YX]ffZ6f#KMY[&]ffZ6KM  ja.KMYKP[&]#b:W'XR^K
R^W"UROP_#Z[M+]#b_br]#X^&N.U_ &`.c6KZ.]#[YKcdVb
fi	i	& `'RM4c6K  Z6Kchc.N'_#UUTV_#MMC


fi	"	&

!$#U1[YKPX

^ A 1GA ! & _#Z.c 4 ff 1 ? [YKPX^ MPj[Pj#1 ? A ! & _#Z.c)1 A ! 1 ? _#Z.cb1 ? A4! 1*+ '
fi^]ffZ6fh_#UU[a6KR^W"UROP_#Z[M4]#b & RIjK#jT`B[a6K2[YKPX^eM?R^W"UTVRoZ6fL& `/[a6K2W.XRo^K&Ro^W'UROP_#Z[M]#b &
_0XK[a.K^e_ *Ro^e_#UB]ffZ6KM,qjXj[PjA ! RmjK#jT`'[a6KUT]#fffROP_#UoUTVq:K_0#KMY[4]ffZ.KM  j
b+OQ]ffN6XMYK#`B[a6KMYKP[3]#b+W.XR^K&R^W'UoROP_#Z[M H;_0[YKM^_VOQ]ffZ[_#RZ WK "N.RTS0_#UTKZ[[YKPX^MIH;OPU_#N.MYKMjJLK
OP_#Z XKMY[YXROQ[]ffN6X_0[Y[YK^W.[RT]ffZ [Y]]ffZ6K[YKPXL
^ H;OPU_#N.MYKhbr]#XK_#O a MYKP[e]#b3WK "N.RTS0_#UTKZ[[YKPX^eIM H;OPU_#N'MYKMPj
[_0[YKc ]#[a6KPXq4RoMYK#`,R9
Z 
fi	i	 &_#Z'
c 	 
	 & `,]ffZ.UTV ]ffZ6KXKPW.XKMYKZ[_0[RTS#KWiKPXWK "N.RS;_#UTKZ'OQKOPU_#MMRM
#KPW.[Pj
 H_S % :<,_
/ VI  % e  +~6uuPYv !  -"w yrxuy1.x -"5y  #vu%
 !  & yo 1 P4
 z3v <q 
k  uv & ! # ff
t;u 
=z.yv5y !0z1
	

 	& !$#

H)S % H :XVXH  % :Ye3H % eH :@ % 3H-:YV-H)e

+ '

J uv <mzuw{#z3 J uwv <q  
a6KOQ]ff^W'UTK6RT[V&XKMN.UT[M\q+KfffRTS#KRoZe[a.RM\W'_0W/KPX\XKPbrKPX:[Y]}MY]ff^K3OQ]ff^W'UK*RT[VeOPU_#MMYKM+q4a.RoOaec6KMKPXS#K
MY]ff^K XKOP_#UoUMPj ]#XKLc6KP[_#RUMOP_#Z d/Kbr]ffN.Z.c RZ 8_0W'_#c.R^eR[YXRT]ffN  M[ # <[YK [Yd/]]#"
j 
3RTS#KZ _
W.X]#d"UTK^ `iq:K2c.KZ6]#[YK}dV  [a6K2OQ]ff^W'UTK^KZ[_0XVW'X]#d'UTK^ ]#b j4JLK_#MMN.^K&[a._0[[a.K&OPU_#MMYKM
9`  _#Z.c L_0XKZ.]q4Z[Y][a6K?XK_#c6KPXj+a6K?br]ffUUT]qRZ6f2OPU_#MMYKM4q4RUU/_#UMY]d/KOQ]ffZ.MRc6KPXKc C
M B:RM([a6KOPU_#MM]#b8_#UoUBU_#Z6fffN._0f#K/M ( MN.O ah[a._0+[ ( ! ( ( )
fi ( - `6qa6KPXK
 - _#UoMY]Z.]q4Z_#!
( ( RoMRZ  _#Z.
c ( - RZ  9j a6K&OP_#Z6]ffZ.ROP_#U " - OQ]ff^W'UTKP[YK2W.X]#d'UTK^ RM!+$ #&%('*)M!$ #C,_
W"_#RTX]#b,br]#X^&N.U_#,
M +	$ % .7 -}RM}RZ !+$ #$%('/)M!$ # RTb(_#Z.c ]ffZ.UTV RTYb $ RMM_0[RM  _0d'UKe_#Z.c 7RMZ6]#[Pj
4a6KOQ]ff^W"UTK^KZ[_0XVeOPU_#MM  " - RM\[a6K3OPU_#MM=]#b9_#UU'U_#Z.fffN._0f#K*M ( MN'Oa[a._0*[ ( ! ( ( . ( - `
qa6KPXK ( ( RoMRZ   _#Z.c ( - RZ  ja6KOP_#Z6]ffZ'ROP_#U  " - OQ]ff^W'UKP[YKW.X]#d"UTK^ RM,!+$ #&0
M +	$ % .7 -,RM(RZ !$ #(0 1/2 04'*)M!+$ #RTb_#Z.c]ffZ'UTVRTb $RM(M_0[RM  _0d'UTK
132 04'*)M!+$ #C_&W'_#RTX(]#b9b1]#X^&N.U_#5
]#X7 RM4Z6]#[Pj
7/l87

\^]76

fi

~33



s~$w

~ww$

  - ! RM[a6KOPUo_#MM]#b_#UoU\U_#Z6fffN'_0f#KM}XKOQ]#fffZ.RTg_0d'UTKRZW/]ffUTV*Z6]ff^eRo_#U8[R^KdV_c.KP[YKPXY
^RZ.RMY[RoO?9N6X RZ6fe^e_#O a.RZ6K}KW" N.RW.W/Kcq4RT[a_#Z  L]#X_#OPUTK#`/RmjK#jT`/_c6KPS*ROQK&_0d'UTK[Y]MY]ffUTS#K2_#ZV

RoZ.MY[_#Z.OQK?]#b_#Z  ]#X4_   W.X]#d"UTK^ RZhN.Z.RT[([R^eK#j  - RM[a6KOQ]#XXKMYW/]ffZ.c'RZ6fOPU_#MM,]#b
bN.Z.OQ[R]ffZW.X]#d'UTK^MPj
 &  - !   RM4[a6K}OPUo_#MM]#b=_#UUU_#Z6fffN._0f#KMXKOQ]#fffZ.RTg_0d"UTK}RZWi]ffUVZ6]ff^R_#U/[R^K}dVh_Z.]ffZ.c6KQ
[YKPX ^eRZ.RM[RO?9N.XRZ6fe^e_#O a.RZ6K}WK "N'RTW.W/Kcq4RT[a_#Z   ]#X_#OPUTK#j44a6KOP_#Z6]ffZ.RoOP_#U &  - OQ]ff^W"UTKP[YK
W'X]#d'UTK^ 		 ff
fi RM&[a6KMKP[]#b4_#UU:[YXRTW'UK

M +:\ !=#  ( %('''%   + % ^ !=#S ( %('''% S ) + % 
 -qa6KPXK
\ _#Z.c ^ _0XK[q+] c.R5M F%]ffRZ[MYKP[M]#bW'X]#W/]ffMRT[RT]ffZ._#U(S0_0XR_0d"UTKM_#Z.c 
 RM_ br]#X^}N'U_LbrX]ff^

3
%` a(j fi W/]ffMR[RTS#K}RZ'MY[_#Z.OQK2]#b\[a.RoMW'X]#d'UTK^ RM3_[YXRTW'UTK +:\ % ^ % 
 -br]#X3q4a.RO a[a6KPXK
K6RMY[M4_ \Iq:]#XUc = `MN.Oah[a'_0[b1]#X_#UWU ^}Iq+]#XUoc%= a q:Ka._S#K= `/
%= a A ! 
3j
U fi  - OQ]ff^W'UTKP[YKW.X]#d'UTK^ 	 
fiRM}[a6KMYKP[2]#b4_#UU
 fi  - ! Q&  - !    j a6KOP_#Z6]ffZ.RoOP_#
[YX RTW'UTKM +:\ ! #  ( %('''%  + % ^8! #S ( %('''% S ) + % 
 -q4a.KPXff
K \ _#Z.G
c ^ _0XK[q:]c.RM FY]ffRZ[MYKP[M?]#b
W'X]#W/]ffMRT[RT]ffZ._#US0_0XR_0d'UTKM_#Z.c 
 RM_br]#X^}N'U_4brX]ff^ 

 ` a /j fi Wi]ffMRT[RTS#K(RZ.MY[_#Z.OQK(]#b"[a.RM
W'X]#d'UTK^ RM3_h[YX RTW'UT
K +:\ % ^ % 
 -?MN.Oa [a'_0[b1]#X?KPS#KPX]
V \4Iq:]#XUc = ` [a6KPXKK 6RMY[Mm
_ ^}Iq:]#XUc
=Ya br]#Xq4a.RO a= `/
%= a A ! 
3j
&  - _#Z.c fi  - _0XKOQ]ff^W'UK*RT[VeOPU_#MMYKM:UT]*OP_0[YKc_0[:[a6KMY]0OP_#UUTKcMYKOQ]ffZ.cUTKPS#KU.]#bB[a.KW/]ffUTV*Z6]ff^eR_#U

a.RTKPX _0XOaV#`.q4a.RoOaW'U_VM4_2W.X]ff^eRZ.KZ[(X]ffUTKRoZhZ.]q4UTKc.f#K?XKPW.XKMYKZ[_0[RT]ffZ_#Z'chXK_#MY]ffZ.RoZ6f6j

8A E  RG  4H l W

D F  aE   *G [H O*	 C	D -F T D -F  D B
6]#X^&N.U_;UoRT[YKPX_#U_#Z.c b1]#X^&N.U_;IS0_0XR_0d"UTKLRZ.c6KPW/KZ.c6KZ'OQKOP_0W.[N.XKMY]ff^Kb1]#X^M]#b2RoZ.c6KPW/KZ.c6KZ.OQK
d/KP[q:KPKZ[a6K2[YXN6[aS;_#UN.KM4]#b=S0_0XR_0d"UTKM4_#Z.c[a6K&W/]ffMMRTd"UTK?[YXN6[aS;_#UoN6KM4]#b+_br]#X^&N.U_*j E]ffN6fffa.UV
MYW/K_0*RZ6f6`&RoMc6KPW/KZ.c6KZ[]ffff
Z .d/KOP_#N.MK+RT[[YKUUM]ffZ6K:M]ff^KP[a.RZ6fW/]ffMRT[RTS#K:_0di]ffN.[  C9^]#XK+W.XKOPRMYKUTV#`
[a6KPXKRM_OQ]ffZ[YKMY[P`+RmjK#jT`_OQ]ffZ FN.Z.OQ[RT]ffZ]#bURT[YKPX_#UM` [a._0[P`_#c.c6Kc[Y][Y] &KZ._0d'UKM]ffZ6K[Y]RZ6brKPX
j6]#XRZ'MY[_#Z.OQK#` & !  k
 I SPRM3c6KPW/KZ.c6KZ[4]ffZ S	`BMRZ'OQc
K S}OP_#Z d/K&RZ.b1KPXXKcbrX]ff^ &sN.Z.c6KPX[a6K
_#MMN'^W.[RT]ffZ[a._0[6RM=[YXN.K#j b OQ]ffN6XMYK#`*q:KOP_#Z'Z6]#[:_#MMN.^KOQ]ffZ[YK *[M([a._0[,_0XK3RZ.OQ]ffZ.MRoMY[YKZ[\q4RT[a
&`:_#Z.c q+KOP_#Z'Z6]#[_#MMN'^m
K R[MYKUTbj fi b1]#X^&N.U_ & q4RoUU\d/KhOQ]ffZ.MRoc6KPXKc _#MeRZ'c6KPW/KZ.c6KZ[brX]ff^
S0_0XR_0d'UT
K >RTb=_#Z.c]ffZ.UVRTb\RT[3RMd/]#[aRoZ.c6KPW/KZ.c6KZ[4brX]ff
^ >_#Z.c RZ.c6KPW/KZ.c.KZ[brX]ff^ %: >9
j N'_#UUTV#`
q:K3OP_#ZhRoZ[YKPXW'XKP[+d/]#[abr]#X^eM:]#b9c6KPW/KZ.c.KZ.OQK_#M,_0d/]ffN6[Z.KMM:XKU_0[RT]ffZ.Mqa6K+
Z & RoM+c6KPW/KZ.c.KZ[:]ffZ
_eUR[YKPX_#WU `'RT[,[YKUUM(]ffZ.KMY]ff^KP[a.RoZ6fe_0d/]ffN6/[ `'_#Z.chq4a.KB
Z & RMc6KPW/KZ.c.KZ[,]ffZ_eS;_0XRo_0d'UT6
K >`'R[,[YKUUM
MY]ff^KP[a'RZ6fe_0d/]ffN6T[ >]#X_0d/]ffN6[%: >j




 $ U

BH lmk 9 {z ;  ; z3{MLq <\q{4L#q{ ; q
a6K2K_#MRKMY[?q:_V[Y]c6K  Z6K2c6KPW/KZ.c6KZ.OQK2d/KP[q:KPKZ_br]#X^&N.U_T&s_#Z.cL_hURT[YKPX_#UY:RM3dV_#MMN'^eRZ6f
[a._0[ &`q4a6KZ W'N6[?RZ[Yk
] 4O4:`OQ]ffZ[_#RZ.Mj EK^RZ.c.RZ.f[a._0[( 3 *P	&RM3[a6KeMYKP[?]#b(URT[YKPX _#UM[a._0[
]*OPOPN6X4RZ[a6
K 4?4 ]#b &3`.[a.RM,OP_#ZdiK3br]#X^e_#UUVK*W.XKMMYKcdV[a6Kbr]ffUUT]	q4RZ6fc6K  Z'RT[RT]ffZC

 [w !0x7

 1/2
k "  {z ;  ; z3$Uy&%9'{MLqff<q{4Lq{ ; q('  uv & P uhNM!0wx /*TN
q!m{uw{ #
yrvuPw%0 ![+(* 1:0z"tff(I/=PuPv ![+(R 
 & yo}Q0ymthv	! P u?MYV*Z[_#OQ[ROP_#UUTV @9RT[%c6KPW/KZ.c6KZ[ !0z NJIwu - MVZ[_#OQ[ROP_#UUTV @9RT[%RZ.c.KPWiKZ'c6KZ[
w !02
x  Key 0z"T
t !0z' 4y /*)m( 3 *Q	&4 JwYuQ	- T )k
4 ( 3 P* 	& K 
\^],+

fif.w~3}33f$3ws

 & yo&Q0ymtv	! PuMYV*Z[_#OQ[ROP_#UoUTV @9RT[%c6KPW/KZ.c6KZ[ 0! z_( y 0z"t'!0z. 4hy vr~*uPwYuey1 ) (  /P~
vr~*#v.& y1 4;z"vIvy00r 4  yv5O%tu -6uQzituQz'v !0z   ,vr~*uPw |y1Pu 1 & yoQ0ymtv	! PuMVZ[_#OQ[ROP_#UUTV
9@ RT[%RZ.c.KPWiKZ'c6KZ[  w 0! x2(



6 X]ff^ [a'RM+c.K  Z.RT[RT]ffZRT[\br]ffUUT]	q4M:R^e^Kc.Ro_0[YKUTV2[a._0[ & RM+MYV*Z[_#OQ[ROP_#UoUTV @R[%RZ.c6KPW/KZ.c6KZ[brX]ff^
( RTb?_#Z.c ]ffZ'UTV RTb3( 3 *P	&6fiK( ! j aN.MP`,RZ ]#Xc6KPX[Y] c.KP[YKPX^eRZ6Kq4a6KP[a6KPX_br]#X^&N.U_ & RM
MYV*Z[_#OQ[RoOP_#UUTV%@9RT[%c6KPW/KZ.c6KZ[(]ffZh_MYKP[/( ]#b8URT[YKPX_#UoMP`R[(MN6OQKM,[Y]eO a6KOq4a6KP[a.KPX,[a6KPXK3K*RoMY[M_
URT[YKPX _#UBRm
Z ( [a._0[4]OPOPN6X MRZ[a6
K 4O4 ]#b &j




 ! :,YF/#P  ~*uNON?P ![ 
 y1 :,THG: P  vr~6uQwu5M!0wYu 1 
 y1& 04 z'vIvy0#1 4
 z3v <q 7  u v  0zit P 
 yv5OYtuR-6uPz"tuPz'v !0z 3P !#vr~
:,0z"t :YP 1\|8~yrTu}yv+yo3I40z'vIQvy00r 4  ry v5Oy1zituR-6uPz"tuPz'v=w !0x 
fiM[a.RoMK *_#^W"UTKRUUN'MY[YX_0[YKMP`3_ W.X]#W/]ffMR[RT]ffZ._#Ub1]#X ^}N.Uo_ OP_#Z K_#MRUTV d/KMYV*Z[_#OQ[ROP_#UoUTV @9RT[%
RZ.c.KPWiKZ'c6KZ[&brX]ff^ _UR[YKPX_#U(q4a.RUKMYV*Z[_#OQ[ROP_#UUTV @R[%c6KPW/KZ.c6KZ[&]ffZ RT[MeZ6KPfff_0[RT]ffZ j a.RoM&RoMe_#M
K*WiKOQ[YKc `*MRZ.OQK & ^e_VR^W'UTVc RZMY]ff^K3OQ]ffZ[YK*[P`6q4a.RoUTKRT[+^_Ved/K_#Uq:_VM:R^W/]ffMMRd'UTK([Y]&c.KPXRTS#K
Y: j(pZ]#[a6KPXq+]#Xc'MP`'[a6K&MYKP[4]#b=UoRT[YKPX_#U.M & RM4MVZ[_#OQ[ROP_#UUTT
V @9RT[%RZ.c6KPW/KZ.c.KZ[(brX]ff^ RMZ6]#[OPU]ffMYKc
N.Z.c.KPX+Z6KPfff_0[RT]ffZ jpZ[YKPXKMY[RZ.fffUTV#`6_}Z6]#[R]ffZ]#bMVZ[_#OQ[ROP_#U/br]#X^}N'U_;IS;_0X R_0d'UTK4RoZ.c6KPW/KZ.c6KZ.OQKOP_#Zd/K
c6K  Z.Kcb1X]ff^ [a6K^]#XK?d"_#MRO?Z6]#[RT]ffZ]#bMYV*Z[_#OQ[ROP_#U @BRoZ.c6KPW/KZ.c6KZ.OQK#j
q!m{uw{ 7 "  {z ;  ; z3$ %9'{4#
L qff<q{ML q{ ; q('  uPv & PuM!0w xN/*TVw[!0xQ
3
% 1Y< 
;#w ymQPQuN![+
fffiE1:0z"t U  /PQPuv1![+
fffi 
 & y1P0yte	v ! Pu(MVZ[_#OQ[ROP_#UUTVNS=_0XYc.KPWiKZ'c6KZ[ !0z < JIwu - (MYV*Z[_#OQ[ROP_#UoUTVNS=_0XYRZ.c6KPW/KZ.c.KZ[
w !0x <QKey #z"T
t !0z. 4y T< )'U	.	 & JwYuQ	 - T< )B
4 U,	6	&4 K 
 & yo}Q0ymt	v !'PuMYVZ[_#OQ[ROP_#UUV S=_0XYc6KPW/KZ.c6KZ[ !0z U y 0z"tB!#z. 4hy 2vr~*uQwuyoh;#w ymQPQu<
yr+
z U  ov  &yo
 .0w O%tRu -6uPz"tuQz"v !0zyrv>1=y u I1y 0z"L
t !0z. 4ey YU.	 & fi U ! 4  ,vr~*uPw |y1Pu 1 &
y1?Q#yt	v ! PuMYV*Z[_#OQ[ROP_#UUTL
V S=_0XYRZ.c.KPWiKZ'c6KZ1[  w !0x U,

	

& ! D/ :-SQ &yoI40z'vIvy5 01 4 '0wIOYtu -6uQz"tuQz'v !0z L
 0z"t%!0zbS#z"t 40z'vIMO
 z3v < q H  uv"
vy0 0r 4 .0w O yrz"tu -6uQzituQz'vw 0! x V 




 V*Z[_#OQ[ROP_#U+ S RZ.c6KPW/KZ.c.KZ.OQKeOP_#Zd/KK_#MRoUTV KW.XKMMYKc_#M}MYV*Z[_#OQ[ROP_#U: @BRZ'c6KPW/KZ.c6KZ.OQKC
& RM,MYV*Z[_#OQ[ROP_#UUTVLS=_0XYRZ.c.KPWiKZ'c6KZ[(brX]ff^:U RTb9_#Z.c]ffZ'UTVRTbRT[,RM,MYV*Z[_#OQ[RoOP_#UUTV%@9RT[%RZ.c.KPWiKZ'c6KZ[
brX]ff^:U . #):%> A> )BU +ffj1?(UTK_0X UTVeKZ6]ffN6fffa `d/]#[ahMYV*Z[_#OQ[ROP_#U br]#X^&N.U_;URT[YKPX _#U"RZ'c6KPW/KZ.c6KZ.OQK3_#Z.c
MYV*Z[_#OQ[RoOP_#U br]#X^}N'U_;IS;_0X R_0d'UTK?RZ.c.KPWiKZ'c6KZ.OQK3OP_#Zd/K?Oa6KO#KcRoZhURZ6K_0X,[R^eK#j
]q:KPS#KPX`[a.KMYK,d'_#MRO+br]#X^eM]#b"RZ'c6KPW/KZ.c6KZ.OQK\MN iKPX8brX]ff^[q:]R^W/]#X[_#Z[8c6X_q4d'_#O*MPj8RXMY[P`
[a6KPV c6] Z6]#[M_0[RMYbrV [a.KW.XRZ'OPRTW'UTK]#b2RXXKUTKPS0_#Z.OQKL]#b2MYVZ[_ C [q:] KW"N.RTS0_#UTKZ[hb1]#X ^}N.Uo_#M_0XK
Z6]#[e_#UTq(_V*MeMYV*Z[_#OQ[ROP_#UUTV RZ.c6KPW/KZ.c6KZ[brX]ff^ [a6KhM_#^eKURT[YKPX_#UM H	S;_0XRo_0d'UTKMPj KOQ]ffZ'cB`+MYV*Z[_#OQ[R 
OP_#UBc6KPW/KZ.c6KZ'OQK3c.]KM,Z6]#[_#Uq:_VMOP_0W'[N6XK3[a6KRZ[N.RT[RTS#K3^K_#Z.RoZ6f2]#bc6KPW/KZ.c.KZ.OQK C8b1]#XRZ'MY[_#Z.OQK#`
& !  ,/ X
: S6/ c
 H SPY3RoMMVZ[_#OQ[ROP_#UUTV @R[%c6KPW/KZ.c6KZ[]ffZ "`X: S	&` S	9MRZ.OQKX: S}OP_#ZLd/K2c.KPXRTS#Kc
brX]ff^ &` &RM?_0d/]ffN6[X: S&RZLM]ff^KMYKZ.MK#j ?:]ffZ[YX_#MY[RZ6fffUV#`[a6KPXKRM?Z6]q(_V [Y]c6KPX RTS#
K S}b1X]ff^ &`
N.Z.UKMM(W.X]*c.N.OPRZ.f&_#ZRoZ.OQ]ffZ.MRM[YKZ.OQV#j
_#Z.c.URoZ6f&MN.Oah_MYKPW"_0X_0[RT]ffZXWK "N.RTXKM(_^]#XKX]#d'N.MY[,Z6]#[RT]ffZ]#bRZ.c6KPW/KZ.c.KZ.OQK#`[Y]d/K?RZ[YX]0
c.N.OQKcRZ[a6K?br]ffUUT]qRZ6fMYKOQ[RT]ffZBj
\^]^]

fi

~33



s~$w

~ww$

BH l87:9 qvxz3{ ; zw&{4L#qff<q{MLq{ ; q
JLKZ6]	q fffRTS#K_PuQxe0z"vy5 06c6K  Z.RT[R]ffZe]#b9RoZ.c6KPW/KZ.c6KZ.OQK#`q4a.RoOac6]KM,Z.]#[(MN iKPX(brX]ff^ [a6K?_0br]#XKQ
^KZ[RT]ffZ6Kc c.X_qd"_#O*MP`:RIjK#jT`+_c6K  Z.RT[RT]ffZ [a._0[ec6]KMeZ6]#[ec.KPWiKZ'c ]ffZ [a6KMYV*Z[_#OQ[RoOP_#Ubr]#X^ RZ
q4a.RoOa br]#X^}N'U_#M2_0XKK *W.XKMMYKc j JLKqRUU+W.X]S#K[a._0[[a.RMMYK^_#Z[ROP_#U4c6K  Z.RT[R]ffZ]#b3RZ.c6KPW/KZ*
c6KZ.OQKc.]KMeZ.]#[eMN /KPXb1X]ff^ [a6KMYKOQ]ffZ.c c.X_qd"_#O ]#bMVZ[_  RoZ.c6KPW/KZ.c6KZ.OQK#`+RmjK#jT`+_Lbr]#X^&N.U_
[a._0[RM,MYK^e_#Z[ROP_#UUTVhc6KPW/KZ.c.KZ[,]ffZ_eURT[YKPX_#UB_#Uq:_VMKZ._0d'UTKM]ffZ6K[Y]c.KPXRTS#K?[a6KUoRT[YKPX_#U RZMY]ff^K
OQ]ffZ[YK [Pj
v & Pu M!0wN
x /T [w !#x 
3
% 81  )
q!m{uw{ H " " qvxz3{ ; z3 ' $Uy&%9'{MLq <\q{4L#q{ ; q'  u
(R 1(0z"fft ( I /=PuP1v ! +(R 
t !0z' 4hy vr~*uQwu,u 2#y1vre
 & yo&Q0ymt	v !'Pu @RT[%RZ'c6KPW/KZ.c6KZ[ - [w !0x  14tuPz !#vut   4 &1,y 2#z"B
M!0wN
x /T 
  ov  
  & #z"t 
 yoI 40z'vI v5y  01 4  y5v Oyrz"tu -6uQzituQz'v w !0x   ,vr~*uPw |y1Pu 1 & y1
P0ymt	v !+P.u @9RT[%c6KPW/KZ.c6KZ[ !0)
z  1=tuQz !#vuc
t   &5 ~*uPu1v ![0rByrvuPw%0
 ![/(   /P~vr~*#v
  &y1tuQz !ffvuB
t PM4
ffr ( 3 *Q	 & 
t (  4 &1By (0zit !0z. 4?y %(fi68 ( 3 *P	 & !
 & yo+Q0ymt	v ! P u @9RT[%RZ.c6KPW/KZ.c.KZ[  w !0x (F19tuQz !ffvu/
  ,vr~6uQw|you$1 & y1P0ymt	v !+P u @RT[%c.KPWiKZ'c6KZ[ !0m
z ( 1+tuPz !#vut (  &5

*Ro^W'UTV}XKPqXRT[RoZ6f?[a6Kc.K  Z.RT[RT]ffZ `ff& RM1@RT[%RoZ.c6KPW/KZ.c6KZ[b1X]ff^!( RTbB_#Z.ce]ffZ'UTVRTb/[a6KPXK4K6RMY[M:_
br]#X^}N'U_ 
LMj[Pj 
  &_#Z.c 
LRM9MYVZ[_#OQ[ROP_#UUV@9RT[%RZ.c.KPWiKZ'c6KZ[/brX]ff^ (4j4aN.M`; @BRoZ.c6KPW/KZ.c6KZ.OQK
RM9Z6]#[_ /KOQ[YKc}dV?[a6K:MYVZ[_#OQ[RO\b1]#X^ RZ?q4a'ROa}_br]#X^}N'U_,RM K*W.XKMMYKcB`;[a._0[9RM`	XKPW"U_#OPRZ6f.&Lq4RT[a
_#ZVh]#b=RT[MWK "N.RTS0_#UTKZ[b1]#X ^}N.Uo_#M4c6]KM4Z6]#[^]*c.RTbrV[a6K}XKU_0[RT]ffZ  .
j *RZ.OQK & RM @9RT[%RZ.c.KPWiKZ'c6KZ[
brX]ff^ ( RTb_#Z.c]ffZ.UVRTb & OP_#Zd/K3^e_#c6K?MYV*Z[_#OQ[ROP_#UoUT%
V @9RT[%RZ.c6KPW/KZ.c.KZ[=brX]ff^ ( qa.RUTKW'XKMYKPXS*RZ6f
UT]#fffROP_#U=WK "N.RS;_#UTKZ'OQK#`9RT[br]ffUUT]	q4M?[a._0[}MYVZ[_#OQ[ROP_#U @RT[%RoZ.c6KPW/KZ.c6KZ.OQK2R^W"URTK
M @RT[%RoZ.c6KPW/KZ.c6KZ.OQK#`
d'N6[,[a.KOQ]ffZS#KPXMK}c6]KMZ6]#[4a.]ffUchRZ[a6K?f#KZ6KPX _#UOP_#MYK#j
& !   /G:XS6/  H]SPYu~00u 
ffr( 3 *P	& ! #  % :-S&+]N !#vuvr~#vY& y1
 z3v <q  uv 
!  / :XSP 1=y1z 8
| ~y0Q~ St!u}z!#v(I--6u0w
 yv5Oy1z"tuR-6uPz"tuQz"vw !0x S%P u3 D/Pu&yv=y1&u	W/*y100TuPz'v:v	! 
 
-G;! yrvyr0uP 4 
fiMRoZh[a6K2OP_#MYK]#b=MYV*Z[_#OQ[ROP_#U8RZ.c6KPW/KZ.c6KZ'OQK#`6q+K2OP_#Zbr]#X^e_#UoRTgPK[a6K}br_#OQ[[a._0[_b1]#X ^}N.Uo_%&
.a _#M8MY]ff^K:K /KOQ[M8]ffZ}[a.K+[YXN.[aS0_#UN6K+]#b'_4S0_0XR_0d"UTKR<"jp%Z.c6KPKc `	q:K,c6K  Z6K:_Z6]#[R]ffZ]#b9MK^e_#Z[RoOP_#U1
br]#X^}N'U_;IS;_0X R_0d'UTKRZ.c.KPWiKZ'c6KZ.OQK#`q4a'ROaOP_#Z}_#UMY]d/KK_#MRoUTVc.K  Z6Kc?brX]ff^ [a.KMYK^e_#Z[ROP_#U1/Z.]#[RT]ffZ
]#b @BRZ.c.KPWiKZ'c6KZ.OQK#j

!

 " " qvxz3{ ; z3 ' $ '{MLqff<q{4Lq{ ; q('  u vG& P ,u !0wxN/*   w !0x 
3
  1< )

q m{uw{


fiE1+#z"t

U I/=PuPv ![+
fi 

& yoQ#ytv	!%P u S=_0XYRoZ.c6KPW/KZ.c6KZ[  w !0x < 1+tuQz #! vut<  4 , &18y 0z"tL!0z. 4ey vr~*uPwYu2u,2#y1vr
 
M!0wxN/T 
 1v  
 &0z"t 
sy14I40z'vIQvy00r 4 .0w Oyrz"tu -6. uQz"tuQz'v= w !0x <  ,vr~6uQw|you$1&sy1
P0ymtv	!'P u =S _0XYc6KPW/KZ.c6KZ[%!0z_<1tuPz!#vut<  ,. 5
& utuQz!ffvuLPM4 
ffU.	&2vr~6u2uPv
!}
 0r ;0wymQQP u+<  /Q ~hvr~#v%<  , &fi

	

.
Do
	fiffy{z	~u zY}y?yp~ff{ fifx8u'x||Mz}u u zY}y/}su*ffD	{ }| yz}T%ytfff~iuluff~uffu*u| }|z}yp
ffyx y ff"!ff?y}suz&x8yz	~ifff~iuluff~uffuY:#lu%	$puff6{&u /ffU}|%&+~iu' | &ffO}su%z}u :y#}suYlu zo
Q)((

fif.w~3}33f$3ws

y1LQ0ymt 	v ! Pu S=_0XRZ.c6KPW/KZ.c6KZ[ w[!#x U 1tuPz!#vut6U  4 , &1y  0zit(!0z. 4 y 'U fi
U	.	&ff!   ,vr~*uPw |y1Pu 1 & yo?P0ytv	!TPu.S=_0Xc6KPW/KZ.c6KZ[ . !0z U 1+tuQz !#vut U  ,. &5
?(UK_0XUTV#`& RM S=_0XYRoZ.c6KPW/KZ.c6KZ[(b1X]ff^ U RTb_#Z.ch]ffZ.UVRTb9[a6KPXK?K6RMY[M_br]#X^&N.U_ 
 MPj[Pj 
 &
_#Z.c 
 RMMYV*Z[_#OQ[RoOP_#UUTV S=_0XYRZ'c6KPW/KZ.c6KZ[eb1X]ff^ Uj$]#XKP]S#KPX`.S=_0XYRZ.c.KPWiKZ'c6KZ.OQKRM[Y]X@9RT[%
RZ.c.KPWiKZ'c6KZ.OQK,_#M\MYV*Z[_#OQ[ROP_#=U S=_0XRZ.c6KPW/KZ.c6KZ'OQK(RoM[Y]}MYV*Z[_#OQ[ROP_#U @9RT[%RZ.c.KPWiKZ'c6KZ.OQK#ffRZ.c.KPKcB` &
RM S=_0XYRoZ.c6KPW/KZ.c6KZ[,b1X]ff:
^ U Rb9_#Z.ch]ffZ'UTVRb & RM @9RT[%RZ.c.KPWiKZ'c6KZ[:brX]ffQ
^ (*$,j
v &! Y/G: PE/ Y+ H Y  u~#0u  U	.	 & ! #  % S>+ N !#vuvr~#Yv & y1
 z3v <q  uY
.0w Oy1z"tRu -6uPz"tuQz"v w !0x  
a.K3c.K  Z.RT[RT]ffZ]#b8MYK^e_#Z[ROP_#U  @BRoZ.c6KPW/KZ.c6KZ.OQK3RM(d'_#MKc]ffZh[a6KMYKP[]#b8URT[YKPX _#UM:]#b9br]#X^&N.U_#M
KW"N.RTS0_#UTKZ[,[Y+
] &j=p%Z[N.R[RTS#KUTV#`6[a.RM,RoM([a6KK_#MRTKM[q:_Vh[Y]c6K  Z.K_Z6]#[RT]ffZh]#bRZ.c.KPWiKZ'c6KZ.OQK[a._0[
RM}Z6]#[}c6KPW/KZ.c.KZ[]ffZ[a6KMVZ[_ ij4]	q+KPS#KPX`\W.X]S*RZ6f[a6KP]#XK^eM&c.RTXKOQ[UVbrX]ff^ [a.RMc6K  Z.RT[RT]ffZ
RM}Z6]#[}MY]K_#MYV#j.]#X&RZ'MY[_#Z.OQK#`8q+Kq4RUU8W'X]S#K[a._0[}c.KP[YKPX^eRZ.RoZ6fhq4a6KP[a6KPX&_br]#X^&N.U_ & RM @9RT[%
c6KPW/KZ.c6KZ[]ffZ URT[YKPX_#RU RMRoZ  `d'N6[[a.RMXKMN.UT[OP_#Z'Z6]#[}d/Kt#y1w3u v 4W.X]S#Kc brX]ff=
^ K  Z'RT[RT]ffZ
] &OP_#Z.Z6]#[d/Kc6]ffZ6Kq4RT[aL_hW/]ffUTV*Z6]ff^eR_#U
 `9MRZ.OQKeOa.KO*RZ6f_#UU=Wi]ffMMRTd'UTK&br]#X^}N'U_#M3WK "N'RTS;_#UKZ[?[Y'
Z6]ffZ*c.KP[YKPX^eRZ.RoMY[RO=fffN.KMMRZ6f6jJLKfffRTS#KZ6]q _MK^e_#Z[RoOP_#U6Oa._0X_#OQ[YKPX RTg_0[RT]ffZ]#b" @BRZ'c6KPW/KZ.c6KZ.OQK#j
 ru=<uwuw{ 
x /*  &yo  yr5v Oy1zitRu -6uPz"tuPz'=v w !0x yvuQwY0r ,y 0z"+
t !0z' 4y  1 !#w}0z 4y1z'vuPIw O
k  !0wN
-"wuPvI#v0y !0z = )_[T 1=y  =.A ! & vr~6uQz &')l V >= % Y
: IFA ! &5
fiM2_c'RTXKOQ[}OQ]#X]ffUU_0XV#`q:Kf#KP[&[a'_0[ & RM @R[%RZ.c6KPW/KZ.c6KZ[?brX]ff^ (RTb_#Z.c ]ffZ.UTVRTb,br]#X_#ZV
URT[YKPX _#WU  ) ( _#Z.c_#ZVhRZ[YKPXW.XKP[_0[RT]ffZ= )_[Ti`'RTb4=;A ! & [a6KZ &'"l V>= % X: mDA ! &j
a'RM=W.X]#WiKPX[VfffRTS#KM,_#ZRc6K_]#ba6]	q  @ c6KPW/KZ.c6KZ.OQKq+]#X*MPjp%Z.c6KPKcB`*RTb & RM @RT[%c6KPW/KZ.c.KZ[
]ffZh_2URT[YKPX_#8U `*[a6KZ[a6KPXK3K *RoMY[M(_#ZhRZ[YKPXW.XKP[_0[RT]ffZ
= MN.Oa[a._0[ =;A ! & _#Z'c
&'"l V>= % X: m A4! &`
q4a.RoOa^K_#Z.M([a._0[_6=;A ! 9_#Z.cLrdi\[a6K?UR[YKPX_#tU 9RZ
= Rc
M J%XK_#UUTVZ6KPKc6KMc L&[Y]e^e_0#K = _2^]*c6KU
]#b &`6[a._0[4RoMP`*[a6KW'_0X[Ro_#UBRZ[YKPXW.XKP[_0[RT]ffZh]#d'[_#RZ6KchdVXK^]S*RZ6f 8RZ= c.]KMZ.]#[M_0[RMYbrT
V &3j
a'RMW.X]#W/KPX[V_#UMY]K *W'U_#RoZ.Mq4aVe @Bc6KPW/KZ.c6KZ'OQKb1]#X ^e_#URTgPKM\[a6KOQ]ffZ.OQKPW'[+]#/b J%[YXN.KRZMY]ff^K
OQ]ffZ[YK [ L*`_#MK *W'U_#RoZ6Kc_0[3[a6K2d/KPfffRZ.Z.RoZ6f]#b\[a.RoMMYKOQ[R]ffZBj3pZ.c.KPKcB` & R
M @9RT[%c6KPW/KZ.c6KZ[4]ff]
Z \Rb
_#Z.c]ffZ.UTVhRTb8[a6KPXK2RM4M]ff^K}OQ]ffZ[YK [eOQ]ffZ.MRMY[YKZ[4q4RT[a &`"_#Z'c[a._0[c6]KMZ6]#[R^W'Ub
V I `/RZhq4a.RoOa
& Ro^W'URTKT
M j(a.RM4OP_#Zd/KW.X]S#KcbrX]ff^ [a6K}_0d/]S#K&W.X]#W/]ffMR[RT]ffZ C=RTb= RM4_#ZRZ[YKPXW.XKP[_0[R]ffZMN.O a
[a._0[F=;A ! & d'N6[F&')l V>= % Y: I A4! &`6[a6KZh[a6K[YKPXT
^ C
1 !  #D> )k
fiOA%=;A ! & b
/ > + . #)%: > A > )k
fi  e =;A ! & / %: >*+;	  #i +
RM=OQ]ffZ'MRMY[YKZ[=q4RT[L
a & rdV2OQ]ffZ'MY[YXN.OQ[RT]ffZ ` 1hRMWK "N.RTS0_#UTKZ[[Y]}[a6K4c.RoM FYN'Z.OQ[RT]ffZ2]#bB_?[YKPX^ WK "N.RS;_#UTKZ[
[Y]= q4R[a_}[YKPX^ WK "N.RS;_#UTKZ[\[Y] &')l V>= % Y: IY .RT[:_#UMY]2a6]ffUc.M\[a._0R[ 1/ & A ! `q4a.RUT/K 1 A4! `[a._0[
RMPB` 1 RM_OQ]ffZ[YK [RZhq4a'ROB
a & Ro^W'URTK'M j
]#XKP]S#KPX
` =X]#Wi]ffMRT[RT]ffZ 2Ma.]q4M[a._0[3]ffN6X?Z6]#[RT]ffZ]#b @9RT[%RZ.c.KPWiKZ'c6KZ.OQKOQ]ffRZ'OPRc6KMq4RT[a[a6K
Z6]#[RT]ffZ]#b J0z'vy O K	x !0z !#	v !#z.5y yr5v 4?5 EVff_#Z G` #G 0` # ff j=9]d/K\^]#XK+W.XKOPRMK#`;_OQ]ffZ'MRMY[YKZ[ Bbr]#X^&N.U_
RM M_#Rc[Y]diK+^]ffZ6]#[Y]ffZ.ROrXKMYW j_#Z[R^e]ffZ6]#[Y]ffZ.RO RZ?S0_0XR_0d'UTRK <RTb*_#Z.c]ffZ.UTV?RTb*RT[RM @9RT[%RZ.c.KPWiKZ'c6KZ[
brX]ff^ Y: < rXKMYW j b1X]ff^ <6 j p%Z[YKPXKMY[RZ6fffUTV#`_ MN.d/OPUo_#MMROP_#U 0 RoZ6b1KPXKZ.OQKXKUo_0[RT]ffZB`OP_#UUTKc z"#5v /*wY0
yrWz PuPwYuPz  uP`4a._#Md/KPKZ c.K  Z6Kc ]ffZ [a'RMef#X]ffN'Z.cs5 E4Vff_#ZB` #G 0` # ff 
j A(_#MRoOP_#UUTV#`4_ b1]#X ^}N.Uo#
_ $
RMeOQ]ffZ'MRc6KPXKc _#M_ OQ]ffZ.MYWK "N6KZ.OQK]#b_b1]#X ^}N.Uo_ & RTb?_#Z.c ]ffZ'UTV RTbR[RM_UT]#fffROP_#U4OQ]ffZ'MYWK "N.KZ.OQK
 &


iorqtsf&*z}u:z} }yff3y#|{|BuffU	{|TuffUfiff*o

Q)(

fi

~33



s~$w

~ww$

]#b}R[P`_#Z.c & @RT[%RoZ.c6KPW/KZ.c6KZ.OQK]#b}_ URT[YKPX _#UR^eW'URTKMRT[M $@9RT[%RZ.c6KPW/KZ.c.KZ.OQK#j fiOPOQ]#Xc.RoZ6fffUTV#`
Z._0[N6X _#URZ6brKPXKZ.OQKW.XKPS#KZ[MN.M4b1X]ff^ OQ]ffZ.MRc6KPXRZ.f He_#M_OQ]ffZ'MYKW"N.KZ.OQK]#b  RT[RoM_XKUTKPS;_#Z[
R^W"UROP_0[RT]ffZ" j fiUoU*]ffN6X+O a._0X_#OQ[YKPXRTg_0[R]ffZXKMN.UT[M=_0d/]ffN6[ @9RT[%RZ.c6KPW/KZ.c.KZ.OQK#`ffRZ.OPUN'c.RZ6fOQ]ff^W'UTK6RT[V
XKMN'UT[MP`+a'_S#K_#Z R^e^Kc.Ro_0[YKRo^W'_#OQ[]ffZ MN'Oa _Z'_0[N6X_#URZ.b1KPXKZ'OQKXKU_0[RT]ffZ rKMYW/KOPR_#UoUTV#`\[a6KPV
c.RTXKOQ[UTVMa.]q [a._0[4[a6KOQ]ff^W'UK *RT[V]#bZ._0[N.X_#UBRZ6brKPXKZ.OQKRM,RoZ  -  j
\X]#W/]ffMRT[RT]ffZ ?RMK_#MRUTVK [YKZ.c.Kc[Y]ebr]#X^&N.U_;IS0_0XR_0d'UTK3RZ.c.KPWiKZ'c6KZ.OQK C

J

 k
yrz'vuQw -"wuPvI#vy00! z

 M!0wx */ T




& y1 '0wIOyrz"tuR-.uQz"tuPz'v w !0x ; 0wymQPQu< y 0zit !0z. 4Ly  1 M !0wh0z4
1=|+u~00u=;A ! & y 
 0z"tT#! z. 4y ?fi 3 *V >= % <*A ! &5

= ) [/
a.Kbr]ffUUT]qRZ6fe^KP[_0[a6KP]#XKP[RO&W.X]#W/KPX[RTKM4]#b= @ RZ.c6KPW/KZ.c.KZ.OQK_0XK2N.MYKcRoZ[a6K}XKMY[]#b[a.RM
MYKOQ[RT]ffZ j
uwruz$r

ru=
< uwuw{ 7
JK
ffr( 3 *P	&59( 3 *P	&
JW
K   &  
1\vr~6uQz 
ffr( 3 *Q	&/! 
ffr( 3 *P 

J	 ,
K r( 3 *Q	& / 
' r( 3 *P	& . r( 3 *Q 
4
J	ffP 
K r( 3 *Q	& H 
' r( 3 *P	& . r( 3 *P 

Jff

K  )-
ffr( 3 *P	
& y 
 #z"tT0! z. 4y :Y ) r( 3 *Q : &










 SRZ.c6KPW/KZ.c6KZ'OQKK6a.RTd'RT[M4MR^eRUo_0X4W.X]#W/KPX[RKMP`'W'UoN.MZ6KPfff_0[RT]ffZLM[_0d'RURT[VrWi]ffRoZ[2 <d/KUT]	q `
q4a.RoOahRMZ.]#[M_0[RM  KcdV @BRZ.c.KPWiKZ'c6KZ.OQK#j


ru=
< uwuw{ H
JK
ffU	.	&5 U	6	&4
JW
K   &  
1\vr~6uQz 
ffU.	&/!.U	. 

J	 ,
K 
U .	& / 
' r( 3 *Q	&4 . 
ffr( 3 *Q 

J	ffP 
K 
U .	& H 
' r( 3 *Q	&4 . 
ffr( 3 *Q 

Jff

K 
ff
U 	. : 
& /!.U	.	&








A:KPV#]ffZ.c [a6KMYKW.X]#W/KPX[RTKMP`( @BRZ'c6KPW/KZ.c6KZ.OQK_#Z.c  S4RZ'c6KPW/KZ.c6KZ.OQKc6] Z6]#[K6a.RTd"RT[e_#ZV
W'_0X[RoOPN.U_0XUTV RZ[YKPXKMY[RZ6fM[YXN.OQ[N6XK#j pZ W'_0X[ROPN.Uo_0X`: @ RZ.c6KPW/KZ.c6KZ'OQK_#Z'c  S4RZ'c6KPW/KZ.c6KZ.OQK
Z6KRT[a.KPX_0XK+^e]ffZ6]#[Y]ffZ.RO=Z.]#X9_#Z[R ^]ffZ6]#[Y]ffZ.RoO=qjXj[Pj9K W"_#Z.MRT]ffZ?]#b& MY[YXKZ6f#[a.KZ.RZ6f,]#X9q+K_0#KZ.RoZ6f
& OP_#ZK_#MRUTV^e_0#K}RT[,Z6]UT]ffZ6f#KPX4RZ'c6KPW/KZ.c6KZ[+brX]ff^ _MYKP[4]#b8UoRT[YKPX_#UM(]#XS0_0XR_0d"UTKM  j
4E KOP_#UU[a'_0[%(  
& Rb._#Z.c]ffZ'UTV?RTbM(fi6r( 3 *Q	4
&  ! 4 ja.RM ^eK_#Z.M[a._0[9[a.K1@ R[%c6KPW/KZ.c6KZ.OQK
]#b & ]ffZm( ]ffZ.UTVR^eW'URTKM\[a6KV@ RT[%c6KPW/KZ.c.KZ.OQK]ffZh_UoRT[YKPX_#U"]#b(`.Z.]#[(_GJ%bN.UU L @ R[%c6KPW/KZ.c6KZ.OQK]ffZ
_#ZVURT[YKPX_#U]#b(4j9p%Z]#[a.KPXq:]#Xc.M`ffRTb*q+K(q(_#Z[8[Y]3Oa6KO}q4a.KP[a6KPX_br]#X^&N.U_ 
& RM @ RT[%c.KPWiKZ'c6KZ[9]ffZ
0z3
4 URT[YKPX _#U.]#b&(4`q:K?Z6KPKc_2Z6]#[R]ffZMY[YX]ffZ6f#KPX([a._#Z @ c6KPW/KZ.c6KZ.OQK#`6OP_#UUTKcbN.UU. B@ c.KPWiKZ'c6KZ.OQK#j
a6KM_#^KOP_#Zhd/KM_#Rcbr]#X S c6KPW/KZ.c6KZ.OQK#j

" >#'$Uyfi $ % L q <q{4L#q{ ; q'  uPv & Pu  !#w xN/* 1w[!#x 

 1( P uI/=PMO
q!m{uw{
Puv ! +(R 0z"t U P u} /PQPuv ![/
fffi 
 & y1brN.UoUTV @ R[%c6KPW/KZ.c6KZ[ !0zk(sy #z"tT!0z. 4y +(9 
ffr( 3 *P	&
U.	&
 & y1brN.UoUTV S=0_ XYc6KPW/KZ.c.KZ[ !0zBU y 0z"t+!0z. 4y .U
Q)(*_

fif.w~3}33f$3ws



& !  /F:XS3/?SlHF:XSPY+y1  /*1 4  yv5O%tuR-6uPz"tuQz"v!0z #  % :XS>+&#z"t  /*1 4 .0w O%tuR-6uPz"tuQz"v
 z3v < q "
!0z #  % S&+ !0'z vwY;vyrz{# 4)1 & y1z!#vI/*r 4  yv5O%tu -6uQzituQz'v !0z #  % S&+

?(UK_0XUTVKZ6]ffN6fffaB`iq4a6KZ.KPS#KPXV& RMbrN.UoUTVL@9RT[%c6KPW/KZ.c6KZ[]ffZ(`/RT[RoM_#UM]bN.UUTVLS=_0XYc6KPW/KZ.c6KZ[
]ffZ U.:(( j 4]	q+KPS#KPX`"[a6K}OQ]ffZS#KPXMKc6]KMZ6]#[4a6]ffUocB`6_#M[a6KW'XKPSR]ffN.M:K*_#^W"UTKMa6]	q4MPj
J a.RUTKhbN.UU, @ c6KPW/KZ.c6KZ.OQKrXKMW j bN.UU, Sc6KPW/KZ.c6KZ.OQK	2OP_#Z d/KOa6KO#Kc RZ URZ.K_0X[R^K
]ffZ.OQK 
ffr ( 3 *P	 &rXKMYW jO U	6	 &4YRMZ6]	q4ZB`,_0[[a6KKZ.c ]#b?[a6KMKOQ[RT]ffZ q:KW'X]S#K[a._0[
c6KP[YKPX^RZ.RZ6f[a6KMYKhMYKP[MRM  .a._0X cB`=_#Z.c [a._0[c6KOPRc.RoZ6fbN.UU+ @ c6KPW/KZ.c6KZ.OQK _#M2q+KUoU:_#M&bN.UU
 S4c.KPWiKZ'c6KZ.OQK	(RM  'OQ]ff^W'UTKP[YK#j
@KP[4N.M,Z6]	q OQ]ffZ.MRc.KPX,[a6K?W'_0X[ROPN'U_0X,OP_#MYK]#bbrN.UoUi @ c6KPW/KZ.c6KZ.OQK3q4a6Km
Z ( ! ( 3 *Q	 & `']#X[a6K
bN.UU( S4c6KPW/KZ.c.KZ.OQKq4a6KZ U ! U	6	 &4 j pbbrN.UoU:c.KPWiKZ'c6KZ.OQKha6]ffUc.M2RZ [a.KMYKOP_#MYKMP`(q:KM_V
[a._0.[ & RoM @9RT[%MR^eW'UR  Kc `ff]#X S=_0XYMRo^W'UR  KcB`XKMWiKOQ[RS#KUTV#j S=_0XYMR^eW'UR  OP_0[R]ffZRoM:_#O a.RTKPS#Kcq4a6KZ
& OQ]ffZ[_#RZ'MZ6] ]OPOPN.XXKZ.OQK]#b_#ZV S;_0XRo_0d'UTKRT[RM S=_0XRZ.c6KPW/KZ.c6KZ[ebrX]ff^hj @9RT[%MR^W"UR  OP_0[RT]ffZ
OQ]#XXKMWi]ffZ'c.M[Y][a6K^]#XKXKM[YXROQ[YKc MRT[N._0[RT]ffZ qa6KPXKh[a6K 4O4]#b & c6]KMeZ6]#[OQ]ffZ[_#RZ _#ZV
]*OPOPN6XXKZ.OQK]#b_eUoRT[YKPX_#UBRT[RM @9RT[%RZ.c.KPWiKZ'c6KZ[:brX]ff^hj
pb'_b1]#X^&N.U
_ & RMZ6]#[ @9RT[%MR^W"UR  KcrXKMYWj S=_0XYMR^W'UR  Kc" `;[a6KZ[a.KPXK:RoM9MY]ff^eK(UoRT[YKPX_#U"rXKMYW j
S0_0XR_0d'UTK	[a._0[\]OPOPN6X MRZ2[a6/K 4O4 ]#b &`ffd'N.[ & RoMZ6]#[ @RT[%c.KPWiKZ'c6KZ[rXKMW j S=_0XYc6KPW/KZ.c6KZ[ 9]ffZ
RT[Pja.RoM\^K_#Z.M+[a._0[([a6KMYVZ[_#OQ[RObr]#X^ RoZeq4a.RoOL
a & RM+K W'XKMMYKcOQ]ffZ[_#RoZ.M(_}UR[YKPX_#U.]#X:S;_0X R_0d'UTK
[a._0[4RoM,RZ.c6KPKchN.MKUTKMMPj

!

 " 'v

<' !mq	L > u3rffvz '  u v & Pu2.M!0w xN/*T  w !0x
 & y1Y@RT[%MRo^W'UR  Kc y 0zitL!#z. 4y /( 3 *P	&ff!.8( 3 *P	&
S _0XYMR^W"UR  KcLy 
 0z"tT#! z. 
4 y .,
U 	.	&/!.U.	&
 & y1 =

q m{uw{


3
%


fiM[a6Khbr]ffUUT]	q4RZ6f K6_#^W'UTKhRUUoN.MY[YX_0[YKMP`\KPS#KPXV b1]#X^&N.U_[a._0[eRoM @RT[%MRo^W'UR  Kc_#UMY]RoMNS=_0XY
MR^eW'UR  Kcd'N6[([a6K}OQ]ffZS#KPXMK}c6]KMZ6]#[a6]ffUochRZ[a6Kf#KZ6KPX _#U OP_#MYK#j



& ! </ :XS%/2SBH :XSP /2 %HOVY(z/uQyvr~*uQwy1  yv5Oy1x.-"y 
:ut&z!#w .0w Oy1x.-"y 
:ut  ~6u
 z3v <q "
u	W/*y100TuPz'v!0wxN/* h  / :XS</LSXH :XSPY?y1 .0w O yrxY-i
y 
:ut PM/6v=yv=y1z!#v  yv5Oy1x.-"y 
(ut +P\yrz"0r 4)1
vr~*uu M*/ y100uQz'v#! w xN*/  h  / :XSPyo P,!#vr~  yv5Oy1x.-"y 
:uth0zit .0w Oy1x.-"y 
(ut 





*Ro^W'UR  Kcbr]#X^}N'U_#Mec6]Z6]#[RZ.OQ]#XW/]#X_0[YK_#ZV N.MYKUTKMMUoRT[YKPX_#UM2]#XeS0_0XR_0d"UTKMPj"fiM[a6KZ6K*[
W.X]#W/]ffMR[RT]ffZ2Ma6]qM\RT[P`ff[a.K4Z6]#[RT]ffZ]#bBMRo^W'UR  Kcbr]#X^&N.U_?_#OQ[N._#UUV&RM=[a6K,W/]ffRZ[qa6KPXKMYV*Z[_#OQ[RoOP_#U

RZ.c.KPWiKZ'c6KZ.OQK3_#Z.cMYK^e_#Z[ROP_#U1,RZ.c6KPW/KZ.c6KZ'OQKOQ]ffRZ'OPRc6K#j


=<

ru uwuw{



 uv &P u}.M!0wx /*T  w !0x


3
%



 & 1y   yrv5O yrx.-"y 
:uthy &0zit !0z. 4y 2vr~*u M!0r !0|yrz{ u	W/yr;#TuPz u}~G!0 t0 M!0weuQ0uPwI4c(
& y1?I40z'vIvy5 01 4  yv5Oyrz"tu -6uQz"tuQz'v w[!0x (sy &0z"t+!0z. 4y /(  4 &~G!0 t0



 & 1y  .#wIO yrx.-"
y 
:uthy }0z"t !0z. 4y vr~*u !#1 !0|yrz{u M/*y100uQzu}~ff!0Tt; M !0wuQ0uPwI4 U
& y1?I40z'vIvy5 01 4 .0w Oyrz"tu -6uQz"tuQz'v w[!0x U y 0z"tT!0z' 4y  U  4 , & ff~ !0Tt;



.

(  1

fiE1

aN.MP`Bqa.RUTK&_b1]#X^&N.U_OP_#ZLK_#MRoUTVd/K @RT[%RZ'c6KPW/KZ.c6KZ[4brX]ff^ _hMYKP[?]#b+UR[YKPX_#UMq4RT[a6]ffN.[d/KQ
R Z6fMYV*Z[_#OQ[RoOP_#UUTV @9RT[%RZ.c.KPWiKZ'c6KZ[b1X]ff^ RT[P` MR^W"UR  OP_0[RT]ffZRM3_q:_V[Y] FY]ffRZ @RT[%RoZ.c6KPW/KZ.c6KZ.OQK
q4RT[a R[MeMYV*Z[_#OQ[ROP_#U4XKMY[YXRoOQ[RT]ffZ rq4a.RoOa RMK_#MRTKPXe[Y]f#X_#MW `,_#Z.c _#Meq:Kq4RUoU:MKPKMY]]ffZB`+K_#MRTKPX

Q)(^\

fi

~33



s~$w

~ww$

[Y]hOa6KORoZ[a6K2f#KZ6KPX _#UOP_#MK	IC @9RT[%RZ.c6KPW/KZ.c.KZ.OQK_#Z.c MYV*Z[_#OQ[RoOP_#U @RT[%RZ'c6KPW/KZ.c6KZ.OQKOQ]ffRoZ.OPRc6K
]ffZ @9RT[%MR^W"UR  Kc b1]#X ^}N.Uo_#MPj a6KM_#^eKa6]ffUc.Mbr]#X S=_0XYRoZ.c6KPW/KZ.c6KZ.OQKL_#Z.c MYVZ[_#OQ[ROP_#U S=_0XY
RZ.c.KPWiKZ'c6KZ.OQK#j
a.KeMY[YXKZ6f#[aL]#b([a6KZ6]#[RT]ffZ ]#b,MR^W"UR  OP_0[RT]ffZURTKM?RoZ [a6Keb_#OQ[[a._0[KPS#KPXV br]#X^&N.U_hOP_#Zd/K
MR^eW'UR  KcW.XKMYKPXSRZ.fRT[M?^e]c6KUoMPja.RoMRoM?N.MYKPbN.Um`B_#M?MR^W"UR  Kcb1]#X^&N.U_#M?OP_#Z d/KMa6]#X[YKPX_#Z.c
K_#MRTKPX4[Y]N.Z.c6KPX MY[_#Z.c[a._#Zhbr]#X^&N.U_#M,OQ]ffZ[_#RZ.RZ.feN.MYKUTKMM4URT[YKPX_#UMj

 P!0wuQ0uPwI4

=<



ru uwuw{

&  
fi

&w[!0x7

 
1 vr~*uPwYuu32ffyoQvr  yv5Oy1x.-"y 
:ut !0wxN/*  
  ov 

*RoZ.OQK @R[%MR^W'UoR  Kc5 A(M4_0XK2_#UMY] S=_0XYMR^W"UR  KcB`6[a'RMW.X]#Wi]ffMRT[RT]ffZh_#UMY]Ma6]	q4M4[a'_0[4KPS#KPXV
 A OP_#Zd/KV=
S _0XYMR^W'UR  KcqRT[a6]ffN6[^]*c.RTbrV*RZ6f2RT[M,MYKP[]#b^]*c6KUMPj

%p Z[YKPXKMY[RZ6fffUTV#`di]#[a @ RZ.c6KPW/KZ.c.KZ.OQK&_#Z'c S4RZ'c6KPW/KZ.c6KZ.OQKOP_#ZLd/KeOa'_0X_#OQ[YKPXRTgPKcq4RT[a*
]ffN6[OQ]ffZ'MRc6KPXRoZ6f [a6KOQ]#XXKMWi]ffZ'c.RZ6fLMYV*Z[_#OQ[RoOZ6]#[RT]ffZ.Me]#b3RoZ.c6KPW/KZ.c6KZ.OQK#` FYN'MY[dV OQ]ff^W"_0XRZ6f
br]#X^}N'U_#M(]#d.[_#RZ6KcdVMYKP[Y[RZ6f[a6K?[YXN6[ahS0_#UN6K?]#bUR[YKPX_#UM:q+Kq:_#Z[[Y]O a6KOh[a6K}c6KPW/KZ.c6KZ.OQK#j
 uPv& P u? M!0wx /*T w[!#x
vIffvuQxuQz'vr20wYuu	M/*yr;0uQz"v 
JK  4 &
JWK &5j h ( A ! T
& jh /
J	MK &OA ! 5
& jh /
Jff
K 5
& j h ( A ! &fi
=<



ru uwuw{




3
  0z"t3 P ?u 2yrvuPw%0 ![5(   ~*u?ziu32vG!/w





a.K_0d/]	S#KhW.X]#W/KPX[RTKM2OP_#Z d/KN.MYKc [Y]Oa.KO q4a6KP[a6KPXe_ br]#X^&N.U_RMN@RT[%c6KPW/KZ.c.KZ[&]ffZ _
URT[YKPX _#Um`6_#M[a6K?br]ffUUT]	q4RZ6f2K6_#^W'UTKMa.]q4MPj
  /GX: S /  S/HGX: SPYey1  yrv5Oy1zituR-6uPz"tuPz'v w[!0x S yrz uwYu -" QPyrz{ S P4
* f 
z3v <q  &!
|yvr~y1z & {ffy1#uwyouv	! 0z yrzI!0z6 y1vuPz'v M!0wxN/T   !0z"vw%;Qvy1z{# 4"1 & y1  yrv5OYtu -6uQz"tuQz'v !#z :XS
 yrz u?wuR-"Ty1z{S PM4 :)|yvr~yrzB& {#yr0uQ & h / .1|8~y0Q~y13z !#v+#v\Tu;v(;? !{#y00r 4evw !0z{
;vr~*u}yrz I!0z6 y1vuPz'v !#w xN/* T! PPvI0yrziut PM4wuR-"Ty1z{SLP4* f |yvr~yrz &fi



fi MRo^eRU_0XW.X]#Wi]ffMRT[RT]ffZa6]ffUc'M=br]#X+ 4
S RoZ.c6KPW/KZ.c6KZ.OQK#`Oa._0X_#OQ[YKPX RTgRZ6f}[a6K4S0_0XR_0d'UKM\_br]#X^&N.U_
& c6KPW/KZ.c'M+]ffZ `'N.MRZ.f&[a.K?b1]#X^&N.U_#M &'gih / _#Z.cB&'gih ( j
 ru=
< uwuw{   uPv & P u M0! wxN/T w[!0x
3
% #z"tb> Pu00wyQPPTu+! 
fi  ~*uziu,2ffv
M!D/*w?vI#vuPxuQz'vr&#wYueu M/*yr;0uQz'v 
J K >  4 ,. &
J WK '& gih /  '& gih (
	J MK &  '& gih /
ffJ 
K &  '& gih ( 







fiM\RZe[a6KOP_#MYK]#bBURT[YKPX_#U.c.KPWiKZ'c6KZ.OQK#`ff[a6K_0di]	S#KW'X]#W/KPX[V2OP_#Zd/KN.MKce[Y] 

_br]#X^&N.U_2RM S=_0XYc.KPWiKZ'c6KZ[,]ffZ_S;_0X R_0d'UTK#j


z3v

/ S8H
<q &"!  

Z.c2]ffN6[\qa6KP[a6KPX

:XSPYy1 ' 0wIOyrz"tuR-.uQz"tuPz'vw[!#x!Sy1z  u3|+u3~0#uY& h /  & h ( . 
Q)(+Q

fif.w~3}33f$3ws

p%Z[YKPXKMY[RZ6fffUTV#`& @BRZ.c.KPWiKZ'c6KZ.OQK_#Z.cs S4RoZ.c6KPW/KZ.c6KZ.OQK OP_#ZsdiK c6KP[YKPX^eRoZ6Kc RZs_#ZsKP
OPRTKZ[eq(_V q4a6KZ & RMfffRTS#KZ RZ MY]ff^eKMYW/KOPR  OhZ6]#X ^e_#U,br]#X^eMP`(Z._#^KUTV#`:W.XRo^KR^W'UoROP_0[YKhZ6]#XY
^e_#Ubr]#X^ ]#X}W.XR^K2R^W"UROP_#Z[3Z.]#X^e_#U8br]#X^hje6]#X&MN.OaZ6]#X^e_#U8br]#X^eMP`@RT[%RZ'c6KPW/KZ.c6KZ.OQK2_#Z.c
S=_0XYRZ'c6KPW/KZ.c6KZ.OQK?OQ]ff^Kc.]q4Zh[Y][a6KRTX4OQ]#XXKMYW/]ffZ.c.RZ6f2MYV*Z[_#OQ[RoOP_#U br]#X^eMPj
Pu !0wxN/* Nw !0x7
3
% 0z"tm(:P uhI/=PuPvV![ff(R ~*uziu,2ffv
ru=<uwuw{   uPv &
vIffvuQxuQz'vr20wYuu	M/*yr;0uQz"v 




JK(  4 &
JW
K 
fi	"	&T U# 19AB1 y1vuQwx vr~#v,t!	uz!ffv I!0z'vI0yrz0z4
yvuQwY0Qw !0x2( +
J	MK 	 
	
& T # 0Ar0y1}L /u}vr~#v,t!uQz!#v ,!0z"vI0y1z#z4yrvuPw%# w[!#x ( + 



v & Pu !#w xN/*   w !0x 
3
% 0z"t U Pu  /PQPuv ![ff
fffi  ~*uziu,2ffv
ru=<uwuw{   uPY
vIffvuQxuQz'vr20wYuu	M/*yr;0uQz"v 




JK U  4 , &
JWK
fi	"	&. T U# 19AB1 y1vuQwx vr~#v,t!	uz!ffv I!0z'vI0yrz0z4;0wy PQu  w !0x U +
J	MK 	 
	
& T # 0Ar0y1}L /u}vr~#v,t!uQz!#v ,!0z"vI0y1z#z400w ymQPPTu w !0x U +



 z3v <q k  uPv& ! 0/D:XS /SHD:XSPY u,~0#u-
 	"	&ff! #  /:XSP +&0z"t 	 
	&ff!$#  % :XS>+
J	/*- 	v ! !{#y00=u	W/*y100TuPz u[K u 0z u; yr 4T!QPQPuQw0uvr~#v & yo  yrv5Oy1zituR-6uPz"tuPz'v  w !0xQS !W! ;y1z{
#v/
 "	 	& 'J 0! w 	 
	&  K  u 0z 0 M! u; yr 4 QvI#vuvr~#v.& y1  yrv5OYtuR-.uQz"tuPz'v !0z :XS0z"t '0wIO
yrz"tuR.- uQz"tuPz'v  w 0! x VLPM4%,0! z. ymtuQwy1z{0z4+![vr~*uQPu}z!0wxe0ff!0wx&



\X]#W/]ffMRT[RT]ffZ Ma6]qM[a._0[&_  AOP_#Z diKK_#MRUTV S=_0XYMR^eW'UR  Kc RmjK#jT`8RZLW/]ffUTV*Z6]ff^eRo_#U8[R^K	
#_ MMY]]ffZ_#M[a6KS0_0XR_0d'UKMRT[4RMYS=_0XYRZ.c.KPWiKZ'c6KZ[4b1X]ff^ a._S#K&d/KPKZc6KP[YKPX ^eRZ6KcBj(p%Z.c6KPKcB`'q+K2OP_#Z
K_#MRUV}c6KMRfffZ_?f#XKPKc6V_#UTf#]#XR[a.^b1]#X+MR^W'UoRTb1V*RZ6f A(MPja.RM8_#UTf#]#XR[a.^OQ]ffZ.MRM[M=RZOQ]ffZ.MRoc6KPXRZ6f
KPS#KPXVS;_0XRo_0d'UT
K >L]#b U	.	 &4RZ_MN.OPOQKMMRTS#K2q:_V#`Bq4a.RUKXKPW'Uo_#OPRZ6+
f & d'
V & gih / q4a6KZ6KPS#KPX & RM
S=_0XYRZ'c6KPW/KZ.c6KZ[:brX]ff^ >j\a'RM,_#UTf#]#XRT[a'^ XN.Z.M,RoZ[R^KW/]ffUTV*Z6]ff^eR_#U/RZ[a.KMRTgPK?]#b & ]ffZ.OQK}[a6K
S0_0XR_0d'UTKM & RM S=_0XRZ.c6KPW/KZ.c6KZ[,brX]ff^ a._S#K}d/KPKZOQ]ff^W"N6[YKcBj fi([4[a6KKZ'cB`.[a6KXKMN.UT[RZ.f  A RM
S=_0XYMRo^W'UR  KcBj
@9RT[%MR^W"URTbrVRZ.f_  ARM?Z6]#[}MY]hK_#MYVLRTb(Z6]_#MMN.^eW.[RT]ffZ.M3_0XK^_#c6K_0d/]ffN6[RT[M?MYV*Z[_ i`8c.N6K
[Y][a6K&br_#OQ[3[a._0[?URT[YKPX _#UM4MRfffZ.M_0XK&OQXN'OPR_#U9a.KPXK#j3pZ.c.KPKcB`iU]]#*RZ6f]ffZ.UTV_0[3[a6K&]OPOPN6XXKZ.OQK&]#b+_
S0_0XR_0d'UTK(RZ.MRc6K\_3br]#X^&N.U_RM8Z6]#[MN6OPRTKZ[8[Y]MY[_0[YK,q4a6KP[a.KPX]#X=Z.]#[[a'RM8RM8_3Wi]ffMRT[RTS#K:]OPOPN6XXKZ.OQK
r]#X=_?Z6KPfff_0[RTS#K,]ffZ.K	 j=6]#X[N.Z._0[YKUV#`ff[N6XZ.RoZ6f_3br]#X^&N.U_RZ[Y]?RT[XM 4O4RMOQ]ff^eW'N6[_0[RT]ffZ._#UoUTVK_#MYV_#M
UT]ffZ6f_#M8RT[9c6]KM8Z6]#[8OQ]ffZ[_#RZ2_#ZV]*OPOPN6XXKZ.OQK:]#b.OQ]ffZ.Z6KOQ[RS#KMURT#K  ]#X _#Z.c}R[W.X]	S#KM8MN6OPRTKZ[
[Y]c6KMRTfffZ_?f#XKPKc6V_#UTf#]#X RT[a.^br]#X @RT[%MR^W'URb1V*RZ6f4
_  A q4a6KZ2[a6K4UoRT[YKPX_#UM8RT[=RM @9RT[%RZ.c.KPWiKZ'c6KZ[
brX]ff^ a._S#Kd/KPKZRc6KZ[R  Kc jpZ.c.KPKcB`.q4RT[a'RZ_#k
Z 4?4 br]#X^&N.U_*`6KPS#KPXVUR[YKPX_#UBOP_#Zd/KOQ]ffZ.MRoc6KPXKc
K_#MRUV_#M_#Z_0[Y]ff^eRO}]#Gd FYKOQ[Pj?a.RM_#UTf#]#XRT[a.^ OQ]ffZ.MRM[MRoZOQ]ffZ.MRoc6KPXRZ6fKPS#KPXVURT[YKPX _#&U =]#*b ( 3 *P	 &
RZ _MN.OPOQKMMRTS#K2q:_V#` q4a.RUKXKPW'Uo_#OPRZ6fKPS#KPXV]*OPOPN6XXKZ.OQK2]#*b \RoZ & d
V   "2OQ]ffZ.MRc6KPXRZ.fKPS#KPXV
URT[YKPX _#UB]#-b ( 3 *P	 &_#M_#Z_0[Y]ff^hj [_0[YKc]#[a6KPXqRMYK#`'qa6KZ =RoM_W/]ffMR[RTS#KrXKMYW jZ.KPfff_0[RTS#K	UoRT[YKPX_#U
> rXKMYW jF%: >/ `"XKPW'U_#OPRoZ6f 8dk
V   "c6]KMZ6]#[^eK_#ZXKPW'Uo_#OPRZ6f %: > rXKMYW 'j >/(dV*    C+]ffZ.UTV[a6K
]*OPOPN6XXKZ.OQKM:]#&b q4RT[a[a6KXRTfffa[+MRTfffZ_0XK?OQ]ffZ.MRoc6KPXKcBja.RoM\_#UTf#]#XR[a.^ X N.Z.M\RoZ[R^eKW/]ffUTV*Z6]ff^eR_#U
RZL[a.KMRTgPKe]#Yb &]ffZ.OQKe[a6KURT[YKPX_#UM & RM @9RT[%RZ.c6KPW/KZ.c.KZ[b1X]ff^ a._S#Kd/KPKZOQ]ff^W'N.[YKcBj fi([}[a6K
KZ.cB`6[a.KXKMN.U[RZ6f  A RM @R[%MR^W'UoR  KcBj

Q)(

fi



~33

HBlIHKJ

uv

<mq 



s~$w

~ww$

[q

J a.RUTKMYV*Z[_#OQ[ROP_#U( @ _#Z.c  4S c.KPWiKZ'c6KZ.OQKOP_#Z d/KK_#MRUTVO a6KO#Kc oR ZURZ.K_0X}[R^eKRZ [a6K
MRTgPK}]#b=[a6K&RZ6W'N6[P`'[a.RMRMb_0XbrX]ff^ /d KRZ6feK*W/KOQ[YKcbr]#XeMYK^e_#Z[ROP_#U14 @ 6c KPW/KZ.c6KZ.OQK}_#Z.c S
c6KPW/KZ.c6KZ'OQK3RoZ[a6Kf#KZ6KPX_#UOP_#MYK C

" ; <   =>$ fi$ % L ff< ML ; ('	   ) 	 )ff
  1 fi	   )
ff )ff
  1fffi 'ff 	   ) 	  )ff
  0zit fi 'ff fiff   ) ff )	
  0wYu  O[I!0x.-"uPvu 
=<



ru uwuw{

k



uv

mq



u

)y

#q q{ q{

q

0

 aN.MP`,_#UT[a.]ffN6fffa [a6KPV UT]]# MR^eW'UTK#`\[a.KW.X]#d"UTK^eM]#bc6KP[YKPX ^eRZ.RZ.fqa6KP[a6KPX_b1]#X ^}N.Uo_RM
RZ.c.KPWiKZ'c6KZ[b1X]ff^ _hURT[YKPX_#U]#X_S0_0XR_0d"UTK&_0XKe_#Ma._0XcL_#MO a6KO*RZ6fhW'X]#W/]ffMRT[RT]ffZ._#UKZ[_#RU^KZ[Pj
p%Z[YKPXKM[RZ6fffUTV#` [a.KeOQ]ff^W'UTK 6RT[V]#b+d/]#[ac.KOPRMRT]ffZW.X]#d"UTK^eMbr_#UoUc6]q4Z [Y] q4a6KZ.KPS#KPXOa6KORoZ6f
RZ"c.KPWiKZ'c6KZ.OQK?d/KOQ]ff^KM[YX _#OQ[_0d'UTK#j fiW'_0X[brX]ff^ [a.KOP_#MYK&]#b8MVZ[_#OQ[ROP_#URZ.c6KPW/KZ.c6KZ'OQK#`.MY]ff^K
]#[a6KPXXKMY[YXROQ[RT]ffZ'M]ffZ &s^_0#KMRZ"c6KPW/KZ.c.KZ.OQK}[YKM[_0d'UTKeRZW/]ffUTV*Z6]ff^eR_#U[R^K#j2\MYW/KOPR_#UUV#`"q:K
f#KPM[ C

ru=
< uwuw{ k'k h~*uQziuP0uPw & P uP 0! z{0ev	!'PT;  ![ NOP M!0w xN/*T;vr~#vyovwYvIQPPTu.M!0w
P TD/Q# W/.uPwI420z6|+uQwy1z{BJIy ru  18vr~*uQwuu,2#y1vrY-!0 4#vyrxu0{Q!0wyvr~x v	!tuPvuPw x2yrziu3|8~6uPvr~*uPw &OA !=1
M!0w&0z=4NOP M!0wxN/Tff1K0z"tQvIQPQu M!0w00wyQPPTuy1z.vI0z'vym#vy0!0z Jy u I1+wuR-"Ty1z{hy1z & ) 0z4
;#w ymQQP u PM2
4 [YXN6K !0w P42br_#UoMYKe{#yr0u2!#w xN/* vr~#v(Qvy1r PuQ !0z{#&v	! Kvr~6uQz ff   ) 	 )ff
  1

)
fiff  ff 	) 
  1 fi '  ff   ) ff )ff
  0z"t fi ' fi	   ) 	 )ff
  0wu}yrz  


%p Z W"_0X[ROPN.U_0X`,q4a6KZ(& RMXKMY[YX ROQ[YKc [Y] _ XKZ._#^_0d'UTK]#XZ ?54 br]#X^&N.U_]#X[Y] d'RZ'_0XV
OPU_#N.MKM?X]ff^ br]#X^&N.U_ `._#UU b1]ffN6Xc.KOPRMRT]ffZW.X]#d'UTK^eM,_0d/]S#KdiKU]ffZ6f[Y] 9j
JLKa._S#K_#UMY]RZS#KMY[RTfff_0[YKc[a6K4OQ]ff^W'UK*RT[V2]#biO a6KO*RZ6f}q4a6KP[a6KPX\_b1]#X^&N.U_3RM @RT[%MRo^W'UR  Kc
_#Z.T
c S=_0XMR^W'UoR  Kc"I C

0z"t fi  2 0 !    fi  fi 132  '#wYu  O[I!0x.-"uPvu 
fiUU8[a6KMYKOQ]ff^W'UTK6RT[V XKMN.UT[M?a._S#KMY]ff^KR^W'_#OQ[?]ffZL[a.K_0W.W.X]ff_#O a6KM[a'_0[KW"UROPRT[UTVZ6KPKc
OQ]ff^W'N.[RZ6f 
ffU,.	&:_#M4_W.XKPW.X]OQKMMRZ6f2[_#MYiRj 4_#^eKUTV#`.q:Ka._S#K}[a6K?br]ffUUT]qRZ6f2XKMN.UM[ C
=<

kff7

=<

k	H



ru uwuw{



ru uwuw{

 #(0 !    fi  fi 132 ' 

  2uvuQwx&yrz.yrz{h|8~6uPvr~*uPw
ffr( 3 P* 	&

! ( JI|8~*uPwY
u ( yoPuv ![&
yvuQwY0
 K 130z"ttuPvuQwx2y1z'y1z{
|8~6uPvr~*uPwU.	& !=A JI|8~*uPwYu?A y1}Puv ![;0wy PQu Key1 " - O[I!0x.-"uPvu 
  ~6u&Pu0w Q~ -"w !QPPTuPx ,!0z6yoQvy1z{hyrzOQ]ff^W'N6[RZ.f
r( 3 *P	&%JIwu	-.u3vyr0uP 4 U	.	& Ky1
yrz   - 0z"ty1 P3!#vr~  O~0wYt0zit O~0wYt 

BH l   ; #u{
a6KW.XKPS*RT]ffN.M&Oa._0X _#OQ[YKPXRTg_0[RT]ffZ.M_#Z'c OQ]ff^W"UTK*R[VLXKMN'UT[M}UK_#c[Y]LMYKPS#KPX_#U "N6KMY[RT]ffZ'MMCq4a6KZ RM
RT[q+]#X[aq4a'RUTK[Y]W.XKPW'X]OQKMM_Z.]q4UTKc.f#K}d'_#MK}dVOQ]ff^W'N.[RZ6fRoZ.c6KPW/KZ.c6KZ.OQKXKU_0[RT]ffZ.M ) 4]	q
Ma6]ffN'Uc [a6KMYKRZ.c6KPW/KZ.c6KZ'OQK&XKU_0[RT]ffZ.M?d/KOQ]ff^eW'N6[YKc ) J a._0[}RoM?[a6KUKPS#KU=]#b(f#KZ6KPX_#UR[V ]#b,[a6K
c6K  Z'RT[RT]ffZ.M(_#Z.chXKMN.UT[M(q:Kfff_S#K&RZ[a.RM4MYKOQ[RT]ffZ )

Q)(i

fif.w~3}33f$3ws

_#ZVWK "N.RTS0_#UTKZ[,O a._0X_#OQ[YKPXRTg_0[R]ffZ.M]#bb1]#X ^}N.Uo_;IS;_0XRo_0d'UTK3RZ.c6KPW/KZ.c6KZ'OQKa._S#Kd/KPKZfffRTS#KZhRZ
[a6K,URT[YKPX _0[N6XK#`0K_#Oa2]#b"q4a.RO a}OQ]ffN.Uoc}MYKPXS#K(_#M_3c6K  Z.RT[RT]ffZK  Z.RT[RT]ffZ<6` ?:]#X]ffUUo_0XV 0`ffK_#Oa2]#b"[a6K
MY[_0[YK^KZ[M3; ff `i  8_#Z.c <RZ =X]#W/]ffMR[RT]ffZ _#Z.c2]#b"[a.KMY[_0[YK^KZ[M;ff `/  8RZ =X]#Wi]ffMRT[RT]ffZff `
MY]e]ffZ.K^e_Vq:]ffZ.c6KPXq4a'ROa]ffZ6K}a._#M,[Y]ed/KN.MYKchRoZhW.X_#OQ[ROQK#j
p%Z^_#ZVW"_0WiKPX M4XKPbrKPXXRoZ6feK *W'UROPRT[UV[Y]br]#X^}N'U_;IS;_0X R_0d'UTKRoZ.c6KPW/KZ.c6KZ.OQK#`'[a6K}W'XR^K&R^W'URT
OP_#ZI[ H;OP_0[YK2O a._0X_#OQ[YKPXRg_0[RT]ffZ 5 \X]#W/]ffMRT[RT]ffZffRM,N.MYKc_#M4_c.K  Z.RT[RT]ffZL5 A:]ffN6[RUoRTKPX` # <6 ]ffa.KPX[V
KP[?_#UmjT` # ff j 
KZ6KPX_#UUVMYW/K_0RZ.f6`"[a.RoMRMZ6]#[[a6K2Oa6K_0W/KMY[q:_V[Y]hOQ]ff^W'N.[YK}[a6K2MYKP[]#b=S0_0XR 
_0d'UTKM?_br]#X^&N.U_c6KPW/KZ.c.M]ffZB`MRZ.OQK&[a6KeMRTgPK2]#5b 
fi	"	 &RoMK *W/]ffZ6KZ[R_#URZ[a6KeMRTgPK2]#b &sRZ[a6K
q:]#XMY[OP_#MYK#ja.RM?O a._0X_#OQ[YKPXRg_0[RT]ffZ RoM3[Y]d/KeN.MYKcLRZW.X_#OQ[ROQKe]ffZ'UTVRTb:[a6KeMYV*Z[_#OQ[ROP_#U\br]#X^ ]#b
& RMMN.Oa [a._0[RT[MW.XR^KhR^eW'UROP_#Z[M&]#XW.XRo^KhR^W'URoOP_0[YKMOP_#Z d/KOQ]ff^W'N6[YKc K_#MRUTV b1X]ff^ RT[
r[a.RM4RM,[a6KOP_#MYK}b1]#XRZ.MY[_#Z.OQKq4a6KZ6KPS#KPX & RM,_ ?X]ff^ b1]#X ^}N.Uo_ 1j ?(UTK_0XUV#`6[a6KOa.K_0WiKM[4q(_V[Y]
OQ]ff^W'N.[YKb1]#X ^}N.Uo_;IS;_0XRo_0d'UTK,RZ.c6KPW/KZ.c.KZ.OQK,OQ]ffZ.MRMY[M+RZN.MRoZ6f_#ZV2]#b/[a6KWK "N.RTS0_#UTKZ[b1]#X ^}N.Uo_0[RT]ffZ.M
]#b =X]#W/]ffMR[RT]ffZ `.q4a.RO ah_#UUBOQ]ffZ.MRoMY[,]#b8S;_#URoc.RT[V[YKMY[MPj
?(a.KO*RZ6feq4a.KP[a6KPX4_ebr]#X^&N.U_ & RM S=_0XYRZ'c6KPW/KZ.c6KZ[,b1X]ff^ _S0_0XR_0d'UT3
K > RM 'OQ]ff^W'UKP[YK#`
q4a.RoOaR^W"URTKM[a._0[+MR^W'URb1V*RZ6f4_Z.]q4UTKc.f#K,d'_#MYKdV2f#KP[Y[RZ6fXRoc2]#biXKc.N.Z.c._#Z[S0_0XR_0d'UTKMZ.KPKc.M
A U.	 &AffOP_#UUM=[Y]2_#Z  ]#X_#OPUK#`*_#Z'cRM\[aN.M+RZ   - _#Z.cZ6]#[\d/KUT]	qsN.Z.UKMM   !  B j6a.RM
^e_VUT]]# W'_0X _#c6W] 6ROP_#U3_#Z.cMY]ff^eKP[R^KM&N.MYKUTKMM[Y]W.XKUR^eRZ'_0XRUTVOQ]ff^W"N6[YKMYKPS#KPX_#U:RZ.M[_#Z.OQKM
]#b4_   ]#ff
X  .a'_0Xc RZ.c6KPW/KZ.c.KZ.OQKeW.X]#d"UTK^ [Y]La6KUW MY]ffUSRZ.f_ MRZ6fffUK	?RZ.M[_#Z.OQK]#b_ 
]#X  .OQ]ff^eW'UTKP[YKLW.X]#d"UTK^hjC4]	q+KPS#KPX`[a'RMZ.KPfff_0[RTS#KOQ]ff^e^KZ[a._#M_ f#KZ6KPX_#U?MOQ]#W/K]ffZ.UTV#`
_#Z.c RZ^e_#ZVLW'_0X[ROPN'U_0XOP_#MYKMP`[a.RoMOP_#ZW'X]S#L
K "N.R[YKeKPOPRTKZ[RZ'c6KPKcB`KPS#KZ q4a6K#
Z &a._#M}Z6]
W'_0X[RoOPN.U_0X?MYV*Z[_#OQ[ROP_#U8br]#X^h`B[a.KeM_0[RM  _0d"RURT[Vh]#X?[a6KN.Z.M_0[RM  _0d'RUR[V]#b &5gih / / : &'gih ( ^e_V
d/K4W'_0X[ROPN.U_0XUV}K_#MYV[Y]&O a6KO. j+'N6X[a6KPX^e]#XK#`RbB[a6K*Z6]	q4UTKc6f#Kd'_#MYK & RM+[Y]2d/.
K "N6KPXRTKc^e_#ZV
[R^KM`4[a6KZ [a6KW.XKPW.X]OQKMMRZ6fW"a._#MYK OQ]ffZ.MRMY[RoZ6f RZ S=_0XYMRo^W'URTbrV*RZ6X
f & dV RTfffZ6]#XRoZ6f N.MKUTKMM
S0_0XR_0d'UTKM,RoM,URT#KUTV[Y]ed/K?q+]#X[aq4a.RoUTK#j

=A E ffffH D 
p%Z[a.RM(MYKOQ[RT]ffZB`6q:Kc6K  Z.Kqa._0[(b1]#Xf#KP[Y[RoZ6feRMP`*W.XKMKZ[MY]ff^eK]#b8RT[M(W.X]#WiKPX[RTKMP`*_#Z.c  Z._#UUVfffRTS#K
MY]ff^K}OQ]ff^W'UTK 6RT[VXKMN'UT[MPj

 lmk

!

4L  ru<qsrq
fi d'_#MRO:q(_V[Y]MR^W'URb1V_  A qjXj[Pjff_MYKP[\]#b/UoRT[YKPX_#UM8]#X\_MYKP[=]#b/S;_0X R_0d'UTKMOQ]ffZ.MRMY[M=RZM!0wm{uvmvy1z{
URT[YKPX _#UMIH	S0_0XR_0d'UTKMRZ&RT[Pj A+KPV#]ffZ'c[a6K,MR^W"UR  OP_0[RT]ffZ[_#MYi`ffb1]#Xf#KP[Y[RZ6f?RM8_3q:_V}[Y]?^e_0#K_3br]#X^&N.U_
RZ.c.KPWiKZ'c6KZ[:brX]ff^ URT[YKPX _#UIM H	S0_0XR_0d'UTKMj @KP[4N.M  X MY[,MY[_0X[q4R[ahURT[YKPX_#U/br]#Xf#KP[Y[RZ6f C
q m{uw{#

z3{

 " 'qrz31> uwr|wqf'{| '  u v1&:P u M!0w xN/*T  w[!#x 

 #z"t ( P uI/=PuPv
![+(*  &'))(%*( 3 *P	& % ((?y1vr~*u1M!0w xN/*Ty1z"t/vy1#uQ 4tu;
=ziut0 !#1 !0|8
 &'")(%+* ( 3 *P	& % ff ! &1
 &'")(%+* ( 3 *P	& % #i +;7! &Tj h ( HL :Y/ & 1
&'")(%+* ( 3 *P	& % #i + . ((/!.&'))(%*( 3 *P &'))(%*( 3 *Q	& % (( % #i +;

!

q m{uw{

Q)(76

fi

~33



s~$w

~ww$

a'RM}c.K  Z.RT[RT]ffZ RM2MY]ffN.Z'c MRZ.OQK[a6K]#Xc6KPXRoZ6fRoZq4a'ROa URT[YKPX _#UM&]#b?( _0XKhOQ]ffZ.MRc6KPXKc c6]KM
Z6]#[4^_0[Y[YKPX 7 j\JLK&OP_#Z_#UMY]W.X]S#K[a._0[[a6Kc.K  Z.RT[RT]ffZh_0d/]	S#K}RM(KW"N.RTS0_#UTKZ[,[Y][a6K]ffZ.KRZq4a.RoOa
]ffRZ
[ j\RM(XKPW"U_#OQKchdV

&'))(%*( 3 *P	& % #i +;1! &5j h ( HL :X2/B&5j h / 
FN.MY[,d/KOP_#N.MYK & _#Z'c &5j h / _0XK?WK "N.RS;_#UTKZ[^]*c.N.UT]:Yj
@KP[(N'M+Z6]	q fffRTS#K?_&MK^e_#Z[RoOP_#UiO a._0X_#OQ[YKPXRg_0[RT]ffZ]#b9UR[YKPX_#U.br]#Xf#KP[Y[RZ6f6j\pb&( !$#i +ff`*[a._0[,RMP`B(
RMOQ]ff^W/]ffMYKc]#b\_MRZ6fffUTKUoRT[YKPX_#Um`"[a6KZb1]#Xf#KP[Y[RoZ6b
f =b1X]ff^ _b1]#X ^}N.UoL
_ & _#^]ffN.Z[M[Y]RZ[YX]*c.N.OPRZ6f
[a6K^e]c6KU &'"lV>= % X: m(b1]#XK_#O a^e]c6KU = ]#b & MN'Oah[a._0[D=9A ! j
 ru=<uwuw{ k 
~*uuPv ![x !tuP  ! D&'))(%* ( 3 *P	 & % #i +;N
 #z P u23u 2 -iwYuQPut; 
d f' e" &'))(%+* ( 3 *Q	 & % #i +;Y ! d f' e"	 & . # &')l V >= % Y: I A2=;A ! & +
! #+= A &')f V>= % mDA ! &+
fi MR^eRUo_0X:MY[_0[YK^eKZ[4OP_#Zd/KfffRTS#KZbr]#X,[a6K?OP_#MYKRZq4a.RO )
a ( RM(OQ]ff^W/]ffMYKc]#b9^e]#XK3[a._#Z]ffZ6K
URT[YKPX _#Umj8pZh[a'RMOP_#MYK#`.br]#XK_#O a^]*c6KU = ]#b &`.q:KOP_#Zbr]#XOQK_#ZVhMN6d'MKP[,]#bUR[YKPX_#UTM ( ( 9( MN.O a
[a._0[F=;A ! ( ( [Y]_#MMN.^K?[a6KS0_#UN6K3b_#UMYK#j
=<



ru uwuw{

k

~*uuPv ![x !tuP  ! D&'))(%*( 3 *P	& % (
(   0z Pu2u32-"wuut0

d 'ei &'))(%*( 3 P* 	& % ((Y

A &'"lV>= % ( ( FA ! & |8~*uQwu6( ( 9(
fiM_eOQ]#X]ffUoU_0XV#`.q:K]#d.[_#RZ[a.Kb1]ffUoUT]q4RoZ6f2W.X]#W/KPX[RTKM(]#b8URT[YKPX_#U/br]#Xf#KP[Y[RZ.fC
J

uwruz$r

 7

!$#+=

 uv &1 
 P u1!0wxN/* ;1 w !0x


3
% #z"tff( ( % ( - 9(R

+



! &'))(%*( 3 *P	& % ( ( 3~ff!# t; 
 & A.
& A ! 
~ff!0Tt; 1(vr~*uQz &'))(%*( 3 *Q	& % ( ( F
 A !.&'))(%+* ( 3 *Q 
 % ( ( ?
 ~ff!0Tt;&;|+uP1 
  O
 ( ( "( - ~ff0! Tt; 1(vr~*uQz &'))(%+* ( 3 *Q	& % ( (  A !.&'")(%+* ( 3 *P	& % ( - ?
 ~ff!0Tt;20|+uQr 
 ?

@KP[4Z6]q OQ]ffZ.MRc6KPX_#ZhK*_#^eW'UTK#j



z3v

 u~0#u&'))(%*( 3 *P	& % #):< +;   3HbV0/S-HbVP
<q k"k uPv &"! :<H)SP0/L 3H)V 

a.K?#KPVW'X]#W/]ffMRT[RT]ffZbr]#X[a6KZ6]#[R]ffZh]#b9br]#Xf#KP[Y[RZ.fRM,[a6K?br]ffUUT]	q4RZ6f2]ffZ6KC

orqtsu%z}yUyMy#}sW:	{}u TuffU&{|Ty:&:z	psU  y{zxWz	~t|u 	 luY x8y5|}uz	|!
npo* 
ff
fi 	5 ff
~ ff
fi *}s'u ff "!#$%& "!#'$ff() +*+),* (.-0/+1 32 -541 76 $ffff789(
* -541 76 $ff
8
(.-0/+1  * 6 $ff:  8;  8;(
*
<=(.-0/+1 32 -54+1 >6 $ff  8?(.-54'1  * 6 $ff  8;(.-0/+1  * 6 $ff  8;  8;(
*8WTTuz}
 ff   ff~   !
Do* "  @  @B}su ffA%3:!#$3:!#$ff()B#*+)C#* D$ff(.-E1 F6 $ffG8H(.-E1JI%*3* -E1KI 6 $8L$ff(.-E1 F6 $ffG8
(.-E1KI%*3* -E1JI&*M<=(.-E1 6 (.-E1JI- iTTu z}% ffNB fffN
~  !

ioR}suX{Ku   O  Wz}D| o
Q)(,+

fif.w~3}33f$3ws

& '))(%*( 3 P* 	& % ((3y1?vr~6u
 uv & Pu} M!0wx */ T. w !0x!

 0z"t(9"(*  
ru=<uwuw{ k
 !{#y00r 4 vw !0z{uQv , !0z6u	W/.uPz uB![ & vr~#v?y1  yrv5Oy1zituR-6uPz"tuPz'v w[!0x ( 	J /*- v	!  !ff{ y5 0u	W/yr;O
uQz u K 


 a.Kbr]ffUUT]	q4RZ6f2R^e^Kc'R_0[YKOQ]ffZ.MKW"N6KZ'OQK?]#b=X]#Wi]ffMRT[RT]ffZ'&KM[_0d'URMa.KM:M[YX]ffZ6f2XKU_0[RT]ffZ'Ma.RTW'M
d/KP[q:KPKZUR[YKPX_#U/b1]#Xf#KP[Y[RZ6f_#Z.cB@RT[%RZ'c6KPW/KZ.c6KZ.OQKC

J uwruz$r  H  uPv & Pu2.!#w xN/*  w !0x 
3
% #z"tff( "(*
(sy &0z"t+!0z. 4y .& .
 &'")(%+* ( 3 P* 	& % (:~ff!0Tt; 

Y& yo  yrv5Oy1zituR-6uPz"tuPz'v  w !0x

a.K?b1]ffUU]q4RZ.f&]ffZ.KfffRTS#KM_#ZR^e^Kc.Ro_0[YK3_0W'W'UROP_0[RT]ffZ]#b8URT[YKPX _#U/b1]#Xf#KP[Y[RoZ6fC
x /*  $y1  y5v Oyrz"tu -6uQzituQz'v w !07
x ( 1vr~6uQz & A !:$ff~ !# t;y 0z"t !0z' 4y 
J uwruz$r     !#w N
&'))(%* ( 3 *Q	 & % ((FA ! $.
a'RM:XKMN.UT[(W.X]S#KM[a'_0[,b1]#Xf#KP[Y[RoZ6feURT[YKPX_#UoM(b1X]ff^ ( c6]KM,Z6]#[_ iKOQ[KZ[_#RoU^KZ[,]#b9br]#X^&N.U_#M
[a._0[_0XK @9RT[%RZ.c6KPW/KZ.c.KZ[ brX]ff^ (4ja'RM9RoMRoZ}MY]ff^eKMYKZ.MYK,_#Z._#UT]#f#]ffN'M8[Y][a.K(OQ]ffZ'OQKPW.[]#b  U[YX_0[RT]ffZ
RZ^]*c._#U9U]#fffROPMe	 
]ffUc6d"U_0[Y[P`  # 9RZ.c.KPKcB`iRbq+K_0XKRZ[YKPXKMY[YKcLRZ*Z6]q4RoZ6fqa6KP[a6KPX & A ! $
]ffZ.UTV2br]#X+br]#X^&N.U_#M $ [a._0[:_0XK @9RT[%RZ.c.KPWiKZ'c6KZ[brX]ff^ (`[a6KZ[a.KURT[YKPX _#UM=]#b ( OP_#ZdiKb1]#Xf#]#[Y[YKZ
RB
Z &j
@KP[N.MZ6]qRZS#KMY[RTfff_0[YK&[a6K}OQ]ff^W"N6[_0[RT]ffZ]#b &'")(%+* ( 3 *P	 & % (: j @KP[N.M  XMY[OQ]ffZ'MRc6KPX O4
br]#X^}N'U_#M &j6]#Xf#KP[Y[RZ6f}URT[YKPX_#UMq4R[a.RZ 64 b1]#X ^}N.Uo_#MRoM_?OQ]ff^W'N6[_0[RT]ffZ'_#UUTVK_#MYV&[_#MYij Z2[a6K
]ffZ6K3a._#Z.cB`br]#Xf#KP[Y[RZ6fURT[YKPX _#UM=q4R[a.RZ_&c.R5M FYN.Z'OQ[RTS#K4br]#X^&N.U_OQ]ff^eKM,c6]q4Z[Y]&b1]#Xf#KP[Y[RZ6f2[a6K^ RZ
KPS#KPXVhc.R5M FYN.Z'OQM[ C
=<



ru uwuw{

k





 0z"tff(="(R 
& ')"( *( 3 *P	& % :( 

H &'))(%*( 3 *P 
 % (( '

 uPv &1 
 Pu}v| ! M!0w xN/*T; w[!0x

&'))(%*( 3 Q* 	&KH 
 % (:"

Z [a6K]#[a6KPX:a._#Z.cB`ffbr]#Xf#KP[Y[RZ.fURT[YKPX_#UoM8q4RT[a.RZ_OQ]ffZ.MRMY[YKZ[=[YKPX^ MR^W"UTVOQ]ffZ.MRoMY[M\RZ2XK^]S
e
RZ6f[a.K^ b1X]ff^ [a6K?[YKPX^TC
k   uBv 1 P u I!0z6yoQvuQz'v vuPw x w !0x 
3
% JyIuQ|+ut;(vr~6u:Puv![:yrvr:
yvuQwY0
 K
0z"tc( "(R  &'")(%+* ( 3 *P 1 % (:  j  
=<



ru uwuw{

UoRT[YKPX_#UM&OP_#Z d/Kbr]#Xf#]#[Y[YKZ brX]ff^ _
rb ]#X^&N.U_B&RoZ Wi]ffUVZ6]ff^R_#U[R^K#jep[RMMN.OPRTKZ[?[Y]c6KUTKP[YKeKPS#KPXV UoRT[YKPX_#U8]#bT( brX]ff^ K_#O a
c.RM5FYN.Z'OQ[+]#b &RTb]ffZ6K?]#b8[a6Kc.RoM FYN'Z.OQ[M(d/KOQ]ff^KM,K^W.[V[a6KZ &'))(%*( 3 *Q	& % (( 9* f ; j
a'RZ6fffMB_0XK:^]#XK\OQ]ff^W"UROP_0[YKcbr]#X9OQ]ffDZ FYN.Z'OQ[RTS#K=br]#X^}N'U_#M &/
j9\MYW/KOPR_#UUV#`[a6KPXK:RM Z6]XKMN.UT[
MR^RU_0XB[Y.] \X]#W/]ffMRT[RT]ffZ ^,b1]#XOQ]ffZ FN.Z.OQ[RTS#K=br]#X^&N.U_#MPj9J a'RUTK &'))(%* ( 3 *Q	 & % (( / &'))(%* ( 3 *P 
 % ((
RM_U]#fffROP_#U.OQ]ffZ.MYWK "N6KZ.OQK4]#b &'))(%+* ( 3 *Q	 &/ 
 % (:(MKPK ?:]#X]ffUoU_0X
V ff `[a6KOQ]ffZS#KPXMYKc.]KM=Z.]#[=a6]ffUc
RZ[a6Kf#KZ6KPX_#U OP_#MYK#j
64

?:]ff^}d'RZ'RZ6f[a.K[q+]LW'XKPSR]ffN.MW'X]#W/]ffMRT[RT]ffZ.M}Ma6]	q4Ma6]q


	

 z3v <q kG7  uPv & !. 1 
"!O:< 1+0z"tff( !$# + .y1z  u  4 
1=|+u~0#u&'))(%*( 3 *P 
 % ((

  .yrz u
&'))(%+* ( 3 *Q	& % (:yoh;#
ymt)12|+u~00u &'))(%+* ( 3 *Q	& % (: / &'")(%+* ( 3 *P 
 % (:Y   :< 
5
.yrz uV&G/ 
y1y1z ,!0z6yoQvuQz'v>1<&'))(%*( 3 *P	& / 
 % (:yoyrz,!#z6 y1vuPz'v:0|+uQr 

	

ff	

Q)(^]

fi

~33



s~$w

~ww$

`*_#ZVZ.]ffZ*IS;_#UoRcOPU_#N'MYK60RM @RT[%RoZ.c6KPW/KZ.c6KZ[=brX]ff^ ( RTb _#Z.c]ffZ.UTVeRTb( 3 *P0; fi
 j *RZ.OQK[a6KOQ]ffZDFYN.Z'OQ[RT]ffZ ]#b?[q+] br]#X^}N'U_#Me[a._0[_0XK @RT[%RoZ.c6KPW/KZ.c6KZ[2b1X]ff^ ( RM @9RT[%
RZ.c.KPWiKZ'c6KZ[?brX]ff^ ( MKPKL\X]#W/]ffMRT[RT]ffZ ff `_#Z.c MRZ.OQKKPS#KPXVLbr]#X^}N'U_RMKW"N'RTS;_#UKZ[[Y] _ ?T4
br]#X^}N'U_*Q` =X]#Wi]ffMRT[RT]ffZ+&3Ma6]	q4M &'))(%*( 3 *Q	& % ((KW"N.RTS0_#UTKZ[[Y][a6KMYKP[]#bB_#UU*OPU_#N.MYKM*04[a'_0[\_0XK
KZ[_#RUTKc2dV & _#Z.c_0XK,b1X]ff^ #0OPU_#N'MYKA ( 3 *P0; fiO( !  +ffj A:KOP_#N.MYK#0OPU_#N.MKA( 3 *P0; fiO( !. +RM
OPUT]ffMYKcN.Z.c6KPXMN6d'MN.^eW.[RT]ffZ2RmjK#jT`#RT[ RMB_MY[_0d"UTK=W.X]*c.N.OQ[R]ffZ  KUc" `	R[ RMBW/]ffMMRTd"UTK9[Y][_0#K+_#c6S0_#Z[_0f#K
]#b}OQ]ffZ'MYWK "N.KZ.OQK  Z.c.RZ.f _#UTf#]#XR[a.^eM MYKPK _0IX "N.RoMP
` 0## `3[Y] c6KPXRTS#K "
_ ?54 XKPW'XKMYKZ[_0[RT]ffZ
]#b&')"( * ( 3 *P	 & % (: j \MWiKOPRo_#UUTV#`=RZ [a6KhOP_#MYKq4a6KPXK & RM_ ?54 br]#X^&N.U_*`=XKMY]ffUN6[RT]ffZ6Id'_#MYKc
OQ]ffZ.MYWK "N6KZ.OQK  Z'c.RZ6fe_#UTf#]#X RT[a.^eMURT#K}[a6]ffMYK2XKPW/]#X[YKcRZ rp%Z6]ffN6K#` # ff]#Xec6KU S=_#Um` ##ffOP_#Z
d/KN.MYKcB6[a'RM,RM,Z6]#[S#KPXVMN6XW.XRoMRZ6fMRZ.OQK?XKMY]ffUoN6[RT]ffZhRM,Z6]#[a'RZ6fd'N6[300w ymQ PPTueuP
yrx2y1zi#v5y !#z.j
p%ZOQ]ffZ[YX_#MY[ [Y][a.K\c.RM FN.Z.OQ[RS#K9br]#X^&N.U_#M MR[N._0[RT]ffZB`[a.KPXK\RMBZ6],fffN'_0X_#Z[YKPK+[a._0[MN'OaOQ]ffZ.MWK "N6KZ'OQKQ
 Z.c'RZ6f_#UTf#]#X RT[a.^eM+XN.ZRZ[Ro^K4W/]ffUTV*Z6]ff^eR_#U"RZe[a.K3RoZ6W'N6[\MRTgPK4q4a.KZ[a6K?RZ.W'N6[\RM:_&OQ]ffZ FYN'Z.OQ[RTS#K
br]#X^}N'U_r]#[a6KPXqRMYK#`9_#M}K *W'U_#RZ6KcRZL[a6Kebr]ffUUT]	q4RZ6f6`9q+Kq+]ffN'Uca._S#K  !  B j 4KPS#KPX[a6KUTKMMP`
br]#Xf#KP[Y[RZ6fURT[YKPX_#UoM,q4RT[a.RoZ_OQ]ffZ FN.Z.OQ[RS#Kbr]#X^}N'U%
_ & OP_#Zd/KK_#MVRZMY]ff^eK}XKM[YXROQ[YKcOP_#MYKMP`iKM%
W/KOPR_#UUTVq4a6KZ & RM,fffRTS#KZdV[a.KMYKP[4]#bRT[M4W.XR^eKR^W'URoOP_0[YKMP.RZh[a.RoMMRT[N._0[R]ffZB`.RT[4RM4MN6OPRTKZ[
[Y]efffRTS#K}N6Wh[a6]ffMYKOPUo_#N.MYKMOQ]ffZ[_#RZ.RZ.fe_eURT[YKPX_#U/brX]ffQ
^ (4j
 ru=<uwuw{ k 
 !0wN
x /*  [w !#2
x 

 0z"c
t ( "(R 
 uPv & Pu&.
	 
 &')"( * ( 3 *P	 & % (:Y7
 !$#0rA 0 )	 
	 &F e)( 3 *P 00M)
fi ( ! +'
a.K:Z6]#[R]ffZ}]#b"URT[YKPX_#Ub1]#Xf#KP[Y[RZ6f3f#KZ6KPX_#URgPKM8[a6K:Z.]#[RT]ffZ&]#b'S0_0XR_0d'UTK+KUR^eRoZ._0[RT]ffZbrX]ff^sW.X]#W/]0
MRT[R]ffZ._#U*UT]#fffRO?r[a._0[q(_#M=_#UTXK_#c6VZ6]	q4Z2dV A:]]ffUK_#MKUR^eRoZ._0[RT]ffZ]#b/^eRc'c.UTK\[YKPX ^eM_#Z.ca._#M8d/KPKZ
f#KZ6KPX_#UoRTgPKc[Y][a6K  X MY[%I]#Xc6KPXOP_#MKRZ_^]#XKXKOQKZ[&W'_#MY[dV @9RZ fi E4KRT[YKPX` # < jp%Z.c6KPKcB`
S0_0XR_0d'UTK}KUR^eRoZ._0[RT]ffZRoM_0d/]ffN6[&;#w ymQ PQu !0wm{uvmvy1z{#`RmjK#jT`/[a6K2]ffZ6K_#O a.RTKPS#KcLZ6]#[?OQ]ffZ.MRoc6KPXRZ6f[a6K
URT[YKPX _#UM,MRTfffZ.MWC
Pu !0wN
x /*   w !0x 

 #z"tLuPYv U Pu
q!m{uw{  " nz$rffz$q'> uwr|wqf{#| '  uPv &
I /=PQPuPv ![+
fi  &')"( 3* U	.	 & % U&?y1vr~*1
u !#w N
x /* yrz"Dt /vy1#uQ 4t;u 
=ziuth0 !0r !0|8 
 ! &1
 &'")(%+* U,	.	 & % ffff
 ! &'gih ( H &'gih / 1
 &'")(%+* U,	.	 & % #D> +;ff
 ! &')"( 3* U	. &')"( 3* U	.	 & % U& % #D> +; 
 &'")(%+* U,	.	 & % #D> + . U}ff
 H)SP0/L 3 H)V  u~0#u&'))(%3* U,.	 & % #  +;  S-H)V 
 z3v <q k%H  uPv &"! :<
fiM=_?c.RTXKOQ[8OQ]ffZ'MYWK "N.KZ.OQK,]#b"[a.Kc6K  Z.RT[R]ffZB`)&'")(%+* U,	.	 & % #D> ( %('''% > ) +;8RoM9WK "N.RTS0_#UTKZ[8[Y][a6K
"N._#Z[R  Kcd/]]ffUTK_#Zb1]#X^&N.U_hN.MN._#UUTVq4RT[abrXKPK?S;_0X R_0d'UTKM 
&c.KZ6]#[YKc ff > ( '('(' ff > ) &j
?(UK_0XUTV KZ6]ffN6fffaB`br]#Xf#KP[Y[RZ.f_S;_0X R_0d'UTK > _#^]ffN'Z[M&[Y] b1]#Xf#KP[Y[RZ6fdi]#[a[a6KURT[YKPX_#U
M > _#Z.c
%: >9j
 ru=<uwuw{ 7 
 !0wN
x /*  [w !#2
x 

  0z"t U!"
fi  u~00u
 uPv & Pu&.
&'))(%3* U	.	 & % U2" .&'))(%* ( 3 *Q	 & % ('$+
a'RMXKMN.UT[P`#[Y]#f#KP[a6KPX\q4RT[a&[a6K(W.XKPS*RT]ffN.M9XKMN.U[M9]ffZUoRT[YKPX_#Ub1]#Xf#KP[Y[RoZ6f6`fffffRTS#KMN.M[a6K(br]ffUUT]	q4RZ6f
OQ]#X]ffUUo_0XRTKM(br]#XS;_0X R_0d'UTK?br]#Xf#KP[Y[RZ6fC

(

!

?(UK_0XUTVKZ6]ffN6fffa

Q (

fif.w~3}33f$3ws

J

uwruz$r



d 'ei &'))(%*3U.	& % D# >*+;Y
J

d 'fe"	& . #fi 3 * V >= % >B A2=;A ! &+ '
uPv &Pu2 M!0wx /*Th0z"t U2"
fffi F&'))(%+* U.	& % U2yovr~6u& !{#y00r 4vw !0z{uQv
!


uwruz$r 
,!#z6Pu M/'uQzuN![ & vr~#v:y1 .0w Oyrz"tu -6uQz"tuQz'v w[!0x U J	/*-v	! !{#y00=u	W/yr;#TuPz u[K 



x /T $ y1 '0Iw Oyrz"tRu -.uQz"tuPz'v w !0x U 1vr~*uPz & A ! $ y  0z"
t !0z. 4 y 
J uwruz$r      M!0wN
&'))(%*3U,.	& % U&FA ! $ 
fi OQ]ffZ'MYWK "N.KZ.OQK(]#b.[a6K,U_0[Y[YKPX8XKMN.U[9RM9[a._0[8b1]#Xf#KP[Y[RZ6f3S;_0X R_0d'UTKM9RM9N'MYKPbrN'Uq4a.KZ]ffZ.UV_MN6d'MYKP[
]#b=S0_0XR_0d'UKM4_0XK&XK_#UUTVN'MYKcRZ[a6N
K "N.KPXRTKMPj44aN.M`"RT1b & XKPW.XKMKZ[M3MY]ff^K&W'RTKOQKM]#b=*Z6]qUTKc6f#K
_0d/]ffN6[=_?MOQKZ._0X RT]?]#biRoZ[YKPXKM[P`_#Z.c2q:K4_0XKRZ[YKPXKMY[YKcRZ&Z.]q4RZ.f?q4a6KP[a6KPX=_?b_#OQ[ $RM8[YXN.KRZ2[a6K
MOQKZ._0X RT]6`[a6KU]#fffROP_#U6]#W/KPX_0[RT]ffZ[Y]}c6]RM8[Y] "N6KPXV&q4a6KP[a.KPX &OA ! $\-j 4]q`RTb"[a.K,Wi]ffMMRTd'UTK:b_#OQ[M $
q:K_0XKRZ[YKPXKMY[YKcRoZhc6]eZ6]#[RZS#]ffUTS#KMY]ff^KS;_0XRo_0d'UTKM U`6[a6KZh[a6KMYK?S0_0XR_0d"UTKM,OP_#Zhd/K?b1]#Xf#]#[Y[YKZ
brX]ff^ &`._#.M "N6KPXVRZ.f&qa6KP[a6KP.X $ RMRo^W'URTKcOP_#Zd/K?c6]ffZ6K?]ffZ&'))(%3* U	.	 & % U2:RZ.MY[YK_#c]#b &j
a.X]ffN6fffa2[a6K,W.XKPSRT]ffN'M9W'X]#W/]ffMRT[RT]ffZB`#MY]ff^eK4_#UTf#]#XRT[a'^eMb1]#X=br]#Xf#KP[Y[RZ.fS;_0X R_0d'UTKM8OP_#Zd/K,K_#MRUV
c6KPXRS#KcLb1X]ff^ _#UTf#]#XR[a.^eM?br]#Xbr]#Xf#KP[Y[RZ6fURT[YKPX_#UMMY]ff^Ke]#b,[a6K^ a._S#Kd/KPKZMY#KP[Oa.Kc d/KPbr]#XK	 j
W/KOPR  OP_#UoUTV#`ffW/]ffUTV*Z6]ff^eR_#U'[R^K_#Uf#]#XRT[a.^eM+br]#X:br]#Xf#KP[Y[RZ.f}S0_0XR_0d'UKM\q4R[a.RZe_ 64b1]#X ^}N.Uo_]#X,_
br]#X^}N'U_fffRS#KZdVe[a.K3MKP[,]#b9RT[M:W.XRo^KR^eW'UROP_0[YKM:OP_#Zd/K]#d'[_#RZ6KcBj [a6KPX([YX_#OQ[_0d"UTKOPU_#MMYKM(]#b
W.X]#W/]ffMR[RT]ffZ._#U9b1]#X^&N.U_#Mbr]#X?S;_0X R_0d'UTK&b1]#Xf#KP[Y[RZ6fhK 6RMY[Pj}.]#X}RZ.M[_#Z.OQK#`B[_0RoZ6fh_#c6S;_#Z[_0f#Ke]#b:[a6K
b_#OQ[\[a._0[ &'")(%+* U,	.	 &/ 
 % U}  &')"( 3* U	.	 & % U2 / &'))(%+* U,	6 
 % U}qa6KZ6KPS#KPX U	6	 &4 fi
U. 
 !  a.]ffUc.MPff
` 3_0Xq4RoOa6K[ ##ffMa6]q:Kc [a._0[S0_0XR_0d'UKbr]#Xf#KP[Y[RZ6fRoZ _br]#X^&N.U_ & OP_#Z
d/Kec6]ffZ6KRoZLURZ6K_0X?[R^eKe_#MMY]]ffZ_#N
M &RMRZ 2,u ,!0.
x -G!;PQ PQu N&uI{#v0y !0z N !0wxe0Y P!0wx  64?4= `
RmjK#jT`=_   fiY
 4O4b1]#X^&N.U_RZ q4a.RO a[a6KhOQ]ffZ FN.Z.OQ[M&]#b4_#ZVOQ]ffZ FN.Z.OQ[RS#KMN.d.b1]#X ^}N.Uo_c6] Z6]#[
Ma._0XK_#ZVeS;_0X R_0d'UTK#jpZ[YKPXKMY[RoZ6fffUTV#`[a6K 64O4b1X _0fff^KZ[:]#b W.X]#Wi]ffMRT[RT]ffZ._#U.U]#fffRO4RM+MY[YXROQ[UVe^]#XK
MN.OPOPRoZ.OQ[[a._#Z&[a6K 64]ffZ6K?rKMYW/KOPR_#UUTV#`0MY]ff^K 64O4b1]#X^&N.U_#M9]ffZ.UTV_#c'^eRT[K *W/]ffZ6KZ[R_#UUTVU_0Xf#K
KW"N.RTS0_#UTKZ[ 64 b1]#X ^}N.Uo_#M 3 3_0Xq4RO a6K fi_0,X "N.RMP` ##ff j
p%Z[a6K}f#KZ6KPX_#U9OP_#MYK#` FN.MY[_#M4br]#X4[a6K&URT[YKPX_#UMRT[N._0[RT]ffZ `.[a6KPXKRoM4Z6]eq(_V[Y]br]#Xf#KP[4KPOPRKZ[UTV
RmjK#jT`6RZWi]ffUVZ6]ff^R_#U6[R^K	\_&MYKP[+]#bBS0_0XR_0d"UTKM\qRT[a.RZe_}br]#X^}N'U_N.Z.UKMM  !  / #j 4KPS#KPX[a6KUTKMMP`
[a6K+b1]ffUU]q4RZ.fc6KOQ]ff^W/]ffMRT[RT]ffZW.X]#W/KPX[V?OP_#Z}d/K\a6KUTW.bN.U0RZ}MY]ff^K+MRT[N._0[RT]ffZ.M+_#OQ[N._#UUTV#`#RT[RoMa6K_SRUV
K*W'UT]ffRT[YKchRZ ]ffa.U_#M,KP[4_#UIjT` ##ff j


=<

ru uwuw{

U 4

7Bk

 uv1&1 
 Puev| ! M!0wxN/T; w[!0xQ
3
% 1#z"t%U Pu /PQPuv ![O
fffi  
& /&'")(%+* U	6 
 % U&

,. & 1\vr~*uPz&'")(%+* U,	.	&G/ 
 % &U 

4]#[YK[a._0[,[a6KOQ]#XXKMYW/]ffZ.c.RZ.fW.X]#W/KPX[Vbr]#XURT[YKPX_#Uibr]#Xf#KP[Y[RZ6fec.]KM,Z6]#[a.]ffUcL_#M_&W.XKPS*RT]ffN.M

K6_#^W'UTKMa.]q4M  j
6]#Xf#KP[Y[RZ6f URT[YKPX_#UM]#X}S0_0XR_0d"UTKM?W.X]	S#KM}a.KUTW.bN.URZS0_0XRT]ffN'MMYKP[Y[RZ.fffMrq:K_#UTXK_#c6VMY#KP[Oa.Kc
MY]ff^K ]#b}[a6K^ RZ [a6KLRZ[YX]c.N'OQ[RT]ffZ" j6]#XRZ.M[_#Z.OQK#`^eRoZ.R^e_#U^]c.KURoZ6b1KPXKZ.OQK r]#XOPRXOPN.^2
MOQXRW.[RT]ffZ >O ?(_0X[aV#`  ff4OP_#Zd/K?K *W.XKMMKcN.MRoZ6feURT[YKPX_#U b1]#Xf#KP[Y[RoZ6frd'N6[Z6]#[c.RXKOQ[UTVN'MRZ6f
S0_0XR_0d'UTKb1]#Xf#KP[Y[RoZ6f6`"qa.ROaMa6]qM4[a6K&RZ[YKPXKMY[4]#b[a.K}^]#XK}f#KZ6KPX_#U9b1]#X ^ q:K}RZ[YX]*c.N.OQKc" j(p%Z*
c6KPKcB`6RT[:RM+q+KUUTIZ6]	q4Z[a._0[,OPUT]ffMYKcq+]#X UcRoZ6b1KPXKZ.OQK4brX]ff^ _*Z6]	q4UTKc6f#Kd'_#MYK & OP_#ZdiKUT]#fffROP_#UUV
Oa'_0X_#OQ[YKPXRTgPKc _#MOPU_#MMROP_#U4KZ[_#RU^eKZ[brX]ff^ & OQ]ff^W"UTKP[YKc q4RT[a M]ff^KL_#MMN.^W'[RT]ffZ.MPj p%Z [a6K
OPRTXOPN'^eMOQXRTW'[RT]ffZb1X_#^eKPq+]#X ; >O ?(_0X[aV#`  ff `fffRTS#KZ _hW'_0X[RT[R]ff
Z +:
 % % -?]#/b 
fi:`9MN.Oa _#M%
MN.^eW.[RT]ffZ.M?_0XK[a6KZ6KPfff_0[R]ffZ.M]#b([a6Kb1]#X^&N.U_#
M  MPj[P
j  c6]KMZ.]#[}OQ]ffZ[_#RZ_#ZV S;_0XRo_0d'UTKbrX]ff^

Q

fi

~33



s~$w

~ww$

K 1OQ]ffZ[_#RZ.RoZ6f2]ffZ.UTVW/]ffMR[RTS#K?URT[YKPX_#UM:d'N'RUT[(N6Whb1X]ff^ 
 _#Z.chURT[YKPX _#UM:d'N.RUT[
 _#Z.cb1]#X4KPS#KPXVhOPU_#N.MYO
6N WbrX]ff^ `'RTb & A4!=1La6]ffUc'MP`*[a6KZ'& A4! 1H  a.]ffUc.M,_#Mq:KUUmj+ "N.RTS0_#UTKZ[UV#`:  RoMOQ]ffZ.MRc.KPXKc_
XK_#MY]ffZ'_0d'UTK_#MMN'^W.[RT]ffZq4a.KZ6KPS#KPX4RT[RM}MYVZ[_#OQ[ROP_#UUV6 S=_0XRZ.c6KPW/KZ.c6KZ[(b1X]ff^  _#Z.cKW"_#Z.c*
RZ6%
f & q4RT[ahRT[4c.]KMZ.]#[4^]*c.RTbrVq4a._0[RoM_#UTXK_#c6V*Z6]q4Z_0di]ffN.[+(*, . (  j ?(UTK_0XUVKZ6]ffN6fffa `.[a6K
MRTfffZ'M4]#b:URT[YKPX_#UM4b1X]ff
^ (* XK_#UoUTV^e_0[Y[YKPXa6KPXK#jMRZ.f]ffN6X3W.XKPSRT]ffN'M4Z6]#[_0[RT]ffZ'MP`4:  RM3_#MMN.^Kc
RTb8_#Z.ch]ffZ.UVRTb   4 ,. :  _#Z.'
c &    & / : \7j fiM4_eOQ]ffZ'MYWK "N.KZ.OQK#`'q:K}c6KPX RTS#K?[a6Kbr]ffUUT]	q4RZ6f
Oa'_0X_#OQ[YKPXRTg_0[RT]ffZ]#b8OPRTXOPN.^eMOQXRTW.[RT]ffZ C


1 0z"tff
 1 1  P u&vr~wuu2t0y1	M!0yrz'v
 uPv &1 
 P u}v| !YM!0wxN/T;  w !0x2
3
% +
Puvr ![00w ymQPPTuQ  w !0x 
fi mJ I/P~hvr~*#v U,	.	& . U. 
'"
 . .  K  v\~ff!0Tt; 
4 ;0wy QP u  w 0! x 
   
 t!	uzff! v I0! z'vI0yrz0z
=<

ru uwuw{

7=7


 	  
 	 & % +:
 % % -YFA ! 

y 0z"t+!0z. 4y 
&OA !.&'")(%+* ( 3 *P	 & / 
 % (/. . (fiff9


 zLvr~*u3{uQziuPw%#;u 

 	  
 	 & % +:
 % % -YFA ! 

y 0z"t+!0z. 4y 
& A !.&'))(%*( 3 *P	& / :@&')"( * ( 3 *P	 & / : 
 % ( ff
.

( .  % ( ff
.

( . 

|8~*uPwYu 
 	  
 yo y1w M/*x&w y -ivy0!0z;}tu;
=ziuty1z#J   0wQvr~Q4)1 IK 
*Ro^eRU_0X3Oa._0X _#OQ[YKPXRTg_0[RT]ffZ.M}OP_#Zd/Kc6KPXRTS#Kc br]#X[a6K]#[a.KPXb1]#X ^eM?]#b(OPUT]ffMYKcLq:]#XUc XK_#M]ffZ.RZ6f
W/]ffRZ[YKc]ffN6[4MY]eb_0Xj
6]#Xf#KP[Y[RZ6f_#UM]eRM,_OQKZ[YX _#U OQ]ffZ.OQKPW.[qa6KZq+K_0XKOQ]ffZ.OQKPXZ6KchqRT[aT"N.KPXV_#Z.Mq+KPXRoZ6f2qjXj[Pj
_}XKMY[YXROQ[YKc[_0Xf#KP[(U_#Z6fffN._0f#K#j\pZ.c.KPKcB`RZ^e_#ZVW.X]#d'UTK^MP`[a6KPXKRM+_}MYKP[:]#bBS0_0XR_0d'UTKM\b1]#X:q4a.RoOa
q:K_0XKZ.]#[eRZ[YKPXKMY[YKc RZ [a6KRTX2[YXN6[a S0_#UN6KLMY] q+KOP_#Z br]#Xf#KP[[a6K^ j 6]#XeRZ.MY[_#Z'OQK#`\RZ [a6K
 fi 1@*fiO4 b1X _#^KPq+]#XdV _#N.[Yg#` (O fiUoUTKMY[YKPX`_#Z.+
c *KU^e_#Z[ # ff `'OQ]ff^eW'RURZ.f_q(_V 'N.KZ[M+]#X
_#OQ[RT]ffZ.M,_#^e]ffN.Z[M[Y]b1]#Xf#KP[Y[RoZ6f2S;_0XRo_0d'UTKMPj *RZ'OQK[a6K?]ffZ'UTVeS0_0XR_0d'UTKM:q:K?_0XK?XK_#UUTVRZ[YKPXKMY[YKchRZ
q4RT[a'RZ_fffRTS#KZ2MYKP[8]#b"OPUo_#N.MYKMXKPW.XKMKZ[RZ.f_W'U_#Z'Z.RZ6fW'X]#d'UTK^sRZ.MY[_#Z'OQK:_0XK([a6]ffMK:XKPW.XKMYKZ[RZ6f
[a6KW'Uo_#Z.MP`Bq:KOP_#ZOQ]ff^W'RoUTK_q(_V_#ZV]#[a6KPXS;_0XRo_0d'UTK#`RTb\[a.RM?c6]KMZ6]#[RZ[YX]*c.N.OQK_#ZRZ'OQXK_#MYK
]#b9MRTgPK3]#b[a6K3XKMN'UT[RZ6f}b1]#X^&N.U_*ffj fiZ6]#[a6KPX,MR[N._0[RT]ffZq4a6KPXK?MN'Oa_2b1]#Xf#KP[Y[RZ6fZ._0[N6X_#UoUTV]OPOPN.XM
RM^e]c6KUTId'_#MYKcc.Ro_0fffZ6]ffMRMe5 E4KRT[YKPX`  # \OQ]ff^W'RUoRZ6fh_q(_VKPS#KPXVS0_0XR_0d"UTKK *OQKPW'[[a6K_0d"Z6]#XY
^e_#UR[V]ffZ.KMc6]KMZ6]#[eXK^e]S#K_#ZV W"RTKOQKh]#bRoZ6b1]#X ^e_0[RT]ffZ XWK "N.RTXKc [Y]OQ]ff^eW'N6[YKh[a6KOQ]ffZ "ROQ[M
_#Z.c[a6Kc.R_0fffZ.]ffMYKM]#b=_eMVMY[YK^j(4aN.M` _0Xq4ROa.K[ # ffMa6]	q4M4a6]	q d/]#[a[a.KMYKP[4]#bOQ]ffZ "ROQ[M
_#Z.c[a6K}MYKP[]#bOQ]ffZ.MRMY[YKZ'OQVId'_#MKcc.Ro_0fffZ6]ffMYKM]#b=_MYV*MY[YK^ RoMOa._0X _#OQ[YKPXRTgPKcdV[a6K}b1]#X ^}N.Uo_]#d6
[_#RZ6Kc dVb1]#Xf#KP[Y[RoZ6fhKPS#KPXV S;_0X R_0d'UTK2K 6OQKPW.[?[a6Ke_0d'Z6]#X ^e_#URT[V]ffZ6KMRoZ [a6KeOQ]ffZ FN.Z.OQ[RT]ffZ ]#b:[a6K
MYV*MY[YK^ c6KMOQX RTW.[RT]ffZ _#Z.c [a6K_S;_#RoU_0d'UTK]#d'MYKPXS0_0[RT]ffZ.Mj \X]S*Rc6Kc [a._0[2[a6KMYV*MY[YK^ c6KMOQX RTW.[RT]ffZ
a._#M  XM[d/KPKZ[N6XZ6Kc RZ[Y] 64?4:`Bb1]#Xf#KP[Y[RoZ6fhOP_#Z diK2_#Oa'RTKPS#KcLRZ URZ6K_0X[R^K2_#Z.c c.R_0fffZ.]ffMYKM

Q_

fif.w~3}33f$3ws

OQ]ffZ[_#RZ.RZ6f_^eRZ'R^e_#UZN.^d/KPX]#b,b_#N.UT[VLOQ]ff^W/]ffZ6KZ[M}OP_#Zd/KeKZN.^KPX _0[YKc RZ r]ffN6[YW'N6[ ?Wi]ffUV
Z6]ff^eRo_#U\[R^eK#jp%Z[YKPXKMY[RZ6fffUTV#`8[a.RoMq:]#XMa.]q4M&[a._0[&[a6Kc.R_0fffZ6]ffMRoM[_#MY c6]KM2Z6]#[&XKW"N.RTXK[a6K
N.MN'_#UUTV K W/KZ.MRTS#K	?OQ]ff^W"N6[_0[RT]ffZ ]#bW.XR^eKR^W'UoROP_0[YKIM H;R^W"UROP_#Z[M?[Y]d/K_#Oa.RTKPS#Kc _#OQ[N._#UoUTV#`
OQ]ff^W'N.[RZ6f2W.XR^eKR^eW'UROP_0[YKIM H;Ro^W'UROP_#Z[M\RoM FN.MY[,_|\D 4?[Y]e_#O a.RTKPS#KS0_0XR_0d"UTKbr]#Xf#KP[Y[RZ.fe_#Z.chZ6]#[
_{ !0'RoZOQ]ffZ.MRM[YKZ.OQVId"_#MYKcc'R_0fffZ6]ffMRM j:6]#Xf#KP[Y[RZ6fKPS#KPXVhS;_0X R_0d'UTK?brX]ff^ _b1]#X ^}N.Uo_e_#UUT]	q4Mbr]#X
OQ]ffZ.MRoMY[YKZ.OQVO a6KO*RZ6fMRoZ.OQK & RMOQ]ffZ.MRoMY[YKZ[RTb._#Z'c}]ffZ'UTVRTb&'))(%+* U,	6	 & % U	.	 &Y9RM9OQ]ffZ'MRMY[YKZ[Pj
a6Kq+KUUTIZ6]	q4
Z 3_S*RM&_#Z.c +N6[Z._#^ _#UTf#]#XRT[a.^ b1]#X&M_0[RM  _0d"RURT[V[YKMY[RoZ6f  3_S*RN
M fi \N.[Z._#^h`
 03rXKOQKZ[UTVXKPSRMRT[YKcedV KOa[YKPX4_#Z.L
c E4RMa[ # <N.Z'c6KPX,[a6K3Z._#^K3c.RTXKOQ[RT]ffZ'_#U.XKMY]ffUoN6[RT]ffZ"
d'_#MRoOP_#UUTVOQ]ffZ'MRMY[MRoZLOQ]ff^W'N6[RZ.f_OPU_#N.M_#U8XKPW.XKMYKZ[_0[RT]ffZ]#b &'))(%+* U.	 & % U,	.	 &Yb1X]ff^ _
?546
 & N.MRZ.f&XKMY]ffUN6[RT]ffZ *Rb[a6KK^eW.[VhOPU_#N.MKRM,Z6]#[f#KZ6KPX _0[YKcB`.[a6KZ & RM,OQ]ffZ.MRM[YKZ[4_#Z'ch[a6K
OQ]ffZS#KPXMYK}_#UoMY]ea6]ffUc.Mj
6]#Xf#KP[Y[RZ6f OP_#Z _#UM] d/KLN.MYKc _#M_ #KPV OQ]ffZ.OQKPW.[RZ ]#Xc.KPXh[Y] ]#Xfff_#Z.RTgPK *Z6]q4UKc6f#KLMY] _#M
[Y] XKPW'U_#OQK]ffZ.KfffUT]#d"_#U4RZ6brKPXKZ.OQKRZ[Y] _ZN.^}diKPX]#bUT]OP_#URZ6brKPXKZ.OQKM_#MMa.]q4Z_#^]ffZ6f ]#[a*
KPXM 3d
V ?]ffa'U_#M?KP[}_#UIj+[ ##ff}_#Z.c fi^eRTX_#Z'c OQp%UTX_#R[a ; 0## ff` OQp%UTX_#RT[a _#Z.
c fi^eRTX; 0#  j
@ ]]ffMYKUTVMYW/K_0RoZ6f6` MN.O aL_0W.W.X]ff_#O a6KM?XKUTV]ffZL[a6KRc6K_[a'_0[?K W'U]ffRT[RZ6f_#UU[a6KW'RTKOQKM3]#b(RZ6br]#XY
^e_0[RT]ffZ fffRS#KZRoZ_*Z6]qUTKc6f#K2d'_#MYKRM[VW'ROP_#UoUTVZ6]#[3XWK "N.RTXKcbr]#X "N6KPXV_#Z.MYq:KPXRZ6f6j.]OPN.MRZ6f
]ffZ q4a._0[RMXKUTKPS0_#Z[[Y] [a6'
K "N6KPXV RMMN6OPRTKZ[Pj J a.RUKMN.O a [YKOa.Z.8R "N6KMc6] Z6]#[UT]q:KPXh[a6K
OQ]ff^W'UK *RT[V ]#b2RZ6brKPXKZ.OQK brX]ff^ [a6KL[a6KP]#XKP[RoOP_#UMRc6K#`[a6KPV OP_#Z UTK_#c [Y] MRTfffZ'R  OP_#Z[W.X _#OQ[ROP_#U
R^W'X]S#K^KZ[MPj 6]#XRoZ.MY[_#Z.OQK#`(_#MMN.^eK[a._0L
[ & OQ]ffZ.MRMY[Me]#b[a.XKPKbr]#X^}N'U_#M 
 ( ` 
 - `:_#Z'c 
 0 j
6]#X_#ZV "N6KPXV &`4UKPT
[ U !   00  ( U. 
10IY  U,	.  j JLKLa._S#K & A !  RTb_#Z.c ]ffZ.UTV Rb
&'))(%3* U,. 00  ( 
10 % UFA !&j8pb U,. 
 0 %fi  0-  ( U	. 
10mY ! `[a'RM\_#^]ffN'Z[M+[Y][YKMY[yrz"tu O
-6uPz"tuQz"v 4eqa6KP[a6KPX&'))(%3* U,. 0-  ( 
10 % UA !  a6]ffUc.M3]#X&'")(%+* U	6 
 0 % U=A !  a.]ffUc.MPj
a.RoMq(_V#`]ffZ6KfffUT]#d'_#URZ.b1KPXKZ'OQK&RM3XKPW'U_#OQKcdV[q:]UT]*OP_#URZ6brKPXKZ.OQKMPff
j 44]	q`M&'))(%+* U,	6 
 ( /

 - % U=4RoMWK "N.RTS0_#UTKZ[3[Y] &')"( 3* U	. 
 ( /-&'))(%3* U,. 
 - % Ufi 	 U	. 
 -  U. 
 ( YY % U= j
fiOPOQ]#Xc.RZ6fffUV#`"KPS#KPXVS;_0X R_0d'UTK}b1X]ff^ 
 - [a._0[?RM4Z6]#[?_S;_0X R_0d'UTK}]#
b 
 ( ]#X?_S;_0X R_0d'UTK}]#b  OP_#Z d/K
br]#Xf#]#[Y[YKZ  XM[+q4R[a.RZ 
 - d/KOP_#N.MYK?RT[:fffRTS#KM,Z6]2RZ6br]#X^e_0[RT]ffZXKUTKPS0_#Z[:[Y]2[a6V
K "N6KPXVi*[aN.M`]ffZ'UTV_
brKPq W'RTKOQKM=]#bi*Z6]	q4UTKc6f#K4a'_S#K4[Y]diK J%W'X]#W'_0fff_0[YKMc Lb1X]ff^ 
 - [Y] 
 ( d/KPbr]#XK_#Z.MYq:KPXRZ.f?[a6YK "N6KPXV#`
_#Z.chbr]#Xf#KP[Y[RZ.f_#UUT]	q4M(b1]#XOa._0X _#OQ[YKPXRTgRZ6f[a.K^ K 6_#OQ[UTV#j
fiMKPS#]##KcRZ[a6K&RoZ[YX]*c.N.OQ[R]ffZB`"_#Z6]#[a.KPX3MOQKZ._0XRT]RZq4a.RO abr]#Xf#KP[Y[RZ6fhRMN.MYKPbN.U9RM4[a'_0[]#b
d/KURTKPb=N6WBc._0[YK#j3p%Z.c6KPKcB`i[a6KPXK_0XK^e_#ZVbr]#X^e_#URg_0[RT]ffZ.M]#b\d/KURTKPb=N6WBc._0[YK}[a._0[?_0XK2d'_#MYKc]ffZL_
br]#X^ ]#b\S0_0XR_0d'UK&br]#Xf#KP[Y[RZ.f6j&a.K&d"_#MRO2MOQKZ._0XR]RoM[a.Kb1]ffUU]q4RZ.f]ffZ6K C3q:Ka._S#K_br]#X^&N.UT
_ &
[a._0[XKPW.XKMKZ[M4]ffN6X*Z6]	q4UTKc6f#K#i[a6KPXK&_0XK&M]ff^K}O a._#Z6f#KM3RZ[a.Kq:]#XUcB`'_#Z'cq4a'_0[q+K&*Z6]qRM
[a._0[&_0b1[YKPX&[a6K^ _b1]#X ^}N.Uo_ $ d/KOQ]ff^KM[YX N6K#jha6KMRo^W'UTKMY[3q(_V[Y]c6K_#U=qRT[aL[a6KN.W/c'_0[YKRM
[Y]_#MMN.^K}[a._0
[ $ XKPW.XKMYKZ[M_#UoUBq+K}Z.]q_0di]ffN.[[a6K[YX N6[aS0_#UN6K]#b[a.KS;_0X R_0d'UTKMRZ $\j fiM_
XKMN'UT[P`q:K3a._S#K?[YG
] J%br]#Xf#KP[ L2b1X]ff
^ & [a6KS;_#UoN6K]#b[a6K3S;_0XRo_0d'UTKM:RL
Z U,	6	 $8 j=a.KPXK_0XKc.R iKPXKZ[
br]#X^e_#URTg_0[R]ffZ.M ]#b6[a.RoMMO a6K^e_*`0d'_#MYKc}]ffZqa6KP[a6KPXbr]#X^&N.U.
_ $hRMOQ]ffZ.MRc6KPXKc[Y]OP_0XXVRZ6br]#X^e_0[RT]ffZ
_0d/]ffN6[S0_0XR_0d'UTKM8RT[=^eKZ[RT]ffZ'MrJ RZ'MUTKP[Y[P=` #0]#X=]ffZ.UTV}]ffZ[a6K,S0_0XR_0d'UTKMR[8c.KPWiKZ'c.M]ffZ 4KPfffZ.KPX`
 # `]#X=_#UoMY]3]ffZ2S;_0XRo_0d'UTKM9XKU_0[YKc2[Y]?c6KPW/KZ.c6KZ[9S0_0XR_0d"UTKMS*R_3_3c6KPW/KZ.c6KZ.OQK:bN.Z.OQ[RT]ffZ 4KPXgRTf6`
# ff j\a'RM*RZ.c&]#biN6WBc._0[YK,MOa6K^_*`ffq4a.RUK:UTKMM9*Z6]	q4Z2[a._#Z2[a6K ]ffMMRd'UTK ]*c6KU/
M fiW.W.X]ff_#O a2dV
J RZ.MUTKP[Y[[ #0 `/a._#M,W.X]S#Kch[Y]ed/K?MN.R[YKcb1]#X,XK_#M]ffZ.RZ6fe_0d/]ffN6[_#OQ[RT]ffZ'M ]ffa6KPX[VKP[4_#UIjT` # 
KPXgRTB
f fi ER  ` ##ff j'N6X[a6KPX^e]#XK#`[a6KW/]ffMMRTd'RUR[V[Y]br]#Xf#KP[URT[YKPX _#UMe_#Z.cLZ6]#[S0_0XR_0d"UTKM RM
_#UMY]3S0_#UN._0d'UK+RZ}[a.RMbrX_#^KPq:]#X[Y]?[_0#K_#OPOQ]ffN.Z[b1]#XW/KPXMRMY[YKZ[9RZ6br]#X^e_0[R]ffZB`#_#MMa6]qZ}XKOQKZ[UTV
dVMY]ff^K]#b4N.M KPXgRTfKP[_#UmjTff` 0#  `\MRZ'OQK[a6KW/]ffU_0XRT[V]#bRZ6br]#X^e_0[RT]ffZRoM]#br[YKZ MRTfffZ.R  OP_#Z[Pj
6]#X2RZ.MY[_#Z'OQK#`9q4a'RUTKbr]#Xf#KP[Y[RZ6f[a6K 'N.KZ[  3 <%brX]ff^ _*Z6]q4UKc6f#Kd'_#MKRMZ.]#[W.X]#d"UTK^e_0[RO0`
br]#Xf#KP[Y[RZ6fe[a.K?WiKPX MRMY[YKZ[ 'N6KZ[:<  3 <%q+]ffN.UochMN6XKUTVd/KRZ._#c6WK "N._0[YK#j

Q I\

fi

~33



s~$w

~ww$

6]#Xf#KP[Y[RZ6f OP_#Z _#UM]d/KN.MYKc [Y] O a._0X_#OQ[YKPXRgPK_ c6KPW/KZ.c6KZ.OQKXKU_0[RT]ffZ OP_#UoUTKc c6K  Z._0d'RUR[V
5 @_#Z.f fi _0X,"N'RMP`N#;dih_#Mq+KUU?_#M[a6KMY[YX]ffZ.f#KMY[Z6KOQKMM_0XVrXKMYW j q:K_0#KMY[MN6OPRTKZ[ 
OQ]ffZ.c.R[RT]ffZ.M]#b:_W'X]#W/]ffMRT[RT]ffZ._#U S;_0XRo_0d'UTK]ffZ _MYKP[ U ]#b=S0_0XR_0d'UKM4fffRTS#KZ _[a6KP]#XV & 5@9RZB`0##*
]ffa6KPX[V#
` @N.;_#MYgPKPqROQg#` fi g_#U_#MP`90# j fiMMa6]	q4Z RZ 5@_#Z6fBfi _0X,"N.RMP` #;d  @RoZB` 0##*
]ffa6KPX[V}KP[_#UmjT` 0#  `_#UU[a.KMYK,Z6]#[RT]ffZ.Ma._S#K^_#ZV&_0W.W'UROP_0[R]ffZ.MRZ}S0_0XRT]ffN.M fip  KUc.M`;RZ'OPUN.c.RZ.f
aVW/]#[a6KMRoM,c.RMOQXRo^eRZ._0[RT]ffZ `_0f#KZ[OQ]ff^e^&N.Z.ROP_0[R]ffZB`6[a6KP]#XVh_0W.W.XW] 6R^e_0[RT]ffZh_#Z.c_0d/c'N.OQ[RT]ffZBj
8RZ._#UoUTV#`d"_#MYKch]ffZhURT[YKPX _#Ui_#Z'cS;_0XRo_0d'UTKb1]#Xf#KP[Y[RoZ6f6`6S;_#UoN._0d'UTK3WK "N.RTS0_#UTKZ.OQK3XKU_0[RT]ffZ'M+]	S#KPX4br]#XY
^&N.U_#MOP_#Z_#UoMY]ed/K?c6K  Z6Kc C

 " yC %qsa1nz3q{ ; q pz3r %qa#1nz3q{ ; q('  uPv &1 
 P u:v| ! M!0w xN/*T;w[!#x=

 1
(Pu2I/=PQPuPv [! +(* 1:0z"t U P u2e /=PPuv1![?
fi 
 & 0z"t 
 0 wu}Q#ytv	!TPu @9RT[%IKW"N'RTS;_#UKZ[3{#yr0uQz)( 1+tuQz!#vut &   
1=y #z"tT!0z. 4y 
&'")(%+* ( 3 *P	& % ( 3 *P	& '(:.&'")(%+* ( 3 *P 
 % ( 3 *P 
 T(:
u S=_0XYIWK "N.RTS0_#UTKZ[3{ffy1#uQz U 1(tuQz!ffvut & 3$ 
1=y #z"t+!0z. 4y 
 & 0z"t 
 0wu}Q#yt	v !TP.
&'")(%+* U,	.	 & % U	.	 &  U& .&'))(%+* U,	6 
 % U	. 
  U&

!

q m{uw{

 I SP4/S?I V20z"t 

z3v <q k   uPv & !  )
0z"t 
 0wu  y v5OYu M/*yr;0uQz'v8{#yr0uPzb(



!

 I e4/:e I V  uvX(

!#):< % V +  &

*N'OaWK "N.RS;_#UTKZ'OQK}XKUo_0[RT]ffZ.MOP_0W.[N6XK2MY]ff^K2br]#X^eM]#b\c6KPW/KZ.c6KZ.OQK}d/KP[q:KPKZ b1]#X^&N.U_#Mja6KPV
0_ XK4N'MYKPbrN'URoZ2[a6K,S;_0X RT]ffN.MMRT[N'_0[RT]ffZ.M8q4a6KPXKRT[RM8XKW"N.RTXKc2[Y]br]#X^e_#UoUTVOa'_0X_#OQ[YKPXRTgPK[a6K,br_#OQ[\[a._0[
[q:]Z.]q4UTKc.f#K}d'_#MKMMa'_0XK&M]ff^K}[a.KP]#XK^eMPj3aN.MP`'[q+]br]#X^&N.U_#M_0XK @9RT[%IKW"N.RTS0_#UTKZ[4fffRTS#KZ](
q4a6KZ.KPS#KPXKPS#KPXVLOPU_#N.MYKOQ]ffZ[_#RZ.RZ.fURT[YKPX_#UMbrX]ff^ ( ]ffZ.UTVRoM?_hUT]#fffROP_#U=OQ]ffZ.MWK "N6KZ'OQK]#b+[a6K  XMY[
br]#X^}N'U_Rb=_#Z.c]ffZ.UTVRTb+RT[RoM_UT]#fffRoOP_#UOQ]ffZ'MYWK "N.KZ.OQK2]#b\[a6KMYKOQ]ffZ.c br]#X^&N.U_*j3p%Z[a.K2M_#^K2S#KRZB`
[q:L
] S=_0XYIWK "N'RTS;_#UKZ[br]#X^&N.U_#M,fffRS#KZ U a._S#K[a6K&M_#^KOPU_#N'M_#UOQ]ffZ.MWK "N6KZ'OQKM4d'N'RUT[,N6WbrX]ff^ Uej
?(UTK_0X UTV#` @RT[%IWK "N.RTS0_#UTKZ.OQKRM^]#XK  Z6KQIf#X_#RZ.Kc [a._#Z S=_0XYIWK "N'RTS;_#UKZ.OQKRZ [a6KMYKZ'MYK[a._0[[q:]
br]#X^}N'U_#M S=_0XYIWK "N.RTS0_#UTKZ[fffRTS#KZ U _0XK_#UMY'
] @RT[%IWK "N.RTS0_#UTKZ[fffRTS#KZ (-!Q(*$(`8[a6KMYKP[]#b4UoRT[YKPX_#UM
d'N.RoUT[?N6W/]ffZ U`d'N6[?[a6KOQ]ffZS#KPX MYKc6]KMZ.]#[a6]ffUcRoZ [a6Kf#KZ6KPX _#U\OP_#MYK#j ]ff^K_0W'W'UROP_0[RT]ffZ'M_0XK
XKU_0[YKc[Y]*Z6]	q4UTKc6f#K_0W'W.XW] 6R^e_0[R]ffZ 
 RoM_OQ]#XXKOQ[_0W.W'XW] 6R^e_0[RT]ffZ]#1b & ]S#KP6
X ( RTb8_#Z.c]ffZ.UV
RTb 
 _#Z.c & _0XN
K @R[%IWK "N.RS;_#UTKZ[4fffRS#K
Z (( `iZ.]#X^e_#URTg_0[R]ffZ r[N6X Z.RZ6f_br]#X^}N'UL
_ & RZ[Y]_ ?54 

dVRZ[YX]*c.N.OPRZ6fZ6KPq MYV*^d/]ffUMRM?_#OPOQKPW.[_0d"UTKe_#M?UT]ffZ6fh_#M3[a6K[q+]hbr]#X^&N.U_#M3_0XKWK "N.RTS0_#UTKZ[3]	S#KPX
[a6K]#X RTfffRZ._#U/U_#Z6fffN'_0f#K#`'RmjK#jT` & _#Z'c 
 _0XV
K S=_0XYIWK "N.RTS0_#UTKZ[fffRTS#KZ U-! U	6	 &4Y `'_#Z.cMY]]ffZ j

 l87 J

uv

<mq 



[q

p[2RoM "N.RT[YKK_#MYV [Y]LW.X]	S#K[a'_0[b1]#Xf#KP[Y[RoZ6fLRM2_LOQ]ff^W'N6[_0[RT]ffZ'_#UUTVK*W/KZ.MRTS#K]#W/KPX_0[RT]ffZ RZ [a6K
f#KZ6KPX_#U:OP_#MYK#jpZ'c6KPKcB`MRZ.OQK_br]#X^&N.U_'& RM}OQ]ffZ.MRMY[YKZ[2RTb_#Z.c]ffZ'UTVRTb &'))(%*( 3 *P	& % ( 3 *Q	&4Y
RM2OQ]ffZ.MRoMY[YKZ[_#Z.c MRoZ.OQK[a.KU_0[Y[YKPXbr]#X^&N.U_RMN@RT[%RoZ.c6KPW/KZ.c6KZ[brX]ff^ KPS#KPXV UR[YKPX_#URIjK#jT`+RT[2RM
KW"N.RTS0_#UTKZ[9[Y]* f  +]#XWK "N.RS;_#UTKZ[[Y3
]   :)	 `0[a6KPXK+RMZ6]4q(_V[Y]3OQ]ff^eW'N6[YK+_br]#X^}N'U_ 
LWK "N.RS;_#UTKZ[
[Y] &'))(%* ( 3 *Q	 & % ((3RZW/]ffUTV*Z6]ff^eR_#U9[R^K#`N'Z.UTKMM  !  9j fiOQ[N'_#UUTV#` q:KeOP_#Zc6KPXRS#K&[a.Ke^]#XK
OQ]ffZ.MY[YX _#RZ.RZ6f3XKMN.UT[P`ffMa6]	q4RZ6f3[a._0[\[a6K3 Fy Eu]#bi_#ZV2b1]#X^&N.U_WK "N.RTS0_#UTKZ[[Y],&')"( * ( 3 *P	 & % (:^e_V
d/KMN6W/KPXW/]ffUTV*Z6]ff^eR_#UoUTV&Uo_0Xf#KPX[a._#Zh[a.KMRTgPK?]#b &j


=<

ru uwuw{

7H

 uPv1& P u !#w xN/*  w[!0x


3
% 0z"tuPvX( P u
=z.yvu& /=PPuvY![3(R 

 u	W/*y100TuPz'v,v	! &'")(%+* ( 3 *P	& % :
( } ov 

 zvr~*u}{uPziuQwY0  0Pu$1vr~*uPwYuey1}z!N-"w[! -G!; yvy0!0z"0!0wxN/* 
QQ

fif.w~3}33f$3ws

vr~*ue yFEu%![ 
 yo.-G!0 40z!0x2y#1 4 P,!D/*z"tutyrz A &ABA (A 1 /*z.u   fi  
,!#z6 ymtuQwuT
t /z'
y #uP 4eyrz ,!0x.-"u32ffyrv54vr~*u,!0wI4 K 

   JI|8~y0Q~Ly1

a'RMXKMN.UT[?Ma.]q4M3[a._0[OQ]ff^eW'N6[RZ6fh_#Z K*W'UROPR[XKPW'XKMYKZ[_0[RT]ffZL]#b &')"( *( 3 *P	& % (:N'Z.c6KPX
[a6Kebr]#X^ ]#b(_W.X]#W/]ffMRT[R]ffZ._#Ub1]#X^&N.U_RMa._0Xc `KPS#KZ RZ_OQ]ff^W"RU_0[RT]ffZ*Id"_#MYKc_0W.W.X]ff_#Oaqa6KPXK
[a6K[Ro^KZ6KPKc6Kch[Y]c6KPXRTS#KMN'Oa_br]#X^&N.U_2RMZ.KPfffUTKOQ[YKcBj
8RZ._#UoUTV#`*q+Ka'_S#K}_#UoMY]ec6KPXRTS#Kc C
=<
 #(0  / '


 " ; uv

7


	 	  )ff
 

ru uwuw{



<mq    u=>)yC fi pz3r %qa#1nz3q{ ; q('
0z"t fi  2 0  /' 
	 	  ff) 
  0 wYu fi  - O ,!0x.-"uPvu 

RG  D B3:H	H6r}O D B? D F#H]}O D B
p%Z[a'RM(MYKOQ[RT]ffZB`*q:K3Ma6]q [a._0[,MYKPS#KPX _#UBZ6]#[RT]ffZ.M:]#b9c.KPWiKZ'c6KZ.OQK3RZ[YX]c.N'OQKcRZ[a6K?UR[YKPX_0[N6XK3_0XK
KW"N.RTS0_#UTKZ[4[Y]6`i]#X3OP_#ZdiK}KW'XKMMYKcRZ[YKPX^M4]#b`iMK^e_#Z[RoOP_#U9RZ'c6KPW/KZ.c6KZ.OQK#j:pZW'_0X[ROPN'U_0X`.q:K
Ma6]	q [a'_0[ A:]ffN6[RUoRTKPX+ M4c.K  Z.RT[RT]ffZ]#b\RZ 'N.KZ.OQK_0d'RUR[V5 A:]ffN6[RoURTKPX` # <RM4RZbr_#OQ[WK "N.RTS0_#UTKZ[[Y]
MYK^e_#Z[ROP_#U1\ S4c.KPWiKZ'c6KZ.OQK#ja6Kc6K  Z.RT[R]ffZ2]#bBXKUTKPS0_#Z.OQK_#M+fffRTS#KZdV @9_0#K^KPV#KPX[ # #:OP_#Z
_#UMY]d/K\W.X]S#Kc2[Y]diK:WK "N.RTS0_#UTKZ[9[Y]? Sc6KPW/KZ.c6KZ'OQK#j Z.K+]#b.[a.K+[q+]?c6K  Z'RT[RT]ffZ.MfffRTS#KZ&dV @9_0#KQ
^KPV#KPX[ # #b1]#X3MY[YXRoOQ[XKUTKPS0_#Z.OQK2OP_#Z_#UM]d/KK *W.XKMMYKcRZ[YKPX ^eM4]#b= Sc6KPW/KZ.c6KZ.OQK#j4a6KMYK
XKMN'UT[M_#UUT]	q b1]#X  Z.c.RZ6f[a6KOQ]ff^W"UTK *R[V&]#bi_#UU*[a.KMYK,b1]#X ^eM8]#bBc6KPW/KZ.c6KZ.OQK(_#M\_?c.RXKOQ[=OQ]#X]ffUU_0XV
[Y][a.KOQ]ff^W'UK *RT[V XKMN.UT[MXKPW/]#X[YKcRZL[a.KW.XKPS*RT]ffN.MMYKOQ[RT]ffZBj6]#X&[a6KM_0#K]#bOQ]ff^W'UTKP[YKZ.KMMP`
q:K}_#UMY]fffRTS#K[a.K}OQ]ff^W'UK *RT[Vh]#b[a6K]#X RTfffRZ._#UBc6K  Z.RT[RT]ffZh]#b=M[YXROQ[XKUKPS;_#Z.OQK5 @_0#K^KPV#KPX` # ff
rq4a.RoOaLRMZ.]#[c.RTXKOQ[UVXKU_0[YKc[Y] S4c6KPW/KZ.c.KZ.OQK	 `q4a'ROa [N6X Z.M?]ffN6[?[Y]d/KeOQ]ff^W"N6[_0[RT]ffZ._#UUV
MR^eW'UTKPX([a._#Zh[a6K}MN6d'MYWK "N6KZ[,c6K  Z.RT[R]ffZfffRS#KZdL
V @9_0#K^KPV#KPX2[ # # j
ff8AC;Dfi

ml k {m#q{ ; qz$m' 
A:]ffN6[RURKPX2[# <RZ[YX]c.N'OQKM_Z6]#[RT]ffZ ]#b:RZ'N.KZ.OQK_0d'RUR[V#jE]ffN6fffa.UVMYW/K_0RoZ6f6`B_br]#X^&N.U_T&sRM
RZ "N6KZ.OQK_0d'UTKb1X]ff^ _MYKP[,]#bS0_0XR_0d'UKM U RTb [a.KPXK?K*RoMY[M,_2MOQKZ._0XRT]eRoZqa.ROa[a6K3[YXN.[aS;_#UN.K]#b
& c6KPW/KZ.c'M+]ffZ[a6K?S0_#UN6K?]#b8[a6K?S0_0XR_0d'UTKM,RoZBUej=4a.RM,Rc6K_OP_#ZdiK?br]#X^e_#UoRTgPKch_#Mbr]ffUUT]	q4MPj
  /=PPuv.![
k  " '{<q{ ; qsz3'' '  uPv & PuVM!0wxN/T  w !0x
3
  0 z"t+U 
& y1RZ'N6KZ.OQK_0d'UK w[!0x U y  0 z"t6!#z. 4 y vr~*uPwYu u32ffyoQvr ]
fi NU O| 0! wTtG= 10z"t v| !
U O| 0! wTt;D= ( 0z"t,= - 1v 6=/
= ( A ! & 0 z"t=/= - A !O: &~ff!0Tt 


fi

!

q m{uw{

p%Z ]#[a6KPXq:]#Xc.MP`[a6KPXKLRM_ MOQKZ._0XR] = RZ q4a.RoOa [a6K b1]#X ^}N.Uo_ & OP_#Z d/K[YXN6K ]#Xhb_#UMYK#`
6c KPW/KZ.c.RoZ6f]ffZ[a6KS;_#UN.Ke]#b([a6KS0_0XR_0d'UKMRZ UejJ a.RoUTKeRZ'N.KZ.OQK_0d'RUR[VU]]#*Mc.RiKPXKZ[?brX]ff^
[a6Kc.K  Z.RT[RT]ffZ'MfffRS#KZRZL[a'RM3W'_0W/KPX`9R[OP_#Zd/KeMa6]	q4Z[a._0[}RZ br_#OQ[&RZ'N.KZ.OQK_0d'RUR[VOQ]ffRZ'OPRc6KM
q4RT[ah Sc6KPW/KZ.c6KZ.OQK#j
 uPv & Pu M!0w xN/*T  w !0x7
3
% 0z"t U I/=PQPuPv![
fffi B& yoeyrz /ffO
uQz  u  PQu1 w !0x U y #z"tT!0z. 4y .& yo .0w OYtuR-6uPz"tuPz'v !0zBU
=<



ru uwuw{

7



fiM2_OQ]ffZ.MKW"N6KZ'OQK#`=_^e]c6KUTI[a6KP]#XKP[ROOa._0X_#OQ[YKPX RTg_0[RT]ffZ ]#b4RZ'N6KZ.OQK_0d'RoURT[V OP_#Z d/KK_#MRUV
c6KPXRS#KcLb1X]ff^ [a6Ke]ffZ6Kb1]#X&SRZ.c.KPWiKZ'c6KZ.OQK#ja.KeOQ]ff^W'UTK6RT[V ]#b,RZ"N6KZ.OQK_0d'RUoRT[VRoM_#ZLK_#MYV
OQ]#X]ffUUo_0XV[Y][a.RM(W.X]#W/KPX[VC  ) fi  '  )ff
  ff
    # RoM  .OQ]ff^W"UTKP[YK#j

Q

fi



~33

s~$w

~ww$

8l 7 Kqqnz3{ ; q
@_0#K^eKPV#KPX}[#`##:RZ[YX]*c.N.OQKM:MYKPS#KPX_#U/br]#X^eM:]#bXKUKPS;_#Z.OQK#jJLK?Ma.]q a6]	q [a.KMYK3b1]#X^M+]#b
XKUTKPS0_#Z.OQK_0XKeMY[YX]ffZ6fffUTVXKUo_0[YKc [Y] SRZ.c.KPWiKZ'c6KZ.OQK#jJLKe_#UM]OQ]ff^eW'UTKP[YK[a6K2XKMN'UT[MfffRTS#KZLRZ
5 @_0#K^eKPV#KPX` # # `dVK 6a.RTd'R[RZ6f[a.KeOQ]ff^W'N6[_0[RT]ffZ'_#UOQ]ff^W'UTK 6RT[V]#b:K_#Oabr]#X^ ]#b:XKUTKPS0_#Z.OQK
RZ[YX]*c.N.OQKchRZ5 @9_0#K^KPV#KPX` # # j

 ! ' 

	  
 # $# #  2
@_0#K^eKPV#KPX+ M,Z6]#[RT]ffZ]#b XKUTKPS0_#Z.OQK]#b_br]#X^&N.U_[Y]&_&MN6dGFYKOQ[(^e_0[Y[YKPX,OP_#Zd/Kc6K  Z6KcRZe[YKPX^M+]#b
W.XRo^K?R^W'URoOP_0[YKM(]#b9[a6Kb1]#X^&N.U_*`._#Mb1]ffUU]q4MMKPK K  Z.RT[RT]ffZeRT
Z @_0#K^KPV#KPX`##IC
q!m{uw{ k'k " rqqnz3{ ; q u z fiffq ; [vxzqsr'  uPv & Pu M!0wx /*T w !0x 
3
%0zit
U I/=PuPv ![c
fi  & y1XKUTKPS0_#Z[&[Y] U y h0z"t !0z. 4y vr~6uQwu,u 2#y1vrL
 -"w yrxuyr.
x -"
0y ffvB
u ![ &
xuQz'vy0!0z.yrz{h;#w ymQPQu [w !#
x U





z3v

  	 /)ff
  1 fi  fi 132  'ff 

<q k

# 1

 uPv &"! /)V#z"t U$! #  % >S +Y& yowuQuQ00z'v(v	! U,

fiM_OQ]ffZ'MYWK "N.KZ.OQK#` @9_0#K^KPV#KPX+ M&Z6]#[RT]ffZ]#b:RTXXKUTKPS0_#Z.OQK]#b,_hb1]#X ^}N.Uo_[Y]_MN.dGF%KOQ[^_0[Y[YKPX
OQ]ffRZ.OPRoc6KM(q4RT[ah SRZ.c6KPW/KZ.c6KZ'OQK#j


 uPv & P u&.!0wxN/*  w[!#x2

 0z"t U  /PQPuv ![/
fffi  &y1wuQuQ;#z'v
ru=<uwuw{ 7
v	! U y 0z"tT!0z. 4y  & oy  .# wIOYtuR-.uQz"tuPz'v !0zBU


 aN.MP`[a.K^e]c6KUTI[a6KP]#XKP[ROOa'_0X_#OQ[YKPXRTg_0[RT]ffZ ]#b S4RoZ.c6KPW/KZ.c6KZ.OQK_#UMY]L_0W.W"URTKM}[Y] RXXKUTKQ
S0_#Z.OQK2]#b\_b1]#X^&N.U_[Y]h_MN.dGF%KOQ[3^e_0[Y[YKPXj?JLK_#UM]a._S#K[a._0[[a6KRTXXKUTKPS0_#Z.OQK}]#b\_b1]#X^&N.U_[Y]
_eMN.Gd F%KOQ[^_0[Y[YKPXOQ]ffRZ'OPRc6KM(q4RT[T
a A:]ffN6[RoURTKPX+ M,c6K  Z.RT[RT]ffZ]#b8RZ 'N.KZ.OQK_0d'RUR[V#j8RZ._#UUTV#`*[a6K}_0di]	S#K
W.X]#W/]ffMR[RT]ffZ_#UUT]qM(b1]#X4_#ZK_#MYVW'X]]#b]#b8OQ]ff^W'UTK 6RT[Vbr]#XXKUTKPS0_#Z.OQK#`.Z._#^KUV#` 2   	 / )ff
  1 fi-
fi 132  '  # 1 ;! ' 
	  
 # $ # #  2 RM  .OQ]ff^W"UTKP[YK#j

 ! 'ff
	  
 # $# #  2
@_0#K^eKPV#KPX4a._#M:RZ[YX]c.N'OQKc[q+]2br]#X^eM:]#bvwy0vwuQuQ00z uPj=a6KOa6X]ffZ6]ffUT]#fffROP_#UUV6  XMY[+]ffZ6K3a._#M
d/KPKZhfffRTS#KZRoZ5@_0#K^KPV#KPX` #ff `"_#M4b1]ffUoUT]q4Mj
k   '  uPv & Pu
q!m{uw{ kff7 " rff ;  rqqnz3{ ; qu z fiffq ; Kv zqsr #y?zwqvxq qsr
M!0w N
x /*TN
  w !0x 
3
% 0z"t U  /PQPuv ![
fi  & y1MY[YXROQ[UTV XKUKPS;_#Z[[Y] U y 0z"t !0z' 4y 
uQ#uQw 4-"wy1xu&yr.x -"5y  #vu ![ & ,!#z'vI0yrz6&;0wymQ PQ1u  w !0x U,
@9_0#K^KPV#KPXa._#M_#UMY]LRZ[YX]c'N.OQKc _#Z6]#[a6KPXZ6]#[R]ffZ ]#bMY[YX ROQ[2XKUTKPS0_#Z.OQK5 @_0#K^KPV#KPX` # # `
^]#XK}c6K^e_#Z.c.RoZ6f}[a'_#Zh[a6K]#XRfffRZ._#Ui]ffZ6K#j KPXKq:KOQ]ffZ.MRoc6KPX_#ZWK "N.RTS0_#UTKZ[c6K  Z.RT[R]ffZBj
 # 2

 
 # 2   	 /)ff
  1 fi  fi 132  'ff 

# 1

k   ('  uPv& P u
q!m{uw{ k	H " rff ;  rqqnz3{ ; qu z fiffq ; Kv zqsr #y?zwqvxq qsr
M!0w xN/*TN w !0x 
3
  0z"t U  /PQPuv ![
fi  & 1y MY[YXROQ[UTV XKUKPS;_#Z[[Y] U y 0z"t !0z' 4y 
vr~*uPwYu?u,#2 y1vr4 -iw yrxuyrx.-"y5 #vu![1& xeuPz'vy0!0z.yrz{200w ymQ PPTuw[!#x U 10z"t2uQ#uQw 4 -"wy1xuyrxY-i
y0#vu
![ & xuQz'vy00! z6N!0z. 4;#w ymQPQu1 w !0x U,

Q 

fif.w~3}33f$3ws

A:]#[ac6K  Z.RT[R]ffZ.MW.XKPS#KZ[\[_#N6[Y]ffUT]#fffRKM+_#Z.cOQ]ffZ[YX_#c'ROQ[Y]#XVbr]#X^}N'U_#MbrX]ff^d/KRZ6fMY[YX ROQ[UTV&XKUTKQ
S0_#Z[=[Y]}_#ZV2MYKP[=]#biS;_0X R_0d'UTKMPj8a6K(d'_#MRO,c.R:/KPXKZ.OQK(d/KP[q:KPKZe[a6KMK[q:]c6K  Z'RT[RT]ffZ.M8RM8[a._0[+RZ2[a6K
 XM[\]ffZ6K3q:Kq(_#Z[:[a'_0[+KPS#KPXVW.XR^eKR^W"UROP_0[YK4]#b & OQ]ffZ[_#RZ.M?#v=Tu;vB_}S;_0X R_0d'UTKb1X]ff^ Ue`*q4a.RUTK
RZ[a6K&MKOQ]ffZ.c OP_#MYK2q+K2R^W/]ffMYK}[a._0[KPS#KPXVW.XR^eK}R^W"UROP_0[YK]#b & ^&N.MY[3OQ]ffZ[_#RZ !0z' 4}S;_0XRo_0d'UTKM
brX]ff^ U > 7j fiM,[a.Kb1]ffUoUT]q4RoZ6f2K *_#^W"UTKMa6]	q4MP`6[a6KPXK_0XKb1]#X ^}N.Uo_#M(b1]#Xqa.ROa[a.K[q:]c6K  Z'RT[RT]ffZ.M
]#bMY[YXROQ[,XKUTKPS;_#Z'OQKc6]Z6]#[OQ]ffRZ'OPRc6K#j

 uPvV& !  HKSQ0z"t U ! # + ~*uQwuy1 !0z. 4#!#ziu -"wy1xuyrx.-"
y0ffvu ![L&1
z"0xuP 4+HS  .y1z  uyrv ,!0z'vI#y1z6#v\Tu;v:e00wyQPPTu ![YU 1=yrv!0r !0|8vr~#v &y1Qvw y0v 4ewuQuQ;#z'v
v	!%U | ow1v J   #uPxu4 uQw 1 K  !0|+uP0uQw 1( yrz uvr~*u-"wy1xuy1x.-"y0#vu,H_Syo&z!#v ,!0x.-G!;ut
]ffZ.UTV]#b400wyQPPTuQ ![VU J P u3 D/Pu3S ) 4 U K 1\yv !0r !0|8&vr~ffv & y1z!#v+Qvw y0v 4wuQuQ00z'v,v	! U | ow ov 
J   #uPxu4uQw 1  [K 


z3v

<q k

	

 a.X]ffN6fffa SRZ.c6KPW/KZ.c.KZ.OQK#`q:K+OP_#Z2c6KPXRS#K\_#Z}_#U[YKPXZ._0[RTS#K:Oa._0X_#OQ[YKPX RTg_0[RT]ffZ&]#b6[a6K+Z.]#[RT]ffZ]#b
vwy0v=wuQuQ00zu(RZ[YX]c'N.OQKcdV @9_0#K^KPV#KPX&[## j:p%Z.c6KPKcB`*_#M,_MY[YX_#RTfffa[Ybr]#Xq:_0X cOQ]ffZ'MYKW"N.KZ.OQK
]#b8[a6Kc6K  Z'RT[RT]ffZB`q+K}a._S#K C
 I/=PuPv ![6
fi  & y1Qvw y0v 4
 uv & Pu!#w xN/*   w !0xQ

  0zit%U 
wuQuQ;#z'v9v	!VU y 40z"tN!0z. 4&y  & yo .0w OYtuR-6uPz"tuPz'v !0z U 0 z"t .0w Oyrz"t u -6uQzituQz'vffw[!#x U,.	&  U
=<



ru uwuw{

7







J K&a._S#K&Rc6KZ[R  Kc[a6K&OQ]ff^W'UTK6RT[V]#b8d/]#[ac6K  Z'RT[RT]ffZ.M:]#bMY[YXROQ[4XKUTKPS0_#Z.OQK#`"_#Z.c[a6KPV[N6XZ
L
]ffN6[[Y]diK?c.RiKPXKZ[MC=[a6K  XMY[c6K  Z'RT[RT]ffZRM,a._0Xc.KPX[a._#Zh[a6KMKOQ]ffZ.c]ffZ6K#j

ru=<uwuw{ 7  " ; uv <mq    u=>)r ; [rqqnz3{ ; q('
JK! # 2  
 # 2   	 /)ff
  1 fi- fi 132  ff'   # 1  ! 'ff
	  
 # $# #  2 J   #uPxeuM4uQw 1  Ky1
fi  - O[I!0x.-"Tuvu 
J WG
K ! # 2  
(# 2   	 /)ff
  1 fi  fi 132  ff'   # 1  ! ' 

	  
 B
# D$# #  2 J   #uQxuM4ffuPw 1 [K y1
" - O[I0! x."- uPvu 


a.KMYKOQ]ff^W'UTK6RT[V XKMN.UT[MhR^eW.X]S#K4a6KP]#XK^ 0 b1X]ff^ 5@_0#K^KPV#KPX`L## `q4a'ROa ]ffZ.UV
W/]ffRZ[M]ffN.[[a.K  'a._0Xc.Z6KMM3]#b\RTXXKUKPS;_#Z.OQK2_#Z.c MY[YXRoOQ[3RXXKUTKPS0_#Z.OQK2_#M?c6K  Z6KcRZ 5@_0#K^KPV#KPX`
# # j
D F  \G 
-$mf D 0HIE D 
p%Z [a'RM2MYKOQ[RT]ffZB`=q:K  XMY[2c.RMOPN.MM}]#[a6KPXXKU_0[YKc q:]#Xi`=[a6KZ YM ]ff^KW/]ffMMRTd"UTKK*[YKZ.MRT]ffZ'M}]#b[a6K
Z6]#[RT]ffZ'M_#Z.cXKMN'UT[M,q:Ka._S#K}W.XKMYKZ[YKchd/KPb1]#XK#j

-$ ]
 l'Ffiff E

8A	  




ml k NZ*?#qsr [qzq%L uwr 
fiM&_#UXK_#c6VKPS#]##KcB`(RZ.c.KPWiKZ'c6KZ.OQKa._#M&d/KPKZ OQ]ffZ.MRc6KPXKc N'Z.c6KPXS0_0XRT]ffN'M}br]#X^eM2RZ S0_0XRT]ffN.Mfi4p
 KUc'MPj

Do Dz} -z}u|uffu+RffO$$puTu'Uu z rn *-y |~|y lu+syx#ffc}yluT:z}y ff |ffz}u|{}u~}y3 y ffUz}yp||| fi
$8y}|uz n !ff !#"6z%$& n&''( *o
 

Q 6

fi

~33



s~$w

~ww$

 ) 	   ) ff )ff
   )

 2 1  1 !  #  1 ) ff  1  

a6KPXK&_0XK]#[a.KPX?b1]#X^M]#b:RZ.c6KPW/KZ.c.KZ.OQK}RZW.X]#Wi]ffMRT[RT]ffZ._#U9UT]#fffRO&[a._0[3q+Kea'_S#KeZ6]#[?OQ]ffZ.MRoc6KPXKc
RZ[a.RM_0X[ROPUTK#`8KMYW/KOPR_#UUTV#`8c6K  Z'_0d'RURT[V#`9OQ]ffZ[YX]ffUU_0d"RURT[V 5 @9_#Z6f fi _0,X "N.RMP` # ;di}_#Mq:KUU:_#M
OQ]ffZ.c.R[RT]ffZ._#U#RZ.c.KPWiKZ'c6KZ.OQK 3_0Xq4RO a6K#` # # jpb Al` C _#Z.c  _0XK\[a6XKPK\c.RoM F%]ffRoZ[ MYKP[M]#b*S;_0XRo_0d'UTKM
_#Z.c & RM_*Z6]	q4UTKc6f#K}d'_#MYK[a.KZ A _#Z.
c C _0XK&OQ]ffZ'c.RT[RT]ffZ._#UoUTVRoZ.c6KPW/KZ.c6KZ[(qjXj[Pj  *Z6]	q4RZ6f
& RTb_#Z.c]ffZ.UVRbbr]#X3_#ZV  Iq:]#XUc = ff8`i]ffZ.OQKq:K}*Z6]	q = ff _#Z.c & `iUTK_0XZ.RZ.fMY]ff^KP[a.RoZ6f_0d/]ffN6[
A OP_#Z.Z6]#[^e_0#K N.MhUTK_0XZ _#ZV[a'RZ6f Z6KPq _0d/]ffN6k
[ C _#Z.c	5y uL#uQw Q j a6KLOQ]ff^W'N6[_0[R]ffZ._#U
RMMN6KM2W/KPX[_#RZ.RZ.f[Y] OQ]ffZ.c.RT[RT]ffZ'_#U,RZ.c6KPW/KZ.c6KZ'OQK_#Z.c [Y] MY[YX]ffZ.f#KPXZ6]#[RT]ffZ'Me_#Meq:KUU,_#MeXKU_0[YKc
Z6]#[RT]ffZ'M:MN.Oa_#M(XKUTKPS0_#Z.OQK3d/KP[q:KPKZhMN6Gd FYKOQ[(^e_0[Y[YKPXM5 @9_0#K^KPV#KPX` # #(_#Z'cZ.]S#KUT[V 	 
XKRZ6KPX
fi 
3KZ6KMYKPXKP[aB`     `a'_S#K\d/KPKZK [YKZ.MRTS#KUTV?MY[N.c.RKcRZ_OQ]ff^W'_#Z.RT]ffZW'_0W/KPX+5 @_#Z6f6` @9RTd/KPX_0[Y]#XK#`
fi_0,X "N.RMP` 0# ff j
 

   
  #

/ )     
  # 	   ) 	 )ff
 
KPS#KPX_#U6_0W'W.X]ff_#Oa.KM8[Y]?diKUoRTKPb.Oa._#Z.f#K^e_0#K,N.MYK(]#b"_K*W'UROPRT[c6KPW/KZ.c6KZ.OQK:XKU_0[R]ffZB`0q4a.RO a&^eK_#Z.M
[a._0[RT[(RM:W'_0X[(]#b9[a6K?RZ6W'N6[3rq4a.RUK]ffN6X M:RoM:Ro^W'UROPR[P`RmjK#jT`6c6KPXRS#KcbrX]ff^ [a6K?RZ.W'N6[  jaN.MP`*OQ]ff^2
W'N6[RoZ6fRZ.c.KPWiKZ'c6KZ.OQKhXKU_0[R]ffZ.MbrX]ff^ _*Z6]qUTKc6f#Kd'_#MYKOP_#Z d/KMKPKZ _#M_#Z N6W'MY[YXK_#^ [_#MY
KZ._0d'UoRZ6fN.M}[Y]LMYW/KOPRTbrV[a.K JYOQ]#XDK L ^eRoZ.R^e_#U1?RZ.c6KPW/KZ.c6KZ'OQKXKU_0[RT]ffZ N6W/]ffZq4a'ROa [a6KdiKUoRTKPb
Oa'_#Z6f#K2]#WiKPX _0[Y]#XRM4d"_#MYKcBB[a.RMOQ]#XKRZ.c.KPWiKZ'c6KZ.OQKXKUo_0[RT]ffZ OP_#Z[a.KZd/K2OQ]ff^W'UTKP[YKcdVMYW/KO
RTbrVRoZ6fK *W'UROPR[UTVMY]ff^K_#c.c'RT[RT]ffZ._#U:c6KPW/KZ.c6KZ.OPRKM}N.MRZ6fZ6]	q4UTKc6f#Kh_0d/]ffN6[2[a6Kc6]ff^_#RZBX
j *N'Oa
_#Z_0W'W.X]ff_#Oa a._#MdiKPKZW.X]#W/]ffMYKcbr]#Xd/KURKPb\XKPS*RMRT]ffZLRoZ ._0X	R Z' _#M}c6K1U ?:KPXX] fi 4KPXgRTf6` # ff `
br]#Xd/KURKPb=N6WBc._0[YK}RZ ; _0,X "N.RMP` # <3_#Z.c  KPXgRTf6` # ff_#Z.cbr]#XXK_#MY]ffZ.RZ6f_0d/]ffN6[3_#OQ[RT]ffZLRZ
 KPXgRT%
f fi ER  ` ##ff j
 

ff
fi ) 	   ) ff )ff
 

/)  
 1 )*#  # !
?:]ffZ[YK[N'_#U,XK_#MY]ffZ.RoZ6f 	
?a.Rc.RZ'R fi 
3RoN.Z.Oa'RTfffUR_*`ff0#2a._#M2d/KPKZ RZ[YX]*c.N.OQKc br]#Xb1]#X^_#URTgRZ6f
c6]ff^e_#RoZ.MRZ2q4a.RoOa*Z6]qUTKc6f#KOP_#ZZ'_0[N6X_#UUTV}d/K(c'RTSRoc6Kc2RZ[Y]W'_0X[M4OQ]ffZ[YK[M j\\_#O aOQ]ffZ[YK[:RM
Oa'_0X_#OQ[YKPXRTgPKchdVRT[M(]	q4ZU_#Z6fffN._0f#K?_#Z.c_#UTW"a._0d/KP[Pj84a6K3Z6]	q4UTKc6f#Kd'_#MYK3]#b_&OQ]ffZ[YK *[4OQ]ffZ[_#RoZ.M
q4a._0[3RMXKUTKPS0_#Z[3[Y]_W'_0X[]#b=[a6K2c6]ff^e_#RZBj]q:KPS#KPX`RT[3RMZ6]#[fffN'_0X_#Z[YKPKc [a'_0[[a.K&c.R iKPXKZ[
W'_0X[M&]#b[a6Khc6]ff^e_#RZ c6]LZ6]#[RZ[YKPX_#OQ[P`\MY]LRZ.b1KPXKZ'OQKRZ ]ffZ6KhOQ]ffZ[YK [e^_V diK_ /KOQ[YKc dV [a6K
*Z6]q4UKc6f#K?]#bMY]ff^K]#[a6KPXOQ]ffZ[YK *[Pj
a.K^e_#RZ c.R iKPXKZ'OQKd/KP[q:KPKZ OQ]ffZ[YK [N._#UXK_#MY]ffZ'RZ6f_#Z.c RZ.c6KPW/KZ.c6KZ'OQKRoM[a._0[e[a6KU_0[%
[YKPX}RoM_MY[N.c.V]#b([a6KeXKUKPS;_#Z.OQKXKU_0[RT]ffZL[a'_0[OP_#Zd/Kec6X_q4Z brX]ff^ "
_ J['_0[ L RmjK#jT`8Z6]#[c.RTS*Rc6Kc
RZ[Y]OQ]ffZ[YK [M \*Z6]qUTKc6f#Kd'_#MK#q4a.KPXK_#M+OQ]ffZ[YK *[N._#U'XK_#MY]ffZ.RZ6f}RM]ffZZ6]	q4UTKc6f#K_0di]ffN.[\MYW/KOPR  O
OQ]ffZ[YK [MP`/[a._0[3RMP`"Z.]q4UTKc.f#K}RM4K W.XKMMYKcdVMYW/KOPRTbrVRoZ6fq4a.RO aOQ]ffZ[YK [?RT[4XKPb1KPX M4[Y]6jp%Z]#[a6KPX
q:]#Xc.MP` [a.KXKUTKPS0_#Z.OQKXKU_0[RT]ffZLRoM?_hXKMN.U[]#b:XK_#MY]ffZ'RZ6fh_0d/]ffN6[?Z.]q4UTKc.f#KeRZLMY[N.c6V*RZ6fc6KPW/KZ*
c6KZ.OQVi6]ffZ[a6K?]#[a6KPXa'_#Z.cB`6RT[,RM(]ffZ6K3]#b[a6Kc._0[_2[a._0[a._#M([Y]d/K3W.X]	SRoc6Kcb1]#X,XK_#M]ffZ.RZ6f_0d/]ffN6[
OQ]ffZ[YK [MPj
 

  	 fi  )  #  1 ) 1 fi  2 2   	 3)ff
  
   	   # ff  

 

 a6Kc6K  Z.R[RT]ffZL]#bRTXXKUTKPS;_#Z'OQKfffRTS#KZ dV @KPSV#`=8RT#KM`_#Z.c *_0fffRTS [##&_#Ro^eM_0[&KMY[_0d'URoMa.RZ6f
q4a.RoOabr_#OQ[M]#b+_*Z6]q4UKc6f#K2d'_#MYK_0XKeRTXXKUTKPS;_#Z[[Y]h[a6Kc6KPXRTS0_0[RT]ffZ]#b+_T"N6KPXV#j}pZW'_0X[RoOPN.U_0X`
[a6KPVLOQ]ffZ.MRoc6KPX_  X MY[%I]#Xc6KPXUT]#fffRoOq4RT[aLZ6]brN.Z'OQ[RT]ffZLMYV*^d/]ffUM?_#Z.c _MYKP[]#b,RZ.b1KPXKZ'OQKXN.UTKMPj fi
*Z6]q4UKc6f#K4d'_#MKRM+_}MYKP[+]#bOPUT]ffMYKcbr]#X^}N'U_#Mrbr]#X^&N.U_#M=qRT[aZ6]}b1XKPK4S0_0XR_0d'UTKM j KPXRS;_0[RT]ffZ]#b_

Q +

fif.w~3}33f$3ws

"N6KPXV_#Z6]#[a6KPX:OPUT]ffMYKcebr]#X^&N.U_8RoM]#d.[_#RZ6KcdV_0W.W"UTVRoZ6f3[a6K4RZ6brKPXKZ.OQK4XN.UTKM[Y]}[a6K*Z6]qUTKc6f#K
d'_#MYK}_#Z.c[a6KUT]#fffRoOP_#U _ *R]ff^eM,]#b[a6K[a6KP]#XV#j
fi br]#X^&N.U
_ ]#b[a.K*Z6]	q4UTKc6f#Kd'_#MYKRM2RTXXKUKPS;_#Z[[Y] [a6Kc6KPXRTS0_0[RT]ffZ ]#b4_#Z6]#[a6KPXbr]#X^&N.U_
7RTb c6]KMZ6]#_
[ J%W'_0X[ROPRTW"_0[YDK Lh[Y][a6KeW'X]OQKMM?]#b,RZ6brKPXXRZ6f
7 brX]ff^ [a6KZ.]q4UTKc.f#Ked'_#MYK#j.]#X
K6_#^W'UTK#`#RZ[a6K+*Z6]	q4UTKc6f#K:d'_#MYK #i\&[  % 
 ; ff % \& ALT I ^h A  +ff`RT[RMOPUK_0X[a'_0%[ \2[ 9RMXKUTKPS;_#Z[
[Y] ^h[  `.q4a'RUTK 
 ; ff(RoM,Z6]#[Pj
a'RMBc6K  Z.R[RT]ffZdiKOQ]ff^eKM OQ]ff^W'URoOP_0[YKc?q4a6KZ?^]#XK\OQ]ff^W'UTK ?MOQKZ._0X RT]ffM _0XK=OQ]ffZ.MRc6KPXKcB%j 4_#^eKUTV#`
@ KPSVKP[4_#Umj\OQ]ffZ.MRc6KPX,[a6XKPKc':R iKPXKZ[ JYOQ]]#Xc.RZ'_0[YKM LGC  XM[P`6q4a6KP[a6KPX_#UUBc.KPXRTS0_0[RT]ffZ.M,_0XKOQ]ffZ'MRc*
KPXKcL]#X FYN'MY[?]ffZ6K#MYKOQ]ffZ.cB`9q4a6KP[a6KPXq+KOQ]ffZ'MRc6KPX_#UoUc6KPXRTS0_0[RT]ffZ.M3]#X FYN.M[^eRZ.R^_#U9]ffZ.KMP8[a.RTXcB`
q4a6KP[a.KPXq+KOQ]ffZ.MRc6KPX}^K^d/KPXMa.RTW [Y][a6KW.X]]#b+]#X FYN.M[c6KPXRTS0_0d'RUR[VbrX]ff^ [a6Kb1]#X^&N.U_#M[a._0[
OQ]ff^W/]ffMYK[a.K?W.X]]#b(RZ[a'RMOP_#MYK#`.q:Ka._S#Kbr]ffN6X,W/]ffMMRTd"UTKO a6]ffROQKM  j
A:KMRc.KM[a6Kb_#OQ[}[a._0[&[a.RM}Z6]#[RT]ffZ ]#bRTXXKUKPS;_#Z.OQKRMd"_#MYKc]ffZ  XM[%I]#Xc6KPX2UT]#fffRO0`8[a6KPXK_0XK
]#[a6KPX`^e]#XKMN6d"MY[_#Z[Ro_#Um`,c.:R /KPXKZ.OQKMd/KP[q:KPKZ RT[_#Z.c [a6KRc.K_#MRZS#KMY[RTfff_0[YKc RZ [a.RoMeW'_0W/KPXj
8RTXMY[P`iRT[RoM4_XKU_0[RT]ffZdiKP[q+KPKZ[q+]b1]#X ^}N.Uo_#MP`'fffRTS#KZ_d'_#Of#X]ffN.Z.c*Z6]q4UKc6f#Kd'_#MK#j fiMMN.O aB`
RT[RM4^]#XK}XKUo_0[YKc[Y]]#[a6KPX?Z6]#[RT]ffZ.M4]#b=XKUTKPS0_#Z.OQK&RZ[a6K2URT[YKPX _0[N6XK5 @9_#Z6+
f fi _0,X "N'RMP` # 0_ j
KOQ]ffZ.c `ffRT[RMd'_#MYKc2]ffZ2[a6K,OQ]ffZ.OQKPW.[]#b"W.X]]#b`#qa.ROa2RM8MY]ff^eKP[a.RZ6f?Z6]#[OQ]ff^W'UKP[YKUTV}c6KPW/KZ.c.KZ[]ffZ
[a6K2MYK^e_#Z[ROPMPj?6]#X?K 6_#^W'UTK#`iXKPW'Uo_#OPRZ6f[a.K&N.MN._#U^]*c.N.MW/]ffZ6KZ.M4q4RT[a[a6K&M[YX_#Z6f#KRZ6brKPXKZ.OQK
XN.UK  %ff%  I 1  19`iq4a.RoOac6]KM4Z.]#[Oa'_#Z6f#K}[a.K}MYK^e_#Z[ROPM4]#b[a6K}UT]#fffRoOPMP`'[a6KZ[a6Kbr]#X^&N.U_

 ; ff]#b,[a6KZ6]	q4UTKc6f#Kd'_#MYK_0di]	S#Kd/KOQ]ff^KM&^e_0fffROP_#UoUTV XKUTKPS0_#Z[&[Y_
] ^h[  j4a.RMRMW/KPXb1KOQ[UV
XK_#MY]ffZ'_0d'UTK3RZ[a.K3_0W'W.X]ff_#Oad%
V @KPSVeKP[,_#UmjT`*q4a.KPXKRo^W.X]	SRZ.fKPOPRTKZ'OQVN.MRZ.f}_2MYW/KOPR  O4W.X]]#b
[a6KP]#XVhRM([a.K_#R^hj
8l 7  q{MLm'{|xuw#r u$u{ zw{ML [q
a6KZ.]#[RT]ffZ.M_#Z.cXKMN.UT[M(W.XKMYKZ[YKcRZ[a.RM(W"_0WiKPXOP_#Zd/K?K[YKZ.c.KcRZhMYKPS#KPX_#U9c.RTXKOQ[R]ffZ.MPj

 '0  2  
 ff/) 



fi 132   # #  )   )

1/2
1/2

ff ) ff) 1 #  1 )M! 1 fi fi 1/2 '
   )  2 ff ! # 2 'ff
 # ' 2  !

0	  2

 ff
    ) 	   ) 	 )ff
 
 	


p[ ^e_V3d/Kq+]#X[a?q+]ffZ'c6KPXRZ6f,_0d/]ffN6[ a6]	q[a6K\Z.]#[RT]ffZ.MB]#b* @BRZ.c.KPWiKZ'c6KZ.OQK8_#Z.c S4RoZ.c6KPW/KZ.c6KZ.OQK
OP_#Zd/Kf#KZ6KPX _#URTgPKcL[Y][a6KeOP_#MYKqa6KPXK[a6Ke*Z6]	q4UTKc6f#Ked"_#MYKRM?Z6]#[}_^KPXKeW'X]#W/]ffMRT[RT]ffZ._#Ubr]#XY
^&N.U_d"N6[+_}W.X]#d'_0d"RURT[V&c.RoMY[YXRTd'N.[RT]ffZ2]S#KP+X [Ti`]#X,_#ZRoZ.OQ]ff^W'UTKP[YKW.X]#d'_0d"RURT[Vr]#X(WK "N.RTS0_#UTKZ[UV
_MYKP[]#b+W.X]#d'_0d'RUR[Vc.RoMY[YXRTd'N.[RT]ffZ.M  `/]#X_M[YX_0[R  Kc *Z6]qUTKc6f#Ked'_#MK#`]#X_#ZV ]#[a6KPX]#Xc'RZ._#U]#X
ZN.^KPXROP_#UBM[YXN.OQ[N6XK#j
8RTXM[P`UTKP[:N.M\Z6]#[RoOQK4[a._0[(MY]ff^K]#bB]ffN6X:O a._0X_#OQ[YKPXRg_0[RT]ffZ.M(UTK_#ce[YN
] "N.RT[YK4RZ[N.RT[RS#K4_#Z.cef#KZ.KPX_#U
Rc6K_#Mj=]e[_0#K[a6KOP_#MYK]#b S4RoZ.c6KPW/KZ.c6KZ.OQK#`*q:Ka._S#K}Ma6]qZh[a._0[,[a6Kbr]ffUUT]	q4RZ6f&[a6XKPKM[_0[YKQ
^KZ[M4_0XK?WK "N.RTS0_#UTKZ[,q4a6KZ & RM,_W.X]#W/]ffMR[RT]ffZ._#U"Z6]	q4UTKc6f#Kd'_#MYK C
_ & c.]KMZ.]#[[YKUU _#ZV[a'RZ6f_0d/]ffN6/[ >`.RoZh_#ZVhOQ]ffZ[YK [P
rdi & OP_#Zd/K?XKPqXRT[Y[YKZWK "N.RTS0_#UTKZ[UTVRoZh_b1]#X ^}N.Uo_ & ? RoZq4a.RO )
a >c6]KM4Z6]#[_0W'WiK_0X
 K  Z.RT[R]ff5
Z < 
O:br]#X_#ZV[q+]RZ[YKPXW.XKP[_0[RT]ffZ'M = _#Z.c = ? [a._0[4c.R iKPX]ffZ'UTVRZ[a.KS;_#UoN6K?fffRTS#KZh[Y] >9`.[a6KM[_0[N.M
]#b0= q4RT[aeXKMYW/KOQ[\[Y] & RmjK#jT`6^]*c6KU']#X:OQ]ffN.Z[YKPX^]*c6KU1=RM+[a6KM_#^eK_#M:[a._0[:]#b =@?9 ?:]#X]ffUoU_0XT
V  j
fiM,[Y]eS0_0XR_0d'UTK?br]#Xf#KP[Y[RZ.fC
c" &'))(%*3U	.	& % U2RM+[a6K3^]ffMY[+f#KZ6KPX_#U/OQ]ffZ.MYKW"N6KZ.OQK]#b &
 :? ]#X]ffUoU_0XV &ff  j

Q I]

[a._0[:RoM S=_0XYRoZ.c6KPW/KZ.c6KZ[=brX]ff^U

fi

~33



s~$w

~ww$

4]q`6[a6KMKc6K  Z.RT[R]ffZ.M+a'_S#K_MN6OPRTKZ[,UTKPS#KU/]#b9f#KZ6KPX_#UR[V[Y]diKK*[YKZ.c6Kch[Y][a6K?OP_#MYK?qa6KPXK
[a6K*Z6]	q4UTKc6f#K?d'_#MK & RM,XKPW"U_#OQKchdV_#Z6]#[a.KPXMY[YXN'OQ[N6XK#j
aN.MP`MY]ff^eK]#bN.M}a'_S#KK[YKZ'c6Kc [a6KZ.]#[RT]ffZ]#bYS=_0XYRZ.c6KPW/KZ.c.KZ.OQKO a._0X_#OQ[YKPXRTgPKc N'MRZ6f
rdiY_#Z.ceS;_0XRo_0d'UTK:b1]#Xf#KP[Y[RoZ6f?[Y]]#Xc.RoZ._#U*OQ]ffZ.c.RT[RT]ffZ'_#UbN.Z.OQ[R]ffZ.M,rXKPW.XKMYKZ[YKce_#M+MY[YX_0[R  Kc2d"_#MYKM 
5 @_#Z.f6` _0,X "N'RMP` fi J RUUR_#^MP` 0#  j a6Kd'_#MROW'N6XW/]ffMYKq(_#M[Y] K [YKZ.c [a6K J%br]#Xf#KP[_#Z.c
K*W'_#Z.Mc L _0W.W.X]ff_#O a _0[hq+]#X b1]#XN.W/c'_0[RZ6f J['_0[ L *Z6]	q4UTKc6f#K d'_#MYKML_#Mhd'XRTMK .V c.RoMOPN.MMYKc RZ
KOQ[RT]ffZ <:[Y]N6WBc._0[RZ.f2^e]#XKMY]#W'a.RoMY[ROP_0[YKchKPW'RoMY[YK^eRO3MY[_0[YKMPj
a.KOP_#MYK4]#bBRZ.OQ]ff^eW'UTKP[YK,W.X]#d"_0d'RURT[RKM9RM\_#UMY]RZ[YKPXKMY[RZ.fMRZ.OQK,f#KZ6KPX _#URTgRZ6f_ `Brdi8_#Z'cO
q4RUoUUTK_#c[Y][a6K2M_#^KRZ[N.RT[RTS#K&Z6]#[RT]ff
Z 0jfiM3[Y]S0_0XR_0d'UTK}b1]#Xf#KP[Y[RZ6f6` RT[3RMZ6]#[?a._0Xc[Y]Z6]#[ROQK
[a._0[4R[OQ]#XXKMYW/]ffZ.c.M([Y][a6Kq:KUU I*Z6]q4ZhZ.]#[RT]ffZh]#b:x#w{ffy1z"#
Fy E#v0y !0z.j

 'ff  0 	  2  ff
    ) ff   ) ff )	
   ) ) 1 )ff
 4! !  
 ff 2 1  1 !  #  1 ) ff  1   
 !
a6K&c6K  Z.R[RT]ffZh]#b= @B,_#Z.c SRZ.c6KPW/KZ.c.KZ.OQK	& OP_#Zd/K}XKPqXRT[Y[YKZKW"N'RTS;_#UKZ[UTVhRZ_br]#X^&N.U_
&6?,RZ qa.RO9
a erXKMYW j=>/ec6]KMZ6]#[_0W.W/K_0XRM "N.RT[YKhf#KZ6KPX _#Um`RZ [a6KMYKZ.MYK[a'_0[[a6KU]#fffROP_#U
U_#Z6fffN'_0f#K0z"t  !#w,[a6KOQ]ffZ.MYWK "N6KZ.OQKXKU_0[RT]ffZ_#Z.c[aN.M([a6KZ6]#[R]ffZh]#bU]#fffROP_#U/KW"N'RTS;_#UKZ.OQK	+^e_V
S0_0XV#`\q4a'ROa KZ._0d'UKM&N'M2[Y] c.X_q brX]ff^ [a.RM&W.X RZ.OPRTW'UKZ6]#[RT]ffZ.M&]#b @ _#Z.c  S4RZ'c6KPW/KZ.c6KZ.OQK
_#Z.c brX]ff^ [a6KZL]ffZ ` Z6]#[RT]ffZ.M3]#b(URT[YKPX_#U8_#Z.c S0_0XR_0d"UTK}br]#Xf#KP[Y[RZ.fb1]#XZ.]ffZ.OPU_#MMRoOP_#UU]#fffROPMPj2a.RM
RM(q4a'_0[a._#M(d/KPKZhc6]ffZ6K_0[4UTK_#M[,W'_0X[UTV.+RZL5 @_0#K^KPV#KPX` # # jEKPXKq:K?d.XRMK .VOQ]ffZ.MRoc6KPX,[q:]
OP_#MYKMMC
R1I /=P,PT;5y  0 !{#0y Qj\a6KMK3U]#fffROPM_0XK3d'N.RoUT[+]ffZh_OPUo_#MMROP_#UBU_#Z.fffN._0f#K?d'N6[,a._S#K_2q+K_0#KPXOQ]ffZ*
MYWK "N6KZ.OQK&XKU_0[RT]ffZ[a._#Z OPU_#MMRoOP_#UUT]#fffRO0j6]#X?RZ.MY[_#Z'OQK#`iRZ^]ffMY[3^}N'UT[RTS0_#UN6KcUT]#fffRoOPMP`ic'N6K[Y][a6K
b_#OQ[&[a'_0[&[a.KK 6OPUN.c6Kc ^eRc'c.UTK 
 H :<6RoM&Z.]#[&_[a6KP]#XK^h`+_br]#X^&N.U_MN.Oa _#M 
 H  X: SFG
/ SP
q4RUoU'c6KPW/KZ.cd/]#[a]ffZ_#Z.c]ffk
Z Srq4a6KPXK_#M,RT[:c.KPWiKZ'c.M\]ffZ'UTVe]ffZ eRZOPU_#MMROP_#UiUT]#fffRO `6d/KOP_#N.MYK3RT[
OP_#Z.Z6]#[+d/K4XKPqX RT[Y[YKZeWK "N.RTS0_#UTKZ[UTV&RoZ[Y]&_br]#X^}N'U_RZq4a.RO a S,c6]KM+Z.]#[+_0W.W/K_0XRoZeW'_0X[RoOPN.U_0X`
RMZ6]#[=_#Uq:_VM\WK "N.RS;_#UTKZ[8[Y]% H2 -: S%6
/ SP8RZMN.OaUT]#fffRoOPM  ]ffZ[a.KOQ]ffZ[YX_0XV#`[a6K,br]#X^&N.U_% H2 <6/ SP
RM(WK "N.RTS0_#UTKZ[,[Y]RZhN.MN._#UB^&N.UT[RS;_#UN.KcU]#fffROPM_#Z.c[aN.Mc6KPW/KZ.c'M(]ffZ ]ffZ'UTV#j
RR1x !t09 !{ff5y P *j 4]q`.[a6KU_#Z.fffN._0f#KRM,]#d'[_#RZ6KchdVK *[YKZ.c.RoZ6fe_eOPU_#MMROP_#UBW.X]#W/]ffMR[RT]ffZ._#UiU_#Z*
fffN._0f#K?q4R[a]ffZ6K3]#X,^]#XK?^]*c._#UR[RTKMP`qa.RUTK[a.K3OQ]ffZ'MYWK "N.KZ.OQK3XKU_0[RT]ffZK *[YKZ.c.M:[a._0[(]#b9OPU_#MMROP_#U
UT]#fffRORZ[a6K2MYKZ.MYK&[a._0[?_^e]c._#UoRT[VIb1XKPK}br]#X^&N.U_RoM_[a6KP]#XK^ RTb+_#Z.c]ffZ'UTVRTb=RT[3RM_[a.KP]#XK^
]#bOPU_#MMRoOP_#U UT]#fffRO j:a6KPXKPbr]#XK#`.XKMN.UT[MMN'Oa_#M =X]#W/]ffMR[RT]ffZ.M _#Z.c 2MY[RUoUB^e_0#KMYKZ.MK_#Z.chq:K
OQ]ffZ FYKOQ[N6XK3[a._0[([a6KPV_0XKM[RUU.S0_#URc j89]2[_0#K_#ZK *_#^W"UTK#`RoZ[a6K?UT]#fffRo6
O fi }q4a.KPX
K  & I	 & RM
_[a6KP]#XK^h`/_ebr]#X^&N.U_MN.Oa_#

M  & ff
/ 	 & H 
4,RMRZ.c6KPW/KZ.c6KZ[,brX]ff^ 
 q4a.RoUTfi
K  & 
/ 	 & H 

RM,Z6]#[Pj




132

ff
  ! # 2  
 #  / '  2  ! /)  
   2 
 #  2  ! #  
G 1 	  	!

 

6]#Xf#KP[Y[RoZ6f_hMYKP[?]#b+S;_0XRo_0d'UTKM3^]*c.R  KM3_*Z6]q4UKc6f#K2d'_#MYKq4a.RoUTKW.XKMYKPXS*RZ6fMY]ff^K]#b:[a6KOQ]ffZ*
MYWK "N6KZ.OQKMP`Z._#^KUTV#`B[a.]ffMYKed'N.RoUT[]ffZ S0_0XR_0d"UTKM3[a._0[_0XKZ.]#[?b1]#Xf#]#[Y[YKZ j [a6KPXbrX_#^KPq:]#X*M_0XK

iorqts  ffy }suu  yzB ff)p|u8iz}y | fi-~i:z} )}y ff xsuff-}su $&ffyx|u~)u f{u8z}uiz}uuffU}u~ Xz}yw
 |  - ~:z} i }yff $F * ~iyUu ffyB|u~Y}yffRu 	|u8~iu ff}yff?$ lu u8  ||Uz}y | fiX~:z} }yff
y ff  | xt i 8}u ||lyTu }sff)'ly  *! $ %*r ) u:}t}su~u ff }y ff   )!G $#"$ !&%(' $) )& *3* *G $).*
xs	s|u~Y}y/}su'~uyTlyp}2 1|3547 6 y+ 5 xX218o z3947o 6 o, - /X/. ypUff -z}y   | fi ~i:z} )4 }y ffy i	ffu~ z}yp
Tz}yf  |   ~:z}  }yff0 
y fffi
fff ~3}suRz}yf  |  ~:z} i }yff:  yff*;<=5~u ffu~
 4 $! *   $ > 4 $ff *3*o $  *Xxty |~|up~ }y}suO}TuO~u ff }y ff" ffy}ff ff}s{/  ||iz}y f| fi ~:w
z} i }yff ff l u'u'i Bi18z}35u476 u~Tyz}u' ypTf{ }| ? fz}{|AO@ z}yf  |  ~:z} i }yff $ su z}u  +iz}y   | fi
~:z} i }y Off yff
*  z}yp xs	s R ff ~) u~ T
 }suX/i   uUff z}yp+ z}ff |upo
Q _&(

fif.w~3}33f$3ws

d'_#MYKce]ffZXKMY[YXRoOQ[RZ6f?[a6K,W/]ffMMRd'UTK "N6KPX RTKM=RZ2]#[a6KPX\q:_VMP`*RmjK#jT`OQ]ffZ.MRc6KPX RZ6f]ffZ.UTVN"N6KPX RTKMRZ ]#XZ
br]#X^  _#N6[Yg#` ?K_0XZ.MP=` fi KU^e_#ZB` #ff j
E4K_#MY]ffZ.RZ6fq4RT[a}Oa._0X _#OQ[YKPXRMY[RO(^]*c6KUM: _#N6[Yg#` K_0XZ.MPD` fi *KU^e_#ZBG` #  RM9d'_#MYKc&]ffZ}N'MRZ6f
]ffZ.UTVL_MN6d'MKP[]#b^]*c6KUM]#b,[a6K]#X RTfffRZ._#Ubr]#X^&N.U_*j fiM}b1]#X&b1]#Xf#KP[Y[RZ6f6`[a.RM}^e_V RZ.OQXK_#MYK[a6K
MRTgPK]#b[a6KXKPW.XKMKZ[_0[RT]ffZ ]#b_*Z6]q4UKc6f#Khd'_#MYKK *Wi]ffZ.KZ[R_#UoUTV#j a'_0Xc6]ffZ _#Z.c E]#[a [ # ff
a._S#KMa6]	q4Z [a._0[O a._0X_#OQ[YKPXRoMY[ROe^]*c6KUM?OP_#Zd/KeN.MYKc br]#XKPOPRTKZ[XK_#MY]ffZ'RZ6fh]ffZMY]ff^eKMYKP[M]#b
XKMY[YX ROQ[YKc "N6KPXRTKMPj
A  E D #G+F\H D  ] O<

JLKa._S#KRZS#KMY[RTfff_0[YKc MKPS#KPX_#U,q(_V*Me]#b3_#Z.MYq:KPXRZ6f [a6Kh#KPV#"N.KMY[RT]ffZ ]#b3c6KP[YKPX^eRZ'RZ6fq4a._0[e_
W.X]#W/]ffMR[RT]ffZ._#U8*Z6]q4UKc6f#Ked'_#MYK[YKUUM}_0di]ffN.[[a6KRoZ"c6KPW/KZ.c6KZ.OPRKM3diKP[q+KPKZ S0_0XR_0d'UTKM_#Z.cbr]#XY
^&N.U_#MPj86]#X=K_#O ae]#b'[a.KZ6]#[RT]ffZ.MMY[N'c.RTKcB`0q:Ka._S#K,XKUo_0[YKceRT[8[Y]?]#[a6KPX=W.XKPSRT]ffN'MUTV?*Z6]q4ZZ.]#[RT]ffZ.M
_#Z.chq:Ka._S#K}MY[N.c'RTKcRT[,b1X]ff^ _eOQ]ff^W'N6[_0[R]ffZ._#UBW/]ffRZ[:]#bS*RTKPq`6fffRTS*RZ6f2d/]#[ahOQ]ff^W'UTK 6RT[VXKMN.UT[M
_#Z.cO a._0X_#OQ[YKPXRTg_0[R]ffZXKMN.U[M9[Y]?d/K,N.MYKc2br]#XW.X_#OQ[RoOP_#UOQ]ff^eW'N6[_0[RT]ffZBj8p%Z2[a6K,URTfffa[8]#b']ffN6XXKMN.UT[MP`
RT[,_0W.W/K_0XM:[a._0[,[a6K3S0_0XRT]ffN.M(br]#X^eM:]#bUT]#fffROP_#UBRZ.c.KPWiKZ'c6KZ.OQK_0XKOPUT]ffMYKUTVOQ]ffZ'Z6KOQ[YKcBj++MYW/KOPR_#UUTV#`
MYKPS#KPX_#U=]#b:[a.K^ a._#c d/KPKZLW.X]#W/]ffMYKc dV c.:R /KPXKZ[?_#N6[a.]#XM?q4RT[a.]ffN6[3diKRoZ6fK W'UoROPRT[YKUTVXKU_0[YKcBj
A:]ffN6[RURKPX+ MRZ "N6KZ.OQK_0d'RUoRT[V_#Z'c @_0#K^KPV#KPX  MXKUTKPS;_#Z'OQK]#b=_br]#X^}N'U_2[Y]_MN6Gd FYKOQ[4^e_0[Y[YKPX3_0XK
Rc6KZ[ROP_#U+[Y]L SRZ.c.KPWiKZ'c6KZ.OQK5 \X]#W/]ffMRT[RT]ffZ'
M _#Z.c ff jLJLKh_#UMY]Lc.RMOPN'MMYKc^&N.O a XKU_0[YKc
q:]#X"`_#Z.c MN6f#f#KMY[hMY]ff^KK [YKZ'MRT]ffZ.M[Y] ^e]#XKf#KZ.KPX_#Ub1X_#^eKPq+]#X [a'_#Z ^eKPXKW'X]#W/]ffMRT[RT]ffZ._#U
UT]#fffRO0j
a.K?b1]ffUU]q4RZ.f}[_0d'UK?fffRTS#KM_MYV*Z[a.KP[RO3SRKPq ]#b^e_#ZVZ6]#[RT]ffZ.M,_#c.c.XKMMYKchRZ[a'RM(W'_0W/KPX_#Z.c
[a6KOQ]#XXKMYW/]ffZ.c.RZ.f&OQ]ff^W"UTK *R[VXKMN.UT[Mj
\X]#d'UTK^
44]#[_0[R]ffZ
K  Z.R[RT]ffZ
?:]ff^W'UTK 6RT[V
V*Z[Pj @RT[%RZ'c6KPW/KZ.c6KZ.OQK
( 3 *Q	 &40)
fi (@!O

@9RT[%RZ.c6KPW/KZ.c.KZ.OQK
( 4 &
fi (@!O 'OQ]ff^W'UKP[YK
ff 
 ' 
  & % ( 3 *P 
 )
KPW/KZ.c6KZ[URT[YKPX_#UoM

ffr ( 3 *Q	 & #iDA #i +  &+
" - OQ]ff^W'UTKP[YK
.N'UU @R[%RZ.c6KPW/KZ.c6KZ'OQK
(= r ( 3 *Q	 &4
'OQ]ff^W'UKP[YK
@9RT[%MR^W"UR  Kc
8 ' #i +  4 &   )k4 ( 3 *Q	 & 'OQ]ff^W'UKP[YK
@9RT[%IWK "N'RTS;_#UKZ.OQK
&   

&'))(%+* ( 3 *Q	 & % ( 3 *P	 & T(:" 
&'))(%+* ( 3 *Q 
 % ( 3 *P 
 T(:
fi  - OQ]ff^W"UTKP[YK
\X]#d'UTK^eM(]ffZURT[YKPX_#UM,_#Z'c[a6KRTX4OQ]ff^W"UTK *R[V#j
\X]#d'UTK^
44]#[_0[RT]ffZ
K  Z.RT[R]ffZ
?:]ff^W'UTK 6RT[V
V*Z[Pj S=_0XYRZ.c6KPW/KZ.c.KZ.OQK
U.	 & fi U ! 

S=_0XYRoZ.c6KPW/KZ.c6KZ.OQK
U 4 , &





&

U


.






fi
&
!


'OQ]ff^W'UKP[YK
ff '
%
.
KPW/KZ.c6KZ[,S;_0X R_0d'UTKM

ff U	.	 & #D< A <  ,. &+
" - OQ]ff^W'UTKP[YK
.N'UU S=_0XYRZ'c6KPW/KZ.c6KZ.OQK
U! 
ff U.	 &
'OQ]ff^W'UKP[YK
S=_0XYMR^W'UR  Kc
r< ' #D<+  4 ,. &  < )B4 U,	.	 & 'OQ]ff^W'UKP[YK
S=_0XYIWK "N.RTS0_#UTKZ.OQK
& 3$ 

&'))(%3* U.	 & % U.	 &  U&" 
&'))(%3* U. 
 % U. 
  U&
fi - OQ]ff^W"UTKP[YK
\X]#d'UTK^M(]ffZhS;_0X R_0d'UTKM,_#Z.c[a6KRTXOQ]ff^W'UK *RT[V#j

Q_

fi

~33



s~$w

~ww$

a.K}b_#OQ[3[a._0[3di]#[a Z6]#[R]ffZ.M]#b=b1]#X ^}N.Uo_;IS;_0XRo_0d'UTK}RoZ.c6KPW/KZ.c6KZ.OQK}_#Z.cb1]#Xf#KP[Y[RoZ6fha._S#Kd/KPKZ
N.MYKch_#M#KPVOQ]ffZ.OQKPW'[M4RZ^e_#ZV fip  KUc.M?RZ.OPUN'c.RZ6f&_#N6[Y]ff^e_0[YKcXK_#MY]ffZ.RZ6f6`*d/KURTKPbXKPSRoMRT]ffZ_#Z.c
N6WBc._0[YK#`.c.Ro_0fffZ6]ffMRMP`6XK_#MY]ffZ.RZ6fe_0d/]ffN6[_#OQ[RT]ffZ.MKP[O0j
a._#M4diKPKZc.RMOPN.MMYKcd/KPb1]#XK	*KOQ[RT]ffZ.M }_#Z.c
< `8MY]q:Keq4RUoU9XKPb1X_#RoZLb1X]ff^ XKPW/K_0[RZ6fR[a6KPXK#ja6Kfff_#RZ]#b:f#KZ.KPX_#URT[V] /KPXKcdV [a6KOQ]#XXKQ
MYW/]ffZ.c.RoZ6fhURT[YKPX_#U I]#X RTKZ[YKcZ6]#[RT]ffZ.MRoZ[YX]*c.N.OQKcRZL[a.RMW'_0W/KPX}a._#M}_#UMY]d/KPKZLKMY[_0d"URMa6Kc rK#jf6jT`
=X]#Wi]ffMRT[RT]ff
Z 0 `_#Z.c[a6KRTX?_0W.W'UoROP_0[RT]ffZ[Y]MYKPS#KPX_#/U fip4W.X]#d"UTK^eM2URT#KOPUT]ffMYKc*Iq:]#XUcXK_#M]ffZ.RZ6f
_#Z.chd/KURKPbN6WBc._0[YK	:a._#M,d/KPKZM#KP[Oa6KcBj
\XR^e_0XRoUTV#`#]ffZ6K4]#bB[a.K^e_#RZ^]#[RTS0_0[RT]ffZ.M\b1]#X+[a6KZ6]#[R]ffZ.M=]#bBbr]#X^&N.U_;IS0_0XR_0d'UTKRoZ.c6KPW/KZ.c6KZ.OQK
_#Z.cb1]#Xf#KP[Y[RZ6f&q:_#M([Y]2R^W.X]S#K3RZ6brKPXKZ.OQK4brX]ff^ _}OQ]ff^eW'N6[_0[RT]ffZ._#UiMRc.K#`dVKZ._0d'URoZ6f[Y]2b1]*OPN.M+]ffZ
XKUTKPS0_#Z[=W'RTKOQKM]#b/*Z6]qUTKc6f#K#ja6KK *[YKZ[\[Y]q4a'ROa[a'RM8f#]ff_#U'OP_#ZdiK,XK_#Oa6KcOP_#Zed/Kc.RMOPN.MMYKc
_0[[a6K}URTfffa[:]#b8]ffN6XOQ]ff^W'UTK 6RT[VXKMN'UT[MM C
v JIyrz K0tRu -6uPz"tuQz  uwYuP ffv5y !0z.?~00u&~y {#T
~ I!0.
x -"3u 2#y5v 4 :a6K3Z6]#[RT]ffZ.M(OQ]ffZ.Z6KOQ[YKc[Y]2 S
  !;.
c.KPWiKZ'c6KZ.OQKe @Bc6KPW/KZ.c6KZ'OQK#`6brN.UoUi S+_#Z.ch @Bc.KPWiKZ'c6KZ.OQK#`.RZ "N6KZ.OQK_0d'RUoRT[V#`XKUTKPS0_#Z.OQK
[Y]_MN.Gd F%KOQ[?^e_0[Y[YKPX_#Z'cLMY[YXROQ[XKUTKPS0_#Z.OQKMYKOQ]ffZ.c br]#X^a._S#Ke_OQ]ff^eW'UTK 6RT[V_0[?[a6K  XMY[
UKPS#KU8]#b\[a6K&W/]ffUTVZ.]ff^eR_#Ua'RTKPX_0XO aV#`Bq4a'ROa ^K_#Z.M[a._0[?[a.KPVOP_#Z d/K&O a6KO#KcdV_M_0[RM%
 _0d'RURT[V]#X H;_#Z'c_#ZN'Z.M_0[RM  _0d'RURT[VMY]ffUTS#KPXj+a6KPVd/KOQ]ff^_
K J%[YX _#OQ[_0d'UTDK Leqa6KZMYV*Z[_#OQ[RoOP_#U
XKMY[YXROQ[RT]ffZ'M_0XK&^e_#c6Kh5 =X]#W/]ffMR[RT]ffZ  j6]#Xf#KP[Y[RZ6fUR[YKPX_#UM]#XS0_0XR_0d'UKM (_#UMY]RMOQ]ff^2
W"N6[_0[RT]ffZ._#UUVK *W/KZ.MRTS#K#jha.KXK^e_#RoZ.RZ6fZ6]#[R]ffZ.M_0XKRZOQ]ff^eW'UTK 6RT[VOPU_#MMYKM}U]OP_0[YKc _0[
[a.K3MKOQ]ffZ.chUTKPS#KUi]#b[a6K3W/]ffUTV*Z6]ff^eR_#U"a.RTKPX_0X OaV#jJL]#XMYK#`.N.Z.c.KPX+[a6K?MY[_#Z'c._0Xc_#MMN.^eW.[RT]ffZ.M
]#b8OQ]ff^W'UK *RT[V[a.KP]#XV#`6[a6K?K *W'UROPRT[(OQ]ff^W'N.[_0[RT]ffZh]#b8URT[YKPX_#Ui]#XS0_0XR_0d'UKbr]#Xf#KP[Y[RZ6feOP_#Z'Z6]#[
d/K_#Oa.RKPS#Kc RZW/]ffUTV*Z6]ff^eR_#U+MYW'_#OQKRZ [a6Kf#KZ.KPX_#U(OP_#MYK5 =X]#W/]ffMR[RT]ffZ    ja'RMW'N'Ma6KM
[Y]	q:_0X c.M[a.KZ6KPfff_0[RTS#KOQ]ffZ.OPUN.MR]ffZ [a._0[_#UU[a.KMYKZ6]#[RT]ffZ'M_0XKa._0X cL[Y]d/KeOQ]ff^W'N6[YKc _0[
UK_#MY[RZ[a.K:q:]#XMY[8OP_#MYK	9K 6OQKPW.[=RTb6[a6K,MRgPK+]#X8[a6K,MYV*Z[_#OQ[RoOP_#U*b1]#X^s]#b.[a.K(RoZ6W'N6[ KZ'_0d'UTKM8RT[Pj
4a6Kb_#OQ[[a'_0[4[a6KMYK}W.X]#d'UK^eMb_#UU RZ[a6K}MKOQ]ffZ.cUKPS#KU]#b[a6K}W/]ffUTVZ.]ff^eR_#UBa.RTKPX _0XOaV_0XK
Z.]#[[a._0[MN6XW.XRMRoZ6f}MRoZ.OQK?[a.RM,RM(qa6KPXK_U_0Xf#K?W"_0X[RTb8Z6]#[,[a6K^e*_ FY]#XRT[V6+]#bR^eWi]#X[_#Z[
W'X]#d'UTK^eM,RoZZ6]	q4UTKc6f#K?XKPW.XKMYKZ[_0[RT]ffZ b_#UUmj
x -iT,u 2#y5v 4 t !u z !#v2z/3u  uQP0w yr 4'-"wYuP0uPz'v [w !0x -iw% v5y  00Q{ !DO
 /6ve ~yT{0~ | !0wQ5v O[ ;Pu ,!#Y
wyvr~x& aN.MP*` fi^eRTX_#Z'
c OQp%UTX_#RT[aa._S#KMa6]q4Z[a6KeOQ]ff^W'N6[_0[R]ffZ._#Ud/KZ6K  [M?[a'_0[OP_#Z
d/K&_#Oa.RTKPS#Kc dVMY[YXN'OQ[N6XRZ6f_  A r[a6X]ffN6fffa[a6K&br]#Xf#KP[Y[RZ6f]#W/KPX_0[RT]ffZ"MY]_#M[Y]h_#Oa'RTKPS#K
RoZ6b1KPXKZ.OQK,_#Z.ceOQ]ffZ.MYWK "N6KZ.OQK  Z.c.RZ.f^]#XK,KPOPRTKZ[UV fi^eRTX fi OQp%UTX_#RT[aB` 0##* OQpUX_#RT[a
fi fi^eRTX` 0#  j fi MR^eRUo_0X9c.RMOQXKPW'_#Z.OQVd/KP[q+KPKZ2[a6K(q:]#XMY[8OP_#MYKMRT[N'_0[RT]ffZ}_#Z'c}[a.K:W'X_#OQ[R 
OP_#U/]ffZ.KM,OP_#ZdiK]#d'MYKPXS#KchRoZ]#[a6KPXc.]ff^e_#RZ.MPKMYW/KOPR_#UUV#`M_0[RoM  _0d'RUoRT[VId'_#MYKcOa6KO#KPXM,br]#X
"N._#Z[R  Kc d/]]ffUTK_#Z b1]#X ^}N.Uo_#M25 A(RTKPXK#` ?(R^_0[Y[Rm9` ?(U_0X#K#`9.N FRT[_*` fi/aNB` ##8J RUoUR_#^eMP`
A(RTKPXK#` ?(U_0X#K#` fi 
?N6W.[_*` 0##N.MKcbr]#X4br]#X^e_#U S#KPX R  OP_0[RT]ffZW'N6XW/]ffMYKMrd/]ffN.Z'c6Kc^]*c6KU
O a6KO*RZ6fK 6a.RTd"RT[4RZ[YKPXKMY[RZ.fOQ]ff^W"N6[_0[RT]ffZ._#Ud/Ka._S*RT]ffN6X M&_#OQ[N._#UoUTV#`B[a6KPV[VW'RoOP_#UUTVW/KPXY
br]#X^ diKP[Y[YKPX[a._#Z MYW/KOPR_#URTgPKc _#UTf#]#X RT[a.^eMP`(_#MMa6]qZ R6
Z ERZ[_#Z.KZB` 0#  `4c6KMYW'R[YK[a6K
b_#OQ[[a._0[[a6KPV_0XKeOQ]ffZ6b1X]ffZ[YKcL[Y][a6KW.X]#d'UK^ ]#b+S0_0XR_0d'UK}br]#Xf#KP[Y[RZ6f RmjK#jT`KURo^eRZ._0[RT]ffZ
]#b8K 6RMY[YKZ[R_#UUTL
V "N'_#Z[R  KcS;_0XRo_0d'UTKM  j
x -G!0wQvI0z'v[w !0uQjJ a._0[q:K^K_#Zq4RT[a J%W.XKPW.X]0
  !0w3u !0#uQw 1 -"wRu -i[w !Wu  yrz{xeD 4 -"TD 4L0z y1.
OQKMMRZ6f L3XKPbrKPXM8[Y]?[a6K,[_#M}]#b/OQ]ff^W'N6[RoZ6feRZ"c6KPW/KZ.c6KZ'OQK+XKUo_0[RT]ffZ.M8_#Z.c2br]#Xf#KP[Y[RZ6%
f P 5u M!0wYu

Do s{-~ }y ff""ffy ffTy ffy}y ff*ff) u z}uffu %lu|u8z}uDy ff%%lu|u' ~}u ypTu  yz}T%yr|ff)ffff ?ff~
~u yff6/$Dff io
 

Q _^_

fif.w~3}33f$3ws

W/KPXbr]#X^eRoZ6fh^]#XKeW.X]#d'UTK^2I]#XRKZ[YKcL[_#MM&MN.O a_#MOQ]ffZ'MYKW"N.KZ.OQK  Z'c.RZ6f6` c.Ro_0fffZ6]ffMRMP`_#O
[R]ffZ H;N6WBc._0[YK#`=c6KOPRMRT]ffZ ^_0RZ.fKP[O0jaN.MP` @9RT[%MR^W"URTbrVRZ.f r]#X S=_0X	MR^W'URb1V*RZ6f_ A
c'N6XRZ6f_W.XKUR^RZ._0XV] "UoRZ6K&W'a._#MYK2OP_#Z W.X]	S#Kea6KUTW.bN.U br]#X?R^W.X]S*RZ6f]ffZ6URZ6K2RZ6brKPXKZ.OQK
MRZ.OQK2MR^W'UoR  OP_0[RT]ffZZ6KPS#KPXRZ.OQXK_#MKM3[a6KMRTgPK2]#b+5
_  Aj fiM?Ma6]q4ZdV \X]#W/]ffMRT[RT]ff
Z   `_
MR^eRU_0X=OQ]ffZ'OPUN.MRT]ffZOP_#Z.Z6]#[\d/Kc6X _q4Z[Y]q4a._0[+OQ]ffZ.OQKPXZ.M\b1]#Xf#KP[Y[RoZ6f6j=a.RM\MYKPK^eM=[Y]}d/K[a6K
W'XROQK\[Y]d/K\W'_#Roc[Y]d/KZ6K  [brX]ff^ [a6K:Wi]	q+KPX8]#b.br]#Xf#KP[Y[RZ.f6j]q:KPS#KPX`[a'RMZ6KPfff_0[RS#K(OQ]ffZ'OPUN*
MRT]ffZ^}N'MY[d/K:[YK^eWiKPXKcdV}[a6K,[q:]br]ffUUT]	q4RZ6f3OQ]ff^e^KZ[MPj Z2[a.K,]ffZ6Ka._#Z.cB`#br]#Xf#KP[Y[RZ.fRM
RoZ[YKPXKM[RZ6f -6uPw4uQR[\RMZ6]#[=]ffZ'UTV&_[Y]]ffU.[a._0[+OP_#Za6KUTWeR^W'X]S*RZ6f?RZ6brKPXKZ.OQKRoZeMY]ff^K4OP_#MKM
d"N6[_#UMY]e_f#]ff_#URZhMYKPS#KPX_#9U fi4p(_0W.W'URoOP_0[RT]ffZ.MPj Z[a6K]#[a6KPX4a._#Z.c `]ffN.X4OQ]ff^W'UTK 6RT[VXKMN.UT[M
XKU_0[YK3[Y][a6K3q+]#XM[(OP_#MK3MRT[N._0[RT]ffZB`]ffZ.UTV#`6_#Z.c `*_#M(KPS#]##KcdiKPbr]#XK#`*br]#Xf#KP[Y[RZ6fRM:brK_#MRTd'UTKRZ
^_#ZV}W.X_#OQ[ROP_#U6OP_#MYKMj88RZ'_#UUTV#`#UTKP[N.M8Z6]#[YK,[a._0[8[a6KPXK_0XK(MKPS#KPX_#U6OQ]ff^W'UTKP[YK,W'X]#W/]ffMRT[RT]ffZ._#U
brX_0fff^KZ[Mb1]#Xq4a.RO abr]#Xf#KP[Y[RZ6fRM4K_#MYV#j4\MWiKOPRo_#UUTV#`'_#M_#c6S#]*OP_0[YKcdV 3_0Xq4RO a6K[ ##ff `
OQ]ff^eW'RURZ.f_  A RZ[Y]_ O4O4 br]#X^&N.U_c.N.XRZ6f_#Z ] iURZ6KMY[YKPW OP_#Z W.X]S#KW.X_#OQ[ROP_#UUV
S0_#UN._0d"UTK=[Y]3_#Oa.RTKPS#K(br]#Xf#KP[Y[RZ6f3RZ}_#ZKPOPRKZ[q:_V#`ffW.X]	SRoc6Kc[a._0[[a6K:MRTgPK\]#b'[a6K+OQ]ff^W"RUTKc
br]#X^ XK^e_#RZ.M4M^e_#UUBKZ.]ffN6fffa rq4a'ROaOP_#Z'Z6]#[4d/KfffN._0X _#Z[YKPKcRZh[a6K}q+]#X MY[4OP_#MYK	 .
j *RoZ.OQKRT[
RoMZ6]#[*Z6]qZq4a6KP[a.KPX[a6K 64O4 brX_0fff^KZ[}RoM?MY[YXROQ[UTV ^]#XKMN.OPOPRZ.OQ[[a._#ZL[a6KeW'XR^K
Ro^W'UROP_0[YKM9]ffZ6K? 3_0Xq4RO a6K fi _0,X "N.RMP` ##ff `[a6K+W'XR^K:R^W'URoOP_0[YKM brX_0fff^KZ[8OP_#Z2_#UM]d/K
[_0Xf#KP[YKcLq4RT[aW.X]  [_#M3_OQ]ff^eW'RU_0[RT]ffZU_#Z6fffN._0f#K2br]#X?MY]ff^K&Z6]	q4UTKc6f#K&d'_#MYKMBKMYW/KOPR_#UUTV#`
M]ff^K2XKOQKZ[_0W'W.X]ff_#Oa.KM3[Y]h[a6KR^W'URoOPRT[,XKPW.XKMKZ[_0[RT]ffZ ]#b+W.XR^eK}R^W"UROP_0[YKM2	 *R^]ffZ fi
c.KU S=_#UI` 0# K 6a.RTd"RT[4S#KPXVMRTfffZ.R  OP_#Z[K^W'RTX ROP_#U W/KPXbr]#X^e_#Z.OQKM2r[a6KPVKZ._0d'UTK&[a6K2OQ]ff^2
W"N6[_0[RT]ffZ]#b9MYKP[M(]#bW.XR^KR^W'UoROP_0[YKM+OQ]ffZ[_#RZ.RoZ6f&N.W[YT
] P / OPU_#N.MKM  ffj fiOPOQ]#X c.RZ6fffUTV#`*[a6KPV
OP_#ZW.X]	S#KS;_#UoN._0d'UTK3br]#X[a6KW.X _#OQ[ROP_#U OQ]ff^W'N.[RZ6f2]#bRZ.c6KPW/KZ.c6KZ'OQK_#Z.cb1]#Xf#KP[Y[RoZ6f6j
D E  	'F &= D 
a6K&[a.RTX c_#N6[a6]#X?a._#Md/KPKZW'_0X[UTVMN6W.W/]#X[YKcdV[a.K}p c6KN@ KZ'MP`/[a6K Z.RS#KPXMRT[K2 c  fi4X[Y]ffRMP`
[a6KYEKP fffRT]ff)
Z 44]#X+
c H8_#M%c6KQ ?(_#Uo_#RM+N'Z.c6KPX[a6K9 fi ?(,,p ? W.X] FYKOQ[P`_#Z.cdV2[a6K\N6X]#WiK_#Z ?:]ff^2
^&N.Z.RT[V87 1 E =X]#f#X_#^j *]ff^KXKMN.UT[M,]#b[a.RoM,W'_0W/KPX_#UTXK_#c6Vh_0W.W/K_0XKcR'
Z KOQ[RT]ffZ  ]#b[a6K
W'_0W/KPX5 @_#Z.f fi _0,X "N'RMP` # 0_ J ?:]ff^W'UK *RT[VXKMN.UT[M3br]#XRZ.c.KPWiKZ'c6KZ.OQK2_#Z.cc6K  Z'_0d'RURT[VRZ
W.X]#W/]ffMR[RT]ffZ._#U/UT]#fffRUO L*` ,[w !^  ![}vr~*u 
	  z"vuQwz"#v0y !0z"0  !0Wz uQwuQz u !0ff
z (wyrz Py -"u ![fi}z !0|,Tut{u
u -"wYuQPuPz'vI#v0y !0z 0z"
t 4u;M !0z.yrz{ Jfi  M K	`'W"_0f#KM  	  ` # j


Q _W\

fi

~33



s~$w

~ww$

&T D

F\H   ff E8E
 ru=<uwuw{ 
 M!0wx /*TV& y1  yrv5Oyrz"tuR-.uQz"tuPz'v  w !0x 8y 40z"tN!0z' 4y  1Q!#w40z4yrz'vuQw -"wuPvI#vy0!0z
= ) [T 1=y F=;A ! & vr~*uPz&'"l V>= % :XmDA ! &5


fi MMN'^K[a._0[ & RM.@RT[%RoZ.c6KPW/KZ.c6KZ[,b1X]ff^ j4a6KZ `'[a6KPXK}K*RoMY[M4_b1]#X ^}N.Uo_ 
 RoZk4?4
=> 
[a._0[?RMKW"N.RTS0_#UTKZ[[Y] &B` _#Z.c c6]KM?Z6]#[?OQ]ffZ[_#RoZGj4a6KZB`/b1]#X_#ZV-= ) [  MN.Oa [a'_0[=BA ! 

q:K3a'_S#K &')lV >= % :YIDA ! 
3j 6 RZ.OQK 
 RoM\KW" N.RTS0_#UTKZ[+[Y] 
& `*q:KOQ]ffZ.OPUoN.c6K[a._0[([a6K?M_#^K3W.X]#W/KPX[V
a6]ffUc'M(b1]#X 3
& j
fiMMN.^eKe[a._0[P`8b1]#X&_#ZVLRZ[YKPXW.XKP[_0[RT]ffZ = )9[/i`4= A ! & R^W'URKM &')fV>= % :YI A ! 
& jJLK
W.X]	S#K[a._0[& RM 9@ RT[%RZ.c6KPW/KZ.c.KZ[:b1X]ff^Qj\p%Z.c6KPKcB`.UTKP[/1d/K?[a6K[YKPX ^ q4a6]ffMYK]ffZ.UTV^]c.KUBRM =,j
a6K?br]ffUUT]	q4RZ6f2KW" N.RTS0_#UTKZ.OQKa6]ffUoc.MMC


ruHu

#U1
#U1


&






A =;A ! & +
A =;A ! & _#Z.c = A4!  +
#U1  A =;A ! & _#Z.c = A 4!  +

 A	=;A! & _#Z.c
 H 1
	fiff    j

. #U1
. #U1

+

1

=;A !  +
A%=;A ! & _#Z'c =;A !  +

 a.K2Uo_0[Y[YKPXMY[YKPWOP_#ZLd/Kc6]ffZ6Kd/KOP_#N.MK= A ! &sR^W'URKM4[a._0[&'"lV>= % :Xm?RM3_#UMY]_h^]*c6KU
]#b &j 44]	q`Rb= A4! [a6KZ91  c6]KMZ.]#[OQ]ffZ[_#RZ j Z [a6K]#[a6KPXha._#Z.cB`RTb= A ! `4[a6KZ
1Hk1 	fiff +   j 1 OP_#ZLd/KXKPqXRT[Y[YKZ_#M_OQ]ffZ FN.Z.OQ[R]ffZL]#b:UoRT[YKPX_#UM?Z6]#[}OQ]ffZ[_#RZ'RZ6fZ6KRT[a.KPX(Z6]#X
RT[M?Z6KPfff_0[R]ffZB
j fiM_XKMN.UT[P`B[a6K_0d/]S#Kebr]#X^&N.U_Lrq4a.RoOa RM?R]
Z 4O4=3c6]KM?Z6]#[OQ]ffZ[_#RZ `Bq4a.RoOa
^K_#Z.M[a._0[ & RYM @RT[%RZ'c6KPW/KZ.c6KZ[+brX]ff^ j

ru=
< uwuw{ 
JK
ffr( 3 *P	&59( 3 *P	&
JW
K   &  
1\vr~6uQz 
ffr( 3 *Q	&/! 
ffr( 3 *P 

J	 ,
K r( 3 *Q	& / 
' r( 3 *P	& . r( 3 *Q 
4
J	ffP 
K r( 3 *Q	& H 
' r( 3 *P	& . r( 3 *P 

Jff

K  )-
ffr( 3 *P	
& y 
 #z"tT0! z. 4y :Y ) r( 3 *Q : &












=>

ruHu

0j49XRTS*R_#Umj

j+(

 [a._0[3RM4KW"N.RS;_#UTKZ[4[Y] &`i_#Z.cMN.Oa[a._0[
 & RTb_#Z.c]ffZ.UTVhRTb8[a6KPXK}K* RM[M_b1]#X^&N.U_ 
RMMYV*Z[_#OQ[RoOP_#UUTV+@RT[%RZ'c6KPW/KZ.c6KZ[+brX]ff^Q(4j1*RZ.OQK &  
`.RT[,br]ffUUT]qM([a._0[ 
  2j

j fi3d/K
_ 4O4 b1]#X^&N.U_KW"N.RTS0_#UTKZ[[Y] & rXKMYWj 
48MPj[PjZ6]URT[YKPX _#U*]#bt(]OPOPN.XM
RoZRT[Pj=4a6KZ  / fi rXKMYW j  H fi3\RM(_br]#X^&N.U_}KW"N'RTS;_#UKZ[+[Y] &/ 
 rXKMYW j1&_H 
 `*q4a.RoOa
RoMRm
Z 4O4:`'_#Z.cZ6]eURT[YKPX_#U/]#bY( ]OPOPN6X MRZhRT[Pj
6< j *[YX_#RTfffa[Yb1]#Xq(_0Xc b1X]ff^ [a6Kbr_#OQ[&[a._0[4_0W.W/K_0XMRZ _ 4O4 b1]#X ^}N.Uo_ &RTb,_#Z.c ]ffZ.UTVLRTbF:Y
_0W'WiK_0X MRZ[a6
K 4O4 b1]#X^ ]#b : &3j
j @KP[ rXKMW
 Y



Q _Q

fif.w~3}33f$3ws

ru=
< uwuw{ 
JK
ffU	.	&5 U	6	&4
JW
K   &  
1\vr~6uQz 
ffU.	&/!.U	. 

J	 ,
K 
U .	& / 
' r( 3 *Q	&4 . 
ffr( 3 *Q 

J	ffP 
K 
U .	& H 
' r( 3 *Q	&4 . 
ffr( 3 *Q 

Jff

K 
ff
U 	. : 
& /!.U	.	&










ruHu=> [ ?RM3[YXRSR_#UI+; ff3_#Z.c   ?_0XKMR^RU_0X3[Y][a6KW.X]]#b:]#b+W/]ffRZ[Me;ff?_#Z'c   3]#b =X]#W/]0
MRT[R]ffZ `8XKPW'U_#OPRoZ6f9JYURT[YKPX _#UFLdV J%S;_0X R_0d'UTKDL*`X,dVG<'`_#Z.c 
ffr( 3 *?dV U	j fiM}[Y]  <ICeRb
> )4 
ffU	.	&+[a6KZh[a.KPXK?K*RoMY[M,_br]#X^}N'U_ 
 KW"N'RTS;_#UKZ[([Y]L& RZqa.ROa)>c6]KM4Z6]#[_0W.W/K_0X
MRZ'OQ+K >hc6]KM+Z6]#[:_0W.W/K_0X:RoZ : 
 KRT[a6KPX`2: 
 RoM\_}b1]#X^&N.U_KW"N.RS;_#UTKZ[\[Y] : & RZeq4a'ROa >c.]KM:Z6]#[
_0W.W/K_0Xj



=<



uPv & P u&YM!0wxN/T w[!0x 

 
y1  yrv5O yrx.-"y 
:uthy &0zit !0z. 4y 2vr~*u M!0r !0|yrz{ u	W/yr;#TuPz u}~G!0 t0 M!0weuQ0uPwI4c(
y1?I40z'vIv5y  01 4  y5v Oyrz"tu -6uQz"tuQz'v w[!0x (sy &0z"t+!0z. 4y /(  4 &~G!0 t0
y1 .#wIO yr.x -"
y 
:uthy }0z"t !0z. 4y vr~*u !#1 !0|yrz{u M/*y100uQzu}~ff!0Tt; M!0wuQ0uPwI4 U
y1?I40z'vI v5y  01 4 .0w Oyrz"tu -6uQz"tuQz'v [w !0x U y 0z"T
t !0z' 4y  U  4 ,. & ~ff!0Tt;

ru uwuw{

 &
&

 &
&

< 

(  1

fiE1

=>
 @9RT[%MR^eW'UR  OP_0[R]ffZ



ruHu

I C fiMMN.^KB& RMN@R[%MR^W'UoR  Kc r[aN.M( 3 *P	& ! r( 3 *Q	&4Y_#Z'c UKP[c( (Rij
pb & R MMYVZ[_#OQ[ROP_#UUV6@RT[%RZ'c6KPW/KZ.c6KZ[&brX]ff^ (4`,[a6KZ9( fi ( 3 *P	&5! `,[aN'M ( fi
8( 3 P* 	&ff! `"RmjK#jT`M(  4 &j
C5fiMMN'^K[a._0[L& RM2Z6]#[ @9RT[%MR^W"UR  KcBj4a6KZ [a6KPXKhK6RMY[M ) ( 3 *Q	&4&MPj[Pj1&
RoL
M @9RT[%RZ.c.KPWiKZ'c6KZ[b1X]ff;
^ j J RT[a ( ! #i +ff`3RT[RoMOPUK_0Xh[a._0[B& RoMMVZ[_#OQ[ROP_#UUTV
@9RT[%c6KPW/KZ.c6KZ[(]ffZ (4`6q4a.RUT
K @R[%RZ.c6KPW/KZ.c6KZ[:b1X]ff^ RT[P`.OQ]ffZ[YX _#c.ROQ[RT]ffZBj
 S=_0XMR^W'UoR  OP_0[RT]ffZBj:a6KW'X]]#bRM4MR^eRUo_0X,[Y][a6K @RT[%MRo^W'UR  OP_0[RT]ffZOP_#MYK#`"XKPW'U_#OPRZ6f J @9RT[%
RoZ.c6KPW/KZ.c6KZ[ LdK
V J[S=_0XRZ.c6KPW/KZ.c6KZ[ L*#` ( dT
V UeM` (Rd
V 
fi:M` 9dV >M` ( 3 *P	 &\dV U	.	 & j




=<



ru uwuw{

&  
fi



P 0! wuQ0uPwI4T&(w !0x
3
  1vr~*uPwYuhu,2#y1vr  yv5Oy1x.-"y 
:ut M!0wx */ T 
  ov 

RoM @RT[%RZ'c6KPW/KZ.c6KZ[b1X]ff^ (  r( 3 *P	&q+K *Z6]q [a'_0[L[a6KPXK K6RMY[ML_
ruHu=> *RoZ.OQK &
4O4 b1]#X^&N.U_ 

 KW"N.RTS0_#UTKZ[}[Y] & MN.Oa [a._0[c( 3 *Q 
@fi :( r( 3 *P	&Y!K`=RmjK#jT`=MN'Oa [a._0[
( 3 *Q 
 
ffr( 3 *P	& j A:V W/]ffRZ[;ff2]#b=X]#W/]ffMRT[R]ffZ" q+Ka._S#K 
ffr( 3 *P 
 ! 8( 3 *P	& j
aN.MP` r( 3 *Q 
4 ( 3 *P 
Y 8( 3 *P	& ! r( 3 *Q 
4 `b1X]ff^ q4a.RoOa q:K OQ]ffZ.OPUN.c6K[a._0[
r( 3 *Q 
4/! ( 3 *P 
 `'RIjK#jT` 
 RM @9RT[%MR^W"UR  KcBj



 ru=
< uwuw{   uv & P u M!0wxN/T w[!0x
M!D/*w?vI#vuPxuQz'vr&#wYueu M/*yr;0uQz'v 



 0z"tbNPu
yvuQwY0 ![(* ~*uziu,2ffv
Q_

fi



~33



JK  4 &
JWK &5j h ( A ! T
& jh
J	MK O
& A ! &5j h /
Jff
K 5
& j h ( A! fi
& 



/

s~$w

~ww$



=>



ruHu

[-I ; ff  @ KP['<&[a.K4S0_0XR_0d'UTK4]#bB_#Z.c_#MMN.^K[a'_0[ &5j h ( A4! &5j h / `q4a.RoOa^eK_#Z.M=[a._0[:[a6KPXKRM
ff_ 
fi  #D< +PIq+]#X Uc
= MN'Oa[a._0[E=;A ! &5j h ( / : &5j h / 6MRZ.OQKV& RM+KW"N.RTS0_#UTKZ[+[Y]: / &Tj h ( #H
 X:  / &Tj h   ` q:K&a._S#K = / #i +A !&5j h ( /  A ! & _#Z'c &'"lV>=G/#i + % :YI! = /#):Y +A !
&Tj h ( / : &Tj h / .a6KZ.OQK#` &'"lV>= / #i + % :YI A4! & _#Z.c[a6KPXKPbr]#XK  & dVL\X]#W/]ffMRT[RT]ffZ 0j
;ff- I     fiMMN.^K &5j h ( A ! &5j h / j=JLK&a._S#K[a6Kbr]ffUUT]qRZ6fOa._#RoZh]#bRo^W'UROP_0[R]ffZ.MMC

&

A!

&

&

A!

:/ 5& j h ( W HL :Y/ 5& j h / 
:/ &5j h / W HL :Y/ &5j h / 

&5j h

/

  -I [   @ KP[?N.M3_#MMN.^eK}[a._0[ & A ! 5& j h / ` _#Z.cW.X]	S#K[a._0[3  4 
& j3p%Z.c6KPKcB`/[a6K2_#MMN.^eW.[RT]ffZ
OP_#Zd/K?XKPqXRT[Y[YKZh_#MWC
 = ) [/  ' > =;A ! & I =;A ! &5j h / 

4]q`0= A !&5j h / RM4KW"N.RTS0_#UTKZ[3[Y]M_V[a'_0[Oa._#Z6fffRoZ6f[a6K2[YXN6[aS;_#UoN6K}]#b'=[Y]hb_#UMYK#` = RM
M[RUUB_^]*c6KUB]#b &jp%Zhb1]#X ^}N.Uo_#MP`

 =) [/ ' >=9A !
&

I &')fV>= % :YIFA ! & 

4a.RM,RM(K6_#OQ[UTV[a6Kc6K  Z.RT[RT]ffZ]#bX  4 &j
;ff-I  <  *_#^K_#M,[a.KW.X]]#b]#b;ff-I    j
 <-I [   *R^eRUo_0X:[Y][a6K?W.X]]#b9]#b,  RI [  j



 ru=
< uwuw{   uPv & PuM!0w xN/*TVw[!0xQ
3
% 0z"tc>(Pueh;0wy PQu ![6
fi  ~6u2ziu,2ffv
M!D/*w?vI#vuPxuQz'vr&#wYueu M*/ yr;0uQz'v 
J K >  4 ,. &
J WK '& gih /  '& gih (
	J MK &  '& gih /
ffJ 
K &  '& gih ( 







=> C=\_#MYVeOQ]ffZ.MKW"N6KZ'OQK4]#bB[a6Kc6K  Z.RT[R]ffZ]#b  S4RZ'c6KPW/KZ.c6KZ.OQK	& RM S=_0XRZ.c6KPW/KZ.c6KZ[brX]ff^



ruHu

>Rb_#Z.c]ffZ'UTVRb9RT[RMY@RT[%RZ'c6KPW/KZ.c6KZ[\brX]ff^ D# > % :Y> ;+  `"_#Z'c = X]#W/]ffMR[RT]ffZ ff  j




v & PuVM!0w xN/*T w[!#x 
3
% 0zit ( Pu /PQPuvY![3(R  ~*ueziu,2ffv
 uP1
ru=<uwuw{ 
vIffvuQxuQz'vr20wYuu	M/*yr;0uQz"v 

Q _D

fif.w~3}33f$3ws



JK(  4 &
JW
K 
fi	"	&T U# 19AB1 y1vuQwx vr~#v,t!	uz!ffv I!0z'vI0yrz0z4
yvuQwY0Qw !0x2( +
J	MK 	 
	
& T # 0Ar0y1}L /u}vr~#v,t!uQz!#v ,!0z"vI0y1z#z4yrvuPw%# w[!#x ( + 



=>



ruHu

 [

;ffIC

I 9C pb& R M @9RT[%RZ.c6KPW/KZ.c.KZ[9b1X]ff^ (4`ff[a6KZ2[a6KPXK,K6RMY[M8_34O4br]#X^&N.U_ 
KW"N.RS;_#UTKZ[
[Y] &sMPj[Pj( 3 *P 
<fik( ! j ?(UTK_0XUVKZ6]ffN6fffa `B[a6KW.X]#W/KPX[V_( 3 *Q 
<fi_( !RoM?MY[RUU
M_0[RM  Kc RTb 
 RM[N.XZ6Kc RZ[Y] 6
4 :`+KMYW/KOPR_#UoUTVq4a.KZ 
 RoMe[N6XZ.Kc RZ[Y] RT[MW'XR^K
Ro^W'UROP_#Z[hZ6]#X^e_#U3br]#X^hj *RZ.OQK [q:] KW"N.RTS0_#UTKZ[hb1]#X ^}N.Uo_#Mha._S#K [a6KM_#^KLW'XR^K
Ro^W'UROP_#Z[MP`6Z6][YKPX^ ]#%b 
 	"	&(OQ]ffZ[_#RZ.M4_eURT[YKPX_#U/brX]ff^Q(4j
C=
fi	i	 &hRoM9
_ 4O4 b1]#X^&N.U_ [a._0[RMKW"N.RS;_#UTKZ[[Y](& _#Z.c MYVZ[_#OQ[ROP_#UUV @9RT[%
RoZ.c6KPW/KZ.c6KZ[+brX]ffQ
^ (4j KZ.OQK#` & RMY@RT[%RZ'c6KPW/KZ.c6KZ[+brX]ff^ (j


  IC 6R^eRU_0X:[Y]e[a6KW.XR^K?R^eW'UROP_#Z[:MRT[N._0[RT]ffZB`.N'MRZ6f 5? 4 RZ.MY[YK_#ch]#b 6
 4:j

 [



  uPv & Pu !#w xN/*  w !0x 

 0z"tTU PuI/=PuPv ![
fi  ~*uziu,2ffv
vIffvuQxuQz'vr20wYuu	M/*yr;0uQz"v 
=<



ru uwuw{



JK U  4 , &
JWK
fi	"	&. T U# 19AB1 y1vuQwx vr~#v,t!	uz!ffv I!0z'vI0yrz0z4;0wy PQu  w !0x U +
J	MK 	 
	
& T # 0Ar0y1}L /u}vr~#v,t!uQz!#v ,!0z"vI0y1z#z400w ymQPPTu w !0x U +



=> +_#MYV&OQ]ffZ'MYKW"N.KZ.OQK,]#b/[a6Kc6K  Z.R[RT]ffZ]#b/ S4RoZ.c6KPW/KZ.c6KZ.OQK#`#W"UN.M =X]#W/]ffMR[RT]ffZ?_0d/]S#K#j



ruHu

=<





P

fiff   ) ff )	
 
=>
 $ L < 4L
ru uwuw{

 	   ) 	 )ff
  1 fi ff   ) ff )	
  1 fi '  ff   ) 	 )ff
 

0wYu  O[I!0x.-"uPvu 

0z"t fi ' 



ruHu

)y

q \q{ #q{

;
q

M a.RTW j}pZ ]#X c6KPX?[Y]Ma6]	q [a._0[ &sRMV@9RT[%c6KPW/KZ.c6KZ[]ffZ (`R[3RoM?MN6OPRTKZ[3[Y]
fffN.KMM_URT[YKPX_#UX(brX]ff^ (s_#Z.c_ 	U	.	&  # U	.:m +;Iq:]#XUc = [a._0[&RM_^]*c6KU]#b
&Tj h ( d'N.[4Z6]#[_^]*c6KUB]#b1&5j h / j4a6KMYK[YKMY[MOP_#ZdiK}_#Oa.RTKPS#KcRZh[Ro^KW/]ffUTV*Z6]ff^eR_#U
RoZ A &A
j
_0Xc.Z.KMMP.j @KP[N'MOQ]ffZ.MRc6KPX4[a.K&^e_0W'W'RZ6fd MPj[PjOd 	& ! +	& / M M - `Bqa6KPXK
M RM(_2W.X]#Wi]ffMRT[RT]ffZ._#U'S;_0XRo_0d'UTK[a._0[c6]KM,Z6]#[(]OPOPN6XRoTZ &37j ?(UTK_0XUTVKZ6% ]ffN6fffaB` d 	 &
OP_#Zd/K(OQ]ff^eW'N6[YKceRZ2[Ro^K,W/]ffUTVZ.]ff^eR_#URZ-A &A
ffj ]#XKP]S#KPX` & RMM_0[RoM  _0d'UTK,RTbi_#Z.c]ffZ.UV
Rb &G/ M sRoM @RT[%c6KPW/KZ.c.KZ[,]ff
Z # M +ffj



K^}diKPX

$

Lqff<q{4Lq{ ;
q

Q_6

fi

~33



s~$w

~ww$

K^}diKPX Ma.RTW j4pZ]#Xc6KPX[Y]Ma6]qs[a._0[ & RM.S=_0XYc6KPW/KZ.c.KZ[]ffZ Ue`BRT[RoM4MN6OPRTKZ[[Y]
fffN.KMM_S;_0X R_0d'UTK> brX]ff^ U _#Z'c_	U,	.	&  #D>*+;Iq+]#X Uc-= [a._0[RoM4Z6]#[_^]*c6KU ]#b
[a.Kbr]#X^&N.U_ &'gih /  &'g ( j=a.RoM=[YKMY[,OP_#Zd/K_#Oa'RTKPS#KchRZ[R^KWi]ffUVZ6]ff^R_#U.RZGA &A
j
_0Xc.Z.KMMPj *R^RU_0X[Y] [a6K a._0X c.Z6KMMW.X]]#be]#b @ c6KPW/KZ.c6KZ.OQK#`&XKPW'U_#OPRoZ62
f J @9RT[%
c.KPWiKZ'c6KZ[ L2dV J[S=_0XYc6KPW/KZ.c6KZ[ L*j

 >#'$Uy

L qff<q{MLq{ ; q
#
K^}diKPX Ma.RTW jY& RMbN.UUV%@R[%c6KPW/KZ.c6KZ[]ffZ_( ! #i (&%('''%  ),+&RTb_#Z.c]ffZ.UTVhRTb &5j  h ( A 4!
&Tj  h / _#Z.cjTjTj_#Z.c &5j h ( A4! &5j h / a6]ffUc.MPj1"N'RTS;_#UKZ[UTV#`& RM4brN'UUTV+@RT[%c6KPW/KZ.c.KZ[
]ffZ ( ! #i ( %('''%  ) +hRTb _#Z.c ]ffZ.UTV Rb,[a6Kbr]#X^&N.U_-3 0- ( 	&5j  h ( / : &5j  h /  / ''' /
  0 - ) 	 &5j  h ( / : &Tj  h / \RM+M_0[RM  _0d'UK#`q4a.KPXK4K_#O a3 0 -&0RM+_}XKZ._#^eRZ6f[a._0[
^_0W'MS0_0XR_0d'UKM?[Y]Z.KPq MYV*^d/]ffUMRZ_N'Z.RTbr]#X^ q:_V6 j 6RZ.OQKe[a.RoM?b1]#X ^}N.Uo_OP_#Zd/K
OQ]ff^eW'N6[YKcRZ[Ro^K?W/]ffUTVZ.]ff^eR_#UiRZ A &A A UA
`6[a6K^K^}d/KPXMa.RTW]#*b fi 'ff 	   ) 0
	 )ff
  [Y]  b1]ffUoUT]q4Mj
_0Xc.Z.KMMPj=.N.UoU' @Bc6KPW/KZ.c.KZ.OQK_#Z'c @Bc.KPWiKZ'c6KZ.OQK3OQ]ffRZ.OPRc6KRZe[a.KOP_#MYKRoZeq4a.RoOa
( RM3OQ]ff^W/]ffMYKc ]#b+_MRZ.fffUTK&UoRT[YKPX_#Umj *RoZ.OQK}[a.K  'a._0Xc.Z.KMM3]#b+ @Bc.KPWiKZ'c6KZ.OQK2a._#M
d/KPKZ W.X]S#KcN.MRZ6f_hMYKP
[ ( OQ]ff^W/]ffMYKc ]#b(_hMRZ.fffUTK&UoRT[YKPX_#Um`B[a6K  'a._0Xc.Z6KMM3]#b+bN.UU
 @ c6KPW/KZ.c6KZ.OQK3br]ffUUT]	q4MPj



 >#'$

L q <q{4L#q{ ; q

K^}diKPX Ma.RTW jB& RMbN.UUVBS=_0XYc6KPW/KZ.c.KZ[}]ffZ#U ! #D> ( %('''% > ) +hRTb,_#Z.c]ffZ.UV Rb &  4
&5g  h / _#Z.cjTjTj_#Z.c'&  4 &'g h / a6]ffUc.MPj,1"N.RS;_#UTKZ[UTV#` & RM4brN.UoUTV S=_0XYc.KPWiKZ'c6KZ[]ffZ
U-!$#D> ( %('''% > ) ++RTb*_#Z.c]ffZ'UTV Rb[a6K\b1]#X^&N.U_   0- ( 	&  &'g  h /  / ''' /<  0- ) 	& 
&5g h / ,RoM4M_0[RM  _0d'UTK#`.qa6KPXKK_#O a  0 -& 0=RM4_XKZ._#^eRZ.f[a._0[^_0W'MS0_0XR_0d'UKM,[Y]
Z.KPq  MYV*^d/]ffUMeRoZ_N.Z'RTb1]#X ^ q:_V6 B
j *RZ'OQK[a.RM?br]#X^&N.U_hOP_#Zd/KOQ]ff^eW'N6[YKcRoZ [R^K
W/]ffUTV*Z6]ff^eRo_#URZ A &A A U A
`#[a6K,^K^}d/KPXMa.RW]#b fi ' fi 	   ) 	 )ff
  [Yff
]  ebr]ffUUT]	q4MPj
_0Xc.Z.KMMPj=.N.UoUi Sc6KPW/KZ.c6KZ.OQK?_#Z.c Sc6KPW/KZ.c6KZ.OQK?OQ]ffRZ.OPRoc6K?RT&b ( RM(OQ]ff^W/]ffMYKc]#b
_3MRoZ6fffUTK+UoRT[YKPX_#Umj *RZ.OQK+[a6K  'a._0Xc.Z.KMM]#b" S4c.KPWiKZ'c6KZ.OQK(a._#MdiKPKZ}W.X]	S#KcN.MRZ.f_
MKPY[ (LOQ]ff^eWi]ffMKc}]#b"_MRZ6fffUTK:URT[YKPX_#Um`;[a6K  .a'_0Xc.Z6KMM9]#b'bN.UU S4c6KPW/KZ.c.KZ.OQK+br]ffUUT]	q4MPj



 h~*uQziuP0uPw & P uP 0! z{0v	! PT;  ![ N?P M!0wxN/T;vr~ffvy12vwYvI PQu M!0w
=<
PTD/ Q# W/.uPwI420z6|+uQwy1z{BJIy ru  18vr~*uQwuu,2#y1vrY-!0 4#vyrxu0{Q!0wyvr~x v	!tuPvuPw x2yrziu3|8~6uPvr~*uPw &OA !=1
M0! w&0z=4NOP M!0wxN/Tff1K0z"tQvIQPQu M!0w00wyQPPTuy1z.vI0z'vym#vy0!0z Jy u I1+wuR-"Ty1z{hy1z & ) 0z4
;#w ymQQP u PM2
4 [YXN6K !0w P42br_#UoMYKe{#yr0u2!#w xN/* vr~#v(Qvy1r PuQ !0z{#&v	! Kvr~6uQz ff   ) 	 )ff
  1
fiff   ) ff 	) 
  1 fi '  ff   ) ff )ff
  0z"t fi ' fi	   ) 	 )ff
  0wu}yrz  


ru uwuw{

K "N6KZ.OQK?]#b =X]#W/]ffMR[RT]ffZ.M 2_#Z.cBj=J a6KZB& d/KUT]ffZ6fffM:[Y]
ruHu=> a.RM(RM(_MY[YX_#Rfffa[Ybr]#Xq(_0XchOQ]ffZ.MYW
_OPU_#MM  ]#b:br]#X^&N.U_#M?[a'_0[RM?[YX_#OQ[_0d"UTKebr]#X}OPU_#N.M_#U "N6KPXV_#Z.Mq+KPXRoZ6f_#Z.cMY[_0d"UTKebr]#XS;_0X R_0d'UTK
RZ.M[_#Z[R_0[R]ffZB`6q+KOP_#ZK_#MRUTVOa.KOhq4a6KP[a.KPX.&OA ! &'gih / _#Z.c'&'gih / A ! & a6]ffUc'MPj



Q_+

fif.w~3}33f$3ws

=<



ru uwuw{


ruHu

=>



%

yC 'v

&

  #(0 !    fi  fi 132  'ff #z"t fi

<' ! ;

 2 0 !  

 fi  fi 1/2 '0wu

 O[I!0x.-"Tuvu 

zuw{

K^}diKPX Ma.RTW j +_#MYV OQ]ffZ.MYKW"N6KZ.OQK]#b3[a6Kbr_#OQ[e[a'_0[  #(0 !    fi  fi 132  ' RM
_ XKMY[YX ROQ[RT]ffZ ]#b fi '  	   ) ff )ff
  [a'_0[2RoMRZ   _#Z.c  RM}OPU]ffMYKc N'Z.c6KPX

W/]ffUTV*Z6]ff^eRo_#U"XKc.N.OQ[RT]ffZ.M j
_0Xc.Z.KMMPjJLK?W.X]	S#K[a._0[,_2br]#X^}N'U_ 
`*d'N.RUT[+]S#KPX_#Zh_#UTW'a._0d/KP['A ! #D> ( %('('('>% > ) +ff`
RoMM_0[RM  _0d'UTK?RTb9_#Z'ch]ffZ.UTVRTb9br]#X^&N.U_ & RYM @BMR^eW'UR  Kc `q4a6KPXK

&"! 


H 
 A3:%Ar0/  >
g 	

0 

@0

qa6KPXK 

 A3:%AffRM:[a6K?b1]#X ^}N.Uo_]#d.[_#RoZ6KcdVXKPW'U_#OPRoZ6f&RoZ 
 K_#O a]*OPOPN6XXKZ'OQK]#b%> 0
rXKMYW j6:%>0:qRT[a-:%> 0,rXKMYWj-> 0I j
8RTX MY[P`:Rb 
 RoMZ6]#[M_0[RM  _0d'UTK#`:Z6KRT[a6KPX 
 A3:%ARoMP`\[aN.M 
=H 
 A3:%Ar2RoMeZ6]#[
M_0[RM  _0d'UK#ffj fiM_2XKMN.UT[P` & RMZ6]#[M_0[RoM  _0d'UTK#`6[aN.MR[RMZ6]#[ @9RT[%MR^W"UR  KcB`d/KOP_#N.MYK
8 ( 3 *P	 &ff ! &d"N6.[ & ^KZ[RT]ffZ.M,S0_0XR_0d"UTK5M > 08_#Z.m
c @0j
fiMMN'^K 
 M_0[RM  _0d'UTK#j ?(UTK_0XUV#G` & RM+M_0[RM  _0d'UK4_#M:q+KU	U CUTKP[E= d/K_^]*c6KU"]#
b &jJLK

W'X]S#K([a._0[ &RoM @9RT[%MR^W"UR Kc3dVMa6]	q4RZ6f[a._0[R[RM @RT[%c.KPWiKZ'c6KZ[9]ffZK_#O aURT[YKPX_#URT[
OQ]ffZ[_#RZ.Mj,JLK&a._S#ff
K ( 3 *P	 &7 ! #D> ( %('('(' % > ) % @ ( %('('('&% @ ) % %: > ( %('('('&% %: > ) % %: @ ( %('('('>% Y: @ ) +ffj
@KP+
[  0 )k( 3 *P	 & j
[ 5
 fiMMN'^K[a'_0[,= A !Q 0j a6KZ `<&'"l V>= % X:  0 A4! &jpZ'c6KPKcB`UTKP[ U	.:  0I
 ! >0
rXKMYW j @0mI C<= M_0[RM  K*M > 0  @0 M]&O a._#Z6fffRZ6f&[a6K[YXN6[aS;_#UoN6K4]#b >0 ]ffZ.UTVrXKMYW j @0
]ffZ'UTV6:UK_#c.M,[Y]_e^e]c6KU/[a'_0[4c6]KMZ6]#[4M_0[RoMYb1
V > 0  @ 0j
; ff [a6KPXq4RMK#`q:Ka._S#K = A ! X
: 0'
j E4KPW'U_#OPRZ6
f  0 dV Y:  0 RZ[a6KW.X]]#b,]#b4OP_#MYK[ 
KZ'_0d'UTKMc6KPX RTSRoZ6f}[a.KK W/KOQ[YKcOQ]ffZ.OPUN.MR]ffZBj

 %

pz3r 'v

<' ! ;

zuw{



fi K^}diKPX Ma.RTW j=+_#MYVhOQ]ffZ.MYKW"N6KZ.OQK}]#b[a.Kbr_#OQ[[a._0[ fi  2 0 !    fi  fi 132 ' 
RoM_XKMY[YXROQ[RT]ffZ]#bfi 'ff fi 	   ) 	 )ff
  [a._0[&RMRZ  _#Z'c   RMOPU]ffMYKc
N'Z.c6KPX,W/]ffUTV*Z6]ff^eR_#UiXKc.N'OQ[RT]ffZ" j
fi _0Xc.Z.KMMPj94a6KW.X]]#bRMMR^eRUo_0Xi[Y]4[a6KW.X]]#b]#b  'a._0Xc.Z6KMM/]#b   #(0 !    fi 
fi 1/2  '`*XKPW'U_#OPRZ.]f J @9RT[%MR^eW'UR  Kc Lq4RT[9
a J[S=_0XYMR^W"UR  KMc L*j


=<
  2 uvuQwx&yrz.yrz{h|8~6uPvr~*uPw
ffr( 3 *P	& !
|8~6uPvr~*uPw
U .	
&  !=A JI|8~*uPwYu?A

ru uwuw{

( IJ |8~*uPwYu( yoPuv ![&
yvuQwY0
 K 130z"ttuPvuQwx2y1z'y1z{



1y }Puv ![;0wy PQu Key1 " - O[I!0x.-"uPvu 
  ~6u&Pu0w Q~ -"w !QPPTuPx ,!0z6yoQvy1z{hyrzOQ]ff^W'N6[RZ.f
r( 3 *P	&%JIwu	-.u3vyr0uP 4 U	.	& Ky1
yrz   - 0z"ty1 P3!#vr~   0z"t  O~*0w%t 

Q _W]

fi



~33



s~$w

~ww$

=> C

ruHu

 KP[YKPX^RZ.RZ6fqa6KP[a6KPXFr( 3 Q* 	&4ff!

( RoM

" - OQ]ff^W'UTKP[YK#j

K^}diKPX Ma.RTW j
8( 3 *P	& !

(RTb,_#Z.c ]ffZ.UTVLRTbR1   & a6]ffUc.Mb1]#X&KPS#KPXV  )=(_#Z.c RR16  4 &
.a ]ffUc.Mb1]#X3KPS#KPXV] ) ( 3 *Q	&4 +(B[a6KMYKP[?]#b+RZ.MY[_#Z.OQKM +	& % ( -MN.O a [a._0[eR1a6]ffUc.MRM
[a.K&N.Z'RT]ffZ]#b\_URoZ6K_0XZN.^}diKPX]#b=W.X]#d'UTK^eMRZ  9`i[aN.MRT[3RMRoZ  BMRo^eRU_0XUV#`.[a6K
MKP[:]#bRZ.MY[_#Z'OQKM +	& % ( -+MN.O a[a'_0[?RoR1=a.]ffUc.M:RM+_&W.X]#d'UTK^ RZ  9j.a.RM+W.X]	S#KM([a6K
^eK^d/KPXMa'RTW[Y] " - ]#b8[a6K?W.X]#d'UK^ ]#bc6KP[YKPX ^eRZ.RZ.f&qa6KP[a6KPXFr( 3 *Q	&4ff! (j
_0Xc.Z.KMMPj
@KPfi
[ + 
 % -\d/K_&W'_#RX+]#b9W.X]#W/]ffMRT[RT]ffZ'_#U.br]#X^}N'U_#MPj8J R[a6]ffN6[(UT]ffMM:]#bf#KZ6KPX _#URT[V#`*q:K?_#M%
MN.^K[a._0[ 
L_#Z.c c.]4Z6]#[Ma'_0XK=_#ZV3S0_0XR_0d'UK8_#Z'c?[a._0[ A ! U. 
/ !$#D> (&%('''% > )+ff`
U,	. /
 !$# ( %('''%   +ffj8.N6X[a6KPX^]#XK(q:K,_#MMN.^K([a._0Y[ A ! 4 2RTb'RT[q:KPXK,Z6]#[8[a6K,OP_#MYK
R[q:]ffN.Uc[a6KZ d/K2MN6OPRTKZ[3[a6KZ [Y]XKPW'U_#OQK 
 dV 
 />Y* H:*4d/KPb1]#XK&W/KPXbr]#X^eRZ6f
[a.K?XKc.N.OQ[RT]ffZi jJLK?W.X]S#K?[a'_0[ + 
 % -:RM,_2W/]ffMR[RTS#KRoZ.MY[_#Z.OQK3]#b !$ #(04'*)M!$ #9`RmjK#jT` 

RoMM_0[RM  _0d'UK&_#Z.c RM3N.Z'M_0[RM  _0d"UTK#`iRb=_#Z.c]ffZ.UTVRTbE
ffr ( 3 *P 
 /:  ! ( 3 *Q	 & 
Y
qa6KPXK &? 
/ ! 
H 

 A3%: Ar / g 	 > 0  @ 0 _#M,RoZ_&W.XKPSRT]ffN'M\W.X]]#b+Z.]#[YK?[a._0[
& 
(_#Z.c c6]Z6]#[Ma'_0XK_#ZVS0_0XR_0d'UKM  j
RrRb 
 RMLM_0[RM  _0d'UTK _#Z.c
RM N.Z.M_0[RM  _0d'UK#`[a6KZ &? 
4RoM @RT[%MRo^W'UR  Kc _#O
OQ]#X c.RZ6f [Y] [a6KW.XKPS*RT]ffN.MW.X]]#b&_#Z.c [aN.M
ffr ( 3 *Q	 &? 
4Y !;( 3 *Q	 & 
Yq4a.KPXK_#M
8 ( 3 *P :  !.`'q4a.RoOa[Y]#f#KP[a6KPXKZ[_#RU [a._0[Dr ( 3 *Q	 & 
0/ : / ! ( 3 *Q	 &? 
4Y j
RoR1 fiMMN.^eK[a._0[ 
 RMN.Z.M_0[RoM  _0d'UTK_#Z'cr ( 3 *P	 &? 
0/: 1 ! ( 3 *P	 &? 
Y ja6KZ
& 
iRM Z6]#[ MR^W'UR  Kc&rXKOP_#UU#[a._0[ U	6 
4iRMBZ6]#[ K^W'[V?_#Z.c?a6KZ.OQK+MY]RWM ( 3 *Q	 &? 
4YY `
[aN.M6
ffr ( 3 *Q	 &? 
4Y "( 3 *Q	 &? 
4Y -j 44]	q`2
ffr ( 3 *P	 &? 
 / : ff ! ( 3 *Q	 & 
Y=Ro^W'URTKM
8 ( 3 *P	 &? 
0/: M)
fi ( 3 *P 1 !O`iq4a.RoOaRMWi]ffMMRTd'UTK3]ffZ.UVRb RM4_e[_#N6[Y]ffUT]#f#V rd/KQ
OP_#N'MYB
K &? 
?_#Z'c c6] Z6]#[2Ma._0XK_#ZVS;_0XRo_0d'UTKM  =d"N6[}RZ[a.RM&OP_#MYK#` &? 
4@/ : RM
RoZ.OQ]ffZ.MRM[YKZ[?_#Z.cL[aN.M r ( 3 *P	 &? 
</ : 
 ! `a6KZ.OQK#&` ( 3 *P	 &? 
Y !`9q4a.RO aLRM
OQ]ffZ[YX_#c.RoOQ[Y]#XV#j
RoRR17 fiMMN.^K?[a._0[ RM,M_0[RoM  _0d'UTK_#Z'c 
ffr ( 3 *P	 &? 
M/ : / ! ( 3 *P	 &? 
Y j
4a6K MYKOQ]ffZ'csOQ]ffZ'c.RT[RT]ffZ OP_#Zsa.]ffUc ]ffZ.UV RTb &? 
hRoMN.Z'M_0[RM  _0d"UTK#`a6KZ.OQK Z6]#[ @9RT[%
MR^W'UR  Kc MRoZ.OQG
K ( 3 *P	 &? 
Y ! 4 ff `?MY] 
 RMN.Z.M_0[RoM  _0d'UTK_#Mhq:KUUm[a.RoM[_0#KMN.M
d"_#O[Y][a6KOP_#MKRR1 `6q4a'ROahUTK_#c'MN.M,[Y]e[a.KM_#^KOQ]ffZ[YX_#c.ROQ[R]ffZ_0fff_#RZBj

 ?(]ff^W'N6[RZ.f r( 3 *Q	&44RoMRoZ   - MRZ.OQK ff   )
( 3 *P	& a6KZ.OQK#`\RT[&RM2MN6OPRKZ[}[Y][YKM[&br]#X2KPS#KPXVK

ff )	
  RMRZ  _#Z.c r( 3 *Q	&46
)=( 3 *P	&q4a6KP[a6KPX2]#XZ6]#[ & RM @9RT[%

RoZ.c6KPW/KZ.c6KZ[b1X]ff^ RT[  jp[+RM+_#UMY]}di]#[a  .a._0X c_#Z.c  .a._0X cBj  .a._0X c.Z6KMM:RM\Ma6]q:Kc
dV [a6K br]ffUUT]	q4RZ6f Wi]ffUVZ6]ff^R_#U,XKc.N'OQ[RT]ffZ b1X]ff^ !$#C fi ?54 br]#X^}N'U_ & RMM_0[RM  _0d'UTK
Rb}_#Z.c ]ffZ.UTV RTb& RMS0_#URc ]#X r( 3 *P &} ! 4 &OPUTK_0XUTV KZ6]ffN6fffaB`S0_#URc ?54 br]#X^&N.U_#M
OP_#Z d/KXKOQ]#fffZ.RTgPKc RZLW/]ffUTV*Z6]ff^eRo_#U9[Ro^K#j4aN.M`<!$# OP_#ZdiKMY]ffUTS#KcRb+q:K*Z6]q a6]q [Y]
OQ]ff^eW'N6[YK 
ffr ( 3 *Q	&4br]#X?_#ZV &`/q4a.RO aMa6]q4M[a._0[?OQ]ff^W"N6[RZ6f
ffr( 3 *Q	&4RM  .a'_0XcBj
4a6K?W.X]]#b9]#b  'a._0Xc.Z6KMM4RMMR^eRU_0X & RMN.Z.M_0[RM  _0d'UKRTb8_#Z.ch]ffZ'UTVRb4& RoMZ6]#[S0_#URc
_#Z'c 
ffr ( 3 *Q &} ! ff j

Q*\(

fif.w~3}33f$3ws

 KP[YKPX^RZ.RZ6fhqa6KP[a6KPXU	6	&4 ! U R M 
  - OQ]ff^eW'UTKP[YK#j K^}diKPX Ma.RTWLbr]ffUUT]	q4M?K_#MRUV
brX]ff^ [a6K+^K^}d/KPXMa.RW]#b6[a.K+OQ]#XXKMWi]ffZ'c.RZ6f(W.X]#d"UTK^ br]#X4r( 3 *Q	4&  j_0Xc.Z6KMM RMMR^eRU_0X
[Y][a6Ka._0Xc'Z6KMM(W.X]]#bb1]#XD8( 3 *P	& /! (4j
 ?(]ff^W'N6[RZ.fU	.	&RM=RoZ



 - _#Z'ceRMd/]#[a  'a._0Xce_#Z.c  .a._0X cBj *R^eRUo_0X[Y]}[a6K

OQ]#XXKMYW/]ffZ.c.RZ.f&XKMN.UT[(br]#XD
ffr( 3 Q* 	& j

=<



ru uwuw{

<



~*uPuv1![x !tuQ
N![F&'))(%+* ( 3 *Q	& % #i +;N 0z Puu,2 -"wu Puth0

d 'fe" &'))(%+* ( 3 Q* 	& % i#  +;Y

&')lV >= % :YI A2=;A ! & +
! #+= A &')fV>= % mDA ! &+
 ruHu=> a6K?W.X]]#b8RM(]#d.[_#RZ6KcR^e^Kc.Ro_0[YKUTVbrX]ff^
[a6Kc.K  Z.RT[RT]ffZ j

 ru=<uwuw{
W ~*uPu1
v ![x !tuQ
N
 ![F&'))(%+* ( 3 *Q	 & % (: 0z P u3u 2 -iwYuQPuth;
d ' ei &'))(%* ( 3 *P	 & % ((Y !$#+= A&'"l V>= % ( ( FA ! & |8~*uQw6u ( ( 9( +
 ruHu=> A+VRZ.c.N'OQ[RT]ffZ]ffZ A (A
j\a6K3d'_#MK3OP_#MKRZq4a.RO b
a ( RM(K^W.[VRM([YXRSR_#UIj @KP[Z6]q _#MMN.^K
[a._0[[a6KW.X]#WiKPX[Vha6]ffUc.Mb1]#X3_#Zk
V ( OQ]ff^W/]ffMYKc]#b hKUTK^KZ[MP`/_#Z.cW'X]S#K}[a._0[R[4a6]ffUc.M4_#M4q:KUU
br]#+X ( . #i +ffj
A:Vhc6K  Z.R[RT]ffZB`
=;A !.&'))(%* ( 3 *P	 & % ( . #i +;
Rb9_#Z.ch]ffZ'UTVRTb =;A ! &')"( * ( 3 *P &'))(%* ( 3 *P	 & % (( % #i +;Y
RTb8_#Z.c]ffZ.UVRTb =;A ! &')"( * ( 3 *P	 & % (:,]#XD&')l V>= % IDA !.&'")(%+* ( 3 *P	 & % (:
RTb8_#Z.c]ffZ.UVRTb &'"l V>= % ( ? FA !.&'))(%* ( 3 *Q	 & % ((:q4a6KPX
K ( ? #i +
MRZ6f&[a6KRZ'c.N.OQ[RT]ffZaVW/]#[a6KMRMP`*q:K?OP_#ZhK W'XKMM:[a6KMYKP[(]#b9^e]c6KUoM+]#b<&'))(%* ( 3 *Q	 & % ((:_#M
[a6K^e]c6KUoM = ? MN'Oah[a._0[&')f V>= ? % ( (  A ! &`.qa6KPX3
K ( ( RM,_#ZVhMN6d'MKP[,]#%b (4j fiM4_XKMN.U[P`
=;A !.&'))(%+* ( 3 *Q	 & % ( . #i +;
RTb9_#Z.c]ffZ.UTVRb &')f V>= % ( ? ff !9= ? _#Z.c &')l V>= ? % ( (  A ! & % q4a6KPX
K ( ? #i +}_#Z.k
c ( ( (
RTb9_#Z.c]ffZ.UTVRb &')f V>= % ( ? (  A ! & q4a6KPX
K ( ? ( "( . #i +
fiM_ XKMN.U[P`=[a6Kh^]*c6KUM&]#b&'")(%+* ( 3 *P	 & % (:&_0XKh[a.K^e]c6KUoM}[a'_0[eOP_#Z diKh^e_0W'WiKc RZ[Y]
^]*c6KUM(]#b & dVbr]#XOPRZ.fe_MN6d'MYKP[,]#bUR[YKPX_#UM(Rm
Z ( [Y]ed/KOQ]ff^K?[YXN6K#j



!

d 'fe"	&

. #

(** &'))(%*( 3 *Q	& % ((&y1
 uPv &:Pu !0wxN/*   w !0xQ

 0zit (
vr~*u !{#y00r 4Qvw[!0z{uQv I!0z6u	M/'uPz u ![.& vr~ #v+yo  yrv5Oy1zituR-6uPz"tuPz'vw[!#x2( 	J /*-v	! !{ffy5 0u	W/*y1*O
0uQzu K 
=<

ru uwuw{

&

 ruHu=> A:V RZ.c'N.OQ[RT]ffZ]ffZ A (A
ja6Kd'_#MYKOP_#MYK A (A !
 RM}[YXRTS*R_#Umj @ KP[N.M2Z6]q _#MMN.^K[a._0[
[a6KeW'X]#W/]ffMRT[RT]ffZ a6]ffUc'M3b1]#X}KPS#KPXV A (AL_#Z.cMa6]q [ a'_0[RT[XK^_#RZ.M3[YXN6Kqa6KZ;A (A ! 0j

Q*\

fi

~33



s~$w

~ww$

@ KP[m(=!

( ? . #i +ffj A+V [a6KLRZ.c.N'OQ[RT]ffZ aVWi]#[a.KMRMP`q:KLOP_#Z _#MMN.^eK[a._0[ &'")(%+* ( 3 *P	& % ( ?o
R M[a6K ^]ffMY[Lf#KZ.KPX_#UOQ]ffZ.MYKW"N6KZ.OQK ]#bT& [a._0[LRM @RT[%RoZ.c6KPW/KZ.c6KZ[brX]ff^ ( ? j 6]#X[a6K M_0#K
]#beMR^eW'UROPRT[V#`?UTKP[ &6??c6KZ.]#[YK [a.RoMbr]#X^}N'U_*j p[XK^e_#RZ.M[Y] Ma.]q [a._0[&'))(%+* ( 3 *Q	& % (: !
&'))(%* ( 3 *Q &'))(%+* ( 3 *Q	& % ( ?  % #i +;! &')"( *( 3 *P	& ? % #i +;?RM3[a6K^]ffMY[?f#KZ6KPX_#U+OQ]ffZ.MYKW"N6KZ.OQKe]#b
&6?i[a._0[4RY
M @RT[%RZ'c6KPW/KZ.c6KZ[+brX]ff^ j\,q:]OP_#MYKM_0XK[Y]ed/K?OQ]ffZ.MRc.KPXKcC
^ >RTb8_#Z.c]ffZ.UTVRTb
  !=>9j &'))(%* ( 3 *Q	 &6? % >B(RM @R[%RZ.c6KPW/KZ.c6KZ[:b1X]ffQ
&'")(%+* ( 3 *P	 & ? % >/FA !.&'")(%+* ( 3 *P	 & ? % >/ gih / a6]ffUoc.MPj8JLKa._S#K
	& ?gih ( HL :%> /T& ? Ygih
/

 	&

?gih ( H & ?gih / 

4a.RM/br]#X^}N'U_(OPUTK_0XUTV3RMB_UT]#fffRoOP_#U#OQ]ffZ.MYKW"N6KZ.OQK\]#b &'))(%*( 3 *Q	& ? % #D>*+; j<KZ.OQK &'")(%+* ( 3 *P	& ? % >/
RoM @RT[%RoZ.c6KPW/KZ.c6KZ[3b1X]ff^ >j
&')"( * ( 3 *P	 &6? % >/RM_UT]#fffRoOP_#U\OQ]ffZ.MYKW"N6KZ.OQK]#bY&6?1jp%Z.c6KPKcB`
br]#X:KPS#KPX+
V & ? )k

i` > ) 
fi:`*q+Ka._S#K & ?  >/ & gi? h (  H Y: >/ & gi? h /  jp[\XK^_#RZ.M
[Y] Ma6]	q [a'_0[KPS#KPXVU]#fffROP_#U+OQ]ffZ'MYWK "N.KZ.OQK 
 ]#.
b &6?[a._0[2RM @9RT[%RZ.c.KPWiKZ'c6KZ[3brX]ff^ > RoM_
U]#fffROP_#U+OQ]ffZ'MYWK "N.KZ.OQK]#bF&'")(%+* ( 3 *P	 &6? % >/ B
j @KP[ 
 _#ZVLbr]#X^&N.U_Mj[P'
j &6?DA ! 
 a6]ffUc.M}_#Z.c

 A ! 
'gih / j 6RZ.OQK & ?   > B
/ & gi? h ( & HL %: > / & gi? h / ,a6]ffUc.MP`'q+K&a._S#K & ? A ! 
 RTb8_#Z.c]ffZ.UV
Rb,di]#[a  >/ &6gi? h (  A ! 
 _#Z.c  Y: >/ &6gi? h /  A ! 
 a6]ffUc jaN.MP`RZ]#X c6KPX}[Y]LMa.]q [a._0[
&'")(%+* ( 3 *P	 & ? % >/A ! 
sa6]ffUc.MP`/RT[3RM3MN6OPRKZ[[Y]hW.X]	S#K[a._0[ & gi? h ( A ! 
 a6]ffUc'MPj *N6W.W/]ffMYK
[a'_0[8RoM9Z6]#[8[a6K(OP_#MYK#j=a.KZB`0[a6KPXK:K *RM[M_#Z2R^eW'UROP_#Z[ 1e]#b & gi? h ( [a._0[RMZ6]#[_#Z2R^W'UoROP_#Z[
]#b 
3j ]q:KPS#KPX`BMRZ'OQK >/ &6?gih ( A ! 
 a6]ffUc.MP`'q+K}Z6]	q [a._0[ > b
/ 1BRM_#ZR^W'UoROP_#Z[,]#b

3j *RZ.OQK 
 RMWK "N'RTS;_#UKZ[[Y][a6K&OQ]ffZ FYN'Z.OQ[RT]ffZ]#b+RT[MW.X R^KR^eW'UROP_0[YKMP`'[a6KPXK}Z.KOQKMM_0XRUV
K6RMY[M,_&W.X R^KRo^W'UROP_0[YK]#b 
 MPj[Pj >
/ 1BFA !_#Z'c 1 A4! a6]ffUcBja'RM+Ro^W/]ffMYKM:[a._05[ >
d/KUT]ffZ.fffM=[Y]d'N6[+Z6]}UR[YKPX_#U6]#tb 1hd/KUT]ffZ6fffM=[Y]8j A:VOQ]ffZ.MY[YXN'OQ[RT]ffZB`gih / RM+_#ZR^eW'UROP_0[YK]#b

5gih / _#Z.cMgih / RM4M[YXROQ[UTVMY[YX]ffZ6f#KPX3[a._#Z8j *RZ'OQK 
CA ! 
'gih / a.]ffUc.MP` 
BA !Mgih / a.]ffUc.M
_#M4q+KUoUmj=a.RM(OQ]ffZ[YX_#c.ROQ[M4[a6Kb_#OQ[[a._0[	LRoM_2W.XR^eK3Ro^W'UROP_0[YK3]#b 
j
: >j\a.K3c.K^]ffZ.MY[YX_0[R]ffZRMMRo^eRU_0X`iN
x /6vI#vyoN
x /6vI0z"t#yoj
  !OY



=<



ru uwuw{

^



  0z"tff(99(   
& ')"(  *( 3 *P	& % :( H
&'))(%*( 3 *P 
 % ((  '

 uPv &1 
Pu}v| ! M!0w xN/*T; w[!0x

&'))(%*( 3 Q* 	&KH 
 % (:"

^ ]c.KUM]#b
=> a6K PO U_#R^ OP_#Zd/KK_#MRoUTV W.X]	S#Kcb1X]ff^Z\X]#W/]ffMRT[RT]ffZ W j p%Z.c6KPKcB`}[a6K 
&'))(%*( 3 *Q	
& H 
 % ((_0XK,[a6K^]*c6KUM<=MN.Oa2[a._0[ &'"lV>= % ( (  A ! 
& H 
`#qa6KPXK/( ( "(4jX44]	q`
&')lV>= % ( (  A !:& H 
a6]ffUc.MRb:_#Z.c]ffZ.UTV RTb6&'"lV>= % ( (  A ! &a6ff] Uoc.M?]#X,&'"lV>= % ( (  A ! 



ruHu

a6]ffUc'MPj
Zh[a.K?]#[a6KPXa._#Z'cB`*[a6K^]*c6KUM(]#b<&'))(%*( 3 *P	& % ((&H&'))(%+* ( 3 *Q 
 % (:(_0XK[a6]ffMYKMN'Oah[a._0[
&')l V>= % ( ( A !& ]#X&'"l V>= % ( (  A ! 
 b1]#X?MY]ff^eKc( ( (j?a.RoM4RMKW"N.RTS0_#UTKZ[[Y][a6K_0di]	S#K
OQ]ffZ.c.R[RT]ffZBj


Q*\*_

fif.w~3}33f$3ws

 uP&v 1 Pu2+,!#z6 y1vuPz'v(vuQwx w[!0x 

 0z"tff(99(R 
ru=<uwuw{ &
&'))(%*( 3 *Q 1 % (: ff j  j   
 


=> C\JLK  X MY[,W.X]	S#K[a6Kbr]ffUUT]	q4RZ6f2UTK^e^e_GC



ruHu

y?qvv

z

#i +

k P !0w20z4LI!0z6yoQvuQz'v(vuQwx 141\|+u~0#u&'))(%*( 3 P*  1 % I  1

,w[!W! %JIuQx2xe K C

;

z$q

;

z$q

;

z$q

k  ))19` y u o`B1 ! /1 ?1j
&'")(%+* ( 3 *P 1 % I  1Bj h ( HL :X#Hb1B 1 ? =1  #i +ffj
:  ))19`By u o`B1 !O:X /1 ?j
7 Y
&'")(%+* ( 3 *P 1 % I 9H1 1 91  #i +ffj
:  ))
4 1_#Z.c Y
4 19j
H  ))
&'")(%+* ( 3 *P 1 % I  1 HL Y: MH1  1=1 #i +ffj

fiMY[YX_#Rfffa[Ybr]#Xq(_0XcRZ.c'N.OQ[RT]ffZ]ffZk(

=<



ru uwuw{



ruHu

	

  &')"(

*(




OQ]ff^eW'UTKP[YKM[a.KW.X]]#bj

   uPv &Pu&.!0wxN/*  w[!#x2

  0z"tff(=
3 *P	& % (:Y7!$#0Ar0 )	 
	&#z"tff( 3 *P0;4fi)(@!O +

( 




=>

C @KP['0 )	 
 &'))(%*( 3 *P	& % ((Y j *RZ.OQK.& A !.&'))(%*( 3 *P	& % ((=_#Z.c &'))(%*( 3 *P	& % ((FA !
0a6]ffUcB` 0RM\_#ZR^W'UoROP_0[YK,]#b&j@4KZ'OQK#`[a.KPXK4K6RMY[M+_W.X R^KR^W"UROP_0[YK+0 ? ]#b& Mj[PjR0 ? A ! 0
a.]ffUc.MPj.*RoZ.OQK
0 c6]KM4Z.]#[OQ]ffZ[_#RZ_#ZVUoRT[YKPX_#UBbrX]ff^( `'[a'RM4RM_#UMY][a6K}OP_#MK}br]#X60 ?j4aN.MP`
0 ?RM_UT]#fffRoOP_#UOQ]ffZ.MYKW"N6KZ.OQK}]#b1& [a'_0[RM.@RT[%RZ'c6KPW/KZ.c6KZ[:brX]ff^ ( j fiM4_OQ]ffZ'MYKW"N.KZ.OQK}]#b
\X]#W/]ffMRT[RT]ffZ &`"RT[4^&N.MY[4diK[a6K}OP_#MYK}[a._0[&'")(%+* ( 3 *P	& % :
( A ! 0 ? j6KZ.OQK#`i[a6KPXK}K*RM[M4_
W'XR^KR^W"UROP_0[YK+0 ? ?]#b &')"( *( 3 *P	& % :( ?MPj[Pj +0 ? ? A ! 0 ?a6]ffUc.Mj&a'RM3R^W'URKM[a'_0[+0 ? ? A !20

a.]ffUc.M_#Mq:KUUm`i_#Z.cMRZ.OQK&d/]#[aOPU_#N'MYKM_0XK}W.X R^K}Ro^W'UROP_0[YKM4]#b=[a6K2M_#^K&b1]#X ^}N.Uo_*`"[a6KPV
_0XKWK "N'RTS;_#UKZ[Pj A(N6[[a'RM,R^W'UoRTKM:[a._0?
[ 05 0 ?/a.]ffUc.MP`*q4a.RoOahOQ]ff^W'UKP[YKM[a6KW'X]]#bj
[ 0hd/K_W.XR^eKR^W'UoROP_0[YKe]#b & [a._0[2c6]KM&Z6]#[2OQ]ffZ[_#RoZ _#ZVUR[YKPX_#U=brX]ffZ
^ (G
j 0hRM
 C @ KP
@9RT[%RZ.c.KPWiKZ'c6KZ[:brX]ffQ
^ (j *RZ.OQ
K 02RM,_#ZR^W'UoROP_0[YK3]#b &3`.RT[^}N'MY[4_#UMY]d/K_#ZR^W'URoOP_0[YK]#b
&'")(%+* ( 3 *P	 & % (: j 6N6d'MYWK "N6KZ[UTV#`"[a.KPXK}K 6RMY[M3_W.XRo^K}R^eW'UROP_0[Yc
K 0+?9]#bE&'))(%* ( 3 *Q	 & % ((
Mj[P
j 0 ? A !20a6]ffUc.MPj *RZ.OQK & A ! &'))(%+* ( 3 *Q	 & % (:3_#Z.cG&'")(%+* ( 3 *P	 & % (:A !20 ? d/]#[aLa6]ffUcB`
q:Ka._S#+
K & A !20 ?=_#Mq:KUUmj KZ.OQK#`9[a.KPXKeK 6RMY[M_hW'XR^KR^W"UROP_0[YK 0+? ?]#b &MPj[P)
j 0+? ?FA ! 0+?
a.]ffUc.MPja6KPXKPbr]#XK#` 0 ? ? A ! 0a6]ffUc'M9_#Z.c2MRoZ.OQK+d/]#[a2OPU_#N.MKM_0XK:W'XR^K:R^W'URoOP_0[YKM]#b"[a6K,M_#^K
br]#X^&N.U_*`6[a6KPV_0XK?WK "N.RTS0_#UTKZ[Pj A:N.[,[a.RM,R^W"URTKM+[a._0?
[ 05 0 ?/a.]ffUc.MP`*q4a.RoOaOQ]ff^W'UTKP[YKM4[a6K
W'X]]#bj



Q*\^\

fi

~33

=<



ru uwuw{

0



s~$w

~ww$

"
fi  
 u~00u
&'))(%*3U	.	& % U2".&'))(%*( 3 Q* 	& % ( $ 

 uPv &Pu&.!0wxN/*  w[!#x2

 0z"t U

=> A:VhRZ.c.N.OQ[R]ffZ]ffZ A UA
j:4a6KW.X]#W/]ffMR[RT]ffZ[YXRTS*R_#UUTVa6]ffUc.M,br]#X A U
A ! *j @ KP[N'M_#MMN.^eKRT[
RM([YXN.Kq4a6KZ6KPS#KPX A U A! ij @ KP[ U !$#D< ( %('''% < , ( +ffj A:Vc.K  Z.RT[RT]ffZ `q:Ka._S#K


ruHu

& '))(%*3U.	& % U2/!.&'))(%+* U,	6 &'")(%+* U,	.	& % #D< ( %('''% < +; % #D< , ( +;
A:Vh[a6KRoZ.c.N.OQ[RT]ffZhaVWi]#[a.KMRMP`&'))(%+* U,	6	& % U}RM,KW"N.RTS0_#UTKZ[4[Y] &'))(%*3U,. 
 % #D< , ( ;+  `
q4a6KPXK 
 RM,c6K  Z.Kch_#MM C

"!.&'")(%+* ( 3 *P	 & % #D> > )#D< ( %('('('>% < ++ . #)%: >9 > ) #D< ( %('('('>% < ++;
JLK&a._S#K&'")(%+* U,	.	 & % U} .&'))(%3* U,. 
 % #D< , ( +; j A:Vc.K  Z.RT[RT]ffZ `
JLK&_#UMY]ea._S#K

&'")(%+* U	6 
 % < , ( ff! 
    h / H 
    h

(

'

&'))(%*( 3 *Q 
 % #D< , ( % :%< , ( +;
! &'")(%+* ( 3 *P &')"( *( 3 *P 
 % #D< , ( +; % #):%< , ( +;
! &'")(%+* ( 3 *PY :%< , ( / 
    h / WH 
    h ( % #):%< , ( +;
!
 < , ( /LY %: < , ( / 
    h / H 
    h ( 	    h ( WHLY :Y< , ( / 
    h / WH 
    h ( 	    h / '
a'RM2MR^W'UoR  KM}[Y]  < , ( / 
    h ( RH 
    h / H 
    h ( `\q4a.RoOa RM_#UM]LKW"N.RS;_#UTKZ[2[Y]

    h / H 
    h ( `'a.KZ.OQK?WK "N'RTS;_#UKZ[,[Y]&'")(%+* U,	. 
 % #D< , ( +; j ?:]ffZ.MYKW"N6KZ[UTV#`6q+K}a._S#K
&'))(%3* U,.	 & % U&
 &'")(%+* ( 3 *P &')"( * ( 3 *P	 & % #D> > ) #D< (&%('('(' % < ++ . #)Y
: > > ) #D< (&%('('(' % <++; % #D< , ( % :%< , ( +;
 &'")(%+* ( 3 *P	 & % U2 '




=<

ru uwuw{



U 4

G

 uPv &1 
:Puv| ! !#w xN/* 0 w[!#x 
3
% 140z"tLU P ue /=PPuv ![6
fi   
& /&'")(%+* U	6 
 % U&

,. & 1\vr~*uPz&'")(%+* U,	.	&G/ 
 % &U 

=> @ KP[N.MOQ]ffZ'MRc6KPX?[a6KeOP_#MKeq4a6KPXK U ! #D< +ffj A:V c.K  Z.RT[RT]ffZ `iq:Kea._S#K

ruHu

&'))(%*3U.	& /


 % #D< +; ! 	&9/ 
	ph / H 	 &./ 
	ph ( j1"N'RTS;_#UKZ[UTV#` &'))(%+* U,	6	&./ 
 % D# < +;  	&h / /

h / TH 	&h ( / 

 h (  j J a6KZ <  4 , &`4q:Ka._S#KX&  &h /  &h ( 
j fiOPOQ]#Xc.RoZ6fffUTV#`
&'))(%*3U,.	& / 
 % #D<+;  &;/  
 h / H . 
 h (   & / &')"( *P 
 % #D<+; j@fi MYY[ X_#Rfffa[Ybr]#Xq(_0Xc





RZ.c'N.OQ[RT]ffZOQ]ff^W'UTKP[YKM4[a6K?W.X]]#bj

1 0zit
 1 1  P u}vr~wYu ut0y1	M!0yrz'v
 uPv &1 
 P u&v| !.!#w xN/* 0  w 0! x 
3
% :
Puvr ![00w ymQPPTuQ  w !0x 
fi mJ I/P~hvr~*#v U,	.	& . 
U . 
'"
 . .  K  v\~ff!0Tt; 
=<

ru uwuw{



Q*\+Q

fif.w~3}33f$3ws

  


 t!	uz!ffv I!0z'vI0yrz0z4;0wy PQu  w !0x



 	  
 	 & % +:
 % % -YFA ! 

y 0z"t+!0z. 4y 
&OA !.&'")(%+* ( 3 *P	 & / 
 % ( . . (fiff9

   zLvr~*u3{uQziuPw%#;u 

 	  
 	 & % +:
 % % -YFA ! 

y 0z"t+!0z. 4y 
& A !.&'))(%*( 3 *P	& / :@&')"( * ( 3 *P	 & / : 
 % (fiff
.

( .  % (fiff
.

( . 



|8~*uPwYu 
 	  
 yo y1w M/*x&w y -ivy0!0z;}tu;
=ziuty1z#J   0wQvr~Q4)1 IK 
=>

ruHu

0j44a.RMhRM_ OQ]ffZ'MYKW"N.KZ.OQK]#b2a6KP]#XK^

j  brX]ff^ 5 =XgPV^&N.MRZ'MYRI` #ff j a.RM[a.KP]#XK^
M[_0[YKM[a._0[RTb 
 c.]KMZ.]#[OQ]ffZ[_#RoZ URT[YKPX _#UMb1X]ff^  `3[a6KZ 
 	  
 	& % +:
 % % -Y A ! 

a.]ffUc.MRTb_#Z.c ]ffZ.UTV RTb?[a6KPXKRMZ6] OPU_#N.MYK_1 MPj[Pj 1 c6]KMZ6]#[OQ]ffZ[_#RZ _#ZV URT[YKPX _#UbrX]ff^
( . . (fiff _#Z'#
c & A ! : 
"H1 d'N6[ & A4! 1jLa.RM&RM}KW"N.RS;_#UTKZ[&[Y] M[_0[YKh[a._0[P`\RTb 
 c6]KM
Z.]#[&OQ]ffZ[_#RZ URT[YKPX_#UoMbrX]ff^  `8[a6KZ 
 	  
 	 & % +:
 % % -Y A ! 
 a.]ffUc.MRb_#Z.c]ffZ'UTVRTb`9br]#X
KPS#KPXV OPU_#N.MYk
K 1 OQ]ffZ[_#RZ.RoZ6f]ffZ.UV UoRT[YKPX_#UMbrX]ff^ ( , . (  `,q:Ka'_S#K &;/ 
 A ! 1 RTb?_#Z.c
]ffZ'UTVRT1b &CA ! 1j3p[?RMK_#MYV[Y]hMYKPK&[a._0[3[a6K2WK "N.RTS0_#UTKZ.OQK}RoM4W.XKMYKPXS#Kcq+]ffN'Uc_#ZVbr]#X^&N.U_
1 @R[%RZ.c6KPW/KZ.c6KZ[b1X]ff^ (T. . ( ff d/KOQ]ffZ.MRc.KPXKc _#ZVbr]#X^&N.U_hOP_#ZLd/K[N6X Z6KcRZ[Y]_#Z
WK "N.RTS0_#UTKZ1
[ ?54 b1]#X ^}N.Uo_ j84aN.M`a6KP]#XK
^ j OP_#Zd/K,XKPW'a6X _#MYKcRZeb1]#Xf#KP[Y[RoZ6f[YKPX^eWM C8Rb

 c.]KM(Z6]#[,OQ]ffZ[_#RZhS0_0XR_0d'UTKM+brX]ff^  `[a.KZ 
 	  
 	 & % +:
 % % -YFA ! 
 a.]ffUc.M(RTb_#Z.c]ffZ.UV
Rb &G/ 
  (   & RTb_#Z.ch]ffZ.UVRTb & A ! &'))(%+* ( 3 *Q	 & / 
 % ( . . ( ff9 j

j44a.RMhRM_

OQ]ffZ'MYKW"N.KZ.OQK]#b2a6KP]#XK^ j  brX]ff^ 5=XgPV^&N.MRZ'MYRI` #ff j a.RM[a.KP]#XK^
M [_0[YKM[a._0[ 
 	  
 	& % +:
 % % -YA ! 
 a6]ffUc.MRTb_#Z.c ]ffZ.UV RTbV& A ! 
 ]#Xe[a6KPXKK*RoMY[Me_
br]#X^&N.U_ MPj[Pj, c6]KMZ6]#[OQ]ffZ[_#RZ_#ZV URT[YKPX_#UbrX]ff^7( . . ( ff` & A ! 
 H  a6]ffUoc.M?_#Z.c

 	  
 	 & % +:
 % % -YFA ! :&jp[\RoMK_#MYV2[Y]MYKPK[a'_0[\MN.O a_?br]#X^&N.U_fi K6RMY[M=Rb"_#Z.c]ffZ'UTV&Rb
[a.K3OQ]ffDZ FYN.Z'OQ[RT]ffZ ]#b9_#UU"[a6K3br]#X^}N'U_#M. MN'Oa[a._0[  c6]KM(Z6]#[,OQ]ffZ[_#RZh_#ZVURT[YKPX_#U"brX]ff^
( . . (fiff_#Z.c &OA ! 
H  a6]ffUc.M9RM9Mj[Pj 
 	  
 	 & % +:
 % % -YFA ! : =j *RZ'OQK &OA ! 
H  a.]ffUc.M
Rb"_#Z.c2]ffZ.UV}RTb & / : 
 A !  a6]ffUc'MP` RoM8WK "N.RS;_#UTKZ[8[Y]&'))(%* ( 3 *Q	 & / : 
 % ( . . ( ff9 jaN.MP`

 	  
 	 & % +:
 % % -Y A ! 
 a6]ffUc.MRTb=_#Z.c]ffZ.UTVRTb &CA ! 
 a.]ffUc.M]#X 
 	  
 	 & % +:
 % % -YA !
:@&')"( * ( 3 *P	 &;/ : 
 % (/. . ( ff92a.]ffUc.MPj p%Z [a6KOP_#MKq4a6KPXK & A ! 
 a6]ffUoc.MP` &;/ : 
 RM
RoZ.OQ]ffZ.MRM[YKZ[P`MY]RT[RM?_#UoMY][a6KOP_#MKe]#b &')"( * ( 3 *P	 & / : 
 % ( . . (fiff jaN.MP`RTb & A ! 

a.]ffUc.MP` 
 	  
 	 & % +:
 % % -Y A ! :<&'))(%* ( 3 *Q	 &/ : 
 % ( . . ( ff /a6]ffUc'M _#M q:KUUm*j fiOPOQ]#Xc.RoZ6fffUTV#`

 	  
 	 & % +:
 % % -YA ! 
 a6]ffUoc.M,RTb8_#Z.ch]ffZ.UTVRTb 
 	  
 	 & % +:
 % % -YFA ! :<&'")(%+* ( 3 *P	 & /
: 
 % ( . . ( ff9a6]ffUc.MPj *RZ'OQK&')"( * ( 3 *P	 & /-: 
 % ( . . (fiff9c6]KM4Z6]#[OQ]ffZ[_#RZ_#ZVUoRT[YKPX_#U
brX]ff^  `6[a6KWi]ffRoZ[ FN.MY[_0d/]S#K}KZ._0d'UTKM,OQ]ffZ.OPUoN.c.RZ6f2[a6K?W'X]]#bj

Q*\

fi

~33



s~$w

~ww$


 !#w N
x /* .
 [w !#x 

 0z"tTuv&( Pu&eI/=PuPv ![/(*   zvr~6u
 uPv & P uY
ru=<uwuw{  
{uPziuQwY0  0Pu$1=vr~*uPwYu?yoz!.-"w !I-G!0 yvy5!0zi0 M!0wxN/T 
u M/*yr;0uQz'vv	!&'))(%*( 3 *P	& % ((1v vr~*uy E	u
![ 
 y1 -G0!  4;z !0x2y0r 4 P,!D/*z"tuty1z A &A OA (A 1 /*z.u   fi  9     


=> a6K FN.MY[R  OP_0[R]ffZhRM,[q:]#br]ffUcC
[
&'))(%*( 3 *Q	& % ((RM[a6KUT]#fffROP_#UoUTV MY[YX]ffZ6f#KM[OQ]ffZ.MYKW"N6KZ.OQKL]#bN& [a'_0[RoML@9RT[%RZ.c.KPWiKZ'c6KZ[
brX]ff^ (j ?:]ffZ.MYKW" N6KZ[UTV#`(b1]#XKPS#KPXV b1]#X ^}N.Uo_1 ) 
3
%i`+q:Ka._S#K & A ! 1 RTb3_#Z.c ]ffZ'UTV Rb
&'))(%*( 3 *Q	& % ((,A ! 19` qa6KPXK( !!( 3 *Q	& +( 3 *P 1B jNA:KOP_#N.MYK&'))(%+* ( 3 *Q	& % (:c6KPW/KZ.c.M]ffZ.UV
]ffZ URT[YKPX_#UoM]#bR( 3 *Q	4& 4fik( 3 *P 1B ` RT[RM_#Z yrz'vuPw	-G!# 0z"v]#b & _#Z.ck1`BRmjK#jT`B_br]#X^&N.U_+$ MPj[Pj &CA !$
_#Z.c $ A ! 1a6]ffUc [aN.M,q:Ka._S#KNO
& A ! &')"( *( 3 *P	& % ( 3 *Q	
&  5( 3 *P 1BYDA ! 19j
;ff9[a6K(K6 RMY[YKZ.OQK,]#bi_W'X]#W/]ffMRT[RT]ffZ._#Ubr]#X^}N'U_*`#RZ[YKPXWi]ffUo_#Z[]#b& _#Z.c1`_#Z.c&]#biMRTgPK+W/]ffUTV*Z6]ff^eRo_#UUTV
d/]ffN.Z.c6Kc RZ9A 
& A A 1@A q+]ffN'Uc Ro^W'UTV[a._0[  ;fi  
   5:
A ]#W.W'_#Z'_'fi * RTW"MYKPX`  #0 `
q4a.RoOahRMOQ]ffZ'MRc6KPXKcS#KPXVN.Z'URT#KUTVRoZhOQ]ff^W'UTK6 RT[V[a6KP]#XV#j



ruHu



ru uwuw{


ruHu

=<

 <

  #(0  3'  	 ff  )ff
 

0z"t

=>
S _0XIKW"N.RS;_#UTKZ'OQK#j
 =

fi

 2 0  3 '  	 ff  )ff
  0wu fi  - O[I!0x.-"Tuvu 

K^}diKPX Ma.RTW C8p%Z]#Xc6KPX,[Y]eOa.KO[a6K^K^}d/KPXMa.RTW[Y][a6KOQ]ff^W"UTK^KZ[_0XVW.X]#d'UTK^h`
fffN.KMMRZ6f_OPU_#N.MYK1d'N'RUT[N.WbrX]ff^ U` _#Z'cO a6KO*RZ6f[a._0[	&A ! 1_#Z.c 
 A4! 1B4]#X
	 & A4! 1 _#Z.c 
 A ! 1B&RM2MN.OPRTKZ[Pja6KhOa6KO M[YKPW OP_#Z diKK_#MRUV_#OPOQ]ff^W'URoMa6Kc
RoZW/]ffUTV*Z6]ff^eRo_#U[R^eK&qa6KZ _#Z  ]#X_#OPUTKeRoM_S;_#RoU_0d'UTK#j4KZ.OQK#`[a6KeOQ]ff^W'UTK^eKZ[_0XV
W'X]#d'UTK^ d/KUT]ffZ6fffM([Y+
] & - j
_0Xc.Z.KMMM C @ KP
[ d d/K[a6Ke^e_0W.W"RZ6f[a._0[_#MMY]*OPR_0[YKM[a6K[YXRTW'U
K +	& % * f   % \ -3[Y][a6K
"N._#Z[R  Kc d/]]ffUTK_#Z br]#X^}N'U_ 8\ ff ^ & rqa6KPXK #i\ % ^ +RM_ W'_0X[RT[RT]ffZ ]#N
b U	.	 &Y j
?,UTK_0XUTVKZ6]ffN6fffaBr
` d RM(W/]ffUTV[R^K#j ]#XKP]S#KPX`'q:Ka._S#K C

8\ ff ^

& RoM,S;_#UoRc

RTb8_#Z.c]ffZ.UVRTb A ! ff ^ ' &
RTb8_#Z.c]ffZ.UVRTb A ! &')"( *3U	.	& % ^e
RTb8_#Z.c]ffZ.UVRTb &6
 ` * f 

6RZ.OQK[a6KS;_#UoRc.RT[VW.X]#d'UK^ br]#X 	  
fi br]#X^}N'U_#M2RMfffi  - OQ]ff^eW'UTKP[YK#`\[a'RM2W.X]	S#KM[a6K

	 	  ff) 
  j

fi  - a._0Xc'Z6KMM(]#b fi  2 0  / '
 @9RT[%IKW"N.RTS0_#UTKZ.OQK#j

K^}diKPX Ma.RTW C KPK[a6Ke^K^}diKPX
JYMj[Pj*( 3 *Q 1 5"(RL*j

Ma.RTW W.X]]#b(_0di]	S#K#`9XKPW'U_#OPRZ6f J%d'N'RUT[?N6WLbrX]ff^ UL0dV

_0Xc.Z.KMMM C@KP[Yd d/K\[a6K,^e_0W.W"RZ6f[a._0[_#MM]OPR_0[YKM +	& % 
 % U - [Y] +	& % 
 % ('$ - jff?(UTK_0XUV
KZ.]ffN6fffaB`-d  +	& % 
 % U -YOP_#Z d/KOQ]ff^W'N.[YKc RZ[R^KW/]ffUTV*Z6]ff^eRo_#U=RZ;A +	& % 
 % U -A
jJLK
a'_S#KMa.]q4Zh[a'_0[.& _#Z.c 
 _0XKVS=_0XIKW"N.RS;_#UTKZ[,fffRTS#KZ U RTb_#Z.c]ffZ.UTVRTb & _#Z'c 
 _0XK
Q*\i

fif.w~3}33f$3ws

@9RT[%IKW"N.RTS0_#UTKZ[&fffRTS#KZ ('$(j KZ.OQK#`'d RM2_W/]ffUTV*Z6]ff^eRo_#U\^e_#ZVI]ffZ.KhXKc.N.OQ[RT]ffZ brX]ff^
fi  2 0  3'  	 ff  ff) 
  [Y]   #(0  3'  	 ff  )ff
  j *RoZ.OQK fi  2 0  /'  	 ff  )	
  RoM fi - a'_0XcB`
[a'RMRM,_#UM][a6KOP_#MYKbr]#X   #(0  /'  	 ff  )	
  j



v & P u !0wxN/*   w !0x
3
% 0z"tLU I/=PQPuPv.![3
fi  & y1}yrz /ffO
 u1
ru=<uwuw{ 
uQz  u PQu1 w !0x U y #z"tT!0z. 4y .& yo .0w OYtuR-6uPz"tuPz'v !0zBU


 ruHu=> =X]#W/]ffMR[RT]ffZ <b1X]ff^
5 A:]ffN6[RURTKPX` # <M[_0[YKM[a._0[N&RMRZ'N6KZ.OQK_0d'UK}brX]ff^ U TR b(_#Z.c
]ffZ.UTVRTb9[a6KPXKK6RMY[M_2W'XR^K?R^W"UROP_#Z[:]#b & [a._0[OQ]ffZ[_#RZ'M4_S0_0XR_0d'UKbrX]ff^ Ue`.q4a.KPXK U oR M,[a6K
MYKP[]#b8OQ]ffZ[YX]ffUU_0d'UK?S;_0XRo_0d'UTKMPj \X]#W/]ffMRT[RT]ffZhOQ]ff^W"UTKP[YKM,[a6KW'X]]#bj


 uPv& Pu& M!0wx /*T. w !0x 
3
%0z"tNU  /=PPuv ![T
fi  &yo3wuQuQ;#z'v
	v ! U y 0z"tT!0z. 4y  & yo .#wIYO tuR-.uQz"tuPz'v !0zBU

=<



ru uwuw{







=> a6K?W.X]]#b8RM([YXRTS*R_#U/brX]ff^ =X]#Wi]ffMRT[RT]ffZhj



ruHu

 uPv & P u2.!0wxN/* w !0x2
3
% #z"t U I/=PQPuPv ![+
fffi Y& yo?Qvw y0v 4
wuQuQ;#z'v9v	!VU y 40z"tN!0z. 4&y  & yo .0w OYtuR-6uPz"tuPz'v !0z U 0z"t .0w Oyrz"tu -6uQzituQz'vffw[!#x U,.	&  U
=<



ru uwuw{







 X]#W/]ffMR[RT]ffZ	& RM S=_0XYc6KPW/KZ.c6KZ[
=> +_#MYVbrX]ff^ [a.Kc6K  Z'RT[RT]ffZ]#b MY[YX ROQ[\XKUKPS;_#Z.OQK#`W'UN'M =
brX]ff^ KPS#KPXVS0_0XR_0d'UTK}]OPOPN.XXRZ6fRZ_W.X R^K}Ro^W'UROP_0[YK]#b & #_ Z.cBS=_0XRZ.c6KPW/KZ.c6KZ[4brX]ff^ _#UU9[a6K


ruHu



XK^e_#RoZ.RZ6f&S;_0X R_0d'UTKM  j

ru=
< uwuw{ 
J K! # 2  
 # 2   	 3 )ff

,!#xY-iTuvu 
J WK ! # 2  
 # 2   	 /)ff

,#! xYi- Tuvu 




=>

ruHu



r

;

 1 -fi  fi 1/2 '
 1 fi  fi 132 ' 

# 1

 !'

# 1

G! '



	  
 #GD$# #  2 J   #uPxu4uQw 1 Keyo fi


	  
(# $# #  2 J   #uPxeuM4uQw 1  [Ky1 "

- O
-

O

n ; q " y?z3qv qwqr k   '
K^}diKPX Ma.RTW j @KP[2N'M&OQ]ffZ'MRc6KPX2[a6KhOQ]ff^W'UK^KZ[_0XVW.X]#d"UTK^hj 
3N6KMM_ OPU_#N'MYKb19`
O a6KO[a._0[,RT[(c6]KM:Z.]#[:OQ]ffZ[_#RZh_#ZVS0_0XR_0d'UTKb1X]ff^ U r[a.RM(OP_#ZdiK3_#Oa'RTKPS#KchRZ[R^K
W/]ffUTV*Z6]ff^eRo_#U=RZ A 1@A A U
A
`\a6KZ'OQKRZ[R^KW/]ffUTVZ.]ff^eR_#U=RZ A &A A U
AMRZ.OQKZ6]W'XR^K
Ro^W'UROP_0[YK4]#b & OP_#ZRZ.OPUN'c6K_S0_0XR_0d'UK[a._0[(c6]KM+Z6]#[:]OPOPN6X:RL
Z &4 j=a6KZO a6KO[a._0[
R[(RoM:_#ZhRo^W'UROP_0[YK]#b &r]ffZ6K?OP_#UU/[Y]_#Z  ]#X_#OPUTK	:_#Z.cOa.KO[a._0[,KPS#KPXVMN6dBOPU_#N.MYK
]#*b 1 ]#d'[_#RZ6KcdVXK^]	SRoZ6fb1X]ff^ R[]ffZ.K]#b:RT[M3URT[YKPX_#UoMRoMZ.]#[_#ZR^W'UoROP_0[YK}]#b &
 OP_#UoUM[Y]_#Z  ]#X_#OPUTK	 j *RZ'OQK&]ffZ'UTV
OP_#UUoM[Y]MN.OaL_#Z ]#X_#OPUKe_0XKXWK "N.RTXKc
[Y]2Oa.KO[a'_0R[ 1RM+_W'XR^KR^eW'UROP_0[YK]#b &`[a.KOQ]ff^W'UK^KZ[_0XVeW.X]#d'UK^]#b! # 2  
(#
] &  - j KZ.OQK#` ! # 2  
 # 2   	 / )ff
  d/KUT]ffZ6fffM([Y] fi - j
2   	 / )ff
  d/KUT]ffZ6fffM([YL
_0Xc.Z.KMMPj @ KP[ #i\ % ^ +}d/K_W'_0X[RT[R]ffZh]#b U	.	 &?rbr]#X_#ZVb1]#X ^}N.Uo%
_ & j 8\ ff ^ & RM
S0_#URc RTb:_#Z.c ]ffZ.UTVRTb\KPS#KPXVW.X R^K2R^W'URoOP_0[YK&]#b &s[a._0[OQ]ffZ[_#RZ.M_S0_0XR_0d"UTK2b1X]ff7
^ \
[rqq z3{

Q*\76

fi

~33



s~$w

~ww$



_#UoMY]OQ]ffZ[_#RZ.M:_?S;_0X R_0d'UTK,brX]ff^!^Rbi_#Z.c]ffZ.UTV2RTbiKPS#KPXVW.XR^eKR^W'URoOP_0[YK,]#b & OQ]ffZ[_#RoZ.M
_2S0_0XR_0d"UTKbrX]ff^ ^ MRZ.OQK U.	 &/ ! \ . ^e\Rb_#Z.c]ffZ.UTVRTb & RM(MY[YX ROQ[UTVXKUTKPS;_#Z[([Y]
^j

r

;

n ; q " y?z3qv qwqr k   '
K^}diKPX Ma.RTW C [YX _#RTfffa[Ybr]#Xq(_0XchbrX]ff^ \X]#W/]ffMRT[RT]ffZ.M 2_#Z'c P*j
_0Xc.Z.KMMM CA:VK6a.RTd"RT[RZ6f_W/]ffUTVZ.]ff^eR_#U#XKc'N.OQ[RT]ffZ}b1X]ff^C!$#(04'*)M!+$#&[Y] ! # 2  
 # 2   0
	 / )	
  1 fi  fi 1/2  '  # 1 .! ' 
	  
 # $ # #  2 j?9]_#ZVW"_#RTX + % 7 -]#b\W.X]#W/]0
MRT[RT]ffZ._#UBbr]#X^&N.U_#MP`iUTKP[D  0 ->7((d/K_b1]#X ^}N.Uo_]#d.[_#RZ6KcbrX]ff^ 7 dVhXKZ._#^eRZ.fRT[M
S0_0XR_0d"UTKMPj dSR]ffN.MUTV#`
Rr6  0 ->7(,RM,M_0[RM  _0d'UTK?RTb8_#Z.c]ffZ.UTVRTbM7 RMPj
4]q`"UTKP[ M d/K?_eZ6KPq S0_0XR_0d"UTK#`6UTKP[
&"! / M / :  0 ->7,
_#Z'X
c U8! U,	. i . # M +ffT
j A+V \X]#W/]ffMRT[RT]ff
Z ` &RV
M S=_0Xc6KPW/KZ.c6KZ[]ffX
Z U RTb(_#Z.c
]ffZ'UTVRb[a6KPXKRoM,_W.XR^eK3Ro^W'UROP_#Z[:]#b & ^eKZ[RT]ffZ'RZ6f_S0_0XR_0d'UTKbrX]ff^ Ue`'RmjK#jT`.RTb8_#Z.c
]ffZ'UTVRb / :  0 ->7,(RM,M_0[RoM  _0d'UTK#`6[aN.MP`.N'MRZ6fR1I C
RoR1 & RM S=_0XYc.KPWiKZ'c6KZ[4]ffZ U Rb_#Z.c]ffZ'UTVRbd/]#[a _#Z.c:7 _0XKM_0[RoM  _0d'UTK#j
4a6KZB`}_0fff_#RoZ _0br[YKPX =X]#W/]ffMR[RT]ffZ ` & RM S=_0XYRZ'c6KPW/KZ.c6KZ[brX]ff^ U	6	 &4  U !
U,	.>3 0 ->7(YRTb_#Z'c ]ffZ'UTV RTbZ.] W'XR^KR^W'URoOP_#Z[]#N
b & ^KZ[RT]ffZ.M_ S;_0X R_0d'UTK
brX]ff^ U	.>  0 ->7(Y `6RmjK#jT`RTbB_#Z.ce]ffZ'UTVRTb 3 0 ->7(\RM=N.Z'M_0[RM  _0d"UTK#`ff[aN.MP`N'MRZ6f
RrIC
RoRR1 & RM S=_0XYRoZ.c6KPW/KZ.c6KZ[,b1X]ff:
^ U	.	 &  U RTb9_#Z'ch]ffZ.UTVRTbM7 RM,M_0[RoM  _0d'UTK#j
4aN.M`'brX]ff^ \X]#W/]ffMRT[RT]ffZ `8RRr,_#Z.c RRR1 `.q:K&f#KP[[a._0[ & RoM4MY[YXRoOQ[UTVhXKUTKPS0_#Z[[YT
] U
Rb_#Z.c]ffZ'UTVRb RM,M_0[RoM  _0d'UTK?_#Z.c 3 0 ->7((RoMZ6]#[Pj
[rqq z3{



Q*\,+

fif.w~3}33f$3ws



&T D F\H  GE

ff



KPXKRM,_M^_#UU/fffUT]ffMM_0XV]#b8N.MYKPbN.Ui[YKPX^eM,qRT[a[a6K?W'U_#OQK?qa6KPXK3[a6KRTX4c.K  Z.RT[RT]ffZOP_#Zd/K3br]ffN.Z.cBj


%$
W.X]#W/]ffMR[RT]ffZ._#UiU_#Z6fffN._0f#Kf#KZ.KPX_0[YKcdB
V U
*KOQ[RT]ffZ j 
('$
MYKP[,]#bURT[YKPX _#UM(d'N.RU[:N6WhbrX]ff^ U
*KOQ[RT]ffZ j 
(',$
MYKP[,]#b8W/]ffMRT[RTS#K?UR[YKPX_#UM(d'N.RoUT[:N.Wb1X]ff:
^ U
*KOQ[RT]ffZ j 
( $.
MYKP[,]#bZ6KPfff_0[RTS#K&URT[YKPX_#UoM+d"N.RUT[(N6WbrX]ff^ U
*KOQ[RT]ffZ j 
U.	 &
MYKP[,]#b8W.X]#W/]ffMR[RT]ffZ._#UiS;_0X R_0d'UTKM,_0W.W/K_0XRoZ6fRB
Z &
*KOQ[RT]ffZ j 
4O4
Z6KPfff_0[RT]ffZZ6]#X^e_#UBbr]#X^
*KOQ[RT]ffZ j 
( 3 *Q	 &4
MYKP[,]#bURT[YKPX _#UM(]OPOPN.XXRZ6fRZ[a.
K 4?4 ]#b &
*KOQ[RT]ffZ j 
Y= $
_ UIq+]#X UcRZ.MY[_#Z[R_0[RT]ffZ'M,]#b_#UoU/S;_0XRo_0d'UTKM(]#b U} *KOQ[RT]ffZ j 
[/$
MYKP[,]#b_#UU U?Iq+]#X Uc.M
*KOQ[RT]ffZ j 
=
q:]#XUcrbN.UU/RZ.MY[_#Z'OPR_0[RT]ffZ"
*KOQ[RT]ffZ j 
d f' e"	 &
MYKP[,]#b^]*c6KUM(]#b &
*KOQ[RT]ffZ j 
 ').} fi=
br]#X^}N'U_2MN.Oa[a._0?
[ d ' ei	 &ff ! fi
*KOQ[RT]ffZ j 
&'gih /
*KOQ[RT]ffZ j 
&'gih (
*KOQ[RT]ffZ j 
&5j h (
*KOQ[RT]ffZ j 
&')l V>= % I
*KOQ[RT]ffZ j 
	 
	 &
MYKP[,]#b8W.XRo^K?R^W'URoOP_0[YKM(]#b &
*KOQ[RT]ffZ j 

fi	i	 &
MYKP[,]#b8W.XRo^K?R^W'URoOP_#Z[M:]#b &
*KOQ[RT]ffZ j 
`

*KOQ[RT]ffZ j 
" " *KOQ[RT]ffZ j 
- ` & - ` fi MYV*Z[_#OQ[RoOP_#U @R[%c6KPW/KZ.c6KZ.OQK
K  Z'RT[RT]ffZ 
MYV*Z[_#OQ[RoOP_#U S=_0XYc6KPW/KZ.c.KZ.OQK
K  Z'RT[RT]ffZ 
 &
MYK^e_#Z[ROP_#U1 @RT[%c6KPW/KZ.c.KZ.OQK
K  Z'RT[RT]ffZ 
r ( 3 *P	 &
URT[YKPX _#UM(MN.Oa[a._0+[   &
K  Z'RT[RT]ffZ 
<  ,. &
MYK^e_#Z[ROP_#U1 S=_0XYc6KPW/KZ.c6KZ.OQK
K  Z'RT[RT]ff5
Z <
r ( 3 *P	 &
S0_0XR_0d'UTKM,MN.Oah[a'_0+[   &
K  Z'RT[RT]ff5
Z <
x -"
y 
:ut
K  Z'RT[RT]ffZ 
 yr5v O yr.
.#Iw O yr.x -"
y 
:ut
K  Z'RT[RT]ffZ 
&'))(%+* ( 3 *Q	 & % (: URT[YKPX _#U"br]#Xf#KP[Y[RZ.f
K  Z'RT[RT]ffB
Z 

&'))(%+* U,	6	 & % (: S0_0XR_0d'UTK3br]#Xf#KP[Y[RZ6f
K Z'RT[RT]ffZ 
&   

@R[%IWK "N.RS;_#UTKZ'OQK
K  Z'RT[RT]ffZ
& $ 

S=_0XYIWK "N.RTS0_#UTKZ.OQK
K  Z'RT[RT]ffZ
RZ "N6KZ.OQK_0d'RUoRT[V
K  Z'RT[RT]ffZ P
XKUTKPS0_#Z.OQK?[Y]_MN6Gd FYKOQ[^e_0[Y[YKPX
K  Z'RT[RT]ffZ 
MY[YXRoOQ[,XKUTKPS0_#Z.OQK?[Y]_eMN.Gd F%KOQ[^_0[Y[YKPX
K  Z'RT[RT]ffZ &

Q*\^]

fi

~33

]$ D B 3



s~$w

~ww$



fi^eRTX`(jT`Yfi

OQpUTX

#_ RT[aB`Yij?;0## j _0X[RT[RT]ffZ*Id'_#MKc U]#fffROP_#U3XK_#MY]ffZ.RoZ6f6j p%Z ,w[!^ uut0yrz{0 ![
vr~6u
	iuP0uPz'vr~  z'vuPw z"ffvy5!0zi0  !0zWuQwuQzu !#z (wy1z Py -"TuQ![ fi}z!0|,ut{uuR-"wuuQz'vIffvy5!0z0zit
4u;M!0z.yrz{ J fi  IK	`"W.W j  # 5 <ff#*j
A(RTKPXK#` fi}jT` ?(Ro^e_0[Y[Rm` fi}jT` ?(U_0X#K#`4
j jT`4.N FRT[_*
` jT.
` fi BaNB`2j}[##ff j V*^d/]ffURO^]*c6KU
O a6KO*RZ6f3N.MRoZ6f  fi W.X]*OQKc.N6XKM9RZ.MY[YK_#c]#=b A  Mjp%Z ,[w !^ uut0yrz{0Y ![ 2u yT{#z  /6	v !0xe#v5y !#z
 !#zWPuPwYuPz  Tu J    K	j
A:]#W.W'_#Z._*` Ej AjT` fi *RTW"MYKPX` j'[ #0 jBa6KOQ]ff^eW'UTK 6RT[V}]#b  Z'RT[YK(brN.Z'OQ[RT]ffZ.MPj6p%ZS0_#Z @ KPKNq+KZB` 6j
+cBj
 ` 0ziQt P3!W!  !  ~*,u !0wYuv5y  0  !0.
x -/6vuQw 	ymuPz  uP` S]ffUI,j fi}`iO a._0W j  <6ji+UMYKPS*RTKP
X 6OPRTKZ.OQK
+N6d'URoMa6KPXM3: 44]#X[a* 4]ffUoU_#Z.c" ` fi^eMY[YKPXc'_#^hj
A:]ffN6[RURKPX` ?3j\[ # < je]	q:_0Xc _UT]#fffRO2br]#X "N._#UoRT[_0[RTS#Kc6KOPRMR]ffZ [a6KP]#XV#j&pZ ,[w !Wuut0y1z{0L
 ![evr~6u
P!D/*wvr~  z'vuQwz"#v0y !0z"#  !0Wz uQwuQz u !0z vr~*u ,w yrz Py -"u ! fi}z !#|,Tut{u Ru -iwYuQPuQz"vI#v5y !#z 0zit
4u;M !0z.yrz{ J fi   
DK	`"W.W =
j D 5 j
?(a6]#W'X_*
` ijT=` fi 8_0XRT*aB` Ej[ ##ff 1j fiZhRZ.OQ]ffZ.MRMY[YKZ.OQV[Y]ffUTKPX_#Z[4^]*c6KUBbr]#Xd/KURTKPb XKPW'XKMYKZ[_0[RT]ffZ
_#Z'cd/KURKPbiXKPSRMRT]ffZBj p%
Z (w !W u ut0yrz{0 ! 3vr~6

u 	.y 2ffvu uQz'vr~  z'vuPw z"ffv5y !0zi0 !0y1z"v  !0Wz uQwuQz u !#z
 wvy 
1Pym0  z"vuQr
yT{uPz  %u J       K	`'W'W j   5  j
3_0Xq4RO a6K#` fi}j[ # # j fi U]#fffROP_#UBZ6]#[RT]ffZ]#b8OQ]ffZ.c.RT[RT]ffZ'_#UiRoZ.c6KPW/KZ.c6KZ.OQK CW.X]#W/KPX[RTKM(_#Z.ch_0W'W'UROP_;
[R]ffZ.MPj  wQvy 
 Pym0  z'vuP1y {uQz uQ`  /[  5 ff ` <Q 5 j
3_0Xq4RO a6K#` fi}j[ # ff 1
j ]c6KUTId'_#MYKchc.R_0fffZ.]ffMRM,N.MRoZ6f&M[YXN.OQ[N6XKchMVMY[YK^ c.KMOQXRTW.[R]ffZ.MPj G!D/*wz"0
!   wQvy 
1y#  z'vuQry {uQz 
u 4uPu0w Q~` #` & 5 j
3_0Xq4RO a6K#*
` fi}j:[ ##ff j ?:]ff^eW'RURZ.f*Z6]qUTKc6f#KRZ[Y]c6KOQ]ff^eWi]ffM_0d'UTKZ6KPfff_0[R]ffZZ6]#X^_#Ub1]#X ^hjp%Z
,[w !^ uut0yrz{0 ![&vr~*u 	.y 2ffvu uQz'vr
~  z'vuQwz"#v0y !0z"0	 !0yrz'v  !0Wz uQwuQz u !0z  wvy 
1y0  z'vuQry {uQz u
J       K	`'W.W j  <)5 #j
3_0Xq4RO a6K#` fi}jT1
` fi _0,X "N.RMP` j[ ##ff j fi W/KPXMYW/KOQ[RTS#Kh]ffZ Z6]	q4UTKc6f#KOQ]ff^eW'RU_0[RT]ffZ j p%Z ,[w !DO
uut0y1z{0L
 ![vr~*u 	iuQ#uQz'vu uQz'vr~  z'vuPw zi#v5y !#z"0 G!0yrz'v  !0Wz uQwuQz +
u !0z  wQvy 
1y#  z'vuQry {uQz u
J       K	`'W.W j ^D 5 &j
3_S*RMP` jT` fi \N6[Z'_#^h` ji[  0 *
j fi OQ]ff^W'N6[RZ.f?W.X]*OQKc.N6XK,br]#1X "N._#Z[R  OP_0[RT]ffZ[a.KP]#XV#	j G!D/*wz"0
! vr~*u   L` P` 0  5 GWj
KOa[YKPX` EjTD` fi6E4RoMaB`Ppj[ # < j RXKOQ[RT]ffZ._#UXKMY]ffUN6[R]ffZBP[a6K+c'_S*RM%IW'N6[Z'_#^ W.X]*OQKc.N6XK#`PXKPSRoMRT[YKcBj
p%Z ,[w !^ uut0yrz{0V
 ![vr~*?u P !D/*wQvr~  z'vuPw z"ffv5y !0zi0  !0Wz uQwuQz u !0zvr~*u (wy1z y -iTuQ ![ fi}z !0|,Tut{u
4Ru -"wuuQz'vI#v0y !0z 0z"t u0 !0z.yrz{ Jfi   
DK;`'W.W 
j   <)5  <Qj
c6KU S=_#Um` fi}j:[ ##ff 
j fi Z6KPq ^KP[a6]*cbr]#X2OQ]ffZ.MYWK "N6KZ.OQK  Z.c.RZ.f_#Z'c OQ]ff^W"RU_0[RT]ffZRoZXKMY[YX ROQ[YKc
Uo_#Z6fffN._0f#K#j#p%Z (w !W u ut0yrz{# ![:vr~*u 	.y 2ffvu uQz'vr3
~ N#v0y !0z"#  !0^z PuPwYuPz  u !#z  wvy 
1y0  z'vuQry {uQz u
J       K	`.W.Wj # 5  <6` X U_#Z.c6] @ j
]ffa6KPX[V#
` jT` @N60_#MYgPKPq4ROQg#`iJsjT` fi _#c._#UoRZ.MY0_; A(N6fff*_ FQ`ij8[ # ff ja.K 7 fi_#Z.cXKU_0[RTS*RTgRZ6f
O a._#Z6f#Kbr]#Xe_#OQ[RT]ffZ N.W/c'_0[YK#jpZ ,[w !^ uut0yrz{0 ![hvr~*u 	.y 2ffvr~  z'vuQwz"#v0y !0z"#  !0Wz uQwuQz B
u !#z
,w yr
z y -"u ! fi}z !0|,ut{u u -"wYuQPuPz'vI#v0y !0z 0z"
t 4u;M !0z.yrz{ Jfi  M K	`'W'W j  5 #j
]ffa6KPX[V#1
` jT` @N.;_#MYgPKPqROQg#`J jT` fi g_#U_#MP` fi}j4; 0#  j ?(]ff^W'N6[RZ.fMY[YX]ffZ6f#KMY[Z6KOQKMM_0XV _#Z.c
q:K_0#KMY[,MN6OPRTKZ[:OQ]ffZ.c.RT[R]ffZ.M]#b  XM[%I]#Xc6KPX\br]#X^&N.U_#MPj/p%Z ,[w !Wuut0y1z{0 ![3vr~*u 	iuP0uQz"vuuQz"vr~
 z"vuQwz"#v0y !0z"0
 G!#y1z'v  !0Wz uQwuQz  u !0z  wQvy 
1Pym0  z'vuP1y {uQz Lu J       K;`.W.W j  <Q 5 WG0j

Q Q)(

fif.w~3}33f$3ws

._0XfffRKPX`6}jT` @_#Z6f6` 6jT` fi _0X,"N'RMP` j?;0## j \X]#W/]ffMRT[RT]ffZ._#U4UT]#fffRO_#Z'c ]ffZ6KQMY[_0f#KLc6KOPRoMRT]ffZ
^_0RZ.f6j p%Z (w !Wuut#y1z{# ![vr~*u 	iuQ#uQz'vr~  z'vuQwz"#vy0!0z"#  !0zWuQwuQz  uX!0z ,w yrzy -"u ![
fi}z !0|,Tut{u uR-"wuuQz'vIffvy5!0z 0z"tu;!0z'y1z{ Jfi  IK;`'W.W j <<Q 5 <Qj
._0X	R Z. _#Mec6KU ?(KPXX]6` @=jT` fi 4KPXgRf6` fij[#ff j A:KURTKPbOa._#Z.f#K_#Z.c c6KPW/KZ.c.KZ.OQK#j pZ ,w[!^ uutDO
yrz{#T
 ![vr~6u 	'y 2vr~  !0^z PuPwYuPz  '
u !0z ~*3u !0wuPv0y #  	 -6,u vrB
 ![ u;!0z'y1z{  P3!D/6vfi}z !0|,Tut{u
J   fi   I K;`'W.W j  < +5 &G0j

KUTbr]ffZ.cB` jT` fi @RbrMO a.RT[Yg#
` S}j[ #   j E4KPW.XKMYKZ[RZ6f_#OQ[RT]ffZ _#Z.c Oa._#Z6f#KdVUT]#fffROW.X]#f#X _#^eMPj
!D/*wz"0 !   !{ff5y  ,[w !{#wY0x2x&yrz{ff` `  
 5j

3a.Roc.RZ.Rm` ?jT
` fi:
?RN.Z.O a.RTfffUR_*`/:j+; 0#  T
j @ ]*OP_#U=^]*c6KUMYK^e_#Z[ROPMP`]#X}OQ]ffZ[YK [N._#U\XK_#MY]ffZ.RoZ6f !
U]OP_#UR[
V OQ]ff^W'_0[RTd"RURT[V#j  wvy 
1Pym0  z"vuQr
yT{uPz  uP`   P` G 5 #j

]ffUc.d'U_0[Y[P1
` Ej3[  # j  !{#0y Q ![ 9yrxuL0z"t  !0.
x -/6vI#v0y !0z.` S]ffUIj ]#
b  	    3u 5v /wu N !#vuQ j
?(KZ[YKPXb1]#X[a.K [N.c6V]#b @9_#Z6fffN._0f#K&_#Z.cpZ.b1]#X^_0[RT]ffZB
` [_#Z6br]#XcB` ? fij

XKRoZ6KPX
` EjT` fi 
KZ.KMYKPXKP[aBff` j Ej([     jLJ a._0[ MZ6KPq )_MYK^e_#Z[ROc6K  Z.R[RT]ffZ]#b4Z.]S#KUT[V#j
p%Z ,[w !Wuut0y1z{0N
 ! }vr~*u,y {#~vr~  z'vuQwz"#v0y !0z"# !0yrz'v  !0Wz uQwuQz N
u !0z  wQvy 
1y#  z'vuQry {uQz u
J       K	`'W.W j <Q0"5 <Q <6j

XKRoZ6KPX` EjT` fi *N.d.X_#^e_#Z.Ro_#ZB` }j8[ # ff j =X]*OQKPKc.RZ.fffM,]#b[a6K fi fifip,b_#UUBMYV*^W/]ffMRN.^ ]ffZXKUTKQ
S0_#Z.OQK#jTj:KO a.Z.ROP_#U E4KPW/]#X[  <0 ,` fi fi fi4p =XKMMPj
KPfffZ6KPX` ij[  # j W/KOPR  OP_0[R]ffZ_#Z.cR^W'UTK^eKZ[_0[RT]ffZ]#bW.X]#f#X_#^M4br]#XN.W/c'_0[RZ6fRZ'OQ]ff^W'UTKP[YK
RoZ6b1]#X ^e_0[RT]ffZec._0[_0d"_#MYKMPj/pZ (w !W u ut0yrz{0 ![vr~*u 	.y 2ffvr~    	     	    	    
	=4;.x -G!0 y /x !0ffz ,w yrz Py -"u ![ }ffvIQ P;u 	4;vuPx& J  	   [K;`.W.W j  <  5 Wj
KPXgRTf6` fi}ji[ # ff j4a6K 7 fi XKPSRoMRT[YKcBj"p
Z (w !W u ut0yrz{0V
 ![vr~6/u P\y Qvr
~  z'vuQwz"#v0y !0z"#  !#Wz PuPwYuPz  u
!#z vr~6
u (wy1z y -iTuQ ![fi}z !0|,ut{u u -"wuPuPz'vI#v0y !0z 0zi
t u;M !0z.yrz{ J fi  I K	`"W.W j <ff"ff5 0*j
KPXgRTf6` fijT` @9_#Z6f6` 6jT` _0,X "N.RM` jT` fi 9]ffUo_#OPMYKP"`B?j=; 0#  j WBc._0[YKMP`B_#OQ[RT]ffZ.M`/_#Z'cW'Uo_#Z.Z.RZ6f6j
p%Z (w !W u ut0yrz{#%
 ![vr~*u 	/uQ0uPz'vuuPz'vr
~  z"vuQwz"#v0y !0z"0 !0yrz'v  !0^z PuPwYuPz  L
u !0z  wvy 
1y0  z'vuP O
yT{uQz  L
u J      K	`.W.Wj  5 & <6j
KPXgRTf6` fi}jT` fi E4R  ` j"[ # ff j WBc._0[YK]#W/KPX_0[RT]ffZ.WM Cff_XKPS*RTKPqjipZ (w !W u ut0yrz{0 ![vr~*u ~yrwvu uQz'vr~
 /*[w !I-.u0z  !#Wz PuPwYuPz  N
u !#z  wvy 
1y0  z"vuQr
yT{uPz  L
u J      M K	`.W.Wj   5 ^j
KPXgRTf6` fi}jT*` fi E4R  ` j[ ##ff ffj \X]#W/]ffMRT[RT]ffZ'_#U0diKUoRTKPbd'_#MK\N6WBc._0[YK=_#Z.c}^eRZ.R^_#U;O a._#Z6f#K#j  wvy 
1y0
 z"vuQr
yT{uPz  uP`  [ ` PQ +5   j
p%Z6]ffN6K#` j/[ # ff j @9RZ6K_0X\XKMY]ffUN6[RT]ffZRZOQ]ffZ.MYWK "N6KZ.OQK$5  Z'c.RZ6f6j  wQvy 
1Pym0  z'vuQr
yT{uPz  uP` ;  5   `
   5    j
_#N6[Yg#` }jTff
` ?K_0XZ'MPff` jT1` fi *KU^e_#ZB` Aj4[ #   j EK_#MY]ffZ'RZ6f q4RT[a Oa'_0X_#OQ[YKPXRMY[RoOh^]c.KUMPjp%Z
,[w !^ uut0yrz{0V
 ![vr~*u,TuP0uPz'vrff
~ Nffv5y !0zi0  !0^z PuPwYuPz  V
u !#z  wvy 
1y0  z'vuQr
yT{uPz  u J       MK	`
W'W j  <)5  j
_#N6[Yg#`F}jT` ?K_0X Z.MP` jT` fi KU^e_#ZB` Aj[ # ff j ]#XZ _0W.W.XW] 6R^e_0[RT]ffZ.M]#b}K^W'RTX ROP_#U3c._0[_*j
 wvy 
1Pym0  z"vuQr
yT{uPz  uP`  
:[  ` &# 5  <Qj
_#N6[Yg#` }jT7
` (O fiUoUTKMY[YKPXff` &jT1` fi KUo^e_#ZB` Aj4[ # ff j \Z'OQ]c.RoZ6fW"U_#Z.M2RZ W.X]#W/]ffMR[RT]ffZ._#U:UT]#fffRO0j
p%Z (w !W u ut0yrz{# ! &vr~*ff
u P\y Qvr
~  z"vuQwz"#v0y !0z"0  !0^z PuPwYuPz  %
u !0z vr~6u (wyrz Py -"u ![fi}z !0|,Tut{u
4Ru -"wuuQz'vI#v0y !0z 0z"t u0 !0z.yrz{ Jfi   I K;`'W.W j   <)5   <6j

QQ

fi

~33



s~$w

~ww$

c6]ffZB`EjT`fi E]#[a ` &j[#ff j EK_#M]ffZ.RZ6fq4RT[a^]c.KUMPj  wvy 
1Pym0  z'vuQr
yT{uPz uP`  i[ 5 ff `
&+5 G  j
?]ffa.Uo_#MP` 6jT` ]#X_#UI` ijT`fiC_0KZ.Z.RI`=Ej9[##ff j \X]#W/]ffMRT[RT]ffZ'_#UBRZ6br]#X^e_0[RT]ffZhMYV*MY[YK^eMj !D/*w z"# ![
 !{#0y e0zit  !#xY- /*vIffvy5!0z'` 0ff ` G 5 G0j
@_0#K^eKPV#KPX` 
&j4[ # ff j fi UT]#fffROP_#U,_#OPOQ]ffN'Z[]#b4XKUTKPS0_#Z.OQK#j p%Z (w !W u ut0yrz{0 ! vr~*uP!D/*wvu uQz'vr~
 z"vuQwz"#v0y !0z"0
 G!#y1z'v  !0Wz uQwuQz  u !0z  wQvy 
1Pym0  z'vuP1y {uQz Lu J       K;`.W.W j   5 #j
@_0#K^eKPV#KPX` 
2j[ # # j EKUKPS;_#Z.OQKb1X]ff^ _#Z KPW'RoMY[YK^eROW/KPXMYW/KOQ[RTS#K#j  wQvy 
1y#  z'vuQry {uQz uQ`
 i[  5 ff `   +5 &j
@_#Z.f6` .jT` @RdiKPX _0[Y]#XK#` jT` fi _0,X "N'RMP` j8; 0# ff j ?(]ffZ.c.RT[RT]ffZ'_#U RZ.c6KPW/KZ.c.KZ.OQKRZW'X]#W/]ffMRT[RT]ffZ._#U
U]#fffRO0j  wQvy 
 Pym0  z'vuQry {uQz uQ`  
 9[  5 ff ` 0 5 &G0j
@_#Z.f6` 6jT` fi _0,X "N'RMP` j\[ # 0_ j ?:]ff^W'UK *RT[VXKMN'UT[M3b1]#XRoZ.c6KPW/KZ.c6KZ.OQK&_#Z.cLc6K  Z'_0d'RURT[VRZ
W'X]#W/]ffMRT[RT]ffZ._#UUT]#fffRO0j'p
Z (w !Wuut#y1z{# ![4vr~6u 	'y 2vr~  z"vuQwz"#v0y !0z"0  !0Wz uQwuQz 
u !0z (wy1z y -iTuQ
! fi}z !0|,ut{u u -"wYuQPuPz'vI#v0y !0z 0z"
t 4u;M !0z.yrz{ Jfi  M K	`'W'W j   5  j
@_#Z.f6

` 6jT` fi8_0,X "N'RMP` j:[ # ;di jh,q:]Z.]#[RT]ffZ.M?]#bc6KPW/KZ.c.KZ.OQKeRZLW.X]#Wi]ffMRT[RT]ffZ._#U8UT]#fffRDO C9OQ]ffZ*
[YX]ffUU_0d'RUoRT[V_#Z'cc6K  Z'_0d'RURT[V#j2pZ ,[w !^ uut0yrz{0T
 ![vr~*
u P\y vu uQz'vr_
~ N}#v5y !#z"0  !#Wz PuPwYuPz  +
u !#z
 wvy 
1Pym0  z"vuQr
yT{uPz  %u J        K;`.W.W j  5   j
@_#Z.f6` 6jT` fi _0,X "N.RMP` j"; 0# ff j EKMY]ffUSRZ.f3RoZ.OQ]ffZ.MRM[YKZ.OPRTKMdV&S0_0XR_0d'UTK(br]#Xf#KP[Y[RZ6f6j"pZ ,[w !^ uuDt O
yrz{# ![2vr~*u (yT{0~vr~  z'vuQwz"#v0y !0z"#  !0Wz uQwuQz u !#z (wyrz Py -"uN
 !  fi}z !0|,Tut{u u -"wYuQPuPz'vID O
v0y !0z0z"
t 4u;M !0z.yrz{ Jfi WK	`'W'W j    5 0*j
@_#Z.f6` 6jT` _0IX "N.RoMPQ` jTff` fi J RoUUR_#^eM` j  fi}j/; 0#  j 4WBc._0[RZ.f?KPW'RMY[YK^eRoOMY[_0[YKMPj p%Z ,[w !^ uut0yrz{0
! 2vr~6ff
u P!D/*wvu uQz'vr~  /vwY0y#z !0yrz'v  !0Wz uQwuQz  L
u !0z  wvy 
1Pym0  z'vuP1y {uQz u J     K;`BW.W j
# +5   j
@ KPSV#` fijT`iR#KMP` EjT` fi *_0fffRTSi` 2j[ # # j W/KPKc.RZ6feN6WRZ6brKPXKZ.OQKMN.MRZ6fXKUTKPS;_#Z'OQKXK_#MY]ffZ'RZ6f C
fisbr]#X^e_#URoM^ _#Z.c_#UTf#]#XR[a.^eMPj  wQvy 
1Pym0  z'vuP1y {uQ
z uQ`  i[  ff `   5   j
@RoZB`6+j; 0## j ZhMY[YX]ffZ6f#KMY[4Z6KOQKMM_0XV_#Z.chq:K_0#KMY[MN.OPRTKZ[(OQ]ffZ'c.RT[RT]ffZ.Mjp%ff
Z (w !Wuut#y1z{#N
 ![
vr~6

u 	iuP0uPz'vr~  z'vuPw z"ffv5y !0zi0  !0Wz uQwuQz u !#z (wy1z Py -"TuQ
 ![ fi}z !0|,ut{
u Ru -"wuuQz'vIffv5y !0z0zit
4u;M !0z.yrz{ J fi  IK	`"W.W 
j &+5 ^Dj
@RoZB` :jT` fi E4KRT[YKPX` Ej:[ # < j6]#Xf#KP[2R[ Tjp%Z (w !Wuut#y1z{#+
 ![vr~*u    P01 	40Yx -!; y /*x !#z
4uQuQ;#
z  uP`'W.W j W <)5 W#j
_0,X "N'RMP` j?[ # < j ]ffMMRd'UTK^]*c6KUM_0W.W.X]ff_#O a SR_RZ.c6KPW/KZ.c6KZ'OQV#j p%Z ,[w !^ uut0yrz{0 ![vr~6u
,TuP0uPz'vr~  /*[w ! -6u0
z  !0Wz uQwuQz N
u !0z  wvy 
1y0  z'vuQr
yT{uPz  %
u J      
K	`.W.Wj   5  <ff*j
_0,X "N'RMP` j+; 0## j ?:]ffZ.MYWK "N6KZ.OQK  Z.c.RZ6f_#UTf#]#XR[a.^eMPjp%Z }0z"Qt P,!W! '!0z 25u u0 	y PQu 4u;M !0z O
yrz{#z"
t z  uPwvI#y1z'5v 4 0z"{uQxuQz'v 	=4	QvuQx& 1 !0 /*xu    { !0wyrvr~x& !0
w z  uPwvI0yrz 0zit
20u Pu; 	y PQu u0 !0z.yrz{ff`"Oa'_0W j `.W.Wj < 5  <Q,j UoNq:KPX fiOP_#c.K^eRO +N6d'URMa6KPXMPj
_0,X "N'RMP
` jT` fi:]#,X "N.KP[P%` 4}j:; 0## j KOQ]ff^eWi]ffMRZ6fW.X]#Wi]ffMRT[RT]ffZ._#U8*Z6]qUTKc6f#Kd"_#MYKM[a.X]ffN6fffa
[Y]#W"ROPM&rK *[YKZ.c6Kc_0d'MY[YX_#OQ[  j4pZ ,[w !^ uut0yrz{0 ![}vr~6u&xuuPvyrz{ !#
z  #wvym0 ;z !0|,ut{ue0zit
/*z uQwQvI0yrz'5v 4 yrz"tRu -.uQz"tuPz  u 1 I!0z"t0yv5y !#z.y1z{ 1+yrWz uQwuQz u j
>O ?(_0X[aV#` .j9[  ff 
j fiW.W'URoOP_0[RT]ffZ.M(]#b=OPRTXOPN'^eMOQXRTW'[RT]ffZ[Y]br]#X^e_#URgRZ6fOQ]ff^e^]ffZ*MKZ.MYK*Z6]	q4U 
Kc.f#K#j  wQvy 
 Pym0  z'vuQry {uQz uQ`  ff` # 5 &j
a._0X

QQ_

fif.w~3}33f$3ws

OQpUTX

_#RT[aB` ijT` fi fi^eRX`:4j;0# j a6KP]#XK^ W.X]	SRoZ6f q4RT[a MY[YXN.OQ[N.XKc [a6KP]#XRTKMPj pZ ,w[!DO
uut0y1z{0L![vr~*u 	iuQ#uQz'vu uQz'vr~  z'vuPw zi#vy5!#z"0G!0yrz'v  !0zWuQwuQzu+!0z  wQvy 
1y#  z'vuQry {uQzu
J       K	`'W.W j  <)5   0j
8_0W'_#c.R^RT[YXRT]ffNB` ?j}j9[ # < j  !0.
x -/6vI#v5y !#z"0  !0.
x -"3u 2#yv540j7fic.c.RM]ffZ*IJLKMUTKPV#j
8_0XRT*aB1
` Ej[ # ff j  uQyI5u   N
1 P uQym0u wuQy1 0y !0z210z"t  -"yrvmvyrz{ T0z{ /6{u`(W.W 7j  5 j@]#fffRO0`
@9_#Z6fffN._0f#K&_#Z.c ?:]ff^W'N.[_0[RT]ffZBj ? G@1
p +N6d'UROP_0[R]ffZ.MPj
8_0Xi`*j .jT` fi
KUc6KPX` fi}j S}j [ # ff j 8_0X[RT[R]ffZ.RZ6f&^KP[a6]*c.M(b1]#X,M_0[RoM  _0d'RUoRT[V[YKMY[RoZ6f2]ffZhU_0Xf#K
br]#X^&N.U_#MPjLp%Z (w !W u ut0yrz{0 ! vr~*u ~yrwvu uQz'vr~  z'vuQwz"#v0y !0z"#  !0Wz uQwuQz B
u !#z  /6	v !0xe#vut
2uDt /v0y !0z J      I K	`.W'W j  <  G5  j
=XgPV^&N.MRZ'MYRI`/9
j ?j+[  #ff j fiZ_#UTf#]#XR[a.^ [Y]OQ]ff^eW'N6[YKOPRTX OPN.^eMOQXRW.[RT]ffZBj  wQvy 
 Pym0  z'vuP1y O
{uQz uQ`  #` < G5   j
EKR[YKPX` Ej[  # j fi [a6KP]#XV]#bc.Ro_0fffZ6]ffMRM}b1X]ff^  X MY[}W'XRZ.OPRTW"UTKMPj  wQvy 
 Pym0  z"vuQr
yT{uPz  uP`  ff`
+5 j
E4RoZ[_#Z6KZ ` 6j*; 0#  j 8_0X[R_#UR^W'URoOPRT[BN.Z6br]ffUc.RZ6f,RZ[a6K _SRoM% \N6[Z'_#^ W.X]OQKc.N.XK=br]#X "N._#Z[R  Kc
d/]]ffUTK_#Zebr]#X^&N.U_0K#jipZ (w !W u ut0yrz{0 ![vr~*
u  P   B!0w G~ !I-L#v     #`W.W j  <)5  j
EVff_#Z /
` /j &j,[ #G  @
j KPbr_#N.U[M&_#Z'c XKPSRMRT]ffZ RoZ MY[YX N.OQ[N6XKc [a6KP]#XRTKMjLpZ (w !W u ut0yrz{# ! vr~6u
	'y 2vr~     	40.x -G!;y /*x !0z  !{ff5y &yrz  !0.x -/6vuQw 	PyIuQz %u J    	   K	`'W'W j   5    j
EVff_#Z 1
` 7j &j[ # ff j (wYtuPwYuB
t -"wYuQPuPz'vI#v0y !0z6 ![vr~*,u !0w yIuj \aBj &j=[a.KMRMP`=p%^W/KPXR_#U ?(]ffUUTKPf#K#`
@]ffZ.c6]ffZBj
*_#Z.c.KPq:_#UUI`*4j[ # ff +
j Puff5v /wu20z"
t P\ /'uQz"vr j br]#X Z'RTS#KPXMRT[L
V \XKMMPj
*RTKPf#KUI` j=[  # j u -"
w u PuPz'vI#v0y !0z uPv /6vyr
y1Q#v0y !0z tu ,!#z.z"0y1P0z  uQuQz  0 M/* -"[w ! -G!; yv0y !0z.ziuP
j
4a K MYKc  \ [_0[P` Z.RTS#KPXMR[ K c0 fiR 	5 _0XMYKRUU
K jRZb1XKZ.Oa" j
*R^e]ffZB` @=jTY
` fi c6KU S=_#Um` fij&; 0#  j =OPRTKZ[OQ]ffZ.MYWK "N6KZ.OQK  Z'c.RZ6f6j pZ ,[w !^ uut0yrz{0 ![vr~6u
	/uQ0uPz'vuuPz'vr~  z'vuQwz"#v0y !0z"0 G!#y1z'v  !0Wz uQwuQz Nu !0z  wvy 
1y0  z'vuP1yT{uQz  %u J       K;`6W.W j
 # 5  j
*N6d'X_#^e_#Z.R_#Z ` }jT` 
XKRoZ6KPX` EjT` fi 9K_0X Um

` .j:[ # # 
j fi4X[R  OPR_#U=p%Z[YKUURTf#KZ.OQK #]ffN6XZ'_#0U C W/KOPR_#U
p%MMN.K?]ffZ EKUTKPS0_#Z.OQK#`/ h[  ff ` # j
_#ZB` ijT` fiK_0XUm` 6j[ # < 
j W/KOPR  OP_0[RT]ffZ_#Z'cKPS0_#UN._0[RT]ffZ]#bW.XKPbrKPXKZ.OQKMbr]#XW'U_#Z.Z.RoZ6f&N'Z.c6KPX
N'Z.OQKPX[_#RZ[V#j(p%Z (w !W u ut0yrz{# ![}vr~*
u P !/wQvr~  z'vuPw zi#v5y !#z"0  !0Wz uQwuQz u !0zvr~*u (wy1z y -iTuQ
! fi}z !0|,ut{u u -"wYuQPuPz'vI#v0y !0z 0z"
t 4u;M !0z.yrz{ Jfi  
K	j
J RUUR_#^MPW` j;:jTD` A:RKPXK#` fi}jT` ?(U_0X#K#`;j jT` fi 
3N.W.[_*` fi}j6; 0## j ?:]ff^d"RZ.RZ6f KOPRMRT]ff
Z 3R_0f#X _#^eM
_#Z'c  fi \X]*OQKc.N6XKM}b1]#X\OPRTKZ[ V*^}di]ffUoRO ]c6KU ?(a.KO*RZ6f6jpZ (w !Wuut#y1z{#T
 ! vr~6u
9|+uP Qvr~  z'vuPw z"ffv5y !0zi0  !#Wz PuPwYuPz  N
u !#z  !#Yx - /*vuPw  ymtu	
t "uQwy 
1 #v0y !06
z J    K	j
J RZ.MUTKP[Y[P` j\[ #0 j -*t#vy1z{  !{#0y # &#vIQ P0Puj ?(_#^d'XRc6f#K2X _#OQ[MRZ a6KP]#XKP[RoOP_#U ?:]ff^2
W"N6[YKPX *OPRTKZ'OQK#j ?(_#^d'XRc6f#K Z.RTS#KPXMRT[L
V =XKMMj

Q Q*\

fiJournal of Artificial Intelligence Research 18 (2003) 351-389

Submitted 10/02; published 5/03

A New General Method to Generate Random Modal Formulae for
Testing Decision Procedures
Peter F. Patel-Schneider

PFPS @ RESEARCH . BELL - LABS . COM

Bell Labs Research
600 Mountain Ave. Murray Hill, NJ 07974, USA

Roberto Sebastiani

RSEBA @ DIT. UNITN . IT

Dip. di Informatica e Telecomunicazioni
Universita di Trento
via Sommarive 14, I-38050, Trento, Italy

Abstract
The recent emergence of heavily-optimized modal decision procedures has highlighted the key
role of empirical testing in this domain. Unfortunately, the introduction of extensive empirical tests
for modal logics is recent, and so far none of the proposed test generators is very satisfactory. To
cope with this fact, we present a new random generation method that provides benefits over previous methods for generating empirical tests. It fixes and much generalizes one of the best-known
methods, the random CNF  test, allowing for generating a much wider variety of problems, covering in principle the whole input space. Our new method produces much more suitable test sets for
the current generation of modal decision procedures. We analyze the features of the new method
by means of an extensive collection of empirical tests.

1. Motivation and Goals
Heavily-optimized systems for determining satisfiability of formulae in propositional modal logics are now available. These systems, including DLP (Patel-Schneider, 1998), FACT (Horrocks,
1998), *SAT (Giunchiglia, Giunchiglia, & Tacchella, 2002), MSPASS (Hustadt, Schmidt, & Weidenbach, 1999), and RACER (Haarslev & Moller, 2001), have more optimizations and are much
faster than the previous generation of modal decision procedures, such as LEAN K (Beckert & Gore,

1997), L OGICS W ORKBENCH (Heuerding, Jager, Schwendimann, & Seyfreid, 1995), KE (Pitt &
Cunningham, 1996) and K SAT (Giunchiglia & Sebastiani, 2000). 1
As with most theorem proving problems, neither computational complexity nor asymptotic algorithmic complexity is very useful in determining the effectiveness of optimizations, so that their
effectiveness has to be determined by empirical testing (Horrocks, Patel-Schneider, & Sebastiani,
2000). Empirical testing directly gives resource consumption in terms of compute time and memory
use; it factors in all the pieces of the system, not just the basic algorithm itself. Empirical testing
can be used not only to compare different systems, but also to tune a system with parameters that
can be used to modify its performance; moreover, it can be used to show what sort of inputs the
system handles well, and what sort of inputs the system handles poorly.
Unfortunately, the introduction of extensive empirical tests for modal logics is recent, and so
far none of the proposed test methodologies are very satisfactory. Some methods contain many



1. For a more complete list see Renate Schmidts Web page listing theorem provers for modal logics at
http://www.cs.man.ac.uk/schmidt/tools/.
c 2003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiPATEL -S CHNEIDER & S EBASTIANI

formulae that are too easy for current heavily-optimized procedures. Some contain high rates of
trivial or insignificant tests. Some generate problems that are too artificial and/or are not a significant
sample of the input space. Finally, some methods generate formulae that are too big to be parsed
and/or handled.
For the reasons described above, we presented (Horrocks et al., 2000) an analytical survey of
the state-of-the art of empirical testing for modal decision procedures. Here instead we present a
new random generation method that provides benefits over previous methods for generating empirical tests, built on some preliminary work (Horrocks et al., 2000). Our new method fixes and much
generalizes the 3CNF  methodology for randomly generating clausal formulae in modal logics
(Giunchiglia & Sebastiani, 1996; Hustadt & Schmidt, 1999; Giunchiglia, Giunchiglia, Sebastiani,
& Tacchella, 2000) used in many previous empirical tests of modal decision procedures. It eliminates or drastically reduces the influence of a major flaw of the previous method, 2 and allows for
generating a much wider variety of problems.
In Section 2 we recall a list of desirable features for good test sets. In Section 3 we briefly
survey the state-of-the-art test methods. In Sections 4 and 5 we present and discuss the basic and
the advanced versions of our new test method respectively, and evaluate their features by presenting
a large amount of empirical results. In Section 6 we provide a theoretical result showing how
the advanced version of our method, in principle, can cover the whole input space. In Section 7
we discuss the features of our new method, and compare it wrt. the state-of-the-art methods. In
Section 8 we conclude and indicate possible future research directions.
A 5-page system description of our random generator has been presented at IJCAR2001 (PatelSchneider & Sebastiani, 2001).

2. Desirable Features for Good Test Sets
The benefits of empirical testing depend on the characteristics of the inputs provided for the testing,
as empirical testing only provides data on these particular inputs. If the inputs are not typical or
suitable, then the results of the empirical testing will not be useful. This means that the inputs
for empirical testing must be carefully chosen. With Horrocks (Horrocks et al., 2000) we have
previously proposed and motivated the following key criteria for creating good test sets.
Representativeness: The ideal test set should represent a significant sample of the whole input
space. A good empirical test set should at least cover a large area of inputs.
Difficulty: A good empirical test set should provide a sufficient level of difficulty for the system(s)
being tested. (Some problems should be too hard even for state-of-the-art systems, so as to
be a good benchmark for forthcoming systems.)
Termination: To be of practical use, the tests should terminate and provide information within a
reasonable amount of time. If the inputs are too hard, then the system may not be able to
provide answers within the established time. This inability of the system is of interest, but
can make system comparison impossible or insignificant.
2. That is, a significant amount of inadvertently trivial problems are generated unless the parameter p is set to 0 (Horrocks et al., 2000). See Section 4.1 for a full discussion of this point.

352

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Scalability: The difficulty of problems should scale up, as comparing absolute performances may
be less significant than comparing how performances scale up with problems of increasing
difficulty.
Valid vs. not-valid balance: In a good test set, valid and not-valid problems should be more or
less equal both in number and in difficulty. Moreover, the maximum uncertainty regarding the
solution of the problems is desirable.
Reproducibility: A good test set should allow for easily reproducing the results.
The following criteria derive from or are significant sub-cases of the main criteria above.
Parameterization: Parameterized inputs with sufficient parameters and degrees of freedom allow
the inputs to range over a large portion of the input space.
Control: In particular, it is very useful to have parameters that control monotonically the key features of the input test set, like the average difficulty and the valid vs. non-valid rate.
Modal vs. propositional balance: Reasoning in modal logics involves alternating between two orthogonal search efforts: pure modal reasoning and pure propositional reasoning. A good test
set should be challenging from both viewpoints.
Data organization: The data should be summarizable so as to make a comparison possible with
a limited effort and plottable so as to enable the qualitative behavior of the system(s) to
be highlighted.
Finally, particular care must be taken to avoid the following problems.
Redundancy: Empirical test sets must be carefully chosen so as not to include inadvertent redundancy. They should also be chosen so as not to include small sub-inputs that dictate the result
of the entire input.
Triviality: A good test set should be flawless, that is, it should not contain significant subsets of
inadvertent trivial problems.
Artificiality: A good empirical test set should correspond closely to inputs from applications.
Over-size: The single problems should not be too big w.r.t. their difficulty, so that the resources
required for parsing and data managing do not seriously influence total performance.
These criteria, which are described and motivated in detail by Horrocks et al. (2000), have been
proposed after a five-year debate on empirical testing in modal logics (Giunchiglia & Sebastiani,
1996; Heuerding & Schwendimann, 1996; Hustadt & Schmidt, 1999; Giunchiglia et al., 2000;
Horrocks & Patel-Schneider, 2002). (Notice that some of these criteria are identical or similar to
those suggested by Heuerding & Schwendimann, 1996.)
The above criteria are general, and in some cases they require some interpretation. First, some
of them have to be implicitly interpreted as unless the user deliberately wants the contrary for some
reason. For instance, it might be the case that one wants to deliberately generate easy problems,
e.g., to be sure that the tested procedure does not take too much time to solve them, or redundant
353

fiPATEL -S CHNEIDER & S EBASTIANI

problems, e.g., to test the effectiveness of some redundancy elimination technique, or satisfiable
problems only, e.g., to test incomplete procedures. To this extent, the key issue here is having a
reasonable form of control over these features, so that one can address not only general-purpose
criteria, but also specific desiderata.
Second, in some cases, there may be a tradeoff between two distinct criteria, so that it may
be necessary to choose only one of them, or to make a compromise. One example is given by
redundancy and artificiality: in some real-world problems large parts of the knowledge base are
irrelevant for the query, whose result is determined by a small subpart of the input; in this sense
eliminating such redundancies may make problems more artificial.
Particular attention must be paid to the problem of triviality, as it has claimed victims in many
areas of AI. In fact, flaws (i.e., inadvertent trivial problems) have been detected in random generators
for SAT (Mitchell, Selman, & Levesque, 1992), CSP (Achlioptas, Kirousis, Kranakis, Krizanc, Molloy, & Stamatiou, 1997; Gent, MacIntyre, Prosser, Smith, & Walsh, 2001), modal reasoning (Hustadt & Schmidt, 1999) and QBF (Gent & Walsh, 1999). Thus, the notion of trivial (and thus
flawed) deserves more comment.
In the work by Achlioptas et al. (1997) flawed problems are those solvable in linear time by
standard CSP procedures, due to the undesired presence of implicit unary constraints causing some
variables value to be inadmissible. A similar notion holds for SAT (Mitchell et al., 1992) and QBF
(Gent & Walsh, 1999). In the literature of modal reasoning, instead, the typical flawed problems
are those whose (un)satisfiability can be verified directly at propositional level, that is, without
investigating any modal successors; this kind of problems are typically solved in negligible time
w.r.t. other problems of similar size and depth (Hustadt & Schmidt, 1999; Giunchiglia et al., 2000;
Horrocks et al., 2000).3 Thus, with a little abuse of notation and when not otherwise specified, in
this paper we will call trivially (un)satisfiable the problems of this kind. 4

3. An Overview of the State-of-the-art
Previous empirical tests have mostly been generated by three methods: hand-generated formulae
(Heuerding & Schwendimann, 1996), randomly-generated clausal modal formulae (Giunchiglia &
Sebastiani, 1996; Hustadt & Schmidt, 1999; Giunchiglia et al., 2000), and randomly-generated
quantified boolean formulae that are then translated into modal formulae (Massacci, 1999).
We have already presented a detailed analysis of these three methods (Horrocks et al., 2000).
Here we present only a quick overview of the latter two methods, as we will refer to them in following sections.5
3.1 The 3CNF  Random Tests
In the 3CNF  test methodology (Giunchiglia & Sebastiani, 1996; Hustadt & Schmidt, 1999;
Giunchiglia et al., 2000), the performance of a system is evaluated on sets of randomly generated 3CNF  formulae. A CNF  formula is a conjunction of CNF  clauses, where each clause
3. Of course here by modal we implicitly assume the modal depth be strictly greater than zero, that is, we do not
consider purely propositional formulas.
4. Notice that we do not use the more suitable expression propositionally (un)satisfiable because the latter has been
used with a different meaning in the literature of modal reasoning (see, e.g., Giunchiglia & Sebastiani, 1996, 2000).
5. The first method (Heuerding & Schwendimann, 1996) is obsolete, as the formulae generated are too easy for current
state-of-the-art deciders (Horrocks et al., 2000).

354

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

is a disjunction of either propositional or modal literals. A literal is either an atom or its negation.

Modal atoms are formulae of the form  , where  is a CNF  clause. A 3CNF  formula is a
CNF  formula where all clauses have exactly 3 literals.
3.1.1 T HE R ANDOM G ENERATOR
A 3CNF  formula is randomly generated according to five parameters: the (maximum) modal
depth  ; the number of clauses in the top-level conjunction  ; the number of propositional variables

; the number of distinct box symbols  ; and the probability  of an atom occurring in a clause at
depth 	 being purely propositional.
The random 3CNF  generator, in its final version (Giunchiglia et al., 2000), works as follows:
a 3CNF  formula of depth  is produced by randomly generating
depth  , and forming their conjunction;



a 3CNF  clause of depth  is produced by randomly generating three distinct, under commutativity of disjunction, 3CNF  atoms of depth  , negating each of them with probability
0.5, and forming their disjunction;











a propositional atom is produced by picking randomly an element of
uniform probability;

3CNF  clauses of

fffifi

with

a 3CNF  atom of depth  is produced by generating with probability  a random
"! 
"!
, where
is picked
propositional atom, and with probability   a 3CNF  atom

"#

randomly in ff 
 and
is a randomly generated 3CNF  clause of depth $% .

Recently Horrocks and Patel-Schneider (2002) have proposed a variant of the 3CNF  random
#
generator of Giunchiglia et al. (2000). They added four extra parameters: &(' and & , representing
#
#",respectively the probability that a propositional and modal atom is negated, and ) +* and )
,
representing respectively the minimum and maximum number of modal literals in a clause, with
equal probability for each number in the range. For their experiments, they always set &.'0/1243
#
#",6and ) +* /5)
/57 . To this extent, 3CNF  formulas can be generated as in the generator of
#
# +*
#",6Giunchiglia et al. (2000) by setting & ' /8&
/9243 and )
/:)
/97 .
3.1.2 T EST M ETHOD & DATA A NALYSIS
The 3CNF  test method works as follows. A typical problem set is characterized by a fixed

,  ,  and  :  is varied in such a way as to empirically cover the 100% satisfiable100%
unsatisfiable transition. Then, for each tuple of the parameters values (data point from now on)
in a problem set, a certain number of 3CNF  formulae are randomly generated, and the resulting
formulae are given in input to the procedure under test, with a maximum time bound. Satisfiability
rates, median/percentile values of the CPU times, and median/percentile values of other parameters,
e.g., number of steps, memory, etc., are plotted against the number of clauses  or the ratio of

clauses to propositional variables <; .
3.2 The Random QBF Tests
In QBF-based benchmarks (such as part of the TANCS99 benchmarks (Massacci, 1999)), system performances are evaluated on sets of random quantified boolean formulae, which are gener355

fiPATEL -S CHNEIDER & S EBASTIANI

ated according to the method described by Cadoli, Giovanardi, and Schaerf (1998) and Gent and
Walsh (1999) and then converted into modal logic by using a variant of the conversion by Halpern
and Moses (1992).
3.2.1 T HE R ANDOM G ENERATOR





Random QBF formulae are generated with alternation depth
and at most
variables at each
alternation. The matrix is a random propositional CNF formula with  clauses of length , with
some constraints on the number of universally and existentially quantified variables within each
clause. (This avoids the problem of generating flawed random QBF formulae highlighted by Gent
& Walsh, 1999.) For instance, a random QBF formula with /97 , /
looks like:

 
	ff
fi  
ff

  
fi  	ff
fi  ff
  
(1)
Here  is a random CNF formula with parameters ,  and . We will denote with  and 
the total number of universally and existentially quantified variables respectively. Clearly, both 
and  are  !#" . Moreover, $ is the modal formula resulting from Halpern and Moses %
conversion, so both the depth and the number of propositional variables of $ are also &'()" .
 

 



  

 

   











3.2.2 T EST M ETHOD & DATA A NALYSIS
The test method, as it was used in the TANCS competition(s) (Massacci, 1999), works as follows.
The tests are performed on single data points. For each data point, a certain number of QBF
formulae are randomly generated, converted into modal logics and the resulting formulae are given
as input to the procedure being tested, with a maximum time bound. The number of tests which
have been solved within the time-limit and the geometrical mean time for successful solutions are
then reported. Data are rescaled to abstract away machine and run-dependent characteristics. This
results typically in a collection of tables presenting a data pair for each system under test, one data
point per row.

4. A New CNF  Generation Method: Basic Version
From our previous analysis (Horrocks et al., 2000) we have that none of the current methods are
completely satisfactory. To cope with this fact, we propose here what we believe is a much more satisfactory method for randomly generating modal formulae. The new method can be seen as an improved and much more general version of the random 3CNF  generation method by Giunchiglia
et al. (2000).
We present our new method by introducing incrementally its new features in two main steps. In
this section we introduce a basic version of the method, wherein






we provide a new interpretation for the parameter  (Section 4.1) that allows for varying 
without causing the flaws described in Horrocks et al. (2000); and
we extend the interpretation for the parameter  (Section 4.3), providing a more fine-grained
way for tuning the difficulty of the generated formulae.

In Section 5, we present the full, advanced version of the method, wherein
356

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE




we further extend the parameters  and  , allowing for shaping explicitly the probability
distribution of the propositional/modal rate and the clause length respectively (Section 5.1);
and



we allow  and  vary with the nesting depth of the subformulae (Section 5.2), allowing for
different distributions at different depths.

To investigate the properties of our CNF  generator we also present a series of experiments with
appropriate settings either to mimic previous generation methodologies or to produce improved or
new kinds of tests.
In all tests we have adopted the testing criteria of the 3CNF  method. For each test set, we
fixed all parameters except  , which was varied to span at least the satisfiability transition area.
(Because of the Valid vs. non-valid balance feature of Section 2, we consider the transition area


to be the interesting portion of the test set.) For almost all test sets we varied  from to   ,



, or   , resulting in integral values for  ;
ranging from  to   , 3  , or   . For each
3 
value of  we generated 100 formulae, a sufficient number to produce reasonably reliable data. A
time limit of 1000 seconds was imposed on each attempt to determine the satisfiability status of a
formula. As it is common practice, we set the number of boxes  to  throughout our testing. This
setting for  produces the hardest formulae (Giunchiglia & Sebastiani, 1996; Hustadt & Schmidt,
1999; Giunchiglia et al., 2000). We performed several test sets with similar parameters, often, but

not always, varying only .
We tested our formulae against two systems, DLP version 4.1 (Patel-Schneider, 1998) and
*SAT version 1.3 (Tacchella, 1999), two of the fastest modal decision procedures. They are available at http://www.bell-labs.com/usr/pfps/dlp and http://www.mrg.dist.unige.it/tac respectively.
All the code used to generate the tests is available at http://www.bell-labs.com/usr/pfps/dlp.
We plotted the results of our test groups (test sets with similar parameters) on six or four plots.
Two plots were devoted to the performance of DLP, one showing the median and one showing the

90th percentile time taken to solve the formulae at each value of  , plotted against <; . For those
test groups were we ran *SAT we also plotted the median and 90th percentile for *SAT.
We also plotted the fraction of the formulae that are determined to be satisfiable or unsatisfiable
by DLP within the time limit.6 To save space, satisfiability and unsatisfiability fractions are plotted
together on a single plot. Satisfiability fractions are higher on the left side of the plot while unsatisfiability fractions are higher on the right. This multiple plotting does obscure some of the details,
but the only information that we are interested in here is the general behavior of the fractions, which
is not obscured. In fact, the multiple plotting serves to highlight the crossover regions, where the
satisfiability and unsatisfiability fractions are roughly equal.
Finally, we plotted the fraction of the formulae where DLP finds a model or determines that
the formula is unsatisfiable without investigating any modal successors. We call these fractions
the trivial satisfiability and trivial unsatisfiability fractions. These last fractions are an estimate
of the number of formulae that are satisfiable in a Kripke structure with no successors like, e.g.,
  fi  and that have no propositional valuations like, e.g.,  6fi  fi   respecfi 
tively. For various reasons, discussed below, they are better indicators of triviality than the more








"







fi"

6. Notice that the two curves are symmetric with respect to 0.5 if and only if no test exceeds the time limit. E.g., if
at some point 40% of the tests are determined to be satisfiable by DLP, 10% are determined to be unsatisfiable and
ff     .
50% are not solved within the time limit, then the two curves are not symmetric at that point, as  	
fi

357

fiPATEL -S CHNEIDER & S EBASTIANI

formal measures used in previous papers. Again, trivial satisfiability and unsatisfiability fractions
are plotted together on a single plot.
To reduce clutter on the plots, we used a line to show the results for each value of  we tested.
To distinguish between the various lines on a plot, we plotted every five or 10 data points with a
symbol, identified in the legend of the plot.
Running the tests presented in this paper required some months of CPU time. Because of this,
we ran our tests on a variety of machines. These machines range in speed from a 296MHz SPARC
Ultra 2 to a 400MHz SPARC Ultra 4 and had between 256MB and 512MB of main memory. No
machines were completely dedicated to our tests, but they were otherwise lightly loaded. Each test
set was run on machines with the same speed and memory. Direct comparison between different
groups of tests thus has to take into account the differences between the various test machines.
4.1 Reinterpreting the Parameter 
One problem with the previous methods for generating CNF  formulae is that the generated formulae can contain pieces that make the entire formula easy to solve. This mostly results from the
presence of strictly-propositional top-level clauses. With the small number of propositional variables in most tests (required to produce reasonable difficulty levels for current systems), only a
few strictly-propositional top-level clauses are needed to cover all the combinations of the propositional literals and make the entire formula unsatisfiable. Previous attempts to eliminate this trivial
unsatisfiability have concentrated on eliminating top-level propositional literals by setting  /5
(Hustadt & Schmidt, 1999; Giunchiglia et al., 2000). (Unfortunately this choice forces 
 ,
as for 9  such formulae are too hard for all state-of-the-art systems.) When each atom in a
clause is generated independently from the other atoms of the clause an approach that modifies the
probability of propositional atoms is necessary to eliminate these problematic clauses.



The first new idea of our approach, suggested previously (Horrocks et al., 2000), works as
follows. Instead of forbidding strictly-propositional clauses except at the maximum modal depth,  ,
by setting  / , we instead require that the ratio between propositional atoms in a clause and the
clause size be as close as possible to the propositional probability  for clauses not at the maximum
modal depth  . 7





For clauses of size  , if  is ;  for some integral , this results in all clauses not at modal
depth  having propositional atoms and  
modal atoms. For other values of  , we allow
or 4 
propositional atoms in each clause not at modal depth  , with probability
either 4 


and    4  , respectively.8 For instance, if  / 2 and  / 7 , then each clause
4
0
contains 1 propositional and 1 modal literal, and the third is propositional with probability 0.8, as


, this eliminates the possibility of strictly
7
2 $
7 2
/
  $9 /
2 . If 
9 ;
propositional clauses, which are the main cause of trivial unsatisfiability, except at modal depth  .

   	

 	

 
 fiff   fiff  fi



fi  

"

fiff

7. Other approaches to eliminating propositional unsatisfiability are possible. For example, it would be possible to
simply remove any strictly-propositional clauses after generation. However, this technique would alter the meaning
of the parameter , that is, the actual probability for a literal to be propositional would become strictly smaller than
, and it will be out of the control of the user.


and
.
8. Remember that




   !#"%$'&)( "+*-,+. /0 1 "2#"+$'&3( "+45,%.
358

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5
N=6
N=7
N=8
N=9

0.8

N=3
N=4
N=5
N=6
N=7
N=8
N=9

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60
L/N

80

100

120

20

DLP median times

40

60
L/N

80

100

120

DLP 90th percentile times

1000

1000

N=3
N=4
N=5
N=6
N=7
N=8
N=9

100

N=3
N=4
N=5
N=6
N=7
N=8
N=9

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60
L/N

80

100

120

20

*SAT median times

40

60
L/N

80

100

120

*SAT 90th percentile times

1000

1000

N=3
N=4
N=5
N=6
N=7
N=8
N=9

100

N=3
N=4
N=5
N=6
N=7
N=8
N=9

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60
L/N

80

Figure 1: Results for

4.1.1 M ODAL D EPTH



100

/

7

120

,

/

,

/

20

40

, and 

/:243

60
L/N

80

100

120

(old method)



Our first experiments were a direct comparison to previous tests. We generated CNF  formulae
with  /17 ,  /  ,  /  , and  / 243 , a setting that has been used in the past, and one that
exhibits some problematic behavior. We used both our new method and the old 3CNF  generation
method by Giunchiglia et al. (2000) briefly described in Section 3.1 (the old method from now
on). We also generated CNF  formulae with  /17 ,  /  ,  /  , and 	/  , the standard
method for eliminating trivially unsatisfiable formulae. (At  /  our new method is the same as
the old 3CNF  generation method.) The results of the tests are given in Figures 1, 2, and 3.
359

fiPATEL -S CHNEIDER & S EBASTIANI

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5
N=6
N=7
N=8
N=9

0.8

N=3
N=4
N=5
N=6
N=7
N=8
N=9

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60
L/N

80

100

120

20

DLP median times
1000

60
L/N

80

100

120

DLP 90th percentile times
1000

N=3
N=4
N=5
N=6
N=7
N=8
N=9

100

40

N=3
N=4
N=5
N=6
N=7
N=8
N=9

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60
L/N

80

100

120

20

*SAT median times
1000

60
L/N

80

100

120

*SAT 90th percentile times
1000

N=3
N=4
N=5
N=6
N=7
N=8
N=9

100

40

N=3
N=4
N=5
N=6
N=7
N=8
N=9

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60
L/N

80

Figure 2: Results for



100

/97

,

120

/

20

,

/

, and 

/:243

40

60
L/N

80

100

120

(our new method)

One aspect of this set of tests is that all three collections have many trivially unsatisfiable formulae out of the satisfiability transition area, even the collection with no top-level propositional atoms.
The trivial unsatisfiability occurs in the collection with no top-level propositional atoms because

/
7 ) and both DLP and *SAT detect
there are only a few top-level modal atoms (e.g., for
clashes between complementary modal literals without investigating any modal successors.
The presence of this large number of trivially unsatisfiable formulae is not actually a serious
problem with these tests. The trivial unsatisfiability only shows up after the formulae are almost

/
7 , which is trivial to
all unsatisfiable already and easy to solve. The only exception is for
solve anyway. However, our new generation method considerably reduces the number of trivially
unsatisfiable formulae and almost entirely removes them from the satisfiable/unsatisfiable transition



360

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5
N=6

0.8

N=3
N=4
N=5
N=6

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60
L/N

80

100

120

20

DLP median times

40

60
L/N

80

100

120

DLP 90th percentile times

1000

1000

N=3
N=4
N=5
N=6

100

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60
L/N

80

100

120

20

*SAT median times

40

60
L/N

80

100

120

*SAT 90th percentile times

1000

1000

N=3
N=4
N=5
N=6

100

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60
L/N

80

Figure 3: Results for



100

/97

120

,

/

20

,

/

, and 

40

/:

60
L/N

80

100

120

(either method).

area. There are some trivially satisfiable formulae in this set of tests, but only a few, and only for
the smallest clause sizes. Their presence does not affect the difficulty of the generated formulae.
The two methods with  /:243 are relatively close in maximum difficulty, with our new method
generating somewhat harder formulae. However, our method produces difficult formulae, for both

DLPand *SAT, over a much broader range of <;
than does the original method.
Changing to  /  results in formulae that are orders of magnitude harder. This is not good,
previous arguments to the contrary notwithstanding, as we would like to have a significant number
of reasonable test sets to work with, and  /  allows only consideration of a very few values for

before the formulae are totally impossible to solve with current systems, resulting in very few
reasonable test sets.
361

fiPATEL -S CHNEIDER & S EBASTIANI

So, at a maximum modal depth of %/  our method results in formulae that are of similar
difficulty to the previously-generated formulae and still have trivially unsatisfiable formulae, but
ones that do not seriously affect the difficulty of the test sets.
4.1.2 M ODAL D EPTH



Restricting attention to a maximum modal depth of  /  is not very useful. Formulae with maximum modal depth of  are not representative of modal formulae in general, particularly as they
have no nested modal operators. Sticking to a maximum modal depth of  seriously limits the
significance of the generated tests.
We would thus like to be able to perform interesting experiments with larger maximum modal
depths. So we performed a set of experiments with a maximum modal depth of  /
. We started
with a set of tests that corresponds to previously-performed experiments.
At depth  /
, in the old method for  / 243 the time curves are dominated by a half-dome
shape, whose steep side shows up where the number of trivially unsatisfiable formulae becomes
large before the formulae become otherwise easy to solve, as shown in Figure 4. In fact, nearly all
the unsatisfiable formulae here are trivially unsatisfiable.
This is an extremely serious flaw, as the difficulty of the test set is being drastically affected
by these trivially unsatisfiable formulae. Changing to  /  is not a viable solution because at
depth  /
such formulae are much too difficult to solve, as shown in Figure 5, where the median
percentile exceeds the timeout before any formulae can be determined to be unsatisfiable, even for
3 propositional variables.
With our new method, as shown in Figure 6, the formulae are much more difficult to solve than
the old method, because there is no abrupt drop-off from propositional unsatisfiability, but they are
much easier to solve than those generated with  /  . Further, trivially unsatisfiable formulae do
not appear at all in the interesting portion of the test sets.
,  /243 ) is not entirely suitable. The formulae
Nevertheless this choice of parameters ( /
are becoming too hard much too early. In particular, there are no unsatisfiable formulae that can

be solved for

7 , and thus the unsatisfiability plots cannot be distinguished from the x axis
(recall Footnote 6). However, our new method does provide some advantages already, providing an
interesting new set of tests, albeit one of limited size.









4.2 Increasing 



fiff

We would like to be able to produce better test sets for depth 9/
and greater. One way of
doing this is to increase the propositional probability  from 243 to something like 2 , increasing
the number of propositional atoms and thus decreasing the difficulty of the generated formulae.
This would be very problematic with previous generation methods as it would result in the trivially
unsatisfiable formulae determining the results for even smaller numbers of clauses  , but with our
method here it is not much of a problem.
To investigate the increasing of the the propositional probability, we ran a collection of tests with
maximum modal depth  /
and propositional probability  / 2 with both the old method and
our new method. The results of these tests are given in Figures 7 and 8. As before, the asymmetries

between the satisfiability and unsatisfiability curves in Figure 8 for /93  are due to the fact that
many tests are not solved by DLP within the time limit (c.f., Footnote 6).

fiff



362

ff

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5
N=6

0.8

N=3
N=4
N=5
N=6

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60

80

100
L/N

120

140

160

180

200

20

DLP median times

40

60

80

100
L/N

120

140

160

180

200

DLP 90th percentile times

1000

1000

N=3
N=4
N=5
N=6

100

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

140

160

180

200

20

*SAT median times

40

60

80

100
L/N

120

140

160

180

200

180

200

*SAT 90th percentile times

1000

1000

N=3
N=4
N=5
N=6

100

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

140

Figure 4: Results for



160

/

7

180

,

200

/

20

,

 , and
/

40



60

/:243

80

100
L/N

120

140

160

(old method)

As expected, the old method produces large numbers of trivially unsatisfiable formulae. These
trivially unsatisfiable formulae show up much earlier than with  /9243 , making the tests considerably easier, especially for *SAT.
Our new method produces hard formulae, but ones that are quite a bit easier than for  / 243 .

/
In particular, DLP solved all instances within the time limit for
. Trivially unsatisfiable
formulae do show up, but only well after the formulae are already unsatisfiable, and they do not
significantly affect the difficulty of the tests.
So our method allows the creation of more-interesting tests at modal depths greater than  ,
simply by adjusting  to a value where the level of difficulty is appropriate. Trivial unsatisfiability
is not a problem, whereas in the old method it was the most important feature of the test.
363

fiPATEL -S CHNEIDER & S EBASTIANI

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

N=3
N=4
N=5

0
20

40

60

80

100
L/N

120

140

160

180

200

20

DLP median times

40

60

80

100
L/N

120

140

160

180

200

DLP 90th percentile times

1000

1000

N=3
N=4
N=5

100

100

10

10

1

1

0.1

0.1

0.01

N=3
N=4
N=5

0.01
20

40

60

80

100
L/N

120

140

160

180

200

20

*SAT median times

40

60

80

100
L/N

120

140

160

180

200

180

200

*SAT 90th percentile times

1000

1000

N=3
N=4
N=5

100

100

10

10

1

1

0.1

0.1

0.01

N=3
N=4
N=5

0.01
20

40

60

80

100
L/N

120

140

Figure 5: Results for



160

/

7

180

,

200

/

20

,

 , and
/



40

60

/8

80

100
L/N

120

140

160

(either method)

4.3 Changing the Size of Clauses
A problem with increasing the propositional probability is that formulae become too propositional
that is, the source of difficulty becomes more and more the propositional component of the problem, and not the modal component. As we are interested in modal decision procedures, we do not
want the main (or only) source of difficulty to be propositional reasoning.
We decided, therefore, to investigate a different method for modifying the difficulty of the generated formulae. We instead allow the number of literals in a clause  to vary in a manner similar to
the number of propositional atoms. If  is an integer then each clause has that many literals. Otherwise, we allow either  or  literals in each clause, with probability    and    ,

 	  

 	

364

 	

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5
N=6

0.8

N=3
N=4
N=5
N=6

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60

80

100
L/N

120

140

160

180

200

20

DLP median times

40

1000

80

100
L/N

120

140

160

180

200

DLP 90th percentile times
1000

N=3
N=4
N=5
N=6

100

60

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

140

160

180

200

20

*SAT median times

40

1000

80

100
L/N

120

140

160

180

200

180

200

*SAT 90th percentile times
1000

N=3
N=4
N=5
N=6

100

60

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

Figure 6: Results for

140



160

/97

180

,

200

,

/

20

 , and
/



40

60

/:243

80

100
L/N

120

140

160

(our new method)

respectively. We then determine the number of propositional atoms in each clause based on the
number of literals in that clause.
We generated CNF  formulae with  /
43 , /
 ,  /
 , and  /243 . The change from


/17 to
/
43 produces fewer disjunctive choices and should result in easier formulae. The
results of these tests are given in Figure 9.
These formulae are much easier than those generated with  / 7 , although they are still quite
hard and form a reasonable source of testing data. Trivially unsatisfiable formulae appear in large
numbers only well after the formulae are all unsatisfiable and relatively easy.
To further illustrate the reduction in difficulty with smaller values of  we generated formulae
 3 , 
/ ,  / , and  /:243 . As shown in Figure 10, these formulae are even easier
using  /





 

365

fiPATEL -S CHNEIDER & S EBASTIANI

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5
N=6

0.8

N=3
N=4
N=5
N=6

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60

80

100
L/N

120

140

160

180

200

20

DLP median times

40

60

80

100
L/N

120

140

160

180

200

DLP 90th percentile times

1000

1000

N=3
N=4
N=5
N=6

100

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

140

160

180

200

20

*SAT median times

40

60

80

100
L/N

120

140

160

180

200

180

200

*SAT 90th percentile times

1000

1000

N=3
N=4
N=5
N=6

100

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

140

Figure 7: Results for



160

/

7

180

,

200

/

20

,

 , and
/

40



60

80

fiff

/:2

100
L/N

120

140

160

(old method)



than for  /
43 . Trivially unsatisfiable formulae do appear, but again only after the formulae
become all unsatisfiable, and not until the formulae become easy, particularly for *SAT.
 3 we now have a reasonable set of formulae for maximum modal depth %/
At  /
.
With a maximum modal depth of , the formulae are much more representative than formulae with
maximum modal depth of  . The formulae are neither too easy nor too hard for current modal
decision procedures so the satisfiability transition can be investigated for significant numbers of
propositional variables.
Further, with this new method we can provide a collection of test sets that vary in difficulty

by varying  . Most previous comparative test sets varied , which is problematic because most

interesting parameter sets become too hard for small values of , in the range of to  .







ff

366

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5
N=6

0.8

N=3
N=4
N=5
N=6

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60

80

100
L/N

120

140

160

180

200

20

DLP median times

40

1000

80

100
L/N

120

140

160

180

200

DLP 90th percentile times
1000

N=3
N=4
N=5
N=6

100

60

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

140

160

180

200

20

*SAT median times

40

1000

60

80

100
L/N

120

140

160

180

200

180

200

*SAT 90th percentile times
1000

N=3
N=4
N=5

100

100

10

10

1

1

0.1

0.1

0.01

N=3
N=4
N=5

0.01
20

40

60

80

100
L/N

120

140



Figure 8: Results for

160

/97

180

,

200

/

,

20

 , and
/



40

60

fiff

/:2

80

100
L/N

120

140

160

(our new method)



/
To illustrate the effects of varying  we generated formulae using
,  /  ,  /5 , and

from  to  . As shown in Figure 11, this produces an interesting set of tests.
 /:243 , varying
The difficulty levels can be set appropriately. Trivially unsatisfiable formulae do appear, but only
after the formulae become unsatisfiable anyway. Trivially unsatisfiable formulae do not influence
the difficulty of the test.

   fi

367

fiPATEL -S CHNEIDER & S EBASTIANI

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5
N=6

0.8

N=3
N=4
N=5
N=6

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60

80

100
L/N

120

140

160

180

200

20

40

DLP median times

60

80

100
L/N

120

140

160

180

200

DLP 90th percentile times

1000

1000

N=3
N=4
N=5
N=6

100

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

140

160

180

200

20

*SAT median times

40

60

80

100
L/N

120

140

160

180

200

180

200

*SAT 90th percentile times

1000

1000

N=3
N=4
N=5
N=6

100

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

Figure 9: Results for

4.3.1 M ODAL D EPTH



140

/

160



43

180

,

200

/

20

,

 , and
/



40

60

/:243

80

100
L/N

120

140

160

(our new method)

7

Our method can be used to generate interesting test sets with modal depth  / 7 . This depth is not
at all interesting with previous methodseither the formulae are immensely difficult, such as for
 /: , or the behavior is dominated by trivial unsatisfiability, such as for  /:243 .
For interesting levels of difficulty, we do have to reduce  to values below 43 . If  is much
larger, the formulae are too hard. However, with 
43 we can produce interesting test sets, as
shown in Figure 12. (The relevant asymmetry between the satisfiable and unsatisfiable rates curves

for
3 is due to the high amount of tests exceeding the time limit.) Here the problems are hard

	3 but doable, and there are no problems with trivially (un)satisfiable formulas.
even for





368



fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5
N=6
N=7

0.8

N=3
N=4
N=5
N=6
N=7

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60

80

100
L/N

120

140

160

180

200

20

DLP median times

40

60

80

100
L/N

120

140

160

180

200

DLP 90th percentile times

1000

1000

N=3
N=4
N=5
N=6
N=7

100

N=3
N=4
N=5
N=6
N=7

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

140

160

180

200

20

*SAT median times

40

60

80

100
L/N

120

140

160

180

200

180

200

*SAT 90th percentile times

1000

1000

N=3
N=4
N=5
N=6
N=7

100

N=3
N=4
N=5
N=6
N=7

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

Figure 10: Results for



140

/

160




3

180

,

200

/

20

,

 , and
/



40

60

/:243

80

100
L/N

120

140

160

(our new method)

Our method now allows us fine control of the difficulty of tests. To make a test easier, we can
just reduce the size of clauses by reducing the value(s) of  , or increase the propositional probability

 . This control was missing with the previous method, as
was restricted to integral value, and,
anyway, was always set to 7 and making  much different from 2  resulted in problems with trivial
unsatisfiability for maximum modal depths greater than 1.

5. A New CNF  Generation Method: Advanced Version
Actually, our generator is much more general than what we have described so far. We allow direct
specification of the probability distribution of the number of propositional atoms in a clause, and
369

fiPATEL -S CHNEIDER & S EBASTIANI

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

C=2.2
C=2.4
C=2.6
C=2.8

0.8

C=2.2
C=2.4
C=2.6
C=2.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60

80
L/N

100

120

140

20

DLP median times

40

60

80
L/N

100

120

140

DLP 90th percentile times

1000

1000

C=2.2
C=2.4
C=2.6
C=2.8

100

C=2.2
C=2.4
C=2.6
C=2.8

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80
L/N

100

Figure 11: Results for


/

120

140

,

/

20

,

 , and
/



40

/9243

60

80
L/N

100

120

140

(our new method)

allow the distribution to be different for each modal depth from the top level to    . We also allow
direct specification of the probability distribution for the number of literals in a clause at each modal
depth. Thus, the probability distribution for the number of propositional atoms depends on both the
modal depth and the number of literals in the clause.
5.1 Generalization: Shaping the Probability Distributions.
The generator has two parameters to control the shape of formulae. The first parameter,  , is a
list of lists (e.g., [[0,0,1]]) telling it how many disjuncts to put in each disjunction at each
modal level. Each internal list represents a finite discrete probability distribution. For instance, the
[0,0,1] says  ;  of the disjunctions have  disjunct,  ;  have disjuncts, and ;  have 7
disjunctions (fixed length 3). Because there is only one element of the list, this frequency is used at
each modal depth, until the last. Other possibilities are, e.g., [[1,1,1,1]] (maximum length 4
with uniform distribution), [[16,8,4,2,1]] (maximum length 5 with exponential distribution),
and so on.
The second parameter,  , is a list of lists of lists (e.g., [[[],[],[0,3,3,0]]]) that controls the propositional/modal rate. The top-level elements are for each modal depth (here all the
same). The second-level elements are for disjunctions with 1,2,3,... disjunctions (here only the third
matters as all disjunctions have three disjuncts). For instance, the [0,3,3,0] says  ; of the
disjunctions have  propositional atoms, 7 ; have  propositional atom, 7 ; have propositional
atoms, and  ; have 7 propositional atoms (that is, our new scheme discussed in the paper with



ff

ff

ff

370



ff

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N=3
N=4
N=5
N=6

0.8

N=3
N=4
N=5
N=6

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60

80

100
L/N

120

140

160

180

200

20

40

DLP median times
1000

100
L/N

120

140

160

180

200

DLP 90th percentile times
N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

140

160

180

200

20

*SAT median times
1000

40

60

80

100
L/N

120

140

160

180

200

180

200

*SAT 90th percentile times
1000

N=3
N=4
N=5
N=6

100

N=3
N=4
N=5
N=6

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80

100
L/N

120

140

160

Figure 12: Results for



80

1000

N=3
N=4
N=5
N=6

100

60



180

200

20

 
/



3

,
/



,

40

/97

60

80

, and 

100
L/N

120

140

160

/:243

/
243 ; the old scheme with  /
243 is represented by [[[],[],[1,3,3,1]]]). Notice that
the first element of the distributions in  represents the value  , whilst the first element of the distributions in  represents the value  . Setting the last element of each distribution to zero [...,0]
eliminates all strictly propositional clauses, which are the main cause of trivial unsatisfiability; this


% ;
of Section 4.1.
is the way we implement the constraint 



"

371

fiPATEL -S CHNEIDER & S EBASTIANI

1
2
3
4
5
6

function rnd CNF  (d,m,L,N,p,C)
for i := 1 to  do
repeat

:= rnd clause(d,m,N,p,C);
until is new(Cl );
return    ;

/* generate  distinct random clauses */

/* discards




if it already occurs */



7 function rnd clause(d,m,N,p,C)
8
:= rnd length(d,C);
/* select randomly the clause length */
9
/* select randomly the prop/modal rate */
 := rnd propnum(d,p,K);
10
repeat
11
for j := 1 to  do
/* generate P distinct random prop. literals */

	
12
:= rnd sign() rnd atom(0,m,N,p,C);
/* generate K-P distinct random modal literals */
13
for j := P+1 to do
	
14
:= rnd sign() rnd atom(d,m,N,p,C);
fiff
15
/	
   	 ;
/* discards Cl if contains repeated atoms */
16
until no repeated atoms in(Cl);
17
return   ;






 "

18 function rnd atom(d,m,N,p,C)
19
if d=0
20
then return rnd propositional atom(N); /* select randomly a prop. atom */
21
else
 !
22
:= rand box(m);
/* select randomly an indexed box */

23
:= rand clause(d-1,m,N,p,C);
"!
24
return  ;
Figure 13: Schema of the new CNF  random generator.
For instance, the plots of Figures 1-12 can be obtained with the following choices of C and p:
Fig.
C
p
C (advanced version)
p (advanced version)
1, 4
3
0.5 (old) [[0,0,1]]
[[[],[],[1,3,3,1]]
2, 6
3
0.5 (new) [[0,0,1]]
[[[],[],[0,3,3,0]]
3, 5
3
0
[[0,0,1]]
[[[],[],[1,0,0,0]]
7
3
0.6 (old) [[0,0,1]]
[[[],[],[8,36,54,27]]
8
3
0.6 (new) [[0,0,1]]
[[[],[],[0,1,4,0]]
9
2.5
0.5 (new) [[0,1,1]]
[[[],[0,3,0],[0,3,3,0]]
10, 12 2.25
0.5 (new) [[0,2,1]]
[[[],[0,3,0],[0,3,3,0]]
11
2.2, 2.4, 0.5 (new) [[0,4,1]], [[0,3,2]] [[[],[0,3,0],[0,3,3,0]]
2.6, 2.8
[[0,2,3]], [[0,1,4]]
Our generator works as described in Figure 13. The function is new(Cl ) checks if  /
  ; rnd length(d,C) selects randomly the clause length according to the   -th dis	

tribution in  (e.g, if  is  and  is [[0,1,1][1,2][1]], it returns  with probability ; 7



372

fiA N EW G ENERAL M ETHOD





TO

G ENERATE R ANDOM M ODAL F ORMULAE

 







and with probability ; 7 ); rnd propnum(d,p,K) selects randomly the number of propositional
 
-th distribution in  (e.g, if  is  ,
is and  is
atoms per clause  according to the 
[[[],[0,1,0],[0,1,0,0]] [[1,0][0,1,0]]], it returns  deterministically); rnd sign
selects randomly either the positive or negative sign with equal probability; no repeated atoms in(Cl)
checks if the clause  contains no repeated atom; Sort(Cl) returns the clause  sorted according

to some criterium; rnd propositional atom(N) selects with uniform probability one of the propo !
sitional atoms fi ; rnd box(m) selects with uniform probability one of the  indexed boxes
.
When eliminating duplicated atoms in a clause, we take care not to disturb these probabilities
by first determining the shape of a clause (rows 8-9 in Figure 13), and only then instantiating that
with propositional variables (rows 10-16 in Figure 13). If a clause has repeated atoms, either propositional or modal, the instantiation is rejected and another instantiation of the shape is performed.
If we did not take care in this way we would generate too few small atoms because there are
fewer small atoms than large atoms, resulting in a greater chance of rejecting small atoms because
of repetition.
The elimination of duplicated atoms in a clause is not only a matter of elimination of redundancies, but also of elimination of a source of flaws. In fact, one might generate top-level clauses like


  fi
   
  fi    fi   
fi
    , which would make the whole formula inconsistent.






 


"


 "ff"
$

Example 5.1 We try to guess a parameter set by which the new random generator can potentially
generate the following CNF  formula :




 fi     fi    fi     fi    fi




 fi 
 fi
  fi
  
 fi


(2)
 fi    fi     fi 














fi



 
 









fi







"


 " fi"
"






"


" "
"

"

"



, /
, /
. At top level we have 0 unary, 2 binary and
After a quick look we set  /  ,  /
2 ternary clauses; at depth 1 we have 2 unary and 4 binary clauses; at depth 2 we have only 6 unary
clauses. Thus, we can set
C = [[0,2,2],[2,4],[6]].
(3)
At top level there are no unary clauses (we represent this fact by the empty list []), the 2 binary clauses have 1 propositional literal, and the 2 ternary clauses have 1 propositional literal; at
depth 1, the 2 unary clauses have 0 propositional literals, while the 4 binary clauses have 1 propositional literal. (There is no need to provide any information for depth 2, as all clauses are purely
propositional.) Thus, we can set
p = [[[],[0,2,0],[0,2,0,0]] [[2,0],[0,4,0]]].

(4)

The two expressions can then be normalized into:
C = [[0,1,1],[1,2],[1]]
p = [[[],[0,1,0],[0,1,0,0]] [[1,0],[0,1,0]]].

(5)

Notice that any other setting of  ,  obtained by changing the non-zero values in (5) into other
non-zero values, or turning zeros into non-zeros (but not vice versa!), will do the work, just with a
different probability. For instance, turning the first list in  into [1,1,1] allows for generating
also unary clauses at top level; anyway, with probability ; 7  the generator may still produce

formulae with only binary and ternary clauses at top level.

 "

373

fiPATEL -S CHNEIDER & S EBASTIANI

Satisfiability and Unsatisfiability Fractions
1

Trivial Satisfiability and Unsatisfiability Fractions
1

N,d=3,3
N,d=4,3
N,d=3,4
N,d=4,4

0.8

N,d=3,3
N,d=4,3
N,d=3,4
N,d=4,4

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

0
20

40

60

80
L/N

100

120

140

20

DLP median times
1000

60

80
L/N

100

120

140

DLP 90th percentile times
1000

N,d=3,3
N,d=4,3
N,d=3,4
N,d=4,4

100

40

N,d=3,3
N,d=4,3
N,d=3,4
N,d=4,4

100

10

10

1

1

0.1

0.1

0.01

0.01
20

40

60

80
L/N

100

120

140

20

40

60

80
L/N

100

120

140



Figure 14: Results for DLPwith  /97  , /97  ,  / [[1,8,1]],
 / [[[1,0],[0,1,0],[0,1,1,0]]].

As an illustration of our general method, we present a set of tests with  /  ,  / 7  ,

,
/ [[1,8,1]], and %/ [[[1,0],[0,1,0],[0,1,1,0]]]. This set of tests
introduces a small fraction of single-literal clauses that contain a modal literal (except at the greatest
modal depth, where they contain, of course, a single propositional literal). The results of tests are
given in Figure 14. Again, trivial instances occur only out the interesting zone. Here we can generate
interesting test sets even with modal depth .


/17 

5.2 Varying the Probability Distributions with the Depth
Our new method provides the ability to fine-tune the distribution of both the size and the propositional/modal rate of the clauses at every depth. This fine tuning results in a very large number
of parameters, and so far in this paper we have only investigated distributions that conform to the
scheme described above or ones that correspond to the 3CNF  generation method previously used.
To give an example of how to vary the probability distributions with the nesting depth of the


clauses, we consider the case with  /
63 , 
/
 ,
/7  63 ,
/ [[1,8,1],[1,2]],

/ [[[1,0],[0,1,0],[0,1,1,0]],[[1,0],[0,1,0]]]. The results of the tests are
given in Figure 15.
The  parameter says that the probability distributions of the length of the clauses occurring at
nesting depth  and  are [1,8,1] and [1,2] respectively. (When not explicitly specified, it is
374

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Satisfiability and Unsatisfiability Fractions

Trivial Satisfiability and Unsatisfiability Fractions

1

1

0.8

0.8

N,d=3,4
N,d=4,4
N,d=5,4
N,d=3,5
N,d=4,5
N,d=5,5

0.6

N,d=3,4
N,d=4,4
N,d=5,4
N,d=3,5
N,d=4,5
N,d=5,5

0.6

0.4

0.4

0.2

0.2

0

0
0

20

40

60

80

100

120

140

0

20

40

60

L/N

80

100

120

140

L/N

DLP median times
1000

DLP 90th percentile times
1000

N,d=3,4
N,d=4,4
N,d=5,4
N,d=3,5
N,d=4,5
N,d=5,5

100

N,d=3,4
N,d=4,4
N,d=5,4
N,d=3,5
N,d=4,5
N,d=5,5

100

10

10

1

1

0.1

0.1

0.01

0.01
0

20

40

60

80

100

120

140

0

20

L/N

40

60

80

100

120

140

L/N




Figure 15: Results for DLPwith /
63 , 
/
 ,
/
7  63 ,
/ [[1,8,1],[1,2]],
 / [[[1,0],[0,1,0],[0,1,1,0]],[[1,0],[0,1,0]]].



considered the last distribution by default, as in the case of depth  .) Thus, the top-level clauses
are on average ;  unary, ;  binary ;  ternary, while the clauses occurring at depth  are
on average ; 7 unary and ; 7 binary.



The  parameter says that the lists of probability distributions of the propositional/modal ratio
at nesting depth  and  are [[1,0],[0,1,0],[0,1,1,0]] and [[1,0],[0,1,0]] respectively. Thus, at every depth, unary clauses have no propositional literal and binary clauses have
 propositional and  modal literal. The top-level ternary clauses have either  or
propositional
literals, with equal probability.



Notice that at top level the distributions are identical to those of Figure 14, whilst at depth 
there are no more ternary clauses and a higher fraction of unary clauses. These slight modifica
tions allow reasonable test sets with  / 3 and
/
3 . Moreover, trivial instances have nearly
disappeared.

6. Generality of the Method

%

We have already observed (Horrocks et al., 2000) that for normal modal logics, from
upward,
there is no loss in the restriction to CNF  formulae, as there is an equivalence between arbitrary
375

fiPATEL -S CHNEIDER & S EBASTIANI

normal modal formulae and CNF  formulae9 . We may wonder how well our generation technique covers the whole space of CNF  formulae, and how well we can approximate a restricted
subclass of this space. Example 5.1 represents an instance of a very general property of our random
generation technique, which we present and discuss below.
Now we assume that the rnd CNF  of Figure 13 is a purely random generator, i.e., it performs all non-deterministic choices independently and in a pure random way. (Of course pseudorandom generators only approximate this feature.) Moreover, with no loss of generality, we restrict
our discussion to CNF  formulae which have no repeated clauses at top level and no repeated
atoms inside any clause at any level, and in which atoms are sorted within each clause, according to the generic function Sort() of Figure 13. The former allows for considering only formulae
which are already simplified out; the latter allows for considering only one representative for each
class of formulae which are equivalent modulo order permutations. As discussed by Giunchiglia


  fi  fi 
et al. (2000), the latter allows for further simplifying subformulae like, e.g., fi  fi





or fi 
fi
 fi
fi  .




"

$

 



 "  
 fi"



"

Let be a sorted CNF  formula of depth  and with  top-level clauses built on all the

"#
propositional atoms fffi    fi and on all the modal boxes ff   
 , which has no repeated
clause at top level and no repeated atoms inside any clause at any level. Then we can construct 

and  so that, for each  , ,  :





(a) the -th element of the  -th sublist in  is non-zero if and only if there is a clause of length
occurring at depth  in , and

(b) the   -the element of the -th sub-sublist of the  -th sublist in  is non-zero if and only if

there is a clause of length occurring at depth  which contains  propositional literals.

$

One possible operative technique to build  and  works as follows. Initialize  as a list of
sublists. Then, for every depth level  ff2     , set the  -th sublist of  as follows:



$






(i) set the size of the sublist as the maximum size of clauses occurring in at depth  ;


(ii) for all 8ff      , count the number of clauses of length occurring in at depth  , and
append the result to the sublist.



Initialize  as a list of  sublists of sub-sublists. Then, for every depth level
the  -th sublist of  as follows:
(i) look at
; 
(ii) for all





: set the size



ff    







$

ff2   8 

of the sublist as the maximum size of clauses occurring in



$

, set

at depth

, generate the -th sub-sublist as follows:



look at  : if the number of clauses of length occurring at depth  is non-zero, then set

the length of the sub-sublist to 8 , else set to 0;

	

9. This holds for all modal normal logics from
upward, as the conversion works recursively on the depth of the
formula, from the leaves to the root, each time applying to sub-formulae the propositional CNF conversion and the
transformation




fffi



 fi

which preserves validity in such logics.

376




ff

 

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE



$

for all 
occurring in
ff2  

  , count the number of clauses of length
 which have  propositional literals, and append the result to the sub-sublist.



at depth

Example 5.1 represents an instance of application of the above technique for construction  and 
 above, but are such
from . Notice that the  and  parameters not only verify points  and
that the probability distributions mimic the actual number of occurrences of the different kinds of
clauses.

$

 "

"

$

Theorem 6.1 Let rnd CNF  be a purely random generator as in Figure 13. Let be a sorted
CNF  formula of depth  and with  top-level clauses built on all the propositional atoms in

 #
fffi    fi  and on all the modal boxes in ff
  
 , which has no repeated clause at top level
and no repeated atoms inside any clause at any level. Let  and  be built from so that to verify
 above. Let  and   be obtained from  and  respectively by substituting some
points  and
zero-values with some non-zero values. Then we have:

 "

$

"

$

(i) rnd CNF  (d,m,L,N,p,C) returns with some non-zero probability  ;
(ii) rnd CNF  (d,m,L,N,p,C) returns with some non-zero probability 

$

 

.

$

Proof The fully-detailed proof is reported in Appendix. Here we sketch the main steps.
The following facts come straightforwardly by induction on the structure of :

$

1. every propositional atom occurring in at some depth  is returned with the same non-zero
probability   by both rnd atom(0,m,N,p,C) and rnd atom(0,m,N,p,C);
 
2. every modal atom  occurring in at some depth  is returned with some non-zero probability  by rnd atom(d-i,m,N,p,C), and is returned with some non-zero probability   
by rnd atom(d-i,m,N,p,C);

$







$




3. every clause  occurring in at some depth  is returned with some non-zero probability
 by rnd clause(d-i,m,N,p,C), and is returned with some non-zero probability    by
rnd clause(d-i,m,N,p,C).







Thus, every top level clause 
	 is returned by rnd clause(d,m,N,p,C) and rnd clause(d,m,N,p,C)
with some non-zero probabilities  	 and  ff	 respectively, being  ff	 fi 	 . From this fact, it comes
straightforwardly that is returned by rnd CNF  (d,m,L,N,p,C) and rnd CNF  (d,m,L,N,p,C)
with some non-zero probabilities  and   respectively, being    .
Q.E.D.
Q.E.D.
From a theoretical viewpoint, Theorem 6.1  shows that our generation technique is very
general, because, for every CNF  formula , there exists a choice for the parameters s.t. a purely
random generator returns with some non-zero probability  .
 is not unique as,
Of course, the choice criterium for  and  suggested by points  and
for example, any other setting obtained from it by turning zeros into non-zeros would match the
requirements. As an extreme case, we might think to do very general choices like

$

"

$

$

C = [[1,1,1,...],...]

 "

"

p = [[[[1,1],[1,1,1],[1,1,1,1]...]]].

(6)

which guarantee to have every possible CNF  formula within a given bound in clause size with
non-zero probability. Anyway, Theorem 6.1   shows that, extending the number of non-zeros
values, the probability of generating decreases.

$

 "

377

fiPATEL -S CHNEIDER & S EBASTIANI

For instance, consider Example 5.1. Turning the first list in  of (5) into [1,1,1] would still
allow for generating the formula (2), but it would allow for generating also unary clauses at top level
with probability  
; 7  , which converges quickly to  with  .

 "

Usually we are not interested in randomly generating one precise formula with some non-zero
probability which would be rather small anyway but rather to randomly generate a class of formulae which are as similar as possible a given target class of formulae. Adding redundant non-zeros
would extend the range of shapes for formulae, extending the variance and lowering the resemblance
to the target class of formulae.

7. Discussion
7.1 The Basic and the Advanced Method
Our new testing method can be used at two different levels, depending on the attitude and on the
skills and experience of the user.
In the basic usage the clause length  is represented by lists with either only one non-zero
element (e.g., [[0,0,1]], meaning clause length ) or only two adjacent non-zero elements
(e.g., [[0,2,1]], meaning clause length or 7 , with probability ; 7 and ; 7 respectively);
similarly, the propositional/modal rate  is represented by lists with either only one non-zero element
(e.g., [[[],[],[0,1,0,0]]], meaning   propositional literal per clause) or only two nonzero adjacent elements (e.g., [[[],[],[0,3,2,0]]], meaning either  or propositional
literals per clause, with probability 7 ; 3 and ; 3 respectively); the distributions do not vary with
the depth.
In the basic way the random generator is used as a flawless 10 extension of the 3CNF 
method of Giunchiglia and Sebastiani (1996), which allows for setting the clause length to either
fixed integer values or to non-integer average values. The number of parameters is kept relatively
small, so that to allow a coarse-grained coverage of a significant subspace with an affordable number
of tests.











In the advanced usage, it is possible to apply any finite probability distributions to both  and  ;
moreover, it is possible to use different distributions at different depths. This opens a huge amount
of possibilities, but requires some skills and experience from the user: the representation of sophisticated multi-level distributions may be rather complicated, and may thus require some practice;
moreover, the usage of complex distributions requires some care, as the presence non-constant distributions in both clause length and propositional/modal rate may significantly enlarge the variance
of the features of the generated formulae, making the effects of the tests more unpredictable and
instance-dependent.
In order to guide the user, we provide some general suggestions for choosing the parameter sets
in a testing session. They come from both theoretical issues and our practical experience in using
the generator.



Avoid generating purely propositional top-level clauses, that is, set p = [[...,0],...].
See Sections 4.1 and 5.1. If possible, avoid generating unary top-level clause, that is, set C
= [[0,...],...]). See also Section 7.5.

10. In the sense of free from the flaw highlighted in the work by Hustadt and Schmidt (1999) and Giunchiglia
et al. (2000).

378

fiA N EW G ENERAL M ETHOD




TO

G ENERATE R ANDOM M ODAL F ORMULAE

In organizing a testing session, fix the parameter sets according to the following order and
directives.
(i) Fix d. With d=1 the search is mostly dominated by its propositional component, with
d>2 it tends to be dominated by its modal component. d=2 is typically a good start.
(ii) Fix m. m substantially partitions the problem into m independent problems. Increasing m,
the samples tend to be more likely-satisfiable. m=1 is typically a good start.
(iii) Set C. Increasing the top level values of C, the samples tend to be more likely-satisfiable
and the propositional component of search increases, so that the transition area moves
to the right and the hardness peaks grow. Average values in  267   for the top level
distributions of C are typically a good start.
(iv) Set p. Decreasing the top level values of p, the modal component of search increases.
For the the top level distributions of p, having on average half of top-level atoms propositional (that is, the  /:243 of Section 4) is typically a good start.
(v) For each choice of the above parameters, increase N, starting from (at least) the maximum length in C, until the desired level of hardness is reached.
(vi) Make L vary within the satisfiability transition area.











When dealing with C and p, focus on top-level clause distributions first. Small variations
of C and p at top level may cause big variations in hardness and satisfiability probability.
Variations at lower levels typically cause much smaller effects.




Use convex distributions: e.g., [1,5,1] and [5,1,5] have the same mean value, but the
variance of the former is much smaller than that of the latter.




Do keep L ranging in the satisfiability transition area: increasing L out of it, the fraction of
trivially unsatisfiable samples can become relevant. To determine the satisfiability transition
area, make a preliminary check with few samples per point (say, 10) using dichotomic search.
Unlike N (and m), the parameters d, C, p make the formulas vary their shape. Thus, we
suggest to group together plots with the same d, C and p values and increasing Ns.

On the whole, the large number of parameters makes it impossible to cover the parameter space
in a reasonable amount of testing. However, just about any CNF  formula shape can be generated
so that the method described in Section 6 can be used to produce random formulae reasonably
similar to some formula(e) of interest.
7.2 Comparison with the Old 3CNF  Method
On the whole, the new method inherits all the features of the old 3CNF  method.


Scalability: Increasing ,  (and also the average clause length in  ) the difficulty of the generated
problems scales up at will. Thus it is possible to compare how the performance of different
systems scale up with problems of increasing difficulty, for each source of difficulty (e.g.,
size, depth, etc.).
Valid vs. not-valid balance: The parameter  allows for tuning the satisfiability rate of the formula
at will. Moreover, it is always possible to choose  to generate testbeds with about a 50%satisfiable rate, which allows for the maximum uncertainty.
379

fiPATEL -S CHNEIDER & S EBASTIANI

Termination: The new method allows for generating test sets of up to depth 3-4 which are run by
state-of-the-art systems in a reasonable amount of time.
Reproducibility: The results of each testbed are easy to reproduce because the generators code
and all the parameters values are made publicly available.
Parameterization: The random generation of CNF  formulae is fully parametric.
Data organization: The most natural way to use the new random generator is to generate tests and
plot data by increasing values of one or two parameters. This allows for easy, quantitative and
qualitative evaluations of the performances of the different procedures under test.
Moreover, the new method improves the 3CNF  method for the following features.
Representativeness: As stated in Section 6, CNF  formulae represent all formulae in the normal
upward, as there is an equivalence-preserving way of converting all
modal logics from
modal formulae into CNF  . From Theorem 6.1, the new method allows for a very finegrained sampling of the class of CNF  formulae.

% 







Difficulty: The random CNF  formulae with 
and
provide challenging test sets for

and
can be well considered
state-of-the art procedures. CNF  formulae with 
as challenges for next-generation systems. (Of course, it is not a problem to generate easy
problems too.)


Control: The parameters ,  and  allow for controlling monotonically the difficulty of the test

set. (E.g., if you increase , you are reasonably sure that your mean/median CPU time plots
will increase.) The parameter  allows for controlling the satisfiability rate. Monotonicity
allows for controlling one feature by simply increasing or decreasing one value, and thus for
eliminating uninteresting areas of the input space.
Modal vs. propositional balance: The size of the Kripke models spanned by the decision procedures has increased exponentially with the higher modal depths reached by the new test sets;
moreover, the probability of repeated top-level atoms has dramatically reduced. 11 Consequently, unlike the tests by Hustadt and Schmidt (1999) and Giunchiglia et al. (2000) the
search is no longer dominated by the pure propositional component of reasoning, and the
empirical results show that a large number of modal successors are explored.
Finally, the new method completely removes or drastically reduces the effects of the following
problems.
Redundancy: Propositional and modal redundancy had already been eliminated in the last versions
of the 3CNF  method (Giunchiglia et al., 2000). Moreover, the new method allows for
eliminating all strictly propositional clauses.
Triviality: The main cause of trivial unsatisfiability has been removed, so that trivially unsatisfiable
formulae have been relegated out of the transition areas in our experiments.
11. The number of possible distinct modal atoms increases hyper-exponentially with

380



(Horrocks et al., 2000).

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Artificiality: Our method allows the user to shape the test formulae so that to maximize the resemblance to the expected typical inputs of his/her system(s). Of course, this is done within
the limits imposed by randomness: the more irregular the typical input formulas, the higher
the variance of the randomly generated formulas, the lower their average resemblance to the
typical input formulas.
Over-size: The new method allows for generating extremely hard problems with reasonable size.
It comes from the analysis of the resulting data that hard problems require very big amounts
of both search branches and modal successors generated, so that the search is not dominated
by parsing and data managing.
The generator presented by Horrocks and Patel-Schneider (2002), extends the 3CNF  generator of Giunchiglia et al. (2000) too. However, our new generator allows for shaping the probability
distributions of both C and p, and for using different distributions at every depth level. In principle,
the generator of Horrocks and Patel-Schneider (2002) allows also for setting the probabilities & '
#
and &
by which propositional and modal atoms are negated. However, this feature is not used
#
very muchin the experiments by Horrocks and Patel-Schneider (2002) & ' is always 243 and &
is different from 243 only in one experiment and adds nothing to the generality of the generator,
so that in our new generator we decided not to re-introduce it.
7.3 Comparison with the QBF-based Method
Before comparing our new CNF  generation method with the QBF-based generation method, we
must notice that, so far, they have been used in different ways, corresponding to the two different
test techniques briefly summarized in Section 3.






In the TANCS competition(s) (Massacci, 1999), the tests have been performed on single
data points, and the results are presented in the form of big tables, each entry consisting
of the number of successful solutions and in the rescaled geometrical mean CPU time for
such solutions. Two or more systems are compared according to their number of successful
solutions, considering the geometrical mean CPU time value only when the result is even.
This is due to the fact that a comparison between geometrical means is possible only if they
are computed on the same number of successful values, or, for a more accurate comparison,
on the same successful values.12 This method was chosen to guarantee the fairness of the
comparison between the competitors, which is the key requirement in a competition.
In this paper instead, we have focused on highlighting both the qualitative and quantitative
behavior of the system(s). Thus we have preferred plots to tables, and we have preferred
representing percentiles CPU times rather than the number of successful solutions and their
geometrical mean times. In fact, the former does not require to distinguish between successful
and non-successful solutions.13 Thus, they are much more suitable for plotting, because
a comparison on geometrical means makes sense only for those data points with the same
number of successful solutions, which is very hard to follow in a plot.

12. In case of tests exceeding the timeout, geometrical means are altered by the truncation introduced by the unsuccessful
solutions. Thus the geometrical mean makes sense only if calculated only on successful results.
13. If the percentage of successful solutions is greater or equal than , then the value -th percentile is not influenced
by the truncation of values introduced by timeouts, otherwise it is equal to the timeout value.

381

fiPATEL -S CHNEIDER & S EBASTIANI

Of course, both generators can be used in both ways. (See Heguiabehere and de Rijke (2001)
for some plots with the random QBF-based method.) Comparing the two approaches above in
organizing and presenting data is not one of the goals of this paper, so we restrict our analysis to the
generation methods, independently from how they have been used so far.
The QBF-based generation method of Massacci (1999) shares with our new CNF  generation
method several features in particular Scalability, Valid vs. not-valid balance, Termination, Reproducibility, Parameterization, Data Organization, Difficulty, Modal vs. propositional balance, Redundancy and Triviality for which considerations which are identical or analogous to

those for our new method hold, once we consider parameters , and  instead of parameters ,
 and  . The following features instead deserve more discussion.





Control: The parameters and allow for controlling monotonically the difficulty of the test set.
The parameter  allows for controlling the satisfiability rate. However, unlike the CNF 
case, the main parameters of the QBF generator (e.g., and ) do not have a direct meaning
wrt. the main characteristics of the resulting modal formulae like, e.g., the modal depth and
the number of propositional variables.



Representativeness: In general QBF formulae are good representatives for the whole class of
quantified boolean formulae, as there is a way to convert a generic quantified boolean formula
into QBF.14 (The randomly generated QBF formulae used by Massacci (1999) restrict to
those having a fixed amount of variables per alternation.) Nevertheless, the class of modalencoded QBF formulae restrict to those having candidate Kripke structures with the very
regular structure imposed by the QBF and/or binary search trees.



Artificiality: Unlike the CNF  case, the main parameters of the QBF generator (e.g., and )
do not have a direct meaning wrt. the main characteristics of the resulting modal formulae.
Thus, it is hard to choose the parameters for the random QBF generator so that to resemble
expected typical inputs of the system(s).
Over-size: One final problem with random modal-encoded QBF formulae is size. Initial versions
of the translation method produced test sets in the 1GB range, which stressed too much the
data-storage and retrieval portion of the provers. (For example, running DLP on these formulae resulted in a 1000s timeout without any significant search.) Although the encoding has
been significantly improved in this sense, the current versions still produce very large modal
formulae, mostly to constrain the Kripke structures.
Similar considerations have been very recently presented by Heguiabehere and de Rijke (2001).
On the whole, we believe that the QBF generation method is still appealing, and that the two
methods can co-exist in any empirical test session.
14. Notice that by QBF here we denote the class of prenex CNF QBF formulae, given by an alternation of quantification
variables ending with an existential one followed by a CNF propositional formula. The conversion works by lifting
quantifiers outside the formula and then converting into k-CNF [k-DNF] the matrix if the innest quantifier is an [a ,
negating the result and pushing down the negation recursively]. The conversion is truth-preserving [truth-inverting].


382

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

7.4 Complexity Issues
From a purely theoretical viewpoint, it is remarked that modal-encoded QBF formulae can capture
the problems in  , while CNF  formulae are stuck at NP (Massacci, 1999) 15 . This statement
requires some clarification.
First, test sets are necessarily finite, therefore it makes no sense to attribute to them a complexity
class. Thus, when speaking of complexity classes for test problems, we do not refer to test sets, but
rather to the infinite sets of formulae we could generate if we could have unbounded values for (at
least one of) the generation parameters. In particular, the statement above means that the infinite set
of QBF formulae with unbounded number of variables per alternation and bounded alternation
depth is complete for  (Garey & Johnson, 1979), while the infinite set of CNF  formulae
with bounded depth and unbounded number of propositional variables is in NP (Halpern, 1995).
Secondly, the alternation depth and the variable number per alternation are not the QBFanalogous of
s modal depth and variable number respectively, as both the latter values for
the resulting modal formulae grow as
. 16 In fact, QBF formulae with bounded alternation
depth and unbounded number of variables per alternation give rise to modal formulae of both
unbounded depth and unbounded number of variables.
Finally, the   vs. NP issue of Massacci (1999) is not a matter of generators, but rather a
matter of how such generators are used, and of how results are organized and presented. In fact,
so far random CNF  testbeds have always been organized by fixing all the parameters except 
(modal depth  included!) and making  vary. This choice, whose goal is to produce data plots
covering the satisfiability transition area, is what causes the testbed formulae to be stuck at NP.
To avoid this fact, one may want to make  vary and to fix all the other parameters, as
satisfiability with unbounded depth and bounded number of propositional variables is PSPACEcomplete (Halpern, 1995).



% 



&  )"



%

7.5 Asymptotic Behavior
Achlioptas et al. (1997) presented a study on the asymptotic behavior of random CSP problems.
They showed that, for most well-known random generation models (which did not reveal flaws in

empirical tests) the probability that problems are trivially unsatisfiable tends to 1 with
	 ,

being the number of variables. Gent et al. (2001) lately explained this discrepancy between
theoretical and empirical results by showing that the above phenomenon happens with significant

probability only for values of which are out of the reach of current CSP solvers.
The problem is due to the possible presence of (implicit) unary constraints causing some variables value to be inadmissible. If this occurs with some non-zero probability, then with non-zero
probability some variable may have all its values inadmissible. This causes a local inconsistency


of the whole problem, which is very easily revealed by the solver. When
fffi , the probability
of not having such situation tends to zero. Analogous problems have been revealed with random
SAT problems generated with the constant probability generation model, as unary clauses are gen-



15. More precisely, Massacci (1999) referred to the 3CNF  formulae of Giunchiglia et al. (2000). The statement holds
also for all the CNF  formulae.
16. As we have already noticed (Horrocks et al., 2000), a better QBF-analogous of the modal depth is the total number
of universally quantified variables ( 
in our case). In fact, like modal
with bounded depth,
the class of QBF formulae with bounded is only complete in NP, as it is possible to guess a tree-like witness
nodes.
with



   

   #


383

 	

fiPATEL -S CHNEIDER & S EBASTIANI

erated with non-zero probability (Mitchell et al., 1992), and with random QBF problems, as implicit
unit clauses, i.e., clauses containing only one existential variable are generated with non-zero
probability (Gent & Walsh, 1999). For the random k-SAT model,
, such problem does not
occur (Friedgut, 1998; Achlioptas et al., 1997).





Our generation model is far more complicated to analyze than the models above. First, CNF 
formulas have a much more complicated structure than random SAT, CSP and QBF formulas, involving a much wider number of parameters. Second, unlike with the models discussed above, the
(constraints described by) CNF  clauses are not picked in a uniform way, as the probability of
 !
"
generating a given CNF  atom
varies strongly with its depth and shape, and it is typically
much smaller than that of generating a propositional atom fi .17 Thus, developing a formal probabilistic analysis for the asymptotic behavior of our model is out of the reach (and of the scope) of
this paper. However, we provide here some heuristic considerations.
The simplest case is when we do not allow the generation of unary clauses at top level, that is,
when C = [[0,...],...], so that we do not have explicit unary constraints. We may still have
 !
 !

 !

 !
 "
 "
  "
 fi  
or

.
implicit unary constraints like, e.g., fi
Anyway, a simple heuristic consideration suggests that, given the big numbers of distinct CNF 
modal atoms which may potentially be generated, such situations are more unlikely than that of
 fi 	  fi   fi 	 in the standard 2-SAT model, which is
having implicit unit constraints like fi
free from the asymptotic local inconsistency problem.
A more critical case is when we allow for the generation of unary clauses at top level, that
is, when C = [[x,...],...],    . In this case we can generate unary clauses, and thus
local inconsistencies, with non-zero probability. Thus, a simple way to avoid this problem is to
restrict the values of  so that not to allow unary top-level clauses, that is, to always set C =
[[0,...],...]. Notice, however, that this hardly becomes a problem in practice if we respect the condition described in Sections 4.1 and 5.1 of avoiding purely propositional top-level
clauses (that is, always set p = [[...,0],...]). In fact, given the big numbers of distinct
CNF  modal atoms which may potentially be generated, the probability of having two contradic !
 !
tory modal unit clauses
,
within the same formula becomes quickly negligible even with
small depths.
Notice that here we have intentionally not considered modal implicit unary constraints like,
 !

 fi 
 ,
e.g., fi
, and being mutually inconsistent modal literals (e.g., /
 !

/


). In fact, detecting such inconsistencies requires investigating recursively the
modal successors, and therefore it is not trivial.



" 







" 
  "
$ $ "

"  

" 

"  

"

"



$

8. Conclusions and Future Work
As shown by the test sets above our new method, in its basic form, allows us to generate a wider
variety of problems covering more of the input space. We can better-tune the difficulty of problems
for various parameter values, including the first reasonable test sets for maximum modal depths
of and 7 . We can produce interesting scaling dimensions, varying more than just the number of

propositional variables . For example, we can now vary the propositional probability  or the
size of clauses  to vary the difficulty of interesting problems. As neither  nor  are restricted to
integral values, we have extremely fine control over the difficulty of test sets. Thus we can create





17. Again, we recall that the number of possible distinct CNF 
depth (Horrocks et al., 2000).



384

atoms increases hyper-exponentially with the modal

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

more interesting test sets where the satisfiable/unsatisfiable transition is explorable with current
decision procedures.
We have drastically reduced the influence of trivial unsatisfiability, which flawed the previous
CNF  methodologies when 0% . We retain the desirable features of the previous CNF  methodologies. Our test sets are easy to reproduce and are not too large.
In our full methodology we have introduced the possibility of shaping the distribution of both
the size and the propositional/modal rate of the clauses. This can be done at each level of modal
depth. This allows for generating a much wider variety of problems, covering in principle the whole

input space. For instance, we have produced a full test set with  / 3 and /93 (Figure 15).
We have not moved closer to application data, as there are no significant direct applications
of modal decision procedures and thus no guidance for the sorts of inputs that would be close to
application inputs. In any case, we believe we have moved closer than ever to the possibility of
approximating given classes of input formulae.
There is still much work to be done using our generation methodology. We can produce more
test sets and try these test sets out on various modal decision procedures. We may also want to
uncover parameter settings where the full generality of our generation method is needed to produce
reasonable test sets.

Acknowledgments
We would like to thank Thomas Eiter and the three anonymous reviewers for their valuable comments and helpful suggestions which greatly improved the quality of the paper. The second author
is supported by a MIUR COFIN02 project, code 2002097822 003, and by the C ALCULEMUS !
IHP-RTN EC project, contract code HPRN-CT-2000-00102, and has thus benefited of the financial contribution of the Commission through the IHP programme.

Appendix A: Fully-detailed Proof of Theorem 6.1

$

Theorem 6.1 Let rnd CNF  be a purely random generator as in Figure 13. Let be a sorted
CNF  formula of depth  and with  top-level clauses built on all the propositional atoms in

 #
fffi    fi  and on all the modal boxes in ff
  
 , which has no repeated clause at top level
and no repeated atoms inside any clause at any level. Let  and  be built from so that to verify
 of Section 6. Let   and   be obtained from  and  respectively by substituting
points  and
some zero-values with some non-zero values. Then we have:

 "

$

"

$

(i) rnd CNF  (d,m,L,N,p,C) returns with some non-zero probability  ;
(ii) rnd CNF  (d,m,L,N,p,C) returns with some non-zero probability 

$

$

 

.

Proof The proof works by induction on the structure of . First, we prove that:

$

1. every propositional atom occurring in at some depth  is returned with the same non-zero
probability   by both rnd atom(0,m,N,p,C) and rnd atom(0,m,N,p,C);

 
2. every modal atom  occurring in at some depth  is returned with some non-zero probability  by rnd atom(d-i,m,N,p,C), and is returned with some non-zero probability   
by rnd atom(d-i,m,N,p,C);




$




385




fiPATEL -S CHNEIDER & S EBASTIANI

$

3. every clause  occurring in at some depth  is returned with some non-zero probability
 by rnd clause(d-i,m,N,p,C), and is returned with some non-zero probability    by
rnd clause(d-i,m,N,p,C).







From point 3. we have that every top level clause  	 is returned by rnd clause(d,m,N,p,C) and
 	 . As
rnd clause(d,m,N,p,C) with some probabilities  	 and   	 respectively, being   	
has no repeated clause, recalling a property of probabilities we have:

$




  
   
 
   
 "  
 
 
     "     " 
      	    "    



		


	     "  








"

:





/



8



 8



 

 8



/

/



	





ff







 	 	




"




(7)

 

8

"
(8)










(9)


Notice that (8) is strictly monotonic in all its components. Thus, 
Now we need to prove points 1, 2 and 3.

 

.

+

$

1. Let fi 	 be a propositional atom in fffi    fi  occurring in at depth  , for some 
 .
Then both rnd atom(0,m,N,p,C) and rnd atom(0,m,N,p,C) invoke rnd propositional atom(N),

which returns fi 	 with probability  / ; .
  
2. Let
be a boxed clause occurring in at depth  , for some  9 and : . Then the
  

occurs in at depth  : . (Notice that <% instead of  	 :
cannot occur
clause
in at depth  , because  is the maximum depth of .)

$

$



$

$

(i) By inductive hypothesis, it follows from point 3. that  is returned with some non-zero
probability  by rnd clause(d-i-1,m,N,p,C). As   , rnd atom(d-i,m,N,p,C) invokes
  
with the non-zero probrand box(m) rand clause(d-i-1,m,N,p,C), which returns
/;
 .
ability 
(ii) By inductive hypothesis, it follows from point 3. that  is returned with some nonzero probability  
 by rnd clause(d-i-1,m,N,p,C). rnd atom(d-i,m,N,p,C)
in  
vokes rand box(m) rand clause(d-i-1,m,N,p,C), which returns
with the non  . Thus,    .
zero probability   /;


 



 
 



 

 

 propositional literals, which occurs in $ at depth
3. Let  be a clause with length and 5
 , for some  . As $ is sorted,  is represented as          	 " ,
	 denote modal literals.
where 
ff denote propositional literals and
(i) By inductive hypothesis, it follows from point 1. that each propositional literal  	 is
  fi 	 by rnd sign()  rnd atom(0,m,N,p,C),
returned with some non-zero probability
and it follows from point 2. that each modal literals
is returned with the non-zero
  
 fi  by rnd sign()  rnd atom(d-i,m,N,p,C).
probability


<   



%
!

   

243

243

386



!



  

!



  

!

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE





By construction of  , the -th element of the  -th sublist in  is non-zero; thus, is
returned with some non-zero probability  	 by rnd length(d-i,C).

By construction of  , the   -the element of the -th sub-sublist of the  -th sublist in  is
!
non-zero; thus,  is returned with some non-zero probability  	 by rnd propnum(d,p,j).18
Similarly to (9), has no repeated atoms inside any clause, so that  is returned by
rnd clause(d-i,m,N,p,C) with the non-zero probability

$





 	   ! 	
/

	



 " 	
243

!

 fi 	







	


   

	



!



 

fi

















 fi

 fi

  



(10)

As with (9), the expression on the right in (10) is strictly monotonic in all its terms  	 ,
 ! 	 ,  fi 	 s,  fi  s within the domain of definition.
(ii) By inductive hypothesis, it follows from point 1. that each propositional literal 	
243
 fi 	 by rnd sign()
is returned with some non-zero probability 243   fi 	

rnd atom(0,m,N,p,C), and it follows from point 2. that each modal literals
is re


turned with some non-zero probability 243 
fi %243  fi by rnd sign() rnd atom(di,m,N,p,C).

By construction of  and   , the -th element of the  -th sublist in   is non-zero; thus,
 is returned with some non-zero probability   	 by rnd length(d-i,C). By construction
of   from  ,   	 fi 	 .

By construction of  and   , the   -the element of the -th sub-sublist of the  -th
!
sublist in  is non-zero; thus,  is returned with some non-zero probability   	 by

!

!
rnd propnum(d,p,j). By construction of   from  ,   	  	 .
As has no repeated atoms inside any clause, it follows that  is returned by rnd clause(di,m,N,p,C) with the non-zero probability






 


 










$

 
 fi


/
  243 "  	   
     
      
 fi  


 
Because of the strict monotonicity of (10) and (11), we have that    .
 

  	    ! 	

	



!

  fi 	

	


     fi



	



!

(11)

Q.E.D.

References
Achlioptas, D., Kirousis, L. M., Kranakis, E., Krizanc, D., Molloy, M. S. O., & Stamatiou, Y. C.
(1997). Random constraint satisfaction: A more accurate picture. In Smolka, G. (Ed.), Principles and Practice of Constraint Programming, Vol. 1330 of Lecture Notes in Computer
Science, pp. 107120, Berlin. Springer.
Beckert, B., & Gore, R. (1997). Free variable tableaux for propositional modal logics. In Automated
Reasoning with Analytic Tableaux and Related Methods: International Conference Tableaux97, Vol. 1227 of Lecture Notes in Artificial Intelligence, pp. 91106, Berlin. Springer.

ff



18. Notice that   is a conditioned probability, that is, the probability of having  propositional literal provided the
clause has  literals. This matches the fact that  is an input in rnd propnum(d,p,j).

387

fiPATEL -S CHNEIDER & S EBASTIANI

Cadoli, M., Giovanardi, A., & Schaerf, M. (1998). An algorithm to evaluate quantified Boolean
formulae. In Proceedings of the 15th National Conference on Artificial Intelligence (AAAI98), pp. 262267, Menlo Park, CA. AAAI Press.
Friedgut, E. (1998). Sharp thresholds of graph properties, and the k-sat problem. Journal of the
American Mathematical Society, 12(4), 10171054.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory of
NP-Completeness. W. H. Freeman, New York.
Gent, I. P., MacIntyre, E., Prosser, P., Smith, B. M., & Walsh, T. (2001). Random constraint satisfaction: Flaws and structure. Journal of Constraints, 6(4), 345372.
Gent, I. P., & Walsh, T. (1999). Beyond NP: The QSAT phase transition. In Proceedings of the
Sixteenth National Conference on Artificial Intelligence and the Eleventh Innovative Applications of Artificial Intelligence Conference (AAAI-99), pp. 648653, Menlo Park, CA. AAAI
Press.
Giunchiglia, E., Giunchiglia, F., Sebastiani, R., & Tacchella, A. (2000). SAT vs. Translation based
decision procedures for modal logics: a comparative evaluation. Journal of Applied NonClassical Logics, 10(2), 145172.
Giunchiglia, E., Giunchiglia, F., & Tacchella, A. (2002). SAT based decision procedures for classical
modal logics. Journal of Automated Reasoning, 28, 143171.
Giunchiglia, F., & Sebastiani, R. (1996). Building decision procedures for modal logics from propositional decision procedures - the case study of modal K. In Proceedings of the Thirteenth
Conference on Automated Deduction, Vol. 1104 of Lecture Notes in Artificial Intelligence,
pp. 583597, Berlin. Springer.
Giunchiglia, F., & Sebastiani, R. (2000). Building decision procedures for modal logics from propositional decision procedures - the case study of modal K(m). Information and Computation,
162(1/2), 158178.
Haarslev, V., & Moller, R. (2001). RACER system description. In Proceedings of the International Joint Conference on Automated Reasoning, IJCAR2001, Vol. 2083 of Lecture Notes
in Computer Science, pp. 701705, Siena, Italy. Springer.
Halpern, J. Y. (1995). The effect of bounding the number of primitive propositions and the depth of
nesting on the complexity of modal logic. Artificial Intelligence, 75(3), 361372.
Halpern, J. Y., & Moses, Y. (1992). A guide to the completeness and complexity for modal logics
of knowledge and belief. Artificial Intelligence, 54(3), 319379.
Heguiabehere, J., & de Rijke, M. (2001). The random modal QBF test set. In IJCAR2001 Workshop
on Issues in the Design and Experimental Evaluation of System for Modal and Temporal
Logics, pp. 5867.
Heuerding, A., Jager, G., Schwendimann, S., & Seyfreid, M. (1995). Propositional logics on the
computer. In Baumgartner, P., Hahnle, R., & Posegga, J. (Eds.), Automated Reasoning with
Analytic Tableaux and Related Methods: International Conference Tableaux95, Vol. 918 of
Lecture Notes in Artificial Intelligence, pp. 310323, Berlin. Springer.
Heuerding, A., & Schwendimann, S. (1996). A benchmark method for the propositional modal
logics K, KT, S4.. Tech. rep. IAM-96-015, University of Bern, Switzerland.
388

fiA N EW G ENERAL M ETHOD

TO

G ENERATE R ANDOM M ODAL F ORMULAE

Horrocks, I. (1998). Using an expressive description logic: FaCT or fiction?. In Cohn, A. G., Schubert, L., & Shapiro, S. C. (Eds.), Principles of Knowledge Representation and Reasoning:
Proceedings of the Sixth International Conference (KR98), pp. 636647. Morgan Kaufmann
Publishers, San Francisco, California.

% 

Horrocks, I., & Patel-Schneider, P. F. (2002). Evaluating optimised decision procedures for propositional modal
satisfiability. Journal of Automated Reasoning, 28(2), 173204.
Horrocks, I., Patel-Schneider, P. F., & Sebastiani, R. (2000). An analysis of empirical testing for
modal decision procedures. Logic Journal of the IGPL, 8(3), 293323.
Hustadt, U., & Schmidt, R. A. (1999). An empirical analysis of modal theorem provers. Journal of
Applied Non-Classical Logics, 9(4), 479522.
Hustadt, U., Schmidt, R. A., & Weidenbach, C. (1999). MSPASS: Subsumption testing with SPASS.
In Lambrix, P., Borgida, A., Lenzerini, M., Moller, R., & Patel-Schneider, P. (Eds.), Proceedings of the 1999 International Workshop on Description Logics (DL99)., pp. 136137.
Massacci, F. (1999). Design and results of Tableaux-99 non-classical (modal) system competition.
In Automated Reasoning with Analytic Tableaux and Related Methods: International Conference Tableaux99, Vol. 1617 of Lecture Notes in Artificial Intelligence, pp. 1418, Berlin.
Springer.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of SAT problems.
In Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 459465, San
Jose, California. American Association for Artificial Intelligence.
Patel-Schneider, P. F. (1998). DLP system description. In Franconi, E., Giacomo, G. D., MacGregor, R. M., Nutt, W., Welty, C. A., & Sebastiani, F. (Eds.), Collected Papers from the International Description Logics Workshop (DL98), pp. 8789. Available as CEUR-WS/Vol-11
from http://SunSITE.Informatik.RWTH-Aachen.DE/Publications/CEUR-WS.
Patel-Schneider, P. F., & Sebastiani, R. (2001). A new system and methodology for generating random modal formulae. In Proceedings of the International Joint Conference on Automated
Reasoning, IJCAR2001, Vol. 2083 of Lecture Notes in Computer Science, pp. 464468,
Siena, Italy. Springer.
Pitt, J., & Cunningham, J. (1996). Distributed modal theorem proving with KE. In Minglioli, P.,
Moscato, U., Mindici, D., & Ornaghi, M. (Eds.), Automated Reasoning with Analytic Tableaux and Related Methods: International Conference Tableaux96, Vol. 1071 of Lecture
Notes in Artificial Intelligence, pp. 160176, Berlin. Springer.
Tacchella, A. (1999). *SAT system description. In Lambrix, P., Borgida, A., Lenzerini, M., Moller,
R., & Patel-Schneider, P. (Eds.), Proceedings of the 1999 International Workshop on Description Logics (DL99)., pp. 142144.

389

fiJournal of Artificial Intelligence Research 18 (2003) 217-261

Submitted 8/02; published 3/03

Interactive Execution Monitoring of Agent Teams
David E. Wilkins
Thomas J. Lee
Pauline Berry

WILKINS @ AI . SRI . COM
TOMLEE @ AI . SRI . COM
BERRY @ AI . SRI . COM

Artificial Intelligence Center, SRI International
333 Ravenswood Ave., Menlo Park, CA 94025 USA

Abstract
There is an increasing need for automated support for humans monitoring the activity of
distributed teams of cooperating agents, both human and machine. We characterize the domainindependent challenges posed by this problem, and describe how properties of domains influence
the challenges and their solutions. We will concentrate on dynamic, data-rich domains where humans are ultimately responsible for team behavior. Thus, the automated aid should interactively
support effective and timely decision making by the human. We present a domain-independent
categorization of the types of alerts a plan-based monitoring system might issue to a user, where
each type generally requires different monitoring techniques. We describe a monitoring framework
for integrating many domain-specific and task-specific monitoring techniques and then using the
concept of value of an alert to avoid operator overload.
We use this framework to describe an execution monitoring approach we have used to implement Execution Assistants (EAs) in two different dynamic, data-rich, real-world domains to assist
a human in monitoring team behavior. One domain (Army small unit operations) has hundreds
of mobile, geographically distributed agents, a combination of humans, robots, and vehicles. The
other domain (teams of unmanned ground and air vehicles) has a handful of cooperating robots.
Both domains involve unpredictable adversaries in the vicinity. Our approach customizes monitoring behavior for each specific task, plan, and situation, as well as for user preferences. Our
EAs alert the human controller when reported events threaten plan execution or physically threaten
team members. Alerts were generated in a timely manner without inundating the user with too
many alerts (less than 10% of alerts are unwanted, as judged by domain experts).

1. Introduction
As automation and reliable, high-bandwidth communication networks become more common, humans are increasingly responsible for monitoring and controlling the activity of distributed teams of
cooperating agents, both human and machine. Such control decisions in many realistic domains are
complex, and require human experience and judgment. Our vision is that human decision makers
will be able to perform more important tasks than continuously monitoring incoming information
by relying on an automated execution aid to alert them when significant new information warrants
their attention. We are primarily interested in domains requiring human control and will describe
two such domains. However, the majority of our techniques and analysis also apply to completely
automated execution monitoring. In fact, in one of our domains we both interact with a human
controller and autonomously adjust robot behavior and plans.
To rapidly make effective control decisions for distributed agent teams, the human needs automated support, for several reasons. First, inexpensive sensors and reliable, high-bandwidth communication networks provide large volumes of pertinent data arriving from sensors, team members,

c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiW ILKINS , L EE , & B ERRY

and other sources. Without automated support, the human cannot cope with the volume of incoming
information. Second, plans that coordinate the activity of several team members, as many as several
hundred in our first domain, can become too complex to monitor without automated help. Third, we
are addressing domains that are dynamic, sometimes requiring responses in a few seconds or less.
Fourth, the automated team members (robots) are complex, with different failure modes and recovery procedures, and automated support for controlling them is often essential. All these challenges
are magnified as the tempo of the decision cycle increases or the user becomes stressed. Thus,
domains with the above properties require an interactive, automated assistant to support humans in
monitoring incoming information and controlling agent teams.
We will concentrate on dynamic, data-rich domains where humans are ultimately responsible
for team behavior. Realistic domains often have adversaries to overcome. These may range from
fairly benign forces of nature that introduce uncertainty, to intelligent adversaries that are trying to
actively thwart plans. An automated execution assistant should interactively support effective and
timely decision making by the human, and interact with the human to take advantage of knowledge
the human possesses that is not explicitly modeled in the machine. Ideally, an execution assistant
would allow its human user to, among other things:
 Guide the system with minimal effort
 Focus on external events, assuming the system will alert the user when human attention is
desirable
 Understand, evaluate, and modify the plans/actions
 Understand why and why not for each action or decision taken/recommended/rejected by the
system
 Have constant multimodal feedback
 Recommend actions and decisions that violate constraints when warranted
One key idea is that rich plan representations allow the execution aid to share context with users,
so both understand the semantics of plans and requests. Understanding the plan is the key to helping
the user deal with the possible information glut created by advanced information systems. The
execution aid uses the plan to filter, interpret, and react to the large volume of incoming information,
and can alert the user appropriately when events threaten the plan or the users physical existence.
Once the user develops trust in the execution aid, there will be a reduction of the need for human
monitoring of the display of the information system, while simultaneously increasing the amount
of relevant information monitored because the aid analyzes every piece of incoming data. Relying
on alerts from an automated aid allows the human to pay attention to more important tasks than
monitoring incoming data, attending to the display only when alerted by the execution aid.
In the next section, we characterize the domain-independent challenges posed by this problem,
concentrating those that are unique to interactive execution aids in dynamic domains with distributed
teams of cooperating agents. Then, we describe how properties of various domains influence these
challenges and their solutions. In Section 4, we present a domain-independent categorization of the
types of alerts a plan-based monitoring system might issue to a user. Next, we describe the concept
of value of information and alerts that is key to reducing unwanted alerts (alarms). Sections 6
218

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

and 7 describe the Execution Assistants we implemented in the small unit operations and robotics
domains, respectively. Sections 6.8 and 7.5 contain the results of evaluations performed in each
domain. Finally, we discuss related work and present our conclusions.

2. Interactive Monitoring Challenges
There has been great interest in plan generation algorithms, but less work on using plans to dynamically control execution. Much execution monitoring work describes monitors in specific domains,
so we first characterize the domain-independent challenges of monitoring agent teams.
There are several universal challenges of execution monitoring that are not particular to dynamic, data-rich domains or interactive monitoring. These issues should be part of a monitoring
ontology and are addressed in our EAs, but we do not stress them in our discussion as they are
discussed elsewhere (Kaminka, Pynadath, & Tambe, 2001; Jonsson, Morris, Muscettola, & Rajan,
2000; Muscettola, Nayak, Pell, & Williams, 1998; Myers, 1999; Wilkins, Myers, Lowrance, &
Wesley, 1995; Coiera, 1993; Durfee, Huber, Kurnow, & Lee, 1997). The issues include the following:
 Sensitivity of the monitor  its ability to detect problems or meet requirements. The system
must remain reactive to incoming data while performing monitoring tasks.
 Temporal reasoning and temporal sensitivity. Execution takes place over time and plans specify future actions, thus making temporal reasoning central.
 Concurrent temporal processes. Multiple tasks or agents may be executing concurrently.
 Synchronization between agents. An execution assistant must get the right information to the
right team members at the right time to support the cooperative activity specified in the plan.
In some domains, this may require doing plan recognition on other team members (Kaminka
et al., 2001).
 False and redundant alarms. Unwanted alarms are ubiquitous in data-rich domains such as
medicine (Koski, Makivirta, Sukuvaara, & Kari, 1990; Tsien, 1997) and the domains described in this paper.
 Combining event-driven and goal-driven behavior. The execution assistant must respond to
unfolding events with acceptable latency while concurrently invoking actions that will continue execution of the (perhaps modified) plan and satisfy user requests. Goal-driven tasks
include responses to events, such as generating modified, new, or contingency plans, and
invoking standard operating procedures.
 Adversarial reasoning, including plan and pattern recognition. Many real-world domains have
adversaries and their activity must be closely monitored.
We are concerned with execution monitoring of agent teams, where team members may be
any combination of humans and/or machines. We concentrate on the challenges that are unique to
interactive execution aids in dynamic domains, and categorize these challenges into the following
four categories.
Adaptivity. The output of an execution assistant must meet human requirements and preferences for monitoring behavior, providing high-value alerts and suggestions. As in all execution
219

fiW ILKINS , L EE , & B ERRY

monitoring, sensitivity is crucial, but in interactive monitoring the sensitivity of the monitor must
also be adaptable. In addition to adapting to user preferences, the analysis done by an execution
assistant and its level of autonomy must be adjustable to operational tempo and incoming data rate.
The system should ideally adapt its output to the users capabilities and cognitive load.
Plan and situation-specific monitoring. Coordinating the activities of many teams members
requires a plan shared by the team. We will assume that plans contain partial orders of tasks for
each team member, as well as any necessary coordinating instructions and commitments (Grosz &
Kraus, 1999). The plan representation also encodes some of the expected outcomes (effects) of plan
execution, so that execution aids can detect deviations. The analysis done by an execution assistant
and any suggested responses must depend on the plan and situation to be effective, because events
often cause a problem for some plans but not for others. We found that monitoring algorithms
must often be tailored to the specific tasks that compose plans. To facilitate interaction, the plan
representations must be understandable by both humans and the system, although the human might
be aided by multiple plan views of the internal representation in a user-friendly interface.
Reactivity. Any execution monitor must react to events and uncertainty introduced by the environment. In dynamic, data-rich domains, particular care must be taken to ensure that the system
remains reactive with high rates of incoming information and fast decision cycles. Resources are
not generally available to perform all desired analyses for every input  for example, projecting
future problems with multiple simulation runs or searching for better plans may be computationally
expensive. There are often no obvious boundaries to the types of support an execution aid might
provide in a real-world domain. Therefore, a balance must be struck between the capabilities provided and resources used. A few examples show the types of issues that arise in practice. In our
first domain, only coarse terrain reasoning was used, as projections using fine-grained terrain data
were computationally expensive. In our robot domain, we had to adjust the time quanta assigned
to processes by the scheduler so that our monitoring processes were executed at least every second. Finally, in domains with dangerous or intelligent adversaries, reacting to their detected activity
becomes a high priority. There has been considerable research on guaranteeing real-time response
(Ash, Gold, Seiver, & Hayes-Roth, 1993; Mouaddib & Zilberstein, 1995), but the tradeoffs are generally different in every application and are usually a critical aspect of the design of an execution
assistant.
High-value, user-appropriate alerts. Alerting on every occurrence of a monitored condition
that is possibly a problem is relatively easy; however, the user would quickly ignore any assistant
that gave so many alerts. The challenge is to not give false alarms and to not inundate the user with
unwanted or redundant alerts. The system must estimate the utility of information and alerts to the
user, give only high-value alerts, and present the alerts in a manner appropriate to their value and the
users cognitive state. We found that a common challenge is to avoid cascading alerts as events get
progressively further away from expectations along any of a number of dimensions (such as time,
space, and resource availability). Another challenge that we will not discuss in depth is aggregating
lower-level data (e.g., sensor fusion), which can reduce the number of alerts by consolidating inputs.
Estimates of the value of alerts can be used to adjust alerting behavior to the users cognitive load.
Interactive alerting during execution naturally leads to the equally important and challenging
topic of human directing of responses and plan modifications. Our monitoring technologies have
been used in continuous planning frameworks (Wilkins et al., 1995; Myers, 1999), but we will limit
the scope of this paper to interactive alerting. We briefly mention some ongoing research on this
topic that we either are using or plan to use in conjunction with our execution aids.
220

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

Agent systems that interact with humans are an active area of research, and the issues are discussed in the literature (Myers & Morley, 2001; Ferguson & Allen, 1998; Schreckenghost & et al.,
2001). Myers and Morley (2001), for example, describe the Taskable Reactive Agent Communities
(TRAC) framework that supports human supervisors in directing agent teams. They address topics
such as adjustable agent autonomy, permission requirements, consultation requirements, and the
ability to communicate strategy preferences as guidance. TRAC is complementary to the execution
monitoring described in this paper.
Another active research area that fits naturally with our execution monitoring approach is theories of collaboration. In fact, we use the SharedPlans theory of collaboration (Grosz & Kraus, 1999)
in our second domain (Ortiz & Hsu, 2002) to direct agents in conjunction with the execution monitor. This theory models the elements of working together in a team as well as the levels of partial
information associated with states of an evolving shared plan. Central to the theory of SharedPlans
is the notion that agents should be committed to providing helpful support to team members. Within
the theory, this notion of helpful behavior has been formally defined (Ortiz, 1999). The work on
collaboration is complimentary with our monitoring approach, but will not be discussed in detail.

3. Monitoring Approach Determined by Domain Features
The domain features and monitoring challenges with which we are concerned are common in many
domains in addition to robot teams and small unit operations (SUO). For example, they occur in
the monitoring of spacecraft (Bonasso, Kortenkamp, & Whitney, 1997; Muscettola et al., 1998) and
monitoring in medicine (Coiera, 1993) for ICU patients or for anesthesia. These domains are also
data rich  medical clinicians have difficulty in using the vast amount of information that can
be presented to them on current monitoring systems (Weigner & Englund, 1990; Coiera, 1993).
In particular, the problem of flooding human users with false or redundant alarms is ubiquitous in
medical monitoring (Koski et al., 1990; Tsien, 1997). One study found that 86% of alarms in a
pediatric ICU were false alarms (Tsien & Fackler, 1997). False alarms distract humans from more
important tasks. Such a false alarm rate would most likely make the monitor useless in fast-paced
operations. Research in these domains has concentrated on automated monitoring, with little or no
emphasis on interactive monitoring.
While the challenges described in the previous section apply to all interactive, dynamic domains, the properties of individual domains influence their solutions. One brief case study shows
how the features of the communication system and the use of legacy agents can indicate a different
monitoring approach for two similar problems. Kaminka et al. (2001) address a problem similar to ours: many geographically distributed team members with a coordinating plan in a dynamic
environment. They use an approach based on applying plan-recognition techniques to the observable actions of team members, rather than communicating state information among team members,
which they refer to as report-based monitoring.
They list four problems with report-based monitoring (Kaminka et al., 2001): (1) intrusive modifications are required to legacy agents to report state, (2) the necessary state information changes
with the monitoring task, (3) the monitored agents and the communication lines have heavy computational and bandwidth burdens, and (4) it assumes completely reliable and secure communication
between the team members. They say that (1) is their main concern, with (3) being next most
important.

221

fiW ILKINS , L EE , & B ERRY

Plan constraint violated
Policy constraint violated
New opportunity detected
Adversarial activity detected
Constraint violation, opportunity, or adversarial activity projected
Contingency plan suggested
System problem detected
Reporting requirement triggered
Figure 1: Top-level categories in alert ontology.

In both of our domains, we use report-based monitoring. Our agents already report their state or
can easily be modified to do so, for example, by attaching Global Positioning (GPS) devices. Our
monitoring tasks can be performed using the reports already available, although one can imagine
adding further functionality that would change the reporting requirements. In our first domain,
reports are distributed by the Situation Awareness and Information Management (SAIM) system
on a high-bandwidth network. SAIM uses novel peer-to-peer (P2P) dissemination algorithms and
forward fusion of sensor reports, greatly reducing bandwidth requirements. P2P is fault tolerant,
allowing any node to be a server. Dissemination is based on an agents current task, geographic
location, and relationship in the hierarchical organization of team members.
In summary, report-based monitoring works in our domains because we rely less on unmodifiable legacy agents, have more reliable communications, and have enough bandwidth available with
our network and dissemination algorithms. Kaminkas approach provides more automated support,
but we must address the problem of modeling the value of information to the user. If Kaminkas
system was extended to interact with humans, we believe our alert ontology and techniques for
avoiding operator overload would be applicable, whether alerts come from sources based on planrecognition or from reports. Because we rely on humans as being ultimately responsible for team
behavior, we do not require as much state information nor complete reliability in communication.
Unreliable communication will degrade monitoring performance, but the human decision maker
must take missing inputs into account when making a decision. The execution assistant can monitor
communications and alert the human to possible communications problems.

4. Types of Alerts
Alerts are used to focus the users attention on an aspect of the situation that the execution aid has
determined to be of high value. We discuss the problem of determining the value of information
and alerts in later sections, which determines whether and how an alert is presented. An alert may
indicate that a response is required, or may just be informative. Many different types of alerts can
be given, and it is useful to categorize alerts, thus providing the beginning of a reusable, domainindependent ontology for execution monitoring.
Figure 1 shows the top-level categories for alerts that we identified by starting with a superset of the categories we found useful in our two domains and then generalizing them to cover a
broad range of domains. It is assumed that execution is directed by a plan that is shared by the
team. These categories generally require different monitoring techniques and different responses to
222

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

detected problems. For example, adversarial activity could have been a subclass of other relevant
classes, but it requires different monitoring techniques. The friendly location data is precise (within
the error of GPS) and trustworthy, while adversarial data comes from fusion engines running on
data from sensor networks. The adversarial data is highly uncertain, may come at significantly different rates, and generally will have different algorithms for determining the value of information,
as adversarial entities are actively trying to thwart your plan and perhaps are trying to kill you.
The top-level categories in our ontology generally differ along the following dimensions that
are important to monitoring:
 Properties of data sources (such as reliability and uncertainty).
 Rates of incoming data
 Method of acquiring data (such as receiving messages, pulling data from databases, doing
plan recognition)
 Monitoring algorithms, including tradeoff of complexity of analysis with reactivity
 Desired responses to alerts
 Value of information algorithms
The different monitoring techniques for each category are often domain specific, and can even
be task specific in some cases, adapting the monitoring as tasks in the plan are executed. Our
monitoring framework integrates these various techniques and then uses the concept of value of an
alert to control interaction with the user.
We briefly discuss each of the top-level categories. We have not provided the next lower level
of the ontology because the space of possibilities is large, with domain-specific concerns important.
For example, adversarial alerts could include subclasses for fixed or mobile adversaries, for size
and capabilities of the adversarial team, for an alliance or tightly coordinated adversarial team, for
adversarial intent or plan, and so forth. Later in the paper, we describe how alerts given by our
implemented execution assistants (EAs) fit into these categories.
Plan constraints. Plans provide most of the expectations of how execution should proceed, so
this category has the richest set of alerts. A fairly large hierarchical ontology could be produced
to describe different types of alerts on plan constraints. Gil and Blythe (1999) present a domainindependent ontology for representing plans and plan evaluations. Each concept in their evaluation
ontology could be a source of an alert when the evaluation becomes sufficiently important to the
user. Plans in real-world domains are often hierarchical, so constraints from different levels or
layers may be violated. It may be desirable to customize alerts based on the hierarchical level of the
plan constraint in question. To indicate the range of possible alerts in this category, we list a few
common examples:
 A coordinating team member (or the agent) is out of position or late.
 The effects of the agents (or a team members) actions were not achieved as expected.
 A team member has retracted a commitment to perform a certain task, requiring a reallocation
of tasks or resources.
223

fiW ILKINS , L EE , & B ERRY

 Conditions required by the plan are not true when expected.
 Resources used by the plan are not available or degraded.
Policy constraints. Most real-world domains have persistent constraints such as policies or
rules of engagement that must not be violated. While these could be considered as part of the
plan by representing them as maintenance conditions that extend over the entire plan, they are
significantly different in practice and are often monitored by different techniques, because they may
require additional domain knowledge or specialized monitoring algorithms, which must be invoked
efficiently. For example, in our domains, we never want our human team members to be killed or
our robots destroyed. Therefore, we monitor the physical safety of our agents at all times and give
alerts to the user when some agent is in danger. Dangers from adversarial agents are covered in
their own category. However, the system should also alert the user to threats from team members
(fratricide) and from the local agents own actions (e.g., a robots battery running low).
New opportunities. Even though the current plan can still be executed without change, it may
be possible to generate a better plan for the current situation as new opportunities arise. Determining
if an execution-time update to the world state permits a more desirable plan is a difficult problem
in general, similar to generating a new plan for the new situation. However, in real-world domains,
there are often methods for detecting new opportunities that indicate a plan revision might be cost
effective. For example, certain key features (such as pop-up targets in military domains) can
represent new opportunities, and there are often encoded standard operating procedures (SOPs)
that can be invoked when triggered by the current situation to improve the plan and/or react to
events. Because our monitoring is interactive, we can avoid the difficult decision of whether to
search for a better plan by alerting the user of high-value opportunities and relying on the user to
judge the best response.
Adversarial activity. This category assumes that our team members are operating in environments with adversaries that are trying to actively thwart team plans. When adversaries are dangerous
(e.g., worthy human opponents), reacting to their detected activity becomes a top priority and, in
our experience, merits customized monitoring algorithms. Recognizing immediate threats to a team
members physical existence or to the accomplishment of the plan is obviously important. In addition, information that allows the human to discern patterns or recognize the opponents plan or
intent is valuable. Our EAs recognize physical threats and adversarial activity not expected by the
plan, but do not currently perform automated plan or intent recognition on data about adversaries.
Both automated plan recognition (Kaminka et al., 2001) and inference of adversarial intent (Franke,
Brown, Bell, & Mendenhall, 2000; Bell, Jr., & Brown, 2002) are active areas of research. If algorithms are developed that reliably recognize adversarial plans or intent while using acceptable
computational resources, they could easily be invoked within our monitoring framework.
Projections. Even though the current plan can still be executed without change for the time
being, it may be possible to predict that a future failure of plan or global constraints will occur, with
varying degrees of certainty. For example, suppose the plan requires a robot to move to location X
by time T, but the robot is getting progressively more behind schedule or more off course. At some
point before T, the system can predict with acceptable certainty that this location constraint will be
violated and alert the user, who may revise the plan. In addition, new opportunities and probable
adversarial activity could be projected. Projection/simulation algorithms can be computationally
expensive, so the execution monitor must adjust its calculation of projections to match available
resources and constraints.
224

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

Contingency plans. The plan may specify contingency plans or subplans, which are to be
invoked when certain, specified conditions arise. The execution monitor should monitor these conditions and alert the user when a contingency plan has been triggered. The system can also notify
all team members automatically if the user decides to switch execution to a contingency plan. Another desirable alert in some domains might be a suggestion by the system that new contingency
plans should be generated for certain situations as events unfold in an unexpected manner. Our EAs
monitor the triggering of contingencies but do not suggest their generation.
System problems. Depending on the domain, the user may want to be alerted of problems with
incoming data streams or in the functioning of the execution assistant itself. For example, if no data
is arriving from the sensors, or over the network from other team members, this may be crucial to
helping the user interpret the situation and system alerts.
Reporting requirements. One of our basic assumptions is that the human user has experience
and knowledge that are not modeled within the system. Therefore, the system cannot always recognize how a new piece of information will affect plan execution. Some information that does not
trigger the above alerts might still be valuable to the user. The system is given reporting requirements that allow it to recognize such information. One generally useful reporting requirement would
be execution status, so the user can quickly determine that execution is proceeding as planned. Reporting requirements may take any number of forms, as appropriate to the domain. The comments
about recognizing new opportunities apply here  domains might specify requirements as SOPs,
key features, declarative statements, or heuristic algorithms. Several things fall under this category,
such as information that reduces uncertainty and/or indicates that the plan is executing as expected.
As another example, a robot might be told to immediately report any murder or fire it witnesses
while executing its planned tasks.

5. Value of Information and Alerts
Algorithms that alert on constraint violations and threats in a straightforward manner inundate the
user in dynamic domains. Unwanted alerts were a problem in both our domains and in many other
domains as well, such as medical monitoring (Koski et al., 1990). An aid that gives alerts every
second will quickly be discarded by the user in stressful situations (if not immediately). To be useful,
an execution aid must produce high-value, user-appropriate alerts. Alerts and their presentation may
also have to be adjusted to the situation, including the users cognitive state (or the computational
state of a software agent). For example, in high-stress situations, tolerances could be increased
or certain types of alerts might be ignored or postponed. In this section, we provide a conceptual
framework for the alerting algorithms in our monitoring framework and our domain-specific EAs.
Our approach is grounded in the concept of determining the value of an alert. First, the system
must estimate the value of new information to the user. Information theory derives from communication theory and the work by Shannon (1948). In this theory, the value of information refers to
the reduction in uncertainty resulting from the receipt of a message, and not to the meaning that
the message (or the uncertainty reduction) has to the receiver (Weinberger, 2002). We use the term
value of information (VOI) in a different sense, namely, the pragmatic import the information has
relative to its receiver. (Of course, the reduction of uncertainty often has pragmatic import.) Like
Weinberger (2002), we assume that the practical value of information derives from its usefulness in
making informed decisions.

225

fiW ILKINS , L EE , & B ERRY

However, alerting the user to all valuable information could have a negative impact in certain
situations, such as when the alert distracts the user from more important tasks, or when too many
alerts overwhelm the user. We therefore introduce the concept of value of an alert (VOA), which
is the pragmatic import (for making informed decisions) of taking an action to focus the users
attention on a piece of information. VOA takes VOI into account but weighs it against the costs and
benefits of interrupting the user. If the user is busy doing something significantly more important,
then issuing an alert might not be valuable, even when VOI is high. VOA must generally estimate
the users cognitive state and current activities. VOA will generally determine the modality and
other qualities of alert presentation (e.g., whether one should flash red text on a computer display or
issue a loud audible warning).
VOI and VOA are highly correlated in most situations, and most general comments about VOI
apply to VOA as well. However, VOA may be low while VOI is high if the user is highly stressed
or preoccupied with more important tasks. It is also possible to have a high VOA and low VOI.
For example, mission-specific monitors might alert the user to information that has been known for
some time (and thus has little or no value as information) because the information is crucial to an
upcoming decision and the user may have forgotten it, or may be behaving in a way that indicates a
lack of awareness.
Weinberger gives a quantitative definition of pragmatic information, assuming a finite set of
alternatives that lead to well-defined outcomes, each of which has some value to the decision maker.
In realistic domains like ours, alternatives and outcomes are not precisely defined. Furthermore,
information and decision theories (including Weinbergers) assume that the decision maker is aware
of (or has processed) previous information and can devote sufficient resources to analyzing the
current information. Under such assumptions of unlimited processing power, VOA and VOI are the
same. In most realistic domains, these assumptions do not hold. Humans are resource bounded and,
during fast-paced operations, alerts and information may be ignored and the user may not realize
the implications of information on a complex plan that coordinates many team members.
5.1 Estimating VOI and VOA
In interactive, dynamic, real-world domains like SUO, we cannot model all alternatives, their payoffs, nor all the other knowledge and probabilities required with enough precision to compute the
theoretical VOI and VOA. Much knowledge about VOI resides only with human experts, and
even they might have different preferences or opinions about VOI. For example, in the SUO domain, the user might be concerned about the public-relations effects of how the plan execution is
reported in the international media. It is precisely because humans have knowledge not modeled
in the system that we want our execution assistants to be interactive. In such realistic domains,
there are generally no obvious boundaries to the types of support the system should provide, and
no precisely defined evaluation functions or payoff matrixes. Thus, Weinbergers theory and formal
techniques for computing the value of information (Athey & Levin, 2001) cannot be applied. Horty
and Pollack (2001) develop some of the foundations for a theory of rational choice that involves
estimating the cost of decisions in the context of plans. Their approach comes closer to addressing
our concerns. However, determining costs and utilities of actions will continue to require human
judgment in many domains, especially if human lives are being put at risk.
Therefore, we developed algorithms that heuristically estimate VOI using domain knowledge,
although quantitative VOI functions can easily be used in our framework. The inputs to our al-

226

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

gorithms are described in Section 5.3. These domain-specific algorithms are, and must be, easily
customized and tuned for user preferences, as well as the situation. They are invoked in domainindependent ways for a variety of purposes by the monitoring framework, and were developed with
feedback from domain experts. We believe it is feasible to use machine-learning techniques to
replace or supplement hand-coded heuristics for VOI/VOA estimation and/or the user preferences
which affect it, but this was not explored.
VOI and VOA are computed qualitatively in our domains, using several domain-specific quantitative measures in the qualitative reasoning process. Issuing an alert is a discrete event, and generally
there are a small number of options for presenting an alert. Therefore, estimating VOA is primarily a problem of categorizing the potential alert into a small number of alert presentation types or
modalities. We need to determine when the VOA crosses thresholds (defined by the VOI/VOA
specification) indicating, for example, that it is valuable to issue an alert, or that the alert should be
issued as high-priority. In our framework, the thresholds are customizable by the user and can be
mission specific, so they can change automatically as different missions in the plan are executed.
The VOI algorithms also determine what information to include in an alert.
Different alert presentations are handled by assigning a qualitative priority to each alert. For example, our SUO EA divides alerts by VOA into four equivalence classes for levels of priority, which
were already defined in the SUO domain. Each priority is presented differently to the user, from
using different modalities to simply using different colors or sizes of text or graphics. Currently, we
use three priority levels in the robotics domain, but may add more in the future as collaborating team
members make more use of the EA. These priority levels can be used to adjust alerting behavior to
the users cognitive load. For example, during fast-paced operations, only the highest-priority alerts
could be presented.
There are several reasons for preferring qualitative reasoning, and we draw on Forbuss work in
describing the advantages (Donlon & Forbus, 1999; Forbus, 2002). Qualitative models fit perfectly
with making decisions, which are discrete events, and effectively divide continuous properties at
their important transitions. Thus, changes in qualitative value generally indicate important changes
in the underlying situation. Qualitative models also facilitate communication because they are built
on the reasoning of human experts and thus are similar to peoples understanding. For example,
the priority levels used in our VOA algorithms have long been named and defined in the military.
Qualitative reasoning is important as a framework for integrating the results of various qualitative
computations in a way humans can understand. Finally, the precision of quantitative models can
be a serious weakness if the underlying models do not accurately reflect the real-world situation.
Precise data can lead to precise but incorrect results in a low-accuracy model, and the precise results
can lead to a false sense of security.
These advantages of qualitative reasoning are apparent in both common sense and military reasoning. Common sense reasoning about continuous quantities is often done qualitatively. The
continuous value is of interest only when a different action or decision is required. For example,
you can ignore your fuel gauge when driving once you have decided whether or not you must refuel before reaching your destination. In addition to the priorities already mentioned, the military
quantizes many continuous properties used to describe terrain in ways that are relevant to military
operations, creating phase lines, decision points, named areas of interest, key terrain avenues of
approach, and so forth. The SUO EA incorporates these quantizations to reason about terrains influence on VOI and VOA and to effectively communicate information in alerts, just as the military
has used them for years to facilitate communication, collaboration, and decision making.
227

fiW ILKINS , L EE , & B ERRY

5.2 Properties of VOI and VOA
VOI and VOA in our dynamic domain depends primarily on whether the information will influence
decisions/responses. The execution aid must also ensure human awareness of high-value data to
support decisions only the human user can make. Thus, the system must estimate or model what the
human needs to know (e.g., by specifying reporting requirements), even if the system cannot predict
how the information might influence a decision. For example, an emerging adversarial or friendly
pattern might be crucial. If the system does not have a human-level ability to recognize plans and
patterns, then it should ensure the human decision maker is aware of the relevant data.
One obvious but important property of VOI is that it is zero if the user is already aware of
the information. Another property is that information indicating that plan execution is proceeding
according to plan can be valuable, because it influences the decision to continue as planned. The
value of such confirming information depends on the features of the domain  such information
will be more valuable in domains with high uncertainty and active adversaries.
Another feature that may be useful in certain domains is classifying the responses suggested
by a piece of information or an alert. For example, any new report may require a significant plan
modification, a minor plan modification, the invocation of a contingency plan, the application of a
standard operating procedure (SOP), or the identification of a new opportunity. However, the type of
response does not necessarily correlate with VOI, as a minor plan modification might be life saving,
while a major modification might simply reduce resource usage by ten percent. The distinction is
important because the simpler responses can more likely be handled in an automated fashion, thus
reducing the need to involve the user.
Determining what information to present in an alert requires addressing human factors. Initially,
it is important to present an alert concisely so the human can determine its import at a glance,
and assess whether to divert his or her attention from other tasks. In our EAs, the user can drill
down for more detailed information on any alert in order to assess the situation more accurately.
Finally, some domains may have concerns other than making informed decisions. For example, the
emotional state of the user or recording data of scientific value might be beneficial. In particular,
if the concern is analyzing or debugging system performance rather than making good execution
decisions, a different VOI estimator can be used to provide alerts about system behavior.
5.3 VOI and VOA Criteria
As described above, the VOI and VOA algorithms will generally be heuristic, domain-specific, and
user customizable. Here we identify most of the inputs that will be applicable to most interactive,
dynamic domains. We started with a superset of the VOI criteria we found useful in our two domains
and then generalized them to be domain independent. (The properties of the user listed below are
estimates from system models of the user, as the users mental state is not accessible.)
 The plan
 Policies
 Users awareness of current situation
 Systems view of current situation
 Users cognitive load
228

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

 Resources, especially time, available for analysis or response
 Information about adversarial agents
 Characterization of uncertainty
 Age of information and age of users awareness
 Source of information
The plan provides several VOI criteria: the plan may provide explicit and implicit decision
points, high-value places, times, team members, and so forth. The value of a task, constraint,
adversarial action, or team member is often determined by the plan structure and plan annotations.
The tasks in the plan can invoke task-specific VOI algorithms within our monitoring framework,
as described in Section 6. Domain policies (or specialized reasoners that implement them) and
reporting requirements should provide the knowledge necessary to determine the value of alerts
about various types of constraint violations and reports. For example, in our domains, we monitor
the physical safety of our agents. Alerts on life-threatening situations have the highest priority.
We noted that VOI tends to zero to the extent the user is already aware of the information. Thus,
determining VOI must access the current view of the situation to determine if arriving reports offer
new information or simply confirm the existing view. In data-rich domains, we assume that the
execution aid may have a more detailed description of the situation than the user (for the aspects of
the situation that are described by incoming data), because the user may be performing other tasks
and monitoring the situation only when he is alerted by the EA. Therefore, the value of alerting
the user will depend on how much the new information differs from the users last situation update,
even if the system has more recent data that differs only slightly from the new information.
Ideally, we would like to model the users cognitive load, and give lower values to noncritical
alerts when the user is consumed with addressing more critical aspects of the situation. Similarly,
we do not want to overload the systems computational resources or ability to remain reactive, so
the value of certain information may depend on the time or resources available to analyze it.
When determining the value of information about adversaries, it is often useful to compare
developing patterns to any information about the adversarys plans or tendencies, which could be
obtained from human intelligence analysts or generated by plan-recognition or pattern-matching
algorithms. As mentioned above, information that reduces uncertainty is valuable in domains with
high uncertainty and active adversaries. VOI can be estimated if we have a characterization of the
uncertainty present in our current view of the situation.
The age of information is also a factor in VOI  outdated reports may have zero value if newer
information has already arrived. When modeling the users awareness, elapsed time is a factor. The
user will be aware of alerts issued in the last few minutes, but may no longer be aware of something
that was brought to her attention yesterday or last week. Thus, the value of a proposed new alert
may increase with elapsed time since a similar alert was issued.
When a variety of sources of information exists, the source is a factor in VOI. Often, different
information sources have inherently different levels of certainty, authority, or importance. For example, the SUO EA accepts reports from both human observers and automated sensors. An EA
with such inputs might want to weigh human observations differently depending on the human and
the situation. In later sections on our implemented EAs, we describe our domain-specific VOI/VOA
algorithms, which have inputs corresponding to the inputs listed above.
229

fiW ILKINS , L EE , & B ERRY

6. Implementing Execution Monitors  Small Unit Operations
We have developed an execution-monitoring framework that can easily be adapted to produce interactive monitors for agent teams in dynamic domains. To support this claim, we describe two
dynamic, data-rich, real-world domains and the Execution Assistants (EAs) we have implemented
using our framework. Our first domain, Army small unit operations (SUO), has hundreds of mobile,
geographically distributed agents, which are a combination of humans, robots, and vehicles. The
other domain, UV-Robotics (Ortiz, Agno, Berry, & Vincent, 2002), is described in Section 7 and
has teams composed of a handful of cooperating, unmanned ground and air vehicles (UGVs and
UAVs) and a human controller. Both domains involve unpredictable adversaries in the vicinity of
the team members.
We originally developed our monitoring framework for the SUO domain using several personmonths of effort, although the majority of the effort was in knowledge acquisition and modeling.
The SUO monitoring framework, described below, was designed to be modular and to support the
easy insertion of domain-specific (and user-customized) system components, such as task models,
monitoring algorithms, and value-of-information estimators. Our design was validated when we
implemented a complex execution monitor in the UV-Robotics domain in about one person-week
(as described in Section 7.2). The UV EA uses the same plan representation and basic architecture
as the SUO EA, but the inputs are different as are the tasks and the monitoring algorithms that
respond to the inputs and generate alerts.
The majority of our framework also applies to completely automated execution monitoring as
demonstrated by the UV EA. A UV EA runs on each robot in the team and is used to autonomously
adjust the robot control by blending desired behaviors and automatically revising plans during execution. The UV EA also provides alerts to any human controller who is monitoring the robots.
While the framework described in this section is general, we follow it with some domain-specific
details which clarify the concepts and tradeoffs. These details may not be of interest to all readers.
6.1 SUO Problem Description
Small unit operations in the military involve hundreds of mobile, geographically distributed soldiers
and vehicles cooperatively executing fast-paced actions against an unpredictable adversary. Computational support is bandwidth restricted and must use lightweight and portable devices. Currently,
the planning decisions are all made by humans, and the plans are not machine understandable.
We implemented the SUO EA as part of a larger system: the Situation Awareness and Information Management (SAIM) system, which distributes timely, consistent situation data to all friendly
agents. SAIM uses new technologies to demonstrate a new concept of automated support (described
below) in the SUO domain. We assume many small teams of agents (human, vehicles, and eventually robots), separated and dispersed throughout a large space, operating in concert to achieve
goals. We assume that each agent has equipment providing robust geolocation (GPS), computing,
and communication capabilities. SAIM also assumes an unpredictable adversary, fast-paced action,
and a rich population of sensors controlled by cooperating team members.
The key innovations of SAIM, in addition to the EA, are a self-organizing peer-to-peer information architecture and forward fusion and tracking. Fusion of information and tracking is distributed
and done close to the source to minimize latency, bandwidth requirements, and ambiguity. Adjudication maintains consistency of distributed databases. The information architecture supports ad hoc
information dissemination based on multicast groups centered on mission, geography, or command.
230

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

Self-elected servers provide the same robustness for information dissemination that the peer-to-peer
network brings to the transport layer.
SAIM provides large volumes of geolocation data  too much information for a human controller to monitor, particularly in high-stress situations. The EA alleviates this problem by using a
machine-understandable plan to filter the information from SAIM and alert the user when events
threaten the user or the execution of the plan. A plan-aware, situation-aware, action-specific EA
can alert appropriately for the situation, thus improving decision making by enabling hands-free
operations, reducing the need for human monitoring, increasing the amount of relevant information
monitored, and prompting the user when action is required.
The complexities of plans, the number of agents, and the volume of data pose a challenge to
existing execution-monitoring techniques. Unlike a lot of AI planning work, particularly in robotics,
most actions in our domain are performed by external agents, mostly humans, and the monitor has
no access to the state of executing agents. Status information must be obtained from external inputs.
We focus on the problem of alerting human users when the situation requires attention; we
assume that the human will modify the plan as needed. This was done for several reasons. First, the
users are unwilling to cede decision making to a machine, so we first develop trust by giving useful
alerts, a capability well suited for automation if the plan can be represented with enough fidelity, and
something that provides obvious value in dealing with the information glut. Second, mistakes can
be a matter of life and death, so systems must be verifiably robust before they are given decisionmaking power. Human decision makers must take imperfect information into account, including
reports from sensor networks, other humans, and execution assistants. Third, demonstrating the
utility of automated, plan-based monitoring in this large and complex domain is likely to facilitate
future acceptance by users of plan-related automation.
name
Battalion
Company
Platoon

Abbrev
BN
CO
PLT

Entities controlled
400-600
about 100
about 30

Figure 2: Echelons in the command hierarchy with EAs.

Execution monitoring requires coordination over multiple echelons (levels in the hierarchy), so
that users know what their subordinates are doing. Figure 2 shows the echelons for which we have
demonstrated the EA. Multiple agents at each echelon must coordinate fast-paced activities over a
wide area in real time. Our task requires the solution of three difficult problems: handling the large
volume of incoming information, developing a sufficiently rich plan representation for capturing
tactical Army plans, and determining when to alert the user.
As mentioned before, the EA must give only high-value alerts to be useful. For example, once
a unit is out of position or late, the system must recognize both the import of this condition and
when the situation has changed sufficiently to issue another alert, without issuing too many alerts.
Consider the seemingly simple example of a plan specifying that a squad of 10 agents should move
to Objective Golf at 0700. What is the location of the squad? An obvious solution is to compute
the centroid of each members location. However, no one is near the centroid if all members are
in a large semicircle with the centroid at the center (this situation arises when the squad follows a
231

fiW ILKINS , L EE , & B ERRY

road around a sweeping curve). If one member is now immobile with his GPS still broadcasting, the
centroid may be seriously inaccurate. Does the centroid need to be near Golf, or is one member near
Golf sufficient, or must all members be near Golf? It depends on the mission (task) and situation. If
the mission is to observe a valley, one member is sufficient, but we might want all members for an
attack. Our solution is to use mission-specific algorithms (specified in the mission model described
in Section 6.5) for reasoning about the location of units.
The EA must avoid cascading alerts as events get progressively further away from expectations
along any of a number of dimensions (such as time, space, and resource availability). In the above
example, how close in time to 0700 should the squad be before there is a problem with achieving the
plans objectives? Similarly, how close in distance to Golf? Again, the time and distance thresholds
that indicate a problem depend on the mission and situation. A human uses his background world
knowledge to quickly determine if a delay affects the plan, but execution aids must have much
knowledge encoded to do this. These problems become exacerbated as the plans and missions
become more complex. Detecting friendly-fire (fratricide) risks poses even more difficult issues,
because there are typically many friendly units in close proximity.
6.2 SUO Approach
Machine understanding of the plan is the key to helping humans deal with the information glut created by advanced situation-awareness systems like SAIM. The plan specifies expectations for how
events will unfold, so the EA can compare actual events to the situations that were anticipated. We
use rich, knowledge-based plan representations (Wilkins & desJardins, 2001) to allow computers to
share context with users, so both understand the semantics of plans and requests.
We had two tasks involving significant knowledge acquisition and domain modeling: (1) we
had to model SUO plans and the actions that compose them, and (2) we had to model the value
of information and various types of alerts for users. We interacted with several domain experts to
develop these models. These tasks were aided by the centuries of analysis and modeling that have
already been done in this domain. For task 1, the Army already has a standard plan representation
called the Operations Order, which has a required structure, but the entries are mostly free text.
Primitive actions in this domain are referred to as missions, and there are Army field manuals that
describe missions in detail. We modeled missions in a hierarchical mission model. Our mission
model and plans are described in Section 6.5. For task 2, there is extensive accumulated experience
and analysis of errors and opportunities that arise during execution of SUO plans, but there are many
tradeoffs to be made. The tradeoffs and our models are described in Sections 6.4, 6.6, and 6.7.
Mission-specific execution monitoring is achieved by a novel integration of mission knowledge
represented as methods with an AI reactive control system. The EA invokes methods at appropriate
points during plan execution. The methods employ mission-specific algorithms and in turn invoke
EA capabilities in a mission-specific manner. Much of the domain and mission knowledge is encoded in the mission model and not explicitly represented in the plan itself, which specifies a partial
order of missions for each team member. The EA uses the plan to invoke the knowledge in the
mission model at the appropriate time and with the appropriate arguments.
Another feature of our approach, particularly for terrain reasoning, is the pervasive use of specialized programs, possibly external to the EA, to perform complex computations that are important
to system performance. By using alternative specialized programs, the EA can easily adapt the granularity of its reasoning and improve performance as better modules become available. For example,

232

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

API functions in our design can be used for terrain reasoning and to compute the enemy strength
from the current tracks.
Our approach builds on SRIs continuous planning technology (Wilkins & desJardins, 2001;
Wilkins & Myers, 1998; Wilkins et al., 1995) and on the domain-independent Act formalism
(Wilkins & Myers, 1995). Act represents procedural knowledge and plans as Acts, provides a
rich set of goal modalities for encoding activity (see Section 6.5), and has been used by several
institutions (Wilkins & Myers, 1998; Durfee et al., 1997). The EA uses P RS (Georgeff & Ingrand,
1989; Wilkins et al., 1995) as its reactive control system (other reactive control systems have similar capabilities, e.g., UM-PRS (Durfee et al., 1997)). P RS is a good framework for constructing the
EA because it supports parallel activities within an agent, and can smoothly interleave responses
to external requests and events with internal goal-driven activities with its uniform processing of
goal- and event-directed behavior. P RS uses procedures encoded as Acts and its extensive graphical
tracing provides valuable insights into EA operation.
6.3 SUO Architecture
The architecture of the EA and its interactions with the SAIM system are shown in Figure 3. We
developed two major modules, the Planning Assistant (PA) and the Execution Assistant (EA), which
assist the user in generating and executing plans, respectively. We implemented only a skeletal
PA to produce machine-understandable plans, using the S IPE 2 hierarchical task network planner
(Wilkins et al., 1995). Both the PA and EA use Acts and a common knowledge base, ontology, and
mission model that is object-based and easily extended. Knowledge about actions is represented in
the mission model, and knowledge of plans, strategies, and procedures is represented as Acts.
The inputs to the EA are plans to execute, location reports, sensor tracks, and messages from
other agents (e.g., reporting mission success and failure, and ordering execution of new plans).
SAIM broadcasts up-to-date locations of all friendly agents, and broadcasts tracks that represent the
results of fusing sensor hits on nonfriendly entities. SAIM provides and the EA supports rates of
more than a dozen such inputs per second.
The EA monitors the current mission for every immediate subordinate of the EA owner, and
alerts on threats to subordinates (subordinate depth is customizable). If events threaten successful
execution of the plan, threaten the user or subordinate units, or trigger planned contingencies, the
EA issues an alert to the user, depending on the value of such an alert as determined by applying
our VOA algorithms. The user must decide how to respond. Our design and technology can also
suggest responses and/or plan modifications (Wilkins et al., 1995), but this was left for future work.
In addition to giving alerts, the SUO EA can dynamically change the command hierarchy, abort
execution of one plan and switch to monitoring a new plan, and reduce unwanted alerts to avoid
inundating the user.
EAs for every unit at every echelon process reports and give alerts locally. SAIM provides the
same tactical picture to all EAs (modulo an EAs registration in SAIM multicast groups). Therefore,
it is not necessary for an EA to report a new threat to its superior, as the superiors EA (as well as
the EA of other affected team members) has the same information and would already have issued a
similar alert. This architecture is fault tolerant because EAs do not rely on reports from subordinates
to determine most alerts. Thus, each EA maintains most of its functionality even if it is not in contact
with other EAs, as long as it gets SAIM position reports from one node.

233

fiW ILKINS , L EE , & B ERRY

PA

EA
PDA Domain KB

PDA Domain KB
Acts used
for planning

Plan
Initializer

Cue:
ACT2
(TEST (ready unit1))
Cue:

ACT1
Answer query

Executable
Plan (Act)

Execution
Manager
Requests, updates

Executable
Plan

Cue:

ACT1
Answer query

Executable plan,
monitors

Advisable
Planner Agent
Partial plan
Task organization
Assets
Guidance

Cue:
ACT2
(TEST (ready unit1))

Common: ontology
mission model

PRS

Common: ontology
mission model

SIPE

Acts used for
monitoring

Requests
Registrations
Notifications
Updates

SimFlex

Watchman
PRS

PRS

SAIM: Persistent Data Store (PDS), Disseminator, ...

Situation
Updates
Requests
Scripted Events

Figure 3: Internal architecture of the EA and PA and their interaction with SAIM. The PDS archives
plans and other data and has a continuously changing picture of the current situation.

As shown in Figure 3, the EA is implemented as multiple asynchronous P RS agents (defined below) to alleviate the computational burden on the central EA Manager agent. Asynchronous agents
provide faster response and better alerts than would a synchronous architecture, because the agents
are always using the latest information available to them without having to wait to synchronize with
other agents. To implement the EA, we extended P RS to monitor temporal constraints and to batch
incoming facts so it could handle much higher data rates.
Internal EA agents (as opposed to external team members) use the Belief-Desire-Intention (BDI)
model of agency (Rao & Georgeff, 1995). Each agent has beliefs about the state of the world, desires
to be achieved, and intentions representing actions that the agent has adopted to achieve its desires.
Each EA agent has its own controller process, which operates on its own database of beliefs, its
own set of intentions, its own monitors, its own set of Acts that encode procedural knowledge about
how to accomplish goals, and its own L ISP functions that implement the primitive actions of the
agent. An EA agent continually applies its Acts to accomplish its current intentions (tasks). The EA
appears as a single agent to SAIM and the outside world. The following are the internal EA agents.
Plan Initializer. This agent gets a plan from the PA and sends messages to the EA Manager
agent after performing all initializations necessary to begin monitoring of plan execution. Primarily,
this involves creating and loading plan monitors, and posting facts in the EA Manager database.
Watchman. This agent monitors incoming message traffic on the SAIM network, mainly by
querying for tracks and other information. It filters irrelevant or insignificantly changed reports,
and sends a message to the EA Manager when any report or message requires its attention. It
simultaneously monitors files of scripted events when such monitoring is requested. The Watchman
inserts events from scripts at appropriate times, interleaving them with live messages.

234

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

EA Manager. This agent begins plan execution immediately after receiving a plan from the
Plan Initializer. The agent implements the core EA functionality  it compares reports from the
Watchman agent to the plan and plan monitors, and generates high-value alerts.
SimFlex. This agent provides a powerful and flexible way to define the execution semantics
of an action using Acts (and thus the full power of PRS). SimFlex (simulated, flexible execution)
enables mission-specific execution monitoring (by having an Act for each mission) and makes the
system easily extendible. For example, if certain missions were to automatically command robotic
vehicles or send messages to other EAs, those actions could be easily implemented in SimFlex.
Most of the actions in our plans are executed by external human agents, in which case this agent
does little except perhaps prompt the user. Some actions, such as reorganizations, are automatically
executed in SimFlex by invoking the Execute-Mission method defined in the mission model.
6.4 SUO Alert Types
While there is extensive accumulated experience regarding the execution of SUO plans, selecting
which types of alerts to detect involved trading off several factors, such as whether the alert can be
detected from available data, the utility of the alert to the user, the cost of implementation, and the
ability to maintain reactivity given the computational expense of detecting the alert. We earlier gave
the example of balancing the usefulness of fine-grained terrain reasoning for movement projection
with its computational impact on reactivity. Thus, modeling the value of information and types of
alerts to be detected involved interaction between the domain experts and system developers. Here
we describe the types of alerts we decided to detect. More details of how we implement these and
model the value of information and alerts is given in Sections 6.6 and 6.7.
Figure 4 describes the 13 types of alerts that are detected by the SUO EA. Most of these are
time and location checking. Comparing these alerts to our categories in Section 4, the proximity
alerts are all instances of adversarial activity detected. The adversarial alerts also fit this category
and the last three adversarial alerts are also of the type plan constraint violated because expectations and requirements specified in the plan (such as locations and routes to monitor) are violated.
The contingency alert, which can be triggered by either friendly or hostile actions, is of the type
contingency plan suggested. The out-of-position, coordination, and schedule alerts are of type plan
constraint violated, but would be of type constraint violation projected when the violation is projected. The fratricide alert is of type policy constraint violated, and the unknown-position alert is of
type reporting requirement.
6.5 SUO Plans and Mission Model
Our hierarchical mission model specifies an ontology for the primitive actions, and has methods
that encode most of the domain knowledge about constraints and expected behaviors. Tailoring
monitoring to each mission is crucial because most behaviors, even something as simple as denoting
the location of a unit, are mission specific. The plan representation is a novel combination of the
mission model and an extended version of the Act formalism (Wilkins & Myers, 1995).1
1. EA plans represent plans as they are expressed in Army operations orders, but only parts of the current Army fiveparagraph order are represented in machine-understandable form. Primarily, task organization and the specific maneuver tasks and coordinating instructions from the Execution Paragraph are represented, but some other aspects are
encoded as well.

235

fiW ILKINS , L EE , & B ERRY

Alert Type
fratricide
out-of-position
unknown-position
coordination
schedule
contingency
contingency
monitored
ave-of-approach
hostile-expected
contact
distance
strength
proximity

FRIENDLY ALERTS
Friendly units pose a threat to each other.
Location constraints in plan violated.
Unknown location of a subordinate/coordinating unit.
Coordinating units cannot synchronize as planned.
Time constraint in plan violated.
An event has triggered a queued contingency.
ADVERSARIAL ALERTS
An event has triggered a queued contingency.
Activity at a monitored map location.
Activity on a monitored route (avenue of approach).
Expected hostile activity absent.
PROXIMITY ALERTS
A friendly units first contact with a hostile entity.
Hostile entities are closer since last alert.
Threat has grown stronger since last alert.
A merged alert of more than one of the above.

Figure 4: Types of alerts generated by the SUO EA.

The Act formalism is a domain-independent AI language for representing the kinds of knowledge about activity used by both plan generation and reactive execution systems. It provides a rich
set of goal modalities for encoding activity, including notions of achievement, maintenance, testing, conclusion, and waiting. This expressiveness is necessary for representing SUO plans, which
must coordinate distributed units, trigger preplanned contingencies, and enforce a variety of execution constraints. The basic unit of representation is an Act, which can be used to encode plans,
strategies, and standard operating procedures (SOPs).
The EA can monitor any plan that is composed of missions from the mission model. The mission
model is derived from Army field manuals and elaboration by domain experts. It includes a set of
mission templates (with associated parameters) that units at various echelons could be ordered to
perform, in either a written or verbal order. Since the mission model is grounded in field manuals, it
is a first step toward formalizing a plan representation that is meaningful to end users yet amenable
for execution monitoring and other AI-related capabilities (e.g., plan generation, replanning, course
of action evaluation).
The mission model is a class hierarchy (implemented in L ISP and CLOS, the Common Lisp
Object System), with inherited methods that encode knowledge about how to monitor a particular
mission. Each leaf class corresponds to a monitorable action that may occur in a plan; each nonleaf
class encapsulates common parameters and behaviors for its subclasses. The mission model allows
most aspects of system behavior to be tailored in a mission-specific manner. Thus, specialized
methods in the mission model can, for example, use mission-specific algorithms for monitoring
progress of a movement. Methods are invoked by the EA Manager but can in turn invoke processing
in the EA Manager by posting mission-specific facts that invoke capabilities of the EA Manager
(there is an API of such facts, important facts are described later).

236

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

Each mission in the model contains a name and parameters that describe the mission. For example, the mission model contains the nonleaf movement-mission class, which contains a destination
parameter and a method for checking that the executing unit has arrived at its destination. Five
different movement missions inherit this behavior. The root class in the model is the mission class,
which encapsulates all parameters and behaviors that are shared by all missions. All missions inherit
start-time and end-time scheduling constraints and methods from this superclass.
Coverage. The mission model formalizes a substantial subset of the missions mentioned in
Army field manuals. We have enumerated 62 mission classes, and have implemented 37 of these, a
superset of those required by our scenarios. The mission model covers multiple echelons, with emphasis on battalion, company, and platoon. It does not model all aspects of missions, only those for
which SAIM can provide monitoring data, that is, those related to time and location. For example,
it does not alert on potential mission failure due to casualties incurred.
Contingencies. The mission model contains the nonleaf contingent-mission class. This class
and its leaf children classes are used to implement a mission sequence that is part of the plan but is
to be executed only when certain conditions are fulfilled. Domain experts term these portions of the
plan branches and sequels. The missions under contingent-mission contain parameters to describe
the condition, specified in the plan, that activates the contingency.
Dynamic resubordination. Army operations orders allow the command hierarchy (termed the
task organization) to be changed during the operation, although existing command and control software does not support dynamic changes to the command hierarchy. The reorganization-mission
class provides this capability in the EA. When a reorganization mission is executed, it causes the
EA to update its representation of the command hierarchy accordingly. This has a substantial effect
on EA behavior, because many EA algorithms use the command hierarchy.
Methods. Each mission provides several methods that are invoked at appropriate times by the
EA to monitor execution of the mission. This set of methods serves as an API for mission-specific
execution monitoring semantics. The following methods comprise the bulk of the API:
Post-Execution-Constraints is the main API method invoked by the EA for monitoring a mission.
It invokes methods that post and enforce various constraints.
Check-Initial-Location, Check-Final-Location confirm that unit(s) are positioned correctly at
the start and end of their mission respectively.
Start-Time-Constraints, End-Time-Constraints check that a mission is beginning and ending
execution as scheduled. These methods usually post facts in the EA Manager to invoke its
Timed Monitor mechanisms.
Location-Constraints enforces location checking of friendly units and hostile tracks for a variety
of missions.
Contingency-Satisfied determines whether a contingent mission sequence should be executed.
Respond-To-Monitored-Red-Activity is the algorithm for responding to hostile activity in places
where the plan calls for monitoring such activity.
Execute-Mission invokes any processing required to execute a mission. It is invoked by posting
a goal in the SimFlex agent, and all internal agents continue P RS execution while ExecuteMission is running.
237

fiW ILKINS , L EE , & B ERRY

Compute-Priority computes the priority of an alert.
Desired-Strength-Ratio is a heuristic that expresses a desired friendly:hostile ratio of combat
power.
Red-Alert-Priority computes the priority of a proximity alert, or whether an alert should be issued
at all, based on recent changes to reported strengths of a friendly unit and nearby hostile
tracks.
Wait-Until-Mission-Start, Wait-Until-Mission-End control interaction with the EA GUI with regard to mission start and end times.
Specialization of methods is useful for expressing desired behavior by the EA. For example,
the Location-Constraints method is specialized on movement-mission, coordination-mission, and
several other missions. For movement missions, the EA checks whether the centroid of the moving
unit is at its destination. For coordination missions, the EA checks whether any elements of two
coordinating units are at the specified coordination point.
6.6 SUO Execution Monitoring
The EA Manager continuously responds to new goals and facts posted in its database. The Watchman agent is asynchronously posting facts to the EA Manager database as it receives messages from
SAIM. Facts so posted include confirmations of mission starts and completions (from subordinate
EAs), orders for aborting the current plan or executing a new plan (from a superior EA), sensor
tracks, calls for fire, and location reports (from the SAIM network).
The methods in the mission model post facts to the EA Manager to invoke mission-specific
monitoring. Examples of such fact-invoked capabilities provided by the EA Manager include monitoring several types of time constraints and monitoring a specified location for activity (with options
for friendly or enemy, and expected or unexpected).
The behavior of the EA Manager is determined by the posted goals and facts, their relative timing, and the set of Acts used to respond. The EA Manager switches its focus to the highest-priority
task on each execution cycle so that all goals and facts generate responses with acceptable latency
(Georgeff & Ingrand, 1989). Execution cycles are on the order of milliseconds. System behavior
is nondeterministic because it depends on exactly which facts and goals are posted during each execution cycle, which may in turn depend on the CPU scheduling of the EA Manager, Watchman,
and SAIM processes. The number of alerts rarely varies  what does vary is the exact times of
alerts (which can vary by a few seconds), and the hostile strengths reported (which can change if
the Watchman agent gets more or fewer CPU cycles to accumulate tracks before the EA Manager
executes).
The EA Manager must constantly monitor the status and behavior of currently executing missions, while simultaneously monitoring up to a dozen incoming facts per second and determining
their impact on the plan. While monitoring a plan, it typically has on the order of 100 intentions
it is trying to accomplish at any one time, and has 107 Acts (Procedures) to apply to its intentions.
Most intentions can cause an alert to be generated. Each unprocessed report and track forms an intention. Typically, five subordinate missions are executing simultaneously. Each produces multiple
intentions: at least one for detecting the start and end of each mission, and a few for each time and

238

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

location constraint (every mission has at least a start time and an end time constraint). For example, for each time constraint, the EA Manager has intentions that monitor if the specified time has
elapsed and if the required event has occurred.
The plan-based monitoring of the EA can be viewed as asynchronously and simultaneously
interleaving the following activities. We describe these in more detail below and mention the most
important design tradeoffs.
 Initiating or aborting plan execution upon request
 Monitoring incoming location and sensor reports
 Monitoring progress of the missions and time constraints specified in executing plans
 Responding to other types of incoming requests
6.6.1 P LAN MONITORING
To monitor a plan, a request goal is posted in the database. This invokes an Initialize-Plan Act that
computes the conditions that should be globally monitored for this plan and posts facts to the EA
Manager database declaring that there is a current plan with monitors. These facts in turn cause
Acts to execute in the EA Manager, which load and execute the plan. The EA Manager traverses
through the parallel branches of this plan as missions complete.
The global monitors are computed using the API function compute-plan-monitoring-data,
which can specify domain-specific monitors. Domain-independent capabilities are also available,
such as having the system determine all predicates in plan preconditions that must be true initially
(as opposed to predicates that are achieved by plan actions that precede them). In the SUO domain,
compute-plan-monitoring-data finds all decision points and named areas of interest specified in
the plan, and sets up monitors for them. This monitoring is accomplished by posting facts in the EA
Manager database that cause the EA to notice any adversarial activity in these locations.
The EA can abort monitoring of one plan and switch to monitoring a new plan. This process
involves removing facts for old missions and monitors from the EA Manager database, aborting
execution of the Acts currently intended for execution, and posting a goal to execute the new plan.
6.6.2 L OCATION REPORTS
The Blue Report Act in Figure 5 is invoked every time a location report is posted in the EA Manager
database, which can happen several times each second. However, the Watchman agent filters location reports that are not of interest to the EA Manager (e.g., for entities irrelevant to the plan of the
EA owner, or because there is no change from the last report), and updates the representation of the
current situation in the EA. The Blue Report Act is specific to the SUO domain, but our framework
requires a similar Act to be written for each type of input that is to be actively monitored. For example, there is a similar UV-Robotics Act that responds to state updates (see Section 7.4). These Acts
are written using the Act-Editor (Wilkins & Myers, 1995), a tool for graphically editing procedural
knowledge (Acts) with an intuitive user interface.
This Act begins by invoking a domain-specific specialized reasoner to check for fratricide risk,
which may have the side effect of giving an alert (using the API function issue-alert). The specialized reasoner can easily be replaced by better fratricide detection algorithms in the future. Next, the
Blue Report Act checks whether the current plan has any expectations for this unit, and if so, it calls
239

fiW ILKINS , L EE , & B ERRY

BLUE-REPORT fact: Check-blue-report
Cue:
(CONCLUDE
(Blue-report Unit.1 X.1 Y.1 Time.2))
Preconditions:
- no entry Setting:
- no entry Resources:
- no entry Properties:
(Authoring-system Act-Editor)

N1:
(ACHIEVE (Check-fratricide-risk Unit.1 X.1 Y.1 Time.2))

N2:
(TEST
(Expected-location Unit.1 Id.1
Dest-map-object.1 Time.1 Loc-fuzz.1)

N4:
(TEST
(Not (Expected-location Unit.1 Id.1
Dest-map-object.1 Time.1 Loc-fuzz.1)))

Comment:

N3:

This Act is invoked every time a
blue location report is posted in
the database. It invokes an API
function to check for fratricide
risk, and another to check the
units expected location
whenever the database has an
expected location (from the plan)
for the unit in the report.

(ACHIEVE
(Check-expected-location Unit.1 Id.1
Dest-map-object.1 Time.1 Loc-fuzz.1 :In-progress))

N5:
(RETRACT (Blue-report Unit.1 X.1 Y.1 Time.2))

Figure 5: Graphical representation of the Act that responds to every friendly location report.
the API function check-expected-location to compare the current location to the expected location,
again posting an alert if appropriate. Finally, the report fact is removed from the database.
Responding to a fused sensor track indicating adversarial activity is controlled by a similar Red
Report Act, which compares adversarial activity to plan expectations. Instead of analyzing fratricide
risk, the Red Report Act invokes a reasoner for evaluating adversarial threats. As described in
Section 6.7, this involves updating a threat envelope for each friendly unit.
6.6.3 M ISSION MONITORING
To explain mission monitoring, we give an example of how a move mission in a plan is monitored.
A move-mission ready for execution has the following parameters:
(move-mission unit start-time-constraint start-time end-time-constraint end-time destination
route formation march-technique contingency contingency-satisfied).

The EA Manager begins execution by calling three methods defined in the mission model: StartTime-Constraints, End-Time-Constraints, and Location-Constraints. Each of these posts facts in the
EA Manager database to invoke mission-specific monitoring capabilities. For example, LocationConstraints (which is specialized to the class movement-mission) posts facts about locations this
mission expects friendly units to occupy and at what time (derived from the destination and route
arguments), and might also post facts about locations where this mission expects adversarial activity
and where adversarial activity should be monitored/alerted.
The EA receives confirmation of mission start from a subordinate EA. Location reports are
continuously posted by the Watchman, and the Act in Figure 5 analyzes them with respect to the
location facts posted by Location-Constraints. Sensor tracks are similarly analyzed by a different
Act. Let us suppose that at some point during mission execution, a track shows activity in a location monitored by this mission. The EA would detect this and invoke the mission-specific method
240

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

Respond-To-Monitored-Red-Activity, which describes how this mission will respond to such an
event. For example, it could issue an alert, abort the move, execute a contingency plan, or ask the
user to choose from a set of such options.
Type
ASAP
ON-ORDER
START-AT
START-NLT
START-NET
END-AT
END-NLT
END-NET

Meaning
start/end not specified
start/end when ordered
start exactly at given time
start no later than given time
start no earlier than given time
end exactly at given time
end no later than given time
end no earlier than given time

Figure 6: Temporal constraint types.

6.6.4 T EMPORAL MONITORING
The mission model includes starting and ending time constraints for every mission. Each time
constraint consists of a temporal constraint type and an absolute time. The temporal constraint
types in the EA are shown in Figure 6. These constraints require two types of monitoring tasks:
detecting when time constraints in the plan have passed without being met, and detecting events
that occur before their specified time.
We extended P RS with a domain-independent Timed Monitor mechanism that provides a general
capability covering all our temporal monitoring requirements. This capability was implemented
in the form of Acts, with some supporting L ISP code. Four special types of timed monitors are
provided, invoked by posting facts with the predicates Check-Not-Later-Than, Check-Not-EarlierThan, Check-In-Window, and Check-Near-Time. We describe our implementation for one of these;
the others are similar. The Act Check-Near-Time checks that an event occurs within a specified
threshold of some time point and can be invoked by a fact of the form:
(Check-Near-Time event.1 time.1 mode.1 fuzz.1)
To succeed, event.1 must occur within fuzz.1 seconds of time.1, with mode.1 indicating whether
this time is absolute or relative (to the time at which this fact is posted). A Timed Monitor Act sets
up a timer that expires at the given time, and P RS reacts appropriately to either the expiration of
the timer or the occurrence of the event, posting facts to the database to note the success or failure
of the temporal constraint. Because the above Acts are fact invoked, these mechanisms enable the
establishment of separate intentions to perform timing, without blocking other processing. This
modularization enables triggers to be set up to independently respond to timing results.
6.6.5 D ESIGN TRADEOFFS
As described in Section 2, a balance must be struck between the capabilities provided and resources
used. The tradeoffs are different in every application and are usually a critical aspect of the design of
an execution assistant. In the SUO domain, terrain reasoning is a key factor in this tradeoff. Using

241

fiW ILKINS , L EE , & B ERRY

fine-grained terrain data to analyze progress or project future failures can overload computational
resources. Therefore, the EA uses coarse terrain reasoning, but our design allows higher-fidelity
terrain reasoners to respond to a defined set of terrain analysis requests. This feature allows the
system to adjust its analysis to the tempo of operations.
Other key features to consider when making tradeoffs between reactivity and capabilities are
the amount of processing done by the mission-monitoring methods, the report-monitoring Acts, and
any specialized reasoners (such as terrain reasonsers) invoked by the methods or Acts. The user
can adjust the frequency of monitoring at any time by customizing parameter settings. Currently,
the SUO EA is not computationally overburdened while analyzing every report in full, but adding
more computationally expensive projections or alerts in the future could cause reconsideration of
this design decision. Finally, the amount of filtering of incoming reports done by the Watchman
agent affects this balance.
6.6.6 OTHER

FEATURES AND IMPLEMENTATION

The EA responds to other requests, such as calls for fire, which are described in Section 6.7. Several
other capabilities were implemented to make the EA easier to use and understand. Two are briefly
mentioned here. We implemented a GUI, not meant for military users, but rather to facilitate evaluation and understanding of the EA. The GUI displays all alerts in different scrollable windows for
each priority level, the current time, and the current mission of each subordinate of the EA owner.
The user can confirm mission starts and ends locally, although this might be done with voice or
some other modality in a fielded system. When a confirmation arrives from a subordinate EA, the
confirmation window for that mission is destroyed. Thus, confirmations and prompts can be given
locally or received in messages, with a seamless interleaving of those two types of confirmation.
The EA, PA, mission model, P RS and S IPE 2 are implemented in C OMMON L ISP, CLIM, and
CLOS. The EA also contains procedural knowledge in the form of Acts. SAIM was implemented
in C++ and Java, using the ACE Object Request Broker for CORBA. C++ was used to interface the
EA to SAIM and CORBA.
6.7 Alert Detection and VOI/VOA
The central task of the EA is to notify the user of important changes to the situation that may demand
attention. The EA must also avoid excessive alerting; otherwise, the user would abandon the EA
as a nuisance. A model of the users cognitive state with respect to awareness of threats would
be ideal, but is unavailable. As described in Section 5, we developed algorithms that heuristically
estimate VOI and VOA using domain knowledge. The inputs to these algorithms are described in
Section 5.3. We avoid excessive alerts by issuing only high-VOA alerts. Our techniques include
 Keeping event histories for each friendly unit, for map coordinates and for important map
locations named in the plan (e.g., decision points).
 Having alerts expire in the sense that they can no longer be used to suppress future alerts.
 Using alert histories for suppressing alerts by time (similar alert given recently), strength
(threat not significantly stronger), and distance (threat not significantly closer).
 Merging several related alerts that apply to subordinates into one alert for the common parent.

242

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

 Providing parameters so the user can customize alerting behavior and VOI/VOA estimates.
The event histories are currently our only model of the users cognitive state, except for global
properties of the situation, such as operational tempo. Our VOA calculations take into account the
frequency and timing of alerts that have already been given. The histories include all alerts that were
issued to the object of the history, and may include additional events, as described in Section 7.4.
We assume that the user is aware of information about which he or she has recently been alerted.
The idea behind having alerts expire is that the user may have forgotten information provided too
far in the past. Thus, the EA will not use alerts older than a specified threshold to reduce its estimate
of the value of giving an alert now.
The EAs behavior must be easily customizable, both by users and by the plan, because users
have different preferences and situations impose different requirements. The EA can be customized
in many different ways. Our VOA algorithms, which recommend alerts and classify them by priority level, are controlled by thresholds and repetition parameters, which allow alerting behavior
to be customized to the user or situation. Examples of customizable VOA parameters are the alert
expiration periods described above (default 12 minutes) and alert suppression intervals (90 seconds
for hostile alerts, 120 seconds for alerts about friendly team members) during which alerts of the
same type about the same objects are suppressed for the given interval. In terms of VOA, another
fratricide alert has no value for the first 120 seconds after the user has been alerted about a fratricide
risk from the same team member.
Examples of customizable VOI parameters are the out-of-position distance threshold (150 m),
thresholds on the strength of adversarial threats, and the time threshold for schedule alerts (30
seconds). The time threshold, for example, would be smaller for tightly coordinated operations, and
larger for more loosely coordinated plans. In terms of VOI, detecting that a team member is late has
no significant value until the tardiness reaches the given 30-second threshold. If certain missions in
the plan change this threshold, say to 10 seconds, it indicates that information about tardiness of 10
to 30 seconds has more value in the context of these missions.
The problem of avoiding unnecessary repetition of similar alerts occurs with every type of alert.
Schedule deviations can become progressively more off schedule, position deviations can become
progressively more out of position, threats can move progressively closer or become progressively
stronger, and fratricide threats can persist over time. An EA must avoid cascading alerts in each of
these cases. In our framework, customizable thresholds are often paired with either customizable
ratios or a customizable sequence of thresholds, which control how often to repeat the alert if the
mission deviates progressively more from expectations. Repeated alerts generally have a lower
VOA and are given lower priorities.
Our evaluation showed that two types of alerts in the SUO domain pose particular problems
for avoiding inundation of the user. These are proximity alerts about adversarial activity and alerts
about fratricide risks among team members. We developed VOI/VOA algorithms especially for
these two types of alerts.
6.7.1 P ROXIMITY

ALERTS

There can be a high volume of sensor tracks near friendly units prior to and during battle; it would
overwhelm the user to see an alert on every change to every track. We keep a threat envelope for
each friendly unit, consisting of tracks close enough to pose a threat to it. Tracks are placed in zero

243

fiW ILKINS , L EE , & B ERRY

or more threat envelopes when they appear or move. Only significant changes to the strength of the
aggregate force in an envelope or the closeness of the nearest track causes an alert.
6.7.2 F RATRICIDE RISKS
Fratricide is one of the biggest dangers on the modern battlefield. This risk increases as the range,
lethality, and accuracy of weapons increase. Increased range increases risk because there is a bigger
area in which every team member must be correctly identified. Increased accuracy increases risk
because an incorrectly targeted team member is more likely to suffer harm. Hopefully, tools like
the EA and SAIM will increase situational awareness and greatly reduce the frequency of incorrect
targeting. Usually, a large number of friendly entities are in close proximity, so many potential
fratricide situations exist.
The EA detects two types of fratricide risks: (1) from calls for fire from other team members
(which appear in messages from SAIM), and (2) friendly units near each other (which are detected
from the geolocation data). In the first case, the user who issues a call for fire is warned and asked for
confirmation if team members are within a given threshold of the target. If the request is confirmed,
a SAIM message is sent to team members, and the EA of any entity within the target threshold
immediately alerts its owner to the risk from the planned fire.
The second case produces far too many alerts if simple algorithms are used. Our algorithms are
based on the Armys notion of unit boundaries, which are specified in the plan. When two units
are within their boundaries, no alert is issued even if they are within weapons range of each other.
Fratricide alerts are issued when one unit is in another units boundaries and within weapons range
of the other unit. We handle numerous special cases, such as when two units are both outside their
boundaries and within weapons range of each other. Detection of other fratricide situations is left
for future work (e.g., misoriented units within their boundary).
6.8 SUO Evaluation
The EA was evaluated with respect to the usefulness of its output, frequency of unwanted alerts,
and real-time performance with realistic data streams. SAIM and the EA were tested against data
produced by a high-fidelity military simulator on two scenarios. The simulator has detailed models
of each type of vehicle and sensor. One scenario lasted 13.5 hours, but only the last 90 minutes
were simulated at high fidelity. (The first 12 hours had a file of scripted events, with a few dozen
tracks and reports.) The second scenario, on the same terrain, was simulated for 20 minutes. The
90-minute simulation had more than 45,000 events passed to the Watchman from SAIM, of which
13,000 were passed on to the EA Manager, which monitors only to the squad level (8 to 10 entities).
During the simulator run, scripted events also simulate messages from any team members that are
not running live (such as messages confirming mission starts and completions). The high fidelity of
the simulation provides realistic data rates and inputs, thus providing some evidence indicating that
the EA will perform as desired in the real world.
Our formal evaluation ran live with SAIM, the simulator, and several team members, each running their own copy of SAIM and the EA on different physical machines. For a shorter development
cycle, we implemented an event generator that reproduces SAIM behavior, making the SAIM network unnecessary. The event generator creates messages from files of scripted events that include
confirmations of mission starts and completions (that normally would come from a subordinate
EA), orders for aborting the current plan or executing a new plan (that normally would come from

244

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

a superior EA), and sensor tracks and location reports (that normally would come from the SAIM
network). Our event scripts contain all messages captured from a run that included the simulator
and SAIM.
6.8.1 Q UALITY

OF ALERTS

Figure 7 presents the total number of alerts by type at each echelon during a typical run. Flash
is the highest of four priorities, and immediate the second highest. Flash alerts are generally life
threatening (first contact with adversarial entities and fratricide), while lower-priority alerts are only
plan threatening.
We analyzed and evaluated the alerts generated from our first and most challenging scenario.
Analysis by SRI and our domain experts indicates that all important situations were alerted. Less
than 10% of alerts were judged to have such low value that they should not have been issued, and no
Flash alerts were so judged. Judging the VOA for each alert is subjective: different domain experts
may have different alerting preferences, and each alert will have some new information. We have
no firm data on the number of unwanted alerts that would lead to performance degradation for a
typical user (or that would cause a user to shut off his EA). It is clear that the 86% false-alarm rate
found in a pediatric ICU (Tsien & Fackler, 1997) would not be acceptable on the battlefield. In our
judgment and that of our domain experts, the rates of low-value alerts we achieved are acceptable.
The number of alerts in Figure 7 is reasonable for a 90-minute interval of fast-paced action, and
further elimination of alerts risks missing a high-value alert. We purposefully erred on the side of
not missing any alerts.
We have compared the alerts generated by EAs operating at different echelons (running on
different machines on the SAIM network) on the same simulation. Our analysis shows that they
detect the same threat at the same time from the same tracks, when the threat is relevant to their
plans. The alerts show plan-specific and mission-specific behavior as expected. Because of the
nondeterminism inherent in our asynchronous agents, the alerts do not always show the exact same
strength, bearing or location of a threat. Figure 8 shows one example, the BN and A CO alerts near
08:05. At this time, 2nd PLT, A CO is moving outside its unit boundary specified in the plan, and a
hostile force appears to the north of A CO moving south. Note that both EAs issue flash fratricide
alerts at 8:05. However, the other alerts are different, and specific to the plan and owner of that EA,
as we would expect.
The plan called for an Attack-By-Fire mission if tracks are observed at location DP2 (a decision
point at which hostile activity calls for a human decision). This immediate alert appears only on the
BN EA (because the contingent fire mission is in only the BN plan) and notifies the user that hostile
entities have entered DP2, triggering the contingency. AA-Diamond is a route, defined in the BN
plan, along which adversaries are likely to approach. The second alert notifies the BN user (only)
of activity on the route and reports the number of entities detected.
Both EAs independently identify the fratricide risk at 8:05, as would the EAs of the two platoons
involved. The message details the two platoons to facilitate a quick response. Next, the BN EA
issues a distance alert after detecting tracks 450m SE of the Recon PLT, which was subordinated to
the BN earlier in the plan (so only the BN EA alerts). These tracks are now closer than when an
earlier first-contact alert was issued. Finally, the out-of-position alert at 07:58 indicates that 2 PLT
is 1 km south of the route specified for its move mission. (The 2 PLT EA simultaneously alerts that
one or more of its subordinate squads are out of position.)

245

fiW ILKINS , L EE , & B ERRY

Number
78
33
3
2
17
5
5
9
36
19
3
1
6
0
3
4
7
6
1
0

Type of Alert
Battalion EA - 41 missions
total alerts over 13.5 hrs, 26 flash
proximity alerts
schedule alerts
out of position alerts
avenue of approach alerts
triggers contingency alerts
at-monitored alerts
fratricide alerts
A CO EA - 11 missions
total alerts over 1.5 hrs, 14 flash
proximity alerts
schedule alerts
out of position alert
avenue of approach alerts
triggers contingency alerts
at-monitored alerts
fratricide alerts
3 PLT, A CO EA - 6 missions
total alerts over 1.5 hrs, 2 flash
proximity alerts
schedule alert
all other alerts

Figure 7: Number of alerts by type at each echelon. The number of missions for each echelon
indicates the size of its plan. Only the last 90 minutes of the 13.5-hour scenario was
simulated at a high fidelity. Of the 78 Battalion alerts, all but 5 were issued over the last
90 minutes.

6.8.2 P ERFORMANCE
Our EA Manager must handle more than 100 simultaneous intentions, while determining the import
of a dozen or more new facts a second and checking alert histories for redundancy. It was not clear
that our system could do all this and still alert the user within 5 seconds of a new fact arriving, as
required by our users. We tested the EA on both scenarios to determine if it met these requirements.
In real time, the EA generated alerts in less than 2 seconds from the receipt of a new fact. We found
that the EA can not only keep up, but can run at between 10x and 20x real time. (There may be
anomalous schedule alerts because of granularity issues at high time expansion rates.) Thus, current
data rates are not close to stressing the system  at 10x real time we are processing an average of 24
events per second in our 90-minute simulation, which is double our design requirement of a dozen
events per second. We did not determine the multiple at which degradation would occur because
it is difficult to detect degradation in such a complex system. We did establish that the EA, using

246

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

From BN EA 0803-0805
DAY 2, 08:04 IMMEDIATE notification:
Red activity at DP2 triggers contingency for Attack-By-Fire
DAY 2, 08:05 ROUTINE notification:
Enemy activity on ave of approach AA-Diamond (8 vehicles)
DAY 2, 08:05 FLASH notification:
Fratricide risk 2 Plt, A Co moved out of position near 3 Plt, B
DAY 2, 08:05 IMMEDIATE notification:
Closest threat (tracked) is now closer- 450m SE of Recon PLT
From A CO EA 0758-0805
DAY 2, 07:58 IMMEDIATE notification:
2 PLT out of position for Move mission is at GL180837, should be at Line-0003 (1000 m. N of 2 PLT)
DAY 2, 08:05 FLASH notification:
Fratricide risk - 2 PLT is out of position near 3-3-B-2-66

Figure 8: BN and A CO alerts around 0805 on Day 2. There was only one A CO alert from 8:03 to
8:06, during which time there were several BN alerts.

100-meter map granularity, is easily sufficient for plan monitoring with SAIM data rates (running
on both Sun Ultra 60s under Solaris and Pentium-based machines under Linux).2
Prior to implementation of the EA, we did a performance evaluation of P RS to determine if it
could handle the input data rates required by the EA. We briefly describe our results as many other
reactive control systems are based on P RS, e.g., UM-PRS (Durfee et al., 1997). We found that it
could not handle more than 12 facts per second without unacceptably long delays, using randomly
generated facts for two predicates where each fact invoked only trivial processing (incrementing
a counter). We determined that the effects of combinatorial P RS algorithms could be avoided by
batching new facts each time through its control loop. We modified the control loop to do so, and
the performance improved remarkably. For a test case of 2,000 facts posted in 1 second, it reduced
time to respond to the first (any) fact by 84%, reduced time to respond to all facts by 72%, and
reduced memory usage by 83%. Experiments showed that a fact batch size near 55 was optimal for
reducing response time, and any value between roughly 25 and 100 was near optimal.
6.8.3 L IMITATIONS
The EA is limited by what has been modeled, by the low fidelity of some models and heuristics, and
by the scenario-specific population of the knowledge base. There are many aspects of plan execution
that we do not currently monitor, although our monitoring framework can be easily extended when
other aspects of plans are modeled. Our selected capabilities are mostly a function of the available
input data and available funding for modeling. The EA can monitor a much broader range of plans
than were used in our scenarios. In fact, it can monitor any plan composed of a partial order of
defined missions for team members.
2. All performance data are from a Sun Ultra 60 under Solaris. All product and company names mentioned in this
document are the trademarks of their respective holders.

247

fiW ILKINS , L EE , & B ERRY

7. Monitoring Robot Teams
We are using a team of robots to cooperatively track and pursue enemy entities that have been detected. Unmanned air vehicles (UAVs) and unmanned combat air vehicles (UCAVs) are a growing
research interest (Musliner, Durfee, & Shin, 1993), led by the availability of cheaper platforms that
are easier to use. The SRI UV-robotics project focuses on building a system to carry out a mission
objective using a team of UGVs and UAVs. Each UGV or UAV is an autonomous agent with its own
view of the world, own onboard reasoning capabilities, and own set of resources (such as power,
computation, and a unique set of sensors). During a mission, there may be limited opportunity to
communicate with the human controller. Therefore, the agents must rely on one another to complete the mission. Our research concentrates on providing reactive regulation of low-level sensor
systems and vehicle controllers so as to attain high-level mission goals, while reacting to unforeseen
circumstances and taking advantage of the evolving situation.
The UV-robotics domain resembles the SUO domain in that it requires the rapid assessment of
the operational situation, the determination of the viability of existing plans and control policies,
and the modification of goals and objectives based on those findings and the available resources.
Unlike the SUO domain, the decisions are made by the (automated) agents themselves and the
agents must negotiate solutions in a cooperative fashion. One of the challenges of UVs (or any
physically mobile agent) is the need for a reactive system. Perception of, and knowledge about,
events and actions in the physical world are generally imprecise. To perform tasks reliably and
repeatedly requires dynamic monitoring.
Just as the SUO EA filters alerts to avoid overloading the human decision maker, we must also
filter alerts to an autonomous agent to avoid overloading its computational resources. Resources are
always limited, particularly on a mobile platform, so a balance must be struck between usefulness
and resources used. A good example of such balance is the computational resources available
onboard our robots. With an infinite number of CPU cycles, we would be able to generate large
numbers of contingency plans and evaluate each with simulation. However, we have only 20% of
the CPU available for robot control and monitoring. Therefore, we have to make design decisions
that limit the complexity of both control and monitoring algorithms, possibly leaving extension
hooks in anticipation of greater processing power in the future.
7.1 UV-Robotics: Problem Description
Our long-term goal is to build, test and validate an architecture for an agent that can support multiple
goals in a dynamic environment of cooperative mobile agents. Initial tasks for our teams include
surveillance and reconnaissance, search and destroy, pursuit, and evasion. A team of robots would
be expected to perform these tasks with minimal supervision. Key components of this architecture
were identified to be negotiation, strategic planning, execution and tasking control, execution monitoring, and recovery from failure. The challenge is to not only have several robots working together
but to have them understand the effects of their actions on common team goals.
One challenge is that an agent may be working toward multiple, possibly conflicting, goals.
Thus, the agent must be constantly evaluating its commitment to actions, or tasks, that contribute to
the satisfaction of these goals. The imprecision of any action or sensory input has to be taken into
account, and its contribution toward the satisfaction of current goals or plans assessed. In addition,
the user must be kept informed of the progress of the team toward its goals. The user does not
want to be actively involved in robot control, but must be able to intervene when necessary. Thus,
248

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

monitoring must both ensure robust autonomous operation and provide the user with a window into
the operation of the team.
7.2 UV-Robotics: Architecture
The SRI UV robot architecture is based on several years of research at SRI into intelligent reactive
control, planning, negotiation, and robot motion control (Wilkins & Myers, 1995; Myers, 1996;
Wilkins & Myers, 1998; Cheyer & Martin, 2001; Konolige & Myers, 1998). It is similar to systems
like SAFER (Holness, Karuppiah, & Ravela, 2001) and SRTA (Vincent, Horling, Lesser, & Wagner,
2001) in its ability to deal with multiple goals at once and evaluate when to discard goals. Figure 9
shows our Multi-Level Agent Adaptation (MLAA) architecture. Clearly, monitoring is pervasive
and serves each layer in the architecture as well as the user (not shown).
Other Agents
Uses
Coordination

Policy Maker
Update/Ask Achievable

Query

Uses

Uses

Strategic Planner

TEAM
LEVEL
STRATEGIC
LEVEL

Update
Update

Resource Mgr.

EA Watchman
Insert Goal
Update

Query

EA Plan Initializer
EA Plan Manager

Update

TACTICAL
LEVEL
(PRS)

Process
Query

Task Blender
Primitive Action
Executor

Update

CONTROL
LEVEL

Low-level Actions

Figure 9: Multi-level Agent Adaptation Architecture.
The coordination module receives goal requests from the human commander or other agents.
The agent participates in a negotiation process to determine its role in achieving the goal. During
negotiation, the agent consults the strategic planner to create a plan, or plan segment (referred to
as a recipe), and assess the recipes viability given current commitments. If the negotiation process
results in the goal and its recipe being accepted, the EA Manager (see Figure 3) instantiates the
recipe and initiates its execution. The Plan Initializer also creates monitoring sentinels for use by
the EA to detect deviation from the recipe during execution. The execution of a recipe involves
activation of tasks that must be blended with other active tasks to maximize the satisfaction of
multiple goals. For example, if the robot needs to reach a waypoint by a set time, take a picture of
a location nearby, and also remain concealed, the task blender modifies the path planner at runtime

249

fiW ILKINS , L EE , & B ERRY

to achieve all three tasks. Finally, the lowest layer in the architecture is the interface between the
tasking architecture and the physical, or simulated, robot controller.
The monitoring in Figure 9 is done by the UV EA, which was created by using the architecture
and representations of the SUO EA. The modular design of the SUO EA made this adaptation
straightforward. The architecture and internal EA agents depicted in Figure 3 were used with little
modification, as were the plan representation and the techniques for monitoring plans, applying
VOI and VOA calculations, and issuing alerts. Our implementation of an initial UV EA (using code
from the SUO EA) was done in about one person-week, an impressive result given the complexity
of the task. The implementation included connecting to new data sources, parsing their messages,
determining and implementing the most valuable monitoring algorithms, integrating with the plans
and missions already defined, and writing domain-specific VOI/VOA algorithms. Achieving some
missions requires recalculating waypoints at least every second while using only 20% of the CPU,
so we had to trade off speed and complexity in both waypoint calculation and monitoring. The
initial version of the UV EA detected the first five types of alerts listed in Section 7.4.
7.3 UV-Robotics: Execution Monitoring Issues
The initial monitoring issues apparent within the UV-Robotics domain can be divided into the following four categories:
 Monitoring the completion of, or progress toward, a basic action (e.g., go to a waypoint)
 Monitoring the satisfaction or completion of the multiple tasks to which the robot is currently
committed (e.g., pursue evader, patrol area, photograph target every 2 hours)
 Monitoring the activity of unknown or adversarial entities
 Monitoring the state of the communication network, the robot, and other team members (e.g.,
communication network quality or integrity, robot mobility, or battery level)
Comparing these to our ontology in Section 4, the first two categories involve the general alert
types plan constraint violated and constraint violation projected. However, they exist at different
levels of abstraction and often have different temporal impact and associated monitoring requirements. The third category cleanly fits the adversarial activity detected alert type and triggers alerts
for both autonomous control and user reporting. The fourth category is essential both to team-based
automated operation and effective user interaction, and involves policy constraint violated alerts,
reporting requirement alerts, and system problem detected alerts.
7.4 The UV-Robotics Execution Assistant
Like the SUO EA, our robot controller uses a rich plan representation to allow team members to
share context and communicate with the user. Primitive actions in this domain are basic motion control and communication requests to the physical robot. A goal request from the user is decomposed
into individual agent plans (recipes) and intentions to aid or interact with other agents. Recipes are
composed of partially ordered sequences of tasks that in turn evolve into primitive actions.
The UV EA uses an internal architecture similar to the SUO EA, as shown in Figure 3. As in
the SUO EA, the EA Manager continually applies its Acts to respond to new goals and facts posted

250

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

in its database. The Acts correspond to algorithms for monitoring requirements at each layer in the
MLAA architecture. Some implement user alerts and others implement autonomous control.
The inputs to the UV EA are plans to execute, policy declarations, status reports (including
location, speed and orientation) from its own sensor suite, and messages from other agents. These
messages include status reports of other agents, reports on mission success or failure, shared information, and requests for help. Depending on communication conditions or policy restrictions,
an agent may, or may not, receive from team members status reports (up-to-date locations) of all
friendly agents and other entities within visual range. Sentinels are extracted from plans and policy
declarations, are evaluated when status reports are received, and may produce alerts. The alerts
produced are designed to serve both the autonomous control via the plan manager component, and
the user, although the needs of each vary considerably.
For our initial experimentation, all monitoring alerts were derived from regular state messages
from each team member. A state message reports the current location, velocity, attitude, and sensor
imprecision of an agent. A UV-Robotics Act similar to the SUO Act in Figure 5 is invoked every
time a location report is posted in the EA Manager database. Such postings happen several times
each second, because each robot receives two such messages every second from its own sensors and
two from each team member, based on network conditions. It also receives similar state messages
about entities within its own field of vision. This means in a team of three robots each agent will be
handling a minimum of at least six state messages per second and possibly many more depending
on the environment. Also, there are messages between agents for sharing information, which we
are not currently considering except when they update state knowledge about adversarial entities.
In the future, the UV EA will be extended to serve the higher layers of the architecture that have
more in common with the SUO-EA alert types and triggers.
Our initial implementation of the UV EA detects the following types of alerts. We plan to
implement additional monitoring during the project.
 At-goal  robot at current waypoint
 Stuck  robot stuck and not at current waypoint
 Divergent  robot diverging from current waypoint
 No-status  robot no longer reporting its state
 Target-visible  robot has a target within its sensor range
 Lost-target  robot lost track of target during pursue mission
 Target-gone  target moved out of assigned sector during pursue mission
 Collision  robot anticipates it will hit a nearby object in the next few seconds
 Handoff  robot has delegated/accepted a task to/from another team member
The UV EA uses the same techniques as the SUO EA (Section 6.7) for estimating VOA and
greatly reducing the number of low-value alerts. In particular, the UV EA keeps event histories for
each team member being monitored. These histories are used to determine the value of information
and alerts, and to detect Stuck, Divergent, and No-status alerts. For example, the history indicates
251

fiW ILKINS , L EE , & B ERRY

the time and the robot location at the last progress check, so if the current waypoint has not changed
and the robot is further away from the waypoint, then the value of issuing a Divergent alert should
be calculated.
The value of issuing an alert takes into consideration customizable latency thresholds and repetition parameters, which are associated with both the automated agent and the user. Some of the
agent parameters are customized to improve performance, while others are a function of the behavior of the robot. For example, the value of a divergent alert will be a function of the expected
velocity of the agent, because an agent traveling at speed will diverge more quickly than a slow
agent. Similarly, a change in orientation will influence the value of an alert because a turning agent,
while not decreasing the distance to the waypoint, may indeed be making progress toward the goal.
An example of how monitoring is used to facilitate autonomous control is illustrated by the
situation where an agent is patrolling a designated area. When an evader becomes visible, the agent
receives a Target-visible alert, which is of type adversarial activity detected. Reacting to either a
high-priority policy to pursue evaders or an explicit plan step, the agent commits to a new goal
Pursue named-evader. This goal is achieved by the activation and blending of three tasks: Follow
named-evader, Relocate named-evader and Search-for named-evader. Thus, the robot will maintain
pursuit even when the evader slips in and out of its field of vision.
The users preferred strategy might be to report the first sighting of the evader or to track its
position, noting whenever it disappears from view. However, the autonomous control requires notification only if the likelihood of recovering visual contact is deteriorating and the robot is searching
aimlessly. At this point, a Target-Lost alert, which is of type plan constraint violated, will be sent to
the agents EA Manager (and possibly the user). In this example, a policy exists for reacting to this
type of alert. It will cause the pursuit goal to be dropped and the original Patrol plan to be resumed.
7.5 UV-Robotics: Evaluation
The UV EA is being evaluated within an SRI experimental framework called the SRI Augmented
Reaility Simulator (SARS) (Ortiz et al., 2002). The framework allows our autonomous agent architecture and software to be tested within an entirely simulated environment, on a team of physical
robots, or a mixture of the two. The physical robots are three pioneer robots from equipped with
GPS, as shown in Figure 10. Initial experiments were carried out in a simulated environment. We
then ran the system in an entirely physical world with a team of two cooperating robots searching
for and pursuing two independent evader robots. We have also run in environments composed of
a combination of physical robots and simulated entities to illustrate scalability and operation with
UAVs. The monitoring technology was effective in ensuring robust execution in all environments,
and in giving human operators insight into the state and activity of each robot. This insight facilitated debugging and the process of moving from the simulated world to the physical robots as
problems were quickly identified.
SARS is specifically designed to simulate robots and UAVs. It produces the same output in terms
of sensors, actuators, and resources (battery status, communication range, and so forth). SARS
computation and simulation is based on a precise 3D model of the environment. SARS is precise
enough that we can mix physical robots moving in the real world with virtual evaders and see the
physical robots following a virtual evader  thus, the name augmented reality. Using SARS, we
are able to simulate a team of UGVs moving and/or UAVs flying in a larger space than we have
available. The team of UAVs may be larger than our available physical UAVs, as well.

252

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

Figure 10: SRI experimental Pioneer UGV.
The initial UV EA implementation was evaluated with respect to the usefulness of its output,
value of alerts, and real-time performance with realistic data streams. Our analysis shows that all
the important situations are alerted during simulated executions, and during tests with actual robots,
which are never exactly reproducible.
No-status alerts have proven useful to the human user, as they indicate a hardware or software
problem on a robot or the network. Such problems are recognized immediately (after the customizable interval of noncommunication has passed) with the UV EA, but take considerably longer to
detect without alerts from the EA. A customizable threshold (which currently defaults to 5 seconds)
determines the value of an alert when the robot has not reported its state for a certain interval.
At-goal, Stuck, and Divergent are essential alerts for the autonomous-control agent-navigation
system, as well as being useful to a human user who wants to monitor the activity of a single
robot. Knowing when the robot has reached a goal point, when it has stopped and is not making
progress toward a goal point, and when it is diverging from the planned route are essential to robust
autonomous operation. Customizable intervals also control these alerts. Subtleties of the domain
must be considered to avoid false alarms. For example, the robot may be paused because of GPS
uncertainty and the GPS should be given time to establish connection with satellites. Also, a robot
takes time to turn and thus should not be regarded as stuck or divergent until turns and steering
adjustments have had time to complete.
Target-visible, Target-lost, and Handoff are useful to both the user and the autonomous controller, particularly when the task is to monitor or pursue a target. The autonomous controller
requires immediate awareness of loss of sensor contact, so it can adjust its lower-level behavior or
sensor parameters to find the evader. However, such immediate alerts would be unproductive for the
human user or the plan-level controller. A customizable interval gives the agent time to relocate the

253

fiW ILKINS , L EE , & B ERRY

evader, possibly avoiding an alert to the human. These types of alerts are the most time critical in
our evaluation domain.
Good tracking of an evader requires recalculating waypoints and orientation at least every second. The UV EA was able to keep up with data inputs, detect occurrences of the types of alerts
mentioned within 1 second, and recalculate waypoint and orientation twice per second. These constraints were not difficult to meet on our desktop machines, but the success of the UV EA on the
slower processors of the physical robot involved tradeoffs of speed with complexity of waypoint
calculation and monitoring. One useful technique is only using the latest state report for an agent
when more than one state report has accumulated during one cycle of the monitoring loop. The
relative CPU access of the various agents and processes also became important. For example, we
had to adjust the time quantum given by the scheduler to our EA processes to ensure that both the
process receiving messages and the various P RS agent processes in our EA were executed frequently
enough for waypoint recalculation. This problem has been alleviated with more recent upgrades in
the onboard computer, but could recur if more computationally expensive projections or alerts are
added to the EA.

8. Related Work
Plan generation has received a lot of attention recently, but rarely are the plans used to control
and monitor execution. Even more rarely are plans monitored that involve the activity of hundreds
of agents requiring tight coordination. Previous work on execution monitoring has focused on
models where the executor performs the planned actions (e.g., a robot controller) and usually has
direct access to internal state information. In the SUO domain, most actions are performed by
external agents, usually humans, and the monitor has no access to the state of its executing agents.
Such indirect execution requires different monitoring techniques, as the executor must use incoming
messages to determine the status of agents and activities and whether actions have been initiated or
completed. The Continuous Planning and Execution Framework (Myers, 1999) has addressed the
indirect execution problem, and our system builds on its ideas. However, our domain requires
monitoring of many more constraints with greater time sensitivity. We have much higher rates of
incoming data, and must customize monitoring of each action to generate appropriate, high-value
alerts.
Robot designers have often avoided the plan representations used by the AI plan-generation
community because of their restrictive assumptions (Pollack & McCarthy, 1999; Arkin, 1998). Both
our domains required an expressive plan representation, and our combination of the Act formalism
with a hierarchical, object-oriented mission model proved sufficiently expressive, providing a rich
set of goal modalities for encoding activity, including notions of achievement, maintenance, testing,
conclusion, and waiting.
The SAM system (Kaminka & Tambe, 1999) at ISI addresses a similar problem: automated
pilot agents on a battlefield. SAM has direct access to its local automated agent and much lower
incoming data rates than the EA. It addresses the difficult problem of plan recognition (of the plans
of other friendly agents). Because humans are not involved, SAM does not need to produce alerts
tailored to human cognitive capabilities. Experiments with SAM showed that distributed monitoring
outperformed centralized monitoring while using simpler algorithms. Our EAs and SAIM use such
a distributed design, building on these insights.

254

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

More recent work at ISI has produced a monitoring agent named OVERSEER (Kaminka et al.,
2001), which also addresses a problem similar to ours: many geographically distributed team members with a coordinating plan in a dynamic environment. address the problem of modeling the value
of information to the user. OVERSEER does not use the report-based monitoring approach adopted
by our EAs, because it must rely on unmodifiable legacy agents and does not have sufficient bandwidth and reliability in communication. A detailed analysis is given in Section 3.
NASAs Remote Agent on Deep Space One (Jonsson et al., 2000; Muscettola et al., 1998) does
autonomous execution monitoring on a spacecraft. Our domains have many of the same requirements as NASAs, including the core requirements of concurrent temporal processes and interacting
recoveries. However, NASAs remote agent is fully automated, which places a heavier burden on
the module that generates plans and responses, but alleviates the burden of having to address human
interaction issues such as those considered in VOA. Monitoring algorithms are not described in detail, but are based on a procedural executive, which we assume is similar to our procedural reactive
control system. In NASAs domain, the agents are mechanical devices onboard the spacecraft,
and their behaviors have been formally modeled. Our agents include humans, whose behaviors are
not easily modeled, so our EAs estimate the value of alerts as they interact with a human decision
maker, who ultimately is responsible for the control decisions.
Work on rationale-based monitoring (Pollack & McCarthy, 1999; Veloso, Pollack, & Cox, 1998)
addressed the problem of monitoring the world during the plan generation process (in causal-link
planners) to see if events invalidate the plan being generated. They monitor subgoals, preconditions,
usability conditions, and user preferences. All these are monitored in our framework when plans are
executed, and our EAs have additional capabilities, such as monitoring policy constraints and applying mission-specific monitoring methods. This rationale-based work does not address time-critical
monitoring during execution time, monitoring large volumes of incoming data, or the problem of
alerting users without overwhelming them.
Doyle (1995) describes a technique to focus the users attention on anomalous system behavior,
particularly sensor behavior. This work would be applicable within the lowest layer in our robotics
control module. It uses causal modeling to understand the normal behavior of a sensor. Anomaly
detection is based on measures of causal distance and distance from normal behavior. The distance
measures are not related to the plan and its goals/actions; instead they measure deviation from typical behavior. The user still has to relate the reported sensor anomaly to its higher-level effects, such
as a threat to plan or action execution. This work provides a monitoring technique for specific sensor and system types that could easily be incorporated in our monitoring framework. The resulting
anomaly detection might give low-level alerts or be a contributory factor in the reasoning process
for higher-level alert classes.
The Phoenix system uses the concept of a plan envelope (Hart, Anderson, & Cohen, 1990)
to represent the a priori expectations of an actions progress. Envelopes are used when an action
executes over time and can be interrupted and altered during execution. The envelope captures
the range of possible performance of an action during successful execution. During execution, the
actual performance of the system is recorded and, if it deviates from the predefined envelope, a
possible failure is detected. This concept provides a useful monitoring technique for specific alerttypes, particularly those concerning actions that consume a variable amount of resources over time.
Envelopes can also identify when an action is performing better than required allowing opportunistic alerts. Envelopes could easily be incorporated in our monitoring framework as an additional
monitoring technique, and could be useful at the higher levels in both our domains.
255

fiW ILKINS , L EE , & B ERRY

The SUO EA provides a capability that does not currently exist, because there is no machineunderstandable representation of the plan on the battlefield. Currently, small-unit warfighters must
monitor all incoming information for relevance, with manual notification of other team members.
The SUO EA also improves on next-generation Army systems such as FBCB2 (Force XXI Battle
Command Brigade and Below) (Garamone, 2001). Unlike FBCB2, the EA alerts only on important
changes, can automatically update the areas to be monitored as the plan is executed, can dynamically change the force structure, and can alert the user to many issues that are not monitored in
other systems, such as fratricide risks, triggering of contingencies, and schedule, coordination and
positional deviations from the plan.

9. Conclusions
We characterized the domain-independent challenges posed by an execution aid that interactively
supports humans monitoring the activity of distributed teams of cooperating agents, both human and
machine. The most important issues for interactive monitoring are adaptivity, plan- and situationspecific monitoring, reactivity, and high-value, user-appropriate alerts. We showed how properties
of various domains influence these challenges and their solutions. We then presented a top-level
domain-independent categorization of the types of alerts a plan-based monitoring system might
issue to a user. The different monitoring techniques generally required for each category are often
domain specific and task specific.
Our monitoring framework integrates these various techniques and then uses the concept of
value of an alert to control interaction with the user. This conceptual framework facilitates integration of new monitoring techniques and provides a domain-independent context for future discussions of monitoring systems. We discussed various design tradeoffs that must be made during the
application of our monitoring framework to a domain (Sections 6.4 and 6.6).
We use this framework to describe a monitoring approach we developed and have used to implement Execution Assistants (EAs) in two different dynamic, data-rich, real-world domains. Our
approach is based on rich plan representations, which allow the execution aid to filter, interpret,
and react to the large volume of incoming information, and alert the user appropriately. An expressive plan representation is necessary for representing SUO plans, which must coordinate distributed
units, trigger contingencies, and enforce a variety of constraints. It is equally important that this
representation be monitorable by machines and meaningful to humans. Our plan representation
and mission model were able to model a representative SUO scenario with enough fidelity to provide value (as judged by our domain experts) and was also sufficient for plans in the UV-Robotics
domain.
We developed a sufficiently rich plan representation by extending an existing plan representation
with a hierarchical, object-oriented mission model that encodes knowledge about primitive actions
and mission-specific monitoring methods. The SUO EA implements a novel integration of these
hierarchical monitoring methods with a reactive control system. The EA invokes the most specific
methods defined in the hierarchy at appropriate points during monitoring.
One central challenge, in our domains as well as medical monitoring, is to avoid overwhelming
the user with unwanted alerts and/or false alarms. We define the concepts of value of information
and value of giving an alert as the principles for determining when to give an alert. We describe the
properties of VOI and VOA, criteria for computing them, the advantages of qualitative reasoning in

256

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

our domains, and the successful use of these concepts in our applications. VOI and VOA algorithms
must be customizable to the user, plan, and situation.
By using an asynchronous multiagent architecture and an extended version of the P RS reactive
control system, we monitored the execution of both SUO and UV-Robotics plans with acceptable
latency, given a dozen or more incoming events per second. P RS extensions include temporal monitors and efficiency improvements. Methods from the mission model are used throughout the SUO
monitoring process for action-specific monitoring. Our evaluation showed that our plan-aware EAs
generated appropriate alerts in a timely manner without overwhelming the user with too many alerts,
although a small percentage of the alerts were unwanted. We have shown the utility of using advanced AI planning and execution technologies in small unit operations.
The application to UV-Robotics showed the generality of our SUO framework and monitoring
concepts. We implemented a complex execution assistant in about one person-week, using code
from the SUO EA. The UV EA uses the same plan representation and basic architecture as the SUO
EA, but the inputs are different as are the tasks and the algorithms that respond to the inputs and
generate alerts.
Future work. The most obvious area for future work in the SUO domain is incorporation of
a planning assistant to complete the loop of continuous planning and execution. This integration
has already been accomplished in the UV-Robotics domain, but the difficulty in the SUO domain
is an interface that allows a soldier to interact effectively with the planning tool, using a wearable
computer in a battlefield situation. Several research programs are addressing this problem, some of
which are mentioned in Section 2.
Within the scope of execution monitoring, future work on our EAs could model and detect other
types of plan deviations (such as loss of surprise or additional types of fratricide risks), project
future failures, and provide higher-fidelity specialized reasoners, particularly for terrain reasoning.
Additional theoretical work on VOI and VOA would support better quantitative estimates of VOI
and VOA. The SUO mission model already has a method for projecting failures and a low-fidelity
projection capability could be easily added. In the UV-Robotics domain, we plan to implement
additional types of alerts in the near future, and extend the UV EA to serve the higher layers of the
architecture that have more in common with the SUO EA alert types and triggers. The fragility of
the UV communication network in hostile domains provides a set of interesting monitoring challenges that may result in the incorporation of specific monitoring-related tasks within cooperative
team missions. Monitoring strategies for uncertain communication environments is an important
research challenge for the UV-Robotics domain. Additional alerts being considered for future implementation include monitoring movement of entities in and out of geographical sectors mentioned
in the plan, monitoring the deterioration or improvement of communication conditions, and monitoring the actions and intentions of coordinating team members to facilitate cooperative behavior.

257

fiW ILKINS , L EE , & B ERRY

ACKNOWLEDGMENTS
The SUO research was supported by Contract F30602-95-C-0235 with the Defense Advanced Research Projects Agency (from the DARPA Planning and Decision Aids Program and the DARPA
Small Unit Operations Program), under the supervision of Air Force Research Laboratory  Rome.
The UCAV research was supported by the Office of Naval Research Unmanned Combat Air Vehicles Program (Contract N00014-00-C-0304). The SRI International Artificial Intelligence Center
supported the writing of this paper. We thank the subject matter experts who assisted us. Our primary collaborators and evaluators were Kenneth Sharpe of SAIC and Richard Diehl of the Institute
for Defense Analyses. We also used the expertise of Andy Fowles, Chris Kearns, and David Miller
of the U.S. Army Dismounted Battlespace Battle Laboratory (DBBL) at Fort Benning, and CPT
Dan Ray of the Mounted Maneuver Battlespace Laboratory (MMBL) at Fort Knox.

References
Arkin, R. (1998). Behavior-based robotics. MIT Press.
Ash, D., Gold, G., Seiver, A., & Hayes-Roth, B. (1993). Guaranteeing real-time response with
limited resources. Artificial Intelligence in Medicine, 5(1), 4966.
Athey, S., & Levin, J. (2001). The value of information in monotone decision problems. Tech. rep.,
Stanford University, Stanford, CA.
Bell, B., Jr., E. S., & Brown, S. M. (2002). Making adversary decision modeling tractable with intent
inference and information fusion. In Proc. of the 11th Conference on Computer Generated
Forces and Behavioral Representation, Orlando, FL.
Bonasso, R. P., Kortenkamp, D., & Whitney, T. (1997). Using a robot control architecture to automate space shuttle operations. In Proc. of the 1997 National Conference on Artificial Intelligence, pp. 949956, Providence, RI. AAAI Press.
Cheyer, A., & Martin, D. (2001). The open agent architecture. Journal of Autonomous Agents and
Multi-Agent Systems, 4(1), 143148.
Coiera, E. (1993). Intelligent monitoring and control of dynamic physiological systems. Artificial
Intelligence in Medicine, 5(1), 18.
Donlon, J., & Forbus, K. (1999). Using a geographic information system for qualitative spatial
reasoning about trafficability. In Proc. of the Qualitative Reasoning Workshop, Loch Awe,
Scotland.
Doyle, R. J. (1995). Determining the loci of anomalies using minimal causal models. In Proc. of
the 1995 International Joint Conference on Artificial Intelligence, pp. 18211827, Montreal,
Quebec, Canada. Morgan Kaufmann Publishers Inc., San Francisco, CA.
Durfee, E. H., Huber, M. J., Kurnow, M., & Lee, J. (1997). TAIPE: Tactical assistants for interaction
planning and execution. In Proc. of Autonomous Agents 97. ACM Press, New York.
Ferguson, G., & Allen, J. (1998). TRIPS: An integrated intelligent problem-solving assistant. In
Proc. of the 1998 National Conference on Artificial Intelligence, pp. 567572. AAAI Press.
Forbus, K. D. (2002). Towards Qualitative Modeling of the Battlespace. Technical report unpublished manuscript, Northwestern University, Evanston, IL.
258

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

Franke, J., Brown, S. M., Bell, B., & Mendenhall, H. (2000). Enhancing teamwork through teamlevel intent inference. In Proc. of the 2000 International Conference on Artificial Intelligence,
Las Vegas, NV.
Garamone, J. (2001). Digital world meets combat during desert exercise. Tech. rep., American
Forces Information Service, www.defenselink.mil/news/Apr2001/.
Georgeff, M. P., & Ingrand, F. F. (1989). Decision-making in an embedded reasoning system. In
Proc. of the 1989 International Joint Conference on AI, pp. 972978, Detroit, MI. Morgan
Kaufmann Publishers Inc., San Francisco, CA.
Gil, Y., & Blythe, J. (1999). A problem-solving method for plan evaluation and critiquing. In Proc.
of the Tenth Banff Knowledge Acquisition for Knowledge-Based Systems Workshop, Banff,
Alberta, Canada.
Grosz, B., & Kraus, S. (1999). The evolution of SharedPlans. In Rao, A., & Wooldridge, M. (Eds.),
Foundations and Theories of Rational Agencies, pp. 227262.
Hart, D. M., Anderson, S. D., & Cohen, P. R. (1990). Envelopes as a vehicle for improving the
efficiency of plan execution. Tech. rep. UM-CS-1990-021, University of Massachusetts,
Amherst, MA.
Holness, G., Karuppiah, D.and Uppala, S., & Ravela, S. C. (2001). A service paradigm for reconfigurable agents. In Proc. of the 2nd Workshop on Infrastructure for Agents, MAS, and Scalable
MAS (Agents 2001), Montreal, Canada.
Horty, J., & Pollack, M. (2001). Evaluating new options in the context of existing plans. Artificial
Intelligence, 127(2), 199220.
Jonsson, A., Morris, P., Muscettola, N., & Rajan, K. (2000). Planning in interplanetary space:
Theory and practice. In Proc. of the 2000 International Conference on AI Planning and
Scheduling, pp. 177186, Breckenridge, CO. AAAI Press, Menlo Park, CA.
Kaminka, G., Pynadath, D., & Tambe, M. (2001). Monitoring deployed agent teams. In Proc. of
Autonomous Agents 01, pp. 308315, Montreal, Canada.
Kaminka, G., & Tambe, M. (1999). Experiments in distributed and centralized socially attentive
monitoring. In Proc. of Autonomous Agents 99, pp. 213220, Seattle, WA.
Konolige, K., & Myers, K. (1998). Artificial Intelligence Based Mobile Robots: Case studies of
Successful Robot Systems, chap. The Saphira architecture: a design for autonomy. MIT Press.
Koski, E., Makivirta, A., Sukuvaara, T., & Kari, A. (1990). Frequency and reliability of alarms in
the monitoring of cardiac postoperative patients. International Journal of Clinical Monitoring
and Computing, 7, 129133.
Mouaddib, A.-I., & Zilberstein, S. (1995). Knowledge-based anytime computation. In Proc. of the
1995 International Joint Conference on Artificial Intelligence, pp. 775783. Morgan Kaufmann Publishers Inc., San Francisco, CA.
Muscettola, N., Nayak, P. P., Pell, B., & Williams, B. C. (1998). Remote agent: To boldly go where
no AI system has gone before. Artificial Intelligence, 103(1-2), 547.
Musliner, D. J., Durfee, E. H., & Shin, K. G. (1993). CIRCA: A cooperative intelligent real-time
control architecture. IEEE Transactions on Systems, Man, and Cybernetics, 23(6).
259

fiW ILKINS , L EE , & B ERRY

Myers, K. L. (1996). A procedural knowledge approach to task-level control. In Proc. of the 1996
International Conference on AI Planning Systems. AAAI Press, Menlo Park, CA.
Myers, K. L., & Morley, D. N. (2001). Human directability of agents. In Proc. 1st International
Conference on Knowledge Capture, Victoria, B.C.
Myers, K. L. (1999). CPEF: A continuous planning and execution framework. AI Magazine, 20,
6370.
Ortiz, C., Agno, A., Berry, P., & Vincent, R. (2002). Multilevel adaptation in teams of unmanned
air and ground vehicles. In First AIAA Unmanned Aerospace Vehicles, Systems, Technologies
and Operations Conference.
Ortiz, C. L. (1999). Introspective and elaborative processes in rational agents. Annals of Mathematics and Artificial Intelligence, 25(12), 134.
Ortiz, C. L., & Hsu, E. (2002). Structured negotiation. In Proc. of the First International Conference
on Autonomous Agents and Multiagent Systems.
Pollack, M. E., & McCarthy, C. (1999). Towards focused plan monitoring: A technique and an
application to mobile robots. In Proc. of the IEEE International Symposium on Computational
Intelligence in Robotics and Automation (CIRA), pp. 144149.
Rao, A. S., & Georgeff, M. P. (1995). BDI-agents: From theory to practice. In Proc. of the First
Intl. Conference on Multiagent Systems, San Francisco.
Schreckenghost, D., & et al. (2001). Adjustable control autonomy for anomaly response in spacebased life support systems. In Proc. of the IJCAI Workshop on Autonomy, Delegation, and
Control.
Shannon, C. (1948). A mathematical theory of communication. Bell System Technical Journal, 27,
379423, 623656.
Tsien, C. (1997). Reducing false alarms in the intensive care unit: A systematic comparison of four
algorithms. In Proc. of the American Medical Informatics Association Annual Fall Symposium.
Tsien, C., & Fackler, J. (1997). Poor prognosis for existing monitors in the intensive care unit.
Critical Care Medicine, 25(4), 614619.
Veloso, M., Pollack, M., & Cox, M. (1998). Rationale-based monitoring for planning in dynamic
environments. In Proc. of the 1998 International Conference on AI Planning Systems, pp.
171180. AAAI Press, Menlo Park, CA.
Vincent, R., Horling, B., Lesser, V., & Wagner, T. (2001). Implementing soft real-time agent control.
In Proceedings of the 5th International Conference on Autonomous Agents. ACM Press.
Weigner, M. B., & Englund, C. E. (1990). Ergonomic and human factors affecting anesthetic vigilance and monitoring performance in the operating room environment. Anesthesiology, 73(5),
9951021.
Weinberger, E. (2002). A theory of pragmatic information and its application to the quasispecies
model of biological evolution. Biosystems, 66(3), 105119.
Wilkins, D. E., & desJardins, M. (2001). A call for knowledge-based planning. AI Magazine, 22(1),
99115.
260

fiI NTERACTIVE E XECUTION M ONITORING OF AGENT T EAMS

Wilkins, D. E., & Myers, K. L. (1995). A common knowledge representation for plan generation
and reactive execution. Journal of Logic and Computation, 5(6), 731761.
Wilkins, D. E., & Myers, K. L. (1998). A multiagent planning architecture. In Proc. of the 1998
International Conference on AI Planning Systems, pp. 154162, Pittsburgh, PA.
Wilkins, D. E., Myers, K. L., Lowrance, J. D., & Wesley, L. P. (1995). Planning and reacting
in uncertain and dynamic environments. Journal of Experimental and Theoretical AI, 7(1),
121152.

261

fiJournal of Artificial Intelligence Research 18 (2003) 491-516

Submitted 11/02; published 6/03

Acquiring Correct Knowledge
for Natural Language Generation
Ehud Reiter
Somayajulu G. Sripada

ereiter@csd.abdn.ac.uk
ssripada@csd.abdn.ac.uk

Department of Computing Science,
University of Aberdeen, Aberdeen AB24 3UE, UK

Roma Robertson

roma.robertson@ed.ac.uk

Division of Community Health Sciences - General Practice Section
University of Edinburgh
Edinburgh EH8 9DX, UK

Abstract
Natural language generation (nlg) systems are computer software systems that produce texts in English and other human languages, often from non-linguistic input data.
nlg systems, like most ai systems, need substantial amounts of knowledge. However, our
experience in two nlg projects suggests that it is difficult to acquire correct knowledge
for nlg systems; indeed, every knowledge acquisition (ka) technique we tried had significant problems. In general terms, these problems were due to the complexity, novelty,
and poorly understood nature of the tasks our systems attempted, and were worsened by
the fact that people write so differently. This meant in particular that corpus-based ka
approaches suffered because it was impossible to assemble a sizable corpus of high-quality
consistent manually written texts in our domains; and structured expert-oriented ka techniques suffered because experts disagreed and because we could not get enough information
about special and unusual cases to build robust systems. We believe that such problems
are likely to affect many other nlg systems as well. In the long term, we hope that new
ka techniques may emerge to help nlg system builders. In the shorter term, we believe
that understanding how individual ka techniques can fail, and using a mixture of different
ka techniques with different strengths and weaknesses, can help developers acquire nlg
knowledge that is mostly correct.

1. Introduction
Natural language generation (nlg) systems use artificial intelligence (ai) and natural language processing techniques to automatically generate texts in English and other human
languages, typically from some non-linguistic input data (Reiter & Dale, 2000). As with
most ai systems, an essential part of building an nlg system is knowledge acquisition (ka),
that is acquiring relevant knowledge about the domain, the users, the language used in the
texts, and so forth.
ka for nlg can be based on structured expert-oriented techniques, such as think-aloud
protocols and sorting, or on machine learning and corpus analysis, which are currently very
popular in other areas of Natural Language Processing. We have used both types of techniques in two nlg projects that included significant ka efforts  stop (Reiter, Robertson, &
Osman, 2003), which generated tailored smoking cessation letters, and SumTime-Mousam
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiReiter, Sripada, & Robertson

(Sripada, Reiter, Hunter, Yu, & Davy, 2001), which generated weather forecasts. In both
projects, and for all techniques tried, the main problem turned out to be knowledge quality;
evaluation and validation exercises identified flaws in the knowledge acquired using every
technique. The flaws were due to a variety of factors, but perhaps the basic underlying
reason for them was the nature of the writing tasks we were attempting to automate. They
were:
 complex (as are many tasks that involve interacting with humans): hence a lot of
knowledge was needed to cover the numerous special cases and unusual circumstances;
 sometimes novel (not done by humans): hence sometimes there were no experts at
the task as a whole, and no existing corpora of texts to analyse;
 poorly understood: hence we did not have good theoretical models to structure the
knowledge being acquired, and fill in gaps in the knowledge acquired from experts or
corpora; and
 ambiguous (allowed multiple solutions): hence different experts and corpus authors
produced very different texts (solutions) from the same input data.
These problems of course occur to some degree in ka for other expert system and natural
language processing tasks, but we believe they may be especially severe for nlg.
We do not have a good solution for these problems, and indeed believe that ka is one
of the biggest problems in applied nlg. After all, there is no point in using ai techniques
to build a text-generation system if we cannot acquire the knowledge needed by the ai
techniques.
In the longer term, more basic research into ka for nlg is badly needed. In the shorter
term, however, we believe that developers are more likely to acquire correct knowledge
when building an nlg system if they understand likely types of errors in the knowledge
acquired from different ka techniques. Also, to some degree the different ka techniques we
have tried have complementary strengths and weaknesses; this suggests using a variety of
different techniques, so that the weaknesses of one technique are compensated for by the
strengths of other techniques.
In the remainder of this paper we give background information on nlg, ka, and our
systems; describe the various ka techniques we used to build our systems and the problems
we encountered; and then discuss more generally why ka for nlg is difficult and how
different ka techniques can be combined.

2. Background
In this section we give some background information on natural language generation and
knowledge acquistion and validation. We also introduce and briefly describe the stop and
SumTime-Mousam systems.

492

fiAcquiring Correct Knowledge for NLG

2.1 Natural Language Generation
Natural Language Generation is the subfield of artificial intelligence that is concerned with
automatically generating written texts in human languages, often from non-linguistic input
data. nlg systems often have three stages (Reiter & Dale, 2000):
 Document Planning decides on the content and structure of the generated text; for
example that a smoking-cessation letter should start with a section that discusses the
pros and cons of smoking.
 Microplanning decides on how information and structure should be expressed linguistically; for example, that the phrase by mid afternoon should be used in a weather
report to refer to the time 1500.
 Surface Realisation generates an actual text according to the decisions made in previous stages, ensuring that the text conforms to the grammar of the target language
(English in our systems).
nlg systems require many types of knowledge in order to carry out these tasks. In particular, Kittredge, Korelsky, and Rambow (1991) point out that nlg systems need domain
knowledge (similar to that needed by expert systems), communication knowledge (similar
to that needed by other Natural Language Processing systems), and also domain communication knowledge (DCK). DCK is knowledge about how information in a domain is usually
communicated, including standard document structures, sublanguage grammars, and specialised lexicons. DCK plays a role in all aspects of language technology (for example, a
speech recogniser will work better in a given domain if it is trained on a corpus of texts
from that domain), but it may be especially important in nlg.
2.2 Knowledge Acquisition and Validation
Knowledge acquisition is the subfield of artificial intelligence that is concerned with acquiring the knowledge needed to build ai systems. Broadly speaking the two most common
types of ka techniques are:
 Techniques based on working with experts in a structured fashion, such as structured interviews, think-aloud protocols, sorting, and laddered grids (Scott, Clayton,
& Gibson, 1991; Buchanan & Wilkins, 1993); and
 Techniques based on learning from data sets of correct solutions (such as text corpora);
these are currently very popular in natural language processing and used for many
different types of knowledge, ranging from grammar rules to discourse models (for an
overview, see Jurafsky & Martin, 2000).
There are of course other possible ka techniques as well, including directly asking experts
for knowledge, and conducting scientific experiments. Some research has been done on
evaluating and comparing ka techniques, but such research can be difficult to interpret
because of methodological problems (Shadbolt, OHara, & Crow, 1999).
Research has also been done on verifying and validating knowledge to check that it
is correct (Adelman & Riedel, 1997). Verification techniques focus on detecting logical
493

fiReiter, Sripada, & Robertson

anomalies and inconsistencies that often reflect mistakes in the elicitation or coding process;
we will not further discuss these, as such errors are not our primary concern in this paper.
Validation techniques focus on detecting whether the knowledge acquired is indeed correct
and will enable the construction of a good system; these are very relevant to efforts to
detect problems in knowledge acquired for nlg. Adelman and Riedel (1997) describe two
general types of validation techniques: (1) having experts check the acquired knowledge and
built systems, and (2) using a library of test cases with known inputs and outputs. In other
words, just as knowledge can be acquired from experts or from data sets of correct solutions,
knowledge can also be validated by experts or by data sets of correct solutions. Knowledge
can also be validated experimentally, by determining if the system as a whole works and has
the intended effect on its users. Of course care must be taken that the validation process
uses different resources than the acquisition process. For example, knowledge acquired from
an expert should not be validated by that expert, and knowledge learned from a data set
should not be validated by that data set.
There has not been a great deal of previous research on knowledge acquisition for nlg;
Reiter, Robertson, and Osman (2000) summarise previous efforts in this area. Generally
corpus analysis (analysis of collections of manually written texts) has been the most popular
ka technique for nlg, as in other areas of Natural Language Processing, although sometimes
it is supplemented by expert-oriented techniques (Goldberg, Driedger, & Kittredge, 1994;
McKeown, Kukich, & Shaw, 1994). Walker, Rambow, and Rogati (2002) have attempted
to learn nlg rules from user ratings of generated texts, which can perhaps be considered a
type of experiment-based ka.
2.3 STOP
stop (Reiter, Robertson, & Osman, 2003) is an nlg system that generates tailored smokingcessation letters. Tailoring is based on a 4-page multiple-choice questionnaire about the
smokers habits, health, concerns, and so forth. An extract from a questionnaire is shown
in Figure 1, and an extract from the stop letter generated from this questionnaire is shown
in Figure 2 (we have changed the name of the smoker to preserve confidentiality). From a
ka perspective, the most important knowledge needed in stop is what content and phrasing
is appropriate for an individual smoker; for example,
 What information should be given in a letter? The example letter in Figure 2, for
instance, emphasises things the smoker dislikes about smoking, confidence building,
and dealing with stress and weight gain; but it does not recommend specific techniques
for stopping smoking.
 Should a letter adopt a positive youll feel better if you stop tone (as done in the
letter in Figure 2), or should it adopt a negative smoking is killing you tone?
stop was never operationally deployed, but it was tested with real smokers in a clinical
trial, during which 857 smokers received stop letters (Lennox, Osman, Reiter, Robertson,
Friend, McCann, Skatun, & Donnan, 2001). This evaluation, incidentally, showed that stop
letters were no more effective than control non-tailored letters.

494

fiAcquiring Correct Knowledge for NLG

SMOKING QUESTIONNAIRE

Please answer by marking the most appropriate box for each question like this: _

Q1 Have you smoked a cigarette in the last week, even a puff?
YES _
Please complete the following questions

Please read the questions carefully.
Q2

Home situation:
Live
_
alone



NO

Please return the questionnaire unanswered in the
envelope provided. Thank you.
If you are not sure how to answer, just give the best answer you can.

Live with

husband/wife/partner



Live with
other adults

0 boys

0. girls

Q3

Number of children under 16 living at home

Q4

Does anyone else in your household smoke? (If so, please mark all boxes which apply)
husband/wife/partner 
other family member 
others 

Q5



Live with
children

How long have you smoked for? 20 years
Tick here if you have smoked for less than a year


Q6

How many cigarettes do you smoke in a day? (Please mark the amount below)

Less than 5 
Q7

5  10 

11  15 _

16  20 

21 - 30 

31 or more

How soon after you wake up do you smoke your first cigarette? (Please mark the time below)

Within 5 minutes 

6 - 30 minutes _

31 - 60 minutes 

Q8

Do you find it difficult not to smoke in places where it is
forbidden eg in church, at the library, in the cinema?

Q9

Which cigarette would you hate most to give up?

Q10

Do you smoke more frequently during the first hours after
waking than during the rest of the day?

Q11 Do you smoke if you are so ill that you are in bed most of the
day?
Q12
Are you intending to stop
smoking in the next 6
months?

YES



NO

_

After 60 minutes 
YES

_NO 

The first one in the morning _
Any of the others 
YES

NO

_

YES

NO

_

Q13 If yes, are you intending to stop smoking
within the next month?
YES NO 
Q14 If no, would you like to stop smoking if it was
easy?
YES Not Sure _
NO 




1

Figure 1: First page of example smoker questionnaire

495

fiSmoking Information for Heather Stewart
We know that all of these make it more likely that you will be able to stop.
Most people who stop smoking for good have more than one attempt.

People stop smoking when they really want to stop. It is encouraging that
you have many good reasons for stopping. The scales show the good
and bad things about smoking for you. They are tipped in your favour.

Overcoming your barriers to stopping...

THINGS YOU LIKE

it's relaxing
it stops stress
you enjoy it
it relieves boredom
it stops weight gain
it stops you craving

THINGS YOU DISLIKE
it makes you less fit
it's a bad example for kids
you're addicted
it's unpleasant for others
other people disapprove
it's a smelly habit
it's bad for you
it's expensive
it's bad for others' health

You could do it...
Most people who really want to stop eventually succeed. In fact, 10
million people in Britain have stopped smoking - and stayed stopped - in
the last 15 years. Many of them found it much easier than they expected.
Although you don't feel confident that you would be able to stop if you
were to try, you have several things in your favour.




You have stopped before for more than a month.
You have good reasons for stopping smoking.
You expect support from your family, your friends, and your
workmates.

You said in your questionnaire that you might find it difficult to stop
because smoking helps you cope with stress. Many people think that
cigarettes help them cope with stress. However, taking a cigarette only
makes you feel better for a short while. Most ex-smokers feel calmer and
more in control than they did when they were smoking. There are some
ideas about coping with stress on the back page of this leaflet.
You also said that you might find it difficult to stop because you would put
on weight. A few people do put on some weight. If you did stop smoking,
your appetite would improve and you would taste your food much better.
Because of this it would be wise to plan in advance so that you're not
reaching for the biscuit tin all the time. Remember that putting on weight
is an overeating problem, not a no-smoking one. You can tackle it later
with diet and exercise.

And finally...
We hope this letter will help you feel more confident about giving up
cigarettes. If you have a go, you have a real chance of succeeding.
With best wishes,
The Health Centre.

Reiter, Sripada, & Robertson

496

Figure 2: Extract from letter generated from Figure 1 questionnaire

You have good reasons to stop...

fiAcquiring Correct Knowledge for NLG

day

hour

12-06-02
12-06-02
12-06-02
12-06-02
12-06-02
12-06-02
13-06-02

6
9
12
15
18
21
0

wind
direction
WSW
WSW
WSW
WSW
SW
SSW
SSW

wind speed
(10m altitude)
10
9
7
7
7
8
10

wind speed
(50m alt)
12
11
9
9
9
10
12

Figure 3: Wind data extract from 12-Jun-2002 numerical weather prediction
Knowledge acquisition in stop was primarily based on structured expert-oriented ka
techniques, including in particular sorting and think-aloud protocols. Knowledge was acquired from five health professionals; three doctors, a nurse, and a health psychologist.
These experts were knowledgeable about smoking and about patient information, but they
were not experts on writing tailored smoking-cessation letters. In fact there are no experts
at this task, since no one manually writes tailored smoking-cessation letters.
It is not unusual for an nlg system to attempt a task which is not currently performed by
human experts; other examples include descriptions of software models (Lavoie, Rambow,
& Reiter, 1997), customised descriptions of museum items (Oberlander, ODonnell, Knott,
& Mellish, 1998), and written feedback for adult literacy students (Williams, Reiter, &
Osman, 2003). Knowledge validation in stop was mostly based on feedback from users
(smokers), and on the results of the clinical trial.
2.4 SumTime-Mousam
SumTime-Mousam (Sripada, Reiter, Hunter, & Yu, 2002) is an nlg system that generates
marine weather forecasts for offshore oil rigs, from numerical weather simulation data. An
extract from SumTime-Mousams input data is shown in Figure 3, and an extract from the
forecast generated from this data is shown in Figure 4. From a ka perspective, the main
knowledge needed by SumTime-Mousam was again what content and expression was best
for users; for example,
 What changes in a meteorological parameter are significant enough to be reported in
the text? The forecast in Figure 4, for example, mentions changes in wind direction
but not changes in wind speed.
 What words and phrases should be used to communicate time? For example, should
1800 be described as early evening (as in Figure 4) or as late afternoon?
SumTime-Mousam is currently being used operationally by a meteorological company, to
generate draft forecasts which are post-edited by human forecasters.
Knowledge acquisition in SumTime-Mousam was based on both corpus analysis of
manually-written forecasts and structured ka with expert meteorologists. Unlike the experts we worked with in stop, the meteorologists we worked with in SumTime-Mousam

497

fiReiter, Sripada, & Robertson

FORECAST 6 - 24 GMT, Wed 12-Jun 2002
WIND(KTS)
10M: WSW 8-13 gradually backing SW by early evening and SSW by
midnight.
50M: WSW 10-15 gradually backing SW by early evening and SSW by
midnight.
WAVES(M)
SIG HT: 0.5-1.0 mainly SW swell.
MAX HT: 1.0-1.5 mainly SW swell.
PER(SEC)
WAVE PERIOD: Wind wave 3-5 mainly 6 second SW swell.
WINDWAVE PERIOD: 3-5.
SWELL PERIOD: 5-7.
WEATHER:
Partly cloudy becoming overcast with light rain around midnight.
VIS(NM):
Greater than 10 reduced to 5-8 in precipitation.
AIR TEMP(C): 8-10 rising 9-11 around midnight.
CLOUD(OKTAS/FT): 2-4 CU/SC 1300-1800 lowering 7-8 ST/SC 700-900 around
midnight.
Figure 4: Extract from forecast generated for 12-Jun-2002
were experienced at writing the target texts (weather forecasts). The forecast corpus included the numerical weather simulation data that the forecasters used when writing the
forecasts, as well as the actual forecast texts (Sripada, Reiter, Hunter, & Yu, 2003).
Knowledge validation in SumTime-Mousam has mostly been conducted by checking
knowledge acquired from the corpus with the experts, and checking knowledge acquired
from the experts against the corpus. In other words, we have tried to make the validation
technique as different as possible from the acquisition technique. We are currently evaluating
SumTime-Mousam as a system by measuring the number of edits that forecasters make
to the computer-generated draft forecasts.

3. Knowledge Acquisition Techniques Tried
In this section we summarise the main ka techniques we used in stop and SumTimeMousam. For each technique we give an example of the knowledge acquired, and discuss
what we learned when we tried to validate the knowledge. Table 1 gives a very high level
overview of the major advantages and disadvantages of the different techniques we tried,
when the different techniques were perhaps most useful, and what types of knowledge they
were best suited to acquiring (using the classification of Section 2.1). As this table shows,
no one technique is clearly best; they all have different strengths and weaknesses. Probably
the best overall ka strategy is to use a mix of different techniques; we will further discuss
this in Section 5.

498

fiAcquiring Correct Knowledge for NLG

Techniques

Advantages

Disadvantages

directly ask
experts
structured ka
with experts
corpus
analysis

get big picture

many gaps, may
not match practice
limited coverage,
experts variable
hard to create,
texts inconsistent,
poor models for nlg
local optimisation,
not major changes

expert
revision

get details,
get rationale
get lots of
knowledge
quickly
fix problems
in knowledge

When
Useful
initial
prototype
flesh out
prototype
robustness,
unusual cases

Types of
Knowledge
domain,
DCK
depends
on expert
DCK,
communication

improve
system

all

Table 1: Summary Evaluation of ka techniques for nlg
3.1 Directly Asking Experts for Knowledge
The simplest and perhaps most obvious ka technique for nlg is to simply ask experts how
to write the texts in question. In both stop and SumTime-Mousam, experts initially
gave us spreadsheets or flowcharts describing how they thought texts should be generated.
In both projects, it also turned out that the experts description of how texts should be
generated did not in fact match how people actually wrote the texts in question. This is a
common finding in ka, and it is partially due to the fact that it is difficult for experts to
introspectively examine the knowledge they use in practice (Anderson, 1995); this is why
proponents of expert-oriented ka prefer structured ka techniques.
For example, at the beginning of SumTime-Mousam, one of the meteorologists gave
us a spreadsheet which he had designed, which essentially encoded how he thought some
parts of weather forecasts should be generated (the spreadsheet did not generate a complete
weather forecast). We analysed the logic used in the spreadsheet, and largely based the first
version of SumTime-Mousam on this logic.
One goal of our analysis was to create an algorithm that could decide when a change
in a parameter value was significant enough so that it should be mentioned in the weather
report. The spreadsheet used context-dependent change thresholds to make this decision.
For example, a change in the wind speed would be mentioned if
 the change was 10 knots or more, and the final wind speed was 15 knots or less;
 the change was 5 knots or more, and the final wind speed was between 15 and 40
knots; or
 the change was 10 knots or more, and the final wind speed was over 40 knots.
The context-dependent thresholds reflect the usage of the weather reports by the users (in
this case, oil company staff making decisions related to North Sea offshore oil rigs). For
example, if a user is deciding how to unload a supply boat, moderate changes in wind
speed dont matter at low speeds (because light winds have minimal impact on supply
boat operations) and at high speeds (because the boat wont even attempt to unload in
very heavy winds), but may affect decisions at in-between speeds. The context-dependent
499

fiReiter, Sripada, & Robertson

thresholds would be expected to vary according to the specific forecast recipient, and should
be set in consultation with the recipient.
From our perspective, there were two main pieces of knowledge encoded in this algorithm:
1. The absolute size of a change determines whether it should be mentioned or not, and
2. The threshold for significance depends on the context and ultimately on how the user
will use the information.
3.1.1 Validation of Direct Expert Knowledge
We checked these rules by comparing them to what we observed in our corpus analysis of
manually written forecasts (Section 3.3). This suggested that while (2) above is probably
correct, (1) may be incorrect. In particular, a linear segmentation model (Sripada et al.,
2002), which basically looks at changes in slope rather than changes in the absolute value of a
parameter, better matches the corpus texts. The expert who designed the spreadsheet model
agreed that segmentation was probably a better approach. He also essentially commented
that one reason for his use of the absolute size model was that this was something that was
easily comprehensible to someone who was neither a programmer nor an expert at numerical
data analysis techniques.
In other words, in addition to problems in introspecting knowledge, it also perhaps is
not reasonable to expect a domain expert to be able to write a sophisticated data analysis
algorithm based on his expertise. This is not an issue if the knowledge needed is purely
declarative, as it is in many ai applications; but if we need procedural or algorithmic
knowledge, we must bear in mind that domain experts may not have sufficient computational
expertise to express their knowledge as a computer algorithm.
3.1.2 Role of Directly Asking Experts for Knowledge
Although the experts spreadsheet in SumTime-Mousam was far from ideal, it was extremely useful as a starting point. It specified an initial system which we could build fairly
easily, and which produced at least vaguely plausible output. Much the same in fact happened in stop, when one of the doctors gave us a flowchart which certainly had many
weaknesses, but which was useful as an initial specification of a relatively easy-to-build and
somewhat plausible system. In both stop and SumTime-Mousam, as indeed in other nlg
projects we have been involved in, having an initial prototype system working as soon as
possible was very useful for developing our ideas and for explaining to domain experts and
other interested parties what we were trying to do.
In terms of the types of knowledge mentioned in Section 2.1, both the stop flowchart
and the SumTime-Mousam spreadsheet specified domain knowledge (for example, how
smokers should be categorised) and domain communication knowledge (for example, the use
of ranges instead of single numbers to communicate wind speed). The stop flowchart did not
specify any generic communication knowledge such as English grammar and morphology;
the author probably believed we knew more about such things than he did. The SumTimeMousam spreadsheet did in effect include a few English grammar rules, but these were just
to get the spreadsheet to work, the author did not have much confidence in them.
500

fiAcquiring Correct Knowledge for NLG

In summary, we think directly asking experts for knowledge is an excellent way to
quickly build an initial system, especially if the nlg developers can supply communication
knowledge that the domain expert may not possess. But once the initial system is in place,
it is probably best to use other ka techniques, at least in poorly understood areas such as
nlg. However, in applications where there is a solid theoretical basis, and the expert can
simply say build your system according to theory X, an experts direct knowledge may
perhaps be all that is needed.
3.2 Structured Expert-Oriented KA: Think-Aloud Protocols
There are numerous types of structured expert-oriented ka techniques, including thinkaloud protocols, sorting, and structured interviews (Scott et al., 1991). We will focus here
on think-aloud protocols, which is the technique we have used the most. We have tried
other structured ka techniques as well, such as sorting (Reiter et al., 2000); we will not
describe these here, but our broad conclusions about other structured ka techniques were
similar to our conclusions about think-aloud protocols.
In a think-aloud protocol, an expert carries out the task in question (in our case, writing
a text) while thinking aloud into an audio (or video) recorder. We used think-aloud
protocols in both stop and SumTime-Mousam. They were especially important in stop,
where they provided the basis for most content and phrasing rules.
A simple example of the think-aloud process is as follows. One of the doctors wrote a
letter for a smoker who had tried to stop before, and managed to stop for several weeks before
starting again. The doctor made the following comments in the think-aloud transcript:
Has he tried to stop smoking before? Yes, and the longest he has managed
to stop  he has ticked the one week right up to three months and thats
encouraging in that he has managed to stop at least once before, because it is
always said that the people who have had one or two goes are more likely to
succeed in the future.
He also included the following paragraph in the letter that he wrote for this smoker:
I see that you managed to stop smoking on one or two occasions before but have
gone back to smoking, but you will be glad to know that this is very common
and most people who finally stop smoking have had one or two attempts in the
past before they finally succeed. What it does show is that you are capable of
stopping even for a short period, and that means you are much more likely to be
able to stop permanently than somebody who has never ever stopped smoking
at all.
After analysing this session, we proposed two rules:
 IF (previous attempt to stop) THEN (message: more likely to succeed)
 IF (previous attempt to stop) THEN (message: most people who stop have a few
unsuccessful attempts first)

501

fiReiter, Sripada, & Robertson

The final system incorporated a rule (based on several ka sessions, not just the above
one) that stated that if the smoker had tried to stop before, and if the letter included a
section on confidence building, then the confidence-building section should include a short
message about previous attempts to stop. If the smoker had managed to quit for more than
one week, this should be mentioned in the message; otherwise the message should mention
the recency of the smokers previous cessation attempt if this was within the past 6 months.
The actual text generated from this rule in the example letter of Figure 2 is
Although you dont feel confident that you would be able to stop if you were to
try, you have several things in your favour.
 You have stopped before for more than a month.
Note that the text produced by the actual stop code is considerably simpler than the
text originally written by the expert. This is fairly common, as are simplifications in the
logic used to decide whether to include a message in a letter or not. In many cases this is
due to the expert having much more knowledge and expertise than the computer system
(Reiter & Dale, 2000, pp 3036). In general, the process of deriving implementable rules
for nlg systems from think-aloud protocols is perhaps more of an art than a science, not
least because different experts often write texts in very different ways.
3.2.1 Validation of Structured KA Knowledge
We attempted to verify some of the rules acquired from stop think-aloud sessions by performing a series of small experiments where we asked smokers to comment on a letter, or to
compare two versions of a letter. Many of the rules were supported by these experiments for example, people in general liked the recap of smoking likes and dislikes (see You have
good reasons to stop. . . section of Figure 2). However, one general negative finding
of these experiments was that the tailoring rules were insufficiently sensitive to unusual or
atypical aspects of individual smokers; and most smokers were probably unusual or atypical
in some way. For example, stop letters did not go into the medical details of smoking (as
none of the think-aloud expert-written letters contained such information), and while this
seemed like the right choice for many smokers, a few smokers did say that they would have
liked to see more medical information about smoking. Another example is that (again based
on the think-aloud sessions) we adopted a positive tone and did not try to scare smokers;
and again this seemed right for most smokers, but some smokers said that a more brutal
approach would be more effective for them.
The fact that our experts did not tailor letters in such ways may possibly reflect the
fact that such tailoring would not have been appropriate for the relatively small number of
specific cases they considered in our think-aloud sessions. We had 30 think-aloud sessions
with experts, who looked at 24 different smoker questionnaires (6 questionnaires were considered by two experts). This may sound like a lot, but it is a drop in the ocean when we
consider how tremendously variable people are.
Comments made by smokers during the stop clinical trial (Reiter, Robertson, & Osman, 2003) also revealed some problems with think-aloud derived rules. For example, we
decided not to include practical how-to-stop information in letters for people not currently intending to stop smoking; smoker comments suggest that this was a mistake. In
502

fiAcquiring Correct Knowledge for NLG

fact, some experts did include such information in think-aloud letters for such people, and
some did not. Our decision not to include this information was influenced by the Stages
of Change theoretical model (Prochaska & diClemente, 1992) of behaviour change, which
states that how-to-stop advice is inappropriate for people not currently intending to stop;
in retrospect, this decision was probably a mistake.
We repeated two of our think-aloud exercises 15 months after we originally performed
them; that is, we went back to one of our experts and gave him two questionnaires he had
analysed 15 months earlier, and asked him to once again think aloud while writing letters
based on the questionnaires. The letters that the expert wrote in the second session were
somewhat different from the ones he had originally written, and were preferred by smokers
over the letters he had originally written (Reiter et al., 2000). This suggests that our experts
were not static knowledge sources, but were themselves learning about the task of writing
tailored smoking-cessation letters during the course of the project. Perhaps this should not
be a surprise given that none of the experts had ever attempted to write such letters before
getting involved with our project.
3.2.2 Role of Structured Expert-Oriented KA
Structured expert-oriented ka was certainly a useful way to expand, refine, and generally
improve initial prototypes constructed on the basis of experts direct knowledge. By focusing
on actual cases and by structuring the ka process, we learned many things which the
experts did not mention directly. We obtained all the types of knowledge mentioned in
Section 2.1, by working with experts with the relevant expertise. For example in stop we
acquired domain knowledge (such as the medical effects of smoking) from doctors, domain
communication knowledge (such as which words to use) from a psychologist with expertise
in writing patient information leaflets, and communication knowledge about graphic design
and layout from a graphic designer.
However, structured expert-oriented ka did have some problems, including in particular coverage and variability. As mentioned above, 30 sessions that examined 24 smoker
questionnaires could not possibly give good coverage of the population of smokers, given
how complex and variable people are. As for variation, the fact that different experts wrote
texts in very different ways made it difficult to extract rules from the think-aloud protocols.
We undoubtably made some mistakes in this regard, such as not giving how-to-stop information to people not currently intending to stop smoking. Perhaps we should have focused
on a single expert in order to reduce variation. However, our experiences suggested that
different experts were better at different types of information, and also that experts changed
over time (so we might see substantial variation even in texts from a single author); these
observations raise doubts about the wisdom and usefulness of a single-expert strategy.
In short, the complexity of nlg tasks means that a very large number of structured ka
sessions may be needed to get good coverage; and the fact that there are numerous ways to
write texts to fulfill a communicative goal means that different experts tend to write very
differently, which makes analysis of structured ka sessions difficult.

503

fiReiter, Sripada, & Robertson

3.3 Corpus Analysis
In recent years there has been great interest in Natural Language Processing and other areas
of ai in using machine learning techniques to acquire knowledge from relevant data sets. For
example, instead of building a medical diagnosis system by trying to understand how expert
doctors diagnose diseases, we can instead analyse data sets of observed symptoms and actual
diseases, and use statistical and machine learning techniques to determine which symptoms
predict which disease. Similarly, instead of building an English grammar by working with
expert linguists, we can instead analyse large collections of grammatical English texts in
order to learn the allowable structures (grammar) of such texts. Such collections of texts
are called corpora in Natural Language Processing.
There has been growing interest in applying such techniques to learn the knowledge
needed for nlg. For example, Barzilay and McKeown (2001) used corpus-based machine
learning to learn paraphrase possibilities; Duboue and McKeown (2001) used corpus-based
machine learning to learn how NP constituents should be ordered; and Hardt and Rambow
(2001) used corpus-based machine learning to learn rules for VP ellipsis.
Some nlg researchers, such as McKeown et al. (1994), have used the term corpus
analysis to refer to the manual analysis (without using machine learning techniques) of a
small set of texts which are written explicitly for the nlg project by domain experts (and
hence are not naturally occurring). This is certainly a valid and valuable ka technique,
but we regard it as a form of structured expert-oriented ka, in some ways similar to thinkaloud protocols. In this paper, corpus analysis refers to the use of machine learning and
statistical techniques to analyse collections of naturally occurring texts.
Corpus analysis in our sense of the word was not possible in stop because we did not have
a collection of naturally occurring texts (since doctors do not currently write personalised
smoking-cessation letters). We briefly considered analysing the example letters produced
in the think-aloud sessions with machine learning techniques, but we only had 30 such
texts, and we believed this would be too few for successful learning, especially given the
high variability between experts. In other words, perhaps the primary strength of corpus
analysis is its ability to extract information from large data sets; but if there are no large
data sets to extract information from, then corpus analysis loses much of its value.
In SumTime-Mousam, we were able to acquire and analyse a substantial corpus of 1099
human-written weather forecasts, along with the data files that the forecasters looked at
when writing the forecasts (Sripada et al., 2003). Details of our corpus analysis procedures
and results have been presented elsewhere (Reiter & Sripada, 2002a; Sripada et al., 2003),
and will not be repeated here.
3.3.1 Validation of Corpus Analysis Knowledge
While many of the rules we acquired from corpus analysis were valid, some rules were
problematical, primarily due to two factors: individual variations between the writers, and
writers making choices that were appropriate for humans but not for nlg systems.
A simple example of individual variation and the problems it causes is as follows. One of
the first things we attempted to learn from the corpus was how to express numbers in wind
statements. We initially did this by searching for the most common textual realisation of
each number. This resulted in rules that said that 5 should be expressed as 5, but 6 should
504

fiAcquiring Correct Knowledge for NLG

form
5
05
6
06

F1
0
0
0
0

F2
7
0
44
0

F3
0
1
0
364

F4
0
46
0
154

F5
122
0
89
0

unknown
4
2
2
13

total
133
49
135
531

Table 2: Usage of 5, 05, 6, 06 in wind statements, by forecaster
be expressed as 06. Now it is probably acceptable for a forecast to always include leading
zeros for single digits (that is, use 05 and 06), and to never include leading zeros (that is,
use 5 and 6). However, it is probably not acceptable to mix the two (that is, use 5 and 06
in the same forecast), which is what our rules would have led to.
The usage of 5, 05, 6, and 06 by each individual forecaster is shown in Table 2. As
this table suggests, each individual forecaster is consistent; forecasters F3 and F4 always
include leading zeros, while forecasters F2 and F5 never include leading zeros. F1 in fact
is also consistent and always omits leading zeros; for example he uses 8 instead of 08. The
reason that the overall statistics favour 5 over 05 but 06 over 6 is that individuals also differ
in which descriptions of wind speed they prefer to use. For example, F1 never explicitly
mentions low wind speeds such as 5 or 6 knots, and instead always uses generic phrases such
as 10 OR LESS; F2, F4, and F5 use a mix of generic phrases and explicit numbers for low
wind speeds; and F3 always uses explicit numbers and never uses generic phrases. Some of
the forecasters (especially F3) also have a strong preference for even numbers. This means
that the statistics for 5 vs. 05 are dominated by F5 (the only forecaster who both explicitly
mentions low wind speeds and does not prefer even numbers); while the statistics for 6 vs.
06 are dominated by F3 (who uses this number a lot because he avoids both generic phrases
and odd numbers). Hence the somewhat odd result that the corpus overall favours 5 over
05 but 06 over 6.
This example is by no means unique. Reiter and Sripada (2002b) explain how a more
complex analysis using this corpus, whose goal was to determine the most common time
phrase for each time, similarly led to unacceptable rules, again largely because of individual
differences between the forecasters.
There are obvious methods to deal with the problems caused by individual variation. For
example, we could restrict the corpus to texts from one author; although this does have the
major drawback of significantly reducing the size of the corpus. We could also use a more
sophisticated model, such as learning one rule for how all single digit numbers are expressed,
not separate rules for each number. Or we could analyse the behaviour of individuals and
identify choices (such as presence of a leading zero) that vary between individuals but are
consistently made by any given individual; and then make such choices parameters which
the user of the nlg system can specify. This last option is probably the best for nlg
systems (Reiter, Sripada, & Williams, 2003), and is the one used in SumTime-Mousam
for the leading-zero choice.
Our main point is simply that we would have been in trouble if we had just accepted our
initial corpus-derived rules (use 5 and 06) without question. As most corpus researchers
are of course aware, the result of corpus analysis depends on what is being learned (for
example, a rule on how to realise 5, or a rule on how to realise all single-digit numbers)
505

fiReiter, Sripada, & Robertson

and on what features are used in the learning (for example, just the number, or the number
and the author). In more complex analyses, such as our analysis of time-phrase choice rules
(Reiter & Sripada, 2002b), the result also depends on the algorithms used for learning and
alignment. The dependence of corpus analysis on these choices means that the results of
a particular analysis are not guaranteed to be correct and need to be validated (checked)
just like the results of other ka techniques. Also, what is often the best approach from an
nlg perspective, namely identifying individual variations and letting the user choose which
variation he or she prefers, requires analysing differences between individual writers. To
the best of our knowledge most published nl corpus analyses have not done this, perhaps
in part because many popular corpora do not include author information.
The other recurring problem with corpus-derived rules was cases where the writers
produced sub-optimal texts that in particular were shorter than they should have been,
probably because such texts were quicker to write. For instance, we noticed that when
a parameter changed in a more or less steady fashion throughout a forecast period, the
forecasters often omitted a time phrase. For example, if a S wind rose steadily in speed
from 10 to 20 over the course of a forecast period covering a calendar day, the forecasters
might write S 8-12 RISING TO 18-22, instead of S 8-12 RISING TO 18-22 BY MIDNIGHT.
A statistical corpus analysis showed that the null time phrase was the most common one
in such contexts, used in 33% of cases. The next most common time phrase, later, was only
used in 14% of cases. Accordingly, we programmed our system to omit the time phrase
in such circumstances. However, when we asked experts to comment on and revise our
generated forecasts (Section 3.4), they told us that this behaviour was incorrect, and that
forecasts were more useful to end users if they included explicit time phrases and did not rely
on the readers remembering when forecast periods ended. In other words, in this example
the forecasters were doing the wrong thing, which of course meant that the rule produced
by corpus analysis was incorrect.
We dont know why the forecasters did this, but discussions with the forecast managers
about this and other mistakes (such as forecast authors describing wind speed and direction
as changing at the same time, even when they actually were predicted to change at different
times) suggested that one possible cause is the desire to write forecasts quickly. In particular,
numerical weather predictions are constantly being updated, and customers want their
forecasts to be based on the most up-to-date prediction; this can limit the amount of time
available to write forecasts.
In fact it can be perfectly rational for human writers to cut corners because of time
limitations. If the forecasters believe, for example, that quickly writing a forecast at the
last minute will let them use more up-to-date prediction data; and that the benefits of more
up-to-date data outweighs the costs of abbreviated texts, then they are making the right
decision when they write shorter-than-optimal texts. An nlg system, however, faces a very
different set of tradeoffs (for example, omitting a time phrase is unlikely to speed up an
nlg system), which means that it should not blindly imitate the choices made by human
writers.
This problem is perhaps a more fundamental one than the individual variation problem,
because it can not be solved by appropriate choices as to what is being learned, what
features are considered, and so forth. Corpus analysis, however it is performed, learns the
choice rules used by human authors. If these rules are inappropriate for an nlg system,
506

fiAcquiring Correct Knowledge for NLG

then the rules learned by corpus analysis will be inappropriate ones as well, regardless of
how the corpus analysis is carried out.
In very general terms, corpus analysis certainly has many strengths, such as looking
at what people do in practice, and collecting large data sets which can be statistically
analysed. But pure corpus analysis does perhaps suffer from the drawback that it gives no
information on why experts made the choices they made, which means that blindly imitating
a corpus can lead to inappropriate behaviour when the human writers face a different set
of constraints and tradeoffs than the nlg system.
3.3.2 Role of Corpus Analysis
Corpus analysis and machine learning are wonderful ways to acquire knowledge if
1. there is a large data set (corpus) that covers unusual and boundary cases as well as
normal cases;
2. the members of the data set (corpus) are correct in that they are what we would like
the software system to produce; and
3. the members of the data set (corpus) are consistent (modulo some noise), for example
any given input generally leads to the same output.
These conditions are probably satisfied when learning rules for medical diagnosis or speech
recognition. However, they were not satisfied in our projects. None of the above conditions
were satisfied in stop, and only the first was satisfied in SumTime-Mousam.
Of course, there may be ways to alleviate some of these problems. For example, we could
try to acquire general communication knowledge which is not domain dependent (such as
English grammar) from general corpora such as the British National Corpus; we could
argue that certain aspects of manually written texts (such as lexical usage) are unlikely to
be adversely affected by time pressure and hence are probably correct; and we could analyse
the behaviour of individual authors in order to enhance consistency (in other words, treat
author as an input feature on a par with the actual numerical or semantic input data). There
is scope for valuable research here, which we hope will be considered by people interested
in corpus-based techniques in nlg.
We primarily used corpus analysis in SumTime-Mousam to acquire domain communication knowledge, such as how to linguistically express numbers and times in weather
forecasts, when to elide information, and sublanguage constraints on the grammar of our
weather forecasts. Corpus analysis of course can also be used to acquire generic communication knowledge such as English grammar, but as mentioned above this is probably best
done on a large general corpus such as the British National Corpus. We did not use corpus
analysis to acquire domain knowledge about meteorology. Meteorological researchers in fact
do use machine learning techniques to learn about meteorology, but they analyse numeric
data sets of actual and predicted weather, they do not analyse textual corpora.
In summary, machine learning and corpus-based techniques are extremely valuable if
the above conditions are satisfied, and in particular offer a cost-effective solution to the
problem of acquiring the large amount of knowledge needed in complex nlg applications
(Section 3.2.2). Acquiring large amounts of knowledge using expert-oriented ka techniques
507

fiReiter, Sripada, & Robertson

is expensive and time-consuming because it requires many sessions with experts; in contrast,
if a large corpus of consistent and correct texts can be created, then large amounts of
knowledge can be extracted from it at low marginal cost. But like all learning techniques,
corpus analysis is very vulnerable to the Garbage In, Garbage Out principle; if the corpus
is small, incorrect, and/or inconsistent, then the results of corpus analysis may not be
correct.
3.4 Expert Revision
In both stop and SumTime-Mousam, we made heavy use of expert revision. That is, we
showed generated texts to experts and asked them to suggest changes that would improve
them. In a sense, expert revision could be considered to be a type of structured expertoriented ka, but it seems to have somewhat different strengths and weaknesses than the
techniques mentioned in Section 3.2, so we treat it separately.
As an example of expert revision, an early version of the stop system used the phrase
there are lots of good reasons for stopping. One of the experts commented during a revision
session that the phrasing should be changed to emphasise that the reasons listed (in this
particular section of the stop letter) were ones the smoker himself had selected in the
questionnaire he filled out. This eventually led to the revised wording It is encouraging that
you have many good reasons for stopping, which is in the first paragraph of the example
letter in Figure 2. An example of expert revision in SumTime-Mousam was mentioned in
Section 3.3; when we showed experts generated texts that omitted some end-of-period time
phrases, they told us this was incorrect, and we should include such time phrases.
In stop, we also tried revision sessions with recipients (smokers). This was less successful
than we had hoped. Part of the problem was the smokers knew very little about stop (unlike
our experts, who were all familiar with the project), and often made comments which were
not useful for improving the system, such as I did stop for 10 days til my daughter threw a
wobbly and then I wanted a cigarette and bought some. Also, most of our comments came
from well-educated and articulate smokers, such as university students. It was harder to get
feedback from less well-educated smokers, such as single mothers living in council (public
housing) estates. Hence we were unsure if the revision comments we obtained were generally
applicable or not.
3.4.1 Validation of Expert Revision Knowledge
We did not validate expert revision knowledge as we did with the other techniques. Indeed,
we initially regarded expert revision as a validation technique, not a ka technique, although
in retrospect it probably makes more sense to think of it as a ka technique.
On a qualitative level, though, expert revision has certainly resulted in a lot of useful
knowledge and ideas for changing texts, and in particular proved a very useful way of
improving the handling of unusual and boundary cases. For example, we changed the way
we described uneventful days in SumTime-Mousam (when the weather changed very little
during a day) based on revision sessions.
The comment was made during stop that revision was best at suggesting specific localised changes to generated text, and less useful in suggesting larger changes to the system.
One of the stop experts suggested, after the system was built, that he might have been
508

fiAcquiring Correct Knowledge for NLG

able to suggest larger changes if we had explained the systems reasoning to him, instead of
just giving him a letter to revise. In other words, just as we asked experts to think-aloud
as they wrote letters, in order to understand their reasoning, it could be useful in revision
sessions if experts understood what the computer system was thinking as well as what
it actually produced. Davis and Lenat (1982, page 260) have similarly pointed out that
explanations can help experts debug and improve knowledge-based systems.
3.4.2 Role of Expert Revision
We have certainly found expert revision to be an extremely useful technique for improving
nlg systems; and furthermore it is useful for improving all types of knowledge (domain,
domain communication, and communication). But at the same time revision does seem to
largely be a local optimisation technique. If an nlg system is already generating reasonable
texts, then revision is a good way of adjusting the systems knowledge and rules to improve
the quality of generated text. But like all local optimisation techniques, expert revision
may tend to push systems towards a local optimum, and may be less well suited to finding
radically different solutions that give a better result.

4. Discussion: Problems Revisited
In section 1 we explained that writing tasks can be difficult to automate because these are
complex, often novel, poorly understood, and allow multiple solutions. In this section we
discuss each of these problems in more detail, based on our experiences with stop and
SumTime-Mousam.
4.1 Complexity
Because nlg systems communicate with humans, they need knowledge about people, language, and how people communicate; since all of these are very complex, that means that in
general nlg systems need a lot of complex knowledge. This is one of the reasons why knowledge acquisition for nlg is so difficult. If we recall the distinction in Section 2.1 between
domain knowledge, domain communication knowledge, and communication knowledge, it
may be that communication knowledge (such as grammar) is generic and hence can be acquired once (perhaps by corpus-based techniques) and then used in many applications. And
domain knowledge is similar to what is needed by other ai systems, so problems acquiring
it are not unique to nlg. But domain communication knowledge, such as the optimal tone
of a smoking letter and how this tone can be achieved, or when information in a weather
forecast can be elided, is application dependent (and hence cannot be acquired generically)
and is also knowledge about language and communication (and hence is complex). Hence
ka for nlg may always require acquiring complex knowledge.
In our experience, the best way to acquire complex knowledge robustly is to get information on how a large number of individual cases are handled. This can be done by corpus
analysis if a suitable corpus can be created. It can also sometimes be done by expert revision, if experts have the time to look at a large number of generated texts; in this regard
it may be useful to tell them to only comment on major problems and to ignore minor
difficulties. But however the knowledge is acquired, it will require a substantial effort.

509

fiReiter, Sripada, & Robertson

4.2 Novelty
Of course, many ai systems need complex knowledge, so the above comments are hardly
unique to nlg. But one aspect of nlg which perhaps is more unusual is that many of the
tasks nlg systems are expected to perform are novel tasks that are not currently done by
humans. Most ai expert systems attempt to replicate the performance of human experts
in areas such as medical diagnosis and credit approval. Similarly, most language technology
systems attempt to replicate the performance of human language users in tasks such as
speech recognition and information retrieval. But many nlg applications are like stop,
and attempt a task that no human performs. Even in SumTime-Mousam, an argument
could be made that the task humans actually perform is writing weather forecasts under
time constraints, which is in fact different from the task performed by SumTime-Mousam.
Novelty is a fundamental problem, because it means that knowledge acquired from
expert-oriented ka may not be reliable (since the experts are not in fact experts at the
actual nlg task), and that a corpus of manually-written texts probably does not exist.
This means that none of the ka techniques described above are likely to work. Indeed,
acquiring novel knowledge is almost the definition of scientific research, so perhaps the only
way to acquire such knowledge is to conduct scientific research in the domain. Of course,
only some knowledge will need to be acquired in this way, even in a novel application it is
likely that much of the knowledge needed (such as grammar and morphology) is not novel.
On the other hand, novelty perhaps is also an opportunity for nlg. One of the drawbacks of conventional expert systems is that their performance is often limited to that of
human experts, in which case users may prefer to consult actual experts instead of computer
systems. But if there are no experts at a task, an nlg system may be used even if its output
is far from ideal.
4.3 Poorly Understood Tasks
A perhaps related problem is that there are no good theoretical models for many of the
choices that nlg systems need to make. For example, the ultimate goal of stop is to
change peoples behaviour, and a number of colleagues have suggested that we base stop
on argumentation theory, as Grasso, Cawsey, and Jones (2000) did for their dietary advice
system. However, argumentation theory focuses on persuading people to change their beliefs
and desires, whereas the goal of stop was more to encourage people to act on beliefs and
desires they already had. In other words, stops main goal was to encourage people who
already wanted to stop smoking to make a serious cessation attempt, not to convince people
who had no desire to quit that they should change their mind about the desirability of
smoking. The most applicable theory we could find was Stages of Change (Prochaska &
diClemente, 1992), and indeed we partially based stop on this theory. However, the results
of our evaluation suggested that some of the choices and rules that we based on Stages of
Change were incorrect, as mentioned in Section 3.2.1.
Similarly, one of the problems in SumTime-Mousam is generating texts that will be
interpreted correctly despite the fact that different readers have different idiolects and in
particular probably interpret words in different ways (Reiter & Sripada, 2002a; Roy, 2002).
Theoretical guidance on how to do this would have been very useful, but we were not able
to find any such guidance.
510

fiAcquiring Correct Knowledge for NLG

The lack of good theoretical models means that nlg developers cannot use such models
to fill in the cracks between knowledge acquired from experts or from data sets, as can be
done by ai systems in better understood areas such as scheduling or configuring machinery.
This in turn means that a lot of knowledge must be acquired. In applications where there is
a good theoretical basis, the goal of ka is perhaps to acquire a limited amount of high-level
information about search strategies, taxonomies, the best way to represent knowledge, etc;
once these have been determined, the details can be filled in by theoretical models. But in
applications where details cannot be filled in from theory and need to be acquired, much
more knowledge is needed. Acquiring such knowledge with structured expert-oriented ka
could be extremely expensive and time consuming. Corpus-based techniques are cheaper if
a large corpus is available; however, the lack of a good theoretical understanding perhaps
contributes to the problem that we do not know which behaviour we observe in the corpus
is intended to help the reader (and hence should be copied by an nlg system) and which
behaviour is intended to help the writer (and hence perhaps should not be copied).
4.4 Expert Variation
Perhaps in part because of the lack of good theories, in both stop and SumTime-Mousam
we observed considerable variation between experts. In other words, different experts wrote
quite different texts from the same input data. In stop we also discovered that experts
changed how they wrote over time (Section 3.2.1).
Variability caused problems for both structured expert-oriented ka (because different
experts told us different things) and for corpus analysis (because variation among corpus
authors made it harder to extract a consistent set of rules with good coverage). However,
variation seems to have been less of a problem with revision. We suspect this is because
experts vary less when they are very confident about a particular decision; and in revision
experts tended to focus on things they were confident about, which was not the case with
the other ka techniques.
In a sense variability may be especially dangerous in corpus analysis, because there is
no information in a corpus about the degree of confidence authors have in individual decisions, and also because developers may not even realise that there is variability between
authors, especially if the corpus does not include author information. In contrast, structured expert-oriented techniques such as think-aloud do sometimes give information about
experts confidence, and also variations between experts are usually obvious.
We experimented with various techniques for resolving differences between experts/authors,
such as group discussions and focusing on the decisions made by one particular expert. None
of these were really satisfactory. Given our experiences with revision, perhaps the best way
to reduce variation is to develop ka techniques that very clearly distinguish between decisions experts are confident in and decisions they have less confidence in.

5. Development Methodology: Using Multiple KA Techniques
From a methodological perspective, the fact that different ka techniques have different
strengths and weaknesses suggests that it makes sense to use a mixture of several different
ka techniques. For example, if both structured expert-oriented ka and corpus analysis
are used, then the explanatory information from the expert-oriented ka can be used to
511

fiReiter, Sripada, & Robertson

help identify which decisions are intended to help the reader and which are intended to
help the writer, thus helping overcome a problem with corpus analysis; and the broader
coverage of corpus analysis can show how unusual and boundary cases should be handled,
thus overcoming a problem with expert-oriented ka.
It also may make sense to use different techniques at different points in the development
process. For example, directly asking experts for knowledge could be stressed at the initial
stages of a project, and used to build a very simple initial prototype; structured ka with
experts and corpus analysis could be stressed during the middle phases of a project, when
the prototype is fleshed out and converted into something resembling a real system; and
revision could be used in the later stages of a project, when the system is being refined and
improved.
This strategy, which is graphically shown in Figure 5, is basically the one we followed
in both stop and SumTime-Mousam. Note that it suggests that knowledge acquisition is
something that happens throughout the development process. In other words, we do not first
acquire knowledge and then build a system; knowledge acquisition is an ongoing process
which is closely coupled with the general software development effort. Of course, this is
hardly a novel observation, and there are many development methodologies for knowledgebased systems that stress iterative development and continual ka (Adelman & Riedel,
1997).
In the short term, we believe that using a development methodology that combines
different ka techniques in this manner, and also validating knowledge as much as possible,
are the best strategies for acquiring nlg knowledge. We also believe that whenever possible
knowledge that is acquired in one way should be validated in another way. In other words,
we do not recommend validating corpus-acquired knowledge using corpus techniques (even
if the validation is done with a held-out test set); or validating expert-acquired knowledge
using expert-based validation (even if the validation is done using a different expert). It
is preferable (although not always possible) to validate corpus-acquired knowledge with
experts, and to validate expert-acquired knowledge with a corpus.
Another issue related to development methodology is the relationship between knowledge acquisition and system evaluation. Although these are usually considered to be separate activities, in fact they can be closely related. For example, we are currently running
an evaluation of SumTime-Mousam which is based on the number of edits that forecasters
manually make to computer-generated forecasts before publishing them; this is similar to
edit-cost evaluations of machine translation systems (Jurafsky & Martin, 2000, page 823).
However, these edits are also an excellent source of data for improving the system via expert
revision. To take one recent example, a forecaster edited the computer-generated text SSE
23-28 GRADUALLY BACKING SE 20-25 by dropping the last speed range, giving SSE
23-28 GRADUALLY BACKING SE. This can be considered as evaluation data (2 token
edits needed to make text acceptable), or as ka data (we need to adjust our rules for eliding
similar but not identical speed ranges).
In other words, real-world feedback on the effectiveness and quality of generated texts
can often be used to either improve or evaluate an nlg system. How such data should
be used depends on the goals of the project. In scientific projects whose goal is to test
hypotheses, it may be appropriate at some point to stop improving a system and use all new
effectiveness data purely for evaluation and hypothesis testing; in a sense this is analogous
512

fiAcquiring Correct Knowledge for NLG

Directly Ask Experts
for Knowledge
Initial prototype

Structured KA
with Experts

Corpus Analysis

Initial version of full system

Expert Revision

Final System
Figure 5: Our Methodology
to holding back part of a corpus for testing purposes. In applied projects whose goal is
to build a maximally useful system, however, it may be more appropriate to use all of the
effectiveness data to improve the quality of the generated texts.

6. Conclusion
Acquiring correct knowledge for nlg is very difficult, because the knowledge needed is
largely knowledge about people, language, and communication, and such knowledge is complex and poorly understood. Furthermore, perhaps because writing is more of an art than a
science, different people write very differently, which further complicates the knowledge acquisition process; and many nlg systems attempt novel tasks not currently done manually,
which makes it very hard to find knowledgeable experts or good quality corpora. Perhaps
because of these problems, every single ka technique we tried in stop and SumTimeMousam had major problems and limitations.
There is no easy solution to these problems. In the short term, we believe it is useful
to use a mixture of different ka techniques (since techniques have different strengths and
weaknesses), and to validate knowledge whenever possible, preferably using a different tech-

513

fiReiter, Sripada, & Robertson

nique than the one used to acquire the knowledge. It also helps if developers understand
the weaknesses of different techniques, such as the fact that structured expert-oriented ka
may not give good coverage of the complexities of people and language, and the fact that
corpus-based ka does not distinguish between behaviour intended to help the reader and
behaviour intended to help the writer.
In the longer term, we need more research on better ka techniques for nlg. If we cannot
reliably acquire the knowledge needed by ai approaches to text generation, then there is
no point in using such approaches, regardless of how clever our algorithms or models are.
The first step towards developing better ka techniques is to acknowledge that current ka
techniques are not working well, and understand why this is the case; we hope that this
paper constitutes a useful step in this direction.

Acknowledgements
Numerous people have given us valuable comments over the past five years as we struggled with ka for nlg, too many to acknowledge here. But we would like to thank Sandra
Williams for reading several drafts of this paper and considering it in the light of her own experiences, and to thank the anonymous reviewers for their very helpful comments. We would
also like to thank the experts we worked with in stop and SumTime-Mousam, without
whom this work would not be possible. This work was supported by the UK Engineering and
Physical Sciences Research Council (EPSRC), under grants GR/L48812 and GR/M76881,
and by the Scottish Office Department of Health under grant K/OPR/2/2/D318.

References
Adelman, L., & Riedel, S. (1997). Handbook for Evaluating Knowledge-Based Systems.
Kluwer.
Anderson, J. (1995). Cognitive Psychology and its Implications (Fourth edition). Freeman.
Barzilay, R., & McKeown, K. (2001). Extracting paraphrases from a parallel corporus.
In Proceedings of the 39th Meeting of the Association for Computation Linguistics
(ACL-01), pp. 5057.
Buchanan, B., & Wilkins, D. (Eds.). (1993). Readings in Knowledge Acquisition and Learning. Morgan Kaufmann.
Davis, R., & Lenat, D. (1982). Knowledge-Based Systems in Artificial Intelligence. McGraw
Hill.
Duboue, P., & McKeown, K. (2001). Empirically estimating order constraints for content
planning in generation. In Proceedings of the 39th Meeting of the Association for
Computation Linguistics (ACL-01), pp. 172179.
Goldberg, E., Driedger, N., & Kittredge, R. (1994). Using natural-language processing to
produce weather forecasts. IEEE Expert, 9 (2), 4553.
Grasso, F., Cawsey, A., & Jones, R. (2000). Dialectical argumentation to solve conflicts
in advice giving: a case study in the promotion of healthy nutrition. International
Journal of Human Computer Studies, 53, 10771115.
514

fiAcquiring Correct Knowledge for NLG

Hardt, D., & Rambow, O. (2001). Generation of VP-ellipsis: A corpus-based approach.
In Proceedings of the 39th Meeting of the Association for Computation Linguistics
(ACL-01), pp. 282289.
Jurafsky, D., & Martin, J. (2000). Speech and Language Processing. Prentice-Hall.
Kittredge, R., Korelsky, T., & Rambow, O. (1991). On the need for domain communication
language. Computational Intelligence, 7 (4), 305314.
Lavoie, B., Rambow, O., & Reiter, E. (1997). Customizable descriptions of object-oriented
models. In Proceedings of the Fifth Conference on Applied Natural-Language Processing (ANLP-1997), pp. 253256.
Lennox, S., Osman, L., Reiter, E., Robertson, R., Friend, J., McCann, I., Skatun, D., & Donnan, P. (2001). The cost-effectiveness of computer-tailored and non-tailored smoking
cessation letters in general practice: A randomised controlled study. British Medical
Journal, 322, 13961400.
McKeown, K., Kukich, K., & Shaw, J. (1994). Practical issues in automatic document
generation. In Proceedings of the Fourth Conference on Applied Natural-Language
Processing (ANLP-1994), pp. 714.
Oberlander, J., ODonnell, M., Knott, A., & Mellish, C. (1998). Conversation in the museum: experiments in dynamic hypermedia with the intelligent labelling explorer. New
Review of Hypermedia and Multimedia, 4, 1132.
Prochaska, J., & diClemente, C. (1992). Stages of Change in the Modification of Problem
Behaviors. Sage.
Reiter, E., & Dale, R. (2000). Building Natural Language Generation Systems. Cambridge
University Press.
Reiter, E., Robertson, R., & Osman, L. (2000). Knowledge acquisition for natural language
generation. In Proceedings of the First International Conference on Natural Language
Generation, pp. 217215.
Reiter, E., Robertson, R., & Osman, L. (2003). Lessons from a failure: Generating tailored
smoking cessation letters. Artificial Intelligence, 144, 4158.
Reiter, E., & Sripada, S. (2002a). Human variation and lexical choice. Computational
Linguistics, 28, 545553.
Reiter, E., & Sripada, S. (2002b). Should corpora texts be gold standards for NLG?. In
Proceedings of the Second International Conference on Natural Language Generation,
pp. 97104.
Reiter, E., Sripada, S., & Williams, S. (2003). Acquiring and using limited user models in
NLG. In Proceedings of the 2003 European Workshop on Natural Language Generation, pp. 8794.
Roy, D. (2002). Learning visually grounded words and syntax for a scene description task.
Computer Speech and Language, 16, 353385.
Scott, A. C., Clayton, J., & Gibson, E. (1991). A Practical Guide to Knowledge Acquisition.
Addison-Wesley.
515

fiReiter, Sripada, & Robertson

Shadbolt, N., OHara, K., & Crow, L. (1999). The experimental evaluation of knowledge acquisition techniques and methods: History, problems and new directions. International
Journal of Human Computer Studies, 51, 729755.
Sripada, S., Reiter, E., Hunter, J., & Yu, J. (2002). Segmenting time series for weather
forecasting. In Applications and Innovations in Intelligent Systems X, pp. 105118.
Springer-Verlag.
Sripada, S., Reiter, E., Hunter, J., & Yu, J. (2003). Summarising neonatal time-series data.
In Proceedings of the Research Note Sessions of the EACL-2003, pp. 167170.
Sripada, S., Reiter, E., Hunter, J., Yu, J., & Davy, I. (2001). Modelling the task of summarising time series data using KA techniques. In Applications and Innovations in
Intelligent Systems IX, pp. 183196. Springer-Verlag.
Walker, M., Rambow, O., & Rogati, M. (2002). Training a sentence planner for spoken
dialogue using boosting. Computer Speech and Language, 16, 409433.
Williams, S., Reiter, E., & Osman, L. (2003). Experiments with discourse-level choices
and readability. In Proceedings of the 2003 European Workshop on Natural Language
Generation, pp. 127134.

516

fiJournal of Artificial Intelligence Research 18 (2003) 1-44

Submitted 5/02; published 1/03

Acquiring Word-Meaning Mappings
for Natural Language Interfaces
Cynthia A. Thompson

cindi@cs.utah.edu

School of Computing, University of Utah
Salt Lake City, UT 84112-3320

Raymond J. Mooney

mooney@cs.utexas.edu

Department of Computer Sciences, University of Texas
Austin, TX 78712-1188

Abstract
This paper focuses on a system, Wolfie (WOrd Learning From Interpreted Examples), that acquires a semantic lexicon from a corpus of sentences paired with semantic
representations. The lexicon learned consists of phrases paired with meaning representations. Wolfie is part of an integrated system that learns to transform sentences into
representations such as logical database queries.
Experimental results are presented demonstrating Wolfies ability to learn useful
lexicons for a database interface in four different natural languages. The usefulness of
the lexicons learned by Wolfie are compared to those acquired by a similar system, with
results favorable to Wolfie. A second set of experiments demonstrates Wolfies ability
to scale to larger and more difficult, albeit artificially generated, corpora.
In natural language acquisition, it is difficult to gather the annotated data needed
for supervised learning; however, unannotated data is fairly plentiful. Active learning
methods attempt to select for annotation and training only the most informative examples,
and therefore are potentially very useful in natural language applications. However, most
results to date for active learning have only considered standard classification tasks. To
reduce annotation effort while maintaining accuracy, we apply active learning to semantic
lexicons. We show that active learning can significantly reduce the number of annotated
examples required to achieve a given level of performance.

1. Introduction and Overview
A long-standing goal for the field of artificial intelligence is to enable computer understanding of human languages. Much progress has been made in reaching this goal, but much also
remains to be done. Before artificial intelligence systems can meet this goal, they first need
the ability to parse sentences, or transform them into a representation that is more easily
manipulated by computers. Several knowledge sources are required for parsing, such as a
grammar, lexicon, and parsing mechanism.
Natural language processing (NLP) researchers have traditionally attempted to build
these knowledge sources by hand, often resulting in brittle, inefficient systems that take
a significant effort to build. Our goal here is to overcome this knowledge acquisition
bottleneck by applying methods from machine learning. We develop and apply methods
from empirical or corpus-based NLP to learn semantic lexicons, and from active learning to
reduce the annotation effort required to learn them.

c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiThompson & Mooney

The semantic lexicon is one NLP component that is typically challenging and time consuming to construct and update by hand. Our notion of semantic lexicon, formally defined
in Section 3, is that of a list of phrase-meaning pairs, where the meaning representation is
determined by the language understanding task at hand, and where we are taking a compositional view of sentence meaning (Partee, Meulen, & Wall, 1990). This paper describes
a system, Wolfie (WOrd Learning From Interpreted Examples), that acquires a semantic
lexicon of phrase-meaning pairs from a corpus of sentences paired with semantic representations. The goal is to automate lexicon construction for an integrated NLP system that
acquires both semantic lexicons and parsers for natural language interfaces from a single
training set of annotated sentences.
Although many others (Sebillot, Bouillon, & Fabre, 2000; Riloff & Jones, 1999; Siskind,
1996; Hastings, 1996; Grefenstette, 1994; Brent, 1991) have presented systems for learning
information about lexical semantics, we present here a system for learning lexicons of phrasemeaning pairs. Further, our work is unique in its combination of several features, though
prior work has included some of these aspects. First, its output can be used by a system,
Chill (Zelle & Mooney, 1996; Zelle, 1995), that learns to parse sentences into semantic
representations. Second, it uses a fairly straightforward batch, greedy, heuristic learning
algorithm that requires only a small number of examples to generalize well. Third, it is
easily extendible to new representation formalisms. Fourth, it requires no prior knowledge
although it can exploit an initial lexicon if provided. Finally, it simplifies the learning
problem by making several assumptions about the training data, as described further in
Section 3.2.
We test Wolfies ability to acquire a semantic lexicon for a natural language interface
to a geographical database using a corpus of queries collected from human subjects and
annotated with their logical form. In this test, Wolfie is integrated with Chill, which
learns parsers but requires a semantic lexicon (previously built manually). The results
demonstrate that the final acquired parser performs nearly as accurately at answering novel
questions when using a learned lexicon as when using a hand-built lexicon. Wolfie is
also compared to an alternative lexicon acquisition system developed by Siskind (1996),
demonstrating superior performance on this task. Finally, the corpus is translated into
Spanish, Japanese, and Turkish, and experiments are conducted demonstrating an ability
to learn successful lexicons and parsers for a variety of languages.
A second set of experiments demonstrates Wolfies ability to scale to larger and more
difficult, albeit artificially generated, corpora. Overall, the results demonstrate a robust
ability to acquire accurate lexicons directly usable for semantic parsing. With such an
integrated system, the task of building a semantic parser for a new domain is simplified. A
single representative corpus of sentence-representation pairs allows the acquisition of both
a semantic lexicon and parser that generalizes well to novel sentences.
While building an annotated corpus is arguably less work than building an entire NLP
system, it is still not a simple task. Redundancies and errors may occur in the training data.
A goal should be to also minimize the annotation effort, yet still achieve a reasonable level
of generalization performance. In the case of natural language, there is frequently a large
amount of unannotated text available. We would like to automatically, but intelligently,
choose which of the available sentences to annotate.

2

fiAcquiring Word-Meaning Mappings

We do this here using a technique called active learning. Active learning is a research
area in machine learning that features systems that automatically select the most informative examples for annotation and training (Cohn, Atlas, & Ladner, 1994). The primary goal
of active learning is to reduce the number of examples that the system is trained on, thereby
reducing the example annotation cost, while maintaining the accuracy of the acquired information. To demonstrate the usefulness of our active learning techniques, we compared
the accuracy of parsers and lexicons learned using examples chosen by active learning for
lexicon acquisition, to those learned using randomly chosen examples, finding that active
learning saved significant annotation cost over training on randomly chosen examples. This
savings is demonstrated in the geography query domain.
In summary, this paper provides a new statement of the lexicon acquisition problem
and demonstrates a machine learning technique for solving this problem. Next, by combining this with previous research, we show that an entire natural language interface can
be acquired from one training corpus. Further, we demonstrate the application of active
learning techniques to minimize the number of sentences to annotate as training input for
the integrated learning system.
The remainder of the paper is organized as follows. Section 2 gives more background
information on Chill and introduces Siskinds lexicon acquisition system, which we will
compare to Wolfie in Section 5. Sections 3 and 4 formally define the learning problem and
describe the Wolfie algorithm in detail. In Section 5 we present and discuss experiments
evaluating Wolfies performance in learning lexicons in a database query domain and for
an artificial corpus. Next, Section 6 describes and evaluates our use of active learning
techniques for Wolfie. Sections 7 and 8 discuss related research and future directions,
respectively. Finally, Section 9 summarizes our research and results.

2. Background
In this section we give an overview of Chill, the system that our research adds to. We also
describe Jeff Siskinds lexicon acquisition system.
2.1 Chill
The output produced by Wolfie can be used to assist a larger language acquisition system;
in particular, it is currently used as part of the input to a parser acquisition system called
Chill (Constructive Heuristics Induction for Language Learning). Chill uses inductive
logic programming (Muggleton, 1992; Lavrac & Dzeroski, 1994) to learn a deterministic
shift-reduce parser (Tomita, 1986) written in Prolog. The input to Chill is a corpus of
sentences paired with semantic representations, the same input required by Wolfie. The
parser learned is capable of mapping the sentences into their correct representations, as well
as generalizing well to novel sentences. In this paper, we limit our discussion to Chills
ability to acquire parsers that map natural language questions directly into Prolog queries
that can be executed to produce an answer (Zelle & Mooney, 1996). Following are two
sample queries for a database on U.S. geography, paired with their corresponding Prolog
query:

3

fiThompson & Mooney

<Sentence, Representation>
Training
Examples

WOLFIE

CHILL

Lexicon
<Phrase, Meaning>

Final
Parser
Prolog

Figure 1: The Integrated System
What is the capital of the state with the biggest population?
answer(C, (capital(S,C), largest(P, (state(S), population(S,P))))).
What state is Texarkana located in?
answer(S, (state(S), eq(C,cityid(texarkana, )), loc(C,S))).
Chill treats parser induction as the problem of learning rules to control the actions of
a shift-reduce parser. During parsing, the current context is maintained in a stack and a
buffer containing the remaining input. When parsing is complete, the stack contains the
representation of the input sentence. There are three types of operators that the parser uses
to construct logical queries. One is the introduction onto the stack of a predicate needed in
the sentence representation due to a phrases appearance at the front of the input buffer.
These operators require a semantic lexicon as background knowledge. For details on this
and the other two parsing operators, see Zelle and Mooney (1996). By using Wolfie, the
lexicon is provided automatically. Figure 1 illustrates the complete system.
2.2 Jeff Siskinds Lexicon Learning Research
The most closely related previous research into automated lexicon acquisition is that of
Siskind (1996), itself inspired by work by Rayner, Hugosson, and Hagert (1988). As we
will be comparing our system to his in Section 5, we describe the main features of his
research in this section. His goal is one of cognitive modeling of childrens acquisition of the
lexicon, where that lexicon can be used for both comprehension and generation. Our goal
is a machine learning and engineering one, and focuses on a lexicon for comprehension and
use in parsing, using a learning process that does not claim any cognitive plausibility, and
with the goal of learning a lexicon that generalizes well from a small number of training
examples.
His system takes an incremental approach to acquiring a lexicon. Learning proceeds in
two stages. The first stage learns which symbols in the representation are to be used in the
4

fiAcquiring Word-Meaning Mappings

(capital, capital(_,_)),
(biggest, largest(_,_)),
(highest point, high_point(_,_)),
(through, traverse(_,_)),
(has, loc(_,_))

(state, state(_)),
(in, loc(_,_)),
(long, len(_,_)),
(capital, capital(_)),

Figure 2: Sample Semantic Lexicon
final conceptual expression that represents the meaning of a word, by using a versionspace approach. The second stage learns how these symbols are put together to form the
final representation. For example, when learning the meaning of the word raise, the
algorithm may learn the set {CAUSE, GO, UP} during the first stage and put them together
to form the expression CAUSE(x, GO(y, UP)) during the second stage.
Siskind (1996) shows the effectiveness of his approach on a series of artificial corpora.
The system handles noise, lexical ambiguity, referential uncertainty, and very large corpora, but the usefulness of lexicons learned is only compared to the correct, artificial
lexicon. The goal of the experiments presented there was to evaluate the correctness and
completeness of learned lexicons. Earlier work (Siskind, 1992) also evaluated versions of his
technique on a quite small corpus of real English and Japanese sentences. We extend that
evaluation to a demonstration of the systems usefulness in performing real world natural
language processing tasks, using a larger corpus of real sentences.

3. The Lexicon Acquisition Problem
Although in the end our goal is to acquire an entire natural language interface, we currently
divide the task into two parts, the lexicon acquisition component and the parser acquisition
component. In this section, we discuss the problem of acquiring semantic lexicons that
assist parsing and the acquisition of parsers. The training input consists of natural language
sentences paired with their meaning representations. From these pairs we extract a lexicon
consisting of phrases paired with their meaning representations. Some training pairs were
given in the previous section, and a sample lexicon is shown in Figure 2.
3.1 Formal Definition
To present the learning problem more formally, some definitions are needed. While in the
following we use the terms string and substring, these extend straight-forwardly to
natural language sentences and phrases, respectively. We also refer to labeled trees, making
the assumption that the semantic meanings of interest can be represented as such. Most
common representations can be recast as labeled trees or forests, and our formalism extends
easily to the latter.
Definition: Let V , E be finite alphabets of vertex labels and edge labels, respectively.
Let V be a finite nonempty set of vertices, l a total function l : V  V , E a set of unordered
pairs of distinct vertices called edges, and a a total function a : E  E . G = (V, l, E, a) is
a labeled graph.

5

fiThompson & Mooney

String s 1: The girl ate the pasta with the cheese.
t 1 with its vertex and edge labels:

Tree t 1
1
2
4

ingest
patient
agent
person food
age type accomp
sex

3
5

6

7

female

child pasta

food

type
cheese

8

Interpretation f 1 from s 1 to t1 :
f 1 (girl) = 2
f 1 (ate") = 1
f 1 (pasta") = 3
f 1 (the cheese") = 7

Figure 3: Labeled Trees and Interpretations
Definition: A labeled tree is a connected, acyclic labeled graph.
Figure 3 shows the labeled tree t1 (with vertices 1-8) on the left, with associated vertex
and edge labels on the right. The function l is:1
{

(1, ingest), (2, person), (3, food), (4, female), (5, child), (6, pasta),
(7, food), (8, cheese) }.

The tree t1 is a semantic representation of the sentence s1 : The girl ate the pasta with the
cheese. Using a conceptual dependency (Schank, 1975) representation in Prolog list form,
the meaning is:
[ingest,

agent:[person, sex:female, age:child],
patient:[food, type:pasta, accomp:[food, type:cheese]]].

Definition: A u-v path in a graph G is a finite alternating sequence of vertices and edges
of G, in which no vertex is repeated, that begins with vertex u and ends with vertex v, and
in which each edge in the sequence connects the vertex that precedes it in the sequence to
the vertex that follows it in the sequence.
Definition: A directed, labeled tree T = (V, l, E, a) is a labeled tree whose edges consist of
ordered pairs of vertices, with a distinguished vertex r, called the root, with the property
that for every v  V , there is a directed r-v path in T , and such that the underlying
undirected unlabeled graph induced by (V, E) is a connected, acyclic graph.
Definition: An interpretation f from a finite string s to a directed, labeled tree t is a
one-to-one function mapping a subset s0 of the substrings of s, such that no two strings in
s0 overlap, into the vertices of t such that the root of t is in the range of f .
1. We omit enumeration of the function e but it could be given in a similar manner, for example ((1,2),
agent) is an element of e.

6

fiAcquiring Word-Meaning Mappings

girl":

person
sex
age
female

pasta": food

type

child

pasta

the cheese": food
type
cheese

ate": ingest

Figure 4: Meanings
The interpretation provides information about what parts of the meaning of a sentence
originate from which of its phrases. In Figure 3, we show an interpretation, f1 , of s1 to t1 .
Note that with is not in the domain of f1 , since s0 is a subset of the substrings of s, thus
allowing some words in s to have no meaning. Because we disallow overlapping substrings
in the domain, both cheese and the cheese could not map to vertices in t1 .
Definition: Given an interpretation f of string s to tree t, and an element p of the domain
of f , the meaning of p relative to s, t, f is the connected subgraph of t whose vertices
include f (p) and all its descendents except any other vertices in the range of f and their
descendents.
Meanings in this sense concern the lowest level of phrasal meanings, occurring at the
terminal nodes of a semantic grammar, namely the entries in the semantic lexicon. The
grammar can then be used to construct the meanings of longer phrases and entire sentences.
This is our motivation for the previously stated constraint that the root must be included
in the range of f : we want all vertices in the sentence representation to be included in the
meaning of some phrase. Note that the meaning of p is also a directed tree with f (p) as its
root. Figure 4 shows the meanings of each phrase in the domain of interpretation function
f1 shown in Figure 3. We show only the labels on the vertices and edges for readability.
Definition: Given a finite set ST F of triples < s1 , t1 , f1 >, . . . , < sn , tn , fn >, where each
si is a finite string, each ti is a directed, labeled tree, and each fi is an interpretation function
from si to ti , let the language LST F = {p1 , . . . , pk } of ST F be the union of all substrings2
that occur in the domain of some fi . For each pj  LST F , the meaning set of pj , denoted
MST F (pj ),3 is the set of all meanings of pj relative to si , ti , fi for some < si , ti , fi > ST F .
We consider two meanings to be the same if they are isomorphic trees taking labels into
account.
For example, given sentence s2 : The man ate the cheese, the labeled tree t2 pictured
in Figure 5, and f2 defined as: f2 (ate) = 1, f2 (man) = 2, f2 (the cheese) = 3; the
2. We consider two substrings to be the same string if they contain the same characters in the same order,
irrespective of their positions within the larger string in which they occur.
3. We omit the subscript on M when the set ST F is obvious from context.

7

fiThompson & Mooney

String s2 : The man ate the cheese."
Tree t2 :

t2 with its vertex and edge labels:
ingest
patient
agent

1
2

4

3

person food
type
age
sex
6

5

male

adult

cheese

Figure 5: A Second Tree
meaning set of the cheese with respect to ST F = {< s1 , t1 , f1 >, < s2 , t2 , f2 >} is {[food,
type:cheese]}, just one meaning though f1 and f2 map the cheese to different vertices
in the two trees, because the subgraphs denoting the meaning of the cheese for the two
functions are isomorphic.
Definition: Given a finite set ST F of triples < s1 , t1 , f1 >, . . . , < sn , tn , fn >, where each
si is a finite string, each ti is a directed, labeled tree, and each fi is an interpretation
function from si to ti , the covering lexicon expressed by ST F is
{(p, m) : p  LST F , m  M (p)}.
The covering lexicon L expressed by ST F = {< s1 , t1 , f1 >, < s2 , t2 , f2 >} is:
{

(girl, [person, sex:female, age:child]),
(man, [person, sex:male, age:adult]),
(ate, [ingest]),
(pasta, [food, type:pasta]),
(the cheese, [food, type:cheese]) }.

The idea of a covering lexicon is that it provides, for each string (sentence) si , a meaning
for some of the phrases in that sentence. Further, these meanings are trees whose labeled
vertices together include each of the labeled vertices in the tree ti representing the meaning
of si , with no vertices duplicated, and containing no vertices not in ti . Edge labels may
or may not be included, since the idea is that some of them are due to syntax, which the
parser will provide; those edges capturing lexical semantics are in the lexicon. Note that
because we only include in the covering lexicon phrases (substrings) that are in the domains
of the fi s, words with the empty tree as meaning are not included in the covering lexicon.
Note also that we will in general use phrase to mean substrings of sentences, whether
they consist of one word, or more than one. Finally the strings in the covering lexicon may
contain overlapping words even though those in the domain of an individual interpretation
function must not, since those overlapping words could have occurred in different sentences.
Finally, we are ready to define the learning problem at hand.

8

fiAcquiring Word-Meaning Mappings

The Lexicon Acquisition Problem:
Given: a multiset of strings S = {s1 , . . . , sn } and a multiset of labeled trees T = {t1 , . . . , tn },
Find: a multiset of interpretation functions, F = {f1 , . . . , fn }, such that the cardinality of
the covering lexicon expressed by ST F = {< s1 , t1 , f1 >, . . . , < sn , tn , fn >} is minimized.
If such a set is found, we say we have found a minimal set of interpretations (or a minimal
covering lexicon). 2
Less formally, a learner is presented with a multiset of sentences (S) paired with their
meanings (T ); the goal of learning is to find the smallest lexicon consistent with this data.
This lexicon is the paired listing of all phrases occurring in the domain of some fi  F
(where F is the multiset of interpretation functions found) with each of the elements in
their meaning sets. The motivation for finding a lexicon of minimal size is the usual bias
towards simplicity of representation and generalization beyond the training data. While
this definition allows for phrases of any length, we will usually want to limit the length
of phrases to be considered for inclusion in the domain of the interpretation functions, for
efficiency purposes.
Once we determine a set of interpretation functions for a set of strings and trees, there
is only one unique covering lexicon expressed by ST F . However, this might not be the
only set of interpretation functions possible, and may not result in the lexicon with smallest
cardinality. For example, the covering lexicon given with the previous example is not a
minimal covering lexicon. For the two sentences given, we could find minimal, though
rather degenerate, lexicons such as:
{

(girl,
(man,

[ingest, agent:[person, sex:female, age:child],
patient:[food, type:pasta, accomp:[food, type:cheese]]]),
[ingest, agent:[person, sex:male, age:adult],
patient:[food, type:cheese]]) }

This type of lexicon becomes less likely as the size of the corpus grows.
3.2 Implications of the Definition
This definition of the lexicon acquisition problem differs from that given by other authors,
including Riloff and Jones (1999), Siskind (1996), Manning (1993), Brent (1991) and others,
as further discussed in Section 7. Our definition of the problem makes some assumptions
about the training input. First, by making f a function instead of a relation, the definition
assumes that the meaning for each phrase in a sentence appears once in the representation
of that sentence, the single-use assumption. Second, by making f one-to-one, it assumes
exclusivity, that each vertex in a sentences representation is due to only one phrase in the
sentence. Third, it assumes that a phrases meaning is a connected subgraph of a sentences
representation, not a more distributed representation, the connectedness assumption. While
the first assumption may not hold for some representation languages, it does not present a
problem in the domains we have considered. The second and third assumptions are perhaps
less problematic with respect to general language use.
Our definition also assumes compositionality: that the meaning of a sentence is derived
from the meanings of the phrases it contains, in addition, perhaps to some connecting
information specific to the representation at hand, but is not derived from external sources
9

fiThompson & Mooney

such as noise. In other words, all the vertices of a sentences representation are included
within the meaning of some word or phrase in that sentence. This assumption is similar
to the linking rules of Jackendoff (1990), and has been used in previous work on grammar
and language acquisition (e.g., Haas and Jayaraman, 1997; Siskind, 19964 ) While there is
some debate in the linguistics community about the ability of compositional techniques to
handle all phenomena (Fillmore, 1988; Goldberg, 1995), making this assumption simplifies
the learning process and works reasonably for the domains of interest here. Also, since we
allow multi-word phrases in the lexicon (e.g., (kick the bucket, die( ))), one objection
to compositionality can be addressed.
This definition also allows training input in which:
1. Words and phrases have multiple meanings. That is, homonymy might occur in the
lexicon.
2. Several phrases map to the same meaning. That is, synonymy might occur in the
lexicon.
3. Some words in a sentence do not map to any meanings, leaving them unused in the
assignment of words to meanings.5
4. Phrases of contiguous words map to parts of a sentences meaning representation.
Of particular note is lexical ambiguity (1 above). Note that we could have also derived an
ambiguous lexicon such as:
{

(girl, [person, sex:female, age:child]),
(ate, [ingest]),
(ate, [ingest, agent:[person, sex:male, age:adult]]),
(pasta, [food, type:pasta]),
(the cheese, [food, type:cheese]) }.

from our sample corpus. In this lexicon, ate is an ambiguous word. The earlier example
minimizes ambiguity resulting in an alternative, more intuitively pleasing lexicon. While
our problem definition first minimizes the number of entries in the lexicon, our learning
algorithm will also exploit a preference for minimizing ambiguity.
Also note that our definition allows training input in which sentences themselves are
ambiguous (paired with more than one meaning), since a given sentence in S (a multiset)
might appear multiple times appear with more than one meaning. In fact, the training data
that we consider in Section 5 does have some ambiguous sentences.
Our definition of the lexicon acquisition problem does not fit cleanly into the traditional
definition of learning for classification. Each training example contains a sentence and its
semantic parse, and we are trying to extract semantic information about some of the phrases
in that sentence. So each example potentially contains information about multiple target
concepts (phrases), and we are trying to pick out the relevant features, or vertices of the
4. In fact, all of these assumptions except for single-use were made by Siskind (1996); see Section 7 for
details.
5. These words may, however, serve as cues to a parser on how to assemble sentence meanings from word
meanings.

10

fiAcquiring Word-Meaning Mappings

representation, corresponding to the correct meaning of each phrase. Of course, our assumptions of single-use, exclusivity, connectedness, and compositionality impose additional
constraints. In addition to this multiple examples in one learning scenario, we do not
have access to negative examples, nor can we derive any implicit negatives, because of the
possibility of ambiguous and synonymous phrases.
In some ways the problem is related to clustering, which is also capable of learning
multiple, potentially non-disjoint categories. However, it is not clear how a clustering
system could be made to learn the phrase-meaning mappings needed for parsing. Finally,
current systems that learn multiple concepts commonly use examples for other concepts as
negative examples of the concept currently being learned. The implicit assumption made
by doing this is that concepts are disjoint, an unwarranted assumption in the presence of
synonymy.

4. The Wolfie Algorithm and an Example
In this section, we first discuss some issues we considered in the design of our algorithm,
then describe it fully in Section 4.2.
4.1 Solving the Lexicon Acquisition Problem
A first attempt to solve the Lexicon Acquisition Problem might be to examine all interpretation functions across the corpus, then choose the one(s) with minimal lexicon size. The
number of possible interpretation functions for a given input pair is dependent on both the
size of the sentence and its representation. In a sentence with w words, there are (w2 )
possible phrases, not a particular challenge.
However, the number of possible interpretation functions grows extremely quickly with
the size of the input. For a sentence with p phrases and an associated tree with n vertices,
the number of possible interpretation functions is:
c!(n  1)!

c
X
i=1

1
.
(i  1)!(n  i)!(c  i)!

(1)

where c is min(p, n). The derivation of the above formula is as follows. We must choose
which phrases to use in the domain of f , and we can choose one phrase, or two, or any
number up to min(p, n) (if n < p we can only assign n phrases since f is one-to-one), or
p
i

!

=

p!
i!(p  i)!

where i is the number of phrases chosen. But we can also permute these phrases, so that
the order in which they are assigned to the vertices is different. There are i! such permutations. We must also choose which vertices to include in the range of the interpretation
function. We have to choose the root each time, so if we are choosing i vertices, we have
n  1 choose i  1 vertices left after choosing the root, or
n1
i1

!

=

(n  1)!
.
(i  1)!(n  i)!
11

fiThompson & Mooney

The full number of possible interpretation functions is then:
min(p,n)

X
i=1

p!
(n  1)!
 i! 
,
i!(p  i)!
(i  1)!(n  i)!

which simplifies to Equation 1. When n = p, the largest term of this equation is c! =
p!, which grows at least exponentially with p, so in general the number of interpretation
functions is too large to allow enumeration. Therefore, finding a lexicon by examining all
interpretations across the corpus, then choosing the lexicon(s) of minimum size, is clearly
not tractable.
Instead of finding all interpretations, one could find a set of candidate meanings for
each phrase, from which the final meaning(s) for that phrase could be chosen in a way that
minimizes lexicon size. One way to find candidate meanings is to fracture the meanings
of sentences in which a phrase appears. Siskind (1993) defined fracturing (he also calls
it the Unlink* operation) over terms such that the result includes all subterms of an
expression plus . In our representation formalism, this corresponds to finding all possible
connected subgraphs of a meaning, and adding the empty graph. Like the interpretation
function technique just discussed, fracturing would also lead to an exponential blowup in
the number of candidate meanings for a phrase: A lower bound on the number of connected
subgraphs for a full binary tree with n vertices is obtained by noting that any subset of
the (n + 1)/2 leaves may be deleted and still maintain connectivity of the remaining tree.
Thus, counting all of the ways that leaves can be deleted gives us a lower bound of 2(n+1)/2
fractures.6 This does not completely rule out fracturing as part of a technique for lexicon
learning since trees do not tend to get very large, and indeed Siskind uses it in many of his
systems, with other constraints to help control the search. However, we wish to avoid any
chance of exponential blowup to preserve the generality of our approach for other tasks.
Another option is to force Chill to essentially induce a lexicon on its own. In this
model, we would provide to Chill an ambiguous lexicon in which each phrase is paired
with every fracture of every sentence in which it appears. Chill would then have to decide
which set of fractures leads to the correct parse for each training sentence, and would only
include those in a final learned parser-lexicon combination. Thus the search would again
become exponential. Furthermore, even with small representations, it would likely lead
to a system with poor generalization ability. While some of Siskinds work (e.g., Siskind,
1992) took syntactic constraints into account and did not encounter such difficulties, those
versions did not handle lexical ambiguity.
If we could efficiently find some good candidates, a standard induction algorithm could
then attempt to use them as a source of training examples for each phrase. However,
any attempt to use the list of candidate meanings of one phrase as negative examples for
another phrase would be flawed. The learner could not know in advance which phrases
are possibly synonymous, and thus which phrase lists to use as negative examples of other
phrase meanings. Also, many representation components would be present in the lists of
more than one phrase. This is a source of conflicting evidence for a learner, even without
the presence of synonymy. Since only positive examples are available, one might think of
using most specific conjunctive learning, or finding the intersection of all the representations
6. Thanks to net-citizen Dan Hirshberg for help with this analysis.

12

fiAcquiring Word-Meaning Mappings

For each phrase, p (of at most two words):
1.1) Collect the training examples in which p appears
1.2) Calculate LICS from (sampled) pairs of these examples representations
1.3) For each l in the LICS, add (p, l) to the set of candidate lexicon entries
Until the input representations are covered, or no candidate lexicon entries remain do:
2.1) Add the best (phrase, meaning) pair from the candidate entries to the lexicon
2.2) Update candidate meanings of phrases in the same sentences as the phrase just learned
Return the lexicon of learned (phrase, meaning) pairs.

Figure 6: Wolfie Algorithm Overview
for each phrase, as proposed by Anderson (1977). However, the meanings of an ambiguous
phrase are disjunctive, and this intersection would be empty. A similar difficulty would be
expected with the positive-only compression of Muggleton (1995).
4.2 Our Solution: Wolfie
The above analysis leads us to believe that the Lexicon Acquisition Problem is computationally intractable. Therefore, we can not perform an efficient search for the best lexicon.
Nor can we use a standard induction algorithm. Therefore, we have implemented Wolfie7 ,
outlined in Figure 6, which finds an approximate solution to the Lexicon Acquisition Problem. Our approach is to generate a set of candidate lexicon entries, from which the final
learned lexicon is derived by greedily choosing the best lexicon item at each point, in the
hopes of finding a final (minimal) covering lexicon. We do not actually learn interpretation
functions, so do not guarantee that we will find a covering lexicon.8 Even if we were to
search for interpretation functions, using a greedy search would also not guarantee covering
the input, and of course it also does not guarantee that a minimal lexicon is found. However,
we will later present experimental results demonstrating that our greedy approach performs
well.
Wolfie first derives an initial set of candidate meanings for each phrase. The algorithm
for generating candidates, LICS, attempts to find a maximally common meaning for each
phrase, which biases toward both finding a small lexicon by covering many vertices of a tree
at once, and finding a lexicon that actually does cover the input. Second, Wolfie chooses
final lexicon entries from this candidate set, one at a time, updating the candidate set as
it goes, taking into account our assumptions of single-use, connectedness, and exclusivity.
The basic scheme for choosing entries from the candidate set is to maximize the prediction
of meanings given phrases, but also to find general meanings. This adds a tension between
LICS, which cover many vertices, and generality, which biases towards fewer vertices. However, generality, like LICS, helps lead to a small lexicon since a general meaning will more
likely apply widely across a corpus.
7. The code is available upon request from the first author.
8. Though, of course, interpretation functions are not the only way to guarantee a covering lexicon  see
Siskind (1993) for an alternative.

13

fiThompson & Mooney

answer/2

1

2

2

S

2

state/1

eq/2

1

1
S

2

C

cityid/2

loc/2
2
1
C

S

1
texarkana
Figure 7: Tree with Variables
Let us explain the algorithm in further detail by way of an example, using Spanish
instead of English to illustrate the difficulty somewhat more clearly. Consider the following
corpus:
1.  Cual es el capital del estado con la poblacion mas grande?
answer(C, (capital(S,C), largest(P, (state(S), population(S,P))))).
2.  Cual es la punta mas alta del estado con la area mas grande?
answer(P, (high point(S,P), largest(A, (state(S), area(S,A))))).
3.  En que estado se encuentra Texarkana?
answer(S, (state(S), eq(C,cityid(texarkana, )), loc(C,S))).
4.  Que capital es la mas grande?
answer(A, largest(A, capital(A))).
5.  Que es la area de los estados unitos?
answer(A, (area(C,A), eq(C,countryid(usa)))).
6.  Cual es la poblacion de un estado que bordean a Utah?
answer(P, (population(S,P), state(S), next to(S,M), eq(M,stateid(utah)))).
7.  Que es la punta mas alta del estado con la capital Madison?
answer(C, (high point(B,C), loc(C,B), state(B),
capital(B,A), eq(A,cityid(madison, )))).

The sentence representations here are slightly different than the tree representations given in
the problem definition, with the main difference being the addition of existentially quantified
variables shared between some leaves of a representation tree. As mentioned in Section 2.1,
the representations are Prolog queries to a database. Given such a query, we can create
a tree that conforms to our formalism, but with this addition of quantified variables. An
example is shown in Figure 7 for the representation of the third sentence. Each vertex is
a predicate name and its arity, in the Prolog style, e.g., state/1, with quantified variables
at some of the leaves. For each outgoing edge (n, m) of a vertex n, the edge is labeled with
the argument position filled by the subtree rooted by m. If there is not an edge labeled
with a given argument position, the argument is a free variable. Each vertex labeled with a
14

fiAcquiring Word-Meaning Mappings

variable (which can occur only at leaves) is an existentially quantified variable whose scope
is the entire tree (or query). The learned lexicon, however, does not need to maintain the
identity between variables across distinct lexical entries.
Another representation difference is that we will strip the answer predicate from the
input to our learner,9 thus allowing a forest of directed trees as input rather than a single
tree. The definition of the problem easily extends such that the root of each tree in the
forest must be in the domain of some interpretation function.
Evaluation of our system using this representation is given in Section 5.1; evaluation
using a representation without variables or forests is presented in Section 5.2. We previously
(Thompson, 1995) presented results demonstrating learning representations of a different
form, that of a case-role representation (Fillmore, 1968) augmented with Conceptual Dependency (Schank, 1975) information. This last representation conforms directly to our
problem definition.
Now, continuing with the example of solving the Lexicon Acquisition Problem for this
corpus, let us also assume for simplification, although not required, that sentences are
stripped of phrases that we know have empty meanings (e.g., que, es, con, and la).
We will similarly assume that it is known that some phrases refer directly to given database
constants (e.g., location names), and remove those phrases and their meaning from the
training input.
4.2.1 Candidate Generation Phase
Initial candidate meanings for a phrase are produced by computing the maximally common
substructure(s) between sampled pairs of representations of sentences that contain it. We
derive common substructure by computing the Largest Isomorphic Connected Subgraphs
(LICS) of two labeled trees, taking labels into account in the isomorphism. The analogous
Largest Common Subgraph problem (Garey & Johnson, 1979) is solvable in polynomial
time if, as we assume, both inputs are trees and if K, the number of edges to include, is
given. Thus, we start with K set equal to the largest number of edges in the two trees being
compared, test for common subgraph(s), and iterate down to K = 1, stopping when one or
more subgraphs are found for a given K.
For the Prolog query representation, the algorithm is complicated a bit by variables.
Therefore, we use LICS with an addition similar to computing the Least General Generalization of first-order clauses (Plotkin, 1970). The LGG of two sets of literals is the least
general set of literals that subsumes both sets of literals. We add to this by allowing that
when a term in the argument of a literal is a conjunction, the algorithm tries all orderings
in its matching of the terms in the conjunction. Overall, our algorithm for finding the LICS
between two trees in the Prolog representation first finds the common labeled edges and
vertices as usual in LICS, but treats all variables as equivalent. Then, it computes the
Least General Generalization, with conjunction taken into account, of the resulting trees as
converted back into literals. For example, given the two trees:

9. The predicate is omitted because Chill initializes the parse stack with the answer predicate, and thus
no word has to be mapped to it.

15

fiThompson & Mooney

Phrase
capital:

grande:
estado:

punta mas:
encuentra:

LICS
largest( , )
capital( , )
state( )
largest( ,state( ))
largest( , )
largest( ,state( ))
state( )
(population(S, ), state(S))
capital( , )
high point( , )
(state(S), loc( ,S))
high point( , )
state( )
(state(S), loc( ,S))

From Sentences
1,4
1,7
1,7
1,2
1,4; 2,4
1,2
1,3; 1,7; 2,3; 2,6; 2,7; 3,6; 6,7
1,6
1,7
2,7
3,7
2,7
2,7
3

Table 1: Sample Candidate Lexical Entries and their Derivation
answer(C, (largest(P, (state(S), population(S,P))), capital(S,C))).
answer(P, (high point(S,P), largest(A, (state(S), area(S,A))))).,
the common meaning is answer( ,largest( ,state( )). Note that the LICS of two trees
may not be unique: there may be multiple common subtrees that both contain the same
number of edges; in this case LICS returns multiple answers.
The sets of initial candidate meanings for some of the phrases in the sample corpus are
shown in Table 1. While in this example we show the LICS for all pairs that a phrase
appears in, in the actual algorithm we randomly sample a subset for efficiency reasons,
as in Golem (Muggleton & Feng, 1990). For phrases appearing in only one sentence
(e.g., encuentra), the entire sentence representation (excluding the database constant
given as background knowledge) is used as an initial candidate meaning. Such candidates
are typically generalized in step 2.2 of the algorithm to only the correct portion of the
representation before they are added to the lexicon; we will see an example of this below.
4.2.2 Adding to the Final Lexicon
After deriving initial candidates, the greedy search begins. The heuristic used to evaluate
candidates attempts to help assure that a small but covering lexicon is learned. The heuristic
first looks at the weighted sum of two components, where p is the phrase and m its candidate
meaning:
1. P (m | p)  P (p | m)  P (m) = P (p)  P (m | p)2
2. The generality of m
Then, ties in this value are broken by preferring less ambiguous (those with fewer current
meanings) and shorter phrases. The first component is analogous the cluster evaluation
16

fiAcquiring Word-Meaning Mappings

heuristic used by Cobweb (Fisher, 1987), which measures the utility of clusters based on
attribute-value pairs and categories, instead of meanings and phrases. The probabilities
are estimated from the training data and then updated as learning progresses to account
for phrases and meanings already covered. We will see how this updating works as we
continue through our example of the algorithm. The goal of this part of the heuristic
is to maximize the probability of predicting the correct meaning for a randomly sampled
phrase. The equality holds by Bayes Theorem. Looking at the right side, P (m | p)2 is
the expected probability that meaning m is correctly guessed for a given phrase, p. This
assumes a strategy of probability matching, in which a meaning m is chosen for p with
probability P (m | p) and correct with the same probability. The other term, P (p), biases
the component by how common the phrase is. Interpreting the left side of the equation, the
first term biases towards lexicons with low ambiguity, the second towards low synonymy,
and the third towards frequent meanings.
The second component of the heuristic, generality, is computed as the negation of
the number of vertices in the meanings tree structure, and helps prefer smaller, more
general meanings. For example, in the candidate set above, if all else were equal, the
generality portion of the heuristic would prefer state( ), with generality value -1, over
largest( ,state( )) and (state(S),loc( ,S)), each with generality value -2, as the
meaning of estado. Learning a meaning with fewer terms helps evenly distribute the
vertices in a sentences representation among the meanings of the phrases in that sentence,
and thus leads to a lexicon that is more likely to be correct. To see this, we note that some
pairs of words tend to frequently co-occur (grande and estado in our example), and
so their joint representation (meaning) is likely to be in the set of candidate meanings for
both words. By preferring a more general meaning, we easily ignore these incorrect joint
meanings.
In this example and all experiments, we use a weight of 10 for the first component of
the heuristic, and a weight of 1 for the second. The first component has smaller absolute
values and is therefore given a higher weight. Modulo this consideration, results are not
overly-sensitive to the weights and automatically setting them using cross-validation on the
training set (Kohavi & John, 1995) had little effect on overall performance. In Table 2 we
illustrate the calculation of the heuristic measure for some of the above fourteen pairs, and
its value for all. The calculation shows the sum of multiplying 10 by the first component of
the heuristic and multiplying 1 by the second component. The first component is simplified
as follows:
| p | | m  p |2
| m  p |2
P (p)  P (m | p)2 =


,
t
| p |2
|p|
where | p | is the number of times phrase p appears in the corpus, t is the initial number
of candidate phrases, and | m  p | is the number of times that meaning m is paired with
phrase p. We can ignore t since the number of phrases in the corpus is the same for each
pair, and has no effect on the ranking. The highest scoring pair is (estado, state( )), so
it is added to the lexicon.
Next is the candidate generalization step (2.2), described algorithmically in Figure 8.
One of the key ideas of the algorithm is that each phrase-meaning choice can constrain the
candidate meanings of phrases yet to be learned. Given the assumption that each portion of
the representation is due to at most one phrase in the sentence (exclusivity), once part of a
17

fiThompson & Mooney

Candidate Lexicon Entry
(capital, largest( , )):
(capital, capital( , )):
(capital, state( , )):
(grande, largest( ,state( ))):
(grande, largest( , )):
(estado, largest( ,state( ))):
(estado, state( )):
(estado, (population(S, ), state(S)):
(estado, capital( , )):
(estado, high point( , )):
(estado, (state(S), loc( ,S))):
(punta mas, high point( , )):
(punta mas, state( )):
(encuentra, (state(S), loc( ,S))):

Heuristic Value
10(22 /3) + 1(1) = 12.33
12.33
12.33
10(22 /3) + 1(2) = 11.3
29
10(22 /5) + 1(2) = 6
10(52 /5) + 1(1) = 49
6
7
7
6
19
10(22 /2) + 1(1) = 19
10(12 /1) + 1(2) = 8

Table 2: Heuristic Value of Sample Candidate Lexical Entries

Given: A learned phrase-meaning pair (l, g)
For all sentence-representation pairs containing l and g, mark them as covered.
For each candidate phrase-meaning pair (p, m):
If p occurs in some training pairs with (l, g) then
If the vertices of m intersect the vertices of g then
If all occurrences of m are now covered then
Remove (p, m) from the set of candidate pairs.
Else
Adjust the heuristic value of (p, m) as needed to account
for newly covered nodes of the training representations.
Generalize m to remove covered nodes, obtaining m0 , and
Calculate the heuristic value of the new candidate pair (p, m0 ).
If no candidate meanings remain for an uncovered phrase then
Derive new LICS from uncovered representations and
calculate their heuristic values.

Figure 8: The Candidate Generalization Phase

18

fiAcquiring Word-Meaning Mappings

representation is covered, no other phrase in the sentence can be paired with that meaning
(at least for that sentence). Therefore, in step 2.2 the candidate meanings for words in
the same sentences as the word just learned are generalized to exclude the representation
just learned. We use an operation analogous to set difference when finding the remaining
uncovered vertices of the representation when generalizing meanings to eliminate covered
vertices from candidate pairs. For example, if the meaning largest( , ) were learned
for a phrase in sentence 2, the meaning left behind would be a forest consisting of the
trees high point(S, ) and (state(S), area(S, )). Also, if the generalization results
in an empty tree, new LICS are calculated. In our example, since state( ) is covered
in sentences 1, 2, 3, 6, and 7, the candidates for several other words in those sentences
are generalized. For example, the meaning (state(S), loc( ,S)) for encuentra, is
generalized to loc( , ), with a new heuristic value of 10(12 /1) + 1(1) = 9. Also, our
single-use assumption allows us to remove all candidate pairs containing estado from the
set of candidate meanings, since the learned pair covers all occurrences of estado in that
set.
Note that the pairwise matchings to generate candidate items, together with this updating of the candidate set, enable multiple meanings to be learned for ambiguous phrases,
and makes the algorithm less sensitive to the initial rate of sampling for LICS. For example,
note that capital is ambiguous in this data set, though its ambiguity is an artifact of
the way that the query language was designed, and one does not ordinarily think of it as
an ambiguous word. However, both meanings will be learned: The second pair added to
the final lexicon is (grande, largest( , )), which causes a generalization to the empty
meaning for the first candidate entry in Table 2, and since no new LICS from sentence 4
can be generated, its entire remaining meaning is added to the candidate meaning set for
both capital and mas.
Subsequently, the greedy search continues until the resulting lexicon covers the training
corpus, or until no candidate phrase meanings remain. In rare cases, learning errors occur
that leave some portions of representations uncovered. In our example, the following lexicon
is learned:
(estado, state( )),
(grande, largest( )),
(area, area( )),
(punta, high point( , )),
(poblacion, population( , )),
(capital, capital( , )),
(encuentra, loc( , )),
(alta, loc( , )),
(bordean, next to( )),
(capital, capital( )).
In the next section, we discuss the ability of Wolfie to learn lexicons that are useful for
parsers and parser acquisition.

19

fiThompson & Mooney

5. Evaluation of Wolfie
The following two sections discuss experiments testing Wolfies success in learning lexicons
for both real and artificial corpora, comparing it in several cases to a previously developed
lexicon learning system.
5.1 A Database Query Application
This section describes our experimental results on a database query application. The first
corpus discussed contains 250 questions about U.S. geography, paired with their Prolog
query to extract the answer to the question from a database. This domain was originally
chosen due to the availability of a hand-built natural language interface, Geobase, to
a database containing about 800 facts. Geobase was supplied with Turbo Prolog 2.0
(Borland International, 1988), and designed specifically for this domain. The questions in
the corpus were collected by asking undergraduate students to generate English questions for
this database, though they were given only cursory knowledge of the database without being
given a chance to use it. To broaden the test, we had the same 250 sentences translated into
Spanish, Turkish, and Japanese. The Japanese translations are in word-segmented Roman
orthography. Translated questions were paired with the appropriate logical queries from
the English corpus.
To evaluate the learned lexicons, we measured their utility as background knowledge
for Chill. This is performed by choosing a random set of 25 test examples and then
learning lexicons and parsers from increasingly larger subsets of the remaining 225 examples
(increasing by 50 examples each time). After training, the test examples are parsed using
the learned parser. We then submit the resulting queries to the database, compare the
answers to those generated by submitting the correct representation to the database, and
record the percentage of correct (matching) answers. By using the difficult gold standard
of retrieving a correct answer, we avoid measures of partial accuracy that we believe do not
adequately measure final utility. We repeated this process for ten different random training
and test sets and evaluated performance differences using a two-tailed, paired t-test with a
significance level of p  0.05.
We compared our system to an incremental (on-line) lexicon learner developed by Siskind
(1996). To make a more equitable comparison to our batch algorithm, we ran his in a simulated batch mode, by repeatedly presenting the corpus 500 times, analogous to running
500 epochs to train a neural network. While this does not actually add new kinds of data
over which to learn, it allows his algorithm to perform inter-sentential inference in both directions over the corpus instead of just one. Our point here is to compare accuracy over the
same size training corpus, a metric not optimized for by Siskind. We are not worried about
the difference in execution time here,10 and the lexicons learned when running Siskinds
system in incremental mode (presenting the corpus a single time) resulted in substantially
lower performance in preliminary experiments with this data. We also removed Wolfies
ability to learn phrases of more than one word, since the current version of Siskinds system
10. The CPU times of the two system are not directly comparable since one is written in Prolog and the
other in Lisp. However, the learning time of the two systems is approximately the same if Siskinds
system is run in incremental mode, just a few seconds with 225 training examples.

20

fiAcquiring Word-Meaning Mappings

90

80

70

Accuracy

60

50

40

30

CHILL+handbuilt
CHILL-testlex
CHILL+Wolfie
CHILL+Siskind
Geobase

20

10

0
0

50

100
150
Training Examples

200

250

Figure 9: Accuracy on English Geography Corpus
does not have this ability. Finally, we made comparisons to the parsers learned by Chill
when using a hand-coded lexicon as background knowledge.
In this and similar applications, there are many terms, such as state and city names,
whose meanings can be automatically extracted from the database. Therefore, all tests
below were run with such names given to the learner as an initial lexicon; this is helpful
but not required. Section 5.2 gives results for a different task with no such initial lexicon.
However, unless otherwise noted, for all tests within this Section (5.1) we did not strip
sentences of phrases known to have empty meanings, unlike in the example of Section 4.
5.1.1 Comparisons using English
The first experiment was a comparison on the original English corpus. Figure 9 shows
learning curves for Chill when using the lexicons learned by Wolfie (CHILL+Wolfie) and
by Siskinds system (CHILL+Siskind). The uppermost curve (CHILL+handbuilt) shows
Chills performance when given the hand-built lexicon. CHILL-testlex shows the performance when words that never appear in the training data (e.g., are only in the test sentences)
are deleted from the hand-built lexicon (since a learning algorithm has no chance of learning
these). Finally, the horizontal line shows the performance of the Geobase benchmark.
The results show that a lexicon learned by Wolfie led to parsers that were almost as
accurate as those generated using a hand-built lexicon. The best accuracy is achieved by
parsers using the hand-built lexicon, followed by the hand-built lexicon with words only in
the test set removed, followed by Wolfie, followed by Siskinds system. All the systems
do as well or better than Geobase by the time they reach 125 training examples. The
differences between Wolfie and Siskinds system are statistically significant at all training
21

fiThompson & Mooney

Lexicon
hand-built
Wolfie
Siskind

Coverage
100%
100%
94.4%

Ambiguity
1.2
1.1
1.7

Entries
88
56.5
154.8

Table 3: Lexicon Comparison
example sizes. These results show that Wolfie can learn lexicons that support the learning
of successful parsers, and that are better from this perspective than those learned by a
competing system. Also, comparing to the CHILL-testlex curve, we see that most of the
drop in accuracy from a hand-built lexicon is due to words in the test set that the system
has not seen during training. In fact, none of the differences between CHILL+Wolfie and
CHILL-testlex are statistically significant.
One of the implicit hypotheses of our problem definition is that coverage of the training
data implies a good lexicon. The results show a coverage of 100% of the 225 training examples for Wolfie versus 94.4% for Siskind. In addition, the lexicons learned by Siskinds
system were more ambiguous and larger than those learned by Wolfie. Wolfies lexicons had an average of 1.1 meanings per word, and an average size of 56.5 entries (after
225 training examples) versus 1.7 meanings per word and 154.8 entries in Siskinds lexicons. For comparison, the hand-built lexicon had 1.2 meanings per word and 88 entries.
These differences, summarized in Table 3, undoubtedly contribute to the final performance
differences.
5.1.2 Performance for Other Natural Languages
Next, we examined the performance of the two systems on the Spanish version of the corpus.
Figure 10 shows the results. The differences between using Wolfie and Siskinds learned
lexicons for Chill are again statistically significant at all training set sizes. We also again
show the performance with hand-built lexicons, both with and without phrases present only
in the testing set. The performance compared to the hand-built lexicon with test-set phrases
removed is still competitive, with the difference being significant only at 225 examples.
Figure 11 shows the accuracy of learned parsers with Wolfies learned lexicons for
all four languages. The performance differences among the four languages are quite small,
demonstrating that our methods are not language dependent.
5.1.3 A Larger Corpus
Next, we present results on a larger, more diverse corpus from the geography domain,
where the additional sentences were collected from computer science undergraduates in
an introductory AI course. The set of questions in the smaller corpus was collected from
students in a German class, with no special instructions on the complexity of queries desired.
The AI students tended to ask more complex and diverse queries: their task was to give five
interesting questions and the associated logical form for a homework assignment, though
again they did not have direct access to the database. They were requested to give at least
one sentence whose representation included a predicate containing embedded predicates, for

22

fiAcquiring Word-Meaning Mappings

100
90
80
70

Accuracy

60
50
40
30

Span-CHILL+handbuilt
Span-CHILL-testlex
Span-CHILL+Wolfie
Span-CHILL+Siskind

20
10
0
0

50

100
150
Training Examples

200

250

200

250

Figure 10: Accuracy on Spanish

100
90
80
70

Accuracy

60
50
40
30

English
Spanish
Japanese
Turkish

20
10
0
0

50

100
150
Training Examples

Figure 11: Accuracy on All Four Languages

23

fiThompson & Mooney

100
90
80
70

Accuracy

60
50
40
30

CHILL
WOLFIE
Geobase

20
10
0
0

50

100

150

200
250
Training Examples

300

350

400

450

Figure 12: Accuracy on the Larger Geography Corpus
example largest(S, state(S)), and we asked for variety in their sentences. There were
221 new sentences, for a total of 471 (including the original 250 sentences).
For these experiments, we split the data into 425 training sentences and 46 test sentences, for 10 random splits, then trained Wolfie and then Chill as before. Our goal was
to see whether Wolfie was still effective for this more difficult corpus, since there were
approximately 40 novel words in the new sentences. Therefore, we tested against the performance of Chill with an extended hand-built lexicon. For this test, we stripped sentences
of phrases known to have empty meanings, as in the example of Section 4.2. Again, we
did not use phrases of more than one word, since these do not seem to make a significant
difference in this domain. For these results, we compare Wolfies lexicons for Chill using
hand-built lexicons without phrases that only appear in the test set.
Figure 12 shows the resulting learning curves. The differences between Chill using
the hand-built and learned lexicons are statistically significant at 175, 225, 325, and 425
examples (four out of the nine data points). The more mixed results here indicate both the
difficulty of the domain and the more variable vocabulary. However, the improvement of
machine learning methods over the Geobase hand-built interface is much more dramatic
for this corpus.
5.1.4 LICS versus Fracturing
One component of the algorithm not yet evaluated explicitly is the candidate generation
method. As mentioned in Section 4.1, we could use fractures of representations of sentences
in which a phrase appears to generate the candidate meanings for that phrase, instead
of LICS. We used this approach and compared it to the previously described method of
using the largest isomorphic connected subgraphs of sampled pairs of representations as

24

fiAcquiring Word-Meaning Mappings

100
90
80
70

Accuracy

60
50
40
30

fractWOLFIE
WOLFIE

20
10
0
0

50

100
150
Training Examples

200

250

Figure 13: Fracturing vs. LICS: Accuracy
candidate meanings. To attempt a more fair comparison, we also sampled representations
for fracturing, using the same number of source representations as the number of pairs
sampled for LICS.
The accuracy of Chill when using the resulting learned lexicons as background knowledge are shown in Figure 13. Using fracturing (fractWOLFIE) shows little or no advantage;
none of the differences between the two systems are statistically significant.
In addition, the number of initial candidate lexicon entries from which to choose is
much larger for fracturing than our LICS method, as shown in Figure 14. This is true even
though we sampled the same number of representations as pairs for LICS, because there
are a larger number of fractures for an arbitrary representation than the number of LICS
for an arbitrary pair. Finally, Wolfies learning time when using fracturing is greater than
that when using LICS, as shown in Figure 15, where the CPU time is shown in seconds.
In summary, these differences show the utility of LICS as a method for generating
candidates: a more thorough method does not result in better performance, and also results
in longer learning times. One could claim that we are handicapping fracturing since we are
only sampling representations for fracturing. This may indeed help the accuracy, but the
learning time and the number of candidates would likely suffer even further. In a domain
with larger representations, the differences in learning time would be even more dramatic.
5.2 Artificial Data
The previous section showed that Wolfie successfully learns lexicons for a natural corpus
and a realistic task. However, this demonstrates success on only a relatively small corpus
and with one representation formalism. We now show that our algorithm scales up well
with more lexicon items to learn, more ambiguity, and more synonymy. These factors are

25

fiThompson & Mooney

600

Number of Candidates

500

400

300

fractWOLFIE
WOLFIE

200

100

0
0

50

100
150
Training Examples

200

250

Figure 14: Fracturing vs. LICS: Number of Candidates

4

3.5

Learning Time (sec)

3

2.5

2

1.5

1

0.5

fractWOLFIE
WOLFIE

0
0

50

100
150
Training Examples

200

Figure 15: Fracturing vs. LICS: Learning Time

26

250

fiAcquiring Word-Meaning Mappings

difficult to control when using real data as input. Also, there are no large corpora available
that are annotated with semantic parses. We therefore present experimental results on an
artificial corpus. In this corpus, both the sentences and their representations are completely
artificial, and the sentence representation is a variable-free representation, as suggested by
the work of Jackendoff (1990) and others.
For each corpus discussed below, a random lexicon mapping words to simulated meanings
was first constructed.11 This original lexicon was then used to generate a corpus of random
utterances each paired with a meaning representation. After using this corpus as input to
Wolfie12 , the learned lexicon was compared to the original lexicon, and weighted precision
and weighted recall of the learned lexicon were measured. Precision measures the percentage
of the lexicon entries (i.e., word-meaning pairs) that the system learns that are correct.
Recall measures the percentage of the lexicon entries in the hand-built lexicon that are
correctly learned by the system:
precision =
recall =

# correct pairs
# pairs learned

# correct pairs
.
# pairs in hand-built lexicon

To get weighted precision and recall measures, we then weight the results for each pair
by the words frequency in the entire corpus (not just the training corpus). This models
how likely we are to have learned the correct meaning for an arbitrarily chosen word in the
corpus.
We generated several lexicons and associated corpora, varying the ambiguity rate (number of meanings per word) and synonymy rate (number of words per meaning), as in Siskind
(1996). Meaning representations were generated using a set of conceptual symbols that
combined to form the meaning for each word. The number of conceptual symbols used in
each lexicon will be noted when we describe each corpus below. In each lexicon, 47.5%
of the senses were variable-free to simulate noun-like meanings, and 47.5% contained from
one to three variables to denote open argument positions to simulate verb-like meanings.
The remainder of the words (the remaining 5%) had the empty meaning to simulate function words. In addition, the functors in each meaning could have a depth of up to two
and an arity of up to two. An example noun-like meaning is f23(f2(f14)), and a verbmeaning f10(A,f15(B)); the conceptual symbols in this example are f23, f2, f14, f10,
and f15. By using these multi-level meaning representations we demonstrate the learning
of more complex representations than those in the geography database domain: none of the
hand-built meanings for phrases in that lexicon had functors embedded in arguments. We
used a grammar to generate utterances and their meanings from each original lexicon, with
terminal categories selected using a distribution based on Zipfs Law (Zipf, 1949). Under
Zipfs Law, the occurrence frequency of a word is inversely proportional to its ranking by
occurrence.
We started with a baseline corpus generated from a lexicon of 100 words using 25 conceptual symbols and no ambiguity or synonymy; 1949 sentence-meaning pairs were generated.
11. Thanks to Jeff Siskind for the initial corpus generation software, which we enhanced for these tests.
12. In these tests, we allowed Wolfie to learn phrases of up to length two.

27

fiThompson & Mooney

100
90
80
70

Accuracy

60
50
40
30

Precision
Recall

20
10
0
0

200

400

600

800
1000
Training Examples

1200

1400

1600

1800

Figure 16: Baseline Artificial Corpus
We split this into five training sets of 1700 sentences each. Figure 16 shows the weighted
precision and recall curves for this initial test. This demonstrates good scalability to a
slightly larger corpus and lexicon than that of the U.S. geography query domain.
A second corpus was generated from a second lexicon, also of 100 words using 25 conceptual symbols, but increasing the ambiguity to 1.25 meanings per word. This time, 1937 pairs
were generated and the corpus split into five sets of 1700 training examples each. Weighted
precision at 1650 examples drops to 65.4% from the previous level of 99.3%, and weighted
recall to 58.9% from 99.3%. The full learning curve is shown in Figure 17. A quick comparison to Siskinds performance on this corpus confirmed that his system achieved comparable
performance, showing that with current methods, this is close to the best performance that
we are able to obtain on this more difficult corpus. One possible explanation for the smaller
performance difference between the two systems on this corpus versus the geography domain is that in this domain, the correct meaning for a word is not necessarily the most
general, in terms of number of vertices, of all its candidate meanings. Therefore, the
generality portion of the heuristic may negatively influence the performance of Wolfie in
this domain.
Finally, we show the change in performance with increasing ambiguity and increasing
synonymy, holding the number of words and conceptual symbols constant. Figure 18 shows
the weighted precision and recall with 1050 training examples for increasing levels of ambiguity, holding the synonymy level constant. Figure 19 shows the results at increasing
levels of synonymy, holding ambiguity constant. Increasing the level of synonymy does not
effect the results as much as increasing the level of ambiguity, which is as we expected.
Holding the corpus size constant but increasing the number of competing meanings for a
word increases the number of candidate meanings created by Wolfie while decreasing the
amount of evidence available for each meaning (e.g., the first component of the heuristic
28

fiAcquiring Word-Meaning Mappings

70

60

Accuracy

50

40

30

Precision
Recall

20

10

0
0

200

400

600

800
1000
Training Examples

1200

1400

1600

1800

Figure 17: A More Ambiguous Artificial Corpus
100

95

90

Recall
Precision

Accuracy

85

80

75

70

65

60
1

1.25

1.5
Number of Meanings per Word

1.75

2

Figure 18: Increasing the Level of Ambiguity
measure) and makes the learning task more difficult. On the other hand, increasing the
level of synonymy does not have the potential to mislead the learner.
The number of training examples required to reach a certain level of accuracy is also
informative. In Table 4, we show the point at which a standard precision of 75% was first

29

fiThompson & Mooney

100

Accuracy

95

90

Recall
Precision

85

80
1

1.25

1.5
Number of Words per Meaning

1.75

2

Figure 19: Increasing the Level of Synonymy
Ambiguity Level
1.0
1.25
2.0

Number of Examples
150
450
1450

Table 4: Number of Examples to Reach 75% Precision
reached for each level of ambiguity. Note, however, that we only measured accuracy after
each set of 100 training examples, so the numbers in the table are approximate.
We performed a second test of scalability on two corpora generated from lexicons an
order of magnitude larger than those in the above tests. In these tests, we use a lexicon
containing 1000 words and using 250 conceptual symbols. We generated both a corpus with
no ambiguity, and one from a lexicon with ambiguity and synonymy similar to that found
in the WordNet database (Beckwith, Fellbaum, Gross, & Miller, 1991); the ambiguity there
is approximately 1.68 meanings per word and the synonymy 1.3 words per meaning. These
corpora contained 9904 (no ambiguity) and 9948 examples, respectively, and we split the
data into five sets of 9000 training examples each. For the easier large corpus, the maximum
average of weighted precision and recall was 85.6%, at 8100 training examples, while for the
harder corpus, the maximum average was 63.1% at 8600 training examples.

6. Active Learning
As indicated in the previous sections, we have built an integrated system for language
acquisition that is flexible and useful. However, a major difficulty remains: the construction
of training corpora. Though annotating sentences is still arguably less work than building
30

fiAcquiring Word-Meaning Mappings

Apply the learner to n bootstrap examples, creating a classifier.
Until no examples remain or the annotator is unwilling to label more examples, do:
Use most recently learned classifier to annotate each unlabeled instance.
Find the k instances with the lowest annotation certainty.
Annotate these instances.
Train the learner on the bootstrap examples and all examples annotated so far.

Figure 20: Selective Sampling Algorithm
an entire system by hand, the annotation task is also time-consuming and error-prone.
Further, the training pairs often contain redundant information. We would like to minimize
the amount of annotation required while still maintaining good generalization accuracy.
To do this, we turned to methods in active learning. Active learning is a research area
in machine learning that features systems that automatically select the most informative
examples for annotation and training (Angluin, 1988; Seung, Opper, & Sompolinsky, 1992),
rather than relying on a benevolent teacher or random sampling. The primary goal of
active learning is to reduce the number of examples that the system is trained on, while
maintaining the accuracy of the acquired information. Active learning systems may construct their own examples, request certain types of examples, or determine which of a set
of unsupervised examples are most usefully labeled. The last approach, selective sampling
(Cohn et al., 1994), is particularly attractive in natural language learning, since there is an
abundance of text, and we would like to annotate only the most informative sentences. For
many language learning tasks, annotation is particularly time-consuming since it requires
specifying a complex output rather than just a category label, so reducing the number of
training examples required can greatly increase the utility of learning.
In this section, we explore the use of active learning, specifically selective sampling, for
lexicon acquisition, and demonstrate that with active learning, fewer examples are required
to achieve the same accuracy obtained by training on randomly chosen examples.
The basic algorithm for selective sampling is relatively simple. Learning begins with a
small pool of annotated examples and a large pool of unannotated examples, and the learner
attempts to choose the most informative additional examples for annotation. Existing work
in the area has emphasized two approaches, certainty-based methods (Lewis & Catlett,
1994), and committee-based methods (McCallum & Nigam, 1998; Freund, Seung, Shamir,
& Tishby, 1997; Liere & Tadepalli, 1997; Dagan & Engelson, 1995; Cohn et al., 1994); we
focus here on the former.
In the certainty-based paradigm, a system is trained on a small number of annotated
examples to learn an initial classifier. Next, the system examines unannotated examples,
and attaches certainties to the predicted annotation of those examples. The k examples with
the lowest certainties are then presented to the user for annotation and retraining. Many
methods for attaching certainties have been used, but they typically attempt to estimate
the probability that a classifier consistent with the prior training data will classify a new
example correctly.

31

fiThompson & Mooney

Learn a lexicon with the examples annotated so far
1) For each phrase in an unannotated sentence:
If it has entries in the learned lexicon then
its certainty is the average of the heuristic values of those entries
Else, if it is a one-word phrase then
its certainty is zero
2) To rank sentences use:
Total certainty of phrases from step 1
# of phrases counted in step 1

Figure 21: Active Learning for Wolfie
Figure 20 presents abstract pseudocode for certainty-based selective sampling. In an
ideal situation, the batch size, k, would be set to one to make the most intelligent decisions
in future choices, but for efficiency reasons in retraining batch learning algorithms, it is
frequently set higher. Results on a number of classification tasks have demonstrated that
this general approach is effective in reducing the need for labeled examples (see citations
above).
Applying certainty-based sample selection to Wolfie requires determining the certainty
of a complete annotation of a potential new training example, despite the fact that individual
learned lexical entries and parsing operators perform only part of the overall annotation task.
Therefore, our general approach is to compute certainties for pieces of an example, in our
case, phrases, and combine these to obtain an overall certainty for an example. Since lexicon
entries contain no explicit uncertainty parameters, we used Wolfies heuristic measure to
estimate uncertainty.
To choose the sentences to be annotated in each round, we first bootstrapped an initial
lexicon from a small corpus, keeping track of the heuristic values of the learned items.
Then, for each unannotated sentence, we took an average of the heuristic values of the
lexicon entries learned for phrases in that sentence, giving a value of zero to unknown
words but eliminating from consideration any words that we assume are known in advance,
such as database constants. Thus, longer sentences with only a few known phrases would
have a lower certainty than shorter sentences with the same number of known phrases; this
is desirable since longer sentences will be more informative from a lexicon learning point
of view. The sentences with the lowest values were chosen for annotation, added to the
bootstrap corpus, and a new lexicon learned. Our technique is summarized in Figure 21.
To evaluate our technique, we compared active learning to learning from randomly selected examples, again measuring the effectiveness of learned lexicons as background knowledge for Chill. We again used the (smaller) U.S. Geography corpus, as in the original
Wolfie tests, using the lexicons as background knowledge during parser acquisition (and
using the same examples for parser acquisition).
For each trial in the following experiments, we first randomly divide the data into a
training and test set. Then, n = 25 bootstrap examples are randomly selected from the

32

fiAcquiring Word-Meaning Mappings

100

80

Accuracy

60

40
WOLF+active
WOLFIE
Geobase
20

0
0

50

100
150
Training Examples

200

250

Figure 22: Using Lexicon Certainty for Active Learning
training examples and in each step of active learning, the least certain k = 10 examples
of the remaining training examples are selected and added to the training set. The result
of learning on this set is evaluated after each step. The accuracy of the resulting learned
parsers was compared to the accuracy of those learned using randomly chosen examples to
learn lexicons and parsers, as in Section 5; in other words, we can think of the k examples
in each round as being chosen randomly.
Figure 22 shows the accuracy on unseen data of parsers learned using the lexicons learned
by Wolfie when examples are chosen randomly and actively. There is an annotation
savings of around 50 examples by using active learning: the maximum accuracy is reached
after 175 examples, versus 225 with random examples. The advantage of using active
learning is clear from the beginning, though the differences between the two curves are only
statistically significant at 175 training examples. Since we are learning both lexicons and
parsers, but only choosing examples based on Wolfies certainty measures, the boost could
be improved even further if Chill had a say in the examples chosen. See Thompson, Califf,
and Mooney (1999) for a description of active learning for Chill.

7. Related Work
In this section, we divide the previous research on related topics into the areas of lexicon
acquisition and active learning.
7.1 Lexicon Acquisition
Work on automated lexicon and language acquisition dates back to Siklossy (1972), who
demonstrated a system that learned transformation patterns from logic back to natural

33

fiThompson & Mooney

language. As already noted, the most closely related work is that of Jeff Siskind, which we
described briefly in Section 2 and whose system we ran comparisons to in Section 5. Our
definition of the learning problem can be compared to his mapping problem (Siskind,
1993). That formulation differs from ours in several respects. First, his sentence representations are terms instead of trees. However, as shown in Figure 7, terms can also be
represented as trees that conform to our formalism with some minor additions. Next, his
notion of interpretation does involve a type of tree, but carries the entire representation of
a sentence up to the root. Also, it is not clear how he would handle quantified variables
in the representation of sentences. Skolemization is possible, but then generalization across
sentences would require special handling. We make the single-use assumption and he does
not. Another difference is our bias towards a minimal number of lexicon entries, while he
attempts to find a monosemous lexicon. His later work (Siskind, 2000) relaxes this to allow
ambiguity and noise, but still biases towards minimizing ambiguity. However, his formal
definition does not explicitly allow lexical ambiguity, but handles it in a heuristic manner.
This, though, may lead to more robustness than our method in the face of noise. Finally,
our definition allows phrasal lexicon entries.
Siskinds work on this topic has explored many different variations along a continuum of
using many constraints but requiring more time to incorporate each new example (Siskind,
1993), versus few constraints but requiring more training data (Siskind, 1996). Thus, perhaps his earlier systems would have been able to learn the lexicons of Section 5 more
quickly; but crucially those systems did not allow lexical ambiguity, and thus also may
not have learned as accurate a lexicon. More detailed comparisons to such versions of the
system are outside the scope of this paper. Our goal with Wolfie is to learn a possibly
ambiguous lexicon from as few examples as possible, and we thus made comparisons along
this dimension alone.
Siskinds approach, like ours, takes into account constraints between word meanings
that are justified by the exclusivity and compositionality assumptions. His approach is
somewhat more general in that it handles noise and referential uncertainty (uncertainty
about the meaning of a sentence and thus multiple possible candidates), while ours is
specialized for applications where the meaning (or meanings) is known. The experimental
results in Section 5 demonstrate the advantage of our method for such an application. He has
demonstrated his system to be capable of learning reasonably accurate lexicons from large,
ambiguous, and noisy artificial corpora, but this accuracy is only assured if the learning
algorithm converges, which did not occur for our smaller corpus in the experiments we ran.
Also, as already noted, his system operates in an incremental or on-line fashion, discarding
each sentence as it processes it, while ours is batch. In addition, his search for word meanings
proceeds in two stages, as discussed in Section 2.2. By using common substructures, we
combine these two stages in Wolfie. Both systems do have greedy aspects, ours in the
choice of the next best lexical entry, his in the choice to discard utterances as noise or create
a homonymous lexical entry. Finally, his system does not compute statistical correlations
between words and their possible meanings, while ours does.
Besides Siskinds work, there are others who approach the problem from a cognitive
perspective. For example, De Marcken (1994) also uses child language learning as a motivation, but approaches the segmentation problem instead of the learning of semantics.
For training input, he uses a flat list of tokens for semantic representations, but does not
34

fiAcquiring Word-Meaning Mappings

segment sentences into words. He uses a variant of expectation-maximization (Dempster,
Laird, & Rubin, 1977), together with a form of parsing and dictionary matching techniques,
to segment the sentences and associate the segments with their most likely meaning. On
the Childes corpus, the algorithm achieves very high precision, but recall is not provided.
Others taking the cognitive approach demonstrate language understanding by the ability
to carry out some task such as parsing. For example, Nenov and Dyer (1994) describe a
neural network model to map between visual and verbal-motor commands, and Colunga
and Gasser (1998) use neural network modeling techniques for learning spatial concepts.
Feldman and his colleagues at Berkeley (Feldman, Lakoff, & Shastri, 1995) are actively
pursuing cognitive models of the acquisition of semantic concepts. Another Berkeley effort,
the system by Regier (1996) is given examples of pictures paired with natural language
descriptions that apply to the picture, and learns to judge whether a new sentence is true
of a given picture.
Similar work by Suppes, Liang, and Bottner (1991) uses robots to demonstrate lexicon learning. A robot is trained on cognitive and perceptual concepts and their associated
actions, and learns to execute simple commands. Along similar lines, Tishby and Gorin
(1994) have a system that learns associations between words and actions, but they use a
statistical framework to learn these associations, and do not handle structured representations. Similarly, Oates, Eyler-Walker, and Cohen (1999) discuss the acquisition of lexical
hierarchies and their associated meaning as defined by the sensory environment of a robot.
The problem of automatic construction of translation lexicons (Smadja, McKeown, &
Hatzivassiloglou, 1996; Melamed, 1995; Wu & Xia, 1995; Kumano & Hirakawa, 1994; Catizone, Russell, & Warwick, 1993; Gale & Church, 1991; Brown & et al., 1990) has a definition
similar to our own. While most of these methods also compute association scores between
pairs (in their case, word-word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One
exception is Melamed (2000); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. Also, Yamazaki, Pazzani,
and Merz (1995) learn both translation rules and semantic hierarchies from parsed parallel
sentences in Japanese and English. Of course, the main difference between this body of
work and this paper is that we map words to semantic structures, not to other words.
As mentioned in the introduction, there is also a large body of work on learning lexical
semantics but using different problem formulations than our own. For example, Collins and
Singer (1999), Riloff and Jones (1999), Roark and Charniak (1998), and Schneider (1998)
define semantic lexicons as a grouping of words into semantic categories, and in the latter
case, add relational information. The result is typically applied as a semantic lexicon for
information extraction or entity tagging. Pedersen and Chen (1995) describe a method
for acquiring syntactic and semantic features of an unknown word, assuming access to an
initial concept hierarchy, but they give no experimental results. Many systems (Fukumoto &
Tsujii, 1995; Haruno, 1995; Johnston, Boguraev, & Pustejovsky, 1995; Webster & Marcus,
1995) focus only on acquisition of verbs or nouns, rather than all types of words. Also, the
authors just named either do not experimentally evaluate their systems, or do not show the
usefulness of the learned lexicons for a specific application.
Several authors (Rooth, Riezler, Prescher, Carroll, & Beil, 1999; Collins, 1997; Ribas,
1994; Manning, 1993; Resnik, 1993; Brent, 1991) discuss the acquisition of subcategoriza35

fiThompson & Mooney

tion information for verbs, and others describe work on learning selectional restrictions
(Manning, 1993; Brent, 1991). Both of these are different from the information required
for mapping to semantic representation, but could be useful as a source of information to
further constrain the search. Li (1998) further expands on the subcategorization work by
inducing clustering information. Finally, several systems (Knight, 1996; Hastings, 1996;
Russell, 1993) learn new words from context, assuming that a large initial lexicon and
parsing system are already available.
Another related body of work is grammar acquisition, especially those areas that tightly
integrate the grammar with a lexicon, such as with Categorial Grammars (Retore & Bonato,
2001; Dudau-Sofronie, Tellier, & Tommasi, 2001; Watkinson & Manandhar, 1999). The
theory of Categorial Grammar also has ties with lexical semantics, but these semantics
have not often been used for inference in support of high-level tasks such as database
retrieval. While learning syntax and semantics together is arguably a more difficult task,
the aforementioned work has not been evaluated on large corpora, presumably primarily
due to the difficulty of annotation.
7.2 Active Learning
With respect to additional active learning techniques, Cohn et al. (1994) were among the
first to discuss certainty-based active learning methods in detail. They focus on a neural
network approach to active learning in a version-space of concepts. Only a few of the
researchers applying machine learning to natural language processing have utilized active
learning (Hwa, 2001; Schohn & Cohn, 2000; Tong & Koller, 2000; Thompson et al., 1999;
Argamon-Engelson & Dagan, 1999; Liere & Tadepalli, 1997; Lewis & Catlett, 1994), and
the majority of these have addressed classification tasks such as part of speech tagging
and text categorization. For example, Liere and Tadepalli (1997) apply active learning
with committees to the problem of text categorization. They show improvements with
active learning similar to those that we obtain, but use a committee of Winnow-based
learners on a traditional classification task. Argamon-Engelson and Dagan (1999) also
apply committee-based learning to part-of-speech tagging. In their work, a committee of
hidden Markov models is used to select examples for annotation. Lewis and Catlett (1994)
use heterogeneous certainty-based methods, in which a simple classifier is used to select
examples that are then annotated and presented to a more powerful classifier.
However, many language learning tasks require annotating natural language text with
a complex output, such as a parse tree, semantic representation, or filled template. The
application of active learning to tasks requiring such complex outputs has not been well
studied, the exceptions being Hwa (2001), Soderland (1999), Thompson et al. (1999). The
latter two include work on active learning applied to information extraction, and Thompson
et al. (1999) includes work on active learning for semantic parsing. Hwa (2001) describes
an interesting method for evaluating a statistical parsers uncertainty, when applied for
syntactic parsing.

8. Future Work
Although Wolfies current greedy search method has performed quite well, a better search
heuristic or alternative search strategy could result in improvements. We should also more
36

fiAcquiring Word-Meaning Mappings

thoroughly evaluate Wolfies ability to learn long phrases, as we restricted this ability in
the evaluations here. Another issue is robustness in the face of noise. The current algorithm
is not guaranteed to learn a correct lexicon in even a noise-free corpus. The addition of noise
complicates an analysis of circumstances in which mistakes are likely to happen. Further
theoretical and empirical analysis of these issues is warranted.
Referential uncertainty could be handled, with an increase in complexity, by forming
LICS from more pairs of representations with which a phrase appears, but not between
alternative representations of the same sentence. Then, once a pair is added to the lexicon,
for each sentence containing that word, representations can be eliminated if they do not
contain the learned meaning, provided another representation does contain it (thus allowing
for lexical ambiguity). We plan to flesh this out and evaluate the results.
A different avenue of exploration is to apply Wolfie to a corpus of sentences paired
with the more common query language, SQL. Such corpora should be easily constructible
by recording queries submitted to existing SQL applications along with their English forms,
or translating existing lists of SQL queries into English (presumably an easier direction to
translate). The fact that the same training data can be used to learn both a semantic
lexicon and a parser also helps limit the overall burden of constructing a complete natural
language interface.
With respect to active learning, experiments on additional corpora are needed to test
the ability of our approach to reduce annotation costs in a variety of domains. It would
also be interesting to explore active learning for other natural language processing problems
such as syntactic parsing, word-sense disambiguation, and machine translation.
Our current results have involved a certainty-based approach; however, proponents of
committee-based approaches have convincing arguments for their theoretical advantages.
Our initial attempts at adapting committee-based approaches to our systems were not very
successful; however, additional research on this topic is indicated. One critical problem is
obtaining diverse committees that properly sample the version space (Cohn et al., 1994).

9. Conclusions
Acquiring a semantic lexicon from a corpus of sentences labeled with representations of
their meaning is an important problem that has not been widely studied. We present both a
formalism of the learning problem and a greedy algorithm to find an approximate solution to
it. Wolfie demonstrates that a fairly simple, greedy, symbolic learning algorithm performs
well on this task and obtains performance superior to a previous lexicon acquisition system
on a corpus of geography queries. Our results also demonstrate that our methods extend
to a variety of natural languages besides English, and that they scale fairly well to larger,
more difficult corpora.
Active learning is a new area of machine learning that has been almost exclusively
applied to classification tasks. We have demonstrated its successful application to more
complex natural language mappings from phrases to semantic meanings, supporting the
acquisition of lexicons and parsers. The wealth of unannotated natural language data,
along with the difficulty of annotating such data, make selective sampling a potentially
invaluable technique for natural language learning. Our results on realistic corpora indicate
that example annotations savings as high as 22% can be achieved by employing active

37

fiThompson & Mooney

sample selection using only simple certainty measures for predictions on unannotated data.
Improved sample selection methods and applications to other important language problems
hold the promise of continued progress in using machine learning to construct effective
natural language processing systems.
Most experiments in corpus-based natural language have presented results on some
subtask of natural language, and there are few results on whether the learned subsystems
can be successfully integrated to build a complete NLP system. The experiments presented
in this paper demonstrated how two learning systems, Wolfie and Chill, were successfully
integrated to learn a complete NLP system for parsing database queries into executable
logical form given only a single corpus of annotated queries, and further demonstrated the
potential of active learning to reduce the annotation effort for learning for NLP.

Acknowledgments
We would like to thank Jeff Siskind for providing us with his software, and for all his help
in adapting it for use with our corpus. Thanks also to Agapito Sustaita, Esra Erdem,
and Marshall Mayberry for their translation efforts, and to the three anonymous reviewers
for their comments which helped improve the paper. This research was supported by the
National Science Foundation under grants IRI-9310819 and IRI-9704943.

References
Anderson, J. R. (1977). Induction of augmented transition networks. Cognitive Science, 1,
125157.
Angluin, D. (1988). Queries and concept learning. Machine Learning, 2, 319342.
Argamon-Engelson, S., & Dagan, I. (1999). Committee-based sample selection for probabilistic classifiers. Journal of Artificial Intelligence Research, 11, 335360.
Beckwith, R., Fellbaum, C., Gross, D., & Miller, G. (1991). WordNet: A lexical database
organized on psycholinguistic principles. In Zernik, U. (Ed.), Lexical Acquisition:
Exploiting On-Line Resources to Build a Lexicon, pp. 211232. Lawrence Erlbaum,
Hillsdale, NJ.
Borland International (1988). Turbo Prolog 2.0 Reference Guide. Borland International,
Scotts Valley, CA.
Brent, M. (1991). Automatic acquisition of subcategorization frames from untagged text.
In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL-91), pp. 209214.
Brown, P., & et al. (1990). A statistical approach to machine translation. Computational
Linguistics, 16 (2), 7985.
Catizone, R., Russell, G., & Warwick, S. (1993). Deriving translation data from bilingual
texts. In Proceedings of the First International Lexical Acquisition Workshop.
38

fiAcquiring Word-Meaning Mappings

Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization with active learning.
Machine Learning, 15 (2), 201221.
Collins, M., & Singer, Y. (1999). Unsupervised models for named entity classification. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing
and Very Large Corpora (EMNLP/VLC-99) University of Maryland.
Collins, M. J. (1997). Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics
(ACL-97), pp. 1623.
Colunga, E., & Gasser, M. (1998). Linguistic relativity and word acquisition: a computational approach. In Proceedings of the Twenty First Annual Conference of the
Cognitive Science Society, pp. 244249.
Dagan, I., & Engelson, S. P. (1995). Committee-based sampling for training probabilistic classifiers. In Proceedings of the Twelfth International Conference on Machine
Learning (ICML-95), pp. 150157 San Francisco, CA. Morgan Kaufman.
De Marcken, C. (1994). The acquisition of a lexicon from paired phoneme sequences and
semantic representations. In Lecture Notes in Computer Science, Vol. 862, pp. 6677.
Springer-Verlag.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical Society B, 39, 138.
Dudau-Sofronie, Tellier, & Tommasi (2001). Learning categorial grammars from semantic
types. In Proceedings of the 13th Amsterdam Colloquium, pp. 7984.
Feldman, J., Lakoff, G., & Shastri, L. (1995). The neural theory of language project
http://www.icsi.berkeley.edu/ntl. International Computer Science Institute, University
of California, Berkeley, CA.
Fillmore, C. (1968). The case for case. In Bach, E., & Harms, R. T. (Eds.), Universals in
Linguistic Theory. Holt, Reinhart and Winston, New York.
Fillmore, C. (1988). The mechanisms of Construction Grammar. In Axmaker, S., Jaisser,
A., & Singmeister, H. (Eds.), Proceedings of the Fourteenth Annual Meeting of the
Berkeley Linguistics Society, pp. 3555 Berkeley, CA.
Fisher, D. H. (1987). Knowledge acquisition via incremental conceptual clustering. Machine
Learning, 2, 139172.
Freund, Y., Seung, H. S., Shamir, E., & Tishby, N. (1997). Selective sampling using the
query by committee algorithm. Machine Learning, 28, 133168.
Fukumoto, F., & Tsujii, J. (1995). Representation and acquisition of verbal polysemy. In
Papers from the 1995 AAAI Symposium on the Representation and Acquisition of
Lexical Knowledge: Polysemy, Ambiguity, and Generativity, pp. 3944 Stanford, CA.

39

fiThompson & Mooney

Gale, W., & Church, K. (1991). Identifying word correspondences in parallel texts. In
Proceedings of the Fourth DARPA Speech and Natural Language Workshop.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the Theory of
NP-Completeness. Freeman, New York, NY.
Goldberg, A. (1995). Constructions: A Construction Grammar Approach to Argument
Structure. The University of Chicago Press.
Grefenstette, G. (1994). Sextant: Extracting semantics from raw text, implementation
details. Integrated Computer-Aided Engineering, 6 (4).
Haas, J., & Jayaraman, B. (1997). From context-free to definite-clause grammars: a typetheoretic approach. Journal of Logic Programming, 30 (1), 123.
Haruno, M. (1995). A case frame learning method for Japanese polysemous verbs. In
Papers from the 1995 AAAI Symposium on the Representation and Acquisition of
Lexical Knowledge: Polysemy, Ambiguity, and Generativity, pp. 4550 Stanford, CA.
Hastings, P. (1996). Implications of an automatic lexical acquisition mechanism. In
Wermter, S., Riloff, E., & Scheler, C. (Eds.), Connectionist, Statistical, and Symbolic Approaches to Learning for natural language processing. Springer-Verlag, Berlin.
Hwa, R. (2001). On minimizing training corpus for parser acquisition. In Proceedings of
the Fifth Computational Natural Language Learning Workshop.
Jackendoff, R. (1990). Semantic Structures. The MIT Press, Cambridge, MA.
Johnston, M., Boguraev, B., & Pustejovsky, J. (1995). The acquisition and interpretation of
complex nominals. In Papers from the 1995 AAAI Symposium on the Representation
and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, pp.
6974 Stanford, CA.
Knight, K. (1996). Learning word meanings by instruction. In Proceedings of the Thirteenth
National Conference on Artificial Intelligence (AAAI-96), pp. 447454 Portland, Or.
Kohavi, R., & John, G. (1995). Automatic parameter selection by minimizing estimated
error. In Proceedings of the Twelfth International Conference on Machine Learning
(ICML-95), pp. 304312 Tahoe City, CA.
Kumano, A., & Hirakawa, H. (1994). Building an MT dictionary from parallel texts based
on linguistic and statistical information. In Proceedings of the Fifteenth International
Conference on Computational Linguistics, pp. 7681.
Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming: Techniques and Applications. Ellis Horwood.
Lewis, D. D., & Catlett, J. (1994). Heterogeneous uncertainty sampling for supervised
learning. In Proceedings of the Eleventh International Conference on Machine Learning (ICML-94), pp. 148156 San Francisco, CA. Morgan Kaufman.
40

fiAcquiring Word-Meaning Mappings

Li, H. (1998). A probabilistic approach to lexical semantic knowledge acquisition and structural disambiguation. Ph.D. thesis, University of Tokyo.
Liere, R., & Tadepalli, P. (1997). Active learning with committees for text categorization. In
Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI97), pp. 591596 Providence, RI.
Manning, C. D. (1993). Automatic acquisition of a large subcategorization dictionary from
corpora. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics (ACL-93), pp. 235242 Columbus, OH.
McCallum, A. K., & Nigam, K. (1998). Employing EM and pool-based active learning
for text classification. In Proceedings of the Fifteenth International Conference on
Machine Learning (ICML-98), pp. 350358 Madison, WI. Morgan Kaufman.
Melamed, I. D. (1995). Automatic evaluation and uniform filter cascades for inducing n-best
translation lexicons. In Proceedings of the Third Workshop on Very Large Corpora.
Melamed, I. D. (2000). Models of translational equivalence among words. Computational
Linguistics, 26 (2), 221249.
Muggleton, S. (Ed.). (1992). Inductive Logic Programming. Academic Press, New York,
NY.
Muggleton, S. (1995). Inverse entailment and Progol. New Generation Computing Journal,
13, 245286.
Muggleton, S., & Feng, C. (1990). Efficient induction of logic programs. In Proceedings of
the First Conference on Algorithmic Learning Theory Tokyo, Japan. Ohmsha.
Nenov, V. I., & Dyer, M. G. (1994). Perceptually grounded language learning: Part 2
DETE: A neural/procedural model. Connection Science, 6 (1), 341.
Oates, T., Eyler-Walker, Z., & Cohen, P. (1999). Using syntax to learn semantics: an
experiment in language acquisition with a mobile robot. Tech. rep. 99-35, University
of Massachusetts, Computer Science Department.
Partee, B., Meulen, A., & Wall, R. (1990). Mathematical Methods in Linguistics. Kluwer
Academic Publishers, Dordrecht, The Netherlands.
Pedersen, T., & Chen, W. (1995). Lexical acquisition via constraint solving. In Papers
from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical
Knowledge: Polysemy, Ambiguity, and Generativity, pp. 118122 Stanford, CA.
Plotkin, G. D. (1970). A note on inductive generalization. In Meltzer, B., & Michie, D.
(Eds.), Machine Intelligence (Vol. 5). Elsevier North-Holland, New York.
Rayner, M., Hugosson, A., & Hagert, G. (1988). Using a logic grammar to learn a lexicon.
Tech. rep. R88001, Swedish Institute of Computer Science.

41

fiThompson & Mooney

Regier, T. (1996). The human semantic potential: spatial language and constrained connectionism. MIT Press.
Resnik, P. (1993). Selection and information: a class-based approach to lexical relationships.
Ph.D. thesis, University of Pennsylvania, CIS Department.
Retore, C., & Bonato, R. (2001). Learning rigid lambek grammars and minimalist grammars
from structured sentences. In Proceedings of the Third Learning Language in Logic
Workshop Strasbourg, France.
Ribas, F. (1994). An experiment on learning appropriate selectional restrictions from a
parsed corpus. In Proceedings of the Fifteenth International Conference on Computational Linguistics, pp. 769774.
Riloff, E., & Jones, R. (1999). Learning dictionaries for information extraction by multilevel bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial
Intelligence (AAAI-99), pp. 10441049 Orlando, FL.
Roark, B., & Charniak, E. (1998). Noun-phrase co-occurrence statistics for semi-automatic
semantic lexicon construction. In Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and COLING-98 (ACL/COLING-98), pp.
11101116.
Rooth, M., Riezler, S., Prescher, D., Carroll, G., & Beil, F. (1999). Inducing a semantically
annotated lexicon via EM-based clustering. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics, pp. 104111.
Russell, D. (1993). Language Acquisition in a Unification-Based Grammar Processing System Using a Real World Knowledge Base. Ph.D. thesis, University of Illinois, Urbana,
IL.
Schank, R. C. (1975). Conceptual Information Processing. North-Holland, Oxford.
Schneider, R. (1998). A lexically-intensive algorithm for domain-specific knowledge acquisition. In Proceedings of the Joint Conference on New Methods in Language Processing
and Computational Natural Language Learning, pp. 1928.
Schohn, G., & Cohn, D. (2000). Less is more: Active learning with support vector machines. In Proceedings of the Seventeenth International Conference on Machine Learning (ICML-2000), pp. 839846 Stanford, CA.
Sebillot, P., Bouillon, P., & Fabre, C. (2000). Inductive logic programming for corpus-based
acquisition of semantic lexicons. In Proceedings of 2nd Learning Language in Logic
(LLL) Workshop Lisbon, Portugal.
Seung, H. S., Opper, M., & Sompolinsky, H. (1992). Query by committee. In Proceedings
of the ACM Workshop on Computational Learning Theory Pittsburgh, PA.
Siklossy, L. (1972). Natural language learning by computer. In Simon, H. A., & Siklossy,
L. (Eds.), Representation and meaning: Experiments with Information Processsing
Systems. Prentice Hall, Englewood Cliffs, NJ.
42

fiAcquiring Word-Meaning Mappings

Siskind, J. M. (2000). Learning word-to-meaning mappings. In Broeder, P., & Murre, J.
(Eds.), Models of Language Acquisition: Inductive and Deductive Approaches. Oxford
University Press.
Siskind, J. M. (1992). Naive Physics, Event Perception, Lexical Semantics and Language
Acquisition. Ph.D. thesis, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA.
Siskind, J. M. (1996). A computational study of cross-situational techniques for learning
word-to-meaning mappings. Cognition, 61 (1), 3991.
Siskind, J. M. (1993). Lexical acquisition as constraint satisfaction. Tech. rep. IRCS-93-41,
University of Pennsylvania.
Smadja, F., McKeown, K. R., & Hatzivassiloglou, V. (1996). Translating collocations for
bilingual lexicons: A statistical approach. Computational Linguistics, 22 (1), 138.
Soderland, S. (1999). Learning information extraction rules for semi-structured and free
text. Machine Learning, 34, 233272.
Suppes, P., Liang, L., & Bottner, M. (1991). Complexity issues in robotic machine learning
of natural language. In Lam, L., & Naroditsky, V. (Eds.), Modeling Complex Phenomena, Proceedings of the 3rd Woodward Conference, pp. 102127. Springer-Verlag.
Thompson, C. A., Califf, M. E., & Mooney, R. J. (1999). Active learning for natural language
parsing and information extraction. In Proceedings of the Sixteenth International
Conference on Machine Learning (ICML-99), pp. 406414 Bled, Slovenia.
Thompson, C. A. (1995). Acquisition of a lexicon from semantic representations of sentences.
In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL-95), pp. 335337 Cambridge, MA.
Tishby, N., & Gorin, A. (1994). Algebraic learning of statistical associations for language
acquisition. Computer Speech and Language, 8, 5178.
Tomita, M. (1986). Efficient Parsing for Natural Language. Kluwer Academic Publishers,
Boston.
Tong, S., & Koller, D. (2000). Support vector machine active learning with applications
to text classification. In Proceedings of the Seventeenth International Conference on
Machine Learning (ICML-2000), pp. 9991006 Stanford, CA.
Watkinson, S., & Manandhar, S. (1999). Unsupervised lexical learning with categorial
grammars using the lll corpus. In Learning Language In Logic (LLL) Workshop Bled,
Slovenia.
Webster, M., & Marcus, M. (1995). Automatic acquisition of the lexical semantics of verbs
from sentence frames. In Proceedings of the 27th Annual Meeting of the Association
for Computational Linguistics (ACL-89), pp. 177184.

43

fiThompson & Mooney

Wu, D., & Xia, X. (1995). Large-scale automatic extraction of an English-Chinese translation lexicon. Machine Translation, 9 (3-4), 285313.
Yamazaki, T., Pazzani, M., & Merz, C. (1995). Learning hierarchies from ambiguous natural
language data. In Proceedings of the Twelfth International Conference on Machine
Learning (ICML-95), pp. 575583 San Francisco, CA. Morgan Kaufmann.
Zelle, J. M. (1995). Using Inductive Logic Programming to Automate the Construction of
Natural Language Parsers. Ph.D. thesis, Department of Computer Sciences, University of Texas, Austin, TX. Also appears as Artificial Intelligence Laboratory Technical
Report AI 96-249.
Zelle, J. M., & Mooney, R. J. (1996). Learning to parse database queries using inductive
logic programming. In Proceedings of the Thirteenth National Conference on Artificial
Intelligence (AAAI-96), pp. 10501055 Portland, OR.
Zipf, G. (1949). Human behavior and the principle of least effort. Addison-Wesley, New
York, NY.

44

fiJournal of Artificial Intelligence Research 18 (2003) 315-349

Submitted 10/02; published 4/03

Structure and Complexity in Planning with Unary Operators
Ronen I. Brafman
Carmel Domshlak

brafman@cs.bgu.ac.il
dcarmel@cs.bgu.ac.il

Department of Computer Science
Ben-Gurion University
P.O. Box 653, 84105 Beer-Sheva, Israel

Abstract
Unary operator domains  i.e., domains in which operators have a single effect  arise
naturally in many control problems. In its most general form, the problem of strips planning in unary operator domains is known to be as hard as the general strips planning
problem  both are pspace-complete. However, unary operator domains induce a natural
structure, called the domains causal graph. This graph relates between the preconditions
and effect of each domain operator. Causal graphs were exploited by Williams and Nayak
in order to analyze plan generation for one of the controllers in NASAs Deep-Space One
spacecraft. There, they utilized the fact that when this graph is acyclic, a serialization
ordering over any subgoal can be obtained quickly. In this paper we conduct a comprehensive study of the relationship between the structure of a domains causal graph and the
complexity of planning in this domain. On the positive side, we show that a non-trivial
polynomial time plan generation algorithm exists for domains whose causal graph induces
a polytree with a constant bound on its node indegree. On the negative side, we show
that even plan existence is hard when the graph is a directed-path singly connected DAG.
More generally, we show that the number of paths in the causal graph is closely related to
the complexity of planning in the associated domain. Finally we relate our results to the
question of complexity of planning with serializable subgoals.

1. Introduction
One of the first well formulated problems addressed by AI researchers was the planning
problem. Simply stated, it involves the generation of a sequence of system transformations,
taken out of a given set of system transformations (called actions or plan operators), whose
combined effect is to move the system from some given initial state into one of a set of
desired goal states. The planning problem is known to be intractable in general (Chapman,
1987), and tractable algorithms exist for very restrictive classes of problems only. This
discouraging fact has not deterred planning researchers. Indeed, many researchers believe
that real-world problems have some properties, or structure, that could be exploited, either
implicitly or explicitly. In this paper we attempt to understand the relationship between
structure and complexity in planning problems in which each action changes the value of a
single variable.
To study the relation between the structure and the complexity in a class of problems
we must identify a set of parameters that characterize it. In the case of planning, a number
of such problem properties have been studied in the past (which we review in more detail
in Section 6). These properties have been mostly syntactical, i.e., they involve restriction
on operators, e.g., the type and number of preconditions or effects that operators have. For
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBrafman & Domshlak

example, Bylander (1994) showed that strips planning in domains where each operator is
restricted to have positive preconditions and one postcondition only is tractable. Backstrom
and Klein (1991b) considered other, more global types of syntactical restrictions, but using
a more refined model in which two types of preconditions are considered: prevail conditions,
which are variable values that are required prior to the execution of the operator and are
not affected by the operator, and preconditions, which are affected by the operator. For
example, they have shown that when operators have a single effect, no two operators have
the same effect, and each variable can be affected only in one context (of prevail conditions)
then the planning problem can be solved in polynomial time. However, these restrictions
are very strict, and it is difficult to find reasonable domains satisfying them.
In this paper we concentrate on more global properties of unary operator domains;
properties that capture some of the interactions between different planning operators. The
tool we use to study these properties is the domains causal graph. A causal graph is a
directed graph whose nodes stand for the domain propositions. An edge (p, q) appears in
the causal graph if and only if some operator that changes the value of q has a prevail
condition involving p. Such a problem structure was introduced by Knoblock (1994) in
the context of automatically generating abstractions for planning. Subsequently, Jonsson
and Backstrom (1998b) introduced the 3S class of planning problems with unary operators,
which was characterized by the acyclicity of the causal graph, and some restrictions on the
operator set. It was shown that determining plan existence for this class of problems is
polynomial, while plan generation is provably intractable.
Complexity results for unary operators would be of theoretical interest alone if one
could not supply interesting problems in which unary operators are used. One interesting
application in which this problem arises is the determination of dominance relationship
between different outcomes in a CP-net (Boutilier, Brafman, Hoos, & Poole, 1999). This
problem is reducible to strips planning with unary operators.
Another example, of greater interest to the planning community, is a planning-based
reactive control system that commands the NASA Deep Space One autonomous spacecraft (Pell, Bernard, Chien, Gat, Muscettola, Nayak, Wagner, & Williams, 1997; Williams
& Nayak, 1996, 1997). This system was hailed by Weld (1999) in his recent survey of AI
planning as one of the most exciting recent developments in the area of planning. Naturally, the complete system (Pell et al., 1997) is very complex, however, its configuration
planning and execution subsystem are of particular interest to us. In the context of controlling Deep-Space One, Williams and Nayak (1996, 1997) present a reactive planner, Burton,
that generates a single control action for the main engine subsystem of the spacecraft, and
compensates for anomalies at every step. Given a high-level goal (for example, thrust in
one of the engines), Burton continually tries to transition the system toward a state that
satisfies the desired goal. What is particularly relevant for us is that Burtons task can be
described as a strips planning problem in which each operator affects only a single variable (hardware component)  Williams and Nayak (1997) argue that in physical hardware
it is usually the case that each state variable is commanded separately. However, Burton is
based on two additional important restrictions: First, the planner is explicitly supplied with
a serialization order for any satisfiable set of goal. Second, all operators must be reversible.
One of the reasons cited for designing Burton as a reactive planner that generates a
single action at a time was the potential intractability of generating whole plans. Indeed,
316

fiStructure and Complexity in Planning with Unary Operators

Williams and Nayak were pessimistic about the prospects of generating whole plans quickly
even for Burton, i.e., for problem instances with serializable sub-goals and single-effect
operators. As our results show, this pessimism was not fully justified.
Our work continues the study of planning with unary operators. This apparently easier
problem is in fact as hard as the general strips planning problem (Bylander, 1994). However, we can obtain finer distinctions and some positive results if we pay closer attention to
the causal structure of the domain. For example, it is easy to show that when the causal
graph is a tree, it is easy to determine a serializability ordering over any set of sub-goals,
and consequently, obtain a plan in polynomial time. In this paper we analyze the relationship between the domains causal graph and the complexity of plan generation and plan
existence. In particular we prove the following results:
 When the causal graph forms a polytree (the induced undirected graph is acyclic), and
its node indegree is bounded by a constant, then plan existence and plan generation
are polynomial.
 When the causal graph is directed-path singly connected (there is at most one directed
path between any pair of nodes), then plan existence is np-complete.
 In general, plan generation for the problems with acyclic causal graphs is provably
intractable, i.e., the problem requires exponential time. The corresponding claim
is derived from a previous result by Jonsson and Backstrom (1998b). However, we
show that the complexity of plan generation for these problems can be bounded by a
function of the number of paths within the causal graph.
Note that the complexity of the problems with polytree causal graphs but with unbounded
node indegree remains an open problem  it is still to be shown whether they can be solved
in polynomial time, or they are np-complete.
Finally, we relate our results to an old open question: how difficult is it to generate
plans for problems with serializable subgoals (Korf, 1987)? This question was stated by
Bylander (1992), and different hypotheses were raised by different researchers. Here, we
present a clear, though somewhat disappointing answer: First, our results suggest that even
when the underlying causal graph of the problem is acyclic (and thus the problem is known
to be serializable), finding a serialization ordering on the problem subgoals may be hard.
Second, we show that even if the actual serialization ordering on the subgoals is known,
solving the problem is not necessarily easy.
The rest of this paper is organized as follows: In Section 2 we first introduce some
basic formalism used in the paper, then discuss, motivate and illustrate the notion of causal
graph. In Sections 3 and 4 we present our results on the relation between the form of
the causal graph and the complexity of the planning problem. In Section 5 we discuss the
sub-goal serializability issue and the impact of our results on it. In Section 6 we describe
some related work on complexity of planning, and connect our work with the previous
results. We summarize in Section 7. Finally, Appendix A provides a short review of the
POP algorithm (Penberthy & Weld, 1992), and Appendix B provides some of the proofs.
317

fiBrafman & Domshlak

2. Basic Formalism and Causal Graphs
In this paper we consider only propositional planning problems, using the propositional
strips with negative goals formalism (Bylander, 1994), in which both positive and negative
preconditions are allowed. Following Backstrom and Klein (1991b), we distinguish between
preconditions and prevail conditions. In the former case the variable involved changes its
value after the operator is executed, while in the latter case the value does not change. The
post-condition of an operator expresses which state variables it changes and what values
these variables will have after executing the operator. The pre-condition specifies which
values these changed variables must have before the operator is executed. The prevail
condition specifies which of the unchanged variables must have some specific value before
execution of the operator and what these values are. Hence, prevail conditions, such as
having a visa, are needed in order to apply an operator, such as Enter-USA, but their
values do not change after the operator is applied. Finally, we assume that an operator is
applicable if and only if both its pre- and prevail conditions are satisfied.
Formally, we assume that a problem instance is given by a quadruple  = hV, , Init, Goali,
where:
 V = {v1 , . . . , vn } is a set of propositional state variables, each one with an associated
binary domain D(vi ). The domain D(vi ) of the variable vi induces an extended domain
D+ (vi ) = D(vi )  {u}, where u denotes the unspecified value.
 Init is an initial, fully specified state, i.e. Init  D(v1 )  . . .  D(vn ).
 Goal is a set of possible goal states. We assume that such a set is specified by a partial
assignment on V, thus Goal  D+ (v1 )  . . .  D+ (vn ).
  = {A1 , . . . , AN } is a finite set of operators of the form hpre, post, prvi, where
pre, post, prv  D+ (v1 )  . . .  D+ (vn ) denote the pre-, post-, and prevail condition,
respectively. In what follows, by pre(A), post(A), and prv(A) we denote the corresponding conditions of an operator A, and by pre(A)[i], post(A)[i], and prv(A)[i] the
corresponding values of the variable vi .
For every vi  V, we must have either pre(A)[i] = u or prv(A)[i] = u. Further,
post(A)[i] 6= u if and only if pre(A)[i] 6= u, in which case post(A)[i] 6= pre(A)[i].
In this paper we analyse only planning problems with unary operators. Therefore, in
what follows, we assume that, for each operator A  , we have that:
1. there exists a variable vi  V, such that pre(A)[i] 6= u, and
2. for each other variable vj  V  {vi }, pre(A)[j] = u.
Note that specifying both pre- and postconditions in case of only propositional variables
is redundant, and we use it only to simplify the presentation. Likewise, our assumption that
post(A) 6= u implies pre(A) 6= u is different from the usual strips formalism, and requires
an exponential time translation in general. However, in our case of only unary operators,
this translation takes only linear time.
318

fiStructure and Complexity in Planning with Unary Operators

2.1 Causal Graphs
Causal graphs were used by Williams and Nayak (1997) as a tool for describing the structure
of planning domains with unary operators. They represent a dependence relation between
the state variables in the domain. A causal graph G is a directed graph whose nodes
correspond to the state variables. An edge from p to q appears in the causal graph if
and only if some operator that changes the value of q has a prevail condition involving
some value of p. Hence the immediate predecessors of q in G are all those variables that
affect our ability to change the value of q. Such a problem structure was introduced by
Knoblock (1994) in the context of automatic generation of abstractions for planning. The
causal graph is an intuitive model which is easily constructed given any planning problem.
Causal graphs are not the only graphical structure that can be derived from a given
planning problem, and effectively exploited in solving it. For instance, graphs in which
operators and literals (and not variables/propositions) are represented by the nodes, and the
edges represent both prevail and preconditions were introduced by Etzioni (1993) and Smith
and Peot (1993). In particular, problem space graphs of Etzioni (1993) and operator graphs
of Smith and Peot (1993) were proposed as mechanisms to reduce the number of threats
that arise during the total-order and partial-order planning, respectively. However, in this
paper we focus on the causal graphs, since they were shown to be especially informative
when all operators are unary (Jonsson & Backstrom, 1998b; Williams & Nayak, 1997).
Causal graphs have an important potential role in the design of autonomous industrial
systems, as argued and demonstrated by Williams and Nayak (1997): Unary operators are
natural when the manipulated objects are hardware components, since the basic control
actions in such systems change the state of a single hardware component. The applicability
of these control actions in any state depends on the state of the affected component as
well as on the state of the related hardware components. This naturally gives rise to a
planning domain with unary operators. Moreover, since the state variables correspond
to hardware components, in the induced causal graph we typically see that the prevail
dependencies between variables are usually implicitly entailed by the inter-composition of
the hardware components. Thus, the causal graph of such domains resembles the structure
of and the relationships between the systems hardware components. This resemblance has
important practical ramifications for system design given the relationship between causal
graph structure and the complexity of plan generation: It enables the system designer to
consider the effect of his hardware design on the systems ability to autonomously generate
control sequences.
A case in point is the planning problem studied by Williams and Nayak (1997), which
had a number of important features: all operators were unary and reversible, and the causal
graph was acyclic. Williams and Nayak argued that acyclic connectivity frequently occurs
in designed systems. However, the requirement that all operators should be reversible seems
to us restrictive, and it has important impact on the complexity of the problem. In the
case of the Burton planner (Williams & Nayak, 1997), there were good reasons to make this
assumption. Burtons reactive nature precludes extensive deliberation on the consequences
of its operators. Thus it leaves open the possibility that operators may degrade the systems
capabilities, leading it to dead-ends. In that case, the restriction to reversible operators was
319

fiBrafman & Domshlak

required in order to achieve a more reliable system. As we show later, in certain cases,
complete plans can be generated efficiently even when the operators are not reversible.
Williams and Nayaks work has another interesting aspect, as noted by Weld (1999).
For a long time, researchers have known that planning problems with serializable subgoals
are likely to be easier to solve. Williams and Nayak recognized that their spacecraft configuration task was serializable (many real-world problems are not), and, more importantly,
they developed a fast algorithm for computing the correct order based on the fact that
the underlying causal graph is acyclic. However, their algorithm makes heavy use of the
fact that all operators are reversible. Informally, reversibility implies that we can solve our
subgoals one by one as long as they are consistent with some topological order of the causal
graph without taking into account any global considerations: any side-effect can always
be undone. Without the assumption of operator reversibility, it is relatively easy to show
that Williams and Nayaks algorithm works only if the causal graph forms a directed chain.
Even when the causal graph is a tree, although the problem is easy, one must take care in
the choice of which subgoal to achieve next when operators are not reversible. As we show
later, when the structure of the causal graph is more complicated than a directed tree then
either the problem is hard or, if not, a more sophisticated algorithm is required.
Finally, we note that the existence of reversible operators might make the problem seem
easier than it actually is. In this paper we present an example of a propositional planning
problem with unary operators, acyclic causal graph, and totally reversible operators, the
minimal solution of which is exponentially long in the size of the problems description.
2.2 Example
In order to illustrate the notion of a causal graph, consider the following example, inspired
by the work of Williams and Nayak (1997) on controlling the main engine subsystem of the
Cassini spacecraft, in general, and its valve driver circuitry, in particular.
Each valve V L (on/off) is controlled by a valve driver V LD (open/close), and a safety
control unit SCU (safe/unsafe). Each driver controls exactly one valve, while a safety
control unit can control several valves. Commands to the driver are sent via a driver
control unit, that consist of two switches, S l and S r , which can be either on or off. The
activating states of S l and S r are described below. A valve reacts (by a state change)
to a command from its driver only if (i) the instruction actually involve a state change
(i.e., an open valve should not be reopened), and (ii) the safety control unit indicates that
manipulating the valve is safe. In addition, the valve can be closed if the safety control unit
indicates an unsafe situation. For simplicity of presentation, Table 1 presents the operator
set for controlling the valves and valve drivers only. The dashed boxes stand for driver
control units, two switches in each.
Now suppose that the valves V L1 and V L2 , with the drivers V LD1 and V LD2 , respectively, are controlled by a shared safety control unit SCU . Given the operator set in
Table 1, the causal graph for controlling this subsystem is presented in Figure 1.

3. Polytree Causal Graphs
Starting at this section, we show how, by bounding the structural complexity of the causal
graph, we can bound the complexity of plan generation. Recall that we use a propositional
320

fiStructure and Complexity in Planning with Unary Operators

Affected component
V LD
VL

pre
close
open
on
of f
on

post
open
close
of f
on
of f

prv
Sl = 1  Sr = 0
Sl = 0  Sr = 1
V LD = close  SCU = saf e
V LD = open  SCU = saf e
SCU = unsaf e

Table 1: A subset of the operator set for the valve circuitry controller example.
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 




S1r

S1l




EE
ww
E

_ _ _ _ _EEE_ _ _ _ _ _www_w _ _ _ _
EE
w
w
E"
{ww

V LD1

GG
G#

SCU

HH
HH
HH
HH
H#



S2r 
S2l G
G
y
G
y


_ _ _ _ _GGG_G _ _ _ _ _ yy_y _ _ _ _



w
ww
ww
w
w
w{ w

V LD2

GG
GG
GG
GG
G#

V L1

yy
|yy

vv
vv
v
vv
v{ v

V L2

Figure 1: Causal graph for the example.
language (binary variables) to describe the state of the world, and each operator is described
by its prevail conditions, single precondition, and single effect (or post-condition). The
precondition and the effect are two literals, one the negation of the other.
A causal graph forms a polytree if there is a single path between every pair of nodes in
the induced undirected graph1 , i.e., the induced undirected graph is a tree. For example, the
causal graph presented in Figure 1 forms a polytree. For this class of problems we present
a planning algorithm which is polynomial if the indegree of all nodes in the causal graph is
bounded by a constant. We argue that this assumption is reasonable if the prevail dependencies reflect the inter-composition of some controlled hardware components (Williams &
Nayak, 1997).
Given a propositional planning instance with a polytree causal graph, we:
1. Provide a general upper bound for the number of times that a variable may be required
to change its value on a valid, irreducible plan.
2. Using this general upper bound, provide a polynomial time procedure, called determinemax-sequence, that, given a variable v, determines the actual maximal number of
times that v can change its value on a valid, irreducible plan.
3. Provide a preprocessing algorithm that: (a) determines whether or not a plan for a
given problem instance of our class exists, and (b) performs a substantial amount of
1. These graphs are also known as singly connected DAGs.

321

fiBrafman & Domshlak

preprocessing for the subsequent step of plan generating. This algorithm is based on
a top-down execution of determine-max-sequence on the variables of the given
problem instance.
4. If the answer of the plan existence check is positive we run a particular deterministic instance of the POP algorithm2 (Penberthy & Weld, 1992), called pop-pcg,
that generates the required plan using the information provided by the preprocessing
algorithm, without backtracking, in linear time.
Informally, this process is based on the following properties of the planning problems
with polytree causal graph. First, the bound achieved in step 1 is necessary for the steps 2-3,
which are the main steps of our technique. By itself, this bound will be valid not only for
a polytree, but for a wider class of directed-path singly connected causal graphs. However,
steps 2-3 will be valid for polytree causal graphs only, because of the following properties
of this form of dependence relation between the variables:
(i) Given a variable v  V, changing the value of a parent (immediate predecessor)
w  pred(v) does not require any changes of neither other parents of v, nor their
predecessors in the causal graph.
(ii) The number of times that a variable v will be able to change its value along a valid
plan for a given problem instance depends directly both on these numbers for pred(v),
and on the actual ordering of the value changes of pred(v).
(iii) From (i) it follows that all the possible orderings of the value changes of pred(v) are
legal. In addition, it will be shown that chosing an ordering for the value changes of
pred(v) will not affect our ability to change the value of any variable except of v.
(iv) The crucial part of the process (steps 2-3) is basically about finding the right ordering of
the right number of value changes of pred(v) for each variable v  V. By synchronizing
these changes to vs parents appropriately, we can increase the number of possible
changes to v.
We start with some notation. First, a valid plan P for a given planning instance 
will be called irreducible if any subplan P 0 of P is not a plan for , in the following sense:
Removal of any subset of (not necessarily subsequent) actions from P makes the resulting
plan either illegal, or its initial state is not Init, or its goal state is not one of the states
specified by Goal. The notion of irreducible plans was introduced by Kambhampati (1995),
where it was exploited for admissible pruning of partial plans during search3 .
2. A short review of the POP algorithm, and the corresponding formalism is provided in Appendix A. For
those familiar with the algorithm, we note one slight technical change, stemming from the use of unary
operators. POP uses two fictitious actions A0 and A to capture the initial and goal state, respectively.
Here, we replace each of these actions by a set of actions, each with a single effect. The (fictitious) action
setting the initial value of variable vi is denoted A0i and the fictitious action whose precondition is the
goal value of variable vi is denoted Ai .
3. Irreducible plans were called in (Kambhampati, 1995) minimal plans. However, we decided to change the
name of this concept in order to prevent an ambiguity between minimal as irreducible and minimal
as optimal.

322

fiStructure and Complexity in Planning with Unary Operators

Now, given a planning instance , let P be the set of all irreducible plans for . We
denote by MaxReq(v) the maximal number of times that a variable v  V changes its value
in the course of execution of an irreducible plan for . Formally, let Req(P, v) be the number
of times that v changes its value in the course of execution of a plan P . Then,
MaxReq(v) = max{Req(P, v)}
P P

Observe that, for any planning problem with unary operators, a variable must change
its value at most once for each required change of its immediate successors in the causal
graph (in order to satisfy the necessary prevail conditions), and then at most once in order
to obtain the value requested by the goal. Thus, for all variables in V, MaxReq(v) satisfies:
X
MaxReq(v)  1 +
MaxReq(u)
(1)
succ(v)

where succ(v) denotes the immediate successors of v in the corresponding causal graph.
Adopting the terminology from (Domshlak & Shimony, 2003; Shimony & Domshlak, 2002),
a directed acyclic graph G is directed-path singly connected if, for every pair of nodes s, t  G,
there is at most one directed path from s to t. The following lemma shows that if the causal
graph forms a directed-path singly connected DAG then we can bound MaxReq(v) by n.
Clearly, all polytrees are directed-path singly connected DAGs, but not vice versa.
Lemma 1 For any solvable problem instance  with a directed-path singly connected causal
graph over n variables, for any variable v, we have that MaxReq(v)  n.
Proof: The proof is by induction on n. For n = 1 it is obvious that MaxReq(v)  1. Now
suppose that when |V| = n  1 then for any v  V,
MaxReq(v)  n  1
Let 0 be some problem instance for which |V 0 | = n. Suppose that the variables in V 0 =
{v1 , . . . vn } are topologically ordered based on the domains causal graph. Clearly, vn is
a leaf node (i.e., succ (vn ) = ). We will denote by  the problem instance obtained by
removing vn from the domain, and the corresponding variable set by V. According to Eq. 1,
for each immediate predecessor v of vn in the causal graph,
newMaxReq(v)  MaxReq(v) + newMaxReq(vn )  MaxReq(v) + 1
where newMaxReq(v) denotes MaxReq(v) with respect to 0 . Generally, since the causal
graph is directed-path singly connected, for each variable v  V 0 ,

MaxReq(v) + 1, if there is a path from v to vn
newMaxReq(v) 
(2)
MaxReq(v),
otherwise
and thus, for each v  V 0 , holds
newMaxReq(v)  n

323

fiBrafman & Domshlak

Recall that MaxReq(v) stands for an upper bound on the number of value changes of v
that may be required by a valid, irreducible plan. However, the maximal achievable number
of value changes of v, denoted by MaxPoss(v) can be greater or less than MaxReq(v). For
example, if v has no predecessors in the causal graph, and there are two operators affecting
v differently, then MaxPoss(v) = .
We denote the upper bound on the feasible number of value changes of v that may
be required in a valid, irreducible plan for  by FMaxReq(v). Informally, no more than
MaxPoss(v) value changes of v can be required and no more than MaxReq(v) value changes
of v should be required, thus
FMaxReq(v) = min(MaxPoss(v), MaxReq(v))

(3)

Determining FMaxReq(v) for all variables requires explicit examination of a given problem instance. Recall that here we restrict the causal graph of  to form a polytree. To
simplify the presentation, we assume that the goal values are specified for all state variables,
i.e. Goal  D(v1 )  . . .  D(vn ). Later we show that this assumption does not affect the
generality of the algorithm. Denote by v 0 and v  the initial and the goal values of v in ,
and by v   the set of all operators affecting v. First we examine the root variables of
the causal graph, then we analyze the rest of the variables.
Denote by pred(v) the immediate predecessors of v in the causal graph. If pred(v) = ,
+
+

then there are at most two operators A
v , Av in v : Av has v as its postcondition, while

Av has the reverse effect. Since these operators have no prevail condition, if both A
v and
+
Av are presented in , then they can be applied one after another an infinite number of
+
times. Therefore, from Eq. 3, FMaxReq(v) = n. If v 6= {A
v , Av } then we have two cases:
If the initial and the goal values of v are the same, then we cannot change the value of v
and reconstruct it later, and thus FMaxReq(v) = 0. Alternatively, if the initial and the goal
values of v are different then if v = {A+
v } then we can achieve the goal value of v but only
once and thus FMaxReq(v) = 1. Otherwise, the goal value of v is unachievable, thus the
given problem instance is unsolvable. Table 2 summarize this analysis.

v0
v0

=

v

6=

v

v

{Av , A+
v}
otherwise
+
{A
v , Av }
+
{Av }
otherwise

FMaxReq(v)
n
0
n
1
no solution

Table 2: FMaxReq(v) values for the root variables in the causal graph.
Now consider a variable v which is presented by an internal node in the causal graph:
pred(v) = {w1 , . . . , wk } =
6 . Observe that the number of possible value changes of v depends
on and only on:
1. The initial and the goal values of v, i.e., v 0 and v  .
324

fiStructure and Complexity in Planning with Unary Operators

2. The set of operators affecting v, i.e., v .
3. The maximally possible (but still reasonable) number of times that predecessors of v
can change their values, i.e., FMaxReq(w1 ), . . . , FMaxReq(wk ).
4. The actual scheduling of the value changes of the predecessors of v.
The last point is crucial  it means that in order to determine FMaxReq(v) we should find
a particular scheduling of the value changes of pred(v) that allows such a maximal number
of value changes for v. The corresponding interleaving sequence of vs values, starting and
finishing by v 0 and v  respectively, with FMaxReq(v) value changes will be called maximal
and will be denoted by (v) (|(v)| = FMaxReq(v) + 1).
From Lemma 1, for 1  i  k, we have FMaxReq(wi )  n, thus the number of different
orderings of value changes of pred(v) can be exponential in n. For instance, when, for
1  i  k, we have FMaxReq(wi ) = n, this number of different orderings can be expressed
as:


k1
n 
YX
n  1 ni + 1
 2nk
j
j1
i=1 j=1

where the correctness of the expression on the left side of the inequality is shown by Lemma 4
(see Appendix B, p. 347). Clearly, we cannot check all these orderings in a naive manner.
Following, we provide an algorithm that determines (v) in time which is polynomial in n.
For clarity of presentation we want to distinguish between the different elements of a
maximal sequence (v). Since all variables are binary, we denote the initial value of v, v 0 ,
by bv and the opposite value by wv (black/white). Similarly, bi and wi will stand for the
corresponding values of the variable vi . If so, we can think about all the operators in 
as described in this language. Likewise, we sequentially number the appearances of each
value of v on (v). For example, biv stands for the ith appearance of the value bv along
(v). To illustrate this notation, suppose that D(v) = {true, f alse}, the initial value of v
is v 0 = true, and FMaxReq(v) = 4. Then, we have:
bv  true
wv  f alse
(v) = b1v  wv1  b2v  wv2  b3v
First, for every variable v, every operator A  v is extended to a set of operators that
explicitly specify prevail values for all parents of v in the causal graph: If |pred(v)| = k,
and the prevail condition of A is specified only in terms of some 0  k 0  k parents4 of
0
v, then A is extended to a set of 2kk operators, where each operator extends A by an
instantiation of the previously unspecified parents of v. For example, consider a variable v
with pred(v) = {u, w}, and an operator
A = {pre : {bv }, post : {wv }, prv : {bu }},
4. For every other parent wj of v, we have prv(A)[j] = u.

325

fiBrafman & Domshlak

the prevail condition of which does not involve w. This operator is extended to a pair of
operators:
A0 = {pre : {bv }, post : {wv }, prv : {bu , bw }}
A00 = {pre : {bv }, post : {wv }, prv : {bu , ww }}

corresponding to the possible values of w. In what follows, we refer to the operator set
resulting from such a compilation of  as ./ . Note that, under the assumption of constantly
bounded maximal indegree  of the causal graph, compiling  into ./ takes only polynomial
+1 , and thus |./ | = O(n2+1 ).
time, since, for every variable v, |./
v |2
Given the maximal sequences (w1 ), . . . , (wk ) and the operator set ./
v we construct
0
a directed graph (denoted as Ge (v)) that captures all (and only) feasible sequences of, up
to n, value changes of v, where each value change is annotated with the corresponding
assignment on pred(v). Although the number of the captured sequences can be exponential
in n, the size of G0e (v) is polynomial in n. With respect to this graph, the problem of finding
the maximal sequence (v) is reduced to the problem of finding a longest path from a given
node to an arbitrary other node in a directed acyclic graph.
The graph G0e (v) is created in three incremental steps. At the first step, given the
maximal sequences (w1 ), . . . , (wk ) and the operator set ./
v we construct a directed labeled
graph G(v) capturing information about all sequences of assignments on pred(v) that can
enable n or less value flips of v. The graph G(v) is defined as follows:
1. G(v) consist of  nodes, where

 n, ((n = 2j) and (v 0 = v  )) or
((n = 2j + 1) and (v 0 6= v  )), j  N
=

n  1, otherwise
2. G(v) forms a 2-colored multichain, i.e., (i) the nodes of the graph are colored by black
and white, starting by black; (ii) there are no two subsequent nodes with the same
color; (iii) for 1  i    1, edges from the node i are only to the node i + 1.
Observe that such a construction of G(v) promises that the color of the last node will
be consistent with v  .
3. The nodes of G(v) are denoted precisely by the elements of the maximal sequence
(v), i.e., biv stands for the ith black node in G(v).
4. Suppose that there are m operators in ./
v that change the value of v from bv to wv . In
i
this case, for each i, there are m edges from biv to wvi , and |./
v |  m edges from wv to
bi+1
v . All edges are labeled by the prevail conditions of the corresponding operators,
i.e., a k-tuple of the values of w1 , . . . , wk . This tuple is denoted by l(e) (label of the
edge e) and its component, corresponding to a predecessor wi , is denoted by l(e)wi .
This formal definition of G(v) is relatively complicated, thus we provide a demonstrating
example: Suppose that we are given a problem instance over 5 variables, and we consider
326

fiStructure and Complexity in Planning with Unary Operators

a variable v with pred(v) = {u, w}, v 0 = bv , and v  = wv . Recall that every operator in
./ is presented as a three-tuple hpre, post, prvi of pre-, post-, and prevail conditions of the
operator respectively. Suppose that:
1
2
(u) = b1u  wu1
(w) = b1w  ww
 b2w  ww
 1
 Av = {pre : {bv }, post : {wv }, prv : {bu , ww }}
./
A2 = {pre : {wv }, post : {bv }, prv : {bu , bw }}
v =
 v3
Av = {pre : {wv }, post : {bv }, prv : {wu , ww }}

For this case, the graph G(v) is presented by Figure 2.
bu bw
bu ww /
b1v

wv1

bu bw

$
:

bu ww /
b2v

wv2

$

3 bu ww /

: bv

wv3

wu ww

wu ww

Figure 2: Example of the graph G(v).
The constructed graph G(v) captures information about all potentially possible executions of the operators in ./
v that can provide us MaxReq(v) or less value changes of v.
Each path, started at the source node of G(v), uniquely corresponds to such an execution.
Although the number of these alternative executions may be exponential in n, this graphical representation is compact: the number of edges in G(v) is O(n  |./
v |). Note that the
information about the number of times that each operator in ./
can
be executed is not
v
captured by G(v). The following two steps add this information indirectly and exploit it to
find a maximal sequence (v).
At the second step of construction, we expand G(v) with respect to the maximal sequences (w1 ), . . . , (wk ) as follows: Each edge e  G(v) (which by definition corresponds
to some operator A  ./
v ), is replaced by a set of edges such that their labels correspond to
all possible assignments of the elements of (w1 ), . . . , (wk ) to l(e) (i.e., prv(A)). Likewise,
we add a dummy source node sv , with an edge from sv to the original source node of G(v)
labeled by a tuple of the first elements of (w1 ), . . . , (wk ) (= initial values of w1 , . . . , wk ).
Similarly, we add a dummy target node tv , with an edge from the original target node of
G(v) to tv labeled by a tuple of the last elements of (w1 ), . . . , (wk ) (= goal values of
w1 , . . . , wk ). We denote this extended graph by G0 (v), and Figure 3 illustrates G0 (v) for the
example above.
The extended graph G0 (v) can be viewed as a projection of the maximal sequences (wi ),
1  i  k, on the graph G(v). Each edge in G(v) may be replaced by O(nk ) edges in G0 (v),
and thus the number of edges in G0 (v) is O(nk+1  |./
v |).
It is easy to see that not all paths in G0 (v) starting at sv are relevant. For example, in
G0 (v) above, an operator instance prevailed by b1u b2w can not be performed after an operator
2 . Thus, now we are faced with the problem of finding a longest
instance prevailed by b1u ww
feasible path from sv to a node in G0 (v), the label of which is consistent with v  . The
following (last) step provides a reduction of the problem of finding a longest feasible path
from sv to a v  -colored node in G0 (v) to a known problem of finding a longest path in a
327

fiBrafman & Domshlak

b1u b1w

b1u b1w
1
b1u ww

sv

b1u b1w

/ b1
v
2
b1u ww

&

b1u b2w
1
8 wv
1 w1
wu
w

% 
2
9 bH v

1
b1u ww

2
b1u ww

1 w2
wu
w

&

b1u b2w
2
8 wv

% 
3
9 bH v

1 w1
wu
w

1
b1u ww

&

3
8 wv

1 w2
wu
w

/ tv

2
b1u ww

1 w2
wu
w

Figure 3: Example of the graph G0 (v).
directed acyclic graph. Let the graph G0e (v) have the edges of G0 (v) as nodes, and let its
edges be defined by all allowed pairs of immediately subsequent edges in G0 (v): (e, e0 ) is
allowed if, for 1  i  k, either l(e)wi = l(e0 )wi or l(e0 )wi appears after l(e)wi on (wi ). Such
a construction is a variant of a so called edge graph known in graph theory; the addition
in our case is the exclusion of non-allowed edges from it. Clearly, G0e (v) can be constructed
2
in time polynomial in size of G0 (v), and the number of edges in G0e (v) is O(n2k+2  |./
v | ).
b1u b1w

b1 b1

2
wu1 ww

2
wu1 ww

u /w J
// JJ
// JJJ
// JJJJ
// JJJ
J
//
JJ
JJ
//
$
$
//
/
1
1
1
2
1
1
1
1
2
/
_
_
_
_
/
bu bw //
b1u ww
bu ww
bu ww
bu bw //
//77
77 /
JJ
7
//77
//
u:
JJ
77 //
7 /
u
//77
//77
JJ
u
/
/
77 /
7 /
JJ
// 77
// 77
u
J$
/
/
7
7
7
7
u
// 7
// 7
//
77 //
7
7
7
1
1
2
/
/
7
7
7
bu bw
wu1 ww
77 //
7 //
// 77
// 77
II
u:
77//
7 //
II
u
// 77
// 77
II
uu
77//
7 //
// 77
// 77
II
uu
u
/
/
7
7
7
7
I$
u
// 
// 

u

// w1 w1
// w1 w1
2
1 w2
1 w2
b1u ww
b
b
u wJ
u w
// u w
// u w
JJ
JJ
JJ
//
JJ
JJ ///
JJ /
JJ /
JJ /
J$ 
$ 

Figure 4: Example of the graph G0e (v).
Figure 4 presents G0e (v) for our example. The dashed edges present the longest path
from the dummy source node to a node that corresponds to a value change from v  to v 
(from bv to wv ). Such a longest path in G0e (v) describes a maximal sequence of value changes
(v), and its length is actually FMaxReq(v) + 1. In our example, (v) = b1v  wv1  b2v  wv2 , and
FMaxReq(v) = 3. Note that if v 0 = v  then the empty path will be also acceptable since,
in general, v does not have to change its value. In this case FMaxReq(v) = 0 and (v) will
consist of only one element which corresponds to the initial (= goal) value of v.
Observe that a longest path in G0e (v) describes not only (v) but also the actual sequence
j
of invocations of the operators from ./
v that provides (v). We denote by {A(bv )} and
j
{A(wv )} the sequences of operator instances that have as effects the corresponding elements
from the sequences {bjv } and {wvj } ({bjv }  {wvj } = (v)) of vs values, respectively. In
what follows, we address these sequences of operator instances as one sequence of operator
328

fiStructure and Complexity in Planning with Unary Operators

Procedure forward-check ()
1. Topologically sort all variables V based on the the causal graph.
2. For each variable v  V, call determine-max-sequence(, v), respecting the above
ordering.
3. If one of the calls to determine-max-sequence return failure, then return failure.
Otherwise return success.
Procedure determine-max-sequence (, v)
1. If pred(v) =  then
(a) If v 0 6= v  and A+
v 6 v , return failure.
(b) Otherwise, determine (v) according to the rules in Table 2, and return success.
2. Otherwise, if pred(v) = {w1 , . . . , wk } then
(a) Construct G(v) (based on v 0 , v  , and ./
v ).
(b) Construct G0 (v) (from G(v), based on (w1 ), . . . (wk )).
(c) Construct G0e (v) (from G0 (v), based on (w1 ), . . . (wk )).
(d) Determine the longest path in G0e (v) to a node corresponding to a v  -ended value
change, and derive (v) and the corresponding sequence of operators from it.
(e) If v 0 6= v  and FMaxReq(v) = 0, return failure. Otherwise, return success.
Figure 5: forward-check algorithm
FMaxReq(v)

instances v = {A(vi )}i=2

vi

=




, where A(vi ) has vi as its effect, and
i+1

bv 2 ,
i
2

 w ,
v

i = 2k + 1

kN

i = 2k

Procedure forward-check in Figure 5 summarizes the presented approach. Note that
finding a set of longest paths from a node to all other nodes in a directed acyclic graph
can be done in time linear in the size of the graph (Wiest & Levy, 1969). Therefore, the
time complexity of a call to the determine-max-sequence procedure with a variable v is
2
bounded by the size of the constructed graph G0e (v) and thus is O(n2k+2 |./
v | ). forwardcheck calls determine-max-sequence n times. Therefore, if the maximal node indegree
is bounded by a constant , then the overall complexity of the algorithm is O(|V|2+3 22+2 ),
i.e., polynomial in the size of the problem description.
Theorem 1 A given problem instance with a polytree causal graph is solvable if and only
if, for each v  V, forward-check succeeds in constructing the maximal sequence (v).
forward-check fails if and only if at least one of the calls to the determine-maxsequence procedure fails. In turn, a call to determine-max-sequence on a variable v
329

fiBrafman & Domshlak

Algorithm: pop-pcg (hA, O, Li, agenda, )
1. Termination: If agenda is empty, return hA, O, Li
2. Goal selection: Let hi , Aneed i be a rightmost pair on the agenda (by definition,
Aneed  A and i is one of the pre/prevail conditions of Aneed ).
3. Operator selection:
(a) If Aneed 6= Ai (i = ij ) then Aadd = A(ij )  i  {A0i }.
(b) Otherwise:
i. Let m = max { j | A(ij )  A}.
ii. If vi is consistent with im (both associated with the same color from {b, w})
then Aadd = A(im ), else Aadd = A(im+1 ).


4. Plan updating: Let L = L  {Aadd i Aneed }, and let O = O  {Aadd < Aneed }. If
Aadd is newly instantiated, then A = A  {Aadd } and O = O  {A0i < Aadd < Ai }
(otherwise A and O remain unchanged).
5. Update goal set: Let agenda = agenda - {hi , Aneed i}. If Aadd is newly instantiated,
then for each of its pre/prevail conditions Q, add hQ, Aadd i to agenda.
6. Threat prevention: If Aadd = A(ij ), j > 1, then, for each A  A, s.t. ij1 belongs
to the prevail conditions of A, add {A < A(ij )} to O.
7. Recursive invocation: pop-pcg(hA, O, Li, agenda, ), where agenda is topologically ordered (based on the causal graph with respect to the precondition part of each
pair).
Figure 6: pop-pcg algorithm
fails if and only if the initial and the goal values of v are different but there is no way to
change the value of v even once. Thus, if forward-check fails, then no plan exists.
To prove the opposite direction we proceed as follows: We define the pop-pcg algorithm
(POP for polytree causal graphs) and show that it will succeed without backtracking if
forward-check succeeds.5 pop-pcg is described in detail in Figure 6, and it works as
follows: First, let us expand each sequence of operator instances i by A(i1 ) (A(b1i )) which
will stand for the dummy operator A0i . (Recall that up until now, only operators of the
form A(ij ) for j > 1 were defined.) The algorithm maintains a goal agenda sorted based
on the causal graph structure: parent variables appear after their descendents. At each
point, the next agenda item is selected; if it requires achieving some value for vi we add the
corresponding operator to the plan with the desired effect (step 3a). Actually, if we would
be ready to accept plans with possible redundant steps, we can omit the next step 3b from
the algorithm by assuming that the goal value of each variable v is the last element of the
5. For a short review of the POP algorithm, the corresponding formalism, and the description of the initial
call to the algorithm, we refer the reader to Appendix A.

330

fiStructure and Complexity in Planning with Unary Operators

maximal sequence (v). However, if we would like our plan to be irreducible, then a careful
decision about the really required number of value changes of each variable is required.
This decision is captured in step 3b by analysis of the value changes of a variable vi that
were found necessary in the previous iterations of the algorithm in order to satisfy the
predecessors of vi in the causal graph. Note that the agenda is sorted with respect to some
reverse topological ordering of the causal graph, thus if an operator affecting vi was selected
from the agenda then no operator affecting some predecessor of vi in the causal graph will
appear on the agenda until the end of the algorithm. No threats arise in pop-pcg, and the
ordering constraints are consistent.
Lemma 2 If forward-check was successful then pop-pcg will return a valid plan.
Proof:

The lemma will follow from the following claims:

1. For every agenda item, there exists an operator that has it as an effect.
2. There are no threats in the output of pop-pcg.
3. The ordering constraints in O are consistent.
4. The agenda will be empty after a polynomial number of steps.
For the proof see Appendix B, p. 343. 
Recall that, for simplicity of presentation, we assumed that the goal values are specified
for all state variables (single goal state), i.e. Goal  D(v1 )  . . .  D(vn ). Now we show that
the presented approach, with minor modifications, works for a set of possible goal states as
well, if such a set is specified by a partial assignment on V, i.e. Goal  D+ (v1 ). . .D+ (vn ).
Note that the latter assumption is widely accepted in the planning literature.
First, no modifications should be done in processing variables that are specified by Goal.
Now, for each variable v, such that v  is not specified by Goal, the modifications are as
follows:
1. The graph G(v) will consist of exactly n nodes. This is correct since (i) according to
Lemma 1, n changes of v have to be sufficient, and (ii) any value change of v can be
its last value change.
2. No changes in construction of G0 (v) and G0e (v).
3. In the procedure determine-max-sequence:
(a) In step 2d, determine the longest path from the dummy source node to any other
node in the graph.
(b) In step 2e, always return success.
Again, this is correct since any value change of v can be its last value change, and, in
particular, v may remain unchanged in a plan for a given problem.
Finally, the pop-pcg algorithm starts with a null plan that contains the end operator
Ai only if vi is specified by Goal.
331

fiBrafman & Domshlak

4. Directed-Path Singly Connected and General DAGs
In this section we analyze planning complexity in face of more complicated causal graphs.
First, we show that when the causal graph is directed-path singly connected even plan
existence is np-complete. Second, we show that for general causal graphs the situation is
even worse. Finally, we characterize an important parameter of the causal graph affecting
planning complexity, which allows us to extend the class of problems which are in np.
Theorem 2 Plan existence for strips planning problems with unary operators and directedpath singly connected causal graph is np-complete.
Proof:

For the proof see Appendix B, p. 346. 

Note that node indegree in the causal graph of the problem created in the proof of
Theorem 2 is bounded by 6. The hardness for directed-path singly connected causal graphs
with maximal indegree lower than 6 is thus open.
The directed-path singly connected structure of the causal graph turns out to be crucial
for guaranteeing reasonable solution times. As we now show, there are solvable propositional planning problems with an arbitrary acyclic (DAG) causal graph that have minimal
solutions of exponential size. Analysis of this class of problems points to the reason for such
provable intractability. This allows us to characterize an important parameter of the causal
graph affecting planning complexity and to extend the class of problems which are in np.
However, all these restricted problems are still np-complete.
Theorem 3 Plan generation for general strips planning problems with unary operators
and acyclic causal graph is provably intractable, i.e. it is harder than np.
This theorem follows from Theorem 5.4 in (Jonsson & Backstrom, 1998b), that shows
that plan generation for the 3S problem class is provably intractable. The point is that the
upper bound for MinPlanSize, presented in Eq. 5, can be exponential in the size of the input
in this case. First, we show by example that this upper bound can be achieved, then we
present some analysis of the reasons for this intractability.
The following example shows that an exponential upper bound can be achieved. It was
used in the proof of Theorem 5.4 in (Jonsson & Backstrom, 1998b), and was originally
presented in a different context by Backstrom and Nebel (1995). Consider a propositional
planning problem with |V| = n, where, for 1  i  n, D(vi ) = {0, 1} and pred(vi ) =
{v1 , . . . vi1 }. The operator set  consist of 2n operators {A1 , A01 , . . . An , A0n } where
pre(Ai )[j] =

post(A0i )[j]


=

0 if j = i
u otherwise


1
pre(A0i )[j] = post(Ai )[j] =
u

 0
0
1
prv(Ai )[j] = prv(Ai )[j] =

u
332

if j = i
otherwise
if j < i  1
if j = i  1
otherwise

fiStructure and Complexity in Planning with Unary Operators

It is easy to see that the causal graph of this problem forms a DAG (see Figure 7), and
an instance of this planning problem with the initial state h0, . . . , 0i and the goal state
h0, . . . , 0, 1i has a unique minimal solution of length 2n  1 corresponding to a Hamilton
path in the state space.
PQRS
WVUT
V1

PQRS
/ WVUT
V2

'

...

4

PQRS
WVUT
Vn1

&
PQRS
/ WVUT
Vn
7

Figure 7: Causal graph for the proof of Theorem 3
Now we show that this escalation in complexity can be parametrized by the form of
the causal graph.
Lemma 3 For any solvable problem instance  with an acyclic causal graph over n variables, for any variable v, we have that:
MaxReq(vi )  1 +

n
X

(vi , vj )

j=i+1

where (vi , vj ) denotes the total number of different, not necessary disjoint, paths from vi
to vj , where variables are ordered via a topological sort of the causal graph.
Proof: The proof is by induction on i. For i = n it is obvious that MaxReq(vn )  1.
Now we assume that the lemma holds for any i > k, and prove it for i = k. Without loss
of generality, assume that succ(vk ) 6= . Otherwise, we simply have that MaxReq(vk )  1.
The proof is straightforward:
Eq. 1

MaxReq(vk )



X

1 +

MaxReq(vik ) 

vik succ(vk )
I.H.



X

1 + |succ(vk )| +

vik succ(vk )

=

1+

n
X

n
X

(vik , vj ) =

j=ik +1

(vk , vj )

j=k+1


Lemma 3 entails that the upper bound for MinPlanSize() for a general planning problem
with unary operators and acyclic causal graph depends on the number of different paths
between the nodes in the causal graph. An immediate conclusion is that there is a significant
class of problems with an acyclic causal graph for which planning is in np. Let a DAG be
called max--connected if the number of different directed paths between every two nodes
in this graph is bounded by .
333

fiBrafman & Domshlak

Theorem 4 Plan generation for strips planning problems with unary operators and max-connected causal graph is np-complete if  is polynomially bounded.
Proof: Membership in np is straightforward: If the variables of a given problem  are
considered in a topological ordering induced by the causal graph, then from Lemma 3 follows
that, for any variable vi , MaxReq(vi )  n. In turn, from this follows that MinPlanSize() 
n2 , and thus, if  is polynomially bounded, then we can guess a minimal plan for  that
could be verified in polynomial time.
The hardness follows from Theorem 2 that shows that even if the causal graph is max-1connected (directed-path singly connected), then plan existence (and thus plan generation)
is hard. 

5. Serializable Subgoals
A set of subgoals is defined to be serializable (Korf, 1987) if there exists an ordering among
the subgoals such that the subgoals can always be solved sequentially without ever violating
a previously solved subgoal in the order. Naturally, not all collections of subgoals are
serializable  sometimes it may be necessary to interleave plans for achieving different goals.
However, when a problem instance is serially decomposable, it is possible to design a set of
macro-operators with respect to which the subgoals are serializable (Korf, 1985).
A problem instance is serially decomposable if there exists some ordering of the state
variables for which the effect of each operator on each state variable depends only on that
state variable and previous state variables in the ordering. Unfortunately, Bylander (1992)
shows that determining serial decomposability of a problem is pspace-complete.
One major open problem put forth by Bylander in this context is: If a problem is
known to be serially decomposable, how difficult is it to determine whether a given instance
is solvable? As far as we know, the only work in this direction was done by Chalasani
et al. (1991), where the serial decomposability of the general permutation problem was
considered. In particular, they showed that this problem is in np, but it is unknown
whether it is np-hard. Recently, some complementary results for Bylanders question were
presented Koehler and Hoffmann (2000). Our results shed more light on this question:
Any problem instance based on a unary operator domain whose causal graph is acyclic is
serially decomposable. Therefore, it can be concluded that finding a solution for serially
decomposable problems may require exponential time (i.e., the problem is in exptime).
However, Bylanders question is about plan existence. In that case, Theorem 3 does not
apply, and we can only apply our np-hardness result (for directed-path singly-connected
graphs), since it addresses plan existence as well.
Weld (1999) hypothesized that: (1) If the underlying causal graph of the planning
problem is acyclic, then a serialization ordering on the subgoals of the problem is obvious;
(2) Serialized subgoals could be solved extremely quickly because no backtracking is required
between them. Although the first observation sounds intuitive, our results suggest that it
is rarely true. The acyclicity of the causal graph implies serializability, but in most of the
cases its structure does not provide us sufficient information about the actual serialization
ordering. Even when the causal graph is a directed tree one must think first before choosing
334

fiStructure and Complexity in Planning with Unary Operators

an ordering. Likewise, our results imply that when the causal graph does not form an
undirected tree determining a subgoal ordering is np-complete, and if the causal graph is
not directed-path singly connected, the problem is even more complex.
The second observation is not always true either. The problem is that it is important to
determine not only the serialization ordering over the subgoals, but also the exact strategies
for achieving them. As we showed, in certain cases, a problem with n serializable subgoals
requires an exponentially long solution. When the domain variables are not binary, the
situation is even worse  some of the corresponding complexity results can be derived from
the computational analysis of Domshlak and Dinitz (2001).

6. Connection with Related Work on Planning Complexity
The idea of analyzing and exploiting structural properties is not new to classical planning,
and in the last few years a number of important results have emerged. Generating plans
in the context of the strips representation language was shown by Bylander (1994) to
be pspace-complete. Despite this fact, the existence of many successful planning systems,
especially in recent years, demonstrates that planning is possible and practical for a wide list
of domains. Bylander argues that the large gap between the theoretical hardness of planning
and its practical success stems from the use of domain-dependent problem analysis and
algorithms. Consequently, various authors have explored the existence of some constrained
problem classes for which planning is easier.
In this section we shortly overview some of the major, previous results on complexity
of planning, and discuss their relationship to the results presented in this paper. For a
more detailed presentation of the previous results discussed below we refer the reader to
the original papers.
6.1 Local Syntactical Restrictions
In his seminal paper, Bylander (1994) presents a number of complexity results for propositional planning, analyzing different planning problems based on the type of formulas used,
the number and type (positive/negative) of operator pre- and postconditions, etc. The work
of Bylander is extended by some interesting, complementary results by Erol at al. (1995).
For example, Bylander shows that propositional planning in domains where each operator
is restricted to have positive preconditions and one postcondition only is tractable. Generally, extremely severe restrictions on operators are required to guarantee tractability, or
even membership in np. Note that Bylander (1994) and Erol et al. (1995) focuses on local
syntactical properties of operators, i.e., properties of single operators.
The only syntactic restriction that we pose on the planning problems in this paper is
the unarity of the operators. Determining plan existence for this, apparently easier class
of problems was shown by Bylander to be as hard as general propositional planning, i.e.
pspace-complete. Note that this result by itself does not entail our Theorem 3, since
planning problems with unary operators may induce causal graphs with cycles. Therefore,
none of our results is entailed by the results presented by Bylander (1994) and Erol et
al. (1995).
335

fiBrafman & Domshlak

6.2 Global Syntactical Restrictions
Backstrom and Klein (1991a, 1991b), and, subsequently, Backstrom and Nebel (1995),
consider other types of restrictions, but using a more refined model (the SAS formalism) in
which:
1. The state variables are multi-valued, and
2. Two types of preconditions are considered: prevail conditions, which are variable
values that are required prior to the execution of the operator and are not affected by
the operator, and preconditions, which are affected by the operator.
In general, four different restrictions were considered in these works:
(P) Post-uniqueness: For each effect there is at most one operator that achieves this effect.
In other words, desired effects determine operators to be used in a plan. Formally, a
problem instance is post-unique if and only if, for each vi  V and x  D(vi ), there is
at most one operator A   such that post(A)[i] = x.
(S) Single-valuedness: At most one value of each state variable appears in the prevail
conditions of the operators. For instance, if a certain operator requires the light
to be on (as a prevail condition), no other operator can use the prevail condition
that the light is off. Formally, a problem instance is single-valued iff there exist no
two operators A, A0   and vi  V such that prv(A)[i] 6= u, prv(A0 )[i] 6= u, and
prv(A)[i] 6= prv(A0 )[i].
(U) Unariness: Each operator affects only one state variable.
(B) Binariness: All state variables have exactly two possible values, i.e. all state variables
are propositional.
All these four properties are syntactical. However, the properties P and S differ from the
properties U and B by the fact that they have a global nature: Post-uniqueness and singlevaluedness restrict not the form of the operators, but a global property of the whole set of
operators. Backstrom and Nebel (1995) showed that US (unariness and single-valuedness)
is the extreme problem class for which plan generation is polynomial. 6
The problems that we analyzed in this paper belong to the problem class UB, by definition. As we already mentioned, even determining plan existence for this class of problems
is pspace-complete. Now consider the problem class PUB. Backstrom and Nebel (1995)
showed that: (i) PUB has instances with exponentially long minimal solutions, thus plan
generation for PUB is requires exponential time; (ii) existence of bounded length plans for
PUB is strongly np-hard; and (iii) the complexity of general plan existence for PUB is still
an open question. Informally it means that strengthening restrictions from UB to PUB
does not reduce the complexity significantly, at least from the practical point of view.
Proposition 1 Every UB problem instance with a tree causal graph is either post-unique,
or can be transformed into an equivalent post-unique problem instance in (low) polynomial
time. Thus, TreeUB  PUB.
6. For a thorough analysis of the complexity of SAS planning, we refer to Backstrom and Nebel (1995).

336

fiStructure and Complexity in Planning with Unary Operators

Proof: Consider a UB problem with a tree causal graph, and suppose that it is not postunique. It means that there exist a variable v  V, with D(v) = {v 0 , v 00 }, such that there
exist two operators A1 , A2  v that change the value of v from v 0 to v 00 , and prv(A1 ) 6=
prv(A2 ).
From the assumption that the causal graph forms a tree it follows that |pred(v)|  1.
If pred(v) = , then it is easy to see that the existence of such a pair of operators is
simply impossible. Therefore, let pred(v) = {w}, where D(w) = {w0 , w00 }. Without loss of
generality assume that prv(A1 )[w] = {w0 }, prv(A2 )[w] = {w00 }. Otherwise, if, for instance,
prv(A1 )[w] = u, then it is easy to see that A2 is a redundant operator.
Observe that in this case, prevail dependence of v on w is redundant: We can replace
the pair of operators A1 , A2 in  by a single operator A that changes the value of v from
v 0 to v 00 without any prevail condition. The replacement of A1 , A2 by A brings us to an
equivalent problem instance in which the operator set v is post-unique. This way we
continue to process iteratively all such problematic variables v until we arrive at a postunique problem instance. 

Proposition 2 There are UB problem instances with a tree causal graph that are not singlevalued, thus TreeUB 6 UBS.
Proof: The proof of Proposition 2 is straightforward: Consider a variable v  V, D(v) =
{v 0 , v 00 }, such that succ(v) = {u, w}. It can be the case that any value change of u will be
prevailed by v 0 , while any value change of w will be prevailed by v 00 . Therefore, restricting
causal graphs even to trees does not entail single-valuedness.7 
Propositions 1 and 2 show that TreeUB is a polynomial subclass of PUB that is not
entailed by any tractability results of Backstrom and Nebel (1995).
Proposition 3 There are UB problem instances with a polytree causal graph that are neither single-valued, nor post-unique.
Proof: The proof is straightforward: Consider a planning problem with a polytree causal
graph, such that there exist a variable v  V with pred(v) = {u, w}, and the following
operator set v :
pre
v0
v0
v 00
v 00

post
v 00
v 00
v0
v0

prv
{u0 , w00 }
{u00 , w0 }
{u0 , w0 }
{u00 , w00 }

Clearly, any problem instance with such v   is neither single-valued, nor postunique, since (i) there is more than one operator achieving any value of v, and (ii) both
values of u (and both values of w) appear in prevail conditions of the operators in v . Note
7. Using the simple construction technique from the proof of Proposition 1 it can be shown that restricting
causal graphs to directed chains only does entails single-valuedness. However, this case is too restrictive.

337

fiBrafman & Domshlak

that the maximal indegree of such a polytree can be minimal, i.e. equal to 2. Thus, the
proposition is valid for any polytree that is not a tree. 
From Proposition 3 it follows that Theorems 1 and 4 introduce new polynomial and
np-easy subclasses of the UB problem class, respectively.
6.3 Structural Restrictions in Propositional Planning
Jonsson and Backstrom (1998b) present the 3S class of planning problems. This class is most
closely related to the problems examined in this paper, since it defines a special subclass of
problems with binary variables, unary operators and acyclic causal graphs. The 3S problem
class is defined by posing some additional, relatively severe, restrictions on the problems
operator set: Each variable v in a 3S problem instance is required to be either (i) static,
i.e., unchangeable; (ii) symmetrically reversible, i.e., for each operator A affecting v, there
exist an operator A0 affecting v with the same prevail conditions and the opposite effect;
or (iii) splitting. For the formal definition of the splitting property we refer to Jonsson
and Backstrom (1998b). Informally, if a binary variable v is splitting then the problem
instance can be split into three, well-defined subproblems that can be solved independently.
For this class of planning problems it was shown that plan existence can be determined in
polynomial time, while plan generation is provably intractable, since there are instances of
3S with exponentially long minimal solutions. In particular, the problem instance that we
used in the proof of Theorem 3 is in 3S.
The complexity analysis by Jnonsson and Backstrom (1998b) is somewhat unique in
the research on complexity of propositional planning, since, to the best of our knowledge,
this was the only attempt to exploit not only syntactical restrictions on the operator set,
but also some structural restrictions on interaction between the variables. Our analysis
can be seen as continuing this direction by looking on the structural restrictions only. We
believe that eliminating the marginal effect of the problem structure on the problems (potential) hardness will allow us to understand better the connection between the component
interactions topology, and the potential complexity of the problem.
6.4 Structural Restrictions in Multi-valued Formalisms
When the variables are no longer propositional, some additional properties of the problems
can be identified, and, possibly, exploited. In particular, additional internal structures of
the problem can be analysed.
Jonsson and Backstrom (1998a) analyze different properties of a multi-valued problem
structure, which is called the domain transition graph. Such a structure is defined for each
state variable of the problem, and it describes possible transitions between different values
of this variable. The domain transition graph of a state variable v is a directed labeled
graph Gv = (V, E), where V is associated with the vs set of possible values, D(v), and
(x, A, y)  E if and only if the operator A can be applied at some state in which v = x, and
its application results in a state in which v = y holds.
Jonsson and Backstrom identify sets of structural restrictions on domain transition
graphs which make planning instances tractable. Roughly, the properties are the following:
(1) The problem domain is interference-safe, i.e., each operator is either unary or irreplace338

fiStructure and Complexity in Planning with Unary Operators

able with respect to every variable it affects. An operator A is irreplaceable with respect
to a variable v if the removal of all edges from Gv that stem from A disconnects some
weakly connected component of Gv . (2) For every variable v, the graph Gv , restricted to
the set of values that appear in the prevail conditions of some operators, is acyclic. (3) Any
sequence of operators annotating a path from x to y in the domain transition graph of v, is
stronger than all shortest such sequences connecting x and y. Here, a sequence A1 , . . . , Ak
is stronger than A01 , . . . , A0l if there is a subsequence Ai1 , . . . , Ail of A1 , . . . , Ak such that for
every 1  j  l, the prevail conditions of A0j are a subset of the prevail conditions of Aij .
Jonsson and Backstrom present a map of the computational complexity of problems with
different restrictions, displaying the frontier between the tractable and intractable cases.
Each domain transition graph combines and structures the influence of many operators
on a particular variable. Therefore, they provide us a more global picture than the operator
set alone. Hence, in spite of the fact that domain transition graphs do not capture the relationship between different variables, they do allow us to express some structural properties
that address interactions between the variables (e.g., see property (2) above).
Observe that domain transition graphs are not very informative in the case of propositional planning, since they are only distinguish between the variables that can be changed
only in one direction and the variables that can be changed in both directions. Although
this property of domain transition graphs allows to distinguish between the polynomial
planning with only positive postconditions, and the pspace-complete planning with both
positive and negative postconditions (Bylander, 1994), it seems to be not very helpful in
further hierarchical refinement of the propositional planning complexity. On the other hand,
there is no a priori reason why the causal graphs will not be informative in the multi-valued
case. Exploiting the properties of causal graphs, together with the properties of domain
transition graphs, seems to be a natural direction to extend the work presented in this paper.
The recent work of Domshlak and Dinitz (2001) on multi-entity off-line coordination can be
seen as investigating connections between the structure of the causal graph, together with
the properties of the domain transition graphs, and the complexity of the corresponding
problems in case of multi-valued domains. To the best of our knowledge, this is the only
work that was done with respect to such a mixed structural analysis, and a lot of work
remains to be done. For instance, combining various properties of the domain transition
graphs studied by Jonsson and Backstrom (1998a), with the properties of the problems
causal graph is a direction for the further research.

7. Summary and Future Work
We have shown that the form of the causal graph for strips planning problems with unary
operators is an important factor in determining the computational complexity of plan generation. In particular, we have shown that a polynomial time algorithm exists for any
problem with a polytree causal graph and the node indegree bounded by a constant. More
generally, this result shows that planning with polytree causal graphs is at most (what is
often referred to in the Bayes nets literature as) locally exponential, i.e., it is exponential in the maximal number of parents of a node. Note that in hardware-control planning
problems the maximal node indegree is expected to be small, since prevail dependencies
between the variables reflect the direct interconnections between the corresponding hard339

fiBrafman & Domshlak

ware components. Likewise we have shown that for a problem with directed-path singly
connected causal graph the maximal plan length is a low order polynomial, but the problem
is np-complete. More generally, we have shown a relation between the number of paths between variables in the causal graph and the computational complexity of the corresponding
planning problem. Finally we have presented the impact of our results on the question of
complexity of planning problems with serializable subgoals, and connected our work with
previous results on planning complexity.
Our work leaves a number of open questions with respect to purely syntactical, and a
mixture of both structural and syntactical restrictions on the planning problems with unary
operators. In the former case, one of the most important directions is a further analysis of
causal graphs with constantly bounded node indegree. It turns out that complexity analysis for this class of problems will be very helpful in understanding various computational
properties of CP-nets (Boutilier et al., 1999). Although here we provided a partial answer
for this question, the general picture of the worst-case complexity for this class of problems
is not clear. For example, if the indegree of the causal graph is known to be bounded by
2, and this is the only structural property of the causal graph, it is even not clear whether
this problem subclass is in np.
In the latter case, various syntactical restrictions can be analysed together with the form
of the causal graph. For example, one may be interested in the computational properties
of the problems with acyclic causal graphs, and the restriction that every operator has at
most  prevail conditions, where  is bounded by a constant. This, as well as many other
related questions with respect to various special cases of planning with unary operators are
of interest for the future work.

Acknowledgments
A preliminary version of this paper appeared in the Sixth International Conference on Artificial Intelligence Planning and Scheduling, April, 2002. We would like to thank the three
anonymous reviewers for their extremely helpful comments. Ronen Brafman is supported
in part by the Paul Ivanier Center for Robotics Research and Production Management.

340

fiStructure and Complexity in Planning with Unary Operators

Appendix A. A Short Review of POP, Causal Links and Threats
We represent a plan as a tuple: hA, O, Li, where A is a set of unary operators, O is a set of
ordering constraints over A, and L is a set of causal links. For example, if A = {A1 , A2 , A3 }
then O might be the set {A1 < A3 , A2 < A3 }. These constraints specify a plan in which
A3 is necessarily the last operator, but do not commit to a particular order on A1 and A2 .
Naturally, the set of ordering constraints must be consistent, i.e., there must exist some

total order satisfying them. A causal link has the form Ap i Ac , where Ap and Ac are
operators and i is a possible value for some propositional variable vi . It denotes the fact
that Ap produces (i.e., has the postcondition) vi = i which is consumed by Ac (i.e., used to
satisfy a pre- or prevail-condition of Ac ). Causal links help us detect whether one operator
At interferes with the work done to enable the execution of some other operator Ac . In that
case, At is said to constitute a threat to one of A0c s causal links. Formally, suppose that


hA, O, Li is a plan, and Ap i Ac is a causal link in L. Let At be a different operator in A.


We say that At threatens Ap i Ac when the following two criteria are satisfied:
 O  {Ap < At < Ac } is consistent, and
 At has i as an effect.
When a partial order plan P contains threats, it is possible that the goal will not be
achieved by some (or all) of the total order plans consistent with P s ordering constraints.
To prevent this, the plan generator must check for threats and remove them by adding one
of two possible ordering constraints: At < Ap (demotion) or Ac < At (promotion).
A tutorial introduction to POP algorithms can be found in (Weld, 1994). POP is a
regressive framework for partial order planning that starts with the null plan and continuously updates it by inserting new actions and removing threats. This process continues until
the precondition and the prevail conditions of every operator in the plan are supported by
some causal link and no threats exist. The first argument to POP is a plan and the second
argument is an agenda of goals that need to be supported by causal links. Each item on the
agenda is represented by a pair hi , Ai where i is either pre- or prevail condition of a plan
action A. The last argument to POP is the whole collection of the operators defined by
the planning instance. The initial call to POP contains the null plan, a specially initialized
agenda, and the operator set  of the given problem.
In this paper we introduce a specialized, deterministic POP algorithm that starts the
planning process using a variant of the null plan which encodes the planning problem. In
particular, if the planning instance has v1 , . . . , vn as the goal then the corresponding null
plan has exactly 2n dummy unary operators, A = {A01 , . . . , A0n , A1 , . . . , An }, n ordering
constraints, O = {{A01 < A1 }, . . . , {A0n < An }}, and no causal links, L = {}. For every
vi  V, A0i is the corresponding start operator - it has neither pre- nor prevail conditions,
and its effect specifies the value of the variable vi in the initial state, which is denoted by
vi0 . Similarly, Ai is the end operator - it has no effect, no prevail conditions, but its
precondition is set to the value of vi in the goal state, which in turn is denoted by vi .8
8. Actually, the goal state may not specify the values of all the variables, thus the number of the end
operators can be less than n. However, for clarity of presentation, we leave this definition of the null
plan.

341

fiBrafman & Domshlak

Our description of the null plan is modified from that of Weld (1994) to better suit the
restriction to unary operators. Likewise, the initial call to our POP algorithm contains the
agenda {hv1 , A1 i, . . . , hvn , An i}.

342

fiStructure and Complexity in Planning with Unary Operators

Appendix B. Proofs and Auxiliary Results
Lemma 2 If forward-check was successful then pop-pcg will return a valid plan.
Proof:

The lemma will follow from the following claims:

1. For every agenda item, there exists an operator that has it as an effect.
2. There are no threats in the output of pop-pcg.
3. The ordering constraints in O are consistent.
4. The agenda will be empty after a polynomial number of steps.
(1+4) The first claim follows from the success of the forward-check procedure.
forward-check implies that for any ij  (vi ) there is an operator instance A(ij ) 
i  {A0i }. Therefore, if ij  (vi ) then the existence of an appropriate Aadd is promised.
Assume to the contrary that ij 6 (vi ) and, without loss of generality, assume that this
is the first iteration that it happens. If so, then for each variable u  succ(vi ), there is no
edge labeled by ij in the graph G0 (u), which is created by the forward-check. From this
follows that Aneed cannot have ij as a prevail condition, and thus Aneed has to affect the
variable vi itself. In this case either Aneed = A(ij+1 ) or Aneed = Ai .
Consider the former case: If Aneed = A(ij+1 ) then ij+1 was previously selected from the
agenda. By our assumption it means that ij+1  (vi ), and it contradicts our assumption
that ij 6 (vi ) since ij is a predecessor of ij+1 on (vi ).
Now consider the last option that Aneed = Ai . If Aadd = A(ij ) then the goal value of
the variable vi is consistent with ij , and A(ij1 )  A (see Step 3(b)ii). If A(ij1 )  A
then ij1 was previously selected from the agenda. By our assumption it means that
ij1  (vi ). However, it contradicts our assumption that ij 6 (vi ) since (vi ), by
definition, terminates with a node consistent with the goal value of vi .
In addition, since we have shown that the only operators added into A are those from
i  {A0i , Ai }, for 1  i  n, the agenda will be empty after O(n2 ) steps.


(2) Suppose that some operator At threatens Ap i Ac , i.e.,
 O  {Ap < At < Ac } is consistent, and
 At has i as an effect.
For a given variable vi , pop-pcg forces the operators affecting vi as follows (Step 4):
A0i  A(i1 ) < A(i2 ) < . . . < A(ix ),

x  FMaxReq(vi )

(4)

Thus Ac can only be an operator with i as a prevail condition. Note that Ap and At
affect the same variable vi . In (1) we already showed that i = ij  (vi ). In that
case At = A(il ), l > j. However, if ij is a prevail condition of Ac then the ordering
constraint {Ac < A(ij+1 )} was added to O at Step 6. If so then from Eq. 4, it follows that
343

fiBrafman & Domshlak

{A(il ) < A(ij+1 ), A(ij+1 ) < A(il )}, l > j, will be implied by O  {Ap < At < Ac }, and it
contradicts with the assumption that O  {Ap < At < Ac } is consistent.
(3) The ordering constraints are consistent if no two operators Ai and Aj are such that
O implies {{Ai < Aj }, {Aj < Ai }}. In what follows, Ai will be used to denote an arbitrary
operator affecting variable vi .
First note that each ordering constraint added in Step 4 or Step 6 is either between
operators affecting the same variable or between operators affecting a variable and its child
(with respect to the causal graph). In particular, if Ai < Aj was added in Step 4 then either
vi = vj or vi  pred(vj ), whereas if Ai < Aj was added in Step 6 then vj  pred(vi ).
Assume, to the contrary that O implies Ai < Aj and Aj < Ai . From the argument
above, we know that there is a, possibly empty, path between vi and vj in the undirected
graph induced by the causal graph. By our structural assumption, we know that such an
undirected path between vi and vj is unique, and thus the situation is as follows: We have
two chains of operators
 : Ai = A1i0 < . . . < Axi00 < A1i1 < . . . < Axi11 < . . . . . . < A1im < . . . < Aximm = Aj
 : Ai = A1i0 > . . . > Ayi00 > A1i1 > . . . > Ayi11 > . . . . . . > A1im > . . . > Ayimm = Aj

such that, for 0  k  m, both xk  1 and yk  1. The corresponding unique undirected
path between vi and vj is:
vi = vi0  vi1  . . .  vim1  vim = vj
Without loss of generality, the internal elements of  and  are disjoint. Otherwise, if there
is an operator B that belongs to the internal parts of both  and  then we can reduce
these chains and deduce Ai < B and Ai > B.
The proof of consistency is as follows:
(a) We prove that if such  and  exist then at least one of them should have at least
one internal element.
(b) We show some useful property of  and , that is exploited in (c).
(c) We show that for 0  k  m, Axikk and Ayikk are different except when x0 = y0 = 1.
Note that (a) together with (c) contradicts our assumption that Aximm = Ayimm .
(a) Assume, to the contrary, that both  and  do not contain any internal elements.
If so, then the algorithm actually adds to O ordering constraints Ai < Aj and Aj < Ai .
If vi and vj are the same variable then Ai < Aj can only stem from Step 4 and only
because Ai has the precondition of Aj as its effect. However, by definition of forwardcheck, Aj can not have the same role w.r.t. Ai and thus it is impossible that Aj < Ai was
added to O. Alternatively, if vi is a parent of vj then Ai < Aj can stem only from Step 4
because Ai has the prevail condition of Aj as its effect. Suppose that Ai = A(bji ) and thus
bji  prv(Aj ). In turn, Aj < Ai can be added only in Step 6 because Aj has the precondition
of Ai as the prevail condition. But if bji  prv(Aj ) then Ai = A(wij ), and this contradicts
344

fiStructure and Complexity in Planning with Unary Operators

our assumption that Ai = A(bji ). Alternatively, we can assume that Ai = A(wij ) but this
situation is completely symmetric, and thus the result will be the same. Hence we proved
that either  or  have to contain at least one internal element. In particular it means
that the next to last elements of  and  are different and this fact is exploited later in the
proof.
(b) Consider subchains of  that consist of operators affecting only one particular variable. For each such a subchain, i.e. for 0  k  m, for 1  j  xk  1, the ordering
j
j+1
constraint Ajik < Aj+1
ik can only stem from Step 4 because Aik has the precondition of Aik
as its effect. Thus, post(Ajik ) = pre(Aj+1
ik ). Similarly, for the subchains of , for 0  k  m
j
and for 2  j  yk , post(Aik ) = pre(Aj1
ik ). In what follows we denote this property of 
and  by local monotonicity.
(c) First suppose that either x0 > 1 or y0 > 1, or both. Consider the following sequence:
 : Ayi00 < Ayi00 1 < . . . < A1i0 = A1i0 < ... < Axi00
From the local monotonicity, the construction of forward-check, and the fact that ||  2
0
it follows that post(Ayi0 ) appears before post(Axi00 ) on the maximal sequence vi0 . Continuing
1

with the next variable vi1 we claim that post(Ayi1 ) has to appear before post(Axi11 ) on vi1 .
(i) If vi0 is a parent of vi1 then Axi00 < A1i1 can only stem from Step 4 because Axi00 has
the prevail condition of A1i1 as its effect. In turn, Ayi00 > A1i1 can only stem from Step 6
because the prevail condition of A1i1 is the precondition of Ayi00 . From the relation between
Axi00 and Ayi00 , and the construction of G0e (vi1 ) in forward-check, it follows that post(A1i1 )
appears before post(A1i1 ) on (vi1 ). Subsequently, from the local monotonicity it follows
that post(Ayi11 ) appears before post(Axi11 ) on (vi1 ).
(ii) Similarly, if vi1 is a parent of vi0 then Ayi00 > A1i1 can only stem from Step 4 because
A1i1 has the prevail condition of Ayi00 as its effect, and Axi00 < A1i1 can only stem from Step 6
because the prevail condition of Axi00 is the precondition of A1i1 . From the relation between
Axi00 and Ayi00 , and the construction of G0e (vi1 ) in forward-check, it follows that post(A1i1 )
appears before post(A1i1 ) on (vi1 ), and again, from the local monotonicity it follows that
post(Ayi11 ) appears before post(Axi11 ) on (vi1 ).
Alternatively, if x0 = y0 = 1 then Axi00 = Ayi00 = Ai . From (a) immediately follows
that A1i1 6= A1i1 , and an analysis similar to the above shows that post(Ayi11 ) appears before
post(Axi11 ) on (vi1 ).
Having established that post(Ayi11 ) appears before post(Axi11 ) on (vi1 ), it is apparent
that an inductive argument will allow us to show that for all k > 0 we have that post(Ayikk )
appears before post(Axikk ) on (vik ). Note that in particular it means that the operators
Axikk and Ayikk are different, and this contradicts our assumption that Aximm = Ayimm . 

345

fiBrafman & Domshlak

Theorem 2
Plan existence for strips planning problems with unary operators and
directed-path singly connected causal graph is np-complete.
Proof: First we show the membership in np. Let MinPlanSize() denote the size of the
minimal plan for a problem instance . Using the MaxReq property of the state variables,
the following upper bound for MinPlanSize() is straightforward from the Lemma 1:
X
MinPlanSize() 
MaxReq(v)  n2
(5)
vV

Thus, if we guess a minimal solution for a given solvable problem, we can verify it in low
polynomial time.
The proof of the hardness is by polynomial reduction from 3-sat to the corresponding
propositional plan generation problem with a directed-path singly connected causal graph.
3-sat is the problem of finding a satisfying assignment for a propositional formula in conjunctive normal form in which each conjunct (clause) has at most three literals.
Let F = C1 . . .Cn be a propositional formula belonging to 3-SAT, and let X1 , . . . , Xm
be the variables used in F. An equivalent propositional planning problem with a directedpath singly connected causal graph can be constructed as follows: The variable set is V =
{X1 , X1 , . . . , Xm , Xm }  {C1 , . . . , Cn }. The variables Xi and Xi have no predecessors in
the causal graph, thus pred(Xi ) = pred(Xi ) = {}. In turn, for 1  i  n, pred(Ci ) =
{Xi1 , Xi1 , Xi2 , Xi2 , Xi3 , Xi3 }, where Xi1 , Xi2 , and Xi3 are the variables that participate in
the ith clause of F. Finally, Init and Goal consist of false and true assignments to all
variables in V, respectively.
Let every operator A   be presented as a three-tuple h{pre}, {post}, {prv}i of pre, post,
and prevail conditions respectively. Then, the corresponding operator set  is specified as
follows:
Xi

={

h{f }, {t}, {}i }

Xi

={

h{f }, {t}, {}i }

 Ci

={

h{f }, {t}, {1i }i, h{f }, {t}, {2i }i, h{f }, {t}, {3i }i

	

where ji (1  j  3) corresponds to the truth assignment on the variable Xij that satisfies
the ith clause of F. Let Ci = (X1  X2  X8 ). Then 1i = {X1 = t, X1 = f }, 2i = {X2 =
f, X2 = t}, and 3i = {X8 = t, X8 = f }.
To illustrate the proposed reduction consider the following example. Formula F consist
of 3 clauses: (x1  x2  x3 )  (x1  x2  x4 )  (x2  x3  x4 ). The causal graph of the
corresponding planning problem is as follows:
@ABC
GFED
@ABC
@ABC
GFED
GFED
@ABC
@ABC
@ABC
GFED
@ABC
GFED
x2 F GFED
x3
x1 F GFED
x4
x
x1
x3
::
- F  2 :
-- FF
;
{
H

::
-- FFF
- F  :
{; {; H x8 x8 x8
:

;
{
:
F
F
H

:
-FF
{;
-
A
x8 x8
:
F
FF :::  A
-{; {; x8 x8 H H
F
FF  : 
:
A

;
{
8
x

F
-H
F :
A
F : {; {; {; x8 x8 x8
H A A
 FFFF ::: 
-:
F
8
x
H

;
{
x8
  FFF ::
-H A A

{; x8 {; x8 F x8 F: :
FF:: - 
H

;
{
--  
8
x
F : H A A
FF::- 
{;
F#    { x {; x8 x8
F#     
   { x
@ABC
GFED
@ABC
GFED
@ABC
GFED
C1
C2
C3
346

@ABC
GFED
x4
x8 8x A
A
A

fiStructure and Complexity in Planning with Unary Operators

This is a propositional planning problem with single-effect operators and an underlying
directed-path singly connected causal graph. Clearly, Goal is reachable ( is solvable) if and
only if a satisfying assignment for F can be found. Thus, plan existence for the propositional
planning problems with directed-path singly connected causal graphs is np-complete. 

Lemma 4 Given k ordered sequences 1 ,    , k of n elements each, the number T [k] of
different merges of 1 ,    , k , preserving the orderings induced by 1 ,    , k on their elements, is given by:


k1
n 
YX
n  1 ni + 1
T [k] =
(6)
j1
j
i=1 j=1

Proof: Considering the merge operation of such k sequences as iterative merge of i ,
2  i  k, with the already merged sequences 1 , . . . , i1 , it is easy to see that T (k) can
be expressed as:
(
T [k  1]  S [n(k  1), n] , k > 1
T [k] =
(7)
1,
k=1
where S[x, y] stands for the number of different, order preserving merges of two ordered
sequences of sizes x and y (without loss of generality, we assume that x  y).
We consider the process of merging two ordered sequences  and  0 , ||  | 0 |, as:
(i) partition of  0 into j sub-sequences,
(ii) partition of  into l sub-sequences, where j  1  l  j + 1, and
(iii) interleaving and order preserving concatenation of the sub-sequences of  and  0 .
First, observe that  0 can be partitioned into 1  j  | 0 | sub-sequences. Second, for
0 |1
any j, the numbers of different partitions corresponding to steps (i) and (ii) are |j1
and

||+1
, respectively. Finally, given a pair of such partitions of  and  0 , there exist exactly
j
one possible interleaving and order preserving concatenation as in step (iii). Therefore, we
have:


y 
X
y1 x+1
S(x, y) =
(8)
j1
j
j=1

and combining Eq. 7 with Eq. 8, we arrive to Eq. 6. 

347

fiBrafman & Domshlak

References
Backstrom, C., & Klein, I. (1991a). Parallel non-binary planning in polynomial time. In
Proceedings of Twelfth International Joint Conference on Artificial Intelligence, pp.
268273, Sydney, Australia. Morgan Kaufmann Publishers.
Backstrom, C., & Klein, I. (1991b). Planning in polynomial time: The SAS-PUBS class.
Computational Intelligence, 7 (3), 181197.
Backstrom, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational
Intelligence, 11 (4), 625655.
Boutilier, C., Brafman, R., Hoos, H., & Poole, D. (1999). Reasoning with conditional ceteris
paribus preference statements. In Proceedings of the Fifteenth Annual Conference on
Uncertainty in Artificial Intelligence, pp. 7180. Morgan Kaufmann Publishers.
Bylander, T. (1992). Complexity results for serial decomposability. In Proceedings of the
Tenth National Conference on Artificial Intelligence, pp. 729734, San Jose, CL. AAAI
Press.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Artificial Intelligence, 69 (1-2), 165204.
Chalasani, P., Etzioni, O., & Mount, J. (1991). Integrating efficient model-learning and
problem-solving algorithms in permutation environments. In Proceedings of the Second
International Conference on Principles of Knowledge Representation and Reasoning,
pp. 8998, Cambridge, MA. Morgan Kaufmann Publishers.
Chapman, D. (1987). Planning for conjunctive goals. Artificial Intelligence, 32 (3), 333377.
Domshlak, C., & Dinitz, Y. (2001). Multi-agent off-line coordination: Structure and complexity. In Proceedings of Sixth European Conference on Planning, Toledo, Spain.
Domshlak, C., & Shimony, S. E. (2003). Efficient probabilistic reasoning in Bayes nets
with mutual exclusion and context specific independence. In Proceedings of the Sixteenth International FLAIRS Conference, Special Track on Uncertain Reasoning, St.
Augustine, FL. AAAI Press. to appear.
Erol, K., Nau, D. S., & Subrahmanian, V. S. (1995). Complexity, decidability and undecidability results for domain-independent planning. Artificial Intelligence, Special Issue
on Planning, 76 (12), 7588.
Etzioni, O. (1993). Acquiring search-control knowledge via static analysis. Artificial Intelligence, 62 (2), 255301.
Jonsson, P., & Backstrom, C. (1998a). State-variable planning under structural restrictions:
Algorithms and complexity. Artificial Intelligence, 100 (12), 125176.
Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence does not imply tractable
plan generation. Annals of Mathematics and Artificial Intelligence, 22 (3-4), 281296.
Kambhampati, S. (1995). Admissible pruning strategies based on plan minimality for planspace planning. In Proceedings of the Fourteenth International Joint Conference on
Artificial Intelligence, pp. 16271635, Montreal, Canada.
348

fiStructure and Complexity in Planning with Unary Operators

Knoblock, C. (1994). Automatically generating abstractions for planning. Artificial Intelligence, 68 (2), 243302.
Koehler, J., & Hoffmann, J. (2000). On reasonable and forced goal orderings and their use
in an agenda-driven planning algorithm. Journal of Artificial Intelligence Research,
12, 338386.
Korf, R. (1985). Macro-operators: A weak method for learning. Artificial Intelligence, 26 (1),
3577.
Korf, R. (1987). Planning as search: A quantitative approach. Artificial Intelligence, 33 (1),
6588.
Pell, B., Bernard, D., Chien, S., Gat, E., Muscettola, N., Nayak, P., Wagner, M., & Williams,
B. (1997). An autonomous spacecraft agent prototype. In Proceedings of the First
International Conference on Autonomous Agents, pp. 253261, Marina del Rey, CL.
ACM Press.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: A sound, complete, partial order planner
for ADL. In Proceedings of Third International Conference on Principles of Knowledge Representation and Reasoning, pp. 103114, Cambridge, MA. Morgan Kaufmann
Publishers.
Shimony, S. E., & Domshlak, C. (2002). Complexity of probabilistic reasoning in (directedpath) singly connected (not polytree!) Bayes networks. submitted for publication.
Smith, D., & Peot, M. (1993). Postponing threats in partial-order planning. In Proceedings of
the Eleventh National Conference on Artificial Intelligence, pp. 500506, Washington,
D.C. AAAI Press.
Weld, D. S. (1994). An introduction to least commitment planning. AI Magazine, 15 (4),
2761.
Weld, D. S. (1999). Recent advances in AI planning. AI Magazine, 20 (2), 93123.
Wiest, J. D., & Levy, F. K. (1969). A Management Guide to PERT/CPM. Prentice Hall.
Williams, B., & Nayak, P. (1996). A model-based approach to reactive self-configuring systems. In Proceedings of the Thirteenth National Conference on Artificial Intelligence,
pp. 971977, Portland, OR. AAAI Press.
Williams, B., & Nayak, P. (1997). A reactive planner for a model-based executive. In
Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence,
pp. 11781185, Nagoya, Japan.

349

fiJournal of Artificial Intelligence Research 18 (2003) 83-116

Submitted 07/02; published 1/03

Learning to Order BDD Variables in Verification
Orna Grumberg
Shlomi Livne
Shaul Markovitch

orna@cs.technion.ac.il
slivne@cs.technion.ac.il
shaulm@cs.technion.ac.il

Computer Science Department
Technion - Israel Institute of Technology
Haifa 32000, Israel

Abstract
The size and complexity of software and hardware systems have significantly increased
in the past years. As a result, it is harder to guarantee their correct behavior. One
of the most successful methods for automated verification of finite-state systems is model
checking. Most of the current model-checking systems use binary decision diagrams (BDDs)
for the representation of the tested model and in the verification process of its properties.
Generally, BDDs allow a canonical compact representation of a boolean function (given an
order of its variables). The more compact the BDD is, the better performance one gets
from the verifier. However, finding an optimal order for a BDD is an NP-complete problem.
Therefore, several heuristic methods based on expert knowledge have been developed for
variable ordering.
We propose an alternative approach in which the variable ordering algorithm gains
ordering experience from training models and uses the learned knowledge for finding
good orders. Our methodology is based on offline learning of pair precedence classifiers
from training models, that is, learning which variable pair permutation is more likely to
lead to a good order. For each training model, a number of training sequences are evaluated.
Every training model variable pair permutation is then tagged based on its performance on
the evaluated orders. The tagged permutations are then passed through a feature extractor
and are given as examples to a classifier creation algorithm. Given a model for which an
order is requested, the ordering algorithm consults each precedence classifier and constructs
a pair precedence table which is used to create the order.
Our algorithm was integrated with SMV, which is one of the most widely used verification systems. Preliminary empirical evaluation of our methodology, using real benchmark
models, shows performance that is better than random ordering and is competitive with
existing algorithms that use expert knowledge. We believe that in sub-domains of models
(alu, caches, etc.) our system will prove even more valuable. This is because it features the
ability to learn sub-domain knowledge, something that no other ordering algorithm does.

1. Introduction
The size and complexity of software and hardware systems have significantly increased in
the past years. As a result, it is harder to guarantee their correct behavior. Thus, formal
methods, preferably computerized, are needed for this task.
One of the most successful methods for automated verification of finite-state systems is
temporal logic model checking (Clarke, Emerson, & Sistla, 1986; Queille & Sifakis, 1981).
Temporal logics are suitable formalisms for describing the behavior of a program over time.
A model checking procedure receives a finite-state model of the system and a specification
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiGrumberg, Livne, & Markovitch

written as a temporal logic formula. It returns yes if the model satisfies the formula
(meaning that the system behaves according to the specification). Otherwise, it returns
no, along with a counter example that demonstrates a bad behavior.
Model checking has been very successful in finding subtle errors in various systems.
It is currently recognized by the hardware industry as an important component of the
development phase of new designs. However, model checking procedures often suffer from
high space requirements, needed for holding the transition relation and the intermediate
results.
One of the most promising solutions to this problem is the use of binary decision diagrams (BDDs) (Akers, 1978; Bryant, 1986) as the basic data structure in model checking.
BDDs are canonical representations of boolean functions and are often very concise in size.
Their conciseness also yields efficiency in computation time. Since it is straightforward to
represent the transition relation and the intermediate results as boolean functions, BDDs
are particularly suitable for model checking. Today, existing industrial BDD-based verifiers, such as IBMs RuleBase (Beer, Ben-David, Eisner, & Landver, 1996) and Motorolas
Verdict (Kaufmann & Pixley, 1997) are used by many companies in their development
infrastructure.
The size of a BDD for a given function is sensitive to the ordering of the variables in
the BDD. However, finding an optimal ordering, which yields a smallest BDD for a given
function, is an NP-complete problem (Bollig & Wegener, 1996). Therefore, several heuristic
algorithms based on expert knowledge have been developed for variable ordering in the
hope of reducing the BDD size. Unfortunately, and in spite of the resources invested, these
algorithms do not produce good enough variable orders. The reason for this may be that
only general rules are used and no domain-specific knowledge is exploited.
The goal of this research is to develop learning techniques for acquiring and using
domain-specific knowledge for variable ordering. We assume the availability of one or more
training models. The training models are used for off-line acquisition of ordering experience
which can be used for ordering variables of a previously unseen model.
We first present a method for converting the ordering learning task into a concept
learning problem. The concept is the set of all ordered variable pairs that are in the right
order. The examples are ordered pairs of variables of a given training model. We show
a statistical method for tagging examples based on evaluated training orders and present
a set of variable-pair features. The result is a standard concept learning problem. We
apply decision tree learning to generate a decision tree for each training model. When used
for an unseen model, we combine the trees and generate a partial order which is used for
generating the required order. We also present an extension of the algorithm which learns
context-based precedence relations.
Our algorithm was integrated with SMV (McMillan, 1993), which is the backbone of
many verification systems. Empirical evaluation of our methodology, using real benchmark
models of hardware designs, shows performance that is much better than random ordering
and is competitive with existing algorithms that use expert knowledge.
Section 2 contains background on model checking. Section 3 presents our main algorithm
with empirical evaluation. Section 4 shows the context-based algorithm. Our conclusions
are presented in Section 5.
84

fiLearning to Order BDD Variables in Verification

2. Background
Model checking was introduced by Clarke and Emerson (1986) and by Queille and Sifakis
(1981) in the early 1980s. They presented algorithms that automatically reason about
temporal properties of finite state systems by exploring the state space. The use of binary
decision diagrams (BDDs) to represent finite state systems and to perform symbolic state
traversal is called symbolic model checking. The use of BDDs has greatly extended the
capacity of model checkers. Models with 2 100 and more states are routinely being verified.
BDDs were introduced by Akers (1978) as compact representations for boolean functions. Bryant (1986) proposed ordered binary decision diagrams (OBDDs) as canonical
representations of boolean functions. He also showed algorithms for computing boolean
operations efficiently on OBDDs.
The following subsection gives an overview of how finite state systems are represented
in symbolic model checking. BDDs are then described and the variable ordering problem is
defined. Existing algorithms for static variable ordering algorithms are reviewed. Finally,
a brief description of machine learning algorithms used for ordering is given.
2.1 Finite State Machines in Symbolic Model Checking
Finite state systems (FSM) can be described by defining the set of possible states in a
system and the transition relation between these states. A state typically describes values
of components (e.g., latches in digital circuits), where each component is represented by
a state variable. Let V = {v0 , v1 , ...vn1 } be the set of variables in a system. Let K vi be
the set of possible values for variable v i . Then a state in the system can be described by
assigning values to all the variables in V . The set of all possible states S A is
SA = Kv0  Kv1 ....  Kvn1 .
A state can be written using a function that is true only in this state:
Vn1
i=0

(vi == cj ),

where cj  Kvi is the value of vi in the state. A set of states can be described by a function
as the disjunction of the functions that represent the states.
Figure 1 shows a 3-bit counter. A state in the 3-bit counter can be described by a
tuple which gives an assignment to the 3 variables v 2 , v1 , v0 . For example, the tuple h1, 0, 0i
represents the state with v2 = 1, v1 = 0, v0 = 0. The corresponding boolean expression for
the state is (v2 == 1)  (v1 == 0)  (v0 == 0).
In order to describe a system, we also need to specify its transition relation. The
transition relation describes all the possible transitions of each system state. It can thus be
described by pairs of states, hpresent state, next statei, where next state is a system state
after a transition from the present state. The variables in V will represent the present state
variables, and for each variable vi  V we will define a corresponding next state variable
vi0  V 0 . V 0 will denote the set of next state variables.
An example of a valid transition for the 3-bit counter is from h0, 0, 0i to h0, 0, 1i. The
boolean function which represents this transition is (v 2 == 0)  (v1 == 0)  (v0 ==
0)  (v20 == 0)  (v10 == 0)  (v00 == 1). The transition relation can be represented by a
85

fiGrumberg, Livne, & Markovitch

V2

V1

V0

Figure 1: 3 bit counter
Present State
v2 v1
v0
0
0
0
0
0
1
0
1
0
0
1
1
1
0
0
1
0
1
1
1
0
1
1
1

Next State
v20 v10 v00
0
0
1
0
1
0
0
1
1
1
0
0
1
0
1
1
1
0
1
1
1
0
0
0

Table 1: 3-bit counter transition relation table
boolean function which is the disjunction of the boolean functions of each of the transitions.
Table 1 shows the transition relation for the 3-bit counter.
An alternative method for describing the transition relation is for each state variable
to define its valid next states. This form is known as the partitioned transition relation.
The transition relation is then described by a set of functions (instead of one), one for each
variable. For variable vi , a boolean function Ti (V, vi0 ) defines the next value of vi , vi0 , given
that the current state of the system is V .
For synchronous systems, in which there is a simultaneous transition of all the system
components, the transition relation is

Vn1
i=0

Ti (V, vi0 ).

In model checking it is common to use the partitioned transition relation form of representation, since it is usually more compact in memory requirements and thus allows the handling
of larger systems. For the 3-bit counter, the next state boolean functions are given below,
where  stands for the boolean operator Xor.

T0 (V, v00 ) : (v00 == v0 )
T1 (V, v10 ) : (v10 == (v0  v1 ))
T2 (V, v20 ) : (v20 == (v2  (v0  v1 ))).
86

fiLearning to Order BDD Variables in Verification

2.2 Binary Decision Diagrams
A binary decision diagram (BDD) is a DAG (directed acyclic graph) representation of a
boolean function. A BDD is composed of two sink nodes and several non-sink nodes. The
two sink nodes, labeled 0 and 1, represent the corresponding boolean values. Each non-sink
node is labeled with a boolean variable v and has two outgoing edges labeled 1 (or then)
and 0 (or else). Each non-sink node represents the boolean function corresponding to its 1
edge if v = 1, or the boolean function corresponding to its 0 edge if v = 0.
An ordered binary decision diagram (OBDD) is a BDD with the constraint that the
variables are ordered, and every root-to-sink path in the OBDD visits the variables in
ascending order.
A reduced ordered binary decision diagram (ROBDD) is an OBDD where each node
represents a distinct logic function. This representation is a canonical BDD representation
and the most compact representation possible for a given boolean function and a variable
ordering.
V2

V2

V2

V2

V1

V1

V1
V0
V0

0

0

0
0
0

0

0

1

0
1

0

0

0

0

1

0

0
1

0

0
1

0

0

0

0

1

1

0

0

V1
V0
V0

0
0

0

0

1

0
1

0

0

1

(a)

(b)

V2

V2

T2

V2

T2

V2

V1
V1
V0
V0

T1

V1

1

V1
V0
V0

T0
1
1
0

1

1

0

0

0

1

1

0

0
1

0

0
0

1

1
0

1

1

0

0

T1
T0
1
0
0

(c)

1

1

1

1

0

0

0

(d)

Figure 2: 3-bit counter transition relation (a),(b) and partitioned transition relation (c),(d)

Figure 2 (a),(b) shows the OBDD and ROBDD (respectively) representations of the
transition relation function for the 3-bit counter. The dashed lines are the 0 edges and the
solid lines are the 1 edges. ROBDDs have only two leaf nodes, one with 1 and one with
0. We drew them several times to enhance readability. ROBDDs can also use complement
edges, which produces an even more compact representation. We did not use complement
edges, also for reasons of readability. Figure 2 (c),(d) shows the OBDD and ROBDD
representations of the partitioned transition relation of the 3-bit counter. The variable
order v2 , v20 , v1 , v10 , v0 , v00 was used in all the representations. Variable ordering algorithms
in model checking place the next state variable v i0 adjacent to the present state variable v i .
87

fiGrumberg, Livne, & Markovitch

For the rest of this document we will refer to ROBDDs as BDDs (unless we explicitly state
otherwise).
Bollig and Wegener (1996) proved that finding an optimal variable ordering is an NPcomplete problem. An order is optimal if it yields a BDD with the smallest number of
nodes. Bryant (1986) pointed out that variable ordering greatly influences the size of the
BDD. He showed that for a boolean function, one variable ordering may yield a BDD that
is exponential in the number of variables, while a different ordering may yield a BDD of
polynomial size.
v1

v1

v3

v2

v2

v3

v4

v4

1

0

1

(a)

0
(b)

Figure 3: ROBDDs for the function F (v1, v2, v3, v4) = (v1 = v3)  (v2 = v4)
Figure 3 gives an example of the effect of variable ordering on the BDD size for the function F (v1, v2, v3, v4) = (v1 = v3)  (v2 = v4). In (a) the variable ordering is v1, v3, v2, v4
and in (b) the variable ordering is v1, v2, v3, v4.
Various algorithms have been developed for variable ordering. Exact algorithms (Ishiura,
Sawada, & Yajima, 1991; Drechsler, Drechsler, & Slobodova, 1998; Friedman & Supowit,
1987) are algorithms that find the optimal order. These algorithms use a method similar
to dynamic programming with pruning to find the optimal order. Due to the complexity
of the problem, exact algorithms are only practical for small cases, and one usually has to
turn to other heuristic methods. These heuristic methods can be roughly divided into two
groups.
1. Static Ordering (Aziz, Tasiran, & Brayton, 1994; Butler, Ross, & Rohit Kapur, 1991;
Chung, Hajj, & Patel, 1993; Fujii, Ootomo, & Hori, 1993; Jain, Adams, & Fujita, 1998;
Fujita, Fujisawa, & Kawato, 1988; Malik, Wang, Brayton, & Sangiovanni-Vincentelli,
1988; Touati, Savoj, Lin, Brayton, & Sangiovanni-Vincetelli, 1990) which try to find
a good ordering before constructing the BDD. Most of these algorithms are based on
the topological structure of the verified system.
2. Dynamic Ordering (Rudell, 1993; Meinel & Slobodova, 1998; Bollig, Lobbing, & Wegener, 1995; Meinel & Slobodova, 1997; Meinel, Somenzi, & Theobald, 1997; Ishiura
et al., 1991; Bern, Meinel, & Slobodova, 1995; Fujita, Kukimoto, & Brayton, 1995;
Mercer, Kapur, & Ross, 1992; Zhuang, Benten, & Cheung, 1996; Drechsler, Becker,
88

fiLearning to Order BDD Variables in Verification

& Gockel, 1996; Panda & Somenzi, 1995; Panda, Somenzi, & Plessier, 1994), which
given a BDD with some variable order, reorder the variables in the hope of finding a
smaller BDD.
In model checking procedures, variable ordering is a central component. At the initial
phase of model checking, when the system is translated into a BDD representation, Static
Ordering is used. The order built at this stage greatly influences the memory usage during
the whole computation. However, since model checking keeps producing and eliminating
BDDs, the variable order should be changed dynamically in order to effect the size of the
current BDDs. Dynamic Ordering is used in order to achieve this goal. It is applied by the
model checking procedure whenever the size of the BDDs reaches a certain threshold.
Since our work introduces a static ordering algorithm based on machine learning, the
next subsection presents a review of the existing static algorithms. Most of these algorithms
were developed for combinational circuits (i.e., models whose outputs depend only on their
current inputs and not on inputs of previous cycles) and were described with hardware
terminology. In order to simplify the description, we will describe them with the terminology
we have used so far.
2.3 Static Ordering
Static ordering algorithms try to find an initial good order for the BDD. To do so, they
extract topological data from the model and use this data to determine an order. All the
algorithms convert the model, described by a set of next state functions, into a directed
graph known as the model connectivity graph. Vertices in the graph are variables and
boolean operations (gates). A variable vertex represents a variable, while a gate vertex
represents a function. The edges ni  nj in the graph are between ni , which is either a
variable or gate vertex, and nj , which is a gate vertex. An edge ni  nj is placed if the
function represented by ni is an operand (i.e., an immediate subfunction) of the function
represented by nj . We can divide the static algorithms into four groups that differ in the
way they use the graph information.
2.3.1 Graph Search Algorithms
The method suggested by Malik et al. (1988) assigns to each vertex a level metric and orders
the variables in decreasing level value. The level of vertices with no out edges is set to be
zero and the level of every other vertex (v i ) is set to be level(vi ) = maxvj |vi vj (level(vj )+1).
This method resembles a BFS (breadth first search) which originates in nodes that have no
out edges, and progresses backwards in the model. Fujita et al. (1988) proposed executing
a DFS (depth first search) from the vertices with no out edges, and progressing backwards.
Variables in this algorithm are added in post order form.
The algorithms of Malik et al. and Fujita et al. were designed for cases where only one
function should be represented in a BDD. This is hardly ever the case in model checking.
Butler et al. (1991) adapted the algorithm of Fujita et al. to models with multiple starting
points (that is, multiple vertices with no out edges). Their heuristic guides the algorithm
to select the first vertex as the vertex that represents the function which depends on the
maximum number of variables. This heuristic also guides the search to advance (backwards)
89

fiGrumberg, Livne, & Markovitch

from an inner vertex to the vertex that leads to the maximum number of different variables.
A tie breaking heuristic (Fujita, Fujisawa, & Matsunaga, 1993) for the enhanced algorithm
advises selecting (in case of a tie) the vertex with the maximum number of out edges.
The DFS-based methods append the variables to the variable order. Another DFS-based
algorithm relies on interleaving the variables in the order (Fujii et al., 1993). The algorithm
adds a variable after the variable which precedes it in the DFS order.
2.3.2 Graph Evaluation Algorithms
Graph evaluation algorithms use the model graph to evaluate the model variables and to
perform guided search based on their evaluation values. Minato et al. (1990) propagate
values backward through the graph, starting from vertices with no out edges, whose value
is set to 1. In vertices of boolean operations, the values on the out edges are summed and
the value obtained is divided equally between the in edges. This is done recursively until a
vertex of a variable is reached. At variable vertices the propagated values are accumulated
as the variable evaluation value. The order is constructed by iteratively adding the variable
with the highest value, removing it from the graph, and updating the values.
Chung et al.(1993) proposed two algorithm frameworks. The first framework is composed of two sweeps. In the first sweep each vertex is assigned a value. The values are set
by a propagating algorithm that starts from variable vertices with no in edges and advances
forward (by their out edges) to all the vertices in the graph. In the second sweep a guided
DFS initiated from vertices with no out edges is executed. This search is executed backward
in the graph and is guided by the maximal value. This means that the order of traversal
among vertex ancestors is according to their assigned value. A number of heuristics to
compute the values of the vertices were proposed:
1. Level-Based sets the value of variables with no input edges to be zero. The value of
the other vertices is set to be the maximal vertex value over its inputs plus one.
2. Fanout-Based propagates two values through the graph (one for each boolean value).
At a boolean operation vertex the values are not summed and passed along. Rather,
they are computed according to the boolean operation at the vertex. The initial values
are of variables with no input edges. Their value is set to be the number of out edges
the variable has.
In the second framework proposed by Chung et al., the shortest distance between each
pair of variables is computed. The total distance of a variable is computed as the sum of
its distances to all the variables. The variable with the lowest total distance is selected as
the first variable. The next variable is selected as the closest variable to the last ordered
variable. Ties are broken according to the distance to previous ordered variables.
All the graph evaluation algorithms try to order the variables so that the variable that
most influences the models next state functions will be first. The algorithms differ by the
methodology they use to order the other variables. Some algorithms order them so that
variables which substantially influence the models next-state functions are placed higher
in the variable order (toward the beginning of the order). Other algorithms place the other
variables according to their proximity to previously ordered variables.
90

fiLearning to Order BDD Variables in Verification

2.3.3 Decomposition Algorithms
Decomposition algorithms break down the model into parts. The algorithms then solve
two different problems. The first is finding a good order for each part, and the second is
finding the order of the parts. The order is constructed by combining the solutions of the
two problems.
The algorithm of Malik et al. was extended and adapted for finite-state machines (FSM)
by Toutai et al. (1990). In their algorithm, a model is decomposed to its next state functions,
each of which is considered separately. Variables of each next state function are ordered
according to Malik et al. The next state functions are then ordered by a cost function.
They are ordered so that functions with many overlapping variables will be adjacent. The
variable order is obtained by adding the variables of the next state functions according to
the order of the parts, while removing variables that already exist.
The algorithm of Aziz et al. (1994) decomposes the model in a different way. A model
is a hierarchical composition that is constructed by joining a number of internal parts that
pass information among themselves. Usually, there is less communication among the parts
than within them. Variables of an internal part tend to depend highly on one another.
The algorithm uses a process communication graph (PCG), which models the hierarchical
structure of the model and the communication between the parts. In a PCG each vertex is
an internal part, and an edge i  j connects vertex i and vertex j if part j depends on a
bit of part i. The PCG has parallel edges i  j, one for each bit value in i that j depends
upon. Alternatively, the edges could be weighted.
Given an order of the parts, an upper bound on the BDD size of the model can be computed. The computation is based on the size of the parts and the amount of communication
between them. Heuristics guided by the upper bound are applied in order to determine the
order of the parts. The order of the variables in each part is decided by one of the previous
ordering algorithms.
2.3.4 Sample-Based Algorithms
Sample-based static algorithms (Jain et al., 1998) are not real static algorithms in the
sense that they do not create the order based on information extracted from the model
description. Sample algorithms perform tests on parts of the model (building transition
relations and reachable states). For each part, a number of orders are evaluated. The good
orders are then merged to create a complete order for the model. Sampling algorithms use
traditional algorithms in order to find the candidate orders for the parts. These candidate
orders are then checked by the sampling algorithm.
2.3.5 Summary
A majority of the graph search algorithms and graph evaluation algorithms were developed
for other problems and adapted for symbolic model checking. Some of the algorithms
were developed in the context of combinational circuits, while others were developed for the
simple case of one function. In symbolic model checking the models are rarely combinational
(their outputs almost always depend also on inputs of previous cycles), and there is more
than one function to display. Adapting the existing algorithms to conform to the needs of
91

fiGrumberg, Livne, & Markovitch

symbolic model checking has had various degrees of success. Most of the adapted algorithms
are heuristic and apply a simple rule with some logical reasoning behind it.
The decomposition algorithms are either heuristic or provide a theoretical upper bound.
However, the bounds they use are rarely realistic; for most models we require much smaller
BDDs. The algorithms are also based on decomposing the model into parts and solving the
ordering of each part using graph search algorithms. Thus, they also inherit the drawbacks
of these algorithms.
Despite the efforts that have been invested and the many algorithms that have been
developed for static ordering, the results are not yet satisfactory. The produced BDDs
are too large to manipulate, and dynamic ordering must be applied. One problem with
the above approaches is their generality: they do not utilize domain-specific knowledge.
Domain-specific knowledge is essential for solving the majority of complex problems. It is
also difficult to retrieve. In the next subsection we discuss machine learning methods for
acquiring domain-specific knowledge for ordering tasks.
2.4 Learning to Order Elements
Learning to order elements can be done by first trying to induce a partial order, which can
then be used for generating a total order. In this context, a partial order is usually called
a preference predicate. Preference predicate induction is based on a set of tagged pairs of
elements where the binary tag identifies the preferred element. Broos and Branting (1994)
present a method for inducing a preference predicate using nearest neighbor classification.
The distance between an untagged pair and each tagged pair is computed as the sum of
distances between the corresponding elements. The closest tagged pair is selected. The
preferred element of the untagged pair is the one matching the preferred element in the
tagged pair.
Utgoff and Saxena (1987) represent a pair A, B by the concatenated feature vector
ha1 , . . . an , b1 , . . . bn i. The preference predicate is a decision tree induced from these examples.
Utgoff and Clouse (1991) represent a preference predicate by a polynomial. Let A =
ha1 , . . . an i , and B = hb1 , . . . bn i be a pair of elements represented by feature vectors. Let
w1 , . . . , wn be a set of weights. The preference predicate P is defined as follows:
P (A, B) =

(

n
1
i=1 wi (ai  bi )  0
0 otherwise

P

Each example represents a linear constraint and the weights are found by solving the set of
constraints.
Cohen, Schapire and Singer (1999) extended the above mechanism by allowing any
preference function fi instead of (ai  bi ) in the above expressions. They also present
two methods for generating a total order based on the induced preference predicate. Both
methods use the preference predicate to construct a graph where the nodes are the elements
to be ordered and a directed edge is placed between two elements that have a precedence
relation. Two algorithms for inferring the order from the graph are given. The first defines
for each node a degree which equals the sum of the outgoing edges minus the sum of the
incoming edges. The order is then constructed by selecting the node with the greatest
92

fiLearning to Order BDD Variables in Verification

degree and removing its edges from the graph. The second algorithm constructs the order
in two stages. In the first stage, all the strongly connected components of the graph are
found, and they are ordered according to the dependencies between them. In the second
stage the elements of each component are ordered using the first algorithm.

3. A Learning Algorithm for Static Variable Ordering
Producing a good variable order requires extensive understanding of BDDs and their relation
to the model they represent. Such knowledge can be manually inserted by a human expert.
However, this task is too complex for large models. Therefore, it is rarely done. Existing
static ordering algorithms use relatively simple heuristic rules that are based on expert
knowledge. These rules look at the model structure to compose the ordering. Since the
rules are to be applied to all variables in all the models, they are general and thus limited
in the ability to produce good orders. Alternatively, we can try to build a program that
automatically acquires more specific knowledge based on ordering experience. In this section
we present such an algorithm.
The first step in building such a learning algorithm is deciding what knowledge we wish
to acquire from the ordering experience. The existing ordering algorithms demonstrate that
the precedence relation between variables is a key consideration for the order creation. The
graph search algorithms and the search-based graph evaluation algorithms try to place a
variable after the variables
that influence its next state value. Generally, a variable order
n
of n variables yields 2 precedence pairs. A precedence pair v i  vj denotes that variable
vi should precede vj in the variable order. For example, the variable order a, b, c, d yields
the precedence pairs a  b, a  c, a  d, b  c, b  d, c  d.
The above task of learning precedence pairs can be transformed into a concept learning
task. A concept learning task is defined by:
 A universe X over which the concept is learned;
 A concept C  a subset of items in X that we want to learn (usually marked by its
associated boolean characteristic function f c );
 A set of examples  pairs of the form hx, f c (x)i, where x  X;
 A set of features  functions above X that allow generalization.
For many learning tasks it is difficult to transform the problem to the format listed
above. It is already clear from the discussion above that the general concept we wish to
learn is the set of variable pairs in which the first should precede the second in the variable
ordering1 .
More precisely, we define the universe over which the concept is learned as the set of all
pairs h(vi , vj ), M i, where (vi ,vj ) is an ordered variable pair comprised of v i and vj , which
are variables in the model M . Since we expect that some pairs will have no preferred order,
we define a ternary instead of a binary concept. The ternary concept has the following
classes:
1. In practice, we will need only a small subset of the precedence pairs for constructing a total order.

93

fiGrumberg, Livne, & Markovitch

1. C+ , the class of all h(vi , vj ), M i for which it is preferable to place v i prior to vj in
order to get a good initial order.
2. C , the class of all h(vi , vj ), M i for which it is preferable to place v i after vj in order
to get a good initial order.
3. C? , the class of all h(vi , vj ), M i for which placing vi before vj is just as likely to lead
to a good variable order as placing v i after vj .
In the following subsections we describe the algorithms for learning and using this concept.
3.1 Algorithm Framework
We start with the description of the general framework of the learning algorithm. Our goal
is to find variable orders that yield BDDs with small number of nodes. Given a training
model, the algorithm first generates a set of orders of its variables. We define a utility
function u over variable orders as following. Each of the orders is used as the initial order
for building the BDD representation of the model 2 . This BDD (denoted M-BDD) includes
the models partitioned transition relation and its set of initial states. The utility u of a
generated order is then defined to be reversely proportional to the the number of nodes in
the M-BDD constructed with this order.
A subset that consists of all the variable pairs that appear together in some next-state
function is selected by the example extractor from all the possible variable pairs. We call
such pairs interacting variable pairs. For example, if next(x) = y  z then (y, z) is an
interacting variable pair. The example tagger tags each of the selected ordered pairs with
one of the classes C+ , C , or C? , based on the evaluated orders. The tagged pairs are
forwarded to the feature extractor which, based on the model, computes for each pair its
feature vector. The learner, which is an ID3 (Quinlan, 1986) decision tree generator, uses
the tagged feature vectors to create a pair precedence classifier.
Several training models are used in this manner to construct different pair precedence
classifiers. When solving a new unseen problem, these pair precedence classifiers are used
by the ordering algorithm to create a variable order.
The learning framework for creating a pair precedence classifier of a training model is
given in Figure 4. The complete data flow is displayed in Figure 5. The following subsections
describe in greater detail the components of the framework.
3.2 The Training Sequence Generator
The goal of the training sequence generator is to produce orders with high variance in quality
which is exploited by the tagger (see Subsection 3.4). The simplest strategy for generating
such sequences is by producing random orders. This is indeed the strategy we have used in
the experiments described in this paper. One potential problem with this approach is with
domains where good orders (or bad orders) are rare. In such a case, a random generator
will not necessarily produce sequences with the desired diversity in quality.
2. We use the SMV (McMillan, 1993) system for this purpose.

94

fiLearning to Order BDD Variables in Verification

Input : Training Model
Output : Precedence Classifier
1. Create sample orders.
2. Use SMV to evaluate the utility of each sample order by the M-BDD size.
3. Find the interacting variable pairs of the training model.
4. Based on the evaluated sampled orders, tag each ordered pair that is based on an
interacting variable pair.
5. Transform each tagged pair to a tagged feature vector.
6. Create a classifier based on the tagged feature vectors.
Figure 4: Training model precedence classifier construction

Training
Order
SMV

Training
Model

Evaluated
Orders

Tagged
Pairs

Example

Feature
Extractor

Tagger

M-BDD

Example

Interacting
Variable
Pairs

Learner

Extractor

Real
Model

Ordering
Algorithm

Classifier

Tagged
Feature
Vectors

Order

Figure 5: Data flow

An alternative approach is to actively try producing orders that are very good and orders
that are very bad, therefore creating a large diversity in quality. One way of producing a
good order is by taking the orders that are the result of the dynamic ordering process.
Another option is by using an existing static ordering algorithm. One interesting idea
is to try and bootstrap the process by using the results of the adaptive ordering algorithm
as training examples thus resulting in progressively more diverse input.
95

fiGrumberg, Livne, & Markovitch

3.3 The Example Extractor
Given a set of n variables, we can extract n  (n  1) example ordered pairs for training.
But should we actually use all these ordered pairs as examples?
There are two main reasons for being selective about what examples to use:
1. Each example carries computational costs associated with tagging, feature extraction,
and the added computation by the induction procedure.
2. Noisy examples are known to have harmful effect on the induction process.
The process of selecting a subset of examples, to be tagged, out of a set of untagged
examples is called selective sampling. There are two common ways of performing selective
sampling. One is by automatic methods that use various general metrics for selecting
informative examples (Lindenbaum, Markovitch, & Rusakov, 1999). The other way is by
using domain specific heuristics about the potential of an example to be informative.
In this work we use the second approach. Consider a function f over m variables,
represented within a BDD of n variables (where m  n). The number of nodes used to
represent f depends only on the relative order of the m variables. This means that changing
the order of the other n  m variables would not influence the BDD representation of the
function f .
The BDD representation of a model to be checked consists of the initial states of the
model and the next-state functions of the variables. Since the BDD representation for the
initial states is typically small, we do not take it into account. Therefore, when looking for
examples, we consider only the next-state functions. Usually, each such function is defined
only over a subset of all the model variables. Thus, the order of a pair of variables (v i , vj ),
that do not appear together in any next-state function is less likely to affect the quality of
the generated order. We therefore filter out such pairs.
3.4 The Example Tagger
An ordered variable pair (vi , vj ) should be tagged as belonging to C+ if it is preferable to
place vi before vj . Let V = {v1 , . . . , vn } be the set of variables of a given model. Let O be
the set of all possible orderings over V . Let O vi vj be the set of all o  O where vi precedes
vj . The ordered variable pair (vi , vj ) is defined to be preferable to (vj , vi ) if and only if
Average{u(o)|o  Ovj vi }  Average{u(o)|o  Ovi vj }.
Since it is not feasible to evaluate all the possible orders, we sample the space of possible
orders, evaluate them and partition the samples to two sets as above. As the averages now
only estimate the real averages, we replace the term smaller in the above definition
with significantly smaller. This is determined by the unpaired t-test, which tests the
significance (with a given confidence) of the difference between the averages of two samples
of two populations.
More precisely, for each variable pair v i , vj , the set of sampled orders S  O is partitioned into two subsets Svi vj  Ovi vj and Svj vi  Ovj vi . An unpaired t-test with
a predetermined confidence level is used to check if the averages of the set utilities differ
significantly. If they do, the ordered pair corresponding to the set with the smaller average
96

fiLearning to Order BDD Variables in Verification

is tagged with + and the other ordered pair is tagged with - (meaning that they belong
to C+ and C , respectively). Otherwise, the average difference is not significant, and both
ordered pairs are tagged with ? (meaning that they belong to C ? ).
A more elaborative approach could use the t-value as a weight on how important a particular order is. These weights could solve conflicts in the ordering process. Such a scheme
would require, however, a method to incorporate weights into the induction algorithm. One
method is by trying to induce a continues function instead of a ternary function.
3.5 The Feature Extractor
If we want to generalize from training models to future unseen models, we cannot represent
the pairs by the variable names. Rather, we should use a representation that can be used
across models. Most induction algorithms require that the examples be represented by
feature vectors.
The process of constructing an appropriate feature set is a crucial part of applying a
learning algorithm to a problem. It is a common knowledge engineering process where a
domain expert comes up with a set of features that might be relevant. It is the role of
the induction algorithm, then, to find out what combination of features are relevant to the
specific problem.
We have come up with a set of features over variable pairs. These features are extracted
from the model connectivity graph. Some of these attributes are inspired by traditional
static ordering algorithms. The attributes can be categorized into three groups:
 Variable attributes are defined on a single variable and try to capture its characteristics
in the model. One example is the variable-dependence attribute, which equals the
number of variables on which a variable depends. This attribute was inspired by the
value used by Butler et al. (1991) to guide the DFS search. A higher value indicates
that a larger portion of the models variables are needed to determine the variables
next-state value. Thus, a higher value may indicate that the variable location should
be lower in the order. Another example is the variable-dependency, which takes the
complementary view of variable-dependence. The attribute equals the number of
variables that depend on a given variable. A higher value may indicate that the
variable should be placed higher in the variable order.
 Symmetric pair attributes are defined on a variable pair v i , vj . These attributes try to
capture the strength of the bond between the two variables, as well as that between
this pair and the other variables in the model. For example, pair-minimal-distance
measures the shortest path between the variables in the model connectivity graph. A
shorter path can indicate a stronger bond between the variables. The distance-based
ordering framework (Chung et al., 1993) uses a similar feature to order variables.
Another example is pair-mutual-dependency, which counts the number of variables
whose next-state function depends on both v i and vj .
 Non-symmetric pair attributes try to capture the relationship between the two variables. For example, the pair-dependency-ratio is the ratio between the variabledependency values of the two variables. If the ratio is relatively high or low, it may
indicate the relative order of the two. pair-ns-distance evaluates the influence of one
97

fiGrumberg, Livne, & Markovitch

variable on the next state value of the other. It does so by measuring the distance
between the variables in the subgraph that represents the next-state function.
The complete list of attributes can be found in Appendix A.
3.6 The Induction Algorithm
After the feature extraction phase, our data is represented as a set of tagged feature vectors.
This type of representation can be used to produce classifiers by many induction algorithms,
including decision trees (Hunt, Marin, & Stone, 1966; Friedman, 1977; Quinlan, 1979;
Breiman, Frieman, Olshen, & Stone, 1984), neural networks (Widrow & Hoff, 1960; Parker,
1985; Rumelhart & McClelland, 1986) and nearest neighbor (Cover & Hart, 1967; Duda &
Hart, 1973). We have decided to use decision tree classifiers because of their relatively fast
learning and fast classification. Fast classification is especially important since we wish to
be competitive with other ordering algorithms and the number of variable pairs we need to
classify is large.
Decision trees have been researched thoroughly in the last decade, producing many
valuable extensions. One such extension enables the decision tree to give not only the
classification of items but also to associate with each such classification a confidence estimation. We have used this variant to allow conflict resolution. This will be described in
Section 3.7.3.
3.7 The Ordering Algorithm
The outcome of the learning process described in the last four subsections is a set of decision
trees, one for each training model.
We could also generate one tree based on the union of generated samples. One advantage
of the multiple-tree approach is that we expect the examples from the same model to be more
consistent, allowing generating compact trees. In contrast, a set of examples coming from
different models is likely to be more noisy, yielding a large tree. In addition, the multiple-tree
version allows us using a voting scheme during the ordering process, as described below.
Given a model M, the algorithm first extracts the interacting variable pairs. Each
of the classifiers is then applied to the feature vector representations of these pairs. For
each classifier, the classifications of all the pairs are gathered to form a precedence table.
These tables are then merged into one table. The order creation algorithm uses the merged
precedence table to construct the models variable order. The following subsections describe
the components in greater detail. Figure 6 shows the data flow in the ordering algorithm.
3.7.1 Building the Precedence Table
To build a precedence table based on a given classifier, the algorithm asks two questions for
each interacting variable pair vi , vj :
1. Should vi  vj ?
2. Should vj  vi ?
98

fiLearning to Order BDD Variables in Verification

Pair
Precedence
Classifier
Pair
Precedence

Real

Pair

Feature

Problem

Extractor

Extractor

Classifier

Pair

Pair

Tree

Table

Pair

Pair

Tree

Table

Merger

Merged
Pair
Precedence
Classifier

Pair

Pair

Tree

Table

Table

Order
Creation
Algorithm

Variable
Order

Figure 6: Ordering algorithm data flow

1
2
3
4
5
6
7
8
9

vi  v j ?
No
No
No
Yes
Yes
Yes
Unknown
Unknown
Unknown

vj  vi ?
No
Yes
Unknown
No
Yes
Unknown
No
Yes
Unknown

vi , vj order
Unknown
v j  vi
Unknown
v i  vj
Unknown
Unknown
Unknown
Unknown
Unknown

Table 2: Pair order table
If the two agree, the pair order is set to the agreed order. If they disagree, the order is
set to unknown. Table 2 summarizes all the possible answers for the two questions and the
resulting pair order.
3.7.2 The Merging Algorithm
After constructing the pair precedence tables from the training models classifiers, we merge
the tables using a voting scheme. For each variable pair v i , vj , we count the number of tables
that vote vi  vj and the number of tables that vote vj  vi . We then decide their pair
order according to the majority (ignoring the unknown votes).
Assuming that the majority vote chooses the order v i  vj , the confidence for this vote
conf (v v )conf (v v )
is computed by vote(vii vjj )+vote(vjjvii) , where vote(vi  vj ) is the number of tables that vote
99

fiGrumberg, Livne, & Markovitch

vi  vj and conf (vi  vj ) is the sum of the confidence values of these votes. vote(v j  vi )
and conf (vj  vi ) are defined similarly. If this value turns out to be lower than 0.1, we set
it to a minimal value of 0.1.
3.7.3 Cycle Resolution
In order to build a total, strict order out of the merged table, the table must not contain
any cycle. However, the above algorithm does not guarantee this. We therefore have to
apply a cycle resolution algorithm that makes the table cycle-free.
The precedence table can be seen as a directed graph in which the nodes are variables,
and there is a weighted edge vi  vj if and only if vi  vj . There are many possible ways to
eliminate cycles in a directed graph. One reasonable bias is removing the least number of
edges. This problem is known as the minimum feedback arc set and is proven to be NP-hard
(Karp, 1972). Approximation algorithms for this problem exist (Even, Naor, Schieber, &
Sudan, 1998), but they are too costly for our purposes.
We use instead a simple greedy algorithm to solve the problem. All the constraints
(edges) are gathered into a list and sorted in a decreasing order according to their weights
(i.e., their confidence). A graph is initialized to hold only the variable vertices. The list of
edges is then traversed and each edge is added if it does not close a cycle.
3.7.4 Pair Precedence Ordering
At this stage of the algorithm, we hold an acyclic merged precedence table. The last step of
the ordering process is to convert the partial order represented by this table to a total order.
This is done by topological ordering. At each stage, the algorithm finds all the minimal
variables, i.e., variables that are not constrained to follow other unordered variable. From
this set, we select a variable vadd with maximal fan-out and add it after the last ordered
variable. We then add all the variables which are larger than v add but do not appear in
any constraint with an unordered variable. We do this because it is desirable to place
interacting variables near each other. The pair precedence ordering (PPO) algorithm is
listed in Figure 7. Figure 8 lists the selection of v add in PPO .
One possible change to the ordering process is to delay the cycle resolution to the last
stage. We call this version cycle resolution on demand. The modified algorithm does not
perform any cycle resolution on the merged table. Instead, the algorithm works with the
merged table that may contain cycles. If the table contains a cycle, the algorithm must
reach a stage where not all the variables are ordered and there are no minimal variables.
In this case the algorithm performs cycle resolution as before and continues the ordering
process.
3.8 Experiments
We performed an empirical evaluation of the PPO algorithm using models from the
ISCAS89 (Brglez, Bryan, & Kozminski, 1989) benchmark. The ISCAS89 benchmark circuits
have been used to empirically evaluate many algorithms that deal with various aspects of
circuit design (Chamberlain, 1995; Wahba & Borrione, 1995; Nakamura, Takagi, Kimura,
& Watanabe, 1998; Long, Iyer, & Abramovici, 1995; Iyer & Abramovici, 1996; Konuk
& Larrabee, 1993). We discovered that some of the circuits are insensitive to the initial
100

fiLearning to Order BDD Variables in Verification

Input : The merged pair precedence table.
Output : A variable order.
Let V be the set of all variables.
Let before(v, V 0 ) = {v 0  V 0 |v  v 0 }.
Let after(v, V 0 ) = {v 0  V 0 |v 0  v}.

1. VC = {vi |bef ore(vi , V ) 6=  or af ter(vi , V ) 6= }
VN C = V  V C
2. While VC 6= 
(a) Vmin = {vi  VC |af ter(vi , VC ) = }
(b) vadd = argmaxvi Vmin |bef ore(vi , VC )|
(c) order = order || vadd

a

b

(d) VC = VC  {vadd }
(e) for each vi  VC
if af ter(vi , VC ) =  and bef ore(vi , VC ) =  then
order = order || vi , VC = VC  {vi }
3. Add VN C to end of variable list.
a. If more than one exists, select one.
b. Add variable to order.

Figure 7: Pair precedence ordering
ordering. This means that the entire sample of initial orders yielded model BDDs of similar
sizes. We eliminated these circuits from the set. Out of the remaining circuits we selected
those with a number of variables that SMV can handle. We ended up with the following
five circuits: s1269 (55), s1423 (91), s1512(86), s4863 (153), s6669 (314). The numbers in
parentheses stand for the number of variables in each model.
We began with an offline learning session where the three smaller models (s1269, s1423,
s1512) are used as training models. For each of these models we generated 200 random
orders and extracted examples as described in the previous section. The algorithm then
induced three precedence classifiers in the form of decision trees.
The number 200 was selected since it proved to be sufficiently large. In real application
the algorithm can be used as an anytime algorithm where training sequences are generated
as long as the user is willing to wait for the offline learner. An alternative approach would
keep aside a validation set that would be used for testing the systems performance. The
training could have then be stopped when the learning curve flattens.
The algorithm was tested on the two larger models (s4863, s6669). For each of the
models, the three learned decision trees were used to generate the merged precedence table.
101

fiGrumberg, Livne, & Markovitch

Pair
Merged
Table

Minimal
Elements
Unordered
Variables

Find
Minimal

Filter
Maximal

Maximal
Minimal
Elements

Selected
Element
Select
One

Variable
Order

Figure 8: Pair precedence ordering v add selection

Our PPO algorithm (with cycle resolution on demand) was compared to the random
algorithm. In addition, we compared our results to two advanced graph search algorithms
for static ordering: the DFS append algorithm of Fujita et al. (1988) and the interleave
algorithm of Fujii et al. (1993). In both algorithms we used the adaptation for multiple
starting points (Butler et al., 1991) and its expanded version, which includes the tie breaking
rule (Fujita et al., 1993). The random results were taken based on 200 variable orders. The
two other algorithms were each run 10 times on every model. The performance of the
ordering algorithms is measured by the number of nodes in the model BDDs (partitioned
transition relation and initial states).
Table 3 and Figure 9 show the results obtained. The table shows that on model s6669,
PPO outperformed the random order by more than 300%. On model s4863, PPO outperformed the random order by 5%.
PPO vs. Random

6

2.5

x 10

random
PPO

2

BDD nodes

1.5

1

0.5

0

s4863

Model

s6669

Figure 9: Comparative histogram of PPO vs. Random

102

fiLearning to Order BDD Variables in Verification

Model
s4863
s6669

Random
Average
STD
849197
121376
2030880 1744493

PPO
Average
STD
807763 100754
713950
35446

Table 3: Comparative table of PPO vs. Random
The comparison of our algorithm to the two static algorithms is given in Figure 10.
The results show that our learning algorithm, after training, becomes competitive with the
existing ordering algorithms written by experts.
5

10

x 10

Append
Interleave
PPO

9
8
7

BDD nodes

6
5
4
3
2
1
0

s4863

Model

s6669

Figure 10: Comparative histogram of PPO vs. Static
To evaluate the utility of the learned knowledge we would like to compare the performance of the ordering process with and without the learned knowledge. Ordering without
any learned knowledge is equivalent to random ordering. The comparison of our results to
the random-ordering algorithm reveals that the learner indeed induced meaningful knowledge during the learning process. Our method is also much more stable than random
ordering on s6669 as indicated by comparing the standard deviation. This large variance
in the results of the random ordering is indeed exploited by our tagging procedure as explained in Section 3.4. The small variance in the results obtained by random ordering on
s4863 can explain why the improvement obtained by the PPO algorithm is much smaller
on this circuit. A more sophisticated training sequence generator, such as those described
in Section 3.2, might have been more successful with that circuit.
The comparison to the hand-crafted algorithms may look disappointing at first look
since the results of the learning system are not better than the existing algorithms. Recall,
however, that we are comparing an automated learning process to human expertise. Most
of the works in empirical machine learning make comparisons between the performance of
various learning algorithms. It is not common to compare the performance of a learning
103

fiGrumberg, Livne, & Markovitch

algorithm with a human expert or an expert system since in most cases it is clear that handcrafted algorithms would outperform automated learning processes. Since there are hardly
any other learning systems that were built to solve the BDD variable ordering problem we
could not make the more common comparison between learning systems.

4. Learning Context-Based Precedence for Static Ordering
The precedence relation is one of the key considerations used by traditional static ordering
algorithms. Another key consideration is the clustering of variables and their subsequent
ordering. The algorithms try to place highly interacting variables near each other.
The effect of the variable clustering in a BDD can be seen in the simple example given in
Figure 3. In this function, switching the two variables v 2 and v3 increases the BDD size by
3 nodes. For this function, all the orders in which the variables of each of the two clusters,
v1 , v3 and v2, , v4 , are kept together yield the minimal BDD representation. Other variable
orders yield a less compact BDD. Thus, in this function, the only key consideration is the
compliance with clustering (precedence is not taken into account).
4.1 Variable Distance
The above discussion leads to the hypothesis that the distance between variables is an important factor when considering alternative orders. One way to obtain distance information
is by learning the distance function between pairs of variables. There are, however, two
problems with this approach:
1. The target distance function is not well-defined across models. For example, if we
train on small models, the absolute distance function is not likely to be applicable for
large models.
2. Information on absolute distances between variables is not sufficient to construct a
good ordering. This is because the absolute distance does not uniquely define the
order between the variables. In fact, it defines two possible orders, where one is the
reverse of the other.
The example in Figure 11 demonstrates that an order and its reverse can yield BDDs
that are significantly different in size. Each of the BDDs in Figure 11 represents two
functions, f1 (a, b, c, d, e) = (a = b = c)  (c = d) and f2 (a, b, c, d, e) = (a = b =
c)  (c = e). The absolute distance between the variables in the orders is clearly the
same. However, the upper BDD is approximately double the size of the lower one.
We wanted to check whether in realistic examples reverse orders can yield BDDs that
are significantly different in size. We tested models from the ISCAS89 benchmarks
and created 5,000 variable orders for each model. For each order, we compared its
quality with the quality of the reversed order. We found that in many cases one order
was exceptionally good while the reversed one was exceptionally bad. Thus, learning
the absolute distance is not sufficient, and more information is needed.
We conclude that there are problems inherent both in learning and in utilizing absolute
distances. Still, clustering is a key consideration and should be pursued. We suggest,
104

fiLearning to Order BDD Variables in Verification

f1

a

f2

b
c
d
e

1

1
0

1

1

1

1

0
0

1

1

0

(a)
f2

e
f1

d
c
b
a

1

1
0

0
0

1

1

0

(b)

Figure 11: ROBDDs for the functions f 1 (a, b, c, d, e) = (a = b = c)  (c = d) and
f2 (a, b, c, d, e) = (a = b = c)  (c = e)

alternatively, learning the relative distance that determines, for variables v i , vj , and vk ,
which of vj , vk should be closer to vi , given that vi precedes the other two.
The remainder of this section describes a method for learning and utilizing context-based
precedence to infer the relative distance between variables.
4.2 Context-Based Precedence
A context precedence relation is a triplet v i  vj  vk : given that vi precedes vj and vk , the
variable vj should come before the variable vk . Thus, the context precedence relation adds
context to pair ordering decisions.
As in pair precedence learning, we define the universe to be the set of pairs h(v i , vj , vk ), M i
where vi , vj , vk are variables in the model M . The universe is divided into three classes,
C+ , C , C? , as before. Examples for these classes are drawn in the same way. The pair
precedence framework can be applied with minor changes to work with context precedence
relations. These minor changes are described below.
4.3 The Example Tagger
A variable triplet (vi , vj , vk ) should be tagged as C+ if, given that vi precedes vj and vk , it is
preferable to place vj before vk (i.e., vi  vj  vk ). As in pair precedence learning, we use a
set of evaluated variable orders for the tagging. Any set of such orders can be partitioned to
three subsets, depending on which of the three variables is first. Given a partition defined
by vi (for example), we can test the order of v j and vk using t-test, as described in Section
105

fiGrumberg, Livne, & Markovitch

3.4. To reduce the number of noisy examples, we use only the partition that yields the most
significant t-test results.
4.4 The Feature Extractor
The attributes of a triplet (vi , vj , vk ) are computed based on the attributes of the two pairs
vi  vj and vi  vk . Each attribute value is the division/subtraction of two corresponding
attribute values from the two pair attributes.
More precisely, assume that the pair v i  vj has attributes f1 (vi , vj ), . . . , fn (vi , vj ) and
the pair vi  vk has attributes f1 (vi , vk ), . . . , fn (vi , vk ). Then the triple (vi , vj , vk ) has
attributes f1 (vi , vj )/f1 (vi , vk ), . . . , fn (vi , vj )/fn (vi , vk ). If some of the fl (vi , vk ) can be 0
then the corresponding attributes are subtracted instead of divided.
As an example consider an attribute f l which is pair minimal distance (see Section 3.5).
If fl (vi , vj )/fl (vi , vk ) is greater than 1 than the shortest path between v i and vj is larger
than the shortest path between vi and vk . This attribute can indicate that v k should appear
closer to vi .
Similarly, if fl is pair mutual dependency then fl (vi , vj )/fl (vi , vk ) > 1 indicates that the
number of variables whose next-state function depends on both v i and vj is greater than
those depending on both vi and vk . This may indicate that it is preferable to keep v i and
vj close together.
4.5 The Ordering Algorithm
The outcome of the learning phase is a set of decision trees, one for each model. This is the
same as in the case of context-free pairs. In this subsection we describe ways to use these
trees for ordering.
4.5.1 Building the Context Precedence Table
While in the case of pair precedence we had a table of size n 2 (where n is the number of
variables), we now produce one such table for each context variable. For each table we
perform inconsistency elimination similar to that described in Section 3.7.1. Here, however,
when we ask the classifier the two questions v j , vk and vk , vj , we add the context variable
vi to the query.
4.5.2 Pair Precedence Ordering with Context Precedence Filtering
The ordering algorithm uses the pair precedence table in the same way as the PPO algorithm. However, it was often found to be the case that the PPO algorithm had several
minimal variables, even after employing the maximal fanout filter. We use the contextbased precedence table to further reduce the size of the set of minimal elements. We use
the variables in the already ordered sequence as context variables and look at their associated tables. If the set of minimal elements contains a pair of variables constrained as
vj  vk in one of the tables, we eliminate vk from the set. Figure 12 lists the code which
when added to the PPO algorithm, accepts a variable set V add (from which we previously
selected randomly), and returns one variable. We call the new algorithm PPO CPF .
Figure 13 lists the selection of vadd in PPOCPF .
106

fiLearning to Order BDD Variables in Verification

Input : The set of candidate variables to be added, V add , and the merged context precedence
table.
Output : A variable to be added.
Let after(v, Vi , Vj ) = {hvi , vj i vi  Vi , vj  Vj |vi  vj  v}.
0
b.1 Vadd
= {vi  Vadd |af ter(vi , VInOrder , Vadd ) = }
0
b.2 If Vadd
6=  then
0
select randomly one variable from V add
else select randomly one variable from V add

Figure 12: Pair precedence ordering with context precedence filtering
Context
Merged
Table

Pair
Merged
Table

Minimal
Elements
Unordered
Variables

Find
Minimal

Filter
Maximal

Maximal
Minimal
Elements

Filter
Context
Constrained

Unconstrained
Minimal
Elements

Selected
Element
Select
One

Variable
Order

Figure 13: Pair precedence ordering with context precedence filtering v add selection

4.6 Experiments
We have evaluated the performance of PPO CPF , performing off-line learning on the
training models followed by ordering of the test models. The results are shown in Figure 14.
For comparison we also show the performance of the PPO algorithm and the two expert
algorithms.
The P P O CP F algorithm outperforms all the other algorithms on the two tested models.
The results show that the context-based precedence relations add valuable information.
We have tested the effect of the resources invested in the learning phase on the performance of the algorithms. Since the learning examples are tagged based on evaluated training
orders, and since the evaluation of the training orders is the most resource-consuming operation, we used the number of these orders as the resource estimator. Figure 15 shows the
learning curves of our algorithms, that is, it shows how the system performance changes
according to the offline resources consumed (the number of training orders evaluated).
Without testing any random order, our system has no knowledge on which to build
the precedence classifiers, and thus its performance is equivalent to random ordering. The
107

fiGrumberg, Livne, & Markovitch

5

10

x 10

Append
Interleave
PPO
CPF
PPO

9
8
7

BDD nodes

6
5
4
3
2
1
0

s4863

Model

s6669

Figure 14: Comparative histogram of ordering algorithms
6

2.2

x 10

s4863
s6669

2

1.8

bdd nodes

1.6

1.4

1.2

1

0.8

0.6
0

20

40

60

80
100
120
number of examples

140

160

180

200

Figure 15: Learning curves of the PPO CPF algorithm for the two testing models

tagging based on 20 orders is too noisy. While it improves the performance of s6669, it
degrades the performance of s4863. Forty orders are sufficient to generate stable tagging,
which yields improved classifiers and therefore improved ordering quality.

5. Discussion
The work described in this paper presents a general framework for using machine learning
methods to solve the static variable ordering problem. Our method assumes the availability
108

fiLearning to Order BDD Variables in Verification

of training models. For each training model, the learning algorithm generates a set of
random orders and evaluates them by building their associated BDDs. Each ordered pair
of interacting variables is then tagged as a good example if it appears more frequently
in highly valued orders. The ordered pairs are converted to feature-based representations
and are then given, with their associated tags, to an induction algorithm. When ordering
variables of a new unseen model, the resulting classifiers (one for each model) are used to
determine the ordering of variable pairs. We also present an extension of this method that
learns context-based ordering.
Our algorithm was empirically tested on real models. Its performance was significantly
better than random ordering, meaning that the algorithm was able to acquire useful ordering
knowledge. Our results were slightly better than existing static ordering algorithms handcrafted by experts. This result is significant if we compare it to applications of learning
systems to other domains. We would surely appreciate an induction algorithm that produces
a classifier with performance comparable to that of an expert system built by a medical
expert. A chess learning program that is able to learn an evaluation function that is
equivalent in power to a function produced by an expert will be similarly appreciated.
We therefore claim that the ability of or learning algorithm to achieve results that are as
good as manually designed algorithms indicates strong learning capabilities.
In most learning algorithms, we expect to get better performance when the testing
problems are similar to the training problems. In the verification domain, we expect to get
good results when the testing and training models come from a family of similar models.
There are several occasions in which models are similar enough to be considered a family:
Models of different versions of a design under development; models which are reduced
versions of a design, each with respect to a different property; models of designs with
a similar functionality like ALUs, arbiters, pipelines and bus controllers. Unfortunately,
due to the difficulty in obtaining suitable real models for our experiments, we ended up
experimenting with training and testing models that are not related. We expect to achieve
much better results for related models.
Compared with previous work in machine learning, our precedence relations most resemble these of Utgoff and Saxena (1987). Our ordering approach, in which we construct a
total order of elements by finding the precedence relation between them, is in essence the
same as that of Cohen, Schapire and Singer (1999). Specifically, the second ordering algorithm of Cohen, Schapire and Singer also uses the topological ordering approach to create
an order. Their algorithm initially finds in the precedence graph the connected components
and, after ordering them (using topological ordering), finds the order in each connected
component. However, since the quality of the final order is determined by the sum of constraints adhered to, all topological orders have theoretically the same quality. We found
that in the BDD variable ordering problem not all topological orders have the same quality.
Thus, we developed a topological ordering that takes into consideration those features that
we recognized as true for variable orders in BDDs.
Our work also differs from previous research in that it introduces the notion of contextbased precedence. Using this concept we were able to create an ordering algorithm that
produces the best results.
There are several directions for extending the work described here. One problem with
our current empirical evaluation is the small number of models. In spite of our extensive
109

fiGrumberg, Livne, & Markovitch

search efforts we were not able to find a large set of suitable examples. The majority of the
known examples are very simple (compared with real industry problems), producing small
model BDD representations with very little variance. We are currently in the process of
approaching companies that use model checking. In this way we hope to obtain additional
real models, preferably from families of the designs described above.
The attributes of the variable pairs were partially based on substantive research in the
field of static algorithms. We could not find such information on which to base contextbased variable attributes. Thus, we also based these attributes on those of the variable
pairs. Nevertheless, we believe that human experts in this field may have information that
can lead to the development of better attributes. The development of such attributes should
help to capture in a better way the context-based precedence concept.
Given our current results, an immediate question is whether the concept of precedence
pairs (context and non-context) can be extended to triplets, quadruples, etc. Such precedence relations take into account a larger part of the model and thus may possess valuable
information. Such an extension, however, could carry high cost during learning and, even
worse, during ordering.
Our framework for solving the static variable ordering problem was shown to be valuable
in model checking. Model checking is only one field of verification in which BDDs are used.
BDDs are also used in verification for simulation and equivalence checking. Our algorithm
can be applied for these problems as well. We are unaware of special static variable ordering
algorithms for these fields, but if such do exist, variable attributes based on these algorithms
should be added.
The most interesting future direction is the generalization of our framework for other
ordering problems. Ordering a set of objects is a very common sub-task in problem solving.
The most common approach for tackling such a problem is to evaluate each object using
a utility function and order the objects according to their utilities. Such an approach is
taken, for example, by most heuristic search algorithms. In many problems, however, it is
much easier to determine the relative order of two objects than to give each object a global
utility value. Few works have applied learning to ordering techniques that are not utility
based (Cohen et al., 1999). The algorithms described in Section 3 and Section 4 can be
applied to any ordering problem if a method for evaluating training orders is available, and
a set of meaningful pair features can be defined.
We believe that the research presented in this paper contributes both to the field of
machine learning and to the field of formal verification. For machine learning, it presents
a new methodology for learning to order elements. This methodology can be applied to
various kinds of ordering problems. For formal verification, it presents new learning-based
techniques for variable ordering. Finding good variable ordering techniques is one of the
key problems in this field.

Appendix A. Variable Pair Attributes
The following definitions and symbols will be used in the attribute description:
 N S(vi ) for the next state function of variable v i
 vi . vj to indicate that variable vi depends on variable vj s value (vj  NS(vi ))
110

fiLearning to Order BDD Variables in Verification

 vi ./ vj to indicate that variable vi interacts with variable vj (vi . vj and/or vj . vi )
 # variables for the number of variables in the model
A.1 Variable Attributes
The attributes computed for vi are
1. Variable-dependence: the number of variables upon which v i depends (|{vj |vi . vj }|)
2. Variable-dependency: the number of variables that depend on v i (|{vj |vj . vi }|)
3. Variable-dependency-size: the sum of function sizes that depend on v i (

P

vj .vi

|{vk  N S(vj )}|)

4. Variable-dependency-average-size:
the average function size dependent on v i
!
P
vj .vi

|{vk N S(vj )}|

|{vj |vj .vi }|

5. Variable-dependence-dependency-ratio: the proportion between the number of vari!
ables on which vi depends and the number of variables that depend on it

|{vj |vi .vj }|
|{vj |vj .vi }|

6. Variable-interaction: the number of variables interacting with v i (|{vj |vi ./ vj }|)
7. Variable-dependence-percentage:
the percentage of model variables on which v i de!
pends

|{vj |vi .vj }|
#variables

8. Variable-dependency-percentage:
the percentage of model variables that depend on v i
!
|{vj |vj .vi }|
#variables

9. Variable-interaction-percentage:
the percentage of model variables interacting with v i
!
|{vj |vi ./vj }|
#variables

A.2 Variable Pair Attributes
The attributes computed for hvi , vj i are
 Symmetric attributes
1. Pair-minimal-distance: the minimal distance between v i ,vj in the model graph
2. Pair-minimal-distance-eval: the minimal distance between v i ,vj in the model
graph divided by the number of times it appears
3. Pair-minimal-dependency: the number of variables that depend on the pair with
the minimal distance
4. Pair-minimal-dependency-eval: the minimal distance between v i ,vj in the model
graph divided by number of variables that depend on the minimal distance
111

fiGrumberg, Livne, & Markovitch

5. Pair-minimal-connection-class: the minimal distance between the v i ,vj connection class (the operators that can be applied on two variables were divided into
classes and the operator that connected the two variables in the minimal distance
class was extracted)
6. Pair-minimal-maximal: the maximal sized NS(v k ) connecting the pair in minimal distance
7. Pair-minimal-maximal-eval: the minimal distance between v i ,vj in the model
graph divided by maximal sized NS(v k ) connecting the pair in minimal distance
8. Pair-sum-distance: the sum of distances between v i ,vj in the model graph
9. Pair-dependency-ns-size: the sum of NS(v k ) sizes that are dependent on vi and
P
vj ( vk .vi & vk .vj |vl  N S(vk )|)

10. Pair-sum-distance-dependency-ratio: the sum of distances between v i ,vj in the
model graph divided by sum of NS(vk ) sizes that are dependent on vi and vj
11. Pair-mutual-dependence: the number of variables on which both v i ,vj depend
(|{vk |vi . vk & vj . vk }|)
12. Pair-mutual-dependency: the number of variables that depend on v i and vj
(|{vk |vk . vi & vk . vj }|)
13. Pair-mutual-interaction: the number of variables that interact with v i and vj
(|{vk |vi ./ vk & vi ./ vk }|)

14. Pair-mutual-ns-dependency: v i depends on vj and vj depends on vi - (vi . vj & vj . vi )
 Non-Symmetric attributes ( those computed for the pair hv i , vj i with relevance to vi )
1. Pair-ns-distance: the distance between v i ,vj in NS(vi )
2. Pair-dependence-ratio: the ratio between the number of variables !
that v i depends
on and the number of variables that v j depends on

|{vl |vi .vl }|
|{vm |vj .vm }|

3. Pair-dependency-ratio: the ratio between the number of variables
! that depend
on vi and the number of variable that depend on v j

|{vl |vl .vi }|
|{vm |vm .vj }|

4. Pair-interaction-ratio: the ratio between the number of variables that !interact
|{vl |vi ./vl }|
|{vm |vj ./vm }|

with vi and the number of variables that interact with v j

5. Pair-dependence-flag: the number of variables that v i depends on
! compared to
the number of variables that vj depends on

|{vl |vi .vl }|
|{vm |vj .vm }|

>= 1.0

6. Pair-interaction-flag: the number of variables that interact with v i compared
to
!
the number of variables that vj interacts with

112

|{vl |vi ./vl }|
|{vm |vj ./vm }|

>= 1.0

fiLearning to Order BDD Variables in Verification

References
Akers, S. (1978). Binary decision diagrams. IEEE Transactions on Computers, C-27 (6),
509516.
Aziz, A., Tasiran, S., & Brayton, R. (1994). BDD variable ordering for interacting finite
state machines. In Proceedings of the 31st Design Automation Conference (DAC), pp.
283288, San Diego, California.
Beer, I., Ben-David, S., Eisner, C., & Landver, A. (1996). RuleBase: An industry-oriented
formal verification tool. In Proceedings of the 33rd Design Automation Conference
(DAC), pp. 655660, Las Vegas, Nevada. IEEE Computer Society Press.
Bern, J., Meinel, C., & Slobodova, A. (1995). Efficient OBDD-based boolean manipulation in CAD beyond current limits. In Proceedings of the 32nd Design Automation
Conference (DAC), pp. 408413, San Francisco, California.
Bollig, B., Lobbing, M., & Wegener, I. (1995). Simulated annealing to improve variable orderings for OBDDs. In Proceedings of the International Workshop on Logic Synthesis,
pp. 5b:5.15.10, Granlibakken, California.
Bollig, B., & Wegener, I. (1996). Improving the variable ordering of OBDDs is NP-complete.
IEEE Transactions on Computers, 45 (9), 9931002.
Breiman, L., Frieman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and
Regression Trees. Wadsworth Publishing Company, Belmont, California, U.S.A.
Brglez, F., Bryan, D., & Kozminski, K. (1989). Combinational profiles of sequential benchmark circuits. In Proceedings of the International Symposium on Circuits and Systems,
pp. 19241934, Portland, Oregon.
Broos, P., & Branting, K. (1994). Compositional instance-based learning. In Proceedings
of the 12th National Conference on Artificial Intelligence, pp. 651656, Menlo Park,
California. AAAI Press.
Bryant, R. (1986). Graph-based algorithms for boolean function manipulation. IEEE Transactions on Computers, C-35 (8), 677691.
Butler, K. M., Ross, D. E., & Rohit Kapur, a. M. R. M. (1991). Heuristics to compute
variable orderings for efficient manipulation of ordered binary decision diagrams. In
Proceedings of the 28th Design Automation Conference (DAC), pp. 417420, San Francisco, California.
Chamberlain, R. (1995). Parallel logic simulation of VLSI systems. In Proceedings of the
32nd Design Automation Conference (DAC), pp. 139143, San Francisco, California.
Chung, P., Hajj, I., & Patel, J. (1993). Efficient variable ordering heuristics for shared
ROBDD. In Proceedings of the International Symposium on Circuits and Systems,
pp. 16901693, Chicago, Illinois.
Clarke, E. M., Emerson, F. A., & Sistla, A. P. (1986). Automatic verification of finite
state concurrent systems using temporal logic specifications. ACM Transactions on
Programming Languages and Systems, 8 (2), 244263.
113

fiGrumberg, Livne, & Markovitch

Cohen, W. W., Schapire, R. E., & Singer, Y. (1999). Learning to order things. Journal of
Artificial Intelligence Research, 10, 243270.
Cover, T. M., & Hart, P. E. (1967). Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13, 2127.
Drechsler, R., Becker, B., & Gockel, N. (1996). Genetic algorithm for variable ordering of
OBDDs. IEEE Proceedings on Computers and Digital Techniques, 143 (6), 364368.
Drechsler, R., Drechsler, N., & Slobodova, A. (1998). Fast exact minimization of BDDs.
In Proceedings of the 35th Design Automation Conference (DAC), pp. 200205, San
Francisco, California.
Duda, R. O., & Hart, P. E. (1973). Pattern Classification and Scene Analysis. John Wiley
and Sons, New York.
Even, G., Naor, J., Schieber, B., & Sudan, M. (1998). Approximating minimum feedback
sets and multi-cuts in directed graphs. Algorithmica, 20, 151174.
Friedman, J. (1977). A recursive partitioning decision rule for nonparametric classification.
IEEE Transactions on Computers, C-26 (4), 404408.
Friedman, S. J., & Supowit, K. J. (1987). Finding the optimal variable ordering for binary
decision diagrams. In Proceedings of the 24th Design Automation Conference (DAC),
pp. 151174, Miami Beach, Florida.
Fujii, H., Ootomo, G., & Hori, C. (1993). Interleaving based variable ordering methods
for ordered binary decision diagrams. In Proceedings of the IEEE/ACM international
conference on Computer-aided design, pp. 3841, Santa Clara, California.
Fujita, M., Fujisawa, H., & Kawato, N. (1988). Evaluation and improvements of boolean
comparison method based on binary decision diagrams. In Proceedings of the International Conference on Computer-Aided Design, pp. 25, Santa Clara, California.
Fujita, M., Fujisawa, H., & Matsunaga, Y. (1993). Variable ordering algorithms for ordered
binary decision diagrams and their evaluation. IEEE Transactions on Computer-Aided
Design of Integrated Circuits and Systems, 12 (1), 612.
Fujita, M., Kukimoto, Y., & Brayton, R. (1995). BDD minimization by truth table permutations. In Proceedings of the International Workshop on Logic Synthesis, pp. 596599,
Lake Tahoe, California.
Hunt, E., Marin, J., & Stone, P. (1966). Experiments in Induction. Academic Press, New
York.
Ishiura, N., Sawada, H., & Yajima, S. (1991). Minimization of binary decision diagrams
based on exchanges of variables. In Proceedings of the International Conference on
Computer-Aided Design, pp. 472475, Santa Clara, California.
Iyer, M., & Abramovici, M. (1996). FIRE: A fault-independent combinational redundancy
identification algorithm. IEEE Transactions on VLSI Systems, 4, 295301.
Jain, J., Adams, W., & Fujita, M. (1998). Sampling schemes for computing variable orderings. In Proceedings of the International Conference on Computer-Aided Design, pp.
631638, San Jose, California.
114

fiLearning to Order BDD Variables in Verification

Karp, R. M. (1972). Reducibility among combinatorial problems. In Miller, R., & Thatcher,
J. (Eds.), Complexity of Computer Computations, pp. 85103, New York. Plenum
Press.
Kaufmann, M., & Pixley, C. (1997). Intertwined development and formal verification of a
60x bus model. In Proceedings of the International Conference on Computer Design:
VLSI in Computers and Processors (ICCD 97), pp. 2530, Austin, Texas.
Konuk, H., & Larrabee, R. (1993). Explorations of sequential atpg using boolean satisfiability. In Proceedings of the 11th IEEE VLSI Test Symposium, pp. 8590.
Lindenbaum, M., Markovitch, S., & Rusakov, D. (1999). Selective sampling for nearest
neighbor classifiers. In Proceedings of the Sixteenth national confernce on Artificial
Intelligence, pp. 366371, Orlando, Florida.
Long, D., Iyer, M., & Abramovici, M. (1995). Identifying sequentially untestable faults
using illegal states. In Proceedings of the 13th IEEE VLSI Test Symposium, pp. 411,
Los Alamitos, California.
Malik, S., Wang, A., Brayton, R., & Sangiovanni-Vincentelli, A. (1988). Logic verification
using binary decision diagrams in a logic synthesis environment. In Proceedings of the
International Conference on Computer-Aided Design, pp. 69, Santa Clara, California.
McMillan, K. (1993). Symbolic Model Checking: An Approach to the State Explosion Problem. Kluwer Academic Publisher.
Meinel, C., & Slobodova, A. (1997). Speeding up variable ordering of OBDDs. In Proceedings of the International Conference on Computer-Aided Design, pp. 338343, Austin,
Texas.
Meinel, C., & Slobodova, A. (1998). Sample method for minimization of OBDDs. In Proceedings of the Conference on Current Trends in Theory and Practice of Informatics,
Vol. 1521 of Lecture Notes in Computer Science, pp. 419428. Springer-Verlag, New
York.
Meinel, C., Somenzi, F., & Theobald, T. (1997). Linear sifting of decison diagrams. In
Proceedings of the 34th Design Automation Conference (DAC), pp. 202207, Anaheim,
California.
Mercer, M. R., Kapur, R., & Ross, D. E. (1992). Functional approaches to generating
orderings for efficient symbolic representations. In Proceedings of the 29th Design
Automation Conference (DAC).
Minato, S., Ishiura, N., & Yajima, S. (1990). Shared binary decision diagrams with attributed edges for efficient boolean function manipulation. In Proceedings of the 27th
Design Automation Conference (DAC), pp. 5257, Orlando, Florida.
Nakamura, K., Takagi, K., Kimura, S., & Watanabe, K. (1998). Waiting false path analysis
of sequential logic circuits for performance optimization. In Proceedings of the International Conference on Computer-Aided Design, pp. 392395, San Jose, California.
Panda, S., & Somenzi, F. (1995). Who are the variables in your neighbourhood. In Proceedings of the International Conference on Computer-Aided Design, pp. 7477, San
Jose, California.
115

fiGrumberg, Livne, & Markovitch

Panda, S., Somenzi, F., & Plessier, B. F. (1994). Symmetry detection and dynamic variable
ordering of decision diagrams. In Proceedings of the International Conference on
Computer-Aided Design, pp. 628631, San Jose, California.
Parker, D. B. (1985). Learning logic. Tech. rep. TR-47, Center for Computational Research
in Economics and Management Science, MIT, Cambridge, MA.
Queille, J., & Sifakis, J. (1981). Specification and verification of concurrent systems in
cesar. In Dezani-Ciancaglini, M., & Montanari, U. (Eds.), Proceedings of the 5th
International Symposium on Programming, Vol. 137 of Lecture Notes in Computer
Science, pp. 337351. Springer-Verlag, New York.
Quinlan, J. R. (1979). Discovering rules by induction from large collections of examples.
In Expert Systems in the Micro Electronic Age, pp. 168201. Edinburgh University
Press.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1 (1), 81106.
Rudell, R. (1993). Dynamic variable ordering for ordered binary decision diagrams. In
Proceedings of the International Conference on Computer-Aided Design, pp. 4247,
Santa Clara, California.
Rumelhart, D. E., & McClelland, J. L. (1986). Parallel distibuted processing: Exploration
in the microstructure of cognition.. Vol. 1,2. MIT Press.
Touati, H., Savoj, H., Lin, B., Brayton, R., & Sangiovanni-Vincetelli, A. (1990). Implicit
state enumeration of finite state machines using BDDs. In Proceedings of the International Conference on Computer-Aided Design, pp. 130133, Santa Clara, California.
Utgoff, P., & Clouse, J. (1991). Two kinds of training information for evaluation function
learning. In Proceedings of the Ninth National Conference on Artificial Intelligence,
pp. 596600, Anaheim, California.
Utgoff, P. E., & Saxena, S. (1987). Learning a preference predicate. In Proceedings of the
Fourth International Workshop on Machine Learning, pp. 115121, Irvine, California.
Wahba, A., & Borrione, D. (1995). Design error diagnosis in sequential circuits. Lecture
Notes in Computer Science, 987, 171188.
Widrow, B., & Hoff, M. E. (1960). Adaptive switching circuits. In 1960 IRE WESCON
Convention Record, pp. 96104, New York.
Zhuang, N., Benten, M., & Cheung, P. (1996). Improved variable ordering of BDDs with
novel genetic algorithm. In Proceedings of the International Symposium on Circuits
and Systems., Vol. 3, pp. 414417, Atlanta, Georgia.

116

fiJournal of Artificial Intelligence Research 18 (2003) 183-215

Submitted 9/01; published 2/03

An Evolutionary Algorithm with Advanced Goal and Priority
Specification for Multi-objective Optimization
Kay Chen Tan
Eik Fun Khor
Tong Heng Lee
Ramasubramanian Sathikannan

ELETANKC@NUS.EDU.SG
EIKFUN.KHOR@SEAGATE.COM
ELELEETH@NUS.EDU.SG
K.SATHI@GSK.COM

National University of Singapore
4 Engineering Drive 3, Singapore 117576
Republic of Singapore

Abstract
This paper presents an evolutionary algorithm with a new goal-sequence domination scheme for
better decision support in multi-objective optimization. The approach allows the inclusion of
advanced hard/soft priority and constraint information on each objective component, and is capable
of incorporating multiple specifications with overlapping or non-overlapping objective functions via
logical OR and AND connectives to drive the search towards multiple regions of trade-off. In
addition, we propose a dynamic sharing scheme that is simple and adaptively estimated according to
the on-line population distribution without needing any a priori parameter setting. Each feature in the
proposed algorithm is examined to show its respective contribution, and the performance of the
algorithm is compared with other evolutionary optimization methods. It is shown that the proposed
algorithm has performed well in the diversity of evolutionary search and uniform distribution of
non-dominated individuals along the final trade-offs, without significant computational effort. The
algorithm is also applied to the design optimization of a practical servo control system for hard disk
drives with a single voice-coil-motor actuator. Results of the evolutionary designed servo control
system show a superior closed-loop performance compared to classical PID or RPT approaches.

1. Introduction
Many real-world design tasks involve optimizing a vector of objective functions on a feasible
decision variable space. These objective functions are often non-commensurable and in
competition with each other, and cannot be simply aggregated into a scalar function for
optimization. This type of problem is known as multi-objective (MO) optimization problem, for
which the solution is a family of points known as a Pareto-optimal set (Goldberg, 1989), where
each objective component of any member in the set can only be improved by degrading at least one
of its other objective components. To obtain a good solution via conventional MO optimization
techniques such as the methods of inequalities, goal attainment or weighted sum approach, a
continuous cost function and/or a set of precise settings of weights or goals are required, which are
usually not well manageable or understood (Grace, 1992; Osyczka, 1984).
Emulating the Darwinian-Wallace principle of survival-of-the-fittest in natural selection and
genetics, evolutionary algorithms (EAs) (Holland, 1975) have been found to be effective and
efficient in solving complex problems where conventional optimization tools fail to work well.
2003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiTAN, KHOR, LEE, & SATHIKANNAN

The EAs evaluate performances of candidate solutions at multiple points simultaneously, and are
capable of approaching the global optimum in a noisy, poorly understood and/or non-differentiable
search space (Goldberg, 1989).
Since Schaffers work (1985), evolutionary algorithm-based search techniques for MO
optimization have been gaining significant attention from researchers in various disciplines. This
is reflected by the high volume of publications in this topic in the last few years as well as the first
international conference on Evolutionary Multi-criteria Optimization (EMO01) held in March
2001 at Zurich, Switzerland. Readers may refer to (Coello Coello, 1996; 1999; Deb, 2001;
Fonseca, 1995; Van Veldhuizen & Lamont, 1998; Zitzler & Thiele, 1999) on detailed
implementation of various evolutionary techniques for MO optimization.
Unlike most conventional methods that linearly combine multiple attributes to form a composite
scalar objective function, the concept of Pareto's optimality or modified selection scheme is
incorporated in an evolutionary MO optimization to evolve a family of solutions at multiple points
along the trade-off surface simultaneously (Fonseca & Fleming, 1993). Among various selection
techniques for evolutionary MO optimization, the Pareto-dominance scheme (Goldberg, 1989)
that assigns equal rank to all non-dominated individuals is an effective approach for comparing the
strengths among different candidate solutions in a population (Fonseca & Fleming, 1993). Starting
from this principle, Fonseca and Fleming (1993) proposed a Pareto-based ranking scheme to
include goal and priority information for MO optimization. The underlying reason is that certain
user knowledge may be available for an optimization problem, such as preferences and/or goals to
be achieved for certain objective components. This information could be useful and incorporated
by means of goal and priority vectors, which simplify the optimization process and allow the
evolution to be directed towards certain concentrated regions of the trade-offs. Although the
ranking scheme is a good approach, it only works for a single goal and priority vector setting,
which may be difficult to define accurately prior to an optimization process for real-world
optimization problems. Moreover, the scheme does not allow advanced specifications, such as
logical AND and OR operations among multiple goals and priorities.
Based on the Pareto-based domination approach, this paper reformulates the domination scheme
to incorporate advanced specifications for better decision support in MO optimization. Besides the
flexibility of incorporating goal and priority information on each objective component, the
proposed domination scheme allows the inclusion of hard/soft priority and constraint
specifications. In addition, the approach is capable of incorporating multiple specifications with
overlapping or non-overlapping objective functions via logical OR and AND connectives to
drive the search towards multiple regions of the trade-off. The paper also proposes a dynamic
sharing scheme, which computes the sharing distance adaptively based upon the on-line
population distribution in the objective domain without the need of any a priori parameter setting.
The dynamic sharing approach is essential since it eliminates the difficulty of manually finding an
appropriate sharing distance prior to an optimization process. The choice of such a distance would
be sensitive to the size and geometry of the discovered trade-offs (Coello Coello, 1999; Fonseca &
Fleming, 1993).
This paper is organized as follows: The formulation of the proposed domination scheme for
better decision support is presented in Section 2. A dynamic sharing scheme that estimates the
184

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

sharing distance adaptively based upon the on-line population distribution is described in Section 3.
Section 4 examines the usefulness and contribution of each proposed feature in the algorithm. The
performance comparison of the proposed algorithm with other evolutionary MO optimization
methods is also shown in the section. Practical application of the proposed algorithm to servo
control system design optimization is given in Section 5. Conclusions are drawn in Section 6.

2. Advanced Goal and Priority Specifications for MO Optimization
A multi-objective optimization problem seeks to optimize a vector of non-commensurable and
often competing objectives, i.e., it tends to find a parameter set P for Min F ( P ) , P  R n , where P
P

= {p1, p2,, pn} is a n-dimensional vector having n decision variables or parameters, and  defines
a feasible set of P. F = {f1, f2,, fm} is an objective vector with m objectives to be minimized. For
a MO optimization problem with simple goal or priority specification on each objective function,
the Pareto-based ranking scheme is sufficient (Fonseca & Fleming, 1993). In practice, however, it
may be difficult to define an accurate goal and priority setting in a priori to an optimization process
for real-world optimization problems. Besides goal and priority information, there could also be
additional supporting specifications that are useful or need to be satisfied in the evolutionary search,
such as optimization constraints or feasibility of a solution. Moreover, the Pareto-based ranking
scheme does not allow advanced specifications, such as logical AND and OR operations
among multiple goals and priorities for better decision support in complex MO optimization. In
this section, a new goal-sequence Pareto-based domination scheme is proposed to address these
issues and to provide hard/soft goal and priority specifications for better controls in the
evolutionary optimization process.
2.1 Pareto-based Domination with Goal Information

This section is about an effective two-stage Pareto-based domination scheme for MO optimization,
which is then extended to incorporate advanced soft/hard goal and priority specifications. Consider
a minimization problem. An objective vector Fa is said to dominate another objective vector Fb
based on the idea of Pareto dominance, denoted by Fa  Fb, iff f a ,i  f b,i  i  {1,2,..., m} and
f a , j < f b, j for some j  {1,2,..., m} . Adopting this basic principle of Pareto dominance, the first

stage in the proposed domination approach ranks all individuals that satisfy the goal setting to
minimize the objective functions as much as possible. It assigns the same smallest cost for all
non-dominated individuals, while the dominated individuals are ranked according to how many
individuals in the population dominate them. The second stage ranks the remaining individuals that
do not meet the goal setting based upon the following extended domination scheme. Let Fa
Fb

)
a

)
a

and

denote the component of vector Fa and Fb respectively, in which Fa does not meet the
185

fiTAN, KHOR, LEE, & SATHIKANNAN

goal G . Then for both Fa and Fb that do not totally satisfy the goal G , the vector Fa is said
to dominate vector Fb (denoted by Fa  Fb ) iff
G

)
a

)
a

( Fa  Fb ) or (abs( Fa -G)  abs( Fb -G))

(1)

For this, the rank begins from one increment of the maximum rank value obtained in the first
stage of the cost assignment. Therefore individuals that do not meet the goal will be directed toward
the goal and the infinum in the objective domain, while those that have satisfied the goal will only
be directed further towards the infinum. Note that the domination comparison operator is
non-commutative ( Fa  Fb  Fb  Fa ). Figure 1 shows an optimization problem with two
G

G

objectives f1 and f2 to be minimized. The arrows in Figure 1 indicate the transformation according
to F' = F  G of the objective function F to F' for individuals that do not satisfy the goal, with
the goal as the new reference point in the transformed objective domain. It is obvious that the
domination scheme is simple and efficient for comparing the strengths among partially or totally
unsatisfactory individuals in a population. For comparisons among totally satisfactory individuals,
the basic Pareto-dominance is sufficient.
To study the computational efficiency in the approach, the population is divided into two
separate groups classified by the goal satisfaction, and the domination comparison is performed
separately in each group of individuals. The total number of domination comparisons for the
two-stage domination scheme is Nc = [ nG( ( nG( -1)+ nG) ( nG) -1)] where nG( is the number of
individuals that completely satisfy the goal G and nG) is the number of individuals partially satisfy
or completely not satisfy the goal G. Note that nG( + nG) = N for a population size of N. Hence, in
any generation, Nc is always less than or equal to the total number of domination comparisons
among all individuals in a population (each individual in the population is compared with (N-1)
individuals), i.e., Nc  Nnc = N ( N  1) . In the next section, the two-stage Pareto-based
domination scheme will be extended to incorporate soft/hard priority specifications for advanced
MO optimization.

186

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

f2

6
6

6

5

5

5

G
4
1

6
2
1

6

f1

Figure 1: Advanced Pareto Domination Scheme with Goal Information
2.2 Goal-Sequence Domination Scheme with Soft/Hard Priority Specifications

One of the advanced capabilities in evolutionary MO optimization is to incorporate cognitive
specification, such as priority information that indicates the relative importance of the multiple
tasks to provide useful guidance in the optimization. Consider a problem with multiple
non-commensurable tasks, where each task is assigned a qualitative form of priority indicating its
relative importance. In general, there exist two alternatives to accomplish these tasks, i.e., to
consider one task at a time in a sequence according to the task priority or to accomplish all tasks at
once before considering any individual task according to the task priority. Intuitively, the former
approach provides good optimization performance for tasks with higher priority and may result in
relatively poor performance for others. This is due to the fact that optimizing the higher priority
tasks may be at the performance expense of the lower priority tasks. This definition of priority is
denoted as hard priority in this paper. On the other hand, the latter approach provides a
distributed approach in which all tasks aim at a compromise solution before the importance or
priority of individual task is considered. This is defined as "soft" priority. Similarly, priorities for
different objective components in MO optimization can be classified as "hard" or "soft" priority.
With hard priorities, goal settings (if applicable) for higher priority objective components must be
satisfied first before attaining goals with lower priority. In contrast, soft priorities will first optimize
the overall performance of all objective components, as much as possible, before attaining any goal
setting of an individual objective component in a sequence according to the priority vector.
To achieve greater flexibility in MO optimization, the two-stage Pareto-based domination
scheme is further extended to incorporate both soft and hard priority specifications with or without
goal information by means of a new goal-sequence domination. Here, instead of having one priority
vector to indicate priorities among the multiple objective components (Fonseca & Fleming, 1998),
two kinds of priority vectors are used to accommodate the soft/hard priority information. Consider
an objective priority vector, Pf  1xm and a goal priority vector, Pg  1xm, where Pf(i) represents
the priority for the ith objective component F(i) that is to be minimized; Pg(i) denotes the priority for
187

fiTAN, KHOR, LEE, & SATHIKANNAN

the ith goal component G(i) that is to be attained; m is the number of objectives to be minimized and
 denotes the natural numbers. The elements of the vector Pf and Pg can take any value in the
natural numbers, with a lower number representing a higher priority and zero representing a dont
care priority assignment. Note that repeated values among the elements in Pf and Pg can be used to
indicate equal priority provided that Pf(i)  Pg(i)  i  {1, 2, , m}, avoiding contradiction of the
priority assignment. With the combination of an objective priority vector Pf and a goal priority
vector Pg, soft and hard priorities can be defined provided that there is more than one preference
among the objective components as given by
 {(Pf : Pf (j) > 1)  (Pg : Pg (j) > 1)} for some j  {1, 2, , m}
(2)
Based on this, a priority setting is regarded as soft iff
 i  {1, 2, , m}  {(Pf : Pf (i) = 1)  (Pg : Pg (i) = 1)}
(3)
else, the priority is denoted as hard.
For example, the settings of Pf = [1, 1, 2, 2] and Pg = [0, 0, 0, 0] for a 4-objective optimization
problem indicate that the first and second objective components are given top priority to be
minimized, as much as possible, before considering minimization of the third and fourth objective
components. Since all elements in Pg are zeros (dont care), no goal components will be considered
in the minimization in this case. On the other hand, the setting of Pf = [0, 0, 0, 0] and Pg = [1, 1, 2, 2]
imply that the first and second objective components are given the first priority to meet their
respective goal components before considering the goal attainment for the third and fourth
objective components. The above two different priority settings are all categorized as hard
priorities since in both cases, objective components with higher priority are minimized before
considering objective components with lower priority. For soft priority as defined in Eq. 3, the
objective priority vector and goal priority vector can be set as Pg = [1, 1, 1, 1] and Pf = [2, 2, 3, 3],
respectively. This implies that the evolution is directed towards minimizing all of the objective
components to the goal region before any attempt to minimize the higher priority objective
components in a sequence defined by the priority vector.
To systematically rank all individuals in a population to incorporate the soft/hard priority
specifications, a sequence of goals corresponding to the priority information can be generated and
represented by a goal-sequence matrix G where the kth row in the matrix represents the goal vector
for the corresponding kth priority. The number of goal vectors to be generated depends on the last
level of priority z, where z is the maximum value of any one element of Pg and Pf as given by
z = max[Pg(i), Pf(j)]

 i, j  {1,2,..., m}

(4)

For this, the goal vectors with kth priority in the goal-sequence matrix Gk(i) for the priority index k
= 1, 2,, z is defined as

G (i )

i = 1,..., m, G k (i ) =  min[F j =1,..., N (i )]
max[F
j =1,..., N (i )]


if Pg (i ) = k
if P f (i ) = k

(5)

otherwise

where N denotes the population size; min[F j =1,..., N (i )] and max[F j =1,..., N (i )] represents the
minimum and maximum value of the ith objective function from the on-line population distribution,
188

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

respectively. In Eq. 5, for any ith objective component of any k priority level, the reason for
assigning Gk(i) with G(i) is to guide the individuals towards the goal regions; min[F j =1,..., N (i )] is
to minimize the corresponding objective component as much as possible; and max[F j =1,..., N (i )] is
to relax the requirements on the individuals to give other objective components more room for
improvement. According to Eq. 5, the goal-sequence matrix Gk(i) is dynamic at each generation, as
the values of min[F j =1,..., N (i )] and max[F j =1,..., N (i )] are dynamically computed depending on the
on-line population distribution. After computing the sequence of goals Gk  k  {1, 2,, z}, the
individuals are first ranked according to the computed goal G1 for the first priority. Then each
group of individuals that has the same ranks will be further compared and ranked according to next
goal G2 for the second priority to further evaluate the individuals' domination in a population. In
general, this ranking process continues until there is no individual with the same rank value or after
ranking the goal Gz that has the lowest priority in the goal-sequence matrix. Note that individuals
with the same rank value will not be further evaluated for those components with dont care
assignments.
With the proposed goal-sequence domination scheme as given in Eq. 5, both hard and soft
priority specifications can be incorporated in MO optimization. Without loss of generality, consider
a two-objective optimization problem, with f1 having a higher priority than f2, as well as a goal
setting of G = [g1, g2]. For soft priority optimization as defined in Eq. 3, the goal priority vector and
objective priority vector can be set as Pg = [1, 1] and Pf = [2, 0], respectively. Let min[F(i)] and
max[F(i)] denote the minimum and maximum value of the i-objective component of F in a
population, respectively. The relevant goals in the goal-sequence matrix for each priority level as
defined in Eq. 5 are then given as G1 = G for the first priority and G2 = {min[F(1)], max[F(2)]}
for the second priority. The goal-sequence domination scheme for the two-objective minimization
problem is illustrated in Figure 2. Here, the rank value of each individual is denoted by r1  r2,
where r1 and r2 is the rank value after the goal-sequence ranking of the first and second priority,
respectively. The preference setting indicates that both g1 and g2 are given the same priority to be
attained in the optimization before individuals are further ranked according to the higher priority of
f1. This is illustrated in Figure 3a, which shows the location of the desired Pareto-front (represented
by the dark region) and the expected evolution direction (represented by the curved arrow) in the
objective domain for an example with an unfeasible goal setting G.
For hard priority optimization as defined in Eqs. 2 and 3, the goal priority vector and objective
priority vector can be set as Pg = [1, 2] and Pf = [0, 0], respectively. According to Eq. 5, this gives a
goal sequence of G1 = [g1, max[F(2)] and G2 = [max[F(1)], g2] for the first and second priority,
respectively. It implies that g1 is given higher priority than g2 to be attained in the optimization.
Figure 3b shows the location of the desired Pareto-front (represented by dark region) and the
expected evolution direction (represented by curved arrow) in the objective domain. As compared
to the solutions obtained in soft priority optimization, hard priority optimization attempts to attain
the first goal component and leads to the solution with better f1 (higher priority) but worse f2 (lower
priority). It should be mentioned that the setting of soft/hard priority may be subjective or problem
189

fiTAN, KHOR, LEE, & SATHIKANNAN

dependent in practice. In general, the hard priority optimization may be appropriate for problems
with well-defined goals in order to avoid stagnation with unfeasible goal settings. Soft priority
optimization is more suitable for applications where moderate performance among various
objective components is desired. Besides soft/hard priority information, there may be additional
specifications such as optimization constraints that are required to be satisfied in the optimization.
These specifications could be easily incorporated in MO optimization by formulating the
constraints as additional objective components to be optimized (Fonseca & Fleming, 1998). This
will be discussed in the next section.
f2
G'2

77
55
56

G'1

g2

4 4
11

78
23
12

g1

f1

Figure 2: Goal-sequence Domination with Goal G = {g1, g2}, Priority Pg = [1, 1] and Pf = [2, 0]

f2

f2

G2'

G '1

max[F(2)]

tinon

o
Evoluti

luo
EEvvooluti

n

desired solution
desired solution

g2

G
G '1

g2

Unfeasible
Region

G

G'2

Unfeasible
Region

g1

g1

f1

max[F(1)]

f1

(a) Soft priority f1 higher than f2
(b) Hard priority f1 higher than f2
Figure 3: Illustration of Soft and Hard Priority with Unfeasible Goal Setting
2.3 Optimization with Soft and Hard Constraints

Constraints often exist in practical optimization problems (Luus et al. 1995; Michalewicz &
Schoenauer, 1996). These constraints are often incorporated in the MO optimization function as
190

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

one of the objective components to be optimized. It could be in the form of "hard" constraint where
the optimization is directed towards attaining a threshold or goal, and further optimization is
meaningless or not desirable whenever the goal has been satisfied. In contrast, a "soft" constraint
requires that the value of objective component corresponding to the constraint is optimized as much
as possible. An easy approach to deal with both hard and soft constraints concurrently in
evolutionary MO optimization is given here. At each generation, an updated objective function Fx#
concerning both hard and soft constraints for an individual x with its objective function Fx can be
computed in a priori to the goal-sequence domination scheme as given by
 G (i ) if [G (i) is hard] & [F x (i) < G (i )]
#
F x (i) = 
otherwise
F x (i )

i = {1,..., m}

(6)

In Eq. 6, any objective component i that corresponds to a hard constraint is assigned to the value
of G(i) whenever the hard constraint has been satisfied. The underlying reason is that there is no
ranking preference for any particular objective component that has the same value in an
evolutionary optimization process, and thus the evolution will only be directed towards optimizing
soft constraints and any unattained hard constraints, as desired.
2.4 Logical Connectives among Goal and Priority Specifications

For MO optimization problems with a single goal or priority specification, the decision maker often
needs to guess an appropriate initial goal or priority vector and then manually observe the
optimization progress. If any of the goal components is too stringent or too generous, the goal
setting will have to be adjusted accordingly until a satisfactory solution can be obtained. This
approach obviously requires extensive human observation and intervention, which can be tedious
or inefficient in practice. Marcu (1997) proposed a method of adapting the goal values based upon
the on-line population distribution at every generation. However, the adaptation of goal values is
formulated in such a way that the search is always uniformly directed towards the middle region of
the trade-offs. This restriction may be undesirable for many applications, where the trade-off
surface is unknown or the search needs to be directed in any direction other than the middle region
of the trade-off surface. To reduce human interaction and to allow multiple sets of goal and priority
specifications that direct the evolutionary search towards a different portion of the trade-off surface
in a single run, the goal-sequence domination scheme is extended in this section to enable logical
statements such as OR (  ) and AND (  ) operations among multiple goal and priority
specifications.
These logical operations can be built on top of the goal-sequence domination procedure for each
specification. By doing this, the unified rank value for each individual can be determined and taken
into effect immediately in the evolution towards the regions concerned. Consider ranking an
objective vector Fx by comparing it to the rest of the individuals in a population with reference to
two different specification settings of Si and Sj, where Si and Sj are the specifications concerning
any set of objective functions with or without goals and priorities. Let these ranks be denoted by
rank(Fx, Si) and rank(Fx, Sj), respectively. The OR and AND operations for the two goal
settings are then defined as,
191

fiTAN, KHOR, LEE, & SATHIKANNAN

rank ( Fx , S i  S j ) = min{rank ( Fx , S i ), rank ( Fx , S j )}

(7a)

rank ( Fx , S i  S j ) = max{rank ( Fx , S i ), rank ( Fx , S j )}

(7b)

According to Eq. 7, the rank value of vector Fx for an OR operation between any two
specifications Si and Sj takes the minimum rank value with respect to the two specification settings.
This is in order to evolve the population towards one of the specifications in which the objective
vector is less strongly violated. In contrast, an AND operation takes the maximum rank value in
order to direct the evolutionary search towards minimizing the amount of violation from both of the
specifications concurrently. Clearly, the AND and OR operations in Eq. 7 can be easily
extended to include general logical specifications with more complex connectives, such as (Si OR
Sj) AND (Sk OR Sl), if desired.

3. Dynamic Sharing Scheme and MOEA Program Flowchart
3.1 Dynamic Sharing Scheme

Fitness sharing was proposed by Goldberg and Richardson (1987) to evolve an equally distributed
population along the Pareto-optimal front or to distribute the population at multiple optima in the
search space. The method creates sub-divisions in the objective domain by degrading an individual
fitness upon the existence of other individuals in its neighborhood defined by a sharing distance.
The niche count, mi =  Nj sh(d i , j ) , is calculated by summing a sharing function over all members of
the population, where the distance di,j represents the distance between individual i and j. The
sharing function is defined as

  d i,j  

sh(d i , j ) = 1    share 

0


if d i , j <  share

(8)

otherwise

with the parameter  being commonly set to 1.
The sharing function in Eq. 8 requires a good setting of sharing distance share to be estimated
upon the trade-off surface, which is usually unknown in many optimization problems (Coello
Coello, 1999). Moreover, the size of objective space usually cannot be predefined, as the exact
bounds of the objective space are often undetermined. Fonseca and Fleming (1993) proposed the
method of Kernel density estimation to determine an appropriate sharing distance for MO
optimization. However, the sharing process is performed in the sphere space which may not
reflect the actual objective space for which the population is expected to be uniformly distributed.
Miller and Shaw (1996) proposed a dynamic sharing method for which the peaks in the parameter
domain are dynamically detected and recalculated at every generation with the sharing distance
remains predefined. However, the approach is made on the assumption that the number of niche
peaks can be estimated and the peaks are all at the minimum distance of 2share from each other.
192

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

Moreover, their formulation is defined in the parameter space to handle multi-modal function
optimization, which may not be appropriate for distributing the population uniformly along the
Pareto-optimal front in the objective domain.
In contrast to existing approaches, we propose a dynamic sharing method that adaptively
computes the sharing distance share to uniformly distribute all individuals along the Pareto-optimal
front at each generation. This requires no prior knowledge of the trade-off surface. Intuitively, the
trade-offs for an m-objective optimization problem are in the form of an (m-1) dimensional
hyper-volume (Tan et al. 1999), which can be approximated by the hyper-volume Vpop(n) of a
hyper-sphere as given by,

V pop

(n)

=

 d (n) 


2 
 m 1

 ! 
 2 

 ( m 1) / 2

m 1

(9)

where d (n ) is the diameter of the hyper-sphere at generation n. Note that computation of the
diameter d (n ) depends on the curvature of the trade-off curve formed by the non-dominated
individuals in the objective space. For a two-objective optimization problem, the diameter d (n ) is
equal to the interpolated distance of the trade-off curve covered by the non-dominated individuals
as shown in Figure 4. Although computation of d (n ) that accurately represents the interpolated
curvature of the non-dominated individuals distribution is complex, it can be estimated by the
average distance between the shortest and the longest possible diameter given by dmin(n) and dmax(n)
respectively (Tan et al. 1999). Let Fx and Fy denote the objective function of the two furthest
individuals in a population. Then dmin(n) is equal to the minimum length between Fx and Fy, and
dmax(n) can be estimated by d1(n) + d2(n) as shown in Figure 4.
The same procedure can also be extended to any multi-dimensional objective space. To achieve
a uniformly distributed population along the trade-off set, the sharing distance share(n) could be
computed as half of the distance between each individual in the (m-1)-dimensional hyper-volume
Vpop(n) covered by the population size N at generation n,
N

 ( m 1) / 2
 m 1

!
 2 

(n)
)
 ( share

m 1

(n)
= V pop

(10)

Substituting Eq. 9 into Eq. 10 gives the sharing distance share(n) at generation n in term of the
diameter d (n ) and the population size N as given by
(n)
 share
= N 1 /(1 m ) 

d (n)
2

(11)

Clearly, Eq. 11 provides a simple computation of share that is capable of distributing the
population evenly along the Pareto front, without the need for any prior knowledge of the usually
193

fiTAN, KHOR, LEE, & SATHIKANNAN

unknown fitness landscape. Moreover, adopting the computation of sharing distance that is
dynamically based upon the population distribution at each generation is also more appropriate and
effective than the method of off-line estimation with pre-assumed trade-off surface as employed in
most existing sharing methods, since the trade-off surface may be changed any time along the
evolution whenever the goal setting is altered.
f2

Fx

dmin(n)
d (n)

Discovered
trade-off curve

d1 (n)
Fy

d2 (n)

f1

Figure 4: The Diameter d (n ) of a Trade-off Curve
3.2 MOEA Program Flowchart

The overall program flowchart of this papers multi-objective evolutionary algorithm (MOEA) is
illustrated in Figure 5. At the beginning of the evolution, a population of candidate solutions is
initialized and evaluated according to a vector of objective functions. Based upon the user-defined
specifications, such as goals, constraints, priorities and logical operations, the evaluated individuals
are ranked according to the goal-sequence domination scheme (described in Section 2) in order to
evolve the search towards the global trade-off surface. The resulted rank values are then further
refined by the dynamic sharing scheme (described in Section 3.1) in order to distribute the
non-dominated individuals uniformly along the discovered Pareto-optimal front. If the stopping
criterion is not met, the individuals will undergo a series of genetic operations which are detailed
within the genetic operations in Figure 6. Here, simple genetic operations consisting of
tournament selection (Tan et al. 1999), simple crossover with mating restriction that selects
individuals within the sharing distance for mating (Fonseca & Fleming, 1998) as well as simple
mutation are performed to reproduce offspring for the next generation.
After the genetic operations, the newly evolved population is evaluated and combined with the
non-dominated individuals preserved from the previous generation. The combined population is
then subjected to the domination comparison scheme and pruned to the desired population size
according to the Switching Preserved Strategy (SPS) (Tan et al. 1999). This maintains a set of
stable and well-distributed non-dominated individuals along the Pareto-optimal front. In SPS, if the
number of non-dominated individuals in the combined population is less than or equal to the
desired population size, extra individuals are removed according to their rank values in order to
promote stability in the evolutionary search towards the final trade-offs. Otherwise, the
194

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

non-dominated individuals with high niched count value will be discarded in order to distribute the
individuals uniformly along the discovered Pareto-optimal front. After the process, the remaining
individuals are allowed to survive in the next generation and this evolutionary cycle is repeated
until the stopping criterion is met.

P opulation initialization
Function evaluation
D om ination com parison

Y es

Is
stopping criterion
m et?

non-dominated individuals

Final
population

No
G enetic operations
Function evaluation
evolved
population



new population

D ynam ic sharing

com bined
population

D om ination com parison

Size(nondom )
> popsize?

No

Filtering - based on
P areto ranked cost

Y es
D ynam ic sharing

Filtering - based on
shared costs

Figure 5: Program Architecture of the MOEA
Genetic Operations for MOEA:
Let,
pop(n) = population in current generation n
Step 1) Perform tournament selection to select individuals from pop(n). The selected population is
called selpop(n).
Step 2) Perform simple crossover and mating restriction for selpop(n) using the dynamic sharing
distance in Step 1. The resulted population is called crosspop(n).
Step 3) Perform simple mutation for crosspop(n). The resulted population is called evolpop(n).
Figure 6: Detailed procedure within the box of genetic operations in Figure 5

195

fiTAN, KHOR, LEE, & SATHIKANNAN

4. Validation Results on Benchmark Problems
This section validates the proposed algorithm in two ways. The first kind of validation (presented in
Section 4.1) illustrates how each of the proposed features, including goal-sequence domination
scheme, hard/soft goal and priority specifications, logical operations among multiple goals and
dynamic sharing, enhances the performance of MOEA in MO optimization. As shown in Section
4.2, the second type of validation compares performance of the proposed MOEA with various
evolutionary algorithms based upon a benchmark problem. Various performance measures are used
in the comparison and the results are then discussed.
4.1 Validation of the Proposed Features in MOEA

In this section, various proposed features in MOEA are examined for their usefulness in MO
optimization. This study adopts the simple two-objective minimization problem (Fonseca &
Fleming, 1993) that allows easy visual observation of the optimization performance. The function
has a large and non-linear trade-off curve, which challenges the algorithms ability to find and
maintain the entire Pareto-optimal front uniformly. The two-objective functions, f1 and f2, to be
minimized are given as
 8
1 

f1 ( x1 ,..., x8 ) = 1  exp    xi 
 i =1 
8



2






2
 8
1  

f 2 ( x1 ,..., x8 ) = 1  exp    xi +
 i =1 
8  


(12)

where,  2  xi < 2, i = 1,2,...,8 . The trade-off line is shown by the curve in Figure 7, where the
shaded region represents the unfeasible area in the objective domain.

f

2

Trade-off
Curve

Unfeasible
Region

f

1

Figure 7: Pareto-optimal Front in the Objective Domain
196

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

The simulations are run for 70 generations with a population size of 100. Standard mutation with
a probability of 0.01 and standard two-point crossover with a probability of 0.7 are used. To study
the merit of the dynamic sharing scheme in MOEA as proposed in Section 3.1, 4 different types of
simulations have been performed. The first type is without fitness sharing. The second and third
employ a fixed sharing distance of 0.01 and 0.1, respectively. The fourth uses the dynamic sharing
scheme which does not require any predefined sharing distance setting. Figure 8 illustrates the
respective population distribution in the objective domain at the end of the evolution. It can be
observed that all of the four simulations are able to discover the final trade-off, but with some
performance difference in terms of the closeness and uniformity of the population distribution
along the trade-off curve.
For the MOEA without fitness sharing as shown in Figure 8a, the population tends to converge
to an arbitrary part of the trade-off curve. This agrees with the findings of Fonseca and Fleming,
(1993). For the MOEA with fitness sharing, as shown in Figures 8b and 8c, the population can be
distributed along the trade-off curve rather well, although the sharing distance of 0.01 provides a
more uniform distribution than that of 0.1. This indicates that although fitness sharing contributes
to population diversity and distribution along the trade-off curve, the sharing distance has to be
chosen carefully in order to ensure the uniformity of the population distribution. This often
involves tedious trial-and-error procedures in order to guess an appropriate sharing distance,
since it is problem dependent and based upon the size of the discovered trade-offs as well as the
number of non-dominated individuals. These difficulties can be solved with the proposed dynamic
sharing scheme, which has the ability to automatically adapt the sharing distance along the
evolution without the need of any predefined parameter, as shown in Figure 8d.

(b) Sharing distance = 0.01

(a) No sharing

197

fiTAN, KHOR, LEE, & SATHIKANNAN

(c) Sharing distance = 0.1
(d) Dynamic sharing
Figure 8: Performance Validation of Dynamic Sharing Scheme in MOEA

f2

f2

To validate the contribution of the switching preserved strategy (SPS) in MOEA, the above
simulation was repeated with different scenarios and settings. Figure 9a depicts the simulation
result without the implementation of SPS, in which the evolution faces difficulty converging to the
trade-off curve. The solid dots represent the non-dominated individuals while the empty circles
represent the dominated individuals. As can be seen, the final population is crowded and the
non-dominated individuals are distributed with some distance away from the trade-off curve.
Figure 9b shows the simulation result for the MOEA with SPS and filtering solely based upon the
Pareto domination. The final population has now managed to converge to the Pareto-optimal front.
However, the non-dominated individuals are not equally distributed and the diversity of the
population is poor: they only concentrate on a portion of the entire trade-off curve (c.f. Figures 8d,
9b). These results clearly show that SPS in MOEA is necessary in order to achieve good stability
and diversity of the population in converging towards the complete set of trade-offs.

Unfeasible
region

Unfeasible
region

f1

f1

(b) With SPS solely based on Pareto ranked cost
(a) Without SPS
Figure 9: Performance Validation of SPS in MOEA

198

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

f2

f2

The proposed goal-sequence domination scheme was also validated for problems with different
goal settings, including a feasible but extreme goal setting of (0.98, 0.2) and an unfeasible goal
setting of (0.7, 0.4) as shown in Figures 10 and 11, respectively. As desired, the population is seen
to concentrate on the preferred region of the trade-off curve at the end of the evolution, regardless
of the unattainable or extreme goal settings. As shown in Figures 10 and 11, MOEA is capable of
uniformly distributing the non-dominated individuals along any trade-offs size resulting from
different goal settings, with the help of the dynamic sharing scheme that automatically computes a
suitable sharing distance for optimal population distribution at each generation.

f1

f1

Figure 10: Feasible but Extreme Goal Setting

Figure 11: Unfeasible Goal Setting

Figure 12 shows the trace of sharing distance during the evolution. The thin and thick lines
represent the average sharing distance without any goal setting (see Figure 8d for the corresponding
Pareto-front) and with the goal setting of (0.7, 0.4) (see Figure 11 for the corresponding
Pareto-front), respectively. Generally, MO optimization without a goal setting has an initially small
size of discovered Pareto-front, which subsequently grows along with the evolution to approach
and cover the entire trade-off region at the end of evolution. This behavior is explained in Figure 12
where the sharing distance increases asymptotically along the evolution until a steady value of
0.0138 is reached. It should be noted that this value is close to the fixed sharing distance of 0.01 in
Figure 8b, which was carefully chosen after trial-and-error procedures. For the case of MOEA with
a goal setting of (0.7, 0.4), the sharing distance increases initially and subsequently decreases to
0.0025 along the evolution, which is lower than the value of 0.0138 (without goal setting). The
reason is that the concentrated trade-off region within the goal setting is smaller than the entire
trade-off region (without goal setting), and hence results in a smaller distance for uniform sharing
of non-dominated individuals. These experiments show that the proposed dynamic sharing scheme
can effectively auto-adapt the sharing distance to arrive at an appropriate value for uniform
population distribution along the discovered trade-off region at different sizes, without the need for
any a priori parameter setting.

199

fiTAN, KHOR, LEE, & SATHIKANNAN

Figure 12: Trace of the Dynamic Sharing Distance Along the Evolution

Non-dominated
individual

f2

f2

Figures 13 and 14 show the MOEA simulation results for the case of an infeasible goal setting
with soft and hard priorities, respectively. In the figures, diamonds represent goals, small circles
represent non-dominated individuals and solid dots represent dominated individuals. For the soft
priority setting in Figure 13, goals are treated as first priority followed by the objective component
of f1 as second priority, i.e., Pg = [1, 1] and Pf = [2, 0]. As can be seen, it provides a distributive
optimization approach for all goals by pushing the population towards the objective component of
f1 that has a higher priority, after taking the goal vector into consideration (c.f. Figures 3a, 13b). In
contrast, Figure 14 shows the minimization results with hard priority setting where priority of f1 is
higher than f2, i.e., Pg = [1, 2] and Pf = [0, 0]. Unlike the soft priority optimization, hard priority
minimizes the objective of f1 until the relevant goal component of g1 = 0.5 is satisfied before
attaining the objective component of f2 with the second goal component of g2 = 0.5, as shown in
Figure 14 (c.f. Figures 3b, 14b). As can be seen, objective values with hard priority settings are
better with higher priority but are worse with lower priority, as compared to the solutions obtained
in soft priority optimization (c.f. Figures 13b, 14b).

Non-dominated
individual

ff1

f1

(a) At generation 5
(b) At generation 70
Figure 13: MOEA Optimization with Unfeasible Goal Setting: f1 has Soft Priority Higher than f2
200

fiNon-dominated
individual

f2

f2

AN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

Non-dominated
individual

ff11

f1

(a) At generation 5
(b) At generation 70
Figure 14: MOEA Optimization with Unfeasible Goal Setting: f1 has Hard Priority Higher than f2

ff22

Figure 15 shows the MOEA minimization result with f1 being a hard constraint. The population
continuously evolves towards minimizing f2 only after the hard constraint of f1 has been satisfied. In
general, objective components with hard constraints may be assigned as hard priorities in order to
meet the hard constraints before minimizing any other objective components.

Non-dominated
individuals

ff11

Figure 15: MOEA Minimization with Hard Constraint on f1
Figures 16 and 17 show the MO optimization results that include multiple goal settings specified
by logical OR (  ) and AND (  ) connectives, respectively. For the OR operation as shown
in Figure 16, the population is automatically distributed and equally spread over the different
concentrated trade-off regions to satisfy the goal settings separately, regardless of the overlapping
or feasibility of the goals. With the proposed dynamic sharing scheme, the sub-population size for
each goal is in general based upon the relative size of the concentrated trade-off surface of that goal,
201

fiTAN, KHOR, LEE, & SATHIKANNAN

and thus individuals are capable of equally distributing themselves along the different concentrated
trade-off regions. For the AND operation as illustrated in Figure 17, the whole population
evolves towards minimizing all the goals G1, G2 and G3 simultaneously. As a result, the individuals
are equally distributed over the common concentrated trade-off surface formed by the three goals,
as desired.
Pareto optimality observation

GG1 1

G
G22
f2

G3
G
3
GG4 4

f1

Figure 16: MOEA Minimization for (G1  G2  G3  G4)
Pareto optimality observation

GG11

G22

f2

G
G33

f1

Figure 17: MOEA Minimization for (G1  G2  G3)
4.2 Performance Comparisons of MOEA

This section studies and compares the performance of the proposed MOEA with other
multi-objective evolutionary optimization methods based upon a benchmark MO optimization
problem. For a comprehensive comparison, various performance measures are used and the
comparison results are discussed in the section.
4.2.1 The Test Problem
202

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

The test problem used for the performance comparisons is a two-objective minimization problem
(Deb, 1999). The problem is chosen because it has a discontinuous Pareto-front which challenges
the evolutionary algorithms ability to find and maintain the Pareto-optimal solutions that are
discontinuously spread in the search space. The problem involves minimizing the objective
functions f1 and f2 as given below,

f1 ( x1 ) = x1
g ( x2 ,..., x10 ) = 1 + 10
h( f 1 , g ) = 1  ( f 1 g )

0.25

(13a)

10
i = 2 xi
,
10  1

 ( f1 g )sin (10f1 )

f 2 ( x1 ) = g ( x2 ,..., x10 )h( f1 , g )

(13b)
(13c)

(13d)

All variables are varied in [0, 1] and the true Pareto-optimal solutions are constituted with xi = 0
 i = 2, , 10 and the discontinuous values of x1 in the range of [0, 1] (Deb, 1999). Figure 18
depicts the discontinuous Pareto-optimal front (in bold). The shaded region represents the
unfeasible region in the search space.

Figure 18: Pareto-optimal Front in the Objective Domain
4.2.2 Current Evolutionary MO Optimization Methods

Besides MOEA, five well-known multi-objective evolutionary optimization methods are used in
the comparison. These approaches differ from each other in their working principles and
mechanisms and have been widely cited or applied to real-world applications. The algorithms are
summarized below and readers may refer to their respective references for detailed information.
(i) Fonseca and Flemings Genetic Algorithm (FFGA): For MO optimization, Fonseca and
Fleming (1993) proposed a multi-objective genetic algorithm (MOGA) with Pareto-based ranking
scheme, in which the rank of an individual is based on the number of other individuals in the current

203

fiTAN, KHOR, LEE, & SATHIKANNAN

population that dominate it. Their algorithm was further incorporated with fitness sharing and
mating restriction to distribute the population uniformly along the Pareto-optimal front.
(ii) Niched Pareto Genetic Algorithm (NPGA): The method of NPGA (Horn & Nafpliotis, 1993)
works on a Pareto-dominance-based tournament selection scheme to handle multiple objectives
simultaneously. To reduce the computational effort, a pre-specified number of individuals are
picked as a comparison set to help determine the dominance. When both competitors end in a tie,
the winner is decided through fitness sharing (Goldberg and Richardson, 1987).
(iii) Strength Pareto Evolutionary Algorithm (SPEA): The main features of SPEA (Zitzler &
Thiele, 1999) are the usage of two populations (P and P) and clustering. In general, any
non-dominated individual is archived in P and any dominated individual that is dominated by
other members in P is removed. When the number of individuals in P exceeds a maximum value,
clustering is adopted to remove the extra individuals in P. Tournament selection is then applied to
reproduce individuals from P + P before the evolution proceeds to the next generation.
(iv) Non-Generational Genetic Algorithm (NGGA): In NGGA (Borges & Barbosa, 2000), a cost
function of an individual is a non-linear function of domination measure and density measure on
that individual. Instead of evolving the whole population at each iteration, a pair of parents is
selected to reproduce two offsprings. An offspring will replace the worst individual in a population
if the offspring has lower cost function than the worst individual.
(v) Murata and Ishibuchis Genetic Algorithm (MIGA): Unlike the above evolutionary
optimization methods, MIGA (Murata & Ishibuchi, 1996) applies the method of weighted-sum to
construct the fitness of each individual in a population. To keep the diversity of the population
along the Pareto-optimal front, the weights are randomly specified when a pair of parent solutions
is selected from a current population for generating the offspring.
4.2.3 Performance Measures

This section considers three different performance measures which are complementary to each
other: Size of space covered (SSC), uniform distribution (UD) index of non-dominated individuals
and number of function evaluation (Neval).
(i) Size of Space Covered (SSC): This measure was proposed by Zitzler and Thiele (1999) as a
measure to quantify the overall size of phenotype space covered (SSC) by a population. In general,
the higher the value of SSC, the larger the space covered by the population and hence the better the
optimization result.
(ii) Uniform Distribution (UD) of Non-dominated Population: Besides the size of space covered
by a population, it is also essential to examine the ability of an evolutionary optimization to
distribute their non-dominated individuals as uniformly as possible along the discovered
204

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

Pareto-optimal front, unless prohibited by the geometry of the Pareto front. This is to achieve a
smooth transition from one Pareto-optimal solution to its neighbors, thus facilitating the
decision-maker in choosing his/her final solution. Mathematically, UD(X') for a given set of
non-dominated individuals X' in a population X, where X'  X, is defined as (Tan et al. 2001a),
UD(X' ) =

1
1 + S nc

(14)

where Snc is the standard deviation of niche count of the overall set of non-dominated individuals X'.
It can be seen that larger value of UD(X) indicates a more uniform distribution and vice versa.
(iii) Number of Function Evaluation (Neval): The computational effort required to solve an
optimization problem is often an important issue, especially when only limited computing
resources are available. In the case that a fixed period of CPU time is allocated and the CPU time
for each function evaluation is assumed to be equal, then more function evaluations being
performed by an optimization indirectly indicates less additional computational effort is required
by the algorithm.
4.2.4 Simulation Settings and Comparison Results
The decimal coding scheme (Tan et al. 1999) is applied to all the evolutionary methods studied in
this comparison, where each parameter is coded in 3-digit decimals and all parameters are
concatenated together to form a chromosome. In all cases, two-point crossover with a probability of
0.07 and standard mutation with a probability of 0.01 are used. A reproduction scheme is applied
according to the method used in the original literature of each algorithm under comparison. The
population size of 100 is used in FFGA, NPGA, NGGA and MOEA, which only require a single
population in the evolution. SPEA and MIGA are assigned a population size of 30 and 70 for their
external/archive and evolving population size, respectively, which form an overall population size
of 100. All approaches under comparison were implemented with the same common sub-functions
using the same programming language in Matlab (The Math Works, 1998) on an Intel Pentium II
450 MHz computer. Each simulation is terminated automatically when a fixed simulation period of
180 seconds is reached. The simulation period is determined, after a few preliminary runs, in such a
way that different performance among the algorithms could be observed. To avoid random effects,
30 independent simulation runs, with randomly initialized population, have been performed on
each algorithm and the performance distributions are visualized in the box plot format (Chambers
et al. 1983; Zitzler & Thiele, 1999).
Figure 19 displays the performance of SSC (size of space covered) for each algorithm. In general,
SPEA and MOEA produce a relatively high value of SSC indicating their ability to have a more
distributed discovered Pareto-optimal front and/or to produce more non-dominated solutions that
are nearer to the global trade-offs. It can also be observed that, compared to the others, FFGA,
SPEA and MOEA are more consistent in the performance of SSC. The performance of UD
(uniform distribution) for all algorithms is summarized in Figure 20. In general, the UD
distributions are mostly overlapping with each other and thus there is too little evidence to draw any
205

fiTAN, KHOR, LEE, & SATHIKANNAN

strong conclusion. However, as the average performance is concerned (see bold horizontal line in
the box plots), SPEA, MIGA and MOEA outperform others slightly and are more consistent in
terms of the measure of UD. Figure 21 shows the distribution of Neval (number of function
evaluation) performed by each algorithm in a specified time. More function evaluations in a fixed
CPU time indirectly indicates that less CPU time is required by the algorithm. Intuitively, this
means less computational efforts are required by the algorithm to find the trade-offs. As shown in
Figure 21, MIGA requires the least algorithm effort while the performances of FFGA, NPGA and
MOEA are moderate in terms of Neval. It can also be observed that SPEA and NGGA are suitable
for problems with time-consuming function evaluations: the effects in algorithm effort become less
significant in these problems. In summary, the results show that MOEA requires moderate
computational effort and exhibits a relatively good performance in terms of SSC and UD on the test
problem, as compared to other MO evolutionary optimization methods in this study.

Figure 19: Box Plot of SSC

Figure 20: Box Plot of UD

206

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

Figure 21: Box Plot of Neval
Figure 22 shows the distribution of non-dominated individuals in the objective domain, where
the range of each axis is identical to the range shown in Figure 18. For each algorithm, the
distribution is the best selected, among the 30 independent runs, with respect to the measure of SSC.
It can be seen from Figure 22 that MOEA benefits from evolving more non-dominated individuals
than the other methods. MOEAs individuals are also better distributed within the trade-off region.

FFGA

NPGA

SPEA

NGGA

MIGA

MOEA

Figure 22: Best Selected Distribution of Non-dominated Individuals from Each Algorithm with
Respect to the Measure of SSC

207

fiTAN, KHOR, LEE, & SATHIKANNAN

5. Application to Practical Servo Control System Design
5.1 The Hard Disk Drive Servo System
A typical plant model of hard disk drive (HDD) servo system includes a driver (power amplifier), a
VCM (Voice Coil Motor) and a rotary actuator that is driven by the VCM. Figure 23 (Goh et al.
2001) shows a basic schematic diagram of a head disk assembly (HDA), where several rotating
disks are stacked on the spindle motor shaft.
VOICE COIL MOTOR
ACTUATOR

DISK

ARM

SUSPENSION AND
RECORDING HEAD

DATA TRACK

Figure 23: A HDD with a Single VCM Actuator Servo System
The dynamics of an ideal VCM actuator is often formulated as a second-order state-space model
(Weerasooriya, 1996),

 y&  0 K y  y   0 
  +   u
  = 
 v&  0 0  v   K v 

(15)

where u is the actuator input (in volts), y and v are the position (in tracks) and the velocity of the
R/W head, Kv is the acceleration constant and Ky the position measurement gain,
where K y = K t m with Kt being the current-force conversion coefficient and m being the mass of
the VCM actuator. The discrete-time HDD plant model used for the evolutionary servo controller
design in this study is given as (Tan et al. 2000),
 1 1.664 
1.384 
 x(k ) + 


1.664  u
1 
0



x(k + 1) = 

(16)

5.2 Evolutionary HDD Controller Design and Implementation

A two-degree-of-freedom (2DOF) control structure is adopted for the read/write head servo system
as shown in Figure 24. For simplicity and easy implementation, a simple first-order discrete-time

208

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

controller with a sampling frequency of 4 kHz is used for the feedforward and feedback controllers,
which is in the form of
 z + ff 1 

K p = K f 
 z + ff 2 

 z + fb1 

K s = K b 
 z + fb2 

(17)

respectively. The control objective during the tracking in HDD is to follow the destination track
with a minimum tracking error. Note that only time domain performance specifications are
considered in this paper, and the design task is to search for a set of optimal controller parameters
{Kf, Kb, ff1, ff2, fb1, fb2} such that the HDD servo system meets all design requirements. These
requirements are that overshoots and undershoots of the step response should be kept less than 5%
since the head can only read or write within 5% of the target; the 5% settling time in the step
response should be less than 2 milliseconds and settle to the steady-state as quickly as possible
(Goh et al. 2001). Besides these performance specifications, the system is also subject to the hard
constraint of actuator saturation, i.e., the control input should not exceed 2 volts due to the
physical constraint on the VCM actuator.

r

Kp

u

+-

VCM

Feedforward
controller

y

Plant
Ks
Feedback controller

Figure 24: The Two Degree-of-freedom Servo Control System
The multi-objective evolutionary algorithm (MOEA) proposed in this paper has been embedded
into a powerful GUI-based MOEA toolbox (Tan et al. 2001b) for ease-of-use and for
straightforward application to practical problems. The toolbox is developed under the Matlab (The
Math Works, 1998) programming environment, which allows users to make use of the versatile
Matlab functions and other useful toolboxes such as Simulink (The Math Works, 1999). It allows
any trade-off scenario for MO design optimization to be examined effectively, aiding
decision-making for a global solution that best meets all design specifications. In addition, the
toolbox is equipped with a powerful graphical user interface (GUI) and is ready for immediate use
without much knowledge of evolutionary computing or programming in Matlab. A file handling
capability for saving all simulation results and model files in a Mat-file format for Matlab or
text-file format for software packages like Microsoft Excel is also available in the toolbox. Through
the GUI window of MOEA toolbox, the time domain design specifications can be conveniently set
as depicted in Figure 25, where Tr, OS, Ts, SSE, u and ue represents the rise time, overshoot,
settling time, steady-state error, control input and change in control input, respectively.

209

fiTAN, KHOR, LEE, & SATHIKANNAN

Figure 25: MOEA GUI Window for Settings of Design Specifications
The simulation adopts a generation and population size of 200, and all the design specifications
listed in Figure 25 have been successfully satisfied at the end of the evolution. The design trade-off
graph is shown in Figure 26, where each line representing a solution found. The x-axis shows the
design specifications and the y-axis shows the normalized cost for each objective. Clearly,
trade-offs between adjacent specifications result in the crossing of the lines between them (e.g.,
steady-state error (SSE) and control effort (u)), whereas concurrent lines that do not cross each
other indicating the specifications do not compete with one another (e.g., overshoots (OS) and
settling time (Ts)).

Figure 26: Trade-off Graph of the HDD Servo Control System Design

210

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

The closed-loop step response of the overall system for an arbitrary selected set of MOEA
designed 2DOF controller parameters given as {Kf, Kb, ff1, ff2, fb1, fb2} = {0.029695, -0.58127,
0.90279, -0.3946, -0.70592, 0.83152} is shown in Figure 27. With a sampling frequency of 4 kHz,
the time domain closed-loop performance of the evolutionary designed controller has been
compared with the manually designed discrete-time PID controller as given in Eq. 18 (Goh et al.
2001) as well as the Robust and Perfect Tracking (RPT) controller (Goh et al. 2001) as given in Eq.
19,
0.13 z 2  0.23 z + 0.1
u=
(r  y )
(18)
z 2  1.25 z + 0.25
x(k + 1) = 0.04 x(k ) + 15179r (k )  453681y (k )

(19)

u (k ) = 3.43  10 7 x(k ) + 0.04r (k )  0.18 y ( k )

It can be seen in Figure 27 that the evolutionary designed 2DOF controller has outperformed
both the PID and RPT controllers, with the fastest rise time, smallest overshoots and shortest
settling time in the closed-loop response. Its control performance is excellent and the destination
track crossover occurs at approximately 1.8 milliseconds.
1.8
1 : MOEA Based 2DOF Controller

1.6

2

2 : PID Controller
3 : RPT Controller

Head Position (Tracks)

1.4
1.2
3

1

1

0.8
0.6
0.4
0.2
0

0

0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009

0.01

Time in Seconds

Figure 27: Closed-loop Servo System Responses with Evolutionary 2DOF, RPT and PID
Controllers
The performance of the evolutionary 2DOF servo control system was further verified and tested
on the physical 3.5-inch HDD with a TMS320 digital signal processor (DSP) and a sampling rate of
4 kHz. The R/W head position was measured using a laser doppler vibrometer (LDV) and the
resolution used was 1 m/volt. Real-time implementation result of the evolutionary HDD servo
control system is given in Figure 28, which is consistent with the simulated step response in Figure
27, and shows an excellent closed-loop performance.

211

fiTAN, KHOR, LEE, & SATHIKANNAN

Output Response
1.4

Tracks ( Actuator Output )

1.2

1

0.8

0.6

0.4

0.2

0

0

0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009
Time in Seconds

0.01

Figure 28: Real-time Implementation Response of the Evolutionary 2DOF Servo System

6. Conclusions
This paper has presented a multi-objective evolutionary algorithm (MOEA) with a new
goal-sequence domination scheme to allow advanced specifications such as hard/soft priorities and
constraints to be incorporated for better decision support in multi-objective optimization. In
addition, a dynamic fitness sharing scheme that is simple in computation and adaptively based upon
the on-line population distribution at each generation has been proposed. Such a dynamic sharing
approach avoids the need for a priori parameter settings or user knowledge of the usually unknown
trade-off surface often required in existing methods. The effectiveness of the proposed features in
MOEA has been demonstrated by showing that each of the features contains its specific merits and
usage that benefit the performance of MOEA. In comparison with other existing evolutionary
approaches, simulation results show that MOEA has performed well in the diversity of
evolutionary search and uniform distribution of non-dominated individuals along the final
trade-offs, without significant computational effort. The MOEA has been applied to the practical
engineering design problem of a HDD servo control system. Simulation and real-time
implementation results show that the evolutionary designed servo system provides excellent
closed-loop transient and tracking performance.

Acknowledgements
The authors wish to thank Andrew Moore and the anonymous reviewers for their valuable
comments and helpful suggestions which greatly improved the paper quality.

212

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

References
Borges, C. C. H., & Barbosa, H. J.C. (2000). A non-generational genetic algorithm for
multiobjective optimization. IEEE Congress on Evolutionary Computation, 1, 172-179.
Chambers, J. M., Cleveland, W. S., Kleiner, B., & Turkey, P. A. (1983). Graphical Methods for
Data Analysis, Wadsworth & Brooks/Cole, Pacific CA.
Coello Coello, C. A. (1996). An Empirical Study of Evolutionary Techniques for Multiobjective
Optimization in Engineering Design, Ph.D. Thesis, Dept. of Computer Science, Tulane University,
New Orleans, LA.
Coello Coello, C. A. (1999). A Comprehensive Survey of Evolutionary-Based Multiobjective
Optimization Techniques. Knowledge and Information Systems, 1(3), 269-308.
Deb, K. (1999). Evolutionary algorithms for multi-criterion optimization in engineering design. In
Miettinen. Evolutionary Algorithms in Engineering and Computer science: Recent Advances in
Genetic Algorithms, Evolution Strategies, Evolutionary Programming, Genetic Programming, and
Industrial Applications, Wiley, New York, 135-161.
Deb, K. (2001). Multi-objective Optimization using Evolutionary Algorithms. John Wiley & Sons,
London.
Fonseca, C. M. (1995). Multiobjective Genetic Algorithms with Application to Control Engineering
Problems. Ph.D. Thesis, Dept. of Automatic Control and Systems Engineering, University of
Sheffield, UK.
Fonseca, C. M., & Fleming, P. J. (1993). Genetic algorithm for multiobjective optimization,
formulation, discussion and generalization. In (Forrest, 1993), 416-423.
Fonseca, C. M. & Fleming, P. J., (1998). Multiobjective optimization and multiple constraint
handling with evolutionary algorithms  Part I: A unified formulation. IEEE Trans. on System, Man,
and Cybernetics-Part A: System and Humans, 28(1), 26-37.
Goh, T. B., Li, Z. M., Chen, B. M., Lee, T. H., & Huang, T., (2001). Design and implementation of
a hard disk drive servo system using robust and perfect tracking approach. IEEE Trans. on Control
Systems Technology, 9(2), 221-233.
Goldberg, D. E., & Richardson, J. (1987). Genetic algorithms with sharing for multimodal function
optimization. Proc. 2nd Int. Conf. on Genetic Algorithms, 41-49.
Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization and Machine Learning.
Addison-Wesley, Reading, Massachusetts.
Grace, A. (1992). Optimisation Toolbox Users Guide, The MathWorks, Inc.
Holland, J. H. (1975). Adaptation in Natural and Artificial Systems. University of Michigan, Ann
Harbor.

213

fiTAN, KHOR, LEE, & SATHIKANNAN

Horn, J., & Nafpliotis, N. (1993). Multiobjective Optimization Using the Niche Pareto Genetic
Algorithm. IlliGAL Report 93005, University of Illinois, Urbana, Illinois, USA.
Luus, R., Hartig, F. & Keil, F. J., (1995). Optimal drug scheduling of cancer chemotherapy by
direct search optimization. Hungarian Journal of Industrial Chemistry, 23, 55-58.
Marcu, T. (1997). A Multiobjective evolutionary approach to pattern recognition for robust
diagnosis of process faults. Proc. of IFAC Fault Detection, Supervision and Safety for Technical
Process, UK, 1183-1188.
Michalewicz, Z., & Schoenauer, M., (1996). Evolutionary algorithms for constrained parameter
optimization problems. Evolutionary Computation, 4(1), 1-32.
Miller, B. L., & Shaw, M. J. (1996). Genetic algorithms with dynamic niche sharing for multimodal
function optimization. IEEE Conf. on Evolutionary Computation, Japan, 786-791.
Murata, T., & Ishibuchi, H., (1996). Multi-objective genetic algorithm and its applications to
flowshop scheduling. Int. Journal of Computers and Engineering, 957-968.
Osyczka, A. (1984). Multicriterion Optimisation in Engineering. Ellis Horwood, Chichester.
Schaffer, J. D. (1985). Multiple-objective optimization using genetic algorithm. Proc. of first Int.
Conf. on Genetic Algorithms, 93-100.
Tan, K. C., Lee, T. H., & Khor, E. F. (1999). Evolutionary algorithms with goal and priority
information for multi-objective optimization. IEEE Congress on Evolutionary Computation, 1,
106-113.
Tan, K. C., Sathikannan, R., Tan, W. W., Loh, A. P., Lee, T. H., & Mamun, A. Al. (2000)
Evolutionary Design and Real-Time Implementation of a Hard Disk Drive Servo Control System.
Int. Conf. on Control 2000, University of Cambridge, UK, Section 6C.
Tan, K. C., Lee, T. H., & Khor, E. F., (2001a). Evolutionary algorithms for multi-objective
optimization: Performance assessments and comparisons. IEEE Congress on Evolutionary
Computation, 2, 979-986.
Tan, K. C., Lee, T. H., Khoo, D., & Khor, E. F., (2001b). A multi-objective evolutionary algorithm
toolbox for computer-aided multi-objective optimization. IEEE Trans. on Systems, Man and
Cybernetics - Part B: Cybernetics, 31(4), 537-556.
The Math Works, Inc. (1998). Using MATLAB, Version 5.
The Math Works, Inc. (1999). Simulink: User's Guild, Version 3.
Van Veldhuizen, D. A., & Lamont, G. B. (1998). Multiobjective evolutionary algorithm research:
A history and analysis. Technical Report TR-98-03, Dept. of Electrical and Computer Eng.,
Graduate School of Eng., Air Force Institute of Technology, Wright-Patterson AFB, Ohio.

214

fiAN EVOLUTIONARY ALGORITHM FOR MULTI-OBJECTIVE OPTIMIZATION

Weerasooriya, S. (1996). The Basic Servo Problem: Technical Report. Data Storage Institute,
National University of Singapore, Singapore.
Zitzler, E., & Thiele, L., (1999). Multiobjective evolutionary algorithms: A comparative case
study and the strength Pareto approach. IEEE Trans. on Evolutionary Computation, 3(4), 257-271.

215

fi
	ff
fi 
			 ! #"$ % 
'&)( *,+.-//021$332465327/

89:;<  =>/
3?
/-!@BA:	%&=C/42?
/0

DFEHGJILKNMPOQSRUTBV)IXWYG<Z[EH\]O2G^Q`_aEcbedfV[ILghDibIkjSKlbLjSILEH\mOQnb]MSEoDqpSGJK<ErV)T
Et\$b]IkO2KlbEtu`vwKcZFKHx9O2KayzGJIb]OG^xxZ|{}O2ILEHKlb]EHu~ILG^pPMS\
s

kN

$][e2L

C <#qH

ff[e2L

'i2'qF]2e2)PkBHe'2B >)
<e9292 
^2
SFce
BF cB#H

>L
t6iBff66iC999626)c! .B2l6 6
6!> e6JB
!tB62!6PlF26t 
 !99929.$69 S6 2B !B

6B
96 B q2)ql
6 6S6 >9B

 B

6iBJB6
9>ff9BL9B

26C6i 2 69292)
B
H C 2<9>> $
.2! H C9
B6 6 B$
B9
6F9B ! PB
l626$cBB
q9
6B
 .i99##
6SYH !B

2ff 6i9i 99i96
  2# <
 69
2, i>  B2#92l6S 62699H69 6292.^ 
669296
H C
 .^cF)^ e6)2 6BC
6)9B

S 2J.<699ffH6 92 P9!B6 $69
[k92!6B6JHC9B) [<6c 6H !B

)26B6<6J
6t9  !B

i 2<.
FLiBff^9[!2. NBN,!6k 6,69
iFS S29tff9B$iB )
B9ffq96
S t
2
,2
B!F696l CB 6J6996
q2ffqS9669 <i
26J#9'<qH.B6 P6 ff6SB6CB. 69!


fi  

 
 fi  !"



fi





)



,&



	

$



%'&






ff








#



.-

/

(


*fi 




+

20 1435 7698;t: 7<=6 5
>@?+AB#CDBFEHGJI*KLDMONDG!MJPQNDRTSDMFGG4?#P"UWVYX[Z\]QV#^_^`ZJaQbcYdfe+\OgQhiMFBYjR=IlkFm#n#n7oqpTSrjMJstjMFG!MFSuKpTS/vxwHS/?+A@RyMFC/v#M
A*pyKLzNDSD{ffMJjKB#pTS[K|E}B#SDCqMJ~}{JpyMFSuKjMFB#G!?7SDpTSDvpTGA*pTCDMFRyE4B#{J{ffMJsDK!MFC2B+E#MFGpTB#SS/MJK|A;?#jwz{ff?7SDGpTGKG;?#P
BNDB#RTpyKBYKpy#MstBYjKJIBxY]dZ+aZV7FX#F]#d!VftgQ*.ofI'B#StCBuNDB#SuKpyKBYKpy#M4?7SDM#I B{ff?7RTRyMF{ffKpy?7S?#P
SND4MJjpT{JB#RHsBYjB#4MJK!MJjGJIuNDGNDB#RTRTEl{ff?7SDCDpyKpT?7SDB#RHsDj?#tBYpTRTpyK|E.KBYtRyMFGJL/M(wHS/?%A*RyMFC/v#M(jMJstjMFG!MFSuK!MFCpTS
KL/Mv#jfBYstLDpT{JB#R#{ff?74s?7S/MFSuK pTGMffsDjMFGG!MFC.pTS.K!MJjG2?#P/C/MJsMFSDC/MFSt{ffMB#SDC.pTSDCDMJs`MFStC/MFSD{ffMjMFRTBYKpy?7StGLDpystG
MJKAMJMFSBYjfpTBYtRyMFGJ.*L/MFG!M"jMFRTBYKpy?7SDGLDpystG@BYjMMFSD{ff?CDMFCNDGpTS/vqKL/MsDjMFG!MFSt{ffM?#j.BYtGMFSD{ffM?#P;RTpTSDwG
MJKAMJMFSS/?HC/MFG.pSKL/Mv#jBYstLL/M4wHS/?+A*RTMFC/v#MjMJsDjMFG!MFS[K!MFCpTSKL/MSND4MJjp{JB#RstBYjKlNDB#SuKpyDMFG
KL/MC/MJsMFSDC/MFSD{ffMFG}MFSD{ff?CDMFCpTSKLDMv#jBYstLIB#SDCB#RTRT?+A*GqNDGK!?pTS[K!j?CDNt{ffMNDSD{ffMJjKB#pS[K|ErpTS[K!?KL/M
4?HC/MFR9WRTRtpTSqB#RTRIB+E#MFGpTB#SS/MJK|A;?#jwHGsDj?%pC/M*B#MJjE}pTSuKNDpyKpy#M*v#jBYstLDpT{JB#RtK!?u?7RP?#jjMJsDjMFG!MFSuKpTS/v
BFYB#pTRTBYRyMwSD?+A*RyMFCDv#M#
WS/?#KL/MJjBYK!K!jB#{ffKpT?7S?#P BFE#MFGpB#S	S/MJKA?#jwHGpTG;KL/MFpyj(BYtpTRpyKE4K!?4MJ~}{JpTMFS[KRyE}s`MJjP?#jjMFB#G?7SDpTS/v
KB#G!wHGg#MFSDG!MFSIlkFm#m#lhiMFBYjRI"kFm#n#n7ofL/MpTSDC/MJsMFSDC/MFSt{ffMFG4jMJsDjMFGMFS[K!MFCpTSKL/M*BYjMKL/M
w#MJEK!?KLtpTGBYtpTRTpyK|E#I*jMFCtND{JpTS/v{LDB#S/v#MFGpTSKLDMwS/?%A*RyMFC/v#MG!KBYK!MxK!?Ry?H{JB#R{ff?74stNDKBYKpy?7SDGJS
B#CDCDpTKpy?7S2IHpT4s?#jKB#SuKGBFHpTS/v7G(pTSG!K!?#jBYv#MljMFuNtpyjMF4MFSuKG(BYjMs?7GGpyRyM@GpSD{ffMWpSDC/MJsMFSDC/MFSD{ffMFGB#RTRy?%A
B4PQB#{ffK!?#jpyFBYKpy?7S?#P'KLDM.v7Ry?#tB#RSND4MJjfpT{JB#R`jMJsDjMFG!MFSuKBYKpy?7SgKL/M!?7pTS[K(stj?#tBYtpTRpyKE}CtpTG!K!jpyN/Kpy?7Sof












 %

-//0l %% !=6^
= C2
 L
!fi;H
 :!	%&% 		&2%% 2 =

fi X	ttu
LDMJjM	LDB#GMJMFSrBRy?#K?#P)A;?#jwpSjMF{ffMFSuK}E#MFBYjG}?7SrKL/MB#NDK!?7BYKpT{RTMFBYjSDpTS/v?#PBFE#MFGpB#S
/S MJK|A;?#jwHG@Pj?7CDBYKBH*?7SDG!MFN/MFSuKRyE#ItKLDMJjMlBYjM"B4v#jMFBYK)B#S[ERyMFBYjfSDpTS/vB#Ryv#?#jfpyKLDG(A@LDpT{LBFE
M	GN/2CDpyHpTC/MFCpTSuK!?KA?v#MFS/MJjB#R@BYsDsDj?7B#{fL/MFGJMJKL/?CtG4tB#G!MFC?7ScY^#]aQ]cY^VYW]^[Z|DZff^[ZJ^fZ
aZ\ffa\gQB#RG!?{JB#RTRyMFCxfcY^/\aQd!VY]^taQV\FZ+ofIB#SDCMJKL/?CtG(tB#G!MFC?7SOBz\ffc#df]^uW^`FaQ]cY^ ( B#SDC	Bz\FZVYd!ff
sDj?H{ffMFCDN/jM#
LDMB#Ryv#?#jpTKLDG(tB#GMFC?7SpTSDC/MJsMFSDCDMFSD{ffM)K!MFG!KGWs`MJjP?#jB}NDB#RTpyKBYKpT#MG!KNDCDEz?#P9KLDMC/MJsMFSH
C/MFSD{ffMB#SDCpTSDCDMJs`MFStC/MFSD{ffMjMFRTBYKpT?7SDGLDpysG*B#4?7S/vzKL/M"BYjfpTBYtRyMFGWpTSOKL/M"CD?7B#pTS2IB#SDCBYK!K!MF4sDK)K!?
tSDCBqS/MJKA?#jwKLDBYK@jMJsDjMFG!MFSuKG(KL/MFGMljMFRTBYKpT?7SDGLDpysGB#G*PBYjWB#Gs?7GGpTtRyM#9L/MJEKL/MJjMJP?#jM.KBYw#MB
RTpTGK@?#P{ff?7SDCDpyKpy?7StB#R'pTStC/MJsMFSDC/MFSD{ffM.jMFRTBYKpy?7SDGLtpystGg?#tKB#pTS/MFCPj?7KL/M4CDBYKBuE4MFB#SDG)?#P{ff?7SDCDp
Kpy?7SDB#RupTSDC/MJsMFSDCDMFSD{ffMK!MFG!KGfo B#G KL/M;pTS/stN/KJIB#SDC.v#MFSDMJjBYK!MB@S/MJKA?#jwKLDBYK'jMJsDjMFGMFS[KGi4?7G!K'?#PKLDMFG!M
jMFRTBYKpT?7SDGLDpysGJ'LDM@{ff?74stNDKBYKpy?7SDB#R`{ff?7G!K;?#P2KLDMFG!MWB#RTv#?#jpyKLDG;pTGB#pTSDRyECDN/M@K!?KLDMWSND`MJjB#SDC
{ff?74stRTMffHpyK|EO?#PGND{fLK!MFG!KGJI'A*LDpT{fL{JB#SB#RTG?	{JB#NDG!MNDS/jMFRTpTBYtRyM"jMFGNtRyKGJ?74M4?#PKL/MB#Ryv#?#jpTKLDG
tB#G!MFC?7SKLDpGWBYstsDj?7B#{L?#tKB#pTSGpT4sRTpyDMFC	?C/MFRGgQCDM(B#4s?7GJI9kFm#m#niC/MB#4s?7GWN/MJK!M#I
kFm#m7uiWMFpTv#MJjFIhBY4hiMFBYjRI9kFm#mYHIkFm#m# WN/MJK!MC/MB#s`?7GFIikFm#m#7ofIA@L/MJjMFB#GW?#KL/MJjBYjM4C/Mff
Gpyv7SDMFCP?#j9v#MFS/MJjfB#R/*)G)gQC/M@B#4s?7G9_WN/MJK!M#IuY###BHtL/MFS/v/Iu;MFRRD pTN2I/kFm#m7uDMJMJwI`kFm#m#
stpTjK!MFGJIt)RyEH4?7N/j*/{L/MFpTSDMFGJIkFm#m#t9MJj}BqhiMFBYjRIkFm#mYH`MJj"N/KL'B#N/jpyK!JMFSIkFm#n#7of
LDMB#RTv#?#jpyKLDG"tB#GMFC?7SrBG{ff?#jpTSDvOPQNDSD{ffKpy?7SBYK!K!MFsDK4K!?tSDCBv#jBYstLKLtBYKBHppyJMFG
KL/MG!MFRTMF{ffK!MFCG{ff?#jM#KL/MG{ff?#jpTS/vPQNDSD{ffKpy?7SpGNDGNDB#RTRTEC/MJtS/MFCB#GB4MFB#GN/jM4?#PtK`MJK|A;MJMFSKL/M
v#jBYstLB#SDCKLDMCtBYKBHWRTRW?#PlKL/MFNDGMBG{ff?#jfpTS/vPNtSD{ffKpy?7SpTS{ff?7ltpSDBYKpy?7SA*pyKLBG!MFBYj{fL
4MJKL/?HCpTSr?#jCDMJj}K!?4MFB#GN/jMKL/MOv#?u?HCDS/MFGGq?#PMFB#{LMffHstRy?#jMFCG!K!jNt{ffKN/jMPj?7KL/MGstB#{ffMO?#P
PMFB#GpytRyMG!?7RTNDKpy?7SDGJiWNDjpTS/vWKL/MMffsRy?#jBYKpy?7SsDj?{ffMFGGFI#KL/M(G{ff?#jpTS/v)PQNDSD{ffKpy?7SpTGBYsDsRTpyMFC"pTS"?#jC/MJjK!?
MJYB#RTNDBYK!M(KL/M(DKSDMFGG9?#P`MFB#{L}{JB#StCDpTCDBYK!M(G!K!jNt{ffKN/jM(K!?KL/MCDBYKBH;B#{LB#RTv#?#jpyKLDpTG9{fLDBYjB#{ffK!MJjpTJMFC
EKL/MG!sMF{Jpyt{G{ff?#jpTSDvPNDSt{ffKpy?7S_B#SDC_G!MFBYj{fLsDj?{ffMFCDNDjMNDG!MFC2LDMxG{ff?#jpTS/vPQNDSD{ffKpT?7SDGBYjM
tB#G!MFC?7SCDpMJjMFSuKsDjfpTSD{JpystRTMFGJI GND{fLB#GMFSuK!j?#sEgQ@MJjG!w#?+HpyKG??#sMJjFI(kFm#mYH(LD?+A'pTN2I
kFm##nDC/M.B#4s?7GJI`kFm#m#ntMJtB#SDMWh'MFBYjfRI`kFm#n7#ofIB+E#MFGpTB#SBYsDstj?7B#{L/MFGgQNDSuKpTS/M#I`kFm#m/IkFm#m#
??#sMJjl*MJjGw#?+HpyKGJIkFm#m#9/jpTMFCDB#Sx?7RTRTMJjFIY##HDjpyMFCDB#SI>@B#{fLDB#ShiMMJ jFIkFm#m#m
WMFpyv#MJj*MF{w#MJjB#S2IqkFm#m#@MF{w#MJjfB#S2IkFm#m#@MF{w#MJjB#SIWMFpyv#MJjO(LDpT{w#MJjfpTS/v/IkFm#m#
OB#CDpyv7B#S,@BYPK!MJjE#IkFm#m/*B#4?7SDp.MJtB#G!KpTB#StpIkFm#m7uqK!MF{wIlY##[ofI?#j	KL/MOpTSDpT"ND
WMFG{ffjpystKpy?7S_MFS/v#KLgQ;?7Nt{wYBYMJjKJI}kFm#m#4/jpyMFCtB#SW?7RTCtG!FpTC/KJI"kFm#m# B#B#{J{fLuNDGFI
kFm#m//N/FN/wHpI2kFm#m#I'kFm#m#pTB#S2IDY##[of
LDMJjM;BYjMB#RTG!?)LuEuDjfpTC.B#Ryv#?#jpTKLDGKLtBYKNtG!M;BW{ff?7lpTSDBYKpy?7S?#PD{ff?7SDG!K!jB#pS[K=tB#GMFC"B#StCG{ff?#jfpTS/vY
tB#G!MFC4MJKLD?CDGF S"GMJ#MJjB#RHA;?#jwG(g=HpTSDv7L"B#RyK!?#jKBHI`kFm#m#ItkFm#m#HHstpyjK!MFG_	MJMJw`ItkFm#m#H)B#GL
WjNDFC/JMFRIkFm#m#m;C/Mz(B#4s?7GJIiDMJjSB# SDC/MJff'NDSDBhNDMJjKBHIY#77oKL/MzpTSDC/MJsMFSDCDMFSD{ffMff=tB#G!MFC
B#SDCrG{ff?#jpTS/vY=tB#GMFCB#Ryv#?#jpTKLDGBYjMOB#pTSuKB#pTS/MFCrB#GzG!MJstBYjBYK!MOsDj?H{ffMFGG!MFGJIA*LDp{LrBYjM{ff?7lpTS/MFC
pTSG!?74M4ABFE#IA@L/MJjMFB#G)KL/M4LuEuDjfpTCDpyFBYKpy?7SsDj?#s?7G!MFCOuE@{JpCB#StCCDM(B#4s?7GgY##HIiY#/k+o)pTG
tB#G!MFCx?7SxKL/MC/MJ#MFRT?#st4MFSuK.?#P(B	G{ff?#jpTSDvPNDSt{ffKpy?7SKLDBYKlNDB#SuKpyDMFGKL/MCtpTG{ffjMJstB#St{JpyMFG)MJKAMJMFS
KL/MpTSDC/MJsMFSDCDMFSD{ffMFG.CDpTGstRTBFE#MFCuEKLDMz{JB#SDCtpTCDBYK!MqSDMJKA?#jwB#SDCKL/MzCDBYKBYtB#G!M#IB#StCKL/MzG!MFBYj{fL
sDj?H{ffMFGGpG(RTpTpyK!MFCzEKL/MjMFGNDRTKG?#PG?74MlpTSDCDMJs`MFStC/MFSD{ffMWK!MFG!KGJ
SKLDpTG.stBYsMJjFI AM}P?{JNtG?7SxKL/MqG{ff?#jpTS/vuG!MFBYj{LBYsDsDj?7B#{fL2@RyKLD?7N/v7LxB#Ryv#?#jpTKLDG.pTSxKLDpTG
{JBYK!MJv#?#jELDBF#M4{ff?74?7StRyENDG!MFCRy?{JB#R'G!MFBYj{fLMJKL/?CtG"gQNDSuKpTS/M#IkFm#mHkY'??#s`MJj)@MJjG!w#?%pTKGJI
kFm#m#(LDpT{w#MJjfpTS/v/IWMFpyv#MJj4@MF{w#MJjB#SIkFm#m#;C/MB#4s?7G.MJK"B#RyIY#7;@MF{w#MJjB#SMJKB#RyI
kFm#m#7ofI@CtN/MK!?KLDM	Mffs?7S/MFSuKpTB#RTRTERTBYjv#MOGpyJM?#PKL/MG!MFBYj{fLG!stB#{ffM#I(KL/MJjMOpTG}Bv#j?+A*pS/vpTSuK!MJj!
MFG!K4pTS?#KL/MJjL/MFN/jpG!KpT{G!MFBYjf{L4MJKL/?HCDGJI9pM#GpNDRBYK!MFCB#SDSDMFB#RTpTS/vg=(LDpT{w#MJjfpTS/vMJK"B#RyI@kFm#m#7ofI
`+f(%== f(=%i==|= 	ff
fiT


fi

 $,.$ !

f #%$ff&(' *)

"$!$ $



KBYtNG!MFBYj{LgQ?7ND{wYBYMJjKJI*kFm#m#NDSuK!MFSDB#NB#NIY##[ofI9DjfB#SD{LB#SDCx?7NDSDCrgQ*pTB#S2I9Y##[ofI
v#MFS/MJKpT{qB#Ryv#?#jpyKLtG.B#SDCMJ#?7RTN/Kpy?7SDBYjEstj?#v#jB#pS/vgQ BYjjf,B SD+ BYv7BHI9h'?#FBH.I -N/jjB#4MFSDCDp=IiON/jv7B
 lNDp !s`MJjfGJI*kFm#m#.	E#MJjfGJI B#G!w#MJE MJHpyK!KJIkFm#m#m)?7S/v/I B#  MFNtS/v/IkFm#m#m7ofI@BYjw#?+
{LtB#pTS?7S[K!MxBYjRT?gQ?H{wYBB#G!K!MFRy?/I*Y#/kY.E#MJjG}MJKB#RyIkFm#m#m7ofI*YBYjpTBYRyM	S/MFpyv7Lu?#jL/??C
G!MFBYj{fLrgQC/MB#4s?7GWh;N/MJjKBHIY#/kJBH hNDMJjKBHIY#/k+ofI B#S[K{ff?7Ry?7SuE	?#sDKppyFBYKpy?7SgQC/M}B#4s?7GJI
/MJjS*B# SDC/MJff NtSDBHIB# 4MJ*h;N/MJjKBHIuY#7HhNDMJjKBHIuY#/k+ofIv#jMJMFC/E"jB#SDC/?7pTJMFC"B#CtBYsDKpy#M(G!MFBYj{fL
sDj?H{ffMFCDN/jMFGgQC/MqB#4s?7GJI DMJjSB# SDC/MJff'NDSDB	h;N/MJjKBHIiY#77ofIB#SDCMFGKpTBYKpy?7S?#P(CDpTGK!jpytN/KpT?7S
B#Ryv#?#jpTKLDG.gQRTB#SD{ff?/IS/FB BYjjB St+ BYv7BHIY#77of
WRTR9?#PKL/MFG!MqMF4stRy?%ECDpy`MJjMFSuK.G!MFBYj{fLMJKL/?CtGtN/KKL/MqGB#4M}G!MFBYjf{LGstB#{ffM#lKL/MqG!stB#{ffM}?#P
*)GFs?7GGpytRyMB#RyK!MJjfSDBYKpy#MpTGlKLDMqG!stB#{ffMz?#PKL/Mz?#jC/MJjfpTS/v7G.?#P(KLDMqYBYjpTBYtRTMFGqgQC/M	B#4s?7GJI
B# 4MJ*h;N/MJjKBHI[Y#7HC/M*B#4s?7G9@NDMJK!M#IuY##YuCDM(B#4s?7Gh;N/MJjKBHI[Y#/kffH/jpTMFCDB#S
?7RRyMJjFItY##H BYjjB St+ BYv7BHIt.NDp !sMJjG(ON/jv7BHI kFm#m#7ofSKLDpTGstBYsMJjFIDLD?+AMJ#MJjFIAMlBYjM4?#jM
pTSuK!MJjMFG!K!MFCpTSOKL/M4G!stB#{ffM4?#P;MFuNtpyB#RTMFSD{ffM"{JRB#GG!MFG)?#P*GqgQhiMFBYjR9MJjBHIkFm#mY[ofIipM#l{JRTB#GG!MFG
?#P9*)GWA*pyKLMFB#{fLjMJsDjMFG!MFSuKpTS/vBCtp`MJjMFS[K*GMJK?#PsDj?#tBYtpRTpyKEqCDpTG!K!jpTtN/Kpy?7SDGF*L/MJjM.pTGB#RG!?}B
SNDlMJj;?#P RyMFBYjSDpS/vlB#Ryv#?#jpTKLDGKLtBYK{JBYjjE?7N/KKL/MWGMFBYj{LpTS}KLDpTGGstB#{ffMgQ@SDCDMJjGG!?7S2IHB#Ctpyv7B#S
hiMJjRB#S2I kFm#m7u LDp{w#MJjpTSDv/I kFm#m#2WB#GL@jfN/FC/JMFRI kFm#m#mOB#CDpyv7B#S2I`@StC/MJjG!?7S2Ih'MJjfRTB#S
9?7RTpTSDGwuE#I2kFm#m#`spyjK!MFG( 	MJMJw`I kFm#m#7of;*LDpTG;PMFBYKN/jMWjMFCDND{ffMFGKL/MGpyJM)?#P'KL/MG!MFBYj{LGstB#{ffM#I
B#RyKL/?7NDv7LjMF{ffMFSuK"jMFGNDRyKGg=)pTRRTpTG!stpTM4hiMJjRTB#SI9Y#/k+o{ff?7S/tj KLtBYKKLDpG.jMFCDNt{ffKpy?7SpTG"S/?#KB#G
pT4s?#jKB#SuK9pTSK!MJjG?#PKL/M**G!sB#{ffMB#G9sDjMJHpy?7NDGRTElL/?#sMFC	gKL/M(jBYKpy??#P`KL/MSND`MJj9?#P*)G
K!?KL/MSuNDMJj?#PiMFuNDpTB#RyMFSt{ffM.{JRTB#GG!MFG@pTGRy?%A;MJj@KLDB#SP?7N/jofLDMsDjpT{ffMAMlLDB+#MlK!?}sBFEP?#j*KLDpTG
jMFCDNt{ffKpy?7SpTG*KLDBYK*KLDMlMJYB#RTNDBYKpy?7SO?#P9KL/M{JB#StCDpTCDBYK!M"G!K!jND{ffKN/jMFG*C/?MFG*S/?#KWKBYw#MB#C/B#SuKBYv#M?#PB#S
pT4s?#jKB#SuKWsDj?#sMJjKE?#PB#SuEOG{ff?#jpS/vqPQNDSD{ffKpT?7SDGJISDB#4MFRyECDMF{ff?74s?7GBYtpTRTpTKE#I2B#SDCKL/MJjMJP?#jMKL/M
{ff?#jjMFGs`?7StCDpTS/vB#Ryv#?#jpyKLtGBYjM.RyMFGG(MJ~}{JpyMFSuKJ
SKLDpTGstBYsMJj"AMsDj?#s?7G!MBS/MJA G!MFBYj{LGstB#{ffMA*LDpT{fLpTG{JRy?7GMFRyExjMFRTBYK!MFCK!?KL/MG!sB#{ffMz?#P
MFNDpyYB#RyMFSD{ffM{JRTB#GGMFG?#P/*)GJI#B#SDC.A*LDpT{fLA;M;LDBF#M{JB#RTRyMFCKL/MGstB#{ffM?#P`d!Zff\aQd]Q+aZV7JX7J ]'HVYdffaQ]QV# X
Y]d!ZFaZ#d!Vft7\OgQ@h*)GofMC/MJtS/M	BxRT?{JB#RGMFBYj{LB#Ryv#?#jpyKLDpTSKLDpTGGstB#{ffM#I(B#SDCrGL/?%A
KLDBYK4ENtGpTS/vBC/MF{ff?74s?7GBYRyMG{ff?#jpTS/vPQNDSD{ffKpy?7SIAM	{JB#SMJB#RNDBYK!MRy?H{JB#RTRyExKLDM	G{ff?#jM	?#PWKL/M
G!K!jNt{ffKN/jMFG9pTSKL/MS/MFpyv7Lu?#jL/??C?#PtKLDM{JNDjjMFSuK@h*I7KLuNtG?#DKB#pTSDpTSDvWB#SMJ~}{JpTMFS[KB#Ryv#?#jfpyKLD
A*LDpRyM}jMJKB#pSDpTS/vB#S[E?#P*KL/MB#C/YB#S[KBYv#MFG?#P@NDGpS/vMFNDpyYB#RyMFSD{ffM{JRTB#GG!MFG"?#PW*)GJ*PK!MJj4KL/M
?#jpyv7pSDB#R9GN/t}pTGGpy?7S?#P(KLDpGstBYsMJjFI(LDpT{w#MJjfpTS/vgY#77olsDj?#s?7G!MFCxB#SD?#KL/MJjRyMFBYjfSDpTS/v	B#Ryv#?#jfpyKLD
KLDBYKzG!MFBYj{LDMFG}pTSKLDMG!stB#{ffMO?#PWMFNDpyYB#RyMFSD{ffM	{JRB#GG!MFG?#P*GzB#StCA*LDpT{fLr{JB#SB#RG!?G{ff?#jMOKL/M
{JB#SDCDpCDBYK!MGK!jND{ffKN/jMFGRy?H{JB#RTRyE#I#NDGpTS/v@B){JB#S/?7StpT{JB#RjMJsDjMFG!MFSuKBYKpy?7S4G{fL/MF4MP?#j9MFuNDpTB#RyMFSt{ffM{JRTB#GG!MFGJI
{JB#RTRyMFCc /@TZJaZV7JX7J ]*HVYdffaQ]QVY X#]dZ+aZ4#d!Vft[\g=h*Gfof
LDM*jMFG!K;?#PKL/M@stBYsMJj;pTG?#jv7B#SDpyJMFCzB#GP?7RTRT?+A*GJG!MF{ffKpy?7SCDpTG{JNDGG!MFGG!?7M*sDjMFRpTpTSDBYjfpyMFG9B#SDC
KL/M)B#C/YB#S[KBYv#MFGB#StCzCtpTGB#C/YB#S[KBYv#MFG?#Pi{JBYjjEHpTS/vl?7NDK;KL/MG!MFBYjf{LzsDj?H{ffMFGGpTSqKL/MWGstB#{ffMFG?#P *)G
B#SDCzMFNDpyYB#RyMFSD{ffM{JRTB#GG!MFG?#Pi*)GJ(MF{ffKpy?7S	C/MFG{ffjpyMFGKLDMWv#jfBYstLDpT{JB#R`?#H!MF{ffKGJID*h;*)GJItKLDBYK
A*pTRR'MpTSt{JRTNDC/MFCpTSKL/M}sDj?#s?7G!MFCG!MFBYj{fLG!sB#{ffM#}SMF{ffKpy?7S/IB	C/MJKB#pRyMFCC/MFG{ffjpTsDKpy?7S?#PKL/M
Ry?H{JB#RG!MFBYjf{L4MJKL/?HCNDGMFCxK!?MffHstRy?#jMKLDpTGG!stB#{ffMqpG.sDj?%pC/MFC2	MF{ffKpy?7S	GL/?%A*G"L/?+AA;M{JB#S
MJYB#RTNDBYK!M)*h**)GMJ~}{JpyMFSuKRyENDGpTSDvlBlC/MF{ff?7s`?7GBYtRyM@G{ff?#jpTS/vPNDSt{ffKpy?7S2MF{ffKpy?7S.{ff?7SuKB#pTSDGKL/M
MffHs`MJjfpT4MFSuKB#R9jMFGNDRTKG)?#PKLDMMJB#RNDBYKpy?7Sx?#PKL/M}sDj?#s?7G!MFCB#Ryv#?#jpTKLD?7SxKLDM}@RBYjgQMFpTSDRTp{L2I
HN/MJjf4?7SDC/KJI.LDB+#MJ??#sMJjFI4kFm#n#m7ofI.StGN/jB#SD{ffMrgQpTSDCDMJjFI?7RTRyMJjFIW*NDGG!MFRTR..B#StBYFBFABHI
kFm#m7#oB#SDC@B#pTRTtSDC/MJj}gQ@DjB#G?7S2I9;j?+A*S2ION/jstLuExpTS/wHRyMJjFIkFm#m#7oS/MJKA?#jwHGJIB#G"A;MFRTR;B#G
?7SCDBYKBYtB#G!MFGPj?7KL/1
M 0OB#{LDpS/MMFBYjSDpS/vO*MJs`?7GpyK!?#jE#M	B#RTG!?pTSD{JRNDC/MqB#SMF4spyjpT{JB#R
{ff?74stBYjfpTG!?7SA*pTKL?#KL/MJj@GKBYK!Mff=?#P=KL/MffBYjK)RTMFBYjSDpTS/v4B#Ryv#?#jfpyKLDGJpTSDB#RTRyE#IHMF{ffKpy?7SO4{ff?7SuKB#pTSDG*KL/M
{ff?7SD{JRTNtCDpTS/vjMFBYjwGB#StCG!?74M.sDj?#s`?7GB#RTGP?#j*PN/KNDjM)A;?#jw`
2

fi X	ttu
8 7%9:;< :=<;> 5 ?>A@B<Lff>	D6 C 4 E6 
1 6  5 8
L/MGMFBYj{LsDj?H{ffMFCDN/jMFGlNDG!MFCA*pTKLDpTSBFE#MFGpTB#SS/MJK|A;?#jwRTMFBYjSDpTS/v	B#RTv#?#jpyKLDGlNDGNDB#RTRTE	?#sMJjBYK!M
?7SKL/M4G!sB#{ffM?#P**)GJ4SKLDpTG){ff?7S[K!MffHKJIiKL/M4sDj?#tRTMF{JB#Sx`MP?#jB#RTRTE	MffstjMFGG!MFCB#GJpy#MFS
B{ff?74stRyMJK!MK!jB#pTStpTS/vG!MJK	gQpM#AMCD?S/?#K{ff?7SDGpC/MJj"}pTGGpTSDv	B#RN/MFGl?#j4RTBYK!MFSuKBYjfpTBYtRyMFGfGo FIH
J  (KMLMLMLK ONGP.?#P9pTSDG!KB#St{ffMFG(?#PiBtSDpyK!MG!MJK?#R
P QYBYjpTBYtRTMFGJTI S4IDtSDCzKL/M*V
 UW)GND{fLKLDBYK
U W HBYjvB
_g U`
 Fo
gk+o
XZY[]\O^
3 54

A*L/MJjM ^ g_U`Fo;pTGBG{ff?#jpTS/v"PQNDSD{ffKpy?7S4MFB#GNDjpTS/vKL/M)DKS/MFGG?#P'B#SuEq{JB#StCDpTCDBYK!M*VUK!?4KL/M
CDBYKB#G!MJK*FIDB#StC1acbzpTGKL/M.PQB#pTRyEq?#PiB#RRKL/Ml*)G@A*pyKLBQxSD?C/MFG - 
OB#S[E}?#P`KLDM@G!MFBYj{fLzsDj?H{ffMFCDN/jMFGJIupTSD{JRTNDCtpTS/vWKL/M@{ff?7?7SDRyE4NDG!MFCqRy?{JB#RG!MFBYj{fLzMJKL/?CtGJI[jMFRTE
?7SBS/MFpyv7Lu`?#jfL/?u?HCG!K!jNt{ffKN/jM4KLDBYK"C/MJtS/MFGKLDM}Ry?H{JB#RjNDRTMFGg?#s`MJjfBYK!?#jGfoNtG!MFCK!?O?+#M}A@pyKLDpTS
KL/M4G!MFBYj{fLG!stB#{ffM#lLDMG!KB#SDCtBYjCS/MFpyv7Lu`?#jfL/?u?HCpSKL/MG!stB#{ffM?#P*)G.NDGMFG@KLDM?#sMJjBYK!?#jG)?#P
BYj{@B#CDCDpyKpy?7SI[BYj{*CDMFRyMJKpy?7S}B#StC}BYj{*jMJ#MJjGB#RIKL/MJjMJEBF#?7pCDpTS/vgQpTS4KL/M@DjG!KB#StC}KL/M@KLDpyjC}{JB#G!M%o
KL/MlpSD{JRTNDGpT?7S}?#PCDpyjMF{ffK!MFC{ffE{JRyMFG@pTSKL/Mv#jBYsL2
LDMxB#Ryv#?#jpyKLD}GKLDBYKG!MFBYj{fL,pTSKLDMxG!stB#{ffMx?#P*)GNDGpTSDvRT?{JB#R4MJKLD?CDGBYjMMJ~}{JpyMFSuK
B#pTStRyEMF{JB#NDG!Mz?#P*KL/MC/MF{ff?74s?7GBYpTRTpyK|EsDj?#sMJjKEKLDBYK4B#S[EG{ff?#jpTSDvOPQNDSD{ffKpy?7StGlMff/LDpytpTKJ
G{ff?#jpS/vlPQNDSD{ffKpy?7S ^ pTGGB#pTCqK!?M4[Zfc /@Hc\JV[ffTZpTP2KL/MWG{ff?#jM)?#P B#S[EqB+E#MFGpTB#S	SDMJKA?#jw}G!K!jNt{ffKN/jM
B+E4`M*MffsDjMFGG!MFC}B#GBsDj?CDNt{ffK@g?#jBlGNDpS4KL/M@RT?#vYG!stB#{ffM%o?#PRT?{JB#RG{ff?#jMFGpTS[#?7RTpTSDv?7SDRyE?7S/M
S/?HC/M.B#SDCpyKGstBYjMFSuKGJ
_g U`
 Fdo Hfe ^Tj lg k KnmGo X lg kDo!o
g7o
^
g Yih

lg k KnmGo X glkDo!o]H
glk KnmGo X glk/o;p gq r=st + g 1 o
g7o
^
A*L/MJjMup gq r=svt + g 1 BYjM"KL/M4G!KBYKpTGKpT{JG)?#PKL/MYBYjpTBYtRyMFGwkOB#StC mxo X glk/o@pTSyFIp=M#KL/M4SuNDMJj@?#P
pTSDGKB#SD{ffMFGpTScFKLtBYK;BYK{fL}MFB#{fL}s?7GGpTtRyM(pTSDG!KB#SuKpTBYKpy?7S?#=P kB#StC mGo X glkDof mGo X glkDoA*pTRRCDMFS/?#K!M
KL/M.sBYjMFS[KGMJK?#PRkzpTSKLDM.*z
 UIDpM# mGo X lg k/{o H J|(} S~ |{ k } UP7
sDj?H{ffMFCDN/jMKLDBYK;{LDB#S/v#MFG?7S/M*BYj{(BYKMFB#{L}?+#M@{JB#S}MJ~}{JpyMFSuKRyElMJYB#RTNDBYK!M*KL/M*pT4stj?+#MF4MFSuK
?#DKB#pTSDMFCEKLtpTGq{fLDB#S/v#M# HND{L_BsDj?{ffMFCDNDjMO{JB#SjMFNtG!MKL/M{ff?74stN/KBYKpy?7StGz{JBYjjpyMFC?7N/KBYK
sDjMJHpy?7NDG G!KBYv#MFGJI[B#SDCl?7SDRyE)KL/MG!KBYKpTG!KpT{JGi{ff?#jjMFGs`?7StCDpTS/vK!?WKL/MYBYjpTBYtRTMFGA*L/?7GMstBYjMFSuK'G!MJKGLDB+#M
MJMFS4?HCDpyDMFCS/MJMFC	K!?4`MjMF{ff?74stN/K!MFC2;L/MB#CDCDpyKpT?7S?#j*C/MFRyMJKpy?7S?#P9B#S	BYjf{   kpTSB*
U {JB#SzKL/MJjMJP?#jM)M@MJYB#RTNDBYK!MFCzE}{ff?74stNDKpTS/vl?7StRyE4?7S/MS/MJARy?{JB#RG{ff?#jM#I
lg k KnmGo X( lg kDo!o?#j
^
j
n
K
G
m
o
lg k
{   k4jMFNDpyjMFGKL/M
Xn lg k/o!ofIHjMFG!sMF{ffKpy#MFRTE#*L/M*MJYB#RTNDBYKpy?7Sq?#P2KL/M@jMJ#MJjGB#R`?#PB#SzBYj	
^{ff?7
j 4stNDKBYKpy?7S?#PKA?qS/MJART?{JB#RG{ff?#jMFGJI
lg k Knmxo X(n lg k/o!o(B#SDC
lg  KnmGo X  lg 2o!of
KlGL/?7NtRTC`MSD?#K!MFCxKLDBYK.MFB#{fLGK!jND^{ffKj N/jMpSKL/M}*G!stB#{ffM}^ j pG.S/?#K.B#RyABFg EHGlCDpy`MJjMFSuKPj?7
KL/M?#KLDMJjGlpTSxK!MJjG.?#P(pyKGljMJsDjMFGMFS[KBYKpy?7S{JBYstBYtpTRpyKE`.pyPA;MzpTS[K!MJjsDjMJKKL/M}BYjf{JGpTSBO*B#G
{JB#NDGB#R/pS[K!MJjB#{ffKpT?7SDGMJKAMJMFSBYjpBYtRyMFGJIYKL/MFSMFB#{fL*jMJsDjMFG!MFSuKGBCDpMJjMFS[K4?HC/MFR7L/?%A;MJ#MJj+I
pyP2AMWGMJMWB"* B#GB"G!MJK;?#P'C/MJsMFSDC/MFSD{ffM pTStC/MJsMFSDC/MFSD{ffM(jMFRBYKpy?7SDGLDpTstG9MJKAMJMFSBYjfpTBYtRyMFGWgKLDBYK
sMJjpyKG;NDG;K!?"PQB#{ffK!?#jpyJMB?7pS[K;sDj?#tBYtpRTpyKE4CDpG!K!jpytNDKpy?7SofI#KL/MFSCDpMJjMFSuK;*G(BFEqjMJsDjMFG!MFSuK
KL/MGB#M4?CDMFR"#MFSpTSKL/M{JB#G!M4?#PNDGpS/vB{JB#NDGB#R9pTSuK!MJjsDjMJKBYKpT?7S2I pyPAM}NDGM"?#G!MJjYBYKpy?7SH
?7SDRyEzCDBYKBgQB#G*?#sDs?7G!MFCK!?MffHsMJjpT4MFSuKB#R2CDBYKB4A*LDMJjM.G!?74MYBYjpTBYtRTMFG(BFEM.B#SDpysNDRTBYK!MFCofIHpyK
F
 '=!v %i D ff %  M=' ,% %D  .=] v  =" v ] = #v % ] D=)=%d ;   
 Dfff
 #d + = wM  
^j



fi

 $,.$ !

f #%$ff&(' *)

"$!$ $



Tp G(NDpyK!M.{ff?7?7S	P?#jKA?}BFE#MFGpB#SOS/MJK|A;?#jwG*K!?MMF4stpyjp{JB#RTRyEpTSDCtpTG!KpTS/v7NtpTGLDBYtRTM#2L/MFSKA?
*)GwU B#SDC1U{JB#SjMJstjMFG!MFSuKKLDM)GB#4MG!MJK(?#P'{ff?7SDCtpyKpy?7SDB#R`pTSDC/MJsMFSDCDMFSD{ffM@B#GGMJjKpy?7SDGJIHAMGBFE
KLDBYKKLDMFG!Ml*)GWBYjMzZ"JH]lVYTZff^ta 0 gQhiMFBYjR' 9MJjBHIikFm#mY[ofI`B#SDCAMlC/MFSD?#K!M.KLDpTG(B#GwUU  
L/MFS	RyMFBYjfSDpTS/vBFE#MFGpTB#SOS/MJKA?#jwHGPj?7 CDBYKB4NDGpTS/v4G{ff?#jpTSDv"PQNDSD{ffKpT?7SDGJIHKA?}CDpMJjMFSuK.gtN/K
MFNDpyYB#RyMFS[Kfo;*GBFEzMWpSDCDpTG!KpS/v7NDpTGLtBYtRyM#I7CDN/MWK!?4KL/M)Mff/pTG!K!MFSD{ffM)?#P'pS[YBYjpTB#SuKsDj?#sMJjKpyMFG;?7S
MFNDpyYB#RyMFS[KW*)GJI2EHpyMFRTCtpTS/v4MFNDB#R'G{ff?#jMFGJ*M{ff?7NDRTCOKBYw#M4B#C/YB#S[KBYv#M?#PKLDpG*pTS?#jC/MJjWK!?qv#MJKB
4?#jMljMFCDND{ffMFCG!sB#{ffM.?#P9G!K!jND{ffKN/jMFGK!?MMffstRT?#jMFC2
LDMWP?7RTRy?%A*pTS/vKL/MJ?#jMF stj?+HpTC/MFGBv#jBYstLtpT{JB#R{ffjpyK!MJjpy?7SzP?#jC/MJK!MJjpSDpTS/vKL/M)MFuNDpTB#RyMFSt{ffMW?#P
KA?q**)GJ
n[ff(ff=ff, b5
c 	5;\.V#d!Z"Z JHl] VYTZff^ta;] VY^cY^tXz] aHZffX}V YZ
u ,
aHZl\JV /}Zl\e#ZJyZFa=cY^xVY^aHZl\JV /}Z +|\aQdDFaQHd!Zffi\ 
L/Ml\e7ZffTZJa=cY^4?#PB* pTG;KL/M)NDSDCDpTjMF{ffK!MFCv#jBYsL}KLDBYKjMFGNDRyKGPj?7pyv7S/?#jpS/vlKL/MWCDpyjMF{ffKpT?7SDB#RTpyK|E
?#PiMJ#MJjEMFC/v#M#;z
 +|\aQdDFaQHd!ZpTSB*
 UpTG(B#S?#jC/MJjMFCK!jpystRTMJK;?#P9S/?HC/MFGJIlg  K k Kn ofI`GND{L	KLDBYK

gk+{o U{ff?7S[KB#pTStGKL/M)BYj{JG  kB#StcC k5  IHB#SDCg7o9KL/MWS/?HC/MF]G B#SDC  BYjM@S/?#KB#CB#{ffMFS[KpTcS U
 HZV7Ya=cY/ZV7.HV#aaZJdf^gQGL/?#jK!MFS/MFCLHLopTS}B*8
 UpTG;B#S}?#jfC/MJjMFCqK!jpystRyMJK?#P S/?HC/MFGJI`lg  K k Kn ofI
GND{fLKLtBY*K U{ff?7SuKB#pTSDG@KL/MBYj{J	G   k	B#St
C k  W>*?#K!M"KLDBYK)pTSB#SLHLOstBYK!K!MJjSlg  K k Kn o*KL/M

S/?HC/MFffG B#SDC {JB#S`M.B#CY!B#{ffMFSuKJ
WS/?#KL/MJj"{LDBYjB#{ffK!MJjfpyFBYKpy?7Sx?#PMFNDpyYB#RyMFSuK.*)GAB#GlsDjMFG!MFSuK!MFCxE(LDpT{w#MJjfpTS/vxgkFm#m#7ofI9K!?Y
v#MJKL/MJjA*pTKLsDj?u?#PKLtBYK)GMJ#MJjB#R9G{ff?#jpTSDvqPQNDSD{ffKpy?7StG@NDGMFCOP?#j.RyMFBYjStpTS/v}BFE#MFGpTB#SxS/MJK|A;?#jwHG)Pj?7
CDBYKBv7py#MWKL/MWGB#4MWG{ff?#jM@K!?MFNDpyYB#RyMFSuK;G!K!jNt{ffKN/jMFG)gQGND{LqPQNDSD{ffKpy?7SDGBYjM){JB#RTRyMFC	\ffc#d!ZnZ FH] YVYyZJ^ta
^`FaQ]cY^/\!of
LDM.{ff?7SD{ffMJsDK*?#PMFNDpyYB#RyMFSD{ffM?#P9*)G@stBYjKpTKpy?7SDG(KL/MG!stB#{ffM.?#P9*)GWpTSuK!?}BG!MJK*?#PiMFNDpyYB
RyMFSD{ffM{JRTB#GGMFGJ@L/MFS/MJ#MJj.BzG{ff?#jMMFNDpyYB#RyMFSuK*PQNDSD{ffKpy?7SpTG*NtG!MFC2IpyKWGMJMFG)SDBYKN/jB#R'K!?G!MFBYj{LP?#j
KL/MWMFG!K;{ff?7S/tv7N/jBYKpy?7SzpTSqKLDpTGS/MJAGstB#{ffM@?#P MFuNDpTB#RyMFSt{ffM@{JRTB#GGMFG;?#P'*)GJ*LDpTG{fLDB#S/v#MpTSqKL/M
G!MFBYj{fLG!sB#{ffMlBFEDjfpTS/v4G!MJ#MJjB#RB#CDB#SuKBYv#MFGJ
 *L/M.G!stB#{ffM.?#PMFNDpyYB#RyMFSuK{JRTB#GG!MFGpG4?#jMjMFCtND{ffMFCKLDB#SKLDMlG!stB#{ffM.?#P*)GgQB#RyKL/?7NDv7L	pyK
pGG!KpTRTRMFSD?#j4?7NDGfofM{ff?7NDRTC	KL/MJjMJP?#jM.MffHs`MF{ffK*K!?}?#DKB#pTS	`MJK!K!MJj*jMFGNDRyKGgA*pyKLKL/MGB#4M
GMFBYj{LMff`?#jKfof
 WG"A;MC/?S/?#K"Gs`MFStCKpT4Mzv#MFS/MJjBYKpTSDvgQNDGpS/vKL/Mq?#sMJjBYK!?#jGC/MJtS/MFCK!??+#MMJKAMJMFS
SDMFpyv7L[?#jpS/v{ff?7S/Dv7N/jBYKpT?7SDGpTS_KL/MG!MFBYj{LG!stB#{ffM%oB#StC_MJB#RNDBYKpTS/v,gQNDGpTS/vKL/MxG{ff?#jfpTS/v
PQNDSD{ffKpT?7So`MFNDpyYB#RyMFS[Ki*)GJI#AM{ff?7NDRTC.?#tKB#pTSl4?#jMMJ~}{JpyMFSuKB#Ryv#?#jpyKLD}GJ'@?+AMJ#MJjFI[B#G'KL/M
jfBYKpy?l?#P2KL/MWSuNtlMJj?#P2MFNDpyYB#RyMFSD{ffM*{JRB#GG!MFG;K!?KL/M@SuNDMJj?#P*GGMJMFGWgMFstpyjpT{JB#RRyE/o
K!?OB#G!EsDK!?#K!M}K!?H#7g=)pTRRTpTG!stpTM" hiMJjRT}B#S2I'Y#/k+ofIKL/MMJ~}{JpTMFSD{ffEpT4sDj?%#MF4MFSuK.BFE
M.G}B#RTR
 *L/MlG!MFBYj{fLpTS	KL/MlG!stB#{ffMl?#P**)G@}BFEMMFB#GpTRyEzK!jBYsDsMFC	pSBRy?H{JB#R2?#sDKpT"NDItB#StCKL/M
GpyKNDBYKpy?7SA?#jG!MFSDGB#GKL/M?#sMJjBYK!?#jGC/MJtSDMFCP?#jKLDpTG"G!stB#{ffM{JB#S4?%#M`MJK|A;MJMFS{ff?7SDDvY
NDjBYKpy?7SDG.{ff?#jjMFG!s?7SDCDpTSDvzK!?OMFuNtpyB#RTMFS[Kl**)GzgA@LDpT{LxA*pTRTRM4MJYB#RTNDBYK!MFCA*pyKLKL/MzGB#4M
G{ff?#jM%ofLDpG9CDpy~}{JNDRTKE{JB#S}MstBYjKpTB#RTRyE"BF#?7pTC/MFCzpyPAM@G!MFBYj{fLqpS4KL/M@GstB#{ffM*?#P2MFNDpyYB#RyMFSD{ffM
{JRB#GG!MFGJ
+`+ ffff+=f=if*=9==
 Ti

TM
Y 
	
"v
fi[" )=; =%=|=
 "; fifiT" c
"v
fi
 22f(O
 fff   !Q; %%; v % ,2= (f='=Q; |= 	f. %='=%'l ` Oi !" *= += J`=%
=f( %= Y   Q; %%; v %ff ll=( ((v l =Y;  u|=ft=% f;  %;=ff  =|= G
= yfff  * D %=Q; = *%; %ff f` Q; +; v  +=l ( v  %=" D J  . v +


fi X	ttu
LDMzCDpGB#C/YB#S[KBYv#MFG4BYjMKLDBYKJI;pTSKLtpTGGstB#{ffM?#PMFuNtpyB#RTMFSD{ffMz{JRB#GG!MFGJIpTK"pTG"4?#jMMffHsMFSDGpy#M
K!?v#MFS/MJjfBYK!MlS/MFpyv7Lu?#jpTS/v{ff?7S/Dv7N/jfBYKpy?7SDGJI/MF{JB#NDG!MAM.B+EzMP?#j{ffMFCK!?4sMJjP?#j G!?74MlwpTStCq?#P
{ff?7SDGpG!K!MFSD{ffE{L/MF{wI7pS?#jC/MJjK!?@MFSDGN/jMKLDBYKKL/MFGM;{ff?7S/Dv7NDjBYKpy?7SDG2jMJsDjMFG!MFSuK MFNDpyYB#RyMFSD{ffM{JRTB#GG!MFG 3 
pTSB#CDCDpTKpy?7S2I[KLDMWMJYB#RTNDBYKpT?7S?#P KL/MS/MFpyv7Lu?#jpTS/v"{ff?7S/Dv7N/jfBYKpy?7SDGBFEB#RTG!?"M4?#jM)MffsMFSDGpy#MWpTP
AMlBYjMlS/?#K@BYtRyMK!?KBYw#M"B#C/B#SuKBYv#M"?#P'KL/MC/MF{ff?74s?7GBYtpTRpyKEqsDj?#sMJjKEz?#PiKL/MlG{ff?#jpS/v4PNtSD{ffKpy?7S2
pTSDB#RTRTE#I KL/MS/MJAGMFBYj{LG!stB#{ffMpyv7LuKpS[K!j?HCDND{ffMqS/MJARy?H{JB#RBHpBKLDBYKBYjMS/?#KlsDjMFG!MFS[K"pTS
*G!stB#{ffM#
S?#jC/MJj4K!?xC/MFGpTv7SB#SMffHstRy?#jpS/vOstj?{ffMFGGP?#j4KL/MG!stB#{ffM?#P@MFuNtpyB#RTMFSD{ffMz{JRB#GG!MFGAM	{ff?7NDRTC
NDG!MqKA?CDpTG!KpTSt{ffKlBYsDsDj?7B#{fL/MFGJKLDMqDjfG!K{ff?7SDGpTG!KG"pTS{ff?7SDGpTC/MJjpTSDv	KLDBYKB#SMFNDpyYB#RyMFSD{ffM{JRTB#GG"pTG
jMJsDjMFG!MFS[K!MFCEqB#SuEz?#P'pyKG{ff?74s?7S/MFSuKGlgQpTSzKLDpTG{JB#G!M#IDpTKpGSDMF{ffMFGGBYjEzK!?BF#?7pTCMJYB#RTNDBYKpS/v44?#jM
KLDB#Sz?7S/M{ff?74s?7S/MFSuKs`MJj({JRTB#GGfofDB#SDCzKL/MG!MF{ff?7SDC{ff?7SDGpTG!KGpTSzNDGpTS/v"B4{JB#S/?7SDpT{JB#R`jMJstjMFG!MFSuKBYKpy?7S
G{fL/MF4M.P?#jKL/Ml{JRB#GG!MFGJ
LDMv#jfBYstLDpT{JB#RW?#H!MF{ffKG	{ff?74?7StRyENDGMFCK!?jMJsDjMFG!MFSuKMFNDpyYB#RyMFSD{ffM{JRTB#GG!MFG?#P"*)GBYjM
V7FX#F]W/VYdaQ]VYX#]dZ+aZz#d!Vft7\"gQhiMFBYjR99MJjBHI9kFm#mY[ogwHS/?+A*SB#G {	5;\!of*LDMFG!Mv#jfBYstLDG
{ff?7SuKB#pTS`?#KLOCDpyjMF{ffK!MFCgQBYj{JGfo*B#SDCNDSDCDpTjMF{ffK!MFCgQRTpS/wGoMFC/v#MFGJItN/K@S/?zCtpyjMF{ffK!MFC{ffEH{JRyMFGJ)py#MFSB
h**
 C/MJtS/MFC?7SB.tStpyK!MG!MJK?#P2S/?HC/MFDG SB#SDCBlSD?C/ffM k } S4IuKL/MP?7RTRy?%A*pTS/vGN/G!MJKG?#P2S/?HC/MFG
BYjMlCDMJtS/MFC2
mxo lg kD]
o H J|ff} S~ |] k } P7ItKLDM.G!MJK?#P'HVYdZff^ta\(?#RP k`

 lg kD]
o H J|ff} S~k |} P7ItKLDM.G!MJK?#Pff[]yYd!ZJ^}?#RP k`
x

o H J|ff} S~k` |} P7IDKLDMlG!MJK?#P^`ZJ]#HcYdf\?#.P k`
 p lg kDd
 lg kDff
o H J|} S~ |Z k }  K ?#Zj k |}  ?#	j k |} P7I2KL/MG!MJKW?#P@V7 JV7Zff^ta\WK!? k`
*
 uHpy?7NDGRTE *  lg k/{
o H mGo? lg kDDo  x  lg kD=o Bpy  lg k/of
WSOBYjx{   k	pS	Bq**
 UpGTc /*DZffyZpyP9pyK@BYsDsMFBYjG@pTSMJ#MJjEO*MFRy?7S/v7pTS/v4K!?qKL/MGB#4M
MFNDpyYB#RyMFSD{ffM4{JRTB#GGB#G U"WSBYj{   kpT
S UpGWGB#pTCK!?MqdiZ #Zffdf\f]=ffTZpyPpTK)pGS/?#K{ff?74sMFRTRyMFC2I
pM#.KLDMJjM4pTG)Bz**
 U MFNDpyYB#RyMFS[KWK!
? UKLDBYK.{ff?7SuKB#pTSDG)KLDMBYjf{ k`lWG)MJ#MJjEO** pTSB
stBYjKp{JNDRTBYj*MFNDpyYB#RyMFSD{ffM"{JRTB#GGWLDB#GWKL/MGB#4M"GMJKW?#P{ff?7s`MFRRyMFCB#SDCOjMJ#MJjGpytRyM"BYj{JGJIBq{JB#S/?7StpT{JB#R
jMJsDjMFG!MFS[KBYKpT?7Sq?#PB#SzMFNDpyYB#RyMFSD{ffMW{JRTB#GGpTGKLDMWh;* {ff?7SDGpTGKpTS/vl?#PB#SBYj{@P?#j;MJ#MJjEz{ff?74sMFRTRyMFC
BYj{pTSzKL/M)MFNDpyYB#RyMFSD{ffM{JRTB#GGFIHB#StCB4RTpTS/w4P?#j(MJ#MJjEzjMJ#MJjGpyRyMWBYjf{YLDpTG;wHpTSDCq?#P'jMJstjMFG!MFSuKBYKpy?7S
LDB#GMJMFS}v7pT#MFSzGMJ#MJjB#RSDB#4MFGJiHV7aaZffd^Og=stpyjK!MFG; 	MJMJw`IkFm#m#7ofI'fc /@yZFaZ {w%Mg D	%o
g=LDp{w#MJjpTSDv/IkFm#m#7ofIWZff\\FZff^taQ]VYi7dVtgQ@StC/MJjGG!?7SxMJK"B#RyIkFm#m7u;)B#GL@jfN/FC/JMFRIkFm#m#m7ofOWG
B{ff?7StG!MFuNDMFSD{ffM}?#P(KL/MJ?#jMFkYIB{ff?74sRyMJK!MFCh;*s?7GG!MFGG!MFGB#SBYjc{   kpTPB#StC?7SDRyEpyP(B
K!jpysRyMJK;?#PiS/?HC/MFGlg  K k Kn o;P?#jfGB[G!K!jfND{ffKN/jMW?#j(KL/MBYj*{   k}pTGjMFNDpyjMFCqK!?4MWCDpTjMF{ffK!MFCCDN/M
K!??#KL/MJj*[G!K!jfND{ffKN/jMFGlgK!?qBF#?7pTCP?#j}pTS/v4BS/MJA_uG!K!jND{ffKNDjM?#j*{ffjMFBYKpTSDv}BCDpyjMF{ffK!MFC{ffE{JRyM%o.gQG!MJM
pyv7N/jMk+of
>@?#K!MKLDBYKqB#SBYjtpTK!jBYjEh*C/?MFGzS/?#KzSDMF{ffMFGGBYjpTRTEjMJsDjMFG!MFSuKqG!?7MOMFNDpyYB#RyMFSD{ffM{JRB#GG
?#P**GJIB#RyKL/?7NDv7LKL/MJjMqpTG"B?7SDMff=K!?Y=?7S/M{ff?#jjMFG!s?7SDC/MFSt{ffM}MJKAMJMFS{ff?74stRyMJK!MFCh**)G4B#SDC
MFNDpyYB#RyMFSD{ffMl{JRTB#GG!MFG?#P*GJ>*MJ#MJjKL/MFRyMFGGJI`{ff?74stRyMJK!MFCh**)G@BYjM{ff?7SDGpC/MJjBYtRyE4?#jM{ff?7
stRTp{JBYK!MFCKLDB#Sv#MFS/MJjB#R9h;*)GJ{LDBYjfB#{ffK!MJjpyFBYKpy?7S?#PKL/MG!sMF{Jpy{lsDj?#s`MJjKpyMFGWKLDBYK.Bh*
"NDG!K(#MJjpyPEpTS?#jC/MJjK!?}`M.B}{ff?74stRyMJK!MFC	h;*AB#G*?#DKB#pTS/MFCE@SDCDMJjGG!?7SMJK*B#R= gkFm#m7#of
 ,
 i==Y=%v #=f= 'v ; v  = ff|==f`"   ; 	=%ff .O Y" @fG  " v % Q= 
 T%%D
 @= O fd 9 ff  (=Q=%9 % 9" ,= = *


fi

 $,.$ !

f #%$ff&(' *)

"$!$ $

z



z
x

u

x

y

u

y
w

w

(a)

(b)

pyv7N/jMkY"gQB[o)BYvB#SDCgo4{ff?74stRTMJK!MFCh;*"(L/MBYj{JG
{ff?74sMFRTRyMFC2HKL/MlBYjf{ k  pTG(jMJ#MJjGpytRTM




I

 ffk



_B#SDC




BYjM


#lBB lZ,, V	%]\"Vc /@TZJaZ 	%] VY^	cY^tXz] 
]Qa\JV#aQ]\
	Z\la/Z;ffcYycYb9]^	cY^`Y]aQ]cY^/\fiff
 ]\VffVY]^	#d!Vft _] QZ W]Qa;fcY^ta=VY]^/\l^`cyHVYdffaQ]QV# XY]dZFaZJX7JTZ\M
 d Z/+aZZ\tJ=#X7d!FVfyZ}tc]^yZJY^uD7faZ	#ffdXqZV#iZ a#ZJZffddX}affVY^V#]cY^dqnZ c F/@DHVYc#9^`a=cZff^ta 4 aHcwZffdZzVY]\.d!Z}ffHaQbcYdc[VY^ cY^D]_!QZcY)^DcY\J^xZF/Zi#aQ] Zff#dZX4^`cF^`[YZ]\
fcY^D^`ZFaZ	ffXzVq ]^De 
  /ZcY^ 	i#HdV7aQ]QcY^   k  7c%Z\l^c7acFfJHdV\VY^]^#/Z}\ft#d!Vftc %
ff]yY#ZJHdfdXZ VY d! VY\ VY ^]k^#} /ZqcF\fDJH#dfd!Vf\qt]^c!V7w a)%yZ V\a.cY^Zcqa/ZWffc#dcY^ 	'#Hd!V#aQ]Qc#^/\#]\V#X7Z]^

u ,

x

z

x

z

x

z

y

y

y

(a)

(b)

(c)

x
t

z
y
(d)

ipTv7N/jMlLDMP?7N/j@CDpMJjMFS[K{ff?7SDDv7N/jBYKpy?7StG({ff?7S[KB#pTStpTS/vB#S	BYj{G  kpTSB{ff?7stRyMJK!MFC	h;*
 MJK;NDGpTRTRTNtG!K!jBYK!M(KL/MWB#C/B#SuKBYv#MFG?#PG!MFBYj{fLDpTS/v"pTSKL/MWG!stB#{ffM@?#P2MFuNDpTB#RyMFSt{ffM*{JRTB#GG!MFG;?#P*)G
jBYKL/MJjKLDB#SzKL/M)G!stB#{ffMW?#P *G(A*pyKLzBGpT4stRyM*MffHB#4sRyM#pyv7N/jM)CtpTG!stRTB+EGKL/M)G!MJK;?#P s`?7GGpytRyM
*)G4pS[#?7RyHpTS/vOKL/jMJMzS/?CDMFG J  K k Kn P7I;A*pyKLBYj{JG"`MJK|A;MJMFS  B#SDC  IB#SDCMJKAMJMFS kB#SDC  
L/M4tjG!K)KL/jMJM}**)GBYjMMFuNtpyB#RTMFS[KJSK!MJjG?#P(pTStC/MJsMFSDC/MFSD{ffMpTS/P?#jBYKpy?7SI2KL/MJERTMFB#CK!?
KL/M*GB#4M*pTSDCDMJs`MFStC/MFSD{ffMG!KBYK!MF4MFSu"K !`lg k Kn ~ 2olg k4B#SDC  BYjM*{ff?7SDCtpyKpy?7SDB#RTRTElpTSDC/MJsMFSDCDMFS[Kv7py#MF%S 2ofI
A*L/MJjMFB#G.KL/M}GKBYK!MF4MFS[#K !lg k Kn ~ $7oqlg kB#StC  BYjM}BYjv7pTStB#RTRyEOpSDC/MJsMFSDC/MFSuKfo@{ff?#jjMFG!s?7SDCDG)K!?OKL/M
P?7N/jKLO?7S/M#@LDMlP?7N/jW**)GBFE`M"GND}BYjpyJMFC	E?7SDRyEK|A;?CDpy`MJjMFSuK@{ff?7stRyMJK!MFCh*)GFI
GL/?%A*S	pTSpyv7NDjM/

&%



fi X	ttu
y

y

x

x

z

z

(a)

(b)

x
y

z

y

z
x

(c)

(d)

pyv7N/jMlD?7N/j@CDpy`MJjMFSuK*)G@A*pyKLKLDjMJM.S/?HC/MFGB#SDCK|A;?qBYj{JG
x

y

z

y

z
x

(a)

(b)

ipTv7N/jM/(A?}CDpy`MJjMFSuKMFuNtpyB#RTMFSD{ffM.{JRTB#GG!MFG*?#Pi**)G
WGiA;M{JB#SG!MJM#IYKL/MGMFBYj{LG!stB#{ffM(B+E.MjMFCtND{ffMFC.ElNDGpS/v*h**)GK!?@jMJsDjMFG!MFSuK'KLDM{JRTB#GG!MFGJ
pTSl?7N/j'Mff/B#4stRTM#IK!?WKA?){JRTB#GG!MFGpTSDGK!MFB#C.?#PDP?7N/j'{ff?7SDDv7N/jBYKpy?7StGJ7pyK'{JB#SM;G!MJMFSgQWSDC/MJjGG?7SlMJKB#RyI
kFm#m7#olKLDBYKlKLDMjBYKpy?O?#P(KL/M}SNDlMJj?#P**)G"K!?OKL/MqSuNDMJjl?#P({JRTB#GGMFGpTG#( 'k#kqP?#j"KL/jMJM
S/?HC/MFGJI[( 'kFn#zP?#jWP?7NDjWS/?HC/MFG@B#SDCO#m##nH)k 'n7Ym#	P?#jWD#MS/?CDMFGJpTSO4?#jMv#MFS/MJjB#R'K!MJjGJI`KL/M
jMFGNtRyKG9?#DKB#pTSDMFCEq)pTRTRTpG!stpyMB#SDC}hiMJjRB#SgY#/k+o;pTSDCDp{JBYK!MKLDBYKKLtpTGjBYKpT?lBYsDsDj?7B#{fL/MFGB.YB#RTN/M
?#P'RTMFGGKLDB#SP?7N/jB#GKL/M.SNDlMJj?#P'SD?C/MFG(pSD{ffjMFB#G!MFGJ*L/MNDG!M)?#P MFNDpyYB#RyMFSD{ffM{JRTB#GGMFGKL/MJjMJP?#jM
MFSuKB#pTRTG}{ff?7Su#MFSDpyMFSuKqGBFHpTS/v7G}pSMffsRy?#jBYKpy?7SrB#StCrMJYB#RTNDBYKpy?7SMff`?#jKJIB#RTKL/?7N/v7LKL/MOv7B#pTSrpTGqS/?#K
G!sMF{ffKB#{JNDRTBYj+
 SxKL/M?#KL/MJj"LDB#SDC2I'KL/M}NDG!M}?#PB{JB#S/?7SDpT{JB#RjMJsDjMFG!MFSuKBYKpy?7SG{L/MFM}B#RTRy?%A*GlNDG.K!?	MffsRy?#jM
KL/MG!stB#{ffMsDj?#v#jMFGGpy#MFRyErB#StCGEG!K!MF}BYKpT{JB#RTRyE#I@A*pyKL/?7N/KzRy?7GpTSDvB#SuENDS/MffHstRy?#jMFC{ff?7S/Dv7NDjBYKpy?7S
NDSDSDMF{ffMFGGBYjpTRTE#"*MJKN/jSDpTSDvqK!?	?7NDj.MffHB#stRyM#I RyMJK.NtGGN/sDs?7G!M"KLDBYK.KL/M4K!jfN/M4?HC/MFR9pTG)KL/M*
CDpTGstRTBFE#MFCpTSpyv7N/jM/B#StCOAMG!KBYjKKLDMGMFBYj{LA@pyKLB#SMF4sDKEv#jBYstLgA*pTKLS/?BYj{JGfof MJK
NDG(B#RTG?B#GGND4M)KLDBYK*KL/M.G!MFBYj{fL	B#Ryv#?#jpyKLt pTC/MFSuKpyDMFGKLDBYKB#SMFC/v#MMJK|A;MJMF
S OB#SDBC kqsDj?HCDND{ffMFG
KL/M4v#jMFBYK!MFGKlpT4sDj?+#MF4MFSuK.pTSKL/M4G{ff?#jM#KKLtpTG4?74MFSuKJIKL/M4K|A;?B#RTK!MJjSDBYKpy#MFGJI   kB#SDC

kgQ{JB#G!Mk}B#SDC{JB#G!MzpTSpyv7N/jMqIjMFG!sMF{ffKpy#MFRyEDofIiBYjM}MFNDpyYB#RyMFS[KJ MJKNDGS/?+AGN/sDs?7G!M
KLDBYK@A;MlCDMF{JpTC/M.K!?}{ff?7StS/MF{ffK*KL/MS/?CDMFZG B#SDC  `BYv7B#pTS	AMlLDB+#MK|A;?q?#sDKpy?7SDGJ{    ?#wj 1  
>@MJ#MJjKL/MFRyMFGGFIYCDMJs`MFStCDpTS/v?7S.KL/MsDjMJHpy?7NDGG!MFRTMF{ffK!MFCl{ff?7S/Dv7NDjBYKpy?7S2IA;M?#DKB#pTS.CDpy`MJjMFSuK?7N/K{ff?7MFG
KLDBYK*BYjMlS/?Ry?7S/v#MJj*MFuNDpTB#RyMFSuKlgQG!MJMlpyv7NDjMl7of
P)A;MLtB#C{LD?7G!MFS{JB#G!MkgKLNDG?#tKB#pTSDpTS/vMFpyKLDMJjq{JB#G!MkYTk	?#jq{JB#GMkY7ofI(AMA?7NDRTCrLDB+#M
MFRTpT}pTSDBYK!MFCKL/M}s`?7GGpytpTRpyKE	?#P(MffHstRy?#jfpTS/vKL/M}**    kIB#SDCKL/MJjMJP?#jM}KL/MqMffsRy?#jpTS/v
sDj?H{ffMFGGlA;?7NDRCLDB+#MqMJMFSxK!jfpT4MFC2WGlKLDM}K!jN/Mz4?HC/MFRpTGlsDjMF{JpTGMFRyEOKLtpTGl*gQ{JB#GMTkzpTS
pyv7N/jM}7ofIiKL/MFSKLDM}G!MFBYj{fLxsDj?H{ffMFGGlA;?7NDRCLDBF#MqK!?pTSD{JRTNtC/M4B#S/?#KL/MJjlBYj{q{ff?7SDS/MF{ffKpTSD1
v kB#StC 
*F =(Yn " .1,ff [=#% 9J;  +@ Mcv  = ff #  J* 9=%) = = ,f"  % %  =f- +M

/



fi

 $,.$ !

f #%$ff&(' *)

"$!$ $



gQ{JB#G!MFGkYTkYTkYI;kYTk?#jkY7ofI'MF{JB#NDG!uM kB#SDC  BYjM{ff?7SDCDpyKpy?7StB#RTRyEC/MJsMFSDC/MFSuK@v7pT#MFS'.K)KLDpTG
4?74MFSuKJI(B#S[ErRT?{JB#RGMFBYj{LsDj?H{ffMFGGA?7NDRTCrGK!?#s gQpTSBRy?{JB#R?#sDKpT"NDqofI`MF{JB#NtG!M	MJ#MJjErRy?H{JB#R
{LtB#S/v#MzgQBYj{.jMJ#MJjGB#R2?#j@BYj{jMF4?%B#RoA;?7NtRTCBYw#MlKLDM.G{ff?#jM.A?#jG!M#
Case 1

z

x

y

Case 2

z

x

y

1.1

z

x

y

2.1

z

x

y

z

x

y

z

x

y

2.2

z

x

y

1.2.1

z

x

y

1.2.2

z

x

y

1.1.1
1.2

 pyv7N/jMl;?H{JB#RG!MFBYj{fLpTSKLDM.G!stB#{ffM.?#P9*)GWpTGK!jBYsts`MFCBYK@BRy?H{JB#R2?#sDKpT"ND
?7StG!MFuNDMFS[KRyE#I?7N/jstN/js?7G!M@{ff?7SDGpTG!KGpTSqB#CDCDpTS/vl?#jjMF4?+HpTS/vMFC/v#MFGgMFpyKL/MJjRTpTS/wHG?#jBYj{JGfoK!?
KL/MWG!K!jND{ffKN/jM*A*pyKLD?7N/K9sDjNDStpTS/vKL/MWG!MFBYj{LzG!stB#{ffM)NDSDSDMF{ffMFGGBYjpTRTE#'M){ff?7NDRTC}KL/MJjMJP?#jM@pTSuK!j?HCDND{ffM
RTpTSDwGpSDG!K!MFB#Cq?#P BYj{JGgA*L/MFSqKL/MJjM)pTGS/?#K;MFSD?7N/v7LqpS/P?#jfBYKpy?7SK!?CDpTGKpTS/v7NDpTGL`MJK|A;MJMFSCDpy`MJjMFSuK
stBYK!K!MJjStG ?#PtBYjf{JGfofI#A*LDpT{fL.A?7NDRTCG!MJj#MB#G'K!MFstRTBYK!MFG'?#jCDESDB#}pT{RTpTSDw#MJjGK!?WMFuNtpyB#RTMFSD{ffMstBYK!K!MJjfSDGJ
L/MJE"jMJsDjMFG!MFS[K;B#S[EYB#RTpTC4{ff?7tpTSDBYKpy?7S?#PBYjf{JGA*LDp{L4jMFGNDRyKG9pTS4B*_MFRy?7SDv7pTS/v)K!?lKL/M@GB#4M
MFNDpyYB#RyMFSD{ffM.{JRTB#GGF
 ?u?#wHpTS/vBYv7B#pTSOBYK*KL/MlsDjMJHpy?7NDG(Mff/B#4stRyM#ItA;M.A?7NDRTC	sDj?H{ffMJMFC	B#GP?7RTRy?%A*GJB#GGNDpTS/v4KLtBYK*pTS
?7N/jG!MFBYjf{LG!sB#{ffM(KL/M(?#sMJjBYK!?#jG9?#PRTpTS/w.B#CtCDpyKpy?7SB#SDC4{ffjMFBYKpT?7S4?#P`L/LstBYK!K!MJjSDG9BYjMB+B#pRTBYtRyM#I#AM
A?7NDRTCDjfG!K*pTSD{JRNDC/M)KL/MRpTS/cw k`G!MF{ff?7SDCtRyE#ItA*LDMFS{ff?7SDGpC/MJjpTS/vKL/M"pTSD{JRTNtGpy?7Sz?#PB}{ff?7SDSDMF{ffKpy?7S
MJKAMJMF%S qB#StC  I7AM(A;?7NDRC4LDBF#MK|A;?l?#sDKpy?7SDGJI7GL/?+A*S4pS"pyv7NDjM/iKLDML/LstBYK!K!MJjS   Bk
B#SDCKL/M.stBYK!K!MJjS  kSKLDpTG@{JB#G!MlKL/MG{ff?#jpTS/v}PNDSt{ffKpy?7SA?7NDRTCB#GGpTv7S	KL/M.v#jMFBYK!MFG!KWG{ff?#jM
K!?KL/MLHLstBYK!K!MJjS   kI/KLNDG(?#tKB#pTSDpTS/vKL/M{ff?#jjMF{ffK**
[< T>t8  43H!<< 65S]7<=<<73 4 <`>T>t8 6 98:,
L/MG{fL/MF4Mz?#P*jMJsDjMFG!MFS[KBYKpT?7SKLtBYK"AMzA*pTRTR;NDG!MqpG"GRTpyv7LuKRyECDpMJjMFS[K"Pj?7 KLDMzP?#jB#RpTG ?#P
{ff?74stRTMJK!MFCh;*)GJO|KpTGlSD?#KS/MF{ffMFGGBYjEP?#j"MFB#{L{ff?7S/tv7N/jBYKpy?7S?#P(?7N/jG!MFBYj{LG!stB#{ffMgA*LDp{L
AM	{JB#RTR)d!Zff\aQd]Q+aZ
 {w%?#<j ;Z{w%oK!?{ff?#jjMFG!s?7SDCK!?xBCDpMJjMFSuKMFuNDpTB#RyMFSt{ffM{JRB#GGJKA?
CDpMJjMFSuK@h*GBFEq{ff?#jjMFG!s?7SDCzK!?KL/M)GB#4M)MFuNtpyB#RTMFSD{ffMW{JRB#GGJ9L/M)B#pTSqjMFB#G!?7SP?#jKLDpTG
pTG"MJ~}{JpyMFSt{ffEEB#RTRy?%A*pTS/vB#SMFNDpyYB#RyMFSD{ffM{JRTB#GGK!?MjMJsDjMFGMFS[K!MFCg?7SDRTEpTSG!?74M{JB#G!MFGo"uE
CDpMJjMFSuK@h*GJIDAMA*pTRTRtv7B#pTSpTSqMJ~}{JpyMFSD{ffEqK!?4MffstRT?#jM@KLDM)GstB#{ffM#MJP?#jMMffstRB#pTSDpTS/vlKLDpTGpTS
v#jMFBYK!MJj)C/MJKB#pTRI/RyMJK@NDG(C/MJtS/MKLDMl{ff?7SD{ffMJsDK?#P*h;*"
=cY^D? X}>@l] .A]Qa9B\JV#aQ]
\ 	Z\"aHZ;&ffc#ycYb9 ]^u= 	<c#C^Y ]QaQ]QcY^DF\ 	ff %V ]\	Vd!Zff\aQd]Q+aZE {w%D
;Z	%E] 	VY^`
HG k } S2 mGo lg kDJo HKI $ML py  lg k/do HK$!
 7c%Z\l^c7afcY^ta=VY]^VY^DXY]d!ZFaZFX7JTZ 

0121

>

N



fi X	ttu

 7c+Z\)^c#afcY^ta=VY]^V#^DXc @TZJaZJX4H^Y]dZFaZ}FX#FyZFi] QZ WVJX7JTZfcY^ta=VY]^D]^uqc#^DX ]^De+\
O ZP#]\a\.]^ aHZJ^Zff]QaHZffd g Do RQOcYd g oJSI $
u]\"fcY^Y]QaQ]cY^\a=V7aZ\"aHV#a(VY^VYd!
Z#P ]\a\l]^ cY^D Xq] .]Qa]\"ZJ]aHZJd(HV#dacVY^[|
/V#aaZffd^cYdla/ZffdZ"]\lVY^`c#aHZffd4VYd!<| cYd]y#]^V#aZOffXqVY^[|"HV#aaZJdf^&4 [cY]^uza=c


/

	





k



 

u~ mxo lk M~




k

mGo? l

H

i

!



u.

WG4B#S*h;* pTGBh**"IpyK4{ff?7NDRTCMz{ff?7StGpTC/MJjMFCK!?MBOjMJsDjMFG!MFSuKBYKpy?7S?#P@BG!MJK4?#P
g MFNDpyYB#RyMFS[Kfo*)GJM}KLDMJjMJP?#jM}NtG!KlC/MJtSDM4A*LDpT{fLKL/M}G!MJK?#P**)GpGjMJsDjMFGMFS[K!MFCuEB
v7py#MFS@h*z"IDpM#L/?%ACDpyjMF{ffKpy?7SB+E}M)v7py#MFSK!?4KL/MRTpTSDwGpTS pTSz?#jC/MJj(K!?MffHK!MFSDC	pyKK!?
B*L/MP?7RTRy?%A*pTS/v4C/MJSDpyKpy?7SzP?#jB#RTpyJMFGKLDpTG(pTC/MFBH
=[Z P[?aZff>@^/l\A]Qc#^BTc!* VUX] lWVY^`RNcY^D X} ] FBff Y) = C l] YZff^V {	5Z(b;Z\ffVYXaV#a@cV 	%U ]\"V#^
 VY^` U V YZ"a/Zl\ffV /}Z\fe#ZffTZJa=c#^ 
 O	   k]\VY^xVYd!]^ aHZffB^   k]\VY\JcVY^VYd!"]1^ U\=^`czV#d]\d!ZY]d!ZFaZF 
 c#d[VYZJ^d" a=c.U d!cFV Y#DZzZ aHUZ\ff7V c+/}Zff\lZ^[c#|a}u/ZJV#^`aZffaZffd!dV#^/aZ\ ^`_]ZJQbZ ua/"Z)H`V#adacFZJdfZ^D\f]\\zc}Y]d!ZFaQ]^uaHZ} ]^/e%\"]^ ]^
MA*pRTRtNDGXM ^x | ;g .o;K!?C/MFS/?#K!M)KL/MG!MJK?#P'**)GKLDBYK(BYjM)MffHK!MFSDGpy?7StG;?#PiB"v7py#MFSh*
 "
GHABK_ ZJd
a  Z"VY`
^ ;Z	%ff  HZJB^ ff
V 6^xb;ZJ |  !;g [.dZ ao	HS^I Z$bTlVY^Z"[Z P7aZJ^[Z}a=cqc[Ja=VY]^V 	5'] Z @a/ZZ P7aZff^D\f]cY^c.VYc^ ;Z{w%]\
e  G JXU Z K P7U aZff ^`} Y]^G^z | ];ga.VYod!Z}U nZ JH8l] UVYTZff ^=ga ] Z qVYaHZ}#] fWZJd!ZJ^ta w%;\aV#aWVY^Z}c[Ja=VY]^`Z.dTc / 

hYi

GL

Qg B[o"WG  LDB#G4S/?CDpTjMF{ffK!MFC{ffE{JRTMgQ{ff?7SDCDpyKpy?7SpTSWMJtSDpTKpy?7Sk+ofIKL/MFSMFpyKLDMJj  pTG4B#RyjMFB#CDEB
*?#jpyKLtB#GlG!?74MRTpTSDwGJ MJKNDG"{ff?7SDGpTCDMJjB#SBYjtpyK!jBYjERTpS/wk`0@GpTS/v{ff?7SDCtpyKpy?7SkqpTS
WMJtSDpyKpT?7SkYIDS/MFpyKL/MJjffS/?#jZk{JB#S	LDBF#MBstBYjMFSuKJMl{JB#S	KLDMFS	CDpyjMF{ffKKL/M.RpTS/w5kzpSqMFpTKL/MJj
CDpyjMF{ffKpy?7SA*pTKL/?7N/K{ffjMFBYKpS/v"B#SLHL}stBYK!K!MJjS29P2AMWCDpTjMF{ffKKL/MWRTpTS/w  k}B#{G   kB#SDcC k}pTGstBYjK
?#P9B#S/?#KL/MJj*RpTS/cw k`  IDKLDMFSA;MCDpyjMF{ffKpyKB#	G k   gQpTS?#jCDMJjK!?}BF#?7pCBS/MJALHLstBYK!K!MJjSof;M
{JB#S{ff?7S[KpSuN/MCDpyjMF{ffKpTS/v	KL/MRTpTSDwG.pSB{LDB#pTSpTSKLDpTGAB+E#IB#SDCKLDpTG.stj?{ffMFGG{JB#SDSD?#Kv#MFS/MJjfBYK!M
BxCDpTjMF{ffK!MFCr{ffEH{JRyM	MF{JB#NDG!M#IB#{J{ff?#jCDpTSDvK!?{ff?7SDCDpTKpy?7SrpTSWMJtSDpTKpy?7S_kY	I LDB#GSD?x{ff?74stRyMJK!MFRyE
NDSDCtpyjMF{ffK!MFC{ffEH{JRyM#
g`o}L/MMffK!MFStGpy?7SsDj?H{ffMFGGq?#P  C/?MFGzSD?#K?CDpTPEKL/MG!w#MFRyMJK!?7S_B#SDCC/?MFGzS/?#K{ffjMFBYK!MS/MJA
LHLsBYK!K!MJjSDGJL/MJjMJP?#jM#IB#RTRKL/MqMffK!MFSDGpy?7SDG?#	P LDBF#MqKL/M}GB#4MzG!w#MFRyMJK!?7SB#SDCKL/MqGB#4M}u
G!K!jNt{ffKN/jMFGgQBuG!K!jNt{ffKN/jM	pTG4BsBYjKpT{JNDRTBYj4{JB#G!M?#P)L/LstBYK!K!MJjSofI(L/MFSD{ffMKLDMJEBYjMMFuNDpTB#RyMFSuKJ
KGL/?7NDRCqMS/?#K!MFCKLtBYK*{ff?7SDCDpyKpT?7SqpS@MJSDpyKpy?7Sk.pTGS/?#K@S/MF{ffMFGGBYjEzK!?4sDj?%#M.KL/MjMFGNDRyKG
Tp Srhj?#s?7GpyKpy?7S,kYLDpTG{ff?7StCDpyKpy?7SrpGpTSD{JRTNtC/MFCK!?MFStGN/jMKLDBYKzKL/MOKEs`M?#P.h* NDG!MFCK!?
jMJsDjMFG!MFS[KGN/tG!MJKG(?#PMFNDpyYB#RyMFSuK*)GWpTG(B#Gv#MFS/MJjfB#RB#Gs?7GGpTtRyM#iS?#KLDMJjA;?#jfCDGJIt{ff?7SDCtpyKpy?7S
M

fi

f #%$ff&(' *)

 $,.$ !

"$!$ $



v7NDBYjB#SuK!MJMFGKLDBYKB#S*h** pTGBjMJsDjMFGMFS[KBYKpy?7Sz?#PKLDM@v#jMFBYK!MFGKSNDMJj?#PMFNDpyYB#RyMFSuK;*)GFI
GN//MF{ffKK!?qKL/MljMFG!K!jpT{ffKpy?7StGpT4s?7G!MFCE{ff?7SDCDpyKpy?7StG.k|}pS	WMJtSDpyKpy?7SkY(WG*AM.A*pRTR2G!MJMpSKL/M
S/MffHKsDj?#s`?7GpyKpy?7S2I7KLtpTGpTGB#{LtpyMJ#MFCzuECDpTjMF{ffKpTS/v.KLDM@pTStpTNtSuNDMJj?#PMFCDv#MFGJ/?#jMffHB#stRyM#I
  
k  A;?7NDRCS/?#K*M.BYB#RTpTC@h*`LDMl*h;*KLtBYK*AMlA?7NDRTCNDG!MpTSKLDpTG*{JB#G!M

pTG ! k`  
GHABKj_ ZJZ
a fZ
V {	5 YZJdf] X]^uaHZ	cY^#]aQ]cY^/\    ]^ dZ 	^D]QaQ]cY^    HZffdZz]\
aHZJ^V}\]^u7yaZ ;	{	5lk \/J	aV#@a ^G | ;g .no ml^G | dg klio 

h

Y
i
L/M"sDj??#PpTG){ff?7SDGK!jND{ffKpy#M#MGLDB#RTR'tNDpTRC	KL/M4*h;*okB#G)P?7RTRy?+A@GJ*KL/M4G!w#MFRTMJK!?7SB#SDCKL/M
LHLstBYK!K!MJjSDG*?#P(
k
BYjMlKLDMlGB#4M"B#G*KL/?7GMpTS WSBYjf{
pTS GLtB#RTRS/?%A,Ml{ff?7SDGpC/MJjMFC
GND{fL}KLtBYK
g o Kl$ B#StC
g /o
gQpyPGND{fLqB#SBYj{WC/?MFG;SD?#K;Mff/pTG!KJIKL/MFS pyKG!MFRyPA?7NDRTC
MB#S*h;*.ofA;M@{ff?7S[#MJjK;KL/M*BYj{
4pTSuK!?KL/M*RTpS/w 9LDpGsDj?H{ffMFGGpTGKL/MFS}jMJsMFBYK!MFC2
pT?7NDGRyE#I`KL/M4h*p
k
?#tKB#pTS/MFCpTSOKLDpTGWABFELDB#GS/?CDpyjMF{ffK!MFC{ffEH{JRyM4B#SDC#MJjpyDMFG){ff?7SDCtpyKpy?7S
pTSWMJtStpyKpy?7SkYl	?#jMJ?%#MJjFIAM"{JB#StS/?#K)?#DKB#pTSBq{ff?7SDDv7N/jBYKpy?7S
kstBYjMFMFSu{JKfB#ofND9G!M SqB#CDCtpyg K2py?7o S2qIk $ {JgB#A;SDM4S/?#?7KStRyLDE	BF#jMFM4B#?%Su#EM{ffK?7L/4MstCDRyMJpTjK!MFMF{ffRTEKpy?7NDSSDCD?#pyP;jMFBY{ffjK!{JMFG)C}A@{ffEHL/{J?7RyG!M@MpTMFSD{JpTKB#pTNDB#B#GRGM*S/BzMF?HpyGC/KN/L/MFDMJGjv#LDjfKB+BYL/#stM)MLBYS/j?#?{ P

	pGS/?#K*sBYjK*?#PB#SuE	{ffE{JRTMpTS ?#j@pTK@pTG(sBYjK*?#P9B}{ffEH{JRyM"pTS KLDBYK@"NDG!K@{ff?7S[KB#pTSBYK)RyMFB#G!K
?7S/MLHLstBYK!K!MJjfSxgQB#SDCKL/MCDpyjMF{ffKpT?7SDG?#P KL/MBYj{JG(pTSzKLDpG;sBYK!K!MJjSA*pTRTR`S/MJ#MJj(M)jMF4?%#MFCofrk pTG
KL/MJjMJP?#jMlB#S*h**"
 MJK*NDG(S/?%AsDj?%#M.KLDBYKn^ g losmt^ gdlk ofpyP
^ g .oKL/MFS B#SDC LtBF#M.KL/MGB#4M
G!w#MFRyMJK!?7SB#SDCxLHLstBYK!K!MJjfSDGJI L/MFSt{ffM B#SDCu
k
B#RTG?	LDBF#MKLDMGB#4MG!w#MFRyMJK!?7SB#SDCxLHLstBYK!K!MJjfSDGJ
?#jMJ?+#MJjFI'B#GWB#RRKL/MBYj{JG)pTSHkBYjMB#RTG!?BYj{JG)pTS "I2pyP
kKL/MFS ^ g lof I`A@LDpT{LOpTS
KN/jSp4stRTpyMFGKLDBYK
L/MJjMJP?#jM#IDB#{J{ff?#jCtpTS/v4K!?qWMJtStpyKpy?7S	I
pTSDB#RRyE#IRyMJKWNDGstj?+#M"KL/MNDSDpTN/MFS/MFGG?#P(k AMB#RyjMFB#C/EwHS/?+A KLDBYK)B#S[E?#KL/MJjW@h*-k
#MJjpyPEHpTS/v4KLDBYKX^ g .ovmw^ gdk o(LDB#GKL/M"GB#4MlGw#MFRyMJK!?7SOB#StCLHLsBYK!K!MJjSDGWB#Gvk@{J{ff?#jCtpTS/v
GL

mxo l {H



mGo? lk {H

Z

G



k

y

J DP





k

G!k







!

mGo? l wH





k



G | ;

k

y

x |

U

}

x | ;

U



5U

E

	



k

}

U

R



k

}





,U

}

k } 
G | ;



G | ;

x |



K!?q{ff?7StCDpyKpy?7Sk.pS	WMJtSDpyKpy?7SkYIKL/M.MFC/v#MFG*KLDBYK*BYjM"S/?#KstBYjK(?#PB#SuE?#PKL/MFG!MlL/LstBYK!K!MJjSDG*tN/K
BYjM}pSD{JpTC/MFSuKK!?KL/MqpTCDCDRTMSD?C/%M kpTSB#SuExLHLsBYK!K!MJj
S   kE  NDGK.M}CDpyjMF{ffK!MFCBFABFE
Pj?7 kgQpS?#jC/MJj(K!?}B+#?7pTC	S/MJALHL	stBYK!K!MJjSDGfof*L/MjMFB#pTStpTS/v"MFC/v#MFG(KLDBYK*BYjM.S/?#K(stBYjK(?#PB#S[E
LHLstBYK!K!MJjSNDGKlM}NDSDCtpyjMF{ffK!MFC2IpTS?#jCDMJjK!?GBYKpTG!PEx{ff?7SDCDpyKpT?7SpSxWMJtSDpyKpy?7SkY	L/MJjMzpTG
KL/MJjMJP?#jM?7SDRyE?7S/M4*h**KLDBYKBYK{LDMFG.Bzv7py#MFSG!w#MFRyMJK!?7SB#SDCBG!MJK)?#P;L/LOsBYK!K!MJjSDGJI2G!c
? k
pTGKLDM?7SDRyE*h;* #MJjfpyPEHpTS/vKLDBY#K ^G | ;g .#o mx^x | dg klofzpyv7NDjM}GL/?%A*GlB#SxMffHB#4sRyM?#PKL/M
{ff?7SDG!K!jfND{ffKpy?7SsDj?{ffMFGGF
LDMP?7RTRT?+A*pTSDvstj?#s?7GpyKpy?7SMFSDGNDjMFG4KLDBYKKLDM	{ff?7SD{ffMJsDK}?#P)*h;* B#RTRy?+A@GNDG4K!?CDMJtS/M	B
stBYjKpTKpy?7SpTSKL/MlGstB#{ffM.?#P*)GJ
GHABTyz_ ZJd
a VY^%   ZaQbcY] fWZffdZff^t{a ;Z	%;M\   /Zffc
^ ^G | ;g .)o |<^G | ;g   ]o HS$!

hYi

 MJK `MB#SuE*"HL/MFScUpTKG!MFRyPpTGBlh;*B#SDC4?#pT?7NDGRyEUHK^G | g_Uof;EBYststRyEHpTS/v)KL/M
jMFGNtRyKpTShj?#s`?7GpyKpy?7SIYA;M{JB#SB#GG!MJjK KLDBYKKL/MJjM;pTG2BGpTSDv7RyM*h;*rGND{fLKLDBYKOU}mt^G | g;lof
GL
(U

SKL/Mstj?#s?7GpyKpy?7SMFRy?+AlI AM}GL/?%AKL/MsDj?#s`MJjKpyMFG)A*LDpT{fLxBYjM}{ff?7}4?7SK!?OB#RRKL/M}*)G
MFRy?7S/v7pTSDv"K!?}KL/M.GB#4M.MffHK!MFSDGpT?7S?#P9B#S	*h;*"


fi X	ttu

b
a

b
c

a

b
c

a

b
c

a

c

x

y

x

y

x

y

x

y

e

d

e

d

e

d

e

d

(a)

(b)

(c)

(d)

pyv7N/jMl*RTRNDG!K!jBYKpTSDv*KL/M{ff?7SDGK!jND{ffKpy?7S4stj?{ffMFGG9pTS4hj?#s?7GpTKpy?7S"gQB[oih;*`g`oiNDStCDpyjMF{ffK
pTS/v4KL/M"BYj{ o  k`'gQ{+oNDSDCDpyjMF{ffKpTS/vKL/MlBYjx{ k   gQCoNDStCDpyjMF{ffKpTSDv"KLDMlBYjG{   7I
KLuNDG?#DKB#pTSDpTSDv"KLDMl*h;*~
 k

A B  bxc 	%;\ZffycY^u}a=c4aHZ"[Z P[aZff^/\]QcY^cWa/ZW\ffVT/Z;Z{w%] )VY^qcY^D X"] @a/ZffX
V#Z"aHZl\ffVT/Zl\fe#ZJyZFa=cY^VY^zaHZl\JV/}Zl["HV7aaZffd^/\i

GH 

h

Y
i
L/MS/MF{ffMFGGBYjE{ff?7SDCDpTKpy?7SpGl?#pT?7NDGJMJK4NDGsDj?+#MKL/MGN/~q{JpyMFS[K{ff?7SDCDpTKpy?7S24RyMJK
GL

U
B#SD
C U
MK|A;?	**)G.A*pTKLO{ff?7?7SG!w#MFRyMJK!?7SxB#SDCLHLstBYK!K!MJjfSDGJ.MGLtB#RTR'{ff?7StG!K!jND{ffKBh* B#G
P?7RTRy?+A@GJKLDMzG!w#MFRTMJK!?7SB#SDCKL/MzLHLxstBYK!K!MJjfSDG"?#wP BYjMzKL/MzGB#4MB#GlKL/?7GMzpTS UB#SDC U  KL/M
MFC/v#MFGWKLDBYK)LDBF#M"KL/MGB#4M"?#jpyMFSuKBYKpy?7SpT
S UB#SDy
C U BYjMCtpyjMF{ffK!MFCpTy
S pTSKL/MGB#MABFE`2KL/M
?#KL/MJj@MFC/v#MFG*pT
S  jMFB#pTS	NtSDCDpyjMF{ffK!MFCDj?7WMJtSDpTKpy?7S	ItpyKpTG({JRTMFBYjKLDBY	K U K U  } ^x | ;g .of
 LDB#GS/?4CDpTjMF{ffK!MFCq{ffEH{JRyMFGMF{JB#NDG!
M U B#StC UtBYjM*GJ  LDB#GS/?{ff?74stRTMJK!MFRyEqNDStCDpyjMF{ffK!MFC
{ffEH{JRyMFGJI.GpTSt{ffMB#RRKL/Mx{ffEH{JRyMFGpT
S U B#SDC U  GLDBYjMxBYKRTMFB#G!K	KL/ML/LstBYK!K!MJjSDGJS_B#CDCtpyKpy?7S2I
 
k  {JB#SDS/?#KMB*GN/Dv#jBYsL)?#
P MF{JB#NDG!MKLDpG2A;?7NtRTCpT4stRTE*KL/MMff/pTG!K!MFSt{ffM?#PHKL/MGN/tv#jBYstLDG
 B#SDC  
 pc
 
k%
k 
S U B#SDC U  IjMFG!sMF{ffKpy#MFRyE#I/B#SDCqKL/MJjMJP?#jMWKL/MFG!MWKA?4*)GA?7NDRTC
S/?#K*LtBF#M.KL/MGB#4MlLHLsBYK!K!MJjSDGJ
LDMJjMJP?#jM#IKL/Mh* GBYKpTG!tMFG"{ff?7StCDpyKpy?7SDGzk|pTSWMJtSDpyKpT?7SkY;EBYsDstRyEHpTS/vOhj?#s?Y
GpyKpT?7S I4AMr{JB#SKL/MFS NDpTRTCB_GpTS/v7RyM*h;*
 k GNt{LKLDBYu
K ^x | ;g .
o m^x | dg klofIL/MFSD{ffM
K
}
|
U
U
^x dg klof
 {LtBYjB#{ffK!MJjpyFBYKpy?7S?#PKL/MMffK!MFStGpy?7S?#P9B#S	@h*KLDBYKA*pRTR``M.NtG!MJPNtR`RBYK!MJj@pTGJ
GHABK ] #Zff^V#6
^ ;Z	% V#^OV 	%UMaHZffy
^ U ]\VY^Z P7aZJ^/\f]cY^c!G  ] VY^`
cY^D X}] laHZ;JcYc#b9]^ufcY^Y]QaQ]cY^/\lHcYff
 VY^` U V YZ"a/Zl\ffV /}Z\fe#ZffTZJa=c#^ 
bG k } S2]  mGo? lg kDXo HKI $a/Zff^ mGo X lg kDdo H mGo? lg k/io 
6G k } S2]  mGo? lg kDdo HK$OV#^ mGo X lg kDJo HKI $OaHZff^ ~ mGo X lg kDMo ~`H k 



hYi

GL


ZZ\\JVYdXzc#^Y]QaQ]QcY^t


fi

f #%$ff&(' *)

 $,.$ !

"$!$ $



g /oq
I $W MJK } mGo? glkDofI pM#yI=  k } "LDMFS2IPj?7 {ff?7SDCtpyKpy?7SzpTS@MJSDpyKpy?7SI
ItpM#yI } mGo X glkDof?#jMJ?%#MJjFI  } mGo? glk/o(   kc  pGB#SLHL	stBYK!K!MJjSpS"
/j?7{ff?7SDCDpTKpy?7S	pTS@MJSDpyKpy?7S	IKLDpTG(?H{J{JN/jG@pyPiB#StC	?7SDRyEpyP.  kc  pTGB#SOLHLstBYK!K!MJjSOpTS
UIDA*LDpT{fLpTGMFNDpyYB#RyMFS[K(K!?  } mGo X lg kDofLDMJjMJP?#jM#I mGo X lg k/{
o H mGo? glkDof
mxo lg kDG
G
m
o
G
m
o
KL/MFS   o k5Ho $	 B#StpTG*C B#SLHX Llg kDso BYHoI K!K!MJ$j4SpTS MJK U B#} StCKL/X MJlgjkDMJPof?#jMl|PpyK@KL/pTGMJjB#MqRTG!pT?qG.B#B#SSD?#LHKL/LMJj"sBYSDK!?K!C/MJMjS pTS} mGIto A*X LDglpkD{ofL I
{ff?7SuK!jB#CDpT{ffKGKL/M)PQB#{ffKKLDBYK mGo lg kDdo HK$H?/I kq{JB#SDS/?#K(LDBF#M.?#jM)KLDB#Sz?7S/M)stBYjMFSuKpS UI/L/MFSD{ffM
~ mGo X lg kDM
o ~H kY
 F]ZJ^ta;fcY^Y]QaQ]Qc#^D
 F
 |ffP  k } Uk  } KL/MFS mxo lg kDo HI $/j?7 {ff?7SDCtpyKpy?7SAMzLDB+#M mxo X lg k/o H mxo lg kDofIL/MFSD{ffM
KL/|MJOP jMJ P?# jGM k5    k5pTG B#S LHpTG(LzB#SOstBYLHK!K!LMJjstSBYK!pTS K!MJ"jSOIHpT?71S SD{ffUM. BYv7B#pSqPj?7{ff?7SDCDpTKpy?7SI mxo X lg k/do H mGo lg kDoB#SDC
{ff?7|SDffP CDpTKpy?7SxkI' AM ?#DpTG"KB#B#pSS LHmGo?Lx slg BYkDK!o K!MJHI jS$pTSB#SDUCIIPKjL/?7MFS {ff~ ?7mGSDo CDX pyKlg pTkD?7MoS~nH I I mGkqo?B# StlgC kD*o mGH o X mxlg k/o `o X HIlg k/of$zxL/?/MJIjMJPjP?#?7j M
 pTGB#S	L/LstBYK!K!MJjSp
 
k%
S "





mGo? lk
H

k } U

y x  &
L

  

= <C B


?

 NYqc 



= C




 MJK(NDGS/?%A_MffHB#}pTS/MWKL/MB#pTSCDpMJjMFSD{ffMFGMJKAMJMFSKL/MCDpMJjMFSuK;jMJstjMFG!MFSuKBYKpy?7SDGJB"jMJsDjMFG!MFSH
KBYKpy?7SB#G!MFC?7Sh*G.MFSDGNDjMFG)KLDBYKMJ#MJjEMFuNtpyB#RTMFSD{ffM4{JRTB#GGLDB#GBNDStpTuNDMjMJstjMFG!MFSuKBYKpy?7S2I
tN/K(KLDMJjM.BYjMlh;*)G@KLDBYK@C/?SD?#K*{ff?#jjMFG!s?7SDCK!?qB#S[EMFNDpyYB#RyMFSD{ffM.{JRTB#GGgQpTS?#KL/MJjA?#jCDGFIDKL/M
BYsDspTS/vPj?7 MFNDpyYB#RyMFSD{ffM{JRTB#GGMFG.K!?h;*)GpTGpTS!MF{ffKpy#M%of  SKL/M?#KLDMJj"LDB#StC2I?7N/j.jMJsDjMff
G!MFSuKBYKpy?7StB#GMFC?7S	*h;*)Gv7NtBYjB#S[K!MJMFG*KLDBYK(MJ#MJjE*h;*{ff?#jjMFG!s?7SDCDGK!?B#SMFNDpyYB#RyMFSD{ffM
{JRTB#GGgsDj?#s`?7GpyKpy?7Sk+otN/KC/?MFGSD?#K;MFSDGN/jM*KLtBYK;MJ#MJjEqMFuNtpyB#RTMFSD{ffM@{JRTB#GGLtB#GB"GpTSDv7RyMjMJsDjMFG!MFSH
KBYKpy?7SgKL/M}BYsDstpTS/vPj?7MFNDpyYB#RyMFSD{ffM{JRTB#GG!MFG*K!?z@h*)GWpTG*?7S[K!?uof@?+AMJ#MJjFIKL/M}BYsDstpTS/v
Pj?7 MFuNtpyB#RTMFSD{ffM4{JRTB#GG!MFGK!?h*GlpTGWtp !MF{ffKpy#M#4pyv7N/jMFGluBHI9uB#SDCu{GL/?+AKLDM4KL/jMJM
*h;*)G{ff?#jjMFG!s?7SDCDpTS/v4K!?zKL/MGB#4MMFuNDpTB#RyMFSt{ffM{JRTB#GGFKL/MB#GG!?H{JpTBYK!MFC{ff?74stRTMJK!MFCOh;*pTG
GL/?%A*S	pTSpyv7NDjMuC2SKLDpTGMff/B#4stRyM#ItKL/M.SuNtlMJj(?#P9*)GWpTSKL/M.MFNDpyYB#RyMFSD{ffM{JRTB#GG*pTGkF

(a)

(b)

(c)

(d)

pyv7N/jMu"gQB[ofI g`oB#StCgQ{+o;LDjMJMW@h*G({ff?#jjMFG!s?7SDCDpS/v.K!?KL/MGB#4MWMFuNtpyB#RTMFSD{ffMW{JRB#GGJgQCo
KL/MlB#GG!?H{JpTBYK!MFC	{ff?7stRyMJK!MFCh;*
WG AM;{JB#S"G!MJM#IYKL/MCDpMJjMFSD{ffMBYsDsMFBYjG A*L/MFSlKL/MJjM;BYjMK!jpTB#SDv7NDRTBYj G!K!jfND{ffKN/jMFGJ'|PHAM;{ff?74sBYjM
KL/M*CDMJtSDpyKpy?7S?#P2@h*gQWMJtSDpyKpT?7S	k+oA*pTKL4KL/M*{fLDBYjB#{ffK!MJjpyFBYKpT?7S}?#P h;*)GgQL/MJ?#jMF7ofI
2

fi X	ttu
AM"B+E	?#tG!MJj#MlKLDBYKWKL/M"MFGG!MFSuKpTB#R'Ctp`MJjMFSD{ffMlpTG@KLDBYK@Bh;*BFEOLDBF#M{ff?74stRyMJK!MFRTE	NDSDCDp
jMF{ffK!MFC{ffEH{JRyMFGJI[tN/KKL/MFG!M{ffEH{JRyMFG"NDG!KM.ffcYd!7VY S*h;*)GJIHNDSDCDpTjMF{ffK!MFC{ffE{JRyMFGBYjMKL/MJjMJP?#jM
P?#jtpTCDCDMFS2IuA@L/MJjMFB#GpTS(h*)GWNDSDCtpyjMF{ffK!MFCS/?7SH{fL/?#jCDB#R2{ffEH{JRyMFG@BYjM.P?#jtpTCDCDMFS2
K@GLD?7NDRTCMlS/?#K!MFCKLDBYK*AM{ff?7NDRC	B#RTG!?zC/MJtS/M.@h*)GWEjMJstRTB#{JpTSDv{ff?7SDCDpyKpy?7SOpTS	WMJP
pTSDpTKpy?7Sxk.P?#jpyKGMFNDpyYB#RyMFSuKJ  HZ"\D#d!Vft]^#/ZJXiZ YZJdfXffHVY]^c /@HcY^Zff^taWc ]\"VaQdZZ
gA*LDp{LrpTG4BG!sMF{Jpyt{zKEsM	?#PW{fL/?#jCDB#R*v#jBYstLofSKLDpGAB+E#I(KL/MGpT}pTRTBYjpyKpTMFGB#SDCCDpMJjMFSD{ffMFG
MJKAMJMFSh;*)GWB#SDC	@h*)GWBYjM.MJ#MFSO{JRyMFBYjMJjF;@SuEq?#PKL/M*h**)G@pSKL/MlGB#4MlMFuNDpT[
B#RyMFSD{ffMO{JRTB#GG}pG4?#DKB#pTS/MFCPj?7KL/M{ff?#jjMFG!s?7SDCDpTS/vh;* uEjMF?+HpTS/vxG!?74MO?#P)KL/MRTpTSDwG
gQ{ff?7Su#MJjKpTS/vqKL/MFpTSuK!?BYj{JGfopS?#jC/MJjK!??#tKB#pTS	B4K!jMJMG!K!jND{ffKN/jM#
HB#pSDpTS/vzKL/MsDj?#tRyMFPj?7 B#S/?#KL/MJjs`MJjfG!sMF{ffKpy#M#I2Pj?7 L/MJ?#jMFkB#SDChj?#s`?7GpyKpy?7S
AMl{JB#SG!MJMKLDBYKKL/Mlj?7RyMstRTB+E#MFC	EKL/M.uG!K!jND{ffKN/jMFGpTSOh;*)GWpTGKL/MlGB#4MB#G*KLDBYKstRTB+E#MFC
EqKL/MLHLstBYK!K!MJjSDG@pTS*h;*)GJ
KWpG*B#RTG!?zpTSuK!MJjMFG!KpTS/vqK!?zSD?#K!MKLDBYKWKL/M"SuNDMJj@?#P**)G)A*LDpT{fLBYjM"MffHK!MFSDGpy?7SDG@?#PBqv7py#MFS
*h;*"I ID{JB#S`M{JB#RT{JNtRTBYK!MFC#MJjEzMFB#GpTRyE`KL/MGN/Dv#jBYsLzpSDCDND{ffMFCquEqMFB#{L	{fLDB#pTS{ff?74s?7S/MFSuK
?#DP _pTG9B)K!jMJM#IHB#SDCKLDpGK!jMJM*{JB#S4MCDpyjMF{ffK!MFC4pTS4CDpMJjMFSuK9AB+EG9EG!MFRyMF{ffKpTS/v.B#SuE?#PKLDMS/?CDMFGB#G
KL/M@j?u?#KS/?HC/M#?#jMJ?%#MJjFI/A;MW{JB#S}stj?{ffMJMFCqpTSDC/MJsMFSDCDMFS[KRyEA*pyKLDpSMFB#{L{LDB#pS}{ff?74s?7S/MFSuKJ9L/M
SNDlMJj?#P2*GpS ^G | ;g .opTG9KLDMJjMJP?#jM*MFNDB#RDK!?KL/MsDj?HCDND{ffK?#P`KLDM@SNDlMJj?#P2S/?HC/MFG9A@pyKLDpTS
MFB#{L{LtB#pTSx{ff?74s?7S/MFSuK.?#	P "MJv7BYjCDpS/vKL/M}SND`MJj?#P(@h*)GKLDBYKjMJsDjMFG!MFSuK.KL/MqGB#4M
MFNDpyYB#RyMFSD{ffM4{JRTB#GGJI2KLtpTG)SuNDMJj@v#j?+A*GMffHs?7S/MFSuKpTB#RTRyEA*pTKLOKLDMGpyJM?#P;KL/M4NDStCDpyjMF{ffK!MFC{JRTpTN/MFG
pTSKLDM"h**"`D?#j@Mff/B#4stRyM#I`pyPiKL/MlGN/tv#jBYstL	pTStCDND{ffMFCEBq{LDB#pS{ff?74s?7S/MFSuK*pTS	Bh**
{ff?7SDGpG!KG?#P'Bl{ff?74stRTMJK!MWGNDDv#jBYstL}?#P S/?HC/MFGJIKL/MFSqKL/M)SuNDMJj?#P@h*jMJstjMFG!MFSuKBYKpy?7SDGpTG
Nn LtpTG?#pT?7NDGRyE	C/?MFG)S/?#K4MFB#SKLDBYKBqGMFBYj{L4MJKLD?CtB#G!MFC?7S*h;*)G"NDG!KWMffsRy?#jM
B#- RTRKL/MFGM.MFuNtpyB#RTMFS[K(jMJstjMFG!MFSuKBYKpy?7SDGJ
 NDjjMFB#G!?7SP?#jNDGpTS/v*h;*)G@pTGB#RT4?7G!K(Mff/{JRTNDGpT#MFRyEsDjB#{ffKp{JB#R9SPQB#{ffKJID*h;*)G@C/?S/?#K
LDB+#M*B){JRyMFBYjG!MFB#SuKpT{JGgKL/MJEBYjMBG!?7MJA*LDBYKLuEuDjfpTC{ffjMFBYKN/jM#I7MJKAMJMFS*GB#SDC4{ff?74sRyMJK!MFC
h**)GfofM	{JB#S?7SDRyEGBFEKLDBYK4*h**)GA?7NDRTC{ff?#jjMFGs`?7StCK!?xG!MJKG?#P*MFNDpyYB#RyMFS[K*)G
A*LDp{L	GLtBYjMB#RTRKL/M{JB#NDGB#R stBYK!K!MJjfSDGA*LDMJjMlB#SOMff`MF{ffK)S/?HC/MlLDB#G@BYKWRyMFB#GKKA?z{JB#NtG!MFG"gQB#StC?7SDRTE
KL/Mz{JB#NDGB#RstBYK!K!MJjSDGA*L/MJjMqBGpTS/v7RyM}{JB#NtG!Mqstj?+#?#w#MFG4B#SMff`MF{ffK4BYjMS/?#K"C/MJK!MJjpTSDMFCof	LDpTGpTG
S/?#K)sDj?#RyMFBYKpT{A*L/MFSAMBYjMs`MJjP?#j}pTS/vq?C/MFR;\FZffTZFaQ]cY^	N/KWpTKWMF{ff?74MFG{ffjfpyKpT{JB#RpyPAMBYjM
C/?7pTSDv}4?HC/MFR;V #Zffd!VF#]^u7(A*pyKL/?7N/K@B}G!MFB#SuKpT{NDSDC/MJjGKB#SDCDpTS/v4?#P9KLDM{JRTB#GG@?#P*h;*)GJI2pyK@A*pTRTR
M.uNtpyK!M.CDpy~}{JNtRyK;K!?qB#GGpyv7S	Bstjpy?#j(K!?KL/MF	
 NDjWpTSuK!MFSuKpy?7SOpG*K!?K!jB#C/M"KL/MNDSDpTN/MFS/MFGG(?#PKL/MjMJsDjMFG!MFS[KBYKpT?7SO?#PMFuNDpTB#RyMFSt{ffM{JRTB#GGMFG@?#P
*)Gg=(h*)GoP?#jB}4?#jM"B#StBYv#MFBYtRyM"?7S/MgQ@h*Gfof*K!MFG!KpS/v}A*LDMJKL/MJjWBqv7py#MFSh*
pTGWB#S@h* pTG*MFB#GpyMJj)KLDB#SK!MFGKpTS/vzA*L/MJKL/MJx
j pTGWBq{ff?74sRyMJK!MFCh*"2SOKL/MtjG!K){JB#G!M#I
KL/M*{ff?7StGpTG!K!MFSD{ffE4{fL/MF{wpTSu#?7Ry#MFGK!MFG!KpTS/v.P?#jKL/M@BYtG!MFSD{ffM@?#PCtpyjMF{ffK!MFCB#SDC}{ff?74stRyMJK!MFRyE4NDStCDpyjMF{ffK!MFC
{ffEH{JRyMFGWgKLDMW{ff?74sRyMffHpTKE4?#P2KL/MFG!MWK!MFG!KGB#SDCqKL/?7G!M@SDMF{ffMFGGBYjEK!?"#MJjpyPEA*L/MJKL/MJjBlCtpyjMF{ffK!MFCv#jfBYstL
pTG.B*pGMffHB#{ffKRTEKLDM}GB#4M%ofIiA*L/MJjMFB#GlpTSKL/MG!MF{ff?7StC{JB#G!M#IpTSB#CDCDpTKpy?7SK!?K!MFG!KpS/vP?#jKL/M
BYtG!MFSt{ffM"?#PCDpyjMF{ffK!MFCB#SDCstBYjKpTB#RTRyECDpyjMF{ffK!MFC{ffEH{JRyMFGJI2AM4B#RTG!?S/MJMFCK!?sMJjP?#j{fL/?#jCDB#RpyKE	K!MFGKG
B#SDC{fL/MF{wzKLDBYK(MFB#{LBYj{pTGstBYjK?#P ?7SDM?#P KL/MpTSDCtND{ffMFCqGN/Dv#jBYstLtGCtpTG!stRTB+E#MFCqpSzpyv7NDjM.L/M
sDjp{ffM"AM}LDB+#MK!?	stB+EOP?#jlNDGpTS/v*h;*)GlpG)KLDBYK.AM}BFE?H{J{JB#Gpy?7SDB#RTRTES/MJMFCK!?MJB#RTNtBYK!M}B#S
MFNDpyYB#RyMFSD{ffM{JRB#GG@?#jMlKLDB#S?7SD{ffM#@SOKL/M"S/MffK)G!MF{ffKpy?7SIAMA@pTRTRMff/B#pTS/ML/?+A BzRy?{JB#R'G!MFBYj{fL
4MJKL/?HCA*LDp{LNDG!MFG(@h*G*{JB#S	B#RTG?4KBYw#MlB#C/YB#S[KBYv#M?#P'KL/M.CDMF{ff?74s?7GBYtpTRTpTKEsDj?#s`MJjKE}?#PB
G{ff?#jpS/v4PNtSD{ffKpy?7SpTS?#jCDMJjK!?MJ~}{JpyMFSuKRyEzMJB#RNDBYK!MlS/MFpyv7Lu?#jpTS/vG!K!jNt{ffKN/jMFGJ


fi

f #%$ff&(' *)

 $,.$ !

"$!$ $



1 : w ?:x :698
]>

.>$]

>

MlA@pTRTR2NDG!MB}Ry?H{JB#R4MJKL/?HC	K!?qMffsRy?#jM.KL/MG!MFBYj{LG!stB#{ffM?#P*h;*)GJLDM.G!KBYjKpTS/v}s`?7pS[K?#P
KL/M)G!MFBYj{fLsDj?H{ffMFGGA*pTRTRt`MWB#SzMF4sDKEz*h;* gQ{ff?#jjMFGs`?7StCDpTS/v.K!?4B#SzMF4sDK|Eq*lofD>*MJ#MJjKLDMff
RyMFGGFIDA;M{ff?7NDRTCGKBYjKPj?7B#S/?#KLDMJj@{ff?7S/Dv7NDjBYKpy?7SpTP'AMlLDB+#M"G!?7M.sDjpy?#j*wS/?%A*RyMFC/v#MBY`?7NDKKL/M
sDjMFGMFSD{ffM4?#jlBYtG!MFSt{ffM4?#PG?74M4MFC/v#MFG.?#jl[G!K!jfND{ffKN/jMFGJM}"NDG!KKLDMFSxC/MJtS/MKL/M4?#sMJjBYK!?#jfGK!?
4?%#MlPj?7 ?7S/M.{ff?7S/tv7N/jBYKpy?7SK!?qB#S/?#KL/MJj@SDMFpyv7L[?#jpS/v"{ff?7SDDv7N/jBYKpy?7S

 -
k

?

PN

 N/j(B#GpT{)?#sMJjBYK!?#jGBYjM.KL/M.pTSD{JRNDGpy?7Sq?#P9B#SMFC/v#M.MJKAMJMFSBsB#pyj(?#P9S/?7SHB#CB#{ffMFS[K@S/?CDMFGB#SDC
KL/M;jMF4?+YB#Ru?#PDB#S.Mff/pTG!KpTS/v*MFC/v#M;MJK|A;MJMFS"BstB#pTj ?#P/B#CB#{ffMFS[KS/?HC/MFG pTSlKL/M;{JNDjjMFSuK {ff?7S/Dv7N/jfBYKpy?7S2
L/MFGMMFC/v#MFG*B+E`MMFpTKL/MJj*CDpyjMF{ffK!MFC?#j@NDSDCtpyjMF{ffK!MFC2
LDM"pTSt{JRTNDGpy?7S?#PB#SpTG!?7RBYK!MFCRpTS/Bw !kOA*pTRTR G!MJj#MB#G)BzK!MF4stRTBYK!MP?#jKL/M"BYjf{JG  kB#SDC
B
k[LD?+AMJ#MJjFI[KL/M(RTpS/
w !kK!?#v#MJKL/MJjA*pTKL"B#SD?#KL/MJjRTpTS/w   jMJsDjMFGMFS[K9B#SuE"{ff?7tpTSDBYKpT?7S?#P
BYj{JG MffH{ffMJsDK'KL/?7G!MKLtBYK {ffjMFBYK!M;SDMJALHLstBYK!K!MJjfSDG;gKL/M;*)G(gQB[ofIHgoB#SDC}gQ{+opTSpyv7N/jM;7ofiSKL/M
{JB#G!M?#PB#CtCDpTS/vWB#S"BYjf{YI#A;M(B+El?#DKB#pTSG!MJ#MJjfB#RCtp`MJjMFS[KS/MFpyv7Lu?#jpTS/v*{ff?7SDDv7N/jBYKpy?7StGJI#C/MJsMFSDCDpTS/v
?7SKL/MlK!?#s`?7RT?#v#Eq?#PKL/M.{JN/jjMFS[K@h*B#SDCKLDM.CDpyjMF{ffKpy?7S?#PiKL/MlBYjf{)`MFpS/v4pTSD{JRTNDCDMFC2iWG(AM
A*pTRRG!MJM#IpyPAMzBYjMzK!MFG!KpTSDvOKL/MzpTSD{JRTNtGpy?7S?#PB#SMFC/v#MzMJKAMJMFSK|A;?S/?HC/MFxG rB#SDC k`IKLDpTGBFE
pTSu#?7Ry#MK!MFG!KpS/vOG!?7M}?#PKL/MqCtp`MJjMFS[KYB#RTpTCx{ff?7SDDv7N/jBYKpy?7StG.?#DKB#pTSDMFCuEKLDM}pTSD{JRTNtGpy?7S?#P(KL/M
RTpTSD
w !k`IKL/MqBYj{   kI KL/M}BYj{   k`IKL/M}L/LstBYK!K!MJjE
S   ky  ?#j.KL/MLHLsBYK!K!MJjS
 


kgA@L/MJjM
pTSKLDMRB#G!KK|A;?x{JB#GMFG4A;?7NtRTCMB#S[ES/?HC/MzGND{LKLDBYKMFpyKL/MJjKL/MRTpS/w
k`  ?#jzKL/MORpTS/w  Mff/pTG!KGzpTSKLDM{JN/jjMFSuKq{ff?7SDDv7N/jBYKpy?7S`of@?+AMJ#MJjFI*KLDMjMF4?%B#R*?#P.B#S
MFC/v#M.A@pTRTR`B#RyAB+EGjMFGNDRyK(pTS?7StRyE}?7S/M.SDMFpyv7L[?#jpS/v"{ff?7SDDv7N/jBYKpy?7S  KL/MJj(?#sMJjBYK!?#jGJIGND{LB#G@BYj{
jMJ#MJjGB#Rg=LDpT{w#MJjpTS/v/IkFm#m#7ofIA*pTRTRS/?#K(M)NtG!MFCzuEz?7N/jG!MFBYj{fL	4MJKL/?HC2LDM)GMJK(?#P'S/MFpyv7Lu?#jpTS/v
{ff?7S/Dv7NDjBYKpy?7SDG@?#P;Bqv7py#MFS*h;*
 A*pTRRKL/MJjMJP?#jM"MKL/MG!MJK)?#PB#RTR'KL/M4CDpMJjMFSuK@@h*)G
?#DKB#pTSDMFCPj?7  EB#CtCDpTS/v?#j*C/MFRTMJKpTS/v4BGpTSDv7RyM)MFC/v#MqgMFpyKL/MJj@CDpyjMF{ffK!MFC?#jWNDSDCDpTjMF{ffK!MFCof
MJP?#jMMffHstRTB#pTSDpS/v}KL/MCDMJKB#pTRTGW?#P;KLDMG!MFBYj{L4MJKL/?HC2IRyMJK.NDGpTRRTNDG!K!jBYK!M"KL/MB#pSpC/MFB#G)uE
4MFB#SDG"?#P(KL/MzP?7RTRy?+A@pTS/v	Mff/B#4stRyM#{ff?7StGpTC/MJj"KL/M*h**pTSpyv7N/jMnI9A*LtpT{LjMJsDjMFG!MFSuKG"KL/M
{JN/jjMFS[K{ff?7S/tv7N/jBYKpy?7S?#P`KL/MG!MFBYj{fLsDj?{ffMFGGgKLtpTG9Dv7N/jM?7StRyECDpG!stRTB+EGiKL/M(stBYjK9?#PKL/M@h*
{ff?#jjMFGs`?7StCDpTS/vK!?KL/M@S/MFpTv7L[?#jL/??HC4?#PKL/M@S/?HC/MF{G B#St5C kDofIB#SDCqB#GGNDM*KLDBYK;A;MWGLDB#RTRDpSD{JRTNDC/M
B#SMFC/v#Ml`MJK|A;MJMFSKL/MlS/?HC/MFG B#SDBC k`

x

y

pyv7N/jMln@>*?HC/MuxLDB#GW?7S/MstBYjMFSuKWB#SDC?7S/M4{fLDpTRTC2IkC/?MFG)S/?#KLDBF#M4B#SuEstBYjMFSuKG)B#SDCLDB#GWKA?
S/MFpyv7Lu`?#jfG
SKLtpTGlGpTKNDBYKpy?7S2IiA;M{JB#SDS/?#KpTSuK!j?HCDND{ffMKL/MRTpTSD
w kMF{JB#NDG!MzA;MqA;?7NDRCHpy?7RTBYK!Mq?7S/Mq?#P
KL/M}{ff?7StCDpyKpy?7SDGCDMJtSDpTS/v*h;*)GgQ{ff?7SDCDpTKpy?7Sk+of}MzBFEpTSuK!j?HCDND{ffM4KL/MBYj{5  kB#StCxpTS
KLDpTG({JB#GM#IBYv7B#pTSOpTS?#jC/MJjK!?}sDjMFG!MJj#M{ff?7SDCDpyKpy?7SkYIKL/MKA?qSDMFpyv7L[?#jG?#RP kNtG!KM{ff?7S[#MJjK!MFC


fi X	ttu
pTSuK!?"{fLDpTRTCDjMFS2iM){JB#SB#RTG?"pTSt{JRTNDC/MKLDMWBYj{wBk9pTSDB#RRyE#I[AMBFE}pSD{JRTNDC/M*KA?Ctp`MJjMFS[KLHL
stBYK!K!MJjStG  k5  I/A*LDMJjM  pGBS/MFpTv7L[?#j?#POkgKLDMW?#KLDMJjS/MFpyv7Lu`?#j"NDG!KMW{ff?7Su#MJjK!MFC	pTSuK!?
Bl{fLDpTRTCI[?7SD{ffMWBYv7B#pTSqpS?#jC/MJj;K!?stjMFG!MJj#MW{ff?7SDCDpyKpy?7SOk+ofL/MFG!M@P?7NDj;CDpMJjMFSuK{ff?7S/Dv7NDjBYKpy?7SDGBYjM
CDpTGstRTBFE#MFCpSipTv7N/jMlm
x

y

x

y

x

y

x

y

pyv7N/jMlm@>*MFpyv7Lu?#jpTS/v.{ff?7SDDv7N/jBYKpy?7StG9?#DKB#pTS/MFC}uE4pTSt{JRTNDCDpTSDvWB#S}MFC/v#MMJKAMJMFScB#SDCckpS4KL/M
{ff?7S/Dv7N/jBYKpT?7S?#Pipyv/`n

 - 
 9 
E B)>  
k





,ff

   N

q

S?#jC/MJjK!?CDMFGpyv7SxBG!EHG!K!MFBYKp{4AB+EK!?C/MJK!MJjpS/M4A*LDpT{fLS/MFpyv7Lu?#jpTS/v*h;*)G"BYjpTG!MPj?7
KL/M)pTSD{JRTNtGpy?7S4?#jKL/MWjMF4?%B#R`?#P B#SzMFC/v#MpTSqB#Sz@h*IDpyK;pTGGN/~q{JpyMFS[KK!?{ff?7SDGpTC/MJjG!?74MRy?H{JB#R
stBYjB#MJK!MJjG?#P`KL/MK|A;?lSD?C/MFGK!?.M{ff?7SDS/MF{ffK!MFC29pyjG!KJIuG!?74M*B#CDCtpyKpy?7SDB#RHS/?#KBYKpy?7SqpTG9pTSuK!j?HCDND{ffMFC2
ffP ~~ujMJsDjMFG!MFSuKGKL/M.{JBYjfCDpTSDB#RTpTKE}?#P9B4G!MJKJItv7py#MFSBSD?C/M pTSB4h**z
 "IA;M.CDMJtS/M#







g 2o
g o I
lg 2o]HV~ p  lg oM~I

 l ]HV~ mGo? l M~
Q 






g o
g 2o
o? lg 2o]HV~
 lg 2oM~
*
 l ]HV~
 l M~
x

tGMJj#MKLDBYKP?#j*B#SuE@h*f"IDKLDM.P?7RRy?+A*pS/vKA?sDj?#sMJjKpyMFG(LD?7RTC2
 lg 2o

  lg o Q  lg 2]o H o  lg o
 lg a
o H
I Z LQ  lg {o HgQL/MFSD{ffM   lg 2o   lg do H o lg 2o!o
 pTP 

BdS)
)tEEh

L/MSNDMJjB#SDCKEsM?#P`SDMFpyv7L[?#jpS/vW{ff?7SDDv7N/jBYKpy?7StG9KLDBYK{JB#S4M(?#DKB#pS/MFC4Pj?7KL/MpTSt{JRTNDGpy?7S
pTS}KL/MW{JNDjjMFSuK@h*?#P B#SqMFC/v#MWMJKAMJMFBS B#StcC k}{JB#SqM@C/MJK!MJjfpTS/MFCqPj?7KL/MWstBYjB#MJK!MJjG
BY?+#M#,*L/MjMFGNDRyKB#SuK}{JB#GNDpTGK!jEBFErM	jMFCDND{ffMFCrK!?G!MJ#MFSG!KBYK!MFGJIA@LDpT{LrA;MLDBF#MRTBYMFRyMFC


fi

 $,.$ !

f #%$ff&(' *)

"$!$ $



Pj?7  K!?1"WS?#jCDMJjWK!?zPQB#{JpTRTpyKBYK!MpyKGWC/MFG{ffjpystKpy?7S2IDAM"GLDB#RTR NtG!MKLDM"C/MF{JpGpy?7S	K!jMJM"GLD?+A*SpTS
pyv7N/jMkJ&Y
None

nG(x)=0 and nG(y)=0?

No

Yes

nG(x)=0 and nG(y)=0

nG(x)0 or nG(y)0

pG(x)=0 and pG(y)=0 ?

pG(x)=0 and pG(y)=0 ?

No

Yes

nG(x)=0 and nG(y)=0
pG(x)=0 and pG(y)=0

nG(x)=0 and nG(y)=0
pG(x)0 or pG(y)0

nG(x)0 or nG(y)0

State

C

No

nG(x)=0 and nG(y)0

pG(x)=0 and pG(y)0
nG(x)0 and nG(y)=0

State F

State G

pG(x)0 and pG(y)= 0

pG(x)=0 and pG(y)=0
nG(x)0 xor nG(y)0

nG(x)0 and nG(y)0

n G ( x) = 0 ?
Yes

No

Yes

pG(x)=0 and pG(y)=0

nG(x)0 or nG(y)0
pG(x)0 or pG(y)0

pG(x)=0 and pG(y)=0
nG(x)0 and nG(y)0 ?

State B

State A

No

Yes

n G ( x) = 0 ?
No

Yes

pG(x)=0 and pG(y)=0
nG(x)=0 and nG(y)0

pG(x)=0 and pG(y)=0
nG(x)0 and nG(y)=0

State D

State E

pyv7N/jMkJH@L/M.K!jMJM"?#Ps?7GGpytRTM)GKBYK!MFG@KLtBYK@B+EjMFGNDRyK(E	B#CDCDpS/vB#S	MFC/v#MMJKAMJMFSOS/?HC/MFG	
B#SDC1k

+BQ=fB=' f ff|  +#=f ,   %'7==  J2= ')| =% W = f 2%= = ff' =W9=e%  % = J) = [9  !  l /= += J
%


d
*
 
    O ;

 * 
.
 

 ff n




;

ff w
R
 ; v

M ;  "

d



fi X	ttu
S"KLDpTGiK!jMJM#I[KLDMRT?+AMJj9`?+?#PMFB#{LS/?7S/=K!MJjpTSDB#Ru#MJjK!Mff{ff?7SuKB#pTSDG9BWK!MFG!K*gQBY?7N/KKL/M(SuNtlMJj
?#PstBYjMFS[KG)?#j)KL/M4SNDlMJjW?#PS/MFpyv7Lu?#jG@?#PS/?HC/MFG*xB#SDCkDof.L/MRy?%A;MJj)?F?#PMFB#{LK!MJj}pTSDB#R
#MJjK!Mff}{ff?7SuKB#pTSDG9KLDM*RTBYMFRH?#PKL/M*G!KBYK!MjMFGNDRyKpTSDvWPj?7P?7RTRy?%A*pTS/v)KL/M*stBYKL4Pj?7KL/Mj?u?#KK!?lKLDBYK
K!MJjpSDB#Rt#MJjK!Mff`*L/MC/MFG{ffjpystKpy?7S?#P'MFB#{L	G!KBYK!M}gQp=M#9KL/MCDpMJjMFS[KS/MFpyv7Lu?#jpTS/v"{ff?7S/Dv7N/jfBYKpy?7SDG
KLDBYK4{JB#SMq?#tKB#pTS/MFCpTSKLDpTG"{JB#G!M%o"{JB#S`MzP?7NDSDCpTSiBYtRyMkYxLDMzN/sts`MJj?FHMFG?#P*B#RTRKL/M
#MJjKpT{ffMFGpTSKL/MK!jMJM}GL/?%AKL/MjMFGK!jpT{ffKpy?7SDGp4s?7G!MFC?7SxMFB#{LpTSuK!MJj4MFCDpTBYK!M4?#jlK!MJjpTSDB#R9GKBYK!M#
/?#j4Mff/B#4stRyM#I;G!KBYK!M	{ff?#jjMFG!s?7SDCDG"K!?BGpyKNDBYKpy?7SA*LDMJjMq?#KLS/?HC/MFuG B#SDC kC/?S/?#K4LDB+#M
S/MFpyv7Lu?#jG(B#SDC	BYK@RyMFB#G!K?7S/Ml?#P'KL/MFLDB#GG!?7M.stBYjMFSuKJ;WRyKL/?7N/v7LKLDM.K!jMJMlLDB#G*G!MJ#MFSCDpy`MJjMFSuK
G!KBYK!MFGJI2KLDMJjM4BYjM?7StRyE	D#M"K!jNDRyE	Ctp`MJjMFS[K)G!KBYK!MFGJIGpSD{ffM"GKBYK!MFGB#SDCI2B#SDCG!KBYK!MFG.B#SDCx
BYjMlGEMJK!jpT{JB#R
KBYK!M


@> NtlMJj(?#P
{ff?7S/Dv7N/jBYKpT?7SDG
k





g 2o





gk+o(?7SDRyEqpTP
g7o(?7SDRyEqpTP

!k



k

Bk
!k

 
k 
5
 


k

g /o k

Q  l

)pyjMF{ffK!MFC 0@StCDpyjMF{ffK!MFC  ?7stRyMJKpTS/v
EH{JRyMFG E{JRTMFG
>*?
>@?
>@?
-MFG +(1
>@?
>@?
.
+
2
1
>@?
>@?
-MFG
>*?
>@?
-MFG
.
+
2
0
1
-MFG
>@?
-MFG +021
>@?
-MFG +31
-F
M G +31
>@?
>*?
>@?
>*?
>@?
-MFG +021
>@?
>*?
>@?
>*?
>@?
-MFG +31
>*?
>@?
>@?
-MFG
-MFG
>@?
>@?
-MFG +.021
-MFG +021
>*?
>@?
>@?
-MFG
-MFG
>@?
>@?
-MFG +31
-MFG +31
g7o(?7SDRyEzpyPOQ  glkDoQ
guo(?7SDRyEzpyOP Q  glosQ

@CtC/MFCMFC/v#MFG

Q  lk

Q  lk

!k
 
k5

Q 

g Dok
glok

!k
 
k

g Do

Bk
 
k

g o



Q  lk




k5






k
Bk
 
k

Q  l

 gg 2DoXoXII B#B#SDSDCC  gg 2DoJoJII 
 l
 lk

H

H

 lk

 l

H
H

' BYRyMkYiBYtRyM?#P9G!KBYK!MFG@KLDBYKBFEjMFGNDRyK(EzB#CDCtpTS/v4B#SMFC/v#M.MJK|A;MJMFSOS/?CDMFGffOB#SDCk
S	iBYtRyM}kYIMFB#{LOj?+A {ff?#jjMFG!s?7SDCDGK!?}B}G!KBYK!M#KL/M.tjG!K{ff?7RTND}S	{ff?7S[KB#pSDGKL/MRTBYMFRTG(?#PKL/M
G!KBYK!MFGJtKL/MG!MF{ff?7SDC{ff?7RTNDSCDpG!stRTB+EGKL/M)K!?#KB#RSuNtlMJj?#P'S/MFpyv7Lu?#jpTS/v{ff?7S/Dv7N/jBYKpT?7SDGKLDBYK{JB#S
M}?#DKB#pS/MFCP?#j"MFB#{LG!KBYK!M#KL/MzKLDpyjC{ff?7RTNDSGL/?+A@GlKL/MCDpMJjMFSuKlK|EusMFG"?#PMFC/v#MFG"KLDBYKJI9P?#j
MFB#{LGKBYK!M#I{JB#SMzB#CDC/MFCK!?OKL/Mz{JN/jjMFSuK{ff?7S/Dv7N/jBYKpT?7S2{ff?7RTNDStG.P?7N/jFID#MB#SDCGpyxA*pTRTR9M
CDpTG{JNDGG!MFCRTBYK!MJjF
0WGpTS/vzKL/MMff/B#4stRyMpTSpyv7N/jM4nI2A;M4GLtB#RTR MffHstRTB#pSKL/M4NDGM"?#PKL/M4C/MF{JpTGpy?7SK!jMJM4B#G)AMFRTRB#G
KL/M"pTSDG!KB#SuKpTBYKpy?7S?#P9KLDMlpTS/P?#jBYKpy?7SpTS	iBYtRyM}kY/?7RRy?+A*pS/vKL/MlCDMF{JpTGpy?7SK!jMJM#I`BYKWRyMJ#MFR;kqgKL/M
j??#K*#MJjK!MfftofI`KL/MlK!MFG!K)pTG(PQB#RTG!M"GpTSD{ffM k	LtB#GKA?S/MFpyv7Lu`?#jfGJKWRyMJ#MFRItKL/MK!MFG!K@pTG@B#RTG!?PQB#RTG!M"B#G
LDB#GW?7S/MlsBYjMFS[KJ)K)RyMJ#MFRi}KL/M"K!MFG!K)pTGK!jNDM#I`GpSD{ffx
M xLDB#GWS/?zS/MFpyv7Lu`?#j+*KWRTMJ#MFR zA;M"jMFB#{fL

/



fi

 $,.$ !

f #%$ff&(' *)

"$!$ $



BOK!MJjpTSDB#R;#MJjK!Mff  N/j{JN/jjMFSuK"{ff?7SDDv7N/jBYKpy?7SKL/MJjMJP?#jM{ff?#jjMFG!s?7SDCtGlK!?G!KBYK!M	i*L/MFS2IuE
Mff/B#pTSDpTSDv	G!KBYK!Mz,pTSxiBYtRyMkYIAMq{JB#S{ff?7S/Dj KLDBYKlAM}jMFB#{fLP?7N/j"CDpMJjMFSuKl{ff?7S/Dv7N/jfBYKpy?7SDG
lg Q  lg k/{
o H_7ofd   J kP7I c J   kPA@pyKL/?7N/KS/MJALHL"stBYK!K!MJjSDGJI7B#StClK|A;?  J   k%  P
A*LDp{Lstj?CtND{ffM@S/MJALHLqstBYK!K!MJjSDGF;?/IuKLDMFG!MWBYjM@KL/MW?7SDRyEGK!jND{ffKN/jMFG;KLDBYK?7N/jB#Ryv#?#jpyKLD"NDG!K
MJYB#RTNDBYK!MA*L/MFS{ff?7SDGpTC/MJjpTSDvlKL/MpTSD{JRNDGpy?7S}?#P KL/M{JB#SDCDpCDBYK!M)MFC/v#M ! k`Szpyv7NDjM"kff/ItA;MGL/?%A
B#SMffHB#4sRyMqP?#j"MFB#{L?#P(KL/MzD#MS/?7SHG!EH4MJK!jp{JB#RG!KBYK!MFG	g?7SD{ffMBYv7B#pTS2I9KLDMFG!MqMff/B#4stRTMFGl?7SDRTE
CDpTGstRTBFE4KLDM@stBYjK?#P'KL/M)*h**)G{ff?#jjMFGs`?7StCDpTS/vK!?"KLDM)SDMFpyv7L[?#jLD?u?HC?#P'KL/MWSD?C/MF(G B#StC kDof
MKL/MJjMJP?#jMLDB+#M.B4G!EHG!K!MFBYKpT{WABFEzK!?"MffHstRy?#jM)B#RRtKL/MS/MFpTv7L[?#jpTSDv{ff?7S/Dv7NDjBYKpy?7SDGA*LDp{L
jMFGNtRyK}Pj?7 B#CDCDpTS/vB#SMFC/v#M#*?%A;MJ#MJjFI)pyKzA*pTRTR*G!?74MJKpT4MFGzMOSDMF{ffMFGGBYjErK!?sMJjP?#j G!?74M
B#CDCDpTKpy?7SDB#RG!K!MJstG@GpTSD{ffM)KL/Ml{ff?7SDDv7N/jBYKpy?7StG?#DKB#pTS/MFC	"NDG!K(M.@h*)GF
 lg k/c
o HI H
 L Q  lg k/Go H[of	K"pTG.KL/MJjMJP?#jM
 pyjfG!KJI'AMNDGKB#pTSuKB#pTSKL/Mq{ff?7SDCtpyKpy?7Srkg 
SDMF{ffMFGGBYjEK!?Ofc /@TZJaZ@KL/Ml{ff?7SDDv7N/jBYKpy?7S	P?#jWG!?74M.?#PKL/M"C/MFG{ffjpyMFCG!KBYK!MFGJIpM#G!?74Ml?#P
KLDMqRTpTSDwG"NDG!K.M}{ff?7Su#MJjK!MFCpTSuK!?BYj{JGJLDM}{ff?74stRyMJKpS/v	sDj?H{ffMFGG{ff?7SDGpTG!KGlpTSDjpS/v	B#S
?#jfpyMFS[KBYKpT?7SpTS	{JB#G{JB#C/M#I`GKBYjKpTS/v4Pj?7KLDMlRTpTS/wHG k` | GND{LKLDBYK*KL/MBYj{NDG!K*pTSuK!j?HCDND{ffMFC
pwG   k"MJKNDG){ff?7SDGpTCDMJjWKLDMGpyKNDBYKpy?7SpTSpyv7N/jMk#kYIA@L/MJjMA;MAB#S[KK!?	{ff?7StS/MF{ffKKL/M
SD?C/MF]G qB#SD%C k`I[B.{JB#GM@{ff?#jjMFGs`?7StCDpTS/vWK!?lG!KBYK!MWu@4?7SDv.KL/M(KL/jMJM*s`?7GGpytRyMS/MFpyv7Lu?#jpTS/v
{ff?7SDDv7N/jBYKpy?7StGJIiRTMJKlNDG.GNDsDs?7G!M4KLDBYKlAMqBYjM}K!MFG!KpTSDv	KL/M?7S/MqA*LDpT{fLxpTSuK!j?HCDND{ffMFGKL/MzLHL
sBYK!K!MJj
S   k1  .L/M*h;*?#DKB#pTSDMFCOPj?7KL/M4{ff?74stRTMJKpTS/vzsDj?H{ffMFGG)pTG)B#RTG!?CDpTG
sRTBFE#MFCzpTS}pyv7NDjMlk#kYLDM*GpHKL}{ff?7RTNtS}pTS}'BYtRTMk*GL/?+A*G;A*LDp{L}GKBYK!MFGB#StC}S/MFpyv7Lu?#jpTS/v
{ff?7SDDv7N/jBYKpy?7StGBFEjMFNDpyjMWKLDM.{ff?74stRyMJKpTSDv4sDj?H{ffMFGGJ

x

y

x

y

w

w

z

z

pyv7N/jMk#kY@ j B#StG!P?#jfBYKpy?7S?#P(B	{ff?7S/tv7N/jBYKpy?7SBYPK!MJj"pTSD{JRTNDCtpTS/v}KLDMstBYK!K!MJjS
{ff?74stRyMJKpTSDv






k



B#SDC

HMF{ff?7SDCDRyE#IYpyK pTGs`?7GGpytRyMKLDBYKiG!?74M;?#PKLDM;S/MFpyv7Lu?#jpTS/v({ff?7S/tv7N/jBYKpy?7SDG'NtG!KMjM!MF{ffK!MFC2I#B#G
KLDMJEv7py#MWjpTG!M@K!?"CDpTjMF{ffK!MFC}?#j({ff?74sRyMJK!MFRyENDSDCtpyjMF{ffK!MFC}{ffEH{JRyMFGgQ{ff?7SDCDpTKpy?7SDG;B#SDClCDMJtSDpTS/v
@h*Gfof4/?#j.Mff/B#4stRyM#IRyMJKNDG){ff?7SDGpTC/MJjKL/M4GpyKNtBYKpy?7SCDpTG!stRBFE#MFCpTSipTv7N/jMkFIA*LDp{L
{ff?#jjMFG!s?7SDCDGK!?xG!KBYK!M	SKLDpTG"{JB#G!M#I;KL/M{ff?7S/Dv7N/jBYKpT?7S?#tKB#pTS/MFCBYPK!MJjpSD{JRTNDCDpS/vKL/M
BYjf{   kB#SDC{ff?74stRyMJKpTSDv}A?7NDRTCOv#MFS/MJjBYK!M4BCDpyjMF{ffK!MFC{ffE{JRTM#)*LDpTG*{ff?7SDDv7N/jBYKpy?7S"NDG!K
KLDMJjMJP?#jM}MjM!MF{ffK!MFC2L/M}{ff?7RNDSDGP?7N/jlB#StCxD#M}pSx'BYRyMk}GL/?+AA*LDpT{fLxG!KBYK!MFGB#SDC
{ff?7SDDv7N/jBYKpy?7StG4BFEjMFuNtpyjMBC/MJK!MF{ffKpy?7S?#P)CDpyjMF{ffK!MFC?#j{ff?74stRTMJK!MFRyENDStCDpyjMF{ffK!MFC{ffE{JRTMFGJI
jMFG!sMF{ffKpy#MFRyE#

N



fi X	ttu

x

y

x

y

ipTv7N/jMkF>@MFpyv7Lu`?#jfpTS/v4{ff?7S/Dv7N/jfBYKpy?7SKLDBYKv7pT#MFGjpTGMWK!?qBCDpyjMF{ffK!MFC{ffE{JRyM

BzhB
)tEEh

L/Ml?#KL/MJj@B#GpT{?#sMJjBYK!?#jFIKL/MljMF4?+YB#R?#PB#SMFC/v#MgMFpTKL/MJj@RTpS/w}?#j)BYj{+o(pG*"ND{LOGpT4stRTMJj(KLDB#S
KL/MB#CDCDpTKpy?7S?#PB#SMFC/v#M#I)GpTSD{ffM?7SDRTEr?7S/MS/MFpyv7Lu?#jpTS/v{ff?7S/Dv7N/jBYKpT?7S_pTGz?#DKB#pTS/MFCA*L/MFSAM
C/MFRyMJK!M"B#SMFCDv#M#W?#jMJ?+#MJjFIpTK@pTG@S/?#K@S/MF{ffMFGGBYjE	K!?zsMJjP?#jfB#S[EK!MFG!K@P?#j)C/MJK!MF{ffKpTS/vzCDpyjMF{ffK!MFCO?#j
NDSDCtpyjMF{ffK!MFC{ffE{JRTMFGJ@?+AMJ#MJjFIpTSKLDpTG){JB#G!MAM4S/MJMFCK!?sDjMFGMJj#M4{ff?7SDCDpyKpy?7S	pTSKL/M4C/MJSDpyKpy?7S
?#P;@h* gQpy]P   k	Mff/pTG!KG)pTE
S qL ~ mxo lg kDMo ~9Q,q?#j mGo? lg 2o Hx
I
7ofIKLtBYK){ff?7NtRTCM"pT?7RTBYK!MFC
$
BYPK!MJjB#SBYjf{}pTG.CDMFRyMJK!MFC2q*LDpTGGpyKNtBYKpy?7SxB+EBYsts`MFBYjl?7SDRyEA*L/MFSAMqBYjM}v#?7pTS/v	K!?OjMF4?+#MzB#S
BYj{   k	B#SDCOMFpyKL/MJj mGo  lg kDffo H J DP?#j mGo  lg kDo H J  K  P7*pTSOKL/MlDjfG!K@{JB#G!M#IB#RRKL/M{LDpRTC/jMFS
?#.P k}KLDBYK*CD?SD?#KLDBF#M.?#KLDMJjstBYjMFSuKG(KLDB#1
S kzNtG!K(M){ff?7Su#MJjK!MFCOpTS[K!?SDMFpyv7L[?#jG?#.P kIDB#StCKLDpTG
sDj?H{ffMFGGWpTGjMJs`MFBYK!MFCG!KBYjKpS/vPj?7MFB#{fL?#P9KL/MFG!M{LDpTRC/jMFS2tpSKL/MGMF{ff?7SDCO{JB#GM#IpTP mxo g  o Hj$
KL/MFSpTSB#CtCDpyKpy?7SK!?KL/MqsDjMJHpy?7NDG.stj?{ffMFGGJIKL/MqBYj{   kx"NDG!KlMz{ff?7S[#MJjK!MFCpS[K!?OKL/MqRTpS/w
 kpyv7NDjMkF4pTRTRNDG!K!jBYK!MFGKL/MFGMlGpyKNDBYKpT?7SDGJLDpGsDj?H{ffMFCDN/jM)?#PK!jB#SDG!P?#jpS/v4BYj{JG*pTSuK!?RTpTSDwG
pTGMff/B#{ffKRyEKL/M.GB#MlB#GKL/M?7SDMlC/MFG{ffjpT`MFCpTSKLDM.sDj??#P ?#P9hj?#s`?7GpyKpy?7S

x

y

x

u

y
u

pyv7N/jM4kF jB#StG!P?#jfpTS/v4BYj{JGpS[K!?}RpTS/wHGBYPK!MJj*jMF4?+HpTS/v4KL/M.BYjf{

 y u 
k

u 

H 


k

W RyKL/?7N/v7LKL/MsDjMJpy?7NtGqC/MFG{ffjpysDKpy?7S?#P"KL/M?#s`MJjfBYK!?#jGKLDBYK	C/MJtSDMKLDMSDMFpyv7L[?#jLD?u?HC?#P"B#S
*h;*pTG)NDpyK!M{ff?7S[#MFStpyMFS[KPj?7 BzsDjB#{ffKpT{JB#R(gQpTstRyMF4MFSuKBYKpy?7Sos?7pTS[KW?#PHpyMJA.IP?#j)KL/M4GBYw#M
?#P9{JRTBYjpyK|E#I/A;MGLDB#RTR2C/MFG{ffjpyM)KL/MFpTSB#S/?#KLDMJjAB+E#;SPQB#{ffKJItAMlGLDB#RTRNDGMD#M.?#sMJjBYK!?#jGJ
o lg  K kDofIB#CDCtpyKpy?7Sz?#P9B#S	BYj
{   k




fi

 $,.$ !

f #%$ff&(' *)

"$!$ $

A

x

y

x

y

B

x

y

x

y

C

x

y

x

y

x

A
D, analogous to E

x

y

A
F, analogous to G

x

y

A

A

x

y

A

x

y

y

A

x

y

x

y

A

x

y

x

y

x

y

A

x

y

x

y

A

x

y



pyv7N/jMkff/@/?#j}MFB#{fLrG!KBYK!M	?#PWKL/MC/MF{JpTGpT?7SK!jMJMpTSpyv/(kJHIB#SMff/B#4stRTMz?#P@KL/M	S/MFpyv7Lu?#jpTS/v
{ff?7S/Dv7N/jBYKpT?7SDG9?#P2B#S}@h*KLDBYK;{JB#S}M(?#DKB#pTS/MFCBYPK!MJj;B#CtCDpTS/vB#SMFCDv#MMJKAMJMFS
S/?CDMFffG OB#SD
C k


fi X	ttu

A  )2g
  g
A )2 g

DofIB#CDCDpTKpy?7Sz?#PB}RTpTS/w5k`
o
l K kDofItC/MFRyMJKpT?7S?#PiB#SBYj{ 
k
 F
Q l K kDofItC/MFRyMJKpy?7S	?#PiBRpTS/5
w k
 F
K Kn ofIB#CDCDpTKpy?7S?#PiB#SzBYjfw
{   k}B#StCz{ffjMFBYKpy?7S?#P KL/M)LHLzstBYK!K!MJjSB  k5  uE
 , gl k

K!jfB#SDG!P?#jpTS/vKL/MRTpTS/5w k` pS[K!?KLDM.BYj{ k5  
LDM.{ff?7SDCDpyKpy?7StGKLDBYKKL/Ml{JNDjjMFSuK**h;*z
 NtG!K(#MJjpyPEzG?KLDBYKMFB#{fL?#PKL/MFG!Ml?#s`MJjfBYK!?#jG
{JB#SMWBYststRTpyMFCqpTS}?#jfC/MJj(K!??#DKB#pTSB"B#RTpCqS/MFpTv7L[?#jpTSDvl*h;* BYjMGL/?%A*SpTSz'BYtRTM*L/MFG!M
{ff?7SDCDpTKpy?7SDG{JB#SzM*MFB#GpTRyE4C/MJjpy#MFCqPj?7KLDM@pTS/P?#jBYKpT?7S}pTSqipTv7N/jMkJ"B#SDCz'BYRyMkY9SqiBYtRyM)I
! kllg !pTSKkDL/o`MjMJ{JstNDjjMFjMFG!MFSuSuKiK@G hB(K!MF*G!K'"P?#7ji>@C/?#MJK!M(K!MFK{ffLDKpTBYS/KivWA;{ffM(?74{JB#stS"RyMJsK!MJMFjRTPEW?#NDj,StCDKpyLtjMFpTG{ffK!K!MFMFC.GKi{ff#EHMJ{JjRyMFElG'MFBYB#PGK!pTMJRTjEpTA*SDG!pyKMJL/jK?7pTN/S/KivB#K{ffL/KMNDRTB#pRTS/RyEw
pTSDGMJjKpTS/vKL/MzRTpTS/w`pyKlpTGl?7SDRyExS/MF{ffMFGGBYjEK!?{fL/MF{wKL/M}Mff/pTG!K!MFSt{ffM}?#P*BstBYKLMJK|A;MJMFS B#SD
C k
Mff/{JRTNDGpy#MFRTE4P?#jMFC}EKL/M)RTpTS/wHGJHpTpRTBYjRyE#`I F  lg   kDoB#SDC F  lg   k%  ojMJsDjMFG!MFSuK;K!MFGKG
P?#jC/MJK!MF{ffKpTS/v"CDpyjMF{ffK!MFC}{ffE{JRyMFG;BYPK!MJjpTSDG!MJjKpTS/v)KL/M*BYjZ{   k4B#SDC4KLDM*LHL4stBYK!K!MJjf5S   k%  pTS
KL/M4{JN/jjMFS[K)*h;*"I2jMFG!sMF{ffKpy#MFRTEgQB#SDCsMJjLDBYstG@{ff?74stRyMJKpTSDvuof|K)GL/?7NDRCOB#RTG?qMS/?#K!MFCKLDBYK
AMz{JB#SsMJjP?#j KL/MFGMqK!MFG!KG"A*pTKL/?7N/KlpTStG!MJjKpTS/vKL/MzBYjf{}?#j"KL/MLHLxstBYK!K!MJjfS2pTSKLDpTGl{JB#GMqAM
?7SDRyES/MJMFCOK!?q{fL/MF{wOKL/MlMff/pTG!K!MFSt{ffMl?#PB}stBYKLPj?7 kK!c? {ff?7S[KB#pSDpTS/vq?7SDRyEMFpyKL/MJjWRTpTS/wHG(?#j)BYj{JG
CDpyjMF{ffK!MFCB+AB+EPj?7
 kgQBstBYjKpTB#RTRTECtpyjMF{ffK!MFCstBYKLPj?7
 kK!
? ofiBYtRyMOB#RG!?GL/?+A*GA*LDp{L
?#sMJjBYK!?#jGzBFEjMFuNtpyjMBs?7G!K=sDj?H{ffMFGGpTSDvG!K!MJspTSr?#jCDMJj}K!?MFSDGN/jMOKLDBYKzKL/MO{ff?#jjMFG!s?7SDCDpTS/v
S/MFpyv7Lu?#jpTS/v{ff?7SDDv7N/jBYKpy?7S?#P pG*B#S@h*`SiBYtRyM"I     | ulg k/oB#SDb
C *Q  lg kDojMJPMJj
K!?}KLDMlsDj?H{ffMFCDN/jMFG(KLDBYKWsDjMFG!MJj#M{ff?7SDCDpyKpy?7StG.klB#SDCqpTSOWMJtSDpyKpy?7SkYItjMFG!sMF{ffKpy#MFRTE#>@?#K!M"KLDBYK
?#K
L   lg  k/oB#SDC   B  | [lg kDo9KBYw#M)KpT4aM g  o9pSKL/MWA;?#jfG!K;{JB#G!M#I/A@L/MJjM  pGKL/MWSuNtlMJj
?#PDRTpTSDwGpSKL/M;GN/Dv#jBYstLlpTSDCDND{ffMFC.uEKL/M{LDB#pS{ff?74g s?7S/MFSuK ?#P rKLDBYKi{ff?7S[KB#pTStOG k`g R wQ ? lg kDoKBYw#MFG
KpT4M }g  g oipTSKL/M(A?#jG!K{JB#G!M#IA*L/MJjM  g pTGKL/MSNDMJj9?#P`BYj{JGpTSKL/MGN/tv#jBYstL4pTSDCtND{ffMFC"uE"KL/M
G!MJKW?#PC/MFG{ffMFStCDB#S[KGW?#]P kOpT
S KLDBYKW?7SDRyE	LtBF#M?7S/MstBYjMFSuKJ= F  lg   k/o*B#SD
C F  lg   k1  o
?#KLKBYw#MKpT4M }g  g oWpTSKL/MA;?#jfG!K.{JB#G!M#I A@L/MJjM  g pTG)KL/M4SNDlMJj)?#PMFC/v#MFG}gMFpyKL/MJj.BYjf{JG?#j
RTpTSDwGfo(pTSKLDM"GNDDv#jBYstLOpTSDCDND{ffMFCuEKL/M"SD?C/MFGWpTSOKL/M{LDB#pTS{ff?74s?7S/MFSuKW?#(P KLDBYK{ff?7S[KB#pTStwG k
K!?#v#MJKL/MJjWA*pyKLKL/MFpTjC/MFG{ffMFSDCDB#SuKGJ


Q

l K k

1 : #@8 =6 u< 5( 5

76
5 8m:]> 7:]=<:t7<6 5 6OC%@ 5 8;<8tT>#[:,7:H>
L/M@G!MFBYj{L4MJKLD?C}A;MWLDBF#M)C/MFG{ffjpT`MFC4}BFEM*BYsDsRTpyMFC4pTSq{ff?7ltpSDBYKpy?7SA@pyKLB#SuEG{ff?#jMWMFuNDpT[
B#RyMFSuKPQNDSD{ffKpy?7S gP?#jqMffHB#4sRyMKL/M@!)I!WIO)B#StCr@MG{ff?#jpTSDvPNDSt{ffKpy?7SDG4BYjMG{ff?#jM
MFNDpyYB#RyMFS[KfofW^ SMFB#G!E,gtN/KzpTS/MJ~}{JpyMFSuKfo"ABFErK!?pTSuK!MJv#jBYK!M?7N/jqG!MFBYjf{L4MJKL/?HCrA*pTKLBG{ff?#jM
MFNDpyYB#RyMFS[KlPNDSt{ffKpy?7SA?7NDRTCM}B#GP?7RRy?+A*GFlv7py#MFSB#S@h*
 K!?MMJYB#RTNDBYK!MFC2IG!MFRyMF{ffKB#S[E
MffHK!MFSDGpy?7SBU ?#Pd B#SDC{ff?74stNDK!M ^ _g U  FofM.{ff?7NDRCqB#RTG?NtG!M)?#KL/MJjlgQS/?7SH=MFNDpyYB#RyMFSuKfoG{ff?#jfpTS/v
PQNDSD{ffKpy?7SDGFIHB#RTKL/?7N/v7LKL/MG{ff?#jM.?#]P  A?7NDRTCC/MJsMFSDC?7S	KL/MlG!MFRyMF{ffK!MFC	MffK!MFSDGpy?7S2
@?+AMJ#MJjFIRyMJK.NDG){ff?7SDGpTCDMJjKL/M{JB#G!M4?#PBC/MF{ff?74s?7GBYtRyM4G{ff?#jfpTS/vPNtSD{ffKpy?7S ^ KL/M4*?#/
KB#pTS/MFCluEB#CDCtpTS/v?#j'jMF4?+HpTS/vWB#SlBYj{Pj?7KL/M{JN/jjMFSuK *
 U{JB#S`MMJB#RTNtBYK!MFClE.4?HCDpyPEpTSDv
?7SDRyEq?7S/MlRy?H{JB#RG{ff?#jM#
_g U J  
kP.`
 Fdo H ^ _g U` Fo  ^ j lg k KnmGo X lg kDo!o ^ j lg k KnmGo X lg k/Oo  J DPo
^
_g Uo J  
kP.`
 Fdo H ^ _g U` Fo  ^ j lg k KnmGo X lg kDo!o ^ j lg k KnmGo X lg kDo  J DPo
^
]>

7

(< k

 <?>ff



fi

 $,.$ !

f #%$ff&(' *)

"$!$ $



?7SDCDpyKpT?7SDG
hi?7G!K=stj?{ffMFGGpTS/v



} I * lg kDof 
gloJH_
I
   glk/oJH_
I
H
pTP   gloaH
I
 B Q  glk/oJH_
I
o? gl K k/o
 lg 2J
 glk/oaH
 glkDoJH
|
y
p

P
g
o

H
I

g
I

Q
I
[

!
o
o

K
/
L
F
M
S


u

l
g
/
k
o










 S F  lg   k/od HK o 7 
a
B
KL/MF
} I *  lg kDof   lg ]
o H_ B   glk/o]H_H
K k/o
 lg a
)
Q
2

l
g

y
p

P
g
l
Q
o

H
I

I
[o
>@?7S/M
B Q  lg kDoJH
 

o
KL/MF6
S   lg ! kD]o HT 7 
F o?  gl K k/o
 } mGo? lg k/o
pTP   glkDo
KL/MFS6wQ ? glk/o
F 
 Q)2gl K kDo
 } p  lg kDo
>@?7S/M
} I *  lg kDof  } p  lg kDof
K Kn o
  gl k
pyP lg lgQ k/ do lg HkDo  QB  Q lg g kD Jo  HIlg Hao  HI   Q  lg Xo HI [oo pTPQ KL/ MFglS k/osaQ   B  | uglk/o
KL/MF
S F  lg   k%  ]o HT o 7 
iBYtRyMl;L/M?#sMJjBYK!?#jGJItKL/MFpyj{ff?7SDCDpTKpy?7SDG?#P9BYsDstRTp{JBYtpTRTpyK|E#IB#StCs`?7GK=sDj?H{ffMFGGpTS/v4jMFuNDpTjMF4MFSuKG


sMJjBYK!?#j

0WGpTS/vC/MF{ff?74s?7GBYtRyMOG{ff?#jpTS/vPNDSt{ffKpy?7SDGJIKL/Mstj?{ffMFGG?#P.G!MFRyMF{ffKpS/v/I(v7py#MFSB#S*h;*"I(B
jMJsDjMFG!MFS[KBYKpT#Mq*B#SDCKL/MFSxMJYB#RTNDBYKpS/vpyKl}BFEM}NDpyK!M}pTSDMJ~}{JpyMFSuKJIiGpSD{ffM4A;MqA;?7NtRTCLDB+#M
K!?jMF{ff?74stN/K!MqKL/MqRy?H{JB#RG{ff?#jMFGlP?#jB#RTRKL/MqS/?CDMFGlpTSDGK!MFB#Cx?#P(?7SDRyE?7S/MzRy?H{JB#RG{ff?#jM#	*LDpTGPQB#{ffK
{JB#SBYw#MqBORyMFBYjSDpTSDv	B#Ryv#?#jpyKLD KLDBYKGMFBYj{L/MFG"pTSKL/M}G!stB#{ffMq?#P(MFuNtpyB#RTMFSD{ffM}{JRTB#GGMFGl?#P(*)G
{ff?7SDGpC/MJjBYtRyEqGRy?+AMJj*KLtB#S	B#S	B#Ryv#?#jpTKLDKLDBYK*G!MFBYjf{L/MFGWpTSKL/M.G!sB#{ffM.?#Pi**)G"gKLtpTGpTGKL/M{JB#G!M
?#PKL/M.B#Ryv#?#jpTKLDsDj?#s?7G!MFCE	LDpT{w#MJjpTS/v/I2kFm#m#7of
 NDjG!MFBYj{fLzMJKL/?Cq{JB#SM*NDGMFCP?#j;C/MF{ff?74s?7GBYRyM*G{ff?#jpS/v.PQNDSD{ffKpy?7SDGG!?lKLDBYKJgk+opyKpTG;S/?#K
S/MF{ffMFGGBYjExK!?K!jB#SDG!P?#j KL/Mz@h*pS[K!?BO**"I9KL/M*h**{JB#SMqMJB#RTNtBYK!MFCCtpyjMF{ffKRyE#I
B#SDCg7o*KL/M4G{ff?#jM"?#P;B#SuEOS/MFpTv7L[?#jpTSDv}*h;*{JB#SM?#DKB#pS/MFCEO{ff?7stN/KpTS/vzBYK4?7G!K)KA?
Ry?H{JB#RG{ff?#jMFGFWRTR2KL/MB#C/B#SuKBYv#MFGW?#PKL/MlG!MFBYj{fLOMJKL/?CtG(?7S	KL/MG!stB#{ffM.?#P9*)G)BYjMlKL/MJjMJP?#jM
jMJKB#pTSDMFC2I/tN/KB44?#jM.jMFCDND{ffMFCB#SDCj?#tNDGKG!MFBYj{LOG!stB#{ffMlpG(NDG!MFC2
MJP?#jM@KLDMFG!MWB#GG!MJjKpy?7SDGBYjM@sDj?+#MFCIRyMJKNDG;MffHB#pS/M@B#SqMffHB#stRyM#?7SDGpC/MJjKL/M)*h;* 
pTSzpyv7N/jMkF"B#SDCzKL/M)KL/jMJM)S/MFpyv7Lu`?#jfpTS/v{ff?7SDDv7N/jBYKpy?7StGsDj?CDNt{ffMFC}EKL/MpTSD{JRNDGpy?7S4?#PiB#SzMFC/v#M
MJKAMJMF
S B#SD1
C k`I  ( I  - B#SDy
C  0 gQB#RTG!?}CtpTG!stRTB+E#MFCpTSpyv7N/jMkF7of

b
a
x

b
c

y

a
x

b
c

a

y

x

b
c

y

a
x

c
y

d

d

d

d

G

G1

G2

G3

ipTv7N/jMkF;WS	@h*f B#SDCKLDjMJMlS/MFpyv7Lu?#jpTS/v{ff?7S/Dv7NDjBYKpy?7SDGw ( I - B#SDC
2

0

fi X	ttu
LDMG{ff?#jM?#P@MFB#{fL?#PWKL/MFG!M	*h;*)GqpTG"MFuNDB#RK!?KLDM	G{ff?#jM?#P@B#SuE?#P@KL/MFpTj"MffHK!MFSDGpT?7SDGJ
pyv7N/jMkFCDpTG!stRBFEHG?7S/MMffK!MFStGpy?7SP?#jMFB#{LS/MFpyv7Lu?#jpTS/v{ff?7S/Dv7NDjBYKpy?7S2
b

b
a

c

x

b

a

y

c

x

a

y

c

x

y

d

d

d

H1

H2

H3

 pyv7N/jMkF9HK!MFSDGpy?7StGZU
M"{JB#SKL/MJjMJP?#jM.AjfpyK!M#
Av,l)    ,l
  
AsCl)    JCl
  
AsCl)    JCl
  

I

( U
-

B#SDC1U 0 ?#PiKL/Ml@h*G ( I - B#SDC 0 pTSpyv/'kF

 AAfifi]]77tt

  RR?t
 ffD
 ?]7?e%

 Afie% t
  R[?FJD
  ?]7

 

/?#j@MFB#{LOMffK!MFStGpy?7S 2i?#P9B#S[ES/MFpTv7L[?#jpTSDv"{ff?7S/tv7N/jBYKpy?7S J|ItpTKpTG(B#RyABFEHGs?7GGpyRyM@K!?}tSDC
B#S	MffHK!MFSDGpT?7S i?#P9KL/Ml{JN/jjMFS[K@*h** GND{fLKLDBYK*KLDMlG{ff?#jMFG@?#P 29B#SDC ?7SDRyECDpMJj


U

Be+D

Be+D

Be+D


 4444FFffffDD


 44Fff D



 AAFFeett


 AFe t



U





z

dU

U

pTS?7S/MRy?{JB#R2G{ff?#jMqgQpyv7N/jMqk+CDpTG!sRTBFEHGKL/MFG!M.MffHK!MFSDGpy?7StGfofMl{JB#SKL/MFSAjpTK!M#

  ll  J      hhee%%tt


  l       h e%t










C



C

  44[[44JJHH


 4[4J H



b
a
x

  ]]77DD


 e+ D



b
c

y

  AAfifitt


 Afi t



a
x

  RReY+Dt

   A?]7+
 ReD
  A7
 

b
c

y



a
x

c
y

d

d

d

H G1

H G2

H G3

ipTv7N/jMk+u;*L/jMJM.CDpMJjMFSuKMffK!MFSDGpy?7SDGZU  ( I,U  - B#StCU  0 ?#PKL/M.*h;*zpTSpyv/ kF
iBYwpS/vpTSuK!?}B#{J{ff?7NDSuKKL/M.sDjMJpy?7NtGMffsDjMFGGpy?7SDGFIAM.?#DKB#pS2
;g  ( 
 Fo H ^ ;g  Fo  ^ j lg k K $7o ^ j glk K o
^


fi

 $,.$ !

f #%$ff&(' *)

"$!$ $



g; - Fo H ^ g;Fo ^ j glk K  o ^ j glk K J   Po
g; 0  Fo H ^ g;Fo ^ j glk K  o ^ j glk K J   Po
^
LDMJjMJP?#jM#IKL/MG{ff?#jM4?#PB#SuES/MFpTv7L[?#jpTSDvq{ff?7S/Dv7NDjBYKpy?7SB+EOM?#DKB#pTS/MFCPj?7 KL/MG{ff?#jM
?#P	E{ff?74stN/KpTSDv?7SDRyEKA?ORy?H{JB#RG{ff?#jMFGFz>@?#K!M}KLDBYKG!?74M}?#PKL/MFGM}Ry?H{JB#RG{ff?#jMFGBFExLDB+#M
B#RyjMFB#CDE	MJMFS{ff?74stN/K!MFCBYKsDjMJHpy?7NDGWpyK!MJjBYKpT?7SDG)?#PKL/M4G!MFBYjf{LsDj?H{ffMFGGJWP?#jMffHB#4sRyM#I ^ j glk K $7o
LDB#CK!?MqNDG!MFCK!?G{ff?#jMKL/MpTSDpTKpTB#RMF4stKE@h*"I;B#SDCMFpTKL/MJj ^Tj lg k K  o?#j ^Tj lg k K  o"{ff?7NDRTC
LDB+#MlMJMFS{ff?74stN/K!MFC	A*L/MFSKL/M.RpTS/cw k`  ?#Zj k  AB#G@pTSDG!MJjK!MFCpS[K!?4KL/MG!K!jND{ffKN/jM#
GHABTz_ ZJG
a fZVY
^ ;Z{w%VY^
   ZVY^tb
X ;Z	%c[Ja=V#]^`ZffXVf7`XY]^ucY^Z
c}a/Zcf/ZJdV#a=c#d\[Zff\ffJd]=Z]^  VuffTZ  a=
c % _ ZJa ^ ZVO\fffcYdZ	nZ Fl] V#yZJ^taWV#^x[Zc /@Hc\JV[ffTZ
^`FaQ]cY^ 
V uOaHZcf/ZJdV#a=c#d"]\  
 Q)2lg  K k/olaHZff^
;g   `
 Fdo H ^ ;g ` Fo  ^ j lg k K $7o  ^ j lg k K J OPo
^
^

ebOaHZcf/ZJdV#a=c#d"]\ ?  g
o



g;   Fo]H

^

bOaHZcf/ZJdV#a=c#d"]\

/olaHZJ^
g;Fo

l K k

g

^

^ j

glk

o"aHZJ^
;g   `FodH
;g Fo{
^

^

uOaHZcf/ZJdV#a=c#d"]\ 
 )2g

g Do

KnmGo lk =

ZebOaHZcf/ZJdV#a=c#d"]\ ?  g
o

g;   FodH

glk

o 

K J  P

^Tj

glk

K J  Kn P

^ j

glk K $7o

l K k

Q

^

hYi

^Tj

DoaHZff^
g;  `Fdo H ^ g;`Fo

ZF

^

glk

^ j

o

J OP

K Kn
 l k



ZF

g Do!o 

KnmGo? lk

/oaHZJ^
g;`Fo

^ j

glk

o 

K J DP

l K k
^

^ j

glk

g Do!o

KnmGo? lk

^ j

glk

o

g /o)

KnmGo? lk

o

J DP

GL

gk+oipTjG!KJI[AM@GLDB#RTR/sDj?+#MWKLDBYKAM@{JB#Sz{ff?7SDG!K!jNt{ffK;B#SqMffK!MFSDGpy?7ScU  ?#P  B#SDCqB#S/?#KL/MJjMffK!MFSDGpy?7S
?#]P ItGND{fLKLDBY	K UB#SD
C U`Ctp`MJj@pTS?7SDRyEq?7S/MlBYj{}gKLtpTG(BYj{MFpTS/v  k/of
 ?7SDGpTCDMJjKL/M){JB#G!MFG.gQB[ofI g`ofIHB#SDCgQ{+ofIHA*LDpT{fLq{ff?#jjMFG!s?7SDCK!?KL/M)B#CDCDpyKpT?7S?#P B#SzMFC/v#MWMJKAMJMFS
B#SD%
C k`ipTS4{JB#GM"gQB[ofI   H  J !kP@B#SDCRTMJ]K U  MB#SMffHK!MFSDGpy?7S?#DP   KLDBYK{ff?7SuKB#pTSDGKLDM*BYj{
 
k[pS{JB#G!MlgofI#A*L/MJjZ
M *,H  J   k,P7IuB#SDCpTS{JB#GMlgQ{+ofI7A*L/MJjZM *,H;g  J k`  Po J  

k5
P7IHRyMJ
K U  `M@B#S[E4MffHK!MFSDGpy?7S}?#OP   gA*LDpT{fLA*pRTRH{ff?7S[KB#pTSqKL/MWBYjff{   k/ofS}B#RTRDKL/jMJM@{JB#GMFGJI
RyMJZK U HU J   kP7MGLDB#RTRsDj?%#M.KLDBY	K UpTG(B#SMffK!MFSDGpy?7S?#]P "
 MFpyjf{ffG!?7KJStIDCDpyKRyE#pTIHGpyP?#}pT?7ND G(} KLDBY*K gQpTSzB#MFSDpyK1C L/MJUj({JLDB#BFG!#M MlKL/}MlGB#H4I M G! w#MFRykDMJofK!IH?7S2KL/ MFS B}B}   WffG U  pTGB#S
MffHK!MFSDGpy?7S?#dP *QIKLDMFS 1}} UIDB#SDCKLtpTGpT4stRTpTMFGKLDBYK 1}} ULDMJjMJP?#jM#IHB#RRKLDM.BYj{JG
U



fi X	ttu
pT
S  BYjM.B#RTG!?BYj{JGpS1U*LDpTGjMFGNDRTKB#RG!?MFSDGN/jMFGKLtBYKMJ#MJjELHLstBYK!K!MJjSpTS pTG(B#RTG?B#S	LHL
stBYK!K!MJjSpTSU
  ofLDI2pyKjfL/CDMFRyE#S I ypyP      f }  UpTG" B# SSDLH{ffMLBYstv7BYB#K!pTS2K!MJI2jB#SGpTUS  U pG@gQB#pTSxSMffMFHpyKK!LDMFMJSDj"Gpy{J?7B#SOG!M ?#P( *\QI`A; M4 {JB#S G!HMJI M" KLD BYK
kB
B 
8}   IDB#SDC	KL/MFS B 
8} "?/
I B#SD
C U LDB+#MlKL/M.GB#4MlLHLsBYK!K!MJjSDGJ
UpTGiKL/MJjMJP?#jMB#SMffK!MFStGpy?7S?#D
P "I[B#{J{ff?#jCDpS/v)K!?.@MJSDpyKpy?7S49>*?#K!M(KLtBYK G  H
I
k mGo X g  ]
oH
mGo
G
m
o
G
m
o

o H X( lg k/o  J DP7
X lg kD]
X( g oB#StC
{ff?7SDGpTC/MJj	{JB#GMFGgQCozB#SDCgM%ofIA@LDpT{L_{ff?#jjMFG!s?7SDCK!?rKL/MxC/MFRTMJKpy?7S?#P"B#SMFC/v#M
 MJKONDGS/?+A
MJKAMJMFy
S B#St
C kgMFpTKL/MJjWBzRTpTSDwq?#jB#SBYj{YIjMFG!sMF{ffKpy#MFRyEDof(pTS{JB#G!M	gQCofIRyMJK UMB#SOMffK!MFSDGpy?7S
?#{P {ff?7SuKB#pTSDpS/v4KL/MlBYjx{   k`tpTS	{JB#G!MgM%ofIRyMJwK U MlB#SuEMffK!MFStGpy?7S	?#{P "S?#KL{JB#G!MFGJI`RyMJK
U!HUp J  
kP7MlA*pRTRtstj?+#MKLDBYZ
K U2pTG(B#SMffK!MFStGpy?7S?#]P *Q
 MFpyjf{ffG!?7KJStIDCDpyKRyE#pTI G(py{JP RyMF BYj@KLDBY*K }  * B#gQSDS/1C ?#K!U M4 KLDLDBFBY#K M KL/M.G B#4HI M G!w# MFRyMJkDK!?7ofI S2K L/MFS  } WGG UpTGB#S
MffHK!MFSDGpy?7S?#P "ItKL/MFS 1} UIB#StC	KL/MJjMJP?#jM } U  *?/I`B#RTR2KL/M"BYj{JG*p
S   BYjM"B#RTG!?
BYj{JG@pT1
S U  ;?#jMJ?+#MJj+ItMJ#MJjELHLsBYK!K!MJjSpT
S   pG(B#RTG!?B#SL/LstBYK!K!MJjSp1
S U 
LDpyjfCDRyE#I pyP     pTGB#SLHLstBYK!K!MJjSpT
S U  gQB#SDCA;MqwSD?+AKLDBYK      H I

 ofI'KLDMFS   
 } U4WG
 
ky
G UpGB#SMffK!MFStGpy?7S?#ZP "I KL/MFS    } "
L/MJjMJP?#jM#I    } gKL/M.jMF4?%B#R'?#PKL/MlBYjx{   k{JB#SDS/?#K*CDMFG!K!j?+E	B#SuELHL	sBYK!K!MJjS
A*L/MJjM   kpTGS/?#KpS[#?7Ry#MFC`of?/I   B#SD
C U  LDBF#MKL/M.GB#4MLHLstBYK!K!MJjStGJ
SxKLDpGAB+E#dI UpTGlB#SxMffHK!MFSDGpy?7S?#wP *	?#jMJ?+#MJj+I9A;Mz{JB#SG!MJMqKLDBYK G  Hf
I
k mGo X  g  G
o H
mGo
G
m
o
G
m
o

J
o H X lg kDo  DP7
X g oB#SDC
Xr lg kD]
g7o*L/MlG{ff?#jMFG*?#{P B#SDy
C *BYjMlKL/M"GB#4MB#G@KL/MlG{ff?#jMFG*?#]P UB#St
C U2jMFG!sMF{ffKpy#MFRyE#IGpTSD{ffM ^ pTG
G{ff?#jMlMFuNDpTB#RyMFSuKJ?#jMJ?+#MJjFIB#G ^ pTGC/MF{ff?74s?7GBYRyM#I/A;M{JB#SAjpyK!M
;g   
 F]o H ^ _g U  ` Fdo HSu ^ j g  KnmGo Xr g  o!]o HSb4  g ^ j g  KnmGo Xr g  o!o ^ j lg k KnmGo X( lg k/o!o H
^
6R g ^ j g  KnmGo X g  o!bo  ^ j g lg  k KnKnmGmGoo X glg k/o!o!{o@o  ^ j lglg kk KnKnmGmGoo X lglg kDkDo!o!oo ^ j lglg kk KnKnmGmGoo X  lglg k/k/o!o!oo HH
X
X
X(
^ j
^ j lg k KnmGo
^ j lg k KnmGo
_g U

F@
o

oH
X lg kDo!o
X( lg k/o!
^
^T j
;g 

F{
o
lg k KnmGo X lg kDo!o^ j
lg k KnmGo Xr lg kDo!o
^
^ j
^ j
guo
 MJK*NDGS/?%A,{ff?7SDGpC/MJj(KL/M.D#MCDpMJjMFS[K({JB#G!MFGF
gQB[oSKLtpTG*{JB#G!M#I`AMlwHS/?+APj?7'BYRyM4KLtBYK mGo? lg k/o H$*?#jMJ?%#MJjFI mGo   lg k/(o H$OgMF{JB#NDG!M
AMBYjM4pTSDG!MJjKpS/vqBRTpS/w/oB#SDC mGo X( lg k/o Hq
I $gMF{JB#NDG!5M U S  ~ mxpTG)o B#Slg MffkDHMo K!~MFHSDGpykY?7IiSp?#M#ffP   mG o KLDBYlg K.kD*o {ff?7H S[KJB#OpSDP7G 
KL/MBYj5{   kDofL/MFS2I2Pj?7 hj?#s?7GpyKpy?7SzAM4?#DKB#pT
Xr
Xr
?#jMJ?+#MJjFI mGo X lg kD]o H mGo Xr lg k/)o  J DPHS$?/It;t guoMF{ff?74MFG
;g   
 F]o H ^ ;g ` Fo  ^ j lg k K $7o ^ j lg k K J OPo
^
g`oDj?7iBYtRyMlAM.v#MJK mxo  lg kDXo HK
I
?#j mxo  lg ao HK
$
I

$
G
m

o

G
m
o
P
lg kDa
o H
I
ItPj?7hj?#s?7GpyKpy?7S	A;M?#DKB#pTS X lg k/o H mGo? lg k/of?#jMJ?+#MJjFI mGo Xr lg k/o H
$
mGo
o  J OPH mGo? lg k/Do  J DP7
X lg kDD
x
m
P o lg kDo H$KL/MFS mGo   lg k/o H J DPgMF{JB#NDGM	A;MBYjMB#CDCtpTS/vKL/MBYj
{   kDof/j?7
G
m
o
x
m
o
x
m

o

hj?#s`?7GpyKpy?7SA;M"?#DKB#pTS Xr lg kD(o H   lg kDo H J OPH
lg kDO
o  J OP7@?#jMJ?+#MJjFI mxo X lg k/(o H
mGo
o  J DPHK$H mGo? lg kDof
X  lg k/
2M

fi

 $,.$ !

f #%$ff&(' *)

"$!$ $



SMFpyKL/MJj@{JB#G!M#It'guoMF{ff?74MFG
;g   `
 Fdo H ^ ;g  Fo@ ^ j glk KnmGo? glk/o!o  ^ j glk KnmGo? glk/oD J OPo
^
gQ{+oS.KLtpTG{JB#G!M#I mGo? lg k/{o HK$B#SDC mGo   glk/o{H J  Kn P79/j?7hj?#s?7GpyKpy?7S.A;M;?#DKB#pTS mxo X  glkDodH
J  Kn P7?#jMJ?+#MJjFI mGo X lg kD]
o H mGo Xr lg kDo  J DPH J  P7;LDMFS2ID guoMF{ff?74MFG
;g   
 F]o H ^ ;g ` Fo  ^ j glk K J  Poi ^ j glk K J  Kn Po
^
gQCoiWG mGo? lg kD]o HK$)B#St%C UpTGB#S4MffHK!MFSDGpT?7S?#=P {ff?7SuKB#pTSDpS/v)KL/MBYj{   kI7Pj?7hj?#s?7GpyKpy?7S4
AMv#MJK mxo X lg k/]o H J DP7	?#jMJ?+#MJjFI mGo Xr lg kD]o H mxo X lg k/o  J DPxHK$S"KLDpTGi{JB#G!M;t/guo'MF{ff?74MFG
;g   
 F]o H ^ ;g ` Fo  ^ j lg k K J DPo' ^ j lg k K $7o
^
gM%o@SKLDpTG){JB#G!M#I'B#G mxo  lg kD2o Hq
I
I hj?#s?7GpTKpy?7SB#GG!MJjKG)KLDBYK mxo X lg k/*o H mGo  lg k/of?#jMJ?+#MJj+I
$
mGo
G
m
o
G
m

o
o H X lg kDo  J DPH  lg kDo  J OP7L/MJjMJP?#jM#It;t guo(MF{ff?74MFG
X  lg k/d
;g   `
 Fdo H ^ ;g  F@o  ^Tj lg k KnmGo  lg kDo!o ^j lg k Knmxo  lg kDo  J DPo
^



]

Bt A 2

qlffB

 

 #.$  

W GA;MLtBF#M}B#RyjMFB#C/EO4MFSuKpy?7S/MFCIKL/MJjMBYjMGMJ#MJjB#R9A?#jwGCDMJ#?#K!MFCxK!?	RyMFBYjStpTS/vBFE#MFGpTB#SS/MJK
A?#jwGFI#A*pyKLDpTSlKL/M(G{ff?#jM%)GMFBYj{L4BYsDstj?7B#{L2I[A*LDpT{fLNDGM;KL/M(G!sB#{ffM?#P{ff?74stRyMJK!MFC4h;*)G9K!?{JBYjjE
?7N/KKL/M.G!MFBYjf{LsDj?H{ffMFGGJ*L/MJjMpTGB4GRTpyv7LuKCDpMJjMFSD{ffM)MJKAMJMFSKL/M?#sMJjBYK!?#jG({ff?7SDGpC/MJjMFCpTSzKL/M
CDpMJjMFSuKA?#jwGF;KL/MB#CDCDpyKpy?7SB#SDCC/MFRTMJKpy?7S	?#PMFC/v#MFGWpTG*{ff?7StGpTC/MJjMFC	uE	OB#CDpyv7B#SOMJK@B#R'gkFm#m#7ofI
A*pyKLtpTSBBYjw#?+LDB#pSx	?7SuK!MBYjfRy?sDj?H{ffMFGGJIA@LDpT{LB#RTG!?sMJjP?#jG.?7S[K!MBYjRy?	GB#stRTpTS/v
Pj?7KLDMWG!sB#{ffMW?#P KL/M@?#jfC/MJjpTS/v7G?#PKL/MWBYjfpTBYtRyMFG;{ff?74stBYKpytRTM*A*pyKLqKL/M){JN/jjMFSuK(h*"D;C/v#M
B#CDCDpTKpy?7SqB#StCC/MFRyMJKpy?7SpTGB#RTG!?4NDGMFCqEHstpyjK!MFGB#SDC	MJMJwgkFm#m#7ofItN/KA*pyKLDpTSzB"v#jMJMFC/EqsDj?H{ffMFGG
KLDBYKDjfG!K)v#j?+A*G.KL/MG!K!jND{ffKNDjMuEB#CDCDpTS/vzMFC/v#MFG.B#StCKL/MFSKLDpTSDG)pyK)ECDMFRyMJKpTS/vzMFC/v#MFGJ4WCDCDp
Kpy?7SDB#R`?#sMJjBYK!?#jGBYjM{ff?7SDGpTC/MJjMFCquELDpT{w#MJjpTS/vgkFm#m#7ofIpTSt{JRTNDCDpTSDv.BYj{WjMJ#MJjGB#RB#SDC{ffjMFBYKpy?7S?#P
uG!K!jND{ffKN/jMFGJ
WRTR'KL/MFG!M44MJKL/?HCDGW4?+#M4KL/j?7N/v7LKLDMGstB#{ffM?#P{ff?74stRyMJK!MFCh;*)G.pTSOKL/M"P?7RRy?+A*pS/v}ABFE`
v7py#MFSKLDMl{JN/jjMFSuK@h;*
 "IBYPK!MJjWG!MFRyMF{ffKpTS/vB#S	?#s`MJjfBYK!?#jFItBYsDstRTEpTSDv"pyK(K!? B#SDC?#DKB#pSDpTS/v4B
S/MFpyv7Lu?#jpTS/v"h*f
 *I/KL/MJEzv#MFS/MJjBYK!MlB	fcY^/\f]\aZJ^ta([Z P7aZJ^/\f]cYc^ U`?#dP *9gQB4*`MFRT?7S/v7pTS/v"K!?
KL/M)MFNDpyYB#RyMFSD{ffM){JRTB#GGjMJsDjMFGMFS[K!MFCzE}KL/M)h*lofItpyP?7S/MWMff/pTG!KGJ9PKLDpTGpTGKL/M{JB#G!Mg?#KL/MJjA@pTG!M
*7pTG S/?#K'B(YB#RTpTCl{ff?7S/Dv7N/jBYKpT?7SofI%KLDMFu
S *7pTGMJB#RTNtBYK!MFC.E){ff?7stN/KpTS/v*KL/MG{ff?#jM;?#!P UI _g U` Fof
L/M"{ff?74stRyMJK!MFCh**jMJsDjMFG!MFSuKBYKpy?7SO?#(P   pGKL/MFSOjMF{ff?+#MJjMFCPj?7pyKGW{ff?7SDGpTGK!MFS[K@^ MffK!MFSDGpy?7S
U 
LDM9sDj?H{ffMFGG2?#P/{L/MF{wpS/vKL/MMff/pTG!K!MFSt{ffM9?#PHB*{ff?7SDGpTG!K!MFSuKMffHK!MFSDGpy?7SlB#SDCv#MFS/MJjBYKpS/vpyKpTG{JBYjjpyMFC
?7N/KA@pyKL4B)sDj?H{ffMFCDN/jM{JB#RRyMFcC {w%a=c# w%gQW?#j_'BYjfGpIDkFm#m#7ofIHA*LDpT{fLjNDSDG9pSKpT4M lg Qao
pTSKLDMlA?#jG!K@{JB#GM#IA*LDMJjM "C/MFS/?#K!MFG@KL/MSNDMJj?#P9MFC/v#MFGWpTSKL/M"h*"WS/?#KL/MJjWsDj?H{ffMFCDN/jM#I
{JB#RTRyMF
C 	%a=cY; {w%I2pTG.pTSu#?#w#MFCpSx?#jC/MJjK!??#DKB#pSxKL/Mz{ff?74stRyMJK!MFCh*jMJstjMFG!MFSuKBYKpy?7S
?#P"KL/MxS/MJA YB#RTpTC{ff?7S/Dv7NDjBYKpy?7S2L/MJjMxBYjMxCDpMJjMFS[KpTstRyMF4MFSuKBYKpy?7SDG?#P*@=K!?Yh;*
gQWSDC/MJjGG?7SMJK;B#R=yIkFm#m7uLDp{w#MJjpTSDv/ItkFm#m#t	MJMJw`I2kFm#m#/h'MFBYjfR`9MJjBHIkFm#mY[of;/?#jMffHB#stRyM#I
KL/MWKpT4M){ff?74stRyMff/pyK|E4?#P KL/M)B#Ryv#?#jpyKLtsDj?#s`?7GMFCEzLDpT{w#MJjpTS/vgkFm#m#7opTsG g o9?7SzKL/M)BF#MJjBYv#M
B#SD
C }lg Qc%opSKL/M.A?#jG!K*{JB#G!M#
 NDjGMFBYj{L4MJKLD?CC/?MFG(S/?#K(S/MJMFCzK!?NDG!M)B#S[Eq?#P KL/MFGMWK|A;?}sDj?H{ffMFCDN/jMFGJpTSz?#jC/MJjK!?4{LDMF{w
KL/M4YB#RTpTCDpTKE	?#PBS/MFpyv7Lu?#jpTS/vz{ff?7S/Dv7N/jBYKpT?7S?#PB#Sx@h* "I'pTK)pG)?7SDRyES/MF{ffMFGGBYjE#I'pSG!?74M

%

2

fi X	ttu
{JB#G!MFGJI;K!?s`MJjP?#jBK!MFG!KK!?xC/MJK!MF{ffK4MFpyKL/MJj4B#SNDSDCtpyjMF{ffK!MFCstBYKL?#jBOsBYjKpTB#RTRyECDpTjMF{ffK!MFCstBYKL
MJKAMJMFSK|A;?S/?HC/MFG*pSygQpT4stRyMFMFS[K!MFCuEsDj?H{ffMFCDN/jMFGa  g=o(B#SDCF  g=opTSMF{ffKpy?7S	/7of  S
KL/M?#KL/MJjLtB#SDC2IY?7SD{ffM(KL/M(G!MFBYj{fL4sDj?H{ffMFGG9LDB#GMffHstRy?#jMFCKLDMSDMFpyv7L[?#jLD?u?HCl?#P=B#SDCC/MJK!MJj}pTS/MFC
KL/M.MFG!K(S/MFpTv7L[?#jpTSDv{ff?7SDDv7N/jBYKpy?7
S I *2pTG(S/?#KB#RyABFEHG*B#SO*h**"IB#SDCAMl"NDG!K(v#MFS/MJjfBYK!M
pyKG"*h;*jMJsDjMFG!MFS[KBYKpT?7S2*LDpTG.v#MFSDMJjBYKpy?7SsDj?H{ffMFCDN/jMzpTG"B#RTG!?#MJjEGpTstRyM#4pyK{ff?7StGpTG!KG"pTS
DjpS/v/IGKBYjKpTS/vPj?7 BGpTS/v7RyM)S/?HC/M kI/B4{JB#G{JB#CDMFCsDj?H{ffMFGGKLDBYK(MFpyKLDMJjCDpyjMF{ffKGRTpTSDwGBFABFEPj?7
k?#j4NDSDCDpTjMF{ffKGlBYj{JGgQpT4sRyMF4MFSuK!MFCEsDj?H{ffMFCDN/jMFG a  B  | [g=olB#SDw
C wQ ? g=olpTSMF{ffKpy?7S/7of
>@?#K!MKLDBYK4B#RTRKLDMFG!MsDj?H{ffMFCDN/jMFG"NDG!MFCEx?7N/j4G!MFBYj{fL4MJKL/?HCBYjMRTMFGG"KpT4Mff{ff?7SDGNtpTS/vOKLDB#S
h**@=K!?Y*B#SDC*@=K!?Yh**"
?#jMpT4s?#jKB#SuKRyE#I?7N/jG!MFBYj{fLMJKL/?C{JB#SKBYw#MB#C/B#SuKBYv#M?#P.KL/MC/MF{ff?74s?7GBYpTRTpyK|E?#P
B#SuErG{ff?#jpTSDvxPNtSD{ffKpy?7SDGJIB#SDCMFB#{L*h**gMff/{ffMJsDKzKL/MOpSDpyKpTB#R?7S/M%o{JB#SrM	MJYB#RTNDBYK!MFCuE
{ff?74stNDKpTS/v?7SDRTEzK|A;?Ry?H{JB#R G{ff?#jMFGJ*@?+AMJ#MJjFI2KL/M"4MJKL/?HCDGtB#GMFC?7S{ff?74stRyMJK!MFCh**)G)S/MJMFC
K!?zjMF{ff?74stN/K!M"B#RTRKLDMRy?H{JB#R G{ff?#jMFGJI2B#RyKL/?7N/v7LKL/MB#Ryv#?#jfpyKLDstj?#s?7G!MFC	ENtS[K!MFSDB#NB#SDCB#N
gY##[ofI A@LDpT{L?#sMJjBYK!MFG)?7S{ff?74sRyMJK!MFCh*)GB#SDCNDG!MFGWKL/jMJM4pTStG!MJjKpy?7SO?#sMJjBYK!?#jGgP?#j)BYjf{JGJI
RTpTSDwGWB#SDCuG!K!jND{ffKN/jMFGfo@pTGB#RTG!?BYtRyMK!?	G{ff?#jMB#S[ES/MFpyv7Lu?#jpTS/vz{ff?7S/Dv7N/jBYKpT?7SNDGpTS/vzKA?Ry?H{JB#R
G{ff?#jMFGFtL/?+AMJ#MJjFIKL/M.YB#RTpTCDpyK|E}{ff?7SDCDpTKpy?7SDG?#P9G!?74M.?#PiKL/MFG!Ml?#s`MJjfBYK!?#jG*BYjM.SD?#K*{ff?#jjMF{ffKJ
pTSDB#RRyE#ILtpT{w#MJjpS/vgY#77o zC/MFG{ffjpT`MFGlB#SB#Ryv#?#jpTKLD KLDBYKG!MFBYj{fL/MFG"pTSxKL/MzG!stB#{ffMq?#P{ff?7
stRyMJK!MFCh;*)GlB#StCpGB#RTG!?BYtRyM"K!?MJB#RNDBYK!M{ff?7S/Dv7N/jfBYKpy?7SDGWuE{ff?74stN/KpS/vq?7StRyEgQN/sK!?P?7N/jo
Ry?H{JB#R'G{ff?#jMFGJ*K)NDG!MFGWGp?#sMJjBYK!?#jGJI2RTpTSDwzB#SDCBYj{"B#CDCDpyKpT?7S2ItRTpTSDwB#StCOBYj{"C/MFRyMJKpT?7S2I{ffjMFBYKpT?7SO?#P
uG!K!jND{ffKN/jMFG(uECDpTjMF{ffKpTS/vKA?B#RyjMFB#C/EzMffHpG!KpTS/vRTpS/wGFIHB#StCjMJ#MJjGB#R?#PBYjf{JGJ;WRTR2KL/Ml?#s`MJjfBYK!?#jG
{JB#SMMJYB#RTNDBYK!MFCNtGpTS/vqKA?	Ry?{JB#RG{ff?#jMFGJI2MffH{ffMJstKjMJ#MJjGB#R9B#SDC{ffjMFBYKpT?7S?#PuG!K!jND{ffKN/jMFGJIKLDBYK
jMFNDpyjM(P?7N/jRy?{JB#RG{ff?#jMFGJ*L/MYB#RTpTCDpyK|E"{ff?7StCDpyKpy?7SDG?#PKL/M*?#sMJjBYK!?#jGBYjM@MFG!KBYtRTpTGL/MFC4MFGG!MFSuKpTB#RTRTE
pTSK!MJj}G?#PKA?{ff?7SDCDpyKpT?7SDGJgk+o)KL/MBYG!MFSD{ffM?#PG!MFpCtpyjMF{ffK!MFC?#jNtSDCDpyjMF{ffK!MFCstBYKLtGMJKAMJMFS
KA?}S/?HC/MFG(KLtBYKC/?S/?#KstB#GG(KL/j?7N/v7L{ffMJjKB#pTS	G!MJK?#PS/?HC/MFGJhI ItB#StCxg7oKL/M.PQB#{ffKKLDBYK*B4{ffMJjKB#pTS
G!MJK?#P2S/?HC/MFGP?#jGBl{JRTpTN/M#i'pTS/w"pTSDG!MJjKpT?7S"B#StC{ffjMFBYKpy?7S}?#PuG!K!jND{ffKNDjMFGS/MJMFCKL/M(DjG!K9K|EusM?#P
{ff?7SDCDpTKpy?7S2IuRpTS/w4B#SDCBYj{WC/MFRyMJKpy?7SzS/MJMFCzKL/M)G!MF{ff?7SDCz?7S/M#IHA*L/MJjMFB#GBYj{WpTSDGMJjKpy?7S}B#StCzBYjf{*jMJ#MJjfGB#R
jMFNDpyjM?#KLO{ff?7SDCtpyKpy?7SDGJWL/H
M stBYKL qB#RTpCDpyKE	{ff?7StCDpyKpy?7SDG*KBYw#M4KpT42M ng ~Z~
 o*pSKL/M"A?#jG!K
{JB#G!M#I`B#SDCKL/`
M !{JRTpuN/M 4{ff?7StCDpyKpy?7SDGKBYw#MKpT4ZM }ng ~Z~ ofItB#RTG!?pSKL/M.A?#jG!K*{JB#G!M#;LDpTG(B#Ryv#?#jfpyKLD
B#RTG!?4jMFuNDpTjMFGKL/uM 	%a=cY; 	%B#SD
C 	5a=cY; {w%sDj?H{ffMFCDN/jMFGK!?MNDG!MFC
?/IB#RyKLD?7N/v7L_KL/MxYB#RTpTCtpyKE{ff?7StCDpyKpy?7SDG?#PKL/M?#sMJjBYK!?#jfGpTS,LtpT{w#MJjpS/v GB#Ryv#?#jpyKLt B#SDC
KL/MFpyjzs?7G!K!sDj?H{ffMFGGpS/vBYjMG!?7MJA*LDBYK4?#jM{ff?74stRyMffKLDB#S?7N/jGJIKLDMB#CDB#SuKBYv#MxpTGqKLDBYKKLDpTG
B#Ryv#?#jpTKLDC/?MFGS/?#K*LtBF#MlB#SuECDN/stRTp{JBYK!MWjMJsDjMFG!MFSuKBYKpy?7SDG*?#PiKLDMMFuNDpTB#RyMFSt{ffM.{JRTB#GG!MFGJ;LDMJKL/MJj
KL/M{ff?74stN/KBYKpT?7SDB#RH{ff?7G!K?#P4?%#MFG9pTSlKLDM(h*G!sB#{ffM{JB#S{ff?74sMFSDGBYK!MP?#jKL/MRTBYjv#MJj9SuNtlMJj
?#P*h;*)G)gQB#SDCKLDM@RTBYjv#MJjSuNtlMJj?#PRy?H{JB#RDG{ff?#jMFGK!?`M@{ff?74stN/K!MFCo9pTGB.}BYK!K!MJj?#PMF4spyjpT{JB#R
MJYB#RTNDBYKpy?7S2I'KLDBYKlA@pTRTRs?7GGpyRyEC/MJsMFSDC?7SKL/
M !G!sBYjG!MFS/MFGG ?#PKL/MqG!sMF{Jpy{4C/?7B#pTSstj?#tRyMF
{ff?7SDGpC/MJjMFC2
1 7#{8ff>$[<> 5 =< 1 >Y: <
SKLDpTGGMF{ffKpy?7S.AMGLDB#RR#C/MFG{ffjpyM9KL/MMffsMJjpTMFS[KG2{JBYjjfpyMFC?7N/K'A*pyKL)?7N/j B#RTv#?#jpyKLDIKL/M?#tKB#pTS/MFC
jMFGNtRyKGJIuB#StC}Bl{ff?7stBYjBYKpy#M)G!KNDC/EA*pTKL4?#KL/MJjB#Ryv#?#jpyKLD}GP?#j;RyMFBYjStpTS/vlBFE#MFGpTB#SS/MJKA?#jwHGJ9M
LDB+#MG!MFRyMF{ffK!MFCSDpTS/MCDpMJjMFSuK'sDj?#tRyMFG'K!?)K!MFG!K?7N/j9B#Ryv#?#jpyKLD	IYB#RR[?#PDA@LDpT{L?7SDRyE{ff?7S[KB#pTSCDpG{ffjMJK!M
YBYjpTBYtRyMFGFiWRTBYjgQipTv7N/jMkFn7ofIHSDGN/jfB#SD{ffMgQpyv7N/jM"kFm7ofI/@B#pTRTtSDC/MJj*gQpyv7N/jMWY[ofI/jMFB#G!KB#SD{ffMJj+I
{ffj!ItRTBYjM+It@?7NDG!Mff=9?#K!MFGFIONDGL/j?u?7I/B#SDC>@N/jfG!MJjE#
FB
  = ; W" #!= )n  ==if; ,vYH %ff   W" /= D Y #|!

/

2

fi

f #%$ff&(' *)

 $,.$ !

"$!$ $



LDMWRTBYjSDMJKA?#jw"CtpTG!stRTB+EGiKL/MjMFRTMJB#SuK9BYjfpTBYtRyMFGB#SDCjMFRBYKpy?7SDGLDpTstG P?#j9KL/MWRTBYj?7SH
yp K!?#jpS/vEHG!K!MF gQMFpTSDRTpT{fL}MJK(B#RyIkFm#n#m7ofItB"CDpTBYv7S/?7GKpT{*BYsDsRTpT{JBYKpy?7SqP?#jstBYKpyMFSuK4?7SDpyK!?#jpS/v/9LDpTG
S/MJK|A;?#jw`I`A*LDpT{fL	{ff?7S[KB#pSDGW74BYjfpTBYtRyMFG@B#SDC	[}BYj{JGJILtB#G*MJMFS	{ff?7SDGpTC/MJjMFCOB#G@B}`MFSt{LDBYjwqP?#j
MJYB#RTNDBYKpTS/vB+E#MFGpTB#SS/MJKA?#jwRyMFBYjSDpS/v4B#Ryv#?#jpyKLD}GJL/M.pTSDstN/K(CDBYKB{ff?7?7SDRyENDG!MFC	BYjMGN//
G!MJKG?#PKL/MWRTBYjCDBYKBYtB#G!MNDpTRyKiuE@MJjG!w#?+HpyKGWgkFm#mHk+ofIA@LDpT{L4{ff?7SuKB#pTSDG;Y###{JB#G!MFGKLDBYK9AMJjM
G!K!?H{LDB#GKpT{JB#RTRyEzv#MFS/MJjBYK!MFCNDGpTSDv"KLDMl@RBYj S/MJKA?#jw`;S?7N/jMffHsMJjpT4MFSuKGJI/AMlLDB+#MNDG!MFC	KL/jMJM
CDBYKBYtB#GMFG(?#PiCtp`MJjMFS[K(GpyJMFG.gKLDMDjG!sK {JB#G!MFGpTSKLDM)WRTBYj CtBYKBYtB#G!M#IDP?#vj 5H_Y## K Y##B#SDC
kJ###[of
10

21

19

20

31

4

27

11

28

29

7

8

13

22
15

6

17

25

18

26

1

2

3

5

23

32

34

35

12

9

16

36

37

24

33

14

30

 pyv7NDjM4kFn;L/M.WRTBYj SDMJKA?#jw
SDGN/jfB#SD{ffMgQpTSDC/MJjMJKqB#RyIkFm#m7#opTG}BxS/MJKA?#jwP?#j}MJYB#RTNDBYKpTSDvx{JBYjqpTSDGN/jB#SD{ffMjpG!wGFrL/M
SDGN/jB#St{ffM@S/MJK|A;?#jwz{ff?7S[KB#pSDG(7.YBYjpTBYRyMFG;B#StCz#"BYj{JGJ9Sq?7N/jMffsMJjpTMFS[KGJIAMWLDB+#MNDG!MFCzD#M
CDBYKBYtB#GMFG*{ff?7SuKB#pTSDpTS/v	kJ###z{JB#G!MFGJItv#MFS/MJjBYK!MFCOPj?7KLDM.StGN/jB#SD{ffMBFE#MFGpTB#SOSDMJKA?#jw
WB#pTRytStC/MJjlgQ*tjB#G!?7SMJK*B#RyI'kFm#m#7o*pTGBqS/?#jBYKpT#MlG!EHG!K!MFKLDBYK@P?#jMF{JB#G!KG@GMJ#MJjMGNt4MJj
LDB#pTR/pSS/?#jKL/MFB#GK!MJjSz?7RT?#jB#C/?/9L/MWB#pTRytStC/MJj9S/MJKA?#jw{ff?7SuKB#pTSDG;#.BYjfpTBYtRyMFG9B#SDCz#BYj{JGJ9S
KLDpTG@{JB#G!M#I`A;M"LDBF#MB#RTG!?zNDG!MFC	t#MCDBYKBYtB#GMFG*A*pTKLkJ###{JB#G!MFG@v#MFS/MJjBYK!MFCPj?7KL/M"@B#pTRTtSDC/MJj
S/MJK|A;?#jw`
jMFB#G!K(B#SD{ffMJjFI{ffj!I2iRBYjM+I2*?7NDGMff=9?#K!MFGJIiONDGL/j?u?7I`B#SDC>@NDjG!MJjE	BYjMCDBYKBYtB#GMFGBFYB#pTR
BYtRyM}Pj?7 KL/M 0B#{fLDpTS/MzMFBYjSDpS/v	MJs?7GpyK!?#jE#zjMFB#G!K(B#SD{ffMJj"{ff?7SuKB#pTSDGqkJ	YBYjpTBYtRTMFG}gm
BYK!K!jpyN/K!MFGJI7K|A;??#P`A@LDpT{L4LtBF#M@}pTGGpTSDv@YB#RTN/MFGJIuB#SDCBtpSDBYjE{JRB#GG9YBYjpTBYtRyM%o'B#SDCq#n#.pTSDGKB#SD{ffMFGJ
L/M{ffj!CDBYKBYtB#G!M4{ff?7SD{ffMJjStG{ffjMFCDpyK){JBYjCBYsDstRpT{JBYKpy?7SDGJWK)LtB#G@[mY{JB#GMFG.B#SDCkFqBYjpBYtRyMFG4gkF
BYK!K!jpyN/K!MFG@B#StCBz{JRTB#GGWYBYjpTBYtRTM%ofIB#SDCG!MJ#MFSYBYjpTBYtRyMFGWLDB+#M}pTGGpTSDv4B#RTNDMFGJ)?#jMJ?%#MJjFI Gp	?#P
KL/MYBYjpTBYtRyMFGipTSKL/M({ffj!CDBYKBYB#G!MBYjM({ff?7S[KpSuN/?7NtGiB#StCAMJjM(CDpTG{ffjMJKpyJMFC"NDGpTS/vWKL/M(.G!EHG
K!MF gQ?7LtBFHpID#?7LDS2I/ ?7S/v/ItB#StRyMJEz h DMJv#MJjFIkFm#muof;RTBYjM+NDG!MFG)kF"BYjfpTBYtRyMFGgkJ4BYK!K!jpyN/K!MFG
B#SDCx{JRTB#GG)BYjpBYtRyMFGJI2?7S/M4P?#jKL/MSND`MJj)?#PKpT4MFGB{ffMJjKB#pSKEsM"?#PG!?7RTBYj BYjM4?{J{JNDjMFCpTS
BYL/?7N/j*s`MJjfpy?C`oB#StC{ff?7S[KB#pTStG.kJ7#pTStG!KB#SD{ffMFGJI/A*pTKL/?7N/KpTGGpS/vYB#RTN/MFGF*?7NDGMff=9?#K!MFG@G!K!?#jMFG
KL/M@#?#K!MFGP?#j;MFB#{fL}?#P2KLD	M 0lu@?7NDG!M?#P*MJsDjMFG!MFSuKBYKpy#MFG(?7S/v#jMFGG4MFS?7SOkF.w#MJE4#?#K!MFGJ/pyK;LDB#G
k+qYBYjpTBYtRyMFG)B#SDC[#jMF{ff?#jCDGB#SDCB#RTRiKL/MYBYjpTBYtRyMFGWMff/{ffMJsDKKA?LDB+#MpTGGpTS/v}YB#RTN/MFGFNtGLH
j??7{ff?7SuKB#pTSDGnHkF4{JB#G!MFG{ff?#jjMFG!s?7SDCtpTS/vK!?G!sMF{JpyMFG?#Pv7pRTRyMFC"NDGL/j?u?7G;pTS4KL/M)@v7BYjpT{JNDGB#SDC
MJspy?#KBDB#pTRTEKLDMJjM4BYjM}#zBYjfpTBYtRyMFG4gQB{JRTB#GG)YBYjpTBYRyM#IG!KBYKpTS/vA*LDMJKL/MJjKL/M"NDGL/j??7 pTG
MFCDpyRyM?#js?7pTG?7S/?7NDGJI2B#SDCx#BYK!K!jpTtN/K!MBYjfpTBYtRyMFGfo*B#SDC?7SDRTE	?7S/M4YBYjpTBYtRyMLDB#GpGGpTS/vqB#RN/MFGJ

fiN

2

fi X	ttu

Age

SocioEcon

GoodStudent

AntiTheft

OtherCar

RiskAversion

HomeBase

Mileage

CarValue

SeniorTrain

VehicleYear

RuggedAuto

Theft

MakeModel

Antilock

Accident

ThisCarDam

OtherCarCost

DrivingSkill

Airbag

DrivQuality

DrivHist

Cushioning

ILiCost

MedCost

ThisCarCost

PropCost

pyv7N/jMkFmL/MSDGN/jfB#SD{ffMS/MJKA?#jw
>WN/jG!MJjE{ff?7SuKB#pTSDGCDBYKB.jMFRBYKpy#MK!?"KL/MMJYB#RTNDBYKpy?7S}?#PBYststRTpT{JBYKpy?7StG9P?#jSuNDjG!MJjEG{LD?u?7RTGFIuB#SDCqLDB#G
m4YBYjpTBYRyMFGB#SDCkF#m#Yz{JB#GMFGJIA@pyKL/?7N/KpGGpTS/v4YB#RTN/MFGF;SB#RTR ?#PKL/M{JB#G!MFGFI`pGGpTS/v4YB#RTN/MFG@BYjM
S/?#K*CtpTG{JBYjC/MFCztNDK(K!jMFBYK!MFCB#G@BCDpTG!KpSD{ffK(G!KBYK!M#
S}KLDM*DjGKG!MJjpyMFG;?#P2MffHs`MJjfpT4MFSuKGJI[AMWB#pK!?{ff?74stBYjMWKL/M*MFLDB+pT?#j?#P?7N/j*h;*@=tB#G!MFC
Ry?H{JB#R9G!MFBYj{fLx4MJKL/?HCrg 
	fiff'o@A@pyKLKL/M{JRTB#GGp{JB#R9Ry?{JB#R9GMFBYj{LxpSOKLDMG!stB#{ffM4?#P*)Gq7g fiffiof
L/MG{ff?#jpS/v}PQNDSD{ffKpy?7SG!MFRyMF{ffK!MFCpG@WMFNgQ*MF{w#MJjB#SxMJK)B#RyI9kFm#m#7ogA*LDp{LpTGWG{ff?#jM"MFuNDpTB#RyMFSuK
B#SDCC/MF{ff?7s`?7GBYtRyM%ofIA*pTKLKLDMstBYjB#4MJK!MJj4jMJstjMFG!MFSuKpTS/vKL/MMFuNtpyB#RTMFS[K4GB#4sRyMGpyJMG!MJK4K!?rk
B#SDCB4NDStpyP?#jfG!K!jfND{ffKN/jM)sDjpT?#jFi*L/MG!KBYjKpTS/vs?7pTSuK?#P KL/M.G!MFBYjf{L	pTGKL/MMFsDKEzv#jBYstLpTSz?#KL
{JB#G!MFGJ
M"LDBF#Ml{ff?7RRyMF{ffK!MFC	KL/MlP?7RTRT?+A*pTSDv"pTSDP?#j}BYKpy?7SBY?7N/K(KL/M.MffHsMJjpT4MFSuKGJ
=    L/M.WMFNG{ff?#jMqgQRy?#v#MJjfGpy?7So;?#PiKL/MlRyMFBYjfS/MFCS/MJKA?#jw`
UEN L/M.SNDlMJj(?#PMFC/v#MFG@pTSD{JRTNDCDMFCqpTSKLDMlRyMFBYjS/MFCSDMJKA?#jw
  LDMzWB#pTSDvCDpG!KB#SD{ffM#I
 H)).WfI9pM#KL/MSuNDMJj?#PWCDpMJjMFSuKMFC/v#MFGFI;B#CDC/MFC_gQofI
CDMFRyMJK!MFCgQ.ofI?#j;A*j?7S/v7RyE"?#jpyMFSuK!MFCgA@pyKL/?7N/KKBYwHpTS/v"pTSuK!?B#{J{ff?7NDSuK;KLDM@CDpMJjMFSD{ffMFGMJKAMJMFS
MFNDpyYB#RyMFSuK.G!K!jND{ffKNDjMFGfogofI'pTSKL/MRTMFBYjS/MFCxS/MJK|A;?#jwA@pyKLjMFG!sMF{ffK.K!?	KLDMv#?7RTCxG!KB#StCDBYjC
SDMJKA?#jwrgKL/M4?#jpyv7pTStB#R'4?HC/MFRof"LDpTG)4MFB#GN/jM4pTG)?7SDRyE{ff?74stN/K!MFCP?#jKL/M4KL/jMJM"K!MFGKlC/?Y
}B#pTSDGA*L/MJjM.B4v#?7RTC	G!KB#StCDBYjCMff/pTG!KGJ
  LDMSuNDMJj?#P2pyK!MJjBYKpy?7StG{JBYjjpTMFC4?7N/KEKL/M*B#Ryv#?#jpTKLDK!?ljMFB#{LqKL/MMFG!KS/MJK|A;?#jwIHpM#
KLDMlSuNtlMJj(?#Pi?#s`MJjfBYK!?#jG*NDGMFCK!?K!jB#SDGP?#jKLDMlpTSDpyKpB#Rv#jfBYstLpTSuK!?}B4Ry?H{JB#R2?#sDKpT"ND
 #t L/M.SNDMJj(?#PpSDCDpyHpTCDNDB#RG*gMFpyKL/MJj***)G*?#jW*h;*)Gfo(MJB#RTNtBYK!MFC	EqKL/M.B#RTv#?#jpyKLD
2i

fi

 $,.$ !

f #%$ff&(' *)

"$!$ $

QGVertMotion

N0_7muVerMo



SubjVertMo

CombVerMo

RaoContMoist

AreaMeso_ALS

CombMoisture

AreaMoDryAir

AMInstabMt

CldShadeConv

ScenRelAMIns

LLIW

LatestCIN

CurPropConv

MountainFcst

SfcWndShfDis

ScenRel3_4

RHRatio

AMDewptCalPl

WindFieldPln

TempDis

SynForcng

MeanRH

CombClouds

OutflowFrMt

Boundaries

LowLLapse

WindFieldMt

WindAloft

AMInsWliScen

ScnRelPlFcst

IRCloudCover

WndHodograph

MorningBound

Scenario

VISCloudCov

CldShadeOth

InsInMt

Date

SatContMoist

MvmtFeatures

MidLLapse

LIfr12ZDENSd

LoLevMoistAd

InsChange

InsSclInScen

MorningCIN

ScenRelAMCIN

AMCINInScen

Dewpoints

CompPlFcst

CapChange

CapInScen

PlainsFcst

N34StarFcst

R5Fcst

U fiU


pyv7N/jMlYHL/M.WB#pTRytStC/MJj(S/MJKA?#jw

L/MSuNtlMJj?#PiY] fWZJd!ZJ^taHG!KBYKpTG!KpT{JGMJYB#RTNDBYK!MFC4CtN/jpTS/v@KL/MMffMF{JNDKpy?7S?#PtKL/M(B#Ryv#?#jfpyKLD
*LDpTG"pTGBNDG!MJPQNDRYB#RTN/MzK!?x4MFB#GNDjMqKLDMzMJ~q{JpyMFSD{ffE?#P@KL/M	B#Ryv#?#jpTKLDGJI9MF{JB#NDG!M4?7GK"?#P

<

2

fi X	ttu
KLDM4jNDSDSDpS/vqKp4M?#PB	G{ff?#jpTS/vY=tB#GMFCRyMFBYjSDpTSDvzB#Ryv#?#jfpyKLDpTGG!sMFSuK.pTSKL/M4MJYB#RTNDBYKpy?7S?#P
GKBYKpTG!KpT{JG*Pj?7KLDMlCDBYKBYtB#G!M#
 U L/M"K!?#KB#R9SuNDMJj@?#PG!KBYKpTG!Kp{JGWNtG!MFCuE	KLDMB#RTv#?#jpyKLD>@?#K!M4KLDBYK)KLDpTGWSuNtlMJj{JB#S
M{ff?7SDGpTC/MJjBYtRTE	v#jMFBYK!MJjlKLtB#SxG!KENDGpS/vzLtB#GLDpTS/vK!MF{fLDSDpTN/MFGAM{JB#SG!K!?#jM}B#SDC
MJ~q{JpyMFS[KRTE4jMJK!jpyMJ#M)B#S[E4stjMJpT?7NDGRyE{JB#RT{JNDRTBYK!MFCzG!KBYKpTGKpT{JGJ|KpTG;S/?#KKL/MJjMJP?#jM)S/MF{ffMFGGBYjE4K!?
jMF{ff?74stN/K!M.KLDMF uEB#{J{ffMFGGpTS/v4KL/MlCtBYKBYtB#G!M#ItKLNDG(v7B#pSDpTS/v4pTSMJ~}{JpTMFSD{ffE#
,YB#RTN/MF*G L/p M@B+#MJjBYv#M.pTSuSzNtlMJjg7?#o!Po;{ffBY?7jf4pTBYstN/RyMFK!MFGC2KLDBYKLDppS[GK!YMJB#j#RTN/MFMWSDMWpTG;pTSqB#RKG!L/?"M)p4CDps?#MJjjKMFB#SuS[K;KG!KBYMFK{JpTB#GNDKpTG!{JMWGKgQL/pMWM#9KpTK4L/MM
jMFuNDpTjMFCgK!q ?r=sv{fft?74g sN/K!M"BG!KBYKpG!KpT{"pTSD{ffjMFB#G!MFGWMffHs?7S/MFS[KpB#RTRyEA*pyKLOKL/MSuNDMJj@?#PBYjpBYtRyMFG
pS[#?7Ry#MFC
  LDMKpTM#I4MFB#GN/jMFCpTS\JZcY^`\fIMFstRy?+E#MFCEKLDM"B#Ryv#?#jfpyKLDK!?RyMFBYjSKL/M4S/MJKA?#jw`
 N/j.pT4stRTMF4MFS[KBYKpT?7SxpTG)AjpTK!K!MFSxpTSKL/M}[ sDj?#v#jB#}pTS/vRTB#S/v7NDBYv#MqB#SDCjNDStGWNtSDC/MJj
'pTSNH`LDpTGYB#RTN/MpTG"?7SDRyEBj?7N/v7L4MFB#GNDjM?#P*KL/MMJ~}{JpTMFSD{ffE?#PWKL/MB#Ryv#?#jpyKLtGJIMff
{JB#NtG!MKL/MJjMBYjMB#S[E{Jpyj{JNDGKB#SD{ffMFG"KLDBYK"}BFEpS tN/MFSD{ffM}KL/MjNDSDSDpS/vKpT4MgMffK!MJjfSDB#R
RT?7B#CDpTS/vpTSBS/MJK|A;?#jw#MFCr{ff?74stN/K!MJj+I{JB#{LDpTSDv?#j4B#SuE?#KL/MJjB#Gs`MF{ffK"?#P*KLDMz{ff?74sN/K!MJjBYj!
{fLDpyK!MF{ffKN/jM#I4MF?#jEstBYv7pS/v/INDG!M?#PWpTjKNDB#R4MF4?#jE#I;KL/jMFB#CDpTSDv/ICDpMJjMFSuK{ff?HC/M#I;MJK{Yof
>@MJ#MJjKL/MFRTMFGGJIAM	LDB+#MK!jpyMFCK!?MFSDGN/jMqKLDBYKKL/MK|A;?B#RTv#?#jpyKLDGjNDSNDStC/MJjKLDM	GB#4M
{ff?7StCDpyKpy?7SDGB#G"PBYjB#G"s`?7GGpytRyM#I'B#SDCKLDMqK|A;?pT4sRyMF4MFSuKBYKpy?7SDG.GLtBYjMz?7G!Kl?#PKLDMq{ff?HC/M#
SzPB#{ffKJI/KLDMWK|A;?B#RTv#?#jpyKLDGLDB+#MMJMFSzpS[K!MJv#jBYK!MFCpS[K!?KL/MRTpyjfB.stB#{wBYv#MqgQB+B#pRTBYtRyM)BYK
 "!#!!%$'&#(#)*$+-,#./$ (-0213(4&65873.-9 of
/?#jKL/M4SDGN/jB#St{ffMB#StC@B#pTRTtSDC/MJjC/?7B#pSDGJI2KL/M4jMJs?#jK!MFCxjMFGNDRyKG)BYjM}KL/MBF#MJjBYv#MzYB#RTN/MFG
B#{ffj?7GGKL/M)D#M.CDBYKBYtB#G!MFG({ff?7StGpTC/MJjMFCL/MWjMFGNDRyKG?#Pi?7N/jMffsMJjp4MFS[KGP?#jG!EHS[KL/MJKp{WCtBYKBHItpM#
WRTBYjIHSDGN/jB#St{ffMWB#SDCWB#pTRytStC/MJjFIuBYjM.CDpTG!stRBFE#MFCpTSz'BYtRTMFGI/B#StC	I/jMFG!sMF{ffKpy#MFRyE#IHA*L/MJjM@AM
B#RTG!?GL/?+AKL/M*WMFNYB#RTN/MFG9P?#jKL/M*K!jN/Ml;g :
<9o9B#SDC4KL/MMFsDKE	7g $6<oiS/MJKA?#jwHGWgA@pyKL4stBYjB#MJK!MJjG
jMff=K!jB#pS/MFCPj?7KL/M}{ff?#jjMFG!s?7SDCDpTSDvCtBYKBYtB#G!cM FofI'A@LDpT{L}BFEG!MJj#MzB#GlBwHpTSDC?#PG{JB#RyM#zL/M
jMFGNtRyKG?#DKB#pTS/MFC	P?#jjMFB#RCDBYKBBYjMCDpTG!stRBFE#MFCpTSiBYtRyMl
WGWA;M{ff?7SDGpTCDMJj@KL/MJjMK!?MB{JRyMFBYjCDpMJjMFSD{ffMMJKAMJMFSOKLDM"jMFGNDRyKG*?#DKB#pTS/MFCP?#j@KL/MG!EHSH
KL/MJKpT{lB#SDCKL/xM 0C/?7B#pTSDGFIAMlGLtB#RTR2CDpTG{JNtGGKL/MFG!MJsBYjBYK!MFRyE#
 D?#j*KL/MlGESuKL/MJKpT{C/?7}B#pTSDG.gQiBYtRyMFG@ItB#SDC7of
 N/j*h**@=tB#G!MFCB#Ryv#?#jpyKLD?7N/K!sMJjP?#jGWKL/M**@=tB#G!MFC?7S/MA*pyKLjMFG!sMF{ffK)K!?
=
KLDMYB#RTN/M?#PHKL/MG{ff?#jpTS/v*PNDSt{ffKpy?7S.NDG!MFClK!?*v7NDpC/M9KL/M;GMFBYj{L2'A;MB#RyABFEHG ?#DKB#pS.MJK!K!MJj
jMFGNDRyKG?7SxKL/MD#M}CtBYKBYtB#G!MFGl{ff?7StGpTC/MJjMFC>*?#K!MqKLDBYK.AM}BYjM}NDGpTS/vB	Ry?#v7BYjpTKLDpT{
#MJjfGpy?7S?#PKL/MOG{ff?#jfpTS/vxPQNDSD{ffKpy?7SIG!?KLDBYKzKL/MOCDpy`MJjMFSt{ffMFG}BYjMND{fLv#jMFBYK!MJjpSB
SD?7SHRy?#v7BYjpyKLtpT{)G{JB#RyM#L/MFG!MjMFGNDRyKGGN/sDs?#jKKL/MlpTCDMFBKLDBYK**h;*)G)BYjMlBYRyMK!?
SDC}S/MJAB#SDCq`MJK!K!MJjRy?H{JB#RtB/pTBlA*pyKLDpSKL/M@G{ff?#jM%)G!MFBYj{fLzBYstsDj?7B#{LzP?#jRTMFBYjSDpTS/v
B+E#MFGpTB#SS/MJKA?#jwHG*pTSKLDpGKEs`Ml?#PiLDpTv7LDRyEqGK!jND{ffKN/jMFCCD?7B#pTSDGJ
 N/j4G!MFBYj{L4MJKL/?HCpTGB#RTG!?sDjMJPMJjBYtRyMzPj?7KL/Ms?7pTS[K"?#PWpyMJA?#P@KL/M	@B#}pTS/v
=
CtpTG!KB#SD{ffMFGJIA*LDpT{fLBYjM"B#RyABFEHG@{ff?7SDGpTC/MJjBYtRTEzRy?%A;MJjWKLDB#SKL/Ml?7S/MFG@?#DKB#pTSDMFC	ENtGpTS/v
KLDMl*G!stB#{ffM#
?#jMJ?%#MJjFI/?7N/j;GMFBYj{Lz4MJKL/?HC}pTGv#MFS/MJjB#RTRTE"4?#jM*MJ~}{JpyMFSuKJ9pyK{JBYjjfpyMFG?7NDKPMJAMJjpyK!MJj!
=
BYKpT?7SDG4g?7SKL/M"D#M{JB#G!MFGfofIMJYB#RTNDBYK!MFGWPMJAMJjpTSDCDpTpTCtNDB#RTGg?7SP?7N/j){JB#G!MFGfofI{ff?74sN/K!MFG
2M

fi

f #%$ff&(' *)

 $,.$ !

fi

 t9

W-X <-Y8Z
< Y8Z
-



[3\

b[
UElKm a m]n

[3\

W-X <-Y8Z
<-Y8Z

UEl ["_o\

[3\

UEl [ m^l \
UEl ["_]_[

l [

cYedgf hjiRq

W-X <-Y8Z
<-Y8Z
U

cYedgf hji%sut

U

Yedgf h;i%sut

U

U

[ l

n a^a]\^a
n a][ l

l

m]m^m

% N%

[3\

n

[

\

K[

%%

n
_

_ ]\

># >k

@A>CBED

][


b[K[][

FGIHKJ

@LMON

n

[ m

\ ]a]\ m

\

\ n

_o\

n^n]n^n

n

[3a

l

\ n

]\

%%%% %

` ^a
` a^a

_

\

m^n

_ l^lKn [

` m _

pa6_

m

% %%

PRQSHKJTMrU

^\3_

./ /&%F/ NN NN / % %% /N/ % N // N %%

PRQSHKJTMVUl]n]n]n

m

p\

n



]a

% / % %%

[3\

n a][

 '

PRQSHKJTMVU 

_

n

Um]m^m a

Yedgf h;iRq





["_

U

Yedgf h;iRk






U

fi




N
/ N% /% /
N/ %/ NN NN %% /N ?% //
?

NNNN%% %
NN%%
%
N
%% N./ /
%% / /
U

c Yedgf hjiRk



>#

"$!$ $

l ]\]\

NN / %% / %F/ % //
n^n [ m

K\][

K[][ m

l

_ m

` m

l

` m _
` m [

m [

NN

K[^\
]a n

n

vCw6x8y{z}|~zKb-y{%j"z}Vyw6b-w6w6x8w"'zK

 Nb
W-X <-Y8Z
<-Y8Z
c{j h
 {; h

>C"

 p^ n _ 
 p^ Kn3l
 p^ n [ n

?

P

 a

[ l
[ m


[

^l



 n
[

 n
_

 DNpJ
[^a
l a

a

 
 \3_ ^
m n
 \  _Ka

>BuDT>I

@A>CBED

FGIHKJ

@LMON

 m^m]n

\ m^m \ l
_ ^l \]\

 `3
m l
 `n 

]n^
  [

Kn [ 

l]

   l] _Ka

n

vw6x2y{z~*"zobw6"zzK2y{%;"zT2bw"-z-w"w"V-w6w6x2w"zK"{ozo"""

jzo/zo*-SfizozK#R'w6'oA;bz-w6w6x2w"zKrj-zozow"'zKpp-'zKAjzo/zo*'w6o
j-zV2"zOow"zKpp-w"-b--*w"''zo}j;-/ow"'zKp%Oz'pw6"-zOwK"zobw6"z
-xfizo"6w6bw6x8y{zK#"y{"zKz'w6'oy{#y{"zKw6'zoj;row"'zKpp
 "z}/%w"2vCw6x2yz}p~


vzAzK-y
-
ow"zw6z"Cw"-oy2{"zRw6xfi
zRw"6w"#w6"zKC"zrV
x8w"'zKzo-{zKezK'}zVrEx2w"'zKzr'zob"
zezK"zKzKo~Axfi"

 Np
W2X Y8Z
 Y8Z
cef  d
 8f d

 [ m o
_ a6_ 
 [ m a  m^l
 l]n    n
 \ m _oa  \


\6_
_ l

?

P


 [
[ l

 

 n
 
p

 



 DNpJ


^

\]a

a 

 
  l  
_ [
 [ n a  m

BuDT
m n
_[ ]
_ p

@ABuD

FRGIH]J

@LMVN

[ l]m [  \
[^a Kn  \

 ` m^

aK["_
a  a

 `a 

\^\
n

vw6x2y{z}~*"zobw6"zzK2y{%;"z}Vw"y{2--zo%-w"w"V-w6w6x2w"'zKr"{ozo"""

[6_]_

fi{22#8

 Np
R PR
 PR
R PR
 PR
R PR
 PR
R PR
 PR

"

 DNbJ

CBED I

 

  ]
a [^a
  a][^a

 JTNpH^BED E H 
  NpJ
_
\  m
_
\  m
\
\

 l  \ 
 l  _ 

 m

 \3_  a
 \3_ ]

 l
p

 [^\ Km
 [^\][ 

]
 

 m

]n

 J
l  a

]n

[ l]lKm

 \

 QH]JN
 \  _

 [

]n  
D NpB
 BN  G  T
 
\  _ n
 [

\ n]m [
#BuJ 6
  M
[   
 m^m [][


R PR
 PR

 _^_   m
 _^_ ]n a

m^
a3_

m _
 n 

R PR
 PR

T ]
 l _  _
T ]l _  _
a

m
a

m

FJ BuNpJ
[  l
\ ]

 l 
 [3a

@ACBED

FRGIH]J

@LMON

   

 ` \

n ` \6_ 

 ` \

n ` \^a]\

  aK[

l [ l
l  n

 n]n3Kn

 `g_ l

m^]n a

 `\ 

 `n 
 `a l

 ]
 m
 n

[^a^a3_
[ n^m 

 `g_ 

 ` \]\
 ` 

lKm 

\  

] ^
a a 
 ] a m

 `a n
 ` \]\

 ` n
 ` m

 p
  _ 

l m
_oaK[ K
a n  _ l

 ` m]m
 `m \

[  
[^[ m

] l
p^

a n 
  \ m

 `g_ l
 ` lKm

^ ` m 
 ` lKn

 `[ l

vw6x2y{z}~*zK-y{%j"zO/%-w6w6x2w"zK

w"y""b{-zKw"bz}w"z'y{u/ow"'zK;Sfi2
-fiIxezo''zo-w"
fiIbzozow"'zKo8w"2fi
%xfizo''zozow"'z"
{zK'fizK'zzo{zK-"z*w"y{""b-o%z



-zK{zow"y{""b-

oy{zKw6by{

{-w6{

'fizoj"bOz"-zo{zKezK'

w"

yw6K~
"*-z-"z

zo{zK-zKw"bzK--zozK


 

w'zK-

*

'zop{zK"}zfizobzKo/zw"

'2w"zzK

''zK'z

xfizK-wK4{""}z

zKw6bb



z

2'zKx22w6{w'zKw6pbzKb-p"zfi]/zo-y

-w"wV2yzR"zozK}zKw6bb
Av-z*zKp'*'zKy{zK'zK
	fiffEOy{]"zo]2/-3w6zobo
"p
-b'p{zK'zKow6fiz;

wy{ow"yw3-

x'zKy{zK-w'y-w6O-w"yy{

zKzKw"zKC-z6w"yz"-z*"b--{3zK-w6'z*zu'zKy{zK{"z/y{ow"yw3--'
4{'zK/-zo"zK#'zKxw"w"-wy'/"y{-/-w6w6bz;"x8-zK
#z'6uow"yy{zK

!
"$#&% ew"y{-;"-bw"ow"yzKw"--z*w6x2y'''"zKj"x22zK"fizobw6'"b"'y-{-o6w"-
-'z
' zK#y{"#y{-R-p-wK"zV"RxfizozK4{'zK-bzo{2y{wKw"y'xfizKz;"x8-zK8p

z2wK"z2y{zKzK#'zKu/}2y{z"zop{-A"ew6x2'zKw6pb
~)(+*!,
-fiIw"--(.*,fi
R"-b

z42y{"z-z*
w"2 
'2w"zKKCzKezK"zKy{"C-bzbw"z"fizobw6'"bw"}zKz
'fizK{"z"zozK"zob{2o
zw6x2

vz2w6bw"zo'zob-'zKxzK'zw"y""b{-w6zzy{zK-"0/121"

y'Vw"--z-xezo3/465 
/ "'zobw6{-rz'-{zK'''"z'zKw6bp

z4ezopzK#o6zK'z%3w"yzK-wK"zxfizozK4zKw"jyy{^o~/121798w"2:/;465

-4zKo 



/<7=8>83?]p@8xfizKrz

-}xfizo*"
3w6bw6x2y{zKzOw"Iv-zr"b}--{w"-zr2{w"y"bw68w6zVzOw"z
w"-bzo{2*z4fizobzK#ow"/zKyyfiw"zyyzK'zKezob;"bw"-zzKw"zKK{zzzo
{
~w"rz#-xfizo"R{'zopw6{-V^

-zKj'zo!7A/465 
/ p8/z2'zz'zobw6{rzoz

[6_oa

fiB CD-FEHGJI*LKebSHEME-ONQPCSR

zxfizK'O"bw62%w"j--
vw6x2y{zK3]#fiHw"-

NSCQT2QNSTUCOFEMNSD-p-HV
WYX[ZA\O

2/'zoO-''zKw"Iv-zbzK-y{V"/zK'zz4ezopzK#Vw6bz-'2ywK"zK

o4

 Nb

"

P




^H_`W-X4 Y8Z
^H_`  Y8Z

^ n 
^] l

[^\
l 

^H_`W-X4 Y8Z
^H_`  Y8Z

 l 6
[ _o\ 
 l [6_o\ 

[^\

^H_`W-X4 Y8Z
^H_`  Y8Z

  n K
a [  
  n K
a [][ 

[ l
lKn




^






n

_ Kn
T n]n]n^n

PRQSHKJTM



n

l

\

n

[6_
n

 aK[

vw6x2y{z:]#~*zK-y;"zryw6b

 Nb

^H_`W-X4 Y8Z
^H_`  Y8Z

 p^ n _ n
 p  _oa^a

"
[ l
[6_

P




 a

l

m3lKm \
a l  n

 n ^
[ [  m3
 \K[ lKn]m 

 `\ 
 ` l]m

 a]\
 m 

p aK[ m]m]n

 n [6_ 
 ^]
]

 n  m \^\
 _]_  \K[ 

 `\ 
 `l a

[  

 K
_ \K[3\3_ n
p a l]n \ l

 n 3
\ _ 
  n  [
]

 n^]n  \ l
 _]_  _ 3
m l

 ` \^\
 `\ n

_  l

 n

[

BuDT

l [ 

a]\ 

-w6w6x2w"zK-vCw6x2bazKw6bb

 DNpJ


 n
[

 a

@LMON

 ^
[^a
_ _ m _[6_

p K

 m
K
 n \ m \
PRQSHKJTM  l]n]n^n
 _^_ m3l _ m
n
[^a



[6_

FGIHKJ

 

 n]n^n

n


_






@ACBED

 DNpJ


PRQSHKJTM

l a
[  l


 
[ l a m _ 
 l^  ]l

BuDTI
 a  
l  [ l

@ABuD

FRGIH]J

@LMVN

_ l  l^l 
 ]l
_ n \ ^

 ` [][
[`  \

 a 
[  a

vw6x2y{zc~*"zobw6"zzK-y%;"rzT2bw"-zw"-vCw6x2da4zKw6bb

 Nb

^H_`W-X4 Y8Z
^H_` Y8Z

 [ m o
_ a3_ 
 [ m a n _ 

"
\3_
_ n



P


 [

 

 n
 


 l

 _



l

 DNpJ

 

CBED I

@ABuD

FRGIH]J

@LMON

\3_
 \ 

m  a 3
m l] \
_ l   ] [

 m^m  a
]  a][

 ` l^] ] a6_  _
 ` l]n  \K[ K _

[` n _
[` n _

 \ K
l n
 
[ l p

vw6x2y{zc~*"zobw6"zzK-y;"zrw"y{2-zow"-vw6x2Ma4zKw6bb
 "z'#zow"2oC w"yyRzow"'zKoAzzo- z"z--bw"-z-w6w6x2w"zKo
zbzK-y{A"x2w"zKxe(.*,
-fiIw"-
-fiIw6zzw"z"Rv-2zK-zKw"y}w6-fizKw6b
j"zO/-w6w6x2w"'zKo*zoz-y

u/-w6w6x2w"'zK-#zKe(+*!,
-fiI

-]"zzzKb-y{

"
-fiIAvzozoj"z"I-zvCw6x2a4zKw6bb-#zK-"'bx2'z{-2ow"y'2]4z
"zozKzKw6bb-zr*

'2w"zw6/y{zKw"*-zrzKy{zK'zK6w"yzK;"*z8w6bw"zo'zobf/g121

w"-h/;45 
/ pRv-*%#'bw"'{z-w6{z
zzK-y"x-w"-zKx
Vyw6b|6""

fiIA8{-zzzo-{

'2w"z"rzoz(.*,fi


-^"zK

"Au/O%-w6w6x2w"'zKjz'-w"ybzK-y{pw"-

jzoz}fiIezob;"b%xezo''zo-w"d(+*!,fiICp

w"2i
 (.*,fi
RI/z'yy-bzoc
 (+*!,


-fiI'xezO-bzo;zobw6x8y{zO'-(+*!,fiIz#zow"-ow"y{--%ow"'zj(+*!,fiI
{

zKezK'-z2w6b'xfizou/zozK0(+*!,
-fiI

fizoj"b%xfizo''zoz}-bbw"-zw"
  "Vz}/%-w6w6x2w"'zKK2zu/w"y{""b-fizo'
j"b byw6py{8~fizKw"pw"y""b{-Cxezo''zo-w"z/"zo*w"-o6w"-xe"w"y{""b-

[6_ m

fi{22#8



 Np

 DNbJ

BuDTI

 

 JTNpH^BED E H  NpJ

@)k  R
 C PR
@)k   PR

  ]
a [^a
  a][^a
\

\

a^\ m a

\

\

\^a n \

@)k  R
 C PR
@)k   PR

 l  \ 
 l  \ 

 m

 m

\ ] _ l

]n

]m

[^[ lKn _

@)k  R
 C PR
@)k   PR

 \3_  a
 \3_  \

 l

@)k  R
 C PR
@)k   PR

 [^\ ]
 [^\  m

 [

@ABuD

FRGIHKJ

@LMON

 [ l
  \


 [ K
 n^m
  a 3

m 

 `n 
 ` a6_

 `m \
 `m a

m]n a
] _o\

m a l _o[
a m _  [

 ` ]n
 ` _

m ` \
  ` 

\  \

 _  m^n

 \]\]\ l

 ` ["_
 ` _

m `_ n
 n `\ n

] ]
[ [
p \K[

   K
 n \
  m]3
]
n l

 ` 
 ` ]m

] `  [
 l ` n

Kn^m^l]l \
 l _  a n

n  
[ 3
 [ l]l

 ]
l n \ ^
 l
 lKm _ l

[#` a 
[#` l _

a]a 
 _ ^l

FJ BuNpJ
l 
[  ^
 a m a
a

]l 
  _

l ^l
\ ]
_ m]m 

 ` n^m
 ` m3l

 a` n3l
 \` ]n

 J

 l

 QH]JN
 ] \ 
 ]m
 a n]m a

 BN G  DTNpB
 l

_  l \ 

 a n
 l^
]

 

l \ l _ n
#BuJ 6 M



@)k  R
 C PR
@)k   PR

 _]_ n^n^
 _]_ n _ 

m^m
m^n

[ m3l
[ l]n

@)k  R
 C PR
@)k   PR

T ]
 l _  _
T ]l _  _
a

a
a

\ a 
]

vw6x2y{zo4~/zK-y%;"-z}O%/-w6w6x8w"'zK-bvCw6x8MazKw6bp

fizoj"b

z'-w"yy-zrzKw"-u/w"-K<(+*,'
2fi


(+*!,fiI

r{zK'fizK'b2--z"

*'zo-w6"zVzo{zK#%-w"

z-w]"zow6b{zKw-b 'zop{zK"zfizobzK' 2w6zy{zKw6b2w"y{""p{-

x2w"'zK*Or{"-zorw"y{""b-j"y{zKw6b2
%wK"zKw" zo*"4o/T-ow"'z"8z
2w6p'-y{#'zK-zK'zKw"bzz:'-w"yu"zy{zKw6pzKzo*"e w"--{'
zVEx2w"'zKy{ow"yw"2w6x2'zKw6bp-zo4{-y}--zozK
"/z-wK"zw"y'-'zKz%jyy{^
w"y{""b-o~
 l
H
 a2{'zKzow"y{
 "|pVw" w"y""b{-
x2w"'zK  --zoezK2zK-z'zK'o
z-'zK w"

-zofizK-zK-z%'zK'x2w"'zKzzKw"bz"e2-{{-w"y-w"y2j"bw6{2m2yy{x2w"8


"pe{wzK2zK-zyzo"zKy
z'#2w"y
'4n
vz:mco'zKw6bp


zoE/"fizoqp

zop;"]4{oAopI

}x2-w6{

{-z:%VzK

b"bj-2{sr)tpAur"'z-w6qr)tzozK-Vw""b-zob"z}6w6bw6x2yzKw"Vz2o
z-'zKw""bzob-2''zK#{-z'"ey"""z}"bzK'fi---zo*"o



V"zoCw"y{""b{-vwuC^*zoA/-''p-'"*sxfiyIlfip^2w6I-'zKI2zofizK-zK-z'zK'*EzK
zorw"y{O]QfizKU/zKyyzp|{C

p

vzru/-zofizK-zK2zEx2w"'zKw"y{""p{-o4lw"-}xHyl"fizobw6'zV-zr'2w"zV"Iz'-{6w"y{zK-z
oyw"zKozobzKw"~r<tz42y{"bzKAz'2w"z"e2bw6z2w6{x2y{zr{w{"zK"b-zob
z-wK"z2oy-zK-z*w"y{""b-r)tz%2w6p'
6--rwV"zK"bzob-rw"-z%w"z


"b--{
zo

ow"

w"z*
'fizoj"b

w"-rEx2w"'zK

-zzK-y"x-w"zK

[^a n

'zKw6pbzo4-oe"b-zo''zK'rrzozo
{

w"zj"bzK

w"y""b{-

vz

fiB CD-FEHGJI*LKebSHEME-ONQPCSR

zK2y{j"}zw"y""b{-lw"-r)t

NSCQT2QNSTUCOFEMNSD-p-HV
WYX[ZA\O

-wK"zxfizozK

"x-w"-zK--]r

8y{zKzK#w6{-

j-bw6zw"y'-oy2zK zjy{4{bw'"ju%w6z^p  "cxfiyIlfi/z2'zK-z";/w6z8w"6w6"z
wK6w"yw6x8y{zw6wSQY;LQYsH@OU)U@Sv@UL+Lw I
v-z'zK'w"2-oy-zK}zK'z/zfizobzKCw6z%ryw6p^ --bw"-z"6w"-Vw"y{8-zoKI 
w"--{}'-zw%VzK6w"yzKo"-z%-xfizo"8zK"zKAzy{zKw6b-zK-zou/"w"-zrw"
-w"-zKo/z}-wK"zyy{zK'zK*w"--{-w"yeezob;"bw"-zzKw"bzKo~

j@$

vz6w"yz"*z
/

2/w]"zKw"

Tj"bw6

%b{'zob{e"pj-2{

 4
a b#/w6b"

O]Lj"r-zy{zKw6b-zKzou/"ev-3w"yz}zKw"-zK-z:'-w"y{"A-zzo*"2
yF"zKy4w"-wfizK-w"yu'zobwu"'z-w6A*

w34-

w"y'"zEz'-{6w"y{zKVw"-

-zKfiw6x2y{z"

d3$

vz~m}-yy{x2w"{zK{x2y{zoI2'w"-zO EzK#'"*2m-yyx2w"e"xfizou/zozKz-b"x2w6x2yS
u-'b{x2
8w"'4ow6'zK'z-w6w6x2w"'zjz}zK8{bow"yfijz'zK--''p{x2{e
w"2z-"x2w6x8y{-''bx2{w"b'ow6'zK
2zKw"bz
w"Oow"yo-yw6'zK

'zy{zKw6b-zK

-zou/"8*ju"z-w6

w"-w"yy-z

w"zw"z y{"-"x2w6x8y{ "z -w6w4
z-wK"z

wzKbzKw""'-yzKw6'bw"-'j"bw6{"Az:m}-yy{x2w"{zK{x2y{zo

2'w"-z"xfizKow"-'zO-*-z-w"%zfizK#w"yfi2yz4{w"--zV'pw"-'j"bw6{ow"xfiz
"zozozKy{e~Ow

2'zK

-zou/"0zKz-m{

Ozq'O-b"x2w6x2yu-''b{x2-{w"ow6'zK

2'w"-zow"

'w

xfizb{''zKzjyy{^%wKz%w"fio

.{w"p|%w"ob-oC3#p~

J 2YR)7?/+|

  >z>fi'


+H
L!
6.
 

e%>fi<?

L!

rzozYe O
 zK"'zKfa42w"-zK'b"#{zKezK'-z-''bx2{:
zo"
6w6bw6x2y{zK[
-zu/

w"-



;"R-zx

 >z>fi'RRzVzKw"bz"I--w"y-j"bw6xfizou/zozK

zo}"%6w6bw6x2y{zK
w"2i/>
pr}z-p'u/'zob}"%zz4-zKb{

w6xfi^"z}"zofizK--z"bw62d--'bw"-'j"bw6{2'ow"yo-yw6-y
z#
' 2w6{ p
a z#'zo-zow6 "-'bw"2';"pw6{"z
m}-yy{x2w"{zK{x2y{zor-'w"-zo~%z-{zoV-3w"yzK8zxfizo''zoV-zzo*"-Vz
2w6w4Vr]/zo"zoK
-VzKw"-z--yxfiz-w"--y{zK{ow"-{

-zw-dm
{ 6w"yz
wKw"y'--ow6'z^"zo-' w-zou/"
  w" zK"zKyyI-"x8w6x2y{-w]"zw-{
mc
{ 6w"yz^p

-z-b'zob

Vy{j"zw"y{""p{-}'z"w"y/'
zKw"yy

zzo'b-w6-yxez2'zK

m{xezKow"2'z*

"-{ozz-%w]"zKw"

"z")%VzK

'zo6w"y-w6'zzK*z2wK"zw"y'2-'zK/'

"/-zw"y{""b{2-zozK-z-zofizK--zK-z'zK'-''zKw"

w"-

"w"p

--{

v-zbzK-y{"VzK'zz4ezopzK#w6z-'8ywK"zKvw6x2y{z
fizoj"bw"-zzKw"zw"2zKw"b-w6w6x8w"'zb{''zK

"6

vzxfizK'3w"y-zj"zKw"p

xfiy
w"2z'zK-xfizK'}3w"y-z

{w"yoovzK'zzKb-y{--ow6'z-w6**zKw6bbzo4zr

'2w"z"4}x2-w6{

{zj/VzK"b--{I2Vfizo{{"z}r{zKezKV'"zow"y{""b{-o~*2y{z

(+*!,fiIw"y{""p{-"2b-'zKAw"zfi]/zoj2yfiw"2"z2-w6{-w"yy{'zK2{"z^C'zKw6bp
zKp'}z '2w"z""w"-I^'Vwy{zKzoz4'zKo"z*"z*;"pzKcr)r
t w"y{""b{2]fizoj"b
xfizo''zor2w" 
-fiI'zow"'zKorOx2'zo"z2w6xfi"M(+*!
, fiIw"-Mr)
t fizoj"b xfizo''zor-w"

-fiI 'zob"m{ ;-ow"'zKj2"z"
[^a 

fi{22#8

 Np

Y

 
PRQSHKJTM

R PR
 PR
P 
 k  R
 
 F  
 

;QL
 6@66
]] l
 \  [^\

] [ ]
 
]  _

Q
 v6v6

v$@vv6

m `  3
n  \
m `   n [6_

^ m \ 
 \]\ m 
 l  m _
 [  l 

a` n K
\ ["_ l
m ` ^ m  n

  LQ

PRQSHKJTM
R PR
 PR
P 
 k  R
 
 F  
 

OS

 l [ m^l \

 ! 

 \  [ m \
 l]l ]]
 l [3a n _

m ` ^l _ n 
m ` ^l \  

 l^l a 

v6Q

v$L   

 \  a ]
 
 l^l a n [
 l^lKm a l

 F  
 

P 
R R
 PR
P 
 k  R
 
 F  
 

P 
R R
 PR
P 
 k  R
 
 F  
 

QQ 

  n a^a]\^a

_"` a l [  l
m `  \3_Ka3_

  Q

v6O

 ] K
_ \]\ 
  n]m  \][
  n a l 
 p] n _ 
 p] Kn3l

6  6

 p m  n 
 p ["_  \

LU

T^ n^l  _

T ]
m l
[ [ ^
T lKn  _

6

gQ 

 l ] a
 l  \ n a

 lKm  K
l n _
 lKn  [][ n
 [ m a  [ m

v$  

 J H  N
l n^
a`  a ]
a`  m _ m]n

v  L

T [    [
T l a  

LQ

Q6@  

m `  _[  m
a`  _ n [
m `  a]a^aK[

T^ _ m  [
T n]m a  _
T n]m \][6_

  UO

 [ m a  m3l

  n^n]n^n
m `  _  m^

  QQ

66v66

l l 
_"`_ ]
_ [
a`   \ n \

 l a]a]\  a

S

 l p _ 









[^\
[6_

l 

 n

[^\



[^\
l [

[ 
[

 n ` l  a ]
]

 a

\3_

[

 n
_

n
[
a

 n

l



]n
 a



m

n

 

 n
 



]



 \

^

l

n



 
 _
 

l

 l
 
^

 

 


n



 n
[

 \

\K[

l
n

m


 

lKn

n






[ m

n



l

]l

_ n




n



[ l
 l

n

[

_

[ l
[ m



n
[

n



\3_
_ l

\



\
 
]

]n ` l  \K[
]n ` [^a l]n 

n

 n




L

 $O

n




p

[][

  ` \ lKm a 




n

[



n





[^\

 _

l




[ 

[6_
]



m

[6_

 a





 \

lKn




m



 \

  U

L6F66  

n

n


_



[6_
 a

[ l
l]







]

 _
[ 

 HKLQn  "NbJ

6  v6

P


 l]n]n]n

;QQ

PRQSHKJTM
P 
R R
 PR
P 
 k  R
 

"

 n]n]n




n

vw6x2y{z"6~zoj"bw"-zzKw"zK;"r-SfizozKyzKw6b-w"y{""b-

v-z;-

'zop{zK"z4fizobzK#w6''zK-'

b{2w-w6w"'zor-b
"bzo'

V-SfizozK#rj

zo3w"y2w6'zzxfizK-wK4{""zw"zw"y{"6

z'bw"-'zo-zK'y{zKw6pzzo*"8 

'R/z2wK"z2-'zK-zh%VzK
f/Vw"-m{

3w"y-zK"zzou/"'b-

zyzKw6bzK-w-w6w6x2w"'z"2{zK'fizK'w-SfizozK-w6w6x8w"'z"~j"zVyw6b

-w"


z'bw"-'zoO-zVyw6b|6""-w6w6x2w"z2'zK-bzo{2y{"8w"2z'zK''zoOj"bzK
z|6""z4row"zKrzVyw6b-w6w6x2w"'zfij"xfi" -pw"-z}w"-
z"Rz}2"z-w6w6x2w"zKr2w6r/z-w]"zxfizozK
-w6w6x2w"zKw"z'zK'zoo

x#

rw"y2-zoK4/z'zKyzK'zK

--w"z'bw"2'zoOw"-w"-"zor"A-zK'z

vzzKb-y{w6z^vCw6x8y{zo
[^a 



zow"

"x8'zo"z2w6zo

fiB CD-FEHGJI*LKebSHEME-ONQPCSR

NSCQT2QNSTUCOFEMNSD-p-HV
WYX[ZA\O

w6z}w"2w"y{""-'zzKb-y{"x-w"zKvw6x2y{z"68zoz-z}w"z}2w6w6x2w"'zKr*zoz-'zKj"
'bw"2w"2'zK'^*zo"zoK

-Oow"'z
2fi


w"y'fizo;"pVxezo''zo2w"i(.*,fi


r)t'zob"<m{R8vzozoj"z"-z"#4xfizK-w]{"r"rw"y{""p{-

w"-

ow"--"xfiz}w6''b{x2-'zK'

]"zob-'

PRQSHKJTM
 PR
 PR
P 
 k  R
 
 F  
 

P 
 R
 PR
P 
 k  R
 
 F  
 

P 
 R
R
P


P 
 k  R
 
 F  
 

Y

 

 Nb

 n]n]n

    
 vv6

;Q
 v ! 

vv66v

 Km3l 

 [  \ l

$UQ

 J H  N
T K
 p ]
 m _ l
[ [6_ 
T] n^n [
 p [ m^l^

U
v 6  

 K
 m 6
[ _
 \  ]
a \
]] n]m

m ` l ^
[ a^a
m ` l ^
[ \ l
a`  l]] _

] _ m 
 \^\  
 l]n^l 

L  U

  U

6v6 

Qv6

T [ n  a]\
T lKn3Km

m `   l _K\

a` [^[ l  a
_"`_ n  a 
a`   n \ 

  [  n ^
 
 p \  ^l

a` [^[^\3_ 

 HKLQn  "NbJ
 l ] \ l
QQ
 [ m a l a l
 l  m 

Q  v6

L6F6 
Kn ` [ m _  n
Kn ` l^l [ ]
 n
  ` \^a ]l \

;QQ 

 l 3
a _  n^
 l]n \]\^a n
 [ m a ^ a

 l a][ m  m

 ULQ

6  

 l p l \

Kn ` l _oa3_K\

vw6x2y{zo~*zoj"bw"2z}zKw"bzK;"z}y{zKw6b2w"y{""b{---w2SezobzK'zK'r'zo
'2w6bz
 -w"yy{"
*z-wK"zow6bb{zK w"-"zo'zop{zKO"%zfizobzKo
r-b w"

-fiI w"y""b{-
{ zw"y""b{-
-"fi'zK x -"zop so6"OopC-w6zKw6bbzK z
%*
j-b

'8w"z" -ow"z"*/z-wK"z
/zoz

u/"z

'zKy{zK'zK

--{{



-"zop#p

-w6w"'zo-'zKx#

fizobzK#w"yA--{-O'zzKb{xfizK

-zr-'z A
 "'zKw"-



T"bzo'w6-2Kw6'z

-"zopSgO/"8C*z2'zK

w-p{"z'-{6w"y{zK#rw"8y{zoz"A'zK

w"-

--#
ze%VzK

z
"p

w''b-bz-p{""g"-8rzoz

}-z-xfizo"jzoz2w6bw"zo'zobz"zo^"zoK/z-'zK-"zbw"-

"-z"p{-w"y-w6w6x2w"'zKK
zKw"b#w"--w6-2Kw6'zKy{]3
j"Vr-'z A
 "'zKw"-

w"-

-x2'zo

"z'"w"yA-w6w|66ow"zK

"";"2pvw6x2y{zK|-'8ywK4zwK"zobw6"z6w"yzKrw"Vz

-"z2w6w"'zo"%zzKyw6{"z2]"zKzK#"%zfizo'uow"'zb"z"x-w"-zK
fizo'uow"'z"z"%fiIA
w"O/zKyyw"Ozbw6{"zz'fizK#Ox#

fiI

x#


-fiI

'z

'-zz'fizKVx#


-fiIR
zw"y'-]
vw6x2y{zK|z}"zKe2-6w"yzK"x2w"zKx%-"zob2

-y{z}-w6w"zop%j"z}8w6b'xfizou/zozK2le-fiIw"y{""p{-w"-fiIR
zw]}"x2zo"z%-w6z*xfizK-w]""fi
-fiIw"-ele-fiI'zo-w6A-{ezozK#o~Cw"y

xfi"w"y{""b-w6z"bzzo{zK#-w" fiIRC{'zozK'-2w6:l82fi
 b-2Ojw"''zo}-w"

-fiIR


{bzK'fizK'

zfizK{"zKzKKxfi"


-fiI
[^a 

w"-le-fiI

"x-w"zw"y{zw"z

fi{22#8

 PR]NpJ
 NbQSHL]N

N 
 HTH  
 N  G  TN

 MqJ   `



 


 J 6 M



 C PR

  
  PR
 LMVN
 H L 
 `n [ 
 ` n]n3l

n ` n]n^n]n
n `n  l a

 MAJ   `

 
  PR
 LMON
 L
 HT

n ` n]n^n]n
 n `n  a 

 `a 

^NbJ

 NbQSHL]N

 ` _

vw6x2y{zK|~/2w6b'{%2"zobSg%*"2y{zo'zK*O

'y-{

w"fi


"zow"I
-fiI
fizoj"b/"b'z-w"

r-'z A
 "'zK-w"

z

'fizo;"p}fiIj
fiIR%T

-

2]"zKzK#pO]/zo"zoKz

zKyw6"z

z-"z-w6w"zo2zozKeVzozKw"jle-fiI

w"ow"'z"fiz-SfizozK-zKrw6zw"yyjzo-yxfiz}wbzK-y{"

-SfizozK-zKzzfizobzKw"yAzoeVw"-w-p"z'4''zKw6zfizobzKw6{
zK'zw"y{""b{-O*-yxez-zKzKw6"bzo}'zK'w6x2y"zKzobw"yR2oy-{2w6xe-zK{
2w6pw6{"zxfizK-wK4{"K

h-~Sf~	ff
fi
z-wK"z


zo"zKy{"fizKwzo

y{4ow"y'zKw6pb

%wK"zKw"zou/"

y{zKw6b2

-w6{zK"'zKw6bb
zK''p'zK

w"y{""b{-/r{-z

''b2zKj


"zvO'zKw6pb

-w6w6x8w"'zKov-zw"jzKw6z"/}w"y{""p{-

z'8w"z"VOo/x2-'zKwzoj"b

Oo%-w6w"yy]2'

w6--w"pj"

'zKw6bp

zo{zK#y{
w

'2w"z

"zo2zK'zK#w6{


yw6'

z'2w"z"

z'-{6w"y{zK-zoyw"'zK"/o
--{
'2w"z
w"#

ow"

 "z{2w6{  -pwzKfiw6x2y{z"p
-zK
2z'zoV"A"fizobw6'"bV-w6r-zo2z}-zzK{#xfi"b''b-bz}"RV'zKw6bp
xfiz"zKyow"yy

zK{#xe"p

/zw"#w"

zK''p'zK

w"-w6-fizK-z2w"z"OppEz"
*

x#

z2w6-w"yzo{zK-

z"A/zz2y"zw

*z

2w6*y{4ow"y"zKK

"zzK-2zK'zKw6pb

ow"

zo3w"y-w6'z

T-/w]"

-w6z'8w"z"/6fizobw"-
Iw6-zw"z
'2w"z"Ar{w

"zoyw"-2ow6fiz"r-bwK"-

'zVzKw6by{zKo{-zK"zO-{bzK{-ovzK'zVb-w6pw"'zob'o%w]zKy{'-zKzV'zKw6bp
-4zK'^/w6p-xfizo''zor-zou/"''b-bzKo
v-zzfizobzKw"yfizKb-y{%^

-w6zKw6bb

zo{zK#y{w"-w"oobw6'zKybzK]"zoO2y{z

zo4x2w"'zKbzK''b'zK*OVow"

%wK"zKw"

zou/"''b2zK%j

-w6w42w"2ow"

fizo'z%{'zo"zobw"y4'w6'z"--z%w6b%wK"zKbw"zo*"y{zKw6b-w"y{""b-o"w"y{-CzK
"{2{2ow"#y{-b]"zzKOOz4fizobzK#rJazKIw"O*zKyyw"O'z-2-'zK
x%2"zobso6"Oop4zozK'fi#-A-w6'zKw6bpw"y{""p{-x2w"zK*ow""x2w"
yy{}xezo''zoAbzK-y{o6bzK'fizK'xfi"zfizK{"zKzKw"-zo{zK-"-w"'zKw6bpzo4-
x2w"'zK

Oo
zK'fizKow"yyj"r--y{''b-bzK4zKyz"{ezKy-w6Oow"xfizw"y'p

fizojzKy{zo-zK'zK#'zKx#
-b

w6z2yz

wp

#w"

w"#



zxfizKy{zo"z-w6*Oow"w"y'xfiz2'zoj2yRw"-
6w6bw6x8y{zKw"-

2yow6'zK

zofizK-zK-z2w6''zob2pw"-

'2w6p'zjzo-zKzKw"-zofizK--zK-zrbzKyw6{-2{2pp
 "%zrzK'zKw6bp
4*zOw6zV2yw"-2'#'zo"bw6'zr-zr'zKp--'zK/zo"zKy{"fizK-R2w6fizo
{2"zfi]/zoj2y
'zKw6bbzo4-o--pw"z-zK--zozKx#}%yw"-zow"yso6"|p
zVw"fiRzo%w"y8so6"Oo"/-zV%w"fiw"2-zowso6"owp/r-2{{-w"yy"zVy{#R"
z
zK2y{%"x-w"zKxzo4x2-w6{vw6x2dazKw6bp
2{wKxez'zozK'
-"be"pw6'zVw""-zo"ezopw6'"K-p-yzK{zo%xfizVwoyw"bow"yfiw6bOzo"zobbw"yfi"%z-"

[^aK[

fiB CD-FEHGJI*LKebSHEME-ONQPCSR

'fizKo{2"fizobw6'"'

zK']

w6-2yow6{"w"y{""b-

NSCQT2QNSTUCOFEMNSD-p-HV
WYX[ZA\O

zw"y#'zK- '/"
 zw"-w6-w6{w"-

'zKw"yI-"x2y{zK/z-zKy"oyw"b{2ow6{


4u

8w6''zob-o

  fi<~MH
  wsvr

v-/"2w"%xezozKb-fi"'zKx#zca2w"-bh

-'zob{-z{zK-owvCzKoy{"

w"-z

w"-p-w--zo%R@'zK%vYo6"oO]6|^T^

-#wz%-2-w"zK/z%w"'yyw3{w:

w"-~*uOo^u"OoCzK'fizK{"zKy{"

"z
 

zw6z"bw6'zo-y'yy{zKw6z


#3w"y-w6x2y{zzKy{r{z}2y{zKzK#w6{"A'zo"zopw"yw"y{""b-o


zozw"4-zozo*zob;"2'zoj2y
zK#w"-b""zK'{-K


zobwj"-

zw6z}w"y'"bw6'zo-yI'z

!
f"
#

rx-bw"'IV{<*b]
 {<-2#"C{<p
''zK
Vo
zae{p

z%w"fio.{RRso6""pj{IzKw6b2p{O{ozK

87zff62;ff:9 (

#x-bzo4y{"""
Vo
.afi{fip

Uy{zo]C{R* "pVw"y{8-zoK~



ff$&%ff!'%H #)( %H " *"( '%H +" (-,.(

j";"bzKow"''zo"zobz//zKw6zoK

ff %#

%<;A! # =

%wK"zKw"

%0/21324#O]65]6

xfizKy{zoR-zou/"OxzKw"2O"*w

$&%ff ">"$# / ff>%Hff'13?30@3fi|6OA5|4K

z%w"fiofi{R.Aso6"]pr##x2bzo-y"";"Oy{zKw6b-xfizKy{zozou/"4o~

/zKzK2oB$&%ff&%HO C# ( %fi "*D( '%H "E(-,

V-zob
ae{w"2{w"
-{Hp

;BFGFH (AH#JI

;O%%H "%P(!,

/zK-yb
*p{3a4zop-o/}8{%-wK"zo"rf
-{'"b-''zK~

S$&%	Tw ( ffffVU #
# #fi
% ffoo3Q]65o""

-zou/"K

OffLKff %>(

zobyw"I O]"p%

yzK-z}oyw"zK;"rw"oy-{"bw68-o

\ Vff U

 # "

ff %  #

% # 0% /M14NKHo"|"A5o"o

-w6bw"'zob{Kw6"

	H  #&%  #  %

1Q4R6e6A53-6

w6"^z'-{6w3

/"fizoK  r p vzw"yw6b
ow"'zV'-{*-"x8w6x2y-;zozK2z'zKb--&'#zKR;"*xezKy{zo

% (!,
%0/ W

{p

>ffYXw ( FSff2%[Z

( % , ff!ff>%fiff ( %	;A! # =

 # "

$&%ff " "$# Q/ ff>%fiff # %

%-zo]{myyzoK#{4-zKyyafi{p mw"-w6KwK%w4mfi O]"pV-w6-"z-"x8w6x2y%zou/"4
r{---zK6w6bw6x8y{zKo \ ! # %fiff]7zffL&% # %0/M140?"Ho4K|A5o3"
{w6bbw^ w6w4Cso6"|p{zKw6b--}%w]"zKw" -zou/"}z'2w"z"
'b-zKVxzK'w6 "*-''bx2{w"y{""p{-o:$&%ff&
 %HO #C( %fi "*"( '%H "(!, $&%ff " "$#`_
/ff>%f	a % ff I% 1B3Mbfi6o A5o6o 4

%yw"-}{ Kw4I{p

/-6w6zoo}3}4 "|pH*zKy{zo-zou/"4-''b-2-z/--
2b-o{8y{z"7ff62ff9 ( ff %# %cZ (MI Ffiffc	H # ffd%Hff>1eNVfgNo8-&5
/-6w6zoo}
A "pihA2aff %# 2%

ff "# ff

, %fiffCj ( l k >% m8,  2( I


(%

% 2U #)(

zKp{-{y{zK"

%  ( # % , ff;ffd%Hffo
g

-zKo-V-{"zobb{u"r'zKpo

%-#z" 

% (!,
n$&%TY ( fffflU # 0% / 
$&%ff ">"# Q/ ffd%Hffo2oA564

2 4]pIvzo"zo2zKzKA".%wK"zKw"zo*"4o

Z ( % , ff;ffd%Hff ( %qp%Hff!!  # % Ca # r
% ;A # =

 # "

%-#z"  *
  3#pOezopw6{-V;"y{zKw6b--r{
$&%Hff ">"# /Qdff %HffKff % ffL14KA5oo"
[^a l

"bw62-ow"yAzKyo

>ff3	fiffdoLffd%>

*"( '%H 
" (-, ;A # =

 # "

fi{22#8

%-#z" 

$-XXQX


 "p

% O6 #C(

U2%



%

2z'zVy{'zobw6bzy{zKw6b---"x2w6x8y'/-zou/"/j

%i(

%rsP% ( j " ffVUA/Qff
2%tUiu Xv%0/ # fi% ffff! # 0% /M1Qb""A5o4o4

-w6w4

%zK- {z/zKyyE}{p

{I  * O]"pr w"y""b{- j"c%wK"zKw"xfizKy{zo-zou/"
2w6w4B$&%rTw ( 
 ffffVU # %0/ %i(!, ;O$2%+Uh	0;]w ?DNofi"|A564

'b-{j

4

v

-{
 /zKyy*{
 p
{
  V 
 p

{IzKw6b2i
 %wK"zKw" -zou/"
 j
-w6w4~ V
zo{zKrw6--w"px2w"'zKj"bw6{-zo""vCzKb
-bzoI{2V-{"zobu"AVy{xfizow4

%zK-

%-"zob-2fi

 "pr

%-"zob-%w "p{zKw6b-

jfff " , >zZ

>Sff

%-"zob-%wso6"Oop{zKw6b-

*D( '%H E" (-, \

%-"zob-^v

O # fi
% ff]7zff&% #

{"VzK"zoK3{Op

zo2Vw"2

 ( l k

%^eV{.p

z'#-3w"y{zK2zoyw"'zK"%wK"zKw"

 # "

-zou/"

( % , ff!ff>%Hff ( %{pE%Hff  # % Ca # 
% ;A # =

z'#-3w"y{zK2zoyw"'zK"%wK"zKw"

% ffL;!184-"A5

 # "

-zou/"

zo 

 # "

''b--zKo

$&%ff ">"$# / ff>%Hff
''b--zKo

rzK"zobw"
"4 "pQ{zKw6b-Y%wK"zKbw"-zou/"K~azKw6bp

I # %HL'a~T&FSff! %}(!,
L$&%w
T ff "$#J
$&%ff ">"$# / ff>%Hff:M%+Uh	  # %  #  % "oA58o

z4fizobzK#w"yzK-yo

( F ( %r;A! # =

% 

%0/}Kff

( % , ff!ff>%Hff ( %WpE%Hff  # % Ca # P
% ;A # =

>ffX " ffdoLff>%H>xZ

$&%Hff ">"# /Qffd%HffoHO]65
% (!,
$&%TY ( fffflU # 0% / y
K6|8
5 KO]#

"Az'#2{3w"yzK%w]"zKw"

'pw"-'j"bw6{-w"yb-w6bw"'zop{Kw6{

% (-,
G$&%PTw ( ffffVU # 0% / v

/"''b2zKo

. # , ><$&%ff&%fi #C( H%  "
>ff )

{C
8V "p-2Kw6--zo'z}-"x2w6x2y{u-''p{x2{2*r{zofizK4
% ( %r$&% ,>(  I  )# ( %9zff ( ' a|1v3df-A
8$-XQXQX2;2 % % O )# ( % 
o 5O#
] 

-zK-z'zozKo

/"fizoK"  {p rzob;"^{KO op
-zou/";b
-w6w4
O! # fiff ffL

\

Ow"
{Qp

Vb-KozKy

zKyj

#

%r;A! # =

 # "

% ]7

2 pe

x-bw"##zw"y{""b{-j"Az-''p-{"8ow"2w"y

% (!,
2w6b'z-w6w4O$&%Y
T  ( fffflU # % / 
$&%ff ">"# Q/ ffd%HffooA58

zw"fio{% p -zofizK-zK-
2zK'zK

%wK"zKbw"zo4j"Cz/--2{O"-"x2w6x8y'

'% # 0% /21v?68|6OA5|3

zou/"4o

30@3e4"&53

M



M

zff

Kff d% ( % # 0% /21vG36Ho4&5|4"6

t

r$&%

(

ff #  " A
; ! # =  #  "

$&%ff " "$# Q/ ff>%fiff'1

2 so6"OopVy
&%fi #C( H%  "8*"( '%H "8(!, ;8FFH (6H#JI ff

{  o
z b w"--zo{--w4 L{ w"zo" -^{Op
"2{Kw6{ ;"
 y{zKw6b- %wK"zKw"zo*"o
ff

e

( % , ff!ff>%fiff ( %p%fiff!!  # % Ca

zKyw6{2-{2Ow"-y{zKw6b-w"y{""b{2j"-y{

*D( &%fi 
" (!, X H FUff! `# I >ff %H  " 2%+U



z/w"fio@{@

. # , ffffd%>Z
>ff ~

-zow4 -@

n

zw"fioH{H{  zob w"-zo{2-w4 -fi {fip
*zow4 H so6"OopA{I4ow"y'zKw6pbzo4-
j"}y{zKw6p-}
 %wK"zKw"zou/"4}-bw4-{-zKzK{#xfi"b z2w"z"/2w6o

7ff62ff:9 (

z

ff %#

%w"fio{Rw{
zKw6bb

w"y{""p{-

%Z (2I FHSff!c	fi # ff>%Hff'140R24+NoCoA58o


e



-zow4 wso6"|pr {'zobw6'zK y{4ow"y
 zob w"2zo{-2w4 Y{3p
;"y{zKw6b2d%wK"zKbw" zou/"4{zK'w6x2w"zK-2{{-w"y

v$&%ff!'%H #)( %H " *D( &%fi " (!, $&%Hff ">"# Q/ ffd%~	a

-zofizK-zK-zV'zK'o

[^a]\

% ff 
I %

183Mb"fioo4&5o"|"

fiB CD-FEHGJI*LKebSHEME-ONQPCSR

z%w"fio{fi

w" zo"ff-8{.p

{


xw"y{#"-bw6{
~

Z (MI Ffi # %0/

n

zow4 fiA-bzKpp{IzKw6p-e%wK"zKbw"zou/"4

azKw6bb2u/

Q

z%w"fio<{R<{~p

NSCQT2QNSTUCOFEMNSD-p-HV
WYX[ZA\O

-{ezozK#'2w"zKo

\

>Gj~LffJ2%+U	

(!,



Vzo'z"   % O]"p Oz2'z"-zofizK-zK-zbzKyw6{-2{2j"
yzKw6b-8y{-zKxfizKyzo-zou/"K
ff! H # H "
 H "
ff " "$# ff w	 % ff %


P$&%

A5

" oo
z%w"fioO{O{Qp

'%

)( % Q*"( '% (!, $&%

I 13M4

/ >% a

"
zo w6--w"pj"Ry{zKw6b--xfizKy{zo-zo*"R2
$&%ff!'%H #)( %H " *D( &%fi "E(!, ;BFFfi (6HO#`I ffPKff %>( % # %0/M14Af"&5|O]#

Vzo'z"   2so6""pe

-zofizK-zK-zOb{'zopw4
z%w"fio<{R<{~p
-zou/"-b

Q

Vzo'z"   so6""p r
 --]4w6ow"-bw"y"b-zob};"
%wK"zKw"
"zKzo w"y{""b{2w"- 
 -yw6'zKw"2zKw"y
w ffff # %
>Sff

$&%T ( VU %0/ W(!,

X # / Q>x$-T \ pZ ( % , ff!ff>%Hff8|"|"|A5|34

z%w"eKz{Rz

#

{p*zow4 z

*so6"]p}a'4b2w"'y{4ow"yAw"-2''b{x8'zK

zKw6bbw"y{"6

g$&%TY ( fffflU # %/ %](-, >ff8$-$-$$&%Hff!'%H #)( %H " 	a I F (@%#  I
I%>m X8o (L" S #)( %HL'aZ (MI Ffi O #C( % 2%tU~TY (  #>"$#&%  # W[;&F #  " \ ( Uff " 

p{-j"y{zKw6b-rxfizKyzo2zou/"4o

( %W;UO&Ffi # oLff	a
A5

% ff

oO 8"K



zw"fio{{)pzow4 %so6"]pMa'p-w"'y{4ow"yR'zKw6bpw"y{""p{-j"yzKw6b4

}7zff2;ff9 (

xfizKy{zozo*"o~jazKw6bb2z'2w"z"%"bzobo

$&%Hff ">"# Q/ ffd%Hff>1Q403dffiooA5o"|

V"K{p

vw6bEz op

8y{zw"y{""p{-

ff %j#

%y;A # =

 # "

'-''p-w-'zKz4'zK-{"%w

8w6w"yy{V"b{zK#'zK"pw62
vCzKb
3zoI]b"#/"-{"z[a4''zKz{Cw6xfi"bw6'"""Vzo2w6zK#
"/2'zo3a4o{zK2z"2OY{
 bzK-w"
Uu{Hp

Vy-'K-oC "pw{IzKw6p-
%wK"zKw"

% (!,
$&%TY ( fffflU # 0% / y
o"oA
5 o"o

jfff " , >zZ

>Sff

zo*"{y{4ow"yI''b2z"

( % , ff!ff>%Hff ( %{pE%Hff  # % Ca # 
% ;A # =

 # "

$&%ff ">"$# / ff>%Hff

myy{zoK8so6""p/zK-c%wK"zKw"w6xfiRzou/"''p-z"n$&%TY ( fffflU #
>ff:	 #H ffff>%>Z ( % , ff;ffd%Hff ( %qpE%Hff  # %Ca # %;A! # =  #  " $&%ff " "$# /Qff>%fifffio6&5o4o4

 bzK-w"
u{p

(-,

t

z zoK
 pA{IzKw6b-%wK"zKw"
 bzK-w"
u{.urw"p-w"
ep{.p
w"{"z-w6w"'zoo~v-z "'2w6bzow"---w6'z w"y{""p{-
w

L

Z ( % , ff;ffd%Hff ( %qp%Hff!!  # % Ca # r
% ;A # =

VzK{"zo]fi{+p

rzK"zopw"
I "p

 # "

 # "

$&%Hff

zKw6by"2 6pz{zKw6b--Oow"2w"y-'zozK;b
% (-, ;;O;$ _ ?G@3H]]3|5]]6
$&%rTY ( fffflU # % / i

VzK{"zo]#{w6"4}{p

#
*D( '%H E" (-, $&%ff ">"$# / ff>%~	a % ff I% 1Bb"Ho"|4&5o3Q]#

>ffOX " ff>off>%>Z

( % , ff;ffd%Hff

zofizK-zK2z%;"pw6{


x$&%ff&%fi #C( %H

VzK{"zo]C{w6"}{pzKw6by  "|p}{IzKw6b-2yzow"2w"yR''b2zKo

[^a3_

. # , ffff>%H>
>ff )

"RzVbb2y{zo-'b{x2{

% (!,
$&%~TY ( fffflU # 0% / L
">"# /Qffd%Hffo"A
5 o6]#

w622yow6{'y{zKw6b--c%wK"zKw"-zou/"K

( %	pE%Hff!!  # H% Ca # 
% ;q # =

%

-zou/"''b--zj

% (-,
6
e$&%YT ( ffffVU # 0% / 
$&%ff ">"# Q/ ffd%Hffoo6A5o4K

p-w6bw"'zobw6{

%0/

"

fi{22#8

Oyy'2z"wae[r{qp

zobyw"


/so6"]p|-zobw6-

% (!,
w"4oy2{"bw62-4zKyo+$&%Tw ( ffffVU # 0
%/]
# %r;A! # =  #  " $&%ff ">"# /Qffd%Hffo6]&8
5 6]]#



w6"]

z'#-3w"y{zK2z

oyw"zK"

>Sff3	fiff>off>%ffff>%><Z ( % , ff!ff>%Hff ( %cp%fiff!!  # 
% Ca

:QKA	; *D( '%H E" (-, Z (2I FHS # %/21B36C6|5o6

Oy{^"zoK   pvw6x2'zKw6bp
8w6
rzK"zobw"I*% "p0/w]"zKw"

zo*"j"4^y{zK"z-^"zo"

*w6'zo;ga-w62{a4#
4r#-b-w"

ff&a}2%+Uu 

\ # % # 0% /"8w"}x-p"z"~~Tv

rzK"zobw"I8{eVzK{"zo]-{fip

+;eUMo@2%Hff

22op

A5

Tn  wKw"


%sP% ( j " ffVU6/ff]u

%q#

zKooO]6| |6

%-"zobH

\

O! # fi
% ff7ffL'% #

Z (2I FHSff! _  % Vff UrFH ( Q #>"# %  # 
 %fiffCj
zK-ow"yITj"bw6da4o{zK-zKo.aw"j"br2{"zob{"

rzob;"]4{oY 4]p

( l k



%


(%

%0/214@3CO]65o3|

% 2U6 #C( 2
% 
g

/"fizoKR   6pbm}w6 ~rzK#'"##ub"zK'4''zK
'b-{"A-"x2w6x2y'Vz4fizo'4''zKr;b-w6w6x2w"zKo
w ffff #

Z ( % , ff;ffd%Hff ( %qp%Hff!!  # % Ca # r
% ;A # =

8
z %w"fiof{R) "|p
Z M( I Ffiffc	H # ffd%Hff>1eNVfgNo6|58"

Vzo'z" -  {wp

"zK2'zK


{zKw6b--

-zKo

;"z4

% (-,
O$&%YT ( VU %0/ 

$&%ff ">"# Q/ ffd%Hffo235o

 # "

(o_

 "pf{zKw6b-
%wK"zKbw"zo*"o~*vz

x22w6{"^y{zK"zw"-'w6ow"yI-w6w4

rzob;"]4{o){)p

#% 

>ff	 #H >

7zff2ff~9 (

ow"-bw"yfiy{#'bzozKo

v;]%$&%2 ( ULU6 #C( %0 ( hA2aff %# 2%r9:ffCj ( l k

 UC "p

ff %h#

%

% /Ow{RzKbo

m46w4
v{.p%w"'zKy{e}so6"]p -b]"zK y{zKw6b--"~%wK"zKw"  zou/"K$&%TY ( fffflU #
(-, >ff:	fidff oL>ff %Hff>ff %H>y
 Z ( % , ff;d
ff %Hff ( %qp%Hff!!  # %Ca # %r;A # =  #  " $&%ff ">"# /Qffd%Hffoo"A5oO]6

%0/
%

m-w]EC{"2
{{I-}{ w"-y{zo"{pQ-zo"zoKm* 3#pdM{c~} w"b2z
yzKw6b-y{x-bw6 c
 $&%Tw ( ffffVU # %0/ %(!, >ffM	 #H >$&%ff&%HO #C( %fi " Z ( % , ff;ffd%Hff ( %
 (6(L"% j # >x
 ;A! # =  #  " $&%ff ">"$# />ff %Hff H]^|5]^|
m-yy{x2w"eae "p$&%
{w" 

,>(  I

 #)( 9
% zff

( ' a}2%tUh	  # %

 #  % *r^"zox2yow6{


{p%w"ob-o  I 3#p~{zKw6b-%wK"zKw"xezKy{zoIzou/"4o-Vw62-w"bx2w"'zK
-z:q{-b2o{2y{z"
HS  # H " ff ">"$# ff Hff
3fio" o"|

]Z (2I F

C( %

$&%

/ >% '13G@

A5

{w6bbw^ w6w4%C{/"Kw4w {Q-bw"zK-2#{w-w4*}{[p m-  fizoboV "p a'p-z
yzKw6b-%"S%
 wK"zKw"zo*"x#O"zKzow"y{""b{-o~Iezob;"bw"-zRw"-w"y{4fi"4'by
% ( %TOsff!' %;O%H " a %# % 2
8w6bw"zo'zobo$-XQXQX
U;2
 % %  )# ( % <
 %+U \ O! # %fi}
ff $&%ff ">"# /Qd
ff %H>ff 13b"
 A
4
o 5"
o 
{w6bbw^ w6w4R{)m- 'fizoboAV{~p

w4R}/ "pi{zKw6b-M/w]"zKw"

zo*"''b2zK

$-XQXX

xzKw6bb-;"zxfizK'"bzob"zK-zow"y{""b{-o



	a % ff

I 1 \ 2 %i2%+UWZEaff&%fiff #  % Q
1 4"3-O]65"|

S;2% % O #)(

w"-{w"
e{er2zob'
Hafi2}{ezobyw"
fie{.p|Ay-;"eV2v "p%wK"zKw"
w]"zobw6

Z (MII " %

w"-

#  #)(

%

4zKy'zKy{zK

%# J
% 	 O #&%  #  %L

;"bw6"^

zSff ( ' a

[^a]a

2%+U

\

z'#2{3w"yzK-z

ff>

% ( %
% Y

4zKy

oyw"'zK"w"4oy-"bw62-o

(U%
1 40R6fio3"|A5o"o64

fiB CD-FEHGJI*LKebSHEME-ONQPCSR



w"-{w"
e{.p

rw6;'zo"fi 3#p3-zKy'zKy{zKw"-

g-]}

"bw68-ow"ye-zKy%-Ooow"

 # O #C(

NSCQT2QNSTUCOFEMNSD-p-HV
WYX[ZA\O

%1bG?6K"|"A58K3

w"o-j"r4zKyC--zow"#u

*"( '%H " (!,

>ffO;

I

; %%>(2_

ff! # 2%M	  # %  #  %

zoz8VOr 
 "p
 w"2w"y-;zozK2zw"-ow"2w"yz42yw"2w6{ 
%
  2
x w""-24]ry{zK"z"
% (!, >ff}X " ffdoLffd%>Z ( % , ff!ff>%Hff ( %p%fiff!!  # %Ca # %;q # =  #  " $&%ff ">"$# /ff>%Hff
$&%cTw (  ffffVU # %0/ <

A5

| -o4



-'zK2w"
R{fp

%w]"zKw"
"zobo 

w"
R%so6""p~o{zK#b"zEx2w"'zKy{zKw6b2"rz'#-3w"y{zK2zoyw"zK"

B7zff2ff:9 (

zou/"4o

ff %#

 # "

$&%ff " "$# / ff>%Hff'1B3G?3G@3."A58o

{U{w";"zo"UmV{p{Izo4{'ovfi p){zKw6b--%wK"zKbw"zou/"4%;b

8y{zo'z}-w6wr{''4b-w"'zKw6bp

( %	pE%Hff!!  # H% Ca # <
% ;q # =

ffd%Hff

: pTY (
$&% , ff;ffd%Hffo[a4w"d

zKw6by

G
(-,

%r;A! # =

 # "

% (!,
$&%<TY ( fffflU # 0% / 
">"# /Qffd%HffoQ]6A
5 "

w"y""b{-K

$&%Hff

I %>m
 # "$#&%  # yKff %d( % # %0/ # %$&%ff ">"# Q
/ ffd%
	a % ff 
w6'zo~w"w"Mmw"w"-


. # , ffff>%>Z
>ff ~

% (!, T
9:ffCj ( l k 

-

( % , ff _

" L %#  " ff

$&%iTY ( fffflU # 0% /

zKw6by -{pAzopw4vOae 6pH~'-{6w"y{zK-z/w"-'4zKb"eow"-w"y44zKyo

>ff:	 #H >Z



-zow4 @

( % , ff;ffd%Hff ( %qpE%Hff  # % Ca # 
% ;A! # =

 # "

$&%ff " "$# Q/ ff>%fiffoo6|5ooO]#

%

\[ff  ( U (@%Y"(  " ff % a:U #&% 2 # ! # U (@% FL; " c ( % % 2U # ( %UffY;fflUff % Uffffff>%fi # 
aWU # %   Ij#  % a2w"-8pIgCzKKVzo2w6bzK"V/2'zojao{zK-z

so6"]p

ff %    #  %

w"2b{2ow"yfi 'zKyy{"zK-z"-V-{"zobb{u"RObw"-w"-w4

azox8w"'w"- 
 O]"p
{ zKw6p-%wK"zKw"-zou/" ; 
I
% (!, > ffz #  ffff>%>Z ( % , ff;ffd%Hff ( %zpE%Hff  # %Ca
2w6w6x2w"'zKox$&%yTw ( ffV
ff U # %0/ x
$&%Hff ">"# /Qffd%Hoff -&5O 

w"-

{dp

22y{zo'z

#

%y;A # =

 # "

zKw6byffC O"
] prvzzK^"zo"Row"-w"yfiy{E'bzozKj'w6'ow"y2w6w4/ 
{ u}Um}w"-w"yvaeS{Izo4{'o-  S{zKzo}22opvp%fiff!!  # %Ca # %;q # =  #  " $&%ff " "$# /Qff>%fiffL"
V''zop-w"~~r
u "4uryyw"-ISoooA5oo

zox8w"z"fi{.p

a4p%w6"2
 O]Lp*'w6zO-zK2{"Iw4zKy;O%%fi "n%(!, 	  # %  #  %
a4-
{qp Rw"y{'"bw4q O "|p V w"y{""b{2 j"-z -'b-{
/"
 'b-zK/j -w6w4$&%Tw ( ffffVU # %0/ %P(-, >ffO9 # %>WZ ( % , ff!ff>%Hff
;q # =  #  " $&%Hff ">"# /Qdff %Hffoo"A5o""
a4-
U{pRw"y'"w4fi


%d(

1B64&53

"%wK"zKw"

( %pE%Hff  # % Ca # %

C "p%%-''b-"%w]"zKw"-zou/"''b-bzK%;-w6w4~

$&%ff!'%H #)( %H " *"( '%H " (-, ;BFFfi (6HO#`I

x-b{zoIb"zow"2w"zo{zKw"y{""p{-

% # 0% /218324C""&58K|46

a2'zKo{2Oy{4K4V{pa4b-zKzKoe "|pZ<L % O C# (
u "'zKVd
r
a w6'o4
 6r
u zoR"e
 ~[
a -p"zowAzobyw6
a2'zKo{<p

zo 

zoz8RV* "p

{zKw6b-

%1TY;fflU

#  #)( b
% 2%tUj	fiff{zKz

%w]"zKw"zou/"4{

-zo'z3w6bw6x2y{zKOj

x$&%WTY ( fffflU # %/ %~(!, >Sff .~#  % 8$&%Hff!'%H #)( %H " Z ( % , ff!ff>%Hff ( %WsP% ( j " fflU6/Qffu
M%+Uu  \ # % # %/"o35o

2w6w4

[^a m

ffKff

#% 

( oLff&a

_

fi{22#8

a'zK8so6""p8O}z*2'z"-;"zKyzo'-zK}yzKw6b-c%wK"zKw"-zou/"KD$&%TY ( fffflU #
(-, >ff:	 # H ff>ff %>
 Z ( % , ff;d
ff %Hff ( %qpE%Hff  # %C a # %;A! # =  #  " $&%ff " "$# /Q>ff %fiff e"A
 5""
a4KU- "|p/-''b2{"/w]"zKw"
% (!, >O
2b-o{8y{z"$&%TY ( ffl
ff U # %/ P
ff 9 # %>W
 Z
/>ff %Hff H"o "A5oO6] |

zou/"4j

-w6w6x2w"zKx2w"'zKzc

( % , ff!ff>%Hff ( %pE%Hff  # % Ca # 
% ;q # =

%0/
%

A{

$&%ff " "$#`_

 # "

a4KU% "pi{IzKw6b2M%w]"zKw"xfizKy{zozou/"4x2w"'zK -z-- 
 zKp{-{
yzK" -b-o2y{z"~%r zo{zK#Ow"y{""b{-
-bz
Yp 'zKb--&'#z"L$&%TY ( 
 fffflU # %0/ %x(!,
>Sff z # !ffd
ff %>
 $&%ff& %fi C# ( %H " Z ( % , ff;d
ff %Hff ( % \ O # %.
ff 7zff& % # %0/-A
o 5Q3
] 4

E
% (!,
T  ( fffflU # 0% / 
Y

vw"I /so6""p
O]#

x2bw"-buw"-4Exfi--

>ff3	 #H ffff>%H>YZ

w"y{""p{-

j"

q{

y{zKw6p-

( % , ff!ff>%Hff ( %pE%Hff  # % Ca # }
% ;A! # =

%wK"zKbw"-zo*"4o<$&%
 #  " $&%ff " "$# /Q>ff %fiff 6
 |5

Azobw4Av{<p CzKw6py#-/ 6p w"-w"y-zou/"K~ea4zKw"ow"- z2zK{"zK-zKoTg
a-w"b#'zoKvae{zo{'oq{R umw"-w"yE]-  3{IzKzo2-oppE%Hff  # %Ca # %;A # =  #  "
$&%Hff ">"# /Qdff %H>ff 1f2V''zob2w"~)
u "b4uyyw"-
2A5]6


zob
Au}{p





Bh )# (2I

w6x8y{zKo

+{R{+{w"

-

U;2%

{w"b'ozK
Aae "|p
ff2 # k
 1Nd4fi"|O6
] 5"o


zKp{-{

%  #)(

%

%i(

{zp

Vpw62-ow"yVw"-

zKobb{"z

{IzK2+m+afi przo"y{-w]

y{zK"

zKyj"-"zK-z

2-w6{w"-4

L$-XQXX

-b2o{2y{zOj"O2w6w-"*-"x2w6x8y'34^y{zK"z"

%YTsff&%r;O%H " a %# % 2%+U

\

[ m]n

O! # %fiff$&%ff " "$# /Qff>%fiff'1436C6]^586]L

fiJournal of Artificial Intelligence Research 18 (2003) 263-313

Submitted 08/02; published 04/03

Exploiting Contextual Independence In Probabilistic Inference
David Poole

poole@cs.ubc.ca

Department of Computer Science,
University of British Columbia,
2366 Main Mall, Vancouver, B.C., Canada V6T 1Z4
http://www.cs.ubc.ca/spider/

Nevin Lianwen Zhang

lzhang@cs.ust.hk

Department of Computer Science,
Hong Kong University of Science and Technology, Hong Kong,
http://www.cs.ust.hk/lzhang/

Abstract
Bayesian belief networks have grown to prominence because they provide compact representations for many problems for which probabilistic inference is appropriate, and there are algorithms
to exploit this compactness. The next step is to allow compact representations of the conditional
probabilities of a variable given its parents. In this paper we present such a representation that
exploits contextual independence in terms of parent contexts; which variables act as parents may
depend on the value of other variables. The internal representation is in terms of contextual factors (confactors) that is simply a pair of a context and a table. The algorithm, contextual variable
elimination, is based on the standard variable elimination algorithm that eliminates the non-query
variables in turn, but when eliminating a variable, the tables that need to be multiplied can depend
on the context. This algorithm reduces to standard variable elimination when there is no contextual
independence structure to exploit. We show how this can be much more efficient than variable
elimination when there is structure to exploit. We explain why this new method can exploit more
structure than previous methods for structured belief network inference and an analogous algorithm
that uses trees.

1. Introduction
Probabilistic inference is important for many applications in diagnosis, perception, user modelling,
and anywhere there is uncertainty about the state of the world from observations. Unfortunately
general probabilistic inference is difficult both computationally and in terms of the number of probabilities that need to be specified. Belief (Bayesian) networks (Pearl, 1988) are a representation of
independence amongst random variables. They are of interest because the independence is useful
in many domains, they allow for compact representations for many practical problems, and there
are algorithms to exploit the compact representations. Note that even approximate inference is
computationally difficult in the worst case (Dagum and Luby, 1993).
Recently there has been work to extend belief networks by allowing more structured representations of the conditional probability of a variable given its parents (DAmbrosio, 1995). This has been
in terms of either causal independencies (Heckerman and Breese, 1994; Zhang and Poole, 1996), parametric forms such as sigmoidal Bayesian networks (Neal, 1992; Saul, Jaakkola and Jordan, 1996), or
by exploiting contextual independencies inherent in stating the conditional probabilities in terms of
rules (Poole, 1993) or trees (Smith, Holtzman and Matheson, 1993; Boutilier, Friedman, Goldszmidt
and Koller, 1996). In this paper we show how an algorithm that exploits conditional independence
2003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiPOOLE & ZHANG

for efficient inference in belief networks can be extended to also exploit contextual independence.
Poole (1997) provides an earlier, less efficient, version in terms of rules. Zhang and Poole (1999)
give an abstract mathematical analysis of how contextual independence can be exploited in inference.
Section 2 introduces belief networks and an algorithm, variable elimination (VE) (Zhang and
Poole, 1994) or Bucket Elimination for belief assessment (Dechter, 1996), for computing posterior
probabilities in belief that is based on nonlinear dynamic programming (Bertel and Brioschi, 1972).
Section 3 presents a representation for conditional probabilities that lets us state contextual independence in terms of confactors. Section 4 shows how the VE algorithm can be extended to exploit
the contextual independence in confactors. Section 5 shows how we can improve efficiency by
reducing the amount of splitting. Section 6 gives some empirical results on standard and random
networks. The details of the experiments are given in Appendix A. Section 7 gives comparisons to
other proposals for exploiting contextual independencies. Section 8 presents conclusions and future
work.

2. Background
In this section we present belief networks and an algorithm, variable elimination, to compute the
posterior probability of a set of query variables given some evidence.
2.1 Belief Networks
We treat random variables as primitive. We use upper case letters to denote random variables. The
domain of a random variable X, written dom(X), is a set of values. If X is a random variable and
v  dom(X), we write X=v to mean the proposition that X has value v. The function dom can be
extended to tuples of variables. We write tuples of variables in upper-case bold font. If X is a tuple
of variables, X1 , . . . , Xk , then dom(X) is the cross product of the domains of the variables. We
write X1 , . . . , Xk  = v1 , . . . , vk  as X1 = v1  . . .  Xk = vk . This is called an instantiation of X.
For this paper we assume there is a finite number of random variables, and that each domain is finite.
We start with a total ordering X1 , . . . , Xn of the random variables.
Definition 1 The parents of random variable Xi , written Xi , are a minimal1 set of the predecessors
of Xi in the total ordering such that the other predecessors of Xi are independent of Xi given Xi . That
is Xi  {X1 , . . . , Xi1 } such that P(Xi |Xi1 . . . X1 ) = P(Xi |Xi ).
A belief network (Pearl, 1988) is an acyclic directed graph, where the nodes are random variables2 . We use the terms node and random variable interchangeably. There is an arc from each
element of Xi into Xi . Associated with the belief network is a set of probabilities of the form
P(X|X ), the conditional probability of each variable given its parents (this includes the prior probabilities of those variables with no parents).
By the chain rule for conjunctions and the independence assumption:
n

P(X1 , . . . , Xn ) =
P(Xi |Xi1 . . . X1 )
i=1

1. If there is more than one minimal set, any minimal set can be chosen to be the parents. There is more than one minimal
set only when some of the predecessors are deterministic functions of others.
2. Some people like to say the nodes are labelled with random variables. In the definition of a graph, the set of nodes
can be any set, in particular, they can be a set of random variables. The set of arcs is a set of ordered pairs of random
variables.

264

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

Y

A

Z

B

C

D

E

A
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a

B
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b

C
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c

D
d
d
d
d
d
d
d
d
d
d
d
d
d
d
d
d

P(e|ABCD)
0.55
0.55
0.55
0.55
0.3
0.3
0.3
0.3
0.08
0.08
0.025
0.5
0.08
0.08
0.85
0.5

Figure 1: A simple belief network and a conditional probability table for E.

=

n


P(Xi |Xi )

(1)

i=1

This factorization of the joint probability distribution is often given as the formal definition of a
belief network.
Example 1 Consider the belief network of Figure 1. This represents a factorization of the joint
probability distribution:
P(A, B, C, D, E, Y , Z)
= P(E|ABCD)P(A|YZ)P(B|YZ)P(C|YZ)P(D|YZ)P(Y )P(Z)
If the variables are binary3 , the first term, P(E|ABCD), requires the probability of E for all 16 cases
of assignments of values to A, B, C, D. One such table is given in Figure 1.
2.2 Belief Network Inference
The task of probabilistic inference is to determine the posterior probability of a variable or variables
given some observations. In this section we outline a simple algorithm for belief net inference called
variable elimination (VE) (Zhang and Poole, 1994; Zhang and Poole, 1996) or bucket elimination
for belief assessment (BEBA) (Dechter, 1996), that is based on the ideas of nonlinear dynamic
programming (Bertel and Brioschi, 1972)4 and is closely related to SPI (Shachter, DAmbrosio and
3. In this and subsequent examples, we assume that variables are Boolean (i.e., with domain {true, false}). If X is a
variable, X=true is written as x and X=false is written as x, and similarly for other variables. The theory and the
implementations are not restricted to binary variables.
4. Bertel and Brioschi (1972) give essentially the same algorithm, but for the optimization problem of finding a minimization of sums. In VE, we use the algorithm for finding the sum of products. VE is named because of the links to

265

fiPOOLE & ZHANG

Del Favero, 1990). This is a query oriented algorithm that exploits the conditional independence
inherent in the network structure for efficient inference, similar to how clique tree propagation
exploits the structure (Lauritzen and Spiegelhalter, 1988; Jensen, Lauritzen and Olesen, 1990).
Suppose we observe variables E1 , . . . , Es have corresponding values o1 . . . os . We want to determine the posterior probability of variable X, the query variable, given evidence E1 =o1  . . .Es =os :
P(X|E1 =o1  . . .  Es =os ) =

P(X  E1 =o1  . . .  Es =os )
P(E1 =o1  . . .  Es =os )

The denominator, P(E1 =o1  . . .  Es =os ), is a normalizing factor:

P(X=v  E1 =o1  . . .  Es =os )
P(E1 =o1  . . .  Es =os ) =
vdom(X)

The problem of probabilistic inference can thus be reduced to the problem of computing the probability of conjunctions.
Let Y = {Y1 , . . . , Yk } be the non-query, non-observed variables (i.e., Y = {X1 , . . . , Xn }  {X} 
{E1 , . . . , Es }). To compute the marginal distribution, we sum out the Yi s:
P(X  E1 =o1  . . .  Es =os )



P(X1 , . . . , Xn ){E1 =o1 ...Es =os }
=
Yk

=


Yk

Y1



n


P(Xi |Xi ){E1 =o1 ...Es =os }

Y1 i=1

where the subscripted probabilities mean that the associated variables are assigned the corresponding
values.
Thus probabilistic inference reduces to the problem of summing out variables from a product
of functions. To solve this efficiently we use the distribution law that we learned in high school:
to compute a sum of products such as xy + xz efficiently, we distribute out the common factors
(which here is x) which results in x(y + z). This is the essence of the VE algorithm. We call the
elements multiplied together factors because of the use of the term in mathematics. Initially the
factors represent the conditional probability tables, but the intermediate factors are just functions on
variables that are created by adding and multiplying factors.
A factor on variables V1 , . . . , Vd is a representation of a function from dom(V1 )  . . .  dom(Vd )
into the real numbers.
Suppose that the Yi s are ordered according to some elimination ordering. We sum out the
variables one at at time.
To sum out a variable Yi from a product, we distribute all of the factors that dont involve Yi
out of the sum. Suppose f1 , . . . , fk are some functions of the variables that are multiplied together
(initially these are the conditional probabilities), then


f1 . . . fk = f1 . . . fm
fm+1 . . . fk
Yi

Yi

the algorithm of Bertel and Brioschi (1972); they refer to their basic algorithm as The elimination of variables one
by one, which is exactly what we do. Bertel and Brioschi (1972) also describe good elimination ordering heuristics
and refinements such as eliminating variables in blocks and forms of conditioning which we dont consider here.
The only difference between VE and BEBA is that BEBA requires an a priori elimination ordering (and exploits
the prior ordering for efficiency), whereas the VE allows for dynamic selection of which variable to eliminate next.

266

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

To compute P(X|E1 =o1  . . .  Es =os )
Let F be the factors obtained from the original conditional probabilities.
1. Replace each f  F that involves some Ei with f{E1 =o1 ,...,Es =os } .
2. While there is a factor involving a non-query variable
Select non-query variable Y to eliminate
Set F = eliminate(Y , F).
3. Return renormalize(F)
Procedure eliminate(Y, F):
Partition F into
{f1 , . . . , fm } that dont contain Y and
{fm+1
, . . . , fr } that do contain Y
Compute f = Y fm+1 t . . . t fr
Return {f1 , . . . , fm , f }
Procedure renormalize({f1 , . . . , fr }):
Compute f = f
1  t . . .  t fr
Compute c = X f
Return f /c

% c is normalizing constant
% divide each element of f by c

Figure 2: The tabular VE algorithm
where f1 . . . fm are those functions that dont involve Yi , and f
m+1 . . . fk are those that do involve Yi .
We explicitly construct a representation for the new function Yi fm+1 . . . fk , and continue summing
out the remaining variables. After all the Yi s have been summed out, the result is a function on X
that is proportional to Xs posterior distribution.
In the tabular implementation of the VE algorithm (Figure 2), a function of d discrete variables
V1 , . . . , Vd , is represented as a d-dimensional table (which can be implemented, for example, as a
d-dimensional array, as a tree of depth d, or, as in our implementation, as a 1-dimensional array based
on a lexicographic ordering on the variables). If f is such a table, let variables(f ) = {V1 , . . . , Vd }.
We sometimes write f as f [V1 , . . . , Vd ] to make the variables explicit. f is said to involve Vi if
Vi  variables(f ).
There are three primitive operations on tables: setting variables, forming the product of tables,
and summing a variable from a table.
Definition 2 Suppose C is a set of variables, c is an assignment C = v, and f is a factor on variables
X. Let Y = X  C, let Z = X  C, and let Z = v be the assignment of values to Z that assigns the
same values to elements of Z as c does. Define set(f , c) be the factor on Y given by:
set(f , c)(Y) = f (Y, Z=v ).
That is, set(f , c) is a function of Y, the variables of f that are not in c, that is like f , but with some
values already assigned. Note that, as a special case of this, if c doesnt involve any variable in f
then set(f , c) = f .
Example 2 Consider the factor f (A, B, C, D, E) defined by the table of Figure 1. Some examples
of the value of this function are f (a, b, c, d, e) = 0.55, and f (a, b, c, d, e) = 1  0.08 = 0.92.
set(f , a  b  e) is a function of C and D defined by the table:
267

fiPOOLE & ZHANG

C
c
c
c
c

D
d
d
d
d

value
0.08
0.08
0.025
0.5

Definition 3 The product of tables f1 and f2 , written f1 t f2 is a table on the union of the variables
in f1 and f2 (i.e., variables(f1 t f2 ) = variables(f1 )  variables(f2 )) defined by:
(f1 t f2 )(X, Y, Z) = f1 (X, Y)f2 (Y, Z)
where Y is variables(f1 )  variables(f2 ), X is variables(f1 )  variables(f2 ), and Z is variables(f2 ) 
variables(f1 ).
Note that t is associative and commutative.
To construct the product of tables, fm+1 t    t fk , we union all of the variables in fm+1 . . . fk , say
these are X1 , . . . , Xr . Then we construct an r-dimensional table so there is an entry in the table for
each combination v1 , . . . , vr where vi  dom(Xi ). The value for the entry corresponding to v1 , . . . , vr
is obtained by multiplying the values obtained from each fi applied to the projection of v1 , . . . , vr
onto the variables of fi .

Definition 4 The summing out of variable Y from table f , written Y f is the table with variables
Z = variables(f )  {Y } such that5


(
f )(Z) =
f (Z  Y =vi )
Y

vi dom(Y )

where dom(Y ) = {v1 , . . . , vs }.
Thus, to sum out Y , we reduce the dimensionality of the table by one (removing the Y dimension),
the values in the resulting table are obtained by adding the values of the table for each value of Y .
Example 3 Consider eliminating B from the factors of Example 1 (representing the belief network
of Figure 1), where all of the variables are Boolean. The factors that contain B, namely those factors
that represent P(E|ABCD) and P(B|YZ), are removed from the set of factors. We construct a factor
f1 (A, B, C, D, E, Y , Z) = P(E|A, B, C, D) t P(B|Y , Z), thus, for example,
f1 (a, b, c, d, e, y, z) = P(e|a  b  c  d)P(b|y  z)
f1 (a, b, c, d, e, y, z) = P(e|a  b  c  d)P(b|y  z)
f1 (a, b, c, d, e, y, z) = P(e|a  b  c  d)P(b|y  z)
f1 (a, b, c, d, e, y, z) = P(e|a  b  c  d)P(b|y  z)
and similarly for the other values of A . . . Z. We then need to sum out B from f1 , producing
f2 (A, C, D, E, Y , Z) where, for example,
f2 (a, c, d, e, y, z) = f1 (a, b, c, d, e, y, z) + f1 (a, b, c, d, e, y, z).
f2 is then added to the set of factors. Note that the construction of f1 is for exposition only; we dont
necessarily have to construct a table for it explicitly.
5. This may look like a circular definition, but the left side defines the summing tables, whereas on the right side we are
summing numbers.

268

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

3. Contextual Independence
In this section we give a formalization of contextual independence. This notion was first introduced
into the influence diagram literature (Smith et al., 1993). We base our definitions on the work of
Boutilier et al. (1996).
Definition 5 Given a set of variables C, a context on C is an assignment of one value to each variable
in C. Usually C is left implicit, and we simply talk about a context. We would say that C are the
variables of the context. Two contexts are incompatible if there exists a variable that is assigned
different values in the contexts; otherwise they are compatible. We write the empty context as true.
Definition 6 (Boutilier et al., 1996) Suppose X, Y, Z and C are sets of variables. X and Y are
contextually independent given Z and context C=c, where c  dom(C), if
P(X|Y=y1  Z=z1  C=c) = P(X|Y=y2  Z=z1  C=c)
for all y1 , y2  dom(Y) for all z1  dom(Z) such that P(Y=y1  Z=z1  C=c) > 0 and P(Y=y2 
Z=z1  C=c) > 0.
We also say that X is contextually independent of Y given Z and context C=c. Often we will
refer to the simpler case when the set of variables Z is empty; in this case we say that X and Y are
contextually independent given context C=c.
Example 4 Given the belief network and conditional probability table of Figure 1,
 E is contextually independent of {C, D, Y , Z} given context a  b.
 E is contextually independent of {C, D, Y , Z} given {B} and context a.
 E is not contextually independent of {C, D, Y , Z} given {A, B} and the empty context true.
 E is contextually independent of {B, D, Y , Z} given context a  c.
 E is contextually independent of {A, B, C, D, Y , Z} given B and context a  c  d.
3.1 Where Does Contextual Independence Arise?
Most of the examples in this paper are abstract as they are designed to show off the various features
of the algorithms or to show pathological cases. In this section we will give some examples to show
natural examples. We are not claiming that contextual independence is always present or able to
be exploited. Exploiting contextual independence should be seen as one of the tools to solve large
probabilistic reasoning tasks.
Example 5 When a child goes into an emergency ward the staff may want to determine if they are
a likely carrier of chicken pox (in order to keep them away from other children). If they havent
been exposed to chicken pox within the previous few weeks, they are unlikely to be a carrier. Thus
whether they are a carrier is independent of the other background conditions given they havent been
exposed. If they have been exposed, but have not had chicken pox before they are likely to be a
carrier. Thus whether they are a carrier is independent of the other background conditions given
they have been exposed and havent had chicken pox before. The other case can involve many other
variables (e.g., the severity and the age of the last time they had chicken pox) to determine how likely
the child is to be a carrier.
269

fiPOOLE & ZHANG

Example 6 Many engineered systems are designed to insulate something from other conditions. The
classic example is central air conditioning (heating and/or cooling in a house). The temperature inside
a house depends on the outside temperature if the air conditioning is off. If the air conditioning is on,
the temperature depends on the setting of the thermostat and not on the outside temperature. Thus the
inside temperature is contextually independent of the outside temperature given the air conditioning
is on and is contextually independent of the thermostat setting given the air conditioning is off.
Example 7 Consider a case where someone is to make a decision based on a questionnaire and the
questions asked depend on previous answers. In this case the decision6 is contextually independent
of the answers to the questions that are not asked given the context of the questions asked. For
example, consider a questionnaire to determine if a bank customer should get a loan that starts
asking the customer if they rent or own their current home. If they own, they are asked a number of
questions about the value of the house which are not asked if they rent. The probability that they get
a loan is contextually independent of the value of the home (and the other information that was not
available to the decision maker) given that the applicant rents their home.
Example 8 When learning a decision network from data, it is often advantageous to build a decision
tree for each variable given its parents (Friedman and Goldszmidt, 1996; Chickering, Heckerman
and Meek, 1997). These decision trees provide contextual independence (a variable is independent
of its predecessors given the context along a path to a leaf in the tree). The reason that this is a
good representation to learn is because there are fewer parameters and more fine control over adding
parameters; splitting a leaf adds many fewer parameters than adding a new parent (adding a new
variable to every context).
3.2 Parent Contexts and Contextual Belief Networks
We use the notion of contextual independence for a representation that looks like a belief network, but
with finer-grain independence that can be exploited for efficient inference in the contextual variable
elimination algorithm.
As in the definition of a belief network, lets assume that we have a total ordering of the variables,
X 1 , . . . , Xn .
Definition 7 Given variable Xi , we say that C=c, where C  {Xi1 . . . X1 } and c  dom(C), is a
parent context for Xi if Xi is contextually independent of the predecessors of Xi (namely {Xi1 . . . X1 })
given C=c.
What is the relationship to a belief network? In a belief network, the rows of a conditional probability
table for a variables form a set of parent contexts for the variable. However, there is often a much
smaller set of smaller parent contexts that covers all of the cases.
Example 9 Consider the belief network and conditional probability table of Figure 1. The predecessors of variable E are A, B, C, D, Y , Z. A set of minimal parent contexts for E is {{a, b}, {a, b},
{a, c}, {a, c, d, b}, {a, c, d, b}, {a, c, d}}. This is a mutually exclusive and exhaustive set of parent
contexts. The probability of E given values for its predecessors can be reduced to the probability of
6. To make this a probabilistic problem, and not a decision problem, consider that the probability is for a third party
to determine the probability distribution over the possible decisions. A similar analysis can be carried out to exploit
contextual independence for decisions (Poole, 1995). The decision makers decisions cant depend on information
she doesnt have.

270

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

A
A=true

Y
A=false

B

0.55

Y=true

C

Z
D

0.3 0.08

Y=false

0.77

0.27

0.17

P(b)
B

0.025

0.5
0.85

Z
Z=true

Z=false

0.29

Y

P(e)
0.79

0.59

P(d)
Figure 3: Tree-structured representations of the conditional probabilities for E, B, and D given their
parents. Left branches correspond to true and right branches to false. Thus, for example,
P(e|a  b) = 0.55, P(e|a  b) = 0.3, P(e|a  c  d  b) = 0.025 etc.

E given a parent context. For example:
P(e|a, b, c, d, y, z) = P(e|a, b)
P(e|a, b, c, d, y, z) = P(e|a, c)
.P(e|a, b, c, d, y, z) = P(e|a, c)
In the belief network, the parents of E are A, B, C, D. To specify the conditional probability of E
given its parents, the traditional tabular representation (as in Figure 1) require 24 = 16 numbers
instead of the 6 needed if we were to use the parent contexts above. Adding an extra variable as a
parent to E doubles the size of the tabular representation, but if it is only relevant in a single context
it may only increase the number of parent contexts by one.
We can often (but not always) represent contextual independence in terms of trees. The left side
of Figure 3 gives a tree-based representation for the conditional probability of E given its parents. In
this tree, internal nodes are labelled with parents of E in the belief network. The left child of a node
corresponds to the variable labelling the node being true, and the right child to the variable being
false. The leaves are labelled with the probability that E is true. For example P(e|a  b) = 0.3,
irrespectively of the value for C or D. In the tree-based representation the variable (E in this case)
271

fiPOOLE & ZHANG

is contextually independent of its predecessors given the context defined by a path through the tree.
The paths through the tree correspond to parent contexts.
Before showing how the structure of parent contexts can be exploited in inference, there are a
few properties to note:
 The elements of a mutually exclusive and exhaustive set of parent contexts are not always the
minimal parent contexts. For example, suppose we have a variable A with parents B and C,
all of which are Boolean. Suppose probability of a is p1 when both B and C are true and
probability p2 otherwise. One mutually exclusive and exhaustive set of parent contexts for
A is {b  c, b  c, b}. b  c is not minimal as c is also a parent context. Another mutually
exclusive and exhaustive set of parent contexts for this example is {b  c, b  c, c}. The set
of minimal parent contexts, {b  c, b, c}, isnt a mutually exclusive and exhaustive set as the
elements are not pairwise incompatible.
One could imagine using arbitrary Boolean formulae in the contexts. This was not done as it
would entail using theorem proving (or a more sophisticated subsumption algorithm) during
inference. We doubt that this would be worth the extra overhead for the limited savings.
 A compact decision tree representation of conditional probability tables (Boutilier et al., 1996)
always corresponds to a compact set of parent contexts (one context for each path through the
tree). However, a mutually exclusive and exhaustive set of parent contexts cannot always be
directly represented as a decision tree (as there isnt always a single variable to split on). For example, the mutually exclusive and exhaustive set of contexts {{a, b}, {a, c}, {b, c}, {a, b, c}, {a, b, c}}
doesnt directly translate into a decision tree. More importantly, the operations we perform
dont necessarily preserve the tree structure. Section 4.12 shows how we can do much better
than an analogous tree-based formulation of our inference algorithm.
Definition 8 A contextual belief network is an acyclic directed graph where the nodes are random
variables. Associated with each node Xi is a mutually exclusive and exhaustive set of parent contexts,
i , and, for each   i , a probability distribution P(Xi | ) on Xi . Thus a contextual belief network
is like a belief network, but we only specify the probabilities for the parent contexts.
For each variable Xi and for each assignment Xi1 =vi1 , . . . , X1 =v1 of values to its preceding
v ...v1
variables, there is a compatible parent context Xi1
. The probability of a complete context (an
i
assignment of a value to each variable) is given by:
P(X1 =v1 , . . . , Xn =vn )
n

P(Xi =vn |Xi1 =vi1 , . . . , X1 =v1 )
=
i=1

=

n


v

P(Xi =vi |Xi1
i

...v1

)

(2)

i=1

This looks like the definition of a belief network (equation (1)), but which variables act as the parents
depends on the values. The numbers required are the probability of each variable for each element
of the mutually exclusive and exhaustive set of parent contexts. There can be many fewer of these
than the number of assignments to parents in a belief network. At one extreme, there are the same
number; at the other extreme there can be exponentially many more assignments of values to parents
than the number of elements of a mutually exclusive and exhaustive set of parent contexts.
272

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

3.3 Parent Skeletons
Although the definition of a contextual belief network specifies the contextual independence we
want, it doesnt give us a way to organize the parent contexts (in much the same way as a belief
network doesnt specify the representation of a conditional probability table). We use the concept
of a parent skeleton as a way to organize the parent contexts; we want to use the indexing provided
by tables while still allowing for the ability to express context-specific independence.
The notion of a parent context is more fine-grained than that of a parent (the set of parents
corresponds to many parent contexts). When there is no context-specific independence, we would
like to not have to consider the parent contexts explicitly, but consider just the parents. We will use a
parent skeleton to cover both parents and parent contexts as special cases, and to interpolate between
them, when the independence depends on some context as well as all values of some other variables.
Definition 9 A parent skeletal pair for variable X is a pair c, V  where c is a context on the
predecessors of X and V is a set of predecessors of X such that X is contextually independent of its
predecessors given V and context c. Note that
context is c  V = v. A parent skeleton for
 a parent

variable X is a set of parent skeletal pairs, { cj , Vj : 0 < j  k}, where the cj are mutually exclusive

and exhaustive (i.e., ci and cj are incompatible if i 
= j, and kj=1 cj  true).
Example 10 A parent skeleton for E from Example 9 is {a, {B}, a  c, {}, a  c  d, {B},
a  c  d, {} .
Parent skeletons form the basis of a representation for contextual belief networks.
For each


variable, X, you select a parent skeleton such that for each parent skeleton pair cj , Vj in the parent
context, cj  Vj = vj is a parent context for X. For each such parent context pair we specify a
probability distribution P(X|cj  Vj = vj ).
3.4 Contextual Factors
Whereas the VE algorithm uses tables both as a representation for conditional probabilities and for
the intermediate representations, the contextual variable elimination algorithm defined below uses
a hybrid of tables and rules (Poole, 1997) that we call contextual factors or confactors. Confactors
cover both tables and rules as special cases.
Definition 10 A contextual factor or confactor is a pair of the form:
c, t
where c is a context, say X1 =vk  . . .  Xk =vk , and t is a table that represents a function on variables
Xk+1 , . . . , Xm , where {X1 , . . . , Xk } and {Xk+1 , . . . , Xm } are disjoint sets of variables. c is called the
body of the confactor and t is called the table of the confactor.
A confactor represents a partial function (Zhang and Poole, 1999) from the union of the variables.
The function only has a value when the context is true, and the value of the function is obtained by
looking up the value in the table.
Just as tables can be used to represent conditional probabilities, confactors can be used to represent
conditional probabilities when there is context-specific independence. In particular, a set of parent
contexts can be represented as a set of confactors with mutually exclusive and exhaustive bodies.
Given a parent skeleton for variable X we can construct a set of confactors for X as follows: for each
c, V  in the parent skeleton for X, we construct a confactor c, t({X}  V ) where t({X = x}  V =
v) = P(X = x|V = v  c).
273

fiPOOLE & ZHANG

Definition 11 A confactor is applicable on a context if the body of the confactor is compatible with
the context.
Definition 12 Given a confactor r = X1 =vk  . . .  Xk =vk , t[Xk+1 , . . . , Xm ] and a context c that
assigns at least the variables X1 . . . Xm , if r is applicable in c, the value of the context c with respect
to the confactor r is the value of t[Xk+1 = vk+1 , . . . , Xm = vm ] where vk+1 , . . . , vm are the values
assigned to Xk+1 , . . . , Xm in c.
Definition 13 A set R of confactors represents a conditional probability P(Xi |X1 . . . Xi1 ) if the
bodies of the confactors are mutually exclusive and exhaustive, and if P(Xi = vi |X1 = v1  . . . 
Xi1 = vi1 ) is equal to the value of the context X1 = v1  . . .  Xi1 = vi1  Xi = vi with respect
to the (unique) confactor in R that is applicable in that context.
Intuitively, the confactors that represent a contextual belief network are a way to organize the
parent contexts. The idea is to represent the parent contexts in tables when there is no context-specific
independence, and when some variables are independent of their predecessors in some context, then
that context can be made a body of the confactors.
Example 11 Consider the conditional probabilities represented in Figure 3. E is independent of its
predecessors given {B} and context a. This leads to the confactor:
a, t1 [B, E]

(3)

E is independent of its predecessors given context a  c. This leads to the confactor:
a  c, t2 [E]

(4)

E is independent of its predecessors given {B} and context a  c  D. This leads to the confactor:
a  c  d, t3 [B, E]

(5)

E is independent of its predecessors given context a  c  d. This leads to the confactor:
a  c  d, t4 [E]

(6)

The full multiset of confactors corresponding to the trees of Figure 3 are given in Figure 4. The
fifth and sixth confactors give the conditional probability for B, and the last two confactors give the
conditional probability for D.
We can now rewrite the definition of a contextual belief network in terms of confactors:
If every conditional probability is represented by a set of confactors, the probability of
a complete context, c is the product of the values of c with respect to the confactors
that are applicable in c. For each complete context and for each variable there is one
confactor containing that variable that is applicable in that context.

4. Contextual Variable Elimination
The general idea of contextual variable elimination (CVE) is to represent conditional probabilities
in terms of confactors, and use the VE algorithm with the confactor representation rather than with
tables. The units of manipulation are thus finer grained than the factors in VE or the members of
the buckets of BEBA; what is analogous to a factor or a member of a bucket consists of multisets of
confactors. Given a variable to eliminate, we can ignore (distribute out) all of the confactors that dont
274

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE




















P(E|A, B, C, D)

P(B|Y , Z)





ff








B
true
a, true
false
false

ff






ff






a  c  d,








B
true
y, true
false
false

Z
true
false
true
false

E
true
false
true
false

B
true
true
false
false

Value
0.55 fi
0.45
0.3
0.7

E
true
false
true
false

ff

E
Value fi
a  c, true 0.08
false 0.92

Value
E
Value fi
0.025 fi ff
a  c  d, true 0.5
0.975
0.85
false 0.5
0.15

Value
Value fi
0.77 fi ff B
y, true 0.27
0.17
0.23
false 0.73
0.83


D



fi ff true
ff D

Value

z, true
P(D|Y , Z)
z, true 0.29



false
0.71
false


false

Y
true
false
true
false

Value
0.79 fi
0.59
0.21
0.41

Figure 4: The confactors corresponding to the trees of Figure 3

275

fiPOOLE & ZHANG

involve this variable. Where there is some contextual independence that goes beyond conditional
independence of variables, the savings can be substantial. If there is no contextual independence, all
of the confactors have empty contexts, and this algorithm reduces to VE.
This section introduces an abstract nondeterministic version of CVE. Section 5 presents a more
concrete version where we explain how to resolve much of the nondeterminism.
The input to CVE is:
 a multiset of confactors that consists of the union of the confactors that represent the conditional
probability distribution of each variable given its predecessors
 a set of query variables
 an observation that is a conjunction of assignments of values to some of the variables
We first consider the case with no observations. Observations are considered in Section 4.7.
Initially and after the elimination of each variable, we maintain a multiset of confactors with the
following program invariant:
The probability of a context c on the non-eliminated variables can be obtained by multiplying the values of context c associated with confactors that are applicable in context c.
For each complete context on the non-eliminated variables and for each variable there
is at least one confactor containing that variable that is applicable in that context7 .
The algorithm will not sum out a variable in all contexts in one step. Rather it will sum out a variable
in different contexts separately. Intermediate to being fully summed out, a variable will be summed
out in some contexts and not in others. The remaining variables should be interpreted relative to
whether the variable has been summed out in context c.
Like VE, the abstract algorithm is made up of the primitive operations of summing out a variable
and multiplying confactors, and also includes a primitive operation of confactor splitting that enables
the other two operations. All of these operations locally preserve this program invariant. They are
described in the next subsections.
4.1 Multiplying Contextual Factors
If we have two confactors with the same context:
b, t1 
b, t2 
we can replace them with their product:
b, t1 t t2 .
7. This second part of the invariant may not be so intuitive, but is important. For example, in Example 11, one may be
tempted to reduce confactor (6) to a  c  d, 0.5 (i.e., where the table is a function of no variables) as the contribution
of the confactors is the same independent of the value of E (the table t4 [E] has value 0.5 for each value of E in
confactor (6)). The first part of the invariant isnt violated. However, if there were no other confactors containing
E that are applicable when a  c  d is true, after summing out E, we want the confactor a  c  d, 1, but before
summing out E we want the confactor a  c  d, 0.5 in order to maintain the first part of the invariant. We would
like to maintain the property that we only consider confactors containing E when eliminating E. The second part of
the invariant allows us to do this without treating this as a special case in our algorithm.

276

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

The program invariant is maintained, as any context incompatible with b isnt affected by this
operation. Any context that is compatible with b, the product of the values of t1 and t2 on that context
is the same as the value of t1 t t2 on that context. The completeness part of the invariant isnt
affected by multiplying.
4.2 Summing Out A Variable That Appears In The Table
Suppose we are eliminating Y , and have a confactor:
b, t
such that table t involves Y , and no other confactor that is compatible with b contains Y , we can
replace this confactor with

b,
t
Y

Note that after this operation Y is summed out in context b.
Correctness: To see why this is correct, consider a context c on the remaining variables (c doesnt
give a value for Y ). If c isnt compatible with b, it isnt affected by this operation. If it is compatible
with b, by elementary probability theory:

P(c) =
P(c  Y =vi )
i

By the program invariant, and because there are no other confactors containing Y that are compatible
with c, P(c  Y =vi ) = pi p, for some product p of contributions of confactors that dont involve
 Y.
Exactly the same confactors will be used for the different values of Y . Thus we have P(c) = p( i pi ),
and so we have maintained the first part of the program invariant. The second part of the program
invariant is trivially maintained.
4.3 Summing Out A Variable In The Body Of Confactors
Suppose we are eliminating Y , with domain {v1 , . . . , vk }, and have confactors:
b  Y =v1 , T1 
...
b  Y =vk , Tk 
such that there are no other confactors that contain Y whose context is compatible with b. We can
replace these confactors with the confactor:
b, T1 t . . . t Tk 
Where t is the additive analogue of t . That is, it follows definition 3, but using addition of the
values instead of multiplication.
Note that after this operation Y is summed out in context b.
Correctness: To see why this is correct, consider a context c on the remaining variables (c doesnt
give a value for Y ). If c isnt compatible with b, it isnt affected by this operation. If it is compatible
with b, by elementary probability theory:

P(c) =
P(c  Y =vi )
i

277

fiPOOLE & ZHANG

we can distribute out all of the other confactors from the product and thus the first part of the invariant
is maintained. Note that the t operation is equivalent to enlarging each table to include the union
of all of the variables in the tables, but not changing any of the values, and then pointwise adding
the values of the resulting tables. The second part is trivially maintained.
The second part of the program invariant implies that we cannot have a confactor of the form
b  Y =vi , pi  without a corresponding confactor for Y =vj , where i 
= j.
4.4 Confactor Splitting
In order to satisfy the prerequisites to be able to multiply confactors and sum out variables, sometimes
we need to split confactors.
If we have a confactor
b, t
we can replace it by the result of splitting it on a non-eliminated variable Y , with domain {v1 , . . . , vk }.
If Y doesnt appear in t, splitting t on T results in the set of confactors:
b  Y =v1 , t
...
b  Y =vk , t
If Y does appear in t, the result is the set of confactors:
b  Y =v1 , set(t, Y =v1 )
...
b  Y =vk , set(t, Y =vk )
where set was defined in Definition 2.
Correctness: The program invariant is maintained as one of the new confactors is used for any
complete context instead of the original confactor. They both give the same contribution.
Example 12 Splitting the first confactor for P(E|A, B, C, D) in Figure 4 on Y gives two confactors:
B
true
a  y, true
false
false

E
true
false
true
false

Value
0.55 fi
0.45
0.3
0.7

(7)

B
true
a  y, true
false
false

E
true
false
true
false

Value
0.55 fi
0.45
0.3
0.7

(8)

ff

ff

278

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

Example 13 Splitting the first confactor for P(B|Y , Z) in Figure 4 on A gives two confactors:
B
true
a  y, true
false
false

Z
true
false
true
false

Value
0.77 fi
0.17
0.23
0.83

(9)

B
true
a  y, true
false
false

Z
true
false
true
false

Value
0.77 fi
0.17
0.23
0.83

(10)

ff

ff

The reason that we may want to do these two splits is that now we can multiply confactors (7) and
(9).
4.5 Examples of Eliminating Variables
The four operations above are all that is needed to eliminate a variable. A variable is eliminated
when it is summed out of all contexts.
Example 14 When we eliminate B from the confactors of Figure 4, we only need to consider the
four confactors that contain B. The preconditions for summing out B or for multiplying are not
satisfied, so we need to split. If we split the first confactor for P(E|A, B, C, D) on Y (as in Example
12) and split the first confactor for P(B|Y , Z) on A (as in Example 13), we produce two confactors,
(7) and (9), that can be multiplied producing:
B
true
true
ff
true
a  y, true
false
false
false
false

E
true
true
false
false
true
true
false
false

Z
true
false
true
false
true
false
true
false

Value
0.4235
0.0935
0.3465 fi
0.0765
0.069
0.249
0.161
0.581

(11)

This is the only confactor that contains B and is applicable in the context a  y, so we can sum out
B from the table, producing the confactor:
ff

E
true
a  y, true
false
false

Z
true
false
true
false

Value
0.4925 fi
0.3425
0.5075
0.6575

(12)

The other nontrivial confactors produced when summing out B are:
ff
E
Value fi
a  y, true 0.3675
false 0.6325

(13)

279

fiPOOLE & ZHANG

E
true
a  c  d  y, true
false
false

Value
0.21475 fi
0.70975
0.78525
0.29025
ff
fi
E
Value
a  c  d  y, true 0.62725
false 0.37275
ff

Z
true
false
true
false

(14)

(15)

See Example 19 below for some trivial confactors produced and how to avoid them.
These confactors should be contrasted with the factor on A, C, D, E, Y , Z (of size 32) that is
produced by eliminating B in VE.
Example 15 Suppose that instead we were to eliminate D from the confactors of Figure 4. This
example differs from the previous example as D appear in the bodies as well as in the tables.
The two confactors for P(E|A, B, C, D) that contain D, namely a  c  d, t3 [B, E] (confactor
(5)), and a  c  d, t4 [E] (confactor (6)) are both compatible with both confactors for P(D|Y , Z).
So we cannot sum out the variable or multiply any confactors.
In order to be able to multiply confactors, we can split confactor (5) on Z producing:
a  c  d  z, t3 [B, E]

(16)

a  c  d  z, t3 [B, E]

(17)

The confactors for P(D|Y , Z) are z, t7 [D] and z, t8 [D, Y ]. We can split the first of these on A
producing
a  z, t7 [D]

(18)

a  z, t7 [D]

(19)

There are no other confactors containing D with context compatible with confactor (18). The
prerequisite required to sum out D in the context a  z is satisfied. This results in the confactor
a  z, 1 where 1 is the factor of no variables that has value 1. This can be removed as the product
of 1 doesnt change anything. Intuitively this can be justified because in the context when A is true
D has no children. We can detect this case to improve efficiency (see Section 4.10).
The confactor (19) can be split on C, producing
a  c  z, t7 [D]

(20)

a  c  z, t7 [D]

(21)

We can sum out D from confactor (20), producing a  c  z, 1, as in the previous case.
We can split confactor (21) on D producing:
a  c  d  z, 0.29

(22)

a  c  d  z, 0.71

(23)

where 0.29 and 0.71 are the corresponding values from t7 [D]. These are functions of no variables,
and so are just numbers.
We can now multiply confactor (22) and (16), producing:
a  c  d  z, 0.29t3 [B, E]

(24)

where 0.29t3 [B, E] is the table obtained by multiplying each element of t3 [B, E] by 0.29.
280

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

We can also split confactor (6) on Z, producing:
a  c  d  z, t4 [E]

(25)

a  c  d  z, t4 [E]

(26)

We can multiply confactors (23) and (25), producing:
a  c  d  z, 0.71t4 [E]

(27)

We now have only complementary confactors for D in the context a  c  z, namely confactors
(24) and (27) so we can sum-out D in this context resulting in
a  c  z, t9 [B, E]

(28)

where t9 [B, E] is 0.29t3 [B, E] t 0.71t4 [E]. In full form this is:
ff

B
true
a  c  z, true
false
false

E
true
false
true
false

Value
0.36225 fi
0.63775
0.6015
0.3985

(29)

The other confactor produced when summing out D is:
B
true
true
ff
true
a  c  z, true
false
false
false
false

E
true
true
false
false
true
true
false
false

Y
true
false
true
false
true
false
true
false

Value
0.12475
0.21975
0.87525 fi
0.78025
0.7765
0.7065
0.2235
0.2935

(30)

4.6 When to Split
Confactor splitting makes the multiset of confactors more complicated, so we have to be careful to
apply this operation judiciously. We need to carry out confactor splitting in order to make identical or
complementary contexts so we can carry out the operations of summing out a variable or multiplying
confactors. These are the only cases we need to split.
Definition 14 Given confactor r1 = c1 , T1  and context c, such that c1 and c are compatible, to
split r1 on c means to split r1 sequentially on each of the variables that are assigned in c that arent
assigned in c1 .
When we split r1 on c, we end up with a single confactor with a context that is compatible with
c; the contexts of all of the other confactors that are produced by the splitting are incompatible with
c. These confactors that are incompatible with c are called residual confactors.
More formally, we can recursively define residual(r1 , c), where r1 = c1 , t1  and c and c1 are
compatible, by:
 residual(r1 , c) = {} if c  c1
281

fiPOOLE & ZHANG

 Else if c 
 c1 , select a variable X that is assigned in c but not in c1 .
residual(r1 , c) = {c1  X=vi , set(t1 , X=vi ) : vi  dom(X)&vi 
= cX }
residual(c1  X=cX , set(t1 , X=cX ), c)
where cX is the value assigned to X in context c. Recall (Definition 2) that set(t, X=vi ) is t
if t doesnt involve X and is the selection of the X=vi values from the table, followed by the
projection onto the remaining variables, if t does involve X.
The results of splitting a confactor on a context is a set of confactors:
split(c1 , t1 , c) = residual(c1 , t1 , c)  {c1  c, t1 }.
Example 16 Consider residual(a  b, t1 [C, D], c  e). Suppose we split on C first, then on E.
This results in two residual confactors: a  b  c, t2 [D] and a  b  c  e, t3 [D]. Note that t2 [D]
is the projection of t1 [C, D] onto C=false and t3 [D] is the projection of t1 [C, D] onto C=true. The
non-residual confactor that we want from the split is a  b  c  e, t3 [D].
If instead we split on E then C, we get the residual confactors: a  b  e, t1 [C, D] and
a  b  c  e, t2 [D], with the same non-residual confactor.
Note that the result can depend on the order in which variables are selected (see below for some
useful splitting heuristics). The algorithms that use the split will be correct no matter which order the
variables are selected, however some orderings may result in more splitting in subsequent operations.
Example 16 highlights one heuristic that seems generally applicable. When we have to split
a confactor on variables that appear in its body and on variables in its table, its better to split on
variables in the table first, as these simplify the confactors that need to be subsequently split.
We can use the notion of a residual to split two rules that are compatible, and need to be multiplied.
Suppose we have confactors r1 = c1 , t1  and r2 = c2 , t2 , that both contain the variable being
eliminated and where c1 and c2 are compatible contexts. If we split r1 on c2 , and split r2 on c1 , we
end up with two confactors whose contexts are identical. Thus we have the prerequisite needed for
multiplying.
Example 17 Suppose we have confactors r1 = a  b  c, t1  and r2 = a  d, t2  that both contain
the variable being eliminated. We can split r1 on the body of r2 , namely a  d, producing the
confactors
a  b  c  d, t1 

(31)

a  b  c  d, t1 
Only the first of these is compatible with r2 . The second confactor is a residual confactor.
We can split r2 on the body of r1 , namely a  b  c, by first splitting r2 on B, then on C, producing
the confactors:
a  b  c  d, t2 
a  b  c  d, t2 

(32)

a  b  d, t2 
Only the second confactor (confactor (32))is compatible with r1 or any of the residual confactors
produced by splitting r1 . Confactors (31) and (32) have identical contexts and so can be multiplied.
282

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

Suppose we have confactors r1 = c1  Y =vi , t1  and r2 = c2  Y =vj , t2 , where c1 and c2
are compatible contexts, and vi 
= vj . If we split r1 on c2 , and split r2 on c1 , we end up with two
confactors whose contexts are identical except for the complementary values for Y . This is exactly
what we need for summing out Y .
If Y is binary with domain {vi , vj }, and there are confactors r1 = c1  Y =vi , t1  and r2 =
c2  Y =vj , t2 , where c1 and c2 are compatible contexts, and there is no other confactor that contains
Y that is compatible with c1 and c2 , summing out Y in the context c1  c2 results in the confactors:
residual(r1 , c2 )  residual(r2 , c1 )  {c1  c2 , t1 t t2 }.
If there are more than two values in the domain, we may need to split each pair of confactors, always
using the results of previous splits for subsequent splits.
Proposition 1 Splitting confactor c1 , t1  on c creates

(|dom(X)|  1)
Xvars(c)vars(c1 )

extra confactors, independently of the order in which the variables are selected to be split, where
vars(c) is the set of variables assigned in context c.
When we have to split, there is a choice as to which variable to split on first. While this choice
does not influence the number of confactors created for the single split, it can influence the number of
confactors created in total because of subsequent splitting. One heuristic was given above. Another
useful heuristic seems to be: given a confactor with multiple possible splits, look at all of the
confactors that need to be combined with this confactor to enable multiplication or addition, and
split on the variable that appears most. For those cases where the conditional probability forms a
tree structure, this will tend to split on the root of the tree first.
4.7 Evidence
As in VE, evidence simplifies the knowledge base. Suppose E1 =o1  . . .  Es =os is observed. There
are three steps in absorbing evidence:
 Remove any confactor whose context contains Ei =oi , where oi 
= oi .
 Remove any term Ei =oi in the context of a confactor.
 Replace each table t with set(t, E1 =o1  . . .  Es =os ) (as in the tabular VE algorithm).
Again note that incorporating evidence only simplifies the confactor base.
Once evidence has been incorporated into the confactor-base, the program invariant becomes:
The probability of the evidence conjoined with a context c on the non-eliminated, nonobserved variables is equal to the product of the probabilities of the confactors that
are applicable in context c. For each context c on the non-eliminated, non-observed
variables and for each variable X there is at least one confactor containing X that is
applicable in context c.
For probabilistic inference, where we will normalise at the end, we can remove any confactor that
doesnt involve any variable (i.e., with an empty context and single number as the table) as a result
of the second or third cases. That is, we remove any confactor that only has observed variables. We
then need to replace equal with proportional in the program invariant.
283

fiPOOLE & ZHANG

Example 18 Suppose d  z is observed given the confactors of Figure 4. The first two confactors for
P(E|A, B, C, D) dont involve D or Z and so are not affected by the observation. The third confactor
is removed as its body is incompatible with the observation. The fourth confactor is replaced by:
ff
E
Value fi
a  c, true 0.5
false 0.5
The first confactor for P(B|Y , Z) is replaced by
ff B
Value fi
y, true 0.17
false 0.83
The first confactor for P(D|Y , Z) is removed and the second is replaced by
ff
Y
Value fi
true, true 0.21
false 0.41
where true represents the empty context.
4.8 Extracting the Answer
Suppose we had a single query variable X. After setting the evidence variables, and eliminating the
remaining variables, we end up with confactors of the form:
{X=vi }, pi 
and of the form
{}, ti [X]
If e is the evidence the probability of X=vi  e is proportional to the product contributions of the
confactors with context X=vi and the selection for the X=vi value for the table. Thus


pi
ti [vi ].
P(X=vi  e) 
X=vi ,pi 

{},ti [X]

Then we have:
P(X=vi  e)
.
P(X=vi |e) = 
vj P(X=vj  e)
Notice that constants of proportionality of the evidence or by removing constants (confactors with
no variables) cancel in the division.
If we had multiple query variables (i.e., we wanted the marginal of the posterior), then we still
multiply the remaining confactors and renormalise.
4.9 The Abstract Contextual Variable Elimination Algorithm
The contextual variable elimination algorithm, is given in Figure 5. A more refined version that does
less splitting is given in Section 5.
The elimination procedure is called once for each non-query, non-observed variable. The order
in which the variables are selected is called the elimination ordering. This algorithm does not imply
284

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

To compute P(X|E1 =o1  . . .  Es =os )
Given multiset R of confactors
1. Incorporate evidence as in Section 4.7.
2. while there is a non-query variable to be eliminated
{
Select non-query variable Y to eliminate;
Call R := eliminate(Y , R);
}
3. Compute posterior probability for X as in Section 4.8
Procedure eliminate(Y, R):
partition R into:
R = those confactors in R that dont involve Y ;
R = {r  R : r involves Y };
while there is {b1 , T1 , b2 , T2 }  R where b1 and b2 are compatible,
{
remove b1 , T1  and b2 , T2  from R ;
split b1 , T1  on b2 putting residual confactors in R ;
split b2 , T2  on b1 , putting residual confactors in R ;
add b1  b2 , T1 t T2  to R ;
}
for every b, t  R such that Y appears in t
{
remove 
b, t from R ;
add b, Y t to R ;
}
while R is not empty
{
if {b  Y =v1 , T1 , . . . , b  Y =vk , Tk }  R
{
remove b  Y =v1 , T1 , . . . , b  Y =vk , Tk  from R ;
add b, T1 t . . . t Tk  to R ;
}
else if {b1  Y =vi , T1 , b2  Y =vj , T2 }  R where b1 and b2 are compatible and b1 
= b2
{
remove b1  Y =vi , T1  and b2  Y =vj , T2  from R ;
split b1  Y =vi , T1  on b2 , putting all created confactors in R ;
split b2  Y =vj , T2  on b1 , putting all created confactors in R ;
}
}
Return R .
t is defined in Section 2.2.
t is defined in Section 4.3.
All set operations are assumed to be on multisets.
Figure 5: Contextual Variable Elimination
285

fiPOOLE & ZHANG

that the elimination ordering has to be given a priori. The other choice points are the order in which
to do multiplication, and the splitting ordering.
Note that in the eliminate algorithm, all set operations are assumed to be on multisets. It is
possible, and not uncommon, to get multiple copies of the same confactor. One example where
this happens is when there is a naive Bayes model with variable C with no parents, and variables
Y1 , . . . , Yn each with only C as a parent. Often the conditional probabilities of some of the Yi s are
the same as they represent repeated identical sensors. If these identical sensors observe the same
value, then we will get identical confactors, none of which can be removed without affecting the
answer.
To see the correctness of the procedure, note that all of the local operations preserve the program
invariants; we still need to check that the algorithm halts. After the first while-loop of eliminate, the
contexts of the confactors in R are mutually exclusive and covering by the second part of the loop
invariant. For all complete contexts on the variables that remain after Y is eliminated, there is either
a compatible confactor with Y in the table, or there is a compatible confactor with Y = vi for every
value vi . The splitting of the second while loop of eliminate preserves the mutual exclusiveness of
the bodies of the confactors in R and when splitting a confactor, the set of created confactors covers
the same context as the original confactor. If there are confactors in R , and the if-condition does
not hold, then there must be a pair of confactors where the else-if condition holds. Thus, each time
through the second while-loop, the number of confactors in R increases or the number of confactors
in R increases and these are both bounded in size by the size of the corresponding factor. Thus
eliminate must stop, and when it does Y is eliminated in all contexts.
4.10 Ones
In a Bayesian network we can remove a non-observed, non-query node with no children without
changing the conditional probability of the query variable. This can be carried out
 recursively. In
VE, if we were to eliminate such variables, we create factors that are all ones (as X P(X|Y ) = 1).
In contextual VE, we can have a more subtle version when a variable may have no children in
some contexts, even if it has children in another context.
Example 19 Consider eliminating B as in Example 14 where the belief network is given in Figure
1 and the structured conditional probabilities are given in Figure 3). In the context a  c, the only
confactors that are applicable are those that define P(B|YZ). As stated, the contextual VE algorithm,
the following three confactors are created:
a  c  y, 1[Z]
a  c  y, 1
where 1[Z] is a function of Z that has value 1 everywhere and 1 is the function of no variables that
has value 1.
Confactors that have contribution 1 can be removed without affecting the correctness of the
algorithms (as long as these confactors arent the only confactors that contain a variable in some
context). It is easy to show that the first part of the program invariant is maintained as multiplying by
1 doesnt affect any number. The second part of the invariant is also maintained, as there are always
the confactors for the child (E in this case) that dont depend on the variable being eliminated, as
well as the confactors for the parents of the variable being eliminated.
286

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

...
S
...

...

FT

FB

OT

...

...

MB

MT

FH

MH

...

...

Figure 6: A fragment of a belief network: OT to eliminate.
It is however probably better to never construct such confactors rather than to construct them
and then throw them away. We show how this can be done in Section 5.1.
4.11 Multi-Valued Variables
We have presented an algorithm that allows for multi-valued variables, where the splitting operation
creates the same number of confactors as there are values in the domain of the variable being split.
There is an alternate method for using multi-valued variables. This is to extend the notion of a
context to allow for membership in a set of values. That is, a context could be a conjunction of terms
of the form X  S where S is a set of values. The original version of equality of the form X=v is the
same as X  {v}. Splitting can be done more efficiently as there is only one residual confactor for
each split. Effectively we treat a multiset of confactors as a unit.
There are examples where this representation can be much more efficient, but it makes the
algorithm much more complicated to explain. There are also examples where the binary splitting is
less efficient as it needs more splits to get the same result.
4.12 Why CVE Does More Than Representing Factors As Trees
It may seem that CVE is a confactor based representation for factors, in much the same way as the
trees in the structured policy iteration (Boutilier, Dearden and Goldszmidt, 1995) for solving MDPs.
In this section we present a detailed example that explains why CVE can be much more efficient
than a tree-based representation of the VE factors.
Example 20 Figure 6 shows a fragment of a belief network. This is an elaboration of Example 6,
where what affects the inside temperature depends on whether the air conditioning is broken or is
working. All the variables are Boolean. We use the following interpretation:
287

fiPOOLE & ZHANG

true

FB

FT

OT
p1

MB

false

p2

p3

MT

OT
p4

p5

P(fh)

p6

p7

p8

P(mh)

Figure 7: Tree-structured conditional probability tables for A and for B. Left branches correspond
to true and right branches to false. Thus p1 = P(a|d  e), p2 = P(a|d  e), etc.

FB Freds air conditioning is broken
FT Freds thermostat setting is high
OT Outside temperature is hot
FH Freds house is hot
MB Marys air conditioning is broken
MT Marys thermostat setting is high
MH Marys house is hot
S Season, that is true if it is Summer
The ancestors of FT , FB, S, MB, and MT are not shown. They can be multiply connected.
Similarly, the descendants of FH and MH are not shown. They can be multiply connected.
The outside temperature (OT ) is only relevant to Freds house being hot (FH) when Freds air
conditioner is broken (FB is true) in which case Freds thermostat setting (FT ) is not relevant. Freds
thermostat setting (FT ) is only relevant to Freds house being hot (FH) when Freds air conditioner
is working (FB is false), in which case the outside (OT ) is not relevant. And similarly for Marys
house. See Figure 7. What is important to note is that FH and MH are dependent, but only if both
air conditioners are broken, in which case the thermostat settings are irrelevant.
Suppose we were to sum out OT in VE. Once OT is eliminated, FH and MH become dependent. In VE and bucket elimination we form a factor f (FH, MH, FT , FB, MB, MT , S) containing
all the remaining variables. This factor represents P(FH, MH|FT , FB, MB, MT , S) (unless there is
a pathological case such as if MT or MB is a descendent of FH, or if FT or FB is a descendent of
MH). One could imagine a version of VE that builds a tree-based representation for this factor. We
show here how the confactor-based version is exploiting more structure than this.
288

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

If we are to take the contextual independence into account, we need to consider the dependence
between FH and MH when both FB and MB are true (in which case FT and MT are irrelevant). For
all of the other contexts, we can treat FH and MH as independent. The algorithm CVE does this
automatically.
The conditional probabilities of Figures 6 and 7 can be represented as the following confactors:
FH
true
fb, true
false
false

OT
true
false
true
false

Value
fi
p1
p2
1  p1
1  p2

(33)

FH
true
fb, true
false
false

FT
true
false
true
false

Value
fi
p3
p4
1  p3
1  p4

(34)

ff

ff

MH
true
mb, true
false
false

OT
true
false
true
false

Value
fi
p5
p6
1  p5
1  p6

(35)

MH
true
mb, true
false
false

MT
true
false
true
false

Value
fi
p7
p8
1  p7
1  p8

(36)

ff

ff

OT
true
true
false
false

S
true
false
true
false

Value
p9
p10
1  p9
1  p10

(37)

Eliminating OT from these confactors results in six confactors:
FH
true
true
ff
true
fb  mb, true
false
false
false
false

MH
true
true
mbalse
mbalse
true
true
false
false

S
true
false
true
false
true
false
true
false

Value
p9  (p5  p1 ) + (1  p9 )  (p6  p2 )
p10  (p5  p1 ) + (1  p10 )  (p6  p2 )
fi
p9  ((1  p5 )  p1 ) + (1  p9 )  ((1  p6 )  p2 )
p10  ((1  p5 )  p1 ) + (1  p10 )  ((1  p6 )  p2 )
p9  (p5  (1  p1 )) + (1  p9 )  (p6  (1  p2 ))
p10  (p5  (1  p1 )) + (1  p10 )  (p6  (1  p2 ))
p9  ((1  p5 )  (1  p1 )) + (1  p9 )  ((1  p6 )  (1  p2 ))
p10  ((1  p5 )  (1  p1 )) + (1  p10 )  ((1  p6 )  (1  p2 ))
289

fiPOOLE & ZHANG

FH
true
fb  mb, true
false
false

ff

ff

FH
true
fb, true
false
false

FT
true
false
true
false

Value
fi
p9  p1 + (1  p9 )  p2
p10  p1 + (1  p10 )  p2
p9  (1  p1 ) + (1  p9 )  (1  p2 )
p10  (1  p1 ) + (1  p10 )  (1  p2 )

Value
fi
p3
p4
1  p3
1  p4

Value
fi
p9  p5 + (1  p9 )  p6
p10  p5 + (1  p10 )  p6
p9  (1  p5 ) + (1  p9 )  (1  p6 )
p10  (1  p5 ) + (1  p10 )  (1  p6 )
ff
fi
S
Value
fb  mb, true p9 + (1  p9 )
false p10 + (1  p10 )
ff

ff

MH
true
fb  mb, true
false
false

S
true
false
true
false

MH
true
mb, true
false
false

MT
true
false
true
false

S
true
false
true
false

Value
fi
p7
p8
1  p7
1  p8

Note that the third and the sixth confactors were there originally and were not affected by
eliminating OT .
The resultant confactors encode the probabilities of {FH, MH} in the context fb  mb. For all
other contexts, CVE considers FH and MH separately. The total table size of the confactors after
OT is eliminated is 24.
Unlike VE or BEBA, we need the combined effect on FH and MH only for the contexts where OT
is relevant to both FH and MH. For all other contexts, we dont need to combine the confactors for
FH and MH. This is important, as combining the confactors is the primary source of combinatorial
explosion. By avoiding combining confactors, we can have a potentially huge saving when the
variable to be summed out appears in few contexts.
4.13 CVE Compared To VE
It is interesting to see the relationship between the confactors generated and the factors of VE for
the same belief network, with the same query and observations and the same elimination ordering.
There are two aspects to the comparison, first the exploitation of contexts, and second the idea of
not multiplying confactors unless you need to. In this section, we introduce a tree-based variable
elimination algorithm (TVE) that uses confactors but doesnt have the property of the example above
where there are confactors that are not multiplied when VE would multiply the corresponding factors.
In order to understand the relationship between VE and CVE for the same query and the same
elimination order, we can consider the VE derivation tree of the final answer. The tree contains all
290

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

initial and intermediate factors created in VE. The parents in the tree of any factor are those factors
that were multiplied or had a variable summed out to produce this factor. Note that this is a tree
(each factor has only one child) as each factor is only used once in VE; once it is multiplied or a
variable is summed from it, it is removed.
For each node in this tree that is created by multiplying two other factors, the number of multiplications in VE is equal to the table size of the resulting factor. For each factor created by summing
out a variable, the number of additions is equal to the size of its parent minus its size.
We can define tree-based variable elimination (TVE) to be a composite of VE and CVE. It uses
confactors as in CVE. Associated with each factor in the VE derivation tree is a set of confactors.
WhenVE multiplies two factors, TVE multiplies (and does the requisite splitting) all of the compatible
confactors associated with the factors being multiplied. TVE is essentially the same as the tree-based
merging of Boutilier (1997) (but Boutilier also does maximization at decisions).
Whenever VE multiplies two factors, TVE multiplies all of the confactors associated with the
factors. The TVE confactors associated with the VE factors will always have a total table size that is
less than or equal to the VE factor size. TVE maintains a set of confactors with mutually exclusive
and covering contexts. The number of multiplications is equal to the resulting table size for each
pairwise multiplication (as each entry is computed by multiplying two numbers). It is easy to see
that TVE always does fewer or an equal number of multiplications than VE.
CVE is like TVE except that CVE doesnt multiply some of the confactors when VE multiplies
two factors. It delays the multiplications until they need to be done. It relies on the hope that
the confactors can be separately simplified before they need to be multiplied. This hope is not
unjustified because if eliminating a variable means that both of the factors need to be multiplied by
other confactors, then they need to be multiplied by each other.
Example 21 If we were to use TVE for Example 20, once OT is eliminated, TVE builds a tree
representing the probability on both FH and MH. This entails multiplying out the confactors that
were not combined in CVE, for example multiplying the third, fifth and sixth factors of theresult of
Example 20, which produces the confactor of the form f b  mb, t(FH, FT , MH, MT , S) . Eliminating OT results in a set of confactors with total table size 72, compared to 24 for VE. Without any
contextual structure, VE builds a table with 27 = 128 values.
It is possible that not multiplying compatible confactors earlier means that we will eventually
have to do more multiplications. The following example is the simplest example we could find where
CVE does more multiplications than VE or TVE. Slight variations in the structure of this example,
however result in CVE doing fewer multiplications.
Example 22 Consider the belief network shown in Figure 8(a). First we will sum out a variable, A,
to create two confactors that are not multiplied in CVE but are multiplied in TVE. We then multiply
one of these confactors by another factor when summing out the second variable, B. We then force
the multiplication when eliminating C.
Suppose that all of the variables are binary except for variable W that has domain size 1000. (The
counter example doesnt rely on non-binary variables; you could just have B having 10 binary parents,
but this makes the arithmetic less clear). In the analysis below we only discuss the multiplications
that involve W , as the other multiplications sum up to less than a hundred, and are dominated by the
multiplications that involve W .
Suppose that we have the following confactors for S:
x, t1 (A, B, C, S)

(38)
291

fiPOOLE & ZHANG

W

W

X

C

B

A

C

B

S

S

T
(a)

(b)

Figure 8: Belief network for Example 22.
x, t2 (B, C, S)

(39)

Suppose that we have the following confactors for T :
x, t3 (A, B, C, T )

(40)

x, t4 (C, T )

(41)

Suppose first that T is observed. Then the confactors for T are replaced by:
x, t5 (A, B, C)

(42)

x, t6 (C)

(43)

Lets now eliminate A. In both CVE and TVE, confactors (38) and (42) and the prior on A are
multiplied together, and A is summed out resulting in:
x, t7 (B, C, S)

(44)

In TVE, confactors (39) and (43) are also multiplied together resulting in:
x, t8 (B, C, S)

(45)

Next we eliminate B. In both CVE and TVE, confactor (44) is multiplied by the factors representing
P(C|B) and P(B|W ). We assume that the factor representing P(B|W ) is multiplied last as this
minimises the number of multiplications. This involves 8008 multiplications (as the product has a
table size of 8000 and the intermediate table is of size 8). Then B is summed out resulting in the
factor:
x, t9 (C, S, W )

(46)

In CVE, confactor (39) is multiplied by the factors representing P(C|B) and P(B|W ). This also
involves 8008 multiplications. Then B is summed out from the product resulting in:
x, t10 (C, S, W )

(47)

In TVE, confactor (45) is multiplied by the factors representing P(C|B) and P(B|W ). This also
involves 8008 multiplications. Then B is summed out from the product resulting in:
x, t11 (C, S, W )

(48)
292

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

When C is eliminated, TVE requires no more multiplications. It just sums out C from the table of
confactor (48). However in CVE, we need to multiply confactors (43) and (47), which involves 4000
multiplications. The resulting confactors from CVE and TVE are identical.
Thus CVE requires about 20000 multiplications, TVE requires about 16000 multiplications. VE,
for the same elimination order, requires about 16000 multiplications.
This should not be surprising, particularly when you realise that VE is not optimal. For a given
elimination ordering, it is sometimes optimal to multiply factors before VE actually multiplies them,
as the following example shows:
Example 23 Consider the belief network in Figure 8(b), with the same domain sizes as in the
previous example. The factors represent P(W ), P(B|W ), P(C), P(S|BC). If we were to eliminate B
then C, it is more efficient to preemptively multiply P(C) by P(S|BC) than to delay the multiplication
till after summing out B (as VE does).
It may seem negative to show that CVE doesnt always do fewer multiplications than VE but
has the overhead of maintaining contexts. However, there seems no reason why the preemptive
multiplication of TVE is optimal either. One possibility is to treat when to multiply and when
to sum out variables as a secondary optimisation problem (Bertel and Brioschi, 1972; Shachter
et al., 1990; DAmbrosio, 1995); unfortunately this optimization is also computationally difficult
(DAmbrosio, 1995). This, however is beyond the scope of this paper.

5. Avoiding Splitting
5.1 Absorption
In this section we characterize a case when we dont need to split during multiplication. Note that
the result of eliminating a variable is exactly the same as before; we are saving because we dont
create the residuals, but rather use the original confactor.
Definition 15 A multiset of confactors R is complete if the contexts of the confactors are mutually
exclusive and covering.
If we have a multiset R of confactors that is complete, it means that we dont have to split other
confactors r that may need to be multiplied by all of the confactors in R. For each residual of r, there
will be another element of R with which it will be compatible. Instead of splitting r, we can just
multiply it by each element of R. This is the intuition behind absorption. Note that we may need to
split elements of R during multiplication.
Suppose we have a complete multiset of confactors R and another confactor r1 = c1 , t1 . Define
absorb(R, c1 , t1 )
= {ci , pi   R : incompatible(ci , c1 )}


(residual(ci , ti , c1 )  {c1  ci , set(t1 , ci ) t set(ti , c1 )})
ci , ti   R
compatible(c1 , ci )
where t is table multiplication (Definition 3).
293

fiPOOLE & ZHANG

Correctness: We can replace R  {r1 } with absorb(R, r1 ) and the program invariant is preserved.
First note that the confactors in absorb(R, r1 ) are complete (and so the second part of the invariant
holds). To see why the first part of the invariant is preserved, consider a complete context c. If
c is compatible with c1 , then in the original confactors, we use one confactor from R as well as
r1 . In the revised confactor base we use the appropriate confactor with the product of the original
probabilities. If c is incompatible with c1 then it is compatible with one element c2 , p2  of R. If
c2 is incompatible with c1 , the confactor to be used is the original confactor, and if c2 is compatible
with c1 , then we use the residual confactor. In each case we get the same contributions from R  {r1 }
and from absorb(R, r1 ).
Example 24 If we were to eliminate B from the confactors of Figure 4 (as in example 14), we can
treat the two confactors for P(B|Y , Z) as the complete multiset of confactors. This means that we
dont need to split the other confactors on Y .
Note that if we try to use the confactors for E as the complete set of confactors when eliminating
B, we dont have to split the confactors on A, C or D, but we have to consider confactors that dont
involve B when eliminating B, and we end up multiplying confactors that dont need to be multiplied.
Note that if R cannot be represented as a decision tree (e.g., if R has confactors corresponding to
the contexts in {{a, b}, {a, c}, {b, c}, {a, b, c}, {a, b, c}}), its possible that there is no way to split r1
that results in as compact a representation as results from absorb(R, r1 ).
It seems as though it is very useful to have a multiset of confactors that is complete, but it is not
of much use if we cannot easily find such a set. First note that if we have a multiset R of confactors
that is complete, then absorb(R, r1 ) is also a multiset of confactors that is complete, which can, in
turn, be used to combine with another confactor.
Initially, for each variable X, the confactors that represent the conditional probability table
P(X|X ) are complete. Moreover they will all contain X and so need to be involved when eliminating
X. These can be used as the initial seed for absorbing other confactors. Unfortunately, after we have
eliminated some variables, the confactors that define the initial conditional probability tables for
some variable X dont exist anymore; they have been split, multiplied by other confactors and added
to other confactors. However, for each variable X, we can still extract a useful multiset of confactors
that all contain X that are complete on the empty context (i.e., are mutually exclusive and covering).
These will be called the confactors for X, and correspond to the confactors with X in the head in an
earlier version of contextual variable elimination (Poole, 1997).
Definition 16 If X is a variable, the confactors for X are a subset of the confactor base defined by:
 Initially the confactors for X are the confactors that define the conditional probability P(X|X ).
 When we split a confactor for X, the confactors created are also confactors for X. Note that
this does not cover the case where we are splitting a confactor on a confactor for X.
 When we multiply a confactor for X with another confactor, the confactor created is a confactor
for X.
 When we add a confactor for X with another confactor (when eliminating another variable Y ,
for example), the resulting confactor is also a confactor for X.
Proposition 2 The confactors for X at any stage of contextual variable elimination are complete.
294

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

To show this, we will show that the three basic operations preserve this property.
 splitting preserves this property, as the resulting confactors are exclusive and cover the context
of the confactor being split.
 multiplication preserves this property as, for any variable X, only one of the confactors involved
in a multiplication can be a confactor for X (as the confactors for X are mutually exclusive) and
the set of contexts covered by the confactors isnt changed by multiplication. This argument
also holds for absorption.
 for addition, note that, for any variable X, either all of the confactors or none of the confactors
involved in an addition are confactors for X when summing out Y . To show this suppose we
have r1 = c  Y = v1 , p1  and r2 = c  Y = v2 , p2  where confactor r1 is a confactor for
X and confactor r2 isnt. Because the confactors for X are mutually exclusive and covering,
there must be a confactor that is covered by X that is applicable in a context when c  Y = v2
is applicable. This confactor cannot mention Y , for otherwise addition isnt applicable, and so
it must also be applicable when c  y = v1 is true, which contradicts the mutual exclusiveness
of the confactors for X. Now it is easy to see that addition preserves this property as the
confactors being summed cover the same contexts as the resulting confactor.
We can also use the idea of the confactors for X to recognise when summing out a variable will
create a table of 1s that can be removed (see Section 4.10). First note that in the original confactors
for X, if, when eliminating X, we dont have to multiply these by other confactors (i.e., they have
no children in this context), then we know that summing out X will produce a table of 1s. We can
do better than this for recognising when we will produce ones. We will use one bit of information
to encode whether a confactor for X is pure for X. Initially all of the confactors for X are pure for
X. If a confactor for X is pure for X, and, when eliminating Y is absorbed into a confactors for Y
that is pure for Y , then the resulting confactor is pure for X. For every other case when a confactor
for X is multiplied by another confactor, the result is not pure for X. If we are summing out X, after
absorption, we can remove all confactors for X that are pure for X. This is correct because we have
maintained the invariant that if we sum out X from the table of a confactor for X that is pure for X we
create a table with only ones. Note that this procedure generalises the idea that we can recursively
remove variables with no children that are neither observed nor queried.
The idea of absorption into the rules for the variable being eliminated described in this section
should only be seen as a heuristic to avoid splitting. It does not necessarily reduce the amount
of work. First note that in variable elimination there is a choice in the order that we multiply
factors. Multiplication of factors is associative and commutative, however, the order in which the
multiplication is carried out affects efficiency, as the following example shows.
Example 25 Suppose variable B has one parent, A and two children C and D, who each only have
B as a parent. When eliminating B we have to multiply the factors representing P(B|A), P(C|B)
and P(D|B). Suppose that B, C and D are binary, and that A has domain size of 1000. When
multiplying two factors the number of multiplications required is equal to the size of the resulting
factor. If we save intermediate results, and multiply these in the order (P(B|A)t P(C|B))t P(D|B),
we will do 12000 multiplications. If we save intermediate results, and multiply these in the order
P(B|A) t (P(C|B) t P(D|B)), we will do 8008 multiplications. If we dont save intermediate
tables, but instead recompute every product, we will do 16000 multiplications.
295

fiPOOLE & ZHANG

If you need to multiply k > 1 factors, where m is the size of the resulting factor, the number of
multiplications is bounded below by k  2 + m (as the final product requires m multiplications and
each other requires at least one multiplication) and bounded above by (k  1)  m (as there are k  1
factor multiplications and each of these requires at most m multiplications).
The associative ordering imposed by absorption into the rules for the variable being eliminated
(which for the example above implies absorbing P(C|B) and P(D|B) into P(B|A)) may not be the
optimal multiplication ordering. The absorption ordering (that saves because it reduced splitting)
should be seen as a heuristic; it may be worthwhile to do a meta-level analysis to determine what
order to multiply (Bertel and Brioschi, 1972; Shachter et al., 1990; DAmbrosio, 1995), but this is
beyond the scope of this paper.
5.2 Summing Out A Variable
Suppose we are eliminating Y , and we have absorbed all of the confactors that contain Y into the
confactors for Y . Then any two confactors in R that contain Y have incompatible contexts. The
contexts for the confactors that contain Y in the table are disjoint from the contexts of the confactors
that contain Y in the body.
Summing out Y from a confactor that contains Y in the table proceeds as before. We can use a
similar trick to absorption to avoid any more splitting when adding confactors that contain Y in the
body.
Suppose Y has domain {v1 , . . . , vs }. The contexts of the confactors with Y =vi in the body are
exclusive and the disjunct is logically equivalent to the disjunct of confactors with Y =vj in the body
for any other value vj .
For each 1  i  s, let Ri = {b, t : b  Y =vi , t  R+ }. Thus Ri is the confactor for Y = vi
in the context, but with Y = vi omitted from the context. We will combine the Ri s using a binary
operation:
R1 g R2 = {c1  c2 , set(t1 , c2 ) t set(t2 , c1 ) : c1 , t1   R1 , c2 , t2   R2 , compatible(c1 , c2 )}
where t is an addition operation defined on tables that is identical to the product t of Definition
3 except that it adds the values instead of multiplying them.
Then R1 g R2 g . . . g Rs is the result from summing out Y from the confactors with Y in the
body.
5.3 Contextual Variable Elimination with Absorption
A version of contextual variable elimination that uses absorption, is given in Figure 9. This is the
algorithm used in the experimental results of Section 6.
The elimination procedure is called once for each non-query, non-observed variable. The order
in which the variables are selected is called the elimination ordering. This algorithm does not imply
that the elimination ordering has to be given a priori.
One of the main issues in implementing this algorithm is efficient indexing for the confactors. We
want to be able to quickly find the confactors for Y , the confactors that contain Y , and the compatible
confactors during addition and absorption. If we are given a prior elimination ordering, we can use
the idea of bucket elimination (Dechter, 1996), namely that a confactor can be placed in the bucket
of the earliest variable in the elimination ordering. When we eliminate Y , all of the confactors that
contain Y are in Y s bucket. If we dont have a prior elimination ordering, we can keep an inverted
296

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

To compute P(X|E1 =o1  . . .  Es =os )
Given
multiset of contextual contribution confactors
1. Incorporate evidence as in Section 4.7.
2. While there is a factor involving a non-query variable
Select non-query variable Y to eliminate;
Call eliminate(Y ).
3. Compute posterior probability for X as in Section 4.8
Procedure eliminate(Y):
partition the confactorbase R into:
R those confactors that dont involve Y
R+ = {r  R : r is a confactor for Y }
R = {r  R : r involves Y and r is not a confactor for Y };
for each r  R
do R+  absorb(R+ , r);
partition R+ into:
Rt = {r  R+ : Y in table of r}
Ri = {b, t : b  Y =vi , t  R+ } for each 1  i 
 s, where dom(Y ) = {v1 , . . . , vs }.
Return confactorbase R  (R1 g    g Rs )  {bi , Y ti  : bi , ti   Rt }.
absorb(R, r) is defined in Section 5.1.
R1 g R2 is defined in Section 5.2.
Figure 9: Contextual Variable Elimination with Absorption

297

fiPOOLE & ZHANG

list of the confactors (for each variable, we have a list of all of the confactors that are for that variable
and a list of the confactors that contain that variable). We then have to maintain these lists as we
create new confactors and delete old ones. We also want to be able to index the confactors so that
we can quickly find other confactors that contain the variable to be eliminated and have compatible
contexts. In our implementation, we compared all of the confactors that contain the variable to be
eliminated, and rejected those with incompatible contexts. Ideally, we should be able to do better
than this, but how to do it is an open question.
There are a number of choice points in this algorithm:
 the elimination ordering.
 the splitting ordering; when computing residuals, which order should the variables be split on.
This is discussed in Section 4.4.
 the order that the elements of R are absorbed. This has an impact similar to the choice of
multiplication ordering for VE (when we have to multiply a number of factors, which order
should they be done); sometimes we can have smaller intermediate factors if the choice is
done appropriately.

6. Empirical Results
An interesting question is whether there are real examples where the advantage of exploiting contextual independence outweighs the overhead of maintaining confactors.
We can easily generate synthetic examples where VE is exponentially worse than contextual
variable elimination (for example, consider a single variable with n, otherwise unconnected, parents,
where the decision tree for the variable only has one instance of each parent variable, and we
eliminate from the leaves). At another extreme, where all contexts are empty, we get VE with very
little overhead. However, if there is a little bit of CSI, it is possible that we need to have the overhead
of reasoning about variables in the contexts, but get no additional savings. The role of the empirical
results is to investigate whether it is ever worthwhile trying to exploit context-specific independence,
and what features of the problem lead to more efficient inference.
6.1 A Pseudo-Natural Example
While it may seem that we should be able to test whether CVE is worthwhile for natural examples
by comparing it to VE for standard examples, it isnt obvious that this is meaningful. With the tablebased representations, there is a huge overhead for adding a new parent to a variable, however there is
no overhead for making a complex function for how a variable depends on its existing parents. Thus,
without the availability of effective algorithms that exploit contextual independence where there is a
small overhead for adding a variable to restricted contexts, it is arguable that builders of models will
tend to be reluctant to add variables, but will tend to overfit the function for how a variable depends on
its parents. As all models are approximations it makes sense to consider approximations to standard
models. As we are not testing the approximations (Dearden and Boutilier, 1997; Poole, 1998), we
will pretend these are the real models.
In this section we produce evidence that there exists networks for which CVE is better than
VE. The sole purpose of this experiment it to demonstrate that there potentially are problems where
it is worthwhile using CVE. We use an instance of the water network (Jensen, Kjrulff, Olesen
298

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

100
0 observations
5 observations
10 observations
CVE=VE

VE runtime (secs)

10

1

0.1

0.01

0.001
0.001

0.01

0.1
1
CVE runtime (secs)

10

100

Figure 10: Scatterplot of runtimes (in msecs) of CVE (x-axis) and VE (y-axis) for the water network.
Full details are in Appendix A.

and Pedersen, 1989) from the Bayesian network repository8 where we approximated the conditional
probabilities to create contextual independencies. Full details of how the examples were constructed
are in Appendix A. We collapsed probabilities that were within 0.05 of each other to create confactors.
The water network has 32 variables and the tabular representation has a table size of 11018 (after
removing variables from tables that made a difference of less that 0.05). The contextual belief
network representation we constructed had 41 confactors and a total table size of 5834.
Figure 10 shows a scatter plot of 60 runs of random queries9 . There were 20 runs each for 0,
5 and 10 observed variables. The raw data is presented in Appendix A. The first thing to notice
is that, as the number of observations increases, inference becomes much faster. CVE was often
significantly faster than VE. There are a few cases where CVE was much worse than VE; essentially,
given the elimination ordering, the context-specific independence didnt save us anything in these
example, but we had to pay the overhead of having variables in the context.
8. http://www.cs.huji.ac.il/labs/compbio/Repository/
9. Note that all of these results are statistically significant to some degree. The least significant is with 10 observations,
that, using the matched-sample t-test (also known as the paired t-test), is significant to the 0.2 level for the logarithm
of the runtimes. The others are significant way below the 0.001 level. The logarithm is appropriate as the difference
in the logarithms corresponds to the multiplicative speedup.

299

fiPOOLE & ZHANG

6.2 Randomised Networks
In order to see how the algorithm depends on the structure inherent in the network, we constructed
a number of parametrized classes of networks. We explicitly constructed networks that display
contextual independence, as if there is not contextual independence this algorithm degenerates to
VE.
We have the following parameters for building random networks:
n the number of variables
s the number of splits (so there will be n + s confactors).
p the probability that a predecessor variable that isnt in the context of a confactor will be in the
table of the confactor.
The exact algorithm for constructing the examples is given in Appendix A.
The variable s controls the number of confactors, and p controls (probabilistically) the size of
the tables. Figure 1110 shows a scatter plot comparing the runtimes of CVE and VE for n = 30 and
p = 0.2 and for three different values of s, 5, 10, and 15.
While this may look reasonable, it should be noticed that the number of splits and the number of
different variables in the splits is strongly correlated in these examples (see Appendix A for details).
However, one of the properties of CVE is that if a variable does not appear in the body of any
confactor, it is never added to the context of a constructed confactor. That is, a variable that only
appears in tables, always stays in tables. Thus it may be conjectured that having fewer variables
appearing in contexts may be good for efficiency.
We carried out another experiment to test this hypothesis. In this experiment, the networks were
generated as before, however, when we went to split a context we attempted to first split it using a
variable that appears in a different context before using a variable that didnt appear in a context. The
full details of the algorithms to generate examples and some example data are given in Appendix A.
Figure 12 shows a scatter plot of comparing the run times for CVE and VE for each of the generated
examples. With this class of networks CVE is significantly faster than VE.

7. Comparison With Other Proposals
In this section we compare CVE with other proposals for exploiting context-specific information.
The belief network with the conditional probability table of Figure 1 (i.e., with the contextual
independence shown in Figure 4) is particularly illuminating because other algorithms do very badly
on it. Under the elimination ordering B, D, C, A, Y , Z, to find the marginal on E, the most complicated
confactor multiset created is the confactor multiset for E after eliminating B (see Example 14) with
a total table size of 16. Observations simplify the algorithm as they mean fewer partial evaluations.
In contrast, VE requires a factor with table size 64 after B is eliminated. Clique tree propagation
constructs two cliques, one containing Y , Z, A, B, C, D of size 26 = 64, and the other containing
A, B, C, D, E of size 32. Neither takes the structure of the conditional probabilities into account.
Note however, that VE and clique tree propagation manipulate tables which can be indexed much
faster than we can manipulate confactors. There are cases where the total size of the tables of the
10. Note that all of these results are statistically significant. The least significant is the s = 10 plot that, using the matchedsample t-test (also known as the paired t-test), is significant to the 0.05 level for the logarithm of the runtimes. The
logarithm is appropriate as the difference in the logarithms corresponds to the multiplicative speedup.

300

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

1000
s=5
s=10
s=15
CVE=VE

VE runtime (secs)

100

10

1

0.1
0.1

1

10
CVE runtime (secs)

100

1000

Figure 11: Scatterplot of runtimes (in seconds) of CVE (x-axis) and VE (y-axis) for randomised
networks

301

fiPOOLE & ZHANG

1000
s=5
s=10
s=15
CVE=VE

VE runtime (secs)

100

10

1

0.1
0.1

1

10
CVE runtime (secs)

100

1000

Figure 12: Scatterplot of runtimes (in seconds) of CVE (x-axis) and VE (y-axis) for random networks
biased towards fewer different variables in the contexts.

302

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

confactors is exponentially smaller than the tables (where added variables are only relevant in narrow
contexts). There are other cases where the table size for the confactors is the same as the table size
in VE, and where the overhead for manipulating contexts will not make CVE competitive with the
table-based methods.
Boutilier et al. (1996) present two algorithms to exploit structure. For the network transformation
and clustering method, the example of Figure 1 is the worst case; no structure can be exploited after
triangulation of the resulting graph. (The tree for E in Figure 3 is structurally identical to the tree
for X(1) in Figure 2 of (Boutilier et al., 1996)). The structured cutset conditioning algorithm does
well on this example. However, if the example is changed so that there are multiple (disconnected)
copies of the same graph, the cutset conditioning algorithm is exponential in the number of copies,
whereas the probabilistic partial evaluation algorithm is linear11 .
This algorithm is most closely related to the tree-based algorithms for solving MDPs (Boutilier
et al., 1995), but these work with much more restricted networks and with stringent assumptions on
what is observable. CVE is simpler than the analogous algorithm by Boutilier (1997) for structured
MDP with correlated action effects that represents conditional probabilities as trees. Section 4.12
shows why we can do better than the tree-based algorithms.
Poole (1997) gave an earlier version of rule-based VE, but it is more complicated in that it
distinguished between the head and the body of rules as part of the inference (although the confactors
for X correspond to the rules with X in the head). CVE is much more efficient than the rule-based VE
as it allows for faster indexing of tables. The notion of absorption was introduced by Zhang (1998),
which motivated the work in a very different way. Zhang and Poole (1999) give a mathematical
analysis of how context-specific independence can be exploited in terms of partial functions. The
union-product is a formalization of the operation we are using between confactors. The current paper
extends all of these in giving a specific algorithm showing how to combine the confactors and tables,
gives a more general analysis of when we need to do confactor splitting and when we dont need to,
gives a more sophisticated method to initialize absorption (maintaining the confactors for a variable)
and gives a much more detailed empirical evaluation (with a new implementation). The ability to
handle ones is also improved.
Finally the representation should be contrasted with that of Geiger and Heckerman (1996).
They use a similarity network that gives a global decomposition of a belief network; for different
contexts they allow for different belief networks. We allow for a local decomposition of conditional
probabilities. The algorithms are not very similar.

8. Conclusion
This paper has presented a method for computing posterior probabilities in belief networks that exhibit
context-specific independence. This algorithm is an instance of variable elimination, but when we
sum out a variable the tables produced may depend on different sets of variables in different contexts.
The main open problem is in finding good heuristics for elimination orderings (Bertel and
Brioschi, 1972). Finding a good elimination ordering is related to finding good triangulations in
building compact junction trees, for which there are good heuristics (Kjrulff, 1990; Becker and
Geiger, 1996). These are not directly applicable to contextual variable elimination (although there
11. This does not mean that all conditioning algorithms need suffer from this. We conjecture that there is a conditioning
algorithm that can extend contextual VE but save space, as in other bucket elimination algorithms (Dechter, 1996) or
the relevant cutsets for probabilistic inference (Darwiche, 1995; Dez, 1996).

303

fiPOOLE & ZHANG

are analogous heuristics that are applicable), as an important criteria in this case is the exact form
of the confactors, and not just the graphical structure of the belief network. This means that it is not
feasible to compute the elimination ordering a priori. We are also investigating anarchic orderings
where we eliminate a variable in some contexts, without eliminating it in every context before we
partially eliminate another variable. We believe that this opportunistic elimination of variables in
contexts has much potential to improve efficiency without affecting correctness.
One of the main potential benefits of this algorithm is in approximation algorithms, where the
confactors allow fine-grained control over distinctions. Complementary confactors with similar
probabilities can be collapsed into a simpler confactor. This can potentially lead to more compact
confactor bases, and reasonable posterior ranges (Dearden and Boutilier, 1997; Poole, 1998).
The work reported in this paper has been followed up by a number of researchers. Tung (2002)
has shown how to exploit context-specific independence in clique trees. Guestrin, Venkataraman and
Koller (2002) extended the contextual variable elimination to multi-agent coordination and planning.

Acknowledgements
This work was supported by the Institute for Robotics and Intelligent Systems, Natural Sciences and
Engineering Research Council of Canada Research Grant OGPOO44121 and Hong Kong Research
Grants Council grant HKUST6093/99E. Thanks to Rita Sharma, Holger Hoos, Michael Horsch and
the anonymous reviewers for valuable comments, and to Valerie McRae for careful proofreading.
The code is available from the first author.

Appendix A. Details of the experiments
A.1 Water Network
In order to construct a pseudo-natural example, we used the water network from the Bayesian
network repository12 and modified it to let it exhibit context-specific independence. For each table,
a variable was declared redundant if the differences in the probabilities for the values of this variable
were less than a threshold of 0.05 from each other (thus, if we chose the midpoint of a reduced
table, the original probabilities were all less than 0.025 from this midpoint). In order to discover
context-specific independence, we carried out a greedy top-down algorithm to build a decision tree.
If we are building the conditional for variable Xi , we chose the variable Y to split on that results in the
maximum number of pairs of numbers where the values for Xi are within the threshold 0.05 of each
other. We then recursively remove redundant variables from each side, and keep splitting. Once we
have built a tree, for each node, we can decide whether to use the tabular representation or a factored
representation. In these experiments, we only committed to the context-based representation when
the total size of the context-based representation (obtained simply by summing the sizes of the tables)
was less than 51% of the tabular representation.
It should be noted that these thresholds were not chosen arbitrarily. If we used 0.03 instead
of 0.05, we didnt find any context-specific independence. If we chose 0.07 instead of 0.05, then
the tabular representation collapses. If we chose 80% or 99% instead of 51% as the threshold for
accepting a change, we got smaller tables, but much larger run times.
12. http://www.cs.huji.ac.il/labs/compbio/Repository/

304

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

CVE
Query Var
#11(CKND_12_15)
#13(CBODN_12_15)
#27(CKND_12_45)
#25(CKNI_12_45)
#22(CKNN_12_30)
#21(CBODN_12_30)
#17(CKNI_12_30)
#22(CKNN_12_30)
#19(CKND_12_30)
#15(CNON_12_15)
#12(CNOD_12_15)
#3(CKND_12_00)
#16(C_NI_12_30)
#30(CKNN_12_45)
#4(CNOD_12_00)
#21(CBODN_12_30)
#5(CBODN_12_00)
#19(CKND_12_30)
#11(CKND_12_15)
#23(CNON_12_30)

Time
15039
620
3808
1708
367
7953
742
363
7939
8177
618
419
429
2902
2496
8042
992
7936
16290
604

Size
1327104
16384
186624
36864
16128
193536
48384
16128
774144
193536
37044
29376
28224
112896
110592
193536
112320
774144
1327104
37044

VE
Time
25368
4552
53965
57414
3821
13997
3677
3846
26654
14599
4264
9414
3799
52648
42270
13619
3334
25637
24753
3535

Size
5308416
442368
15925248
7077888
442368
1769472
442368
442368
2359296
1769472
442368
1327104
442368
15925248
5308416
1769472
442368
2359296
5308416
442368

Figure 13: Data for random queries with no observed variables for the water network
Figure 13 shows some of the details of some of the data with no observations. Figures 14 and 15
shows some of the details of some of the data with 5 and 10 observation respectively. These make up
the data shown in Figure 10 of the main paper. We show the index of the variable given the ordering
in the repository (we started counting at 0).
All of the times are in milliseconds for a Java implementation running on 700 MHz Pentium
running Linux with 768 megabytes of memory. In order to allow for the variation in run times due
to garbage collection each evaluation was run three times and the smallest time was returned. The
space is for the maximum table created in VE or the maximum size of the sum of all of the confactors
created during the elimination of one variable.
A.2 Randomised Experiments
The following procedure was used to generate the random networks. Given the n variables, we
impose a total ordering. We build a decision tree for each variable. The leaves of the decision trees
correspond to contexts and a variable that the tree is for. We start with the empty decision tree for
each variable. We randomly (with uniform probability) choose a leaf and a variable. If the variable
is a possible split (i.e., it is a predecessor in the total ordering of the variable the leaf is for and
the context corresponding to the leaf doesnt commit a value to that variable), we split that leaf on
that variable. This is repeated until we have n + s leaves. Then for each leaf, we built a confactor
that has the same context and each predecessor of the variable that the leaf is for is included in the
305

fiPOOLE & ZHANG

Observed
#1=2 #2=2 #26=2 #28=1 #30=1
#6=2 #8=1 #11=2 #14=2 #24=1
#8=3 #14=0 #15=3 #18=3 #20=2
#5=1 #9=0 #12=2 #15=0 #16=3
#6=1 #7=0 #11=1 #13=3 #25=2
#2=1 #6=0 #13=0 #19=1 #25=0
#0=3 #2=1 #18=2 #20=1 #23=1
#4=3 #10=3 #12=3 #17=0 #28=2
#3=1 #11=1 #19=1 #25=1 #31=3
#10=3 #19=0 #21=0 #26=0 #27=0
#11=1 #13=0 #21=1 #25=2 #29=2
#3=0 #23=1 #26=1 #27=0 #28=3
#9=2 #10=1 #13=1 #25=2 #26=1
#9=1 #11=0 #14=2 #15=1 #27=0
#2=0 #10=2 #15=2 #17=1 #24=1
#2=0 #7=3 #15=1 #21=2 #27=2
#0=3 #2=0 #6=0 #7=2 #16=2
#2=2 #20=2 #24=2 #25=0 #30=0
#8=2 #11=2 #19=2 #25=1 #29=3
#9=1 #12=3 #13=0 #19=2 #21=3

Query Var
#8(C_NI_12_15)
#22(CKNN_12_30)
#6(CKNN_12_00)
#10(CBODD_12_15)
#27(CKND_12_45)
#18(CBODD_12_30)
#17(CKNI_12_30)
#6(CKNN_12_00)
#14(CKNN_12_15)
#16(C_NI_12_30)
#24(C_NI_12_45)
#15(CNON_12_15)
#30(CKNN_12_45)
#25(CKNI_12_45)
#8(C_NI_12_15)
#16(C_NI_12_30)
#29(CBODN_12_45)
#21(CBODN_12_30)
#9(CKNI_12_15)
#26(CBODD_12_45)

CVE
Time
Size
84
9216
15
816
9
336
54
576
1184
28224
123
9216
16
1728
284
6912
1450
36864
1076
49152
353
28080
14258 193536
75
9216
65
4096
12
576
29
1008
7
336
453
49152
76
9216
31
1024

VE
Time
579
156
26
71
3210
224
87
162
1041
521
13928
2600
2646
120
767
531
1304
877
216
440

Size
36864
9216
2304
6912
147456
12288
5184
20736
147456
49152
1327104
442368
248832
4096
110592
82944
147456
49152
9216
49152

Figure 14: Data for random queries with 5 randomly observed variables for the water network

306

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

Observed
#2=3 #3=0 #7=2 #12=1 #13=2
#19=1 #21=2 #22=2 #26=1 #30=1
#0=0 #3=0 #8=3 #12=3 #13=0
#16=2 #18=2 #26=3 #27=2 #28=1
#1=2 #6=0 #8=0 #11=2 #17=0
#20=3 #22=1 #23=1 #24=2 #26=2
#2=2 #5=3 #8=0 #9=1 #10=1
#17=2 #18=1 #22=2 #28=0 #30=1
#4=2 #7=1 #8=2 #12=1 #13=2
#15=3 #17=0 #19=0 #22=2 #31=2
#1=1 #7=1 #9=1 #10=0 #11=0
#13=3 #14=1 #23=1 #24=1 #30=2
#2=1 #4=0 #6=0 #15=0 #18=0
#22=2 #23=3 #24=2 #29=2 #30=1
#3=0 #10=0 #14=1 #16=3 #19=0
#24=1 #25=2 #28=3 #30=1 #31=1
#1=2 #2=3 #3=1 #9=2 #10=0
#14=0 #16=3 #25=1 #28=3 #30=2
#2=3 #5=1 #6=0 #11=2 #12=0
#17=1 #22=0 #24=3 #27=0 #28=1
#8=3 #9=2 #10=0 #11=1 #12=1
#14=2 #15=3 #19=0 #22=2 #26=3
#1=0 #7=2 #8=2 #13=0 #15=3
#17=2 #20=3 #26=1 #27=0 #31=3
#4=2 #5=3 #6=1 #9=1 #10=0
#12=2 #17=0 #19=1 #25=0 #29=0
#0=0 #2=1 #11=1 #13=1 #17=2
#21=3 #22=1 #23=1 #24=2 #30=2
#4=1 #9=0 #10=0 #11=1 #12=2
#23=1 #25=2 #29=1 #30=0 #31=2
#1=2 #6=0 #7=1 #10=3 #12=1
#15=3 #16=1 #17=2 #23=2 #24=1
#1=2 #2=2 #3=2 #5=2 #9=2
#13=1 #15=0 #22=1 #25=2 #30=0
#0=3 #1=1 #5=1 #6=0 #7=1
#8=2 #15=3 #17=0 #24=3 #25=0
#0=2 #6=1 #8=0 #9=2 #10=2
#16=0 #18=0 #19=0 #21=0 #26=0
#1=1 #3=0 #4=2 #9=1 #10=3
#13=0 #14=2 #22=0 #23=0 #30=2

Query

CVE
Time Size

VE
Time

Size

#14

30

2304

40

2304

#7

14

256

17

768

#25

7

256

7

256

#23

4

192

4

192

#28

10

256

8

256

#22

9

336

10

768

#25

14

768

31

2304

#7

544

9216

121

9216

#8

10

1024

16

1024

#25

22

768

22

768

#5

3

84

4

192

#24

11

256

8

256

#23

6

256

23

1024

#27

10

576

18

768

#5

77

4096

141

12288

#27

3

64

4

144

#18

4

112

65

12288

#21

76

768

16

1024

#13

7

576

12

768

#6

37

768

17

768

Figure 15: Data for random queries with 10 randomly observed variables for the water network

307

fiPOOLE & ZHANG

generate_random_CBN(n, s, p) :
create n Boolean variables X1 , . . . , Xn
Let S = {{}, Xi  : i  i  n}
{S is a set of context-variable pairs}
repeat
choose random c, Xi   S
choose random j such that 1  j < n
if j < i and Xj doesnt appear in c:



replace c, Xi   S with c  Xj = true, Xi and c  Xj = false, Xi
until there are n + s elements of S
for each c, Xi   S
Let V = {Xi }
for each j < i
if Xj doesnt appear in c
with probability p add Xj to V
create a random table T on the variables in V
add confactor c, T  to the contextual belief network
Figure 16: Algorithm for constructing the randomised examples
confactor with probability p. The variable that the leaf is for is also in the confactor. We assign
random numbers the probabilities (these numbers wont affect anything in the results assuming that
the times for operations on floating point numbers isnt affected by the actual values of the numbers).
The algorithm to construct the random examples used in Section 6.2 is given in Figure 16. Note
that choose random means to choose randomly from a uniform distribution.
For the examples biased towards using fewer variables, we replaced the line:




replace c, Xi   S with c  Xj = true, Xi and c  Xj = false, Xi
with
if there is k < i such that Xk doesnt appear in c and Xk is used in another context
then
replace c, Xi   S with c  Xk = true, Xi  and c  Xk = false, Xi 
else




replace c, Xi   S with c  Xj = true, Xi and c  Xj = false, Xi
where, if more than one such k exists, the k is chosen uniformly from all of the values that satisfy
the condition.
Figure 17 shows some of the details of some of the data shown in Figure 11. All of the times
are for a Java implementation running on a 700 MHz Pentium running Linux with 768 megabytes of
memory. In order to allow for the variation in run times due to garbage collection, each evaluation
was run three times and the smallest time was returned.
For each generated example, the table of Figure 17 shows
 n the number of variables
308

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

n

s

p

30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30

5 (5)
5 (5)
5 (5)
5 (5)
5 (5)
5 (5)
5 (5)
5 (4)
5 (4)
5 (3)
10 (9)
10 (9)
10 (10)
10 (8)
10 (7)
10 (8)
10 (9)
10 (8)
10 (9)
10 (8)
15 (10)
15 (11)
15 (11)
15 (12)
15 (11)
15 (11)
15 (13)
15 (12)
15 (12)
15 (10)

0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2

CBN
Size
10922
1846
1964
1600
1668
904
1738
1744
3060
2692
3842
1262
3908
4904
10456
1790
2054
3608
6392
6180
2724
5896
2096
3674
2388
1938
4188
2806
3512
1700

BN
Size
8398266
132692
13456
4908
8292
1906
10786
18538
87292
69602
530622
36070
80704
33568176
314126
28758
24452
58352
1188654
42344
2104338
8425520
2239982
39928
552768
49388
351374
111632
126464
541498

CVE
Time
MTS
31388 2097152
9653
393216
2022
131072
3922
196608
60304
614400
637
32768
720
42048
2223
131072
11681
524288
5325
262144
22288
524288
4063
147456
112537 1966080
81450 5111808
31627
589824
1590
98304
13642
262144
24948
819200
403347 5767168
10078
253952
56636 1572864
185925 6389760
27065
825344
6393
360448
8623
425984
11299
438272
18602
432776
3213
138240
16118
258048
5986
172032

VE
Time
84801
12418
3748
5572
61612
1005
1340
2546
12298
9530
31835
11038
31214
203284
44079
2974
5253
18574
359992
17501
49316
260645
42180
9631
13641
27303
38843
5463
11479
8414

Figure 17: Comparisons of random networks that exhibit CSI.

309

MTS
4194304
524288
131072
524288
2097152
65536
131072
262144
1048576
524288
2097152
524288
4194304
16777216
2097152
262144
262144
1048576
33554432
1048576
2097152
16777216
2097152
524288
524288
2097152
2097152
1048576
1048576
524288

fiPOOLE & ZHANG

 s the number of splits and, in parentheses, the number of different variables on which the splits
occur (different leaves can be split on the same variable).
 p the probability of splitting on a variable it is possible to split on.
 CBN size: the total size (summing over the size of the tables) of the contextual belief network
representation.
 BN size: the total size of the factors for the corresponding belief network (i.e., assuming the
probabilities are stored in tables).
 CVE time is the runtime (in msecs) of contextual variable elimination and CVE MTS is the
maximum sum of the table sizes created for the elimination of a single variable.
 VE time: the runtime (in msecs) of variable elimination and VE MTS is the maximum table
size created for the elimination of a single variable.

References
Becker, A. and Geiger, D. (1996). A sufficiently fast algorithm for finding close to optimal junction trees, in E. Horvitz and F. Jensen (eds), Proc. Twelfth Conf. on Uncertainty in Artificial
Intelligence (UAI-96), Portland, OR, pp. 8189.
Bertel, U. and Brioschi, F. (1972). Nonserial dynamic programming, Vol. 91 of Mathematics in
Science and Engineering, Academic Press.
Boutilier, C. (1997). Correlated action effects in decision theoretic regression, in Dan Geger and
Prakash Shenoy (ed.), Proceedings of the Thirteenth Annual Conference on Uncertainty in
Artificial Intelligence (UAI97), Providence, Rhode Island, pp. 3037.
Boutilier, C., Dearden, R. and Goldszmidt, M. (1995). Exploiting structure in policy construction,
Proc. 14th International Joint Conf. on Artificial Intelligence (IJCAI-95), Montral, Qubec,
pp. 11041111.
Boutilier, C., Friedman, N., Goldszmidt, M. and Koller, D. (1996). Context-specific independence
in Bayesian networks, in E. Horvitz and F. Jensen (eds), Proc. Twelfth Conf. on Uncertainty in
Artificial Intelligence (UAI-96), Portland, OR, pp. 115123.
Chickering, D. M., Heckerman, D. and Meek, C. (1997). A Bayesian approach to learning Bayesian
networks with local structure, Proc. Thirteenth Conf. on Uncertainty in Artificial Intelligence
(UAI-97), pp. 8089.
Dagum, P. and Luby, M. (1993). Approximating probabilistic inference in Bayesian belief networks
is NP-hard, Artificial Intelligence 60(1): 141153.
DAmbrosio (1995). Local expression languages for probabilistic dependence, International Journal
of Approximate Reasoning 13(1): 6181.
310

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

n

s

p

30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30

5 (3)
5 (2)
5 (2)
5 (2)
5 (1)
5 (2)
5 (2)
5 (1)
5 (3)
5 (2)
10 (3)
10 (3)
10 (2)
10 (3)
10 (2)
10 (2)
10 (3)
10 (2)
10 (2)
10 (3)
15 (2)
15 (3)
15 (3)
15 (2)
15 (3)
15 (4)
15 (3)
15 (3)
15 (3)
15 (2)

0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2
0.2

CBN
Size
1602
6108
17526
892
1540
1156
2344
1406
7740
4184
3038
888
3372
2240
1106
778
1888
9740
1102
4078
2710
1246
2046
1588
2260
2842
3074
1834
4362
3142

BN
Size
3658
7240
19632
4164
2640
5572
68676
7284
209652
8838
1119562
5176
4222408
30968
11660
4582
72260
413892
7744
298438
50698
84836
75956
138888
20230
67385366
533738
278426
209186
151160

CVE
Time
662
1583
6754
1604
261
608
12462
2197
17736
8053
30669
565
21834
20308
741
274
6659
11271
313
61140
8845
1935
54571
14280
522
322274
85480
18560
22872
4476

MTS
49152
131072
393216
81920
16512
40960
1048576
81920
851968
528384
1048576
24576
917504
524800
32768
18432
262144
868352
16384
1048576
524288
90112
1310720
458752
28672
10485760
2752512
753664
1441792
164096

VE
Time
604
1945
8833
3119
457
816
9370
4021
18279
16437
53732
2625
108732
129885
433
656
11986
179564
1395
79858
39265
3652
141386
43059
1815
726989
89431
43554
131704
26426

MTS
65536
131072
1048576
131072
32768
65536
1048576
262144
2097152
1048576
2097152
131072
4194304
4194304
65536
65536
524288
8388608
65536
4194304
2097152
262144
8388608
2097152
131072
33554432
8388608
1048576
4194304
1048576

Figure 18: Some of the details of data from Figure 12 (biased towards fewer different variables)

311

fiPOOLE & ZHANG

Darwiche, A. (1995). Conditioning algorithms for exact and approximate inference in causal networks, in P. Besnard and S. Hanks (ed.), Proc. Eleventh Conf. on Uncertainty in Artificial
Intelligence (UAI-95), Montreal, Quebec, pp. 99107.
Dearden, R. and Boutilier, C. (1997). Abstraction and approximate decision theoretic planning,
Artificial Intelligence 89(1): 219283.
Dechter, R. (1996). Bucket elimination: A unifying framework for probabilistic inference, in
E. Horvitz and F. Jensen (eds), Proc. Twelfth Conf. on Uncertainty in Artificial Intelligence
(UAI-96), Portland, OR, pp. 211219.
Dez, F. (1996). Local conditioning in Bayesian networks, Artificial Intelligence 87(12): 120.
Friedman, N. and Goldszmidt, M. (1996). Learning Bayesian networks with local structure, Proc.
Twelfth Conf. on Uncertainty in Artificial Intelligence (UAI-96), pp. 252262.
Geiger, D. and Heckerman, D. (1996). Knowledge representation and inference in similarity networks
and Bayesian multinets, Artificial Intelligence 82: 4574.
Guestrin, C., Venkataraman, S. and Koller, D. (2002). Context specific multiagent coordination and
planning with factored MDPs, The Eighteenth National Conference on Artificial Intelligence
(AAAI-2002), Edmonton, Canada.
Heckerman, D. and Breese, J. (1994). A new look at causal independence, Proc. of the Tenth
Conference on Uncertainty in Artificial Intelligence, pp. 286292.
Jensen, F. V., Kjrulff, U., Olesen, K. G. and Pedersen, J. (1989). Et forprojekt til et ekspertsystem
for drift af spildevandsrensning (an expert system for control of waste water treatment  a
pilot project), Technical report, Judex Datasystemer A/S, Aalborg, Denmark. In Danish.
Jensen, F. V., Lauritzen, S. L. and Olesen, K. G. (1990). Bayesian updating in causal probabilistic
networks by local computations, Computational Statistics Quarterly 4: 269282.
Kjrulff, U. (1990). Triangulation of graphs - algorithms giving small total state space, Technical
Report R 90-09, Department of Mathematics and Computer Science, Strandvejen, DK 9000
Aalborg, Denmark.
Lauritzen, S. L. and Spiegelhalter, D. J. (1988). Local computations with probabilities on graphical
structures and their application to expert systems, Journal of the Royal Statistical Society, Series
B 50(2): 157224.
Neal, R. (1992). Connectionist learning of belief networks, Artificial Intelligence 56: 71113.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference,
Morgan Kaufmann, San Mateo, CA.
Poole, D. (1993). Probabilistic Horn abduction and Bayesian networks, Artificial Intelligence
64(1): 81129.
312

fiEXPLOITING CONTEXTUAL INDEPENDENCE IN PROBABILISTIC INFERENCE

Poole, D. (1995). Exploiting the rule structure for decision making within the independent choice
logic, in P. Besnard and S. Hanks (eds), Proc. Eleventh Conf. on Uncertainty in Artificial
Intelligence (UAI-95), Montral, Qubec, pp. 454463.
Poole, D. (1997). Probabilistic partial evaluation: Exploiting rule structure in probabilistic inference, Proc. 15th International Joint Conf. on Artificial Intelligence (IJCAI-97), Nagoya, Japan,
pp. 12841291.
Poole, D. (1998). Context-specific approximation in probabilistic inference, in G.F. Cooper and
S. Moral (ed.), Proc. Fourteenth Conf. on Uncertainty in Artificial Intelligence, Madison, WI,
pp. 447454.
Saul, L., Jaakkola, T. and Jordan, M. (1996). Mean field theory for sigmoid belief networks, Journal
of Artificial Intelligence Research 4: 6176.
Shachter, R. D., DAmbrosio, B. D. and Del Favero, B. D. (1990). Symbolic probabilistic inference
in belief networks, Proc. 8th National Conference on Artificial Intelligence, MIT Press, Boston,
pp. 126131.
Smith, J. E., Holtzman, S. and Matheson, J. E. (1993). Structuring conditional relationships in
influence diagrams, Operations Research 41(2): 280297.
Tung, L. (2002). A clique tree algorithm exploiting context-specific independence, Masters thesis,
Department of Computer Science, University of British Columbia.
Zhang, N. L. (1998). Inference in Bayesian networks: The role of context-specific independence,
Technical Report HKUST-CS98-09, Department of Computer Science, Hong Kong University
of Science and Technology.
Zhang, N. L. and Poole, D. (1999). On the role of context-specific independence in probabilistic
reasoning, Proc. 16th International Joint Conf. on Artificial Intelligence (IJCAI-99), Stockholm,
Sweden, pp. 12881293.
Zhang, N. and Poole, D. (1994). A simple approach to Bayesian network computations, Proc. of the
Tenth Canadian Conference on Artificial Intelligence, pp. 171178.
Zhang, N. and Poole, D. (1996). Exploiting causal independence in Bayesian network inference,
Journal of Artificial Intelligence Research 5: 301328.

313

fiJournal of Artificial Intelligence Research 18 (2003) 117-147

Submitted 7/02; published 2/03

Translation of Pronominal Anaphora between English and
Spanish: Discrepancies and Evaluation
Jesus Peral

jperal@dlsi.ua.es

Dept. Lenguajes y Sistemas Informaticos
University of Alicante
Alicante, SPAIN

Antonio Ferrandez

antonio@dlsi.ua.es

Dept. Lenguajes y Sistemas Informaticos
University of Alicante
Alicante, SPAIN

Abstract
This paper evaluates the different tasks carried out in the translation of pronominal
anaphora in a machine translation (MT) system. The MT interlingua approach named
AGIR (Anaphora Generation with an Interlingua Representation) improves upon other
proposals presented to date because it is able to translate intersentential anaphors, detect co-reference chains, and translate Spanish zero pronouns into Englishissues hardly
considered by other systems. The paper presents the resolution and evaluation of these
anaphora problems in AGIR with the use of different kinds of knowledge (lexical, morphological, syntactic, and semantic). The translation of English and Spanish anaphoric
third-person personal pronouns (including Spanish zero pronouns) into the target language
has been evaluated on unrestricted corpora. We have obtained a precision of 80.4% and
84.8% in the translation of Spanish and English pronouns, respectively. Although we have
only studied the Spanish and English languages, our approach can be easily extended to
other languages such as Portuguese, Italian, or Japanese.

1. Introduction
The anaphora phenomenon can be considered one of the most difficult problems in natural
language processing (NLP). The etymology of the term anaphora originates with the Ancient
Greek word anaphora (o), which is made up of the separate words,  (back,
upstream, back in an upward direction) and o (the act of carrying), and which
denotes the act of carrying back upstream.
Presently, various definitions of the term anaphora exist, but the same concept underlies
all of them. Halliday & Hassan (1976) defined anaphora as the cohesion (presupposition)
which points back to some previous item. A more formal definition was proposed by Hirst
(1981), which defined anaphora as a device for making an abbreviated reference (containing
fewer bits of disambiguating information, rather than being lexically or phonetically shorter)
to some entity (or entities) in the expectation that the receiver of the discourse will be able
to disabbreviate the reference and, thereby, determine the identity of the entity. Hirst
refers to the entity as an anaphor, and the entity to which it refers is its antecedent:
 [Mary]i went to the cinema on Thursday. Shei didnt like the film.
c
2003
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiPeral & Ferrandez

In this example, the pronoun she is the anaphor and the noun phrase Mary is the
antecedent. This type of anaphora is the most common type, the so-called pronominal
anaphora.
The anaphora phenomenon can be further broken down into two processes: that of
resolution and generation. Resolution refers to the process of determining the antecedent
of an anaphor; generation is the process of creating references over a discourse entity.
In the context of machine translation, the resolution of anaphoric expressions is of crucial
importance in order to translate/generate them correctly into the target language (Mitkov
& Schmidt, 1998). Solving the anaphora and extracting the antecedent are key issues for
correct translation into the target language. For instance, when translating into languages
which mark the gender of pronouns, resolution of the anaphoric relation is essential. Unfortunately, the majority of MT systems do not deal with anaphora resolution, and their
successful operation usually does not go beyond the sentence level.
We have employed a computational system that focuses on anaphora resolution in order
to improve MT quality and have then measured the improvements. The SUPAR (Slot
Unification Parser for Anaphora Resolution) system is presented in the work of Ferrandez,
Palomar, & Moreno (1999). This system can deal with several kinds of anaphora with
good results. For example, the system resolves pronominal anaphora in Spanish with a
precision rate of 76.8% (Palomar et al., 2001); it resolves one-anaphora in Spanish dialogues
with a precision rate of 81.5% (Palomar & Martnez-Barco, 2001), and it resolves definite
descriptions in Spanish direct anaphora and bridging references with precision rates of 83.4%
and 63.3%, respectively (Munoz, Palomar, & Ferrandez, 2000). In the work presented here,
we have used an MT system exclusively for pronominal anaphora resolution and translation.
This kind of anaphora is not usually taken into account by most of the MT systems, and
therefore pronouns are usually translated incorrectly into the target language. Although
we have focused on pronominal anaphora, our approach can be easily extended to other
kinds of anaphora, such as one-anaphora or definite descriptions previously resolved by the
SUPAR system.
It is important to emphasize that in this work we only resolve and translate personal
pronouns in the third person whose antecedents appear before the anaphorthat is, an
anaphoric relation between the pronoun and the antecedent is established, and cataphoric
relations (in which the antecedent appears after the anaphor) are not taken into account.
This paper focuses on the evaluation of the different tasks carried out in our approach
that lead to the final task: the translation of the pronominal anaphora into the target
language. The main contributions of this work are a presentation and evaluation of the
multilingual anaphora resolution module (English and Spanish) and an exhaustive evaluation of the pronominal anaphora translation between these languages.
The paper is organized as follows: Section 2 shows the anaphora-resolution needs in MT
and the deficiencies of traditional MT systems to resolve this phenomenon conveniently.
Section 3 presents the analysis module of our approach. In Section 4, we identify and
evaluate the NLP problems related to pronominal anaphora resolved in our system. Section
5 presents the generation module of the system. In Section 6, the generation module
is evaluated in order to measure the efficiency of our proposal. Finally, we present our
conclusions.
118

fiTranslation of Pronominal Anaphora between English and Spanish

2. Anaphora Resolution and its Importance in MT
As noted earlier, anaphora resolution is of crucial importance in order to translate anaphoric
expressions correctly into a target language. Let us consider the sentences (Hutchins &
Somers, 1992):
1. [The monkey]i ate the banana because iti was hungry.
2. The monkey ate [the banana]i because iti was ripe.
3. The monkey ate the banana because it was tea-time.
In each sentence the pronoun it refers to something different: in sentence (1), it refers
to the monkey, in sentence (2) to the banana, and in sentence (3), to the abstract notion
of time. If we wish to translate these sentences into Spanish or German (languages which
mark the gender of pronouns), anaphora resolution will be absolutely essential since, in
these languages, pronouns take the gender of their antecedents. Therefore, in Spanish, we
would obtain the following pronouns: (1) este (in the masculine form since the antecedent
the monkeyis masculine), (2) esta (femininethe banana), and (3) an omitted pronoun
(since the second clause of the sentence is impersonal in Spanish and does not need any
subject). On the other hand, in German we would obtain: (1) er (masculine antecedent),
(2) sie (feminine antecedent), and (3) es (neutral).
Besides these problems, originated by the gender of anaphoric expressions in different
languages, there are other differences (that we have named discrepancies) which influence
the process of translation of these expressions. These discrepancies have been previously
studied by other authors. Mitkov & Schmidt (1998) present several problems to be taken
into account in the translation of pronominal anaphors between different languages (German, French, English, Malay, and Korean); Nakaiwa & Ikeara (1992) treat the problem of
the translation of elliptical constructions in a JapaneseEnglish MT system; and Mitkov et
al. (1994) and Geldbach (1999) present the discrepancies in an EnglishKorean MT system
and a RussianGerman MT system, respectively.
Another difference between languages is that of number discrepancies, in which certain
nouns are referred to by a singular pronoun in one language and by a plural noun in the
other. For example, the word people is plural in English, whereas in Spanish or German it
is singular. Hence, in translations from English to Spanish, or from English to German, the
plural pronoun will become a singular pronoun.
On the other hand, although in the majority of cases language-pairs pronouns in the
source language are translated by target-language pronouns that correspond to the antecedent of the anaphor, there are some exceptions. In most of these cases, pronominal
anaphors are simply omitted in the target language. For instance, in translations from
English to Spanish, pronouns are frequently not translated because of the typical Spanish
elliptical zero-subject construction. Other languages with typical zero constructions are
Japanese, Italian, Thai, or Chinese.
In some languages, however, the pronoun is directly translated by its antecedent. For
example, in EnglishMalay translations there is a tendency to replace the it pronoun with
its antecedent, in which case the translator must first be able to identify the antecedent.
119

fiPeral & Ferrandez

Some languages translate pronouns into different expressions, depending on the syntactic
and semantic information of the antecedent. For example, in EnglishKorean translation
pronouns can be elliptically omitted, or they can be translated into definite noun phrases,
into their antecedent, or into different Korean pronouns.
All the above-mentioned problems in the translation of anaphoric expressions into a
target language show that it is very important to carry out a detailed analysis of these
expressions (including their resolution and the identification of the antecedent).
Because the majority of MT systems only handle one-sentence input, they usually cannot
deal with anaphora resolution, and if they do, their successful operation usually does not go
beyond the sentence level. In order to assess the deficiencies of MT systems, we analyzed the
characteristics of different MT systems, with an emphasis on those characteristics related
to anaphora resolution and translation into a target language. An overview of our analysis
can be seen in Table 1.
MT system
Systran
Meteo
SUSY
Ariane
Eurotra
METAL
Candide
Inter-Nostrum
IXA
Episteme
KANT
DLT
DLT (BKB)
Rosetta
CREST
kosmos
a.
b.
c.
d.
e.
f.

Strategya
Direct
Direct
Transfer
Transfer
Transfer
Transfer
Transfer
Transfer
Transfer
Transfer
Interlingua
Interlingua
Interlingua
Interlingua
Interlingua
Interlingua

Restrictb
No
Yes
No
No
No
No
Yes
Yes
No
No
Yes
No
No
No
Yes
Yes

Partialc
Yes
No
No
No
No
Yes
No
Yes
Yes
No
No
No
No
No
No
No

Anaphord
Yes
No
Yes
Yes
No
Yes
No
No
No
No
Yes
No
Yes
No
Yes
No

Corefere
No
No
No
No
No
No
No
No
No
No
No
No
No
No
Yes
No

Zerof
Yes
No
No
Yes
No
Yes
No
No
No
No
Yes
No
No
No
Yes
No

Strategy of translation: direct, transfer, or interlingua.
Restricted domain.
Partial parsing.
Resolution of intersentential anaphora.
Identification of co-reference chains.
Translation of zero pronouns into the target language.

Table 1: Characteristics of main MT systems
The table reflects a number of different system characteristics.
1. MT system. The MT systems studied included Systran (Toma, 1977; Wheeler,
1987); Meteo (Chandioux, 1976, 1989); SUSY (Maas, 1977, 1987); Ariane (Boitet &
Nedobejkine, 1981; Boitet, 1989); Eurotra (Varile & Lau, 1988; Allegranza, Krauwer,
& Steiner, 1991); METAL (Bennet & Slocum, 1985; Thurmair, 1990); Candide (Berger
120

fiTranslation of Pronominal Anaphora between English and Spanish

et al., 1994); Inter-Nostrum (Canals-Marote et al., 2001a, 2001b); IXA (Daz-Ilarraza,
Mayor, & Sarasola, 2000, 2001); Episteme (Amores & Quesada, 1997; Quesada &
Amores, 2000); KANT (Goodman, 1989; Nirenburg, 1989; Mitamura, Nyberg, & Carbonell, 1991); DLT (Witkam, 1983; Schubert, 1986); DLT with Bilingual Knowledge
Bank (BKB) (Sadler, 1989); Rosetta (Appelo & Landsbergen, 1986; Landsbergen,
1987); CREST (Farwell & Helmreich, 2000); and kosmos (Mahesh & Nirenburg,
1995a, 1995b).
2. Strategy of translation. This characteristic indicates the strategy used by the MT system in accordance with the existence of intermediate representations: direct, transfer,
or interlingua.
3. Restricted domain. This characteristic tells if the texts of the source language are of
a specific domain (restricted domain).
4. Partial parsing. This characteristic indicates if the MT system carries out a partial parsing of the source text by identifying only some constituents (noun phrases,
prepositional phrases, etc.) and some relations between them.
5. Resolution of intersentential anaphora. This characteristic indicates whether the MT
system resolves intersentential anaphora. If it does not, then the anaphoric expressions
that have their antecedents in previous sentences will be incorrectly translated into
the target language, in most of cases.
6. Identification of co-reference chains. This characteristic tells us if the co-reference
chains of the source text are identified after resolving intersentential anaphora.
7. Translation of zero pronouns. This characteristic indicates if the MT system detects
and resolves omitted pronouns (zero pronouns1 ) in the source language that are compulsory in the target language.
After analyzing the characteristics of the primary commercial MT systems, we concluded
that there is no MT system that can work on unrestricted texts, resolve intersentential
anaphora, identify the co-reference chains of the text, and translate zero pronouns into the
target language after carrying out a partial parsing of the source text.
Unlike other systems, such as the KANT interlingua system, the Meteo system, and the
Candide system, among others, that are designed for well-defined domains, our interlingua
MT approach, called AGIR (Anaphora Generation with an Interlingua Representation),
works on unrestricted texts. Although we could have applied full parsing to these texts, we
have instead utilized partial parsing, due to the unavoidable incompleteness of the grammar.
This is a main difference between our system and other interlingua systems, such as the
DLT system (which is based on a modification of Esperanto), the Rosetta system (which
experiments with Montague semantics as the basis for an interlingua), the KANT system,
and others.
After parsing and solving pronominal anaphora, an interlingua representation of the
entire text is obtained. To do this, sentences are split into clauses, and a complex feature
1. This kind of pronouns will be presented in detail in Section 4.1.

121

fiPeral & Ferrandez

structure based on semantic roles (agent, theme, etc.) is generated for each one. For each
clause, the different semantic roles that appear will be identified and linked with one entity
of the text. If there is an anaphor in the text, it will be linked with the entity that represents
its antecedent. The AGIRs interlingua representation has been presented in more detail in
(Peral, Palomar, & Ferrandez, 1999; Peral & Ferrandez, 2000a).
From the interlingua representation, the translation of the anaphor (including the intersentential anaphor), the detection of co-reference chains of the whole text, and the translation of Spanish zero pronouns into English have been carried out. AGIR has been designed to deal with all these issues which are hardly considered by most of the real MT
systems. Furthermore, our approach can be used for other applications, for example, for
cross-language information retrieval, summarization, etc.
It is important to note that although some of the above-mentioned MT systems resolve
different problems, such as zero pronouns or pronominal anaphora, their results are not very
satisfactory. Furthermore, we present some examples (extracted from the corpora used
in the evaluation of our approachsee Section 4) of incorrect SpanishEnglishSpanish
translations of pronouns done by Systran2 that AGIR does correctly3 :
 (S) Siempre cre que a lo que yo aspiraba era a la comunicacion perfecta con un
hombre, o, mejor dicho, con el hombre, con ese prncipe azul de los suenos de infancia,
un ser que sabra adivinarme hasta en los mas menudos pliegues interiores. Ahora he
aprendido no solo que [esa fusion]i es imposible, sino ademas que i es probablemente
indeseable.
 (E) I always thought that to which I aspired I was to the perfect communication
with a man, or, rather, with the man, that blue prince of the childhood dreams, a
being who would know to guess to me until in slightest you fold interiors. Now I have
learned not only that that fusion is impossible, but in addition that he is probably
undesirable.
In this example, Systran incorrectly translates into English the zero pronoun of the
last sentence of the paragraph, proposing the pronoun he instead of the pronoun it (the
antecedent is the noun phrase esa fusionthat fusion). Our system proposed the correct
pronoun. It is important to note that although the zero pronoun is identified by Systran,
it is incorrectly solved and subsequently incorrectly translated.
 (S) Al pasar de [la luminosidad]i de la calle, y tal vez por contraste con ellai , impresionaba la oscuridad interior y el vaco de la Catedral, en la que apenas haba a la
vista cuatro o cinco personas.
 (E) When happening of the luminosity of the street, and perhaps in contrast with
her, it impressed the inner dark and the emptiness of the Cathedral, in which as soon
as there were at sight four or five people.
2. A free trial of the commercial product SYSTRANLinks (copyright 2002 by SYSTRAN S.A.) has been
used to translate between the English and Spanish languages all the corpora used in the evaluation of
our approach. (URL = http://w4.systranlinks.com/config, visited on 06/22/2002).
3. In this paper, we have used the symbols (S) and (E) to represent Spanish and English texts, respectively.
The symbol  indicates the presence of the omitted pronoun. In the examples, the pronoun and the
antecedent have an index; co-indexing indicates co-reference between them.

122

fiTranslation of Pronominal Anaphora between English and Spanish

In this case, Systran incorrectly translates into English the pronoun ella (with antecedent
la luminosidad the luminosity), by proposing the pronoun her instead of it.
 (E) If you have not already done so, unpack [your printer]i and the accessory kit that
came with iti .
 (S) Si usted no ha hecho ya as pues, desempaquete su impresora y el kit de accesorios
eso vino con el.
This example shows an incorrect EnglishSpanish translation of the pronoun it. In this
case, the pronoun is incorrectly solved (the antecedent is the noun phrase your printer,
feminine) and then it is incorrectly translated (pronoun el masculineinstead of pronoun
estafeminine).
All the above examples illustrate that the translation of pronouns could be notably
improved if their antecedents were correctly identified and, subsequently, pronouns were
translated into the target language.

3. AGIRs Analysis Module
AGIR system architecture is based on the general architecture of an MT system that uses an
interlingua strategy. Translation is carried out in two stages: (1) from the source language
to the interlingua, and (2) from the interlingua into the target language. Modules for
analysis are independent from modules for generation. Although our present work has only
studied the Spanish and English languages, our approach can be easily extended to other
languages, for exampe, to multilingual system, in the sense that any analysis module can
be linked to any generation module.
In AGIR the analysis is carried out using SUPAR (slot unification parser for anaphora
resolution) (Ferrandez et al., 1999). SUPAR is a computational system that focuses on
anaphora resolution. It can deal with several kinds of anaphora, such as pronominal
anaphora, one-anaphora, or definite descriptions4 . The SUPARs input is a grammar defined by means of the grammatical formalism SUG (slot unification grammar ). A translator
that transforms SUG rules into Prolog clauses has been developed. This translator provides
a Prolog program that will parse each sentence. SUPAR can perform either a full or a partial parsing of the text with the same parser and grammar. In this study, partial-parsing
techniques have been utilized due to the unavoidable incompleteness of the grammar and
the use of unrestricted texts (corpora) as input.
The analysis of the source text is carried out in several steps. The first step of the
analysis module is the lexical and morphological analysis of the input text. Because of
the use of unrestricted texts as input, the system obtains the lexical and morphological
information of the texts lexical units from the output of a part-of-speech (POS) tagger.
The word as it appears in the corpus, its lemma, and its POS tag (with morphological
information) is supplied for each lexical unit in the corpus.
4. One-anaphora has the following structure in English: a determiner and the pronoun one with some
premodifiers or postmodifiers (the red one; the one with the blue bow ). This kind of anaphors in Spanish
consists of noun phrases in which the noun has been omitted (el rojo; el que tiene el lazo azul ). In
definite descriptions, anaphors are formed by definite noun phrases that refer to objects that are usually
uniquely determined in the context.

123

fiPeral & Ferrandez

The next step is the parsing of the text (which includes the lexical and morphological information extracted in the previous stage). Before applying the parsing, the text is
split into sentences. The output will be the slot structure (SS) that stores the necessary
information5 for the subsequent stages.
In the third step, a module of word-sense disambiguation (WSD) is used to obtain a
single sense for the different texts lexical units. The lexical resources, WordNet (Miller,
Beckwith, Fellbaum, Gross, & Miller, 1990) and EuroWordNet (Vossen, 1998), have been
used in this stage6 .
The SS, enriched with the information from the previous steps, will be the input for the
next step, in which NLP problems (anaphora, extraposition, ellipsis, etc.) will be treated
and solved. In this work, we have focused on the resolution of NLP problems related to
pronominal anaphora. After this step, a new slot structure (SS) is obtained. In this new
structure, the correct antecedentchosen from the possible candidates after applying a
method based on constraints and preferences (Ferrandez et al., 1999)for each anaphoric
expression will be stored along with its morphological and semantic information. The new
structure SS will be the input for the final step of the analysis module.
In the last step, AGIR generates the interlingua representation of the entire text. This
is the main difference between AGIR and other MT systems, which process the input text
sentence by sentence. The interlingua representation will allow the correct translation of
the intersentential and intrasentential pronominal anaphora into the target language. Moreover, AGIR allows the identification of co-reference chains of the text and their subsequent
translation into the target language.
The interlingua representation of the input text is based on the clause as the main unit
of this representation. Once the text has been split into clauses, AGIR uses a complex
feature structure for each clause. This structure is composed of semantic roles and features
extracted from the SS of the clause. The notation we have used is based on the one used
in KANT interlingua.
It is important to emphasize that the interlingua lexical unit has been represented in
AGIR using the word and its correct sense in WordNet. After accessing the ILI (interlingual-index) module of EuroWordNet, we will be able to generate the lexical unit into the
target language.
Once the semantic roles have been identified, the interlingua representation will store
the clauses with their features, the different entities that have appeared in the text and the
relations between them (such as anaphoric relations). This representation will be the input
for the generation module.
5. The SS stores the following information for each constituent: constituent name (NP, PP, etc.), semantic
and morphological information, discourse marker (identifier of the entity or discourse object), and the
SS of its subconstituents.
6. In the evaluation of our approach, we have only used an English corpus (SemCor) where all content words
are annotated with their WordNet sense; this sense has been used to identify the semantic category of the
word. The remaining corpora do not have information about the senses of the content words; therefore, a
set of heuristics has been used to identify their semantic categories. Currently, a WSD module (Montoyo
& Palomar, 2000) is being developed in our Research Group, which will be incorporated into our system
in the future.

124

fiTranslation of Pronominal Anaphora between English and Spanish

4. Resolution of NLP Problems in AGIR
The fourth stage of the AGIRs analysis module allows for the resolution of NLP problems. Our present work focuses on the resolution of NLP problems related to pronominal
anaphora in the source language so as to translate these anaphoric expressions correctly
into the target language. We are only describing the translation of anaphoric, third-person,
personal pronouns into the target language. Therefore, we have only focused on the discrepancies between Spanish and English in the treatment of these pronouns. In the next
two subsections, we will describe the syntactic discrepancies treated and solved in AGIR
(Spanish zero pronouns) and the anaphora resolution module of the system.
4.1 Elliptical Zero-Subject Constructions (Zero Pronouns)
The Spanish language allows for the omission of the pronominal subject of the sentences.
These omitted pronouns are usually called zero pronouns. Whereas in other languages (e.g.,
in Japanese), zero pronouns may appear in either the subjects or the objects grammatical
position, in Spanish texts zero pronouns only appear in the position of the subject.
In MT systems, the correct detection and resolution of zero pronouns in the source
language is of crucial importance if these pronouns are compulsory in the target language.
In the following example, a Spanish sentence that contains a zero pronoun and its translation
into English with the equivalent compulsory pronoun are shown.
 (S) [Ese hombre]i era un boxeador profesional. i Perdio unicamente dos combates.
 (E) [That man]i was a professional boxer. Hei only lost two fights.
We should remark that zero pronouns can also occur in English, although they appear
less frequently, since they usually are used in coordinated sentences in which the zero pronoun usually refers to the subject of the sentence. Although zero pronouns have already
been studied in other languages, such as Japanesewith a resolution percentage of 78% in
the work of (Okumura & Tamura, 1996), they have not yet been studied in Spanish texts.
(Ferrandez & Peral, 2000) has presented the first algorithm for Spanish zero-pronoun resolution. Basically, in order to translate Spanish zero pronouns into English, they must first
be located in the text (ellipsis detection) and then resolved (anaphora resolution) (Peral &
Ferrandez, 2000b):
 Zero-pronoun detection. In order to detect zero pronouns, sentences should be divided
into clauses, since the subject can only appear between the clause constituents. After
that, a noun-phrase (NP) or a pronoun is sought, for each clause, through the clause
constituents on the lefthand side of the verb, unless it is imperative or impersonal.
Such an NP or pronoun must agree in person and number with the verb of the clause.
 Zero-pronoun resolution. After the zero pronoun has been detected, our computational
system inserts the pronoun in the position in which it has been omitted. This pronoun
will be detected and resolved in the following module of anaphora resolution. Person
and number information is obtained from the clause verb. Sometimes, in Spanish, the
gender information of the pronoun can be obtained from the object when the verb
125

fiPeral & Ferrandez

is copulative. In these cases, the subject must agree in gender and number with its
object whenever the object can have either a masculine or feminine linguistic form.
4.1.1 Evaluation
To evaluate this task, two experiments were performed: an evaluation of zero-pronoun
detection and an evaluation of zero-pronoun resolution. In both experiments the method
was tested on two kinds of corpora. In the first instance, we used a portion of the LEXESP7
corpus that contains a set of thirty-one documents (38,999 words) from different genres and
written by different authors. The LEXESP corpus contains texts of different styles and
on different topics (newspaper articles about politics, sports, etc.; narratives about specific
topics; novel fragments; etc.). In the second instance, the method was tested on a fragment
of the Spanish version of Blue Book (BB) corpus (15,571 words), a technical manual that
contains the handbook of the International Telecommunications Union (CCITT) published
in English, French, and Spanish. Both corpora are automatically tagged by different taggers.
We randomly selected a subset of the LEXESP corpus (three documents 6,457 words)
and a fragment of the Blue Book corpus (4,723 words) as training corpora. The remaining
fragments of the corpora were reserved for test data.
It is important to emphasize that all the tasks presented in this paper were automatically
evaluated after the annotation of each pronoun (including zero pronouns). To do so, each
anaphoric, third-person, personal pronoun was annotated with the information about its
antecedent and its translation into the target language. Furthermore, co-reference chains
were identified. The annotation phase was accomplished in the following manner: (1)
two annotators (native speakers) were selected for each language, (2) an agreement was
reached between the annotators with regard to the annotation scheme, (3) each annotator
annotated the corpora, and (4) a reliability test (Carletta et al., 1997) was done on the
annotation in order to guarantee the results. The reliability test used the kappa statistic
that measures agreement between the annotations of two annotators in making judgments
about categories. In this way, the annotation is considered a classification task consisting of
defining an adequate solution among the candidate list. According to Carletta et al. (1997),
a k measurement such as 0.68 < k < 0.80 allows us to draw encouraging conclusions, and
a measurement k > 0.80 means there is total reliability between the results of the two
annotators. In our tests, we obtained a kappa measurement of 0.83. Therefore, we consider
the annotation obtained for the evaluation to be totally reliable.
4.1.2 Evaluation of Zero-Pronoun Detection
In the evaluation of zero-pronoun detection, the training phase was used to carry out modifications in the grammar in order to improve the processes of partial parsing and clause
splitting. After this training, we conducted a blind test over the entire test corpus. To
achieve this sort of evaluation, several different subtasks may be considered. First, each
verb must be detected. This task is easily accomplished since both corpora have been pre7. The LEXESP corpus belongs to the project of the same name, carried out by the Psychology Department
of the University of Oviedo and developed by the Computational Linguistics Group of the University
of Barcelona, with the collaboration of the Language Processing Group of the Catalonia University of
Technology, Spain.

126

fiTranslation of Pronominal Anaphora between English and Spanish

viously tagged. The second task is to classify the verbs into two categories: (a) verbs whose
subjects have been omitted, and (b) verbs whose subjects have not. The obtained results
with the LEXESP and Blue Book corpora appear in Table 2.

LX
BB

1st
240
0

Verbs with subject omitted
Verbs with subject not omitted
P(%) 2nd P(%)
3rd
P(%) 1st P(%) 2nd P(%)
3rd
P(%)
96.7
54
98.1 1,227 97.1 31
71
17
94.1 1,085 83.3
PRECISION = 97.1%
PRECISION = 83.1%
0
0
0
121
97.5
0
0
0
0
351
82
PRECISION = 97.5%
PRECISION = 82.0%
GLOBAL PRECISION = 90.4%
Table 2: Zero-pronoun detection, evaluation phase

The table is divided into two parts, corresponding to categories (a) and (b) previously
mentioned. For each category, the number of verbs in first, second, and third person,
together with their precision (P), are represented. Precision was defined as the number
of verbs correctly classified (subject omitted or not) divided by the total number of verb
classifications attempted for each type. For example, in the LEXESP corpus 1,227 verbs in
the third person with their subjects omitted were classified, and the precision obtained was
97.1%.
Discussion. In the detection of zero pronouns the following results were obtained: for
the LEXESP corpus, precisions of 97.1% and 83.1% were obtained for verbs whose subjects
were omitted or were not omitted, respectively; for the BB corpus, precisions of 97.5% and
82% were obtained. For both corpora, an overall precision of 90.4% (2,825 out of a total of
3,126) was obtained for this task.
From these results, we have extracted the following conclusions:
 There are no meaningful differences between the results obtained with each corpus.
 The BB corpus has no verbs in either the first or second person. This is explained by
considering the style of the corpus: it is a technical manual which usually consists of
a series of isolated definitions done by the writer.
 The rate of precision for the detection of verbs whose subjects are not omitted is
lower (approximately 15%) than for the detection of verbs whose subjects are omitted.
There are several reasons for this:
 The POS tagger does not identify impersonal verbs. This problem has been
partly resolved heuristically, by the choice of impersonal verbs (e.g., llover to
rain), but it cannot be resolved for all impersonal verbs. For example, the verb
ser (to be) is not usually impersonal, but it can be in certain constructions (e.g.,
Es hora de desayunar It is time to have breakfast).
 The ambiguity and the unavoidable incompleteness of the grammar affects the
process of clause splitting, and therefore affects the detection of the possible
subject for the clause on the lefthand side of the verb.
127

fiPeral & Ferrandez

Since ours is the first study done specifically on Spanish texts and since the design of
the detection stage mainly depends upon the structure of the language in question, we have
not compared our results with those of other published works. Such comparisons would
prove to be insignificant8 .
Finally, it is important to emphasize the importance of this phenomenon in Spanish.
Specifically, in both corpora, the subject is omitted in 52.5% (1,642 out of 3,126) of the
verbs. Furthermore, this phenomenon is even more important in narrative texts (57.3%
in the LEXESP corpus) than in the technical manuals (25.6% in the BB corpus). These
percentages show the importance of correctly detecting these kinds of pronouns in an MT
system so as to conveniently translate them into the target language.
4.1.3 Evaluation of Zero-Pronoun Resolution
After zero pronouns have been detected, they are then resolved in the subsequent module
of anaphora resolution (explained in the following subsection). Basically, an algorithm that
combines different kinds of knowledge by distinguishing between constraints and preferences
is used (Ferrandez et al., 1999; Palomar et al., 2001).
The set of constraints and preferences presents two basic differences between zeropronoun and pronominal anaphora resolution:
1. Zero-pronoun resolution has the constraint of agreement only in person and number,
whereas pronominal anaphora resolution also requires gender agreement.
2. Two new preferences to solve zero pronouns are used: (a) preference is given to
candidates in the same sentence as the anaphor that have also been the solution of a
zero pronoun in the same sentence as the anaphor, and (b) in the case where the zero
pronoun has gender information, preference is given to those candidates that agree in
gender.
In evaluating zero-pronoun resolution so as to obtain the best order of preferences (one
that produces the best performance), we used the training phase to identify the importance
of each kind of knowledge. To do this, we analyzed the antecedent for each pronoun in the
training corpora, and we identified their configurational characteristics with reference to
the pronoun (e.g., if the antecedent was a proper noun, if the antecedent was an indefinite
NP, if the antecedent occupied the same position with reference to the verb as the anaphor
before or after, etc.). Subsequently, we constructed a table that showed how often each
configurational characteristic was valid for the solution of a particular pronoun (e.g., the
solution of a zero pronoun was a proper noun 63% of the time, for a reflexive pronoun, it
was a proper noun 53% of the time, etc.). In this way, we were able to define the different
patterns of Spanish pronoun resolution and apply them in order to obtain the evaluation
results that are presented in this paper. The order of importance was determined by first
sorting the preferences according to the percentage of each configurational characteristic;
8. In order to compare our system with other systems, in Section 6.2 we evaluate pronoun translation
(including zero pronouns) between Spanish and English using the commercial product SYSTRANLinks
and the Spanish LEXESP corpus. The evaluation highlights the deficiencies of zero-pronoun detection,
resolution, and translation (out of 559 anaphoric, third-person, zero pronouns in the LEXESP corpus,
only 266 were correctly translated into Englisha precision of only 47.6%).

128

fiTranslation of Pronominal Anaphora between English and Spanish

that is, preferences with higher percentages were considered more important than those with
lower percentages. After several experiments on the training corpora, an optimal order for
each type of anaphora was obtained. Since in this phase we processed texts from different
genres and by different authors, we can state that the final set of preferences obtained and
their order of application can be used with confidence on any Spanish text.
After the training, we conducted a blind test over the entire test corpus, the results for
which are shown in Table 3.
Cataphoric
LEXESP
BB
TOTAL

640
76
716

Exophoric
28
8
36

Anaphoric
Correct Total
455
559
30
37
485
596

P(%)
81.4
81.1
81.4

Table 3: Zero-pronoun resolution, evaluation phase
It is important to mention here that out of 3,126 verbs in these corpora, 1,348 (Table
2) are zero pronouns in the third person and will be resolved. In Table 3 we present a
classification of these third-person zero pronouns, which has been conveniently divided into
three categories:
1. Cataphoric. This category is comprised of those zero pronouns whose antecedents,
that is, the clause subjects, come after the verb. For instance, in the following Spanish
sentence i Compro [un nino]i en el supermercado ([A boy]i bought in the supermarket), the subject, un nino (a boy), appears after the verb, compro (bought). These
kinds of verbs are quite common in Spanish (P = 53.1%, 716 out of 1,348), as can be
seen in Table 3, and represents one of the main difficulties in resolving anaphora in
Spanish: the structure of a sentence is more flexible than in English. These represent
intonationally marked sentences, where the subject does not occupy its usual position
in the sentence, that is, before the verb. Cataphoric zero pronouns will not be resolved in AGIR, since semantic information is needed to be able to discard all of their
antecedents and to give preference to those that appear within the same sentence and
clause after the verb.
For example, the sentence  Compro un regalo en el supermercado ([He] bought a
present in the supermarket) has the same syntactic structure as the previous sentence,
i.e., verb, NP, and PP, where the object function of the NP can only be distinguished
from the subject by means of semantic knowledge.
2. Exophoric. This category consists of those zero pronouns whose antecedents do not
appear linguistically in the text (they refer to items in the external world rather than
things referred to in the text). Exophoric zero pronouns will not be resolved by the
system.
3. Anaphoric. This category is comprised of those zero pronouns whose antecedents are
found before the verb. These kinds of pronouns will be resolved by our system.
129

fiPeral & Ferrandez

In Table 3 the numbers of cataphoric, exophoric, and anaphoric zero pronouns for each
corpus are shown. For anaphoric pronouns, the number of pronouns correctly solved as
well as the obtained precision, P (number of pronouns correctly solved divided by the
number of solved pronouns) is presented. For example, in the LEXESP corpus, there are
640 cataphoric, 28 exophoric, and 559 anaphoric zero pronouns. From these anaphoric
pronouns, only 455 were correctly solved, giving a precision of 81.4%.
Discussion. In zero-pronoun resolution, the following results have been obtained: LEXESP corpus, P = 81.4%; BB corpus, P = 81.1%. For the combined corpora, an overall
precision for this task of 81.4% (485 out of 596) was obtained. The overall recall, R (the
number of pronouns correctly solved divided by the number of real pronouns) obtained was
79.1% (485 out of 613).
From these results, we have extracted the following conclusions:
 There are no meaningful differences between the results obtained from each corpus.
 Errors in the zero-pronoun-resolution stage are originated by different causes:
 exceptions in the application of preferences that imply the selection of an incorrect antecedent as solution of the zero pronoun (64% of the global mistakes)
 the lack of semantic information9 , causing an error rate of 32.4%
 mistakes in the POS tagging (3.6%)
Since the results provided by other works have been obtained for different languages
(English), texts, and sorts of knowledge (e.g., Hobbs and Lappin full parse the text), direct
comparisons are not possible. Therefore, in order to accomplish this comparison, we have
implemented some of these approaches in SUPAR10 , adapting them for partial parsing and
Spanish texts. Although these approaches were not proposed for zero pronouns and the
comparison will not be fully fair, we have implemented them since that is the only way to
compare our proposal directly with some well-known anaphora-resolution algorithms.
We have also compared our system with the typical baseline of proximity preference
(i.e., the antecedent that appears closest to the anaphora is chosen from among those that
satisfy the constraintsmorphological agreement and syntactic conditions). We have also
compared our system with the baseline presented by Hobbs (1978)11 and Lappin & Leass
method (Lappin & Leass, 1994). Moreover, we also compared our proposal with centering
approach by implementing functional centering (Strube & Hahn, 1999). The precisions
obtained with these different approaches and AGIR are shown in Table 4. As can be seen,
the precision obtained in AGIR is better than those obtained using the other proposals.
9. It is important to mention here that semantic information was not available for the Spanish corpora.
10. A detailed study of these implementations in SUPAR is presented in Palomar et al. (2001).
11. Hobbss baseline is frequently used to compare most of the work accomplished on anaphora resolution.
Hobbss algorithm does not work as well as ours because it carries out a full parsing of the text. Furthermore, the manner in which the syntactic tree is explored using Hobbss algorithm is not the best one
for Spanish, since it is nearly a free-word-order language.

130

fiTranslation of Pronominal Anaphora between English and Spanish

LEXESP
BB

Proximity
54.9
48.6

Hobbs
60.4
62.2

Lappin
66.0
67.6

Strube
59.7
59.5

AGIR
81.4
81.1

Table 4: Zero-pronoun resolution in Spanish, comparison of AGIR with other approaches

4.2 The Anaphora-Resolution Module
The anaphora-resolution module used in AGIR is based on the module presented in (Ferrandez et al., 1999; Palomar et al., 2001) for the SUPAR system. The algorithm identifies
noun phrase (NP) antecedents of personal, demonstrative, reflexive, and zero pronouns in
Spanish. It identifies both intrasentential and intersentential antecedents and is applied to
the syntactic analysis generated by SUPAR. It also combines different forms of knowledge
by distinguishing between constraints and preferences. Whereas constraints are used as
combinations of several kinds of knowledge (lexical, morphological, and syntactic), preferences are defined as a combination of heuristic rules extracted from a study of different
corpora.
A constraint defines a property that must be satisfied in order for any candidate to be
considered as a possible solution of the anaphor. The constraints used in the algorithm
are the following: morphological agreement (person, gender, and number) and syntactic
conditions on NP-pronoun non-co-reference.
A preference is a characteristic that is not always satisfied by the solution of an anaphor.
The application of preferences usually involves the use of heuristic rules in order to obtain a
ranked list of candidates. Some examples of preferences used in our system are the following:
(a) antecedents that are in the same sentence as the anaphor, (b) antecedents that have
been repeated more than once in the text, (c) antecedents that appear before their verbs
(i.e., the verb of the clause in which the antecedent appears), (d) antecedents that are
proper nouns, (e) antecedents that are an indefinite NP, and so on.
In order to solve pronominal anaphors, they must be first located in the text (anaphora
detection) and then resolved (anaphora resolution):

 Anaphora detection. In the algorithm, all the types of anaphors are identified from
left to right as they appear in the sentences slot structure obtained after the partial
parsing. To identify each type of pronoun, the information stored in the POS tags
has been used. In the particular case of zero pronouns, they have been detected in a
previous stage, as previously described.
 Anaphora resolution. After the anaphor has been detected, the corresponding method,
based on constraints and preferences, is applied to solve it. Each type of anaphor has
its own set of constraints and preferences, although they all follow the same general
algorithm: constraints are applied first, followed by preferences. Constraints discard
some of the candidates, whereas preferences simply sort the remaining candidates.
131

fiPeral & Ferrandez

4.2.1 Evaluation
In evaluating the algorithm for anaphora resolution12 , we looked at pronominal anaphora
resolution in Spanish and English, respectively. For the Spanish evaluation, the method
was tested on the portion of the LEXESP corpus previously used to evaluate zero-pronoun
detection and resolution. For English, we tested the method on two kinds of corpora. In the
first instance, we used a portion of the SemCor collectionpresented in (Landes, Leacock,
& Tengi, 1998)which contains a set of eleven documents (23,788 words) in which all
content words are annotated with the most appropriate WordNet sense. The SemCor corpus
contains texts about different topics (law, sports, religion, nature, etc.) and was written
by different authors. In the second instance, the method was tested on a portion of the
MTI13 corpus, which contains seven documents (101,843 words). The MTI corpus contains
computer science manuals on different topics (commercial applications, word processing
applications, device instructions, etc.). Both English corpora are automatically tagged by
different taggers.
We randomly selected a subset of the SemCor corpus (three documents6,473 words)
and another subset of the MTI corpus (two documents24,264 words) as training corpus.
The remaining fragments of the corpora were reserved for test data.
In the two tasks, the training phase was used to identify the importance of each kind of
knowledge to obtain the optimal order of the preferences.
4.2.2 Evaluation of Anaphora Resolution in Spanish
An evaluation of the algorithm for anaphora resolution in Spanish has been given in detail
in the work of Palomar et al. (2001). In this paper, we present the obtained results of the
evaluation of this task in AGIR over a different portion of the LEXESP corpus. Furthermore,
non-anaphoric complement pronouns, that is, complement pronouns that appear next to
the previous indirect object when it has been moved from its theoretical place after the
verb (A Pedroi lei vi ayer I saw Pedro yesterday), were not resolved because this kind of
pronoun does not appear in the English translation. For these reasons, the results of the
two works are slightly different.
After the training phase, the algorithm was evaluated over the test corpus. In this
evaluation, only lexical, morphological, and syntactic information was used. Table 5 shows
the results of this evaluation.

LEXESP

Comp

P(%)

Ref

P(%)

98

82.6

105

92.4

PP
notPP
71

P(%)
70.4

PP
inPP
46

P(%)

Total

76.1

320

P(%)
Total
82.2

Table 5: Anaphora resolution in Spanish, evaluation phase
12. As previously mentioned, only anaphoric, third-person, personal pronouns will be resolved in order to
translate them into the target language.
13. This corpus was provided by the Computational Linguistics Research Group of the School of Humanities,
Languages and Social Studies, University of Wolverhampton, England. The corpus is anaphorically
annotated indicating the anaphors and their correct antecedents.

132

fiTranslation of Pronominal Anaphora between English and Spanish

In Table 5 the occurrences of personal pronouns in the LEXESP corpus are shown. The
different types are: Comp (complement personal pronouns), Ref (reflexive pronouns), PPnotPP (personal pronouns not included in a prepositional phrase), and PPinPP (personal
pronouns included in a prepositional phrase). For each type, the obtained precision, P (the
number of pronouns correctly solved divided by the number of solved pronouns), is shown.
The last two columns represent the total number of personal pronouns and the obtained
precision.
Discussion. In pronominal anaphora resolution in Spanish, we obtained a precision of
82.2% (263 out of 320). The recall, R (number of pronouns correctly solved divided by the
number of real pronouns), obtained was of 79% (263 out of 333).
After analyzing the results, the following conclusions were extracted:
 In the resolution of reflexive pronouns, a high precision (92.4%) was obtained. This
higher percentage is because the antecedent of these pronouns is usually the closest NP
to the pronoun and it is in the same sentence. Therefore, after applying preferences,
few errors are produced.
 Analyzing the errors in the remaining pronouns, it is important to mention the complexity of the LEXESP corpus itself. It consists of several narrative documents, sometimes with a very complex style, with long sentences (with an average of 24.6 words
per sentence). This implies a large number of candidates per anaphor after applying
constraints (an average of 16.6).
 Errors were originated by different causes:
 exceptions in the application of preferences (66.7% of the global mistakes)
 a lack of semantic information (29.8%)
 mistakes in the POS tagging (3.5%)
We compared our proposal with the approaches previously presented in the evaluation
of zero-pronoun resolution. As shown in Table 6, the precision obtained using AGIR is
better than those for the other proposals.

LEXESP

Proximity
52.5

Hobbs
65.3

Lappin
73.3

Strube
68.3

AGIR
82.2

Table 6: Anaphora resolution in Spanish, comparison of AGIR with other approaches

4.2.3 Evaluation of Anaphora Resolution in English
The algorithm for anaphora resolution in English is based on the one developed for Spanish,
and it has been conveniently adapted for English. The main difference between the two
algorithms consists in a different order of the preferences obtained after the training phase.
After this phase, we extracted the following conclusions:
133

fiPeral & Ferrandez

 Spanish has more morphological information than English. As a consequence, morphological constraints in Spanish discard more candidates than constraints in English.
 Spanish is a nearly free-order language, in which the different constituents of a sentence
(subject, object, etc.) can appear almost at any position. For this reason, the preference of syntactic parallelism has a more important role in the anaphora-resolution
method in English than in Spanish.
 Spanish sentences are usually longer than English ones. This fact implies more candidates for Spanish anaphors than for English ones.
After the training phase, the algorithm was evaluated over the test corpus. In the
evaluation phase, two experiments were carried out. In the first experiment, only lexical,
morphological, and syntactic information was used. The obtained results with the SemCor
and MTI corpora appear in Table 7.
SEMCOR
MTI

He
116
1

She
10
0

It
38
347

They
50
56

Him
34
0

Her
0
0

Them
6
66

Corr
175
361

Total
254
470

P(%)
68.9
76.8

Table 7: Anaphora resolution in English, evaluation phase: experiment 1
The table shows the number of pronouns (classified by type) for the different corpora.
The last three columns represent the number of correctly solved pronouns, the total number
of pronouns, and the obtained precision, respectively. For instance, in the MTI corpus a
precision of 76.8% was obtained.
Discussion. In pronominal anaphora resolution in English, the following results were
obtained in the first experiment: SemCor corpus, P = 68.9%, R = 66%; MTI corpus, P =
76.8%, R = 72.9%.
From these results, we have extracted the following conclusions:
 The types of pronouns vary considerably according to the corpus. In the SemCor
corpus, 15% of the pronouns are occurrences of the it pronoun, whereas in the MTI
corpus this percentage is 73.8%. This fact is explained by the kind and domain of
each corpus. The SemCor is a corpus with a narrative style which contains a lot of
person entities14 that are referred to in the text with the use of personal pronouns
(he, she, and they). On the other hand, the MTI corpus is a collection of technical
manuals that contains almost no person entities. Rather, most references are to object
entities, using it pronouns.
 In the SemCor corpus, errors originated from different causes:
 The lack of semantic information caused 57% of the global mistakes. There were
seventeen mistakes in the resolution of it pronouns, in which the system proposed
14. If we use a basic ontology based on semantic features, at the top level, entities could be classified into
three main categories: person, animal, and object.

134

fiTranslation of Pronominal Anaphora between English and Spanish

a person entity as solutions for these pronouns. On the other hand, twenty-eight
occurrences of the pronouns he, she, him, and her were incorrectly solved due to
the system proposing an object or animal entity as the solution.
 There were exceptions in the applications of preferences (38%), mainly due to
the existence of a large number of candidates compatible with the anaphor15 .
 There were mistakes in the POS tagging (5%).
 In the MTI corpus, errors were mainly produced in the resolution of it pronouns
(73.4% of the global mistakes). The it pronoun lacks gender information (it is valid for
masculine and feminine) and subsequently there are a lot of candidates per anaphor16 .
This fact originates errors in the application of preferences. The remaining errors are
originated by the lack of semantic information.
After analyzing the results, it was observed that the precision of the SemCor corpus was
approximately 8% lower than that for the MTI corpus. The errors in the SemCor corpus
mainly originated with the lack of semantic information. Therefore, in order to improve
the obtained results, a second experiment was carried out with the addition of semantic
information.
The modifications to the second experiment were the following:
 Two new semantic constraintspresented in (Saiz-Noeda, Peral, & Suarez, 2000)
were added to the morphological and syntactic constraints:
1. The pronouns he, she, him, and her must have as the antecedents person entities.
2. The pronoun it must have as its antecedent a non-person entity.
To apply these new constraints, the twenty-five top concepts of WordNet (the concepts
at the top level in the ontology) were grouped into three categories: person, animal,
and object. Subsequently, WordNet was consulted with the head of each candidate,
and thus the semantic category of the antecedent was obtained.
 This experiment was exclusively carried out with the SemCor corpus because it is the
only one in which content words are annotated with their WordNet sense.
Table 8 shows the number of pronouns (classified by type) for the different corpora after
these changes were incorporated.
As shown in Table 8, the addition of the two simple semantic constraints resulted in
considerable improvement in the obtained precision (approximately 18%) for the SemCor
corpus. We concluded that the use of semantic information (such as new constraints and
preferences) in the process of anaphora resolution will improve the results obtained.
15. The sentences of the SemCor corpus are very long (with an average of 24.3 words per sentence). This
fact implies a large number of candidates per anaphor (an average of 15.2) after applying constraints.
16. The sentences of the MTI corpus are not very long (with an average of 15.5 words per sentence). However,
the candidates per anaphor, after applying constraints, are high (an average of 13.6).

135

fiPeral & Ferrandez

SEMCOR
MTI

He
116
1

She
10
0

It
38
347

They
50
56

Him
34
0

Her
0
0

Them
6
66

Corr
220
361

Total
254
470

P(%)
86.6
76.8

Table 8: Anaphora resolution in English, evaluation phase: experiment 2
Finally, Table 9 compares anaphora resolution using AGIR with the other approaches
previously presented17 . It is important to emphasize the high percentages obtained using
our system and Hobbss method in the SemCor corpus; both systems incorporate semantic information18 into their methods using semantic constraints (selectional restrictions),
whereas none of the other authors incorporate semantics in their approaches.
SEMCOR
MTI

Proximity
37.0
54.9

Hobbs
81.9
66.0

Lappin
59.4
75.1

Strube
59.4
63.2

AGIR
86.6
76.8

Table 9: Anaphora resolution in English, comparison of AGIR with other approaches

5. AGIRs Generation Module
The interlingua representation of the source text is taken as input for the generation module.
The output of this module is the target text, that is, the representation of the source texts
meaning with words of the target language.
The generation phase is split into two modules: syntactic generation and morphological
generation. Although the approach presented here is multilingual, we have focused on the
generation into the Spanish and English languages.
5.1 Syntactic Generation
In syntactic generation, the interlingua representation is converted by transformational rules
into an ordered surface-structure tree, with appropriate labeling of the leaves with target
language grammatical functions and features. The basic task of syntactic generation is to
order constituents in the correct sequence for the target language. However, the aim of
this work is only the translation of pronominal anaphora into the target language, so we
have only focused on the discrepancies between the Spanish and English languages in the
translation of the pronoun.
In syntactic generation, Spanish elliptical zero-subject constructions were studied. This
phenomenon was conveniently treated and solved in the analysis module. Therefore, all the
17. As mentioned earlier, all the results presented here were automatically obtained after the anaphoric
annotation of each pronoun. After the tagging and the partial parsing of the source text, pronominal
anaphora were resolved and translated into the target language. None of the intermediate outputs needed
to be adjusted manually in order to be processed subsequently.
18. Hobbs proposed the use of semantic information using selectional restrictions as a straightforward extension of his method in order to improve the obtained results in anaphora resolution.

136

fiTranslation of Pronominal Anaphora between English and Spanish

necessary information to translate these constructions has been stored in the interlingua
representation.
5.2 Morphological Generation
The final stage of the generation module is morphological generation, in which we mainly
have to treat and solve number and gender discrepancies in the translation of pronouns.
5.2.1 Number Discrepancies
This problem is generated by the discrepancy between words of different languages that
express the same concept. These words can be referred to a singular pronoun in the source
language and to a plural pronoun in the target language. In order to take into account
number discrepancies in the translation of the pronoun into English or Spanish, a set of
morphological (number) rules is constructed. The lefthand side of the number rule contains
the interlingua representation of the pronoun, whereas the righthand side contains the
pronoun in the target language.
5.2.2 Gender Discrepancies
Gender discrepancies come from existing morphological differences between different languages. For instance, English has less morphological information than Spanish. The English
plural personal pronoun they can be translated into the Spanish pronouns ellos (masculine)
or ellas (feminine); the singular personal pronoun it can be translated into el/este (masculine) or ella/esta (feminine), etc. In order to take into account such gender discrepancies
in the translation of the pronoun into English or Spanish, a set of morphological (gender)
rules was constructed.

6. Evaluation of the Generation Module
In this step, we tested the AGIRs generation module by evaluating the translation of
English pronouns into Spanish, and the translation of Spanish pronouns into English.
As mentioned earlier, the generation module takes the interlingua representation as
input. Previously, Spanish zero pronouns were detected (90.4% P) and resolved (81.4%
P), and anaphoric third-person personal pronouns were resolved in Spanish (82.2% P)
and English (86.6% P in the SemCor corpus with semantic information, and 76.8% P in
the MTI corpus without semantic information). Non-referential uses of it pronouns were
automatically detected, obtaining an 88.7% P on unrestricted texts19 .
19. In order to detect pleonastic it pronouns in AGIR, a set of rules, based on pattern recognition, that
allow for the identification of this type of pronoun is constructed. These rules were based on the work of
(Lappin & Leass, 1994; Paice & Husk, 1987; Denber, 1998), which dealt with this problem in a similar
way. We have used the information provided by the POS tagger in order to improve the detection of the
different patterns. We have evaluated the method using journalistic texts for a portion of the Federal
Register corpus that contains a set of 313 documents (156,831 words). In the detection of pleonastic it
pronouns a 88.7% P (568 out of 640) was obtained. Finally, it is very important to point out the high
percentage of it pronouns in the test corpus that are pleonastic (32.9%). This fact demonstrates the
importance of the correct detection of this kind of pronoun in any MT system.

137

fiPeral & Ferrandez

Once the interlingua representation was obtained, the method proposed for pronominal
anaphora translation into the target language was based on the treatment of number and
gender discrepancies.
6.1 Pronominal Anaphora Translation into Spanish
In this experiment, the translation of English, third-person, personal pronouns into Spanish
was evaluated.
We tested the method on the portions of the SemCor and MTI corpora used previously
in the process of anaphora resolution. The training corpus was used for improving the
number and gender rules. The remaining fragments of the corpora were reserved for test
data.
We needed to know the semantic category (person, animal, or object) and the grammatical gender (masculine or feminine) of the pronouns antecedent in order to apply the
number and gender rules. In the SemCor corpus, the WordNet sense was used to identify the antecedents semantic category. In the MTI corpus, due to the lack of semantic
information, a set of heuristics was used to determine the antecedents semantic category.
With regard to information about the antecedents gender, an EnglishSpanish electronic dictionary was used since the POS tag does not usually provide gender and number
information. The dictionary was incorporated into the system as a database. For each
English word, the dictionary provides a translation into Spanish, and the words gender
and number in Spanish.
The number and gender rules were applied using this morphological and semantic information. We conducted a blind test over the entire test corpus, and the obtained results
appear in Table 10.

SEMCOR
MTI
TOTAL

Subject
197
239
436

Compl
47
231
288

Correct
229
353
582

Total
254
470
724

P(%)
90.2
75.1
80.4

Table 10: Translation of pronominal anaphora into Spanish, evaluation phase

The evaluation of this task was automatically carried out after the anaphoric annotation of each pronoun. This annotation included information about the antecedent and the
translation into the target language of the anaphor. To do so, the human annotators translated the anaphors according to the criteria established by the morphological rules. For
example, the pronoun it with subject function was translated into the Spanish pronoun el
if its antecedent was of the animal type and masculine; on the other hand, if its antecedent
was of the object type and masculine, it was translated into the Spanish pronoun este;
and so on. In the SpanishEnglish translation, the pronoun el with subject function was
translated into the English pronoun he if its antecedent was a person type and masculine;
138

fiTranslation of Pronominal Anaphora between English and Spanish

on the other hand, if its antecedent was an object/animal type and masculine/feminine, it
was translated into the English pronoun it; and so on20 .
Table 10 shows the anaphoric pronouns of each corpus classified by grammatical function: subject and complement (direct or indirect object). The last three columns represent
the number of pronouns successfully solved, the total number of solved pronouns, and the
obtained precision, respectively. For instance, the SemCor corpus contains 197 pronouns
with subject function and 47 complement pronouns. The precision obtained in this corpus
was of 90.2% (229 out of 254).
Discussion. In the translation of English personal pronouns in the third person into
Spanish, an overall precision of 80.4% (582 out of 724) was obtained. Specifically, 90.2% P
and 75.1% P were obtained in the SemCor and MTI corpora, respectively.
From these results, we have extracted the following conclusions:
 In the SemCor corpus, all of the instances of the English pronouns he, she, him, and
her were correctly translated into Spanish. There are two reasons for this:
 The semantic roles of these pronouns were correctly identified in all of the cases.
 These pronouns contain the necessary grammatical information (gender and
number) that allows the correct translation into Spanish, independent of the
antecedent proposed as a solution by the AGIR system.
The errors in the translation of the pronouns it, they, and them were originated by
the following different causes:
 There were mistakes in the anaphora-resolution stage, that is, the antecedent
proposed by the system was not the correct one (44.4% of the global mistakes).
This caused an incorrect translation into Spanish mainly due to the fact that the
proposed antecedent and the correct one had different grammatical genders.
 There were mistakes in the identification of the semantic role of the pronouns
that caused the application of an incorrect morphological rule (44.4%). These
mistakes mainly originated in an incorrect process of clause splitting.
 There were mistakes originated by the EnglishSpanish electronic dictionary
(11.2%). Two circumstances could occur: (a) the word did not appear in the
dictionary; and (b) the words gender in the dictionary was different from the
real words gender, since the word had different meanings.
 In the MTI corpus, nearly all the pronouns were it, they, and them (96.2% of the total
pronouns). The errors in the translation of these pronouns originated in the same
causes as those in the SemCor corpus, although the percentages were different:
 There were mistakes in the anaphora-resolution stage (22.9% of the mistakes).
 There were mistakes in the identification of the pronouns semantic role (62.9%).
20. In the automatic evaluation, a pronoun was considered as correctly translated when the pronoun proposed
by the system was the same as that proposed by the human annotator. With this criterion, we evaluated
the correct application of the corresponding morphological rule.

139

fiPeral & Ferrandez

 There were mistakes that originated in the EnglishSpanish dictionary (14.2%).
In this corpus, there were a large number of technical words that did not appear
in the electronic dictionary.
 After analyzing the results, we observed that the precision of the SemCor corpus
was approximately 15% higher than that obtained by the MTI corpus. The lower
percentage obtained by the MTI corpus were the result of the corpus itself (most
of the pronouns in this corpus are it, they, and them), and of the lack of semantic
information.
In order to measure the efficiency of our proposal, we compared our system with one of
the most representative MT systems of the moment: Systran. Systran was designed and
built more than thirty years ago, and it is being continually modified in order to improve
its translation quality. Moreover, it is easily accessible to Internet users through the service
of MT on the webBABELFISH21 which provides free translations between different
languages. With regard to the problem of pronominal anaphora resolution and translation,
Systran is one of the best MT systems studied (see Section 2) because, like our own system,
it treats the problems of intersentential pronominal anaphora and Spanish zero pronouns on
unrestricted texts after carrying out a partial parsing of the source text. As was mentioned
in Section 2, a free trial of the commercial product SYSTRANLinks22 was used to translate
between the English and Spanish languages the evaluation corpora. The results appear in
Table 11.
SEMCOR
MTI

SYSTRANLinks
75.4
58.1

AGIR
82.5
69.3

Table 11: Translation of pronominal anaphora (complement pronouns only) into Spanish,
SYSTRANLinks and AGIR
The evaluation of the SYSTRANLinks output was carried out by a human translator by
hand. Pronouns judged as acceptable by the translator were considered correctly translated;
otherwise, they were considered incorrectly translated.
Table 11 only shows the evaluation of English complement pronoun translation into
Spanish because Systran did not translate all the subject pronouns into Spanish. By analyzing the Systran outputs of both corpora, we extracted the following conclusions:
 All the instances of the English pronouns he and she (always with subject function)
were correctly translated into their Spanish equivalents el and ella.
 All the instances of the English pronouns it and they with subject function were
omitted in Spanishzero pronouns. These pronouns were not resolved in English,
and subsequently were not translated into Spanish.
21. URL = http://www.babelfish.altavista.com (visited on 03/11/2002).
22. URL = http://w4.systranlinks.com/config (visited on 06/22/2002).

140

fiTranslation of Pronominal Anaphora between English and Spanish

On the other hand, in our AGIR system, we have evaluated the correct application of
the morphological rule to translate all source pronouns into target pronouns. A subsequent
task must decide if the pronoun in the target language (a) must be generated as our system
proposes, (b) must be substituted by another kind of pronoun (e.g., a possessive pronoun),
or (c) must be eliminated (i.e., Spanish zero pronouns). Therefore, we have only taken into
account the complement pronoun translation in order to make a fair comparison between
the two systems.
As shown in Table 11, the precision obtained using AGIR is approximately 711% higher
(depending on the corpus) than the one obtained using Systran. The errors in Systran originated in mistakes in the anaphora-resolution stage that caused incorrect translations, since
the proposed antecedents and the correct ones have different grammatical gender. These
errors can occur in intrasentential anaphors (as presented in Section 2) or in intersentential
anaphors, as in the following example extracted from the corpora:
 (E) [This information]i is only valid for Linux on the Intel platform. Much of iti
should be applicable to Linux on other processor architectures, but I have no first
hand experience or information.
 (S) Esta informacion es solamente valida para Linux en la plataforma de Intel. Mucho
de el debe ser aplicable a Linux en otras configuraciones del procesador, pero no tengo
ninguna experiencia o informacion de primera mano.
This example shows an incorrect EnglishSpanish translation of the pronoun it done by
Systran. In this case, the antecedent (this information, feminine) is in the previous sentence
to the anaphor. It is incorrectly solved, and then it is incorrectly translated (the pronoun
el masculineinstead of the pronoun estafeminine).
6.2 Pronominal Anaphora Translation into English
In this experiment, the translation of Spanish, third-person, personal pronouns and zero
pronouns (excluding reflexive pronouns) into English was evaluated. We tested the method
on the portion of the LEXESP corpus that was previously used in the process of anaphora
resolution.
We needed to know the semantic category and the grammatical gender of the pronouns
antecedent in order to apply the number and gender rules. In the LEXESP corpus, due to
the lack of semantic information, a set of heuristics was used to determine the antecedents
semantic category. On the other hand, the information about the antecedents gender was
provided by the POS tag of the antecedents head. We conducted a blind test over the
entire test corpus, and the results appear in Table 12.

LEXESP

Subject
630

Compl
145

Correct
657

Total
775

P(%)
84.8

Table 12: Translation of pronominal anaphora into English, evaluation phase
141

fiPeral & Ferrandez

Discussion. In the translation of Spanish personal pronouns in the third person into
English, an overall precision of 84.8% (657 out of 775) was obtained. From these results,
we extracted the following conclusions:
 All the instances of the Spanish plural pronouns (ellos, ellas, les, los, las, and the
zero pronouns in plural corresponding to the English pronouns they and them), were
correctly translated into English. There are two reasons for this:
 The semantic roles of these pronouns were correctly identified in all of the cases.
 The equivalent English pronouns (they and them) lack gender information, that
is, they are valid for masculine and feminine. Therefore, the antecedents gender
did not influence the translation of these pronouns.
 The errors occurred in the translation of the Spanish singular pronouns (el, ella, le,
lo, la, and in zero pronouns in singular corresponding to the English pronouns he, she,
it, him, and her ). There were different causes for these errors:
 There were mistakes in the anaphora-resolution stage (79.7% of the global mistakes), which caused an incorrect translation into Spanish, mainly due to the
proposed antecedent and the correct one having different grammatical gender.
Sometimes both had the same gender, but they had different semantic categories.
 There were mistakes in the application of the heuristic used to identify the antecedents semantic category (20.3%). This involved the application of an incorrect
morphological rule.
Our proposal was compared with the SYSTRANLinks output. As shown in Table 13, the
precision obtained by the AGIR system was approximately 28% higher than that obtained
by Systran.

LEXESP

SYSTRANLinks
56.9

AGIR
84.8

Table 13: Translation of pronominal anaphora into English, SYSTRANLinks and AGIR

The low results obtained in Systran are mainly the result of errors that occurred in the
translation of Spanish zero pronouns. Specifically, out of 775 Spanish pronouns, 334 errors
occurred, and 293 of them (87.7% of the global errors) originated in the translation of zero
pronouns, whereas the remainder (12.3%) originated in the translation of the remaining
not-omitted pronouns. The errors in the translation of zero pronouns mainly originated in
their incorrect resolution.

142

fiTranslation of Pronominal Anaphora between English and Spanish

7. Conclusion
In this paper we have evaluated the different tasks carried out in our MT approach (for
Spanish and English languages) that allowed the correct pronominal anaphora translation
into the target language. We have shown the importance of the resolution of anaphoric
expressions in any MT system for correct translations into the target language, and how
the main MT systems do not conveniently resolve this phenomenon.
Our approach, called AGIR, works on unrestricted texts to which partial-parsing techniques have been applied. After parsing and solving NLP problems, an interlingua representation of the entire text is obtained. This fact is one of the main advantages of our system
since several problems (hardly solved by the majority of MT systems) can be treated and
solved. These problems are the translation of intersentential anaphora, the detection of
co-reference chains, and the translation of Spanish zero pronouns into English.
In the evaluation, we obtained a precision of 80.4% and 84.8% in the translation of
Spanish and English pronominal anaphora, respectively. Previously, Spanish zero pronouns
had been resolved (with a precision of 81.4%) and anaphoric personal pronouns had been
resolved in English (with precisions of 86.6% and 76.8% in the SemCor corpus with semantic
information and in the MTI corpus without it, respectively) and in Spanish (with a precision
of 82.2%).
In addition, we carried out an exhaustive comparison with some well-known anaphoraresolution algorithms. Finally, we also compared pronoun translation with one of the most
representative MT systems at the moment: Systran. In all of these comparisons, AGIR was
shown to perform better.
A very important conclusion was extracted during the evaluation phase: the adding of semantic information improves the precision of the anaphora-resolution process considerably,
and therefore the corresponding precision of the anaphora-translation process. Currently,
the addition of this kind of information in the different stages of the AGIR system is being
studied in order to improve the overall performance of the system.
The resolution and translation of new types of references, such as definite descriptions
or anaphora originated by demonstrative pronouns, will be studied in the future. Moreover,
the addition of new languages to the interlingua approach will be taken into account.

Acknowledgments
The authors wish to thank Manuel Palomar for his helpful revisions of this paper; Ferran
Pla, Ruslan Mitkov, and Richard Evans for having contributed their corpora; and Rafael
Munoz, Maximiliano Saiz-Noeda, Patricio Martnez-Barco, and Juan Carlos Trujillo for
their suggestions and willingness to help in any task related to this paper. We are also
grateful for the helpful comments of the anonymous reviewers of several conference papers
in which we presented our preliminary work.
This research has been supported by the Spanish Government, under projects TIC20000664-C02-02 and FIT-150500-2002-416.
143

fiPeral & Ferrandez

References
Allegranza, V., Krauwer, S., & Steiner, E. (1991). Introduction. Machine Translation
(Eurotra Special Issue), 6 (2), 6171.
Amores, J., & Quesada, J. (1997). Episteme. Procesamiento del Lenguaje Natural, 21, 115.
Appelo, L., & Landsbergen, J. (1986). The machine translation project Rosetta. In Gerhardt, T. (Ed.), I. International Conference on the State of the Art in Machine Translation in America, Asia and Europe: Proceedings of IAI-MT86, IAI/EUROTRA-D,
pp. 3451 Saarbrucken (Germany).
Bennet, W., & Slocum, J. (1985). The LRC machine translation system. Computational
Linguistics, 11, 111121.
Berger, A., et al. (1994). The Candide system for Machine Translation. In Proceedings of
the ARPA Workshop on Speech and Natural Language, pp. 157163 Morgan Kaufman
Publishers.
Boitet, C. (1989). Geta project. In Nagao, M. (Ed.), Machine Translation Summit, pp.
5465. Ohmsha, Tokyo.
Boitet, C., & Nedobejkine, N. (1981). Recent developments in Russian-French machine
translation at Grenoble. Linguistics, 19, 199271.
Canals-Marote, R., et al. (2001a). El sistema de traduccion automatica castellano-catalan
interNOSTRUM. Procesamiento del Lenguaje Natural, 27, 151156.
Canals-Marote, R., et al. (2001b). The Spanish-Catalan machine translation system interNOSTRUM. In Proceedings of Machine Translation Summit VIII, pp. 7376 Santiago
de Compostela (Spain).
Carletta, J., et al. (1997). The Reliability of a Dialogue Structure Coding Scheme. Computational Linguistics, 23 (1), 1332.
Chandioux, J. (1976). METEO: un systeme operationnel pour la traduction automatique
des bulletins metereologiques destines au grand public. META, 21, 127133.
Chandioux, J. (1989). Meteo: 100 million words later. In Hammond, D. (Ed.), American Translators Association Conference 1989: Coming of Age, pp. 449453. Learned
Information, Medford, NJ.
Daz-Ilarraza, A., Mayor, A., & Sarasola, K. (2000). Reusability of wide-coverage linguistic
resources in the construction of a multilingual machine translation system. In Proceedings of the Machine Translation and multilingual applications in the new millennium
(MT2000), pp. 12.112.9 Exeter (UK).
Daz-Ilarraza, A., Mayor, A., & Sarasola, K. (2001). Inclusion del par castellano-euskara
en un prototipo de traduccion automatica multilingue. In Proceedings of the Second
International Workshop on Spanish Language Processing and Language Technologies
(SLPLT-2), pp. 107111 Jaen (Spain).
144

fiTranslation of Pronominal Anaphora between English and Spanish

Denber, M. (1998). Automatic Resolution of Anaphora in English. Eastman Kodak Co.,
Imaging Science Division.
Farwell, D., & Helmreich, S. (2000). An interlingual-based approach to reference resolution. In Proceedings of the Third AMTA/SIG-IL Workshop on Applied Interlinguas:
Practical Applications of Interlingual Approaches to NLP (ANLP/NAACL2000), pp.
111 Seattle, Washington (USA).
Ferrandez, A., Palomar, M., & Moreno, L. (1999). An empirical approach to Spanish
anaphora resolution. Machine Translation, 14 (3/4), 191216.
Ferrandez, A., & Peral, J. (2000). A computational approach to zero-pronouns in Spanish. In Proceedings of the 38th Annual Meeting of the Association for Computational
Linguistics (ACL2000), pp. 166172 Hong Kong (China).
Geldbach, S. (1999). Anaphora and Translation Discrepancies in Russian-German MT.
Machine Translation, 14 (3/4), 217230.
Goodman, K. (1989). Special Issues on Knowledge-Based Machine Translation, Parts I and
II. Machine Translation, 4 (1/2).
Halliday, M., & Hasan, R. (1976). Cohesion in English. Longman English Language Series
9. Longman, London.
Hirst, G. (1981). Anaphora in Natural Language Understanding. Springer-Verlag, Berlin.
Hobbs, J. (1978). Resolving pronoun references. Lingua, 44, 311338.
Hutchins, W., & Somers, H. (1992). An Introduction to Machine Translation. Academic
Press Limited, London.
Landes, S., Leacock, C., & Tengi, R. (1998). Building semantic concordances. In Fellbaum, C. (Ed.), WordNet: An Electronic Lexical Database, pp. 199216. MIT Press,
Cambridge, Mass.
Landsbergen, J. (1987). Montague grammar and machine translation. In Whitelock, P.,
Wood, M., Somers, H., Johnson, R., & Bennet, P. (Eds.), Linguistic theory and computer applications, pp. 113147. Academic Press, London.
Lappin, S., & Leass, H. (1994). An algorithm for pronominal anaphora resolution. Computational Linguistics, 20 (4), 535561.
Maas, H. (1977). The Saarbrucken automatic translation system (SUSY). In Proceedings
of the Third European Congress on Information Systems and Networks, Overcoming
the language barrier, pp. 585592 Munchen (Germany).
Maas, H. (1987). The MT system SUSY. In King, M. (Ed.), Machine translation today: the
state of the art, Edinburgh Information Technology Series 2, pp. 209246. Edinburgh
University Press.
145

fiPeral & Ferrandez

Mahesh, K., & Nirenburg, S. (1995a). A situated ontology for practical NLP. In Proceedings
of Workshop on basic ontological issues in knowledge sharing (IJCAI95) Montreal
(Canada).
Mahesh, K., & Nirenburg, S. (1995b). Semantic classification for practical Natural Language Processing. In Proceedings of the Sixth ASIS SIG/CR Classification Research
Workshop: An interdisciplinary meeting, pp. 7994 Chicago, Illinois (USA).
Miller, G., Beckwith, R., Fellbaum, C., Gross, D., & Miller, K. (1990). WordNet: An on-line
lexical database. International journal of lexicography, 3 (4), 235244.
Mitamura, T., Nyberg, E., & Carbonell, J. (1991). An efficient interlingua translation
system for multi-lingual document production. In Proceedings of Machine Translation
Summit III Washington, DC (USA).
Mitkov, R., Kim, H., Lee, H., & Choi, K. (1994). Lexical transfer and resolution of pronominal anaphors in Machine Translation: the English-to-Korean case. Procesamiento del
Lenguaje Natural, 15, 2337 (Grupo 2. Traduccion Automatica e Interfaces).
Mitkov, R., & Schmidt, P. (1998). On the complexity of pronominal anaphora resolution
in machine translation. In Martn-Vide, C. (Ed.), Mathematical and computational
analysis of natural language. John Benjamins Publishers, Amsterdam.
Montoyo, A., & Palomar, M. (2000). WSD algorithm applied to a NLP system. In
Bouzeghoub, M., Kedad, Z., & Metais, E. (Eds.), Natural Language Processing and
Information Systems, Vol. 1959 of Lecture Notes in Computer Science, pp. 5465
Versailles (France). Springer-Verlag.
Munoz, R., Palomar, M., & Ferrandez, A. (2000). Processing of Spanish Definite Descriptions. In Cairo, O., Sucar, L., & Cantu, F. (Eds.), MICAI 2000: Advances in Artificial
Intelligence, Vol. 1793 of Lecture Notes in Artificial Intelligence, pp. 526537 Acapulco
(Mexico). Springer-Verlag.
Nakaiwa, H., & Ikehara, S. (1992). Zero pronoun resolution in a Japanese-to-English Machine Translation system by using verbal semantic attributes. In Proceedings of the
Third Conference on Applied Natural Language Processing (ANLP92), pp. 201208
Trento (Italy).
Nirenburg, S. (1989). Knowledge-based machine translation. Machine Translation, 4, 524.
Okumura, M., & Tamura, K. (1996). Zero pronoun resolution in Japanese discourse based
on centering theory. In Proceedings of the 16th International Conference on Computational Linguistics (COLING96), pp. 871876 Copenhagen (Denmark).
Paice, C., & Husk, G. (1987). Towards the automatic recognition of anaphoric features
in English text: the impersonal pronoun it. Computer Speech and Language, 2,
109132.
Palomar, M., & Martnez-Barco, P. (2001). Computational approach to anaphora resolution
in Spanish dialogues. Journal of Artificial Intelligence Research, 15, 263287.
146

fiTranslation of Pronominal Anaphora between English and Spanish

Palomar, M., et al. (2001). An algorithm for anaphora resolution in Spanish texts. Computational Linguistics, 27 (4), 545567.
Peral, J., & Ferrandez, A. (2000a). An application of the Interlingua System ISS for SpanishEnglish pronominal anaphora generation. In Proceedings of the Third AMTA/SIG-IL
Workshop on Applied Interlinguas: Practical Applications of Interlingual Approaches
to NLP (ANLP/NAACL2000), pp. 4251 Seattle, Washington (USA).
Peral, J., & Ferrandez, A. (2000b). Generation of Spanish zero-pronouns into English. In
Christodoulakis, D. (Ed.), Natural Language Processing - NLP 2000, Vol. 1835 of
Lecture Notes in Artificial Intelligence, pp. 252260 Patras (Greece). Springer-Verlag.
Peral, J., Palomar, M., & Ferrandez, A. (1999). Coreference-oriented Interlingual Slot
Structure and Machine Translation. In Proceedings of the ACL Workshop Coreference
and its Applications, pp. 6976 College Park, Maryland (USA).
Quesada, J., & Amores, J. (2000). Diseno e implementacion de sistemas de Traduccion
Automatica. Universidad de Sevilla. Secretariado de publicaciones, Sevilla.
Sadler, V. (1989). Working with analogical semantics: disambiguation techniques in DLT.
Distributed Language Translation 5. Foris, Dordrecht.
Saiz-Noeda, M., Peral, J., & Suarez, A. (2000). Semantic compatibility techniques for
anaphora resolution. In Proceedings of ACIDCA2000, pp. 4348 Monastir (Tunisia).
Schubert, K. (1986). Linguistic and extra-linguistic knowledge. Computers and Translation,
1, 125152.
Strube, M., & Hahn, U. (1999). Functional Centering - Grounding Referential Coherence
in Information Structure. Computational Linguistics, 25 (5), 309344.
Thurmair, G. (1990). Complex lexical transfer in METAL. In Proceedings of TMI90, pp.
91107 Austin, Texas (USA).
Toma, P. (1977). Systran as a multilingual machine translation system. In Proceedings of
the Third European Congress on Information Systems and Networks, Overcoming the
language barrier, pp. 569581 Munchen (Germany).
Varile, G., & Lau, P. (1988). Eurotra: practical experience with a multilingual machine
translation system under development. In Proceedings of the Second Conference on
Applied Natural Language Processing (ANLP88), pp. 160167 Austin, Texas (USA).
Vossen, P. (1998). EuroWordNet: Building a Multilingual Database with WordNets for
European Languages. The ELRA Newsletter, 3 (1), 712.
Wheeler, P. (1987). Systran. In King, M. (Ed.), Machine translation today: the state of the
art, Edinburgh Information Technology Series 2, pp. 192208. Edinburgh University
Press.
Witkam, A. (1983). Distributed language translation: feasibility study of multilingual facility
for videotex information networks. BSO, Utrecht.

147

fiJournal of Artificial Intelligence Research 18 (2003) 45-81

Submitted 08/02; published 01/03

Monte Carlo Methods for Tempo Tracking
and Rhythm Quantization
Ali Taylan Cemgil
Bert Kappen

SNN, Geert Grooteplein 21 cpk1 - 231, University of Nijmegen
NL 6525 EZ Nijmegen, The Netherlands

cemgil@snn.kun.nl
bert@snn.kun.nl

Abstract

We present a probabilistic generative model for timing deviations in expressive music performance. The structure of the proposed model is equivalent to a switching state
space model. The switch variables correspond to discrete note locations as in a musical
score. The continuous hidden variables denote the tempo. We formulate two well known
music recognition problems, namely tempo tracking and automatic transcription (rhythm
quantization) as filtering and maximum a posteriori (MAP) state estimation tasks. Exact computation of posterior features such as the MAP state is intractable in this model
class, so we introduce Monte Carlo methods for integration and optimization. We compare
Markov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated annealing and iterative improvement) and sequential Monte Carlo methods (particle filters). Our
simulation results suggest better results with sequential methods. The methods can be
applied in both online and batch scenarios such as tempo tracking and transcription and
are thus potentially useful in a number of music applications such as adaptive automatic
accompaniment, score typesetting and music information retrieval.
1. Introduction
Automatic music transcription refers to extraction of a human readable and interpretable
description from a recording of a musical performance. Traditional music notation is such
a description that lists the pitch levels (notes) and corresponding timestamps.
Ideally, one would like to recover a score directly from the audio signal. Such a representation of the surface structure of music would be very useful in music information retrieval
(Music-IR) and content description of musical material in large audio databases. However,
when operating on sampled audio data from polyphonic acoustical signals, extraction of a
score-like description is a very challenging auditory scene analysis task (Vercoe, Gardner,
& Scheirer, 1998).
In this paper, we focus on a subproblem in music-ir, where we assume that exact timing
information of notes is available, for example as a stream of MIDI1 events from a digital
keyboard.
A model for tempo tracking and transcription from a MIDI-like music representation
is useful in a broad spectrum of applications. One example is automatic score typesetting,
1. Musical Instruments Digital Interface. A standard communication protocol especially designed for digital
instruments such as keyboards. Each time a key is pressed, a MIDI keyboard generates a short message
containing pitch and key velocity. A computer can tag each received message by a timestamp for real-time
processing and/or recording into a file.

c 2003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiCemgil & Kappen
the musical analog of word processing. Almost all score typesetting applications provide a
means of automatic generation of a conventional music notation from MIDI data.
In conventional music notation, the onset time of each note is implicitly represented by
the cumulative sum of durations of previous notes. Durations are encoded by simple rational
numbers (e.g., quarter note, eighth note), consequently all events in music are placed on
a discrete grid. So the basic task in MIDI transcription is to associate onset times with
discrete grid locations, i.e., quantization.
However, unless the music is performed with mechanical precision, identification of the
correct association becomes dicult. This is due to the fact that musicians introduce
intentional (and unintentional) deviations from a mechanical prescription. For example
timing of events can be deliberately delayed or pushed. Moreover, the tempo can uctuate
by slowing down or accelerating. In fact, such deviations are natural aspects of expressive
performance; in the absence of these, music tends to sound rather dull and mechanical.
On the other hand, if these deviations are not accounted for during transcription, resulting
scores have often very poor quality.
Robust and fast quantization and tempo tracking is also an important requirement for
interactive performance systems; applications that \listen" to a performer for generating an
accompaniment or improvisation in real time (Raphael, 2001b; Thom, 2000). At last, such
models are also useful in musicology for systematic study and characterization of expressive
timing by principled analysis of existing performance data.
From a theoretical perspective, simultaneous quantization and tempo tracking is a
\chicken-and-egg" problem: the quantization depends upon the intended tempo interpretation and the tempo interpretation depends upon the quantization. Apparently, human
listeners can resolve this ambiguity (in most cases) without any effort. Even persons without
any musical training are able to determine the beat and the tempo very rapidly. However,
it is still unclear what precisely constitutes tempo and how it relates to the perception of
the beat, rhythmical structure, pitch, style of music etc. Tempo is a perceptual construct
and cannot directly be measured in a performance.
The goal of understanding tempo perception has stimulated a significant body of research on the psychological and computational modeling aspects of tempo tracking and
beat induction, e.g., see (Desain & Honing, 1994; Large & Jones, 1999; Toiviainen, 1999).
These papers assume that events are presented as an onset list. Attempts are also made
to deal directly with the audio signal (Goto & Muraoka, 1998; Scheirer, 1998; Dixon &
Cambouropoulos, 2000).
Another class of tempo tracking models are developed in the context of interactive
performance systems and score following. These models make use of prior knowledge in the
form of an annotated score (Dannenberg, 1984; Vercoe & Puckette, 1985). More recently,
Raphael (2001b) has demonstrated an interactive real-time system that follows a solo player
and schedules accompaniment events according to the player's tempo interpretation.
Tempo tracking is crucial for quantization, since one can not uniquely quantize onsets
without having an estimate of tempo and the beat. The converse, that quantization can
help in identification of the correct tempo interpretation has already been noted by Desain
and Honing (1991). Here, one defines correct tempo as the one that results in a simpler
quantization. However, such a schema has never been fully implemented in practice due
to computational complexity of obtaining a perceptually plausible quantization. Hence

46

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization
quantization methods proposed in the literature either estimate the tempo using simple
heuristics (Longuet-Higgins, 1987; Pressing & Lawrence, 1993; Agon, Assayag, Fineberg,
& Rueda, 1994) or assume that the tempo is known or constant (Desain & Honing, 1991;
Cambouropoulos, 2000; Hamanaka, Goto, Asoh, & Otsu, 2001).
Our approach to transcription and tempo tracking is from a probabilistic, i.e., Bayesian
modeling perspective. In Cemgil et al. (2000), we introduced a probabilistic approach to
perceptually realistic quantization. This work also assumed that the tempo was known or
was estimated by an external procedure. For tempo tracking, we introduced a Kalman filter
model (Cemgil, Kappen, Desain, & Honing, 2001). In this approach, we modeled the tempo
as a smoothly varying hidden state variable of a stochastic dynamical system.
In the current paper, we integrate quantization and tempo tracking. Basically, our
model balances score complexity versus smoothness in tempo deviations. The correct tempo
interpretation results in a simple quantization and the correct quantization results in a
smooth tempo uctuation. An essentially similar model is proposed recently also by Raphael
(2001a). However, Raphael uses an inference technique that only applies for small models;
namely when the continuous hidden state is one dimensional. This severely restricts the
models one can consider. In the current paper, we survey general and widely used state-ofthe-art techniques for inference.
The outline of the paper is as follows: In Section 2, we propose a probabilistic model for
timing deviations in expressive music performance. Given the model, we will define tempo
tracking and quantization as inference of posterior quantities. It will turn out that our model
is a switching state space model in which computation of exact probabilities becomes intractable. In Section 3, we will introduce approximation techniques based on simulation,
namely Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC) (Doucet,
de Freitas, & Gordon, 2001; Andrieu, de Freitas, Doucet, & Jordan, 2002). Both approaches
provide exible and powerful inference methods that have been successfully applied in diverse fields of applied sciences such as robotics (Fox, Burgard, & Thrun, 1999), aircraft
tracking (Gordon, Salmond, & Smith, 1993), computer vision (Isard & Blake, 1996), econometrics (Tanizaki, 2001). Finally we will present simulation results and conclusions.

2. Model
Assume that a pianist is improvising and we are recording the exact onset times of each key
she presses during the performance. We denote these observed onset times by y0 ; y1 ; y2 : : :
yk : : : yK or more compactly by y0:K . We neither have access to a musical notation of the
piece nor know the initial tempo she has started her performance with. Moreover, the
pianist is allowed to freely change the tempo or introduce expression. Given only onset
time information y0:K , we wish to find a score 1:K and track her tempo uctuations z0:K .
We will refine the meaning of  and z later.
This problem is apparently ill-posed. If the pianist is allowed to change the tempo
arbitrarily it is not possible to assign a \correct" score to a given performance. In other
words any performance y0:K can be represented by using a suitable combination of an
arbitrary score with an arbitrary tempo trajectory. Fortunately, the Bayes theorem provides
an elegant and principled guideline to formulate the problem. Given the onsets y0:K , the
best score 1:K and tempo trajectory z0:K can be derived from the posterior distribution

47

fiCemgil & Kappen
that is given by
1
p(y j ; z )p( ; z )
p(y0:K ) 0:K 1:K 0:K 1:K 0:K
a quantity, that is proportional to the product of the likelihood term p(y0:K j1:K ; z0:K ) and
the prior term p(1:K ; z0:K ).
In rhythm transcription and tempo tracking, the prior encodes our background knowledge about the nature of musical scores and tempo deviations. For example, we can construct a prior that prefers \simple" scores and smooth tempo variations.
The likelihood term relates the tempo and the score to actual observed onset times. In
this respect, the likelihood is a model for short time expressive timing deviations and motor
errors that are introduced by the performer.

p(1:K ; z0:K jy0:K ) =

/ c1
/ c2
/ : : :F
/ ck
EE
FF
CC
F
EE
EE
CC
FF
EE
EE
CC
FF
EE
EE
CC
FF
CC
E" 
E" 
# 
!: : :

c0 EE

1

k 1

2

/ :::
/ 1
/ 2
BB
@@
BB
BB
@@
BB
BB
@@
BB
BB
@@
BB
B
@@
B! 
B! 
/ 1
/ 2
/ :::

0 B

0


y0



y1



y2

/ :::
CC Quantization
CC
HH
CC
HH
CC
HH
CC
H# 
!
/ ck

1 HH

k

Score

/ k
/ :Tempo
::
Trajectory
@@
EE
@
EE
@@
EE
@@
EE
@@
E" 
@
/ k
/ : :Noiseless
:
onsets

k 1E
/

/



k 1


:::

:::

Locations



yk 1

yk

: Observed
::
Onsets

Figure 1: Graphical Model. Square and oval nodes correspond to discrete and continuous
variables respectively. In the text, we sometimes refer to the continuous hidden
variables (k ; k ) by zk . The dependence between  and c is deterministic. All
c,  ,  and  are hidden; only onsets y are observed.

2.1 Score prior
To define a score 1:K , we first introduce a sequence of quantization locations c0:K . A
quantization location ck specifies the score time of the k'th onset. We let k denote the
interval between quantization locations of two consecutive onsets
k = ck

ck 1

(1)

For example consider the conventional music notation
which encodes the score 1:3 =
[1 0:5 0:5]. Corresponding quantization locations are c0:3 = [0 1 1:5 2].
One simple way of defining a prior distribution on quantization locations p(ck ) is specifying a table of probabilities for ck mod 1 (the fraction of ck ). For example if we wish to


48





fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization
allow for scores that have sixteenth notes and triplets, we define a table of probabilities for
the states c mod 1 = f0; 0:25; 0:5; 0:75g[f0; 0:33; 0:67g. Technically, the resulting prior
p(ck ) is periodic and improper (since ck are in principle unbounded so we can not normalize
the distribution).
However, if the number of states of ck mod 1 is large, it may be dicult to estimate the
parameters of the prior reliably. For such situations we propose a \generic" prior as follows:
We define the probability, that the k'th onset gets quantized at location ck , by p(ck ) /
exp( d(ck )) where d(ck ) is the number of significant digits in the binary expansion of ck
mod 1. For example d(1) = 0, d(1:5) = 1, d(7 + 9=32) = 5 etc. The positive parameter  is
used to penalize quantization locations that require more bits to be represented. Assuming
that quantization locations of onsets are independent a-priori, (besides being increasing in
k, i.e., ck  ck 1 ), P
the prior probability of a sequence of quantization locations is given by
p(c0:K ) / exp(  K
k=0 d(ck )). We further assume that c0 2 [0; 1). One can check that
) < p(
). We can generalize this
such a prior prefers simpler notations, e.g., p(
prior to other subdivisions such triplets and quintiplets in Appendix A.
Formally, given a distribution on c0:K , the prior of a score 1:K is given by






6

p(1:K ) =

X

c0:K











6

p(1:K jc0:K )p(c0:K )

(2)

Since the relationship between c0:K and 1:K is deterministic, p(1:K jc0:K ) is degenerate for
any given c0:K , so we have

p(1:K ) / exp



K
k
X
X

!

d( k0 )
(3)
=1 k0 =1
One might be tempted to specify a prior directly on 1:K and get rid of c0:K entirely.
However, with this simpler approach it is not easy to devise realistic priors. For example,
consider a sequence of note durations [1 1=16 1 1 1 : : : ]. Assuming a factorized prior on
 that penalizes short note durations, this rhythm would have relatively high probability
whereas it is quite uncommon in conventional music.
k

2.2 Tempo prior
We represent the tempo in terms of its inverse, i.e., the period, and denote it with . For
example a tempo of 120 beats per minute (bpm) corresponds to  = 60=120 = 0:5 seconds.
At each onset the tempo changes by an unknown amount k . We assume the change k
is iid with N (0; Q ). 2 We assume a first order Gauss-Markov process for the tempo
k = k 1 + k
(4)
Eq. 4 defines a distribution over tempo sequences 0:K . Given a tempo sequence, the
\ideal" or \intended" time k of the next onset is given by

k = k 1 + k k 1 + k

(5)

2. We denote a (scalar or multivariate) Gaussian distribution p(x) with mean vector  and covariance
1
matrix P by N (; P )=
^ j2P j 2 exp( 21 (x )T P 1 (x )).

49

fiCemgil & Kappen
The noise term k denotes the amount of accentuation (that is deliberately playing a
note ahead or back in time) without causing the tempo to be changed. We assume k 
N (0; Q ). Ideal onsets and actually observed \noisy" onsets are related by

yk = k + k

(6)

The noise term k models small scale expressive deviations or motor errors in timing of individual notes. In this paper we will assume that k has a Gaussian distribution parameterized
by N (0; R).
The initial tempo distribution p(0 ) specifies a range of reasonable tempi and is given
by a Gaussian with a broad variance. We assume an uninformative (at) prior on 0 . The
conditional independence structure is given by the graphical model in Figure 1. Table 1
shows a possible realization from the model.
We note that our model is a particular instance of the well known switching state space
model (also known as conditionally linear dynamical system, jump Markov linear system,
switching Kalman filter) (See, e.g., Bar-Shalom & Li, 1993; Doucet & Andrieu, 2001;
Murphy, 2002).

k 0
1
2
3
k
...
ck 0 1/2 3/2 2 . . .
k 0.5 0.6 0.7 . . . . . .
k 0 0.25 0.85 1.20 . . .
yk 0 0.23 0.88 1.24 . . .


(





(

Table 1: A possible realization from the model: a ritardando. For clarity we assume  = 0.
In the following sections, we will sometimes refer use zk = (k ; k )T and refer to z0:K
as a tempo trajectory. Given this definition, we can compactly represent Eq. 4 and Eq. 5 by

zk =



1 k
0 1



zk 1 + k

(7)

where k = (k ; k ).

2.3 Extensions
There are several possible extensions to this basic parameterization. For example, one could
represent the period  in the logarithmic scale. This warping ensures positivity and seems
to be perceptually more plausible since it promotes equal relative changes in tempo rather
than on an absolute scale (Grubb, 1998; Cemgil et al., 2001). Although the resulting model
becomes nonlinear, it can be approximated fairly well by an extended Kalman filter (BarShalom & Li, 1993).
A simple random walk model for tempo uctuations such as in Eq. 7 seems not to be
very realistic. We would expect the tempo deviations to be more structured and smoother.

50

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization
In our dynamical system framework such smooth deviations can be modeled by increasing
the dimensionality of z to include higher order \inertia" variables (Cemgil et al., 2001). For
example consider the following model,

0
k
B
1;k
B
B
2;k
B
B
@ ...

D 1;k

1
0
10
1 k k 0 : : : 0
k 1
C
B
C
B
0 1 0 0 : : : 0 C B 1;k 1
C
B
C
B
C
B
2;k 1
= B0 0
C
C
B
C
B
C
B
.
.
..
A
@ .. ..
A@
A
.
0 0

D 1;k 1

1
C
C
C
+ k
C
C
A

(8)

We choose this particular parameterization because we wish to interpret 1 as the slowly
varying \average" tempo and 2 as a temporary change in the tempo. Such a model is useful
for situations where the performer uctuates around an almost constant tempo; a random
walk model is not sucient in this case because it forgets the initial values. Additional state
variables 3 ; : : : ; D 1 act like additional \memory" elements. By choosing the parameter
matrix A and noise covariance matrix Q, one can model a rich range of temporal structures
in expressive timing deviations.
The score prior can be improved by using a richer model. For example to allow for
different time signatures and alternative rhythmic subdivisions, one can introduce additional
hidden variables (See Cemgil et al. (2000) or Appendix A) or use a Markov chain (Raphael,
2001a). Potentially, such extensions make it easier to capture additional structure in musical
rhythm (such as \weak" positions are followed more likely by \strong" positions). On the
other hand, the number of model parameters rapidly increases and one has to be more
cautious in order to avoid overfitting.
For score typesetting, we need to quantize note durations as well, i.e., associate note
offsets with quantization locations. A simple way of accomplishing this is to define an
indicator sequence u0:K that identifies whether yk is an onset (uk = 1) or an offset (uk =
0). Given uk , we can redefine the observation model as p(yk jk ; uk ) = uk N (0; R) + (1
uk )N (0; Roff ) where Roff is the observation noise associated with offsets. A typical model
would have Roff  R. For Roff ! 1, the offsets would have no effect on the tempo process.
Moreover, since uk are always observed, this extension requires just a simple lookup.
In principle, one must allow for arbitrary long intervals between onsets, hence k are
drawn from an infinite (but discrete) set. In our subsequent derivations, we assume that the
number of possible intervals is fixed a-priori. Given an estimate of zk 1 and observation yk ,
almost all of the virtually infinite number of choices for k will have almost zero probability
and it is easy to identify candidates that would have significant probability mass.
Conceptually, all of the above listed extensions are easy to incorporate into the model
and none of them introduces a fundamental computational diculty to the basic problems
of quantization and tempo tracking.

51

fiCemgil & Kappen
2.4 Problem Definition
Given the model, we define rhythm transcription, i.e., quantization as a MAP state estimation problem
1: K = argmax p(1:K jy0:K )
(9)
p(1:K jy0:K ) =

Z 1:K

dz0:K p(1:K ; z0:K jy0:K )

and tempo tracking as a filtering problem
X
zk = argmax p(1:k ; zk jy0:k )
zk

1:k

(10)

The quantization problem is a smoothing problem: we wish to find the most likely score

1:K given all the onsets in the performance. This is useful in \oine" applications such as
score typesetting.
For real-time interaction, we need to have an online estimate of the tempo/beat zk .
This information is carried forth by the filtering density p(1:k ; zk jy0:k ) in Eq.10. Our
definition of the best tempo zk as the maximum is somewhat arbitrary. Depending upon
the requirements of an application, P
one can make use of other features of the filtering
density. For example, the variance of 1:k p(1:k ; zk jy0:k ) can be used to estimate \amount
of confidence" in tempo interpretation or arg maxzk ;1:k p(1:k ; zk jy0:k ) to estimate most
likely score-tempo pair so far.
Unfortunately, the quantities in Eq. 9 and Eq. 10 are intractable due to the explosion in
the number of mixture components required to represent the exact posterior at each step k
(See Figure 2). For example, to calculate the exact posterior in Eq. 9 we need to evaluate
the following expression:
Z
1
dz0:K p(y0:K jz0:K ; 1:K )p(z0:K j1:K )p(1:K )
(11)
p(1:K jy0:K ) =

Z
1
= p(y0:K j1:K )p(1:K )
(12)
Z
P
where the normalization constant is given by Z = p(y0:K ) = 1:K p(y0:K j1:K )p(1:K ). For
each trajectory 1:K , the integral over z0:K can be computed stepwise in k by the Kalman
filter (See appendix B.1). However, to find the MAP state of Eq. 11, we need to evaluate
p(y0:K j1:K ) independently for each of the exponentially many trajectories. Consequently,
the quantization problem in Eq. 9 can only be solved approximately.
For accurate approximation, we wish to exploit any inherent independence structure of
the exact posterior. Unfortunately, since z and c are integrated over, all k become coupled
and in general p(1:K jy0:K ) does not possess any conditional independence structure (e.g.,
a Markov chain) that would facilitate ecient calculation. Consequently, we will resort to
numerical approximation techniques.

3. Monte Carlo Simulation
Consider a high dimensional probability distribution
1
p(x) = p (x)
Z

52

(13)

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization

0.6

0.6

0.4

0.4

0.2

0.2

2.2769

0

0

4.6765
10.5474





2.6972

0.2

0.2
3.2828
5.0002

0.4

0.4

0.6

0.8
0.5

0.6

0

0.5

1

1.5


2

2.5

3

3.5

(a)

0.8
0.5

0

0.5

1

3

3.5

1.5


2

2.5

3

3.5

(b)

0.6

0.4
0.4593
0.2

7.9036
10.3422
6.6343



0

0.2

10.1982
0.76292
2.393

0.4

2.7957

0.6

0.8
0.5

Figure 2:

0

0.5

1

1.5


2

2.5

(c)

Example demonstrating the explosion of the number of components to represent the
exact posterior. Ellipses denote the conditional marginals p(k ; !k jc0:k ; y0:k ). (We show
the period in logarithmic scale where !k = log2 k ). In this toy example, we assume
that a score consists only of notes of length and , i.e., k can be either 1=2 or 1.
(a) We start with a unimodal posterior p(0 ; !0 jc0 ; y0 ), e.g., a Gaussian centered at
(; !) = (0; 0). Since we assume that a score can only consist of eight- and quarter
notes, i.e., k 2 f1=2; 1g. the predictive distribution p(1 ; !1jc0:1; y0) is bimodal where
the modes are centered at (0:5; 0) and (1; 0) respectively (shown with a dashed contour
line). Once the next observation y1 is observed (shown with a dashed vertical line around
 = 0:5), the predictive distribution is updated to yield p(1 ; !1 jc0:1 ; y0:1 ). The numbers
denote the respective log-posterior weight of each mixture component. (b) The predictive
distribution p(2 ; !2jc0:1; y0:1) at step k = 2 has now 4 modes, two for each component
of p(1; !1jc0:1; y0:1). (c) The number of components grows exponentially with k.




53

(

fiCemgil & Kappen
R
where the normalization constant Z = dxp (x) is not known but p (x) can be evaluated
at any particular x. Suppose we want to estimate the expectation of a function f (x) under
the distribution p(x) denoted as
Z

hf (x)ip(x) = dxf (x)p(x)
e.g., the mean of x under p(x) is given by hxi. The intractable integration can be approximated by an average if we can find N points x(i) , i = 1 : : : N from p(x)

hf (x)ip(x)  N1

N
X

=1

i

f (x(i) )

(14)

When x(i) are generated by independently sampling from p(x), it can be shown that as N
approaches infinity, the approximation becomes exact.
However, generating independent samples from p(x) is a dicult task in high dimensions but it is usually easier to generate dependent samples, that is we generate x(i+1) by
making use of x(i) . It is somewhat surprising, that even if x(i) and x(i+1) are correlated
(and provided ergodicity conditions are satisfied), Eq. 14 remains still valid and estimated
quantities converge to their true values when number of samples N goes to infinity.
A sequence of dependent samples x(i) is generated by using a Markov chain that has
the stationary distribution p(x). The chain is defined by a collection of transition probabilities, i.e., a transition kernel T (x(i+1) jx(i) ). The definition of the kernel is implicit, in
the sense that one defines a procedure to generate the x(i+1) given x(i) . The Metropolis
algorithm (Metropolis & Ulam, 1949; Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller,
1953) provides a simple way of defining an ergodic kernel that has the desired stationary
distribution p(x). Suppose we have a sample x(i) . A candidate x0 is generated by sampling from a symmetric proposal distribution q(x0 jx(i) ) (for example a Gaussian centered
at x(i) ). The candidate x0 is accepted as the next sample x(i+1) if p(x0 ) > p(x(i) ). If x0
has a lower probability, it can be still accepted, but only with probability p(x0 )=p(x(i) ).
The algorithm is initialized by generating the first sample x(0) according to an (arbitrary)
proposal distribution.
However for a given transition kernel T , it is hard to assess the time required to converge
to the stationary distribution so in practice one has to run the simulation until a very large
number of samples have been obtained, (see e.g., Roberts & Rosenthal, 1998). The choice
of the proposal distribution q is also very critical. A poor choice may lead to the rejection of
many candidates x0 hence resulting in a very slow convergence to the stationary distribution.
For a large class of probability models, where the full posterior p(x) is intractable, one
can still eciently compute marginals of form p(xk jx k ), x k = x1 : : : xk 1 ; xk+1 ; : : : xK
exactly. In this case one can apply a more specialized Markov chain Monte Carlo (MCMC)
algorithm, the Gibbs sampler given below.
1. Initialize x(0)
1:K by sampling from a proposal q(x1:K )
2. For i = 0 : : : N

1

54

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization

 For k = 1; : : : ; K , Sample

(i)
x(ki+1)  p(xk jx(1:i+1)
(15)
k 1 ; xk+1:K )
In contrast to the Metropolis algorithm, where the new candidate is a vector x0 , the
Gibbs sampler uses the exact marginal p(xk jx k ) as the proposal distribution. At each
step, the sampler updates only one coordinate of the current state x, namely xk , and the
new candidate is guaranteed to be accepted.
Note that, in principle we don't need to sample xk sequentially, i.e., we can choose k
randomly provided that each slice is visited equally often in the limit. However, a deterministic scan algorithm where k = 1; : : : K , provides important time savings in the type of
models that we consider here.
3.1 Simulated Annealing and Iterative Improvement
Now we shift our focus from sampling to MAP state estimation. In principle, one can use the
samples generated by any sampling algorithm (Metropolis-Hastings or Gibbs) to estimate
the MAP state x of p(x) by argmax p(x(i) ). However, unless the posterior is very much
i=1:N
concentrated around the MAP state, the sampler may not visit x even though the samples
x(i) are obtained from the stationary distribution. In this case, the problem can be simply
reformulated to sample not from p(x) but from a distribution that is concentrated at local
maxima of p(x). One such class of distributions are given by pj (x) / p(x)j . A sequence of
exponents 1 < 2 <    < j < : : : is called to be a cooling schedule or annealing schedule
owing to the inverse temperature interpretation of j in statistical mechanics, hence the
name Simulated Annealing (SA) (Aarts & van Laarhoven, 1985). When j ! 1 suciently
slowly in j , the cascade of MCMC samplers each with the stationary distribution pj (x) is
guaranteed (in the limit) to converge to the global maximum of p(x). Unfortunately, for this
convergence result to hold, the cooling schedule must go very slowly (in fact, logarithmically)
to infinity. In practice, faster cooling schedules must be employed.
Iterative improvement (II) (Aarts & van Laarhoven, 1985) is a heuristic simulated annealing algorithm with a very fast cooling schedule. In fact, j = 1 for all j . The eventual
advantage of this greedy algorithm is that it converges in a few iterations to a local maximum. By restarting many times from different initial configurations x, one hopes to find
different local maxima of p(x) and eventually visit the MAP state x . In practice, by using
the II heuristic one may find better solutions than SA for a limited computation time.
From an implementation point of view, it is trivial to convert MCMC code to SA (or II)
code. For example, consider the Gibbs sampler. To implement SA, we need to construct
a cascade of Gibbs samplers, each with stationary distribution p(x)j . The exact one time
slice marginal of this distribution is p(xk jx k )j . So, SA just samples from the actual
(temperature=1) marginal p(xk jx k ) raised to a power j .
3.2 The Switching State Space Model and MAP Estimation
To solve the rhythm quantization problem, we need to calculate the MAP state of the
posterior in Eq. 11
p(1:K jy0:K )

Z

/ p(1:K ) dz0:K p(y0:K jz0:K ; 1:K )p(z0:K j1:K )
55

(16)

fiCemgil & Kappen
This is a combinatorial optimization problem: we seek the maximum of a function p(1:K jy0:K )
that associates a number with each of the discrete configurations 1:K . Since it is not feasible
to visit all of the exponentially many configurations to find the maximizing configuration
1: K , we will resort to stochastic search algorithms such as simulated annealing (SA) and
iterative improvement (II). Due to the strong relationship between the Gibbs sampler and
SA (or II), we will first review the Gibbs sampler for the switching state space model.
The first important observation is that, conditioned on 1:K , the model becomes a linear
state space model and the integration on z0:K can be computed analytically using Kalman
filtering equations. Consequently, one can sample only 1:K and integrate out z . The
analytical marginalization, called Rao-Blackwellization (Casella & Robert, 1996), improves
the eciency of the sampler (e.g., see Doucet, de Freitas, Murphy, & Russell, 2000a).
Suppose now that each switch variable k can have S distinct states and we wish to
) ; i = 1 : : : N g. A naive implementation of the
generate N samples (i.e trajectories) f1:(iK
Gibbs sampler requires that at each step k we run the Kalman filter S times on the whole
observation sequence y0:K to compute the proposal p(k j1:(ik) 1 ; k(i+1:1)K ; y0:K ). This would
result in an algorithm of time complexity O(NK 2 S ) that is prohibitively slow when K is
large. Carter and Kohn (1996) have proposed a much more time ecient deterministic scan
Gibbs sampler that circumvents the need to run the Kalman filtering equations at each
step k on the whole observation sequence y0:K . See also (Doucet & Andrieu, 2001; Murphy,
2002).
The method is based on the observation that the proposal distribution p(k j ) can
be factorized as a product of terms that either depend on past observations y0:k or the
future observations yk+1:K . So the contribution of the future can be computed a-priori by
a backward filtering pass. Subsequently, the proposal is computed and samples k(i) are
generated during the forward pass. The sampling distribution is given by

p(k j k ; y0:K ) / p(k j k )p(y0:K j1:K )

(17)

where the first term is proportional to the joint prior p(k j k ) / p(k ;  k ). The second
term can be decomposed as

p(y0:K j1:K ) =
=

Z
Z

dzk p(yk+1:K jy0:k ; zk ; 1:K )p(y0:k ; zk j1:K )

(18)

dzk p(yk+1:K jzk ; k+1:K )p(y0:k ; zk j1:k )

(19)

Both terms are (unnormalized) Gaussian potentials hence the integral can be evaluated
analytically. The term p(yk+1:K jzk ; k+1:K ) is an unnormalized Gaussian potential in zk and
can be computed by backwards filtering. The second term is just the filtering distribution
p(zk jy0:k ; 1:k ) scaled by the likelihood p(y0:k j1:k ) and can be computed during forward
filtering. The outline of the algorithm is given below, see the appendix B.1 for details.
1. Initialize 1:(0)K by sampling from a proposal q(1:K )
2. For i = 1 : : : N

 For k = K 1; : : : ; 0,
56

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization



{ Compute p(yk+1:K jzk ; k(i+1:1)K )
For k = 1; : : : ; K ,
{ For s = 1 : : : S
 Compute the proposal
p(k = sj ) / p(k = s;  k )

Z

dzk p(y0:k ; zk j1:(ik) 1 ; k = s)p(yk+1:K jzk ; k(i+1:1)K )

{ Sample k(i) from p(k j )
The resulting algorithm has a time complexity of O(NKS ), an important saving in terms
of time. However, the space complexity increases from O(1) to O(K ) since expectations
computed during the backward pass need to be stored.
At each step, the Gibbs sampler generates a sample from a single time slice k. In
certain types of \sticky" models, such as when the dependence between k and k+1 is
strong, the sampler may get stuck in one configuration, moving very rarely. This is due to
the fact that most singleton ips end up in low probability configurations due to the strong
dependence between adjacent time slices. As an example, consider the quantization model
and two configurations [: : : k ; k+1 : : : ] = [: : : 1; 1 : : : ] and [: : : 3=2; 1=2 : : : ]. By updating
only a single slice, it may be dicult to move between these two configurations. Consider
an intermediate configuration [: : : 3=2; 1 : : : ]. Since the duration (k + k+1 ) increases, all
future quantization locations ck:K are shifted by 1=2. That may correspond to a score that
is heavily penalized by the prior, thus \blocking" the path.
To allow the sampler move more freely, i.e., to allow for more global jumps, one can
sample from L slices jointly. In this case the proposal distribution takes the form
p(k:k+L 1j ) / p(k:k+L 1;  (k:k+L 1)) 
Z
dzk+L 1 p(y0:k+L 1; zk+L 1 j1:(ik) 1 ; k:k+L 1)p(yk+L:K jzk+L 1 ; k(i+L1):K )
Similar to the one slice case, terms under the integral are unnormalized Gaussian potentials
(on zk+L 1 ) representing the contribution of past and future observations. Since k:k+L 1
has S L states, the resulting time complexity for generating N samples is O(NKS L ), thus in
practice L must be kept rather small. One remedy would be to use a Metropolis-Hastings
algorithm with a heuristic proposal distribution q(k:k+L 1jy0:K ) to circumvent exact calculation, but it is not obvious how to construct such a q.
One other shortcoming of the Gibbs sampler (and related MCMC methods) is that the
algorithm in its standard form is inherently oine; we need to have access to all of the
observations y0:K to start the simulation. For certain applications, e.g., automatic score
typesetting, a batch algorithm might be still feasible. However in scenarios that require
real-time interaction, such as in interactive music performance or tempo tracking, online
methods must be used.
3.3 Sequential Monte Carlo
Sequential Monte Carlo, a.k.a. particle filtering, is a powerful alternative to MCMC for
generating samples from a target posterior distribution. SMC is especially suitable for
application in dynamical systems, where observations arrive sequentially.

57

fiCemgil & Kappen
The basic idea in SMC is to represent the posterior p(x0:k 1 jy0:k 1 ) at time k 1 by
a (possibly weighted) set of samples fx(0:i)k 1 ; i = 1 : : : N g and extend this representation
to f(x(0:i)k 1 ; x(ki) ); i = 1 : : : N g when the observation yk becomes available at time k. The
common practice is to use importance sampling.
3.3.1 Importance Sampling

Consider again a high dimensional probability distribution p(x) = p (x)=Z with an unknown
normalization constant. Suppose we are given a proposal distribution q(x) that is close to
p(x) such that high probability regions of both distributions fairly overlap. We
P generate
independent samples, i.e., particles, x(i) from the proposal such that q(x)  Ni=1 (x
x(i) )=N . Then we can approximate
1 p (x)
q(x)
(20)
p(x) =
Z q(x)
N

X
 Z1 pq((xx)) N1 (x x(i) )
(21)
i=1
N
X
w(i)

(22)
PN (j) (x x(i) )
j =1 w
i=1
where w(i) = p (x(i) )=q(x(i) ) are the importance weights. One can interpret w(i) as correction factors to compensate for the fact that we have sampled from the \incorrect" distribution q(x). Given the approximation in Eq.22 we can estimate expectations by weighted
averages
N
X
hf (x)ip(x) 
w~ (i) f (x(i) )

where w~ (i) = w(i) =

PN
j

=1

(23)

i

=1 w

(j ) are the normalized importance weights.

3.3.2 Sequential Importance Sampling

Now we wish to apply importance sampling to the dynamical model

p(x0:K jy0:K )

/

K
Y
k

=0

p(yk jxk )p(xk jx0:k 1 )

(24)

where x = fz;  g. In principle one can naively apply standard importance sampling by using
an arbitrary proposal distribution q(x0:K ). However finding a good proposal distribution
can be hard if K  1. The key idea in sequential importance sampling is the sequential
construction of the proposal distribution, possibly using the available observations y0:k , i.e.,

q(x0:K jy0:K ) =

K
Y
k

=0

58

q(xk jx0:k 1 ; y0:k )

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization
Given a sequentially constructed proposal distribution, one can compute the importance
weight recursively as

p (x(0:i)k jy0:k ) p(yk jx(ki) )p(x(ki) jx(0:i)k 1 ; y0:k 1 ) p(y0:k 1 jx(0:i)k 1 )p(x(0:i)k 1 )
=
(25)
q(x(0:i)k jy0:k )
q(x(ki) jx(0:i)k 1 y0:k )
q(x(0:i)k 1 jy0:k 1 )
p(yk jx(ki) )p(x(ki) jx(0:i)k 1 ; y0:k 1 ) (i)
=
wk 1
(26)
q(x(ki) jx(0:i)k 1 y0:k )

wk(i) =

The sequential update schema is potentially more accurate than naive importance sampling since at each step k, one can generate a particle from a fairly accurate proposal
distribution that takes the current observation yk into account. A natural choice for the
proposal distribution is the filtering distribution given as

q(xk jx(0:i)k 1 y0:k ) = p(xk jx(0:i)k 1 ; y0:k )

(27)

In this case the weight update rule in Eq. 26 simplifies to

wk(i) = p(yk jx(0:i)k 1 )wk(i) 1
In fact, provided that the proposal distribution q is constructed sequentially and past sampled trajectories are not updated, the filtering distribution is the optimal choice in the sense
of minimizing the variance of importance weights w(i) (Doucet, Godsill, & Andrieu, 2000b).
Note that Eq. 27 is identical to the proposal distribution used in Gibbs sampling at k = K
(Eq 15). At k < K , the SMC proposal does not take future observations into account; so
we introduce discount factors wk to compensate for sampling from the wrong distribution.
3.3.3 Selection

Unfortunately, the sequential importance sampling may be degenerate, in fact, it can be
shown that the variance of wk(i) increases with k. In practice, after a few iterations of
the algorithm, only one particle has almost all of the probability mass and most of the
computation time is wasted for updating particles with negligible probability.
To avoid the undesired degeneracy problem, several heuristic approaches are proposed
in the literature. The basic idea is to duplicate or discard particles according to their
normalized importance weights. The selection procedure can be deterministic or stochastic. Deterministic selection is usually greedy; one chooses N particles with the highest
importance weights. In the stochastic case, called resampling, particles are drawn with a
probability proportional to their importance weight wk(i) . Recall that normalized weights
fw~k(i) ; i = 1 : : : N g can be interpreted as a discrete distribution on particle labels (i).

3.4 SMC for the Switching State Space Model
The SIS algorithm can be directly applied to the switching state space model by sampling
directly from xk = (zk ; k ). However, the particulate approximation can be quite poor if z

59

fiCemgil & Kappen
0.6

0.4

0.2



0

0.2

0.4

0.6

0.8
0.5

0

0.5

1

1.5


2

2.5

3

3.5

Figure 3: Outline of the algorithm. The ellipses correspond to the conditionals
p(zk jk(i) ; y0:k ). Vertical dotted lines denote the observations yk . At each step
k, particles with low likelihood are discarded. Surviving particles are linked to
their parents.

is high dimensional. Hence, too many particles may be needed to accurately represent the
posterior.
Similar to the MCMC methods introduced in the previous section, eciency can be
improved by analytically integrating out z0:k and only sampling from 1:k . In fact, this
form of Rao-Blackwellization is reported to give superior results when compared to standard
particle filtering where both  and z are sampled jointly (Chen & Liu, 2000; Doucet et al.,
2000b). The improvement is perhaps not surprising, since importance sampling performs
best when the sampled space is low dimensional.
The algorithm has an intuitive interpretation in terms of a randomized breadth first tree
search procedure: at each new step k, we expand N kernels to obtain S  N new kernels.
Consequently, to avoid explosion in the number of branches, we select N out of S  N
branches proportional to the likelihood, See Figure 3. The derivation and technical details
of the algorithm are given in the Appendix C.
The tree search interpretation immediately suggests a deterministic version of the algorithm where one selects (without replacement) the N branches with highest weight. We
will refer to this method as a greedy filter (GF). The method is also known as split-track
filter (Chen & Liu, 2000) and is closely related to Multiple Hypothesis Tracking (MHT)
(Bar-Shalom & Fortmann, 1988). One problem with the greedy selection schema of GF is
the loss of particle diversity. Even if the particles are initialized to different locations in z0 ,
(e.g., to different initial tempi), mainly due to the discrete nature of the state space of k ,
most of the particles become identical after a few steps k. Consequently, results can not
be improved by increasing the number of particles N . Nevertheless, when only very few
particles can be used, say e.g., in a real time application, GF may still be a viable choice.

60

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization

(i) is optimal. We
Figure 4: A hypothetical situation where neither of the two particles 1:5
would obtain eventually a higher likelihood configuration by interchanging 3
between particles.
3.5 SMC and estimation of the MAP trajectory
Like MCMC, SMC is a sampling method. Hence comments made in Section 3.1 about the
) jy )
eventual suboptimality of estimating the MAP trajectory from particles as arg max p(1:(iK
0:K
also apply here. An hypothetical situation is shown in figure 4.
One obvious solution is to employ the SA \trick" and raise the proposal distribution to
a power p(k j) . However, such a proposal will be peaked on a very few  at each time
slice. Consequently, most of the particles will become identical in time and the algorithm
eventually degenerates to greedy filtering.
An algorithm for estimating the MAP trajectory from a set of SMC samples is recently
proposed in the literature (Godsill, Doucet, & West, 2001). The algorithm relies on the
observation that once the particles x(ki) are sampled during the forward pass, one is left with
N
a discrete distribution defined on the (discrete) supportN
X1:K = K
k=1 Xk . Here Xk denotes
is the support of the filtering distribution a time k and is the Cartesian product between
S
sets. Formally, Xk is the set of distinct samples at time k and is given by Xk = i fx(ki) g.
The distribution p(X1:K jy1:K )3 is Markovian because the original state transition model
is Markovian, i.e., the posterior can be represented exactly by
p(X1:K jy1:K ) /

K
Y

p(yk jXk )p(Xk jXk 1 )
=1
Consequently, one can find the best MAP trajectory arg max p(X1:K ) by using an algorithm
that is analogous to the Viterbi algorithm for hidden Markov models (Rabiner, 1989).
However, this idea does not carry directly to the case when one applies Rao-Blackwellization. In general, when a subset of the hidden variables
NisK integrated out,Sall time
slices of the posterior p( 1:K jy1:k ) are coupled, where 1:K = k=1 k and k = i fk(i) g.
One can still employ a chain approximation and run Viterbi, (e.g., Cemgil & Kappen,
2002), but this does not guarantee to find arg max p( 1:K jy1:k ).
On the other hand, because k(i) are drawn from a discrete set, several particles become
identical so k has usually a small cardinality when compared to the number of particles
N . Consequently, it becomes feasible to employ SA or II on the reduced state space 1:K ;
possibly using a proposal distribution that extends over several time slices L.
k

3. By a slight abuse of notation we use the symbol Xk both as a set and as a general element when used
in the argument of a density, p(yk jXk ) means p(yk jxk ) s.t. xk 2 Xk

61

fiCemgil & Kappen
) ; i = 1 : : : N g, we
In practice, for finding the MAP solution from the particle set f1:(iK
) )p( (i) ) and apply iterative
propose to find the best trajectory i = arg maxi p(y0:K j1:(iK
1:K

improvement starting from the initial configuration 1:(iK) .

4. Simulations
We have compared the inference methods in terms of the quality of the solution and execution time. The tests are carried out both on artificial and real data.
Given the true notation 1:true
K , we measure the quality of a solution in terms of the
log-likelihood difference
L = log

p(y0:K j1:K )p(1:K )
true
p(y0:K j1:true
K )p(1:K )

and in terms of edit distance

e(1:K ) =

K
X

(1 (k

ktrue ))

=1
The edit distance e(1:K ) gives simply the number of notes that are quantized wrongly.
k

4.1 Artificial data: Clave pattern
(c = [1, 2, 4, 5:5,
The synthetic example is a repeating \son-clave" pattern
7 : : : ]) with uctuating tempo. We repeat the pattern 6 times and obtain a score 1:K with
K = 30.
Such syncopated rhythms are usually hard to transcribe and make it dicult to track
the tempo even for experienced human listeners. Moreover, since onsets are absent at
prominent beat locations, standard beat tracking algorithms usually loose track.
Given score 1:K , we have generated 100 observation sequences y0:K by sampling from
the tempo model in Eq. 7. We have parameterized the observation noise variance4 as
Q = k Qa + Qb . In this formulation, the variance depends on the length of the interval
between consecutive onsets; longer notes in the score allow for more tempo and timing
uctuation. For the tests on the clave example we have not used a prior model that reects
true source statistics, instead, we have used the generic prior model defined in Section 2.1
with  = 1.
All the example cases are sampled from the same score (clave pattern). However, due
to the use of the generic prior (that does not capture the exact source statistics well) and a
relatively broad noise model, the MAP trajectory 1: K given y0:K is not always identical to
;i
the original clave pattern. For the i'th example, we have defined the \ground truth" 1:true
K as
the highest likelihood solution found using any sampling technique during any independent
run. Although this definition of the ground truth introduces some bias, we have found
this exercise more realistic as well as more discriminative among various methods when
compared to, e.g.,, using a dataset with essentially shorter sequences where the exact MAP
7

7



>









>

4. The noise covariance parameters were R = 0:022 , Qa = 0:062 I and Qb = 0:022 I . I is a 2  2 identity
matrix.

62

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization
trajectory can be computed by exhaustive enumeration. The wish to stress that the main
aim of the simulations on synthetic dataset is to compare effectiveness of different inference
techniques; we postpone the actual test whether the model is a good one to our simulations
on real data.
We have tested the MCMC methods, namely Gibbs sampling (Gibbs), simulated annealing (SA) and iterative improvement (II) with one and two time slice optimal proposal
and for 10 and 50 sweeps. For each onset yk , the optimal proposal p(k j) is computed
always on a fixed set, = f0; 1=4; 2=4 : : : 3g. Figure 6 shows a typical run of MCMC.
Similarly, we have implemented the SMC for N = f1; 5; 10; 50; 100g particles. The
selection schema was random drawing from the optimal proposal p(k j) computed using
one or two time slices. Only in the special case of greedy filtering (GF), i.e., when N = 1, we
have selected the switch with maximum probability. An example run is shown in Figure 5.
We observe that on average SMC results are superior to MCMC (Figure 7). We observe
that, increasing the number of sweeps for MCMC does not improve the solution significantly.
On the other hand, increasing the number of particles seems to improve the quality of the
SMC solution monotonically. Moreover, the results suggest that sampling from two time
slices jointly (with the exception of SA ) does not have a big effect. GF outperforms a
particle filter with 5 particles that draws randomly from the proposal. That suggests that
for PF with a small number of particles N , it may be desirable to use a hybrid selection
schema that selects the particle with maximum weight automatically and randomly selects
the remaining N 1.
We compare inference methods in terms of execution time and the quality of solutions (as
measured by edit distance). As Figure 8 suggests, using a two slice proposal is not justified.
Moreover it seems that for comparable computational effort, SMC tends to outperform all
MCMC methods.

4.2 Real Data: Beatles
We evaluate the performance of the model on polyphonic piano performances. 12 pianists
were invited to play two Beatles songs, Michelle and Yesterday. Both pieces have a relatively
simple rhythmic structure with ample opportunity to add expressiveness by uctuating the
tempo. The original score is shown in Figure 9(a). The subjects had different musical education and background: four professional jazz players, four professional classical performers
and four amateur classical pianists. Each arrangement had to be played in three tempo
conditions, three repetitions per tempo condition. The tempo conditions were normal, slow
and fast tempo, all in a musically realistic range and all according to the judgment of the
performer. Further details are reported in (Cemgil et al., 2001).
4.2.1 Preprocessing

The original performances contained several errors, such as missing notes or additional notes
that were not on the original score. Such errors are eliminated by using a matching technique (Heijink, Desain, & Honing, 2000) based on dynamical programming. However, visual
inspection of the resulting dataset suggested still several matching errors that we interpret
as outliers. To remove these outliers, we have extended the quantization model with a two
state switching observation model, i.e., the discrete space consists of (k ; ik ). In this simple

63

fiCemgil & Kappen

1

0.8

0.6

0.4



0.2

0

0.2

0.4

0.6

0.8

1

0

2

4

6

8


10

12

14

16

Figure 5: Particle filtering on clave example with 4 particles. Each circle denotes the mean
(k(n) ; !k(n) ) where !k(n) = log2 k . The diameter of each particle is proportional
to the normalized importance weight at each generation. '*' denote the true
(; !) pairs; here we have modulated the tempo deterministically according to
!k = 0:3 sin(2ck =32), observation noise variance is R = 0:0252 .

64

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization

0

Log Likelihood

10

20
Gibbs
SA
II
GF
Desired

30

40

Figure 6:

1

10

20
30
Gibbs Sweep

40

50

Typical runs of Gibbs sampling, Simulated Annealing (SA) and Iterative Improvement
(II) on clave example. All algorithms are initialized to the greedy filter solution. The
annealing schedule for SA was linear from 1 = 0:1 to 33 = 10 and than proceeding deterministically by 34:50 = 1. When SA or II converge to a configuration, we reinitialize
by a particle filter with one particle that draws randomly proportional to the optimal
proposal. Sharp drops in the likelihood correspond to reinitializations. We see that, at
the first sweep, the greedy filter solution can only be slightly improved by II. Consequently the sampler reinitializes. The likelihood of SA drops considerably, mainly due to
the high temperature, and consequently stabilizes at a suboptimal solution. The Gibbs
sampler seems to explore the support of the posterior but is no able to visit the MAP
state in this run.

65

fiCemgil & Kappen

Log Likelihood Difference

0

5

10

15

20
1 Slice
2 Slice
25

SA

Gibbs

II

GF

PF

(a) Likelihood Difference
30

1 Slice
2 Slice

Edit Distance

25
20
15
10
5
0
SA

Gibbs

II

GF

PF

(b) Edit Distance. MCMC results with 10 sweeps are omitted.

Figure 7:

Comparison of inference methods on the clave data. The squares and ovals denote the
median and the vertical bars correspond to the interval between %25 and %75 quantiles.
We have tested the MCMC methods (Gibbs, SA and II) independently for 10 and 50
(shown from left to right). The SMC methods are the greedy filter (GF) and particle filter
(PF). We have tested filters with N = f5; 10; 50; 100g particles independently (shown
from left to right.).

66

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization

SA1

SA2

Median Edit Distance

20

15

PF25

PF15

PF110

10
GF1

II1 PF210

GF2

Gi1

Gi2
II2

5

0

PF150

PF1100

PF250
PF2100

Flops (log scale)

Figure 8:

Comparison of execution time in terms of oating point operations. For all methods, the
first number (1 or 2) denotes the number slices used by the optimal proposal distribution.
For the particle filter (PF), the second number denotes the number of particles. The
dashed lines are merely used to connect related methods.

outlier detection mechanism, each switch ik is a binary indicator variable specifying whether
the onset yk is an outlier or not. We assume that all indicators are independent a-priori
and have a uniform prior. The observation model is given by p(yk jik ; k ) = N (0; Rik ) 5 .
Since the score 1:K is known, the only unknown discrete quantities are the indicators i0:K .
We have used greedy filtering followed by iterative improvement to find the MAP state
of indicators i0:K and eliminated outliers in our further studies. For many performances,
there were around 2 4 outliers, less than 1% of all the notes. The resulting dataset can
be downloaded from the url http://www.snn.kun.nl/cemgil.
4.2.2 Parameter Estimation

We have trained tempo tracking models with different dimensionality D, where D denotes
the dimension of the hidden variable z . In all of the models, we use a transition matrix that
has the form in Eq. 8.
Since the true score is known, i.e., the quantization location ck of each onset yk is given,
we can clamp all the discrete variables in the model. Consequently, we can estimate the
observation noise variance R, the transition noise variance Q and the transition matrix
coecients A from data.
We have optimized the parameters by Expectation-Maximization (EM) for the linear
dynamical systems (Shumway & Stoffer, 1982; Ghahramani & Hinton, 1996) using all perfor5. We took Rik =0 = 0:002 and Rik =1 = 2.

67

fiCemgil & Kappen
mances of \Yesterday" as training data. Similarly, the score prior parameters are estimated
by frequency counts from the score of \Yesterday" 6 . All tests are carried out on \Michelle".
4.2.3 Results

In Figure 9 we show the result of typesetting a performance with and without tempo
tracking. Due to uctuations in tempo, the quality of the automatically generated score is
very poor. The quality can be significantly improved by using our model.
Figure 10 shows some tempo tracking examples on Michelle dataset for pianists from
different background and training. We observe that in most cases the results are satisfactory.
In Figure 11, we give a summary of test results on Michelle data in terms of the loglikelihood and edit distance as a function of model order and number of particles used for
inference. Figure 11(a) shows that the median likelihood on test data is increasing with
model order. This suggests that a higher order filter is able to capture structure in pianists' expressive timing. Moreover, as for the sythetic data, we see a somewhat monotonic
increase in the likelihood of solutions found when using more particles.
The edit distance between the original score and the estimates are given in Figure 11(b).
Since both pieces are arranged for piano, due to polyphony, there are many onsets that are
associated with the same quantization location. Consequently, many ktrue in the original
score are effectively zero. In such cases, typically, the corresponding inter onset interval
yk yk 1 is also very small and the correct quantization (namely k = 0) can be identified
even if the tempo estimate is completely wrong. As a consequence, the edit distance remains
small. To make the task slightly more challenging, we exclude the onsets with ktrue = 0
from edit distance calculation.
We observe that the extra prediction ability obtained using a higher order model does
not directly translate to a better transcription. The errors are around 5% for all models.
On the other hand, the variance of edit distance for higher order models is smaller. This
suggests that higher order models tend to be more robust against divergence from the
original tempo track.

5. Discussion
We have presented a switching state space model for joint rhythm quantization and tempo
tracking. The model describes the rhythmic structure of musical pieces by a prior distribution over quantization locations. In this representation, it is easy to construct a generic
prior that prefers simpler notations and to learn parameters from a data set. The prior on
quantization locations c0:K translates to a non-Markovian distribution over a score 1:K .
Timing deviations introduced by performers (tempo uctuation, accentuations and motor errors) are modeled as independent Gaussian noise sources. Performer specific timing
preferences are captured by the parameters of these distributions.
Given the model, we have formulated rhythm quantization as a MAP state estimation
problem and tempo tracking as a filtering problem. We have introduced Markov chain
6. The maximum likelihood parameters for a model of dimension D = 3 are found to be: a = 0:072, R =
0:0132 and q = 0:0082 , q1 = 0:0072 and q2 = 0:0502 . The prior p(c) is p(0) = 0:80, p(1=3) = 0:0082,
p(1=2) = 0:15 p(5=6) = 0:0418. Remaining p(c) are set to 10 6 .

68

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization

Michelle
Lennon/McCartney

bb 4 




& b b 4   n     n   ww         n   


w
? b b b 44
w



w
b 


1

Piano

   
  

   
   

6

bb   
& b b 
 
? bb
bb 


  
 n  


n 


bb
& b b  n   


? bb

bb 

  

 

6

11







n



n

 n  n  .   w n 
  . J
  


n
 n n

  
 n  


n 





3

 n  n   
  

n

n

n

      

 
 

      
   

        



   


 

      
 


  
 
           

              




   
            

 
                        
 


  


           


















 
      
 
   
  


 
  



   

   
 
   


      
           

      
                     






    

 






   

10

3

13

17
3

bb
& b b  . n.
 .
? bb 
bb
16

n n 


bb 
& b b      

J
? bb
bb
n 

21

bb
& b b n w
w
? bb 
bb

3

j

        ..  .


 b     ..

 ..

 



j
 .
 ..

  


   

n



 



 

 n



j
 ww
w

 

.

   

n




j
 


   







20

24

  

  
    
  
 

27

26



n ww
w

 n n

w



(a) Original Score






































(b) Typesetting without
processing by the model.
Due to uctuations in
tempo, the quality of the
score is poor.

Figure 9:



  
         
  




     
  
       

   

 
  
    
        
  
 
 
   
   
q = 130

6

11

16

3

    
   
 
   







3

 
 



3











 



        



  
 

    









   
 
 
               
   

   


 
 
  

  
  
    
 

  
       
  






  


 

  
  




   

20



24

(c) Typesetting after tempo
tracking and quantization
with a particle filter.

Results of Typesetting the scores.

69



  
  

 
  
  




         
       
 
      
 

           



fiCemgil & Kappen

0

0
Estimated
Original

Estimated
Original

0.4

0.4
2

log 

2

log 

k

0.2

k

0.2

0.6

0.6

0.8

0.8

1

0

10

20

30



40

50

60

1
10

70

0

10

20

30

k



40

50

60

70

80

k

(a) Professional Jazz Pianist

(b) Amateur

0

0
Estimated
Original

Estimated
Original

0.2

0.2

0.4
0.6

2

log 

2

log 

k

0.8

k

0.4

0.6

1

1.2
1.4

0.8

1.6
1.8

1
10

0

10

20

30



40

50

60

70

80

k

0

10

20

30



40

50

60

70

k

(c) Professional Classical Pianist. The
filter temporarily loses track.

Figure 10:

2

(d) Tracking at twice the rate of the
original tempo.

Examples of filtered estimates
of z0:K = [k ; k ]T from the Beatles data set. Circles
original
denote the mean of p(zk j1:k ; y0:k ) and \x" denote mean p(zk j1: k ; y0:k ) obtained by
SMC. It is interesting to note different timing characteristics. For example the classical
pianist uses a lot more tempo uctuation than the professional jazz pianist. Jazz pianist
slows down dramatically at the end of the piece, the amateur \rushes", i.e., constantly
accelerates at the beginning. The tracking and quantization results for (a) and (b)
are satisfactory. In (a), the filter loses track at the last two notes, where the pianist
dramatically slows down. In (c), the filter loses track but catches up again. In (d), the
filter jumps to a metrical level that is twice as fast as the original performance. That
would translate to a duplication in note durations only.

70

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization

500

480

Likelihood

460

440

420

400

380

360

2

3
Model Dimension

4

(a) Likelihood. The dashed horizontal line shows the median
likelihood of the original score of Michelle under each model.
50

45

40

Percent Edit Distance

35

30

25

20

15

10

5

0

2

3
Model Dimension

4

(b) Edit Distance

Figure 11:

SMC results on the test data (108 performances of Michelle). For each model we show
the results obtained with N = 1; 10; 20 and 50 particles. The \-" show the median of
the best particle and \x" denote the median after applying iterative improvement. The
vertical bars correspond to the interval between %25 and %75 quantiles.
71

fiCemgil & Kappen
Monte Carlo (MCMC) and sequential Monte Carlo (SMC) to approximate the respective
distributions.
The quantization model we propose is similar to that of (Raphael, 2001a). For transcription, Raphael proposes to compute arg max p(c0:K ; z0:K jy0:K ) and uses a message propagation scheme that is essentially analogous to Rao-Blackwellized particle filtering. To prevent
the number of kernels from explosion, he uses a deterministic selection method, called
\thinning". The advantage of Raphael's approach is that the joint MAP trajectory can
be computed exactly, provided that the continuous hidden state z is one dimensional and
the model is in a parameter regime that keeps the number of propagated Gaussian kernels
limited, e.g., if R is small, thinning can not eliminate many kernels. One disadvantage is
that the number of kernels varies depending upon the features of the filtering distribution;
it is dicult to implement such a scheme in real time. Perhaps more importantly, simple extensions such as increasing the dimensionality of z or introducing nonlinearities to
the transition model would render the approach quickly invalid. In contrast, Monte Carlo
methods provide a generic inference technique that allow great exibility in models one can
employ.
We have tested our method on a challenging artificial problem (clave example). SMC
has outperformed MCMC in terms of the quality of solutions, as measured in terms of the
likelihood as well as the edit distance. We propose the use of SMC for both problems. For
finding the MAP quantization, we propose to apply iterative improvement (II) to the SMC
solution on the reduced configuration space.
The correct choice of the score prior is important in the overall performance of the
system. Most music pieces tend to have a certain rhythmical vocabulary, that is certain
rhythmical motives reoccur several times in a given piece. The rhythmic structure depends
mostly upon the musical genre and composer. It seems to be rather dicult to devise
a general prior model that would work well in a large spectrum of styles. Nevertheless,
for a given genre, we expect a simple prior to capture enough structure sucient for good
transcription. For example, for the Beatles dataset, we have estimated the prior by counting
from the original score of \Yesterday". The statistics are fairly close to that of \Michelle".
The good results on the test set can be partially accounted for the fact that both pieces
have a similar rhythmical structure.
Conditioned on the score, the tempo tracking model is a linear dynamical system. We
have optimized several tempo models using EM where we have varied the dimension of
tempo variables z . The test results suggest that increasing the dimensionality of z improves
the likelihood. However, increase in the likelihood of the whole dataset does not translate
directly to overall better quantization results (as measured by edit distance). We observe
that models trained on the whole training data fail consistently for some subjects, especially
professional classical pianists. Perhaps interestingly, if we train \custom" models specifically
optimized for the same subjects, we can improve results significantly also on test cases.
This observation suggests a kind of multimodality in the parameter space where modes
correspond to different performer regimes. It seems that a Kalman filter is able to capture
the structure in expressive timing deviations. However, when averaged over all subjects,
these details tend to be wiped out, as suggested by the quantization results that do not
vary significantly among models of different dimensions.

72

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization
A related problem with the edit distance measure is that under an \average" model, the
likelihood of the desired score (e.g., original score of \Michelle") may have a lower likelihood
than a solution found by an inference method. In such cases increasing the likelihood may
even decrease the edit distance. In some test cases we even observe solutions with a higher
likelihood than the original notation where all notes are wrong. In most of these cases, the
tempo trajectory of the solution correspond to the half or twice of the original tempo so
consequently all note durations are halved or doubled (e.g., all whole notes are notated as
half notes, all half notes as quarters e.t.c.). Considering the fact that the model is \self
initializing" its tempo, that is we assume a broad uncertainty a-priori, the results are still
satisfactory from a practical application perspective.
One potential shortcoming of our model is that it takes only timing information of onsets
into account. In reality, we believe that pitch and melodic grouping as well as articulation
(duration between note onsets and offsets) and dynamics (louder or softer) provide useful
additional information for tempo tracking as well as quantization. Moreover, current model
assumes that all onsets are equally relevant for estimation. That is probably in general not
true: for example, a kick-drum should provide more information about the tempo than a
ute. On the other hand, our simulations suggest that even from such a limited model one
can obtain quite satisfactory results, at least for simple piano music.
It is somewhat surprising, that SMC, basically a method that samples from the filtering
distribution outperforms an MCMC method such as SA that is specifically designed for
finding the MAP solution given all observations. An intuitive explanation for relatively
poorer MCMC results is that MCMC proceeds first by proposing a global solution and then
tries to improve it by local adjustments. A human transcriber, on the other hand, would
listen to shorter segments of music and gradually write down the score. In that respect,
the sequential update schema of SMC seems to be more natural for the rhythm transcription problem. Similar results, where SMC outperforms MCMC are already reported in the
literature, e.g., in the so-called \Growth Monte Carlo" for generating self-avoiding random
walks (Liu, Chen, & Logvinenko, 2001). It seems that for a large class of dynamical problems, including rhythm transcription, sequential updating is preferable over batch methods.
We note that theoretical convergence results for SA require the use of a logarithmic
cooling schedule. It seems that our cooling schedule was too fast to meet this requirement;
so one has to be still careful in interpreting the poor performance as a negative SA result.
We maintain that by using a richer neighborhood structure in the configuration space (e.g.,
by using a block proposal distribution) and a slower cooling schedule, SA results can be
improved significantly. Moreover, MCMC methods can be also be modified to operate
sequentially, for example see (Marthi, Pasula, Russell, & Peres, 2002).
Another family of inference methods for switching state space models rely on deterministic approximate methods. This family includes variational approximations (Ghahramani &
Hinton, 1998) and expectation propagation (Heskes, 2002). It remains an interesting open
question whether deterministic approximation methods provide an advantage in terms of
computation time and accuracy; in particular for the quantization problem and for other
switching state space models. A potential application of the deterministic approximation
techniques in a MCMC schema can be in designing proposal distributions that extend over
several time slices. Such a schema would circumvent the burden for computing the optimal
proposal distribution exhaustively hence allowing more global moves for the sampler.

73

fiCemgil & Kappen
Our current results suggest the superiority of SMC for our problem. Perhaps the most
important advantage of SMC is that it is essentially an \anytime" algorithm; if we have
a faster computer we can increase the number of particles to make use of the additional
computational power. When computing time becomes short one can decrease the number
of samples. These features make SMC very attractive for real-time applications where one
can easily tune the quality/computation-time tradeoff.
Motivated by the practical advantages of SMC and our positive simulation results, we
have implemented a prototype of SMC method in real-time. Our current computer system
(a 800 MHz P3 laptop PC running MS Windows) allows us to use up to 5 particles with
almost no delay even during busy passages. We expect to significantly improve the eciency
by translating the MATLABc constructs to native C code. Hence, the method can be used
as a tempo tracker in an automatic interactive performance system and as a quantizer in
an automatic score typesetting program.

Acknowledgments
This research is supported by the Technology Foundation STW, applied science division
of NWO and the technology programme of the Dutch Ministry of Economic Affairs. We
would like to thank the associate editor Daphne Koller and the anonymous reviewers for
their comments that helped us significantly to improve the article. We would also like to
thank to Ric Ashley, Peter Desain, Henkjan Honing and Paul Trilsbeek for their suggestions
and contributions in data collection. Moreover we gratefully acknowledge the pianists from
Northwestern University and Nijmegen University for their excellent performances.

Appendix A. A generic prior model for quantization locations c
In traditional western music notation, note durations are generated by recursive subdivisions
starting from a whole note, hence it is also convenient to generate quantization locations
in a similar fashion by regular subdivisions. We decompose a quantization location into an
integer part and a fraction: c = bcc + (c mod 1). For defining a prior, we will only use the
fraction.
The set of all fractions can be generated by recursively subdividing the unit interval
[0; 1). We let S = [si ] denote a subdivision schema, where [si ] is a (finite) sequence of
arbitrary integers (usually small primes such as 2,3 or 5). The choice of a particular S
depends mainly on the assumed time signature. We generate the set of fractions C as
follows: At first iteration, we divide the unit interval into s1 intervals of equal length and
append the endpoints c0 of resulting intervals into the set C . At each following iteration i,
we subdivide all intervals generated by the previous iteration into si equal parts and append
all resulting endpoints to C . Note that thisQprocedure generates a regular grid where two
neighboring grid points have the distance 1= i si . We denote the iteration number at which
the endpoint c0 is first inserted to C as the depth of c0 (with respect to S ). This number will
be denoted as d(c0 jS ). It is easy to see that this definition of d coincides with the number
of significant bits to represent c mod 1 when S = [2; 2; : : : ].
As an illustirative example consider the subdivision S = [3; 2]. At the first iteration, the
unit interval is divided into s1 = 3 equal intervals, and the resulting endpoints 0, 1=3, and

74

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization
2=3 are inserted into C with depths d(0) = d(1=3) = d(2=3) = 1. At the second iteration,
the new endpoints 1=6, 3=6 and 5=6 are inserted to C and are assigned the depth 2.
Given an S , we can define a distribution on quantization locations

p(ck jS ) / exp( d(ck mod 1jS ))
If we wish to consider several time signatures, i.e., different subdivision schemata, we can
interpret S as a hidden indicator variable and define
P a prior p(S ). In this case, the prior
becomes a multinomial mixture given by p(ck ) = S p(ck jS )p(S ). For further details and
empirical results justifying such a choice see (Cemgil et al., 2000).

Appendix B. Derivation of two pass Kalman filtering Equations
Consider a Gaussian potential with mean  and covariance  defined on some domain
indexed by x.
1
1
(28)
(x) = Z  N (; ) = Z j2j 2 exp( (x )T  1 (x ))
2
R
where dx(x) = Z > 0. If Z = 1 the potential is normalized. The exponent in Eq. 28 is
a quadratic form so the potential can be written as
1 T
x Kx)
(29)
(x) = exp(g + hT x
2
where
1
K 1 T 1
K= 1
h=  1 
g = log Z + log j j
h K h
2
2 2
To denote a potential in canonical form we will use the notation

(x) = Z  N (; )  [h; K; g]
and we will refer to g, h and K as canonical parameters. Now we consider a Gaussian
potential on (x1 ; x2 )T . The canonical representation is

(x1 ; x2 ) =



h1
h2

 
;

K11 K12
K21 K22

 
;g

In models where several variables are interacting, one can find desired quantities by applying
three basic operations defined on Gaussian potentials. Those are multiplication, conditioning, and marginalization. The multiplication of two Gaussian potentials on the same index
set x follows directly from Eq. 29 and is given by
0 (x) = a (x)  b (x)
[h0 ; K 0 ; g0 ] = [ha ; Ka ; ga ]  [hb ; Kb ; gb ] = [ha + hb ; Ka + Kb ; ga + gb ]
If the domain of a and b only overlaps on a subset, then potentials are extended to the
appropriate domain by appending zeros to the corresponding dimensions.

75

fiCemgil & Kappen
The marginalization operation is given by

(x1 ) =

Z

K12 K221 h2 ; K11

(x1 ; x2 ) = [h1

x2

K12 K221 K21 ; g0 ]

where g0 = g 21 log jK22 =2 j + 21 h2 T (K22 ) 1 h2 and g is the initial constant term of (x1 ; x2 ).
The conditioning operation is given by
(x1 ; x2 = x^2 ) = [h1 K12 x^2 ; K11 ; g0 ]

1 x^T K22 x^2 .
2 2

where g0 = g + hT2 x^2

B.1 The Kalman Filter Recursions
Suppose we are given the following linear model subject to noise
zk = Azk 1 + k
yk = Czk + k
where A and C are constant matrices, k  N (0; Q) and k  N (0; R)
The model encodes the joint distribution
K
Y

p(yk jzk )p(zk jzk 1 )
=1
p(z1 jz0 ) = p(z1 )

p(z1:K ; y1:K ) =

k

(30)
(31)

1
1 T 1
p(z1 ) = [P 1 ; P 1 ; log j2P j
 P ]
2 
   T2 1

TR 1
1
0
C
R
C
C
p(y1 jz1 ) =
;
; log j2Rj
0
R 1C
R 1
2
1
1 T 1
p(y1 = y^1 jz1 ) = [0 + C T R 1 y^1 ; C T R 1 C; log j2Rj
y^ R y^1 ]
2
2 1

   T 1

TQ 1
1
0
A
Q
A
A
p(z2 jz1 ) =
; log j2Qj
0 ;
Q 1A
Q 1
2
:::
B.1.1 Forward Message Passing

Suppose we wish to compute the likelihood

p(y1:K ) =

Z

zK

p(yK jzK ) : : :

Z

z2

p(z3 jz2 )p(y2 jz2 )

Z
z1

p(z2 jz1 )p(y1 jz1 )p(z1 )

7 We can compute this integral by starting from z1 and proceeding to zK . We define forward
\messages" ff as
R
R
7. We let z  dz

76

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization

 ff1j0 = p(z1 )
 k=1:K
{ ffkjk = p(yk = y^k jzk )ffkjk 1
R
{ ffk+1jk = zk p(zk+1 jzk )ffkjk
The forward recursion is given by
 ff1j0 = [P 1 ; P 1; 12 log j2P j
 k = 1:::K

1 T P 1 ]
2

{ ffkjk = [hkjk ; Kkjk ; gkjk ]
hkjk = C T R 1 y^k + hkjk 1
Kkjk = C T R 1 C + Kkjk 1
gkjk = gkjk 1 21 log j2Rj 12 y^1T R 1 y^k
{ ffk+1jk = [hk+1jk ; Kk+1jk ; gk+1jk ]
Mk = (AT Q 1 A + Kkjk ) 1
hk+1jk = Q 1 AMk hkjk
Kk+1jk = Q 1 Q 1 AMk AT Q 1
gk+1jk = gkjk 21 log j2Qj + 12 log j2Mk j + 12 hTkjk Mk hkjk
B.1.2 Backward Message Passing

We can compute the likelihood also by starting from yK .

p(y1:K ) =

Z

z1

p(z1 )p(y1 jz1 )

Z

z2

p(z2 jz1 )p(y2 jz2 ) : : :

Z
zK

In this case the backward propagation can be summarized as

 fiK jK +1 = 1
 k = K :::1
{ fikjk = p(yk = y^k jzk )fikjk+1
R
{ fik 1jk = zk p(zk jzk 1 )fikjk
The recursion is given by

 [hK jK +1; KK jK +1; gK jK +1] = [0; 0; 0]
 k = K :::1
{ fikjk = [hkjk ; Kkjk ; gkjk ]
hkjk = C T R 1 y^k + hkjk+1
Kkjk = C T R 1 C + Kkjk+1

77

p(zK jzK 1 )p(yK jzK )

fiCemgil & Kappen
gkjk = 21 log j2Rj 12 y^kT R 1 y^k + gkjk+1
{ fik 1jk = [hk 1jk ; Kk 1jk ; gk 1jk ]
Mk = (Q 1 + Kkjk ) 1
hk 1jk = AT Q 1 Mk hkjk
Kk 1jk = AT Q 1 (Q Mk )Q 1 A
gk 1jk = gkjk 21 log j2Qj + 12 log j2Mk j + 12 h Tkjk Mk hkjk
B.2 Kalman Smoothing
Suppose we wish to find the distribution of a particular zk given all the observations y1:K .
We just have to combine forward and backward messages as
p(zk jy1:K )

/ p(yk+1:K ; zk ; y1:k )

= p(y1:k ; zk )p(yk+1:K jzk )
= ffkjk  fikjk+1
= [hkjk + hkjk+1 ; Kkjk + Kkjk+1; gkjk + gkjk+1]

Appendix C. Rao-Blackwellized SMC for the Switching State space
Model
We let i = 1 : : : N be an index over particles and s = 1 : : : S an index over states of  . We
denote the (unnormalized) filtering distribution at time k 1 by
(ki) 1 =^ p(y0:k 1; zk 1 j1:(ik) 1 )
Since y0:k 1 are observed, (ki) 1 is a Gaussian potential on zk 1 with parameters Zk(i) 1 
N
((i) ; (i) ). Note that the normalization constant Zk(i) 1 is the data likelihood p(y0:k 1j1:(ik) 1 ) =
R k (1i) k 1
dzk k 1 . Similarly, we denote the filtered distribution at the next slice conditioned on
k = s by

(ksji) =
^

Z

dzk 1 p(yk jzk )p(zk jzk 1 ; k = s)(ki) 1
(32)
= p(y0:k ; zk j1:(ik) 1 ; k = s)
We denote the normalization constant of (ksji) by Zk(sji). Hence the joint proposal on s and
(i) is given by
qk(sji) =

Z

dzk (ksji)  p(k = s; 1:(ik) 1 )
= p(k = s; 1:(ik) 1 ; y0:k )

The outline of the algorithm is given below:
 Initialize. For i = 1 : : : N , (0i) p(y0; x0)

78

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization

 For k = 1 : : : K
{ For i = 1 : : : N , s = 1 : : : S
Compute (ksji) from (ki) 1 using Eq.32.
qk(sji) Zk(sji)  p(k = s; 1:(ik) 1 )
{ For i = 1 : : : N
Select a tuple (sjj )  qk
1:(ik) (1:(jk) 1 ; k = s)
(ki) (ksjj )
P (sjj)
wk(i)
s qk
Note that the procedure has a \built-in" resampling schema for eliminating particles
with small importance weight. Sampling jointly on (sji) is equivalent to sampling a single
s for each i and then resampling i according to the weights wk(i) . One can also check that,
since we are using the optimal proposal distribution of Eq.27, the weight at each step is
given by wk(i) = p(1:(ik) 1 ; y0:k ).

References

Aarts, E. H. L., & van Laarhoven, P. J. M. (1985). Statistical cooling: A general approach to
combinatorial optimization problems. Philips Journal of Research, 40 (4), 193{226.
Agon, C., Assayag, G., Fineberg, J., & Rueda, C. (1994). Kant: A critique of pure quantification.
In Proceedings of the International Computer Music Conference, pp. 52{9, Aarhus, Denmark.
International Computer Music Association.
Andrieu, C., de Freitas, N., Doucet, A., & Jordan, M. I. (2002). An introduction to MCMC for
machine learning. Machine Learning, to appear.
Bar-Shalom, Y., & Fortmann, T. E. (1988). Tracking and Data Association. Academic Press.
Bar-Shalom, Y., & Li, X.-R. (1993). Estimation and Tracking: Principles, Techniques and Software.
Artech House, Boston.
Cambouropoulos, E. (2000). From MIDI to traditional musical notation. In Proceedings of the AAAI
Workshop on Artificial Intelligence and Music: Towards Formal Models for Composition, Performance and Analysis, Austin, Texas.
Carter, C. K., & Kohn, R. (1996). Markov Chain Monte Carlo in conditionally Gaussian state space
models. Biometrika, 83 (3), 589{601.
Casella, G., & Robert, C. P. (1996). Rao-Blackwellisation of sampling schemas. Biometrika, 83,
81{94.
Cemgil, A. T., Desain, P., & Kappen, H. J. (2000). Rhythm quantization for transcription. Computer
Music Journal, 24:2, 60{76.
Cemgil, A. T., & Kappen, H. J. (2002). Rhythm quantization and tempo tracking by sequential
Monte Carlo. In Dietterich, T. G., Becker, S., & Ghahramani, Z. (Eds.), Advances in Neural
Information Processing Systems 14. MIT Press.
Cemgil, A. T., Kappen, H. J., Desain, P., & Honing, H. (2001). On tempo tracking: Tempogram
representation and Kalman filtering. Journal of New Music Research, 28:4, 259{273.
Chen, R., & Liu, J. S. (2000). Mixture Kalman filters. J. R. Statist. Soc., 10.
79

fiCemgil & Kappen
Dannenberg, R. (1984). An on-line algorithm for real-time accompaniment. In Proceedings of ICMC,
pp. 193{198, San Francisco.
Desain, P., & Honing, H. (1991). Quantization of musical time: a connectionist approach. In Todd,
P. M., & Loy, D. G. (Eds.), Music and Connectionism., pp. 150{167. MIT Press., Cambridge,
Mass.
Desain, P., & Honing, H. (1994). A brief introduction to beat induction. In Proceedings of ICMC,
San Francisco.
Dixon, S., & Cambouropoulos, E. (2000). Beat tracking with musical knowledge. In Horn, W. (Ed.),
Proceedings of ECAI 2000 (14th European Conference on Artificial Intelligence), Amsterdam.
Doucet, A., & Andrieu, C. (2001). Iterative algorithms for state estimation of jump Markov linear
systems. IEEE Trans. on Signal Processing, 49 (6), 1216{1227.
Doucet, A., de Freitas, N., & Gordon, N. J. (Eds.). (2001). Sequential Monte Carlo Methods in
Practice. Springer-Verlag, New York.
Doucet, A., de Freitas, N., Murphy, K., & Russell, S. (2000a). Rao-Blackwellised particle filtering
for dynamic Bayesian networks. In Uncertainty in Artificial Intelligence.
Doucet, A., Godsill, S., & Andrieu, C. (2000b). On sequential Monte Carlo sampling methods for
Bayesian filtering. Statistics and Computing, 10 (3), 197{208.
Fox, D., Burgard, W., & Thrun, S. (1999). Markov localization for mobile robots in dynamic
environments. Journal of Artificial Intelligence Research (JAIR), 11.
Ghahramani, Z., & Hinton, G. (1998). Variational learning for switching state-space models. Neural
Computation, 12 (4), 963{996.
Ghahramani, Z., & Hinton, G. E. (1996). Parameter estimation for linear dynamical systems. (crgtr-96-2). Tech. rep., University of Totronto. Dept. of Computer Science.
Godsill, S., Doucet, A., & West, M. (2001). Maximum a posteriori sequence estimation using Monte
Carlo particle filters. Annals of the Institute of Statistical Mathematics, 52 (1), 82{96.
Gordon, N. J., Salmond, D. J., & Smith, A. F. M. (1993). Novel approach to nonlinear/nonGaussian Bayesian state estimation. In IEE Proceedings Part F, Radar and Signal Processing,
Vol. 140(2), pp. 107{113.
Goto, M., & Muraoka, Y. (1998). Music understanding at the beat level: Real-time beat tracking
for audio signals. In Rosenthal, D. F., & Okuno, H. G. (Eds.), Computational Auditory Scene
Analysis.
Grubb, L. (1998). A Probabilistic Method for Tracking a Vocalist. Ph.D. thesis, School of Computer
Science, Carnegie Mellon University, Pittsburgh, PA.
Hamanaka, M., Goto, M., Asoh, H., & Otsu, N. (2001). A learning-based quantization: Estimation of
onset times in a musical score. In Proceedings of the 5th World Multi-conference on Systemics,
Cybernetics and Informatics (SCI 2001), Vol. X, pp. 374{379.
Heijink, H., Desain, P., & Honing, H. (2000). Make me a match: An evaluation of different approaches
to score-performance matching. Computer Music Journal, 24(1), 43{56.
Heskes, T. Zoeter, O. (2002). Expectation propagation for approximate inference in dynamic
Bayesian networks. In Proceedings UAI.
Isard, M., & Blake, A. (1996). Contour tracking by stochastic propagation of conditional density.
In ECCV (1), pp. 343{356.
Large, E. W., & Jones, M. R. (1999). The dynamics of attending: How we track time-varying events.
Psychological Review, 106, 119{159.
Liu, J. S., Chen, R., & Logvinenko, T. (2001). A theoretical framework for sequential importance
sampling with resaampling. In Doucet, A., de Freitas, N., & Gordon, N. J. (Eds.), Sequential
Monte Carlo Methods in Practice, pp. 225{246. Springer Verlag.
80

fiMonte Carlo Methods for Tempo Tracking and Rhythm Quantization
Longuet-Higgins, H. C. (1987). Mental Processes: Studies in Cognitive Science. MIT Press, Cambridge. 424p.
Marthi, B., Pasula, H., Russell, S., & Peres, Y. (2002). Decayed MCMC filtering. In Proceedings of
UAI.
Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., & Teller, E. (1953). Equations of state
calculations by fast computing machines. Journal of Chemical Physics, 21, 1087{1091.
Metropolis, N., & Ulam, S. (1949). The Monte Carlo method. Journal of the American Statistical
Assoc., 44(247), 335{341.
Murphy, K. P. (2002). Dynamic Bayesian Networks: Representation, Inference and Learning. Ph.D.
thesis, University of California, Berkeley.
Pressing, J., & Lawrence, P. (1993). Transcribe: A comprehensive autotranscription program.. In
Proceedings of the International Computer Music Conference, pp. 343{345, Tokyo. Computer
Music Association.
Rabiner, L. R. (1989). A tutorial in hidden Markov models and selected applications in speech
recognation. Proc. of the IEEE, 77 (2), 257{286.
Raphael, C. (2001a). A mixed graphical model for rhythmic parsing. In Proc. of 17th Conf. on
Uncertainty in Artif. Int. Morgan Kaufmann.
Raphael, C. (2001b). A probabilistic expert system for automatic musical accompaniment. Journal
of Computational and Graphical Statistics, 10 (3), 467{512.
Roberts, G. O., & Rosenthal, J. S. (1998). Markov Chain Monte Carlo: Some practical implications
of theoretical results. Canadian Journal of Statistics, 26, 5{31.
Scheirer, E. D. (1998). Tempo and beat analysis of acoustic musical signals. Journal of Acoustical
Society of America, 103:1, 588{601.
Shumway, R. H., & Stoffer, D. S. (1982). An approach to time series smoothing and forecasting
using the em algorithm. J. Time Series Analysis, 3 (4), 253{264.
Tanizaki, H. (2001). Nonlinear and non-Gaussian state-space modeling with Monte Carlo techniques:
A survey and comparative study. In Rao, C., & Shanbhag, D. (Eds.), Handbook of Statistics,
Vol.21: Stochastic Processes: Modeling and Simulation. North-Holland.
Thom, B. (2000). Unsupervised learning and interactive jazz/blues improvisation. In Proceedings of
the AAAI2000. AAAI Press.
Toiviainen, P. (1999). An interactive midi accompanist. Computer Music Journal, 22:4, 63{75.
Vercoe, B., & Puckette, M. (1985). The synthetic rehearsal: Training the synthetic performer. In
Proceedings of ICMC, pp. 275{278, San Francisco. International Computer Music Association.
Vercoe, B. L., Gardner, W. G., & Scheirer, E. D. (1998). Structured audio: Creation, transmission,
and rendering of parametric sound representations. Proc. IEEE, 86:5, 922{940.

81

fi