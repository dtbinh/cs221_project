Journal of Artificial Intelligence Research 38 (2010) 535-568

Submitted 05/10; published 08/10

Logical Foundations of RDF(S) with Datatypes
Jos de Bruijn
Stijn Heymans

bruijn@kr.tuwien.ac.at
heymans@kr.tuwien.ac.at

Vienna University of Technology
Favoritenstrae 9-11, A-1040 Vienna, Austria

Abstract
The Resource Description Framework (RDF) is a Semantic Web standard that provides
a data language, simply called RDF, as well as a lightweight ontology language, called
RDF Schema. We investigate embeddings of RDF in logic and show how standard logic
programming and description logic technology can be used for reasoning with RDF. We
subsequently consider extensions of RDF with datatype support, considering D entailment,
defined in the RDF semantics specification, and D* entailment, a semantic weakening of D
entailment, introduced by ter Horst. We use the embeddings and properties of the logics to
establish novel upper bounds for the complexity of deciding entailment. We subsequently
establish two novel lower bounds, establishing that RDFS entailment is PTime-complete
and that simple-D entailment is coNP-hard, when considering arbitrary datatypes, both in
the size of the entailing graph. The results indicate that RDFS may not be as lightweight
as one may expect.

1. Introduction
The Resource Description Framework (RDF) (Klyne & Carroll, 2004), together with its
vocabulary description language RDF Schema (RDFS) (Brickley & Guha, 2004), constitutes
the basic language of the Semantic Web. Statements in RDF are triples of the form hs, p, oi.
Sets of triples are called RDF graphs: intuitively, each triple can be viewed as an edge from
node s to node o with label p. Here, s, o, and p are constant symbols  uniform resource
identifiers (URIs) or literals (e.g., strings)  or anonymous identifiers, called blank nodes.
Consider, for example, the graphs S = {ho, rdf:type, Ai, hA, rdfs:subClassOf, Bi} and
E = {ho, rdf:type, Bi}. Hayes (2004) defines notions of RDF and RDFS entailment. We
have that, compared with RDF entailment, RDFS entailment gives additional meaning to
rdfs:subClassOf statements: S RDFS-entails E, but S does not RDF-entail E.
The RDF semantics specification (Hayes, 2004) defines four increasingly expressive normative entailment relations between RDF graphs, namely simple, RDF, RDFS, and D
entailment, where the latter extends RDFS entailment with support for datatypes (e.g.,
strings and integers). Furthermore, it defines extensional RDFS (eRDFS) entailment as
a possible extension of RDFS entailment that is more in line with description logic-based
languages such as OWL DL (Patel-Schneider, Hayes, & Horrocks, 2004) and OWL 2 DL
(Motik, Patel-Schneider, & Parsia, 2009b). Intuitively, the difference between the RDFS
and eRDFS entailment regimes is that, for the latter, whenever an ontological relation (e.g.,
subclass or property domain) implicitly holds in an interpretation, the corresponding RDF
statement (rdfs:subClassOf, rdfs:domain, respectively) must be true, whereas this is not
c
2010
AI Access Foundation. All rights reserved.

fide Bruijn & Heymans

always the case with the RDFS entailment regime. The following example illustrates this
difference.
Example 1. Let S be the graph
{hmother, rdfs:subPropertyOf, parenti, hparent, rdfs:domain, P ersoni}
which says that P erson is in the domain of parent, and the property mother is a subproperty of parent. Using eRDFS entailment we can conclude from S that P erson is in the
domain of mother:
S |=erdfs hmother, rdfs:domain, P ersoni
since it must the case that the subject of any mother triple has the type P erson; thus,
P erson is implicitly in the domain of mother. We cannot draw this conclusion when using
RDFS entailment; in RDFS, only explicitly asserted domain constraints can be derived.
We further also consider D* entailment (ter Horst, 2005), which is a semantic weakening of D entailment for the purpose of more efficient computation of consequences. D*
entailment extends RDFS entailment, but is not more expensive in terms of computational
complexity.
There have been several investigations into the formal properties of the RDF semantics
(Gutierrez, Hurtado, & Mendelzon, 2004; Gutierrez, Hurtado, Mendelzon, & Perez, 2010;
de Bruijn, Franconi, & Tessaris, 2005; ter Horst, 2005): Gutierrez et al. (2004, 2010) reconstruct the semantics from a graph database perspective, and de Bruijn et al. (2005)
reconstruct the semantics from a logical language perspective. The investigation of the
RDF semantics by ter Horst (2005) stays very close to the RDF specification. Additionally,
ter Horst shows that the entailment rules for computing RDFS entailment presented in the
original specification (Hayes, 2004) are not complete with respect to the RDFS semantics.
These reconstructions have led to a number of complexity results for RDF entailment. In
particular, simple, RDF, and RDFS entailment are NP-complete in the combined size of the
graphs. This high complexity is due to the presence of blank nodes (essentially existentially
quantified variables): if the entailed graph is known to be ground, the respective problems
turn out to be decidable in polynomial time. These bounds have not been shown to be
tight. As we will show in Section 5, the bound is tight for RDFS entailment, but not for
simple and RDF entailment, which can be decided in logarithmic space.
To investigate the relationship between RDF and logic we embed the various RDF
entailment regimes in F-Logic (Kifer, Lausen, & Wu, 1995), which is a syntactic extension
of first-order logic (FOL) with object oriented modeling constructs. F-Logic has constructs
to explicitly specify attributes, as well as generalization/specialization and instantiation
relationships. Like RDFS, the syntax of F-Logic has some seemingly higher-order features,
namely, the same identifier can be used for a class, an instance, and an attribute. However,
the semantics of F-Logic is strictly first-order (Kifer et al., 1995). It turns out that the
attribute value construct in F-Logic is exactly equivalent to the triple construct in RDF,
and the typing (class membership) construct in F-Logic is very close in spirit to the one in
RDF.
In addition, we consider the embedding of a large subset of extensional RDFS in FOL and
the tractable description logic language DL-LiteR (Calvanese, Giacomo, Lembo, Lenzerini,
536

fiLogical Foundations of RDF(S) with Datatypes

& Rosati, 2007), thereby showing that, under certain restrictions, extensional RDFS can be
seen as a standard first-order knowledge representation language.
Our contributions with this paper can be summarized as follows.
1. We define embeddings of simple, RDF, RDFS, and extensional RDFS into F-Logic,
and show that simple, RDF, and RDFS entailment can be decided using standard logic
programming techniques, as their embeddings are in the Horn fragment of F-Logic.
2. We define an alternative, direct embedding of extensional RDFS into the Horn fragment of F-Logic for a fragment of RDF graphs, namely those in which the RDFS
vocabulary is only used in a standard way. We subsequently exploit earlier results
about the relationship between F-Logic statements and description logic statements
(de Bruijn & Heymans, 2008) to show that extensional RDFS reasoning with ground
RDF graphs can be reduced to reasoning in the tractable description logic DL-LiteR
(Calvanese et al., 2007).
3. We extend the embeddings mentioned under 1. with support for datatypes, considering
both D* and D entailment. The embeddings of the extensions of simple, RDF, and
RDFS entailment with D* datatype support are all essentially in the Horn fragment
of F-Logic. The extensions of simple, RDF, and RDFS with D datatype support can
be embedded in the Horn fragment of F-Logic when suitably restricting the datatypes
that may be considered.
4. We analyze the complexity of deciding the mentioned entailment relations. From the
mentioned embeddings we obtain a number of novel complexity upper bounds, namely,
simple and RDF entailment, as well as their extensions with datatypes (under suitable
restrictions), are in LogSpace in the size of the entailing graph and a large fragment
of extensional RDFS entailment is in NP in the combined size of the graphs and in
PTime in the size of the entailing graph. We also establish a novel PTime lower bound
for RDFS entailment and a novel coNP lower bound for simple entailment extended
with D datatype support, when considering arbitrary datatypes, both in the size of
the entailing graph. See Table 2 on page 553 for an overview of the complexity results
for RDF.
The structure of the remainder of the paper is as follows. In Section 2 we review the logics
under consideration, namely F-Logic and DL-LiteR . In Section 3 we review the RDF(S)
semantics, define embeddings into F-Logic and FOL, show faithfulness of these embeddings,
and demonstrate the relationship with DL-LiteR . In Section 4 we consider extensions of
the RDF entailment regimes with datatype support based on both D* and D entailment
and embeddings of these extensions into logic. In Section 5 we extensively investigate the
complexity of the various RDF entailment regimes. We conclude the paper and outline
future work in Section 6.
This paper extends a paper we published at the International Semantic Web Conference
(de Bruijn & Heymans, 2007) with embeddings of the D* and D entailment regimes and
novel lower bounds for the complexity of deciding RDFS, D* , and D entailment.
537

fide Bruijn & Heymans

For reasons of legibility, the definitions of the various RDF-related notions of interpretation may be found in Appendix A, the embeddings of the RDF entailment regimes may
be found in Appendix B, and the proofs of Sections 3 and 4 may be found in Appendix C.

2. Preliminaries
In this section we review F-Logic and DL-LiteR .
2.1 Frame Logic
We consider Frame Logic (F-Logic) as defined by Kifer, Lausen, and Wu (1995). To simplify
matters, and because these constructs are not necessary for the embedding of RDF, we do
not consider function symbols, parameterized methods, functional (single-valued) methods,
inheritable methods, and compound molecules, following de Bruijn and Heymans (2008).
The signature of an F-language L is of the form  = hC, Pi with C and P disjoint sets of
constant and predicate symbols; each predicate symbol has an associated arity n  0. Let
V be a set of variable symbols. Terms and atomic formulas are constructed as usual: x  V
and c  C are terms and >, , p(t1 , . . . , tn ), and t1 = t2 are atomic formulas, with p  P
an n-ary predicate symbol, and t1 , . . . , tn terms.
A molecule in F-Logic is one of the following statements: (i) an is-a assertion of the
form t1 : t2 , which states that an individual t1 is of type t2 , or (ii) a data molecule (called
method by Kifer et al., 1995) of the form t1 [t2  t3 ], with t1 , t2 , and t3 terms, which
states that an individual t1 has an attribute t2 with value t3 . An F-Logic molecule is ground
if it does not contain variables.
Formulas of an F-language L are either atomic formulas, molecules, or compound formulas which are constructed in the usual way from atomic formulas, molecules, and the
logical connectives , , , , the quantifiers ,  and the auxiliary symbols ( and ). We
denote universal closure, i.e., the universal quantification of every variable that has a free
occurrence in the formula, with ().
A theory is a set of formulas. A theory or formula is called equality-free if the equality
symbol = does not appear in it.
F-Logic Horn formulas are of the form ()B1  . . .  Bn  H, with B1 ,. . . , Bn , and H
atomic formulas or molecules. F-Logic Datalog formulas are F-Logic Horn formulas such
that every variable in H occurs in some equality-free Bi . The latter condition is called
safeness.
An F-structure is a tuple I = hU, U , IC , I , IP i, where U is a non-empty set and U is a
binary relation over U . A constant symbol c  C is interpreted as an element of the domain:
IC (c)  U . An n-ary predicate symbol p  P is interpreted as a relation over the domain
U : IP (p)  U n . I associates a binary relation over U with each k  U : I (k)  U  U .
Variable assignments B are defined in the usual way.
Given an F-structure I, a variable assignment B, and a term t of L, tI,B is defined as:
I,B
x = xB for variable symbol x and tI,B = IC (t) for t  C.
Satisfaction of atomic formulas and molecules  in I, given a variable assignment B,
denoted (I, B) |=f , is defined as
 (I, B) |=f >,
(I, B)6|=f ,
538

fiLogical Foundations of RDF(S) with Datatypes

I,B
(I, B) |=f p(t1 , . . . , tn ) iff (tI,B
1 , . . . , tn )  IP (p),
I,B
I,B
(I, B) |=f t1 = t2 iff t1 = t2 ,
(I, B) |=f t1 : t2 iff tI,B
U tI,B
1
2 , and
I,B I,B
(I, B) |=f t1 [t2  t3 ] iff ht1 , t3 i  I (tI,B
2 ).
This extends to arbitrary formulas in the usual way. An F-structure I satisfies a formula
, denoted I |=f , if (I, B) |=f  for every variable assignment B. I satisfies a theory   L
if it satisfies all formulas in ; in this case, I is called a model of . A theory  F-entails
a formula   L, denoted  |=f , iff for every model I of , I |=f .
A Herbrand F-structure is an F-structure I = hU, U , IC , I , IP i such that U is the
set of constants and for every constant symbol c  C, IC (c) = c. As an abuse of notation,
for Herbrand structures we use I to denote both the structure and the set of ground atomic
formulas satisfied by the structure. Finally, a Herbrand F-structure I is a minimal Herbrand
model of a theory  if it is a model and there is no Herbrand F-structure I0 that is a model
of  such that I0 I.






Classical first-order logic (FOL) is F-Logic without molecules. Contextual first-order logic
is classical FOL where C and P are not required to be disjoint, predicate symbols do not
have an associated arity, and for every structure I = hU, U , IC , I , IP i, IP assigns a
relation IiP (p)  U n to every p  P, for every nonnegative integer i. We denote satisfaction
and entailment in classical and contextual first-order logic using the symbols |= and |=c ,
respectively. Contextual FOL is sometimes also referred to as FOL with punning.
F-Logic can be straightforwardly embedded into FOL, as shown in (Kifer et al., 1995,
Theorem 18.1).
Proposition 1. Let  and  be an F-Logic theory and formula that do not contain the
binary and ternary predicate symbols isa and data, respectively, and let 0 and 0 be the
FOL theory and formula obtained from  and  by replacing every is-a molecule a : b with
isa(a, b) and every data molecule a[b  c] with data(a, b, c). Then,
 |=f  iff 0 |= 0
2.2 DL-LiteR
A DL-LiteR (Calvanese et al., 2007) language consists of pairwise disjoint sets of concept
(NC ), role (NR ), and individual (NI ) identifiers. Concepts and roles in DL-LiteR are
defined as follows:
Cl  A | R
Cr  A | R | A | R
R, R0  P | P 
with A  NC and P  NR , Cl and Cr left- (resp., right-)hand side concepts, and R and R0
roles.
A DL-LiteR knowledge base K = (T , A) consists of a TBox T , which is a set of inclusion
axioms of the forms
Cl v Cr
R v R0
539

fide Bruijn & Heymans

DL syntax FOL syntax
(A, X)
A(X)
(P, X, Y )
P (X, Y )

(P , X, Y ) P (Y, X)
(R, X)
y((R, X, y))
(A, X)
(A, X)
(R, X)
y((R, X, y))
y is a new variable

DL syntax
(Cl v Cr )
(R1 v R2 )
(A(a))
(P (a1 , a2 ))

FOL syntax
x((Cl , x)  (Cr , x))
x, y((R1 , x, y)  (R2 , x, y))
A(a)
P (a1 , a2 )

Table 1: Mapping of DL-LiteR to FOL
and an ABox A, which is a set of concept and role membership assertions of the forms
A(a)

P (a1 , a2 )

with a, a1 , a2  NI .
We define the semantics of DL-LiteR through a translation to FOL, in the form of the
mapping function , which is defined in Table 1.1 The mapping  extends naturally to sets
of axioms and assertions.
Given a DL-LiteR knowledge base K = (T , A), the FOL equivalent of K is the FOL
theory  = (K) = (T )  (A).
Contextual DL-LiteR is like DL-LiteR , except that the sets of concept (NC ), role (NR ),
and individual (NI ) identifiers are not required to be disjoint. The semantics of a contextual
DL-LiteR knowledge base K = (T , A) is given through the same mapping (K), which
yields a contextual FOL theory. Note that contextual DL-LiteR is essentially a subset of
the QL profile of OWL 2 (Motik, Grau, Horrocks, Wu, Fokoue, & Lutz, 2009a).

3. RDF and RDF Schema
We first review the definitions of the RDF syntax and semantics. We then proceed with
the embedding of graphs and axiomatization of the entailment regimes into F-Logic, and
finally the embedding of extensional RDFS into FOL and DL-LiteR .
3.1 RDF(S) Syntax and Semantics
We proceed with a review of the definitions of the RDF syntax (Klyne & Carroll, 2004) and
semantics (Hayes, 2004).
A vocabulary V = hC, PL, T Li consists of a set C of RDF URI references (simply referred
to as URIs), a set PL of plain literals (i.e., Unicode character strings with an optional
language tag), and a set T L of typed literals (i.e., pairs (s, u) of a Unicode string s and a
URI u, denoting a datatype); see (Klyne & Carroll, 2004, Sections 6.4, 6.5, 6.6) for more
details about the specific form of these symbols. Note that C, PL, and T L are mutually
disjoint. The symbols in V are collectively referred to as names.
Let B be a set of blank nodes that is disjoint from the set of names in V . Terms are
names or blank nodes. A generalized RDF graph S is a set of generalized triples hs, p, oi 
1. Borgida (1996) discusses the relationship between description logics and first-order logic in detail.

540

fiLogical Foundations of RDF(S) with Datatypes

subject, predicate, object  with s, p, o  C  PL  T L  B. A normal RDF graph S is a set
of normal triples hs, p, oi, with s  C  B, p  C, and o  C  PL  T L  B.2 A ground triple
is a triple that does not contain blank nodes. A ground generalized, respectively normal
RDF graph is a set of ground generalized, respectively normal triples. With bl(hs, p, oi)  B
(resp., bl(S)  B) we denote the set of blank nodes in a triple hs, p, oi (resp., graph S).
In the remainder, whenever speaking about triples or RDF graphs, we mean generalized
triples, respectively generalized RDF graphs, unless stated otherwise.
An interpretation is a tuple I = hIR, IP, LV, IS, IL, IEXTi, where IR is a non-empty set,
called the domain, IP is a set of properties, LV  IR is a set of literal values with PL  LV,
IS is a mapping IS : C  IR  IP, IL is a mapping IL : T L  IR, and IEXT is an extension
function IEXT : IP  2(IRIR) .
Given an interpretation I, a subset of the blank nodes B 0  B, and a mapping A : B 0 
IR, which is used to interpret blank nodes, for any given term t we define tI,A as:
 if t  C, then tI,A = IS(t),
 if t  T L, then tI,A = IL(t), and

 if t  PL, then tI,A = t,
 if t  B 0 , then tI,A = A(t).

An interpretation I satisfies a triple hs, p, oi with respect to a mapping A : B 0  IR, with
bl(hs, p, oi)  B 0 , denoted (I, A) |= hs, p, oi, if pI,A  IP and hsI,A , oI,A i  IEXT(pI,A ). I
satisfies a graph S with respect to a mapping A : bl(S)  IR, denoted (I, A) |= S, if
(I, A) |= hs, p, oi for every hs, p, oi  S.
An interpretation I satisfies an RDF graph S, denoted I |= S, if (I, A) |= S for some
mapping A : bl(S)  IR; in this case, I is a model of S. Any interpretation is an sinterpretation (simple interpretation).
The notions of rdf -, rdfs-, and erdfs-interpretation are defined through additional conditions on s-interpretation. For example, an s-interpretation is an rdf -interpretation only if
for every object k, k  IP iff hk, IS(rdf:Property)i  IEXT (IS(rdf:type)) and it satisfies
the triple hrdf:nil, rdf:type, rdf:Listi. Triples that are required to be satisfied by every
x-interpretation are called x-axiomatic triples, for x  {rdf, rdfs, erdfs} or simply axiomatic
triples when the entailment regime is clear from the context. The precise definitions of rdf -,
rdfs-, and erdfs-interpretation are found in Appendix A.
Entailment and Satisfiability Given a vocabulary V and an entailment regime x 
{s, rdf, rdfs, erdfs}, a generalized (resp., normal) RDF graph S x-entails a generalized (resp.,
normal) RDF graph E, denoted S |=x E, if every x-interpretation of V that is a model of
S is also a model of E.
Given an entailment regime x  {s, rdf, rdfs, erdfs}, a generalized (resp., normal) RDF
graph S is x-satisfiable if it has a model that is an x-interpretation; otherwise it is xunsatisfiable. The following observations can be made about satisfiability for the various
entailment regimes; the observations concerning normal RDF graphs are due to Hayes
(2004).
Proposition 2.
1. Every generalized and every normal RDF graph is s-satisfiable.
2. Every normal RDF graph is rdf-satisfiable.
2. Normal RDF graphs correspond the RDF graphs defined by Hayes (2004). In contrast to normal RDF,
generalized RDF graphs allow blank nodes and literals in predicate, and literals in subject positions.

541

fide Bruijn & Heymans

3. There is a generalized RDF graph that is rdf-unsatisfiable.
4. There is a normal (and generalized) RDF graph that is rdfs- and erdfs-unsatisfiable.
3.2 Embedding RDF in Logic
We translate a graph to a conjunction of data molecules, where URIs and literals are
constant symbols and blank nodes are existentially quantified variables. We axiomatize the
entailment regimes using sets of formulas that are independent from the graphs. In the
remainder we assume that RDF graphs are finite.
Given a vocabulary V = hC, PL, T Li, an F-language L conforms with V if it has a
signature of the form  = hC 0 , Pi, with C 0  C  PL  T L.3
Definition 1. Let V be a vocabulary, let S be an RDF graph of V , let bl(S) = {b1 , . . . , bn }
be the set of blank nodes appearing in S, let hs, p, oi be a triple in S, and let L be an
F-language that conforms with V . Then,
tr(hs, p, oi) = s[p  o] and
^

tr(S) =  b1 , . . . , bn
{tr(hs, p, oi) | hs, p, oi  S}
are formulas of L.
The axiomatizations of the entailment regimes are theories x , with x  {s, rdf, rdfs,
erdfs}, which are defined in Appendix B.
If  is an F-Logic formula in prenex normal form with only existential quantifiers, then
sk() denotes the Skolemization of , i.e., every existentially quantified variable is replaced
with a globally unique new constant symbol. This extends to theories in the natural way.
Proposition 3. Let S be an RDF graph of a vocabulary V and let x  {s, rdf, rdfs} be an
entailment regime. Then, sk(tr(S))  x can be equivalently rewritten to a set of F-Logic
Horn formulas.
We have that erdfs cannot be equivalently rewritten to a set of Horn formulas, because
of the use of universal quantification in the antecedents of some of the implications in erdfs .
We now show faithfulness of our embedding.
Theorem 1. Let S and E be RDF graphs of a vocabulary V , and let x  {s, rdf, rdfs, erdfs}
be an entailment regime. Then,
S |=x E
S is x-satisfiable

iff

tr(S)  x |=f tr(E) and

iff

tr(S)  x has a model.

The following corollary follows immediately from Theorem 1 and the classical results
about Skolemization (see, e.g., Fitting, 1996).
Corollary 1. Let S and E be RDF graphs of a vocabulary V , and let x  {s, rdf, rdfs,
erdfs} be an entailment regime. Then,
S |=x E if and only if sk(tr(S))  x |=f tr(E).
3. Even though typed literals are pairs in RDF, we treat them simply as constant symbols in our embedding.

542

fiLogical Foundations of RDF(S) with Datatypes

Observe that rdf , rdf s , and erdf s are infinite due to the infinite set of RDF axiomatic
triples. However, for checking RDF entailment we need only a finite subset of x . Given
an RDF graph S, let xS be obtained from x by removing all formulas originating from
axiomatic triples involving container membership properties (i.e., rdf: 1, rdf: 2, . . . ) not
appearing in S, with the exception of the axiomatic triples involving rdf: 1.
Proposition 4. Let S and E be RDF graphs and let x  {s, rdf, rdfs, erdfs} be an entailment
regime. Then,
S |=x E if and only if sk(tr(S))  xSE |=f tr(E).
By Proposition 3 we have that sk(tr(S))  s , tr(S)sk  rdf , and sk(tr(S))  rdfs are
equivalent to sets of Horn formulas. Therefore, Proposition 4 implies that simple, RDF, and
RDFS entailment can be computed using reasoners that can compute ground entailment of
F-Logic Horn theories, such as FLORA-2 (Yang, Kifer, & Zhao, 2003). Notice that tr(E)
can be seen as a boolean conjunctive query (i.e., a yes/no query), where the existentially
quantified variables in tr(E) are the non-distinguished variables.
3.3 Direct Embedding of Extensional RDFS
We now consider an alternative, direct embedding of the extensional RDFS entailment
regime. This embedding, rather than axiomatizing the entailment regime, embeds ontological statements, e.g., rdfs:subClassOf statements, directly as formulas.
We first define the notion of standard use of the RDF(S) vocabulary, which intuitively
corresponds to not using the vocabulary in locations where it can change the semantics of
the RDF(S) ontology vocabulary (e.g., hrdf:type, rdfs:subPropertyOf, ai).
Definition 2. Let S be an RDF graph. Then, S has only standard use of the RDF(S)
vocabulary if
 rdf:type, rdfs:subClassOf, rdfs:domain, rdfs:range, and rdfs:subPropertyOf do
not appear in subject or object positions of any triple in S and
 rdfs:ContainerMembershipProperty, rdfs:Resource, rdfs:Class, rdfs:Datatype,
and rdf:Property appear only in object positions of rdf:type-triples in S.
We are now ready to define the direct embedding trerdfs of the extensional RDFS entailment regime for graphs with only standard use of RDFS vocabulary. While trerdfs deals with
an important part of the RDF(S) vocabulary, the axiomatization of the eRDFS semantics
of the remainder of the RDF(S) vocabulary may be found in Appendix B, in the form of
the theory erdfs-V , where V is a vocabulary.
543

fide Bruijn & Heymans

Definition 3. Let hs, p, oi be an RDF triple. Then,
trerdfs (hs, type, Datatypei) =
=
Propertyi)
trerdfs (hs, type, oi) =
trerdfs (hs, subClassOf, oi) =
trerdfs (hs, subPropertyOf, oi) =
trerdfs (hs, domain, oi) =
trerdfs (hs, range, oi) =
trerdfs (hs, p, oi) =

trerdfs (hs, type, ContainerMembership-

s : Datatype  x(x : s  x : Literal),
s : ContainerMembershipProperty
x, y(x[s  y]  x[member  y]),
s : o,
x(x : s  x : o),
x, y(x[s  y]  x[o  y]),
x, y(x[s  y]  x : o),
x, y(x[s  y]  y : o), and
s[p  o], otherwise.

Let S be an RDF graph and let bl(S) = {b1 , . . . , bn } be the set of blank nodes in S. Then,
^
trerdfs (S) = { b1 , . . . , bn ( {trerdfs (hs, p, oi) | hs, p, oi  S})}
We say that a term t occurs in a property position if it occurs as the predicate of
a triple, as the subject or object of an rdfs:subPropertyOf triple, as the subject of an
rdfs:domain or rdfs:range triple, or the graph contains ht, rdf:type, rdf:Propertyi or
ht, rdf:type, ContainerMembershipPropertyi. A term t occurs in a class position if it occurs as the subject or object of an rdfs:subClassOf triple, as the object of an rdfs:domain,
rdfs:range, or rdf:type triple, as the subject of a triple ht, rdf:type, rdfs:Classi, or as
the subject of a triple ht, rdf:type, rdfs:Datatypei.
Let S be an RDF graph with only standard use of the RDF(S) vocabulary. The property
(resp., class) vocabulary of S consists of all the names appearing in property (resp., class)
positions in S or the RDF(S) axiomatic triples with only standard use of the RDF(S)
vocabulary.
Given two RDF graphs S and E with only standard use of the RDF(S) vocabulary, we
write E E S if the property, resp. class vocabularies of E are subsets of the property, resp.
class vocabularies of S, there are no blank nodes in class or property positions in E,4 and
rdfs:Resource, rdfs:Class, and rdf:Property do not appear in E.
Theorem 2. Let S and E be RDF graphs with only standard use of the RDFS vocabulary
such that E E S. Then,
S |=erdfs E iff trerdfs (S)  erdfs-V |=f trerdfs (E)
We define erdfs-V
analogously to erdfs
S
S , i.e., it does not contain statements concerning
container membership properties not appearing in the graph S, with the exception of rdf: 1.
The following proposition follows from an argument analogous to the proof of Property 4.
Proposition 5. Let S and E be RDF graphs with only standard use of the RDFS vocabulary
such that E E S. Then,
erdfs
S |=erdfs E iff sk(trerdfs (S))  erdfs-V
(E).
SE |=f tr

4. This restriction on the use of blank nodes in the entailed graph was not mentioned in the extended
abstract of this paper (de Bruijn & Heymans, 2007). This was an error.

544

fiLogical Foundations of RDF(S) with Datatypes

We have that whenever E does not contain the terms rdfs:subClassOf, rdfs:subPropertyOf, rdfs:domain, and rdfs:range, trerdfs (E) is a conjunction of atomic molecules
prefixed by an existential quantifiers (i.e., a conjunctive query).
We have that sk(trerdfs (S))  erdfs-V
SE is a finite set of Horn formulas. Therefore, if
the graphs satisfy the mentioned conditions, query answering techniques used in F-Logic
reasoners such as FLORA-2 (Yang et al., 2003) can be used for checking extensional RDFS
entailment.
3.4 Embedding Extensional RDFS into First-Order Logic
We now consider an embedding of extensional RDFS entailment into first-order logic (FOL),
based on the direct embedding of extensional RDFS in F-Logic defined above.
We say that an F-Logic theory  is translatable to contextual FOL if  does not contain
unary or binary predicates and for every molecule of the form t1 [t2  t3 ] or t1 : t2 holds that
t2 is a constant symbol. F O() is the contextual FOL theory obtained from  by replacing:
 every data molecule t1 [t2  t3 ] with the atomic formula t2 (t1 , t3 ) and
 every is-a molecule t1 : t2 with the atomic formula t2 (t1 ).
The following proposition follows immediately from an earlier result (de Bruijn & Heymans,
2008, Theorem 3.2).
Proposition 6. Let , respectively , be an equality-free F-Logic theory, respectively formula, that is translatable to contextual FOL. Then,
 |=f  iff F O() |=c F O().
We say that an RDF graph S is a non-higher-order graph if S does not contain blank
nodes in class or property positions, and has only standard use of the RDFS vocabulary.
Observe that if S is a non-higher-order RDF graph, then trerdfs (S)  erdfs-V is translatable
to contextual FOL. Notice also that every ground RDF graph that has only standard use
of the RDFS vocabulary is a non-higher-order RDF graph.
Theorem 3. Let S and E be non-higher-order RDF graphs such that E E S. Then,
S |=erdfs E iff F O(trerdfs (S)  erdfs-V ) |=c F O(trerdfs (E)).
Proof. Follows immediately from Theorem 2, the fact that F O(trerdfs (S)) and
F O(trerdfs (E)) do not contain the equality symbol, and Proposition 6.
Concerning the relationship with DL-LiteR , we make the following observation.
Proposition 7. Let S be a ground non-higher-order graph.5 Then, F O(trerdfs (S)erdfs-V )
can be equivalently rewritten to the FOL equivalent  = (K) of a contextual DL-LiteR
knowledge base K.
Analogous to Proposition 5, one may discard the axiomatic triples concerning container
membership properties that are not used, and thus one only needs to reason with a finite
knowledge base.
5. Note that, when considering a variant of DL-LiteR that allows existentially quantified variables in the
ABox  also allowed in OWL DL  this restriction could be relaxed to S being a non-ground non-higherorder RDF graph.

545

fide Bruijn & Heymans

4. Extensions with Datatypes
The entailment regimes we dealt with in the previous section do not consider many of
the useful datatypes (e.g., strings, integers). In fact, rdf:XMLLiteral is the only datatype
that was considered. The RDF semantics specification (Hayes, 2004) defines the notion
of D entailment (datatype entailment), which extends RDFS entailment with support for
datatypes. Ter Horst (2005) defines the notion of D* entailment, which is also an extension
of RDFS entailment, but semantically weaker than D entailment. We first review D*
entailment, after which we review D entailment. Both semantics were originally defined as
extensions of RDFS entailment. However, one might extend any of the entailment regimes
we considered with datatype support. Therefore, we consider extensions of simple, RDF,
RDFS, and extensional RDFS entailment with both kinds of datatype semantics. We first
review the datatype semantics, after which we present embeddings of both semantics into
F-Logic. Finally, we discuss a notion of normalization that may be used to remove equality
statements from the embeddings to speed up processing.
4.1 Extension of the RDF Entailment Regimes with Datatypes
Datatypes define sets of concrete data values (e.g., strings and integers), along with their
lexical representations. A datatype is a tuple d = hLd , V d , L2V d i consisting of
 a lexical space Ld , which is a set of character strings (e.g., 0, 1, 01, . . . , in the
case of an integer datatype),
 a value space V d , which is a set of values (e.g., the numbers 0, 1, 2, . . . , in the case
of an integer datatype), and
 a lexical-to-value mapping L2V d , which is a mapping from the lexical space to the
value space (e.g., {0 7 0, 1 7 1, 01 7 1, . . .}, for an integer datatype).
A simple datatype map D is a partial mapping from URIs to datatypes. A simple datatype
map D is a datatype map if D(rdf:XMLLiteral) = xml where xml is the built-in XML
literal datatype as defined in the RDF specification (Klyne & Carroll, 2004). With dom(D)
and ran(D) we denote the domain and range of D, respectively.
Given a simple datatype map D, we call a typed literal (s, u)  T L well-typed if u 
dom(D) and s  LD(u) ; (s, u) is ill-typed if u  dom(D) and s 
/ LD(u) .
We now review the notions of D* and D entailment. Similar to the previous section,
the definitions of D* - and D-interpretations can be found in Appendix A.
D* entailment Given a simple datatype map D, an RDF graph S s-D* entails an RDF
graph E, denoted S |=s-D* E, if every s-D* -interpretation that is a model of S is a model
of E.
Given a datatype map D, an RDF graph S x-D*-entails an RDF graph E, denoted
S |=x-D* E, if every x-D* -interpretation that is a model of S is a model of E, for x 
{rdf, rdfs, erdfs}.
Notice that if dom(D) = {rdf:XMLLiteral} and x  {rdf, rdfs, erdfs}, then x-D* entailment corresponds to x-entailment, with the exception that when considering rdf -D* entailment, the triple hrdf:XMLLiteral, rdf:type, rdfs:Datatypei is additionally entailed.
In addition, if dom(D) = , then s-D* -entailment corresponds to s-entailment.
546

fiLogical Foundations of RDF(S) with Datatypes

The following example shows how equality may be introduced by the D* semantics.
Example 2. Consider a datatype map that contains the XML schema string datatype
(Peterson, Gao, Malhotra, Sperberg-McQueen, & Thompson, 2009). Certain equalities hold
between plain literals without language tags and typed literals of this datatype, because the set
of plain literals without language tags corresponds to the value space of the string datatype.
So, equalities such as a = (a, string) and xxx = (xxx, string) necessarily hold.
Similar for equalities between datatypes. For example, if the datatype map contains both
integer and decimal, then further equalities such as (1, integer) = (1, decimal) and
(1, decimal) = (1.0, decimal) necessarily hold.
D entailment Given a simple datatype map D, an RDF graph S s-D-entails an RDF
graph E, denoted S |=s-D E, if every s-D-interpretation which is a model of S is a model
of E.
Given a datatype map D, an RDF graph S x-D-entails an RDF graph E, denoted
S |=x-D E, if every x-D-interpretation which is a model of S is a model of E, for x 
{rdf, rdfs, erdfs}. An RDF graph S x-D-entails an RDF graph E, denoted S |=x-D E, if
every x-D-interpretation which is a model of S is also a model of E.
There are two main differences between D* entailment and D entailment: (i) D entailment allows for easy extension towards languages which can express equality between
URIs denoting datatypes; whenever two URIs denote the same datatype, typed literals
with these two URIs as datatypes are interpreted in the same way (see Example 3); and (ii)
D entailment directly links the class extension of a datatype with the value space of this
datatype. The latter complicates the evaluation of entailment somewhat, and was likely the
main motivation for the introduction of D* entailment. The complication becomes clear
when declaring blank nodes as members of specific datatypes, as illustrated in Example 4.
Example 3. Consider an extension of D entailment with equality by imposing the following
condition on interpretations:
(+) An interpretation I satisfies a triple hx, owl:sameAs, yi with respect to a
blank node assignment A iff xI,A = y I,A .
Now consider a datatype map D = {bool 7 boolean}, where boolean is defined as follows:
 Lboolean = {1, 0, t, f },
 V boolean = {true, f alse}, and
 L2V boolean = {1 7 true, 0 7 f alse, t 7 true, f  7 f alse},
and an RDF graph S = {hmyBool, owl:sameAs, booli, ha, b, (1, myBool)i}. In D-interpretations, typed literals of which the datatype URIs are interpreted the same are interpreted the same as well. Therefore, under D entailment extended with condition (+) the
triple ha, b, (t, myBool)i can be derived from S: (1, myBool) and (t, myBool) are
both interpreted as L2V boolean (1) = L2V boolean (t) = true; hence, (1, myBool) and
(t, myBool) are interpreted in the same way in every interpretation. Similarly, it can be
shown that the triples ha, b, (1, bool)i and ha, b, (t, bool)i are entailed by S.
547

fide Bruijn & Heymans

None of these derivations is valid when considering D* entailment extended with condition (+). In fact, because myBool is not in the domain of D, (1, myBool) is interpreted
as an arbitrary (abstract) symbol; it is treated in the same way as a URI.
Example 4. Consider a datatype map D that includes the XML schema datatypes string
and integer (Peterson et al., 2009), which have disjoint value spaces. Consider also the
graph S = {h : x, rdf:type, stringi, h : x, rdf:type, integeri}. In an rdfs-D*-interpretation I the class extensions of string and integer are not necessarily the same as the
value spaces of the respective datatypes. Therefore, there may be an object k  IR that is
neither an integer nor a string, but which is in the class extensions of both string and
integer. Consequently, there is an rdfs-D*-interpretation that is a model of S and S is
rdfs-D*-satisfiable.
In an rdfs-D-interpretation the class extensions of string and integer are necessarily
the same as the value spaces of the respective datatypes. Since these value spaces are disjoint,
there can be no object that is both in the class extension of string and in the class extension
of integer. Therefore, S is not rdfs-D-satisfiable.
4.2 Embeddings of Datatypes in Logic
Given a datatype map D, we use a set of formulas y  L, defined in Appendix B, to axiomatize the semantics of an entailment regime y  {x-D*, x-D}, with x  {s, rdf, rdfs, erdfs}.
Analogous to Proposition 3, we have:
Proposition 8. Let S be an RDF graph of a vocabulary V . Then, sk(tr(S))  y , with
y  {s-D*, rdf-D*, rdfs-D*, s-D, rdf-D, rdfs-D}, can be equivalently rewritten to a set of FLogic Horn formulas.
We first show faithfulness of our embedding of D* entailment.
Theorem 4. Let S and E be RDF graphs of a vocabulary V , let D be a datatype map, and
let x  {s, rdf, rdfs, erdfs} be an entailment regime. Then,
S |=x-D* E if and only if tr(S)  x-D* |=f tr(E) and
S is x-D*-satisfiable iff tr(S)  x-D* has a model.
We now turn to x-D-entailment. It turns out that when considering datatype maps with
arbitrary datatypes, one needs to reason by case (see Proposition 14 in Section 5), which
complicates matters. We therefore restrict ourselves to definite datatypes, which do not
bring this complication. An example of a definite datatype map is one that includes only
the set of datatypes in the OWL 2 EL and QL profiles (Motik et al., 2009a).
Definition 4. A datatype map D is definite if
 the value space of every datatype d  ran(D) is infinite,
 for any n  1 distinct datatypes d1 , . . . , dn  ran(D) holds that either (a) the value
spaces are disjoint, i.e., V di  V dj =  (1  i < j  n) or (b) their intersection is
infinite, i.e., V d1      V dn is an infinite set, and
548

fiLogical Foundations of RDF(S) with Datatypes

 for no two datatypes d1 , d2  ran(D) holds that d1  V d2 .
Theorem 5. Let S and E be RDF graphs of a vocabulary V , let D be a definite datatype
map, and let x  {s, rdf, rdfs, erdfs} be an entailment regime. Then,
S |=x-D E if and only if tr(S)  x-D |=f tr(E) and
S is x-D-satisfiable iff tr(S)  x-D has a model.
4.3 Normalization of Datatypes
The set of equality statements in the axiomatizations x-D* and x-D is potentially large
and, in general, polynomial in the size of the vocabulary V . In addition, it requires equality
reasoning, which tends to deteriorate the performance of a reasoner. We discuss how to
normalize the embedding of a graph in F-Logic, thereby removing the need for expressing
equality.
Given a vocabulary V , we assume a strict (e.g., lexicographical) S
order < on the set of
D
literals PL  T L. For a given datatype map D, we define V = udom(D) V D(u) , i.e.,
the values in D. For each v  V D , we define the literals that represent the value v as:
v = {(s, u)  T L | L2V D(u) (s) = v}  {l  PL | l = v}. The representation of v, denoted
r(v), is the least element in v according to the order <.
Given a set of formulas   L such that L conforms with V , the datatype normalization
of , denoted ()n , is obtained from  by replacing every plain literal l  PL with r(l) and
replacing every well-typed literal (s, u)  T L with r(L2V D(u) (s)).
Observe that the only equality statements in the normalizations (tr(S)  x-D )n and
(tr(S)  x-D* )n are trivial statements of the form t = t, where t is a literal. Therefore,
these statements may be discarded.
The following proposition follows straightforwardly from the shape of the axiomatizations y and the definition of normalization.
Proposition 9. Let S and E be RDF graphs of a vocabulary V , let D be datatype map D,
and let y  {s-D*, rdf-D*, rdfs-D*, erdfs-D*, s-D, rdf-D, rdfs-D, erdfs-D}. Then,
tr(S)  y |=f tr(E) iff (tr(S)  y )n |=f (tr(E))n

5. Complexity
In this section we review the complexity of the various RDF entailment relations and present
several novel results, exploiting the embeddings presented in Sections 3 and 4.
The complexity of non-ground simple entailment and RDFS entailment, and upper bounds
for ground entailment are known from the literature, and analogous results for RDF entailment follow immediately. Recall that, although the set of axiomatic triples is infinite,
only a finite subset, linear in the size of the graphs, needs to be taken into account when
checking entailment (cf. Proposition 4).
Proposition 10 (Gutierrez et al., 2004, 2010; ter Horst, 2005; de Bruijn et al., 2005). The
decision problems S |=s E, S |=rdf E, S |=rdf s E, and S |=rdfs-D* E, given RDF graphs S
549

fide Bruijn & Heymans

and E, are NP-complete in the combined size of S and E, and polynomial in the size of S.
If E is ground, then the respective problems are in PTime.
In addition, the problems S |=erdf s E and S |=rdfs-D E are NP-hard.
The membership proofs by Gutierrez et al. (2004, 2010), ter Horst (2005), and de Bruijn
et al. (2005) rely on the fact that the set of all (relevant) entailed triples of a given graph
can be computed in polynomial time using the RDFS entailment rules (ter Horst, 2005);
the problem can then be reduced to subgraph homomorphism. From Corollary 1 and the
fact that the problem of checking ground entailment in Datalog (Dantsin, Eiter, Gottlob,
& Voronkov, 2001) is polynomial in the size of the data (i.e., tr(S)) we obtain a novel
argument for membership.
NP-hardness of non-ground entailment has been shown through a reduction from a
known NP-hard problem (ter Horst, 2005).
From the embedding in F-Logic (Theorem 1), we obtain the following upper bound for
the complexity of simple and RDF entailment.
Proposition 11. Let S and E be RDF graphs. If E is fixed, the problems S |=s E and
S |=rdf E are decidable in LogSpace in the size of S. The problems S |=s E and S |=rdf E
are decidable in LogSpace in the combined size of the graphs if E is ground.
Proof Sketch. It is easy to see that the only fact that could potentially be recursively derived from rdf is rdf:type[rdf:type  rdf:Property]; however, rdf:type[rdf:type 
rdf:Property]  rdf . Thus, sk(tr(S)) and sk(tr(S))  rdf may be treated as nonrecursive
Datalog programs.
The proposition then follows straightforwardly from Corollary 1 and the fact that ground
entailment in nonrecursive Datalog is in LogSpace in the size of the data (Abiteboul, Hull,
& Vianu, 1995), with the data being the input RDF graphs.
It turns out that we cannot obtain a LogSpace upper bound for RDFS entailment. In
fact, it turns out that ground rdfs-, and hence ground rdfs-D* - and rdfs-D-entailment, is
PTime-hard.
Proposition 12. There exist ground RDF graphs S and E such that the decision problems
S |=rdf s E, S |=rdfs-D* E, and S |=rdfs-D E are PTime-hard.
Proof. We proceed by reduction from the PTime-hard problem path system accessibility
(Jones & Laaser, 1974; Gary & Johnson, 1979), which is defined as:
Instance: A set X of nodes, subsets S, T  X of source and terminal nodes, and a relation
R  X  X  X.
Question: A node x  X is accessible if x  S or if there exist accessible nodes y, z  X
such that hx, y, zi  R. Is there an accessible terminal node t  T ?
In the remainder sp is short for rdfs:subPropertyOf.
We now encode this problem into RDFS. The graph G is constructed as follows:
 for every source node x  S include the triple hx, sp, spi,
 for every terminal node x  T include the triple ha, sp, xi, and
 for every tuple hx, y, zi  R include the triple hx, y, zi.
550

fiLogical Foundations of RDF(S) with Datatypes

We show that a node t  X is accessible iff G |=rdf s ht, sp, spi. It follows that there
exists an accessible node iff G |=rdf s ha, sp, spi.
() We proceed by induction. Base case: if t  S then ht, sp, spi  G, so clearly
G |=rdf s ht, sp, spi.
Induction step: consider ht, y, zi  R such that y, z are accessible. We have that ht, y, zi
is included in G and G |=rdf s hy, sp, spi and G |=rdf s hz, sp, spi, since y and z are accessible.
Condition 10 in Table 5 implies that G |=rdf s ht, sp, zi. By transitivity of sp (condition 9
in Table 5) we can subsequently conclude that G |=rdf s ht, sp, spi.
() Assume, on the contrary, that t  X is not accessible. It is then straightforward to
construct an rdfs-interpretation I such that I |=rdf s G and I 6|=rdf s ht, sp, spi, a contradiction.
Using the correspondence of Proposition 7, the results on the complexity of reasoning
in DL-LiteR (Calvanese et al., 2007), and the classical results on skolemization (Fitting,
1996) we obtain the following result for extensional RDFS entailment. Recall the notion of
standard use of the RDFS vocabulary from Definition 2.
Proposition 13. Let S and E be RDF graphs with only standard use of the RDFS vocabulary such that E E S. Then, the problem of deciding S |=erdf s E is NP-complete, and
NLogSpace-complete if E is ground.
Proof. Assume that E is ground. We first demonstrate membership.
We have that F O(sk(trerdfs (S))  erdfs-V ) is a theory of contextual FOL that is equivalent to a contextual DL-LiteR knowledge base (by Proposition 7). If E is ground, then,
as a straightforward consequence from Theorems 2 and 3,
erdfs-V
) |=c F O(trerdfs (E)).
S |=erdf s E iff F O(sk(trerdfs (S))  SE

A contextual DL-LiteR theory c (resp., formula c ) can be straightforwardly rewritten
to a corresponding classical DL-LiteR theory  (resp., formula ) such that
c |=c c iff  |= .
Since this transformation is linear in the size of the knowledge base, the complexity of
deciding satisfiability and entailment of contextual DL-LiteR knowledge bases is the same
as that of DL-LiteR knowledge bases, namely NLogSpace (Calvanese et al., 2007).
Hardness is shown by reduction from a known NLogSpace-hard problem: Graph reachability (Papadimitriou, 1994) can be encoded using subclass statements: edges in the
graph are represented in the RDF graph S by rdfs:subClassOf-triples and t is reachable
from s iff S |=erdfs {hs, rdfs:subClassOf, ti}.
This result immediately leads to the following NP algorithm for deciding S |=erdf s E, in
case E is not ground:
1. Guess a mapping  from blank nodes in E to ground terms in F O(sk(trerdfs (S))erdfs-V
SE ).
erdfs-V
erdfs
erdfs
2. Check whether F O(sk(tr
(S))  SE ) |=c F O(tr
(E)).
This algorithm is clearly sound and complete, since the theory F O(sk(trerdfs (S))  erdfs-V
SE )
is universal.
NP-hardness follows from NP-hardness of simple entailment (Proposition 10), which is
straightforwardly encoded into extensional RDFS entailment.
551

fide Bruijn & Heymans

For x-D-entailment with arbitrary datatype maps we obtain the following novel lower
bound.
Proposition 14. There are RDF graphs S and E and a datatype map D such that deciding
S |=s-D E is coNP-hard in the size of S.
Proof. We proceed by reduction from the complement of graph k-colorability for k  3,
i.e., the nonexistence of a k-coloring. This problem is coNP-complete (Gary & Johnson,
1979):
Instance: A graph G = hV, Ei and a positive integer k  |V | such that k  3.
Question: A k-coloring is an assignment from nodes to colors f : V  {1, 2, . . . , k} such
that no two adjacent nodes share the same color, i.e., if hu, vi  E, then f (u) 6= f (v). Is it
the case that there is no k-coloring?
Let D be a datatype map that includes rdf:XMLLiteral and that maps a URI d to
a datatype D(d) with an ordered value space of cardinality k, let S be the smallest RDF
graph such that:
 for every v  V , hv, rdf:type, di  S and
 for every hu, vi  E, hu, R, vi  S,
where R is a URI, and let H = {h :x, R, :xi}, where :x is a blank node.
We now show that G does not have a k-coloring if and only if S |=s-D H.
() Assume, on the contrary, that S 6|=s-D H, which means there is an s-D-interpretation I such that I |= S and I 6|= H. Therefore, (*) there is no s  IR such that
hs, si  IEXT(IS(R)). Consider any hu, vi  E; by (*) we have that IS(u) 6= IS(v). Since
hu, rdf:type, di, hv, rdf:type, di  S, IS(u), IS(v)  D(d), by condition 20 in Table 8. Now
let f (v) = IS(v) for every v  V . We have that f is a k-coloring, a contradiction.
() Analogously, if there exists a k-coloring, one can construct an s-D-interpretation
that is a model of S, but not of H.
A polynomial (resp., logspace) datatype map D is a datatype map for which holds
0
that deciding well-typedness of literals and deciding L2V D(u) (s) = L2V D(u ) (s0 ) and l =
L2V D(u) (s), where l is a plain literal and (s, u), (s0 , u0 ) are well-typed literals, can be done
in PTime (resp., LogSpace).
Considering definite datatype maps, we obtain the following lower bound from Theorem
5 and the data complexity of Datalog, exploiting Skolemization, analogous to Corollary 1,
and exploiting the fact that we need to take into account only a subset of the RDF(S)
axiomatic triples, analogous to Proposition 4.
Proposition 15. Let D be a definite polynomial datatype map. Then, the decision problems
S |=s-D E, S |=rdf-D E, and S |=rdfs-D E are NP-complete in the combined size of S and E,
and polynomial in the size of S. If E is ground, then the respective problems are in PTime.
It turns out that, analogous to the case without datatypes, we can further refine the
upper bounds of simple- and rdf -entailment.
Lemma 1. Let  be a theory and let D be a logspace datatype map. Then, ()n can be
computed in LogSpace.
552

fiLogical Foundations of RDF(S) with Datatypes

Entailment
|=s ,|=rdf ,|=rdf s
|=s ,|=rdf
|=rdf s
|=erdf s
|=erdf s

Restrictions on S
none
none
none
none
stand. RDFS

|=erdf s

stand. RDFS

Restrictions on E
none
ground
ground
none
stand. RDFS
stand. RDFS,
ground

Complexity
NP-complete
LogSpace
P-complete
NP-hard
NP-complete
NLogSpace-complete

Table 2: Complexity of Entailment S |=x E in RDF, measured in the combined size of S
and E
Entailment
x=s
x=rdf
x=rdfs


LogSpace
LogSpace
P-complete

D*
LogSpace
LogSpace
P-complete

definite D
LogSpace
LogSpace
P-complete

D
coNP-hard
coNP-hard
coNP-hard

Table 3: Complexity of Entailment S |=x-D E and S |=x-D* E, measured in the size of S
Proof. With WL we denote the set of plain and well-typed literals, and with < the lexicographical ordering over WL. If l is a plain literal, we define v l = l; if (s, u) is a well-typed
literal, v (s,u) = L2V D(u)(s) . The following algorithm returns the representation of a literal
00
l  WL in LogSpace: iterate over all literals l0 < l, until the least literal l00 such that v l = v l
00
is found; observe that deciding l0 < l and deciding v l = v l can be done in LogSpace.
From the lemma we obtain the following upper bound, by considerations analogous to
Proposition 11 and the fact that the axioms in D \rdf do not introduce recursion.
Proposition 16. Let S and E be RDF graphs and let D be a logspace datatype map. Then,
the problems S |=s-D* E and S |=rdf-D* E are decidable in LogSpace in the size of S, and in
the combined size of the graphs if E is ground.
Furthermore, if D is definite, the problems S |=s-D E and S |=rdf-D E are decidable in
LogSpace in the size of S, and in the combined size of the graphs if E is ground.
Table 2 summarizes the complexity of reasoning with the entailment regimes of RDF;
stand. RDFS stands for only standard use of the RDFS vocabulary; S and E are such
that E E S. The results in the first and fourth line of the table, and the upper bound
for ground rdfs-entailment were previously known (Gutierrez et al., 2004; de Bruijn et al.,
2005; ter Horst, 2005). To the best of our knowledge, the other results are novel.
Table 3 summarizes the complexity of reasoning with datatypes, measured in the size of
the entailing graph S. Definite D stands for D entailment restricted to definite datatype
maps. The LogSpace results require the datatype map D to be logspace as well, i.e., it
must be decidable in LogSpace whether two literals are equal under the interpretation given
by D. We suspect that many datatype maps of interest are logspace  examples are the
XML schema datatypes (Peterson et al., 2009). The upper bounds for rdfs- and rdfs-D*553

fide Bruijn & Heymans

entailment are known from the literature (ter Horst, 2005). To the best of our knowledge,
the other results in the table are novel.

6. Conclusions and Future Work
We have presented embeddings of the different RDF entailment regimes in F-Logic, and
we have shown how deductive database and description logic technology can be used for
reasoning with RDF.
Known complexity results from the fields of deductive databases and description logics
resulted in several novel upper bounds, in particular, ground simple- and rdf -entailment
are in LogSpace, as are the respective extensions with D* datatype semantics; non-ground
(resp., ground) erdfs-entailment of graphs with only standard use of the RDFS vocabulary
is in NP (resp., NLogSpace). To the best of our knowledge these are the first known upper
bounds for extensional RDFS entailment for a nontrivial subset of RDF graphs. For the
case of extensions of simple-, rdf -, and rdfs-entailment with D datatype support, the upper
bounds for non-ground and ground entailment are the same as for D* entailment when
considering definite datatypes, which do not require reasoning by case.
In addition, we have established several lower bounds through reductions from known
hard problems. In particular, rdfs-entailment turns out to be PTime-hard and simpleentailment extended with D datatype support turns out to be coNP-hard, both in the size
of the entailing graph. We also found a matching lower bound for the NLogSpace result for
ground erdfs-entailment of graphs with only standard use of the RDFS vocabulary.
The negative result concerning ground rdfs-entailment (i.e., PTime-hardness) might
come as a surprise because the language seems far less expressive than other PTime-hard
languages (e.g., variable-free Datalog (Dantsin et al., 2001) and DL-LiteR,u , an extension
of DL-LiteR (Calvanese et al., 2007)). The PTime-hardness proof suggests that the complexity originates from the possibility to use RDFS vocabulary in arbitrary places in RDF
statements, e.g., rdfs:subPropertyOf in the object position of a triple. Indeed, ground
entailment in the minimal RDFS fragment by Munoz, Perez, and Gutierrez (2009) can
be decided in O(nlogn).6 We suspect that the minimal RDFS fragment can be extended
with many useful features, such as class and property declarations and the RDFS metadata
vocabulary, without compromising the O(nlogn) upper bound. This is a topic for future
work.
The negative result concerning D entailment, even when not considering the RDFS vocabulary (i.e., coNP-hardness), suggests that one should restrict oneself to a weaker datatype
semantics such as D* or one should use only definite datatype maps, which precludes the
use of finite datatypes such as bool or int (Peterson et al., 2009). The latter approach was
taken in the specification of the tractable fragments (also called profiles) of OWL 2 (Motik
et al., 2009a), which has a datatype semantics similar to the D semantics.
The investigation reported on in this paper has formed the basis for the specification of
combinations of RIF-BLD rules (RIF Working Group, 2010a), which are essentially Horn
logic formulas, with RDF graphs. The RIF RDF and OWL specification (RIF Working
Group, 2010b) gives a model-theoretic account of the semantics of RIF-RDF combinations
6. This minimal RDFS disallows the use of any RDF(S) vocabulary besides the properties in the RDF(S)
ontology vocabulary, and allows the use of these properties only in the predicate position of triples.

554

fiLogical Foundations of RDF(S) with Datatypes

and suggests how such combinations can be embedded into RIF-BLD rules, based on the
embedding in Section 3.2. A particular challenge for future work is the combination of RDF
graphs with extensions of RIF-BLD that allow nonmonotonic negation in the rules, and the
interaction of this negation with blank nodes.
Another topic for future investigation is the precise relationship between extensional
RDFS and OWL. In particular, the relationship between extensional RDFS with only standard use of the RDFS vocabulary and the OWL 2 QL fragment of OWL 2 (Motik et al.,
2009a), which is based on contextual DL-LiteR . The embedding in the proof of Proposition
7 provides a promising starting point.

Acknowledgments
Jos de Bruijn was partially supported by the European Commission under the projects
Knowledge Web (IST-2004-507482) and ONTORULE (FP7 231875). Stijn Heymans was
partially supported by the Austrian Science Fund (FWF) under projects P20305 and P20840
and by the European Commission under the project ONTORULE (FP7 231875).

Appendix A. RDF(S) Semantics
In this appendix we define the notions of RDF, RDFS, eRDFS, D* , and D interpretations
(Hayes, 2004; ter Horst, 2005). Recall the definition of interpretation in Section 3.1.
RDF Interpretations

The RDF vocabulary consists of the following symbols:

rdf:type rdf:Property rdf:XMLLiteral rdf:nil rdf:List rdf:Statement rdf:subject
rdf:predicate rdf:object rdf:first rdf:rest rdf:Seq rdf:Bag rdf:value rdf:Alt
rdf: 1 rdf: 2 . . .
The RDF ontology vocabulary consists of the symbols rdf:type and rdf:Property. Note
that rdf: i, for any positive integer i, is part of the RDF vocabulary. Thus, the RDF
vocabulary is infinite. In the remainder, we omit the prefix rdf: when using the RDF
vocabulary.
A typed literal (s, XMLLiteral) is a well-typed XML literal if s is in the lexical space
of XMLLiteral, as defined in (Klyne & Carroll, 2004, Section 5.1); the XML value of s,
denoted xml (s), is in one-to-one correspondence with s. If s is not in the lexical space of
XMLLiteral, then (s, XMLLiteral) is an ill-typed XML literal.
Given an interpretation I, the class extension of an object x  IR is the set of elements
connected to x via type, i.e., the instances of x. It is defined as ICEXT(x) = {k | hk, xi 
IEXT(IS(type))}.
An interpretation I of a vocabulary V = hC, PL, T Li is an rdf-interpretation if V includes the RDF vocabulary and conditions 14 in Table 4 hold in I.
RDFS Interpretations

The RDFS vocabulary consists of:

rdfs:domain rdfs:range rdfs:Resource rdfs:Literal rdfs:Datatype rdfs:Class
rdfs:subClassOf rdfs:subPropertyOf rdfs:member rdfs:Container rdfs:label
rdfs:ContainerMembershipProperty rdfs:comment rdfs:seeAlso rdfs:isDefinedBy
555

fide Bruijn & Heymans

1

2
3

4

IS(type), IS(subject), IS(predicate), IS(object), IS(first), IS(rest),
IS(value), IS( 1), IS( 2), . . .  IP
IS(nil)  ICEXT(IS(List))
IP = ICEXT(IS(Property))
if (s, XMLLiteral)  T L is a well-typed XML literal, then
IL((s, XMLLiteral)) = xml (s), IL((s, XMLLiteral))  LV, and
IL((s, XMLLiteral))  ICEXT(IS(XMLLiteral))
if (s, XMLLiteral)  T L is an ill-typed XML literal, then
IL((s, XMLLiteral)) 
/ LV and IL((s, XMLLiteral)) 
/ ICEXT(IS(XMLLiteral))

Table 4: Conditions on RDF interpretations
The RDFS ontology vocabulary consists of the symbols rdfs:subClassOf, rdfs:subPropertyOf, rdfs:domain, rdfs:range, rdfs:Class, and rdfs:Datatype. In the remainder we
omit the prefix rdfs: when using the RDFS vocabulary.
We say that an rdf -interpretation I of a vocabulary V is an rdfs-interpretation if V
includes the RDFS vocabulary and conditions 515 depicted in Table 5 hold in I. As a
shortcut, we define IEXTp (o) = {s | hs, IS(o)i  IEXT(IS(p))}.
An RDF (resp, RDFS) axiomatic triple is a triple that is satisfied in every rdf -(resp,
rdfs-) interpretation. Conditions 1 and 5 correspond to the RDF(S) axiomatic triples in the
following way; see also (Hayes, 2004, Sections 3.1 and 4.1):
 IS(s)  IP corresponds to the axiomatic triple hs, type, rdf:Propertyi,
 IS(s)  IEXTp (o) corresponds to the axiomatic triple hs, p, oi, and
 IS(s)  ICEXT(IS(c)) corresponds to the axiomatic triple hs, type, ci.
Extensional RDFS Interpretations The normative RDFS semantics, reviewed above,
is also called the intensional RDFS semantics. The RDF semantics specification (Hayes,
2004) also defines an extensional RDFS semantics (eRDFS).
An rdfs-interpretation I is an erdfs-interpretation if the conditions depicted in Table 6
hold.
D* Interpretations Given a vocabulary V and a simple datatype map D, an s-interpretation of V is an s-D*-interpretation if V includes dom(D) and conditions 1619 in Table
7 are satisfied for each u  dom(D).
Given a vocabulary V and datatype map D, an rdf (resp., rdfs, erdfs)-interpretation I
of V is an rdf -D* (resp., rdfs-D* , erdfs-D* )-interpretation if I is an s-D* -interpretation.
D Interpretations Given a vocabulary V and a simple datatype map D, an s-D* -interpretation of V is an s-D-interpretation if it satisfies conditions 2022 in Table 8 for each
u  dom(D).
Given a vocabulary V and a datatype map D, an rdf (resp., rdfs, erdfs)-interpretation
I of V is an rdf -D (resp., rdfs-D, erdfs-D)-interpretation if I is an s-D-interpretation.
556

fiLogical Foundations of RDF(S) with Datatypes

5

6
7
8
9
10
11
12
13
14
15

IS(type), IS(member), IS(seeAlso), IS(isDefinedBy), IS(comment),
IS(label), IS(value), IS( 1), IS( 2), . . .  IEXTdomain (Resource)
IS(domain), IS(range), IS(subPropertyOf)  IEXTrdfs:domain (Property)
IS(subClassOf)  IEXTrdfs:domain (Class)
IS(subject), IS(predicate), IS(object)  IEXTdomain (Statement)
IS(first), IS(rest)  IEXTdomain (List)
IS(subject), IS(predicate), IS(object), IS(member), IS(first), IS(seeAlso),
IS(isDefinedBy), IS(value), IS( 1), IS( 2), . . .  IEXTrange (Resource)
IS(comment), IS(label)  IEXTrange (Literal)
IS(subPropertyOf)  IEXTrange (Property)
IS(type), IS(domain), IS(range), IS(subClassOf)  IEXTrange (Class)
IS(rest)  IEXTrange (List)
IS(Alt), IS(Bag), IS(Seq)  IEXTsubClassOf (Container)
IS(ContainerMembershipProperty)  IEXTsubClassOf (Property)
IS(isDefinedBy)  IEXTsubPropertyOf (seeAlso)
IS(XMLLiteral)  ICEXT(IS(Datatype))
IS(XMLLiteral)  IEXTsubClassOf (Literal)
IS(Datatype)  IEXTsubClassOf (Class)
IS( 1), IS( 2), . . .  ICEXT(IS(ContainerMembershipProperty))
IR = ICEXT(IS(Resource))
LV = ICEXT(IS(Literal))
if hx, yi  IEXT(IS(domain)) and hu, vi  IEXT(x), then u  ICEXT(y)
if hx, yi  IEXT(IS(range)) and hu, vi  IEXT(x), then v  ICEXT(y)
IEXT(IS(subPropertyOf)) is transitive and reflexive on IP
if hx, yi  IEXT(IS(subPropertyOf)), then IEXT(x)  IEXT(y)
if x  ICEXT(Class), then x  IEXTsubClassOf (Resource)
if hx, yi  IEXT(IS(subClassOf)), then ICEXT(x)  ICEXT(y)
IEXT(IS(subClassOf)) is transitive and reflexive on ICEXT(Class)
if x  ICEXT(ContainerMembershipProperty), then x  IEXTsubPropertyOf (member)
if x  ICEXT(Datatype), then x  IEXTsubClassOf (Literal)

Table 5: Conditions on RDFS interpretations

70
80
100
120

hx, yi  IEXT(IS(domain)) if and only if (if hu, vi  IEXT(x), then u  ICEXT(y))
hx, yi  IEXT(IS(range)) if and only if (if hu, vi  IEXT(x), then v  ICEXT(y))
hx, yi  IEXT(IS(subPropertyOf)) if and only if x, y  IP and IEXT(x)  IEXT(y)
hx, yi  IEXT(IS(subClassOf)) if and only if
x, y  ICEXT(Class) and ICEXT(x)  ICEXT(y)

Table 6: Conditions on eRDFS interpretations

557

fide Bruijn & Heymans

16 IS(u) = D(u)
17 IS(u)  ICEXT(IS(Datatype))
if (s, u)  T L and s  LD(u) , then IL((s, u)) = L2V D(u) (s)  LV and
18
IL((s, u))  ICEXT(D(u))
if (s, u)  T L and s 
/ LD(u) , then IL((s, u)) 
/ LV and
19
IL((s, u)) 
/ ICEXT(D(u))
Table 7: Conditions on D* interpretations
20 ICEXT(IS(u)) = V D(u)  LV
if (s, u0 )  T L, IS(u0 ) = IS(u) and s  LD(u) , then
21
IL((s, u0 )) = L2V D(u) (s)
22 if (s, u0 )  T L, IS(u0 ) = IS(u) and s 
/ LD(u) , then IL((s, u0 )) 
/ LV
Table 8: Conditions on D-interpretations

Appendix B. Embeddings
This appendix contains the axiomatization x of the entailment regimes x  {s, rdf, rdf,
erdfs} and the axiomatization of the datatype entailment regimes x-D* , x-D , referenced
from Sections 3 and 4.
Following the convention in Appendix A we omit the prefixes rdf: and rdfs: when using
the RDF and RDF vocabularies.
B.1 RDF Entailment Regimes
The axiomatization of the s, rdf , rdfs, and erdfs entailment regimes, denoted x , for
x  {s, rdf, rdfs, erdfs}, is defined in Table 9.
B.2 Datatype Entailment Regimes
The axiomatization of the D* and D entailment regimes, denoted x-D* and x-D , respectively, for x  {s, rdf, rdfs, erdfs}, is defined in Table 10.
Note that D entailment requires that whenever two URIs are mapped to the same
individual in a given interpretation, the URIs can be used interchangeably in typed literals.
However, since equality between URIs cannot be stated in RDF(S)  or indeed inferred 
we do not need to consider this case in our embeddings.
B.3 Extensional RDFS
Let V = hC, PL, T Li be a vocabulary. The mapping function trerdfs , defined in Section
3.3, deals with the eRDFS semantics of most of the RDF(S) vocabulary through direct
embedding. We define here the theory erdfs-V , which deals with the remainder of the
RDF(S) vocabulary.
558

fiLogical Foundations of RDF(S) with Datatypes

s = 
rdf = {tr(hs, p, oi) | hs, p, oi is an RDF axiomatic triple}
{t[type  XMLLiteral] | t  T L is a well-typed XML literal}
{illD(t) | t  T L is an ill-typed XML literal}
{x(y, z(y[x  z])  x[type  Property]),
x(x[type  XMLLiteral]  illD(x)  )}
rdfs = rdf  {tr(hs, p, oi) | hs, p, oi is an RDFS axiomatic triple}
{t[type  Literal] | t  PL}
{x(x[type  Resource]),
u, v, x, y(x[domain  y]  u[x  v]  u[type  y]),
u, v, x, y(x[range  y]  u[x  v]  v[type  y]),
x(x[type  Property]  x[subPropertyOf  x]),
x, y, z(x[subPropertyOf  y]  y[subPropertyOf  z]  x[subPropertyOf  z]),
x, y(x[subPropertyOf  y]  z1 , z2 (z1 [x  z2 ]  z1 [y  z2 ])),
x(x[type  Class]  x[subClassOf  Resource]),
x, y(x[subClassOf  y]  z(z[type  x]  z[type  y])),
x(x[type  Class]  x[subClassOf  x]),
x, y, z(x[subClassOf  y]  y[subClassOf  z]  x[subClassOf  z]),
x(x[type  ContainerMembershipProperty]  x[subPropertyOf  member]),
x(x[type  Datatype]  x[subClassOf  Literal]),
x(x[type  Literal]  illD(x)  )}
erdfs = rdfs  {x, y(u, v(u[x  v]  u[type  y])  x[domain  y]),
x, y(u, v(u[x  v]  v[type  y])  x[range  y]),
x, y(x[type  Property]  y[type  Property]  u, v(u[x  v] 
u[y  v])  x[subPropertyOf  y]),
x, y(x[type  Class]  y[type  Class]  u(u[type  x]  u[type  y]) 
x[subClassOf  y])}

Table 9: Axiomatization of the RDF entailment regimes.

erdfs-V = {trerdfs (hs, p, oi) | hs, p, oi is an RDF(S) axiomatic triple with
only standard use of the RDF(S) vocabulary}
{t : XMLLiteral | t  T L is a well-typed XML literal}
{t : illxml | t  T L is an ill-typed XML literal}
{t : Literal | t  PL}
{x(x : Literal  x : illxml  )}

Appendix C. Proofs
This appendix contains the proofs of the propositions and theorems in Sections 3 and 4.
C.1 Proof of Proposition 2
Consider a generalized RDF graph S, an interpretation I = hIR, IP, IS, IEXTi for which
holds that IP = IR includes every term in S, IS(c) = c for any URI c, IL(l) = l for
any typed literal l, and for every triple hs, p, oi  S, (s, o)  IEXT (p), and a blank node
559

fide Bruijn & Heymans

V -D*-= = {l = (s, u) | l  PL, (s, u)  T L is a well-typed literal, and
l = L2V D(u) (s)}
{(s, u) = (s0 , u0 ) | (s, u), (s0 , u0 )  T L are distinct well-typed literals and
0
L2V D(u) (s) = L2V D(u ) (s0 )}
x-D* = x  V -D*-= 
{(s, u)[type  u] | (s, u)  T L is a well-typed literal}
{illD(t) | t  T L is an ill-typed literal}
{u[type  Datatype] | u  dom(D)}
{x(illD(x)  x[type  u]  ) | u  dom(D)}
x-D = x-D* 
{(s, u0 )[type  u] | (s, u0 )  T L is a well-typed literal,
0
u  dom(D), and L2V D(u ) (s)  V D(u) }
{s[type  u] | s  PL, u  dom(D), and s  V D(u) }
{x(x[type  u]  dt(x, u)) | u  dom(D)}
{x(dt(x, u1 )  dt(x, u2 ))   | u1 , u2  dom(D).V D(u1 )  V D(u2 ) = }
{dt(l, u)   | l  PL, u  dom(D), l 6 V D(u) }
0
{dt((s, u), u0 )   | (s, u)  T L, u0  dom(D), L2V D(u) (s) 6 V D(u ) }
Table 10: Axiomatization of the datatype entailment regimes, x  {s, rdf, rdfs, erdfs}.
assignment A : bl(S)  IR that maps every blank node to itself. Clearly, (I, A) |= S, I |= S,
and I is an s-interpretation. Therefore, S is s-satisfiable.
It is easy to see that the following generalized RDF graph is rdf -, and hence rdfs- and
erdfs-unsatisfiable, by the negation in condition 4 in Table 4:
S = {h(<notXML, XMLLiteral), type, XMLLiterali}: (<notXML, XMLLiteral) is
an ill-typed XML literal, so by condition 4 in Table 4, IL((<notXML, XMLLiteral)) 
/
ICEXT(IS(XMLLiteral)). However, if the graph were satisfied in some rdf -interpretation,
it must be the case that IL((<notXML, XMLLiteral))  ICEXT(IS(XMLLiteral)), a contradiction.
Hayes (2004) observed that one can create a similar situation with a normal RDF graph
and a range constraint; this graph is rdfs- and hence erdfs-unsatisfiable.
C.2 Proof of Theorem 1
We first show that S 6|=x E iff tr(S)  x 6|=f tr(E). From this follows immediately that
S |=x E iff tr(S)  x |=f tr(E).
() Let V = hC, PL, T Li be the vocabulary of S and E and let L be an F-language that
conforms with V . Assume that S 6|=x E. This means that there is an x-interpretation
I = hIR, IP, LV, IS, IL, IEXTi such that I |= S and I 6|= E. We construct a corresponding
F-structure I = hU, U , IC , I , IP i in the following way:
(i) U = IR  IP,
(ii) IF (t) = IS(t) for every URI reference t  C, IF (t) = t for every plain literal t  PL,
and IF (t) = IL(t) for every typed literal t  T L,
560

fiLogical Foundations of RDF(S) with Datatypes

(iii) I (k) = IEXT(k) for every k  IP and I (k) =  for every k 
/ IP,
(iv) IP (illD) = {u | t  T L is an ill-typed XML literal and IL(t) = u}.
It is straightforward to verify that I |=f tr(S)  x and I6|=f
x 6|=f tr(E).

tr(E).

Hence, tr(S) 

() Assume that tr(S)  x 6|=f tr(E). This means that (by (Fitting, 1996, Theorem 5.9.4)
and Proposition 6) there is a Herbrand F-structure I = hU, U , IC , I , IP i such that
I |=f tr(S)  x and I6|=f tr(E). Since I is a Herbrand F-structure, U includes all constant
symbols, and IC maps every constant symbol to itself. We construct a corresponding
interpretation I = hIR, IP, LV, IS, IL, IEXTi as follows:
(i) IP = {p | hp, IF (Property)i  I (IF (type))}  {p | s, o.hs, oi  I (p)},
(ii) LV = PL  {xml(s) | ((s, XMLLiteral)  T L  (s, XMLLiteral) is a well-typed XML
literal)}  {l | hl, IF (Literal)i  I (IF (type))},
(iii) IR = U  LV,
(iv) IS(t) = IF (t) for every URI t  C, IL((s, u)) = xml(s) if (s, u)  T L is a well-typed
XML literal; IL((s, u)) = IF ((s, u)) for (s, u)  T L if (s, u)  T L is not a well-typed
XML literal, and
(v) for any p  U and any hs, oi  I (p): hs0 , o0 i  IEXT(p), where s0 (resp., o0 ) is: if there
is some (t, XMLLiteral)  T L such that (t, XMLLiteral) is a well-typed XML literal
and IF ((t, XMLLiteral)) = s (resp.,    = o), then s0 = xml(t) (resp., o0 = xml(t));
otherwise s0 = s (resp., o0 = o).
One can verify that I is an x-interpretation, I |= S, and I 6|= E. Hence, S 6|= E.
The second part of the theorem can be shown analogously.
C.3 Proof of Proposition 4
By Corollary 1 we have that S |=x E iff sk(tr(S))  x |=f tr(E). Therefore, we need to
show sk(tr(S))  xSE |=f tr(E) iff sk(tr(S))  x |=f tr(E).
() Trivial, since sk(tr(S))  xSE  sk(tr(S))  x .
() Consider the case x = erdfs. Let I be a minimal Herbrand model of sk(tr(S))  x and
let I0 be obtained from I by removing all triples involving container membership properties
n that appear in x \xSE . We can verify, e.g., by doing a case analysis on the shape of
the triples in S, that I0 is a minimal model of sk(tr(S))  xSE . Similarly, one can verify
that if I |=f tr(E), then I0 |=f tr(E).
Analogous for x  {s, rdf, rdfs}.
C.4 Proof of Theorem 2
We prove both directions by contraposition.
() Assume trerdfs (S)  erdfs-V 6|=f trerdfs (E). This means that there is a Herbrand Fstructure I = hU, U , IC , I , IP i such that I |=f trerdfs (S)  erdfs-V and I6|=f trerdfs (E).
561

fide Bruijn & Heymans

We define xml0 (x) = xml(s) if x is a well-typed XML literal (s, XMLLiteral); otherwise
xml0 (x) = x. We construct a corresponding interpretation I = hIR, IP, LV, IS, IL, IEXTi as
follows:
(i) LV = {xml0 (l) | l U IF (Literal)},
(ii) IP = IR = U  LV  {type, subClassOf, domain, range, subPropertyOf},
(iii) IS(t) = t for every URI reference t in S, E, or the RDF(S) vocabulary,
(iv) IL(t) = xml0 (t) for any t  T L,
(v) for any p  U and any hs, oi  I (p), hxml0 (s), xml0 (o)i  IEXT(p),
(vi) IEXT(type) is the smallest relation such that
(a) IEXT(type)  {hxml0 (s), xml0 (o)i | s U o};
(b) ICEXT(Resource) = ICEXT(Class) = IR; and
(c) ICEXT(Property) = IP,
(vii) IEXT(domain) is the set of all tuples hx, yi, x, y  IR, such that (if hu, vi  IEXT(x),
then u  ICEXT(y)); analogous for IEXT(subClassOf), IEXT(subPropertyOf), and
IEXT(range) (see Table 6 for the precise conditions).
One can verify that I |= S, and I 6|= E, since S and E have only standard use of the RDFS
vocabulary, E does not include occurrences of Resource, Class, or Property, and the class
and property vocabularies of E are subsets of the respective vocabularies of S.
I clearly satisfies conditions 14 in Table 4, conditions 615 in Table 5, and conditions
0
7 120 in Table 6. To verify that I satisfies condition 5 in Table 4 one only needs to keep in
mind that ICEXT(Resource) = ICEXT(Class) = IR and ICEXT(Property) = IP. So, I
is an erdfs-interpretation and thus S 6|=erdfs E.
() Assume S 6|=erdf s E. This means there is some erdfs-interpretation I0 such that, for any
URI t, IS(t) = t (making I0 similar to a Herbrand interpretation) and such that I0 |= S and
I0 6|= E. Let I = hIR, IP, LV, IS, IL, IEXTi be an erdfs-interpretation obtained from I0 such
that IP = IR and ICEXT(IS(Class)) = IR, and such that IEXT is minimally extended
to satisfy the semantic conditions in Tables 4, 5, and 6. Clearly, there must be such an
erdfs-interpretation and I |= S. We also have, by the restrictions on the class and property
vocabularies as well as the non-occurrence in E of Resource, Class, and Property, that
I 6|= E.
We construct a corresponding F-Logic interpretation I = hU, U , IC , I , IP i as follows:
(i) U = IR, (ii) IF (t) = t for every URI or plain literal t, IF (t) = t for every t  T L, (iii)
I (k) = IEXT(k) for every k  U , and (iv) U = IEXT(IS(type)).
It can be straightforwardly verified that I |=f trerdfs (S)  erdfs-V and I6|=f trerdfs (E).
Therefore, it must be the case that trerdfs (S)  erdfs-V 6|=f trerdfs (E).
562

fiLogical Foundations of RDF(S) with Datatypes

C.5 Proof of Proposition 7
 is obtained from F O(trerdfs (S)  erdfs-V ) in the following way:
(i) Class membership and property value statements of the forms A(a), P (a1 , a2 ) are
included as such,
(ii) Subclass and subproperty statements are included as such,
(iii) Domain constraints of the form x, y(P (x, y)  A(x)) are rewritten to role-typing
statements of the form x(y(P (x, y))  A(x)),
(iv) Range constraints of the form x, y(P (x, y)  A(y)) are rewritten to role-typing statements of the form x(y(P (y, x))  A(x)), and
(v) Constraints of the form x(A(x)  B(x)  ) are rewritten to x(A(x)  B(x)).
 and F O(trerdfs (S)) are obviously equivalent, and it is easy to verify that  is the FOL
equivalent of a contextual DL-LiteR knowledge base.
C.6 Proof of Theorem 4
We first establish the second part of the theorem, i.e., S is x-D*-satisfiable iff tr(S)  x-D*
has a model.
() Let V = hC, PL, T Li be the vocabulary of S and let L be an F-language that conforms
with V . Assume that S is x-D*-satisfiable. This means that there is an x-D*-interpretation
I = hIR, IP, LV, IS, IL, IEXTi such that I |= S. We construct a corresponding F-structure
I = hU, U , IC , I , IP i in the following way (analogous to the construction in the 
direction in the proof of Theorem 1):
(i) U = IR  IP,
(ii) IF (t) = IS(t) for every URI reference t  C, IF (t) = t for every plain literal t  PL,
IF (t) = IL(t) for every typed literal t  T L,
(iii) I (k) = IEXT(k) for every k  IP,
(iv) IP (illD) = {u | t  T L is an ill-typed literal and IL(t) = u}.
Clearly, I |=f tr(S).
If a literal t  T L is an ill-typed XML literal, then clearly it is an ill-typed literal.
Then, there is no ill-typed literal t such that IS(t)  LV (by condition 19 in Table 7),
and hence there is no ill-typed literal t such that IS(t)  ICEXT(Literal) or IS(t) 
ICEXT(XMLLiteral), by condition 4 in Table 4 (if x = rdf, x = rdfs, or x = erdfs) and
condition 6 in Table 5 (if x = rdfs or x = erdfs). Satisfaction of x is then established
straightforwardly.
Consider a well-typed literal (s, u) and a plain literal l. In case l = L2V D(u) (s),
IL((s, u)) = l, by condition 18 in Table 7, and thus IF ((s, u)) = IF (l) = l and I |=f l = (s, u),
by (ii). Analogous for the case of two distinct well-typed literals. Therefore, I |=f V -D*-= .
563

fide Bruijn & Heymans

Consider the definition of x-D* in Table 10. We have established that I |=f x 
Satisfaction of the second, first, third, and fourth sets of formulas in the table
follows immediately from, respectively, (iv), and conditions 18, 17, and 19 in Table 7.
Therefore, I |=f x-D* .
This establishes I |=f tr(S)  x-D* .
V -D*-= .

() Assume that tr(S)  x-D* has a model. Let  = tr(S)  x-D* .
Let  be obtained from  by replacing every occurrence of = with  and adding the
usual congruence axioms (cf. Fitting, 1996, Chapter 9). It is known that this axiomatization
of equality preserves satisfiability and entailment in first-order logic (Fitting, 1996, Theorem
9.3.9). This is also the case for F-Logic, by Proposition 1.
We extend the signature of  with a set of URI references C 0 , disjoint from C, with
cardinality |bl(S)|; i.e., the signature is 0 = hC  PL  T L  C 0 , P  {}i. Since  has
a model (as  has), there exists, by classical results, a Herbrand F-structure I such that
I |=f  . We have that U = C  C 0  PL  T L.
For any u  U , define  as follows:
 if u  C such that u  dom(D), (u) = D(u),
 if (s, u)  T L is a well-typed literal and u  dom(D), ((s, u)) = L2V D(u) (s),
 otherwise, (u) = u.
We construct a corresponding interpretation I = hIR, IP, LV, IS, IL, IEXTi:
(i) IP = {(p) | hp, IF (Property)i  I (IF (type))}  {(p) | s, o.hs, oi  I (p)},
(ii) LV = PL  {L2V D(u) (s) | (s, u)  T L, u  dom(D), (s, u) is a well-typed literal)} 
{(l) | hl, IF (Literal)i  I (IF (type)) & (x = rdfs or x = erdfs)},
(iii) IR = U  LV,
(iv) IS(u) = (u) for every URI reference u  C; IL((s, u)) = ((s, u)) for every (s, u) 
T L, and
(v) for any p  IP, IEXT is the smallest set such that hs, oi  I (p) implies h(s), (o)i 
IEXT((p)).
It is easy to see that I |= S. Remains to verify that I is an x-D*-interpretation. Verifying
that I is an x-interpretation is straightforward. It remains to verify the satisfaction of the
conditions in Table 7.
Satisfaction of condition 16 follows directly from the definition of I and . For condition
17, we have that u  dom(D) and thus u[type  Datatype]  x-D* . As I satisfies x-D*
we have that hIF (u), IF (Datatype)i  I (IF (type)), and so h(IF (u)), (IF (Datatype))i 
IEXT((IF (type))). By construction of IS, this yields IS(u)  ICEXT(IS(Datatype)).
Consider some (s, u)  T L such that u  dom(D) and s  LD(u) . Then, (s, u) is welltyped, and so IL((s, u)) = L2V D(u) (s)  LV. By definition of x-D* , we have (s, u)[type 
u]  x-D* ; it follows that L2V D(u) (s)  ICEXT(D(u)). This establishes satisfaction of
condition 18.
564

fiLogical Foundations of RDF(S) with Datatypes

Condition 19 is satisfied by the fact that LV does not contain ill-typed literals. Indeed,
PL  {L2V D(u) (s) | (s, u)  T L, u  dom(D), (s, u) is a well-typed literal)} does not
contain ill-typed literals and if x is rdfs or erdfs, there is no ill-typed literal t such that
I |=f t[type  Literal], by the last axiom in the definition of rdfs in Table 9.
It is easy to verify, for both directions, that we have I 6|= E iff I6|=f tr(E). The first part of
the theorem follows.
C.7 Proof of Theorem 5
We first show correspondence of satisfiability.
() Let V = hC, PL, T Li be the vocabulary of S and E, let rdf be an x-D-interpretation
that satisfies S and let L be an F-language that conforms with V . We construct an Fstructure I = hU, U , IC , I , IP i that corresponds to I, using steps (i)(iv) as in the ()
direction of the proof of Theorem 4, with the additional step
(v) IP (dt) = {hx, ui | x  ICEXT(u) and u  ran(D)}.
From the argument in the () direction in the proof of Theorem 4 follows that I |=f tr(S)
x-D* . Consider x-D \ x-D* , as defined in Table 10. Satisfaction of the first set follows
immediately from conditions 20 and 21 in Table 8. Satisfaction of the second set follows
immediately from condition 20 in Table 8. Satisfaction of the third set follows immediately
from (v).
Consider any two u1 , u2  dom(D) such that V D(u1 )  V D(u2 ) = . By condition 20 in
Table 8, ICEXT(IS(u1 ))ICEXT(IS(u2 )) = . From (v) then follows that there is no k  U
such that hk, IF (u1 )i  IP (dt) and hk, IF (u2 )i  IP (dt). Consequently, I6|=f x(dt(x, u1 ) 
dt(x, u2 )) and thus the fourth set of sentences is satisfied.
Consider some (s, u)  T L and some u0  dom(D) such that IL((s, u)) = L2V D(u) (s) 
/
0)
0
D(u
V
. By condition 20 in Table 8, ICEXT(IS(u0 )) = V D(u ) , and thus IL((s, u)) 
/
0
0
0
ICEXT(IS(u )). From (v) follows hIF ((s, u)), IF (u )i 
/ IP (dt) and thus I6|=f dt((s, u), u ),
establishing satisfaction of the sixth set. The argument for the fifth set is obtained by
replacing (s, u)  T L with l  PL.
We thus obtain I |=f x-D . Therefore, tr(S)  x-D has a model.
() Assume  = tr(S)  x-D has a model.
Let  be obtained from  as in the proof of Theorem 4 and let I = hU, U , IC ,
I , IP i be a Herbrand F-structure that is a model of  . We construct a corresponding
interpretation I = hIR, IP, LV, IS, IL, IEXTi in the following way. W.l.o.g. we assume that
no value space V d , for d  ran(D), contains any typed literal t  T L.
We observe that (*) for any two d1 , d2  dom(D) must hold that either V D(d1 ) and
D(d
2 ) are disjoint or their overlap is infinite, since D is definite. In addition, if V D(d1 ) and
V
V D(d2 ) are disjoint, then, by satisfaction of the fourth set in the definition of x-D , (**)
I6|=f t[type  d1 ]  t[type  d2 ] for any t  C  C 0 .
For a given URI u  C  C 0 we define the mapping  as follows:
 if u  dom(D), then (u) = D(u),
 if hu, u0 i  I (type), for some u0  dom(D), then (u) = v, where v is such that
565

fide Bruijn & Heymans

 v  V D(u1 )      V D(un ) , where u1 , . . . , un  dom(D) are all the datatype
identifiers such that hu, u1 i, . . . , hu, un i  I (type);
 there is no u0  C such that v = (u0 ); and
0

 there is no (s, u0 )  T L such that u0  dom(D) and v = L2V D(u ) (s);
such a v must exist, because u cannot be a member of two disjoint datatypes, by (**),
and V D(u1 )      V D(un ) is an infinite set, by Definition 4,
 otherwise, (u) = u.
For a given literal l  PL  T L we define  as:
 if l = (s, u)  T L is a well-typed literal, then (s, u) = L2V D(u) (s),
 otherwise (l) = IF (l).
One can verify that  is such that for any two distinct t1 , t2  C  PL  T L, either (t1 ) =
(t2 ) and ht1 , t2 i  IP () (by definition of V -D*-= ) or (t1 ) 6= (t2 ).
We construct an RDF interpretation I = hIR, IP, LV, IS, IL, IEXTi, similar to the construction in the () direction of the proof of Theorem 4. Note that the respective constructions differ only in steps (ii) and (v).
(i) IP = {(p) | hp, IF (Property)i  I (IF (type))}  {(p) | s, o.hs, oi  I (p)},
S
(ii) LV = PL  {V d | d  ran(D)}  {(l) | hl, IF (Literal)i  I (IF (type)) & (x =
rdfs or x = erdfs)},
(iii) IR = U  LV,
(iv) IS(u) = (u) for every u  C; IL((s, u)) = ((s, u)) for every (s, u)  T L, and
(v) IEXT is the smallest set such that
 ICEXT(IS(u)) = V D(u) for every u  dom(D), and
 for any p  IP and hs, oi  I (p), h(s), (o)i  IEXT((p)).
Satisfaction of all conditions up to and including 19 are established analogous to the ()
direction in the proof of Theorem 4. Notice that condition 20 is satisfied in I by (v).
Consider a typed literal t = (s, u0 )  T L and a datatype identifier u  dom(D). If
IS(u0 ) = IS(u), then it must be the case that D(u0 ) = D(u), by construction of I. If
0
0
s  LD(u ) , then (s, u0 ) is a well-typed literal, and thus IL((s, u0 )) = L2V D(u ) = L2V D(u) ,
0
by (iv). If s 
/ LD(u ) , then (s, u0 ) is an ill-typed literal and IL((s, u0 )) = (s, u0 ) 
/ LV,
because LV does not contain ill-typed literals. Therefore, conditions 21 and 22 in Table 8
are satisfied.
Consequently, I is an x-D-interpretation. We have that I |= S and and thus S is x-Dsatisfiable.
The second part of the theorem follows from the observation that, for both directions, we
have I 6|= E iff I6|=f tr(E).
566

fiLogical Foundations of RDF(S) with Datatypes

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations of Databases. Addison-Wesley.
Borgida, A. (1996). On the relative expressiveness of description logics and predicate logics.
Artificial Intelligence, 82 (12), 353367.
Brickley, D., & Guha, R. V. (2004). RDF vocabulary description language 1.0: RDF schema.
Recommendation 10 February 2004, W3C.
Calvanese, D., Giacomo, G. D., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning and efficient query answering in description logics: the dl-lite family. Journal
of Automated Reasoning, 39, 385429.
Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity and expressive
power of logic programming. ACM Computing Surveys (CSUR), 33 (3), 374425.
de Bruijn, J., Franconi, E., & Tessaris, S. (2005). Logical reconstruction of normative RDF.
In Proceedings of the Workshop OWL: Experiences and Directions (OWLED-2005).
de Bruijn, J., & Heymans, S. (2007). Logical foundations of (e)RDF(S): Complexity
and reasoning. In Proceedings of the 6th International Semantic Web Conference
(ISWC2007), pp. 8699. Springer.
de Bruijn, J., & Heymans, S. (2008). On the relationship between description logic-based
and f-logic-based ontologies. Fundamenta Informaticae, 82 (3), 213236.
Fitting, M. (1996). First Order Logic and Automated Theorem Proving (second edition).
Springer.
Gary, M. R., & Johnson, D. S. (1979). Computers and Intractability  A Guide to the
Theory of NP-Completeness. W.H. Freeman and Company, New York, NY, USA.
Gutierrez, C., Hurtado, C., & Mendelzon, A. O. (2004). Foundations of semantic web
databases. In Proceedings of the 23rd ACM Symposium on Principles of Database
Systems (PODS2004), pp. 95106. ACM Press.
Gutierrez, C., Hurtado, C. A., Mendelzon, A. O., & Perez, J. (2010). Foundations of
semantic web databases. Journal of Computer and System Sciences. In Press.
Hayes, P. (2004). RDF semantics. Recommendation 10 February 2004, W3C.
Jones, N. D., & Laaser, W. T. (1974). Complete problems for deterministic polynomial
time. In Proceedings of the 6th Annual ACM Symposium on Theory of Computing
(STOC1974), pp. 4046, Seattle, Washington, USA. ACM Press.
Kifer, M., Lausen, G., & Wu, J. (1995). Logical foundations of object-oriented and framebased languages. Journal of the ACM, 42 (4), 741843.
Klyne, G., & Carroll, J. J. (2004). Resource description framework (RDF): Concepts and
abstract syntax. Recommendation 10 February 2004, W3C.
Motik, B., Grau, B. C., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2009a). OWL 2 web
ontology language profiles. Recommendation 27 October 2009, W3C.
Motik, B., Patel-Schneider, P. F., & Parsia, B. (2009b). OWL 2 web ontology language
structural specification and functional-style syntax. Recommendation 27 October
2009, W3C.
567

fide Bruijn & Heymans

Munoz, S., Perez, J., & Gutierrez, C. (2009). Simple and efficient minimal RDFS. Journal
of Web Semantics, 7 (3), 220234.
Papadimitriou, C. H. (1994). Computational Complexity. Addison Wesley.
Patel-Schneider, P. F., Hayes, P., & Horrocks, I. (2004). OWL web ontology language
semantics and abstract syntax. Recommendation 10 February 2004, W3C.
Peterson, D., Gao, S., Malhotra, A., Sperberg-McQueen, C. M., & Thompson, H. S. (2009).
W3C XML schema definition language (XSD) 1.1 part 2: Datatypes. Working draft
3 December 2009, W3C.
RIF Working Group (2010a). RIF basic logic dialect. Recommendation 22 June 2010, W3C.
RIF Working Group (2010b). RIF RDF and OWL compatibility. Recommendation 22 June
2010, W3C.
ter Horst, H. J. (2005). Completeness, decidability and complexity of entailment for RDF
schema and a semantic extension involving the OWL vocabulary. Journal of Web
Semantics, 3 (23), 79115.
Yang, G., Kifer, M., & Zhao, C. (2003). FLORA-2: A rule-based knowledge representation and inference infrastructure for the semantic web. In Proceedings of the Second International Conference on Ontologies, Databases and Applications of Semantics
(ODBASE2003). Springer.

568

fiJournal of Artificial Intelligence Research 38 (2010) 687-755

Submitted 01/10; published 08/10

Automatic Induction of Bellman-Error Features
for Probabilistic Planning
Jia-Hong Wu
Robert Givan

JW @ ALUMNI . PURDUE . EDU
GIVAN @ PURDUE . EDU

Electrical and Computer Engineering
Purdue University, W. Lafayette, IN 47907 USA

Abstract
Domain-specific features are important in representing problem structure throughout machine
learning and decision-theoretic planning. In planning, once state features are provided, domainindependent algorithms such as approximate value iteration can learn weighted combinations of
those features that often perform well as heuristic estimates of state value (e.g., distance to the
goal). Successful applications in real-world domains often require features crafted by human experts. Here, we propose automatic processes for learning useful domain-specific feature sets with
little or no human intervention. Our methods select and add features that describe state-space regions of high inconsistency in the Bellman equation (statewise Bellman error) during approximate
value iteration. Our method can be applied using any real-valued-feature hypothesis space and
corresponding learning method for selecting features from training sets of state-value pairs. We
evaluate the method with hypothesis spaces defined by both relational and propositional feature
languages, using nine probabilistic planning domains. We show that approximate value iteration
using a relational feature space performs at the state-of-the-art in domain-independent stochastic
relational planning. Our method provides the first domain-independent approach that plays Tetris
successfully (without human-engineered features).

1. Introduction
There is a substantial gap in performance between domain-independent planners and domainspecific planners. Domain-specific human input is able to produce very effective planners in all
competition planning domains as well as many game applications such as backgammon, chess, and
Tetris. In deterministic planning, work on TLPLAN (Bacchus & Kabanza, 2000) has shown that
simple depth-first search with domain-specific human input, in the form of temporal logic formulas
describing acceptable paths, yields an effective planner for a wide variety of competition domains.
In stochastic planning, feature-based value-function representations have been used with humanselected features with great success in applications such as backgammon (Sutton & Barto, 1998;
Tesauro, 1995) and Tetris (Bertsekas & Tsitsiklis, 1996). The usage of features provided by human experts is often critical to the success of systems using such value-function approximations.
Here, we consider the problem of automating the transition from domain-independent planning to
domain-specific performance, replacing the human input with automatically learned domain properties. We thus study a style of planner that learns from encountering problem instances to improve
performance on subsequently encountered problem instances from the same domain.
We focus on stochastic planning using machine-learned value functions represented as linear
combinations of state-space features. Our goal then is to augment the state-space representation
c
2010
AI Access Foundation. All rights reserved.

fiW U & G IVAN

during planning with new machine-discovered features that facilitate accurate representation of the
value function. The resulting learned features can be used in representing the value function for
other problem instances from the same domain, allowing amortization of the learning costs across
solution of multiple problem instances. Note that this property is in contrast to most competition
planners, especially in deterministic planning, which retain no useful information between problem instances. Thus, our approach to solving planning problems can be regarded as automatically
constructing domain-specific planners, using domain-independent techniques.
We learn features that correlate well to the statewise Bellman error of value functions encountered during planning, using any provided feature language with a corresponding learner to select
features from the space. We evaluate this approach using both relational and propositional feature
spaces. There are other recent approaches to acquiring features in stochastic planning with substantial differences from our approach which we discuss in detail in Section 5 (Patrascu, Poupart,
Schuurmans, Boutilier, & Guestrin, 2002; Gretton & Thiebaux, 2004; Sanner & Boutilier, 2009;
Keller, Mannor, & Precup, 2006; Parr, Painter-Wakefield, Li, & Littman, 2007). No previous work
has evaluated the selection of relational features by correlation to statewise Bellman error.
Recent theoretical results (Parr et al., 2007) for uncontrolled Markov processes show that exactly capturing statewise Bellman error in new features, repeatedly, will lead to convergence to the
uncontrolled optimal value for the value function selected by linear-fixed-point methods for weight
training. Unfortunately for machine-learning approaches to selecting features, these results have
not been transferred to approximations of statewise Bellman-error features: for this case, the results
in the work of Parr et al. (2007) are weaker and do not imply convergence. Also, none of this theory has been transferred to the controlled case of interest here, where the analysis is much more
difficult because the effective (greedy) policy under consideration during value-function training is
changing.
We consider the controlled case, where no known theoretical properties similar to those of Parr
et al. (2007) have been shown. Lacking such theory, our purpose is to demonstrate the capability
of statewise Bellman error features empirically, and with rich representations that require machine
learning techniques that lack approximation guarantees. Next, we give an overview of our approach, introducing Markov decision processes, value functions, Bellman error, feature hypothesis
languages and our feature learning methods.
We use Markov decision processes (MDPs) to model stochastic planning problems. An MDP is
a formal model of a single agent facing a sequence of action choices from a pre-defined action space,
and transitioning within a pre-defined state space. We assume there is an underlying stationary
stochastic transition model for each available action from which state transitions occur according to
the agents action choices. The agent receives reward after each action choice according to the state
visited (and possibly the action chosen), and has the objective of accumulating as much reward as
possible (possibly favoring reward received sooner, using discounting, or averaging over time, or
requiring that the reward be received by a finite horizon).
MDP solutions can be represented as state-value functions assigning real numbers to states. Informally, in MDP solution techniques, we desire a value function that respects the action transitions
in that good states will either have large immediate rewards or have actions available that lead to
other good states; this well-known property is formalized in Bellman equations that recursively
characterize the optimal value function (see Section 2). The degree to which a given value function
fails to respect action transitions in this way, to be formalized in the next section, is referred to as
the Bellman error of that value function, and can be computed at each state.
688

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Intuitively, statewise Bellman error has high magnitude in regions of the state space which
appear to be undervalued (or overvalued) relative to the action choices available. A state with high
Bellman error has a locally inconsistent value function; for example, a state is inconsistently labeled
with a low value if it has an action available that leads only to high-value states. Our approach is to
use machine learning to fit new features to such regions of local inconsistency in the current value
function. If the fit is perfect, the new features guarantee we can represent the Bellman update
of the current value function. Repeated Bellman updates, called value iteration, are known to
converge to the optimal value function. We add the learned features to our representation and then
train an improved value function, adding the new features to the available feature set.
Our method for learning new features and using them to approximate the value function here
can be regarded as a boosting-style learning approach. A linear combination of features can be
viewed as a weighted combination of an ensemble of simple hypotheses. Each new feature learned
can be viewed as a simple hypothesis selected to match a training distribution focused on regions
that the previous ensemble is getting wrong (as reflected in high statewise Bellman error throughout
the region). Growth of an ensemble by sequentially adding simple hypotheses selected to correct
the error of the ensemble so far is what we refer to as boosting style learning.
It is important to note that our method scores candidate features by correlation to the statewise
Bellman error of the current value function, not by minimizing the statewise Bellman error of some
value function found using the new candidate feature. This pre-feature-addition scoring is much
less expensive than scoring that involves retraining weights with the new feature, especially when
being repeated many times for different candidates, relative to the same current value function. Our
use of pre-feature-addition scoring to select features for the controlled setting enables a much more
aggressive search for new features than the previously evaluated post-feature-addition approach
discussed in the work of Patrascu et al. (2002).
Our approach can be considered for selecting features in any feature-description language for
which a learning method exists to effectively select features that match state-value training data.
We consider two very different feature languages in our empirical evaluation. Human-constructed
features are typically compactly described using a relational language (such as English) wherein the
feature value is determined by the relations between objects in the domain. Likewise, we consider
a relational feature language, based on domain predicates from the basic domain description. (The
domain description may be written, for example, in a standard planning language such as PPDDL in
Younes, Littman, Weissman, & Asmuth, 2005.) Here, we take logical formulas of one free variable
to represent features that count the number of true instantiations of the formula in the state being
evaluated. For example, the number of holes feature that is used in many Tetris experiments
(Bertsekas & Tsitsiklis, 1996; Driessens, Ramon, & Gartner, 2006) can be interpreted as counting
the number of empty squares on the board that have some other filled squares above them. Such
numeric features provide a mapping from states to natural numbers.
In addition to this relational feature language, we consider using a propositional feature representation in our learning structure. Although a propositional representation is less expressive
than a relational one, there exist very effective off-the-shelf learning packages that utilize propositional representations. Indeed, we show that we can reformulate our feature learning task as a
related classification problem, and use a standard classification tool, the decision-tree learner C4.5
(Quinlan, 1993), to create binary-valued features. Our reformulation to classification considers
only the sign, not the magnitude, of the statewise Bellman error, attempting to learn features that
characterize the positive-sign regions of the state space (or likewise the negative-sign regions). A
689

fiW U & G IVAN

standard supervised classification problem is thus formulated and C4.5 is then applied to generate
a decision-tree feature, which we use as a new feature in our value-function representation. This
propositional approach is easier to implement and may be more attractive than the relational one
when there is no obvious advantage in using relational representation, or when computing the exact
statewise Bellman error for each state is significantly more expensive than estimating its sign. In
our experiments, however, we find that our relational approach produces superior results than our
propositional learner. The relational approach also demonstrates the ability to generalize features
between problem sizes in the same domain, an asset unavailable in propositional representations.
We present experiments in nine domains. Each experiment starts with a single, constant feature, mapping all states to the same number, forcing also a constant value function that makes no
distinctions between states. We then learn domain-specific features and weights from automatically
generated sampled state trajectories, adjusting the weights after each new feature is added. We
evaluate the performance of policies that select their actions greedily relative to the learned value
functions. We evaluate our learners using the stochastic computer-game Tetris and seven planning domains from the two international probabilistic planning competitions (Younes et al., 2005;
Bonet & Givan, 2006). Our method provides the first domain-independent approach to playing
Tetris successfully (without human-engineered features). Our relational learner also demonstrates
superior success ratio in the probabilistic planning-competition domains as compared both to our
propositional approach and to the probabilistic planners FF-Replan (Yoon, Fern, & Givan, 2007)
and FOALP (Sanner & Boutilier, 2006, 2009). Additionally, we show that our propositional learner
outperforms the work of Patrascu et al. (2002) on the same SysAdmin domain evaluated there.

2. Background
Here we present relevant background on the use of Markov Decision Processes in planning.
2.1 Markov Decision Processes
We define here our terminology for Markov decision processes. For a more thorough discussion of
Markov decision processes, see the books by Bertsekas and Tsitsiklis (1996) and Sutton and Barto
(1998). A Markov decision process (MDP) M is a tuple (S, A, R, T, s0 ). Here, S is a finite state
space containing initial state s0 , and A selects a non-empty finite available action set A(s) for each
state s in S. The reward function R assigns a real reward to each state-action-state triple (s, a, s )
where action a is enabled in state s, i.e., a is in A(s). The transition probability function T maps
state-action pairs (s, a) to probability distributions over S, P(S), where a is in A(s).
Given discount factor 0   < 1 and policy  mapping each state s  S to an action in A(s), the
value function V  (s) gives the expected discounted reward obtained from state s selecting action
(s) at each state encountered and discounting future rewards by a factor of  per time step. There

is at least one optimal policy   for which V  (s), abbreviated V  (s), is no less than V  (s) at
every state s, for any policy . The following Q function evaluates an action a with respect to a
future-value function V ,
X
Q(s, a, V ) =
T (s, a, s )[R(s, a, s ) + V (s )].
s S

Recursive Bellman equations use Q() to describe V  and V  as follows. First, V  (s) =
Q(s, (s), V  ). Then, V  (s) = maxaA(s) Q(s, a, V  ). Also using Q(), we can select an ac690

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

tion greedily relative to any value function. The policy Greedy(V ) selects, at any state s, the action
arg maxaA(s) Q(s, a, V ).
Value iteration iterates the operation
U(V )(s) = max

aA(s)

X

T (s, a, s )[R(s, a, s ) + V (s )],

s S

computing the Bellman update U(V ) from V , producing a sequence of value functions converging
in the sup-norm to V  , regardless of the initial V used.
We define the statewise Bellman error B(V, s) for a value function V at a state s to be
U(V )(s)  V (s). We will be inducing new features based on their correlation to the statewise
Bellman error, or based on the sign of the statewise Bellman error. The sup-norm distance of a
value function V from the optimal value function V  can be bounded using the Bellman error magnitude, which is defined as maxsS |B(V, s)| (e.g., see Williams & Baird, 1993). We use the term
statewise Bellman error to emphasize the distinction from the widely used sup-norm Bellman
error.
We note that computing U(V ), and thus statewise Bellman error, can involve a summation over
the entire state space, whereas our fundamental motivations require avoiding such summations.
In many MDP problems of interest, the transition matrix T is sparse in a way that set of states
reachable in one step with non-zero probability is small, for any current state. In such problems,
statewise Bellman error can be computed effectively using an appropriate representation of T . More
generally, when T is not sparse in this manner, the sum can be effectively approximately evaluated
by sampling next states according to the distribution represented by T .
2.2 Modeling Goal-oriented Problems
Stochastic planning problems can be goal-oriented, where the objective of solving the problem is
to guide the agent toward a designated state region (i.e., the goal region). We model such problems
by structuring the reward and transition functions R and T so that any action in a goal state leads
with positive reward to a zero-reward absorbing state, and reward is zero everywhere else. We
retain discounting to represent our preference for shorter paths to the goal. Alternatively, such
problems can be modeled as stochastic shortest path MDPs without discounting (Bertsekas, 1995).
Our techniques can easily be generalized to formalisms which allow varying action costs as well,
but we do not model such variation in this work.
More formally, we define a goal-oriented MDP to be any MDP meeting the following constraints. Here, we use the variables s and s for states in S and a for actions in A(s). We require that
S contain a zero-reward absorbing state , i.e., such that R(, a, s) = 0 and T (, a, ) = 1 for all
s and a. The transition function T must assign either one or zero to triples (s, a, ), and we call the
region of states s for which T (s, a, ) is one the goal region. The reward function is constrained
so that R(s, a, s ) is zero unless s = . In constructing goal-oriented MDPs from other problem
representations, we may introduce dummy actions to carry out the transitions involving  described
here.
2.3 Compactly Represented MDPs
In this work, we consider both propositional and relational state representations.
691

fiW U & G IVAN

In relational MDPs, the spaces S and A(s) for each s are relationally represented, i.e., there
is a finite set of objects O, state predicates P , and action names N used to define these spaces as
follows. A state fact is an application p(o1 , . . . , on ) of an n-argument state predicate p to object
arguments oi , for any n1 . A state is any set of state facts, representing exactly the true facts in that
state. An action instance a(o1 , . . . , oS
n ) is an application of an n-argument action name to n objects
oi , for any n. The action space A = sS A(s) is the set of all action instances.
MDPs with compactly represented state and action spaces also use compact representations
for the transition and reward functions. One such compact representation is the PPDDL planning
language, informally discussed in the next subsection and formally presented in the work of Younes
et al. (2005).
In propositional problems, the action space is explicitly specified and the state space is compactly specified by providing a finite sequence of basic state properties called state attributes, with
Boolean, integer, or real values. A propositional state is then any vector of values for the state
attributes.
Given a relational MDP, an equivalent propositional MDP can be easily constructed by grounding, in which an explicit action space is constructed by forming all action-name applications and a
set of state attributes is computed by forming all state-predicate applications, thus removing the use
of the set of objects in the representation.
2.4 Representing PPDDL Planning Problems using MDPs
We discuss how to represent goal-oriented stochastic planning problems defined in standardized
planning languages such as PPDDL (Younes et al., 2005) as goal-oriented MDPs. We limit our
focus to problems in which the goal regions can be described as (conjunctive) sets of state facts. We
reference and follow the approach used in the work of Fern, Yoon, and Givan (2006) here regarding
converting from planning problems to compactly represented MDPs in a manner that facilitates generalization between problem instances. We first discuss several difficult representational issues and
then finally pull that discussion together in a formal definition of the MDP we analyze to represent
any given PPDDL problem instance. We do not consider quantified and/or disjunctive goals, but
handling such goals would be an interesting and useful extension of this work.
2.4.1 P LANNING D OMAINS

AND

P ROBLEMS

A planning domain is a distribution over problem instances sharing the same state predicates PW ,
action names N , and action definitions. Actions can take objects as parameters, and are defined
by giving discrete finite probability distributions over action outcomes, each of which is specified
using add and delete lists of state facts about the action parameters.
Given a domain definition, each problem instance in the domain specifies a finite object set O,
initial state si and goal condition G. The initial state is given as a set of state facts and the goal
condition is given as a conjunction of state facts, each constructed from the predicates in PW .
1. Each state predicate has an associated arity indicating the number of objects it relates. The state predicate can be
applied to that number of objects from the domain to form a ground state fact that can be either true or false in
each state; states are then the different possible ways to select the true state facts. Likewise, each action name has an
associated arity that is a natural number indicating the number of objects the action will act upon. The action name
can then be applied to that number of objects to form a grounded action.

692

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

2.4.2 PPDDL R EPRESENTATION
PPDDL is the standard planning language for the international probabilistic planning competitions.
In PPDDL, a planning domain syntax and a planning problem syntax is defined. To completely
define a planning instance, one has to specify a domain definition and a problem definition using
the respective syntax. Conditional effects and quantified preconditions are allowed in the domain
definition.
In planning competitions, it has been customary to specify planning domains by providing problem generators that accept size parameters as input and then output PPDDL problem instances.
These generators thus specify size-parameterized planning domains. It is important to note, however, that not all problem generators provided in the recent planning competitions specify planning
domains according to the definition used here. In particular, some problem generators vary the
action set or the state predicates between the instances generated. The relationship between the
different problem instances generated by such generators is much looser than that required by our
definition, and as such these domains are somewhat more like arbitrary collections of planning
problems.
Because our logical language allows generalization between problems only if those problems
share the same state and action language, we limit our empirical evaluation in Section 7 to domains
that were provided with problem generators that specify planning domains as just defined here,
i.e., without varying the action definitions between instances (or for which we can easily code such
a generator). We refer to domains with such generators as planning domains with fixed action
definitions.
2.4.3 G ENERALIZATION B ETWEEN P ROBLEMS OF VARYING S IZE
Because the object set varies in size, without bound, across the problem instances of a domain, there
are infinitely many possible states within the different instances of a single domain. Each MDP we
analyze has a finite state space, and so we model a planning domain as an infinite set of MDPs
for which we are seeking a good policy (in the form of a good value function), one MDP for each
problem instance2 .
A value function for an infinite set of MDPs is a mapping from the disjoint union of the state
spaces of the MDPs to the real numbers. Such a value function can be used greedily as a policy
in any of the MDPs in the set. However, explicit representation of such a value function would
have infinite size. Here, we will use knowledge representation techniques to compactly represent
value functions over the infinite set of problem instance MDPs for any given planning domain. The
compact representation derives from generalization across the domains, and our approach is fundamentally about finding good generalizations between the MDPs within a single planning domain.
Our representation for value functions over planning domains is given below in Sections 2.5 and 4.
In this section, we discuss how to represent as a single finite MDP any single planning problem
instance. However, we note that our objective in this work is to find good value functions for
the infinite collections of such MDPs that represent planning domains. Throughout this paper, we
assume that each planning domain is provided along with a means for sampling example problems
from the domain, and that the sampling is parameterized by difficulty (generally, problem size) so
2. In this paper we consider two candidate representations for features; only one of these, the relational representation,
is capable of generalizing between problem sizes. For the propositional representation, we restrict all training and
testing to problem instances of the same size.

693

fiW U & G IVAN

that easy example problems can be selected. Although, PPDDL does not provide any such problem
distributions, benchmark planning domains are often provided with problem generators defining
such distributions: where such generators are available, we use them, and otherwise we code our
own distributions over problem instances.
2.4.4 G ENERALIZING B ETWEEN P ROBLEMS

WITH

VARYING G OALS

To facilitate generalization between problem instances with different goals, and following the work
of Martin and Geffner (2004) and Fern et al. (2006), we translate a PPDDL instance description into
an MDP where each state specifies not only what is true in the state but also what the goal is. Action
transitions in this MDP will never change the goal, but the presence of that goal within the state
description allows value functions (that are defined as conditioning only on the state) to depend on
the goal as well. The goal region of the MDP will simply be those MDP states where the specified
current state information matches the specified goal information.
Formally, in translating PPDDL problem instances into compact MDPs, we enrich the given set
of world-state predicates PW by adding a copy of each predicate indicating the desired state of that
predicate. We name the goal-description copy of a predicate p by prepending the word goal- to
the name. The set of all goal-description copies of the predicates in PW is denoted PG , and we take
PW  PG to be the state predicates for the MDP corresponding to the planning instance. Intuitively,
the presence of goal-p(a,b) in a state indicates that the goal condition requires the fact p(a, b) to be
part of the world state. The only use of the goal predicates in constructing a compact MDP from a
PPDDL description is in constructing the initial state, which will have the goal conditions true for
the goal predicates.
We use the domain Blocksworld as an example here to illustrate the reformulation (the same
domain is also used as an example in Fern et al., 2006). The goal condition in a Blocksworld
problem can be described as a conjunction of ground on-top-of facts. The world-state predicate
on-top-of is in PW . As discussed above, this implies that the predicate goal-on-top-of is in PG .
Intuitively, one ground instance of that predicate, goal-on-top-of(b1,b2), means that for a state in
the goal region, the block b1 has to be directly on the top of the block b2.
2.4.5 S TATES

WITH NO

AVAILABLE ACTIONS

PPDDL allows the definition of domains where some states do not meet the preconditions for any
action to be applied. However, our MDP formalism requires at least one available action in every
state. In translating a PPDDL problem instance to an MDP we define the action transitions so
that any action taken in such a dead state transitions deterministically to the absorbing  state.
Because we consider such states undesirable in plan trajectories, we give these added transitions a
reward of negative one unless the source state is a goal state.
2.4.6 T HE R ESULTING MDP
We now pull together the above elements to formally describe an MDP M = (S, A, R, T, s0 )
given a PPDDL planning problem instance. As discussed in Section 2.3, the set S is defined by
specifying the predicates and objects available. The PPDDL description specifies the sets N of
action names and O of objects, as well as a set PW of world predicates. We construct the enriched
set P = PW  PG of state predicates and define the state space as all sets of applications of these
predicates to the objects in O. The set A(s) for any state s is the set of PPDDL action instances built
694

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

from N and O for which s satisfies the preconditions, except that if this set is empty, A(s) is the set
of all PPDDL action instances built from N and O. In the latter case, we say the state is dead. The
reward function R is defined as discussed previously in Section 2.2; i.e., R(s, a, s ) = 1 when the
goal condition G is true in s, R(s, a, s ) = 1 when s is a non-goal dead state, and zero otherwise.
We define T (s, a, s ) according to the semantics of PPDDL augmented with the semantics of 
from Section 2.2T (s, a, ) will be one if s satisfies G, s is dead, or s = , and zero otherwise.3
Transiting from one state to another never changes the goal condition description in the states given
by predicates in PG . The MDP initial state s0 is just the PPDDL problem initial state si augmented
by the goal condition G using the goal predicates from PG . If a propositional representation is
desired, it can be easily constructed directly from this relational representation by grounding.
2.5 Linear Approximation of Value Functions
As many previous authors have done (Patrascu et al., 2002; Sanner & Boutilier, 2009; Bertsekas &
Tsitsiklis, 1996; Tesauro, 1995; Tsitsiklis & Roy, 1997), we address very large compactly represented S and/or A by implicitly representing value functions in terms of state-space features
f : S  R. Our features f must select a real value for each state. We describe two approaches to
representing and selecting such features in Section 4.
Recall from Section 1 that our goal is to learn a value function for a family of related MDP
problems. We assume that our state-space features are defined across the union of the state spaces
in the family.
We represent
value functions using a linear combination of l features extracted from s, i.e., as
P
V (s) = li=0 wi fi (s), where f0 (s) = 1. Our goal is to find features fi (each mapping states to real
values) and weights wi so that V closely approximates V  . Note that a single set of features and
weight vector defines a value function for all MDPs in which those features are defined.
Various methods have been proposed to select weights wi for linear approximations (see, e.g.,
Sutton, 1988 or Widrow & Hoff, 1960). Here, we review and use a trajectory-based approximate
value iteration (AVI) approach. Other training methods can easily be substituted. AVI constructs a
1
2
T
finite sequence of value functions
one. Each value function
Pl V  , V , . . . , V , and returns the last

is represented as V (s) = i=0 wi fi (s). To determine weights wi+1 from V  , we draw a set
of training states s1 , s2 , . . . , sn by following policy Greedy(V  ) in different example problems
sampled from the provided problem distribution at the current level of problem difficulty. (See
Section 3 for discussion of the control of problem difficulty.) The number of trajectories drawn and
the maximum length of each trajectory are parameters of this AVI method. For each training state s,
we compute the Bellman update U(V  )(s) from the MDP model of the problem instance. We can
then compute wi+1 from the training states using
wi+1 = wi +

1 X
fi (sj )(U(V  )(sj )  V  (sj )),
ni

(1)

j

where  is the learning rate and ni is the number of states s in s1 , s2 , . . . , sn for which fi (s) is
non-zero. Weight updates using this weight-update formula descend the gradient of the L2 distance
between V  and U(V  ) on the training states, with the features first rescaled to normalize the
3. Note that according to our definitions in Section 2.2, the dead states are now technically goal states, but have
negative rewards.

695

fiW U & G IVAN

effective learning rate to correct for feature values with rare occurrence in the training set.4 Pseudocode for our AVI method and for drawing training sets by following a policy is available in Online
Appendix 1 (available on JAIR website), on page 2.
Here, we use the greedy policy to draw training examples in order to focus improvement on the
most relevant states. Other state distributions can be generated that are not biased by the current
policy; in particular, another option worth considering, especially if feature learning is stuck, would
be the long random walk distribution discussed in the work of Fern, Yoon, and Givan (2004). We
leave detailed exploration of this issue for future work. For a more substantial discussion of the
issues that arise in selecting the training distribution, please see the book by Sutton and Barto (1998).
It is worth noting that on-policy training has been shown to converge to the optimal value function
in the closely related reinforcement learning setting using the SARSA algorithm (Singh, Jaakkola,
Littman, & Szepesvari, 2000).
In general, while AVI often gives excellent practical results, it is a greedy gradient-descent
method in an environment that is not convex due to the maximization operation in the Bellman error
function. As such, there is no guarantee on the quality of the weight vector found, even in the case
of convergence. Convergence itself is not guaranteed, and, in our experiments, divergent weight
training was in fact a problem that required handling. We note that our feature-discovery methods
can be used with other weight-selection algorithms such as approximate linear programming, should
the properties of AVI be undesirable for some application.
We have implemented small modifications to the basic weight update rule in order to use AVI
effectively in our setting; these are described in Section 5 in Online Appendix 1 (available on JAIR
website).

3. Feature-Discovering Value-function Construction
In planning, once state features are provided, domain-independent algorithms such as AVI can learn
weighted combinations of those features that often perform well as heuristic estimates of state value
(e.g., distance to the goal). We now describe methods to select and add features that describe
state-space regions of high inconsistency in the Bellman equation (statewise Bellman error) during
approximate value iteration. Our methods can be applied using any real-valued-feature hypothesis
space with a corresponding learning method for selecting features to match a real-valued function
on a training set of states. Here, we will use the learner to select features that match the statewise
Bellman error function.
As noted above, we use a boosting style learning approach in finding value functions, iterating
between selecting weights and generating new features by focusing on the Bellman error in the
current value function. Our value function representation can be viewed as a weighted ensemble of
single-feature hypotheses. We start with a value function that has only a trivial feature, a constant
feature always returning the value one, with initial weight zero. We iteratively both retrain the
weights and select new features matching regions of states for which the current weighted ensemble
has high statewise Bellman error.
We take a learning from small problems approach and learn features first in problems with
relatively lower difficulty, and increase problem difficulty over time, as discussed below. Lower
difficulty problems are typically those with smaller state spaces and/or shorter paths to positive
4. In deriving this gradient-descent weight-update formula, each feature fi is scaled by ri =

696

q

n
,
ni

giving fi = ri fi .

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

r
Initial feature vector 
r
Initial weight vector w
Initial problem difficulty D

Difficulty at
target level or out
of time?

Yes

r
Final r
and w

No

Increase problem
difficulty
r D. Keep
and  .

Learn new feature
correlating to the Bellman
error for states in the
training
r set, and add it
to  . Keep the current
problem difficulty D.

r
w

r

Select w approximately
minimizing
error
r Bellman
r
of V = w  

Done

Reweighted value
r r
function V = w 

Yes

Performance at
current difficulty
meets threshold?

No

Generate feature
training set

Figure 1: Control flow for feature learning. Boxes with double borders represent assumed subroutines for our method. We assume that the problem distribution is parameterized by
problem difficulty (such as problem size).

feedback (e.g. goal states). Learning initially in more difficult problems will typically lead to
inability to find positive feedback and random-walk behavior; as a result learning first in lower
difficulty problems has been found more effective (Martin & Geffner, 2004; Yoon, Fern, & Givan,
2002). We show experimentally in Section 7 that good value functions for high difficulty problems
can indeed be learned in this fashion from problems of lower, increasing difficulties.
Our approach relies on two assumed subroutines, and can be instantiated in different ways by
providing different algorithms for these subroutines. First, a method of weight selection is assumed;
this method takes as input a problem domain and a fixed set of features, and selects a weight vector
for a value function for the problem domain using the provided features. We intend this method
to heuristically or approximately minimize L Bellman error in its choice of weight vector, but
in practice it may be easier to adjust weights to approximate L2 Bellman error. Second, a feature
hypothesis space and corresponding learner are assumed to be provided by the system designer.
The control flow for our approach is shown in Figure 1. Each iteration at a fixed problem
distribution selects weights for the current feature set (using any method attempting to minimize
L Bellman error) to define a new value function V , selects a training set of states for feature
learning, then learns a new feature correlating well to the statewise Bellman error of V , adding
that feature to the feature set. A user-provided performance-threshold function  detects when to
increase the problem difficulty. A formalization of this control flow is given in Figure 2, in the form
of pseudo-code.
697

fiW U & G IVAN

Feature-discovering Value-function Construction



Inputs:
Initial feature vector  0 , initial weight vector 
w 0,
Sequence of problem distributions D1 , D2 ,    , Dmax of increasing difficulty,
Performance threshold function  .
// (D, V ) tests the performance of value function V in distribution D.



Outputs:
Feature vector  , weight vector 
w



 

   0, 
w 
w 0, d  1

1.

while not (d > max or out of time)




Select 
w approximately minimizing Bellman error of V = 
w   over Dd



if  (Dd , 
w  )

2.
3.
4.
5.

then d  d + 1

6.

else
Generate a sequence of training states T using Dd

7.
8.
9.
10.




Learn new feature f correlating to the Bellman error feature B(
w   , )
for the states in T






  (  ; f ), 
w  (
w ; 0)

 
return  , 
w

Notes:
1. B(, ) is the statewise-Bellman error function, as defined in Section 2.1.
2. The code for approximate value iteration AVI, shown in Online Appendix 1 (available on JAIR website) on
page 2, is an example implementation of line 3.



3. The code for draw(Greedy(
w   ), N
), shown in Online Appendix 1 on page 2, is an example impletraining

mentation of line 7. Ntraining is the number of states in the feature training set. Duplicated states are removed
as specified in Section 3.1.



4. The beam-search code for learning relational features beam-search-learn(score(, T, B(
w   , ))) is an
example implementation of line 8, where beam-search-learn is shown in figure 3 in Section 3, and score is
defined in Section 4.2.

Figure 2: Pseudo-code for learning a set of features.
For the experiments reported in Section 7, we evaluate the following choices for the assumed
subroutines. For all experiments we use AVI to select weights for feature sets. We evaluate two
choices for the feature hypothesis space and corresponding learner, one relational and one propositional, as described in Section 4.
Separate training sets are drawn for weight selection and for the feature learning; the former
will depend on the weight selection method, and is described for AVI in Section 2.5, and the latter
is described in this section.
Problem difficulty is increased when sampled performance of the greedy policy at the current
difficulty exceeds user-specified performance thresholds. In our planning-domain experiments, the
698

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

performance parameters measured are success ratio (percentage of trials that find the goal) and average successful plan length (the average number of steps to the goal among all successful trials).
The non-goal-oriented domains of Tetris and SysAdmin use different performance measures: average total reward for Tetris and Bellman error for SysAdmin (to facilitate comparison with Patrascu
et al., 2002).
We also assume a user-provided schedule for problem difficulty increases in problems where
difficulty is parameterized by more than one parameter (e.g., size may be measured in by the number
of objects of each type); further domain-independent automation of the increase in difficulty is a
topic for future research. We give the difficulty-increase schedules and performance thresholds for
our experiments in the section presenting the experiments, Section 7.
3.1 Training Set Generation
The training set for selection of a new feature is a set of states. The training set is constructed by
repeatedly sampling an example problem instance from the problem distribution at the current level
of difficulty, and applying the current greedy policy Greedy(V ) to that problem instance to create
a trajectory of states encountered. Every state (removing duplicates) encountered is added to the
training set. The size of the feature-selection training set and the maximum length of each training
trajectory are specified by the user as parameters of the algorithm.
Retaining duplicate states in the training set is another option that can be considered. Our preliminary empirical results have not favored this option, but it is certainly worth further exploration.
We note that the goal of finding a near-optimal value function does not necessarily make reference
to a state distribution: the most widely used notion of near-optimal in the theory of MDPs is
the sup-norm distance to V  . Moreover, the state distribution represented by the duplicates in our
training sets is typically the distribution under a badly flawed policy; heeding this distribution can
prevent correcting Bellman error in critical states that are visited by this policy, but visited only
rarely. (These states may be, for instance, rarely visited good exits from the visited state region
that are being misunderstood by the current value function.) At this point, our primary justification
for removing duplicates is the empirical performance we have demonstrated in Section 7.
Similar reasoning would suggest removing duplicate states in the training sets for AVI weight
training, described in Section 2.5. Because there are many large AVI training sets generated in our
experiments, duplicate removal must be carefully handled to control runtime; for historical reasons,
our experiments shown here do not include duplicate removal for AVI.
A possible problem occurs when the current greedy policy cannot reach enough states to complete the desired training set. If 200 consecutive trajectories are drawn without visiting a new state
before the desired training set size is reached, the process is modified as follows. At that point,
the method attempts to complete the training set by drawing trajectories using random walk (again
using sampled example problems from the current problem distribution). If this process again leads
to 200 consecutive trajectories without a new state, the method terminates training-set generation
and uses the current training set even though it is smaller than the target size.
3.2 Applicability of the Method
Feature-discovering value-function construction as just described does not require complete access
to the underlying MDP model. Our AVI updates and training set generation are both based on the
following computations on the model:
699

fiW U & G IVAN

1. Given a state s the ability to compute the action set A(s).
2. Given a state s, action a  A(s), and value function V , the ability to compute the Q-value
Q(s, a, V ).
3. Given a state s and action a  A(s), the ability to draw a state from the next state distribution
defined by T (s, a, s ).
4. Given a state s, the ability to compute the features in the selected feature language on s and
any computations on the state required for the selected feature learner. As examples,
(a) in Section 4, we introduce a relational feature language and learner that require knowledge of a set of domain predicates (and their arities) such that each state is a conjunctive
set of predicate facts (see Section 2.3),
(b) and, also in Section 4, we describe a propositional feature language and learner that
require knowledge of a set of propositional state attributes such that each state is a truth
assignment to the attributes.
The first three items enable the computation of the Bellman update of s and the last item enables
computation of the estimated value function given the weights and features defining it as well as the
selection of new features by the feature learner. These requirements amount to substantial access to
the problem model; as a result our method must be considered a model-based technique.
A consequence of these requirements is that our algorithm cannot be directly applied to the
standard reinforcement learning setting where the only model access is via acting in the world
without the ability to reset to selected states; in this setting Bellman error computations for particular
states cannot necessarily be carried out. It would be possible to construct a noisy Bellman error
training set in such a model-free setting and it would be appropriate future work to explore the use
of such a training set in feature learning.
While the PPDDL planning domains studied provide all the information needed to perform these
computations, our method also applies to domains that are not natural to represent in PPDDL. These
can be analyzed by our method once the above computations can be implemented. For instance, in
our Tetris experiments in Section 7.2, the underlying model is represented by providing hand-coded
routines for the above computations within the domain.
3.3 Analysis
MDP value iteration is guaranteed to converge to the optimal value function if conducted with
a tabular value-function representation in the presence of discounting (Bertsekas, 1995). Although
weight selection in AVI is designed to mimic value iteration, while avoiding a tabular representation,
there is no general guarantee that the weight updates will track value iteration and thus converge
to the optimal value function. In particular, there may be no weighted combination of features that
represents the optimal value function, and likewise none that represents the Bellman update U(V )
for some value function V produced by AVI weight training process. Our learning system introduces
new features to the existing feature ensemble in response to this problem: the training set used to
select the new feature pairs states with their statewise Bellman error. If the learned feature exactly
captures the statewise Bellman-error concept (by exactly capturing the training set and generalizing
700

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

successfully) then the new feature space will contain the Bellman update of the value function used
to generate the training data.
We aim to find features that approximate the Bellman error feature, which we take to be a
function mapping states to their statewise Bellman error. Theoretical properties of Bellman error
features in the uncontrolled Markov processes (i.e., without the max operator in the Bellman equation) have recently been discussed in the work of Parr et al. (2007), where the addition of such
features (or close approximations thereof) is proven to reduce the weighted L2 -norm distance between the best weight setting and the the true (uncontrolled) value V  , when linear fixed-point
methods are used to train the weights before feature addition. Prior to that work (in Wu & Givan,
2005), and now in parallel to it, we have been empirically exploring the effects of selecting Bellman
error features in the more complex controlled case, leading to the results reported here.
It is clear that if we were to simply add the Bellman error feature directly, and set the corresponding weight to one, the resulting value function would be the desired Bellman update U(V )
of the current value function V . Adding such features at each iteration would thus give us a way
to conduct value iteration exactly, without enumerating states. But each such added feature would
describe the Bellman error of a value function defined in terms of previously added features, posing
a serious computational cost issue when evaluating the added features. In particular, each Bellman
error feature for a value function V can be estimated at any particular state with high confidence by
evaluating the value function V at that state and at a polynomial-sized sample of next states for each
action (based on Chernoff bounds).
However, if the value function V is based upon a previously added Bellman-error feature, then
each evaluation of V requires further sampling (again, for each possible action) to compute. In this
manner, the amount of sampling needed for high confidence grows exponentially with the number of
successive added features of this type. The levels of sampling do not collapse into one expectation
because of intervening choices between actions, as is often the case in decision-theoretic sampling.
Our feature selection method is an attempt to tractably approximate this exact value iteration method
by learning concise and efficiently computable descriptions of the Bellman-error feature at each
iteration.
Our method can thus be viewed as a heuristic approximation to exact value iteration. Exact
value iteration is the instance of our method obtained by using an explicit state-value table as the
feature representation and generating training sets for feature learning containing all states  to
obtain exact value iteration we would also omit AVI training but instead set each weight to one.
When the feature language and learner can be shown to approximate explicit features tightly
enough (so that the resulting approximate Bellman update is a contraction in the L norm), then it is
easy to prove that tightening approximations of V  will result if all weights are set to one. However,
for the more practical results in our experiments, we use feature representations and learners for
which no such approximation bound relative to explicit features is known.

4. Two Candidate Hypothesis Spaces for Features
In this section we describe two hypothesis spaces for features, a relational feature space and a
propositional feature space, along with their respective feature learning methods. For each of the
two feature spaces, we assume the learner is provided with a training set of states paired with their
statewise Bellman error values.
701

fiW U & G IVAN

Note that these two feature-space-learner pairs lead to two instances of our general method and
that others can easily be defined by defining new feature spaces and corresponding learners. In this
paper we empirically evaluate the two instances presented here.
4.1 Relational Features
A relational MDP is defined in terms of a set of state predicates. These state predicates are the basic
elements from which we define a feature-representation language. Below, we define a generalpurpose means of enriching the basic set of state predicates. The resulting enriched predicates
can be used as the predicate symbols in standard first-order predicate logic. We then consider any
formula in that logic with one free variable as a feature, as follows5 .
A state in a relational MDP is a first-order interpretation. A first-order formula with one free
variable is then a function from such states to natural numbers which maps each state to the number
of objects in that state that satisfy the formula. We take such first-order formulas to be real-valued
features by normalizing to a real number between zero and onethis normalization is done by
dividing the feature value by the maximum value that the feature can take, which is typically the
total number of objects in the domain, but can be smaller than this in domains where objects (and
quantifiers) are typed. A similar feature representation is used in the work of Fawcett (1996).
This feature representation is used for our relational experiments, but the learner we describe
in the next subsection only considers existentially quantified conjunctions of literals (with one free
variable) as features. The space of such formulas is thus the effective feature space for our relational
experiments.
Example 4.1: Take Blocksworld with the table as an object for example, on(x, y) is
a predicate in the domain that asserts the block x is on top of the object y, where y
may be a block or the table. A possible feature for this domain can be described as y
on(x, y), which is a first-order formula with x as the one free variable. This formula
means that there is some other object immediately below the block object x, which
essentially excludes the table object and the block being held by the arm (if any) from
the object set described by the feature. For n blocks problems, the un-normalized value
of this feature is n for states with no block being held by the arm, or n  1 for states
with a block being held by the arm.
4.1.1 T HE E NRICHED P REDICATE S ET
More interesting examples are possible with the enriched predicate set that we now define. To enrich
the set of state predicates P , we add for each binary predicate p a transitive closure form of that
predicate p+ and predicates min-p and max-p identifying minimal and maximal elements under
that predicate. In goal-based domains, recall that our problem representation (from Section 2.4)
includes, for each predicate p, a goal version of the predicate called goal-p to represent the desired
state of the predicate p in the goal. Here, we also add a means-ends analysis predicate correct-p to
represent p facts that are present in both the current state and the goal.
So, for objects x and y, correct-p(x,y) is true if and only if both p(x, y) and goal-p(x,y) are
true. p+(x, y) is true of objects x and y connected by a path in the binary relation p. The relation
max-p(x) is true if object x is a maximal element with respect to p, i.e., there exists no other object
5. Generalizations to allow multiple free variables are straightforward but of unclear utility at this time.

702

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

y such that p(x, y) is true. The relation min-p(x) is true if object x is a minimal element with respect
to p, i.e., there exists no other object y such that p(y, x) is true.
We formally define the feature grammar in Online Appendix 1 (available on JAIR website) on
page 3.
Example 4.1 (cont.): The feature y correct-on(x, y) means that x is stacked on top of
some object y both in the current state and in the goal state. The feature y on+(x, y)
means that in the current state, x is directly above some object y, i.e., there is a sequence
of on relations traversing a path between x and y, inclusively. The feature max-on+(x)
means that x is the table object when all block-towers are placed on the table, since the
table is the only object that is not on any other object. The feature min-on+(x) means
that there is no other object on top of x, i.e., x is clear.
4.2 Learning Relational Features
We select first-order formulas as candidate features using a beam search with a beam width W . We
present the pseudo-code for beam search in Figure 3. The search starts with basic features derived
automatically from the domain description and repeatedly derives new candidate features from the
best scoring W features found so far, adding the new features as candidates and keeping only the
best scoring W features at all times. After new candidates have been added a fixed depth d times,
the best scoring feature found overall is selected to be added to the value-function representation.
Candidate features are scored for the beam search by their correlation to the Bellman error feature
as formalized below.
Specifically, we score each candidate feature f with its correlation coefficient to the Bellman
error feature B(V, ) as estimated by a training set. The correlation coefficient between functions

 (s)}
 and  is defined as corr-coef(,  ) = E{(s) (s)}E{(s)}E{
. Instead of using a known
   
distribution to compute this value, we use the states in the training set s and compute a sampled
version by using the following equations to approximate the true expectation E and the true standard
deviation  of any random variable X:
1 X
X(s ),
Es {X(s)} =
|s | 
s s

X,s =

corr-coef-sampled(,  , s ) =

s

1 X
(X(s )  E{X(s)})2 ,
|s | 
s s

Es {(s) (s)}  Es {(s)}Es { (s)}
.
,s  ,s

The scoring function for feature selection is then a regularized version of the correlation coefficient
between the feature and the target function 
score(f, s , ) = |corr-coef-sampled(f, , s )|(1  depth(f )),
where the depth of a feature is the depth in the beam search at which it first occurs, and  is a
parameter of the learner representing the degree of regularization (bias towards low-depth features).
703

fiW U & G IVAN

beam-search-learn
Inputs:

Feature scoring function fscore : features  [0, 1]

Outputs:

New feature f

System parameters:

W : Beam width
maxd : Max number of beam-search iterations
: Degree of regularization, as defined in Section 4.2

1.
2.
3.
4.
5.
6.
7.
8.

I  the set of basic features, as defined in Section 4.2.
d  1, F  I.
repeat
Set beam B to the highest scoring W candidates in F .
Candidate feature set F  B.
for each candidate f1  B
for each candidate f2  (B  I), f2 6= f1
F = F  combine(f1 , f2 ).

9.
10.
11.

d  d + 1.
until (d > maxd ) or (highest score so far  (1  d)).
return the maximum scoring feature f  F .

Notes:
1. Feature scoring function fscore(f ) is used to rank candidates in lines 4 and 11. A discussion of a sample
scoring function, used in our relational experiments, is given in Section 4.2.
2. Candidate scores can be cached after calls to fscore, so that no candidate is scored twice.
3. The value (1  d) is the largest score a feature of depth d can have.

Figure 3: Pseudo-code for beam search.

The value score(f, s , B(V, )) is then the score of how well a feature f correlates to the Bellman error feature. Note that our features are non-negative, but can still be well correlated to the
Bellman error (which can be negative), and that the presence of a constant feature in our representation allows a non-negative feature to be shifted automatically as needed.
It remains only to specify which features in the hypothesis space will be considered initial, or
basic, features for the beam search, and to specify a means for constructing more complex features
from simpler ones for use in extending the beam search. We first take the state predicate set P in
a domain and enrich P as described in Section 4.1. After this enrichment of P , we take as basic
features the existentially quantified applications of (possibly negated) state predicates to variables
with zero or one free variable6 . A grammar for basic features is defined as follows.
6. If the domain distinguishes any objects by naming them with constants, we allow these constants as arguments to the
predicates here as well.

704

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Definition: A basic feature is an existentially quantified hliterali expression with at
most one free variable (see Figure 3 in Online Appendix 1, available on JAIR website,
on page 3).
A feature with no free variables is treated technically as a one-free-variable feature where that
variable is not used; this results in a binary feature value that is either zero or the total number of
objects, because instantiating the free variable different ways always results in the same truth value.
We assume throughout that every existential quantifier is automatically renamed away from every
other variable in the system. We can also take as basic features any human-provided features that
may be available, but we do not add such features in our experiments in this paper in order to clearly
evaluate our methods ability to discover domain structure on its own.
At each stage in the beam search we add new candidate features (retaining the W best scoring
features from the previous stage). The new candidate features are created as follows. Any feature in
the beam is combined conjunctively with any other, or with any basic feature. The method of combination of two features is described in Figure 4. This figure shows non-deterministic pseudo-code
for combining two input features, such that any way of making the non-deterministic choices results
in a new candidate feature. The pseudo-code refers to the feature formulas f1 and f2 describing the
two features. In some places, these formulas and others are written with their free variable exposed,
as f1 (x) and f2 (y). Also substitution for that variable is notated by replacing it in the notation, as
in f1 (z).
The combination is by conjoining the feature formulas, as shown in line 2 of Figure 4; however,
there is additional complexity resulting from combining the two free variables and possibly equating
bound variables between the two features. The two free variables are either equated (by substitution) or one is existentially quantified before the combination is done, in line 1. Up to two pairs
of variables, chosen one from each contributing feature, may also be equated, with the resulting
quantifier at the front, as described in line 3. Every such combination feature is a candidate.
This beam-search construction can lead to logically redundant features that are in some cases
syntactically redundant as well. We avoid syntactically redundant features at the end of the beam
search by selecting the highest scoring feature that is not already in the feature set. Logical redundancy that is not syntactic redundancy is more difficult to detect. We avoid some such redundancy
automatically by using ordering during the beam search to reduce the generation of symmetric expressions such as    and   . However, testing logical equivalence between features in our
language is NP-hard (Chandra & Merlin, 1977), so we do not deploy a complete equivalence test
here.
Example 4.2: Assume we have two basic features z p(x, z) and w q(y, w). The set
of the possible candidates that can be generated by combining these two features are:
When line 3 in Figure 4 runs zero times,
1. (x z p(x, z))  (w q(y, w)), from xf1 (x)  f2 (y)
2. (z p(x, z))  (y w q(y, w)), from f1 (x)  yf2 (y), and
3. (z p(x, z))  (w q(x, w)), from f1 (x)  f2 (x)
and when line 3 runs one time,
4. u ((z p(u, z))  (q(y, u))), from equating x and w in item 1 above,
5. u (x p(x, u))  (q(y, u)), from equating x and z in item 1 above,
705

fiW U & G IVAN

combine
Inputs:

Features f1 (x), f2 (y)

Outputs:

Set of features {o1 }

return the set of all features o1 that can result from:
1.

Perform one of
a. f1 = (x)f1 (x)
b. f2 = (y)f2 (y)
c. f2 = f2 (x)

2.

o1 = f1  f2

3.

Perform the following variable equating step zero, one, or two times:
a. Let v be a variable occurring in f1 and o1 .
Let e1 be the expression of the form (v)1 (v) that occurs in o1
b. Let w be a variable occurring in f2 and o1 .
Let e2 be the expression of the form (w)2 (w) that occurs in o1
c. Let u be a new variable, not used in o1
d. o2 = replace e1 with 1 (u) and replace e2 with 2 (u) in o1
e. o1 = (u)o2

Notes:
1. The choice between 1a, 1b, and 1c, the choice of number of iterations of step 3, and the choices of e1 and e2
in steps 3a and 3b are all non-deterministic choices.
2. Any feature that can be produced by any run of this non-deterministic algorithm is included in the set of
features that is returned by combine.
3. It is assumed that f1 and f2 have no variables in common, by renaming if necessary before this operation.

Figure 4: A non-deterministic algorithm for combining two feature formulas.

6. u (p(x, u)  (w q(u, w))), from equating z and y in item 2 above,
7. u (p(x, u)  (y q(y, u))), from equating z and w in item 2 above, and
8. u (p(x, u)  ( q(x, u))), from equating z and w in item 3 above.
The first three are computed using cases 1a, 1b, and 1c, respectively. The remaining
five derive from the first three by equating bound variables from f1 and f2 .
Features generated at a depth k in this language can easily require enumerating all k-tuples
of domain objects. Since the cost of this evaluation grows exponentially with k, we bound the
706

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

maximum number of quantifiers in scope at any point in any feature formula to q, and refuse to
consider any feature violating this bound.
The values W , , d, and q are the parameters controlling the relational learner we evaluate in
this paper. How we set these parameters is discussed further in the experimental setup description
in Section 6.
We provide a brief discussion on the motivations for our feature combination method. First, we
note that additive combination of features can represent disjunctions of features7 ; hence, we only
consider conjunction during feature combination. Here, we have chosen to conjoin features in
multiple ways, varying the handling/combining of the free and bound variables. We do not believe
our choice to be uniquely effective, but provide it as an example realization of the proposed featurediscovery architecture.
Any choice of feature representation and combination method must trade off between the cost
of evaluation of more choices and the potential gain in quality of the selected features. Here, we
have chosen to limit individual features to conjunction; effectively, we have limited the features to
Horn clauses over the predicates and their negations, with univariate heads.
4.3 Propositional Features
Here we discuss a second candidate hypothesis space for features, using a propositional representation. We use decision trees to represent these propositional features. A detailed discussion of
classification using decision trees can be found in the book by Mitchell (1997). A decision tree is
a binary tree with internal nodes labeled by binary tests on states, edges labeled yes and no
representing results of the binary tests, and leaves labeled with classes (in our case, either zero or
one). A path through the tree from the root to a leaf with label l identifies a labeling of some set of
stateseach state consistent with the state-test results on the path is viewed as labeled l by the tree.
In this way, a decision tree with real number labels at the leaves is viewed as labeling all states with
real numbers, and is thus a feature.
We learn decision trees from training sets of labeled states using the well known C4.5 algorithm
(Quinlan, 1993). This algorithm induces a tree greedily matching the training data from the root
down. We use C4.5 to induce new featuresthe key to our algorithm is how we construct suitable
training sets for C4.5 so that the induced features are useful in reducing Bellman error.
We include as possible state tests for the decision trees we induce every grounded predicate
application8 from the state predicates, as well as every previously selected decision-tree feature
(each of which is a binary test because all leaf labels are zero or one).
4.4 Learning Propositional Features
To construct binary features, we use only the sign of the Bellman error feature, not the magnitude. The sign of the statewise Bellman error at each state serves as an indication of whether the
state is undervalued or overvalued by the current approximation, at least with respect to exactly
representing the Bellman update of the current value function. If we can identify a collection of
undervalued states as a new feature, then assigning an appropriate positive weight to that feature
7. Representing the disjunction of overlapping features using additive combination can be done with a third feature
representing the conjunction, using inclusion/exclusion and a negative weight on the conjunction.
8. A grounded predicate application is a predicate applied to the appropriate number of objects from the problem instance.

707

fiW U & G IVAN

will increase their value. Similarly, identifying overvalued states with a new feature and assigning
a negative weight will decrease their value. We note that the domains of interest are generally too
large for state-space enumeration, so we will need classification learning to generalize the notions
of overvalued and undervalued across the state space from training sets of sample states.
To enable our method to ignore states that are approximately converged, we discard states with
statewise Bellman error near zero from either training set. Specifically, among the states with negative statewise Bellman error, we discard any state with such error closer to zero than the median
within that set; we do the same among the states with positive statewise Bellman error. More sophisticated methods for discarding training data near the intended boundary can be considered in
future research; these will often introduce additional parameters to the method. Here, we seek an
initial and simple evaluation of our overall approach. After this discarding, we define + to be
the set of all remaining training pairs with states having positive statewise Bellman error, and 
likewise those with negative statewise Bellman error.
We then use + as the positive examples and  as the negative examples for a supervised
classification algorithm; in our case, C4.5 is used. The hypothesis space for classification the space
of decision trees built with tests selected from the primitive attributes defining the state space and
goal; in our case, we also use previously learned features that are decision trees over these attributes.
The concept resulting from supervised learning is then treated as a new feature for our linear approximation architecture, with an initial weight of zero.
Our intent, ideally, is to develop an approximately optimal value function. Such a value function
can be expected to have Bellman error at many states, if not every state; however, low state-wise
error in some states does not contribute to high sup-norm Bellman error. Our discarding training
states with low statewise Bellman error reflects our tolerance of such low error below some threshold
representing the degree of approximation sought. Note that the technical motivation for selecting
features based upon Bellman error focuses on reducing the sup-norm Bellman error; given this
motivation, we are not as interested in finding the exact boundary between positive and negative
Bellman error as we are in identifying which states have large magnitude Bellman error (so that that
large-magnitude error can be addressed by feature addition).
We observe that there is limited need to separately learn a feature matching  due to the
following representability argument. Consider a binary feature F and its complement F , so that
exactly one of F and F is true in each state. Given the presence of a constant feature in the feature
set, adding F or F to the feature set yields the same set of representable value functions (assigning
weight w to F has the same effect as assigning weight w to F and adding w to the weight of the
constant feature).
4.5 Discussion
We discuss below the generalization capability, learning time, and heuristic elements of our feature
learning method.
4.5.1 G ENERALIZATION ACROSS VARYING D OMAIN S IZES
The propositional feature space described above varies in size as the number of objects in a relational
domain is varied. As a result, features learned at one domain size are not generally meaningful (or
even necessarily defined) at other domain sizes. The relational approach above is, in contrast, able
to generalize naturally between different domains sizes. Our experiments report on the ability of
708

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

the propositional technique to learn within each domain size directly, but do not attempt to use that
approach for learning from small problems to gain performance in large problems. This is a major
limitation in producing good results for large domains.
4.5.2 L EARNING T IME
The primary motivation for giving up generalization over domain sizes in order to employ a propositional approach is that the resulting learner can use highly efficient, off-the-shelf classification
algorithms. The learning times reported in Section 7 show that our propositional learner learns new
features orders of magnitude faster than the relational learner.
4.5.3 H EURISTIC E LEMENTS

OF THE

M ETHOD

As mentioned earlier, our algorithm heuristically approximates the repeated addition of Bellman
error features to a linear value-function approximation in order to carry out value iteration. Also
as mentioned earlier, value iteration itself is guaranteed to converge to the optimal value function.
However, due to the scale of problems we target, heuristic approximations are required. We discuss
the motivations for each heuristic approximation we employ briefly here.
First, we do not compute exact Bellman error features. Instead, we use machine learning to fit
a training set of sample states and their Bellman error values. The selection of this training set is
done heuristically, using trajectories drawn from the current greedy policy. Our use of on-policy
selection of training data is loosely motivated by on-policy convergence results for reinforcement
learning (Singh et al., 2000), and serves to focus training on relevant states. (See Section 3.1.)
Second, for the relational instance of our feature framework, the beam-search method we use to
select the highest scoring relational feature (with the best fit to the Bellman error) is ad-hoc, greedy,
and severely resource bounded. The fit obtained to the Bellman error is purely heuristic. We provide
our heuristic method for this machine learning problem only as an example, and we intend future
research to provide better relational learners and resulting better planning performance. Heuristic
elements of the current method are further discussed in Appendix A.3. Our work here can be
viewed as providing a reduction from stochastic planning to structured machine learning of numeric
functions. (See Section 3.)
Third, for the propositional instance of our feature framework,, the learner C4.5 selects hypotheses greedily. Also, our reduction to C4.5 classification relies on an explicit tolerance of approximation in the form of the threshold used to filter training data with near-zero Bellman error. The
motivation for this approximation tolerance is to focus the learner on high Bellman error states and
allow the method to ignore almost converged states. (See Section 4.4.)
Fourth, fundamental to this work is the use of a linear approximation of the value function and
gradient-descent-based weight selection (in this case AVI). These approximation methods are a key
approach to handling large state spaces and create the need for feature discovery. Our AVI method
includes empirically motivated heuristic methods for controlling step size and sign changes in the
weights. (See Section 5 in Online Appendix 1, available on JAIR website.)
Fifth, we rely on human input to select the sequence of problem difficulties encountered during
feature discovery as well as the performance thresholds at which problem difficulty increases. We
believe this aspect of the algorithm can be automated in future research. (See Section 3.)
709

fiW U & G IVAN

5. Related Work
Automatic learning of relational features for approximate value-function representation has surprisingly not been frequently studied until quite recently, and remains poorly understood. Here, we
review recent work that is related on one or more dimensions to our contribution.
5.1 Feature Selection Based on Bellman Error Magnitude
Feature selection based on Bellman error has recently been studied in the uncontrolled (policyevaluation) context in the work of Keller et al. (2006) and Parr et al. (2007), with attribute-value
or explicit state spaces rather than relational feature representations. Feature selection based on
Bellman error is further compared to other feature selection methods in the uncontrolled context
both theoretically and empirically in the work of Parr, Li, Taylor, Painter-Wakefield, and Littman
(2008).
Here, we extend this work to the controlled decision-making setting and study the incorporation
of relational learning and the selection of appropriate knowledge representation for value functions
that generalize between problems of different sizes within the same domain.
The main contribution of the work of Parr et al. (2007) is formally showing, for the uncontrolled
case of policy evaluation, that using (possibly approximate) Bellman-error features provably tightens approximation error bounds, i.e., that adding an exact Bellman error-feature provably reduces
the (weighted L2 -norm) distance from the optimal value function that can be achieved by optimizing the weights in the linear combination of features. This result is extended in a weaker form to
approximated Bellman-error features, again for the uncontrolled case. The limitation to the uncontrolled case is a substantial difference from the setting of our work. The limited experiments shown
use explicit state-space representations, and the technique learns a completely new set of features
for each policy evaluation conducted during policy iteration. In contrast, our method accumulates
features during value iteration, at no point limiting the focus to a single policy. Constructing a
new feature set for each policy evaluation is a procedure more amenable to formal analysis than
retaining all learned features throughout value iteration because the policy being implicitly considered during value iteration (the greedy policy) is potentially changing throughout. However, when
using relational feature learning, the runtime cost of feature learning is currently too high to make
constructing new feature sets repeatedly practically feasible.
Parr et al. (2007) builds on the prior work by Keller et al. (2006) that also studied the uncontrolled setting. That work provides no theoretical results nor any general framework, but provides
a specific approach to using Bellman error in attribute value representations (where a state is represented as a real vector) in order to select new features. The approach provides no apparent leverage
on problems where the state is not a real vector, but a structured logical interpretation, as is typical
in planning benchmarks.
5.2 Feature Discovery via Goal Regression
Other previous methods (Gretton & Thiebaux, 2004; Sanner & Boutilier, 2009) find useful features
by first identifying goal regions (or high reward regions), then identifying additional regions by regressing through the action definitions from previously identified regions. The principle exploited
is that when a given state feature indicates value in the state, then being able to achieve that feature
in one step should also indicate value in a state. Regressing a feature definition through the action
710

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

definitions yields a definition of the states that can achieve the feature in one step. Repeated regression can then identify many regions of states that have the possibility of transitioning under some
action sequence to a high-reward region.
Because there are exponentially many action sequences relative to plan length, there can be
exponentially many regions discovered in this way, as well as an exponential increase in the size of
the representation of each region. Both exponentials are in terms of the number of regression steps
taken. To control this exponential growth in the number of features considered, regression has been
implemented with pruning optimizations that control or eliminate overlap between regions when it
can be detected inexpensively as well as dropping of unlikely paths. However, without a scoring
technique (such as the fit to the Bellman-error used in this paper) to select features, regression still
generates a very large number of useless new features. The currently most effective regression-based
first-order MDP planner, described in the work of Sanner and Boutilier (2009), is only effective
when disallowing overlapping features to allow optimizations in the weight computation. Yet clearly
most human-designed feature sets in fact have overlapping features.
Our inductive technique avoids these issues by considering only compactly represented features,
selecting those which match sampled statewise Bellman error training data. We provide extensive
empirical comparison to the First-Order Approximate Linear Programming technique (FOALP)
from the work of Sanner and Boutilier (2009) in our empirical results. Our empirical evaluation
yields stronger results across a wide range of probabilistic planning benchmarks than the goalregression approach as implemented in FOALP (although aspects of the approaches other than the
goal-regression candidate generation vary in the comparison as well).
Regression-based approaches to feature discovery are related to our method of fitting Bellman
error in that both exploit the fact that states that can reach valuable states must themselves be valuable, i.e. both seek local consistency. In fact, regression from the goal can be viewed as a special
case of iteratively fitting features to the Bellman error of the current value function. Depending
on the exact problem formulation, for any k, the Bellman error for the k-step-to-go value function
will be non-zero (or otherwise nontrivially structured) at the region of states that reach the goal first
in k + 1 steps. Significant differences between our Bellman error approach and regression-based
feature selection arise for states which can reach the goal with different probabilities at different
horizons. Our approach fits the magnitude of the Bellman error, and so can smoothly consider the
degree to which each state reaches the goal at each horizon. Our approach also immediately generalizes to the setting where a useful heuristic value function is provided before automatic feature
learning, whereas the goal-regression approach appears to require goal regions to begin regression.
In spite of these issues, we believe that both approaches are appropriate and valuable and should be
considered as important sources of automatically derived features in future work.
Effective regression requires a compact declarative action model, which is not always available9 .
The inductive technique we present does not require even a PDDL action model, as the only deductive component is the computation of the Bellman error for individual states. Any representation
from which this statewise Bellman error can be computed is sufficient for this technique. In our empirical results we show performance for our planner on Tetris, where the model is represented only
by giving a program that, given any state as input, returns the explicit next state distribution for that
state. FOALP is inapplicable to such representations due to dependence on logical deductive rea9. For example, in the Second International Probabilistic Planning Competition, the regression-based FOALP planner
required human assistance in each domain in providing the needed domain information even though the standard
PDDL model was provided by the competition and was sufficient for each other planner.

711

fiW U & G IVAN

soning. We believe the inductive and deductive approaches to incorporating logical representation
are both important and are complementary.
The goal regression approach is a special case of the more general approach of generating candidate features by transforming currently useful features. Others that have been considered include
abstraction, specialization, and decomposition (Fawcett, 1996). Research on human-defined concept transformations dates back at least to the landmark AI program AM (Davis & Lenat, 1982).
Our work uses only one means of generating candidate features: a beam search of logical formulas
in increasing depth. This means of candidate generation has the advantage of strongly favoring concise and inexpensive features, but may miss more complex but very accurate/useful features. But
our approach directly generalizes to these other means of generating candidate features. What most
centrally distinguishes our approach from all previous work leveraging such feature transformations
is the use of statewise Bellman error to score candidate features. FOALP (Sanner & Boutilier, 2006,
2009) uses no scoring function, but includes all non-pruned candidate features in the linear program
used to find an approximately optimal value function; the Zenith system (Fawcett, 1996) uses a
scoring function provided by an unspecified critic.
5.3 Previous Scoring Functions for MDP Feature Selection
A method, from the work of Patrascu et al. (2002), selects features by estimating and minimizing
the L1 error of the value function that results from retraining the weights with the candidate feature
included. L1 error is used in that work instead of Bellman error because of the difficulty of retraining
the weights to minimize Bellman error. Because our method focuses on fitting the Bellman error
of the current approximation (without retraining with the new feature), it avoids this expensive
retraining computation during search and is able to search a much larger feature space effectively.
While the work of Patrascu et al. (2002) contains no discussion of relational representation, the L1
scoring method could certainly be used with features represented in predicate logic; no work to date
has tried this (potentially too expensive) approach.
5.4 Other Related Work
We include discussion of additional, more distantly related research directions as Appendix A, divided into the following subsections:
1. Other relevant feature selection methods (Fahlman & Lebiere, 1990; Utgoff & Precup, 1997,
1998; Rivest & Precup, 2003; Mahadevan & Maggioni, 2007; Petrik, 2007);
2. Structural model-based and model-free solution methods for Markov decision processes, including
(a) Relational reinforcement learning (RRL) systems (Dzeroski, DeRaedt, & Driessens,
2001; Driessens & Dzeroski, 2004; Driessens et al., 2006),
(b) Policy learning via boosting (Kersting & Driessens, 2008),
(c) Fitted value iteration (Gordon, 1995), and
(d) Exact value iteration methods in first-order MDPs (Boutilier, Reiter, & Price, 2001;
Holldobler & Skvortsova, 2004; Kersting, Van Otterlo, & De Raedt, 2004);
712

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

3. Inductive logic programming algorithms (Muggleton, 1991; Quinlan, 1996; Karalic & Bratko,
1997);
4. Approximate policy iteration for relational domains (Fern et al., 2006), with a discussion on
relational decision-list-policy learners (Khardon, 1999; Martin & Geffner, 2004; Yoon et al.,
2002);
5. Automatic extraction of domain knowledge (Veloso, Carbonell, Perez, Borrajo, Fink, &
Blythe, 1995; Kambhampati, Katukam, & Qu, 1996; Estlin & Mooney, 1997; Fox & Long,
1998; Gerevini & Schubert, 1998).

6. Experimental Setting
We present experiments in nine stochastic planning domains, including both reward-oriented and
goal-oriented domains. We use Pentium 4 Xeon 2.8GHz machines with 3GB memory. In this section, we give a general overview of our experiments before giving detailed results and discussion for
individual domains in Section 7. Here, first, we briefly discuss the selection of evaluation domains
in Section 6.1. Second, in Section 6.2 we set up an evaluation of our relational feature learner by
comparison to variants that replace key aspects of the algorithm with random choice to determine
their importance. Additional details, including many experimental parameter settings, can be found
in Online Appendix 1 (available on JAIR website) in Section 3.
6.1 Domains Considered
In all the evaluation domains below, it is necessary to specify a discount factor  when modeling the
domain as an MDP with discounting. The discount factor effectively specifies the tradeoff between
the goals of reducing expected plan length and increasing success rate.  is not a parameter of our
method, but of the domain being studied, and our feature-learning method can be applied for any
choice of . Here, for simplicity, we choose  to be 0.95 throughout all our experiments. We note
that this is the same discount factor used in the S YS A DMIN domain formalization that we compare
to from the previous work by Patrascu et al. (2002).
6.1.1 T ETRIS
In Section 7.2 we evaluate the performance of both our relational and propositional learners using
the stochastic computer-game T ETRIS, a reward-oriented domain where the goal of a player is to
maximize the accumulated reward. We compare our results to the performance of a set of handcrafted features, and the performance of randomly selected features.
6.1.2 P LANNING C OMPETITION D OMAINS
In Section 7.3, we evaluate the performance of our relational learner in seven goal-oriented planning domains from the two international probabilistic planning competitions (IPPCs) (Younes et al.,
2005; Bonet & Givan, 2006). For comparison purposes, we evaluate the performance of our propositional learner on two of the seven domains (B LOCKSWORLD and a variant of B OXWORLD described
below). Results from these two domains illustrate the difficulty of learning useful propositional features in complex planning domains. We also compare the results of our relational planner with
two recent competition stochastic planners FF-Replan (Yoon et al., 2007) and FOALP (Sanner &
713

fiW U & G IVAN

Boutilier, 2006, 2009) that have both performed well in the planning competitions. Finally, we
compare our results to those obtained by randomly selecting relational features and tuning weights
for them. For a complete description of, and PPDDL source for, the domains used, please see the
work of Younes et al. (2005) and Bonet and Givan (2006).
Every goal-oriented domain with a problem generator from the first or second IPPC was considered for inclusion in our experiments. For inclusion, we require a planning domain with fixed
action definitions, as defined in Section 2.4, that in addition has only ground conjunctive goal regions. Four domains have these properties directly, and we have adapted three more of the domains
to have these properties:
1. In B OXWORLD, we modify the problem generator so that the goal region is always a ground
conjunctive expression. We call the resulting domain C ONJUNCTIVE -B OXWORLD.
2. In F ILEWORLD, we construct the obvious lifted version, and create a problem generator restricted to three folders because in this domain the action definitions vary with the number of
folders. We call the resulting domain L IFTED -F ILEWORLD 3.
3. In T OWERS OF H ANOI, we create our own problem generator.
The resulting selection provides seven IPPC planning domains for our empirical study. We provide
detailed discussions on the adapted domains in Section 2 of Online Appendix 1 (available on JAIR
website), as well as discuss the reasons for the exclusion of domains.
6.1.3 S YS A DMIN
We conclude our experiments by comparing our propositional learner with a previous method by Patrascu et al. (2002), using the the same S YS A DMIN domain used for evaluation there. This empirical
comparison on the S YS A DMIN domain is shown in Section 7.4.
6.2 Randomized Variants of the Method
Our major contribution is the introduction and evaluation of a feature learning framework in the
controlled setting based on scoring with Bellman-error (BE Scoring). Our empirical work instantiates this framework with a relational feature-learning algorithm of our design based on greedy
beam-search. Here, we compare the performance of this instance of our framework with variants
that replace key aspects with randomized choice, illustrating the relative importance of those features. In the two random-choice experiments, we adapt our method in one of the following two
ways:
1. Labeling the training states with random scores instead of Bellman Error scores. The target
value in our feature training set is a random number from -1 to 1. This algorithm is called
Random Scoring.
2. Narrowing the beam during search randomly rather than greedily. We eliminate scoring during the beam search, instead using random selection to narrow the beam; only at the end of
the beam search is scoring used to select the best resulting candidate. This algorithm is called
Random Beam Narrowing.
714

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

The original algorithm, which labels training data with Bellman error and narrows the beam greedily rather than randomly, is called Greedy Beam Search/BE Scoring in our plots. For these
comparisons, we only consider the relational feature representation, as that is where our beam
search method is used. Experiments with the two variants introduced here, presented below in
Sections 7.2.4 and 7.3.4, show that our original method selects features that perform much better
than randomly selected features, and that the greediness in the beam search is often (but not always)
important in achieving good performance.

7. Experimental Results
We present experimental results for T ETRIS, planning competition domains, and S YS A DMIN in this
section, starting with an introduction on the structure of our result presentation.
7.1 How to Read Our Results
The task of evaluating a feature-learning planning system is subtle and complex. This is particularly
a factor in the relational case because generalization between problem sizes and learning from small
problems must be evaluated. The resulting data is extensive and highly structured, requiring some
training of the reader to understand and interpret. Here we introduce to the reader the structure of
our results.
In experiments with the propositional learning (or with randomly selected propositional features), the problem size never varies within one run of the learner, because the propositional representation from Section 4.3 cannot generalize between sizes. We run a separate experiment for each
size considered. Each experiment is two independent trials; each trial starts with a single trivial
feature and repeatedly adds features until a termination condition is met. After each feature addition, AVI is used to select the weights for combining the features to form a value function, and the
performance of that value function is measured (by sampling the performance of the greedy policy).
We then compute the average (of the two trials) of the performance as a function of the number
of features used. Since this results in a single line plot of performance as a function of number
of features, several different fixed-problem-size learners can be compared on one figure, with one
line for each, as is done for example in Figures 7 and 14. The performance measure used varies
appropriately with the domain as presented below.
We study the ability of relational representation from Section 4.1 to generalize between sizes.
This study can only be properly understood against the backdrop of the flowchart in Figure 1. As
described in this flowchart, one trial of the learner will learn a sequence of features and encounter
a sequence of increasing problem difficulties. One iteration of the learner will either add a new
feature or increase the problem difficulty (depending on the current performance). In either case,
the weights are then retrained by AVI and a performance measurement of the resulting greedy policy
is taken. Because different trials may increase the size at different points, we cannot meaningfully
average the measurements from two trials. Instead, we present two independent trials separately
in two tables, such as the Figures 5 and 12. For the first trial, we also present the same data a
second time as a line plot showing performance as a function of number of features, where problem
size changes are annotated along the line, such as the plots in Figures 6 and 13. Note that success
ratio generally increases along the line when features are added, but falls when problem size is
increased. (In T ETRIS, however, we measure rows erased rather than success ratio, and rows
715

fiW U & G IVAN

erased generally increases with either the addition of a new feature or the addition of new rows to
the available grid.)
To interpret the tables showing trials of the relational learner, it is useful to focus on the first
two rows, labeled # of features and Problem difficulty. These rows, taken together, show the
progress of the learner in adding features and and increasing problem size. Each column in the table
represents the result in the indicated problem size using the indicated number of learned features.
From one column to the next, there will be a change in only one of these rowsif the performance
of the policy shown in a column is high enough, it will be the problem difficulty that increases, and
otherwise it will be the number of features that increases. Further adding to the subtlety in interpreting these tables, we note that when several adjacent columns increase the number of features,
we sometimes splice out all but two of these columns to save space. Thus, if several features are
added consecutively at one problem size, with slowly increasing performance, we may show only
the first and last of these columns at that problem size, with a consequent jump in the number of
features between these columns. We likewise sometimes splice out columns when several consecutive columns increase problem difficulty. We have found that these splicings not only save space
but increase readability after some practice reading these tables.
Performance numbers shown in each column (success ratio and average plan length, or number
of rows erased, for T ETRIS) refer to the performance of the weight-tuned policy resulting for that
feature set at that problem difficulty. We also show in each column the performance of that value
function (without re-tuning weights) on the target problem size. Thus, we show quality measures
for each policy found during feature learning on both the current problem size at that point and on
the target problem size, to illustrate the progress of learning from small problems on the target size
via generalization.
We do not study here the problem of deciding when to stop adding features. Instead, in both
propositional and relational experiments, trials are stopped by experimenter judgment when additional results are too expensive for the value they are giving in evaluating the algorithm. However,
we do not stop any trials when they are still improving unless unacceptable resource consumption
has occurred.
Also, in each trial, the accumulated real time for the trial is measured and shown at each point
during the trial. We use real time rather than CPU time to reflect non-CPU costs such as paging due
to high memory usage.
7.2 Tetris
We now present experimental results for T ETRIS.
7.2.1 OVERVIEW

OF

T ETRIS

The game T ETRIS is played in a rectangular board area, usually of size 10  20, that is initially
empty. The program selects one of the seven shapes uniformly at random and the player rotates and
drops the selected piece from the entry side of the board, which piles onto any remaining fragments
of the pieces that were placed previously. In our implementation, whenever a full row of squares
is occupied by fragments of pieces, that row is removed from the board and fragments on top of
the removed row are moved down one row; a reward is also received when a row is removed. The
process of selecting locations and rotations for randomly drawn pieces continues until the board
is full and the new piece cannot be placed anywhere in the board. T ETRIS is stochastic since
716

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

the next piece to place is always randomly drawn, but this is the only stochastic element in this
game. T ETRIS is also used as an experimental domain in previous MDP and reinforcement learning
research (Bertsekas & Tsitsiklis, 1996; Driessens et al., 2006). A set of human-selected features is
described in the book by Bertsekas and Tsitsiklis (1996) that yields very good performance when
used in weighted linearly approximated value functions. We cannot fairly compare our performance
in this domain to probabilistic planners requiring PPDDL input because we have found no natural
PPDDL definition for T ETRIS.
Our performance metric for T ETRIS is the number of rows erased averaged over 10,000 trial
games. The reward-scaling parameter rscale (defined in Section 5 in Online Appendix 1 on page 8)
is selected to be 1.
7.2.2 T ETRIS R ELATIONAL F EATURE L EARNING R ESULTS
We represent the T ETRIS grid using rows and columns as objects. We use three primitive predicates:
fill(c, r), meaning that the square on column c, row r is occupied; below(r1 , r2 ), meaning that row
r1 is directly below row r2 ; and beside(c1 , c2 ), meaning that column c1 is directly to the left of
column c2 . The quantifiers used in our relational T ETRIS hypothesis space are typed using the types
row and column.
There are also state predicates representing the piece about to drop; however, for efficiency
reasons our planner computes state value as a function only of the grid, not the next piece. This
limitation in value-function expressiveness allows a significantly cheaper Bellman-backup computation. The one-step lookahead in greedy policy execution provides implicit reasoning about the
piece being dropped, as that piece will be in the grid in all the next states.
We conduct our relational T ETRIS experiments on a 10-column, n-row board, with n initially
set to 5 rows. Our threshold for increasing problem difficulty by adding one row is a score of at
least 15 + 20(n  5) rows erased. The target problem size for these experiments is 20 rows. The
results for the relational T ETRIS experiments are given in Figures 5 and 6 and are discussed below.
7.2.3 T ETRIS P ROPOSITIONAL F EATURE L EARNING R ESULTS
For the propositional learner, we describe the T ETRIS state with 7 binary attributes that represent
which of the 7 pieces is currently being dropped, along with one additional binary attribute for each
grid square representing whether that square is occupied. The adjacency relationships between the
grid squares are represented only through the procedurally coded action dynamics. Note that the
number of state attributes depends on the size of the T ETRIS grid, and learned features will only
apply to problems of the same grid size. As a result, we show separate results for selected problem
sizes.
We evaluate propositional feature learning in 10-column T ETRIS grids of four different sizes: 5
rows, 7 rows, 9 rows, and 20 rows. Results from these four trials are shown together in Figure 7 and
the average accumulated time required to reach each point on Figure 7 is shown in Figure 8. These
results are discussed below.
7.2.4 E VALUATING THE I MPORTANCE OF B ELLMAN - ERROR S CORING AND G REEDY
B EAM - SEARCH IN T ETRIS
Figure 9 compares our original algorithm with alternatives that vary from it on either training set
scoring or greediness of beam search, as discussed in Section 6.2. For the two alternatives, we use
717

fiW U & G IVAN

Trial #1
# of features
Problem difficulty
Score
Accumulated time (Hr.)
Target size score

0
5
0.2
0.0
0.3

1
5
0.5
2
1.3

2
5
1.0
4.2
1.4

3
5
3.0
5.2
1.8

11
5
18
20
178

11
6
31
21
238

12
6
32
24
261

17
6
35
39
176

17
7
55
42
198

18
7
56
46
211

18
8
80
50
217

18
9
102
57
221

18
10
121
65
220

18
15
234
111
268

18
20
316
178
317

Trial #2
# of features
Problem difficulty
Score
Accumulated time (Hr.)
Target size score

0
5
0.2
0.0
0.3

1
5
0.6
2.4
1.7

8
5
16
15
104

8
6
28
15
113

12
6
36
27
108

12
7
53
29
116

14
7
56
39
130

14
11
133
66
192

15
11
136
76
196

15
12
151
87
199

16
12
156
97
206

16
13
167
103
211

17
13
168
110
211

26
13
175
211
219

26
14
192
220
225

27
14
210
236
218

27
17
238
276
231

28
17
251
295
231

29
17
240
318
233

33
17
241
408
231

Figure 5: T ETRIS performance (averaged over 10,000 games). Score is shown in average rows
erased, and problem difficulty is shown in the number of rows on the T ETRIS board. The
number of columns is always 10. Difficulty increases when the average score is greater
than 15+20*(n-5), where n is the number of rows in the T ETRIS board. Target problem
size is 20 rows. Some columns are omitted as discussed in Section 7.1.

Average Rows Erased

Tetris, Relational, Trial 1
350
300
250
200
150
100
50
0

1020
1015

106

106

105

105
0

1010
107

2

4

6

8

10

12

14

16

18

Number of Features

Figure 6: Plot of the average number of lines erased over 10,000 T ETRIS games after each run of
AVI training during the learning of relational features (trial 1). Vertical lines indicate
difficulty increases (in the number of rows), as labeled along the plot.

718

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Average Rows Erased

Tetris, Propositional
14
12
10
8
6
4
2
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50

Number of Features
105

107

109

1020

Figure 7: Plot of the average number of lines erased in 10,000 T ETRIS games after each iteration
of AVI training during the learning of propositional features, averaged over two trials.

Accumulated Time (Hr.)

Tetris, Propositional
160
140
120
100
80
60
40
20
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50

Number of Features
105

107

109

1020

Figure 8: Plot of the accumulated time required to reach each point in Figure 7, averaged over two
trials.

the same schedule used for the original Greedy Beam Search/BE Scoring algorithm in T ETRIS by
starting with the 10  5 problem size. However, the performance of these two alternatives is never
good enough to increase the problem size.
7.2.5 E VALUATING H UMAN - DESIGNED F EATURES IN T ETRIS
In addition to evaluating our relational and propositional feature learning approach, we also evaluate
how the human-selected features described in the book by Bertsekas and Tsitsiklis (1996) perform
in selected problem sizes. For each problem size, we start from all weights zero and use our AVI
719

fiW U & G IVAN

Average Rows Erased

Impact of Greedy Beam Search and BE Scoring
18
16
14
12
10
8
6
4
2
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50

Number of Relational Features
Greedy Beam Search/BE Scoring (original algorithm)
Random Scoring (variant 1)
Random Beam Narrowing (variant 2)

Figure 9: Plot of the average number of lines erased in 10,000 T ETRIS games for relational features
learned from the original algorithm and the two alternatives as discussed in Section 6.2.
For Random Scoring and Random Beam Narrowing, the results are averages over two
independent trials. Trials of these two variants are terminated when they fail to make
progress for several feature additions. For comparison purposes, trial one of the original
Greedy Beam Search/BE Scoring method is shown, reaching the threshold for difficulty
increase after eleven feature additions (trial two did even better).

Average rows erased, Trial 1
Average rows erased, Trial 2

10  5 10  7 10  9 10  20
19
86
267 17,954
19
86
266 18,125

Figure 10: The average number of lines erased in 10,000 T ETRIS games for the best weighted
combination of human features found in each of two trials of AVI and four problem
sizes.

process described in Section 2.5 to train the weights for all 21 features until the performance appears
30
3
to 1+k/100
as human-designed features
to converge. We change the learning rate  from 1+k/100
require a larger step-size to converge rapidly. The human-designed features are normalized to a
value between 0 and 1 here in our experiments. We run two independent trials for each problem size
and report the performance of the best-performing weight vector found in each trial, in Figure 10.
7.2.6 P ERFORMANCE C OMPARISON B ETWEEN D IFFERENT A PPROACHES

TO

T ETRIS

Several general trends emerge from the results on T ETRIS. First of all, the addition of new learned
features is almost always increasing the performance of the resulting tuned policy (on the current
size and on the target size), until a best performance point is reached. This suggests we are in fact
720

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Relational Prop. 10  5 Prop.10  7 Prop.10  9 Prop.10  20
Average feature learning
time (Min.)

167

44

52

60

44

Figure 11: Table for the average feature learning time for relational and propositional approaches.
selecting useful features. We also find clear evidence of the ability of the relational representation
to usefully generalize between problem sizes: substantial performance is developed on the target
problem size without ever training directly in that size.
We find that the best performance of learned propositional features is much lower than that of
learned relational features in all problem sizes shown here, even though a larger feature training set
size and many more learned features are used for the propositional approach. This suggests that
the rich relational representation indeed is able to better capture the dynamics in T ETRIS than the
propositional representation.
We find that the performance of using random features in T ETRIS is significantly worse than that
of using learned features, demonstrating that our performance improvements in feature learning are
due to useful feature selection (using Bellman error), not simply due to increasing the number of
features.
Our learned relational feature performance in 10  20 T ETRIS is far worse than that obtained
by using the human-selected features with AVI in the same size. However, in 10  5 T ETRIS the
relational feature performance is close to that of the human-designed features. The human-designed
features are engineered to perform well in the 10  20 T ETRIS hence some concepts that are useful
in performing well in smaller problem sizes may not exist in these features.
7.2.7 T IME TO L EARN E ACH F EATURE
In Figure 11 we show the average time required to learn a relational feature or a propositional feature
in T ETRIS.
The time required to learn a relational feature is significantly longer than that required to learn
a propositional feature, even though for the propositional approach a larger feature training set size
is being used.
7.2.8 C OMPARISON

TO

P REVIOUS T ETRIS - SPECIFIC L EARNERS

In evaluating domain-independent techniques on T ETRIS, we must first put aside the strong performance already shown many times in the literature for domain-dependent techniques on that domain.
Then, we must face the problem that there are no published domain-independent comparison points
in order to define a state-of-the-art target to surpass. For the latter problem, we provide a baseline
from two different approaches to random feature selection, and show that our targeted feature selection dramatically improves on random selection. For the former problem, we include below a
discussion of the domain-specific elements of key previous published results on T ETRIS.
There have been many previous domain-specific efforts at learning to play T ETRIS (Bertsekas
& Tsitsiklis, 1996; Szita & Lorincz, 2006; Lagoudakis, Parr, & Littman, 2002; Farias & Van Roy,
2004; Kakade, 2001). Typically, these provide human-crafted domain-dependent features, and deploy domain-independent machine learning techniques to combine these features (often by tuning
721

fiW U & G IVAN

weights for a linear combination). As an example, a domain-specific feature counting the number
of covered up holes in the board is frequently used. This feature is plausibly derived by human
reasoning about the rules of the game, such as realizing that such holes are difficult to fill by later
action and can lead to low scores. In all prior work, the selection of this feature is by hand, not by
an automated feature-selection process (such as our scoring of correlation to Bellman error). Other
frequently used domain-specific features include column height and difference in height of adjacent columns, again apparently selected as relevant by human reasoning about the rules of the
game.
The key research question we address, then, is whether useful features can be derived automatically, so that a decision-making situation like T ETRIS can be approached by a domain-independent
system without human intervention. Our method is provided only a domain-state representation using primitive horizontal and vertical positional predicates, and a single constant feature. To our
knowledge, before this research there is no published evaluation on T ETRIS that does not rely
on domain-specific human inputs such as those just discussed. As expected, our performance on
T ETRIS is much weaker than that achieved by domain-specific systems such as those just cited.
7.3 Probabilistic Planning Competition Domains
Throughout the evaluations of our learners in planning domains, we use a lower plan-length cutoff of
1000 steps when evaluating success ratio during the iterative learning of features, to speed learning.
We use a longer cutoff of 2000 steps for the final evaluation of policies for comparison with other
planners and for all evaluations on the target problem size. The reward-scaling parameter rscale
(defined in Section 5 in Online Appendix 1 on page 8) is selected to be 1 throughout the planning
domains.
For domains with multi-dimensional problem sizes, it remains an open research problem on how
to change problem size in different dimensions automatically to increase difficulty during learning.
Here, in C ONJUNCTIVE -B OXWORLD and Z ENOTRAVEL, we hand-design the sequence of increasing problem sizes.
As discussed in Section 6.1.2, we evaluate our feature learners in a total of seven probabilistic planning competition domains. In the following paragraphs, we provide a full discussion of
B LOCKSWORLD and C ONJUNCTIVE -B OXWORLD, with abbreviated results for the other five domains. We provide a full discussion of the other five domains in Appendix B.
Our relational feature learner finds useful value-function features in four of these domains
(B LOCKSWORLD, C ONJUNCTIVE -B OXWORLD, T IREWORLD, and L IFTED -F ILEWORLD 3). In the
other three domains (Z ENOTRAVEL, E XPLODING B LOCKSWORLD, and T OWERS OF H ANOI), our
relational feature learner makes progress in representing a useful fixed-size value function for the
training sizes, but fails to find features that generalize well to problems of larger sizes.
7.3.1 B LOCKSWORLD
In the probabilistic, non-reward version of B LOCKSWORLD from the first IPPC, the actions pickup
and putdown have a small probability of placing the handled block on the table object instead of on
the selected destination.
For our relational learner, we start with 3 blocks problems. We increase from n blocks to n + 1
blocks whenever the success ratio exceeds 0.9 and the average successful plan length is less than
30(n  2). The target problem size is 20 blocks. Results are shown in Figures 12 and 13.
722

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Trial #1
# of features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size SR
Target size Slen.

0
1
2
2
3
3
3
3
3
3
3
4
4
5
10
15
1.00 1
1 0.95 1
1
1 0.97
89 45 20 133 19
33 173 395
0.5 1.0 1.5 2.2 3.3 3.9 10
36
0
0
0
0 0.98 0.96 0.98 0.97




761 724 754 745

Trial #2
# of features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size SR
Target size Slen.

0
3
1
80
0.5
0


1
2
2
3
3
3
3
3
3
4
4
5
10
15
1
1 0.94 1
1
1 0.96
48 19 125 17
34 167 386
1.0 1.4 2.0 3.3 3.8 9.4 33
0
0
0 0.97 0.98 0.98 0.98



768 750 770 741

Figure 12: B LOCKSWORLD performance (averaged over 600 problems) for relational learner. We
add one feature per column until success ratio exceeds 0.9 and average successful plan
length is less than 30(n  2), for n blocks, and then increase problem difficulty for the
next column. Plan lengths shown are successful trials only. Problem difficulties are
measured in number of blocks, with a target problem size of 20 blocks. Some columns
are omitted as discussed in Section 7.1.

For our propositional learner, results for problem sizes of 3, 4, 5, and 10 blocks are shown in
Figure 14.
Our relational learner consistently finds value functions with perfect or near-perfect success
ratio up to 15 blocks. This performance compares very favorably to the recent RRL (Driessens
et al., 2006) results in the deterministic B LOCKSWORLD, where goals are severely restricted to, for
instance, single ON atoms, and the success ratio performance of around 0.9 for three to ten blocks
(for the single ON goal) is still lower than that achieved here. Our results in B LOCKSWORLD show
the average plan length is far from optimal. We have observed large plateaus in the induced value
function: state regions where all states are given the same value so that the greedy policy wanders.
This is a problem that merits further study to understand why feature-induction does not break such
plateaus. Separately, we have studied the ability of local search to break out of such plateaus (Wu,
Kalyanam, & Givan, 2008).
The performance on the target size clearly demonstrates successful generalization between sizes
for the relational representation.
The propositional results demonstrate the limitations of the propositional learner regarding lack
of generalization between sizes. While very good value functions can be induced for the small
problem sizes (3 and 4 blocks), slightly larger sizes of 5 or 10 blocks render the method ineffective.
In 10 block problems, the initial random greedy policy cannot be improved because it never finds
723

fiW U & G IVAN

Success Ratio

Blocksworld, Relational, Trial 1
3 blocks

1

3 blocks

3 blocks

4, 5, 10 blocks
15 blocks

0.95
4 blocks
0.9
0.85
0.80

Successful Plan Length

0

1

2

400

3
15 blocks

300
200
3 blocks

100

3 blocks

4 blocks

10 blocks

3 blocks

5 blocks
4 blocks

0
0

1

2

3

Number of Features

Figure 13: B LOCKSWORLD success ratio and average successful plan length (averaged over 600
problems) for the first trial from Figure 12 using our relational learner.

the goal. In addition, these results demonstrate that learning additional features once a good policy
is found can degrade performance, possibly because AVI performs worse in the higher dimensional
weight space that results.
7.3.2 C ONJUNCTIVE -B OXWORLD
The probabilistic, non-reward version of B OXWORLD from the first IPPC is similar to the more
familiar Logistics domain used in deterministic planning competitions, except that an explicit connectivity graph for the cities is defined. In Logistics, airports and aircraft play an important role
since it is not possible to move trucks from one airport (and the locations adjacent to it) to another airport (and the locations adjacent to it). In B OXWORLD, it is possible to move all the boxes
without using the aircraft since the cities may all be connected with truck routes. The stochastic
element introduced into this domain is that when a truck is being moved from one city to another,
there is a small chance of ending up in an unintended city. As described in Section 6.1, we use
C ONJUNCTIVE -B OXWORLD, a modified version of B OXWORLD, in our experiments.
We start with 1-box problems in our relational learner and increase from n boxes to n + 1 boxes
whenever the success ratio exceeds 0.9 and the average successful plan length is better than 30n.
All feature-learning problem difficulties use 5 cities. We use two target problem sizes: 15 boxes
724

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Blocksworld, Propositional

Success Ratio

1
0.8
0.6
0.4
0.2

Successful Plan Length

450
400
350
300
250
200
150
100
50
0

Accumulated Time (Hr.)

0

90
80
70
60
50
40
30
20
10
0

0

2

4

6

8

10

0

2

4

6

8

10

0

2

4

6

8

10

Number of Features

3 blocks

4 blocks

5 blocks

10 blocks

Figure 14: B LOCKSWORLD performance success ratio and average successful plan length (averaged over 600 problems), and accumulated run-time for our propositional learner, averaged over two trials.

725

fiW U & G IVAN

Trial #1
# of features
0
1
2
Problem difficulty
1
1
1
Success ratio
0.97 1
1
Plan length
226 84 23
Accumulated time (Hr.) 7.2 10 13
0.98 1
1
Target size #1 SR
Target size #1 Slen.
1056 359 93
Target size #2 SR
0.16 0.90 0.97
Target size #2 Slen.
1583 996 238

2
2
1
37
14
1
91
0.97
230

2
3
1
44
16
1
90
0.96
233

2
5
1
54
21
1
92
0.98
244

2
10
1
77
42
1
90
0.96
240

2
11
1
80
49
1
92
0.98
238

2
12
1
313
57
1
355
0.90
1024

2
13
1
87
65
1
90
0.98
240

2
15
1
92
84
1
91
0.96
239

Trial #2
# of features
0
1
2
Problem difficulty
1
1
1
Success ratio
0.97 1
1
Plan length
235 85 24
Accumulated time (Hr.) 7.3 11 14
Target size #1 SR
0.96 1
1
1019 365 90
Target size #1 Slen.
Target size #2 SR
0.19 0.9 0.97
Target size #2 Slen.
1574 982 226

2
2
1
34
16
1
91
0.97
230

2
3
1
43
18
1
91
0.98
233

2
5
1
54
23
1
92
0.98
233

2
9
1
72
39
1
89
0.97
242

2
10
1
299
45
1
359
0.92
1006

2
11
1
80
51
1
89
0.98
231

2
12
1.00
310
60
1
363
0.91
1026

2
13
1
84
68
1
90
0.97
240

2
15
1
91
86
1
90
0.96
233

Figure 15: C ONJUNCTIVE -B OXWORLD performance (averaged over 600 problems). We add one
feature per column until success ratio is greater than 0.9 and average successful plan
length is less than 30n, for n boxes, and then increase problem difficulty for the next
column. Problem difficulty is shown in number of boxes. Throughout the learning
process the number of cities is 5. Plan lengths shown are successful trials only. Two
target problem sizes are used. Target problem size #1 has 15 boxes and 5 cities. Target
problem size #2 has 10 boxes and 10 cities. Some columns are omitted as discussed in
Section 7.1.

and 5 cities, and 10 boxes and 10 cities. Relational learning results are shown in Figures 15 and 16,
and results for the propositional learner on 5 cities with 1, 2, or 3 boxes are shown in Figures 17.
In interpreting the C ONJUNCTIVE -B OXWORLD results, it is important to focus on the average
successful plan-length metric. In C ONJUNCTIVE -B OXWORLD problems, random walk is able to
solve the problem nearly always, but often with very long plans10 . The learned features enable
more direct solutions as reflected in the average plan-length metric.
Only two relational features are required for significantly improved performance in the problems
we have tested. Unlike the other domains we evaluate, for the C ONJUNCTIVE -B OXWORLD domain
10. We note that, oddly, the IPPC competition domain used here has action preconditions prohibiting moving a box away
from its destination. These preconditions bias the random walk automatically towards the goal. For consistency with
the competition results, we retain these odd preconditions, although these preconditions are not necessary for good
performance for our algorithm.

726

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Success Ratio

Conjuctive-Boxworld, 5 Cities, Relational, Trial 1
1 box

1

1, 2, 3, 5, 10, and 15 boxes

1 box

0.95
0.90
0

1

2

250

Successful Plan Length

1 box
200
150
100

15 boxes
10 boxes

1 box
5 boxes
3 boxes

50

2 boxes
1 box

0
0

1

2

Number of Features

Figure 16: C ONJUNCTIVE -B OXWORLD success ratio and average successful plan length (averaged
over 600 problems) for the first trial using our relational learner.

the learned features are straightforwardly describable in English. The first feature counts how many
boxes are correctly at their target city. The second feature counts how many boxes are on trucks.
We note the lack of any features rewarding trucks for being in the right place (resulting in
longer plan lengths due to wandering on value-function plateaus). Such features can easily be written in our knowledge representation (e.g. count the trucks located at cities that are the destinations
for some package on the truck), but require quantification over both cities and packages. The severe
limitation on quantification currently in our method for efficiency reasons prevents consideration of
these features at this point. It is also worth noting that regression-based feature discovery, as studied in the work of Gretton and Thiebaux (2004) and Sanner and Boutilier (2009), can be expected
to identify such features regarding trucks by regressing the goal through the action of unloading
a package at the destination. Combining our Bellman-error-based method with regression-based
methods is a promising future direction.
Nevertheless, our relational learner discovers two concise and useful features that dramatically
reduce plan length relative to the initial policy of random walk. This is a significant success for
automated domain-independent induction of problem features.
727

fiW U & G IVAN

Conjunctive-Boxworld, Propositional

Success Ratio

1
0.8
0.6
0.4
0.2

Successful Plan Length

0
0

2

4

6

8

10

0

2

4

6

8

10

0

2

4

6

8

10

500
450
400
350
300
250
200
150
100
50
0

Accumulated Time (Hr.)

450
400
350
300
250
200
150
100
50
0

Number of Features
1 box

2 box

3 box

Figure 17: C ONJUNCTIVE -B OXWORLD performance (averaged over 600 problems) and accumulated run-time for propositional learner, averaged over two trials. Throughout the learning process the number of cities is 5.

728

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

One trial of the relational feature learner in C ONJUNCTIVE -B OXWORLD takes several days,
even though we have fixed the number of cities for the training problems at five cities. New techniques are required for improving the efficiency of feature learning before we can provide results
for training in larger numbers of cities. Our results here demonstrate that the current representation
and learning methods adequately manage small city graphs even with larger and larger numbers of
boxes to deliver, and that the resulting value functions successfully generalize to 10-city problems.
In this domain, a well known weakness of AVI is apparent. While AVI often works in practice,
there is no theoretical guarantee on the quality of the weight vector found by AVI training. (Alternatively, an approximate linear programming step could replace AVI training to provide a more
expensive but perhaps more robust weight selection.) In the C ONJUNCTIVE -B OXWORLD results,
AVI training goes astray when selecting weights in the 12 box domain size in Trial 1. As a result,
the selected weights overemphasize the first feature, neglecting the second feature. This is revealed
in the data shown because the plan-length performance degrades significantly for that one column
of data. When AVI is repeated at the next problem size (13 boxes), good performance is restored.
A similar one-column degradation of plan length occurs in trial 2 at the 10-box and 12-box sizes.
For our propositional experiments in the C ONJUNCTIVE -B OXWORLD, we note that, generally,
adding learned propositional features degrades the success-rate performance relative to the initial
random walk policy by introducing ineffective loops into the greedy policy. The resulting greedy
policies find the goal in fewer steps than random walk, but generally pay an unacceptable drop in
success ratio to do so. The one exception is the policy found for one-box problems using just two
propositional features, which significantly reduces plan length while preserving success ratio. Still,
this result is much weaker than that for our relational feature language.
These problems get more severe as problem size increases, with 3-box problems suffering severe
degradation in success rate with only modest gains in successful plan length. Also please note
that accumulated runtime for these experiments is very large, especially for 3-box problems. AVI
training is very expensive for policies that do not find the goal. Computing the greedy policy at each
state in a long trajectory requires considering each action, and the number of available actions can
be quite large in this domain. For these reasons, the propositional technique is not evaluate at sizes
larger than three boxes.
7.3.3 S UMMARY R ESULTS

FROM

A DDITIONAL D OMAINS

In Figures 18 to 20, we present summary results from five additional probabilistic planning domains. For detailed results and full discussion of these domains, please see Appendix B. From the
summary results, we can see that our feature learning approach successfully finds features that perform well across increasing problem sizes in two of these five domains, T IREWORLD and L IFTED F ILEWORLD 3. In the other three domains (Z ENOTRAVEL, T OWERS OF H ANOI, and E XPLODING
B LOCKSWORLD), feature learning is able to make varying degrees of progress on fixed small problem sizes, but that progress (sometimes quite limited) does not generalize well as size increases.
7.3.4 E VALUATING THE R ELATIVE I MPORTANCE OF B ELLMAN - ERROR S CORING AND
G REEDY B EAM - SEARCH IN G OAL - ORIENTED D OMAINS
Figure 21 compares our original algorithm with alternatives that vary from it on either training set
scoring or greediness of beam search, as discussed in Section 6.2. For each trial of each variant, we
generate a greedy policy for each domain using feature selection within our relational representation
729

fiW U & G IVAN

Tireworld, Trial 1

Success Ratio

1
0.9

4 nodes

4, 5 nodes

6 nodes

20, 30 nodes

6 nodes

9 nodes

9, 10 nodes

3

4

4 nodes

0.8
0.7
0.6
4 nodes

0.5
0.4
0

Successful Plan Length

0

1

2

5

6

20, 30 nodes

5

4 nodes

4

4 nodes

4 nodes

3

9, 10 nodes
6, 9 nodes

2

4, 5, 6 nodes

1
0
0

1

2

3

4

5

Number of Features

Zenotravel, Trial 1
Success Ratio

1
3 cities, 1 person, 1 aircraft

0.8
0.6

3 cities, 2 people, 2 aircraft

0.4

3 cities, 2 people, 2 aircraft

0
0.2

Successful Plan Length

0

1

2

3

4

5

6

7

8

9

500
400

3 cities, 2 people, 2 aircraft

3 cities, 2 people, 2 aircraft

300
200

3 cities, 1 person, 1 aircraft

100
0
0

1

2

3

4

5

6

7

8

9

Number of Features

Figure 18: Summary results for T IREWORLD and Z ENOTRAVEL. For full discussion and detailed
results, please see Appendix B.

730

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Exploding Blocksworld, Trial 1

Success Ratio

1
0.8

3 blocks

3 blocks
3 blocks

0.6

3 blocks
4 blocks

0.4

4 blocks

0.2
0

Successful Plan Length

0

1

2

3

4

5

6

7

8

9

10

6
5

4 blocks

4

4 blocks

3
3 blocks

3 blocks

2

3 blocks

3 blocks

1
0
0

1

2

3

4

5

6

7

8

9

10

Number of Features

Tower of Hanoi, Trial 1

Success Ratio

1
2 discs

0.8
0.6

4 discs

0.4

3 discs

0.2

5 discs

4 discs
0

Successful Plan Length

0

1

2

3

4

5

6

7

20
9

8

38
10

50
40
30

3 discs

20
10

4 discs

2 discs
0
0

1

2

3

4

5

6

7

8

Number of Features

Figure 19: Summary results for E XPLODING B LOCKSWORLD and T OWERS
discussion and detailed results, please see Appendix B.

731

OF

H ANOI. For full

fiW U & G IVAN

Success Ratio

Lifted-Fileworld3, Trial 1
1 file

1

1 file

1 file

1 and 2 files

14 to 16 files

16 files

2 to 14 files

16 to 20 files

0.95
0.90
0

1

2

3

4

5

6

7

Successful Plan Length

60
16 files
14 files

20 files

40

14 files
15 files

13 files
12 files
10 files

11 files

16 files

16 files

20
1 file

1 file
1 file

4 files

2 files

3 files
2 files

1 file

0
0

1

2

3

4

5

6

7

Number of Features

Figure 20: Summary results for L IFTED -F ILEWORLD 3. For full discussion and detailed results,
please see Appendix B.

(alternating AVI training, difficulty increase, and feature generation as in the original algorithm).
During each trial, in each domain, we select the best performing policy, running the algorithm until
the target problem difficulty is reached or there is no improvement for at least three feature additions;
in the latter case generating at least nine features. We evaluate each greedy policy acquired in this
manner, measuring the average target-problem-size performance in each domain, and average the
results of two trials. The results are shown in Figure 21.
In no domain does the alternative Random Scoring perform comparably to the original Greedy
Beam Search/BE Scoring, with the exception of three domain/size combinations where both learners
perform very poorly (Z ENOTRAVEL, 10-block E XPLODING B LOCKSWORLD, and 5-disc T OWERS
OF H ANOI ). The alternative Random Beam Narrowing is sometimes adequate to replace the original
approach, but in some domains, greedy beam search is critical to our performance.
7.3.5 C OMPARISON

TO

FF-R EPLAN

AND

FOALP

We compare the performance of our learned policies to FF-Replan and FOALP on each of the
PPDDL evaluation domains used above. We use the problem generators provided by the planning
competitions to generate 30 problems for each tested problem size except for T OWERS OF H ANOI
732

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Domain
Size
Greedy Beam/BE Scoring (orig.) SR
Greedy Beam/BE Scoring (orig.) SLen.
Random Scoring (var. 1) SR
Random Scoring (var. 1) SLen.
Random Beam Narrowing (var. 2) SR
Random Beam Narrowing (var. 2) SLen.
Random walk SR
Random walk SLen.

BW
20
0.98
748
0

0.01
258
0


Box Box Tire Zeno EX-BW EX-BW TOH TOH File
(15,5) (10,10) 30 (10,2,2)
5
10
4
5 30
1
0.98 0.92 0.11
0.34
0.03 0.51 0.00 1
90
235
5
1137
6
23
4
14 65
0.99 0.21 0.67 0.05
0.27
0.01 0.24 0.03 1
946 1582
6
910
6
12
13 26 215
1
0.99 0.91 0.13
0.35
0.02 0.25 0.01 1
90
242
6
1127
8
19
38 84 250
0.97 0.18 0.18 0.06
0.13
0
0.09 0.00 1
1038 1579
6
865
4

14 14 251

Figure 21: Target-problem-size performance (averaged over 600 problems) for relational features
learned from the original algorithm and the two alternatives as discussed in Section 6.2,
and from random walk, averaged over the best results of two independent trials for each
target problem size.

and L IFTED -F ILEWORLD 3, where there is one fixed problem for each problem size. We evaluate
the performance of each planner 30 times for each problem, and report in Fig. 22 the success ratio
of each planner in each problem size (averaged over all attempts). Our policies, learned from the
two independent trials shown above, are indicated as RFAVI #1 and RFAVI #2. Each planner has a
30-minute time limit for each attempt. The average time required to finish a successful attempt for
the largest problem size in each domain is reported in Figure 23.
For each of the two trials of our learner in each domain, we evaluate here the policy that performed the best in the trial on the (first) target problem size. (Here, a policy is a set of features
and a corresponding weight vector learned by AVI during the trial.) Performance is measured by
success rate, with ties broken by plan length. Any remaining ties are broken by taking the later
policy in the trial from those that are tied. In each case, we consider that policy to be the policy
learned from the trial.
The results show that our planners performance is incomparable with that of FF-Replan (winning in some domains, losing in others) and generally dominates that of FOALP.
RFAVI performs the best of the planners in larger B LOCKSWORLD, C ONJUNCTIVE B OXWORLD, and T IREWORLD problems. RFAVI is essentially tied with FF-Replan in performance
in L IFTED -F ILEWORLD 3. RFAVI loses to FF-Replan in the remaining three domains, E XPLODING
B LOCKSWORLD, Z ENOTRAVEL, and T OWERS OF H ANOI. Reasons for the difficulties in the last
three domains are discussed above in the sections presenting results for those domains. We note that
FOALP does not have a learned policy in Z ENOTRAVEL, E XPLODING B LOCKSWORLD, T OWERS
OF H ANOI , and L IFTED -F ILEWORLD 3.
RFAVI relies on random walk to explore plateaus of states not differentiated by the selected
features. This reliance frequently results in long plan lengths and at times results in failure. We
have recently reported elsewhere on early results from ongoing work remedying this problem by
using search in place of random walk (Wu et al., 2008).
The RFAVI learning approach is very different from the non-learning online replanning used
by FF-Replan, where the problem is determinized, dropping all probability parameters. It is an
733

fiW U & G IVAN

RFAVI #1
RFAVI #2
FF-Replan
FOALP

15 blocks BW
1 (483)
1.00 (463)
0.93 (52)
1 (56)

20 blocks BW
1 (584)
1.00 (578)
0.91 (71)
0.73 (73)

25 blocks BW
0.85 (1098)
0.85 (1099)
0.7 (96)
0.2 (96)

30 blocks BW
0.75 (1243)
0.77 (1227)
0.23 (118)
0.07 (119)

RFAVI #1
RFAVI #2
FF-Replan
FOALP

(10BX,5CI)Box
1 (76)
1 (75)
1 (70)
1 (35)

(10BX,10CI)Box
0.97 (225)
0.97 (223)
0.98 (256)
0.70 (257)

(10BX,15CI)Box
0.93 (459)
0.93 (454)
0.93 (507)
0.28 (395)

(15BX,5CI)Box
1 (90)
1 (90)
1 (88)
0.99 (56)

RFAVI #1
RFAVI #2
FF-Replan
FOALP

20 nodes Tire
0.87 (5)
0.85 (4)
0.76 (2)
0.92 (4)

30 nodes Tire
0.85 (7)
0.84 (7)
0.73 (3)
0.90 (5)

40 nodes Tire
0.98 (6)
0.97 (6)
0.83 (3)
0.91 (5)

(10CI,2PR,2AT)Zeno
0.06 (1240)
0.07 (1252)
1 (99)
N/A

RFAVI #1
RFAVI #2
FF-Replan
FOALP

5 blocks EX-BW
0.25 (8)
0.25 (8)
0.91 (7)
N/A

10 blocks EX-BW 4 discs TOH
0.02 (30)
0.43 (4)
0.01 (35)
0.47 (4)
0.45 (20)
0.57 (3)
N/A
N/A

5 discs TOH
0 ()
0 ()
0.37 (7)
N/A

(20BX,20CI)Box
0.82 (959)
0.82 (989)
0.35 (1069)
0.0 (711)

30 files Lifted-File
1 (65)
1 (65)
1 (66)
N/A

Figure 22: Comparison of our planner (RFAVI) against FF-Replan and FOALP. Success ratio for a
total of 900 attempts (30 attempts for T OWERS OF H ANOI and L IFTED -F ILEWORLD 3)
for each problem size is reported, followed by the average successful plan length in
parentheses. The two rows for RFAVI map to two learning trials shown in the paper.

30 BW (20,20) BX 40 Tire (10,2,2) Zeno 10 EX-BW 5 TOH 30 Files
RFAVI #1
106s
83s
1s
51s
2s

1s
RFAVI #2
105s
86s
0s
51s
3s

1s
FF-Replan 872s
739s
0s
1s
8s
3s
10s
FOALP
16s
173s
24s
N/A
N/A
N/A
N/A
Figure 23: Average runtime of the successful attempts, from the results shown in Figure 22, on the
largest problem size for each domain.

734

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

important topic for future research to try to combine the benefits obtained by these very different
planners across all domains.
The dominance of RFAVI over FOALP in these results implies that RFAVI is at the state of the
art among first-order techniques  those that work with the problem in lifted form and use lifted
generalization. Although FOALP uses first-order structure in feature representation, the learned
features are aimed at satisfying goal predicates individually, not as a whole. We believe that the
goal-decomposition technique can sometimes work well in small problems but does not scale well
to large problems.
In these comparisons, it should also be noted that FOALP does not read PPDDL domain descriptions directly, but requires human-written domain axioms for its learning, unlike our completely
automatic technique (requiring only a few numeric parameters characterizing the domain). This
requirement for human-written domain axioms is one of the reasons why FOALP did not compete
in some of the competition domains and does not have a learned policy for some of the domains
tested here.
In C ONJUNCTIVE -B OXWORLD11 , we note that FF-Replan uses an all outcomes problem determinization that does not discriminate between likely and unlikely outcomes of truck-movement
actions. As a result, plans are frequently selected that rely on unlikely outcomes (perhaps choosing
to move a truck to an undesired location, relying on the unlikely outcome of accidentally moving
to the desired location). These plans will usually fail, resulting in repeated replanning until FF luckily selects the high-likelihood outcome or plan execution happens to get the desired low-likelihood
outcome. This behavior is in effect similar to the behavior our learned value function exhibits because, as discussed in Section 7.3.2, our learner failed to find any feature rewarding appropriate
truck moves. Both planners result in long plan lengths due to many unhelpful truck moves. However, our learned policy conducts the random walk of trucks much more efficiently (and thus more
successfully) than the online replanning of FF-Replan, especially in the larger problem sizes. We
believe even more dramatic improvements will be available with improved knowledge representation for features.
7.4 SysAdmin
A full description of the S YS A DMIN domain is provided in the work of Guestrin, Koller, and Parr
(2001). Here, we summarize that description. In the S YS A DMIN domain, machines are connected
in different topologies. Each machine might fail at each step, and the failure probability depends on
the number of failed machines connected to it. The agent works toward minimizing the number of
failed machines by rebooting machines, with one machine rebooted at each time step. For a problem
with n machines and a fixed topology, the dynamic state space can be sufficiently described by n
propositional variables, each representing the on/off status of a certain machine.
We test this domain for the purpose of direct comparison of the performance of our propositional techniques to the published results in the work of Patrascu et al. (2002). We test exactly the
topologies evaluated there and measure the performance measure reported there, sup-norm Bellman
error.
We evaluate our method on the exact same problems (same MDPs) used for evaluation in the
work of Patrascu et al. (2002) for testing this domain. Two different kinds of topologies are tested:
11. We hand-convert the nested universal quantifiers and conditional effects in the original BOXWORLD domain definition
to an equivalent form without universal quantifiers and conditional effects to allow FF-Replan to read the domain.

735

fiW U & G IVAN

S

S

Cycle Topology

3-legs Topology

Figure 24: Illustration of the two topologies in the S YS A DMIN domain (10 nodes). Each node
represents a machine. The S label indicates a server machine, as specified in the work
of Patrascu et al. (2002).

3-legs and cycle. The 3-legs topology has three three-node legs (each a linear sequence of three
connected nodes) each connected to a single central node at one end. The cycle topology arranges
the ten nodes in one large cycle. There are 10 nodes in each topology. These two topologies
are illustrated in Figure 24. The target of learning in this domain is to keep as many machines
operational as possible, so the number of operating machines directly determines the reward for
each step. Since there are only 10 nodes and the basic features are just the on/off statuses of the
nodes, there are a total of 1024 states. The reward-scaling parameter rscale (defined in Section 5 in
Online Appendix 1, available on JAIR website, on page 8) is selected to be 10.
The work by Patrascu et al. (2002) uses L (sup norm) Bellman error as the performance
measurement in S YS A DMIN. Our technique, as described above, seeks to reduce mean Bellman
error more directly than L Bellman error. We report the L Bellman error, averaged over two
trials, in Figure 25. Also included in Figure 25 are the results shown in the work of Patrascu et al.
(2002). We select the best result shown there (from various algorithmic approaches) from the 3-legs
and cycle topologies shown in their paper. These correspond to the d-o-s setting for the cycle
topology and the d-x-n setting for the 3-legs topology, in the terminology of that paper.
Both topologies show that our algorithm reduces the L Bellman error more effectively per
feature as well as more effectively overall than the experiments previously reported in the work of
Patrascu et al. (2002). Both topologies also show Bellman error eventually diverges as AVI cannot
handle the complexity of the error function as dimensionality increases. Our algorithm can still
achieve low Bellman error by remembering and restoring the best-performing weighted feature set
once weakened performance is detected.
We note that our superior performance in reducing Bellman error could be due entirely or in
part to the use of AVI for weight training instead of approximate linear programming (ALP), the
method used by Patrascu et al. However, no such systematic superiority is known for AVI over ALP,
so these results suggest superior performance of the feature learning itself.
736

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

SysAdmin, 3-Legs Topology
33

10

12

9

Bellman Error

8
7
6
5
4
3
2
1
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Number of Features
3-legs, Learned

3-legs, Patrascu

SysAdmin, Cycle Topology
25 42

10
9

Bellman Error

8
7
6
5
4
3
2
1
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

Number of Features
Cycle, Learned
Cycle, Patrascu

Figure 25: L Bellman error for the S YS A DMIN domain (10 nodes) for two topologies. Values for
the results from the work of Patrascu et al. (2002) are taken from Figure 2 and 3 of the
work of Patrascu et al. (2002).

7.5 Demonstration of Generalization Across Problem Sizes
An asset of the relational feature representation presented in this paper is that learned relational
features are applicable to any problem size in the same domain. In section 2.4, we have discussed
737

fiW U & G IVAN

Target problem sizes
10  20 Tetris 15 blocks BW (15 box, 5 city) BX 30 nodes Tire 30 files Lifted-File
Intermediate problem sizes 10  10 Tetris 10 blocks BW (10 box, 5 city) BX 15 nodes Tire 10 files Lifted-File
Generalize from target size
Learn in intermediate size
Random walk

55
119
0.1

1 (171)
1 (170)
0 ()

1 (76)
1 (188)
0.97 (893)

0.88 (4)
0.89 (4)
0.29 (6)

1 (25)
1 (25)
1 (88)

Figure 26: Performance in intermediate-sized problems by generalization. We show here the
performance of value functions learned in target problem sizes when evaluated on
intermediate-sized problems, to demonstrate generalization between sizes. For comparison, also on intermediate-sized problems, we show the performance of value functions learned directly in the intermediate size as well as the performance of random
walk. Generalization results and intermediate size learning results are averages of two
trials. For T ETRIS, average accumulated rows erased are shown. For the goal-oriented
domains, success ratio and successful plan length (in parentheses) are shown for each
domain.

the modeling of a planning domain as an infinite set of MDPs, one for each problem instance in
the domain. Over this infinite set of MDPs, a feature vector plus a weight vector defines a single
value function that is well defined for every problem instance MDP. Here we discuss the question of whether our framework can find a single feature/weight vector combination that generalizes
good performance across problem sizes, i.e., for the value function V defined by such combination,
whether Greedy(V ) performs similarly well in different problem sizes.
Throughout Section 7, we have demonstrated the direct application of learned feature/weight
vectors to target problem sizes, (without retraining of weights)these results are shown in the
target-size lines in the result tables for each domain. In T ETRIS, B LOCKSWORLD, C ONJUNCTIVE B OXWORLD, T IREWORLD, and L IFTED -F ILEWORLD 3, the target-size lines demonstrate direct
successful generalization to target sizes even when the current problem sizes is significantly smaller.
(In the other domains, there was either no notion of problem size (S YS A DMIN), or insufficient planning progress to significantly increase problem size when learning from small problems (E XPLOD ING B LOCKSWORLD , Z ENOTRAVEL , and T OWERS OF H ANOI ).)
In this subsection, we consider the generalization from (larger) target sizes to selected intermediate sizes in these five domains. Specifically, we take the weight vectors and feature vectors
resulting from the end of the trials (i.e. with weight vector retrained at the target sizes), and apply
directly to selected intermediate problem sizes without weight retraining. For the trials that terminate before learning reaches the target problem sizes12 , we take the weights and features that result
in the best performing policy at the terminating problem sizes. The generalization results are shown
in Figure 26; for comparison, that table also shows the performance on the same intermediate-sized
problems of the value function that was learned directly at the that size, as well as the performance
of random walk on that size.
12. Note that one of the trials in T ETRIS terminates before reaching target size due to non-improving performance,
and the two trials in L IFTED -F ILEWORLD 3 terminate as target-size performance already reaches optimality before
learning reaches the target size. Still, although a few of the value functions were learned at smaller sizes than the
target size, all of the value functions evaluated for generalization were learned at significantly larger sizes than the
intermediate evaluation size.

738

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

In each domain shown the random walk result is much weaker than the generalization result,
showing the presence of generalization of learned value functions across problem sizes. In the
four goal-oriented planning domains, applying the value functions learned in the target sizes equals
the performance achieved by value functions learned directly in the intermediate sizes (with better
performance in C ONJUNCTIVE -B OXWORLD). In T ETRIS, however, the generalization result does
not match the result of learning in the intermediate size. We note that in some domains, solution
strategy is invariant with respect to the problem size (e.g. destroying incorrect towers to form correct
ones in B LOCKSWORLD). For some domains the best plan/strategy may change dramatically with
size. For example, in T ETRIS, a larger number of rows in the board allows strategies that temporary
stack uncompleted rows, but smaller number of rows favors strategies that complete rows as quickly
as possible. Thus one should not necessarily expect generalization between domain sizes in every
domainthis conclusion can be expected to hold whether we are considering the generalization of
value functions or of policies.
We have included a discussion of policy-based generalization in the related work section (Appendix A.4), focusing on our previous work on approximate policy iteration. However, we note that
policies that generalize between problems of different sizes are no more or less well defined than
value functions which generalize between such problems. In our previous API work, we defined
policies that select actions for states of any domain size; in this work we define value functions that
assign numeric values to states of any domain size. None of this work guarantees finding a good
or optimal policy or value function; as far as we know, some problems admit good compact value
functions, some admit good compact policies, some admit both, and some neither.

8. Discussion and Future Research
We have presented a general framework for automatically learning state-value functions by featurediscovery and gradient-based weight training. In this framework, we greedily select features from
a provided hypothesis space (which is a parameter of the method) to best correlate with Bellman
error features, and use AVI to find weights to associate with these features.
We have proposed two different candidate hypothesis spaces for features. One of these two
spaces is a relational one where features are first-order formulas with one free-variable, and a beamsearch process is used to greedily select a hypothesis. The other hypothesis space we have considered is a propositional feature representation where features are decision trees. For this hypothesis
space, we use a standard classification algorithm C4.5 (Quinlan, 1993) to build a feature that best
correlates with the sign of the statewise Bellman error, instead of using both the sign and magnitude.
The performance of our feature-learning planners is evaluated using both reward-oriented and
goal-oriented planning domains. We have demonstrated that our relational planner represents the
state-of-the-art for feature-discovering probabilistic planning techniques. Our propositional planner
does not perform as well as our relational planner, and cannot generalize between problem instances,
suggesting that knowledge representation is indeed critical to the success of feature-discovering
planners.
Although we present results for a propositional feature-learning approach and a relation featurelearning approach, the knowledge representation difference is not the only difference between the
approaches. Historically, our propositional approach was originally conceived as a reduction to
classification learning, and so does not attempt to capture the magnitude of the Bellman error during
739

fiW U & G IVAN

feature selection, but rather focuses only the sign of the error. In contrast, our relational approach
counts objects in order to match the magnitude of the Bellman error.
Because of this difference, we cannot attribute all of the performance differences between the
approaches to the knowledge representation choice. Some differences in performance could be due
to the choice to match sign only in the propositional feature selection. A possible future experiment
to identify the sources of performance variation would use a propositional representation involving
regression trees (Dzeroski, Todorovski, & Urbancic, 1995) to capture the magnitude of the error.
This representation might possibly perform somewhat better than the decision-tree representation
shown here, but of course would still not enable the generalization between sizes that the relational
feature learner exhibits.
Bellman-error reduction is of course just one source of guidance that might be followed in
feature discovery. During our experiments in the IPPC planning domains, we find that in many
domains the successful plan length achieved is much longer than optimal, as we discussed above in
Section 7.3.5. A possible remedy other than deploying search as in our previous work (Wu et al.,
2008) is to learn features targeting the dynamics inside plateaus, and use these features in decisionmaking when plateaus are encountered.

Acknowledgments
This material is based upon work supported in part by the National Science Foundation under Grant
No. 0905372.

Appendix A. Other Related Work
A.1 Other Feature Selection Approaches
A.1.1 F EATURE S ELECTION

VIA

C ONSTRUCTIVE F UNCTION A PPROXIMATION

Automatic feature extraction in sequential decision-making has been studied in the work of Utgoff
and Precup (1997), via constructive function approximation (Utgoff & Precup, 1998). This work can
be viewed as a forerunner of our more general framework, limited to propositional representations,
binary-valued features, and new features that are single-literal extensions of old features by conjunction. Also in the work of Rivest and Precup (2003) a variant of Cascade-Correlation (Fahlman
& Lebiere, 1990), a constructive neural network algorithm, is combined with TD-learning to learn
value functions in reinforcement learning. Cascade-Correlation incrementally adds hidden units
to multi-layered neural networks, where each hidden unit is essentially a feature built upon a set
of numerically-valued basic features. Our work provides a framework generalizing those prior efforts into a reduction to supervised learning, with explicit reliance on the Bellman error signal, so
that any feature hypothesis space and corresponding learner can be deployed. In particular, we
demonstrate our framework on both binary propositional features using C4.5 as the learner and rich
numeric-valued relational features using a greedy beam-search learner. Our work provides the first
evaluation of automatic feature extraction in benchmark planning domains from the several planning
competitions.
While the work of Utgoff and Precup (1997) implicitly relies on Bellman error, there is no
explicit construction of a Bellman error training set or discussion of selecting features to correlate
740

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

to Bellman error. For instance, their work focuses first on refining a current feature for which weight
updates are converging poorly (high variance in weight updates), whereas our work focuses first on
finding a feature that correlates to statewise Bellman error, regardless of whether that feature refines
any current feature. In addition, their work selects features online while the weights of the current
features are being adjusted, so there is no stationary target value function for which the Bellman
error is considered in the selection of the next new feature. In contrast, our work separates weight
training and new feature selection completely. (These differences are perhaps in part due to the
reinforcement learning setting used in Utgoff & Precup, 1997, as opposed to the planning setting of
our work.)
The selection of hidden unit feature in Cascade-Correlation (Fahlman & Lebiere, 1990) is based
on the covariance between feature values and errors of the output units. For output units that are
estimating a value function, with training data providing the Bellman update of that value function,
the output unit error is just Bellman error. Thus, the hidden units learned in the work of Rivest
and Precup (2003) are approximations of Bellman-error features just as our learned features are, although this is not made explicit in that work. By making the goal of capturing Bellman error explicit
here, we provide a general reduction that facilitates the use of any learning method to capture the
resulting feature-learning training sets. In particular, we are able to naturally demonstrate generalization across domain sizes in several large domains, using a relational feature learner. In contrast,
the single test domain in the work of Rivest and Precup (2003) has a small fixed size. Nonetheless,
that work is an important precursor to our approach.
A.1.2 F EATURE C ONSTRUCTION

VIA

S PECTRAL A NALYSIS

Feature-learning frameworks for value functions based upon spectral analysis of state-space connectivity are presented in the work of Mahadevan and Maggioni (2007) and Petrik (2007). In these
frameworks, features are eigenvectors of connectivity matrices constructed from random walk (Mahadevan & Maggioni, 2007) or eigenvectors of probabilistic transition matrices (Petrik, 2007). Such
features capture aspects of long-term problem behaviours, as opposed to the short-term behaviours
captured by the Bellman-error features. Bellman-error reduction requires iteration to capture longterm behaviors.
Reward functions are not considered at all during feature construction in the work of Mahadevan and Maggioni (2007); but in the work of Petrik (2007), reward functions are incorporated in the
learning of Krylov basis features, an variant of our Bellman error features (Parr et al., 2008), to complement the eigenvector features. However, even in Petriks framework, reward is only incorporated
in features used for policy evaluation rather than in the controlled environment we consider.
Essential to our work here is the use of machine learning in factored representations to handle
very large statespaces and to generalize between problems of different sizes. Both of these spectral
analysis frameworks are limited in this respect (at least at the current state of development). The approach by Petrik (2007) is presented only for explicit statespaces, while a factorization approach for
scaling up to large discrete domains is proposed in the work of Mahadevan and Maggioni (2007). In
that approach, features are learned for each dimension in the factorization, independent of the other
dimensions. We believe the assumption of independence between the dimensions is inappropriate
in many domains, including the benchmark planning domains considered in our work. The Mahadevan and Maggioni factorization approach also suffers the same drawbacks as our propositional
approach: the solution has to be recomputed for problems of different sizes in the same domain and
741

fiW U & G IVAN

so lacks the flexibility to generalize between problems of different sizes provided by our relational
approach.
A.2 Structural Model-based and Model-free Solution Methods for Markov Decision
Processes
A.2.1 R ELATIONAL R EINFORCEMENT L EARNING
In the work of Dzeroski et al. (2001), a relational reinforcement learning (RRL) system learns
logical regression trees to represent Q-functions of target MDPs. This work is related to ours since
both use relational representations and automatically construct functions that capture state value.
In addition to the Q-function trees, a policy tree learner is also introduced in the work of Dzeroski
et al. (2001) that finds policy trees based on the Q-function trees. We do not learn an explicit policy
description and instead use only greedy policies for evaluation.
The logical expressions in RRL regression trees are used as decision points in computing the
value function (or policy) rather than as numerically valued features for linear combination, as in our
method. Generalization across problem sizes is achieved by learning policy trees; the learned value
functions apply only to the training problem sizes. To date, the empirical results from this approach
have failed to demonstrate an ability to represent the value function usefully in familiar planning
benchmark domains. While good performance is shown for simplified goals such as placing a
particular block A onto a particular block B, the technique fails to capture the structure in richer
problems such as constructing particular arrangements of Blocksworld towers. RRL has not been
entered into any of the international planning competitions. These difficulties representing complex
relational value functions persist in extensions to the original RRL work (Driessens & Dzeroski,
2004; Driessens et al., 2006), where again only limited applicability is shown to benchmark planning
domains such as those used in our work.
A.2.2 P OLICY L EARNING VIA B OOSTING
In the work of Kersting and Driessens (2008), a boosting approach is introduced to incrementally
learn features to represent stochastic policies. This is a policy-iteration variant of our featurelearning framework, and clearly differs from our work as policy representations are learned instead
of value function representations. Using the regression tree learner TILDE (Blockeel & De Raedt,
1998), the feature learner demonstrated advantages against previous RRL work in the task of accomplishing on(A,B) in a 10-block problem. Applicability to a simple continuous domain (the corridor
world) is also demonstrated. As in the line of RRL work, only limited applicability to benchmark
planning domains is shown here. One probable source of this limited applicability is the model-free
reinforcement-learning setting where the system does not model the problem dynamics explicitly.
A.2.3 F ITTED VALUE I TERATION
Gordon (1995) has presented a method of value iteration called fitted value iteration that is suitable
for very large state spaces but does not require direct feature selection. Instead, the method relies on
a provided kernel function measuring similarity between states. Selection of this kernel function can
be viewed as a kind of feature selection, as the kernel identifies which state aspects are significant
in measuring similarity. To our knowledge, techniques from this class have not been applied to
large relational planning problems like those evaluated in this paper. We do note that selection of
742

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

a single relational kernel for all domains would measure state similarity in a domain-independent
manner and thus we believe such a kernel could not adapt to the individual domains the way our
work here does. Thus we would expect inferior performance from such an approach; however, this
remains to be investigated. Selection of domain-specific kernels for stochastic planning domains,
automatically, is also yet to be explored.
A.2.4 E XACT VALUE I TERATION IN F IRST- ORDER MDP S
Previous work has used lifted techniques to exactly solve first-order MDPs by reformulating exact
solution techniques from explicit MDPs, such as value iteration. Boutilier et al. (2001) and Holldobler and Skvortsova (2004) have independently used two different first-order languages (situation
calculus and fluent calculus, respectively) to define first-order MDPs. In both works, the Bellman update procedure in value iteration is reformulated using the respective calculus, resulting in
two first-order dynamic-programming methods: symbolic dynamic programming (SDP), and firstorder value iteration (FOVI). Only a simple boxworld example with human-assisted computation is
demonstrated in the SDP work, but the method serves as a basis for FOALP (Sanner & Boutilier,
2009), which replaces exact techniques with heuristic approximation in order to scale the techniques
to benchmark planning domains. Application of FOVI on planning domains is only demonstrated
on the colored blocksworld benchmark, and is limited to under 10 blocks (Holldobler, Karabaev, &
Skvortsova, 2006).
In the work of Kersting et al. (2004), constraint logic programming is used to define a relational
value iteration method. MDP components, such as states, actions, and rewards, are first abstracted
to form a Markov decision program, a lifted version of an MDP. A relational Bellman operation
(ReBel) is then used to define updates of Q-values and state values. Empirical study of the ReBel
approach has been limited to 10-step backups from single-predicate goals in the blocksworld and
logistics domains.
Exact techniques suffer from difficulty in representing the full complexity of the state-value
function for arbitrary goals in even mildly complex domains. These previous works serve to illustrate the central motivation for using problem features to compactly approximate the structure of a
complex value function, and thus to motivate the automatic extraction of features as studied in this
work.
A.3 Comparison to Inductive Logic Programming Algorithms
The problem of selecting a numeric function on relational states to match the Bellman-error training
set is a first-order regression problem for which there are some available systems described in the
Inductive logic programming (ILP) literature (Quinlan, 1996; Karalic & Bratko, 1997).
It is important to note that most ILP work has studied the learning of classifiers on relational
data (Muggleton, 1991), but here we are concerned with learning numeric functions on relational
data (such as our states). The latter problem is called first-order regression within the ILP literature, and has received less study than relational classification. Here, we choose to design our
own proof-of-concept relational learner for our experiments rather than use one of the few previous
systems. Separate work is needed to compare the utility of this relational learner with previous
regression systems; our purpose here is to demonstrate the utility of Bellman-error training data
for finding decision-theoretic value-function features. Our simple learner here suffices to create
state-of-the-art domain-independent planning via automatic feature selection.
743

fiW U & G IVAN

ILP classification systems often proceed either from general to specific, or from specific to
general, in seeking a concept to match the training data. For regression, however, there is no such
easy ordering of the numeric functions to be searched. We design instead a method that searches
a basic logical expression language from simple expressions to more complex expressions, seeking
good matches to the training data. In order to control the branching factor, while still allowing more
complex expressions to be considered, we heuristically build long expressions out of only those
short expressions that score best. In other words, we use a beam search of the space of expressions.
There are several heuristic aspects to our method. First, we define a heuristic set of basic expressions from which our search begins. Second, we define an heuristic method of combining
expressions to build more complex expressions. These two heuristic elements are designed so that
any logical formula without disjunction, with one free variable, can be built by repeated combination from the basic expressions. Finally, the assumption that high-scoring expressions will be built
only out of high-scoring parts is heuristic (and often not true). This critical heuristic assumption
makes it likely that our learner will often miss complex features that match the training data well.
There is no known method that guarantees tractably finding such features.
A.4 Approximate Policy Iteration for Relational Domains
Our planners use greedy policies derived from learned value functions. Alternatively, one can directly learn representations for policies. The policy-tree learning in the work of Dzeroski et al.
(2001), discussed previously in Appendix A.2.1, is one such example. Recent work uses a relational decision-list language to learn policies for small example problems that generalize well to
perform in large problems (Khardon, 1999; Martin & Geffner, 2004; Yoon et al., 2002). Due to
the inductive nature of this line of work, however, the selected policies occasionally contain severe
flaws, and no mechanism is provided for policy improvement. Such policy improvement is quite
challenging due to the astronomically large highly structured state spaces and the relational policy
language.
In the work of Fern et al. (2006), an approximate version of policy iteration addressing these
issues is presented. Starting from a base policy, approximate policy iteration iteratively generates
training data from an improved policy (using policy rollout) and then uses the learning algorithm in
the work of Yoon et al. (2002) to capture the improved policy in the compact decision-list language
again. Similar to our work, the learner in the work of Fern et al. (2006) aims to take a flawed
solution structure and improve its quality using conventional MDP techniques (in that case, finding
an improved policy with policy rollout) and machine learning. Unlike our work, in the work of Fern
et al. (2006) the improved policies are learned in the form of logical decision lists. Our work can be
viewed as complementary to this previous work in exploring the structured representation of value
functions where that work explored the structured representation of policies. Both approaches are
likely to be relevant and important to any long-term effort to solve structured stochastic decisionmaking problems.
We note that feature-based representation, as considered here and generally in the MDP literature, is used to represent value functions rather than policies. Compact representation of policies
can be done via value functions (with greedy execution) or more directly, for example, using decision lists. The previous API work just discussed uses a direct representation for policies, and never
uses any compact representation of value functions. Instead, sampling of value functions is used in
the policy evaluation step of policy iteration.
744

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

One can imagine a different and novel approach to API in which the compact feature-based
representation is used for value functions, with greedy execution as the policy representation. In
that approach, feature discovery similar to what we explore here for value iteration could be designed to assist the policy evaluation phase of the policy iteration. We leave further development
and evaluation of that idea to future work. We expect the two approaches to API, as well as our
current approach to value iteration, to have advantages and disadvantages that vary with the domain
in ways that have yet to be well understood. Some domains have natural compact direct policy
representations (run if you see a tarantula), whereas others are naturally compactly represented
via value functions (prefer restaurants with good review ratings). Research in this area must
eventually develop means to combine these compact representations effectively.
A.5 Automatic Extraction of Domain Knowledge
There is a substantial literature on learning to plan using methods other than direct representation
of a value function or a reactive policy, especially in the deterministic planning literature. These
techniques are related to ours in that both acquire domain specific knowledge via planning experience in the domain. Much of this literature targets control knowledge for particular search-based
planners (Estlin & Mooney, 1997; Kambhampati et al., 1996; Veloso et al., 1995), and is distant
from our approach in its focus on the particular planning technology used and on the limitation to
deterministic domains. It is unclear how to generalize this work to value-function construction or
probabilistic domains.
However, the broader learning-to-plan literature also contains work producing declarative
learned domain knowledge that could well be exploited during feature discovery for value function representation. In the work of Fox and Long (1998), a pre-processing module called TIM is
able to infer useful domain-specific and problem-specific structures, such as typing of objects and
state invariants, from descriptions of domain definition and initial states. While these invariants are
targeted in that work to improving the planning efficiency of a Graphplan based planner, we suggest
that future work could exploit these invariants in discovering features for value function representation. Similarly, in the work of Gerevini and Schubert (1998), DISCOPLAN infers state constraints
from the domain definition and initial state in order to improve the performance of SAT-based planners; again, these constraints could be incorporated in a feature search like our method but have not
to date.

Appendix B. Results and Discussions for Five Probabilistic Planning Competition
Domains
In Section 7.3, we have presented the results of our relational and propositional feature learners for
B LOCKSWORLD and C ONJUNCTIVE -B OXWORLD. Here we present the results of our relational
feature learner for the following five probabilistic planning competition domains: T IREWORLD,
Z ENOTRAVEL, E XPLODING B LOCKSWORLD, T OWERS OF H ANOI, and L IFTED -F ILEWORLD 3.
B.1 Tireworld
We use the T IREWORLD domain from the second IPPC. The agent needs to drive a vehicle through
a graph from the start node to the goal node. When moving from one node to an adjacent node,
the vehicle has a certain chance of suffering a flat tire (while still arriving at the adjacent node).
745

fiW U & G IVAN

Trial #1
# of features
0
1
2
3
3
3
4
4
5
Problem difficulty
4
4
4
4
5
6
6
9
9
Success ratio
0.52 0.81 0.84 0.86 0.86 0.84 0.88 0.85 0.86
Plan length
4
3
4
2
2
2
3
3
4
Accumulated time (Hr.) 0.3 3.1 12 17 18 18 19 21 22
0.17 0.53 0.81 0.83 0.83 0.82 0.90 0.91 0.91
Target size SR
Target size Slen.
5
4
9
5
4
4
6
6
6

5
10
0.86
4
23
0.91
6

Trial #2
# of features
0
1
2
3
3
3
4
4
4
Problem difficulty
4
4
4
4
5
6
6
10 20
0.52 0.81 0.85 0.86 0.93 0.81 0.89 0.85 0.86
Success ratio
Plan length
4
3
3
2
3
2
3
4
4
Accumulated time (Hr.) 0.5 3.7 6.9 10 11 11 12 14 18
Target size SR
0.19 0.49 0.80 0.82 0.91 0.62 0.92 0.91 0.90
Target size Slen.
7
3
9
4
5
2
5
5
6

4
30
0.88
5
24
0.88
6

5
20
0.91
5
29
0.92
5

5
30
0.91
5
36
0.92
6

Figure 27: T IREWORLD performance (averaged over 600 problems) for relational learner. We add
one feature per column until success ratio exceeds 0.85 and average successful plan
length is less than 4n, for n nodes, and then increase problem difficulty for the next
column. Plan lengths shown are successful trials only. Problem difficulties are measured
in number of nodes, with a target problem size of 30 nodes. Some columns are omitted
as discussed in Section 7.1.

The flat tire can be replaced by a spare tire, but only if there is such a spare tire present in the node
containing the vehicle, or if the vehicle is carrying a spare tire. The vehicle can pick up a spare
tire if it is not already carrying one and there is one present in the node containing the vehicle. The
default setting for the second-IPPC problem generator for this domain defines a problem distribution
that includes problems for which there is no policy achieving the goal with probability one. Such
problems create a tradeoff between goal-achievement probability and expected number of steps to
the goal. How strongly our planner favors goal achievement versus short trajectories to the goal is
determined by the choice of the discount factor made in Section 6.1.
We start with 4-node problems in our relational learner and increase from n nodes to n + 1
nodes whenever the success ratio exceeds 0.85 and the average successful plan length is better than
4n steps. The target problem size is 30 nodes. The results are shown in Figures 18 and 27.
In T IREWORLD, our relational learner again is able to find features that generalize well to large
problems. Our learner achieves a success ratio of about 0.9 on 30 node problems. It is unknown
whether any policy can exceed this success ratio on this problem distribution; however, neither
comparison planner, FOALP nor FF-Replan, finds a higher success-rate policy.
We note that some improvements in success rate in this domain will necessarily be associated
with increases in plan length because success-rate improvements may be due to path deviations to
acquire spare tires.
746

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Trial #1
# of features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size SR
Target size Slen.

0
3,1,1
0.79
253
0.75
0.06
916

1
3,1,1
0.8
255
1.7
0.08
1024

1
3,2,2
0.59
413
3.4
0.09
1064

2
3,2,2
0.52
440
7.1
0.09
1114

3
3,2,2
0.54
437
11
0.12
1050

4
3,2,2
0.55
450
15
0.11
1125

5
3,2,2
0.54
411
19
0.10
1111

6
3,2,2
0.52
440
25
0.08
1115

7
3,2,2
0.56
426
30
0.11
1061

8
3,2,2
0.53
428
36
0.08
1174

9
3,2,2
0.55
451
41
0.12
1195

Trial #2
# of features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size SR
Target size Slen.

0
3,1,1
0.77
262
1.3
0.05
814

1
3,1,1
0.79
254
2.3
0.10
1008

2
3,1,1
0.80
233
3.3
0.10
1007

2
3,2,2
0.55
391
5.3
0.09
1067

3
3,2,2
0.55
425
8.9
0.09
1088

4
3,2,2
0.50
415
13
0.08
1014

5
3,2,2
0.53
422
17
0.10
1078

6
3,2,2
0.12
0
22
0.02
0

7
3,2,2
0.12
0
29
0.02
0

8
3,2,2
0.12
0
36
0.02
0

9
3,2,2
0.10
0
43
0.01
0

Figure 28: Z ENOTRAVEL performance (averaged over 600 problems) for relational learner. The
problem difficulty shown in this table lists the numbers of cities, travelers, and aircraft,
with a target problem size of 10 cities, 2 travelers, and 2 aircraft. We add one feature
per column until success ratio exceeds 0.8, and then increase problem difficulty for the
next column. Plan lengths shown are successful trials only.

B.2 Zenotravel
We use the Z ENOTRAVEL domain from the second IPPC. The goal of this domain is to fly all travelers from their original location to their destination. Planes have (finite-range, discrete) fuel levels,
and need to be re-fuelled when the fuel level reaches zero to cont inue flying. Each available activity
(boarding, debarking, flying, zooming, or refueling) is divided into two stages, so that an activity
X is modelled as two actions start-X and finish-X. Each finish-X activity has a (high) probability
of doing nothing. Once a start action is taken, the corresponding finish action must be taken
(repeatedly) until it succeeds before any conflicting action can be started. This structure allows the
failure rates on the finish actions to simulate action costs (which were not used explicitly in the
problem representation for the competition). A plane can be moved between locations by flying or
zooming. Zooming uses more fuel than flying, but has a higher success probability.
We start with a problem difficulty of 3 cities, 1 traveler, and 1 aircraft using our relational
feature learner. Whenever the success ratio exceeds 0.8, we increase the number n of travelers and
aircraft by 1 if the number of cities is no less than 5n  2, and increase the number of cities by one
otherwise. The target problem size is 10 cities, 2 travelers, and 2 aircraft. Z ENOTRAVEL results for
the relational learner are shown in Figures 18 and 28.
747

fiW U & G IVAN

The relational learner is unable to find features that enable AVI to achieve the threshold success
rate (0.8) for 3 cities, 2 travelers, and 2 aircraft, although 9 relational features are learned. The trials
were stopped because no improvement in performance was achieved for several iterations of feature
addition. Using a broader search (W = 160, q = 3, and d = 3) we are able to find better features
and extend the solvable size to several cities with success rate 0.9 (not shown here as all results in
this paper use the same search parameters, but reported in Wu & Givan, 2007), but the runtime also
increases dramatically, to weeks. We believe the speed and effectiveness of the relational learning
needs to be improved to excel in this domain, and a likely major factor is improved knowledge
representation for features so that key concepts for Z ENOTRAVEL are easily represented.
Trial two in Figure 28 shows a striking event where adding a single new feature to a useful value
function results in a value function for which the greedy policy cannot find the goal at all, so that
the success ratio degrades dramatically immediately. Note that in this small problem size, about
ten percent of the problems are trivial, in that the initial state satisfies the goal. After the addition
of the sixth feature in trial two, these are the only problems the policy can solve. This reflects the
unreliability of our AVI weight-selection technique more than any aspect of our feature discovery:
after all, AVI is free to assign a zero weight to this new feature, but does not. Additional study of
the control of AVI and/or replacement of AVI by linear programming methods is indicated by this
phenomenon; however, this is a rare event in our extensive experiments.
B.3 Exploding Blocksworld
We also use E XPLODING B LOCKSWORLD from the second IPPC to evaluate our relational planner.
This domain differs from the normal Blocksworld largely due to the blocks having certain probability of being detonated when they are being put down, destroying objects beneath (but not the
detonating block). Blocks that are already detonated once will not be detonated again. The goal
state in this domain is described in tower fragments, where the fragments are not generally required
to be on the table. Destroyed objects cannot be picked up, and blocks cannot be put down on destroyed objects (but a destroyed object can still be part of the goal if the necessary relationships
were established before or just as it was destroyed).
We start with 3-block problems using our relational learner and increase from n blocks to n + 1
blocks whenever the success ratio exceeds 0.7. The target problem sizes are 5 and 10 blocks.
E XPLODING B LOCKSWORLD results for the relational learner are shown in Figures 19 and 29.
The results in E XPLODING B LOCKSWORLD are not good enough for the planner to increase the
difficulty beyond 4-block problems, and while the results show limited generalization to 5-block
problems, there is very little generalization to 10-block problems.
Our performance in this domain is quite weak. We believe this is due to the presence of many
dead-end states that are reachable with high probability. These are the states where either the table
or one of the blocks needed in the goal has been destroyed, before the object in question achieved the
required properties. Our planner can find meaningful and relevant features: the planner discovers
that it is undesirable to destroy the table, for instance. However, the resulting partial understanding of the domain cannot be augmented by random walk (as it is in some other domains such as
B LOCKSWORLD and C ONJUNCTIVE -B OXWORLD) to enable steady improvement in value, leading to the goal; random walk in this domain invariably lands the agent in a dead end. Very short
successful plan length, low probability of reaching the goal, and (not shown here) very high unsuccessful plan length (caused by wandering in a dead end region) suggest the need for new techniques
748

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Trial #1
# of features
0
1
2
3
Problem difficulty
3
3
3
3
Success ratio
0.56 0.58 0.56 0.63
Plan length
1
2
1
2
Accumulated time (Hr.) 0.6 1.4 2.2 3.1
0.12 0.12 0.14 0.22
Target size #1 SR
Target size #1 Slen.
3
3
3
5
Target size #2 SR
0
0
0 0.00
Target size #2 Slen.


 10

4
3
0.56
1
4.2
0.20
4
0.00
4

Trial #2
# of features
0
1
2
3
4
Problem difficulty
3
3
3
3
3
Success ratio
0.56 0.56 0.55 0.63 0.55
Plan length
1
2
1
2
1
Accumulated time (Hr.) 0.6 1.3 2.1 2.9 3.7
Target size #1 SR
0.14 0.15 0.12 0.18 0.17
4
3
4
6
4
Target size #1 Slen.
Target size #2 SR
0
0
0 0.01 0.00
Target size #2 Slen.


 19 18

5
6
7
7
8
9
3
3
3
4
4
4
0.68 0.62 0.71 0.4 0.45 0.43
1
2
2
4
5
4
5.9 8.7 11 12 20 28
0.31 0.16 0.34 0.33 0.31 0.31
6
9
6
6
5
5
0.03 0 0.02 0.03 0.02 0.02
24  19 26 23 22

5
3
0.75
2
4.6
0.33
6
0.02
26

5
4
0.45
4
5.3
0.31
6
0.01
27

6
4
0.45
5
14
0.32
6
0.01
15

7
4
0.43
5
22
0.31
6
0.02
21

8
4
0.42
4
31
0.28
5
0.01
15

10
4
0.44
5
38
0.29
5
0.01
15

9
4
0.36
4
39
0.30
5
0.01
18

Figure 29: E XPLODING B LOCKSWORLD performance (averaged over 600 problems) for relational
learner. Problem difficulties are measured in number of blocks. We add one feature per
column until success ratio exceeds 0.7, and then increase problem difficulty for the next
column. Plan lengths shown are successful trials only. Target problem size #1 has 5
blocks, and target problem size #2 has 10 blocks.

aimed at handling dead-end regions to handle this domain. These results demonstrate that our technique relies on random walk (or some other form of search) so that the learned features need not
completely describe the desired policy.
B.4 Towers of Hanoi
We use the domain T OWERS OF H ANOI from the first IPPC. In this probabilistic version of the wellknown problem, the agent can move one or two discs simultaneously, but there is a small probability
of going to a dead-end state on each move, and this probability depends on whether the largest disc
has been moved and which type of disc move (one or two at a time) is being used. We note that
there is only one planning problem in each problem size here.
It is important to note that 100% success rate is generally unachievable in this domain due to
the unavoidable dead-end states.
749

fiW U & G IVAN

Trial #1
# of features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size #1 SR
Target size #1 Slen.
Target size #2 SR
Target size #2 Slen.

0
1
1
2
3
3
4
5
6
7
8
8 20 38
2
2
3
3
3
4
4
4
4
4
4
5
5
5
0.70 0.75 0.11 0.44 0.73 0
0
0
0
0 0.51 0
0
0
4
2
43
26
4





4



0.0 0.0 0.1 0.2 0.3 0.4 0.5 1.1 1.2 2.1 2.2 2.3 18 53
0.07 0.15 0.01 0.08 0.03 0
0
0
0
0 0.52 0.53 0 0.43
13
9
90
95
37





4
4

4
0.00 0
0
0 0.00 0
0
0
0
0
0
0
0
0
11



107 









Trial #2
# of features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size #1 SR
Target size #1 Slen.
Target size #2 SR
Target size #2 Slen.

0
0
1
2
3
3
4
5
6
7
8
2
3
3
3
3
4
4
4
4
4
4
0.71 0.23 0.14 0.42 0.75 0
0
0
0
0 0.53
4
12
37
25
4





4
0.0 0.0 0.2 0.3 0.3 0.4 0.5 1.1 1.9 2.3 2.6
0.1 0.09 0.0 0.09 0.03 0
0
0
0
0 0.49
14
11 105 95
41





4
0.00 0.1
0
0 0.00 0
0
0
0
0
0
16
29


107 






8
5
0

2.7
0

0


20
5
0

6
0

0


38
5
0

16
0

0


Figure 30: T OWERS OF H ANOI performance (averaged over 600 problems) for relational learner.
We add one feature per column until success ratio exceeds 0.7n1 for n discs, and then
increase problem difficulty for the next column. Plan lengths shown are successful trials
only. Problem difficulties are measured in number of discs, with a target problem size #1
of 4 discs and size #2 of 5 discs. Some columns are omitted as discussed in Section 7.1.

We start with the 2-disc problem in our relational learner and increase the problem difficulty
from n discs to n + 1 discs whenever the success ratio exceeds 0.7n1 . The target problem sizes are
4 and 5 discs. T OWERS OF H ANOI results for the relational learner are shown in Figures 19 and 30.
The learner is clearly able to adapt to three- and four-disc problems, achieving around 50%
success rate on the four disc problem in both trials. The optimal solution for the four disc problem
has success rate 75%. This policy uses single disc moves until the large disc is moved and then
uses double disc moves. Policies that use only single disc moves or only double disc moves can
achieve success rates of 64% and 58%, respectively, on the four disc problem. The learned solution
occasionally moves a disc in a way that does not get closer to the goal, reducing its success.
Unfortunately, the trials show that an increasing number of new features are needed to adapt
to each larger problem size, and in our trials even 38 total features are not enough to adapt to the
five-disc problem. Thus, we do not know if this approach can extend even to five discs. Moreover,
the results indicate poor generalization between problem sizes.
We believe it is difficult for our learner (and for humans) to represent a good value function
across problem sizes. Humans deal with this domain by formulating a good recursive policy, not by
establishing any direct idea of the value of a state. Finding such a recursive policy automatically is
an interesting open research question outside the scope of this paper.
750

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

B.5 Lifted-Fileworld3
As described in Section 6.1, we use the domain L IFTED -F ILEWORLD 3, which is a straightforwardly
lifted form of F ILEWORLD from the first IPPC, restricted to three folders. To reach the goal of filing
all files, an action needs to be taken for each file to randomly determine which folder that file should
go into. There are actions for taking out a folder, putting a file in that folder, and returning the folder
to the cabinet. The goal is reached when all files are correctly filed in the targeted folders.
We note that both F ILEWORLD and L IFTED -F ILEWORLD 3 are very benign domains. There
are no reachable dead ends and very few non-optimal actions, each of which is directly reversible.
Random walk solves this domain with success rate one even for thirty files. The technical challenge
posed then is to minimize unnecessary steps so as to minimize plan length. The optimal policy
solves the n-file problem with between 2n + 1 and 2n + 5 steps, depending on the random file types
generated.
Rather than preset a plan-length threshold for increasing difficulty (as a function of n), here we
adopt a policy of increasing difficulty whenever the method fails to improve plan length by adding
features. Specifically, if the success ratio exceeds 0.9 and one feature is added without improving
plan length, we remove that feature and increase problem difficulty instead.13
We start with 1 file problems in our relational learner and increase from n files to n + 1 files
whenever the performance does not improve upon feature addition. The target problem size is 30
files. L IFTED -F ILEWORLD 3 results for the relational learner are shown in Figures 20 and 31.
The results show that our planner acquires an optimal policy for the 30-file target size problem
after learning four features, in each of the two trials. The results in this domain again reveal the
weakness of our AVI weight-selection method. Although four features are enough to define an optimal policy, as problem difficulty increases, AVI often fails to find the weight assignment producing
such a policy. When this happens, further feature addition can be triggered, as in trial 1. In this
domain, the results show that such extra features do not prevent AVI from finding good weights on
subsequent iterations, as the optimal policy is recovered again with the larger feature set. Nonetheless, here is another indication that improved performance may be available via work on alternative
weight-selection approaches, orthogonal to the topic of feature selection.

References
Bacchus, F., & Kabanza, F. (2000). Using temporal logics to express search control knowledge for
planning. Artificial Intelligence, 116, 123191.
Bertsekas, D. P. (1995). Dynamic Programming and Optimal Control. Athena Scientific.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Blockeel, H., & De Raedt, L. (1998). Top-down induction of first-order logical decision trees.
Artificial Intelligence, 101, 285297.
Bonet, B., & Givan, R. (2006). Non-deterministic planning track of the 2006 international planning
competition. Website. http://www.ldc.usb.ve/ bonet/ipc5/.
13. It is possible to specify a plan-length threshold function for triggering increase in difficulty in this domain, as we
have done in other domains. We find that this domain is quite sensitive to the choice of that function, and in the end
it must be chosen to trigger difficulty increase only when further feature addition is fruitless at the current difficulty.
So, we have directly implemented that automatic method for triggering difficulty increase.

751

fiW U & G IVAN

Trial #1
# of features
0 1 2
1 1 1
Problem difficulty
Success ratio
1 1 1
14 8 4
Plan length
Accumulated time (Hr.) 0.0 0.0 0.0
Target size SR
1 1 1
Target size Slen.
251 134 87

3
1
1
3
0.0
0


3
2
1
7
0.1
0


4
2
1
6
0.1
0


4
3
1
9
0.1
0


4
4
1
11
0.2
1.00
87

4
12
1
29
5.9
1
93

4
13
1
31
7.3
1
65

4
14
1
49
8.9
1
90

5
14
1
37
10
1
91

5
15
1
35
13
1
65

5
16
1
55
15
1
91

6
16
1
37
17
1
65

7
16
1
37
19
1
65

7
18
1
41
37
1
65

7
19
1
43
49
1
111

7
20
1
45
62
1
65

Trial #2
# of features
0 1 2
Problem difficulty
1 1 1
Success ratio
1 1 1
14 8 4
Plan length
Accumulated time (Hr.) 0.0 0.0 0.0
1 1 1
Target size SR
Target size Slen.
251 135 88

3
1
1
3
0.0
0


3
2
1
7
0.1
0


4
2
1
6
0.1
0


4
3
1
9
0.1
0


4
4 4 4 4
4
5 8 9 10
1
1 1 1 1
12 14 21 23 25
0.2 0.6 2.5 3.1 3.9
0.96 1 1 1 1
85 88 82 82 91

4
14
1
33
9.0
1
96

4
15
1
35
11
1
87

4
16
1
62
13
1
91

4
17
1
65
19
1
93

4
18
1
41
27
1
97

4
19
1
43
30
1
65

4
20
1
49
34
1
65

4
23
1
91
50
1
107

4
24
1
53
66
1
82

4
25
1
55
74
1
65

4
8
1
21
2.4
1.00
82

4
10
1
25
3.8
1
91

4
11
1
30
4.8
1
88

Figure 31: L IFTED -F ILEWORLD 3 performance (averaged over 600 problems) for relational
learner. We add one feature per column until success ratio exceeds 0.9 and adding one
extra feature does not improve plan length, and then increase problem difficulty for the
next column (after removing the extra feature). Plan lengths shown are successful trials
only. Problem difficulties are measured in number of files, with a target problem size of
30 files. Some columns are omitted as discussed in Section 7.1.

Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming for first-order MDPs.
In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence,
pp. 690700.
Chandra, A., & Merlin, P. (1977). Optimal implementation of conjunctive queries in relational data
bases. In Proceedings of the Ninth Annual ACM Symposium on Theory of Computing, pp.
7790.
Davis, R., & Lenat, D. (1982). Knowledge-Based Systems in Artificial Intelligence. McGraw-Hill,
New York.
Driessens, K., & Dzeroski, S. (2004). Integrating guidance into relational reinforcement learning.
Machine Learning, 57, 271304.
Driessens, K., Ramon, J., & Gartner, T. (2006). Graph kernels and gaussian processes for relational
reinforcement learning. Machine Learning, 64, 91119.
Dzeroski, S., DeRaedt, L., & Driessens, K. (2001). Relational reinforcement learning. Machine
Learning, 43, 752.
Dzeroski, S., Todorovski, L., & Urbancic, T. (1995). Handling real numbers in ILP: A step towards
better behavioural clones. In Proceedings of the Eighth European Conference on Machine
Learning, pp. 283286.
752

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Estlin, T. A., & Mooney, R. J. (1997). Learning to improve both efficiency and quality of planning.
In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, pp.
12271232.
Fahlman, S., & Lebiere, C. (1990). The cascade-correlation learning architecture. In Advances in
Neural Information Processing Systems 2, pp. 524  532.
Farias, V. F., & Van Roy, B. (2004). Tetris: A study of randomized constraint sampling. In Probabilistic and Randomized Methods for Design Under Uncertainty. Springer-Verlag.
Fawcett, T. (1996). Knowledge-based feature discovery for evaluation functions. Computational
Intelligence, 12(1), 4264.
Fern, A., Yoon, S., & Givan, R. (2004). Learning domain-specific control knowledge from random
walks. In Proceedings of the Fourteenth International Conference on Automated Planning
and Scheduling, pp. 191199.
Fern, A., Yoon, S., & Givan, R. (2006). Approximate policy iteration with a policy language bias:
Solving relational Markov decision processes. Journal of Artificial Intelligence Research, 25,
75118.
Fox, M., & Long, D. (1998). The automatic inference of state invariants in TIM. Journal of Artificial
Intelligence Research, 9, 367421.
Gerevini, A., & Schubert, L. (1998). Inferring state constraints for domain-independent planning.
In Proceedings of the Fifteenth National Conference on Artificial Intelligence, pp. 905912.
Gordon, G. (1995). Stable function approximation in dynamic programming. In Proceedings of the
Twelfth International Conference on Machine Learning, pp. 261268.
Gretton, C., & Thiebaux, S. (2004). Exploiting first-order regression in inductive policy selection.
In Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence, pp. 217
225.
Guestrin, C., Koller, D., & Parr, R. (2001). Max-norm projections for factored MDPs. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, pp. 673680.
Holldobler, S., Karabaev, E., & Skvortsova, O. (2006). FluCaP: A heuristic search planner for
first-order MDPs. Journal of Artificial Intelligence Research, 27, 419439.
Holldobler, S., & Skvortsova, O. (2004). A logic-based approach to dynamic programming. In
Proceedings of the Workshop on Learning and Planning in Markov ProcessesAdvances
and Challenges at the Nineteenth National Conference on Artificial Intelligence, pp. 3136.
Kakade, S. (2001). A natural policy gradient. In Advances in Neural Information Processing
Systems 14, pp. 15311538.
Kambhampati, S., Katukam, S., & Qu, Y. (1996). Failure driven dynamic search control for partial
order planners: An explanation based approach. Artificial Intelligence, 88(1-2), 253315.
Karalic, A., & Bratko, I. (1997). First order regression. Machine Learning, 26, 147176.
Keller, P., Mannor, S., & Precup, D. (2006). Automatic basis function construction for approximate dynamic programming and reinforcement learning. In Proceedings of the Twenty-Third
International Conference on Machine Learning, pp. 449456.
753

fiW U & G IVAN

Kersting, K., Van Otterlo, M., & De Raedt, L. (2004). Bellman goes relational. In Proceedings of
the Twenty-First International Conference on Machine Learning, pp. 465472.
Kersting, K., & Driessens, K. (2008). Non-parametric policy gradients: A unified treatment of
propositional and relational domains. In Proceedings of the Twenty-Fifth International Conference on Machine learning, pp. 456463.
Khardon, R. (1999). Learning action strategies for planning domains. Artificial Intelligence, 113(12), 125148.
Lagoudakis, M. G., Parr, R., & Littman, M. L. (2002). Least-squares methods in reinforcement
learning for control. In SETN 02: Proceedings of the Second Hellenic Conference on AI, pp.
249260.
Mahadevan, S., & Maggioni, M. (2007). Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes. Journal of Machine Learning
Research, 8, 21692231.
Martin, M., & Geffner, H. (2004). Learning generalized policies from planning examples using
concept languages. Applied Intelligence, 20, 919.
Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.
Muggleton, S. (1991). Inductive logic programming. New Generation Computing, 8(4), 295318.
Parr, R., Li, L., Taylor, G., Painter-Wakefield, C., & Littman, M. (2008). An analysis of linear
models, linear value-function approximation, and feature selection for reinforcement learning.
In Proceedings of the Twenty-Fifth International Conference on Machine Learning, pp. 752
759.
Parr, R., Painter-Wakefield, C., Li, L., & Littman, M. (2007). Analyzing feature generation for
value-function approximation. In Proceedings of the Twenty-Fourth International Conference
on Machine Learning, pp. 737744.
Patrascu, R., Poupart, P., Schuurmans, D., Boutilier, C., & Guestrin, C. (2002). Greedy linear valueapproximation for factored Markov decision processes. In Proceedings of the Eighteenth
National Conference on Artificial Intelligence, pp. 285291.
Petrik, M. (2007). An analysis of Laplacian methods for value function approximation in MDPs.
In Proceedings of the twentith International Joint Conference on Artificial Intelligence, pp.
25742579.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann.
Quinlan, J. R. (1996). Learning first-order definitions of functions. Journal of Artificial Intelligence
Research, 5, 139161.
Rivest, F., & Precup, D. (2003). Combining TD-learning with cascade-correlation networks. In
Proceedings of the Twentieth International Conference on Machine Learning, pp. 632639.
Sanner, S., & Boutilier, C. (2006). Practical linear value-approximation techniques for first-order
MDPs. In Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, pp. 409417.
Sanner, S., & Boutilier, C. (2009). Practical solution techniques for first-order MDPs. Artificial
Intelligence, 173(5-6), 748788.
754

fiAUTOMATIC I NDUCTION OF B ELLMAN -E RROR F EATURES FOR P ROBABILISTIC P LANNING

Singh, S., Jaakkola, T., Littman, M., & Szepesvari, C. (2000). Convergence results for single-step
on-policy reinforcement-learning algorithms. Machine Learning, 38(3), 287308.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning,
3, 944.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Szita, I., & Lorincz, A. (2006). Learning tetris using the noisy cross-entropy method. Neural
Computation, 18, 29362941.
Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications of the ACM,
38(3), 5868.
Tsitsiklis, J., & Roy, B. V. (1997). An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5), 674690.
Utgoff, P. E., & Precup, D. (1997). Relative value function approximation. Tech. rep., University
of Massachusetts, Department of Computer Science.
Utgoff, P. E., & Precup, D. (1998). Constuctive function approximation. In Motoda, & Liu (Eds.),
Feature Extraction, Construction, and Selection: A Data-Mining Perspective, pp. 219235.
Kluwer.
Veloso, M., Carbonell, J., Perez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integrating planning
and learning: The PRODIGY architecture. Journal of Experimental and Theoretical AI, 7(1),
81120.
Widrow, B., & Hoff, Jr, M. E. (1960). Adaptive switching circuits. IRE WESCON Convention
Record, 96104.
Williams, R. J., & Baird, L. C. (1993). Tight performance bounds on greedy policies based on
imperfect value functions. Tech. rep., Northeastern University.
Wu, J., & Givan, R. (2007). Discovering relational domain features for probabilistic planning.
In Proceedings of the Seventeenth International Conference on Automated Planning and
Scheduling, pp. 344351.
Wu, J., Kalyanam, R., & Givan, R. (2008). Stochastic enforced hill-climbing. In Proceedings of the
Eighteenth International Conference on Automated Planning and Scheduling, pp. 396403.
Wu, J., & Givan, R. (2005). Feature-discovering approximate value iteration methods. In Proceedings of the Symposium on Abstraction, Reformulation, and Approximation, pp. 321331.
Yoon, S., Fern, A., & Givan, R. (2002). Inductive policy selection for first-order MDPs. In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence, pp. 568576.
Yoon, S., Fern, A., & Givan, R. (2007). FF-Replan: A baseline for probabilistic planning. In Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling, pp. 352358.
Younes, H., Littman, M., Weissman, D., & Asmuth, J. (2005). The first probabilistic track of the
international planning competition. Journal of Artificial Intelligence Research, 24, 851887.

755

fiJournal of Artificial Intelligence Research 38 (2010) 189-221

Submitted 10/09; published 05/10

Constructing Reference Sets from Unstructured,
Ungrammatical Text
Matthew Michelson

mmichelson@fetch.com

Fetch Technologies
841 Apollo St, Ste 400
El Segundo, CA 90245 USA

Craig A. Knoblock

knoblock@isi.edu

University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Abstract
Vast amounts of text on the Web are unstructured and ungrammatical, such as classified ads, auction listings, forum postings, etc. We call such text posts. Despite their
inconsistent structure and lack of grammar, posts are full of useful information. This paper presents work on semi-automatically building tables of relational information, called
reference sets, by analyzing such posts directly. Reference sets can be applied to a number of tasks such as ontology maintenance and information extraction. Our reference-set
construction method starts with just a small amount of background knowledge, and constructs tuples representing the entities in the posts to form a reference set. We also describe
an extension to this approach for the special case where even this small amount of background knowledge is impossible to discover and use. To evaluate the utility of the machineconstructed reference sets, we compare them to manually constructed reference sets in the
context of reference-set-based information extraction. Our results show the reference sets
constructed by our method outperform manually constructed reference sets. We also compare the reference-set-based extraction approach using the machine-constructed reference
set to supervised extraction approaches using generic features. These results demonstrate
that using machine-constructed reference sets outperforms the supervised methods, even
though the supervised methods require training data.

1. Introduction
There are vast amounts of unstructured, ungrammatical data on the Web. Sources of such
data include classified ads, auction listings, and bulletin board/forum postings. We call this
unstructured, ungrammatical data posts. Figure 1 shows example posts, in this case a
set of classified ads for cars from the Craigslist site. We consider posts to be unstructured
because one cannot assume that the ordering of the terms will be consistent from post to
post. For instance, in Figure 1 we cannot assume that the car make (e.g., Audi) will precede
the car model (e.g., A4). Further, we consider posts ungrammatical because they would
not yield a useful grammatical parse (it would yield all nouns almost exclusively).
Although posts are unstructured and ungrammatical, they are full of useful information.
The posts of Figure 1 contain information about cars such as the different varieties of cars
c
2010
AI Access Foundation. All rights reserved.

fiMichelson & Knoblock

Figure 1: Example posts for cars

for sale, their price, etc. Therefore, analyzing posts as a corpus of information can yield a
number of insights.
In this paper we focus on analyzing posts to build reference sets, which are relational
tables of entities and their attributes. For instance, a reference set about cars would include
attributes such as a car make (e.g., Mercury), a car model (Sable), and a car trim (GS).
Reference sets can be used as background information for a number of tasks, and in particular, there has been recent interest on using reference sets as background knowledge for
information extraction (Michelson & Knoblock, 2007, 2008; Mansuri & Sarawagi, 2006; Cohen & Sarawagi, 2004). For completeness, we describe previous work on reference-set-based
information extraction in more detail in Section 2.
Our goal is to exploit the dense information content of posts to construct reference sets
with as little human intervention as possible. Posts are numerous, freely available, and
generally have a large number of useful attributes packed into short strings. Further, posts
sometimes cover more obscure entities than one might find in general Web text (another
possible corpus for mining a reference set). For instance, there are many categories of
items on auction sites that are specific and particular. Such items may not be mentioned
frequently in Web text outside of the posts. Therefore, posts are an attractive data source
for building reference sets.
We contrast our approach to manually constructing reference sets, which poses a number
of challenges. Most importantly, a user must discover the correct source(s) from which to
build the reference set. While reference sets can be built by extracting information from
semi-structured websites using wrappers (Muslea, Minton, & Knoblock, 2001; Crescenzi,
Mecca, & Merialdo, 2001), the challenge is not in parsing the data, but rather in choosing the
correct sources. There are often cases where it is difficult to find sources that enumerate the
reference set of interest, and therefore building the reference set itself becomes a challenging
problem of finding multiple sources to use, aggregating their records, and taking care of
duplicates. For instance, as we demonstrate in our results, it is difficult to find single sources
that enumerates very specific attributes such as the model numbers of laptops. There is a
Dell Inspiron 1720, an Inspiron E1405, and a 2650, just to name a few. This happens across
IBM laptops, Dell laptops, HP laptops, etc. To create a reference set that encompasses all
of the laptop model numbers would require finding and scraping a multitude of websites.
Yet, this is a useful attribute for extraction since the difference between an Inspiron 1720
and E1405 is important to the end user. However, posts are generally an aggregation of
the items people are interested in, so by using posts for the reference set one can discover
a more exhaustive list.
190

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Further, the data for reference sets is constantly changing. For example, again considering the laptop domain, new laptop model numbers are released every few months as new
hardware improvements are made. Therefore, even if a comprehensive laptop reference set
is built from multiple sources, it will become stale after just a few months, and the reference set will need to be updated each time any of the sources is updated with new laptops.
This creates an additional challenge for creating these reference sets from wrapped sources
because the sources must be constantly monitored for changes to assure that the reference
set reflects those changes.
Lastly, as we stated above, much of this work is motivated by using reference sets as
background knowledge to aid information extraction (Section 2). However, even if a user
can extract a reference set from Web sources using wrappers, there is no guarantee that the
reference set is appropriate given the data for extraction. Concretely, assume the goal is
to extract car information from Craigslist classifieds such as those in Figure 1, and assume
a user built a reference set about American cars using Web sources. This reference set is
not useful if the posts are for cars from outside the United States. We call this mismatch
between the reference set and the posts a coverage problem because the entities in the
posts are not covered by the records in the reference set.
Such coverage mismatches also happen when the data is constantly changing, such as
the laptops described above. In this case, if websites for new laptops are used to create
the reference set, there might be a mismatch for the used laptops that are generally sold
in classified ads (in fact, we demonstrate just this effect in our results). Therefore, there
are a number of advantages to building reference sets from posts instead of Web sources,
although of course these could be complementary processes.
This paper elaborates on our previous work on building reference sets from posts (Michelson & Knoblock, 2009). First, we describe our previous method in more detail. Second, this
previous method exploits a small amount of background knowledge to build the reference
set, and so in this paper we also significantly extend the algorithm to handle the special case
when no background knowledge about the source is known. We demonstrate that for even
a totally unknown source we can still construct a reference set that is useful for extraction.
The rest of the paper is as follows. Section 2 describes in more detail the process of
using reference sets for information extraction. Section 3 then describes our method for
constructing reference sets from the posts themselves. Section 4 presents our experiments
and analyzes the results. Section 5 discusses the related work, and Section 6 presents our
conclusions and future work.

2. Previous Work using Reference Sets for Information Extraction
We motivate our work on constructing reference sets by putting it in the context of discovering reference sets for use in information extraction. In particular, we focus on the
task of extracting information from posts using reference sets, as they have been shown
previously to aid this task quite a bit (Michelson & Knoblock, 2005, 2007). Information
extraction is the task of parsing the desired attributes from some text (posts in this case).
Using the example posts of Figure 1, after extraction, we would like the posts of Figure 1
to be separated into a make attribute, a model attribute, and a trim attribute. Specifically, the third post could be annotated with extractions such as MAKE={Honda} (which
191

fiMichelson & Knoblock

is implied!), MODEL={Accord}, and TRIM = {EX}. Once done, this would allow for
structured queries and integration over the posts, using the extracted attributes instead of
the full text of the post.
Given that posts do not conform to English grammar, Natural Language Processing approaches to extraction would not work on posts (Ciravegna, 2001). Further, since the posts
are unstructured, wrapper methods would not work either (Muslea et al., 2001; Crescenzi
et al., 2001). Instead, recent work showed that extraction from posts benefits from exploiting reference sets (Michelson & Knoblock, 2005, 2007).
Previous work on exploiting reference sets for extraction uses a two step process, shown
in Figure 2. In the first step, the algorithm discovers the best matches between each post
and the tuples in the reference set. Then, during the second step, the algorithm performs
information extraction by comparing the tokens in each post to the attributes of the matches
from the reference set. For example, the token Accord would be extracted as a model
since it would match the model attribute of the matching tuple from the reference set.
The key idea is that without labeled data, the system can perform information extraction
by relying solely on the information in the reference set, rather than some grammatical
or structural patterns. Recent work shows how to perform reference-set-based extraction
automatically (Michelson & Knoblock, 2007) or using techniques from machine learning
(Michelson & Knoblock, 2008).

Figure 2: Reference-set based extraction
This previous work on reference-set-based extraction assumes the user has a manually
constructed reference set to supply to the algorithm. This is a strong assumption and
has many potential difficulties as we stated above. Therefore, this helps motivate the
approach we take in this paper, building reference sets from posts, with little (or no) human
intervention. The method in this paper does not rely on any labeled data, and may only
require a small amount of background knowledge to produce clean and useful reference
sets. These machine-constructed reference sets can then be applied by plugging them into
a reference-set-based extraction framework for automatic extraction from posts.
192

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Within the context of information extraction, we emphasize that there are two main
benefits for using the posts themselves to construct the reference set. First, the challenge
of discovering sources from which to manually build the reference set is overcome since
the posts themselves can be used. Second, since the reference set is constructed from the
posts, it will have overlapping coverage by definition. As we show in our experiments, our
approach is especially useful for covering those attributes that are particularly difficult to
cover with manual reference sets, such as laptop model numbers, which constantly change
and are hard to enumerate.
We note that although we motivate this work on building reference-sets in the context of
information extraction, these machine constructed reference sets are useful for many other
tasks that require background knowledge. We can use them as structured entities to fill
in taxonomies, help construct ontologies for the Semantic Web, or simply see what tuples
of a reference set might exist in the posts, which can help in topic classification or query
formulation. We emphasize that the information extraction task as described here is one
application of these reference sets, and it is a useful proxy for comparing the utility of
various reference sets (as we do in our experiments). However, the work described in this
paper focuses on the specific task of reference set construction, rather than extraction which
is described elsewhere.

3. Reference Set Construction
The intuition for constructing reference sets from posts is that reference set tuples often
form hierarchy-like structures, which we call entity trees. Consider the simple reference
set of three cars in Figure 3. Each tuple in the reference set of cars has two attributes,
a make and a model. The model is a more specific version of the make (i.e., an Accord
is a Honda), and so the Accord attribute value is a child node to the Honda value. The
same can be said of the Civic value, and so we can generate an entity tree rooted on the
value Honda, with two children: Civic and Accord. The same argument applies to turn the
Ford tuple into its own entity tree. However, the entity trees rooted on Honda and Ford
are disjoint, since they do not share any ancestors. So a reference set is really a forest of
entity trees. So, once an algorithm constructs the set of entity trees, it can then traverse
them, outputting a tuple for each path from root to leaf, and the union of all of these tuples
creates a reference set. Therefore, to construct a reference set from posts, the goal is really
to build the forest of entity trees from the posts.
Our general approach to building reference sets from posts decomposes into two highlevel steps, shown in Figure 4. The first step breaks the posts into bigrams. As an example,
the post, Honda civic is cool yields the bigrams: {Honda, civic}, {civic, is}, etc.1 Creating
bigrams is a pre-processing step, since the second step of the algorithm takes the full set of
bigrams as input and generates the reference set.
The second step of our algorithm has three sub-components. First, the algorithm generates an initial set of entity trees based upon the bigrams (shown as step 2.1). Next, it
1. Note that our algorithm only considers ordered bigrams, rather than all combination of token pairs.
This is done for efficiency, since we measured an average post length of 8.6 tokens across thousands of
posts, which would generate more than 40,320 possible token pairs to check, per post, if all pairs are
considered.

193

fiMichelson & Knoblock

Figure 3: A reference set and its entity trees

iteratively adds new attributes to the entity trees that are general token attributes, which
we define below (step 2.2). Finally, the algorithm traverses each entity tree from root to
leaf, outputting each path to generate the reference set (step 2.3). We discuss our methods
for creating entity trees and discovering general tokens in detail in the following subsections.

Posts

Step 1

--------------------------------------------------------------------------

Construct Bi-Grams

Step 2.1:
Create Entity Trees
Step 2

Step 2.2:
Discover general tokens
Step 2.3:
Form Reference Set

Figure 4: Constructing reference sets from posts

194

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

3.1 Creating Entity Trees
As stated above, the first step in our approach converts the set of input posts into a set of
bigrams. To build entity trees from the bigrams we use a modified version of the Sanderson
and Croft (1999) heuristic for finding token subsumptions, with the notion that if token x
subsumes y, then y is a child node under x in the entity tree that contains x. We define
the rule for subsumption given terms x and y as:2
x subsumes y  P (x|y)  0.75 and P (y|x)  P (x|y)
As an example, consider the four posts shown in Table 1. If we consider the token
pair Honda with Civic we see that the conditional probability of Honda given Civic
(P (Honda|Civic)) is 1.0, while P (Civic|Honda) is 0.5 (since Honda occurs in four posts
while Honda occurs with Civic in two). Therefore, the subsumption rule fires, and Civic
becomes a child node to Honda. Similarly, Accord becomes a child of Honda, resulting in
the entity tree of Figure 3.
Table 1: Four example posts
Honda civic is cool
Look at cheap Honda civic
Honda accord rules
A Honda accord 4 u!

Since we only consider the ordered bigrams, building entity trees based on the Sanderson
and Croft heuristic requires the assumption that the order in the entity tree is reflected in
the posts. That is, users tend to order the attributes from more general (e.g., car makes)
to more specific (e.g., car models). Also, this ordering needs to hold in at least enough of
the posts to make the subsumption rule fire.
Yet, once our approach builds the entity trees, it constructs useful reference sets which
can be used effectively for extraction, as shown in our experiments. Therefore, our approach
leverages the little ordering it does find in the bigrams to build the reference set, which we
can then use to extract values from posts where the ordering assumption does not hold.
Further, given that our approach finds effective entity trees at all reflects the notion that
users do tend to order attributes from general to specific. If this were not the case, the
discovered entity trees would have little utility for extraction later.
The Sanderson and Croft heuristic above is defined for single tokens x and y, yet not all
attribute values are unigrams. Therefore, to handle bigrams, we add the constraint that if
x subsumes y and y subsumes x, we merge x and y into a single node (attribute value). For
instance, given attribute values Crown and Victoria if Crown subsumes Victoria
and Victoria subsumes Crown we merge them into a single value Crown Victoria
(which is subsumed by the common parent Ford). We note this bigram was actually
merged using our approach. To extend this to n-grams, one simply checks all token pairs
against each other for subsumption.
2. Note, we require terms x and y to co-occur at least once for this rule to be considered.

195

fiMichelson & Knoblock

In the reference sets constructed by our algorithm for our experiments, 5.49% of the
attribute values are n-grams containing more than a single token, which is a large enough
percentage to validate including the merge heuristic in our approach. Therefore, our technique is not solely applicable to the case where reference set values are single tokens. However, there are cases where our merging heuristic does not perform well, which we discuss
in more detail in our discussion in Section 4.3.
The directionality imposed by the Sanderson and Croft heuristic is important for our
approach. Specifically, the Sanderson and Croft heuristic uses conditional probabilities
which imposes a directionality on the relationship between the tokens and allows them to
form subsumption relationships. That is, if x subsumes y, then x is a parent of y based
on the directionality imposed by the conditional probability. We need this directionality
because the entity trees must have a relative subsumption ordering to align the trees into
a reference set. More specifically, using the example from Figure 3, when building the
reference set from the entity trees, we know that the columns of the reference set will be
aligned based on their positions in the entity tree. So, the roots of entity tree form the
leftmost column of the reference set table (Honda, Ford), their children (Honda, Accord,
Focus) form the next right column, etc., down to the leaves of the entity trees. So, when
tracing the paths from root to leaf, we can be sure that the tuples outputted from disjoint
trees will still align correctly in the columns of the reference set because of the ordering
imposed by the directionality.
This is in contrast to other probabilistic measures of term co-occurrences such as Pointwise Mutual Information (PMI).3 Specifically, PMI is symmetric and therefore while it
provides a strong measure of term relationships, it is unclear how to order terms based on
this measure, and therefore, even if one were to use such a symmetric relationship to find
reference set tuples, it is unclear how to align disjoint tuples (e.g., Honda and Ford tuples)
based on this metric since it does not impose a relative ordering of the attributes. For this
reason, we require an asymmetric measure.
3.2 Discovering General Tokens
Once we have constructed the initial entity trees, we then iterate over the posts to discover
possible terms that occur across trees. Specifically, subsumption is determined by the
conditional probabilities of the tokens, but when the second token is much more frequent
than the first token, the conditional probability will be low and not yield a subsumption.
This occurs when the attribute value appears across entity trees (reference set tuples).
Since the second term occurs more frequently than the first across tuples, we call this the
general token effect.
An example of this general token effect can be seen with the trim attribute of cars. For
instance, consider the posts in Table 2 which show the general token effect for the trim
value of LE. These posts show the LE trim occurring with a Corolla model, a Camry
model, a Grand AM model, and a Pathfinder. Since LE happens across many different
posts in many varying bigrams, we call it a general token, and its conditional probability
will never be high enough for subsumption. Thus it is never inserted into an entity tree.
3. For terms x and y, PMI is defined as P M I(x, y) = log

196

p(x,y)
p(x)p(y)

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Table 2: Posts with a general trim token: LE
2001 Nissan Pathfinder LE - $15000
Toyota Camry LE 2003 - 20000 $15250
98 Corolla LE 145K, Remote entry w/ alarm, $4600
1995 Pontiac Grand AM LE (Northern NJ) $700
To compensate for this general token peculiarity, we iteratively run our subsumption
process, where for each iteration after the first, we consider the conditional probability using
a set of the first tokens from bigrams that all share the common second token in the bigram.
Note, however, this set only contains bigrams whose first token is already a node in an entity
tree, otherwise the algorithm may be counting noise in the conditional probability. This is
the reason we can only run this after the first iteration. The algorithm iterates until it no
longer generates new attribute values.
To make this clear, consider again the LE trim. This is a possible cross-tree attribute because it would occur in the disjoint subtrees rooted on CAMRY, COROLLA, and
PATHFINDER. By iterating, the algorithm considers the following conditional probability
for subsumption, assuming the models of Camry, Corolla and Pathfinder have already been
discovered:
P ({CAM RY  COROLLA  P AT HF IN DER}|LE)
Now, if this conditional probability fits the heuristic for subsumption, then LE is added
as a child to the nodes CAMRY, COROLLA and PATHFINDER in their own respective
entity trees. Iterating is important for this step since our method only tests a general token
for subsumption if the terms it occurs with as a bigram are already in some entity trees.
So, if LE occurred with other tokens, but none of them were in an entity tree already, our
method ignores them as noise. Our approach must iterate since new general tokens are only
considered if they occur with an attribute in an entity tree, so our approach may discover
a new general token and add it to an entity tree, which in turn allows the approach to
discover another new general token. In our experiments we describe the effects of iterating
versus not. The final step of our approach from Figure 4 then turns the entity trees into
a reference set by tracing down the paths of the trees, outputting the nodes as columns in
the reference set tuples.
However, blindly applying the above subsumption method can lead to noisy entity trees.
A common occurrence in auction listings, for instance, is to include the term Free Shipping
or Free Handling. If such phrases occur frequently enough in the posts, the subsumption
rule will fire, creating an entity tree rooted on Free with the children Shipping and Handling.
Clearly this is a noisy tree and when used it would introduce noisy extractions. Therefore,
the following two subsections describe different approaches to handling noise in the process
of constructing entity trees.
3.3 Seed-Based Reference Set Construction
Our first approach to dealing with noise exploits a small amount of background knowledge,
called seeds, to focus the construction of the entity trees. Specifically, we use the method
of Figure 4 to build entity trees, with the added constraint that each entity tree must be
197

fiMichelson & Knoblock

rooted on a given seed value. If we only gave Honda as a seed, then only one entity tree
rooted on Honda would be constructed. Even if other entity trees are discovered, they are
discarded. It is easy to discover an exhaustive list of seeds on the Web and including too
many seeds is not a issue as our algorithm simply removes any entity tree that consists
solely of a root from the constructed reference set (i.e., a singleton set).
One of the key intuitions behind our approach is that the set of root nodes for entity
trees are generally much easier to discover online than nodes farther down the trees. For
instance, if we consider laptops, the manufacturers of laptops (the roots) are fairly easy to
discover and enumerate. However, as one traverses farther down the entity trees, say to the
model numbers, it becomes hard to find this information. Yet, just this small set of seeds
is enough to improve the process of reference set construction substantially, as we show
in the results where we compare against reference sets constructed without the seed-based
constraint. Also, importantly, the attributes farther down the tree change more over time
(new model numbers are released often), while the seeds infrequently change (there are few
new computer manufacturers). So, when using the reference set for information extraction,
coverage becomes less of an issue when only considering the roots versus all of the attributes
in a reference set tuple.
In this manner we construct a reference set directly from the posts, using the seeds to
constrain the noise that is generated in the final reference set. Table 3 describes our full
algorithm for constructing entity trees using seeds, which are then turned into a reference
set by outputting a tuple for each path from root to leaf in each tree.
3.4 Locking-Based Reference Set Construction
The seed-based method handles noise by exploiting a small amount of background knowledge. Here we describe a second approach to dealing with the noise for the case where the
seeds are impossible to discover or too costly to find and use.
This approach revolves around a locking mechanism. The seeds in the seed-based
method constrain the possible entity trees by limiting the attributes that can become roots
of the entity trees. This, in turn, leads to cleaner and more useful reference sets. Therefore,
when lacking seeds, the goal should be to introduce a constraining mechanism that prevents
noise from being introduced into the entity trees. Our approach is to lock the levels of
the entity trees at certain points, such that after locking no new attribute values can be
introduced at that level for any of the entity trees.
Since many of the attributes we discover are specifications of more general attributes
(such as car models specify makes), there is a point at which although we may be discovering new values for the more specific attributes (car models), we have saturated what we
can discover for the parent attribute (car makes). Further, once we saturate the parent
attributes, if the algorithm does introduce new values representing new parents, they are
likely noise. So, the algorithm should lock the parent attribute at the saturation point,
and only allow the discovery new attributes that are below the level of locked attribute in
the hierarchies.
Consider the example shown in Figure 5. At iteration t the algorithm has discovered
two entity trees, one rooted with the car make Ford and one rooted with the car make
Honda. Each of these makes also has a few models associated with them (their children).
198

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Table 3: Entity tree construction using seeds
MineReferenceSet(Posts P , Seeds S)
/* First, break posts into bigrams */
Bigrams B  MakeBigrams(P )
/* Build the entity trees rooted on the seeds */
AddedNodes N  {}
For each {x, y}  B
/* {x, y} is a bigram */
If x  S
/* check x is a seed */
If x subsumes y
y is child of x in entity tree
N N y
/*Find all childrens children, and their children, etc.*/
While N is not null
For each {s, t}  B
where s  N
N N -s
If s subsumes t
t is child of s in tree
N N t
/* Iterate to discover general token nodes */
/* Start with unique nodes already in the entity trees */
AllEntityNodes  UniqueList(All Entity Trees)
/* Keep iterating while find new terms */
FoundNewTerms  true
While FoundNewTerms is true
FoundNewTerms  false
/* Consider terms {p0 , . . ., pn } in entity the trees
that all form bigrams with non-entity tree term q */

For each ( {p0 , . . ., pn }, q) s.t. {p0 , . . ., pn }  AllEntityNodes
S
If {p0 , . . ., pn } subsumes q
/* consider P ( pi |q) */
q becomes child of each pi in trees
AllEntityNodes  AllEntityNodes  q
FoundNewTerms  true

The bottom of the figure shows some future time (iteration t+y), at which point the system
decides to lock the make attribute, but not the model and trim. Therefore, the algorithm
can still add car models (as it does with the Taurus model to the Ford make) and car trims
(as with the LX trim for the Civic model). However, since the make attribute is locked, no
more make attributes are allowed, and therefore the hierarchy that would have been rooted
on the make Brand with model New (which is noise) is not allowed. This is why it is
shown on the right as being crossed out.
In this manner, the locking acts like a pre-pruner of attribute values in a similar spirit
to the seeds. The intent is that the algorithm will lock the attributes at the right time so
as to minimize the number of noisy attributes that may be introduced at later iterations.
This works because noise is often introduced as the algorithm iterates, but not in the
199

fiMichelson & Knoblock

Figure 5: Locking car attributes
beginning. In our example, instead of post-pruning away the hierarchy rooted on Brand,
the algorithm instead prevented it from being added by locking the make attribute.
The assumption is that for deeper levels of the tree, more posts will need to be examined
because the attributes represented in these levels of the trees are rarer. However, the higher
the level of the tree, the more common the attribute value and therefore the fewer posts will
need to be examined. This is an assumption that we will exploit to justify locking in a top
down manner. That is, we can lock the roots of the entity trees before their children because
we assume that we will need to see fewer posts to discover the roots than the children. This
assumption carries down the tree, such that we lock the leaves last because we need to see
the most posts to generate the leaves. So, the locking approach terminates if all attributes
(i.e., all levels of the entity trees) become locked, since then there is no point in processing
any more posts after that.
Based on our assumption, we model the locking mechanism as a requesting service. Our
assumption is that we need to look at fewer posts for higher up in the trees, and that by
doing so we help eliminate noise (justified in our experiments by comparing locking to not
locking). So rather than process all of the posts at once, we instead have the algorithm
request batches of posts and process them, building the entity trees using the approach of
Figure 4. After receiving the next batch of posts, the algorithm builds up the new set of
entity trees, compares them to the previously discovered ones, and the algorithm decides
whether to lock a level. If it does not lock, it requests more posts to examine. It keeps
requesting posts to examine until all levels of the entity trees are locked. We note, however,
that as each batch comes in for processing, it is combined with all of the previously seen
posts. This way, there is a gradual build up of the number of posts the system examines for
a given level, and so in this sense the algorithm iterates because it examines the previous
posts each time it receives a new batch.
Therefore, we can leverage the notion that the locking process requires continuous requests for more posts from the user until it locks all attributes. Essentially, at each request,
the machine compares what it can discover using the given posts to what it can discover
200

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

using the previous set of given posts (note that the newly requested set supersedes the
previous set). For example, the algorithm may start by examining 100 posts to build a
reference set. Then, if not all levels in the entity trees are locked, the algorithm requests
100 more posts and now analyzes these 200 posts to build the reference set. The algorithm
compares what it can discover by using the first 100 posts to what it can discover using the
combined 200 posts. If the algorithm thinks it cannot discover more useful attributes from
the 200 posts than the 100 posts, then it locks certain attributes (Again, note that the 100
posts are a subset of the 200 posts).
To do this comparison for locking, the algorithm compares the entropies for generating
a given attribute (e.g., member of a given level in an entity tree), based upon the likelihood
of a token being labeled as the given attribute. So, in the cars domain we would calculate
the entropy of a token being labeled a make, model, or trim (note that these label names
are given post-hoc, and the machine simply regards them as attribute1 , attribute2 , etc.).
For clarity, if we define pmake (x) as the probability of arbitrary token x being labeled as a
make attribute (e.g., falling into the level of an entity tree associated with make), then we
define the entropy, H(make) as:4
X

H(make) = 

pmake (x)  log(pmake (x))

xtokens

So, for any given attribute A, we use pA (x) and define H(A) as:
H(A) = 

X

pA (x)  log(pA (x))

xtokens

The entropy of labeling a particular attribute can be interpreted as the uncertainty
of labeling tokens with the given attribute. So, as we see more and more posts, if the
entropy does not change from seeing 100 posts to seeing 200 posts, then the uncertainty
in labeling that attribute is steady so there is no need to keep mining for that attribute
in more posts. However, we cannot directly compare the entropies across runs, since the
underlying amounts of data are different. So, we use normalized entropy instead. That is,
for attribute A (e.g., make), given N posts, we define the normalized entropy H(A)N :
H(A)N =

H(A)
logN

Although the entropy provides a measure of uncertainly for token labels, it does not
provide a sufficient comparison between runs over varying numbers of posts. To provide
an explicit comparison, we analyze the percent difference between the normalized entropies
across runs. For instance, using attribute A, we would compare the entropies from runs
across 100 posts and 200 posts (defined as H(A)100 and H(A)200 respectively), by computing
the percent difference between them:
P D(H(A)100 , H(A)200 ) =

1
2

|H(A)100  H(A)200 |
(H(A)100 + H(A)200 )

If this value is a minimum (ideally 0), we know we can lock that attribute at 100 posts,
since using the 200 posts did not yield more information as the entropies are essentially
4. This assumes we have built a reference set from which to compute the probabilities pmake (x).

201

fiMichelson & Knoblock

the same (i.e., the uncertainty is steady). So, the algorithm locks an attribute when it
finds the minimum percent difference between entropies across runs for that attribute. This
minimum is found using a greedy search over the previously calculated PD values. Table 4
summarizes the above technique for locking the attributes when mining a reference set.
Table 4: Locking attributes
LockingAttributes(Attributes A, Posts Pi , Posts Pj ,
ReferenceSet RSi , ReferenceSet RSj , LockedAttributes L)
/* Pi is the first i set of posts */
/* Pj is the next set of j posts, such that Pi  Pj */
/* RSi and RSj are the reference sets for Pi and Pj
respectively and are used with the posts to compute likelihoods pA (x) */

/* L is the set of previously locked attributes */
for each attribute a  A
if a is not locked
/* Compute entropies H(a)i and H(a)j as defined above */
H(a)i  Compute entropy for a given Pi and RSi
H(a)j  Compute entropy for a given Pj and RSj
If PD(H(a)i , H(a)j ) is a minimum over previous values AND
Parent(a)  L /* Parent of a is locked */
Lock(a)
LLa
return L

There are two small items to note. First, we add a heuristic that an attribute may only
lock if its parent attribute is already locked, to prevent children from locking before parents.
Second, although the above discussion repeatedly mentions the machine requesting more
posts from a user, note that the algorithm can automatically request its own posts from
the sources using technology such as RSS feeds, alleviating any human involvement beyond
supplying the URL to the source.
Therefore, by using the above technique to lock the attributes as we go, even if we
are generating new children attribute values, the parents will be locked, which acts as an
automatic pruning method. Further, we now know when to stop asking for posts, so we
have a stopping criteria for the number of posts that we need to run the mining algorithm.
This stopping criteria is important since the assumption is that the algorithm has exhausted
possible reference set membership only when it locks all levels. Note, this assumption could
be violated if the algorithm is not supplied with enough posts (since it would never lock all
levels).
Table 5 ties all of the aspects together describing our Iterative Locking Algorithm
(ILA) for mining reference set tuples from posts without seeds. Essentially, the approach is
the same as the seed-based method: for each batch of posts, the algorithm constructs the
entity trees by scanning the data and iterating for the general tokens. The key difference
is that ILA does not constrain the roots of the entity trees to be seeds, but instead uses
locking to block attributes from being added to the trees. Further, this locking approach
processes batches of posts at a time, so the flow is different in that it requests a batch of
posts, builds/refines the entity trees, tests the trees for the locking conditions, and then
proceeds with the next batch of posts.
202

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Table 5: ILA method for mining reference sets
MineReferenceSet(Posts, x, y)
# x is the number of posts to start with
# y is the number of posts to add each iteration
Posts Px  GetPosts(x)
ReferenceSet RSx  BuildReferenceSet(Px )
/* Algorithm from Table 3, except
ignore constraint that tree roots are seeds */

finished  false
LockedAttributes L  {}
while(finished is false)
x  x+y
Posts Py  GetPosts(x)
ReferenceSet RSy  BuildReferenceSet(Py , L)
/* Same as in Table 3, except
only add nodes to unlocked levels of the trees */

Attributes A  GetAttributes(RSx )
/* GetAttributes returns columns of found reference set */
L  L  LockingAttributes(A, Px , Py , RSx , RSy , L)
If |L| equals |A| /* Same size, so all attributes are locked */
finished  true
Px  Py
RSx  RSy

4. Experiments
The goal of this research is to to construct reference sets for tasks such as information
extraction from posts. However, directly comparing the reference sets constructed in various
ways has a number of problems. First, it is unclear how to directly compare reference sets
quantitatively. This problem has been noted elsewhere as well in the context of comparing
hierarchies (e.g., Bast, Dupret, Majumdar, & Piwowarski, 2006). For instance, there are not
clear measures on how to define similarities when comparing hierarchies. Second, without
context, it is hard to judge the reference sets. That is, one reference set may be huge and
comprehensive, which is high utility for a task such as ontology construction, but might be
of little use for extraction due to coverage mismatches. Another reference set may be quite
noisy (therefore bad for ontology construction), but actually be preferred for extraction
since its coverage is better.
Therefore, instead of measuring the goodness of reference sets directly, we instead put
them in the context of an information extraction task by using them for reference-set-based
extraction from posts. Then we can compare the extraction results and use these results as
a proxy for the utility of the reference set. The assumption is that the extraction results
reflect the reference sets utility since a very noisy reference set will lead to poor extraction,
as would a reference set with poor coverage to the posts.
In the following subsections we test our two different approaches to constructing reference sets: our seed-based approach and our locking based approach (ILA).
4.1 Experiments: Seed-based Approach
For our first experiment, we test the effectiveness of using the seed-based approach for
building reference sets. For this experiment, we compare our seed-based approach to both
full, manually constructed reference sets extracted from online sources (which we call the
203

fiMichelson & Knoblock

manual approach) and to a version of the seed-based approach that does not constrain
the entity trees to be rooted on seed values (called no seeds). By comparing the seedbased method to a manually constructed reference set, we can test for coverage issues
that stem from collecting reference sets online (versus the posts themselves). Further, this
comparison allows us to analyze the trade-off between the high cost in building a manual
reference set (versus the low cost of finding seeds) and the gains in accuracy by using the
manual reference set. Using no seeds tests the effectiveness of constraining the entity
tree roots to the seeds. That is, we expect that the no seeds method generates a much
noisier reference set (leading to poor extraction) than the seed-based method that requires
the constraint.
Therefore, for this first experiment, we use the manual and no seed approaches
as baselines to compare the seed-based reference set. For our procedure, for each of our
experimental data sets, we build three different reference sets (one manual, one based on
seeds, and one without the seeds) and then pass the reference sets to a system that can
exploit them to perform automatic extraction (Michelson & Knoblock, 2007). We then
compare the extraction results using the standard metrics of recall, precision, and F1 measure. Since the only difference for the extraction algorithm is the reference set provided,
the extraction results serve as a proxy for the quality of the reference set both in terms of
how well it overlaps with the posts (the coverage) and how clean it is (since noise leads to
poor extractions), which is what we want to test.
The goal of our extraction task is to extract the values for given attributes. For instance, using Figure 1, we should extract the model={Accord} and trim={EX}. However,
our approach to constructing reference sets does not supply these attribute names. Our
method discovers attribute values such as Honda and Accord, but it internally labels
their associated attribute names as attribute0 and attribute1 , instead of Make and Model.
Therefore, to clarify the results we manually label the attribute names as given by the manually constructed reference sets. We do not feel this is much of a hindrance in our method.
If a user can find a set of seeds, the user should also be able to find appropriate attributes
names. In fact, it is significantly more challenging to discover the attribute values than just
finding their names.
4.1.1 Data for Seed-based Experiments
We used three real-world data sets as our experimental data. The first set contains classified ads for used cars for sale from the classified site Craigslist.org. Of these, we labeled 1,000 posts to test the extraction of the make (e.g., Honda), model (e.g., Civic),
and trim (e.g., DX) attributes. The second set consists of classified ads for used laptops
from Craigslist.org as well. Again we labeled 1,000 posts for extracting the manufacturer
(e.g., IBM), model (e.g., Thinkpad), and model number (e.g., T41). Our last data set contains posts about skis for sale on eBay. We labeled 1,000 of the posts for extraction of
the brand (e.g., Rossignol), the model (e.g., Bandit), and the model specification (e.g., B3,
called the spec). The data is summarized in Table 6.
We need full, manually constructed reference sets for comparison. For the Cars domain,
we collected 27,000 car tuples by pulling data from the Edmunds.com car buying site for
cars and combining it with data from a classic car site, SuperLambAuto.com. For the
204

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Table 6: Three experimental data sets
Name
Cars
Laptops
Skis

Source
Craigslist
Craigslist
eBay

Attributes
make, model, trim
manufacturer, model, model num.
brand, model, model spec.

Num. Posts
2,568
2,921
4,981

Laptops domain, we scraped 279 laptops off of the online retailer Overstock.com. Lastly,
for the Skis domain, we built 213 ski tuples from the skis.com website and cleaned them to
remove certain stop words.5
The seeds for our seed-based method also came from freely available sources. For the
car domain, the seeds consist of 102 car makes, again from Edmunds. The laptop seeds are
40 manufacturers, culled from Wikipedia, and the ski seeds are 18 ski brands pulled from
Skis.com.
4.1.2 Results for Seed-based Experiments
Table 7 shows the field-level extraction results for each attribute in each domain, comparing
the three methods.6 Again, the manual method uses the full reference set constructed
from online sources (shown in parentheses in the table), the no seed method is the method
without seed-based constraint, and our full technique is called seed-based.
Table 8 summarizes the results, showing the number of attributes where our seed-based
method outperformed the other techniques in terms of F1 -measure. It also shows the number
of attributes where the seed-based technique is within 5% of the other methods F1 -measure
(including the attributes where the seed-based method outperforms the other method). An
F1 -measure within 5% is a competitive result.
The results show that our seed-based method builds a cleaner reference set than the
fully automatic approach that ignores the seeds since the seed-based approach outperforms
the No seed approach on every single attribute. The seed-based method builds a cleaner,
more effective reference set, and that leads to more effective extraction.
The results also support the notion that using the posts themselves to generate a reference set yields reference sets with better coverage than those constructed manually from a
single source. Not only does the seed-based method outperform the manual reference sets
on a majority of attributes (5/9), the seed-based methods reference set better represents
the most specific attributes (ski model, ski model spec., laptop model, and laptop model
num.), which are those attributes that are likely to cause coverage problems. For these attributes, only 53.15% of the unique attribute values in the seed-based reference set exist in
the manually constructed reference set. Therefore, the coverage is quite different, and given
that the seed-based approach performs better on these attributes, its coverage is better.
For example, it is important to note that Overstock sells new computers, while the
laptops for sale on Craigslist are generally used, older laptops. So, while there is a match
5. The posts and reference sets for our experiments are available at www.mmichelson.com.
6. Field-level results are strict in that an extraction is correct only if all the tokens that should be labeled
are, and no extra tokens are labeled.

205

fiMichelson & Knoblock

Table 7: Extraction results comparing seed-based method
Make
Manual (Edmunds)
No seed
Seed-based
Model
Manual (Edmunds)
No seed
Seed-based
Trim
Manual (Edmunds)
No seed
Seed-based

Cars
Recall
92.51
79.31
89.15
Recall
79.50
64.77
73.50
Recall
38.01
23.45
31.08

Prec.
99.52
84.30
99.50
Prec.
91.86
84.62
93.08
Prec.
63.69
54.10
50.59

F1 -Meas.
95.68
81.73
94.04
F1 -Meas.
85.23
73.38
82.14
F1 -Meas.
47.61
32.71
38.50

Brand
Manual (Skis.com)
No seed
Seed-based
Model
Manual (Skis.com)
No seed
Seed-based
Model Spec.
Manual (Skis.com)
No seed
Seed-based

Skis
Recall
83.62
60.59
80.30
Recall
28.12
51.86
62.07
Recall
18.28
42.37
50.97

Prec.
87.05
55.03
96.02
Prec.
67.95
51.25
78.79
Prec.
59.44
63.55
64.93

F1 -Meas.
85.30
57.68
87.46
F1 -Meas.
39.77
51.55
69.44
F1 -Meas.
27.96
50.84
57.11

Laptops
Manufacturer
Recall
Manual (Overstock) 84.41
No seed
51.27
Seed-based
73.01
Model
Recall
Manual (Overstock) 43.19
No seed
54.47
Seed-based
70.42
Model Num.
Recall
Manual (Overstock) 6.05
No seed
25.58
Seed-based
34.42

Prec.
95.59
46.22
95.12
Prec.
80.88
49.52
77.34
Prec.
78.79
77.46
86.05

F1 -Meas.
89.65
48.61
82.61
F1 -Meas.
56.31
51.87
73.72
F1 -Meas.
11.23
38.46
49.17

Table 8: Summary results of seed-based method versus others
Outperforms
Within 5%

Seed vs. No seed
9/9
9/9

Seed vs. Manual
5/9
7/9

between the manufacturers (since the laptop manufacturers dont change quickly), even if
the used laptops are six months older than the new ones for sale there will be a mismatch
between some models and for many of the model numbers. This coverage mismatch using
the manual reference sets is very clear for the laptop model numbers and ski model
specifications. Both of these are attributes that change quite frequently over time as new
models come out. This is in contrast to ski brands and laptop manufacturers (our seeds)
which change much less frequently and so can be enumerated with less of a concern toward
coverage. We note that we chose Wikipedia because of its comprehensive list of laptop
manufacturers, but Wikipedia enumerates far fewer models and model numbers than the
Overstock reference set and so would be a worse choice for a manual reference set.
206

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Also, we note that our seed-based technique is competitive on 7/9 attributes when
compared to the full, manually constructed reference sets. Yet, the number of seeds is
drastically smaller than the number of tuples manually constructed for those reference sets.
So, even though we are starting with a much tinier set of knowledge, we still retain much of
the benefit of that knowledge by leveraging it, rather than having to explicitly enumerate
all of the tuple attributes ourselves. This is important as it is much easier to find just the
seeds. Therefore, the cost (in manual terms) is much lower for the seed-based approach, but
it does not give up accuracy performance as compared to the manual approach for building
reference sets.
The one attribute where the manual reference set drastically outperforms our seed-based
method is the trim attribute for cars, where the difference is roughly 9% in F1 -measure.
This is mostly due to the fact that we use field level results, and when the seed-based
technique constructs the trim attribute it sometimes leaves out certain attribute tokens. For
instance, consider the example where the extracted trim should be 4 Dr DX. Here, the
seed-based technique only includes DX as the reference set tuples attribute. Meanwhile,
the manually constructed reference set contains all possible tokens since it is scraped from a
comprehensive source (its attribute value is 4 Dr DX 4WD). So, although our seed-based
technique finds the DX token and labels it correctly as a trim, it misses the 4 Dr part of
the extraction, so the whole extraction is counted as incorrect using field level results.
Overall, the machine-constructed reference sets yield better extraction results for attributes that occur higher up in the entity trees. The extraction results are best for the
attributes at the roots of the entity trees, then the attributes that are children of the roots,
and then the leaves of the entity trees. This is largely a discovery issue. The set of possible
attribute values generally grows as one traverses the tree (e.g., there are more values for
laptop models than manufacturers, and more model numbers than models, etc.). Therefore,
the algorithm needs to see more and more posts to overcome the lack of evidence to discover
the attributes farther down the entity trees. So, seeing many more posts should generate
enough evidence to compensate for this issue.
One limitation of our seed-based technique versus the manual construction of reference
sets has to do with the inclusion of certain attributes. Surprisingly, there is not enough
repetition in the posts for discovering the years of the cars as attributes. This is due
to various factors including the variety of year representations (all four digits, two digits,
strange spellings, etc.) and the placement in the posts of the years (since we consider
bigrams for subsumptions). However, the full manual reference set does contain years and
as such it can extract this attribute, while the seed-based method cannot. Therefore, since
our seed-based method was unable to learn how to fit the year attributes into the entity
tree, it fails to extract it, and so we remove this attribute from our extraction results (as
its results are essentially 0). Nonetheless, although a manual reference set may include
an attribute that cannot be discovered automatically, the reference set might have terrible
coverage with the posts, limiting its utility. So, we feel it is important to deal with coverage,
as our seed-based method does.

207

fiMichelson & Knoblock

4.1.3 Results for Iterating for General Tokens
We also tested the effect of iterating to capture general tokens versus simply stopping after
the first pass over the posts. We again use extraction results as the proxy for comparing these
reference sets. In this case, the assumption is that the iterative method will capture more
general tokens and therefore construct a fuller reference set that yields better extraction
results than when the algorithm stops after the first pass over the posts. Table 9 shows the
comparable F1 -measure results for extraction comparing the Single Pass to the Iterative
approach.

Table 9: Comparing iterating to not iterating (seed-based method)
Cars
Make
Model
Trim

Single Pass (F1 )
93.29
78.42
16.44

Iterative (F1 )
94.04
82.14
38.50

Single Pass (F1 )
81.77
73.52
49.50

Iterative (F1 )
82.61
73.72
49.17

Single Pass (F1 )
87.30
67.03
42.75

Iterative (F1 )
87.46
69.44
57.11

Laptops
Manufacturer
Model
Model Num.
Skis
Brand
Model
Model Spec.

As expected, the iterative technique yields better results. The iterative method outperforms the single-pass approach on every attribute except for one (Laptop Model Numbers,
where the F1 -measure decreases by -0.33%). Interestingly, when iterating, the algorithm
improves more for attributes at deeper levels of the entity trees. That is, when comparing
the extraction results for the roots of the entity trees, there is almost no difference. However, at the second level of the entity trees there is a slight improvement (around +3.5%
F1 -measure for Car Models and +2.5% for Ski Models), and comparing the leaves of the
entity trees there is the most improvement (+22% increase for Car Trims, and +15% increase for Ski Model Specifications). So, it seems that the iterating does in fact capture
more general tokens which can be used for successful extractions, and it seems that general
tokens seem to occur more farther down the trees. We note that the algorithm only iterates
a few times for each domain, and since it almost always helps extraction (sometimes quite
dramatically), it is a useful component of the seed-based approach to constructing reference
sets.
208

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

4.1.4 Entity Tree Analysis
Although extraction experiments serve as the best metric for the actual utility of the seedbased reference sets, we also ran experiments on the generated entity trees themselves. We
examined whether attribute values are consistent in their placement in the entity trees (the
column homogeny). For instance, given known car models such as Civic we measure
if the model values are mostly placed as car model attributes (second level in the tree)
or misplaced as car trims (third level). However, measuring this directly without domain
expertise is difficult. Instead, we compare the attribute values in the seed-based reference
set to those in the manually constructed reference sets, and for those values that match, we
measure if they are the for same attribute (i.e., their columns match) or not. This yields
a measure of the column homogeny for the seed-based reference set, based on the manual
reference set, which is assumed to be clean. However, it is an approximate measure because
not all values in the seed-based reference set match those in the manual reference set, since
they differ in coverage (see the previous results).
Nonetheless, the results of this approximate measurement indicate a good level of homogeny amongst the seed-based attributes. For skis, only 1.7% of the found attribute values
are in the wrong column, while for cars 2.9% of the values are in the wrong columns. Both
the skis and cars had a common error of placing the specific attribute (model spec or car
trim) one spot higher in the entity tree than it should have been. However, this approximation is misleading for laptops. In the laptops domain, we found perfect column homogeny
using this measure, but this is because we can only measure the column homogeny for
attributes that match in both the seed-based and manual reference sets. Yet, there were
obvious column homogeny errors, such as cpu speeds being placed as model numbers. Since
these did not match into the manual reference set, they were ignored by our homogeny
measuring experiment. Given that we have enough domain expertise, we did a manual
calculation for this set and determined that 8.09% of the tuples in the seed-based set have
a cpu speed or other variant as a model number which is incorrect. However, even at 8%
this is a good result for homogeny.
4.1.5 Comparison Against Supervised Methods
Although our experiments are meant to test the utility of the reference set (not the extraction algorithm itself), one aspect to analyze is reducing the burden on the user for
the seed-based approach. That is, by comparing our seed-based reference set (which uses
an automatic extractor) to a supervised machine learning approach to extraction, we can
examine the amount of labeled data needed for the supervised system to garner similar
results. This yields insight into the gain in terms of labor cost because generating the set
of seeds is much less costly than labeling data to train a classifier, and we would like to
examine how much labeled data is needed to get comparable extraction results.
To analyze the user effort, we compare the seed-based results above to a common machine learning approach for extraction: Conditional Random Fields (CRF) (Lafferty, McCallum, & Pereira, 2001). For this we used MALLET (McCallum, 2002) to implement two
different CRF extractors. One, called CRF-Orth, uses orthographic features of the tokens for extraction, such as capitalization, number containment, etc. The second extractor,
CRF-Win, uses the same orthographic features and also considers a two-word sliding win209

fiMichelson & Knoblock

dow around a token as a feature. These extractors are built to reflect common features and
techniques for CRF-based extraction. Then, we perform 10-fold cross validation for each
extractor (varying the amount of the data for training) noting that each fold is independent,
and we compare the average field-level extraction results using the supervised approaches
to the automatic approach using the seed-based reference set.
Our first experiment compares the seed-based method to each CRF using just 10% of
the data for training. This experiment compares the seed-based method to a supervised
method using a small enough amount of labeled data to reflect real-world cost constraints.
If the seed-based method outperforms the CRFs on a majority of attributes, then it is an
effective method for extraction that is also cost effective since it outperforms the supervised
methods when they are supplied with a realistic amount of training data. Table 10 shows
the summary extraction results for this experiment, similar in format to those of Table 8
where we show the number of times the seed-based method outperforms and is competitive
with another method.
Table 10: Summary results comparing the seed-based method to CRFs (10% training data)
Outperforms
Within 5%

Seed vs. CRF-Win
7/9
9/9

Seed vs. CRF-Orth
6/9
7/9

Table 10 shows that our seed-based method outperforms the two CRF extractors on
a majority of the attributes, even though the cost in creating a seed list is significantly
less than the cost of labeling the data and creating features for training our specific CRFs.
Further, the table shows that a technique that relies heavily on structure, such as CRF-Win,
performs worse on extraction from posts as compared to other methods.
One aspect to analyze based on these results is the amount of training data needed
for the supervised methods to outperform the seed-based methods. For this analysis, we
trained the CRFs with 10%, 30%, and 50% of the data, and we note at what amount of
training data the CRFs outperform the seed-based method. In certain cases, even with
50% of the data used for training, the CRF did not outperform the seed-based method (we
denote this amount as >50% in the table). Table 11 shows the amount of data needed for
each CRF to oupteform the seed-based method, broken down by each specific attribute for
each domain.
From these results we see that there are quite a few cases when either 50% of the data (or
even more) is needed by the supervised approaches to outperform the seed-based method
(3/9 for CRF-Orth and 5/9, a majority, for CRF-Win). Therefore, there is a large gain
in terms of cost when using the seeds, as so much labeled data would be needed for the
supervised systems. In fact, there are attributes where both extractors never outperformed
the seed-based approach, even when given 50% of the data for training. Further, we note
that only once, for CRF-Orth in the Skis domain, is less than 50% of the data required
to outperform the seed-based method for all of the attributes. In this case, using 30% of
the data for training is sufficient to outperform the seed-based method on all attribute, but
labeling 30% of the data in this domain is far costlier than generating the list of 18 ski
210

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Table 11: Amount of training data to outperform seed-based approach
Cars
Make
Model
Trim

CRF-Orth
>50%
50%
10%

CRF-Win
>50%
>50%
30%

CRF-Orth
50%
30%
10%

CRF-Win
>50%
>50%
10%

CRF-Orth
30%
30%
10%

CRF-Win
50%
30%
30%

Laptops
Manufacturer
Model
Model Num.
Skis
Brand
Model
Model Spec.

brands used as seeds. Lastly, in no domain would just 10% of the data allow the CRFs
to outperform the seed-based method on all attributes. Therefore, the seed-based method
provides a much less costly approach to this extraction task based on the amount of training
data needed.
There is one case where both CRF methods perform well with a relatively small amount
of training data, the laptop model number. This attribute fits well for the orthographic
features (since it usually has capital letters and numbers in it), and the extractor can
generalize well with just 10% training data for extracting it. This argues that certain
attributes may benefit from being extracted using generalized features (such as those from
CRF-Orth) versus reference-set membership (as our method does). Therefore, we plan to
investigate hybrid methods that combine the best of both the CRF-based extractors and
reference-set based extractors.
The above set of experiments demonstrate the utility of our seed-based approach. Comparing the seed-based method to a baseline of a manually constructed reference set, we
showed that the seed-based method outperforms the manual reference set on a majority of
attributes (especially those for which coverage is difficult), while requiring less user effort
to construct. Therefore, the reference set constructed using the seeds has better coverage
than the manually constructed reference sets and can be used effectively, even though it is
cheaper to construct. Further, we showed that the constraint that our seed-based method
uses (constraining the roots to be seeds) does indeed have a strong impact on the results,
versus not using this constraint. Lastly, we compared using a seed-based approach to a
supervised machine learning approach to judge the comparable amount of training data
needed to outperform the seed-based method, and we show that indeed, it takes quite a bit
of training data as compared to the small number of seeds.
211

fiMichelson & Knoblock

4.2 Experiments: Locking Approach
Our next set of experiments analyze our locking-based approach to building reference sets.
As stated above, the locking based technique is appropriate for the special case where seeds
are too costly or impossible to find. Therefore, our locking based approach is an alternative
to the no seed method described in our previous experiments. The experimental procedure
for these experiments is the exactly same as above. We use the same data sets and compare
the reference sets constructed in different ways (seed-based, no seed, and locked) by
passing them to the same extraction mechanism and comparing the extraction results as a
proxy for the reference sets utility.
For the locking algorithm, we must specify the number of posts to add at each locking
iteration. We set this value to 200, which is large enough to limit the total possible number
of iterations (versus say, 20), but also small enough to allow the algorithm to converge
before seeing all of the posts (versus, say 1,000, which may be too coarse). Table 12 shows
the total number of posts (and iterations) required for the locking algorithm to lock all
levels of the entity trees, and therefore converge, returning the constructed reference set.
We note that for all domains, the total number of posts required for the locking algorithm
to converge was less than the total number of posts for that domain. Table 12 also shows
the total number of posts for each domain.

Table 12: Locking convergence results
Domain
Cars
Laptops
Skis

Total Posts Required
for Locking
2,000
2,400
4,400

Total Possible Posts

Iterations

2,568
2,921
4,981

10
12
22

Table 13 shows the comparative field-level extraction results for the different domains,
using the reference sets generated by the different methods (seed-based, no seed, and
locked).
Based upon the results of Table 13, we see that locking is a good alternative to simply
not locking at all (no seeds). For the cars and the skis domains, there is only one difference
in F1 -measure that is statistically significant using a two-tailed test at 95% confidence (the
Car Trim attribute). However, in the laptops domain, the locking method outperforms no
seed on all of the attributes, largely due to its increase in precision, which is a direct result
of locking noise out of the entity trees. Therefore, it is a good strategy to attempt the
locking approach (versus the no seed approach) since based upon these results, in the
worst case, it will have a minimal negative effect on the generated reference set (evidenced
by the Car Trim attribute), but it can yield significantly cleaner reference set in the best
case (as shown by the laptop results). Further, the locking algorithm converged (i.e., was
able to lock all levels before it saw all posts) in all domains, so it was able to produce
reference sets without the burden of requiring additional posts.
212

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Table 13: Extraction results comparing locking method
Make
Locked
No seed
Seed-based
Model
Locked
No seed
Seed-based
Trim
Locked
No seed
Seed-based

Cars
Recall
79.64
79.31
89.15
Recall
65.30
64.77
73.50
Recall
19.54
23.45
31.08

Prec.
84.46
84.30
99.50
Prec.
83.24
84.62
93.08
Prec.
52.13
54.10
50.59

F1 -Meas.
81.84
81.73
94.04
F1 -Meas.
72.22
73.38
82.14
F1 -Meas.
28.28
32.71
38.50

Brand
Locked
No seed
Seed-based
Model
Locked
No seed
Seed-based
Model Spec.
Locked
No seed
Seed-based

Skis
Recall
60.84
60.59
80.30
Recall
51.33
51.86
62.07
Recall
39.14
42.37
50.97

Prec.
55.26
55.03
96.02
Prec.
48.93
51.25
78.79
Prec.
56.35
63.55
64.93

F1 -Meas.
57.91
57.68
87.46
F1 -Meas.
50.10
51.55
69.44
F1 -Meas.
46.29
50.84
57.11

Manufacturer
Locked
No seed
Seed-based
Model
Locked
No seed
Seed-based
Model Num.
Locked
No seed
Seed-based

Laptops
Recall Prec.
60.42 74.35
51.27 46.22
73.01 95.12
Recall Prec.
61.91 76.18
54.47 49.52
70.42 77.34
Recall Prec.
27.91 81.08
25.58 77.46
34.42 86.05

F1 -Meas.
66.67
48.61
82.61
F1 -Meas.
68.31
51.87
73.72
F1 -Meas.
41.52
38.46
49.17

4.3 Experiments: Assumptions for Constructing Reference Sets
In this section we examine some of the assumptions made on the data from which we are
able to construct the reference sets using the seed-based method (or the locking method,
if necessary). First, we mentioned previously that we do not need to assume that the
constructed reference set is filled only with single token attributes, as roughly 6% of the
attributes in the constructed reference sets are n-grams using the seed-based method. We
note, however, that other researchers have also discussed the difficulty in discovering n-grams
for concept hierarchies from text. For example, some previous work discards concepts from
the topic hierarchy if they consist of multiple terms (Bast et al., 2006).
Despite the fact that our merging heuristic can yield n-gram values for attributes, there
are cases where the heuristic breaks down. Specifically, it does not perform well in domains
where most of the attribute values are multiple tokens, and those tokens occur with each
other in various frequencies. For example, consider if users are selling items for sports teams,
such as a San Diego Chargers helmet, and a San Francisco 49rs t-shirt. In this case,
the Diego and Francisco tokens will be subsumed by San creating an unnecessary
entity tree, rather than joining both of the terms together. This is the main failure of
our merging approach: the algorithm will force a subsumption relationship because the
213

fiMichelson & Knoblock

merging rule fails to fire. However, we note that our merging heuristic never causes the
subsumption rule not to fire, rather it fails to do so itself and therefore creates errant entity
trees. Therefore, since it never destroys information for an entity tree (e.g., never causes
subsumption to fail) we consider two approaches to improving the merge heuristic. First, we
could make the approach more aggressive in merging. Second, we could perform some posthoc analysis of the constructed entity trees to fix these errors. For instance, one could use
outside information, such as an ontology or corpus statistics, to determine the likelihood
of Franscisco being a child of San versus being merged into a single term and therefore
cleaning up the hierarchy to account for failed merging. Improving our merging method is
a future research challenge.
While it might seem that it is necessary for the attributes to always appear adjacent
to one another in the posts (e.g., make model trim with no other tokens between them),
this is not the case. In fact, on average, across the three domains we measured 0.115 tokens
in between each attribute in the posts. This is significantly larger than 0, and it implies
that indeed there are tokens between the attributes in a number of the posts. Further, it
might seem that we must always see the correct order in the posts (e.g., never see a Car
trim attribute before a Car make attribute) in order to successfully construct the reference
sets. However, again, due to the unstructured nature of the posts, we see that this is not
the case. In fact, we only see perfect ordering for less than half of the posts in the three
domains (45.2%), where perfect ordering is the full ordering defined by the entity tree (e.g.,
a Car post with a make attribute a model attribute and a trim attribute, in that order). In
fact, in cars 0.61% of the posts have the Car trim attribute coming before any of the other
attributes, which is a strange and random ordering. Therefore, we do not need to assume
that the ordering is always correct or that there will not be extraneous tokens in between
the attributes. And we do not need to assume that each attribute is a single token. This is
because there are enough cases where the ordering does reflect the entity tree, and words
are seen together, such that the subsumption heuristic can fire to produce a correct entity
tree.
Yet, there are a certain number of assumptions we do make in order to construct a reference set. First, we must assume that there is some reasonable hierarchical structure for the
entity trees (e.g., Car makes are more general than Car models). Although this constraint
holds for an enormous set of categories (e.g., all items for sale, descriptive categories such
as geographical and person data, etc.) there are some categories that lack this characteristic (e.g., personal ads). We see that this is the case when we analyze information about
hotels, using the Bidding For Travel extraction data set from previous work (Michelson
& Knoblock, 2008). In this data set, the goal is to extract hotel information, such as hotel
names and local areas from posts to an internet forum where users talk about their deals
they received for hotel accommodations. When we give our seed-based method the set of
hotel names as seeds (from the data set) and try to build a reference set from the roughly
2,500 posts from BiddingForTravel.com, we expect it to construct entity trees with the local
areas as children of the hotel names. However, the resulting reference set is not constructed
well. In fact, the algorithm only finds 20 hotel tuples out of the 132 possible. This is mostly
due to the fact that even the users themselves, who created these posts, cannot decide on
a consistent hierarchical structure for this data. Analyzing these posts, users put the hotel
name immediately before the local area 40.58% of the time and the local area immediately
214

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

before the hotel name 27.17% of the time. Therefore, even to the users themselves there are
least two intepretations for representing the data as entity trees, and it is ambiguous as to
whether we should expect the hotel names as roots or the areas as roots. We note that it
is often the case that for the same hotel the users flip the mentions of these attributes. So,
the assumption that we do make is not that the posts themselves are structured, but that
the structure of the entity trees is consistent (and agreed upon) such that the algorithm
can reconstruct the entity trees based on the users posts. That is, for this case, all entity
trees should be rooted on either hotel names, or local areas, but not a mixture as we see
reflected by the users posts. Further, this data is particularly difficult for constructing
reference sets because the terms for the attributes are freely intermixed. That is, tokens
from a hotel name and the local area are sometimes interspersed, which makes it difficult
for the machine to determine which terms go together in an attribute. This is largely due
to our limiting of bi-grams to being in order (which we do for efficiency), and perhaps
if we extend the algorithm to consider all possible combinations of bigrams (perhaps by
extending our method to a distributed approach) we could handle this issue.

5. Related Work
The focus of this research is in creating reference sets from posts. As the reference sets are
flattened entity trees, the work that most closely resembles ours is the research on creating
term hierarchies (subsumptions) from text. There are alternative methods for building
term hierarchies. However, these methods are not as well suited as the Sanderson and Croft
method (1999) that we chose for our problem. First, since our data is ungrammatical,
we cannot use subsumption methods that rely on Formal Concept Analysis, which relate
verbs to nouns in the text to discover subsumptions (Cimiano, Hotho, & Staab, 2005).
We have plenty of nouns in posts, but almost no verbs. Further, since our algorithm runs
iteratively due to the general token problem, we need a method that runs efficiently.
Our algorithm using the Sanderson and Croft method runs in O(kn) time where n is the
number of tokens from the posts, and k is the number of iterations for general tokens, since
our process scans the posts to create the bigrams and calculate the probabilities and then
considers only those with high enough probabilities. This is in contrast to other methods for
term subsumption that use Principle Component Analysis (PCA) (Dupret & Piwowarski,
2006; Bast et al., 2006) and run in O(n3 ) time with respect to the token-by-token matrix
(which may be sparse). Therefore, these PCA methods are not suitable to a large number
of tokens and for more than one iteration. There is also previous work that uses outside
information sources, such as links between image tags and the users who supply them on
Flickr, to aid in the building of term hierarchies (Schmitz, 2006). We do not have explicit
links between terms and users. Lastly, there is previous work that uses Google to determine
term dependencies (Makrehchi & Kamel, 2007). However, we cannot assume that the terms
we encounter in posts will occur across many webpages. In fact, our method will work even
if the posts were the only place that mentions the entities on the entire Web (provided
there are enough posts). However, for such a case, which might occur if the posts are for
obscure items, we could not leverage their Google-based method. Further, although there
are a few alternative approaches to building term hierarchies, there is also one larger, more
fundamental difference between our problem and the previous work on ontology creation.
215

fiMichelson & Knoblock

All of the previous methods build up a single, monolithic conceptual hierarchy. In our case,
we instead aim to build a number of disjoint entity trees, which we can then flatten into a
reference set.
Along the lines of discovering term hierarchies from text, there is also work that aims
to extract features from text such as product reviews. Since the entity trees our method
constructs often include product features (e.g., Laptop models), our work constructs similar
output to these methods. Approaches to feature extraction include association rule mining
for finding frequent product features (Hu & Liu, 2004) and leveraging the Web to aid
in feature extraction (Popescu & Etzioni, 2005). However, these methods rely on natural
language processing, such as part-of-speech (POS) tagging to parse the reviews into possible
features. However, our posts are not grammatical enough to support part-of-speech tagging,
and so we cannot use such features of the data.
Recently, even some of the large commercial search engines have begun to construct
reference sets from Web data. Google has built Google Squared,7 which allows users
to type a query category and returns a square which is a reference set related to the
category. This product provides an intuitive interface for including/excluding attributes
(columns) and naming them appropriately. However, when we tested the application for
our experimental domains (providing queries of cars, laptops, and skis) we found that
the columns in the square were not broken down into the constituent attributes as finely as
our reference sets. For instance, for both laptops and skis, there is a single attribute (called
item name) that essentially functions as a post describing the entity. For laptops, the
item name combines the manufacturer, model, and model number into a single attribute,
and for skis it combines the brand, model, and model spec. For cars, the square sometimes
combined the make and model of the car into the item name. Yet, it is encouraging
that such a large company finds reference sets important and useful enough to devote
an application to them, and therefore we feel that methods such as ours could greatly
complement this technology by providing a means to build even finer grained reference sets
to include in the squares.
As stated, the goal of this work is to build reference sets, which can be used in a number
of tasks, including ontology maintenance, query formulation, and information extraction.
Given that we chose information extraction as the mechanism for evaluating our reference
sets, for completeness we describe related work on information extraction to put the application of our constructed reference sets in context. We also point readers to the previous
work on reference-set-based information extraction, which also present comparisons to other
extraction methods (Michelson & Knoblock, 2005, 2007, 2008, 2009).
We note that there are information extraction techniques that are based on CRFs that
directly use reference sets in the form of either single columns of a reference set (dictionaries) (Cohen & Sarawagi, 2004) or full, relational databases (Mansuri & Sarawagi, 2006),
and in fact, these approaches focus on extraction from unstructured text, similar to posts.
Again, our work in this paper complements these methods well as either a constructed reference set can be split column-wise to produce dictionaries, or the reference-set itself can
be used as the relational database.
7. www.google.com/squared

216

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Similarly to the reference-set-based extraction methods described above, there are extraction methods that use ontologies as their background information (Embley, Campbell,
Jiang, Liddle, Lonsdale, Ng, & Smith, 1999). Later versions of their work even talk about
using ontology-based information extraction as a means to semantically annotate unstructured data such as car classifieds (Ding, Embley, & Liddle, 2006). Although the ways in
which the methods use the background information differ (the ontology-based method performs keyword-lookup into the ontology along with structural and contextual rules, while
the reference-set-based methods use either machine-learning or string-similarity methods
to match posts to reference-set members), our reference-set construction method presented
here can complement this ontology-based extraction well. One difficulty with an ontologybased method is that creating and maintaining an ontology is an expensive data engineering
task. Perhaps reference-set construction methods such as this one can ease this burden by
providing a method that can discover ontology instances and automatically map their relations. Along these lines are methods that use informal ontologies, such as Wikipedia, as
their background information (Wu, Hoffmann, & Weld, 2008; Kazama & Torisawa, 2007).
Again, our method is complementary here, especially as it is unclear whether Wikipedia
would cover some of the more obscure tuples we could generate for our reference set. In
fact, as we showed in the Laptop domain, the Wikipedia source of laptops was not nearly
as comprehensive as those from Overstock, which themselves did not appropriately cover
the posts in the same way our generated reference set did.
While there are previous approaches that use outside information to aid extraction (such
as ontologies and reference sets), there are also quite a few unsupervised methods for extraction. Although they do not use reference sets, we include them here for completeness in
addressing the extraction problem. One set of these techniques focuses on finding relations
from the Web. Aggregating these relations can sometimes yield reference sets, such as a table of person X being born in country Y (Cafarella, Downey, Soderland, & Etzioni, 2005;
Hassan, Hassan, & Emam, 2006; Pasca, Lin, Bigham, Lifchits, & Jain, 2006). However,
this research differs from reference-set-based methods for posts because they extract such
relations from Web pages which allows them to learn and exploit specific extraction patterns. These patterns assume that similar structures will occur again to make the learned
extraction patterns useful, but such structural assumptions about posts cannot be made
beyond the redundancy of bigrams. Again, however, such work complements ours quite
well, as both methods could perhaps be used together to build reference sets from both
Web pages and posts.
We note that the task of extracting information from posts that may not have an associated reference set (e.g., apartment listings, for which an entity tree would be hard to
define) has received attention in the past as well. So, the space of extraction from posts
both with and without reference sets is covered by complementing the previous work on
reference-set-based extraction from posts with this other work on extraction from posts. In
particular, some previous work uses information about the structure of the ads to perform
extraction (e.g., the idea that for certain types of posts attributes are often multi-token)
(Grenager, Klein, & Manning, 2005). More similar to our work is that which uses prototype learning for extraction from posts where seed examples for each of the attributes
to extract, called prototypes, are provided as background knowledge (Haghighi & Klein,
2006). Our machine-constructed reference sets could provide these prototypes automating
217

fiMichelson & Knoblock

such an approach even further. One interesting approach that can be used for both posts
that have an associated reference set and those that do not was presented by Chang, et. al.
(2007), where the extraction algorithm encodes and uses various constraints for extraction.
One such supported constraint is dictionary membership. As we described previously, such
dictionaries can be built directly from the reference sets that our approach constructs.

6. Conclusion
This paper presents a method for constructing reference sets from unstructured, ungrammatical text on the Web. Once discovered, these reference sets can be used for tasks including ontology maintenance, query formulation, and information extraction. We demonstrate
the utility of the machine constructed reference-sets by comparing them to manually constructed reference sets in an information extraction task, and we show that the machine
constructed reference sets yield better extraction results.
In the future we plan to investigate synonym discovery and its relationship to automatically constructing reference sets. For instance, we may be able to automatically
merge branches of the hierarchy if they are synonyms referring to the same object (such as
Lenovo and IBM laptops). Further, we plan to investigate the topic of dynamic data
integration using automatically mined reference sets. That is, once the system discovers a
reference set, we would want it to bring in other related sources for data integration.

Acknowledgments
This research is based upon work supported in part by the National Science Foundation
under award number CMMI-0753124, in part by the Air Force Office of Scientific Research
under grant number FA9550-07-1-0416, and in part by the Defense Advanced Research
Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition
Services Division, under Contract No. NBCHD030010.
The U.S. Government is authorized to reproduce and distribute reports for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions
contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of any of the
above organizations or any person connected with them.

References
Bast, H., Dupret, G., Majumdar, D., & Piwowarski, B. (2006). Discovering a term taxonomy
from term similarities using principal component analysis. In Semantics, Web and
Mining., LNAI 4289, pp. 103120. Springer.
Cafarella, M. J., Downey, D., Soderland, S., & Etzioni, O. (2005). Knowitnow: fast,
scalable information extraction from the web. In Proceedings of the conference on
Human Language Technology and Empirical Methods in Natural Language Processing
(HLT-EMNLP), pp. 563570. Association for Computational Linguistics.
218

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Chang, M.-W., Ratinov, L., & Roth, D. (2007). Guiding semi-supervision with constraintdriven learning. In Proceedings of the 45th Annual Meeting of the Association of
Computational Linguistics, pp. 280287. Association for Computational Linguistics.
Cimiano, P., Hotho, A., & Staab, S. (2005). Learning concept hierarchies from text corpora
using formal concept analysis. Journal of Artificial Intelligence Research, 24, 305339.
Ciravegna, F. (2001). Adaptive information extraction from text by rule induction and
generalisation.. In Proceedings of the 17th International Joint Conference on Artificial
Intelligence, pp. 12511256. Morgan Kaufman.
Cohen, W., & Sarawagi, S. (2004). Exploiting dictionaries in named entity extraction: combining semi-markov extraction processes and data integration methods. In Proceedings
of the 10th ACM International Conference on Knowledge Discovery and Data Mining,
pp. 8998. ACM Press.
Crescenzi, V., Mecca, G., & Merialdo, P. (2001). Roadrunner: Towards automatic data
extraction from large web sites. In Proceedings of 27th International Conference on
Very Large Data Bases, pp. 109118. VLDB Endowment.
Ding, Y., Embley, D. W., & Liddle, S. W. (2006). Automatic creation and simplified querying of semantic web content: An approach based on information-extraction ontologies.
In ASWC, LNCS 4185, pp. 400414. Springer.
Dupret, G., & Piwowarski, B. (2006). Principal components for automatic term hierarchy
building. In SPIRE, LNCS 4209, pp. 3748. Springer.
Embley, D. W., Campbell, D. M., Jiang, Y. S., Liddle, S. W., Lonsdale, D. W., Ng, Y. K.,
& Smith, R. D. (1999). Conceptual-model-based data extraction from multiple-record
web pages. Data Knowl. Eng., 31 (3), 227251.
Grenager, T., Klein, D., & Manning, C. D. (2005). Unsupervised learning of field segmentation models for information extraction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pp. 371378. Association for Computational Linguistics.
Haghighi, A., & Klein, D. (2006). Prototype-driven learning for sequence models. In
Proceedings of the main conference on Human Language Technology Conference of
the North American Chapter of the Association of Computational Linguistics, pp.
320327. Association for Computational Linguistics.
Hassan, H., Hassan, A., & Emam, O. (2006). Unsupervised information extraction approach using graph mutual reinforcement. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 501508. Association
for Computational Linguistics.
Hu, M., & Liu, B. (2004). Mining and summarizing customer reviews. In Proceedings of
the 10th ACM International Conference on Knowledge Discovery and Data Mining,
pp. 168177. ACM Press.
219

fiMichelson & Knoblock

Kazama, J., & Torisawa, K. (2007). Exploiting wikipedia as external knowledge for
named entity recognition. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning
(EMNLP-CoNLL), pp. 698707. Association for Computational Linguistics.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th
International Conference on Machine Learning, pp. 282289. Morgan Kaufmann.
Makrehchi, M., & Kamel, M. S. (2007). Automatic taxonomy extraction using google and
term dependency.. In Proceedings of IEEE/WIC/ACM International Conference on
Web Intelligence, pp. 321325. IEEE Computer Society.
Mansuri, I. R., & Sarawagi, S. (2006). Integrating unstructured data into relational
databases. In Proceedings of the International Conference on Data Engineering, p. 29.
IEEE Computer Society.
McCallum, A. (2002).
Mallet:
http://mallet.cs.umass.edu.

A

machine

learning

for

language

toolkit.

Michelson, M., & Knoblock, C. A. (2005). Semantic annotation of unstructured and ungrammatical text. In Proceedings of the 19th International Joint Conference on Artificial
Intelligence, pp. 10911098. Morgan Kaufmann.
Michelson, M., & Knoblock, C. A. (2007). Unsupervised information extraction from unstructured, ungrammatical data sources on the world wide web. International Journal
of Document Analysis and Recognition (IJDAR), Special Issue on Noisy Text Analytics, 10, 211226.
Michelson, M., & Knoblock, C. A. (2008). Creating relational data from unstructured and
ungrammatical data sources. Journal of Artificial Intelligence Research (JAIR), 31,
543590.
Michelson, M., & Knoblock, C. A. (2009). Exploiting background knowledge to build
reference sets for information extraction. In Proceedings of the 21st international jont
conference on Artifical intelligence, pp. 20762082. Morgan Kaufmann.
Muslea, I., Minton, S., & Knoblock, C. A. (2001). Hierarchical wrapper induction for
semistructured information sources. Autonomous Agents and Multi-Agent Systems,
4 (1/2), 93114.
Pasca, M., Lin, D., Bigham, J., Lifchits, A., & Jain, A. (2006). Organizing and searching
the world wide web of facts - step one: the one-million fact extraction challenge. In
Proceedings of the 21st National Conference on Artificial Intelligence (AAAI), pp.
14001405. AAAI Press.
Popescu, A.-M., & Etzioni, O. (2005). Extracting product features and opinions from
reviews. In HLT 05: Proceedings of the conference on Human Language Technology
and Empirical Methods in Natural Language Processing, pp. 339346, Morristown,
NJ, USA. Association for Computational Linguistics.
220

fiConstructing Reference Sets from Unstructured, Ungrammatical Text

Sanderson, M., & Croft, B. (1999). Deriving concept hierarchies from text. In Proceedings of
the 22nd International ACM Conference on Research and Development in Information
Retrieval, pp. 206213. ACM Press.
Schmitz, P. (2006). Inducing ontology from flickr tags. In Proceedings of the Workshop on
Collaborative Web Tagging.
Wu, F., Hoffmann, R., & Weld, D. S. (2008). Information extraction from wikipedia:
moving down the long tail. In Proceedings of the 14th ACM international conference
on Knowledge discovery and data mining, pp. 731739. ACM Press.

221

fiJournal of Artificial Intelligence Research 38 (2010) 415-473

Submitted 01/10; published 07/10

Resource-Driven Mission-Phasing Techniques for Constrained
Agents in Stochastic Environments
Jianhui Wu
Edmund H. Durfee

jianhuiw@umich.edu
durfee@umich.edu

Computer Science and Engineering, University of Michigan
Ann Arbor, MI 48109 USA

Abstract
Because an agents resources dictate what actions it can possibly take, it should plan
which resources it holds over time carefully, considering its inherent limitations (such as
power or payload restrictions), the competing needs of other agents for the same resources,
and the stochastic nature of the environment. Such agents can, in general, achieve more of
their objectives if they can use  and even create  opportunities to change which resources
they hold at various times. Driven by resource constraints, the agents could break their
overall missions into an optimal series of phases, optimally reconfiguring their resources at
each phase, and optimally using their assigned resources in each phase, given their knowledge
of the stochastic environment.
In this paper, we formally define and analyze this constrained, sequential optimization
problem in both the single-agent and multi-agent contexts. We present a family of mixed
integer linear programming (MILP) formulations of this problem that can optimally create
phases (when phases are not predefined) accounting for costs and limitations in phase creation.
Because our formulations simultaneously also find the optimal allocations of resources at each
phase and the optimal policies for using the allocated resources at each phase, they exploit
structure across these coupled problems. This allows them to find solutions significantly faster
(orders of magnitude faster in larger problems) than alternative solution techniques, as we
demonstrate empirically.

1. Introduction
An omnipresent issue in realistic application domains for autonomous agents is that agents
are resource-limited. Resources enable action. For example, an agent with a camera can
capture an image, an agent with a gripper can manipulate objects, an agent with an auxiliary
battery pack can take more actions before it must recharge, and an agent with an additional
memory chip can solve larger computational problems. Given the resources it possesses, an
agent should utilize them to take the best sequences of actions that it can, depending on its
objectives and environment.
In this paper, we consider the situation where agents have some degree of control over
the resources they can choose to possess, subject to inherent (unavoidable) limitations of the
agents themselves, as well as to contention over resources. Agents inherent limitations stem
from what we will call capacity constraints. For example, a mobile robot agent (say, a Mars
Rover) might have weight limitations for the payload it can carry, such that it cannot carry a
camera and gripper at the same time. Or the physical configuration of such resources might
preclude some combinations, such as if the gripper arm necessarily obstructs the camera view.
The power drawn across combinations of peripherals might exceed an agents power supply,
c
2010
AI Access Foundation. All rights reserved.

fiWu & Durfee

or the computational cycles demanded by a combination over a time interval might exceed
the agents processing power. In short, for any of a number of reasons, an agent might lack
the capacity to effectively possess all of the resources that it might find useful, in which case
it needs to determine which subset combination of resources will, in expectation, allow it to
act most effectively over time, given uncertainty over the future evolution of its environment.
In a multiagent setting, an agent might fail to possess a potentially useful resource not
only because of capacity constraints, but also because of resource scarcity constraints. For
example, there might be fewer instruments of some type, such as working cameras or grippers,
than there are robots (Mars Rovers), in which case a cooperative agent should only get one
of these resources if it expects to make better use of it than other agents who would not get
it. Along similar lines, if the number of licenses for running a particular piece of software
are limited, then cooperative agents should allocate them in the best possible way for their
collective benefit. Or, if the number of satellites to remotely control in order to acquire needed
images is restricted, then assignments of these to agents should be done judiciously.
Dolgov and Durfee (2006) looked at these kinds of problems, studying efficient techniques
by which agents can assess the value of alternative resource combination (bundle) assignments in terms of the execution policies (and the expected utilities of those policies) that the
resources enable. That work focused on the question of finding an optimal static resource
allocation for an agent (or multiple agents) where the agent(s) then make sequential decisions
in a stochastic environment.
The significant new contribution of the work we present in this paper is to consider sequentiality not only in agents actions but also in the allocation of resources.1 In the single
agent case, an agent might plan to change, in the midst of execution, how it utilizes its limited capacity. For example, it might return to the toolbox at its base station to drop off
one instrument (e.g., a camera) and pick up another (e.g., a gripper). It might power down
one peripheral and power up another, or terminate one process to create another. In the
multiagent case, agents might at particular times swap who possesses or controls different
instruments, or who holds the licenses for various software packages.
More precisely, in this paper we present formulations for defining, and algorithms for solving, several classes of single- and multi-agent sequential resource allocation decision problems
for agents acting in stochastic environments. These problems are characterized by agents operating in multiple phases, where the set of resources held by an agent (and thus the actions
that it can perform) are constant within a phase, but can change from one phase to another.
The challenges that we tackle in this paper thus involve deciding how resource constraints
should drive agents overall missions to be best broken up into planned phases, and how
agents should decide which resources to hold in each phase. As we shall see, these questions
are intertwined with each other, and also with questions about how agents should formulate
policies for pursuing their objectives in each phase.
1.1 Simple Illustrating Single-Agent Example
To drive home the problem in a very simple form, and to provide a running example that
we will use to illustrate the formalisms, notations, and algorithms in the coming sections,
we here present a simple schematic example to illustrate the Single-agent Resource-driven
1. This paper brings together and significantly extends work previously reported in conferences (Wu & Durfee,
2005, 2007a).

416

fiResource-Driven Mission-Phasing Techniques

a1 (0.1), noop (0.8)

1

2

-5

-20
a1 (0.9), noop (0.2)

a2 (1.0), noop (1.0)

3

a4 (0.1), noop (0.5)

a5 (0.2), noop (0.8)

-5
a4 (0.1), noop (0.3)
), noop
a3 (0.9

(0.05)

a3 (0.1), noop (0.95)

4

-5

5
a4 (0.8), noop (0.2)

-5

a5 (0.8), noop (0.2)

6

200

Figure 1: A simple single-agent example.

Mission Phasing (S-RMP) problem. We will present a simple multiagent example problem
later in this paper.
In this problem, shown in Figure 1, the agent begins in state S1 and moves among states
S1 through S5 until it reaches and stops in state S6 . Associated with each state Si is the
reward ri the agent receives for reaching that state, so, for example, reaching state S2 is bad
(incurring a reward of -20) while reaching state S6 is good (providing a reward of +200).
The agent has some degree of control over its trajectory among states based on the action
that it chooses to take in each state. For example, in state S1 it has a choice of two actions, a1
or noop (where noop is the action of not taking any action, passively letting the environment
dynamics change the agents state). As shown in the figure, if it takes action a1 it has a
probability of 0.1 of reaching state S2 , and of 0.9 of reaching state S3 , whereas noop reaches
states S2 and S3 with probabilities 0.8 and 0.2 respectively. So, if we look at the agents first
decision, it would appear that it should choose action a1 over noop to reduce the likelihood
of the higher negative reward for reaching S2 .
However, to actually take action a1 , the agent needs to have a particular resource, which
we will call o1 . More generally, for each of the actions ai that the agent can take, it needs
resource oi , other than for the noop action (which for notational convenience we will sometimes
refer to as a0 ), which does not require any resources. Thus, if when setting out from state
S1 the agent anticipated taking actions a1 , a3 , and a5 , say, it would want to set out having
resources o1 , o3 , and o5 .
Unfortunately for the agent in this simple example, we will say that its capacity is limited
such that it can have at most only a single resource at any given time. Now, before setting out
from S1 it will need to decide which resource would, in expectation, allow it to make action
choices that would maximize its total reward by the time it reaches S6 . This is a simple
417

fiWu & Durfee

instance of the type of problem solved by Dolgov and Durfee (2006), and we will show their
solution shortly as a stepping stone to our algorithm.
In the S-RMP problem, we consider a generalization of Dolgov and Durfees problem,
where the agent has access to instances of all resources oi not only in state S1 , but also
in some other states. After the agent sets off from state S1 with the resource it considers
most valuable, it traverses states until it reaches one of these other states, say state Si . In
state Si , the agent can reconfigure its resources, subject as always to its capacity constraints,
and thus switch to a new phase of execution, where the set of actions that it can take are
different. We thus refer to states like Si (and, in a degenerate way, S1 ) as phase-switching
states. Referring back to our earlier examples of capacity-limited agents, a phase-switching
state could correspond to the agent arriving at a location holding a cache of instruments (a
toolbox), or it could correspond to reaching a time, place, or situation (e.g., a holding
pattern) where the environment is less dynamic such that the agent can safely power down
some peripherals and power up others. Simple human examples of phase-switching states
include the state I am in when I enter a parking lot, where now I can access my car, and the
state of being on a highway entrance ramp, where as a driver I am given a buffer to change
my driving behaviors and car speed to prepare to safely merge into high-speed traffic.
As we shall see, even in problems where the phase-switching states are static and predefined
for the agent, phase-switching opportunities can complicate the agents decisions about what
actions to take, because now it might choose actions not only based on the rewards of reaching
states but also based on the benefits to future action choices of reaching phase-switching states.
Further, in some classes of problems, an agent itself might be able to decompose its problem
into phases by deciding which states it would like as phase-switching states. For example, it
could choose where it would like toolboxes (or entrance ramps) placed in its environment,
or in which circumstances it would like to be buffered from environmental dynamics as it
reconfigures itself. The agent would generally face constraints on creating phase-switching
states, such as bounds on the number of such states (e.g., it might have a limited number of
toolboxes to distribute), or incur a cost each time it creates a state in which it is buffered
(e.g., every holding pattern introduces costly delays).
Thus, referring back to Figure 1, the Single-agent Resource-driven Mission Phasing (SRMP) problem generally involves optimally deciding which states (besides the start state S1 )
to designate as phase-switching states, optimally allocating resources in each of those states,
and optimally choosing actions enabled by the resources during each phase. As we shall
see later, the multiagent extension to this further requires that the multiple agents agree on
their phase-switching states, since resource reallocation means potentially swapping (control
over) different resources, and for each switch how best to (re)distribute their limited resources
amongst themselves.
1.2 Paper Overview
While the idea of reconfiguring resources to improve agent performance is fairly straightforward, as the preceding example suggests it can be a challenging problem to reconfigure
resources optimally. The primary goal of our study in this paper is to design computationally efficient algorithms to exactly solve this class of challenging problems. Toward this end,
we develop a suite of algorithms that can formulate complex resource-driven mission-phasing
problems into compact mathematical formulations. Thereafter, by simultaneously solving the
418

fiResource-Driven Mission-Phasing Techniques

problem decomposition (phase creation), resource (re)configuration, and policy formulation
problems, these algorithms can fruitfully exploit problem structure, which often results in a
significant reduction in computational cost.
This paper is organized as follows. Section 2 introduces background techniques. Section 3
starts with a relatively simple single-agent resource-driven mission-phasing problem where
phase-switching states are known a priori. Exploiting such fixed phase-switching states, we
can work out a particular, efficient algorithm. We then describe solution algorithms for solving
general resource-driven mission-phasing problems, in which an agent needs to determine for
itself where to reconfigure resources, how to reconfigure resources, and what are optimal
executable policies subject to the (re)configured resources. Section 4 extends our resourcedriven mission-phasing techniques presented in Section 3 to a class of multiagent systems for
sequentially allocating resources among a group of cooperative agents. This section follows
a similar progression as in Section 3, in terms of giving the agents increasing latitude in
determining when to reallocate resources. Then, we contrast our work with related work in
Section 5, and finally, Section 6 concludes the paper with a summary of this work, and a
discussion of questions that remain open together with possible future research directions.

2. Background
We will formulate the single- and multi-agent resource-driven mission phasing problems using
the well-established formalism of Markov Decision Processes (MDPs), with extensions to constrained MDPs. This section summarizes the relevant aspects of these previously-developed
formalisms, and illustrates them using the example previously discussed in Section 1.1.
2.1 Markov Decision Processes
In general, a classical discrete-time, fully-observable Markov Decision Process with a finite
state space and a finite action space can be defined as a four-tuple hS, A, P, Ri (Puterman,
1994), where:
 S is a finite state space, represented as a set of n states {1, ...i, ...n}.
 A is a finite action space. For a state i  S, Ai  A represents the set of actions that
can be executed at the state i.
 P = {pi,a,j } represents state transition probability where pi,a,j is the probability that
the agent reaches state j if it executes action a in state i.
P
P
For any state i and action a, j pi,a,j must be no greater than one. j pi,a,j = 1 means
that
P the agent will always stay in the system when executing action a in state i, while
j pi,a,j < 1 means that there is some probability of the agent being out of the system
(which can be equivalently interpreted as the agent entering a sink state where the agent
would stay forever) when executing action a in state i (Kallenberg, 1983).
 R = {ri,a } is the (bounded) reward function where ri,a is the reward that the agent will
receive if it executes action a in state i.
Running Example: MDP Encoding.
is easily represented as a MDP:

The example introduced in Section 1.1 (Figure 1)

419

fiWu & Durfee

 S = {S1 , S2 , ...S6 }.
 A = {a0 = noop, a1 , a2 , ...a5 }.
 P = {pS1 ,a0 ,S2 = 0.8, pS1 ,a1 ,S2 = 0.1, ...}.
 R = {rS1 ,a0 = 5, rS1 ,a1 = 5, ...}.
The Markov decision process is an extension of the well-known Markov chain. The main
property of a MDP is that it possesses the Markov property (Bellman, 1957): if the current
state of a MDP at time t is known, transitions to a new state at time t + 1 only depend on
the current state and the action chosen at it, but are independent of the previous history of
states.
In a MDP, the decision-making agent chooses its actions based upon its observation of
the current state of the world, with the motivation of maximizing its aggregate reward. A
deterministic stationary policy for a MDP is defined as a mapping from states to actions:  :
i  a where i  S and a  Ai . The objective of the decision-making agent is to find an optimal
policy that maximizes some predefined cumulative function of rewards. Let {i0 , i1 , ..., it , ...}
and {a0 , a1 , ..., at , ...} represent particular state and action sequences generated by following
the policy  starting in state i0 , and let E[ ] denote the expectation function. Then a typical
cumulative reward function of a non-discounted MDP can be defined as:
U () = E[


X

rit ,at ]

t=0

Similarly, the cumulative reward function of a discounted MDP with the discount factor 
can be defined as:2

X
()t  rit ,at ]
U () = E[
t=0

Although in general the mission-phasing techniques in this paper will also apply to discounted MDPs and other contracting MDPs (Kallenberg, 1983; Puterman, 1994; Sutton &
Barto, 1998), we illustrate them in this paper using transient, non-discounted
MDPs.3 NonP
discounted MDPs were described above. In a transient MDP (in which j pi,a,j < 1 at some
states), an agent will eventually leave the corresponding Markov chain, after running a policy
for a finite number of steps (Kallenberg, 1983). In other words, given a finite state space, it is
assumed that the agent visits any state only a finite number of times
P for any policy, which in
turn means that the total expected reward function U () = E[ 
t=0 rit ,at ] is bounded even
for a non-discounted MDP. The running example problem
of
Section
1.1 is an example of this
P
kind of MDP, where state S6 acts as a sink state ( j p6,a,j = 0).
2.2 Linear Programming
The value iteration and policy iteration algorithms are widely used in solving classical MDPs
(Kallenberg, 1983; Puterman, 1994; Sutton & Barto, 1998). However, it is surprisingly hard to
extend these algorithms to incorporate additional constraints without considerably increasing
2. In this paper, (a)b represents an exponent, while ab represents a superscript.
3. The transient MDP of interest in this work is a subclass of contracting MDPs.

420

fiResource-Driven Mission-Phasing Techniques

the size of the state space and/or the action space of the MDP model. For that reason, a
number of researchers have proposed and utilized an alternative solution approach, which is
based upon mathematical programming (Altman, 1998; Feinberg, 2000; Dolgov & Durfee,
2006). A procedure for formulating an MDP into a linear program (whose solution yields an
optimal policy maximizing the total expected reward) is described below. Our work extends
this approach.
Let xi,a , which is often called the occupation measure or visitation frequency (e.g., Dolgov
& Durfee, 2006),
denote the expected number of times action a is executed in state i. Then
P P
the function i a xi,a  ri,a can be used to represent the total expected reward, and the
problem of finding an optimal policy to the MDP is equivalent to solving the following linear
program:
XX
max
xi,a  ri,a
(1)
i

a

subject to:
X
XX
pi,a,j  xi,a
xj,a = j +
a

i

: j

a

xi,a  0

: i, a

where j is the probability that the agent
P state
P j, and the constraint (named
P is initially in
the probability conservation constraint) a xj,a = j + i a pi,a,j  xi,a guarantees that the
expected number of times state j is visited must equal the initial probability distribution at
state j plus the expected number of times state j is entered via all possible transitions.
When the linear program Eq. 1 is solved, it is trivial to derive an optimal policy that
specifies the action(s) to take in a given state. Specifically, a policy  that assigns a probability
x
of P i,a
to executing action a in state i will maximize the total expected reward. If any statea xi,a
action probability in  has a value other than zero or one, the optimal policy is randomized;
otherwise it is deterministic.
Running Example: MDP Policy Formulation. If we consider the problem introduced
in Section 1.1 (Figure 1) but ignore the agents capacity constraints (such that the agent
has in every state the resources needed for its full set of actions A = {a0 , a1 , ...a5 }), then
this corresponds to a classical (what we will refer to as an unconstrained) MDP. Using the
above policy formulation algorithm, the agent can easily compute its optimal policy, which is
[S1  a1 , S2  noop/a2 , S3  a3 , S4  a4 , S5  a5 , S6  noop], and the total expected
reward is 174.65.
2.3 Constrained MDPs
Formulating unconstrained MDPs as linear programs makes it straightforward to take into
account additional constraints, including the agent capacity constraints and resource constraints. Several of such constrained optimization problems have been investigated by Dolgov
and Durfee (2006). In what follows, we summarize that work.
A constrained MDP that models agent capacity limitations can be represented as hM, , Ci,
where:
 M is the classical MDP (Section 2.1), represented as hS, A, P, Ri.
421

fiWu & Durfee

  = {i } indicates the probability distribution over initial states.
 C is the agent capacity constraints, represented as hO, C, U, , i, where:
 O = {o} is a finite set of indivisible non-consumable execution resources, e.g., O =
{camera, spectrometer, gripper, etc.}.
 C = {c} is a finite set of capacities of the agent, e.g., C = {weight, space, etc.}.
 U = {uo,a,i } represents resource requirements for executing actions, where uo,a,i 
{0, 1} indicates whether the agent requires resource o to execute action a in state
i.4 For example, uo=camera, a=take picture, i=any state = 1 says that a prerequisite for
taking a picture is having a camera.
  = {o,c } defines resource capacity costs, where o,c is the amount of agent capacity
c required to hold one unit of resource o. For example, o=camera, c=weight = 2 and
o=camera, c=space = 1 says that carrying a camera will consume two units of the
carrying weight and one unit of the carrying space of the agent.
  = {c } specifies the limits of the agent capacities, e.g., c=weight = 4 denotes a
maximum weight of four units that an agent can carry.
Running Example: Constraint Formulation.
tion 1.1, the agent constraint components are:

In the simple running example from Sec-

 O = {o1 , o2 , ...o5 }.
 C = {hold}.
 U = {uoi ,ai ,si = 1 : 1  i  5}.
  = {oi ,chold = 1 : 1  i  5}.
  = {chold = 1}.
The linear programming formulation (Eq. 1) paves the way for incorporating agent capacity constraints. Namely, the capacity limitations can be modeled by adding the following
mathematical constraints (shown in Eq. 2) on occupation measures xi,a to the linear program
in Eq. 1.
X
XX
o,c  (
uo,a,i  xi,a )  c
: c
(2)
o

i

a

where (z) is a step function, defined as
(z) =



1
0

z>0
otherwise

The constraint indicates that, given the resource requirement parameter uo,a,i = 1, the agent
will have to employ o,c amount of its capacity c to hold resource o if it decides to execute
action a in one or more states i in its policy.
4. To simplify the presentation, it is assumed that the resource requirement is binary, which implies that an
agent will not be interested in more than one unit of a particular resource, but most of results presented
in this paper can be generalized to non-binary resource requirement cases without much difficulty.

422

fiResource-Driven Mission-Phasing Techniques

Note that the (z) function is a nonlinear function. In general, directly solving nonlinear
constrained optimization problems is difficult. Fortunately, there is a simple way to transform the nonlinear constraint Eq. 2 into linear constraints through introducing some integer
variables (Dolgov & Durfee, 2006). The reformulation of Equation 2 is depicted below.
P P
i
a uo,a,i  xi,a
 o
: o
X
X
o,c  o  c
: c
o

o  {0, 1}

: o

where o , a binary integer in the set {0, 1}, is introduced to indicate whetherP
thePagent uses its
limited capacity to hold resource
o. X is a constant that is no less than sup i a xi,a , which
P P
P P
i
a uo,a,i xi,a
never exceeds one (because i a uo,a,i  xi,a 
is applied
to guarantee
X
P Pthat
P
P
i
a xi,a  X). One way to compute X is to solve an unconstrained
i
a xi,a  sup
MDP:
XX
X = max
xi,a
(3)
i

a

subject to:
X
XX
xj,a = j +
pi,a,j  xi,a
a

: j

a

i

xi,a  0

: i, a

To summarize, the constrained MDP that models the agents capacity limitations can be
formulated into a mathematical program Eq. 4 (i.e., by putting Eq. 1 and the above integer
linear constraints together), whose solution will yield an optimal capacity usage configuration
and an optimal executable policy.

max

XX
i

xi,a  ri,a

subject to:
X
XX
xj,a = j +
pi,a,j  xi,a
a

i

P P
i

a uo,a,i

 xi,a

X
X
o,c  o  c

(4)

a

: j

a

 o

: o
: c

o

xi,a  0

: i, a

o  {0, 1}

: o

In Eq. 4, pi,a,j , ri,a , j , uo,a,i , o,c , c , and X are constants, while xi,a are continuous
variables and o are binary integer variables, which indicates that Eq. 4 is a mixed integer
linear program (MILP).
423

fiWu & Durfee

Mixed integer linear programming is the discrete version of linear programming with an
additional requirement that particular variables must be integers. Although MILPs are NPhard in the number of integer variables, they can be solved by a variety of highly optimized
algorithms and tools (Cook, Cunningham, Pulleyblank, & Schrijver, 1998; Wolsey, 1998).
Recently, there has been substantial progress on using MILPs in automated planning (Earl &
DAndrea, 2005; Kautz & Walser, 2000; van Beek & Chen, 1999; Vossen, Ball, Lotem, & Nau,
1999). The automated resource-driven mission-phasing techniques (which are also NP-hard
as is shown later) presented in this paper are based upon the MILP as well.
Running Example: Constrained MDP Solution. In the simple running example from
Section 1.1, the constraint permits an agent to hold only one resource (and thus to be capable
of executing an action other than noop in only one state). Given the MDP and constraint
parameters from this problem, and computing the constant X = 70.24 using Eq. 3, we apply
a MILP solver such as CPLEX (www.ilog.com) to easily derive an optimal solution to the
MILP:
[(x1,0 , x1,1 ), (x2,0 , x2,2 ), (x3,0 , x3,3 ), (x4,0 , x4,4 ), (x5,0 , x5,5 ), x6,0 ]
=[(3.47, 0), (3.03, 0), (5.21, 0), (4.95, 0), (0, 1.25), 1]
[1 , 2 , 3 , 4 , 5 ] = [0, 0, 0, 0, 1]
That is, the optimal policy is [S1  noop, S2  noop, S3  noop, S4  noop, S5 
a5 , S6  noop], and the corresponding total expected reward is reduced to 65.02 (from 174.65
in the unconstrained case) due to the limitation on the agent capacity. This is the optimal
policy for the constrained agent that uses a single policy throughout its entire mission. We
will use this example as we go along to illustrate the degree to which our automated missionphasing techniques can improve that expected reward.

3. Resource Reconfiguration in Single-Agent Systems
We now turn to our new techniques and results that build on the work of others as summarized in the previous section. In particular, we extend the representations and techniques for
solving constrained MDPs where resources are allocated prior to execution, to sequential constrained MDPs where resource allocations can change during execution when particular states
are reached. As previously mentioned when we described the example problem (Section 1.1),
we refer to the intervals during which an agents resources cannot change as a phase, and
the states that connect phases (representing an opportunitybut not an obligationto change
the resource allocation) as phase-switching states. We assume that the full complement of
resources (e.g., a full toolbox) is available at each phase-switching state, and that an agent
cannot pick up or discard a resource except at a phase-switching state; relaxing this assumption is discussed as future work (Section 6.2).
At the extreme, if every state is a phase-switching state, then the agent is effectively
unconstrained (unless there can exist an action whose necessary resources total capacity
requirements alone exceed the agents capacity limits). In general, however, there will be
restrictions on which states can (or should) be phase-switching states. We will consider
several cases in this section. These range from where the phase-switching states are inherently
predetermined by the environment (e.g., the placement of toolboxes, or of shelters from
424

fiResource-Driven Mission-Phasing Techniques

domain dynamics, is dictated to the agent), to where the number of phase-switching states
is bounded by the environment but which states are designated as phase-switching states is
decided by the agent (e.g., the number of toolboxes or shelters is fixed but the agent can
choose where they are placed), to where the number of phase-switching states is unbounded
but where creating a phase-switching state incurs a cost (e.g., toolboxes or shelters can be
bought and placed as the agent wishes) such that the agent will want to be selective so as to
not spend more on creating a phase-switching state than the improvement the phase-switching
state will make to its expected reward.
This section begins by giving a formal definition of the single-agent resource-driven missionphasing (S-RMP) problem in Section 3.1, and then Section 3.2 analyzes and discusses the
computational complexity of the S-RMP problem, illustrating why standard approaches are
computationally intractable for solving the problem. Section 3.3 and Section 3.4 present, analyze, and illustrate solution algorithms for the variations of the S-RMP problem mentioned
above. We present experimental results in Section 3.5, where the effectiveness and efficiency
of our automated mission-phasing techniques are empirically evaluated. Finally, Section 3.6
summarizes the contributions of the work described in this section.
3.1 Problem Definition
Formally, a single-agent resource-driven mission-phasing (S-RMP) optimization problem is a
generalization of the constrained MDP optimization problem presented in Section 2.3, composed of a Markov decision process M, an initial probability distribution , agent capacity
constraint C, and resource reconfiguration constraint5 R, where:
 M is a classical MDP, as described in Section 2.1.
  = {i } is a probability distribution over states, where i is the probability that the
agent starts in state i.
 C is the agent capacity constraint, as described in Section 2.3.
 R is the resource reconfiguration constraint (sometimes also called phase-switching constraint) that specifies restrictions on creating phase-switching states at which the constrained agent can reconfigure its resources and adjust its use of its limited capacities.
A typical resource reconfiguration constraint R can be formulated as h, i (and one of
its generalizations will be discussed in Section 3.4.2), where:
  = {i } indicates phase-switching state creation costs, where i denotes the cost
for making state i  S into a phase-switching state.
   0 specifies a cost limit for creating phase-switching states.
For notational convenience, we also define S  S as the set of eligible phase-switching
states (indicating which of the states in S can potentially become a phase-switching state).
By definition, S is a (not necessarily proper) subset of S, where for each i  S , i  . That
is, a state in S is inherently ineligible to be a phase-switching state if its cost to make into a
phase-switching state exceeds the agents cost limit all by itself .
5. Because resource reconfiguration comes along with phase switching, in the following discussion, resource
reconfiguration constraints are sometimes called phase-switching constraints to improve readability.

425

fiWu & Durfee

Running Example: Resource Reconfiguration Constraints. Given the example in
Section 1.1, here are a few of many possible specifications of the Resource Reconfiguration
Constraints.
 When 1   and i >  : i 6= 1, then S = {S1 } and this is the constrained MDP
situation described in Section 2.3.
 When i = 0 : i (so, S = S), this is the case described at the beginning of this section
where every state is a phase-switching state, and thus this equates to an unconstrained
MDP unless the agent has at least one action that needs resources whose capacity requirements exceed a capacity constraint.
 If S  S is predefined, then i = 0 : i  S but i >  : i  S  S . This is the case
where the phase-switching states are dictated to the agent.
 When i = 1 : i, and  = n where 0 < n < |S|, then S = S and this is the case where
the agent can select any subset of n states to be phase-switching states.
 When i > 0 : i, and  = , then S = S and this is the case where the agent
could turn any (and all) states into phase-switching states, but if the costs incurred are
subtracted from the agents reward it might choose to leave some (or most!) states as
non-phase-switching states.
Given the inputs M, , C, and R, the objective of the S-RMP optimization problem is
to maximize the total expected utility of the capacity-restricted agent by identifying a set of
phase-switching states S 0 = {sk }  S that decompose the overall problem into a collection
of phases, and, for each phase k, determining a resource configuration k and an executable
policy  k that should be adopted by the agent at the entry to that phase (at state sk ).
Specifically, from a constrained optimization perspective, the S-RMP optimization problem can be formulated as follows:
Objective:
maximize the utility of the overall policy 
subject to the following constraints:
i) The set of phase-switching states S 0 = {sk } should satisfy the phase-switching constraint
R.
ii) Within each phase k, resource configuration k should satisfy the agent capacity constraint C.
iii) Within each phase k, policy  k should be executable with respect to the resource configuration k .
iv) The overall policy  is composed of phase policies  k , i.e., phase policy  k is adopted
by the agent when it encounters a phase-switching state sk  S 0 in the midst of its
execution.
426

fiResource-Driven Mission-Phasing Techniques

Clearly, the S-RMP optimization problem involves three intertwined components: i) problem decomposition, ii) resource configuration, and, iii) policy formulation. Problem decomposition (which creates phase-switching states) lays the foundation for resource configuration
and reconfiguration; resource configuration dictates what policies are executable in each phase;
policy formulation determines transitions within and among phases as well as what rewards
can be accrued by the agent, which in turn determines the utility of problem decomposition
and resource (re)configuration.
Each of these three component problems and some combinations of them have been investigated in a number of research fields (but none of the prior approaches is computationally
tractable to the S-RMP optimization problem that tightly couples problem decomposition,
resource configuration, and policy formulation). A comprehensive discussion that contrasts
our work with prior work is postponed to Section 5 after our computationally efficient solution
approach is presented.
3.2 Computational Complexity Analysis
Before describing our new solution techniques for S-RMP problems, we first analyze the
computational complexity of the S-RMP optimization problem and illustrate why standard
approaches are not computationally tractable for solving it.
Theorem 3.1. S-RMP optimization is NP-complete.
Proof: The proof of S-RMP optimization being NP-hard is trivial, because one of its
special cases, which includes only one phase (i.e., the agent can only configure its resources
at the beginning of mission execution), has been proven to be NP-hard through a reduction
from the well-known KNAPSACK problem (Dolgov, 2006; Dolgov & Durfee, 2006).
The presence in NP can be proven in the following way. For a MDP with n states, it
is clear that there can be at most n phases (i.e., n phases in the extreme situation where
every state is a phase-switching state). By featuring phase id (assuming each phase has a
unique id) in the state representation, a generalized MDP with at most n2 states can be
constructed in polynomial time and the phase policies can be combined into an overall policy
to this generalized MDP in polynomial time too. Given the generalized MDP and its policy,
the problem is reduced to solving a Markov chain. Since a Markov chain can be verified in
polynomial time, S-RMP optimization is in NP.
Given its presence in both NP and NP-hard, S-RMP optimization is proven to be NPcomplete. 
S-RMPs complexity is evident when considering how straightforward solution methods
would perform. In previous work (Wu, 2008), we compare our solution method to both a
brute-force search algorithm and a MDP-expansion-based approach. We briefly summarize
that comparison here. The brute-force search algorithm enumerates all possible problem
decomposition schemes (legal combinations of phase-switching states), and, for each scheme
enumerates all possible ways to configure and reconfigure resources, and, finally, for each possible problem decomposition and resource (re)configuration, derives optimal phase policies that
are executable with respect to the configured resources. This quickly becomes intractable
for non-trivial S-RMP optimization problems. The MDP-expansion-based approach instead
427

fiWu & Durfee

incorporates resources into the state representation and models resource reconfiguration activities as explicit actions, and is more efficient than the brute-force algorithm, but still scales
poorly because folding resources into states increases the state space size exponentially.
Neither of these approaches exploit key problem structure stemming from the coupled
problems. The brute-force approach deals with S-RMP component problems in isolation
and sequentially, while the MDP-based approach combines the resource-configuration and
policy-formulation components in a nave way that results in an exponentially larger policy
formulation problem. In contrast, as we will see, our new solution algorithms can take advantage of problem structure by formulating problem decomposition, resource configuration, and
policy formulation problems into a compact mathematical program and solving these component problems simultaneously, using a highly optimized tool. As will be shown in Section 3.5,
the algorithms presented in this section can often find exact solutions to a complex S-RMP
problem within a reasonable time.
3.3 Exploiting Fixed Phase-Switching States
So far, we have formally defined the S-RMP optimization problem and theoretically analyzed
its computational complexity. In this and the next subsections, we present and illustrate our
computationally efficient automated mission-phasing algorithms for solving S-RMP optimization problems.
We begin by first examining the simple variation of the S-RMP optimization problem
where the phase-switching states are predetermined; that is, where the phase-switching states
S  S are given to the agent, such that i  S , i = 0. This variation fits problems
where the opportunities to reconfigure resources and switch policies (e.g., the placement of
equipment and/or technicians needed for (un)loading or refitting) are dictated by the agents
environment rather than being a choice of the agent. Exploiting the fact that phase-switching
states are fixed, we can devise a particular, efficient algorithm (while a more general but
generally slower algorithm will be presented in Section 3.4).
Decomposition techniques for planning in stochastic domains are widely used for large environments with many states (and a detailed discussion of problem decomposition techniques
will be given in Section 5). In those approaches, states are partitioned into small regions,
a policy is computed for each region, and then these local policies are pieced together to
obtain an overall policy (Parr, 1998; Precup & Sutton, 1998; Lane & Kaelbling, 2001). Our
automated mission-phasing techniques are analogous to those decomposition techniques 
partitioning a mission into multiple phases leads to smaller state and action spaces in each
phase  though our motivation for mission phasing is to handle the constraints on policies
agents can execute rather than to reduce the computational cost during policy formulation.
Nonetheless, we can exploit these ideas.
Our algorithm for solving S-RMP optimization problems with predefined phase-switching
states is based upon abstract MDPs. An abstract MDP is composed of abstract states, each
of which corresponds to a mission phase. The action for an abstract state is the policy
used in its corresponding phase (which is conceptually similar to options, Sutton, Precup, &
Singh, 1999). It is here assumed that none of the constraints is associated with more than
one phase. The discussion of more general constraints is postponed to the next subsection.
Since it is assumed that agent constraints in one phase cannot be affected by policy
choices in another phase, the abstract MDP is an unconstrained MDP (at the abstract level)
428

fiResource-Driven Mission-Phasing Techniques

even though internally each phase is still a constrained MDP. The algorithm thus uses a policy
iteration approach at the abstract level together with an embedded MILP solver within phases.
The embedded MILP solver finds possible executable policies and their expected rewards
for each of the phases, while different policies may have different probabilities of reaching
the various phase-switching states at the edges of the phase. The outer policy iteration
algorithm at the abstract level iteratively searches for the combination of phase policies that
maximizes the reward across the whole mission.
The detailed procedure of the abstract MDP solver is illustrated below:
1. Partitioning the mission into phases.
When phase-switching states are given, partitioning a mission into multiple phases is
straightforward. Start from a phase-switching state, and then keep expanding through
all connected transitions until encountering other phase-switching states. The resulting
state space is the phase state space corresponding to that phase-switching state.6
2. Policy iteration.
The following policy iteration algorithm is adopted after the state space is partitioned.
(a) For each phase-switching state s, solve the MDP corresponding to the phase beginning in that state as if it were an unconstrained MDP, and compute state value
V (s). These V (s) are used as initial values of phase-switching states since they
provide informed (as opposed to random) estimates that in our experience tend to
work well, especially for under-constrained problems.
(b) In the abstract MDP, each phase is treated as an abstract state and each policy for
a phase is treated as an abstract action for that phases abstract state. The policy
iteration algorithm alternates between the following two steps:
Policy improvement: Rather than enumerating all possible policies (abstract actions) for a phase (abstract state), the algorithm uses a constrained MDP solver
(that was shown in Eq. 4) to calculate the optimal policy in the phase, given the
current values V (s) associated with the (outgoing) neighboring phase-switching
states.
Policy evaluation: Given abstract actions, calculate V (s) for each phase-switching
state s. For small state spaces, standard linear algebra methods are often the
best solutions for policy evaluation. For larger state spaces, a simplified value
iteration algorithm might be preferable (simplified because the policy in each
phase is fixed) (Puterman, 1994).
Unlike much best-response hill-climbing work, the above abstract MDP has fixed state
transition functions and fixed reward functions in both the abstract level and the phase level
because the agent enters a phase always at the same phase-switching state, which guarantees
the above policy iteration algorithm will return an optimal solution.
Theorem 3.2. The abstract MDP policy iteration procedure will converge to an optimal
solution.
6. Note that exploiting factored state representations can lead to other, more efficient (although generally only
approximate) algorithms for partitioning (Kim & Dean, 2001; Guestrin, Koller, Parr, & Venkataraman,
2003).

429

fiWu & Durfee

Proof: In each iteration, the new abstract policy is a strict improvement over the previous
one. Since the total expected reward of the abstract MDP is bounded (because the total expected reward of the corresponding unconstrained MDP is bounded), the iteration procedure
will eventually converge.
At the convergence point, both the phase MDPs and the abstract MDP satisfy the Bellman
optimality equation (because of the nature of the linear programming solver and the policy
iteration algorithm), indicating that the derived policy is an optimal policy. 
Running Example: Optimizing for Predetermined Phase-Switching States. We
now return to our running example introduced in Section 1.1 to illustrate how the total expected
reward can be improved if the agent can reconfigure its resources at some states. Let us say
that the agent is told that it is able to reconfigure resources and switch policies at states S1 , S3
and S4 . These three phase-switching states decompose the example problem into three phases.
The corresponding abstract MDP is constructed and shown in Figure 2, which is composed of
three abstract states.
Using the abstract MDP policy-iteration algorithm just described and using the same parameters as before (especially that an executable policy cannot have more than one action that
is not a noop), the state values of the phase-switching states eventually converge to
V (S1 ) = 113.65

V (S3 ) = 120.65

V (S4 ) = 123.05

The optimal policy in phase I is [S1  a1 , S2  noop] (with resource o1 ), the optimal policy in
phase II is [S2  noop, S3  noop, S5  a5 , S6  noop] (with resource o5 ), and the optimal
policy in phase III is [S2  noop, S4  noop, S5  a5 , S6  noop] (also with resource o5 ).
The total expected reward is now 113.65, which is 74.8% higher than the expected reward for
the constrained MDP case (where resource allocation only happens in state S1 ) thanks to the
additional phase-switching states.
Thanks to the policy iteration procedure, the abstract MDP solver generally converges
quickly. However, it should be noted that two limitations are inherent in the abstract MDP
solver. One of the limitations is that the abstract MDP solver requires that phase-switching
states are known a priori, which restricts its applicability (although we can combine it with
some phase-switching-state heuristic search techniques). The other limitation is due to the
possible existence of constraints running across multiple phases. The abstract MDP solver
cannot cope with constraints associated with multiple abstract states, such as restrictions
on the expected number of visits to a particular state that belongs to multiple phases. In
contrast, the general S-RMP solution algorithms that we present in the next section do not
have such limitations.
3.4 Determining Optimal Phase-Switching States
In a general S-RMP optimization problem, phase-switching states are not completely predetermined. Instead, given a defined set of eligible phase-switching states S , a set of costs {i }
(where i denotes the cost for making eligible state i into a phase-switching state), and a cost
0
limit
P , the objective of the agent is to find an optimal phase-switching set S  S subject
to iS 0 i  , along with optimal resource configurations and optimal executable policies
within each phase, to maximize its expected cumulative reward.
430

fiResource-Driven Mission-Phasing Techniques

Phase I
S1

Phase II

S2

S2

S3
S5
S4

S2
S6

S5
Phase III
S6

Figure 2: An abstract MDP with three phases.

As mentioned, the abstract MDP solver presented in Section 3.3 cannot be directly used for
the general S-RMP optimization problem. In this section, we construct a mixed integer linear
program, the solution to which yields the optimal set of phase-switching states maximizing
the total expected reward, as well as optimal resource configurations and executable policies
within each phase. We make a simplifying assumption that a state with a positive probability
j of being the initial state is always a phase-switching state. This assumption makes the
presentation clearer and the representation more concise, as well as sidestepping the question
of what the default agent policy might be (since that is what it would use if it could not
configure resources in an initial state).
Let xki,a be the expected number of times action a is executed in state i within phase k.
P P
P
Clearly, if state i is not reachable in phase k, then xki,a = 0. Let jk = a xkj,a  i a pi,a,j 
xki,a where pi,a,j is the state transition probability, then jk provides a way to characterize
transitions among phases. If state j is not a phase-switching state, then jk = 0 for any k,
P
) must
since within any phase the expected number of times of visiting state j ( a xkj,aP
P equal
the expected number of times of entering state
P j through all possible transitions ( i a pi,a,j 
xki,a ). If state j is a phase-switching state, k jk = j . Recall that j is the initial probability
P k
distribution for state j.
k j = j guarantees that the total expected number of times of
visiting state j must equal the initial probability distribution for state j plus the total expected
number of times of entering state j through all possible transitions.
Now, we can formulate the S-RMP optimization problem into a mixed integer linear
program, which is shown in Eq. 5. The formulation builds on the MILP for the constrained
given in Eq. 4, but incorporates phase information. The objective function
P P PMDP
k r
x
i,a in the MILP represents the total expected reward accumulated across
i
a
k i,a
431

fiWu & Durfee

all phases, where ri,a is the MDP reward function.

max

XXX
k

i

xki,a  ri,a

(5)

a

subject to:
probability conservation constraints:
X
XX
pi,a,j  xki,a
xkj,a = jk +
a

i

X

k
k
xi,a

jk = j

: j

0

: k, i, a

capacity constraints:
P P
k
i
a uo,a,i  xi,a
 ko
X
X
o,c  ko  c

o
ko

: k, j

a

: o, k
: c, k

 {0, 1}

: o, k

phase-switching constraints:
jk
 j
X
X
j  j  

: k, j

j

j  {0, 1}

: j

P P
P
 As stated above, the constraint a xkj,a = jk + i a pi,a,j xki,a models the conservation
of probability within each phase.

P k
 =  indicates the probability conservation constraint across
 The constraint
P P
P
P P
P k k jP Pj k
phases, i.e., k j = k ( a xj,a  i a pi,a,j  xki,a ) = a xj,a  i a pi,a,j  xi,a =
P
j , where xi,a = k xki,a is the total expected number of times action a is executed in
state i.
P P
P
uo,a,i xki,a
 The capacity constraints i a X
 ko and o o,c  ko  c arePa multi-phase
P
version of the capacity constraints discussed in Eq. 4, where X = max i a xi,a can
be computed by using Eq. 3.

432

fiResource-Driven Mission-Phasing Techniques

k

 j in the constraint Xj  j is a binary variable, where j = 1 when state j is a
phase-switching state, and j = 0 otherwise. We can prove X  sup jk as follows:
sup jk = sup(

X

xkja 

a

 sup

X

XX
i

paij xkia )

a

xkja

a



XXX
i

a

xkia

k

=X
k

Therefore, this constraint and the constraint j  {0, 1} guarantee that k : Xj >
0  j = 1, which implies that a state must be a phase-switching state if there is some
transition leakage at that state in any phase.
P
 The constraint j j  j   says that the cost of creating phase-switching states
must be no greater than the cost limit .
 Other constraints denote the ranges of variables. Note that there are no range restrictions for jk .
By definition, j and ko in the solution to Eq. 5 indicate phase-switching states and
resource configuration (within each phase), respectively. Once we have solved Eq. 5, we can
use the resulting values of xki,a to derive an optimal overall policy as follows.
1. Compute the optimal policy for each phase.
k =
This is the same as before  at state i, action a is executed with probability i,a
xk
P i,ak .
a xi,a

k } to denote the phase policy in
In the following discussion, we use  k = {i,a

phase k.
2. Determine the phase policy to adopt at a phase-switching state.
This is also trivial. The agent should choose phase policy  k with probability
phase-switching state i for maximizing its total expected reward, where xki =

xk
P i k at
P k xik
a xi,a .

Running Example: Selecting an Optimal Fixed Number of Phase-Switching States.
We now illustrate the solution algorithm on our running example depicted in Figure 1. Recall
that, as was shown in Section 3.3, when the agent is allowed to reconfigure its resources and
switch its policy at S1 , S3 and S4 , its total expected reward is 113.65 (higher than the reward
65.02 in one-shot constrained MDP case, but still much lower than the optimal reward 174.65
in the unconstrained MDP case). Rather than starting with predefined phase-switching states,
we now assume that 1 = 0, i{2,...,6} = 1, and  = 2. That is to say, two additional phaseswitching states besides the initial state S1 can be chosen by the agent from any states in the
system.
We use the same transition probability pi,a,j , reward ri,a , initial probability distribution j ,
resource requirement cost uo,a,i , capacity cost o,c , capacity limit c , and the constant value
433

fiWu & Durfee

X as in Section 1.1. The phase-switching costs i and the cost limit  are given above. The
optimal integer solution to the mixed integer linear program Eq. 5 is:
[1 , 2 , 3 , 4 , 5 , 6 ] = [1, 0, 1, 0, 1, 0]
fi 1 1 1 1 1 fi fi
fi
fi 1 , 2 , 3 , 4 , 5 fi fi 1, 0, 0, 0, 0 fi
fi 2 2 2 2 2 fi fi
fi
fi  ,  ,  ,  ,  fi = fi 0, 0, 1, 0, 0 fi
1
2
3
4
5
fi
fi
fi fi
fi 3 , 3 , 3 , 3 , 3 fi fi 0, 0, 0, 0, 1 fi
1
2
3
4
5

That is, the optimal set of phase-switching states is S 0 = {S1 , S3 , S5 }. Examining the
continuous variables xki,a (not shown here because there are too many of them) shows that
the total expected reward of the agent is 173.80, which is close to the optimal unconstrained
reward of 174.65. The derived solution is to choose resource o1 and adopt the policy [S1 
a1 , S2  noop] at S1 , switch to resource o3 and policy [S3  a3 , S4  noop] at S3 , and switch
to resource o5 and policy [S2  noop, S5  a5 , S6  noop] at S5 .
3.4.1 Variation: Maximizing the Total Reward, Accounting for Cost
This subsection demonstrates the extensibility of our MILP-based algorithm by showing how
easily it can be revised to work for another useful variation of the S-RMP optimization problem
where the phase-switching states are not predetermined (Section 3.3) nor is the cost of creating
phase-switching states bounded (Section 3.4). We now assume that any state could be a
phase-switching state, that as many states as desired could be phase-switching states, and
that (similarly as in Section 3.4) there is a cost associated with treating a state as a phaseswitching state. However, instead of being subject to some cost limits, these costs are now
calibrated with the rewards associated with executing policies. Now the optimization problem
is to maximize the total expected reward, accounting for the costs of creating phase-switching
states, without predetermining which are the phase-switching states or how many there will
be. As shown below, designing an algorithm for such problems is trivial. It is just a simple
mathematical reformulation of Eq. 5. The details are presented in Eq. 6 (which omits the
probability conservation and capacity constraints because they are unchanged from Eq. 5).
max

XXX
k

i

xki,a  ri,a 

a

X

i   i

(6)

i

subject to:
probability conservation constraints (unchanged)
capacity constraints (unchanged)
phase-switching constraints:
jk
 j
X
j  {0, 1}

: k, j
: j

where
cost for creating phase-switching state i, and the objective function
i represents the P
P P P
k
i i  i represents the total expected reward of the policy minus
k
i
a xi,a  ri,a 
the cost for creating phase-switching states.
434

fiResource-Driven Mission-Phasing Techniques

Case
unconstrained (Section 2.1)
one-shot (Section 2.3)
3 fixed phases (Section 3.3)
2 added chosen phases (Section 3.4)
unlimited phases balancing cost (Section 3.4.1)

Phase-Switching States
{s1 , s2 , s3 , s4 , s5 , s6 }
{s1 }
{s1 , s3 , s4 }
{s1 , s3 , s5 }
{s1 , s5 }

Expected Utility
-75.35
65.02
13.65
73.80
102.55

Table 1: Comparison of the solutions to the example problem, given that the cost of creating
each additional phase-switching state is 50.

Running Example: Selecting Optimal Phase-Switching States Based on Cost.
Let us revisit our running example to illustrate how the above algorithm can be used to solve
this variation of the S-RMP optimization problem. Suppose that 1 = 0 (assuming the initial
state is already a phase-switching state) and i = c for any other state. Using the above MILP
formulation (Eq. 6), we can find that when 0 < c  0.85 the optimal phase-switching states
are [S1 , S3 , S4 , S5 ], when 0.85 < c  21.25, the optimal phase-switching states are [S1 , S3 , S5 ],
when 21.25 < c  87.53, the optimal phase-switching states are [S1 , S5 ], and when c > 87.53
the optimal decision is not to create additional phase-switching states besides the initial state
S1 . As expected, the number of phase-switching states decreases as the cost of creating phaseswitching states increases.
As a specific example, when c = 50, the optimal set of phase-switching states is {S1 , S5 }.
The optimal resource configuration and executable policy in the phase initiated at S1 are
{o3 } and [S1  noop, S2  noop, S3  a3 , S4  noop], respectively; the optimal resource
configuration and executable policy in the phase initiated at S5 are {o5 } and [S2  noop, S3 
noop, S4  noop, S5  a5 ], respectively. The policy utility is 152.55 (reward)501 (cost) =
102.55.
To better understand and compare this solution with the example solutions derived in
the previous sections, Table 1 shows their solution utilities (where the utility is defined as
the expected reward of the policy minus the cost of creating phase-switching states). Not
surprisingly, the utility of the solution presented in this subsection is higher than the others
since it is derived by the algorithm (Eq. 6) that explicitly balances the costs and expected
benefits for creating phase-switching states.
3.4.2 Variation: Cost Associated with State Features
A final variation that we briefly describe, which has similarities to the multiagent approach
described later in Section 4, is the case where the conditions that enable resource reconfiguration (phase switching) are associated with a subset of the world features, rather than a fully
grounded state. As a simple example, consider the situation where resources (e.g., software
packages, control over a satellite, etc.) can be licensed/leased at particular times and for
particular intervals, such as hourly. That is, the agent will identify, at the start of each hour,
which resources (within its capacity constraints) to hold for the next hour. It could be in any
number of states (e.g., physical locations, pending task queue, etc.), but the resource reconfiguration can take place in any of them. Similarly, in the example where a robot reconfigures
435

fiWu & Durfee

resources at a toolbox, the critical feature for the phase switch is that it is in a state whose
location feature corresponds to a toolbox, regardless of other state features (e.g., direction it
is facing or battery level).
More generally, let us say that the MDP state space S consists of L disjoint subsets
S1 , S2 , ..., Sl , ..., SL , where each subset contains states that have identical values for the
phase-switching feature(s) (e.g., all states in Sl have clock time t). Thus, if any state within
Sl is a phase-switching state then all states in Sl are phase-switching states as well. Let
l denote the cost for making all the Sl states into phase-switching states, i.e., the cost to
enable phase switching in worlds where the critical feature(s) take on their common value(s)
in Sl , and let  denote the cost limit for creating phase-switching states. Clearly, this is
a generalization of the previous phase-switching constraint: when every Sl contains exactly
one state, this representation is equivalent to the phase-switching constraint R previously
presented in Section 3.1.
The new mixed integer linear program with the generalized phase-switching constraint is
formulated in Eq. 7, which again is very similar to Eq. 5, except for some minor revisions in
the portion of phase-switching constraints. As before, constraints unchanged from Eq. 5 are
not repeated.
XXX
max
xki,a  ri,a
(7)
k

i

a

subject to:
probability conservation constraints (unchanged)
capacity constraints (unchanged)
phase-switching constraints:
jk
 l
X
X
l  l  

: k, l, jSl

l

l  {0, 1}

: l

where binary variable l denotes whether Sl is a phase-switching set.
Running Example: Selecting Optimal Phase Switching Features. In our running
example, suppose now that the state space is composed of Sl=1 = {S1 }, Sl=2 = {S2 , S3 },
Sl=3 = {S4 , S5 }, and Sl=4 = {S6 }, and that l=1 = 0, l6=1 = 1, and  = 1. The solution to
Eq. 7 will yield a policy with a reward 165.68 using phase-switching states {S1 , S4 , S5 }, where
the spending of one unit of cost creates both phase-switching state S4 and phase-switching
state S5 .
3.5 Experimental Evaluation
To this point, we have described variations of single-agent resource-driven mission-phasing
problems and techniques for solving them, using a simple example to illustrate these ideas.
Ultimately, the significance of these techniques hinges on their computational efficiency in
solving problems that are more difficult. In this section, we give an empirical evaluation of
our techniques focusing on problems in a more complex state space and with a larger resource
436

fiResource-Driven Mission-Phasing Techniques

set. Our experiments are implemented on a simplified Mars rover domain simulation in which
an autonomous rover operates in a stochastic environment. Following much of the literature
on similar problems (Bererton, Gordon, & Thrun, 2003; Dolgov & Durfee, 2006), the Mars
rover domain is represented using a grid world.
3.5.1 Experimental Setup
The grid world (see Figure 3c) has some number of wall locations through which the rover
cannot move. Each of the other locations is associated with an execution resource, which,
if held by the rover, allows the rover to move with confidence and safety in that location to
its desired next location. For example, the resources could be sensors for the conditions in
different locations (dusty, foggy, overgrown, etc.), or different kinds of wheels (for navigating
sand, rocks, etc.). The rover agent can also move without holding the appropriate resource
in a location, but this will result in greater uncertainty in action outcomes (it could blunder
or slip, and thus arrive in a different location than desired) and possibly cause damage to the
rover, as detailed shortly.
In addition, there are multiple tasks randomly distributed in the grid world. When the
rover reaches a location that has a task, if the rover currently carries the task-required execution resource, the rover can choose to perform a do action (that carries out the task) and
receive a reward. Once any task is carried out, the mission is accomplished and the rover will
leave the system (the experimental run terminates).
Our experiences running a variety of experiments indicate that the trends in the results
presented in this section are not sensitive to exact parameter settings, but for the sake of
reproducibility, we describe the detailed parameters below. The procedure of building a
random grid world is illustrated in Figure 3. When a n  n grid world is built, 40% of the
locations are randomly chosen as wall locations, and 10% of the locations are randomly chosen
as task locations. To avoid simple test problems, we only use grid worlds whose number of
reachable locations (from the rovers starting location) is greater than half of the total number
of locations (i.e., greater than n2 /2).
At each task location, there is a task that could be accomplished by the rover and generate
a reward. To make the problem interesting and challenging, we distinguish tasks by setting
different rewards for them. We sort tasks by their Manhattan distances to the starting location
of the rover (the smallest distance first), and let the ith task have a reward i. Therefore, it is
not always true that the rover would desire and pursue high-reward tasks because low-reward
tasks are closer to the rover and might be easier and safer to complete.
The rover always starts at the S (START) location in the left bottom corner of the grid
(which is never assigned to be a wall), and its objective is to maximize its expected reward.
At each time step, the rover chooses an action in its action set {wait, up, left, down, right,
safe-up, safe-left, safe-down, safe-right, do}. Actions wait, up, left, down, and right can be
executed without requiring the rover to carry any particular resource. In contrast, performing
a safe-moving action safe-up, safe-left, safe-down, or safe-right in a non-wall location requires
a particular resource (related to that location), which is randomly uniformly selected from
resource set O when the problem is built. Analogously, performing action do at a task location
requires a particular resource that is also randomly uniformly selected. It should be pointed
out that performing an action needs at most one resource, but a resource may (by chance)
437

fiWu & Durfee

40% walls

START
location

S
(a)
T

T
10% tasks,
the closer to START,
the lower reward

T

T

T
T
T
S
(b)
T

T

1

T

T

for each non-task
location, a random
resource for movement

2
3

T

for each task location,
a random resource for movement
and a random resource for task

T
T
S
(c)

Figure 3: The procedure of creating a random grid world. (a) 40% of the locations are randomly chosen as walls. (b) 10% of the locations are randomly chosen for tasks. (c)
resource requirements are randomly set.

438

fiResource-Driven Mission-Phasing Techniques

enable the agent to move safely in multiple locations, and/or carry out multiple tasks. The
resource requirement information is known to the rover a priori.
The following lists the detailed action parameters used in our experiments:
wait can be executed in any non-wall location without requiring any resource. After the
execution of this action, the rover will stay at its current location with probability 0.95,
and be out of the system with probability 0.05 (e.g., running out of battery).
up, down, left, right can be executed in any non-wall location without requiring any resource. Each of these actions achieves its intended effect with probability 0.4, moves
the rover into each of the other three directions (except the intended direction) with
probability 0.1, keeps the rover in the current location with probability 0.1, and causes
damage to the rover (and then the rover is out of the system) with probability 0.2.
Furthermore, if the rover bumps into a wall, it will stay at its current location.
safe-up, safe-down, safe-left, safe-right can be executed only in a location whose required resource is currently held by the rover. Compared to an unsafe-moving action,
such a safe-moving action achieves the intended effect with a much higher probability
0.95, and fails (leaves the system) with a lower probability of 0.05. Similarly as before,
when the rover bumps into a wall, it stays at its current location.
do can be executed only for tasks whose required resources are currently held by the rover.
When action do is executed, the rover receives the reward of the task at its current
location, and leaves the system (since the mission is accomplished).
The capacity of the rover is restricted: the capacity limit is  , and carrying each resource
will incur one unit capacity cost. That is to say, the rover can carry no more than  resources.
We run experiments on a Core 2 Duo machine and use CPLEX 10.1 as our MILP solver. In
our experiments, each average data point is computed from 20 randomly generated problems.
3.5.2 Improvements to Solution Quality
We start the evaluation by showing the improved reward from using the phasing strategy
over the approach that does not consider the possibility of switching resources in the midst
of execution. Let us first consider the case where there are five supply stations distributed in
the environment (the first station is always at the START location and the remaining four
stations are randomly uniformly distributed in the non-wall locations when the problem is
generated). Other parameters are set as follows: n = 8, i.e., the size of the grid world is 8
by 8, and |O| = 9, i.e., there are nine different types of resources in the system. Figure 4
gives average rewards for these experiments, where the error bars here and in the graphed
results throughout this paper show the standard deviation. Clearly, exploiting the resource
reconfiguration opportunities (using the abstract MDP solver presented in Section 3.3) can
considerably improve the performance of the rover, e.g., receiving a reward on average about
40% higher than the reward when not taking advantage of the supply stations, when the rover
can carry only three resources.
Figure 4 also compares the performance of the rover between the case where the locations
of supply stations are randomly pre-selected and the case where the locations of the same
number of supply stations (i.e., five phases, given that i=ST ART = 0, i6=ST ART = 1, and
439

fiWu & Durfee

4.5

4

5 optimal phases

average reward

3.5

5 fixed phases

3

nonphasing

2.5

2

1.5

1

0.5

0

1

2

3

4

5

6

7

8

9

number of carried resources

Figure 4: Exploiting fixed phase-switching states increases the agents reward, and finding
optimal phase-switching states further increases the reward.

4.5

4

average reward

3.5

3

2.5

2

1.5

1

0.5

0

1

1.5

2

2.5

3

3.5

4

4.5

5

5.5

6

number of phases

Figure 5: The reward increases as the number of phases increases.

 = 4) can be determined by the rover itself. As expected, finding optimal phase-switching
states (which can be done by using the MILP algorithm presented in Section 3.4) is valuable
in tightly constrained environments. For example, on average, it yields a reward about 46%
higher than the approach that randomly selects phase-switching states when the number of
carried resources is limited to  = 3.
Figure 5 examines the effectiveness of the resource-driven mission-phasing approach from
another perspective, showing the average reward of the rover as a function of the number of
phase-switching states that can be built in the environment (with other system parameters
n = 8, |O| = 9, and  = 3). We can see that (as expected) breaking the mission into multiple
phases can significantly improve the total expected reward of the constrained rover. For
440

fiResource-Driven Mission-Phasing Techniques

example, setting up two additional supply stations in the 8  8 grid world environment (and
so breaking it into three phases) can almost double the average reward that the rover can
gain without using phasing.
3.5.3 Computational Efficiency
A major objective of the work presented in this section is the design of a computationallyefficient solution approach for the S-RMP optimization problem. Section 3.2 has given a
theoretical analysis of the computational complexity of the S-RMP optimization problem;
this subsection is intended to empirically evaluate the efficiency of the solution approach
presented in this section in solving complex S-RMP optimization problems. To make the
presentation concise, only the runtime performance of the MILP-based algorithm described
in Section 3.4 is shown, i.e., focusing on the standard S-RMP optimization problem defined
in Section 3.1.7
The two prior straightforward algorithms described in Section 3.2 (the brute-force search
approach based upon enumeration and the MDP-based approach incorporating resource features in the MDP state representation) can also be used to solve the S-RMP optimization
problem. Enumerating all decompositions, and then, for each, enumerating all possible resource configurations and reconfigurations can be thought of as a (very slow) brute-force
search algorithm for our formulated MILP. Therefore, we do not report its empirical results,
since state-of-art MILP solvers (such as CPLEX which we use) usually follow more sophisticated branch-and-bound (B&B) strategies, and it is well established in the mathematical
programming literature that the B&B approach is, in general, significantly better than the
straightforward brute-force search (in both the runtime for finding an optimal solution and
the anytime performance of finding a good solution). Our search of the Artificial Intelligence
and the Operations Research literatures indicates that the MDP-based approach is the only
existing approach (besides the brute-force search) that is directly applicable to the S-RMP
optimization problem. We will thus focus on comparing our MILP-based algorithm and the
MDP-based algorithm in the following discussion.
In the MDP-based algorithm, a new drop-all action and |O| new pick-one actions
are added into the original action space (instead of adding 2|O| resource reconfiguration
actions). That is to say, rather than performing resource reconfiguration in one step, the
agent now switches to a new bundle of resources by first implementing a drop-all action
and then sequentially performing pick-one actions until it has all its desired resources. Our
experience is that this revised algorithm is more computationally efficient than the version
with the exponential-size action space.
Note that the MDP resulting from incorporating resource features in the state representation is still a constrained MDP because phase-switching constraints place restrictions on
which states resource-reconfiguration-related actions can be performed in. The constrained
MDP solver (Eq. 4) has been shown to be efficient in solving large constrained MDPs (Dolgov,
2006), and so this work uses it for solving such remodeled constrained MDPs.8
7. Our experiments (Wu, 2008) also show that the trends of results for other variations of the S-RMP optimization problem are similar to those described in this subsection.
8. In the MILP formulation for solving the remodeled constrained MDP, the number of binary variables equals
the number of states specified in the S-RMP problem definition. That is, the runtime of the MDP-based
algorithm is exponential to the input size but not doubly exponential.

441

fiWu & Durfee

120

runtime (seconds)

100

80

60

40

20

0

1

2

3

4

5

6

7

8

9

10

number of phases

Figure 6: The runtime increases and then decreases as the number of phases increases.

To provide a better idea about the computational complexity of our experimental domain
and solution techniques, we begin by showing what a hard resource-driven mission-phasing
problem is, particularly along the dimension of the number of phases that can be created. We
use the same parameters as in Figure 5, but analyze runtime instead. The results are shown in
Figure 6, which demonstrates how the running time for deriving an optimal S-RMP solution
varies as the number of supply stations that can be created in the environment increases. In
the figure, the solid line shows the average, error bars show standard deviation, and each data
point, which is shown as a , corresponds to a single run.
As shown in the figure, the running time is low when the number of phases is small, and
it gradually increases as the number of phases increases. This is not surprising, because the
number of variables (both continuous variables and binary variables) in the MILP formulation
is linear in the number of phases. However, the interesting observation is that, after some
point, the runtime starts to decrease although the size of the MILP still keeps increasing.
We believe this is because, when the number of allowable phases is large, there are several
different ways to set up phase-switching states while achieving the same maximum reward. In
other words, the S-RMP optimization problem with a large number of phase-switching states
becomes under-constrained, and might have a large number of different optimal solutions.
The MILP-based algorithm presented in this work can effectively exploit this property, and
reduce computational costs. Based upon this complexity profile, to highlight the ability of
solving hard problem instances, the following experiments set the phase-switching cost limit
 to 2 (except in the case where we examine how the running time changes as the number of
phases increases). This means that there can be up to three phases in the system, assuming
that creating each additional phase-switching state incurs one unit cost.
Figure 7 compares the average time for finding an optimal solution between our MILPbased algorithm and the standard MDP-based algorithm relative to the number of phases 
(top-left figure), the number of carried resources c (top-right figure), the number of resource
types |O| (bottom-left figure), and the size9 of the grid world n (bottom-right figure). We can
9. Recall the grid world is size n  n.

442

fiResource-Driven Mission-Phasing Techniques

500
MDPbased
MILPbased

runtime (seconds)

runtime (seconds)

500
400
300
200
100
0

1

2

3

4

5

6

MDPbased
MILPbased
400
300
200
100
0

7

number of phases

3

4

5

6

7

500
MDPbased
MILPbased

runtime (seconds)

runtime (seconds)

2

number of carried resources

500
400
300
200
100
0

1

4

6

8

10

MDPbased
MILPbased
400
300
200
100
0

12

number of resource types

5

6

7

8

9

10

size of grid world

Figure 7: Runtime comparison between the MILP-based algorithm and the MDP-based algorithm. The MILP-based algorithm finds an exact solution to a S-RMP optimization
problem faster than the standard MDP-based algorithm. Parameters are set as follows. Top-left figure: n = 8,  = 3, |O| = 9,  = {0, 1.., 6}. Top-right figure: n = 8,
 = {1, ..., 7}, |O| = 9,  = 2. Bottom-left figure: n = 8,  = 3, |O| = {3, 4, ..., 12},
 = 2. Bottom-right figure: n = {5, 6, ..., 10},  = 3, |O| = 9,  = 2.

see that our MILP-based algorithm is in expectation considerably faster than the MDP-based
algorithm, particularly in complex problem instances.
In the top-left figure, the results of the MILP-based algorithm are the same as those shown
in Figure 6, which have already been discussed. Notably, unlike the other three figures, the
curve of the MDP-based algorithm in this figure does not monotonically increase as the value of
the input parameter increases. This is because the input parameter in this figure, the number
of phases, does not affect the size of the state space of the expanded MDP. Furthermore,
the constrained MDP method (Eq. 4) used to solve the expanded MDP can exploit problem
structure when the problem becomes under-constrained. This explains why the running time
decreases after some point (but the time is still much higher than that of the MILP-based
approach).
The top-right figure also demonstrates a trend for the running time of the MILP-based
algorithm decreasing after the value of the input parameter (i.e., the number of resources
that can be carried by the rover) is above a particular threshold. The reason is the same as
443

fiWu & Durfee

that used to explain Figure 6  the MILP-based algorithm, where the MILP solver utilizes
branch-and-bound, can effectively discover and exploit the fact that the problem becomes
under-constrained. In contrast, the MDP-based algorithm incorporating resource features
into the state representation leads to a MDP whose size grows very rapidly as the number
of resources that can be carried increases, and thus results in a significant increase in the
running time.
As illustrated in the bottom-left figure and the bottom-right figure, the average runtime
of the MILP-based algorithm also increases considerably more slowly than the MDP-based
algorithm, although, unlike the top-left and top-right figures, the runtime monotonically increases as either the number of resource types or the size of the grid world increases. This is
because, in general, the increases of these two parameters will not make the problem become
under-constrained by themselves.
The reason for the significant reduction in computational cost is that our MILP-based
approach can formulate the S-RMP optimization problem in a compact (as opposed to exponential) formulation, which paves the way for taking advantage of state-of-the-art MILP
solvers to effectively solve the coupled problems of problem decomposition, resource configuration, and policy formulation. It is important to emphasize that the MILP-based approach
uses no approximation techniques (and so it will find optimal solutions). The compactness
of the formulation is because the MILP-based approach folds the process of solving a NPcomplete S-RMP problem into the process of solving a NP-complete MILP (where the MILP
can be solved efficiently by state-of-the-art solvers).
Specifically, the MDP-based approach models resources in the MDP representation regardless of valuations of subsets of the resources, and then it reasons over the generalized
MDP to determine an optimal way of configuring and reconfiguring resources. In contrast,
our MILP-based solver finds an exact S-RMP solution by taking advantage of the embedded
branch-and-bound MILP method to discard subsets of fruitless candidate solutions (through
upper and lower estimated bounds). Although the MILP-based approach and the MDP-based
approach have similar worst-case runtimes, i.e., requiring exponential time to enumerate all
possible ways of sequentially configuring resources (which is reasonable because S-RMP is NPcomplete), the average-case performance of the MILP-based approach is often much better
than the MDP-based approach because of the effectiveness of the branch-and-bound algorithm
for pruning suboptimal solutions. This is particularly significant in cases where suboptimal
decompositions can be detected easily and early because a large number of possible resource
configurations and executable policies can then be discarded without much computational
effort.
3.6 Summary
To this point, we have analyzed several variations of a single-agent resource-driven missionphasing problem, corresponding to several cases of phase-switching constraints, and presented
a suite of computationally efficient algorithms for finding and using mission phases. We
have shown through analysis and experiments that our approach can considerably reduce the
computational cost for finding an exact solution to a complex S-RMP optimization problem
in comparison with prior approaches. In the remainder of this paper, we will extend such
techniques into multiagent stochastic systems.
444

fiResource-Driven Mission-Phasing Techniques

4. Resource Reallocation in Multiagent Systems
Additional complications arise when an agent that is deciding which resources to hold is
part of a multiagent system, because of potential competition for scarce, shared resources.
For example, there might be only be a few satellites to remotely control to acquire desired
images, or a small number of licenses to simultaneously run a software package. An individual
agent might be unable to procure all of its desired resources (even when its capacity does not
restrict the amount of resources it can hold) because some other agents may want those
resources as well. For cooperative agents, an optimal allocation will distribute resources to
agents so as to maximize the agents aggregate reward, meaning that a shared resource (such
as control of a satellites sensor) should be given to the agent that can use it best, given its
goals, potential actions, and other resources possessed.
This problem is in general doubly-exponential. Not only could each agent need to consider
exponentially many combinations of resources that it might possess, but then collectively the
agents could need to consider the exponentially many joint combinations of their individual
combinations, filtering out those that exceed shared resource constraints, and returning the
best of those that remain. Dolgov and Durfee (2005, 2006) showed that a much more efficient
algorithm is possible that exploits problem structure for the case where an agents valuation
for a resource bundle is based on the expected value for an MDP policy that utilizes the
actions made possible by holding the resources in the bundle. We will summarize that work
in Section 4.2, but the approach is similar to their solution method for resource assignment to
a single agent with capacity constraints described in Section 2.3, and like that work solves the
one-shot allocation problem. Hence, like that work it does not consider the possibility that
the agents might be able to redistribute resources among themselves in the midst of mission
execution.
This section thus focuses on solving sequential resource redistribution (reallocation) problems, along with the problem of optimizing agents policies for the execution phases between
redistribution events, by building on ideas from Dolgov and Durfees work as well as the SRMP techniques presented in the previous section. The remainder of this section thus largely
follows a parallel structure to the preceding S-RMP presentation. We begin (Section 4.1) by
introducing a simple problem that we will use for running examples through the remainder
of the section. Then we will summarize (and using the example illustrate) the prior work of
Dolgov and Durfee (2005, 2006) on one-shot allocation in Section 4.2, and define the sequential multiagent resource-driven mission phasing problem in Section 4.3. After analyzing the
problems complexity (Section 4.4), we then consider a sequence of variations on the problem
(again paralleling the S-RMP description), beginning with the case where the phase-switching
states are pre-defined (Section 4.5) and then where they can be chosen (Section 4.6), and for
each, present, analyze, and illustrate solution algorithms. Both efficiency and optimality of
our techniques are empirically evaluated in Section 4.7. Finally, Section 4.8 summarizes the
contributions of the work presented in this section.
4.1 A Multiagent Example
We here describe a simple multiagent resource-allocation example problem that we will use
to illustrate the various solution approaches throughout this section. In this example, two
cooperative agents attempt to maximize their total expected reward over a ten time step
interval. Each agent has three tasks. At each time step, an agent can choose to continue its
445

fiWu & Durfee

Task 1
----------------------

Task 2
----------------------

Task 3
----------------------

Reward:
RL :
DL:
Resource:

Reward:
RL :
DL:
Resource:

Reward:
RL :
DL:
Resource:

10
1
4

1

12
2
10

2

28
5
8

1

2

Task 1
----------------------

Task 2
----------------------

Task 3
----------------------

Reward:
RL :
DL:
Resource:

Reward:
RL :
DL:
Resource:

Reward:
RL :
DL:
Resource:

26
1
7

1

2

6
3
8

1

12
6
10

2

Figure 8: A simple two-agent example.

previously started task (if there is one and if the required resources are still assigned to that
agent), to start a new task (and abort its current task if there is one), or simply to do nothing.
In addition, we say that a task that has been aborted previously (and thus has failed) can be
re-tried, but no task can be accomplished more than once.
Figure 8 shows the detailed information of the tasks in the example problem, including
release (RL) time (i.e., the earliest time the task can be started successfully), deadline (DL)
(i.e., the latest time the task can finish successfully), reward if the task completes successfully,
and resource prerequisites. For example, agent 1 can start (or continue) its task 1, which
will incur a reward 10 if accomplished, at any time step within the interval [1, 4) given that
it has one unit of resource 1 at that time. To concentrate on the multiagent issues, for this
problem we will assume that each agent has sufficient capacity to carry both resources 1 and
2. The uncertainty in this problem is in the amount of time required to execute a task. Here,
we say that, if an agent starts a task and does not abort it during execution, then the agent
has probability 0.3, 0.4, and 0.3 of accomplishing it within one, two, and three time steps,
respectively.10
When there are multiple instances each of resources 1 and 2, then this problem degenerates
to two independent unconstrained MDP problems, one for each agent. As illustrated in
Figure 9, each agent will hold both resources it needs throughout the full time interval. Using
a standard policy formulation algorithm (e.g., value iteration), we can easily compute the
optimal unconstrained policy for each agent, which yields a total expected reward across the
two agents of 93.64.
4.2 Background: Integrated Resource Allocation and Policy Formulation
Stochastic planning in multiagent environments is typically much more challenging than in
single-agent environments, particularly when each agent has only a partial view of the global
environment. Previous complexity analyses have shown that the general decentralized Markov
decision process (Dec-MDP) is NEXP complete (Bernstein, Zilberstein, & Immerman, 2000;
10. This problem representation is a simplification of the TAEMS modeling approach (Lesser, Decker, Wagner,
Carver, Garvey, Horling, Neiman, Podorozhny, Prasad, Raja, Vincent, Xuan, & Zhang, 2004; Wagner,
Raja, & Lesser, 2006) as used in the DARPA COORDINATORS project (Wagner, Phelps, Guralnik, &
VanRiper, 2004; Wu & Durfee, 2007b).

446

fiResource-Driven Mission-Phasing Techniques

1

1

1

1

1

1

1

1

1

1

2

2

2

2

2

2

2

2

2

1

1

1

1

1

1

1

1

1

2

2

2

2

2

2

2

2

2

2

3

4

5

6

7

8

9

10

Figure 9: Optimal resource allocation when resources are unlimited.

Goldman & Zilberstein, 2004). Fortunately, in many application domains, the actions taken
by one agent may not impact other agents transitions. For example, when a few delivery
robots operate in a large open environment, interactions may be rare and easily avoidable.
The development of efficient algorithms for such loosely-coupled systems has gained much
attention among many researchers (e.g., Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling, Dean,
& Boutilier, 1998; Becker, Zilberstein, Lesser, & Goldman, 2004; Dolgov & Durfee, 2005).
Our approach specifically draws upon the prior approach designed by Dolgov and Durfee
(2005, 2006), which extends their work summarized in Section 2.3 to the multiagent resource
allocation case. Their work assumes that a group of cooperative agents are coupled through
sharing resources (i.e., actions selected by one agent might restrict the actions available to
others), but that the actions executed by one agent cannot impact the rewards and transitions
of others. As is typical in the resource-allocation research literature, the work also assumes
that, once the resources are distributed, the utility that each agent can achieve is only a
function of its own resource assignment and does not depend on what resources are given to
other agents and how they use their resources.
A multiagent constrained MDP with scarce shared resources can be represented as the
tuple hM, , Ci, as next specified. Note that the specification essentially represents an independent instance of a constrained MDP (Section 2.3) for each agent, except that O (the set
of resources) and  (the bounds on the number copies of each resource) are shared across all
agents.
 M = {Mm } is a set of classical MDPs, where Mm represents agent ms MDP, modeled
m
in the same way as described in Section 2.1. That is, Mm = hS m , Am , {pm
i,a,j }, {ri,a }i
m
m
where S is the finite state space of agent m, A is the finite action space of agent m,
pm
i,a,j is the probability that agent m reaches its state j when it executes action a in its
m is the reward agent m receives when it performs action a in its state i.
state i, and ri,a
  = {m } specifies the initial probability distribution over states for each agent m,
where im is the probability that agent m is initially in its state i.
 C represents resource constraints on agents in the system, which can be represented as
hO, C, U, , , i where:
447

fiWu & Durfee

 O is the finite set of shared, indivisible, non-consumable execution resources.11
 C = {C m } specifies the finite set of capacities for each agent m.
 U = {U m } gives the resource requirements for each agents actions, where the
binary parameter um
o,a,i  {0, 1} indicates whether agent m requires resource o to
execute action a when it is in its state i.
  = {m } represents the capacity costs for each agent m.
 } captures the capacity limits for each agent m.
  = {m
  = {o } specifies (shared) resource limitations, where o is the maximum number
of copies of resource o that can be distributed to agents in the system.
Running Example: Multiagent Constraint Formulation. In the simple running example from Section 4.1, the agents constraint components C are summarized below, where 
represents any state and unspecified values of u are zero.
 O = {o1 , o2 }.
 C = {{hold}, {hold}}.
 U = {{u1o1 ,a1 , = 1, u1o2 ,a2 , = 1, u1o1 ,a3 , = 1, u1o2 ,a3 , = 1}, {u2o1 ,a1 , = 1, u2o2 ,a1 , =
1, u2o1 ,a2 , = 1, u2o2 ,a3 , = 1}}.
  = {{o11 ,chold = 1, o12 ,chold = 1}, {o21 ,chold = 1, o22 ,chold = 1}}.
  = {{c1hold = 2}, {c2hold = 2}}.
  = {o1 = 1, o2 = 1}.
The algorithm devised by Dolgov and Durfee (2006) is presented below in Eq. 8, for clarity
leaving out the capacity constraints to focus on the multiagent constraints. Analogously to
the single-agent case, the continuous variable xm
i,a represents the expected number of times
agent m executes action a in its state i, and the binary variable m
o represents whether one
unit of resource o is assigned to agent m prior to execution.

max

XXX
m

i

m
xm
i,a  ri,a

(8)

a

11. For simplicity, we will here assume that all resources are shared. If agents can also draw from a cache of
private resources then this is a straightforward extension but would add unnecessary complication to the
presentation.

448

fiResource-Driven Mission-Phasing Techniques

subject to:
probability conservation constraints:
X
XX
m
m
xm
pm
j,a = j +
i,a,j  xi,a
a
xm
i,a

i

: m, j

a

0

: m, i, a

resource constraints:
P P m
m
i
a uo,a,i  xi,a
 m
o
X
X
m
m
o,c
 m
o  c

: m, o
: c, m

o

X

m
m
o

m
o = o

: o

 {0, 1}

: m, o

P P P
m
 The objective function m i a xm
i,a  ri,a represents the sum of cumulative rewards
among all agents, based upon the assumption that the agents are coupled only through
resources (i.e., actions taken by one agent will not impact other agents rewards and
transitions).
P
P P m
m
m
 The constraint a xm
j,a = j +
i
a pi,a,j xi,a guarantees probability conservation at
every state for every agent, which is a multiagent version of the probability conservation
constraint in the single-agent MDP formulation (Eq. 1).
P P
 X is a constant equal to or greater than sup i a xm
i,a (where in a finite horizon MDP,
X can be set to the finite horizon
T
since
each
agent
can only execute T actions within
P P m
m
i
a uo,a,i xi,a
m
 o implies that xm
that horizon). The constraint
i,a must be zero (i.e.,
X
=
1
(i.e., agent m must
action a cannot be executed by agent m in state i) when um
o,a,i
m
have resource o to execute actionPa P
in its state i) and o = 0. xm
i,a is unrestricted
m by definition.

x
otherwise since X is no less than i a um
i,a
o,a,i
P
 The constraint m m
o = o guarantees that the total amount of resource o allocated
across all agents must equal the amount of available resource o (assuming the resources
will
P bemcompletely assigned). This constraint can be easily relaxed to the constraint
m o  o by introducing an additional dummy agent to keep unallocated resources.
The optimal joint policy can be easily derived from the solution to the above MILP in a
similar way to that discussed in Section 2.2. That is, to maximize the total expected reward
xm
of the group of agents, agent m should choose a with probability P i,a
m when it is in its state
a xi,a
i.
Running Example: Multiagent Constrained MDP Solution. Continuing the simple
running example from Section 4.1, we can apply the above solution technique to find an optimal
allocation of resources 1 and 2 at the outset to the agents assuming that there is only one
copy of each available. Eq. 8 finds that the optimal one-shot allocation is to give all resources
to agent 1 and let agent 2 idle over the entire execution, as shown in Figure 10, and the total
449

fiWu & Durfee

1

1

1

1

1

1

1

1

1

1

2

2

2

2

2

2

2

2

2

2

3

4

5

6

7

8

9

10

Figure 10: Optimal one-shot resource allocation when resources are scarce.

expected reward is 49.64 in this case, much lower than the reward (93.64) in the unconstrained
case.
4.3 Multiagent Resource-Driven Mission Phasing Problem Definition
The Multiagent Resource-driven Mission Phasing (M-RMP) problem is the sequential version
of Dolgov and Durfees (one-shot) multiagent resource allocation problem as just described.
A general multiagent mission-phasing problem can be solved exactly by using the S-RMP
solution approach presented in Section 3 on the joint state and action spaces of the interacting
agents (assuming that each agent has a full view of the joint state), but such a solution
methodology would suffer from the curse of dimensionality since the sizes of the joint state
and action spaces grow exponentially with the number of agents. Thus, we will exploit the
loose-coupling assumption: agents only interact through their (now potentially repeated)
contention for shared resources, and are otherwise transition and reward independent.
In the multiagent case of sequential resource (re)allocation, each phase corresponds to a
particular distribution of shared resources among the agents. We will say that the transition
from one phase into another occurs when all of the agents reach a state where they relinquish
possession of the resources they currently hold, and then acquire their resources for the next
phase.12 However, unlike the single-agent case where the agent knows when it has reached
a phase-switching state, in the multiagent case each agent can only observe its local state,
and so detecting a joint phase-switching state would require agents to communicate state
information and individually track the full joint state. This in turn means that each would
need to have a policy that maps the (exponentially larger space of) joint states to local actions,
effectively introducing transition and reward dependencies and exploding the complexity of
the problem.
Our work instead has the agents exploit a deterministically-changing feature of the joint
state that they all can observe: a (synchronized) clock. Phase switchingredistributing
resources such as software licenses or control of a satellites sensorsoccurs at pre-arranged
(and preferably carefully chosen) times. We already saw in Section 3.4.2 a (single-agent) RMP
12. While in principle different subsets of agents could swap resources among themselves, in this paper we only
consider the case where all agents engage in swaps, though of course since an agent can acquire the same
resources it relinquishes the effect could be that only a subset of the agents is materially involved in any
given swap.

450

fiResource-Driven Mission-Phasing Techniques

variation where phase-switching states were grouped based on the value(s) of a (subset of)
feature(s). M-RMP builds on that variation, effectively partitioning the exponentially-sized
joint state space into subsets based on the states time feature, and determining the time(s)
at which resource (re)distributions will take place, no matter what the other details of the
agents states are or whether they are all finished using their currently-held resources yet.
Phasing based only on time has both advantages and limitations. One key advantage is
that all it requires that agents know at runtime about each other is that they all know what
time it is (and that this is common knowledge). In domains where reliable mutual observation
or communication at runtime is impractical (for example, in some military operations), synchronizing actions based on clock time has long been the norm. A second advantage is that a
future time is guaranteed to be reached. In contrast, if our agents need other conditions to be
met to exchange resources (for example, they need to be in the same location and/or all be
idle), then in some applications it might be impossible to guarantee that such a state will ever
occur. Or to force such cases to occur, agents would need to know more about each others
states (where others are and/or how soon their current tasks will end) to reach a hand-off
point. The M-RMP techniques we describe can in principle be extended to such cases, but in
practice the greater the need for agents to have global state awareness, the lesser we expect
such problems to exhibit the kinds of structure that M-RMP exploits.
For this paper, we also assume that resource redistributions always succeed (resources do
not somehow get misplaced during a transfer); our discussion of future work (Section 6.2)
will talk about the implications of relaxing this assumption.
Based on the preceding, we now formally define the multiagent resource-driven missionphasing (M-RMP) problem. It is a generalization of the multiagent constrained MDP described in Section 4.2, and is a tuple hM, , C, and Ri, where the components M, , and C
are as defined in Section 4.2, and R is defined as follows.
 R specifies constraints on resource reallocation. We capture the efforts required for such
resource reallocation activities as costs h{t }, i:13
 {t } indicates resource reallocation costs, where t denotes the cost for reconfiguring the resource assignment at time t. Note that t is only associated with time t
regardless of what resources and how many of them are reassigned. A variation of
resource reallocation constraints where the reallocation cost depends on the amount
of resources being transferred will be discussed and analyzed in Section 4.6.1 after
presenting the solution algorithm to the M-RMP optimization problem defined in
this section.
  specifies the limit on the amount of cost that could be spent in resource reallocation. For example, t=any time = 1 and  = 4 means that at most four resource
reconfiguration events could be scheduled during a particular mission execution.
Running Example: M-RMP Formulation. Continuing the simple running example
from Section 4.1 and building on the encoding from Section 4.2, the agents reallocation constraints R are summarized below, assuming the agents can reallocate resources three times
besides at the initial time 1.
13. This is different from buffer pool research (Lehman & Carey, 1986; Sacco & Schkolnick, 1982), which often
assumes that buffer size can be changed immediately and free of charge.

451

fiWu & Durfee

  = {1 = 0, i = 1 : 2  i  10}.
  = 3.
The objective of the M-RMP optimization problem is to maximize the total expected
reward of a group of agents within a finite time horizon by judiciously reallocating the limited, shared resources among the agents over time. Although much simpler than a general
decentralized MDP problem, such an automated multiagent mission-phasing problem is still
computationally challenging because it needs to determine not only how to initially allocate
limited shared resources, but also when to reallocate resources, what the best way of reallocating resources at those times are, and what the best executable policies with respect to the
reallocated resources are. As in S-RMP, these three component problems  mission decomposition, resource allocation, and policy formulation  are strongly intertwined. The utility
of decomposing a problem into phases and the utility of allocating resources for each phase
are unknown until executable policies are formulated and evaluated, but the policies cannot
be formulated until the phases are built and the resources are allocated.
4.4 Computational Complexity Analysis
This section starts by theoretically analyzing the computational complexity of the M-RMP
optimization problem.
Theorem 4.1. M-RMP optimization is NP-complete.
Proof: It is trivial to prove that the M-RMP optimization problem is NP-hard. Given that
its special case  one-shot resource allocation and policy formulation  can be proven to be
NP-complete through a reduction from the KNAPSACK problem (Dolgov, 2006), M-RMP
optimization is NP-hard.
Given a solution to the M-RMP problem, the satisfaction of resource constraints and
resource reallocation constraints can be verified in linear time. After that, for each agent,
incorporating its policy into its MDP model, the M-RMP optimization problem becomes a
Markov chain, which can be solved in polynomial time. That is, M-RMP optimization is in
NP.
With both NP and NP-hard, M-RMP optimization is NP-complete. 
Given this result, it is not surprising that the prior approaches (summarized below) that
could be applied to finding an exact solution to the M-RMP optimization problem are not
computationally efficient.
Decentralized MDP. Modeling resources into the MDP state representation and formulating resource-reconfiguration activities as actions is a possible (but slow) way to solve
a S-RMP problem (Section 3), but is generally infeasible for the M-RMP problem.
Since the outcome of an agents resource-reallocation action (e.g., acquiring a resource)
depends on whether another agent takes a corresponding action (e.g., releasing a resource) before or at the same time, the resulting Decentralized MDP (Dec-MDP) is
not transition independent. A general Dec-MDP is NEXP-complete (Bernstein et al.,
2000), meaning the M-RMP involves solving a NEXP-complete problem with an input
exponential in the number of resources.
452

fiResource-Driven Mission-Phasing Techniques

Combinatorial and stochastic optimization. Although each phase in a M-RMP problem is a one-shot resource-allocation and policy-formulation problem, directly using the
integrated combinatorial and stochastic optimization approach (Section 4.2) to solve
each phase independently and then piecing these phase policies together is, in general,
infeasible. Besides having to enumerate all possible decompositions, solving each phase
independently requires knowing the initial state probability distribution jm for Eq. 8.
Unfortunately, jm of a phase generally depends on the policy of its preceding phase, but
the policy of a preceding phase usually can only be optimized with respect to already
knowing the expected utilities attainable in the current and future phases.

Auction-based resource allocation. In a resource allocation approach based on using auctions (Pekec & Rothkopf, 2003; De Vries & Vohra, 2003), each agent bids a set of valuations over its possible sequential resource assignments to a central auction, which then
decides how to sequentially allocate resources among the agents. Unfortunately, this
approach does not scale. For example, if a group of m = 5 agents wants to maximize
the total expected reward within T = 10 time steps, (re)distributing o = 5 shared resources at most k = 3 times (twice after the initial allocation), then each agent needs to
t1
solve Ck1
 (2)ok = 1, 179, 648 non-trivial problems to evaluate all possible sequential
resource assignments. Then, the auction faces a winner determination problem (WDP)
where each of the 5 agents submits 1, 179, 648 bids  a daunting task.

As in the S-RMP problem, our solution is to formulate the problem so as to simultaneously solve the coupled problems of mission decomposition, resource allocation, and policy
formulation, to exploit interactions among them and to reduce computational cost.
4.5 Exploiting a Fixed Resource Reallocation Schedule
As in Section 3.3, we begin with a simple variant of the problem, where the schedule of
reallocating resources is predetermined, i.e., resource reallocation cost t = 0 if time step t is
specified in a predefined schedule, t > 0 otherwise, and the cost limit  = 0.
Section 4.4 explained why directly applying the (one-shot) integrated combinatorial and
stochastic optimization approach to each phase independently and then piecing phase policies
together is generally infeasible. Our approach instead links the phases together by modeling
transition probability conservation. The details are given in the following MILP. Note that,
to highlight M-RMPs emphasis on resource (re)allocation, we continue here and throughout
the remainder of this section to omit the capacity constraints.

max

XXX
m

i

a

453

m
xm
i,a  ri,a

(9)

fiWu & Durfee

subject to:
probability conservation constraints:
X
XX
m
m
xm
pm
j,a = j +
i,a,j  xi,a
a
xm
i,a

i

0

: m, i, a

resource constraints:
P m
P
m
a uo,a,i  xi,a
iSk
 m,k
o
T
X
m,k
= o
o

m
m,k
o

: m, j

a

 {0, 1}

: k, m, o
: o, k
: k, m, o

The probability conservation constraints are the same as in Eq. 8, while the resource
constraints are now associated with (superscripted by) the phase k.
indicate whether
or not agent m is assigned one
Phase-specific binary variables m,k
o
P
= o says that the amount of
unit of resource o during phase k. The constraint m m,k
o
resource o allocated in any phase k must equal the amount of o available (again, we assume a
m,k
are linked with the constraint
dummy agent can hold any unwanted resources). xm
i,a and o
P

xm
i,a

P

m
um
o,a,i xi,a
T

 m,k
(where Sk represents the set of states within phase k). That is,
o
 0 (i.e., action a is not executable in state i by agent m within phase k) if um
o,a,i = 1 (i.e.,

iSk

a

a requires resource o) and m,k
= 0 (i.e., agent m does not have resource o during phase k)
o
m
for any resource o. Otherwise, xi,a is not restricted since at most T actions can be executed
over the finite time horizon T .
Deriving an optimal sequential resource allocation and a joint policy from the solution
to Eq. 9 is straightforward. At the start time of phase k, resources are redistributed in the
following way: if m,k
= 1, then a unit of resource o is assigned to agent m. Every agent m
o
m
m = Pxi,a
should adopt its policy i,a
m to maximize the total expected reward of the group of
a xi,a
agents.
Running Example: Optimizing for a Predetermined Phase-Switching Schedule.
Returning to the running example (Section 4.1), we will see whether the total expected reward can be improved when the resources can be reallocated during execution. Let us say that
the predetermined schedule says that resources can be redistributed at times 1, 3, 6, and 8,
decomposing the example problems time horizon into four phases of roughly equal duration.
Formulating and solving this M-RMP problem with Eq. 9 yields the sequential allocation depicted in Figure 11. Compared to the one-shot distribution (Section 4.2, Figure 10), agent 2
no longer idles over the entire horizon, and the total expected reward increases to 65.04, 31%
higher than attained by the one-shot allocation.
4.6 Determining an Optimal Resource Reallocation Schedule
Without a predetermined resource reallocation schedule, agents can be free (within constraints) to determine for themselves when to reassign resources to achieve their remaining
goals better. That is (Section 4.3), given inputs M, , C, and R, the M-RMP optimization
454

fiResource-Driven Mission-Phasing Techniques

1

1

1

1

1

2

2

2

2

2

1

1

1

2

2

2

4

3

5

6

7

8

1

1

2

2
9

10

Figure 11: Optimal sequential resource allocation for four predefined phases.

problem is to find an optimal resource reallocation schedule (subject to the resource reallocation constraints R), and to find the optimal resource allocation among agents (subject to
the resource constraints C) within each phase, as well as to derive optimal executable phase
policies for each agent. The complexity of this problem and limitations of straightforward
approaches to solving it were described in Section 4.4.
We instead extend the MILP in Eq. 9 to also reason about problem decomposition. The
extension is shown in Eq. 10, where (probability conservation) constraints unchanged from
Eq. 9 are omitted. To model the constraints on total resource reallocation costs and the
occurrences of resource-reallocation events, this new formulation represents the resource constraints at each time step (instead of at each phase), and introduces supplementary constraints
to model phase transitions.
XXX
m
max
xm
(10)
i,a  ri,a
m

i

a

subject to:
probability conservation constraints (unchanged)
resource constraints:
XX
m
m,t
um
o,a,i  xi,a  o
X

: t, m, o

a

iSt

m,t
= o
o

m
m,t
o

: o, t

 {0, 1}

: t, m, o

reallocation constraints:
m,t1
m,t
 t
o  o

: o, t > 1, m

t=1 = 1
X
t  t  
t

t  {0, 1}

: t

m , xm , um ,  , and T have the same definitions as before. S represents the set
where ri,a
o
t
i,a
o,a,i
indicates
whether
resource o is
of states associated with time t. New binary variable m,t
o

455

fiWu & Durfee

2

1

1

1

1

2

2

2

2

3

1

1

1

2

2

2

1

1

2

2

1

4

5

6

7

8

9

10

Figure 12: Optimal sequential resource allocation for four phases without a predefined schedule.

assigned to agent m at time t, and the resource constraints guarantee that the total amount
of allocated resources must equal the total amount of available resources at any time point.
To model the cost constraints on resource reallocation, Eq. 10 introduces new binary
variables t to represent whether the resources are to be redistributed at time t. To sidestep
the question of what the default resource allocation might be, it is assumed that the resources
are always initially allocated at the beginning of the execution, i.e., t=1 = 1. Note that
m,t
 om,t1 can never be greater than one since m,t
and om,t1 are binary values in
o
o
m,t1
m,t
 t thus points out that t must be one if any agent m
{0, 1} ; the constraint o  o
procures any different resource at time t compared to time t  1. In other words, any resource
reassignment
at time t will lead to t = 1, which means that we can use the constraint
P
t t  t   to limit the total cost for resource reallocation.

By definition, there is a one-to-one mapping between possible sequential resource allocations and possible integer solutions. In addition, given a particular sequential resource
allocation, the MILP would be reduced to a linear program whose solution space is equivalent
to the executable policy space (because resource constraints would prune unexecutable actions). In other words, the MILP solution space includes the best way of allocating resources
together with the best way of utilizing the allocated resources, and so finding an optimal
solution to the MILP is equivalent to finding an optimal way of sequentially allocating and
utilizing resources.

Running Example: Optimizing for a Fixed Number of Phase-Switching Times.
Consider what happens if the agents can determine for themselves a set of reallocation times
given an upper bound of four for the size of this set, i.e., t=1 = 0, t6=1 = 1, and  = 3.
Using Eq. 10, the optimal schedule to reallocate resources is computed as {1, 4, 5, 8}. Figure 12
depicts the detailed allocation. This schedule gives high priority to and allots sufficient time
for agents to accomplish their high-reward tasks (i.e., task 3 of agent 1, and task 1 of agent
2). As a result, the total expected reward for the two agents increases to 72.25, which is
11.1% higher than for the fixed set of 4 reallocation times that were more evenly spaced out
(Section 4.5, Figure 11), and 45.5% higher than the one-shot case (Section 4.2, Figure 10).
456

fiResource-Driven Mission-Phasing Techniques

4.6.1 Variation: Maximizing the Total Reward, Accounting for Cost
As in the S-RMP problem (Section 3.4.1), we can consider a variation of the M-RMP problem
where neither the resource-reallocation schedule is predefined (Section 4.5) nor the number of
times for reallocating resources is restricted (Section 4.6), but rather that a cost is incurred
each time resources are reallocated and this cost is calibrated with the utility of the MDP
policy. Thus the optimization problem is to maximize the total expected reward, accounting
for the costs of redistributing the resources during execution.
We begin by examining a binary-cost case where, if a resource reallocation is scheduled
at time t, it will charge the group of agents a constant fee t regardless of what resources
and how many of them are redistributed at that time. In general, coping with such binary
reallocation costs is relatively easy because Eq. 10 has paved the way to characterize time
steps for resource reassignments.
Eq. 11 shows the changed components of the solution
algorithm
to this problem,
compared
P
P P
P
m 


t , and

r
to Eq. 10. Eq. 11 adopts a new objective function m i a xm
t
i,a
i,a
t
P
removes the constraint t t  t   that is no longer applicable since the agents can now
reallocate resources as frequently as they desire.
XXX
X
m
max
xm
t   t
(11)
i,a  ri,a 
m

subject to:

i

a

t

probability conservation constraints (unchanged)
resource constraints (unchanged)
reallocation (cost) constraints:
m,t1
m,t
 t
o  o

: o, t > 1, m

t=1 = 1
t  {0, 1}

: t

Next we consider the more difficult variation where the cost incurred in redistributing
resources is based on the amount of resources being transferred among the agents. Since it
is assumed that the agents are cooperative, it does not matter which agent involved in an
exchange pays the resource transfer costs. Without loss of generality, let us say that agent m
when it obtains one unit of resource o at time t from someone else, and the
pays the cost cm,t
o
agent releasing that resource pays no cost.
is used to represent whether resource o is currently held by agent m at time
As before, m,t
o
t. The cost that agent m should pay for getting resource o at time t can then be represented
m,t
m,t1
as cm,t
) where function (z) is a piecewise linear function, defined as:
o  (o  o

z z>0
(z) =
0 otherwise
This piecewise linear constraint can be equivalently represented using multiple linear constraints by introducing continuous variables m,t
o . The new MILP formulation is shown in
Eq. 12, where again only the groups of changed constraints compared to Eq. 10 are shown.
XXX
XXX
m
m,t
xm
max
cm,t
(12)
i,a  ri,a 
o  o
m

i

a

o

457

m

t

fiWu & Durfee

2

1

1

1

1

2

2

2

2

3

1

1

1

1

1

2

2

2

2

2

1

4

5

6

7

8

9

10

Figure 13: Optimal sequential resource allocation, given that the transfer cost is 5 per unit.

subject to:
probability conservation constraints (unchanged)
resource constraints (unchanged)
reallocation (cost) constraints:
m,t=1
= m,t=1
o
o

: o, m

m,t1
m,t
 m,t
o
o  o

m,t
o

: o, t > 1, m

0

: o, t, m

 om,t1 . In
 m,t
 0 and m,t
is constrained by m,t
That is, when t > 1, m,t
o
o
o
o
m,t
m,t
m,t1
m,t
m,t1
other words, o  1 when o > o
(i.e., o = 1, and o
= 0), and m,t

o
0Punder
other
circumstances.
Note
that
the
objective
function
of
Eq.
12
is
to
maximize
P P P m,t m,t
P P P m,t m,t
P P m
m
o
m
t co o
o
m
t co o , implying the second term
m
i
a xi,a ri,a 
should be as small as possible for an optimal solution that yields the highest expected utility.
m,t
That is, m,t
= 1 when
o should reach its lower bound for any optimal solution to Eq. 12, i.e., o
m,t
m,t1
m,t
o > o
(i.e., when agent m acquires resource o at time t) and o = 0 otherwise,
to represent the piecewise linear cost
which exactly matches our expectation of using m,t
o
m,t1
function (m,t


).
o
o
Running Example: Optimizing Total Reward Accounting for Cost. Consider how
the above algorithm manages the transfer of resources when each transfer incurs cost in the
running example problem from Section 4.1. When transferring one unit of any resource costs
5, the optimal sequential resource allocation, which is shown in Figure 13, is to transfer only
four units of resources over the entire execution (two units at the initial time 1, one unit at
time 4, and one unit at time 5). Not surprisingly, as the transfer cost increases, the amount
of resources to be transferred decreases, and vice versa.
Table 2 compares this resulting schedule with the schedules derived in the previous sections,
but now incorporating the cost of 5 for each resource transfer. As expected, the algorithm in
Eq. 12 yields a reallocation schedule with the highest utility, which is 48.72.
4.7 Experimental Evaluation
We analyzed the computational complexity of the M-RMP problem in Section 4.4; here, we
empirically evaluate the effectiveness and computational efficiency of the MILP-based solution
458

fiResource-Driven Mission-Phasing Techniques

Case
one-shot (Figure 10)
4 fixed times (Figure 11)
3 added times (Figure 12)
unlimited times balancing cost (Figure 13)

Total Resource (Re)Assignments
2
7
5
4

Utility
39.64
30.04
47.25
48.72

Table 2: Comparison of the resource reallocation schedules to the example problem, given
that the transfer cost is 5 per unit.

algorithms we developed in this section, using a grid world environment similar to that used
for the S-RMP evaluation in Section 3.5.14
4.7.1 Experimental Setup
Each test problem instance includes m cooperative agents where each agent operates in its own
nn grid world that is independent of all others. The starting location of each agent is always
at the center of its grid world.15 The objective of the group of agents is to maximize their total
expected reward within T time steps. Like in the single-agent test problems (Section 3.5),
when a grid world is generated, 40% of the locations are randomly chosen as walls, and 10%
are randomly chosen as task locations. The rewards of the tasks are randomly set, i.e., the
ith task (in a random order) is given reward i.
Each task is temporally constrained by its release time and deadline. The release time
is the time step when the task becomes available, i.e., attempting the task before then will
return zero reward. The deadline is when the task becomes unavailable, i.e., finishing the
task after then will also return zero reward. These temporal constraints are randomly set.
A tasks release time is an integer uniformly and randomly selected in the range [1, T  2]
where T is the time horizon, and its deadline is always three time steps later. Thus, task is
time window is [ti , ti + 3) where ti is a random integer in [1, T  2]. A task can be repeated
multiple times (and each time it will give the same reward) within its time window.
The action space of each agent is {wait, up, left, down, right, safe-up, safe-left, safe-down,
safe-right, do}. All actions except the do action have exactly the same definitions as before
(Section 3.5). The resource prerequisite of the do action is also the same as before, but its
outcome no longer always terminates the execution immediately. Instead, it terminates with
probability 0.05, and otherwise the agent stays in the same location (with probability 0.95)
and can repeat the task or move to another task until the time horizon T is reached. This
change makes the test problems more interesting and complex, since each agent now uses
resources throughout the experiment rather than just up until completing its first task.
The system is constrained by resource limitations. There are |O| different resource types
in the system. Further, there is only one instance of each resource type, which is shared by
the m agents.
14. An empirical evaluation in the domain with problems similar to (but more complex than) the running
example used in this section (Figure 8) can be found in the work of Wu and Durfee (2007a).
15. Starting in the center makes the problem more interesting and challenging than starting in a corner, allowing
the agent to potentially visit a larger fraction of the grid world sooner.

459

fiWu & Durfee

25

5 optimal phases
average reward

20

5 fixed phases
15

nonphasing

10

5

0

2

3

4

5

6

7

8

9

10

number of agents

Figure 14: Exploiting fixed phases increases the reward, and finding optimal phases further
increases the reward.

4.7.2 Improvements to Solution Quality
Figure 14 demonstrates the improvement of our sequential resource allocation approaches
over the prior one-shot resource allocation approach. The x-axis of the figure represents the
number of agents in the world, and the y-axis specifies the total expected reward of the
group of agents.16 Other parameters are set as follows: T = 10, n = 5, and |O| = 5. We
can see that, by taking into account resource reallocation opportunities during execution,
the agents can gain a considerably higher reward. For example, in the case that five fixed
resource (re)allocation times (one at the initial time step and the other four randomly and
uniformly selected when the test problem is defined) are available in the midst of execution,
our mission-phasing approach, using Eq. 9 and denoted as 5-fixed-phases, on average achieves
a reward 50% higher than that of not exploiting resource-reallocation opportunities. We
can also see that (as expected) finding and using the optimal resource-allocation and phaseswitching time points can further improve the system performance, e.g., the 5-optimal-phases
approach (using Eq. 10 and assuming that four additional phase-switching points besides the
one at the initial time step can be created) achieves an average reward about 20% higher than
the aforementioned 5-fixed-phases solution.
Another interesting observation from Figure 14 is that the improvement of sequential resource allocation over one-shot resource allocation increases as the number of agents increases.
This is because, given that the number of resources is fixed at 5, the more agents there are,
the scarcer the resources are. Hence, assigning a resource to the right agent at the right
time becomes increasingly important to performance as the constrainedness of the system
increases.
Figure 15 uses the same parameters as Figure 14 (i.e., T = 10, n = 5, and |O| = 5), but
now holds the number of agents constant at m = 5, and shows how much better agents that
can choose phase-switching times (Eq. 10) can do as the number of phase-switching times
16. In this section, each average data point is computed from 20 random test problems.

460

fiResource-Driven Mission-Phasing Techniques

25

average reward

20

15

10

5

0

1

2

3

4

5

6

7

8

9

10

number of phases

Figure 15: The reward increases as the phase-switching cost limit (that determines the number of phases) increases.

allowed rises. However, note that, unlike the S-RMP optimization problem, even if the agents
can reallocate resources at every time step, they usually cannot achieve the same reward as
in the unconstrained case with unlimited resources (which has a reward of 37.2 on average in
these test problems). This is because, at each time step, some agents might not be able to
acquire their most desired resources because there simply are not enough of these resources
to go around.
4.7.3 Computational Efficiency
To understand the impact of the number of phases on the computational cost and to choose
hard M-RMP test problems for the computational efficiency evaluation to follow, we now
run experiments with the same parameters as in Figure 15, but collect and examine the results
of average runtime for finding exact solutions to the test problems. As shown in Figure 16, the
MILP-based solution approach can exploit over-constrainedness (when the number of phases
is small) and under-constrainedness (when the number of phases is large) to improve efficiency.
This complexity profile indicates that on average, with these parameter settings, the problems
are most difficult when constrained to 3 phases (that is  = 2 and as usual t=1 = 0). In
the experiments that follow, one parameter is varied at a time while the others retain their
default settings, and these variations create larger and more complex instances than used to
generate Figure 16. Hence, the phase-switching cost limit  is by default set to 3 to avoid
simpler over-constrained problems.
We compare our MILP-based algorithm (Eq. 10) with the WDP-based algorithm (using
the auction-based resource allocation strategy), which is the most computationally-efficient
approach among the three prior related approaches discussed in Section 4.4. Recall that the
WDP-based algorithm involves two steps. First, each agent submits valuations of its possible
T 1
sequential resource allotments to a central agent. The number of bids is CK1
 (2)|O|K (as
explained in Section 4.4). Second, the central agent solves a winner determination problem.
461

fiWu & Durfee

70

runtime (seconds)

60

50

40

30

20

10

0

1

2

3

4

5

6

7

8

9

10

number of phases

Figure 16: The runtime increases and then decreases as the number of times of resource
reallocation increases.

Let us assume that the central agent has a perfect filtering method (although it usually
does not), and so it only needs to consider and evaluate valid combinations of bids. This
T 1
T 1
assumption reduces the number of possible combinations from (CK1
 (2)|O|K )m to CK1

(m)|O|K where the base m in the exponentiation (m)|O|K is because there are m different
ways to allocate one resource among m agents.
However, even with this enhancement, the WDP-based algorithm is still computationally
intractable for moderately complex M-RMP problems (where it often cannot find an exact
solution within 100 hours of cpu time). Note that the lower bound of the running time of the
T 1
T 1
(2)|O|K tbid +CK1
(m)|O|K teval
WDP-based algorithm can be approximated as CK1
where tbid is the average runtime of evaluating a sequential resource allotment (i.e., a bid) by
modeling and solving an unconstrained finite-horizon MDP, and teval is the average runtime
of evaluating a feasible combination of agents bids. This work uses a sampling method to
estimate the runtime, i.e., tbid and teval are estimated from 100,000 random runs.
Figure 17 compares the average runtime results under various parameter settings.17 Note
that the y-axis is in a logarithmic scale. These results illustrate and emphasize that the
MILP-based algorithm, which formulates and simultaneously solves the coupled problems of
mission decomposition, resource allocation, and policy formulation using a single compact
MILP formulation, can effectively and fruitfully exploit the inter-relationships among these
component problems. As a result, it is significantly faster than the WDP-based approach that
considers the component problems in isolation.
4.8 Summary
In this section, we have presented, analyzed, and empirically evaluated an MILP-based approach that automates the process of finding and using optimal resource reallocation schedules
17. Neither MILP nor WDP uses parallel computation.

462

fiResource-Driven Mission-Phasing Techniques

15

15

10
WDPbased
MILPbased

runtime (seconds)

runtime (seconds)

10

10

10

5

10

0

10

WDPbased
MILPbased
10

10

5

10

0

2

4

6

8

10

10

number of phases
15

8

10

15

10
WDPbased
MILPbased

runtime (seconds)

runtime (seconds)

6

number of resource types

10

10

10

5

10

0

10

4

WDPbased
MILPbased
10

10

5

10

0

6

8

10

12

10

14

horizon

4

6

8

10

number of agents

Figure 17: Runtime comparison between the MILP-based algorithm and the WDP-based algorithm. Parameters are set as follows: Top-left figure n = 5, T = 10, m = 5,
|O| = 5, and  = {1, 2, ..., 9}. Top-right figure n = 5, T = 10, m = 5,
|O| = {4, 5, ..., 10}, and  = 3. Bottom-left figure n = 5, T = {6, 7, ..., 14},
m = 5, |O| = 5, and  = 3. Bottom-right figure n = 5, T = 10, m = {3, 4, ..., 10},
|O| = 5, and  = 3.

463

fiWu & Durfee

for a group of agents operating in complex environments with resource limitations and with
uncertainties. Our analytical and experimental results have shown that the approach can
greatly reduce computational cost compared to prior approaches.

5. Related Work
The resource-driven mission phasing (RMP) problem involves three intertwined component
problems: mission (problem) decomposition, resource configuration, and policy formulation.
Each of these component problems has been studied in a wide variety of research fields. The
combinations of any two of them have also gained much attention in recent years. This section
gives an overview of related work, and discusses why those prior approaches are not directly
applicable to the RMP problem of interest in this paper.
As was presented in Section 3.1 and Section 4.3, the RMP problems are defined by extending an unconstrained MDP model to include resource constraints and phase-switching
constraints. The organization of this section follows the way of that definition. It begins with
a discussion of policy formulation techniques, followed by a discussion of resource configuration techniques. It then reviews problem decomposition techniques and their combinations
with policy formulation and/or resource configuration work. This section concludes with a
discussion of the mode-transition research that is related to this work but does not fit clearly
into the previous categories.
5.1 Policy Formulation.
The well-known Markov decision process has been described in Section 2.1. By formulating
a sequential decision-making problem into a MDP model, a number of efficient (polynomialtime) solvers, such as the value iteration and policy iteration algorithms, can be used to
compute an optimal policy (Puterman, 1994).
However, directly applying these algorithms in resource-constrained systems, such as the
resource-driven mission-phasing problem, typically involves incorporating resource features in
the MDP state representation (and so actions can be conditioned on resource availability),
which will result in an exponential increase in the size of the state space (Meuleau et al.,
1998), i.e., the well known curse of dimensionality challenge. It has been shown in the empirical results (Section 3.5.3) that the exponential-size state space can result in computational
inefficiency.
5.2 Resource Configuration.
Because in some domains it is impossible (or expensive) to resolve resource constraints by
modifying the agents physical architecture (for example, adding another battery to a robot
already deployed on Mars), improving the performance of a constrained agent under its limited architecture has been an active subject in recent years, i.e., a class of bounded optimality problems (Russell, 2002). The Cooperative Intelligent Real-Time Control Architecture
(CIRCA) is one such research effort (Musliner, Durfee, & Shin, 1993, 1995). CIRCA uses
a simple greedy, myopic approach to compute feasible policies. It starts with building an
optimal unconstrained policy without worrying about its real-time requirements, and then
greedily repairs the policy until executable on the real-time system.
464

fiResource-Driven Mission-Phasing Techniques

Not surprisingly, the (fast) greedy approach adopted by CIRCA might result in suboptimal policies that cannot fully utilize the agents capacity. Several other recent studies have
proposed alternative algorithms for searching for a policy that is executable within the agent
capacity constraints and that optimizes the expected (possibly discounted) reward accrued
over the entire agent execution. For example, Altman (1998) adopted a Lagrangian and dual
LP approach to solve constrained MDPs with total cost criteria. Feinberg (2000) analyzed
the complexity of constrained discounted MDPs. Of particular relevance to the work in this
paper is the study of strongly-coupled resource allocation and policy formulation problems by
Dolgov and Durfee (2006). Their approach implements simultaneous combinatorial optimization and stochastic optimization via reduction to mixed integer linear programming, which
has been recapped in Section 2.2. However, these prior studies on constrained agents are
based upon the assumption that the agents limited capacity is configured by the resources it
procures prior to execution but cannot be reconfigured during plan execution.
5.3 Problem Decomposition.
In the literature of stochastic planning, a number of decomposition algorithms have been
proposed to speed up the planning process. The discovery of recurrent classes of MDPs
is one such decomposition strategy, which can find an exact state space decomposition in
an environment with uncertainties (Puterman, 1994; Boutilier, Dean, & Hanks, 1999). A
recurrent class represents a special absorbing subset of the state space, which means that once
an agent enters a recurrent class it remains there forever no matter what policy it adopts.
Puterman (1994) has suggested a variation of the Fox-Landi algorithm (Fox & Landi, 1968)
to discover recurrent classes. With the discovery of the recurrent classes, the MDP solver
can derive an optimal overall policy by building an optimal policy in each recurrent class
independently and then constructing and solving a reduced MDP consisting only of transient
states (i.e., removing the recurrent classes in the MDP).
Of course, not all application problems can be exactly decomposed into independent subproblems. However, many of them are composed of multiple weakly-coupled sub-problems
where the number of states and transitions connecting two neighboring sub-problems is relatively small. A number of heuristic decomposition methods have been designed to exploit such
weakly-coupled relationships. As an example, in the robot navigation domain (Parr, 1998;
Precup & Sutton, 1998; Lane & Kaelbling, 2001), doorways (or similar connection structures,
such as bridges) can be used to break a large environment into blocks of states, e.g., one block
for each room. Two neighboring blocks are only connected by a small number of doorway
states. Once a weakly-coupled state space is decomposed into several pieces, there are a few
methods that can be used to efficiently build an overall policy based upon sub-problem policies. One common method is to let each sub-problem iteratively exchange information with its
neighboring sub-problems, and repeatedly revise its sub-policy (if necessary) based upon its
updated knowledge about utilities or values of its neighbors until an overall (approximately)
optimal solution is derived (Dean & Lin, 1995).
Besides the application in stochastic planning, decomposition techniques have also been
shown to be beneficial for resource management in many realistic application domains. Several resource allocation algorithms have been developed for the problem of allocating a set of
heterogeneous resources with availability constraints to maximize a given utility function (Wu
& Castanon, 2004; Palomar & Chiang, 2006; Reveliotis, 2005). For example, Wu and Cas465

fiWu & Durfee

tanon (2004) presented an approximate solution algorithm using decomposition combined with
dynamic programming, and their experimental results showed that the algorithm produces
near-optimal results with much reduced computational effort.
In addition to the Artificial Intelligence (AI) techniques discussed above, decomposition
techniques, which are often integrated with hierarchical control (also called multilevel control
in some literature), have received much attention in recent years in Operations Research,
Operations Management, Systems Theory, Control Theory, and several other fields (Sethi,
Yan, Zhang, & Zhang, 2002; Antoulas, Sorensen, & Gugercin, 2001; Xiao, Johansson, & Boyd,
2004; Phillips, 2002; Teneketzis, Javid, & Sridhar, 1980). Many manufacturing systems are
large and complex; the management of such systems requires recognizing and reacting to a
wide variety of events that could be deterministic or stochastic. Obtaining exact optimal
policies to run these systems is often very difficult both theoretically and computationally.
By exploiting the fact that real-world systems are often characterized by several decision subsystems, e.g., a company consists of departments of marketing, production, personnel, and
so on, one popular way to deal with the computational complexity challenge is to develop
methods of hierarchical decision-making for these systems. The fundamental ideas are to
reduce the overall complex problem into multiple smaller, manageable sub-problems, to solve
these sub-problems, and to coordinate solutions to the sub-problems so that overall system
objectives and constraints are satisfied (Sethi et al., 2002).
To summarize, it is well established that utilizing decomposition can greatly reduce computational costs in many situations. However, all the aforementioned prior decomposition
techniques are not directly applicable to the RMP optimization problem. The underlying
reason is that decomposition points that are good at reducing computational efforts are not
necessarily (and possibly completely unrelated to) the optimal points for constrained agents
to reconfigure resources. It is worth emphasizing that RMP decomposition tackles capacity
constraints instead of computation time constraints. Indeed, in general, the mission decomposition in the RMP solution will not in itself reduce computational requirements because a
policy in one phase can usually only be optimized with respect to the policies planned for
possible subsequent phases.
5.4 Mode Transition.
Finally, it is important to distinguish the resource-driven mission-phasing research from the
mode-transition research implemented in the fields of Operations Research and Control
Theory (Schrage & Vachtsevanos, 1999; Wills, Kannan, Sander, Guler, Heck, Prasad, Schrage,
& Vachtsevanos, 2001; Karuppiah, Grupen, Hanson, & Riseman, 2005). At first glance, these
two research fields have a lot in common: they both work on transitions from one subproblem to another, and both take into account resource reconfigurations. However, it should
be pointed out that they emphasize distinct aspects, and are applicable to different application
domains.
First of all, in the mode-transition approach, operational modes are usually tightly associated with some explicit actions (e.g., hover and fly-forward modes in the helicopter example
described by Schrage & Vachtsevanos, 1999), corresponding to some particular states (e.g.,
sleep, search, seed, and final modes defined by Bojinov, Casal, & Hogg, 2002), or characterized
with some explicit purposes (e.g., passing through a narrow tunnel and then traversing rough
terrain requires a self-reconfiguring robot to adjust its shape to achieve its goal better, Rus
466

fiResource-Driven Mission-Phasing Techniques

& Vona, 2001). In contrast to the explicit definition or representation of modes in the modetransition research, phases in the RMP problem are usually much more difficult to identify.
The phasing information is hidden in the MDP model, and finding optimal phases is usually
a challenging task.
Second, in the mode-transition research, mode transition and resource reconfiguration
are often triggered by real-time events, e.g., responding to an unexpected disastrous event
and reconfiguring resources for fault toleration (Drozeski, 2005). In contrast, the resourcedriven mission-phasing study assumes that a decision-making agent has complete information
about the environment prior to its execution, and one of its main objectives is to find the
optimal points for reconfiguring resources and capacity usage. That is, phase switching in
RMP is a choice of the agent instead of a reactive response to an exogenous event. More
specifically, the RMP techniques presented in this paper utilize sequential decision-making to
identify optimal resource reconfiguration and policy switching states. They emphasize how
to reconfigure resources and switch policies so that the agent(s) would not (or would be less
likely to) enter into the predicament of encountering undesirable events, instead of studying
how to reconfigure resources in real-time to respond to an unexpected event.
Finally, much prior mode-transition research, particularly in the Control Theory literature,
investigates how to perform a smooth functional transition among modes, but the work in
this paper simply assumes that there are aggregate resource (re)configuration actions, each
of which can be a sequence of primitive actions of arranging resources. This paper does not
address the details of how the agents mechanically implement mode-transition and resourcereconfiguration actions.

6. Conclusion
The work in this paper designed, analyzed, and evaluated a suite of computationally efficient
algorithms that can automatically identify and utilize resource reconfiguration opportunities
in resource-constrained environments. The analytical and experimental results illustrated
and emphasized that the mission phasing approach, incorporating problem decomposition,
resource allocation, and policy formulation, can help constrained agents judiciously and effectively exploit resource reconfiguration opportunities to improve their performance.
This section concludes the paper with a summary of the main contributions of this work
and a discussion of several promising future research directions.
6.1 Summary of Contributions
. This work explicitly took into account known opportunities in the midst of execution to
reconfigure resources and switch policies, and designed computationally efficient algorithms (including an abstract MDP algorithm for single-agent resource reconfiguration
problems and a MILP-based algorithm for multiagent resource reallocation problems)
to optimize the use of these fixed opportunities in complex stochastic systems. The empirical results (Figure 4 and Figure 14) confirmed that exploiting such phase-switching
opportunities can considerably improve performance, particularly in tightly constrained
systems (the reward doubles in some test cases).
. As an extension to utilizing fixed phase-switching opportunities, Section 3.4 (for singleagent systems) and Section 4.6 (for multiagent systems) presented MILP-based algo467

fiWu & Durfee

rithms that are able to automate the process of finding and using mission phases in
stochastic, constrained systems, which not only eliminates the need for having phases
predefined, but also avoids potential sub-optimality caused by phases being improperly
predefined.
. The automated resource-driven mission-phasing algorithms presented in this work are
computationally efficient. By capturing a whole mission-phasing problem into a compact mathematical formulation and then simultaneously solving the coupled problems
of mission decomposition, resource allocation, and policy formulation, the presented
algorithms can effectively exploit problem structure, which results in a significant reduction in computational cost in comparison with an approach that considers mission
decomposition, resource allocation, and policy formulation in isolation (e.g., a reduction
from hours to seconds as was shown in Figure 17).
. Unlike much prior work where agents reactively (and often greedily) reconfigure resources when exogenous events occur, this work, based upon Markov decision processes
and sequential decision-making theory, can proactively determine and optimally utilize resource reconfiguration opportunities. It provides a new computationally efficient
resource-reconfiguration mechanism for resource-constrained environments.
6.2 Future Work
Although this paper presented a suite of algorithms to improve agent performance in constrained stochastic systems, there is still much interesting work remaining. Below, we point
out a few promising research directions to overcome some of the limitations of the work we
have presented in this paper.
. Resource Constraints and Time Limitations
Resource-driven mission-phasing problems are NP-complete. Although the solution approaches designed in this work can exploit problem structure to reduce computational
cost, finding an exact solution to a complex RMP problem might still be difficult, particularly in time-limited environments. One approach to handling such problems is to
adopt approximate methods. Our preliminary investigations into developing anytime
algorithms for solving problems with both resource constraints and time limitations has
shown promise (Wu, 2008) but more work remains in this area, including comparing
methods grounded in the RMP concepts with heuristic and greedy techniques for allocating resources to agents.
. More Flexible Resource Reallocation Options
The work we discussed in this paper assumed a clear delineation between two kinds of
states: states where resources can be (re)allocated in any way desired (phase-switching
states) and states where resource allocations cannot change. More generally, it might
be the case that states could exist where only limited resource reallocations could occur (e.g., a partially filled toolbox, or a subset of other agents with whom to swap
resources), leading to more challenging reasoning problems for agents to decide which
such states to seek out and avail themselves of. Further, an agent could even potentially
468

fiResource-Driven Mission-Phasing Techniques

create such a state on the fly by dropping off resources in a well-chosen state to be
gainfully retrieved (perhaps by another agent) at a future time. Obviously, as problems
get increasingly complicated in these kinds of ways, modeling resources as part of state
and incorporating actions of picking up and dropping off resources becomes important,
leading to MDP-based solution techniques as described in this paper (e.g., Sections 3.2
and 3.5.3). Finding methods for having more of this kind of flexibility without incurring
the costs of the MDP-based techniques is a challenging direction for future work.
. Resource Reallocation and Decentralized MDPs
A limiting assumption made in this work is that, once a resource reallocation is scheduled, participant agents will always be able to successfully redistribute resources among
themselves at that scheduled time, regardless of what their other state features values
are. We plan to relax this assumption in the future to consider sequential resource
allocation problems with additional constraints on when and where the agents are able
to exchange resources. For example, physical agents might only be able to exchange
resources when they are at the same location at the same time. Or, as another example, a task might not be interruptible once it has started, which means that it may be
impossible to reassign the resources used by that task until the task has completed.
Decentralized MDPs are one possible way to solve such problems. Some of our preliminary work (Wu & Durfee, 2006), which is not included in this paper, has developed a
MILP-based algorithm for solving transition independent Dec-MDPs. That work linked
the Dec-MDP formulation with the MILP formulation, and pointed out one way to
characterize resource constraints in the MILP formulation. In the future, we will dig
deeper in this direction.
. Application and Evaluation in Other Settings
Our work has so far focused on testing our techniques on problems that are small enough
to solve using slower standard approaches (to confirm that our techniques are optimal)
and on generated spaces of problems that allow us to probe the efficacy of our techniques
in settings that vary in controlled ways. Applying our techniques to more extensive
and realistic domains is an important avenue to follow to identify their strengths and
weaknesses better. An example application that we are particularly interested in, and
that prompted this work in its early stages, is intelligent real-time control. Systems such
as CIRCA (Musliner et al., 1993, 1995) use AI techniques to construct real-time control
plans that are composed of a set of sense-act tasks that are scheduled at frequencies
to ensure safe operation. Often, all of the desired sense-act tasks cannot fit into the
schedule, so the most important combination of tasks must be chosen. In applications
such as controlling an unmanned aircraft (Atkins, Abdelzaher, Shin, & Durfee, 2001),
different combinations might be better in different phases of activity (takeoff, cruising,
landing, etc.). Similarly, for a formation of aircraft, which aircraft is responsible for
detecting and reacting to a particular event can shift as time progresses in the mission
(Musliner, Goldman, & Krebsbach, 2005). While prior work has used heuristic, local
search techniques to find good solutions to such phasing problems, the techniques in
this paper have the potential of finding optimal mission decompositions.
469

fiWu & Durfee

Acknowledgments
This material is based upon work supported in part by the DARPA/IPTO COORDINATORs
program and the Air Force Research Laboratory under Contract No. FA875005C0030, and
by the Air Force Office of Scientific Research under Contract No. FA9550-07-1-0262. The
views and conclusions contained in this document are those of the authors, and should not
be interpreted as representing the official policies, either expressed or implied, of the Defense
Advanced Research Projects Agency, the Air Force, or the U.S. Government.
The authors thank Dmitri Dolgov and the three anonymous reviewers for their very helpful
suggestions and comments, and Stefan Witwicki and Jim Boerkoel for their help in proofreading this article.

References
Altman, E. (1998). Constrained Markov decision processes with total cost criteria: Lagrange
approach and dual LP. Methods and Models in Operations Research, 48, 387417.
Antoulas, A. C., Sorensen, D. C., & Gugercin, S. (2001). A survey of model reduction methods
for large-scale systems. Contemporary Mathematics, 280, 193220.
Atkins, E. M., Abdelzaher, T. F., Shin, K. G., & Durfee, E. H. (2001). Planning and resource
allocation for hard real-time, fault-tolerant plan execution. Autonomous Agents and
Multi-Agent Systems, 4 (1-2), 5778.
Becker, R., Zilberstein, S., Lesser, V. R., & Goldman, C. V. (2004). Solving transition independent decentralized Markov decision processes. Journal of Artificial Intelligence
Research, 22, 423455.
Bellman, R. (1957). A Markov decision process. Journal of Mathematical Mechanics, 6,
679684.
Bererton, C. A., Gordon, G. J., & Thrun, S. (2003). Auction mechanism design for multi-robot
coordination. In Advances in Neural Information Processing Systems, pp. 879886.
Bernstein, D. S., Zilberstein, S., & Immerman, N. (2000). The complexity of decentralized control of Markov decision processes. In Proceedings of the 16th Conference in Uncertainty
in Artificial Intelligence, pp. 3237.
Bojinov, H., Casal, A., & Hogg, T. (2002). Multiagent control of self-reconfigurable robots.
Artificial Intelligence, 142, 99120.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research, 11, 194.
Cook, W., Cunningham, W., Pulleyblank, W., & Schrijver, A. (1998). Combinatorial Optimization. John Wiley & Sons, New York.
De Vries, S., & Vohra, R. (2003). Combinatorial auctions: A survey. INFORMS Journal on
Computing, 15 (3), 284309.
Dean, T., & Lin, S. H. (1995). Decomposition techniques for planning in stochastic domains.
In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pp.
304309.
470

fiResource-Driven Mission-Phasing Techniques

Dolgov, D. A. (2006). Integrated Resource Allocation and Planning in Stochastic Multiagent
Environments. Ph.D. thesis, Computer Science Department, University of Michigan.
Dolgov, D. A., & Durfee, E. H. (2005). Computationally-efficient combinatorial auctions for
resource allocation in weakly-coupled MDPs. In Proceedings of the 4th International
Joint Conference on Autonomous Agents and Multiagent Systems, pp. 657664.
Dolgov, D. A., & Durfee, E. H. (2006). Resource allocation among agents with MDP-induced
preferences. Journal of Artificial Intelligence Research, 27, 505549.
Drozeski, G. R. (2005). A Fault-Tolerant Control Architecture for Unmanned Aerial Vehicles.
Ph.D. thesis, Georgia Institute of Technology.
Earl, M., & DAndrea, R. (2005). Iterative MILP methods for vehicle control problems. IEEE
Transactions on Robotics, 21 (6), 11581167.
Feinberg, E. (2000). Constrained discounted Markov decision processes and Hamiltonian
cycles. Mathematics of Operations Research, 25, 130140.
Fox, B., & Landi, D. M. (1968). An algorithm for identifying the ergodic subchains and
transient states of a stochastic matrix. Communications of the ACM, 2, 619621.
Goldman, C. V., & Zilberstein, S. (2004). Decentralized control of cooperative systems:
Categorization and complexity analysis. Journal of Artificial Intelligence Research, 22,
143174.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
for factored MDPs. J. Artif. Int. Res., 19 (1), 399468.
Kallenberg, L. (1983). Linear Programming and Finite Markovian Control Problems. Mathematisch Centrum, Amsterdam.
Karuppiah, D., Grupen, R., Hanson, A., & Riseman, E. (2005). Smart resource reconfiguration
by exploiting dynamics in perceptual tasks. In IEEE/RSJ International Conference on
Intelligent Robots and Systems, pp. 1513  1519.
Kautz, H., & Walser, J. (2000). Integer optimization models of AI planning problems. Knowledge Engineering Review, 15(1), 101117.
Kim, K.-E., & Dean, T. (2001). Solving factored MDPs via non-homogeneous partitioning.
In IJCAI01: Proceedings of the 17th international joint conference on Artificial intelligence, pp. 683689.
Lane, T., & Kaelbling, L. P. (2001). Toward hierarchical decomposition for planning in
uncertain environments. In Proceedings of the 2001 IJCAI Workshop on Planning under
Uncertainty and Incomplete Information, pp. 17.
Lehman, T. J., & Carey, M. J. (1986). A study of index structures for main memory database
management systems. In Proceedings of the 12th International Conference on Very Large
Data Bases, pp. 294303.
Lesser, V., Decker, K., Wagner, T., Carver, N., Garvey, A., Horling, B., Neiman, D., Podorozhny, R., Prasad, M. N., Raja, A., Vincent, R., Xuan, P., & Zhang, X. (2004). Evolution of the GPGP/TAEMS domain-independent coordination framework. Autonomous
Agents and Multi-Agent Systems, 9 (1), 87143.
471

fiWu & Durfee

Meuleau, N., Hauskrecht, M., Kim, K.-E., Peshkin, L., Kaelbling, L. P., Dean, T., & Boutilier,
C. (1998). Solving very large weakly coupled Markov decision processes. In Proceedings
of the 15th National Conference on Artificial Intelligence, pp. 165172.
Musliner, D. J., Durfee, E. H., & Shin, K. G. (1993). CIRCA: A cooperative intelligent real
time control architecture. IEEE Transactions on Systems, Man, and Cybernetics, 23 (6),
15611574.
Musliner, D. J., Durfee, E. H., & Shin, K. G. (1995). World modeling for the dynamic
construction of real-time control plans. Artificial Intelligence, 74 (1), 83127.
Musliner, D. J., Goldman, R. P., & Krebsbach, K. D. (2005). Deliberation scheduling strategies
for adaptive mission planning in real-time environments. In Anderson, M., & Oates, T.
(Eds.), Metacognition in Computation, Vol. SS-05-04 of AAAI Technical Report, pp.
98105. AAAI Press.
Palomar, D., & Chiang, M. (2006). A tutorial on decomposition methods for network utility
maximization. IEEE Journal on Communications, 24 (8), 1439  1451.
Parr, R. (1998). Flexible decomposition algorithms for weakly coupled Markov decision problems. In Proceedings of the 14th Conference in Uncertainty in Artificial Intelligence, pp.
422430.
Pekec, A., & Rothkopf, M. (2003). Combinatorial auction design. Management Science,
49 (11), 14851503.
Phillips, A. (2002). Functional decomposition in a vehicle control system. In Proceedings of
the 2002 American Control Conference, pp. 37133718.
Precup, D., & Sutton, R. (1998). Multi-time models for temporally abstract planning. Advances in Neural Information Processing Systems, 10, 10501056.
Puterman, M. L. (1994). Markov Decision Processes. John Wiley & Sons, New York.
Reveliotis, S. A. (2005). Real-Time Management of Resource Allocation Systems: A Discrete
Event Systems Approach. Springer-Verlag New York.
Rus, D., & Vona, M. (2001). Crystalline robots: Self-reconfiguration with compressible unit
modules. Autonomous Robots, 10 (1), 107124.
Russell, S. (2002). Rationality and intelligence. In Elio, R. (Ed.), Common Sense, Reasoning,
and Rationality. Oxford University Press, USA.
Sacco, G., & Schkolnick, M. (1982). A mechanism for managing the buffer pool in a relational
database system using the hot set model. Proceedings of the 8th International Conference
on Very Large Data Bases, 257262.
Schrage, D., & Vachtsevanos, G. (1999). Software-enabled control for intelligent UAVs. In Proceedings of 1999 International Symposium on Computer Aided Control System Design,
pp. 528532.
Sethi, S. P., Yan, H., Zhang, H., & Zhang, Q. (2002). Optimal and hierarchical controls in dynamic stochastic manufacturing systems: A survey. Manufacturing & Service Operations
Management, 4 (2), 133170.
472

fiResource-Driven Mission-Phasing Techniques

Sutton, R., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework
for temporal abstraction in reinforcement learning. Artificial Intelligence Journal, 112,
181211.
Sutton, R., & Barto, A. (1998). Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA.
Teneketzis, D., Javid, S. H., & Sridhar, B. (1980). Control of weakly-coupled Markov chains.
In Proceedings of the 1980 19th IEEE Conference on Decision and Control including the
Symposium on Adaptive Processes, pp. 137143.
van Beek, P., & Chen, X. (1999). CPlan: A constraint programming approach to planning.
In Proceedings of the 16th National Conference on Artificial Intelligence, pp. 585590.
Vossen, T., Ball, M., Lotem, A., & Nau, D. (1999). On the use of integer programming models
in AI planning. In Proceedings of the 16th International Joint Conference on Artificial
Intelligence, pp. 304309.
Wagner, T., Phelps, J., Guralnik, V., & VanRiper, R. (2004). An application view of COORDINATORS: Coordination managers for first responders. In AAAI, pp. 908915.
Wagner, T., Raja, A., & Lesser, V. (2006). Modeling uncertainty and its implications to
sophisticated control in TAEMS agents. Autonomous Agents and Multi-Agent Systems,
13 (3), 235292.
Wills, L., Kannan, S., Sander, S., Guler, M., Heck, B., Prasad, J., Schrage, D., & Vachtsevanos,
G. (2001). An open platform for reconfigurable control. Control Systems Magazine,
IEEE, 21 (3), 4964.
Wolsey, L. A. (1998). Integer Programming. John Wiley & Sons, New York.
Wu, C., & Castanon, D. (2004). Decomposition techniques for temporal resource allocation.
In IEEE Conference on Decision and Control, pp. 3798 3803.
Wu, J. (2008). Mission-Phasing Techniques for Constrained Agents in Stochastic Environments. Ph.D. thesis, University of Michigan.
Wu, J., & Durfee, E. H. (2005). Automated resource-driven mission phasing techniques for
constrained agents. In Proceedings of the 4th International Joint Conference on Autonomous Agents and Multiagent Systems, pp. 331338.
Wu, J., & Durfee, E. H. (2006). Mixed-integer linear programming for transition-independent
decentralized MDPs. In Proceedings of the 5th International Joint Conference on Autonomous Agents and Multiagent Systems, pp. 10581059.
Wu, J., & Durfee, E. H. (2007a). Sequential resource allocation in multi-agent systems with
uncertainties. In Proceedings of the 6th International Joint Conference on Autonomous
Agents and Multiagent Systems, pp. 760767.
Wu, J., & Durfee, E. H. (2007b). Solving large TAEMS problems efficiently by selective
exploration and decomposition. In Proceedings of the 6th International Joint Conference
on Autonomous Agents and Multiagent Systems, pp. 291298.
Xiao, L., Johansson, M., & Boyd, S. P. (2004). Simultaneous routing and resource allocation
via dual decomposition. IEEE Transactions on Communications, 52 (7), 11361144.

473

fiJournal of Artificial Intelligence Research 38 (2010) 569-631

Submitted 12/09; published 08/10

Cause Identification from Aviation Safety Incident Reports
via Weakly Supervised Semantic Lexicon Construction
Muhammad Arshad Ul Abedin
Vincent Ng
Latifur Khan

arshad@student.utdallas.edu
vince@hlt.utdallas.edu
lkhan@utdallas.edu

Department of Computer Science
Erik Jonsson School of Engineering & Computer Science
The University of Texas at Dallas
800 W. Campbell Road; MS EC31
Richardson, TX 75080 U.S.A.

Abstract
The Aviation Safety Reporting System collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. To effectively reduce these incidents, it is vital to accurately identify why these incidents occurred.
More precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and only those shaping factors that are responsible for
the incidents described in a report. We investigate two approaches to cause identification.
Both approaches exploit information provided by a semantic lexicon, which is automatically constructed via Thelen and Riloffs Basilisk framework augmented with our linguistic
and algorithmic modifications. The first approach labels a report using a simple heuristic, which looks for the words and phrases acquired during the semantic lexicon learning
process in the report. The second approach recasts cause identification as a text classification problem, employing supervised and transductive text classification algorithms to
learn models from incident reports labeled with shaping factors and using the models to
label unseen reports. Our experiments show that both the heuristic-based approach and
the learning-based approach (when given sufficient training data) outperform the baseline
system significantly.

1. Introduction
Safety is of paramount importance when it comes to the aviation industry. In 2007 alone,
there were 4659 incidents1 , including 26 fatal accidents with 750 casualties2 . To improve the
aviation safety situation, the Aviation Safety Reporting System (ASRS) was established in
1976 to make safety incident data available to researchers. ASRS collects voluntarily submitted reports about aviation safety incidents written by flight crews, attendants, controllers
and other related parties. The reports contain a number of fixed fields and a free text narrative describing the incident. However, the data has grown to be quite large over the years
and it is getting increasingly difficult, if not impossible, to analyze these reports by human
means. It has become necessary that these reports be analyzed through automated means.
1. http://asrs.arc.nasa.gov/
2. http://www.flightsafety.gov/
c
2010
AI Access Foundation. All rights reserved.

fiAbedin, Ng & Khan

To take full advantage of this data to reduce safety incidents, it is necessary to extract
from the reports both what happened and why. Once both are known, then it is possible to
identify the correlations between the incidents and their causes, and take fruitful measures
toward eliminating the causes. However, the fixed fields in the reports are devoted to various
aspects of what happened during the incidents, and there is no fixed field that indicates
the incidents causes. Instead, the reporter discusses in the report narrative what he thinks
caused the incident, along with the incident description. Thus the cause of the incident has
to be extracted by analyzing the free text narrative. As an example, a report is shown next
to illustrate the task:
Report#424362. WHILE descending into lit we encountered Instrument Meteorological Conditions; rime ice; rain; and moderate chop. as I turned to
a heading with the Auto-Pilot direct lit the attitude indicator remained in a
bank. XCHKING; I noticed the Radio Magnetic IndicatorS were 55 degree off
headings. I switched to #2 and corrected the course. the Auto-Pilot and flight
director were kicked off. I continued to have problems with the altitude select
and Auto-Pilot as I attempted to re-engage it. it was during radar vectors to
the approach and descent to 2300 feet that we noticed our altitude at 2000 feet
Mean Sea Level. we stopped the descent and climbed to 2300 feet Mean Sea
Level. Air Traffic Control noted our altitude deviation at the time we noticed.
we were thankful for their backup during a time of flight director problems in
our cockpit. this occurred at the end of a 13 hour crew day; bad weather; instrument problems; and lack of crew rest. the First Officer (Pilot Not Flying)
in the right seat; had only 4 hours of rest due to inability to go to sleep the
night before. we were tired from a trip lit-ORL-lit. we had not eaten in about
7 hours.3
Posse et al. (2005) identify 14 most important cause types, or shaping factors, that can
influence the occurrence of the aviation safety incident described in an ASRS report. These
shaping factors are the contextual factors that influenced the reporters behavior in the
incident and thus contributed to the occurrence of the incident. Some of these factors can
be attributed to humans (e.g., a pilot or a flight attendant has psychological Pressure, an
overly heavy Taskload, or an unprofessional Attitude that impacts his performance), while
some are related to the surrounding environment (e.g., Physical Environment such as snow,
and Communication Environment such as auditory interference). A detailed description of
these 14 shaping factors can be found in Section 2.1.
In the above report, we find that the incident was influenced by three shaping factors,
namely Physical Environment (which concerns bad weather, as mentioned above), Resource
Deficiency (which concerns problems with the equipment), and Duty Cycle (which refers
to physical exhaustion due to long hours of duty without adequate rest or replenishment).
These three shaping factors are indicated by different words and phrases in the report. For
instance, the bad weather condition is expressed using phrases such as rime ice, rain and
moderate chop, while the details of the equipment problem appear as sentence fragments like
3. To improve readability, the report has been preprocessed from its original form using the steps described
in Section 2.2.

570

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

attitude indicator remained in a bank, 55 degree off headings and flight director problems.
The issue with the long hours of duty is illustrated by the sentence fragments like 13 hour
crew day and tired from a trip. The goal of our cause identification task for the aviation
safety domain, then, is to identify which of the 14 shaping factors contributed to the incident
described in a report using the lexical cues appearing in the report narrative.
However, as mentioned earlier, the sheer volume of the data makes it prohibitive to
analyze all the reports manually and identify the associated shaping factors. Thus, the
focus of our research is automated cause identification from the ASRS reports, which involves
automatically analyzing the report narrative and identifying the responsible shaping factors.
This brings our problem into the domain of Natural Language Processing (NLP).
Since we have a set of texts (i.e., the report narratives) and a set of possible labels for
these texts (i.e., the shaping factors), this task is most naturally cast as a text classification
task. However, unlike topic-based text classification, cause-based text classification has not
been addressed extensively in the NLP community. Previous work on causal analysis is quite
different in nature from our cause-based text classification task. More specifically, previous
cause analysis works do not involve text classification, focusing instead on determining
the existence of a causal relation between two sentences or events. For instance, there has
been some work on causal analysis for question answering, where a question may involve the
cause(s) of an event (e.g., Kaplan & Berry-Rogghe, 1991; Garcia, 1997; Khoo, Chan, & Niu,
2000; Girju, 2003). Here, the focus is on finding causal relationship between two sentence
components. As another example, causal analysis on equipment malfunction reports have
been attempted by Grishman and Ksiezyk (1990), whose work is restricted to the analysis
of reports related to one specific piece of equipment they studied. They analyze cause-effect
relations between events leading to the malfunction described in the reports.
Cause identification from aviation safety reports is a rather challenging problem, as a
result of a number of factors specific to the ASRS dataset. First, unlike many NLP problems
where the underlying corpus is composed of a set of well-edited texts such as newspaper
reports, reviews, legal and medical documents4 , the ASRS reports are mostly written in
informal manner, and since they have not been edited except for removing author-identity
information, the reports tend to contain spelling and grammatical mistakes. Second, they
employ a large amount of domain-specific acronyms, abbreviations and terminology. Third,
the incident described in a report may have been caused by more than one shaping factor.
Thus reports can have multiple shaping factor labels, making the task more challenging
than binary classification, or even multi-class problems where each instance has only one
label. Above all, the scarcity of labeled data for this task, coupled with highly imbalanced
class distributions, makes it difficult to acquire an accurate classifier via supervised learning.
Previous work on cause identification for the ASRS reports was done primarily by the
researchers at NASA (see Posse et al., 2005) and, to our knowledge, has involved manual
analysis of the reports. Specifically, NASA brought together experts on aviation safety,
human factors, linguistics and English language to participate in a series of brainstorming
sessions, and generated a collection of seed keywords, simple expressions and template
expressions related to each shaping factor. Then they labeled the reports with the shaping
factors by looking for the related expressions in the report narrative. However, there is a
4. Recently, work has started on processing blogs, which may not be so grammatical either, but blogs
typically are not full of domain-specific terminology.

571

fiAbedin, Ng & Khan

major weakness associated with this approach: it involves a large amount of human effort
on identifying the relevant keywords and expressions, and yet the resulting list of keywords
and expressions is by no means exhaustive. Moreover, they evaluated their approach on
only 20 manually labeled reports. Such a small-scale evaluation is by no means satisfactory
as judged by current standard in NLP research. One of our contributions in this research
is the annotation of 1333 ASRS reports with shaping factors, which serve as a standard
evaluation dataset against which different cause identification methods can be compared.
In this paper, we investigate two alternative approaches to cause identification, both
of which exploit information provided by an automatically constructed semantic lexicon.
More specifically, in view of the large amount of human involvement in NASAs work, we
aim to replace the manual selection of seed words with a bootstrapping approach that
automatically constructs a semantic lexicon. Specifically, motivated by Thelen and Riloffs
(2002) Basilisk framework, we learn a semantic lexicon, which consists of a set of words and
phrases semantically related to each of the shaping factors, as follows. Starting from a small
set of seed words and phrases, we augment these seeds in each iteration by automatically
finding a fixed number of words and phrases related to the seeds from the corpus and adding
them to the seed list. Most importantly, however, we propose four modifications to the
Basilisk framework that can potentially improve the quality of the generated lexicon. The
first is a linguistic modification: in addition to using parse-based features (e.g., subjectverb and verb-object features) as in Basilisk, we employ features that can be computed
more robustly (e.g., N-grams). The remaining three are all algorithmic modifications to the
Basilisk framework, involving (1) the use of a probabilistic semantic similarity measure, (2)
the use of a common word pool, and (3) the enforcement of minimum support and maximum
generality constraints for words and their extraction patterns, which favors the addition of
frequently-occurring content-bearing words and disfavors overly-general extraction patterns.
As mentioned above, we investigate two approaches to cause identification that exploit
the automatically learned semantic lexicon. The first approach is a heuristic approach,
which, motivated by Posse et al. (2005), labels a report with a shaping factor if it contains
at least a word or a phrase that is relevant to the shaping factor. Unlike Posse et al.s
work, where these relevant words and phrases employed by the heuristic procedure are
all manually identified, we automatically acquire these words and phrases via the semisupervised semantic lexicon learning procedure described above. The second approach is
a machine-learning approach that is somewhat orthogonal to NASAs approach: instead
of having a human identify seed words and phrases relevant to each shaping factor, we
have humans annotate a small subset of the available incident reports with their shaping
factors, and then apply a machine learning algorithm to train a classifier to automatically
label an unseen report, using combinations of N-gram features and words and phrases
automatically acquired by the aforementioned semantic lexicon learning procedure. As we
will see, we acquire this cause identifier using Support Vector Machines (SVMs), which have
been shown to be effective for topic-based text classification. Since we only have a small
number of labeled reports, we also attempt to combine labeled and unlabeled reports using
the transductive version of SVMs.
Since our approaches rely on simple linguistic knowledge sources that involve N-grams
and words and phrases automatically acquired during the semantic lexicon learning procedure, one may argue that the use of these simple features are not sufficient for cause
572

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

identification. It is important to point out that we are by no means arguing that these
features are sufficient for cause identification. However, the use of these simple features is
relevant for the task and is motivated by the work performed by the NASA researchers,
who, as mentioned above, have manually identified seed words and phrases for each shaping
factor (Posse et al., 2005). Our semantic lexicon learning procedure precisely aims to learn
such words and phrases. While our error analysis reveals that these simple linguistic features
are not sufficient for learning cause identification (and that more sophisticated knowledge
sources are needed to improve performance), as one of the first attempts to tackle this cause
identification task, we believe that the use of these simple features is a good starting point
and establishes a baseline against which future studies on this domain-specific problem can
be compared.
We evaluate the aforementioned two approaches on our manually annotated ASRS reports. Our experiments show a number of interesting results. First, the best performance is
achieved using the heuristic approach, where we label a report on the basis of the presence of
the automatically acquired lexicon words and phrases in the report, achieving an F-measure
of 50.21%. More importantly, this method significantly surpasses the performance of our
baseline system, which labels a report on the basis of the presence of a small set of manually
identified seed words and phrases. These results suggest that employing an automatically
acquired semantic lexicon is relevant and useful for cause-based text classification of the
ASRS reports. Second, the words and phrases in the learned semantic lexicon, when used
as features for training SVMs in the classification approach, do not improve the performance
of an SVM classifier that is trained solely on N-gram based features when the amount of
training data is small. However, when we increase the amount of training data (by crossvalidation), using the lexicon words and phrases as features in addition to unigrams and
bigrams helps improve classifier performance statistically significantly. In particular, we
have observed an F-measure of 53.66% from the SVM classifiers using a combination of
unigrams, bigrams and lexicon words and phrases as features. These results again confirm
that the words and phrases from the learned semantic lexicon are relevant and valuable
features for identifying the responsible shaping factors. Nevertheless, the magnitude of
the improvement indicates that there is still much room for improvement, which may be
achieved by using deeper semantic features.
In summary, we believe that our work on automated cause identification makes five
primary contributions:
 We show that instead of manually analyzing all the incident reports to identify the
relevant shaping factors, it is possible to reduce the amount of human effort required
for this task by manually analyzing only a small subset of the reports and identifying
the shaping factors of the rest of the reports by using automated methods.
 We propose several modifications to Thelen and Riloffs (2002) semi-supervised lexicon learning framework, and show that our Modified Basilisk framework allows us
to acquire a semantic lexicon that yields significantly better performance for cause
identification than the original Basilisk framework. Equally importantly, none of
our modifications are geared towards the cause identification task, and hence they
are applicable more generally to the semantic lexicon learning task. In fact, our addi573

fiAbedin, Ng & Khan

tional experiments suggest that Modified Basilisk yields better accuracy than Original
Basilisk when bootstrapping general semantic categories.
 We show that semantic lexicon learning is useful for cause identification from the ASRS
reports. In particular, the words and phrases from the learned semantic lexicon can
be profitably used to improve both a heuristic-based approach and a learning-based
approach (when given sufficient training data) to cause identification. In addition, we
believe that in any similar cause identification task where the causes are described
in the text, it may be useful to learn a semantic lexicon containing key words and
phrases related to the different types of possible causes and use these key words and
phrases as features for machine learning.
 In an attempt to deduce the weaknesses of our approaches and help direct future
research, we have performed an analysis of the errors made by the best-performing
system, namely the heuristic approach using the semantic lexicon learned by our
modified Basilisk method on a randomly chosen subset of the test reports.
 We have manually annotated a subset of the reports with the relevant shaping factors.
This set of annotated reports, which have been made publicly available, can serve as
a standard evaluation set for this task in future research and also for comparing to
other approaches to cause identification.
The rest of the paper is organized as follows. In Section 2, we discuss the dataset, the
shaping factors, and how the reports were preprocessed and annotated. Section 3 defines
the baseline, which simply looks for a small set of manually extracted seed words and
phrases in the report narratives. In Section 4, we describe our semantic lexicon learning
procedure, which is based on the Basilisk lexicon learning procedure (Thelen & Riloff,
2002) augmented with our modifications. In Section 5, we discuss our heuristic-based and
learning-based approaches to cause identification. We evaluate these two approaches in
Section 6 and discuss related work in Section 7. Finally, in Section 8, we summarize our
conclusions and discuss future work.

2. Dataset
The dataset used in this research is the aviation safety incident reports publicly available
from the website of Aviation Safety Reporting System5 . We used all 140,599 reports collected during the period from January 1998 to December 2007. Each report contains a
free text narrative written by the reporter and several fixed fields about the incident like
the time and place of the incident, environment information, details about the aircrafts
involved, the reporting persons credentials, details like anomaly, detector, resolution and
consequence about the incident itself, and a description of the situation. In other words,
the fixed fields in a report contain various information about what happened, and under
what physical circumstances, but do not cover why the incident took place. As discussed
by Posse et al. (2005) and Ferryman, Posse, Rosenthal, Srivastava, and Statler (2006), only
the narrative of a report contains information on the shaping factors of the incident. For
5. Available at http://asrs.arc.nasa.gov/search/database.html

574

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

this reason, we decided to analyze only the free-text narrative of a report using NLP techniques to identify what the shaping factor(s) of the incident may be, and we constructed
the corpus for this task by combining the narratives of these 140,599 reports.
2.1 Shaping Factors
The incidents described in the ASRS reports happen for a variety of reasons. Posse et al.
(2005) focus on the 14 shaping factors, or simply shapers. Following is a short description
of these shaping factors, taken verbatim from the work of Posse et al..
1. Attitude: Any indication of unprofessional or antagonistic attitude by a controller
or flight crew member.
2. Communication Environment: Interferences with communications in the cockpit
such as noise, auditory interference, radio frequency congestion, or language barrier.
3. Duty Cycle: A strong indication of an unusual working period e.g., a long day, flying
very late at night, exceeding duty time regulations, having short and inadequate rest
periods.
4. Familiarity: Any indication of a lack of factual knowledge, such as new to or unfamiliar with company, airport, or aircraft.
5. Illusion: Illusions include bright lights that cause something to blend in, black hole,
white out, or sloping terrain.
6. Physical Environment: Unusual physical conditions that could impair flying or
make things difficult, such as unusually hot or cold temperatures inside the cockpit,
cluttered workspace, visual interference, bad weather, or turbulence.
7. Physical Factors: Pilot ailment that could impair flying or make things more difficult, such as being tired, fatigued, drugged, incapacitated, influenced by alcohol,
suffering from vertigo, illness, dizziness, hypoxia, nausea, loss of sight, or loss of hearing.
8. Preoccupation: A preoccupation, distraction, or division of attention that creates
a deficit in performance, such as being preoccupied, busy (doing something else), or
distracted.
9. Pressure: Psychological pressure, such as feeling intimidated, pressured, pressed for
time, or being low on fuel.
10. Proficiency: A general deficit in capabilities, such as inexperience, lack of training,
not qualified, not current, or lack of proficiency.
11. Resource Deficiency: Absence, insufficient number, or poor quality of a resource,
such as overworked or unavailable controller, insufficient or out-of-date chart, equipment malfunction, inoperative, deferred, or missing equipment.
575

fiAbedin, Ng & Khan

12. Taskload: Indicators of a heavy workload or many tasks at once, such as shorthanded crew.
13. Unexpected: Something sudden and surprising that is not expected.
14. Other: Anything else that could be a shaper, such as shift change, passenger discomfort, or disorientation.
2.2 Preprocessing
For our semantic lexicon learning approach to cause identification, we need to identify
(1) the part-of-speech (POS) of each word in the text, (2) the phrases or chunks in the
sentences, and (3) the grammatical roles of the words and their governing words. Ideally, to
achieve high accuracies on these three tagging tasks, we would manually annotate a section
of the ASRS corpus with the appropriate annotations (e.g., POS tags, chunks) and train
appropriate taggers on it to tag the rest of the corpus. However, this by itself is a laborintensive task, and is beyond the scope of this paper. Therefore, we have used publicly
available tools trained on standard corpora for these three tasks. It is inevitable that this
will not produce the most accurate automatic annotations of our corpus, but as we will see,
this has not caused problem in this task.
From our corpus, we first identify sentence boundaries using the tool MXTERMINATOR6 . Second, we run the POS tagger CRFTagger (Phan, 2006b), which uses the Penn
Treebank tag set (Marcus, Santorini, & Marcinkiewicz, 1993), on the sentences detected by
MXTERMINATOR. Third, we run the chunker CRFChunker (Phan, 2006a) on the tagged
text to identify different types of phrases. Also, the Minipar parser (Lin, 1998) is run on the
sentences to identify the grammatical roles of the words. However, the report text has to be
preprocessed before applying these tools for reasons described in the following paragraphs.
The reports in the ASRS data set are usually informally written, using various domain
specific abbreviations and acronyms. In general, as observed by van Delden and Gomez
(2004), Posse et al. (2005) and Ferryman et al. (2006), these narratives tend to be written
in short, abbreviated manner, and tend to contain poor grammar. Also, the text has been
converted to all upper-case. Following is an example of the narrative of a report:
TAXIING FROM THE RAMP AT LAF AT NIGHT. MADE A WRONG TURN
AND CROSSED RWY 10/28; THE ACTIVE AT THE TIME. THERE WAS
NO SIGN TO INDICATE WHICH RWY I WAS XING. I CLRED BOTH DIRECTIONS BEFORE XING. WE WERE THE ONLY ACFT ON THE FIELD
AT THE TIME. NO MENTION ON THE ATIS OF SIGNS BEING OUT OR
CONSTRUCTION ON THE RAMP AREA. THE CTLR DIDNT QUESTION
US; IT WAS I WHO BROUGHT THE SIT UP AFTER I HAD CROSSED
THE ACTIVE RWY. COMMUTER OPS OF 3 DAYS OF HVY FLYING;
REDUCED REST; NO RWY SIGNS AND BUSY DOING LAST MIN COMMUTER PAPER WORK CHANGES; ALL CONTRIBUTED TO THE RWY
INCURSION. 12 HR DAY 6 HR FLT TIME.
6. ftp://ftp.cis.upenn.edu/pub/adwait/jmx/, trained on the Wall Street Journal corpus

576

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

These reports need some preprocessing before NLP techniques can be applied to them,
since these off-the-shelf tools (e.g., the POS tagger) were all trained on mixed-case texts.
For example, running CRFTagger (which was trained on the WSJ corpus with correct cases)
on the first two sentences yield the following:
1. TAXIING/NNP FROM/NNP THE/DT RAMP/NNP AT/IN LAF/NNP AT/IN
NIGHT/NN ./.
2. MADE/NNP A/DT WRONG/NNP TURN/NNP AND/CC CROSSED/VBD
RWY/NNP 10/28/CD ;/: THE/DT ACTIVE/NNP AT/IN THE/DT TIME/NN ./.
As can be seen, the tagger mislabels the words TAXIING, FROM, MADE, WRONG
and ACTIVE as proper nouns (NNP), instead of tagging them as verb, preposition, verb,
adjective and adjective respectively. This occurs because a good feature for detecting proper
nouns in a sentence is the case of its first character. Since all the words begin with a capital
letter, the tagger mistakes a significant portion of these words as NNP. Another reason that
the tagger performs poorly on this corpus is that a lot of abbreviations appear in the text.
For example, XING and HVY are short for crossing and heavy. But since they are not
likely to be known to a POS tagger trained on a standard well-edited corpus, they would
be identified as unknown words, and most likely be tagged as nouns instead of verb and
adjective respectively. Similar problems have been observed for the parsers and chunkers.
For this reason, we decided to preprocess the text by expanding the abbreviations and
restoring the cases of the words.
To expand the acronyms and abbreviations, we rely on the official list of acronyms and
abbreviations used in the ASRS reports7 . In a small number of cases, the same abbreviation
or acronym may have more than one expansion. For example, ARR may mean either arrival
or arrive. In such cases we arbitrarily chose one of the possibilities8 . Then, to restore case,
a set of English word lists, place names and person names9 were applied to the text to
identify the known words. If a word in the report text appeared in the word lists, then it
was converted to lower case. All the other unknown words were left uppercase. The result
of this process on the aforementioned narrative is as follows:
TAXIING from the ramp at LAF at night. made a wrong turn and crossed
runway 10/28; the active at the time. there was no sign to indicate which
runway I was crossing. I cleared both directions before crossing. we were the
only aircraft on the field at the time. no mention on the Automatic Terminal
Information Service of signs being out or construction on the ramp area. the
controller DIDNt question us; it was I who brought the situation up after I
had crossed the active runway. commuter operations of 3 days of heavy flying;
7. See http://akama.arc.nasa.gov/ASRSDBOnline/pdf/ASRS_Decode.pdf.
8. A better option would be to disambiguate between the alternative expansions based on context (e.g.,
the method followed by Banko & Brill, 2001). However, the number of such ambiguities in the acronyms
and abbreviations list is small (10, to be exact), and they are either the same POS or variations of the
same word. Thus the effect of these ambiguities on the performance of the NLP tools is expected to be
minimal.
9. http://wordlist.sourceforge.net/

577

fiAbedin, Ng & Khan

reduced rest; no runway signs and busy doing last minute commuter paper work
changes; all contributed to the runway incursion. 12 hour day 6 hour flight time.
We ran the POS tagger, CRFTagger, on this processed text and did not observe any
errors. For example, the tagged version of the aforementioned two sentences are:
1. TAXIING/VBG from/IN the/DT ramp/NN at/IN LAF/NNP at/IN night/NN ./.
2. made/VBN a/DT wrong/JJ turn/NN and/CC crossed/VBD runway/NN 10/28/CD
;/: the/DT active/JJ at/IN the/DT time/NN ./.
Both sentences have been correctly tagged. However, our case restoration method is
arguably too simplistic. Hence, to determine if we need to perform more fine-grained case
restoration, we sought a measure of how much would we gain from accurately restoring
the case of the words in the sentences over the present heuristic method. To check this,
we randomly picked 100 sentences from the corpus. We first ran the POS tagger on these
sentences after they were case-restored by the aforementioned heuristic case restoration
method. Then, we manually corrected the capitalization of these sentences and re-ran the
POS tagger on the case-restored sentences. When the tags thus generated were compared,
we found 99.7% agreement, which means that we are not likely to gain much in terms of
POS tagging accuracy from correctly case restored text than the heuristically case restored
text. Of the five differences out of 2049 words, three were NNPs mislabeled as NNs, which
essentially has no effect on outcomes of our research. Therefore, the marginal utility from
applying more sophisticated case restoration methods does not seem enough to justify the
additional effort necessary, and we limit our preprocessing step to the expansion of abbreviations and acronyms followed by the heuristic case restoration procedure described above.
The complete flow of preprocessing is shown in Figure 1.
2.3 Human Annotation Procedure
Recall that we need reports labeled with the shaping factors for training the cause identification classifiers and testing the performance of our two approaches to cause identification.
Additionally, in order to learn a semantic lexicon via bootstrapping, we need a small set of
seed words and phrases related to each shaping factor as a starting point. As a result, after
performing language normalization, we performed two types of annotations: (1) labeling a
set of reports with shaping factors, and (2) identifying a set of seed words and phrases from
the reports. The annotation procedure is described in more detail in the following sections.
2.3.1 Annotating Reports with Shaping Factors
While NASA has previously developed a heuristic approach to tackle the cause identification
task (Posse et al., 2005), this approach was evaluated on only 20 manually annotated reports,
which is far from satisfactory as far as establishing a strong baseline method is concerned.
Thus we decided to annotate a set of reports ourselves for evaluating our automatic cause
identification methods.
Out of the complete set of 140,599 reports, we chose a random set of 1333 reports for
annotation. This subset was divided into two parts. The first part, consisting of 233 reports,
578

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Figure 1: Flow chart of text preprocessing
was annotated by two persons (one undergraduate student and one graduate student). For
each report, they were asked to answer the following question:
Which shaping factor(s) were responsible for the incident described in the report?
Our annotators were trained in a similar way as those who labeled the 20 reports used in
the evaluation by the NASA researchers (see Posse et al., 2005). Specifically, as background
reading, the annotators were referred to the works of Posse et al. and Ferryman et al. (2006),
both of which describe the shaping factors, and also give some examples of the words and
phrases that indicate the influence of the shaping factors on the described incidents. The
definitions of the shapers are repeated in Section 2.1. Following Posse et al.s method,
our annotators were explicitly instructed to adhere to these definitions as much as possible
when annotating the reports with shaping factors. After the annotations were completed,
the inter-annotator agreement was computed using the Krippendorffs (2004)  statistics
as described by Artstein and Poesio (2008), using the Measuring Agreement on Set-valued
Items (MASI) scoring metric (Passonneau, 2004). The observed inter-annotator agreement,
, in this case was found to be 0.72, which indicates reliable agreement. Out of the 233
reports, they completely agreed on the annotations of 80 reports, completely disagreed on
100 reports and partially agreed on 53 reports. The annotators were then asked to discuss
the discrepancies. During the discussion, it was found that the discrepancies could be
579

fiAbedin, Ng & Khan

primarily attributed to the vagueness of the descriptions of the shaping factors in Posse et
al.s paper, some of which were interpreted differently by the two annotators.
The annotators then agreed on how the descriptions of the shapers should be interpreted,
and resolved all the differences in their annotation. After the discussion, the remaining 1100
reports were annotated by one of the annotators. The other annotator was also asked to
annotate a subset of these reports (100 reports) for cross-verification purpose10 , and the
inter-annotator agreement, , in this case was observed to be 0.66. The 1333 reports
annotated by the first annotator were divided into three sets: a training set (233 reports)
for training the cause identification classifiers, a held-out development set (100 reports)
for parameter tuning, and a test set (1000 reports) for evaluating the performance of our
approaches to cause identification. The distribution of the shaping factors in the training,
development and test sets are shown in the second, third and fourth columns of Table 1.
2.3.2 Extracting Seed Words and Phrases
In a separate process, the first author went through the first 233 reports that both annotators
worked on, and selected words and phrases relevant to each of the shaping factors. His
judgment of whether a word or phrase is relevant to a shaping factor was based on a careful
reading of the description of the shaping factors in the works of Posse et al. (2005) and
Ferryman et al. (2006), as well as the example seed words selected by the NASA experts
that were shown in these two papers. The specific task in this case was:
In each report, is there any word or phrase that is indicative of any of the
shaping factors? If there is, then identify it and assign it to the appropriate
shaping factor.
Note that these seed words and phrases were chosen without regard to the shaping factor
annotation of the document; they were picked on the possibility of their being relevant to
the respective shaping factors. The number of seed words and phrases for each shaping
factor is shown in the last column of Table 1. As we can see, 177 seed words and phrases
were manually selected from the 233 training reports. For completeness, we also show all the
seed words and phrases extracted from these reports in Appendix A. To facilitate further
research on this topic, the annotated data we have used in this research is made available
at http://www.utdallas.edu/~maa056000/asrs.html.
Since there is no gold standard against which we can compare this list of annotated
words and phrases, it is difficult to directly compute its precision. However, to get a rough
idea of its precision, we asked one of the annotators to examine the list and identify all and
only those words and phrases in the list that he believes are correct. There was disagreement
over only one word. This yields a precision of 99.44%, which provides suggestive evidence
that the annotation is fairly reliable. These manually identified words and phrases were
used by our baseline cause identification system (see Section 3) and also served as seeds for
our semantic lexicon learning procedure (see Section 4).
10. It is a fairly standard procedure in NLP research to cross-annotate only a subset of the data when
complexity and cost of individual annotation is high. See the works of Zaidan, Eisner, and Piatko (2007)
and Kersey, Di Eugenio, Jordan, and Katz (2009), for instance.

580

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Table 1: Distribution of shaping factors in the training, test and development sets
Shaping factor
Reports in Reports in
Reports in
Seed
training set
test set development words
test set
Attitude
17
30
5
8
Communication Environment
11
90
18
5
Duty Cycle
9
26
3
10
Familiarity
12
50
8
9
Illusion
1
2
0
1
Other
36
217
36
8
Physical Environment
43
265
40
45
Physical Factors
10
35
3
8
Preoccupation
25
110
10
9
Pressure
5
30
3
10
Proficiency
43
247
23
12
Resource Deficiency
112
507
33
47
Taskload
6
29
7
2
Unexpected
3
10
1
3
Total
233
1000
100
177

3. Baseline System For Cause Identification
As discussed in the introduction, the goal of our research is to label the incident reports with
the shaping factors that caused the incidents. To evaluate the performance of our cause
identification methods, we need a baseline that uses the same amount of training data
as all the methods described in this paper and performs reasonably well on the test set.
Given that cause identification is a relatively new and under-investigated task, no standard
baseline has been adopted for this task. In fact, to our knowledge, the only related works
on cause identification for the aviation safety domain were conducted by the researchers at
NASA (see Posse et al., 2005; Ferryman et al., 2006). As a result, we construct a baseline
system motivated by Posse et al.s work. Specifically, the baseline takes as input a set of
seed words and phrases manually collected for each of the shaping factors (see Section 2.3.2),
and labels a report with the Occurrence Heuristic: for each seed word and phrase found
in the report, the baseline annotates the report with the shaping factor associated with
the seed. For example, 11 hour duty day is a seed phrase associated with the shaping
factor Duty Cycle. Then, the Occurrence Heuristic will label any report that contains the
phrase 11 hour duty daywith Duty Cycle. This approach is simple but attractive because
(1) it does not need any training, (2) it can be evaluated very easily, by searching for the
seed words in the narrative of the report being labeled, and (3) a report can potentially
be labeled with more than one shaping factors. If the seed words and phrases are indeed
relevant to their respective shaping factors, then they should identify the reports related to
the shaping factors with a high degree of precision.
581

fiAbedin, Ng & Khan

4. Semantic Lexicon Learning
As described in Section 3, the baseline uses the seed words and phrases manually extracted
from 233 reports in combination with the Occurrence Heuristic to label the reports with
shaping factors. However, the reports used for evaluation may not contain exactly the
same words and phrases, but they may contain different variations, synonyms, or words
and phrases that are semantically similar to the seed words and phrases. Thus the baseline
may not be able to label these reports correctly by only looking for the words and phrases
in the seed words list.
To address this potential problem, we propose to use semantic lexicon learning algorithms to learn more words and phrases semantically similar to the seed words and phrases
from the reports corpus containing narratives from 140,599 reports. Using a weakly supervised bootstrapping algorithm may allow us to learn a large number of useful words and
phrases from the corpus that would have required huge amounts of human effort had it been
done manually. Below, we first describe the general bootstrapping approach in Section 4.1.
Then, in Section 4.2, we describe the Basilisk framework for learning the semantic lexicon
from an unannotated corpus (Thelen & Riloff, 2002). Finally, in Section 4.3, we discuss our
modifications to the Basilisk framework.
4.1 Weakly Supervised Lexicon Learning
As mentioned earlier, we employ a weakly supervised bootstrapping approach for building
the semantic lexicon. We use the manually extracted seed words and phrases for each
shaping factor (described in Section 2.3.2) to create the initial semantic lexicon. Then we
select words and phrases from the unannotated reports that are semantically similar to the
words already appearing in the semantic lexicon. The reports in the corpus do not need to
be labeled with shaping factors. The semantic similarity between two words is measured
using features extracted from the corpus for each word. This process is repeated iteratively:
in each iteration, a certain number of words are added to the semantic lexicon, and the
words in this augmented lexicon are used as the seeds for the following iteration. This
process is shown in Figure 2.

Figure 2: Flow chart of the lexicon learning procedure

582

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

4.2 Basilisk Framework
Basilisk (Bootstrapping Approach to SemantIc Lexicon Induction using Semantic Knowledge)
is an instantiation of the aforementioned generic semantic lexicon learning framework (Thelen & Riloff, 2002). The Basilisk framework works by first identifying all the patterns for
extracting all the noun phrases in the corpus that appear in one of three syntactic roles:
subject, direct object, or prepositional phrase object. For example, as discussed by Thelen and Riloff, in the sentence John was arrested because he collaborated with Smith and
murdered Brown, the extraction patterns are <subject> was arrested, which extracts
John, murdered <object> which extracts Brown and collaborated with <pp object>
which extracts Smith. Then, for each semantic category Sk , a pattern pool is constructed
with patterns that tend to extract words in Sk . To measure the tendency of a pattern Pj
to extract words in Sk , the R log F metric is used, which is defined as:
R log F (Pj ) =

Fj
 log (Fj )
Nj

(1)

Here, Fj is the number of (distinct) words in Sk that pattern Pj extracts, and Nj is the
total number of (distinct) words in the corpus that Pj extracts. This metric is high for both
high precision patterns (i.e., patterns that extract primarily words in Sk ) and high recall
patterns (i.e., patterns that extract a large number of words in Sk ). At each iteration i, the
top (20 + i) patterns (in terms of their R log F scores) are put into the pattern pool for Sk .
Depleted patterns (i.e., patterns that have all their extracted words already in the semantic
lexicon) are not considered in this step. Then, the head nouns of all the phrases extracted
by the resulting patterns in the pattern pool are put into the word pool of Sk .
Next, a subset of the words in the word pool is selected to be added to the seed words
list. Those words from the word pool are chosen that are most relevant to Sk . More
specifically, for each word Wi in the word pool for Sk , first the AvgLog score is calculated,
which is defined as follows:

AvgLog (Wi , Sk ) =

W
Pi
X

log2 (Fj + 1)

j=1

W Pi

(2)

Here, W Pi is the number of patterns that extract word Wi , and for each pattern Pj that
extracts Wi , Fj is the number of words extracted by Pj that belong to Sk . Then, for each
semantic category Sk , five words are chosen that have the highest AvgLog score for the
category Sk .
For multi-category learning, Thelen and Riloff (2002) experimented with different scoring metrics and reported that they achieved the best performance by calculating the diff
score for each word. For a given word in the word pool for a semantic category, the diff
score takes into consideration what score this word gets for the other categories, and returns
a score based on the words score for this semantic category relative to the other categories.
More precisely, the diff score is defined as follows:
dif f (Wi , Sk ) = AvgLog (Wi , Sk )  max (AvgLog (Wi , Sl ))
l6=k

583

(3)

fiAbedin, Ng & Khan

Here, Sk is the semantic category for which Wi is being evaluated. Thus the diff score is
high if there is strong evidence that Wi belongs to semantic category Sk but little evidence
that it belongs to the other semantic categories. For each semantic category, the diff score
is calculated for each word in the categorys word pool, and the top five words with the
highest diff score are added to the lexicon for that category. Two additional checks are
made at this stage: (1) if a word in the word pool has been added to some other category in
an earlier iteration, that word is discarded, and (2) if the same word is found in more than
one word pool then it is added to the category for which it has the highest score11 . When
this is completed for all the semantic categories, the iteration ends, and the next iteration
begins with the augmented lexicon.
4.3 Modifications to the Basilisk Framework
As we will see later in this subsection, an analysis of the framework reveals that in some
cases the words selected by Basilisk may not be the most relevant ones. For this reason, we
propose three algorithmic modifications to the Basilisk framework: (1) using a new semantic
similarity measure, (2) merging the word pools to one single pool for assigning words to the
semantic categories, and (3) imposing minimum support and maximum generality criteria on
patterns and words added to the pattern pools and the word pools. In addition, we propose
one linguistic modification, in which we employ a type of feature that can be computed in
a robust manner from the words and phrases in the corpus, namely, the N-gram features.
The rest of this subsection discusses these modifications.
4.3.1 Modification 1: New Semantic Similarity Measure
As seen in Section 4.2, the Basilisk framework uses the AvgLog scoring function to measure
the semantic similarity between words. The diff score for multi-category learning also uses
the AvgLog function to compute the evidence for a word belonging to a semantic category
relative to the other categories. However, a closer examination of the AvgLog function shows
that it may not be able to properly predict semantic similarity under all circumstances. To
understand the reason, let us first make the following observations: if pattern Pj occurs
1000 times, but extracts words in category Sk only 5 times, it is unlikely that Pj is strongly
related to Sk . Similarly, if word Wi occurs 1000 times, but is extracted by pattern Pj only 5
times, Pj should have small influence on the classification of Wi . However, the AvgLog score
will not be able to take these factors into consideration, precisely because it considers only
the absolute number of semantic category members extracted by the patterns that extract
the word but not the frequency of extraction. To see why this is the case, let us consider the
word Wi that is extracted by three patterns P1 , P2 and P3 , with the frequencies as shown in
Table 2. If each of P1 , P2 and P3 extract five distinct seed words, then the AvgLog score for
the word W would be 2.32, irrespective of the fact that the patterns actually extract a word
in the seed words list only a tiny fraction of their occurrence in the corpus. P1 extracts a
seed word 5% of its occurrence, P2 does so 1% time, and P3 , the pattern that extracts W
most often, extracts a lexicon word only 0.5% of the times it appears in the text. Clearly,
11. This approach effectively assumes that each word can belong to at most one category. This is a reasonable
assumption in this specific task since the shaping factors have very distinct meanings.

584

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

the patterns would not suggest that Wi is related to the semantic category, yet it gets a
good score.
Table 2: Illustration of the problem with AvgLog: How unrelated words may have a high
similarity score. Here Wi is a word that appears in the corpus and is extracted by
the patterns P1 , P2 and P3

Patterns that extract Wi
Number of times Wi is extracted by the pattern Pj
Number of times pattern Pj occurs in the text
Number of times a word in category Sk is extracted by the pattern Pj
Number of category words extracted by the pattern Pj
log2 (Fj + 1)
AvgLog (Wi )

P1
10
100
5
5
2.32

P2
P3
20
70
500 1000
5
5
5
5
2.32 2.32
2.32

Keeping this in mind, we propose our probabilistic metric, SemProb, which computes the
probability that the word Wi belongs to the semantic category Sk given that it is extracted
by the patterns P1 , P2 , . . . , Pn . More specifically, SemProb is calculated as follows:
SemP rob (Wi , Sk ) = P rob (Sk |Wi )
X
=
P rob (Sk |Pj )  P rob (Pj |Wi )

(4)

Pj

In other words, SemProb assumes that the semantic category Sk and the word Wi are
conditionally independent given Pj , a pattern that extracts Wi . The probabilities in this
equation are estimated using maximum likelihood estimation from the corpus. Specifically,
to compute P rob (Pj |Wi ), we divide the number of times Pj extracts Wi in the corpus by the
total number of times that Wi appears in the corpus. To compute P rob (Sk |Pj ), we divide
the number of times Pj extracts a word in the semantic category Sk by the total number
of times Pj appears in the corpus. For a given word Wi and a given semantic category
Sk , the sum of the products of these two quantities over all the patterns that extract Wi
gives the probability of category Sk given word Wi . This method does not suffer from the
problem faced by AvgLog since it depends on the probability of the word being extracted
by the patterns and the patterns probability of extracting words in the category. For the
same example in Table 2, the SemProb metric for the word Wi is 0.0105, illustrating how
low the probability of Wi s belonging to the semantic category Sk is. The details are given
in Table 3.
4.3.2 Modification 2: Common Word Pool
Since we have to compute Eqn (4) for every word in the word pool for each of the categories
and assign the word to the semantic category for which the probability is highest, we change
the framework so that we have only one common word pool for all the semantic categories.
585

fiAbedin, Ng & Khan

Table 3: Illustration of the effectiveness of SemProb: How unrelated words get low similarity
score.

Patterns that extract Wi
Number of times that Wi is extracted by the pattern Pj
Number of times pattern Pj occurs in the text
Number of times a word in category Sk is extracted by the pattern Pj
P rob (Wi is extracted by Pj )
P rob (Pj extracts a word in Sk )
P rob (Wi is extracted by Pj )  P rob (Pj extracts a word in Sk )
SemP rob (Wi , Sk ) = P rob (Wi belongs to semantic category Sk )

P1
10
100
5
0.1
0.05
0.005

P2
P3
20
70
500
1000
5
5
0.2
0.7
0.01
0.005
0.002 0.0035
0.0105

We still have separate pattern pools for different semantic categories, but the words related
to patterns in the pattern pools will be put into the same common word pool, and allocated
to the most probable semantic category from there. If there are separate word pools for each
semantic category, then we have to add a fixed number of words to each category in each
iterations. Such a constraint may undesirably cause a word to be added to a category that
is not the most likely. However, since we have only one word pool after our modification, we
do not have the constraint that we have to add a fixed number of words to each category,
and we can assign each word to its most likely category. Thus the number of words added
to different categories may vary in the same iteration.
4.3.3 Modification 3: Minimum Support and Maximum Generality
There are some scenarios in which the SemProb metric can produce undesirable results. For
example, consider a very infrequent word Wi that occurs in the entire corpus exactly once.
Assume that pattern Pj , which extracts Wi , extracts words in semantic category Sk with
70% probability. So, according to SemProb, the probability that Wi belongs to Sk becomes
70%. However, this is not sufficient evidence for Wi to belongs Sk . Such cases not being too
uncommon, we have imposed a minimum word frequency constraint on the words that are
put into the word pool, so that words that appear less than a certain number of times are
not considered. A pattern that appears too infrequently in the corpus can also lead to such
a problem. Consider a very infrequent pattern, Pj , that appears exactly twice in the corpus
and extracts two words. If one of these words happen to be a seed word, then the other
word will have a 50% probability to belong to the category of the seed word and Pj will have
R log F value of 0.5. However, since Pj is so infrequent, it does not convey a good evidence
for membership in the semantic category, and we should not allow Pj to put words into the
word pool. Therefore, we disallow such low frequency patterns from being included in the
pattern pool by adding the constraint that the patterns put into the pattern pool must also
have a minimum pattern frequency. Besides these two constraints imposed on the frequency
of occurrence of the words and the patterns, we employ two additional constraints. The first
586

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

is the maximum pattern generality constraint: motivated by Rychly and Kilgarriff (2007),
we remove from consideration patterns that are too general (i.e., patterns that extract too
many words), by imposing an upper limit on the number of distinct words that a pattern
to be added to a pattern pool can extract. The second is the maximum word frequency
constraint: since content-bearing words are likely to have a lower frequency (see Davidov
& Rappoport, 2006), we impose an upper limit on the maximum number of times a word
appears in the corpus. The four thresholds associated with these four frequency-based
constraints will be tuned automatically using the held-out development set.
4.3.4 Modification 4: N-gram Patterns
In addition to the parse-tree-based subject-verb and verb-object patterns already employed
by Basilisk, we also employ N-gram-based extraction patterns, with the goal of more robustly capturing the context in which the words appear. We construct N-gram extraction
patterns as follows. For each noun and adjective, X, in the corpus, we create two N-gram
patterns for extracting X: (a) the preceding N words + hXi, and (b) hXi + the succeeding
N words. For example, in the sentence ... a solid line of thunderstorms was detected ...,
the bigram patterns for thunderstorms would be: line of hXi and hXi was detected.
The complete sentence is approaching the ATL area a solid line of thunderstorms was
detected in the vicinity of the airport, and the words and their extracting bigram patterns
would be:
 ATL: approaching the hXi, hXi area a
 area: the ATL hXi, hXi a solid
 solid: area a hXi, hXi line of
 line: a solid hXi, hXi of thunderstorms
 thunderstorms: line of hXi, hXi was detected
 vicinity: in the hXi, hXi of the
 airport: of the hXi
In addition to constructing N-gram patterns for extracting words, we also construct
N-gram patterns for extracting phrases. To do so, we first remove articles (a, an, the) and
possessive pronouns and adjectives (e.g., my, his) from the beginning of the phrases in the
corpus. For each noun phrase and adjective phrase, X, that appears in the corpus, we
create two N-gram patterns for extracting X: (a) The preceding N words + hXi, and (b)
hXi + the succeeding N words. For example, from the sentence this was the last of 5 legs
and approaching the end of an 8 hour duty day and 7 hour hard time flying day, we would
extract the following phrases with the following bigram patterns:
 5 legs: last of hXi, hXi and approaching
 end: and approaching hXi, hXi of an
587

fiAbedin, Ng & Khan

 8 hour duty day: end of hXi, hXi and 7
 7 hour hard time flying day: day and hXi
Thus we use three types of patterns in our experiments: bigram patterns for extracting
words, bigram patterns for extracting phrases, and parse-tree-based subject-verb and verbobject patterns. All these patterns were generated from the reports corpus generated by
combining the narratives of the 140,599 unlabeled reports described in Section 2.2. As
we will see, not all three types of patterns are beneficial to use as far as performance
is concerned. In Section 6, we will show how to automatically select the best subset of
patterns to use based on the development set.

5. Semantic Lexicon-Based Approaches to Cause Identification From
ASRS Reports
We investigate a heuristic-based approach and a learning-based approach to cause identification, both of which exploit information provided by an automatically acquired semantic
lexicon. This section describes the details of these two approaches.
5.1 Heuristic-Based Approach
The heuristic-based approach operates in essentially the same way as the baseline cause
identification system described in Section 3, where the Occurrence Heuristic is used to label
a report with shaping factors. The only difference is that the words and phrases used
by the Occurrence Heuristic in the baseline are manually identified, whereas those in our
heuristic-based approach are acquired by our Modified Basilisk procedure.
5.2 Learning-Based Approach
Our learning-based approach to the cause identification problem is to recast it as a classification task. Note that we have a multi-class multi-labeled classification task: there are 14
classes and each report can be labeled with more than one class. A number of approaches
have been proposed to tackle multi-class multi-labeled classification tasks. In the rest of
this section, we describe the three existing approaches to multi-class multi-labeled text classification that we explore in our experiments (Section 5.2.1), and provide an overview of
the theory of Support Vector Machines (SVMs), the underlying learning algorithm we use
to train classifiers employed by these three approaches (Section 5.2.2).
5.2.1 Three Approaches to Multi-Class Multi-Labeled Text Classification
One-Versus-All. In this approach, we train one binary classifier for each shaping factor
Sk to determine whether a report will be labeled with Sk . More specifically, we follow the
One-Versus-All classification scheme: for a given Sk , the reports in the training set that
contains Sk in its set of labels (assigned by the annotator) are the positive instances for the
binary classifier and the rest of the reports in the training set are the negative instances.
After training, we apply the classifiers to a report in the test set independently of other
reports, and label the report with each Sk for which the corresponding classifier classifies
588

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

the report as positive. Thus we convert cause identification to a multi-class multi-labeled
document classification task.
While any learning algorithm can be used in principle to train classifiers for this OneVersus-All scheme, we use Support Vector Machines12 for training and testing the classifiers,
primarily due to its successes in various text classification tasks. Each classifier is trained
with two types of features: (1) unigrams and bigrams from the report narratives, and (2)
words and phrases from the semantic lexicon. The feature values are TF*IDF values.
While our shaping factor-labeled data set of 1333 reports is substantially larger than the
set of 20 reports annotated by the NASA researchers (see Section 1), it is arguably fairly
small from a machine learning perspective. Hence, it is conceivable that the performance
of our SVM classifiers would be limited by the small size of the training data. As a result,
we investigate whether we can improve the One-Versus-All approach using a transductive
SVM, which is a version of the inductive SVM described above that attempts to improve
classifier performance by combining both labeled and unlabeled data (see Section 5.2.2
for an overview of transductive learning). For our cause identification task, the unlabeled
reports in the test set serve as unlabeled data in the transductive learning procedure.
MetaLabeler. As our second approach, we employ MetaLabeler (Tang, Rajan, & Narayanan,
2009) for classifying multi-class multi-labeled text data. Here, a model is first learned that
predicts the number of labels that an instance may have. In addition, a set of binary classifier models, one for each possible label, are learned to predict the likelihood of each label
for an instance. When an instance is classified, the first model predicts K, the number of
possible labels for that instance, and from the output of the second set of classifiers, K
labels are chosen with the highest likelihood for that instance.
In our implementation of this approach, the first model is learned using SVMmulticlass ,
which is an implementation of multi-class SVM described by Crammer and Singer (2002)13 .
The second set of classifiers are the same set described in Section 5.2.2. But in this case,
for a given instance x, the decision functions f (x) = w  x  b for each of the classifiers are
evaluated, and the positive decision values are sorted. Then the top K labels corresponding
to the highest values of the decision functions are assigned to the instance. Both the
multiclass classifier and the set of binary classifiers are trained using the same types of
features as in the One-Versus-All approach, namely unigrams and bigrams from the reports,
and words and phrases from the semantic lexicon. The feature values are also the same as
in One-Versus-All approach, namely TF*IDF values.
Ensembles of Pruned Sets. In the Pruned Sets approach (Read, Pfahringer, & Holmes,
2008), the multi-class multi-label text classification problem is transformed into a multiclass single-label text classification problem by selecting a subset of the label combinations
most frequently occurring in the dataset and assigning a unique pseudo-label to each chosen
label combination.
The first step in this algorithm is to choose the label sets for training. In this step,
those label sets are chosen that meet the minimum frequency requirement in the training
set. Using the minimum frequency constraint prunes away infrequently occurring label sets
that have frequency less than p, leaving only label combinations that are frequent and thus
12. As implemented in the SVMlight software package by Joachims (1999)
13. Available at http://svmlight.joachims.org/svm_multiclass.html

589

fiAbedin, Ng & Khan

more important. The training instances that are labeled with the pruned label sets are
also removed from the training set. The minimum cardinality parameter, b, is then used
to reintroduce some of the pruned instances back to the training set in order to minimize
the information loss from the pruning process. First the label sets of the rejected instances
are broken down into smaller subsets of at least size b. Then those new subsets that have
frequency higher than p are reintroduced, and the pruned training instances whose label
sets are supersets of these newly accepted label sets are reinstated into the training set. The
role of the parameter b in this case is to ensure that not too many such instances with small
label sets are put back, because that will cause the average number of labels to reduce,
resulting in smaller number of labels per instance at classification time.
The next step is to learn classifiers on the selected label sets. First, each accepted label
set is assigned a unique pseudo-label, thus transforming the multi-label classification problem into a single-label classification problem. Then an ensemble of M classifiers is learned
to predict these pseudo-labels given an instance (using the same multi-class SVM implementation as in MetaLabeler), where each classifier in the ensemble is trained on a different
random sample of the training data. Since (1) the label sets for training the classifiers
represent only a subset of all the label combinations present in the original training data
and (2) the test data may contain label combinations that are not present in the training
data, having an ensemble of classifiers allows the system to generate label combinations not
observed at training time. For example, let the label combinations {l1 , l3 } and {l2 , l3 } be
present in the training data. Then, if one classifier in the ensemble labels a test instance
with {l1 , l3 } and another classifier in the ensemble labels the same instance with {l2 , l3 },
then that instance may be labeled with {l1 , l2 , l3 } (depending on the actual voting policy in
effect at classification time) even if this combination is not present in the training data. The
classifiers in the ensemble are built using the same two types of features as the One-VersusAll approach, namely unigrams and bigrams from the reports and words and phrases from
the semantic lexicon learned by our modified Basilisk framework.
Finally, when classifying an instance, each of the M classifiers assigns one pseudo-label
to the instance. These pseudo-labels are then mapped back to the original label combination
and the vote for each actual label is counted and normalized by dividing by the number of
classifiers, M , in order to bring the prediction for each possible label to the range between
0.0 and 1.0. Then a threshold t is used such that each label that has a prediction value
greater than or equal to t is assigned to the instance. This scheme is used to make it possible
to assign label combinations unseen at training time to the test instances.
5.2.2 An Overview of Support Vector Machines
SVMs have been shown to be very effective in text classification (Joachims, 1999). Below
we describe two versions of SVMs: (1) inductive SVMs, which learn a classifier solely from
labeled data, and (2) transductive SVMs, which learn a classifier from both labeled and
unlabeled data.
Inductive SVMs. Given a training set consisting of data points belonging to two classes,
an inductive SVM aims to find a separating hyperplane that maximizes the distance from
the separating hyperplane to the nearest data points. These nearest data points act as the
support vectors for the plane.
590

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

More formally, let D be the data set with m data points where
D = {(xi , ci ) |xi  Rn , ci  {1, 1} , 1  i  m}

(5)

Each point xi is represented as an n-dimensional vector and is associated with a class label
ci . The inductive SVM classifier attempts to find a hyperplane w  x  b = 0 that is at the
maximum distance from the nearest data points of opposite labels. This hyperplane would
be in the middle of the two hyperplanes containing the support vectors of each class. These
2
. Therefore, the
two hyperplanes are wxb = 1 and wxb = 1, and their distance is |w|
desired separating hyperplane can be found by solving the following quadratic programming
optimization problem:
Minimize
subject to

1
|w|2
2
ci (w  xi  b)  1, 1  i  m

(6)

However, in practice many classes are not linearly separable. To handle these cases, a set
of slack variables is used to represent the misclassification of point xi . Then the problem
becomes:
X
1
|w|2 + C
i
Minimize
2
i

subject to

ci (w  xi  b)  1  i , i > 0, 1  i  m

(7)

where the i are additional variables representing training errors and C is a constant representing trade-off between training error and margin. More details can be found in Cortes
and Vapnik (1995). In our experiments, we use the radial basis function (RBF)
kernel,



2
where every dot product is replaced by the function k (x, x ) = exp |x, x | , for  > 0.
In addition, both  and C are chosen by cross-validation on the training set.
Transductive SVMs. In the transductive setting, in addition to the set of labeled data
points, we also exploit a set of unlabeled data points, T = {xi |xi  Rn , 1  i  k}, that
are taken from the test set. As described by Joachims (1999), the goal is then to minimize
the expected number of classification errors over the test set. The expected error rate is
defined in Vapnik (1998) as follows:
Z
1X
(8)
 (hL (xi ) , ci ) dP (x1 , c1 ) . . . dP (xk , ck )
R (L) =
k
i

where L = D  T , hL is the hypothesis learned from L, and  (a, b) is zero if a = b
and one otherwise. The labeling ci of the test data and the hyperplane that maximizes the
separations of both training and testing positive and negative instances are found by solving
the following quadratic programming optimization problem, which is a modified version of
Eqn (7):
X
X
1
j
|w|2 + C
i + C 
Minimize
2
i

subject to

j

ci (w  xi  b)  1  i , i > 0, 1  i  m

cj w  xj  b  1  j , j > 0, 1  j  k
591

(9)

fiAbedin, Ng & Khan

Similar to the inductive SVM in Section 5.2.2, we use the RBF kernel in our experiments
involving the transductive SVM.

6. Evaluation
The goal of our evaluation is to study the effectiveness of our two approaches to cause identification, namely the semantic lexicon learning approach and the classification approach.
We do so by testing the performance of the approaches on a randomly chosen set of reports
that have been manually annotated with the shaping factors that caused the incidents described in them (Section 2.3.1). We start by describing the experimental setup (Section 6.1),
followed by the baseline results (Section 6.2) and the performance of our two approaches
(Sections 6.3 and 6.4). We then describe the experiment where we increase the amount
of training data available to the classification approach and investigate how this impacts
performance (Section 6.5). After that, we perform an analysis of the errors of the bestperforming approach (Section 6.6) and conduct additional experiments in an attempt to
gain a better insight into the cause identification task that can help direct future research
(Section 6.7). Finally, we present a summary of the major conclusions that we draw from
the experiments (Section 6.8).
6.1 Experimental Setup
As described in Section 2.3, out of the 140,599 reports in the entire corpus, we have manually
annotated 1333 incident reports with the shaping factors. We have used the first 233 of
them to (1) manually extract the initial seed words and phrases for the semantic lexicon
learning procedure, and (2) train classifiers for identifying shaping factors associated with
a report. Of the remaining reports, we have used 1000 reports as test data and 100 reports
as development data (for parameter tuning).
6.1.1 Evaluation Metrics
As mentioned in Section 2.1, there are 14 shaping factors, and a report may be labeled
with one or more of these shaping factors. We evaluate the performance of our cause
identification approaches based on how well the automatic annotations match the human
annotations of the reports in the test set. For evaluation, we use precision, recall and
F-measure, which are computed as described by Sebastiani (2002). Specifically, for each
shaping factor Si , i = 1, 2, . . . 14, let ni be the number of reports in the test set that the
human annotator has labeled with Si , i.e., the number of true Si -labeled reports in the test
set. Further, let pi be the number of reports that an automatic labeling scheme Ci has
labeled with Si , and let tpi be the number of reports that Ci has labeled correctly with Si .
Then, for the shaping factor Si , we have the following performance metrics:
 Precisioni is the fraction of reports that are really caused by shaping factor Si among
all the reports that are labeled with Si by the labeling scheme.
P recisioni =

592

tpi
pi

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

 Recalli is the percentage of reports really caused by shaping factor Si that are labeled
by the labeling scheme with the shaping factor Si .
Recalli =

tpi
ni

Thus we obtain a measure of the labeling schemes performance for each of the shaping
factors. To obtain the overall performance of the labeling scheme, we sum these counts
(i.e., ni , pi and tpi ) over all shaping factors and compute the micro-averaged precision,
recall and F-measure from the aggregated counts as described by Sebastiani and repeated
as follows:
P
tpi
P recision = Pi
pi
Pi
tpi
Recall = Pi
i ni
2  P recision  Recall
F -measure =
P recision + Recall
Thus for each labeling scheme we have one set of overall scores reflecting its performance
over all classes.
6.1.2 Statistical Significance Tests
To determine whether a labeling scheme is better than another, we apply two statistical
significance tests  McNemars test (Everitt, 1977; Dietterich, 1998) and the stratified approximate randomization test (Noreen, 1989)  to test whether the difference in their performances is really statistically significant. McNemars test compares two labeling schemes
on the basis of errors (i.e., whether both the labeling schemes are making the same mistakes), and the stratified approximate randomization test compares the labeling schemes
on F-measure. Both tests have been extensively used in machine learning and NLP literature. In particular, stratified approximate randomization is the standard significance test
employed by the organizers of the Message Understanding Conferences to determine if the
difference in F-measure scores achieved by two information extraction systems is significant (see Chinchor, 1992; Chinchor, Hirschman, & Lewis, 1993). Since we are ultimately
concerned about the difference in F-measure scores between two labeling schemes in cause
identification, our discussion of statistical significance in the rest of this section will be focused solely on the stratified approximate randomization test. For both tests, we determine
significance at the level of p < 0.05.
6.2 Baseline System
Recall that we use as our baseline the heuristic method described in Section 3, where the
Occurrence Heuristic is used to label a report using the seed words and phrases manually
extracted from the 233 training reports. Results, shown in the Experiment 1 section of
Table 4, are reported in terms of precision (P), recall (R), and F-measure (F). The last
two columns show whether a particular automatic labeling scheme is significantly better
593

fiAbedin, Ng & Khan

than the baseline with respect to McNemars test (MN) and stratified approximate randomization test (AR) [Statistical significance and insignificance are denoted by a X and an
X, respectively]. When evaluated on the 1000 reports in the test set, the baseline achieves
a precision of 56.48%, a recall of 40.47% and an F-measure of 47.15%.
Table 4: Report labeling performance of different methods.
Approach Feature Set
P
R
F MN
AR
Experiment 1: Baseline
Heuristic Seed words
56.48 40.47 47.15 N/A N/A
Experiment 2: Semantic lexicon approach
Lexicon from modified Basilisk
53.15 47.57 50.21
X
X
Heuristic
Lexicon from original Basilisk
49.23 42.78 45.78
X
X
Experiment 3: Supervised One-Versus-All classification approach
Unigrams
37.54 64.50 47.46
X
X
Unigrams and bigrams
42.19 47.39 44.64
X
X
SVM
Lexicon words
48.72 37.08 42.11
X
X
Unigrams and lexicon words
37.05 65.96 47.45
X
X
Unigrams, bigrams, lexicon words 51.19 36.59 42.68
X
X
Experiment 4: Transductive One-Versus-All classification approach
Unigrams
11.84 67.78 20.16
X
X
Unigrams and bigrams
50.00 33.86 40.38
X
X
SVM
Lexicon from modified Basilisk
42.83 30.64 35.73
X
X
Unigrams and lexicon words
51.30 38.29 43.85
X
X
Unigrams, bigrams, lexicon words 55.90 32.77 41.32
X
X
Experiment 5: MetaLabeler approach
Unigrams
58.80 16.63 25.92
X
X
Unigrams and bigrams
66.02 20.51 31.30
X
X
SVM
Lexicon words
63.23 17.11 26.93
X
X
Unigrams and lexicon words
70.29 20.39 31.61
X
X
Unigrams, bigrams, lexicon words 68.79 24.21 35.82
X
X
Experiment 6: Ensembles of pruned sets approach
Unigrams
22.44 63.05 33.09
X
X
Unigrams and bigrams
22.22 67.42 33.42
X
X
SVM
Lexicon from modified Basilisk
20.72 73.67 32.35
X
X
Unigrams and lexicon words
23.72 85.25 37.12
X
X
Unigrams, bigrams, lexicon words 16.93 71.42 27.37
X
X
Experiment 7: Additional training data with 5-fold cross-validation
Unigrams
42.21 63.65 50.76
X
X
Unigrams and bigrams
43.58 58.31 49.88
X
X
SVM
Lexicon words
56.06 40.41 46.97
X
X
Unigrams and lexicon words
54.75 52.43 53.56
X
X
Unigrams, bigrams, lexicon words 54.81 52.55 53.66
X
X

594

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

6.3 Experiments with Semantic Lexicon Approach
Recall that in the semantic lexicon learning approach, we label a report in the test set using
the Occurrence Heuristic in combination with the semantic lexicon learned by the modified
Basilisk framework described in Section 4.3. Before showing the results of this approach,
we first describe how we tune the parameters of the modified Basilisk framework.
6.3.1 Parameters
Our modified Basilisk framework has five parameters to tune. The first four are the thresholds resulting from the four frequency-based constraints involving minimum support and
maximum generality (see Modification 3 in Section 4.3.3). More specifically, the four
threshold parameters are (1) the minimum frequency of a word (M inW ), (2) the maximum frequency of a word (M axW ), (3) the minimum frequency of a pattern (M inP ), and
(4) the maximum number of words extracted by a pattern (M axP ). In addition, recall from
Section 4.3.4 that we have three types of patterns (namely, subject-verb/verb-object patterns, bigram patterns for extracting words, and bigram patterns for extracting phrases).
Our fifth parameter is the pattern parameter, which determines which subset of these
three types of patterns to use. Our goal is to tune these five parameters jointly on the
development set. In other words, we want to find the parameter combination that yields
the best F-measure when the Occurrence Heuristic is used to label the reports in the development set. However, to maintain computational tractability, we need to limit the number
of values that each parameter can take. Specifically, we limit ourselves to five different combinations of the four threshold parameters (see Table 5), and for each such combination,
we find which subset of the three types of patterns yields the best F-measure on the development set. Hence the total number of experiments we need to run is 35 (= 7 (the number
of (non-empty) subsets from the three types of patterns)  5 (the number of combinations
of the first four parameters)). Our experiment indicates that combination 3 in Table 5,
together with the bigram patterns for extracting phrases, yields the best F-measure on the
development set, and is therefore chosen to be the best parameter combination involving
these five parameters.
The new words and phrases acquired in the first two iterations of modified Basilisk
by using this parameter combination are shown in Appendix B. Here we see that no new
words are acquired in the first two iterations for eight of the 14 categories. The reasons
are that (1) unlike the original Basilisk framework, modified Basilisk employs a common
word pool, thus no longer requiring that five words must be added to each category in each
bootstrapping iteration; and (2) the application of minimum support to words has led to
the filtering of infrequently-extracted words. These two reasons together ensure that the
modified Basilisk framework focuses on learning high-precision words for each category.
6.3.2 Results
The semantic lexicon learned using the best parameter combination (based on the performance on the development set) is used to label the reports the test set. As we can see from
row 1 of Experiment 2 of Table 4, the Modified Basilisk approach achieves a precision of
53.15%, a recall of 47.57% and an F-measure of 50.21%. In comparison to the baseline,
this method has a lower precision and a higher recall. The increased recall shows that more
595

fiAbedin, Ng & Khan

Table 5: Combinations of the four threshold parameters for the modified Basilisk framework.
Combination
Combination
Combination
Combination
Combination
Combination

1
2
3
4
5

M inW
25
25
10
10
10

M axW
2500
2500
2500
2500
5000

M inP
250
100
250
250
250

M axP
100
100
100
250
100

reports are covered by the expanded lexicon. However, the learned lexicon also contains
some general words that have resulted in a drop in precision. Overall, it has a higher Fmeasure, which is statistically significantly better than that of the baseline according to
both significance tests. This vindicates our premise that learning more words and phrases
relevant to the shaping factors will help us identify the shaping factors of more reports.
6.3.3 Results Using Original Basilisk
To better understand whether our proposed linguistic and algorithmic modifications to
the Basilisk framework (see Section 4.3) are indeed beneficial to our cause identification
task, we repeated the experiment described above, except that we replaced the lexicon
generated using the modified Basilisk framework with one generated using the original
Basilisk framework. More specifically, we implemented the original Basilisk framework as
described by Thelen and Riloff (2002), but with one minor difference: in the case of the
bigram patterns extracting phrases, the word pools described in Section 4.2 were populated
with entire phrases instead of only head words. This was done because the seed words list
extracted in Section 2.3.2 contains both words and phrases and hence we would like to learn
entire phrases.
The only parameter to tune for the original Basilisk framework is the pattern parameter,
which, as mentioned above, determines which subset of the three types of patterns to use.
Therefore, we construct seven lexicons (corresponding to the seven non-empty subsets of
the three types of patterns) using the original Basilisk framework, and determine which
lexicon yields the best performance on the development set. Our experiment indicates that
the best development result was achieved when only the bigram patterns for extracting
phrases were used. Applying the corresponding semantic lexicon in combination with the
Occurrence Heuristic to classify reports in the test set, we observe a precision of 49.23%,
a recall of 42.78% and an F-measure of 45.78% (see row 2 of the Experiment 2 section
of Table 4). This lower precision and higher recall indicates that the lexicon has learned
words that are very general (i.e., words that appear in many of the reports and with little
discriminative power). The new words and phrases acquired in the first two iterations of
original Basilisk are shown in Appendix C. As can be seen, the original Basilisk framework
adds a lot of words, but many of them are not relevant to the shaping factors to which they
were added, and some are not semantically similar to the seed words for that shaping factor.
596

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Hence, although recall improves by a small amount, precision drops significantly, leading
to a precipitation in F-measure. These results suggest that our proposed modifications to
the original Basilisk framework are indeed beneficial as far as our cause identification task
is concerned.
6.4 Experiments with Classification Approach
Recall that in the classification approach to cause identification, we train an SVM classifier
for each shaping factor Sk to determine whether a report should be labeled as Sk . As desired,
this approach allows a report in the test set to potentially receive multiple labels, since the
resulting 14 SVM classifiers are applied independently to each report. To investigate the
effect of different feature sets on the performance of cause identification, we employ five
feature sets in our experiments: (1) unigrams only; (2) unigrams and bigrams; (3) lexicon
words only; (4) unigrams and lexicon words; and (5) unigrams, bigrams and lexicon words.
The unigrams and bigrams were generated from the reports in the training set by first
removing stop-words and ignoring case information, while the semantic lexicon was the
one constructed by our modified Basilisk framework. Before showing the results of our
supervised and transductive experiments, we first describe the parameters associated with
the classification approach.
6.4.1 Parameters
For each SVM classifier, we have two parameters to tune. The first parameter is the
percentage of features to use. Feature selection has been shown to improve performance
in text classification tasks (Yang & Pedersen, 1997). As a result, we employ information
gain (IG), one of the most effective methods for feature selection according to Yang and
Pedersens experimental results. Since we assume that the words from the semantic lexicon
are all relevant to cause identification, we do not apply feature selection to the lexicon words.
Rather, we apply feature selection only to the unigrams and bigrams. More specifically, if
only unigrams are used as features (as in the first of the five feature sets mentioned at the
beginning of this subsection), we select the N % unigrams with the highest IG, where the
value of N is tuned using the development set. When both unigrams and bigrams are used
as features (as in second and fifth feature sets), we combine the unigrams and bigrams into
one feature set and select the N % unigrams and bigrams with the highest IG, where the
value of N is again tuned using the development set. In our experiments, we tested 10
values for N : 10, 20, . . ., 100.
The second parameter associated with the SVM classifiers is the classification threshold.
By default, SVM sets the classification threshold to 0, meaning that every data point with
a classification value above 0 is classified as positive, and the rest will be classified as
negative. However, since an SVM classifier is trained to optimize classification accuracy,
the best classification threshold may not be 0 for our cause identification task, where the
goal is to optimize F-measure. As a result, we parameterize the classification threshold,
allowing it to take one of 21 values: 2.0, 1.8, . . . , 1.8, 2.0.
As usual, we tune the two parameters described above jointly rather than independently.
In other words, for each possible value combination of the percentages of features and
597

fiAbedin, Ng & Khan

classification threshold, we compute the F-measure of the classifiers on the development set
over all the classes and choose the value pair that yields the maximum F-measure.
To get a better idea of how these two parameters impact performance, we show in
Figure 3 how F-measure changes on the development set as we vary the values of the
two parameters, from the experiment where the underlying SVM classifiers employ only
unigrams as features. As we can see, the best F-measure was achieved by employing the
top 50% unigrams and a classification threshold of 0.8. Using the default parameter values
(no feature selection and a classification threshold of 0) yields a F-measure of approximately
18%. Overall, these results provide suggestive evidence that both parameters can have a
large impact on performance.
F-measure Vs. classification threshold
for different percentages of unigram features
100
Top 10% Unigrams
Top 20% Unigrams
Top 30% Unigrams
Top 40% Unigrams
Top 50% Unigrams
Top 60% Unigrams
Top 70% Unigrams
Top 80% Unigrams
Top 90% Unigrams
Top 100% Unigrams

90
80

F-measure (%)

70
60
50
40
30
20
10
0
-2

-1.5

-1

-0.5
0
0.5
Classification threshold

1

1.5

2

Figure 3: Variation of F-measure with different percentages of unigram features and classification thresholds used for SVM classification.

6.4.2 Supervised One-Versus-All Classifiers: Results and Discussions
Results of the supervised One-Versus-All classification approach using the five feature sets
described above are shown in the Experiment 3 section of Table 4.14 As we can see, when
feature sets 1 (unigrams only) and 4 (unigrams and lexicon words) are used, we achieve
the best results  F-measure scores of 47.46% and 47.45%, respectively. However, even
these best results are statistically indistinguishable from the baseline result (according to
approximate randomization test), and are significantly worse than the result produced by
14. Recall that in the supervised approach, the SVM classifiers were trained on only the 233 reports in the
training set.

598

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

the modified Basilisk approach (row 1 of Experiment 2) [see Appendix D, which contains
statistical significance test results that we obtained by applying stratified approximate randomization test to each pair of experiments in Table 4].
In fact, they also indicate that the Occurrence Heuristic has made more effective use
of the learned semantic lexicon than the SVM classifiers: the SVM classifiers trained with
only the lexicon words as features (row 3 of Experiment 3) produced a significantly worse
F-measure score (42.11%) than that of the Occurrence Heuristic (50.21%), due to large
drops in both recall and precision. Overall, these results suggest that the supervised approach performs worse than the heuristic-based semantic lexicon approach in this task. We
hypothesize that the limited amount of training data available to the SVM learner has contributed to the poor performance of the supervised approach. We will test this hypothesis
in Section 6.5
Two additional observations are worth mentioning. First, comparing rows 1 and 4 of
Experiment 3, we see that the lexicon words are not useful for cause identification in the
presence of unigrams. Second, comparing rows 1 and 2 and then rows 4 and 5 of Experiment
3, we see that using bigrams hurts performance. A likely reason can be attributed to
our feature selection method: since we choose the top N % features, the bigram features
significantly outnumber the unigram features, thus potentially diminishing the effect of
the latter. One solution to this problem is to employ separate parameters when selecting
unigrams and bigrams, but we decided against this choice, as it would lead to an explosion
in the size of the parameter space.
6.4.3 Transductive One-Versus-All Classifiers: Results and Discussions
To investigate whether it is useful to exploit unlabeled data, we employ transductive SVM
to combine labeled and unlabeled data. Essentially, we repeated the experiments in the
supervised One-Versus-All classification approach, except that we trained each transductive
SVM classifier using both the (labeled) reports in the training set and the (unlabeled)
reports in the test set as described in Section 5.2.2. The two parameters  the percentage of
features used and the classification threshold  are tuned jointly to maximize F-measure on
the development set, as described in the supervised approach, except that the transductive
SVMs used in the parameter tuning step are trained using the training set as labeled data
and the development set as unlabeled data.
Results of these transductive SVM classifiers are shown in the Experiment 4 section of
Table 4. Overall, the transductive results are significantly worse than the corresponding
results in Experiment 3. However, the conclusions that we can draw from the transductive
results are slightly different from those drawn from the supervised results. First, using
bigrams significantly improves performance when the lexicon words are absent (comparing
rows 1 and 2 of Experiment 3) but hurts performance when the lexicon words are present
(comparing rows 4 and 5). Second, adding lexicon words to the unigram-only feature
set (comparing rows 1 and 4) significantly improves performance, suggesting the potential
usefulness of the lexicon features. Nevertheless, Experiments 3 and 4 both indicate that (1)
using only lexicon words as features are far from adequate, and (2) the best performance is
achieved when lexicon words are added to unigrams as features.
599

fiAbedin, Ng & Khan

6.4.4 Results From Additional Supervised Approaches
Next, we present the results from the two additional supervised approaches, namely MetaLabeler and ensembles of pruned sets (Section 5.2.1). The feature sets used by both
approaches are the same as those used by the One-Versus-All method. As in the OneVersus-All method, both of these approaches use SVM as the underlying learning algorithm
for classifier training.
MetaLabeler. The only parameter that needs to be tuned for the MetaLabeler approach
is the percentage of features to use (N ), which was selected based on classification performance (F-measure) on the development set.
Results of the MetaLabeler approach are shown in the Experiment 5 section of Table 4. There are some interesting points about the these results. First, the MetaLabeler
method results in much better precision than the other methods. Second, this method
shows consistent performance improvement when bigram features are added, as can be seen
by comparing the first and second, and fourth and fifth rows of the MetaLabeler results.
Third, the inclusion of the lexicon word features are also found to improve performance,
as seen by comparing the first and fourth, and second and fifth, rows of the MetaLabeler
results. These two observations show that the MetaLabeler approach can properly take
advantage of the increasingly richer feature sets used in these experiments, with the best
performance occurring when all types of features are used (fifth row). Unfortunately, the
approach suffers from poor recall, a fact that prevents it from even matching, let alone
surpassing, the F-measure scores of the other methods. Since the method discards the less
probable labels when it assigns the labels to the documents, precision is much improved
but recall suffers.
Ensembles of Pruned Set. Among the parameters of the ensembles of pruned sets
approach, the number of classifiers in the ensemble, M , and the size of the sample of the
training data on which each classifier in the ensemble was trained, were chosen to be the
same ones used by Read et al. (2008), namely 10 and 63% respectively. The rest of the
parameters of the pruned set approach, namely the minimum cardinality (b), the minimum
support (p), the percentage of features to use (N ), and the threshold for label assignment (t)
were selected jointly based on classification performance (F-measure) on the development
set. The values from which the specific value of b was chosen was 2, 3 and 5. The possible
values of p tested in this experiment was 3, 5 and 10. The threshold parameter t was chosen
from the values 0.1, 0.2, . . . , 1.0, and the percentage of features, N was chosen from the
values 10%, 20%, . . . , 100%. Thus we had 900 parameter combinations for each feature set,
and from these parameter combinations, the combination for which the performance on the
development test set was best (in terms of F-measure) was chosen for running the system
on the test set.
Results of the pruned set approach are shown in the Experiment 6 section of Table 4.
Here, we see the best performance for the combination of unigram and lexicon word features,
better than the performance using the unigrams and lexicon words individually. However,
performance degraded with the inclusion of bigrams into this combination. Precision is
much lower than those of the other methods, which indicates that the selection of the label
sets from the training set of only 233 reports may not have been adequate.
600

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

6.5 Experiments Using Additional Training Data
The results of the above experiments are somewhat surprising: the best-performing supervised classification approach  the One-Versus-All approach  performs significantly worse
than the modified Basilisk approach. We hypothesize that its poor performance can be attributed to the scarcity of (labeled) training data. To test this hypothesis, we conducted a
set of experiments in which we increased the amount of training data for the One-Versus-All
supervised classification approach by applying cross-validation. More specifically, we take
the test set of 1000 reports and split it into five disjoint subsets of equal size, T1 , T2 , . . . , T5 .
Then, for each i we construct the training set by merging all Tj , where i 6= j, with the
original training set of 233 reports. After that, we train an SVM classifier on this merged
training set and test on the set Ti . When this is done over all five folds, we compute the
F-measure over the entire test set. In other words, the results we report for this set of
experiments are not F-measure scores averaged over the five folds. We again experimented
with the five set of features used in the supervised experiments in Section 6.4. The two
parameters, the percentage of features used and the classification threshold, are tuned in
exactly the same way as in the supervised experiments.
Results of this set of experiments are shown in the Experiment 7 section of Table 4. In
comparison to the results of Experiment 3, F-measure increases uniformly and significantly.
This provides empirical evidence that the performance of the supervised classifiers is limited
by the amount of data on which they were trained. With feature sets 4 (unigrams and
lexicon words) and 5 (unigrams, bigrams and lexicon words), we achieve the best results
 F-measure scores of 53.56% and 53.66% respectively  the difference between which
is statistically insignificant. These two results are in turn significantly better than that of
modified Basilisk (row 1 of Experiment 2), according to the approximate randomization
test. In addition, except for feature set 3 (lexicon words only), results obtained in this
experiment are significantly better than that of the baseline, again according to approximate
randomization test. Overall, these results suggest the difficulty of the cause identification
task: by comparing rows 4 of Experiments 3 and 5, we see that F-measure increases by only
about 6% as the number of training reports is increased from 233 to 1033.
A few more points deserve mentioning. As in previous learning-based experiments, using only lexicon words as features yields the worst result in this set of experiments, and
combining unigrams and lexicon words still yields one of the best results. Nevertheless,
in comparison to Experiment 3, while using bigrams still does not improve performance,
it does not hurt performance (from a statistical significance point of view). Perhaps more
importantly, comparing rows 1 and 4 of Experiment 7, we see that augmenting unigrams
with lexicon words yields significantly better performance. This indicates that the lexicon
words are indeed useful features for cause identification, but their usefulness may not be
revealed when a small labeled training set is used, as seen in Experiment 3. Learning algorithms attempt to learn which features are important or relevant for the given classification
task based on the training examples they see, and the more there are training examples,
the better they are able to learn the relevance of the features. Our results show a very
poignant illustration of this phenomenon: the SVM learner is able to use the lexicon word
features effectively only when given a large number of training instances. This can be seen
more clearly from the SVM learning curves in Section 6.7.3. This indicates that lexicon
601

fiAbedin, Ng & Khan

words are useful as features only when we have sufficiently large training data. However,
lexicon words may still be used effectively in ways other than as linguistic features even if
the training set is small, as we can see from the results of Experiment 2, which uses the
lexicon words in combination with the Occurrence Heuristic to achieve performances that
are statistically significantly better than the baseline.
6.6 Error Analysis and Lessons Learned
In order to gain a clearer insight into our cause identification problem and help direct
future research, we manually analyzed the errors made by the best-performing system (i.e.,
the heuristic based approach using the semantic lexicon learned by our modified Basilisk
framework) on a randomly chosen 100-report subset of the test set. More specifically, we
looked at the false negatives (cases in which the annotator labeled a report with a shaping
factor but the system did not) and false positives (cases in which the system labeled a
report with a shaping factor but the annotator did not). For each false negative, we tried
to determine why the system failed to correctly label the report, and for each false positive,
we tried to determine why the system labeled the report erroneously. Table 6 shows the
number of false positives and false negatives along with the reasons for these errors that we
discovered in our analysis. The following sections discuss the errors and their reasons in
more detail. Note that since a shaping factor may be indicated by more than one keyword
in a single report, there can be more than one reason for a false negative (positive) error.
Thus the sum of the frequencies of different types of false negative (positive) errors is greater
than the total number of false negatives (positives).
Table 6: Error analysis details: different reasons for the false positive and false negative
errors.
False negatives
Sentence fragments bigger than phrases
Implicit causes that cannot be identified by keywords
Phrases that were not learned
False positives
Keyword was too general
Keyword indicates concept that appears in the report but
does not contribute to the incident
Wrongly learned keyword
Keyword was used in a negative context
Keyword was used in a hypothetical context

58
24
23
14
83
50
32

Percentage
41.38%
39.66%
24.14%

6
3
1

7.23%
3.61%
1.20%

60.24%
38.55%

False negatives. For each false negative error, we read the report narrative to identify
some word, phrase or sentence fragment that may indicate the shaping factor that our
system missed. From this analysis, we identified three reasons for the false negatives as
follows:
602

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

1. Required sentence fragments larger than phrase. We identified 24 sentence
fragments that are bigger than phrases (i.e., those that consist of two or more phrases).
For example, the sentence fragment having never been to DCA before consists of 4
phrases: having never been, to, DCA and before. Together, they convey the meaning
that the reporter was unfamiliar with DCA, but it is not possible to identify a single
word or phrase that conveys the same meaning. Since our framework learns only
phrases, it was not possible to learn these sentence fragments.
2. Cause not identifiable by specific words or phrases. In 21 instances, no specific
word, phrase or sentence fragment could be identified that could pinpoint the shaping
factors responsible for the incident. For example, a number of reports, including
report#566757, describe incidents in which there is a miscommunication between the
pilot and the air traffic controller, but that miscommunication must be understood by
following their conversation. A human reading the report can easily understand that
the pilot is claiming the controller said one thing and the controller is claiming he
said something different, but to detect that kind of a scenario, a machine would need
to generate a complete model of the discourse that identifies the specific topic of the
conversation, the participants, the claims each participant makes about the topic, the
fact that the claims are contradictory, and also the fact that the contradiction arises
from miscommunication between them. The preprocessed narrative of this report is
shown in Appendix E.
3. Missing phrases. In 14 cases the necessary phrase was missing from the semantic
lexicon learned by our modified Basilisk framework. Out of these 14 phrases, six
phrases were too infrequent to be considered by our modified Basilisk framework due
to the minimum frequency criterion. For example, the phrase temperature flux
appears only once in the entire corpus and hence was not considered by our system.
Two phrases were verb phrases, which could not have been learned as we focused
only on learning noun phrases and adjective phrases. There are four phrases that
are not semantically similar to any seed word for their shaping factors. For example,
the phrase garbled transmission is not semantically similar to any seed word for
the shaping factor Communication Environment, such as disturbance, static, radio
discipline, congestion and noise. Finally, there are two phrases that should have been
learned by the system, but were not learned because at the time they were put into
the word pool, other words with higher scores were selected instead.
False positives. In the case of false positives, we looked into the report narrative and
the keyword that was found in the content to determine why the indication of the shaping
factor for the incident described in the report was incorrect. The different reasons that we
identified are as follows.
1. Too general keywords. We have observed a large number of false positives due to
keywords being too general (i.e., keywords that have been extracted or learned for a
shaping factor but may appear in other phrases that are not related to that shaping
factor). For example, the keyword failure is a correct indicator of Resource Deficiency
as it appears in text like complete electrical failure, alternator failure, etc., but
when it appears in text like failure to follow Air Traffic Control instructions, it does
603

fiAbedin, Ng & Khan

not indicate Resource Deficiency as a shaping factor. We have identified 50 cases that
were caused by keywords being too general.
2. Concept present but not contributing to incident. Another frequently faced
problem is that sometimes the concepts identified by the keywords are present in a
report, but they do not act as a shaper for the incident described in the report. For
example, in report#324831, the reporter mentions that he was flying solo, which is
an indication of Taskload, but the incident was due to Physical Environments, namely
snow and foggy weather. The fact that he was flying solo is merely mentioned as a
part of his description of the overall situation. The preprocessed version of this report
is also given in Appendix E. In total, we observed 32 such cases.
3. Incorrectly learned words and phrases. There were six cases in which the semantic lexicon learner learned incorrect words and phrases that were not related to
the shaping factors to which they were assigned. For example, the framework incorrectly learned the word further for the shaping factor Resource Deficiency, and thus
a number of reports were mislabeled with Resource Deficiency.
4. Negative context. There were three cases in which the keyword appeared in a
negative context, which is typically signaled by a contextual valence shifter such as
no and hardly (Polanyi & Zaenen, 2006). For example, the keyword aircraft damage, an indicator of Resource Deficiency, appears in report#569901 as no apparent
aircraft damage, which results in a false positive.
5. Hypothetical context. There was one case in which the keyword appeared in a
hypothetical context in which the reporter conjectures about a possible scenario. The
keyword single pilot, an indicator of Taskload, appeared in report#534432 as this
could happen to a pilot especially if he was single pilot, resulting in a false positive.
Lessons learned. Our error analysis provides valuable insight into the nature of the
problem as well as hints on how one should proceed in order to improve the performance
of the system. By analyzing the most frequent errors, we present the following lessons
learned from the analysis. First of all, it is more useful to learn high-precision keywords
and phrases than general ones as the largest part of the false positive errors can be attributed
to having too general keywords. However, such high-precision keywords and phrases are
more likely to have low frequencies, and hence one would have to adapt learning methods
to learn useful words and phrases from infrequent ones. Second, one must take into account
the fact that relevant portions of the text may be larger than phrases, even going up to
clause or sentences. These cannot be identified by learning words or phrases, or N-grams
of reasonable size. Thus, more robust methods are needed that can learn useful sentence
fragments or useful sentence structures. Finally, there are cases in which one cannot hope to
identify using methods that look for keywords, phrases, sentence fragments or even sentence
structures, i.e., cases in which the cause of the incident has to be understood from the
discourse, and cases in which a concept is present in the description and yet plays no part
in the incident. Much deeper analysis than simple bag-of-anything models are needed for
avoiding these two types of errors, which between themselves represent almost one third
of all errors in the analyzed subset. The former needs a method to distinguish relevant
604

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

sentences from irrelevant ones. For example, Patwardhan and Riloff (2007) discuss a relevant
sentence classifier that is trained on a small set of seed patterns and a set of documents
marked as relevant and irrelevant that can be useful in this context. The latter problem
requires a discourse analysis method that, as discussed earlier, can model the conversations
and identify relations correctly. This shows that though it is possible to identify shaping
factors from these reports using words and phrases to a certain extent, much deeper natural
language techniques are needed to accurately identify the full range of causes.
6.7 Additional Analyses
In this section we present the outcomes of a number of additional analyses that we performed
on our cause identification task and our approaches to this task. In Section 6.7.1 we study
the relative difficulties of classifying the different shaping factors. In Sections 6.7.2 and 6.7.3
we show the learning curves of the semantic lexicon based approach and the learning based
approach respectively, i.e., how the performances of these two approaches vary as they are
provided with different amounts of training data. Finally in Section 6.7.4 we discuss the
outcomes of the experiment conducted to determine if our modifications to the Basilisk
framework is useful for learning general semantic categories.
6.7.1 Per-Class Results
To get an insight into which of the classes are difficult to classify, we perform an analysis
of per-class performance of two labeling schemes: the best heuristic-based method (i.e., the
Occurrence Heuristic using the lexicon learned by the modified Basilisk framework) [see the
first part of Table 7] and the best learning-based method (i.e., the 5-fold SVM classifiers
using unigrams, bigrams and the lexicon words as features) [see the second part of Table 7].
In conjunction with Table 1, two classes stand out most prominently as difficult to classify
 Illusion and Taskload. Both of these classes have very little representation in the training,
test and development sets, have a very small number of seed words, and result in very poor
performance by each of the approaches. The more easily identifiable classes were Physical
Environment, Physical Factors, Resource Deficiency and Preoccupation, in which both the
labeling schemes had F-measures better than 40%. In general these classes had better
representation in the training, testing and development sets, and also had a reasonable
number of words and phrases in the semantic lexicon. We believe that this difference in
characteristics of the classes is a valuable insight that will be helpful in future work.
6.7.2 Lexicon Learning Curve
As mentioned in Section 2.3.2, we have used a total of 177 seed words and phrases. At
a first glance, this number of seeds may seem large as far as bootstrapping experiments
are concerned. However, considering the fact that these 177 seeds are distributed over 14
shaping factors, we only have an average of 12.6 words and phrases per shaping factor.
Nevertheless, it would still be interesting to examine how cause identification performance
will be affected if we reduce the number of seeds for each shaping factor used by Modified
Basilisk in the bootstrapping process. As a result, we ran a set of experiments to measure
cause identification performance that uses the semantic lexicon learned by Modified Basilisk
when it is given different number of seed words, where the parameters specific to Modified
605

fiAbedin, Ng & Khan

Table 7: Per-class performance results. The upper table shows the per-class performance of
the Occurrence Heuristic using the lexicon learned by the modified Basilisk framework.
The lower table shows the per-class performance of 5-fold SVM classifiers using unigrams,
bigrams and lexicon words as features.

Shaping Factor
Attitude
Communication Environment
Duty Cycle
Familiarity
Illusion
Other
Physical Environment
Physical Factors
Preoccupation
Pressure
Proficiency
Resource Deficiency
Taskload
Unexpected
Overall

TP
3
9
3
31
0
25
195
22
78
14
40
360
0
4
784

FN
27
81
23
19
2
192
70
13
32
16
207
147
29
6
864

TN
957
888
973
872
996
766
638
958
822
902
723
225
965
976
11661

FP
13
22
1
78
2
17
97
7
68
68
30
268
6
14
691

Precision
18.75%
29.03%
75.00%
28.44%
0.00%
59.52%
66.78%
75.86%
53.42%
17.07%
57.14%
57.32%
0.00%
22.22%
53.15%

Recall
10.00%
10.00%
11.54%
62.00%
0.00%
11.52%
73.58%
62.86%
70.91%
46.67%
16.19%
71.01%
0.00%
40.00%
47.57%

F-measure
13.04%
14.88%
20.00%
38.99%
0.00%
19.31%
70.02%
68.75%
60.94%
25.00%
25.24%
63.44%
0.00%
28.57%
50.21%

Shaping Factor
Attitude
Communication Environment
Duty Cycle
Familiarity
Illusion
Other
Physical Environment
Physical Factors
Preoccupation
Pressure
Proficiency
Resource Deficiency
Taskload
Unexpected
Overall

TP
2
20
10
18
0
52
182
20
55
6
102
399
0
0
866

FN
28
70
16
32
2
165
83
15
55
24
145
108
29
10
782

TN
964
871
962
924
998
685
623
955
848
961
639
247
971
990
11638

FP
6
39
12
26
0
98
112
10
42
9
114
246
0
0
714

Precision
25.00%
33.90%
45.45%
40.91%
0.00%
34.67%
61.90%
66.67%
56.70%
40.00%
47.22%
61.86%
0.00%
0.00%
54.81%

Recall
6.67%
22.22%
38.46%
36.00%
0.00%
23.96%
68.68%
57.14%
50.00%
20.00%
41.30%
78.70%
0.00%
0.00%
52.55%

F-measure
10.53%
26.85%
41.67%
38.30%
0.00%
28.34%
65.12%
61.54%
53.14%
26.67%
44.06%
69.27%
0.00%
0.00%
53.66%

606

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Basilisk are set as described in Section 6.3.1. More specifically, we chose the top 3, 4, 5, 6,
7, 10, 15 and 20 seed words and phrases for each shaping factor (in terms of the frequency
in the entire corpus), and ran the modified Basilisk framework for ten iterations using the
aforementioned parameters.
Note, however, that not all shaping factors have the same number of manually selected
seed words and phrases. For example, Illusion, Taskload and Unexpected have 1, 2 and 3
seed words and phrases respectively, whereas Resource Deficiency and Physical Environment
have 47 and 45 respectively (see the last column of Table 1). Hence, for those experiments
where the number of seeds used for each shaping factor exceeds the number of manually
selected seeds for a shaper, all the manually selected seeds were used. For example, since
Unexpected has only three manually selected seeds, all of them were used in the experiments
in which at least three seeds are used for each shaping factor.
The Occurrence Heuristic was then used with the lexicons thus generated to evaluate
their performance on the test set. The resulting learning curve, in terms of F-measure
on the test set of 1000 reports, is shown in Figure 4. In addition, since the baseline to
which we compare the performance is based on the seed words, the baseline learning curve
corresponding to each reduced seed words set is also shown. As expected, increasing the
number of seed words monotonically improves the F-measure. However, the improvement
over the baseline is particularly small when fewer than seven seed words are used, and the
highest improvement is observed for seven seed words and phrases. From then on, adding
more seeds improves the overall performance, but the improvement over the baseline slowly
diminishes.
Lexicon Learning Curve
50
48
46

F-measure (%)

44
42
40
38
36
Baseline
Performance of learned lexicon

34
32
30
0

2

4

6

8
10
12
Number of Seeds

14

16

18

20

Figure 4: Variation of F-measure with different number of seeds words per category.

607

fiAbedin, Ng & Khan

SVM Learning Curve
60
F-measure on Test Set
58
56

F-measure (%)

54
52
50
48
46
44
42
40
0

100

200

300 400 500 600 700
Number of Training Instances

800

900 1000

Figure 5: Variation of F-measure with different number of training reports.

6.7.3 SVM Learning Curve
As discussed in Section 6.5, we hypothesize that the failure of the SVM classifiers to perform better than the baseline is due to the scarcity of the training instances available to
the learner. One may argue that SVM has been shown to work well for small datasets.
So, a natural question is: how much smaller will the training set be before we can see a
statistically significant drop in cause identification performance? To answer this question,
we plot a learning curve for the One-Versus-All classification approach, using as features a
combination of unigrams, bigrams, and lexicon word features in a five-fold cross validation
setting, which is the setting that yields the best performance in Table 4. Specifically, we
generated random subsets of the training sets of sizes 50, 100, . . . , 1000 instances. Parameters, namely the percentage of features and the classification threshold, were chosen in the
same way as the original experiment as described in Section 6.5, and the F-measure was
evaluated on the entire test set. The curve is shown in Figure 5, where each data point
is computed by averaging the results over five independent runs. As we can see, there is
a general trend of performance improvement with the increase in the number of training
instances. In addition, when trained on only 50% of the training set, we see that the cause
identification system started to perform statistically significantly worse than the system
that was trained on all of the available instances according to the stratified approximate
randomization test.
608

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

6.7.4 General Usefulness of Our Modifications to Basilisk
In order to test whether the modifications we made to the Basilisk framework are useful
to lexicon learning in general, we added two general categories to the shaping factors in
the bootstrapping experiments, namely People and Equipment. These two categories were
added because, firstly, words and phrases added to these categories would be easy to verify
(i.e., whether they are words or phrases representing people or equipment), and secondly,
they are similar to the original context in which the Basilisk framework was originally evaluated (i.e., learning words in the categories BUILDING, EVENT, HUMAN, LOCATION,
TIME, and WEAPON from terrorism reports). These two additional categories were added
to the seed lexicon described in Section 2.3.2, which was then bootstrapped by running Original Basilisk and Modified Basilisk separately for ten iterations, with the parameters specific
to these Basilisk frameworks set in the same way as described in Section 6.3.1. The seed
words for these two categories were selected in the same manner as done by Thelen and
Riloff (2002), i.e., the phrases in the corpus were sorted by frequency and the five most
frequent phrases belonging to these categories were manually identified. Below are the seed
words used in the two categories:
 People: Captain, controller, First Officer, RPTR, passenger
 Equipment: aircraft, airplane, Collision Avoidance System II, engine, Auto-Pilot
In order to verify which of the words and phrases learned by the two frameworks correctly
belong to the assigned category, the first author and a computer science graduate student
not affiliated with this research went over the generated lexicons. Appendices F and G
show the lexicons generated by Original Basilisk and Modified Basilisk respectively. To
facilitate analysis, we divide the words and phrases in each generated lexicon into three
categories: (1) those that are determined as correct by both human judges; (2) those that
are determined as correct by only one judge; and (3) those that are determined as incorrect
by both judges.
For the lexicon generated by Original Basilisk, we find that in the category People, 29 of
the 50 words and phrases were determined as correct by both judges, and 6 were determined
by exactly one of the judges as correct; in the category Equipment, 6 of the 50 words and
phrases were correct according to both judges, and 22 were correct according to exactly one
of the judges. On the other hand, for the lexicon generated by Modified Basilisk, we find
that in the category People, 44 of the 50 words and phrases were determined as correct by
both judges, and 3 were determined by exactly one of the judges as correct; in the category
Equipment, 34 of the 50 words and phrases were correct according to both judges, and 9
were correct according to exactly one of the judges. This comparison clearly shows that
the modifications that we made to the Basilisk framework are not specific to this particular
task; rather, these modifications have improved the lexicon building performance in general.
6.8 Summary of Conclusions
We end this section by providing a summary of the major conclusions we draw from the
experiments.
609

fiAbedin, Ng & Khan

 Our heuristic approach to cause identification, which labels a report using the Occurrence Heuristic in combination with the words and phrases automatically acquired
using our Modified Basilisk framework, surpasses the performance of the baseline system, which applies the Occurrence Heuristic in combination with the seed words and
phrases manually identified from the training documents. The difference in F-measure
between these two systems is statistically significant according to both McNemars test
and the stratified approximate randomization test. This suggests that the words and
phrases in the semantic lexicon learned via Modified Basilisk are relevant and effective
for cause identification.
 Adding the learned lexicon words to an N-gram-based feature set for training SVM
classifiers is beneficial for cause identification only if the training set is sufficiently
large, as exhibited by the statistically significant increase in F-measure. This provides
suggestive evidence that the words and phrases in the semantic lexicon learned via
Modified Basilisk are relevant and useful features for cause identification.
 When used in combination with the Occurrence Heuristic, the semantic lexicon learned
by our Modified Basilisk framework offers significantly better performance for the
cause identification task than the one obtained using the original Basilisk framework. Additional experiments reveal that Modified Basilisk is not only useful for
cause identification, but it also offers performance superior to Original Basilisk when
bootstrapping general semantic categories such as People and Equipment.
 Among the three multi-class multi-labeled text classification approaches we experimented with, One-Versus-All works significantly better than MetaLabeler and Pruned
Sets for cause identification. Transductive learning, when used in combination with
the One-Versus-All approach, significantly hurts performance, suggesting that unlabeled data cannot be profitably exploited given the fairly small amount of labeled
data.
 Our best system achieves an F-measure of around 53.7%, which indicates that cause
identification is a difficult task, and that there is a lot of room for improvement. To
provide directions for future research, we performed an analysis of the errors made
by the best-performing system. In particular, we found that performance is currently
limited in part by several factors. First, there are a number of cases in which the
relevant text indicating the responsible shaping factor may be larger than phrases.
Second, indicators for a shaping factor may be mentioned in a report without influencing the incident described in the report. Finally, there are some cases in which the
shaping factors cannot be identified by simply looking at the words, phrases or even
sentence fragments  much deeper analysis is required in these cases.
 Increasing the number of seed words and phrases employed by Modified Basilisk improves cause identification performance, but the marginal performance improvement
for each added seed diminishes with successive additions. In other words, these results
seem to suggest that using more seed words will be unlikely to improve much over the
current performance; rather it would be more promising to start with a small number
of high frequency seeds and improve upon the bootstrapping process.
610

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

 The learning curve plotted for the One-Versus-All classification approach shows that
cause identification performance increases with the number of training instances. In
particular, when trained on only 50% of the training set, we see that the resulting cause
identification system performs statistically significantly worse than the one trained on
all of the available instances.
Overall, while our approaches rely on automatically learned lexicon words and phrases
that are not adequate for cause identification, they are relevant for the task. As mentioned previously, their use is motivated by the labor-intensive procedure that the NASA
researchers employed in manually identifying seed words and phrases for each shaping factor (Posse et al., 2005). Our work represents one of the first attempts to tackle this cause
identification task, and we believe that the use of simple features is a good starting point
and establishes a baseline against which future studies on this problem can be compared.
The main take home message from this research is that though it is possible to solve
the problem that we set out to solve, namely automated cause identification, by learning
relevant keywords or sentence fragments and other suitable bag-of-words models, there
remains a significant portion of the data that remains unlabeled or mislabeled through
these methods. To match the performance level achieved in other topical text classification
tasks, much deeper linguistic analysis like relevant sentence detection and discourse analysis
methods like identifying disagreements, disputes and hostile attitudes will be needed. This
lesson should be the cornerstone of further research in this area.

7. Related Work
In this section, we describe some other works related to our research. In particular, our
discussion focuses on causal analysis as well as approaches to semantic lexicon construction and text classification, and is organized as follows. First, we discuss causal analysis
as it has appeared in different fields. Second, we discuss the different semantic lexicon
learning algorithms. Third, we discuss works that deal with extraction pattern learning.
Fourth, we describe different algorithms for unsupervised word clustering and thesaurus
building. Finally, we include a discussion of related work on multi-class multi-labeled text
classification.
Causal analysis. Major research on causality has been performed mainly in the fields
of philosophy and psychology. In the field of philosophy, seminal works in causality have
been conducted by Hume (1739, 1748), who provides one of the most influential definitions
of cause as an object followed by another, and where all the objects, similar to the first,
are followed by objects similar to the second. Or, in other words, where, if the first object
had not been, the second never existed. This has been the basis of later, much stronger
definitions of causation (e.g., Lewis, 1973; Ganeri, Noordhof, & Ramachandran, 1996).
Notable investigations on causation in the field of psychology include those by Cheng (1997),
who defines causation in terms of the probabilistic contrast model; Griffiths and Tenenbaum
(2005), who discuss learning about cause and effect relationships using causal graphical
models; and Halpern and Pearl (2005), who provide explanations of causality by means
of structural equations governing random variables representing events. Although these
611

fiAbedin, Ng & Khan

works provide important background and definitions contributing to the understanding of
causality, in order to identify causes from naturally written text we must turn to NLP.
In the field of NLP, there is little work on cause identification similar to our problem.
Research on causality focuses mainly on identifying causal relations between two sentence
components. For instance, Girju (2003) describes a method for automatically discovering
lexico-semantic patterns that refer to causation. In particular, she focuses on the explicit
intra-sentential pattern hN P1 verb N P2 i, where verb is a simple causative. She also shows
how these patterns can be used to improve the performance of a system for answering
cause-effect type questions. Khoo et al. (2000) use graphical pattern matching to identify
causal relations from medical article abstracts. They use hand-crafted patterns that are
matched against the parse trees of the sentences. The subtrees of the parse tree that match
the patterns are extracted as causes or effects. Similarly, Garcia (1997) uses hand-crafted
extraction patterns to identify causal relations from sentences in the French language. The
limitation of these approaches is that they focus on identifying causal relations from the
same sentence, whereas our reports are multi-sentence discourses.
Grishman and Ksiezyk (1990) use domain modeling, discourse analysis and causal inference to find cause-effect relations between events leading up to equipment malfunctions
from short equipment failure reports. More specifically, they first apply syntactic analysis
to produce parse trees for the sentences in the reports using an augmented context-free
grammar. Then they apply semantic analysis to map (1) verbs and syntactic relations into
domain-specific predicates and relations and (2) noun phrases into references to components
in the domain model. Finally, they apply discourse analysis to these predicates to construct
a time-graph showing the temporal and causal relationships between the elementary facts.
The temporal relations are derived from text structures and words (e.g., when, then,
etc.) and the order of appearance in text, but the causal relations are determined by querying a simulation model of the equipment that is built using domain knowledge. Specifically,
each possible causal link is posed as a query to the model to test if the relation holds.
Overall, their method relies heavily on the domain model of the equipment being studied,
and their research focuses on only one specific piece of equipment.
NASAs own research on identifying causes of incidents from the report narratives have
been performed by Posse et al. (2005), who describe a specific experiment in which they
brought together experts to manually analyze the report narratives and identify words,
phrases and expressions related to each of the shaping factors, as mentioned earlier. Later
work by Ferryman et al. (2006) take these manually extracted expressions as ground truth
and compare the anomalies described in the reports to the shaping factors derived from
applying these expressions to the same reports. Specifically, they do not attempt to learn
these expressions automatically; rather, they focus on finding possible correlations between
the shaping factors and the anomalies.
Algorithms for semantic lexicon learning. A number of semantic lexicon learning
algorithms follow an iterative bootstrapping approach, starting from a small number of
semantically labeled seed words. Roark and Charniak (1998) propose a method of constructing semantic lexicons based on co-occurrence statistics of nouns in conjunctions, lists
and appositives. They start with a small seed nouns list, and iteratively add similar words
to that list. The word similarity is measured by the ratio of how many times the word occur
612

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

together with a seed word to the number of times the word appear in the corpus. After
construction, they rank the words by a log-likelihood statistic (Dunning, 1993). However,
due to the general brevity of the reports, such co-occurrences and lists are rather few in our
corpus, and it is more useful for us to use context-based similarities like Thelen and Riloff
(2002). They describe their Basilisk framework for learning semantic lexicon using extraction patterns as features. The apply a weakly supervised bootstrapping approach in which
they start from a small manually constructed seed lexicon and iteratively add semantically
similar words to it. This, described in more detail in Section 4.2, has been the basis for our
lexicon learning approach.
A number of improvements to the Basilisk framework, and more generally to bootstrapping approaches, have been proposed. In the Basilisk framework, the number of iterations
is a parameter that has to be chosen arbitrarily. Rather than making an arbitrary choice,
Yangarber (2003) proposes a method for detecting termination of unsupervised semantic
pattern learning processes. The method requires that the documents must be labeled as
relevant or irrelevant. Since such information is not available for our corpus, it is not useful for us. Curran, Murphy, and Scholz (2007) suggest an improvement over traditional
bootstrapping methods by discarding words and contexts that appear to be related to more
than one category, in order to minimize semantic drift and enforce mutual exclusion. On
the other hand, we handle such cases by comparing the conditional probabilities for the
different categories to which such words can belong. Zhang, Zhou, Huang, and Wu (2008)
present bootstrapping with the graph mutual reinforcement-based bootstrapping (GMR)
(Hassan, Hassan, & Emam, 2006), a modification of the Basilisk method. Similar to us,
they explore using N-grams to capture context, but they use a different set of pattern and
word scoring formulas. For learning multiple categories simultaneously, they introduce a
scoring system based on entropy of a pattern. They report better results than Basilisk on
the MUC-4 dataset (see Sundheim, 1992).
Among non-bootstrapping approaches, Ando (2004) presents a new method of constructing semantic lexicons from an unannotated corpus using a set of semantic classes and a set
of seed words and phrases for each semantic class. She uses spectral analysis to improve
the feature vectors by projecting the useful portions of the vectors into a subspace and removing the harmful portions of the vectors. The resultant feature vectors are then used
by a centroid-based classifier using cosine similarity measure to label the words. Avancini,
Lavelli, Sebastiani, and Zanoli (2006) take a classification approach to semantic lexicon
construction. They cast the problem as a term (meaning both words and phrases) categorization task (dual of the document categorization task), and similar to the bag-of-word
model, they represent the terms as bag-of-documents. They use a variation of the adaptive
boosting algorithm, AdaBoost.M H KR , which is trained on a small seed lexicon and then
used to classify the noun terms in the corpus to zero, one or more semantic categories.
Algorithms for learning extraction patterns. Our approach to semantic lexicon construction uses extraction patterns as features, and here we present some methods that aim
to improve the extraction pattern collection process. Riloff (1996) describes the AutoSlog-TS
system that learns extraction patterns from untagged text. However, it needs a pre-classified
corpus that have the text classified as relevant or irrelevant; as we mentioned earlier, we do
not have access to such information. Phillips and Riloff (2007) present a method of boot613

fiAbedin, Ng & Khan

strapping algorithm to learn role-identifying nouns, which are then used to learn important
extraction patterns, and also role-identifying expressions. However, their focus is mainly on
identifying the roles of words in events.
Patwardhan and Riloff (2007) provide another extraction pattern learning approach
using relevant regions. They require the documents to be pre-classified into relevant and
irrelevant documents. Using a small set of seed patterns, they classify the sentences in these
documents into relevant and irrelevant sentences. Then semantically appropriate extraction patterns are learned using a semantic affinity metric and separated into primary and
secondary patterns. This approach is also not directly usable to us due to the unavailability
of documents pre-classified into relevant and irrelevant categories.
Recently, the Internet has increasingly been used in natural language research. Patwardhan and Riloff (2006) use the AutoSlog-TS system (Riloff, 1996) to learn domain specific
extraction patterns by processing documents retrieved by querying the web with selected
domain-specific words. Using the web is an interesting and promising enhancement and, as
mentioned in Section 8, we intend to extend our work using the Google corpus (Brants &
Franz, 2006).
Algorithms for thesaurus building and unsupervised word clustering. Another
area of research that is very closely related to semantic lexicon learning is thesaurus building.
Building a thesaurus requires discovering groups of semantically similar words, though it
stops short of assigning semantic class labels to the words. Thus it shares the problem of
measuring semantic similarity and grouping similar words with the semantic lexicon building
task. Here we discuss several approaches to the thesaurus building task.
Clustering has been used extensively in thesaurus building, mostly because of its unsupervised nature and ability to handle large volumes of data. The seminal work in this
direction has been by Pereira, Tishby, and Lee (1993), who present an unsupervised method
for soft clustering of words using distributions of the words in different contexts. This approach generates overlapping word clusters, grouping words based on the contexts that they
appear in. Baker and McCallum (1998) use Pereira et al.s distributional clustering technique to perform feature space reduction for supervised classification with nave Bayes by
using the clusters as features. Lin and Pantel (2001) present their approach of generating a
collection of sets of semantically similar words, or concepts, using their clustering method,
UNICON, with dependency relations as features. Pantel and Lin (2002) present another
clustering approach, clustering by committee, using contextual features with point wise
mutual information as feature values, that they compare as better than Lin and Pantels
results. Rohwer and Freitag (2004) present a clustering-based automatic thesaurus building
process from an unannotated corpus. They propose an information theoretic co-clustering
algorithm that groups together highly frequent words into clusters of similar part-of-speech
category. Then they pursue an additional process, lexicon optimization, to grow the lexicon
by assigning the less frequent words to their most likely clusters.
Among non-cluster-based methods, Davidov and Rappoport (2006) present an unsupervised method of discovering groups of words that have similar meanings. They achieve
this by (1) identifying high frequency words and content words, (2) identifying symmetric
lexical relationship patterns, and (3) applying a graph clique-set algorithm to generate word
categories from co-occurrence information of the content words in the symmetric patterns.
614

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Concentrating on the performance issues that plague attempts to build thesaurus from a
large corpus, Rychly and Kilgarriff (2007) present two methods of improving performance
of general context-based thesaurus building algorithms. The first method is to compare
only those word pairs that have some context in common. The second method is to use the
heuristic of removing those contexts that are too general (i.e., contexts that have more than
a certain number of distinct words). In our research, we have adopted the second method
(see Section 4.3). They also applied a partitioned sequential approach to the construction
process. Though thesaurus building does not usually require an annotated corpus or a set
of seed words and phrases, it is not directly applicable to the task of growing a semantic
lexicon where we have to learn words in specific semantic categories. This is because the
method has no control over which words are being learned and which classes the discovered
word groups belong to. It may be possible to adapt this method to that of semantic lexicon
growing by classifying the word groups into the semantic classes by using the seed words and
phrases. However, the method has to be extended to extract noun and adjective phrases.
Algorithms for multi-class multi-labeled text classification. As mentioned previously, cause identification, when cast as a text classification problem, is a multi-class
multi-labeled text classification problem, since there are 14 shaping factors in total and
each document may be labeled with more than one shaping factors. There are several
popular approaches to solving a multi-class multi-labeled text classification problem. The
first, and one of the approaches followed in this research, is to independently train a binary
classifier for each class, and apply each classifier on a test instance in isolation. In our
case, the underlying learner is Support Vector Machines (Joachims, 1998). Godbole and
Sarawagi (2004) suggest a number of improvements to this scheme, namely, including class
labels suggested by a preliminary set of classifiers as features, removing negative examples
too close to the classification hyperplane, and selectively removing some classes from the
one-versus-others classifications scheme. Another notable method, followed by Tsoumakas
and Vlahavas (2007) and Read et al. (2008), is to treat each unique set of labels as a
new label, thus converting the problem to a multi-class single-labeled one. Their works
differ from each other in the construction of the new labels. The former, called RAndom
k-LabELsets, or RAKEL, builds an ensemble of classifiers by randomly sampling label sets
of size k; whereas the latter adopts the method of filtering observed label sets by minimum
support. Tang et al. (2009), on the other hand, take a different approach: they train one
classifier that predicts the number of labels that a test instance would have, and then choose
that many labels for that instance based on output of another classifier that ranks the labels
by likelihood for that instance. All these works use SVM as their underlying learner. In
addition, all these approaches make the assumption that the classes are correlated to a high
degree. However, an analysis of our dataset does not present evidence of such a strong
correlation. Of the 140 documents with multiple labels in the test set, there are 68 unique
label sets, of which only seven have a frequency of at least five. Thus increasing the number
of labels would only aggravate an already imbalanced class distribution.
Among other approaches, we mention two systems that use probabilistic generative models. McCallum and Nigam (1999) propose a system that starts with a small set of keywords
and unlabeled documents, and learns a nave Bayes classifier in a bootstrapping process from
the keyword-induced labels by using hierarchical shrinkage and expectation maximization
615

fiAbedin, Ng & Khan

on a held-out data set. Ueda and Saito (2002) present another generative model called Parametric Mixture Models, which treats multi-labeled text as a parametric mixture of words
relevant to each label. Their work is closely related to Latent Dirichlet Allocation (Blei,
Ng, & Jordan, 2003). The generative models usually assume that a document related to a
particular topic would have a high frequency of words related to that topic. In our research,
the documents are mostly devoted to description of the event that occurred, and the cause
of that event is only mentioned briefly. This makes generative models less suitable for the
task at hand as generative models would more likely generate models related to the events
and not the causes. A more comprehensive review of different approaches to multi-class
multi-label text classification can be found in the work of Tsoumakas and Katakis (2007).

8. Conclusions
We have investigated two approaches to the cause identification task, the goal of which is to
understand why an aviation safety incident happened via the identification of the causes, or
shaping factors, that are responsible for the incident. Both approaches exploit information
provided by a semantic lexicon, which is automatically constructed via Thelen and Riloffs
(2002) Basilisk framework augmented with our three algorithmic modifications (namely, the
use of a probabilistic similarity measure, the use of a common word pool, and the enforcement of minimum support and maximum generality constraints for words and extraction
patterns) and one linguistic modification (the use of N-gram-based extraction patterns).
The heuristic-based approach labels a report by employing the Occurrence Heuristic, which
simply looks for the words and phrases acquired during the semantic lexicon learning process in the report. The learning-based approach labels a report by employing inductive
and transductive support vector machines to learn models from reports labeled with shaping factors. Our experimental results indicate that the heuristic-based approach and the
supervised learning approach (when given sufficient training data) both significantly outperform our baseline, which, motivated by NASAs work, labels a report simply by using
the Occurrence Heuristic in combination with a set of manually-identified seed words and
phrases. More importantly, results of the heuristic-based approach indicate that our modifications to the original Basilisk framework are beneficial as far as cause identification is
concerned, and results of the learning-based approach indicates the usefulness of the lexicon
words when they are used in combination with unigrams as features for training an SVM
classifier. Overall, what we set out to prove was that it is possible to automate the cause
identification task by manually analyzing a small number of reports and using the information thus generated to train machine learning methods to identify the shaping factors of the
rest of the reports. Our experiments have been able to prove the feasibility of this approach,
and also the usefulness of learning a semantic lexicon and using the words in it as features.
Nevertheless, our best system achieves an F-measure of around 53.7%, which indicates that
cause identification is a difficult task, and that there is a lot of room for improvement. In
particular, our analysis of the errors made by the best system on 100 randomly chosen test
documents provides valuable insights into the task as well as directions for future research.
From our experience from this current research, we intend to extend our work in the
following directions. First and foremost, we plan to extend our approach to handle text
fragments bigger than phrases. Second, in order to improve the quality of labeling, we
616

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

propose to work on improving the lexicon learning performance further by using different
semantic similarity measures. For instance, we would like to study the performance of the
semantic similarities and weighting functions suggested by Curran and Moens (2002) in
our context. Third, we plan to use more thoroughly normalized text for better parsing
and tagging, as well as relevant region information (Ko, Park, & Seo, 2004; Patwardhan
& Riloff, 2007). Fourth, we propose to augment the semantic lexicon, specifically by using
the Google N-grams corpus (Brants & Franz, 2006) to extract frequent N-gram patterns
for words. Fifth, we propose to explore other more recent lexicon construction methods
like unsupervised word clustering (Pantel & Ravichandran, 2004), spectral analysis, mutual
exclusion bootstrapping, co-clustering and exploiting symmetric patterns. Finally, in order
to handle the shaping factors that are difficult to identify from words occurring in the
reports, we propose to employ much deeper analysis of the text at the semantic level. We
have also taken the step of making our annotated incident reports publicly available, and
we hope that we can stimulate research on this under-investigated problem in the NLP
community.

Acknowledgments
The authors would like to thank the anonymous reviewers who provided us with comments
that were invaluable in improving the quality of the paper. This research was supported
in part by NASA grant NNX08AC35A. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the
views or official policies, either expressed or implied, of NASA.

617

fiAbedin, Ng & Khan

Appendix A: Seed Words
Below are the seed words manually extracted from the 233 reports in the training set (see
Section 2.3.2 for details).
Shaping Factor
Attitude
Communication
Environment
Duty Cycle
Familiarity
Illusion
Other
Physical
Environment

Physical Factors
Preoccupation
Pressure
Proficiency

Resource
Deficiency

Taskload
Unexpected

Seed words
get HOMEITIS, attitude, inattentiveness, get THEREITIS, complacency, overconfidence, sarcastic, inattention
disturbance, static, radio discipline, congestion, noise
11 hour duty day, inadequate rest, last of 4 legs, heavy flying, reduced
rest, all-night flight, 12 hour day, red eye, ten leg day, all night
familiarization, not familiar, new, first departure, unfamiliar, unfamiliarity, very familiar, low time, first landing
bright lights
noise abatement policy, disoriented, confused, medical emergency,
economic considerations, disorientation, drunk passenger, confusion
cold, clouds, dark, setting sun, sun glare, obscured, visibility, hazy
stratus, birds, fog bank, solid overcast, snow, weather, rime, gust, low
weather, surface winds, jet blast, lightning, sea gulls, high ceilings,
hot, tailwind, chop, very dark, sea gull, winds, scattered, high tailwinds, extremely dark, too bright, icing, turbulence, RPTED wind,
terrain, bird strike, crosswind, thunderstorm, glare, reduced visibility, high flying birds, fog, severe winter weather, cloud, ice
very tired, HYPOXIA, tiredness, tired, fatigued, disorientation, fatigue, no rest
distracted, preoccupied, mental lapse, busy, DISTRS, distraction,
attention, inattention, absorbed
hurry, running late, pressure, low on fuel, fuel considerations, behind
schedule, late, peer pressure, under pressure, rushing
mistakes, mistaken, new hire, inexperience, forgotten, less than 100
hours, newly rated, training, recent pilot, inadvertently, bad turn,
MISINTERPED
loose connection, erratic, blown, overheated, bang, collapse, no idea,
unavailable, placarded, crack, Out Of Service, damage, smoke, inoperative, failure, leak, deferred items, communication failure, loss,
unreliable, FDRS problem, bump, shaking, master caution, inadequately lighted, unreadable, disconnected, malfunction, shudder, absence, hazard, inaccurate, UNFLAGGED, fire, broken, fluctuations,
compressor stall, deferral, unusable, wrong, intermittently, warning,
discrepancies, faulty, deferred, intermittent, missing
single pilot, solo
unexpected, suddenly, UNFORECAST

618

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Appendix B: Sample Lexicon Words Learned by Modified Basilisk
Below are the semantic lexicon words learned by the modified Basilisk framework in the
first two iterations.
Shaping Factor
Attitude
Communication
Environment
Duty Cycle
Familiarity
Illusion
Other
Physical
Environment
Physical Factors
Preoccupation

Pressure
Proficiency
Resource
Deficiency
Taskload
Unexpected

New words

aligned, fairly new, more familiar
initial confusion, minimum fuel emergency, misunderstanding,
weather emergency
TRWS, conflict message, cumulonimbus, large cells, numerous thunderstorms, occasionally severe, thunderstorm cells, weather buildups,
weather cell, weather en route
first factor
adequate attention, as much attention, close attention, close enough
attention, crew attention, enough attention, much attention, proper
attention, strict attention, very close attention

different, amiss, awry, obviously wrong, resulting loss, seriously
wrong, slight loss, temporary loss, terribly wrong, very wrong

619

fiAbedin, Ng & Khan

Appendix C: Sample Lexicon Words Learned by Original Basilisk
Below are the semantic lexicon words learned by the original Basilisk framework in the first
two iterations.
Shaping Factor
Attitude

Communication
Environment

Duty Cycle

Familiarity

Illusion

Other

Physical
Environment

Physical Factors

Preoccupation

New words
Air Traffic Control security, aileron yoke displacement, anomalous VFR Omni-Directional Radio Range information, assured TFR
avoidance, betrayal, concern and urgency, forgetting air carrier X,
magnified problem, operational complacency, unseen and unknown
turbulence
9001 noise, BNA runway 31 approach plate, LIGHTSPEED 20K
noise, Non- noise, OVERSPD bell, active noise, clearance delivery
transmission, left engine stall, static and Emergency Locator Transmitter, stuck trim or elevator movement
10 plus 16 layover, 2 different time frames, 3 back-to-back  continuous duty trips, 4 hour break, 69 minutes, 8 hour 15 minute flight time
day, Pacific Daylight Time departure, TPA flight, XC15 departure,
scheduled 2- leg continuous duty
S partial unfamiliarity, S perceived familiarity, Command familiarity, Command unfamiliarity, blue panel indication light, dispatch
work desks, generally unfamiliar, inexperience and unfamiliarity, new
everyday, past experience and familiarity
1 1/2 Nautical Mile SSW, 1/2-1/4 point, 10  off end, 50 feet side, Elmendorf required use, about 1/2 mile downwind, airspace E, foxtrot
intersection, lateral boundaries, mile right
misinformation, Flight Management System/heading anomalies, confusion/conflict, disoriented and confused, intense panic, micro sleep,
miss numerous times, mistake inconvenience, note closure problem,
start terror
MHT class Celsius, STRATO-cumulus, Thur morning, clouds underneath, compacted snow and ice, fair weather cumulus, next morning
weather, puffy cumulus clouds, thin scattered clouds, well developed
cumulus clouds
HYPOXIA/carbon monoxide, Minimum Equipment List 24-32-02,
basically tired, cardiac distress, indicating system problem, internal
bleeding, interrupted fuel flow, oncoming seizure, stress overload,
upper respiratory problems
Captain and First Officer attention, Terminal Radar Approach Control Facility distraction, close enough attention, consequently my attention, good enough attention, lip service, mind or attention, much
mind, real attention, real close attention
Continued on next page

620

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Shaping Factor
Pressure

Proficiency

Resource
Deficiency

Taskload

Unexpected

New words
Minimum Equipment List pressure, consistent answer, elevator pressure, intense pressure, part # Coordinated Universal Time, repercussion, right engine pressure, significant pressure, slow gear, wheel
pressure
& P school, CL65 ground school, basic flight training, hard lesson,
initial and annual proficiency training, occurrence and strive, rating
training, several military flying clubs, situation event, time limited
simulator sessions
Air Traffic Control loss, altitude deviation/loss, apparently inoperative, either inoperative, even a reexamining, intermittent or inoperative, known traffic conflict or loss, observed loss, recently a Los,
thankfully accurate
16500 # turboprop, A320 type aircraft, AVIAT husky A1 two place
tail DRAGGER aircraft, Cessna 402 type aircraft, Cessna model
421 type aircraft, L1011-250, LGA-MHT flight, McDonnell Douglas
MD11, more solo cross country FLTS, solo cross country privileges
significant, 1 jolt, approximately 5-10 sec, choppy and aircraft, consistently moderate, contributing workload factor, industry issue, just
as severe, rapid and immediate, real cushion

621

fiAbedin, Ng & Khan

Appendix D: Additional Stratified Approximate Randomization Tests

MLW

OLW

SVM-U

SVM-UB

SVM-L

SVM-UL

SVM-UBL

SVMT-U

SVMT-UB

SVMT-L

SVMT-UL

SVMT-UBL

SVM5-U

SVM5-UB

SVM5-L

SVM5-UL

SVM5-UBL

Methoda
SW
MLW
OLW
SVM-U
SVM-UB
SVM-L
SVM-UL
SVM-UBL
SVMT-U
SVMT-UB
SVMT-L
SVMT-UL
SVMT-UBL
SVM5-U
SVM5-UB
SVM5-L
SVM5-UL
SVM5-UBL

SW

To ascertain the statistical significance of the difference between the F-measure scores of the
different report labeling methods, we performed the stratified approximate randomization
test with 9,999 shuffles between all pairs of the results of Experiments 1 through 5 in Table 4.
The table below shows if the method in the column is statistically significantly better than
the method in the row at the level of p < 0.05. As before, statistical significance and
insignificance are denoted by a X and an X, respectively.

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
-

a. Legend: SW = Occurrence Heuristic using seed words, MLW = Occurrence Heuristic using modified
Basilisk lexicon, OLW = Occurrence Heuristic using original Basilisk lexicon, SVM-U = SVM using
unigrams, SVM-UB = SVM using unigrams and bigrams, SVM-L = SVM using lexicon words, SVM-UL
= SVM using unigrams and lexicon words, SVM-UBL = SVM using unigrams, bigrams and lexicon
words, SVMT-U = transductive SVM using unigrams, SVMT-UB = transductive SVM using unigrams
and bigrams, SVMT-L = transductive SVM using lexicon words, SVMT-UL = transductive SVM using
unigrams and lexicons, SVMT-UBL = transductive SVM using unigrams, bigrams and lexicon words,
SVM5-U = 5-fold SVM using unigrams, SVM5-UB = 5-fold SVM using unigrams and bigrams, SVM5-L
= 5-fold SVM using lexicon words, SVM5-UL = 5-fold SVM using unigrams and lexicon words, SVM5UBL = 5-fold SVM using unigrams, bigrams and lexicon words.

622

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Appendix E: Sample Preprocessed Reports
Report ACN#324831
RETURNING to waukegan regional airport from practice area located between 5-20 mile
W of the airport; flying solo as a student pilot; at about 3000 feet Mean Sea Level Visual
Flight Rules. cloud area about 5 mile W of the airport obscured view ahead so I reduced
altitude to proceed Visual Flight Rules and returned to 3000 feet after passing the thin
cloud line. the area to the n; containing fix references for airport location; was shrouded in
clouds and in fog at ground level. the same was true of the lake michigan shoreline to the
E. the ground was also substantially snow covered. although the airspace over the airport
was undoubtedly clear; as was the practice area; orientation to the field was lost to me.
I climbed to 4500 feet to increase the overview; without benefit. returning to 3000 feet;
I flew to what I believed to be n of the airfield to landfall the airport. I must have been
to the S; however; and proceeding S I flew into ord class B airspace. coinciding to being
lost; I contacted waukegan tower; not then realizing that I had flown as Federal Aviation
Regulation S as I had. I was directed to contact ord approach on the frequency given; and
beginning with ord approach was vectored back to waukegan airport; frequency changed to
tower control and blessedly cleared to land. the time lost was between 1 hour 15 minutes
and 1 1/2 hours.
Report ACN#566757
THE following event occurred while REPOSITIONING; by taxi; from the W side to the S
side of isp airport. I initially contacted longitude island tower asking permission to REPOS
from the W side to the OPS Base Operations Office of the tower (the S side). the controller
replied start taxi via taxiway W up to but hold short of runway 6. I read back the
instructions stating to start taxi via taxiway W holding short of runway 6. as I was taxiing;
there was an aircraft on taxiway W holding short of runway 6; performing a run-up. the
controller asked if I was able to get around the aircraft. I replied that I was able. the
controller then said use caution taxiing around the aircraft and cross runway 6. as I was
taxiing across runway 6; I noticed an aircraft on short final for runway 6. I was clear of
the runway before the aircraft touched down. the controller then came on the frequency
and said you were instructed to hold short of runway 6. I replied you cleared me across
runway 6. the controller said call the tower when you park. I replied roger; I will call
you when I park. I called and talked to the controller a few minutes later and he said you
were instructed to hold short of runway 6. again I told him that he had cleared me across
the runway. I feel that pilots and controllers need to listen to each other and decipher what
is said before acting on it.

623

fiAbedin, Ng & Khan

Appendix F: Lexicon Learned by Original Basilisk for Categories People
and Equipment
The following table shows the words and phrases learned by the original Basilisk framework
for the categories people and equipment (see Section 6.7.4).
Category
People

Equipment

New words
Agreed by both judges as correct: ABQ tower procedure specialist, ACN 126721 reporter, AFSFO, AVP tower specialist, Air Route
Traffic Control Center specialist, Air Traffic Control facility reps,
BDR tower specialist, BHM control, BUF field operations officer,
CAE tower specialist, Chicago quality control, DFW maintenance
manager, Flight Service Station dispatcher, SFOLM the Captain,
SII program manager, Stearman pilot, TLH supervisor, bur local
controller, casino manager, cos Air Traffic Control chief, flight test
engineers, local balloon repairman, outbound Captain and First Officer, repair facility and pilot, shift boss, spokesperson, station management individual, technician desk, tower supervisor/manager
Identified by one judge as correct: Flight Standards District
Office ORL, again maintenance supervisor, approach controller verbatim, freighter aircraft and approach, him and tower, passenger and
fatigue
Agreed by both judges as incorrect: ACN 670635, ACN 682482,
AT6 aircraft, B737-300/500 SRM, EMB service manual, Non-air
carrier aircraft, RPTR ACN 518698, RPTR ACN 601074, RPTR
ACN 658075, RPTR ACN 664336, RPTR ACN 676343, RPTR ACN
88920, cabin or company, other aircraft center, reliable research resources
Agreed by both judges as correct: Collision Avoidance System II 10 Distance Measuring Equipment screen, Collision Avoidance
System II B737-200, Collision Avoidance System II EHSI, Collision
Avoidance System II IVSI display, Collision Avoidance System II
Missed Approach Point page, Collision Avoidance System II RR,
Continued on next page

624

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Category

New words
Identified by one judge as correct: Collision Avoidance System II VSI overlay, Resolution Advisory  stopped and aircraft, Collision Avoidance System II stop climb  alert, Collision Avoidance
System II traffic ;  then climb  advisory, Collision Avoidance System II traffic ; traffic  aural warning, Collision Avoidance System
II 3 mile circle, Collision Avoidance System II 5 mile scale, Collision Avoidance System II 6 mile scale, Collision Avoidance System II POPUP traffic, Collision Avoidance System II Resolution
Advisory alerts, Collision Avoidance System II Resolution Advisory
climb priority, Collision Avoidance System II Resolution Advisory
climb warning, Collision Avoidance System II Resolution Advisory
signals, Collision Avoidance System II Resolution Advisory zone,
Collision Avoidance System II Resolution Advisory/Traffic Advisory
alert, Collision Avoidance System II Resolution Advisory/Traffic Advisory alerts/advisories, Collision Avoidance System II Resolution
Advisory/altitude deviation, Collision Avoidance System II Traffic
Advisory and Resolution Advisory alerts, Collision Avoidance System II WINDSHEAR warning, Collision Avoidance System II advisory alert, Collision Avoidance System II advisory alert and warning,
Collision Avoidance System II warning and aircraft
Agreed by both judges as incorrect: Collision Avoidance System II 10 Oclock and 2 1/2 to 3 mile, Collision Avoidance System II Resolution Advisory climb  command, Collision Avoidance
System II Resolution Advisory area, Collision Avoidance System
II Resolution Advisory climb or descent, Collision Avoidance System II Resolution Advisory data tag, Collision Avoidance System
II Resolution Advisory descent, Collision Avoidance System II Resolution Advisory green band target, Collision Avoidance System II
Resolution Advisory increase climb, Collision Avoidance System II
Resolution Advisory maneuvering, Collision Avoidance System II
Resolution Advisory messages, Collision Avoidance System II Resolution Advisory recovery procedure, Collision Avoidance System
II Resolution Advisory requirement, Collision Avoidance System II
Resolution Advisory requiring climb, Collision Avoidance System
II Resolution Advisory resolution, Collision Avoidance System II
Traffic Advisory notification, Collision Avoidance System II Traffic
Advisory/Resolution Advisory aircraft, Collision Avoidance System
II Traffic Advisory/Resolution Advisory event, Collision Avoidance
System II action requirements, Collision Avoidance System II advice,
Collision Avoidance System II advisories and instructions, Collision
Avoidance System II caution, Collision Avoidance System II quit

625

fiAbedin, Ng & Khan

Appendix G: Lexicon Learned by Modified Basilisk for Categories People
and Equipment
The following table shows the words and phrases learned by the modified Basilisk framework
for the categories people and equipment (see Section 6.7.4).
Category
People

Equipment

New words
Agreed by both judges as correct:  First Officer,  my First Officer, ; First Officer, CP, Captain, Captain RPTR, Captain trainee,
Co-Captain, Co-pilot, First Officer, First Officer # 2, Initial Operating Experience Captain, PAXS, Pilot Flying and First Officer,
Potomac controller, RPTING Captain, RPTING First Officer, RPTING pilot, RPTR Captain, RPTR pilot, S/O, ZOA supervisor, air
carrier Y pilot, aircraft X pilot, aircraft commander, all the passenger, analyst, and First Officer, baron pilot, biplane pilot, controller,
facility person, first observer, flight attendant # 3, flight attendants
and passenger, flying Captain, forward observer, passenger, passenger and crew, passenger and flight attendants, right seat pilot, second observer, sic, specialist, student Captain, supervisor/Controller,
tower Controller, tower operator, training pilot
Identified by one judge as correct: RPTR, gate and passenger,
so First Officer
Agreed by both judges as incorrect: departure and departure,
neither the Captain, which CLRLY
Agreed by both judges as correct: # 1 Auto-Pilot, #
2 Auto-Pilot, 3 AUTOPLTS, AUTOFLT, AUTOTHROTTLE,
AUTOTHROTTLE and Auto-Pilot, AUTOTHROTTLES, AUTOTHROTTLES and Auto-Pilot, AUTOTHRUST, Auto-Pilot
# 1, Auto-Pilot # 2, Auto-Pilot B, Auto-Pilot and AUTOTHRUST, Auto-Pilot and PMs, Auto-Pilot and throttles, AutoPilot/AUTOTHROTTLES, Cessna 180, Collision Avoidance System
II system, ENGS # 2 and # 3, aircraft ABCD, aircraft Auto-Pilot,
aircraft engine, allowed aircraft, automatic pilot, automatic throttle,
automatic throttles, autopilot, center Auto-Pilot, craft, emergency
engine, left Auto-Pilot, left hand engine, parked plane, right AutoPilot
Identified by one judge as correct:
problem engine, maintenance and aircraft, later aircraft, aircraft and aircraft, Collision
Avoidance System II alert, # 1 Constant Speed Drive, Auto-Pilot
and AUTOTHROTTLES, WDB 2, perf
Agreed by both judges as incorrect: aircraft beginning, aircraft
parallel, normal and aircraft, person or property, persons or property,
so aircraft, time aircraft

626

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

References
Ando, R. K. (2004). Semantic lexicon construction: Learning from unlabeled data via
spectral analysis. In Proceedings of the 8th Conference on Computational Natural
Language Learning, pp. 916.
Artstein, R., & Poesio, M. (2008). Inter-coder agreement for computational linguistics.
Computional Linguistics, 34 (4), 555596.
Avancini, H., Lavelli, A., Sebastiani, F., & Zanoli, R. (2006). Automatic expansion of
domain-specific lexicons by term categorization. ACM Transactions on Speech and
Language Processing (TSLP), 3 (1), 130.
Baker, L. D., & McCallum, A. K. (1998). Distributional clustering of words for text classification. In Proceedings of the 21st Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pp. 96103.
Banko, M., & Brill, E. (2001). Mitigating the paucity-of-data problem: Exploring the effect of training corpus size on classifier performance for natural language processing.
In Proceedings of the 1st International Conference on Human Language Technology
Research.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of
Machine Learning Research, 3, 9931022.
Brants, T., & Franz, A. (2006). Web 1T 5-gram Version 1. Linguistic Data Consortium,
Philadelphia, USA.
Cheng, P. W. (1997). From covariation to causation: A causal power theory. Psychological
Review, 104 (2), 367405.
Chinchor, N. (1992). The statistical significance of the MUC-4 results. In Proceedings of
the 4th Message Understanding Conference, pp. 3050.
Chinchor, N., Hirschman, L., & Lewis, D. D. (1993). Evaluating message understanding systems: An analysis of the Third Message Understanding Conference (MUC-3).
Computational Linguistics, 19, 409449.
Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning, 20 (3), 273
197.
Crammer, K., & Singer, Y. (2002). On the algorithmic implementation of multiclass kernelbased vector machines. Journal of Machine Learning Research, 2, 265292.
Curran, J. R., & Moens, M. (2002). Improvements in automatic thesaurus extraction.
In Proceedings of the ACL 2002 Workshop on Unsupervised Lexical Acquisition, pp.
5966.
Curran, J. R., Murphy, T., & Scholz, B. (2007). Minimising semantic drift with mutual
exclusion bootstrapping. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pp. 172180.
627

fiAbedin, Ng & Khan

Davidov, D., & Rappoport, A. (2006). Efficient unsupervised discovery of word categories
using symmetric patterns and high frequency words. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics, pp. 297304.
Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10 (7), 18951923.
Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19 (1), 6174.
Everitt, B. S. (1977). The Analysis of Contingency Tables. Chapman and Hall.
Ferryman, T. A., Posse, C., Rosenthal, L. J., Srivastava, A. N., & Statler, I. C. (2006). What
happened, and why: Toward an understanding of human error based on automated
analyses of incident reports Volume II. Tech. rep. NASA/TP2006-213490, National
Aeronautics and Space Administration.
Ganeri, J., Noordhof, P., & Ramachandran, M. (1996). Counterfactuals and preemptive
causation. Analysis, 56 (4), 219225.
Garcia, D. (1997). COATIS, an NLP system to locate expressions of actions connected
by causality links. In Proceedings of the 10th European Workshop on Knowledge
Acquisition, Modeling and Mangement, pp. 347352.
Girju, R. (2003). Automatic detection of causal relations for question answering. In Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question
Answering, pp. 7683.
Godbole, S., & Sarawagi, S. (2004). Discriminative methods for multi-labeled classification.
In Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data
Mining, pp. 2230.
Griffiths, T. L., & Tenenbaum, J. B. (2005). Structure and strength in causal induction.
Cognitive Psychology, 51, 334384.
Grishman, R., & Ksiezyk, T. (1990). Causal and temporal text analysis: The role of the
domain model. In Proceedings of the 13th International Conference on Computational
Linguistics, pp. 126131.
Halpern, J. Y., & Pearl, J. (2005). Causes and explanations: A structural-model approach.
Part I: Causes. The British Journal for the Philosophy of Science, 56, 843887.
Hassan, H., Hassan, A., & Emam, O. (2006). Unsupervised information extraction approach
using graph mutual reinforcement. In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pp. 501508.
Hume, D. (1999 (Original work published 1748)). An Enquiry Concerning Human Understanding. Oxford University Press, USA.
Hume, D. (2000 (Original work published 1739)). A Treatise of Human Nature. Oxford
University Press, USA.
Joachims, T. (1999). Advances in Kernel Methods - Support Vector Learning, chap. Making
large-Scale SVM Learning Practical. MIT-Press.
628

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Joachims, T. (1998). Text categorization with suport vector machines: Learning with many
relevant features. In Proceedings of the 10th European Conference on Machine Learning, pp. 137142.
Joachims, T. (1999). Transductive inference for text classification using support vector
machines. In Proceedings of the 16th International Conference on Machine Learning,
pp. 200209.
Kaplan, R., & Berry-Rogghe, G. (1991). Knowledge-based acquisition of causal relationships
in text. Knowledge Acquisition, 3 (3), 317337.
Kersey, C., Di Eugenio, B., Jordan, P., & Katz, S. (2009). KSC-PaL: A peer learning agent
that encourages students to take the initiative. In Proceedings of the 4th Workshop
on Innovative Use of NLP for Building Educational Applications, pp. 5563.
Khoo, C. S. G., Chan, S., & Niu, Y. (2000). Extracting causal knowledge from a medical
database using graphical patterns. In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pp. 336343.
Ko, Y., Park, J., & Seo, J. (2004). Improving text categorization using the importance of
sentences. Information Processing and Management, 40 (1), 6579.
Krippendorff, K. (2004). Content analysis: An introduction to its methodology. Sage Publications, Inc.
Lewis, D. (1973). Causation. Journal of Philosophy, 70 (17), 556567.
Lin, D. (1998). Dependency-based evaluation of MINIPAR. In Proceedings of the LREC
Workshop on Evaluation of Parsing Systems, pp. 317329.
Lin, D., & Pantel, P. (2001). Induction of semantic classes from natural language text. In
Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 317322.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building a large annotated
corpus of English: The Penn Treebank. Computational Linguistics, 19 (2), 313330.
Special Issue on Using Large Corpora.
McCallum, A., & Nigam, K. (1999). Text classification by bootstrapping with keywords,
EM and shrinkage. In Proceedings of the ACL Workshop on Unsupervised Learning
in Natural Language Processing, pp. 5258.
Noreen, E. W. (1989). Computer-Intensive Methods for Testing Hypotheses : An Introduction. Wiley.
Pantel, P., & Lin, D. (2002). Discovering word senses from text. In Proceedings of the 8th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pp. 613619.
Pantel, P., & Ravichandran, D. (2004). Automatically labeling semantic classes. In Proceedings of the Human Language Technology Conference of the North American Chapter
of the Association for Computational Linguistics, pp. 321328.
Passonneau, R. (2004). Computing reliability for coreference annotation. In Proceedings of
the Fourth International Conference on Language Resources and Evaluation, Vol. 4,
pp. 15031506.
629

fiAbedin, Ng & Khan

Patwardhan, S., & Riloff, E. (2006). Learning domain-specific information extraction patterns from the web. In Proceedings of the COLING/ACL Workshop on Information
Extraction Beyond The Document, pp. 6673.
Patwardhan, S., & Riloff, E. (2007). Effective information extraction with semantic affinity
patterns and relevant regions. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language
Learning, pp. 717727.
Pereira, F. C. N., Tishby, N., & Lee, L. (1993). Distributional clustering of English words.
In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pp. 183190.
Phan, X.-H. (2006a). CRFChunker: CRF English Phrase Chunker. http://crfchunker.
sourceforge.net/.
Phan, X.-H. (2006b). CRFTagger: CRF English POS Tagger.
sourceforge.net/.

http://crftagger.

Phillips, W., & Riloff, E. (2007). Exploiting role-identifying nouns and expressions for information extraction. In Proceedings of International Conference on Recent Advances
in Natural Language Processing.
Polanyi, L., & Zaenen, A. (2006). Contextual valence shifters. In Computing Attitude and
Affect in Text: Theory and Applications, pp. 110. Springer Verlag.
Posse, C., Matzke, B., Anderson, C., Brothers, A., Matzke, M., & Ferryman, T. (2005).
Extracting information from narratives: An application to aviation safety reports. In
Proceedings of the 2005 IEEE Aerospace Conference, pp. 36783690.
Read, J., Pfahringer, B., & Holmes, G. (2008). Multi-label classification using ensembles
of pruned sets. In Proceedings of the 8th IEEE International Conference on Data
Mining, pp. 9951000.
Riloff, E. (1996). Automatically generating extraction patterns from untagged text. In
Proceedings of the 13th National Conference on Artificial Intelligence, pp. 10441049.
Roark, B., & Charniak, E. (1998). Noun-phrase co-occurrence statistics for semi-automatic
semantic lexicon construction. In Proceedings of the 17th International Conference on
Computational Linguistics, pp. 11101116.
Rohwer, R., & Freitag, D. (2004). Towards full automation of lexicon construction. In
Proceedings of the Computational Lexical Semantics Workshop at HLT-NAACL 2004,
pp. 916.
Rychly, P., & Kilgarriff, A. (2007). An efficient algorithm for building a distributional
thesaurus (and other Sketch Engine developments). In Proceedings of the ACL 2007
Demo and Poster Sessions, pp. 4144.
Sebastiani, F. (2002). Machine learning in automated text categorization. ACM Computing
Surveys, 34 (1), 147.
Sundheim, B. M. (1992). Overview of the fourth message understanding evaluation and
conference. In Proceedings of the Fourth Message Understanding Conference, pp. 3
21.
630

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Tang, L., Rajan, S., & Narayanan, V. K. (2009). Large scale multi-label classification via
metalabeler. In Proceedings of International World Wide Web Conference, pp. 211
220.
Thelen, M., & Riloff, E. (2002). A bootstrapping method for learning semantic lexicons
using extraction pattern contexts. In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pp. 214221.
Tsoumakas, G., & Katakis, I. (2007). Multi-label classification: An overview. International
Journal of Data Warehousing and Mining, 3 (3), 113.
Tsoumakas, G., & Vlahavas, I. P. (2007). Random k -labelsets: An ensemble method for
multilabel classification. In Proceedings of the 18th European Conference on Machine
Learning, Vol. 4701 of Lecture Notes in Computer Science, pp. 406417.
Ueda, N., & Saito, K. (2002). Parametric mixture models for multi-labeled text. In Advances
in Neural Information Processing Systems 15, pp. 721728.
van Delden, S., & Gomez, F. (2004). Retrieving NASA problem reports: A case study
in natural language information retrieval. Data and Knowledge Engineering, 48 (2),
231246.
Vapnik, V. N. (1998). Statistical Learning Theory. Wiley.
Yang, Y., & Pedersen, J. O. (1997). A comparative study on feature selection in text categorization. In Proceedings of the 14th International Conference on Machine Learning,
pp. 412420.
Yangarber, R. (2003). Counter-training in discovery of semantic patterns. In Proceedings
of the 41st Annual Meeting of the Association for Computational Linguistics, pp.
343350.
Zaidan, O. F., Eisner, J., & Piatko, C. (2007). Using annotator rationales to improve machine learning for text categorization. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational
Linguistics, pp. 260267.
Zhang, Q., Zhou, Y., Huang, X., & Wu, L. (2008). Graph mutual reinforcement based
bootstrapping. Information Retrieval Technology, 4993/2008, 203212.

631

fiJournal of Artificial Intelligence Research 38 (2010) 371-413

Submitted 2/10; published 7/10

Approximate Model-Based Diagnosis
Using Greedy Stochastic Search
Alexander Feldman

a.b.feldman@tudelft.nl

Delft University of Technology
Mekelweg 4, 2628 CD, Delft, The Netherlands

Gregory Provan

g.provan@cs.ucc.ie

University College Cork
College Road, Cork, Ireland

Arjan van Gemund

a.j.c.vangemund@tudelft.nl

Delft University of Technology
Mekelweg 4, 2628 CD, Delft, The Netherlands

Abstract
We propose a StochAstic Fault diagnosis AlgoRIthm, called Safari, which trades off
guarantees of computing minimal diagnoses for computational efficiency. We empirically
demonstrate, using the 74XXX and ISCAS85 suites of benchmark combinatorial circuits,
that Safari achieves several orders-of-magnitude speedup over two well-known deterministic algorithms, CDA and HA , for multiple-fault diagnoses; further, Safari can compute a
range of multiple-fault diagnoses that CDA and HA cannot. We also prove that Safari
is optimal for a range of propositional fault models, such as the widely-used weak-fault
models (models with ignorance of abnormal behavior). We discuss the optimality of Safari in a class of strong-fault circuit models with stuck-at failure modes. By modeling the
algorithm itself as a Markov chain, we provide exact bounds on the minimality of the diagnosis computed. Safari also displays strong anytime behavior, and will return a diagnosis
after any non-trivial inference time.

1. Introduction
Model-Based Diagnosis (MBD) is an area of artificial intelligence that uses a system model,
together with observations about system behavior, to isolate sets of faulty components (diagnoses) that explain the observed behavior according to some minimality criterion. The
standard MBD formalization (Reiter, 1987) frames a diagnostic problem in terms of a set
of logical clauses that include mode-variables describing the nominal and fault status of
system components; from this the diagnostic status of the system can be computed given
an observation of the systems sensors. MBD provides a sound and complete approach to
enumerating multiple-fault diagnoses, and exact algorithms can guarantee finding a diagnosis optimal with respect to the number of faulty components, probabilistic likelihood,
etc.
The biggest challenge (and impediment to industrial deployment) is the computational
complexity of the MBD problem. The MBD problem of determining if there exists a diagnosis with at most k faults is NP-hard for the arbitrary propositional models we consider in
this article (Bylander, Allemang, Tanner, & Josephson, 1991; Friedrich, Gottlob, & Nejdl,
1990). Computing the set of all diagnoses is harder still, since there are possibly exponenc
2010
AI Access Foundation. All rights reserved.

fiFeldman, Provan, & van Gemund

tially many such diagnoses. Since almost all proposed MBD algorithms have been complete
and exact, with some authors proposing possible trade-offs between completeness and faster
consistency checking by employing methods such as BCP (Williams & Ragno, 2007), the
complexity problem still remains a major challenge to MBD.
To overcome this complexity problem, we propose a novel approximation approach for
multiple-fault diagnosis, based on a stochastic algorithm. Safari (StochAstic Fault diagnosis AlgoRIthm) sacrifices guarantees of optimality, but for diagnostic systems in which
faults are described in terms of an arbitrary deviation from nominal behavior, Safari can
compute diagnoses several orders of magnitude faster than competing algorithms.
Our contributions are as follows. (1) This paper introduces an approximation algorithm
for computing diagnoses within an MBD framework, based on a greedy stochastic algorithm.
(2) We show that we can compute minimal-cardinality diagnoses for weak fault models in
polynomial time (calling an incomplete SAT-solver that implements Boolean Constraint
Propagation1 (BCP) only), and that more general frameworks (such as a sub-class of strong
fault models) are also amenable to this class of algorithm. (3) We model Safari search as a
Markov chain to show the performance and optimality trade-offs that the algorithm makes.
(4) We apply this algorithm to a suite of benchmark combinatorial circuits, demonstrating order-of-magnitude speedup over two state-of-the-art deterministic algorithms, CDA
and HA , for multiple-fault diagnoses. (5) We compare the performance of Safari against
a range of Max-SAT algorithms for our benchmark problems. Our results indicate that,
whereas the search complexity for the deterministic algorithms tested increases exponentially with fault cardinality, the search complexity for this stochastic algorithm appears
to be independent of fault cardinality. Safari is of great practical significance, as it can
compute a large fraction of minimal-cardinality diagnoses for discrete systems too large or
complex to be diagnosed by existing deterministic algorithms.

2. Technical Background
Our discussion continues by formalizing some MBD notions. This paper uses the traditional
diagnostic definitions (de Kleer & Williams, 1987), except that we use propositional logic
terms (conjunctions of literals) instead of sets of failing components.
Central to MBD, a model of an artifact is represented as a propositional formula over
some set of variables. Discerning two subsets of these variables as assumable and observable 2
variables gives us a diagnostic system.
Definition 1 (Diagnostic System). A diagnostic system DS is defined as the triple DS =
hSD, COMPS, OBSi, where SD is a propositional theory over a set of variables V , COMPS 
V , OBS  V , COMPS is the set of assumables, and OBS is the set of observables.
Throughout this paper we will assume that OBS  COMPS =  and SD 6|=. Not all
propositional theories used as system descriptions are of interest to MBD. Diagnostic systems can be characterized by a restricted set of models, the restriction making the problem
1. With formulae in Conjunctive Normal Form (CNF), BCP is implemented through the unit resolution
rule.
2. In the MBD literature the assumable variables are also referred to as component, failure-mode, or
health variables. Observable variables are also called measurable, or control variables.

372

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

of computing diagnosis amenable to algorithms like the one presented in this paper. We
consider two main classes of models.
Definition 2 (Weak-Fault Model). A diagnostic system DS = hSD, COMPS, OBSi belongs
to the class WFM iff for COMPS = {h1 , h2 , . . . , hn }, SD is equivalent to (h1  F1 ) (h2 
F2 )  . . .  (hn  Fn ) and COMPS  V  = , where V  is the set of all variables appearing
in the propositional formulae F1 , F2 , . . . , Fn .
Note the conventional selection of the sign of the health variables h1 , h2 , . . . hn . Alternatively, negative literals, e.g., f1 , f2 , . . . , fn can be used to express faults, in which case a
weak-fault model is in the form (f1  F1 )  . . .  (fn  Fn ). Other authors use ab for
abnormal or ok for healthy.
Weak-fault models are sometimes referred to as models with ignorance of abnormal
behavior (de Kleer, Mackworth, & Reiter, 1992), or implicit fault systems. Alternatively,
a model may specify faulty behavior for its components. In the following definition, with
the aim of simplifying the formalism throughout this paper, we adopt a slightly restrictive
representation of faults, allowing only a single fault-mode per assumable variable. This can
be easily generalized by introducing multi-valued logic or suitable encodings (Hoos, 1999).
Definition 3 (Strong-Fault Model). A diagnostic system DS = hSD, COMPS, OBSi belongs
to the class SFM iff SD is equivalent to (h1  F1,1 )  (h1  F1,2 )  . . .  (hn  Fn,1 ) 
(hn  Fn,2 ) such that 1  i, j  n, k  {1, 2}, {hi }  COMPS, F{j,k} is a propositional
formula, and none of hi appears in Fj,k .
Membership testing for the WFM and SFM classes can be performed efficiently in many
cases, for example, when a model is represented explicitly as in Def. 2 or Def. 3.
2.1 A Running Example
We will use the Boolean circuit shown in Fig. 1 as a running example for illustrating all
the notions and algorithms in this paper. The subtractor, shown there, consists of seven
components: an inverter, two or-gates, two xor-gates, and two and-gates. The expression
h  (o  i) models the normative (healthy) behavior of an inverter, where the variables i,
o, and h represent input, output and health respectively. Similarly, an and-gate is modeled
as h  [o  (i1  i2 )] and an or-gate by h  [o  (i1  i2 )]. Finally, an xor-gate is specified
as h  [o   (i1  i2 )].
The above propositional formulae are copied for each gate in Fig. 1 and their variables
renamed in such a way as to properly connect the circuit and disambiguate the assumables,
thus obtaining a propositional formula for the Boolean subtractor, given by:
SDw = {h1  [i   (y  p)]}  {h2  [d   (x  i)]}  [h3  (j  y  p)] 
 [h4  (m  l  j)]  [h5  (b  m  k)]  [h6  (x  l)] 
 [h7  (k  y  p)]

(1)

A strong-fault model for the Boolean circuit shown in Fig. 1 is constructed by assigning
fault-modes to the different gate types. We will assume that, when malfunctioning, the
output of an xor-gate has the value of one of its inputs, an or-gate can be stuck-at-one,
373

fiFeldman, Provan, & van Gemund

x
y
p

h1

h3

i

j

h2

d

h5

b

h6

l

h4

h7

m

k

Figure 1: A subtractor circuit
an and-gate can be stuck-at-zero, and an inverter behaves like a buffer. This gives us the
following strong-fault model formula for the Boolean subtractor circuit:
SDs = SDw  [h1  (i  y)]  [h2  (d  x)]  (h3  j) 
 (h4  m)  (h5  b)  [h6  (x  l)]  (h7  k)

(2)

For both models (SDs and SDw ), the set of assumable variables is COMPS = {h1 , h2 , . . . , h7 }
and the set of observable variables is OBS = {x, y, p, d, b}.
2.2 Diagnosis and Minimal Diagnosis
The traditional query in MBD computes terms of assumable variables which are explanations for the system description and an observation.
Definition 4 (Health Assignment). Given a diagnostic system DS = hSD, COMPS, OBSi,
an assignment  to all variables in COMPS is defined as a health assignment.
A health assignment  is a conjunction of propositional literals. In some cases it is convenient to use the set of negative or positive literals in . These two sets are denoted as
Lit  () and Lit + (), respectively.
In our example, the all nominal assignment is 1 = h1  h2  . . .  h7 . The health
assignment 2 = h1  h2  h3  h4  h5  h6  h7 means that the two and-gates from Fig. 1
are malfunctioning. What follows is a formal definition of consistency-based diagnosis.
Definition 5 (Diagnosis). Given a diagnostic system DS = hSD, COMPS, OBSi, an observation , which is an instantiation of some variables in OBS, and a health assignment , 
is a diagnosis iff SD     6|=.
Traditionally, other authors (de Kleer & Williams, 1987) arrive at minimal diagnosis by computing a minimal hitting set of the minimal conflicts (broadly, minimal health assignments
incompatible with the system description and the observation), while this paper makes no
use of conflicts, hence the equivalent, direct definition above.
There is a total of 96 possible diagnoses given SDw and an observation 1 = x  y  p 
b  d. Example diagnoses are 3 = h1  h2  . . .  h7 and 4 = h1  h2  h3  . . .  h7 .
Trivially, given a weak-fault model, the all faulty health assignment (in our example
374

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

a = h1  . . .  h7 ) is a diagnosis for any instantiation of the observable variables in OBS
(cf. Def. 2).
In the analysis of our algorithm we need the opposite notion of diagnosis, i.e., health
assignments inconsistent with a model and an observation. In the MBD literature these
assignments are usually called conflicts. Conflicts, however, do not necessarily instantiate
all variables in COMPS. As in this paper we always use full health instantiations, the use
of the term conflict is avoided to prevent confusion.
In the MBD literature, a range of types of preferred diagnosis has been proposed.
This turns the MBD problem into an optimization problem. In the following definition we
consider the common subset-ordering.
Definition 6 (Minimal Diagnosis). A diagnosis   is defined as minimal, if no diagnosis
  exists such that Lit  (  )  Lit  (  ).
Consider the weak-fault model SDw of the circuit shown in Fig. 1 and an observation
2 = x  y  p  b  d. In this example, two of the minimal diagnoses are 5 =
h1  h2  h3  h4  h5  h6  h7 and 6 = h1  h2  . . .  h5  h6  h7 . The diagnosis
7 = h1  h2  h3  h4  h5  h6  h7 is non-minimal as the negative literals in 5 form
a subset of the negative literals in 7 .
Note that the set of all minimal diagnoses characterizes all diagnoses for a weak-fault
model, but that does not hold in general for strong-fault models (de Kleer et al., 1992).
In the latter case, faulty components may exonerate each other, resulting in a health
assignment containing a proper superset of the negative literals of another diagnosis not to
be a diagnosis. In our example, given SDs and 3 = x  y  p  b  d, it follows that
8 = h1  h2  h3  h4  . . .  h7 is a diagnosis, but 9 = h1  h2  h3  h4  . . .  h7
is not a diagnosis, despite the fact that the negative literals in 9 form a superset of the
negative literals in 8 .
Definition 7 (Number of Minimal Diagnoses). Let the set  (SD  ) contain all minimal diagnoses of a system description SD and an observation . The number of minimal
diagnoses, denoted as | (SD  )|, is defined as the cardinality of  (SD  ).
Continuing our running example, | (SDw  2 )| = 8 and | (SDs  3 )| = 2. The number
of non-minimal diagnoses of SDw  2 is 61.
Definition 8 (Cardinality of a Diagnosis). The cardinality of a diagnosis, denoted as ||,
is defined as the number of negative literals in .
Diagnosis cardinality gives us another partial ordering: a diagnosis is defined as minimal
cardinality iff it minimizes the number of negative literals.
Definition 9 (Minimal-Cardinality Diagnosis). A diagnosis   is defined as minimalcardinality if no diagnosis   exists such that |  | < |  |.
The cardinality of a minimal-cardinality diagnosis computed from a system description SD
and an observation  is denoted as MinCard (SD  ). For our example model SDw and an
observation 4 = x  y  p  b  d, it follows that MinCard (SDw  4 ) = 2. Note that
in this case all minimal diagnoses are also minimal-cardinality diagnoses.
375

fiFeldman, Provan, & van Gemund

A minimal cardinality diagnosis is a minimal diagnosis, but the opposite need not hold.
In the general case, there are minimal diagnoses which are not minimal-cardinality diagnoses. Consider the example SDw and 2 given earlier in this section, and the two resulting
minimal diagnoses 5 and 6 . From these two, only 5 is a minimal-cardinality diagnosis.
Definition 10 (Number of Minimal-Cardinality Diagnoses). Let the set  (SD  ) contain all minimal-cardinality diagnoses of a system description SD and an observation .
The number of minimal-cardinality diagnoses, denoted as | (SD  )|, is defined as the
cardinality of  (SD  ).
Computing the number of minimal-cardinality diagnoses for the running example results in
| (SDw  2 )| = 2, | (SDs  3 )| = 2, and | (SDw  4 )| = 4.
2.3 Converting Propositional Formulae to Clausal Form
Our approach is related to satisfiability, and Safari uses a SAT solver. SAT solvers commonly accept their input in Conjunctive Normal Form (CNF), although there exist SAT
solvers that work directly on propositional formulae (Thiffault, Bacchus, & Walsh, 2004).
Converting a propositional formula to CNF can be done with (Tseitin, 1983) or without
(Forbus & de Kleer, 1993) the introduction of intermediate variables. In both cases important structural information is lost, which may lead to performance degradation when
checking if a formula is consistent or when computing a solution.
Lemma 1. A fault-model SD = F1  F2  . . .  Fn (SD  WFM or SD  SFM) with
n = |COMPS| component variables can be converted to CNF in time O(|COMPS|) where
 is the time for converting the largest subformula Fi (1  i  n) to CNF.
Proof (Sketch). The conversion of SD to CNF can be done by (1) converting each subformula
Fi to CNF and (2) concatenating the resulting CNFs in the final CNF equivalent of SD.
The complexity of (1) is O(n) while the complexity of (2) is, in the worst-case, O(2m ) < ,
where m is the largest number of variables in a subformula Fi . As a result, the total time
for converting SD is dominated by  and it is linear in |COMPS|.
Lemma 1 is useful in the cases in which each subformula Fi is small. This is the case in many
practical situations where SD is composed of small component models. This is also the case
with our experimental benchmark (cf. Sec. 6) where the model of a combinational circuit
is the conjunction of fault models of simple logic gates (x-bit and-gates, typically x < 10,
xor-gates, etc.). Ideally, Safari would use a non-CNF SAT solver, but for practical reasons
we have constrained our reasoning to diagnostic models with concise CNF encodings.
Consider, for example, the formula (x1  y1 )  (x2  y2 )      (xn  yn ), which is
in Disjunctive Normal Form3 (DNF) and, converted to CNF, has 2n clauses. Although
similar examples of propositional formulae having exponentially many clauses in their CNF
representations are easy to find, they are artificial and are rarely encountered in MBD.
Furthermore, the Boolean circuits with which we have tested the performance of Safari
do not show exponential blow-up when converted to CNF.
3. Note that all DNF formulae are also propositional formulae.

376

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

2.4 Complexity of Diagnostic Inference
This section discusses the complexity of the problems in which we are interested, namely
the problem of computing a single or the set of all minimal diagnoses, using two minimality
criteria, subset-minimality () and cardinality-minimality (). We assume as input a CNF
formula defined over a variable set V , of which  = |COMPS| are assumable (or fault)
variables. Table 1 introduces the notation that we use to define these 4 types of diagnosis.
Table 1: Summary of definitions of types of diagnosis of interest
Symbol

Diagnoses






1
1
all
all

Preference Criterion





(subset-minimality)
(cardinality-minimality)
(subset-minimality)
(cardinality-minimality)

The complexity of computing the set of all diagnoses is harder than computing a single
diagnosis, since the number of diagnoses is, in the worst case, exponential in the input size
(number of components). This problem is bounded from below by the problem of counting
the number of diagnoses. This problem has been shown to be #co-NP -Complete (Hermann
& Pichler, 2007).
If we restrict our clauses to be Horn or definite Horn, then we can reduce the complexity
of the problems that we are solving, at the expense of decreased model expressiveness. Under
a Horn-clause restriction, for SD  WFM, determining if a first minimal diagnosis exists
is in P . Under the same restriction, for SD  SFM, deciding if a first minimal diagnosis
exists is NP-hard (Friedrich et al., 1990). In both cases (SD  WFM, SFM) deciding if a
next diagnosis exists is NP-hard.
The diagnosis problems of interest in this article are intractable in the worst-case. The
complexity of a closely-related problem, Propositional Abduction Problems (PAPs), has
been studied by Eiter and Gottlob (1995). They show that for a propositional PAP, the
problem of determining if a solution exists is P2 -complete. Computing a minimal diagnosis
is a search problem, and hence it is more difficult to pose a decision question for proving
complexity results. Consequently, one can just note that computing a diagnosis minimal
with respect to  /  requires O(log |COMPS|) calls to an NP oracle (Eiter & Gottlob,
1995), asking the oracle at each step if a diagnosis containing at most k faulty components
exists.
Results on abduction problems indicate that the task of approximate diagnosis is intractable. Roth (1996) has addressed the problems of abductive inference, and of approximating such inference. Roth focuses on counting the number of satisfying assignments for
a range of AI problems, including some instances of PAPs. In addition, Roth shows that
approximating the number of satisfying assignments for these problems is intractable.
Abdelbar (2004) has studied the complexity of approximating Horn abduction problems,
showing that even for a particular Horn restriction of the propositional problem of interest,
the approximation problem is intractable. In particular, for an abduction problem with
costs assigned to the assumables (which can be used to model the preference-ordering ),
377

fiFeldman, Provan, & van Gemund

he has examined the complexity of finding the Least Cost Proof (LCP) for the evidence
(OBS), where the cost of a proof is taken to be the sum of the costs of all hypotheses that
must be assumed in order to complete the proof. For this problem he has shown that it is
NP -hard to approximate an LCP within a fixed ratio r of the cost of an optimal solution,
for any r < 0.
Safari approximates the intractable problems denoted in Table 1. We show that for
WFM, Safari can efficiently compute a single diagnosis that is minimal under  by using
a satisfiability oracle. For SD  SFM, Safari generates a sound but possibly sub-optimal
diagnosis (or set of diagnoses). We have referred to papers indicating that it is intractable
to approximate, within a fixed ratio, a minimal diagnosis. In the following, we adopt a
stochastic approach that cannot provide fixed-ratio guarantees. However, Safari trades off
optimality for efficiency and can compute most diagnoses with high likelihood.

3. Stochastic MBD Algorithm
In this section we discuss an algorithm for computing multiple-fault diagnoses using stochastic search.
3.1 A Simple Example (Continued)
Consider the Boolean subtractor shown in Fig. 1, its weak-fault model SDw given by (1),
and the observation 4 from the preceding section. The four minimal diagnoses associated
to SDw 4 are: 1 = h1 h2 h3 h4 h5 h6 h7 , 2 = h1 h2 h3 h4 h5 h6 h7 ,
3 = h1  h2  . . .  h6  h7 , and 4 = h1  h2  h3  . . .  h6  h7 .
A nave deterministic algorithm would check the consistency of all the 2|COMPS| possible health assignments for a diagnostic problem, 128 in the case of our running example.
Furthermore, most deterministic algorithms first enumerate health assignments of small
cardinality but with high a priori probability, which renders these algorithms impractical in
situations when the minimal diagnosis is of a higher cardinality. Such performance is not
surprising even when using state-of-the art MBD algorithms which utilize, for example conflict learning, or partial compilation, considering the bad worst-case complexity of finding
all minimal diagnoses (cf. Sec. 2.4).
In what follows, we will show a two-step diagnostic process that requires fewer consistency checks. The first step involves finding a random non-minimal diagnosis as a starting
point (cf. Sec. 3.2 for details on computing random SAT solutions with equal likelihood).
The second step attempts to minimize the fault cardinality of this diagnosis by repeated
modification of the diagnosis.
The first step is to find one random, possibly non-minimal diagnosis of SDw  4 . Such
a diagnosis we can obtain from a classical DPLL solver after modifying it in two ways: (1)
not only determine if the instance is satisfiable but also extract the satisfying solution and
(2) find a random satisfiable solution every time the solver is invoked. Both modifications
are trivial, as DPLL solvers typically store their current variable assignments and it is easy
to choose a variable and value randomly (according to a uniform distribution) instead of
deterministically when branching. The latter modification may possibly harm a DPLL
variable or value selection heuristics, but later in this paper we will see that this is of no
378

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

concern for the type of problems we are considering as diagnostic systems are typically
underconstrained.
In the subtractor example we call the DPLL solver with SDw  4 as an input and we
consider the random solution (and obviously a diagnosis) 5 = h1  h2  h3  h4  h5 
h6  h7 (|5 | = 4). In the second step of our stochastic algorithm, we will try to minimize
5 by repetitively choosing a random negative literal, flipping its value to positive (thus
obtaining a candidate with a smaller number of faults), and calling the DPLL solver. If the
new candidate is a diagnosis, we will try to improve further this newly discovered diagnosis,
otherwise we will mark the attempt a failure and choose another negative literal. After
some constant number of failures (two in this example), we will terminate the search and
will store the best diagnosis discovered so far in the process.
After changing the sign of h7 in 5 we discover that the new health assignment is
not consistent with SDw  4 , hence it is not a diagnosis and we discard it. Instead,
the algorithm attempts changing h6 to h6 in 5 , this time successfully obtaining a new
diagnosis 6 = h1  h2  h3  h4  h5  h6  h7 of cardinality 3. Next the algorithm
tries to find a diagnosis of even smaller cardinality by randomly choosing h1 and h7 in
6 , respectively, and trying to change their sign, but both attempts return an inconsistency.
Hence the climb is aborted and 6 is stored as the current best diagnosis.
Repeating the process from another random initial DPLL solution, gives us a new diagnosis 7 = h1  h2  h3  h4  h5  h6  h7 . Changing the sign of h7 , again, leads
to inconsistency, but the next two flips (of h4 and h2 ) lead to a double-fault diagnosis
8 = h1  h2  . . .  h6  h7 . The diagnosis 8 can not be improved any further as it is
minimal. Hence the next two attempts to improve 8 fail and 8 is stored in the result.
This process is illustrated in Fig. 2, the search for 6 is on the left and for 8 on the right.
Gates which are shown in solid black are suspected as faulty when the health assignment
they participate in is tested for consistency, and inconsistent candidates are crossed-out.
Let us consider the result. We have found two diagnoses: 6 and 8 , where 6 is not
a minimal diagnosis. This we have done at the price of 11 calls to a DPLL subroutine.
The suboptimal diagnosis 6 is of value as its cardinality is near the one of a minimal
diagnosis. Hence we have demonstrated a way to find an approximation of all minimal
diagnoses, while drastically reducing the number of consistency checks in comparison to a
deterministic algorithm, sacrificing optimality. Next we will formalize our experience into
an algorithm, the behavior of which we will analyze extensively in the section that follows.
Diagnosing a strong-fault model is known to be strictly more difficult than a weak-fault
model (Friedrich et al., 1990). In many diagnostic instances this problem is alleviated by
the fact that there exist, although without a guarantee, continuities in the diagnostic search
space similar to the one in the weak-fault models. Let us discuss the process of finding a
minimal diagnosis of the subtractors strong-fault model SDs and the observation 2 (both
from Sec. 2.1).
The six distinct diagnoses 9 , . . . , 14 of SDs and 2 are shown in Fig. 3. Of these only
9 and 10 are minimal such that |9 | = |10 | = 3. It is visible in Fig. 3 that in all diagnoses
component variables h2 and h5 are false, while h1 and h7 are true (healthy). Hence, any
satisfying assignment of SDs  2 would contain h1  h2  h5  h7 . Starting from the
maximal-cardinality diagnosis 14 , we must flip the variables h3 , h4 , and h6 in order to
reach the two minimal diagnoses. The key insight is that, as shown in Fig. 3, this is always
379

fiFeldman, Provan, & van Gemund

Figure 2: An example of a stochastic diagnostic process
h2
9 
13 
14 
h2
9 
11 
14 

h5

h4

h6

h3

h1

h7

























h5

h4

h6

h3

h1

h7

























h2
10 
12 
14 
h2
10 
13 
14 

h5

h4

h6

h3

h1

h7

























h5

h4

h6

h3

h1

h7

























Figure 3: Diagnoses of a strong-fault model
possible by flipping a single literal at a time from health to faulty and receiving another
consistent assignment (diagnosis).
In what follows we will formalize our experience so far in a stochastic algorithm for
finding minimal diagnoses.
3.2 A Greedy Stochastic Algorithm
Algorithm 1 shows the pseudocode of Safari.
380

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

Algorithm 1 Safari: A greedy stochastic hill climbing algorithm for approximating the
set of minimal diagnoses
1: function Safari(DS, , M, N ) returns a trie
inputs: DS = hSD, COMPS, OBSi, diagnostic system
, term, observation
M , integer, climb restart limit
N , integer, number of tries
local variables: SDcnf , CNF
m, n, integers
,   , terms
R, set of terms, result
2:
SDcnf  WffToCNF(SD)
3:
for n = 1, 2, . . . , N do
4:
  RandomDiagnosis(SDcnf , )
 Get a random SAT solution.
5:
m0
6:
while m < M do
7:
   ImproveDiagnosis()
 Flip an unflipped health variable.

8:
if SDcnf     6|= then
 Consistency check.
9:
  
10:
m0
11:
else
12:
mm+1
13:
end if
14:
end while
15:
unless IsSubsumed(R, ) then
16:
AddToTrie(R, )
17:
RemoveSubsumed(R, )
18:
end unless
19:
end for
20:
return R
21: end function

Safari accepts two input parameters: M and N . There are N independent searches
that start from randomly generated starting points. The algorithm tries to improve the
cardinality of the initial diagnoses (while preserving their consistency) by randomly flipping fault literals. The change of a sign of literal is done in one direction only: from faulty
to healthy. Each attempt to find a minimal diagnosis terminates after M unsuccessful attempts to improve the current diagnosis stored in . Thus, increasing M will lead to a
better exploration of the search space and, possibly, to diagnoses of lower cardinality, while
decreasing it will improve the overall speed of the algorithm.
Safari uses a number of utility functions. WffToCNF converts the propositional
formula in SD to CNF (cf. Sec 2.3). The ImproveDiagnosis subroutine takes a term  as
an argument and changes the sign of a random negative literal in . If there are no negative
literals, the function returns its original argument.
381

fiFeldman, Provan, & van Gemund

The implementation of RandomDiagnosis uses a modified DPLL solver returning a
random SAT solution of SD  . Consider the original DPLL algorithm (Davis, Logemann,
& Loveland, 1962) without the unit resolution rule. One can show that if, in the event
of branching, the algorithm chooses unassigned variables and their polarity with equal
probability, the DPLL algorithm is equally likely to compute any satisfiable solution (if such
exists). Note that the order in which variables are assigned does not matter. Of course,
the DPLL algorithm may end-up with a partial assignment, i.e., some of the variables are
dont care. This is not a problem because the partial assignment can be extended to
a full satisfiable assignment by randomly choosing the signs of the unassigned variables
from a uniform distribution. Taking into consideration the unit resolution rule, does not
change the likelihood of the modified DPLL solver finding a particular solution because it
only changes the order in which variables are assigned. A formal proof that this modified
DPLL solver computes a SAT assignment with equal probability is beyond the scope of this
paper, but the idea is to build a probabilistic model of the progress of the DPLL solver. This
probabilistic model is a balanced tree where nodes iterate between branching and performing
unit resolution (assigning values to zero or more unit clauses). As the branching probability
is set to be equal and all leaf nodes (SAT solutions) are at equal depth, one can show the
equal likelihood of arriving to any SAT solution. As most up-to-date SAT solvers are based
on DPLL, creating a randomized DPLL solver that computes any satisfiable solution with
equal probability is not difficult. Of course, random polarity decisions may effect negatively
branching heuristics (Marques-Silva, 1999) but such analysis is also beyond the scope of
this paper.
Similar to deterministic methods for MBD, Safari uses a SAT-based procedure for
checking the consistency of SD. To increase the implementation efficiency of Safari,
we combine a BCP-based LTMS engine (McAllester, 1990) and a full-fledged DPLL solver in
two-stage consistency checking. Experimentation shows that combining LTMS and DPLL
in such a way allows an order-of-magnitude Safari speed-up compared to pure DPLL, while
the soundness and completeness properties of consistency checking are preserved.
We have implemented the two-stage consistency checking as follows. First, Safari calls
a BCP-based LTMS (Forbus & de Kleer, 1993) to check if SD     |=. If the result
is UNSAT then the candidate  is not a diagnosis.4 If the LTMS result is not UNSAT, it
means that the consistency of the candidate is unknown and a call to a complete DPLL
engine is needed. For the full DPLL checking we use POSIT (Freeman, 1995) or MiniSat
(Een & Sorensson, 2003).
Safari benefits from the two-stage SAT procedure because a typical MBD instance
involves many consistency checks (O(|COMPS|2 ) for N = 1, M = |COMPS|). As SD  
does not change during the search and each time only a small number of assumption clauses
have to be updated, the incremental nature of LTMS greatly improves the search efficiency.
Even though the DPLL running time per instance is the same as LTMS (DPLL performs
BCP when doing unit propagation), DPLL construction is expensive and should be avoided
when possible. DPLL initialization is typically slow as it involves building data structures
for clauses and variables, counting literals, initializing conflict databases, etc. On the other
hand, our implementation of LTMS is both incremental (does not have to be reinitialized
4. It can be shown that if a BCP consistency check of SD     returns UNSAT, then the formula is
UNSAT (the opposite is not necessarily true).

382

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

before each consistency check) and efficient as it maintains only counters for each clause.
Each counter keeps the number of unassigned literals. Assigning a value to a variable
requires decrementing some or all of the clause counters. If a counter becomes zero, a
contradiction handler is signaled.
There is no guarantee that two diagnostic searches, starting from random diagnoses,
would not lead to the same minimal diagnosis. To prevent this, we store the generated
diagnoses in a trie R (Forbus & de Kleer, 1993), from which it is straightforward to extract
the resulting diagnoses by recursively visiting its nodes. A diagnosis  is added to the trie R
by the function AddToTrie, iff no subsuming diagnosis is contained in R (the IsSubsumed
subroutine checks on that condition). After adding a diagnosis  to the resulting trie R, all
diagnoses contained in R and subsumed by  are removed by a call to RemoveSubsumed.
3.3 Basic Properties of the Greedy Stochastic Search
Before we continue with the topics of completeness and optimality, we show that Safari is
sound, i.e., it returns diagnoses only.
Lemma 2 (Soundness). Safari is sound.
Proof (Sketch). The consistency check in line 8 of Alg. 1 guarantees that only terms  for
which it holds that SD     6|= will be added to the result set R. According to Def. 5
these terms  are diagnoses.
One of the key factors in the success of the proposed algorithm is the exploitation of the
continuity of the search-space of diagnosis models, where by continuity we mean that we
can monotonically reduce the cardinality of a non-minimal diagnosis. Through the exploitation of this continuity property, Safari can be configured to guarantee finding a minimal
diagnosis in weak fault models in a polynomial number of calls to a satisfiability oracle.
The hypothesis which comes next is well studied in prior work (de Kleer et al., 1992),
as it determines the conditions under which minimal diagnoses represent all diagnoses of a
model and an observation. This paper is interested in the hypothesis from the computational
viewpoint: it defines a class of models for which it is possible to establish a theoretical bound
on the optimality and performance of Safari.
Hypothesis 1 (Minimal Diagnosis Hypothesis). Let DS = hSD, COMPS, OBSi be a diagnostic system and   a diagnosis for an arbitrary observation . The Minimal Diagnosis Hypothesis (MDH) holds in DS iff for any health assignment  such that Lit  ()  Lit  (  ),
 is also a diagnosis.
It is easy to show that MDH holds for all weak-fault models. There are other theories
SD 6 WFM for which MDH holds (e.g., one can directly construct a theory as a conjunction
of terms for which MDH holds). Unfortunately, no necessary condition is known for MDH
to hold in an arbitrary SD. The lemma which comes next is a direct consequence of MDH
and weak-fault models.
Lemma 3. Given a diagnostic system DS = hSD, COMPS, OBSi, SD  WFM, and a
diagnosis  for some observation , it follows that  is non-minimal iff another diagnosis
  can be obtained by changing the sign of exactly one negative literal in .
383

fiFeldman, Provan, & van Gemund

Proof (Sketch). From Def. 2 and SD  WFM, it follows that if  is a minimal diagnosis,
any diagnosis   obtained by flipping one positive literal in  is also a diagnosis. Applying
the argument in the other direction gives us the above statement.
Safari operates by performing subset flips on non-minimal diagnoses, attempting to compute minimal diagnoses. We next formalize this notion of flips, in order to characterize
when Safari will be able to compute a minimal diagnosis.
Definition 11 (Subset Flip  ). Given a diagnostic system DS = hSD, COMPS, OBSi and
a health assignment  with a non-empty set of negative literals (Lit  () 6= ), a subset
flip  turns one of the negative literals in  to a positive literal, i.e., it creates a health
assignment   with one more positive literal.
We next characterize flips based on whether they produce consistent models after the flip.
Definition 12 (Valid Subset Flip). Given a diagnostic system DS = hSD, COMPS, OBSi,
an observation , and a non-minimal diagnosis , a valid flip exists if we can perform a
subset flip in  to create   such that SD      6|=.
Given these notions, we can define continuity of the diagnosis search space in terms of literal
flipping.
Definition 13 (Continuity). A system model SD and an observation  satisfy the continuity
property with respect to the set of diagnoses  (SD), iff for any diagnosis k  (SD)
there exists a sequence  = h1 , 2 ,    , k1 , k , k+1 ,    , n i, such that for i =
1, 2,    , n  1, it is possible to go from i to i+1 via a valid subset flip, i  (SD  ),
and n   (SD  ).
The above definition allows for trivial continuity in the cases when a model and an observation lead to minimal diagnoses only (no non-minimal diagnoses). As we will see in Sec. 6,
models and observations such that all diagnoses are minimal are rare in practice (of course,
such problems can be created artificially). Note that the Safari algorithm still works and
its theoretical properties are preserved even in the case of trivial continuity.
Given Def. 13, we can easily show the following two lemmata:
Lemma 4. If SD satisfies MDH, then it satisfies the continuity property.
Proof. Follows directly from Hypothesis 1 and Def 13.
Lemma 5. SD  WFM satisfies the continuity property.
Proof (Sketch). It is straightforward to show that if SD  WFM then SD satisfies MDH.
Then from Lemma 4 it follows that SD satisfies the continuous property.
Our greedy algorithm starts with an initial diagnosis and then randomly flips faulty assumable variables. We now use the MDH property to show that, starting with a non-minimal
diagnosis , the greedy stochastic diagnosis algorithm can monotonically reduce the size of
the seed diagnosis to obtain a minimal diagnosis through appropriately flipping a fault
variable from faulty to healthy; if we view this flipping as search, then this search is continuous in the diagnosis space.
384

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

Proposition 1. Given a diagnostic system DS = hSD, COMPS, OBSi, an observation ,
and SD  WFM, Safari configured with M = |COMPS| and N = 1 returns one minimal
diagnosis.
Proof. The diagnosis improvement loop starts, in the worst case, from a health assignment
 which is a conjunction of negative literals only. Necessarily, in this case,  is a diagnosis
as SD  WFM. A diagnosis   that is subsumed by  would be found with at most M
consistency checks (provided that   exists) as M is set to be equal to the number of literals
in  and there are no repetitions in randomly choosing of which literal to flip next. If, after
trying all the negative literals in , there is no diagnosis, then from Lemma 3 it follows that
 is a minimal diagnosis.
Through a simple inductive argument, we can continue this process until we obtain a
minimal diagnosis.
From Proposition 1 it follows that there is an upper bound of O(|COMPS|) consistency
checks for finding a single minimal diagnosis. In most of the practical cases, however, we
are interested in finding an approximation to all minimal-cardinality diagnoses. As a result
the complexity of the optimally configured Safari algorithm becomes O(|COMPS|2 S),
where S is the number of minimal-cardinality diagnoses for the given observation. Section 5
discusses in more detail the computation of multiple minimal-cardinality diagnoses.
The number of assumable variables in a system of practical significance may exceed
thousands, rendering an optimally configured Safari computationally too expensive. In
Sec 4 we will see that while it is more computationally efficient to configure M < |COMPS|,
it is still possible to find a minimal diagnosis with high probability.
It is simple to show that flip-based search algorithms are complete for continuous diagnosis search spaces given weak fault models, i.e., SD  WFM, and models that follow
MDH, i.e., Lemma 3. We can formally characterize the guarantee of finding a minimal diagnosis with Safari in terms of a continuous diagnosis space. Note that this is a sufficient,
but not necessary, condition; for example, we may configure Safari to flip multiple literals
at a time to circumvent problems of getting trapped in discontinuous diagnosis spaces.
Theorem 1. Given a diagnostic system DS = hSD, COMPS, OBSi, and a starting diagnosis
, Safari configured with M = |COMPS| and N = 1 is guaranteed to compute a minimal
diagnosis if the diagnosis space is continuous.
Proof. Given an initial diagnosis , Safari attempts to compute a minimal diagnosis by
performing subset flips. If the diagnosis space is continuous, then we know that there exists
a sequence of valid flips leading to a minimal diagnosis. Hence Safari is guaranteed to find
a minimal diagnosis from .
Finally, we show that Safari provides a strong probabilistic guarantee of computing all
minimal diagnoses.
Theorem 2. The probability of Safari, configured with M = |COMPS|, of computing all
minimal diagnoses of a diagnostic system DS = hSD, COMPS, OBSi and an observation 
is denoted as Pr . Given a continuous diagnosis space (SD, ), it holds that Pr  1 for
N  .
385

fiFeldman, Provan, & van Gemund

Proof (Sketch). Since (1) the search space is continuous, (2) at each step there is a non-zero
probability of flipping any unflipped literal, and (3) there is a polynomial upper bound of
steps (|COMPS|) for computing a diagnosis, Safari can compute any non-minimal diagnosis
with non-zero probability. Hence as N  , Safari will compute all minimal diagnoses.

3.4 Complexity of Inference Using Greedy Stochastic Search
We next look at the complexity of Safari, and its stochastic approach to computing sound
but incomplete diagnoses. We show that the primary determinant of the inference complexity is the consistency checking. Safari randomly computes a partial assignment , and
then checks if  can be extended to create a satisfying assignment during each consistency
check, i.e., it checks the consistency of  with SD. This is solving the satisfiability problem (SAT), which is NP-complete (Cook, 1971). We will show how we can use incomplete
satisfiability checking to reduce this complexity, at the cost of completeness guarantees.
In the following, we call  the complexity of a consistency check, and assume that there
are  components that can fail, i.e.,  = |COMPS|.
Lemma 6. Given a diagnostic system DS = hSD, COMPS, OBSi with SD  WFM, the
worst-case complexity of finding any minimal diagnosis is O( 2 ), where  is the cost of a
consistency check.
Proof. There is an upper bound of  succeeding consistency checks for finding a single
minimal diagnosis since there is a maximum of  steps for computing the all healthy
diagnosis. As Safari performs a consistency check after each flip and at each step the
algorithm must flip at most  literals, the total complexity is O( 2 ).
In most practical cases, however, we are interested in finding an approximation to all minimal-cardinality diagnoses.
As 
a result the complexity of the optimally configured Safari


||
algorithm becomes O    , where || is the cardinality of the minimal-cardinality
diagnoses for the given observation (cf. Sec. 6.6).
The complexity of BCP is well-known, allowing us to get more precise bounds on the
worst-case complexity of computing one minimal-diagnosis with Safari. In what follows
we will assume that SD is represented in CNF (cf. Sec. 2.3).
Lemma 7. Given a diagnostic system DS = hSD, COMPS, OBSi, SD  WFM, and SD
having c clauses and n variables, the worst-case complexity under WFM of finding any
minimal diagnosis is O( 2 cn) when using BCP for consistency checks.5
Proof (Sketch). An implementation of BCP (Forbus & de Kleer, 1993) maintains a total of
c counters for the number of unsatisfied literals in each clause. A consistency check requires
decrementing some or all counters for each of the n variables in SD. This gives us an upper
bound of O(cn) on the execution time of BCP. Combining the complexity of BCP with
Lemma 6 gives us the desired result.
5. More efficient implementations of BCP exist (Zhang & Stickel, 1996).

386

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

4. Optimality Analysis (Single Diagnosis)
In contrast to deterministic algorithms, in the Safari algorithm there is no absolute guarantee that the optimum solution (minimal diagnosis) is found. Below we will provide an
intuition behind the performance of the Safari algorithm by means of an approximate,
analytical model that estimates the probability of reaching a diagnostic solution of specific
minimality.
4.1 Optimality of Safari in Weak-Fault Models
We will start by considering a single run of the algorithm without retries where we will
assume the existence of only one minimal diagnosis. Next, we will extend the model by
considering retries.
4.1.1 Basic Model
Consider a diagnostic system DS = hSD, COMPS, OBSi such that SD  WFM, and an
observation  such that  manifests only one minimal diagnosis . For the argument that
follows we will configure Safari with M = 1, N = 1, and we will assume that the starting
solution is the trivial all faulty diagnosis.
When Safari randomly chooses a faulty variable and flips it, we will be saying that it
is a success if the new candidate is a diagnosis, and a failure otherwise. Let k denote the
number of steps that the algorithm successfully traverses in the direction of the minimal
diagnosis of cardinality ||. Thus k also measures the number of variables whose values are
flipped from faulty to healthy in the process of climbing.
Let f (k) denote the probability distribution function (pdf) of k. In the following we
derive the probability p(k) of successfully making a transition from k to k + 1. A diagnosis
at step k has k positive literals and |COMPS|  k negative literals. The probability of the
next variable flip being successful equals the probability that the next negative to positive
flip out of the H  k negative literals does not conflict with a negative literal belonging to
a diagnosis solution . Consequently, of the ||  k literals only COMPS|  ||  k literals
are allowed to flip, and therefore the success probability equals:
p (k) =

||
|COMPS|  ||  k
=1
|COMPS|  k
|COMPS|  k

(3)

The search process can be modeled in terms of the Markov chain depicted in Fig. 4, where
k equals the state of the algorithm. Running into an inconsistency is modeled by the
transitions to the state denoted fail.
The probability of exactly attaining step k (and subsequently failing) is given by:
f (k) = (1  p(k + 1))

k
Y

p(i)

(4)

i=0

Substituting (3) in (4) gives us the pdf of k:

k 
Y
||
||
1
f (k) =
|COMPS|  k + 1
|COMPS|  i
i=0

387

(5)

fiFeldman, Provan, & van Gemund

k=0

1  p(0)

p(0)

k=1

1  p(1)

p(1)

k=2

1  p(2)

p(i)

k=i

p(n  1)

1  p(i + 1)

k=n

1

fail

Figure 4: Model of a Safari run for M = 1 and a single diagnosis  (n = |COMPS|  ||)

At the optimum goal state k = |COMPS|  || the failure probability term in (5) is correct
as it equals unity.
If p were independent of k, f would be geometrically distributed, which implies that the
chance of reaching a goal state k = |COMPS||| is slim. However, the fact that p decreases
with k moves the probability mass to the tail of the distribution, which works in favor of
reaching higher-k solutions. For instance, for single-fault solutions (|| = 1) the distribution
becomes uniform. Figure 5 shows the pdf for problem instances with |COMPS| = 100 for
an increasing fault cardinality ||. In order to decrease sampling noise, the empirical f (k)
values in Fig. 5 are computed by taking the average over 10 samples of k.
0.1

0.1
|| = 1
|| = 5
|| = 10

0.08

|| = 1
|| = 5
|| = 10

0.08

f(k)

0.06

f(k)

0.06
0.04

0.04

0.02

0.02

0

0

20

40

60

80

100

k

0

0

20

40

60

80

100

k

Figure 5: Empirical (left) and analytic (right) f (k) for no retries and a single diagnosis
In the next section we show that retries will further move probability mass towards the
optimum, increasing the tail of the distribution, which is needed for (almost always) reaching
optimality.
4.1.2 Modeling Retries
In this section we extend the model to account for retries, which has a profound effect on
the resulting pdf of f . Again, consider the transition between step k and k + 1, where the
algorithm can spend up to m = 1, . . . , M retries before exiting with failure. As can be
388

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

seen by the algorithm (cf. Alg. 1), when a variable flip produces an inconsistency a retry is
executed while m is incremented.
From elementary combinatorics we can compute the probability of having a diagnosis
after flipping any of M different negative literals at step k. Similar to (3), at stage k there
are |COMPS|  k faulty literals from which M are chosen (as variable flips leading to
inconsistency are recorded and not attempted again, there is no difference between choosing
the M variables in advance or one after another). The probability of advancing from stage
k to stage k + 1 becomes:
||
p (k) = 1 

M
|COMPS|k
M

(6)

The progress of Safari can be modeled for values of M > 1 as a Markov chain, similar to
the one shown in Fig. 4 with the transition probability of p replaced by p . The resulting
pdf of the number of successful steps becomes:
#
"
k
||
||
Y

M
M
(7)
1  |COMPS|i
f (k) = |COMPS|k+1
M

i=0

M

It can be seen that (5) is a restricted case of (7) for M = 1.
The retry effect on the shape of the pdf is profound. Whereas for single-fault solutions
the shape for M = 0 is uniform, for M = 1 most of the probability mass is already located
at the optimum k = |COMPS|  ||. Fig. 6 plots f for a number of problem instances with
increasing M . As expected, the effect of M is extremely significant. Note that in case of
the real system, for M = |COMPS| the pdf would consist of a single, unit probability spike
at |COMPS|  ||.
Although we were unable to find an analytic treatment of the transition model above, the
graphs immediately show that for large M the probability of moving to k = |COMPS|  ||
is very large. Hence, we expect the pdf to have a considerable probability mass located at
k = |COMPS|  ||, depending on M relative to |COMPS|.
4.2 Optimality of Safari in Strong-Fault Models
From the above analysis we have seen that in WFM it is easy, starting from a non-minimal
diagnosis, to reach a subset minimal diagnosis. As will be discussed in more detail below,
this is not necessarily the case for strong-fault models. In many practical cases, however,
strong-fault models exhibit, at least partially, behavior similar to MDH, thus allowing greedy
algorithms like Safari to achieve results that are close to the optimal values.
4.2.1 Partial Continuity in Strong-Fault Stuck-At Models
In what follows we will restrict our attention to a large subclass of SFM, called SFSM
(Struss & Dressler, 1992).
Definition 14 (Strong-Fault Stuck-At Model). A system DS = hSD, COMPS, OBSi belongs to the class SFSM iff SD is equivalent to (h1  F1 )  (h1  l1 )      (hn 
Fn )  (hn  ln ) such that 1  i, j  n, {hi }  COMPS, Fj is a propositional formula,
none of hi appears in Fj , and lj is a positive or negative literal in Fj .
389

fiFeldman, Provan, & van Gemund

M=2

M=2

0.04

0.04
|| = 5
|| = 10

0.03
f(k)

f(k)

0.03

0.02

0.01

0

|| = 5
|| = 10

0.02

0.01

0

20

40

60

80

0

100

0

20

40

k
M=4

100

60

80

100

0.12
|| = 5
|| = 10

0.1

|| = 5
|| = 10

0.1
0.08
f(k)

0.08
f(k)

80

M=4

0.12

0.06

0.06

0.04

0.04

0.02

0.02

0

60
k

0

20

40

60

80

100

k

0

0

20

40
k

Figure 6: Empirical (left) and analytic (right) f  (k) for multiple retries and a single diagnosis

MDH (cf. Hypothesis 1) does not hold for SFSM models. Consider an adder whose inputs
and outputs are all zeroes, and whose gate models are all stuck-at-1 when faulty. In this
case, the all nominal assignment is a diagnosis, but, for example, a stuck-at-1 output gate
is not a diagnosis (there is a contradiction with the zero output).
Many practical observations involving SFSM models, however, lead to partial continuity. This means that there are groups of diagnoses that differ in at most one literal, i.e., a
flip based search can improve the cardinality of a diagnosis. We next formalize this notion.
Definition 15 (Partial Continuity). A system model SD and an observation  satisfy the
partial continuity property with respect to a set S  (SD  ), iff for every diagnosis 
such that i  S satisfying Lit  () \ Lit  (i ) there exists a finite sequence of valid subset
flips from i to .
At one extreme of the spectrum, SD and  satisfy the partial continuity property with
respect to the set of all of its diagnoses while at the other extreme, the partial continuity
390

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

property is satisfied with respect to a singleton S (consider, for example, SD  WFM
where S consists of the single all faulty diagnosis).
Note that the continuous property is trivally satisfied with respect to any diagnosis
k  (SD  ), i.e., there always exists a sequence containing k only ( = hk i). We are
only interested in the non-trivial cases, for which || > 1.
Consider a system SD and an observation  that satisfy the partial continuity property
with respect to some diagnosis k . We say that the diagnoses in the flip sequence  that
contains k form a continuous subspace. Alternatively, given a diagnostic system SD and an
observation , a continuous diagnostic subspace of SD is a set of diagnoses   (SD)
with the property that, for any diagnosis   , there is another diagnosis    such that
|Lit  ()|  |Lit  ()| = 1.
Unfortunately, in the general SFSM case, we cannot derive bounds for the sizes of the
continuous subspaces, and hence, for the optimality of Safari. In what follows, and with
the help of a few examples, we illustrate the fact that partial continuity depends on the
model and the observation and then we express the optimality of Safari as a function of this
topologically-dependent property. Later, in Sec. 6, we collect empirical data that continuous
subspaces leading to near-optimal diagnoses exist for a class of benchmark SFSM circuits.
Our first example illustrates the notion of discontinuity (lack of partial continuity with
respect to any diagnoses). We show a rare example of a model and and an observation
leading to a set of diagnoses that contains diagnoses of cardinality m and m + q (q > 1),
but has no diagnoses of cardinality m + 1, m + 2,    , m + q  1.
A Discontinuity Example Consider, for example, the Boolean circuit shown in Fig. 7
and modeled by the propositional formula:

[h1  (y  x)]  [h1  (y  x)]
(8)
SDd =
[h2  (y  x)]  [h2  (y  x)]
and an observation d = x  y. Note, that SDd 6 SFSM. There are exactly two diagnoses
of SDd  d : 15 = h1  h2 and 16 = h1  h2 . Note that this model cannot have single
faults. As only 15 is minimal, |15 | = 0, and |16 | = 2, if the algorithm starts from 16 it
is not possible to reach the minimal diagnosis 15 by performing single flips. Similarly we
can construct models which impose an arbitrarily bad bound on the optimality of Safari.
Such models, however, are not common and we will see that the greedy algorithm performs
well on a wide class of strong-fault models.
h1
x

y
h2

Figure 7: A two inverters circuit
Obviously, continuity in the distribution of the cardinalities in a set of diagnoses is a necessary (but not sufficient) condition for Safari to progress. Such models impose arbitrary
difficulty to Safari, leading to suboptimal diagnoses of any cardinality.
391

fiFeldman, Provan, & van Gemund

An Example of Partial Continuity We continue the running example started in Sec. 2.
First, we create a system description SDsa for a SFSM model. Let SDsa = SDw  SDf ,
where SDw is given by (1). The second part of SDsa , the strong fault description SDf ,
specifies that the output of a faulty gate must be stuck-at-1:
SDf = (h1  i)  (h2  d)  (h3  j)  (h4  m) 
 (h5  b)  (h6  l)  (h7  k)

(9)

It is clear that SDsa  SFSM. We next compute the diagnoses of SDsa  1 (1 = x  y 
p  b d). There is one minimal diagnosis of SDsa  1 and it is 5 = h1  h2  h3     h7
(cf. Fig. 8). If we choose the two literals h3 and h4 from 5 and change the signs of h3
and h4 , we create two new health assignments: 15 = h1  h2  h3  h4  h5  h6  h7
and 16 = h1  h2  h3  h4  h5  h6  h7 . It can be checked that both 15 and 16 are
diagnoses, i.e., SDsa  1  15 6|= and SDsa  1  16 6|=. Note that 15 and 16 are
diagnoses of the weak-part of the model, i.e., {15 , 16 }  (SDw  1 ). This follows from
MDH and the fact that 5 is a minimal diagnosis of SDw  1 . Furthermore, 15 is also a
diagnosis in the strong-fault stuck-at model (15  (SDsa  1 )) because SDw  1  h3
does not lead to a contradictory value for j in the strong-fault part SDf . A similar argument
applies to 16 : SDw  1  h4 does not contradict m in SDf . Equivalently, if negating h3
in 5 , which makes j stuck-at-1, results in a diagnosis, and negating h4 in 5 , which makes
m stuck-at-1, also results in a diagnosis, negating both h3 and h4 in 5 will also result in
a diagnosis (consider the fact that the fault mode of h4 sets m only, but does not impose
constraints on j). The above argument can be extended similarly to h5 , h6 , and h7 . Hence,
any assignment of COMPS containing h1  h2 is a diagnosis of SDsa  1 , no matter what
combination of signs we take for h3 , h4 , h5 , h6 , and h7 . Note that a health assignment
containing h4 is a diagnosis conditioned on k = 1.
x=1
y=1
p=1

h2

d=0

h6

h1
i=1

l=0
h4

h3

m=0

j=1

h5

b=1

h7
k=1

Figure 8: Continuous subspace in a strong-fault, stuck-at-1 model of a subtractor
Consider an alternative way of computing a set of ambiguous diagnoses of SDsa  1 . Given
SDsa 1 5 , we can compute a consistent assignment to all internal variables (for example
by propagation). There is exactly one such assignment  and it is  = i  j  k  l  m,
SDsa  1  5   6|= (cf. Fig. 8). Note that for components h1 , h3 , h5 , and h7 , a
change in the state of a component (healthy or faulty) does not lead to a different output
value. For example the output j of the h3 or-gate is 1 because the gate is healthy and its
392

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

inputs are 1 but j would also be 1 for a stuck-at-1 or-gate (h3 ). As a result, no diagnostic
reasoner can determine if the components in the dashed region of Fig. 8 are healthy or faulty
(stuck-at-1). Equivalently, one can change the signs of h3 , h5 , and h7 in the diagnosis 5
and the resulting assignments are still diagnoses. We call the set of components modeled
by h1 , h3 , h5 , and h7 an ambiguity group. Clearly, Safari can start from a diagnosis
17 = h1  h2  h3  h4  h5  h6  h7 (|17 | = 4) and reach 5 (|5 | = 1) by
performing valid subset flips.
To make our reasoning precise, we restrict the class of SFSM models to exclude malformed circuits such as ones having disconnected inputs or outputs, etc. Furthermore, we
assume that each component has exactly one output (the set of all component output variables is denoted as COUT). The latter is not a big restriction as multi-output component
models can be replaced by multiple components, each having a single output.6
Definition 16 (Well-Formed Diagnostic System (Wfds)). The diagnostic system DS =
hSD, COMPS, OBSi is well-formed (DS  Wfds) iff for any observation  and for any
diagnosis   (SD  ), there is exactly one assignment  to all component outputs
COUT such that SD       6|=.
Consider an SFSM model SD = (h1  F1 )  (h1  l1 )      (hn  Fn )  (hn  ln ).
We denote as COMPS the set of those hi (1  i  n) for which the respective li literals are
negative (cf. Def. 14), i.e., COMPS is the set of components whose failure modes are stuckat-0. Similarly, we use COMPS+ for the set of component variables whose stuck-at li literals
are positive (COMPS  COMPS+ = COMPS, COMPS  COMPS+ = ). In a Wfds,
an observation  and a diagnosis  force the output of each component either to a negative
or to a positive value. We denote the set of health variables whose respective component
outputs are forced to negative values as G (DS, , ). Similarly, we have G+ (DS, , ) for
the components whose outputs have positive values. With all this we can define the notion
of a component ambiguity group.
Definition 17 (Component Ambiguity Group). Given a system DS = hSD, COMPS, OBSi,
SD  SFSM, SD  Wfds, an observation , and a diagnosis   (SD), the component
ambiguity group U(DS, , ), U  COMPS, is defined as U(DS, , ) = {G (DS, , ) 
COMPS }  {G+ (DS, , )  COMPS+ }.
Finally, we show that a component ambiguity group leads to a continuous subspace. In
the general case we cannot say much about the size of the component ambiguity groups.
From experimentation, we have noticed that it is difficult to assign the inputs of an SFSM
to values that generate small continuous subspaces (either SD   |=, or SD   leads
to large component ambiguity groups). Of course, it is possible to consider an adder, or a
multiplier, for example, whose inputs are all zeroes and whose gate models are all stuck-at-1
when faulty, but the number of such inputs/circuit combinations is small.
Proposition 2. A diagnostic system SD, SD  SFSM, SD  Wfds, and an observation
 entail continuous diagnostic subspaces.
6. Any multi-output Boolean function can be replaced by a composition of single-output Boolean functions.

393

fiFeldman, Provan, & van Gemund

Proof. From Def. 16 and the fact that SD  Wfds it follows that the output values of a
subset of the components have the same sign as the models stuck-at value. We denote this
set as COMPS , COMPS  COMPS. Any health assignment  that differs only in signs
of components belonging to COMPS is also a diagnosis. If the set of diagnoses of SD  
contains all possible assignments to the assumables in COMPS then those diagnoses form
a continuous space (cf. Def. 17).
To best illustrate Proposition 2, consider the or-gate modeled by h3 in Fig. 8. Its output is
1 either because the gate is healthy and one of the gates inputs is 1, or because the gate is
stuck-at-1. In this situation, it is not possible to determine if the component is healthy or
faulty.
Clearly, |U(DS, , )| is a lower bound for the progress of Safari in stuck-at models.
It can be shown that if Safari starts from a diagnosis  of maximum cardinality for the
given subspace, Safari is guaranteed (for M = |COMPS|) to improve the cardinality of
 by at least |U(DS, , )|. In practice, Safari can proceed even further as the stuckat ambiguity groups are only one factor of diagnostic uncertainty. A stuck-at component
effectively disconnects inputs from outputs, hence gates from the fan-in region are not
constrained. For instance, continuing our example, for h5 , all predecessors in the cone of
h5 (components h3 , h4 , h5 , h6 , and h7 ) constitute a continuous health subspace.
Contrary to a component ambiguity group, this set is conditional on the health state of
another component. A thorough study of stuck-at continuity is outside the scope of this
paper but as we shall see in Sec. 6, continuous subspaces justify Safari experiments on
stuck-at models.
4.2.2 Performance Modeling with Stuck-At Models
To further study the optimality of Safari in strong-fault models, we first define a case
in which the algorithm cannot improve a non-minimal diagnosis by changing the sign of a
faulty literal. Note that the existence of such cases is not a sufficient condition for Safari
to be suboptimal, as it is possible to reach a minimal diagnosis by first changing the sign
of some other faulty literal, thus circumventing the missing diagnosis.
From the preceding section we know that the number of invalid flips does not depend
on k, i.e., it is determined by the observation vector and the fault modes. The probability
of Safari to progress from any non-minimal diagnosis becomes
p(k) = 1 

||+|X|
M
|COMPS|k
M

(10)

where |X| is the number of invalid flips. The ratio of the number of invalid flips |X| to
|COMPS| we will call SFM density d. The density d gives the average probability of trying
an invalid flip throughout the diagnostic search. An approximation of the probability of
success of Safari is:
p(k) = 1 

||
M
|COMPS|k
M

394

d

(11)

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

Plugging p into (4) allows us to predict f (k) for the SFM models for which our assumptions
hold. This pdf, both measured from an implementation of Safari and generated from (4)
and (11) is shown in Fig. 9 for different values of the density d.
M = 4, |COMPS| = 100, || = 10

M = 4, |COMPS| = 100, || = 10

0.12

0.12
d=0
d = 0.1
d = 0.25
d = 0.5

0.1

0.08
f(k)

f(k)

0.08
0.06

0.06

0.04

0.04

0.02

0.02

0

0

20

40

60

d=0
d = 0.1
d = 0.25
d = 0.5

0.1

80

100

k

0

0

20

40

60

80

100

k

Figure 9: Empirical (left) and analytic (right) f  (k) for various diagnostic densities, multiple
retries and a single diagnosis

From Fig. 9 it is visible that increasing the density d leads to a shift of the probability
density of the length of the walk k to the left. The effect, however, is not that profound
even for large values of d, and is easily compensated by increasing M , as discussed in the
preceding sections.
It is interesting to note that bounds on d can be computed from SD (independent of ),
and these bounds can be used to further improve the performance of Safari.
4.3 Validation
In the preceding sections we have illustrated the progress of Safari with synthetic circuits
exposing specific behavior (diagnoses). In the remainder of this section we will plot the pdf
of the greedy search on one of the small benchmark circuits (for more information on the
74181 model cf. Sec. 6).
The progress of Safari with a weak-fault model of the 74181 circuit is shown in Fig. 10.
We have chosen a difficult observation leading to a minimal diagnosis of cardinality 7 (left)
and an easy observation leading to a single fault diagnosis (right). Both plots show that
the probability mass shifts to the right when increasing M and the effect is more profound
for the smaller cardinality.
The effect of the stuck-at-0 and stuck-at-1 fault modes (SFM) on the probability of
success of Safari is shown in Fig. 11.
Obviously, in this case the effect of increasing M is smaller, although still depending
on the difficulty of the observation vector. Last, even for small values of M , the absolute
probability of Safari finding a minimal diagnosis is sizeable, allowing the use of Safari
395

fiFeldman, Provan, & van Gemund

74181, || = 7

74181, || = 1

0.5

0.5
M=1

M=1

M=2

0.4

M=2

0.4

M=3
0.3

M=3
0.3

0.2

0.2

0.1

0.1

0

0

10

20

M=4

f(k)

f(k)

M=4

30
k

40

50

0
30

60

40

50
k

60

70

Figure 10: Empirical f  (k) for a weak-fault model of the 74181 circuit with observations
leading to two different minimal-cardinality diagnoses and various M

74181, || = 6, S-A-0

74181, || = 6, S-A-1

0.2

0.2
M=1

0.15

M=4
0.1

0.05

0
20

M=2

0.15

M=3
f(k)

f(k)

M=1

M=2

M=3
M=4

0.1

0.05

30

40
k

50

60

0
10

20

30

40

50

60

k

Figure 11: Empirical f  (k) for stuck-at-0 and stuck-at-1 strong-fault models of the 74181
circuit with various M

as a practical anytime algorithm which always returns a diagnosis, the optimality of which
depends on the time allocated to its computation.

5. Optimality Analysis (Multiple Diagnoses)
The preceding section described the process of computing one diagnosis with Safari (N =
1). In this section we discuss the use of Safari in computing (or counting) all minimalcardinality diagnoses (N > 1). For the rest of the section we will assume that Safari is
configured with M = |COMPS|.
396

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

Consider a system description SD (SD  WFM) and an observation . The number
of minimal diagnoses | (SD  )| can be exponential in |COMPS|. Furthermore, in practice, diagnosticians are interested in sampling from the set of minimal-cardinality diagnoses
 (SD  ) (recall that  (SD  )   (SD  )) as the minimal-cardinality diagnoses
cover a significant part of the a posteriori diagnosis probability space (de Kleer, 1990). In
what follows, we will see that Safari is very well suited for that task.
Theorem 3. The probability of Safari configured with M = |COMPS| computing a minimal diagnosis of cardinality || in a system with |COMPS| component variables approaches
|COMPS||| for |COMPS|/||  .
Proof (Sketch). Assume a minimal diagnosis of cardinality || exists. From Proposition 1
it follows that Safari configured with M = |COMPS| is guaranteed to compute minimal
diagnoses. Starting from the all faulty assignment, consider a step k in improving the
diagnosis cardinality. If state k contains more than one diagnosis, then at state k+1, Safari
will either (1) flip a literal belonging to this diagnosis (note that a literal may belong to
more than one diagnosis) and subsequently prevent Safari from reaching this diagnosis or
(2) flip a literal belonging to a diagnosis which has already been invalidated (i.e., one or
more of its literals have been flipped at an earlier step).
The probability that a solution of cardinality || survives a flip at iteration k (i.e., is
not invalidated) is:
p (k) = 1 

|COMPS|  ||  k
||
=
|COMPS|  k
|COMPS|  k

(12)

Similarly to our basic model (Sec. 4.1.1), the probability that a diagnosis  survives until
it is returned by the algorithm:
|COMPS|||1

Y

f (|COMPS|  ||  1) =

|COMPS|||1

p(i) =

i=0

Y
i=0

|COMPS|  ||  i
|COMPS|  i

(13)

Rewriting the right hand side of Eq. (13) gives us:
f (|COMPS|  ||  1) =

(|COMPS|  ||)!
||!(|COMPS|  ||)!
=
(|| + 1)(|| + 2)    |COMPS|
|COMPS|!

(14)

Since
1
(|COMPS|  ||)!
=
|COMPS|!
(|COMPS|  || + 1)(|COMPS|  || + 2)    |COMPS|

(15)

it holds that
(|COMPS|  ||)!
= |COMPS|||
|COMPS|!
|COMPS|/||
lim

(16)

As a result, for small || relative to |COMPS|,
f (|COMPS|  ||  1) = ||!|COMPS|||
which gives us the above theorem.
397

(17)

fiFeldman, Provan, & van Gemund

The distribution hi (||) of the cardinalities of the minimal diagnoses in  (SD ) depends
on the topology of SD and on ; i.e., we can create SD and  having any hi (||). We denote
the cardinality distribution of the minimal diagnoses computed by Safari as h(||).
Theorem 3 gives us a termination criterion for Safari which can be used for enumerating
and counting minimal-cardinality diagnoses. Instead of running Safari with
P a fixed N , it
is sufficient to compute the area under the output distribution function
h. This value
P
will converge to a single value, hence we can terminate Safari after the change of
h
drops below a fixed threshold. Note that Safari is efficient in enumerating the minimalcardinality diagnoses, as they are computed with a probability that is exponentially higher
than that of the probability of computing minimal diagnoses of higher-cardinality, as shown
in Theorem 3.
Corollary 1. Safari computes diagnoses of equal cardinality with equal probability.
Proof (Sketch). From Theorem 3 it follows that the probability of success f of Safari in
computing a diagnosis  depends only on || and not on the actual composition of .
The above corollary gives us a simple termination criterion for Safari in the cases when
all minimal diagnoses are also minimal-cardinality diagnoses; it can be proven that in this
case all minimal-cardinality diagnoses are computed with the same probability.
We will see that, given an input cardinality distribution hi (||), Safari produces an
output distribution h(||) that is highly skewed to the right, due to Theorem 3. To facilitate
the study of how Safari transforms hi (||) into h(||) we will use a Monte Carlo simulation
of Safari. The advantage is that the Monte Carlo simulation is much simpler for analysing
the run-time behavior of Safari than studying the algorithm itself.
Algorithm 2 Monte Carlo simulation of Safari
1: function SafariSimulate(, N ) returns a cardinality distribution
inputs:  , a set of minimal diagnoses
N , integer, number of tries
local variables: hi , h, vectors, cardinality distributions
b, vector, fault distribution, n, i, c, integers
2:
hi  CardinalityDistribution( )
3:
for n  1, 2, . . . , N do
4:
for c  1, 2, . . . , |hi | do
5:
b[c]  c  hi [c]
6:
end for
7:
for i  1, 2, . . . , | | do
 
8:
c  DiscreteInverseRandomValue Pb b
9:
b[c]  b[c]  c
10:
end for
11:
h[c]  h[c] + 1
12:
end for
13:
return h
14: end function
398

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

Algorithm 2 simulates which diagnoses from the input set of minimal diagnoses  are
reached by Safari in N tries. The auxiliary subroutine CardinalityDistribution
computes the input distribution hi by iterating over all diagnoses in  . We store the input
cardinality distribution hi and the resulting cardinality distribution h in vectors (note the
vector sums in lines 7 and 8 and the division of a vector by scalar in line 8).
The outermost loop of Alg. 2 (lines 3  12) simulates the N runs of Safari. This is done
by computing and updating an auxiliary vector b, which contains the distribution of the
component variables in  according to the cardinalities of the diagnoses these variables
belong to. Initially, b is initialized with the number of literals in single faults in position 1,
the number of literals in double faults in position 2 (for example if there are three double
faults in hi , b[2] = 6), etc. This is done in lines 4  6 of Alg. 2. We assume that diagnoses
do not share literals. This restriction can be easily dropped by counting all the assumables
in the input  (the latter assumption does not change the results of this section).
Lines 7  10 simulate the process of the actual bit flipping of Safari. At each step
the simulation draws a random literal from the probability distribution function (pdf ) Pb b ;
this is done by the DiscreteInverseRandomValue function in line 8. Each bit flip
invalidates a diagnosis from the set  , i.e., a diagnosis of cardinality c cannot be reached
by Safari. After a diagnosis has been invalidated, the vector b is updated, for example, if
the simulation invalidates a quadruple fault, b[4] = b[4]  4 (line 9). Note that the number
of iterations in the loop in lines 7  10 equals the number of diagnoses in  . As a result
after terminating this loop, the value of the integer variable c is equal to the cardinality of
the last invalidated diagnosis. The latter is the diagnosis which Safari computes in this
run. What remains is to update the resulting pdf with the right cardinality (line 11).
The simulation in Alg. 2 links the distribution of the actual diagnoses in  to the
distribution of the cardinalities of the diagnoses returned by Safari. As  can be arbitrarily set, we will apply Alg. 2 to a range of typical input distributions. The results of the
simulation as well as the results of running Safari on synthetic problems with the same
input distributions are shown in Fig. 12.
Fig. 12 shows (1) that Alg. 2 predicts the actual behavior of Safari (compare the second
and third column of plots), and (2) that Safari computes diagnoses of small cardinality
in agreement with Theorem 3. The only case when the output distribution is not a steep
exponential is when the cardinalities in the set of the input minimal diagnoses grow exponentially. Table 2 summarizes the parameters of exponential fits for the input cardinality
distributions shown in Fig. 12 (a is the initial (zero) cardinality,  is the decay constant, and
R2 is the coefficient of determination). We have seen that Safari is suited for computing
multiple diagnoses of small probability of occurrence. In the next section we will provide
an alternative argument leading to similar conclusions.

6. Experimental Results
This section discusses empirical results measured from an implementation of Safari. In order to compare the optimality and performance of Safari to various diagnostic algorithms,
we have performed more than a million diagnosis computations on 64 dual-CPU nodes belonging to a cluster. Each node contains two 2.4 GHz AMD Opteron DP 250 processors
and 4 Gb of RAM.
399

fiFeldman, Provan, & van Gemund

degenerate input

prediction (model)

0.5
0

1

h(||)

1

h(||)

h(||)

1

0.5
0

0

50
||

100

0

0.5
0

50
||

100

0
0

50
||

100

h(||)

h(||)

0.2
0.1

20

exponential input

10
||

0

h(||)

h(||)
20
||

40

0

prediction (model)
0.3

5

10

||

0.2
0.1
0

40

0.3

h(||)

h(||)

h(||)
0

20
||

SAFARI

0.4

0

0.5
0

0

reverse exponential input

0.2

20

1

0.5

40

10
||

SAFARI

0
20
||

0.1

prediction (model)

0
0

0.2

20

1

0.2

100

0
0

0.4

50
||

0.3

0
10
||

0

SAFARI

0.3

0.05

100

0.5

prediction (model)

0.1

50
||

SAFARI

h(||)

h(||)

h(||)

0.5

0

0

1

normal input

h(||)

100

1

0

h(||)

50
||

prediction (model)

1

0

0.5
0

uniform input

0

SAFARI

0.2
0.1

0

5
||

10

0

0

5
||

Figure 12: Predicted and actual cardinality distributions
400

10

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

Table 2: Fit coefficients to exponential and goodness of fit for the cardinality distribution
in Fig. 12
a



R2

576
423
69 470
385

0.44
0.34
4.26
0.33

1
0.99
1
0.95

Input Distribution
Uniform
Normal
Exponential
Reverse Exponential

The default configuration of Safari (when not stated otherwise) was M = 8 and N = 4;
that is, Safari is configured for a maximum number of 8 retries before giving up the
climb, and a total of 4 attempts. To provide more precise average run-time optimality and
performance data, all stochastic algorithms (i.e., ones based on SLS Max-SAT and Safari)
have been repeatedly run 10 times on each model and observation vector.
6.1 Implementation Notes and Test Set Description
We have implemented Safari in approximately 1 000 lines of C code (excluding the LTMS,
interface, and DPLL code) and it is a part of the Lydia package.7
Traditionally, MBD algorithms have been tested on diagnostic models of digital circuits
like the ones included in the ISCAS85 benchmark suite (Brglez & Fujiwara, 1985). As
models derived from ISCAS85 are large (from a traditional diagnostic perspective), we
have also considered four medium-sized circuits from the 74XXX family (Hansen, Yalcin,
& Hayes, 1999). In order to provide both weak- and strong-fault cases, we have translated
each circuit to a weak, stuck-at-0 (S-A-0), and stuck-at-1 (S-A-1) model. In the stuck-at
models, the output of each faulty gate is assumed to be the same constant (cf. Def. 14).
The performance of diagnostic algorithms depends to various degrees on the observation
vectors (algorithm designers strive to produce algorithms, the performance of which is not
dependent on the observation vectors). Hence, we have performed our experimentation
with a number of different observations for each model. We have implemented an algorithm
(Alg. 3) that generates observations leading to diagnoses of different minimal-cardinality,
varying from 1 to nearly the maximum for the respective circuits (for the 74XXX models
it is the maximum). The experiments omit nominal scenarios as they are trivial from the
viewpoint of MBD.
Algorithm 3 uses a number of auxiliary functions. RandomInputs (line 3) assigns
uniformly distributed random values to each input in IN (note that for the generation of
observation vectors we partition the observable variables OBS into inputs IN and outputs
OUT and use the input/output information which comes with the original 74XXX/ISCAS85
circuits for simulation). Given the all healthy health assignment and the diagnostic system, ComputeNominalOutputs (line 4) performs simulation by propagating the input
assignment . The result is an assignment  which contains values for each output variable
in OUT.
7. Lydia, Safari, and the diagnostic benchmark can be downloaded from http://fdir.org/lydia/.

401

fiFeldman, Provan, & van Gemund

Algorithm 3 Algorithm for generation of observation vectors
1: function MakeAlphas(DS, N, K) returns a set of observations
inputs: DS = hSD, COMPS, OBSi, diagnostic system
OBS = IN  OUT, IN  OUT = 
N , integer, number of tries for Safari
K, integer, maximal number of diagnoses per cardinality
local variables: , , n , , terms
c, integer, best cardinality so far
A, set of terms (observation vectors), result
2:
for k  1, 2, . . . , K do
3:
  RandomInputs(IN)
4:
  ComputeNominalOutputs(DS, )
5:
c0
6:
for all v  OUT do
7:
n    Flip(, v)
8:
  SmallestCardinalityDiagnosis(Safari(SD, n , |COMPS|, N ))
9:
if || > c then
10:
c  ||
11:
A  A  n
12:
end if
13:
end for
14:
end for
15:
return A
16: end function
The loop in lines 6  13 increases the cardinality by greedily flipping the values of the
output variables. For each new candidate observation n , Alg. 3 uses the diagnostic oracle
Safari to compute a minimal diagnosis of cardinality c. As Safari returns more than
one diagnosis (up to N ), we use SmallestCardinalityDiagnosis to choose the one of
smallest cardinality. If the cardinality c of this diagnosis increases in comparison to the
previous iteration, the observation is added to the list.
By running Alg. 3 we get up to K observations leading to faults of cardinality 1, 2, . . . , m,
where m is the cardinality of the MFMC diagnosis (Feldman, Provan, & van Gemund,
2008b) for the respective circuit. Alg. 3 clearly shows a bootstrapping problem. In order
to create potentially difficult observations for Safari, we require Safari to solve those
difficult observations. Although we have seen in Sec. 5 that Safari is heavily biased
towards generating diagnoses of small cardinality, there is no guarantee. To alleviate this
problem, for the generation of observation vectors, we have configured Safari to compute
subset-minimal diagnoses with M = |COMPS| and N increased to 20.
Table 3 provides an overview of the fault diagnosis benchmark used for our experiments.
The third and fourth columns show the number of observable and assumable variables, which
characterize the size of the circuits. The next three columns show the number of observation
vectors with which we have tested the weak, S-A-0, and S-A-1 models. For the stuck-at
models, we have chosen those weak-fault model observations which are consistent with their
402

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

Table 3: An overview of the 74XXX/ISCAS85 benchmark circuits
Name

Description

74182
74L85
74283
74181

4-bit
4-bit
4-bit
4-bit

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

27-channel interrupt controller
32-bit SEC circuit
8-bit ALU
32-bit SEC circuit
16-bit SEC/DEC
12-bit ALU
8-bit ALU
9-bit ALU
32-bit multiplier
32-bit adder

Variables
|OBS| |COMPS|

carry-lookahead generator
magnitude comparator
adder
ALU

Observations
Weak S-A-0 S-A-1

14
14
14
22

19
33
36
65

250
150
202
350

150
58
202
143

82
89
202
213

43
73
86
73
58
373
72
301
64
315

160
202
383
546
880
1 193
1 669
2 307
2 416
3 512

301
835
1 182
836
846
1 162
756
2 038
404
1 557

301
235
217
836
846
134
625
158
274
255

301
835
335
836
846
123
743
228
366
233

respective system descriptions (as in strong-fault models it is often the case that SD |=,
we have not considered such scenarios).
6.2 Comparison to Complete Algorithms
Table 4 shows the results from comparing Safari to implementations of two state-of-the-art
complete and deterministic diagnostic algorithms: a modification for completeness of CDA
(Williams & Ragno, 2007) and HA (Feldman & van Gemund, 2006). Table 4 shows, for
each model and for each algorithm, the percentage of all tests for which a diagnosis could
be computed within a cut-off time of 1 minute.
As it is visible from the three rightmost columns of Table 4, Safari could find diagnoses for all observation vectors, while the performance of the two deterministic algorithms
(columns two to seven) degraded with the increase of the model size and the cardinality of
the observation vector. Furthermore, we have observed a degradation of the performance of
CDA and HA with increased cardinality of the minimal-cardinality diagnoses, while the
performance of Safari remained unaffected.
6.3 Comparison to Algorithms Based on ALLSAT and Model Counting
We have compared the performance of Safari to that of a pure SAT-based approach,
which uses blocking clauses for avoiding duplicate diagnoses (Jin, Han, & Somenzi, 2005).
Although SAT encodings have worked efficiently on a variety of other domains, such as
planning, the weak health modeling makes the diagnostic problem so underconstrained that
an uninformed ALLSAT strategy (i.e., a search not exploiting the continuity imposed by
the weak-fault modeling) is quite inefficient, even for small models.
403

fiFeldman, Provan, & van Gemund

Table 4: Comparison of CDA , HA , and Safari [% of tests solved]
Name

Weak

CDA
S-A-0

74182
74L85
74283
74181

100
100
100
79.1

100
100
100
98.6

100
100
100
97.7

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

74.1
29
11.6
3.8
0
0
0
0
0
0

75.4
45.5
44.7
4.7
0
0
0
0
0
0

73.1
27.7
32.2
5.4
0
0
0
0
0
0

S-A-1

Weak

HA
S-A-0

S-A-1

Weak

100
100
100
100

100
100
100
100

100
100
100
100

100
100
100
100

100
100
100
100

100
100
100
100

100
100
100
100
100
100
100
100
100
100

100
100
100
100
100
100
100
100
100
100

100
100
100
100
100
100
100
100
100
100

71.1
24.1
12.4
10.8
6.1
5
1.1
1.1
3.5
3.9

94.7
77.9
62.2
10.6
6
64.2
3.8
8.2
5.1
7.8

69.1
25.9
41.5
12.2
6.5
44.7
2.2
5.7
3.3
12

Safari
S-A-0 S-A-1

To substantiate our claim, we have experimented with the state-of-the-art satisfiability
solver RelSat, version 2.02 (Bayardo & Pehoushek, 2000). Instead of enumerating all
solutions and filtering the minimal diagnoses only, we have performed model-counting, whose
relation to MBD has been extensively studied (Kumar, 2002). While it was possible to solve
the two smallest circuits, the solver did not terminate for any of the larger models within
the predetermined time of 1 hour. The results are shown in Table 5.
The second column of Table 5 shows the model count returned by RelSat, with sample single-fault observations from our benchmark. The third column reports the time for
model counting. This slow performance on relatively small diagnostic instances leads us
to the conclusion that specialized solvers like Safari are better suited for finding minimal
diagnoses than off-the-shelf ALLSAT (model counting) implementations that do not encode
inference properties similar to those encoded in Safari.
We have used the state-of-the-art, non-exact model counting method SampleCount
(Gomes, Hoffmann, Sabharwal, & Selman, 2007) to compute lower bounds of the model
counts. The results are shown in the third and fourth columns of Table 5. Configured with
the default settings ( = 3.5, t = 2, z = 20, cutoff 10 000 flips), SampleCount could not
find lower bounds for circuits larger than c1355. Although the performance of SampleCount is significantly better than RelSAT, the fact that SampleCount computes lower
bounds and does not scale to large circuits prevent us from building a diagnosis algorithm
based on approximate model counting.
A satisfiability-based method for diagnosing an optimized version of ISCAS85 has been
used by Smith, Veneris, and Viglas (2004). In a more recent paper (Smith, Veneris, Ali, &
Viglas, 2005), the SAT-based approach has been replaced by a Quantified Boolean Formula
(QBF) solver for computing multiple-fault diagnoses. These methods report good absolute
404

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

Table 5: Model count and time for counting
RelSat
Models
Time [s]

Name

SampleCount
Models
Time [s]

74182
74L85
74283
74181

3.9896  107
8.3861  1014
 1.0326  1015
 5.6283  1015

1
340
> 3 600
> 3 600

 3.526359  106
 7.412344  1013
 3.050026  1014
 1.538589  1027

0.2
0.3
0.3
1.1

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552












7.2045  1018
3.6731  1020
9.4737  1039
1.4668  1028
2.1704  1031
9.0845  1015
4.8611  1019
9.3551  1016
1.0300  1018
1.0049  1016

> 3 600
> 3 600
> 3 600
> 3 600
> 3 600
> 3 600
> 3 600
> 3 600
> 3 600
> 3 600

 1.496602  1067
 7.549183  1083
 8.332702  10166
 7.488300  10233







9.9
13.1
42.7
99.8







execution time for single and double-faults (and we believe that they scale well for higher
cardinalities), but require modifications of the initial circuits (i.e., introduce cardinality and
test constraints) and suggest specialized heuristics for the SAT solvers in order to improve
the search performance. Comparison of the performance of Safari to the timings reported
by these papers would be difficult due to a number of reasons like the use of different and
optimized benchmark sets, trading-off memory for speed, rewriting the original circuits, etc.
6.4 Performance of the Greedy Stochastic Search
Table 6 shows the absolute performance of Safari (M = |COMPS|, N = 4). This varies
from under a millisecond for the small models, to approx. 30 s for the largest strong-fault
model. These fast absolute times show that Safari is suitable for on-line reasoning tasks,
where autonomy depends on speedy computation of diagnoses.
For each model, the minimum and maximum time for computing a diagnosis has been
computed. These values are shown under columns tmin and tmax , respectively. The small
range of tmax  tmin confirms our theoretical results that Safari is insensitive to the fault
cardinalities of the diagnoses it computes. The performance of CDA and HA , on the
other hand, is dependent on the fault cardinality and quickly degrades with increasing fault
cardinality.
6.5 Optimality of the Greedy Stochastic Search
From the results produced by the complete diagnostic methods (CDA and HA ) we know
the exact cardinalities of the minimal-cardinality diagnoses for some of the observations. By
considering these observations, which lead to single and double faults, we have evaluated
405

fiFeldman, Provan, & van Gemund

Table 6: Performance of Safari [ms]
Weak
Name

tmin

S-A-0
tmax

74182
74L85
74283
74181

0.41
0.78
0.92
2.04

1.25
7.47
4.84
6.94

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

8.65
14.19
48.08
95.03
237.77
500.54
984.31
1 950.12
2 105.28
4 557.4

38.94
31.78
88.87
141.59
349.96
801.12
1 300.98
2 635.71
2 688.34
6 545.21

tmin
0.39
0.72
0.88
2.13

S-A-1
tmax
4.41
1.89
3.65
22.4

tmin
0.40
0.69
0.92
2.07

tmax
0.98
4.77
5.2
7.19

7.58
30.59
7.96
38.27
11.03
30.32
10.79
31.11
37.08
80.74
38.47
81.34
76.57
150.29
83.14
135.29
196.13
300.11
217.32
442.91
646.95 1 776.72
463.24
931.8
1 248.5
2 516.46
976.56 2 565.18
3 346.49 7 845.41 2 034.5
4 671.17
2 246.84 3 554.4 1 799.18 2 469.48
9 975.04 32 210.71 5 338.97 12 101.61

the average optimality of Safari. Table 7 shows these optimality results for the greedy
search. The second column of Table 7 shows the number of observation vectors leading to
single faults for each weak-fault model. The third column shows the average cardinality of
Safari. The second and third column are repeated for the S-A-0 and S-A-1 models.
Table 7 shows that, for SD  WFM, the average cardinality returned by Safari is
near-optimal for both single and double faults. The c1355 model shows the worst-case
results for the single-fault observations, while c499 is the most difficult weak-fault model for
computing a double-fault diagnosis. These results can be improved by increasing M and N
as discussed in Sec. 4.
With strong-fault models, results are close to optimal for the small models and the
quality of diagnosis deteriorates for c3540 and bigger. This is not surprising, considering
the modest number of retries and number of flips with which Safari was configured.
6.6 Computing Multiple Minimal-Cardinality Diagnoses
We next show the results of experiments supporting the claims made in Sec. 5. For that,
we have first chosen these observations  for which we could compute | (SD  )| with a
deterministic algorithm like CDA or HA (mostly observations leading to single or double
faults). We have then configured Safari with M = |COMPS| and N = 10| (SD  )|.
Finally, from the diagnoses computed by Safari we have filtered the minimal-cardinality
ones. The results are summarized in Table 8.
Table 8 repeats the same columns for weak, S-A-0, and S-A-1 models and the data
in these columns are to be interpreted as follows. The columns marked with | | show
the minimal and maximal number of minimal-cardinality diagnoses per model as computed
by a deterministic algorithm. The columns Mc show the percentage of minimal-cardinality
406

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

Table 7: Optimality of Safari [average cardinality]
Single Faults
S-A-0
S-A-1
# Card. # Card.

Name

Weak
# Card.

74182
74L85
74283
74181

50
50
50
50

1
1.04
1.08
1.19

37
18
34
36

1
1.02
1.59
2.81

40
40
46
46

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

58
84
50
84
52
29
8
14
13
27

1.19
1.49
1
1.66
1.05
1.03
1.01
1
1
1.01

52
53
39
82
49
39
23
9
13
11

1.06
1.49
1.1
1
2.91
1.77
2.5
3.54
28.83
17.37

37
84
40
84
52
28
16
12
12
18

1
1.03
1.88
2.6

Weak
# Card.

Double Faults
S-A-0
S-A-1
# Card. # Card.

50
50
50
50

2
2.12
2.2
2.25

38
17
45
36

1.04 82
1.01 115
1.05 50
1.02
6
4.79 
2.06 13
3.74 
5.4
7
28.68
1
23.38 16

2.46
3.27
2.01
2.15

2.12

2
2
2

80
34
34
7
2
24
1
3
1
4

2
2.06
2.41
3.61

18
35
42
43

2
2.07
2.6
3.16

2.25 48
3.01 115
2.14 35
2
18
3
3
2.78 15
4.9

3.7
1
27

18.5
6

2.15
2.03
2.07
2.07
3.17
3.27

3.8

27.53

Table 8: % of all minimal-cardinality diagnoses computed by Safari
Weak
Mc

Name

| |

74182
74L85
74283
74181

1  25
1  78
1  48
1  133

100
99.2
97.9
97.4

c432
c499
c880
c1355
c1908
c2670
c3540
c5315
c6288
c7552

1  99
1  22
2  646
5  2 770
2  1 447
1  76
1  384
1  235
1  154
1  490

94.2
78.5
99.9
79.4
96.6
100
81.5
97.7
100
93.1

S-A-0
Mc Mf

| |

0
2
3
1

12
14
1  16
1  16

100
100
93.8
88.6

0
0
0
4.07

1  20
1  49
1  29
1  57

100
99.7
84.9
96.7

0
0
4
6.36

1  40
1  15
1  160
2  648
2  579
1  20
1  153
1  24
1  73
4  236

89.7
96.3
96.9
95.7
85.2
97.1
88.8
81.7
78.1
90.8

0
6
0
0
1.85
0
7.98
7.04
5.1
13.55

1  18
1  16
1  210
2  347
2  374
1  181
1  171
1  30
1  101
1  168

97
94.8
97.5
95.2
82.3
89.7
78.2
93.4
82.1
78

0
0
0
0.52
1.24
0
7.27
8.24
1.22
12.1

7.14
1.51
0
1.02
2.61
2.34
8.52
1.74
13.1
2.17

| |

S-A-1
Mc Mf

Mf

diagnoses returned by Safari (from all minimal-cardinality diagnoses) for those  for which
| (SD  )| > 1. The columns Mf show the percentage of observations for which Safari
could not compute any minimal-cardinality diagnosis.
407

fiFeldman, Provan, & van Gemund

The results shown in Table 8 show that even for moderate values of N (N  27 770),
Safari was capable of computing a significant portion of all minimal-cardinality diagnoses.
This portion varies from 78.5% to 100% for weak-fault models and from 78% to 100% for
strong-fault models. The percentage of cases in which Safari could not reach a minimalcardinality diagnosis is limited (at most 13.55%) and is mainly in the cases in which there
exists only one single-fault diagnosis. Note that even in the cases in which Safari cannot
compute any minimal-cardinality diagnoses, the result of Safari can still be useful. For
example, a subset-minimal diagnosis of small cardinality differing in one or two literals
only nevertheless brings useful diagnostic information (a discussion on diagnostic metrics is
beyond the scope of this paper).
6.7 Experimentation Summary
We have applied Safari to a suite of benchmark combinatorial circuits encoded using
weak-fault models and stuck-at strong fault models, and shown significant performance
improvements for multiple-fault diagnoses, compared to two state-of-the-art deterministic
algorithms, CDA and HA . Our results indicate that Safari shows at least an order-ofmagnitude speedup over CDA and HA for multiple-fault diagnoses. Moreover, whereas the
search complexity for the deterministic algorithms tested increases exponentially with fault
cardinality, the search complexity for this stochastic algorithm appears to be independent
of fault cardinality.
We have compared the performance of Safari to that of an algorithm based on MaxSAT, and Safari shows at least an order-of-magnitude speedup in computing diagnoses.
We have compared the optimality of Safari to that of an algorithm based on SLS MaxSAT, and Safari consistently computes diagnoses of smaller cardinality whereas the SLS
Max-SAT diagnostic algorithm often fails to compute any diagnosis.

7. Related Work
This paper (1) generalizes Feldman, Provan, and van Gemund (2008a), (2) introduces important theoretical results for strong-fault models, (3) extends the experimental results
there, and (4) provides a comprehensive optimality analysis of Safari.
On a gross level, one can classify the types of algorithms that have been applied to solve
MBD as being based on search or compilation. The search algorithms take as input the diagnostic model and an observation, and then search for a diagnosis, which may be minimal
with respect to some minimality criterion. Examples of search algorithms include A -based
algorithms, such as CDA (Williams & Ragno, 2007) and hitting set algorithms (Reiter,
1987). Compilation algorithms pre-process the diagnostic model into a form that is more
efficient for on-line diagnostic inference. Examples of such algorithms include the ATMS
(de Kleer, 1986) and other prime-implicant methods (Kean & Tsiknis, 1993), DNNF (Darwiche, 1998), and OBDD (Bryant, 1992). To our knowledge, all of these approaches adopt
exact methods to compute diagnoses; in contrast, Safari adopts a stochastic approach to
computing diagnoses.
At first glance, it seems like MBD could be efficiently solved using an encoding as
a SAT (Jin et al., 2005), constraint satisfaction (Freuder, Dechter, Ginsberg, Selman, &
Tsang, 1995) or Bayesian network (Kask & Dechter, 1999) problem. However, one needs to
408

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

take into account the increase in formula size (over a direct MBD encoding), in addition to
the underconstrained nature of MBD problems.
Safari has close resemblance to Max-SAT (Hoos & Stutzle, 2004) and we have conducted extensive experimentation with both complete (partial and weighted) and SLS-based
Max-SAT. As the results of these experiments are long, we have published them in a separate technical report (Feldman, Provan, & van Gemund, 2009a). The results show that
although Max-SAT can compute diagnoses in many of the cases, the performance of MaxSAT degrades when increasing the circuit size or the cardinality of the injected faults. In
particular, Safari outperforms Max-SAT by at least an order-of-magnitude for the class of
diagnostic problems we have considered. In the case of SLS-based Max-SAT, the optimality
of Max-SAT-based inference is significantly worse than that of Safari.
We show that Safari exploits a particular property of MBD problems, called diagnostic
continuity, which improves the optimality of Safari compared to, for example, straightforward ALLSAT encodings (Jin et al., 2005). We experimentally confirm this favorable
performance and optimality of Safari. Although Safari has close resemblance to MaxSAT, Safari exploits specific landscape properties of the diagnostic problems, which allow
(1) simple termination criteria and (2) optimality bounds. Due to the hybrid nature of
Safari (the use of LTMS and SAT), Safari avoids getting stuck in local optima and performs better than Max-SAT based methods. Incorporating approaches from Max-SAT, and
in particular SAPS (Hutter, Tompkins, & Hoos, 2002), in future versions of Safari may
help in solving more general abduction problems, which may not expose the continuous
properties of the models we have considered.
Stochastic algorithms have been discussed in the framework of constraint satisfaction
(Freuder et al., 1995) and Bayesian network inference (Kask & Dechter, 1999). The latter
two approaches can be used for solving suitably translated MBD problems. It is often the
case, though, that these encodings are more difficult for search than specialized ones.
MBD is an instance of constraint optimization, with particular constraints over failure
variables. MBD has developed algorithms to exploit these domain properties, and our
proposed approach differs significantly from almost all MBD algorithms that appear in the
literature. While most advanced MBD algorithms are deterministic, Safari borrows from
SLS algorithms that, rather than backtracking, may randomly flip variable assignments
to determine a satisfying assignment. Complete MBD algorithms typically make use of
preferences, e.g., fault-mode probabilities, to improve search efficiency; Safari uses this
technique on top of its stochastic search over the space of diagnoses.
A closely-related diagnostic approach is that of Fijany, Vatan, Barrett, James, Williams,
and Mackey (2003), who map the minimal-hitting set problem into the problem of finding
an assignment with bounded weight satisfying a monotone SAT problem, and then propose
to use efficient SAT algorithms for computing diagnoses. The approach of Fijany et al. has
shown speedups in comparison with other diagnosis algorithms; the main drawback is the
number of extra variables and clauses that must be added in the SAT encoding, which is
even more significant for strong fault models and multi-valued variables. In contrast, our
approach works directly on the given diagnosis model and requires no conversion to another
representation.
Our work bears the closest resemblance to preference-based or Cost-Based Abduction
(CBA) (Charniak & Shimony, 1994; Santos Jr., 1994). Of the algorithmic work in this
409

fiFeldman, Provan, & van Gemund

area, the primary paper that adopts stochastic local search is by Abdelbar, Gheita, and
Amer (2006). In this paper, they present a hybrid two-stage method that is based on
Iterated Local Search (ILS) and Repetitive Simulated Annealing (RSA). The ILS stage of
the algorithm uses a simple hill-climbing method (randomly flipping assumables) for the
local search phase, and tabu search for the perturbation phase. RSA repeatedly applies
Simulated Annealing (SA), starting each time from a random initial state. The hybrid
method initially starts from an arbitrary state, or a greedily-chosen state. It then applies
the ILS algorithm; if this algorithm fails to find the optimal solution after a fixed number
 of hill-climbing steps8 or after a fixed number R of repetitions of the perturbation-local
search cycle,9 ILS-based search is terminated and the RSA algorithm is run until the optimal
solution is found.
Our work differs from that of Abdelbar et al. (2006) in several ways. First, our initial
state is generated using a random SAT solution. The hill-climbing phase that we use next
is similar to that of Abdelbar et al.; however, we randomly restart should hill-climbing not
identify a better diagnosis, rather than applying tabu search or simulated annealing. Our
approach is simpler than that of Abdelbar et al., and for the case of weak fault models
is guaranteed to be optimal; in future work we plan to compare our approach to that of
Abdelbar et al. for strong fault models.
In 2009 Safari competed against the diagnostic algorithms NGDE (de Kleer, 2009)
and RODON (Bunus, Isaksson, Frey, & Munker, 2009) in the synthetic track of the first
diagnostic competition DXC09 (Kurtoglu, Narasimhan, Poll, Garcia, Kuhn, de Kleer, van
Gemund, & Feldman, 2009). The conditions under which the DXC09 experiments were
conducted were similar to the ones described in this paper. The CPU and memory performance of Safari were an order of magnitude better than the competing algorithms despite
the fact that NGDE and RODON performed better than the complete algorithms discussed
in this section. In this paper, in addition to computational metrics, we have informally
used the minimality of a diagnosis as an optimality criterion. The DXC09 organizers, however, have defined a utility metric which approximates the expected repair effort of a circuit
(Feldman, Provan, & van Gemund, 2009b). With this utility metric, Safari scored slightly
worse than the two competing algorithms, which is to be expected as Safari trades off
diagnostic precision for computational efficiency. We refer the reader to the DXC papers
mentioned above for a more thorough analysis of the competition results.

8. Conclusion and Future Work
We have described a greedy stochastic algorithm for computing diagnoses within a modelbased diagnosis framework. We have shown that subset-minimal diagnoses can be computed
optimally in weak fault models and in an important subset of strong fault models, and that
almost all minimal-cardinality diagnoses can be computed for more general fault models.
8. Hill-climbing proceeds as follows: given a current state s with a cost of f (s), a neighbouring state s
is generated by flipping a randomly chosen assumable hypothesis. If f (s ) is better than f (s), then s
becomes the current state; otherwise, it is discarded. If  iterations elapse without a change in the
current state, the local search exits.
9. Perturbation-local search, starting from a current state s with a cost of f (s), randomly chooses an
assumable variable h, and applies tabu search to identify a better state by flipping h based on its tabu
status.

410

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

We argue that Safari can be of broad practical significance, as it can compute a significant fraction of minimal-cardinality diagnoses for systems too large or complex to be
diagnosed by existing deterministic algorithms.
In future work, we plan to experiment on models with a combination of weak and strong
failure-mode descriptions. We also plan on experimenting with a wider variety of stochastic
methods, such as simulated annealing and genetic search, using a larger set of benchmark
models. Last, we plan to apply our algorithms to a wider class of abduction and constraint
optimization problems.

References
Abdelbar, A. M. (2004). Approximating cost-based abduction is NP-hard. Artificial Intelligence, 159 (1-2), 231239.
Abdelbar, A. M., Gheita, S. H., & Amer, H. A. (2006). Exploring the fitness landscape and
the run-time behaviour of an iterated local search algorithm for cost-based abduction.
Experimental & Theoretical Artificial Intelligence, 18 (3), 365386.
Bayardo, R. J., & Pehoushek, J. D. (2000). Counting models using connected components.
In Proc. AAAI00, pp. 157162.
Brglez, F., & Fujiwara, H. (1985). A neutral netlist of 10 combinational benchmark circuits
and a target translator in fortran. In Proc. ISCAS85, pp. 695698.
Bryant, R. E. (1992). Symbolic Boolean manipulation with ordered binary-decision diagrams. ACM Computing Surveys, 24 (3), 293318.
Bunus, P., Isaksson, O., Frey, B., & Munker, B. (2009). RODON - a model-based diagnosis
approach for the DX diagnostic competition. In Proc. DX09, pp. 423430.
Bylander, T., Allemang, D., Tanner, M., & Josephson, J. (1991). The computational complexity of abduction. Artificial Intelligence, 49, 2560.
Charniak, E., & Shimony, S. E. (1994). Cost-based abduction and MAP explanation. Artificial Intelligence, 66 (2), 345374.
Cook, S. A. (1971). The complexity of theorem-proving procedures. In Proc. STOC71, pp.
151158.
Darwiche, A. (1998). Model-based diagnosis using structured system descriptions. Journal
of Artificial Intelligence Research, 8, 165222.
Davis, M., Logemann, G., & Loveland, D. (1962). A machine program for theorem-proving.
Communications of the ACM, 5 (7), 394397.
de Kleer, J. (1986). An assumption-based TMS. Artificial Intelligence, 28 (2), 127162.
de Kleer, J. (1990). Using crude probability estimates to guide diagnosis. Artificial Intelligence, 45 (3), 381291.
de Kleer, J. (2009). Minimum cardinality candidate generation. In Proc. DX09, pp. 397
402.
de Kleer, J., Mackworth, A., & Reiter, R. (1992). Characterizing diagnoses and systems.
Artificial Intelligence, 56 (2-3), 197222.
411

fiFeldman, Provan, & van Gemund

de Kleer, J., & Williams, B. (1987). Diagnosing multiple faults. Artificial Intelligence,
32 (1), 97130.
Een, N., & Sorensson, N. (2003). An extensible SAT-solver. In Proc. SAT03, Vol. 2919 of
Lecture Notes in Computer Science, pp. 502518. Springer.
Eiter, T., & Gottlob, G. (1995). The complexity of logic-based abduction. Journal of the
ACM, 42 (1), 342.
Feldman, A., Provan, G., & van Gemund, A. (2008a). Computing minimal diagnoses by
greedy stochastic search. In Proc. AAAI08, pp. 911918.
Feldman, A., Provan, G., & van Gemund, A. (2008b). Computing observation vectors for
max-fault min-cardinality diagnoses. In Proc. AAAI08, pp. 911918.
Feldman, A., Provan, G., & van Gemund, A. (2009a). A family of model-based diagnosis
algorithms based on Max-SAT. Tech. rep. ES-2009-02, Delft University of Technology.
Feldman, A., Provan, G., & van Gemund, A. (2009b). The Lydia approach to combinational
model-based diagnosis. In Proc. DX09, pp. 403408.
Feldman, A., & van Gemund, A. (2006). A two-step hierarchical algorithm for model-based
diagnosis. In Proc. AAAI06, pp. 827833.
Fijany, A., Vatan, F., Barrett, A., James, M., Williams, C., & Mackey, R. (2003). A novel
model-based diagnosis engine: Theory and applications. In Proc. IEEE Aerospace03,
Vol. 2, pp. 901910.
Forbus, K., & de Kleer, J. (1993). Building Problem Solvers. MIT Press.
Freeman, J. W. (1995). Improvements to Propositional Satisfiability Search Algorithms.
Ph.D. thesis, University of Pennsylvania.
Freuder, E. C., Dechter, R., Ginsberg, M. L., Selman, B., & Tsang, E. P. K. (1995). Systematic versus stochastic constraint satisfaction. In Proc. IJCAI95, Vol. 2, pp. 20272032.
Friedrich, G., Gottlob, G., & Nejdl, W. (1990). Physical impossibility instead of fault
models. In Proc. AAAI90, pp. 331336.
Gomes, C. P., Hoffmann, J., Sabharwal, A., & Selman, B. (2007). From sampling to model
counting. In Proc. IJCAI07, pp. 22932299.
Hansen, M., Yalcin, H., & Hayes, J. (1999). Unveiling the ISCAS-85 benchmarks: A case
study in reverse engineering. IEEE Design & Test, 16 (3), 7280.
Hermann, M., & Pichler, R. (2007). Counting complexity of propositional abduction. In
Proc. IJCAI07, pp. 417422.
Hoos, H. (1999). SAT-encodings, search space structure, and local search performance. In
Proc. IJCAI99, pp. 296303.
Hoos, H., & Stutzle, T. (2004). Stochastic Local Search: Foundations and Applications.
Morgan Kaufmann Publishers Inc.
Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling and probabilistic smoothing:
Efficient dynamic local search for SAT. In Proc. CP02, pp. 233248.
412

fiApproximate Model-Based Diagnosis Using Greedy Stochastic Search

Jin, H., Han, H., & Somenzi, F. (2005). Efficient conflict analysis for finding all satisfying
assignments of a Boolean circuit. In Proc. TACAS05, pp. 287300.
Kask, K., & Dechter, R. (1999). Stochastic local search for Bayesian networks. In Proc.
AISTAT99, pp. 113122.
Kean, A., & Tsiknis, G. K. (1993). Clause management systems. Computational Intelligence,
9, 1140.
Kumar, T. K. S. (2002). A model counting characterization of diagnoses. In Proc. DX02,
pp. 7076.
Kurtoglu, T., Narasimhan, S., Poll, S., Garcia, D., Kuhn, L., de Kleer, J., van Gemund, A.,
& Feldman, A. (2009). First international diagnosis competition - DXC09. In Proc.
DX09, pp. 383396.
Marques-Silva, J. P. (1999). The impact of branching heuristics in propositional satisfiability
algorithms. In Proc. EPIA99, pp. 6274.
McAllester, D. A. (1990). Truth maintenance. In Proc. AAAI90, Vol. 2, pp. 11091116.
Reiter, R. (1987). A theory of diagnosis from first principles. Artificial Intelligence, 32 (1),
5795.
Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence, 82 (1-2),
273302.
Santos Jr., E. (1994). A linear constraint satisfaction approach to cost-based abduction.
Artificial Intelligence, 65 (1), 128.
Smith, A., Veneris, A., Ali, M. F., & Viglas, A. (2005). Fault diagnosis and logic debugging
using Boolean satisfiability. IEEE Transactions on CAD of Integrated Circuits and
Systems, 24 (10), 16061621.
Smith, A., Veneris, A., & Viglas, A. (2004). Design diagnosis using Boolean satisfiability.
In Proc. ASP-DAC04, pp. 218223.
Struss, P., & Dressler, O. (1992). Physical negation - integrating fault models into the General Diagnostic Engine. In Readings in Model-Based Diagnosis, pp. 153158. Morgan
Kaufmann Publishers Inc.
Thiffault, C., Bacchus, F., & Walsh, T. (2004). Solving non-clausal formulas with DPLL
search. In Proc. CP04, pp. 663678.
Tseitin, G. (1983). On the complexity of proofs in propositional logics. In Siekmann, J., &
Wrightson, G. (Eds.), Automation of Reasoning: Classical Papers in Computational
Logic (19671970), Vol. 2. Springer-Verlag.
Williams, B., & Ragno, R. (2007). Conflict-directed A* and its role in model-based embedded systems. Journal of Discrete Applied Mathematics, 155 (12), 15621595.
Zhang, H., & Stickel, M. E. (1996). An efficient algorithm for unit propagation. In Proc.
AI-MATH96, pp. 166169.

413

fiJournal of Artificial Intelligence Research 38 (2010) 271-305

Submitted 12/09; published 06/10

Developing Approaches for Solving a Telecommunications
Feature Subscription Problem
David Lesaint

david.lesaint@bt.com

Business Modelling and Operational Transformation,
BT Research,
UK.

Deepak Mehta
Barry OSullivan
Luis Quesada
Nic Wilson

d.mehta@4c.ucc.ie
b.osullivan@4c.ucc.ie
l.quesada@4c.ucc.ie
n.wilson@4c.ucc.ie

Cork Constraint Computation Centre,
University College Cork,
Ireland.

Abstract
Call control features (e.g., call-divert, voice-mail) are primitive options to which users
can subscribe off-line to personalise their service. The configuration of a feature subscription involves choosing and sequencing features from a catalogue and is subject to constraints
that prevent undesirable feature interactions at run-time. When the subscription requested
by a user is inconsistent, one problem is to find an optimal relaxation, which is a generalisation of the feedback vertex set problem on directed graphs, and thus it is an NP-hard
task. We present several constraint programming formulations of the problem. We also
present formulations using partial weighted maximum Boolean satisfiability and mixed integer linear programming. We study all these formulations by experimentally comparing
them on a variety of randomly generated instances of the feature subscription problem.

1. Introduction
Information and communication services, from news feeds to internet telephony, are playing
an increasing, and potentially disruptive, role in our daily lives. As a result, service providers
seek to develop personalisation solutions allowing customers to control and enrich their
service. In telephony, for instance, personalisation relies on the provisioning of call control
features. A feature is an increment of functionality which, if activated, modifies the basic
service behaviour in systematic or non-systematic ways, e.g., do-not-disturb, multi-media
ring-back tones, call-divert-on-busy, credit-card-calling.
Modern service delivery platforms provide the ability to implement features as modular
applications and compose them on demand when setting up live sessions, that is, consistently with the feature subscriptions preconfigured by participants. The architectural style
commonly found in platforms that are based on the Session Initiation Protocol (Rosenberg,
Schulzrinne, Camarillo, Johnston, Peterson, Sparks, Handley, & Schooler, 2002; Sparks,
2007) notably, the Internet Multimedia Subsystem (Poikselka, Mayer, Khartabil, & Niemi,
2006), consists of chaining applications between end-points. In this context, a personalic
2010
AI Access Foundation. All rights reserved.

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

sation approach consists of exposing catalogues of call-control features to subscribers and
letting them select and sequence the features of their choice.
Not all sequences of features are acceptable, however, due to the possible occurrence of
feature interactions (Calder, Kolberg, Magill, & Reiff-Marganiec, 2003). A feature interaction is some way in which a feature modifies or influences the behaviour of another feature
in generating the systems overall behaviour (Bond, Cheung, Purdy, Zave, & Ramming,
2004). For instance, a do-not-disturb feature will block any incoming call and cancel the
effect of any subsequent feature subscribed by the callee. This is an undesirable interaction:
as shown in Figure 1, the call originating from caller X will never reach call-logging feature
of callee Y. However, if call-logging is placed before do-not-disturb then both features will
play their role.

Figure 1: An example of an undesirable feature interaction.
Distributed Feature Composition (dfc) provides a method and a formal architecture to
address feature interactions (Jackson & Zave, 1998, 2003; Bond et al., 2004). The method
consists of constraining the selection and sequencing of features by prescribing constraints
that prevent undesirable interactions. These feature interaction resolution constraints are
represented in a catalogue as precedence or exclusion constraints. A precedence constraint,
i  j, means that if the features i and j are part of the same sequence then i must precede
j. An exclusion constraint between i and j means that they cannot be together in any
sequence. Note that an exclusion constraint between i and j can be expressed as a pair of
two precedence constraints i  j and j  i. Undesirable interactions are then avoided by
rejecting any sequence that does not satisfy the catalogue precedence constraints.
Informally, a feature subscription is defined by a set of features, a set of precedence
constraints specified by a user and a set of precedence constraints prescribed by the feature catalogue. The task is to find a sequence of the user-selected features subject to the
catalogue precedence constraints and the user-specified precedence constraints. It may not
always be possible to construct such a sequence, in which case the task is to find a relaxation of the feature subscription that is consistent and closest to the initial requirements of
the user (Lesaint, Mehta, OSullivan, Quesada, & Wilson, 2008b). In this paper, we show
that checking the consistency of a feature subscription is polynomial in time, but finding
an optimal relaxation of a subscription, when inconsistent, is NP-hard.
We present several formulations of finding an optimal relaxation of a feature subscription using constraint programming. We present a simple constraint optimisation problem
formulation of our problem and investigate the impact of maintaining three different levels
of consistency on decision variables within depth-first branch and bound. The first one is
arc consistency (Rossi, van Beek, & Walsh, 2006a), which is commonly used. The second is
singleton arc consistency and the third is restricted singleton arc consistency (rsac). We
also present a formulation of our problem based on a soft global constraint, which we call
SoftPrec (Lesaint, Mehta, OSullivan, Quesada, & Wilson, 2009). We further present a
272

fiApproaches for Solving a Telecommunications Feature Subscription Problem

formulation based on the weighted constraint satisfaction problem framework (Rossi, van
Beek, & Walsh, 2006b). We also consider partial weighted maximum satisfiability (Biere,
Heule, van Maaren, & Walsh, 2009), and mixed integer linear programming. We present
the formulations using these approaches and discuss their differences with respect to the
constraint programming formulations.
Notice that finding an optimal relaxation of a feature subscription is a generalisation of
the well-known feedback vertex set problem as well as the feedback arc set problem (Garey
& Johnson, 1979). Given a directed graph G = hV, Ei with set of vertices V and set
of edges E, the feedback vertex (arc) set problem is to find a smallest V 0  V (E 0 
E) whose deletion makes the graph acyclic. Although in this paper we focus only on a
particular telecommunication problem, the techniques studied here are also applicable to
other domains where the feedback vertex/arc set problem is encountered, e.g., circuit design,
deadlock prevention, vlsi testing, stabilization of synchronous systems (Festa, Pardalos,
& Resende, 1999, Section 5). There are also applications in chemistry when it comes to
sorting a list of samples of complex mixtures according to their compositions in the presence
of missing data, i.e., when not all components are measured in all samples (Fried, Hordijk,
Prohaska, Stadler, & Stadler, 2004).
The remainder of this paper is organised as follows. Section 2 presents the necessary
background required for this paper. We introduce the notion of feature subscription in
Section 3. In Section 4 we reformulate the original problem in order to relate it more easily
to well-known problems existing in the literature. In Section 5 we present an algorithm
for dealing with symmetries introduced when the original subscription is reformulated. We
introduce the notion of relaxation of an inconsistent subscription in Section 6 and prove
that finding an optimal relaxation of an inconsistent subscription is NP-Hard. In Section
7 we model the problem of finding such an optimal relaxation as a constraint optimisation
problem. In Section 8, we present two other constraint programming approaches based
on the notions of global constraints and weighted constraint satisfaction problems. In
Sections 9 and 10, the partial weighted maximum satisfiability and mixed integer linear
programming formulations of the problem are described. The empirical evaluation of all
these approaches is shown in Section 11. Finally our conclusions and future directions are
presented in Section 12.

2. Background
In this section we present a set of concepts on binary relations and constraint programming
that will be used in the next sections.
2.1 Binary Relations
A binary relation over a finite set X is an association of elements of X with elements of X.
Let R be a binary relation over a finite set X. A relation R on a set X is irreflexive if and
only if there is no x  X such that hx, xi  R. A relation R on a set X is transitive if and
only if for all x, y and z in X, [hx, yi  R][hy, zi  R]  [hx, zi  R]. The transitive closure
of a binary relation R on a set X is the smallest transitive relation on X that contains R.
We use the notation R to denote the transitive closure of R. A relation R on a set X is
asymmetric if and only if for all x, y in X, [hx, yi  R]  [hy, xi 6 R]. A relation R on a set
273

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

X is total if and only if for any x, y in X, either hx, yi  R or hy, xi  R. A strict partial
order is a binary relation that is irreflexive and transitive. A strict total order is a binary
b
relation that is transitive, asymmetric and total. The transpose of a relation R, denoted R,
is the set {hy, xi|hx, yi  R}. The restriction of R on the set Y , denoted RY , is the set
{hx, yi  R|{x, y}  Y }. Any binary relation R on set X can also be viewed as a directed
graph where the nodes correspond to the elements in X and ordered pairs in R correspond
to the edges of the graph.
2.2 Constraint Programming
Constraint Programming (cp) has been successfully used in many applications such as planning, scheduling, resource allocation, routing, and bio-informatics (Wallace, 1996). Problems are primarily stated as a Constraint Satisfaction Problems (csps), that is a finite set
of variables with finite domains, together with a finite set of constraints. A solution of a
csp is an assignment of a value to each variable such that all constraints are satisfied simultaneously. The basic approach for solving a csp instance is to use a backtracking search
algorithm that interleaves two processes: constraint propagation and labelling. Constraint
propagation helps in pruning values that cannot lead to a solution of the problem. Labelling
involves assigning values to variables that may lead to a solution.
A binary constraint is said to be arc consistent if for every value in the domain of every
variable, there exists a value in the domain of the other such that the pair of values satisfies
the constraint between the variables. A non-binary constraint is generalised arc consistent
if and only if for any value for a variable in its scope, there exists a value for every other
variable in the scope such that the tuple satisfies the constraint (Rossi et al., 2006a). A
csp is said to be Arc Consistent (ac) if all its constraints are (generalised) arc consistent.
A csp is said to be Singleton Arc Consistent (sac) if it has non-empty domains and for
any assignment of a variable the resulting subproblem can be made ac (Bessiere, Stergiou,
& Walsh, 2008). Mixed consistency means maintaining different levels of consistency on
different variables of a problem. It has been shown that maintaining sac on some variables
and ac on the remaining variables of certain problems, such as job shop scheduling and
radio link frequency assignment, can reduce the solution time (Lecoutre & Patrick, 2006).
Various generalisations of csps have been developed, where the objective is to find a
solution that is optimal with respect to certain criteria such as costs, preferences or priorities.
One of the most significant is the Constraint Optimisation Problem (cop). Here the goal
is to find an optimal solution that either maximises or minimises an objective function
depending upon the problem. The simplest cop formulation retains the csp limitation of
allowing only hard constraints but adds an objective function over the variables.
A depth-first branch and bound search algorithm is generally used to find a solution
of a cop having an optimal value. In the case of maximisation, branch and bound search
algorithm keeps the current optimal value of the solution while traversing the search tree.
This value is a lower bound on the optimal value of the objective function. At each node of
the search tree, the search algorithm computes an overestimation of the global value. This
value is an upper bound on the best solution that extends the current partial solution. If
the lower bound is greater than or equal to the upper bound, then a solution of a greater
274

fiApproaches for Solving a Telecommunications Feature Subscription Problem

value than the current optimal value cannot be found below the current node, so the current
branch is pruned and the algorithm backtracks.

3. Configuring Feature Subscriptions
In Distributed Feature Composition (dfc) each feature is implemented by one or more
modules called Feature Box Types (fbt) and each fbt has many run-time instances called
feature boxes. For simplicity, in this paper we assume that each feature is implemented by
a single feature box and we associate features with feature boxes.

SOURCE REGION

TARGET REGION
features

CATALOGUE

CL
OCS
OCS

TCS TDR CFU

CL

CL

<

TCS

<

TDR

<

CL

<

<

<>

CFU

SUBSCRIPTIONS

CONFIGURATION
target sub. of Y

source sub. of X

OCS

TDR

TCS

target sub. of Z

CL

TCS

ROUTING

zone of X

ZONES

feature
box types

X

src=x
trg=y

OCS

feature
boxes
zone of Y

src=x
trg=y

TDR

TCS

Y
zone of Z

src=x
trg=z

CL

src=x
trg=z

TCS

src=x
trg=z

Z

Figure 2: DFC: Catalogues, subscriptions and sessions.
Dfc establishes a dialogue between endpoints by routing a set-up request encapsulating source and target addresses that are associated with source and target feature boxes
respectively. Addresses may change along the way and dfc routers evolve the connection
path accordingly. Starting from the feature box initiating the call, feature boxes are incorporated one after the other until a terminating box is reached. A router is used at each
step to locate the next box and relay the set-up request. As shown in the third row of
Figure 2, the routing method decomposes the connection path into a source and a target
region and each region is further partitioned into zones. A source (target) zone is a sequence
of feature boxes that execute for the same source (target) address. The first source zone
is associated with the source address encapsulated in the initial set-up request, i.e, zone of
275

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

X in Figure 2. A change of source address in the source region, caused for instance by an
identification feature, triggers the creation of a new source zone. If no such change occurs
and the zone cannot be expanded further, routers switch to the target region. Likewise,
a change of target address in the target region, as performed by Time-Dependent-Routing
(tdr) in Figure 2, triggers the creation of a new target zone. If no such change occurs and
the zone cannot be expanded further as for Z in Figure 2, the request is sent to the final
box identified by the encapsulated target address.
Dfc routers are only concerned with locating feature boxes and assembling zones into
regions. They do not make decisions as to the type and ordering of feature boxes appearing
in a zone. They simply fetch this information from the pre-configured feature subscription
that is associated with the address and region of the zone and use it to construct the zone.
For instance, the zone of Z in Figure 2 results from the sequence of feature box types
subscribed to by Z in the target region.
Subscriptions are pre-configured from the feature catalogue published by the service
provider. The catalogue is a set of features. Features are classified as source, target or
reversible (i.e., a subset of features that are both source and target) based on whether
they can be subscribed to in the source region, the target region or both. For instance,
the catalogue shown in the first row of Figure 2 includes Originating-Call-Screening (ocs)
as a source feature, Terminating-Call-Screening (tcs), Time-Dependent-Routing (tdr),
and Call-Forwarding-Unconditional (cfu) as target features, and Call-Logging (cl) as a
reversible feature. A source feature is activated on behalf of a caller while a target feature
is activated on behalf of a callee.
Constraints are formulated by designers on pairs of source features and pairs of target
features to prevent undesirable feature interactions (Zave, 2003). A precedence constraint
imposes a routing order between two features. The order is specified with respect to the
direction of an outgoing call if the features are source (e.g., ocs must precede cl in Figure 2)
and with respect to the direction of an incoming call if the features are target (e.g., cl must
precede tcs). An exclusion constraint makes two features mutually exclusive, as for the
case of cl and cfu in Figure 2. We encode an exclusion constraint between two features fi
and fj as the pair of precedence constraints fi  fj and fj  fi . For the sake of simplicity,
we treat precedence constraints as ordered pairs, i.e., the precedence constraint fi  fj is
also viewed as hfi , fj i.
Definition 1 (Catalogue). A catalogue is a tuple hFs , Hs , Ft , Ht i where:
 Fs is the finite set of source features,
 Ft is the finite set of target features,
 Fs  Ft is the finite set of reversible features,
 Hs is the set of source precedence constraints over Fs , and
 Ht is the set of target precedence constraints over Ft .
The source (target) subscription associated with an address is a subset of source (target) catalogue features, a set of catalogue precedence constraints between source (target)
features, and a set of user precedence constraints between source (target) features. For
276

fiApproaches for Solving a Telecommunications Feature Subscription Problem

instance, the target subscription of Y shown in the second row of Figure 2 includes the
target features tdr and tcs and the user precedence tdr  tcs meaning that tdr should
appear before tcs in the connection path.
Definition 2 (Feature Subscription). Given a catalogue hFs , Hs , Ft , Ht i, a feature subscription is defined to be a pair of tuples Ss = hFs , Hs , Ps i and St = hFt , Ht , Pt i where:
 Fs and Ft are the user selected source and target features respectively such that Fs 
Fs , Ft  Ft and Fs  Ft = Ft  Fs , i.e., any reversible feature in Fs  Ft appears in
both Fs and Ft ;
 Hs is the set of source catalogue precedence constraints in Fs given by Hs = Hs Fs
{(f  g)  (Fs  Ft )2 : g  f  Ht };
 Ht is the set of target catalogue precedence constraints in Ft given by Ht = Ht Ft
{(f  g)  (Fs  Ft )2 : g  f  Hs };
 Ps is the set of source user precedence constraints over Fs , which satisfies Ps  {(f 
g)  (Fs  Ft )2 : g  f  Pt };
 Pt is the set of target user precedence constraints over Ft , which satisfies Pt  {(f 
g)  (Fs  Ft )2 : g  f  Ps }.
Configuring a feature subscription involves selecting, parameterising and sequencing
features in each region consistently with the catalogue constraints and other integrity rules
(Jackson & Zave, 2003). In particular, the source and target regions of a subscription must
include the same reversible features in inverse order, i.e. source and target regions are not
configured independently.
Definition 3 (Consistency of Feature Subscriptions). We say that a feature subscription
S = hhFs , Hs , Ps i, hFt , Ht , Pt ii is consistent if and only if there exists a strict total order Ts
on Fs and a strict total order Tt on Ft such that
1. Ts  Hs  Ps
2. Tt  Ht  Pt
3. for all f, g  Fs  Ft , f  g  Ts  g  f  Tt .
The following configuration services may be provided to users submitting a feature
subscription:
 (verification) Check the consistency of the subscription.
 (filtering) If the feature subscription is consistent, then compute its anti-subscription,
i.e., the set of features and precedence constraints that would make it inconsistent if
added.
 (partial completion) If the feature subscription is consistent, then compute the
transitive closure of each region, i.e., (Hs  Ps ) and (Ht  Pt ) .
277

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

 (completion) If the feature subscription is consistent, then compute a pair of strict
total orders on source and target features such that points 1, 2 and 3 of Definition 3
are respected.
 (relaxation) If the feature subscription is inconsistent, then suggest consistent subscriptions obtained out of it by removing one more features or user precedences.
We formalise these tasks in the next section and describe their time complexities after
reformulating the original definition of feature subscription.

4. Reformulating the Original Definition of Feature Subscription
By definition, a catalogue includes two sets of features and two sets of precedence constraints. In this section, we reformulate a catalogue by merging its source and target
feature sets and by merging its source and target precedence sets. We transform feature
subscriptions accordingly and show that the consistency of a subscription is equivalent to
the acyclicity of its transformation. The new definitions are simpler and this reformulation
allows us to establish relations with the other well-known problems existing in the literature.
The principle of the reformulation of a catalogue is to inverse and merge the target
precedences with the source precedences. Specifically, a catalogue hFs , Hs , Ft , Ht i is rect i, where H
ct is the transpose of Ht such that
formulated as hFc , Hc i  hFs  Ft , Hs  H
2
ct . The definitions of (consistent) feature subscription
hi, ji  Ft : hi, ji  Ht  hj, ii  H
are adapted as follows.
Definition 4 (Feature Subscription). A feature subscription S of catalogue hFc , Hc i is a
tuple hF, H, P i, where F  Fc , H = Hc F , and P is a set of (user defined) precedence
constraints on F .
Definition 5 (Consistency of the Reformulated Feature Subscription). A feature subscription hF, H, P i of a catalogue hFc , Hc i is defined to be consistent if and only if there exists a
total order T on F such that T  H  P .
Definition 6 (Corresponding Subscription). Let hFs , Hs , Ft , Ht i be an original catalogue
ct i be its reformulation. Given a feature subscription S o =
and hFc , Hc i  hFs  Ft , Hs  H
o
o
o
o
o
o
hhFs , Hs , Ps i, hFt , Ht , Pt ii of catalogue hFs , Hs , Ft , Ht i and a feature subscription S r =
hF r , H r , P r i of the catalogue hFc , Hc i, we say that S r corresponds to S o if the following
co , and P r = P o  P
co .
holds: F r = Fso  Fto , H r = Hso  H
s
t
t
Due to the composition of the source and target catalogues into a single catalogue, a
feature subscription is consistent if and only if both source and target regions are consistent
in the DFC sense.
Proposition 1 (Equivalence of Subscription Consistency). Let hFs , Hs , Ft , Ht i be an origct i be its reformulation. A feature subscription
inal catalogue and hFc , Hc i  hFs  Ft , Hs  H
o
o
o
o
o
o
o
S = hhFs , Hs , Ps i, hFt , Ht , Pt ii of catalogue hFs , Hs , Ft , Ht i is consistent if and only if
the corresponding subscription S r = hF r , H r , P r i of catalogue hFc , Hc i is consistent.
278

fiApproaches for Solving a Telecommunications Feature Subscription Problem

co , and P r = P o  P
co .
Proof. From Definition 6 we have F r = Fso  Fto , H r = Hso  H
s
t
t
r
r
r
r
() If S is consistent then there exists a total order T on F such that T  H r  P r .
r o . Both T o and T o are total orders on F o and F o
Let Tso = T r Fso and let Tto = T\
Ft
s
t
s
t
co on F o  F o ,
respectively. Since, Tso  Hso  Pso , Tto  Hto  Pto , and Tso is equivalent to T
s
t
t
S o is also consistent.
() If S o is consistent then there exist two total orders Tso and Tto on Fso and Fto respectively
co on Fso  Fto . We will
such that Tso  Hso  Pso , Tto  Hto  Pto , and Tso is equivalent to T
t
o
r
o
c is acyclic. This implies that S is consistent (see Definition 5), since
prove that Ts  T
t
co . Note that, for
T r  H r  P r , where T r is any total order on F r extending Tso  T
t
co if and only if hf, f 0 i  Tso . We will prove that
f, f 0  Fso we have hf, f 0 i  Tso  T
t
co is acyclic by contradiction. Assume that Tso  T
co is not acyclic. Thus there exists
Tso  T
t
t
a cycle, and, in particular, a cycle of minimum cardinality, say, k. Therefore there exists
co , where we define
some f1 , . . . , fk ,  F r such that for all i = 0, . . . , k, hfi , fi+1 i  Tso  T
t
fk+1 = f1 and f0 = fk . Suppose that fi  Fso \ Fto for some i  1. Then, we must have
hfi1 , fi i  Tso and hfi , fi+1 i  Tso which implies that hfi1 , fi+1 i  Tso by transitivity of Tso .
But then we still have a cycle if we omit fi , which contradicts the minimality of the cycle
length k. We have shown, for all i  1, that fi  Fto and so hfi , fi+1 i  Tto . Transitivity of
Tto implies that hf1 , fk+1 i  Tto , i.e., hf1 , f1 i  Tto , which contradicts Tto being a strict total
order.

Proposition 2 (Complexity of Consistency Checking). Determining whether a feature subscription hF, H, P i is consistent or not can be checked in O(|F | + |H| + |P |).
Proof. We use Topological Sort (Cormen, Leiserson, & Rivest, 1990). In Topological Sort
we are interested in ordering the nodes of a directed graph such that if a directed edge
hi, ji is in the set of edges of the graph then node i is less than node j in the order. In
order to use Topological Sort for detecting whether a feature subscription is consistent, we
associate nodes with features and edges with precedence constraints. Then, the subscription
is consistent if and only if for all edges hi, ji in the graph associated with the subscription,
i precedes j in the order computed by Topological Sort. As the complexity of Topological
Sort is linear with respect to the size of the graph (i.e., the sum of the number of nodes and
the number of edges of the graph) detecting whether a feature subscription is consistent is
O(|F | + |H| + |P |).
Definition 7 (Anti-subscription). Given a catalogue hFc , Hc i and a consistent feature subscription S = hF, H, P i, the anti-subscription is the tuple hFa , Pa i defined as follows. f  Fc
is an element of Fa if and only if the directed graph associated with the subscription obtained after adding feature f , i.e., hF  {f }, Hc F {f }  P i, is cyclic;  i, j  F , i  j is
in Pa if and only if the directed graph associated with the subscription obtained after adding
precedence i  j, i.e., hF  {i, j}, Hc F {i,j}  P  {i  j}i, is cyclic.
The definition of anti-subscription suggests one way of computing the anti-subscription
of a given subscription. In order to test whether a feature/precedence belongs to the antisubscription we check the consistency of the resulting subscription. As there are O(|Fc |)
279

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

features and O(|Fc |2 ) precedences, the worst-case time complexity of computing an antisubscription is at most O(|Fc |2  (|F | + |H| + |P |)).
Definition 8 (Partial Order of a Consistent Subscription). Given a consistent subscription
hF, H, P i, the partial order of the subscription is the transitive closure (H  P ) of the
relation H  P .
The worst-case complexity of finding this transitive closure is O(|F |3 ).
Definition 9 (Total Order of a Consistent Subscription). A total order of consistent subscription S is a topological sort of the directed graph hF, H  P i, i.e., a total order extending
the relation H  P .
The worst-case complexity of finding such a total order is linear in time with respect to
the size of the corresponding graph.

5. Symmetry Inherent in the Reformulation
One of the services provided to an end-user when configuring a feature subscription is
the computation of all compatible pairs of total orders on source and target features. In
this section, we show that when an original subscription, as defined in Section 3, is reformulated, as described in Section 4, symmetries are introduced. Two total orders in
the reformulated subscription are symmetric if they correspond to the same pair of total orders (on source and target features) in the original subscription. More formally,
let S o = hhFso , Hso , Pso i, hFto , Hto , Pto ii be a subscription of the catalogue hFs , Hs , Ft , Ht i,
and S r = hF r , H r , P r i be the corresponding subscription of the catalogue hFc , Hc i 
ct i, i.e., F r = F o  F o , H r = H o  H
co , and P r = P o  P
co . A pair of
hFs  Ft , Hs  H
s
t
s
s
t
t
o
total orders hTs , Tt i is compatible with S if Conditions (1), (2) and (3) of Definition 3
hold. There is a many-to-one relation between the set of total orders of S r (see Definition
9) and the set of compatible pairs of total orders of S o .
Let us consider the subscription S o where Fso = {1, 2, 3}, Fto = {2, 3, 4}, Hso = {1  2},
o
Ht = {4  3}, Pso and Pto are empty. The corresponding S r would have F r = {1, 2, 3, 4},
H r = {1  2, 3  4}, and P r = . Both S o and S r are consistent. The set of total orders
of S r , and the set of compatible pairs of total orders of S o are shown in Table 1. The
cardinality of the former set is six, while for the latter is only five. The last two total orders
of S r correspond to the last compatible pair of total orders of S o . This is due to the fact
that the union of a total order on source features and the transpose of a total order on
target features in S o is not necessarily a total order. For example for the last pair of total
orders of S o in Table 1, the union of 3  1  2 and 3  4  2 do not result in a total order,
since there is no order between 1 and 4.
The repetition of the computation of the symmetric pairs of total orders of the original subscription from the total orders of the reformulated subscription is not desirable.
In order to compute a compatible pair of total orders only once, we use the algorithm
GetSolutions(S r ), as shown in Algorithm 1. This algorithm has two nested loops. In
the first loop it selects a total order on the set of reversible features and then extends this
total order to generate a set of total orders on source features and a set of total orders on
target features. In the second loop a total order on source features and a total order on
280

fiApproaches for Solving a Telecommunications Feature Subscription Problem

Table 1: Total orders on F r , Fso , and Fto .
Sr
So
r
o
F
Fs
Fto
1234
123 432
1324
132 423
1342
132 243
3124
312 423
3142
312 243
3412

target features are selected from the previously generated sets. Due to the fact that the
source features and the target features are ordered independently in GetSolutions(S r ),
no unnecessary ordering is imposed between the source features and the target features.
Algorithm 1 GetSolutions(S r )
Require:
 S r = hF r , H r , P r i is a consistent subscription, where F r = Fso  Fto , Fso is the set
of source features, Fto is the set of target features, and Fro = Fso  Fto is the set of
reversible features in a corresponding subscription So .
 GetTotalOrders(hF, Oi) generates the set of all total orders that extend a
given acyclic binary relation O defined on a set of features F .
 , R , S , and T are set to (H r  P r ) , Fro , Fso , and Fto respectively.
Ensure: PTOs is the set of pairs of compatible total orders on Fso and Fto respectively.
1: PTOs  
2: RTOs  GetTotalOrders(hFro , R i)
3: for all r  RTOs do
4:
STOs  GetTotalOrders(hFso , S  r i)
5:
TTOs  GetTotalOrders(hFto , T  r i)
6:
for all s  STOs, t  TTOs do
ct i}
7:
PTOs  PTOs  {hs , 
8: return PTOs
The algorithm computes and saves all total orders on a given set of reversible features in
RT Os, and for a given total order on the set of reversible features it computes and saves all
the total orders on source and target features in ST Os and T T Os respectively. However,
this is presented in the algorithm for the purpose of clarity. In practice, a total order is
computed lazily, i.e., a total order is only computed when is needed, thus avoiding the need
of keeping all the total orders generated in memory.
The amortised time complexity of computing all the total orders extending a given
acyclic binary relation is linear with respect to the number of total orders (Pruesse &
Ruskey, 1994). Assuming that there are r total orders on Fro and at most s , and t total
281

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

orders on Fso and Fto that are consistent with a given total order on Fro respectively, the
time complexity of GetSolutions is O(r  s  t ). The computation of all the pairs of
compatible total orders could be impractical when the size of the resulting set is very large.
Therefore, in those cases the computation of the number of total orders could be restricted
to a pre-specified number, and a heuristic can be used to select r in Line 3, and s and
t in Line 6 of Algorithm 1.
There may be some pairs of total orders on Fso and Fto that are more desirable than
others. For instance, it would be more desirable to present an end-user those pairs of
total orders that are more easy to extend (in terms of the addition of a feature or a user
precedence). One way of doing this is to use the notion of anti-subscription (see Definition
7). Each pair of total orders can be associated with an anti-subscription. The size of the
anti-subscription is the sum of the number of features and precedences that are involved
in it. The pairs of total orders can be ordered in the increasing size of their corresponding
anti-subscriptions. The size of an anti-subscription in some sense reflects how constrained
a pair of total orders is with respect to the future addition of the number of features and
user precedences that an end-user may consider in his/her subscription in the future.

6. Relaxations of Feature Subscriptions
If an input feature subscription is not consistent then the goal is to relax it by dropping one
or more features or user precedence constraints to generate a consistent feature subscription
that is closest to the initial users requirements. Therefore, we introduce a function w :
F  P  N that assigns weights to features and user precedence constraints, indicating
the importance to the user of the features and user precedences. These weights could be
elicited directly through data mining or analysis of user interactions. In the rest of the
paper a feature subscriptionPis denoted byPS = hF, H, P, wi. The value of the subscription
S is defined by Value(S) = f F w(f ) + P w().
Definition 10 (Relaxation). A relaxation of a feature subscription hF, H, P, wi of a catalogue hFc , Hc i is a subscription hF 0 , H 0 , P 0 , w0 i such that F 0  F , H 0 = HF 0 , P 0  P F 0
and w0 is w restricted to F 0  P 0 .
Definition 11 (Optimal Relaxation). Let RS be the set of all consistent relaxations of a
feature subscription S. We say that Si  RS is an optimal relaxation of S if it has maximum
value among all consistent relaxations, i.e., if and only if there does not exist Sj  RS such
that Value(Sj ) > Value(Si ).
Proposition 3 (Complexity of Finding an Optimal Relaxation). Finding an optimal relaxation of a feature subscription is NP-hard.
Proof. Given a directed graph G = hV, Ei, the Feedback Vertex Set Problem is to find a
smallest V 0  V whose deletion makes the graph acyclic. This problem is known to be NPhard (Garey & Johnson, 1979). We prove that finding an optimal relaxation is NP-hard
by a reduction from the feedback vertex set problem. The feedback vertex set problem can
be reduced to our problem by associating the nodes of the directed graph V with features
F , the edges E with catalogue precedence constraints H. We set P to  and define w by
w(f ) = 1, for all f  F . Thus, finding an optimal relaxation of S = hF, H, P, wi corresponds
282

fiApproaches for Solving a Telecommunications Feature Subscription Problem

to finding a biggest set of nodes V 00 such that the deletion of V  V 00 from G results in an
acyclic graph. Therefore, we conclude that finding an optimal relaxation of an inconsistent
subscription is NP-hard.
The most challenging operation on feature subscriptions is to find an optimal relaxation
of a subscription that is not consistent, since it is NP-Hard. In the remainder of the paper
we focus only on this particular task.

7. Basic COP Model for Finding an Optimal Relaxation
In this section we model the problem of finding an optimal relaxation of a feature subscription hF, H, P, wi of catalogue hFc , Hc i as a constraint optimisation problem (Lesaint,
Mehta, OSullivan, Quesada, & Wilson, 2008c).
Variables and Domains. We associate each feature i  F with two variables: a Boolean
variable bfi and an integer variable pfi . A Boolean variable bfi is instantiated to 1 or 0
depending on whether feature i is included in the subscription or not, respectively. The domain of each integer variable pfi is {1, . . . , |F |}. Assuming that the computed subscription
is consistent, an integer variable pfi corresponds to the position of the feature i in a sequence, which is consistent with the optimal relaxation. We associate each user precedence
constraint (i  j)  P with a Boolean variable bpij . A Boolean variable bpij is instantiated
to 1 or 0 depending on whether i  j is respected in the computed subscription or not,
respectively. A variable v is associated with the value of the subscription, the initial lower
bound of which is 0 and the initial upper bound is the sum of the weights of all the features
and user precedences.
Constraints. A catalogue precedence constraint (i  j)  H that feature i should be
before feature j can be expressed as follows:
bfi  bfj  (pfi < pfj ).
Note that the constraint is activated only if the selection variables bfi and bfj are instantiated
to 1. A user precedence constraint (i  j)  P that i should be placed before j in their
subscription can be expressed as follows:
bpij  (bfi  bfj  (pfi < pfj )).
Note that if a user precedence constraint holds then the features i and j are included in the
subscription and also the feature i is placed before j, that is, the selection variables bfi and
bfj are instantiated to 1 and pfi < pfj is true.
The value of the subscription is equal to the sum of the weights of the included features
and included user precedences. This constraint can be expressed as the following:
X
X
v=
bfi  w(i) +
bpij  w(i  j).
(1)
iF

(ij)P

Enforcing arc consistency on Equation (1), in general, is exponential (Zhang & Yap, 2000).
Therefore, cp solvers perform only bounds consistency on this constraint, which is equivalent
283

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

to enforcing arc consistency on the the following pair of constraints, which can be seen as
a decomposition of Equation (1):
X
X
bpij  w(i  j).
(2)
v
bfi  w(i) +
iF

v

X

(ij)P

bfi  w(i) +

iF

X

bpij  w(i  j).

(3)

(ij)P

In order to reason about the complexities of enforcing different consistency techniques we
always assume that the two inequality constraints are used instead of the equality constraint.
Objective.

The objective is to find an optimal relaxation of a feature subscription.

We have investigated the impact of maintaining three different levels of consistency
within branch and bound search. The first is arc consistency and the rest are mixed consistencies. In the following sections we shall describe these consistency techniques and present
their worst-case time complexities when enforced on any instance of feature subscription, if
formulated as described above. The results for the complexities that are presented below
are based on the assumption that only the Boolean variables associated with the inclusion/exclusion of features and user precedences are the decision variables. We remark that
if the problem is arc-consistent after instantiating all the Boolean variables then it is also
globally consistent.
7.1 Arc Consistency
Let e be the sum of the number of user precedences and the number of catalogue precedences,
let n be the sum of the number of features and the number of user precedences, and
let d be the number of features. The complexity of achieving arc consistency (ac) on a
(catalogue/user) precedence constraint is constant with respect to the number of variables.
A catalogue precedence constraint is made arc-consistent when any of the Boolean variables
involved in the constraint is initialised or any of the domains of the position variables is
modified. Thus, a catalogue precedence constraint can be made arc-consistent at most
(1 + 1 + (d  1) + (d  1)) times, which is effectively 2d times. A user precedence constraint
can be made arc-consistent at most 2d + 1 times. Since there are, in total, e precedence
constraints, the worst-case time complexity of imposing arc consistency on all the precedence
constraints is O(e d), which is also optimal. In addition, arc consistency is also enforced
on the linear inequalities (2) and (3), the complexity of which is linear with respect to the
number of Boolean variables. Whenever a Boolean variable is instantiated the constraint
is revised and since there are n Boolean variables, it can be made arc-consistent at most n
times. Therefore, the worst-case time complexity of enforcing arc consistency on the linear
inequalities is O(n2 ), which is optimal. Thus, the worst-case time complexity of enforcing
ac on an instance of basic cp model for finding an optimal relaxation is O(e d + n2 ).
7.2 Singleton Arc Consistency
Maintaining a higher level of consistency can be expensive in terms of time. However, if
more values can be removed from the domains of the variables, the search effort can be
284

fiApproaches for Solving a Telecommunications Feature Subscription Problem

reduced and this may save time. We shall investigate the effect of maintaining Singleton
Arc Consistency (sac) on the Boolean variables and ac on the remaining variables and
denote it by sacb . We have used the sac-1 (Debruyne & Bessiere, 1997) algorithm for
enforcing sac on the Boolean variables. Enforcing sac on the Boolean variables in a sac-1
manner works by traversing a list of 2n variable-value pairs. For each instantiation of a
Boolean variable x to each value 0/1, if there is a domain wipeout while enforcing ac then
the value is removed from the corresponding domain and ac is enforced. Each time a value
is removed, the list is traversed again. Since there are 2n variable-value pairs, the number
of calls to the underlying arc consistency algorithm is at most 4n2 . Thus the worst-case
time complexity of sacb is O(n2 (e d + n2 )).
sacb does not have an optimal worst-case time complexity. In sacb arc consistency can
be enforced on a subproblem obtained by restricting a Boolean variable to a single value at
most 2n times, and each time arc consistency is established from scratch. However, one can
take the incremental property of arc consistency into account to obtain an optimal version
of sacb . Following the work of Lecoutre (2009) an arc consistency algorithm is said to be
incremental if and only if its worst-case time complexity is the same when it is applied once
on a given network P and when it is applied up to m times on P where between any two
consecutive executions, at least one value has been deleted. Here m is the sum of the domain
sizes of all the variables involved in the problem P . The idea behind an optimal version
is that we do not want to achieve arc consistency from scratch in each subproblem, but,
instead, benefit from the incremental property of the underlying arc consistency algorithm.
This results in the asymptotic complexity of O(e d + n2 ) for enforcing arc consistency 2n
times. Thus, the time complexity of an optimal version of sacb would be O(n (e d + n2 )).
7.3 Restricted Singleton Arc Consistency
The main problem with sac-1 is that deleting a single value triggers the loop again. The
Restricted Singleton Arc Consistency (rsac) avoids this by considering each variable-value
pair only once (Prosser, Stergiou, & Walsh, 2000). We investigate the effect of enforcing
(rsac) on the Boolean variables and ac on the remaining variables, and denote it by rsacb .
The worst-case time complexity of rsacb is O(n (e d + n2 )).

8. Other CP Models
In this section we present two more cp approaches. The first approach uses a global constraint that achieves a higher level of consistency by taking into account the cycles of the
precedence constraints. In the second approach we model the problem as a weighted constraint satisfaction problem.
8.1 Global Constraint
A global constraint captures a relation between several variables. It takes into account the
structure of the problem to prune more values. For instance, if a user has selected a set of
features, F = {1, 2, 3, 4} and if these features are constrained by the catalogue precedences
1  2, 2  1, 3  4 and 4  3, and if three features are required to be included in the
subscription then one can infer that the problem is inconsistent without doing any search.
285

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

This is possible by inferring cycles from the precedence constraints and using them to prune
the bounds of the objective function.
The soft global precedence constraint SoftPrec was proposed by Lesaint et al. (2008a).
It holds if and only if there is a strict partial order on the selected features subject to the
relevant hard (catalogue) precedence constraints and the selected soft (user) precedence
constraints, and the value of the subscription is within the provided bounds. As shown
by Lesaint et al. (2008a), achieving ac for SoftPrec is NP-complete since there is no
way to determine in polynomial time whether there is a strict partial order whose value is
between the given bounds. Therefore, ac is approximated by pruning the domains of the
variables based on the filtering rules that follow from the definition of SoftPrec. The
time-complexity for achieving this pruning is O(|F |3 ), which is polynomial. The upper
bound of the value of the subscription is pruned based on the incompatibilities that are
inferred between pairs of features, and the dependencies between user precedences and
their corresponding features. The pruning rules of SoftPrec are used within branch and
bound search to find an optimal relaxation of a feature subscription.
Let hF, H, P, wi be a subscription. Let bf be a vector of Boolean variables associated
with F . We say that feature i is included if bf(i) = 1, and i is excluded if bf(i) = 0. We
abuse the notation by using bf(i) to mean bf(i) = 1, and bf(i) to mean bf(i) = 0. A similar
convention is adopted for the other Boolean variables. Let bp be a |F |2 matrix of Boolean
variables. Here bp is intended to represent a strict partial order on the included features F 0
which is compatible with the catalogue constraints restricted to F 0 .
Definition 12 (SoftPrec). Let S = hF, H, P, wi be a feature subscription, bf and bp be
vectors of Boolean variables, and v be an integer variable, SoftPrec(S, bf, bp, v) holds if
and only if
1. bp is a strict partial order restricted to bf, i.e.,
i, j  F : bp(i, j)  bf(i)  bf(j)
(restricted),
i, j  F : bp(i, j)  bp(j, i)
(asymmetric),
i, j, k  F : bp(i, j)  bp(j, k)  bp(i, k) (transitive),
2. bp is compatible with H restricted to bf, i.e.,
(i  j)  H : bf(i)  bf(j)  bp(i, j),
3. v =

P

iF

bf(i)  w(i) +

P

(ij)P

bp(i, j)  w(i  j).

The set of constraints in this cp model only contains SoftPrec. The decision variables
in this model are bf and bp. A solution of SoftPrec is a consistent relaxation of the
subscription hF, H, P, wi. Notice that the feedback vertex set problem (Garey & Johnson,
1979) can be expressed in terms of SoftPrec by associating vertices with features and arcs
with catalogue precedence constraints. Therefore, achieving generalised arc consistency on
SoftPrec is NP-hard.
286

fiApproaches for Solving a Telecommunications Feature Subscription Problem

8.2 Weighted CSP Model
The classical csp framework has been extended by associating weights (or costs) with
tuples (Larrosa, 2002). The Weighted Constraint Satisfaction Problem (wcsp) is a specific
extension that relies on a specific valuation structure S(k) defined as follows.
Definition 13 (Valuation Structure). S(k) is a triple ({0, . . . , k}, , ) where: k  {1, . . . , }
is either a strictly positive natural number or infinity, {0, 1, . . . , k} is the set of naturals less
than or equal to k,  is the sum over the valuation structure defined as: ab = min{k, a+b},
 is the standard order among naturals.
A wcsp instance is defined by a valuation structure S(k), a set of variables (as for
classical csp instances) and a set of constraints. A domain is associated with each variable
and a cost function with each constraint. More precisely, for each constraint C and each
tuple t that can be built from the domains associated with the variables involved in C, a
value in {0, 1, . . . , k} is assigned to t. When a constraint C assigns the cost k to a tuple
t, it means that C forbids t. Otherwise, it is permitted by C with the corresponding cost.
The cost of an instantiation of variables is the sum (using operator ) over all constraints
involving variables instantiated. An instantiation is consistent if its cost is strictly less
than k. The goal of the wcsp problem is to find a full consistent assignment of variables
with minimum cost. A wcsp formulation for finding an optimal relaxation of the input
subscription hF, H, P, wi, when inconsistent, is outlined below.
The maximum acceptable cost is
X
X
k=
w(i) +
w().
iF

P

We associate each feature i  F with an integer variable pfi . The domain of each integer
variable, D(pfi ), is {0, . . . , |F |}. If pfi is instantiated to 0, it indicates that i is excluded
from the subscription.
A unary cost function Ci : D(pfi )  {0, w(i)} assigns costs to assignments of variable
pfi in the following way:

0
if a > 0
Ci (a) =
w(i) if a = 0
A catalogue precedence constraint (i  j)  H is associated with a binary cost function
Hij : D(pfi )  D(pfj )  {0, k} that assigns costs to assignments of variables pfi and pfj in
the following way:

0 if a = 0  b = 0  a < b
Hij (a, b) =
k otherwise
A user precedence constraint (i  j)  P is associated with a binary cost function Pij :
D(pfi )  D(pfj )  {0, w(i  j)} assigns costs to assignments of variables pfi and pfj in the
following way:

0
if a 6= 0  b 6= 0  a < b
Pij (a, b) =
w(i  j) otherwise
Note that if a user precedence constraint holds then the features i and j are included in the
subscription and also the feature i is placed before j, that is, the integer variables pfi and
pfj are instantiated to any value greater than 0 and pfi < pfj is true.
287

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

9. Boolean Satisfiability
The Boolean Satisfiability Problem (sat) is a decision problem an instance of which is an
expression in propositional logic. The problem is to decide whether there is an assignment
of true and false values to the variables that will make the expression true. The expression
is normally written in conjunctive normal form. The Partial Weighted Maximum Boolean
Satisfiability Problem (pwmsat) is an extension of sat that includes the notions of hard and
soft clauses. Any solution should respect the hard clauses. Soft clauses are associated with
weights. The goal is to find an assignment that satisfies all the hard clauses and minimises
the sum of the weights of the unsatisfied soft clauses. In this section we present Boolean
satisfiability formulations for finding an optimal relaxation of a feature subscription.
9.1 Atom-based Encoding
In an atom-based encoding, each atom, like f  g, is associated with a propositional variable
and the asymmetricity and transitivity properties of the precedence relation are explicitly
encoded. An atom-based encoding of finding an optimal relaxation of a feature subscription
hF, H, P, wi is outlined below.
Variables. Let PrecDom be the set of possible precedence constraints that can be defined
on F , i.e., {i  j : {i, j}  F  i 6= j}). For each feature i  F there is a Boolean
variable bfi , which is true or false depending on whether feature i is included or not in the
computed subscription. For each precedence constraint (i  j) there is a Boolean variable
bpij , which is true or false depending on whether the precedence constraint holds or not in
the computed subscription. If bpij is true, then, roughly speaking, it means that features i
and j are included, and i precedes j.
Clauses. Each weighted-clause is represented by a tuple hw, ci, where w is the weight of
the clause c. Note that the hard clauses are associated with weight >, which represents an
infinite penalty for not satisfying them.
Each catalogue precedence constraint, (i  j)  H, must be satisfied if the features i
and j are included in the computed subscription. This is modelled by adding the following
hard clause:
h>, (bfi  bfj  bpij )i.
The precedence relation should be transitive and asymmetric in order to ensure that
the subscription graph is acyclic. To ensure asymmetricity, the following clause is added
for every pair {i  j, j  i}  PrecDom:
h>, (bpij  bpji )i.

(4)

Both bpij and bpji can be false. However, if one of them is true the other one should be
false.
To ensure transitivity, for every {i  j, j  k}  PrecDom, the following clause is added:
h>, (bpij  bpjk  bpik )i.

(5)

Note that Rule (5) need only be applied to hi, j, ki such that i 6= k since precedence constraints are not reflexive because of Rule (4).
288

fiApproaches for Solving a Telecommunications Feature Subscription Problem

Each precedence constraint (i  j)  PrecDom is only satisfied when its corresponding
features i and j features are included. This is ensured by considering the following clauses:
h>, (bpij  bfi )i

h>, (bpij  bfj )i.

We need to penalise any solution that does not include a feature i  F or a user precedence
constraint (i  j)  P . This is done by adding the following clauses:
hw(i), (bfi )i

hw(i  j), (bpij )i.

The cost of violating these clauses is the weight of the feature i and the weight of the user
precedence constraint i  j respectively.
Reducing the Variables and Clauses. It is straightforward to realise that the atom
based encoding described in the previous section requires (n2 ) Boolean variables and
(n3 ) clauses, where n is the number of features1 . We now describe two techniques which
can reduce the number of variables and clauses. The subscription contains a cycle if and
only if the transitive closure of H  P contains a cycle. Therefore, instead of associating
a Boolean variable with each possible precedence constraint, it is sufficient to associate
Boolean variables only with the precedence constraints in the transitive closure of H  P .
Reducing the Boolean variables will also reduce the transitive clauses, especially when the
input subscription graph is not dense. Otherwise, Rule (5) will generate |F |  (|F |  1) 
(|F |  2) transitivity clauses and Rule (4) will generate (|F |  (|F |  1))/2 asymmetricity
clauses. For example, for the subscription hF, H, P, wi with F = {1, 2, 3, 4, 5, 6}, H = {1 
2, 2  1, 3  4, 4  3, 5  6, 6  5}, and P = , Rules (4) and (5) will generate 120
transitivity clauses and 15 asymmetricity clauses respectively. Since any relaxation of the
given subscription respecting the clauses generated by Rule (4) is acyclic, the 120 transitivity
clauses and 12 asymmetricity clauses are redundant. Thus, if PrecDom is instead set to be
the transitive closure of H  P , then Rules (4) and (5) would not generate any redundant
clauses. We further reduce the number of transitivity clauses h>, (bpij  bpjk  bpik )i by
considering only those where none of j  i, k  j, and i  k are in H, especially when the
input subscription graph is not sparse. The reason for this is that these transitivity clauses
are always entailed due to the enforcement of the catalogue precedence constraints. This
reduction in the number of clauses might reduce the memory requirement and also might
have an impact on the efficiency of unit propagation, which in turn may reduce the runtime.
9.2 Symbol-based Encoding
Another sat approach based on a symbol-based encoding of partial order constraints is
presented by Codish et al. (2009). Partial order constraints (Codish, Lagoon, & Stuckey,
2008) are basically propositional formulae except that propositions can also be statements
about a partial order on a finite set of symbols. In a symbol-based encoding the transitivity
and asymmetricity properties of a precedence relation are enforced implicitly.
Here also a Boolean variable bfi is associated with each feature i  F indicating whether
i is included or excluded. A Boolean variable bpij is associated with each precedence
1. Given a function g(n), (g(n)) denotes the set of functions f (n) such that there exist positive constants
c1 , c2 and n0 such that 0  c1 g(n)  f (n)  c2 g(n) for all n  n0 (Cormen et al., 1990).

289

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

constraint (i  j)  H  P . For each catalogue constraint (i  j)  H the following clause
is added: h>, (bfi  bfj  bpij )i. For each precedence constraint i  j  (H  P ) the
following clauses are added: h>, (bpij  bfi )i and h>, (bpij  bfj )i. For each precedence
constraint i  j  (H  P ) the propositional constraint bpij  Ji  jK is encoded2 . This
intuitively means that if bpij is true then i precedes j. Two different ways of encoding
a precedence constraint Ji  jK are presented by Codish et al. (2009), which are called
the unary encoding and the binary encoding. A brief description of them is presented in
Section 9.2.1 and Section 9.2.2, which will provide a basis for their theoretical comparisons.
Advanced techniques for encoding the objective function have also been proposed by
Codish et al. (2009). However the encoding of the objective function is orthogonal to
the way the precedences are encoded. As our purpose is to compare the encoding of the
precedence constraints, we omit the details of the encoding of the objective function for the
symbol-based encoding proposed by Codish et al. (2009). Instead, we assume that in this
approach the objective function is encoded as it is done in the atom-based case. Therefore,
in the pwmsat setting the following soft clauses are added for features and user precedences:
hw(i), bfi i and hw(i  j), bpij i.
9.2.1 Unary Encoding
In the symbol-based unary encoding (Codish et al., 2009) each feature is associated with
an ordered set of Boolean variables that represents the unary encoding of its position. The
unary encoding of a non-negative integer m  n is an assignment of values to a sequence of
n Boolean variables hm1 , . . . , mn i such that m1  m2      mn . The integer-value of such
a representation is the number of variables mi taking value 1. For example, the sequence
11100000 represents the number m = 3 using n = 8 variables. For each pair of consecutive
variables in the sequence, say mk and mk+1 , a clause h>, (mk+1 mk )i is introduced to the
encoding in order to enforce that if mk+1 is assigned 1 then its predecessor in the sequence,
mk , must be assigned 1. Let i and j be two non-negative integer variables that can be
assigned values less than or equal to n. Let hi1 , . . . , in i and hj1 , . . . , jn i be the sequences of
n Boolean variables that represent the unary-encodings of i and j respectively. The unaryencoding of i  j is denoted by hi1 , . . . , in i  hj1 , . . . , jn i, which means that the number of
variables assigned the values 1 in the sequence hi1 , . . . , in i is less than the number of variables
assigned the values 1 in the sequence hj1 , . . . , jn i. Notice that hi1 , . . . , in i  hj1 , . . . , jn i holds
if and only if in holds, j1 holds, and hi1 , . . . , in i  hj2 , . . . , jn , 0i holds. Here hj2 , . . . , jn , 0i
encodes an integer between 0 and n  1, which is the predecessor of hj1 , . . . , jn i. The
inequality hi1 , . . . , in i  hj2 , . . . , jn , 0i can be encoded as follows: 1  k  n1, ik  jk+1 .
The resulting weighted clauses for bpij  Ji  jK are hbpij  in i, hbpij  j1 i, and
1  k  n  1, h>, (bpij  ik  jk+1 )i. Overall, the symbol-based unary encoding
requires (n2 ) propositional variables (n per feature) and involves (k n) clauses (n per
precedence constraint), where k = |H  P |.
9.2.2 Binary Encoding
In the symbol-based binary encoding each feature is associated with an ordered set of
Boolean variables that represents the binary log encoding of its position. The binary encod2. Ji  jK is a Boolean formula that is satisfiable if and only if i precedes j.

290

fiApproaches for Solving a Telecommunications Feature Subscription Problem

ing of a non-negative integer a  n is a sequence of values assigned
to k variables v1 , . . . , vk ,
P
where k = dlog2 ne. The value of such a representation is 1mk 2km  vm . For example, the sequence 101 represents the number 5 using 3 variables. A precedence constraint
is encoded using a lexicographical comparator (Apt, 2003). Given two numbers in binary
encoded form hi1 , . . . , ik i and hj1 , . . . , jk i, a precedence constraint hi1 , . . . , ik i < hj1 , . . . , jk i
holds if and only if there exists m > 0 such that im < jm and for all l < m, il = jl . The resulting encoding is not in conjunctive normal form. Therefore, the Tseitin transformation3
(Tseitin, 1968) is used to obtain the corresponding formula in conjunctive normal form.
For a given precedence constraint, the Tseitin transformation introduces (log n) variables
and clauses, since log n is the length of the formula associated with the given precedence
constraint. Overall, the symbol-based binary encoding requires (n log n) propositional
variables and involves (k log n) clauses, where k = |H  P |.
9.3 Comparison of the Encodings
Unit Propagation (up) is a central component of a search-based sat solver. Given a unit
clause l, unit propagation applies the following rules: (1) every clause containing l is removed, and (2) l is removed from every clause that contains this literal. These rules are
applied until a fixed-point is reached. The application of these two rules leads to a new set
of clauses that is equivalent to the old one. Unit propagation detects inconsistency when
an empty clause is generated.
Let ae, seu , and seb denote the atom-based encoding, the symbol-based unary encoding,
and the symbol-based binary encoding respectively. The difference between these encodings
is the way they encode acyclicity. In ae acyclicity is encoded explicitly by adding transitivity
and asymmetricity clauses. In seu and seb acyclicity is encoded implicitly by associating
each feature with a set of Boolean variables that represent its position (an integer value)
and a precedence constraint is expressed in terms of these positions. The Boolean variables
denoting the inclusion (or exclusion) of features and user precedences are called problem
variables. These variables are common to all the encodings. An optimal relaxation can
be expressed in terms of the problem variables. In order to show that unit propagation
on one encoding is stronger than unit propagation on another encoding, we need to map
the decisions of one encoding to the other one. Unfortunately, it is not possible to map
the decisions between the atom-based and the symbol-based encodings. For example, an
assignment of a position variable in the symbol-based encodings cannot be expressed in
terms of the assignments to the variables of ae. Nevertheless, in the following, we prove that
unit propagation in ae is stronger than unit propagation in seb when a set of assignments
are restricted to the problem variables.
Proposition 4. Given a set of assignments restricted to the problem variables, if unit
propagation detects inconsistency in seb then it also detects inconsistency in ae, but the
converse is not true.
3. Given a propositional formula, the Tseitin transformation obtains an equivalent formula in conjunctive
normal form by associating a new variable with every subformula of the original formula and applying
the following equivalences: (i) s0  (s1 s2 )  {(s0 s1 s2 ), (s0 s1 ), (s0 s2 )}, (ii) s0  (s1 s2 ) 
{(s0  s1  s2 ), (s0  s1 ), (s0  s2 )}, and (iii) s0  s1  {(s0  s1 ), (s0  s1 )}.

291

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

Proof. The atom-based and the symbol-based binary encoding differ only on the encoding of the acyclicity, i.e., the encoding of the transitivity and asymmetricity properties of
the precedence relation. In the symbol-based binary encoding transitivity and asymmetricity properties are implicitly captured by the clauses corresponding to the propositional
constraints of the form bpij  Ji  jK. Therefore, in order to prove that if up detects inconsistency in seb then it also detects inconsistency in ae, it is sufficient to show that if bpij is
falsified due to violation of Ji  jK in seb under unit propagation, the same happens in ae.
The clauses corresponding to Ji  jK are not defined in terms of the problem variables and
none of these clauses are unary4 . Therefore, up can not falsify bpij in seb . This trivially
implies that, when only a set of problem variables are instantiated, up in ae detects any
inconsistency that is detected by up in seb .
Now we show that there exists a case where an inconsistency is detected by up in
ae but it is not detected in seb . Let F = {i, j, k} be a set of features, H = , and
P = {i  j, j  k, k  i} be a set of user precedence constraints. In all the encodings we
have a Boolean variable per user precedence constraint: bpij , bpjk and bpki and we assume
that bpij , bpjk and bpki are set to true. In ae the unit resolution of bpij and bpjk with
the transitive clause bpij  bpjk  bpik yields bpik , and the unit-resolution of bpik with
bpki  bpik yields bpki , which results in an empty clause when resolved with bpki . In
seb , an ordered set of Boolean variables is associated with each feature. As there are 3
features, two Boolean variables are required per feature. Therefore each feature i, j and
k is associated with hi1 , i2 i, hj1 , j2 i, and hk1 , k2 i respectively that are used to encode a
precedence constraint. For each precedence constraint, say i  j, a set of clauses that
encode the propositional constraint bpij  (i1  j1 )  ((i1  j1 )  (i2  j2 )) are also
added. The formulae associated with j  k and k  i are encoded similarly. Although
bpij and bpjk are set to true, up does not infer bpik , since none of the clauses obtained
by applying Tseitin transformation is unary. Therefore, unlike ae, seb does not detect the
inconsistency.
Thus, we can infer that if unit propagation detects inconsistency in seb then it also
detects inconsistency in ae, but the converse is not true.
Given a set of assignments restricted to the problem variables, if unit propagation detects
inconsistency in seu then it also detects inconsistency in ae, and the converse is also true.
This follows directly from the explanation of the symbol-based unary encoding and the
atom-based encoding. Notice that both encodings detect cycles consisting of two features
of the form i  j and j  i. If the cycles involve more than two features i  j, j  k, k  i
both of them will infer i  k which will result in a cycle consisting of two features i and k.

10. Mixed Integer Linear Programming
In linear programming the goal is to optimise an objective function subject to linear equality and inequality constraints. When some variables are forced to be integer-valued, the
problem is called Mixed Integer Linear Programming (mip) problem. The standard way
4. When there are only 2 features, the clauses corresponding to Ji  jK in seb are unary, in which case
inconsistency can be detected by up if it exists. However, the same inconsistency will be detected in the
atom-based encoding.

292

fiApproaches for Solving a Telecommunications Feature Subscription Problem

of expressing these problems is by presenting the function to be optimised, the linear constraints to be respected and the domain of the variables involved. Both the basic cop
formulation and the atom-based pwmsat formulation for finding an optimal relaxation of a
feature subscription hF, H, P, wi can be translated into a mip formulation. The translation
of the pwmsat formulation into mip is straightforward. For this particular formulation we
observed that cplex was not able to solve even simple problems within a time limit of 4
hours. In this paper, we only present the mip formulation that corresponds to the basic
cop formulation as presented in Section 2.2.
Variables. For each i  F , we use a binary variable bfi and a real variable pfi . A binary
variable bfi is equal to 1 or 0 depending on whether feature i is included or not. A real
variable pfi , 1  pfi  |F |, if bfi is set to 1, is used to determine the position of the feature
i in the computed subscription. For each user precedence constraint (i  j)  P , we use
a binary variable bpij . It is instantiated to 1 or 0 depending on whether the precedence
constraint i  j holds or not.
Linear Inequalities. If the features i and j are included in the computed subscription
and if (i  j)  H then the position of feature i must be less than the position of feature j.
To this effect, we need to translate the underlying implication (bfi  bfj  (pfi < pfj )) into
the following linear inequality:
pfi  pfj + n  bfi + n  bfj  2n  1 .

(6)

Here, n is a constant that is equal to the number of features, |F |, selected by the user.
When both bfi and bfj are 1, Inequality (6) will force (pfi < pfj ). Note that this is not
required for any user precedence constraint (i  j)  P , since it can be violated.
A user precedence (i  j)  P is equivalent to the implication bpij  (pfi < pfj )bfi bfj ,
which in turn is equivalent to the conjunction of the three implications (bpij  (pfi < pfj )),
(bpij  bfi ) and (bpij  bfj ). These implications can be translated into the following
inequalities:
(7)
pfi  pfj + n  bpij  n  1
bpij  bfi  0

(8)

bpij  bfj  0 .

(9)

Inequality (7) means that bpij = 1 forces pfi < pfj to be true. Also, if bpij = 1 then both
bfi and bfj are equal to 1 from Inequalities (8) and (9) respectively.
Objective Function. The objective is to find an optimal relaxation of a feature subscription configuration problem hF, H, P, wi that maximises the sum of the weights of the
features and the user precedence constraints that are selected:
X
X
Maximise
w(i)  bfi +
w(i  j)  bpij .
iF

(ij)P

11. Experimental Results
In this section, we shall describe the empirical evaluation of finding an optimal relaxation of
randomly generated feature subscriptions using constraint programming, partial weighted
maximum Boolean satisfiability and integer linear programming.
293

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

11.1 Problem Generation and Experimental Settings
In order to compare the different approaches we generated and experimented with a variety
of random catalogues and many classes of random feature subscriptions. All the random
selections below are performed with uniform distributions. A random catalogue is defined
by a tuple hfc , Bc , Tc i. Here, fc is the number of features, Bc is the number of binary
constraints and Tc  {, , } is a set of types of constraints. Note that i  j means
that in any given subscription both features i and j cannot exist together. A random
catalogue is generated by selecting Bc pairs of features randomly from fc (fc  1)/2 pairs
of features. Each selected pair of features is then associated with a type of constraint
that is selected randomly from Tc . A random feature subscription is defined by a tuple
hfu , pu , wi. Here, fu is the number of features that are selected randomly from fc features,
pu is the number of user precedence constraints between the pairs of features that are
selected randomly from fu (fu  1)/2 pairs of features, and w is an integer greater than 0.
Each feature and each user precedence constraint is associated with an integer weight that
is selected randomly between 1 and w inclusive.
We generated catalogues of the following forms: h50, 250, {, }i, h50, 500, {, , }i
and h50, 750, {, }i. For each random catalogue, we generated classes of feature subscriptions of the following forms: h10, 5, 4i, h15, 20, 4i, h20, 10, 4i, h25, 40, 4i, h30, 20, 4i, h35, 35, 4i,
h40, 40, 4i, h45, 90, 4i and h50, 5, 4i. Note that h50, 250, {, }i is the default catalogue and
the value of w is 4 by default, unless stated otherwise. For each catalogue 10 instances of
feature subscriptions were generated and their mean results are reported in the paper5 . We
remark that only 4 randomly generated instances were consistent out of the 270 generated
instances. These consistent instances are instances of the feature subscription class h10, 5, 4i
of catalogue h50, 250, {, }i.
All the experiments were performed on a pc pentium 4 (cpu 1.8 ghz and 768mb of
ram) processor. The performances of all the approaches are measured in terms of search
nodes (#nodes) and runtime in seconds (time). The time reported is the time spent in
both finding the optimal solution and proving optimality. We used the time limit of 14,400
seconds (i.e., 4 hours) to cut the search. No initial bounds were computed for any of the
approaches.
11.2 Evaluation of Constraint Programming Formulations
For the basic constraint optimisation problem model as presented in Section 7 we first investigated the effect of Maintaining Arc Consistency (mac) within branch and bound search.
We also studied the effect of maintaining different levels of consistency on different sets of
variables within a problem. In particular we investigated, (1) maintaining singleton arc
consistency on the Boolean variables and mac on the remaining variables (see Section 7.2),
and (2) maintaining restricted singleton arc consistency on the Boolean variables and mac
on the remaining variables (see Section 7.3); the former is denoted by msacb and the latter by mrsacb . All the branch and bound search algorithms were tested with two different
variable ordering heuristics: dom/deg (Bessiere & Regin, 1996) and dom/wdeg (Boussemart,
Hemery, Lecoutre, & Sais, 2004). Here dom is the domain size, deg is the original degree
5. All the generated instances are available on http://4c.ucc.ie/~lquesada/FeatureSubscription/page/
instances.htm.

294

fiApproaches for Solving a Telecommunications Feature Subscription Problem

of a variable, and wdeg is the weighted degree of a variable. All the experiments for the
basic constraint optimisation problem formulation were done using choco6 (version 2.1)
a Java library for constraint programming systems. Some results for all the three branch
and bound search algorithms with the dom/deg variable ordering heuristic are presented in
Table 2 and with the dom/wdeg variable ordering heuristic are presented in Table 3.
Table 2: Average results of mac, mrsacb and msacb with dom/deg heuristic.
hfu , pu i
h20, 10i
h25, 40i
h30, 20i
h35, 35i
h40, 40i

time
0.2
9.8
5.6
125.2
1,716.9

MAC
#nodes
1,691
70,233
29,076
479,650
5,307,530

MRSACb
time
#nodes
0.0
45
0.5
174
0.6
179
7.3
1,269
68.8
9,830

MSACb
time
#nodes
0.0
44
0.6
156
0.7
157
8.1
1,083
75.1
8,466

Table 3: Average results of mac, mrsacb and msacb with dom/wdeg heuristic.
hfu , pu i
h20, 10i
h25, 40i
h30, 20i
h35, 35i
h40, 40i

time
0.1
3.3
2.4
76.9
889.0

MAC
#nodes
701
20,096
10,511
248,447
2,255,713

MRSACb
time
#nodes
0.0
42
0.5
164
0.5
161
5.5
932
45.9
6,105

MSACb
time
#nodes
0.0
41
0.6
145
0.6
142
6.3
798
52.9
5,184

Tables 2 and 3 clearly show that maintaining (r)sac on the Boolean variables and ac
on the integer variables dominates maintaining ac on all the variables. To the best of
our knowledge this is the first time that such a significant improvement has been observed
by maintaining a partial form of singleton arc consistency during search. As the problem
size increases the difference in terms of the number of nodes visited by mrsacb and msacb
increases. Note that mrsacb usually visits more nodes than those visited by msacb , but the
difference between them is not that significant. This suggests that the level of consistency
enforced by rsac on the instances of feature subscription problem is very close to that
enforced by sac. Despite visiting more nodes, mrsacb usually requires less time than msacb .
On average, all the three search algorithms perform better with the dom/wdeg heuristic
than with the dom/deg heuristic. Note that in the remainder of the paper the results that
correspond to the basic cop model are obtained using mrsacb with the dom/wdeg variable
ordering heuristic.
We remark that the underlying algorithms in mac and mrsacb that enforce ac and
rsacb respectively have an optimal worst-case time complexity. However, the underlying
algorithm of msacb that enforces sacb does not have an optimal worst-case time complexity.
Implementing an algorithm to enforce sacb that has an optimal worst-case time complexity
is not only cumbersome but also has a higher space requirement. The works of Bessiere et al.
(2004, 2005) provide evidence that when an optimal algorithm for enforcing sac is used as
a preprocessor it is very expensive both in terms of running time and space. Therefore,
6. http://choco.sourceforge.net/

295

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

maintaining it during search, as in our case, could be even more expensive. Indeed there
exists other sub-optimal but efficient algorithms for enforcing singleton arc consistency on
constraints networks, as proposed by Lecoutre et al. (2005) and, it remains to see whether
any of these efficient algorithms can reduce the running time of msacb .
Notice that sacb can prune more values than rsacb . However, in practice, the difference
between their pruning on the instances of feature subscriptions is not much, which is evident
based on the number of nodes and time shown in Tables 2 and 3. We recall that rsacb
enforces partial sacb . At a given node in the search tree, rsacb enforces arc consistency
at most one time for each assignment of a value to each Boolean variable, whereas sacb
can enforce arc consistency at most n times in the worst-case. Here n is the sum of the
Boolean variables associated with features and user precedences. Nevertheless, in practice,
we observed that it was much less. For example, for any instance of feature subscription of
the class h40, 40i arc consistency was enforced at most 7 times for any variable-value pair,
which is much less than n = 80. This also justifies the use of a non-optimal version of
algorithm to enforce sacb .
Our wcsp formulation for finding an optimal relaxation of a feature subscription was
also tested. For this purpose toulbar2 (a generic solver for wcsp) was used7 . In general the
results in terms of time were poor. We remark that a solution of the wcsp model is a total
order on the features whose position variables are assigned values greater than 0. Due to
holes (when a feature is excluded) different assignments of the position variables may lead
to the same total order. Thus, more search effort could be spent for the wcsp formulation.
We recall that in the basic cop model the decision variables are only the Boolean variables
that indicate the inclusion/exclusion of features and user precedences and not the position
variables. Therefore, an optimal solution of the basic cop model may not necessarily be
a total order on the included features. Nevertheless, it can be obtained by computing a
topological sort on the included user precedences and the catalogue precedences defined
over the included features.
In order to remove the symmetries the wcsp formulation, as described in Section 8.2,
can be augmented. One way could be to associate costs with the values (greater than 0)
of the position variables in such a way that there is a unique assignment of values to the
variables, which is optimal for a given strict partial order. Our preliminary investigation
suggested that the number of nodes were reduced but at the expense of increasing the time.
In our current setting, the wcsp approach has been used as a black box. Indeed, certain
improvements can be made which may improve the performance in terms of time. For
example, stronger soft consistency techniques can be applied similar to the singleton arc
consistency for the cop model, which is more efficient for feature subscription problem.
We also investigated the impact of using the global constraint SoftPrec. This global
constraint was implemented in choco. The results obtained by using it are denoted by
sp. Five variants of SoftPrec have been investigated by Lesaint et al. (2009). The
results presented in this paper correspond to the variant that was observed to be the best
in terms of time, which Lesaint et al. (2009) denoted by sp4 . The results in Tables 68 show that SoftPrec always outperforms mrsacb on average. However, Lesaint et al.
(2008a) theoretically showed that the pruning achieved by maintaining rsac on the Boolean
7. http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro

296

fiApproaches for Solving a Telecommunications Feature Subscription Problem

variables of the cop model and ac on the remaining variables is incomparable with the
pruning achieved by using SoftPrec.
11.3 Evaluation of the Boolean Satisfiability Formulations
The evaluation of the atom-based pwmsat encoding of feature subscription was carried out
on three different solvers: (a) sat4j8 (version 2.1.1), an efficient library of sat solvers in
Java that implements the minisat specification (Een & Sorensson, 2003); (b) minisat+9
(version 1.13+), a pseudo-Boolean solver implemented on top of minisat (Een & Sorensson,
2006); and (c) clasp10 (version 1.3.0), an answer set solver that supports Boolean constraint
solving (Gebser, Kaufmann, & Schaub, 2009). As the two last solvers are pseudo-Boolean
solvers, the pwmsat instances were translated into linear pseudo-Boolean instances by
associating each clause with a linear pseudo-Boolean constraint, and defining the objective
function as the weighted sum of the soft clauses in the pwmsat model (de Givry, Larrosa,
Meseguer, & Schiex, 2003).
The results of the evaluation are summarized in Table 4. We remark that the results for
the sat4j solver, especially for the dense catalogues, are roughly 10 times faster in terms of
time when compared to those presented by Lesaint et al. (2008c). This is simply due to the
advances in the version of the sat4j that has been used to obtain the results. Despite that,
sat4j is significantly outperformed by both minisat+ and clasp. We observed up to a one
order-of-magnitude gap in those cases where the catalogue is sparse. clasp and minisat+
seem to be incomparable in our instances. Even though clasp performed better on our
toughest category of instances h45, 90i, clasp spent 27% more time solving the whole set of
instances. We also noticed that clasp seems to be more sensitive to the number of features
in sparse instances. While we observed a gap of one order-of-magnitude between categories
h45, 90i and h50, 4i in the h50, 250, {, }i catalogue with sat4j and minisat+, the gap
observed with clasp was not that significant.
Table 4: Results for the atom-based encoding using different SAT solvers.
hf, pi
h30, 20i
h35, 35i
h40, 40i
h45, 90i
h50, 4i

h50, 250, {, }i
sat4j
clasp
minisat+
0.6
0.1
1.2
2.7
0.8
3.0
18.2
6.9
8.0
1,156.4 111.1
119.6
90.8
79.0
11.9

h50, 500, {, , }i
sat4j
clasp
minisat+
0.5
0.0
0.7
0.7
0.1
1.3
1.2
0.1
2.0
3.6
0.4
5.7
3.7
0.6
3.8

h50, 750, {, }i
sat4j
clasp
minisat+
0.8
0.2
0.7
2.5
0.8
2.0
8.0
3.2
4.5
46.7
13.8
25.5
147.1
43.8
12.8

We now compare the atom-based encoding with the symbol-based unary and binary
encodings as described in Section 9.2. In order to do a fair comparison between these
encodings we need to solve the same instances of feature subscription on the same machine
using the same solver. As we did not have access to the instances of feature subscription
for seu and seb encodings, we use the results of the experiments run by Daniel Le Berre11
for all the three encodings: ae, seu and seb on the same instances of feature subscription
8.
9.
10.
11.

http://www.sat4j.org/
http://minisat.se/MiniSat+.html
http://www.cs.uni-potsdam.de/clasp/
http://www.cril.univ-artois.fr/~leberre/

297

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

using sat4j solver (version 2.1.0) on a pc pentium 4 (cpu 3 ghz). Codish et al. (2009) have
also made these results public.
Table 5 presents results for feature subscriptions of different sizes of different catalogues
for three encodings: ae, seu , and seb . The experimental results show that ae is, in general,
more efficient than seb , which is consistent with the fact that unit propagation on ae
is strictly stronger than unit propagation on seb . Note that ae is up to two orders-ofmagnitude faster than seb . Notice that seb never outperforms both seu and ae on any
class of feature subscription.
Table 5: Mean results in terms of time obtained using ae, seu , seb encodings in sat4j.
subscription
h10, 5i
h15, 20i
h20, 10i
h25, 40i
h30, 20i
h35, 35i
h40, 40i
h45, 90i
h50, 4i

h50, 250, {, }i
ae
seu
seb
0.05
0.12
0.17
0.12
0.69
0.32
0.15
0.76
0.36
0.41
1.70
1.87
0.58
1.66
1.22
1.40
3.46
7.12
9.20
9.06
21.03
484.16 161.37
1,844.01
30.72
7.09
11.97

h50, 500, {, , }i
ae
seu
seb
0.07 0.29
0.15
0.13 0.95
0.30
0.17 1.24
0.42
0.27 1.75
1.23
0.31 2.21
1.49
0.57 3.15
3.35
0.91 3.73
5.31
2.34 8.85
22.11
2.39 4.91
8.77

h50, 750, {, }i
ae
seu
seb
0.07
0.31
0.16
0.14
1.12
0.47
0.18
1.32
0.79
0.35
2.44
5.90
0.47
3.58
9.16
1.33
7.19
49.65
3.22
15.67
153.75
24.64
64.79
1205.12
61.57 41.87
618.66

Although the results reported in Tables 1, 2 and 3 of the works of Codish et al. (2008,
2009) suggest that seb is much better than ae, the results shown in Table 5 contradict this
conclusion. The results obtained by using seb are significantly outperformed by those obtained by using ae. This apparent conflict could be for one of several reasons. The results
reported by Codish et al. (2008) were based on different instances for different encodings
and the instances used for the symbol-based encoding were very much easier and in fact
some large size instances with 50 features were already consistent. Also, the experiments
for different encodings were conducted on different machines. Codish et al. (2008, 2009)
obtained the results for the symbol-based encoding and the atom-based encodings using
different solvers. The experiments for seb were done using a solver, which has been implemented on top of minisat, while for ae the results were obtained using the sat4j solver.
It is apparent from Table 4 that the use of different solvers can make a huge difference in
terms of runtime. In fact, we have observed a huge improvement for ae when tested with
the minisat+ solver. This latter fact suggests that the speed up observed by Codish et al.
(2008, 2009) could be mostly because of the use of minisat. Also, notice that the results
depicted in Table 5 are in accordance with the fact that unit propagation in the atom-based
encoding is strictly stronger than unit propagation in the symbol-based binary encoding.
Although unit propagation on ae encoding is equivalent to unit propagation on seu
encoding when assignments are restricted to problem variables, empirically it is not always
possible to observe this due to the exploration of the search trees in different orders. Table 5
shows that ae and seu are incomparable in terms of time. Therefore, it is not possible
to conclude superiority of any of the two approaches. We have also been informed that
the instances of the symbol-based encodings also include the computation of the objective
function, and the comparison of the value of the objective function with an upper bound as
described by Codish et al. (2009). However, they are not needed when applying the pwmsat
solver of sat4j. These extra clauses may indeed prevent the symbol-based approaches to
298

fiApproaches for Solving a Telecommunications Feature Subscription Problem

perform at their best. Nevertheless, most of the clauses of the symbol-based encodings are
coming from the encoding of the precedence constraints.
Finding an optimal relaxation of a feature subscription using a sat solver can be decomposed into three tasks: (a) the encoding of the strict partial order, (b) the encoding
of the objective function, and (c) the underlying search algorithm of the sat solver. Improving any of these tasks can improve the whole approach for solving the problem. In this
paper we have focused on task (a), which is mainly about the encoding of the precedence
constraints. We remark that (a), (b) and (c) are orthogonal tasks, so any of the techniques
for tasks (b) and (c) can certainly be used with any of the techniques for task (a). The
different encodings of precedence constraints can be fairly compared when the same (or the
best suited) techniques of tasks (b) and (c) are used. Codish et al. (2008, 2009) propose
several techniques for (b) and (c), e.g., the encoding of the sum constraint and the use of
dichotomic search for the optimisation aspect. It may be possible to improve the results of
atom-based encoding further by using these techniques.
11.4 Comparison between CP, SAT and MIP-based approaches
The performances of using constraint programming (cp), partial weighted maximum satisfiability (sat) and mixed integer linear programming (mip) approaches are presented in
Tables 6, 7 and 8. The mip model of the problem was solved using ilog cplex12 (version
10.1). For the cp approaches the results are presented for mrsacb and the global constraint
denoted by sp. For the sat approaches we use the results obtained by using clasp and
minisat+. All the approaches solved all the instances within the time limit. Since in general finding an optimal relaxation is NP-hard, we need to investigate which approach can
do it in reasonable time. The best approach in terms of time is represented in bold letters
for each class of feature subscription.
Table 6: Catalogue h50, 250, {, }i.
MIP
hfu , pu i
h30, 20i
h35, 35i
h40, 40i
h45, 90i
h50, 4i

#nodes
208
905
2,616
9,818
1,754

time
0.4
2.0
9.1
77.4
6.1

CP
MRSACb
SP
#nodes
time
#nodes
time
161
0.5
115
0.2
932
5.6
744
2.8
6,105
45.9
2,707
12.3
104,789 1,256.1
103,065
971.3
26,494
218.1
9,133
36.5

SAT
CLASP
MINISAT+
#nodes
time #nodes
time
5,258
0.1
3,938
1.2
11,565
0.8
9,757
3.0
37,331
6.9
20,368
8.0
310,595
111.1
133,303
119.6
196,684
79.0
26,087
11.9

The results presented in Table 6 suggest that the mip approach performs better than the
cp and sat approaches for the hardest feature subscription instances of the sparse catalogue
h50, 250, {, }i, in particular for h45, 90i and h50, 4i classes of feature subscriptions, and for
the remaining classes of feature subscription of the catalogue h50, 250, {, }i, the sat approach based on the clasp solver is the winner. For the dense catalogue h50, 750, {, }i,
the mip approach is significantly slower than the other approaches. Notice that the results for the mip approach have improved significantly when compared with the results
presented by Lesaint et al. (2008c). This is because of the usage of real-valued variables
for the positions of features. The results presented in Tables 7 and 8 for the catalogues
12. http://www.ilog.com/products/cplex/

299

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

Table 7: Catalogue h50, 500, {, , }i.
MIP
hfu , pu i
h30, 20i
h35, 35i
h40, 40i
h45, 90i
h50, 4i

#nodes
48
112
160
573
258

CP
time
0.4
1.1
1.8
18.2
1.5

MRSACb
#nodes
time
66
0.2
158
1.0
229
1.8
687
9.6
768
6.2

SP
#nodes
time
53
0.1
111
0.4
188
1.0
620
6.1
954
3.6

SAT
CLASP
MINISAT+
#nodes
time #nodes
time
2,066
0.0
4,298
0.7
2,999
0.1
6,838
1.3
4,005
0.1
8,897
2.0
7,265
0.4
19,791
5.7
8,887
0.6
16,511
3.8

Table 8: Catalogue h50, 750, {, }i.
MIP
hfu , pu i
h30, 20i
h35, 35i
h40, 40i
h45, 90i
h50, 4i

#nodes
3,761
13,485
28,461
43,958
163,686

time
9.3
67.9
229.0
539.1
1,644.4

CP
MRSACb
SP
#nodes
time #nodes
time
578
2.2
168
0.4
1,997
11.4
396
1.9
5,229
36.7
993
5.8
19,190
207.8
2,902
29.7
31,580
253.1
5,569
28.2

SAT
CLASP
MINISAT+
#nodes
time
#nodes
time
4,633
0.2
5,125
0.7
9,285
0.8
12,611
2.0
20,905
3.2
22,284
4.5
60,676 13.8
60,531
25.5
130,920
43.8
45,802 12.8

h50, 500, {, , }i and h50, 750, {, }i, respectively, suggest that the sat approaches
perform significantly better than the mip and cp approaches. In particular, the sat approach based on the clasp solver is the winner for all the classes except for the h50, 4i class
of feature subscription of catalogue h50, 750, {, }i, where it is outperformed by the cp
approach based on the global constraint and the sat approach based on minisat+.
Even though mrsacb and SoftPrec are outperformed by at least one of the other
approaches in all the cases, they are never the worst with respect to the total time required
for solving all the instances as shown in Figure 3. In particular the cp approach based
on SoftPrec is very competitive in those cases where the catalog is dense. Figure 3 also
shows that the pseudo-Boolean solvers clasp and minisat+ perform better in terms of
total time when compared with the other approaches. It should be noted that clasp and
minisat+ are implemented in C++ and use restarts, while mrsacb and SoftPrec are
implemented in the Java-based choco solver and they do not use restarts. Both clasp
and minisat+ perform poorly when compared with respect to the number of nodes visited
during search. This shows that the time spent by clasp and minisat+ at each node is
considerably less than the time spent by the remaining approaches. There is of course the
opportunity to improve the per-node speed of the cp approaches by implementing them
in a C++ based solver. We also remark that both clasp and minisat+ consume more
memory than the cp-based approaches and the mip approach. To illustrate this, we also
computed the sum of the problem sizes of all the instances for all the approaches. Here,
the problem size of an instance is the sum of the number of variables, the domain sizes
of all the variables, and the arity of all the constraints. Figure 4 depicts the plot for the
total problem size for each approach. The total problem size for clasp and minisat+ is
roughly two orders-of-magnitude more than the other approaches. We, therefore, conclude
that clasp and minisat+ do not offer scalability.
300

fiApproaches for Solving a Telecommunications Feature Subscription Problem

1e+07

search nodes (logscale)

CLASP

MINISAT+

MIP

MRSAC

SP
1e+06
0

5

10

15
time (seconds)

20

25

30

Figure 3: Total time and nodes required to solve all the instances by different approaches.

units of problem size -- see text (logscale)

1e+08

1e+07

1e+06

100000
MIP

MRSAC

SP
approach

CLASP

MINISAT+

Figure 4: Total problem size of all the instances for different approaches.

301

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

12. Conclusions and Future Work

In this paper we have focussed on the task of finding an optimal relaxation of feature subscription when the users preferences violate the technical constraints defined by a set of
distributed feature composition rules. We reformulated the problem of finding an optimal
relaxation, and showed that it is a generalisation of the Feedback Vertex Set problem, which
makes the problem NP-hard. We developed cpbased methods for finding an optimal relaxation of feature subscription. In particular we presented three models: a basic constraint
optimisation problem model, a model based on a global constraint, and a weighted csp
model. For the basic cop model, we studied the effect of maintaining arc consistency and
two mixed consistencies during branch and bound search. Our experimental results suggest
that maintaining (restricted) singleton arc consistency on the Boolean variables and arc consistency on the integer variables outperforms mac significantly. The former approach was
outperformed empirically by the cp approach based on the SoftPrec global constraint.
We also compared the cpbased approaches with the sat-based approaches and a mixed
integer linear programming approach. In the partial weighted maximum satisfiability case
we presented an atom-based encoding and investigated two symbol-based encodings. When
the set of assignments are restricted to problem variables unit propagation on the atombased encoding is strictly stronger than the unit propagation on the symbol-based binary
encoding, and the former is equivalent to the unit propagation on the symbol-based unary
encoding. Empirically, the atom-based encoding is better than the symbol-based binary
encoding, and it is incomparable with the symbol-based unary encoding. Overall, the
results suggest that when the catalogue is sparse mip is better in terms of runtime on hard
instances. When the catalogue is dense the sat approach based on clasp is better in terms
of runtime. The sat approach based on minisat+ and the cp approach based on the global
constraint are also very competitive on the dense catalogues. Overall, the pseudo-Boolean
solvers clasp and minisat+ perform better in terms of total time when compared with
the other approaches.
The approaches considered in this paper are mostly one-stage approaches in the sense
that the exploration is started without any approximation of the optimum value. In the
future we would like to consider a two-stage approach where, at the first stage, a heuristic
is used to compute an approximation of the optimal solution, and at the second stage, the
exploration is carried out taking the approximate value as an initial lower bound. The
cp approach based on wcsp was explored the least. It may be possible to improve its
performance by using different models that overcome the problem of symmetric solutions
and stronger consistency techniques similar to singleton arc consistency in the case of the
basic cop model. In the current settings the performance of all the approaches in terms of
time includes the time taken to prove the optimality of the solution. In the future, we would
like to compare all the presented approaches and also local search methods in terms of their
anytime profiles (i.e. solution qualities over time). It would be interesting to investigate the
impact of restarts on all the approaches.
302

fiApproaches for Solving a Telecommunications Feature Subscription Problem

Acknowledgments
This material is based upon work supported by the Science Foundation Ireland under Grant
No. 05/IN/I886, 08/PI/I1912, and Embark Post Doctoral Fellowships No. CT1080049908
and No. CT1080049909. The authors would like to thank Hadrien Cambazard, Daniel Le
Berre and Alan Holland for their support in using choco, sat4j and cplex respectively.
The authors would also like to thank Simon de Givry for his help in the wcsp formulation
of the problem. Thanks also to Michael Codish for providing the symbol-based encoding
instances to Daniel Le Berre. We thank all the reviewers for providing valuable comments
that helped us to improve the quality of the paper.

References
Apt, K. (2003). Principles of Constraint Programming. Cambridge University Press.
Bessiere, C., & Debruyne, R. (2004). Optimal and suboptimal singleton arc consistency
algorithms. In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI 2005), pp. 5459.
Bessiere, C., & Regin, J.-C. (1996). MAC and combined heuristics: Two reasons to forsake FC (and cbj?) on hard problems. In Proceedings of the Second International
Conference on Principles and Practice of Constraint Programming (CP 1996), pp.
6175.
Bessiere, C., Stergiou, K., & Walsh, T. (2008). Domain filtering consistencies for non-binary
constraints. Artificial Intelligence, 172 (6-7), 800822.
Biere, A., Heule, M. J. H., van Maaren, H., & Walsh, T. (Eds.). (2009). Handbook of
Satisfiability, Vol. 185 of Frontiers in Artificial Intelligence and Applications, chap. 19,
p. 980. IOS Press.
Bond, G. W., Cheung, E., Purdy, H., Zave, P., & Ramming, C. (2004). An Open Architecture
for Next-Generation Telecommunication Services. ACM Transactions on Internet
Technology, 4 (1), 83123.
Boussemart, F., Hemery, F., Lecoutre, C., & Sais, L. (2004). Boosting systematic search by
weighting constraints.. In de Mantaras, R. L., & Saitta, L. (Eds.), Proceedings of the
16th European Conference on Artificial Intelligence (ECAI 2004), pp. 146150. IOS
Press.
Calder, M., Kolberg, M., Magill, E. H., & Reiff-Marganiec, S. (2003). Feature Interaction:
A Critical Review and Considered Forecast. Computer Networks, 41 (1), 115141.
Codish, M., Lagoon, V., & Stuckey, P. J. (2008). Telecommunications feature subscription as
a partial order constraint problem. In Proceedings of the 24th International Conference
on Logic Programming (ICLP 2008), pp. 749753.
Codish, M., Genaim, S., & Stuckey, P. (2009). A declarative encoding of telecommunications
feature subscription in SAT. In Proceedings of the 11th ACM SIGPLAN conference on
Principles and practice of declarative programming (PPDP 2009), pp. 255266, New
York, NY, USA. ACM.
303

fiLesaint, Mehta, OSullivan, Quesada, & Wilson

Codish, M., Lagoon, V., & Stuckey, P. (2008). Logic programming with satisfiability. Theory
and Practice of Logic Programming, 8 (1), 121128.
Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction to Algorithms. The MIT Press.
de Givry, S., Larrosa, J., Meseguer, P., & Schiex, T. (2003). Solving max-sat as weighted
csp.. pp. 363376.
Debruyne, R., & Bessiere, C. (1997). Some practical filtering techniques for the constraint
satifaction problem. In Proceedings of the 15th International Joint Conference on
Artificial Intelligence (IJCAI 1997), pp. 412417, Nagoya, Japan.
Een, N., & Sorensson, N. (2003). An extensible SAT-solver. In Proceedings of the Sixth
International Conference on Theory and Applications of Satisfiability Testing (SAT
2003), pp. 502518.
Een, N., & Sorensson, N. (2006). Translating pseudo-boolean constraints into SAT. Journal
on Satisfiability, Boolean Modeling and Computation (JSAT), 2 (1-4), 126.
Festa, P., Pardalos, P., & Resende, M. (1999). Feedback set problems. Tech. rep. 99.2.2,
AT&T Labs Research.
Fried, C., Hordijk, W., Prohaska, S., Stadler, C., & Stadler, P. (2004). The footprint sorting
problem. Journal of Chemical Information and Modeling, 44 (2), 332338.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the The Theory
of NP-Completeness. W. H. Freeman and Company.
Gebser, M., Kaufmann, B., & Schaub, T. (2009). The conflict-driven answer set solver
clasp: Progress report. In Proceedings of the 10th International Conference on Logic
Programming and Nonmonotonic Reasoning (LPNMR 2009), pp. 509514, Berlin,
Heidelberg. Springer-Verlag.
Jackson, M., & Zave, P. (1998). Distributed Feature Composition: a Virtual Architecture
for Telecommunications Services. IEEE Transactions on Software Engineering (TSE),
24 (10), 831847.
Jackson, M., & Zave, P. (2003). The DFC Manual. AT&T.
Larrosa, J. (2002). Node and arc consistency in weighted CSP. In Proceedings of the
Eighteenth National Conference on Artificial Intelligence (AAAI 2002), pp. 4853,
Menlo Park, CA, USA. American Association for Artificial Intelligence.
Lecoutre, C., & Patrick, P. (2006). Maintaining singleton arc consistency. In Proceedings
of the 3rd International Workshop on Constraint Propagation And Implementation
(CPAI2006), pp. 4761, Nantes, France.
Lecoutre, C. (2009). Constraint Networks: Techniques and Algorithms. Wiley Blackwell.
Lecoutre, C., & Cardon, S. (2005). A greedy approach to establish singleton arc consistency. In Proceedings of the Nineteenth International Joint Conference on Artificial
Intelligence (IJCAI 2005), pp. 199204.
Lesaint, D., Mehta, D., OSullivan, B., Quesada, L., & Wilson, N. (2009). A Soft Global
Precedence Constraint. In Proceedings of the 21st International Joint Conference on
Artificial Intelligence (IJCAI-09), Pasadena, CA, USA.
304

fiApproaches for Solving a Telecommunications Feature Subscription Problem

Lesaint, D., Mehta, D., OSullivan, B., Quesada, L., & Wilson, N. (2008a). Consistency
techniques for finding an optimal relaxation of a feature subscription. In Proceeding
of the 20th IEEE International Conference on Tools with Artificial Intelligence(ICTAI
2008), pp. 283290.
Lesaint, D., Mehta, D., OSullivan, B., Quesada, L., & Wilson, N. (2008b). Personalisation
of Telecommunications Services as Combinatorial Optimisation. In Proceedings of
The Twentieth Conference on Innovative Applications of Artificial Intelligence (IAAI
2008), pp. 16931698, Chicago, USA. AAAI Press.
Lesaint, D., Mehta, D., OSullivan, B., Quesada, L., & Wilson, N. (2008c). Solving a
Telecommunications Feature Subscription Configuration Problem. In Proceedings of
the 14th International Conference on Principles and Practice of Constraint Programming (CP 2008), pp. 6781.
Poikselka, M., Mayer, G., Khartabil, H., & Niemi, A. (2006). The IMS: IP Multimedia
Concepts and Services (2nd edition). John Wiley and Sons.
Prosser, P., Stergiou, K., & Walsh, T. (2000). Singleton Consistencies. In Dechter, R.
(Ed.), Proceedings of the 6th International Conference on Principles and Practice of
Constraint Programming(CP 2000), pp. 353368.
Pruesse, G., & Ruskey, F. (1994). Generating linear extensions fast. The SIAM Journal on
Computing, 23 (2), 373386.
Rosenberg, J., Schulzrinne, H., Camarillo, G., Johnston, A., Peterson, J., Sparks, R., Handley, M., & Schooler, E. (2002). SIP: Session initiation protocol RFC 3261 (proposed
standard updated by RFCs 3265, 3853, 4320)..
Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006a). Handbook of Constraint Programming,
chap. 3. Elsevier Science Inc.
Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006b). Handbook of Constraint Programming,
chap. 9. Elsevier Science Inc.
Sparks, R. (2007). SIP: Basics and Beyond. ACM Queue, 5 (2), 2233.
Tseitin, G. S. (1968). On the complexity of derivations in the propositional calculus. Studies
in Mathematics and Mathematical Logic, Part II, 115125.
Wallace, M. (1996). Practical applications of constraint programming. Constraints Journal,
1 (1), 139168.
Zave, P. (2003). An Experiment in Feature Engineering. In McIver, A., & Morgan, C.
(Eds.), Programming Methodology, pp. 353377. Springer-Verlag.
Zhang, Y., & Yap, R. (2000). Arc consistency on n-ary monotonic and linear constraints.
In Proceedings of the 6th International Conference on Principles and Practice of Constraint Programming (CP 2002), pp. 470483, Sigapore. Springer-Verlag.

305

fiJournal of Artificial Intelligence Research 38 (2010) 513-534

Submitted 04/10; published 08/10

Algorithms for Closed Under
Rational Behavior (CURB) Sets
Michael Benisch
George B. Davis
Tuomas Sandholm

mbenisch@cs.cmu.edu
gbd@cs.cmu.edu
sandholm@cs.cmu.edu

School of Computer Science
Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA 15213 USA

Abstract
We provide a series of algorithms demonstrating that solutions according to the fundamental game-theoretic solution concept of closed under rational behavior (CURB) sets in
two-player, normal-form games can be computed in polynomial time (we also discuss extensions to n-player games). First, we describe an algorithm that identifies all of a players best
responses conditioned on the belief that the other player will play from within a given subset
of its strategy space. This algorithm serves as a subroutine in a series of polynomial-time
algorithms for finding all minimal CURB sets, one minimal CURB set, and the smallest
minimal CURB set in a game. We then show that the complexity of finding a Nash equilibrium can be exponential only in the size of a games smallest CURB set. Related to this,
we show that the smallest CURB set can be an arbitrarily small portion of the game, but
it can also be arbitrarily larger than the supports of its only enclosed Nash equilibrium.
We test our algorithms empirically and find that most commonly studied academic games
tend to have either very large or very small minimal CURB sets.

1. Introduction
For noncooperative multi-agent settings, game-theoretic solution concepts help players choose
strategies, help modelers predict outcomes, and help mechanism designers guarantee properties of the systems they create. Significant attention has been given to algorithms for computing solutions according to concepts of subgame perfect Nash equilibrium (e.g., minimax
search and --pruning), Nash equilibrium (Lemke & Howson, 1964; Porter, Nudelman,
& Shoham, 2004; Sandholm, Gilpin, & Conitzer, 2005), correlated equilibrium (Gilboa &
Zemel, 1989), iterative dominance (Knuth, Papadimitriou, & Tsitsiklis, 1988; Conitzer &
Sandholm, 2005a), and other related concepts (Conitzer & Sandholm, 2005b).
The Nash equilibrium concept, under which each player weakly prefers its strategy as
long as the other players do not deviate from theirs, remains the most important pointvalued game-theoretic solution concept. However, it has been shown that, even in twoplayer games with binary utilities, computing a single Nash equilibrium is PPAD-complete
(Chen & Deng, 2006; Abbott, Kane, & Valiant, 2005), suggesting that no algorithms exist
for computing these equilibria in worst-case polynomial time (Daskalakis, Goldberg, &
Papadimitriou, 2009).
c
2010
AI Access Foundation. All rights reserved.

fiBenisch, Davis, & Sandholm

There are other fundamental solution concepts that have some known advantages over
Nash equilibria, andas we will showsolutions according to some of these concepts can be
found in polynomial time, even in the worst case. Specifically, we will study the fundamental
concept of closed under rational behavior (CURB) strategy sets in two-player, normal-form
games. A game can have multiple Nash equilibria, but each of those is a point, or a single
(potentially mixed) strategy for each player. In contrast, a CURB set can contain multiple
strategies for each player, and it is stable so long as players choose any (potentially mixed)
strategies from within the set.
CURB sets are based on the notion of rationalizability, which was introduced by Pearce
(1984) and Bernheim (1984). Rationalizability is, by now, a widely known, robust gametheoretic solution concept and has been used to study various applications, such as first-price
auctions (Battigalli & Siniscalchi, 2003). Its main insight is that rationality restricts players
from ever playing strategies that are not best responses given the beliefs they hold about
their opponents. Strategies that are not best responses to a set of consistent beliefs about
opposing strategies are said not to be rationalizable. In two-player games, the process of
iteratively eliminating strategies that are dominated, that is strategies that are not best
responses to any opponent strategy, captures the concept of rationalizability. It emulates
the players assumptions that an opponent will never play a strategy that is not a best
response to one of the players own remaining strategies (Pearce, 1984).
The set of all players rationalizable strategies has the property that no players best
response to any pure or mixed strategy inside the set lies outside the set  in other words, the
set is CURB. However, this CURB set may also have CURB subsets, which demonstrates
how CURB sets extend the notion of rationalizability. Basu and Weibull (1991) introduced
the notion of a minimal CURB set, or a CURB set that does not contain any CURB subsets,
and proved that each minimal CURB set is guaranteed to contain the supports of at least
one Nash equilibrium.
The minimal CURB set solution concept has since been motivated from several perspectives in academic literature, including the following:
 Mixed-strategy Nash equilibria (the type guaranteed to exist in every game) can be
highly unstable, because a player may be indifferent between some of its strategies.
Strict Nash equilibria, where players strictly prefer their strategies in equilibrium, are
a more stable alternative, but they are not guaranteed to exist. Minimal CURB sets
always exist and have been referred to as the nearest set-valued generalization of
strict Nash equilibria, since they are the smallest sets of strategies that include all
ways of choosing among the indifferences of an equilibrium (Basu & Weibull, 1991).
 A CURB set can be viewed as a subspace of strategies within which any best-response
dynamic (even a best-response dynamic of mixed strategies) will stay. Thus, CURB
sets have been used as a solution concept to describe the strategy subspace where
iteratively adapting agents will eventually settle (Hurkens, 1995).
 Voorneveld et.al. enumerated a number of other properties of minimal CURB sets
that illustrate the stability of set-based solution concepts over point-valued concepts,
such as Nash equilibria (Voorneveld, Kets, & Norde, 2005).
514

fiAlgorithms for Closed Under Rational Behavior (CURB) Sets

In order for a solution concept to be operational, it must also be accompanied by algorithms for applying it. Finding minimal CURB sets has been previously considered
challenging (Pruzhansky, 2003), and, prior to this work on CURB sets, little had been done
on the problem from a computational standpoint. To our knowledge, the only such work to
predate ours was that of Pruzhansky, which studied sequential games of perfect information.
Such games are relatively simple, in that they contain exactly one minimal CURB set, and
a straightforward algorithm can quickly find it by exploiting the sequential representation
(Pruzhansky, 2003). In this paper, we present the first thorough computational treatment
of CURB sets in general two-player games. We show that, in these settings, minimal CURB
sets are actually easy to find: the time complexity is polynomial in the size of the game,
even in the worst case.
The primary source of complexity for our algorithms is a linear programming-based
subroutine for finding all of a players best responses (i.e., utility-maximizing pure strategies)
conditioned on the belief that the other player will play from within a given subset of its
strategy space. This problem can be solved fast for two players, in this case it involves
solving a simple linear feasibility program, but the mathematical program we use is of
degree p  1, where p is the number of players, and with p = 3 the constraints are already
quadratic. On the plus side, our CURB set algorithms only make a polynomial number of
calls to this subroutine. If future research is able to identify polynomial-time algorithms for
finding all of a players best responses in n-player games, then our CURB set algorithms
will also be polynomial time in those settings. Additionally, our algorithms have been useful
as templates for the development of other algorithms to compute related solution concepts
in n-player games (Brandt, Brill, Fischer, & Harrenstein, 2009; Jordan & Wellman, 2010).
The rest of the paper is organized as follows. We begin with some preliminaries on
our notations and definitions. Next, we present and analyze our algorithm for finding
conditional best responses, which serves as the main subroutine of our CURB set finding
algorithms. We then present and analyze a family of polynomial-time algorithms for twoplayer, normal-form games that compute all of a games minimal CURB sets, a single one of
its minimal CURB sets, and its smallest minimal CURB set. Finally, we discuss additional
applications of our results, including the potential for our CURB set algorithms to bound
the theoretical complexity of finding Nash equilibria.

2. Preliminaries
We describe and analyze our algorithms in the classic game-theoretic setting of a two-player,
normal-form game represented as a matrix with rows corresponding to the pure strategies
(or actions) of one player, player r, and columns corresponding to the pure strategies of
the other, player c. (For shorthand, we will often omit the term pure and refer to a pure
strategy simply as a strategy.) As is typical in game theory, the players are assumed to be
fully-rational, utility-maximizing agents.
Each row in the game matrix corresponds to a strategy, sr , from the set of all player rs
strategies, Sr . Likewise, each column corresponds to a strategy, sc , from the set of all of
player cs strategies, Sc . The cell corresponding to strategies sr and sc contains two entries,
one indicating the real-valued utility of the row player when sr and sc are played, ur (sr , sc ),
and the other indicating that of the column player when those two strategies are played,
515

fiBenisch, Davis, & Sandholm

uc (sr , sc ). Using these entities, we also refer to a game, G, as a tuple, G = hSr , Sc , ur , uc i.
The size of the game, which we refer to as n, is the total number of strategies it contains,
n = |Sr | + |Sc |.
A mixed strategy, or mixture, is a probability distribution over pure strategies, or a
function, m
Pi , that maps from each of player is pure strategies to a probability, mi : Si 
[0, 1] and sSi mi (s) = 1. The supports of a mixture are all of the pure strategies in the
mixture with non-zero probability. The set of all possible mixtures with supports in some
set of strategies, Si , is denoted M (Si ), and can be thought of as a simplex with degree
|Si |  1. A pure strategy can be represented as a point-mass mixture, that is a mixture
with all of its probability mass on one strategy.
A strategy profile is a set of pure or mixed strategies, with one for each player. When
a mixed-strategy profile is played, a players utility is assumed to be its expected utility, which is given by summing that players utility for each possible pure-strategy profile
weighted
by the
Pprofiles joint probability according to the mixtures, e.g., ur (mr , mc ) =
P
m
(s
)
r
r
sc Sc mc (sc )ur (sr , sc ). We occasionally use the notation i to refer to the
sr Sr
player or players other than some player i. When used as a subscript on a strategy-related
entity with more than two players, we intend for the i to refer to one instance of the entity
per player (e.g., mi is a mixed-strategy profile containing one mixture per player other
than i).
Player is best responses to a mixed strategy of the other player(s), mi , are given by
the function i (mi ). These are the pure strategies that maximize player is utility if the
other player(s) play mi .
For a set of the other players pure strategies, Si , we define i (Si ) to be a function
that
S returns player is best responses to every mixture with supports in Si , i (Si ) =
mM (Si ) i (m). In Section 3, we describe an algorithm for computing the pure strategies
in i (Si ) that serves as a subroutine in our CURB set algorithms, and we refer to the
strategies it computes as conditionally rational. We define (S) (without a subscript i) as
the union of the sets i (Si ) for all players.
A CURB set, S, is formally defined as a set of pure strategies (with at least one strategy
for each player) that contains the best responses to any mixture over itself: if S is CURB
and players believe that no strategy outside of S will be played with positive probability by
their opponents, then such strategies will indeed not be played by rational players. Using
the notation above, a set, S, is CURB if (S)  S. (The entire game is trivially CURB
by this definition.) We refer to the number of strategies in a CURB set as its size. The
intersection of two CURB sets, S1 and S2 , is the set of strategies attained by taking the
intersection of their strategy sets, SI = S1  S2 . Two CURB sets overlap if they share a
strategy (i.e., their intersection is non-empty).
A Nash equilibrium is a pure- or mixed-strategy profile, {mr , mc }, such that each players
strategy is at least as good as its best response to the others, ur (mr , mc ) = ur (sr , mc ) and
uc (mr , mc ) = uc (mr , sc ), where sr  r (mc ) and sc  c (mr ). A strict Nash equilibrium is
a pure-strategy profile, {sr , sc }, where each players strategy is its only best response to the
others, r (sc ) = {sr } and c (sr ) = {sc }. A CURB set that contains only one strategy per
player is also a pure-strategy Nash equilibrium.
516

fiAlgorithms for Closed Under Rational Behavior (CURB) Sets

3. Finding Conditional Best Responses
Finding all of a players best responses conditioned on the belief that the other player will
play from within some subset of its total strategy space, is a problem of interest in its own
right, and it plays a central role in our computation of CURB sets. This section describes
a polynomial-time algorithm, all conditionally rational, for doing just that. The algorithm below is for the row player; the column players variant is symmetric. The inputs
to the algorithm are a set of row-player strategies to consider, Sr , a set of column-player
strategies they may be played against, Sc , and the row players utility function, ur .
function all conditionally rational(Sr , Sc , ur )
Sr  
for each row strategy, sr  Sr do
if there exists a feasible solution to the following linear feasibility program:
find psc such that
X

p sc

= 1

(1)

sc Sc

X

X



psc ur (sr , sc )

sc Sc

psc ur (s0r , sc )

s0r  Sr \ sr

(2)

sc Sc

p sc



0 sc  Sc

(3)

then Sr  Sr  sr
return Sr
For each row strategy, sr  Sr , a linear feasibility program (LFP) (i.e., a linear program
with no objective) is constructed to find a mixture of probabilities over column-player
strategies, psc , such that sr is the row players best response. The constraints of the LFP
ensure that the mixture is valid (sums to one), and that the row players utility by playing
sr against psc is greater than or equal to that of any other strategy. If the LFP has a feasible
solution, sr is added to the set of best responses to be returned.
The computational complexity of the algorithms described in this paper depend on the
total number of strategies in the game, n, and the complexity of solving an LFP with
a number of variables and constraints bounded by n, which we will denote as LFP(n).
LFPs can be solved in low-order polynomial time, even in the worst case, and the fastest
known algorithms for LFPs have better worst-case guarantees than the fastest known linear
programming algorithms (Ye, 2006). In our experiments, we solve the LFP using the simplex
algorithm, which has exponential worst-case time complexity, but is known to outperform
polynomial-time linear programming algorithms in practice.
Proposition 1. The all conditionally rational algorithm returns a players best responses to every mixture over the input strategy set, and nothing else. Its worst-case time
complexity is (n)  LFP(n).
517

fiBenisch, Davis, & Sandholm

Proof. Since all conditionally rational runs this program on all strategies and includes
them in the return set only if the LFP is feasible, it must be correct. Since the LFP is
executed once for each strategy, and its size is bounded by n, all conditionally rational
has complexity as shown.

4. Finding CURB Sets
We now turn our attention to the problem of finding CURB sets. The algorithm below
finds the smallest CURB set that contains a given seed strategy within a given subgame.
(The returned set is not necessarily minimally CURB.) The algorithm repeatedly alternates
between players, each time calling all conditionally rational to add the strategies necessary for maintaining the CURB property. If an iteration passes without any strategies
being added, the algorithm has converged.
function min containing CURB(sr , G = hSr , Sc , ur , uc i)
Sr  {sr }, Sc  
converged  false
while converged do
converged  true
for i  {r, c} do
 ,u )
Si0  all conditionally rational(Si , Si
i

0
if Si \ Si 6=  then
converged  false

Si  Si  Si0
return Sr  Sc
It is worth noting that on the second-to-last line of the algorithm (Si  Si  Si0 ) it is
necessary to merge the old strategies, Si , with the newly identified strategies, Si0 , because
Si0 is not always a superset of Si . If, instead, Si were replaced by Si0 , then it would be
possible for the seed strategy to be eliminated during the algorithms first iteration. For
example, consider the following game.

sr1
sr2

sc1
1,1
0,1

sc2
0,0
1,0

If strategy sr2 is used as a seed, then on the first iteration Sr is initialized to {sr2 }, Sc is
then set to {sc1 }, and finally Sr is set to {sr1 }. Thus, without the merge the algorithm
would output a subgame that does not contain the seed strategy.
Proposition 2. The min containing CURB algorithm has worst-case runtime (n2 ) 
LFP(n).
Proof. Every two calls made to all conditionally rational must add a strategy to the
return set, or min containing CURB will terminate. Since at most n strategies can be added
this way, the complexity of min containing CURB is (n2 )  LFP(n).
518

fiAlgorithms for Closed Under Rational Behavior (CURB) Sets

Theorem 1. The min containing CURB algorithm is correct, that is, the returned set, S  ,
is the smallest set of strategies that both 1) contains the given seed strategy, sr , and 2) is
CURB.
Proof. The convergence of the algorithm implies that no strategies outside of S  are best
responses to some mixture with supports in S  . Therefore, (S  )  S  , and S  is CURB.
To prove that S  is the smallest CURB set containing sr , we can use induction on the
strategies that are added.
Base Case: Initially, S  contains only sr and c (sr ). At this point, S  is a subset of
the the smallest CURB set containing sr .
Inductive Step: Each time a new strategy, s , is added to S  , it is necessarily a best
response to some mixture, m  M (S  ), over the strategies already contained in S  . Since
strategies are never removed from S  during execution, m will remain a valid mixture.
Therefore, each strategy that is added is necessary to maintain the CURB property.
We will now present three algorithms that use min containing CURB to determine a
games minimal CURB sets. To facilitate our discussion of these algorithms, we first present
three results regarding CURB set structure, which, to the best of our knowledge, were not
previously known.
Theorem 2. If each of two intersecting strategy sets is CURB, then their intersection is
also CURB.
Proof. Consider two CURB sets, SA and SB , with nonempty intersection, SI . For any
mixture over strategies in SI belonging to (without loss of generality) the row player, there
exists a set of pure strategies that are the column players best responses, Sc . Because SA
is CURB, it also contains all of the strategies in Sc (i.e., Sc  SA ); likewise Sc  SB .
Therefore, Sc is within their intersection, SI .
Since the intersection of two CURB sets must be CURB and contained in both sets, we
also have the following two corollaries.
Corollary 1. Distinct minimal CURB sets cannot overlap (i.e., share rows or columns).
Corollary 2. Each strategy belongs to at most one minimal CURB set.
4.1 Finding All Minimal CURB Sets
The broadest query one can make regarding the minimal CURB set structure of a game
is asking for all of its minimal CURB sets. This is useful, for example, in the adaptive
agent context to identify regions of the strategy space in which learning agents are likely to
settle (Hurkens, 1995).
The all MC algorithm does this by first checking each pair of strategies for size-two
CURB sets (i.e., pure-strategy Nash equilibria) and adding them to the return set of minimal
CURB sets. Since this operation is only (n2 ), it can be done as a preprocessing step
without affecting the algorithms worst-case time complexity, and the strategies it finds can
be eliminated from future consideration. The all MC algorithm then determines all of the
minimal CURB sets in the remaining subgame by calling min containing CURB with each
row strategy, in turn, as a seed.
519

fiBenisch, Davis, & Sandholm

At first, we call min containing CURB using the entire remaining subgame as an input.
However, we accelerate subsequent calls to min containing CURB by maintaining a map
between each strategy and the smallest CURB set in which it has been discovered so far.
(The entries added to this map are also stored as candidate minimal CURB sets.) When a
new strategy is used as a seed, we use the smallest known CURB set containing that strategy
as the second input to min containing CURB. Whenever a smaller CURB set containing a
new seed is identified, we eliminate all of the candidate minimal CURB sets that contain
the newly found one. Once each strategy has been used as a seed, all MC terminates and
returns the remaining candidate minimal CURB sets.
Proposition 3. The all MC algorithm finds all of a games minimal CURB sets, and
nothing else. Its worst-case runtime is (n3 )  LFP(n). Its best-case runtime is (n2 ).
Proof. By Corollary 1, the minimal CURB set for any strategy must either equal, or be
contained by, any other CURB set in which that strategy is found. Therefore, restricting
the min containing CURB search to the smallest CURB set in which a strategy has been
found so far is valid, and the main loop of all MC will discover all minimal CURB sets in
the game. Since any CURB set that is not minimal must have contained one of the minimal
CURB sets discovered, it is removed when the smaller CURB set is discovered (or not added
if the smaller set was previously discovered).
In the worst case, all MC must call min containing CURB n times, with the full game
as a parameter, giving time complexity (n3 )  LFP(n). The best-case complexity follows
from a best-case game where each strategy is part of a pure-strategy Nash equilibrium.
4.2 Finding One Minimal CURB Set
Rather than finding all minimal CURB sets in a game, it may be desirable to find a single
minimal CURB set. To complete this quickly, we first choose a random seed strategy and
check if it is part of any size-two CURB sets (i.e., part of a pure-strategy Nash equilibrium),
which takes O(n) time. If that fails, we use the min containing CURB algorithm with the
randomly-chosen strategy as the seed and the full game as the second input. Since the
resulting CURB set might not be minimal, we recur within it by choosing, as a seed, a
contained strategy that has not yet been used. We repeat this until all strategies in the
current set have been used as seeds, at which point we terminate and return the remaining
set. This constitutes the one MC algorithm.
If the game has more than one CURB set, one MC will be faster than all MC because
it will never leave the CURB set in which it starts. The exact speed of one MC in practice
will depend on the first seed chosen. If it happens to be in a small CURB set, one MC will
run fast. In the worst case, where the entire game is the only CURB set, one MC executes
all of the same steps as all MC.
Proposition 4. The one MC algorithm returns one of a games minimal CURB sets. Its
worst-case time complexity is (n3 )  LFP(n). Its best-case time complexity is (n).
Proof. If there are no other minimal CURB sets, then the entire game is minimally CURB
and will be returned. If there are any other minimal CURB sets, one of them will be
discovered when a strategy inside it is used as a seed.
520

fiAlgorithms for Closed Under Rational Behavior (CURB) Sets

In the worst case, when the whole game is minimally CURB, one MC must call the
min containing CURB algorithm n times, with the full game as an input, giving time complexity (n3 )  LFP(n). The best-case complexity follows from a best-case game where a
strategy that is in a CURB set of size two is chosen as a seed.
4.3 Finding the Smallest Minimal CURB Set
As a different type of query, one may be interested in finding one of a games smallest
minimal CURB sets. This is important, for example, if the CURB set is used for future
computations and the complexity of those future computations increases with the size of
the CURB set (e.g., for Nash equilibrium finding as we will discuss later in the paper). We
find one of a games smallest minimal CURB sets using a pseudo-parallelization of all MC.
First, we use the same preprocessor as in all MC that checks each pair of strategies for a
size-two CURB set and returns one, if found. If that fails, we construct a candidate set for
each row strategy containing only that strategy. We insert the sets into a priority queue,
where sets containing the fewest strategies are given highest priority. We repeatedly pop
the smallest candidate set from the queue, and add all the necessary best responses to keep
it CURB by calling all conditionally rational for each player. If new strategies are
added for either player, the resulting set is inserted back into the queue, and it is prioritized
based on its new size. The algorithm terminates when a candidate set is removed from the
queue that fails to admit any new best responses. That set is returned and it is one of the
games smallest minimal CURB sets (we denote the size of this set as n ). We call this
algorithm small MC.
Proposition 5. The small MC algorithm returns one of the games smallest minimal CURB
sets. Its worst-case runtime is (n n2 )  LFP(n). Its best-case runtime is (n2 ).
Proof. At the time small MC terminates, all conditionally rational has been called on
each row and column strategy in the set with no new best responses having been added.
Therefore, the returned set is CURB. Since all other candidate sets on the queue must be
as large, or larger than the returned set (and future exploration can only add strategies to
these sets), this set is at least as small as the smallest CURB set in the game, and each of
the games smallest CURB sets is also minimal.
Whenever a candidate set is fathomed, at least one new strategy must be added or
small MC will terminate. Since there are n candidate sets, and n strategies in the returned
set, in the worst case n  n sets have been fathomed at termination. Since each examination of a candidate set involves a call to all conditionally rational, the complexity
of small MC is as claimed. Priority queue operations are logarithmic in the size of the
game, and in the worst case there are n  n such operations. Thus, the overall worst-case
complexity is (n n2 + n n log n)  LFP(n), which is (n n2 )  LFP(n). The proof of the
best-case complexity is the same as in Proposition 3.
4.4 Experimental Results
We implemented the algorithms above and conducted experiments on their performance using most of the instance generators of the main benchmark collection for solving normal-form
games, GAMUT (Nudelman, Wortman, Shoham, & Leyton-Brown, 2004). The GAMUT
521

fiBenisch, Davis, & Sandholm

collection includes a variety of commonly studied game types from the academic game
theory literature. It is also specifically designed to test different aspects of scalability for
game-solving algorithms, for example, most of the generators allow one to create multiple
game instances of any given size.1 In this section we show that the complexity of our algorithms depends primarily on the size of the game and the size of its smallest CURB set.
We then proceed to explore the distribution of CURB set sizes in the different game types.
We first report the runtime of our algorithms on two representative GAMUT game
distributions: random games, and covariant games. Figure 1 (left) shows how each of our
minimal CURB set finding algorithms scales with game size on a data set of over 1000
random, square normal-form games with between 20 and 100 strategies. The results show
that small MC is faster than all MC on random games, which is consistent with their time
complexities, considering that many random games have small CURB sets. While the
worst-case time complexity of one MC and all MC is the same, experimentally one MC is
faster because it only needs to find one minimal CURB set. We can also see that, for large
random games, small MC performs slightly better than one MC, since these games tend to
contain both small and large CURB sets and one MC is more likely to start in the larger
ones. On the other hand, in games with only large CURB sets, one MC tends to be faster,
as we will show later.
We observed that the performance on random games, illustrated in Figure 1 (left), was
typical of that of many other GAMUT instance distributions. However, to show potentially
differing performance, we also report on experiments with the covariant game class, in which
utilities for both players are drawn from the same distribution with a specified covariance.
(In our experiments we set the covariance parameter to be 0.5.) This class (and setting)
have been shown to be particularly challenging for Nash equilibrium finding algorithms,
such as the Lemke-Howson and Porter-Nudelman-Shoham algorithms (Lemke & Howson,
1964; Porter et al., 2004). Figure 1 (right) shows that the all MC algorithm scales similarly
on random and covariant games, while the other two algorithms lose their speed advantages
when applied to this class.
The distribution of CURB sets in random games is shown as solid dots in Figure 2. Most
random games have small smallest CURB sets (in fact, often sets of size two), and those that
do not, tend to have very large smallest CURB sets. On the other hand, the distribution of
smallest CURB set sizes in covariant games (shown in Figure 2, hollow squares) has almost
no small smallest CURB sets and many large smallest CURB sets. This is consistent with
the observed hardness of these games for support enumeration-based Nash equilibrium
finding algorithms, which typically try to find equilibria with small supports first (Porter
et al., 2004). This disparity also explains the lowered performance on covariant games of
the two minimal CURB finding algorithms that have time complexities dependent on the
size of the smallest minimal CURB set, small MC and one MC.
Figure 3 shows the distribution of smallest CURB set size for 1000 instances from each
of the twenty-four other distributions emitted by GAMUT generators. Using a variety of
game generators, as we have done here, has become a primary way of testing game-solving
algorithms (Porter et al., 2004; Sandholm et al., 2005), and we used the same parameter
settings in the distributions as those prior papers. For covariant games, the suffixes Pos,
1. We did not benchmark on the GAMUT games that only have a fixed size, such as Chicken, Prisoners
Dilemma, and Battle of the Sexes, because they are trivial to solve from a computational perspective.

522

fiAlgorithms for Closed Under Rational Behavior (CURB) Sets

Random games
500

all_MC
one_MC
small_MC

400
300

Runtime (sec)

Runtime (sec)

500

Covariant games

200
100
0

all_MC
one_MC
small_MC

400
300
200
100
0

20 30 40 50 60 70 80 90 100
Game size (n)

20 30 40 50 60 70 80 90 100
Game size (n)

Figure 1: Scalability of our algorithms in game size for random (left) and covariant (right)
games.

Small games (n = 20)
100

Covariant
Random

80

% of games

% of games

100

Large games (n = 40)

60
40
20
0

80

Covariant
Random

60
40
20
0

0

5

10

15

20

0 5 10 15 20 25 30 35 40

Smallest CURB set size

Smallest CURB set size

Figure 2: Distribution of smallest CURB set size in random and covariant (r = 0.5) games,
where n = 20 and n = 40 (3,000 games were generated for each distribution and
value of n).

Rand, and Zero refer to positive, random, and zero covariance parameters, respectively.
For distributions that take a graph as input, CG, RG, and SG refer to complete,
random, and star graphs.
All of these distributions, like random and covariant games, exhibited very few mediumsized smallest CURB sets. Most of the instances had a smallest CURB set that was extreme:
either a pure strategy equilibrium or the entire game. With some of the generators, all of
the instances lie at the same extreme. Interestingly, some generators (e.g., Polymatrix)
produced a significant number of games with CURB sets of one or more specific, nonextremal sizes. It is also notable that using different graph parameters for Local Effect and
Polymatrix games had no effect on their smallest CURB set size distributions, suggesting
523

fiBenisch, Davis, & Sandholm

BidirectionalLEG-CG

20

0

0

5

10

15

20

% of games

% of games

% of games
0

0

10

15

0

0

0

5

10

15

20

PolymatrixGame-CG
100
80
60
40
20
0
0

5
10
15
20
Smallest CURB set size

TravelersDilemma
100
80
60
40
20
0

5
10
15
20
Smallest CURB set size

5
10
15
20
Smallest CURB set size

20

Smallest CURB set size

0

UniformLEG-SG

5
10
15
20
Smallest CURB set size

15

CovariantGame-Zero

5
10
15
20
Smallest CURB set size

100
80
60
40
20
0

10

100
80
60
40
20
0

PolymatrixGame-SW

5
10
15
20
Smallest CURB set size

5

Smallest CURB set size

20

100
80
60
40
20
0

UniformLEG-RG

0

5

100
80
60
40
20
0

5
10
15
20
Smallest CURB set size

100
80
60
40
20
0

0

MinimumEffortGame
% of games

0

20

Smallest CURB set size

% of games

% of games

20

100
80
60
40
20
0

15

100
80
60
40
20
0

PolymatrixGame-Road

% of games
5
10
15
20
Smallest CURB set size

15

10

CovariantGame-Rand

% of games

% of games

% of games

0

UniformLEG-CG

0

10

5

100
80
60
40
20
0

Smallest CURB set size

LocationGame

5
10
15
20
Smallest CURB set size

100
80
60
40
20
0

5

100
80
60
40
20
0

PolymatrixGame-RG

0

0

Smallest CURB set size

5
10
15
20
Smallest CURB set size

100
80
60
40
20
0

20

% of games
0

DispersionGame

0

15

100
80
60
40
20
0

Smallest CURB set size

100
80
60
40
20
0

10

CovariantGame-Pos
% of games

% of games

BinaryRandomGame
100
80
60
40
20
0

5

Smallest CURB set size

% of games

15

% of games

10

Smallest CURB set size

BidirectionalLEG-SG

% of games

5

BidirectionalLEG-RG
100
80
60
40
20
0

5
10
15
20
Smallest CURB set size

WarOfAttrition
% of games

0

100
80
60
40
20
0

% of games

% of games

% of games

BertrandOligopoly
100
80
60
40
20
0

100
80
60
40
20
0
0

5
10
15
20
Smallest CURB set size

Figure 3: Distribution of smallest CURB set size in sets of 1000 games with n = 20 from
various GAMUT distributions.

that the type of graph used may not change the fundamental structure of these types of
games.
To better understand how the minimal CURB set finding algorithms scale with the size
of the smallest CURB set, we bucketed the n = 20 random and covariant games according
to the size of their smallest CURB sets. (For n = 40, the buckets for medium-sized smallest
CURB sets were nearly empty, making it impossible for us to estimate mean runtimes with
meaningful accuracy.) Figure 4 plots the average runtime and 95% confidence intervals
for each bucket. On games with very small CURB sets, small MC is fastest, but it is
outperformed by both one MC and all MC as the size of the smallest CURB set grows.
524

fiAlgorithms for Closed Under Rational Behavior (CURB) Sets

The somewhat surprising runtime performance of the latter two algorithms is due to their
leveraging of information across calls to min containing CURB with different seeds. Because
small MC performs all the searches in parallel, this information is unavailable.
Random games
2

all_MC
one_MC
small_MC

1.5

Runtime (sec)

Runtime (sec)

2

Covariant games

1
0.5
0

all_MC
one_MC
small_MC

1.5
1
0.5
0

0

5
10
15
20
Smallest CURB set size

0

5
10
15
20
Smallest CURB set size

Figure 4: Average runtime on games were n = 20, with varying smallest CURB set sizes.

5. CURB Sets and Nash Equilibria
Minimal CURB sets and Nash equilibria both model strategy subspaces which are mutually
reinforced given the rationality of agents and their common knowledge. In their original
work on minimal CURB sets, Basu and Weibull showed that every minimal CURB set
contains the supports of at least one Nash equilibrium (Basu & Weibull, 1991). We observe
that this result suggests a secondary use for finding minimal CURB sets: our algorithms
can be used to preprocess a game so that a Nash equilibrium finding algorithm can restrict
its attention to one of the games minimal CURB sets, rather than running on the entire
game. As we will show, this can theoretically yield an arbitrarily large reduction of the
search space.
The most common prior preprocessing technique for Nash equilibrium finding, iterated
removal of dominated strategies, attempts to eliminate strategies that cannot be played with
any probability in any Nash equilibrium (Knuth et al., 1988; Gilboa, Kalai, & Zemel, 1993).
The same is true of another recent preprocessing technique, the generalized eliminability
method (Conitzer & Sandholm, 2005b). One comparative advantage of minimal CURB
set-based elimination is that it can eliminate strategies that are played in some equilibria,
while guaranteeing that the resulting set still contains the supports of at least one.
First, we will show that a CURB set-based preprocessor can reduce search space size by
an arbitrary amount even on games where prior preprocessing techniques cannot eliminate
anything.
Theorem 3. For any r,c,r0 , and c0 such that r  2, c  2, 1 < r0  r, and 1 < c0  c,
there exists normal form games of size r  c, with the following properties:
a) it contains a minimal CURB set with shape r0  c0 ,
525

fiBenisch, Davis, & Sandholm

b) iterated elimination of dominated strategies (even domination by mixed strategies)
cannot eliminate any strategies,
c) the recursive preprocessing technique (that can eliminate strategies that belong to some
equilibrium as long as some other equilibrium remains) (Conitzer & Sandholm, 2006)
can eliminate at most one strategy per player, and
d) the general eliminability method (Conitzer & Sandholm, 2005b) cannot eliminate any
strategies.
Proof. We first present a family of games, . Let r0 c0 denote a game from this family of
size r0  c0 . The following generator produces such a game where r0 , c0  2. Assign the
utilities,
 u(sr1 , sc1 ) = u(sr2 , sc2 ) = (0, 1) and u(sr1 , sc2 ) = u(sr2 , sc1 ) = (1, 0).
0

Then, for i  [2, b r2 c], set
0

, 1) and u(sr2i1 , sc2 ) = ( 2i2
 u(sr2i1 , sc1 ) = ( r 2i+2
r0
r 0 , 0),
0

 u(sr2i , sc1 ) = ( r (2i1)
, 0) and u(sr2i , sc2 ) = ( 2i1
r0
r 0 , 1).
If r0 is odd there will be one remaining row. In this case, set the following utilities,
0

1
 if r0 is odd, u(sr0 , sc1 ) = ( r10 , 12 ) and u(sr0 , sc2 ) = ( r r1
0 , 2 ).
0

Next, for j  [2, b c2 c], set
0

 u(sr1 , sc2j1 ) = (0, c 2j+2
) and u(sr2 , sc2j1 ) = (1, 2j2
c0
c0 ),
0

 u(sr1 , sc2j ) = (1, c (2j1)
) and u(sr2 , sc2j ) = (0, 2j1
c0
c0 ).
If c0 is odd there will be one remaining column. In this case, set the following utilities,
0

 if c0 is odd, u(sr1 , sc0 ) = ( 12 , c10 ) and u(sr2 , sc0 ) = ( 21 , c c1
0 ).
For example, the game 3,4 is as follows.
3,4
sr1
sr2
sr3

sc1
0,1
1,0
1 1
3, 2

sc2
1,0
0,1
2 1
3,2

sc3
0, 12
1, 12
-3,-3

sc4
1, 14
0, 34
-3,-4

Any game generated in this way has a Nash equilibrium where the row player mixes
evenly between its first two strategies, and the column player mixes evenly among all of its
strategies. This game also has an equilibrium where the column player mixes evenly between
its first two strategies, and the row player mixes evenly among all of its strategies. Thus,
every strategy in r0 c0 is part of some equilibrium. Additionally, each column strategy is a
526

fiAlgorithms for Closed Under Rational Behavior (CURB) Sets

best response to a mixture over the first two row strategies (and, to any column strategy,
one of those two is a best response), and vice-versa. Thus, r0 c0 has a single minimal CURB
set and it includes the entire game.
We now construct an r  c game, with a minimally CURB r0  c0 subset, by putting the
game r0 c0 in the top left and the game (rr0 )(cc0 ) in the bottom right. All other utilities
are set to arbitrary negative values, such that no two are exactly the same. The resulting
game is shown in Figure 5.

Figure 5: r  c game with arbitrary reduction to an r0  c0 CURB set, irreducible by prior
techniques.
It is irreducible by (iterated) dominance and by general eliminability because every
strategy participates in some Nash equilibrium. The game is irreducible (other than a
single strategy per player) by the recursive preprocessor because the row players utilities
are distinct within each column, and the column players utilities are distinct within each
row (except for the last row or column in each  game, if it has an odd number of rows or
columns).
Three factors curb the promise of minimal CURB set algorithms as powerful preprocessors for Nash equilibrium finding. First, the fastest Nash equilibrium finding algorithms,
while requiring exponential time in the worst case, tend to run faster than the CURB set
finding algorithms on many types of games (at least the best known implementations of
these algorithms). Second, the smallest CURB set can be arbitrarily large (up to the size
of the entire game, in which case the preprocessor does not eliminate any strategies from
consideration). Third, as we will now show, even after the smallest minimal CURB set has
been identified, the remaining search space (CURB set size) can be arbitrarily larger than
the size of the supports of a contained Nash equilibrium.
To prove this, we will use the following family of games that contain large minimal
CURB sets and small-support equilibria. For any integer k > 0, we define the game k as
follows. As in the previous proof, assign the utilities u(sr1 , sc1 ) = u(sr2 , sc2 ) = (0, 1) and
u(sr1 , sc2 ) = u(sr2 , sc1 ) = (1, 0), and let Z be an arbitrarily large value (essentially ).
Then, for i  [3, 2 + k],
 u(sri , sc1 ) = (Z, ), u(sr1 , sci ) = (, Z),
 u(sri , sci ) = (0, 0),
 u(sri , sci1 ) = (1 + , 0), u(sri1 , sci ) = (0, 1 + ), and
527

fiBenisch, Davis, & Sandholm

 for all j > i + 1 and j  2 + n,
u(sri , scj ) = (0, Z), and u(srj , sci ) = (Z, 0)
For example, the game 2 is as follows.
2
sr1
sr2
sr3
sr4

sc1
0,1
1,0
Z,
Z,

sc2
1,0
0,1
1+,0
Z, 0

sc3
,Z
0,1+
0,0
1+,0

sc4
,Z
0,Z
0,1 + 
0,0

With respect to the strategic structure of games from this class, we have the following
results.
Lemma 1. For i > 2, the row (column) players strategy sri (sci ) is a best response to the
column (row) players strategy sci1 (sri1 ). The column (row) players strategy sc1 (sr1 ) is
a best response to the row (column) players strategy srn+2 (scn+2 ).
Proposition 6. k has a single minimal CURB set and it includes the entire game.
Proof. Strategies sr1 , sr2 , sc1 , and sc2 must be included in some minimal CURB set, as they
are the best responses to each other in the subgame containing them, and this subgame
admits no pure-strategy Nash equilibrium. Based on Lemma 1, we can see that when i = 3,
the row (column) players strategy sr3 (sc3 ) is a best response to the column (row) players
second strategy. This forces the third strategy of each player into the minimal CURB set
containing the first two strategies of each player, and inductively each additional strategy
is added in the same way.
Proposition 7. In k , the only Nash equilibrium is the the mixed-strategy profile where
sr1 , sr2 , sc1 , and sc2 are each played with probability 21 .
Proof. Assume, for contradiction, that this is not the case, that is, there exists a mixture,
mr , over the rows Mr , comprising the row players profile in a Nash equilibrium, and
sr1 
/ Mr . Along with our assumption, the definition of Nash equilibrium implies that there
must exist a mixture, mc , over columns Mc such that r (mc ) = Mr and c (mr ) = Mc .
Since sr1 is not in Mr by assumption, there exists i > 1 such that sri is the lowest numbered
support in Mr , and the definition of  specifies the outcome, u(sri , scj ) = (0, Z), when
j > i + 1.
The column players Nash equilibrium supports cannot contain any such scj , because
placing any positive probability on this strategy will lead to a highly negative expected
payoff and playing the pure strategy sc1 provides guaranteed payout of at least 0. If we
exclude these strategies, sci+1 (the highest-numbered remaining column strategy) is the only
remaining strategy, other than sc1 , which provides non-zero utility against mixtures on rows
 i. In other words, it dominates all column strategies on the row players supports Mr ,
aside from one: sc1 .
528

fiAlgorithms for Closed Under Rational Behavior (CURB) Sets

Since dominated strategies cannot be played in equilibrium, Mc is constrained to a
subset of {sc1 , sci+1 }. If Mc contains sc1 , Mr must not include any srj with j > 2, due to
the highly negative expected payoff of any mixture including such strategies (as discussed
above). In this case, the only remaining possible equilibrium row mixture is the pure
strategy sr2 , the best response to which is sc3 . Since, by Corollary 2, n has no pure
Nash equilibrium, this cannot constitute an equilibrium, contradicting our assumption.
Alternatively, if Mc does not include sc1 , then mc must be the pure strategy sci+1 , and
Lemma 1 provides a pure-strategy best response to any pure strategy with i > 2. This
would, again, form a pure strategy Nash equilibrium, which we have shown cannot exist.
The above reasoning can also be inverted to show that a contradiction is caused by the
assumption that Mc does not contain sc1 .
We have shown that the row players equilibrium mixture must contain r1 and r2 , and
that the column players equilibrium mixture must contain c1 and c2 and no other strategies
can be in either players supports, since it would lead one of them to have to highly negative
utility.
The  game demonstrates that it is possible to construct very large CURB sets that are
loose around the supports of an enclosed Nash equilibrium, giving us the following general
result.
Theorem 4. A Nash equilibrium with supports consisting of two strategies for each player
can be the only Nash equilibrium in an arbitrarily large minimal CURB set.
These results imply that minimal CURB set algorithms will not always be effective as
preprocessors for Nash equilibrium finding. However, on game instances that have a small
CURB set or a relatively tight minimal CURB set,2 these algorithms have the potential to
yield a significant speed improvement.
Furthermore, the existence of the polynomial-time algorithm for detecting a games
smallest CURB set (small MC) allows us to offer the following theoretical result of potential
general interest.
Theorem 5. The complexity of finding a Nash equilibrium for a two-player normal-form
game can be super-polynomial only in the size of the games smallest CURB set (not in the
size of the full game).
The relationship between the complexity of finding a minimal CURB set and that of
finding a Nash equilibrium is surprising in several ways. For one, it is not obvious that
finding a minimal CURB set should be easier than finding a Nash equilibrium, since, like
Nash equilibria, CURB sets have an exponential space of possible supports which are chosen through maximization processes for both players. Yet from a theoretical, worst-case
perspective, Nash equilibrium finding is PPAD-complete (which is widely believed to be a
strictly harder complexity class than P) and, as we showed earlier in this paper, minimal
CURB set finding is polynomial time.
It is worth noting that games with very small support equilibria, which include all games
with very small CURB sets, are already known to be easily solvable for Nash equilibria
2. If the game has a relatively tight CURB set, a Nash equilibrium can be found quickly by enumerating
strategies of the CURB to be left out from the supports.

529

fiBenisch, Davis, & Sandholm

using techniques such as support enumeration. In particular, on games whose smallest
CURB set size is logarithmic in the full game size, both support enumeration and CURB
set preprocessing permit a guarantee of polynomial runtime in finding a Nash equilibrium.
CURB set preprocessing has the additional advantage that it can also be used to simplify
games with larger equilibrium supports, where support enumeration is exponential. For
example, consider the Gk game class described by Sandholm, Gilpin and Conitzer (2005),
which generates games with a single equilibrium, and that equilibrium contains half the
strategies in its support. We also determined that these games have a single CURB set,
and that CURB set includes exactly the supports of the equilibrium. Games from this class
can be padded, using the embedding technique in our Theorem 3, to become arbitrarily
large games without introducing any additional equilibria or CURB sets. On those games,
CURB set detection offers a polynomial-time method for reducing the game to the point
where algorithms not based on support enumeration can be applied.
What about the complexity of the two problems (Nash equilibrium finding and CURB
set finding) in practice? As Figure 6 shows, the average runtimes of our smallest CURB
set finding algorithm and the Lemke-Howson Nash equilibrium finding algorithm (using the
implementation in Gambit, McKelvey, McLennan, & Turocy, 2004) seem to scale similarly
with input game size (when one does not explicitly generate those pathological cases which
produce exponential behavior in the latter, Savani & von Stengel, 2004). In fact, the CURB
set algorithms are slower (by two orders of magnitude) on average than Lemke-Howson.
This experimental performance agrees with intuition, but is the reverse of the theoretical
state of knowledge regarding worst-case complexity.

1000

small_MC
Lemke Howson

100
10
1
0.1
0.01

Covariant games
Logscale runtime (sec)

Logscale runtime (sec)

Random games

20 30 40 50 60 70 80 90 100
Game size (n)

1000

small_MC
Lemke Howson

100
10
1
0.1
0.01
20 30 40 50 60 70 80 90 100
Game size (n)

Figure 6: Average runtime and 95% confidence intervals of small MC and Lemke Howson
as a function of game size.

It is worth pointing out that an algorithm that builds in part on our work for one of
the CURB set problems (finding all minimal CURB sets) has been presented in a working
paper (Klimm, Sandholm, & Weibull, 2010), and it appears to scale more favorably than
ours. However, the algorithm has not been directly compared with Lemke-Howson or any
other Nash equilibrium finding algorithms, so its relative value related to preprocessing
remains to be seen.
530

fiAlgorithms for Closed Under Rational Behavior (CURB) Sets

The root cause of the complexity of Nash equilibrium search has proved elusive, as two
candidates that were considered potential culprits have been shown not to affect worstcase complexity: games with two players and binary utilities are just as difficult as the
general case, even if both restrictions apply simultaneously (Chen & Deng, 2006; Abbott
et al., 2005). The fact that bounding the smallest CURB set size does serve to bound
the difficulty of Nash equilibrium search suggests that we can further isolate the cause of
equilibrium search complexity as being endemic to minimal CURB sets, rather than games
in general. In this regard, we observe that the special two-player game used by Chen and
Deng to show PPAD-completeness (Chen & Deng, 2006) is itself a single minimal CURB
set, and remains such under Abbott et al.s (Abbott et al., 2005) transformation to binary
utilities.

6. Conclusions
We presented a thorough computational treatment of CURB sets, an important set-valued,
game-theoretic solution concept, including several algorithms for finding CURB sets in
two-player, normal-form games. Our algorithms find all minimal CURB sets (all MC),
one minimal CURB set (one MC), and a smallest minimal CURB set (small MC), all in
polynomial time. The algorithms are based on basic properties of CURB sets that we prove,
such as the fact that minimal CURB sets cannot overlap. The algorithms use dovetailing
with a priority queue, and exploiting information across overlapping, non-minimal CURB
sets, to further improve speed.
Experiments on random games showed that, unsurprisingly, small MC tends to be the
fastest, one MC is second, and all MC is the slowest. However, on covariant games the speed
advantage of the former two disappears. The runtime of those algorithms is primarily
determined by the size of the smallest CURB set, and on covariant games, which tend to
have larger CURB sets, those algorithms (especially one MC) suffer.
Our algorithms also enable the study of CURB set size distributions of different game
classes. We showed that the instance distributions from GAMUT are mainly extremal, in
the sense that a given game generator will yield mostly games with pure-strategy equilibria
and/or games where the game itself is the sole minimal CURB set. However, curiously,
some of the generators yield a significant number of games with smallest CURB sets of
specific non-extremal sizes.
We also examined the potential for using our algorithms as preprocessors for Nash equilibrium finding algorithms. We proved that our technique can eliminate an arbitrarily large
portion of the game from consideration, while guaranteeing that the remaining subgame
contains at least one Nash equilibrium from the full game. This is the case even for games
where prior preprocessing techniques, including the iterated removal of dominated strategies, are powerless.
On the downside, we showed that the smallest CURB set can be arbitrarily large and/or
arbitrarily loose. Furthermore, on many distributions, we showed that current Nash equilibrium finding algorithms run faster, on average, than the CURB set algorithms. This is
surprising in that the theoretical worst-case complexity of the two problems is the reverse.
We demonstrated that the worst-case complexity of finding a Nash equilibrium is polynomial in all known aspects of the game except the size of its smallest CURB set. Taken
531

fiBenisch, Davis, & Sandholm

together with our CURB set finding algorithms that are polynomial time even in the worst
case, and the fact that Nash equilibrium finding is super-polynomial in the worst case (unless PPAD=P), we observe that the essence of the worst-case complexity of finding a Nash
equilibrium is the complexity of finding a Nash equilibrium within a minimal CURB set.
While the CURB set definition is for any number of players, we presented all of our
algorithms for the two-player setting. For a larger number of players, the only obstacle to
finding minimal CURB sets is finding conditional best responses quickly as a subroutine.
We showed that this problem can be solved fast for two playersin these settings it involves
solving a simple linear feasibility program. However, the mathematical program we use is
of degree p  1, where p is the number of players, and with three players the constraints
are already quadratic. On the plus side, our algorithms only make a polynomial number
of calls to this subroutine. Therefore, if future research is able to identify polynomial-time
algorithms for finding all of a players conditional best responses in n-player games, then
our CURB set algorithms will also be polynomial time in those settings.

Acknowledgments
This material is based upon work supported by National Science Foundation ITR grant
0205435, IGERT grant 9972762, and IIS grants 0121678, 0427858, and 0905390, as well
as Office of Naval Research grant N00014-02-1-0973, and a Sloan Fellowship. We would
also like to thank our anonymous reviewers, Vincent Conitzer, and Andrew Gilpin for their
helpful input and advice.

References
Abbott, T., Kane, D., & Valiant, P. (2005). On the complexity of two-player win-lose games.
In Proceedings of the Symposium on Foundations of Computer Science (FOCS), pp.
113122.
Basu, K., & Weibull, J. W. (1991). Strategy subsets closed under rational behavior. Economics Letters, 36 (2), 141146.
Battigalli, P., & Siniscalchi, M. (2003). Rationalizable bidding in first-price auctions. Games
and Economic Behavior, 45 (1), 3872.
Bernheim, B. D. (1984). Rationalizable strategic behavior. Econometrica, 52 (4), 100728.
Brandt, F., Brill, M., Fischer, F., & Harrenstein, P. (2009). Computational aspects of
Shapleys saddles. In Proceedings of the International Conference on Autonomous
Agents and Multi-Agent Systems (AAMAS), pp. 209216.
Chen, X., & Deng, X. (2006). Settling the complexity of two-player Nash-equilibrium.
In Proceedings of the Symposium on Foundations of Computer Science (FOCS), pp.
261272.
Conitzer, V., & Sandholm, T. (2005a). Complexity of (iterated) dominance. In Proceedings
of the ACM Conference on Electronic Commerce (ACM EC), pp. 8897.
532

fiAlgorithms for Closed Under Rational Behavior (CURB) Sets

Conitzer, V., & Sandholm, T. (2005b). A generalized strategy eliminability criterion and
computational methods for applying it.. In Proceedings of the National Conference
on Artificial Intelligence (AAAI), pp. 483488.
Conitzer, V., & Sandholm, T. (2006). A technique for reducing normal form games to compute a Nash equilibrium. In Proceedings of the International Conference on Automated
Agents and Multi-Agent Systems (AAMAS), pp. 537544.
Daskalakis, C., Goldberg, P. W., & Papadimitriou, C. H. (2009). The complexity of computing a nash equilibrium. Communications of the ACM, 52 (2), 8997.
Gilboa, I., Kalai, E., & Zemel, E. (1993). The compleixty of eliminating dominated strategies. Mathematics of Operations Research, 18, 553565.
Gilboa, I., & Zemel, E. (1989). Nash and correlated equilibria: Some complexity considerations. Games and Economic Behavior, 1, 8093.
Hurkens, S. (1995). Learning by forgetful players. Games and Economic Behavior, 11 (1),
304329.
Jordan, P., & Wellman, M. (2010). Algorithms for finding approximate formations in games.
In Proceedings of the National Conference on Artificial Intelligence (AAAI), pp. 798
804.
Klimm, M., Sandholm, T., & Weibull, J. W. (2010). Finding all minimal sCURB sets in
finite games. Mimeo, 3/24/2010.
Knuth, D. E., Papadimitriou, C. H., & Tsitsiklis, J. N. (1988). A note on strategy elimination
in bimatrix games. OR Letters, 7 (3), 103107.
Lemke, C., & Howson, J. (1964). Equilibrium points of bimatrix games. Journal of the
Society of Industrial and Applied Mathematics, 12, 413423.
McKelvey, R. D., McLennan, A. M., & Turocy, T. L. (2004). Gambit: Software tools for
game theory, version 0.97.1.5. http://econweb.tamu.edu/gambit.
Nudelman, E., Wortman, J., Shoham, Y., & Leyton-Brown, K. (2004). Run the GAMUT:
A comprehensive approach to evaluating game-theoretic algorithms.. In Proceedings
of the International Conference on Automated Agents and Multi-Agent Systems (AAMAS), pp. 880887.
Pearce, D. G. (1984). Rationalizable strategic behavior and the problem of perfection.
Econometrica, 52 (4), 102950.
Porter, R., Nudelman, E., & Shoham, Y. (2004). Simple search methods for finding a
Nash equilibrium. In Proceedings of the National Conference on Artificial Intelligence
(AAAI), pp. 664669.
Pruzhansky, V. (2003). On finding CURB sets in extensive games. International Journal
of Game Theory, 32 (2), 205210.
Sandholm, T., Gilpin, A., & Conitzer, V. (2005). Mixed-integer programming methods
for finding Nash equilibria. In Proceedings of the National Conference on Artificial
Intelligence (AAAI), pp. 495501.
533

fiBenisch, Davis, & Sandholm

Savani, R., & von Stengel, B. (2004). Exponentially many steps for finding a Nash equilibrium in a bimatrix game. In Proceedings of the Symposium on Foundations of
Computer Science (FOCS), pp. 258267.
Voorneveld, M., Kets, W., & Norde, H. (2005). An axiomatization of minimal CURB sets.
International Journal of Game Theory, 33, 479490.
Ye, Y. (2006). Improved complexity results on solving real-number linear feasibility problems. Mathematical Programming, 106 (2), 339363.

534

fiJournal of Artificial Intelligence Research 38 (2010) 1-48

Submitted 11/09; published 05/10

Using Local Alignments for Relation Recognition
Sophia Katrenko
Pieter Adriaans
Maarten van Someren

S.Katrenko@uva.nl
P.W.Adriaans@uva.nl
M.W.vanSomeren@uva.nl

Informatics Institute, University of Amsterdam
Science Park 107, 1098XG Amsterdam, the Netherlands

Abstract
This paper discusses the problem of marrying structural similarity with semantic relatedness for Information Extraction from text. Aiming at accurate recognition of relations,
we introduce local alignment kernels and explore various possibilities of using them for this
task. We give a definition of a local alignment (LA) kernel based on the Smith-Waterman
score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences. We show how distributional similarity
measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge. Our experiments suggest that the LA kernel yields promising results
on various biomedical corpora outperforming two baselines by a large margin. Additional
series of experiments have been conducted on the data sets of seven general relation types,
where the performance of the LA kernel is comparable to the current state-of-the-art results.

1. Introduction
Despite the fact that much work has been done on automatic relation extraction (or recognition) in the past few decades, it remains a popular research topic. The main reason for the
keen interest in relation recognition lies in its utility. Once concepts and semantic relations
are identified, they can be used for a variety of applications such as question answering
(QA), ontology construction, hypothesis generation and others.
In ontology construction, the relation that is studied most is the is-a relation (or hypernymy), which organizes concepts in a taxonomy (Snow, Jurafsky, & Ng, 2006). In information retrieval, semantic relations are used in two ways, to refine queries before actual
retrieval, or to manipulate the output that is returned by a search engine (e.g. identifying
whether a fragment of text contains a given relation or not). The most widely used relations
for query expansion are hypernymy (or broader terms from a thesaurus) and synonymy.
Semantic relations can also be useful at different stages of question answering. They have
to be taken into account when identifying the type of a question and they have to be considered at actual answer extraction time (van der Plas, 2008). Yet another application of
relations is constructing a new scientific hypothesis given the evidence found in text. This
type of knowledge discovery from text is often based on co-occurrence analysis and, in many
cases, was corroborated via experiments in laboratories (Swanson & Smalheiser, 1999).
Another reason why extraction of semantic relations is of interest lies in the diversity of
relations. Different relations need different extraction methods. Many existing information
extraction systems were originally designed to work for generic data (Grishman & Sundheim, 1996), but it became evident that domain knowledge is often necessary for successful

c
2010
AI Access Foundation. All rights reserved.

fiKatrenko, Adriaans, & van Someren

extraction. For instance, relation extraction in the biomedical domain would require an
accurate recognition of named entities such as gene names (Clegg, 2008), and in the area
of food it needs information on relevant named entities such as toxic substances.
Also for generic relations syntactic information is often not sufficient. Consider, for
instance, the following sentences (with the arguments of the relations written in italics):
(1)

Mary looked back and whispered: I know every tree in this forest, every scent.
(Part-Whole relation)

(2)

A person infected with a particular flu virus strain develops antibodies against that
virus. (Cause-Effect relation)

(3)

The apples are in the basket. (Content-Container relation)

All these sentences exemplify binary relations, namely Part-Whole (tree is part of a
forest), Cause-Effect (virus causes flu) and Content-Container (apples are contained
in basket). We can easily notice that the syntactic context in (1) and (3) is the same, namely,
the arguments in both cases are connected to each other by the preposition in. However,
this context is highly ambiguous because even though it allows us to reduce the number
of potential semantic relations, it is still not sufficient to be able to discriminate between
Part - Whole and Content - Container relation. In other words, world knowledge
about trees, forests, apples and baskets is necessary to classify relations correctly. The
situation changes even more drastically if we consider example (2). Here, there is no explicit
indication for causation. Nevertheless, by knowing what a flu and a virus is, we are
able to infer that Cause - Effect relation holds.
The examples in (1), (2) and (3) highlight several difficulties that characterize semantic
relation extraction. Generic relations very often occur in nominal complexes such as flu
virus in (2) and lack of sentential context boosts such approaches as paraphrasing (Nakov,
2008). However, even for noun compounds one has to combine world knowledge with the
compounds context to arrive at the correct interpretation.
Computational approaches to the relation recognition problem often rely on a two-step
procedure. First, the relation arguments are identified. Depending on the relation at hand,
this step often involves named entity recognition of the arguments of the relations. The
second step is to check whether the relation holds. If relation arguments are provided (e.g.,
basket and apples in (3)), relation extraction is reduced to the second step. Previous
work on relation extraction suggests that in this case the accuracy of relation recognition
is much higher than in the case when they have to be discovered automatically (Bunescu
et al., 2005). Furthermore, most existing solutions to relation extraction (including work
presented in this paper) focus on relation examples that occur within a single sentence and
do not consider discourse (McDonald, 2005). Recognizing relations from a wider scope is
an interesting enterprise, but it would require to take into account anaphora resolution and
other types of linguistic analysis.
Approaches to relation extraction that are based on hand-written patterns are timeconsuming and in many cases need an expert to formulate and test the patterns. Although
patterns are often precise, they usually produce poor recall (Thomas et al., 2000). In
general, hand-written patterns can be of two types. The first type is sequential and based
2

fiUsing Local Alignments for Relation Recognition

on frequently occurring sequences of words in a sentence. Hand-written sequential patterns
were initially used for extraction of Hypernymy (Hearst, 1992), with several attempts to
extend them to other relations. The second type of patterns (Khoo, Chan, & Niu, 2000) take
the syntactic structure of a sentence into account. The dependency structure of a sentence
can usually be represented as a tree and the patterns then become subtrees. Such patterns
are sometimes referred to as graphical patterns. To identify examples of the Cause-Effect
relation, Khoo et al. (2000) applied this type of patterns to texts in the medical domain.
This study showed that graphical patterns are sensitive to the errors made by the parsers,
do not cover all examples in the test data and extract many spurious instances.
An alternative to using hand-written patterns is supervised Machine Learning. Then,
relations are labeled and used to train a classifier that can recognize these relations in new
texts. One approach is to learn generalized extraction patterns where patterns are expressed
as characters, words or syntactic categories of words. Other approaches involve clustering
based on co-occurrence (Davidov & Rappoport, 2008). In recent years kernel-based methods
have become popular because they can handle high-dimensional problems (Zelenko et al.,
2003; Bunescu & Mooney, 2006; Airola et al., 2008). These methods transform text fragments, complete sentences or segments around named entitites or verbs, to vectors, and
apply Support Vector Machines to classify new fragments.
Some Machine Learning methods use prior knowledge that is given to the system in
addition to labeled examples (Scholkopf, 1997, p. 17). The use of prior knowledge is often
motivated by, for example, poor quality of data and data sparseness. Prior knowledge can be
used in many ways, from changing the representation of existing training examples to adding
more examples from unlabelled data. For NLP tasks, prior knowledge exists in the form of
manually (or automatically) constructed ontologies or large collections of unannotated data.
These enrich the textual data and thereby improve the recognition of relations (Sekimizu,
Park, & Tsujii, 1998; Tribble & Fahlman, 2007). Recently, Zhang et al. (2008) showed that
semantic correlation of words can be learned from unlabelled text collections, transferred
among documents and used further to improve document classification. In general, while
use of large collections of text allows us to derive almost any information needed, it is done
with varying accuracy. In contrast, existing resources created by humans can provide very
precise information, but it is less likely that they will cover all possible areas of interest.
In this paper, as in the work of Bunescu and Mooney (2006), we use the syntactic
structure of sentences, in particular, dependency paths. This stems from the observation
that linguistic units are organized in complex structures and understanding how words or
word senses relate to each other often requires contextual information. Relation extraction
is viewed as a supervised classification problem. A training set consists of examples of a
given relation and the goal is to construct a model that can be applied to a new, unseen
data set, to recognize all instances of the given relation in this new data set. For recognition
of relations we use a kernel-based classifier that is applied to dependency paths. However,
instead of a vector-based kernel we directly use similarity between dependency paths and
show how information from existing ontologies or large text corpora can be employed.
The paper is organized as follows. We start by reviewing existing kernel methods that
work on sequences (Section 2). In Section 3, we give the definition of a local alignment kernel
based on the Smith-Waterman measure. We proceed by discussing how it can be used in
the context of natural language processing (NLP) tasks, and particularly for extracting
3

fiKatrenko, Adriaans, & van Someren

relations from text (Section 3.2). Once the method is described, we report on two types of
the data sets (biomedical and generic) used in the experiments (Section 4) and elaborate
on our experiments (Sections 5 and 6). Section 7 discusses our findings in more detail.
Section 8 concludes the paper by discussing possible future directions.

2. Kernel Methods
The past years have witnessed a boost of interest in kernel methods, their theoretical analysis
and practical applications in various fields (Burges, 1998; Shawe-Taylor & Christianini,
2000). The idea of having a method that works with different structures and representations,
starting from the simplest representation using a limited number of attributes to complex
structures such as trees, seems indeed very attractive.
Before we define a kernel function, recall the standard setting for supervised classification. For a training set S of n objects (instances) (x1 , y1 ), . . . , (xn , yn ) where x1 , . . . , xn  X
are input examples in the input space X with their corresponding labels y1 , . . . , yn  {0,1},
the goal is to infer a function h : X  {0, 1} such that it approximates a target function t.
However, h can still err on the data which has to be reflected in a loss function, l(h(xi ), yi ).
Several loss functions have been proposed in the literature so far, the best known of which
is the zero-one loss. This loss is a function that outputs 1 each time a method errs on a
data point (h(xi ) 6= yi ), and 0 otherwise.
The key idea of kernel methods lies in the implicit mapping of objects to a highdimensional space (by using some mapping function ) and considering their inner product
(similarity) k(xi , xj ) =< (xi ), (xj ) >, rather than representing them explicitly. Functions that can be used in kernel methods have to
symmetric and positive semi-definite,
Pbe
n Pn
whereby positive semi-definiteness is defined by i=1 j=1 ci cj k(xi , xj )  0 for any n > 0,
any objects x1 , . . . , xn  X , and any choice of real numbers c1 , . . . , cn  R. If a function
is not positive semi-definite, the algorithm may not find the global optimal solution. If
the requirements w.r.t. symmetry and positive semi-definiteness are met, a kernel is called
valid.
Using the idea of a kernel mapping, Cortes and Vapnik (1995) introduced support vector
machines (SVM) as a method which seeks the linear separation between two classes of the
input points by a function f (x) such that f (x) = wT (x) + b, wT  Rp , b  R and
h(x) = sign(f (x)). Here, wT stands for the slope of the linear function and b for its
offset. Often, there can exist several functions that separate data well, but not all of them
are equally good. A hyperplane that separates mapped examples with the largest possible
margin would be the best option (Vapnik, 1982).
SVMs solve the following optimization problem:
n

X
1
k w k2 +C
l(h(xi ), yi )
f (x)=wT x+b 2
argmin

(4)

i=1

In Equation 4, the first part of the equation corresponds to the margin maximization
(by minimizing 12 k w k2 ), while the second takes into account the error on the training
set which has to be minimized (where C is a penalty term). The hyperplane that is found
may correspond to a non-linear boundary in the original input space. There exist a number
4

fiUsing Local Alignments for Relation Recognition

of standard kernels such as the linear kernel, the Gaussian kernel and others. Information
about the data or the problem can motivate the choice of a particular kernel. It has been
shown by Haussler (1999) that a complex kernel (referred to as a convolution kernel ) can
be defined using simpler kernels.
Other forms of machine learning representations for using prior knowledge were defined
along with the methods for exploiting it. Inductive logic programming offers one possible
solution to use it explicitly, in the form of additional Horn clauses (Camacho, 1994). In the
Bayesian learning paradigm information on the hypothesis without seeing any data is encoded in a Bayesian prior (Mitchell, 1997) or in a higher level distribution in a hierarchical
Bayesian setting. It is less obvious though how to represent and use prior knowledge in other
learning frameworks. In the case of SVMs, there are three possible ways of incorporating
prior knowledge (Lauer & Bloch, 2008). These are named sampling methods (prior knowledge is used here to generate new data), kernel methods (prior knowledge is incorporated
in the kernel function by, for instance, creating a new kernel), and optimization methods
(prior knowledge is used to reformulate the optimization problem by, for example, adding
additional constraints). The choice of a kernel can be based on general statistical properties
of the domain, but an attractive possibility is to incorporate explicit domain knowledge into
the kernel. This can improve a kernel by smoothing the space: instances that are more
similar have a higher probability of belonging to the same class than with a kernel without
prior knowledge.
In what follows, we review a number of kernels on strings that have been proposed
in the research community over the past years. A very natural domain to look for them
is the biomedical field where many problems can be formulated as string classification
(protein classification and amino acid sequences, to name a few). Sequence representation
is, however, not only applicable to the biomedical area, but can also be considered for
many natural language processing tasks. After introducing kernels that have been used in
biomedicine, we move to the NLP domain and present recent work on relation extraction
employing kernel methods.
2.1 The Spectrum Kernel
Leslie, Eskin, and Noble (2002) proposed a discriminative approach to protein classification.
For any sequence x  X , the authors define the m-spectrum as the set S of all contiguous
subsequences of x whose length is equal to m. All possible m-long subsequences q  S
are indexed by the frequency of their occurrence (q (x)). Consequently, a feature map for
a sequence x and alphabet A equals m (x) = (q (x))qAm . The spectrum kernel for two
sequences x and y is defined as the inner product between the corresponding feature maps:
kS (x, y) =< m (x), m (y) >.
Now, even assuming contiguous subsequences for small m, the feature space to consider
is very large. The authors propose to detect all subsequences of length m by using a suffix
tree method which guarantees fast computation of the kernel matrix. The spectrum kernel
was tested on the task of protein homology detection, where the best results were achieved
by setting m to a relatively small number (3). The novelty of Leslie et al.s (2002) method
lies in its generality and its low computational complexity.

5

fiKatrenko, Adriaans, & van Someren

2.2 Mismatch Kernels
The mismatch kernel that was introduced later by Leslie et al. (2004) is essentially an extension of the latter. An obvious limitation of the spectrum kernel is that all considered
subsequences are contiguous and should match exactly. In the mismatch kernel the contiguity is preserved while the match criterion is changed. In other words, instead of looking
for all possible subsequences of length m for a given subsequence, one is searching for all
possible subsequences of length m allowing up to r mismatches. Such a comparison will
result in a larger subset of subsequences, but the kernels defined in this way can still be calculated rather fast. The kernel is formulated similarly to the spectrum kernel and the only
major difference is in computing the feature map
P for all sequences. More precisely, a feature
map for a sequence x is defined as m,r (x) = qS m,r (q) where m,r (q) = ( (q))Am .
 (q) is binary and indicates whether sequence  belongs to the set of m-length sequences
that differ from q at most in r elements (1) or it does not (0). It is clear that if r is set to
0, the mismatch kernel is reduced to the spectrum kernel. The complexity of the mismatch
kernel computation is linear with respect to the sum of the sequence lengths.
The authors also show that the mismatch kernel not only yields state-of-the-art performance on a protein classification task but also outputs subsequences that are informative
from a biological point of view.
2.3 Kernel Methods and NLP
One of the merits of kernel methods is the possibility of designing kernels for different structures, such as strings or trees. In the NLP field (and in relation extraction, in particular)
most work roughly falls into two categories. In the first, kernels are defined over the plain
text using sequences of words. The second uses linguistic structures such as dependency
paths or trees or the output of shallow parsing. In this short review we do not take a
chronological perspective but rather start with the methods that are based on sequences
and proceed with the approaches that make use of syntactic information.
In the same year in which the spectrum kernel was designed, Lodhi et al. (2002) introduced string subsequence kernels that provide flexible means to work with text data.
In particular, subsequences are not necessarily contiguous and are weighted according to
their length (using a decay factor ). The length of the subsequences is fixed in advance.
The authors claim that even without the use of any linguistic information their kernels are
able to capture semantic information. This is reflected in the better performance on the
text classification task compared to the bag-of-words approach. While Lodhi et al.s (2002)
kernel works on sequences of characters, a kernel proposed by Cancedda et al. (2003) is
applied to word sequences. String kernels can be also extended to syllable kernels which
proved to do well on text categorization (Saunders, Tschach, & Shawe-Taylor, 2002).
Because all these kernels can be defined recursively, their computation is efficient. For
instance, the time complexity of Lodhi et al.s (2002) kernel is O(n|s||t|), where n is the
length of the subsequence, and t and s are documents.
2.3.1 Subsequence Kernels
In the recognition of binary relations, the most natural way is to consider words located
around and between relation arguments. This approach was taken by Bunescu and Mooney
6

fiUsing Local Alignments for Relation Recognition

(2005b) whose choice of sequences was motivated by textual patterns found in corpora. For
instance, they observed that some relations are expressed by subject-verb-object constructions while others are part of the noun and prepositional phrases. As a result, three types
of sequences were considered: fore-between (words before and between two named entities),
between (words only between two entities) and between-after (words between and after two
entities). The length of sequences is restricted. To handle data sparseness, the authors
generalize over existing sequences using PoS tags, entity types and WordNet synsets. A
generalized subsequence kernel is recursively defined as the number of weighted sparse subsequences that two sequences share. In the absence of syntactic information, an assumption
is made that long subsequences are not likely to represent positive examples and as such
are penalized. This subsequence kernel is computed for all three types of sequences and the
resulting relation kernel is defined as a sum over the three subkernels. Experimental results
on a biomedical corpus are encouraging, showing that the relation kernel performs better
than manually written patterns and an approach based on longest common subsequences.
A method proposed by Giuliano et al. (2006) was largely inspired by the work of Bunescu
and Mooney (2005b). However, instead of looking for subsequences in three types of sequences, the authors treat them as a bag-of-words and define what is called a global kernel
as follows. First, each sequence type (pattern) P is represented by a vector whose elements
are counts of how many times each token was used in P . A local kernel is defined similarly
but only using words surrounding named entities (left and right context). A final shallow
linguistic kernel is defined as the combination of the global and the local kernels. Experiments on biomedical corpora suggest that this kernel outperforms the subsequence kernel
by Bunescu and Mooney.
2.3.2 Distributional Kernels
Recently, O Seaghdha and Copestake (2008) introduced distributional kernels on co- occurrence probability distributions. The co-occurrence statistics they use are in the form of
either syntactic relations or n-grams. They show that it is possible to derive kernels from
such distances as Jensen-Shannon divergence (JSD) or Euclidean distance (L2 ) (Lee, 1999).
JSD is a smoothed version of the Kullback-Leibler divergence, an information-theoretic measure of the dissimilarity between two probability distributions. The main motivation behind
this approach lies in the fact that distributional similarity measures proved to be useful for
NLP tasks. To extract co-occurrence information, the authors use two corpora, the British
National Corpus (BNC) and the Web 1T 5-Gram Corpus (which contains 5-grams with
their observed frequency counts and was collected from the Web). Distributional kernels
proved to be successful for a number of tasks such as compound interpretation, relation
extraction and verb classification. On all of them, the JSD kernel clearly outperforms
Gaussian and linear kernels. Moreover, estimating distributional similarity on the BNC
corpus yields performance similar to the results obtained on the Web 1T 5-Gram Corpus.
This is an interesting finding because the BNC corpus was used to estimate similarity from
syntactic relations whereas the latter corpus contains n-grams only. Most importantly, the
method of O Seaghdha and Copestake provides empirical support for the claim that using
distributional similarity is beneficial for relation extraction.

7

fiKatrenko, Adriaans, & van Someren

2.3.3 Kernels for Syntactic Structures
Kernels defined for unpreprocessed text data seem attractive because they can be applied
directly to text from any language. However, as general as they are, they can lose precision when compared to the methods that use syntactic analysis. Re-ranking parsing
trees (Collins & Duffy, 2001) was one of the first applications of kernel methods to NLP
problems. To accomplish this goal, the authors rely on the subtrees that a pair of trees have
in common. Later on, Moschitti (2006) explored convolution kernels on dependency and
constituency structures to do semantic role labeling and question classification. This work
introduces a novel kernel which is called a partial tree kernel (PT). It is essentially built
on two kernels proposed before, the subtree kernel (ST) that contains all descendant nodes
from a target root (including leaves) and the subset tree kernel (SST) that is more flexible
and allows internal subtrees which do not necessarily encompass leaves. A partial tree is
a generalization of a subset tree whereby partial structures of a grammar are allowed (i.e.,
parts of the production rules such as [VP [V]] form a valid PT). Moschitti demonstrated
that PTs obtain better performance on dependency structures than SSTs, but the latter
yield better results on constituent trees.
2.3.4 Kernel on Shallow Parsing Output
Zelenko et al. (2003) use shallow parsing and designed kernels to extract relations from text.
In contrast to full parsing, shallow parsing produces partial interpretations of sentences.
Each node in such a tree is enriched with information on roles (that correspond to the
arguments of a relation). The similarity of two trees is determined by the similarity of
their nodes. Depending on how similarity is computed, Zelenko et al. define two types of
kernels, contiguous subtree kernels and sparse kernels. Both types were tested on two types
of relations, person-affiliation and organization-location exhibiting good performance. In
particular, sparse kernels outperform contiguous subtree kernels leading to the conclusion
that partial matching is important when dealing with typically sparse natural language
data. However, the computation of the sparse kernel takes O(mn3 ) time (where m and n
are the number of children of two relation examples, i.e. shallow trees, under consideration,
m  n), while the algorithm for the contiguous subtree kernel runs in time O(mn).
2.3.5 Shortest Path Kernel
Bunescu and Mooneys (2005a) shortest path kernel represents yet another approach for
relation extraction that is kernel-based and relies on information found in dependency trees.
A main assumption here is that not the entire dependency structure is relevant, and one
can focus on the path that is connecting two relation arguments instead. The more similar
these paths are, the more likely two relation examples belong to the same category. In spirit
with their previous work, Bunescu and Mooney seek generalizations over existing paths by
adding information sources like part of speech (PoS) categories or named entity types.
The shortest path between relation arguments is extracted and a kernel between two
sequences (paths) x = {x1 , . . . , xn } and x0 = {x01 , . . . , x0m } is computed as follows:

8

fiUsing Local Alignments for Relation Recognition

0

kB (x, x ) =



0Q
n

0
i=1 f (xi , xi )

m 6= n
m=n

(5)

In Equation 5, f (xi , x0i ) is the number of features shared by xi and x0i . Bunescu and
Mooney (2005a) use several features such as word (e.g., protesters), part of speech tag (e.g.,
N N S), generalized part of speech tag (e.g., N oun), and entity type (e.g., P ERSON ) if
applicable. In addition, a direction feature ( or ) is employed. Here we reproduce an
example from their paper.
Example 1 Given two dependency paths that exemplify the relation Located such as his
 actions  in  Brcko and his  arrival  in  Beijing, both paths are expanded by
additional features as those mentioned above. It is easy to see that comparing path (6) to
path (7) gives us a score of 18 (3111213 = 18).





Brcko


his
actions
 NNP
in
 P RP
  []   N N S   [] 
 []  
 N oun
IN
P ERSON
N oun
LOCAT ION













(6)


Beijing

 NNP

 []  

 N oun
LOCAT ION

(7)





arrival
his
  []   N N
  [] 
 P RP
N oun
P ERSON



in
IN



The time complexity of the shortest path kernel is O(n), where n stands for the length
of the dependency path.
Dependency paths are also considered in other recent work on relation recognition (Erkan,
Ozgur, & Radev, 2007). Here, Erkan et al. (2007) use dependency paths as input and
compare them by means of cosine similarity or edit distance. The authors motivate their
choice by the need to compare dependency paths of different length. Further, various machine learning methods are used to do classification, including SVM and transuctive SVM
(TSVM), which is an extension of SVM (Joachims, 1999). In particular, TSVM makes use
of labeled and unlabeled data by first classifying the unlabeled examples and then searching
for the maximum margin that separates positive and negative instances from both sets. The
authors conclude that edit distance performs better than the cosine similarity measure, and
that TSVM slightly outperforms SVM.
Airola et al. (2008) propose a graph kernel which makes use of the entire dependency
structure. In their work, each sentence is represented by two subgraphs, one of which is
built from the dependency analysis, and the other corresponds to the linear structure of the
sentence. Further, a kernel is defined on all paths between any two vertices in the graph.
The method by Airola et al. (2008) achieves state-of-the-art performance on biomedical
data sets, and is further discussed, together with the shortest path kernel and the work

9

fiKatrenko, Adriaans, & van Someren

by Erkan et al. (2007), in Section 5 on relation extraction in the biomedical domain in this
paper.
Finally, kernels can be defined not only on graphs of syntactic structures, but also on
graphs of a semantic network. This is illustrated by O Seaghdha (2009), who uses graph
kernels on the graph built from the hyponymy relations in WordNet. Even though no
syntactic information is utilized, such kernels proved to perform well on the extraction of
various generic relations.
All kernels that we reviewed in this section deal with sequences or trees albeit in different ways. The empirical findings suggest that kernels that allow partial matching usually
perform better when compared to methods where similarity is defined on an exact match.
To alleviate the problem of exact matching, some researchers suggested generalizing over
elements in existing structures (Bunescu & Mooney, 2005a) while others opted for a flexible
comparison. In our view, these types of methods can complement each other (Saunders
et al., 2002). As flexible as the partial matching methods are, they may suffer from low precision when the penalization of the mismatch is low. The same holds for approaches that use
generalization strategies because they may easily overgeneralize. A possible solution would
be to combine both, provided that mismatches are penalized well and generalizations are
semantically plausible rather than based on part of speech categories. This idea is further
explored in the present paper and evaluated on the relation recognition task.
In a nutshell, the goals of this paper are the following: (i) a study of the possibilities
of using the local alignment kernel for relation extraction from text, (ii) an exploration of
the use of prior knowledge in the alignment kernel and (iii) an extensive evaluation with
automatic recognition of two types of relations, biomedical and generic.

3. A Local Alignment Kernel
One can note from our short overview of the kernels designed for NLP above that many
researchers use partial structures and propose variants such as subsequence kernels (Bunescu
& Mooney, 2005b), a partial tree kernel (Moschitti, 2006), or a kernel on shallow parsing
output (Zelenko et al., 2003) for relation extraction. In this paper we focus on dependency
paths as input and formulate the following requirements for a kernel function:
 it should allow partial matching so that the similarity can be measured for paths of
different length
 it should be possible to incorporate prior knowledge
Recall that by prior knowledge we mean information that comes either from larger corpora or from existing resources such as ontologies. For instance, knowing that development
is synonymous to evolution in some contexts can help to recognize that two different words
are close semantically. Such information is especially useful if the meaning is relevant for
detecting relations that may differ in form.
In the following subsection we will define a local alignment kernel that satisfies these
requirements and show how to incorporate prior knowledge.

10

fiUsing Local Alignments for Relation Recognition

3.1 Smith-Waterman Measure and Local Alignments
Our work here is motivated by the recent advances in the biomedical field. It has been shown
that it is possible to design valid kernels based on a similarity measure for strings (Saigo,
Vert, & Akutsu, 2006). For example, Saigo, Vert, Ueda, and Akutsu (2004) consider the
Smith-Waterman (SW) similarity measure (Smith & Waterman, 1981) (see below) to measure the similarity between two sequences of amino acids.
String distance measures can be divided into measures based on terms, edit-distance
and Hidden Markov models (HMM) (Cohen, Ravikumar, & Fienberg, 2003). Term-based
distances such as measures based on the TF-IDF score, consider a pair of word sequences as
two sets of words ignoring their order. In contrast, string edit distances (or string similarity
measures) treat entire sequences and compare them using transformation operations, which
convert a sequence x into a sequence x0 . Examples of these are the Levenshtein distance,
and the Needleman-Wunsch (Needleman & Wunsch, 1970) and Smith-Waterman (Smith
& Waterman, 1981) measures. The Levenshtein distance has been used in the natural
language processing field as a component in a variety of tasks, including semantic role
labeling (Sang et al., 2005), construction of paraphrase corpora (Dolan, Quirk, & Brockett,
2004), evaluation of machine translation output (Leusch, Ueffing, & Ney, 2003), and others.
The Smith-Waterman measure is mostly used in the biological domain, there are, however,
some applications of a modified Smith-Waterman measure to text data as well (Monge &
Elkan, 1996; Cohen et al., 2003). HMM-based measures present probabilistic extensions of
edit distances (Smith, Yeganova, & Wilbur, 2003).
Our hypothesis is that string similarity measures are the best basis for a kernel for
relation extraction. In this case, the order in which words appear is likely to be relevant
and sparse data usually prevents estimation of probabilities (as in the work of Smith et al.,
2003). In general, two sequences can be aligned in several possible ways. It is possible to
search either for an alignment which spans entire sequences (global alignment), or for an
alignment which is based on similar subsequences (local alignment). Both in the case of
sequences of amino acids and in relation extraction, local patterns are likely to be the most
important factor that determines similarity. Therefore we need a similarity measure that
emphasizes local alignments.
Formally, we define a pairwise alignment  of at most L elements for two sequences
x = x1 x2 . . . xn and x0 = x01 x02 . . . x0m , as a pairing  = {l (i, j)}, l = 1, . . . , L, 1  i  n,
1  j  m, 1  l  n, 1  l  m. In Example 2 (ii), the third element of the first sequence
is aligned with the first element of the second one, which is denoted by 1 (3, 1).
Example 2 Given the sequences x=abacde and x0 =ace, two possible alignments (with gaps
indicated by -) are as follows.
(i) global alignment
a
a

b
-

a
-

c
c

d
-

e
e

Alignment:

 = {1 (1, 1), 2 (4, 2), 3 (6, 3)}

a
a

c
c

d
-

e
e

Alignment:

 = {1 (3, 1), 2 (4, 2), 3 (6, 3)}

(ii) local alignment
a
-

b
-

11

fiKatrenko, Adriaans, & van Someren

In this example, the number of gaps inserted in x0 to align it with x and the number
of elements that match is the same in both cases. Yet, both in the biological and in
the linguistic context we may prefer alignment (ii), because closely matching substrings,
local alignments, are a better indicator for similarity than shared items that are far apart.
It is, therefore, better to use a measure that puts less or no weight on gaps before the
start or after the end of strings (as in Example 2 (ii)). This can be done using a local
alignment mechanism that searches for the most similar subsequences in two sequences.
Local alignments are employed when sequences are dissimilar and are of different length,
while global alignments are considered when sequences are of roughly the same length. From
the measures we have mentioned above, the Smith-Waterman measure is a local alignment
measure, and the Needleman-Wunsch measure compares two sequences based on global
alignments.
Definition 1 (Global alignment) Given two sequences x = x1 . . . xn and x0 = x01 . . . x0m ,
their global alignment is a pair of sequences y and y0 both of the same length, which are
obtained by inserting zero or more gaps before the first element of either x or x0 , and after
each element of x and of x0 .
Definition 2 (Local alignment) Given two sequences x = x1 . . . xn and x0 = x01 . . . x0m ,
their local alignment is a pair of subsequences  of x and  of x0 , whose similarity is
maximal.
To clarify what we mean by local and global alignments, we give a definition of both the
Smith-Waterman and Needleman-Wunsch measures. Given two sequences x = x1 x2 . . . xn
and x0 = x01 x02 . . . x0m of length n and m respectively, the Smith-Waterman measure is defined
as a similarity score of their best local alignment:

sw(x, x0 ) =

max

A(x,x0 )

s(x, x0 , )

(8)

In the equation above, s(x, x0 , ) is a score of a local alignment  of sequence x and x0
and A denotes the set of all possible alignments. The best local alignment can be efficiently
found using dynamic programming. To do this, one fills in a matrix SW with partial
alignments as follows:

0



SW(i  1, j  1) + d(xi , x0j )
SW (i, j) = max
1in,
 SW(i  1, j)  G


1jm
SW(i, j  1)  G

(9)

In Equation 9, d(xi , x0j ) denotes a substitution score between two elements xi and x0j and
G stands for a gap penalty. Using this equation it is possible to find partial alignments, that
are stored in a matrix in which the cell (i, j) reflects the score for alignment between x1 . . . xi

12

fiUsing Local Alignments for Relation Recognition

a
c
e

0
0
0
0

a
0
2
1
0

b
0
1
1
0

a
0
2
1
0

c
0
1
4
3

d
0
0
3
3

e
0
0
2
5

a
c
e

(a) Smith-Waterman measure

0
0
0
0

a
0
2
1
0

b
0
1
1
0

a
0
0
0
0

c
0
-1
2
1

d
0
-1
1
1

e
0
-1
0
3

(b) Needleman-Wunsch measure

Table 1: Matrices for computing Smith-Waterman and Needleman-Wunsch scores for sequences x=abacde and x0 =ace, a gap G = 1, substitution score d(xi , x0j ) = 2 for
xi = x0j , and d(xi , x0j ) = 1 for xi 6= x0j .

and x01 . . . x0j . The cell with the largest value in the matrix contains the Smith-Waterman
score.
The Needleman-Wunsch measure, which searches for global alignments, is defined similarly, except for the fact that the cells in a matrix can contain negative scores:

 NW(i  1, j  1) + d(xi , x0j )
NW (i, j) = max
NW(i  1, j)  G
1in,

NW(i, j  1)  G
1jm

(10)

The Smith-Waterman measure can be seen as a modification of the Needleman-Wunsch
method. By disallowing negative scores in a matrix, the regions of high dissimilarity are
avoided and, as a result, local alignments are preferred. Moreover, while the NeedlemanWunsch score equals the largest value in the last column or last row, the Smith-Waterman
similarity score corresponds to the largest value in the matrix.
Let us reconsider Example 2 and show how the global and local alignments for alignments
for two sequences x=abacde and x0 =ace are obtained. To arrive at actual alignments, one
has to set the gap parameter G and the substitution scores. Assume we use the following
settings: a gap G = 1, substitution score d(xi , x0j ) = 2 for xi = x0j , and d(xi , x0j ) = 1
for xi 6= x0j . These values have been chosen for illustrative purpose only, but in a realistic
case, e.g., alignment of protein sequences, the choice of the substitution scores is usually
motivated by biological evidence. For gapping, Smith and Waterman (1981) suggested
to use a gap value which is at least equal to the difference between a match (d(xi , x0j ),
xi = x0j ) and a mismatch (d(xi , x0j ), xi 6= x0j ). Then, the Smith-Waterman and NeedlemanWunsch similarity scores between x and x0 can be calculated according to Equation 9 and
Equation 10 as given in Table 1.
First, the first row and the first column in the matrix are initialized to 0. Then, the
matrix is filled in by computing the maximum score for each cell as defined in Equation 9
and Equation 10. The score of the best local alignment is equal to the largest element in
13

fiKatrenko, Adriaans, & van Someren

the matrix (5), and the Needleman-Wunsch score is 3. Note that it is possible to trace back
which steps are taken to arrive at the final alignment (the cells in boldface). A left-right
step corresponds to an insertion, a top-down step to a deletion (these lead to gaps), and a
diagonal step implies an alignment of two sequences elements.
Since we prefer to use local alignments on dependency paths, a natural choice would
be to use the Smith-Waterman measure as a kernel function. However, Saigo et al. (2004)
observed that the Smith-Waterman measure may not result in a valid kernel because it
may not be positive semi-definite. They give a definition of the LA kernel, which states
that two sequences are similar if they have many local alignments with high scores, as in
Equation 11.
kL (x, x0 ) =

X

0

es(x,x ,)

(11)

A(x,x0 )

Here, s(x, x0 , ) is a local alignment score and ( 0) is a scaling parameter.
To define the LA kernel kL (as in Equation 11) for two sequences x and x0 , it is needed to
take into account all transformation operations that are used in local alignments. First, one
has to define a kernel on elements that corresponds to individual alignments, ka . Second,
since this type of alignment allows gaps, there should be another kernel for gapping, kg . Last
but not least, recall that by local alignments only parts of the sequences may be aligned, and
some elements of x and x0 may be left out. These elements do not influence the alignment
score and a kernel used in these cases, k0 , can be set to a constant, k0 (x, x0 ) = 1. Finally,
the LA kernel is a composition of several kernels (k0 , ka , and kg ), which is in the spirit of
convolution kernels (Haussler, 1999).
According to Saigo et al. (2004), similarity of the aligned sequences elements (ka kernel)
is defined as follows:

0
if |x| =
6 1 or |x0 | =
6 1
0
ka (x, x ) =
(12)
0)
d(x,x
e
otherwise
If either x, or x0 has more than one element, this kernel would result in 0. Otherwise,
it is calculated using the substitution score d(x, x0 ) of x and x0 . This score reflects how
similar two sequences elements are and, depending on the domain, can be computed using
prior knowledge from the given domain.
The gapping kernel is defined similarly to the alignment kernel in Equation 12, whereby
the scaling parameter  is preserved, but the gap penalties are used instead of a similarity
function between two elements:
0

kg (x, x0 ) = e(g(|x|)+g(|x |))

(13)

Here, g stands for the gap function. Naturally, for a gap of length 0 this function returns
zero. For gaps of length n, it is reasonable to define a gap in terms of a gap opening o and
a gap extension e, g(n) = o + e  (n  1). In this case it is possible to decide whether longer
gaps should be penalized more than the shorter ones, and how much. For instance, if there
14

fiUsing Local Alignments for Relation Recognition

are three consecutive gaps in the alignment, the first gap is counted as a gap opening, and
the other two as a gap extension. If in consecutive gaps (i.e., gaps of length n > 1) each gap
is of equal importance, the gap opening has to be equal to the gap extension. If, however,
the length of gaps does not matter, one would prefer to penalize the gap opening more, and
to give a little weight to the gap extension.
All these kernels can be combined as follows:
k(r) (x, x0 ) = k0  (ka  kg )r1  ka  k0

(14)

In Equation 14, k(r) (x, x0 ) stands for an alignment of r elements in x and x0 with possibly
r  1 gaps. Similarity of the aligned elements is calculated by ka , and gapping by kg . Since
there could be up to r  1 gaps, this corresponds to the following part of the equation:
(ka  kg )r1 . Further, because there is the rth aligned element, one more ka is added. Given
the discussion above, k0 is added to the initial and final part. As follows from Equation 14,
if there are no elements in x and x0 aligned, k(r) equals k0 , which is 1. If all elements of x
and x0 are aligned with no gaps, the value of k(r) is (ka )r .
Finally, the LA kernel is equal to the sum taken over all possible local alignments for
sequences x and x0 :

0

kL (x, x ) =


X

k(i) (x, x0 )

(15)

i=0

The results in the biological domain suggest that kernels based on the Smith-Waterman
distance are more relevant for the comparison of amino acids than string kernels (Saigo et al.,
2006). It is not clear whether this holds when applied to natural language processing tasks.
In our view, it could depend on the parameters which are used, such as the substitution
matrix and the penalty gaps.
3.1.1 Computational complexity
The LA kernel, as many other kernels discussed in Section 2, can be efficiently calculated using dynamic programming. For any two sequences x and x0 , of length n and m respectively,
its complexity is proportional to n  m. Additional costs may come from the substitution matrix, which, unlike in the biomedical domain, can become very large. However, the
look-up of the substitution scores can be done in an efficient manner as well, which leads
to fast kernel computation. For instance, calculating a kernel matrix for the largest data
set used in this paper, AImed (3,763 instances), takes 805 seconds on a 2.93 GHz Intel(R)
Core(TM)2 machine.
3.2 Designing a Local Alignment Kernel for Relation Extraction
The Smith-Waterman measure is based on transformations, in particular deletions of elements that are different between strings. However, elements that are different may still be
similar to some degree. These similarities can be used as part of the similarity measure.
For example, if two elements are words that are different but that are synonyms, then we
count them as less different than when they are completely unrelated. We will call these
15

fiKatrenko, Adriaans, & van Someren

similarities substitution scores (Equation 12) and define them in two different ways: on
the basis of distributional similarity and on the basis of semantic relatedness in an ontology.
For Example 1 we would like to be able to infer that Brcko is similar to Beijing, even
though these two words do not match exactly. Furthermore, if we have phrases his arrival
in Beijing and his arrival in January, then we would like our kernel to say that Brcko is
more similar to Beijing than to January. The use of such information as prior knowledge
makes it possible to measure similarity between two words, one in the test set and the
other in the training set, even if they do not match exactly. Below we review two types of
measures that are based on statistical distributions and on relatedness in WordNet.
3.2.1 Distributional Similarity Measures
There are a number of distributional similarity measures proposed over the years, including
Cosine, Dice and Jaccard coefficients. Distributional similarity measures have been extensively studied before (Lee, 1999; Weeds, Weir, & McCarthy, 2004). The main hypothesis
behind distributional measures is that words occurring in the same context should have
similar meaning (Firth, 1957). Context can be defined either using proximity in text, or
employing grammatical relations. In this paper, we use the first option where context is a
sequence of words in text and its length is set in advance.
Measure

Formula

Cosine

d(xi , x0j ) = P

Dice

d(xi , x0j ) =

L2

d(xi , x0j ) =

P (c|xi )P (c|x0j )
P
0 2
2
c P (c|xi )
c P (c|xj )

P

c

2F (xi )F (x0j )
F (xi )F (x0j )

qP

c (P (c|xi )

 P (c|x0j ))2

Table 2: A list of functions used to estimate distributional similarity measures.
We have chosen the following measures: Dice, Cosine and L2 (Euclidean) whose definitions are given in Table 2. In the definition of Cosine and L2, it is possible to use either
frequency counts or probability estimates derived from unsmoothed relative frequencies.
Here, we adopt the definitions given by Lee (1999), which are based on probability estimates P . Recall that x and x0 are two sequences we would wish to compare, with their
corresponding elements xi and x0j . Further, c stands for a context. In the definition of the
Dice coefficient, F (xi ) = {c : P (c|xi ) > 0}. We are mainly interested in symmetric measures
(d(xi , x0j ) = d(x0j , xi )) because a symmetric positive semi-definite matrix is required by kernel methods. The Euclidean measure as defined in Table 2 does not necessarily vary from 0
to 1. For this reason, given a list of pairs of words (xi , x0j ) where xi is fixed and j = 1, . . . , s
with their corresponding L2 score, the maximum value maxj d(xi , x0j ) is detected and used
to normalize all scores on the list. Furthermore, unlike Dice and Cosine, which return 1 in
the case two words are equal, the Euclidean score equals 0. In the next step, we substract
the obtained normalized value from 1 to ascertain that all scores are within an interval [0, 1]
16

fiUsing Local Alignments for Relation Recognition

and the largest value (1) is assigned to identical words. In our view, this procedure will
make a comparison of the selected distributional similarity measures with respect to their
influence on the LA kernel more transparent.
Distributional similarity measures are very suitable if no other information is available.
In the case that data is annotated by means of some taxonomy (e.g., WordNet), it is
possible to consider measures defined over this taxonomy. Availability of hand-crafted
resources, such as WordNet, that comprise various relations between concepts, enables
making distinctions between different concepts in a subtle way.
3.2.2 WordNet Relatedness Measures
For generic relations, the most commonly used resource is WordNet (Fellbaum, 1998), which
is a lexical database for English. In WordNet, words are grouped together in synsets where
a synset consists of a list of synonymous words or collocations (e.g., fountain pen), and
pointers that describe the relations between this synset and other synsets (Fellbaum, 1998).
WordNet can be employed for different purposes such as studying semantic constraints for
certain relation types (Girju, Badulescu, & Moldovan, 2006; Katrenko & Adriaans, 2008),
or enriching the training set (Giuliano et al., 2007; Nulty, 2007).
To compare two concepts given their synsets c1 and c2 we use five different measures
that have been proposed in the past years. Most of them rely on the notions of the length
of the shortest path between two concepts c1 and c2 , len(c1 , c2 ), the depth of a node in the
WordNet hierarchy (which is equal to the length of the path from the root to the given
synset ci ), dep(ci ), and a least common subsumer (or lowest super-ordinate) between c1
and c2 , lcs(c1 , c2 ), which in turn is a synset. To the measures that are exclusively based
on these notions belong conceptual similarity proposed by Palmer and Wu (1995) (simwup
in Equation 16) and the formula of scaled semantic similarity introduced by Leacock and
Chodorow (1998) (simlch in Equation 17). 1 The major difference between them lies in the
fact that simlch does not consider the least common subsumer of c1 and c2 but uses the
maximum depth of the WordNet hierarchy instead. Conceptual similarity ignores this and
focuses on the subhierarchy that includes both synsets.

simwup (c1 , c2 ) =

2  dep(lcs(c1 , c2 ))
len(c1 , lcs(c1 , c2 )) + len(c2 , lcs(c1 , c2 )) + 2  dep(lcs(c1 , c2 ))

simlch (c1 , c2 ) =  log

len(c1 , c2 )
2  maxcW ordN et dep(c)

(16)

(17)

Aiming at combining information from several sources, Resnik (1995) introduced yet another measure that is grounded in information content (simres in Equation 18). Intuitively,
if two synsets c1 and c2 are located deeper in the hierarchy and the path from one synset to
another is short, they should be similar. If the path between two synsets is long and their
least common subsumer is placed relatively close to the root, this indicates that the synsets
1. In all equations of similarity measures defined over WordNet, subscripts refer to the similarity measure
itself (e.g., lch, wup in simlch and in simwup , respectively)

17

fiKatrenko, Adriaans, & van Someren

c1 and c2 do not have much in common. To quantify this intuition, it is necessary to derive a
probability estimate for lcs(c1 , c2 ) which can be done by employing existing corpora. More
precisely, p(lcs(c1 , c2 )) stands for the probability of encountering an instance of a concept
lcs(c1 , c2 ).
simres (c1 , c2 ) =  log p(lcs(c1 , c2 ))

(18)

One of the biggest shortcomings of Resniks method is the fact that only the least
common subsumer appears in Equation 18. One can easily imagine a full-blown hierarchy
where the relatedness of the concepts subsumed by the same lcs(ci , cj ) can heavily vary.
In other words, by using lcs only, one is not able to make subtle distinctions between two
pairs of concepts that share the least common subsumer. To overcome this, Jiang and
Conrath (1997) proposed a solution that takes into account information about the synsets
being compared (simjcn in Equation 19). By comparing Equation 19 against Equation 18,
we will notice that now the equation incorporates not only the probability of encountering
lcs(c1 , c2 ), but also the probability estimates for c1 and c2 .
simjcn (c1 , c2 ) = 2 log p(lcs(c1 , c2 ))  (log p(c1 ) + log p(c2 ))

(19)

Lin (1998) defined the similarity between two concepts using how much commonality
and differences between them are involved. Similarly to the two previous approaches, he uses
information theoretic notions and derives the similarity measure simlin given in Equation 20.

simlin (c1 , c2 ) =

2  log p(lcs(c1 , c2 ))
log p(c1 ) + log p(c2 )

(20)

In the past, semantic relatedness measures were evaluated on different NLP tasks (Budanitsky & Hirst, 2006; Ponzetto & Strube, 2007) and it can be concluded that no measure
performs the best for all problems. In our evaluation, we use semantic relatedness for the
validation of generic relations and study in depth how they contribute to the final results.
3.2.3 Substitution Matrix for Relation Extraction
Until now, we have discussed two possible ways of calculating the substitution score d(, ), by
using either distributional similarity measures, or measures defined on WordNet. However,
dependency paths which are generated by parsers may contain not only words (or lemmata),
but also syntactic functions such as subjects, objects, modifiers, and others. To take this
into account, we revise the definition of d(, ). We assume sequences x = x1 x2 . . . xn and
x0 = x01 x02 . . . x0m to contain words (xi  W where W refers to a set of words) and syntactic
functions accompanied by direction (xi 
/ W ). The elements of W are unique words (or
lemmata) which are found in the dependency paths, for instance, for the paths his 
actions  in  Brcko and his  arrival  in  Beijing in Example (1) in Section 2.3.5,
W = {his, actions, in, Brcko, arrival, Beijing}. The dependency paths we use in the present
work include information on syntactic functions, for instance awareness
joy. In this case, W = {awareness, come, joy} and W = {
18

prep f rom

prep f rom nsubj



,  }.



nsubj

come 

fiUsing Local Alignments for Relation Recognition

Then,

d(xi , x0j )




 1
0
d0 (xi , x0j ) =


0



0

xi , x0j  W
xi , x0j 
/ W & xi = x0j
0
xi , xj 
/ W & xi 6= x0j
xi  W & x0j 
/W
xi 
/ W & x0j  W

(21)

Equation (21) states that whenever the element xi of the sequence x is compared against
the element x0j of the sequence x0 , their substitution score is equal either to (i) the similarity
score in the case both elements are words (lemmata), or to (ii) 1, if both elements are the
same syntactic function, or to (iii) 0, in any other case.
As follows from our discussion on similarity measures above, there are two ways to define
d(xi , x0j ), using either distributional similarity between xi and x0j (Section 3.2.1), or their
WordNet similarity, provided that they are annotated with WordNet synsets (Section 3.2.2).

4. Experimental Set-up
In this section, we describe the data sets that we have used in the experiments and provide
information on the data collections used for estimating distributional similarity.
4.1 Data
To evaluate the performance of the LA kernel, we consider two types of text data, domainspecific data, which comes from the biomedical domain and generic or domain-independent
data which represents a variety of well-known and widely used relations such as PartWhole and Cause-Effect.
Like other work, we extract a dependency path between two nodes corresponding to the
arguments of a binary relation. We also assume that each analysis results in a tree and since
it is an acyclic graph, there exists a unique path between each pair of nodes. We do not
consider, however, other structures that might be derived from the full syntactic analysis
as in, for example, subtrees (Moschitti, 2006).
4.1.1 Biomedical Relations
Corpora We use three corpora that come from the biomedical field and contain annotations of either interacting proteins - BC-PPI2 (1,000 sentences), AImed (Bunescu & Mooney,
2005b) or the interactions among proteins and genes LLL (77 sentences in the training set
and 87 in the test set, Nedellec, 2005). The BC-PPI corpus was created by sampling sentences from the BioCreAtive challenge, the AImed corpus was sampled from the Medline
collection. The LLL corpus was composed by querying Medline with the term Bacillus subtilis. The difference among all three corpora lies in the directionality of interactions. As
Table 3 shows, relations in the AImed corpus are strictly symmetric, in LLL they are asymmetric and BC-PPI contains both types. The differences in the number of training instances
for the AImed corpus can be explained by the fact that they correspond to the dependency
2. Available from http://www2.informatik.hu-berlin.de/~hakenber/.

19

fiKatrenko, Adriaans, & van Someren

paths between named entities. If parsing fails or produces several disconnected graphs per
sentence, no dependency path is extracted.
Parser
LinkParser
LinkParser
Stanford
Stanford
Enju

Data set
LLL (train)
LLL (test)
BC-PPI
AImed
AImed

#examples
618
476
664
3763
5272

#pos
153
83 a
250
922
918

direction
asymmetric
asymmetric
mixed
symmetric
symmetric

a. Even though the actual annotations for the test data are not given, the number of interactions in the
test data set is provided by the LLL organizers.

Table 3: Statistics of the biomedical data sets LLL, BC-PPI, and AImedd. In this table, #pos
stands for the number of positive examples per data set and #examples indicates
the number of examples in total.

The goal of relation extraction in all three cases is to output all correct interactions
between biomedical entities (genes and proteins) that can be found in the input data. The
biomedical entities are already provided, so there is no need for named entity recognition.
There is a discrepancy between the training and the test sets used for the LLL challenge.
Unlike the training set, where each sentence has an example of at least one interaction, the
test set contains sentences with no interaction. The organizers of the LLL challenge distinguish between sentences with and without coreferences. Sentences with coreferences are
usually appositions, as shown in one of the examples below. The first sentence in (4.1.1) is
an example of a sentence without coreferences (with interaction between ykuD and SigK),
whereas the second one is a sentence with coreference (with interaction between spoIVA
and sigmaE). More precisely, spoIVA refers to the phrase one or more genes which are
known to interact with sigmaE. We can therefore infer that spoIVA interacts with sigmaE. Sentences without coreferences form a subset, which we refer to as LLL-nocoref, and
sentences with coreferences are part of the separate subset LLL-coref.
(22) ykuD was transcribed by SigK RNA polymerase from T4 of sporulation.
(23) Finally, we show that proper localization of SpoIVA required the expression of one
or more genes which, like spoIVA, are under the control of the mother cell
transcription factor sigmaE.
It is assumed here that relations in the sentences with coreferences are harder to recognize. To show how the LA kernel performs on both subsets, we report the experimental results on the full set of test data (LLL-all), and on its subsets (LLL-coref and LLL-nocoref).
Syntactic analysis We analyzed the BC-PPI corpus with the Stanford parser. The LLL
corpus has already been preprocessed by the LinkParser and its output was checked by
experts. To enable comparison with the previous work, we used the AImed corpus parsed

20

fiUsing Local Alignments for Relation Recognition

by the Stanford parser 3 and by the Enju parser 4 (which exactly correspond to the input in
the experiments by Erkan et al., 2007 and Stre et al., 2008). Unlike the Stanford parser,
Enju is based on a Head-driven Phrase Structure Grammar (HPSG). The output of the
Enju parser can be presented in two ways, either as predicate argument structure or as a
phrase structure tree. Predicate argument structures describe relations between words in
a sentence, while phrase structure presents a sentence structure in the form of clauses and
phrases. In addition, Enju was trained on the GENIA corpus and includes a model for
parsing biomedical texts.
(24) Cbf3 contains three proteins, Cbf3a, Cbf3b and Cbf3c.

contains

dobj

nsubj

proteins

Cbf3

num

conj and

conj and conj and

three

Cbf3a
nsubj

Cbf3b
dobj

Cbf3  contains  proteins
nsubj
dobj
Cbf3  contains  proteins
nsubj
dobj
Cbf3  contains  proteins

Cbf3b

conj and

 Cbf3a
 Cbf3b
conj and
 Cbf3c
conj and

Figure 1: Stanford parser output and representation for Example (24).
Figure 1 shows a dependency tree obtained by the Stanford parser for the sentence in
(24). This sentence mentions three interactions among proteins, more precisely, between
Cbf3 and Cbf3a, Cbf3 and Cbf3b, and Cbf3 and Cbf3c. All three dependency
paths contain words (lemmata) and syntactic functions (such as subj for a subject) plus the
direction of traversing the tree. Figure 2 presents the output for the same sentence provided
by the Enju parser. The upper part refers to the phrase structure tree and the lower part
shows the paths extracted from the predicate argument structure. The two parsers clearly
differ in their output. First, the Stanford parser conveniently generates the same paths
for all three interaction pairs while the Enju analyzer does not. Second, the output of the
Stanford parser excludes prepositions or conjunctions that are attached to the syntactic
functions whereas the Enju analyzer lists them in the parsing results. Such differences
3. Available from http://nlp.stanford.edu/software/lex-parser.shtml.
4. Available from http://www-tsujii.is.s.u-tokyo.ac.jp/enju/.

21

fiKatrenko, Adriaans, & van Someren

lead to different input sequences that are later fed into the LA kernel. Consequently, the
variations in input may translate into differences in the final performance.

Cbf3
Cbf3
Cbf3

ARG1/verb



ARG1/verb



ARG1/verb



contain
contain
contain

ARG2/verb



ARG2/verb



ARG2/verb



protein
protein
protein

ARG1/app



ARG1/app



ARG1/app



,
,
,

ARG2/app



ARG2/app



ARG2/app



Cbf3a
Cbf3a
Cbf3a

ARG1/coord



ARG1/coord



,

ARG2/coord

and



Cbf3b

ARG2/coord



Cbf3c

Figure 2: Enjus output and representation for Example (24).
In addition, in most work employing AImed, the dependency paths such as these in
Figure 1 and Figure 2 are preprocessed in the following way. The actual named entities that
are the arguments of the relation are replaced by a label, e.g. PROTEIN. Consequently, the
nsubj

dobj

conj and

first path in Figure 1 becomes PROTEIN  contains  proteins  PROTEIN.
To be able to compare our results on AImed with the performance reported in the work of
Erkan et al. (2007) and Stre et al. (2008), we use exactly the same dependency paths with
argument labels. However, to study whether using labels instead of actual named entities
has an impact on the final results for the LLL data set, we carry out two experiments. In the
first one, the dependency paths contain named entities, whereas in the second they contain
labels. The second experiment is referred to by adding a word LABEL to its name (as
LLL-all-LABEL in Table 7).
4.1.2 Generic Relations
The second type of relations that we consider are generic relations. Their arguments are
sometimes annotated using external resources such as WordNet, which makes it possible to
use semantic relatedness measures defined over them. An example of such an approach is

22

fiUsing Local Alignments for Relation Recognition

data used for the SemEval-2007 challenge, Task 04: Classification of Semantic Relations
between Nominals (Girju et al., 2009).
The goal of Task 4 was to classify seven semantic relations (Cause - Effect, Instrument - Agency, Product - Producer, Origin - Entity, Theme - Tool, Part Whole and Content - Container), whose examples were collected from the Web using
some predefined queries. In other words, given a set of examples and a relation, the expected output would be a binary classification of whether an example belongs to the given
relation or not. The arguments of the relation were annotated by synsets from the WordNet
hierarchy, as in Figure 3. Given this sentence and a pair (spiritual awareness, joy) with the
corresponding synsets joy%1:12:00 and awareness%1:09:00, this would mean that a classifier has to decide whether this pair is an example of the Cause-Effect relation. This
particular sentence was retrieved by quering the Web with the phrase joy comes from *.
The synsets were manually selected from the WordNet hierarchy. There are seven semantic
relations used in this challenge, which gives seven binary classification problems.

Genuine <e1>joy</e1> comes from <e2>spiritual awareness</e2> on life and an absolute clarity of direction, living for a purpose.
WordNet(e1) = joy%1:12:00, WordNet(e2) = awareness%1:09:00,
Query: joy comes from *, Cause-Effect(e2, e1) = true

Figure 3: An annotated example of Cause - Effect from the SemEval-2007, Task 4
training data set.

relation type
Origin - Entity
Product - Producer
Theme - Tool
Instrument - Agency
Part - Whole
Content - Container
Cause - Effect

#examples (train)
140
140
140
140
140
140
140

#pos (train)
54
85
58
71
65
65
73

#examples (test)
81
93
71
78
72
74
80

direction
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric

Table 4: Distribution of the SemEval-2007, Task 4 examples (training and test data), where
#pos stands for the number of positive examples per data set and #examples
indicates the number of examples in total.

Syntactic analysis To generate dependency paths, all seven data sets used in SemEval 2007, Task 4, were analyzed by the Stanford parser. The dependency path for the sentence
in Figure 3 is given in (25).
23

fiKatrenko, Adriaans, & van Someren

(25) awareness#n#1

prep f rom



nsubj

come  joy#n#1

Here, words annotated with WordNet have their PoS tag attached, followed by the sense.
For instance, awareness is a noun and in the current context its first sense is used, which
corresponds to awareness#n#1.
4.2 Substitution Matrix
To build a substitution matrix for the LA kernel, we use either distributional similarity
or WordNet semantic relatedness measures. For a data set of dependency paths, which
contains t unique elements (words and syntactic functions), the size of the matrix is t  t.
If k elements out of t are words, the number of substitution scores to be computed by
distributional similarity (or semantic relatedness) measures equals k(k + 1)/2. This is due
to the fact that the measures we use are symmetric. The substitution matrix is built for
each corpus we used in the experiments, which results in three substitution matrices for
the biomedical domain (for BC-PPI, LLL, and AImed) and seven substitution matrices for
generic relations. In what follows, we discuss the settings which were used for calculating
the substitution matrix in more detail.
Distributional similarity can be estimated either by using contextual information (O
Seaghdha & Copestake, 2008), or by exploring grammatical relations between words (Lee,
1999). In this work we opt for contextual information. This is motivated by the presence
of words belonging to different parts of speech in the dependency paths. For instance,
even though, according to dependency grammar theory (Melcuk, 1988), adjectives do not
govern other words, they may still occur in the dependency paths. In other words, even if
parsing does not fail, it may produce unreliable syntactic structures. To be able to compare
words of any part of speech, we have decided to estimate distributional similarity based on
contextual information, rather than on grammatical relations.
While computing distributional similarity, it may happen that a given word xi does not
occur in the corpus. To handle such cases, we always set d(xi , xi ) = 1 (the largest possible
similarity score), and d(xi , x0j ) = 0 when xi 6= x0j (the lowest possible similarity score).
4.2.1 Biomedical domain
To estimate distributional similarity for the biomedical domain, we use the TREC 2006
Genomics collection (Hersch, Cohen, Roberts, & Rakapalli, 2006) which contains 162,259
documents from 49 journals. All documents have been preprocessed by removing HTMLtags, citations in the text and reference sections and stemmed by the Porter stemmer (van
Rijsbergen, Robertson, & Porter, 1980). Furthermore, the query-likelihood approach with
Dirichlet smoothing (Chen & Goodman, 1996) is used to retrieve document passages given a
query. All passages are ranked according to their likelihood of generating the query. Dirichlet smoothing is used to avoid zero probabilities and poor probability estimates (which may
happen when words do not occur in the documents). All k unique words occurring in the set
of dependency paths sequences are fed as queries to collect a corpus for estimating similarity.
Immediate context surrounding each pair of words is used to calculate the distributional
similarity of these words. We set the context window to 2 (2 tokens to the right and 2

24

fiUsing Local Alignments for Relation Recognition

tokens to the left of a word in focus) and do not perform any kind of further preprocessing
such as PoS tagging.
4.2.2 Generic relations
For generic relations, we use all WordNet relatedness measures described in Section 3.2.2.
We have already shown that the WordNet relatedness measures work only on synsets, which
assumes that all words have to be manually annotated with information from WordNet.
Since this is done only for the relations arguments (see the example in Figure 3), and for
no other words in sentences (and, correspondingly, in the dependency paths), we build a
substitution matrix as follows. For any two words annotated with WordNet, their substitution score equals a value returned by a relatedness measure being used. For any other word
pair, it equals 1 whenever the words are identical, and 0 otherwise. 5 For example, if we
consider the words in the dependency path in (25) and the Wu-Palmer (wup) relatedness
measure, the substitution scores that we obtain are as follows:

d(awareness#n#1, awareness#n#1) = 1
d(awareness#n#1, come) = 0
d(awareness#n#1, joy#n#1) = 0.35
d(prep from, come) = 0
d(prep from, joy#n#1) = 0
d(come, nsubj) = 0
d(nsubj, nsubj) = 1
d(joy#n#1, joy#n#1) = 1

d(awareness#n#1, prep from) = 0
d(awareness#n#1, nsubj) = 0
d(prep from, prep from) = 1
d(prep from, nsubj) = 0
d(come, come) = 1
d(come, joy#n#1) = 0
d(nsubj, joy#n#1) = 0

Figure 4: The substitution scores for the dependency path in (25) using wup measure.
Syntactic relations (prep from, subj) are accompanied by the direction of the
dependency tree traversal (either  or ).

In the dependency path (25), there are 5 unique elements (t), 2 of which are annotated
with WordNet synsets (k). Consequently, there are 5*6/2 = 15 substitution scores in total,
3 of which are computed using WordNet relatedness.
To compute WordNet relatedness, we use the WordNet::Similarity package for WordNet 3.0 (Pedersen, Patwardhan, & Michelizzi, 2004).
4.3 Baselines and Kernel Settings
In this section, we discuss two baselines and kernel settings.
4.3.1 Baselines
To test how well local alignment kernels perform compared to kernels proposed in the past,
we implemented the shortest path kernel described in the work of Bunescu and Mooney
5. This also applies to the cases when the relation arguments could not have been annotated with WordNet
information.

25

fiKatrenko, Adriaans, & van Someren

(2005a) (Section 2.3.5) as one of the baselines (Baseline I). This method seems to be the
most natural choice because it operates on the same data structures (dependency paths).
Similarly to Bunescu and Mooneys (2005a) work, in our experiments we use lemma, part of
speech tag and direction, but we do not consider entity type or negative polarity of items.
The choice of the LA kernel in this paper was motivated not only by its ability to
compare sequences in a flexible way, but also because of the possibility to explore additional
information (not present in the training set) via a substitution matrix. The other baseline,
Baseline II, is used to test whether the choice of similarity measures affects the results. In
this case, the substitution scores d(, ) are not calculated using distributional similarity or
WordNet relatedness, but generated randomly within the interval [0, 1].
4.3.2 Kernel settings
The kernels we compute are used together with the support vector machine tool LibSVM
(Chang & Lin, 2001) to detect hyperplanes separating positive examples from negative
ones. Before plugging all kernel matrices for 10-fold cross-validation into LibSVM, they are
normalized as in Equation 26.

0

0

k(x, y)

k(x , y ) = p

k(x, x)k(y, y)

(26)

To handle imbalanced data sets (most notably AImed and BC-PPI), the examples are
weighted using inverse-class probability (i.e. all training examples of class A are weighted
1/prob(A) where prob(A) is the fraction of training examples with class A). All significance
tests were done using a two-tailed paired t-test with confidence level 95% ( = 0.05).
In addition, in all experiments we tuned the penalty parameter C (Equation 4) in the
range (26 , 24 , . . . , 212 ).
To use the LA kernel, one has to set the following parameters: the gap opening cost,
the gap extension cost, and the scaling parameter . In our cross-validation experiments,
the gap opening cost is set to 1.2, the extension cost to 0.2 and the scaling parameter  to
1. The choice of the scaling value was motivated by the experiments on amino acids in the
biological domain (Saigo et al., 2004). After initial experiments, we present here a further
study where the parameter values are varied.

5. Experiment I: Domain-Specific Relations
The goal of this evaluation is to study the behavior of the LA kernel on domain-specific
relations in the biomedical domain. In this section, we report on the experiments conducted
on three biomedical corpora using the LA kernel based on the distributional similarity measures, two baselines and results published previously (e.g., using the graph kernel by Airola
et al., 2008 or the tree kernel by Stre et al., 2008). To the best of our knowledge, string
kernels have not been applied to dependency paths yet. However, a gap-weighted string
kernel (described in Section 2) also allows gapping and can be thus compared to the LA
kernel. To test how Lodhi et al.s (2002) kernel performs on dependency paths, we use it

26

fiUsing Local Alignments for Relation Recognition

on all three corpora. We have not tuned parameters of this string kernel and set the length
of subsequences to 4 and the decay factor  to 0.5. 6
5.1 LLL and BC-PPI Data Sets
This subsection presents results on two biomedical data sets, BC-PPI and LLL. Whenever
possible, we also discuss the performance previously reported in the literature.
The 10-fold cross-validation results on the BC-PPI corpus are presented in Table 5 and
on the LLL training data set in Table 6. The LA kernel based on the distributional similarity
measures (LA-Dice, LA-Cosine and LA-L2) performs significantly better than the two baselines. Recall that Baseline I corresponds to the shortest path approach (Section 2.3.5) and
Baseline II is the LA kernel with the randomly generated substitution scores. In contrast
to Baseline I, it is able to handle sequences of different lengths including gaps. According
to Equation 5, a comparison of any two sequences of different lengths results in the 0-score.
Nevertheless, it still yields high recall, while precision is much lower. This can be explained
by the fact that the shortest path uses PoS tags. Even though two sequences of the same
length can be very different, their comparison may still result in a non-zero score, provided
that their part of speech tags match. Furthermore, Baseline II suggests that accurate estimation of substitution scores is important for achieving good performance. Baseline II may
yield better results than Baseline I, but randomly generated substitution scores degrade the
performance.
Method
LA-Dice
LA-Cosine
LA-L2
Baseline I
Baseline II
Gap-weighted string kernel (Lodhi et al., 2002)

Precision
75.56
76.40
77.56
32.04
66.36
72.00

Recall
79.72
80.66
79.31
75.63
54.48
75.31

F-score
77.56
78.13
78.42
45.00
59.80
73.62

Table 5: 10-fold cross-validation on the BC-PPI data set.
At first glance, the choice of the distributional similarity measures does not affect the
overall performance yielded by the LA kernel. On the BC-PPI data, the method based on
the L2 measure outperforms the methods based on Dice (p.07) and on Cosine, but the
differences in the latter case are not significant. No statistically significant differences were
observed between the method based on Dice and Cosine.
In contrast to the BC-PPI data set, the kernels which use Dice and Cosine measures
on the LLL data set significantly outperform the one based on L2 (at p1.22107 and
p1.33106 , respectively).
On both data sets, the LA method using distributional similarity measures significantly
outperforms the baselines. Interestingly, the gap-weighted string kernel by Lodhi et al.
(2002) yields good performance too and seems to be a better choice than the subsequence
6. Lodhi et al. (2002) have mentioned in their paper that the F1 numbers (with respect to SSK) seem to
peak at a subsequence length between 4 and 7.

27

fiKatrenko, Adriaans, & van Someren

kernel based on shallow linguistic information (Giuliano et al., 2006). Recent work on
LLL (Fundel, Kueffner, & Zimmer, 2007) employs dependency information but, in contrast
to our method, it serves as the representation on which extraction rules are defined. Airola
et al. (2008) apply a graph kernel-based approach to extract interactions and use, among
others, the LLL and AImed data sets. As can be seen in Table 6, their method yields results
which are comparable to the gap-weighted string kernel on the dependency paths. To the
best of our knowledge, the performance achieved by the LA kernel on the LLL training set
is the highest (in terms of the F-score) among the results which have been reported in the
literature.
Method
LA-Dice
LA-Cosine
LA-L2
Baseline I
Baseline II
Graph kernel (Airola et al., 2008)
Gap-weighted string kernel (Lodhi et al., 2002)
Shallow linguistic kernel (Giuliano et al., 2006)
Rule-based method (Fundel et al., 2007)

Precision
88.76
88.63
86.80
39.02
65.82
72.5
83.66
62.10
68

Recall
81.62
82.09
75.04
100.00
41.32
87.2
71.11
61.30
83

F-score
85.03
85.23
80.49
56.13
50.76
76.8
76.88
61.70
75

Table 6: 10-fold cross-validation on the LLL-all training data set.
We also apply our method to the LLL test data (Table 7). 7 Even though the performance on the test set is poorer, LA-Dice outperforms both baselines. In addition, the
gap-weighted string kernel (Lodhi et al., 2002) seems to perform much worse on the test
set. For the LA kernel, precision is high, while recall decreases (and most drastically for
the data subset which includes co-references). This might be due to the fact that for some
sentences only incomplete parses are generated and, consequently, no dependency paths
between the entities are found. For 91 out of 567 possible interaction pairs generated on
the test data, there is no dependency path extracted. In contrast, the approach reported by
Giuliano et al. (2006) does not make use of syntactic information, and on the data subset
without coreferences achieves higher recall.
On the other hand, lower recall can also be caused by using actual names of proteins
and genes as arguments. In the work reported before, the relation arguments and other
named entities are often replaced by their types (e.g., PROTEIN) and these are used as
input for the learning algorithm. We conducted additional experiments using named entity
types in the dependency paths, which led to a great improvement in terms of recall and
F-score (Table 7, LLL-coref-LABEL, LLL-nocoref-LABEL, LLL-coref-LABEL). Our method
clearly outperforms the shallow linguistic kernel and also achieves better results than the
best-performing system in the LLL competition (Sbest ), which, according to Nedellec (2005),
applied Markov logic to the syntactic paths.
7. Airola et al. (2008) do not report on the performance on the LLL data set and, for this reason, information
on the graph all-paths kernel is not included in Table 7.

28

fiUsing Local Alignments for Relation Recognition

Data set
LLL-coref
LLL-nocoref
LLL-all
LLL-all
LLL-all
LLL-coref-LABEL
LLL-nocoref-LABEL
LLL-all-LABEL
LLL-coref
LLL-nocoref
LLL-all
LLL-all
LLL-all

Method
LA-Dice
LA-Dice
LA-Dice
Baseline I
Baseline II
LA-Dice
LA-Dice
LA-Dice
Shallow linguistic kernel (Giuliano
Shallow linguistic kernel (Giuliano
Shallow linguistic kernel (Giuliano
Gap-weighted string kernel (Lodhi
Sbest (Nedellec, 2005)

et
et
et
et

al.,
al.,
al.,
al.,

2006)
2006)
2006)
2002)

Precision
52.3
70.7
72.7
48.6
12.9
60.0
69.0
74.5
29.0
54.8
56.0
56.0
60.9

Recall
37.9
53.7
48.1
43.3
45.7
51.7
53.7
53.0
31.0
62.9
61.4
16.8
46.2

F-score
44.0
61.0
57.9
45.8
20.1
55.5
60.4
61.9
30.0
58.6
58.6
25.9
52.6

Table 7: Results on the LLL test data set.

5.2 AImed Data Set
Yet another data set that we consider is AImed. This data set has often been used for
experiments on relation extraction in the biomedical domain, which enables comparison
with other methods. It should be noted, however, that in this particular case, a corpus
is a collection of documents (abstracts). This may lead to two ways of performing 10-fold
cross-validation. One possibility lies in randomly splitting data in 10 parts, while the other
is to do cross-validation on the level of documents. The experiments we report here are
done using the first setting and can be directly compared against the methods described in
the work of Stre et al. (2008), Erkan et al. (2007) and Giuliano et al. (2006). In addition,
we use the same dependency paths for the LA kernel as the ones employed by Stre et al.
and Erkan et al.. The results by Airola et al. (2008) and by Bunescu (2007) are obtained
by cross-validating on the level of documents.
We conducted experiments by setting the distributional measure to Dice, referred to as
LA-Dice in Table 8. In the upper part of the table we used dependency paths generated
by the Stanford parser and in the lower part those obtained by Enju. As we discussed in
Section 2, Erkan et al. (2007) use similarity measures to compare dependency paths, but
they do not consider any additional sources whose information can be incorporated into the
learning procedure. They, however, experiment with supervised (SVM) and semi-supervised
learning (TSVM), where the number of training instances is varied. Table 8 shows the best
performance that was achieved by Erkan et al.s (2007) method. Among models based
on SVM, the one with Cosine distance, SVM-Cos, yields the best results. In the TSVM
setting, the one with the Edit measure performs the best. We observe that LA-Dice slightly
outperforms both and has, in particular, high precision.
In their work, Stre et al. (2008) explore several parsers and combinations of features.
The features include not only paths from Enju, but also word dependencies generated by
data-driven KSDEP parser, and word features. KSDEP parser is based on a probabilistic

29

fiKatrenko, Adriaans, & van Someren

shift-reduce algorithm (Sagae & Tsujii, 2007). In general, the method by Stre et al. also
uses SVM, but in this case it focuses on tree kernels (discussed in Section 2.3.3). To make a
fair comparison, we conducted experiments on the paths obtained by deep syntactic analysis
(Enju parser) and compared our scores against Stre et al.s (2008) results. In contrast
to the previous experiments, we achieve higher recall but lower precision. Overall, the LA
kernel yields better performance than the one reported by Stre et al. However, when
different sets of features are combined (parses from Enju and KSDEP plus word features Enju+KSDEP+W in Table 8), the overall performance can be improved.
Method
LA-Dice
Baseline I (Bunescu, 2007)
Baseline II
SVM-Cos (Erkan et al., 2007)
TSVM-Edit (Erkan et al., 2007)
Gap-weighted string kernel (Lodhi et al., 2002)
LA-Dice
Tree kernel (Stre et al., 2008)
Tree kernel (Stre et al., 2008)
Graph kernel (Airola et al., 2008)
Shallow linguistic kernel (Giuliano et al., 2006)

Parser
Stanford
Collins
Stanford
Stanford
Stanford
Stanford
Enju
Enju
Enju+KSDEP+W
Charniak-Lease
none

Precision
69.09
69.08
48.89
61.99
59.59
67.25
71.16
76.0
78.1
52.9
60.9

Recall
54.63
35.00
25.06
54.99
60.68
54.67
46.71
39.7
62.7
61.8
57.2

F-score
61.02
46.46
33.07
58.09
59.96
60.31
56.40
52.0
69.5
56.4
59.0

Table 8: 10-fold cross-validation on the AImed data set.
Bunescu (2007) reports the evaluation results on the AImed corpus in the form of a
precision-recall curve. If we consider the highest precision that was obtained in our experiments (69.09 or 71.16, depending on the input), this roughly corresponds to a recall of
35% in his plot (referred to as Baseline I in Table 8). In sum, the shortest path approach
never approaches performance of the LA kernel on any of the biomedical data sets that
were studied here. The other baseline, Baseline II, achieves the lowest scores from all the
methods presented here.
Table 8 illustrates that not only various methods have been trained on the AImed corpus,
but also many different parsers have been used. It should be noted that the graph kernel has
been trained and tested on the syntactic representation generated by the Charniak-Lease
parser, and the shortest path kernel has explored dependency paths obtained from the
Collins parser. The Charniak-Lease parser is a statistical parser trained on the biomedical
data (Lease & Charniak, 2005), whose phrase structures can be transformed into dependencies. Likewise, the Collins parser is a statistical parser (Collins, 1999). This leads to the
question whether the choice of syntactic parser has a significant impact on the extraction
results. To compare the impact of the syntactic parsers on relation extraction for AImed,
Miyao et al. (2008) have conducted a complex study with eight parsers (including the Stanford analyzer) and five parse representations 8 . They consider two cases. In the first one,
parsers have not been trained on biomedical data. Regardless of the parser being used in
their experiments, accuracy for the extraction task is similar. In the second experiment,
8. These are either various dependency tree formats (e. g., in the Stanford dependency format), or phrase
structures, or predicate-arguments structures.

30

fiUsing Local Alignments for Relation Recognition

parsers have been re-trained on domain-specific data. In this case, it has been shown that
the relation extraction results can be improved. The actual gain, however, can vary from
one parser to another.
For the AImed data, the LA kernel with the Dice measure gives state-of-the-art results.
It is outperformed only by approaches that use more information than just dependency
paths.
5.3 LA Kernel Parameters
Saigo et al. (2004) have already shown that the scaling parameter  (Equation 11) has a
significant impact on accuracy. We have also carried out additional experiments by varying
gap values and the value of . Results are visualized in Figure 5. The opening and extension
gap values are separated by the slash symbol and the values on the X-axis in the form a/b
should be read as the opening gap is set to a and the extension gap is equal to b. The
kernel matrices were normalized and all examples were weighted. According to our previous
experiments, the results yielded by the Dice measure do not significantly differ from the
ones achieved by the Cosine measure and we selected the Dice measure to conduct all
experiments. The performance on the BC-PPI data set is shown in Figure 5.

F-score
76
74
72
70
68
66
64
62
60
58

76
74
72
70
68
66
64
62
60
58

12/2
12/12
gaps

24/2
24/12

0.1

0.3

0.5

0.8

1

5

scaling

Figure 5: Varying gaps and the scaling () parameter on the BC-PPI data set (10-fold
cross-validation): F-score.

31

fiKatrenko, Adriaans, & van Someren

80
79
78
77
76
75
74
73
72
71
70

Precision
90
85
80
75
70
65

5

60

1

12/2

0.8
12/12
gaps

scaling

0.5

24/2

0.3
24/12

0.1

Figure 6: Varying gaps and the scaling () parameter on the BC-PPI data set (10-fold
cross-validation): Precision.

75
70

Recall

65

90

60

80

55

70

50

60

45

50

5

40

1

12/2

0.8
12/12
gaps

0.5

24/2

scaling

0.3
24/12

0.1

Figure 7: Varying gaps and the scaling () parameter on the BC-PPI data set (10-fold
cross-validation): Recall.
32

fiUsing Local Alignments for Relation Recognition

The results in Figure 5 indicate that decreasing  leads to a decrease in overall performance. Moreover, varying gap values causes subtle changes in the F-score, but these
changes are not as drastic as changes due to the lower .
Changes in the F-score are more likely to be explained by variances in precision and
recall. To investigate this matter, we look at how both measures depend on parameter
changes. If  is set to a low value, one can expect that this will nearly diminish the impact
of the substitution matrix, i.e. similarity among elements. For this reason we hypothesize
that larger values of the scaling parameter  should result in higher recall. Indeed, Figure 7
supports this hypothesis and the recall plot resembles the one for the F-score. Varying
parameter values has a much lower impact on precision (Figure 6) but nonetheless precision
does decrease as the  parameter becomes larger.
Overall,  seems to influence the final results the most, although gap values make a
contribution as well. According to the results we obtained, setting an extension gap e to a
large value (or equal to the opening gap o) is undesirable. Since the scaling parameter  is
applied not only to the substitution matrix but to the gap values as well, setting  below
0.5 decreases the effects of gap penalization and similarity of elements. Consequently, the
best performance is achieved by setting  to 1. This suggests that the final performance of
the LA kernel is influenced by a combination of parameters and their choice is crucial for
obtaining good performance.

6. Experiment II: Generic Relations
Another series of experiments was carried out on seven generic relations from the SemEval
- 2007 challenge, Task 4. The choice of the data sets in this case was motivated by two
factors. First, semantic relations used here differ from the relations from the biomedical
domain. Second, since the arguments of relations are annotated with WordNet, it becomes
possible to explore information from WordNet and use it as prior knowledge for the LA
kernel.
Many participants of this challenge considered WordNet either explicitly (Tribble &
Fahlman, 2007; Kim & Baldwin, 2007), or as a part of a complex system (Giuliano et al.,
2007). Since it is not always obvious how to use WordNet so that it yields the best performance, many researchers have made additional decisions such as use of supersenses (Hendrickx et al., 2007), selection of a predefined number of high-level concepts (Nulty, 2007), or
cutting the WordNet hierarchy at a certain level (Bedmar et al., 2007). Some other systems
such as the one by Nakov (2007) were based solely on information collected from the Web.
Even though it became evident that the best performing systems used WordNet, the variance in the results is remarkable and it is not clear whether this difference in performance
can be explained by the machine learning methods being used, the combination of features,
or by some other factors.
The SemEval-2007 Task-4 data set includes some relation examples which are nominal
compounds (like coffee maker), and this greatly reduces availability of information between
two arguments in the dependency paths. The relation arguments in this case are linked by
one grammatical relation (e.g., coffee and maker are linked by the grammatical relation
nn, which corresponds to noun compound). We assume, therefore, information coming
from WordNet to be especially helpful when the dependency paths are that short. In all our

33

fiKatrenko, Adriaans, & van Someren

experiments we used 5 relatedness measures defined earlier in Section 3.2 plus one additional
measure which is called random. The random measure indicates that the relatedness values
between any two relation arguments were generated randomly (within [0, 1]) and is thus very
suitable as a baseline (Baseline II). Similarly to the experiments in the biomedical domain,
another baseline is the shortest path kernel (Baseline I). Note that in the Task 4 overview
paper, Girju et al. (2007) reported on three baselines, which, in their case, were (i) guessing
true or false for all examples, depending on which class is the majority class in the test
set (Baseline III), (ii) always guessing true (Baseline IV), and (iii) guessing true or false
with the probability that corresponds to the class distribution in the test set (Baseline V).
The first question of interest is what implications the choice of semantic relatedness
measure has for the performance of the LA kernel. To answer this question, we perform
10-fold cross-validation on the training set (Figure 9, Figure 10 and Figure 11). Among
all 5 measures only jcn and resnik fail to perform better than the random score. In most
cases, the Resnik score is outperformed by other measures. The behaviour of the LeacockChodorow score (lch) and jcn varies from one semantic relation to another. For instance, use
of jcn seems to boost precision for Cause-Effect, Part-Whole, Product - Producer,
and Theme - Tool. For the remaining three relations it is clearly not the best-performing
measure.
To check whether there are differences between relatedness measures, we have carried
out significance tests comparing all measures for all relations. Our findings are summarized
in Table 9. Here, the symbol  between two relatedness measures stands for the measure
equivalence, or, in other words, indicates that there is no significant difference. Similarly
to the experiments in the biomedical field, all significance tests were conducted using a
two-tailed paired t-test with confidence level 95%. In addition, for any two measures a and
b, a > b means that a performs significantly better than b. For instance, the ranking for
Cause - Effect in Table 9 should be read as follows. The two best performing measures
are wup and lch, which significantly outperform lin, followed by random and res, which, in
turn, yield significantly better results than jcn. It can be seen from this table that wup and
lch are clearly the best performing measures for all seven relations (each of them is the best
measure for six out of seven relations).
Relation type
Cause - Effect
Instrument - Agency
Product - Producer
Origin - Entity
Theme - Tool
Part - Whole
Content - Container

Ranking
wup  lch > lin
wup  lch > lin
wup  lch > lin
wup  lch > lin
lch > lin  wup
wup  lin  lch
wup > lch > lin

>
>

>
>
>


res  random > jcn
res > jcn  random
jcn  res > random
res  jcn > random
res > jcn > random
res > jcn  random
res > jcn  random

Table 9: Ranking of the relatedness measures with respect to their accuracy on the training sets ( stands for measure equivalence, a > b indicates that the measure a
significantly outperforms b).

34

fiUsing Local Alignments for Relation Recognition

For each relation, we applied the best performing measure on the training set for this
particular relation to the test data. The results are reported in Table 10. On average, the LA
kernel employing the WordNet relatedness measures significantly outperforms two baselines.
Moreover, when compared to the best results of the SemEval-2007 competition (Beamer
et al., 2007), our method approaches performance yielded by the best system (bestSV ).
This system used not only various lexical, syntactic, and semantic feature sets, but also
expanded the training set by adding examples from many different sources. We have already
mentioned in Section 2 that the recent work by O Seaghdha (2009) explores WordNet
structure and graph kernels to classify semantic relations. The overall performance which
is achieved by this method (Table 10) is comparable to the one by the LA kernel, but it is
unclear whether there are any semantic relations for which one of the approaches performs
better.
Relation type
Cause - Effect
Instrument - Agency
Product - Producer
Origin - Entity
Theme - Tool
Part - Whole
Content - Container
Average
Baseline I
Baseline II
Baseline III
Baseline IV
Baseline V
bestSV
Gap-weighted string kernel (Lodhi et al., 2002)
WordNet kernels (O Seaghdha, 2009)

Accuracy
61.25
75.64
75.27
74.07
73.24
80.56
71.62
73.09
58.23
55.83
57.0
48.5
48.5
76.3
61.19
74.1

Precision
62.50
73.17
76.71
75.86
67.86
70.00
74.29
71.48
52.50
61.61
81.3
48.5
48.5
79.7
66.2
-

Recall
60.98
78.95
90.32
61.11
65.52
80.77
68.42
72.30
54.30
55.50
42.9
100.0
57.1
69.8
47.52
-

F-score
61.73
75.95
82.96
67.69
66.67
75.00
71.23
71.60
49.19
53.93
30.8
64.8
48.5
72.4
43.02
71.0

measure
lch
wup
lch
wup
lch
wup
wup

Table 10: Results on the SemEval-2007, Task 4 test data set (selecting the best performing
measure on the training set for each relation).

In addition, we report results on the SemEval Task 4 test set per relatedness measure
(Table 11), which are averages over all seven relations. Similarly to our findings on the
training set, wup and lch are the best performing measures on test data as well.
One would expect that the optimal use of prior knowledge should allow us to reduce
the number of training instances without significant changes in performance. To study how
(and whether) the amount of training data influences the results on the test set, we split
the training set in several subsets, creating a model for each subset and applying it to the
SemEval-2007, Task 4 test data. The split corresponds to the split used by the challenge
organizers. As Figure 8 9 suggests, most relations are recognized well even when a relatively
small data sample is used. The exception is the Theme-Tool relation where increasing the
9. The model trained on only 35 Origin-Entity examples classifies none of the test examples as positive,
for this reason there is no point in Figure 8 for this relation given 35 training examples.

35

fiKatrenko, Adriaans, & van Someren

training data clearly helps. This finding is in line with the results of Giuliano et al. (2007)
whose system was a combination of kernels on the same data. Their results also indicate
that all relations but one (Theme-Tool) are extracted well, even if only a quarter of the
training set is used.
Relatedness measure
wup
lch
lin
res
jcn
random

Accuracy
72.91
72.96
65.27
62.94
55.55
56.57

Precision
71.20
72.31
62.01
62.51
52.25
53.10

Recall
72.56
70.93
67.07
59.66
69.28
52.94

F-score
71.62
71.02
63.65
60.46
57.07
52.83

Table 11: Results on the SemEval-2007, Task 4 test data set, averages for all 7 relations
per WordNet relatedness measure.

Learning curve
100
90
80
70

F-score

60
50
Cause-Effect
Instrument-Agency
Product-Producer
Origin-Entity
Theme-Tool
Part-Whole
Content-Container

40
30
20
10
0

35

70
105
training examples

140

Figure 8: Learning curve on the SemEval-2007, Task 4 test data set.
Some other recent work on the SemEval Task 4 data set includes investigation of distributional kernels (O Seaghdha & Copestake, 2008), pattern clusters (Davidov & Rappoport,
2008), relational similarity (Nakov & Hearst, 2008), and WordNet kernels. Unlike WordNet
kernels, the first three approaches do not use WordNet. O Seaghdha and Copestake (2008)
report an accuracy of 70.7 and the F-score of 67.5 as the best results yielded by distributional kernels and the best performance of Davidov and Rappoports (2008) method is
an accuracy of 70.1, and the F-score of 70.6. WordNet kernels, similarly to our findings
with the LA kernel, yield better accuracy than methods not using WordNet (74.1), but the

36

fiUsing Local Alignments for Relation Recognition

Cause-Effect
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Instrument-Agency
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Product-Producer
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Figure 9: 10-fold cross-validation on the training set (Cause - Effect, Instrument Agency and Product - Producer relations).

37

fiKatrenko, Adriaans, & van Someren

Origin-Entity
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Theme-Tool
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Part-Whole
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Figure 10: 10-fold cross-validation on the training set (Origin - Entity, Theme - Tool
and Part - Whole relations).

38

fiUsing Local Alignments for Relation Recognition

Content-Container
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Figure 11: 10-fold cross-validation on the training set (Content - Container relation).

F-score is comparable to the performance reported by O Seaghdha and Copestake (2008)
and by Davidov and Rappoport (2008).

7. Discussion
In this section we revisit the goals that were stated at the end of Section 2 and discuss our
findings in more detail.
7.1 The LA Kernel for Relation Extraction
We have introduced the LA kernel, which has proven to be effective for biomedical problems,
in the NLP domain and showed that it is well suited for relation extraction. In particular, the experiments in two different domains either outperform existing methods or yield
performance on par with existing state-of-the-art kernels.
One of the motivations for using the LA kernel in the relation extraction task is to
exploit prior knowledge. Here, we explore two possibilities, distributional similarity and
information provided by WordNet.
7.1.1 Distributional Similarity Measures
In our setting, we consider three distributional measures that have already been studied
before. For instance, Lee (1999) uses them to detect similar nouns based on verb-object
co-occurrence pairs. The results suggest the Jaccard coefficient (which is related to the
Dice measure) to be one of the best performing measures followed by some others including
Cosine. Euclidean distance fell into the group with the largest error rates. Given previous
work by Lee (1999), one would expect Euclidean distance to achieve worse results than
39

fiKatrenko, Adriaans, & van Someren

the other two measures. Indeed, on the LLL corpus, the LA kernel employing L2 shows
a significant decrease in performance. As to the other measures, the method using Dice
significantly outperforms the one based on the L2 measure only on the LLL corpus while
there is no significant improvement on the BC-PPI data set. Based on the experiments we
have conducted, we conclude that the LA kernel using Dice and Cosine measures performs
similarly on the LLL data set and the BC-PPI corpus. Given the results on various biomedical corpora (and different settings we have experimented with), we obtained experimental
support for choosing the Dice or Cosine measure over the Euclidean distance.
7.1.2 WordNet Similarity Measures
For generic relations, semantic relatedness plays a significant role. The difference in the
F-score between models that use semantic relatedness and the kernel where the relatedness
values are generated randomly (Baseline II) amounts to nearly 20%. All measures exhibit
different performance on the seven generic relations that we have considered. We can
observe, for instance, that wup, lch, and lin almost always yield the best results, no matter
what relation is considered. We found the Resnik score and Jiang and Conraths measure
to yield lower results than other measures. Even though the F-scores per relation vary
quite substantially (by placing Cause-Effect, Theme-Tool, Origin-Entity among
the most difficult relations to extract), two measures, wup and lch, are the top-performing
measures for all seven relations. These two measures explore the WordNet taxonomy using
a length of the paths between two concepts, or their depth in the WordNet hierarchy and,
consequently, belong to the path-based measures. The other three measures, res, lin and
jcn are information content based measures, and here relatedness between two concepts
is defined through the amount of information they share. Our experiments with the LA
kernel on generic relation recognition suggest that, in this particular case, the path-based
measures should be preferred over the information content based measures.
We should stress, however, that this is the evaluation of the semantic relatedness measures in the context of relation recognition, and one can by no means draw a conclusion
that the top measures for other NLP tasks will stay the same. For example, Budanitsky
and Hirst (2006) use semantic relatedness measures to detect malapropism and show that
Jiang and Conraths measure (jcn) yields the best results, followed by Lins measure (lin),
and the one by Leacock and Chodorow (lch), and then by Resniks measure (res). Our
results are quite similar to their findings if we consider the res measure, but jcn is not on
the top of the accuracy ranking list for any of the seven semantic relations that we have
studied.
7.2 Factors and Parameters that Influence the LA Kernel Performance
Our experiments in two domains have shown that the LA kernel either outperforms existing
methods on the same corpora, or yields performance on par with existing state-of-the-art
kernels.
7.2.1 Baselines
An advantage of the LA kernel over the Bunescu shortest path method (Baseline I) is that
it is capable of handling paths of different lengths. By allowing gaps and penalizing them,
40

fiUsing Local Alignments for Relation Recognition

the final kernel matrix becomes less sparse. The shortest path approach also attempts to
generalize over the dependency paths, but it usually overgeneralizes which leads to high
recall scores (Table 5 and Table 6) but to poor overall performance. One explanation for
overgeneralization may be that this method accounts well for structural similarity (provided
sequences are of the same length) but fails to provide finer distinctions among dependency
paths. Consider, for example, two sequences trip  makes  tram and coffee  makes
 guy, whereby the first path represents a negative instance of the Product-Producer
relation and the second path corresponds to a positive one. Even though they do not match
exactly, the elements that do not match are all nouns in singular. Consequently, comparison
according to the shortest path method will result in a relatively high similarity score. In
contrast, the LA kernel will consider similarity of the elements and the pairs trip-coffee
and tram-guy will obtain low scores.
In addition, Baseline II, which is based on randomly generated substitution scores,
performs poor for all data sets (or comparable to Baseline I). This leads us to the conclusion
that accurate estimation of similarities is another reason why the LA kernel performs well
on relation extraction.
7.2.2 Comparison with Other Methods
As we have already pointed out, the obvious shortcoming of Baseline I is its inability to
handle dependency paths of different length. For this reason, we have also applied the
gap-weighted string kernel (Lodhi et al., 2002) to all data sets. In this case, dependency
paths can be compared in a flexible way because gapping is allowed, but no other additional
information is used. This kernel outperforms Baseline I by increasing precision of relation
extraction while preserving a relatively high recall. The only data set where it fails to yield
good results is the LLL test data, and we believe this is due to the differences in the LLL
training and test data. For all data sets, the LA kernel achieves better performance than
the gap-weighted string kernel. The margin, however, is different for different data sets. In
the biomedical domain, the differences between the two methods can more clearly be seen
on the BC-PPI and LLL data sets, while the results on the AImed corpus are comparable.
However, other methods tested on AImed do not get higher scores unless they use more
features than just dependency paths. This holds for both types of cross-validation used
on this corpus. For generic relations, the difference between the LA kernel and the gapweighted string kernel is much larger. In particular, in the case of the gap-weighted kernel,
precision is high, but recall is much lower. This can be explained by the fact that generic
relations benefit from the knowledge found in WordNet and recall achieved by the LA kernel
is, therefore, high. The gap-weighted kernel has access only to information found in the
dependency paths and, for this reason, fails to find more relations.
The LA kernel also achieves the best performance on the LLL training set, outperforming
the graph kernel (Airola et al., 2008), the shallow linguistic kernel (Giuliano et al., 2006)
and the rule-based system by Fundel et al. (2007). All three have used different input for
their methods, varying from plain text to dependency structures. For this reason, a direct
comparison is unfortunately not possible, but we can conclude that the methods employing
dependency information always are among the best performing approaches.

41

fiKatrenko, Adriaans, & van Someren

Two other approaches whose performance has been reported on the AImed data set include the tree kernel (Stre et al., 2008) and TSVM (Erkan et al., 2007). Both of them
explore syntactic information in different ways. While Stre et al. consider subtrees, the
method of Erkan et al. has more similarities with our approach because it relies on the
dependency path comparison. To do this comparison, they only use information already
available in the dependency paths (SVM setting), or more dependency paths (TSVM setting). According to Lauer and Bloch (2008), TSVMs fall into the category using prior
knowledge by sampling methods, because it explores prior knowledge by generating new
examples. In contrast, we employ information from large unlabeled text sources in order to
enable finer comparison of the dependency paths and always work in the supervised learning
setting. Using the same evaluation procedure as in the work of Stre et al. and Erkan et al.
we show that the LA kernel outperforms both methods, but the differences on this data set
are much smaller than on the other data sets we have used.
7.2.3 The LA Parameters
We have demonstrated that the choice of LA parameters is crucial for achieving good performance. In our experiments, the scaling parameter  contributes to the overall performance
at most, but the other parameters such as gap values have to be taken into account as well.
When  approaches infinity, the LA kernel approximates the Smith-Waterman distance,
but increasing  does not necessarily have a positive impact on the final performance. This
finding is in line with the results reported by Saigo et al. (2004) on the homology detection
task. The best performance is yielded by setting the scaling parameter to 1 or a bit higher,
and by penalizing the gap extension less than the gap opening.

8. Conclusions and Future Work
We have presented a novel approach to relation extraction that is based on the local alignments of sequences. Using an LA kernel provides us an opportunity to explore various
sources of information and to study their role in relation recognition. Possible future directions include, therefore, an examination of other distributional similarity measures, studying
their impact on the extraction of generic relations, and looking for other sources of information which could be helpful for relation recognition. It may be interesting to consider
relational similarity (Turney, 2006), which looks for the correspondence between relation
instances. In this case, one should be able to infer that doctor corresponds to scalpel in
a similar way as fisherman to net (where both (scalpel, doctor) and (net, fisherman) are
examples of Instrument - Agency).
Despite the sparseness problem that might occur when WordNet-based measures are
used, these measures have an advantage over the distributional measures by treating elements to be compared as concepts rather than words. In the NLP community, a few steps
have been already taken to solve this problem by clustering words in large corpora aiming
at word sense discovery (Pennacchiotti & Pantel, 2006). Recently, Mohammad (2008) in
his thesis investigated the compatibility of distributional measures with ontological ones.
By using corpus statistics and a thesaurus, the author introduced distributional profiles of
senses and defined distance measures on them. Even though this new approach to calculat-

42

fiUsing Local Alignments for Relation Recognition

ing similarity was tested on generic corpora, it would be of a certain interest to apply it to
domain-specific data.
Overall, local alignment kernels provide a flexible means to work with data sequences.
First, they allow a partial match between sequences which is particularly important when
dealing with text. Second, it is possible to incorporate prior knowledge in the learning
process while preserving kernel validity. In general, LA kernels can be applied to other
NLP problems as long as the input data is in the form of sequences.

Acknowledgments
The authors wish to thank Simon Carter and Gerben de Vries for their comments and
proofreading, and three anonymous reviewers for their highly valuable feedback. They
also acknowledge the input from the Adaptive Information Management (AIM) group at
the University of Amsterdam. The preliminary version of this work has been dicussed
at the 22nd International Conference on Computational Linguistics (CoLing 2008) and at
the Seventh International Tbilisi Symposium on Language, Logic and Computation (2007).
This work was carried out in the context of the Virtual Laboratory for e-Science project
(www.vl-e.nl). This project is supported by a BSIK grant from the Dutch Ministry of
Education, Culture and Science (OC&W) and is part of the ICT innovation program of the
Ministry of Economic Affairs (EZ).

References
Airola, A., Pyysalo, S., Bjorne, J., Pahikkala, T., Ginter, F., & Salakoski, T. (2008). Allpaths graph kernel for protein-protein interaction extraction with evaluation of crosscorpus learning. BMC Bioinformatics, 9 (Suppl II).
Beamer, B., Bhat, S., Chee, B., Fister, A., Rozovskaya, A., & Girju, R. (2007). UIUC:
A Knowledge-rich Approach to Identifying Semantic Relations between Nominals.
In Proceedings of the Workshop on Semantic Evaluations (SemEval), Prague, Czech
Republic.
Bedmar, I. S., Samy, D., & Martinez, J. L. (2007). UC3M: Classification of semantic relations
between nominals using sequential minimal optimization. In SemEval-2007.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32 (1), 1347.
Bunescu, R. C. (2007). Learning for Information Extraction. Ph.D. thesis, Department of
Computer Sciences, University of Texas at Austin.
Bunescu, R. C., Ge, R., Kate, R. J., Marcotte, E. M., Mooney, R. J., Ramani, A. K., &
Wong, Y. W. (2005). Comparative experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence in Medicine, 33, 139155.
Bunescu, R. C., & Mooney, R. J. (2005a). A shortest path dependency kernel for relation
extraction. In Joint Conference on Human Language Technology / Empirical Methods
in Natural Language Processing (HLT/EMNLP), Vancouver, BC.

43

fiKatrenko, Adriaans, & van Someren

Bunescu, R. C., & Mooney, R. J. (2005b). Subsequence kernels for relation extraction.
In Proceedings of the 19th Conference on Neural Information Processing Systems,
Vancouver, BC.
Bunescu, R. C., & Mooney, R. J. (2006). Text Mining and Natural Language Processing,
chap. Extracting Relations from Text. From Word Sequences to Dependency Paths.
Springer.
Burges, C. J. C. (1998). A tutorial on support vector machines for pattern recognition.
Data Mining and Knowledge Discovery, 2 (2), 121167.
Camacho, R. (1994). The use of background knowledge in inductive logic programming.
Report.
Cancedda, N., Gaussier, E., Goutte, C., & Renders, J.-M. (2003). Word-sequence kernels.
Journal of Machine Learning Research, 3, 10591082.
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: a library for support vector machines. Software
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
Chen, S. F., & Goodman, J. (1996). An empirical study of smoothing techniques for language
modeling. In ACL96.
Clegg, A. B. (2008). Computational-Linguistic Approaches to Biological Text Mining. Ph.D.
thesis, University of London.
Cohen, W. W., Ravikumar, P., & Fienberg, S. (2003). A comparison of string distance
metrics for name-matching tasks. In IIWeb 2003, pp. 7378.
Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Collins, M., & Duffy, N. (2001). Convolution kernels for natural language. In Advances in
Neural Information Processing Systems 14, pp. 625632. MIT Press.
Cortes, C., & Vapnik, V. (1995). Support vector networks. Machine Learning, 20 (3),
273297.
Davidov, D., & Rappoport, A. (2008). Classification of semantic relationships between
nominals using pattern clusters. In Proceedings of ACL-08:HLT, pp. 227235.
Dolan, W. B., Quirk, C., & Brockett, C. (2004). Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In COLING 2004, Geneva,
Switzerland.
Erkan, G., Ozgur, A., & Radev, D. R. (2007). Semi-supervised classification for extracting
protein interaction sentences using dependency parsing. In 2007 Joint Conference
on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pp. 228237.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press.
Firth, J. R. (1957). A synopsis of linguistic theory 19301955. Studies in Linguistic Analysis.
Philological Society, Oxford. Reprinted in Palmer, F. (ed.), 1968.
Fundel, K., Kueffner, R., & Zimmer, R. (2007). RelEx - relation extraction using dependency
parse trees. Bioinformatics, 23 (3).
44

fiUsing Local Alignments for Relation Recognition

Girju, R., Badulescu, A., & Moldovan, D. (2006). Automatic discovery of part-whole relations. Computational Linguistics, 32 (1), 83135.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2007). SemEval2007 Task 04: Classification of semantic relations between nominals. In ACL 2007.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2009). Classification of semantic relations between nominals. Language Resources and Evaluation,
43 (2), 105121.
Giuliano, C., Lavelli, A., Pighin, D., & Romano, L. (2007). FBK-IRST: Kernel methods for
semantic relation extraction. In SemEval-2007.
Giuliano, C., Lavelli, A., & Romano, L. (2006). Exploiting shallow linguistic information
for relation extraction from biomedical literature. In EACL 2006.
Grishman, R., & Sundheim, B. (1996). Message Understanding Conference - 6: A brief
history. In Proceedings of the 16th International Conference on Computational Linguistics.
Haussler, D. (1999). Convolution kernels on discrete structures. Tech. rep. UCS-CRL-99-10,
UC Santa Cruz.
Hearst, M. (1992). Automatic acquisition of hyponyms from large text data. In Proceedings
of COLING-92, pp. 539545.
Hendrickx, I., Morante, R., Sporleder, C., & van den Bosch, A. (2007). ILK: Machine
learning of semantic relations with shallow features and almost no data. In SemEval2007.
Hersch, W., Cohen, A. M., Roberts, P., & Rakapalli, H. K. (2006). TREC 2006 genomics
track overview. In Proceedings of the 15th Text Retrieval Conference.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based on corpus statistics
and lexical taxonomy. In Proceedings of International Conference on Research in
Computational Linguistics (ROCLING X), pp. 1933.
Joachims, T. (1999). Transductive inference for text classification using Support Vector
Machines. In Proceedings of ICML.
Katrenko, S., & Adriaans, P. (2008). Semantic types of some generic relation arguments:
Detection and evaluation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT),
Columbus, USA.
Khoo, C. S. G., Chan, S., & Niu, Y. (2000). Extracting causal knowledge from a medical database using graphical patterns. In Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pp. 336343, Morristown, NJ, USA.
Association for Computational Linguistics.
Kim, S. N., & Baldwin, T. (2007). MELB-KB: Nominal classifications as noun compound
interpretation. In SemEval-2007.
Lauer, F., & Bloch, G. (2008). Incorporating Prior Knowledge in Support Vector Machines
for Classification: a Review. Neurocomputing, 71, 15781594.
45

fiKatrenko, Adriaans, & van Someren

Leacock, C., & Chodorow, M. (1998). Combining local context and WordNet similarity for
word sense identification. MIT Press, Cambridge, MA.
Lease, M., & Charniak, E. (2005). Parsing biomedical literature. In Proceedings of IJCNLP.
Lee, L. (1999). Measures of distributional similarity. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,
pp. 2532.
Leslie, C., Eskin, E., Cohen, A., Weston, J., & Noble, W. S. (2004). Mismatch string kernels
for discriminative protein classification. Bioinformatics, 20 (4), 467476.
Leslie, C., Eskin, E., & Noble, W. S. (2002). The spectrum kernel: A string kernel for SVM
protein classification. In Pacific Symposium on Biocomputing 7, pp. 566575.
Leusch, G., Ueffing, N., & Ney, H. (2003). A novel string-to-string distance measure with
applications to machine translation evaluation. In Machine Translation Summit IX,
pp. 240247, New Orleans, LO.
Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings of the 15th
International Conference on Machine Learning, pp. 296304.
Lodhi, H., Saunders, C., Shawe-Taylor, J., Christianini, N., & Watkins, C. (2002). Text
classification using string kernels. Journal of Machine Learning Research, 2, 419444.
McDonald, R. (2005). Extracting relations from unstructured text. Tech. rep. MS-CIS-0506, UPenn.
Melcuk, I. (1988). Dpendency syntax: theory and practice. SUNY Press.
Mitchell, T. (1997). Machine Learning. McGraw Hill.
Miyao, Y., Stre, R., Sagae, K., Matsuzaki, T., & Tsuji, J. (2008). Task-oriented evaluation
of syntactic parsers and their representations. In Proceedings of ACL-08:HLT, pp. 46
54.
Mohammad, S. (2008). Measuring Semantic Distance using distributional profiles of concepts. Ph.D. thesis, Graduate Department of Computer Science of University of
Toronto.
Monge, A. E., & Elkan, C. (1996). The field matching problem: Algorithms and applications.
In KDD 1996, pp. 267270.
Moschitti, A. (2006). Efficient convolution kernels for dependency and constituent syntactic
trees. In ECML 2006, pp. 318329.
Nakov, P. (2007). UCB: System description for SemEval task #4. In SemEval-2007.
Nakov, P. (2008). Paraphrasing verbs for noun compound interpretation. In Proceedings of
the Workshop on Multiword Expressions (MWE08), in conjunction with the Language
Resources and Evaluation conference, Marrakech, Morocco, 2008.
Nakov, P., & Hearst, M. A. (2008). Solving relational similarity problems using the web as
a corpus. In Proceedings of ACL-08:HLT.
Nedellec, C. (2005). Learning Language in Logic - Genic Interaction Extraction Challenge.
In Proceedings of the Learning Language in Logic workshop.
46

fiUsing Local Alignments for Relation Recognition

Needleman, S. B., & Wunsch, C. D. (1970). A general method applicable to the search for
similarities in the amino acid sequence of two proteins. Journal of Molecular Biology,
48 (3), 443453.
Nulty, P. (2007). UCD-PN: Classification of semantic relations between nominals using
WordNet and web counts. In SemEval-2007.
O Seaghdha, D. (2009). Semantic classification with WordNet kernels. In Proceedings of the
North American Chapter of the Association for Computational Linguistics - Human
Language Technologies Conference (NAACL-HLT), Boulder, CO.
O Seaghdha, D., & Copestake, A. (2008). Semantic classification with distributional kernels.
In Proceedings of CoLing 2008, Manchester, UK.
Palmer, M., & Wu, Z. (1995). Verb semantics for English-Chinese translation. Tech. rep.,
Technical Report No. MS-CIS-95-39, Department of Computer & Information Science,
University of Pennsylvania.
Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet::Similarity - Measuring the
Relatedness of Concepts. In Proceedings of the Nineteenth National Conference on
Artificial Intelligence (AAAI-04), pp. 10241025, San Jose, CA.
Pennacchiotti, M., & Pantel, P. (2006). Ontologizing semantic relations. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the
44th annual meeting of the Association for Computational Linguistics, pp. 793800,
Morristown, NJ, USA. Association for Computational Linguistics.
Ponzetto, S. P., & Strube, M. (2007). Knowledge Derived from Wikipedia for Computing
Semantic Relatedness. Journal of Artificial Intelligence Research, 30, 181212.
Resnik, P. (1995). Using information content to evaluate semantic similarity. In Proceedings
of the 14th International Joint Conference on Artificial Intelligence, pp. 448453.
Stre, R., Sagae, K., & Tsuji, J. (2008). Syntactic features for protein-protein interaction
extraction. In 2nd International Symposium on Languages in Biology and Medicine,
pp. 6.16.14.
Sagae, K., & Tsujii, J. (2007). Dependency parsing and domain adaptation with LR models
and parser ensembles. In Proceedings of EMNLP-CoNLL.
Saigo, H., Vert, J.-P., & Akutsu, T. (2006). Optimizing amino acid substitution matrices
with a local alignment kernel. BMC Bioinformatics, 7:246.
Saigo, H., Vert, J.-P., Ueda, N., & Akutsu, T. (2004). Protein homology detection using
string alignment kernels. Bioinformatics, 20 (11), 16821689.
Sang, E. F. T. K., Canisius, S., van den Bosch, A., & Bogers, T. (2005). Applying spelling
error correction techniques for improving semantic role labeling. In Proceedings of the
Ninth Conference on Natural Language Learning, CoNLL-2005, Ann Arbor, MI.
Saunders, C., Tschach, H., & Shawe-Taylor, J. (2002). Syllables and other string kernel
extensions. In ICML 2002.
Scholkopf, B. (1997). Support vector learning. Ph.D. thesis, Berlin Technical University.

47

fiKatrenko, Adriaans, & van Someren

Sekimizu, T., Park, H. S., & Tsujii, J. (1998). Identifying the interaction between genes
and gene products based on frequently seen verbs in Medline abstracts. Genome
Informatics, 9, 6271.
Shawe-Taylor, J., & Christianini, N. (2000). Support Vector Machines and Other KernelBased Learning Methods. Cambridge University Press.
Smith, L. H., Yeganova, L., & Wilbur, W. J. (2003). Hidden Markov models and optimized
sequence alignment. Computational Biology and Chemistry, 27 (1), 77  84.
Smith, T. F., & Waterman, M. S. (1981). Identification of common molecular subsequences.
Journal of Molecular Biology, 147, 195197.
Snow, R., Jurafsky, D., & Ng, A. Y. (2006). Learning named entity hyponyms for question
answering. In Proceedings of COLING/ACL.
Swanson, D. R., & Smalheiser, N. R. (1999). Implicit text linkages between Medline records:
Using Arrowsmith as an aid to scientific discovery. Library Trends, 48 (1).
Thomas, J., Milward, D., Ouzounis, C., & Pulman, S. (2000). Automatic extraction of
protein interactions from scientific abstracts. In Proceedings of Pacific Symposium on
Biocomputing.
Tribble, A., & Fahlman, S. E. (2007). CMU-AT: Semantic distance and background knowledge for identifying semantic relations. In SemEval-2007.
Turney, P. D. (2006). Similarity of semantic relations. Computational Linguistics, 32 (3),
379416.
van der Plas, L. (2008). Automatic Lexico-Semantic Acquisition for Question Answering.
Ph.D. thesis, University of Groningen.
van Rijsbergen, C. J., Robertson, S. E., & Porter, M. F. (1980). New models in probabilistic
information retrieval. Tech. rep. 5587, British Library Research and Development
Report.
Vapnik, V. (1982). Estimation of Dependences Based on Empirical Data. New York:
SPringer Verlag.
Weeds, J., Weir, D., & McCarthy, D. (2004). Characterising measures of lexical distributional similarity. In Proceedings of CoLing 2004.
Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods for relation extraction.
Journal of Machine Learning Research, 3, 10831106.
Zhang, Y., Schneider, J., & Dubrawski, A. (2008). Learning the semantic correlation: An
alternative way to gain from unlabeled text. In Proceedings of the 22nd Conference
on Neural Information Processing Systems, Vancouver, BC.

48

fiJournal of Artificial Intelligence Research 38 (2010) 633-685

Submitted 03/10; published 08/10

Non-Transferable Utility Coalitional Games
via Mixed-Integer Linear Constraints
Gianluigi Greco

ggreco@mat.unical.it

Dipartimento di Matematica
Universita della Calabria
I-87036 Rende, Italy

Enrico Malizia
Luigi Palopoli
Francesco Scarcello

emalizia@deis.unical.it
palopoli@deis.unical.it
scarcello@deis.unical.it

DEIS
Universita della Calabria
I-87036 Rende, Italy

Abstract
Coalitional games serve the purpose of modeling payoff distribution problems in scenarios where agents can collaborate by forming coalitions in order to obtain higher worths than
by acting in isolation. In the classical Transferable Utility (TU) setting, coalition worths
can be freely distributed amongst agents. However, in several application scenarios, this
is not the case and the Non-Transferable Utility setting (NTU) must be considered, where
additional application-oriented constraints are imposed on the possible worth distributions.
In this paper, an approach to define NTU games is proposed which is based on describing allowed distributions via a set of mixed-integer linear constraints applied to an
underlying TU game. It is shown that such games allow non-transferable conditions on
worth distributions to be specified in a natural and succinct way. The properties and the
relationships among the most prominent solution concepts for NTU games that hold when
they are applied on (mixed-integer) constrained games are investigated. Finally, a thorough analysis is carried out to assess the impact of issuing constraints on the computational
complexity of some of these solution concepts.

1. Introduction
Cooperative game theory providesunder the concept of coalitional gamesan elegant
framework for modeling multi-agent systems where agents might collaborate with other
agents, by forming coalitions in order to guarantee themselves some advantage. Within
this framework, each coalition S  N (where N is the set of all the players, also called the
grand-coalition), is assigned a certain worth v(S)  R, and the outcome of the game is a
vector of real payoffs (xi )iN that is meant to specify the distribution of the worth granted
to the players as the result of the game.
Coalitional games are very often classified according to the mechanisms underlying payoff
distributions. The best known and most widely studied class therein is that of coalitional
games with transferable utility (or TU games) (Osborne & Rubinstein, 1994), where no
constraints whatsoever are imposed over the way coalitional worths can be distributed
amongst coalition members. In this context, several outcomes might be associated with a

c
2010
AI Access Foundation. All rights reserved.

fiGreco, Malizia, Palopoli, & Scarcello

given game, and hence a relevant question is to understand which outcomes most properly
capture the rational behavior of the players. This matter has been extensively studied in
economics and social sciences (Aumann & Hart, 2002). In fact, various solution concepts
have been proposed in the literature to identify worth distributions that embody some
rational concept of stability, i.e., that are somehow immune to deviations caused by
groups of players who may decide to leave the grand-coalition and to form sub-coalitions in
order to claim for higher worths.
There are cases, however, where players cannot freely distribute the coalition worth so
that a pure TU framework is not appropriate for such modeling purposes (Aumann & Peleg,
1960). To deal with those scenarios, coalitional games without transferable utility (or NTU
games) have been introduced in the literature, where the worth function is defined as to
return all those allowed worth distributions (called consequences, in this setting) associated
with any given coalition, rather than associating just one real value with it. In fact, it is
easily understood that NTU games are more general than TU ones, since any game of the
latter kind can be expressed as an NTU game where any possible worth distribution among
the members of a coalition S is a consequence for S.
1.1 Modeling NTU Specifications via Mixed-Integer Linear Constraints on
TU Games
Enhancing TU games with application-oriented constraints over the set of all possible outcomes is an approach that has been exploited in the literature in order to model nontransferable scenarios. The first occurrence of the name constrained games goes back to
the seventies, and is due to Aumann and Dreze (1974), who considered games with coalition structures, where players are partitioned in groups S1 , . . . , Sk , and where any outcome
(xi )iN must allocate the total payoff v(S
P j ) exactly amongst the members of each group Sj ,
that is, so as to satisfy the equalities iSj xi = v(Sj ), for 1  j  k. However, Aumann
and Dreze noticed in their turn that considering constraints over TU games was not a novel
idea, since the core and the nucleolus (which are two prominent solution concepts for TU
games) were indeed defined by Gillies (1959) and Schmeidler (1969), respectively, on games
with outcomes restricted to subsets of R|N | .
Recently, constrained games have been reconsidered under the pragmatic perspective
of modeling some relevant application scenarios, such as price formation (Byford, 2007)
and autonomic wireless networks (Jiang & Baras, 2007). As a matter of fact, however,
they received considerably little attention over the years. In particular, no general framework was proposed in the literature and no systematic study of the (analytical, as well as
computational) properties of this kind of approaches was conducted so far.
In this paper, we embark on a systematic formalization of constrained games, and we
investigate a framework allowing to succinctly specify non-transferable conditions on the
outcomes of an underlying TU game, via a set of constraints expressed as mixed-integer
linear (in)equalities.1 Note that such constrained games are defined on top of an underlying
TU specification, and hence they are expected to retain some of the nice properties of the
transferable setting. However, their ability of restricting the set of possible outcomes makes
1. A good source for basic notions and results on mixed-integer linear programming is the book by
Nemhauser and Wolsey (1988).

634

fiMixed-Integer Constrained Coalitional Games

them fit the more general framework of NTU games, from which they smoothly inherit the
solution concepts that we shall use.
By allowing integer variables, the constrained games studied in this paper will improve
the expressiveness of classical NTU formalizations, in that admissible outcomes might be
possibly restricted over non-convex and non-comprehensive regions (definitions for these
properties are recalled in Section 3.2). Indeed, those NTU games that attracted much attention in earlier literature do not allow to specify arbitrary consequences (Aumann & Hart,
2002). Rather, according to the classical definition due to Aumann and Peleg (1960), an
NTU game is actually a game that must satisfy additional conditions such as, in particular, convexity and comprehensiveness. This view features several nice properties under a
mathematical perspective (Weber, 1994; McLean, 2002), and it influenced several other proposals for defining NTU games which appeared in the literature, where further additional
conditions are often considered. However, this view is not appropriate to model application
scenarios where those required properties do not naturally hold.
In fact, the framework of constrained games proposed in this paper can be viewed as a
framework to (succinctly) define NTU games where convexity and comprehensiveness do not
necessarily hold. This is an important peculiarity of our approach from a knowledge representation perspective. An intuitive exemplification of those scenarios where this peculiarity
might be very useful is illustrated below.
Example 1.1. Three brothers, Tim, John and Jim, aged 10, 8 and 5, resp., have collected
into a piggy money-box all the small Euro coins (values 1, 2, 5, and 10 cents) that Mom
every week has given to each of them since the age of four. Now, the time has come to
break the money-box and divide its content. In order to avoid quarrels among the kids,
Mom decides that the distribution has to go with their ages, so that Tim will deserve at
least 10/8 the money John will get and John, in its turn, will receive at least 8/5 of Jims
money share. (Jim is not very happy with that, but agrees to comply with Moms rule).
The money-box gets broken and the little treasure of seven Euros and ninety Euro cents,
as resulting from the available coin set including one-hundred 1-cent coins, seventy 2-cent
coins, fifty 5-cents coins, and thirty 10-cent coins, can then be divided amongst the kids.
Note that this scenario is based on the non-transferable condition that the treasure
cannot freely be distributed amongst the brothers. The specific distribution rule, however,
does not fit the classical NTU formalization by Aumann and Peleg (1960).
On the other hand, it is easily seen that the scenario can be modeled by means of a
set of linear (in)equalities, with a few variables taking values from the set Z of integer
numbers. In this example, admissible outcomes can indeed be identified as the solutions to
the following set of mixed-integer linear (in)equalities (the three brothers Tim, John and
Jim are denoted by using the indexes 1, 2 and 3, respectively):

635

fiGreco, Malizia, Palopoli, & Scarcello





























xi = 1  i1 + 2  i2 + 5  i5 + 10  i10 , 1  i  3
11 + 21 + 31 = 100
12 + 22 + 32 = 70
15 + 25 + 35 = 50
110 + 210 + 310 = 30
x1  10/8  x2
x2  8/5  x3
i1 , i2 , i5 , i10  0, 1  i  3
xi  R, i1 , i2 , i5 , i10  Z, 1  i  3

Note that the auxiliary variables ij denote the number of coins of value j taken by
player i, the first five equalities encode restrictions on the domains of the variables as defined
by the available coin set, and the subsequent two inequalities encode Moms rule (which
can be seen, for instance, as playing the role of a central market regulation authority). 
1.2 Contribution and Organization
Despite the intuitiveness of the modeling approach adopted in Example 1.1, there is no
reference framework in the literature accounting for it, both because of the specificity of
Moms rule and because money distribution is constrained by the available coin set, so that
those allowed outcomes do not form a convex set.
Proposing and investigating a framework that may serve to model such kinds of scenarios
is the main contribution of this paper. In more detail:
 We define a formal framework for NTU games based on mixed-integer linear constraints applied to an underlying TU game, we discuss its modeling capabilities, and
we show how main solution concepts for NTU gamesin particular, core, bargaining
set, kernel, nucleolus, and Shapley valuespecialize within this novel framework.
 We analyze the impact of constraints on the basic properties of such solution concepts.
Moreover, we highlight similarities and differences featured by constrained games as
opposed to TU games, by investigating in particular whether an outcome that is stable
(under these concepts) in a TU game remains stable if constraints are issued.
 We assess the impact of adding constraints on the computational complexity underlying some of these concepts. In particular, we consider games in characteristic function
form (von Neumann & Morgenstern, 1944) within a setting where worths are given
as oracles. In this context, we discuss both the intrinsic difficulty of checking whether
a given worth distribution is in the core or in the bargaining set, and of deciding the
non-emptiness of these solutions. Complexity results for constrained games are also
compared with those characterizing TU games.
The rest of the paper is organized as follows. In Section 2, we overview some basic
notions of cooperative game theory. The formal framework of constrained games is defined
in Section 3. The properties of this novel framework are illustrated in Section 4, and its
analysis from the computational viewpoint is carried out in Section 5. A discussion and a
few concluding remarks are reported in Section 6.

636

fiMixed-Integer Constrained Coalitional Games

2. Coalitional Games
An important issue in cooperative game theory is to determine payoff distributions for the
agents in scenarios where they can collaborate by forming coalitions, in order to obtain
higher worths than by acting in isolation. In this context, one usually does not take care of
other relevant problems emerging in the coalition formation process, such as the coalition
value calculation and the coalition structure generation problemsan excellent overview of
these problems and a state-of-the-art algorithm facing the latter one can be found in the
work by Rahwan, Ramchurn, Jennings, and Giovannucci (2009).
Coalitional games have been introduced by von Neumann and Morgenstern (1944) in
order to model payoff distributions problems in scenarios where utility can be freely transferred among players. In these cases, coalitional games can be described by associating a
payoff with each possible coalition. Thus, a coalitional game with transferable utility (TU)
is a pair hN, vi, where N is a finite set of players, and v is a function (v : 2N 7 R) that
associates with every coalition S  N a real number v(S), called the worth of S.
Scenarios where utility cannot be freely transferred among players were first formalized
by Aumann and Peleg (1960). In these scenarios, games have to be described by specifying
all the possible payoff distributions for the players in each coalition, rather than by just one
(global) payoff. For any coalition S  N , let |S| denote the cardinality of S, and let RS be
the |S|-dimensional real coordinate space, whose coordinates are labeled with the members
of S; in particular, given a payoff vector x  RS , xi denotes the component associated
with the player i  S. Then, a coalitional game without transferable utility (NTU) is a
pair hN, V i, where N is a finite set of players, and V is a function associating with every
coalition S  N a set of payoff vectors V (S)  RS , also called consequences.
Note that NTU games are generalizations of TU games. In particular, according to the
standard encoding2 discussed, e.g., in the handbook edited by Aumann and Hart (2002) and
in the book by Peleg and Sudholter (2007), the TU game hN, vi will be viewed throughout
this paper as the NTU game hN, Vv i, where:


fiX
Sfi
Vv (S) = x  R fi
xi  v(S) ,  S  N.
(1)
iS

Let G = hN, V i be an NTU game. A consequence x  V (N ) is an imputation of G if
the following two properties hold (see, e.g., Peleg, 1963; Peleg & Sudholter, 2007):

(1) Efficiency: for each y  V (N ), there is a player i  N such that xi  yi this property
is also known as weak Pareto optimality; and
(2) Individual Rationality: for each player i  N , xi  max{ yi | yi  V ({i}) }.
The set of all imputations of an NTU game G is denoted by X(G). If G is actually a TU
game, i.e., G = hN, vi (or, equivalently, G = hN, Vv i), it is immediate to check that:


fiX
Nfi
X(G) = x  R fi
xi = v(N ) and xj  v({j}), j  N .
iN

2. Indeed, this encoding allows most of the solution concepts originally defined for TU games to be smoothly
generalized to the NTU frameworkas we shall discuss later in the section.

637

fiGreco, Malizia, Palopoli, & Scarcello

In particular, note that xj  v({j}) encodes the individual rationality of player j at x.
An outcome for G is an imputation taken from X(G) specifying a payoff distribution for
all the players of the game. This outcome should represent a kind of agreement amongst
players, which has to be stable with respect to the possibility that subsets of players get
an incentive to deviate from it, by forming coalitions on their own. Depending on the criterium adopted to define this concept of stability, various (solution) concepts for coalitional
games can be defined. The most relevant solution concepts for coalitional gamessuch as
the core, the bargaining set, the nucleolus, the kernel, and the Shapley valuehave been
originally defined within the TU framework (see, e.g., Osborne & Rubinstein, 1994). Several
efforts have been subsequently paid to apply these concepts within the more general NTU
framework (see, e.g., Aumann & Hart, 2002). Natural extensions have been defined in some
cases, while natural counterparts are still missing and looked for in others.
In the following, we shall provide an overview of the definitions of the basic solution
concepts for TU games and their canonical extensions to NTU games.
2.1 Core
The concept of the core goes back to the work by Edgeworth (1881). In the TU framework,
it has been formalized by Gillies (1959), and it was first extended to the NTU framework
by Aumann (1961). In fact, this is the solution concept that enjoys the most canonical
extension to the NTU case, which is the one presented next.
Let G = hN, V i be an NTU game. For any coalition S  N , a vector y  RS of
real numbers is called S-feasible if y  V (S). Let x be a consequence in RN . Then, the
pair (y, S) is an objection to x if y is an S-feasible payoff vector such that yk > xk for all
k  Sin this case, the coalition S is also said to block x via y.
Definition 2.1. The core C (G) of an NTU game G = hN, V i is the set of all imputations
x to which there is no objection; that is,
C (G) = {x  X(G) |S  N, y  V (S) such that yk > xk , k  S } .



Thus, an imputation x in the core is stable because there is no coalition whose members will receive a higher payoff than in x by leaving the grand-coalition.
The application of Definition 2.1 over TU games exactly coincides with the original
formulation by Gillies (1959). Moreover, it is easily seen that, when applied over TU
games, Definition 2.1 can be equivalently restated as illustrated next (see, e.g., Osborne &
Rubinstein, 1994). For a coalition
S  N and a payoff vector x  RN , we define x(S) as
P
the value of the expression iS xi .
Definition 2.2. The core C (G) of a TU game G = hN, vi is the set of all imputations
x  X(hN, Vv i) such that, for each coalition S  N , x(S)  v(S).


Thus, the core of a coalitional game with transferable utility and |N | players is defined
by a set of inequalities over |N | variables and, in fact, it is a polytope in RN .

638

fiMixed-Integer Constrained Coalitional Games

2.2 Bargaining Set
The concept of bargaining set was defined by Aumann and Maschler (1964), and has many
variants even within the TU context (see, e.g., Maschler, 1992). A natural extension to the
NTU framework was given by Peleg (1963), which is discussed next.
Let G = hN, V i be an NTU game, and x be a consequence in V (N ). Let S  N be
a coalition, and y be an S-feasible payoff vector (i.e., y  V (S)). The pair (y, S) is an
objection of player i against player j to x if i  S, j 
/ S, and yk > xk for all k  S.
A counterobjection to the objection (y, S) of i against j to x is a pair (z, T ) where j  T ,
i
/ T , and z is a T -feasible payoff vector such that zk  xk for all k  T \ S and zk  yk
for all k  T  S. If there does not exist any counterobjection to (y, S), we say that (y, S)
is a justified objection.
Definition 2.3. The bargaining set B(G) of an NTU game G is the set of all imputations
x to which there is no justified objection.

Note that the above definitions straightforwardly apply to TU games, and coincide for
them with the one originally proposed by Aumann and Maschler (1964). For the sake of
completeness, we just recall here that y (resp., z) is an S-feasible (resp., T -feasible) payoff
vector in the TU game hN, vi if y  Vv (S)
P(resp., z  Vv (T )) holds;
P that is, if y(S)  v(S)
(resp., z(T )  v(T ))recall that y(S) = iS yi (resp., z(T ) = iT zi ).
2.3 Nucleolus

The nucleolus is a solution concept introduced by Schmeidler (1969). Its definition is based
on the notion of excess e(S, x, V ) of a coalition S at an imputation x, which is a measure
of the dissatisfaction of S at x.
In the case of TU games (where v denotes the worth function), it is widely accepted that
the canonical definition of the excess is e(S, x, Vv ) = v(S)  x(S). Then, for each vector
x  RN , let us define (x) as the vector where the excesses associated with all coalitions
(but the empty one) are arranged in non-increasing order:
(x) = (e(S1 , x, V ), e(S2 , x, V ), . . . , e(S2|N| 1 , x, V )).
Let (x)[i] denote the i-th element of (x). For a pair of imputations x and y, we say
that (x) is lexicographically smaller than (y), denoted by (x)  (y), if there exists a
positive integer q such that (x)[i] = (y)[i] for all i < q and (x)[q] < (y)[q].
Since the excess is a measure of dissatisfaction, the imputations lexicographically minimizing the vector of the excesses are very natural candidates to be the stable outcomes
for the game. This is indeed the idea underlying the definition of the nucleolus, as it was
defined by Schmeidler (1969) for TU games.
Definition 2.4. The nucleolus N (G) of a TU game G = hN, vi is the set
N (G) = {x  X(hN, Vv i) | y  X(G) such that (y)  (x)}.



For games that do not fit the TU framework, the above definition can still be used
provided that a suitable generalization of the concept of excess is conceived. The most
639

fiGreco, Malizia, Palopoli, & Scarcello

influential approach to define excess functions for NTU games was proposed by Kalai (1975),
who axiomatized the properties that such functions should satisfy as to retain some of the
nice features of the underlying TU specifications. These properties are as follows:
1. Let x, y  RN . If xi = yi for all players i  S, then e(S, x, V ) = e(S, y, V ) holds, for
each function V ;
2. Let x, y  RN . If xi < yi for all players i  S, then e(S, x, V ) > e(S, y, V ) holds, for
each function V ;
3. Let x  RS . If there is no vector y  V (S) such that, i  S, yi > xi , then
e(S, x, V ) = 0 holds, for each function V ;
4. e(S, x, V ) is continuous jointly in x and V .
As an example, a prototypical excess function discussed by Kalai is the following:


t
, i  S .
eK (S, x, V ) = sup t  R | y  V (S) such that yi = xi +
|S|

(2)

This function coincides with the canonical excess function v(S)x(S) whenever it is applied
on TU games (Kalai, 1975).
2.4 Kernel
The kernel is a solution concept originally introduced in the TU framework by Davis and
Maschler (1965) to help to understand the properties of the bargaining set.
For a TU game hN, vi, define the surplus si,j (x) of player i against player j at an
imputation x as the value si,j (x) = maxS|iS,j / S e(S, x, Vv ) = maxS|iS,j / S (v(S)  x(S)).
Definition 2.5. The kernel K (G) of a TU game G = hN, vi is the set:
K (G) = {x  X(hN, Vv i) | si,j (x) > sj,i(x)  xj = v({j}), i, j  N, i 6= j}.



Note that the above definition for TU games is again based on the notion of excess.
Intuitively, the surplus of player i against j at x is the highest payoff that player i can
gain (or the minimal amount i can lose, if it is a negative value) without the cooperation
of j, by forming coalitions with other players that are satisfied at x; thus, si,j (x) is the
weight of a possible threat of i against j. In particular, player i has more bargaining
power than j at x if si,j (x) > sj,i (x); however, player j is immune to such threat whenever
xj = v({j}), since in this case j can obtain v({j}) even by operating alone. We say that
player i outweighs player j at x if si,j (x) > sj,i(x) and xj > v({j}). The kernel is then the
set of all imputations where no player outweighs another one.
Generalizing the kernel to NTU games is based on considering generalizations of the
excess function, as for the nucleolus. Again, an influential approach, which is recalled next,
is due to Kalai. However, it is worthwhile noticing here that other approaches have also been
proposed in the literature (see, e.g., Orshan & Zarzuelo, 2000; Peleg & Sudholter, 2007).
Indeed, differently from the solution concepts discussed so far, variations of the kernel (and
640

fiMixed-Integer Constrained Coalitional Games

related concepts, such as the prekernel, which is the focus of the extensions cited above) to
NTU games are still subject of research and debate (cf. Serrano, 1997).
Let G = hN, V i be an NTU game. We say that a payoff vector t  RN is a transfer from
player j to player i if tj  0, ti  0, and tk = 0, for each player k  N \ {i, j}. The transfer
t is justified at an imputation x, if for every real number , 0 <  < 1, the vector y = x + t
(such that yk = xk + tk , for each k  N ) is an individually rational vector in V (N ) and
(x + t)  (x)of course, an excess function for G satisfying Kalais axiomatization must
be used in order to define the excess vectors. The kernel K (G) of G is the set: K (G) =
{x  X(G) | there is no justified transfer from player j to player i at x, i, j  N, i 6= j}.
2.5 Shapley Value
The Shapley value is a solution concept introduced in the TU framework by Shapley (1953).
This concept associates with every TU game G = hN, vi a unique payoff vector (G)  RN ,
where each component (G)i , which is called the Shapley value of player i, indicates the
worth to be assigned to player i, based upon her ability in cooperation as measured by the
expected marginal contribution of player i to forming coalitions (as formalized below).
Let  be a permutation on the set N of players. For any player i, we denote by pi the
set of players preceding i in . The marginal contribution of player i to the coalition pi is
v(pi  {i})  v(pi ). If permutations are chosen uniformly at random from the set  of all
possible permutations,
P the expected marginal contribution of player i in the game G is the
value (G)i = |N1 |!  v(pi  {i})  v(pi ) or, equivalently:
(G)i =

X

SN \{i}

|S|!(|N |  |S|  1)!
(v(S  {i})  v(S)) .
|N |!

The Shapley value is the unique payoff vector satisfying the following properties, which
constitute its axiomatic characterization3 :
P
(1) Efficiency:
iN (G)i = v(N ).

(2) Symmetry: Two players i and j are symmetric if, for each S  N with i, j 
/ S,
v(S  {i}) = v(S  {j}). If players i and players j are symmetric, then (G)i = (G)j .
(3) Dummy: A player i is dummy if, for each S  N \ {i}, v(S  {i})  v(S) = v({i}). If
player i is a dummy player, then (G)i = v({i}).
(4) Additivity: Let G  = hN, wi be a TU game, and G  = hN, v + wi be the TU game such
that (v+w)(S) = v(S)+w(S) for each coalition S  N . Then, (G  )i = (G)i +(G  )i .

Note that the Shapley value might not satisfy the individual rationality, and thus it is
not necessarily an imputation. Payoff distributions that are efficient, but not necessarily
individually rational, are called pre-imputations in the literature.
Generalizing the Shapley value to the NTU framework is not straightforward. Different
extensions of the Shapley value for NTU games have been proposed. Each of them, when
3. The characterization reported here is the one that can be found most often in the literature. However,
the original axiomatic formulation of Shapley requires the carrier axiom instead of the efficiency and
dummy axioms; the two axiomatizations are equivalent (Shapley, 1953; Winter, 2002).

641

fiGreco, Malizia, Palopoli, & Scarcello

evaluated on the NTU version of a TU game, coincides with the standard Shapley value for
TU games. Here we discuss the generalization proposed by Shapley (1969) himself in the
formulation reported by McLean (2002), and we refer the interested reader to this latter
work for an extended treatment of the subject of values for NTU games, and to the paper
by Hart (2004) for a comparison between the most notable three of them.
Let G = hN, V i be an NTU game. For a vector   RN of strictly positive real numbers,
let G be the game hN, v i where
(
)
X
v (S) = sup
i zi | z  V (S) .
iS

The TU game G is said to be defined for G if v (S) is finite for each S.
Definition 2.6. Let G = hN, V i be an NTU game. A vector x  RN is a Shapley NTU
value payoff for G if there exists a vector   RN of strictly positive real numbers such
that: x  V (N ); G is defined for G; and i xi = (G )i for each player i  N . The set of
all Shapley NTU values for G is denoted by (G).

Shapley NTU values fulfill, with some adaptations, the same axioms characterizing
standard TU Shapley values. Actually, these axioms do not suffice to uniquely characterize
the NTU counterpart, and other axioms have to be issued in order to define unambiguously
the NTU Shapley value. An axiomatization for the NTU case was given by Aumann (1985).
The interested reader is referred again to the work by McLean (2002), for more on this issue.
2.6 Properties of Solution Concepts for TU Games
We conclude by recalling some well-known properties of the solution concepts discussed
above, when they are applied over TU games.
Proposition 2.7 (see, e.g., Osborne & Rubinstein, 1994). Let G = hN, vi be a TU game
such that X(G) 6= . Then:
(1) |N (G)| = 1;
(2) N (G)  K (G) (hence, K (G) 6= );
(3) K (G)  B(G) (hence, B(G) 6= );
(4) C (G)  B(G); and,
(5) C (G) 6=  implies N (G)  C (G).
Note that there is no relationship between the Shapley value and the other solution
concepts (just recall that the Shapley value is not necessarily an imputation).

3. Constrained Games via Mixed-Integer Linear (In)Equalities
Assume that a TU game G = hN, vi is given and consider the problem of modeling and
dealing with constraints to be imposed on feasible worth distributions amongst players in G.
642

fiMixed-Integer Constrained Coalitional Games

These constraints might be implied by the very nature of the domain at hand (e.g., when the
worth is not arbitrarily divisible), or because they reflect some hard preferences expressed
by the players or by some regulation authorityrecall again Example 1.1. Our approach
to encode application-oriented constraints within a classical coalitional TU game setting
is by defining a set of mixed-integer linear (in)equalities, which have to be satisfied by the
imputations of the game G. This approach is first formalized below; subsequently, we shall
illustrate its modeling capabilities and discuss its relationships with the TU framework.
We start by recalling that a mixed-integer linear (in)equality is a linear (in)equality
where some variables might be constrained to take values from the set Z of integers. For a
set LC of mixed-integer linear (in)equalities, we denote by real (LC) and int(LC) the sets of
all the variables in LC defined over R and Z, respectively. Moreover, we assume that worth
distributions can be constrained by defining inequalities via player and auxiliary variables.
A player variable has the form xi , where i  N is a player in the underlying TU game,
and it is meant to encode the worth that has to be assigned to player i in the outcomes
of the game. The (possibly empty) set of the auxiliary variables in LC is then the set
real (LC)  int(LC) \ {xi | i  N }. Auxiliary variables are sometimes useful for the modeling
purposes, as we illustrated in Example 1.1.
Let us now proceed with our formalization. Let G = hN, vi be a TU game, and recall
from Section 2 that G can be viewed as the NTU game hN, Vv i. Let LC be a set of mixedinteger linear (in)equalities. Define (LC) as the set of all the solutions to LC. Moreover, for
a coalition S  N , let (LC)[S] denote the projection of (LC) onto the subspace associated
with payoff domains for players in S; that is, a vector y with index set S belongs to (LC)[S]
if and only if there is a vector x  (LC) such that xi = yi holds, for each i  S.
Intuitively, a constrained game on LC is defined by restricting the consequences of an
underlying TU game G to those belonging to the solution space of the set LC of mixed-integer
linear (in)equalities projected onto the subspace associated with player variablesrecall
that further auxiliary variables may occur in LC. This is formalized below.
Definition 3.1 (Mixed-Integer Constrained Games). Let G = hN, vi be a TU game and
let LC be a set of mixed-integer linear (in)equalities. Then, the (mixed-integer) constrained
game G|LC is the NTU game hN, VLC i where VLC (S) = Vv (S)  (LC)[S]. That is,


fiX
Sfi
VLC (S) = x  R fi
xi  v(S) and x  (LC)[S] ,  S  N.

iS
3.1 Modeling Capabilities of Constrained Games

Constraining a TU game G via a set LC of (in)equalities that do not involve integer variables
(i.e., int(LC) = ) is an abstraction of those approaches in the literature that consider
specific sets of (in)equalities over real variables (such as Aumann & Dreze, 1974; Byford,
2007; Jiang & Baras, 2007). In particular, this capability might be exploited to:
(1) State hard preferences on the worth distributions.
As an example, consider a game G = hN, vi over the set of players N = {1, 2, 3, 4},
and where v(N ) = 10. Assume that players 3 and 4 together require to get at least a

643

fiGreco, Malizia, Palopoli, & Scarcello

half of the worth. Then, this requirement can be modeled as:

x3 + x4  5
x1 , x2 , x3 , x4  R
Moreover, by allowing integer variables, completely novel modeling capabilities emerge
in our setting w.r.t. earlier approaches. Indeed, integer variables can be used to isolate
non-convex regions, which might be needed to model specific application requirements that
are NTU in their very nature, as exemplified below.
(2) Consider alternative scenarios.
By allowing integer variables, we may model alternative preferences of the players, i.e.,
we may enforce disjunctions of preferences. For instance, consider a scenario where
either players 1 and 2 must get together no more than 3, or players 2 and 3 must get
together no more than 5. In this case, we have two constraints (i.e., x1 + x2  3 and
x2 + x3  5) and the goal is to define a set of (in)equalities prescribing that at least
one of them is satisfied. To this end, an auxiliary integer variable can be used:

x1 + x2  3 + U  y




 x2 + x3  5 + U  (1  y)
0y1



x ,x ,x  R

 1 2 3
yZ

where the constant value U is an upper bound on the worth of any coalition. Indeed,
in the case where y = 1, the constraint x1 + x2  3 + U is trivially satisfied (because U
is sufficiently large), and thus, we just enforce x2 +x3  5. Symmetrically, if y = 0, the
constraint x2 + x3  5 + U is trivially satisfied, and thus, we just enforce x1 + x2  3.
Of course, with simple manipulations, one may easily specify other kinds of alternatives, e.g., the fact that at least (or at most) k given constraints are to be satisfied.

(3) Restrict worth functions over specific domains.
When domains are required to be integer intervals, this is rather obvious. For instance,
assume that x3 should take values from the domain {4, 5, 6, 7, 8, 9, 10}. Then, we may
simply consider the following constraints:

 4  x3  10
x ,x ,x  R
 1 2 4
x3  Z

In order to model more general scenarios, we can have (as in point (2) above) mixedinteger linear (in)equalities to be defined over auxiliary variables. For instance, assume
that player 2 wants either to take the whole worth for herself (even when forming
coalitions with other players) or, whenever this is not possible, to get nothing. This
can be modeled by a few constraints, over an additional variable win fact, notice

644

fiMixed-Integer Constrained Coalitional Games

that v(N ) is a constant value for a game given at hand:

x2 = v(N )  w



0w1
x ,x ,x ,x  R


 1 2 3 4
wZ

Note that Example 1.1 basically presents a more realistic case, where several auxiliary
variables are used to restrict money distributions to the available coin set.
Now that the basic modeling capabilities of constrained games have been discussed,
in order to illustrate possible applications of the resulting framework, it is convenient to
preliminarily observe two of its properties (which are related to the use of integer variables).
First, it is easy to check that, over constrained games, we may deal with imputation
sets of arbitrary sizes.4
Proposition 3.2. Let G = hN, vi be a TU game and let X  X(G) be an arbitrary finite
set of imputations. Then, there is a finite set of constraints LC such that X(G|LC ) = X .
In addition, integer variables might be used to succinctly specify exponentially many
imputations via polynomially many (in)equalities.
Proposition 3.3. There exists a class C = {G|nLC }n>0 of constrained games such that each
game G|nLC is over n + 1 players, LC consists of 2  n + 1 inequalities, and |X(G|LC )| = 2n .
We believe that the setting emerging from the above properties is rather appealing
from a knowledge representation perspective. Indeed, one may exploit constrained games
to naturally model scenarios where non-transferable conditions emerge, by devising very
compact specifications of the desired restrictions on how utilities may be transferred among
coalition members. In fact, various circumstances can be envisaged where the usage of
constrained games is a natural choice; for instance, whenever the worth to be distributed
among the agents comes as a set of indivisible goods, as exemplified below.
Example 3.4 (Distributing indivisible goods). A certain region of the country is
famous for hosting several producers of two kinds of goods, named  and . For each
producer i  {1, . . . , n}, let i and i denote the quantity of  and  pieces produced by i,
respectively. By assembling together one piece  and one piece , a novel kind of indivisible
good can be obtained and, in fact, commercializing the assembled product is a much more
advantageous business than selling  and  separately. Therefore, an agreement is found
amongst producers in the area in order to assemble the pieces of  and  that are overall
available, provided that the resulting units of the assembled product are (fairly) distributed
amongst the involved producers, which would like to independently commercialize them.
This scenario can be easily modeled within our framework as follows. First, we associate
with every coalition S  {1, . . . , n}, the number of pieces of the assembled product that S
can produce. Thus, we just define:
X
X
v(S) = min(
i ,
i ).
iS

iS

4. For the sake of exposition, proofs of the propositions stated in this section are reported in the Appendix.

645

fiGreco, Malizia, Palopoli, & Scarcello

Then, since the assembled product is indivisible, any possible worth distribution is a
vector of non-negative integers, which can immediately be modeled via the following set of
constraints LC = {xi  0, xi  Z, 1  i  n}. In particular, (LC) is not a convex region,
so that earlier modeling perspectives on NTU games, such as those present in the handbook
edited by Aumann and Hart (2002), do not apply here.

There are cases, however, where the worth might practically be assumed to be divisible,
but specific constraints regulate its actual distribution. Notably, even in these cases, integer
variables may play a crucial role as illustrated next.
Example 3.5 (Service composition). Assume that a service T can be acquired for 100
Eurosfor the sake of simplicity, we assume that money is divisible, for otherwise worth
distributions might simply be restricted over a discrete domain as in Example 3.4 or in
Example 1.1. Supplying the service T implies executing m tasks, named t1 , . . . , tm .
Assume also that there is a set {1, . . . , n} of agents, each one capable of carrying out
some of those m tasks, and let sij denote the ability of agent i to perform the task tj (sij = 1
means that agent i is able to perform tj , whereas sij = 0 means that she is not capable to
do so).PThus, a coalition S  {1, . . . , n} is capable of supporting the service T in the case
where iS sij  1, for each j  {1, . . . , m}.
Assume, moreover, that in order to complete T , not only all of its tasks must be completed, but agents contributing to T must be able to exchange some partial results returned
by performing required tasks. Establishing a communication infrastructure guaranteeing
the needed result-transfers to take place has a specific cost for each coalition S, which we
denote by com(S) < 100. Hence, the amount of money that might finally be distributed
amongst players in S is described by the following worth function:

P
i
100  com(S) if
iS sj  1, j  {1, . . . , m}
v(S) =
0
otherwise.
Note that the above scenario defines a classical TU game G = h{1, . . . , n}, vi. However,
things may be significantly different if we assume that each agent i has to sustain an internal
cost, say cij , whenever actually performing the task tj , and that, hence, she may decide not
to perform the task at all, if it is not convenient. Indeed, in this case, letting ji  {0, 1}
be a variable denoting whether i is actually performing tj , it is P
natural to state that the
i
i
total internal cost for agent i (which is given by the expression m
j=1 j  cj ) should not
exceed what the agent gets from the worth distribution. Hence, utilities cannot be freely
distributed and, for a proper modeling of this more realistic scenario, the game has to be
enriched with the following set of constraints:
X

ji = 1, j  {1, . . . , m}




iN



m

X



ji  cij , i  {1, . . . , n}
 xi 
j=1
LC =



0  ji  sij , i  {1, . . . , n}, j  {1, . . . , m}






x1 , . . . , xn  R



 i
j  Z, i  {1, . . . , n}, j  {1, . . . , m}
646

fiMixed-Integer Constrained Coalitional Games

With respect to this formalization, it is worthwhile noting that if (LC) is empty then the
service cannot be provided at all, and indeed X(G|LC ) would be empty in its turn. Otherwise,
i.e., if (LC) 6= , the imputations of G|LC correspond to those worth distributions associated
with some legal staffing for the tasks, rather than to all arbitrary possible worth distributions
(as it would be in the plain TU case).
It is worthwhile contrasting the above formulation with an alternative TU one, where
the constraints in LC are directly encoded in the definition of worth function, instead of
using a separate component thereof, as done in the NTU framework we are proposing here.
For instance, one may add to the condition for v(S) = 100  com(S) the requirement
that there exists an element x  (LC) such that for each task tj , there is a player i  S
with ji 6= 0 (in x). This way, we can ensure that the payoff 100  com(S) is assigned
to each coalition S which is formed by players that can perform some task conforming
with cost constraints, and that can jointly complete T this refined modeling perspective
is exactly the one underlying the class of linear programming games (see, e.g., Owen, 1975).
However, this approach would not prescribe how the payoff 100  com(S) has to be actually
distributed amongst the players in S. In fact, while focusing on accurately modeling the
worth function, it cannot guarantee that the outcome of the game (according to any chosen
solution concept) fulfils the desired constraints on the distribution of payoffs for the single
players. In other words, using constraints in the definition of the worth function may be
useful in certain cases for more careful modeling purposes, but cannot in general replace
the use of external constraints to actually constrain the allowed worth distributions. 
As an important remark, we note here that the structure of the above example may
well be used as a guideline in the formalization of other application scenarios. Indeed,
the basic idea is to use mixed-integer linear constraints to define solutions for combinatorial problems associated with feasible worth distributions (reflecting, e.g., the costs of
such solutions). Thus, while we have contextualized the approach to the case of a staffing
problem, very similar encodings can be used to define constrained games suited to deal with
scheduling and planning problems, just to cite a few.
3.2 A Closer Look at Constrained Games
We can now resort to the presentation of the framework of constrained games by analyzing the structure of their consequences, and the role played by the individual rationality
requirement in this context. This analysis will provide important bases for our subsequent
treatment of the analytical and the computational properties of NTU solution concepts as
applied on constrained games.
3.2.1 Consequences in Constrained Games
In order to understand the nature of constrained games, it is convenient to take a closer
look at the structure of the function VLC . In fact, while in principle an NTU game hN, V i
does not impose any requirement on the function associating a set of consequence V (S) with
each coalition S  N , those NTU games that attracted much attention in the literature do
not actually allow arbitrary consequences to be specified (Aumann & Hart, 2002). Indeed,
the sets of consequences are usually required to satisfy some additional conditions, which
are (de facto) conceived as to guarantee that nice properties hold over the solution concepts
647

fiGreco, Malizia, Palopoli, & Scarcello

of interest. Some of the assumptions that have been most often considered in the literature
(not necessarily required to simultaneously hold) are recalled next. For each S  N , V (S)
might be required to be:
(1) (Upper) Bounded: there is a real number a  R such that for each x  V (S), and for
each i  S, xi  a holds;
(2) Closed: V (S) contains its own boundaries;
(3) Compact: V (S) is both closed and bounded;
(4) Comprehensive: if x  V (S), y  RN and (i  S)(yi  xi ), then y  V (S);
(5) Convex: for each pair x, y  V (S), and for each real number t, 0 < t < 1, the vector
(1  t)x + ty belongs to V (S).
(6) Non-empty: |V (S)| > 0.
In the case of constrained games, we do not explicitly ask for the above requirements
to be satisfied, thereby giving rise to a setting with very powerful modeling capabilities (as
we discussed in Section 3.1). The differences between constrained games and classical NTU
games are illustrated next.
Consider the function VLC associated with a constrained game G|LC . The first difference
concerns property (1), because VLC (S) is not required to be bounded (as for the TU case,
where individual payoffs for players in S are not bounded in general, since the only requirement is that their sum does not exceed the worth v(S) associated with Ssee Equation (1)
in Section 2). Actually, this is not a substantial difference given that, in any possible worth
distribution for G|LC corresponding to any solution concept illustrated in Section 2, the
payoff of each player is bounded.
Similarly, it is easy to see that there might be cases where property (2) does not hold
in the context of constrained games. Indeed, VLC (S) might be not closed whenever LC
contains some strict inequality that excludes the boundary of VLC (S). However, it is known
that such cases are undesirable for some solution concepts, and hence we shall consider
constraints based on non-strict inequalities only. Property (3) is the combination of the
first two properties, and thus the above lines of reasoning still apply.
The differences in the remaining three properties, instead, do characterize the framework
of constrained games and are at the basis of its modeling capabilities. In fact, the ability to
handle sets of consequences that are not comprehensive and convex, and that are possibly
empty for some coalition, is an important peculiarity of constrained games from a knowledge
representation perspective. Indeed, we may lose comprehensiveness each time a constraint
on the payoff distribution is given which states that some players are required to get at least
a certain worth; we may lose convexity (as well as comprehensiveness) each time integrality
constraints are involved. Moreover, we may deal with an empty set of consequences for
some coalition S whenever there is no feasible way to distribute the worth associated with
S according to the constraints that players in S must satisfy.
Example 3.6. Consider the TU game G = hN, vi such that N = {1, 2, 3}, v({1, 2, 3}) = 3,
and v(S) = 0 for each S  {1, 2, 3}. Consider a scenario where the worth in G is restricted
648

fiMixed-Integer Constrained Coalitional Games

to be an integer value (i.e., payoff distributions are taken from Z{1,2,3} ), and where players
1 and 2 require to get at least 2. These constraints can be modeled as follows:

x1 + x2  2
LC =
x1 , x2 , x3  Z
Note that (LC)[{1, 2, 3}] = {x  Z{1,2,3} | x1 + x2  2}, (LC)[{1, 2}] = {x  Z{1,2} |
x1 +x2  2}, (LC)[{1, 3}] = Z{1,3} , (LC)[{2, 3}] = Z{2,3} , (LC)[{1}] = Z{1} , (LC)[{2}] =
Z{2} , and (LC)[{3}] = Z{3} . Then, the constrained game G|LC = hN, VLC i is such that:
VLC ({1, 2, 3}) =
=
VLC ({1, 2})
=
VLC ({1, 3})
=
=
VLC ({2, 3})
VLC ({i})
=

{x  R{1,2,3} | x1 + x2 + x3  3}  (LC)[{1, 2, 3}] =
{x  Z{1,2,3} | x1 + x2 + x3  3, x1 + x2  2};
{x  R{1,2} | x1 + x2  0}  (LC)[{1, 2}] = ;
{x  R{1,3} | x1 + x3  0}  (LC)[{1, 3}] = {x  Z{1,3} | x1 + x3  0};
{x  R{2,3} | x2 + x3  0}  (LC)[{2, 3}] = {x  Z{2,3} | x2 + x3  0};
{x  R{i} | xi  0}  (LC)[{i}] = {x  Z{i} | xi  0}; (i  {1, 2, 3})

Despite the very simple constraints considered for G, it is immediate to check that (e.g.)
VLC (N ) is not comprehensive and not convex, and that VLC ({1, 2}) is empty. Indeed, the
integrality constraints immediately lead to loose comprehensiveness and convexity. Moreover, the fact that players 1 and 2 require to get at least 2 implies that the coalition {1, 2}
will never form to deviate from the grand-coalition, given that these two players cannot
guarantee themselves any worth when acting without player 3 (indeed, v({1, 2}) = 0). 
3.2.2 Individual Rationality in Constrained Games
A further important issue to be pointed out for constrained games is related to the individual
rationality requirement over the set of imputations. Let G = hN, V i be an NTU game, and
recall from Section 2 that any imputation x  X(G) must be such that for each player
i  N , xi  max{ yi | yi  V ({i}) }.
Consider a constrained game G|LC where G = hN, Vv i is the underlying TU game. By
Definition 3.1, the set VLC ({i}) coincides with Vv ({i})  (LC)[{i}]. Then, because of the individual rationality requirement, for each player i, xi  max{ yi | yi  Vv ({i})(LC)[{i}] }.
Note that a special case occurs when VLC ({i}) = Vv ({i})  (LC)[{i}] = . In this case,
indeed, max{ yi | yi  VLC ({i}) } is not defined (as a real value). An approach might be
therefore to observe that the game is over-constrained, and then stop the analysis. This
approach is, in fact, consistent with several NTU formalizations requiring that the set of
consequences is non-empty, for each possible coalition (see Section 3.2.1).
However, in Example 3.6 we have already pointed out that empty sets of consequences
may naturally emerge from constrained games, because VLC (S) =  reflects the fact that
coalition S cannot form to deviate from the grand-coalition, for there is no worth distribution
that can be in principle granted to its members alone (according to the underlying TU
game), and that satisfies the constraints at hand. Consequently, a finer-grained perspective
should be considered to deal with the individual rationality requirement, in this special case
where VLC ({i}) = , for some player i  N .
The basic observation is that VLC ({i}) =  necessarily implies that xi > v({i}) holds, for
each imputation x  VLC (N ). Thus, in these extreme scenarios, the individual rationality
649

fiGreco, Malizia, Palopoli, & Scarcello

constraint is conceptually satisfied (though formally undefined) for each possible imputation
x, since constraints in LC require xi to be larger than v({i}). Technically, we stress here that
the same conclusion is implied by defining max{} = , which is the standard extension
of max over empty sets.
In the light of the above perspective, we can show that individual rationality is preserved when constraints are added to a given TU game.
Proposition 3.7. Let G = hN, vi be a TU game and let x be a payoff vector that is
individually rational w.r.t. G (i.e., xi  v({i}), for each player i  N ). Then, for each set
LC of constraints, x is individually rational w.r.t. the constrained game G|LC .

4. Solution Concepts for Constrained Games
Constrained coalitional games are special cases of NTU games, and therefore they inherit
from them the various solution concepts we have discussed in Section 2. Thus, for a constrained (and, as such, NTU) game G|LC , it is of interest to compute the core, the bargaining
set, the nucleolus, the kernel, and the Shapley value. In this section, we will study the properties of these concepts, by highlighting similarities and differences featured by constrained
games as opposed to TU games.
In a nutshell, we will show that the properties of TU games stated in Proposition 2.7
still hold over constrained games for all solution concepts, but for the bargaining set that
might be empty in some games. Moreover, the portion of the core of a TU game G that
satisfies all the constraints is preserved, in the sense that it is a subset of the core of
the constrained game G|LC built on top of G. On the other hand, for the other solutions
concepts, the preservation property holds in special cases only.
Before illustrating our results, it is useful to state a property relating the imputation
set of a constrained game with the imputation set of the underlying TU game.
Proposition 4.1. Let G|LC be a constrained game where G = hN, vi.
(LC)[N ]  X(G|LC ).

Then, X(G) 

Proof. Let x be a payoff vector in X(G)  (LC)[N ]. Since x  X(G) then x  Vv (N ).
We are also assuming that x  (LC)[N ] and hence we have that x  VLC (N )recall, by
Definition 3.1, that VLC (N ) = Vv (N )  (LC)[N ]. Being x  X(G), x is also efficient w.r.t.
G, meaning that for each y  Vv (N ), there is a player i  N with xi  yi . Moreover, since
VLC (N )  Vv (N ), x is also efficient w.r.t. G|LC . Finally, by Proposition 3.7, we know that x
is individually rational w.r.t. G|LC . Thus, x  X(G|LC ).
Based on the above result, to show that an imputation x  X(G) also belongs to X(G|LC ),
in the following we shall just show that x satisfies the constraints in LC, thereby avoiding
to explicitly reason on the efficiency and the individual rationality of x.
4.1 Relationships Among Solution Concepts
We start our analysis by investigating whether the properties of the basic solution concepts
(as they appear from Proposition 2.7) are preserved in the setting of constrained games.

650

fiMixed-Integer Constrained Coalitional Games

4.1.1 Counterparts of Proposition 2.7.(1) and Proposition 2.7.(2)
Let us begin by focusing on the first two properties in Proposition 2.7, which pertain to the
nucleolus. In the TU framework, the nucleolus always consists of exactly one imputation.
In the constrained framework, the properties of this solution concept are intimately related
to the closeness of (LC) (and, in turn, of X(G|LC )), i.e., to whether (LC) contains its
own boundaries. Recall from Section 3 that (LC) might not be closed only due to the
occurrence of some strict inequalities in LC.
Proposition 4.2. There exists a constrained game G|LC (with int(LC) = ) such that
X(G|LC ) 6=  and where N (G|LC ) =  (for the excess function eK in Equation (2) on
page 640).
Proof. Consider the game G over players {1, 2}, where v({1, 2}) = 1 and v({1}) = v({2}) =
0. Given the constraint LC = {x1 < 12 , x1  R}, one may note that X(G|LC ) 6= . Indeed,
observe that max{x1 | x1  VLC ({1})} = max{x2 | x2  VLC ({2})} = 0, since VLC ({i}) =
Vv ({i})  (LC)[{i}] = {xi  R{i} | xi  0}  (LC)[{i}] = {xi  R{i} | xi  0}, for
each i  {1, 2}. Thus, a payoff vector x  X(G|LC ) is just required to satisfy x1  0 and
x2  0, in order to be individually rational. In particular, we claim that X(G|LC ) = {x 
R{1,2} | x1 + x2 = 1, x1 < 12 , x1  0, x2  0}. Indeed, any vector x  VLC (N ) = {x  RN |
x1 + x2  1, x1 < 12 } such that x1 < 12 and x2 < 12 is not efficient, given that there is a
vector y  VLC (N ) such that x1 < y1 < 12 and x2 < y2 < 12 , with y1 + y2 < 1. Moreover,
any vector x  VLC (N ) such that x1 < 12 , x2 > 12 , and x1 + x2 < 1 is also not efficient,
given the existence of a vector y  VLC (N ) such that x1 < y1 = x1 + 1x22 x1 < 12 and
x2 < y2 = x2 + 1x22 x1 , with (therefore) y1 + y2 = 1.
Consider now an imputation x with x1 + x2 = 1 and x1 < 12 (and hence x2 > 12 ).
Then, we can always build an imputation y 6= x such that y1 = x1 + ( 12  x1 )/2 > x1 and
y2 = 1  y1 < x2 ; just notice here that y1 < 12 holds. For this new imputation, it is the case
that (y)  (x) holds w.r.t. the excess function eK in Equation (2). Indeed, by recalling
that VLC ({i}) = {xi  R{i} | xi  0} and that xi  0 and yi  0 hold, for each i  {1, 2}, we
have (x) = (0, x1 , x2 ) and (y) = (0, y1 , y2 ). Thus, there is no imputation x that
belongs to N (G|LC ), i.e., N (G|LC ) = . Note that (LC) is not closed.
In practical applications of linear programming, one may deal with non-strict inequalities
only (see, e.g., Papadimitriou & Steiglitz, 1998); in these cases (i.e., whenever X(G|LC ) is
closed and hence compact, since it is always bounded), the nucleolus is not empty. This
property was observed to hold by Kalai (1975) along with the relationship of the nucleolus
with the kernel. These properties can be restated in our context as follows.
Proposition 4.3 (cf. Kalai, 1975). Let G|LC be a constrained game with X(G|LC ) 6= .
Then,
(1) if X(G|LC ) is compact, then |N (G|LC )|  1;
(2) N (G|LC )  K (G|LC ) (hence, K (G|LC ) 6=  whenever X(G|LC ) is compact).
In the following, examples and counterexamples will be built avoiding the use of strict
inequalities.
651

fiGreco, Malizia, Palopoli, & Scarcello

4.1.2 Counterparts of Propositions 2.7.(3), (4), and (5)
Let us now move to analyze the counterpart of Proposition 2.7.(3). To this end, we first
notice that, unlike in the TU case, the bargaining set can sometimes be empty.
Proposition 4.4. There exists a constrained game G|LC (with int(LC) = ) such that
X(G|LC ) 6=  and B(G|LC ) = .
Proof. Consider the TU game G over players {1, 2, 3, 4}, whose worths are as follows:
v({1, 2, 3, 4}) = 3, v({1, 2}) = 2, v({2, 3, 4}) = 3, v({1, 3, 4}) = 3, v({2}) = 1, and v(S) = 0
for any other coalition S  {1, 2, 3, 4}. Consider moreover the following set of constraints:

x1 + x2 + x3 + x4 = 3




x
 2 + x3 + x4 = 3
x1 + x3 = 1
LC =


x + x4 = 1


 1
x1 , x2 , x3 , x4  R

Let x be the payoff vector with x1 = 0 and x2 = x3 = x4 = 1. Observe that x satisfies
the constraints in LC. Moreover, x is an imputation of X(G); thus, by Proposition 4.1, x
belongs to X(G|LC ). In fact, x is the only vector in (LC) and, therefore, it is the only
imputation in X(G|LC ). Thus, to prove that B(G|LC ) = , we have just to show that x is
not contained in B(G|LC ). To this end, consider the objection (y, {1, 2}) of player 1 against
player 3, where y1 = 13 and y2 = 53 . Note that y1 + y2 = v({1, 2}) = 2, y2 > 1 and y1 > 0,
and thus y is {1, 2}-feasible. Moreover, recall that v({3}) = v({3, 4}) = v({2, 3}) = 0. It
follows that counterobjections of 3 against 1 to (y, {1, 2}) may be only constructed over
the coalition {2, 3, 4}. Assume that (z, {2, 3, 4}) is one such a counterobjection. For player
2, which belongs to the intersection of the two coalitions {1, 2} and {2, 3, 4}, z2  y2 > 1
holds. Because of the constraint z2 + z3 + z4 = 3, this entails z3 + z4 < 2. However, this
is impossible since we should have also z3  x3 = 1 and z4  x4 = 1. Thus, there are
no possible counterobjections to the objection (y, {1, 2}) to x. It follows that x does not
belong to B(G|LC ) and, hence, B(G|LC ) = , even though X(G|LC ) 6= .
As a consequence, we derive that the counterpart of Proposition 2.7.(3) does not hold
over constrained games. Indeed, we may just consider the game G|LC defined in the proof
of Proposition 4.4 and observe that, since X(G|LC ) 6= , by Proposition 4.3, we know that
K (G|LC ) 6= .
Corollary 4.5. There is a constrained game G|LC (with int(LC) = ) such that B(G|LC ) = 
while K (G|LC ) 6=  (and thus K (G|LC ) 6 B(G|LC )).
Below we complete the picture pertaining to the bargaining set, by showing that the
core is always included in it. This provides the counterpart of Proposition 2.7.(4).
Proposition 4.6. Let G|LC be a constrained game. Then, C (G|LC )  B(G|LC ).
Proof. Consider an imputation x  C (G|LC ) and assume by contradiction that x 
/ B(G|LC ).
By this, there must exist an objection (y, S) to x. Therefore, it must be the case that y is an
S-feasible payoff vector in G|LC and yk > xk , for each k  S. This implies that x 
/ C (G|LC ):
a contradiction. Thus, x  B(G|LC ).
652

fiMixed-Integer Constrained Coalitional Games

Figure 1: Illustration of the Solution Concepts in Example 4.8.
We finally stress that the counterpart of Proposition 2.7.(5) is already known from the
work by Kalai (1975), and can be restated in the settings of constrained games as follows.
Proposition 4.7 (cf. Kalai, 1975). Let G|LC be a constrained game. Then, C (G|LC ) 6= 
implies N (G|LC )  C (G|LC ).
4.2 Preservation of Solution Concepts
We continue our investigation by turning to the problem of assessing whether an outcome
that is stable (under some solution concept) in a TU game remains stable when constraints
are issued. A crucial issue here is to what extent the imputation set is affected by the
constraints imposed over the game. This issue is illustrated next.
Example 4.8. Consider the TU game G = hN, vi where N = {1, 2}, v({1, 2}) = 2, and
v({1}) = v({2}) = 0. It is immediate to check that X(G) = {x  R{1,2} | x1 + x2 = 2  x1 
0  x2  0}. The solution concepts for G are as follows (see Figure 1 for an illustration):
Core. For any imputation x  X(G) and for each coalition S  {1, 2}, it is the case that
x(S)  v(S). Thus, C (G) = X(G).
Bargaining Set. Since C (G)  B(G) (recall Proposition 2.7) and since B(G)  X(G), we
immediately have that B(G) = C (G) = X(G).
Nucleolus. Let x  X(G) be an imputation. Considering the standard excess function
for TU games, we have that either (x) = (0, x2 , x1 ) or (x) = (0, x1 , x2 ),
depending on whether x1  x2 or x2 > x1 . Indeed, just recall that v({1, 2}) =
x({1, 2}) = 2 and v({1}) = v({2}) = 0. Thus, the lexicographically minimum excess
vector is obtained at the imputation x such that x1 = x2 = 1, i.e., N (G) = {x}.
Kernel. Since N (G)  K (G) (recall Proposition 2.7), we have that x  K (G). Consider
now an imputation x  X(G) such that x 6= x. Assume that x1 > 1 (the same line
of reasoning applies to the case where x2 > 1), and thus x2 < 1. For the standard
excess function on TU games, s1,2 (x) = x1 and s2,1 (x) = x2 are the surpluses at x,
653

fiGreco, Malizia, Palopoli, & Scarcello

because v({1}) = v({2}) = 0. Then, s2,1 > s1,2 holds, and in order to have x  K (G),
it should be x1 = v({1}) = 0, which is not the case, because x1 > 1. It follows that
K (G) = {x}.
Shapley Value. Note that the two players of G are symmetric and hence their Shapley
value must be the same. Thus, (G) = (1, 1).
Now, consider the constraints LC = {x1 +x2  1, x1 , x2  R}. Then, it is easily checked that
X(G|LC ) = {x  R{1,2} | x1 + x2 = 1  x1  0  x2  0}, and hence X(G)  X(G|LC ) = . By
applying the same line of reasoning as above (and by considering Kalais excess function in
Equation (2) for the nucleolus and the kernel), we derive that C (G|LC ) = B(G|LC ) = X(G|LC )
and N (G|LC ) = K (G|LC ) = {y}, where y1 = y2 = 12 (see, again, Figure 1).
Moreover, as for the Shapley NTU values of G|LC , note that only vectors  = (1 , 2 )
with 1 = 2 =  > 0 are such that G|LC  is definedfor
every vector
P G|LC . In fact, for 	
 = (1 , 2 ) with 1 6= 2 the value v|LC  (N ) = sup
iN i zi | z  V (N ) is infinite.
Indeed, in pre-imputations players are not necessarily individually rational, and hence in
this game one player may get an unbounded negative value, as long as the other one gets
an unbounded positive value such that their sum is 2. Now, for every  = (, ), the worth
function of the TU game G|LC  is v|LC  ({1}) = v|LC  ({2}) = 0, and v|LC  (N ) = . Since
the two players are symmetric, the Shapley values of this family of games are of the form
(G|LC  ) = ( 2 , 2 ). Therefore, no consequence x  V |LC (N ) different from ( 12 , 12 ) admits a
vector  = (1 , 2 ), with 1 = 2 , such that i xi = (G|LC  )i for both players 1 and 2. We
conclude that the singleton {y} is also the set of all Shapley NTU values of G|LC .
Thus, all the solutions concepts for the constrained game G|LC are completely unrelated
to those of G.

Note that, in the above example, the fact that no solution concept is preserved is not
by chance. Indeed, recall that core, bargaining set, nucleolus, and kernel are defined as
refinements of the set of all possible imputations. Therefore, in the extreme scenario where
X(G)  X(G|LC ) =  holds (for a constrained game G|LC built on top of the TU game G),
none of these solution concepts can be preserved.
Fact 4.9. Let G|LC be a constrained game such that X(G)  X(G|LC ) = . Then,
(1) C (G)  C (G|LC ) = ;
(2) B(G)  B(G|LC ) = ;
(3) N (G)  N (G|LC ) = ; and,
(4) K (G)  K (G|LC ) = .
Moreover, recall that Shapely NTU values are refinements of the set of all possible payoff
distributions associated with the grand-coalition (and, in particular, of the pre-imputations
for TU games). Thus, if (G) 6 VLC (N ) (as in Example 4.8), this solution concept cannot
be preserved. In general, since (G) is a pre-imputation, the following holds.
Fact 4.10. Let G|LC be a constrained game, with G = hN, vi. Assume that VLC (N )  {x 
Vv (N ) | x is efficient } = . Then, (G) 6 (G|LC ).
654

fiMixed-Integer Constrained Coalitional Games

In the light of the above observations, it is however of interest to analyze whether
preservation properties hold with respect to (pre)imputations of both G and G|LC .
As an example, it is of interest to establish the relationship between payoff vectors in
C (G) X(G|LC ) (i.e., vectors that are in the core of the TU gameand thus are imputations
for this gameand that are also imputations for the constrained game) and payoff vectors
in C (G|LC )  X(G) (i.e., vectors that are in the core of the constrained gameand thus are
imputations for this gameand that are also imputations for the TU game). Exploring
these relationships, for each solution concept, is addressed in the rest of this section.
4.2.1 Core
Our first result concerns the core, and shows that imputations that are in the core of the
TU game and satisfy the constraints are also in the core of the resulting constrained game.
Proposition 4.11. Let G|LC be a constrained game. Then, C (G)  X(G|LC )  C (G|LC ) 
X(G).
Proof. Let G = hN, vi be a TU game, and recall from Section 2 that G can be equivalently
viewed as the NTU game hN, Vv i. Assume that x is a payoff vector in C (G), and hence in
X(G). Then, there is no coalition S  N and no vector y  Vv (S) such that yi > xi , i  S.
By Definition 3.1, for the NTU game G|LC = hN, VLC i, it is the case that VLC (S)  Vv (S),
for each S  N . Therefore, there is no coalition S  N and no vector y  VLC (S) such that
yi > xi , i  S. That is, if x  X(G|LC ), then x belongs to C (G|LC ).
However, the above inclusion can be strict in some cases, even if no imputation is affected
by the constraints, i.e., even if X(G) = X(G|LC ).
Proposition 4.12. There exists a constrained game G|LC (with int(LC) = ) such that
X(G) = X(G|LC ), C (G) =  and C (G|LC ) 6= . Thus, C (G)  X(G|LC )  C (G|LC )  X(G).
Proof. Consider the TU game G = hN, vi such that N = {1, 2, 3}, v({1}) = 1, v({2}) = 1,
v({3}) = 2, v({1, 2}) = 3, v({1, 3}) = 0, v({2, 3}) = 0, and v({1, 2, 3}) = 4. Notice that
X(G) = {x} with x1 = 1, x2 = 1, and x3 = 2; and that C (G) = . In particular, for
the latter equality, consider the pair (y, {1, 2}) such that y1 = y2 = 32 . Since y({1, 2}) =
v({1, 2}), y1 > x1 and y2 > x2 , we have that (y, {1, 2}) is an objection to x, which therefore
does not belong to C (G).
Consider now the following set of constraints:

x1 + x2  2
LC =
x1 , x2  R
It is easily seen that x satisfies LC. Thus, x  X(G|LC ) holds by Proposition 4.1. Moreover,
since (LC)[{i}] = R{i} holds for each player i  N , because there is no constraint on
worths of singleton coalitions, the individual rationality constraint on G|LC prescribes that
for each x  X(G|LC ): x1  v({1}) = 1, x2  v({2}) = 1, and x3  v({3}) = 2. Since
v({1, 2, 3}) = 4, x is in fact the only imputation in X(G|LC ). Thus, X(G) = X(G|LC ).
To conclude the proof, let us now observe that, in the constrained game G|LC , there is
no {1, 2}-feasible vector z with z1 > x1 and z2 > x2 ; indeed, just observe that z1 + z2  2
holds because of the constraints, while x1 + x2 = 2. That is, there is no objection to x,
which is therefore an imputation in C (G|LC ).
655

fiGreco, Malizia, Palopoli, & Scarcello

4.2.2 Bargaining Set
As far as the bargaining set is concerned, we can show that there are constrained games
whose bargaining set is completely unrelated with that of the underlying TU games. This
is because objections and counterobjections are not necessarily restricted to the set of
the possible imputations. Thus, constraints may radically alter the feasibility properties
of certain payoff vectors, yet without affecting the imputation set. This is shown in the
following two propositions.
Proposition 4.13. There exists a constrained game G|LC (with int(LC) = ) such that
X(G) = X(G|LC ), and B(G)  X(G|LC ) 6 B(G|LC )  X(G).
Proof. Consider the TU game G = hN, vi such that N = {1, 2, 3, 4, 5}, v({1, 2, 3, 4, 5}) = 8,
v({2, 3, 4}) = 8, v({1, 3, 4}) = 7, v({1, 2}) = 2, v({5}) = 1, and v(S) = 0 for each other
coalition S  N . Consider the imputation x such that x1 = 0, x2 = 1, x3 = 3, x4 = 3, and
x5 = 1. We claim that x  B(G). Indeed, let (y, S) be an objection to x. This objection can
be carried out through three different coalitions, each of them having a counterobjection:
 First, we can have S = {2, 3, 4}, y2 > x2 = 1, y3 > x3 = 3, y4 > x4 = 3, and y2 + y3 +
y4  v({2, 3, 4}) = 8. In this case (y, S) is an objection against player 1 or against
player 5. In the former case, (z, {1}) with z1 = x1 = 0 is a trivial counterobjection to
(y, S); in the latter case, (z, {5}) with z5 = x5 = 1 is a counterobjection to (y, S).
 Second, we can have S = {1, 3, 4}, y1 > x1 = 0, y3 > x3 = 3, y4 > x4 = 3, and
y1 + y3 + y4  v({1, 3, 4}) = 7. In this case, (y, S) is an objection of some player in S
against player 2 or against player 5. As we observed above, (z, {5}) with z5 = x5 = 1
is a trivial counterobjection to any objection against 5. Thus, let us assume that
(y, S) is an objection against player 2. If (y, S) is an objection of player 3 or 4,
we may just consider the pair (z, {1, 2}) with z1 = y1 and z2 = x2 . Indeed, note
that y1 < 1 holds and, thus, z1 + z2 < 1 + x2 = 2 = v({1, 2}). Therefore, z is
{1, 2}-feasible, and (z, {1, 2}) is a counterobjection to (y, S). On the other hand, if
(y, S) is an objection of player 1 against player 2 to x, we may consider the pair
(w, {2, 3, 4}) such that w2 = x2 , w3 = y3 , and w4 = y4 . Note that y3 + y4 < 7 and,
thus, w2 + w3 + w4 < x2 + 7 = 1 + 7 = v({2, 3, 4}). Then, w is {2, 3, 4}-feasible, and
(w, {2, 3, 4}) is a counterobjection to (y, S).
 Finally, we can have S = {1, 2}, y1 > x1 = 0, y2 > x2 = 1, and y1 +y2  v({1, 2}) = 2.
In this case, (y, S) is an objection of some player in S against 3, 4, or 5. Let us consider
the first two cases, since (z, {5}) with z5 = x5 = 1 is a trivial counterobjection to
objections against 5. If (y, S) is an objection of player 1 (against player 3 or 4), we
may consider the pair (z, {2, 3, 4}) such that z2 = y2 , z3 = x3 , and z4 = x4 . Note that
y2 < 2 and, thus, z2 +z3 +z4 < 2+x3 +x4 = 2+6 = v({2, 3, 4}) = 8. Hence, (z, {2, 3, 4})
is a counterobjection to (y, S). If (y, S) is an objection of player 2 (against player 3 or
4), we may consider the pair (w, {1, 3, 4}) such that w1 = y1 , w3 = x3 , and w4 = x4 .
Note that y1 < 1 and, thus, w1 + w3 + w4 < 1 + x3 + x4 = 1 + 6 = v({1, 3, 4}) = 7.
Hence, (w, {1, 3, 4}) is a counterobjection to (y, S).

656

fiMixed-Integer Constrained Coalitional Games

Consider now the following set of constraints:

x2 + x3 + x4  7
LC =
x2 , x3 , x4  R
It is immediate to check that X(G) = X(G|LC ); indeed, the individual rationality over
player 5 forces x5  1; given that v({1, 2, 3, 4, 5}) = 8, the above constraint is therefore
logically implied for all individually rational vectors in VLC (N ). However, LC plays a crucial
role concerning the formation of the coalition {2, 3, 4}. Indeed, consider the objection
(y, {1, 2}) of player 1 against player 3 to x, where y1 = 12 and y2 = 32 . Any counterobjection
(z, T ) to (y, {1, 2}) must be such that T = {2, 3, 4}. Thus, z2  y2 = 32 , z3  x3 = 3, and
z4  x4 = 3 must hold. It follows that z2 + z3 + z4 > 7 and, hence, z 6 VLC (T ). Since (y, S)
is a justified objection, x 6 B(G|LC ).
Proposition 4.14. There exists a constrained game G|LC (with int(LC) = ) such that
X(G) = X(G|LC ), and B(G|LC )  X(G) 6 B(G)  X(G|LC ).
Proof. Consider the TU game G = hN, vi such that N = {1, 2, 3}, v({1}) = v({2}) = 1,
v({3}) = 0, v({1, 3}) = v({2, 3}) = 4, v({1, 2}) = 5, and v({1, 2, 3}) = 3. Consider the
imputation x such that x1 = x2 = x3 = 1. We observe that x 
/ B(G). Indeed, consider
an objection (y, {1, 2}) of player 1 against player 3 such that y1 = 1 + 12 and y2 = 3 + 12
(observe that y({1, 2}) = v({1, 2}). Player 3 cannot counterobject either as a singleton,
since v({3}) < x3 , or through coalition {2, 3}, since for each vector z  R{2,3} such that
z2  y2 > 3 and z3  x1 = 1, we have z({2, 3}) > 4 = v({2, 3}). It follows that x 6 B(G).
Consider now the following set of constraints:

x1 + x2  4
LC =
x1 , x2  R
It is immediate to check that X(G) = X(G|LC ). Moreover, let us notice that no player has
a justified objection against player 1 or 2 to x, since they can counterobject as singletons;
indeed, just observe that v({i}) = 1 = xi , for i  {1, 2}. Consider, then, an objection
(y, {1, 2}) of player 1 against player 3, such that y1  x1 = 1 and y2  x2 = 1. Since, y must
belong to VLC ({1, 2}), we have that y2  3 holds. Thus, the pair (z, {2, 3}) with z2 = 3  y2
and z3 = x3 = 1 is a counterobjection to (y, {1, 2}), because z({2, 3}) = 4 = v({2, 3}). By
the symmetry in the game definition, the same line of reasoning as above applies to show
that also player 2 has no justified objections against player 3. Therefore, x  B(G|LC ).
4.2.3 Nucleolus and Kernel
Let us move to analyze the nucleolus and the kernel. As in the case of the bargaining set,
no preservation property holds, as demonstrated next.
Proposition 4.15. There exists a constrained game G|LC (with int(LC) = ) such that
X(G) = X(G|LC ) 6= , K (G)  K (G|LC ) = , and N (G)  N (G|LC ) =  (for Kalais excess
function in Equation (2) on page 640).

657

fiGreco, Malizia, Palopoli, & Scarcello

Proof. Consider the TU game G = hN, vi such that N = {1, 2, 3}, v({1, 2, 3}) = 3,
v({1, 2}) = 5, v({1, 3}) = 4, v({2, 3}) = 3, and v(S) = 0, for each other coalition S  N .
Consider an imputation x that belongs to K (G), and consider the expressions: s1,3 (x)
s3,1 (x) = (5  x1  x2 )  (3  x2  x3 ) = 2  x1 + x3 and s1,2 (x)  s2,1 (x) = (4  x1  x3 ) 
(3  x2  x3 ) = 1  x1 + x2 . By Definition 2.5, we then get that 2  x1 + x3 > 0 implies
x3 = 0, that 2  x1 + x3 < 0 implies x1 = 0, that 1  x1 + x2 > 0 implies x2 = 0, and that
1  x1 + x2 < 0 implies x1 = 0. By simple algebraic calculations, the above relationships
together with the individual rationality of x (i.e., x1  0, x2  0, and x3  0) entail that
x1  x2 = 1 and x1  x3 = 2 must hold. In turn, since x1 + x2 + x3 = 3, the latter two
equations uniquely determine the value of x. In particular, K (G) is the singleton {x} such
that x1 = 2, x2 = 1 and x3 = 0. Moreover, since N (G)  K (G) and |N (G)| = 1 (see
Proposition 2.7), we have that N (G) = K (G).
Consider now the following set of constraints:

x1 + x2  3



x1 + x3  3
LC =
x + x3  3


 2
x1 , x2 , x3  R

Notice that the above constraints do not modify the imputation set, that is, X(G) =
X(G|LC ) 6= . Moreover, observe that V |LC = Vv , where v  is the worth function of the game
G  = hN, v  i in which v  ({1, 2, 3}) = v  ({1, 2}) = v  ({1, 3}) = v  ({2, 3}) = 3, and v  (S) = 0
for all other coalitions S  N . By this, the excess function eK reported in Equation (2) coincides with the canonical TU excess, and the definitions of kernel NTU and nucleolus NTU
coincide with those for TU games (cf. Kalai, 1975). So the kernel and nucleolus of G|LC are
those of G  . Finally, it is easily checked that K (G  ) is the singleton {x } such that x1 = 1,
x2 = 1 and x3 = 1 (and, thus, K (G  ) = N (G  )). It follows that K (G)  K (G|LC ) =  and
N (G)  N (G|LC ) = .
4.2.4 Shapley Value
Let us conclude our analysis with the Shapley value. Below, we show that this solution concept is preserved whenever the set of all pre-imputations is not modified by the constraints.
Proposition 4.16. Let G|LC be a constrained game. If the sets of pre-imputations of G and
G|LC coincide, then (G|LC ) = {(G)}.

Proof. Let pX(G) = {x  RN | x(N ) = v(N )} be the set of all pre-imputations of the TU
G = hN, vi; in fact, recall that pX(G) contains all the efficient payoff vectors in Vv (N ) = {x 
RN | x(N )  v(N )}. Let pX(G|LC ) = {x  Rn | x(N )  v(N )x  (LC)[N ]x is efficient}
be the set of all pre-imputations of G|LC . Since pX(G) = pX(G|LC ), it must be the case that
(LC)[N ]  Vv (N ) = {x  Rn | x(N )  v(N )}. Therefore, VLC (N ) = Vv (N )  (LC)[N ] =
Vv (N ). For each coalition S and vector y  RS , consider now the vector x  RN with
xi = yi , for each i  S, and xi = (v(N )  y(S))/|N \ S|, for each i  (N \ S). Note that
x(N ) = v(N ) and, hence, x  Vv (N ) and x  (LC)[N ]. Therefore, y belongs to (LC)[S].
Thus, (LC)[S] = RS and, hence, VLC (S) = Vv (S)  (LC)[S] = Vv (S), for each S  N .
Finally, since VLC (N ) = Vv (N ) also holds (and thus VLC (S) = Vv (S), for each S  N ), it
follows that (G|LC ) = {(G)} (see, e.g., McLean, 2002).
658

fiMixed-Integer Constrained Coalitional Games

However, the Shapley value (G) is not preserved in general, even if (G) is an imputation, and if imputation sets are not affected by the constraints.
Proposition 4.17. There exists a constrained game G|LC (with int(LC) = ) such that
(G)  X(G), X(G) = X(G|LC ), and (G) 
/ (G|LC ).
Proof. Consider the TU game G = hN, vi such that N = {1, 2, 3}, v({1, 2, 3}) = 3,
v({1, 2}) = 4, v({1, 3}) = 3, v({2, 3}) = 3, and v(S) = 0, for each other coalition S  N . By
simple calculations, one may compute the Shapley value (G), and notice that (G)1 = 7/6,
(G)2 = 7/6, and (G)3 = 4/6. Thus, (G)  X(G).
Consider now the following set of constraints:

x1 + x2  3
LC =
x1 , x2  R
and notice that they do not modify the imputation set, that is, X(G) = X(G|LC ) 6= .
Indeed, the inequality x1 + x2  3 is logically implied by the worth of the grand-coalition
(which forces x1 + x2 + x3  3) and by the individual rationality of all players (i.e., x1  0,
x2  0, and x3  0). For the sake of completeness, note that the constrained game G
does not fit the hypothesis of Proposition 4.16 since pX(G) 6= pX(G|LC ) (indeed, any payoff
vector x with x1 + x2 > 3 and x(N )  v(N ) = 3 is a pre-imputation for G but not for G|LC ,
because x1 + x2  3 is not satisfied).
Moreover, observe that V |LC = Vv , where v  is the worth function of the game G  =
hN, v  i in which v  ({1, 2, 3}) = v  ({1, 2}) = v  ({1, 3}) = v  ({2, 3}) = 3, and v  (S) = 0 for all
other coalitions S  N . This suffices to conclude that (G|LC ) = {(G  )} (see, e.g., McLean,
2002). However, it is easily checked that (G  ) is such that (G  )1 = (G  )2 = (G  )3 = 1.
Thus, (G) 
/ (G|LC ).

5. Complexity Analysis
In this section, we shall look at the core and the bargaining set for (constrained) coalitional
games from a computational viewpoint. In particular, our aim is to shed light on the impact
of issuing constraints w.r.t. the intrinsic complexity of these notions, and to assess whether
any price has to be paid for the increased expressiveness of constrained gamesfor the sake
of completeness, background notions on complexity theory are reported in the Appendix.
We argue that it is in fact sensible to analyze these computational properties, as this
corresponds to analyzing the feasibility of using such concepts under the thesis of bounded
rationality, that is, that decisions taken by realistic agents cannot involve unbounded resources to support reasoning (Simon, 1972). Moreover, it is worthwhile noting that studying
such matters might hopefully guide the design of effective computation algorithms.
We leave as future work a complexity analysis of the other solution concepts, where
it would be interesting to consider various kinds of Kalais excess functions with different
computational properties.
5.1 Setup and Problems Analyzed
In the analysis that follows, we assume games to be provided in characteristic function form,
i.e., we deal with scenarios where coalition worths are returned by some given function (von
659

fiGreco, Malizia, Palopoli, & Scarcello

Neumann & Morgenstern, 1944). For instance, the games discussed in Example 3.4 and
in Example 3.5 are in characteristic function form. Moreover, by following the general
framework proposed by Bilbao (2000), we assume that the input for any reasoning problem
consists of a constrained game G|LC where the worth function v is given as an oracle. In
particular, we shall consider two types of oracles:
(1) Oracles computable in polynomial time in the size ||G|LC || of the game representation.
5 For instance, the game in Example 3.4 fits this framework, as well as the game in
Example 3.5, provided that the cost function com(S) (of establishing a communication
infrastructure over the agents in S) comes as an oracle computable in polynomial time.
(2) Oracles computable in non-deterministic polynomial time in the size ||G|LC || of the
game representation. For instance, the game in Example 3.5 may fit this setting
in the cautious perspective where we require that, for any coalition S, the value
v(S) = 100  com(S) can actually be obtained in some imputation. That is, if we
add the condition that there is an element x  (LC) such that for each task tj ,
there is a player i  S with ji 6= 0 (in x), i.e., that the task T can actually be
performed by coalition S while conforming with all the costs constraints. Here, we
also require of course that com(S) is computable in non-deterministic polynomial
time. Note that such powerful worth-functions can be used to encode NP-complete
problems reflecting results of complex algorithmic procedures, such as those arising
in allocation, scheduling and routing scenarios, to name a few.
Let us remark that the framework where the worth function is an oracle computable in
polynomial time encompasses all those settings where games are (implicitly) described over
some kind of compact structures, and where simple calculations on such encodings are to
be performed to compute the worth of any given coalitionnoticeable and very influential
settings of this type are the graph and hypergraph games (Deng & Papadimitriou, 1994),
the marginal contribution nets (Ieong & Shoham, 2005), the games in multi-issue domains
(Conitzer & Sandholm, 2004), and the weighted voting games (Elkind & Pasechnik, 2009;
Elkind, Goldberg, Goldberg, & Wooldridge, 2009). Therefore, our membership results will
immediately carry over to the various classes of games cited above, whereas hardness results
are specific to the oracle setting, and do not hold in general for these (sub)settings.
Within the setting discussed above, we shall next focus on checking whether a given
imputation satisfies the conditions needed to be in the core or in the bargaining set. Thus,
given a constrained game G|LC and a vector x, the following problems will be considered:
 Core-Check: Is x in C (G|LC )?
 BargainingSet-Check: Is x in B(G|LC )?
In addition, recall from Section 4 that the core and the bargaining set might be empty
for constrained games. Thus, it is sensible as well to study the following problems:
5. As usual, it is implicitly assumed that the game representation includes the list of players, so that,
for every coalition S, ||S||  ||G|LC ||. Otherwise, one should more formally say, e.g., that oracles are
computable in polynomial time in the combined size of G|LC and S.

660

fiMixed-Integer Constrained Coalitional Games

Problem

Constrained

Constrained
(int(LC)  {xi |i  N })

Constrained
(int(LC) = )

TU

Core-Check
BargainingSet-Check
Core-NonEmptiness
BargainingSet-NonEmptiness

DP -complete
P
2 -complete
P
2 -complete
P
3 -complete

co-NP-complete
P
2 -complete
P
2 -complete
P
3 -complete

co-NP-complete
P
2 -complete
P
2 -complete
P
3 -complete

co-NP-complete
P
2 -complete
co-NP-complete
trivial

Figure 2: Complexity Results for Constrained Games. Hardness results hold even on cohesive games with worth functions given as polynomial-time oracles. Membership
results hold on non-deterministic polynomial-time worth-function oracles, without any assumption on the representation of real numbers.

 Core-NonEmptiness: Is C (G|LC ) 6= ?
 BargainingSet-NonEmptiness: Is B(G|LC ) 6= ?
Overview of the Results. A summary of our results is reported in Figure 2. Note there
that four settings emerge from our analysis: TU games, constrained games without integer
variables (i.e., int(LC) = ), constrained games without auxiliary integer variables (i.e.,
int(LC)  {xi | i  N }), and arbitrary constrained games. In fact, we stress that hardness
results are established without the use of auxiliary real variables, while membership results
(for constrained games) hold even if variables of this kind actually occur. Thus, auxiliary
real variables play no computational role in the setting of constrained games.
Concerning the checking problems, Figure 2 evidences that Core-Check is co-NPhard for TU games, and in co-NP for constrained games where auxiliary integer variables
are not allowedas said above, there is no bound in the membership result on the number
of considered auxiliary real variables. By allowing the use of auxiliary integer variables,
Core-Check becomes DP -hard, and in fact complete for this class. Thus, auxiliary integer
variables cause a slight increase of complexity for this solution concept. On the other hand,
it emerged that the occurrence of real variableseither player variables or auxiliary ones
and of integer player variables is completely immaterial from a computational perspective.
As far as the BargainingSet-Check is concerned, we can deliver the good news that
adding constraints does not alter the complexity w.r.t. the TU case. Indeed, this problem
P
is P
2 -hard for TU games, and it is in 2 whichever constraints are considered.
Concerning the non-emptiness problems, we show instead that constraints may radically
alter the computational properties. Indeed, CoreNonEmptiness raises one level up
in the polynomial hierarchy, from co-NP in absence of constraints (Malizia, Palopoli, &
Scarcello, 2007) to P
2 , while BargainingSet-NonEmptiness is trivial on TU games
(since this concept is always non-empty there), but it becomes P
3 -complete on constrained
games. Interestingly, in both cases, auxiliary and integer variables do not play any role.
Indeed, hardness results are established for the basic case where int(LC) =  (and without
auxiliary variables), while membership results hold for arbitrary constraints.
In the following, all hardness results will be shown to hold in the simplest case of
(deterministic) polynomial-time worth-function oracles. Moreover, membership results will
not assume any a-priori bound on the representation size of real numbers. To this end, some
661

fiGreco, Malizia, Palopoli, & Scarcello

non-trivial technical matters will be faced next, to show that algorithms can safely work
with as few as polynomially many bits, for any solution concept considered in this paper.
5.2 Hardness Results (on Cohesive Games with Polynomial-Time Oracles)
In this section, we shall establish our hardness results. In particular, in order to highlight
the intrinsic difficulty associated with the solution concepts, constructions are reported
over kinds of worth functions that are simple not only from a computational viewpoint,
in that they are given via oracles computable in polynomial time, but also from an algebraic
viewpoint, as they induce cohesive games.
We recall here that a (TU) game is cohesive
if its worth function v is such that, for each
P
partition S of the players in N , v(N )  SS v(S) holds (Osborne & Rubinstein, 1994)a
condition often imposed in order to guarantee that the grand-coalition actually forms. Note
that earlier proofs of complexity results on compactly specified games (see, e.g., Deng &
Papadimitriou, 1994; Greco, Malizia, Palopoli, & Scarcello, 2009b; Ieong & Shoham, 2005)
do generally exploit constructions over games that are not cohesive and, hence, they do not
entail the hardness results stated in this paper. In fact, our results interestingly show that
cohesivity does not simplify reasoning with solution concepts for coalitional games.
In order to establish hardness results, we exploit a number of reductions that refer to
Boolean formulae. Let  be a Boolean formula, and let vars() = {W1 , . . . , Wn } be the set
of Boolean variables occurring in . Recall that a literal is either a Boolean variable Wi or
its negation Wi . The former is called a positive literal, while the latter is called a negative
literal. We denote by vars() = {Wi | Wi  vars()} the set of negative literals for the
variables occurring in . Literals are associated with game players in most proofs. For a
set of players S, define (S) to be the truth assignment where Wi  vars() is true if Wi
occurs in S, and false, otherwise. The fact that (S) satisfies  is denoted by (S) |= .
Moreover, we say that a coalition S  vars()vars() is consistent w.r.t. a set of variables
Y  vars() if, for each Wi  Y , |{Wi , Wi }  S| = 1 holds. In the case where Y = vars(),
we simply say that S is consistent.
We start by demonstrating hardness results for the various membership-checking problems. The first result is the co-NP-hardness of Core-Check, which is established on the
basis of rather standard arguments reported below just for the sake of completeness. In
particular, the reader may find it useful to check that the reduction exploited in the proof is
based on games that are cohesive, which makes it different from earlier complexity results
given in the literature for specific kinds of compactly specified games.
Theorem 5.1. Core-Check is co-NP-hard, even for cohesive TU games with polynomialtime oracles and if the input vector is an imputation.
Proof. Recall that deciding whether a Boolean formula  over the variables X1 , . . . , Xn
is not satisfiable, i.e., deciding whether there exists no truth assignment to the variables
making  true, is a co-NP-complete problem (Johnson, 1990).
Given such a formula , we build in polynomial time the TU game G() = hN, vi, where
N = vars()  {w, e} and where, for each set of players S, v is such that:

 1 if S = N,
v(S) =
1 if e 
/ S  w  S  (S) |= , and

0 otherwise.
662

fiMixed-Integer Constrained Coalitional Games

Consider, now, the vector x where xe = 1 and xp = 0 for each other player p, and note
that x is an imputation. We claim that: x  C (G()) if and only if  is not satisfiable.
() x  C (G()) implies that there is no coalition S and S-feasible payoff vector y with
yi > xi , for each i  S. Consider any coalition/assignment S such that e 
/ S and
w  S, and observe that x(S) = 0. Since x  C (G()), we must have v(S) = 0, which
entails that (S) does not satisfy , by definition of the worth function. Given that
there is a one-to-one correspondence between coalitions S (with e 
/ S and w  S)
and truth assignments for , we conclude that  is not satisfiable.
() If x 
/ C (G()), there must exist a coalition S  N such that x(S) < v(S), which
is only possible if x(S) = 0 and v(S) = 1. By construction of the worth function, it
follows that S  N , e 
/ S, w  S and (S) |= . That is,  is satisfiable.
Finally, observe that the role of player w is to guarantee that the game is cohesive.
Indeed, for any partition S of N , there is at most one set S  S that contains w, and hence
that may get 1 as its coalition worth.
When considering constrained games and arbitrary input vectors (i.e., not necessarily
imputations), Core-Check turns out to be slightly more difficult than in the previous
case. In fact, we stress here that the use of auxiliary integer variables is crucial in order to
establish the result illustrated next.
Theorem 5.2. Core-Check is DP -hard, even for cohesive constrained games with polynomial-time oracles.
Proof. Given a pair of Boolean formulae (,  ), deciding whether  is not satisfiable and 
is satisfiable is a prototypical DP -complete problem (Johnson, 1990). Assume, w.l.o.g., that
 = c1 . . .cm , with ci = ti,1 ti,2 ti,3 , for each i  {1, . . . , m}. That is,  is in conjunctive
normal form and every clause contains exactly three literals. Moreover, let vars( ) =
{Y1 , . . . , Y } and vars() = {X1 , . . . , Xn }, and assume w.l.o.g. that vars()  vars( ) = .
Consider the TU game G() = hN, vi built in the proof of Theorem 5.1, and recall that
N = vars()  {w, e} and that the vector x (where xe = 1 and xp = 0, for each player
p  N with p 6= e) belongs to C (G()) if and only if  is not satisfiable.
Consider then the following set of constraints:

1  TYj  0, j  {1, . . . , }



(ti,1 ) + (ti,2 ) + (ti,3 )  1, i  {1, . . . , m}
LC =
x  R, p  N


 p
TYj  Z, j  {1, . . . , }

where (ti,h ) denotes the expression 1  Tti,h if ti,h is a negative literal, and the expression
Tti,h if ti,h is a positive literal. Note that players in N are actually not constrained in LC.
Therefore, if (LC) = , then (LC)[N ] =  trivially holds (since (LC)[N ] is the restriction
of an empty set over RN ). Otherwise, i.e., if (LC) 6= , then (LC)[N ] = RN and therefore
these constraints are immaterial. Of course, if (LC) = , then there is no imputation
of G()|LC ; otherwise, all the solution concepts for G() are preserved in G()|LC , since
constraints do not play any role in this case.
663

fiGreco, Malizia, Palopoli, & Scarcello

Observe now that, for each j  {1, . . . , }, TYj is constrained over the domain {0, 1}
as to encode the truth value of the Boolean variable Yj . Clearly, LC can be computed in
polynomial time from , and it is immediate to check that (LC) 6=  if and only if  is
satisfiable. It follows that the vector x is in the core of G()|LC if and only if x belongs to
the core of G() (i.e.,  is not satisfiable) and (LC) 6=  (i.e.,  is satisfiable).
We now turn to the study of the bargaining set. Notice that for the class of graph games
(which is an instance of the more general framework we are considering here) completeness
for BargainingSet-Check in P
2 has recently been established by Greco et al. (2009b).
Clearly enough, this result already implies that BargainingSet-Check is P
2 -hard on
TU games with polynomial-time oracles. Below, we show that this hardness result still
holds on games with polynomial-time oracles which are moreover cohesive.
Theorem 5.3. BargainingSet-Check is P
2 -hard, even for cohesive TU games with
polynomial-time oracles and if the input vector is an imputation.
Proof. We show a polynomial-time reduction from the problem of deciding whether a quantified Boolean formula H = Y1 , . . . , Yn Z1 , . . . , Zq  is valid, which is a well-known P
2complete problem (Johnson, 1990). Let Y = {Y1 , . . . , Yn } and Z = {Z1 , . . . , Zq } denote the
sets of universally and existentially quantified variables, respectively.
Based on H, we build a game G(H) = hN, vi, where N = vars()  vars()  {a, a }
and where, for each set of players S, v is such that:

2 if S = N,



1 if |S| = n and S is consistent w.r.t. {Y1 , . . . , Yn },
v(S) =
1 if S is consistent, |{a, a }  S| = 1, and (S) |= ,



0 otherwise.
Let x be the imputation with xa = xa = 1 and xp = 0, for each other player p.
The construction of G(H) and x is defined as to guarantee two basic properties, which are
intuitively illustrated next:

(1) Recall that an objection (y, S) of player i against player j to x is such that i  S,
j
/ S, y(S)  v(S) and yk > xk , for each k  S. Since v(S) > x(S) must hold for any
objection (y, S), it is the case that objections are one-to-one associated with truth
assignments for the variables in Y; indeed, this is to have v(S) = 1 (and x(S) = 0).
Let (Y \ S) be the truth assignment associated with coalition S.
(2) Recall that a counterobjection (z, T ) to an objection (y, S) of player i against player j
to x is such that i 
/ T , j  T , z(T )  v(T ), zk  yk , for each k  T  S, and zk  xk ,
for each k  T \ S. If (y, S) is an objection against a player j 
/ {a, a }, then (z, {j})
with zj = 0 is a trivial counterobjection. On the other hand, counterobjections (z, T )
to objections (y, S) against a or a are necessarily such that T S = , because z(T ) 
1 and xa = xa = 1. In particular, z(T ) = 1 must hold. Thus, these counterobjections
are one-to-one associated with all the possible satisfying truth assignments for the
variables in H, which are moreover obtained as extensions of the assignment (Y \ S).

664

fiMixed-Integer Constrained Coalitional Games

By Definition 2.3, x is in the bargaining set of G(H) if and only if for each objection
(i.e., assignment (Y \ S) to the variables in Y), there is a counterobjection (i.e., satisfying
assignment obtained by extending (Y \ S)). Therefore, the following claim holds, whose
formal proof is reported in the Appendix:
Claim A. x  B(G(H)) if and only if H is valid.
To conclude the proof, note that the game is cohesive. Indeed, for each coalition S
where v(S) = 1, it is the case that |S  {Y1 , . . . , Yn , Y1 , . . . , Yn }| = n. Thus, given any three
coalitions S1 , S2 and S3 with v(S1 ) = v(S2 ) = v(S3 ) = 1, it must be the case that two
of them overlap over some players. Therefore, any partition S of N contains at most two
coalitions getting a worth greater than 0, and the result follows since v(N ) = 2.
In the remainder of the section we prove our hardness results for non-emptiness problems.
We start by showing that adding constraints to the game causes the complexity of the nonemptiness problem for the core to raise one level up in the polynomial hierarchyfrom
co-NP in absence of constraints (Malizia et al., 2007) to P
2 . Note that in the proof
below, integer and auxiliary variables do not play any role.
Theorem 5.4. Core-NonEmptiness is P
2 -hard, even for cohesive constrained games
with polynomial-time oracles, and where integer and auxiliary variables are not allowed.
Proof. Deciding whether a quantified Boolean formula F = X1 , . . . , Xn Y1 , . . . , Yq  is
valid is a well-known P
2 -complete problem (Johnson, 1990).
Based on F , we build in polynomial time the game G(F ) = hN, vi, where N = vars()
vars()  {a} and where, for each set of players S, v is such that:

 3  n if S = N,
v(S) =
n
if S is consistent and (S) 6|= ,

0
otherwise.

In addition, we build in polynomial time a set LC that, for each 1  i  n, contains the
following constraints:

xXi + xXi = 1




x Xi  0



xXi  0
LC =
xa = 2  n





,x  R
x

 Xi Xi
xa  R
First, note that LC forces xXi + xXi = 1, and forces xa to take value 2  n. Thus, since
v(N ) = 3  n, any imputation x for the constrained game G(F )|LC does not distribute any
worth to the players associated with the variables in {Y1 , . . . , Yq }. An imputation x is then
associated with an assignment (x) to the variables in {X1 , . . . , Xn } such that Xi is true
in (x) if and only if xXi < 1note that we are associating 1 with false, here.
To understand the salient features of the reduction, recall now that an objection (y, S)
to an imputation x is such that y  VLC (S) and yk > xk for all k  S. Since y(S) > x(S)
holds, we have only to take care of coalitions S  N such that S is consistent and (S) is
not a satisfying truth assignment. Recall that VLC (S) = {x  RS | x(S)  v(S)}  (LC)[S];
665

fiGreco, Malizia, Palopoli, & Scarcello

thus, for any such objection (y, S) with Xi  S (resp., Xi  S), we have yXi  1 (resp.,
yXi  1). Therefore, S cannot include players in {X1 , X1 , ..., Xn , Xn } getting a worth 1 in
x. It follows that the set of all possible objections (y, S) to any imputation x corresponds to
a superset of all truth assignments (S) which are not satisfying and which are extensions
of (x). This correspondence allows us to establish the following result (whose formal proof
is deferred to the Appendix).
Claim B. C (G(F )|LC ) 6=  if and only if F is valid.
To conclude the proof, we notice that G(F )|LC is cohesive. Indeed, each coalition S with
v(S) = n must be consistent, and thus |S  (vars()  vars())| = n + q. Therefore, given
any three coalitions S1 , S2 and S3 with v(S1 ) = v(S2 ) = v(S3 ) = n, it must be the case
that two of them overlap over some players. It follows that any partition S of N contains
at most two coalitions getting worth n.
The non-emptiness problems for the bargaining set is trivial over TU games, since this
concept is always non-empty there. This is no longer the case over constrained games, where
this problem turns out to be quite difficult. As in the proof of Theorem 5.4, integer and
auxiliary variables play no role in the result shown below.
Theorem 5.5. BargainingSet-NonEmptiness is P
3 -hard,even for cohesive constrained
games with polynomial-time oracles,and where integer and auxiliary variables are not allowed.
Proof. Deciding the validity of the formula P = X1 , . . . , Xm Y1 , . . . , Yn Z1 , . . . , Zq  is a
well-known P
3 -complete problem (Johnson, 1990).
Based on P , we build in polynomial time a game G(P ) = hN, vi, where N = vars() 
vars()  {a, w} and where, for each set of players S, v is such that:


 m + 1 if S = N


1
if w  S  |S| = n + 1




S is consistent w.r.t. {Y1 , . . . , Yn },

v(S) =
1
if S = {Xi , Xi }, for some i


1
if a  S  S \ {a} is consistent 





(S) |= , and


0
otherwise.
We also build in polynomial time a set LC that, for each 1  i  m, contains the following
constraints:

xXi + xXi = 1




x Xi  0



xXi  0
 xa = 1




 xXi , xXi  R

xa  R

First, we observe that, because of the above constraints and of the fact that v(N ) =
m+1, in any imputation of this game all players Yj , 1  j  n, and all players Zr , 1  r  q,
get payoff 0. Moreover, any imputation x for which there is an index i, 1  i  m, such
that xXi > 0 and xXi > 0 cannot belong to the bargaining set of G(P )|LC , for the objection
666

fiMixed-Integer Constrained Coalitional Games

1
(y, {w, Y1 , ..., Yn }) against player a such that yYj = n+1
is justified. Indeed, if (z, T ) were
a counterobjection with a  T , we would have za  xa = 1 (indeed, xa = 1 is prescribed
by LC). Moreover, because of the definition of the worth function, T would be such that
T \ {a} is consistent, i.e., for each i  {1..., m}, |T  {Xi , Xi }| = 1. Assume that Xi  T
(the same line of reasoning applies if Xi  T ). Then, zXi  xXi > 0 must hold and we
would have z(T ) > 1, which is impossible since v(T )  1 and since v(T )  z(T ) holds for
any counterobjection. Thus, the set of imputations x that might possibly belong to the
bargaining set are restricted to those where variables xXi and xXi take distinct values from
the set {0, 1}. As a result, we can associate any such imputation x of the constrained game
G(P )|LC with an assignment (x) to the variables in {X1 , . . . , Xm } such that Xi is true in
(x) if and only if xXi = 0. Note that we are associating 0 with true here.
In fact, in order to show the correctness of the reduction, we may basically follow the
spirit of the proof of Theorem 5.4. For any imputation x (with the properties illustrated
above), the set of the possible objections (y, S) corresponds to the set of all possible truth
assignments (Y \ S) for the variables in Y = {Y1 , ..., Yn }. Objections that might be
possibly justified are then restricted to those against player a, for which counterobjections
correspond to satisfying assignments extending (x) and (Y \ S). Thus, the following can
be shown, whose detailed proof is reported in the Appendix.

Claim C. B(G(P )|LC ) 6=  if and only if P is valid.
Finally, note that the game is cohesive. Indeed, consider any partition S of the players
in N , and a coalition S  S where v(S) = 1. In the case where a  S and S is consistent
w.r.t. vars(), then there cannot exist any other coalition S   S with v(S  ) = 1 and
with S  = {Xi , Xi } for some i. In addition, there can exist at most one further coalition
S   SP
with v(S  ) = 1 (for |S  | = n + 1, w  S  , and S  is consistent w.r.t. {Y1 , . . . , Yn }).
Thus, SS v(S)  2. Similarly,
P if there is no coalition S  S such that a  S and S is
consistent w.r.t. vars(), then SS v(S)  m + 1. Indeed, S might contain the coalitions
{X1 , X1 }, . . . , {Xm , Xm }, plus at most one coalition that is consistent w.r.t. {Y1 , . . . , Yn }
and which gets worth 1. In particular, S cannot contain two coalitions S  and S  consistent
w.r.t. {Y1 , . . . , Yn } and with v(S  ) = v(S  ) = 1, as w should be contained in both.
5.3 Membership Results
We now complete the picture of the complexity arising in the context of constrained games
by proving membership results that, together with the proofs in the previous section, provide
the completeness results reported in Figure 2. In particular, we shall consider the case where
the worth function v is an oracle that can be computed in deterministic polynomial time
in the size ||G|LC || of the constrained game, while deferring a discussion about how these
results can be extended to the case where v is an oracle computable in non-deterministic
polynomial time to Section 5.3.1.
We start our analysis by stating the complexity of checking whether a vector is an
imputation.
Lemma 5.6. Deciding whether a vector is an imputation is in DP for constrained games.
In particular, the problem is in co-NP for constrained games without auxiliary integer
variables, and in P for constrained games without integer variables.
667

fiGreco, Malizia, Palopoli, & Scarcello

Proof. Let G = hN, vi be a TU game and let LC be a set of constraints. Let x be a vector
assigning a payoff value to each player in N . Recall that x is an imputation in X(G|LC )
if: (1) x  VLC (N ) = {x  RN | x(N )  v(N )}  (LC)[N ]; (2) x is efficient; and (3) x is
individually rational.
(1) Checking whether x(N )  v(N ) is feasible in polynomial time. Moreover, checking
whether x  (LC)[N ] is feasible in NP. Indeed, we can consider the set of linear
inequalities LC derived from LC by replacing all player variables by their values according to x. Note that LC is a mixed integer linear program defined over the variables
(if any) in real (LC)  int(LC) \ {xi | i  N }, and that x  (LC)[N ] if and only if LC
is satisfiable. By well-known results on mixed integer linear programming (see, e.g.,
Nemhauser & Wolsey, 1988), LC admits a solution if and only if it admits a solution
that can be represented with polynomially many bits (in the size of LC ). Thus, the
problem can be solved by first guessing in NP a vector x assigning a value to each
variable in real (LC)  int(LC) \ {xi | i  N }, and by subsequently checking whether x
satisfies all the constraints in LC (which is feasible in polynomial time). Of course, if
int(LC)  {xi | i  N }, then LC is a linear program without integer variables. In this
special case, the satisfiability of LC can be checked in P (see, e.g., Papadimitriou &
Steiglitz, 1998).
(2) Recall that x is efficient if for each x  VLC (N ), there is a player i  N such that
xi  xi . Consider the set of linear inequalities LC derived from LC by adding the
|N | + 1 inequalities: x(N )  v(N ), and xi > xi for each i  N . Then, x is efficient if
and only if LC is not satisfiable. This latter task is feasible in co-NP, since LC is a
mixed integer linear program whose satisfiability can be checked in NPsee (1). In
the special case where int(LC) = , LC does not contain integer variables and, hence,
its (un)satisfiability can be checked in polynomial time.
(3) Recall that x is individually rational if for each player i  N , xi  max{ xi | xi 
VLC ({i}) }. Consider the set of linear inequalities LC
i derived from LC by adding the
two inequalities xi  v({i}) and xi > xi . The individual rationality holds if and only
if LC
i is not satisfiable, for each i  N . As in the point (2) above, this task is feasible
in co-NP in general, and in polynomial time whenever int(LC) = .
We can now conclude that deciding whether x is an imputation is the conjunction of
problem (1), which is feasible in NP, and of problems (2) and (3), which are feasible in
co-NP. Thus, the problem is in DP .
In the case where int(LC)  {xi | i  N } holds, (1) is feasible in polynomial time and,
hence, deciding whether x is an imputation is in co-NP.
Finally, if int(LC) = , problems (1), (2), and (3) are feasible in polynomial time.
Let us now consider the membership of Core-Check. This proof is routine and is
reported for the sake of completeness only.
Theorem 5.7. Core-Check is in DP . In particular, it is in co-NP for constrained
games without auxiliary integer variables.

668

fiMixed-Integer Constrained Coalitional Games

Proof. Let x be the input vector for the game G|LC , where G = hN, vi. We have to check
that x satisfies the conditions of the core and that x is indeed an imputation.
Concerning the former task, recall that the complementary problem of deciding whether
x is not in the core amounts to finding a coalition S and a vector x  VLC (S) such that
xi > xi , i  S. Consider the set of linear inequalities LCS derived from LC by adding the
|S| + 1 inequalities x(S)  v(S), and xi > xi i  S. Then, x is not in the core if there is
a coalition S such that LCS is satisfiable. This task can be therefore solved by guessing in
NP a coalition S together with a vector x assigning a value to each variable in LCS , and
by subsequently checking that x does indeed satisfy all the constraints in LCS . It follows
that deciding whether x satisfies the conditions of the core is feasible in co-NP.
Concerning the task of checking whether x is an imputation, we use the results in
Lemma 5.6. Thus, for general games, Core-Check can be solved by the conjunction of a
problem in co-NP and a problem in DP . Of course, this is again a problem feasible in DP .
Moreover, if int(LC)  {xi | i  N } holds, then Core-Check is feasible in co-NP.
Deriving the membership result for BargainingSet-Check on constrained games requires a more sophisticated line of reasoning. We start by recalling that, for TU games, it
is has been shown that BargainingSet-Check is in P
2 (Greco et al., 2009b). In fact,
this result has been established by exploiting a characterization for the bargaining set that
does not hold in the presence of constraints. Below, by exploiting a completely different
proof technique, we shall show that, surprisingly, the presence of the constraints does not
alter the computational properties of this problem.
For the following proofs, we recall that given a set LC of linear (in)equalities over n
real variables, the set (LC) is a polyhedron in Rn , whose faces are given by the halfspaces
associated with the (in)equalities in LC, and whose vertices are given by the intersection of n
inequalities from LC, and hence can be represented with polynomially many bits in the size
of LC (see, e.g., Papadimitriou & Steiglitz, 1998; Nemhauser & Wolsey, 1988). A bounded
polyhedron is called a polytope. Moreover, we use the following notation. Let S  N be a
set of players and let yS be the set of variables {yk | k  S}. We denote by LCyS the copy
of the system of mixed-integer linear inequalities LC where every player variable xi , with
i  S, is renamed as yi , and every other variable v in LC is renamed as vyS .
Lemma 5.8. Let G = hN, vi be a TU game, LC be a set of constraints, and x be an
imputation for G|LC that does not belong to B(G|LC ). Then, there exists a justified objection
to x that is representable with polynomially many bits.
/ S,
Proof. Since x 
/ B(G|LC ), there are two players i and j, a coalition S with i  S and j 
and an S-feasible vector y such that (y, S) is a justified objection of i against j to x. Let
LCi,j,S be the system consisting of the (in)equalities in LCyS plus the |S| + 1 inequalities:
y(S)  v(S) and yk > xk , k  S. Then, the set (LCi,j,S )[yS ] consists of all the S-feasible
vectors y such that (y, S) is an objection of i against j to x.
Let us now consider possible candidate counterobjections. For any T  N with j  T
and i 
/ T , let LCi,j,S,T be the system including the (in)equalities in LCyS and in LCzT , plus
the inequalities y(S)  v(S), yk > xk , k  S, z(T )  v(T ), zk  yk , k  T  S, and
zk  xk , k  T \ S. Note that (LCi,j,S,T )[yS ] contains all vectors y over the index set S
such that there exists a counterobjection through T , and hence of the form (z, T ), to the
669

fiGreco, Malizia, Palopoli, & Scarcello

Figure 3: Illustration of Claim D, with coalitions T1 , T2 , and T3 .
objection (y, S) of i against j to x. It follows that the set of all vectors y such that (y, S)
is a justified objection of i against j to x is the set:
[
(LCi,j,S )[yS ] \
(LCi,j,S,T )[yS ].
T |iT
/ jT

To conclude the proof we claim the following.
S
i,j,S,T )[y ] contains a point (i.e., a justified obClaim D. (LCi,j,S )[yS ] \ T |iT
S
/ jT (LC
jection to x) that can be represented with polynomially many bits.
To prove the claim, let us consider the following geometrical arguments: Consider first
the case where LCi,j,S and LCi,j,S,T (for each T | i 
/ T  j ST ) contain no integer variables,
i,j,S,T )[y ]. The
and let P be a maximal convex subset of (LCi,j,S )[yS ] \ T |iT
S
/ jT (LC
S
vertices of P , which are points of R , are given by the intersection of at most |S| (independent) halfspaces that are facets of (LCi,j,S )[yS ] or of some (LCi,j,S,T )[yS ], and thus
they can be represented with polynomially many bits. In fact, P might not contain its own
boundaries. Thus, if some of its vertices actually belongs to P , then the result straightforwardly holds. On the other hand, if P is a possibly open segment with endpoints y a and y b
(representable with polynomially many bits), then the middle point ym necessarily belongs
to P (since P is convex) and can be represented with polynomially many bits. Finally, if
the polytope P has more than two vertices (as shown in Figure 3), then it must have at
least three vertices y a , y b , and y c that do not belong to the same face of P . Therefore,
the barycenter y of the triangle with vertices y a , y b , and y c belongs to P , and it can be
represented with polynomially many bits, because this is the case for y a , y b , and y c .
670

fiMixed-Integer Constrained Coalitional Games

To conclude the proof, we observe that integer variables in LCi,j,S and LCi,j,S,T (for
each T | i 
/ T  j  T ) can be easily preprocessed. Roughlythe technical details are
reported in the Appendix, since (LCi,j,S )[yS ] is a polytope by construction of LCi,j,S and
since, therefore, its vertices can be represented with polynomially many bits, all integer
components of interest (basically those falling within (LCi,j,S )[yS ]) can be represented
with polynomially many bits, as well. Thus, to find a point with polynomially many bits
as asked for in Claim D, we can iterate over all the possible combinations
of integer values
S
i,j,S
i,j,S,T )[y ] by
and, at each step, evaluate the expression (LC
)[yS ] \ T |iT
S
/ jT (LC
replacing all integer values with the combination of values at hand. Of course, the resulting
expression does not involve integer variables, all the inequalities in it are still representable
with polynomially many bits, and therefore the above line of reasoning applies.
Armed with the above lemma, we can state the complexity of BargainingSet-Check.
Theorem 5.9. BargainingSet-Check is in P
2.
Proof. We show that the complementary problem of deciding whether a vector x is not in
the bargaining set of a given constrained game G|LC is in P
2 . We start by checking whether
x is an imputation in DP (cf. Lemma 5.6)recall that DP is contained in P
2 . If this is
the case, by Lemma 5.8 we guess in non-deterministic polynomial time a justified objection
to x, that is, a coalition S, two players i  S and j 
/ S, and a vector y such that (y, S)
is an objection of i against j to x. Consider the system LC of the (in)equalities obtained
from LCi,j,S (recall its definition in the proof of Lemma 5.8) by replacing player variables
associated with the coalition S by the respective values in y. Of course, (y, S) is an objection
if and only if LC is satisfiable. By well-known results on mixed integer linear programming
(see, e.g., Nemhauser & Wolsey, 1988), LC admits a solution if and only if it admits a
solution that can be represented with polynomially many bits. Therefore, within the same
non-deterministic step, we can also guess an assignment (call it w ) to all the variables in
LC , and then check in polynomial time that w actually satisfies all the constraints (i.e.,
that y is actually an objection).
To conclude the algorithm for solving BargainingSet-Check, we now have to check
that there is no counterobjection (z, T ) to the objection (y, S) of i against j to x. This task
requires a co-NP oracle call. In particular, the oracle works by checking the complementary
condition in NP. To this end, in a non-deterministic step, it first guesses a coalition T with
j  T and i 
/ T . Consider now the system LC including the (in)equalities in LCzT , plus
the inequalities z(T )  v(T ), zk  yk , k  T  S, and zk  xk , k  T \ S. Then, there
is a counterobjection (z, T ) to (y, S) for some vector z if and only if LC is satisfiable. As
in the case above, a solution to LC is guaranteed to exist which can be represented with
polynomially many bits, so that this solution (call it w ) can be guessed within the same
non-deterministic step by the oracle. In fact, to check that w actually satisfies LC is
trivially feasible in polynomial time.
We now turn to analyze non-emptiness problems. We start with the non-emptiness
of the core, which is a co-NP-complete problem for TU games (Malizia et al., 2007).
Constraints do play a role here, since we have shown that Core-NonEmptiness is P
2hard (cf. Theorem 5.4). Below, we confirm that this is the exact complexity of this problem.

671

fiGreco, Malizia, Palopoli, & Scarcello

Theorem 5.10. Core-NonEmptiness is in P
2.
Proof. Let us again adopt the notation used in the proof of Lemma 5.8. Let S  N be a
coalition, and LCS be the set of mixed-integer linear (in)equalities including the (in)equalities
in LCxN and LCyS , plus the inequalities y(S)  v(S) and yi > xi , for each i  S. We get:
[
C (G|LC ) = X(G|LC ) \
(LCS )[xN ].
SN

Let LCX be the set of the (in)equalities in LCxN plus the inequality x(N )  v(N ). Moreover,
for each player i, let LCi be the set of the (in)equalities LCxN and LCy , plus the inequalities
{i}

x(N )  v(N ) and xi < yi  v({i}). Then, the set (LCi )[xN ] consists of all the vectors
that are not individually rational (w.r.t. player i). Thus,

C (G|LC ) =

X

(LC )[xN ] \

[

i

!

(LC )[xN ]

iN

\

[

(LCS )[xN ].

SN

In particular, note that the efficiency condition on imputations is guaranteed. Indeed,
the points that are not efficient are removed, because they belong to the set (LCN )[xN ],
which is considered above for S = N .
By applying the same line of reasoning as in Claim D (in Lemma 5.8) on the above
expressions, we have that, if C (G|LC ) is not empty, then it contains an imputation that is
representable with polynomially many bits. Thus, we can decide the non-emptiness of the
core by first guessing in NP a vector x. Then, we may call a DP oracle (corresponding
to the invocation of an NP and a co-NP oracle) to check that x is an imputation (cf.
Lemma 5.6), and finally we can verify that x is in the core with a further call to a co-NP
oracle. In particular, this latter oracle works by checking the complementary condition in
NP, i.e., it checks whether x is not in the core. To this end, the oracle guesses in a nondeterministic step a coalition S. Consider now the system LC formed by the (in)equalities
in LCyS plus the |S| + 1 inequalities: y(S)  v(S) and yk > xk , k  S. Then, there is an
objection (y, S) to x for some vector y if and only if LC is satisfiable. Again, LC admits a
solution if and only if it admits a solution that can be represented with polynomially many
bits. Therefore, within the same non-deterministic step, we can also guess an assignment
(call it w ) to all the variables in LC , and then check in polynomial time that w actually
satisfies all the constraints.
Now, we can complete our picture with the bargaining set.
Theorem 5.11. BargainingSet-NonEmptiness is in P
3.
Proof. Consider again the setting in the proof of Lemma 5.8. Let i and j be two players
in N . For any coalition S with i  S and j 6 S, let LCi,j,S be the system consisting of the
(in)equalities in LCxN and in LCyS plus the |S| + 1 inequalities: y(S)  v(S) and yk > xk ,
k  S. Moreover, for each pair of sets of players S and T with i  S \ T and j  T \ S,
let LCi,j,S,T be the system of mixed integer inequalities including the inequalities of the
systems LCxN , LCyS , and LCzT , plus the inequalities y(S)  v(S), yk > xk for each k  S,
672

fiMixed-Integer Constrained Coalitional Games

z(T )  v(T ), zk  yk for each k  T  S, and zk  xk for each k  T \ S. That is, we
proceed in the same way as in the proof of Lemma 5.8, but here the components of vector
x are variables of this linear program, while in the previous lemma they were fixed values.
Observe now that (LCi,j,S,T )[xN  yS ] contains all pairs hx, yi such that there exists a
counterobjection (z, T ) to the objection (y, S) of i against j to x, and that (LCi,j,S )[xN yS ]
consists of all pairs hx, yi such that y is an S-feasible vector with i  S and j 6 S such that
(y, S) is an objection to x. Then, the set
[
(i, j, S) = (LCi,j,S )[xN  yS ] \
(LCi,j,S,T )[xN  yS ]
T |iT
/ jT

is the set of all pairs hx, yi such that (y, S) is a justified objection of i against j to x.
Therefore,
[
B(G|LC ) = X(G|LC ) \
(i, j, S)[xN ],
SN iSj S
/

where, by considering efficiency and individual rationality (see the notation in the proof of
Theorem 5.10), we have
 [
(LCi )[xN ].
X(G|LC ) = (LCX )[xN ] \ (LCN )[xN ] \
iN

By slightly adapting the proof of Claim D (in Lemma 5.8), one may show that if the
bargaining set is not empty, there exists a vector x  B(G|LC ) which can be represented
with polynomially many bits. Therefore, BargainingSet-NonEmptiness can be solved
by first guessing in non-deterministic polynomial time such a vector x. Then, we may call
a DP oracle to check that x is an imputation (cf. Lemma 5.6), and finally we can verify
that x is indeed in the bargaining set with a further call to a P
2 oracle, in order to solve
BargainingSet-Check on input x (cf. Theorem 5.9).
5.3.1 Extension to More General Worth Functions
In all the membership results above, we have assumed that worth functions are polynomial-time computable and, within this setting, we have shown that various hardness results
are indeed tight. Thus, the reader might be inclined to believe that, by considering more
powerful worth functions, the complexity for these problems may consistently increase. Surprisingly, this is not the case. Indeed, we can show that nothing has to be paid if more
powerful worth functions that encode NP-complete problems are considered.
To this end, let vG|LC denote the worth function of a game G|LC , and define the worthfunction graph for any class C of constrained games as the set of tuples WC = {h(G|LC , S), wi |
G|LC  C  vG|LC (S) = w}. Recall, e.g., from the work by Johnson (1990), that such a
function is computable in non-deterministic polynomial time if there is an integer k such
that WC is (i) k-balanced, i.e., ||w||  (||G|LC || + ||S||)k , and (ii) k-decidable, i.e., there
is a non-deterministic Turing machine that decides whether a given tuple t belongs to WC
in O(||t||k ) time. More precisely, since vG|LC is a partial (standard) single-valued function
(multi-valued functions are also considered in the literature), the class of functions that we
consider is called NPSV (see, e.g., Selman, 1994).
673

fiGreco, Malizia, Palopoli, & Scarcello

The complexity of various solution concepts for TU games within a setting where worth
functions are given as oracles computable in NPSV has been analyzed in an extended
version of the work by Greco et al. (2009b). There, it emerged that all the membership
results in Figure 2 hold on any class C of games having such worth functions. Very roughly,
the basic observation is that if we consider NPSV worth-functions, any non-deterministic
algorithm M that guesses in polynomial-time some coalition S for a game G|LC  C, can
at the same time (with just a polynomial-time delay) guess a worth w and an additional
string c (of polynomial size w.r.t. ||G|LC || + ||S||), which acts as a certificate to decide
whether the tuple h(G|LC , S), wi belongs to the NP set WC . Thus, the complexity of any
(non-deterministic) algorithm that uses the value vG|LC (S) after a guess of the coalition S is
not affected by replacing a polynomial-time worth-functions with NPSV worth-functions.
By exploiting this line of reasoning, it is easy to adapt proofs of membership results in order
to deal with such more general worth functions.
Theorem 5.12. The membership results in Figure 2 hold on any class C of games whose
worth functions are in NPSV.

6. Discussion and Conclusion
Imposing linear constraints on the outcomes of games is an approach that has been explored
by several authors in the context of non-cooperative strategic games (e.g., Charnes, 1953;
Semple, 1997; Ryan, 1998). However, in the context of cooperative games this approach
has received considerably less attention and, indeed, no general framework was proposed in
the literature and no analysis of its properties was conducted so far.
In this paper, we have faced the issue by conducting a systematic study of constrained
games within a framework where constraints are defined as mixed-integer linear (in)equalities imposed over an underlying TU game. Seemingly close to the class of constrained
games is the class of linear programming games (see, e.g., Owen, 1975), where the worth
v(S) of a coalition S is implicitly given as a linear program (e.g., as the maximum of a
given objective function over a feasible region (LC) defined in terms of a set of linear
(in)equalities LC). Of course, this approach differs from the setting of constrained games
where the role of LC is, instead, to govern the distribution of the worths within an NTU
perspective. Moreover, differently from classical NTU formalizations, constrained games
allow to define non-convex and non-comprehensive sets of worth distributions, which is an
appealing modeling capability that emerged to be useful in several application domains.
Finally, the resulting game framework has been analyzed with respect to the preservation
and the computational properties of some relevant solution concepts.
It is worthwhile noticing that the framework we have discussed in this paper shares the
spirit of the recent arguments by Shoham (2008), who advocated the use of a broader vocabulary than the fairly terse one characterizing the early foundations of the game theory. Also
relevant are those proposals that reconsidered basic concepts of cooperative games in the
light of a modeling perspective that is closer to the requirements of computer science applications: seminal and influential directions of this type give rise, in particular, to coalitional
skill games (Bachrach & Rosenschein, 2008), qualitative coalitional games (Wooldridge &
Dunne, 2004), coalitional resource games (Wooldridge & Dunne, 2006), Bayesian coalitional
games (Ieong & Shoham, 2008), multi-attribute coalitional games (Ieong & Shoham, 2006),
674

fiMixed-Integer Constrained Coalitional Games

temporal qualitative coalitional games (Agotnes, van der Hoek, & Wooldridge, 2006), and
cooperative Boolean games (Dunne, van der Hoek, Kraus, & Wooldridge, 2008). In the
light of the above approaches, an interesting avenue of further research may be to consider
more expressive kinds of constraints, formulated for instance via logic-based languages, and
where preference criteria can be adopted in place of hard constraints.
Other avenues of research are related to some technical questions that were not explored
in the paper. First, our complexity analysis focused on the notions of the core and the
bargaining set, which are founded on the concepts of objections and counterobjections. Of
course, it would be interesting to complement our results with the analysis of the kernel, the
nucleolus, and the Shapely value. Actually, hardness results for the kernel and the nucleolus
of TU (graphical) games have recently been illustrated by Greco et al. (2009b), and indeed
they trivially provide lower bounds for the complexity of such solution concepts in the
setting of constrained games. However, providing tighter computational bounds requires
a deeper understanding of the computational aspects underlying Kalais axiomatization,
which is outside the scope of this paper. Furthermore, for the Shapley value, it is of interest
to study other extensions that have been provided in the literature for NTU games and to
assess their behavior when applied to constrained games.
Moreover, our hardness results have been shown to hold even by restricting the underlying TU games to use cohesive worth functions only. It might be of interest to study the
complexity when different specific kinds of functions are considered (for instance, monotone,
superadditive, weakly superadditive, or convex ones6 ). Similarly, assessing to which extent
considering such specific kinds of worth functions affects the analytical properties studied
in Section 4 is an interesting question which we leave for further research.
Finally, from a modeling viewpoint, we recall that the framework proposed in the paper
exploits one set of linear (in)equalities to constrain the outcomes of all coalitions. Thus, in
the light of adding modeling power to this framework, it might be of interest to study a natural generalization where each coalition is equipped with its specific set of linear (in)equalities.
In particular, this setting would call for conceiving suitable mechanisms to compactly represent (exponentially many) different sets of constraints, and for defining formal measures
for the expressivity of such compact representations for constraint-based NTU games.
Acknowledgments
A coalitional game framework for dealing with linear constraints imposed on TU games
was first illustrated by the authors in an extended abstract published in the proceedings of
the 8th International Conference on Autonomous Agents and Multiagent Systems (Greco,
Malizia, Palopoli, & Scarcello, 2009a). There, some solution concepts have been defined
and studied, which were based on proposing ad-hoc adaptations of solution concepts for
TU games. Following the suggestions of the anonymous referees, the constrained game
framework proposed in the present paper fits instead the framework of NTU games, in its
most general form. Thus, the solution concepts studied in this paper are just given as
suitable specializations of the standard solution concepts defined for NTU games.
6. The worth function v : 2N 7 R is monotone if v(S)  v(T ) holds, for each pair of coalitions S, T  N
such that S  T ; v is superadditive if v(S  T )  v(S) + v(T ) holds, for each pair of coalitions S, T  N
such that S  T = ; v is weakly superadditive if v(S  {i})  v(S) + v({i}), S  N and i  N \ S; v
is convex if v(S  T ) + v(S  T )  v(S) + v(T ), S, T  N (see, e.g., Peleg & Sudholter, 2007).

675

fiGreco, Malizia, Palopoli, & Scarcello

Appendix A. Computational Complexity
In this appendix we recall some basic definitions of complexity theory, by referring the
reader to the work by Johnson (1990) for more on this topic.
A.1 The Complexity of Decision Problems: P, NP, and co-NP
Decision problems are maps from strings (encoding the input instance over a fixed alphabet,
e.g., the binary alphabet {0, 1}) to the set {yes, no}. The class P is the set of decision
problems that can be solved by a deterministic Turing machine in polynomial time with
respect to the input size, that is, with respect to the length of the string that encodes the
input instance. For a given input x, its size is usually denoted by ||x||.
Throughout the paper, we often refer to computations carried out by non-deterministic
Turing machines, too. Recall that these are Turing machines that, at some points of the
computation, may not have one single next action to perform, but a choice between several
possible next actions. A non-deterministic Turing machine answers a decision problem if
on any given input x: (i ) there is at least one sequence of choices leading to halt in an
accepting state if x is a yes instance; and (ii ) all possible sequences of choices lead to
some rejecting state if x is a no instance.
The class of decision problems that can be solved by non-deterministic Turing machines
in polynomial time is denoted by NP. Problems in NP enjoy a remarkable property: any
yes instance x has a certificate of its being a yes instance, which has polynomial length
and which can be checked in polynomial time (in the size ||x||). As an example, the problem
of deciding whether a Boolean formula  over the variables X1 , . . . , Xn is satisfiable, i.e.,
deciding whether there exists some truth assignment to the variables making  true, is a
well-known problem in NP; in fact, any satisfying truth assignment for  is obviously a
certificate that  is a yes instance, i.e., that  is satisfiable.
The class of problems whose complementary problems are in NP is denoted by co-NP.
As an example, the problem of deciding whether a Boolean formula  is not satisfiable is
in co-NP. Of course, the class P is contained both in NP and co-NP.
The class DP is the class of problems that can be defined as a conjunction of two
problems, one from NP and one from co-NP, respectively. For instance, it is in DP to
decide whether, for a given pair of Boolean formulae (,  ),  is satisfiable and  is not.
A.2 Further Complexity Classes: The Polynomial Hierarchy
Throughout the paper, we also refer to a particular type of computation called computation
with oracles. Intuitively, oracles are subroutines that have unary cost.
P
P
The classes P
k , k , and k , forming the polynomial hierarchy, are defined as follows:
P
P
P
P
k1 , P = Pk1 , and P = co-P where
P
0 = 0 = P and for all k  1, k = NP
k
k
k
co-P
denotes
the
class
of
problems
whose
complementary
problem
is
solvable
in P
k
k.
P
P
Here, k (resp., k ) models computability by a non-deterministic (resp., deterministic)
P
polynomial-time Turing machine that may use an oracle in P
k1 . Note that 1 coincides
with NP, and that P
1 coincides with co-NP.
A well-known problem at the k-th level of the polynomial hierarchy is deciding the
validity of a quantified Boolean formula with k quantifier alternations. A quantified Boolean
676

fiMixed-Integer Constrained Coalitional Games

formula (short: QBF) with k quantifier alternations has the form Q1 X1 Q2 X2 ...Qk Xk ,
where k  1, Xi (1  i  k) is a set of variables, Qi  {, } (1
Sk  i  k), Qi 6= Qi+1
(1  i < k), and  is Boolean formula over the variables in i=1 Xi . The set of all
quantified Boolean formulae with k quantifier alternations and Q1 =  (resp., Q1 = ) is
denoted by QBFk, (resp., QBFk, ). Deciding the validity of a quantified Boolean formula
P
in QBFk, (resp., QBFk, ) is a well-known problem in P
k (resp., k ). Note that for
k = 1, this problem coincides with the problem of deciding whether the Boolean formula 
is satisfiable (resp., not satisfiable), which is indeed in NP (resp., co-NP).
A.3 Reductions Among Decision Problems
A decision problem A1 is polynomially reducible to a decision problem A2 , denoted by
A1 p A2 , if there is a polynomial time computable function h such that, for every x, h(x)
is defined and x is a yes instance of A1 if and only if h(x) is a yes instance of A2 . A
decision problem A is complete for the class C of the polynomial hierarchy (beyond P) if A
belongs to C and every problem in C is polynomially reducible to A. Thus, problems that
are complete for the class C are the most difficult of the problems in C.
It is worthwhile observing that the problems discussed in this section are known to be
complete for the classes in which the membership has been pointed out. In particular,
deciding the validity of a QBFk, (resp., QBFk, ) formula is the prototypical P
k -complete
(resp., P
-complete)
problem.
k

Appendix B. Proofs in Section 3
Proposition 3.2 Let G = hN, vi be a TU game and let X  X(G) be an arbitrary finite
set of imputations. Then, there is a finite set of constraints LC such that X(G|LC ) = X .
Proof. Consider the game G = hN, vi, the set X = {x1 , . . . , xk } of imputations of G, and
the set of constraints:

xi = x1i  y 1 +    + xki  y k , 1  i  |N |




 0  yj  1, 1  j  k
LC =
y1 +    + yk = 1


x , . . . , xn  R


 11
y , . . . , yk  Z
where x1i , . . . , xki (for 1  i  |N |) are constants.
It is immediate to check that VLC (N ) = X . Moreover, we observe that each vector
xj  VLC (N ) (1  j  k) is efficient, since it cannot be dominated by any other vector in
X (just notice that X is a set of imputations for G). Finally, notice that each imputation
x  X is individually rational w.r.t. G. Thus, by Proposition 3.7, x is individually rational
w.r.t. the constrained game too. It follows that VLC (N ) = X(G|LC ).
Proposition 3.3 There exists a class C = {G|nLC }n>0 of constrained games such that each
game G|nLC is over n + 1 players, LC consists of 2  n + 1 inequalities, and |X(G|LC )| = 2n .

677

fiGreco, Malizia, Palopoli, & Scarcello

Proof. Consider the class C = {G|nLC }n>0 where the game G n = hN, vi is such that N =
{1, . . . , n, n + 1}, v(N ) = n, v(S)
Pn= 0, for each coalition S  N , and LC = {0n xi n1, xi 
Z, 1  i  n}  {xn+1  n  i=1 xi }. It can be easily checked that |X(G|LC )| = 2 .

Proposition 3.7 Let G = hN, vi be a TU game and let x be a payoff vector that is individually rational w.r.t. G (i.e., xi  v({i}), for each player i  N ). Then, for each set LC of
constraints, x is individually rational w.r.t. the constrained game G|LC .
Proof. Let x be a payoff vector such that xi  v({i}), for each player i  N . Consider
the constrained game G|LC and a player i  N . If VLC ({i}) = Vv ({i})  (LC)[{i}] = ,
we trivially have that xi > . Otherwise, i.e., if VLC ({i}) 6= , we can just notice that
max{ yi | yi  VLC ({i}) }  v({i}). Thus, we have that xi  max{ yi | yi  VLC ({i}) }.

Appendix C. Proofs of Claims in Section 5
Claim A. x  B(G(H)) if and only if H is valid.
Proof. Let us study the structure of a possible objection to x. Recall that (y, S) is an
objection of player i against player j to x if and only if: i  S, j 
/ S, y(S)  v(S) and
yk > xk , for each k  S. Thus, we observe that v(S) = 1 must hold, in order to improve the
payoffs of its members, and because a worth value equal to 2 can only be obtained by the
grand-coalition. In addition, since xa = xa = 1 and since y(S)  v(S) = 1, it must also be
the case that {a, a }  S = , because these players get 1 in the current imputation x. Due
to the definition of the worth function, this entails that S is consistent over the variables Y,
i.e., it is a set of n players corresponding to literals over the universally quantified variables.
We thus associate with each possible objection (y, S) to x (where y(S)  1 and yk > 0,
k  S) a truth-value assignment to the variables in Y such that, 1  i  n, Yi is assigned
false if Yi  S (and true if Yi  S). According to our notation, this means that such an
objection is associated with the truth-value assignment (Y \ S). But we can define the
converse, as well. For any truth-value assignment Y for the universally quantified variables,
its associated objection is the pair (y, S) such that S = {Yk  Y | Y (Yk ) = false}  {Yk |
1
Yk  Y, Y (Yk ) = true}, and yp = |S|
, for each p  S. Note that (y, S) is an objection of

any player in S against player a or a to x, and that (Y \ S) = Y .
Indeed, if (y, S) is an objection against a player j 
/ {a, a }, then (z, {j}) with zj = 0
is a trivial counterobjection, since v({j}) = xj = 0 and j 
/ S. It follows that the set of
objections that are possibly justified has to be restricted to those against player a or player
a . Next, we consider the case of an objection against a, but exactly the same arguments
hold for objections against a . Let (y, S) be an objection of i against player a to x. Any
counterobjection (z, T ) must now be with a  T (and i 
/ T ). Thus, in order to have
za  xa = 1, it must be the case that v(T )  1 and, actually, that v(T ) = 1, due to the
definition of the worth function. In particular, the latter (with the fact that za = 1) entails
that, for each player p 6= a with p  T , it holds that zp = 0. Thus, T  S must be empty,
because all members of S get something according to y, T \{a} is consistent, and (T ) |= .
In particular, since T  S = , according to this satisfying assignment, Yi is true in (T )
if and only if Yi 
/ S. It follows that (T ) coincides with (Y \ S) over the universally
quantified variables, and thus it is in fact an extension of this truth-value assignment to
678

fiMixed-Integer Constrained Coalitional Games

the set of all variables occurring in the formula . Conversely, note that every satisfying
assignment for  that extends (Y \ S) corresponds to such a counterobjection to (y, S).
Given the above observations, we show the claim: x  B(G(H))  H is valid.
() Assume that x  B(G(H)). Let Y be any truth-value assignment for the universally
quantified variables, and let (y, S) be the objection to x associated with Y . In
particular, (Y \ S) = Y , by construction. Since x  B(G(H)), there exists a valid
counterobjection (z, T ) to (y, S), and we have seen above that its corresponding truthvalue assignment (T ) is an extension of (Y \ S) to the set of all variables vars(),
and that (T ) |= . It follows that H is valid.
() Assume that x 
/ B(G(H)). Then, there is a justified objection (y, S) against a (or
a ) to x. It follows from the discussion above that there is no truth-value assignment
that is able to extend the assignment (Y \ S) to all the variables vars(), and to
satisfy . Indeed, such an extension would be associated with a counterobjection to
(y, S). It follows that H is not valid.
Claim B. C (G(F )|LC ) 6=  if and only if F is valid.
Proof.
() Assume that x  C (G(F )|LC ), i.e., that S  N , there is no S-feasible payoff vector
such that yi > xi for each i  S. We claim that (x) is a truth assignment to the
variables in {X1 , . . . , Xn } witnessing the validity of F . Indeed, assume, for the sake
of contradiction, that there is a truth assignment Y to the variables in {Y1 , . . . , Yq }
such that (x)  Y 6|= . Consider the coalition S such that S is consistent and
(S) = (x)  Y . By definition of the worth function, v(S) = n. Moreover, observe
that x(S) < n holds, by definition of the assignment (x) and given that xYj = 0 and
xYj = 0, for each imputation x and variable Yj (1  j  q). Note, in fact, that if
Xi  S then Xi is true in (x) and then xXi < 1, and if Xi  S then Xi is false in
miniS (1xi )
(x) and then xXi = 0 (because in this case xXi = 1 holds). Now, let  =
n+q
and notice that  > 0, since xi < 1 is, in particular, prescribed by the definition of
(x). Consider the vector y  RS such that yi = xi +  for each i  S. Note that
y(S)  n because |S| = n + q; moreover, y  VLC (S) holds because v(S) = n and
because constraints are satisfied at y (just notice that S contains exactly one player
in {Xi , Xi }, for each variable Xi , which is associated with a payoff less than or equal
to 1 in y and that constraints xXi + xXi = 1 do not play any role here because only
one player Xi or Xi is in S and hence given a variable yXi (resp., yXi ) with yXi  1
(resp., yXi  1) always exists a non negative value for yXi (resp., yXi ) such that
yXi + yXi = 1). Since yi > xi , for each i  S, we then conclude that x 
/ C (G(F )|LC ),
which is impossible.
() Assume that there is a truth assignment  to the variables in {X1 , . . . , Xn } witnessing
the validity of F , and let x be an imputation such that (x) coincides with , where
in particular xXi = 1 (resp., xXi = 0) if Xi is false (resp., true) in . We claim
that x  C (G(F )|LC ). Indeed, assume for the sake of contradiction, that there is a
coalition S and an S-feasible payoff vector y such that yi > xi for each i  S. Since
679

fiGreco, Malizia, Palopoli, & Scarcello

v(S)  y(S) > x(S) and since x(S)  0, given the definition of the worth function,
it is actually the case that S is consistent (and, thus, (S) is a truth assignment)
and that (S) is not satisfying. In particular, recall that VLC (S) = {x  RS | x(S) 
v(S)}  (LC)[S]; thus, for each player Xi  S (resp., Xi  S), we have yXi  1 (resp.,
yXi  1). Therefore, S cannot include any player in {X1 , X1 , ..., Xn , Xn } getting a
worth 1 in x. It follows that (S) is an extension of (x), which is moreover not
satisfying. Thus, (x) would not witness the validity of F , which is impossible.
Claim C. B(G(P )|LC ) 6=  if and only if P is valid.
Proof. Consider any imputation x where xa = 1, and where xXi and xXi take distinct values
from the set {0, 1}, for each variable Xi  {X1 , ..., Xm }. Any objection (y, S) to x must be
such that v(S) = 1 (which is indeed the maximum available worth over each coalition S 
N ) and there is no player i  S with xi = 1. It follows that objections are necessarily of the
form (y, S) where w  S, |S| = n+1, and S is consistent w.r.t. {Y1 , . . . , Yn }. In other words,
any objection (y, S) to x is such that S contains w plus one universal player per universally
quantified variable, and thus it is uniquely associated with a truth-value assignment to the
universally quantified variables Y = {Y1 , . . . , Yn }. Let (Y \ S) denote this assignment,
where we set Yj = true if and only if Yj 
/ S, for any 1  j  n. We define also the converse:
given any truth value assignment Y to the universally quantified variables, its associated
objection is the pair (y, S) such that S = {Yj | Y (Yj ) = false}{Yj | Y (Yj ) = true}{w},
1
, for every k  S.
and yk = |S|
Now, if (y, S) is an objection against a player j 
/ {a, X1 , . . . , Xm , X1 , . . . , Xm }, then
(z, {j}) with zj = 0 is a trivial counterobjection. Indeed, such a player j does not belong
to S and may be either an element of {Y1 , . . . , Yn } or an element of {Z1 , . . . , Zq }. In either
cases, v({j}) = xj = 0. On the other hand, if (y, S) is an objection against a player Xi (or,
Xi ), then (z, {Xi , Xi }) with zXi = xXi and zXi = xXi is again a counterobjection, because
v({Xi , Xi }) = z({Xi , Xi }) = 1 and {Xi , Xi }  S = . It follows that the set of objections
that are possibly justified has to be restricted to the objections against player a. Let (y, S)
be an objection of i  S against player a to x. Any counterobjection (z, T ) must have
a  T . Thus, in order to have za  xa = 1, it must be the case that v(T )  1 and, actually,
that v(T ) = 1, due to the definition of the worth function. In particular, the latter entails
that for each player p 6= a with p  T , it holds that zp = 0. Thus, T  S must be empty
and, in particular, the only possibility is that T \ {a} is consistent, and (T ) |= . Finally,
since T  S =  and x(p) = 0 for each p  T with p 6= a, we have that (T ) is a satisfying
assignment for  where: Xi is true in (T ) if and only if Xi is true in (x); Yi is true in
(T ) if and only if Yi 
/ S, and thus Yi is true in (Y \ S). That is, (T ) is a complete
assignment for  that extends both partial assignments (x) and (Y \ S).
By exploiting the above observations, we can now prove the claim.
() Assume that there exists x  B(G(P )|LC ). We have seen that such an imputation x
is associated with a truth-value assignment (x) to the variables in {X1 , . . . , Xm }
recall that an imputation x for which there is an index i, 1  i  m, such that xXi > 0
and xXi > 0 cannot belong to the bargaining set of G(P )|LC . Moreover, we have seen
that every assignment Y to the universally quantified variables corresponds to a
680

fiMixed-Integer Constrained Coalitional Games

possible objection (y, S) to x, and since x belongs to the bargaining set there must
exist a valid counterobjection (z, T ) to (y, S) associated with a satisfying truth-value
assignment for  that extends both partial assignments (x) and Y . This means
that x is a witness of the validity of P .
() If P is valid then there is an assignment X to the variables in {X1 , . . . , Xm } that
witnesses its validity. Consider the imputation x such that, 1  i  m, xXi = 0
and xXi = 1 if X (Xi ) = true, and xXi = 1 and xXi = 0 otherwise. Moreover,
xa = 1 and all other players get 0. Since, for every extension of X to the universally
quantified variables (corresponding to a possible objection (y, S) to x), there exists a
further extension to all variables that satisfies  (corresponding to a counterobjection
to (y, S)), it follows that x  B(G(P )|LC ).
S
i,j,S,T )[y ] contains a point (i.e., a justified obClaim D. (LCi,j,S )[yS ] \ T |iT
S
/ jT (LC
jection to x) that can be represented with polynomially many bits.
Proof. The case where (LCi,j,S )[yS ] and (LCi,j,S,T )[yS ] (for each T | i 
/ T  j  T ) do not
contain integer variables has already been addressed in the proof of Lemma 5.8. We next
show how to preprocess such integer variables, if they occur in the programs at hand.
Recall first that (LCi,j,S )[yS ] is bounded, since LCi,j,S contains the |S| + 1 inequalities:
y(S)  v(S) and yk > xk , k  S. This implies that we can assume, w.l.o.g., that (LCi,j,S )
is bounded in its turn. Indeed, by standard arguments in linear programming it follows that
any point in (LCi,j,S )[yS ] can be obtained as the projection of a point in (LCi,j,S ) whose
auxiliary components (i.e., those not associated with the variables in yS ) are bounded by a
polynomial in the size of LCi,j,S . Thus, this bound can be made explicit in the definition of
LCi,j,S , without altering the projection (LCi,j,S )[yS ].
Second, we observe that we can also assume, w.l.o.g, that (LCi,j,S,T ) is bounded too,
for each T such that i 
/ T  j  T . Indeed, in the definition of LCi,j,S,T , we may constrain
each variable in yS to range within the minimum and maximum values it may assume in
(LCi,j,S )[yS ]note that these extreme values can be represented with polynomially many
bits, since they are achieved S
on some vertices of (LCi,j,S )[yS ]. This modification does not
i,j,S
i,j,S,T )[y ]. Thus, (LCi,j,S,T )[y ] is bounded,
alter the set (LC
)[yS ] \ T |iT
S
S
/ jT (LC
so that (LCi,j,S,T ) can be assumed to be bounded, toosee above.
We can now resume the main proof and show how integer variables can be easily prefC denote the
processed. Let LC be any program in {LCi,j,S }  {LCi,j,S,T | i 
/ T  j  T }, let L
fC) can be represented
linear relaxation of LC, and recall that each vertex of the polytope (L
f
with polynomially many bits. Since (LC) is contained in (LC), such values of the components of the vertices are bounds for every integer component of any vector in (LC), which
can thus be represented with polynomially many bits. Let U be the set of all the admissible
values of all such integer components. Let I(LC) denote the set of all the possible assignments of values from the integer variables of LC to U , and for any assignment z  I(LC),
let LChzi denote the linear program where each integer variable in int(LC) is replaced by its
corresponding value in z. Now, for any pair of assignments z and w belonging to I(LCi,j,S )
and to I(LCi,j,S,T ), respectively, let us say that w matches with z (w.r.t. yS ) if z and w
coincide on their restrictions over yS int(LCi,j,S )int(LCi,j,S,T ). S
Furthermore, let W be the
i,j,S,T )).
set of all non-integer variables in yS , that is, yS \ (int(LCi,j,S )  T |iT
/ jT int(LC
681

fiGreco, Malizia, Palopoli, & Scarcello

S
i,j,S,T )[y ] contains a point that can be
Then, the set (LCi,j,S )[yS ] \ T |iT
S
/ jT (LC
represented with polynomially many bits (resp., is empty) if and only if there is an element
z in I(LCi,j,S ) such that (resp., for each element z in I(LCi,j,S )):


[
[
(LCi,j,S hzi)[W] \ 
(LCi,j,S,T hwi)[W]
T |iT
/ jT

wI(LCi,j,S,T ) | w matches with z

contains an element that can be represented with polynomially many bits (resp., it is empty).
Note that the above expression has the same form as the original one, but no integer
variable occurs in it. To conclude, just observe that LCi,j,S hzi and LCi,j,S,T hwi (T | i 
/
T  j  T ) can be represented with polynomially many bits (w.r.t. the size of the original
mixed-integer linear programs), since they are obtained by mapping integer variables into
values that are representable with polynomially many bits.

References
Agotnes, T., van der Hoek, W., & Wooldridge, M. (2006). Temporal qualitative coalitional
games. In Nakashima, H., Wellman, M. P., Weiss, G., & Stone, P. (Eds.), Proceedings
of the 5th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2006), pp. 177184, Hakodate, Japan.
Aumann, R. J. (1961). The core of a cooperative game without side payments. Transactions
of the American Mathematical Society, 98, 539552.
Aumann, R. J. (1985). An axiomatization of the non-transferable utility value. Econometrica, 53 (3), 599612.
Aumann, R. J., & Dreze, J. H. (1974). Cooperative games with coalition structures. International Journal of Game Theory, 3 (4), 217237.
Aumann, R. J., & Hart, S. (Eds.). (1992, 1994, 2002). Handbook of Game Theory With
Economic Applications, Volume 1,2 and 3, Vol. 11 of Handbooks in Economics.
North-Holland, Amsterdam, The Netherlands.
Aumann, R. J., & Maschler, M. (1964). The bargaining set for cooperative games. In
Advances in Game Theory, pp. 443476. Princeton University Press, Princeton, NJ,
USA.
Aumann, R. J., & Peleg, B. (1960). Von Neumann-Morgenstern solutions to cooperative
games without side payments. Bulletin of the American Mathematical Society, 66 (3),
173179.
Bachrach, Y., & Rosenschein, J. S. (2008). Coalitional skill games. In Padgham, L., Parkes,
D. C., Muller, J., & Parsons, S. (Eds.), Proceedings of the 7th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2008), pp. 10231030,
Estoril, Portugal.
Bilbao, J. M. (2000). Cooperative Games on Combinatorial Structures, Vol. 26 of Theory
and Decision Library C. Kluwer Academinc Publishers, Reading, MA, USA.
Byford, M. C. (2007). A constrained coalitional approach to price formation. In North
American Summer Meetings of the Econometric Society, Durham, NC, USA.
682

fiMixed-Integer Constrained Coalitional Games

Charnes, A. (1953). Constrained games and linear programming. Proceedings of the National
Academy of Sciences of the United States of America, 39 (7), 639641.
Conitzer, V., & Sandholm, T. (2004). Computing shapley values, manipulating value division schemes, and checking core membership in multi-issue domains. In McGuinness,
D. L., & Ferguson, G. (Eds.), Proceedings of the 19th National Conference on Artificial
Intelligence (AAAI-04), pp. 219225, San Jose, CA, USA.
Davis, M., & Maschler, M. (1965). The kernel of a cooperative game.. Naval Research
Logistics Quarterly, 12, 223259.
Deng, X., & Papadimitriou, C. H. (1994). On the complexity of cooperative solution concepts. Mathematics of Operations Research, 19 (2), 257266.
Dunne, P. E., van der Hoek, W., Kraus, S., & Wooldridge, M. (2008). Cooperative boolean
games. In Padgham, L., Parkes, D. C., Muller, J., & Parsons, S. (Eds.), Proceedings
of the 7th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2008), pp. 10151022, Estoril, Portugal.
Edgeworth, F. Y. (1881). Mathematical Psychics: An essay on the mathematics to the moral
sciences. C. Kegan Paul & Co., London.
Elkind, E., Goldberg, L. A., Goldberg, P. W., & Wooldridge, M. (2009). On the computational complexity of weighted voting games. Annals of Mathematics and Artificial
Intelligence, 56 (2), 109131.
Elkind, E., & Pasechnik, D. (2009). Computing the nucleolus of weighted voting games.
In Mathieu, C. (Ed.), Proceedings of the 20th Annual ACM-SIAM Symposium on
Discrete Algorithms (SODA09), pp. 327335, New York, NY, USA.
Gillies, D. B. (1959). Solutions to general non-zero-sum games. In Tucker, A. W., & Luce,
R. D. (Eds.), Contributions to the Theory of Games, Volume IV, Vol. 40 of Annals of
Mathematics Studies, pp. 4785. Princeton University Press, Princeton, NJ, USA.
Greco, G., Malizia, E., Palopoli, L., & Scarcello, F. (2009a). Constrained coalitional games:
formal framework, properties, and complexity results (extended abstract). In Sierra,
C., Castelfranchi, C., Decker, K. S., & Sichman, J. S. (Eds.), Proceedings of the
8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2009), pp. 12951296, Budapest, Hungary.
Greco, G., Malizia, E., Palopoli, L., & Scarcello, F. (2009b). On the complexity of compact coalitional games. In Boutilier, C. (Ed.), Proceedings of the 21th International Joint Conference on Artificial Intelligence (IJCAI-09), pp. 147152, Pasadena,
CA, USA. An extended version is available as the technical report arXiv:0810.3136
(http://arxiv.org/abs/0810.3136).
Hart, S. (2004). A comparison of non-transferable utility values. Theory and Decision,
56 (12), 3546.
Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: a compact representation
scheme for coalitional games. In Riedl, J., Kearns, M. J., & Reiter, M. K. (Eds.), Proceedings of the 6th ACM Conference on Electronic Commerce (EC05), pp. 193202,
Vancouver, BC, Canada.
683

fiGreco, Malizia, Palopoli, & Scarcello

Ieong, S., & Shoham, Y. (2006). Multi-attribute coalitional games. In Feigenbaum, J.,
Chuang, J., & Pennock, D. M. (Eds.), Proceedings of the 7th ACM Conference on
Electronic Commerce (EC06), pp. 170179, Ann Arbor, MI, USA.
Ieong, S., & Shoham, Y. (2008). Bayesian coalitional games. In Fox, D., & Gomes, C. P.
(Eds.), Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI08), pp. 95100, Chicago, IL, USA.
Jiang, T., & Baras, J. S. (2007). Fundamental tradeoffs and constrained coalitional games
in autonomic wireless networks. In Proceedings of the 5th International Symposium on
Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt07),
pp. 18, Limassol, Cyprus.
Johnson, D. S. (1990). A catalog of complexity classes. In van Leeuwen, J. (Ed.), Handbook
of Theoretical Computer Science, Volume A: Algorithms and Complexity, pp. 67161.
The MIT Press, Cambridge, MA, USA.
Kalai, E. (1975). Excess functions for cooperative games without sidepayments. SIAM
Journal of Applied Mathematics, 29 (1), 6071.
Malizia, E., Palopoli, L., & Scarcello, F. (2007). Infeasibility certificates and the complexity
of the core in coalitional games. In Veloso, M. M. (Ed.), Proceedings of the 20th
International Joint Conference on Artificial Intelligence (IJCAI-07), pp. 14021407,
Hyderabad, India.
Maschler, M. (1992). The bargaining set, kernel, and nucleolus. In Aumann, R. J., & Hart,
S. (Eds.), Handbook of Game Theory, Volume 1, Vol. 11 of Handbooks in Economics,
chap. 18. North-Holland, Amsterdam, The Netherlands.
McLean, R. P. (2002). Values of non-transferable utility games. In Aumann, R. J., & Hart,
S. (Eds.), Handbook of Game Theory, Volume 3, Vol. 11 of Handbooks in Economics,
chap. 55. North-Holland, Amsterdam, The Netherlands.
Nemhauser, G. L., & Wolsey, L. A. (1988). Integer and combinatorial optimization. WileyInterscience Series in Discrete Mathematics and Optimization. Wiley-Interscience,
New York, NY, USA.
Orshan, G., & Zarzuelo, J. M. (2000). The bilateral consistent prekernel for ntu games.
Games and Economic Behavior, 32 (1), 6784.
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. The MIT Press,
Cambridge, MA, USA.
Owen, G. (1975). On the core of linear production games. Mathematical Programming,
9 (1), 358370.
Papadimitriou, C., & Steiglitz, K. (1998). Combinatorial Optimization: Algorithms and
Complexity (2nd edition). Dover Publications.
Peleg, B. (1963). Bargaining sets of cooperative games without side payments. Israel Journal
of Mathematics, 1 (4), 197200.
Peleg, B., & Sudholter, P. (2007). Introduction to the Theory of Cooperative Games (2nd
edition). Theory and Decision Library. Springer, Berlin, Germany.

684

fiMixed-Integer Constrained Coalitional Games

Rahwan, T., Ramchurn, S. D., Jennings, N. R., & Giovannucci, A. (2009). Anytime algorithm for optimal coalition structure generation. Journal of Artificial Intelligence
Research, 34, 521567.
Ryan, M. J. (1998). Constrained games, intervening duality and experimenter-experiment
interactions. European Journal of Operational Research, 110 (2), 326341.
Schmeidler, D. (1969). The nucleolus of a characteristic function game. SIAM Journal of
Applied Mathematics, 17 (6), 11631170.
Selman, A. L. (1994). A taxonomy of complexity classes of functions. Journal of Computer
and System Sciences, 48 (2), 357381.
Semple, J. (1997). Constrained games for evaluating organizational performance. European
Journal of Operational Research, 96 (1), 103112.
Serrano, R. (1997). Reinterpreting the kernel. Journal of Economic Theory, 77 (1), 5880.
Shapley, L. S. (1953). A value for n-person games. In Kuhn, H. W., & Tucker, A. W. (Eds.),
Contributions to the Theory of Games, Volume II, Vol. 28 of Annals of Mathematics
Studies, pp. 307317. Princeton University Press, Princeton, NJ, USA.
Shapley, L. S. (1969). Utility comparison and the theory of games. In La Decision, pp.
251263. Editions du Centre National de le Recherche Scientifique, Paris.
Shoham, Y. (2008). Computer science and game theory. Communications of the ACM,
51 (8), 7479.
Simon, H. A. (1972). Theories of bounded rationality. In McGuire, C. B., & Radner, R.
(Eds.), Decision and Organization, Vol. 12 of Studies in Mathematical and Managerial
Economics, pp. 161176. North-Holland, Amsterdam, The Netherlands.
von Neumann, J., & Morgenstern, O. (1944). Theory of Games and Economic Behavior
(1st edition). Princeton University Press, Princeton, NJ, USA.
Weber, R. J. (1994). Games in coalitional form. In Aumann, R. J., & Hart, S. (Eds.),
Handbook of Game Theory, Volume 2, Vol. 11 of Handbooks in Economics, chap. 36.
North-Holland, Amsterdam, The Netherlands.
Winter, E. (2002). The shapley value. In Aumann, R. J., & Hart, S. (Eds.), Handbook of
Game Theory, Volume 3, Vol. 11 of Handbooks in Economics, chap. 53. North-Holland,
Amsterdam, The Netherlands.
Wooldridge, M., & Dunne, P. E. (2004). On the computational complexity of qualitative
coalitional games. Artificial Intelligence, 158 (1), 2773.
Wooldridge, M., & Dunne, P. E. (2006). On the computational complexity of coalitional
resource games. Artificial Intelligence, 170 (10), 835871.

685

fiJournal of Articial Intelligence Research 38 (2010) 339-369

Submitted 11/09; published 07/10

Mixed Strategies in Combinatorial Agency
Moshe Babaio

moshe@microsoft.com

Microsoft Research - Silicon Valley
Mountain View, CA 94043 USA

Michal Feldman

mfeldman@huji.ac.il

School of Business Administration
and Center for the Study of Rationality,
Hebrew University of Jerusalem,
Jerusalem, Israel

Noam Nisan

noam@cs.huji.ac.il

School of Computer Science,
The Hebrew University of Jerusalem,
Jerusalem, Israel

Abstract
In many multiagent domains a set of agents exert eort towards a joint outcome, yet the
individual eort levels cannot be easily observed. A typical example for such a scenario is
routing in communication networks, where the sender can only observe whether the packet
reached its destination, but often has no information about the actions of the intermediate
routers, which inuences the nal outcome. We study a setting where a principal needs to
motivate a team of agents whose combination of hidden eorts stochastically determines an
outcome. In a companion paper we devise and study a basic combinatorial agency model
for this setting, where the principal is restricted to inducing a pure Nash equilibrium. Here
we study various implications of this restriction. First, we show that, in contrast to the
case of observable eorts, inducing a mixed-strategies equilibrium may be benecial for the
principal. Second, we present a sucient condition for technologies for which no gain can be
generated. Third, we bound the principals gain for various families of technologies. Finally,
we study the robustness of mixed equilibria to coalitional deviations and the computational
hardness of the optimal mixed equilibria.

1. Introduction
In this paper we study Combinatorial Agency with Mixed Strategies, this section reviews
some background on Combinatorial Agency with pure strategies and then present our results
for mixed strategies.
1.1 Background: Combinatorial Agency
The well studied principal-agent problem deals with how a principal can motivate a
rational agent to exert costly eort towards the welfare of the principal. The diculty
in this model is that the agents action (i.e. whether he exerts eort or not) is invisible
to the principal and only the nal outcome, which is probabilistic and also inuenced
c
2010
AI Access Foundation. All rights reserved.

fiBabaioff, Feldman & Nisan

by other factors, is visible1 . This problem is well studied in many contexts in classical
economic theory and we refer the readers to introductory texts on economic theory such
as the work of Mass-Colell, Whinston, and Green (1995), Chapter 14. In these settings, a
properly designed contract, in which the payments are contingent upon the nal outcome,
can inuence a rational agent to exert the required eort.
In many multiagent settings, however, a set of agents work together towards a joint
outcome. Handling combinations of agents rather than a single agent is the focus of the
work by Babaio, Feldman, and Nisan (2006a). While much work was previously done
on motivating teams of agents (e.g., Holmstrom, 1982; Strausz, 1996), our emphasis is on
dealing with the complex combinatorial structure of dependencies between agents actions.
In the general case, each combination of eorts exerted by the n dierent agents may result
in a dierent expected gain for the principal. The general question asks, given an exact
specication of the expected utility of the principal for each combination of agents actions,
which conditional payments should the principal oer to which agents as to maximize his
net utility?
We view this problem of hidden actions in computational settings as a complementary
problem to the problem of hidden information that is the heart of the eld of Algorithmic
Mechanism Design (Nisan, Roughgarden, Tardos, & Vazirani, 2007; Nisan & Ronen, 2001).
In recent years, computer science and articial intelligence have showed a lot of interest
in algorithmic mechanism design. In particular, they imported concepts from game theory
and mechanism design for solving problems that arise in articial intelligence application
domains, such as computer networks with routers as autonomous software agents.
Communication networks serve as a typical application to our setting. Since many computer networks (such as the Internet and mobile ad-hoc networks) are used and administered
by multiple entities with dierent economic interests, their performance is determined by
the actions among the various interacting self-interested parties. Thus, taking into account
the economic and strategic considerations together with the technical ones may be crucial
in such settings. Indeed, recent years have seen a urry of research employing game theoretic models and analysis for better understanding the eect of strategic considerations on
network design and performance.
An example that was discussed in the work of Feldman, Chuang, Stoica, and Shenker
(2007) is Quality of Service routing in a network: every intermediate link or router may
exert a dierent amount of eort (priority, bandwidth, etc.) when attempting to forward
a packet of information. While the nal outcome of whether a packet reached its destination
is clearly visible, it is rarely feasible to monitor the exact amount of eort exerted by each
intermediate link  how can we ensure that they really do exert the appropriate amount
of eort? For example, in Internet routing, IP routers may delay or drop packets, and in
mobile ad hoc networks, devices may strategically drop packets to conserve their constrained
energy resources. Aside from forwarding decisions, which are done in a sequential manner,
some eort decisions take place prior to the actual packet transmission, and are done in
a simultaneous manner. There are many examples for such decisions, among them are the
quality of the hardware, appropriate tuning of the routers, and more. Our focus is on these
1. Invisible here is meant in a wide sense that includes not precisely measurable, costly to determine,
or non-contractible (meaning that it can not be upheld in a court of law).

340

fiMixed Strategies in Combinatorial Agency

a-priori eort decisions, since they are crucial to the quality of the transmission, and it is
harder to detect agents who shirk with respect to these matters.
In the general model presented in the work of Babaio et al. (2006a), each of n agents
has a set of possible actions, the combination of actions by the players results in some
outcome, where this happens probabilistically. The main part of the specication of a
problem in this model is a function (the technology) that species this distribution for
each n-tuple of agents actions. Additionally, the problem species the principals utility
for each possible outcome, and for each agent, the agents cost for each possible action.
The principal motivates the agents by oering to each of them a contract that species a
payment for each possible outcome of the whole project. Key here is that the actions of
the players are non-observable (hidden-actions) and thus the contract cannot make the
payments directly contingent on the actions of the players, but rather only on the outcome
of the whole project.
Given a set of contracts, each agent optimizes his own utility; i.e., chooses the action that
maximizes his expected payment minus the cost of the action. Since the outcome depends
on the actions of all players together, the agents are put in a game here and are assumed
to reach a Nash Equilibrium (NE). The principals problem is that of designing the optimal
contract: i.e. the vector of contracts to the dierent agents that induce an equilibrium that
will optimize his expected utility from the outcome minus his expected total payment. The
main diculty is that of determining the required Nash equilibrium point.
Our interest in this paper, as in the work of Babaio et al. (2006a), is focused on the
binary case: each agent has only two possible actions exert eort and shirk and there
are only two possible outcomes success and failure. Our motivating examples come
from the following more restricted and concrete structured subclass of problem instances:
Every agent i performs a subtask which succeeds with a low probability i if the agent does
not exert eort and with a higher probability i > i , if the agent does exert eort. The
whole project succeeds as a deterministic Boolean function of the success of the subtasks.
For example, the Boolean AND and OR functions represent the respective cases where
the agents are complementary (i.e., where the project succeeds if and only if all the agents
succeed) or substitutive (i.e., where the project succeeds if and only if at least one of the
agents succeeds). Yet, a more restricted subclass of problem instances are those technologies
that can be represented by read-once networks with two specied source and sink nodes,
in which every edge is labeled by a single agent, and the project succeeds (e.g., a packet
of information reaches the destination) if there is a successful path between the source and
the sink nodes.
1.2 This Paper: Mixed Equilibria
The focus in the work by Babaio et al. (2006a) was on the notion of Nash-equilibrium in
pure strategies: we did not allow the principal to attempt inducing an equilibrium where
agents have mixed strategies over their actions. In the observable-actions case (where the
principal can condition the payments on the agents individual actions) the restriction to
pure strategies is without loss of generality: mixed actions can never help since they simply
provide a convex combination of what would be obtained by pure actions.
341

fiBabaioff, Feldman & Nisan

Yet, surprisingly, we show this is not the case for the hidden-actions case which we are
studying: in some cases, a Mixed-Nash equilibrium can provide better expected utility to
the principal than what he can obtain by equilibrium in pure strategies. In particular, this
already happens in the case of two substitutive agents with a certain (quite restricted) range
of parameters (see Section 3).
While inducing mixed strategy equilibria might be benecial for the principal, mixed
Nash equilibrium is a much weaker solution concept than pure Nash equilibrium, as was
already observed by Harsanyi (1973). As opposed to Nash equilibria in pure strategies, the
guarantees that one obtains are only in expectation. In addition, any player can deviate from
his equilibrium strategy without lowering his expected payo even if he expects all other
players to stick to their equilibrium strategies. Moreover, best-response dynamics converge
to pure proles, and there is no natural dynamics leading to a mixed Nash equilibrium.
As a result, if the principal cannot gain much by inducing a Nash equilibrium in mixed
strategies, he might not be willing to tolerate the instability of this notion. Our main goal
is to quantify the principals gain from inducing mixed equilibrium, rather than pure. To do
that, we analyze the worst ratio (over all principals values) between the principals optimal
utility with mixed equilibrium, and his optimal utility with pure equilibrium. We term this
ratio the price of purity (POP) of the instance under study.
The price of purity is at least 1 by denition, and the larger it is, the more the principal
can gain by inducing a mixed equilibrium compared to a pure one. We prove that for
super-modular technologies (e.g. technologies with increasing returns to scale) which
contains in particular the AN D Boolean function, the price of purity is trivial (i.e., P OP =
1). Moreover, we show that for any other Boolean function, there is an assignment of
the parameters (agents individual success probabilities) for which the obtained structured
technology has non trivial POP (i.e., P OP > 1). (Section 4).
While the price of purity may be strictly greater than 1, we obtain quite a large number
of results bounding this ratio (Section 5). These bounds range from a linear bound for
very general families of technologies (e.g., P OP  n for any anonymous or sub-modular
technology) to constant bounds for some restricted cases (e.g., P OP  1.154... for a family
of anonymous OR technologies, and P OP  2 for any technology with 2 agents).
Additionally, we study some other properties of mixed equilibrium. We show that mixed
Nash equilibria are more delicate than pure ones. In particular, we show that unlike the
pure case, in which the optimal contract is also a strong equilibrium (Aumann, 1959)
(i.e., resilient to deviations by coalitions), an optimal mixed contract (in which at least two
agents truly mix) never satises the requirements of a strong equilibrium (Section 6).
Finally, we study the computational hardness of the optimal mixed Nash equilibrium,
and show that the hardness results from the pure case hold for the mixed case as well
(Section 7).

2. Model and Preliminaries
We focus on the simple binary action, binary outcome scenario where each agent has two
possible actions (exert eort or shirk) and there are two possible outcomes (failure,
success). We begin by presenting the model with pure actions (which is a generalization
of the model of Winter, 2004), and then move to the mixed case. A principal employs a set
342

fiMixed Strategies in Combinatorial Agency

of agents N of size n. Each agent i  N has a set of two possible actions Ai = {0, 1} (binary
action), the low eort action (0) has a cost of 0 (ci (0) = 0), while the high eort action (1) as
a cost of ci > 0 (ci (1) = ci ). The played prole of actions determine, in a probabilistic way,
a contractible outcome, o  {0, 1}, where the outcomes 0 and 1 denote project failure and
success, respectively (binary-outcome). The outcome is determined according to a success
function t : A1  . . .  An  [0, 1], where t(a1 , . . . , an ) denotes the probability of project
success where players play with the action prole a = (a1 , . . . , an )  A1  . . .  An = A.
We use the notation (t, c) to denote a technology (a success function and a vector of costs,
one for each agent). We assume that everything but the eort of the agents is common
knowledge.
The principals value of a successful project is given by a scalar v > 0, where he gains
no value from a project failure. In this hidden-actions model the actions of the players are
invisible, but the nal outcome is visible to him and to others, and he may design enforceable
contracts based on this outcome. We assume that the principal can pay the agents but not
ne them (known as the limited liability constraint). The contract to agent i is thus given
by a scalar value pi  0 that denotes the payment that i gets in case of project success. If
the project fails, the agent gets no money (this is in contrast to the observable-actions
model in which payment to an agent can be contingent on his action). The contracts to all
the agents public, all agents know them before making their eort decisions.
Given this setting, the agents have been put in a game, where the utility of agent i
under the prole of actions a = (a1 , . . . , an )  A is given by ui (a) = pi  t(a)  ci (ai ). As
usual, we denote by ai  Ai the (n  1)-dimensional vector of the actions of all agents
excluding agent i. i.e., ai = (a1 , . . . , ai1 , ai+1 , . . . , an ). The agents will be assumed to
reach Nash equilibrium, if such an equilibrium exists. The principals problem (which is our
problem in this paper) is how
 to design the contracts pi as to maximize his own expected
utility u(a, v) = t(a)  (v  iN pi ), where the actions a1 , . . . , an are at Nash-equilibrium.
In the case of multiple Nash equilibria, in our model we let the principal choose the desired
one, and suggest it to the agents, thus focusing on the best Nash equilibrium.2
As we wish to concentrate on motivating agents, rather than on the coordination between
agents, we assume that more eort by an agent always leads to a better probability of
success. Formally, i  N, ai  Ai we have that t(1, ai ) > t(0, ai ). We also assume
that t(a) > 0 for any a  A.
We next consider the extended game in which an agent can mix between exerting eort
and shirking (randomize over the two possible pure actions). Let qi denote the probability
that agent i exerts eort, and let qi denote the (n  1)-dimensional vector of investment
probabilities of all agents except for agent i. We can extend the denition of the success
function t to the range of mixed strategies, by taking the expectation.
t(q1 , . . . , qn ) =



n

( qiai  (1  qi )(1ai ) )t(a1 , . . . , an )

a{0,1}n i=1

2. While in the pure case (Babaio, Feldman, & Nisan, 2006b), the best Nash equilibrium is also a strong
equilibrium, this is not the case in the more delicate mixed case (see Section 6). Other variants of NE
exist. One variant, which is similar in spirit to strong implementation in mechanism design, would be
to take the worst Nash equilibrium, or even, stronger yet, to require that only a single equilibrium exists
(as in the work of Winter, 2004).

343

fiBabaioff, Feldman & Nisan

Note that for any agent i and any (qi , qi ) it holds that t(qi , qi ) = qi  t(1, qi ) + (1  qi ) 
t(0, qi ). A mixed equilibrium prole in which at least one agent mixes with probability
pi  [0, 1] is called a non-degenerate mixed equilibrium.
In pure strategies, the marginal contribution of agent i, given ai  Ai , is dened to
be: i (ai ) = t(1, ai )  t(0, ai ). For the mixed case we dene the marginal contribution
of agent i, given qi to be: i (qi ) = t(1, qi )  t(0, qi ). Since t is monotone, i is a
positive function.
We next characterize what payment can result in an agent mixing between exerting
eort and shirking.
Claim 2.1 Agent is best response is to mix between exerting eort and shirking with probability qi  (0, 1) only if he is indierent between ai = 1 and ai = 0. Thus, given a prole
of strategies qi , agent i mixes only if:
pi =

ci
ci
=
i (qi )
t(1, qi )  t(0, qi )

which is the payment that makes him indierent between exerting eort and(shirking. The
)

q
.
expected utility of agent i, who exerts eort with probability qi is: ui (q) = ci  it(q)
i
(qi )
Proof: Recall that ui (q) = t(q)  pi  qi  ci , thus ui (q) = qi  ui (1, qi ) + (1  qi )  ui (0, qi ).
Since i maximizes his utility, if qi  (0, 1), it must be the case that ui (1, qi ) = ui (0, qi ).
ci
.
2
Solving for pi we get that pi = i (q
i )
A prole of mixed strategies q = (q1 , . . . , qn ) is a Mixed Nash equilibrium if for any agent
i, qi is agent is best response, given qi .
The principals expected utility under the mixed Nash prole q is given
=
 by u(q, v)
ci
(v  P )  t(q), where P is the total payment in case of success, given by P = i|qi >0 i (q
.
i )

An optimal mixed contract for the principal is an equilibrium mixed strategy prole q (v)
that maximizes the principals utility at the value v. In Babaio et al. (2006a) we show a
similar characterization of optimal pure contract a  A. An agent that exerts eort is paid
ci
i (ai ) , and the utilities are the same as the above, when given the pure prole. In the
pure Nash case, given a value v, an optimal pure contract for the principal is a set of agents
S  (v) that exert eort in equilibrium, and this set maximizes the principals utility at the
value v.
A simple but crucial observation, generalizing a similar one in the work of Babaio
et al. (2006a) for the pure Nash case, shows that the optimal mixed contract exhibits some
monotonicity properties in the value.
Lemma 2.2 (Monotonicity lemma): For any technology (t, c) the expected utility of
the principal at the optimal mixed contract, the success probability of the optimal mixed
contract, and the expected payment of the optimal mixed contract, are all monotonically
non-decreasing with the value.
The proof is postponed to Appendix A, and it also shows that the same monotonicity
also holds in the observable-actions case. Additionally, the lemma holds in more general
settings, where each agent has an arbitrary action set (not restricted to the binary-actions
model considered here).
344

fiMixed Strategies in Combinatorial Agency

We wish to quantify the gain by inducing mixed Nash equilibrium, over inducing pure
Nash. We dene the price of purity as the worse ratio (over v) between the maximum
utilities that are obtained in mixed and pure strategies.
Denition 2.3 The price of purity P OP (t, c) of a technology (t, c) is dened as the worse
ratio, over v, between the principals optimal utility in the mixed case and his optimal utility
in the pure case. Formally,
)
(

t(q  (v)) v  i|q (v)>0 i (qci (v))
i
i
(
)
P OP (t, c) = Supv>0

ci

t(S (v)) v  iS  (v) i (ai )
where S  (v) denotes an optimal pure contract and q  (v) denotes an optimal mixed contract,
for the value v.
The price of purity is at least 1, and may be greater than 1, as we later show. Additionally, it is obtained at some value that is a transition point of the pure case (a point in
which the principal is indierent between two optimal pure contracts).
Lemma 2.4 For any technology (t, c), the price of purity is obtained at a nite v that is a
transition point between two optimal pure contracts.
2.1 Structured Technology Functions
In order to be more concrete, we next present technology functions whose structure can be
described easily as being derived from independent agent tasks  we call these structured
technology functions. This subclass gives us some natural examples of technology functions,
and also provides a succinct and natural way to represent technology success functions.
In a structured technology function, each individual succeeds or fails in his own task
independently. The projects success or failure deterministically depends, maybe in a complex way, on the set of successful sub-tasks. Thus we will assume a monotone Boolean
function f : {0, 1}n  {0, 1} which indicates whether the project succeeds as a function of
the success of the n agents tasks.
A structured technology function t is dened by t(a1 , . . . , an ) being the probability
that f (x1 , . . . , xn ) = 1 where the bits x1 , . . . , xn are chosen according to the following
distribution: if ai = 0 then xi = 1 with probability i  [0, 1) (and xi = 0 with probability
1  i ); otherwise, i.e. if ai = 1, then xi = 1 with probability i > i (and xi = 0 with
probability 1  i ). Thus, a structured technology is dened by n, f and the parameters
{i , i }iN .
Let us consider two simple structured technology functions, AND and OR. First
consider
the AND technology: f (x1 , . . . , xn ) is the logical conjunction of xi (f (x) =

x
).
Thus the project succeeds only if all agents succeed in their tasks. This is shown
iN i
graphically as a read-once network in Figure 1(a). For this technology, the probability
of success is the product of the individual
success probabilities. Agent i succeeds with

probability iai  i1ai , thus t(a) = iN iai  i1ai .
Next,consider the OR technology: f (x1 , . . . , xn ) is the logical disjunction of xi
(f (x) = iN xi ). Thus the project succeeds if at least one of the agents succeed in their
345

fiBabaioff, Feldman & Nisan

S

x1 x2

xn

x1
x2
t

S

t

xn
(b) OR technology

(a) AND technology

Figure 1: AND and OR technologies. In AND (a), the project is successful if a packet is routed
along a linear path (where each agent controls an edge), and in OR (b), the project is
successful if a packet is routed at least along one edge.

tasks. This is shown graphically as a read-once network in Figure 1(b). For this technology,
the probability of success is 1 minus the probability
that all of them fail. Agent i fails with
probability (1  i )ai  (1  i )1ai , thus t(a) = 1  iN (1  i )ai  (1  i )1ai .
These are just two simple examples. One can consider other more interesting examples
as the Majority function (the project succeed if the majority of the agents are successful),
or the OR-Of-ANDs technology, which is a disjunction over conjunctions (several teams,
the project succeed if all the agents in any one of the teams are successful). For additional
examples see the work of Babaio et al. (2006a).
A success function t is called anonymous
if it is symmetric with respect to the players.

I.e. t(a1 , . . . , an ) depends only on i ai . For example, in an anonymous OR technology
there are parameters 1 >  >  > 0 such that each agent i succeed with probability 
with no eort, and with probability  >  with eort. If m agents exert eort, the success
probability is 1  (1  )m  (1  )nm .
A technology has identical costs if there exists a c such that for any agent i, ci = c.
As in the case of identical costs the POP is independent of c, we use P OP (t) to denote
the POP for technology t with identical costs. We abuse notation and denote a technology
with identical costs by its success function t. Throughout the paper, unless explicitly stated
otherwise, we assume identical costs. A technology t with identical costs is anonymous if t
is anonymous.

3. Example: Mixed Nash Outperforms Pure Nash!
If the actions are observable (henceforth, the observable-actions case), then an agent that
exerts eort is paid exactly his cost, and the principals utility equals the social welfare.
In this case, the social welfare in mixed strategies is a convex combination of the social
welfare in pure strategies; thus, it is clear that the optimal utility is always obtained in pure
strategies. However, surprisingly enough, in the hidden-actions case, the principal might
gain higher utility when mixed strategies are allowed. This is demonstrated in the following
example:

346

fiMixed Strategies in Combinatorial Agency

Figure 2: Optimal mixed contracts in OR technologies with 2 agents. The areas indicated by 0,
1, and 2 correspond to areas where it is optimal that 0, 1, or 2 agents, respectively,
exert eort with probability 1. The white area corresponds to both agents exert eort
with the same non-trivial probability, q. For any xed , q increases in v.

Example 3.1 Consider an anonymous OR technology with two agents, where c = 1,  =
1 = 2 = 1  1 = 1  2 = 0.09 and v = 348. It holds that t(0, 0) = 1  (1  )2 =
0.172, t(0, 1) = t(1, 0) = 1  (1  )(1  ) = 0.9181, and t(1, 1) = 1  (1  )2 = 0.992.
Consider the mixed strategy q1 = q2 = 0.92. It holds that: t(0, 0.92) = 0.08  t(0, 0) +
0.92  t(0, 1) = 0.858, t(1, 0.92) = 0.92  t(1, 1) + 0.08  t(1, 0) = 0.986, and t(0.92, 0.92) =
0.082  t(0, 0) + 0.08  0.92  t(0, 1)  2 + 0.922  t(1, 1) = 0.976. The payment to each player
1
= 7.837, thus the principals
under a successful project is pi (0.92, 0.92) = t(1,0.92)t(0,0.92)
utility under the mixed strategies q1 = q2 = 0.92 and v = 348 is u((0.92, 0.92), 348) =
t(0.92, 0.92)  (348  2  7.837) = 324.279.
While the principals utility under the mixed prole q1 = q2 = 0.92 is 324.279, the
optimal contract with pure strategies is obtained when both agents exert eort and achieves
a utility of 318.3. This implies that by moving from pure strategies to mix strategies, one
gains at least 324.27/318.3 > 1.0187 factor improvement (which is approximately 1.8%).
A worse ratio exists for the more general case (in which it does not necessarily hold that
 = 1  ) of  = 0.0001,  = 0.9 and v = 233. For this case we get that the optimal pure
contract is with one agent, gives utility of 208.7, while the mixed contract q1 = q2 = 0.92
gives utility of 213.569, and the ratio is at least 1.0233 (approximately 2.3%).
To complete the example, Diagram 2 presents the optimal contract for OR of 2 agents,
as a function of  (when  = 1  ) and v. It shows that for some parameters of  and v,
the optimal contract is obtained when both agents exert eort with equal probabilities.
The following lemma (proved in Appendix A.1) shows that optimal mixed contracts in
any anonymous OR technology (with n agents) have this specic structure. That is, all
agents that do not shirk, mix with exactly the same probability.

347

fiBabaioff, Feldman & Nisan

Lemma 3.2 For any anonymous OR technology (any  > , c, n) and value v, either the
optimal mixed contract is a pure contract, or, in the optimal mixed contract k  {2, . . . n}
agents exert eort with equal probabilities q1 = . . . = qk  (0, 1), and the rest of the agents
exert no eort.

4. When is Pure Nash Good Enough?
Next, we identify a class of technologies for which the price of purity is 1; that is, the
principal cannot improve his utility by moving from pure Nash equilibrium to mixed Nash
equilibrium. These are technologies for which the marginal contribution of any agent is nondecreasing in the eort of the other agents. Formally, for two pure action proles a, b  A
we denote b  a if for all j, bj j aj (eort bj is at least as high as the eort aj ).
Denition 4.1 A technology success function t exhibits (weakly) increasing returns to
scale (IRS)3 if for every i, and every pure proles b  a
t(bi , bi )  t(ai , bi )  t(bi , ai )  t(ai , ai )
Any AND technology exhibits IRS (Winter, 2004; Babaio et al., 2006a). For IRS
technologies we show that P OP = 1.
Theorem 4.2 Assume that t is super-modular. For any cost vector c, P OP (t, c) = 1.
Moreover, a non-degenerate mixed contract is never optimal.
Proof: For a mixed prole q = (q1 , q2 , . . . , qn ), let S(q) be the support of q, that is, i  S(q)
if and only if qi > 0, and for any agent i  S(q) let Si = S(q) \ {i} be the support of q
excluding i. Similarly, for a pure prole a = (a1 , a2 , . . . , an ) let S(a) be the support a. Under
ci
. Similarly, under
the mixed prole q, agent i  S(q) is being paid pi (qi ) = t(1,qi )t(0,q
i )
ci
the pure prole a, agent i  S(a) is being paid pi (S(a) \ {i}) = pi (ai ) = t(S(a))t(S(a)\{i})
,
where t(T ) is the success probability when aj = 1 for j  T , and aj = 0 for j 
/ T . We also
denote i (T ) = t(T )  t(T \ {i}).
We show that if q is a non-degenerate mixed prole (i.e., at least one agent in q exerts
eort with probability qi  (0, 1)), the prole in which each agent in S(q) exerts eort with
probability 1 yields a higher utility to the principal.
By Lemma 5.3 (see Section 5), it holds that pi (qi )  minT Si pi (T ), where pi (T ) =
ci
i (T ) . But if t exhibits IRS, then i (T ) is an increasing function by denition (see Section 4), therefore minT Si pi (T ) = pi (Si ). Therefore it holds that for any i  S(q),
pi (qi )  pi (Si ), thus:


pi (qi ) 
pi (Si )
iS(q)

iS(q)

3. Note that t exhibits IRS if and only if it is super-modular.

348

fiMixed Strategies in Combinatorial Agency

In addition, due to the monotonicity of t, it holds that t(q) < t(S(q)). Therefore,



u(q, v) = t(q) v 
pi (qi )


iS(q)

< t(S(q)) v 

 t(S(q)) v 


iS(q)




pi (qi )

pi (Si )

iS(q)

= u(S(q), v)
where u(S(q), v) is the principals utility under the pure prole in which all the agents in
S(q) exert eort with probability 1, and the rest exert no eort.
2
We show that AN D (on some subset of bits) is the only function such that any structured
technology based on this function exhibits IRS, that is, this is the only function such that for
any choices of parameters (any n and any {i , i }iN ), the structured technology exhibits
IRS. For any other Boolean function, there is an assignment for the parameters such that the
created structured technology is essentially OR over 2 inputs (Lemma B.1 in Appendix B),
thus it has non-trivial POP (recall Example 3.1). For the proof of the following theorem
see Appendix B.
Theorem 4.3 Let f be any monotone Boolean function with n  2 inputs, that is not
constant and not a conjunction of some subset of the input bits. Then there exist parameters
{i , i }ni=1 such that the POP of the structured technology with the above parameters (and
identical cost c = 1) is greater than 1.0233.
Thus, our goal now is to give upper bounds on the POP for various technologies.

5. Quantifying the Gain by Mixing
In this section we present bounds on the price of purity for general technologies, following
by bounds for the special case of OR technology.
5.1 POP for General Technologies
We rst show that the POP can be bounded by the principals price of unaccountability (Babaio et al., 2006b), whose denition follows.
Denition 5.1 The principals price of unaccountability P OUP (t, c) of a technology (t, c)
is dened as the worst ratio (over v) between the principals utility in the observable-actions
case and the hidden-actions case:

 (v))  v 
t(Soa
 (v) ci
iSoa

P OUP (t, c) = Supv>0
ci
t(S  (v))  v  iS  (v) i (a
i )
 (v) is the optimal pure contract in the observable-actions case, and S  (v) is the
where Soa
optimal pure contract in the hidden-actions case.

349

fiBabaioff, Feldman & Nisan

Theorem 5.2 For any technology t it holds that P OUP (t)  P OP (t).
Proof: Both P OUP (t) and P OP (t) are dened as supremum over utilities ratio for a given
value v. We present a bound for any v, thus it holds for the supremum. The denominator in
both case is the same: it is the optimal utility of the principal in the hidden-actions case with
pure strategies. The numerator in the POP is the optimal principal utility in the hiddenactions case with mixed strategies. Obviously, this is at most the optimal principal utility
in the observable-actions case with mixed strategies. It has already been observed that in
the observable-actions case mixed strategies cannot help the principal (see Section 3), i.e.,
the principal utility with mixed strategies equals the principal utility with pure strategies.
The assertion of the theorem follows by observing that the optimal principal utility with
pure strategies in the observable-action case is the numerator of P OUP .
2
However, this bound is rather weak. To best see this, note that the principals price of
unaccountability for AND might be unbounded (e.g., Babaio et al., 2006b). Yet, as shown
in Section 4.2, P OP (AN D) = 1.
In this section we provide better bounds on technologies with identical costs. We begin
by characterizing the payments for a mixed contract. We show that under a mixed prole,
each agent in the support of the contract is paid at least the minimal payment to a single
agent under a pure prole with the same support, and at most the maximal payment.
For a mixed prole q = (q1 , q2 , . . . , qn ), let S(q) be the support of q, that is, i  S(q)
if and only if qi > 0. Similarly, for a pure prole a = (a1 , a2 , . . . , an ) let S(a) be the
ci
support a. Under the mixed prole q, agent i  S(q) is being paid pi (qi ) = t(1,qi )t(0,q
.
i )
Similarly, under the pure prole a, agent i  S(a) is being paid pi (S(a) \ {i}) = pi (ai ) =
ci
t(S(a))t(S(a)\{i}) , where t(T ) is the success probability when aj = 1 for j  T , and aj = 0
for j 
/ T.
Lemma 5.3 For a mixed prole q = (q1 , q2 , . . . , qn ), and for any agent i  S(q) let Si =
S(q) \ {i} be the support of q excluding i. It holds that
maxT Si pi (T )  pi (qi )  minT Si pi (T )
Proof: We show that for any agent i  S(q), the increase in the success probability from him
exerting eort when some other players play mixed strategies, is a convex combination of
the increases in the success probability
when
in the support play pure strategies.

nthe aagents
i
Recall that: t(q1 , . . . , qn ) = a{0,1}n ( i=1 qi  (1  qi )(1ai ) )t(a1 , . . . , an ).
Let t be the technology t restricted to the support S = S(q), that is, if i1 , . . . , iS are
the agents in S then t (ai1 , ai2 , . . . , aiS ) is dened to be the value of t on a, when aj = 0
for any agent j 
/ S, and aj = aik for j = ik  S. t is dened on mixed strategies in the
expected way. Thus,
i (qi ) = t(1, qi )  t(0, qi )
= t (1, qSi )  t (0, qSi )

 aj
=
(
qj  (1  qj )(1aj ) ) t (1, a) 
a{0,1}|S|1 jSi

=



(





(



a{0,1}|S|1 jSi
a
qj j

 (1  qj )(1aj ) )(t (1, a)  t (0, a))

a{0,1}|S|1 jSi

350

qj j  (1  qj )(1aj ) )t (0, a)
a

fiMixed Strategies in Combinatorial Agency

We conclude that i (qi ) is a convex combination of i (bi ) for b with support S(b) 
Si . Therefore, minT Si (t({i}  T )  t(T ))  i (qi )  maxT Si (t({i}  T )  t(T )).
Thus,
maxT Si 1/(t({i}  T )  t(T )) = 1/minT Si (t({i}  T )  t(T ))
 1/i (qi ) = pi (qi )
 1/maxT Si (t({i}  T )  t(T ))
= minT Si 1/(t({i}  T )  t(T ))
2
In what follows, we consider two general families of technologies with n agents: anonymous technologies and technologies that exhibit decreasing returns to scale (DRS). DRS
technologies are technologies with decreasing marginal contribution (more eort by others
decrease the contribution of an agent). For both families we present a bound of n on the
POP.
We begin with a formal denition of DRS technologies.
Denition 5.4 A technology success function t exhibits (weakly) decreasing returns to
scale (DRS)4 if for every i, and every b  a
t(bi , bi )  t(ai , bi )  t(bi , ai )  t(ai , ai )
Theorem 5.5 For any anonymous technology or a (non-anonymous) technology that exhibits DRS, it holds that P OP (t)  n.
For the proof of this theorem as well as the proofs of all claims that appear later in this
section, see Appendix C. We also prove a bound on the POP for any technology with 2
agents (even not anonymous), and an improved bound for the anonymous case.
Theorem 5.6 For any technology t (even non-anonymous) with 2 agents, it holds that
P OP (t)  2. If t is anonymous then P OP (t)  3/2.
We do not provide bounds for non-anonymous technologies, this is left as an open
problem for future research. We believe that the linear bound for anonymous and DRS
technologies are not tight and we conjecture that there exists a universal constant C that
bounds the POP for any technology. Moreover, our simulations seem to indicate that a nonanonymous OR technology with 2 agents yields the highest possible POP. This motivates
us to explore the POP for the OR technology in more detail.
5.2 POP for the OR Technology
As any OR technology (even non-anonymous) exhibits DRS (see Appendix A.1), this implies
a bound of n on the POP of the OR technology. Yet, for anonymous OR technology we
present improved bounds on the POP. In particular, if  = 1   < 1/2 we can bound the
POP by 1.154....
4. Note that t exhibits DRS if and only if it is submodular.

351

fiBabaioff, Feldman & Nisan

Theorem 5.7 For any anonymous OR technology with n agents:
n

 n  (n  1). (b) POP goes to 1 as n goes
1. If 1 >  >  > 0: (a) P OP  1(1)

to  (for any xed ) or when  goes to 1 (for any xed n  2).
2. If

1
2

>  = 1   > 0: (a) P OP 

to 0 or as  goes to

1
2


2(32 3)

(=
3( 32)

1.154..). (b) POP goes to 1 as  goes

(for any xed n  2).

While the bounds for anonymous OR technologies for the case in which  = 1   are
much better than the general bounds, they are still not tight. The highest POP we were able
to obtain by simulations was of 1.0233 for  > , and 1.0187 for  = 1   (see Section 3),
but deriving the exact bound analytically is left as an open problem.

6. The Robustness of Mixed Nash Equilibria
In order to induce an agent i to truly mix between exerting eort and shirking, pi must
be equal exactly to ci /i (qi ) (see claim 2.1). Even under an increase of  in pi , agent
i is no longer indierent between ai = 0 and ai = 1, and the equilibrium falls apart.
ci
This is in contrast to the pure case, in which any pi  i (a
will maintain the required
i )
equilibrium. This delicacy exhibits itself through the robustness of the obtained equilibrium
to deviations in coalitions (as opposed to the unilateral deviations as in Nash). A strong
equilibrium (Aumann, 1959) requires that no subgroup of players (henceforth coalition)
can coordinate a joint deviation such that every member of the coalition strictly improves
his utility.
Denition 6.1 A mixed strategy prole q  [0, 1]n is a strong equilibrium (SE) if there
does not exist any coalition   N and a strategy prole q  i [0, 1] such that for any
 , q ) > u (q).
i  , ui (q
i

In the work of Babaio et al. (2006b) we show that under the payments that induce the
pure strategy prole S  as the best pure Nash equilibrium (i.e., the pure Nash equilibrium
that maximizes the principals utility), S  is also a strong equilibrium. In contrast to the
pure case, we next show that any non-degenerate mixed Nash equilibrium q in which there
exist at least two agents that truly mix (i.e., i = j s.t. qi , qj  (0, 1)), can never be a strong
equilibrium. This is because if the coalition  = {i|qi  (0, 1)} deviate to q in which each
i   exerts eort with probability 1, each agent i   strictly improves his utility (see
proof in Appendix D).
Theorem 6.2 If the mixed optimal contract q includes at least two agents that truly mix
(i = j s.t. qi , qj  (0, 1)), then q is not a strong equilibrium.
In any OR technology, for example, it holds that in any non-degenerate mixed equilibrium at least two agents truly mix (see lemma 3.2). Therefore, no non-degenerate contract
in the OR technology can be a strong equilibrium.
As generically a mixed Nash contract is not a strong equilibrium while a pure Nash
contract always is, if the pricipal wishes to induce a strong Nash equilibrium (e.g., when
the agents can coordinate their moves), he can restrict himself to inducing a pure Nash
equilibrium, and his loss from doing so is bounded by the POP (see Section 5).
352

fiMixed Strategies in Combinatorial Agency

7. Algorithmic Aspects
The computational hardness of nding the optimal mixed contract depends on the representation of the technology and how it is being accessed. For a black-box access and for
the special case of read-once networks, we generalize our hardness results of the pure case
(Babaio et al., 2006b) to the mixed case. The main open question is whether it is possible
to nd the optimal mixed contract in polynomial time, given a table representation of the
technology (the optimal pure contract can be found in polynomial time in this case). Our
generalization theorems follow (see proofs in Appendix E).

Theorem 7.1 Given as input a black box for a success function t (when the costs are
identical), and a value v, the number of queries that is needed, in the worst case, to nd the
optimal mixed contract is exponential in n.

Even if the technology is a structured technology and further restricted to be the sourcepair reliability of a read-once network (see (Babaio et al., 2006b)), computing the optimal
mixed contract is hard.
Theorem 7.2 The optimal mixed contract problem for read once networks is #P -hard
(under Turing reductions).

8. Conclusions and Open Problems
This paper studies a model in which a principal induces a set of agents to exert eort through
individual contracts that are based on the nal outcome of the project. The focus of this
paper is the question how much the principal can benet when inducing a Nash equilibrium
in mixed strategies instead of being restricted to a pure Nash equilibrium (as was assumed
in the original model). We nd that while in the case of observable actions mixed equilibria
cannot yield the principal a higher utility level than pure ones, this can indeed happen
under hidden actions. Yet, whether or not mixed equilibria improve the principals utility
depends on the technology of the project. We give sucient conditions for technologies
in which mixed strategies yield no gain to the principal. Moreover, we provide bounds on
the principals gain for various families of technologies. Finally, we show that an optimal
contract in non-degenerated mixed Nash equilibrium is not a strong equilibrium (in contrast
to a pure one) and that nding such an optimal contract is computationally challenging.
Our model and results raise several open problems and directions for future work. It
would be interesting to study the principals gain (from mixed strategies) for dierent
families of technologies, such as series-parallel technologies. Additionally, the model can be
extended beyond the binary eort level used here. Moreover, our focus was on inducing some
mixed Nash equilibrium, but that equilibrium might not be unique. One can consider other
solution concepts such as a unique Nash equilibrium or iterative elimination of dominated
strategies. Finally, it might be of interest to study the performance gap between pure and
mixed Nash equilibria in domains beyond combinatorial agency.
353

fiBabaioff, Feldman & Nisan

9. Acknowledgments
Michal Feldman is partially supported by the Israel Science Foundation (grant number
1219/09) and by the Leon Recanati Fund of the Jerusalem school of business administration.

Appendix A. General
Lemma 2.2 ( Monotonicity lemma) For any technology (t, c) the expected utility of
the principal at the optimal mixed contract, the success probability of the optimal mixed
contract, and the expected payment of the optimal mixed contract, are all monotonically
non-decreasing with the value.
Proof: Suppose the proles of mixed actions q 1 and q 2 are optimal for v1 and v2 < v1 ,
respectively. Let P 1 and P 2 be the total payment in case a successful project, corresponding
to the minimal payments that induce q 1 and q 2 as Nash equilibria, respectively. The utility
is a linear function of the value, u(a, v) = t(a)  (v  P ) (P is the total payments in case
of successful project). As q 1 is optimal at v1 , u(q 1 , v1 )  u(q 2 , v1 ), and as t(a)  0 and
v1 > v2 , u(q 2 , v1 )  u(q 2 , v2 ). We conclude that u(q 1 , v1 )  u(q 2 , v2 ), thus the utility is
monotonic non-decreasing in the value.
Next we show that the success probability is monotonic non-decreasing in the value. q 1
is optimal at v1 , thus:
t(q 1 )  (v1  P 1 )  t(q 2 )  (v1  P 2 )
q 2 is optimal at v2 , thus:
t(q 2 )  (v2  P 2 )  t(q 1 )  (v2  P 1 )
Summing these two equations, we get that (t(q 1 )  t(q 2 ))  (v1  v2 )  0, which implies that
if v1 > v2 then t(q 1 )  t(q 2 ).
Finally we show that the expected payment is monotonic non-decreasing in the value.
As q 2 is optimal at v2 and t(q 1 )  t(q 2 ), we observe that:
t(q 2 )  (v2  P 2 )  t(q 1 )  (v2  P 1 )  t(q 2 )  (v2  P 1 )
or equivalently, P 2  P 1 , which is what we wanted to show.
2
We note that the above lemma also holds for the case of proles of pure actions, and
for the observable-actions case (by exactly the same arguments).
Lemma 2.4 For any technology (t, c), the price of purity is obtained at a nite v that is a
transition point between two optimal pure contracts.
Proof: Clearly for a large enough value v  , the ratio is 1, as in both cases all agents exert
maximal eort. For small enough values the principal will choose not to contract with any
agent in both cases (and the ratio is 1). This is true as at a value that is smaller than any
agents cost, an optimal contract is to contract with no agent in both cases. Let v be the
supremum on all values for which the principal will choose not to contract with any agent
in both cases. Now, the ratio is a continuous function on the compact range [v, v  ], thus its
supremum is obtained, for some value that is at most v  .
354

fiMixed Strategies in Combinatorial Agency

We have seen that the POP is obtained at some value v, we next prove that it is obtained
at a transition point of the pure case. If P OP = 1 the claim clearly holds, thus we should
only consider the case that P OP > 1. Let v be the maximal value for which the POP is
obtained. Assume in contradiction that v is not a transition point between two optimal
pure contracts, and that a and q are optimal for the pure and mixed cases, respectively. As
P OP > 1, q is non-degenerate and u(q, v) > u(a, v). Let P (a) and P (q) denote the total
payment in case of success for a and q, respectively. We next consider two options.
We rst consider the case that t(a)  t(q). We show that in this case, the utilities ratio
for v  , for some  > 0 is worse than the utilities ratio for v, and we get a contradiction.
For  > 0 small enough, the optimal pure contract is still a, and u(q, v  ) > 0. Let q be
the optimal mixed contract at v  . It holds that
P OP 

u(q , v  )
u(q, v  )
u(q, v)  t(q)  
u(q, v)

=
>
u(a, v  )
u(a, v  )
u(a, v)  t(a)  
u(a, v)

where the last strict inequality is by the following argument.
u(q, v)  t(q)  
u(q, v)
>
u(a, v)  t(a)  
u(a, v)



t(q)  u(a, v) < t(a)  u(q, v)



P (q) < P (a)

and P (q) < P (a) as u(q, v) = t(q)(v  P (q)) > t(a)(v  P (a)) = u(a, v) and t(a)  t(q).
Next we consider the case that t(q) > t(a). If P (q) < P (a), the argument that was
presented above shows that the utilities ratio for v  , for some  > 0, is worse than the
utilities ratio for v, and we get a contradiction. On the other hand, if P (q)  P (a) we show
that the utilities ratio for v + , for some  > 0, is at least as large as the utilities ratio for
v, in contradiction to v being the maximal value for which the POP is obtained. For  > 0
small enough, the optimal pure contract is still a (as v is not a transition point between
pure contracts). Let q be the optimal mixed contract at v + . It holds that
P OP 

u(q, v + )
u(q, v) + t(q)  
u(q, v)
u(q , v + )

=

u(a, v + )
u(a, v + )
u(a, v) + t(a)  
u(a, v)

where the last inequality is by the following argument.
u(q, v) + t(q)  
u(q, v)

u(a, v) + t(a)  
u(a, v)

 t(q)  u(a, v)  t(a)  u(q, v)



P (q)  P (a)

which holds by our assumption.
2
The following corollary of Lemma 2.4 will be helpful in nding the POP for technologies
with 2 agents.
Corollary A.1 Assume that technology t with 2 agents and with identical costs exhibits
DRS, then the POP is obtained at the transition point of the pure case, to the contract with
both agents.
Proof: By Lemma 2.4 the POP is obtained at a transition point of the pure case. If there
is a single transition point, between 0 agents and 2 agents, the claim holds. If contracting
with a single agent is sometimes optimal, it must be the case that the single agent that is
contracted is the agent with the (possibly weakly) highest success probability (agent i such
355

fiBabaioff, Feldman & Nisan

that t({i})  t({j}) where j = i, which implies that i = t({i})t()  t({j})t() = j ).
Thus we only need to show that the POP is not obtained at the transition point v between
0 agents and the contract with agent i. Assume that q is the optimal mixed contract at
v, and that P (q) is the total payment in case of success. If q gives the same utility as the
contract {i}, we are done.
Otherwise, u(q, v) > u({i}, v) , and by Corollary C.9 it holds that P (q)  c1 , thus
t(q) > t({i}). This implies that the utilities ratio at the value v +  for  > 0 small enough
is worse than the ratio for v (by the argument presented in Lemma 2.4 for the case that
t(q) > t(a)).
2
A.1 Analysis of the OR Technology

Lemma 3.2 For any anonymous OR technology (any  > , c, n) and value v, either the
optimal mixed contract is a pure contract, or, in the optimal mixed contract k  {2, . . . n}
agents exert eort with equal probabilities q1 = . . . = qk  (0, 1), and the rest of the agents
exert no eort.
Proof: First, observe that it cannot by the case that all agents but one exert no eort, and
this single agent mix with probability 0 < qi < 1. This is so as the principal would rather
change the prole to qi = 1 (pays the same, but gets higher success probability). Suppose
by contradiction that a contract that induces a prole (qi , qj , qij ) such that qi , qj  (0, 1]
and qi = qj (qi > qj without loss of generality) is optimal. For agent k, we denote the
probability of failure of agent k in his task by (qk ). That is, (qk ) = 1  (qk  + (1  qk )) =
1   + (  )qk =  + qk where  =   .
We show that for a suciently small  > 0, the mixed prole q  = (qi , qj + (qji ) , qij )
(q )

(qi )
}, ) obtains a better contract, in
(for  such that q   [0, 1]. i.e.,  < min{qi , (1  qi ) (q
j)
contradiction to the optimality of the original contract.


For the OR technology,t(q) = 1  kN (qk ) = 1  (q), where (q) = kN (qk ).
We also denote ij (q) = k=i,j (qk ). The change in success probability is related to the
(
)

new product (qi  )   qj + ji  :

(qi )



(qj )

(

)
(qj )
= (qi  )   qj + 
(qi )
)
(
(qj )
= ((qi )  )  (qj ) + 
(qi )
(qj )
(qj )
= (qi )(qj )  (qj ) + 
(qi )   2 2
(qi )
(qi )
(qj )
= (qi )(qj )   2 2
(qi )
356

fiMixed Strategies in Combinatorial Agency

Therefore the new success probability t(q  ) has increased by the change:
j
, qij )
i
(
)
(qj )
= 1  (qi  )   qj + 
 ij (q)
(qi )
)
(
(qj )
= 1  (qi )(qj )   2 2
 ij (q)
(qi )
(
)
 2 2 (q)
 2 2 (q)
= t(q) +
= t(q) 1 +
((qi ))2
t(q)  ((qi ))2

t(q  ) = t(qi  , qj +

2 2

  (q)

We denote z() = t(q)((q
2 , thus t(q ) = t(q)  (1 + z()), where z() > 0 for any .
i ))
After showing that the success probability increases, we are left to show that for suciently small , the total payment decreases. The payment to agent l is given by:

pl =

c
c  (ql )
c

=
=
t(1, ql )  t(0, ql )
(  ) m=l (qm )
(  )  t(q)

The change in the payment of agent k is
c  (qk )
c  (qk )

(  )  t(q) (  )  t(q  )
(
)
(qk )
c
=
 (qk ) 
t(q)  (  )
(1 + z())
(
)
c
=
 (qk )  (qk ) + (qk )  z()
t(q)  (  )  (1 + z())
(
)
= W ()  (qk )  (qk ) + (qk )  z()

pk  pk =

c
for W () = t(q)()(1+z())
For agent k = i, j, as (qk ) = (qk ) we get pk  pk = W ()  (qk )  z(). For agent i, as
(qi )  (qi ) =  we get pi  pi = W ()  ( + (qi )  z()). For agent j, as (qj )  (qj ) =

 (qji ) we get pj  pj = W ()  ( (qji ) + (qj )  z()).
By summing over all agents we get
(q )

(q )


kN

pk 


kN

pk =



(pk  pk )

kN

= (pi  pi ) + (pj  pj ) +



(pk  pk )

k=i,j

)

(qj )
+ z() 
(qk )
= W ()    
(qi )
kN
( (
)
)

(qj )
= W ()   1 
+ z() 
(qk )
(qi )
(

kN

357

fiBabaioff, Feldman & Nisan

which is positive by the following observations. W () > 0 and z() > 0 for any , and clearly

(qj )
kN (qk ) > 0. Additionally, (1  (qi ) ) > 0 as  =    < 0, and (qi ) < (qj ) as
pi > p j .
To conclude, we have show that the success probability of q  is greater than the success
probability of q, and the payments are lower, thus the utility of the principal increases when
he moves from q to q  , which is a contradiction to the optimality of q.
2
Observation A.2 The OR technology exhibits DRS.
Proof: Let ra , rb  [0, 1]n be two proles of actions, such that rb  ra (for any i, rib  ria ).
b )  t (r a , r b )  t (r b , r a )  t (r a , r a ). Indeed,
We need to show that for every i, ti (rib , ri
i i
i i i
i i
i
i


b
b
ti (rib , ri
)  ti (ria , ri
) = 1  (1  rib ) (1  rjb )  (1  (1  ria ) (1  rjb ))
j=i


= (rib  ria ) (1  rjb )

j=i

j=i



(rib



ria )


(1  rja )
j=i

= 1  (1  rib )



(1  rja )  (1  (1  ria ) (1  rja ))
j=i

=

a
ti (rib , ri
)



j=i

a
ti (ria , ri
)

2

Appendix B. When is Pure Nash Good Enough?
Lemma B.1 Let f : {0, 1}n  {0, 1} for n  2 be a monotone Boolean function that is
not constant and not a conjunction of some subset of the input bits. Then there exist an
assignment to all but two of the bits such that the restricted function is a disjunction of the
two bits.
Proof: By induction on the number of bits the function depends on. The base case is n = 2,
where the only monotone function that is not constant and not a conjunction of some subset
of the input bits is the disjunction of two input bits.
Let xi be a variable on which f depends (which must exist since f is not constant). Let
|x
=a
i
f
= f (a, xi ) denote the function f restricted to xi = a. We denote h = f |xi =0 and
|x
g = f i =1 . As f is monotone, f = x  f |xi =1 + f |xi =0 = g  x + h, where f |xi =1  f |xi =0 (that
is, for any xi , if f (0, xi ) = 1 then f (1, xi ) = 1, and if f (1, xi ) = 0 then f (0, xi ) = 0).
If h is not constant and not a conjunction of some subset of the input bits, then we continue
by induction using h by setting x = 0. Similarly If g is not constant and not a conjunction
of some subset of the input bits, then we continue by induction using g by setting x = 1.
So we are left with the case where both h and g are conjunctions of some subset of
the variables (where the constant 1 is considered to be the conjunction of the empty set of
variables, and it is easy to verify that h and g cannot be the constant 0). Since f depends
on xi , we have that h = g, and since h  g, there exists some variable xj (j = i) that is in
358

fiMixed Strategies in Combinatorial Agency

the set of variables whose conjunction is h but not in that of g. Now set all variables but
xi and xj to 1, and we are left with xi + xj .
2
Theorem B.2 Let f be any monotone Boolean function with n  2 inputs, that is not
constant and not a conjunction of some subset of the input bits. Then there exist parameters
{i , i }ni=1 such that the POP of the structured technology with the above parameters (and
identical cost c = 1) is greater than 1.0233.
Proof: By Lemma B.1 there is an assignment to all but two variables such that the restricted
function over the two variables is an OR function. For these two variables we choose the
parameters according to the worst POP we know of for an OR technology (see Section 3).
For the rest of the variables we choose parameters such that for the value for which the
worst utilities ratio is achieved, all the rest of the agents exert no eort and provide success
probabilities that are (almost) the success probabilities dictated by the assignment. Next
we make this argument formal.
Recall that by Lemma B.1 there is an assignment to all but two variables such that the
restricted function over the two variables is an OR function. Let i1 and i2 be the indices of
these two variables. In Section 3 we have observed that for OR technology with two agents
with values v = 233, 1 = 2 = 0.0001 and 1 = 2 = 0.9, the POP is at least 1.0233.
We embed this into an instance of an OR technology with n agents by considering a value
v = 233 and success probabilities as follows: For agents i1 and i1 , let i1 = i2 = 0.0001 and
i1 = i2 = 0.9. For the rest of the agents, x a suciently small  > 0. Then set i = 1  
and i = 1  2 if i was set to 1 in the assignment, and set i = 2 and i =  if i was set
to 0 in the assignment.
When  > 0 is small enough the payment needed to induce every agent i = i1 , i2 to
exert eort (for any prole of eorts of the others) will be greater than v as it is inversely
proportional to the increase in the success probability due to is eort, and this goes to
zero with . Thus, for a small enough  all agents i = i1 , i2 will not exert eort in the
optimal contract, but each such agent i will provide an almost sure success in the case
the assignment of variable i is 1, and an almost sure failure in the case the assignment of
variable i was zero. The created technology is essentially the same as the OR technology
with agents i1 and i2 with i1 = i2 = 0.0001, i1 = i2 = 0.9, and for the value v = 233 the
POP will be at least 1.0233.
2

Appendix C. Quantifying the Gain by Mixing
C.1 POP for n Agents
We observe that for any technology, the POP is bounded by the ratio between the success
probability when all agents exert eort, and the success probability when none of the agents
exert eort. This simple bound shows that if the success probability when none of the agents
exert eort is at least some positive constant, the POP is bounded by a constant.
Observation C.1 For any technology (t, c) with set of agents N , P OP (t) 
359

t(N )
t() .

fiBabaioff, Feldman & Nisan

Proof: For any given value v, the utility of the principal with the optimal mixed Nash is
at most v  t(N ), while the utility of the principal with the optimal pure Nash is at least
)
t(N )
v  t(), thus the POP is bounded by vt(N
2
vt() = t() .
From this point we only consider technologies with identical costs. The following lemma
shows that anonymous technologies as well as any technology that exhibits DRS have POP
at most n.
Lemma C.2 Assume that for a technology t with n agents the following holds: For any
optimal mixed contract q with support S, there is a pure prole a with support T  S such
that
 t(a) 

t(S)
|S|

 For each agent i  T , and any pure prole b with support R  S it holds that t(1, ai )
t(0, ai )  t(1, bi )  t(0, bi ).
Then the P OP (t)  n.
Proof: We rst observe that P (a), the total payment under the prole a in the case of
success, is at most P (q), the total payment under the prole q. As T  S, the set of agents
that are paid under a is a subset of the set of agents that are paid under q. Each agent in
T is paid at least as much under q, as he is paid under a (by the second condition, as the
increase in success probability under q is a convex combination of the increase in success
probability for pure proles with support R  S). Thus, P (a)  P (q), and U (a) > 0. We
conclude that
t(q)(v  P (q))
t(q)
t(S)
u(q, v)



 |S|
u(a, v)
t(a)(v  P (a))
t(a)
t(a)
where the last inequality is derived from the rst condition. This implies that the POP is
bounded by n.
2
Corollary C.3 For any anonymous technology t with n agents, P OP (t)  n.
Proof: Assume that for the value v the mixed prole q is optimal, and its support is of size
k. Let tm be the success probability if m agents exert eort, and let m = tm  tm1 . Let
m = argmaxmk m .
By the denition of m the second condition holds. The rst condition holds as:
ktm  k(t0 +tm t0 )  t0 +k(tm t0 )  t0 +k(tm tm1 ) = t0 +km  t0 +(tk t0 ) = tk
2
Corollary C.4 For any technology t with n agents that exhibits DRS and has identical
costs, P OP (t)  n.
Proof: Let agent i  S be the agent with maximal individual contribution in S, the support
of q (t({i})  t({j}) for all j  S). DRS ensures that the two conditions of Lemma C.2
hold.
2
The following holds for OR technology with n agents (even non-anonymous), as it exhibits DRS. In particular, even if a single agent has i > 1/2 we get a bound of 2 on the
POP.
360

fiMixed Strategies in Combinatorial Agency

Observation C.5 Assume that the technology t with n agents (with identical costs) exhibits
t(N )
DRS, then P OP (t)  t({i})
, for agent i with maximal individual contribution (t({i}) 
t({j}) for all j  N ).
Proof: Let agent j  S be the agent with maximal individual contribution in S, the support
of q. Following the proof of Lemma C.2, as t({i})  t({j}) and P (q)  P ({j})  P ({i}),
and u(q, v) > 0 ,this implies that u({i}, v)  u({j}, v) > 0. Thus the optimal pure contract
a gives utility of at least u({i}, v) > 0, therefore for any v we have the bound
u(q, v)
u(q, v)
t(q)(v  P (q))
t(S)
t(N )

=


u(a , v)
u({i}, v)
t({i})(v  P ({i}))
t({i})
t({i})
which implies that the POP is bounded by

t(N )
t({i}) .

2

Corollary C.6 For any anonymous technology with n agents that exhibits DRS, it holds
that P OP (t)  ttn1 .
C.2 POP for Anonymous OR
As OR exhibits DRS, the following in a direct corollary of Observation C.5.
Corollary C.7 For any anonymous OR technology with n agents, it holds that P OP (OR) 
tn
t1 .
Theorem 5.7 For any anonymous OR technology with n agents:
n

1. If 1 >  >  > 0: (a) P OP  1(1)
 n  (n  1). (b) POP goes to 1 as n goes

to  (for any xed ) or when  goes to 1 (for any xed n  2).
2. If

1
2

>  = 1   > 0: (a) P OP 

to 0 or as  goes to

1
2


2(32 3)

(=
3( 32)

1.154..). (b) POP goes to 1 as  goes

(for any xed n  2).

Proof: Based on Corollary C.7, P OP 

t(1n )
,
t(1,0n1 )

all the results are based on this bound.

1. Proof of part 1(a):
t(1n )
1  (1  )n
1  (1  )n
1  (1  )n
=

=
t(1, 0n1 )
1  (1  )(1  )n1
1  (1  )

Additionally,


1  (1  )n 
=
(1  )j = 1 +
(1  )j  1 +
(1  ) = n  (n  1)
1  (1  )
n1

n1

n1

j=0

j=1

j=1

and this concludes the proof.
361

fiBabaioff, Feldman & Nisan

2. Proof of part 1(b):
t(1n )
1  (1  )n
=
t(1, 0n1 )
1  (1  )(1  )n1
this expression goes to 1 for any xed  >  > 0, when n goes to , as (1  )n and
(1  )n1 goes to zero.
1(1)n
,


Additionally, we saw that P OP 
goes to 1, the POP goes to 1.

thus it is clear that if n is xed and 

3. Proof of part 2(a): We rst bound the POP for the case of anonymous OR with 2
agents and with  = 1   < 1/2. For this case the POP is bounded by
t(1, 1)
(2  )
= 2
t(0, 1)
 +1

2
The derivative of this ratio is (22
3  1. This is
2 +1)2 , which equals to zero at  =
a maximum point since the second derivative is negative, and the ratio at this point
equals to 1.154... Therefore, t(1,1)
t(1,0)  1.154... Observation C.8 below shows that for
any n  2 it holds that

t(1n )
t(1,0n1 )



t(1,1)
t(0,1)

4. Proof of part 2(b): The expression
or

thus the same bound holds for any n.

t(1n )
t(1,0n1 )

=

1 n
1(1)n1

goes to 1 when  goes to 0

1
2.

2
For anonymous OR technology with n agents and  = 1   < 1/2 we can bound the
POP by 1.154...
Observation C.8 Let ORn, denote the anonymous OR technology of n agents with  =
1   < 1/2. For any k  3 it holds that
P OP (ORk, ) 

ORk, (1k )
ORk1, (1k1 )

ORk, (1, 0k1 )
ORk1, (1, 0k2 )

thus for any k  3 it holds that
P OP (ORk, ) 

ORk, (1k )
OR2, (1, 1)

 1.154...
k1
ORk, (1, 0 )
OR2, (1, 0)

Proof: For the technology ORk, it holds that
ORk, (1k )
1  k
=
ORk, (1, 0k1 )
1    (1  )k1
Thus we need to show that for any k  3
1  k
1   k1

1    (1  )k1
1    (1  )k2
362

fiMixed Strategies in Combinatorial Agency

which holds if and only if
1   k    (1  )k2 +  k+1  (1  )k2  1   k1    (1  )k1 +  k  (1  )k1
which holds if and only if
 k1 (1  ) +   (1  )k2  (1  (1  )) +  k  (1  )k2  ((1  )  )  0
by dividing by  2  (1  ), this holds if and only if
 k3 + (1  )k3 +  k2  (1  )k3 (1  2  )  0
which holds as 1     thus (1  )k3   k3 and  k2  (1  )k3 (1  2  )  0.

2

C.3 POP for 2 Agents
Let us now consider the case that n = 2, and prove a better bound on the POP. We have
shown that the POP for IRS technology is 1. Since an anonymous technology with 2 agents
exhibits either IRS or DRS, we only need to handle the DRS case. Let 1 = t1  t0 and
2 = t2  t1 . Assume that 1 =   2 for some   1 (DRS).
The following is a corollary of Lemma 5.3.
Corollary C.9 For a DRS technology over 2 agents, assume w.l.o.g. that t({1})  t({2})
and denote 1 = t({1})  t(). For any mixed prole q = (q1 , q2 ) it holds that each agent
is paid at least c1 .
Proof: As t({1})  t({2}) it implies that 1 = t({1})  t()  t({2})  t(), and DRS
implies that 1 = t({1})  t()  t({1, 2})  t({1}) and t({2})  t()  t({1, 2})  t({2}),
2
thus Lemma 5.3 implies that each agent is paid at least c1 .
Theorem C.10 For any anonymous technology t with 2 agents, it holds that the P OP (t) 
3/2.
Proof: Let u((q1 , q2 ), v) be the utility of the principal for mixed prole (q1 , q2 ) when his value
for the project is v. Let P (q1 , q2 ) denote the total payment to both agents if the project is
successful. Similarly, let u((a1 , a2 ), v) be the utility of the principal for pure prole (a1 , a2 )
when his value is v.
For a given value v, let (q1 , q2 ) be the optimal mixed contract, and let (a1 , a2 ) be the
u((q1 ,q2 ),v)
optimal pure contract. We show that for any value v it holds that u((a
 3/2, which
1 ,a2 ),v)
is sucient to prove the theorem.
If the optimal mixed prole is a pure prole, the ratio is 1, thus we only need to handle
the case that the prole (q1 , q2 ) is not pure (a non-degenerate mixed contract). In this case,
as u((q1 , q2 ), v) = t(q1 , q2 )(vP (q1 , q2 )) > 0, it holds that vP (q1 , q2 ) > 0. By corollary C.9
this implies that u((1, 0), v) > 0 as P (q1 , q2 )  c1 . Thus u((a1 , a2 ), v)  u((1, 0), v) > 0, so
u((q1 , q2 ), v)
u((q1 , q2 ), v)
t(q1 , q2 )(v  P (q1 , q2 ))
t(q1 , q2 )
t(1, 1)
t2




=
c
u((a1 , a2 ), v)
u((1, 0), v)
t(1, 0)(v  1 )
t(1, 0)
t(1, 0)
t1
363

fiBabaioff, Feldman & Nisan

Now we consider two cases. First we consider the case that t0  2 . In this case
t2
t0 +  1 +  2
2 +   2 +  2
2+
1
3
u((q1 , q2 ), v)

=

=
=1+

u((a1 , a2 ), v)
t1
t0 +  1
2 +   2
1+
1+
2
to replace t0 with 2 we use Lemma C.13.
Next we consider the case that t0 < 2 . In this case we look at the value v  for which
the principal is independent between contracting with 1 or 2 agents. At v = v  it holds
c
that t(1, 0)  (v  c1 ) = t(1, 1)  (v  2c2 ), thus v  2 = v  (t2  t1 ) = t2 2c2  t1  
, thus
2
c


it holds that v = (2 )2 (2  t2  t1 ). For a value v  v it is enough to bound the ratio
1 ,q2 ),v)
while for a value v  v  it is enough to bound the ratio u((q
u((1,1),v) . We bound
each of these ratios separately.
2
1 +2
By Lemma C.13, for the case that 0  t0 < 2 , tt21 = t0 +
 (1+)
= 1 + 1 .
t0 +1
2

For a value v  v
)
(
v  2c1
u((q1 , q2 ), v)
t(q1 , q2 )(v  P (q1 , q2 ))
t2 v  P (q1 , q2 )
1





1
+

u((1, 0), v)
t(1, 0)(v  c1 )
t1
v  c1

v  c1
)
) (
(
1
1
 1  v
1+

c  1  1

u((q1 ,q2 ),v)
u((1,0),v) ,

Now, as

2
t2

v
c

=

2
t0 +(1+)2

1
=
 1  1



2
2 +(1+)2

1
(2
(2 )2

=

1
2+ ,

we conclude that

1
2
2
=

=
2  t2  t1  2
(2  1)  t2
 t2  t1 )  1
1
(2  1)(2 + )

Thus
u((q1 , q2 ), v)

u((1, 0), v)

(

1
1+


) (
 1

v
c

1
 1  1

)

(


1
1+


) (
 1

1
(2  1)(2 + )

)

Lemma C.11 shows that the function on the RHS is bounded by 3/2 for any   1.
1 ,q2 ),v)
Finally, for a value v  v  , it is enough to bound the ratio u((q
u((1,1),v) .
v
u((q1 , q2 ), v)
t(q1 , q2 )(v  P (q1 , q2 ))
=

2c
u((1, 1), v)
t(1, 1)(v  2 )
v

2c
1
2c
2

=

v
v

2c
2
 2c2

Intuitively, as the fraction goes to 1 as  goes to 1, this implies that for suciently small 
the fraction is less than 3/2. Formally,
v

2c
2
 2c2

=1+

v
)
(
1

1+2


2c
2



v

2
(2
(2 )2

2c
2
2c
2

2(1  1 )
1+2
=1+ v
c  2  2

(

1


)


v
c

1

 2  2

1
2(  1)2
2(  1)2
1+
=1+

2

t

t

2


(2  1)t1
 t2  t1 )  2
2
1
2
364

fiMixed Strategies in Combinatorial Agency

1+

2(  1)
(2  1)

2(1)
is
We nd the maximum of the RHS. The derivative by  of 1 + (21)

maximum is obtained at  = 1 +


for  = 1 +

2
2 ),


2
2

2(22 4+1)
.
(21)2 2

The

(it is a maximum as the second derivative is negative

and the maximum is 1 +


2 

(1+ 2)(1+ 22 )

2

< 1.35.

Lemma C.11 For   1 it holds that
(
) (
)
1
1
1+
 1
 3/2

(2  1)(2 + )
Proof:
(
f () =
Let h() =

3+ 3
4 ,

1
1+


22 +33
(21) .

) (
 1

1
(2  1)(2 + )

)

(
=

The derivative of h() by  is

1
1
+2

)


82 +123
,
2 (21)2

22 + 3  3
(2  1)
it has a maximum at

and the value of the maximum is lower than 2.072.
1
We look at  = 8/5. As the function 1  +2
is an increasing function of  (for   1),
(
)
1
we get that for any   , f ()  1  +2  2.072 = 13
18  2.072 < 3/2 To conclude the
proof we show that f () is a decreasing function of , for any    = 8/5. The derivative
of f () by  is
2(24 + 43  42  9 + 3)
2 (2  1)2 (2 + )2
Thus we only need to show that 24 + 43  42  9 + 3 > 0 for any    = 8/5. This
holds as 43 42 = 42 (1) > 0 for any  > 1, and 24 9+3  24 9+3 = 1067
625 > 0
4
3
for any    (as the function 2  9 + 3 has derivative 8  9 which is positive for any
  91/3 /2, thus it is a monotonically increasing function for    = 8/5 > 1.05 > 91/3 /2
).
2
Theorem C.12 For any technology t (even non-anonymous) with 2 agents and identical
costs, it holds that the P OP (t)  2.
Proof: If the technology is anonymous, we have already proved a stronger claim. Assume
that it is not, then w.l.o.g. assume that t(1, 0)  t(0, 1). We have shown that the prole
(0, 1) is never optimal, this implies (by the same argument as we have seen in the case that
the technology is anonymous), that
P OP 

u((q1 , q2 ), v)
u((q1 , q2 ), v)
t(1, 1)


u((a1 , a2 ), v)
u((1, 0), v)
t(1, 0)

If the technology exhibits IRS, then we know that POP=1. To conclude the proof we show
t(1,1)
that if the technology does not exhibits IRS then t(1,1)
t(1,0)  2. Assume that t(1,0) > 2, we
show that the technology exhibits IRS. This is true since

t(1,1)
t(1,0)

> 2 implies:

t(1, 1)  t(1, 0) > t(1, 0)  t(1, 0)  t(0, 0)
365

fiBabaioff, Feldman & Nisan

and as t(1, 0 > t(0, 1) it also holds
t(1, 1)  t(0, 1) > t(1, 1)  t(1, 0)  t(1, 0)  t(0, 1)  t(0, 1)  t(0, 0)
2

which implies IRS.
Lemma C.13 If a  b  0 and x  y > 0 then

a+x
a+y



b+x
b+y .

Appendix D. The Robustness of Mixed Nash Equilibria
Theorem 6.2 If the mixed optimal contract q includes at least two agents that truly mix
(i, j s.t. qi , qj  (0, 1)), then q is not a strong equilibrium.
Proof: Let Q be the support of q (i.e., Q = {i|qi > 0}), and let k = |Q|. Recall that
ci
the optimal payments that induce the strategy prole q are pi = i (q
(where i (qi ) =
i )
t(1, qi )  t(0, qi )) for any i  Q, and pi = 0 for any i  N \ Q. Let  = {i|qi  (0, 1)}
(||  2 by assumption), and consider a deviation of the coalition  into a pure strategy
prole q , in which for any i  , qi = 1. q  denote the new prole (i.e., q  = (q , q )).
We next show that for any i  , ui (q) < ui (q  ), thus q is not resilient to a deviation
by . Since qi  (0, 1), i must be indierent between ai = 0 and ai = 1 (see claim 2.1);
therefore, is utility in q is:
ui (q) = ui (0, qi ) = ci

t(0, qi )
i (qi )

The utility of i in q  is:
ci
ui (q ) = t(q )pi ci = t(q )
ci = ci
i (qi )






(

t(q  )
1
i (qi )

)

(
= ci

t(q  )  t(1, qi ) + t(0, qi )
i (qi )

)

Therefore, ui (q  ) > ui (q) if and only if t(q  )  t(1, qi ) > 0, which holds by the assumption
that ||  2 and the monotonicity of t.
2

Appendix E. Algorithmic Aspects
E.1 Results for the Mixed Case
We next show that in the black box model, exponential number of queries is needed to
determine the optimal mixed contract. We have proved this for the optimal pure contract
(for completeness we present the claim as Theorem E.2, taken from (Babaio et al., 2006a)),
and now show that it also holds for the mixed case.
Theorem 7.1 Given as input a black box for a success function t (when the costs are
identical), and a value v, the number of queries that is needed, in the worst case, to nd the
optimal mixed contract is exponential in n.
Proof: We show that the optimal mixed contract for the technology presented in Theorem E.2 at the value c(k + 1/2) has support exactly T , thus the claim is a direct result of
Theorem E.2.
366

fiMixed Strategies in Combinatorial Agency

Assume that q is optimal mixed contract at the value c(k + 1/2). The support of q must
be of size at most k, otherwise the payment in case of success is at least c(k +1) > c(k +1/2)
(as each agent in the support must be paid at least c), which implies negative utility. If he
support is of size at most k and is not exactly T , then there is at least one agent that is
paid c/ > c(k + 1/2) for suciently small  > 0. Thus in any such case the utility is again
negative.
2
Next we show that for read-one network the optimal mixed contract is #P -hard. It is
based on a theorem from the work of Babaio et al. (2006a) cited as Theorem E.3 below.
Theorem 7.2 The Optimal Mixed Contract Problem for Read Once Networks is #P -hard
(under Turing reductions).
Proof: We use the reduction presented in Theorem E.3. We prove that for x close enough
to 1/2, at the transition point from E to E  {x} of the pure case, the optimal mixed
contract is pure (also E and E  {x}). This implies that we can use the same argument
of Theorem E.3 to calculate the network reliability (which is #P -hard) using an algorithm
for the optimal mixed contract.
Lemma E.1 below presents a generalization of a lemma from the work of Babaio et al.
(2006a) to the mixed case. The lemma implies that at the value v that x is rst entered
to the support of the optimal mixed contract q, the contract for x is optimal for the value
v  t(E). But for a single edge, the only optimal mixed contracts are pure, thus x exerts
eort with probability 1. Additionally, the contract for the original graph (with edges E)
is optimal for the value v  (1  x ), thus for x close enough to 1/2, v is large enough such
that the optimal mixed contract is with all agents exerting eort with probability 1 (pure).
2

Let g and h be two Boolean functions on disjoint inputs and let f = g h (i.e., take
their networks in series). The optimal mixed contract for f for some v, denoted by qS , is
composed of the h-part and the g-part, call them mixed prole for these parts qT and qR
respectively.

Lemma E.1 Let qS be an optimal mixed contract for f = g h on v. Then, qT is an
optimal mixed contract for h on v  tg (qR ), and qR is an optimal mixed contract for g on
v  th (qT ).
The proof is the same as the proof for the pure case, presented in the work of Babaio et al.
(2006b).
E.2 Results from the work of Babaio et al. (2006b) for the Pure Case
The following results are cited from the work of Babaio et al. (2006b), for completeness.
Theorem E.2 Given as input a black box for a success function t (when the costs are
identical), and a value v, the number of queries that is needed, in the worst case, to nd the
optimal contract is exponential in n.
Proof: Consider the following family of technologies. For some small  > 0 and k = n/2
we dene the success probability for a given set T as follows. If |T | < k, then t(T ) = |T |  .
If |T | > k, then t(T ) = 1  (n  |T |)  . For each set of agents T of size k, the technology
tT is dened by t(T ) = 1  (n  |T |)   and t(T ) = |T |   for any T = T of size k.
367

fiBabaioff, Feldman & Nisan

For the value v = c  (k + 1/2), the optimal contract for tT is T (for the contract T the
utility of the principal is about v  c  k = 1/2  c > 0, while for any other contract the utility
is negative).
( n )
 2 sets of size k, then it cannot always
If the algorithm queries about at most n/2
determine the optimal contract (as any
( n of) the sets that it has not queried about might be
the optimal one). We conclude that n/2
 1 queries are needed to determine the optimal
contract, and this is exponential in n.
2
Let t(E) denote the probability of success when each edge succeeds with probability
e . We rst notice that even computing the value t(E) is a hard problem: it is called the
network reliability problem and is known to be #P  hard (Provan & Ball, 1983). Just a
little eort will reveal that our problem is not easier:
Theorem E.3 The Optimal Contract Problem for Read Once Networks is #P -hard (under
Turing reductions).
Proof: We will show that an algorithm for this problem can be used to solve the network
reliability problem. Given an instance of a network reliability problem < G, {e }eE >
(where e denotes es probability of success), we dene an instance of the optimal contract
problem as follows: rst dene a new graph G which is obtained by Anding G with a
new player x, with x very close to 12 and x = 1  x . For the other edges, we let e = e
and e = e /2. By choosing x close enough to 12 , we can make sure that player x will enter
the optimal contract only for very large values of v, after all other agents are contracted (if
we can nd the optimal contract for any value, it is easy to nd a value for which in the
original network the optimal contract is E, by keep doubling the value and asking for the
c
is larger than that
optimal contract. Once we nd such a value, we choose x s.t. 12
x
value). Let us denote x = 1  2x .
The critical value of v where player x enters the optimal contract of G , can be found
using binary search over the algorithm that supposedly nds the optimal contract for any
network and any value. Note that at this critical value v, the principal is indierent between
the set E and E  {x}. Now when we write the expression for this indierence, in terms of
t(E) and ti (E) , we observe the following.
(
t(E)x  v 


iE

c
x  ti (E \ i)

)

(
= t(E)(1x ) v 


iE

c
c

(1  x )  ti (E \ i) t(E)  x

)

if and only if
(1  x )  c
(x )2  v
thus, if we can always nd the optimal contract we are also able to compute the value
of t(E).
2
t(E) =

References
Aumann, R. (1959). Acceptable Points in General Cooperative n-Person Games. In Contributions to the Theory of Games, Vol. 4.
368

fiMixed Strategies in Combinatorial Agency

Babaio, M., Feldman, M., & Nisan, N. (2006a). Combinatorial agency. In ACM EC06,
pp. 1828.
Babaio, M., Feldman, M., & Nisan, N. (2006b). Combinatorial agency. In Full version.
Feldman, M., Chuang, J., Stoica, I., & Shenker, S. (2007). Hidden-Action in Multi-Hop
Routing.. In IEEE JSAC Special Issue on Non-Cooperative Behavior in Networking.
Harsanyi, J. C. (1973). Games with randomly disturbed payos: a new rationale for mixedstrategy equilibrium points. International Journal of Game Theory, 2 (1), 123.
Holmstrom, B. (1982). Moral Hazard in Teams. Bell Journal of Economics, 13, 324340.
Mass-Colell, A., Whinston, M., & Green, J. (1995). Microeconomic Theory. Oxford University Press.
Nisan, N., & Ronen, A. (2001). Algorithmic mechanism design. Games and Economic
Behaviour, 35, 166  196. A preliminary version appeared in STOC 1999.
Nisan, N., Roughgarden, T., Tardos, E., & Vazirani, V. V. (2007). Algorithmic Game
Theory. Cambridge University Press.
Provan, J. S., & Ball, M. O. (1983). The complexity of counting cuts and of computing the
probability that a graph is connected. SIAM J. Comput., 12 (4), 777788.
Strausz, R. (1996). Moral hazard in sequential teams. Departmental Working Paper. Free
University of Berlin.
Winter, E. (2004). Incentives and Discrimination. American Economic Review, 94, 764773.

369

fiJournal of Articial Intelligence Research 38 (2010) 85-133

Submitted 05/09; published 05/10

BnB-ADOPT:
An Asynchronous Branch-and-Bound DCOP Algorithm
William Yeoh

wyeoh@usc.edu

Computer Science Department,
University of Southern California,
Los Angeles, CA 90089, USA

Ariel Felner

felner@bgu.ac.il

Information Systems Engineering,
Deutsche Telekom Labs,
Ben-Gurion University of the Negev,
Beer-Sheva, 85104, Israel

Sven Koenig

skoenig@usc.edu

Computer Science Department,
University of Southern California,
Los Angeles, CA 90089, USA

Abstract
Distributed constraint optimization (DCOP) problems are a popular way of formulating and
solving agent-coordination problems. A DCOP problem is a problem where several agents coordinate their values such that the sum of the resulting constraint costs is minimal. It is often
desirable to solve DCOP problems with memory-bounded and asynchronous algorithms. We introduce Branch-and-Bound ADOPT (BnB-ADOPT), a memory-bounded asynchronous DCOP search
algorithm that uses the message-passing and communication framework of ADOPT (Modi, Shen,
Tambe, & Yokoo, 2005), a well known memory-bounded asynchronous DCOP search algorithm,
but changes the search strategy of ADOPT from best-first search to depth-first branch-and-bound
search. Our experimental results show that BnB-ADOPT finds cost-minimal solutions up to one
order of magnitude faster than ADOPT for a variety of large DCOP problems and is as fast
as NCBB, a memory-bounded synchronous DCOP search algorithm, for most of these DCOP
problems. Additionally, it is often desirable to find bounded-error solutions for DCOP problems
within a reasonable amount of time since finding cost-minimal solutions is NP-hard. The existing bounded-error approximation mechanism allows users only to specify an absolute error bound
on the solution cost but a relative error bound is often more intuitive. Thus, we present two
new bounded-error approximation mechanisms that allow for relative error bounds and implement
them on top of BnB-ADOPT.

1. Introduction
A distributed constraint optimization (DCOP) problem consists of agents, each responsible for taking
on (= assigning itself) a value from its nite domain of values. The agents coordinate their values,
which are subject to constraints. Two agents are constrained if they share a constraint. Each
constraint has an associated constraint cost, which depends on the values of the constrained agents.
A (complete) solution is an assignment of values to all agents, and a partial solution is an assignment
of values to a subset of agents. The solution cost of a (partial or complete) solution is the sum of the
constraint costs of all constraints resulting from the given assignment of values to agents. Solving a
DCOP problem optimally means nding a solution with minimal solution cost and is NP-hard (Modi
et al., 2005).
Formulating agent-coordination problems as constraint optimization (COP) problems, a specic
type of weighted constraint satisfaction problems (Schiex, Fargier, & Verfaillie, 1995; Bistarelli,

c
2010
AI Access Foundation. All rights reserved.

fiYeoh, Felner & Koenig

a1

a2
a3

a1
a4

a2
a3

(a)

a4

a1
0
0
1
1
a2
0
0
1
1

a2
0
1
0
1
a3
0
1
0
1

Constraint Cost
5
8
20
3
Constraint Cost
5
4
3
3

(b)

a1
0
0
1
1
a2
0
0
1
1

a3
0
1
0
1
a4
0
1
0
1

Constraint Cost
5
10
20
3
Constraint Cost
3
8
10
3

(c)

Figure 1: Example DCOP Problem
Montanari, Rossi, Schiex, Verfaillie, & Fargier, 1999), is more general than formulating them as the
more common constraint satisfaction problems (Dechter, 2003). Constraint satisfaction problems
have constraints that are either satised or unsatised. Solving a constraint satisfaction problem
means nding a solution such that all constraints are satised. An example application is the
scheduling of jobs in a job-shop, where constraints express that some jobs can only be performed
by certain machines and some jobs can only be performed after some other jobs. There could
potentially be multiple solutions that satisfy all constraints. However, some solutions might be more
desirable than others. For example, one might prefer the solution with the shortest completion time.
Unfortunately, constraint satisfaction problems cannot capture these preferences. However, COP
problems are able to do so by using the constraint costs to represent the preferences.
DCOP algorithms are better suited compared to COP algorithms for problems that are naturally distributed. As a result, DCOP algorithms have been applied to coordinating unmanned
aerial vehicles (Schurr, Okamoto, Maheswaran, Scerri, & Tambe, 2005), scheduling meetings (Maheswaran, Tambe, Bowring, Pearce, & Varakantham, 2004b; Petcu & Faltings, 2005b; Greenstadt,
Grosz, & Smith, 2007; Zivan, 2008; Yeoh, Varakantham, & Koenig, 2009), coordinating sensor networks (Lesser, Ortiz, & Tambe, 2003; Zhang, Xing, Wang, & Wittenburg, 2003; Modi et al., 2005;
Jain, Taylor, Tambe, & Yokoo, 2009; Stranders, Farinelli, Rogers, & Jennings, 2009; Zivan, Glinton,
& Sycara, 2009), synchronizing trac lights (Junges & Bazzan, 2008), planning truck routes (Ottens
& Faltings, 2008) and managing power distribution networks (Kumar, Faltings, & Petcu, 2009).
It is common to visualize a DCOP problem as a constraint graph where the vertices are the
agents and the edges are the constraints. Most DCOP algorithms operate on a pseudo-tree, which
is a spanning tree of the (completely connected) constraint graph with the property that edges
in the constraint graph connect a vertex with one of its ancestor or descendant vertices in the
constraint tree (Freuder & Quinn, 1985; Bayardo & Miranker, 1995). An edge of the constraint
graph that is not part of the pseudo-tree is a backedge. An agent c is a pseudo-child agent of
agent p if agent c is a descendant agent of agent p in the pseudo-tree and they are constrained
via a backedge. Similarly, agent p is the pseudo-parent agent of agent c. Sibling subtrees represent
independent DCOP subproblems (since no two agents in dierent sibling subtrees share a constraint).
Figure 1(a) shows the constraint graph of an example DCOP problem with four agents that can
each take on value 0 or value 1, Figure 1(b) shows one possible pseudo-tree where the assignments of
values to agents a3 and a4 are independent DCOP subproblems (the dotted line is a backedge), and
Figure 1(c) shows the constraint costs. For our example DCOP problem, a cost-minimal solution
results if all agents take on value 1. The minimal solution cost is 12.
1.1 DCOP Algorithms
We now provide a taxonomy of DCOP algorithms. Figure 2 shows the taxonomy. DCOP algorithms
are divided into two groups: complete and incomplete DCOP algorithms. Complete DCOP algorithms nd cost-minimal solutions while incomplete DCOP algorithms are often faster but typically
nd suboptimal solutions.
86

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

DCOP Algorithms

Incomplete Algorithms
e.g., DBA, DSA, MGM,
k-optimal algorithms

Complete Algorithms

Partially Centralized Algorithms
e.g., OptAPO

Fully Decentralized
Algorithms

Search Algorithms
e.g., SBB, ADOPT,
NCBB, AFB

Inference Algorithms
e.g., DPOP

Figure 2: Taxonomy of DCOP Algorithms
1.1.1 Incomplete DCOP Algorithms
Incomplete DCOP algorithms typically use local search to nd locally optimal solutions and can
thus potentially get trapped in local minima. Nevertheless, since solving DCOP problems optimally
is NP-hard, such DCOP algorithms are desirable for large DCOP problems where nding costminimal solutions might be slow. DBA (Yokoo & Hirayama, 1996), DSA (Fitzpatrick & Meertens,
2003), MGM (Maheswaran, Pearce, & Tambe, 2004a) and the more recent class of k-optimal DCOP
algorithms (Pearce & Tambe, 2007; Bowring, Pearce, Portway, Jain, & Tambe, 2008; Greenstadt,
2009) are examples of incomplete DCOP algorithms.
1.1.2 Complete DCOP Algorithms
Complete DCOP algorithms are generally divided into two groups, namely partially centralized and
fully decentralized DCOP algorithms.
Partially Centralized DCOP Algorithms
Partially centralized DCOP algorithms allow some agents to transfer their constraint information
(= information regarding the constraints that they are involved in) to a central agent for processing. OptAPO (Mailler & Lesser, 2004) is an example of a partially centralized DCOP algorithm
that uses cooperative mediation, where certain agents act as mediators to solve overlapping DCOP
subproblems centrally.
Fully Decentralized DCOP Algorithms
Fully decentralized DCOP algorithms do not have central agents that collect constraint information of other agents that are not constrained with them. Rather, every agent has access to only
its own constraint information. Fully decentralized DCOP algorithms are generally divided into two
groups, namely DCOP inference and search algorithms.
 DCOP inference algorithms: DCOP inference algorithms typically use dynamic programming to propagate aggregated constraint costs from one agent to another agent and thus reduce
87

fiYeoh, Felner & Koenig

DCOP
Algorithm
SBB
ADOPT
NCBB
AFB
BnB-ADOPT

Search
Strategy
DFBnB
best-first
DFBnB
DFBnB
DFBnB

Agent
Operation
sequential & synchronous
concurrent & asynchronous
sequential & synchronous
concurrent & asynchronous
concurrent & asynchronous

Communication
point-to-point with neighbors
point-to-point with neighbors
point-to-point with neighbors
broadcast to all agents
point-to-point with neighbors

Agent
Ordering
chain
tree
tree
chain
tree

Table 1: Properties of DCOP Search Algorithms

the DCOP problem size by one agent at each step. They repeat this procedure until the DCOP
problem size is reduced to only one agent and the solution space (= space of all possible partial solutions) thus cannot be reduced anymore. The sole remaining agent has then sucient
knowledge to nd a cost-minimal solution. DPOP (Petcu & Faltings, 2005b) is an example
of a DCOP inference algorithm. The number of messages sent between agents is only linear
in the number of agents. However, its memory requirements are exponential in the induced
width of the DCOP problem. The induced width depends on the number of backedges in the
pseudo-tree. It can be as large as the number of agents minus one if the constraint graph is
fully connected and every agent is thus constrained with every other agent.
 DCOP search algorithms: DCOP search algorithms use search strategies to search through
the solution space to nd a cost-minimal solution. ADOPT (Modi et al., 2005) uses best-rst
search, and SBB (Hirayama & Yokoo, 1997), NCBB (Chechetka & Sycara, 2006), AFB (Gershman, Meisels, & Zivan, 2009) and our new DCOP search algorithm, BnB-ADOPT, use
depth-rst branch-and-bound search. Their memory requirements are only polynomial in the
number of agents. However, the number of messages sent between agents can be exponential
in the number of agents.
Therefore, both groups of fully decentralized DCOP algorithms are desirable under dierent
conditions as there is a tradeo between space (memory requirements) and time (number of messages
sent).
1.2 Motivation
We now describe the motivation behind our work.
1.2.1 BnB-ADOPT
We study DCOP search algorithms because they can be memory-bounded. This property is important for applications, such as sensor networks, where every agent/sensor has only a xed amount of
memory available. As a result, several DCOP search algorithms, such as SBB, ADOPT, NCBB and
AFB, were developed with this limitation in mind. As described earlier, their memory requirements
are polynomial in the number of agents. Table 1 shows the properties of these DCOP search algorithms as well as the properties of our new DCOP search algorithm, BnB-ADOPT. We now describe
each property in more detail and justify the properties of BnB-ADOPT.
 Search strategy: ADOPT uses best-rst search to search the solution space, while SBB,
NCBB and AFB use depth-rst branch-and-bound (DFBnB) search. Best-rst search repeatedly searches for the next best partial solution until it nds a cost-minimal solution. The next
best partial solution is the cost-minimal partial solution among all partial solutions that have
not yet been found. Depth-rst branch-and-bound search starts by nding a complete (but

88

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

often suboptimal) solution and stores its solution cost as the upper bound. It then continues
to search for a solution whose solution cost is less than the upper bound. It stores the solution
cost of this solution as the upper bound, and the search proceeds until it can no longer nd a
solution whose solution cost is less than the upper bound.
For centralized search, it is known that search problems with depth-bounded search trees can
often be solved faster with depth-rst branch-and-bound search than with memory-bounded
best-rst search because memory-bounded best-rst search algorithms, such as RBFS (Korf,
1993), need to repeatedly reconstruct partial solutions that they purged from memory. Depthrst branch-and-bound search algorithms are memory-bounded but do not suer from this
problem (Zhang & Korf, 1995). Since DCOP problems are search problems with depthbounded search trees, we hypothesize that depth-rst branch-and-bound search might be faster
than best-rst search. Therefore, we decided that BnB-ADOPT should use depth-rst branchand-bound search.
 Agent operation: Agents of SBB and NCBB operate sequentially. Only agents with tokens
are active while the other agents remain idle. Once the token-holding agents are done, they pass
their tokens on and then remain idle. On the other hand, agents of ADOPT and AFB operate
concurrently (= at all times). Agents that operate concurrently might be able to solve DCOP
problems faster than agents that operate sequentially since the former agents can perform
potentially useful computation instead of having to wait for other agents. Therefore, we
decided that all agents of BnB-ADOPT should operate concurrently. Agents of SBB and NCBB
also operate synchronously. Communication between agents is often in form of messages.
Synchronous agents operate in cycles (Modi et al., 2005). A cycle is the time required for an
agent to process all incoming messages in its queue and send all outgoing messages, which
are then processed by the receiving agents in the next cycle (see Section 6.1 for more details).
Therefore, all agents wait until the last agent is done sending its messages before they start
a new cycle. On the other hand, asynchronous agents, such as agents of ADOPT and AFB,
are able to operate independently of each other, which often increases robustness (Silaghi,
Landwehr, & Larrosa, 2004). For example, all synchronous agents are aected if a single
communication link suers from congestion while only a small number of asynchronous agents
are aected. We therefore decided that agents of BnB-ADOPT should operate asynchronously.
 Communication:
DCOP search algorithms such as SBB, ADOPT and NCBB restrict
communication to agents that share constraints. This restriction is motivated by applications
such as sensor networks where communication is restricted to neighboring agents/sensors due
to their limited communication radius. Neighboring sensors share constraints since they need
to coordinate to sense the areas near them. DCOP search algorithms such as AFB do not have
this restriction and allow agents to broadcast messages to all other agents. We decided that
agents of BnB-ADOPT should obey the restrictions of applications such as sensor networks
and thus communicate only with neighboring agents.
 Agent ordering: All DCOP search algorithms mentioned above start with a pre-processing
step that arranges the agents into a pseudo-tree. DCOP search algorithms such as SBB and
AFB arrange the agents into a chain, while ADOPT and NCBB arrange the agents into a tree.
A tree ordering can capture independent DCOP subproblems (represented as sibling subtrees)
while a chain ordering can not. DCOP search algorithms that operate on trees can thus
operate on independent DCOP subproblems independently, while DCOP search algorithms
that operate on chains can not. Therefore, we decided that BnB-ADOPT should arrange
agents into a tree.
ADOPT has all preferred properties mentioned above except that it uses best-rst search. We
therefore introduce BnB-ADOPT, a memory-bounded asynchronous DCOP search algorithm that
89

fiYeoh, Felner & Koenig

uses the message passing and communication framework of ADOPT but changes the search strategy
of ADOPT from best-rst search to depth-rst branch-and-bound search.
1.2.2 Bounded-Error Approximations
Solving DCOP problems optimally is NP-hard, which makes it advantageous to allow users to trade
o solution cost for a smaller runtime. It is also desirable to have the error of the resulting solution
cost be bounded to provide guarantees on the solution cost. ADOPT is, to the best of our knowledge,
the only DCOP search algorithm with this property. Its Absolute Error Mechanism allows its users
to specify an absolute error bound on the solution cost, for example, that the solution cost should
be at most 10 larger than the minimal solution cost. However, it is often much more desirable to
specify a relative error bound on the solution cost, for example, that the solution cost should be
at most 10 percent larger than the minimal solution cost or, equivalently, 1.1 times larger than the
minimal solution cost. This cannot be done with the Absolute Error Mechanism without knowing
the minimal solution cost a priori. Thus, we propose two approximation mechanisms that allow users
to specify a relative error bound on the solution cost, namely the Relative Error Mechanism and the
Weighted Heuristics Mechanism, and implement them on top of BnB-ADOPT. These approximation
mechanisms allow BnB-ADOPT to nd solutions with bounded errors faster than cost-minimal
solutions.
1.3 Experimental Results
We experimentally compare ADOPT, BnB-ADOPT and NCBB on three dierent DCOP problem
types, namely graph coloring problems, sensor network problems and meeting scheduling problems.
Our results show that BnB-ADOPT is up to one order of magnitude faster (measured in the number
of non-concurrent constraint checks and the number of cycles) than ADOPT on a variety of large
DCOP problems. BnB-ADOPT can also be inferred to be faster than SBB since ADOPT is faster
than SBB (Modi et al., 2005). BnB-ADOPT is also as fast as NCBB on most of these DCOP
problems. Our results for the suboptimal variants of BnB-ADOPT show that the Weighted Heuristics
Mechanism dominates both the Absolute Error Mechanism and Relative Error Mechanism.
1.4 Article Structure
This article is organized as follows: We formalize DCOP problems in Section 2 and describe our
DCOP search algorithm, BnB-ADOPT, in Section 3. We describe approximation mechanisms that
allow BnB-ADOPT to nd solutions with bounded error in Section 4. We outline correctness and
completeness proofs of BnB-ADOPT in Section 5. Lastly, we present our experimental evaluations
in Section 6 and our conclusions in Section 7.

2. DCOP Problems
In this section, we formally dene distributed constraint optimization (DCOP) problems and describe
their solution space.
2.1 Definition of DCOP Problems
A DCOP problem is dened by the following elements:
 a nite set of agents A = {a1 , a2 , ..., an };
 a set of nite domains D = {Dom(a1 ), Dom(a2 ), ..., Dom(an )}, where Dom(ai ) is the domain
of possible oating point values of agent ai  A; and

90

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

a1

OR

A

OR

AND

0

0

AND

a

b

OR

a2

a2

OR

B

C

5

AND

OR

AND

8

a3
10 14

a4
3

20

a3
8

8

13

a3

a4
10

3

3

25

a4
7

3

a3
8

23

a4
6

c

AND

10

D

OR

3

AND

d

g

E
h

i

e

F
j

k

(a)

G
l

m

f

H
n

o

I
p

q

J
r

s

K
t

u

v

(b)

Figure 3: AND/OR Search Tree
 a set of binary constraints F = {f1 , f2 , ..., fm }, where each constraint fi : Dom(ai1 ) 
Dom(ai2 )  R+  , species its non-negative constraint cost as a function of the values
of the distinct agents ai1 and ai2 that share the constraint.
The above denition assumes that each agent takes on one value rather than multiple values,
for example, a dierent value for each constraint that it is involved in. These DCOP problems
are more commonly formulated as each agent being responsible for the assignments of values to
multiple variables. However, there exist techniques that reduce such DCOP problems to our DCOP
problems (Burke & Brown, 2006). Thus, we use the terms agent and variable interchangeably. The
above denition also assumes that constraints are binary (= between two agents) rather than n-ary
(= between n agents). One should be able to extend BnB-ADOPT to solve DCOP problems with nary constraints by using the same techniques that were proposed to extend ADOPT to solve DCOP
problems with n-ary constraints (Modi et al., 2005). Additionally, we assume that the messages sent
between agents can be delayed by a nite amount of time but are never lost.
2.2 Search Trees
The solution space of DCOP problems can be visualized with search trees. Traditional search trees
or, synonymously, OR search trees (Marinescu & Dechter, 2009) assign values to agents sequentially.
They do not utilize the fact that the values of agents that belong to independent DCOP subproblems
do not have to be assigned sequentially. AND/OR search trees are based on pseudo-trees and remedy
this issue (Marinescu & Dechter, 2009). Thus, we use AND/OR search trees and refer to them as
search trees in this article. Their depth is bounded by (twice) the number of agents.
Figure 3(a) shows the search tree that is based on the pseudo-tree in Figure 1(b). Figure 3(b)
labels each node of the search tree with an identier to allow us to refer to the nodes easily. Circular
nodes are OR nodes (labeled with upper-case letters) and correspond to agents. For example, the
agent of node C is agent a2 . Left branches of OR nodes correspond to the agents taking on value
0 and right branches correspond to the agents taking on value 1. Square nodes are AND nodes
(labeled with lower-case letters) and correspond to the partial solutions from the root node to those
nodes. For example, the partial solution of node f is {(a1 , 1), (a2 , 1)}. The subtree rooted at an
AND node represents the DCOP subproblem that assumes the partial solution of the AND node.
For example, the subtree rooted at node f represents the DCOP subproblem of assigning values
to agents a3 and a4 given that {(a1 , 1), (a2 , 1)}. The number of independent DCOP subproblems
within this DCOP subproblem is indicated by the number of branches exiting the AND node. For
example, there are two branches exiting node f , indicating that there are two independent DCOP
subproblems, namely of assigning values to agents a3 and a4 . The numbers in the AND nodes
are the delta costs of the nodes. The delta cost of an AND node is dened to be the sum of the
constraint costs of all constraints in its partial solution that involve the agent of its parent OR node.

91

fiYeoh, Felner & Koenig

For example, the partial solution of node v is {(a1 , 1), (a2 , 1), (a4 , 1)}. There are two constraints in
this partial solution, namely the constraint between agents a1 and a2 , which has constraint cost 3,
and the constraint between agents a2 and a4 , which also has constraint cost 3. Since the parent
node of node v is node K with agent a4 , the delta cost of node v is 3, namely the constraint cost of
the latter constraint. The former constraint is not included since it does not involve agent a4 . The
solution cost of a partial solution of an AND node is the sum of the delta costs of all AND nodes
along the branch from the root node to that node. For example, the solution cost of the partial
solution of node v (= 6) is the sum of the delta costs of nodes b, f and v. In our example DCOP
problem, a cost-minimal solution is the union of the partial solutions of nodes t and v (all agents
take on value 1). Thus, the minimal solution cost (= 12) is the sum of the delta costs of nodes b, f ,
t and v.

3. BnB-ADOPT
In this section, we present Branch-and-Bound ADOPT (BnB-ADOPT). We do not describe BnBADOPT as a modication of ADOPT since this approach requires the readers to have an in-depth
understanding of ADOPT. Instead, we give a stand-alone description of BnB-ADOPT that requires
no knowledge of ADOPT, with the intention of creating a self-contained and hopefully easy-to-read
description.
3.1 Search Strategies of ADOPT and BnB-ADOPT
We rst describe centralized versions of the search strategies of ADOPT and BnB-ADOPT and omit
technical details since these are described in more detail in later sections.
3.1.1 Search Strategy of ADOPT
ADOPT (Modi et al., 2005) is a popular DCOP search algorithm (Modi & Ali, 2004; Ali, Koenig,
& Tambe, 2005; Bowring, Tambe, & Yokoo, 2006; Davin & Modi, 2006; Pecora, Modi, & Scerri,
2006; Choxi & Modi, 2007; Silaghi & Yokoo, 2009; Matsui, Silaghi, Hirayama, Yokoo, & Matsuo,
2009) that traverses the search tree in a best-rst search order. We now describe a simplied version
of best-rst search. The complete version can be found in (Marinescu & Dechter, 2007). Bestrst search maintains a list that initially contains only the child AND nodes of the root node. It
repeatedly performs the following operations: It expands the AND node with the smallest solution
cost in the list by removing that node from the list and adding the grandchild AND nodes of that
node into the list. For our example DCOP problem, best-rst search expands the AND nodes in the
search tree in Figure 3 for the rst time in the following order, where the numbers in parentheses
indicate the solution costs of the partial solutions of the expanded nodes: a (0), b (0), f (3), c (5),
v (6), i (8), d (8) and t (9).
Figure 4 shows a simplied trace of ADOPT on our example DCOP problem. ADOPT terminates
after fteen steps with minimal solution cost 12. The numbers in the AND nodes are the delta costs
r
of the nodes. The lower bound LBX
r is an optimistic estimate of the minimal solution cost. It is
the smallest underestimated solution cost, over all solutions. The underestimated solution cost of
a solution is the sum of the delta costs of all AND nodes of that solution whose parent OR node
is the root node or whose grandparent AND node is expanded. For example, the underestimated
solution cost of the solution {(a1 , 1), (a2 , 1), (a3 , 1), (a4 , 1)} is 3 if node b is expanded and nodes f , t
r
and v are not expanded. The upper bound U BX
r is a pessimistic estimate of the minimal solution
cost. It is the solution cost of the solution with the smallest solution cost found so far. ADOPT
r
r
terminates when the upper bound U BX
r is no larger than the lower bound LBX r . In order to be
memory-bounded, ADOPT maintains only one branch of the search tree (shaded grey in the gure)
from the root node to the currently expanded node and thus needs to repeatedly reconstruct nodes

92

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

LBrXr = 0

A

OR

AND

a

b

AND

OR

B

C

OR

c

AND

AND

d

D

OR

g

E
h

i

e

F
j

k

G
l

m

f

H
n

o

I
p

q

r

s

K
t

u

OR

v

AND

0

0

a2

a2

5

AND

J

a3

8
a4

10 14 3

a3
8

a1

OR

AND

OR

8 13 10 3 25 7

AND

a3

0
a2
8

10 14 3

=5
= infinity

0

a4

a3
8

OR

3
a4

8 13 10 3 25 7

3

a1

OR

AND

20
a3

a4

a3

8 23 6 10 3

AND

a3

0

0
a2
8

a4

10 14 3

a3

OR

AND

a3

0

8
a4

10 14 3

a3
8

8

3

a3

a1

AND

OR

8 23 6 10 3

OR

AND

a3

AND

a3

0
a2
8

10 14 3

=8
= infinity

0

a4

a3

0

8

10 14 3

8

8

AND

OR

3

a3

AND

a3
10 14 3

8

= 12
= infinity

0

0
a2

a3

OR

AND

a3

8

8 13 10 3 25 7

10 14 3

8

OR

AND

a3

0
a2
8

10 14 3

a3
8

OR

a3

8 13 10 3 25 7

3
a4

3

a3

8 23 6 10 3

OR

AND

a3

3

8 13 10 3 25 7

OR

AND

a3

0

0
a2
8

a4

10 14 3

a3
8

10 14 3

8

a1

AND

OR

3

a3

8 13 10 3 25 7

AND

a3

0
a2

10 14 3

8

20
a4

a3

8 13 10 3 25 7

OR

AND

0

0
a2

a3
10 14 3

8
a4

a3
8

20
a4

a3

8 13 10 3 25 7

3
a4

3

a3

3
a4

3

a3

a4

8 23 6 10 3

Step 15

Figure 4: Trace of Simplied Memory-Bounded Best-First Search (Centralized ADOPT)

93

a4

8 23 6 10 3

= 12
= 12

0

a3

a4

8 23 6 10 3

Step 14

LBrXr
UBrXr

8
a4

3

a3

a2
5

AND

a4

8 23 6 10 3

a2
5

AND

OR

3
a4

a1 UBrXr = infinity

OR

Step 13
OR

a3

LBrXr = 12

3
a4

8 13 10 3 25 7

Step 12

20
a4

OR

AND

20
a3

a4

a4

8 23 6 10 3

a2
5

AND

a4

= 12
= infinity
0

8

3

a3

Step 11

LBrXr
UBrXr

a2

a3

3
a4

a1 UBrXr = infinity

OR

8 23 6 10 3

0

a4

a3

LBrXr = 8

a3

a2
5

AND

a4

20
a4

OR

3
a4

8 13 10 3 25 7

a1

a4

8 23 6 10 3

0

a4

AND

20

OR

3

a3

a2
5

AND

a4

=8
= infinity

a3

a4

3
a4

Step 8

LBrXr
UBrXr

0

a3

a3

a1 UBrXr = infinity

OR

8 23 6 10 3

a2

a4

AND

20
a4

8

20
a4

Step 10

LBrXr
UBrXr

8
a4

3

0

5

AND

a4

8 23 6 10 3

a2
5

AND

OR

10 14 3

a3

LBrXr = 8

a3

a2

Step 9
a1

8
a4

OR

3
a4

8 13 10 3 25 7

a1

OR

3
a4

8 13 10 3 25 7

OR

AND

a3

AND

20
a3

a4

OR

AND

20
a3

a4

OR

0
a2

Step 5

a2

a3

a4

8 23 6 10 3

0

5

AND

a4

8 23 6 10 3

0

a4

3

a3

a2

Step 7

a2
5

AND

OR

3

a3

a2
5

AND

a4

3
a4

a1 UBrXr = infinity

OR

a1 UBrXr = infinity

OR

a3

8 13 10 3 25 7

OR

3
a4

8 13 10 3 25 7

Step 6
LBrXr
UBrXr

20
a4

LBrXr = 8

3
a4

8 13 10 3 25 7

OR

8

AND

20
a3

a4

OR

AND

20
a3

a4

10 14 3

a3

Step 4

a2
5

AND

OR

8
a4

LBrXr = 8

=5
= infinity

a2

LBrXr = 8

OR

AND

a3

Step 2

LBrXr
UBrXr

0

5

AND

a4

a1 UBrXr = infinity

AND

OR

0
a2

5

AND

a4

8 23 6 10 3

a2

Step 3
OR

3

a3

0
a2

Step 1

a2
5

AND

OR

OR

3
a4

a1 UBrXr = infinity

OR

AND

20
a3

a4

Identifiers
LBrXr
UBrXr

LBrXr = 3

a1 UBrXr = infinity

OR

fiYeoh, Felner & Koenig

LBrXr = 0

A

OR

AND

a

b

AND

OR

B

C

OR

c

AND

AND

d

D

OR

g

E
h

i

e

F
j

k

G
l

m

f

H
n

o

I
p

q

r

s

K
t

u

OR

v

AND

0

0

a2

a2

5

AND

J

a3

8
a4

10 14 3

a3
8

a1

OR

AND

OR

AND

a3

0
a2
8

10 14 3

=0
= infinity

0

a4

a3

8 13 10 3 25 7

8

AND

OR

3
a4

8 13 10 3 25 7

3

a1

OR

20
a3

a4

a1

AND

OR

a3

AND

a3

OR

AND

a3

8

8 13 10 3 25 7

8

10 14 3

a3

a1

AND

OR

8

AND

a3

3

a3

8 23 6 10 3

OR

AND

a3
10 14 3

8

0
a2

10 14 3

8

AND

a3

0
a2
8

a4

10 14 3

a3
8

OR

a3

8 13 10 3 25 7

3
a4

3

a3

a4

8 13 10 3 25 7

OR

3

a3

8 23 6 10 3

OR

AND

a3

8

10 14 3

8

OR

AND

0

0
a2

a3

8
a4

10 14 3

a3
8

20
a4

3

a3

a4

8 13 10 3 25 7

3

a3

a4

8 23 6 10 3

OR

3
a4

3

a1 UBrXr = 18

AND

20
a3

8 13 10 3 25 7

Step 9

a4

8 23 6 10 3

LBrXr = 12

0

a4

3

a3

a2

OR

a2

a3

a4

Step 8

LBrXr = 12
UBrXr = 18

0

a4

3

a3

8 13 10 3 25 7

5

AND

a4

8 23 6 10 3

a2
5

AND

a4

20
a4

a1 UBrXr = 18

AND

3

a3

a1

AND

20
a4

OR

0

OR

20

OR

a4

8 23 6 10 3

LBrXr = 0

=0
= 18
0

a4

3

a3

Step 5

LBrXr
UBrXr

8

a4

a2
5

AND

a4

8 23 6 10 3

a2

a3

3

a3

8 13 10 3 25 7

Step 7

0

a3

3

0

a4

20
a4

a1 UBrXr = 18

OR

a3

a2
5

AND

a4

LBrXr = 3
UBrXr = 18

8
a4

8

AND

3
a4

8 13 10 3 25 7

a1

OR

3
a4

a2
5

AND

OR

10 14 3

a3

OR

20
a3

a4

Step 6
OR

8
a4

LBrXr = 0

=0
= infinity
0

a4

AND

20
a3

a4

a3

Step 2

LBrXr
UBrXr

a2

OR

0

8

10 14 3

=0
= 18

a2

a3

AND

0
a2

Step 4

LBrXr
UBrXr

0

a4

OR

0
a2
5

AND

a4

8 23 6 10 3

0

5

AND

a4

8 23 6 10 3

a2
5

AND

OR

3

a3

a2

Step 3
OR

OR

Step 1

a2
5

AND

OR

AND

3
a4

a1 UBrXr = infinity

OR

20
a3

a4

Identifiers
LBrXr
UBrXr

LBrXr = 0

a1 UBrXr = infinity

OR

a3

8 23 6 10 3

Step 10

OR

AND

0
a2

5

AND

a4

0
a2

a3
10 14 3

8
a4

a3
8

20
a4

a3

8 13 10 3 25 7

3
a4

3

a3

a4

8 23 6 10 3

Step 11

LBrXr = 12

a1 UBrXr = 12

OR

AND

OR

AND

0
a2

5

AND

OR

0
a2

a3
10 14 3

8
a4

a3
8

20
a4

a3

8 13 10 3 25 7

3
a4

3

a3

a4

8 23 6 10 3

Step 12

Figure 5: Trace of Simplied Depth-First Branch-and-Bound Search (Centralized BnB-ADOPT)
that it purged from memory. For example, in Step 3, ADOPT has the branch to node f in memory.
The next node that best-rst search expands is node c, and ADOPT discards the branch to node f
in Step 4. In Steps 6 and 7, it then needs to reconstruct the discarded branch to node f in order to
expand node v in Step 8.
3.1.2 Search Strategy of BnB-ADOPT
We now describe a simplied version of depth-rst branch-and-bound search. The complete version
r
r
can be found in (Marinescu & Dechter, 2009). We use the same denitions of LBX
r and U BX r as
described earlier for Figure 4. Depth-rst branch-and-bound search maintains a stack that initially
contains only the child AND nodes of the root node. It expands the AND node on top of the

94

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

stack by removing that node from the stack and performing the following check. If the solution
r
cost of that node is no smaller than the upper bound U BX
r , it prunes that node and repeats the
operation. Otherwise, it adds the grandchild AND nodes of that node to the top of the stack and
r
repeats the operation. It terminates when the upper bound U BX
r is no larger than lower bound
r
LBX r . Depth-rst branch-and-bound search can add the grandchild AND nodes of an expanded
AND node (and the child AND nodes of the root node) in decreasing order of their solution costs
instead of a random order to the top of the stack. This ordering ensures that depth-rst branchand-bound search expands the grandchild AND node with the smallest solution cost rst. We use
this improvement throughout the article. For our example DCOP problem, depth-rst branch-andbound search expands the AND nodes in the search tree in the following order, where it prunes the
nodes in brackets: a (0), c (5), i (8), j (13), g (15), [h (19)], d (8), n (11), k (16), [m (18)], [l (21)],
b (0), f (3), v (6) and t (9). Figure 5 shows a trace of depth-rst branch-and-bound search for our
example DCOP problem. It is memory-bounded without having to repeatedly reconstruct nodes
that it purged from memory but expands some nodes that a best-rst search does not expand, such
as node j in Step 4. The depth-rst branch-and-bound search terminates after twelve steps with
minimal solution cost 12, which is three steps fewer than ADOPT.
3.2 Description of BnB-ADOPT
We now provide an incremental description of BnB-ADOPT. First, we provide the notations and key
terms of BnB-ADOPT. Then, we describe how BnB-ADOPT updates its bounds, adheres to memory
limitations, performs depth-rst search and performs branch-and-bound. Finally, we introduce our
enhanced nal version of BnB-ADOPT and show both its pseudocode and its trace for our example
DCOP problem.
3.2.1 Notation and Key Terms
We adopt the following notation from ADOPT to describe BnB-ADOPT:
 V alInit(a)  Dom(a) is the initial value of agent a  A;
 CD(a)  A is the set of child and pseudo-child agents of agent a  A;
 C(a)  CD(a) is the set of child agents of agent a  A;
 pa(a)  A is the parent agent of agent a  A except for the root agent;
 P (a)  A is the set of ancestor agents (including the parent agent) of agent a  A;
 SCP (a)  P (a) is the set of ancestor agents (including the parent agent) of agent a  A that
are parent or pseudo-parent agents of agent a or one (or more) of its descendant agents; and
 CP (a)  SCP (a) is the set of ancestor agents (including the parent agent) of agent a  A
that are parent or pseudo-parent agents of agent a.
We adopt the following key terms from ADOPT to describe BnB-ADOPT:
 Context (X): The context X a of agent a is the set of values of all ancestor agents of agent
a. The context X r of the root agent r is always equal to {}.
a
 Delta cost (): The delta cost X
a (d) is the sum of the constraint costs of all constraints
that involve both agent a and one of its ancestor agents, under the assumption that agent a
takes on value d and its ancestor agents take on the values in context X a . In the search tree,
a
a
X
 (a, d). For example,
a (d) is the delta cost of the AND node that has partial solution X
a2
{(a1 ,1)} (1) is the delta cost of node f in Figure 3.

95

fiYeoh, Felner & Koenig

a
a
 Gamma cost (): The gamma costs X
a (d) and X a are dened as follows:

a
a
X
a (d) := X a (d) +



c
X
a (a,d)

(1)

cC(a)
a
X
a :=

min

a
{X
a (d)}

dDom(a)

(2)

a
for all agents a, all values d and all contexts X a . Thus, the gamma cost X
a (d) is the sum of
the constraint costs of all constraints that involve agent a or one of its descendant agents (that
is, either both agent a and one of its ancestor agents, both agent a and one of its descendant
agents, both a descendant agent and an ancestor agent of agent a or two descendant agents of
agent a) minimized over all possible values of its descendant agents, under the assumption that
agent a takes on value d and its ancestor agents take on the values in context X a . In the search
a
a
 (a, d). For
tree, X
a (d) is the gamma cost of the AND node that has partial solution X
a2
a
example, {(a1 ,1)} (1) is the gamma cost of node f in Figure 3. The gamma cost X
a is the sum
of the constraint costs of all constraints that involve agent a or one of its descendant agents
minimized over all possible values of agent a and its descendant agents, under the assumption
that the ancestor agents of agent a take on the values in context X a . In the search tree, the
a
gamma cost X
a is the gamma cost of the OR node whose agent is agent a and whose parent
a2
is the gamma cost of node C in
AND node has partial solution X a . For example, {(a
1 ,1)}
Figure 3. Therefore, the gamma cost of an AND node is the sum of its delta cost and the
gamma costs of its child OR nodes, and the gamma cost of an OR node is the minimum of
the gamma costs of its child AND nodes. For example, the gamma cost of node f in Figure 3
is the sum of its delta cost and the gamma costs of nodes J and K, and the gamma cost of
node C in Figure 3 is the minimum of the gamma costs of nodes e and f .
r
Solving a DCOP problem optimally means to determine X
r for the root agent r or, equivalently,
r
the gamma cost of the root node since X r is the minimal solution cost. It is not dicult for the
agents to cache information that allows them to determine a cost-minimal solution.

3.2.2 Updating the Bounds
Every agent a of BnB-ADOPT stores and updates several bounds on the gamma costs, namely
a,c
a
a
a
a
lba,c
X a (d), LBX a (d), LBX a , ubX a (d), U BX a (d) and U BX a for all values d, all child agents c and all
a
contexts X , maintaining the following bound property:
a
LBX
a 
a
LBX
a (d)
a,c
lbX a (d)




a
X
a
a
X
a (d)
c
X
a (a,d)

a
 U BX
a

(3)




(4)
(5)

a
U BX
a (d)
a,c
ubX a (d)

In the search tree,
a
a
 LBX
a and U BX a are lower and upper bounds, respectively, (on the gamma cost) of the OR
node whose agent is agent a and whose parent AND node has partial solution X a ;
a
a
 LBX
a (d) and U BX a (d) are lower and upper bounds, respectively, (on the gamma cost) of the
AND node that has partial solution X a  (a, d); and
a,c
 lba,c
X a (d) and ubX a (d) are lower and upper bounds, respectively, (on the gamma cost) of the
OR node whose agent is agent c and whose parent AND node has partial solution X a  (a, d).

96

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

a2
a2
a2
For example, LB{(a
and U B{(a
are bounds of node C in Figure 3, LB{(a
(1) and
1 ,1)}
1 ,1)}
1 ,1)}
a2 ,a3
a2 ,a3
a2
U B{(a1 ,1)} (1) are bounds of node f , and lb{(a1 ,1)} (1) and ub{(a1 ,1)} (1) are bounds of node J.
a3
a3
2 ,a3
2 ,a3
lba{(a
(1), uba{(a
(1), LB{(a
and U B{(a
are bounds of node J, but agent
1 ,1)}
1 ,1)}
1 ,1),(a2 ,1)}
1 ,1),(a2 ,1)}
a2 maintains the rst two bounds while agent a3 maintains the last two bounds.
Each agent a uses the following update equations for all values d, all child agents c and all
a,c
a,c
contexts X a to initialize its bounds lba,c
X a (d) and ubX a (d), where the heuristic values hX a (d) are
a,c
c
oating point numbers that are admissible and thus satisfy 0  hX a (d)  X a (a,d) :

a,c
lba,c
X a (d) := hX a (d)

uba,c
X a (d)

:= 

(6)
(7)

Agent a then uses repeatedly the following update equations for all values d, all child agents c,
all contexts X a and all contexts X c (= X a  (a, d)) to tighten the bounds:
a,c
c
lba,c
X a (d) := max{lbX a (d), LBX c }
 a,c
a
a
LBX
lbX a (d)
a (d) := X a (d) +

(8)
(9)

cC(a)
a
LBX
a :=

uba,c
X a (d) :=
a
U BX
a (d)

:=

a
U BX
a :=

a
min {LBX
a (d)}
dDom(a)
c
min{uba,c
X a (d), U BX c }

a
X
uba,c
a (d) +
X a (d)
cC(a)

min

a
{U BX
a (d)}

dDom(a)

(10)
(11)
(12)
(13)

The updates maintain the bound property and improve the bounds monotonically, that is,
the lower bounds are monotonically non-decreasing and the upper bounds are monotonically nona
a
a
increasing.1 After a nite amount of time, U BX
a  LBX a for all agents a and all contexts X .
r
r
BnB-ADOPT terminates when its termination condition U BX r  LBX r for the root agent r is
r
r
r
r
satised. Then, U BX
r  LBX r and the bound property U BX r  LBX r together imply that
r
r
r
U BX r = X r = LBX r , and the DCOP problem is solved optimally.
Figure 6 shows a simplied trace of the updates of the (lower and upper) bounds for our example
DCOP problem. We assume that the updates proceed sequentially from the leaf agents to the root
agent. Due to this simplication, the lower and upper bounds of each node are identical to its
gamma cost and independent of the heuristic values. The numbers in the nodes are their bounds.
Two agents maintain the bounds of OR nodes except for the root node. The gure shows the bounds
that the parent agent maintains rather than the bounds that the child agent maintains. For example,
the number in node B is the bounds that agent a1 rather than agent a2 maintains. The bounds
that the child agent maintains can be computed by taking the minimum of the bounds of the child
AND nodes of the OR node. Agents update the bound of an AND node to the sum of its delta cost
and the bounds of its child OR nodes according to update equations 9 and 12. They update the
bound of an OR node to the minimum of the bounds of its child AND nodes according to update
equations 10 and 13. A more detailed description of the trace is as follows:
 Step 1: Leaf agent a3 updates the bounds of AND nodes g, h, k, l, o, p, s and t to their
delta costs according to update equations 9 and 12 and the bounds of OR nodes D, F , H and
1. Leaf agents use the same update equations. Since they do not have child agents, the sums over their child agents
a (d) = U B a (d) =  a (d) for all leaf agents a, all values d and all contexts X a .
evaluate to 0. For example, LBX
a
Xa
Xa

97

fiYeoh, Felner & Koenig

A

OR

0

OR

0

OR

AND

a

b

AND

0

0

AND

0

OR

B

C

OR

0

0

OR

0

c

AND

D

OR

AND

d

g

E
h

i

e

F
j

k

G
l

m

f

H
n

o

I
p

q

J
r

s

K
t

0

AND

u

OR

v

AND

0

0
0

10 14 3

0
8

0
0

0

8 13 10 3 25 7

Identifiers

0
0
3

0

8 23 6 10 3

Step 1

18

OR

18

10 14 3

3

8
8

30
3

7

8 13 10 3 25 7

12
3

3

6

3

8 23 6 10 3

Step 2

12
12

18

AND

AND

AND

10

19

12

OR

AND

OR

OR

0

18

AND

0

0

10
10 14 3

19
3

8
8

30
3

7

8 13 10 3 25 7

12
3

3

6

3

8 23 6 10 3

Step 3

Figure 6: Simplied Trace of the Updates of the (Lower and Upper) Bounds
J to the minimum of the bounds of their child AND nodes according to update equations 10
and 13. Similarly, leaf agent a4 updates the bounds of AND nodes i, j, m, n, q, r, u and
v to their delta costs according to update equations 9 and 12 and the bounds of OR nodes
E, G, I and K to the minimum of the bounds of their child AND nodes according to update
equations 10 and 13. The bounds of OR nodes D to K are not shown in the gure since they
are not (yet) maintained by agent a2 .
 Step 2: Agent a2 updates the bounds of OR nodes D to K that it maintains to the bounds
of the same OR nodes that leaf agents a3 and a4 maintain according to update equations 8
and 11, the bounds of AND nodes c to f to the sum of their delta costs and the bounds of their
child OR nodes according to update equations 9 and 12 and the bounds of OR nodes B and
C to the minimum of the bounds of their child AND nodes according to update equations 10
and 13. The bounds of OR nodes B and C are not shown in the gure since they are not (yet)
maintained by agent a1 .
 Step 3: Agent a1 updates the bounds of OR nodes B and C that it maintains to the bounds
of the same OR nodes that agent a2 maintains according to update equations 8 and 11, the
bounds of AND nodes a and b to the sum of their delta costs and the bounds of their child OR
nodes according to update equations 9 and 12 and the bounds of OR node A to the minimum
of the bounds of its child AND nodes according to update equations 10 and 13. Since the
lower and upper bounds of a node are equal to its gamma cost, the lower and upper bounds
of the root node are equal to its gamma cost, which in turn is equal to the minimal solution
cost. The propagation terminates after three steps with minimal solution cost 12.
3.2.3 Adhering to Memory Limitations
Our description of BnB-ADOPT so far assumes no memory limitations. However, BnB-ADOPT is
a memory-bounded DCOP search algorithm with memory requirements per agent that are linear
in the number of agents. We now describe how BnB-ADOPT adheres to these memory limitations
using techniques that were introduced for ADOPT but apply to BnB-ADOPT as well.
The simplied trace in Figure 6 assumes that every agent a maintains its bounds for all values d,
all child agents c and all contexts X a . The number of contexts can be exponential in the depth of the
agent in the pseudo-tree. For our example DCOP problem, agent a3 has four dierent contexts for
the four dierent combinations of values of its ancestor agents a1 and a2 . An agent cannot maintain
98

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

an exponential number of bounds due to the memory limitations. Therefore, every agent maintains
its bounds for only one context at any given time. This context is stored in the variable X a for
agent a. The size of the context is at most linear in the number of agents. The number of bounds of
an agent is now linear in the product of its domain cardinality and the number of its child agents.
Thus, the memory requirements per agent are only linear in the number of agents if the domain
cardinality and the magnitude of the bounds (and the other variables) are constant for each agent.
3.2.4 Performing Depth-First Search
Our description of BnB-ADOPT so far applies to ADOPT as well. However, BnB-ADOPT uses
depth-rst branch-and-bound search and ADOPT uses best-rst search. We now describe how
BnB-ADOPT implements depth-rst search.
Agents of BnB-ADOPT send messages that are similar to that of ADOPT but processes them
dierently. They send messages of three dierent types, namely VALUE, COST and TERMINATE
messages. At the start, every agent a initializes its context X a , uses update equations 6, 9, 10, 7, 12
a
and 13 to initialize its bounds and takes on its best value da := arg mindDom(a) {LBX
a (d)}. It sends
VALUE messages to all child agents and a COST message to its parent agent. It then repeatedly
waits for incoming messages, processes them, possibly takes on a dierent value and again sends
VALUE messages to all child agents and a COST message to its parent agent. A description of the
three message types and how agents process them is as follows:
 VALUE messages: An agent a with context X a and value da sends VALUE messages to
all child agents with the desired context X a  (a, da ), which is its context augmented with its
value. Leaf agents do not have child agents and thus do not send VALUE messages. VALUE
messages thus propagate contexts down the pseudo-tree.
When an agent receives a VALUE message, it checks whether its context is identical to the
desired context in the VALUE message. If it is not, then the agent changes its context to the
desired context in the VALUE message. In either case, it then executes the common program
(see below).
 COST messages: An agent a sends COST messages to its parent agent with its identity
a
a
a, its context X a and its bounds LBX
a and U BX a . The root agent does not have a parent
agent and thus does not send COST messages. COST messages thus propagate bounds up the
pseudo-tree.
When an agent receives a COST message, it checks whether its context and the context in the
COST message are compatible. Two contexts are compatible if no agent takes on dierent
values in the two contexts. If they are, then the agent uses update equations 8 to 13 with the
bounds in the COST message to improve its bounds for the value in the message. In either
case, it then executes the common program (see below).
r
r
 TERMINATE messages: When the termination condition U BX
r  LBX r is satised,
the root agent r sends TERMINATE messages (without parameters) to all child agents to
inform them that the search is complete and then terminates. When an agent receives such
a TERMINATE message, it sends TERMINATE messages to all child agents and terminates
as well. Leaf agents do not have child agents and thus do not send TERMINATE messages.
TERMINATE messages thus propagate down the pseudo-tree until all agents terminate.

The common program is as follows:
 Context change: If an agent a changed its context X a , it executes the following statements:
It uses update equations 6, 9, 10, 7, 12 and 13 to initialize its bounds and takes on its best
a
value da := arg mindDom(a) {LBX
a (d)}. It then sends VALUE messages to all child agents
and a COST message to its parent agent.
99

fiYeoh, Felner & Koenig

A

OR

0

OR

0

OR

AND

a

b

AND

0

0

AND

5

OR

B

C

OR

0

0

OR

5

c

AND

D

OR

AND

d

g

E
h

i

e

F
j

k

G
l

m

f

H
n

o

I
p

q

J
r

s

K
t

5

AND

u

OR

v

AND

8

0

0

10 14 3

8

X

0

X

X

X

X

X X

X X

X X

X X

X X

X X

OR

AND

0

18

AND

0

Identifiers

10

8
3

10 14 3

8

X

X

X

X

X X

X X

X X

X X

X X

X X

0

OR

AND

8

0

AND

8

0

AND

18

OR

8

0

OR

8

0

OR

18

18

AND

OR

AND

8

10

3

X X

X X

0

X
0

8 13 10 3

X

18

AND

X

X

X

X

X X

X X

X X

X X

OR

AND

19

10

3

X X

X X

8

8 13 10 3

Cycle 3

X
X

X

X

X X

X X

X X

X X

OR

AND

10

19
3

10 14 3

8

X

X

X

X

X X

X X

X X

X X

X X

X X

3

OR

18

0

AND

18

3

AND

18

OR

18

0

OR

18

3

OR

18

X

OR

AND

10
10 14 3

X
3
8

20

3

X

AND

X

X

0

0

0

0

X X

X X

X X

X X

X X

X X

OR

AND

X

20

X

X

X

X

0

X X

X X

X X

X X

X X

Cycle 6

3
0

0

0

X X 23 6 10 3

OR

AND

3
X

20

X

X

X

X

0

X X

X X

X X

X X

X X

12
0

6

3

X X 23 6 10 3

Cycle 8

12

OR

AND

18

OR

18

12
12

X

AND

AND

3

X

AND

Cycle 7

OR

X

3

Cycle 5

3

OR

X

8

AND

AND

0

18

AND

X

0

Cycle 4

0

OR

X
3

X

0

Cycle 2

0

OR

X

0

Cycle 1

0

OR

X

0

X

20

X

X

X

X

0

X X

X X

X X

X X

X X

12
0

6

3

X X 23 6 10 3

Cycle 9

Figure 7: Trace of the Updates of the Lower Bounds
 No context change: If an agent a did not change its context X a , it executes the following
a
a
a
a
statements: If U BX
a  LBX a (d ) for its value d , then the context of the agent augmented
with its value cannot be completed to a solution whose solution cost is smaller than the solution
a
cost of the best solution found so far for its context X a (= U BX
a ) and the agent thus takes on
a
a
its best value d := arg mindDom(a) {LBX a (d)}. It then sends VALUE messages to all child
agents and a COST message to its parent agent.
Assume that the context X a of an agent a does not change. After a nite amount of time,
a
a
a
a
U BX
The agent then takes on its best value and repeats the
a  LBX a (d ) for its value d .
a
a
procedure. After a nite amount of time, U BX
a  LBX a (d) for all values d, which implies that
a
a
a
a
a
U BX a  LBX a . The agent takes on every value d at most once until U BX
a  LBX a since LBX a (d)
a
remains unchanged and U BX a is monotonically non-increasing once the agent changes its value
from d to a dierent value, which prevents the agent from changing its value back to d before
a
a
U BX
a  LBX a . BnB-ADOPT thus performs depth-rst search. Then, after a nite amount of time,
r
r
r
r
r
r
r
U BX r  LBX r and the bound property U BX
r  LBX r together imply that U BX r = X r = LBX r
for the root agent r, and the DCOP problem is solved optimally.
Figures 7 and 8 show traces of the updates of the lower and upper bounds, respectively, for our
example DCOP problem. BnB-ADOPT uses the zero heuristic values. The initial context of every
100

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

A

OR

inf

OR

inf

OR

AND

a

b

AND

inf

inf

AND

inf

OR

B

C

OR

inf

inf

OR

inf

c

AND

D

OR

AND

d

g

E
h

i

e

F
j

k

G
l

m

f

H
n

o

I
p

q

J
r

s

inf

AND

K
t

u

OR

v

AND

inf

inf
inf

10 14 3

8

X

inf

X

X

X

X

X X

X X

X X

X X

X X

X X

OR

AND

10

inf
3

10 14 3

8

X

X

X

X

X X

X X

X X

X X

X X

X X

Cycle 2

18

OR

18

OR

18

inf

AND

18

inf

AND

18

OR

18

inf

OR

18

inf

OR

18

18

OR

AND

inf

10

3

X X

X X

inf

X
inf

8 13 10 3

X

18

AND

X

X

X

X

X X

X X

X X

X X

OR

AND

19

10

3

X X

X X

8

8 13 10 3

Cycle 3

X
X

X

X

X X

X X

X X

X X

OR

AND

10

19
3

10 14 3

8

X

X

X

X

X X

X X

X X

X X

X X

X X

18

OR

18

inf

AND

18

inf

AND

18

OR

18

inf

OR

18

inf

OR

18

X

OR

AND

10
10 14 3

X
3
8

inf

inf

X

AND

X

X

inf

inf

inf

inf

X X

X X

X X

X X

X X

X X

OR

AND

X

inf

X

X

X

X

inf

X X

X X

X X

X X

X X

Cycle 6

inf
inf

inf

inf

X X 23 6 10 3

OR

AND

inf
X

inf

X

X

X

X

inf

X X

X X

X X

X X

X X

12
inf

6

3

X X 23 6 10 3

Cycle 8

12

OR

AND

18

OR

18

12
12

X

AND

AND

inf

X

AND

Cycle 7

OR

X

3

Cycle 5

18

OR

X

8

AND

AND

inf

18

AND

X

inf

Cycle 4

18

OR

X
3

X

inf

Cycle 1

18

X

inf

AND

AND

0

18

AND

inf

Identifiers
OR

X

inf

X

inf

X

X

X

X

inf

X X

X X

X X

X X

X X

12
inf

6

3

X X 23 6 10 3

Cycle 9

Figure 8: Trace of the Updates of the Upper Bounds
agent assigns value 0 to all ancestor agents of the agent. We partition time into cycles. Agents
maintain their bounds for only one context at any given time. Nodes in the gures are crossed out if
their agent does not maintain their bounds. AND nodes are shaded if their partial solution is equal
to the context of the agent of their parent OR node augmented with its value. For example, agents
a1 , a3 and a4 take on value 0 in Cycle 2, and agent a2 takes on value 1. The context of agent a1 is
{}, the context of agent a2 is {(a1 , 0)} and the contexts of agents a3 and a4 are {(a1 , 0), (a2 , 0)}. A
description of the trace is as follows:
 Cycle 1: Root agent a1 initializes its context X a1 to {}. It initializes the lower bounds of
nodes B (= lbaX1a,a1 2 (0)) and C (= lbaX1a,a1 2 (1)) to 0 since it uses the zero heuristic values. It
a1
updates the lower bound of node a (= LBX
a1 (0)) to the sum of its delta cost (= 0) and the
lower bound of node B (= 0) according to the update equations. It updates the lower bound
a1
of node b (= LBX
a1 (1)) to the sum of its delta cost (= 0) and the lower bound of node C (= 0)
a1
according to the update equations. It updates the lower bound of node A (= LBX
a1 ) to the
minimum of the lower bound of node a (= 0) and the lower bound of node b (= 0) according to
the update equations. It initializes the upper bounds of nodes B and C to innity. It updates
the upper bounds of nodes a, b and A to innity according to the update equations. It takes

101

fiYeoh, Felner & Koenig

on its best value. It can take on either value 0 or value 1 since the lower bounds of nodes a
and b are both 0. It takes on value 0 and sends a VALUE message to its child agent a2 .
Agent a2 initializes its context X a2 to {(a1 , 0)}. It initializes the lower bounds of nodes D, E,
F and G to 0. It updates the lower bounds of nodes c, d and B to 5, 8 and 5, respectively. It
initializes the upper bounds of nodes D, E, F and G to innity. It updates the upper bounds
of nodes c, d and B to innity. The bounds of node B that agent a2 maintains are not shown
in the gures. It takes on its best value 0, sends VALUE messages to its child agents a3 and
a4 and sends a COST message to its parent agent a1 .
Leaf agent a3 initializes its context X a3 to {(a1 , 0), (a2 , 0)}. It updates the lower bounds of
nodes g and h to their delta costs 10 and 14, respectively, since leaf agents do not have child
agents. It updates the lower bound of node D to 10. It updates the upper bounds of nodes g
and h to their delta costs 10 and 14, respectively, since leaf agents do not have child agents. It
updates the upper bound of node D to 10. The bounds of node D that leaf agent a3 maintains
are not shown in the gures. It takes on its best value 0 and sends a COST message to its
parent agent a2 .
Leaf agent a4 initializes its context X a4 to {(a1 , 0), (a2 , 0)}. It updates the lower bounds of
nodes i and j to their delta costs 3 and 8, respectively. It updates the lower bound of node E
to 3. It updates the upper bounds of nodes i and j to their delta costs 3 and 8, respectively. It
updates the upper bound of node E to 3. The bounds of node E that leaf agent a4 maintains
are not shown in the gures. It takes on its best value 0 and sends a COST message to its
parent agent a2 .
In summary, the following messages are sent during Cycle 1:
 message (VALUE, {(a1 , 0)}) from agent a1 to agent a2 ;
 message (VALUE, {(a1 , 0), (a2 , 0)}) from agent a2 to agent a3 ;
 message (VALUE, {(a1 , 0), (a2 , 0)}) from agent a2 to agent a4 ;
 message (COST, a2 , {(a1 , 0)}, 5, ) from agent a2 to agent a1 ;
 message (COST, a3 , {(a1 , 0), (a2 , 0)}, 10, 10) from agent a3 to agent a2 ; and
 message (COST, a4 , {(a1 , 0), (a2 , 0)}, 3, 3) from agent a4 to agent a2 .
 Cycle 2: Root agent a1 receives the COST message sent by its child agent a2 in Cycle 1. Since
the context of agent a1 (= {}) is compatible with the context in the message (= {(a1 , 0)}), it
improves its bounds. It updates the bounds of node B to the bounds in the message (= 5 and
innity, respectively). It updates the bounds of nodes a, b and A. It does not change its value
a1
a1
) = 5 for its value da1 = 0) is still smaller than
since the lower bound of node a (= LBX
a1 (d
a1
the upper bound of node A (= U BX a1 = ). It sends a VALUE message to its child agent
a2 .
Agent a2 receives the VALUE message sent by its parent agent a1 in Cycle 1. Its context
(= {(a1 , 0)}) remains unchanged since it is the same as the desired context in the message
(= {(a1 , 0)}). Agent a2 also receives the COST messages sent by its child agents a3 and a4
in Cycle 1. Since the context of agent a2 (= {(a1 , 0)}) is compatible with the contexts in the
messages (= {(a1 , 0), (a2 , 0)}), it improves its bounds. It updates the bounds of node D to
the bounds in the rst message (= 10 and 10, respectively) and the bounds of node E to the
bounds in the second message (= 3 and 3, respectively). It updates the bounds of nodes c, d
a2
a2
) = 18 for its value
and B. It changes its value since the lower bound of node c (= LBX
a2 (d
a2
a2
d = 0) is no longer smaller than the upper bound of node B (= U BX a2 = 18). It takes on its
best value 1, sends VALUE messages to its child agents a3 and a4 and sends a COST message
to its parent agent a1 .

102

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

Leaf agents a3 and a4 receive the VALUE messages sent by their parent agent a2 in Cycle 1.
Their contexts (= {(a1 , 0), (a2 , 0)}) remain unchanged since they are the same as the desired
context in the message (= {(a1 , 0), (a2 , 0)}). They send the same COST messages as before to
their parent agent a2 .
In summary, the messages sent during Cycle 2 are identical to the ones sent during Cycle 1,
except for the messages sent by agent a2 , which are as follows:
 message (VALUE, {(a1 , 0), (a2 , 1)}) from agent a2 to agent a3 ;
 message (VALUE, {(a1 , 0), (a2 , 1)}) from agent a2 to agent a4 ; and
 message (COST, a2 , {(a1 , 0)}, 8, 18) from agent a2 to agent a1 .
The VALUE messages are dierent because agent a2 changed its value from 0 to 1. The COST
message is dierent because agent a2 changed its bounds.
 Cycles 3-9: The messages sent during Cycle 3 are identical to the ones sent during Cycle 2,
except for the messages sent by agents a3 and a4 , which are as follows:
 message (COST, a3 , {(a1 , 0), (a2 , 1)}, 8, 8) from agent a3 to agent a2 ; and
 message (COST, a4 , {(a1 , 0), (a2 , 1)}, 3, 3) from agent a4 to agent a2 .
The COST messages are dierent because agents a3 and a4 changed their contexts. The
termination condition holds after a nite amount of time when the upper bound of node A
a1
a1
(= U BX
a1 = 12) is no larger than the lower bound of node A (= LBX a1 = 12). Root agent a1
sends TERMINATE messages to all child agents, and the TERMINATE messages propagate
down the pseudo-tree until all agents terminate. BnB-ADOPT terminates after nine cycles
with minimal solution cost 12.
3.2.5 Performing Branch-and-Bound
We now rene our description of BnB-ADOPT by explaining how the agents implement branchand-bound search to make BnB-ADOPT faster. Every agent a of BnB-ADOPT now also maintains
a
the variable threshold T HX
a , which it initializes to innity. The threshold of the root agent always
remains innity. Every other agent uses its threshold for pruning, meaning that it can change its
value earlier than previously.
 First change: If an agent a did not change its context X a , it previously executed the following
a
a
a
a
statements: If U BX
a  LBX a (d ) for its value d , then the agent took on its best value. It
then sent VALUE messages to all child agents and a COST message to its parent agent. Now, if
a
a
a
a
a
T HX
a  LBX a (d ), then the agent also takes on its best value. Thus, if min{T HX a , U BX a } 
a
a
LBX
(d
),
then
the
agent
takes
on
its
best
value
and
thus
potentially
changes
its
value,
which
a
a
a
is earlier than previously. min{T HX
,
U
B
}
is
the
pruning
quantity.
a
Xa
 Second change: An agent a with context X a and value da sends VALUE messages
to all its child agents, which previously contained only the desired context X a  (a, da ).
a
a
a
a
VALUE messages now also contain the desired threshold min{T HX
a , U BX a }  X a (d ) 

a,c a
c C(a)\c lbX a (d ) for the child agent c. When agent c receives a VALUE message, it sets
its threshold to the desired threshold and then proceeds as described earlier. The desired
a
a
a
reaches its
threshold is set such that the lower bound LBX
a (d ) of agent a for its value d
c
pruning quantity (and agent a thus potentially changes its value) when the lower bound LBX
c
of agent c reaches the desired threshold. This property can be veried as follows:

103

fiYeoh, Felner & Koenig



c
a
a
a
a
LBX
c  min{T HX a , U BX a }  X a (d ) 



a
lba,c
X a (d )

(14)

c C(a)\c



a
a
a
a
a
lba,c
X a (d )  min{T HX a , U BX a }  X a (d ) 
a,c
a
a
a
a
a
 min{T HX
a , U BX a }  X a (d )  lb a (d ) 
X



a
lba,c
X a (d )

(15)

c C(a)\c





a
lba,c
X a (d )

(16)

c C(a)\c



a,c
a
a
a
a
a
min{T HX
a , U BX a }  X a (d ) + lb a (d ) +
X



a
a
a
a
min{T HX
a , U BX a }  X a (d ) +



a
lba,c
X a (d )

(17)

c C(a)\c


a
lba,c
X a (d )

(18)

c C(a)
a
a
a
a
min{T HX
a , U BX a }  LBX a (d )

(19)

3.2.6 Further Enhancements
We continue to rene our description of BnB-ADOPT by explaining a number of additional enhancements, which were introduced for ADOPT.
 Reduced contexts: The agents now use reduced contexts, which are subsets of the contexts
described previously. The reduced context X1a of agent a contains the values of all ancestor
agents p  SCP (a), while the context X2a described previously contains the values of all
a
a
a
ancestor agents p  P (a). The agents can use reduced contexts since X
a = X a and X a (d) =
1
2
1
a
X2a (d) for all values d. Agents now use reduced contexts because they need to change their
contexts and thus initialize their bounds less often when they receive VALUE messages since
their contexts are then more often identical to the desired contexts in the VALUE messages.
For our example DCOP problem, the reduced context of agent a4 contains the values of only
agent a2 rather than the values of agents a1 and a2 . Therefore, the following pairs of nodes in
the search tree are actually the same node: nodes i and q, nodes j and r, nodes m and u, and
nodes n and v.
 VALUE and COST messages: An agent sends VALUE messages to all child agents, which
previously contained the desired context and the desired threshold. The desired context is the
context of the agent augmented with its value. When an agent receives a VALUE message,
it previously checked whether its context is identical to the desired context in the VALUE
message. If it was not, then the agent changed its context to the desired context in the
VALUE message. Agents now update their contexts dierently to reduce the size of the
VALUE messages. An agent sends VALUE messages to all child and pseudo-child agents with
its identity, value and desired threshold, which is innity for its pseudo-child agents. When an
agent receives a VALUE message, it sets its threshold to the desired threshold if the message
is from its parent agent. It also checks whether the value of the ancestor agent in the VALUE
message is more recent than the value of the ancestor agent in its context. If it is, then the
agent changes the value of the ancestor agent in its context to the value of the ancestor agent in
the VALUE message. However, the context of an agent does not only contain the values of its
parent and pseudo-parent agents but also the values of its ancestor agents that are the parent
or pseudo-parent agents of one (or more) of its descendant agents, and ancestor agents that
are not constrained with the agent cannot send VALUE messages to the agent. However, they
send VALUE messages to their pseudo-child agents, at least one of which is a descendant agent
of the agent, and the information then propagates up the pseudo-tree with COST messages
until it reaches the agent. When an agent receives a COST message, it now checks whether
104

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

the value of an ancestor agent in the context of the COST message is more recent than the
value of the ancestor agent in its context. If it is, then the agent changes the value of the
ancestor agent in its context to the value of the ancestor agent in the context of the COST
message. Our example DCOP problem is too simple to allow us to illustrate the propagation
of the information up the pseudo-tree. However, imagine that a new agent a5 is a child agent
of agent a4 and is constrained with agents a1 and a4 . The context of agent a4 then contains
the value of agent a1 but agent a1 cannot send VALUE messages to agent a4 . However, agent
a1 sends VALUE messages to agent a5 . Agent a5 changes the value of agent a1 in its context
and sends COST messages with its context to agent a4 , which then changes the value of agent
a1 in its context as well.
The agents now need to determine whether the value of an agent in VALUE messages or in the
contexts of COST messages is more recent than the value of the agent in their contexts. Every
agent a therefore now also maintains a counter IDa and increments it whenever it changes its
value. Therefore, a larger ID indicates a more recent value. The values of agents in contexts
are now labeled with their IDs, and VALUE messages contain the identity of the sending agent,
its value, its ID and the desired threshold.
 Bounds: Whenever an agent changes its context X a , it previously initialized its bounds and
took on its best value. The (reduced) context of a child agent of an agent can now be a strict
subset of the (reduced) context of the agent since the parent or some pseudo-parent agents of
the agent might not be (parent or) pseudo-parent agents of the child agent or its descendant
agents. If the context of child agent c does not contain the values of any agents whose values
changed in the context of agent a, then agent a does not initialize its lower bounds lba,c
X a (d)
and upper bounds uba,c
(d)
for
agent
c
and
all
values
d
before
it
takes
on
its
best
value.
Agents
Xa
use this optimization because they need to initialize their bounds less often this way. For our
example DCOP problem, if agent a2 changes its context from {(a1 , 0)} to {(a1 , 1)} (where the
IDs are omitted for simplicity), then it does not initialize its lower bounds lbaX2a,a2 4 (d) and upper
bounds ubaX2a,a2 4 (d) for child agent a4 and all values d since the context of agent a4 does not
contain the value of agent a1 .
Additionally, if an agent a changes its context due to a COST message from its child agent c
and its new context X a is compatible with the context in the COST message, then agent a
a,c
can set its lower bound lba,c
X a (d) and upper bound ubX a (d) for agent c and the value d of agent
a in the COST message to the bounds in the COST message before it takes on its best value.
Agents use this optimization because the bounds in the COST message are more informed than
the initialized bounds. Our example DCOP problem is too simple to allow us to illustrate this
optimization. However, imagine again that a new agent a5 is a child agent of agent a4 and is
constrained with agents a1 and a4 . Assume that the context of agent a4 is {(a1 , 0), (a2 , 0)}
(where the IDs are again omitted for simplicity) and it receives a COST message from agent
a5 with context {(a1 , 1), (a4 , 0)}. Agent a4 then changes its context to {(a1 , 1), (a2 , 0)}, sets
4 ,a5
4 ,a5
its lower bound lba{(a
(0) and its upper bound uba{(a
(0) to the bounds in the
1 ,1),(a2 ,0)}
1 ,1),(a2 ,0)}
COST message and initializes all other bounds before it takes on its best value.

3.2.7 Pseudocode
Figure 9 shows the BnB-ADOPT pseudocode of every agent. The pseudocode does not index
variables with the context since this context is implicitly given by the variable X a . It uses the
predicate Compatible(X, X  ) = (a,d,ID)X,(a ,d ,ID )X  (a = a  d 
= d ) that determines if two
contexts X and X  are compatible, that is, if no agent takes on two dierent values in the two contexts
[Lines 35, 44, 46, 48 and 51]. The pseudocode also uses the procedure PriorityMerge(X, X  ) that
executes X  := {(a , d , ID )  X  | (a,d,ID)X (a = a )}  {(a , d , ID )  X  | (a,d,ID)X (a =

105

fiYeoh, Felner & Koenig

procedure Start()
[01]
X a := {(p, ValInit(p), 0) | p  SCP (a)};
[02]
IDa := 0;
[03]
forall c  C(a), d  Dom(a)
[04]
InitChild(c, d);
[05]
InitSelf ();
[06]
Backtrack();
[07]
loop forever
[08]
if (message queue is not empty)
[09]
while(message queue is not empty)
[10]
pop msg o message queue;
[11]
When Received(msg);
[12]
Backtrack();
procedure InitChild(c, d)
[13]
lba,c (d) := ha,c (d);
[14]
uba,c (d) := ;
procedure InitSelf ()

[15]
da := arg mindDom(a) { a (d) + cC(a) lba,c (d)};
a
a
[16]
ID := ID + 1;
[17]
T H a := ;
procedure Backtrack()
[18]
forall d  Dom(a) 
[19]
LB a (d) :=  a (d) + cC(a) lba,c (d);

[20]
UB a (d) :=  a (d) + cC(a) uba,c (d);
a
[21]
LB := mindDom(a) {LB a (d)};
[22]
UB a := mindDom(a) {UB a (d)};
[23]
if (LB a (da )  min{T H a , UB a })
[24]
da := arg mindDom(a) {LB a (d)} (choose the previous da if possible);
[25]
if a new da has been chosen
[26]
IDa := IDa + 1;
[27]
if ((a is root and UB a  LB a ) or termination message received)
[28]
Send(TERMINATE) to each c  C(a);
[29]
terminate execution;


[30]
Send(VALUE, a, da , IDa , min{T H a , UB a }   a (da )  c C(a)\c lba,c (da )) to each c  C(a);
a
a
[31]
Send(VALUE, a, d , ID , ) to each c  CD(a) \ C(a);
[32]
Send(COST, a, X a , LB a , UB a ) to pa(a) if a is not root;
procedure When Received(VALUE, p, dp , IDp , T H p )
[33]
X  := X a ;
[34]
PriorityMerge((p, dp , ID p ), X a );
[35]
if (!Compatible(X  , X a ))
[36]
forall c  C(a), d  Dom(a)
[37]
if (p  SCP (c))
[38]
InitChild(c, d);
[39]
InitSelf ();
[40]
if (p = pa(a))
[41]
T H a := T H p ;
procedure When Received(COST, c, X c , LB c , UB c )
[42]
X  := X a ;
[43]
PriorityMerge(X c , X a );
[44]
if (!Compatible(X  , X a ))
[45]
forall c  C(a), d  Dom(a)
[46]
if (!Compatible({(p, dp , ID p )  X  | p  SCP (c)},X a ))
[47]
InitChild(c,d);
[48]
if (Compatible(X c , X a ))
[49]
lba,c (d) := max{lba,c (d), LB c } for the unique (a , d, ID)  X c with a = a;
[50]
uba,c (d) := min{uba,c (d), UB c } for the unique (a , d, ID)  X c with a = a;
[51]
if (!Compatible(X  , X a ))
[52]
InitSelf ();
procedure When Received(TERMINATE)
[53]
record termination message received;

Figure 9: Pseudocode of BnB-ADOPT
a  ID  ID )}  {(a, d, ID)  X | (a ,d ,ID )X  (a = a  ID > ID )} and thus replaces the values

106

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

of agents in context X  with more recent values, if available, of the same agents in context X [Lines
34 and 43].
The code is identical for every agent except that the variable a is a self variable that points to
the agent itself. At the start, BnB-ADOPT calls Start() for every agent. When an agent a receives
a VALUE message from an ancestor agent, then the When Received handler for VALUE messages
is called with p being the ancestor agent, dp being the value of the ancestor agent, IDp being the
ID of the ancestor agent and T H p being the desired threshold for agent a if the ancestor agent is
its parent agent (and innity otherwise) [Line 11]. When agent a receives a COST message from a
child agent, then the When Received handler for COST messages is called with c being the child
c
agent, X c being the context of the child agent, LB c being the lower bound LBX
c of the child agent
c
c
and U B being the upper bound U BX c of the child agent [Line 11]. Finally, when agent a receives a
TERMINATE message from its parent agent, then the When Received handler for TERMINATE
messages is called without any arguments [Line 11].
BnB-ADOPT uses the same message passing and communication framework as ADOPT and
has the same memory requirements. It uses similar VALUE, COST and TERMINATE messages,
a similar strategy to update the context of an agent based on VALUE messages from its ancestor
agents and COST messages from its child agents, the same semantics for the bounds and the same
update equations to update these bounds. BnB-ADOPT and ADOPT both use thresholds but BnBADOPT uses the thresholds for pruning while ADOPT uses them to reconstruct partial solutions
that were purged from memory. Thus, BnB-ADOPT uses a dierent threshold initialization [Line
17], dierent desired threshold calculation [Line 30] and dierent termination condition [Line 27].
BnB-ADOPT also diers from ADOPT in that it maintains IDs that agents use to indicate the
recency of their values and labels the values of agents in contexts with their IDs.
3.2.8 Trace
Figures 10 and 11 show traces of the updates of the lower and upper bounds, respectively, for our
example DCOP problem, and Table 2 shows a trace of the update of all variables. BnB-ADOPT
uses the heuristic values haX1a,a1 2 (0) := 3, haX1a,a1 2 (1) := 6, haX2a,a2 3 (0) := 2, haX2a,a2 3 (1) := 2, haX2a,a2 4 (0) := 2
and haX2a,a2 4 (1) := 2 for all contexts X a1 and X a2 . These heuristic values were chosen by hand. Every
agent assigns the value of all its ancestor agents in its initial context to 0. We partition time into
cycles as in Figures 7 and 8 and continue to use the conventions made in the context of those gures.
 Cycle 1: Root agent a1 initializes its context X a1 to {} [Line 1]. It initializes the lower bounds
of nodes B (= lbaX1a,a1 2 (0)) and C (= lbaX1a,a1 2 (1)) to their heuristic values 3 and 6, respectively
a1
[Line 13]. It updates the lower bound of node a (= LBX
a1 (0)) to the sum of its delta cost
(= 0) and the lower bound of node B (= 3) according to the update equations [Line 19]. It
a1
updates the lower bound of node b (= LBX
a1 (1)) to the sum of its delta cost (= 0) and the
lower bound of node C (= 6) according to the update equations [Line 19]. It updates the
a1
lower bound of node A (= LBX
a1 ) to the minimum of the lower bound of node a (= 3) and
the lower bound of node b (= 6) according to the update equations [Line 21]. It initializes the
upper bounds of nodes B and C to innity [Line 14]. It updates the upper bounds of nodes a,
b and A to innity according to the update equations [Lines 20 and 22]. It takes on its best
value 0 since the lower bound of node a (= 3) is smaller than the lower bound of node b (= 6)
[Line 15], initializes its ID IDa1 to 1 [Lines 2 and 16], initializes its threshold T H a1 to innity
[Line 17] and sends VALUE messages to its child agent a2 and pseudo-child agent a3 [Lines 30
and 31].
Agent a2 initializes its context X a2 to {(a1 , 0, 0)} [Line 1]. It initializes the lower bounds of
nodes D, E, F and G to their heuristic value 2 [Line 13]. It updates the lower bounds of nodes
c, d and B to 9, 12 and 9, respectively [Lines 19 and 21]. It initializes the upper bounds of
nodes D, E, F and G to innity [Line 14]. It updates the upper bounds of nodes c, d and B
to innity [Lines 20 and 22]. The bounds of node B that agent a2 maintains are not shown
107

fiYeoh, Felner & Koenig

Cycle
X a1
da1
ID a1
T H a1
LB a1 (0)
LB a1 (1)
LB a1
U B a1 (0)
U B a1 (1)
U B a1
lba1 ,a2 (0)
lba1 ,a2 (1)
uba1 ,a2 (0)
uba1 ,a2 (1)
X a2
da2
ID a2
T H a2
LB a2 (0)
LB a2 (1)
LB a2
U B a2 (0)
U B a2 (1)
U B a2
lba2 ,a3 (0)
lba2 ,a3 (1)
uba2 ,a3 (0)
uba2 ,a3 (1)
lba2 ,a4 (0)
lba2 ,a4 (1)
uba2 ,a4 (0)
uba2 ,a4 (1)
X a3
da3
ID a3
T H a3
LB a3 (0)
LB a3 (1)
LB a3
U B a3 (0)
U B a3 (1)
U B a3
X a4
da4
ID a4
T H a4
LB a4 (0)
LB a4 (1)
LB a4
U B a4 (0)
U B a4 (1)
U B a4

1

2

3

4

5

6

7

8

9

0
1

3
6
3



3
6


(a1 , 0, 0)
0
1

9
12
9



2
2


2
2


(a1 , 0, 0)
(a2 , 0, 0)
0
1

10
14
10
10
14
10
(a2 , 0, 0)
0
1

3
8
3
3
8
3

0
1

9
6
6



9
6


(a1 , 0, 1)
1
2

18
12
12
18

18
10
2
10

3
2
3

(a1 , 0, 1)
(a2 , 0, 1)
0
1

10
14
10
10
14
10
(a2 , 0, 1)
0
1

3
8
3
3
8
3

0
1

12
6
6
18

18
12
6
18

(a1 , 0, 1)
1
2

18
12
12
18

18
10
2
10

3
2
3

(a1 , 0, 1)
(a2 , 1, 2)
0
2
8
8
13
8
8
13
8
(a2 , 1, 2)
1
2
8
10
3
3
10
3
3

0
1

12
6
6
18

18
12
6
18

(a1 , 0, 1)
0
3
18
18
19
18
18
19
18
10
8
10
8
3
3
3
3
(a1 , 0, 1)
(a2 , 1, 2)
0
2
8
8
13
8
8
13
8
(a2 , 1, 2)
1
2
8
10
3
3
10
3
3

1
2

18
6
6
18

18
18
6
18

(a1 , 0, 1)
0
3
18
18
19
18
18
19
18
10
8
10
8
3
3
3
3
(a1 , 0, 1)
(a2 , 0, 3)
0
3
10
10
14
10
10
14
10
(a2 , 0, 3)
0
3
3
3
8
3
3
8
3

1
2

18
6
6
18

18
18
6
18

(a1 , 1, 2)
1
4
18
25
8
8



2
2


3
3
3
3
(a1 , 1, 2)
(a2 , 0, 3)
1
4
10
25
7
7
25
7
7
(a2 , 0, 3)
0
3
3
3
8
3
3
8
3

1
2

18
8
8
18

18
18
8
18

(a1 , 1, 2)
1
4
18
30
8
8
30

30
7
2
7

3
3
3
3
(a1 , 1, 2)
(a2 , 1, 4)
1
5
12
23
6
6
23
6
6
(a2 , 1, 4)
1
4
13
10
3
3
10
3
3

1
2

18
8
8
18
30
18
18
8
18
30
(a1 , 1, 2)
1
4
18
30
12
12
30
12
12
7
6
7
6
3
3
3
3
(a1 , 1, 2)
(a2 , 1, 4)
1
5
12
23
6
6
23
6
6
(a2 , 1, 4)
1
4
13
10
3
3
10
3
3

1
2

18
12
12
18
12
12
18
12
18
12
(a1 , 1, 2)
1
4
18
30
12
12
30
12
12
7
6
7
6
3
3
3
3
(a1 , 1, 2)
(a2 , 1, 4)
1
5
6
23
6
6
23
6
6
(a2 , 1, 4)
1
4
3
10
3
3
10
3
3

Table 2: Trace of the Update of all Variables of BnB-ADOPT

108

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

A

OR

3

OR

6

OR

AND

a

b

AND

3

6

AND

9

OR

B

C

OR

3

6

OR

9

c

AND

D

OR

AND

d

g

E
h

i

e

F
j

k

G
l

m

f

H
n

o

I
p

q

J
r

s

K
t

9

AND

u

OR

v

AND

12

2

2

10 14 3

8

X

2

X

X

X

X

X X

X X

X X

X X

X X

X X

OR

AND

6

18

AND

2

Identifiers

10

12
3

10 14 3

8

X

X

X

X

X X

X X

X X

X X

X X

X X

6

OR

AND

12

6

AND

12

6

AND

18

OR

12

6

OR

12

6

OR

18

18

AND

OR

AND

12

10

3

X X

X X

2

X
2

8 13 10 3

X

18

AND

X

X

X

X

X X

X X

X X

X X

OR

AND

19

10

3

X X

X X

8

8 13 10 3

Cycle 3

X
X

X

X

X X

X X

X X

X X

OR

AND

10

19
3

10 14 3

8

X

X

X

X

X X

X X

X X

X X

X X

X X

8

OR

18

6

AND

18

8

AND

18

OR

18

6

OR

18

8

OR

18

X

OR

AND

X

X

X

X

X X

X X

X X

25
X

2

X X 25 7

8
3

3

8

X

AND

2

3

X X

X X

OR

AND

X

30

X

X

X

X

7

X X

X X

X X

X X

X X

Cycle 6

8
3

2

3

X X 23 6 10 3

AND

8
X

30

X

X

X

X

7

X X

X X

X X

X X

X X

12
3

6

3

X X 23 6 10 3

Cycle 8

12

OR

AND

18

OR

18

12
12

X

AND

AND

OR

8

X

AND

Cycle 7

OR

X

3

Cycle 5

8

OR

X

8

AND

AND

6

18

AND

X

6

Cycle 4

6

OR

X
3

X

2

Cycle 2

6

OR

X

2

Cycle 1

6

OR

X

6

X

30

X

X

X

X

7

X X

X X

X X

X X

X X

12
3

6

3

X X 23 6 10 3

Cycle 9

Figure 10: Trace of the Update of the Lower Bounds of BnB-ADOPT
in the gure. It takes on its best value 0 [Line 15], initializes its ID to 1 [Lines 2 and 16],
initializes its threshold to innity [Line 17] and sends VALUE messages to its child agents a3
and a4 and a COST message to its parent agent a1 [Lines 30-32].
Leaf agent a3 initializes its context X a3 to {(a1 , 0, 0), (a2 , 0, 0)} [Line 1]. It updates the lower
bounds of nodes g and h to their delta costs 10 and 14, respectively, since leaf agents do not
have child agents [Line 19]. It updates the lower bound of node D to 10 [Line 21]. It updates
the upper bounds of nodes g and h to their delta costs 10 and 14, respectively, since leaf agents
do not have child agents [Line 20]. It updates the upper bound of node D to 10 [Line 22]. The
bounds of node D that leaf agent a3 maintains are not shown in the gure. It takes on its
best value 0 [Line 15], initializes its ID to 1 [Lines 2 and 16], initializes its threshold to innity
[Line 17] and sends a COST message to its parent agent a2 [Line 32].
Leaf agent a4 initializes its (reduced) context X a4 to {(a2 , 0, 0)} [Line 1]. It updates the lower
bounds of nodes i and j to their delta costs 3 and 8, respectively [Line 19]. It updates the
lower bound of node E to 3 [Line 21]. It updates the upper bounds of nodes i and j to their
delta costs 3 and 8, respectively [Line 20]. It updates the upper bound of node E to 3 [Line
22]. The bounds of node E that leaf agent a4 maintains are not shown in the gure. It takes

109

fiYeoh, Felner & Koenig

A

OR

inf

OR

inf

OR

AND

a

b

AND

inf

inf

AND

inf

OR

B

C

OR

inf

inf

OR

inf

c

AND

D

OR

AND

d

g

E
h

i

e

F
j

k

G
l

m

f

H
n

o

I
p

q

J
r

s

inf

AND

K
t

u

OR

v

AND

inf

inf
inf

10 14 3

8

X

inf

X

X

X

X

X X

X X

X X

X X

X X

X X

10

OR

AND

inf
3

10 14 3

8

X

X

X

X

X X

X X

X X

X X

X X

X X

Cycle 2

18

OR

18

OR

18

inf

AND

18

inf

AND

18

OR

18

inf

OR

18

inf

OR

18

18

OR

AND

inf

10

3

X X

X X

inf

X
inf

8 13 10 3

X

18

AND

X

X

X

X

X X

X X

X X

X X

OR

AND

19

10

3

X X

X X

8

8 13 10 3

Cycle 3

X
X

X

X

X X

X X

X X

X X

10

OR

AND

19
3

10 14 3

8

X

X

X

X

X X

X X

X X

X X

X X

X X

18

OR

18

inf

AND

18

inf

AND

18

OR

18

inf

OR

18

inf

OR

18

X

OR

AND

X

X

X

X

X X

X X

X X

inf
X

inf

X X 25 7

inf
3

3

8

X

AND

inf

3

X X

X X

OR

AND

X

30

X

X

X

X

7

X X

X X

X X

X X

X X

Cycle 6

inf
3

inf

3

X X 23 6 10 3

30

X

X

X

7

X X

X X

X X

X X

X X

12
3

6

3

X X 23 6 10 3

Cycle 8

12

OR

AND

18

OR

18

12
12

X

AND

AND

30
X

X

OR

AND

30

X

AND

Cycle 7

OR

X

3

Cycle 5

18

OR

X

8

AND

AND

inf

18

AND

X

inf

Cycle 4

18

OR

X
3

X

inf

Cycle 1

18

X

inf

AND

AND

inf

18

AND

inf

Identifiers
OR

X

inf

X

30

X

X

X

X

7

X X

X X

X X

X X

X X

12
3

6

3

X X 23 6 10 3

Cycle 9

Figure 11: Trace of the Update of the Upper Bounds of BnB-ADOPT
on its best value 0 [Line 15], initializes its ID to 1 [Lines 2 and 16], initializes its threshold to
innity [Line 17] and sends a COST message to its parent agent a2 [Line 32].
In summary, the following messages are sent during Cycle 1:
 message (VALUE, a1 , 0, 1, ) from agent a1 to agent a2 ;
 message (VALUE, a1 , 0, 1, ) from agent a1 to agent a3 ;
 message (VALUE, a2 , 0, 1, ) from agent a2 to agent a3 ;
 message (VALUE, a2 , 0, 1, ) from agent a2 to agent a4 ;
 message (COST, a2 , {(a1 , 0, 0)}, 9, ) from agent a2 to agent a1 ;
 message (COST, a3 , {(a1 , 0, 0), (a2 , 0, 0)}, 10, 10) from agent a3 to agent a2 ; and
 message (COST, a4 , {(a2 , 0, 0)}, 3, 3) from agent a4 to agent a2 .
 Cycle 2: Root agent a1 receives the COST message sent by its child agent a2 in Cycle 1. Since
the context of agent a1 (= {}) is compatible with the context in the message (= {(a1 , 0, 0)}), it
improves its bounds. It updates the bounds of node B to the bounds in the message (= 9 and
innity, respectively) [Lines 48-50]. It updates the bounds of nodes a, b and A [Lines 18-22].
110

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

a1
a1
It does not change its value since the lower bound of node a (= LBX
) = 9 for its value
a1 (d
a1
a1
a1
d = 0) is still smaller than its pruning quantity (= min{T HX a1 , U BX a1 } = min(, ) =
). It sends VALUE messages to its child agent a2 and pseudo-child agent a3 [Lines 30-31].

Agent a2 receives the VALUE message sent by its parent agent a1 in Cycle 1. It updates its
context from {(a1 , 0, 0)} to {(a1 , 0, 1)} since the ID of agent a1 in its context (= 0) is smaller
than the ID in the message (= 1) [Line 34]. Its threshold (= ) remains unchanged since it
is the same as the desired threshold (= ) in the message. Agent a2 also receives the COST
messages sent by its child agents a3 and a4 in Cycle 1. Since its context (= {(a1 , 0, 1)}) is compatible with the contexts in the messages (= {(a1 , 0, 0), (a2 , 0, 0)} and {(a2 , 0, 0)}, respectively),
it improves its bounds. It updates the bounds of node D to the bounds in the rst message
(= 10 and 10, respectively) and the bounds of node E to the bounds in the second message (= 3
and 3, respectively) [Lines 48-50]. It updates the bounds of nodes c, d and B [Lines 18-22]. It
a2
a2
) = 18 for its value da2 = 0) is
changes its value since the lower bound of node c (= LBX
a2 (d
a2
a2
no longer smaller than its pruning quantity (= min{T HX a2 , U BX
a2 } = min(, 18) = 18). It
takes on its best value 1 [Line 24], increments its ID to 2 [Lines 25-26], sends VALUE messages
to its child agents a3 and a4 [Lines 30-31] and sends a COST message to its parent agent a1
[Line 32].
Leaf agent a3 receives the VALUE messages sent by its parent agent a2 and pseudo-parent
agent a1 in Cycle 1. It updates its context from {(a1 , 0, 0), (a2 , 0, 0)} to {(a1 , 0, 1), (a2 , 0, 1)}
since the IDs of agents a1 and a2 in its context (= 0 and 0, respectively) are smaller than
the IDs in the messages (= 1 and 1, respectively) [Line 34]. Its threshold (= ) remains
unchanged since it is the same as the desired threshold (= ) in the message. Its bounds are
not reinitialized since its context is compatible with its previous context [Line 35]. It sends
the same COST message as before to its parent agent a2 [Line 32].
Leaf agent a4 receives the VALUE message sent by its parent agent a2 in Cycle 1. It updates its
contexts from {(a2 , 0, 0)} to {(a2 , 0, 1)} since the ID of agent a2 in its context (= 0) is smaller
than the ID in the message (= 1) [Line 34]. Its threshold (= ) remains unchanged since it is
the same as the desired threshold (= ) in the message. Its bounds are not reinitialized since
its context is compatible with its previous context [Line 35]. It sends the same COST message
as before to its parent agent a2 [Line 32].
In summary, the messages sent during Cycle 2 are identical to the ones sent during Cycle 1,
except for the messages sent by agents a2 , a3 and a4 , which are as follows:
 message (VALUE, a2 , 1, 2, 8) from agent a2 to agent a3 ;
 message (VALUE, a2 , 1, 2, 8) from agent a2 to agent a4 ; and
 message (COST, a2 , {(a1 , 0, 1)}, 12, 18) from agent a2 to agent a1 .
 message (COST, a3 , {(a1 , 0, 1), (a2 , 0, 1)}, 10, 10) from agent a3 to agent a2 ; and
 message (COST, a4 , {(a2 , 0, 1)}, 3, 3) from agent a4 to agent a2 .
The VALUE messages are dierent because agent a2 changed its value from 0 to 1. The COST
messages are dierent because agent a2 changed its bounds and its context and agents a3 and
a4 changed their contexts.
 Cycles 3-9: The messages sent during Cycle 3 are identical to the ones sent during Cycle 2,
except for the messages sent by agents a3 and a4 , which are as follows:
 message (COST, a3 , {(a1 , 0, 1), (a2 , 1, 2)}, 8, 8) from agent a3 to agent a2 ; and
 message (COST, a4 , {(a2 , 1, 2)}, 3, 3) from agent a4 to agent a2 .

111

fiYeoh, Felner & Koenig

The COST messages are dierent because agents a3 and a4 changed their contexts. The
termination conditions holds after a nite amount of time when the upper bound of node A
a1
a1
(= U BX
a1 = 12) is no larger than the lower bound of node A (= LBX a1 = 12) [Line 27]. Root
agent a1 sends TERMINATE messages to all child agents [Line 28], and the TERMINATE
messages propagate down the pseudo-tree [Line 28] until all agents terminate. BnB-ADOPT
terminates after nine cycles with minimal solution cost 12.

4. Bounded-Error Approximations
In this section, we present three approximation mechanisms that allow BnB-ADOPT to trade o
solution cost for a smaller runtime. They bound the error on the solution cost by a user-dened
error bound. First, we modify the Absolute Error Mechanism of ADOPT (Modi et al., 2005) to
work with BnB-ADOPT. This approximation mechanism allows users to specify an absolute error
bound on the solution cost (for example, that the solution cost should be at most 10 larger than the
minimal solution cost). However, it is often much more desirable to specify a relative error bound on
the solution cost (for example, that the solution cost should be at most 10 percent larger than the
minimal solution cost or, equivalently, 1.1 times larger than the minimal solution cost). This cannot
be done with the Absolute Error Mechanism without knowing the minimal solution cost a priori.
Thus, we introduce two approximation mechanisms that allow users to specify a relative error bound
on the solution cost, namely the Relative Error Mechanism and the Weighted Heuristics Mechanism.
All approximation mechanisms let the root agent r (and only the root agent) maintain the
limit limr . The root agent uses this limit in the same way in the termination condition for all
r
r
approximation mechanisms but updates it dierently. The termination condition U BX
r  LBX r on
r
r
Line 27 of the pseudocode of BnB-ADOPT is replaced with U BX r  lim . The root agent updates
the limit between Lines 26 and 27 in the pseudocode, outside of the preceding if statement.
4.1 Absolute Error Mechanism
The Absolute Error Mechanism of ADOPT requires a user-dened absolute error bound 0  b < 
that species that the solution cost should be at most b larger than the minimal solution cost. This
approximation mechanism can easily be modied for BnB-ADOPT by setting the limit as follows:
limr

:=

r
b + LBX
r

(20)

BnB-ADOPTAEM is the resulting variant of BnB-ADOPT with the Absolute Error Mechanism.
BnB-ADOPTAEM terminates once the upper bound of the root node (which is equal to the solution
cost of the solution with the smallest solution cost found so far) is no larger than the limit (which
is equal to the absolute error bound b plus the lower bound of the root node, which is a lower
bound on the minimal solution cost). BnB-ADOPTAEM terminates with a solution cost that is
equal to the upper bound of the root node although the minimal solution cost could be as small as
the lower bound of the root node. It thus terminates with a solution cost that is at most b larger
than the minimal solution cost. Figure 12 shows a trace of BnB-ADOPTAEM with absolute error
bound b = 24 for our example DCOP problem. BnB-ADOPTAEM terminates after three cycles
with suboptimal solution cost 18, which is six cycles faster than BnB-ADOPT.
4.2 Relative Error Mechanism
It is often much more desirable to specify a relative error bound on the solution cost rather than
an absolute error bound. Fortunately, the Absolute Error Mechanism of BnB-ADOPT can easily
be changed to the Relative Error Mechanism by setting the limit as follows. The Relative Error
112

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

a

3

OR

a

lim 1 = 27
a
UB 1 = infinity

a

lim 1 = 30
a
UB 1 = infinity

6

OR

6

OR

AND

3

6

AND

9

6

AND

12

OR

3

6

OR

9

6

OR

12

9

AND

OR

AND

2

12
2

10 14 3

8

X

X

18

AND

2

2

X

X

X

X

X X

X X

X X

X X

X X

X X

OR

AND

10

12
3

10 14 3

8

X

X

2

2

X

X

X

X

X X

X X

X X

X X

X X

X X

Cycle 1

OR

AND

6
6

18

AND

lim 1 = 30
a
UB 1 = 18

12

10

3

X X

X X

2

X
2

8 13 10 3

Cycle 2

X

X

X

X

X

X X

X X

X X

X X

Cycle 3

Figure 12: Trace of the Update of the Lower Bounds of BnB-ADOPTAEM with b = 24
a

3

OR

a

lim 1 = 9
a
UB 1 = infinity

a

lim 1 = 18
a
UB 1 = infinity

6

OR

6

OR

AND

3

6

AND

9

6

AND

12

OR

3

6

OR

9

6

OR

12

9

AND

OR

AND

2

12
2

10 14 3

8

X

X

18

AND

2

2

X

X

X

X

X X

X X

X X

X X

X X

X X

OR

AND

10

12
3

10 14 3

8

X

X

2

2

X

X

X

X

X X

X X

X X

X X

X X

X X

Cycle 1

Cycle 2

OR

AND

6
6

18

AND

lim 1 = 18
a
UB 1 = 18

12

10

3

X X

X X

2

X
2

8 13 10 3

X

X

X

X

X

X X

X X

X X

X X

Cycle 3

Figure 13: Trace of the Update of the Lower Bounds of BnB-ADOPTREM with p = 3
Mechanism requires a user-dened relative error bound 1  p <  that species that the solution
cost should be at most p times larger than the minimal solution cost:
limr

r
:= p  LBX
r

(21)

BnB-ADOPTREM is the resulting variant of BnB-ADOPT with the Relative Error Mechanism.
BnB-ADOPTREM terminates once the upper bound of the root node (which is equal to the solution
cost of the solution with the smallest solution cost found so far) is no larger than the limit (which
is equal to the relative error bound p times the lower bound of the root node, which is a lower
bound on the minimal solution cost). BnB-ADOPTREM terminates with a solution cost that is
equal to the upper bound of the root node although the minimal solution cost could be as small as
the lower bound of the root node. It thus terminates with a solution cost that is at most p times
larger than the minimal solution cost. Figure 13 shows a trace of BnB-ADOPTREM with relative
error bound p = 3 for our example DCOP problem. BnB-ADOPTREM terminates after three cycles
with suboptimal solution cost 18, which is six cycles faster than BnB-ADOPT.
4.3 Weighted Heuristics Mechanism
There is a second way of implementing a relative error bound for BnB-ADOPT since BnB-ADOPT
uses admissible heuristic values. It is common practice in the context of A* to trade o solution
cost for a smaller runtime by using weighted heuristic values (Pohl, 1973), which are derived from
admissible heuristic values by multiplying them with a user-dened weight 1  w < . The
resulting heuristic values can be inadmissible. A* is then no longer guaranteed to nd cost-minimal
solutions but it is guaranteed to terminate with a solution cost that is at most w times larger than
the minimal solution cost (Pohl, 1970). This approximation mechanism can easily be modied for
BnB-ADOPT by setting the limit as follows:
limr

:=

113

r
LBX
r

(22)

fiYeoh, Felner & Koenig

a

9

OR

a

lim 1 = 9
a
UB 1 = infinity

a

lim 1 = 17

lim 1 = 18

17 UBa1 = infinity

OR

18 UBa1 = 18

OR

AND

9

18

AND

17

18

AND

20

OR

9

18

OR

17

18

OR

20

17

AND

OR

AND

6

20
6

10 14 3

8

X

X

21

AND

6

6

X

X

X

X

X X

X X

X X

X X

X X

X X

OR

AND

10

20
6

10 14 3

8

X

X

6

X

X

X

X

X X

X X

X X

X X

X X

X X

Cycle 1

Cycle 2

OR

AND

18

21

AND

6

18

20

10

6

X X

X X

6

X
6

8 13 10 3

X

X

X

X

X

X X

X X

X X

X X

Cycle 3

Figure 14: Trace of the Update of the Lower Bounds of BnB-ADOPTW HM with w = 3
and by initializing the lower bounds lba,c
X a (d) as follows:
lba,c
X a (d)

:= w  ha,c
X a (d)

(23)

for all agents a, all values d, all child agents c and all contexts X a . BnB-ADOPTW HM is the
resulting variant of BnB-ADOPT with the Weighted Heuristics Mechanism. BnB-ADOPTW HM
terminates once the upper bound of the root node (which is equal to the solution cost of the solution
with the smallest solution cost found so far) is no larger than the limit (which is equal to the lower
bound of the root node, which is a lower bound on w times the minimal solution cost). BnBADOPTW HM terminates with a solution cost that is equal to the upper bound of the root node
although the minimal solution cost could be as small as the lower bound of the root node divided
by w. It thus terminates with a solution cost that is at most w times larger than the minimal
solution cost. Figure 14 shows a trace of BnB-ADOPTW HM with w = 3 for our example DCOP
problem. BnB-ADOPTW HM terminates after three cycles with suboptimal solution cost 18, which
is six cycles faster than BnB-ADOPT.

5. Correctness and Completeness
In this section, we prove the correctness and completeness of BnB-ADOPT and its suboptimal
variants. All denitions, lemmata, theorems and corollaries hold for BnB-ADOPT and its suboptimal variants except when mentioned otherwise. Therefore, each agent a uses the following update
equation for all values d, all child agents c and all contexts X a to initialize its bounds lba,c
X a (d):
a,c
lba,c
X a (d) := w  hX a (d)

(24)

where the weight w is a oating point number that satises 1  w <  and the heuristic values
ha,c
X a (d) are oating point numbers that satisfy
c
0  ha,c
X a (d)  X a (a,d)

(25)

Messages are sent at the end of a cycle and received in the beginning of a cycle.  is the largest
duration between the time a message is sent and the time it is processed, and  is the largest duration
of a cycle.
Lemma 1. If two contexts X and X  of an arbitrary agent a  A agree on the values of all ancestor
a
a
= X
agents p  SCP (a) of agent a, then X
.
Proof. By denition, X a  X is the (reduced) context that contains the values of all ancestor agents
a
is the sum of the constraint costs of all constraints that
p  SCP (a) of agent a. The gamma cost X
114

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

involve agent a or one of its descendant agents minimized over all possible values of agent a and its
descendant agents, under the assumption that the ancestor agents of agent a take on the values in
a
thus depends only on the values of the ancestor agents (including
context X. The gamma cost X
the parent agent) of agent a that are the parent or pseudo-parent agents of agent a or one (or more)
of its descendant agents, that is, the values of all ancestor agents p  SCP (a) of agent a. Therefore,
a
a
a
a
= X
X
a . Similarly, X  = X a .
Definition 1. Contexts are correct i the IDs of the values of all agents in the contexts are equal
to the IDs of the agents, which implies that the values of all agents in the contexts are equal to the
values of the agents.
Lemma 2. If the context X a of an arbitrary agent a  A does not change for a period of time,
a
a
then the lower bounds lba,c
X a (d), LBX a (d) and LBX a are monotonically non-decreasing and the upper
a,c
a
a
bounds ubX a (d), U BX a (d) and U BX a are monotonically non-increasing during that period of time
for all values d  Dom(a) and all child agents c  C(a).
a
Proof. Since the context X a does not change, the delta values X
a (d) are constant and the bounds
(once initialized) are updated according to update equations 8 to 13. Thus, the lower bounds are
monotonically non-decreasing and the upper bounds are monotonically non-increasing.

Lemma 3. If the value of an arbitrary ancestor agent p  SCP (a) of an arbitrary agent a  A does
not change between the current time T and a future time t with t  T + |A|  ( + ) + , then the
value of agent p and its ID in the context of agent a are equal to the value of agent p and its ID,
respectively, between some time t and time t with t  t.
Proof. Assume that the value of an arbitrary ancestor agent p  SCP (a) of an arbitrary agent a  A
does not change between the current time T and a future time t with t  T + |A|  ( + ) + . There
are the following two cases.
 Case 1: If agent p is a parent or pseudo-parent agent of agent a, then it sent a VALUE message
to agent a with its value and ID at time t  T + , that is, in the same cycle in which it took
on the value that it has at time T since the duration of that cycle is no larger than . (The
agents send VALUE messages at the end of every cycle.) Agent a receives the VALUE message
by time t +  since messages are delivered with nite delay . It then updates the value of
agent p and its ID in its context by time t +  +  since the update is done in the same cycle
and the duration of that cycle is no larger than . Thus, the value of agent p and its ID in
the context of agent a are equal to the value of agent p and its ID, respectively, between some
time t and time t with t  t  t +  +   T +  + 2    t since agent p does not change
its value between time t and time t.
 Case 2: If agent p is not a parent or pseudo-parent agent of agent a, then one of its pseudo-child
agents c is a descendant agent of agent a. Agent p sent a VALUE message to agent c with its
value and ID at time t  T + . Agent c receives the VALUE message by time t + . It then
updates the value of agent p and its ID in its context and sends a COST message to its parent
agent pa(c) with its updated context by time t +  + . (The agents send COST messages at
the end of every cycle.) Agent pa(c) receives the COST message by time t + 2   + . It then
updates the value of agent p and its ID in its context and sends a COST message to its parent
agent pa(pa(c)) with its updated context by time t + 2  ( + ). This process continues until
agent a updates the value of agent p and its ID in its context by time t + n  ( + ), where
n  |A| is the number of messages in the chain of messages. Thus, the value of agent p and its
ID in the context of agent a are equal to the value of agent p and its ID, respectively, between
some time t and time t with t  t  t + n  ( + )  T + |A|  ( + ) +   t since agent
p does not change its value between time t and time t.

115

fiYeoh, Felner & Koenig

Corollary 1. If the values of all ancestor agents p  SCP (a) of an arbitrary agent a  A do not
change between the current time T and a future time t with t  T + |A|  ( + ) + , then the context
of agent a is correct between some time t and time t with t  t.
c
c
c
Lemma 4. If LBX
c  w  X c  w  U BX c at all times for all child agents c  C(a) of an arbitrary
a,c
a,c
c
c
agent a and their contexts X , then lbX a (d)  w  X
a (a,d)  w  ubX a (d) at all times for the context
X a of agent a, all values d  Dom(a) and all child agents c  C(a).

Proof. We prove the lemma by induction on the number of times that agent a changes its context
a,c
or updates its bounds lba,c
X a (d) and ubX a (d) for an arbitrary value d and an arbitrary child agent c
after agent a initializes its bounds. The conclusion of the lemma holds after agent a with context
X a initializes its bounds since
a,c
lba,c
X a (d) = w  hX a (d)

(Eq. 24)

w

(Eq. 25)

c
X
a (a,d)


= w  uba,c
X a (d)

(Eq. 7)

for the (unchanged or new) context X a of agent a (induction basis). Now assume that the lemma
holds after agent a changed its context or updated its bounds a number of times (induction assumption). We show that it then also holds after agent a changes its context or updates its bounds one
more time (induction step). There are the following two cases (where we split the operations after
receiving a COST message into two parts).
 Case 1: The conclusion of the lemma holds when agent a changes its context from X a to X a
after receiving a VALUE or COST message and the two contexts agree on the values of all
ancestor agents p  SCP (c) since agent a then does not change its bounds and thus
(d) = lba,c
lba,c
X a (d)
X a
c
 w  X
a (a,d)

(induction assumption)

c
w  X
a (a,d)
a,c
ubX a (d)
c
X
a (a,d)
c
X a (a,d)

(Lemma 1)

=
uba,c
(d)
X a

(premise of case)

=

=

(premise of case)
(induction assumption)
(Lemma 1)

after receiving the VALUE or COST message (since contexts X a and X a agree on the values
of all ancestor agents p  SCP (c)).
 Case 2: The conclusion of the lemma holds when agent a updates its bounds from lba,c
X a (d) and
a,c
a,c
a,c


ubX a (d) to lbX a (d) and ubX a (d), respectively, after receiving a COST message from some child
c
c
c
agent c with bounds LBX
that is compatible with its context X a
c and U BX c and context X
and in which agent a has value d since

116

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

 a,ca (d) = max{lba,ca (d), LB c c }
lb
X
X
X
c
c
 max{w  X
a (a,d) , w  X c }

(Eq. 8)
(induction assumption and premise of lemma)

c
c
= max{w  X
a (a,d) , w  X a (a,d) }

=w
 a,ca (d)
ub
X

(Lemma 1)

c
X
a (a,d)

c
= min{uba,c
X a (d), U BX c }
c
c
 min{X
a (a,d) , X c }

(Eq. 11)
(induction assumption and premise of lemma)

c
c
= min{X
a (a,d) , X a (a,d) }

=

(Lemma 1)

c
X
a (a,d)

after receiving the COST message (since contexts X a  (a, d) and X c agree on the values of
all ancestor agents p  SCP (c)).
a,c
c
Thus, lba,c
X a (d)  w  X a (a,d)  w  ubX a (d) at all times for all values d  Dom(a) and all child
agents c  C(a).
a
a
a
a
a
a
Lemma 5. LBX
a (d)  w  X a (d)  w  U BX a (d) and LBX a  w  X a  w  U BX a at all times for
a
all values d  Dom(a) and the context X of an arbitrary agent a  A.

Proof. We prove the lemma by induction on the depth of an agent in the pseudo-tree. The lemma
holds for a leaf agent a in the pseudo-tree with context X a since
a
a
LBX
a (d) = X a (d)

(Eq. 9)

a
X
a (d)
a
X a (d)
a
X
a (d)

(Eq. 1)
(Eq. 12)

=
a
U BX
a (d) =
=

(Eq. 1)

a
a
a
a
for all values d at all times. Thus, LBX
a (d) = X a (d)  w  X a (d) = w  U BX a (d) for all values d
at all times. Furthermore,

a
LBX
a =

=

a
U BX
a

min

a
{LBX
a (d)}

(Eq. 10)

dDom(a)

min

a
{X
a (d)}

(see above)

dDom(a)

a
= X
a
= min

(Eq. 2)
(Eq. 13)

a
{U BX
a (d)}
dDom(a)

=

min

a
{X
a (d)}

(see above)

dDom(a)
a
= X
a

(Eq. 2)

a
a
a
a
at all times. Thus, LBX
a = X a  w  X a = w  U BX a at all times (induction basis). Now assume
that the lemma holds for all agents of depth d in the pseudo-tree (induction assumption). We show
that it then also holds for all agents of depth d  1 in the pseudo-tree each time after they update
their bounds (induction step). The lemma holds for agent a with context X a since

117

fiYeoh, Felner & Koenig



a
a
LBX
a (d) = X a (d) +

lba,c
X a (d)

(Eq. 9)

cC(a)



a
 X
a (d) +

c
w  X
a (a,d)

(induction assumption and Lemma 4)

cC(a)
a
 w  X
a (d)
a
a
U BX
a (d) = X a (d) +

(Eq. 1)



uba,c
X a (d)

(Eq. 12)

cC(a)



a
 X
a (d) +

c
X
a (a,d)

(induction assumption and Lemma 4)

cC(a)
a
= X
a (d)

(Eq. 1)

a
a
a
Thus, LBX
a (d)  w  X a (d)  w  U BX a (d) at all times for all values d  Dom(a). Furthermore,

a
LBX
a =



min

a
{LBX
a (d)}

(Eq. 10)

dDom(a)

min

a
{w  X
a (d)}

(see above)

dDom(a)

=w

min

a
{X
a (d)}

dDom(a)

a
= w  X
a
a
U BX
a

=


(Eq. 2)

a
min {U BX
a (d)}
dDom(a)

min

(Eq. 13)

a
{X
a (d)}

(see above)

dDom(a)

a
= X
a

(Eq. 2)

a
a
a
Thus, LBX
a  w  X a  w  U BX a at all times.

Definition 2. The potential of an agent a  A with context X a is
a
LBX
a (d)}.



dDom(a) {w

a
 U BX
a (d) 

Lemma 6. If the context X a of an arbitrary agent a  A no longer changes, then the potential of
the agent is monotonically non-increasing and decreases by more than a positive constant every time
the agent changes its value.
a
a
Proof. The lower bounds LBX
a (d) are monotonically non-decreasing and the upper bounds U BX a (d)
are monotonically non-increasing for all values d according to Lemma 2 since the context X a of
agent a no longer changes. Therefore, the potential of agent a is monotonically non-increasing.
a
a
Furthermore, agent a changes its value d to a new value only if mindDom(a) {LBX
a (d)} < LBX a (d)
a
[Line 24]. Thus, the lower bound LBX a (d) must have strictly increased between the time when the
agent changed its value to d and the time when it changes its value d to a new value. Thus, its
potential has decreased by more than a positive constant, namely the smallest possible increase of the
a
lower bound LBX
a (d). Assume that all constraint costs, weights and heuristic values are integers.
Then, the smallest possible increase is bounded from below by one because the only possible values of
a
LBX
a (d) are combinations of all constraint costs and weighted heuristic values. A similar statement
holds if all constraint costs, weights and heuristic values are oating point numbers since they can
then all be transformed into integers by multiplying them with the same suciently large integer.

Lemma 7. All agents change their values only a nite number of times.
118

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

Proof. Assume that the lemma does not hold and choose an agent a that changes its value an
innite number of times but whose ancestor agents p  SCP (a) change their values only a nite
number of times. Then, there exists a time when the ancestor agents do not change their values
any longer. There exists a (later) time when agent a no longer changes its context X a according to
Corollary 1. Every time agent a changes its value afterwards, its potential decreases by more than
a positive constant according to Lemma 6, towards minus innity. However, its potential cannot
a
a
become negative since LBX
a (d)  w  U BX a (d) for all values d according to Lemma 5, which is a
contradiction. Thus, all agents change their values only a nite number of times.
a
Lemma 8. If BnB-ADOPT and its suboptimal variants do not terminate earlier, then U BX
a 
a
a
LBX a after a nite amount of time for all agents a  A and their contexts X .

Proof. We prove the lemma by induction on the depth of an agent in the pseudo-tree. There exists
a time when no agent changes its value any longer according to Lemma 7. There exists a (later)
time when the contexts of all agents are correct and no longer change according to Corollary 1. Let
X a be the context of agent a at this point in time for all agents a. There exists an (even later)
a,c
a
a
a
a
time when the bounds lba,c
X a (d), LBX a (d), LBX a , ubX a (d), U BX a (d) and U BX a no longer change
a
for all agents a, all values d and all child agents c since (1) the lower bounds lba,c
X a (d), LBX a (d) and
a,c
a
a
a
LBX a are monotonically non-decreasing and the upper bounds lbX a (d), U BX a (d) and U BX
a are
monotonically non-increasing for all agents a, all values d and all child agents c according to Lemma
a
a
a
a
a
a
2, (2) LBX
a (d)  w  X a (d)  w  U BX a (d) and LBX a  w  X a  w  U BX a for all agents a and
a,c
a,c
all values d according to Lemma 5, (3) lbX a (d)  w  ubX a (d) for all agents a, all values d and all
child agents c according to Lemma 4 and (4) the smallest possible increases of the lower bounds and
the smallest possible decreases of the upper bounds are larger than a positive constant since the
only possible values of the bounds are combinations of all constraint costs and heuristic values, as
explained in more detail in the proof of Lemma 6. Consider the rst COST message that each agent
sends after this time and the earliest time when all of these COST messages have been processed by
their receiving agents. The lemma holds for a leaf agent a in the pseudo-tree with context X a since
a
a
LBX
a (d) = X a (d)
a
= X
a (d)

(Eq. 9)
(Eq. 1)

a
a
U BX
a (d) = X a (d)
a
= X
a (d)

(Eq. 12)
(Eq. 1)

for all values d after the considered time. Furthermore,
a
LBX
a =

=

a
U BX
a

min

a
{LBX
a (d)}

min

a
{X
a (d)}

(Eq. 10)

dDom(a)

(see above)

dDom(a)

a
= X
a
= min

(Eq. 2)
(Eq. 13)

a
{U BX
a (d)}
dDom(a)

=

min

a
{X
a (d)}

(see above)

dDom(a)

a
= X
a

(Eq. 2)

a
a
after the considered time. Thus, U BX
a = LBX a after the considered time (induction basis). Now
assume that the lemma holds for all agents of depth d in the pseudo-tree after the considered time
(induction assumption). We show that it then also holds for all agents of depth d  1 in the pseudotree after the considered time (induction step). For agent a with context X a

119

fiYeoh, Felner & Koenig

a
a
LBX
a (d) = X a (d) +



lba,c
X a (d)

(Eq. 9)

c
max{lba,c
X a (d), LBX c }

(Eq. 8)

cC(a)
a
= X
a (d) +



cC(a)
a
 X
a (d) +



c
LBX
c

cC(a)



a
X
a (d)

+



c
U BX
c

(induction assumption)

cC(a)
a
 X
a (d) +



c
min{uba,c
X a (d), U BX c }

cC(a)

=

a
X
a (d)

+



uba,c
X a (d)

(Eq. 11)

cC(a)
a
= U BX
a (d)

(Eq. 12)

a
for its value d after the considered time since all bounds no longer change. Thus, U BX
a (d) 
a
(d)
for
its
value
d
after
the
considered
time.
Since
agent
a
does
not
change
its
value
d after
LBX
a
a
a
a
a
the considered time, it must hold that LBX
(d)
<
min{T
H
,
U
B
}
[Line
23]
or
LB
a
Xa
Xa
X a (d) =
a
mindDom(a) {LBX
(d)}
[Line
24].
The
rst
disjunct
implies
that
a

a
a
a
min{T HX
a , U BX a }  U BX a
a
 U BX
a (d)
a
 LBX a (d)

(Eq. 13)
(see above)

a
a
< min{T HX
a , U BX a }

(rst disjunct)

for its value d, which is a contradiction. The second disjunct implies that
a
a
U BX
a  U BX a (d)
a
 LBX
a (d)

=

min

(Eq. 13)
(see above)

a
{LBX
a (d)}

(second disjunct)

dDom(a)

a
= LBX
a

(Eq. 10)

a
a
for its value d and thus that U BX
a  LBX a .

Theorem 1. BnB-ADOPT and its suboptimal variants terminate after a nite amount of time.
a
a
Proof. If BnB-ADOPT and its suboptimal variants do not terminate earlier, then U BX
a  LBX a
after a nite amount of time for all agents a  A and their contexts X a according to Lemma 8.
r
r
r
r
In particular, U BX
for the root agent r, where limr = LBX
r  LBX r  lim
r for BnB-ADOPT
r
r
with
b

0
for
BnB-ADOPT
and
limr = p  LBX
and BnB-ADOPTW HM , limr = b + LBX
r
r
AEM
with p  1 for BnB-ADOPTREM according to Section 4. Thus, both the termination condition
r
r
r
r
U BX
of its suboptimal
r  LBX r of BnB-ADOPT and the termination condition U BX r  lim
variants are satised.
r
Theorem 2. BnB-ADOPT terminates with the minimal solution cost X
r.

120

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

Proof. BnB-ADOPT terminates after a nite amount of time according to Theorem 1. The solution
r
r
r
cost of BnB-ADOPT is the upper bound U BX
r of the root agent r. U BX r  LBX r upon termination
r
r
r
according to its termination condition. w  U BX r  w  X r  LBX r according to Lemma 5.
r
r
r
Therefore, U BX
r = X r = LBX r since w = 1.
Theorem 3. BnB-ADOPTAEM terminates with a solution cost that is bounded from above by the
r
user-dened absolute error bound b plus the minimal solution cost X
r.
Proof. BnB-ADOPTAEM terminates after a nite amount of time according to Theorem 1. The
r
r
r
solution cost of BnB-ADOPTAEM is the upper bound U BX
r of the root agent r. U BX r  lim =
r
r
r
b + LBX r upon termination according to its termination condition. LBX r  w  X r according to
r
r
Lemma 5. Therefore, U BX
r  b + X r since w = 1.
Theorem 4. BnB-ADOPTREM terminates with a solution cost that is bounded from above by the
r
user-dened relative error bound p times the minimal solution cost X
r.
Proof. BnB-ADOPTREM terminates after a nite amount of time according to Theorem 1. The
r
r
r
solution cost of BnB-ADOPTREM is the upper bound U BX
r of the root agent r. U BX r  lim =
r
r
r
p  LBX r upon termination according to its termination condition. LBX r  w  X r according to
r
r
Lemma 5. Therefore, U BX
r  p  X r since w = 1.
Theorem 5. BnB-ADOPTW HM terminates with a solution cost that is bounded from above by the
r
user-dened weight w times the minimal solution cost X
r.
Proof. BnB-ADOPTW HM terminates after a nite amount of time according to Theorem 1. The
r
r
r
solution cost of BnB-ADOPTW HM is the upper bound U BX
r of the root agent r. U BX r  lim =
r
r
r
upon
termination
according
to
its
termination
condition.
LB

w


according
to
LBX
r
Xr
Xr
r
r
Lemma 5. Therefore, U BX

w


.
r
Xr

6. Experimental Evaluations
In this section, we compare BnB-ADOPT to two other memory-bounded DCOP search algorithms
that also restrict communication to agents that share constraints, namely ADOPT and NCBB. We
also compare the three suboptimal variants of BnB-ADOPT to each other. We use the distributed
DFS algorithm with the max-degree heuristic (Hamadi, Bessiere, & Quinqueton, 1998) that is used
by ADOPT to construct the pseudo-trees. We use DP2 (Ali et al., 2005) that is used by ADOPT to
pre-calculate the heuristic values for ADOPT and BnB-ADOPT. DP2 solves a relaxed version of the
given DCOP problem (where backedges are ignored) with a dynamic programming based approach.
NCBB calculates its own heuristic values during the search rather than in a pre-processing step.
6.1 Runtime Metrics
We use two common runtime metrics, namely non-concurrent constraint checks (Meisels, Kaplansky,
Razgon, & Zivan, 2002) and cycles (Modi et al., 2005).
 Non-concurrent constraint checks (NCCCs): NCCCs are a weighted sum of processing
and communication time. Every agent a maintains a counter N CCC a , which is initialized
to 0. The agent assigns N CCC a := N CCC a + 1 every time it performs a constraint check
to account for the time it takes to perform the constraint check. It assigns N CCC a :=

max{N CCC a , N CCC a + t} every time it receives a message from agent a to account for the

time it takes to wait for agent a to send the message (N CCC a ) and the transmission time
of the message (t). We use t = 0 to simulate fast communication and t = 1000 to simulate
slow communication. The number of NCCCs then is the largest counter value of any agent.

121

fiYeoh, Felner & Koenig

Sensors
1
3

2

Targets
5

6

7

8

9

4
10

11

12

13

Constraints

A unit

Figure 15: Example: Allocating Targets

Figure 16: Example: Scheduling Meetings

NCCCs are a good runtime metric if the ratio of processing and communication time can be
estimated reliably.
 Cycles: Cycles are time slices. A cycle is the time required for an agent to process all incoming
messages in its queue and send all outgoing messages, which are then processed by the receiving
agents in the next cycle. Thus, the number of cycles indicates the length of the longest chain
of messages between agents. Cycles are a good runtime metric if the communication time is
much larger than the processing time. Cycles will become a better and better runtime metric
in the future since the communication time is expected to remain relatively stable while the
processing time is expected to decrease (Silaghi, Lass, Sultanik, Regli, Matsui, & Yokoo, 2008).
6.2 DCOP Problem Types
We use three DCOP problem types in our experiments, namely graph coloring problems, sensor
network problems and meeting scheduling problems.
 Graph coloring: Graph coloring problems involve coloring the vertices of a graph, taking
restrictions between the colors of adjacent vertices into account. The agents are the vertices,
their domains are the colors, and the constraints are between adjacent vertices. We vary the
number of vertices from 5 to 15, the constraint density (= the ratio between the number of
constraints and the number of agents) from 2 (sparse graphs) to 3 (dense graphs) and the
range of constraint costs from a range of 0 to 1 (small range) to a range of 0 to 10,000 (large
range). Each agent always has three possible values. We average the experimental results over
50 DCOP problem instances with randomly generated constraints and randomly generated
integer constraint costs.
 Sensor network: Sensor network problems involve assigning targets to sensors in a sensor
network, taking restrictions in the availability of the sensors, restrictions in the number of
sensors that need to track each target and the priorities of the targets into account. The
agents are the targets, their domains are the time slots when they can be tracked, and the
constraints are between adjacent targets (Maheswaran et al., 2004b). Figure 15 shows a sensor
network where the targets are located on a grid and each target is surrounded by four sensors,
all of which are needed to track the target. We vary the number of targets from 4 to 15. We
always use 8 time slots. The cost of assigning a time slot to a target that is also assigned to an
adjacent target is innity (to be precise: 1,000,000) since the same sensor cannot track both
targets during the same time slot. The cost of targets that are not tracked during any time
slot is 100. All other costs are in the range of 0 to 100. We average the experimental results
over 50 DCOP problem instances with randomly generated integer constraint costs.
 Meeting scheduling: Meeting scheduling problems involve scheduling meetings between the
employees of a company, taking restrictions in their availability as well as their priorities into
account. The agents are the meetings, their domains are the time slots when they can be
held, and the constraints are between meetings that share participants (Maheswaran et al.,
122

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

Graph Coloring, Density = 2
Communication Cost = 0

Graph Coloring, Density = 2
Communication Cost = 1000

1.E+08
ADOPT
BnB-ADOPT
NCBB

1.E+04

NCCC

NCCC

1.E+05

1.E+03
1.E+02

ADOPT
BnB-ADOPT
NCBB

1.E+07
1.E+06
1.E+05

5

6

7

8

9

10

11

12

13

14

5

6

7

Number of Vertices

8

9

(a)
Graph Coloring, Density = 2

12

13

14

Graph Coloring, Density = 3
Communication Cost = 0

1.E+06

ADOPT
BnB-ADOPT
NCBB

NCCC

1.E+05
Cycles

11

(b)

1.E+04

1.E+03
ADOPT
BnB-ADOPT
NCBB

1.E+04
1.E+03
1.E+02

1.E+02
5

6

7

8
9
10 11
Number of Vertices

12

13

7

14

8

9

11

12

13

14

(d)

Graph Coloring, Density = 3
Communication Cost = 1000

1.E+09

10

Number of Vertices

(c)

Graph Coloring, Density = 3
1.E+05

Cycles

ADOPT
BnB-ADOPT
NCBB

1.E+08
NCCC

10

Number of Vertices

1.E+07
1.E+06
1.E+05

1.E+04
ADOPT
BnB-ADOPT
NCBB

1.E+03
1.E+02

7

8

9

10

11

12

13

14

7

8

9

10

11

12

13

14

Number of Vertices

Number of Vertices

(e)

(f)

Figure 17: Experimental Results Comparing ADOPT, BnB-ADOPT and NCBB for Graph Coloring
Problems with Constraint Costs Ranging from 0 to 10,000

2004b). Figure 16 shows a hierarchical organization with 4 units of a supervisor and its three
subordinates. For example, supervisor 2 has three subordinates 5, 6 and 7. In each unit, we
assume ve possible meetings: one of the entire unit (e.g., 2, 5, 6, 7), two parent-child meetings
(e.g., 2, 5 and 2, 7) and two sibling-sibling meetings (e.g., 5, 6 and 6, 7). We vary the number
of meetings from 5 (1 unit) to 20 (4 units). We always use 8 time slots. The cost of assigning
a time slot to a meeting that has at least one participant who has another meeting during the
same time slot is innity (to be precise: 1,000,000) since the same person cannot attend more
than one meeting at a time. The cost of a non-scheduled meeting is 100. All other costs are in
the range of 0 to 100. We average the experimental results over 50 DCOP problem instances
with randomly generated integer constraint costs.

123

fiYeoh, Felner & Koenig

1.E+05

Graph Coloring, Density = 2
Communication Cost = 0

1.E+08
1.E+07
NCCC

NCCC

1.E+04

Graph Coloring, Density = 2
Communication Cost = 1000

1.E+03
ADOPT
BnB-ADOPT
NCBB

1.E+02

1.E+06
ADOPT
BnB-ADOPT
NCBB

1.E+05

1.E+01

1.E+04
0-1

0-10

0-100

0-1,000 0-10,000

0-1

Range of Constraint Costs

0-10

0-100

0-1,000 0-10,000

Range of Constraint Costs

(a)

(b)

Graph Coloring, Density = 2
1.E+06

1.E+04

Graph Coloring, Density = 3
Communication Cost = 0

NCCC

Cycles

1.E+05
1.E+03
ADOPT
BnB-ADOPT
NCBB

1.E+02

1.E+04
1.E+03

ADOPT
BnB-ADOPT
NCBB

1.E+02

1.E+01

1.E+01
0-1

0-10

0-100

0-1,000 0-10,000

0-1

Range of Constraint Costs

(c)

1.E+09

0-100

0-1,000 0-10,000

(d)

Graph Coloring, Density = 3
Communication Cost = 1000

Graph Coloring, Density = 3
1.E+05

1.E+08

1.E+04
Cycles

NCCC

0-10

Range of Constraint Costs

1.E+07
1.E+06

ADOPT
BnB-ADOPT
NCBB

1.E+05

1.E+03

ADOPT
BnB-ADOPT
NCBB

1.E+02

1.E+04

1.E+01
0-1

0-10

0-100

0-1,000 0-10,000

Range of Constraint Costs

0-1

0-10

0-100

0-1,000 0-10,000

Range of Constraint Costs

(e)

(f)

Figure 18: Experimental Results Comparing ADOPT, BnB-ADOPT and NCBB for Graph Coloring
Problems with 10 Vertices

6.3 Experimental Results: Optimal DCOP Search Algorithms
We rst compare BnB-ADOPT to ADOPT and NCBB. Figure 17 shows our experimental results
for graph coloring problems with constraint costs ranging from 0 to 10,000, where we varied the
number of vertices, while Figure 18 shows our experimental results for graph coloring problems
with 10 vertices, where we varied the range of constraint costs. Figures 17(a-c) and 18(a-c) show
the results for coloring sparse graphs, and Figures 17(d-f) and 18(d-f) show the results for coloring
dense graphs. The y-axes are in log scale and show the runtimes in NCCCs or cycles. DCOP search
algorithms on sparse graphs are faster than on dense graphs because, for example, there is a larger
likelihood of independent DCOP subproblems in sparse graphs. BnB-ADOPT is generally faster
than NCBB on sparse graphs but not on dense graphs because BnB-ADOPT allows agents to send
messages only to their parent agents in the pseudo-tree (along edges of the pseudo-tree) but NCBB

124

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

Sensor Network
Communication Cost = 1000

1.E+06

1.E+09

1.E+05

1.E+08
NCCC

NCCC

Sensor Network
Communication Cost = 0

1.E+04
1.E+03

ADOPT
BnB-ADOPT
NCBB

1.E+02

ADOPT
BnB-ADOPT
NCBB

1.E+07
1.E+06
1.E+05

1.E+01

1.E+04
4

5

6

4

7 8 9 10 11 12 13 14 15
Number of Targets

5

6

(a)

(b)

Sensor Network

Meeting Scheduling
Communication Cost = 0

1.E+05

1.E+06
ADOPT
BnB-ADOPT
NCBB

1.E+05
NCCC

Cycles

1.E+04
1.E+03
1.E+02

1.E+04
ADOPT
BnB-ADOPT
NCBB

1.E+03

1.E+01

1.E+02
4

5

6

5

7 8 9 10 11 12 13 14 15
Number of Targets

6

7

(c)

13

14

15

Meeting Scheduling
1.E+05

1.E+07

1.E+04
Cycles

1.E+08

1.E+06
ADOPT
BnB-ADOPT
NCBB

1.E+05

8
9 10 11 12
Number of Meetings

(d)

Meeting Scheduling
Communication Cost = 1000

NCCC

7 8 9 10 11 12 13 14 15
Number of Targets

1.E+03
ADOPT
BnB-ADOPT
NCBB

1.E+02

1.E+04

1.E+01
5

6

7

8
9 10 11 12
Number of Meetings

13

14

15

(e)

5

6

7

8
9 10 11 12
Number of Meetings

13

14

15

(f)

Figure 19: Experimental Results Comparing ADOPT, BnB-ADOPT and NCBB for Sensor Network
and Meeting Scheduling Problems

allows agents also to send messages to their pseudo-parent agents (along backedges of the pseudotree). Thus, agents of NCBB receive updates faster than agents of BnB-ADOPT. This eect is more
prevalent in dense graphs since there are more backedges in dense graphs. However, the dierence
between BnB-ADOPT and NCBB becomes negligible when communication is slow.
Figure 17 shows that BnB-ADOPT is at least half an order of magnitude faster than ADOPT
when the number of vertices is small. The speedup over ADOPT increases as the number of vertices
gets larger and the DCOP problems thus become more complex. Similarly, Figure 18 shows that the
speedup over ADOPT increases as the range of constant costs increases and the DCOP problems
thus become more complex. However, ADOPT can be faster than BnB-ADOPT for simple DCOP
problems. For example, ADOPT requires fewer cycles than BnB-ADOPT for DCOP problems with
constraint costs ranging from 0 to 1. Figure 19 shows the same trend for sensor network and meeting
scheduling problems. The reason for this behavior is as follows. ADOPT uses memory-bounded best125

fiYeoh, Felner & Koenig

Sensor Network
Communication Cost = 0

1.E+05

Sensor Network
Communication Cost = 1000

1.E+06

NCCC

NCCC

1.E+04
1.E+03
ADOPT

1.E+02

1.E+05
ADOPT

BnB-ADOPT

BnB-ADOPT

1.E+01

1.E+04
0.5

0.6

0.7
0.8
Weight

0.9

1

0.5

0.6

(a)

0.9

1

0.9

1

(b)

Sensor Network
1.E+03

Sensor Network
Unique Contexts Explored

1.E+02
No. of
Contexts

Cycles

0.7
0.8
Weight

1.E+02
ADOPT

ADOPT
BnB-ADOPT

BnB-ADOPT

1.E+01

1.E+01
0.5

0.6

0.7
0.8
Weight

0.9

1

0.5

0.6

(c)

0.7
0.8
Weight

(d)
Sensor Network
Repeated Contexts Explored

No. of
Contexts

1.E+03
1.E+02

ADOPT
BnB-ADOPT

1.E+01
1.E+00
0.5

0.6

0.7
0.8
Weight

0.9

1

(e)

Figure 20: Experimental Results on the Cause of Speedup for ADOPT and BnB-ADOPT
rst search and thus exploits the heuristic values well but needs to repeatedly reconstruct partial
solutions that it purged from memory, especially if the heuristic values are poorly informed. BnBADOPT uses depth-rst branch-and-bound search and thus does not exploit the heuristic values
quite as well but does not have to repeatedly reconstruct partial solutions. ADOPT can thus be
faster than BnB-ADOPT for DCOP problems with well informed heuristic values, such as simple
DCOP problems.
We conrm this intuition with an additional experiment on sensor network problems with four
targets and dierent informedness of heuristic values. We use the heuristic values cha,c
X a (d) for 0.5 
c  1, where ha,c
X a (d) are the heuristic values calculated by DP2, as used until now. Figures 20(a-c)
show the number of NCCCs for dierent weights c. When the heuristic values are well informed (large
weights), ADOPT can indeed be faster than BnB-ADOPT. Since ADOPT relies on the heuristic
values more than BnB-ADOPT, the speedup of ADOPT is much larger than that of BnB-ADOPT
as the heuristic values get more informed. Figures 20(d) and 20(e) show the number of unique

126

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

(= dierent) and repeated contexts per agent for dierent weights c. When the heuristic values are
well informed (large weights), agents of ADOPT explore fewer unique contexts than agents of BnBADOPT since they are more focused in their search. However, when the heuristic values are poorly
informed (small weights), they explore more unique contexts. Agents of ADOPT explore many more
repeated contexts than agents of BnB-ADOPT since they need to reconstruct partial solutions that
they purged from memory. Agents of BnB-ADOPT explore a few repeated contexts even though it
does not have to reconstruct partial solutions. The reason for this behavior is the distributed nature
of BnB-ADOPT. For example, assume that the context of an agent is {(a1 , 0), (a2 , 0)} and the next
context of a centralized variant of BnB-ADOPT would be {(a1 , 1), (a2 , 1)} (where the IDs are omitted
for simplicity). The agent updates its context to {(a1 , 1), (a2 , 0)} when it receives the message from
agent a1 that it takes on value 1. The agent then updates its context to {(a1 , 1), (a2 , 1)} when it
receives the message from agent a2 that it takes on value 1. Thus, the agent explores the intermediate
context {(a1 , 1), (a2 , 0)} that a centralized variant of BnB-ADOPT would not explore. It counts as a
repeated context if the agent explores this context intentionally in the future. Overall, BnB-ADOPT
tends to be faster than ADOPT if the heuristic values are poorly informed (small weights). Thus,
BnB-ADOPT has great potential as a DCOP search algorithm since heuristic values are often poorly
informed for complex DCOP problems, such as DCOP problems with large numbers of agents, large
domains, large numbers of constraints or large ranges of constraint costs.
6.4 Experimental Results: Suboptimal Variants of BnB-ADOPT
We now compare the three suboptimal variants of BnB-ADOPT to each other. The experimental
setup is identical to the one for the optimal DCOP search algorithms, except as follows: For graph
coloring problems, the number of vertices is 10, the range of constraint costs is 0 to 10,000 and the
constraint density is 2; for sensor network problems, the number of targets is 9; and for meeting
scheduling problems, the number of meetings is 10. We measure the runtimes in cycles. (The results
for NCCCs are similar.) However, we report normalized runtimes, that is, the runtimes divided by
the runtime for nding a cost-minimal solution with BnB-ADOPT. Thus, the normalized runtime
0.25 refers to one quarter of the number of cycles that it takes to nd a cost-minimal solution with
BnB-ADOPT. Similarly, we report normalized solution costs, that is, the solution costs divided by
the minimal solution costs. Thus, the normalized solution cost 2.5 refers to a solution cost that is
two and a half times larger than the minimal solution cost. We vary the relative error bound (which
is the worst acceptable normalized solution cost) from 1.0 to 4.0. The relative error bound is p for
BnB-ADOPTREM and w for BnB-ADOPTW HM . We pre-calculate the minimal solution costs to
set the correct value of b for BnB-ADOPTAEM . For example, if the minimal solution cost is 100 and
the relative error bound is 2.5, then p = 2.5 for BnB-ADOPTREM , w = 2.5 for BnB-ADOPTW HM
and b = (2.5  1)  100 = 150 for BnB-ADOPTAEM .
Figure 21(a-c) shows our experimental results for graph coloring problems. Figure 21(a) shows
that the normalized solution costs of all three suboptimal variants increase as the relative error
bound increases. However, the solution costs remain much smaller than the error bound. For
example, the normalized solution costs of all three suboptimal variants are less than 1.3 (rather
than 3) when the relative error bound is 3. The normalized solution costs of BnB-ADOPTAEM are
usually larger than the normalized solution costs of BnB-ADOPTREM for the same relative error
r
r
bound. The reason for this behavior is that BnB-ADOPTAEM terminates when U BX
=
r  lim
r
r
r
r
b + LBX r = (p  1)  X r + LBX r , where X r is the minimal solution cost. Thus, the solution cost of
r
r
r
BnB-ADOPTAEM can be at most U BX
r  LBX r  (p  1)  X r larger than the minimal solution
r
r
r
cost. On the other hand, BnB-ADOPTREM terminates when U BX
r  lim = p  LBX r . Thus, the
r
r
r
solution cost of BnB-ADOPTREM can be at most U BX r  LBX r  (p  1)  LBX r larger than the
minimal solution cost. The absolute error bound of BnB-ADOPTAEM is thus no smaller than the
r
r
absolute error bound of BnB-ADOPTREM since X
r  LBX r but is initially strictly greater than
r
r
the absolute error bound of BnB-ADOPTREM since X r > LBX
r during most of the search.

127

fiYeoh, Felner & Koenig

Graph Coloring
Solution Cost of BnB-ADOPT Variants

Graph Coloring
Computation Time of BnB-ADOPT Variants
1.00
Normalized Runtimes
(Cycles)

Normalized Costs

1.35
1.30
1.25
1.20
1.15

Weighted Heuristics (WHM)
Absolute Error (AEM)
Relative Error (REM)

1.10
1.05
1.00
1.00

1.50

2.00
2.50
3.00
Relative Error Bound

3.50

0.80

0.40
0.20
0.00
1.00

4.00

Weighted Heuristics (WHM)
Absolute Error (AEM)
Relative Error (REM)

0.60

1.50

2.00
2.50
3.00
Relative Error Bound

(a)
Graph Coloring
Performance of BnB-ADOPT Variants

Sensor Network
Performance of BnB-ADOPT Variants
1.00

0.80

Normalized Runtimes
(Cycles)

Normalized Runtimes
(Cycles)

4.00

(b)

1.00
Weighted Heuristics (WHM)
Absolute Error (AEM)
Relative Error (REM)

0.60
0.40
0.20
0.00
1.00

3.50

1.05

1.10
1.15
1.20
Normalized Costs

1.25

1.30

0.80

Weighted Heuristics (WHM)
Absolute Error (AEM)
Relative Error (REM)

0.60
0.40
0.20
0.00
1.00

1.02

1.04

(c)

1.06
1.08
1.10
Normalized Costs

1.12

1.14

(d)
Meeting Scheduling
Performance of BnB-ADOPT Variants
Normalized Runtimes
(Cycles)

1.00
0.80

Weighted Heuristics (WHM)
Absolute Error (AEM)
Relative Error (REM)

0.60
0.40
0.20
0.00
1.00

1.05

1.10
1.15
Normalized Costs

1.20

1.25

(e)

Figure 21: Experimental Results Comparing the Suboptimal Variants of BnB-ADOPT
Figure 21(b) shows that the normalized runtimes of all three suboptimal variants decrease as the
relative error bound increases. They decrease to almost 0 when the relative error bound is about 2.0.
Therefore, all three suboptimal variants terminate almost immediately after nding the rst solution.
The normalized runtimes of BnB-ADOPTAEM are usually smaller than the normalized runtimes of
BnB-ADOPTREM for the same relative error bound since BnB-ADOPTAEM can terminate with a
suboptimal solution cost that is within its absolute error bound but not yet within the absolute error
bound of BnB-ADOPTREM if the absolute error bound of BnB-ADOPTAEM is strictly greater than
the absolute error bound of BnB-ADOPTREM . In other words, BnB-ADOPTAEM can terminate
r
r
r
with a suboptimal solution cost (p  1)  LBX
r < U BX r  (p  1)  X r while BnB-ADOPTREM can
not.
Figure 21(c) shows the normalized runtimes needed to achieve a given normalized solution cost.
BnB-ADOPTW HM terminates faster than BnB-ADOPTAEM , which in turn terminates faster than
BnB-ADOPTREM . For example, the normalized runtime needed to achieve the normalized solu-

128

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

tion cost 1.05 is about 0.18 for BnB-ADOPTW HM , 0.30 for BnB-ADOPTAEM and 0.35 for BnBADOPTREM . Thus, BnB-ADOPTW HM is the suboptimal variant of BnB-ADOPT with the best
performance. Figures 21(d-e) show the same trend for sensor network and meeting scheduling problems.

7. Conclusions
In this article, we introduced Branch-and-Bound ADOPT (BnB-ADOPT), a memory-bounded
DCOP search algorithm. BnB-ADOPT uses the message passing and communication framework
of ADOPT but changes the search strategy of ADOPT from best-rst search to depth-rst branchand-bound search to make ADOPT faster by taking advantage of the fact that DCOP problems have
depth-bounded search trees. The other properties of BnB-ADOPT are similar to those of ADOPT.
BnB-ADOPT allows agents to operate concurrently (in order to decrease the runtime) and asynchronously (in order to increase robustness). BnB-ADOPT restricts communication to agents that
share constraints (in order to t the restrictions of applications such as sensor networks). Finally,
BnB-ADOPT orders agents into a pseudo-tree (in order to take advantage of independent DCOP
subproblems). Our experimental results showed that BnB-ADOPT nds cost-minimal solutions up
to one order of magnitude faster than ADOPT for a variety of large DCOP problems and is as fast
as NCBB for most of these DCOP problems. The reason for this behavior is the following: Agents
of NCBB operate sequentially and are thus often idle. ADOPT can construct fewer partial solutions
than BnB-ADOPT but has to reconstruct some partial solutions that it purged from memory. The
advantage of ADOPT with respect to the number of constructed partial solutions decreases and
its disadvantage with respect to the number of reconstructed partial solutions increases as heuristic
values become more poorly informed. Thus, BnB-ADOPT has great potential as a DCOP search
algorithm since heuristic values are often poorly informed for complex DCOP problems such as
DCOP problems with large numbers of agents, large domains, large numbers of constraints or large
ranges of constraint costs.
We also investigated three approximation mechanisms that trade o the solution cost of BnBADOPT for a smaller runtime, namely the Absolute Error Mechanism from ADOPT (resulting in
BnB-ADOPTAEM ), the new Relative Error Mechanism (resulting in BnB-ADOPTREM ) and the
new Weighted Heuristics Mechanism (resulting in BnB-ADOPTW HM ). The two new approximation mechanisms allow users to specify a relative error bound, which is often more meaningful than
an absolute error bound. The Weighted Heuristics Mechanism dominated both the Absolute Error Mechanism and the Relative Error Mechanism in our experiments and should apply to other
DCOP search algorithms as well since they all benet from using heuristic values to focus their
searches (Yeoh, Koenig, & Sun, 2008b).
In the future, we plan to improve BnB-ADOPT in the following ways: First, we would like to
reduce the number of sent messages and handle lost messages. Second, we would like to study
how dierent pseudo-tree arrangements (Atlas & Decker, 2007; Sultanik, Lass, & Regli, 2009) and
pre-processing techniques (Matsui et al., 2009) aect the eciency of BnB-ADOPT. Finally, we
would like to compare BnB-ADOPT and its approximation mechanisms to other DCOP algorithms,
including OptAPO, DPOP and their variants (Petcu & Faltings, 2005a, 2006).

Acknowledgments
This article is an extension of two earlier publications (Yeoh, Felner, & Koenig, 2008a; Yeoh et al.,
2008b) and contains additional expositions, examples and proofs. We thank Anton Chechetka for
providing us with his implementation of NCBB and the anonymous reviewers for their helpful
comments. This research was done while Ariel Felner spent his sabbatical at the University of
Southern California, visiting Sven Koenig. This research has been partly supported by a U.S. Army

129

fiYeoh, Felner & Koenig

Research Laboratory (ARL) and U.S. Army Research Oce (ARO) award to Sven Koenig under
grant W911NF-08-1-0468, by a Oce of Naval Research (ONR) award to Sven Koenig under grant
N00014-09-1-1031, by a National Science Foundation (NSF) award to Sven Koenig under grant
0413196 and by an Israeli Science Foundation (ISF) award to Ariel Felner under grants 728/06 and
305/09. The views and conclusions contained in this document are those of the authors and should
not be interpreted as representing the ocial policies, either expressed or implied, of the sponsoring
organizations, agencies, companies or the U.S. government.

References
Ali, S., Koenig, S., & Tambe, M. (2005). Preprocessing techniques for accelerating the DCOP
algorithm ADOPT. In Proceedings of the International Joint Conference on Autonomous
Agents and Multiagent Systems (AAMAS), pp. 10411048.
Atlas, J., & Decker, K. (2007). A complete distributed constraint optimization method for nontraditional pseudotree arrangements. In Proceedings of the International Joint Conference on
Autonomous Agents and Multiagent Systems (AAMAS), pp. 736743.
Bayardo, R., & Miranker, D. (1995). On the space-time trade-o in solving constraint satisfaction problems. In Proceedings of the International Joint Conference on Articial Intelligence
(IJCAI), pp. 558562.
Bistarelli, S., Montanari, U., Rossi, F., Schiex, T., Verfaillie, G., & Fargier, H. (1999). Semiring-based
CSPs and valued CSPs: Basic properties and comparison. Constraints, 4 (3), 199240.
Bowring, E., Pearce, J., Portway, C., Jain, M., & Tambe, M. (2008). On k-optimal distributed
constraint optimization algorithms: New bounds and algorithms. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp.
607614.
Bowring, E., Tambe, M., & Yokoo, M. (2006). Multiply-constrained distributed constraint optimization. In Proceedings of the International Joint Conference on Autonomous Agents and
Multiagent Systems (AAMAS), pp. 14131420.
Burke, D., & Brown, K. (2006). Eciently handling complex local problems in distributed constraint
optimisation. In Proceedings of the European Conference on Articial Intelligence (ECAI), pp.
701702.
Chechetka, A., & Sycara, K. (2006). No-commitment branch and bound search for distributed
constraint optimization. In Proceedings of the International Conference on Autonomous Agents
and Multiagent Systems (AAMAS), pp. 14271429.
Choxi, H., & Modi, P. (2007). A distributed constraint optimization approach to wireless network
optimization. In Proceedings of the AAAI-07 Workshop on Conguration, pp. 18.
Davin, J., & Modi, P. (2006). Hierarchical variable ordering for multiagent agreement problems.
In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent
Systems (AAMAS), pp. 14331435.
Dechter, R. (Ed.). (2003). Constraint Processing. Morgan Kaufmann.
Fitzpatrick, S., & Meertens, L. (2003). Distributed coordination through anarchic optimization.
In Lesser, V., Ortiz, C., & Tambe, M. (Eds.), Distributed Sensor Networks: A Multiagent
Perspective, pp. 257295. Kluwer.
Freuder, E., & Quinn, M. (1985). Taking advantage of stable sets of variables in constraint satisfaction problems. In Proceedings of the International Joint Conference on Articial Intelligence
(IJCAI), pp. 10761078.

130

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

Gershman, A., Meisels, A., & Zivan, R. (2009). Asynchronous Forward-Bounding for distributed
COPs. Journal of Articial Intelligence Research, 34, 6188.
Greenstadt, R. (2009). An overview of privacy improvements to k-optimal DCOP algorithms (extended abstract). In Proceedings of the International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS), pp. 12791280.
Greenstadt, R., Grosz, B., & Smith, M. (2007). SSDPOP: Improving the privacy of DCOP with
secret sharing. In Proceedings of the International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS), pp. 10981100.
Hamadi, Y., Bessiere, C., & Quinqueton, J. (1998). Distributed intelligent backtracking. In Proceedings of the European Conference on Articial Intelligence (ECAI), pp. 219223.
Hirayama, K., & Yokoo, M. (1997). Distributed partial constraint satisfaction problem. In Proceedings of the International Conference on Principles and Practice of Constraint Programming
(CP), pp. 222236.
Jain, M., Taylor, M., Tambe, M., & Yokoo, M. (2009). DCOPs meet the real world: Exploring
unknown reward matrices with applications to mobile sensor networks. In Proceedings of the
International Joint Conference on Articial Intelligence (IJCAI), pp. 181186.
Junges, R., & Bazzan, A. (2008). Evaluating the performance of DCOP algorithms in a real world,
dynamic problem. In Proceedings of the International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS), pp. 599606.
Korf, R. (1993). Linear-space best-rst search. Articial Intelligence, 62 (1), 4178.
Kumar, A., Faltings, B., & Petcu, A. (2009). Distributed constraint optimization with structured
resource constraints. In Proceedings of the International Joint Conference on Autonomous
Agents and Multiagent Systems (AAMAS), pp. 923930.
Lesser, V., Ortiz, C., & Tambe, M. (Eds.). (2003). Distributed Sensor Networks: A Multiagent
Perspective. Kluwer.
Maheswaran, R., Pearce, J., & Tambe, M. (2004a). Distributed algorithms for DCOP: A graphical game-based approach. In Proceedings of the International Conference on Parallel and
Distributed Computing Systems (PDCS), pp. 432439.
Maheswaran, R., Tambe, M., Bowring, E., Pearce, J., & Varakantham, P. (2004b). Taking DCOP to
the real world: Ecient complete solutions for distributed event scheduling. In Proceedings of
the International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS),
pp. 310317.
Mailler, R., & Lesser, V. (2004). Solving distributed constraint optimization problems using cooperative mediation. In Proceedings of the International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS), pp. 438445.
Marinescu, R., & Dechter, R. (2007). Best-rst AND/OR search for graphical models. In Proceedings
of the AAAI Conference on Articial Intelligence (AAAI), pp. 11711176.
Marinescu, R., & Dechter, R. (2009). AND/OR branch-and-bound search for combinatorial optimization in graphical models. Articial Intelligence, 173 (16-17), 14571491.
Matsui, T., Silaghi, M., Hirayama, K., Yokoo, M., & Matsuo, H. (2009). Directed soft arc consistency
in pseudo trees. In Proceedings of the International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS), pp. 10651072.
Meisels, A., Kaplansky, E., Razgon, I., & Zivan, R. (2002). Comparing performance of distributed
constraints processing algorithms. In Proceedings of the Distributed Constraint Reasoning
Workshop, pp. 8693.

131

fiYeoh, Felner & Koenig

Modi, P., & Ali, S. (2004). Distributed constraint reasoning under unreliable communication. In
Zhang, W., & Sorge, V. (Eds.), Frontiers in Articial Intelligence and Applications, Vol. 112,
pp. 141150. IOS Press.
Modi, P., Shen, W.-M., Tambe, M., & Yokoo, M. (2005). ADOPT: Asynchronous distributed constraint optimization with quality guarantees. Articial Intelligence, 161 (1-2), 149180.
Ottens, B., & Faltings, B. (2008). Coordinating agent plans through distributed constraint optimization. In Proceedings of the ICAPS-08 Workshop on Multiagent Planning.
Pearce, J., & Tambe, M. (2007). Quality guarantees on k-optimal solutions for distributed constraint
optimization problems. In Proceedings of the International Joint Conference on Articial
Intelligence (IJCAI), pp. 14461451.
Pecora, F., Modi, P., & Scerri, P. (2006). Reasoning about and dynamically posting n-ary constraints
in ADOPT. In Proceedings of the Distributed Constraint Reasoning Workshop, pp. 5771.
Petcu, A., & Faltings, B. (2005a). Approximations in distributed optimization. In Proceedings of
the International Conference on Principles and Practice of Constraint Programming (CP), pp.
802806.
Petcu, A., & Faltings, B. (2005b). A scalable method for multiagent constraint optimization. In
Proceedings of the International Joint Conference on Articial Intelligence (IJCAI), pp. 1413
1420.
Petcu, A., & Faltings, B. (2006). ODPOP: An algorithm for open/distributed constraint optimization. In Proceedings of the National Conference on Articial Intelligence (AAAI), pp. 703708.
Pohl, I. (1970). First results on the eect of error in heuristic search. Machine Intelligence, 5,
219236.
Pohl, I. (1973). The avoidance of (relative) catastrophe, heuristic competence, genuine dynamic
weighting and computational issues in heuristic problem solving. In Proceedings of the International Joint Conference on Articial Intelligence (IJCAI), pp. 1217.
Schiex, T., Fargier, H., & Verfaillie, G. (1995). Valued constraint satisfaction problems: Hard and
easy problems. In Proceedings of the International Joint Conference on Articial Intelligence
(IJCAI), pp. 631637.
Schurr, N., Okamoto, S., Maheswaran, R., Scerri, P., & Tambe, M. (2005). Evolution of a teamwork
model. In Sun, R. (Ed.), Cognition and Multi-Agent Interaction: From Cognitive Modeling to
Social Simulation, pp. 307327. Cambridge University Press.
Silaghi, M., Landwehr, J., & Larrosa, J. (2004). Asynchronous branch & bound and A* for disWCSPs
with heuristic function based on consistency-maintenance. In Zhang, W., & Sorge, V. (Eds.),
Frontiers in Articial Intelligence and Applications, Vol. 112, pp. 4962. IOS Press.
Silaghi, M., Lass, R., Sultanik, E., Regli, W., Matsui, T., & Yokoo, M. (2008). The operation point
units of distributed constraint solvers. In Proceedings of the Distributed Constraint Reasoning
Workshop, pp. 116.
Silaghi, M., & Yokoo, M. (2009). ADOPT-ing: Unifying asynchronous distributed optimization with
asynchronous backtracking. Autonomous Agents and Multi-Agent Systems, 19 (2), 89123.
Stranders, R., Farinelli, A., Rogers, A., & Jennings, N. (2009). Decentralised coordination of mobile
sensors using the Max-Sum algorithm. In Proceedings of the International Joint Conference
on Articial Intelligence (IJCAI), pp. 299304.
Sultanik, E., Lass, R., & Regli, W. (2009). Dynamic conguration of agent organizations. In
Proceedings of the International Joint Conference on Articial Intelligence (IJCAI), pp. 305
311.

132

fiBnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm

Yeoh, W., Felner, A., & Koenig, S. (2008a). BnB-ADOPT: An asynchronous branch-and-bound
DCOP algorithm. In Proceedings of the International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS), pp. 591598.
Yeoh, W., Koenig, S., & Sun, X. (2008b). Trading o solution cost for smaller runtime in DCOP
search algorithms (short paper). In Proceedings of the International Joint Conference on
Autonomous Agents and Multiagent Systems (AAMAS), pp. 14451448.
Yeoh, W., Varakantham, P., & Koenig, S. (2009). Caching schemes for DCOP search algorithms.
In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent
Systems (AAMAS), pp. 609616.
Yokoo, M., & Hirayama, K. (1996). Distributed breakout algorithm for solving distributed constraint
satisfaction problems. In Proceedings of the International Conference on Multiagent Systems
(ICMAS), pp. 401408.
Zhang, W., & Korf, R. (1995). Performance of linear-space search algorithms. Articial Intelligence,
79 (2), 241292.
Zhang, W., Xing, Z., Wang, G., & Wittenburg, L. (2003). An analysis and application of distributed
constraint satisfaction and optimization algorithms in sensor networks. In Proceedings of the
International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS),
pp. 185192.
Zivan, R. (2008). Anytime local search for distributed constraint optimization. In Proceedings of
the AAAI Conference on Articial Intelligence (AAAI), pp. 393398.
Zivan, R., Glinton, R., & Sycara, K. (2009). Distributed constraint optimization for large teams
of mobile sensing agents. In Proceedings of the International Conference on Intelligent Agent
Technology (IAT), pp. 347354.

133

fiJournal of Artificial Intelligence Research 38 (2010) 223-269

Submitted 11/09; published 05/10

Grounding FO and FO(ID) with Bounds
Johan Wittocx
Maarten Marien
Marc Denecker

johan.wittocx@cs.kuleuven.be
maarten.marien@cs.kuleuven.be
marc.denecker@cs.kuleuven.be

Katholieke Universiteit Leuven
Department of Computer Science
Celestijnenlaan 200A, 3001 Heverlee, Belgium

Abstract
Grounding is the task of reducing a first-order theory and finite domain to an equivalent
propositional theory. It is used as preprocessing phase in many logic-based reasoning systems. Such
systems provide a rich first-order input language to a user and can rely on efficient propositional
solvers to perform the actual reasoning.
Besides a first-order theory and finite domain, the input for grounders contains in many applications also additional data. By exploiting this data, the size of the grounders output can often
be reduced significantly. A common practice to improve the efficiency of a grounder in this context
is by manually adding semantically redundant information to the input theory, indicating where
and when the grounder should exploit the data. In this paper we present a method to compute
and add such redundant information automatically. Our method therefore simplifies the task of
writing input theories that can be grounded efficiently by current systems.
We first present our method for classical first-order logic (FO) theories. Then we extend it
to FO(ID), the extension of FO with inductive definitions, which allows for more concise and
comprehensive input theories. We discuss implementation issues and experimentally validate the
practical applicability of our method.

1. Introduction
Grounding, or propositionalization, is the task of reducing a first-order theory and finite domain to
an equivalent propositional theory, called a grounding. Grounding is used as a preprocessing phase
in many logic-based reasoning systems. It serves to provide the user with a rich input language,
while enabling the system to rely on efficient propositional solvers to perform the actual reasoning.
Examples of systems that rely on grounding can be found in the area of finite first-order model
generation (Claessen & Sorensson, 2003; McCune, 2003; East, Iakhiaev, Mikitiuk, & Truszczynski,
2006; Mitchell, Ternovska, Hach, & Mohebali, 2006; Torlak & Jackson, 2007; Wittocx, Marien, &
Denecker, 2008d). Such systems are in turn used as part of theorem provers (Claessen & Sorensson,
2003) and for lightweight software verification (Jackson, 2006). Currently, almost all Answer Set
Programming (ASP) systems rely on grounding as a preprocessing phase (Gebser, Schaub, & Thiele,
2007; Perri, Scarcello, Catalano, & Leone, 2007; Syrjanen, 2000; Syrjanen, 2009). Also in planning
systems (Kautz & Selman, 1996) and relational data mining (Krogel, Rawles, Zelezny, Flach, Lavrac,
& Wrobel, 2003) grounding is frequently used. This large number of applications indicates the
importance of grounding in logic-based reasoning systems and the need to develop efficient grounders.
A basic (naive) grounding method is by instantiating the variables in the input theory by all
possible combinations of domain elements. Grounding in this way is polynomial in the size of the
domain but exponential in the maximum width of a formula in the input theory, and may easily
produce groundings of unwieldy size. Several techniques have been developed to efficiently produce
smaller groundings. There are two main categories of such techniques. In the first, the input theory
is rewritten such that the maximum width of the formulas decreases. Methods like clause splitting
(Schulz, 2002) and partitioning (Ramachandran & Amir, 2005) belong to this category.

c
2010
AI Access Foundation. All rights reserved.

fiWittocx, Marien, & Denecker

The second type of techniques is applicable when besides the finite domain, additional data is
available. This is often the case in practical model generation problems, such as the ones that are
typical in ASP. In a graph problem the data could be an encoding of the input graph; in the context of
planning, it could be a description of the initial and goal state, etc. Sometimes the data is explicitly
available, e.g., in the form of a database, sometimes it is implicit, e.g., as a set of ground facts in
the input theory. The second type of techniques aims at efficiently computing small groundings by
taking the data into account.
Observe that both types of techniques can be combined in a grounder. In this paper we mainly
focus on a technique of the second category. To explain the intuition underlying our method, consider
the following model generation problem.
Example 1. Let T1 the first-order logic theory over the vocabulary {Edge, Sub}, consisting of the
two sentences
uv (Sub(u, v)  Edge(u, v))

(1)

xyz (Sub(x, y)  Sub(x, z)  y = z),

(2)

T1 expresses that Sub is a subgraph of Edge with at most one outgoing edge in each vertex. Computing such a subgraph of a given graph G = hV, Ei can be cast as a model generation problem with
input theory T1 and data G. The data can be represented as a structure I for the subvocabulary
1 = {Edge} with domain V and EdgeI = E. A solution can be obtained by generating a model
of T1 that expands I with an interpretation of Sub.
Applying the naive grounding algorithm produces |V |2 instantiations of (1) and |V |3 instantiations of (2). By taking the data into account, atoms over Edge and = can be substituted by their
truth value in I . Simplifying the resulting grounding then eliminates |E| instantiations of (1) and
|V | instantiations of (2). Smart grounding algorithms interleave this substitution and simplification
with the grounding process in order to avoid creating unnecessary parts of the grounding.
Observe that substituting atoms over 1 and then simplifying still produces a grounding of size
O(|V |3 ). Indeed, the simplified grounding of (2) is the set of binary clauses Sub(i, j)  Sub(i, k)
such that i, j, k  V and i 6= j. This set has size |V |3  |V |.
Some grounders apply reasoning on the ground theory to reduce it even further. In the example,
the simplified grounding of (1) consists of the clauses Sub(i, j) such that (i, j) 6 E. Since these
are unit clauses, each of them is certainly true in every model of the ground theory. It follows
that each binary clauses Sub(i, j)  Sub(i, k) such that either Sub(i, j) or Sub(i, k) belongs
to the simplified grounding of (1) is certainly true in every model of the ground theory and thus
can be omitted from the simplified grounding of (2). The result is a grounding of size |E ./1=1 E|,
where ./1=1 denotes the natural join matching the first columns. For a sparse graph, |E ./1=1 E| is
much smaller than |V |3 . However, since reasoning on the ground theory does not avoid creating all
instantiations of a formula, it does not significantly speed up the grounding process.
One way to avoid a large grounding without relying on reasoning on the ground theory is by
adding redundant information to formulas. This method is frequently used in ASP. For example,
xyz(Edge(x, y)  Sub(x, y)  Edge(x, z)  Sub(x, z)  y = z)

(3)

is equivalent to (2) given (1), but its grounding (without reasoning on the ground theory) is equal
to the one obtained by the kind of reasoning on the ground theory illustrated above. This illustrates
how adding redundant information may sometimes dramatically reduce the size of the grounding.
Since current grounders are optimized to ground formulas like (3) without trying all instances,
grounding may also speed up a lot.
However, manually adding redundancy to formulas has its disadvantages: it leads to more complex and hence, less readable theories. Worse, it might introduce errors. It requires a good understanding of the used grounder, since it depends on the grounder what information is beneficial to
add and where. Also, a human developer could easily miss useful information.
224

fiGrounding FO and FO(ID) with Bounds

The above motivates a study of automated methods for deriving such redundant information and
of principled ways of adding it to formulas. We develop an algorithm that, given a model generation
problem with input theory T and input data I , derives such redundant information, in the form
of a pair of a symbolic upper and lower bound for each subformula of T . Each of these bounds
is a formula over the vocabulary of I . For instance, for Example 1, our algorithm will compute
Edge(x, y) as upper bound for Sub(x, y), meaning that if Edge(x, y) is not true, then Sub(x, y) is not
true either. We also show how to insert these bounds in the formulas of T . For example, inserting the
upperbound Edge(x, y) for Sub(x, y) and the upperbound Edge(x, z) for Sub(x, z) transforms (2)
into (3).
The rest of this paper is organized as follows. In the next section we recall some notions from
first-order logic (FO) and we introduce the notations used throughout the paper. In Section 3 we
formally define grounding and model generation with additional data. In Section 4 we introduce
upper- and lowerbounds for formulas. We present an any-time algorithm to compute them in the
context of FO input theories. We show how the bounds can be used to rewrite the input theory to
an equivalent theory that has a smaller grounding.
Although many search problems can be cast concisely and naturally as FO model generation
problems, some problems require richer logics than FO. One such logic is FO(ID), an extension
of FO with inductive definitions. Such definitions can be used to represent, e.g., the concept of
reachability in a graph. In Section 5 we extend our rewriting method to FO(ID).
In Section 6 we discuss how to implement our algorithm to compute bounds. As a case study,
we show for one particular grounding algorithm how it can be adapted to exploit bounds directly.
We also present experimental results that indicate the impact of our method on grounding size and
time. We end with related work and conclusions.
The current paper extends our previous work (Wittocx, Marien, & Denecker, 2008c). Besides
proofs for all main propositions and a more thorough experimental validation, also the following
parts were added:
 The theoretical result stating that our rewriting method certainly yields smaller groundings
(Proposition 23);
 The extension of the rewriting method to FO(ID) (Section 5);
 The section about implementation issues (Section 6).

2. Preliminaries
In this section, we introduce the conventions and notations used in this paper. We assume the reader
is familiar with FO.
2.1 First-Order Logic
A vocabulary  is a tuple hP , F , V i where P , F and V are respectively sets of predicate
symbols, function symbols and variables. We identify constants with zero-arity function symbols.
Abusing notation, we will often leave out V and simply write hP , F i to represent . A vocabulary
 is a subvocabulary of , denoted   , if P  P , F  F and V  V .
Throughout this paper variables are denoted by lowercase letters, predicate and function symbols
by uppercase letters. Each predicate and function symbol has an associated arity n  N. We often
denote a predicate symbol P by P/n and a function symbol F by F/n to indicate their arities.
Tuples and sets of variables are denoted by x, y, z. A term over  is inductively defined by
 A variable x   is a term;
 If F/n is a function symbol of  and t1 , . . . , tn are terms over , then F (t1 , . . . , tn ) is a term.
225

fiWittocx, Marien, & Denecker

Tuples of terms are denoted by t, t1 , t2 , . . . . A first-order logic formula over  is inductively defined
by
 If P/n is a predicate symbol and t1 , . . . , tn are terms, then P (t1 , . . . , tn ) is a formula.
 If t1 and t2 are two terms, then t1 = t2 is a formula.
 If  and  are formulas and x is a variable, then ,   ,   , x  and x  are formulas.
We use   ,    and t1 6= t2 as a shorthands for respectively   , (  )  (  ) and
(t1 = t2 ). An atom is a formula of the form P (t) or t1 = t2 . A literal is an atom or the negation
of an atom.
An occurrence of a formula  as subformula in a formula  is positive, respectively negative, if
it occurs in the scope of an even, respectively odd, number of negations.
For a formula , we often write [x] to indicate that x are its free variables. That is, if y  x,
then y occurs in , but not in the scope of a quantifier y or y in . For a variable x and a term t,
the formula [x/t] denotes the result of replacing all free occurrences of x in  by t. This notation
is extended to tuples of variables and terms of the same length. A sentence is a formula without
free variables. A theory is a finite set of sentences.
A -interpretation I consists of a domain D and
 a domain element xI  D for each variable x  V ;
 a relation P I  Dn for each predicate symbol P/n  P ;
 a function F I : Dn  D for each function symbol F/n  F .
A -structure is an interpretation of only the relation and function symbols of . The restriction
of a -interpretation I to a vocabulary    is denoted by I| . Vice versa, I is called an expansion
of I| to . For a variable x and domain element d, I[x/d] is the interpretation that assigns d to
x and corresponds to I on all other symbols. This notation is extended to tuples of variables and
domain elements of the same length. An interpretation I is called finite if its domain is finite.
The value tI of a term t in an interpretation I, and the satisfaction relation |= are defined as
usual (e.g., Enderton, 2001). I is called a model of a formula  if I |= . We denote by T1 |= T2
that every model of theory T1 is also a model of theory T2 .
A query is an expression of the form {x | }, where the free variables of  are among x. A tuple
d of domain elements is an answer to {x | } in a structure I if I[x/d] |= . The set of all answers
to {x | } in I is denoted by {x | }I .
2.2 Rewriting and Term Normal Form
In this paper we will use the following well-known equivalences to rewrite formulas to logically
equivalent formulas.
1. Moving quantifiers
xy 



xy 



yx 

(4)

yx 

(5)

x (  )



(x )  (x )

(6)

x (  )



(x )  (x )

(7)

x (  )



  (x )

if x does not occur free in 

(8)

x (  )



  (x )

if x does not occur free in 

(9)

226

fiGrounding FO and FO(ID) with Bounds

2. Moving negations
(  )



()  ()

(10)

(  )



()  ()

(11)

(x )



x ()

(12)

(x )



x ()

(13)

3. Flattening terms
P (t1 , . . . , ti , . . . , tn )  x (x = ti  P (t1 , . . . , ti1 , x, ti+1 , . . . , tn ))

(14)

where x does not occur in P (t1 , . . . , tn ).
To facilitate the presentation, we will sometimes require that formulas are in term normal form
(TNF). We say that a formula  is in TNF, if every atomic subformula of  is of the form P (x),
F (x) = y or x = y, and all negations occur directly in front of atoms. Using (10)(14), every
formula can be transformed in an equivalent formula in TNF. We say that a theory is in TNF if all
its sentences are.
2.3 SAT
A vocabulary  is propositional if F =  and every predicate symbol in P has arity zero. A
propositional theory (PC theory) is a theory over a propositional vocabulary. A propositional clause
is a disjunction of propositional literals. A PC theory is in conjunctive normal form (CNF) if all
its sentences are clauses. The Boolean satisfiability problem (SAT) is the NP-complete problem of
deciding for a PC theory whether it is satisfiable. The NP search problem corresponding to a SAT
problem is the problem of computing a witness of the decision problem in the form of a model of
the theory. SAT solvers typically operate by constructing such a model.
Contemporary SAT solvers exhibit impressive performance. As such, many NP problems can
be solved efficiently by translating them to SAT. For instance, this is done in the areas of model
generation (Claessen & Sorensson, 2003; McCune, 2003), planning (Kautz & Selman, 1996) and
relational data mining (Krogel et al., 2003). Most modern SAT solvers expect a CNF theory as
input, instead of a general PC theory. When the input is a satisfiable theory, they return a model
as a witness to their answer.

3. Model Generation and Grounding
Model generation is the problem of computing a model of a logic theory T , usually in the context
of a given finite domain, typically the Herbrand Universe. A model generator allows to decide the
satisfiability of the theory in the context of this fixed domain. This is useful, e.g., in the context
of lightweight verification (Jackson, 2006). Beyond determining satisfiability, there is a broad class
of problems of which the answers are naturally given by the models of a declarative domain theory.
For example, the model of a theory specifying a scheduling domain typically contains a (correct)
schedule. Thus, a model generator applied to this theory will solve the scheduling problem for
this domain.1 This idea of model generation as a declarative problem solving paradigm has been
pioneered in the area of ASP (Marek & Truszczynski, 1999; Niemela, 1999). In this area, answers
to a problem are given by the models of an ASP theory.
As mentioned in the introduction, many practical model generation problems contain additional
data besides the input theory and finite domain. This data can be implicit in the input theory. For
1. For a set of problems of this kind, see, e.g., the benchmarks of the ASP-competition (http://dtai.cs.kuleuven.
be/events/ASP-competition).

227

fiWittocx, Marien, & Denecker

example, ASP problems can be split into two parts: a non-ground theory and a list of ground facts.
The latter part essentially represents given data. In other contexts (Mitchell & Ternovska, 2005;
Torlak & Jackson, 2007; Wittocx et al., 2008d), the data is given as a (partial) structure interpreting
part of the vocabulary of the input theory. In this paper we assume without loss of generality that
the data is represented by a structure. In practice, it is often the case that some preprocessing,
e.g., materializing a view on a database, needs to be done before the data is in this format (see also
Section 5.3.2).
3.1 The Model Expansion Search Problem
Model generation with an input theory and input structure is called model expansion. Model expansion for a logic L, denoted MX(L), is defined as follows.
Definition 1. Let T be an L-theory over a vocabulary ,  a subvocabulary of  and I a finite
-structure. The model expansion search problem with input hT, I i is the problem of computing a
-structure M such that M |= T and M | = I .
The vocabulary  is called the input vocabulary of the problem, the vocabulary \ the expansion
vocabulary. I is called the input structure. We denote by M |=I T that M is a solution to the
model expansion search problem with input hT, I i. Similarly, for a formula  over  we denote by
M |=I  that M expands I to  and satisfies .
Observe that if  = , model expansion reduces to model checking, while if  = h, i, it reduces
to model generation for T with a given finite size. Also, if T is a theory over a vocabulary 
containing no function symbols of arity greater than zero, Herbrand model generation for T can be
simulated by model expansion. Indeed, let  = h, F i, and I the structure with the Herbrand
universe of T such that C I = C for every constant C  F .
We illustrate model expansion by two examples. In the examples in this paper, we often use
many-sorted FO, since this leads to more concise and readable sentences. In many-sorted FO, the
domain of an interpretation is partitioned in sorts (or types), each variable has an associated sort,
each n-ary predicate symbol has an n-tuple of associated sorts and each n-ary function symbol
an associated (n + 1)-tuple of sorts. If I is an interpretation and variable x has associated sort
s, then xI  sI , where sI denotes the set of domain elements of sort s. Similarly, if P/n has
associated sorts (s1 , . . . , sn ), then P I  sI1      sIn , if F/n has associated sorts (s1 , . . . , sn+1 ), then
F I : sI1      sIn  sIn+1 . We often denote P by P (s1 , . . . , sn ) and F by F (s1 , . . . , sn ) : sn+1 to
indicate their associated sorts.
Example 2 (Graph Colouring). The graph colouring problem is the problem of colouring a given
graph with a given set of colours such that adjacent vertices have different colours. To express this
problem in MX(FO), let V tx and Col be sorts and let  = h{Edge(V tx, V tx)}, i. The sort Col
denotes the given set of colours, the given graph is represented by V tx and Edge. Let  be the
vocabulary hP , {Colour(V tx) : Col}i and T the theory that consists of the sentence
v1 v2 (Edge(v1 , v2 )  Colour(v1 ) 6= Colour(v2 )).
Then model expansion with input theory T and input vocabulary  expresses the graph colouring
problem. Indeed, for any M |=I T , ColourM is a proper colouring of the graph represented by I .
Example 3 (SAT). To represent the SAT problem in MX(FO), let  be a vocabulary containing
the two sorts Atom and Clause, representing the atoms and the clause of the input CNF theory,
and the two predicates P osIn(Atom, Clause) and N egIn(Atom, Clause), to represent the positive,
respectively negative, occurrences of atoms in clauses. The theory given by
c a ((P osIn(a, c)  T rue(a))  (N egIn(a, c)  T rue(a)))

228

fiGrounding FO and FO(ID) with Bounds

over  = hP  {T rue(Atom)}, i expresses the SAT problem: for any M |=I T , the propositional
structure represented by T rueM is a model of the CNF theory represented by I . Indeed, the theory
forces that every clause contains at least one true literal.
As shown by Mitchell and Ternovska (2005), it follows from Fagins (1974) theorem that model
expansion for FO captures NP, in the following sense:
 For any fixed T and  the problem of deciding whether there exists a model of T expanding
an input structure I is in NP.
 Vice versa, for any NP decision problem X on the class of finite -structures there is a
vocabulary    and a first-order -theory T such that model expansion with input theory
T expresses X, i.e., I belongs to X iff there exists a -structure M such that M |=I T .
This result proves that any NP problem X can be expressed by an MX(FO) problem, and hence
shows the broad applicability of MX(FO) solvers to solve NP problems.
As illustrated by the examples above, it is the intention that the theory T is an intuitive representation of a problem X. Not all NP problems can be represented in a natural manner in MX(FO).
For instance, the problem of deciding whether a graph is connected can be expressed in MX(FO),
but this requires a non-trivial encoding of a fixpoint operator in FO. Model expansion for richer
logics than FO is better suited for such problems. In Section 5 we consider MX for FO(ID), an
extension of FO with inductive definitions.
3.2 Reducing MX(FO) to SAT
For the rest of this paper, let T be a theory over a vocabulary ,  a subvocabulary of  and I a
finite -structure with domain D.
Since for every FO theory T , deciding whether T has a model expanding I is in NP, this
problem can be reduced to a SAT problem Tprop in polynomial time. However, if we want to find
models of T expanding I by using a SAT solver, we need a method to translate models of Tprop
into models of T . Moreover, if we are interested in finding all models of T expanding I , a oneto-one correspondence between these models and the models of Tprop is needed. In this paper we
focus on reductions that preserve all models, which is the setting in the ASP paradigm (Marek &
Truszczynski, 1999; Niemela, 1999).
Let  be the vocabulary of Tprop . To have a one-to-one correspondence between the models of
T expanding I and the models of Tprop , it should be possible to represent -structures expanding
I by  -structures. The most natural way to accomplish this is by choosing  such that it contains
a symbol Pd for every P/n  P and d  Dn , and a symbol Fd,d0 for every F/n  F and
(d, d0 )  Dn+1 . A  -structure making Pd , respectively Fd,d0 true then corresponds to a  structure
M such that d  P M , respectively F M (d) = d0 . In this manner, every -structure expanding I
has a corresponding  -structure. Vice versa, every  -structure A satisfying the requirement that
for every function symbol F/n and d  Dn , there is exactly one d0  D such that Fd,d0 is true
in A, corresponds to a -structure with the same domains as I . That is, there is a one-to-one
correspondence between the  -structures satisfying for every function symbol F/n and d  Dn the
formula


! 
_
^
^

Fd,d0  
(15)
Fd,d0  Fd,d0 
d0 D

d01 D

d02 D\d01

1

2

and the -structures with domain D.
Denote by dom(I ) the vocabulary  extended with a new constant symbol d for every d  D.
We call these new constants domain constants. Abusing notation, we will denote both domain
elements and their corresponding domain constants by d. For a formula [x] and a tuple d of

229

fiWittocx, Marien, & Denecker

domain constants, we call [x/d] an instance of . For a -interpretation M expanding I and a
formula  containing domain constants, we denote by M |=  that the expansion of M to dom(I )
defined by interpreting every domain constant by its corresponding domain element, satisfies .
Definition 2. Two formulas 1 and 2 over dom(I ) are I -equivalent if M |=I 1 iff M |=I 2 ,
for every -interpretation M .
The following are some straightforward results about I -equivalence.
Lemma 3.
1. Two logically equivalent formulas are I -equivalent.
V
2. dD [x/d] is I -equivalent to x [x].
W
3. dD [x/d] is I -equivalent to x [x].
4. If 0 and  0 are I -equivalent to respectively  and , then 0 , 0   0 , 0   0 , x 0 and
x 0 are I -equivalent to respectively ,   ,   , x  and x .
5. If  is a subformula of  and is I -equivalent to  0 , then the result of replacing  by  0 in 
is I -equivalent to .
A formula is in ground normal form (GNF) if it contains no quantifiers and all its atomic
subformulas are of the form P (d1 , . . . , dn ), F (d1 , . . . , dn ) = d or d1 = d2 , where d1 , . . . , dn , d are
domain constants. A theory is in GNF if all its sentences are in GNF. A GNF theory is essentially
propositional: by replacing in a GNF theory T every atom P (d) by Pd , F (d) = d0 by Fd,d0 , di = dj
by > or  if, respectively, i = j or i 6= j, and adding the formula (15) for every function symbol F/n
and d  Dn , we obtain a propositional theory Tprop such that the models of T and Tprop correspond.
Also note the similarity between GNF and TNF theories.
Definition 4. A grounding for T with respect to I is a GNF theory Tg over dom(I ) such that T
and Tg are I -equivalent. Tg is called reduced if it does not contain symbols of .
3.2.1 Grounding Algorithms
For the rest of this section, we assume that T is a theory in TNF. As explained in Section 2.2, we can
make this assumption without loss of generality. Below we introduce, as a reference, the grounding
for T with respect to I obtained by the naive grounding algorithm mentioned in the introduction.
We call this grounding the full grounding and define it formally by induction.
Definition 5. The full grounding Grfull (, I ) of a TNF sentence  with respect to I is defined by



if  is a literal





Gr
(
)

Gr
(
)
if  is equal to 1  2
full
2
 full 1
Grfull () = Grfull (1 )  Grfull (2 ) if  is equal to 1  2
(16)

V


if  is equal to x [x]

dD Grfull ([x/d])


W
Gr
([x/d])
if  is equal to x [x]
full
dD
The full grounding for T with respect to I is the theory consisting of the full groundings of all
sentences in T with respect to I .
We denote the full grounding by Grfull (T, I ), or by Grfull (T ) if I is clear from the context. It
follows directly from Lemma 3 that Grfull (T, I ) is indeed a grounding for T with respect to I . The
size of the full grounding is exponential in the maximal nesting depth of quantifiers in sentences of
T , and polynomial in the domain size of I .

230

fiGrounding FO and FO(ID) with Bounds

An inductive definition like (16) can be evaluated in a top-down or bottom-up way. Both approaches are applied in current grounders. On the one hand, there are grounders that go top-down
through the syntax trees of the sentences in T . When a subformula  of the form x [x], respectively x [x] is reached, the grounding of [x/d] is constructed for every domain constant d,
and then  is replaced by the conjunction, respectively disjunction, of all these groundings. The
grounder of the dlv system (Perri et al., 2007) and the grounders gringo (Gebser et al., 2007) and
GidL (Wittocx, Marien, & Denecker, 2008b) take this approach.
Other grounders go bottom-up through the syntax trees. For each subformula [x] a table is
computed consisting of tuples d and corresponding groundings of [x/d]. These tables are computed
first for atomic formulas and subsequently for compound formulas. For example, let [x, y, z] be
the formula [x, y]  [y, z] and assume the tables for  and  have been computed. Then the
table for  is computed by taking the natural join of the tables for  and  on the value for y, and
constructing the grounding for [x/dx , y/dy , z/dz ] as the (possibly simplified) conjunction of the
groundings for [x/dx , y/dy ] and [y/dy , z/dz ]. Examples of grounders with a bottom-up approach
are lparse (Syrjanen, 2000; Syrjanen, 2009), kodkod (Torlak & Jackson, 2007) and mxg (Mitchell
et al., 2006).
To obtain a reduced grounding for T with respect to I one could first construct the full grounding
and then replace every subformula  over  dom(I ) in it by > if I |=  and by  otherwise. The
result can further be simplified by recursively replacing    by , >   by , etc. The resulting
grounding is the one computed by most current grounding algorithms and is often a lot smaller than
the full grounding. We denote it by Grred (T, I ), or by Grred (T ) if I is clear from the context.
Smart grounding algorithms do not use the approach outlined above, but try to avoid creating the
full grounding by substituting ground formulas over the input vocabulary  as soon as possible. For
example, a grounder with a top-down approach constructs the grounding of x [x], by grounding
all instances [x/d] one by one and then making the conjunction. During this process, all instances
[x/d] that are detected to be certainly true are omitted. As soon as an instance [x/d] is detected
to be certainly false,  is returned as grounding for x [x].
A grounder using the bottom-up approach can reduce the size of the tables it computes by not
storing tuples that have some default value, e.g., >, as corresponding grounding. In particular, if
[x] is a formula over , it only stores the tuples d such that I 6|= [x/d]. By reducing the size of
the tables in this way, the reduced grounding can be obtained much more efficiently.

4. Grounding with Bounds
In this section we present our method for reducing grounding size. As mentioned in the introduction,
it is based on computing bounds for subformulas of the input theory T . Each bound for a subformula
[x] is a formula over the input vocabulary . It describes a set of tuples d for which [x/d] is
certainly true (false) in every model of T expanding any I . The larger the set described by a
bound, the more precise the bound is. Observe that the fact that bounds are formulas over  means
that they can be evaluated using the given structure I .
In Section 4.1, we formally define bounds. Then we indicate how bounds can be inserted in T
to obtain a new theory T 0 . The reduced grounding of T 0 is often a lot smaller than the reduced
grounding of T . The more precise the inserted bounds are, the smaller the grounding of T 0 becomes.
However, we will see that T 0 is in general weaker than T and that additional axioms have to be
added to T 0 to obtain equivalence with T . These additional axioms need to be grounded as well
so that, if we are not careful, the total size of the grounded theory does not decrease at all. In
Section 4.3, we search for sufficient conditions on the bounds to guarantee a smaller grounding.
In Section 4.4, we show how to derive bounds. Our method works in two stages. First, bounds
for all subformulas of T are computed using an any-time algorithm. The longer the algorithm runs,
the more precise bounds are derived. Often, the bounds derived at this stage do not lead to smaller
groundings, for the reason explained in the previous paragraph. In the second stage, bounds that
231

fiWittocx, Marien, & Denecker

satisfy the conditions to guarantee smaller groundings are derived from the ones computed in the
first stage.
4.1 Bounds
We distinguish between two kinds of bounds.
Definition 6. A certainly true bound (ct-bound) over  with respect to T for a formula [x] is a
formula ct [y] over  such that y  x and T |= x (ct [y]  [x]). Vice versa, a certainly false
bound (cf-bound) over  with respect to T for [x] is a formula cf [z] over  such that z  x and
T |= x (cf [z]  [x]).
We do not mention  and T if they are clear from the context.
Intuitively, a ct-bound ct for [x] provides for every structure I a lower bound for the set of
tuples for which  is true in every model of T expanding I . Indeed, for every M |=I T we have
that {x | ct }I  {x | }M . Vice versa, a cf-bound cf provides a lower bound on the set of tuples
for which  is false: {x | cf }I  {x | }M for every M |=I T . Observe that the negation of
a ct-bound, respectively cf-bound, gives an upper bound on the set of tuples for which  is false,
respectively true, in at least one model of T expanding I .
Example 4 (Example 1 ctd.). Let 1 be the subformula Sub(x, y)  Sub(x, z) of T1 . Then
Edge(x, y)  Edge(x, z) is a cf-bound over 1 with respect to T1 for 1 . Indeed, one can derive from (1) that T1 entails
xyz ((Edge(x, y)  Edge(x, z))  1 ) .
Observe that > is a ct-bound for every sentence of T . Indeed, for every sentence  of T , T |= 
and therefore T |= >  . Also,  is a ct-bound as well as a cf-bound for every formula. We call
 the trivial bound. Intuitively, the trivial bound contains no information at all: {x | }I =  for
every I and x. According to the following definition, it is the least precise bound.
Definition 7. Let [y] and [z] be two (ct- or cf-) bounds for [x]. We say that [y] is more precise
than [z] if x ([z]  [y]) is valid.
If  is a more precise bound for [x] than ,  provides a larger lower bound because {x | }I 
{x | }I for every I .
Definition 8. A c-map C for T over  is a mapping from all subformulas  of T to tuples
(C ct (), C cf ()), where C ct () and C cf () are respectively a ct- and cf-bound for  over  with
respect to T .
The notion of precision pointwise extends to c-maps. That is, if C1 and C2 are two c-maps for T ,
then C1 is more precise than C2 iff for every subformula  of T , C1ct () is more precise than C2ct ()
and C1cf () is more precise than C2cf ().
Let M be a model of T and C a c-map for T over . From the definition of ct- and cf-bounds
it follows immediately that for every subformula [x] of T , both M |= x (C ct ()  ) and M |=
x (C cf ()  ) hold. We say that a structure satisfies C if it has precisely this property.
Definition 9. Let C be a c-map for T over . Then the theory C is defined by
C ={x (C ct ()  ) | [x] is a subformula of T }
 {x (C cf ()  ) | [x] is a subformula of T }.
A structure I satisfies C if I |= C.

232

fiGrounding FO and FO(ID) with Bounds

Clearly, if C is a c-map for T over  and M |=I T , then M |= C. We call two formulas [x] and
[x] C-equivalent if {x | }I = {x | }I for each structure I that satisfies C. Equivalently,  and 
are C-equivalent if C |= x (  ).
A c-map is inconsistent if some formula  is both certainly true and false for some tuple, according
to that c-map:
Definition 10. A c-map C for T over  is inconsistent if x (C ct ()  C cf ()) is valid for some
subformula [x] of T . A c-map C is I -inconsistent if I |= x (C ct ()  C cf ()) for some subformula
of T .
Proposition 11. If there exists an I -inconsistent c-map for T over , then M 6|=I T for every
M . If there exists an inconsistent c-map for T over , then M 6|=I T for every M and I .
Proof. Let C be an I -inconsistent c-map for T over  and [x] a subformula of T such that I |=
x (C ct ()  C cf ()). Then there exists a tuple of domain elements d such that I [x/d] |= C ct ()
and I [x/d] |= C cf (). Assume towards a contradiction that M |=I T . Then M |= C, and hence
M [x/d] |= C ct ()   and M [x/d] |= C cf ()  . Since M | = I , it follows that M [x/d] |=  and
M [x/d] |= . This is a contradiction.
To prove the second statement, let C be an inconsistent c-map for T over . Then C is a also an
I -inconsistent c-map for every -structure I . As such, for any I there is no model of T expanding
I .
4.2 C-Transformation
For the rest of this section, fix a c-map C for T over . We now show how to insert the bounds of C
into the sentences of T . This insertion is based on the following lemma.
Lemma 12. Let [x] be a subformula of T . Then  is C-equivalent to   C ct () and to   C cf ().
Proof. We have to prove that C |= x (  (  C ct ())) and C |= x (  (  C cf ())). The
former immediately follows from the fact that C |= x (C ct ()  ), the latter from the fact that
C |= x (C cf ()  ) .
As a corollary of lemma 12 we have the following lemma.
Lemma 13. Let  be a sentence of T and  a subformula of . If  0 is the result of replacing
the subformula  in  by   C ct (), by   C cf () or by (  C cf ())  C ct (), then M |=  iff
M |=  0 for every M that satisfies C.
Observe that if C ct () = C cf () = , then both C ct () and C cf () are logically equivalent
to . Hence, in this case the sentence  0 in Lemma 13 is essentially the sentence . Intuitively,
adding trivial bounds to a sentence  does not change the sentence at all.
The bounds assigned by C can be inserted in T by applying the transformation of Lemma 13
to all subformulas of T . The result is called a c-transformation of T , and is formally defined as
follows.
Definition 14 (c-transformation). A c-transformation of a subformula  of T with respect to C,
denoted Chi, is the formula (0  C cf ())  C ct () where 0 is defined by


if  is an atom





Chi
if  is equal to 



Chi  Chi if  is equal to   
0 :=

Chi  Chi if  is equal to   




x Chi
if  is equal to x 



x Chi
if  is equal to x 
233

fiWittocx, Marien, & Denecker

A c-transformation ChT i of T with respect to C consists of a c-transformation with respect to C of
every sentence of T .
From Lemma 13, we derive the following.
Lemma 15. T and ChT i are C-equivalent.
In general T and ChT i are not logically equivalent. ChT i may have models that do not satisfy C,
and therefore cannot be models of T . For example, let C be the c-map that assigns (>, ) to every
sentence and (, ) to every other subformula of T . Then all sentences in ChT i are of the form
  > and hence ChT i simplifies to >, which is in general not equivalent to T . To obtain from ChT i
a theory that is equivalent to T , we must add C.
Theorem 16. If C is a c-map for T over  and C the theory defined in Definition 9, then ChT i  C
is equivalent to T .
Proof. Let M be a model of T . Then M |= C, and because of Lemma 13, M |= ChT i  C. On the
other hand, if M |= ChT i  C, then by Lemma 13, M |= T .
Corollary 17. If C is a c-map for T over , then T and ChT i  C are I -equivalent for any structure I .
4.3 Atom-Based and Atom-Equal C-Maps
Corollary 17 implies that we can compute a grounding for T with respect to I by first computing
a c-map C for T over  and then grounding ChT i  C. This approach is beneficial if the reduced
grounding of ChT i  C is smaller than the reduced grounding of T , and can be constructed at least
as fast. In general these conditions are not satisfied. The more precise c-map C is, the smaller the
reduced grounding of ChT i becomes, but the larger the reduced grounding of C is:
Proposition 18. If C1 is more precise than C2 , then Grred (C1 hT i) is smaller than Grred (C2 hT i).
Moreover, every subformula that occurs in Grred (C1 hT i) also occurs in Grred (C2 hT i).
Proof. (Sketch) Let [x] be a subformula of T and d a tuple of domain elements. It suffices to
show that if C2 hi[x/d] is replaced by >, respectively , when grounding, then this is also the case
for C1 hi[x/d]. This can be proven by induction. For the base case, assume  is an atom. Then
C2 hi[x/d] is the formula ((  C2cf ())  C2ct ())[x/d]. If this formula is replaced by > or  when
grounding, there are three possibilities:  is a formula over , I [x/d] |= C2ct () or I [x/d] |= C2cf ().
Since C1 is more precise than C2 , I [x/d] |= C2ct () implies I [x/d] |= C1ct () and I [x/d] |= C2cf ()
implies I [x/d] |= C1cf (). We conclude that if C2 hi[x/d] is replaced by > or  when grounding,
then this is also the case for C1 hi[x/d]. The inductive case is similar.
Proposition 19. If C1 is more precise than C2 , then Grred (C1 ) is larger than Grred (C2 ).
Proof. (Sketch) Every sentence in C1 is of the form x (C1ct ()  ) or x (C1cf ()  ). The
number of instances of C1ct ()   in the reduced grounding of C1 is equal to the number of d such
that I [x/d] |= C1ct (). Similarly for C1cf ()  . Since C2 is less precise than C1 , the number
of instances in Grred (C2 ) of the corresponding sentences x (C2ct ()  ) and x (C2cf ()  ) is
smaller.
A c-map that is useful to reduce grounding size should therefore not be too precise, in order to
avoid a large theory Grred (C), but still be precise enough to decrease the size of Grred (ChT i). In
this section, we present sufficient conditions to ensure these properties. We first define a class of
c-maps that avoid a blow-up of Grred (C) by ensuring C can be replaced by an equivalent, smaller
and easy-to-find theory C A . As such, Grred (C) can be replaced by the smaller theory Grred (C A ). In
the class we present, C A is a subset of C, namely the set of sentences in C that stem from the atomic
subformulas of T :
234

fiGrounding FO and FO(ID) with Bounds

Definition 20. Define the theory C A by
C A ={x (C ct ()  ) | [x] is an atomic subformula of T }
 {x (C cf ()  ) | [x] is an atomic subformula of T }.
We call C atom-based if C A |= C.
Example 5 (Example 1 ctd.). Let C2 be the c-map that assigns (, (Edge(x, y)  Edge(x, z)))
to Sub(x, y)  Sub(x, z) and (, ) to every other subformula. C2 is not atom-based, since (C 2 )A is
equivalent to >, while C 2 contains the sentence
xyz ((Edge(x, y)  Edge(x, z))  (Sub(x, y)  Sub(x, z))).

(17)

Let C3 be the c-map that assigns (, Edge(x, y)) to Sub(x, y), (, Edge(x, z)) to Sub(x, z) and
corresponds to C2 on all other subformulas of T1 . C3 is atom-based. Indeed, (C 3 )A consists of the
(equivalent) sentences
xy (Edge(x, y)  Sub(x, y))

(18)

xz (Edge(x, z)  Sub(x, z))

(19)

and C 3 consists of the sentences (17), (18) and (19). Both (18) and (19) imply (17), and therefore,
(C 3 )A |= C 3 .
Clearly, a c-map assigning (, ) to every non-atomic subformula of T is an example of an atombased c-map. As such, any c-map can be transformed into an atom-based one by replacing every
bound assigned to a non-atomic subformula by . In the next section, we show how to compute
more interesting atom-based c-maps.
Observe that Grred (C A ) contains only unit clauses. Combining the definition of atom-based
c-map and Theorem 16 immediately gives the following result.
Proposition 21. Let C be an atom-based c-map for T over . Then T and ChT iC A are equivalent,
and hence I -equivalent for every -structure I .
To obtain small groundings using bounds, it is important that the information in the bounds is
exploited wherever possible. In particular, if a ct- or cf-bound  is assigned to an atom P (x), then a
similar bound should be assigned to every other atom of the form P (y). We call a c-map atom-equal
if it has exactly this property for all atomic subformulas of T . That is, C is atom-equal if it assigns
essentially the same bounds to atomic subformulas over the same predicate or function symbol:
Definition 22. A c-map C for a TNF theory T over  is atom-equal if for every predicate symbol
cf
P/n there exist formulas ct
P [x1 , . . . , xn ] and P [x1 , . . . , xn ] such that for every atom P (y1 , . . . , yn )
ct
cf
that occurs in T , C (P (y1 , . . . , yn )) is equal to ct
P [x1 /y1 , . . . , xn /yn ] and C (P (y1 , . . . , yn )) is equal
cf
to P [x1 /y1 , . . . , xn /yn ], and similarly for function symbols.
Note that if no predicate or function symbol occurs more than once in a theory T , then every
c-map for T is atom-equal.
Example 6 (Example 1 ctd.). Let T2 be the theory obtained by adding the sentence w Sub(w, w)
to T1 . The only predicate that occurs more than once in T2 is the predicate Sub. Let C4 be a c-map
for T2 that assigns the following bounds to the atomic subformulas of T2 over Sub: (, Edge(u, v))
to Sub(u, v), (, Edge(x, y)) to Sub(x, y), (, Edge(x, z)) to Sub(x, z) and (, Edge(w, w)) to
cf
Sub(w, w). Then C4 is atom-equal. Indeed, if we take ct
Sub =  and Sub = Edge(x1 , x2 ), then
the conditions of Definition 22 are satisfied for predicate Sub.

235

fiWittocx, Marien, & Denecker

For an atom-equal c-map C, C A in general contains many equivalent sentences. For example,
for the c-map C4 as in Example 6, (C 4 )A contains amongst others, the equivalent sentences (18)
and (19). It also contains w Edge(w, w)  Sub(w, w), which is implied by (18). As a result,
if C is an atom-equal c-map, grounding C A in a naive way yields a grounding that contains several
formulas more than once. In the following proposition, we assume this redundancy is removed. In
other words, we assume a grounding algorithm for C A that never adds the same GNF formula more
than once to the grounding. This can be accomplished by grounding instead of C A the sentences
cfb
ctb
cfb
x (ctb
P  P (x)) and x (P  P (x)) for every predicate symbol P , where P and P are as
in Definition 22, and similarly for function symbols.
Proposition 23. Let C be an atom-based, atom-equal c-map for a TNF theory T . If T has a model
expanding I , then Grred (ChT i  C A ) is at most as large as Grred (T ).
In the proof, we denote the size of a theory Tg by |Tg |.
Proof. The outline of this proof is as follows. First, we show that every subformula that occurs
in Grred (ChT i), occurs in Grred (T ). Then, we prove that no atom occurring in Grred (C A ) occurs
in Grred (ChT i). Next, we show that every atom occurring in Grred (C A ) occurs at least once in
Grred (T ). Since we assumed Grred (C A ) does not contain any formula more than once, it follows that
|Grred (ChT i)|  |Grred (T )|  |Grred (C A )|, which concludes the proof.
We can directly apply Proposition 18 to show that every subformula of Grred (ChT i) occurs in
Grred (T ): if C 0 is the trivial c-map, then Grred (T ) is equal to Grred (C 0 hT i), and clearly C is more
precise than C 0 .
We now show that none of the atoms occurring in Grred (C A ) occur in Grred (ChT i). Let P (d)
be an atom occurring in Grred (ChT i). Then there is an atomic subformula P (x) of T such that
d 6 {x | C ct (P (x))}I and d 6 {x | C cf (P (x))}I . Because C is atom-equal, it follows that for any
subformula P (y) occurring in T , neither d  {y | C ct (P (y))}I nor d  {y | C cf (P (y))}I . Therefore
P (d) does not occur in Grred (C A ).
It remains to show that every atom that occurs in Grred (C A ) also occurs in Grred (T ). Let M be a
model of Grred (T ). Such a model exists because we assumed that T has a model expanding I . Let
P (d) be an atom that does not occur in Grred (T ). If P is a predicate of the input vocabulary, then
P (d) does not occur in Grred (C A ) either. If on the other hand, P is in the expansion vocabulary, then
the structure M 0 obtained from M by swapping the truth value of P (d) is also a model of Grred (T ).
Since Grred (ChT i  C A ) is I -equivalent to Grred (T ) and P 6 , it follows that M |= Grred (C A ) and
M 0 |= Grred (C A ). Because Grred (C A ) only contains unit clauses, we conclude that P (d) does not
occur in Grred (C A ).
We now have the following algorithm to create a small grounding for T with respect to I : first
compute an atom-based, atom-equal c-map C for T over  (We will present an algorithm for this in
Section 4.4). If C is I -inconsistent, output  and stop. Else, output Grred (ChT i  C A ).
It follows from Propositions 11 and 21 that the result of this algorithm is indeed a grounding for
T with respect to I . Observe that the first step of this algorithm is independent of I . If one has
to solve several model expansion problems with a fixed input theory T and input vocabulary , but
varying I , it suffices to compute C only once.
To perform the last step of the algorithm, one could apply any off-the-shelf grounder on input
ChT i  C A .
4.4 Computing Bounds
We now present an algorithm to compute a (non-trivial) c-map C. It is based on our work on
approximate reasoning for FO (Wittocx, Marien, & Denecker, 2008a). In general the resulting cmap is neither atom-based nor atom-equal, but an atom-based, atom-equal c-map can be derived
from it.
236

fiGrounding FO and FO(ID) with Bounds

4.4.1 Refining C-Maps
Constructing a non-trivial c-map can be done by starting from the least precise c-map, i.e., the one
that assigns (, ) to every subformula of T , and then gradually refining it. Each refinement step
consists of three operations:
1. Choose a subformula  of T .
2. Compute from the current c-map C a new ct-bound rct or cf-bound rcf for . Below, we
elaborate on this step: we present six different ways to obtain new ct- or cf-bounds, called
refinement bounds, from T and C. If the sentences of T are represented by their syntax trees,
each node corresponds to a subformula of T . Bottom-up refinement bounds are bounds for a
node computed by considering the bounds assigned by C to its children. Vice versa, top-down
refinement bounds are computed by looking at the parents and siblings of a node. Axiom
refinement bounds are bounds for the roots, i.e., for the sentences of T , while input, copy and
functional refinement bounds are in practice mainly bounds for atomic subformulas of T .
3. Substitute C ct () by C ct ()  rct , respectively C cf () by C cf ()  rcf .
According to the following lemma, a refinement step yields a new bound for  that is more precise
than the one assigned by C.
Lemma 24. If  and  are two ct-bounds for  with respect to T , then    is also a ct-bound for
. Moreover,    is more precise than  and more precise than . The same holds for cf-bounds.
Proof. Let  and  be two ct-bounds for [x]. By definition, T |= x (  ) and T |= x (  ).
Therefore T |= x ((  )  ), which proves that    is a ct-bound for . Since |=   (  )
and |=   (  ),    is a more precise bound than  and . The proof for cf-bounds is
similar.
We conclude that repeatedly applying refinement steps leads to a more and more precise c-map.
The resulting algorithm is an any-time algorithm. In Section 6 we will discuss a stop criterion for
the algorithm. We will also give examples where it can reach a fixpoint, and examples where it
cannot.
We now present the different ways to obtain refinement bounds.
Input Refinement Let [x] be a formula over the input vocabulary . Since T |= x ([x]  [x])
and T |= x ([x]  [x]), it is clear that [x] is a ct-bound and [x] a cf-bound for [x]. We
call these input refinement ct- and cf-bounds.
Axiom Refinement If  is a sentence of T , then > is an axiom refinement ct-bound for . This
refinement bound states that a sentence of T is true in every model of T .
Bottom-Up Refinement For a compound subformula , depending on its structure, Table 1
gives the bottom-up refinement ct-bound rct and cf-bound rcf for  with respect to C. It is rather
straightforward to obtain these formulas. For instance, the formula in the bottom-right of the table
indicates that if  is the formula   , then  is certainly false for those tuples for which both 
and  are certainly false. Or, more formally, if both T |= C cf ()   and T |= C cf ()  , then
T |= C cf ()  C cf ()  (  ).
Top-Down Refinement In the case of top-down refinements, the bounds of a formula  are used
to construct refinement bounds for one of its direct subformulas  (i.e.,  is one of s children
in the syntax tree). The top-down refinement ct-bounds rct and cf-bounds rcf for  are given in
Table 2. In this table, the tuple y denotes the free variables of  that do not occur in  and x0
denotes a new variable. We illustrate some of these refinement bounds. For further explanation why

237

fiWittocx, Marien, & Denecker




rct

rcf

cf

ct

C ()

C ()

ct

x 

x C ()

x C cf ()

x 

x C ct ()

x C cf ()



C ct ()  C ct ()

C cf ()  C cf ()

ct



ct

C cf ()  C cf ()

C ()  C ()

Table 1: Bottom-up refinement bounds



rct


x 

C cf ()
C ct ()

x 

C ct ()  x0 (x 6= x0  C cf ()[x/x0 ])

   or   

y C ct ()

   or   

y (C ct ()  C cf ())



rcf



C ct ()

x 

C ()  x (x 6= x0  C ct ()[x/x0 ])

x 

C cf ()

   or   

y (C cf ()  C ct ())

   or   

y C cf ()

0

cf

Table 2: Top-down refinement bounds

238

fiGrounding FO and FO(ID) with Bounds

these bounds are in a certain sense the most precise ones that can be obtained, we refer to our work
on approximate reasoning (Wittocx et al., 2008a).
Let  be the formula x P (x, y). Recall that intuitively, the ct-bound C ct () indicates for which
domain elements d, x P (x, d) is certainly true. For such a d and an arbitrary d0  D, P (d0 , d)
must be true. Hence, C ct () is a ct-bound for . Indeed, since x does not occur free in C ct (),
T |= xy (C ct ()  P (x, y)) follows from T |= y (C ct ()  x P (x, y)).
Now let  be the formula P (x)  Q(x, y). If we know that P (d1 )  Q(d1 , d2 ) is certainly false,
but Q(d1 , d2 ) is certainly true, then P (d1 ) must be certainly false. Hence, y C cf ()  C ct () is a
cf-bound for P (x).
Let  be the formula x P (x, y) and assume that x P (x, dy ) is certainly true, but for all d0x ,
except dx , P (d0x , dy ) is certainly false. Then we can conclude that P (dx , dy ) must be true. This is
precisely what is expressed by the formula C ct ()  x0 (x 6= x0  C cf ()[x/x0 ]).
Functional Refinement If [x, y] is the formula F (x) = y, functional refinement bounds for 
take into account that F is a function. The functional refinement ct-bound rct and cf-bound rcf
are given by:
rct := y 0 (y 0 6= y  C cf ()[y/y 0 ])
rcf := y 0 (C ct ()[y/y 0 ]  y 6= y 0 )
where y 0 is a new variable. Informally, the first of these formulas indicates that F (x) is certainly
equal to y if for every y 0 6= y, F (x) is certainly not equal to y 0 . The second one says that F (x) is
certainly not equal to y if F (x) is certainly equal to y 0 for some y 0 6= y.
Copy Refinement Let [x1 , . . . , xn ] and [y1 , . . . , ym ] be two formulas such that [x1 /z, . . . , xn /z]
and [y1 /z, . . . , ym /z] are the same, modulo a renaming of their non-free variables. That is,  and
 have exactly the same syntax tree, but their variables may differ. Denote by E(, ) the set of all
equalities xi = yj such that for some occurrence
V of xi in , yj occurs in the corresponding position
ct
in . Then the formula y1 . . . yV
(C
()

E(, )) is a copy refinement ct-bound for  and
m
the formula y1 . . . ym (C cf ()  E(, )) is a copy refinement cf-bound for . We also say that
these are the copy-refinement bounds from  to .
Example 7. Let  be the formula P (x1 , x1 )  s Q(x2 , s) and  the formula P (y1 , y2 )  t Q(y2 , t).
Because [x1 /z, x2 /z] is equal to [y1 /z, y2 /z] modulo the renaming of s by t, these formulas satisfy
the requirement for copy refinement. The set E(, ) is given by {x1 = y1 , x1 = y2 , x2 = y2 } and
hence,
y1 y2 (C ct ()  x1 = y1  x1 = y2  x2 = y2 )
is a copy refinement ct-bound for . Observe that if C ct () does not contain bounded occurrences
of x1 or x2 , this formula is equivalent to the simpler formula C ct ()[y1 /x1 , y2 /x1 ]  x1 = x2 .
One-Step Refinements We call rct (rcf ) a refinement ct-bound (cf-bound) for  with respect to
C if it is an input, axiom, bottom-up, top-down, functional or copy refinement ct-bound (cf-bound)
for  with respect to C. Lemma 25 states that a refinement ct-bound (cf-bound) is indeed a ct-bound
(cf-bound).
Lemma 25. If rct is a refinement ct-bound for  with respect to C, then it is a ct-bound for .
Similarly for cf-bounds.
Proof. The proof consists of a simple analysis of all cases. We proved some of the cases when we
introduced input, bottom-up and top-down refinement. The proof of the other cases is similar.
Definition 26. Let C be a c-map for T over ,  a subformula of T , rct a refinement ct-bound and
rcf a refinement cf-bound for  with respect to C. An assignment C r that corresponds to C, except
that it assigns C r () = (C ct ()  rct , C cf ()) or C r () = (C ct (), C cf ()  rcf ) is called a one-step
refinement of C.
239

fiWittocx, Marien, & Denecker

From Lemma 24 and 25 we obtain the following result.
Proposition 27. Every one-step refinement of a c-map for T over  is a c-map for T over .
As already mentioned at the beginning of this section, one can compute a c-map for T over  by
first assigning (, ) to every subformula of T and then repeatedly applying one-step refinements.
We call this nondeterministic any-time algorithm the refinement algorithm.
Example 8 (Example 1 ctd.). Figure 1 shows a possible run of the refinement algorithm for input
T and . Here, the sentences of T1 are represented by their syntax trees. The numbers indicate at
which step the bounds are refined. The trivial bounds are not shown.
In step (1), ct-bound  for the first sentence is replaced by   > using axiom refinement. Of
course, this new bound can be simplified to >. For all following steps, the figure shows simplified
bounds. In step (2) and (3) the bounds of subformula Edge(u, v) are refined by input refinement.
Then, top-down refinement is used to set the ct-bound of Sub(u, v)  Edge(u, v) to >. Next, by
top-down refinement, Edge(u, v) becomes the ct-bound for Sub(u, v) and then the cf-bound for
Sub(u, v).
In a similar way, the cf-bound y 6= z is derived for subformula Sub(x, y)  Sub(x, z) (step (7)
 (12)). Then, by copy refinement, the cf-bounds for Sub(x, y) becomes uv (Edge(u, v)  u =
x  v = y), wich simplifies to Edge(x, y). Likewise, after simplification, Edge(x, z) is the copy
refinement cf-bound for Sub(x, z). Finally, two steps of bottom-up refinement are used to set the
ct-bound of (Sub(x, y)  Sub(x, z)) to y 6= z  Edge(x, y)  Edge(x, z).
At this step, a fixpoint is reached: every one-step refinement that can be performed yields a
bound that is logically equivalent to the one it tries to refine.
Example 9. Consider a simplified planning problem, where actions should be scheduled such that
if an action ap is a precondition of an action a0 , then ap is performed at an earlier time point than
a0 . This problem is described by the theory T3 , consisting of the sentence
a0 ap t0 P rec(ap , a0 )  Do(a0 , t0 )  (tp tp < t0  Do(ap , tp )).
From this sentence, it follows that if a chain of i actions must be executed before a0 can be executed,
then a0 cannot be executed before the ith timepoint. Therefore, for any i > 0, the following formula
is a cf-bound for Do(a0 , t0 ) over 2 = {P rec, <}:
a1    ai (P rec(a1 , a0 )  . . .  P rec(ai , ai1 ))  t1    ti (t1 < t0  . . .  ti < ti1 ).
Denote this formula by i . For any n > 0 and a sufficient number of steps, the refinement algorithm
can derive that n := 1  . . .  n is a cf-bound for Do(a0 , t0 ). Clearly, for n1 6= n2 , n1 is not
logically equivalent to n2 . This indicates that the refinement algorithm will not reach a fixpoint
for input T3 and 2 .
As shown by the examples, there are several issues concerning the practical implementation of
the refinement algorithm.
1. Due to the non-deterministic nature of the algorithm, a heuristic is needed to choose which
bounds to refine and which kind of refinement to apply. A reasonable choice is to first apply
all possible axiom and input refinements. Then, top-down refinement for formula  is applied
only if a bound for its parent or one of its siblings in the syntax tree has recently been refined.
Similarly, bottom-up refinement is applied if a bound for one of s children has been refined.
Such a strategy was used in Example 1.
2. The bounds should be simplified at regular time points, i.e., they should be replaced by equivalent but smaller formulas. If bounds are not simplified, they can only grow in size, rapidly
leading to formulas of unwieldy size. A simplification algorithm is discussed in Section 6.
240

fiGrounding FO and FO(ID) with Bounds

u, v





(5) ct: Edge(u, v)

(1) ct:   >

(4) ct: >

Edge(u, v)

(2) ct: Edge(u, v)
(3) cf: Edge(u, v)

(6) cf: Edge(u, v)

Sub(u, v)

x, y, z



(11) ct: y 6= z



(7) ct: >

(10) ct: >

y=z

(16) ct: y 6= z
Edge(x, y)  Edge(x, z)

(8) ct: y = z
(9) cf: y 6= z

(12) cf: y 6= z



(15) cf: y 6= z
Edge(x, y)  Edge(x, z)
(13) cf: Edge(x, y)

Sub(x, y)

Sub(x, z)

(14) cf: Edge(x, z)

Figure 1: Refining a c-map

241

fiWittocx, Marien, & Denecker

3. To be able to detect that a fixpoint has been reached, one needs to find out that two bounds
are equivalent. In general this is undecidable. To detect a fixpoint in at least some cases, one
could use an FO theorem prover (and restrict its running time).
In case a fixpoint cannot be reached or detected, another stop criterion is needed. For example,
one could restrict the number of one-step refinements, or the total time the refinement algorithm can use. Another stop criterion, and a simple fixpoint check are discussed in Section 6.
4.4.2 Extracting an Atom-Based and Atom-Equal C-Map
The c-maps obtained by the refinement algorithm are in general neither atom-based nor atom-equal.
To derive from an arbitrary c-map C an atom-equal c-map that is at least as precise as C, we first
collect for each predicate P all bounds that are assigned to occurrences of P in the theory. Then the
disjunction of these bounds is assigned as new bound to each occurrence of P . Because all bounds
assigned to atoms over P are then essentially the same, we have an atom-equal c-map. We now
present this method more formally:
Definition 28. Let C be a c-map for a TNF theory T and P/n a predicate. Let P (x11 , . . . , x1n ),
. . . , P (xm1 , . . . , xmn ) be all occurrences of P in T and let y1 , . . . , yn be n new variables. Denote by
ict , respectively icf , the formulas
x0i1    x0in (C ct (P (xi1 , . . . , xin ))[xi1 /x0i1 , . . . , xin /x0in ]  y1 = x0i1  . . .  yn = x0in )
and
x0i1    x0in (C cf (P (xi1 , . . . , xin ))[xi1 /x0i1 , . . . , xin /x0in ]  y1 = x0i1  . . .  yn = x0in ),
where the variables x0ij are new variables. The ct-copy closure of P (xk1 , . . . , xkn ) with respect to
W
C is the disjunction 1im ict [y1 /xk1 , . . . , yn /xkn ]. The cf-copy closure of P (xk1 , . . . , xkn ) is the
W
formula 1in icf [y1 /xk1 , . . . , yn /xkn ]. The copy-closure for atoms of the form F (x) = y is defined
similarly.
We denote the ct-copy closure of an atom  by copyCct (), and its cf-copy closure by copyCcf ().
Definition 29. The copy-closure of C is the c-map that assigns (copyCct (), copyCcf ()) to every
atomic subformula  of T , and corresponds to C on all other subformulas.
Example 10. Let T4 be the theory consisting of the sentences x (P (x)  R(x)) and y (Q(y) 
R(y)) and let C5 be a c-map over 3 = {P, R} that assigns (P (x), ) to R(x) and (Q(y), ) to R(y).
The copy-closure of C5 assigns
((x0 (P (x0 )  x0 = x))  (x0 (Q(x0 )  x0 = x)), (x0 (  x0 = x))  (x0 (  x0 = x)))
to R(x). These bounds simplify to (P (x)  Q(x), ). Likewise, the copy-closure of C5 assigns to
R(y) bounds that simplify to (P (y)  Q(y), ).
Proposition 30. The copy-closure of a c-map is an atom-equal c-map.
Proof. This follows immediately from the definition of atom-equal c-map
W since for everyWpredicate
symbol P (or function symbol F ), the same bounds, namely the formulas 1in ict and 1in icf
mentioned in definition 28, are assigned to every atom over P (respectively F ).
Recall that a c-map C is atom-based if C is implied by C A , i.e., by all sentences in C that stem from
bounds for atomic subformulas of T . A method to derive an atom-based c-map from an arbitrary
c-map is based on the following observation. Let C be a c-map for T over  and let [x] be the
subformula    of T . If C ct () is the formula C ct ()  C ct (), i.e., it is the bottom-up refinement
ct-bound for  with respect to C, then T |= x (C ct ()  ) is implied by T |= x (C ct ()  )
and T |= x (C ct ()  ). It is easy to check that the same property holds for all other bottom-up
refinement bounds:
242

fiGrounding FO and FO(ID) with Bounds

Lemma 31. Let C be a c-map for T over  and [x] a subformula of T , and let rct and rcf be the
bottom-up refinement bounds for  with respect to C. If S is the set of direct subformulas of , i.e.,
its children in the syntax tree, and T 0 is the theory given by
T 0 := {y C ct ()   | [y]  S}  {y C cf ()   | [y]  S},
then T 0 |= x rct   and T 0 |= x rcf  .
Definition 32. A c-map C for T is called a bottom-up c-map if for every non-atomic subformula
 of T , C ct () is the bottom-up ct-refinement bound for  with respect to C, and C cf () is the
bottom-up cf-refinement bound for  with respect to C.
The next proposition follows directly from Lemma 31.
Proposition 33. A bottom-up c-map C is atom-based.
Observe that a bottom-up c-map C for T is completely determined by the bounds it assigns to
the atomic subformulas of T . Hence, given a c-map, one can derive a bottom-up c-map from it by
retaining the bounds for the atomic subformulas and then computing the corresponding bottom-up
c-map. We conclude that we can derive an atom-based, atom-equal c-map from an arbitrary c-map
by deriving an atom-based c-map from its copy-closure.
Example 11 (Example 1 ctd.). Let C6 be the fixpoint shown in Figure 1. This c-map is atom-equal
(and equivalent to its copy-closure). The bottom-up c-map derived from C6 is shown in Figure 2.
Observe that this c-map is less precise than C6 . For instance, the cf-bound assigned by C6 to the
conjunction Sub(x, y)  Sub(x, z) is a disjunction of two bounds, namely bound y 6= z, obtained by
top-down refinement, and bound Edge(x, y)  Edge(x, z), obtained by bottom-up refinement. In
the c-map of Figure 2, only the latter bound is present.
For the c-map in Figure 2, the c-transformation of Sub(x, y)  Sub(x, z) is given by
((Sub(x, y)  Edge(x, y))  (Sub(x, z)  Edge(x, z)))  (Edge(x, y)  Edge(x, z)).
This formula contains repeated constraints Edge(x, y) and Edge(x, z) on the variables x, y and z.
In general bottom-up c-maps produce many such repetitions. These could easily be eliminated to
speed up the grounding process, but it depends on the used grounding algorithm which ones are
best deleted.

5. Inductive Definitions
Although all NP problems can be cast as MX(FO) problems, modelling such problems using pure
FO can be extremely complex. In practice, modelling is often enhanced considerably by using
extensions of FO with constructs such as inductive definitions, subsorts, aggregates, partial functions
and arithmetic. For this enriched language we have implemented the model generator idp (Wittocx
et al., 2008b; Wittocx & Marien, 2008).2
In this paper we focus on grounding of the extension of FO with inductive definitions. It is
well-known that in arbitrary domains, inductively definable concepts such as reachability are not
FO-expressible. In finite domains however, they can be encoded (e.g., by encoding the fixpoint
construction), but the process is tedious and leads to large theories. In this section we will extend
the refinement algorithm to FO(ID) (Denecker, 2000; Denecker & Ternovska, 2008). This language
extends FO with a construct for representing some of the most common types of inductive definitions: monotone induction and non-monotone induction such as induction over a well-founded order
and iterated inductive definitions. Such definitions have many applications in real-life computational problems, e.g., in planning problems or problems involving reachability or dynamic systems
(Denecker & Ternovska, 2008, 2007). At the same time, FO(ID) is also an integration of FO and
logic programming.
2. idp can be downloaded from http://dtai.cs.kuleuven.be/krr/software.html

243

fiWittocx, Marien, & Denecker

u, v





ct: Edge(u, v)

ct: >

ct: >

Edge(u, v)

ct: Edge(u, v)
cf: Edge(u, v)

cf: Edge(u, v)

Sub(u, v)

ct: xyz (y = z  Edge(x, y)  Edge(x, z))

x, y, z



ct: y = z  Edge(x, y)  Edge(x, z)

ct: Edge(x, y)  Edge(x, z)



y=z

ct: y = z
cf: y 6= z

cf: Edge(x, y)  Edge(x, z)

cf: Edge(x, y)



Sub(x, y)

Sub(x, z)

cf: Edge(x, z)

Figure 2: A bottom-up c-map

244

fiGrounding FO and FO(ID) with Bounds

5.1 Three-Valued Structures
While FO(ID) has a standard two-valued semantics, three-valued structures are used in the formal
semantics of definitions. Indeed, an inductive definition defines a set by describing how to construct
it. In the semantics, the intermediate stages of the construction are recorded by three-valued sets,
representing for any object whether it belongs to the set or not, or whether this has not yet been
derived. We therefore recall the basic concepts of three-valued logic.
We denote the truth values true, false and unknown by respectively t, f and u. A three-valued
-interpretation I consists of a domain D and


 a domain element xI  D for each variable x;


 a function P I : Dn  {t, f, u} for each predicate symbol P/n;


 a function F I : Dn  D for each function symbol F/n.

If P I (d) 6= u for every tuple d of domain elements and predicate symbol P , then I is two-valued:

it corresponds to the interpretation I that assigns d  P I iff P I (d) = t for every predicate P and
corresponds to I on all other symbols.
The truth order  on the set of truth values is induced by f < u < t, the precision order p is
induced by u <p f and u <p t. These orders are extended to three-valued -structures: if I and J
correspond on F , then we define


 I  J iff P I (d)  P J (d) for every d and P ;


 I p J iff P I (d) p P J (d) for every d, P .

Observe that two-valued structures are maximally precise three-valued structures. On the other

hand, the least precise three-valued structure assigns P I (d) = u for every d and P .

We define the truth value I()
of a formula  in a three-valued interpretation I with domain D
by the standard Kleene semantics:
 (t1 , . . . , tn )) := P I(tI, . . . , tI );
 I(P
n
1
 1  2 ) := lub {I(
 1 ), I(
 2 )};
 I(
 1  2 ) := glb {I
 1 , I(
 2 )};
 I(


 I(x
) := lub {I[x/d]()
| d  D};


 I(x
) := glb {I[x/d]()
| d  D}.
An atom of the form P (d), where d is a tuple of domain constants, is called a domain atom. For
 (d)/v] the interpretation that assigns
a truth value v and a domain atom P (d), we denote by I[P
v to P (d) and corresponds to I on all other symbols. This notation is extended to sets of domain
atoms.
5.2 Inductive Definitions
An FO(ID) theory is a set of FO sentences and definitions. A definition  is a finite set of rules of
the form3
x (P (x)  ),
3. Usually, nested terms are allowed as arguments of P , but to facilitate the presentation, we only allow variables as
arguments in this paper.

245

fiWittocx, Marien, & Denecker

where P is a predicate and  an FO formula. The free variables of  should be among x. P (x) is
called the head of the rule,  the body. Predicates that occur in the head of a rule of  are called
defined predicates of . The set of all defined predicates of  is denoted Def(). All other symbols
are called open with respect to . The set of open symbols of  is denoted by Open().
Observe that an FO(ID) theory has the appearance of an FO theory augmented with a collection
of logic programs. As illustrated by Denecker and Ternovska (2008), this entails that FO(ID)s
definitions can not only be used to represent mathematical concepts, but also for the sort of common
sense knowledge that is often represented by logic programs, such as (local forms of) the closed world
assumption, inheritance, exceptions, defaults, causality, etc.
The semantics of definitions is given by their well-founded model (Van Gelder, Ross, & Schlipf,
1991). As argued by Denecker and Ternovska (2008), the well-founded semantics correctly formalizes
the semantics of all of the above mentioned types of inductive definitions in mathematics. We borrow
the presentation of this semantics from Denecker and Vennekens (2007).
Definition 34. Let  be a definition and I a three-valued structure. A well-founded induction for
 above I is a sequence hJ i0 of three-valued structures such that

1. J0 assigns P J0 (d) = u, if P is a defined predicate and corresponds to I on the open symbols;

2. For each limit ordinal   , J = lubp {J |  < };
3. For every ordinal , J+1 relates to J in one of the following ways:

(a) J+1 = J [P (d)/t] for some domain atom P (d) such that P J (d) = u and for some rule

x (P (x)  ) in , J [x/d]() = t.

(b) J+1 = J [U/f], where U is a set of domain atoms, such that for each P (d)  U , P J (d) =
u and for all rules x (P (x)  ) in , J+1 [x/d]() = f.

Intuitively, (a) says that a domain atom P (d) can be made true if there is a rule with P (x)
as head and body  such that [x/d] is already true. On the other hand (b) explains that P (d)
can be made false if there is no possibility of making a corresponding body true, except by circular
reasoning. The set U , commonly called an unfounded set, is a witness to this: making all atoms in
U false also makes all corresponding bodies false.
A well-founded induction is called terminal if it cannot be extended anymore. The limit of a
terminal well-founded induction is its last element. Denecker and Vennekens (2007) show that each
terminal well-founded induction for  above I has the same limit, which corresponds to the well Open() , and is denoted by wfm (I).
 The well-founded model is
founded model of  extending I|
three-valued in general.
A two-valued structure I satisfies a definition  if I = wfm (I). An FO(ID) theory T is a
finite set of FO sentences and definitions. I satisfies T if it satisfies all definitions and sentences
in T . If  is a definition over  and J a |Open() -structure, there exists at most one expansion
I of J to  such that I |= . A definition is called total if for any |Open() -structure J there
is precisely one expansion I of J to  that satisfies . Intuitively, total definitions correspond to
well-formed definitions: for every defined predicate P , they define for each tuple of domain elements
whether d belongs to the relation denoted by P or not. If a definition is not total, this typically
indicates an error. Hence in practice, all definitions that occur in MX(FO(ID)) specifications are
total. For example, this is the case for all MX(FO(ID)) specifications used in the second ASPcompetition (Denecker, Vennekens, Bond, Gebser, & Truszczynski, 2009). In general, checking
whether a definition is total is undecidable. However, there are several broad and easily recognizable
classes of total definitions. For example, all monotone and stratified definitions are total.
We give some examples of definitions and MX(FO(ID)) problems.

246

fiGrounding FO and FO(ID) with Bounds

Example 12. Definition 1 defines relation T C to be the transitive closure of relation R.


xy (T C(x, y)  R(x, y)).
1 =
xy (T C(x, y)  z (T C(x, z)  T C(z, y))).
Example 13. To cast the problem of finding a Hamiltonian path in a given graph as an MX(FO(ID))
problem, let
 = h{Edge/2}, i
 = {P  {Ham/2, Reached/1}, {Start/0}i.
Predicate Ham represents the edges that form the path and Reached the vertices that are in the
path. The constant Start represents the first vertex of the path. Let T be the theory
v1 v2 (Ham(v1 , v2 )  Edge(v1 , v2 )).
v1 v2 v3 (Ham(v1 , v2 )  Ham(v1 , v3 )  v2 = v3 ).
v1 v2 v3 (Ham(v1 , v3 )  Ham(v2 , v3 )  v1 = v2 ).
v Ham(v, Start).
v Reached(v).


v (Reached(v)  v = Start).
.
v (Reached(v)  w (Reached(w)  Ham(w, v))).
Then model expansion for input structure T and input vocabulary  expresses the Hamiltonian path
problem: in every model M |=I T , the collection of edges (v1 , v2 )  HamM forms a Hamiltonian
path in the graph represented by EdgeI .
A well-known concept that we will use later on in this section is the completion of a definition.
The completion of a definition  is an FO theory that is weaker than , and is defined as follows.
Definition 35. The completion of a definition  is the FO theory that contains for every P  Def()
the sentence
x (P (x)  ((x = y 1  1 )  . . .  (x = y n  n ))),
where y 1 (P (y 1 )  1 ), . . . , y n (P (y n )  n ) are the rules in  with P in the head.
We denote the completion of  by Comp(). Clearly, every body of a rule in  occurs in
Comp(). If T is a theory then we denote by Comp(T ) the result of replacing in T all definitions
by their completion. The following result states that the completion of T is weaker than T .
Theorem 36 (Denecker & Ternovska, 2008).  |= Comp() and T |= Comp(T ) for every definition
 and FO(ID) theory T .
The SAT(ID) problem is the problem of deciding whether a given propositional FO(ID) theory
is satisfiable. Currently there exist three SAT(ID) solvers. IDsat (Pelov & Ternovska, 2005) works
by translating a SAT(ID) problem into an equivalent SAT problem and then calls a SAT solver.
MidL (Marien, Wittocx, & Denecker, 2007) and MiniSAT(ID) (Marien, Wittocx, Denecker, &
Bruynooghe, 2008) take a native approach. Marien (2009) provides details on the specific form
of propositional FO(ID) theories accepted by these solvers, and a method to transform arbitrary
propositional FO(ID) theories into this form.
5.3 Grounding Inductive Definitions
Like MX(FO) problems, MX(FO(ID)) problems can be reduced to SAT(ID) problems by grounding.
In this section we extend grounding and the refinement algorithm of Section 4 to FO(ID). Without
loss of generality (Marien, Gilis, & Denecker, 2004), we assume that none of the predicates of the
input vocabulary  is defined by a definition in T , and no predicate is defined by more than one
definition. Moreover, we assume that every rule body is in TNF.
247

fiWittocx, Marien, & Denecker

5.3.1 Full and Reduced Grounding
Let T be an FO(ID) theory. As for FO, a grounding Tg for T with respect to I is a propositional
FO(ID) theory that is I -equivalent to T . We extend the notion of full and reduced grounding to
definitions.
Definition 37. The full grounding of a rule x P (x)   with respect to I is the set {P (d) 
Grfull ([x/d]) | d  Dn }, where n is the number of variables in x. Similarly, the reduced grounding
of x (P (x)  ) is the set {P (d)  Grred ([x/d]) | d  Dn }. The full (reduced) grounding of a
definition  is the union of the full (reduced) groundings of all rules in .
The full (reduced) grounding of an FO(ID) theory T is the set of the full (reduced) groundings
of all sentences and definitions in T .
5.3.2 Definitions Depending Only on 
We say that a definition  depends on expansion symbols if Open() 6 . If  does not depend on
expansion symbols, then the interpretation of every predicate in Def() is the same in every model
M of T expanding I . Indeed, for such a definition and any M |=I T , M |Open() is completely
determined by I . Therefore also wfm (M ) only depends on I .
The deductive database literature describes several algorithms to compute wfm (M ) for a definition that does not depend on expansion symbols. Most of them are only defined for definitions
where every rule body is a conjunction of atoms. But some of them, such as the Rete algorithm
(Forgy, 1982) and the semi-naive evaluation technique (Ullman, 1988), can easily be adapted to
handle full FO bodies.
Assume  is a definition that does not depend on expansion symbols. Let  be the vocabulary
hP  Def(), F i and I the  -structure such that I | = I and I |= . Then clearly, M |=I T
iff M |=I T for any structure M . However, a grounding for T \  with respect to  can be obtained
more efficiently, since Grred (T \ , I ) is necessarily smaller than Grred (T, I ). Indeed, T \  is a
subtheory of T , and Grred (T \ , I ) does not contain symbols of Def(), while Grred (T, I ) does.
Observe also that the set of c-maps for T over  is a superset of the set of c-maps for T over ,
since the bounds assigned by the former c-maps are formulas over  , instead of only over . As such,
c-maps computed by the refinement algorithm for T over  might yield more efficient grounding
compared to c-maps computed for T over .
5.3.3 Bounds for Definitions
We now extend the refinement algorithm to FO(ID).
Definition 38. A formula  is a subformula of an FO(ID) theory T if it is a subformula of a sentence
in T or a subformula of a rule body in a definition of T . A c-map for T over  is an assignment of
a ct- and cf-bound over  to every subformula of T .
Note that a c-map does not assign bounds to heads of rules in a definition.
Our strategy to compute a c-map for an FO(ID) theory T is simple: construct the completion of
T and apply the refinement algorithm on Comp(T ) to obtain a c-map C for Comp(T ). The restriction
of C to the subformulas of T is a c-map for T . Indeed, every subformula  of T occurs in Comp(T )
and since T |= Comp(T ), Comp(T ) |= x (C ct ()  ) and Comp(T ) |= x (C cf ()  ), also
T |= x (C ct ()  ) and T |= x (C cf ()  ).
In order to use a c-map for grounding, we lift the definition of c-transformation to FO(ID)
theories.
Definition 39. Let C be a c-map for a theory T and  a definition in T . The c-transformation
of a rule x (P (t)  ) of  is given by x (P (t)  Chi). The c-transformation Chi of a

248

fiGrounding FO and FO(ID) with Bounds

definition  is the set of c-transformations of rules in . The c-transformation of T is the set of the
c-transformations of the formulas and definitions in T .
We also lift the notion of C-equivalence to definitions.
Definition 40. Two definitions 1 and 2 are C-equivalent if for every structure I that satisfies C,
I |= 1 iff I |= 2 .
However, Lemma 15 does not hold for FO(ID) theories: for a definition , Chi is not necessarily
C-equivalent to .
Example 14. Let  be the empty vocabulary and T the theory
P
{P  P }.
This theory is unsatisfiable because the definition {P  P } has only one model, in which P is false.
This contradicts the sentence in T . Clearly, > is a ct-bound for P . If C is a c-map for T over 
assigning (>, ) to P , then Ch{P  P }i = {P  (P  )  >}, which is equivalent to {P  >}.
This definition has only a model that assigns true to P . Since this model also satisfies C, we conclude
that {P  P } and Ch{P  P }i are not C-equivalent.
Definition 41. Let  a definition of T . We call c-map C for T -tolerant if Chi and  are
C-equivalent. We call C T -tolerant if it is -tolerant for every definition  of T .
In the following, we say that a formula occurs positively (negatively) in a definition  if it occurs
positively (negatively) in a body of a rule in .
Proposition 42. Let  be a definition of a theory T . Then a c-map C for T over  is -tolerant
if for every subformula  of  that contains a predicate P  Def(), the following hold:
1. If  is not total, then C ct () = C cf () = .
2. If  occurs positively in  and P occurs positively in , then C ct () = .
3. If  occurs negatively in  and P occurs negatively in , then C cf () = .
Note that the c-map of Example 14 violates the second condition. We will prove Proposition 42
by inductively constructing for any structure I that satisfies C, a sequence of three-valued structures
that is a well-founded induction above I for both  and Chi. If I |= , we show that a terminal
sequence with this property can be constructed, proving that I also satisfies Chi. If I 6|= , a
sequence with this property can be constructed such that its last element is not less precise than I.
This shows that I does not satisfy Chi either. To construct a well-founded induction for both 
and Chi, we prove that each step that extends a well-founded induction for  is also a valid step
to extend it for Chi. Step (3a) in Definition 34 is covered by Lemma 43, step (3b) by Lemma 44.
Lemma 43. Let I be a structure that satisfies a c-map C for T over  and let J p I be a three  is two-valued. Then J()


valued interpretation such that J|
p J(Chi)
for every subformula  of
T.
Proof. We prove this lemma by induction. First assume [x] is an atom. Then Chi is the formula




 ct ()) must
(  C cf ())  C ct (). If J()
= u, then clearly J(Chi)
p J().
If J()
= f, then J(C


 cf ()) = f
be false, since I |= C. Therefore J(Chi)
= f. If on the other hand, J()
= t, then J(C

and hence, J(Chi)
= t.
The inductive cases are all very similar to the base case. We prove one of them. Assume  is

the formula   . Then Chi is the formula ((Chi  Chi)  C cf ())  C ct (). If J()
= f, then




 ct ()) = f, we conclude
J()
= J()
= f, and by induction J(Chi)
= J(Chi)
= f. Since also J(C


 cf ()) = f. Also J()


that J(Chi)
= f. If on the other hand J()
= t, then J(C
= t or J()
= t,



and therefore J(Chi)
= t or J(Chi)
= t. Hence J(Chi)
= t.
249

fiWittocx, Marien, & Denecker

Lemma 44. Let  be a definition of T and C a c-map for T over  that satisfies the three conditions
of Proposition 42. Let I be a structure that satisfies C and J p I a three-valued interpretation such
  is two-valued. If U is a set of domain atoms defined in  and unknown in J,
 then for every
that J|

subformula  of  such that J[U/f]() 6= u, the following hold:


 J[U/f]()
 J[U/f](Chi)
if  occurs negatively in ;


 J[U/f]()
 J[U/f](Chi)
if  occurs positively in ;


Proof. Denote H := J[U/f].
If J()
6= u, the result follows immediately from Lemma 43.


=u
We prove the case where J()
= u by induction. Assume that  is an atom P (x). Since J()
J
cf
and H() 6= u, we know that P (x )  U and H() = f. Therefore H(Chi) = H((  C ()) 
C ct ()) = H(C ct ()). If  occurs negatively in , then we have to prove that H()  H(Chi).
Since H() = f, this inequality holds regardless the value of C ct () and C cf () in H. If on the
other hand,  occurs positively, we have to prove that H()  H(Chi). Since H() = f and
H(Chi) = H(C ct ()), this inequality can only hold if H(C ct ()) = f. Because the conditions on C
ensure that C ct () = , we can conclude that indeed H(C ct ()) = f.
We omit the inductive cases, since they are very similar to the base case.
Proof of Proposition 42. Let I be a structure that satisfies C. We have to prove that I |=  iff
I |= Chi. If  is not total, the proof is trivial, since then  and Chi are equivalent.
Now assume that  is total and let hJ i0 be a well-founded induction for both  and Chi
above I. We will prove that if J is not two-valued, and J <p I, there exists a J+1 such that
hJ i0+1 is again a well-founded induction for  and Chi. Also observe that if  is a limit
ordinal and hJ i0< is a well-founded induction for both  and Chi, then the same holds for
hJ i0 .
This is sufficient to conclude the proof. Indeed, if I |= , we can keep on extending the sequence
until we end up in I, and derive that I |= Chi. If I 6|= , then we will eventually extend the
well-founded induction with a structure J+1 6p I. But then, the well-founded model of Chi will
also be more precise than J+1 , which shows that I 6|= Chi.
Assume that J is not two-valued and J <p I. Because  is total, there exists a J+1 such
that hJ i0+1 is a well-founded induction for . We have to prove that it is also a well-founded
induction for Chi. There are two possibilities:
 J+1 = J [P (d)/t] for some domain atom P (d) and there is a rule x (P (x)  ) in 
such that J [x/d]() = t. By Lemma 43, also J [x/d](Chi) = t. Hence, hJ i0+1 is a
well-founded induction for Chi.
 J+1 = J [U/f] and for every P (d)  U and rule x (P (x)  ) in , J+1 [x/d]() = f.
By Lemma 44, we conclude that also J+1 [x/d](Chi) = f. Therefore, hJ i0+1 is a wellfounded induction for Chi.

From Proposition 42 we derive the following procedure to compute a T -tolerant c-map for a
theory T . First compute a c-map C for T that is not necessarily T -tolerant. Then, for every
definition  of T and every subformula  of , replace C ct () and C cf () by , if this is required
to satisfy the conditions of Proposition 42.
We conclude that the following algorithm produces a correct grounding for FO(ID) theory T :
1. Compute a c-map C for T over .
2. If C is inconsistent with respect to I , output  and stop.
3. Else, derive an atom-based, T-tolerant c-map C 0 from C.
4. Output Grred (C 0 hT i  C 0 A ), using any off-the-shelf grounder for FO(ID).
250

fiGrounding FO and FO(ID) with Bounds

6. Implementation and Experiments
So far we have focussed mostly on grounding size. Proposition 23 guaranteed that grounding with
bounds produces smaller groundings. In this section we are concerned with the efficiency and
practical implementation of grounding with bounds. A first issue was mentioned at the end of
Section 4.4.2: an atom-based c-map C computed by the refinement algorithm contains many repeated
constraints on variables. To ground ChT i efficiently, such repetitions should be avoided as much as
possible. Secondly, an efficient grounder consults bounds as soon as possible. In particular, it
should use bounds to avoid unnecessary instantiations of variables, rather than to remove these
instantiations afterwards. As a case study, we will show in detail how to adapt a basic top-down
style grounding algorithm to efficiently exploit bounds. We sketch how the same principles can be
applied for a bottom-up style grounder.
In the second part of this section we discuss some aspects of implementing the refinement algorithm. As we mentioned in Section 4.4.1, there are several issues concerning the practical implementation of this algorithm. In particular, a method to simplify bounds is needed, as well as a good
stop criterion. We show how these issues can be addressed by representing bounds as first-order
binary decision diagrams.
Finally, we report on our implementation, called GidL, of the refinement and grounding algorithm. We present experimental results that show the impact of using bounds on grounding size and
time.
6.1 Case Study: Top-Down Grounding with Bounds
For the rest of this section, assume T is in TNF and fix an I -consistent, atom-based c-map C for T
over . We call a formula of the form    or x  a disjunctive formula. Vice versa, a conjunctive
formula is a formula of the form    or x .
We now present a simple top-down style grounding algorithm that exploits bounds without
constructing ChT i  C A explicitly. The algorithm is shown in Algorithm 1. Basically, it consults
the bounds assigned by C whenever it substitutes the free variables of a formula [x] by domain
constants d. If according to the bounds, [x/d] is certainly true, i.e., I [x/d] |= C ct (), then the
grounding of [x/d] is not computed. Instead, the algorithm then proceeds as if [x/d] is equal
to >. Similarly if [x/d] is certainly false. In this way, the algorithm avoids creating unnecessary
instantiations. One can check that if C is the trivial c-map, Algorithm 1 reduces to a straightforward
top-down style grounding algorithm that produces Grfull (T ).
Line 1 of Algorithm 1 checks whether one of the sentences of T is certainly false. If this is the
case, then clearly T is unsatisfiable (cf. Definition 10), and this can be reported immediately. Before
a sentence is grounded, line 4 checks whether this sentence is certainly true according to C. Only
sentences that are not certainly true are grounded. Observe that both checks are simple syntactic
checks and can be executed in constant time.
Function groundConj gets as input a formula [x] and returns a grounding for x [x]. In
particular, if  is a sentence, then the result of applying groundConj to  is a grounding for .
In groundConj, universal quantifiers are implicitly pushed inside conjunctions. That is, if [x]
is a conjunction 1  . . .  n , then for every i  [1, n], the grounding of x i is computed by
applying groundConj to i . The conjunction of these groundings is returned as grounding for x .
According to equivalence (6) of Section 2.2, this transformation yields an equivalent formula.
Function groundConj only consults the c-map when variables are substituted by domain constants or when the input formula is an atom. As such, groundConj ignores (eliminates) the bounds
assigned to conjunctive formulas. As we mentioned at the end of Section 4.4.2, this is important to
avoid repeated constraints on a variable.
In groundConj([x]), only those substitutions [x/d] for which I [x/d] 6|= C ct () are grounded
(see, e.g., line 12). Indeed, the other substitutions yield a formula that is certainly true in all models
of T expanding I , and can therefore be omitted from the ground conjunction C that is computed.
251

fiWittocx, Marien, & Denecker

Algorithm 1: Ground with Bounds
Input: T , , I and C
Output: A grounding Tg for T with respect to I
1
2

3
4

5
6

7
8
9
10
11

12

if C cf () = > for some sentence  of T then return ;
Tg := ;
// Ground all sentences of T
for every sentence  of T do
if C ct () 6= > then Add groundConj() to Tg ;
// Ground all definitions of T
for every definition  of T do
Add groundDef() to Tg ;
// Add the grounding of C A
for every atomic subformula [x] of T do
for every d such that I [x/d] |= C ct () do
Add [x/d] to Tg ;
for every d such that I [x/d] |= C cf () do
Add [x/d] to Tg ;
return Tg ;

Function groundConj([x])
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

C := ;
switch [x] do
case  is a literal
for all d such that I 6|= C ct ()[x/d] do
if I |= C cf ()[x/d] then return ;
else Add [x/d] to C;
case  = y [x, y]
return groundConj([x, y]);
V
case  =S i i
C := i groundConj(i );
case  is a disjunctive formula
for all d such that I 6|= C ct ()[x/d] do
if I |= C cf ()[x/d] then return ;
else Add groundDisj([x/d]) to C;
return

V

C;

252

fiGrounding FO and FO(ID) with Bounds

Function groundDisj([x])
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

D := ;
switch [x] do
case  is a literal
for all d such that I 6|= C cf ()[x/d] do
if I |= C ct ()[x/d] then return >;
else Add [x/d] to D;
case  = y [x, y]
return groundDisj([x, y]);
W
case  =S i i
D := i groundDisj(i );
case  is a conjunctive formula
for all d such that I 6|= C cf ()[x/d] do
if I |= C ct ()[x/d] then return >;
else Add groundConj([x/d]) to D;
return

W

D;

Function groundDef()

8

g := ;
for every rule x (P (x)  [y]) in  do
z := x \ y;
for every d such that I 6|= C cf ([y/d]) do
if I |= C ct ([y/d]) then g := >;
else g := groundConj([y/d]);
n := the number of variables in z;
0
0
Add P (x)[y/d, z/d ]  g to g for every d  Dn ;

9

return g ;

1
2
3
4
5
6
7

253

fiWittocx, Marien, & Denecker

Before [x/d] is grounded, it is checked whether this substitution yields a formula that is certainly
false (see, e.g., line 13). If this is the case, the whole conjunction C will certainly be false, and
therefore  is returned immediately. Observe that implicitly the formula C ct ()  (C cf ()  ) is
grounded. Hence the correctness of groundConj follows from Lemma 13.
Function groundDisj is dual to groundConj. On input [x], it returns a grounding for x [x].
It implicitly pushes existential quantifiers through disjunctions and eliminates the bounds assigned
to disjunctive formulas.
Function groundDef returns a grounding for its input definition . It grounds the rules of 
one-by-one. For each rule x (P (x)  [y]), only those substitutions [y/d] that are possibly true
are tried (line 4). If [y/d] is certainly true, it is replaced by > (line 5).
In lines 7-11 of Algorithm 1, the theory C A is grounded. Recall that this is necessary to obtain
a grounding that is I -equivalent to T (see Proposition 21). Observe that if C is the trivial c-map,
no output is produced when lines 7-11 are executed.
The computationally expensive steps in Algorithm 1 are the steps where the truth values in I
of (some of the) bounds assigned by C are computed. For large bounds, these steps can become
infeasible. Indeed, the expression complexity of FO is PSPACE-complete (Stockmeyer, 1974). As
such, grounding with too complex bounds may take more time and space than constructing the
full grounding and simplifying it afterwards. The stop criterion of Section 6.2.3 for the refinement
algorithm is designed to avoid too complex bounds. Our experiments in Section 6.3 show that
carefully restricting the complexity of the bounds leads to faster grounding.
We stress that Algorithm 1 is just one example of a grounding algorithm that exploits bounds.4
The principle of consulting bounds as soon as possible can be applied to adapt other grounding
algorithms as well. For example, recall that a bottom-up style grounder starts by storing all instances
of atomic subformulas of T in a table. To exploit bounds efficiently, a bottom-up grounder should
consult the bounds while constructing these tables and leave out, e.g., all instances that are certainly
false. As such, it avoids unnecessary large tables, which in turn improves the speed of the subsequent
grounding steps.
6.2 Implementing the Refinement Algorithm and Querying Bounds
In this section we discuss some aspects of implementing the refinement algorithm. As mentioned
above, applying a simplification method for first-order formulas to simplify the bounds at regular
time points is essential for a good implementation. One can use Goubaults (1995) method for this
purpose. To this end, the bounds need to be represented by first-order binary decision diagrams.
We show in this section that such a representation can be applied without too much overhead when
applying one-step refinements. Moreover, using binary decision diagrams leads to extra benefits:
we obtain a cheap equivalence check for bounds and an elegant algorithm to query bounds, which
is needed to implement Algorithm 1. At the end of this section we discuss a stop criterion for the
refinement algorithm and we discuss an implementation.
6.2.1 First-Order Binary Decision Trees and Diagrams
We borrow the definition of first-order BDDs from Goubault (1995). Let , 1 and 2 be three
formulas. The ternary if-then-else operator is denoted by _, and defined by  _ 1 ; 2 :=
(  1 )  (  2 ). The formula  _ 1 ; 2 is also represented by the graph shown in Figure 3.
Definition 45 (Goubault, 1995). FO binary decision trees (BDTs) and kernels are defined by
simultaneous induction:
 An atom is a kernel;
4. The question whether top-down grounders can be made more efficient than bottom-up grounders is outside the
scope of this paper, and still undecided.

254

fiGrounding FO and FO(ID) with Bounds


yEEE
EE
y
EE
y
E"
|y
2
1
Figure 3: Graph representation of the formula  _ 1 ; 2
 If  is a BDT and x a variable, then x  is a kernel;
 > and  are BDTs;
 If  is a kernel and 1 and 2 are BDTs, then  _ 1 ; 2 is a BDT.
Observe that the graph representation of a BDT is a tree whose nodes are atoms or existentially
quantified BDTs.
Goubault (1995) showed that for every FO formula  there exists a BDT 0 such that  and 0
are equivalent. In an actual implementation, sharing, reducing and ordering are applied to obtain
a simplified and compact representation of BDTs. Such representations are called reduced ordered
binary decision diagrams (BDDs). Sharing means that isomorphic subtrees are stored at the same
address in memory. Reducing involves exhaustively replacing subtrees of the form  _ ;  by . A
BDT  is ordered if the kernels appear in some fixed order on every path in the graph representation
of .
As mentioned above, there are several important benefits of using BDDs to represent bounds for
a formula:
 An implementation of the refinement algorithm using BDDs allows us to use the simplification
algorithm for BDDs of Goubault (1995).
 As explained in Section 4.4, to detect that the refinement algorithm has reached a fixpoint,
one needs to check the equivalence of bounds. Often, the BDDs representing two equivalent
formulas will be equal.5 Hence, a cheap (but necessarily incomplete) equivalence check for two
bounds consists of checking the syntactic equality of the two BDDs representing them. Since
equal BDDs are stored at the same address, this check is done in constant time.
 As we will show in Section 6.2.2, querying a bound [x], i.e., finding all tuples d such that
I [x/d] |= , can easily be implemented directly on a BDD representation of . Querying a
bound is one of the main operations performed by a grounding algorithm that exploits bounds
directly (such as Algorithm 1).
On the other hand, using BDDs does not result in too much overhead when computing a c-map. If
,  and [x, y] are represented by BDDs, then a BDD representing , x , x ,   ,   
and [x/x0 , y] can be computed efficiently (Bryant, 1986; Goubault, 1995). This implies that every
one-step refinement on a c-map C can be implemented efficiently, even if the bounds assigned by C
are BDDs.
6.2.2 Querying a Bound
In Algorithm 1, the main operation performed on a bound [x] is querying: finding tuples d of
domain constants such that I |= [x/d]. Finding a tuple d such that I 6|= [x/d] corresponds
to querying . We now show that querying a bound [x] can be done directly on the BDD
representation by a simple backtracking algorithm.
5. For propositional BDDs, this is always the case.

255

fiWittocx, Marien, & Denecker

P (x)
yEEE
EE
y
EE
y
E"
|y
Q(x,
y)
R(x)
RRR
l
R
l
R
R
l

lRR
 l l RRRRR
)
ul

>
Figure 4: A BDD representing the formula (P (x)  Q(x, y))  (P (x)  R(x))
The idea is to traverse the BDD, starting from the root, and trying to end up in the leaf >. At
each inner node [y] _ 1 ; 2 , the free variables in that node are replaced by domain constants dy .
If I |= [y/dy ], the algorithm continues via 1 , otherwise via 2 . If it ends up in , it backtracks.
If on the other hand, it ends up in >, the performed substitutions constitute an answer for .
Function query implements the sketched query algorithm. It gets a bound [x] as input and
returns a substitution [x/d] such that I |= [x/d]. If no such substitution exists, it returns FAIL.
This algorithm can easily be adapted to return all answers to [x] instead of just one.
Function query([x])
1
2
3
4
5
6
7
8
9

if  = > then return the empty substitution;
else if  = [y] _ 1 ;  then
for every tuple d such that I |= [y/d] do
 := query(1 [y/d]);
if  6= FAIL then return   [y/d]
else if  = [y] _ ; 2 then
for every tuple d such that I 6|= [y/d] do
 := query(2 [y/d]);
if  6= FAIL then return   [y/d]

14

else if  is of the form [y] _ 1 ; 2 then
for every tuple d  D|y| do
if I |= [y/d] then  := query(1 [y/d]);
else  := query(2 [y/d]);
if  6= FAIL then return   [y/d]

15

return FAIL;

10
11
12
13

In lines 3 and 7, the algorithm needs to find tuples d such that respectively I |= [y/d] and
I 6|= [y/d]. If [y] is an atom P (y), this can be implemented by consulting the table P I . If  is
a kernel x [x, y], function query can be applied recursively to find the tuples. Indeed, any answer
(d0 , d) to [x, y] provides a tuple d such that I |= [y/d]. Vice versa, I 6|= [y/d] if [x, y/d] has
no answer.
We illustrate the query algorithm on an example.
Example 15. Let [x, y] be the BDD shown in figure 4, and let {a, b} be the domain of I ,
P I = {b}, RI = {} and QI = {(b, b)}. To find an answer for [x, y], the query algorithm starts
at the root P (x). Since none of its children are equal to , every domain constant is tried. Assume
domain constant a is tried first. Because a 6 P I , the algorithm continues with node R(a) _ >; .
Because the else child of this node is  and a 6 RI , the algorithm returns to the root and tries
256

fiGrounding FO and FO(ID) with Bounds

domain element b. Since b  P I , it goes to node Q(b, y) _ >; . Since the else child of this node
is , the algorithm tries those substitutions d for y such that (b, y/d)  QI . Thus, y is substituted
by b. Finally, answer [x/b, y/b] is returned.
6.2.3 A Stop Criterion for the Refinement Algorithm
As shown in Section 4.4, the c-map refinement algorithm does not reach a fixpoint on certain inputs.
Also, even in the case a fixpoint can be found, computing it may take a long time, and the bounds
assigned by the fixpoint can be so complex that querying becomes very inefficient. Using such
bounds may severely slow down grounding. This indicates the need for a good stop criterion.
Simple Stop Criteria A very simple stop criterion limits the number of one-step refinements
that may be performed to a given maximum number m. This m may depend on the theory T . For
instance, m can be set to C  (number of subformulas in T ), where C is some fixed constant.
A slightly less naive technique, which can be combined with the previous, limits the complexity
of the bounds by putting a fixed upper bound N on the number of nodes the BDD representation
of a bound may have. If a one-step refinement would lead to a new bound with more nodes than N ,
this refinement is not performed. As this limits the number of applicable one-step refinements, the
probability of reaching a fixpoint increases.
Stop Criteria via Estimators The experiments we present in Section 6.3 indicate that there
exist appropriate values for C and N that produce positive results on most of the examples. Still,
on some problems, grounding slows down severely, while the size of the produced grounding does
not decrease. One of these problems is the following clique problem (entry 6 in Table 4).
Example 16. Recall that a clique is a maximally connected graph. Let
 = h{Edge/2}, i,
 = hP  {Clique/1}, i
and T the theory
xy (Clique(x)  Clique(y)  (x = y  Edge(x, y))).
x ((y (Clique(y)  x 6= y  Edge(x, y)))  Clique(x)).
If EdgeI is symmetric, i.e., I represents an undirected graph, a model of T expanding I is a clique
in I that is not contained in a strictly larger clique in I . Within a small number of iterations,
the refinement algorithm finds for Clique(x) the ct-bound x0 x 6= x0  Edge(x, x0 ). This formula
expresses that Clique(x) is certainly true in every solution if x is directly connected to every other
vertex in the input graph. Clearly, for most graphs, no vertex satisfies this condition. So, for most
graphs,  would be an equally precise ct-bound, but would allow much faster querying.
The situation is worse for the cf-bound for Clique(x). Since for an undirected graph, every
single vertex is a clique, and thus occurs in at least one of the solutions, the cf-bound is necessarily
unsatisfiable with respect to T . Yet, our implementation of the refinement algorithm came up with
x0 (Edge(x, x0 )  x 6= x0  (x00 (x0 6= x00  Edge(x0 , x00 )))) as cf-bound. The query algorithm
outlined above takes cubic time in the number of vertices to find out that no x satisfies this formula.
To avoid the problems illustrated by the example above, one could estimate the reward of a
bound versus the cost of evaluating it. Recall that more precise bounds yield smaller grounding
sizes. Therefore, the reward of a bound  is dictated by its precision. Given I , it is possible to
find a good estimate for the number of answers to  in I (Demolombe, 1980), which is in turn a
measure for the precision of . For a fixed query algorithm, one can also estimate the cost cost()
of computing an answer in I to a query . In the following, we assume that the reward of a bound
is a positive real number, and its cost a strictly positive real number.
257

fiWittocx, Marien, & Denecker

Given the reward and the cost of bounds, the complexity of a bound  can be limited by
restricting the ratio
cost()
r() :=
.
reward() + 1
If a one-step refinement would replace a bound 1 by 2 , but r(1 ) < r(2 ), then this refinement
is not performed. Clearly, for all bounds  assigned by a c-map C computed according to this
restriction, r()  r() holds. Observe that to apply this restriction, an input structure I is
needed. However, the obtained bounds are independent of I .
It is beyond the scope of this paper to describe in detail estimators for the reward and cost of
bounds. The fairly naive estimator used for the experiments in the next section assigns ratios of
the order O(|DI |), respectively O(|DI |3 ), to the ct-bound, respectively cf-bound, mentioned in
Example 16. As such, if |DI | is large enough, these bounds will be avoided.
6.2.4 Implementation of the Refinement Algorithm
Our implementation of the refinement algorithm, including the heuristic for choosing refinement
bounds (Section 4.4.1) and stop criterion, is presented by Algorithm 6. The algorithm maintains a
queue Q of one-step refinements that will be applied. Each of these is represented by a tuple hr, i,
where r is the type of the refinement, e.g., axiom refinement, and  the formula on which r will be
applied.
Algorithm 6: Refinement Algorithm
1
2
3
4
5
6
7
8
9
10
11
12
13

Q := ; C := the trivial c-map for T ;
for all sentences  of T do Q.push(haxiom, i);
for all subformulas  of T over  do
Q.push(hct-input, i); Q.push(hcf-input, i);
while Q 6=  and the maximum number of refinements is not reached do
hr, i := Q.pop();
if r is a ct-refinement then
 := the r-refinement bound for  with respect to C;
 := simplify(C ct ()  );
if  6= C ct () and  is not too complex then
C ct () := ;
for all hr, i such that the r-refinement bound for  contains C ct () do
Q.push(hr, i);

15

else
...

16

return C;

14

// Similar code for cf-refinements

As explained in Section 4.4.1, our implementation starts by scheduling all possible axiom- and
input-refinements. If in a later stage a bound is changed (line 11), then all refinement bounds that
contain this bound are scheduled to be applied (line 13). For example, assume that T contains the
formula    and that the ct-bound of  is refined. Then bottom-up ct-refinement for    is
scheduled since the bottom-up ct-refinement bound for that formula is given by C ct ()  C ct (),
which contains C ct (). For the same reason also top-down cf-refinement for  is scheduled.
The algorithm applies all scheduled refinements, unless the maximum number of refinement steps
is reached (line 5). The other part of the discussed stop criterion is applied in line 10. If the newly

258

fiGrounding FO and FO(ID) with Bounds

computed bound  is too complex, i.e., its BDD representation contains too many nodes or the ratio
r() is above a certain threshold,  is not used.
If BDDs are used to represent the bounds assigned by C, line 8 can be implemented in linear time
in the size of C. If we use Goubaults simplification algorithm for BDDs for implementing line 9,
the worst case complexity of this step is non-elementary in the size of C ct ()   (Goubault, 1995).
The estimators we used to implement line 10 take linear time in the size of . It may seem that the
complexity of the simplification method limits the practical applicability of Algorithm 6. However,
since large BDDs usually do not pass the test in line 10, the simplification method is rarely applied
on large BDDs. In the experiments of the next section, the running time of the refinement algorithm
is negligible compared to the running time of the grounding algorithm.
6.3 Experiments
We implemented Algorithm 1 and Algorithm 6, using BDDs to represent bounds. The resulting
grounder is called GidL. In this section, we present experiments, obtained with GidL, that show
the impact of using bounds on grounding size and time.
As input for GidL, we used 37 benchmark problems, mainly taken from Asparagus.6 The details
about the experiments are available at http://dtai.cs.kuleuven.be/krr/software.html. We
used four different versions of GidL:
GidLnb : Assigns h, i as bound to every atomic subformula  over the input vocabulary, and
h, i to every other subformula. As such, it creates the reduced grounding of the input
theory.
GidLbu : Assigns h, i as bound to every atomic subformula  over the input vocabulary and
then applies bottom-up refinements to obtain a bottom-up c-map.
GidLmn : Limits the refinement algorithm to 4  (number of subformulas in T ) one-step refinements
and allows a maximum of 4 internal nodes in each BDD used to represent the bounds. According to previous experiments (Wittocx et al., 2008b), this is the best setting when limiting
the number of nodes.
GidLr : Limits the refinement algorithm to 4  (number of subformulas in T ) one-step refinements.
It limits the complexity of the derived bounds by estimating the number of answers and the
cost, as described in the previous section.
In Table 3, the influence of bounds on the grounding size is shown. The second and third column
show the ratio of the grounding size obtained with GidLmn and GidLr compared to Grred (T ). For
GidLnb and GidLbu , this ratio is always equal to 1. When interpreting Table 3, it is important to
note that small reductions in grounding size are not important. The reason being that all reductions
that can be obtained by the refinement algorithm are also obtained by applying unit propagation
on the grounding (see Section 7 for a discussion). Since there exist very efficient implementations
of unit propagation, it is not beneficial to let the refinement algorithm find small reductions at a
relatively high cost. We see that both GidLmn and GidLr reduce the grounding size with more than
50% in around 30% of the benchmarks. In 7, respectively 6, of the benchmarks there is a spectacular
reduction of more than 95%.
More important than reductions in size are reductions in grounding time. Table 4 shows the
running times of the different versions of GidL, and (between brackets) the ratio of the running
time to the running time of GidLnb . The running time of the refinement algorithm is included (it
never took more than 0.02 seconds). A time-out (###) of 600 seconds was used.
On many benchmarks, the reduction in grounding time with respect to GidLnb is due to the
reduction in grounding size. Yet there are also several benchmarks where time decreases a lot, while
6. http://asp.haiti.cs.uni-potsdam.de/

259

fiWittocx, Marien, & Denecker

there is almost no reduction in size. This is mostly due to the creation of a bottom-up c-map, as can
be seen from the running times of GidLbu . Applying bottom-up refinements leads to the assignment
of non-trivial bounds to non-atomic subformulas. This allows for earlier pruning by a top-down style
grounder, and hence faster grounding.
From Table 4, we can see that GidLmn performs quite well. On half of the benchmarks, it is
more than 44% faster than GidLnb . It is also more than 20% faster than GidLbu on half of the
benchmarks. There are some outliers however. On benchmarks 6 and 11, it is far slower than GidLbu ,
while not producing a significantly smaller grounding. This indicates the use of a complex bound
with relatively small reward. Compared to GidLmn , GidLr is faster and more robust, indicating
that using estimators for the reward and cost of bounds pays off in most cases. In only two of the
benchmarks, our naive estimator makes a wrong guess. In benchmark 1, a bound with high cost
and no reward is allowed, in benchmark 7, a bound with low cost and high reward is not allowed by
GidLr . It is part of future work to implement improved estimators.
We conclude from our experiments that grounding with bounds is applicable in practice. It often
leads to smaller grounding sizes on standard benchmark problems, and if the bounds are carefully
restricted, it yields a significant speed up. Since the time to compute bounds is small compared to
the overall grounding time, computing them is essentially for free.
In general, a smaller grounding does not necessarily lead to faster propositional model generation.
For example, grounding size (and time) increases when symmetry breaking formulas are added, but
these formulas may drastically improve the overall solving time (Torlak & Jackson, 2007). Another
example are clause-learning SAT solvers: the clauses learnt by these solvers are redundant, but
may improve the solving time by orders of magnitude. The question arises whether our method of
grounding with bounds may lead to slower overall model generation time compared to grounding
without bounds. This is not the case. The experiments above show that in general, grounding with
bounds is faster than grounding without bounds. Since grounding with bounds also produces smaller
groundings, the subsequent initialization phase of the SAT solver is executed faster. If T1 and T2
are two groundings obtained by grounding the same input theory and structure with, respectively
without bounds, it can be shown7 that the typical simplification steps applied in this initialization
phase transform T1 and T2 in exactly the same simplified theory T3 . Thus, after initialization, the
SAT solver is applied on exactly the same theory, whether or not the grounder used bounds. It
follows that in general, the overall model generation time does not increase when bounds are applied
while grounding.

7. Related Work
In the previous sections we described a method to obtain fast and compact grounding. Several such
methods have been described in the literature. Some of them are  like ours  preprocessing
techniques that rewrite the input theory. Other techniques involve reasoning on the propositional
level. In this section we provide an overview. We indicate which ones can be applied to improve
GidL. We also give an overview of existing grounders.
7.1 Methods to Optimize Grounding
Derivation of Bounds To our knowledge, the methods proposed in the literature to derive bounds
are less general than the one we presented in this paper. This is illustrated by Table 5, where we show
for several grounders the impact of manually adding redundant information. For all the grounders
in this table except GidL, manually adding redundancy may have a serious impact. For some
grounders, the need to add redundancy can sometimes be avoided by writing the input theory in a
specific format. For example, the grounder gringo (Gebser et al., 2007) uses a syntactic check to
derive bounds: it derives that predicate q of the input vocabulary is a bound for predicate p if p
7. The exact formulation and the proof of the property are beyond the scope of this paper.

260

fiGrounding FO and FO(ID) with Bounds

Nr
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37

Benchmark name
15puzzle
Battleship
Blocked N-queens
Blocksworld
Bounded spanningtree
Clique
Hierarchical clustering
Graph colouring
Debugging
Fastfood
FO-hamcircuit
Golomb ruler
Graph partitioning
Algebraic groups
Hamiltonian circuit
Tower of Hanoi
Knighttour
Labyrinth
Magic series
Maze generation
Mirror puzzle
Missionaries
N-queens
Pigeonhole
Disjunctive scheduling
Slitherlink
Social golfer
Sokoban
Solitaire
Spanningtree
Sudoku
Tarski
Toughnut
Train scheduling
Waterbucket
Weight bounded dominating set
Wire routing
Average
# < 1.00
# < 0.50
# < 0.05

GidLmn
1.00
0.89
0.02
0.33
0.12
1.00
0.03
1.00
0.86
1.00
0.94
0.54
0.94
0.99
0.01
1.00
0.00
0.99
1.00
0.90
1.00
0.03
1.00
1.00
0.83
0.04
1.00
0.59
1.00
0.06
0.75
1.00
0.00
0.25
0.36
1.00
0.92
0.66
24
12
7

GidLr
1.00
1.00
0.02
0.33
0.12
1.00
0.72
1.00
1.00
1.00
0.99
1.00
1.00
1.00
0.01
1.00
0.00
0.99
1.00
0.90
1.00
0.03
1.00
1.00
0.83
0.04
1.00
0.59
0.73
0.06
0.75
1.00
0.00
0.25
0.36
1.00
0.99
0.70
20
11
6

Table 3: Impact of bounds on grounding size

261

fiWittocx, Marien, & Denecker

Nr
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
Total
Avg. gain
Median gain

GidLnb
6.13
0.19
9.66
22.33
8.52
3.13
0.32
2.57
0.30
###
###
14.05
0.03
9.68
70.75
2.32
12.22
8.80
1.83
2.77
0.12
17.4
4.62
4.92
151.15
0.25
5.47
2.78
0.43
6.86
###
4.42
4.23
4.06
3.16
1.45
0.06
2186.98

GidLbu
2.00 (0.33)
0.18 (0.95)
10.83 (1.12)
16.76 (0.75)
8.52 (1.00)
3.73 (1.19)
0.34 (1.06)
2.71 (1.05)
0.30 (1.00)
### (1.00)
5.87 (0.01)
3.54 (0.25)
0.04 (1.33)
9.58 (0.99)
71.50 (1.01)
1.83 (0.79)
10.35 (0.85)
8.83 (1.00)
1.76 (0.96)
2.80 (1.01)
0.11 (0.92)
18.08 (1.04)
4.60 (1.00)
5.01 (1.02)
151.66 (1.00)
0.13 (0.52)
5.47 (1.00)
2.66 (0.96)
0.43 (1.00)
6.79 (0.99)
2.34 (0.00)
4.53 (1.02)
4.23 (1.00)
2.14 (0.53)
3.07 (0.97)
1.42 (0.98)
0.06 (1.00)
974.20 (0.45)
12 %
0%

GidLmn
2.07
(0.34)
0.16
(0.84)
2.22
(0.23)
5.80
(0.26)
3.01
(0.35)
51.77 (16.54)
0.05
(0.16)
2.69
(1.05)
0.48
(1.60)
17.59
(0.03)
37.86
(0.06)
4.13
(0.29)
0.03
(1.00)
11.20
(1.16)
2.56
(0.04)
1.96
(0.84)
0.06
(0.00)
8.83
(1.00)
1.79
(0.98)
0.51
(0.18)
0.12
(1.00)
2.29
(0.13)
4.62
(1.00)
4.90
(1.00)
172.50
(1.14)
0.02
(0.08)
5.37
(0.98)
1.57
(0.56)
0.46
(1.07)
0.59
(0.09)
1.07
(0.00)
3.67
(0.83)
0.53
(0.13)
0.65
(0.16)
1.76
(0.56)
0.03
(0.02)
0.08
(1.33)
355.00
(0.16)
0%
44 %

GidLr
5.73 (0.93)
0.17 (0.89)
2.67 (0.28)
5.80 (0.26)
1.16 (0.14)
3.73 (1.19)
0.31 (0.97)
2.72 (1.06)
0.47 (1.57)
16.52 (0.03)
6.06 (0.01)
3.40 (0.24)
0.02 (0.67)
9.60 (0.99)
1.81 (0.03)
1.83 (0.79)
0.10 (0.01)
8.73 (0.99)
1.81 (0.99)
0.17 (0.06)
0.10 (0.83)
2.68 (0.15)
4.64 (1.00)
4.90 (1.00)
171.54 (1.13)
0.02 (0.08)
5.41 (0.99)
1.54 (0.55)
0.49 (1.14)
0.57 (0.08)
1.06 (0.00)
3.64 (0.82)
0.53 (0.13)
0.47 (0.12)
2.04 (0.65)
0.03 (0.02)
0.08 (1.33)
272.55 (0.12)
40%
33%

(For GidLmn and GidLr , the time to compute the bounds is included.)

Table 4: Impact of bounds on grounding time

262

fiGrounding FO and FO(ID) with Bounds

gringo
dlv
lparse
psgrnd
gidl

constr
76.33
339.37
63.25
44.79
0.26

redun
1.59
4.23
0.78
0.72
0.42

defin
0.60
2.81
63.58
n/a
n/a

Table 5: Grounding times (in seconds) for the Hamiltonian circuit problem with an input graph of
200 nodes and 1800 edges. Encoding constr uses a constraint to state that each edge in
the cycle should be an edge of the graph. Encoding redun adds redundancy to include this
bound in all rules and constraints. Encoding defin contains no redundancy, but limits the
possible edges in the cycle to the edges in the graph while defining the search space for the
cycle.

is defined by a choice rule of the form, e.g., {p(X)} :- q(X). However, if this rule is replaced by
{p(X)} :- dom(X), where dom denotes the domain, and the constraint :- p(X),not q(X),dom(X)
is added, q is still a bound for p, but this is not detected by gringo, as can be seen in Table 5.
The grounder of the dlv system (Perri et al., 2007) may derive bounds by reasoning on the
propositional level. As we explain below, the order in which rules and constraints are grounded is of
crucial importance for such a method to pay off. Since dlv grounds rules before constraints, using
a constraint to state that q is a bound for p does not improve grounding time.
Propagation on the Propositional Level One of the techniques to produce smaller groundings
consists of applying a constraint propagation method on the ground theory Tg and replacing by
>, respectively , every ground literal that is derived to be true, respectively false. The resulting
theory is then simplified. This technique is applied by the grounder psgrnd (East et al., 2006), which
uses unit propagation (Davis & Putnam, 1960) and complete one-atom lookahead (Li & Anbulagan,
1997) as propagation methods. The latter is performed once the grounding is finished, the former
is triggered each time a unit clause is added to the grounding. If an inconsistency is detected by
unit propagation, the grounding process is terminated immediately. Observe that this technique
yields small groundings but does not improve grounding speed, except for the (rare) case where the
propagation method detects an inconsistency during grounding. Indeed, it does not avoid computing
all ground instances of the formulas in the input theory.
If a propositional constraint propagation method is applied while the grounding is being constructed, the derived information could be used to refine bounds. For instance, if unit-propagation
derives that the domain atom P (d1 , . . . , dn ) is true, then x1 = d1  . . .  xn = dn is a ct-bound for
P (x1 , . . . , xn ). These bounds could be used to speed up the construction of the rest of the grounding.
For this method to be effective, however, some careful fine-tuning of the order in which sentences
are grounded is required. It may even be necessary to alternatingly compute partial groundings of
different sentences. To the best of our knowledge, this process has not been worked out or implemented with unit-propagation or one-atom lookahead as underlying propagation method. On the
other hand, most ASP grounders apply it for the following limited propagation method: if all rules
defining a predicate P are grounded, it is concluded that a domain atom P (d) is certainly true if
it occurs in a ground rule of the form P (d)  >, and certainly false if it does not occur in the
head of any ground rule. In this case, a good grounding order can be derived from the dependency
graph of the input theory (e.g., Cadoli & Schaerf, 2005; Perri et al., 2007). In GidL, this strategy
is implemented for grounding definitions.
Sharing A second technique is called sharing and consists of detecting subformulas in the ground
theory Tg that occur more than once. If such a subformula  is detected, all its occurrences in Tg
are replaced by a new atom P , and the sentence P   is added. If  is a large formula and occurs
263

fiWittocx, Marien, & Denecker

often in Tg , this may result in a significant grounding size reduction. Also, sharing improves the
propagation in SAT solvers.
Shlyakhter, Sridharan, Seater, and Jackson (2003) present an algorithm to detect identical subformulas on the first-order level, Torlak and Jackson (2007) for the propositional level. In GidL,
we implemented a simple sharing technique using dynamic
V programming. We adapted function
groundConj soVthat instead of returning a conjunction C, it creates a new atom P , adds the
sentence P  C to the grounding, and returns P . If groundConj is applied
multiple times on
V
the same input , the same predicate P is returned each time, but P  C is added only once.
Function groundDisj is adapted in a similar fashion.
Clause splitting Clause splitting is a well-known rewriting technique applied in MACE style
model generation (McCune, 2003). It consists of splitting a first-order clause
xyz (1 [x, z 1 ]  2 [y, z 2 ])

(20)

where x 6 z 2 , y 6 z 1 and z = z 1  z 2 into two new clauses
xz 1 (1 [x, z 1 ]  S(z 1  z 2 ))

(21)

yz 2 (S(z 1  z 2 )  2 [y, z 2 ]).

(22)

Here, S is a new predicate symbol. The full grounding of (20) is of the size O(|D|3 ), while the full
grounding of (21) and (22) has only size O(|D|2 ).
If sharing is implemented by adapting the functions groundConj and groundDisj as explained
above, the effect of clause splitting can be obtained by moving quantifiers according to the equivalences (4), (5), (8) and (9) of Section 2.2. For instance, we can apply equivalences (4) and (8) to
replace (20) by xz (1  (y 2 )). Grounding the latter while applying sharing has the same effect
as clause splitting. Similarly, the grounding size of xyz (1 [x, z 1 ]  2 [y, z 2 ]) can be reduced by
replacing this formula by xz (1  (y 2 )).
The simple heuristic to guide clause splitting described by Claessen and Sorensson (2003) can
directly be applied to choose which quantifiers to move inside. We conclude that clause splitting
could easily be incorporated in GidL.
Database Techniques Several techniques for optimizing querying in databases can be used to
optimize grounding. Examples are join-ordering strategies, backjumping and indexing techniques.
One of the most basic techniques to improve grounding speed consists of reordering (long) conjunctions or disjunctions of literals to speed up grounding. Which order is best depends on the
grounding algorithm. Different strategies are described by, e.g, Leone, Perri, and Scarcello (2001),
Syrjanen (1998, 2009) and in the database literature (Garcia-Molina, Ullman, & Widom, 2000).
There is no problem implementing a similar technique in GidL. Also, reordering the nodes in the
BDD representation of the bounds could optimize querying. It is part of future work to investigate
such reordering strategies for BDDs.
One of the important methods in the dlv grounder is the use of a backjumping technique (Perri
et al., 2007) to efficiently find all instances of a conjunction 1  . . .  n that are possibly true,
given (an overestimation of) the possibly true instances of each of the conjuncts i . In GidL, this
backjumping technique is applied to implement line 12 of function groundDisj. Indeed, if  is
the formula 1  . . .  n , then line 12 amounts to finding all possible instances of , while the
cf-bounds for 1 , . . . , n provide an overestimation of the possibly true instances of these conjuncts.
Similarly, the backjumping technique is applied to improve line 12 of groundConj, where all possibly
false instances of a disjunction are calculated.
Catalano, Leone, and Perri (2008) present an adaptation of indexing strategies for grounding.
Partition-Based Reasoning Ramachandran and Amir (2005) describe a sophisticated grounding
technique that can reduce the grounding size of FO theories, depending on the availability of some
264

fiGrounding FO and FO(ID) with Bounds

graphical structure in these theories. This technique is not directly applicable in our case, since it
produces groundings that are not necessarily I -equivalent to the input theory. The only guarantee
is that the ground theory is satisfiable iff the input problem is satisfiable.
7.2 Grounders
A non-native approach to ground an MX(FO(ID)) problem consists of first translating it to an
equivalent normal logic program under the well-founded semantics. This translation is described
by Marien et al. (2004). Next, a (slightly adapted) grounder for ASP is used to ground the logic
program. This is the approach taken by MXidL (Marien, Wittocx, & Denecker, 2006).
The first native grounding algorithm for MX(FO) and MX(FO(ID)) was described by Patterson, Liu, Ternovska, and Gupta (2007). It is based on relational algebra and takes a bottom-up
approach (see Section 3.2.1). To construct a grounding of a sentence , it first creates all possible
groundings of the atomic subformulas. Then it combines these groundings using relational algebra
operations, working its way up the syntax tree. Finally, a grounding for  is obtained. Mitchell
et al. (2006) report on an implementation, called mxg, of the algorithm.
kodkod (Torlak & Jackson, 2007) is an MX grounder for a syntactic variant of FO. Like mxg,
it works in a bottom-up way. It represents intermediate groundings by (sparse) matrices. One
of the features of kodkod is that it allows a user to give part of a solution to an MX problem
as a three-valued structure. Specifically, the user can force that some atoms P (d), where P is an
expansion predicate, are certainly true (or certainly false). kodkod then takes advantage of this
information to produce smaller groundings. GidL also allows for a three-valued structure as input.
When applying the refinement algorithm, the set of tuples d for which the user indicates that P
should be true is then used as initial ct-bound for P instead of . Similarly for the cf-bound. This
leads to more efficient and compact groundings.
mace (McCune, 2003) and paradox (Claessen & Sorensson, 2003) are finite model generators
for FO. They work by choosing a domain and grounding the input theory to SAT. If the resulting
grounding is unsatisfiable, the domain size is increased and the process is repeated. The grounding
algorithm in mace and paradox basically constructs the full grounding and simplifies it afterwards.
Small groundings are obtained by first rewriting the input theory using, e.g., clause splitting. Also
methods that build the grounding incrementally are applied in these systems to avoid recomputing
every grounding from scratch.
East et al. (2006) developed the grounder psgrnd for M X(P S pb ). P S pb is a fragment of FO(ID),
extended with pseudo-boolean constraints. As explained above, psgrnd performs reasoning on the
ground theory to reduce memory usage and grounding size. The experiments performed by East
et al. (2006) show that carefully designed data structures are of key importance to build an efficient
grounder.
ASP grounders take as input a normal logic program and transform it into an equivalent ground
normal logic program. As such, these grounders do not deal with (deeply) nested formulas. Currently, there are three ASP grounders: lparse (Syrjanen, 2000; Syrjanen, 2009), gringo (Gebser
et al., 2007) and the grounding component of dlv (Perri et al., 2007). All of them use techniques
from database theory to perform grounding efficiently.
Finally, we mention the grounder spec2SAT (Cadoli & Schaerf, 2005). Its input theories are
in the np-spec language, a language with Datalog-like syntax and semantics based on model minimality. The grounding algorithm implemented in spec2SAT is basically a simplified version of the
grounding algorithm of dlv.
It would be interesting to compare the efficiency of the above mentioned grounders experimentally. However, it is currently not possible to conduct such an experiment in a scientifically fair way.
There are several reasons for this. First, all grounders have a different input language, making it
impossible to run them on the same input. Also, there are several output languages for grounders.
A richer output language leads to more compact and fast grounding. For instance, for some prob-

265

fiWittocx, Marien, & Denecker

lems, lparses output size is necessarily cubic in the input domain size, while GidLs output format
allows for quadratic size. Thirdly, even if the input and output languages of all grounders were the
same, an expert could easily manipulate experiments by carefully choosing his modelling style. For
example, if he does not manually add bounds to the input theories, GidL has an advantage. If
bodies of rules are not ordered, dlv is more likely to produce good results. Etc. Finally, because
of the large amount of data processed by grounders, carefully designed data structures and an optimized implementation of the core grounding algorithm is very important to achieve fast grounding
(East et al., 2006). However, several of the above mentioned grounders are not yet optimized in that
sense. As such, it is difficult to derive conclusions about grounding algorithms by experimentally
comparing the efficiency of current implementations of these algorithms.

8. Conclusions
We presented a method to compute for a given theory, upper and lower bounds for all subformulas
of that theory. We showed how these bounds can be used for efficiently creating small groundings
in the context of Model Expansion for FO and FO(ID). Our method frees a user from manually
discovering bounds and adding them to a theory.
We presented a top-down style grounding algorithm that incorporates bounds. We discussed
implementation issues and showed by experiments that our method works in practice: on many
benchmark problems, it leads to significant reductions in grounding size and time.
Future work includes the extension of our algorithm to compute bounds for richer logics, such as,
e.g., extensions of FO with aggregates and arithmetic. On the implementation side, we plan to use
more sophisticated estimators to evaluate whether a computed bound is beneficial for grounding.

Acknowledgments
Research supported by Research Foundation-Flanders (FWO-Vlaanderen) and by GOA 2003/08
Inductive Knowledge Bases. Johan Wittocx is research assistant of the Research FoundationFlanders (FWO-Vlaanderen).

References
Baral, C., Brewka, G., & Schlipf, J. S. (Eds.). (2007). Logic Programming and Nonmonotonic
Reasoning, 9th International Conference, LPNMR 2007, Tempe, AZ, USA, May 15-17, 2007,
Proceedings, Vol. 4483 of Lecture Notes in Computer Science. Springer.
Bryant, R. E. (1986). Graph-based algorithms for boolean function manipulation. IEEE Transactions
on Computers, 35, 677691.
Cadoli, M., & Schaerf, A. (2005). Compiling problem specifications into SAT. Artificial Intelligence,
162 (1-2), 89120.
Catalano, G., Leone, N., & Perri, S. (2008). On demand indexing for the DLV instantiator. In
Faber, W., & Lee, J. (Eds.), Workshop on Answer Set Programming and Other Computing
Paradigms (ASPOCP).
Claessen, K., & Sorensson, N. (2003). New techniques that improve MACE-style model finding. In
Workshop on Model Computation (MODEL).
Davis, M., & Putnam, H. (1960). A computing procedure for quantification theory. Journal of the
ACM, 7 (3), 201215.

266

fiGrounding FO and FO(ID) with Bounds

Demolombe, R. (1980). Estimation of the number of tuples satisfying a query expressed in predicate
calculus language. In International Conference on Very Large Data Bases (VLDB), pp. 5563.
IEEE Computer Society.
Denecker, M. (2000). Extending classical logic with inductive definitions. In Lloyd, J. W., Dahl, V.,
Furbach, U., Kerber, M., Lau, K.-K., Palamidessi, C., Pereira, L. M., Sagiv, Y., & Stuckey,
P. J. (Eds.), International Conference on Computational Logic (CL), Vol. 1861 of Lecture Notes
in Computer Science, pp. 703717. Springer.
Denecker, M., & Ternovska, E. (2007). Inductive situation calculus. Artificial Intelligence, 171 (5-6),
332360.
Denecker, M., & Ternovska, E. (2008). A logic of nonmonotone inductive definitions. ACM Transactions on Computational Logic (TOCL), 9 (2), Article 14.
Denecker, M., & Vennekens, J. (2007). Well-founded semantics and the algebraic theory of nonmonotone inductive definitions.. In Baral et al. (Baral, Brewka, & Schlipf, 2007), pp. 8496.
Denecker, M., Vennekens, J., Bond, S., Gebser, M., & Truszczynski, M. (2009). The second answer
set programming competition. In Erdem, E., Lin, F., & Schaub, T. (Eds.), International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR), Vol. 5753 of Lecture
Notes in Computer Science, pp. 637654. Springer.
East, D., Iakhiaev, M., Mikitiuk, A., & Truszczynski, M. (2006). Tools for modeling and solving
search problems. AI Communications, 19 (4), 301312.
Enderton, H. B. (2001). A Mathematical Introduction To Logic (Second edition). Academic Press.
Fagin, R. (1974). Generalized first-order spectra and polynomial-time recognizable sets. Complexity
of Computation, 7, 4374.
Forgy, C. (1982). Rete: A fast algorithm for the many patterns/many objects match problem.
Artificial Intelligence, 19 (1), 1737.
Garcia-Molina, H., Ullman, J. D., & Widom, J. (2000). Database System Implementation. PrenticeHall.
Gebser, M., Schaub, T., & Thiele, S. (2007). GrinGo : A new grounder for answer set programming..
In Baral et al. (Baral et al., 2007), pp. 266271.
Goubault, J. (1995). A BDD-based simplification and skolemization procedure. Logic Journal of
IGPL, 3 (6), 827855.
Jackson, D. (2006). Software Abstractions: Logic, Language, and Analysis. MIT Press, Cambridge,
MA.
Kautz, H. A., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic and stochastic search. In National Conference on Artificial Intelligence and Innovative Applications of
Artificial Intelligence (AAAI/IAAI), pp. 11941201. AAAI Press.
Krogel, M.-A., Rawles, S., Zelezny, F., Flach, P. A., Lavrac, N., & Wrobel, S. (2003). Comparative
evaluation of approaches to propositionalization. In Horvath, T. (Ed.), International Conference on Inductive Logic Programming (ILP), Vol. 2835 of Lecture Notes in Computer Science,
pp. 197214. Springer.
Leone, N., Perri, S., & Scarcello, F. (2001). Improving ASP instantiators by join-ordering methods.
In Eiter, T., Faber, W., & Truszczynski, M. (Eds.), International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR), Vol. 2173 of Lecture Notes in Computer
Science, pp. 280294. Springer.
Li, C. M., & Anbulagan (1997). Heuristics based on unit propagation for satisfiability problems.
In International Joint Conference on Artificial Intelligence (IJCAI), pp. 366371. Morgan
Kaufman.
267

fiWittocx, Marien, & Denecker

Marek, V. W., & Truszczynski, M. (1999). Stable models and an alternative logic programming
paradigm. In Apt, K. R., Marek, V. W., Truszczynski, M., & Warren, D. S. (Eds.), The Logic
Programming Paradigm: a 25-Year Perspective, pp. 375398. Springer-Verlag.
Marien, M. (2009). Model Generation for ID-Logic. Ph.D. thesis, Department of Computer Science,
K.U.Leuven, Leuven, Belgium.
Marien, M., Gilis, D., & Denecker, M. (2004). On the relation between ID-Logic and Answer Set
Programming.. In Alferes, J. J., & Leite, J. A. (Eds.), European Conference on Logics in
Artificial Intelligence (JELIA), Vol. 3229 of Lecture Notes in Computer Science, pp. 108120.
Springer.
Marien, M., Wittocx, J., & Denecker, M. (2006). The IDP framework for declarative problem solving.
In Search and Logic: Answer Set Programming and SAT, pp. 1934.
Marien, M., Wittocx, J., & Denecker, M. (2007). MidL: a SAT(ID) solver. In 4th Workshop on
Answer Set Programming: Advances in Theory and Implementation, pp. 303308.
Marien, M., Wittocx, J., Denecker, M., & Bruynooghe, M. (2008). SAT(ID): Satisfiability of propositional logic extended with inductive definitions. In Kleine Buning, H., & Zhao, X. (Eds.),
International Conference on Theory and Applications of Satisfiability Testing (SAT), Vol. 4996
of Lecture Notes in Computer Science, pp. 211224. Springer.
McCune, W. (2003). Mace4 reference manual and guide. CoRR, cs.SC/0310055.
Mitchell, D. G., & Ternovska, E. (2005). A framework for representing and solving NP search
problems.. In Veloso, & Kambhampati (Veloso & Kambhampati, 2005), pp. 430435.
Mitchell, D. G., Ternovska, E., Hach, F., & Mohebali, R. (2006). Model expansion as a framework
for modelling and solving search problems. Tech. rep. TR 2006-24, Simon Fraser University,
Canada.
Niemela, I. (1999). Logic programs with stable model semantics as a constraint programming
paradigm. Annals of Mathematics and Artificial Intelligence, 25 (3-4), 241273.
Patterson, M., Liu, Y., Ternovska, E., & Gupta, A. (2007). Grounding for model expansion in
k-guarded formulas with inductive definitions. In Veloso, M. M. (Ed.), International Joint
Conference on Artificial Intelligence (IJCAI), pp. 161166.
Pelov, N., & Ternovska, E. (2005). Reducing inductive definitions to propositional satisfiability. In
Gabbrielli, M., & Gupta, G. (Eds.), International Conference on Logic Programming (ICLP),
Vol. 3668 of Lecture Notes in Computer Science, pp. 221234. Springer.
Perri, S., Scarcello, F., Catalano, G., & Leone, N. (2007). Enhancing DLV instantiator by backjumping techniques. Annals of Mathematics and Artificial Intelligence, 51 (2-4), 195228.
Ramachandran, D., & Amir, E. (2005). Compact propositional encodings of first-order theories.. In
Veloso, & Kambhampati (Veloso & Kambhampati, 2005), pp. 340345.
Schulz, S. (2002). A comparison of different techniques for grounding near-propositional cnf formulae.
In Haller, S. M., & Simmons, G. (Eds.), International Florida Artificial Intelligence Research
Society Conference (FLAIRS), pp. 7276. AAAI Press.
Shlyakhter, I., Sridharan, M., Seater, R., & Jackson, D. (2003). Exploiting subformula sharing in
automatic analysis of quantified formulas. Poster presented at Theory and Applications of
Satisfiability Testing (SAT), 6th International Conference.
Stockmeyer, L. J. (1974). The complexity of decision problems in automata and logic. Ph.D. thesis,
Massachusetts Institute of Technology.
Syrjanen, T. (1998). Implementation of local grounding for logic programs with stable model semantics. Tech. rep. B18, Helsinki University of Technology, Finland.

268

fiGrounding FO and FO(ID) with Bounds

Syrjanen, T. (2000).
lparse.ps.gz.

Lparse 1.0 users manual. http://www.tcs.hut.fi/Software/smodels/

Syrjanen, T. (2009). Logic Programs and Cardinality Constraints: Theory and Practice. Doctoral
dissertation, TKK Dissertations in Information and Computer Science TKK-ICS-D12, Helsinki
University of Technology, Faculty of Information and Natural Sciences, Department of Information and Computer Science, Espoo, Finland.
Torlak, E., & Jackson, D. (2007). Kodkod: A relational model finder. In Grumberg, O., & Huth, M.
(Eds.), International Conference on Tools and Algorithms for the Construction and Analysis
of Systems (TACAS), Vol. 4424 of Lecture Notes in Computer Science, pp. 632647. Springer.
Ullman, J. D. (1988). Principles of database and knowledge-base systems, Vol. I. Computer Science
Press, Inc., New York, NY, USA.
Van Gelder, A., Ross, K. A., & Schlipf, J. S. (1991). The well-founded semantics for general logic
programs. Journal of the ACM, 38 (3), 620650.
Veloso, M. M., & Kambhampati, S. (Eds.). (2005). Proceedings, The Twentieth National Conference
on Artificial Intelligence and the Seventeenth Innovative Applications of Artificial Intelligence
Conference, July 9-13, 2005, Pittsburgh, Pennsylvania, USA. AAAI Press / The MIT Press.
Wittocx, J., & Marien, M. (2008). The idp system. http://www.cs.kuleuven.be/~dtai/krr/
software/idpmanual.pdf.
Wittocx, J., Marien, M., & Denecker, M. (2008a). Approximate reasoning in first-order logic theories.
In Brewka, G., & Lang, J. (Eds.), International Conference on Knowledge Representation and
Reasoning (KR), pp. 103112. AAAI Press.
Wittocx, J., Marien, M., & Denecker, M. (2008b). GidL: A grounder for FO+ . In Pagnucco, M., &
Thielscher, M. (Eds.), Workshop on Nonmonotonic Reasoning (NMR), pp. 189198. University
of New South Wales.
Wittocx, J., Marien, M., & Denecker, M. (2008c). Grounding with bounds. In Fox, D., & Gomes,
C. P. (Eds.), AAAI Conference on Artificial Intelligence, pp. 572577. AAAI Press.
Wittocx, J., Marien, M., & Denecker, M. (2008d). The idp system: a model expansion system for
an extension of classical logic. In Workshop on Logic and Search (LaSh), pp. 153165.

269

fiJournal of Artificial Intelligence Research 38 (2010) 475-511

Submitted 04/10; published 08/10

A Minimum Relative Entropy Principle
for Learning and Acting
Pedro A. Ortega
Daniel A. Braun

peortega@dcc.uchile.cl
dab54@cam.ac.uk

Department of Engineering
University of Cambridge
Cambridge CB2 1PZ, UK

Abstract
This paper proposes a method to construct an adaptive agent that is universal with
respect to a given class of experts, where each expert is designed specifically for a particular
environment. This adaptive control problem is formalized as the problem of minimizing
the relative entropy of the adaptive agent from the expert that is most suitable for the
unknown environment. If the agent is a passive observer, then the optimal solution is the
well-known Bayesian predictor. However, if the agent is active, then its past actions need
to be treated as causal interventions on the I/O stream rather than normal probability
conditions. Here it is shown that the solution to this new variational problem is given
by a stochastic controller called the Bayesian control rule, which implements adaptive
behavior as a mixture of experts. Furthermore, it is shown that under mild assumptions,
the Bayesian control rule converges to the control law of the most suitable expert.

1. Introduction
When the behavior of an environment under any control signal is fully known, then the
designer can choose an agent that produces the desired dynamics. Instances of this problem include hitting a target with a cannon under known weather conditions, solving a maze
having its map and controlling a robotic arm in a manufacturing plant. However, when
the environment is unknown, then the designer faces the problem of adaptive control. For
example, shooting the cannon lacking the appropriate measurement equipment, finding the
way out of an unknown maze and designing an autonomous robot for Martian exploration.
Adaptive control turns out to be far more difficult than its non-adaptive counterpart. This
is because any good policy has to carefully trade off explorative versus exploitative actions,
i.e. actions for the identification of the environments dynamics versus actions to control it
in a desired way. Even when the environments dynamics are known to belong to a particular class for which optimal agents are available, constructing the corresponding optimal
adaptive agent is in general computationally intractable even for simple toy problems (Duff,
2002). Thus, finding tractable approximations has been a major focus of research.
Recently, it has been proposed to reformulate the problem statement for some classes of
control problems based on the minimization of a relative entropy criterion. For example, a
large class of optimal control problems can be solved very efficiently if the problem statement
is reformulated as the minimization of the deviation of the dynamics of a controlled system
from the uncontrolled system (Todorov, 2006, 2009; Kappen, Gomez, & Opper, 2010). In
this work, a similar approach is introduced for adaptive control. If a class of agents is
c
2010
AI Access Foundation. All rights reserved.

fiOrtega & Braun

given, where each agent is tailored to a different environment, then adaptive controllers can
be derived from a minimum relative entropy principle. In particular, one can construct an
adaptive agent that is universal with respect to this class by minimizing the average relative
entropy from the environment-specific agent.
However, this extension is not straightforward. There is a syntactical difference between
actions and observations that has to be taken into account when formulating the variational
problem. More specifically, actions have to be treated as interventions obeying the rules of
causality (Pearl, 2000; Spirtes, Glymour, & Scheines, 2000; Dawid, 2010). If this distinction
is made, the variational problem has a unique solution given by a stochastic control rule
called the Bayesian control rule. This control rule is particularly interesting because it
translates the adaptive control problem into an on-line inference problem that can be applied
forward in time. Furthermore, this work shows that under mild assumptions, the adaptive
agent converges to the environment-specific agent.
The paper is organized as follows. Section 2 introduces notation and sets up the adaptive
control problem. Section 3 formulates adaptive control as a minimum relative entropy
problem. After an initial, nave approach, the need for causal considerations is motivated.
Then, the Bayesian control rule is derived from a revised relative entropy criterion. In
Section 4, the conditions for convergence are examined and a proof is given. Section 5
illustrates the usage of the Bayesian control rule for the multi-armed bandit problem and
undiscounted Markov decision processes. Section 6 discusses properties of the Bayesian
control rule and relates it to previous work in the literature. Section 7 concludes.

2. Preliminaries
In the following both agent and environment are formalized as causal models over I/O
sequences. Agent and environment are coupled to exchange symbols following a standard
interaction protocol having discrete time, observation and control signals. The treatment
of the dynamics are fully probabilistic, and in particular, both actions and observations are
random variables, which is in contrast to the typical decision-theoretic agent formulation
treating only observations as random variables (Russell & Norvig, 2010). All proofs are
provided in the appendix.
Notation. A set is denoted by a calligraphic letter like A. The words set & alphabet
and element & symbol are used to mean the same thing respectively. Strings are finite
concatenations of symbols and sequences are infinite
concatenations. An denotes the set
S

of strings of length n based on A, and A := n0 An is the set of finite strings. Furthermore, A := {a1 a2 . . . |ai  A for all i = 1, 2, . . .} is defined as the set of one-way
infinite sequences based on the alphabet A. Tuples are written with parentheses (a1 , a2 , a3 )
or as strings a1 a2 a3 . The notation ai := a1 a2 . . . ai is a shorthand for a string starting from the first index. Also, symbols are underlined to glue them together like ao in
aoi := a1 o1 a2 o2 . . . ai oi . The function log(x) is meant to be taken w.r.t. base 2, unless
indicated otherwise.
Interactions. The possible I/O symbols are drawn from two finite sets. Let O denote the
set of inputs (observations) and let A denote the set of outputs (actions). The set Z := AO
is the interaction set. A string aot or ao<t at is an interaction string (optionally ending in
476

fiA Minimum Relative Entropy Principle for Learning and Acting

at or ot ) where ak  A and ok  O. Similarly, a one-sided infinite sequence a1 o1 a2 o2 . . . is
an interaction sequence. The set of interaction strings of length t is denoted by Z t . The
sets of (finite) interaction strings and sequences are denoted as Z  and Z  respectively.
The interaction string of length 0 is denoted by .
I/O System. Agents and environments are formalized as I/O systems. An I/O system
is a probability distribution Pr over interaction sequences Z  . Pr is uniquely determined
by the conditional probabilities
Pr(at |ao<t ),

Pr(ot |ao<t at )

(1)

for each aot  Z  . These conditional probabilities can either represent a generative law
(propensity) in case of issuing a symbol or an evidential probability (plausibility) in the
case of observing a symbol. Which of the two interpretations applies in a particular case
becomes apparent once the I/O system is coupled to another I/O system.

Agent
P

a1 o1 a2 o2 a3 o3 a4 o4 a5 o5

Environment
Q

Figure 1: The model of interactions. The agent P and the environment Q define a probability distribution over interaction sequences.

Interaction System. Let P, Q be two I/O systems. An interaction system (P, Q) is a
coupling of the two systems giving rise to the generative distribution G that describes the
probabilities that actually govern the I/O stream once the two systems are coupled. G is
specified by the equations
G(at |ao<t ) := P(at |ao<t )
G(ot |ao<t at ) := Q(ot |ao<t at )
valid for all aot  Z  . Here, G models the true probability distribution over interaction
sequences that arises by coupling two systems through their I/O streams. More specifically,
for the system P, P(at |ao<t ) is the probability of producing action at  A given history
ao<t and P(ot |ao<t at ) is the predicted probability of the observation ot  O given history
477

fiOrtega & Braun

ao<t at . Hence, for P, the sequence o1 o2 . . . is its input stream and the sequence a1 a2 . . .
is its output stream. In contrast, the roles of actions and observations are reversed in the
case of the system Q. Thus, the sequence o1 o2 . . . is its output stream and the sequence
a1 a2 . . . is its input stream. The previous model of interaction is fairly general, and many
other interaction protocols can be translated into this scheme. As a convention, given an
interaction system (P, Q), P is an agent to be constructed by the designer, and Q is an
environment to be controlled by the agent. Figure 1 illustrates this setup.
Control Problem. An environment Q is said to be known iff the agent P has the property
that for any aot  Z  ,
P(ot |ao<t at ) = Q(ot |ao<t at ).
Intuitively, this means that the agent knows the statistics of the environments future
behavior under any past, and in particular, it knows the effects of given controls. If the
environment is known, then the designer of the agent can build a custom-made policy into
P such that the resulting generative distribution G produces interaction sequences that are
desirable. This can be done in multiple ways. For instance, the controls can be chosen
such that the resulting policy maximizes a given utility criterion; or such that the resulting
trajectory of the interaction system stays close enough to a prescribed trajectory. Formally,
if Q is known, and if the conditional probabilities P(at |ao<t ) for all aot  Z  have been
chosen such that the resulting generative distribution G over interaction sequences given
by
G(at |ao<t ) = P(at |ao<t )

G(ot |ao<t at ) = Q(ot |ao<t at ) = P(ot |ao<t at )
is desirable, then P is said to be tailored to Q.
Adaptive Control Problem. If the environment Q is unknown, then the task of designing an appropriate agent P constitutes an adaptive control problem. Specifically, this
work deals with the case when the designer already has a class of agents that are tailored
to the class of possible environments. Formally, it is assumed that Q is going to be drawn
with probability P (m) from a set Q := {Qm }mM of possible systems before the interaction starts, where M is a countable set. Furthermore, one has a set P := {Pm }mM
of systems such that for each m  M, Pm is tailored to Qm and the interaction system
(Pm , Qm ) has a generative distribution Gm that produces desirable interaction sequences.
How can the designer construct a system P such that its behavior is as close as possible to
the custom-made system Pm under any realization of Qm  Q?

3. Adaptive Systems
The main goal of this paper is to show that the problem of adaptive control outlined in
the previous section can be reformulated as a universal compression problem. This can be
informally motivated as follows. Suppose the agent P is implemented as a machine that is
interfaced with the environment Q. Whenever the agent interacts with the environment,
the agents state changes as a necessary consequence of the interaction. This change in
state can take place in many possible ways: by updating the internal memory; consulting
478

fiA Minimum Relative Entropy Principle for Learning and Acting

a random number generator; changing the physical location and orientation; and so forth.
Naturally, the design of the agent facilitates some interactions while it complicates others.
For instance, if the agent has been designed to explore a natural environment, then it might
incur into a very low memory footprint when recording natural images, while being very
memory-inefficient when recording artificially created images. If one abstracts away from
the inner workings of the machine and decides to encode the state transitions as binary
strings, then the minimal amount of resources in bits that are required to implement these
state changes can be derived directly from the associated probability distribution P. In
the context of adaptive control, an agent can be constructed such that it minimizes the
expected amount of changes necessary to implement the state transitions, or equivalently,
such that it maximally compresses the experience. Thereby, compression can be taken as a
stand-alone principle to design adaptive agents.
3.1 Universal Compression and Nave Construction of Adaptive Agents
In coding theory, the problem of compressing a sequence of observations from an unknown
source is known as the adaptive coding problem. This is solved by constructing universal compressors, i.e. codes that adapt on-the-fly to any source within a predefined class
(MacKay, 2003). Such codes are obtained by minimizing the average deviation of a predictor from the true source, and then by constructing codewords using the predictor. In this
subsection, this procedure will be used to derive an adaptive agent (Ortega & Braun, 2010).
Formally, the deviation of a predictor P from the true distribution Pm is measured
by the relative entropy 1 . A first approach would be to construct an agent B so as to
minimize the total expected relative entropy to Pm . This is constructed as follows. Define
the history-dependent relative entropies over the action at and observation ot as
X
Pm (at |ao<t )
at
(ao<t ) :=
Dm
Pm (at |ao<t ) log
Pr(at |ao<t )
a
t

ot
(ao<t at ) :=
Dm

X
ot

Pm (ot |ao<t at ) log

Pm (ot |ao<t at )
,
Pr(ot |ao<t at )

where Pm (ot |ao<t at ) = Qm (ot |ao<t at ) because the Qm are known and where Pr will be the
argument of the variational problem. Then, one removes the dependency on the past by
averaging over all possible histories:
X
at
at
:=
(ao<t )
Pm (ao<t )Dm
Dm
ao<t

ot
Dm

:=

X

ot
(ao<t at ).
Pm (ao<t at )Dm

ao<t at

Finally, the total expected relative entropy of Pr from Pm is obtained by summing up all
time steps and then by averaging over all choices of the true environment:
D := lim sup
t

X

P (m)

m

t
X
 =1


o
a
.
+ Dm
Dm

(2)

1. The relative entropy is also known as the KL-divergence and it measures the average amount of extra
bits that are necessary to encode symbols due to the usage of the (wrong) predictor.

479

fiOrtega & Braun

Using (2), one can define a variational problem with respect to Pr. The agent B that one
is looking for is the system Pr that minimizes the total expected relative entropy in (2), i.e.
B := arg min D(Pr).
Pr

The solution to Equation 3 is the system B defined by the set of equations
X
B(at |ao<t ) =
Pm (at |ao<t )wm (ao<t )
m

B(ot |ao<t at ) =

X
m

Pm (ot |ao<t at )wm (ao<t at )

(3)

(4)

valid for all aot  Z  , where the mixture weights are
P (m)Pm (ao<t )

m P (m )Pm (ao<t )
P (m)Pm (ao<t at )
.
wm (ao<t at ) := P

m P (m )Pm (ao<t at )
wm (ao<t ) := P

(5)

For reference, see the work of Haussler and Opper (1997) and Opper (1998). It is clear
that B is just the Bayesian mixture over the agents Pm . If one defines the conditional
probabilities
P (at |m, ao<t ) := Pm (at |ao<t )
(6)
P (ot |m, ao<t at ) := Pm (at |ao<t at )

for all aot  Z  , then Equation 4 can be rewritten as
B(at |ao<t ) =
B(ot |ao<t at ) =

X
m

X
m

P (at |m, ao<t )P (m|ao<t ) = P (at |ao<t )
P (ot |m, ao<t at )P (m|ao<t at ) = P (ot |ao<t at )

(7)

where the P (m|ao<t ) = wm (ao<t ) and P (m|ao<t at ) = wm (ao<t at ) are just the posterior
probabilities over the elements in M given the past interactions. Hence, the conditional
probabilities in (4) that minimize the total expected divergence are just the predictive
distributions P (at |ao<t ) and P (ot |ao<t at ) that one obtains by standard probability theory,
and in particular, Bayes rule. This is interesting, as it provides a teleological interpretation
for Bayes rule.
The behavior of B can be described as follows. At any given time t, B maintains a
mixture over systems Pm . The weighting over them is given by the mixture coefficients
wm . Whenever a new action at or a new observation ot is produced (by the agent or
the environment respectively), the weights wm are updated according to Bayes rule. In
addition, B issues an action at suggested by a system Pm drawn randomly according to the
weights wt .
However, there is an important problem with B that arises due to the fact that it is not
only a system that is passively observing symbols, but also actively generating them. In
the subjective interpretation of probability theory, conditionals play the role of observations
480

fiA Minimum Relative Entropy Principle for Learning and Acting

made by the agent that have been generated by an external source. This interpretation suits
the symbols o1 , o2 , o3 , . . . because they have been issued by the environment. However, symbols that are generated by the system itself require a fundamentally different belief update.
Intuitively, the difference can be explained as follows. Observations provide information
that allows the agent inferring properties about the environment. In contrast, actions do
not carry information about the environment, and thus have to be incorporated differently
into the belief of the agent. In the following section we illustrate this problem with a simple
statistical example.
3.2 Causality
Causality is the study of the functional dependencies of events. This stands in contrast to
statistics, which, on an abstract level, can be said to study the equivalence dependencies
(i.e. co-occurrence or correlation) amongst events. Causal statements differ fundamentally
from statistical statements. Examples that highlight the differences are many, such as
do smokers get lung cancer? as opposed to do smokers have lung cancer?; assign
y  f (x) as opposed to compare y = f (x) in programming languages; and a  F/m
as opposed to F = m a in Newtonian physics. The study of causality has recently enjoyed
considerable attention from researchers in the fields of statistics and machine learning.
Especially over the last decade, significant progress has been made towards the formal
understanding of causation (Shafer, 1996; Pearl, 2000; Spirtes et al., 2000; Dawid, 2010).
In this subsection, the aim is to provide the essential tools required to understand causal
interventions. For a more in-depth exposition of causality, the reader is referred to the
specialized literature.
To illustrate the need for causal considerations in the case of generated symbols, consider
the following thought experiment. Suppose a statistician is asked to design a model for a
simple time series X1 , X2 , X3 , . . . and she decides to use a Bayesian method. Assume she
collects a first observation X1 = x1 . She computes the posterior probability density function
(pdf) over the parameters  of the model given the data using Bayes rule:
p(|X1 = x1 ) = R

p(X1 = x1 |)p()
,
p(X1 = x1 | )p( ) d

where p(X1 = x1 |) is the likelihood of x1 given  and p() is the prior pdf of . She can
use the model to predict the next observation by drawing a sample x2 from the predictive
pdf
Z
p(X2 = x2 |X1 = x1 ) = p(X2 = x2 |X1 = x1 , ) p(|X1 = x1 ) d,
where p(X2 = x2 |X1 = x1 , ) is the likelihood of x2 given x1 and . Note that x2 is not
drawn from p(X2 = x2 |X1 = x1 , ). She understands that the nature of x2 is very different
from x1 : while x1 is informative and does change the belief state of the Bayesian model,
x2 is non-informative and thus is a reflection of the models belief state. Hence, she would
never use x2 to further condition the Bayesian model. Mathematically, she seems to imply
that
p(|X1 = x1 , X2 = x2 ) = p(|X1 = x1 )
481

fiOrtega & Braun

if x2 has been generated from p(X2 |X1 = x1 ) itself. But this simple independence assumption is not correct as the following elaboration of the example will show.
The statistician is now told that the source is waiting for the simulated data point x2
in order to produce a next observation X3 = x3 which does depend on x2 . She hands in x2
and obtains a new observation x3 . Using Bayes rule, the posterior pdf over the parameters
is now
p(X3 = x3 |X1 = x1 , X2 = x2 , ) p(X1 = x1 |) p()
R
(8)
p(X3 = x3 |X1 = x1 , X2 = x2 ,  ) p(X1 = x1 | ) p( ) d
where p(X3 = x3 |X1 = x1 , X2 = x2 , ) is the likelihood of the new data x3 given the old
data x1 , the parameters  and the simulated data x2 . Notice that this looks almost like the
posterior pdf p(|X1 = x1 , X2 = x2 , X3 = x3 ) given by
R

p(X3 = x3 |X1 = x1 , X2 = x2 , ) p(X2 = x2 |X1 = x1 , ) p(X1 = x1 |) p()
p(X3 = x3 |X1 = x1 , X2 = x2 ,  ) p(X2 = x2 |X1 = x1 ,  ) p(X1 = x1 | ) p( ) d

with the exception that in the latter case, the Bayesian update contains the likelihoods of
the simulated data p(X2 = x2 |X1 = x1 , ). This suggests that Equation 8 is a variant of the
posterior pdf p(|X1 = x1 , X2 = x2 , X3 = x3 ) but where the simulated data x2 is treated
in a different way than the data x1 and x3 .
Define the pdf p such that the pdfs p (), p (X1 |), p (X3 |X1 , X2 , ) are identical to
p(), p(X1 |) and p(X3 |X2 , X1 , ) respectively, but differ in p (X2 |X1 , ):
p (X2 |X1 , ) = (X2  x2 ).
where  is the Dirac delta function. That is, p is identical to p but it assumes that the
value of X2 is fixed to x2 given X1 and . For p , the simulated data x2 is non-informative:
 log2 p (X2 = x2 |X1 , ) = 0.
If one computes the posterior pdf p (|X1 = x1 , X2 = x2 , X3 = x3 ), one obtains the result
of Equation 8:
R

p (X3 = x3 |X1 = x1 , X2 = x2 , ) p (X2 = x2 |X1 = x1 , ) p (X1 = x1 |) p ()
p (X3 = x3 |X1 = x1 , X2 = x2 ,  )p (X2 = x2 |X1 = x1 ,  ) p (X1 = x1 | ) p ( ) d
p(X3 = x3 |X1 = x1 , X2 = x2 , ) p(X1 = x1 |) p()
=R
.
p(X3 = x3 |X1 = x1 , X2 = x2 ,  ) p(X1 = x1 | ) p( ) d

Thus, in order to explain Equation 8 as a posterior pdf given the observed data x1 and x3
and the generated data x2 , one has to intervene p in order to account for the fact that x2
is non-informative given x1 and . In other words, the statistician, by defining the value of
X2 herself2 , has changed the (natural) regime that brings about the series X1 , X2 , X3 , . . .,
which is mathematically expressed by redefining the pdf.
Two essential ingredients are needed to carry out interventions. First, one needs to
know the functional dependencies amongst the random variables of the probabilistic model.
This is provided by the causal model, i.e. the unique factorization of the joint probability
2. Note that this is conceptually broken down into two steps: first, she samples x2 from p(X2 |X1 = x1 );
and second, she imposes the value X2 = x2 by setting p (X2 |X1 , ) = (X2  x2 ).

482

fiA Minimum Relative Entropy Principle for Learning and Acting

distribution over the random variables encoding the causal dependencies. In the general
case, this defines a partial order over the random variables. In the previous thought experiment, the causal model of the joint pdf p(, X1 , X2 , X3 ) is given by the set of conditional
pdfs
p(), p(X1 |), p(X2 |X1 , ), p(X3 |X1 , X2 , ).
Second, one defines the intervention that sets X to the value x, denoted as X  x, as
the operation on the causal model replacing the conditional probability of X by a Dirac
delta function (X  x) or a Kronecker delta xX for a continuous or a discrete variable X
respectively. In our thought experiment, it is easily seen that
p (, X1 = x1 , X2 = x2 , X3 = x3 ) = p(, X1 = x1 , X2  x2 , X3 = x3 )
and thereby,
p (|X1 = x1 , X2 = x2 , X3 = x3 ) = p(|X1 = x1 , X2  x2 , X3 = x3 ).
Causal models contain additional information that is not available in the joint probability
distribution alone. The appropriate model for a given situation depends on the story that
is being told. Note that an intervention can lead to different results if the respective causal
models differ. Thus, if the causal model had been
p(X3 ), p(X2 |X3 ), p(X1 |X2 , X3 ), p(|X1 , X2 , X3 )
then the intervention X2  x2 would differ from p , i.e.
p (, X1 = x1 , X2 = x2 , X3 = x3 ) 6= p(, X1 = x1 , X2  x2 , X3 = x3 ),
even though both causal models represent the same joint probability distribution. In the
following, this paper will use the shorthand notation x := X  x when the random variable
is obvious from the context.
3.3 Causal Construction of Adaptive Agents
Following the discussion in the previous section, an adaptive agent P is going to be constructed by minimizing the expected relative entropy to the expected Pm , but this time
treating actions as interventions. Based on the definition of the conditional probabilities in
Equation 6, the total expected relative entropy to characterize P using interventions is going to be defined. Assuming the environment is chosen first, and that each symbol depends
functionally on the environment and all the previously generated symbols, the causal model
is given by
P (m), P (a1 |m), P (o1 |m, a1 ), P (a2 |m, a1 , o1 ), P (o2 |m, a1 , o1 , a2 ), . . .
Importantly, interventions index a set of intervened probability distributions derived from
a base probability distribution. Hence, the set of fixed intervention sequences of the form
a1 , a2 , . . . indexes probability distributions over observation sequences o1 , o2 , . . .. Because
of this, one defines a set of criteria indexed by the intervention sequences, but it will be
483

fiOrtega & Braun

clear that they all have the same solution. Define the history-dependent intervened relative
entropies over the action at and observation ot as
at
(ao<t ) :=
Cm

X
at

ot
(ao<t at ) :=
Cm

X
ot

P (at |m, ao<t ) log2

P (at |m, ao<t )
Pr(at |ao<t )

P (ot |m, ao<t at ) log2

P (ot |m, ao<t at )
,
Pr(ot |ao<t at )

where Pr is a given arbitrary agent. Note that past actions are treated as interventions. In
particular, P (at |m, ao<t ) represents the knowledge state when the past actions have already
been issued but the next action at is not known yet. Then, averaging the previous relative
entropies over all pasts yields
at
=
Cm

X

ao<t
ot
=
Cm

at
(ao<t )
P (ao<t |m)Cm

X

ao<t at

ot
(ao<t at ).
P (ao<t at |m)Cm

at (ao ) and C ot (ao a ),
Here again, because of the knowledge state in time represented by Cm
<t
<t t
m
the averages are taken treating past actions as interventions. Finally, define the total exat + C ot ) over time, averaged over
pected relative entropy of Pr from Pm as the sum of (Cm
m
the possible draws of the environment:

C := lim sup
t

X

P (m)

m

t
X
 =1


o
a
+ Cm
Cm
.

(9)

The variational problem consists in choosing the agent P as the system Pr minimizing
C = C(Pr), i.e.
P := arg min C(Pr).
(10)
Pr

The following theorem shows that this variational problem has a unique solution, which will
be the central theme of this paper.
Theorem 1. The solution to Equation 10 is the system P defined by the set of equations
X

P(at |ao<t ) = P (at |ao<t ) =

m

P(ot |ao<t at ) = P (ot |ao<t at ) =

P (at |m, ao<t )vm (ao<t )

X
m

P (ot |m, ao<t at )vm (ao<t at )

(11)

valid for all aot  Z  , where the mixture weights are
Qt1

 =1 P (o |m, ao< a )
.
Qt1


 =1 P (o |m , ao< a )
m P (m )

vm (ao<t at ) = vm (ao<t ) := P

P (m)

484

(12)

fiA Minimum Relative Entropy Principle for Learning and Acting

Bayesian Control Rule: Given a set of operation modes {P (|m, )}mM
over interaction sequences in Z  and a prior distribution P (m) over the
parameters M, the probability of the action at+1 is given by
X
P (at+1 |m, aot )P (m|aot ),
(13)
P (at+1 |aot ) =
m

where the posterior probability over operation modes is given by the recursion
P (ot |m, ao<t )P (m|ao<t )
.


m P (ot |m , ao<t )P (m |ao<t )

P (m|aot ) = P

Table 1: Summary of the Bayesian control rule.
The theorem says that the optimal solution to the variational problem in (10) is precisely
the predictive distribution over actions and observations treating actions as interventions
and observations as conditionals, i.e. it is the solution that one would obtain by applying
only standard probability and causal calculus. This provides a teleological interpretation for
the agent P akin to the nave agent B constructed in Section 3.1. The behavior of P differs
in an important aspect from B. At any given time t, P maintains a mixture over systems
Pm . The weighting over these systems is given by the mixture coefficients vm . In contrast
to B, P updates the weights vm only whenever a new observation ot is produced by the
environment. The update follows Bayes rule but treats past actions as interventions by
dropping the evidence they provide. In addition, P issues an action at suggested by an
system m drawn randomly according to the weights vm .
3.4 Summary
Adaptive control is formalized as the problem of designing an agent for an unknown environment chosen from a class of possible environments. If the environment-specific agents are
known, then the Bayesian control rule allows constructing an adaptive agent by combining
these agents. The resulting adaptive agent is universal with respect to the environment
class. In this context, the constituent agents are called the operation modes of the adaptive
agent. They are represented by causal models over the interaction sequences, i.e. conditional
probabilities P (at |m, ao<t ) and P (ot |m, ao<t ) for all aot  Z  , and where m  M is the
index or parameter characterizing the operation mode. The probability distribution over
the input stream (output stream) is called the hypothesis (policy) of the operation mode.
Table 1 collects the essential equations of the Bayesian control rule. In particular, there the
rule is stated using a recursive belief update.

4. Convergence
The aim of this section is to develop a set of sufficient conditions of convergence and then
to provide a proof of convergence. To simplify the exposition, the analysis has been limited
485

fiOrtega & Braun

to the case of controllers having a finite number of input-output models.

4.1 Policy Diagrams
In the following we use policy diagrams as a useful informal tool to analyze the effect of
policies on environments. Figure 2 illustrates an example.

state space
s

ao

s

policy

Figure 2: A policy diagram. One can imagine an environment as a collection of states
connected by transitions labeled by I/O symbols. The zoom highlights a state s
where taking action a  A and collecting observation o  O leads to state s .
Sets of states and transitions are represented as enclosed areas similar to a Venn
diagram. Choosing a particular policy in an environment amounts to partially
controlling the transitions taken in the state space, thereby choosing a probability
distribution over state transitions (e.g. a Markov chain given by the environmental
dynamics). If the probability mass concentrates in certain areas of the state space,
choosing a policy can be thought of as choosing a subset of the environments
dynamics. In the following, a policy is represented by a subset in state space
(enclosed by a directed curve) as illustrated above.

Policy diagrams are especially useful to analyze the effect of policies on different hypotheses about the environments dynamics. An agent that is endowed with a set of operation
modes M can be seen as having hypotheses about the environments underlying dynamics,
given by the observation models P (ot |m, ao<t at ), and associated policies, given by the action models P (at |m, ao<t ), for all m  M. For the sake of simplifying the interpretation of
policy diagrams, we will assume the existence of a state space T : (A  O)  S mapping
I/O histories into states. Note however that no such assumptions are made to obtain the
results of this section.
4.2 Divergence Processes
The central question in this section is to investigate whether the Bayesian control rule converges to the correct control law or not. That is, whether P (at |aot )  P (at |m , ao<t ) as t 
 when m is the true operation mode, i.e. the operation mode such that P (ot |m , ao<t at ) =
Q(ot |ao<t at ). As will be obvious from the discussion in the rest of this section, this is in
general not true.
As it is easily seen from Equation 13, showing convergence amounts to show that the
posterior distribution P (m|ao<t ) concentrates its probability mass on a subset of operation
486

fiA Minimum Relative Entropy Principle for Learning and Acting

modes M having essentially the same output stream as m ,
X
X
P (at |m, ao<t )P (m|ao<t ) 
P (at |m , ao<t )P (m|ao<t )  P (at |m , ao<t ).
mM

mM

Hence, understanding the asymptotic behavior of the posterior probabilities
P (m|aot )
is crucial here. In particular, we need to understand under what conditions these quantities
converge to zero. The posterior can be rewritten as
Q
P (aot |m)P (m)
P (m) t =1 P (o |m, ao< a )
.
=P
P (m|aot ) = P
Qt




m M P (aot |m )P (m )
m M P (m )
 =1 P (o |m , ao< a )

If all the summands but the one with index m are dropped from the denominator, one
obtains the bound
P (m|aot ) 

t
P (m) Y P (o |m, ao< a )
,
P (m )
P (o |m , ao< a )
 =1

which is valid for all m  M. From this inequality, it is seen that it is convenient to
analyze the behavior of the stochastic process


dt (m km) :=

t
X
 =1

ln

P (o |m , ao< a )
P (o |m, ao< a )

which is the divergence process of m from the reference m . Indeed, if dt (m km)   as
t  , then
t
P (m)
P (m) Y P (o |m, ao< a )

= lim
 edt (m km) = 0,



t P (m )
P (o |m , ao< a ) t P (m )

lim

 =1

and thus clearly P (m|aot )  0. Figure 3 illustrates simultaneous realizations of the
divergence processes of a controller. Intuitively speaking, these processes provide lower
bounds on accumulators of surprise value measured in information units.
A divergence process is a random walk whose value at time t depends on the whole
history up to time t1. What makes these divergence processes cumbersome to characterize
is the fact that their statistical properties depend on the particular policy that is applied;
hence, a given divergence process can have different growth rates depending on the policy
(Figure 4). Indeed, the behavior of a divergence process might depend critically on the
distribution over actions that is used. For example, it can happen that a divergence process
stays stable under one policy, but diverges under another. In the context of the Bayesian
control rule this problem is further aggravated, because in each time step, the policy that
is applied is determined stochastically. More specifically, if m is the true operation mode,
then dt (m km) is a random variable that depends on the realization aot which is drawn
from
t
Y

 =1

P (a |m , ao )P (o |m , ao a ),
487

fiOrtega & Braun

dt
1
2
3
4
t

0

Figure 3: Realization of the divergence processes 1 to 4 associated to a controller with
operation modes m1 to m4 . The divergence processes 1 and 2 diverge, whereas 3
and 4 stay below the dotted bound. Hence, the posterior probabilities of m1 and
m2 vanish.

dt
1

2
3

2

1
0

3
t

Figure 4: The application of different policies lead to different statistical properties of the
same divergence process.

488

fiA Minimum Relative Entropy Principle for Learning and Acting

where the m1 , m2 , . . . , mt are drawn themselves from P (m1 ), P (m2 |ao1 ), . . . , P (mt |ao<t ).
To deal with the heterogeneous nature of divergence processes, one can introduce a
temporal decomposition that demultiplexes the original process into many sub-processes
belonging to unique policies. Let Nt := {1, 2, . . . , t} be the set of time steps up to time t.
Let T  Nt , and let m, m  M. Define a sub-divergence of dt (m km) as a random variable
X P (o |m , ao a )
<
gm (m; T ) :=
ln
P (o |m, ao< a )
 T

drawn from

Pm ({ao } T |{ao } T  ) :=


Y
 T

 Y

P (a |m , ao< )
P (o |m , ao< a ) ,
 T

where T := Nt \ T and where {ao } T  are given conditions that are kept constant. In
this definition, m plays the role of the policy that is used to sample the actions in the time
steps T . Clearly, any realization of the divergence process dt (m km) can be decomposed
into a sum of sub-divergences, i.e.
X
gm (m; Tm ),
dt (m km) =
(14)
m

where {Tm }mM forms a partition of Nt . Figure 5 shows an example decomposition.
dt
1
2
3

t

0

Figure 5: Decomposition of a divergence process (1) into sub-divergences (2 & 3).
The averages of sub-divergences will play an important role in the analysis. Define the
average over all realizations of gm (m; T ) as
X
Pm ({ao } T |{ao } T  )gm (m; T ).
Gm (m; T ) :=
(ao ) T

Notice that for any   Nt ,
X
P (o |m , ao< a )
P (a |m , ao< )P (o |m , ao< a ) ln
 0,
Gm (m; { }) =
P (o |m, ao< a )
ao


because of Gibbs inequality. In particular,

Gm (m ; { }) = 0.

Clearly, this holds as well for any T  Nt :
m

Gm (m; T )  0,

Gm (m ; T ) = 0.
489

(15)

fiOrtega & Braun

4.3 Boundedness
In general, a divergence process is very complex: virtually all the classes of distributions
that are of interest in control go well beyond the assumptions of i.i.d. and stationarity. This
increased complexity can jeopardize the analytic tractability of the divergence process, such
that no predictions about its asymptotic behavior can be made anymore. More specifically,
if the growth rates of the divergence processes vary too much from realization to realization, then the posterior distribution over operation modes can vary qualitatively between
realizations. Hence, one needs to impose a stability requirement akin to ergodicity to limit
the class of possible divergence-processes to a class that is analytically tractable. For this
purpose the following property is introduced.
A divergence process dt (m km) is said to have bounded variation in M iff for any  > 0,
there is a C  0, such that for all m  M, all t and all T  Nt
fi
fi
fi
fi
figm (m; T )  Gm (m; T )fi  C
with probability  1  .

dt

1

2

3

t

0

Figure 6: If a divergence process has bounded variation, then the realizations (curves 2 &
3) of a sub-divergence stay within a band around the mean (curve 1).
Figure 6 illustrates this property. Boundedness is the key property that is going to be
used to construct the results of this section. The first important result is that the posterior
probability of the true input-output model is bounded from below.
Theorem 2. Let the set of operation modes of a controller be such that for all m  M the
divergence process dt (m km) has bounded variation. Then, for any  > 0, there is a  > 0,
such that for all t  N,

P (m |aot ) 
|M|

with probability  1  .
4.4 Core

If one wants to identify the operation modes whose posterior probabilities vanish, then it
is not enough to characterize them as those modes whose hypothesis does not match the
true hypothesis. Figure 7 illustrates this problem. Here, three hypotheses along with their
associated policies are shown. H1 and H2 share the prediction made for region A but differ
490

fiA Minimum Relative Entropy Principle for Learning and Acting

in region B. Hypothesis H3 differs everywhere from the others. Assume H1 is true. As long
as we apply policy P2 , hypothesis H3 will make wrong predictions and thus its divergence
process will diverge as expected. However, no evidence against H2 will be accumulated. It
is only when one applies policy P1 for long enough time that the controller will eventually
enter region B and hence accumulate counter-evidence for H2 .
H1

H2

B
A

H3

B
A

P3

P1
P2

Figure 7: If hypothesis H1 is true and agrees with H2 on region A, then policy P2 cannot
disambiguate the three hypotheses.

But what does long enough mean? If P1 is executed only for a short period, then the
controller risks not visiting the disambiguating region. But unfortunately, neither the right
policy nor the right length of the period to run it are known beforehand. Hence, an agent
needs a clever time-allocating strategy to test all policies for all finite time intervals. This
motivates the following definition.
The core of an operation mode m , denoted as [m ], is the subset of M containing
operation modes behaving like m under its policy. More formally, an operation mode
m
/ [m ] (i.e. is not in the core) iff for any C  0,  > 0, there is a  > 0 and a t0  N,
such that for all t  t0 ,
Gm (m; T )  C
with probability  1  , where Gm (m; T ) is a sub-divergence of dt (m km), and Pr{ 
T }   for all   Nt .
In other words, if the agent was to apply m s policy in each time step with probability at
least , and under this strategy the expected sub-divergence Gm (m; T ) of dt (m km) grows
unboundedly, then m is not in the core of m . Note that demanding a strictly positive
probability of execution in each time step guarantees that the agent will run m for all
possible finite time-intervals. As the following theorem shows, the posterior probabilities of
the operation modes that are not in the core vanish almost surely.
Theorem 3. Let the set of operation modes of an agent be such that for all m  M the
divergence process dt (m km) has bounded variation. If m 
/ [m ], then P (m|aot )  0 as
t   almost surely.
4.5 Consistency
Even if an operation mode m is in the core of m , i.e. given that m is essentially indistinguishable from m under m s control, it can still happen that m and m have different
policies. Figure 8 shows an example of this. The hypotheses H1 and H2 share region A but
491

fiOrtega & Braun

differ in region B. In addition, both operation modes have their policies P1 and P2 respectively confined to region A. Note that both operation modes are in the core of each other.
However, their policies are different. This means that it is unclear whether multiplexing the
policies in time will ever disambiguate the two hypotheses. This is undesirable, as it could
impede the convergence to the right control law.
H2

H1
B

B
P2

P1

A

A

Figure 8: An example of inconsistent policies. Both operation modes are in the core of each
other, but have different policies.

Thus, it is clear that one needs to impose further restrictions on the mapping of hypotheses into policies. With respect to Figure 8, one can make the following observations:
1. Both operation modes have policies that select subsets of region A. Therefore, the
dynamics in A are preferred over the dynamics in B.
2. Knowing that the dynamics in A are preferred over the dynamics in B allows us to
drop region B from the analysis when choosing a policy.
3. Since both hypotheses agree in region A, they have to choose the same policy in order
to be consistent in their selection criterion.
This motivates the following definition. An operation mode m is said to be consistent
with m iff m  [m ] implies that for all  < 0, there is a t0 , such that for all t  t0 and all
ao<t at ,
fi
fi
fi
fi
fiP (at |m, aot )  P (at |m , aot )fi < .

In other words, if m is in the core of m , then ms policy has to converge to m s policy.
The following theorem shows that consistency is a sufficient condition for convergence to
the right control law.
Theorem 4. Let the set of operation modes of an agent be such that: for all m  M the
divergence process dt (m km) has bounded variation; and for all m, m  M, m is consistent
with m . Then,
P (at |ao<t )  P (at |m , ao<t )
almost surely as t  .
492

fiA Minimum Relative Entropy Principle for Learning and Acting

4.6 Summary
In this section, a proof of convergence of the Bayesian control rule to the true operation
mode has been provided for a finite set of operation modes. For this convergence result to
hold, two necessary conditions are assumed: boundedness and consistency. The first one,
boundedness, imposes the stability of divergence processes under the partial influence of the
policies contained within the set of operation modes. This condition can be regarded as
an ergodicity assumption. The second one, consistency, requires that if a hypothesis makes
the same predictions as another hypothesis within its most relevant subset of dynamics,
then both hypotheses share the same policy. This relevance is formalized as the core of an
operation mode. The concepts and proof strategies strengthen the intuition about potential
pitfalls that arise in the context of controller design. In particular we could show that
the asymptotic analysis can be recast as the study of concurrent divergence processes that
determine the evolution of the posterior probabilities over operation modes, thus abstracting
away from the details of the classes of I/O distributions. The extension of these results to
infinite sets of operation modes is left for future work. For example, one could think
of partitioning a continuous space of operation modes into essentially different regions
where representative operation modes subsume their neighborhoods (Grunwald, 2007).

5. Examples
In this section we illustrate the usage of the Bayesian control rule on two examples that
are very common in the reinforcement learning literature: multi-armed bandits and Markov
decision processes.
5.1 Bandit Problems
Consider the multi-armed bandit problem (Robbins, 1952). The problem is stated as follows.
Suppose there is an N -armed bandit, i.e. a slot-machine with N levers. When pulled, lever
i provides a reward drawn from a Bernoulli distribution with a bias hi specific to that lever.
That is, a reward r = 1 is obtained with probability hi and a reward r = 0 with probability
1hi . The objective of the game is to maximize the time-averaged reward through iterative
pulls. There is a continuum range of stationary strategies, each one parameterized by N
probabilities {si }N
i=1 indicating the probabilities of pulling each lever. The difficulty arising
in the bandit problem is to balance reward maximization based on the knowledge already
acquired with attempting new actions to further improve knowledge. This dilemma is known
as the exploration versus exploitation tradeoff (Sutton & Barto, 1998).
This is an ideal task for the Bayesian control rule, because each possible bandit has a
known optimal agent. Indeed, a bandit can be represented by an N -dimensional bias vector
m = [m1 , . . . , mN ]  M = [0; 1]N . Given such a bandit, the optimal policy consists in
pulling the lever with the highest bias. That is, an operation mode is given by:
hi = P (ot = 1|m, at = i) = mi

si = P (at = i|m) =

493

(

1 if i = maxj {mj },
0 else.

fiOrtega & Braun

m2

0

a)

1

b)
m1  m2

m2

1

1

m2  m1 , m 3

m3
0

m1

0
0

m1

1

1

Figure 9: The space of bandit configurations can be partitioned into N regions according
to the optimal lever. Panel a and b show the 2-armed and 3-armed bandit cases
respectively.

To apply the Bayesian control rule, it is necessary to fix a prior distribution over the
bandit configurations. Assuming a uniform distribution, the Bayesian control rule is
Z
(16)
P (at+1 = i|m)P (m|aot )
P (at+1 = i|aot ) =
M

with the update rule given by
Q
r
N
Y
mj j (1  mj )fj
P (m) t =1 P (o |m, a )
=
P (m|aot ) = R
Qt



B(rj + 1, fj + 1)
 =1 P (o |m , a ) dm
M P (m )
j=1

(17)

where rj and fj are the counts of the number of times a reward has been obtained from
pulling lever j and the number of times no reward was obtained respectively. Observe that
here the summation over discrete operation modes has been replaced by an integral over
the continuous space of configurations. In the last expression we see that the posterior
distribution over the lever biases is given by a product of N Beta distributions. Thus,
sampling an action amounts to first sample an operation mode m by obtaining each bias
mi from a Beta distribution with parameters ri + 1 and fi + 1, and then choosing the
action corresponding to the highest bias a = arg maxi mi . The pseudo-code can be seen in
Algorithm 1.
Simulation: The Bayesian control rule described above has been compared against two
other agents: an -greedy strategy with decay (on-line) and Gittins indices (off-line). The
test bed consisted of bandits with N = 10 levers whose biases were drawn uniformly at
the beginning of each run. Every agent had to play 1000 runs for 1000 time steps each.
Then, the performance curves of the individual runs were averaged. The -greedy strategy
selects a random action with a small probability given by t and otherwise plays the
lever with highest expected reward. The parameters have been determined empirically to
the values  = 0.1, and  = 0.99 after several test runs. They have been adjusted in a way
to maximize the average performance in the last trials of our simulations. For the Gittins
method, all the indices were computed up to horizon 1300 using a geometric discounting
of  = 0.999, i.e. close to one to approximate the time-averaged reward. The results are
shown in Figure 10.
494

fiA Minimum Relative Entropy Principle for Learning and Acting

Algorithm 1 BCR bandit.
for all i = 1, . . . , N do
Initialize ri and fi to zero.
end for
for t = 1, 2, 3, . . . do
Sample m using (17).
{ Interaction }
Set a  arg maxi mi and issue a.
Obtain o from environment.

Avg. Reward

{Update belief}
if o = 1 then
ra = ra + 1
else
fa = fa + 1
end if
end for

0.85
0.80

Bayesian control rule
-greedy
Gittins indices

0.75
0.70
0

200

400

600

800

1000

0

200

400

600

800

1000

% Best Lever

100
80
60
40
20
0

Figure 10: Comparison in the N -armed bandit problem of the Bayesian control rule (solid
line), an -greedy agent (dashed line) and using Gittins indices (dotted line).
1,000 runs have been averaged. The top panel shows the evolution of the average
reward. The bottom panel shows the evolution of the percentage of times the
best lever was pulled.

495

fiOrtega & Braun

It is seen that -greedy strategy quickly reaches an acceptable level of performance, but
then seems to stall at a significantly suboptimal level, pulling the optimal lever only 60% of
the time. In contrast, both the Gittins strategy and the Bayesian control rule show essentially the same asymptotic performance, but differ in the initial transient phase where the
Gittins strategy significantly outperforms the Bayesian control rule. There are at least three
observations that are worth making here. First, Gittins indices have to be pre-computed
off-line. The time complexity scales quadratically with the horizon, and the computations
for the horizon of 1300 steps took several hours on our machines. In contrast, the Bayesian
control rule could be applied without pre-computation. Second, even though the Gittins
method actively issues the optimal information gathering actions while the Bayesian control
rule passively samples the actions from the posterior distribution over operation modes, in
the end both methods rely on the convergence of the underlying Bayesian estimator. This
implies that both methods have the same information bottleneck, since the Bayesian estimator requires the same amount of information to converge. Thus, active information gathering
actions only affect the utility of the transient phase, not the permanent state. Other efficient algorithms for bandit problems can be found in the literature (Auer, CesaBianchi, &
Fischer, 2002).
5.2 Markov Decision Processes
A Markov Decision Process (MDP ) is defined as a tuple (X , A, T, r): X is the state space;
A is the action space; Ta (x; x ) = Pr(x |a, x) is the probability that an action a  A
taken in state x  X will lead to state x  X ; and r(x, a)  R := R is the immediate
reward obtained in state x  X and action a  A. The interaction proceeds in time steps
t = 1, 2, . . . where at time t, action at  A is issued in state xt1  X , leading to a reward
rt = r(xt1 , at ) and a new state xt that starts the next time step t + 1. A stationary closedloop control policy  : X  A assigns an action to each state. For MDPs there always
exists an optimal stationary deterministic policy and thus one only needs to consider such
policies. In undiscounted MDPs the average rewardPper time step for a fixed policy  with
initial state x is defined as  (x) = limt E [ 1t t =0 r ]. It can be shown (Bertsekas,
1987) that  (x) =  (x ) for all x, x  X under the assumption that the Markov chain for
policy  is ergodic. Here, we assume that the MDPs are ergodic for all stationary policies.
In order to keep the intervention model particularly simple3 , we follow the Q-notation
of Watkins (1989). The optimal policy   can then be characterized in terms of the optimal
average reward  and the optimal relative Q-values Q(x, a) for each state-action pair (x, a)
that are solutions to the following system of non-linear equations (Singh, 1994): for any

3. The brute-force adaptive agent for this problem would roughly look as follows. First, the agent
starts with a prior distribution over all MDPs, e.g. product of Dirichlet distributions over the transition
probabilities. Then, in each cycle, the agent samples a full transition matrix from the distribution and
solves it using dynamic programming. Once it has computed the optimal policy, it uses it to issue the
next action, and then discards the policy. Subsequently, it updates the distribution over MDPs using
the next observed state. However, in the main text we follow a different approach that avoids solving
an MDP in every time step.

496

fiA Minimum Relative Entropy Principle for Learning and Acting

state x  X and action a  A,
Q(x, a) +  = r(x, a) +

X

x X

i
h
 
Q(x
,
a
)
Pr(x |x, a) max

a

fi
i
h
  fi
= r(x, a) + Ex max
x,
a
.
Q(x
,
a
)
fi


(18)

a

The optimal policy can then be defined as   (x) := arg maxa Q(x, a) for any state x  X .
Again this setup allows for a straightforward solution with the Bayesian control rule,
because each learnable MDP (characterized by the Q-values and the average reward) has
a known solution   . Accordingly, an operation mode m is given by m = [Q, ]  M =
R|A||O|+1 . To obtain a likelihood model for inference over m, we realize that Equation 18
can be rewritten such that it predicts the instantaneous reward r(x, a) as the sum of a mean
instantaneous reward m plus a noise term  given the Q-values and the average reward 
for the MDP labeled by m
r(x, a) = Q(x, a) +   max
Q(x , a ) + max
Q(x , a )  E[max
Q(x , a )|x, a]
a
a
a
|
{z
}
{z
}
|
noise 

mean instantaneous reward m (x,a,x )

Assuming that  can be reasonably approximated by a normal distribution N(0, 1/p) with
precision p, we can write down a likelihood model for the immediate reward r using the
Q-values and the average reward, i.e.
r
o
n p
p

 2
P (r|m, x, a, x ) =
(19)
exp  (r  m (x, a, x )) .
2
2

In order to determine the intervention model for each operation mode, we can simply exploit
the above properties of the Q-values, which gives
(
1 if a = arg maxa Q(x, a )
P (a|m, x) =
(20)
0 else.
To apply the Bayesian control rule, the posterior distribution P (m|at , xt ) needs to be
computed. Fortunately, due to the simplicity of the likelihood model, one can easily devise a
conjugate prior distribution and apply standard inference methods (see Appendix A.5). Actions are again determined by sampling operation modes from this posterior and executing
the action suggested by the corresponding intervention models. The resulting algorithm is
very similar to Bayesian Q-learning (Dearden, Friedman, & Russell, 1998; Dearden, Friedman, & Andre, 1999), but differs in the way actions are selected. The pseudo-code is listed
in Algorithm 2.
Simulation: We have tested our MDP-agent in a grid-world example. To give an intuition
of the achieved performance, the results are contrasted with those achieved by R-learning.
We have used the R-learning variant presented in the work of Singh (1994, Algorithm 3)
together with the uncertainty exploration strategy (Mahadevan, 1996). The corresponding
update equations are

Q(x, a)  (1  )Q(x, a) +  r   + max
Q(x , a )

a

(21)
 
  (1  ) +  r + max
Q(x
,
a
)

Q(x,
a)
,

a

497

fiOrtega & Braun

Algorithm 2 BCR-MDP Gibbs sampler.
Initialize entries of  and  to zero.
Set initial state to x  x0 .
for t = 1, 2, 3, . . . do
{Gibbs sweep}
Sample  using (30).
for all Q(y, b) of visited states do
Sample Q(y, b) using (31).
end for
{ Interaction }
Set a  arg maxa Q(x, a ) and issue a.
Obtain o = (r, x ) from environment.
{Update hyperparameters}
 )(x,a,x )+p r
(x, a, x )  (x,a,x
(x,a,x )+p
(x, a, x )  (x, a, x ) + p
Set x  x .
end for

goal

membranes

b) Bayesian control rule

c) R-learning, C=5

d) R-learning, C=30

initial 5,000 steps

a) 7x7 Maze

e) R-learning, C=200

f) Average Reward
0.4
0.3

C=30
C=5

0.2

low
probability

C=200
0.1

last 5,000 steps

high
probability

Bayesian control rule
0.0
0

125

250

375

500

x1000 time steps

Figure 11: Results for the 77 grid-world domain. Panel (a) illustrates the setup. Columns
(b)-(e) illustrate the behavioral statistics of the algorithms. The upper and lower
row have been calculated over the first and last 5,000 time steps of randomly
chosen runs. The probability of being in a state is color-encoded, and the arrows
represent the most frequent actions taken by the agents. Panel (f) presents the
curves obtained by averaging ten runs.

498

fiA Minimum Relative Entropy Principle for Learning and Acting

Average Reward
BCR
R-learning, C = 200
R-learning, C = 30
R-learning, C = 5

0.3582  0.0038
0.2314  0.0024
0.3056  0.0063
0.2049  0.0012

Table 2: Average reward attained by the different algorithms at the end of the run. The
mean and the standard deviation has been calculated based on 10 runs.

where ,  > 0 are learning rates. The exploration strategy chooses with fixed probability
C
pexp > 0 the action a that maximizes Q(x, a) + F (x,a)
, where C is a constant, and F (x, a)
represents the number of times that action a has been tried in state x. Thus, higher values
of C enforce increased exploration.
In a study (Mahadevan, 1996), a grid-world is described that is especially useful as
a test bed for the analysis of RL algorithms. For our purposes, it is of particular interest
because it is easy to design experiments containing suboptimal limit-cycles. Figure 11, panel
(a), illustrates the 7  7 grid-world. A controller has to learn a policy that leads it from
any initial location to the goal state. At each step, the agent can move to any adjacent
space (up, down, left or right). If the agent reaches the goal state then its next position
is randomly set to any square of the grid (with uniform probability) to start another trial.
There are also one-way membranes that allow the agent to move into one direction but
not into the other. In these experiments, these membranes form inverted cups that the
agent can enter from any side but can only leave through the bottom, playing the role
of a local maximum. Transitions are stochastic: the agent moves to the correct square
9
with probability p = 10
and to any of the free adjacent spaces (uniform distribution) with
1
probability 1  p = 10 . Rewards are assigned as follows. The default reward is r = 0.
If the agent traverses a membrane it obtains a reward of r = 1. Reaching the goal state
assigns r = 2.5. The parameters chosen for this simulation were the following. For our
MDP-agent, we have chosen hyperparameters 0 = 1 and 0 = 1 and precision p = 1.
For R-learning, we have chosen learning rates  = 0.5 and  = 0.001, and the exploration
constant has been set to C = 5, C = 30 and to C = 200. A total of 10 runs were carried
out for each algorithm. The results are presented in Figure 11 and Table 2. R-learning only
learns the optimal policy given sufficient exploration (panels d & e, bottom row), whereas
the Bayesian control rule learns the policy successfully. In Figure 11f, the learning curve of
R-learning for C = 5 and C = 30 is initially steeper than the Bayesian controller. However,
the latter attains a higher average reward around time step 125,000 onwards. We attribute
this shallow initial transient to the phase where the distribution over the operation modes
is flat, which is also reflected by the initially random exploratory behavior.

6. Discussion
The key idea of this work is to extend the minimum relative entropy principle, i.e. the
variational principle underlying Bayesian estimation, to the problem of adaptive control.
499

fiOrtega & Braun

From a coding point of view, this work extends the idea of maximal compression of the
observation stream to the whole experience of the agent containing both the agents actions
and observations. This not only minimizes the amount of bits to write when saving/encoding
the I/O stream, but it also minimizes the amount of bits required to produce/decode an
action (MacKay, 2003, Ch. 6).
This extension is non-trivial, because there is an important caveat for coding I/O sequences: unlike observations, actions do not carry any information that could be used for
inference in adaptive coding because actions are issued by the decoder itself. The problem
is that doing inference on ones own actions is logically inconsistent and leads to paradoxes
(Nozick, 1969). This seemingly innocuous issue has turned out to be very intricate and
has been investigated intensely in the recent past by researchers focusing on the issue of
causality (Pearl, 2000; Spirtes et al., 2000; Dawid, 2010). Our work contributes to this body
of research by providing further evidence that actions cannot be treated using probability
calculus alone.
If the causal dependencies are carefully taken into account, then minimizing the relative
entropy leads to a rule for adaptive control which we called the Bayesian control rule. This
rule allows combining a class of task-specific agents into an agent that is universal with
respect to this class. The resulting control law is a simple stochastic control rule that is
completely general and parameter-free. As the analysis in this paper shows, this control
rule converges to the true control law under mild assumptions.
6.1 Critical Issues
 Causality. Virtually every adaptive control method in the literature successfully treats
actions as conditionals over observation streams and never worries about causality.
Thus, why bother about interventions? In a decision-theoretic setup, the decision
maker chooses a policy     maximizing
P the expected utility U over the outcomes
  , i.e.   := arg max E[U |] =  Pr(|)U (). Choosing    is formally
equivalent to choosing the Kronecker delta function  as the probability distribution
over policies. In this case, the conditional probabilities Pr(|) and Pr(|) coincide,
since
Pr(, ) = Pr()Pr(|) =  Pr(|) = Pr(, ).
In this sense, the choice of the policy causally precedes the interactions. As we have
discussed in Section 3 however, when there is uncertainty about the policy (i.e. Pr() 6=
 ), then causal belief updates are crucial. Essentially, this problem arises because
the uncertainty over the policy is resolved during the interactions. Hence, treating
actions as interventions seamlessly extends them to the status of random variables.
 Where do prior probabilities/likelihood models/policies come from? The predictor in
the Bayesian control rule is essentially a Bayesian predictor and thereby entails (almost) the same modeling paradigm. The designer has to define a class of hypotheses
over the environments, construct appropriate likelihood models, and choose a suitable
prior probability distribution to capture the models uncertainty. Similarly, under sufficient domain knowledge, an analogous procedure can be applied to construct suitable
operation modes. However, there are many situations where this is a difficult or even
500

fiA Minimum Relative Entropy Principle for Learning and Acting

intractable problem in itself. For example, one can design a class of operation modes
by pre-computing the optimal policies for a given class of environments. Formally, let
 be a class of hypotheses modeling environments and let  be class of policies. Given
a utility criterion U , define the set of operation modes M := {m } by constructing each operation mode as m := (,   ),    , where   := arg max E[U |, ].
However, computing the optimal policy   is in many cases intractable. In some
cases, this can be remedied by characterizing the operation modes through optimality
equations which are solved by probabilistic inference as in the example of the MDP
agent in Section 5.2. Recently, we have applied a similar approach to adaptive control
problems with linear quadratic regulators (Braun & Ortega, 2010).
 Problems of Bayesian methods. The Bayesian control rule treats an adaptive control
problem as a Bayesian inference problem. Hence, all the problems typically associated
with Bayesian methods carry over to agents constructed with the Bayesian control
rule. These problems are of both analytical and computational nature. For example,
there are many probabilistic models where the posterior distribution does not have a
closed-form solution. Also, exact probabilistic inference is in general computationally
very intensive. Even though there is a large literature in efficient/approximate inference algorithms for particular problem classes (Bishop, 2006), not many of them are
suitable for on-line probabilistic inference in more realistic environment classes.
 Bayesian control rule versus Bayes-optimal control. Directly maximizing the (subjective) expected utility for a given environment class is not the same as minimizing
the expected relative entropy for a given class of operation modes. The two methods
are based on different assumptions and optimality principles. As such, the Bayesian
control rule is not a Bayes-optimal controller. Indeed, it is easy to design experiments
where the Bayesian control rule converges exponentially slower (or does not converge
at all) than a Bayes-optimal controller to the maximum utility. Consider the following
simple example: Environment 1 is a k-state MDP in which only k consecutive actions
A reach a state with reward +1. Any interception with a B-action leads back to the
initial state. Consider a second environment which is like the first but actions A and
B are interchanged. A Bayes-optimal controller figures out the true environment in k
actions (either k consecutive As or Bs). Consider now the Bayesian control rule: The
optimal action in Environment 1 is A, in Environment 2 is B. A uniform ( 21 , 21 ) prior
over the operation modes stays a uniform posterior as long as no reward has been
observed. Hence the Bayesian control rule chooses at each time-step A and B with
equal probability. With this policy it takes about 2k actions to accidentally choose a
row of As (or Bs) of length k. From then on the Bayesian control rule is optimal
too. So a Bayes-optimal controller converges in time k, while the Bayesian control
rule needs exponentially longer. One way to remedy this problem might be to allow
the Bayesian control rule to sample actions from the same operation mode for several
time steps in a row rather than randomizing controllers in every cycle. However, if
one considers non-stationary environments this strategy
  can also break down. Consider, for example, an increasing MDP with k = 10 t , in which a Bayes-optimal
controller converges in 100 steps, while the Bayesian control rule does not converge
at all in most realizations, because the boundedness assumption is violated.
501

fiOrtega & Braun

6.2 Relation to Existing Approaches
Some of the ideas underlying this work are not unique to the Bayesian control rule. The
following is a selection of previously published work in the recent Bayesian reinforcement
learning literature where related ideas can be found.
 Compression principles. In the literature, there is an important amount of work
relating compression to intelligence (MacKay, 2003; Hutter, 2004b). In particular, it
has been even proposed that compression ratio is an objective quantitative measure of
intelligence (Mahoney, 1999). Compression has also been used as a basis for a theory
of curiosity, creativity and beauty (Schmidhuber, 2009).
 Mixture of experts. Passive sequence prediction by mixing experts has been studied
extensively in the literature (Cesa-Bianchi & Lugosi, 2006). In a study on onlinepredictors (Hutter, 2004a), Bayes-optimal predictors are mixed. Bayes-mixtures can
also be used for universal prediction (Hutter, 2003). For the control case, the idea
of using mixtures of expert-controllers has been previously evoked in models like the
MOSAIC-architecture (Haruno, Wolpert, & Kawato, 2001). Universal learning with
Bayes mixtures of experts in reactive environments has been studied in the work of
Poland and Hutter (2005) and Hutter (2002).
 Stochastic action selection. The idea of using actions as random variables, and the
problems that this entails, has been expressed in the work of Hutter (2004b, Problem
5.1). The study in Section 3 can be regarded as a thorough investigation of this open
problem. Other stochastic action selection approaches are found in the thesis of Wyatt (1997) who examines exploration strategies for (PO)MDPs, in learning automata
(Narendra & Thathachar, 1974) and in probability matching (Duda, Hart, & Stork,
2001) amongst others. In particular, the thesis discusses theoretical properties of
an extension to probability matching in the context of multi-armed bandit problems.
There, it is proposed to choose a lever according to how likely it is to be optimal and
it is shown that this strategy converges, thus providing a simple method for guiding
exploration.
 Relative entropy criterion. The usage of a minimum relative entropy criterion to
derive control laws underlies the KL-control methods developed in the work of Todorov
(2006, 2009) and Kappen et al. (2010). There, it has been shown that a large class
of optimal control problems can be solved very efficiently if the problem statement
is reformulated as the minimization of the deviation of the dynamics of a controlled
system from the uncontrolled system. A related idea is to conceptualize planning as
an inference problem (Toussaint, Harmeling, & Storkey, 2006). This approach is based
on an equivalence between maximization of the expected future return and likelihood
maximization which is both applicable to MDPs and POMDPs. Algorithms based on
this duality have become an active field of current research. See for example the work
of Rasmussen and Deisenroth (2008), where very fast model-based RL techniques are
used for control in continuous state and action spaces.
502

fiA Minimum Relative Entropy Principle for Learning and Acting

7. Conclusions
This work introduces the Bayesian control rule, a Bayesian rule for adaptive control. The
key feature of this rule is the special treatment of actions based on causal calculus and the
decomposition of an adaptive agent into a mixture of operation modes, i.e. environmentspecific agents. The rule is derived by minimizing the expected relative entropy from the
true operation mode and by carefully distinguishing between actions and observations. Furthermore, the Bayesian control rule turns out to be exactly the predictive distribution over
the next action given the past interactions that one would obtain by using only probability
and causal calculus. Furthermore, it is shown that agents constructed with the Bayesian
control rule converge to the true operation mode under mild assumptions: boundedness,
which is related to ergodicity; and consistency, demanding that two indistinguishable hypotheses share the same policy.
We have presented the Bayesian control rule as a way to solve adaptive control problems
based on a minimum relative entropy principle. Thus, the Bayesian control rule can either
be regarded as a new principled approach to adaptive control under a novel optimality
criterion or as a heuristic approximation to traditional Bayes-optimal control. Since it
takes on a similar form to Bayes rule, the adaptive control problem could then be translated
into an on-line inference problem where actions are sampled stochastically from a posterior
distribution. It is important to note, however, that the problem statement as formulated
here and the usual Bayes-optimal approach in adaptive control are not the same. In the
future the relationship between these two problem statements deserves further investigation.

Acknowledgments
We thank Marcus Hutter, David Wingate, Zoubin Ghahramani, Jose Aliste, Jose Donoso,
Humberto Maturana and the anonymous reviewers for comments on earlier versions of this
manuscript and/or inspiring discussions. We thank the Ministerio de Planificacion de Chile
(MIDEPLAN) and the Bohringer-Ingelheim-Fonds (BIF) for funding.

Appendix A. Proofs
A.1 Proof of Theorem 1
Proof. The proof follows the same line of argument as the solution to Equation 3 with
the crucial difference that
treated as interventions. Consider without loss of
P actions are
at in Equation 9. Note that the relative entropy can be
generality the summand m P (m)Cm
written as a difference of two logarithms, where only one term depends on Pr to be varied.
Therefore, one can pull out the other term and write it as a constant c. This yields

c

X
m

P (m)

X

ao<t

P (ao<t |m)

X
at

503

P (at |m, ao<t ) ln Pr(at |ao<t ).

fiOrtega & Braun

Substituting P (ao<t |m) by P (m|ao<t )P (ao<t )/P (m) using Bayes rule and further rearrangement of the terms leads to
XX
X
=c
P (m|ao<t )P (ao<t )
P (at |m, ao<t ) ln Pr(at |ao<t )
m ao<t

=c

X

P (ao<t )

at

X
at

ao<t

P (at |ao<t ) ln Pr(at |ao<t ).

P
The inner sum has the form  x p(x) ln q(x), i.e. the cross-entropy between q(x) and p(x),
which is minimized when q(x) = p(x) for all x. Let P denote the optimum distribution
for Pr. By choosing this optimum one obtains P(at |ao<t ) = P (at |ao<t ) for all at . Note that
the solution to this variational problem is P
independent of the P
weighting P (ao<t ). Since the
o
a and
same argument applies to any summand m P (m)Cm
m P (m)Cm in Equation 9,
their variational problems are mutually independent. Hence,
P(at |ao<t ) = P (at |ao<t )

P(ot |ao<t ) = P (ot |ao<t at )

for all aot  Z  . For P (at |ao<t ), introduce the variable m via a marginalization and then
apply the chain rule:
X
P (at |ao<t ) =
P (at+1 |m, ao<t )P (m|ao<t ).
m

The term P (m|aot ) can be further developed as

P (ao<t |m)P (m)


m P (ao<t |m )P (m )
Qt1
P (m)  =1 P (a |m, ao< )P (o |m, ao< a )
=P
Qt1



m P (m )
 =1 P (a |m , ao< )P (o |m , ao< a )
Qt1
P (m)  =1 P (o |m, ao< a )
.
=P
Qt1


m P (m )
 =1 P (o |m , ao< a )

P (m|ao<t ) = P

The first equality is obtained by applying Bayes rule and the second by using the chain
rule for probabilities. To get the last equality, one applies the interventions to the causal
factorization. Thus, P (a |m, ao< ) = 1 and P (o |m, ao< a ) = P (o |m, ao< a ). The
equations characterizing P (ot |ao<t at ) are obtained similarly.
A.2 Proof of Theorem 2
Proof. As has been pointed out in (14), a particular realization of the divergence process
dt (m km) can be decomposed as
X
dt (m km) =
gm (m ; Tm ),
m

where the gm (m ; Tm ) are sub-divergences of dt (m km) and the Tm form a partition of Nt .
However, since dt (m km) has bounded variation for all m  M, one has for all   > 0, there
is a C(m)  0, such that for all m  M, all t  Nt and all T  Nt , the inequality
fi
fi
fi
fi
figm (m ; Tm )  Gm (m ; Tm )fi  C(m)
504

fiA Minimum Relative Entropy Principle for Learning and Acting

holds with probability  1    . However, due to (15),
Gm (m ; Tm )  0
for all m  M. Thus,

gm (m ; Tm )  C(m).

If all the previous inequalities hold simultaneously then the divergence process can be
bounded as well. That is, the inequality
dt (m km)  M C(m)

(22)

holds with probability  (1    )M where M := |M|. Choose
(m)
(m) := max{0, ln PP(m
 ) }.
(m)
Since 0  ln PP(m
Using the
 )  (m), it can be added to the right hand side of (22).

definition of dt (m km), taking the exponential and rearranging the terms one obtains


P (m )

t
Y



 =1

(m)

P (o |m , ao< a )  e

P (m)

t
Y

 =1

P (o |m, ao< a )

where (m) := M C(m) + (m)  0. Identifying the posterior probabilities of m and m
by dividing both sides by the normalizing constant yields the inequality
P (m |aot )  e(m) P (m|aot ).
2

This inequality holds simultaneously for all m  M with probability  (1    )M and in
particular for  := minm {e(m) }, that is,
P (m |aot )  P (m|aot ).
But since this is valid for any m  M, and because maxm {P (m|aot )} 
P (m |aot ) 

1
M,

one gets


,
M

with probability
 1   for arbitrary  > 0 related to   through the equation   :=

M2
1
1  .
A.3 Proof of Theorem 3
Proof. The divergence process dt (m km) can be decomposed into a sum of sub-divergences
(see Equation 14)
X
gm (m; Tm ).
(23)
dt (m km) =
m

m

Furthermore, for every
 M, one has that for all  > 0, there is a C  0, such that for
all t  N and for all T  Nt
fi
fi
fi
fi
figm (m; T )  Gm (m; T )fi  C(m)
505

fiOrtega & Braun

with probability  1    . Applying this bound to the summands in (23) yields the lower
bound
X
X

gm (m; Tm ) 
Gm (m; Tm )  C(m)
m

m

(1    )M ,

which holds with probability 
where M := |M|. Due to Inequality 15, one has




that for all m 6= m , Gm (m; Tm )  0. Hence,
X

Gm (m; Tm )  C(m)  Gm (m; Tm )  M C
m

where C := maxm {C(m)}. The members of the set Tm are determined stochastically; more
specifically, the ith member is included into Tm with probability P (m |aoi )  /M for
some  > 0 by Theorem 2. But since m 
/ [m ], one has that Gm (m; Tm )   as t  

with probability  1   for arbitrarily chosen   > 0. This implies that
lim dt (m km)  lim Gm (m; Tm )  M C  

t

t

with probability  1  , where  > 0 is arbitrary and related to   as  = 1  (1    )M +1 .
Using this result in the upper bound for posterior probabilities yields the final result
P (m) dt (m km)
e
= 0.
t P (m )

0  lim P (m|aot )  lim
t

A.4 Proof of Theorem 4
Proof. We will use the abbreviations pm (t) := P (at |m, ao<t ) and wm (t) := P (m|ao<t ).
Decompose P (at |ao<t ) as
X
X
pm (t)wm (t) +
pm (t)wm (t).
(24)
P (at |ao<t ) =
m[m
/ ]

m[m ]

The first sum on the right-hand side is lower-bounded by zero and upper-bounded by
X
X
pm (t)wm (t) 
wm (t)
m[m
/ ]

m[m
/ ]

because pm (t)  1. Due to Theorem 3, wm (t)  0 as t   almost surely. Given  > 0
and   > 0, let t0 (m) be the time such that for all t  t0 (m), wm (t) <  . Choosing
t0 := maxm {t0 (m)}, the previous inequality holds for all m and t  t0 simultaneously with
probability  (1    )M . Hence,
X
X
pm (t)wm (t) 
wm (t) < M  .
(25)
m[m
/ ]

m[m
/ ]

To bound the second sum in (24) one proceeds as follows. For every member m  [m ],
one has that pm (t)  pm (t) as t  . Hence, following a similar construction as above,
one can choose t0 such that for all t  t0 and m  [m ], the inequalities
fi
fi
fi
fi
fipm (t)  pm (t)fi < 
506

fiA Minimum Relative Entropy Principle for Learning and Acting

hold simultaneously for the precision  > 0. Applying this to the second sum in Equation 24
yields the bounds
X
X
X


pm (t) +  wm (t).
pm (t)wm (t) 
pm (t)   wm (t) 
m[m ]

m[m ]

Here pm (t)  
that




m[m ]

are multiplicative constants that can be placed in front of the sum. Note
1

X

m[m ]

wm (t) = 1 

X

m[m
/ ]

wm (t) > 1  .

Use of the above inequalities allows simplifying the lower and upper bounds respectively:
 X
pm (t)  
wm (t) > pm (t)(1   )    pm (t)  2 ,
m[m ]

pm (t) + 

 X


m[m ]

(26)

wm (t)  pm (t) +  < pm (t) + 2 .

Combining the inequalities (25) and (26) in (24) yields the final result:
fi
fi
fi
fi

P
(a
|ao
(t)
)

p
fi
fi < (2 + M ) = ,
t
m
<t

which holds with probability  1   for arbitrary  > 0 related to   as   = 1 
and arbitrary precision .



M

1

A.5 Gibbs Sampling Implementation for MDP Agent
Inserting the likelihood given in Equation 19 into Equation 13 of the Bayesian control rule,
one obtains the following expression for the posterior
P (m|at , ot ) =
=

P (x |m, x, a)P (r|m, x, a, x )P (m|a<t , o<t )






M P (x |m , x, a)P (r|m , x, a, x )P (m |a<t , o<t ) dm
P (r|m, x, a, x )P (m|a<t , o<t )
R
,




M P (r|m , x, a, x )P (m |a<t , o<t ) dm
R

(27)

where we have replaced the sum by an integration over m , the finite-dimensional real space
containing only the average reward and the Q-values of the observed states, and where we
have simplified the term P (x |m, x, a) because it is constant for all m  M.
The likelihood model P (r|m , x, a, x ) in Equation 27 encodes a set of independent normal distributions over the immediate reward with means m (x, a, x ) indexed by triples
(x, a, x )  X  A  X . In other words, given (x, a, x ), the rewards are drawn from a
normal distribution with unknown mean m (x, a, x ) and known variance  2 . The sufficient statistics are given by n(x, a, x ), the number of times that the transition x  x
under action a, and r(x, a, x ), the mean of the rewards obtained in the same transition.
The conjugate prior distribution is well known and given by a normal distribution with
hyperparameters 0 and 0 :
r
n
2 o
0


0
.
(28)
exp  2 m (x, a, x )  0
P (m (x, a, x )) = N(0 , 1/0 ) =
2
507

fiOrtega & Braun

The posterior distribution is given by
P (m (x, a, x )|at , ot ) = N((x, a, x ), 1/(x, a, x ))
where the posterior hyperparameters are computed as
0 0 + p n(x, a, x ) r(x, a, x )
0 + p n(x, a, x )
(x, a, x ) = 0 + p n(x, a, x ).

(x, a, x ) =

(29)

By introducing the shorthand V (x) := maxa Q(x, a), we can write the posterior distribution over  as
P (|at , ot ) = N(, 1/S)
(30)
where
 =

1 X
(x, a, x )((x, a, x )  Q(x, a) + V (x )),
S
x,a,x
X
S=
(x, a, x ).
x,a,x

The posterior distribution over the Q-values is more difficult to obtain, because each
Q(x, a) enters the posterior distribution both linearly and non-linearly through . However,
if we fix Q(x, a) within the max operations, which amounts to treating each V (x) as a
constant within a single Gibbs step, then the conditional distribution can be approximated
by


P (Q(x, a)|at , ot )  N Q(x, a), 1/S(x, a)
(31)
where

Q(x, a) =

X
1
(x, a, x )((x, a, x )   + V (x )),
S(x, a) 
x
X
(x, a, x ).
S(x, a) =
x

We expect this approximation to hold because the resulting update rule constitutes a contraction operation that forms the basis of most stochastic approximation algorithms (Mahadevan, 1996). As a result, the Gibbs sampler draws all the values from normal distributions. In each cycle of the adaptive controller, one can carry out several Gibbs sweeps to
obtain a sample of m to improve the mixing of the Markov chain. However, our experimental
results have shown that a single Gibbs sweep per state transition performs reasonably well.
Once a new parameter vector m is drawn, the Bayesian control rule proceeds by taking the
optimal action given by Equation 20. Note that only the  and  entries of the transitions
that have occurred need to be represented explicitly; similarly, only the Q-values of visited
states need to be represented explicitly.
508

fiA Minimum Relative Entropy Principle for Learning and Acting

References
Auer, P., CesaBianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed
bandit problem. Machine Learning, 47, 235256.
Bertsekas, D. (1987). Dynamic Programming: Deterministic and Stochastic Models.
Prentice-Hall, Upper Saddle River, NJ.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
Braun, D. A., & Ortega, P. A. (2010). A minimum relative entropy principle for adaptive
control in linear quadratic regulators. In The 7th conference on informatics in control,
automation and robotics, Vol. 3, pp. 103108.
Cesa-Bianchi, N., & Lugosi, G. (2006). Prediction, Learning and Games. Cambridge University Press.
Dawid, A. P. (2010). Beware of the DAG!. Journal of Machine Learning Research, (to
appear).
Dearden, R., Friedman, N., & Andre, D. (1999). Model based bayesian exploration. In
In Proceedings of Fifteenth Conference on Uncertainty in Artificial Intelligence, pp.
150159.
Dearden, R., Friedman, N., & Russell, S. (1998). Bayesian q-learning. In AAAI
98/IAAI 98: Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, pp. 761768. American Association for Artificial Intelligence.
Duda, R. O., Hart, P. E., & Stork, D. G. (2001). Pattern Classification (Second edition).
Wiley & Sons, Inc.
Duff, M. O. (2002). Optimal learning: computational procedures for bayes-adaptive markov
decision processes. Ph.D. thesis. Director-Andrew Barto.
Grunwald, P. (2007). The Minimum Description Length Principle. The MIT Press.
Haruno, M., Wolpert, D., & Kawato, M. (2001). Mosaic model for sensorimotor learning
and control. Neural Computation, 13, 22012220.
Haussler, D., & Opper, M. (1997). Mutual information, metric entropy and cumulative
relative entropy risk. The Annals of Statistics, 25, 24512492.
Hutter, M. (2002). Self-optimizing and pareto-optimal policies in general environments
based on bayes-mixtures. In COLT.
Hutter, M. (2003). Optimality of universal Bayesian prediction for general loss and alphabet.
Journal of Machine Learning Research, 4, 971997.
Hutter, M. (2004a). Online prediction  bayes versus experts. Tech. rep.. Presented at
the EU PASCAL Workshop on Learning Theoretic and Bayesian Inductive Principles
(LTBIP-2004).
Hutter, M. (2004b). Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, Berlin.
509

fiOrtega & Braun

Kappen, B., Gomez, V., & Opper, M. (2010). Optimal control as a graphical model inference
problem. JMLR (to appear).
MacKay, D. J. C. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.
Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms,
and empirical results. Machine Learning, 22 (1-3), 159195.
Mahoney, M. V. (1999). Text compression as a test for artificial intelligence. In AAAI/IAAI,
pp. 486502.
Narendra, K., & Thathachar, M. A. L. (1974). Learning automata - a survey. IEEE
Transactions on Systems, Man, and Cybernetics, SMC-4 (4), 323334.
Nozick, R. (1969). Newcombs problem and two principles of choice. In Rescher, N. (Ed.),
Essays in Honor of Carl G. Hempel, pp. 114146. Reidel.
Opper, M. (1998). A bayesian approach to online learning. Online Learning in Neural
Networks, 363378.
Ortega, P. A., & Braun, D. A. (2010). A bayesian rule for adaptive control based on causal
interventions. In The third conference on artificial general intelligence, pp. 121126.
Pearl, J. (2000). Causality: Models, Reasoning, and Inference. Cambridge University Press,
Cambridge, UK.
Poland, J., & Hutter, M. (2005). Defensive universal learning with experts. In ALT.
Rasmussen, C. E., & Deisenroth, M. P. (2008). Recent Advances in Reinforcement Learning,
Vol. 5323 of Lecture Notes on Computer Science, LNAI, chap. Probabilistic Inference
for Fast Learning in Control, pp. 229242. Springer-Verlag.
Robbins, H. (1952). Some aspects of the sequential design of experiments. Bulletin American
Mathematical Socierty, 58, 527535.
Russell, S., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach (3rd edition).
Prentice-Hall.
Schmidhuber, J. (2009). Simple algorithmic theory of subjective beauty, novelty, surprise,
interestingness, attention, curiosity, creativity, art, science, music, jokes. Journal of
SICE, 48 (1), 2132.
Shafer, G. (1996). The art of causal conjecture. The MIT Press.
Singh, S. P. (1994). Reinforcement learning algorithms for average-payoff markovian decision
processes. In National Conference on Artificial Intelligence, pp. 700705.
Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, Prediction and Search (2nd
edition). Springer-Verlag, New York.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA.
Todorov, E. (2006). Linearly solvable markov decision problems. In Advances in Neural
Information Processing Systems, Vol. 19, pp. 13691376.
510

fiA Minimum Relative Entropy Principle for Learning and Acting

Todorov, E. (2009). Efficient computation of optimal actions. Proceedings of the National
Academy of Sciences U.S.A., 106, 1147811483.
Toussaint, M., Harmeling, S., & Storkey, A. (2006). Probabilistic inference for solving
(po)mdps. Tech. rep. EDI-INF-RR-0934, University of Edinburgh.
Watkins, C. (1989). Learning from Delayed Rewards. Ph.D. thesis, University of Cambridge,
Cambridge, England.
Wyatt, J. (1997). Exploration and Inference in Learning from Reinforcement. Ph.D. thesis,
Department of Artificial Intelligence, University of Edinburgh.

511

fiJournal of Artificial Intelligence Research 38 (2010) 307-338

Submitted 01/10; published 06/10

Fast Set Bounds Propagation Using a BDD-SAT Hybrid
Graeme Gange
Peter J. Stuckey

ggange@csse.unimelb.edu.au
pjs@csse.unimelb.edu.au

National ICT Australia, Victoria Laboratory
Department of Computer Science and Software Engineering
The University of Melbourne, Vic. 3010, Australia

Vitaly Lagoon

lagoon@cadence.com

Cadence Design Systems
270 Billerica Rd, Chelmsford, MA 01824, USA

Abstract
Binary Decision Diagram (BDD) based set bounds propagation is a powerful approach
to solving set-constraint satisfaction problems. However, prior BDD based techniques incur the significant overhead of constructing and manipulating graphs during search. We
present a set-constraint solver which combines BDD-based set-bounds propagators with
the learning abilities of a modern SAT solver. Together with a number of improvements
beyond the basic algorithm, this solver is highly competitive with existing propagation
based set constraint solvers.

1. Introduction
It is often convenient to model a constraint satisfaction problem (CSP) using finite set
variables and set relationships between them. A common approach to solving finite domain
CSPs is using a combination of backtracking search and a constraint propagation algorithm.
The propagation algorithm attempts to enforce consistency on the values in the domains
of the constraint variables by removing values from the domains of variables that cannot
form part of a complete solution to the system of constraints. The most common level
of consistency is set bounds consistency (Gervet, 1997) where the solver keeps track for
each set of which elements are definitely in or out of the set. Many solvers use set bounds
consistency including ECLiPSe (IC-PARC, 2003), Gecode (GECODE, 2008), and ILOG
SOLVER (ILOG, 2004).
Set bounds propagation is supported by solvers since stronger notions of propagation
such as domain propagation require representing exponentially large domains of possible
values. However, Lagoon and Stuckey (2004) demonstrated that it is possible to use reduced
ordered binary decision diagrams (BDDs) as a compact representation of both set domains
and of set constraints, thus permitting set domain propagation. A domain propagator
ensures that every value in the domain of a set variable can be extended to a complete
assignment of all of the variables in a constraint. The use of the BDD representation comes
with several additional benefits. The ability to easily conjoin and existentially quantify
BDDs allows the removal of intermediate variables, thus strengthening propagation, and
also makes the construction of propagators for global constraints straightforward.
Given the natural way in which BDDs can be used to model set constraint problems,
it is therefore worthwhile utilising BDDs to construct other types of set solver. Indeed
c
2010
AI Access Foundation. All rights reserved.

fiGange, Stuckey, & Lagoon

it has been previously demonstrated (Hawkins, Lagoon, & Stuckey, 2004, 2005) that set
bounds propagation can be efficiently implemented using BDDs to represent constraints
and domains of variables. A major benefit of the BDD-based approach is that it frees
us from the need to laboriously construct set bounds propagators for each new constraint
by hand. Moreover, correctness and optimality of such BDD-based propagators follow by
construction. The other advantages of the BDD-based representation identified above still
apply, and the resulting solver performs very favourably when compared with existing set
bounds solvers.
But set bounds propagation using BDDs still constructs BDDs during propagation,
which is a considerable overhead. In this paper we show how we can perform BDD-based
set bounds propagation using a marking algorithm that perform linear scans of the BDD
representation of the constraint without constructing new BDDs. The resulting set bounds
propagators are substantially faster than those using BDDs.
The contributions of this paper are:
 Efficient set bounds propagators: No new BDDs are constructed during propagation, so it is very fast.
 Graph reuse: We can reuse a single BDD for multiple copies of the same constraint,
and hence handle larger problems.
 Ordering flexibility: We are not restricted to a single global ordering of Booleans
for constructing BDDs.
 Filtering: We can keep track of which parts of the set variable can really make a
difference, and reduce the amount of propagation.
Pure set-bounds propagation tends to perform badly, however, in problems where a large
number of similar regions of the search space must be explored. We therefore embed the
set-bounds propagators in MiniSAT (Een & Sorensson, 2003), to provide SAT-style clause
learning.
In the next section, we introduce propagation-based solving for set problems, and briefly
discuss SAT solving. In Section 3 we discuss binary decision diagrams (BDDs) and how
to implement set bounds propagation using BDDs. Then in Section 4, we present the
propagation algorithm used by the hybrid solver, together with a number of variations
upon the standard algorithm. In Section 5, we show how to incorporate reason generation
with BDD propagation to build a hybrid solver In Section 6 we test the performance of
the solver on a variety of set-constraint problems, and compare with other set-constraint
solvers. In Section 7 we discuss related work, before concluding in Section 8.

2. Propagation-based Solving
Propagation based approaches to solving set constraint problems represent the problem
using a domain storing the possible values of each set variable, and propagators for each
constraint, that remove values from the domain of a variable that are inconsistent with values for other variables. Propagation is combined with backtracking search to find solutions.
308

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

A domain D is a complete mapping from the fixed finite set of variables V to finite
collections of finite sets of integers. The domain of a variable v is the set D(v). A domain
D1 is said to be stronger than a domain D2 , written D1  D2 , if D1 (v)  D2 (v) for all
v  V. A domain D1 is equal to a domain D2 , written D1 = D2 , if
V D1 (v) = D2 (v) for all
variables v  V. A domain D can be interpreted as the constraint vV v  D(v).
For set constraints we will often be interested in restricting variables to take on convex
domains. A set of sets K is convex if a, b  K and a  c  b implies c  K. We use
interval notation [a, b] where a  b to represent the (minimal) convex set K including a
and b. For any finite collection of sets K = {a1 , a2 , . . . , an }, we define the convex closure
of K: conv (K) = [aK a, aK a]. We extend the concept of convex closure to domains by
defining ran(D) to be the domain such that ran(D)(v) = conv (D(v)) for all v  V.
A valuation  is a set of mappings from the set of variables V to sets of integer values,
written {v1 7 d1 , . . . , vn 7 dn }. A valuation can be extended to apply to constraints
involving the variables in the obvious way. Let vars be the function that returns the set
of variables appearing in an expression, constraint or valuation. In an abuse of notation,
we say a valuation is an element of a domain D, written   D, if (vi )  D(vi ) for all
vi  vars().
2.1 Constraints, Propagators and Propagation
A constraint is a restriction placed on the allowable values for a set of variables. We shall
use primitive set constraints such as (membership) k  v, (equality) u = v, (subset) u  w,
(union) u = v  w, (intersection) u = v  w, (cardinality) |v| = k, (upper cardinality bound)
|v|  k, (lexicographic order) u < v, where u, v, w are set variables, k is an integer. We can
also construct more complicated constraints which are (possibly existentially quantified)
conjunctions of primitive set constraints. We define the solutions of a constraint c to be
the set of valuations  on vars(c) that make the constraint true.
We associate a propagator with every constraint. A propagator f is a monotonically
decreasing function from domains to domains, so D1  D2 implies that f (D1 )  f (D2 ),
and f (D)  D. A propagator f is correct for a constraint c if and only if for all domains
D: { |   D}  solns(c) = { |   f (D)}  solns(c)
A propagation solver solv (F, D) for a set of propagators F and a domain D repeatedly applies the propagators in F starting from the domain D until a fixpoint is reached.
solv (F, D) is the weakest domain D   D where f (D  ) = D  for all f  F .
Example 1 A small example of a set-constraint problem would be to, given a universe
consisting of the elements {1, 2, 3, 4}, find values for variables x, y, z such that z = x  y,
|x| = 3, |y| = 3, |z| = 2, 3 
/ z, 1  z and 2 
/ y.
The unique solution to this problem is  = {x 7 {1, 2, 4}, y 7 {1, 3, 4}, z 7 {1, 4}}.
2.2 Set Bounds Consistency
A domain D is (set) bounds consistent for a constraint c if for every variable v  vars(c)
the upper bound of D(v) is the union of the values of v in all solutions of c in D, and the
lower bound of D(v) is the intersection of the values of v in all solutions of c in D. We
309

fiGange, Stuckey, & Lagoon

define the set bounds propagator for a constraint c as
(
{i |     solns(D  c)  i  (v)} if v  vars(c)
ub(c)(D)(v) =
ub(v)
otherwise
lb(c)(D)(v) =

(

{i |     solns(D  c)  i  (v)}
lb(v)

if v  vars(c)
otherwise

sb(c)(D)(v) = [lb(c)(D)(v), ub(c)(D)(v)]
Then sb(c)(D) is always bounds consistent with c.
Example 2 Continuing the example from the previous section, the initial bounds of the
variables x, y, z are D(x) = D(y) = D(z) = [, {1, 2, 3, 4}], as no values are explicitly
included or excluded from the domains. As first 3 
/ z is added, then 1  z and finally
2 
/ y, the bounds are reduced, and the consequences of these changes are propagated
among the variables as follows:
Propagator
D(x)
D(y)
D(z)
[, {1, 2, 3, 4}]
[, {1, 2, 3, 4}]
[, {1, 2, 3, 4}]
3
/z
[, {1, 2, 3, 4}]
[, {1, 2, 3, 4}]
[, {1, 2, 4}]
1z
[, {1, 2, 3, 4}]
[, {1, 2, 3, 4}]
[{1}, {1, 2, 4}]
z =xy
[{1}, {1, 2, 3, 4}]
[{1}, {1, 2, 3, 4}]
[{1}, {1, 2, 4}]
2
/y
[{1}, {1, 2, 3, 4}]
[{1}, {1, 3, 4}]
[{1}, {1, 2, 4}]
|y| = 3
[{1}, {1, 2, 3, 4}]
[{1, 3, 4}, {1, 3, 4}] [{1}, {1, 2, 4}]
z =xy
[{1}, {1, 2, 4}]
[{1, 3, 4}, {1, 3, 4}]
[{1}, {1, 4}]
|z| = 2
[{1}, {1, 2, 4}]
[{1, 3, 4}, {1, 3, 4}] [{1, 4}, {1, 4}]
|x| = 3
[{1, 2, 4}, {1, 2, 4}] [{1, 3, 4}, {1, 3, 4}] [{1, 4}, {1, 4}]
Once 1  z is fixed, 1 is added to lb(z). Since z = x  y, any element in lb(z) must
also be in lb(x) and lb(y). Once 2 
/ y has been set, |ub(y)| = 3 and since |ub(y)|  |y| = 3
this means y = ub(y) = {1, 3, 4}. This means that 2 
/ z since z = x  y. Since 3 
/ ub(z),
at least one of x or y must not contain 3. Once 3  lb(y) has set, it can be determined
that 3 
/ ub(x). Since |ub(z)| = 2 this forces z = ub(z) = {1, 4}. Finally the constraint
|x| = 3 then results in the value of x becoming fixed. The corresponding valuation is
 = {x 7 {1, 2, 4}, y 7 {1, 3, 4}, z 7 {1, 4}}, which is the solution provided in Example 1.
2.3 Boolean Satisfiability (SAT)
Boolean Satisfiability or SAT solvers are a special case of propagation-based solvers, restricted to Boolean variables and clause constraints.
The Davis-Putnam-Logemann-Loveland (DPLL) algorithm (Davis, Logemann, & Loveland, 1962), on which most modern SAT solvers are based, is also a propagation-based
approach to solving SAT problems. It interleaves two phases  search, where an unfixed
variable is assigned a value, and propagation (so called unit propagation).
Modern SAT solvers incorporate sophisticated engineering to propagate constraints very
fast, to record as nogoods part of the search that lead to failure, and to automate the search
310

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

SAT Engine

Search

conflict
analysis

unit propagation

Clause Database

Figure 1: Architecture for the SAT solver.
by keeping track of how often a variable is part of the reason for causing failure (activity) and
concentrating search on variables with high activity. Modern SAT solvers also frequently
restart the search from scratch relying on nogoods recording to prevent repeated search,
and activity to drive the search into more profitable areas. See e.g., the report by Een and
Sorensson (2003) for a good introduction to modern SAT solving.
A rough architecture of a modern SAT solver is illustrated in Figure 1. Search starts the
unit propagation process which interacts with the clause database and may detect failure,
which initiates conflict analysis. Unit propagation records for each literal that is made
true, the clause that explains why the literal become true. Conflict analysis uses the graph
of explanations to construct a nogood which is a resolvent of clauses causing the failure
that adds to the strength of unit propagation. This is stored in the clause database and
causes search to backjump. It prevents the search revisiting the same set of decisions. Not
detailed here are activity counters which record which variables are most responsible for
failure, these are the variables chosen for labelling by the search.

3. Binary Decision Diagrams
We assume a set B of Boolean variables with a total ordering . A Boolean variable can
take the value 0 (false) or 1 (true). We make use of the following Boolean operations:
 (conjunction),  (disjunction),  (negation),  (implication),  (bi-implication) and
 (existential quantification). We denote by V F the formula x1    xn F where V =
 V F we mean V  F where V  = vars(F ) \ V .
{x1 , . . . , xn }, and by 
Reduced Ordered Binary Decision Diagrams are a well-known method of representing
Boolean functions on Boolean variables using directed acyclic graphs with a single root.
Every internal node n(v, f, t) in a BDD r is labelled with a Boolean variable v  B, and
has two outgoing arcs  the false arc (to BDD f ) and the true arc (to BDD t). Leaf
nodes are either F (false) or T (true). Each node represents a single test of the labelled
variable; when traversing the tree the appropriate arc is followed depending on the value of
311

fiGange, Stuckey, & Lagoon

7654
0123
v3 E
EE
"
7654
0123

v4

|
7654
0123
v5

|
7654
0123
v6 E
EE
"
7654
0123
v7 DD
D!
"  }r t

F

T

7654
0123
x1 F
FF
|
"
7654
0123
7654
0123
x2 F
x2 F
FF
FF
" |
"
|
7654
0123
7654
0123
7654
0123
x3 F
x3
x3 F
FF
FF
"7654
"
|
|
0123
7654
0123
x4 F
x4
FF
"7654
|
0123
x5 EE
E" 
 }
"  |

T

(a)

F

(b)

Figure 2: BDDs for (a) v3  v4  v5  v6  v7 . (b) x1 + x2 + x3 + x4 + x5  2. The node
n(v, f, t) is shown as a circle labelled v with a dotted arc to the f BDD, and a solid arc to
the t BDD.

the variable. Define the size |r| as the number of internal nodes in a BDD r, and VAR(r)
as the set of variables v  B appearing in some internal node in r.
Reduced Ordered Binary Decision Diagrams (BDDs) (Bryant, 1986) require that the
BDD is: reduced, that is it contains no identical nodes (nodes with the same variable label
and identical true and false arcs) and has no redundant tests (no node has both true and
false arcs leading to the same node); and ordered, if there is an arc from a node labelled
v1 to a node labelled v2 then v1  v2 . A BDD has the nice property that the function
representation is canonical up to variable reordering. This permits efficient implementations
of many Boolean operations.
A Boolean variable v is said to be fixed in a BDD r if either for every node n(v, f, t)  r t
is the constant F node, or for every node n(v, f, t) f is the constant F node. Such variables
can be identified in a linear time scan over the domain BDD (see e.g., Hawkins et al.,
2005). For convenience, if  is a BDD, we write JK to denote the BDD representing the
conjunction of the fixed variables of .

Example 3 Figure 2(a) gives an example of a BDD representing the formula v3  v4 
v5 v6 v7 . Figure 2(b) gives an example of a more complex BDD representing the formula
x1 + x2 + x3 + x4 + x5  2 where we interpret the Booleans as 0-1 integers. One can verify
that the valuation {x1 7 1, x2 7 0, x3 7 1, x4 7 0, x5 7 0} makes the formula true by
following the path right, left, right, left, left from the root.
3.1 Set Propagation using BDDs
The key step in building set propagation using BDDs is to realize that we can represent a
finite set domain using a BDD.
3.1.1 Representing domains
If v is a set variable ranging over subsets of {1, . . . , N }, then we can represent v using the
Boolean variables V (v) = {v1 , . . . , vN }  B, where vi is true iff i  v. We will order the
312

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

variables v1  v2     vN . We can represent a valuation  using a formula

R() =

^

vvars()




^

^

vi 

i(v)

i{1,...,N }(v)



vi  .

W
Then a domain of variable v, D(v) can be represented as  = aD(v) R({v 7 a}). This
formula can be represented by a BDD. The set bounds of v can be obtained by extracting
the fixed variables from this BDD, JK.
For example the valuation  of Example 1 is represented by the formula R():
x1  x2  x3  x4  y1  y2  y3  y4  z1  z2  z3  z4 .
And the domain D(v) = [{3, 6, 7}, {1, 2, 3, 6, 7, 8, 9}] is represented by the BDD in Figure 2(a) since v3 , v6 and v7 are true so 3, 6, 7 are definitely in the set, and v4 and v5 are
false so 4 and 5 are definitely not in the set.
3.1.2 Representing constraints
We can similarly model any set constraint c as a BDD B(c) using the Boolean variable
representation V (v) of its set variables v. By ordering the variables in each BDD carefully
we can build small representations of the formulae. The pointwise order of Boolean variables
is defined as follows. Given set variables u  v  w ranging over sets from {1, . . . , N } we
order the Boolean variables as u1  v1  w1  u2  v2  w2     uN  vN  wN .
The representation B(c) is simply solns(c) R(). For primitive set constraints (using
the pointwise order) this size is linear in N . For more details see the work of Hawkins et al.
(2005). The BDD representation of |x|  2 is shown in Figure 2(b), for N = 5.
3.1.3 BDD-based Set Bounds Propagation
We can build a set bounds propagator, more or less from the definition, since we have BDDs
to represent domains and constraints.
 = B(c) 

^

D(v  )

v vars(c)

sb(c)(D)(v) = V (v) JK
We simply conjoin the domains to the constraint obtaining , then extract the fixed variables
from the result, and then project out the relevant part for each variable v. The set bounds
propagation can be improved by removing the fixed variables as soon as possible. The
improved definition is given by Hawkins et al. (2004). Overall the complexity can be made
O(|B(c)|).
The updated set bounds can be used to simplify the BDD representing the propagator.
Since fixed variables will never interact further with propagation they can be projected out
of B(c), so we can replace B(c) by VAR(JK) .
313

fiGange, Stuckey, & Lagoon

bdd2sat(node) {
switch node {
F: return (0, {}) ;
T : return (1, {});
n(v, f, t):
if (visit[node] 6= ) return(visit[node],{});
let n be a new Boolean variable;
visit[node] = n ;
(f  , Cf ) = bdd2sat(f );
(t , Ct ) = bdd2sat(t);
return (n , {v  t  n , v  f   n , v  t  n , v  f   n ,
t  f   n , t  f   n }  Cf  Ct );
}
}
Figure 3: Pseudo-code for Tseitin transformation of BDD rooted at node where n is the
Boolean variable encoding the truth value of node.
3.2 Tseitin Transformation
It is possible to convert any Boolean circuit to a pure SAT representation; the method
for doing so is generally attributed to Tseitin (1968). Figure 3 gives pseudo code for the
translation of a BDD rooted at node, returning a pair of (Boolean variable, set of clauses).
The clauses enforce that the Boolean variable takes the truth value of the BDD. Like most
BDD algorithms it relies on marking the visited nodes to ensure each node is visited at
most once. It assumes the array visit[] is initially all bottom , and on first visiting a node
stores the corresponding Boolean variable in visit[]. A more comprehensive discussion of
the Tseitin transformation is presented by Een and Sorensson (2006).
The constraint is enforced by fixing the variable corresponding to the root node to
true. An advantage of replacing a BDD by its Tseitin representation is that we can use an
unmodified SAT solver to then tackle BDD-based set constraint problems. We shall see in
Section 6 that this approach cannot compete with handling the BDDs directly.

4. Faster Set-bounds Propagation
While set bounds propagation using BDDs is much faster than set domain propagation and
often better than set domain propagation (or other variations of propagation for sets) it
still creates new BDDs. This is not necessary as long as we are prepared to give up the
simplifying of BDDs that is possible in set bounds propagation.
We do not represent domains of variables as BDDs, but rather as arrays of Boolean
domains. A domain D is an array where, for variable v ranging over subsets of {1, . . . , N }:
0
/ D[vi ] indicates i  v, and 1 
/ D[vi ] indicates i 
/ v. If D[vi ] = {0, 1}, we dont know
whether i is in or not in v. Hence D(v) = [{i|0 
/ D[vi ]}, {i|1  D[vi ]}].
The BDD representation of a constraint B(c) is built as before. A significant difference
is that since constraints only communicate through the set bounds of variables we do not
314

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

need them to share a global variable order hence we can if necessary modify the variable
order used to construct B(c) for each c, or use automatic variable reordering (which is
available in most BDD packages) to construct B(c). Another advantage is that we can
reuse the BDD for a constraint c(x) on variables x for the constraint c(y) on variables y
(as long as they range over the same initial sets), that is, the same constraint on different
variables. Hence we only have to build one such BDD, rather than one for each instance of
the constraint.
The set bounds propagator sb(c(x)) for constraint c(x) is now implemented as follows. A
generic BDD representation r of the constraint c(y) is constructed. The propagator copies
the domain description of the actual parameters x1 , . . . , xn onto a domain description E
for formal parameters y 1 , . . . , y n . It constructs an array E where E[yij ] = D[xji ]. Let
V = {yij | 1  j  n, 1  i  N } be the set of Boolean variables occurring in the
constraint c(y). The propagator executes the code bddprop(r, V, E) shown in Figures 4 and
5 which returns (r  , V  , E  ). If r  = F the propagator returns a false domain, otherwise the
propagator copies back the domains of the formal parameters to the actual parameters so
D[xji ] = E[yij ]. We will come back to the V  argument in the next subsection.
The procedure bddprop(r, V, E) traverses the BDD r as follows. We visit each node
n(v, f, t) in the BDD in a top-down memoing manner. We record if, under the current
domain, the node can reach the F node, and if it can reach the T node. If the f child
can reach the T node we add support for the variable v taking value 0. Similarly if the t
child can reach T we add support for the variable v taking 1. If the node can reach both
F and T we record that the variable v matters to the computation of the BDD. After the
visit we reduce the variable set for the propagator to those that matter, and remove values
with no support from the domain. The procedure assumes a global time variable which is
incremented between each propagation, which is used to memo the marking phase. The
top(n, V ) function returns the variable in the root node of n or the largest variable (under
) in V if n = T or n = F.
As presented bddprop has time complexity O(|r|  |V |) where |r| is the number of nodes
appearing in BDD r. In practice the complexity is O(|r| + |V |) since the |V | factor arises
from handling long arcs, where a node n(v, f, t) has a child node (f or t) are labelled by
a Boolean different from that next in the order  after v. For set constraints the length of
a long arc is typically bounded by the arity of the set constraint. It is possible to create
a version of bddprop which is strictly O(|r|) by careful handling of long arcs. We did so,
but in practice it was slower than the form presented here. bddprop has space complexity
O(|V | + |r|) the first component for maintaining the domains of variables and the second
for memoing the BDD nodes.
Example 4 Consider the BDD for the constraint x = y  z when N = 2 shown in Figure 6(a). Assuming a domain E where E[y1 ] = {1} (1  y) and E[z2 ] = {1} (2  z),
and the remaining variables take value {0, 1}, the algorithm traverses the edges shown with
double lines in Figure 6(b). No path from x1 , or x2 following the f arc reaches T hence 0
is not added to E  [x1 ] or E  [x2 ]. As a result E[x1 ] and E[x2 ] are set to {1}. Hence we have
determined 1  x and 2  x.
Also, no nodes for z1 are actually visited, and the left node for y2 only reaches F and
the right node only reaches T . Hence matters[z1 ] and matters[y2 ] are not marked with the
315

fiGange, Stuckey, & Lagoon

bddprop(r,V ,E) {
for (v  V ) {
E  [v] = {};
}
(reachf , reacht ) = bddp(r, V, E);
if (reacht ) return (F, , E);
vars = ;
for (v  V ) {
if (E  [v] 6= E[v]) {
E[v] = E  [v];
}
if (E[v] = {0, 1}  matters[v]  time) vars = vars  {v};
}
return (r, vars , E);
}
Figure 4: Pseudo-code for BDD-propagation.
current time. The set of vars collected by bddprop is empty, since the remaining variables
are fixed.
4.1 Waking up Less Often
In practice a bounds propagation solver does not blindly apply each propagator until fixpoint, but keeps track of which propagators must still be at fixpoint, and only executes
those that may not be. For set bounds this is usually managed as follows. To each set
variable v is attached a list of propagators c that involve v. Whenever v changes, these
propagators are rescheduled for execution.
We can do better than this with the BDD based propagators. The algorithm bddprop
collects the set of Boolean variables that matter to the BDD, that is can change the result.
If a variable that does not matter becomes fixed, then set bounds propagation cannot learn
any new information. We modify the wakeup process as follows. Each propagator stores
a list vars of Boolean variables which matter given the current domain. When a Boolean
variable xji becomes fixed we traverse the list of propagators involving xji and wake those
propagators where xji occurs in vars. On executing a propagator we revise the set vars
stored in the propagator. Note the same optimization could be applied to the standard
approach, but requires the overhead of computing vars which here is folded into bddprop.
It is possible to instead do propagator wake-up on literals, rather than variables. In this
case, we observe that fixing a variable v to true matters to a node n(v, f, t) iff T is reachable
from f and F is reachable from t  the converse holds for v. In terms of the pseudo-code
in Figure 5, the line
if (reachf  reacht ) matters[v] = time;
may therefore be replaced with
316

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

bddp(node,V ,E) {
 if (in set(fset, node)) { return (1, 0)};
switch node {
F: return (1,0);
T : return (0,1);
n(v, f, t):
if (visit[node]  time) return save[node];
reachf = 0; reacht = 0;
if (0  E[v]) {
(rf0 , rt0 ) = bddp(f, V, E);
reachf = reachf  rf0 ;
reacht = reacht  rt0 ;
if (rt0 ) {
for (v   V, v  v   top(f, V ))
E  [v  ] = E[v  ];

E [v] = E  [v]  0;
}
}
if (1  E[v]) {
(rf1 , rt1 ) = bddp(t, V, E);
reachf = reachf  rf1 ;
reacht = reacht  rt1 ;
if (rt1 ) {
for (v   V, v  v   top(t, V ))
E  [v  ] = E[v  ];

E [v] = E  [v]  1;
}
}
if (reachf  reacht ) matters[v] = time;
save[node] = (reachf , reacht );  if (reacht) { insert(fset, node) };
visit[node] = time;
return (reachf , reacht );
}
}
Figure 5: Pseudo-code for processing the constraint graph during propagation. Modifications necessary for using dead-subgraph memoization are shown on the right.

if (rt0  rf 1) matters[v] = time;
if (rt1  rf 0) matters[v] = time;

While this allows for propagators to wake up less frequently, propagator execution is slower
due to keeping track of additional reachability information.
317

fiGange, Stuckey, & Lagoon

7654
0123
x1 C
CC
CC
C!
7654
0123
y1
 
 
0123
z
 7654

{ 1
 {{{
! }{{
7654
0123
x2 C
CC
CC
C!
7654
0123
y2


 
 7654
0123
z2

 {{{

{

!  }{{

}
7654
0123
y1

7654
0123
z1

 

 
}
 7654
0123
y
   2
 

   7654
0123
z
  || 2
|
 ff~|vt |

F

T

(a)

7654
0123
x1CCC
CCCC
CCCC
C %
7654
0123
y1



 
 7654
z1

 0123
 {{{

{
! 
 }{{
7654
0123
x2CCC
CCCC
CCCC
C %
7654
0123
y2



 ff
 7654
0123


 {{{z2


{
{
 {{
! 
 y {{{

y
7654
0123
y1

7654
0123
z1

 

 
y
 7654
0123
y
   2
 
ff
   7654
0123
z
  |||| 2
 |||

 ff
 zvt |||

F

T

(b)

Figure 6: (a) The BDD representing x = y  z where N = 2. (b) The edges traversed
by bddprop, when E[y1 ] = {1} and E[z2 ] = {1} and E[v] = {0, 1} otherwise, are shown
doubled.
4.2 Dead Subgraph Memoization and Shortcutting
The algorithm as presented above always explores all reachable parts of the graph in order
to determine the set of supported values. However, a number of improvements for MultiDecision Diagrams (MDDs) were presented by Cheng and Yap (2008) which reduce the
portion of the graph which must be traversed in order to enforce consistency. These are dead
subgraph memoization, which avoids traversal of subgraphs which cannot provide support
for any values, and shortcutting, which recognizes situations where it is only necessary to
find one path to T to ensure consistency. These can readily be adapted to a BDD-based
set constraint solver.
4.2.1 Dead Subgraph Memoization
The key observation for dead subgraph memoization is that, as search progresses, paths
along the graph to T are only ever removed. As such, if T becomes unreachable from
a node n, the subgraph incident from n need never again be explored until the solver
backtracks. Thus, if the set of dead nodes can be maintained, it is possible to progressively
eliminate subgraphs during propagation.
We keep for each instance of a constraint c(x) a failure set, fset which records which
nodes can not reach T (and hence are equivalent to F). During propagation, once a node n
is shown to have no path to T , it is added to the failure set fset. When a node is processed,
we first check if it is in fsetif so, we terminate early, otherwise we proceed as normal.
The modifications necessary for this are shown on the right in Figure 5. For simplicity the
pseudo-code treats fset as a global.
A method for efficiently maintaining the failure sets was presented by Cheng and Yap
(2008), which uses sparse-set data structures to provide efficient lookup, insertion and backtracking. The set fset is maintained as a pair of arrays: sparse and dense and a counter
318

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

insert(S, n) {
S.sparse[n] = S.members
S.dense[S.members] = n
S.members++
}
in set(S, n) {
index = S.sparse[n]
return index < S.members
 S.dense[index] == n
}
(a) Sparse set operations

insert(S, n) {
old index = S.sparse[n]
swap value = S.dense[S.members]
S.sparse[n] = S.members
S.dense[S.members] = n
S.sparse[swap value] = old index
S.dense[old index] = swap value
S.members++
}
in set(S, n) {
return S.sparse[n] < S.members
}
(b) Modified sparse-set operations

Figure 7: Pseudo-code for conventional sparse-set operations, and the corresponding modified versions.

members. n  fset if sparse[n] < members and dense[sparse[n]] = n. The operations for
insertion and testing are shown in Figure 7(a). Crucially we can backtrack to earlier forms
of the set simply by resetting members to its value at that time.
These structures can be improved slightly by the observation that checking membership
will occur significantly more often than insertion. Pseudo-code for the modified sparse-set
operations are given in Figure 7(b). While insertion operations become more expensive, the
overall computation time is reduced.

Example 5 Consider the set illustrated in Figure 8(a). The elements in the set are {1, 7}.
We can determine that the element 4 is not in the set S0 , as sparse[4] is not strictly less
than members, indicated by the arrow in Figure 8(a).
To insert an element v using the standard sparse-set operations, we merely overwrite
dense[members] with v, and set the value of sparse[v] to members. This is shown in
Figure 8(b), inserting 3 into S0 . At this point, both sparse[3] and sparse[4] have the value
2. To test if 4  S0 , it is not sufficient to determine if sparse[4] < members. One must also
check that dense[sparse [4]] = 4.
When inserting v using the modified operations, as illustrated in Figure 8(c), we swap
the values of sparse[v] and sparse [dense[members]], and likewise switch the values of
dense[members] and dense[sparse [v]]. This maintains the property that v  S  sparse[v] <
members.
319

fiGange, Stuckey, & Lagoon

0
sparse

dense

1

2

0

1

7

3

4

6

2

5

6

0

7
sparse

1

4

1
0

dense

3

2

1

7

3

4

2

2

3

2

5

6

7
1

3

3

(a) S0 = {1, 7}

(b) S0  {3}
0

sparse

dense

1

2

0

1

7

3

4

2

6

3

5

6

7
1

4

3

(c) S0  {3} using modified operations

Figure 8: A sparse representation for sets. (a) A possible state of the data structure representing S0 = {1, 7}. (b) Inserting 3 into the data structure using the standard operations.
sparse [3] is updated to point to the next element of dense, and the corresponding entry in
dense points back to 3. Notably, both sparse [3] and sparse [4] now point to dense[2]. (c)
Inserting 3 into the data structure using the modified operations. After the operation, both
the sparse and dense arrays are maintained such that v dense[sparse[v]] = v.
Dead subgraph memoization comes with a space cost of O(|r|) to store the failure set
f set. It reduces the time complexity of bddprop to O((|r||f set|)|V |) and O(|r||f set|+
|V |) in practice.
4.2.2 Shortcutting
Shortcutting is an optimization to propagation on the BDD which notices that if all values
in the current domains of variables vi , vi+1 ,    , vN are fully supported, then we do not
need to examine the rest of the nodes involving those variables. We keep a high water mark
hwater which shows the least variable all of whose values are supported. If we ever reach a
node numbered at or below the high water mark we only need to prove that it reaches T ,
we do not need to fully explore the sub-graph below it.
A modified propagation algorithm taking into account shortcutting (and dead subgraph
minimization) is given in Figures 9 and 10. The high water mark hwater is originally larger
than the greatest variable appearing in the BDD.
The principle difference of imp bddp is that if we reach a node with variable at or below
the high water mark we use the simplified form shortcut bddp which only checks whether
the node can reach T . The only other complexity is to update the high water mark hwater
when we find all values of v are supported (E[v] = E  [v]). shortcut bddp has to be careful
to mark all variables in nodes visited that reach T as mattering to the propagator.
320

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

imp bddp(node,V ,E) {
if (in set(fset, node)) return (1, 0);
switch node {
F : return (1,0);
T : return (0,1);
n(v, f, t):
if (visit[node]  time) return save[node];
if (v  hwater) return shortcut bddp(node, V, E);
reachf = 0; reacht = 0; maxvar = v;
if (0  E[v]) {
(rf0 , rt0 ) = imp bddp(f, V, E);
reachf = rf0 ; reacht = rt0 ;
if (rt0 ) {
maxvar = top(f, V );
E  [v] = E  [v]  0;
if (hwater  top(f, V )  E  [v] == E[v]) {
hwater = v;
reachf = 1;
goto cleanup;
}
}
}
if (1  E[v]) {
(rf1 , rt1 ) = imp bddp(t, V, E);
reachf = reachf  rf1 ; reacht = reacht  rt1 ;
if (rt1 ) {
maxvar = max(maxvar, top(t, V );
E  [v] = E  [v]  1;
if (hwater  top(t, V )  E  [v] == E[v]) {
hwater = v;
}
}
}
if (reacht ):
insert(fset, node);
cleanup:
for (v  V, v  v  maxvar)
E  [v] = E[v];
if (reachf  reacht ) matters[v] = time;
save[node] = (reachf , reacht );
visit[node] = time;
return (reachf , reacht );
}
}

Figure 9: Pseudo-code for processing the constraint graph during propagation, using deadsubgraph memoization and shortcutting.
Example 6 Consider the BDD for the constraint |y  z| = 1 when N = 3 shown in
Figure 11(a). As no variables are fixed, we first explore the false paths, and find the T
node. This provides complete support for y2 , x3 , y3 , so the high-water mark is updated to
y2 . When searching for support for x2 false, we no longer need to find support for anything
beneath the high-water mark  we need only find a single path to true from the node
labelled y2 . The high water mark then increases to y1 . Likewise, when finding support for
x1 , everything below that point is already supported, so we explore only the first path to
T . The edges explored are shown doubled in 11(b).
Example 6 also illustrates that the impact of shortcutting is highly dependent on the
order in which branches are searched, and the structure of the constraint  if we were to
321

fiGange, Stuckey, & Lagoon

shortcut bddp(node,V ,E) {
if (in set(fset, node)) return (1,0);
switch node {
T : return (0,1);
n(v, f, t):
rf0 = 0;
if (visit[node]  time) return save[node];
if (0  E[v]) {
(rf0 , rt0 ) = shortcut bddp(f, V, E);
if (rt0 ) {
if (1  E[v]) { matters[v] = time; rf0 = 1; }
visit[node] = time; save[node] = (rf0 , 1);
return save[node];
}
}
if (1  E[v]) {
(rf1 , rt1 ) = shortcut bddp(t, V, E);
if (rt1 ) {
if (rf0 ) { matters[v] = time; rf1 = 1; }
visit[node] = time; save[node] = (rf1 , 1);
return save[node];
}
}
insert(fset, node);
return (1, 0);
}
}
Figure 10: Pseudo-code for the shortcut phase.

explore the true branches first, rather than the false branches, we would need to explore all
nodes to find support for all variables. Clearly shortcutting does not change the asymptotic
time or space complexity of the algorithm. Note that shortcutting for BDDs is more complex
than the approach used by Cheng and Yap (2008) since they do not treat long arcs in
MDDs.

5. Hybrid SAT Solver
Despite very fast propagation, a pure set bounds-based solver nevertheless suffers from an
inability to analyze the reasons for failure, which results in repeated exploration of similar
dead subtrees. This limits the performance of the solver on many hard problem instances.
In order to address this, we construct a hybrid solver which embeds BDD-based set
bounds propagators within an efficient SAT solver. Search and conflict analysis are per322

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

7654
0123
x1 C
CC
CC
C!
7654
0123
y1 C
CC
CC
C!
 }
7654
0123
7654
0123
x2 C
x2 C
CC
CC
CC
CC
C!
C!
7654
0123
7654
0123
y2 C
y2
CC
CC
C!  }
7654
0123
x3 C
CC
CC
C!
7654
0123
y3
 	 }t

ff  }t

 

T

7654
0123
x1CCC
CCCC
CCCC
C %
7654
0123
y1 C
CC
CC
C!
ff y
7654
0123
7654
0123
x2 C
x2CCC
CC
CCCC
CCCC
CC
C %
C!
7654
0123
7654
0123
y2 C
y2
CC
CC
C!  }
7654
0123
x3 C
CC
CC
C!
7654
0123
y3

F

 

T

(a)

F
(b)

Figure 11: (a) The BDD representing |x  y|  1 where N = 3. A node n(v, f, t) is shown
as a circle around v with a dotted arrow to f and full arrow to t. (b) The edges traversed
by imp bddp, when E[v] = {0, 1} for all v, are shown doubled.
formed in the SAT solver, and the BDD propagators are used to generate inferences and
clauses for the SAT solver to use during propagation.
5.1 Efficient Reason Generation
Key to a successful SAT solver is the recording of nogoods, small subsets of the current
variable assignments which independently result in failure. This allows similar subtrees to
be eliminated from consideration, hence significantly reducing the search space.
In order to construct nogoods, it is necessary to explain the reason why each literal was
set. in order to determine the chain of reasoning which resulted in a contradiction. In a
pure SAT solver this is easy, as each variable is either a decision variable, or associated with
a clause that caused propagation.
BDD-based propagation methods, however, do not automatically provide explanations
for inference. The naive approach for generating a reason clause for a BDD inference is
to enumerate all the fixed variables which occur in the propagator, and construct a clause
from the negations:
^
_
li  l  l 
li
li f ix(B)

li f ix(B)

Unfortunately, this often results in very large reason clauses, particularly in the case
of merged propagators or global constraints. As smaller clauses result in stronger nogoods
being generated by the SAT solver, it is preferable to determine the minimal set of variables
required to cause propagation, and include only those variables in the clause.
A method for constructing such minimal clauses was demonstrated by Hawkins and
Stuckey (2006), but this method involves constructing new BDDs, eliminating redundant
variables until the minimal BDD is constructed, then reading off the variables remaining
323

fiGange, Stuckey, & Lagoon

in the BDD. Given the propagation algorithm herein avoids expensive BDD operations, we
do not wish to use them for explanation.
Given that a set of assignments {l0 , . . . , lk } entail a literal l with respect to a constraint
C, it is also true that


^
li   
C  l 
i{0...k}

As a result, the problem of finding a minimal reason for a given inference from a BDD
is equivalent to fixing l and unfixing as many variables as possible without rendering T
reachable.
The algorithm presented by Subbarayan (2008) provides a method to do this by traversing a static graph, again avoiding the need to construct intermediate BDDs. The algorithm,
given in Figure 12, traverses each node n(v, f, t) in a top-down memoing manner. At each
node, it records if, given to the current domain, the T node is reachable. If the variable
v has been assigned a value, it also records if T is reachable from the conflicting edge;
any such edges must not become relaxed, otherwise the partial assignment is no longer a
conflict.
The graph is then traversed a second time, this time in a breadth-first manner. For each
variable v, if all nodes which have been reached corresponding to v variable may be relaxed
without opening a path to T , the v is unfixed. If the v remains fixed, v is marked as part
of the reason, and only the node corresponding to the value of v is marked as reachable.
Otherwise, v is not in the minimal reason, and both the f and t nodes are marked as
reached. The procedure returns the reason as a clause. The procedure is O(|r|) in time and
space complexity, but note this is O(|r|) per new propagation that has to be explained!
Example 7 Consider the constraint and assignments obtained in Example 4. It was determined that E[y1 ] = {1}E[z2 ] = {1}  E[x2 ] = {1} (or equivalently, 1  y2  z  2  x).
As such, the naive reason clause to explain 2  x would be y1  z2  x2 ; however, it is
possible to construct a smaller clause than this.
In order to construct the minimal reason for E[x2 ] = {1}, we first set E[x2 ] = {0}.
The corresponding graph is shown in Figure 13(a), with nodes that are consistent with the
partial assignment shown doubled. Note that as the solid edge from x2 is not consistent
with the assignment, T is not reachable along a doubled path from the root node.
The algorithm then determines the set of nodes from which T is reachable  these
nodes shown doubled in Figure 13(b). These nodes must remain unreachable along the
final reason; as such, the nodes which must remain fixed are the x2 node and the leftmost
z2 node.
Finally, the algorithm progressively unfixes any variables which would not provide a
path to T (in this case, y1 ). The final path is shown in Figure 13(c), the resulting inference
being E[z2 ] = {1}  E[x2 ] = {1}; the corresponding reason clause is x2  z2 .
5.2 Lazy Reason Generation
The simplest way to use reason generation is a so called eager generation, where whenever a
BDD propagator makes a new inference, a minimal reason clause is generated and added to
324

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

construct reason(r,V ,D,var,sign) {
Let r = n(v, t, f )
Dold = D[var];
D[var] = {1  sign};
forall (nodes n  r) visit[n] := 
mark reason(r,V,D);
reached [v] = {r};
if (sign)
mark reason(node,V ,D) {
reason = var;
if (visit[node] 6= ) return visit[node];
else
Let node = n(v, t, f )
reason = var;
reachhi = mark reason(t, V, D);
for (v   V ) {
reachlow = mark reason(f, V, D);
fixedvar = false;
reacht = false;
for (n  reached[v  ]) {
if (0  D[v])
fixedvar = fixedvar  fixed [n];
reacht = reacht  reachlow ;
}
else
if (fixedvar  v  6= var) {
fixed[node] = reachlow ;
if (0  D[v])
if (1  D[v])
reason = reason  v;
reacht = reacht  reachhi ;
else
else
reason = reason  v;
fixed[node] = reachhi ;
}
visit[node]
= reacht;
for (n(vn , fn , tn )  reached[v  ]) {
return reacht ;
if (fixedvar  1  D[v  ])
}
reached[vn ] = reached[vn ]  tn ;
if (fixedvar  0  D[v  ])
reached[vn ] = reached[vn ]  fn ;
}
}
D[var] = Dold ;
return reason;
}
Figure 12: Pseudo-code for the reason generation algorithm by Subbarayan (2008). Constructs a minimal set of variables required to cause the inference var = sign.
the SAT solver. These clauses, however, cannot make any meaningful contribution to search
until a conflict is detected  they cannot cause any propagation until the solver backtracks
beyond the fixed variable, and no conflict clauses are constructed until there is a conflict.
As there is a degree of overhead in adding and maintaining a large set of these clauses in the
solver, it may be better to delay constructing these reasons until they are actually required
to explain a conflict.
We can instead apply the reason generation only when the SAT conflict analysis asks
for the explanation of a literal set by the BDD solver. We call this lazy generation. In
325

fiGange, Stuckey, & Lagoon

7654
0123
x1CCC
CCCC
CCCC
C %
7654
0123
y1



 
 7654
z1

 0123
{{{{{{{

{
% 
 y {{{{
7654
0123
x2 C
CC
CC
C!
7654
0123
y2



 ff
 7654
0123


 {{{z2


{
{
 {{
! 
 y {{{

y
7654
0123
y1

7654
0123
z1

 
 
 
 
y
0123
y2
  7654
  
   ff
   7654
0123
z2
 |||||
 ||||

  
 zrzt |

F

T

7654
0123
x1EEE
EEEE
EEEE
E &
7654
0123
y1
ffff
ff
ff
ffff 
ffffffff 7654
z1
ff
ff 0123
ffffffffxxxxxxx
ff
ffx
% 	 ffffx xxxx
7654
0123
x2 C
CC
CC
C!
@ABC
GFED
?>=<
89:;
y2
 

 ff
@ABC
?>=<
89:;
  GFED
z2
  |||

|
|
 ||
 
y |||

y
7654
0123
y1

7654
0123
z1

 
 
 
 
{
  7654
0123
y2
  
 
  
ff
   7654
0123
z
    2
  

  ff{ s{t

F
(a)

T

(b)

w
7654
0123
y1
ff
7654
0123
z1

 
 
 
 
x
  7654
y2
 0123
  
   ff
   GFED
?>=<
89:;
z2
  @ABC
 }}}}}}
 }

  
 zrzt }}}

F

7654
0123
x1EEE
EEEE
EEEE
E &
7654
0123
y1
fifi
fi
fi
fifi ff
fifififi 7654
z1
fi
fi 0123
fifififi{{{{{{{
fi
fi{{
& 	 fifiy {{{
@ABC
GFED
?>=<
89:;
x2 C
CC
CC
C!
7654
0123
y2
 
 
0123
 7654
z

 || 2

|

|
" }||

T

(c)

Figure 13: (a) The BDD representing x = y  z where N = 2, with E[y1 ] = {1}, E[z2 ] = {1}
and E[x2 ] = {0}. Edges consistent with the partial assignment are shown doubled. (b)
Nodes which must remain unreachable in the reason are shown doubled. (c) Edges reachable
along the minimal reason are shown doubled, as are nodes which remain fixed.

order to do so, we must determine the state of the propagator which caused the inference.
We implement this by recording the order in which literals become fixed in a propagator.
When generating a reason for a variable v becoming fixed, we look at each variable in the
propagator, and unfix any variable v  such that time(v)  time(v  ), then restore them after
the reason is constructed.
5.3 Hybrid Architecture
The hybrid SAT solver embeds BDD propagators inside the SAT engine. The architecture
is illustrated in Figure 14. The usual SAT engine architecture is shown on the left. BDD
propagation is added as shown on the right. Unit propagation causes Boolean literals to
be fixed which may require that BDD propagators need to be awoken. We attach to each
Boolean variable representing part of a set variable x the BDD propagators involving that
set variable. When unit propagation reaches a fixpoint, the trail of fixed literals is traversed
and each BDD propagator that includes one of these literals is scheduled for execution. If we
are using filtering, it is only scheduled if the literal is one which matters to the propagator.
Then we execute the scheduled BDD propagators using imp bddp. If the BDD propagator
fixes some literals then these are added to the trail of the unit propagation engine. If we
are using eager reason generation then we also immediately build a clause explaining the
propagation and add it to the clause database and record this clause as the reason for the
propagation of the literal.
If we are using lazy reason generation, instead we record as the reason simply a pointer to
the BDD propagator which causes the literal to be fixed. Then if conflict analysis demands
an explanation for the literal, we call the reason generation for the BDD propagator, using
326

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

SAT Engine
Search

BDD Propagator
Filtering

conflict
analysis

unit
propagation

imp_bddp

reason
generation

Clause Database

Figure 14: Architecture for the hybrid BDD-SAT solver.

the state at the time when the literal was fixed, to build an explaining clause. This is
used in conflict analysis. We replace the reason for the literal in the trail by the generated
explanation clause and also add the explanation clause to the database.
The implementation inherits almost all features of the underlying SAT solver. Eager
reason clauses are added as nogoods, and deleted when the SAT solver decides to eliminate
nogoods, lazy reason clauses are only generated on demand during conflict analysis. They
are added to the clause database even though this is not necessary, since its makes memoing
which explanations have been already performed simpler. The hybrid solver can make use of
restarting activity based search, and restarts, although we also extend the search capabilities
to allow some simple static searches as these can be preferable for the set problems we tackle.

6. Experimental Results
We built a hybrid SAT solver implementing the algorithms described above. The solver is
based on MiniSAT 2.0 (dated 070721) (Een & Sorensson, 2003), which has been modified
to include the BDD-based propagation engine. BDDs are constructed using the BuDDy
BDD package (http://sourceforge.net/projects/buddy/) All BDDs are constructed at the
beginning of execution, then converted to the static graph used during propagation. Indeed,
for many of the smaller problems solved in Section 6, the majority of the solution time is
used in constructing the BDDs.
The BDD propagators are executed at a lower priority level than unit propagation, in
order to detect conflict as early as possible. Reason clauses which are generated by the setbounds propagator are added to the SAT solver as learnt clauses, as otherwise the number of
clauses added to the solver during propagation of hard problems can overwhelm the solver.
Experiments were conducted on a 3.00GHz Core2 Duo with 2 Gb of RAM running
Ubuntu GNU/Linux 8.10. All problems were terminated if not completed within 10 minutes.
327

fiGange, Stuckey, & Lagoon

We experimented on 3 classes of set benchmarks: social golfers, Steiner systems, and
Hamming codes. Unless otherwise specified, the hybrid solver is always executed using lazy
reason generation.
We compare with the Gecode 3.1.0 set bounds propagation solver since it is acknowledged as one of the fastest solvers available, as well as ECLiPSE 6.0 #100. We also compare with published results of the Cardinal (Azevedo, 2007) and Length-Lex (Yip &
Van Hentenryck, 2009) solvers on the same problems.
6.1 Social Golfers
A common set benchmark is the Social Golfers problem, which consists of arranging
N = g  s golfers into g groups of s players for each of w weeks, such that no two players
play together more than once. Again, we use the same model as used by Lagoon and
Stuckey (2004), using a w  g matrix of set variables vij where 1  i  w and 1  j  g.
V V

g
w
< (v , . . . , v )) 
partition
|v
|
=
s

i1
ig
ij
i=1
V
 i=1
V j=1V

V
w1 w
|v

v
|

1

v

v
j1
jl
i,j{1...w}, i6=j
k,l{1...g} ik
i=1
j=i+1 i1
(

Vw

The global constraint partition< ensures its arguments are pairwise disjoint and imposes
a lexicographic order on its arguments, i.e. vi1 <    < vig . The corresponding propagator
is based on a single BDD. We construct BDD propagators for each of the constraint forms
|v  v  |  1, v  v  and |v| = s. Note that the first form would typically be decomposed
into u = v  v   |u|  1 in a normal set bounds propagator.
The hybrid solver constructs one BDD for each of the 4 terms in the above equation,
instantiating constraints accordingly.
Table 1 shows the results using a static search strategy on easy problems. The search
fixes the elements of the sets vij is order v11 , v12 , . . . , v1g , v21 , . . . , vwg , always trying to first
place the least element in the set then excluding it from the set. We compare against the
reported results for the original BDD-SAT hybrid solver of Hawkins and Stuckey (2006)
versus a number of variations of our hybrid. base is the base solver of Figures 4 and 5,
while +f indicates with filtering of Section 4.1 added, +s indicates with dead subgraph
memoization and shortcutting added (Section 4.2) using the original sparse set code, +i
is these optimizations with the improved sparse set code. We also combine filtering with
the other optimizations. The table shows time and number of fails for each variant, where
the solvers with identical failure behaviour are grouped together. Note that filtering can
change the search by reordering the propagations and hence changing the nogoods that are
generated, while the other optimizations cannot except that shortcutting can change the
results of filtering (and hence change search). While filtering improves on the base line,
dead subgraph memoization and shortcutting do not, although we can see the benefit of
the improved sparse set operations. Comparing against the solver of Hawkins and Stuckey
(2006), which was run on a () 2.4GHz Pentium 4, we find that, slightly different number
of backtracks and slightly faster machine not withstanding, the solver presented here is
roughly an order of magnitude faster.
Table 2 shows the results using VSIDS search on easy problems. It compares against
the solver of Hawkins and Stuckey (2006) and a Tseitin decomposition. The results are the
328

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

Problem
2,5,4
2,6,4
2,7,4
2,8,5
3,5,4
3,6,4
3,7,4
4,5,4
4,6,5
4,7,4
4,9,4
5,4,3 
5,5,4
5,7,4
5,8,3
6,4,3 
6,5,3
6,6,3
7,5,3
7,5,5 
Total

Hawkins
time
fails
0.10
11
0.10
45
0.20
90
0.80
472
0.10
11
0.20
48
0.70
81
0.20
11
0.70
81
0.80
105
1.90
32
12.00
9568
2.30
1167
1.50
159
0.90
12
2.10
908
0.90
282
0.40
5
18.20
6152
0.80
100
44.90
19340

base
0.03
0.04
0.06
2.84
0.02
0.04
0.12
0.03
0.25
0.11
0.18
2.58
0.42
0.18
0.06
0.51
0.13
0.05
3.79
0.20
11.64

+s
0.02
0.04
0.07
3.15
0.02
0.05
0.08
0.03
0.27
0.14
0.18
3.00
0.48
0.25
0.10
0.60
0.14
0.04
4.67
0.20
13.53

Static Search
Hybrid
+i
fails
+f
0.02
19
0.02
0.05
126
0.05
0.07
148
0.07
3.13
8856
0.47
0.04
19
0.02
0.07
129
0.05
0.11
165
0.10
0.04
19
0.02
0.26
559
0.07
0.15
171
0.18
0.18
40
0.14
2.92
10294
2.35
0.46
1328
0.33
0.21
217
0.24
0.07
10
0.08
0.57
1699
0.33
0.16
278
0.09
0.05
5
0.03
4.45
7616
2.10
0.18
121
0.18
13.19
31819
6.92

fails
19
153
265
1119
19
156
282
19
77
288
40
10209
1293
335
10
1079
261
5
5702
121
21452

+fs
0.02
0.05
0.07
0.50
0.03
0.06
0.14
0.02
0.10
0.17
0.14
2.69
0.40
0.25
0.06
0.32
0.11
0.03
3.08
0.21
8.45

+fi
0.02
0.04
0.07
0.50
0.02
0.06
0.12
0.03
0.09
0.17
0.14
2.69
0.36
0.24
0.10
0.33
0.14
0.04
2.97
0.20
8.33

fails
19
153
265
1119
19
156
282
19
77
288
40
10188
1297
335
10
922
257
5
6302
121
21874

Table 1: First-solution performance results on the Social Golfers problem using a static,
first-element in set ordering. Instances marked with () are unsatisfiable, entries marked
with  did not complete within 10 minutes.

same as for Table 1, and overall VSIDS is better than static search. The table illustrates
some of the difficulty of comparing systems using VSIDS search, since small differences can
drastically change the search space. The solver +f is the best except for a bad-performance
on 7,5,3. The base solver is around 5 times faster per failure than the solver of Hawkins
and Stuckey (2006). The Tseitin decomposition is not competitive, even if we discount the
results on 7,5,3.
For social golfers, dead-subset memoization and shortcutting provide no advantage
(when we discount the drastically different search for 7,5,3 using VSIDS). While the number
of nodes processed can be reduced slightly, this is not enough to repay the additional cost
of computation at each node.
Table 3 compares the reason generation strategies: eager reasoning which constructs reasons as soon as inference is detected; and lazy reasoning which only those reasons necessary
to determine the first UIP or perform conflict clause minimization.
Table 3 compares the base solver with and without filtering (since dead subgraph memoization and shortcutting do not help here) on harder social golfer problems using a static
search. It shows time (base) as well as the number of reasons generated and fails in order
to find a first solution. For these harder examples filtering is highly beneficial. Here we can
see that the number of reasons generated by lazy reasoning is about half of that required by
eager reasoning, but it doesnt make that much difference to the computation time, since
propagation dominates the time spent in the solver. Interestingly not adding reasons eagerly also seems to generate slightly better nogoods as the search is usually smaller. Table 4
shows the results using VSIDS search on these harder instances. It appears the advantages
329

fiGange, Stuckey, & Lagoon

Problem
2,5,4
2,6,4
2,7,4
2,8,5
3,5,4
3,6,4
3,7,4
4,5,4
4,6,5
4,7,4
4,9,4
5,4,3 
5,5,4
5,7,4
5,8,3
6,4,3 
6,5,3
6,6,3
7,5,3
7,5,5 
Total

Hawkins
time
fails
0.10
22
0.10
64
0.20
119
1.30
622
0.10
24
0.30
58
0.60
92
0.40
122
1.30
304
1.00
98
2.00
59
5.60
5876
1.90
581
1.50
104
1.70
425
0.20
71
4.30
2801
1.00
275
18.00
7018
2.00
139
43.60
18874

base
0.04
0.02
0.03
0.10
0.04
0.05
0.06
0.05
0.26
0.09
0.16
1.23
4.14
0.16
0.08
0.18
0.25
0.07
8.81
0.14
15.96

+s
0.03
0.03
0.03
0.12
0.04
0.04
0.06
0.06
0.26
0.11
0.18
1.42
5.16
0.13
0.10
0.17
0.27
0.06
11.08
0.11
19.46

+i
0.03
0.02
0.03
0.11
0.02
0.04
0.06
0.06
0.26
0.10
0.18
1.35
4.80
0.13
0.10
0.17
0.29
0.07
10.72
0.12
18.66

VSIDS Search
Hybrid
fails
+f
fails
4
0.02
4
20
0.03
20
13
0.04
13
109
0.10
109
51
0.03
51
80
0.06
80
78
0.07
79
108
0.04
116
309
0.13
158
102
0.10
103
36
0.14
36
5869
0.56
3139
9846
0.91
2487
77
0.11
84
29
0.10
29
425
0.14
479
369
0.18
409
36
0.07
70
18949
39.35
93789
47
0.10
47
36557
42.28
101302

+fs
0.02
0.02
0.05
0.10
0.03
0.04
0.06
0.06
0.14
0.09
0.18
0.69
0.77
0.13
0.10
0.30
0.17
0.08
2.47
0.13
5.63

+fi
0.03
0.02
0.04
0.10
0.04
0.04
0.10
0.06
0.15
0.08
0.15
0.67
0.74
0.12
0.10
0.28
0.16
0.09
2.38
0.10
5.45

fails
4
20
13
109
51
80
79
116
205
103
36
3184
1754
84
29
1013
397
70
4554
47
11948

Tseitin
time
fails
0.03
7
0.04
37
0.06
55
0.09
78
0.05
170
0.07
268
0.12
469
0.14
1143
0.49
3156
0.25
1020
0.63
1037
4.74
26769
0.58
3475
1.16
3596
0.52
918
2.83
17595
1.85
8675
1.09
3547
45.54
77786
0.93
1977
61.21
151778

Table 2: First-solution performance results on the Social Golfers problem using a VSIDS
search strategy.
Social Golfers
Problem
7,5,3
2,6,5
4,6,5
6,10,3
9,10,3
10,10,3
Total

base
6.34
0.14
0.46
0.90
16.91
109.43
134.18

Lazy Reason Generation
reasons
fails
+f
reasons
62630
13071
5.49
65447
1673
581
0.03
317
7058
1067
0.38
7026
6675
871
0.72
6973
15522
3857
15.74
15103
28110
11462
130.04
32580
121668
30909
152.40
127446

fails
13079
66
1037
946
3708
13128
31964

base
8.76
0.17
0.55
1.04
27.88
198.90
237.30

Eager Reason
reasons
fails
117323
13273
3026
581
11833
1066
9942
820
34181
4039
81270
12755
257575
32534

Generation
+f
reasons
7.38
117657
0.04
740
0.48
11967
0.87
10101
25.77
32945
187.11
79461
221.65
252871

Table 3: First-solution performance results on harder Social Golfers problems, using a static
least-element in set search method. Results are given comparing eager and lazy reason
generation.

of lazy reasoning are increased by the use of VSIDS, presumably because the better nogoods
are then more useful in driving search.
Finally Table 5 compares against a number of different systems. We use the model of
social-golfers described in the work of Yip and Van Hentenryck (2009), which in addition
fixes the first week, and the first group of the second week to eliminate symmetric solutions.
We use the instances reported by Yip and Van Hentenryck (2009). We show results for our
base solver with and without filtering. We compare against Gecode 3.1.0 and Eclipse 6.0
#100, both which implement a set bounds propagation combined with limited cardinality
reasoning, on an identical MiniZinc model of social-golfers running on our 3GHz Core2Duo.
Gecode arguably represents the state of the art for set bounds propagation solving. We also
compare against the published results of the Cardinal solver (Azevedo, 2007), which uses
330

fails
12598
66
1028
849
3853
12338
30732

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

Social Golfers
Problem
7,5,3
2,6,5
4,6,5
6,10,3
9,10,3
10,10,3
Total

base
0.20
0.02
0.06
0.22
0.92
1.51
2.93

Lazy
reasons
2096
38
176
188
1743
1917
6158

Reason
fails
217
4
18
7
110
139
495

Generation
+f
reasons
1.03
12638
0.02
38
0.06
176
0.18
188
0.45
666
0.64
641
2.38
14347

fails
2608
4
18
7
68
46
2751

base
0.22
0.03
1.28
0.34
1.59
2.06
5.52

Eager Reason
reasons
fails
4328
212
350
4
28026
1565
1824
7
6685
134
7707
200
48920
2122

Generation
+f
reasons
2.18
63780
0.04
350
0.08
1604
0.34
1823
1.20
6310
1.08
4310
4.92
78177

fails
4911
4
40
7
107
57
5126

Table 4: First-solution performance results on harder Social Golfers problems, using a
VSIDS search method. Results are given comparing eager and lazy reason generation.
more complex cardinality reasoning for set solving, using a () Pentium 4 2.4GHz machine,
and the recently published results for the Length-Lex solver of Yip and Van Hentenryck
(2009), which maintains bounds on sets variables in terms of the length-lex order (see Gervet
& Van Hentenryck, 2006; Yip & Van Hentenryck, 2009 for details) running on a () C2DM 2.53GHz machine. The pure set bounds solvers cannot compete with our approach
since the search space without using nogood recording is just too big. None of the other
systems except Length-lex can solve all of these instances. One can a see a drastic
difference between number of failures for Gecode, which uses set bounds propagation
without learning, versus our base solver. Gecode can sometimes require less failures on
easy problems since it combines cardinality reasoning with bounds reasoning, but on hard
problems the advantages of learning to prune similar searches in other parts of the tree
dominates completely. The stronger pruning of Length-lex compared to set bounds
means it can often improve on fails compared to base but learning is more robust. The
hybrid solver is overall around an order of magnitude faster than Length-Lex.
6.2 Steiner Systems
Another commonly used benchmark for set constraint solvers is the calculation of small
Steiner systems. A Steiner system S(t, k, N ) is a set X of cardinality N and a collection
C of subsets of X of cardinality k (called blocks), such that any t elements
of X are in

exactly one block. Any Steiner system must have exactly m = Nt / kt blocks (Theorem
19.2 of van Lint & Wilson, 2001).
We model the Steiner problem similarly to Lagoon and Stuckey (2004) extended for the
case of more general Steiner Systems. We model each block as a set variable s1 , . . . , sm ,
with the constraints:
m
^

(|si | = k) 

i=1
m1
^

m
^

(|si  sj |  t  1  si < sj )

i=1 j=i+1

For comparison with the results of Azevedo (2007) and Yip and Van Hentenryck (2009),
we construct a dual model with additional variables d1 , . . . , dN , with additional constraints
331

fiGange, Stuckey, & Lagoon

Problem
4,4,2
5,4,2
6,4,2
7,4,2
4,4,3
5,4,3
6,4,3
4,4,4
5,4,4
3,5,2
4,5,2
5,5,2
6,5,2
7,5,2
8,5,2
9,5,2
3,5,3
4,5,3
5,5,3
6,5,3
7,5,3
2,5,4
3,5,4
4,5,4
5,5,4
3,5,5
4,5,5
5,5,5
6,5,5
7,5,5
2,6,3
3,6,3
4,6,3
5,6,3
6,6,3
2,6,4
3,6,4
2,6,5
3,6,5
4,6,5
5,6,5
3,6,6
2,7,2
2,7,3
2,7,4
3,7,4
4,7,4
5,7,4
2,7,5
2,7,6
2,7,7
5,8,3
4,8,4
2,8,5
4,9,4
6,10,3
9,10,3
10,10,3
4,10,4
5,10,4
Total

Gecode
0.00
0
0.00
0
0.00
0
0.00
0
0.00
6
1.72
5781
0.01
45
0.00
0
0.00
0
0.00
1
0.00
2
0.00
2
0.00
6
0.01
17
0.01
22
0.01
17
0.01
49
0.03
73
0.03
105
110.33
335531


0.09
1090
0.11
605
0.09
298
0.19
410
0.00
1
0.01
13
0.04
45
0.03
30
8.11
12274
0.00
0
0.01
30
0.01
23
0.09
311
0.15
388
1.91
16608
3.68
15948


547.91
1893577
275.38
584532
96.89
145371
0.01
8
0.00
1
0.00
10
5.64
39833
13.40
46621
10.54
24216
11.32
18785




0.01
0


26.92
56844


8.93
11854






17.33
16345
34.62
36294



Eclipse
0.56
0.54
0.55
0.62
0.56
11.21
0.66
0.57
0.61
0.54
0.57
0.61
0.66
0.71
0.81
0.86
0.58
0.65
0.76


1.26
1.10
0.98
1.40
0.65
0.74
0.84
0.96
161.12
0.56
0.69
0.71
1.08
1.32
8.12
10.16



265.40
0.82
0.60
0.64
17.43
27.23
19.38
17.17


0.86

46.03

12.18



17.10
46.58


Cardinal

165.63
94.67



1.89
3.13
28.65



1.20
1.75
4.62



2.82
6.37
12.46
17.18

1.01

42.45



length-lex
0.01
0
0.01
0
0.01
0
0.01
0
0.01
0
0.40
732
0.02
29
0.06
111
0.05
57
0.01
0
0.01
0
0.01
0
0.01
0
0.02
1
0.02
1
0.02
1
0.01
1
0.01
1
0.02
5
0.41
316
74.59
46117
0.01
11
0.02
24
0.14
194
1.87
1947
0.06
93
4.72
6876
54.27
50623
29.21
15769
0.01
1
0.00
0
0.01
1
0.01
1
0.02
6
0.04
10
0.01
14
0.03
42
0.05
118
2.54
3351
32.60
31270
28.76
6758
0.82
661
0.01
0
0.01
1
0.01
0
0.03
21
0.05
26
0.36
152
0.31
574
0.78
1271
0.28
0
34.52
45477
0.06
18
0.25
307
0.21
94
5.86
2941
233.80
45437
210.80
25246
0.27
104
0.58
149
719.11
286960

base
0.01
0.02
0.02
0.02
0.01
0.46
0.03
0.02
0.02
0.02
0.01
0.02
0.01
0.04
0.06
0.12
0.02
0.03
0.03
0.80
6.32
0.02
0.03
0.07
0.98
0.02
0.05
0.07
0.15
3.28
0.02
0.03
0.04
0.04
0.39
0.04
0.06
0.13
0.15
0.45
15.24
0.04
0.03
0.02
0.02
0.07
0.08
0.22
0.45
1.44
0.06
0.09
0.12
0.56
0.19
0.91
16.66
110.80
0.52
0.76
162.39

+f
3
4
8
13
9
1877
57
3
8
1
3
5
8
14
24
40
10
18
30
2516
13071
21
36
191
2592
0
29
64
167
4728
4
4
11
15
932
58
97
581
532
1067
26495
0
0
0
19
69
62
243
1944
6031
0
71
67
2145
90
871
3857
11462
409
576
83262

0.01
0.02
0.02
0.04
0.02
0.35
0.04
0.02
0.03
0.02
0.03
0.02
0.03
0.04
0.05
0.14
0.02
0.01
0.03
0.56
5.50
0.02
0.02
0.08
0.74
0.02
0.04
0.06
0.15
8.17
0.02
0.01
0.02
0.04
0.27
0.02
0.05
0.05
0.08
0.35
2.85
0.03
0.03
0.03
0.05
0.07
0.08
0.16
0.07
0.18
0.05
0.11
0.10
0.23
0.19
0.73
15.64
129.66
0.46
0.65
168.58

3
4
8
13
9
1799
57
3
8
1
3
5
8
14
24
40
10
16
30
2345
13079
21
36
189
2356
0
27
64
221
11736
4
4
11
15
959
58
97
66
178
1037
7295
0
0
0
19
72
63
234
133
566
0
70
67
291
100
946
3708
13128
419
596
62265

Table 5: Comparison against solvers using different propagation mechanisms, using the
model and instances described by Yip and Van Hentenryck (2009).  denotes failure to
complete a test-case in 10 minutes (or 15 minutes for Cardinal). A blank entry means
there is no published result to compare.
as shown:
m ^
N
^

(j  si  i  dj ) 

i=1 j=1
N
^

(|dj | =

j=1

332

mk
)
N

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

Problem
2,3,7
2,3,9
2,3,13
2,3,15
2,3,19
2,3,21
2,3,25
2,3,27
2,3,31
2,3,33

Gecode
time
fails
0.00
0
0.00
3
0.03
18
0.04
0
0.65
144
2.61
413









Eclipse
time
0.53
0.56
1.20
2.15
8.09






Card
time
0.01
0.05
0.61
0.91
7.94
39.07

48.52

Length-lex
time
fails
0.00
0
0.01
1
0.05
10
0.09
0
0.46
164
1.04
448
14.07
5100
23.55
7066
5.29
0

Static Hybrid
base
fails
0.01
0
0.02
1
0.06
9
0.07
0
0.37
78
0.82
225
7.10
2474
12.88
3401
5.38
0
443.07
111923

VSIDS Hybrid
base
fails
0.03
5
0.02
17
0.02
24
0.32
295
0.07
106
39.19
42688


229.59
113373


19.30
8228

Tseitin
time
fails
0.04
59
0.81
3804
1.93
7879
143.81
124205
14.01
34089











Table 6: First-solution performance results on the alternate Steiner Systems instances using
a dual model. Gecode and the sequential hybrid use a sequential least-element in set search
strategy over the dual variables. The VSIDS hybrid and Tseitin decomposition use VSIDS
search.  denotes failure to complete a test-case in 10 minutes due to either timeout or
a memory error. A blank entry means there is no published result to compare.


We create BDD propagators for each of the the constraint forms |v| = mk
N and |v  v | 


t  1  v < v  |v| = k  |v | = k. Again note in non-BDD based set bounds solvers the last
form would typically be five separate constraints. The channelling component j  v  i  v 
is not explicitly represented. Instead, the underlying Boolean variables are re-used.

In Table 6, we use the model and search strategy used by Azevedo (2007), restricting
the number of times a given element can occur in the sets s1 , . . . , sm . We compare against
Gecode and Eclipse using the same MiniZinc model, as well as the published results
of Cardinal and Length-Lex. The model used by the hybrid solver constructs one
constraint for each pair of set variables, conjoining cardinality, intersection and ordering
constraints. On those instances where a significant amount of search occurs, we again see
a massive improvement beyond the performance of any of the pure set bounds propagation
solvers. Our hybrid solver and Length-Lex are the most robust. We can see the hybrid
requires the least search and is somewhat faster than Length-Lex. We also compare
versus VSIDS search. The Steiner problems illustrate how a specialized search strategy can
be better than the generic VSIDS approach. We can see that the Tseitin decomposition is
not competitive for these problems

6.3 Fixed-weight Hamming Codes
The problem of finding maximal Hamming codes can also be expressed as a set-constraint
problem. A Hamming code with distance d and length l is a set of l-bit codewords such that
each pair of codewords must have at least d bits which differ. A variation of this problem
is to find maximal codes where all codewords have exactly w bits set.
333

fiGange, Stuckey, & Lagoon

Problem
8,4,4
9,4,3
9,4,4
9,4,5
9,4,6
10,4,3
10,4,4
10,4,5
10,4,6
10,4,7
10,6,5

Fixed-weight Hamming Codes
Length-lex
Static Hybrid
time
fails
+f
+fs
+fi
0.07
110
0.16
0.17
0.16
2.05
4617
7.13
7.47
7.25










0.40
908
1.67
1.66
1.61
359.30
629822


















1.99
4415



0.03
158
78.99
78.14
78.92

Gecode
time
fails




















280.97
2175542

fails
897
29985


10541





92349

Table 7: Results on hard Hamming instances with a static least-element in set search order,
with no additional symmetry breaking.
Problem
8,4,4
9,4,3
9,4,4
9,4,5
9,4,6
10,4,3
10,4,4
10,4,5
10,4,6
10,4,7
10,6,5

+f
0.08
0.30
56.55
69.28
0.43
102.64
53.72

509.13
110.65
0.71

Fixed-weight Hamming Codes
VSIDS Hybrid
Tseitin
+fs
+fi
fails
time
fails
0.08
0.10
282
11.15
50530
0.26
0.24
1627
16.18
67876
45.95
43.39 210183


59.56
56.65 307786


0.39
0.36
2589
14.84
55292
90.37
86.09 638214


38.72
37.04
91781







404.65
385.48 987682


101.37
103.20
727465


0.57
0.54
5057
157.86
693148

Table 8: Results on hard Hamming instances with a VSIDS, with no additional symmetry
breaking.
A formulation for this problem is:
m
^

(|si | = w) 

i=1
m1
^

m
^

(|si  sj |  d  si < sj )

i=1 j=i+1

where s  s = (s  s )  (s  s) is the symmetric difference. This is similar in structure
to the formulation for the Steiner Systems; however, rather than having a fixed number
of sets, we find the maximal code by repeatedly adding new sets and the corresponding
constraints until no solution can be found. The unsatisfiability of n codewords proves that
the maximal code has n  1 codewords. We create BDD propagators for the constraint form
|v  v  |  d  v < v   |v| = w  |v  | = w.
We compare on two different models of the fixed-weight Hamming code problems, one
just using the description above, and another where the first two sets are fixed to remove
symmetries. We compare against Gecode, the published results of Length-Lex with our
334

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

Problem
8,4,4
9,4,3
9,4,4
9,4,5
9,4,6
10,4,3
10,4,4
10,4,5
10,4,6
10,4,7
10,6,5

Gecode
time
fails
15.29
29869
66.72
216598




47.72
101832










0.07
546

Fixed-weight Hamming Codes
Length-lex
Static Hybrid
time
fails
+f
+fs
+fi
0.07
110
0.04
0.04
0.04
2.05
4617
0.28
0.29
0.28


18.09
17.95
17.99


55.90
56.52
57.14
0.40
908
0.04
0.04
0.04
359.30
629822


















1.99
4415
6.16
6.24
6.29
0.03
158
0.02
0.02
0.02

fails
51
2130
43318
71777
208




22857
70

Table 9: Results on hard Hamming instances with a sequential least-element in set search
order, with fixed first and second sets.
Problem
8,4,4
9,4,3
9,4,4
9,4,5
9,4,6
10,4,3
10,4,4
10,4,5
10,4,6
10,4,7
10,6,5

+f
0.03
0.08
1.28
4.82
0.06
2.76
20.53
143.50
64.10
2.21
0.03

Fixed-weight Hamming Codes
VSIDS Hybrid
Tseitin
+fs
+fi
fails
time
fails
0.05
0.03
61
1.06
6194
0.06
0.06
300
3.19
19952
1.10
1.04
4466
319.05
407762
4.20
4.03
21651
186.64
244474
0.05
0.06
256
1.31
8328
2.56
2.37
16755
120.22
226380
15.45
14.66
34503


104.29 104.39
184051


51.76
48.96 131379


2.05
1.96
13533
58.06
112269
0.04
0.03
145
0.10
1044

Table 10: Results on hard Hamming instances with a VSIDS search strategy, with fixed
first and second sets.
hybrid using a static search strategy (the same least element in set strategy as used for Social
Golfers), as well as the hybrid solver and a Tseitin decomposition using VSIDS search. For
our systems we compare with and without shortcutting and our optimized implementation.
Since we are not sure which model was used by Length-Lex we report it results for both
models.
Tables 7 to 10 show the results on the 11 hard instances reported by Hawkins et al.
(2005). Clearly on these problems the VSIDS hybrid is the most robust. It can solve
all but one instance in the basic model, and all with the additional symmetry breaking.
This example also clearly shows the potential advantages of shortcutting and our improved
data structures: these do not change the search but improve the time by 18% and 21%
respectively for the base model, and 24% and 26% respectively for the improved model.
Once more Tseitin decomposition is not competitive.

7. Related Work
Set-constraint problems have been an active area of research in the past decade. Many
of the earlier solvers, beginning with PECOS (Puget, 1992), used the set-bounds repre335

fiGange, Stuckey, & Lagoon

sentation combined with a fixed set of propagation rules for each constraint. This general
approach was also used by Conjunto (Gervet, 1997), ECLi PSe (IC-PARC, 2003), ILOG
Solver (ILOG, 2004) and Mozart (Muller, 2001). However, as set-bounds are a relatively
weak approximation of the domain of a set variable, a variety of variations have been developed to improve the propagation strength of set-constraint solvers. These include solvers
which combine set-bounds representation with either cardinality information, such as that
proposed by Azevedo (2002, 2007), lexicographic bounds information (Sadler & Gervet,
2004) or both (Gervet & Van Hentenryck, 2006; Yip & Van Hentenryck, 2009).
BDD-based approaches to set-constraint solving, such as that presented by Hawkins
et al. (2005) differs greatly from these approaches, as it is possible to perform propagation
over arbitrary constraints; Lagoon and Stuckey (2004) also demonstrated the feasibility of
a BDD-based solver which maintains a complete domain representation of set variables.
These directly BDD-based algorithms were used to construct the earlier hybrid solver
presented by Hawkins and Stuckey (2006), which is conceptually similar to the solver presented in this paper. The solver presented here is much more efficient, and includes improvements such as filtering and shortcutting not present in the solver of Hawkins and Stuckey
(2006). The solver of Damiano and Kukula (2003) also combines BDD solving and SAT
solving, but rather than building BDDs from a high-level problem description and lazily
constructing a SAT representation, instead takes a CNF SAT representation and constructs
a BDD from a collection of clauses with the primary goal of variable elimination. It is
essentially equivalent to the base solver.
The underlying BDD propagation algorithm is similar to propagation of the case constraint of SICStus PRolog (SICS, 2009) and Multi-valued Decision Diagrams (MDDs) (see
e.g., Cheng & Yap, 2008). Indeed we have adapted the dead subgraph memoization and
shortcutting devices of Cheng and Yap (2008) to BDD propagation. Propagators for case
and MDDs do not presently use filtering or generate reasons.
Finally the hybrid set solver we present in this paper is an example of a lazy clause
generation solver (Ohrimenko, Stuckey, & Codish, 2007, 2009). The BDD propagators can
be understood as lazily creating a clausal representation of the set constraints encoded in
the BDD, as search progresses.

8. Concluding Remarks
In this paper we have improved BDD-based techniques for set-bounds propagation, having
demonstrated an approach which avoids the need for expensive BDD construction and
manipulation operations. This traversal-based method, when combined with filtering to
reduce the number of redundant propagator executions and dead subgraph memoization
and shortcutting, is at least an order of magnitude faster than previous techniques which
construct BDDs during runtime (Hawkins et al., 2005).
Furthermore, when integrated into a modern SAT solver with clause learning and augmented with a method for generating nogoods, the new hybrid solver is capable of solving
hard problem instances several orders of magnitude faster than pure bounds set solvers.
Overall the hybrid solver is robust and highly competitive with any other propagation
based set-solvers we are aware of.
336

fiFast Set Bounds Propagation Using a BDD-Sat Hybrid

In many set problems there are significant numbers of symmetries and there is a large
body of work solving set problems with symmetry breaking techniques (see e.g., Puget,
2005). It would be interesting to combine symmetry breaking with our hybrid solver.

9. Acknowledgments
Part of this work was published previously (Gange, Lagoon, & Stuckey, 2008). NICTA is
funded by the Australian Government as represented by the Department of Broadband,
Communications and the Digital Economy and the Australian Research Council.

References
Azevedo, F. (2002). Constraint Solving over Multi-valued Logics. Ph.D. thesis, Faculdade
de Ciencias e Tecnologia, Universidade Nova de Lisboa.
Azevedo, F. (2007). Cardinal: A finite sets constraint solver. Constraints, 12 (1), 93129.
Bryant, R. (1986). Graph-based algorithms for Boolean function manipulation. IEEE Trans.
Comput., 35 (8), 677691.
Cheng, K., & Yap, R. (2008). Maintaining generalized arc consistency on ad hoc r-ary
constraints. In 14th International Conference on Principles and Process of Constraint
Programming, pp. 509523.
Damiano, R., & Kukula, J. (2003). Checking satisfiability of a conjunction of BDDs. In
Proceedings of Design Automation Conference, pp. 818823.
Davis, M., Logemann, G., & Loveland, D. (1962). A machine program for theorem-proving.
Communications of the ACM, 5, 394397.
Een, N., & Sorensson, N. (2003). An extensible SAT-solver. In Giunchiglia, E., & Tacchella,
A. (Eds.), Proceedings of SAT 2003, Vol. 2919 of LNCS, pp. 502518.
Een, N., & Sorensson, N. (2006). Translating pseudo-boolean constraints into SAT. Journal
on Satisfiability, Boolean Modeling and Computation, 2, 126.
Gange, G., Lagoon, V., & Stuckey, P. (2008). Fast set bounds propagation using BDDs. In
18th European Conference on Artificial Intelligence, pp. 505509.
GECODE (2008). Gecode. www.gecode.org. Accessed Jan 2008.
Gervet, C. (1997). Interval propagation to reason about sets: Definition and implementation
of a practical language. Constraints, 1 (3), 191246.
Gervet, C., & Van Hentenryck, P. (2006). Length-lex ordering for set CSPs. In Proceedings
of the National Conference on Artificial Intelligence, pp. 4853.
Hawkins, P., Lagoon, V., & Stuckey, P. (2004). Set bounds and (split) set domain propagation using ROBDDs. In 17th Australian Joint Conference on Artificial Intelligence,
Vol. 3339 of LNCS, pp. 706717.
Hawkins, P., Lagoon, V., & Stuckey, P. (2005). Solving set constraint satisfaction problems
using ROBDDs. Journal of Artificial Intelligence Research, 24, 106156.
337

fiGange, Stuckey, & Lagoon

Hawkins, P., & Stuckey, P. (2006). A hybrid BDD and SAT finite domain constraint solver.
In Proceedings of the 8th International Symposium on Practical Aspects of Declarative
Languages, Vol. 3819 of LNCS, pp. 103117.
IC-PARC (2003). The ECLiPSe constraint logic programming system. [Online, accessed
Oct 2008]. http://www.eclipse-clp.org/.
ILOG (2004). ILOG Solver. [Online, accessed Oct 2008]. http://www.ilog.com/.
Lagoon, V., & Stuckey, P. (2004). Set domain propagation using ROBDDs. In Proceedings of the 10th International Conference on Principles and Practice of Constraint
Programming, Vol. 3258 of LNCS, pp. 347361.
van Lint, J. H., & Wilson, R. M. (2001). A Course in Combinatorics (2nd edition). Cambridge University Press.
Muller, T. (2001). Constraint Propagation in Mozart. Doctoral dissertation, Universitat des
Saarlandes, Naturwissenschaftlich-Technische Fakultat I, Fachrichtung Informatik,
Saarbrucken, Germany.
Ohrimenko, O., Stuckey, P., & Codish, M. (2007). Propagation = lazy clause generation.
In Bessiere, C. (Ed.), Proceedings of the 13th International Conference on Principles
and Practice of Constraint Programming, Vol. 4741 of LNCS, pp. 544558. SpringerVerlag.
Ohrimenko, O., Stuckey, P., & Codish, M. (2009). Propagation via lazy clause generation.
Constraints, 14 (3), 357391.
Puget, J.-F. (1992). PECOS: a high level constraint programming language. In Proceedings
of SPICIS92, Singapore.
Puget, J.-F. (2005). Symmetry breaking revisited. Constraints, 10 (1), 2346.
Sadler, A., & Gervet, C. (2004). Hybrid set domains to strengthen constraint propagation
and reduce symmetries. In Wallace, M. (Ed.), Proceedings of the 10th International
Conference on Principles and Practice of Constraint Programming (CP04), No. 3258
in LNCS. Springer-Verlag.
SICS (2009). Sicstus prolog. www.sics.se/sicstus.
Subbarayan, S. (2008). Efficent reasoning for nogoods in constraint solvers with BDDs.
In Proceedings of Tenth International Symposium on Practical Aspects of Declarative
Languages, Vol. 4902 of LNCS, pp. 5357.
Tseitin, G. (1968). On the complexity of derivation in propositional calculus. Studies in
Constructive Mathematics and Mathematical Logic, Part 2, 115125.
Yip, J., & Van Hentenryck, P. (2009). Evaluation of length-lex set variables. In Proceedings of the 15th International Conference on Principles and Practice of Constraint
Programming, pp. 817832.

338

fiJournal of Artificial Intelligence Research 38 (2010) 135-187

Submitted 12/09; published 05/10

A Survey of Paraphrasing and Textual Entailment Methods
Ion Androutsopoulos
Prodromos Malakasiotis

ION @ AUEB . GR
RULLLER @ AUEB . GR

Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece

Abstract
Paraphrasing methods recognize, generate, or extract phrases, sentences, or longer natural language expressions that convey almost the same information. Textual entailment methods, on the
other hand, recognize, generate, or extract pairs of natural language expressions, such that a human
who reads (and trusts) the first element of a pair would most likely infer that the other element is
also true. Paraphrasing can be seen as bidirectional textual entailment and methods from the two
areas are often similar. Both kinds of methods are useful, at least in principle, in a wide range of
natural language processing applications, including question answering, summarization, text generation, and machine translation. We summarize key ideas from the two areas by considering in turn
recognition, generation, and extraction methods, also pointing to prominent articles and resources.

1. Introduction
This article is a survey of computational methods for paraphrasing and textual entailment. Paraphrasing methods recognize, generate, or extract (e.g., from corpora) paraphrases, meaning phrases,
sentences, or longer texts that convey the same, or almost the same information. For example, (1)
and (2) are paraphrases. Most people would also accept (3) as a paraphrase of (1) and (2), though
it could be argued that in (3) the construction of the bridge has not necessarily been completed,
unlike (1) and (2).1 Such fine distinctions, however, are usually ignored in paraphrasing and textual
entailment work, which is why we say that paraphrases may convey almost the same information.
(1)
(2)
(3)

Wonderworks Ltd. constructed the new bridge.
The new bridge was constructed by Wonderworks Ltd.
Wonderworks Ltd. is the constructor of the new bridge.

Paraphrasing methods may also operate on templates of natural language expressions, like (4)
(6); here the slots X and Y can be filled in with arbitrary noun phrases. Templates specified at
the syntactic or semantic level may also be used, where the slot fillers may be required to have
particular syntactic relations (e.g., verb-object) to other words or constituents, or to satisfy semantic
constraints (e.g., requiring Y to denote a book).
(4)
(5)
(6)

X wrote Y .
Y was written by X.
X is the writer of Y .

1 Readers familiar with tense and aspect theories will have recognized that (1)(3) involve an accomplishment of
Vendlers (1967) taxonomy. The accomplishments completion point is not necessarily reached in (3), unlike (1)(2).

c
2010
AI Access Foundation. All rights reserved.

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Textual entailment methods, on the other hand, recognize, generate, or extract pairs hT, Hi of
natural language expressions, such that a human who reads (and trusts) T would infer that H is most
likely also true (Dagan, Glickman, & Magnini, 2006). For example, (7) textually entails (8), but (9)
does not textually entail (10).2
(7)
(8)
(9)
(10)

The drugs that slow down Alzheimers disease work best the earlier you administer them.
Alzheimers disease can be slowed down using drugs.
Drew Walker, Taysides public health director, said: It is important to stress that this is not a confirmed
case of rabies.
A case of rabies was confirmed.

As in paraphrasing, textual entailment methods may operate on templates. For example, in a
discourse about painters, composers, and their work, (11) textually entails (12), for any noun phrases
X and Y . However, (12) does not textually entail (11), when Y denotes a symphony composed by
X. If we require textual entailment between templates to hold for all possible slot fillers, then (11)
textually entails (12) in our examples discourse, but the reverse does not hold.
(11)
(12)

X painted Y .
Y is the work of X.

In general, we cannot judge if two natural language expressions are paraphrases or a correct
textual entailment pair without selecting particular readings of the expressions, among those that
may be possible due to multiple word senses, syntactic ambiguities etc. For example, (13) textually
entails (14) with the financial sense of bank, but not when (13) refers to the bank of a river.
(13)
(14)

A bomb exploded near the French bank.
A bomb exploded near a building.

One possibility, then, is to examine the language expressions (or templates) only in particular
contexts that make their intended readings clear. Alternatively, we may want to treat as correct any
textual entailment pair hT, Hi for which there are possible readings of T and H, such that a human
who reads T would infer that H is most likely also true; then, if a system reports that (13) textually
entails (14), its response is to be counted as correct, regardless of the intended sense of bank.
Similarly, paraphrases would have possible readings conveying almost the same information.
The lexical substitution task of SEMEVAL (McCarthy & Navigli, 2009), where systems are required to find an appropriate substitute for a particular word in the context of a given sentence,
can be seen as a special case of paraphrasing or textual entailment, restricted to pairs of words.
SEMEVAL s task, however, includes the requirement that it must be possible to use the two words
(original and replacement) in exactly the same context. In a similar manner, one could adopt a
stricter definition of paraphrases, which would require them not only to have the same (or almost
the same) meaning, but also to be expressions that can be used interchangeably in grammatical sentences. In that case, although (15) and (16) are paraphrases, their underlined parts are not, because
they cannot be swapped in the two sentences; the resulting sentences would be ungrammatical.
(15)
(16)

Edison invented the light bulb in 1879, providing a long lasting source of light.
Edisons invention of the light bulb in 1879 provided a long lasting source of light.

A similar stricter definition of textual entailment would impose the additional requirement that H
and T can replace each other in grammatical sentences.
2 Simplified

examples from RTE-2 (Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, & Szpektor, 2006).

136

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

1.1 Possible Applications of Paraphrasing and Textual Entailment Methods
The natural language expressions that paraphrasing and textual entailment methods consider are
not always statements. In fact, many of these methods were developed having question answering
(QA) systems in mind. In QA systems for document collections (Voorhees, 2001; Pasca, 2003;
Harabagiu & Moldovan, 2003; Molla & Vicedo, 2007), a question may be phrased differently than
in a document that contains the answer, and taking such variations into account can improve system
performance significantly (Harabagiu, Maiorano, & Pasca, 2003; Duboue & Chu-Carroll, 2006;
Harabagiu & Hickl, 2006; Riezler, Vasserman, Tsochantaridis, Mittal, & Liu, 2007). For example,
a QA system may retrieve relevant documents or passages, using the input question as a query to an
information retrieval or Web search engine (Baeza-Yates & Ribeiro-Neto, 1999; Manning, 2008),
and then check if any of the retrieved texts textually entails a candidate answer (Moldovan & Rus,
2001; Duclaye, Yvon, & Collin, 2003).3 If the input question is (17) and the search engine returns
passage (18), the system may check if (18) textually entails any of the candidate answers of (19),
where we have replaced the interrogative who of (17) with all the expressions of (18) that a named
entity recognizer (Bikel, Schwartz, & Weischedel, 1999; Sekine & Ranchhod, 2009) would ideally
have recognized as person names.4
(17)
(18)

(19)

Who sculpted the Doryphoros?
The Doryphoros is one of the best known Greek sculptures of the classical era in Western Art. The
Greek sculptor Polykleitos designed this work as an example of the canon or rule, showing the
perfectly harmonious and balanced proportions of the human body in the sculpted form. The sculpture
was known through the Roman marble replica found in Herculaneum and conserved in the Naples
National Archaeological Museum, but, according to Francis Haskell and Nicholas Penny, early connoisseurs passed it by in the royal Bourbon collection at Naples without notable comment.
Polykleitos/Francis Haskell/Nicholas Penny sculpted the Doryphoros.

The input question may also be paraphrased, to allow more, potentially relevant passages to be
obtained. Question paraphrasing is also useful when mapping user questions to lists of frequently
asked questions (FAQs) that are accompanied by their answers (Tomuro, 2003); and natural language
interfaces to databases often generate question paraphrases to allow users to understand if their
queries have been understood (McKeown, 1983; Androutsopoulos, Ritchie, & Thanisch, 1995).
Paraphrasing and textual entailment methods are also useful in several other natural language
processing applications. In text summarization (Mani, 2001; Hovy, 2003), for example, an important processing stage is typically sentence extraction, which identifies the most important sentences
of the texts to be summarized. During that stage, especially when generating a single summary from
several documents (Barzilay & McKeown, 2005), it is important to avoid selecting sentences (e.g.,
from different news articles about the same event) that convey the same information (paraphrases)
as other sentences that have already been selected, or sentences whose information follows from
other already selected sentences (textual entailment).
Sentence compression (Knight & Marcu, 2002; McDonald, 2006; Cohn & Lapata, 2008; Clarke
& Lapata, 2008; Cohn & Lapata, 2009; Galanis & Androutsopoulos, 2010), often also a processing
stage of text summarization, can be seen as a special case of sentence paraphrasing, as suggested by
3 Culicover

(1968) discussed different types of paraphrasing and entailment, and proposed the earliest computational
treatment of paraphrasing and textual entailment that we are aware of, with the goal of retrieving passages of texts that
answer natural language queries. We thank one of the anonymous reviewers for pointing us to Culicovers work.
4 Passage (18) is based on Wikipedias page for Doryphoros.

137

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Zhao et al. (2009), with the additional constraint that the resulting sentence must be shorter than the
original one and still grammatical; for example, a sentence matching (5) or (6) could be shortened by
converting it to a paraphrase of the form of (4). Most sentence compression work, however, allows
less important information of the original sentence to be discarded. Hence, the resulting sentence is
entailed by, it is not necessarily a paraphrase of the original one. In the following example, (21) is
a compressed form of (20) produced by a human.5
(20)
(21)

Mother Catherine, 82, the mother superior, will attend the hearing on Friday, he said.
Mother Catherine, 82, the mother superior, will attend.

When the compressed sentence is not necessarily a paraphrase of the original one, we may first
produce (grammatical) candidate compressions that are textually entailed by the original sentence;
hence, a mechanism to generate textually entailed sentences is useful. Additional mechanisms are
needed, however, to rank the candidates depending on the space they save and the degree to which
they maintain important information; we do not discuss additional mechanisms of this kind.
Information extraction systems (Grishman, 2003; Moens, 2006) often rely on manually or automatically crafted patterns (Muslea, 1999) to locate text snippets that report particular types of
events and to identify the entities involved; for example, patterns like (22)(24), or similar patterns
operating on syntax trees, possibly with additional semantic constraints, might be used to locate
snippets referring to bombing incidents and identify their targets. Paraphrasing or textual entailment methods can be used to generate additional semantically equivalent extraction patterns (in the
case of paraphrasing) or patterns that textually entail the original ones (Shinyama & Sekine, 2003).
(22)
(23)
(24)

X was bombed
bomb exploded near X
explosion destroyed X

In machine translation (Koehn, 2009), ideas from paraphrasing and textual entailment research
have been embedded in measures and processes that automatically evaluate machine-generated
translations against human-authored ones that may use different phrasings (Lepage & Denoual,
2005; Zhou, Lin, & Hovy, 2006a; Kauchak & Barzilay, 2006; Pado, Galley, Jurafsky, & Manning,
2009); we return to this issue in following sections. Paraphrasing methods have also been used
to automatically generate additional reference translations from human-authored ones when training machine translation systems (Madnani, Ayan, Resnik, & Dorr, 2007). Finally, paraphrasing
and textual entailment methods have been employed to allow machine translation systems to cope
with source language words and longer phrases that have not been encountered in training corpora
(Zhang & Yamamoto, 2005; Callison-Burch, Koehn, & Osborne, 2006a; Marton, Callison-Burch, &
Resnik, 2009; Mirkin, Specia, Cancedda, Dagan, Dymetman, & Szpektor, 2009b). To use an example of Mirkin et al. (2009b), a phrase-based machine translation system that has never encountered
the expression file a lawsuit during its training, but which knows that pattern (25) textually entails
(26), may be able to produce a more acceptable translation by converting (27) to (28), and then
translating (28). Some information would be lost in the translation, because (28) is not a paraphrase
of (27), but the translation may still be preferable to the outcome of translating directly (27).
(25)

X filed a lawsuit against Y for Z.

5 Example

from Clarke et al.s paper, Written News Compression Corpus (Clarke & Lapata, 2008); see Appendix A.

138

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

(26)
(27)
(28)

X accused Y of Z.
Cisco filed a lawsuit against Apple for patent violation.
Cisco accused Apple of patent violation.

In natural language generation (Reiter & Dale, 2000; Bateman & Zock, 2003), for example
when producing texts describing the entities of a formal ontology (ODonnell, Mellish, Oberlander, & Knott, 2001; Androutsopoulos, Oberlander, & Karkaletsis, 2007), paraphrasing can be used
to avoid repeating the same phrasings (e.g., when expressing properties of similar entities), or to
produce alternative expressions that improve text coherence, adhere to writing style (e.g., avoid
passives), or satisfy other constraints (Power & Scott, 2005). Among other possible applications,
paraphrasing and textual entailment methods can be employed to simplify texts, for example by replacing specialized (e.g., medical) terms with expressions non-experts can understand (Elhadad &
Sutaria, 2007; Deleger & Zweigenbaum, 2009), and to automatically score student answers against
reference answers (Nielsen, Ward, & Martin, 2009).
1.2 The Relation of Paraphrasing and Textual Entailment to Logical Entailment
If we represent the meanings of natural language expressions by logical formulae, for example in
first-order predicate logic, we may think of textual entailment and paraphrasing in terms of logical
entailment (|=). If the logical meaning representations of T and H are T and H , then hT, Hi is a
correct textual entailment pair if and only if (T  B) |= H ; B is a knowledge base, for simplicity
assumed here to have the form of a single conjunctive formula, which contains meaning postulates
(Carnap, 1952) and other knowledge assumed to be shared by all language users.6 Let us consider
the example below, where logical terms starting with capital letters are constants; we assume that
different word senses would give rise to different predicate symbols. Let us also assume that B
contains only . Then (T  ) |= H holds, i.e., H is true for any interpretation (e.g., modeltheoretic) of constants, predicate names and other domain-dependent atomic symbols, for which T
and  both hold. A sound and complete automated reasoner (e.g., based on resolution, in the case
of first-order predicate logic) could be used to confirm that the logical entailment holds. Hence, T
textually entails H, assuming again that the meaning postulate  is available. The reverse, however,
does not hold, i.e., (H  ) 6|= T ; the implication () of  would have to be made bidirectional
() for the reverse to hold.
T

:

Leonardo da Vinci painted the Mona Lisa.

T

: isPainterOf(DaVinci, MonaLisa)

H

:

H


Mona Lisa is the work of Leonardo da Vinci.

: isWorkOf(MonaLisa, DaVinci)
: x y isPainterOf(x, y)  isWorkOf(y, x)

Similarly, if the logical meaning representations of T1 and T2 are 1 and 2 , then T1 is a paraphrase of T2 iff (1  B) |= 2 and (2  B) |= 1 , where again B contains meaning postulates and
common sense knowledge. Ideally, sentences like (1)(3) would be represented by the same formula, making it clear that they are paraphrases, regardless of the contents of B. Otherwise, it may
6 Zaenen

et al. (2005) provide examples showing that linguistic and world knowledge cannot often be separated.

139

fiA NDROUTSOPOULOS & M ALAKASIOTIS

sometimes be unclear if T1 and T2 should be considered paraphrases, because it may be unclear if
some knowledge should be considered part of B.
Since natural language expressions are often ambiguous, especially out of context, we may again
want to adopt looser definitions, so that T textually entails H iff there are possible readings of T and
H, represented by H and T , such that (T  B) |= H , and similarly for paraphrases. Thinking of
textual entailment and paraphrasing in terms of logical entailment allows us to borrow notions and
methods from logic. Indeed, some paraphrasing and textual entailment recognition methods map
natural language expressions to logical formulae, and then examine if logical entailments hold. This
is not, however, the only possible approach. Many other, if not most, methods currently operate on
surface strings or syntactic representations, without mapping natural language expressions to formal
meaning representations. Note, also, that in methods that map natural language to logical formulae,
it is important to work with a form of logic that provides adequate support for logical entailment
checks; full first-order predicate logic may be inappropriate, as it is semi-decidable.
To apply our logic-based definition of textual entailment, which was formulated for statements,
to questions, let us use identical fresh constants (in effect, Skolem constants) across questions to
represent the unknown entities the questions ask for; we mark such constants with question marks
as subscripts, but in logical entailment checks they can be treated as ordinary constants. In the following example, the user asks H, and the system generates T . Assuming that the meaning postulate
 is available in B, (T B) |= H , i.e., for any interpretation of the predicate symbols and constants,
if (T  B) is true, then H is necessarily also true. Hence, T textually entails H. In practice, this
means that if the system manages to find an answer to T , perhaps because T s phrasing is closer to
a sentence in a document collection, the same answer can be used to respond to H.
T (generated) :
T

Who painted the Mona Lisa?

: isAgent(W? )  isPainterOf(W? , MonaLisa)

H(asked) :

Whose work is the Mona Lisa?

H

: isAgent(W? )  isWorkOf(MonaLisa,W? )



: x y isPainterOf(x, y)  isWorkOf(y, x)

A logic-based definition of question paraphrases can be formulated in a similar manner, as
bidirectional logical entailment. Note also that logic-based paraphrasing and textual entailment
methods may actually represent interrogatives as free variables, instead of fresh constants, and they
may rely on unification to obtain their values (Moldovan & Rus, 2001; Rinaldi, Dowdall, Kaljurand,
Hess, & Molla, 2003).
1.3 A Classification of Paraphrasing and Textual Entailment Methods
There have been six workshops on paraphrasing and/or textual entailment (Sato & Nakagawa, 2001;
Inui & Hermjakob, 2003; Dolan & Dagan, 2005; Drass & Yamamoto, 2005; Sekine, Inui, Dagan,
Dolan, Giampiccolo, & Magnini, 2007; Callison-Burch, Dagan, Manning, Pennacchiotti, & Zanzotto, 2009) in recent years.7 The Recognizing Textual Entailment (RTE) challenges (Dagan et al.,
2006; Bar-Haim et al., 2006; Giampiccolo, Magnini, Dagan, & Dolan, 2007; Giampiccolo, Dang,
7 The

proceedings of the five more recent workshops are available in the ACL Anthology.

140

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Magnini, Dagan, & Dolan, 2008), currently in their fifth year, provide additional significant thrust.8
Consequently, there is a large number of published articles, proposed methods, and resources related to paraphrasing and textual entailment.9 A special issue on textual entailment was also recently
published, and its editorial provides a brief overview of textual entailment methods (Dagan, Dolan,
Magnini, & Roth, 2009).10 To the best of our knowledge, however, the present article is the first
extensive survey of paraphrasing and textual entailment.
To provide a clearer view of the different goals and assumptions of the methods that have been
proposed, we classify them along two dimensions: whether they are paraphrasing or textual entailment methods; and whether they perform recognition, generation, or extraction of paraphrases
or textual entailment pairs. These distinctions are not always clear in the literature, especially the
distinctions along the second dimension, which we explain below. It is also possible to classify
methods along other dimensions, for example depending on whether they operate on language expressions or templates; or whether they operate on phrases, sentences or longer texts.
The main input to a paraphrase or textual entailment recognizer is a pair of language expressions
(or templates), possibly in particular contexts. The output is a judgement, possibly probabilistic, indicating whether or not the members of the input pair are paraphrases or a correct textual entailment
pair; the judgements must agree as much as possible with those of humans. On the other hand, the
main input to a paraphrase or textual entailment generator is a single language expression (or template) at a time, possibly in a particular context. The output is a set of paraphrases of the input, or
a set of language expressions that entail or are entailed by the input; the output set must be as large
as possible, but including as few errors as possible. In contrast, no particular language expressions
or templates are provided to a paraphrase or textual entailment extractor. The main input in this
case is a corpus, for example a monolingual corpus of parallel or comparable texts, such as different
English translations of the same French novel, or clusters of multiple monolingual news articles,
with the articles in each cluster reporting the same event. The system outputs pairs of paraphrases
(possibly templates), or pairs of language expressions (or templates) that constitute correct textual
entailment pairs, based on the evidence of the corpus; the goal is again to produce as many output
pairs as possible, with as few errors as possible. Note that the boundaries between recognizers, generators, and extractors may not always be clear. For example, a paraphrase generator may invoke a
paraphrase recognizer to filter out erroneous candidate paraphrases; and a recognizer or a generator
may consult a collection of template pairs produced by an extractor.
We note that articles reporting actual applications of paraphrasing and textual entailment methods to larger systems (e.g., for QA, information extraction, machine translation, as discussed in
Section 1.1) are currently relatively few, compared to the number of articles that propose new paraphrasing and textual entailment methods or that test them in vitro, despite the fact that articles of the
second kind very often point to possible applications of the methods they propose. The relatively
small number of application articles may be an indicator that paraphrasing and textual entailment
methods are not used extensively in larger systems yet. We believe that this may be due to at least
two reasons. First, the efficiency of the methods needs to be improved, which may require combining recognition, generation, and extraction methods, for example to iteratively produce more
training data; we return to this point in following sections. Second, the literature on paraphrasing
8 The RTE

challenges were initially organized by the European PASCAL Network of Excellence, and subsequently as
part of NISTs Text Analysis Conference.
9 A textual entailment portal has been established, as part of ACL s wiki, to help organize all relevant material.
10 The slides of Dagan, Roth, and Zazottos ACL 2007 tutorial on textual entailment are also publicly available.

141

fiA NDROUTSOPOULOS & M ALAKASIOTIS

and textual entailment is vast, which makes it difficult for researchers working on larger systems to
assimilate its key concepts and identify suitable methods. We hope that this article will help address
the second problem, while also acting as an introduction that may help new researchers improve
paraphrasing and textual entailment methods further.
In Sections 2, 3, and 4 below we consider in turn recognition, generation, and extraction methods
for both paraphrasing and textual entailment. In each of the three sections, we attempt to identify
and explain prominent ideas, pointing also to relevant articles and resources. In Section 5, we
conclude and discuss some possible directions for future research. The URLs of all publicly available
resources that we mention are listed in appendix A.

2. Paraphrase and Textual Entailment Recognition
Paraphrase and textual entailment recognizers judge whether or not two given language expressions
(or templates) constitute paraphrases or a correct textual entailment pair. Different methods may
operate at different levels of representation of the input expressions; for example, they may treat the
input expressions simply as surface strings, they may operate on syntactic or semantic representations of the input expressions, or on representations combining information from different levels.
2.1 Logic-based Approaches to Recognition
One possibility is to map the language expressions to logical meaning representations, and then
rely on logical entailment checks, possibly by invoking theorem provers (Rinaldi et al., 2003; Bos
& Markert, 2005; Tatu & Moldovan, 2005, 2007). In the case of textual entailment, this involves
generating pairs of formulae hT , H i for T and H (or their possible readings), and then checking
if (T  B) |= H , where B contains meaning postulates and common sense knowledge, as already
discussed. In practice, however, it may be very difficult to formulate a reasonably complete B. A
partial solution to this problem is to obtain common sense knowledge from resources like WordNet
(Fellbaum, 1998) or Extended WordNet (Moldovan & Rus, 2001). The latter also includes logical
meaning representations extracted from WordNets glosses. For example, since assassinate is a
hyponym (more specific sense) of kill in WordNet, an axiom like the following can be added to B
(Moldovan & Rus, 2001; Bos & Markert, 2005; Tatu & Moldovan, 2007).
x y assassinate(x, y)  kill(x, y)
Additional axioms can be obtained from FrameNets frames (Baker, Fillmore, & Lowe, 1998;
Lonneker-Rodman & Baker, 2009), as discussed for example by Tatu et al. (2005), or similar resources. Roughly speaking, a frame is the representation of a prototypical situation (e.g., a purchase), which also identifies the situations main roles (e.g., the buyer, the entity bought), the types
of entities (e.g., person) that can play these roles, and possibly relations (e.g., causation, inheritance)
to other prototypical situations (other frames). VerbNet (Schuler, 2005) also specifies, among other
information, semantic frames for English verbs. On-line encyclopedias have also been used to obtain background knowledge by extracting particular types of information (e.g., is-a relationships)
from their articles (Iftene & Balahur-Dobrescu, 2007).
Another approach is to use no particular B (meaning postulates and common sense knowledge),
and measure how difficult it is to satisfy both T and H , in the case of textual entailment recognition, compared to satisfying T on its own. A possible measure is the difference of the size of
142

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

the minimum model that satisfies both T and H , compared to the size of the minimum model that
satisfies T on its own (Bos & Markert, 2005); intuitively, a model is an assignment of entities,
relations etc. to terms, predicate names, and other domain-dependent atomic symbols. The greater
this difference the more knowledge is required in B for (T  B) |= H to hold, and the more difficult
it becomes for speakers to accept that T textually entails H. Similar bidirectional logical entailment
checks can be used to recognize paraphrases (Rinaldi et al., 2003).
2.2 Recognition Approaches that Use Vector Space Models of Semantics
An alternative to using logical meaning representations is to start by mapping each word of the
input language expressions to a vector that shows how strongly the word cooccurs with particular
other words in corpora (Lin, 1998b), possibly also taking into account syntactic information, for
example requiring that the cooccurring words participate in particular syntactic dependencies (Pado
& Lapata, 2007). A compositional vector-based meaning representation theory can then be used to
combine the vectors of single words, eventually mapping each one of the two input expressions to a
single vector that attempts to capture its meaning; in the simplest case, the vector of each expression
could be the sum or product of the vectors of its words, but more elaborate approaches have also
been proposed (Mitchell & Lapata, 2008; Erk & Pado, 2009; Clarke, 2009). Paraphrases can then
be detected by measuring the distance of the vectors of the two input expressions, for example by
computing their cosine similarity. See also the work of Turney and Pantel (2010) for a survey of
vector space models of semantics.
Recognition approaches based on vector space models of semantics appear to have been explored much less than other approaches discussed in this article, and mostly in paraphrase recognition (Erk & Pado, 2009). They could also be used in textual entailment recognition, however, by
checking if the vector of H is particularly close to that of a part (e.g., phrase or sentence) of T . Intuitively, this would check if what H says is included in what T says, though we must be careful with
negations and other expressions that do not preserve truth values (Zaenen et al., 2005; MacCartney
& Manning, 2009), as in (29)(30). We return to the idea of matching H to a part of T below.
(29)
(30)

T : He denied that BigCo bought SmallCo.
H: BigCo bought SmallCo.

2.3 Recognition Approaches Based on Surface String Similarity
Several paraphrase recognition methods operate directly on the input surface strings, possibly after
applying some pre-processing, such as part-of-speech (POS) tagging or named-entity recognition,
but without computing more elaborate syntactic or semantic representations. For example, they
may compute the string edit distance (Levenshtein, 1966) of the two input strings, the number of
their common words, or combinations of several string similarity measures (Malakasiotis & Androutsopoulos, 2007), including measures originating from machine translation evaluation (Finch,
Hwang, & Sumita, 2005; Perez & Alfonseca, 2005; Zhang & Patrick, 2005; Wan, Dras, Dale, &
Paris, 2006). The latter have been developed to automatically compare machine-generated translations against human-authored reference translations. A well known measure is BLEU (Papineni,
Roukos, Ward, & Zhu, 2002; Zhou et al., 2006a), which roughly speaking examines the percentage
of word n-grams (sequences of consecutive words) of the machine-generated translations that also
occur in the reference translations, and takes the geometric average of the percentages obtained for
different values of n. Although such n-gram based measures have been criticised in machine transla143

fiA NDROUTSOPOULOS & M ALAKASIOTIS

tion evaluation (Callison-Burch, Osborne, & Koehn, 2006b), for example because they are unaware
of synonyms and longer paraphrases, they can be combined with other measures to build paraphrase (and textual entailment) recognizers (Zhou et al., 2006a; Kauchak & Barzilay, 2006; Pado
et al., 2009), which may help address the problems of automated machine translation evaluation.
In textual entailment recognition, one of the input language expressions (T ) is often much longer
than the other one (H). If a part of T s surface string is very similar to Hs, this is an indication
that H may be entailed by T . This is illustrated in (31)(32), where H is included verbatim in T .11
Note, however, that the surface string similarity (e.g., measured by string edit distance) between H
and the entire T of this example is low, because of the different lengths of T and H.
(31)

(32)

T : Charles de Gaulle died in 1970 at the age of eighty. He was thus fifty years old when, as an unknown officer recently promoted to the rank of brigadier general, he made his famous broadcast from
London rejecting the capitulation of France to the Nazis after the debacle of May-June 1940.
H: Charles de Gaulle died in 1970.

Comparing H to a sliding window of T s surface string of the same size as H (in our example, six
consecutive words of T ) and keeping the largest similarity score between the sliding window and
H may provide a better indication of whether T entails H or not (Malakasiotis, 2009). In many
correct textual entailment pairs, however, using a single sliding window of a fixed length may still
be inadequate, because H may correspond to several non-continuous parts of T ; in (33)(34), for
example, H corresponds to the three underlined parts of T .12
(33)
(34)

T : The Gaspe, also known as la Gaspesie in French, is a North American peninsula on the south shore
of the Saint Lawrence River, in Quebec.
H: The Gaspe is a peninsula in Quebec.

One possible solution is to attempt to align the words (or phrases) of H to those of T , and
consider T H a correct textual entailment pair if a sufficiently good alignment is found, in the
simplest case if a large percentage of T s words are aligned to words of H. Another approach would
be to use a window of variable length; the window could be, for example, the shortest span of T
that contains all of T s words that are aligned to words of H (Burchardt, Pennacchiotti, Thater, &
Pinkal, 2009). In any case, we need to be careful with negations and other expressions that do
not preserve truth values, as already mentioned. Note, also, that although effective word alignment
methods have been developed in statistical machine translation (Brown, Della Pietra, Della Pietra,
& Mercer, 1993; Vogel, Ney, & Tillmann, 1996; Och & Ney, 2003), they often perform poorly on
textual entailment pairs, because T and H are often of very different lengths, they do not necessarily
convey the same information, and textual entailment training datasets are much smaller than those
used in machine translation; see MacCartney et al.s (2008) work for further related discussion and
a word alignment method developed especially for textual entailment pairs.13
2.4 Recognition Approaches Based on Syntactic Similarity
Another common approach is to work at the syntax level. Dependency grammar parsers (Melcuk,
1987; Kubler, McDonald, & Nivre, 2009) are popular in paraphrasing and textual entailment re11 Example

from the dataset of RTE-3 (Giampiccolo et al., 2007).
example from the dataset of the RTE-3 (Giampiccolo et al., 2007).
13 Cohn et al. (2008) discuss how a publicly available corpus with manually word-aligned paraphrases was constructed.
Other word-aligned paraphrasing or textual entailment datasets can be found at the ACL Textual Entailment Portal.
12 Modified

144

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

A mathematician solved the problem The problem was solved by a young mathematician

solved
subj

solved

obj

obj aux

mathematician

problem

problem

det

det

det

A

the

The

by

was

mathematician
det
a

mod
young

Figure 1: Two sentences that are very similar when viewed at the level of dependency trees.
search, as in other natural language processing areas recently. Instead of showing hierarchically the
syntactic constituents (e.g., noun phrases, verb phrases) of a sentence, the output of a dependency
grammar parser is a graph (usually a tree) whose nodes are the words of the sentence and whose
(labeled) edges correspond to syntactic dependencies between words, for example the dependency
between a verb and the head noun of its subject noun phrase, or the dependency between a noun
and an adjective that modifies it. Figure 1 shows the dependency trees of two sentences. The exact
form of the trees and the edge labels would differ, depending on the parser; for simplicity, we show
prepositions as edges. If we ignore word order and the auxiliary was of the passive (right) sentence, and if we take into account that the by edge of the passive sentence corresponds to the subj
edge of the active (left) one, the only difference is the extra adjective of the passive sentence. Hence,
it is easy to figure out from the dependency trees that the two sentences have very similar meanings,
despite their differences in word order. Strictly speaking, the right sentence textually entails the left
one, not the reverse, because of the word young in the right sentence.
Some paraphrase recognizers simply count the common edges of the dependency trees of the
input expressions (Wan et al., 2006; Malakasiotis, 2009) or use other tree similarity measures. A
large similarity score (e.g., above a threshold) indicates that the input expressions may be paraphrases. Tree edit distance (Selkow, 1977; Tai, 1979; Zhang & Shasha, 1989) is another example
of a similarity measure that can be applied to dependency or other parse trees; it computes the sequence of operator applications (e.g., add, replace, or remove a node or edge) with the minimum
cost that turns one tree into the other.14 To obtain more accurate predictions, it is important to devise
an appropriate inventory of operators and assign appropriate costs to the operators during a training
stage (Kouylekov & Magnini, 2005; Mehdad, 2009; Harmeling, 2009). For example, replacing a
noun with one of its synonyms should be less costly than replacing it with an unrelated word; and
removing a dependency between a verb and an adverb should perhaps be less costly than removing
a dependency between a verb and the head noun of its subject or object.
In textual entailment recognition, one may compare Hs parse tree against subtrees of T s parse
tree (Iftene & Balahur-Dobrescu, 2007; Zanzotto, Pennacchiotti, & Moschitti, 2009). It may be
possible to match Hs tree against a single subtree of T , in effect a single syntactic window on T ,
as illustrated in Figure 2, which shows the dependency trees of (33)(34); recall that (34) does not
match a single window of (33) at the surface string level.15 This is also a further example of how
operating at a higher level than surface strings may reveal similarities that may be less clear at lower
14 EDITS ,
15 Figure

a suite to recognize textual entailment by computing edit distances, is publicly available.
2 is based on the output of Stanfords parser. One might argue that North should modify American.

145

fiA NDROUTSOPOULOS & M ALAKASIOTIS

peninsula
subj
Gaspe
det

aux

det
a

is

mod

mod

North

mod

American

mod

The

known
mod

mod

also

on

in

obj

obj

North

Quebec

det mod mod
as

the

south

of

obj

obj

Gaspesie

River

mod
la

mod

det mod mod

mod
the

in

Saint

Lawrence

obj
French

Figure 2: An example of how dependency treespeninsula
may make it easier to match a short sentence (subsubj
aux
mod one.
tree inside the dashed line) to a part of a det
longer
Gaspe

a

is

in
obj

det

levels. Another example is (35)(36);
The although (35) includes
Quebecverbatim (36), it does not textually
entail (36).16 This is clear when one compares the syntactic representations of the two sentences:
Israel is the subject of was established in (36), but not in (35). The difference, however, is not
evident at the surface string level, and a sliding window of (35) would match exactly (36), wrongly
suggesting a textual entailment.
(35)
(36)

T : The National Institute for Psychobiology in Israel was established in 1979.
H: Israel was established in 1979.

Similar arguments can be made in favour of computing similarities at the semantic level (Qiu,
Kan, & Chua, 2006); for example, both the active and passive forms of a sentence may be mapped
to the same logical formula, making their similarity clearer than at the surface or syntax level. The
syntactic or semantic representations of the input expressions, however, cannot always be computed
accurately (e.g., due to parser errors), which may introduce noise; and, possibly because of the
noise, methods that operate at the syntactic or semantic level do not necessarily outperform in
practice methods that operate on surface strings (Wan et al., 2006; Burchardt, Reiter, Thater, &
Frank, 2007; Burchardt et al., 2009).
2.5 Recognition via Similarity Measures Operating on Symbolic Meaning Representations
Paraphrases may also be recognized by computing similarity measures on graphs whose edges do
not correspond to syntactic dependencies, but reflect semantic relations mentioned in the input expressions (Haghighi, 2005), for example the relation between a buyer and the entity bought. Relations of this kind may be identified by applying semantic role labeling methods (Marquez, Carreras,
16 Modified

example from Haghighi et al.s (2005) work.

146

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Litkowski, & Stevenson, 2008) to the input language expressions. It is also possible to compute
similarities between meaning representations that are based on FrameNets frames (Burchardt et al.,
2007). The latter approach has the advantage that semantically related expressions may invoke the
same frame (as with announcement, announce, acknowledge) or interconnected frames (e.g.,
FrameNet links the frame invoked by arrest to the frame invoked by trial via a path of temporal precedence relations), making similarities and implications easier to capture (Burchardt et al.,
2009).17 The prototypical semantic roles that PropBank (Palmer, Gildea, & Kingsbury, 2005) associates with each verb may also be used in a similar manner, instead of FrameNets frames. Similarly,
in the case of textual entailment recognition, one may compare Hs semantic representation (e.g.,
semantic graph or frame) to parts of T s representation.
WordNet (Fellbaum, 1998), automatically constructed collections of near synonyms (Lin, 1998a;
Moore, 2001; Brockett & Dolan, 2005), or resources like NOMLEX (Meyers, Macleod, Yangarber,
Grishman, Barrett, & Reeves, 1998) and C AT VAR (Habash & Dorr, 2003) that provide nominalizations of verbs and other derivationally related words across different POS categories (e.g., to invent
and invention), can be used to match synonyms, hypernymshyponyms, or, more generally, semantically related words across the two input expressions. According to WordNet, in (37)(38)
shares is a direct hyponym (more specific meaning) of stock, slumped is a direct hyponym
of dropped, and company is an indirect hyponym (two levels down) of organization.18 By
treating semantically similar words (e.g., synonyms, or hypernyms-hyponyms up to a small hierarchical distance) as identical (Rinaldi et al., 2003; Finch et al., 2005; Tatu, Iles, Slavick, Novischi, &
Moldovan, 2006; Iftene & Balahur-Dobrescu, 2007; Malakasiotis, 2009; Harmeling, 2009), or by
considering (e.g., counting) semantically similar words across the two input language expressions
(Brockett & Dolan, 2005; Bos & Markert, 2005), paraphrase recognizers may be able to cope with
paraphrases that have very similar meanings, but very few or no common words.
(37)
(38)

The shares of the company dropped.
The organizations stock slumped.

In textual entailment recognition, it may be desirable to allow the words of T to be more distant
hyponyms of the words of H, compared to paraphrase recognition. For example, X is a computer
textually entails X is an artifact, and computer is a hyponym of artifact four levels down.
Measures that exploit WordNet (or similar resources) and compute the semantic similarity
between two words or, more generally, two texts have also been proposed (Leacock, Miller, &
Chodorow, 1998; Lin, 1998c; Resnik, 1999; Budanitsky & Hirst, 2006; Tsatsaronis, Varlamis, &
Vazirgiannis, 2010).19 Some of them are directional, making them more suitable to textual entailment recognition (Corley & Mihalcea, 2005). Roughly speaking, measures of this kind consider
(e.g., sum the lengths of) the paths in WordNets hierarchies (or similar resources) that connect the
senses of corresponding (e.g., most similar) words across the two texts. They may also take into
account information such as the frequencies of the words in the two texts and how rarely they are
encountered in documents of a large collection (inverse document frequency). The rationale is that
frequent words of the input texts that are rarely used in a general corpus are more important, as in
17 Consult,

for example, the work of Erk and Pado (2006) for a description of a system that can annotate texts with
FrameNet frames. The FATE corpus (Burchardt & Pennacchiotti, 2008), a version of the RTE 2 test set (Bar-Haim et al.,
2006) with FrameNet annotations, is publicly available.
18 Modified example from the work of Tsatsaronis (2009)
19 Pedersens WordNet::Similarity package implements many of these measures.

147

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Training stage
(P1,1, P2,1) X
(P1,2, P2,2) 

(P1,n, P2,n) 

<f1,1, , fm,1> X
<f1,2, , fm,2> 

<f1,n, , fm,n> 

Vector creation
Preprocessing

Learning
Algorithm
Trained
Classifier

Classification stage
(P1, P2) ?

<f1, , fm> ?

Vector creation

Trained
Classifier

Preprocessing
Decision: 

Figure 3: Paraphrase and textual entailment recognition via supervised machine learning.
information retrieval; hence, the paths that connect them should be assigned greater weights. Since
they often consider paths between word senses, many of these measures would ideally be combined
with word sense disambiguation (Yarowski, 2000; Stevenson & Wilks, 2003; Kohomban & Lee,
2005; Navigli, 2008), which is not, however, always accurate enough for practical purposes.
2.6 Recognition Approaches that Employ Machine Learning
Multiple similarity measures, possibly computed at different levels (surface strings, syntactic or
semantic representations) may be combined by using machine learning (Mitchell, 1997; Alpaydin, 2004), as illustrated in Figure 3.20 Each pair of input language expressions hP1 , P2 i, i.e., each
pair of expressions we wish to check if they are paraphrases or a correct textual entailment pair,
is represented by a feature vector h f1 , . . . , fm i. The vector contains the scores of multiple similarity measures applied to the pair, and possibly other features. For example, many systems also
include features that check for polarity differences across the two input expressions, as in this is
not a confirmed case of rabies vs. a case of rabies was confirmed, or modality differences, as
in a case may have been confirmed vs. a case has been confirmed (Haghighi, 2005; Iftene &
Balahur-Dobrescu, 2007; Tatu & Moldovan, 2007). Bos and Markert (2005) also include features
indicating if a theorem prover has managed to prove that the logical representation of one of the
input expressions entails the other or contradicts it. A supervised machine learning algorithm trains
a classifier on manually classified (as correct or incorrect) vectors corresponding to training input
pairs. Once trained, the classifier can classify unseen pairs as correct or incorrect paraphrases or
textual entailment pairs by examining their features (Bos & Markert, 2005; Brockett & Dolan, 2005;
Zhang & Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Burchardt et al., 2007; Hickl, 2008;
Malakasiotis, 2009; Nielsen et al., 2009).
A preprocessing stage is commonly applied to each input pair of language expressions, before
converting it to a feature vector (Zhang & Patrick, 2005). Part of the preprocessing may provide
20 WEKA (Witten & Frank, 2005) provides implementations of several well known machine learning algorithms, including C 4.5 (Quinlan, 1993), Naive Bayes (Mitchell, 1997), SVMs (Vapnik, 1998; Cristianini & Shawe-Taylor, 2000;
Joachims, 2002), and AdaBoost (Freund & Schapire, 1995; Friedman, Hastie, & Tibshirani, 2000). More efficient implementations of SVMs, such as LIBSVM and SVM - LIGHT, are also available. Maximum Entropy classifiers are also very
effective; see chapter 6 of the book Speech and Language Processing (Jurafsky & Martin, 2008) for an introduction;
Stanfords implementation is frequently used.

148

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

information that is required to compute the features; for example, this is when a POS tagger or a
parser would be applied.21 The preprocessing may also normalize the input pairs; for example, a
stemmer may be applied; dates may be converted to a consistent format; names of persons, organizations, locations etc. may be tagged by their semantic categories using a named entity recognizer;
pronouns or, more generally, referring expressions, may be replaced by the expressions they refer to
(Hobbs, 1986; Lappin & Leass, 1994; Mitkov, 2003; Molla, Schwitter, Rinaldi, Dowdall, & Hess,
2003; Yang, Su, & Tan, 2008); and morphosyntactic variations may be normalized (e.g., passive
sentences may be converted to active ones).22
Instead of mapping each hP1 , P2 i pair to a feature vector that contains mostly scores measuring the similarity between P1 and P2 , it is possible to use vectors that encode directly parts of P1
and P2 , or parts of their syntactic or semantic representations. Zanzotto et al. (2009) project each
hP1 , P2 i pair to a vector that, roughly speaking, contains as features all the fragments of P1 and P2 s
parse trees. Leaf nodes corresponding to identical or very similar words (according to a WordNetbased similarity measure) across P1 and P2 are replaced by co-indexed slots, to allow the features
to be more general. Zanzotto et al. define a measure (actually, different versions of it) that, in effect, computes the similarity of two pairs hP1 , P2 i and hP10 , P20 i by counting the parse tree fragments
(features) that are shared by P1 and P10 , and those shared by P2 and P20 . The measure is used as a
kernel in an Support Vector Machine (SVM) that learns to separate positive textual entailment pairs
hP1 , P2 i = hT, Hi from negative ones. A (valid) kernel can be thought of as a similarity measure that
projects two objects to a highly dimensional vector space, where it computes the inner product of
the projected objects; efficient kernels compute the inner product directly from the original objects,
without computing their projections to the highly dimensional vector space (Vapnik, 1998; Cristianini & Shawe-Taylor, 2000; Joachims, 2002). In Zanzotto et al.s work, each object is a hT, Hi
pair, and its projection is the vector that contains all the parse tree fragments of T and H as features.
Consult, for example, the work of Zanzotto and Dell Arciprete (2009) and Moschitti (2009) for
further discussion of kernels that can be used in paraphrase and textual entailment recognition.
2.7 Recognition Approaches Based on Decoding
Pairs of paraphrasing or textual entailment expressions (or templates) like (39), often called rules,
that may have been produced by extraction mechanisms (to be discussed in Section 4) can be used
by recognizers much as, and often in addition to synonyms and hypernyms-hyponyms.
(39)

X is fond of Y   X likes Y 

Given the paraphrasing rule of (39) and the information that child is a synonym of kid and
candy a hyponym of sweet, a recognizer could figure out that (40) textually entails (43) by
gradually transforming (40) to (43) as shown below.23
(40)

Children are fond of sweets.

21 Brills (1992) POS tagger is well-known and publicly available. Stanfords tagger (Toutanova, Klein, Manning, &
Singer, 2003) is another example of a publicly available POS tagger. Commonly used parsers include Charniaks (2000),
Collins (2003), the Link Grammar Parser (Sleator & Temperley, 1993), MINIPAR, a principle-based parser (Berwick,
1991) very similar to PRINCIPAR (Lin, 1994), MaltParser (Nivre, Hall, Nilsson, Chanev, Eryigit, Kuebler, Marinov, &
Marsi, 2007), and Stanfords parser (Klein & Manning, 2003).
22 Porters stemmer (1997) is well-known. An example of a publicly available named-entity recognizer is Stanfords.
23 Modified example from Bar-Haim et al.s (2009) work.

149

fiA NDROUTSOPOULOS & M ALAKASIOTIS

(41)
(42)
(43)

Kids are fond of sweets.
Kids like sweets.
Kids like candies.

Another recognition approach, then, is to search for a sequence of rule applications or other
transformations (e.g., replacing synonyms, or hypernyms-hyponyms) that turns one of the input
expressions (or its syntactic or semantic representation) to the other. We call this search decoding,
because it is similar to the decoding stage of machine translation (to be discussed in Section 3),
where a sequence of transformations that turns a source-language expression into a target-language
expression is sought. In our case, if a sequence is found, the two input expressions constitute a
positive paraphrasing or textual entailment pair, depending on the rules used; otherwise, the pair is
negative. If each rule is associated with a confidence score (possibly learnt from a training dataset)
that reflects the degree to which the rule preserves the original meaning in paraphrase recognition,
or the degree to which we are confident that it produces an entailed expression, we may search for
the sequence of transformations with the maximum score (or minimum cost), much as in approaches
that compute the minimum (string or tree) edit distance between the two input expressions. The pair
of input expressions can then be classified as positive if the maximum-score sequence exceeds a
confidence threshold (Harmeling, 2009). One would also have to consider the contexts where rules
are applied, because a rule may not be valid in all contexts, for instance because of the different
possible senses of the words it involves. A possible solution is to associate each rule with a vector
that represents the contexts where it can be used (e.g., a vector of frequently occurring words in
training contexts where the rule applies), and use a rule only in contexts that are similar to its
associated context vector; with slotted rules, one can also model the types of slot values (e.g., types
of named entities) the rule can be used with the work of Pantel, Bhagat, Coppola, Chklovski, and
Hovy (2007), and Szpektor, Dagan, Bar-Haim, and Goldberger (2008).
Resouces like WordNet and extraction methods, however, provide thousands or millions of rules,
giving rise to an exponentially large number of transformation sequences to consider.24 When
operating at the level of semantic representations, the sequence sought is in effect a proof that the
two input expressions are paraphrases or a valid textual entailment pair, and it may be obtained
by exploiting theorem provers, as discussed earlier. Bar-Haim et al. (2007) discuss how to search
for sequences of transformations, seen as proofs at the syntactic level, when the input language
expressions and their reformulations are represented by dependency trees. In subsequent work (BarHaim et al., 2009), they introduce compact forests, a data structure that allows the dependency trees
of multiple intermediate reformulations to be represented by a single graph, to make the search
more efficient. They also combine their approach with an SVM-based recognizer; sequences of
transformations are used to bring T closer to H, and the SVM recognizer is then employed to judge
if the transformed T and H consitute a positive textual entailment pair or not.
2.8 Evaluating Recognition Methods
Experimenting with paraphase and textual entailment recognizers requires datasets containing both
positive and negative input pairs. When using discriminative classifiers (e.g., SVMs), the negative
training pairs must ideally be near misses, otherwise they may be of little use (Schohn & Cohn,
2000; Tong & Koller, 2002). Near misses can also make the test data more challenging.
24 Collections of transformation rules and resources that can be used to obtain such rules are listed at the ACL Textual
Entailment Portal. Mirkin et al. (2009a) discuss how to evaluate collections of textual entailment rules.

150

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

method
Corley & Mihalcea (2005)
Das & Smith (2009)
Finch et al. (2005)
Malakasiotis (2009)
Qiu et al. (2006)
Wan et al. (2006)
Zhang & Patrick (2005)
BASE 1
BASE 2

accuracy (%)
71.5
76.1
75.0
76.2
72.0
75.6
71.9
66.5
69.0

precision (%)
72.3
79.6
76.6
79.4
72.5
77.0
74.3
66.5
72.4

recall (%)
92.5
86.1
89.8
86.8
93.4
90.0
88.2
100.0
86.3

F-measure (%)
81.2
82.9
82.7
82.9
81.6
83.0
80.7
79.9
78.8

Table 1: Paraphrase recognition results on the MSR corpus.

The most widely used benchmark dataset for paraphrase recognition is the Microsoft Research
(MSR) Paraphrase Corpus. It contains 5,801 pairs of sentences obtained from clusters of online news
articles referring to the same events (Dolan, Quirk, & Brockett, 2004; Dolan & Brockett, 2005).
The pairs were initially filtered by heuristics, which require, for example, the word edit distance
of the two sentences in each pair to be neither too small (to avoid nearly identical sentences) nor
too large (to avoid too many negative pairs); and both sentences to be among the first three of
articles from the same cluster (articles referring to the same event), the rationale being that initial
sentences often summarize the events. The candidate paraphrase pairs were then filtered by an
SVM -based paraphrase recognizer (Brockett & Dolan, 2005), trained on separate manually classified
pairs obtained in a similar manner, which was biased to overidentify paraphrases. Finally, human
judges annotated the remaining sentence pairs as paraphrases or not. After resolving disagreements,
approximately 67% of the 5,801 pairs were judged to be paraphrases. The dataset is divided in
two non-overlapping parts, for training (30% of all pairs) and testing (70%). Zhang and Patrick
(2005) and others have pointed out that the heuristics that were used to construct the corpus may
have biased it towards particular types of paraphrases, excluding for example paraphrases that do
not share any common words.
Table 1 lists all the published results of paraphrase recognition experiments on the MSR corpus
we are aware of. We include two baselines we used: BASE1 classifies all pairs as paraphrases;
BASE 2 classifies two sentences as paraphrases when their surface word edit distance is below a
threshold, tuned on the training part of the corpus. Four commonly used evaluation measures are
used: accuracy, precision, recall, and F-measure with equal weight on precision and recall. These
measures are defined below. TP (true positives) and FP (false positives) are the numbers of pairs
that have been correctly or incorrectly, respectively, classified as positive (paraphrases). TN (true
negatives) and FN (false negatives) are the numbers of pairs that have been correctly or incorrectly,
respectively, classified as negative (not paraphrases).
precision =
accuracy =

TP
TP+FP ,

recall =

TP+TN
TP+TN+FP+FN ,

F-measure =

TP
TP+FN ,
2precisionrecall
precision+recall

All the systems of Table 1 have better recall than precision, which implies they tend to over-classify
pairs as paraphrases, possibly because the sentences of each pair have at least some common words
and refer to the same event. Systems with higher recall tend to have lower precision, and vice versa,
as one would expect. The high F-measure of BASE1 is largely due to its perfect recall; its precision
151

fiA NDROUTSOPOULOS & M ALAKASIOTIS

method
Bensley & Hickl (2008)
Iftene (2008)
Siblini & Kosseim (2008)
Wang & Neumann (2008)
BASE 1
BASE 2

accuracy (%)
74.6
72.1
68.8
70.6
50.0
54.9

precision (%)

65.5


50.0
53.6

recall (%)

93.2


100.0
73.6

F-measure (%)

76.9


66.7
62.0

Table 2: Textual entailment recognition results (for two classes) on the RTE-4 corpus.

is significantly lower, compared to the other systems. BASE2 , which uses only string edit distance,
is a competitive baseline for this corpus. Space does not permit listing published evaluation results
of all the paraphrase recognition methods that we have discussed. Furthermore, comparing results
obtained on different datasets is not always meaningful.
For textual entailment recognition, the most widely used benchmarks are those of the RTE challenges. As an example, the RTE-3 corpus contains 1,600 hT, Hi pairs (positive or negative). Four
application scenarios where textual entailment recognition might be useful were considered: information extraction, information retrieval, question answering, and summarization. There are 200
training and 200 testing pairs for each scenario; Dagan et al. (2009) explain how they were constructed. The RTE-4 corpus was constructed in a similar way, but it contains only test pairs, 250 for
each of the four scenarios. A further difference is that in RTE-4 the judges classified the pairs in
three classes: true entailment pairs, false entailment pairs where H contradicts T (Harabagiu, Hickl,
& Lacatusu, 2006; de Marneffe, Rafferty, & Manning, 2008), and false pairs where reading T does
not lead to any conclusion about H; a similar pilot task was included in RTE-3 (Voorhees, 2008).
The pairs of the latter two classes can be merged, if only two classes (true and false) are desirable.
We also note that RTE-3 included a pilot task requiring systems to justify their answers. Many of
the participants, however, used technical or mathematical terminology in their explanations, which
was not always appreciated by the human judges; also, the entailments were often obvious to the
judges, to the extent that no justification was considered necessary (Voorhees, 2008). Table 2 lists
the best accuracy results of RTE-4 participants (for two classes only), along with results of the two
baselines described previously; precision, recall, and F-measure scores are also shown, when available. All four measures are defined as in paraphrase recognition, but positives and negatives are now
textual entailment pairs.25 Again, space does not permit listing published evaluation results of all
the textual entailment recognition methods that we have discussed, and comparing results obtained
on different datasets is not always meaningful.
It is also possible to evaluate recognition methods indirectly, by measuring their impact on the
performance of larger natural language processing systems (Section 1.1). For instance, one could
measure the difference in the performance of a QA system, or the degree to which the redundancy
of a generated summary is reduced when using paraphrase and/or textual entailment recognizers.

25 Average precision, borrowed from information retrieval evaluation, has also been used in the RTE challenges.
Bergmair (2009), however, argues against using it in RTE challenges and proposes alternative measures.

152

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

3. Paraphrase and Textual Entailment Generation
Unlike recognizers, paraphrase or textual entailment generators are given a single language expression (or template) as input, and they are required to produce as many output language expressions
(or templates) as possible, such that the output expressions are paraphrases or they constitute, along
with the input, correct textual entailment pairs. Most generators assume that the input is a single
sentence (or sentence template), and we adopt this assumption in the remainder of this section.
3.1 Generation Methods Inspired by Statistical Machine Translation
Many generation methods borrow ideas from statistical machine translation (SMT).26 Let us first introduce some central ideas from SMT, for the benefit of readers unfamiliar with them. SMT methods
rely on very large bilingual or multilingual parallel corpora, for example the proceedings of the European parliament, without constructing meaning representations and often, at least until recently,
without even constructing syntactic representations.27 Let us assume that we wish to translate a
sentence F, whose words are f1 , f2 , . . . , f|F| in that order, from a foreign language to our native language. Let us also denote by N any candidate translation, whose words are a1 , a2 , . . . , a|N| . The best
translation, denoted N  , is the N with the maximum probability of being a translation of F, i.e:
N  = arg max P(N|F) = arg max
N

N

P(N)P(F|N)
= arg max P(N)P(F|N)
N
P(F)

(44)

Since F is fixed, the denominator P(F) above is constant and can be ignored when searching for
N  . P(N) is called the language model and P(F|N) the translation model.
For modeling purposes, it is common to assume that F was in fact originally written in our native
language and it was transmitted to us via a noisy channel, which introduced various deformations.
The possible deformations may include, for example, replacing a native word with one or more
foreign ones, removing or inserting words, moving words to the left or right etc. The commonly
used IBM models 1 to 5 (Brown et al., 1993) provide an increasingly richer inventory of word
deformations; more recent phrase-based SMT systems (Koehn, Och, & Marcu, 2003) also allow
directly replacing entire phrases with other phrases. The
 foreign sentence
ff F can thus be seen as the
result of applying a sequence of transformations D = d1 , d2 , . . . , d|D| to N, and it is common to
search for the N  that maximizes (45); this search is called decoding.
N  = arg max P(N) max P(F, D|N)
N

D

(45)

An exhaustive search is usually intractable. Hence, heuristic search algorithms (e.g., based on beam
search) are usually employed (Germann, Jahr, Knight, Marcu, & Yamada, 2001; Koehn, 2004).28
Assuming for simplicity that the individual deformations di () of D are mutually independent,
P(F, D|N) can be computed as the product of the probabilities of Ds individual deformations. Given
a bilingual parallel corpus with words aligned across languages, we can estimate the probabilities
26 For

an introduction to SMT, see chapter 25 of the book Speech and Language Processing (Jurafsky & Martin,
2008), and chapter 13 of the book Foundations of Statistical Natural Language Processing (Manning & Schuetze,
1999). For a more extensive discussion, consult the work of Koehn (2009).
27 See Koehns Statistical Machine Translation site for commonly used SMT corpora and tools.
28 A frequently used SMT system that includes decoding facilities is Moses.

153

fiA NDROUTSOPOULOS & M ALAKASIOTIS

of all possible deformations di (). In practice, however, parallel corpora do not indicate word alignment. Hence, it is common to find the most probable word alignment of the corpus given initial
estimates of individual deformation probabilities, then re-estimate the deformation probabilities
given the resulting alignment, and iterate (Brown et al., 1993; Och & Ney, 2003).29
The translation model P(F, D|N) estimates the probability of obtaining F from N via D; we are
interested in Ns with high probabilities of leading to F. We also want, however, N to be grammatical, and we use the language model P(N) to check for grammaticality. P(N) is the probability
of encountering N in our native language; it is estimated from a large monolingual corpus of our
language, typically assuming that the probability of encountering word ai depends only on the preceding n  1 words. For n = 3, P(N) becomes:
P(N) = P(a1 )  P(a2 |a1 )  P(a3 |a1 , a2 )  P(a4 |a2 , a3 )    P(a|N| |a|N|2 , a|N|1 )

(46)

A language model typically also includes smoothening mechanisms, to cope with n-grams that are
very rare or not present in the monolingual corpus, which would lead to P(N) = 0.30
In principle, an SMT system could be used to generate paraphrases, if it could be trained on a
sufficiently large monolingual corpus of parallel texts. Both N and F are now sentences of the same
language, but N has to be different from the given F, and it has to convey the same (or almost the
same) information. The main problem is that there are no readily available monolingual parallel
corpora of the sizes that are used in SMT, to train the language model on them. One possibility
is to use multiple translations of the same source texts; for example, different English translations
of novels originally written in other languages (Barzilay & McKeown, 2001), or multiple English
translations of Chinese news articles, as in the Multiple-Translation Chinese Corpus. Corpora of
this kind, however, are still orders of magnitude smaller than those used in SMT.
To bypass the lack of large monolingual parallel corpora, Quirk et al. (2004) use clusters of
news articles referring to the same event. The articles of each cluster do not always report the same
information and, hence, they are not parallel texts. Since they talk about the same event, however,
they often contain phrases, sentences, or even longer fragments with very similar meanings; corpora
of this kind are often called comparable. From each cluster, Quirk et al. select pairs of similar
sentences (e.g., with small word edit distance, but not identical sentences) using methods like those
employed to create the MSR corpus (Section 2.8).31 The sentence pairs are then word aligned as
in machine translation, and the resulting alignments are used to create a table of phrase pairs as
in phrase-based SMT systems (Koehn et al., 2003). A phrase pair hP1 , P2 i consists of contiguous
words (taken to be a phrase, though not necessarily a syntactic constituent) P1 of one sentence that
are aligned to different contiguous words P2 of another sentence. Quirk et al. provide the following
examples of discovered pairs.

29 GIZA ++

is often used to train IBM models and align words.
chapter 4 of the book Speech and Language Processing (Jurafsky & Martin, 2008) and chapter 6 of the book
Foundations of Statistical Natural Language Processing (Manning & Schuetze, 1999) for an introduction to language
models. SRILM (Stolcke, 2002) is a commonly used tool to create language models.
31 Wubben et al. (2009) discuss similar methods to pair news titles. Barzilay & Elhadad (2003) and Nelken & Shieber
(2006) discuss more general methods to align sentences of monolingual comparable corpora. Sentence alignment methods
for bilingual parallel or comparable corpora are discussed, for example, by Gale and Church (1993), Melamed (1999),
Fung and Cheung (2004), Munteanu and Marcu (2006); see also the work of Wu (2000). Sentence alignment methods
for parallel corpora may perform poorly on comparable corpora (Nelken & Shieber, 2006).
30 See

154

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

P1

P2

injured
Bush administration
margin of error
...

wounded
White House
error margin
...

Phrase pairs that occur frequently in the aligned sentences may be assigned higher probabilities; Quirk et al. use probabilities returned by IBM model 1. Their decoder first constructs a lattice
that represents all the possible paraphrases of the input sentence that can be produced by replacing
phrases by their counterparts in the phrase table; i.e., the possible deformations di () are the phrase
replacements licensed by the phrase table.32 Unlike machine translation, not all of the words or
phrases need to be replaced, which is why Quirk et al. also allow a degenerate identity deformation did ( ) =  ; assigning a high probability to the identity deformation leads to more conservative
paraphrases, with fewer phrase replacements. The decoder uses the probabilities of di () to compute P(F, D|N) in equation (45), and the language model to compute P(N). The best scored N  is
returned as a paraphrase of F; the n most highly scored Ns could also be returned. More generally,
the table of phrase pairs may also include synonyms obtained from WordNet or similar resources,
or pairs of paraphrases (or templates) discovered by paraphrase extraction methods; in effect, Quirk
et al.s construction of a monolingual phrase table is a paraphrase extraction method. A language
model may also be applied locally to the replacement words of a deformation and their context to
assess whether or not the new words fit the original context (Mirkin et al., 2009b).
Zhao et al. (2008, 2009) demonstrated that combining phrase tables derived from multiple resources improves paraphrase generation. They also proposed scoring the candidate paraphrases
by using an additional, application-dependent model, called the usability model; for example, in
sentence compression (Section 1.1) the usability model rewards Ns that have fewer words than F.
Equation (45) then becomes (47), where U(F, N) is the usability model and i are weights assigned
to the three models; similar weights can be used in (45).
N  = arg max U(F, N)1 P(N)2 max P(F, D|N)3
N

D

(47)

Zhao et al. actually use a log-linear formulation of (47); and they select the weights i that maximize an objective function that rewards many and correct (as judged by human evaluators) phrasal
replacements.33 One may replace the translation model by a paraphrase recognizer (Section 2) that
returns a confidence score; in its log-linear formulation, (47) then becomes (48), where R(F, N) is
the confidence score of the recognizer.
N  = arg max[1 logU(F, N) + 2 log P(N) + 3 logR(F, N)]
N

(48)

Including hyponyms-hypernyms or textual entailment rules (Section 2.7) in the phrase table
would generate sentences N that textually entail or are entailed (depending on the direction of the
32 Chevelu

et al. (2009) discuss how other decoders could be developed especially for paraphrase generation.
a reluctant paraphrasing setting (Dras, 1998), for example when revising a document to satisfy length requirements, readability measures, or other externally imposed constraints, it may be desirable to use an objective function that
rewards making as few changes as possible, provided that the constraints are satisfied. Dras (1998) discusses a formulation
of this problem in terms of integer programming.
33 In

155

fiA NDROUTSOPOULOS & M ALAKASIOTIS

rules and whether we replace hyponyms by hypernyms or the reverse) by F. SMT-inspired methods,
however, have been used mostly in paraphrase generation, not in textual entailment generation.
Paraphrases can also be generated by using pairs of machine translation systems to translate the
input expression to a new language, often called a pivot language, and then back to the original
language. The resulting expression is often different from the input one, especially when the two
translation systems employ different methods. By using different pairs of machine translation systems or different pivot languages, multiple paraphrases may be obtained. Duboue and Chu-Carroll
(2006) demonstrated the benefit of using this approach to paraphrase questions, with an additional
machine learning classifier to filter the generated paraphrases; their classifier uses features such as
the cosine similarity between a candidate generated paraphrase and the original question, the lengths
of the candidate paraphrase and the original question, features showing whether or not both questions are of the same type (e.g., both asking for a person name), etc. An advantage of this approach
is that the machine translation systems can be treated as black boxes, and they can be trained on
readily available parallel corpora of different languages. A disadvantage is that translation errors
from both directions may lead to poor paraphrases. We return to pivot languages in Section 4.
In principle, the output of a generator may be produced by mapping the input to a representation
of its meaning, a process that usually presupposes parsing, and by passing on the meaning representation, or new meaning representations that are logically entailed by the original one, to a natural
language generation system (Reiter & Dale, 2000; Bateman & Zock, 2003) to produce paraphrases
or entailed language expressions. This approach would be similar to using language-independent
meaning representations (an interlingua) in machine translation, but here the meaning representations would not need to be language-independent, since only one language is involved. An approach
similar to syntactic transfer in machine translation may also be adopted (McKeown, 1983). In that
case, the input language expression (assumed to be a sentence) is first parsed. The resulting syntactic representation is then modified in ways that preserve, or affect only slightly, the original meaning
(e.g., turning a sentence from active to passive), or in ways that produce syntactic representations
of entailed language expressions (e.g., pruning certain modifiers or subordinate clauses). New language expressions are then generated from the new syntactic representations, possibly by invoking
the surface realization components of a natural language generation system. Parsing, however, the
input expression may introduce errors, and producing a correct meaning representation of the input,
when this is required, may be far from trivial. Furthermore, the natural language generator may be
capable of producing language expressions of only a limited variety, missing possible paraphrases
or entailed language expressions. This is perhaps why meaning representation and syntactic transfer
do not seem to be currently popular in paraphrase and textual entailment generation.
3.2 Generation Methods that Use Bootstrapping
When the input and output expressions are slotted templates, it is possible to apply bootstrapping
to a large monolingual corpus (e.g., the entire Web), instead of using machine translation methods.
Let us assume, for example, that we wish to generate paraphrases of (49), and that we are given a
few pairs of seed values of X and Y , as in (50) and (51).
(49)
(50)
(51)

X is the author of Y .
hX = Jack Kerouac,Y = On the Roadi
hX = Jules Verne,Y = The Mysterious Islandi

We can retrieve from the corpus sentences that contain any of the seed pairs:
156

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

(52)
(53)
(54)

Jack Kerouac wrote On the Road.
The Mysterious Island was written by Jules Verne.
Jack Kerouac is most known for his novel On the Road.

By replacing the known seeds with the corresponding slot names, we obtain new templates:
(55)
(56)
(57)

X wrote Y .
Y was written by X.
X is most known for his novel Y .

In our example, (55) and (56) are paraphrases of (49); however, (57) textually entails (49), but is
not a paraphrase of (49). If we want to generate paraphrases, we must keep (55) and (56) only; if we
want to generate templates that entail (49), we must keep (57) too. Some of the generated candidate
templates may neither be paraphrases of, nor entail (or be entailed by) the original template. A good
paraphrase or textual entailment recognizer (Section 2) or a human in the loop would be able to filter
out bad candidate templates; see also Duclaye et al.s (2003) work, where Expectation Maximization
(Mitchell, 1997) is used to filter the candidate templates. Simpler filtering techniques may also be
used. For example, Ravichandran et al. (2002, 2003) assign to each candidate template a pseudoprecision score; roughly speaking, the score is computed as the number of retrieved sentences that
match the candidate template with X and Y having the values of any seed pair, divided by the
number of retrieved sentences that match the template when X has a seed value and Y any value,
not necessarily the corresponding seed value.
Having obtained new templates, we can search the corpus for new sentences that match them;
for example, sentence (58) matches the generated template (56). From the new sentences, more seed
values can be obtained, if the slot values correspond to types of expressions (e.g., person names) that
can be recognized reasonably well, for example by using a named entity recognizer or a gazetteer
(e.g., a large list of book titles); from (58) we would obtain the new seed pair (59). More iterations
may be used to generate more templates and more seeds, until no more templates and seeds can be
discovered or a maximum number of iterations is reached.
(58)
(59)

Frankenstein was written by Mary Shelley.
hX = Mary Shelley,Y = Frankensteini

Figure 4 illustrates how a bootstrapping paraphrase generator works. Templates that textually entail
or that are textually entailed by an initial template, for which seed slot values are provided, can be
generated similarly, if the paraphrase recognizer is replaced by a textual entailment recognizer.
If slot values can be recognized reliably, we can also obtain the initial seed slot values automatically by retrieving directly sentences that match the original templates and by identifying the
slot values in the retrieved sentences.34 If we are also given a mechanism to identify sentences of
interest in the corpus (e.g., sentences involving particular terms, such as names of known diseases
and medicines), we can also obtain the initial templates automatically, by identifying sentences of
interest, identifying slot values (e.g., named entities of particular categories) in the sentences, and
using the contexts of the slot values as initial templates. In effect, the generation task then becomes
an extraction one, since we are given a corpus, but neither initial templates nor seed slot values.
TEASE (Szpektor, Tanev, Dagan, & Coppola, 2004) is a well-known bootstrapping method of this
34 Seed

slot values per semantic relation can also be obtained from databases (Mintz, Bills, Snow, & Jurafsky, 2009).

157

fiA NDROUTSOPOULOS & M ALAKASIOTIS

kind, which produces textual entailment pairs, for example pairs like (60)(61), given only a monolingual (non-parallel) corpus and a dictionary of terms. (60) textually implies (61), for example in
contexts like those of (62)(63), but not the reverse.35
(60)
(61)
(62)
(63)

X prevents Y
X reduces Y risk
Aspirin prevents heart attack.
Aspirin reduces heart attack risk.

does not specify the directionality of the produced template pairs, for example whether (60)
textually entails (61) or vice versa, but additional mechanisms have been proposed that attempt to
guess the directionality; we discuss one such mechanism, LEDIR (Bhagat, Pantel, & Hovy, 2007), in
Section 4.1 below. Although TEASE can also be used as a generator, if particular input templates are
provided, we discuss it further in Section 4.2, along with other bootstrapping extraction methods,
since in its full form it requires no initial templates (nor seed slot values). The reader is reminded
that the boundaries between recognizers, generators, and extractors are not always clear.
Similar bootstrapping methods have been used to generate information extraction patterns (Riloff
& Jones, 1999; Xu, Uszkoreit, & Li, 2007). Some of these methods, however, require corpora annotated with instances of particular types of events to be extracted (Huffman, 1995; Riloff, 1996b;
Soderland, Fisher, Aseltine, & Lehnert, 1995; Soderland, 1999; Muslea, 1999; Califf & Mooney,
2003), or texts that mention the target events and near-miss texts that do not (Riloff, 1996a).
Marton et al. (2009) used a similar approach, but without iterations, to generate paraphrases of
unknown source language phrases in a phrase-based SMT system (Section 1.1). For each unknown
phrase, they collected contexts where the phrase occurred in a monolingual corpus of the source
language, and they searched for other phrases (candidate paraphrases) in the corpus that occurred
in the same contexts. They subsequently produced feature vectors for both the unknown phrase and
its candidate paraphrases, with each vector showing how often the corresponding phrase cooccurred
with other words. The candidate paraphrases were then ranked by the similarity of their vectors
to the vector of the unknown phrase. The unknown phrases were in effect replaced by their best
paraphrases that the SMT system knew how to map to target language phrases, and this improved
the SMT systems performance.
TEASE

3.3 Evaluating Generation Methods
In most generation applications, for example when rephrasing queries to a QA system (Section 1.1),
it is desirable not only to produce correct outputs (correct paraphrases, or expressions that constitute
correct textual entailment pairs along with the input), but also to produce as many correct outputs as
possible. The two goals correspond to high precision and recall, respectively. For a particular input
si , the precision pi and recall ri of a generator can now be defined as follows (cf. Section 2.8). TPi
is the number of correct outputs for input si , FPi is the number of wrong outputs for si , and FNi is
the number of outputs for si that have incorrectly not been generated (missed).
pi =

TPi
TPi +FPi ,

ri =

TPi
TPi +FNi

The precision and recall scores of a method over a set of inputs {si } can then be defined using
micro-averaging or macro-averaging:
35 Example

from the work of Szpektor et al. (2004).

158

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

<X=Jack Kerouac, Y=On the road>
<X=Jules Verne, Y=The Mysterious Island>

Search engine

<X=Virginia Wolf, Y=Mrs. Dalloway>
<X=Mary Shelley, Y=Frankestein>

Jack Kerouac wrote On the road
The Mysterious Island was written by Jules Verne
Jack Kerouac is most known for his novel On the road

Identify new seeds

Known seeds to slots

X wrote Y
Y was written by X
X is most known for his novel Y

Virginia Wolf wrote Mrs. Dalloway
Frankestein was written by Mary Shelley

X wrote Y
Y was written by X

Search engine

Paraphrase Recognizer

Figure 4: Generating paraphrases of X wrote Y  by bootstrapping.

macro-precision = i pi , macro-recall = i ri
micro-precision =

i TPi
,
i (TPi +FPi )

micro-recall =

i TPi
i (TPi +FNi )

In any case, however, recall cannot be computed in generation, because FNi is unknown; there are
numerous correct paraphrases of an input si that may have been missed, and there are even more (if
not infinite) language expressions that entail or are entailed by si .36
Instead of reporting recall, it is common to report (along with precision) the average number
of outputs, sometimes called yield, defined below, where we assume that there are n test inputs.
A better option is to report the yield at different precision levels, since there is usually a tradeoff
between the two figures, which is controlled by parameter tuning (e.g., selecting different values of
thresholds involved in the methods).
yield =

1 n
 (TPi + FPi )
n i=1

Note that if we use a fixed set of test inputs {si }, if we store the sets Oref
i of all the correct
outputs that a reference generation method produces for each si , and if we treat each Oref
i as the set
of all possible correct outputs that may be generated for si , then both precision and recall can be
computed, and without further human effort when a new generation method, say M, is evaluated.
FNi is then the number of outputs in Oref
i that have not been produced for si by M; FPi is the number
of Ms outputs for si that are not in Oref
i ; and TPi is the number of Ms outputs for si that are included
ref
in Oi . Callison-Burch et al. (2008) propose an evaluation approach of this kind for what we call
paraphrase generation. They use phrase alignment heuristics (Och & Ney, 2003; Cohn et al., 2008)
36 Accuracy (Section 2.8) is also impossible to compute in this case; apart from not knowing FN , the number of
i
outputs that have correctly not been generated (TNi ) is infinite.

159

fiA NDROUTSOPOULOS & M ALAKASIOTIS

to obtain aligned phrases (e.g., resign, tender his resignation, leave office voluntarily) from
manually word-aligned sentences with the same meanings (from the Multiple-Translation Chinese
Corpus). Roughly speaking, they use as {si } phrases for which alignments have been found; and
ref
for each si , Oref
i contains the phrases si was aligned to. Since Oi , however, contains much fewer
phrases than the possible correct paraphrases of si , the resulting precision score is a (possibly very
pessimistic) lower bound, and the resulting recall scores only measure to what extent M managed
to discover the (relatively few) paraphrases in Oref
i , as pointed out by Callison-Burch et al.
To the best of our knowledge, there are no widely adopted benchmark datasets for paraphrase
and textual entailment generation, unlike recognition, and comparing results obtained on different
datasets is not always meaningful. The lack of generation benchmarks is probably due to the fact
that although it is possible to assemble a large collection of input language expressions, it is practically impossible to specify in advance all the numerous (if not infinite) correct outputs a generator
may produce, as already discussed. In principle, one could use a paraphrase or textual entailment
recognizer to automatically judge if the output of a generator is a paraphrase of, or forms a correct
entailment pair with the corresponding input expression. Current recognizers, however, are not yet
accurate enough, and automatic evaluation measures from machine translation (e.g., BLEU, Section
2.3) cannot be employed, exactly because their weakness is that they cannot detect paraphrases and
textual entailment. An alternative, more costly solution is to use human judges, which also allows
evaluating other aspects of the outputs, such as their fluency (Zhao et al., 2009), as in machine translation. One can also evaluate the performance of a generator indirectly, by measuring its impact on
the performance of larger natural language processing systems (Section 1.1).

4. Paraphrase and Textual Entailment Extraction
Unlike recognition and generation methods, extraction methods are not given particular input language expressions. They typically process large corpora to extract pairs of language expressions
(or templates) that constitute paraphrases or textual entailment pairs. The generated pairs are stored
to be used subsequently by recognizers and generators or other applications (e.g., as additional entries of phrase tables in SMT systems). Most extraction methods produce pairs of sentences (or
sentence templates) or pairs of shorter expressions. Methods to discover synonyms, hypernymhyponym pairs or, more generally, entailment relations between words (Lin, 1998a; Hearst, 1998;
Moore, 2001; Glickman & Dagan, 2004; Brockett & Dolan, 2005; Hashimoto, Torisawa, Kuroda,
De Saeger, Murata, & Kazama, 2009; Herbelot, 2009) can be seen as performing paraphrase or
textual entailment extraction restricted to pairs of single words.
4.1 Extraction Methods Based on the Distributional Hypothesis
A possible paraphrase extraction approach is to store all the word n-grams that occur in a large
monolingual corpus (e.g., for n  5), along with their left and right contexts, and consider as paraphrases n-grams that occur frequently in similar contexts. For example, each n-gram can be represented by a vector showing the words that typically precede or follow the n-gram, with the values
in the vector indicating how strongly each word co-occurs with the n-gram; for example, pointwise
mutual information values (Manning & Schuetze, 1999) may be used. Vector similarity measures,
for example cosine similarity or Lins measure (1998a), can then be employed to identify n-grams
160

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

that occur in similar contexts by comparing their vectors.37 This approach has been shown to be
viable with very large monolingual corpora; Pasca and Dienes (2005) used a Web snapshot of approximately a billion Web pages; Bhagat and Ravichandran (2008) used 150 GB of news articles
and reported that results deteriorate rapidly with smaller corpora. Even if only lightweight linguistic
processing (e.g., POS tagging, without parsing) is performed, processing such large datasets requires
very significant processing power, although linear computational complexity is possible with appropriate hashing of the context vectors (Bhagat & Ravichandran, 2008). Paraphrasing approaches of
this kind are based on Harriss Distributional Hypothesis (1964), which states that words in similar
contexts tend to have similar meanings. The bootstrapping methods of Section 3.2 are based on
a similar hypothesis that phrases (or templates) occurring in similar contexts (or with similar slot
values) tend to have similar meanings, a hypothesis that can be seen as an extension of Harriss.
Lin and Pantels (2001) well-known extraction method, called DIRT, is also based on the extended Distributional Hypothesis, but it operates at the syntax level. DIRT first applies a dependency
grammar parser to a monolingual corpus. Parsing the corpus is generally time-consuming and,
hence, smaller corpora have to be used, compared to methods that do not require parsing; Lin and
Panel used 1 GB of news texts in their experiments. Dependency paths are then extracted from the
dependency trees of the corpus. Let us consider, for example, sentences (64) and (67). Their dependency trees are shown in Figure 5; the similarity between the two sentences is less obvious than
in Figure 1, because of the different verbs that are now involved. Two of the dependency paths that
can be extracted from the trees of Figure 5 are shown in (65) and (68). The labels of the edges are
augmented by the POS-tags of the words they connect (e.g., N:subj:V instead of simply subj).38
The first and last words of the extracted paths are replaced by slots, shown as boxed and numbered
POS -tags. Roughly speaking, the paths of (65) and (68) correspond to the surface templates of (66)
and (69), respectively, but the paths are actually templates specified at the syntactic level.
(64)
(65)
(66)
(67)
(68)
(69)

A mathematician found a solution to the problem.
N1 :subj:V  found  V :obj:N  solution  N:to: N2
N1 found [a] solution to N2
The problem was solved by a young mathematician.
N3 :obj:V  solved  V :by: N4
N3 was solved by N4 .

DIRT imposes restrictions on the paths that can be extracted from the dependency trees; for
example, they have to start and end with noun slots. Once the paths have been extracted, it looks for
pairs of paths that occur frequently with the same slot fillers. If (65) and (68) occur frequently with
the same fillers (e.g., N1 = N4 = mathematician, N2 = N3 = problem), they will be included
as a pair in DIRTs output (with N1 = N4 and N2 = N3). A measure based on mutual information
(Manning & Schuetze, 1999; Lin & Pantel, 2001) is used to detect paths with common fillers.
Lin and Pantel call the pairs of templates that DIRT produces inference rules, but there is
no directionality between the templates of each pair; the intention seems to be to produce pairs of
near paraphrases. The resulting pairs are actually often textual entailment pairs, not paraphrases,
37 Zhitomirsky-Geffet

and Dagan (2009) discuss a bootstrapping approach, whereby the vector similarity scores (initially computed using pointwise mutual information values in the vectors) are used to improve the values in the vectors;
the vector similarity scores are then re-computed.
38 For consistency with previous examples, we show slightly different labels than those used by Lin and Pantel.

161

fiA NDROUTSOPOULOS & M ALAKASIOTIS

found
subj

solved
obj

mathematician

solution

det
A

obj aux

det
a

problem

to

by

was

mathematician

det

problem

The

det
a

mod
young

det
the

Figure 5: Dependency trees of sentences (64) and (67).
and the directionality of the entailment is unspecified.39 Bhagat et al. (2007) developed a method,
called LEDIR, to classify the template pairs hP1 , P2 i that DIRT and similar methods produce into three
classes: (i) paraphrases, (ii) P1 textually entails P2 and not the reverse, or (iii) P2 textually entails
P1 and not the reverse; with the addition of LEDIR, DIRT becomes a method that extracts separately
pairs of paraphrase templates and pairs of directional textual entailment templates. Roughly speaking, LEDIR examines the semantic categories (e.g., person, location etc.) of the words that fill P1 and
P2 s slots in the corpus; the categories can be obtained by following WordNets hypernym-hyponym
hierarchies from the filler words up to a certain level, or by applying clustering to the words of
the corpus and using the clusters of the filler words as their categories.40 If P1 occurs with fillers
from a substantially larger number of categories than P2 , then LEDIR assumes P1 has a more general
meaning than P2 and, hence, P2 textually entails P1 ; similarly for the reverse direction. If there is no
substantial difference in the number of categories, P1 and P2 are taken to be paraphrases. Szpektor
and Dagan (2008) describe a method similar to DIRT that produces textual entailment pairs of unary
(single slot) templates (e.g., X takes a nap  X sleeps) using a directional similarity measure
for unary templates.
Extraction methods based on the (extended) Distributional Hypothesis often produce pairs of
templates that are not correct paraphrasing or textual entailment pairs, although they share many
common fillers. In fact, pairs involving antonyms are frequent; according to Lin and Pantel (2001),
DIRT finds X solves Y  to be very similar to X worsens Y ; and the same problem has been
reported in experiments with LEDIR (Bhagat et al., 2007) and distributional approaches that operate
at the surface level (Bhagat & Ravichandran, 2008).
Ibrahim et al.s (2003) method is similar to DIRT, but it assumes that a monolingual parallel
corpus is available (e.g., multiple English translations of novels), whereas DIRT does not require
parallel corpora. Ibrahim et al.s method extracts pairs of dependency paths only from aligned
sentences that share matching anchors. Anchors are allowed to be only nouns or pronouns, and they
match if they are identical, if they are a noun and a compatible pronoun, if they are of the same
semantic category etc. In (70)(71), square brackets and subscripts indicate matching anchors.41
The pair of templates of (72)(73) would be extracted from (70)(71); for simplicity, we show
sentences and templates as surface strings, although the method operates on dependency trees and
39 Template

pairs produced by DIRT are available on-line.
an introduction to clustering methods, consult chapter 14 of Foundations of Statistical Natural Language Processing (Manning & Schuetze, 1999).
41 Simplified example from Ibrahim et al.s work (2003).
40 For

162

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

paths. Matching anchors become matched slots. Heuristic functions are used to score the anchor
matches (e.g., identical anchors are preferred to matching nouns and pronouns) and the resulting
template pairs; roughly speaking frequently rediscovered template pairs are rewarded, especially
when they occur with many different anchors.
(70)
(71)
(72)
(73)

The [clerk]1 liked [Bovary]2 .
[He]1 was fond of [Bovary]2 .
X liked Y .
X was fond of Y .

By operating on aligned sentences of monolingual parallel corpora, Ibrahim et al.s method may
avoid, to some extent, producing pairs of unrelated templates that simply happen to share common
slot fillers; the resulting pairs of templates are also more likely to be paraphrases, rather than simply
textual entailment pairs, since they are obtained from aligned sentences of a monolingual parallel
corpus. Large monolingual parallel corpora, however, are more difficult to obtain than non-parallel
corpora, as already discussed. An alternative is to identify anchors in related sentences from comparable corpora (Section 3.1), which are easier to obtain. Shinyama and Sekine (2003) find pairs
of sentences that share the same anchors within clusters of news articles reporting the same event.
In their method, anchors are named entities (e.g., person names) identified using a named entity
recognizer, or pronouns and noun phrases that refer to named entities; heuristics are employed to
identify likely referents. Dependency trees are then constructed from each pair of sentences, and
pairs of dependency paths are extracted from the trees by treating anchors as slots.
4.2 Extraction Methods that Use Bootstrapping
Bootstrapping approaches can also be used in extraction, as in generation (Section 3.2), but with the
additional complication that there is no particular input template nor seed values of its slots to start
from. To address this complication, TEASE (Szpektor et al., 2004) starts with a lexicon of terms of a
knowledge domain, for example names of diseases, symptoms etc. in the case of a medical domain;
to some extent, such lexicons can be constructed automatically from a domain-specific corpus (e.g.,
medical articles) via term acquisition techniques (Jacquemin & Bourigault, 2003). TEASE then
extracts from a (non-parallel) monolingual corpus pairs of textual entailment templates that can be
used with the lexicons terms as slot fillers. We have already shown a resulting pair of templates,
(60)(61), in Section 3.2; we repeat it as (74)(75) below. Recall that TEASE does not indicate the
directionality of the resulting template pairs, for example whether (74) textually entails (75) or vice
versa, but mechanisms like LEDIR (Section 4.1) could be used to guess the directionality.
(74)
(75)

X prevents Y
X reduces Y risk

Roughly speaking, TEASE first identifies noun phrases that cooccur frequently with each term of
the lexicon, excluding very common noun phrases. It then uses the terms and their cooccurring
noun phrases as seed slot values to obtain templates, and then the new templates to obtain more
slot values, much as in Figure 4. In TEASE, however, the templates are actually slotted dependency
paths, and the method includes a stage that merges compatible templates to form more general
ones.42 If particular input templates are provided, TEASE can be used as a generator (Section 3.2).
42 Template

pairs produced by TEASE are available on-line.

163

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Barzilay and McKeown (2001) also used a bootstrapping method, but to extract paraphrases
from a parallel monolingual corpus; they used multiple English translations of novels. Unlike
previously discussed bootstrapping approaches, their method involves two classifiers (in effect, two
sets of rules). One classifier examines the words the candidate paraphrases consist of, and a second
one examines their contexts. The two classifiers use different feature sets (different views of the
data), and the output of each classifier is used to improve the performance of the other one in
an iterative manner; this is a case of co-training (Blum & Mitchell, 1998). More specifically, a
POS tagger, a shallow parser, and a stemmer are first applied to the corpus, and the sentences are
aligned across the different translations. Words that occur in both sentences of an aligned pair
are treated as seed positive lexical examples; all the other pairs of words from the two sentences
become seed negative lexical examples. From the aligned sentences (76)(77), we obtain three seed
positive lexical examples, shown in (78)(80), and many more seed negative lexical examples, two
of which are shown in (81)(82).43 Although seed positive lexical examples are pairs of identical
words, as the algorithm iterates new positive lexical examples are produced, and some of them may
be synonyms (e.g., comfort and console) or pairs of longer paraphrases, as will be explained
below.
(76)
(77)
(78)
(79)
(80)
(81)
(82)

He tried to comfort her.
He tried to console Mary.
hexpression1 = he, expression2 = he, +i
hexpression1 = tried, expression2 = tried, +i
hexpression1 = to, expression2 = to, +i
hexpression1 = he, expression2 = tried, i
hexpression1 = he, expression2 = to, i

The contexts of the positive (similarly, negative) lexical examples in the corresponding sentences are then used to construct positive (or negative) context rules, i.e., rules that can be used to
obtain new pairs of positive (or negative) lexical examples. Barzilay and McKeown (2001) use the
POS tags of the l words before and after the lexical examples as contexts, and in their experiments
set l = 3. For simplicity, however, let us assume that l = 2; then, for instance, from (76)(77) and
the positive lexical example of (79), we obtain the positive context rule of (83). The rule says that
if two aligned sentences contain two sequences of words, say 1 and 2 , one from each sentence,
and both 1 and 2 are preceded by the same pronoun, and both are followed by to and a (possibly different) verb, then 1 and 2 are positive lexical examples. Identical subscripts in the POS
tags denote identical words; for example, (83) requires both 1 and 2 to be preceded by the same
pronoun, but the verbs that follow them may be different.
(83)

hleft1 = (pronoun1 ), right1 = (to1 , verb), left2 = (pronoun1 ), right1 = (to1 , verb), +i

In each iteration, only the k strongest positive and negative context rules are retained. The
strength of each context rule is its precision, i.e., for positive context rules, the number of positive
lexical examples whose contexts are matched by the rule divided by the number of both positive and
negative lexical examples matched, and similarly for negative context rules. Barzilay and McKeown
(2001) used k = 10, and they also discarded context rules whose strength was below 95%. The
resulting (positive and negative) context rules are then used to identify new (positive and negative)
43 Simplified

example from the work of Barzilay and McKeown (2001).

164

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Slot 1

planes

bombers

start

Baghdad
bombed

forces

Iraqi

Slot 2

Baghdad
start

Iraqi
Iraqi
Slot 3

planes

capital
military

end

capital

bombed

was

base

by

army
Slot 4

end

forces

Figure 6: Word lattices obtained from sentence clusters in Barzilay and Lees method.
lexical examples. From the aligned (84)(85), the rule of (83) would figure out that tried is a
synonym of attempted; the two words would be treated as a new positive lexical example, shown
in (86).
(84)
(85)
(86)

She tried to run away.
She attempted to escape.
hexpression1 = tried, expression2 = attempted, +i

The context rules may also produce multi-word lexical examples, like the one shown in (87).
The obtained lexical examples are generalized by replacing their words by their POS tags, giving
rise to paraphrasing rules. From (87) we obtain the positive paraphrasing rule of (88); again, POS
subscripts denote identical words, whereas superscripts denote identical stems. The rule of (88)
says that any sequence of words consisting of a verb, to, and another verb is a paraphrase of any
other sequence consisting of the same initial verb, to, and another verb of the same stem as the
second verb of the first sequence, provided that the two sequences occur in aligned sentences.
(87)
(88)

hexpression
1 = start to talk, expression2 = start talking, +i


ff
generalized expression1 = (verb0 , to, verb1 ), generalized expression2 = (verb0 , verb1 ), +

The paraphrasing rules are also filtered by their strength, which is the precision with which they
predict paraphrasing contexts. The remaining paraphrasing rules are used to obtain more lexical
examples, which are also filtered by the precision with which they predict paraphrasing contexts.
The new positive and negative lexical examples are then added to the existing ones, and they are
used to obtain, score, and filter new positive and negative context rules, as well as to rescore and
filter the existing ones. The resulting context rules are then employed to obtain more lexical examples, more paraphrasing rules, and so on, until no new positive lexical examples can be obtained
from the corpus, or a maximum number of iterations is exceeded. Wang et al. (2009) added more
scoring measures to Barzilay and McKeowns (2001) method to filter and rank the paraphrase pairs
it produces, and used the extended method to extract paraphrases of technical terms from clusters
of bug reports.
4.3 Extraction Methods Based on Alignment
Barzilay and Lee (2003) used two corpora of the same genre, but from different sources (news
articles from two press agencies). They call the two corpora comparable, but they use the term with
165

fiA NDROUTSOPOULOS & M ALAKASIOTIS

a slightly different meaning than in previously discussed methods; the sentences of each corpus
were clustered separately, and each cluster was intended to contain sentences (from a single corpus)
referring to events of the same type (e.g., bomb attacks), not sentences (or documents) referring to
the same events (e.g., the same particular bombing). From each cluster, a word lattice was produced
by aligning the clusters sentences with Multiple Sequence Alignment (Durbin, Eddy, Krogh, &
Mitchison, 1998; Barzilay & Lee, 2002). The solid lines of Figure 6 illustrate two possible resulting
lattices, from two different clusters; we omit stop-words. Each sentence of a cluster corresponds to
a path in the clusters lattice. In each lattice, nodes that are shared by a high percentage (50% in
Barzilay and Lees experiments) of the clusters sentences are considered backbone nodes. Parts of
the lattice that connect otherwise consecutive backbone nodes are replaced by slots, as illustrated in
Figure 6. The two lattices of our example correspond to the surface templates (89)(90).
(89)
(90)

X bombed Y .
Y was bombed by X.

The encountered fillers of each slot are also recorded. If two slotted lattices (templates) from different corpora share many fillers, they are taken to be a pair of paraphrases (Figure 6). Hence, this
method also uses the extended Distributional Hypothesis (Section 4.1).
Pang et al.s method (2003) produces finite state automata very similar to Barzilay and Lees
(2003) lattices, but it requires a parallel monolingual corpus; Pang et al. used the Multiple-Translation
Chinese Corpus (Section 3.1) in their experiments. The parse trees of aligned sentences are constructed and then merged as illustrated in Figure 7; vertical lines inside the nodes indicate sequences
of necessary constituents, whereas horizontal lines correspond to disjunctions.44 In the example of
Figure 7, both sentences consist of a noun phrase (NP) followed by a verb phrase (VP); this is reflected to the root node of the merged tree. In both sentences, the noun phrase is a cardinal number
(CD) followed by a noun (NN); however, the particular cardinal numbers and nouns are different
across the two sentences, leading to leaf nodes with disjunctions. The rest of the merged tree is
constructed similarly; consult Pang at al. for further details. Presumably one could also generalize
over cardinal numbers, types of named entities etc.
Each merged tree is then converted to a finite state automaton by traversing the tree in a depthfirst manner and introducing a ramification when a node with a disjunction is encountered. Figure 8
shows the automaton that corresponds to the merged tree of Figure 7. All the language expressions
that can be produced by the automaton (all the paths from the start to the end node) are paraphrases.
Hence, unlike other extraction methods, Pang et al.s (2003) method produces automata, rather than
pairs of templates, but the automata can be used in a similar manner. In recognition, for example, if
two strings are accepted by the same automaton, they are paraphrases; and in generation, we could
look for an automaton that accepts the input expression, and then output other expressions that can
be generated by the same automaton. As with Barzilay and Lees (2003) method, however, Pang et
al.s (2003) method is intended to extract mostly paraphrase, not simply textual entailment pairs.
Bannard and Callison-Burch (2005) point out that bilingual parallel corpora are much easier to
obtain, and in much larger sizes, than the monolingual parallel or comparable corpora that some
extraction methods employ. Hence, they set out to extract paraphrases from bilingual parallel corpora commonly used in statistical machine translation (SMT). As already discussed in Section 3.1,
phrase-based SMT systems employ tables whose entries show how phrases of one language may be
44 Example

from Pang et al.s work (2003).

166

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Parse trees of
aligned sentences
S

S

CD
twelve

NP

VP

NN
people

VB
died

NP
CD
12

VP

NN
persons

AUX
were

VB
killed

merge
trees
NP VP
AUX

CD NN

12

persons

twelve

people

VB

VB
killed

were
died

Figure 7: Merging parse trees of aligned sentences in Pang et al.s method.
12

died

persons

start

end
people

twelve

were

killed

Figure 8: Finite state automaton produced by Pang et al.s method.
replaced by phrases of another language; phrase tables of this kind may be produced by applying
phrase alignment heuristics (Och & Ney, 2003; Cohn et al., 2008) to word alignments produced
by the commonly used IBM models. In the case of an English-German parallel corpus, a phrase
table may contain entries like the following, which show that under control has been aligned with
unter kontrolle in the corpus, but unter kontrolle has also been aligned with in check; hence,
under control and in check are a candidate paraphrase pair.45
English phrase

German phrase

...
under control
...
in check
...

...
unter kontrolle
...
unter kontrolle
...

More precisely, to paraphrase English phrases, Bannard and Callison-Burch (2005) employ a
pivot language (German, in the example above) and a bilingual parallel corpus for English and the
pivot language. They construct a phrase table from the parallel corpus, and from the table they
estimate the probabilities P(e| f ) and P( f |e), where e and f range over all of the English and pivot
language phrases of the table. For example, P(e| f ) may be estimated as the number of entries (rows)
that contain both e and f , divided by the number of entries that contain f , if there are multiple rows
for multiple alignments of e and f in the corpus, and similarly for P( f |e). The best paraphrase e2 of
each English phrase e1 in the table is then computed by equation (91), where f ranges over all the
pivot language phrases of the phrase table T .
e2 = arg max P(e2 |e1 ) = arg max
e2 6=e1

45 Example

 P( f |e1 )P(e2 | f , e1 )  arg emax
 P( f |e1 )P(e2 | f )
2 6=e1

e2 6=e1 f T

f T

from the work of Bannard and Callison-Burch (2005).

167

(91)

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Multiple bilingual corpora, for different pivot languages, can be used; (91) becomes (92), where C
ranges over the corpora, and f now ranges over the pivot language phrases of Cs phrase table.
e2 = arg max 



e2 6=e1 C
f T (C)

P( f |e1 )P(e2 | f )

(92)

Bannard and Callison-Burch (2005) also considered adding a language model (Section 3.1) to
their method to favour paraphrase pairs that can be used interchangeably in sentences; roughly
speaking, the language model assesses how well one element of a pair can replace the other in sentences where the latter occurs, by scoring the grammaticality of the sentences after the replacement.
In subsequent work, Callison-Burch (2008) extended their method to require paraphrases to have the
same syntactic types, since replacing a phrase with one of a different syntactic type generally leads
to an ungrammatical sentence.46 Zhou et al. (2006b) employed a method very similar to Bannard
and Callison-Burchs to extract paraphrase pairs from a corpus, and used the resulting pairs in SMT
evaluation, when comparing machine-generated translations against human-authored ones. Riezler
et al. (2007) adopted a similar pivot approach to obtain paraphrase pairs from bilingual phrase tables, and used the resulting pairs as paraphrasing rules to obtain paraphrases of (longer) questions
submitted to a QA system; they also used a log-linear model (Section 3.1) to rank the resulting
question paraphrases by combining the probabilities of the invoked paraphrasing rules, a language
model score of the resulting question paraphrase, and other features.47
The pivot language approaches discussed above have been shown to produce millions of paraphrase pairs from large bilingual parallel corpora. The paraphrases, however, are typically short
(e.g., up to four or five words), since longer phrases are rare in phrase tables. The methods can also
be significantly affected by errors in automatic word and phrase alignment (Bannard & CallisonBurch, 2005). To take into consideration word alignment errors, Zhao et al. (2008) use a log-linear
classifier to score candidate paraphrase pairs that share a common pivot phrase, instead of using
equations (91) and (92). In effect, the classifier uses the probabilities P( f |e1 ) and P(e2 | f ) of (91)
(92) as features, but it also uses additional features that assess the quality of the word alignment
between e1 and f , as well as between f and e2 . In subsequent work, Zhao et al. (2009) also consider
the English phrases e1 and e2 to be paraphrases, when they are aligned to different pivot phrases
f1 and f2 , provided that f1 and f2 are themselves a paraphrase pair in the pivot language. Figure 9
illustrates the original and extended pivot approaches of Zhao et al. The paraphrase pairs h f1 , f2 i
of the pivot language are extracted and scored from a bilingual parallel corpus as in the original approach, by reversing the roles of the two languages. The scores of the h f1 , f2 i pairs, which roughly
speaking correspond to P( f2 | f1 ), are included as additional features in the classifier that scores the
resulting English paraphrases, along with scores corresponding to P( f1 |e1 ), P(e2 | f2 ), and features
that assess the word alignments of the phrases involved.
Zhao et al.s (2008, 2009) method also extends Bannard and Callison-Burchs (2005) by producing pairs of slotted templates, whose slots can be filled in by words of particular parts of speech
(e.g., Noun1 is considered by Noun2   Noun2 considers Noun1 ).48 Hence, Zhao et al.s patterns
are more general, but a reliable parser of the language we paraphrase in is required; let us assume
again that we paraphrase in English. Roughly speaking, the slots are formed by removing subtrees
46 An

implementation of Callison-Burchs (2008) method and paraphrase rules it produced are available on-line.
et al. (2007) also employ a paraphrasing method based on an SMT system trained on question-answer pairs.
48 A collection of template pairs produced by Zhao et al.s method is available on-line.

47 Riezler

168

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

e1

paraphrase

e2

e1

paraphrase

f1

align

align

f

e2

paraphrase

f2

Figure 9: Illustration of Zhao et al.s pivot approaches to paraphrase extraction.
from the dependency trees of the English sentences and replacing the removed subtrees by the POS
tags of their roots; words of the pivot language sentences that are aligned to removed words of the
corresponding English sentences are also replaced by slots. A language model is also used, when
paraphrases are replaced in longer sentences. Zhao et al.s experiments show that their method outperforms DIRT, and that it is able to output as many paraphrase pairs as the method of Bannard and
Callison-Burch, but with better precision, i.e., fewer wrongly produced pairs. Most of the generated
paraphrases (93%), however, contain only one slot, and the method is still very sensitive to word
alignment errors (Zhao et al., 2009), although the features that check the word alignment quality
alleviate the problem.
Madnani et al. (2007) used a pivot approach similar to Bannard and Callison-Burchs (2005)
to obtain synchronous (normally bilingual) English-to-English context-free grammar rules from
bilingual parallel corpora. Parsing an English text with the English-to-English synchronous rules
automatically paraphrases it; hence the resulting synchronous rules can be used in paraphrase generation (Section 3). The rules have associated probabilities, which are estimated from the bilingual
corpora. A log-linear combination of the probabilities and other features of the invoked rules is
used to guide parsing. Madnani et al. employed the English-to-English rules to parse and, thus,
paraphrase human-authored English reference translations of Chinese texts. They showed that using the additional automatically generated reference translations when tuning a Chinese-to-English
SMT system improves its performance, compared to using only the human-authored references.
We note that the alignment-based methods of this section appear to have been used to extract
only paraphrase pairs, not (unidirectional) textual entailment pairs.
4.4 Evaluating Extraction Methods
When evaluating extraction methods, we would ideally measure both their precision (what percentage of the extracted pairs are correct paraphrase or textual entailment pairs) and their recall (what
percentage of all the correct pairs that could have been extracted have actually been extracted). As
in generation, however, recall cannot be computed, because the number of all correct pairs that
could have been extracted from a large corpus (by an ideal method) is unknown. Instead, one may
again count the number of extracted pairs (the total yield of the method), possibly at different precision levels. Different extraction methods, however, produce pairs of different kinds (e.g., surface
strings, slotted surface templates, or slotted dependency paths) from different kinds of corpora (e.g.,
monolingual or multilingual parallel or comparable corpora); hence, direct comparisons of extraction methods may be impossible. Furthermore, different scores are obtained, depending on whether
the extracted pairs are considered in particular contexts or not, and whether they are required to be
interchangeable in grammatical sentences (Bannard & Callison-Burch, 2005; Barzilay & Lee, 2003;
169

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Callison-Burch, 2008; Zhao et al., 2008). The output of an extraction method may also include pairs
with relatively minor variations (e.g., active vs. passive, verbs vs. nominalizations, or variants such
as the X company bought Y vs. X bought Y ), which may cause methods that produce large numbers of minor variants to appear better than they really are; these points also apply to the evaluation
of generation methods (Section 3.3), though they have been discussed mostly in the extraction literature. Detecting and grouping such variants (e.g., turning all passives and nominalizations to active
forms) may help avoid this bias and may also improve the quality of the extracted pairs by making
the occurrences of the (grouped) expressions less sparse (Szpektor & Dagan, 2007).
As in generation, in principle one could use a paraphrase or textual entailment recognizer to
automatically score the extracted pairs. However, recognizers are not yet accurate enough; hence,
human judges are usually employed. When extracting slotted textual entailment rules (e.g., X
painted Y  textually entails Y is the work of X), Szpektor et al. (2007) report that human judges
find it easier to agree whether or not particular instantiations of the rules (in particular contexts)
are correct or incorrect, as opposed to asking them to assess directly the correctness of the rules.
A better evaluation strategy, then, is to show the judges multiple sentences that match the lefthand side of each rule, along with the corresponding transformed sentences that are produced by
applying the rule, and measure the percentage of these sentence pairs the judges consider correct
textual entailment pairs; this measure can be thought of as the precision of each individual rule.
Rules whose precision exceeds a (high) threshold can be considered correct (Szpektor et al., 2007).
Again, one may also evaluate extraction methods indirectly, for example by measuring how
much the extracted pairs help in information extraction (Bhagat & Ravichandran, 2008; Szpektor
& Dagan, 2007, 2008) or when expanding queries (Pasca & Dienes, 2005), by measuring how
well the extracted pairs, seen as paraphrasing rules, perform in phrase alignment in monolingual
parallel corpora (Callison-Burch et al., 2008), or by measuring to what extent SMT or summarization
evaluation measures can be improved by taking into consideration the extracted pairs (CallisonBurch et al., 2006a; Kauchak & Barzilay, 2006; Zhou et al., 2006b).

5. Conclusions
Paraphrasing and textual entailment is currently a popular research topic. Paraphrasing can be seen
as bidirectional textual entailment and, hence, similar methods are often used for both. Although
both kinds of methods can be described in terms of logical entailment, they are usually intended to
capture human intuitions that may not be as strict as logical entailment; and although logic-based
methods have been developed, most methods operate at the surface, syntactic, or shallow semantic
level, with dependency trees being a particularly popular representation.
Recognition methods, which classify input pairs of natural language expressions (or templates)
as correct or incorrect paraphrases or textual entailment pairs, often rely on supervised machine
learning to combine similarity measures possibly operating at different representation levels (surface, syntactic, semantic). More recently, approaches that search for sequences of transformations
that connect the two input expressions are also gaining popularity, and they exploit paraphrasing
or textual entailment rules extracted from large corpora. The RTE challenges provide a significant
thrust to recognition work, and they have helped establish benchmarks and attract more researchers.
Generation methods, meaning methods that generate paraphrases of an input natural language
expression (or template), or expressions that entail or are entailed by the input expression, are currently based mostly on bootstrapping or ideas from statistical machine translation. There are fewer
170

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Main ideas discussed
Logic-based inferencing
Vector space semantic models
Surface string similarity measures
Syntactic similarity measures
Similarity measures on symbolic meaning representations
Machine learning algorithms
Decoding (transformation sequences)
Word/sentence alignment
Pivot language(s)
Bootstrapping
Distributional hypothesis
Synchronous grammar rules

R-TE
X
X
X
X
X
X
X

R-P
X
X
X
X
X
X
X

G-TE

X
X

G-P

X
X
X
X
X
X
X

E-TE

E-P

X

X
X

X
X
X
X
X

Table 3: Main ideas discussed and tasks they have mostly been used in. R: recognition; G: generation, E: extraction; TE: textual entailment, P: paraphrasing.

publications on generation, compared to recognition (and extraction), and most of them focus on
paraphrasing; furthermore, there are no established challenges or benchmarks, unlike recognition.
Nevertheless, generation may provide opportunities for novel research, especially to researchers
with experience in statistical machine translation, who may for example wish to develop alignment
or decoding techniques especially for paraphrasing or textual entailment generation.
Extraction methods extract paraphrases or textual entailment pairs (also called rules) from
corpora, usually off-line. They can be used to construct resources (e.g., phrase tables or collections of rules) that can be exploited by recognition or generation methods, or in other tasks (e.g.,
statistical machine translation, information extraction). Many extraction methods are based on the
Distributional Hypothesis, though they often operate at different representation levels. Alignment
techniques originating from statistical machine translation are recently also popular and they allow
existing large bilingual parallel corpora to be exploited. Extraction methods also differ depending
on whether they require parallel, comparable, or simply large corpora, monolingual or bilingual. As
in generation, most extraction research has focused on paraphrasing, and there are no established
challenges or benchmarks.
Table 3 summarizes the main ideas we have discussed per task, and Table 4 lists the corresponding main resources that are typically required. The underlying ideas of generation and extraction
methods are in effect the same, as shown in Table 3, even if the methods perform different tasks;
recognition work has relied on rather different ideas. Generation and extraction have mostly focused
on paraphrasing, as already noted, which is why fewer ideas have been explored in generation and
extraction for (unidirectional) textual entailment.
We expect to see more interplay among recognition, generation, and extraction methods in the
near future. For example, recognizers and generators may use extracted rules to a larger extent;
recognizers may be used to filter candidate paraphrases or textual entailment pairs in extraction
or generation approaches; and generators may help produce more monolingual parallel corpora
or recognition benchmarks. We also expect to see paraphrasing and textual entailment methods
being used more often in larger natural language processing tasks, including question answering,
information extraction, text summarization, natural language generation, and machine translation.
171

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Main ideas discussed
Logical-based inferencing
Vector space semantic models
Surface string similarity measures
Syntactic similarity measures
Similarity measures operating on
symbolic meaning representations
Machine learning algorithms
Decoding (transformation sequences)
Word/sentence alignment
Pivot language(s)
Bootstrapping
Distributional hypothesis
Synchronous grammar rules

Main typically required resources
Parser producing logical meaning representations, inferencing engine,
resources to extract meaning postulates and common sense knowledge from.
Large monolingual corpus, possibly parser.
Only preprocessing tools, e.g., POS tagger, named-entity recognizer, which
are also required by most other methods.
Parser.
Lexical semantic resources, possibly parser and/or semantic role labeling
to produce semantic representations.
Training/testing datasets, components/resources needed to compute features.
Synonyms, hypernyms-hyponyms, paraphrasing/TE rules.
Large parallel or comparable corpora (monolingual or multilingual), possibly
parser.
Multilingual parallel corpora.
Large monolingual corpus, recognizer.
Monolingual corpus (possibly parallel or comparable).
Monolingual parallel corpus.

Table 4: Main ideas discussed and main resources they typically require.

Acknowledgments
We thank the three anonymous reviewers for their valuable comments. This work was funded by
the Greek PENED project Combined research in the areas of information retrieval, natural language
processing, and user modeling aiming at the development of advanced search engines for document
collections, which was co-funded by the European Union (80%) and the Greek General Secretariat
for Research and Technology (20%).

Appendix A. On-line Resources Mentioned
A.1 Bibliographic Resources, Portals, Tutorials
ACL 2007 tutorial on textual entailment: http://www.cs.biu.ac.il/dagan
/TE-Tutorial-ACL07.ppt.
ACL Anthology: http://www.aclweb.org/anthology/.
Textual Entailment Portal: http://www.aclweb.org/aclwiki/index.php?
title=Textual Entailment Portal.
A.2 Corpora, Challenges, and their Datasets
Cohn et al.s paraphrase corpus: Word-aligned paraphrases;
http://www.dcs.shef.ac.uk/tcohn/paraphrase corpus.html.
FATE: The RTE-2 dataset with FrameNet annotations;
http://www.coli.uni-saarland.de/projects/salsa/fate/.
MSR Paraphrase Corpus: Paraphrase recognition benchmark dataset;
http://research.microsoft.com/en-us/downloads/607d14d9-20cd-47e3-85bc-a2f65cd28042/.
172

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Multiple-Translation Chinese Corpus: Multiple English translations of Chinese news articles;
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2002T01.
RTE challenges, PASCAL Network of Excellence: Textual entailment recognition challenges and
their datasets; http://pascallin.ecs.soton.ac.uk/Challenges/.
RTE track of NISTs Text Analysis Conference: Continuation of PASCALs RTE;
http://www.nist.gov/tac/tracks/.
Written News Compression Corpus: Sentence compression corpus;
http://jamesclarke.net/research/.
A.3 Implementations of Machine Learning Algorithms
LIBSVM: SVM implementation; http://www.csie.ntu.edu.tw/cjlin/libsvm/.
Stanfords Maximum Entropy classifier: http://nlp.stanford.edu/software/index.shtml.
SVM-Light: SVM implementation; http://svmlight.joachims.org/.
Weka: Includes implementations of many machine learning algorithms;
http://www.cs.waikato.ac.nz/ml/weka/.
A.4 Implementations of Similarity Measures
EDITS: Suite to recognize textual entailment by computing edit distances; http://edits.fbk.eu/.
WordNet::Similarity: Implementations of WordNet-based similarity measures;
http://wn-similarity.sourceforge.net/.
A.5 Parsers, POS Taggers, Named Entity Recognizers, Stemmers
Brills POS tagger: http://en.wikipedia.org/wiki/Brill tagger.
Charniaks parser: http://flake.cs.uiuc.edu/cogcomp/srl/CharniakServer.tgz.
Collins parser: http://people.csail.mit.edu/mcollins/code.html.
Link Grammar Parser: http://www.abisource.com/projects/link-grammar/.
MaltParser: http://w3.msi.vxu. se/nivre/research/MaltParser.html.
MINIPAR: http://www.cs.ualberta.ca/lindek/minipar.htm.
Porters stemmer: http://tartarus.org/martin/PorterStemmer/.
Stanfords named-entity recognizer, parser, tagger: http://nlp.stanford.edu/software/index.shtml.
173

fiA NDROUTSOPOULOS & M ALAKASIOTIS

A.6 Statistical Machine Translation Tools and Resources
Giza++: Often used to train IBM models and align words; http://www.fjoch.com/GIZA++.html.
Koehns Statistical Machine Translation site: Pointers to commonly used SMT tools, resources;
http://www.statmt.org/.
Moses: Frequently used SMT system that includes decoding facilities;
http://www.statmt.org/moses/.
SRILM: Commonly used to create language models;
http://www.speech.sri.com/projects/srilm/.
A.7 Lexical Resources, Paraphrasing and Textual Entailment Rules
Callison-Burchs paraphrase rules: Paraphrase rules extracted from multilingual parallel corpora
via pivot language(s); the implementation of the method used is also available;
http://cs.jhu.edu/ccb/.
DIRT rules: Template pairs produced by DIRT; http://demo.patrickpantel.com/.
Extended WordNet: Includes meaning representations extracted from WordNets glosses;
http://wordnet.princeton.edu/.
FrameNet: http://framenet.icsi.berkeley.edu/.
Nomlex: English nominalizations of verbs; http://nlp.cs.nyu.edu/nomlex/
TEASE rules: Textual entailment rules produced by TEASE;
http://www.cs.biu.ac.il/szpekti/TEASE collection.zip.
VerbNet: http://verbs.colorado.edu/mpalmer/projects/verbnet.html.
WordNet: http://xwn.hlt.utdallas.edu/.
Zhao et al.s paraphrase rules: Paraphrase rules with slots corresponding to POS tags, extracted
from multilingual parallel corpora via pivot language(s);
http://ir.hit.edu.cn/phpwebsite/index.php?
module=documents&JAS DocumentManager op=viewDocument&JAS Document id=268.

References
Alpaydin, E. (2004). Introduction to Machine Learning. MIT Press.
Androutsopoulos, I., Oberlander, J., & Karkaletsis, V. (2007). Source authoring for multilingual
generation of personalised object descriptions. Nat. Lang. Engineering, 13(3), 191233.
Androutsopoulos, I., Ritchie, G. D., & Thanisch, P. (1995). Natural language interfaces to databases
 an introduction. Nat. Lang. Engineering, 1(1), 2981.
Baeza-Yates, R., & Ribeiro-Neto, B. (1999). Modern Information Retrieval. Addison Wesley.
174

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Baker, C. F., Fillmore, C. J., & Lowe, J. B. (1998). The Berkeley FrameNet project. In Proc. of the
17th Int. Conf. on Comp. Linguistics, pp. 8690, Montreal, Quebec, Canada.
Bannard, C., & Callison-Burch, C. (2005). Paraphrasing with bilingual parallel corpora. In Proc. of
the 43rd Annual Meeting of ACL, pp. 597604, Ann Arbor, MI.
Bar-Haim, R., Berant, J., & Dagan, I. (2009). A compact forest for scalable inference over entailment and paraphrase rules. In Proc. of the Conf. on EMNLP, pp. 10561065, Singapore.
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B., & Szpektor, I. (2006).
The 2nd PASCAL recognising textual entailment challenge. In Proc. of the 2nd PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.
Bar-Haim, R., Dagan, I., Greental, I., & Shnarch, E. (2007). Semantic inference at the lexicalsyntactic level. In Proc. of the 22nd Conf. on Artificial Intelligence, pp. 871876, Vancouver,
BC , Canada.
Barzilay, R., & Elhadad, N. (2003). Sentence alignment for monolingual comparable corpora. In
Proc. of the Conf. on EMNLP, pp. 2532, Sapporo, Japan.
Barzilay, R., & Lee, L. (2002). Bootstrapping lexical choice via multiple-sequence alignment. In
Proc. of the Conf. on EMNLP, pp. 164171, Philadelphia, PA.
Barzilay, R., & Lee, L. (2003). Learning to paraphrase: an unsupervised approach using multiplesequence alignment. In Proc. of the HLT Conf. of NAACL, pp. 1623, Edmonton, Canada.
Barzilay, R., & McKeown, K. (2001). Extracting paraphrases from a parallel corpus. In Proc. of the
39th Annual Meeting of ACL, pp. 5057, Toulouse, France.
Barzilay, R., & McKeown, K. R. (2005). Sentence fusion for multidocument news summarization.
Comp. Linguistics, 31(3), 297327.
Bateman, J., & Zock, M. (2003). Natural language generation. In Mitkov, R. (Ed.), The Oxford
Handbook of Comp. Linguistics, chap. 15, pp. 284304. Oxford University Press.
Bensley, J., & Hickl, A. (2008). Workshop: Application of LCCs GROUNGHOG system for RTE -4.
In Proc. of the Text Analysis Conference, Gaithersburg, MD.
Bergmair, R. (2009). A proposal on evaluation measures for RTE. In Proc. of the ACL Workshop on
Applied Textual Inference, pp. 1017, Singapore.
Berwick, R. C. (1991). Principles of principle-based parsing. In Berwick, R. C., Abney, S. P.,
& Tenny, C. (Eds.), Principle-Based Parsing: Computation and Psycholinguistics, pp. 137.
Kluwer, Dordrecht, Netherlands.
Bhagat, R., Pantel, P., & Hovy, E. (2007). LEDIR: An unsupervised algorithm for learning directionality of inference rules. In Proc. of the Conf. on EMNLP and the Conf. on Computational
Nat. Lang. Learning, pp. 161170, Prague, Czech Republic.
Bhagat, R., & Ravichandran, D. (2008). Large scale acquisition of paraphrases for learning surface
patterns. In Proc. of the 46th Annual Meeting of ACL: HLT, pp. 674682, Columbus, OH.
Bikel, D. M., Schwartz, R. L., & Weischedel, R. M. (1999). An algorithm that learns whats in a
name. Machine Learning, 34(1-3), 211231.
Blum, A., & Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In Proc.
of the 11th Annual Conf. on Computational Learning Theory, pp. 92100, Madison, WI.
175

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Bos, J., & Markert, K. (2005). Recognising textual entailment with logical inference. In Proc. of
the Conf. on HLT and EMNLP, pp. 628635, Vancouver, BC, Canada.
Brill, E. (1992). A simple rule-based part of speech tagger. In Proc. of the 3rd Conf. on Applied
Nat. Lang. Processing, pp. 152155, Trento, Italy.
Brockett, C., & Dolan, W. (2005). Support Vector Machines for paraphrase identification and corpus
construction. In Proc. of the 3rd Int. Workshop on Paraphrasing, pp. 18, Jeju island, Korea.
Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., & Mercer, R. L. (1993). The mathematics of
statistical machine translation: Parameter estimation. Comp. Linguistics, 19(2), 263311.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures of lexical semantic relatedness. Comp. Linguistics, 32(1), 1347.
Burchardt, A., & Pennacchiotti, M. (2008). FATE: A FrameNet-annotated corpus for textual entailment. In Proc. of the 6th Language Resources and Evaluation Conference, Marrakech,
Marocco.
Burchardt, A., Pennacchiotti, M., Thater, S., & Pinkal, M. (2009). Assessing the impact of frame
semantics on textual entailment. Nat. Lang. Engineering, 15(4).
Burchardt, A., Reiter, N., Thater, S., & Frank, A. (2007). A semantic approach to textual entailment: System evaluation and task analysis. In Proc. of the ACL - PASCAL Workshop on Textual
Entailment and Paraphrasing, pp. 1015, Prague, Czech Republic. ACL.
Califf, M., & Mooney, R. (2003). Bottom-up relational learning of pattern matching rules for information extraction. Journal of Machine Learning Research, 4, 177210.
Callison-Burch, C. (2008). Syntactic constraints on paraphrases extracted from parallel corpora. In
Proc. of the Conf. on EMNLP, pp. 196205, Honolulu, HI.
Callison-Burch, C., Cohn, T., & Lapata, M. (2008). ParaMetric: An automatic evaluation metric for
paraphrasing. In Proc. of the 22nd Int. Conf. on Comp. Linguistics, pp. 97104, Manchester,
UK .
Callison-Burch, C., Dagan, I., Manning, C., Pennacchiotti, M., & Zanzotto, F. M. (Eds.). (2009).
Proc. of the ACL - IJCNLP Workshop on Applied Textual Inference. Singapore.
Callison-Burch, C., Koehn, P., & Osborne, M. (2006a). Improved statistical machine translation
using paraphrases. In Proc. of the HLT Conf. of the NAACL, pp. 1724, New York, NY.
Callison-Burch, C., Osborne, M., & Koehn, P. (2006b). Re-evaluating the role of BLEU in machine
translation research. In Proc. of the 11th Conf. of EACL, pp. 249256, Trento, Italy.
Carnap, R. (1952). Meaning postulates. Philosophical Studies, 3(5).
Charniak, E. (2000). A maximum-entropy-inspired parser. In Proc. of the 1st Conf. of NAACL, pp.
132139, Seattle, WA.
Chevelu, J., Lavergne, T., Lepage, Y., & Moudenc, T. (2009). Introduction of a new paraphrase
generation tool based on Monte-Carlo sampling. In Proc. of the 47th Annual Meeting of ACL
and the 4th Int. Joint Conf. on Nat. Lang. Processing of AFNLP, pp. 249252, Singapore.
Clarke, D. (2009). Context-theoretic semantics for natural language: an overview. In Proc. of
the EACL workshop on Geometrical Models of Nat. Lang. Semantics, pp. 112119, Athens,
Greece.
176

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Clarke, J., & Lapata, M. (2008). Global inference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelligence Research,, 31(1), 399429.
Cohn, T., Callison-Burch, C., & Lapata, M. (2008). Constructing corpora for the development and
evaluation of paraphrase systems. Comp. Linguistics, 34(4), 597614.
Cohn, T., & Lapata, M. (2008). Sentence compression beyond word deletion. In Proc. of the 22nd
Int. Conf. on Comp. Linguistics, Manchester, UK.
Cohn, T., & Lapata, M. (2009). Sentence compression as tree transduction. Journal of Artificial
Intelligence Research, 34(1), 637674.
Collins, M. (2003). Head-driven statistical models for natural language parsing. Comput. Linguistics, 29(4), 589637.
Corley, C., & Mihalcea, R. (2005). Measuring the semantic similarity of texts. In Proc. of the ACL
Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pp. 1318, Ann
Arbor, MI.
Cristianini, N., & Shawe-Taylor, J. (2000). An Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge University Press.
Culicover, P. (1968). Paraphrase generation and information retrieval from stored text. Mechanical
Translation and Computational Linguistics, 11(12), 7888.
Dagan, I., Dolan, B., Magnini, B., & Roth, D. (2009). Recognizing textual entailment: Rational,
evaluation and approaches. Nat. Lang. Engineering, 15(4), ixvii. Editorial of the special
issue on Textual Entailment.
Dagan, I., Glickman, O., & Magnini, B. (2006). The PASCAL recognising textual entailment challenge. In Quinonero-Candela, J., Dagan, I., Magnini, B., & dAlche Buc, F. (Eds.), Machine
Learning Challenges. Lecture Notes in Computer Science, Vol. 3944, pp. 177190. SpringerVerlag.
Das, D., & Smith, N. A. (2009). Paraphrase identification as probabilistic quasi-synchronous recognition. In Proc. of the 47th Annual Meeting of ACL and the 4th Int. Joint Conf. on Nat. Lang.
Processing of AFNLP, pp. 468476, Singapore.
de Marneffe, M., Rafferty, A., & Manning, C. (2008). Finding contradictions in text. In Proc. of the
46th Annual Meeting of ACL: HLT, pp. 10391047, Columbus, OH.
Deleger, L., & Zweigenbaum, P. (2009). Extracting lay paraphrases of specialized expressions from
monolingual comparable medical corpora. In Proc. of the 2nd Workshop on Building and
Using Comparable Corpora: from Parallel to Non-parallel Corpora, pp. 210, Singapore.
Dolan, B., & Dagan, I. (Eds.). (2005). Proc. of the ACL workshop on Empirical Modeling of Semantic Equivalence and Entailment. Ann Arbor, MI.
Dolan, B., Quirk, C., & Brockett, C. (2004). Unsupervised construction of large paraphrase corpora: Eploiting massively parallel news sources. In Proc. of the 20th Int. Conf. on Comp.
Linguistics, pp. 350356, Geneva, Switzerland.
Dolan, W. B., & Brockett, C. (2005). Automatically constructing a corpus of sentential paraphrases.
In Proc. of the 3rd Int. Workshop on Paraphrasing, pp. 916, Jeju island, Korea.
177

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Dras, M. (1998). Search in constraint-based paraphrasing. In Proc. of the 2nd Int. Conf. on Natural
Lang. Processing and Industrial Applications, pp. 213219, Moncton, Canada.
Drass, M., & Yamamoto, K. (Eds.). (2005). Proc. of the 3rd Int. Workshop on Paraphrasing. Jeju
island, Korea.
Duboue, P. A., & Chu-Carroll, J. (2006). Answering the question you wish they had asked: The
impact of paraphrasing for question answering. In Proc. of the HLT Conf. of NAACL, pp.
3336, New York, NY.
Duclaye, F., Yvon, F., & Collin, O. (2003). Learning paraphrases to improve a question-answering
system. In Proc. of the EACL Workshop on Nat. Lang. Processing for Question Answering,
pp. 3541, Budapest, Hungary.
Durbin, R., Eddy, S., Krogh, A., & Mitchison, G. (1998). Biological Sequence Analysis. Cambridge
University Press.
Elhadad, N., & Sutaria, K. (2007). Mining a lexicon of technical terms and lay equivalents. In Proc.
of the Workshop on BioNLP, pp. 4956, Prague, Czech Republic.
Erk, K., & Pado, S. (2006). Shalmaneser  a toolchain for shallow semantic parsing. In Proc. of the
5th Language Resources and Evaluation Conference, Genoa, Italy.
Erk, K., & Pado, S. (2009). Paraphrase assessment in structured vector space: Exploring parameters
and datasets. In Proc. of the EACL Workshop on Geometrical Models of Nat. Lang. Semantics,
pp. 5765, Athens, Greece.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press.
Finch, A., Hwang, Y. S., & Sumita, E. (2005). Using machine translation evaluation techniques to
determine sentence-level semantic equivalence. In Proc. of the 3rd Int. Workshop on Paraphrasing, pp. 1724, Jeju Island, Korea.
Freund, Y., & Schapire, R. E. (1995). A decision-theoretic generalization of on-line learning and
an application to boosting. In Proc. of the 2nd European Conf. on Computational Learning
Theory, pp. 2337, Barcelona, Spain.
Friedman, J., Hastie, T., & Tibshirani, R. (2000). Additive logistic regression: a statistical view of
boosting. Annals of Statistics, 28(2), 337374.
Fung, P., & Cheung, P. (2004). Multi-level bootstrapping for extracting parallel sentences from a
quasi-comparable corpus. In Proc. of the 20th Int. Conf. on Comp. Linguistics, pp. 1051
1057, Geneva, Switzerland.
Galanis, D., & Androutsopoulos, I. (2010). An extractive supervised two-stage method for sentence
compression. In Proc. of the HLT Conf. of NAACL, Los Angeles, CA.
Gale, W., & Church, K. (1993). A program for aligning sentences in bilingual corpora. Comp.
Linguistics, 19(1), 75102.
Germann, U., Jahr, M., Knight, K., Marcu, D., & Yamada, K. (2001). Fast decoding and optimal
decoding for machine translation. In Proc. of the 39th Annual Meeting on ACL, pp. 228235,
Toulouse, France.
Giampiccolo, D., Dang, H., Magnini, B., Dagan, I., & Dolan, B. (2008). The fourth PASCAL recognizing textual entailment challenge. In Proc. of the Text Analysis Conference, pp. 19,
Gaithersburg, MD.
178

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Giampiccolo, D., Magnini, B., Dagan, I., & Dolan, B. (2007). The third PASCAL recognizing textual entailment challenge. In Proc. of the ACL-Pascal Workshop on Textual Entailment and
Paraphrasing, pp. 19, Prague, Czech Republic.
Glickman, O., & Dagan, I. (2004). Acquiring lexical paraphrases from a single corpus. In Nicolov, N., Bontcheva, K., Angelova, G., & Mitkov, R. (Eds.), Recent Advances in Nat. Lang.
Processing III, pp. 8190. John Benjamins.
Grishman, R. (2003). Information extraction. In Mitkov, R. (Ed.), The Oxford Handbook of Comp.
Linguistics, chap. 30, pp. 545559. Oxford University Press.
Habash, N., & Dorr, B. (2003). A categorial variation database for english. In Proc. of the HLT
Conf. of NAACL, pp. 1723, Edmonton, Canada.
Haghighi, A. D. (2005). Robust textual inference via graph matching. In Proc. of the Conf. on
EMNLP , pp. 387394, Vancouver, BC , Canada.
Harabagiu, S., & Hickl, A. (2006). Methods for using textual entailment in open-domain question
answering. In Proc. of the 21st Int. Conf. on Comp. Linguistics and the 44th Annual Meeting
of ACL, pp. 905912, Sydney, Australia.
Harabagiu, S., Hickl, A., & Lacatusu, F. (2006). Negation, contrast and contradiction in text processing. In Proc. of the 21st National Conf. on Artificial Intelligence, pp. 755762, Boston,
MA .
Harabagiu, S., & Moldovan, D. (2003). Question answering. In Mitkov, R. (Ed.), The Oxford
Handbook of Comp. Linguistics, chap. 31, pp. 560582. Oxford University Press.
Harabagiu, S. M., Maiorano, S. J., & Pasca, M. A. (2003). Open-domain textual question answering
techniques. Nat. Lang. Engineering, 9(3), 231267.
Harmeling, S. (2009). Inferring textual entailment with a probabilistically sound calculus. Nat.
Lang. Engineering, 15(4), 459477.
Harris, Z. (1964). Distributional Structure. In Katz, J., & Fodor, J. (Eds.), The Philosphy of Linguistics, pp. 3349. Oxford University Press.
Hashimoto, C., Torisawa, K., Kuroda, K., De Saeger, S., Murata, M., & Kazama, J. (2009). Largescale verb entailment acquisition from the Web. In Proc. of the Conf. on EMNLP, pp. 1172
1181, Singapore.
Hearst, M. (1998). Automated discovery of Wordnet relations. In Fellbaum, C. (Ed.), WordNet: An
Electronic Lexical Database. MIT Press.
Herbelot, A. (2009). Finding word substitutions using a distributional similarity baseline and immediate context overlap. In Proc. of the Student Research Workshop of the 12th Conf. of EACL,
pp. 2836, Athens, Greece.
Hickl, A. (2008). Using discourse commitments to recognize textual entailment. In Proc. of the
22nd Int. Conf. on Comp. Linguistics, pp. 337344, Manchester, UK.
Hobbs, J. (1986). Resolving pronoun references. In Readings in Nat. Lang. Processing, pp. 339
352. Morgan Kaufmann.
Hovy, E. (2003). Text summarization. In Mitkov, R. (Ed.), The Oxford Handbook of Comp. Linguistics, chap. 32, pp. 583598. Oxford University Press.
179

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Huffman, S. (1995). Learning information extraction patterns from examples. In Proc. of the IJCAI
Workshop on New Approaches to Learning for Nat. Lang. Processing, pp. 127142, Montreal,
Quebec, Canada.
Ibrahim, A., Katz, B., & Lin, J. (2003). Extracting structural paraphrases from aligned monolingual
corpora. In Proc. of the ACL Workshop on Paraphrasing, pp. 5764, Sapporo, Japan.
Iftene, A. (2008). UAIC participation at RTE 4. In Proc. of the Text Analysis Conference, Gaithersburg, MD.
Iftene, A., & Balahur-Dobrescu, A. (2007). Hypothesis transformation and semantic variability rules
used in recognizing textual entailment. In Proc. of the ACL - PASCAL Workshop on Textual
Entailment and Paraphrasing, pp. 125130, Prague, Czech Republic.
Inui, K., & Hermjakob, U. (Eds.). (2003). Proc. of the 2nd Int. Workshop on Paraphrasing: Paraphrase Acquisition and Applications. Sapporo, Japan.
Jacquemin, C., & Bourigault, D. (2003). Term extraction and automatic indexing. In Mitkov, R.
(Ed.), The Oxford Handbook of Comp. Linguistics, chap. 33, pp. 599615. Oxford University
Press.
Joachims, T. (2002). Learning to Classify Text Using Support Vector Machines: Methods, Theory,
Algorithms. Kluwer.
Jurafsky, D., & Martin, J. H. (2008). Speech and Language Processing (2nd edition). Prentice Hall.
Kauchak, D., & Barzilay, R. (2006). Paraphrasing for automatic evaluation. In Proc. of the HLT
Conf. of NAACL, pp. 455462, New York, NY.
Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. In Proc. of the 41st Annual
Meeting of ACL, pp. 423430, Sapporo, Japan.
Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: A probalistic approach
to sentence compression. Artificial Intelligence, 139(1), 91107.
Koehn, P. (2004). Pharaoh: a beam search decoder for phrase-based statistical machine translation
models. In Proc. of the 6th Conf. of the Association for Machine Translation in the Americas,
pp. 115124, Washington, DC.
Koehn, P. (2009). Statistical Machine Translation. Cambridge University Press.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. In Proc. of the HLT
Conf. of NAACL, pp. 4854, Edmonton, Canada. ACL.
Kohomban, U., & Lee, W. (2005). Learning semantic classes for word sense disambiguation. In
Proc. of the 43rd Annual Meeting of ACL, pp. 3441, Ann Arbor, MI.
Kouylekov, M., & Magnini, B. (2005). Recognizing textual entailment with tree edit distance algorithms. In Proc. of the PASCAL Recognising Textual Entailment Challenge.
Kubler, S., McDonald, R., & Nivre, J. (2009). Dependency Parsing. Synthesis Lectures on HLT.
Morgan and Claypool Publishers.
Lappin, S., & Leass, H. (1994). An algorithm for pronominal anaphora resolution. Comp. Linguistics, 20(4), 535561.
Leacock, C., Miller, G., & Chodorow, M. (1998). Using corpus statistics and WordNet relations for
sense identification. Comp. Linguistics, 24(1), 147165.
180

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Lepage, Y., & Denoual, E. (2005). Automatic generation of paraphrases to be used as translation
references in objective evaluation measures of machine translation. In Proc. of the 3rd Int.
Workshop on Paraphrasing, pp. 5764, Jesu Island, Korea.
Levenshtein, V. (1966). Binary codes capable of correcting deletions, insertions, and reversals.
Soviet Physice-Doklady, 10, 707710.
Lin, D. (1994). PRINCIPAR: an efficient, broad-coverage, principle-based parser. In Proc. of the
15th Conf. on Comp. Linguistics, pp. 482488, Kyoto, Japan. ACL.
Lin, D. (1998a). Automatic retrieval and clustering of similar words. In Proc. of the the 36th Annual
Meeting of ACL and 17th Int. Conf. on Comp. Linguistics, pp. 768774, Montreal, Quebec,
Canada.
Lin, D. (1998b). An information-theoretic definition of similarity. In Proc. of the 15th Int. Conf. on
Machine Learning, pp. 296304, Madison, WI. Morgan Kaufmann, San Francisco, CA.
Lin, D. (1998c). An information-theoretic definition of similarity. In Proc. of the 15th Int. Conf. on
Machine Learning, pp. 296304, Madison, WI.
Lin, D., & Pantel, P. (2001). Discovery of inference rules for question answering. Nat. Lang.
Engineering, 7, 343360.
Lonneker-Rodman, B., & Baker, C. (2009). The FrameNet model and its applications. Nat. Lang.
Engineering, 15(3), 414453.
MacCartney, B., Galley, M., & Manning, C. (2008). A phrase-based alignment model for natural
language inference. In Proc. of the Conf. on EMNLP, pp. 802811, Honolulu, Hawaii.
MacCartney, B., & Manning, C. (2009). An extended model of natural logic. In Proc. of the 8th Int.
Conf. on Computational Semantics, pp. 140156, Tilburg, The Netherlands.
Madnani, N., Ayan, F., Resnik, P., & Dorr, B. J. (2007). Using paraphrases for parameter tuning in
statistical machine translation. In Proc. of 2nd Workshop on Statistical Machine Translation,
pp. 120127, Prague, Czech Republic.
Malakasiotis, P. (2009). Paraphrase recognition using machine learning to combine similarity measures. In Proc. of the 47th Annual Meeting of ACL and the 4th Int. Joint Conf. on Nat. Lang.
Processing of AFNLP, Singapore.
Malakasiotis, P., & Androutsopoulos, I. (2007). Learning textual entailment using SVMs and string
similarity measures. In Proc. of the ACL - PASCAL Workshop on Textual Entailment and Paraphrasing, pp. 4247, Prague. ACL.
Mani, I. (2001). Automatic Summarization. John Benjamins.
Manning, C. D. (2008). Introduction to Information Retrieval. Cambridge University Press.
Manning, C. D., & Schuetze, H. (1999). Foundations of Statistical Natural Language Processing.
MIT press.
Marquez, L., Carreras, X., Litkowski, K. C., & Stevenson, S. (2008). Semantic role labeling: an
introduction to the special issue. Comp. Linguistics, 34(2), 145159.
Marton, Y., Callison-Burch, C., & Resnik, P. (2009). Improved statistical machine translation using
monolingually-derived paraphrases. In Proc. of Conf. on EMNLP, pp. 381390, Singapore.
181

fiA NDROUTSOPOULOS & M ALAKASIOTIS

McCarthy, D., & Navigli, R. (2009). The English lexical substitution task. Lang. Resources &
Evaluation, 43, 139159.
McDonald, R. (2006). Discriminative sentence compression with soft syntactic constraints. In Proc.
of the 11th Conf. of EACL, pp. 297304, Trento, Italy.
McKeown, K. (1983). Paraphrasing questions using given and new information. Comp. Linguistics,
9(1).
Mehdad, Y. (2009). Automatic cost estimation for tree edit distance using particle swarm optimization. In Proc. of the 47th Annual Meeting of ACL and the 4th Int. Joint Conf. on Nat. Lang.
Processing of AFNLP, pp. 289292, Singapore.
Melamed, D. (1999). Bitext maps and alignment via pattern recognition. Comp. Linguistics, 25(1),
107130.
Melcuk, I. (1987). Dependency Syntax: Theory and Practice. State University of New York Press.
Meyers, A., Macleod, C., Yangarber, R., Grishman, R., Barrett, L., & Reeves, R. (1998). Using
NOMLEX to produce nominalization patterns for information extraction. In Proc. of the
COLING - ACL workshop on the Computational Treatment of Nominals, Montreal, Quebec,
Canada.
Mintz, M., Bills, S., Snow, R., & Jurafsky, D. (2009). Distant supervision for relation extraction
without labeled data. In Proc. of the 47th Annual Meeting of ACL and the 4th Int. Joint Conf.
on Nat. Lang. Processing of AFNLP, pp. 10031011, Singapore.
Mirkin, S., Dagan, I., & Shnarch, E. (2009a). Evaluating the inferential utility of lexical-semantic
resources. In Proc. of the 12th Conf. of EACL, pp. 558566, Athens, Greece.
Mirkin, S., Specia, L., Cancedda, N., Dagan, I., Dymetman, M., & Szpektor, I. (2009b). Sourcelanguage entailment modeling for translating unknown terms. In Proc. of the 47th Annual
Meeting of ACL and the 4th Int. Joint Conf. on Nat. Lang. Processing of AFNLP, pp. 791
799, Singapore.
Mitchell, J., & Lapata, M. (2008). Vector-based models of semantic composition. In Proc. of the
46th Annual Meeting of ACL: HLT, pp. 236244, Columbus, OH.
Mitchell, T. (1997). Machine Learning. Mc-Graw Hill.
Mitkov, R. (2003). Anaphora resolution. In Mitkov, R. (Ed.), The Oxford Handbook of Comp.
Linguistics, chap. 14, pp. 266283. Oxford University Press.
Moens, M. (2006). Information Extraction: Algorithms and Prospects in a Retrieval Context.
Springer.
Moldovan, D., & Rus, V. (2001). Logic form transformation of WordNet and its applicability to
question answering. In Proc. of the 39th Annual Meeting of ACL, pp. 402409, Toulouse,
France.
Molla, D., Schwitter, R., Rinaldi, F., Dowdall, J., & Hess, M. (2003). Anaphora resolution in E X TR A NS. In Proc. of the Int. Symposium on Reference Resolution and Its Applications to
Question Answering and Summarization, pp. 2325, Venice, Italy.
Molla, D., & Vicedo, J. (2007). Question answering in restricted domains: An overview. Comp.
Linguistics, 33(1), 4161.
182

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Moore, R. C. (2001). Towards a simple and accurate statistical approach to learning translation relationships among words. In Proc. of the ACL Workshop on Data-Driven Machine Translation,
Toulouse, France.
Moschitti, A. (2009). Syntactic and semantic kernels for short text pair categorization. In Proc. of
the 12th Conf. of EACL, pp. 576584, Athens, Greece.
Munteanu, D. S., & Marcu, D. (2006). Improving machine translation performance by exploiting
non-parallel corpora. Comp. Linguistics, 31(4), 477504.
Muslea, I. (1999). Extraction patterns for information extraction tasks: a survey. In Proc. of the
AAAI Workshop on Machine Learning for Information Extraction, Orlando, FL.
Navigli, R. (2008). A structural approach to the automatic adjudication of word sense disagreements. Nat. Lang. Engineering, 14(4), 547573.
Nelken, R., & Shieber, S. M. (2006). Towards robust context-sensitive sentence alignment for
monolingual corpora. In Proc. of the 11th Conf. of EACL, pp. 161168, Trento, Italy.
Nielsen, R., Ward, W., & Martin, J. (2009). Recognizing entailment in intelligent tutoring systems.
Nat. Lang. Engineering, 15(4), 479501.
Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit, G., Kuebler, S., Marinov, S., & Marsi, E. (2007).
M ALT PARSER: a language-independent system for data-driven dependency parsing. Nat.
Lang. Engineering, 13(2), 95135.
Och, F. J., & Ney, H. (2003). A systematic comparison of various stat. alignment models. Comp.
Ling., 29(1), 1921.
ODonnell, M., Mellish, C., Oberlander, J., & Knott, A. (2001). ILEX: An architecture for a dynamic
hypertext generation system. Nat. Lang. Engineering, 7(3), 225250.
Pado, S., Galley, M., Jurafsky, D., & Manning, C. D. (2009). Robust machine translation evaluation
with entailment features. In Proc. of the 47th Annual Meeting of ACL and the 4th Int. Joint
Conf. on Nat. Lang. Processing of AFNLP, pp. 297305, Singapore.
Pado, S., & Lapata, M. (2007). Dependency-based construction of semantic space models. Comp.
Ling., 33(2), 161199.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The Propositional Bank: an annotated corpus of
semantic roles. Comp. Linguistics, 31(1), 71105.
Pang, B., Knight, K., & Marcu, D. (2003). Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences. In Proc. of the Human Lang. Techn. Conf. of
NAACL , pp. 102109, Edmonton, Canada.
Pantel, P., Bhagat, R., Coppola, B., Chklovski, T., & Hovy, E. H. (2007). ISP: Learning inferential
selectional preferences. In Proc. of the HLT Conf. of NAACL, pp. 564571, Rochester, NY.
Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). BLEU: a method for automatic evaluation of
machine translation. In Proc. of the 40th Annual Meeting on ACL, pp. 311318, Philadelphia,
PA .
Pasca, M. (2003). Open-domain question answering from large text collections (2nd edition). Center
for the Study of Language and Information.
183

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Pasca, M., & Dienes, P. (2005). Aligning needles in a haystack: Paraphrase acquisition across the
Web. In Proc. of the 2nd Int. Joint Conf. on Nat. Lang. Processing, pp. 119130, Jeju Island,
Korea.
Perez, D., & Alfonseca, E. (2005). Application of the BLEU algorithm for recognizing textual
entailments. In Proc. of the PASCAL Challenges Worshop on Recognising Textual Entailment,
Southampton, UK.
Porter, M. F. (1997). An algorithm for suffix stripping. In Jones, K. S., & Willet, P. (Eds.), Readings
in Information Retrieval, pp. 313316. Morgan Kaufmann.
Power, R., & Scott, D. (2005). Automatic generation of large-scale paraphrases. In Proc. of the 3rd
Int. Workshop on Paraphrasing, pp. 7379, Jesu Island, Korea.
Qiu, L., Kan, M. Y., & Chua, T. (2006). Paraphrase recognition via dissimilarity significance classification. In Proc. of the Conf. on EMNLP, pp. 1826, Sydney, Australia.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann.
Quirk, C., Brockett, C., & Dolan, W. B. (2004). Monolingual machine translation for paraphrase
generation. In Proc. of the Conf. on EMNLP, pp. 142149, Barcelona, Spain.
Ravichandran, D., & Hovy, E. (2002). Learning surface text patterns for a question answering
system. In Proc. of the 40th Annual Meeting on ACL, pp. 4147, Philadelphia, PA.
Ravichandran, D., Ittycheriah, A., & Roukos, S. (2003). Automatic derivation of surface text patterns for a maximum entropy based question answering system. In Proc. of the HLT Conf. of
NAACL , pp. 8587, Edmonton, Canada.
Reiter, E., & Dale, R. (2000). Building Natural Language Generation Systems. Cambridge University Press.
Resnik, P. (1999). Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. Journal of Artificial Intelligence
Research, 11, 95130.
Riezler, S., Vasserman, A., Tsochantaridis, I., Mittal, V., & Liu, Y. (2007). Statistical machine
translation for query expansion in answer retrieval. In Proc. of the 45th Annual Meeting of
ACL , pp. 464471, Prague, Czech Republic.
Riloff, E. (1996a). Automatically generating extraction patterns from untagged text. In Proc. of the
13th National Conf. on Artificial Intelligence, pp. 10441049, Portland, OR.
Riloff, E. (1996b). An empirical study of automated dictionary construction for information extraction in three domains. Artificial Intelligence, 85(12), 101134.
Riloff, E., & Jones, R. (1999). Learning dictionaries for information extraction by multi-level bootstrapping. In Proc. of the 16th National Conf. on Artificial Intelligence, pp. 474479, Orlando,
FL.
Rinaldi, F., Dowdall, J., Kaljurand, K., Hess, M., & Molla, D. (2003). Exploiting paraphrases in a
question answering system. In Proc. of the 2nd Int. Workshop in Paraphrasing, pp. 2532,
Saporo, Japan.
Sato, S., & Nakagawa, H. (Eds.). (2001). Proc. of the Workshop on Automatic Paraphrasing. Tokyo,
Japan.
184

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Schohn, G., & Cohn, D. (2000). Less is more: active learning with Support Vector Machines. In
Proc. of the 17th Int. Conf. on Machine Learning, pp. 839846, Stanford, CA.
Schuler, K. K. (2005). VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
Univ. of Pennsylvania.
Sekine, S., Inui, K., Dagan, I., Dolan, B., Giampiccolo, D., & Magnini, B. (Eds.). (2007). Proc. of
the ACL - PASCAL Workshop on Textual Entailment and Paraphrasing. Prague, Czech Republic.
Sekine, S., & Ranchhod, E. (Eds.). (2009). Named Entities  Recognition, Classification and Use.
John Benjamins.
Selkow, S. (1977). The tree-to-tree editing problem. Information Processing Letters, 6(6), 184186.
Shinyama, Y., & Sekine, S. (2003). Paraphrase acquisition for information extraction. In Proc. of
the ACL Workshop on Paraphrasing, Sapporo, Japan.
Siblini, R., & Kosseim, L. (2008). Using ontology alignment for the TAC RTE challenge. In Proc.
of the Text Analysis Conference, Gaithersburg, MD.
Sleator, D. D., & Temperley, D. (1993). Parsing English with a link grammar. In Proc. of the 3rd Int.
Workshop on Parsing Technologies, pp. 277292, Tilburg, Netherlands and Durbuy, Belgium.
Soderland, S. (1999). Learning inf. extraction rules for semi-structured and free text. Mach. Learning, 34(13), 233272.
Soderland, S., Fisher, D., Aseltine, J., & Lehnert, W. G. (1995). CRYSTAL: Inducing a conceptual
dictionary. In Proc. of the 14th Int. Joint Conf. on Artificial Intelligence, pp. 13141319,
Montreal, Quebec, Canada.
Stevenson, M., & Wilks, Y. (2003). Word sense disambiguation. In Mitkov, R. (Ed.), The Oxford
Handbook of Comp. Linguistics, chap. 13, pp. 249265. Oxford University Press.
Stolcke, A. (2002). SRILM  an extensible language modeling toolkit. In Proc. of the 7th Int. Conf.
on Spoken Language Processing, pp. 901904, Denver, CO.
Szpektor, I., & Dagan, I. (2007). Learning canonical forms of entailment rules. In Proc. of Recent
Advances in Natural Lang. Processing, Borovets, Bulgaria.
Szpektor, I., & Dagan, I. (2008). Learning entailment rules for unary templates. In Proc. of the
22nd Int. Conf. on Comp. Linguistics, pp. 849856, Manchester, UK.
Szpektor, I., Dagan, I., Bar-Haim, R., & Goldberger, J. (2008). Contextual preferences. In Proc. of
the 46th Annual Meeting of ACL: HLT, pp. 683691, Columbus, OH.
Szpektor, I., Shnarch, E., & Dagan, I. (2007). Instance-based evaluation of entailment rule acquisition. In Proc. of the 45th Annual Meeting of ACL, pp. 456463, Prague, Czech Republic.
Szpektor, I., Tanev, H., Dagan, I., & Coppola, B. (2004). Scaling Web-based acquisition of entailment relations. In Proc. of the Conf. on EMNLP, Barcelona, Spain.
Tai, K.-C. (1979). The tree-to-tree correction problem. Journal of ACM, 26(3), 422433.
Tatu, M., Iles, B., Slavick, J., Novischi, A., & Moldovan, D. (2006). COGEX at the second recognizing textual entailment challenge. In Proc. of the 2nd PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
185

fiA NDROUTSOPOULOS & M ALAKASIOTIS

Tatu, M., & Moldovan, D. (2005). A semantic approach to recognizing textual entailment. In Proc.
of the Conf. on HLT and EMNLP, pp. 371378, Vancouver, Canada.
Tatu, M., & Moldovan, D. (2007). COGEX at RTE 3. In Proc. of the ACL - PASCAL Workshop on
Textual Entailment and Paraphrasing, pp. 2227, Prague, Czech Republic.
Tomuro, N. (2003). Interrogative reformulation patterns and acquisition of question paraphrases. In
Proc. of the 2nd Int. Workshop on Paraphrasing, pp. 3340, Sapporo, Japan.
Tong, S., & Koller, D. (2002). Support Vector Machine active learning with applications to text
classification. Machine Learning Research, 2, 4566.
Toutanova, K., Klein, D., Manning, C. D., & Singer, Y. (2003). Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of the HLT Conf. of NAACL, pp. 173180,
Edmonton, Canada.
Tsatsaronis, G. (2009). Word Sense Disambiguation and Text Relatedness Based on Word Thesauri.
Ph.D. thesis, Department of Informatics, Athens University of Economics and Business.
Tsatsaronis, G., Varlamis, I., & Vazirgiannis, M. (2010). Text relatedness based on a word thesaurus.
Artificial Intelligence Research, 37, 139.
Turney, P., & Pantel, P. (2010). From frequency to meaning: Vector space models of semantics.
Artificial Intelligence Research, 37, 141188.
Vapnik, V. (1998). Statistical learning theory. John Wiley.
Vendler, Z. (1967). Verbs and Times. In Linguistics in Philosophy, chap. 4, pp. 97121. Cornell
University Press.
Vogel, S., Ney, H., & Tillmann, C. (1996). HMM-based word alignment in statistical translation. In
Proc. of the 16th Conf. on Comp. Linguistics, pp. 836841, Copenhagen, Denmark.
Voorhees, E. (2001). The TREC QA track. Nat. Lang. Engineering, 7(4), 361378.
Voorhees, E. (2008). Contradictions and justifications: Extensions to the textual entailment task. In
Proc. of the 46th Annual Meeting of ACL: HLT, pp. 6371, Columbus, OH.
Wan, S., Dras, M., Dale, R., & Paris, C. (2006). Using dependency-based features to take the parafarce out of paraphrase. In Proc. of the Australasian Language Technology Workshop, pp.
131138, Sydney, Australia.
Wang, R., & Neumann, G. (2008). An divide-and-conquer strategy for recognizing textual entailment. In Proc. of the Text Analysis Conference, Gaithersburg, MD.
Wang, X., Lo, D., Jiang, J., Zhang, L., & Mei, H. (2009). Extracting paraphrases of technical terms
from noisy parallel software corpora. In Proc. of the 47th Annual Meeting of ACL and the 4th
Int. Joint Conf. on Nat. Lang. Processing of AFNLP, pp. 197200, Singapore.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools and Techniques.
Morgan Kaufmann.
Wu, D. (2000). Alignment. In Dale, R., Moisl, H., & Somers, H. (Eds.), Handbook of Nat. Lang.
Processing, pp. 415458. Marcel Dekker.
Wubben, S., van den Bosch, A., Krahmer, E., & Marsi, E. (2009). Clustering and matching headlines
for automatic paraphrase acquisition. In Proc. of the 12th European Workshop on Nat. Lang.
Generation, pp. 122125, Athens, Greece.
186

fiA S URVEY OF PARAPHRASING AND T EXTUAL E NTAILMENT M ETHODS

Xu, F., Uszkoreit, H., & Li, H. (2007). A seed-driven bottom-up machine learning framework
for extracting relations of various complexity. In Proc. of the 45th Annual Meeting of the
Association of Comp. Linguistics, pp. 584591, Prague, Czech Republic.
Yang, X., Su, J., & Tan, C. L. (2008). A twin-candidate model for learning-based anaphora resolution. Comp. Linguistics, 34(3), 327356.
Yarowski, D. (2000). Word-sense disambiguation. In Dale, R., Moisl, H., & Somers, H. (Eds.),
Handbook of Nat. Lang. Processing, pp. 629654. Marcel Dekker.
Zaenen, A., Karttunen, L., & Crouch, R. (2005). Local textual inference: Can it be defined or circumscribed?. In Proc. of the ACL workshop on Empirical Modeling of Semantic Equivalence
and Entailment, pp. 3136, Ann Arbor, MI.
Zanzotto, F. M., & Dell Arciprete, L. (2009). Efficient kernels for sentence pair classification. In
Proc. of the Conf. on EMNLP, pp. 91100, Singapore.
Zanzotto, F. M., Pennacchiotti, M., & Moschitti, A. (2009). A machine-learning approach to textual
entailment recognition. Nat. Lang. Engineering, 15(4), 551582.
Zhang, K., & Shasha, D. (1989). Simple fast algorithms for the editing distance between trees and
related problems. SIAM Journal of Computing, 18(6), 12451262.
Zhang, Y., & Patrick, J. (2005). Paraphrase identification by text canonicalization. In Proc. of the
Australasian Language Technology Workshop, pp. 160166, Sydney, Australia.
Zhang, Y., & Yamamoto, K. (2005). Paraphrasing spoken Chinese using a paraphrase corpus. Nat.
Lang. Engineering, 11(4), 417434.
Zhao, S., Lan, X., Liu, T., & Li, S. (2009). Application-driven statistical paraphrase generation. In
Proc. of the 47th Annual Meeting of ACL and the 4th Int. Joint Conf. on Nat. Lang. Processing
of AFNLP, pp. 834842, Singapore.
Zhao, S., Wang, H., Liu, T., & Li, S. (2008). Pivot approach for extracting paraphrase patterns from
bilingual corpora. In Proc. of the 46th Annual Meeting of ACL: HLT, pp. 780788, Columbus,
OH .
Zhitomirsky-Geffet, M., & Dagan, I. (2009). Bootstrapping distributional feature vector quality.
Computational Linguistics, 35, 435461.
Zhou, L., Lin, C.-Y., & Hovy, E. (2006a). Re-evaluating machine translation results with paraphrase
support. In Proc. of the Conf. on EMNLP, pp. 7784.
Zhou, L., Lin, C.-Y., Munteanu, D. S., & Hovy, E. (2006b). PARA E VAL: Using paraphrases to
evaluate summaries automatically. In Proc. of the HLT Conf. of NAACL, pp. 447454, New
York, NY.

187

fiJournal of Artificial Intelligence Research 38 (2010) 49-84

Submitted 11/09; published 05/10

Change in Abstract Argumentation Frameworks:
Adding an Argument
Claudette Cayrol
Florence Dupin de Saint-Cyr
Marie-Christine Lagasquie-Schiex

ccayrol@irit.fr
bannay@irit.fr
lagasq@irit.fr

IRIT, Universite Paul Sabatier,
118 route de Narbonne, 31062 Toulouse, France

Abstract
In this paper, we address the problem of change in an abstract argumentation system.
We focus on a particular change: the addition of a new argument which interacts with
previous arguments. We study the impact of such an addition on the outcome of the argumentation system, more particularly on the set of its extensions. Several properties for this
change operation are defined by comparing the new set of extensions to the initial one, these
properties are called structural when the comparisons are based on set-cardinality or setinclusion relations. Several other properties are proposed where comparisons are based on
the status of some particular arguments: the accepted arguments; these properties refer to
the evolution of this status during the change, e.g., Monotony and Priority to Recency.
All these properties may be more or less desirable according to specific applications. They
are studied under two particular semantics: the grounded and preferred semantics.

1. Introduction
Argumentation has become an influential approach to handle Artificial Intelligence problems
including defeasible reasoning (see e.g., Pollock, 1992; Dung, 1995; Bondarenko, Dung,
Kowalski, & Toni, 1997; Chesnevar, Maguitman, & Loui, 2000; Prakken & Vreeswijk, 2002;
Amgoud & Cayrol, 2002; Nute, 2003), and modeling agents interactions (see e.g., Amgoud,
Maudet, & Parsons, 2000; Kakas & Moratis, 2003). Argumentation is basically concerned
with the exchange of interacting arguments. This set of arguments may come either from
a dialogue between several agents but also from the available (and possibly contradictory)
pieces of information at the disposal of one unique agent. Usually, the interaction between
arguments takes the form of a conflict, called attack. For example, a logical argument
can be a pair hset of assumptions, conclusioni, where the set of assumptions entails the
conclusion according to some logical inference schema. A conflict occurs, for instance, when
the conclusion of an argument contradicts an assumption of another argument.
The main issue for any argumentation system is the selection of acceptable sets of arguments, called extensions, based on the way arguments interact (intuitively, an acceptable
set of arguments must be in some sense coherent and strong enough, e.g., able to defend
itself against all attacking arguments). So, the outcome of an argumentation system is
often defined by the set of its extensions but, depending on the applications, it may be also
defined as the set of arguments that belongs to every extension. It is convenient to explore
the concept of extension through argumentation frameworks, and especially Dungs (1995)
c
2010
AI Access Foundation. All rights reserved.

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

framework, which abstracts from the arguments nature, and represents interaction under
the form of a binary relation attack on a set of arguments.
Recent works have considered the dynamics of such abstract argumentation frameworks
(Cayrol, Dupin de Saint-Cyr, & Lagasquie-Schiex, 2008; Rotstein, Moguillansky, Garca, &
Simari, 2008b; Boella, Kaci, & van der Torre, 2009a, 2009b). The problem is to study how
the outcome changes when the set of arguments and/or the set of attacks between them
are changed. In this paper, we focus on the case when a new argument and its interactions
are added to an argumentation system. We study the impact of such an addition on the
set of initial extensions. This leads us to identify some properties of the change operation
with respect to the modification it induces on the outcome. This study has two main
applications, the first one concerns computational issues, while the second one concerns the
definition of dialogue strategies. On one hand, the interest for computational processing
is that knowledge about the properties of the change may help to deduce what are the
modifications in the extensions. For instance, it is useful to know conditions under which
change will not modify the previous extensions. On the other hand, knowing the impact of
adding an argument may help choosing the good one in order to achieve a given goal. For
instance, in a multi-agent setting, i.e., when several agents may present several arguments,
the results presented in this paper will help one agent to determine which arguments she
should present in order that the outcome of the dialogue satisfies desired properties. For
example, if she wants to widen the debate, the argument that must be added should induce
a change producing larger extensions (i.e. that contain more arguments, see Section 3 and
Section 5).
The paper is organized as follows. Section 2 recalls the basic concepts in argumentation.
Section 3 settles a definition of change in argumentation. Many features can be taken into
account in order to characterize a change operation. We first propose a class of properties
based on the impact of the change on the structure of the resulting set of extensions (see
Section 3.2). In a second step, we define several other properties regarding the arguments
themselves, particularly those which are accepted after change (see Section 3.3). These
properties are defined regardless of the semantics.
Then, we focus on a particular change: the addition of a new argument which may
interact with previously introduced arguments. Section 4 is dedicated to the study of the
properties of this addition in the case of two particular semantics, the grounded and the
preferred semantics. We give conditions under which a given property is satisfied. Section 5
discusses the related approaches in the literature. All the proofs (and two important lemmas) are given in Appendix A. Some additional examples are presented in Appendix B for
illustrating the other change operations.
Note that this paper generalizes a previous work (Cayrol et al., 2008), where argument
addition, called revision, was restricted to one argument having only one interaction
with the existing argumentation system. Here, the added argument may interact with any
number of previous arguments. Moreover, a broader analysis of this generalized addition
is provided by considering new properties such as, e.g., Monotony, and by establishing new
connections between the different properties.
50

fiChange in Argumentation Systems

2. Basic Concepts in Argumentation Frameworks
The present work lies in the frame of the general theory of abstract argumentation frameworks proposed by Dung (1995). Such an abstract framework assumes that a set of arguments is given, as well as the different conflicts between them, and focuses on the definition
of the status of arguments.
Definition 1 (Argumentation framework) An argumentation framework hA, Ri is a
pair, where A is a non-empty set and R is a binary relation on A, called attack relation.
Let A, B  A, (A, B)  R or equivalently ARB means that A attacks B, or B is attacked
by A.
In the following, hA, Ri is an argumentation framework, and we assume that the set of
arguments A is finite. First, it is easy to extend the concept of attack to sets of arguments.
Definition 2 (Attack from and to a set) Let A  A and S  A.1
 S attacks A iff X  S such that XRA.
 A attacks S iff X  S such that ARX.
The main issue of any argumentation system is the selection of acceptable sets of arguments. Intuitively, an acceptable set of arguments must be in some sense coherent and
strong enough (e.g., able to defend itself against every attacking argument). An argumentation semantics defines the properties required for a set of arguments to be acceptable (this
is a collective acceptability). The selected sets of arguments under a given semantics are
called extensions of that semantics. The set of extensions characterizes the outcome of an
argumentation system. We recall the basic concepts used for defining usual semantics:
Definition 3 (Conflict-free, defense) Let A  A and S  A.
 S is conflict-free iff A, B  S such that ARB.
 S defends A iff S attacks each argument which attacks A. The set of arguments which
S defends will be denoted by F(S). F is called the characteristic function of hA, Ri.
The literature proposes an increasing variety of semantics, refining Dungs traditional
ones (Baroni, Giacomin, & Guida, 2005; Caminada, 2006; Dung, Mancarella, & Toni, 2006;
Coste-Marquis, Devred, & Marquis, 2005). In this paper, only the most well-known traditional semantics are considered: the grounded, preferred and stable semantics.
Definition 4 (Acceptability semantics) Let E  A.
 E is admissible iff E is conflict-free and defends all its elements (i.e. E  F(E)).
 E is a preferred extension iff E is a maximal (w.r.t. set-inclusion) admissible set.
1. In this paper, we use  to denote strict inclusion and  to denote classical inclusion.

51

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

 E is the grounded extension iff E is the least fixed point (w.r.t. set-inclusion) of the
characteristic function F.
 E is a stable extension iff E is conflict-free and attacks each argument which does not
belong to E.
An argumentation framework can be represented as a directed graph, called attack
graph, where nodes are the arguments and edges represent the attack relation. Throughout
the paper, examples are using this graph representation.
Example 1
A = {A, B, C, D, F } and R = {(A, B), (B, A), (B, C), (C, D), (D, F ), (F, C)}.
The admissible sets are {}, {A}, {B} and {B, D}.
B
C
D
The preferred extensions are {A} and {B, D}.
The
grounded extension is {}.
A
F
{B, D} is the unique stable extension.
Using the graph-based representation of an argumentation framework, we extend the
definition of individual attack as follows:
Definition 5 (indirect attack and defense) Let G denote the attack graph associated
with hA, Ri. Let A, B  A.
 A indirectly attacks B iff there is an odd-length path from A to B in the attack graph
G.
 A indirectly defends B iff there is an even-length path (with non-zero length) from A
to B in the attack graph G.
Note that the case when A attacks B is considered as a particular case of indirect attack.
Dung (1995) has proved the following results.
Proposition 1 Let hA, Ri be an argumentation framework.
1. There is at least one preferred extension, always a unique grounded extension, while
there may be zero, one or many stable extensions.
2. Each admissible set is included in a preferred extension.
3. Each stable extension is a preferred extension, the converse is false.
4. The grounded extension is included in each preferred extension.
5. Each argument which is not attacked belongs to the grounded extension (hence to each
preferred and to each stable extension).
6. If R is finite, the grounded extension can be computed by iteratively applying the
function F from the empty set.
52

fiChange in Argumentation Systems

The presence of cycles in the attack graph has often raised some problems, namely for the
stable semantics, for which it may happen that no extension exists. Note that some authors
only consider attack graphs without odd-length cycles, arguing that an odd-length cycle
carries counterintuitive information. The following results give properties of the preferred,
grounded and stable extensions depending on the existence of cycles in the attack graph.
Proposition 2 (Dunne & Bench-Capon, 2001, 2002) Let G denote the attack graph associated with hA, Ri.
1. If G contains no cycle, hA, Ri has a unique preferred extension, which is also the
grounded extension and the unique stable extension.
2. If {} is the unique preferred extension of hA, Ri, G contains an odd-length cycle.
3. If hA, Ri has no stable extension, G contains an odd-length cycle.
4. If G contains no odd-length cycle, preferred and stable extensions coincide.
5. If G contains no even-length cycle, hA, Ri has a unique preferred extension.
Now that acceptable sets of arguments have been defined, it is possible to define a status
for an individual argument.
Definition 6 (Argument status) Let hA, Ri be an argumentation framework and A 
A. Given a semantics s:
 A is skeptically accepted under s iff A belongs to each extension of hA, Ri under s.
 A is credulously accepted under s iff A belongs to at least one extension of hA, Ri
under s.
 A is rejected under s iff A does not belong to any extension of hA, Ri under s.
Obviously, credulous and skeptical acceptance coincide under the grounded semantics.

3. Change in Argumentation
We introduce a formal definition of change in argumentation which enables to distinguish
between four types of change. Then we define properties for change in argumentation. First,
we consider the impact of a change operation on the structure of the set of extensions, and
we study how this structure is modified. This point of view leads to the definition of
structural properties. Then, we consider the impact of a change operation on the set of
arguments which are accepted. Finally, the connections between both classes of properties
are studied.
Note that for most of the properties that we introduce, the definition is general in the
sense that it can be applied to any type of change. In Section 4 (where we give conditions
for satisfying these properties), we will focus on the particular case of the addition of an
argument and its interactions.
53

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

3.1 Definition
In this section, we give a definition of change in argumentation. The change may concern
the set of arguments and/or the set of attacks between them. So, at least four cases can be
encountered:
Definition 7 (Change operations) Let hA, Ri be an argumentation framework.
 adding only one interaction i0 between two existing arguments of A (i0 = (X, Y ) with
X  A and Y  A) is a change operation defined by:
hA, Rii i0 = hA, R  {i0 }i
 removing only one existing interaction i0 of hA, Ri (i0  R) is a change operation
defined by:
hA, Rii i0 = hA, R \ {i0 }i
 adding only one argument Z 6 A and a set of interactions concerning Z denoted by
Iz is a change operation defined by:
a

hA, Rii (Z, Iz ) = hA  {Z}, R  Iz i
Here, Iz is supposed to be a non-empty set of pairs of arguments (either of the form
(X, Z) or (Z, X) with X  A)2
 removing only one argument Z  A which interacts with other arguments is a change
operation defined by:
a
hA, Rii Z = hA \ {Z}, R \ Iz i
Here, Iz denotes the set of all the interactions concerning Z, that is the set {(Z, X) |
(Z, X)  R}  {(X, Z)|(X, Z)  R}3
Note that the case of adding a new argument (resp. removing an existing argument)
which does not interact with any other argument is trivial: it has only to be added to (resp.
removed from) each extension. Indeed, change is more interesting when the concerned
argument interacts with previous ones.
In a very recent work about dynamics of argumentation (Boella et al., 2009a, 2009b),
the four types of change defined above have been introduced under different names, respectively attack refinement, attack abstraction, argument refinement and argument abstraction. However, only the operations of attack refinement, attack abstraction and argument
abstraction have been studied and in a more restricted context (see Section 5 for a discussion).
In the following, we identify an argumentation framework hA, Ri with its associated
attack graph G. We write X  G instead of X is an argument represented by a node of G.
The set of extensions of hA, Ri is denoted by E (with E1 , . . . , En denoting the extensions).
2. Note that, by this definition, it is impossible to have (Z, Z) in Iz .
3. Note that if Z is removed, the set of interactions concerning Z must be also removed.

54

fiChange in Argumentation Systems

A change operation produces a new framework hA , R i represented by a graph G  , with
a new set of extensions E (with E1 , . . . , Ep denoting the extensions).
As explained above, changing an argumentation framework may modify the set of extensions. Given a semantics, the modifications are more or less important. It depends on
the kinds of interactions that are added or removed and more precisely on the status of the
arguments involved in these interactions.
The impact of a change can be studied from two points of view:
 the first one concerns the structure of the set of extensions and it can address either
the comparison of the number of extensions before and after the change, or, if this
number remains unchanged, the comparison of the contents of the extensions before
and after the change;
 the second point of view concerns the status of some particular arguments.
So, in the next sections, we propose two classes of general properties for a change
operation, one for each point of view. The proposed properties characterize the relation
between a particular framework and the resulting framework after a change.
3.2 Structural Properties
Structural properties, presented in this section, are based on the impact of the change on
the structure of the set of extensions. Note that for each property, the definition is general
in the sense that the type of change operation is not specified: it can consist in adding
one interaction, removing one interaction, adding an argument and a set of interactions
concerning this argument, or removing one argument. However, for sake of clarity, each
a
property will be illustrated in this section with examples for the change operation i ; the
reader will find some examples for the other change operations in Appendix B.
Let hA, Ri be an argumentation framework and E be the set of extensions of hA, Ri
(under a given semantics s). Various situations may be encountered in the general case.
E may be empty (implying that s is the stable semantics), may be reduced to a singleton
{E1 } (where E1 may be empty), or may contain more than one extension {E1 , . . . , En }. The
situation with only one non-empty extension is convenient for the determination of the
status of an argument. By contrast, when several extensions exist, different choices are
available. Table 1 summarizes the various definitions presented below.
We first consider the decisive property for a change operation, meaning that G  has a
unique non-empty extension, while it was not the case for G.
Definition 8 (Decisive change) The change from G to G  is decisive iff E = , or E =
{{}}, or E = {E1 , . . . , En }, n  2, and E = {E  }, E  6= {}.
Example 2
1. Under the stable (resp. grounded or preferred) semantics, the change
Iz = {(Z, A)} is decisive since:
A

B

Z

C

E =  (resp. E = {{}}),
E = {{Z, B}}
55

ai with Z

and

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

Property for a change operation
the change is decisive
the change is restrictive
the change is questioning
the change is destructive
the change is expansive
the change is conservative
the change is altering

Characterization of the property
E =  or E = {{}} or |E| > 2
and |E | = 1 and E 6= {{}}
|E| > |E | > 2
|E| < |E |
E 6=  and E 6= {{}}
E =  or E = {{}}
|E| = |E | and

Ej  E , Ei  E, Ei  Ej
E = E
|E| = |E | and
Ei  E s.t. Ej  E , Ei 6 Ej

Table 1: Structural properties for a change operation
2. Under the grounded semantics, the change
since:
Z

A

B

C

B

Z

C

F

D

B

ai

with Z and Iz = {(Z, A)} is decisive

ai

with Z and Iz = {(Z, A), (B, Z)} is

E = {{A}, {B, D}},
E = {{Z, B, D}}

4. Under the preferred semantics, the change
decisive since:
A

with Z and Iz = {(Z, A)} is decisive

E = {{}},
E = {{Z, B}}

3. Under the preferred semantics, the change
since:
A

ai

E = {{A}, {B}},
E = {{B}} (note that Z is rejected)

Z

A weaker requirement is the decrease of the number of choices. A change such that G 
has strictly less extensions than G, but still has at least two, is called restrictive4 . Note
that the restrictive property does not make sense under the grounded semantics, since there
is always a unique grounded extension.
Definition 9 (Restrictive change) The change from G to G  is restrictive iff E = {E1 ,
. . . , En }, n  2, and E = {E1 , . . . , Ep }, with n > p  2.
Example 3
1. Under the preferred (or stable) semantics, the change
is restrictive since:
A

B

C

Z

F

D

ai

with Z and Iz = {(Z, A)}

E = {{A, C, F }, {A, D}, {B, D}, {B, F }},
E = {{Z, C, F }, {Z, B, D}, {Z, B, F }}

4. In the work of Cayrol et al. (2008), this kind of change was called selective.

56

fiChange in Argumentation Systems

2. Under the preferred semantics, the change
restrictive since:
A

B

C

Z

ai

with Z and Iz = {(Z, A), (B, Z)} is

E = {{A}, {B}, {C}},
E = {{B}, {C, Z}} (note that Z is not skeptically
accepted)

An opposite point of view enables to consider changes which raise ambiguity, by increasing the number of extensions. This is the case for instance when G has at least one
non-empty extension and G  has strictly more extensions than G. A slightly different situation occurs when G has no extension or an empty one, while G  has more than one extension.
In that case, change brings some information, but is not decisive. Such changes are called
questioning. As for the restrictive property, the questioning property does not make sense
under the grounded semantics.
Definition 10 (Questioning change) The change from G to G  is questioning iff E =
{E1 , . . . , Ep }, with p  2, and either E = , or E = {E1 , . . . , En } and p > n  1.
Example 4
1. Under the preferred (or stable) semantics, the change
is questioning since:
A

B

D

Z

C

F

B

Z

C

D

G

F

ai

A

D

B

C

with Z and Iz = {(Z, A)} is questioning

E = ,
E = {{Z, B, F }, {Z, B, G}}

3. Under the preferred semantics, the change
(Z, B), (B, Z)} is questioning since:
Z

with Z and Iz = {(Z, A)}

E = {{A, D, F }},
E = {{Z, B, C}, {Z, B, F }, {Z, D, C}, {Z, D, F }}

2. Under the stable semantics, the change
since:
A

ai

ai

with Z and Iz = {(Z, A), (A, Z),

E = {{A, D}, {B, D}},
E = {{A, D}, {B, D}, {Z}} (note that Z is not skeptically accepted)

Pursuing along the previous line, we consider changes leading to a kind of decisional
dead-end. This is the case when G has at least one non-empty extension and G  has no
extension, or an empty one5 . Such a change is called destructive.
Definition 11 (Destructive change) The change from G to G  is destructive iff E =
{E1 , . . . , En }, n  1, Ei 6= {} and E =  or E = {{}}.
Example 5
5. These are two different cases but they have the same impact: there is no possible decision because no
argument is accepted.

57

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

1. Under the stable semantics, the change
since:
A

B

D

H

Z

C

G

F

ai

with Z and Iz = {(Z, A)} is destructive

E = {{A, D, F }, {A, D, G}},
E = 
a

2. Under the preferred (or grounded) semantics, the change i with Z and Iz = {(Z, A),
(B, Z)} is destructive since:
B

A

E = {{A}},
E = {{}}

Z

a

3. Under the preferred semantics, the change i with Z and Iz = {(Z, A), (Z, B), (F, Z)}
is destructive since:
B

A

C

Z

F

D

E = {{A}, {B, D}},
E = {{}}

So far, the considered changes have an impact on the number of extensions. Now, we
are interested in changes which may modify the content of extensions, without modifying
the number of extensions. The most interesting situation occurs when each extension of G 
strictly includes one extension of G, the number of extensions being the same. Such changes
are called expansive.
Definition 12 (Expansive change) The change from G to G  is expansive iff G and G 
have the same number of extensions and each extension of G  strictly includes an extension
of G.
Example 6 Under the preferred (or stable) semantics, the change
{(B, Z)} is expansive since:
A

B

C

Z

D

ai

with Z and Iz =

E = {{A, C}, {A, D}},
E = {{Z, A, C}, {Z, A, D}}

In the particular case when the set of extensions remains unchanged, the change is called
conservative.
Definition 13 (Conservative change) The change from G to G  is conservative iff G and
G  have exactly the same extensions, that is E = E .
Example 7
1. Under the preferred semantics, the change
vative since:
A

B

C

Z

B

Z

with Z and Iz = {(B, Z)} is conser-

ai

with Z and Iz = {(A, Z)} is conser-

E = {{}},
E = {{}}

2. Under the preferred semantics, the change
vative since:
A

ai

C

E = {{A, C}},
E = {{A, C}}

58

fiChange in Argumentation Systems

3. Under the preferred semantics, the change
vative since:
A

B

C

Z

D

ai

with Z and Iz = {(A, Z)} is conser-

E = {{A, C}, {A, D}},
E = {{A, C}, {A, D}}

Otherwise, it may happen that G and G  have the same number of extensions but some
extensions (and sometimes all of them) are altered. This is called an altering change.
Definition 14 (Altering change) The change from G to G  is altering iff G and G  have
the same number of extensions and there exists at least one extension Ei of G such that Ej
extension of G  , Ei * Ej .
It is the case for instance when each extension of G  has a non-empty intersection with
(but does not include) an extension of G.
Example 8
1. Under the grounded semantics, the change
since:
A

B

Z

C

D

B

C

with Z and Iz = {(Z, A)} is altering

ai

with Z and Iz = {(Z, E), (F, Z)} is

E = {{A, D},
E = {{Z, B, D}}

2. Under the preferred semantics, the change
altering since:
A

ai

D

E

F

Z

E = {{A, C, E}},
E = {{A, C}} (note that Z is rejected)

The above discussion can be summarized on Table 2. In this table, it can be checked
that cells with #i correspond to situations which cannot occur:
#1 and #2 The only acceptability semantics in which an argumentation framework may
have no extension is the stable semantics. However, with the stable semantics, an
argumentation framework cannot have an empty extension when its set of arguments
is not empty. And, by assumption, the cases #1 and #2 correspond to argumentation
frameworks with non-empty sets of arguments (because by assumption either Iz 6= 
or there exists one interaction i = (X, Y ), so there is at least one X in G and this X
and eventually Z belong to G  ). So these cases do not occur for any change operation
and any acceptability semantics considered in this paper.
Note that the structural properties presented on Table 2 are mutually exclusive (that is
a change operation cannot satisfy two of them).
59

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

E =



E=

{{}}

{{}}

conservative
#2

#1
conservative

{E1 }

{E1 , . . . , Ep }
p2

decisive

questioning

conservative
expansive
altering

{E1 }

destructive
{E1 , . . . , En }
n2

decisive

questioning
n < p:
questioning
n > p:
restrictive
n = p:
conservative
expansive
altering

With Ei 6= {} and Ei 6= {}. Each cell of the table contains the name of the corresponding
property for a change operation.

Table 2: Structural properties of a change operation
3.3 Status-Based Properties
In this section, we are interested in the impact of a change operation on the status of some
particular arguments.
 First, we are interested in the status of the arguments which were accepted before
change. This leads to propose a property called Monotony, which can be defined
for any type of change.
 Another interesting issue concerns the status of the argument which is added in a
a
change. Obviously, it concerns only the change operation i ; This leads to propose a
property called Priority to Recency which only makes sense for one type of change.
3.3.1 Monotony
Inspired by what has been done in the field of non-monotonic inference, we define a property
of monotony for expressing that arguments accepted before change remain accepted after
change. Since our aim is to define general properties, we make no assumption about the
number of extensions, and we have to consider different cases for acceptance of an argument
(credulously or skeptically accepted).
A monotony definition is straightforward under a semantics providing only one extension
(such as the grounded semantics, for instance). Following Definition 6, an argument A is
accepted (credulously or skeptically) in hA, Ri iff it belongs to the (unique) extension of
G. So, in that particular case, monotony means that the extension of G is included in the
extension of G  . When there are several extensions, monotony can take different forms. A
credulous form corresponds to the case where each argument credulously accepted from G
60

fiChange in Argumentation Systems

is also credulously accepted from G  . A skeptical form corresponds to the case when each
argument skeptically accepted from G is also skeptically accepted from G  . So these ideas
lead to the following definition:
Definition 15 (Monotony)
 The change from G to G  satisfies Monotony iff each extension of G is included in at
least one extension of G  .
 The change from G to G  satisfies Credulous Monotony6 iff the union of the extensions
of G is included in the union of the extensions of G  .
 The change from G to G  satisfies Skeptical Monotony iff the intersection of the extensions of G is included in the intersection of the extensions of G  .
a

For the change operation i , Examples 2.1, 2.2, 4.3, 6, 7 illustrate the case when the
a
property of Monotony holds; and, again for the change operation i , Examples 2.3, 2.4,
3.1, 3.2, 4.1, 4.2, 5, 8.1, 8.2 illustrate the case when the property of Monotony does not
hold7 .
Obviously, Monotony implies Credulous Monotony. However, Monotony does not imply
Skeptical Monotony (see Example 4. 3) and Skeptical Monotony does not imply Monotony
(see Examples 2.3, 2.4, 3.1, 3.2). Under a semantics providing only one extension, the three
notions of Monotony coincide.
The Monotony property is defined at the level of extensions. A similar notion can be
defined at the level of arguments:
Definition 16 (Partial Monotony for an argument) Let X be an argument.
The change from G to G  satisfies Partial Monotony for X iff when X belongs to an extension
of G, it also belongs to at least one extension of G  .
It is easy to prove that Monotony (resp. Credulous Monotony) implies Partial Monotony
for each argument of G. It is not the case with the property of Skeptical Monotony (see the
argument A in Example 2.4).
3.3.2 Priority to Recency
The next property concerns the status of the argument which is added in a change. Inspired by what has been done in the field of belief revision (see Alchourron, Gardenfors, &
Makinson, 1985), and the postulate concerning the priority of the new piece of information,
we define a property for expressing that the new argument is accepted after change. This
a
property called Priority to Recency8 makes sense only for the change operation i .
6. Credulous Monotony is related to the well-known decision problem of credulous acceptance in argumentation (see Definition 6).
7. In Appendix B, the reader will find some examples illustrating the property of Monotony for the other
change operations.
8. This property is not a characteristic postulate in AGMs sense; it has just been inspired by the Success
postulate proposed by Alchourron et al. (1985).

61

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

a

Definition 17 (Priority to Recency) The change i from G to G  satisfies Priority to
Recency iff G  has at least one extension and the added argument Z belongs to each extension
of G  .
Examples 2.1 to 2.3, 3.1, 4.1, 4.2, 6, 8.1 are examples of change satisfying Priority to
Recency. Examples 2.4, 3.2, 4.3, 5, 7, 8.2 are examples of change that do not satisfy Priority
to Recency.
3.4 Connections between Properties
Some links between structural properties and status-based properties can be established.
The following propositions enumerate results that hold for any type of change.
Proposition 3
 A conservative change always satisfies Monotony and Skeptical Monotony.
 An expansive change always satisfies Monotony and Skeptical Monotony.
 A decisive change which satisfies Monotony also satisfies Skeptical Monotony.
 In the particular case of a semantics providing only one extension, a change satisfies
Monotony (and Skeptical Monotony) iff it is either decisive, or expansive, or conservative.
Proposition 4
 A destructive change never satisfies Monotony.
 An altering change never satisfies Monotony.
 A restrictive change never satisfies Monotony.
a

Moreover, in the particular case of the change i , other connections between structural
properties and Priority to Recency can be established.
Proposition 5
 A conservative change
 A destructive change

ai

ai

never satisfies Priority to Recency.

never satisfies Priority to Recency.

And in the particular case of grounded, stable and preferred semantics, we have:
Proposition 6 Under the grounded, stable and preferred semantics, an expansive change
ai always satisfies Priority to Recency.
From the above results and examples given in Sections 3.2 and 3.3, inclusion links
a
between different changes of the type i are synthesized on Figure 19 . Table 3 gives the
references of the examples and propositions used for identifying these links.
9. The inclusion of Expansive changes into the operations that satisfy Priority to Recency that is
shown in Figure 1, was checked only for the stable, grounded and preferred semantics  see Proposition 6
(hence, it may not hold for other semantics).

62

fiChange in Argumentation Systems

Monotony
Destructive
Conservative

Questioning
Expansive
Decisive

Priority to recency
Restrictive

Altering

Figure 1: Inclusion links between changes of the type

conservative
decisive
destructive
expansive
altering
questioning
restrictive

Priority to Recency
Never satisfied (Conseq. 5)
May hold (Ex. 2.1 to 2.3)
and not (Ex. 2.4)
Never satisfied (Conseq. 5)
Hold under stable, grounded, preferred sem. (Prop. 6)
May hold (Ex. 8.1)
and not (Ex. 8.2)
May hold (Ex. 4.1)
and not (Ex. 4.3)
May hold (Ex. 3.1)
and not (Ex. 3.2)

ai

Monotony
Always satisfied(Conseq. 3)
May hold (Ex. 2.1)
and not (Ex. 2.3)
Never satisfied (Conseq. 4)
Always (Conseq. 3)
Never (Conseq. 4)
May hold (Ex. 4.3)
and not (Ex. 4.1)
Never (Conseq. 4)

a

Table 3: Synthesis about connections between structural and status-based properties of i

63

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

4. Characterizing Argument Addition under Grounded or Preferred
Semantics
a

In this section, we focus on the change i , i.e., the addition of exactly one argument Z
that interacts with at least one argument belonging to A. Indeed, adding an argument
which may interact with the existing ones is a very frequently encountered type of change
in real-life situations. Besides, this type of change is sufficiently complex to provide a rich
analysis of properties and results.
a
Moreover, we consider the change i under the grounded and the preferred semantics.
We have chosen these two semantics because they are the most well-known traditional
semantics for which the existence of extensions is guaranteed.
Our purpose is to identify conditions under which a given property is satisfied for a
a
change operation i . These conditions concern the added argument and the associated
interactions, and may depend on the semantics.
Arguably, some properties seem more desirable than others according to the context.
For instance, a decisive change operation will reduce ignorance, since after the change one
and only one extension remains, enabling to determine the status of each argument (which
was not always the case before the change). An expansive change will raise the number of
accepted arguments, which is interesting for achieving a goal of persuasion for instance. A
conservative change keeps the extensions unchanged, and so is interesting if we want to add
an argument without changing our state of knowledge. The properties of Monotony and
Priority to Recency are desirable when we focus on some particular arguments, which we
want to get in the resulting extensions.
In constrast, a questioning or destructive operation will increase ignorance, which seems
to be less interesting.
An altering operation enforces to have a new look at the problem, since nothing is
kept from the state before change (the same number of extension remains but they are all
different from the previous ones). According to this discussion, we provide:
 sufficient conditions (CS) under which some interesting properties hold (e.g., decisive,
expansive, conservative, monotonic, satisfying Priority to Recency);
 necessary conditions (CN) for some undesirable properties (e.g., questioning, destructive, altering), in order to avoid those properties.
In the following subsections, we consider the change
ment Z and the interactions Iz , such that:

ai with the addition of the argu-

a

hA, Rii (Z, Iz ) = hA  {Z}, R  Iz i
4.1 Argument Addition under the Grounded Semantics
Under the grounded semantics, we have E = {E} and E = {E  }.
The following result gives a condition under which a given accepted argument X remains
a
accepted after the change i (hence Partial Monotony holds for X).
Proposition 7 Under the grounded semantics, if X belongs to E, and Z does not indirectly
a
attack X, then i satisfies Partial Monotony for X (i.e. X belongs to E  ).
64

fiChange in Argumentation Systems

Example 9 Under the grounded semantics:
C

B

Z

E = {{A, B}}, E = {{Z, B}}
Z does not indirectly attack B and B  E, so B  E 

A

ai

The following result gives a condition under which the change
Recency.

satisfies Priority to

Proposition 8 Under the grounded semantics, if Z is not attacked by G, then
Priority to Recency (i.e. Z belongs to E  ).

ai satisfies

Example 10 Under the grounded semantics:
A

B

Z

D

C

E = {{A, C}}, E = {{Z}}

Let us first study the particular case when E = {}.
Proposition 9 Under the grounded semantics,
 if E = {} then the following equivalence holds: E  = {} iff Z is attacked by G;
S
 moreover, if E = {} and Z is not attacked by G, then E  = {Z}  i1 F i ({Z}).
So, in case E = {}, we have:

ai is conservative).

 Either Z is attacked by G and then E  = {} (and the change

 Or Z is not attacked by G and then E  contains Z and all the arguments which are
a
indirectly defended by Z (and the change i is decisive).
As a consequence of Proposition 9, we have:
Corollary 1 Under the grounded semantics,
 if E = {} and Z is not attacked by G, then the change
 if the change

ai

ai

is decisive;

is decisive, then Z is not attacked by G and hence Z attacks G.

Example 11 Under the grounded semantics, the following change
A

B

D

C

Z

ai

is decisive:

E = {{}}, E = {{Z, A, D}}

Now, we study the particular case when E =
6 {}.
The following result gives a condition under which the change

ai satisfies Monotony.

Proposition 10 Under the grounded semantics, if E =
6 {} and Z does not attack E, then
ai satisfies Monotony (i.e. E  E  ).
And more precisely, we have two conditions (one for a conservative change
a
another one for an expansive change i ):
65

ai

and

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

Proposition 11 Under the grounded semantics, if E =
6 {} and Z does not attack E, we
have:
 if E does not defend Z, then E  = E. (The change

ai

is conservative).

S
 if E defends Z, then E  = E  {Z}  i1 F i ({Z}). Moreover, in that case, if Z does
a
not attack G, E  reduces to E  {Z}. (The change i is expansive).
Example 12 Under the grounded semantics, the following change
A

B

ai

is expansive:

C

E = {{A}}, E = {{Z, A, D}}
Z

F

D

As a consequence of Proposition 11, we have another condition under which the change

ai satisfies Priority to Recency:

Corollary 2 Under the grounded semantics, if E =
6 {}, Z does not attack E, and E defends
a
Z, then i satisfies Priority to Recency (i.e. Z belongs to E  ).
Example 13 Under the grounded semantics:
A

B

C

E = {{A}}, E = {{Z, A}}
Z

F

D

Note that Corollary 2 does not hold if E does not defend Z.
Example 14 Under the grounded semantics:
A

B

C

Z

F

D

E = E  = {A}.
So, E = E = {{A}}.

Another interesting point is the fact that some properties of the change
satisfied under the grounded semantics:
Proposition 12 Under the grounded semantics, a change
restrictive.

ai

ai cannot be

is never questioning, nor

a

The case of a destructive change i is also interesting because it is sufficient to add an
attack against each unattacked argument for obtaining such a change:
Proposition 13 Under the grounded semantics, if E =
6 {}, if Z attacks each unattacked
a
argument Ai of G and if Z is attacked in G  then the change i is destructive; the converse
also holds.
66

fiChange in Argumentation Systems

4.2 Argument Addition under the Preferred Semantics
Under the preferred semantics, there is always at least one extension. E may be reduced
to a singleton {E1 } (where E1 may be empty), or may contain more than one extension
{E1 , . . . , En }. Similarly, E may be reduced to a singleton {E1 } (where E1 may be empty),
or may contain more than one extension {E1 , . . . , En }.
a
The following result gives a condition under which the change i satisfies Priority to
Recency.
a

Proposition 14 Under the preferred semantics, if Z is not attacked by G, then i satisfies
Priority to Recency (i.e. Z belongs to each Ei ).
Example 15
A

B

Z

C

A

B

Under the preferred semantics:
E = {{A, C}, {B}},
E = {{Z, B}, {Z, C}}
C

Z

E = {{A, C}, {B}},
E = {{Z, A, C}}

The following proposition establishes that admissible sets of G can be kept in some cases
a
(so, in these cases the change i can be neither altering, nor restrictive):
Proposition 15 Under the preferred semantics,
 if Z does not attack Ei , then Ei remains admissible in G  ;
 if Z does not attack Ei and Ei defends Z, then Ei  {Z} is admissible in G  .
Example 16 Under the preferred semantics:
Z

A

B

C

E = {{}},
{}{Z} is admissible in G  but E = {{Z, B}}.

Example 12 (continued) Under the preferred semantics, E = {{A}}, {A}  {Z} is
admissible in G  , nevertheless, E = {{Z, A, D}}.
Note that other preferred extensions may appear in G  .
Example 17 Under the preferred semantics:
A

B

C

E = {{A}},
E = {{Z, A}, {Z, C}}

Z

As a consequence of Proposition 15, we have another condition under which the change

ai satisfies Priority to Recency.

Corollary 3 Under the preferred semantics, if Z attacks no extension of G, and if each Ei
a
defends Z, then i satisfies Priority to Recency (i.e. Z belongs to each Ei ).
67

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

Example 18 Under the preferred semantics:
A

B

C

F

Z

D

E = {{A, C}, {A, D}},
E = {{Z, A, C}, {Z, A, D}}

The following result gives a condition under which the change

ai is decisive.

Proposition 16 Under the preferred semantics, if E = {{}} and Z is not attacked by G
a
and there is no even-length cycle in G then E = {E  } and Z belongs to E  (so, i is
decisive).
Example 11 (continued) Under the preferred semantics, E = {{}}, E = {{Z, A, D}}
Note that, if even-length cycles exist in the graph, the change
extensions; this change would be a questioning one:
A

B

D

Z

C

F

ai

may induce several

E = {{}},
E = {{Z, A, D}, {Z, A, F }}

For this reason, we have considered graphs without even-length cycle in Proposition 16.
a
The following result gives a necessary condition for i to be a decisive change (and
also a condition for being a conservative change).
Proposition 17 Under the preferred semantics, if Z attacks no argument of G and E =
a
{{}}, then E = {{}}; or equivalently, if E = {{}} the change i by Z is decisive only if
Z attacks G.
The following result relates to the case where there exists a non empty extension in G
a
and also gives conditions for i either to be a conservative change, or to be an expansive
one.
Proposition 18 Under the preferred semantics, if Z attacks no argument of G, and E 6=
{{}}, then for each i:
 if Ei defends Z, then Ei  {Z} is an extension of G  ;
 if Ei does not defend Z, then Ei is an extension of G  ;
moreover, G and G  have the same number of extensions.
Example 6 (continued) Under the preferred semantics, the change
= {{A, C}, {A, D}} and E = {{Z, A, C}, {Z, A, D}}

ai

is expansive: E

As a consequence of the previous results, we have a condition under which the change

ai satisfies Monotony.

Proposition 19 Under the preferred semantics, if Z attacks no extension of G then the
a
change i satisfies Monotony.
68

fiChange in Argumentation Systems

In the particular case of a non controversial argumentation framework, we obtain a cona
dition under which the change i satisfies Skeptical Monotony. The notion of controversial
argument has been introduced by Dung, who has proved that an argumentation framework
without any controversial argument has nice properties. Roughly speaking, an argument X
is controversial if it indirectly attacks and indirectly defends a same argument Y .
Proposition 20 Under the preferred semantics, assume that G contains no controversial
a
argument. If Z does not attack i1 Ei , then the change i satisfies Skeptical Monotony,
that is i1 Ei  i1 Ei .
As under the grounded semantics, there exists a proposition about the destructive change

ai :

Proposition 21 Under the preferred semantics, if E 6= {{}}, if there is no even-length
cycle in G  , if each unattacked argument Ai of G is attacked in G  and if Z is attacked in G 
a
then the change i is destructive.
4.3 Synthesis of the Results
Tables 4 and 5, display a summary of necessary (CN) or sufficient (CS) conditions for a
a
property to hold for a change i (in some cases, several CS  resp. CN  may be given
denoted by CS, CS , . . .  resp. CN, CN , . . .).
In these tables, E, E , E, E  , Ei , Ej denote respectively the set of extensions before change,
after change, the grounded extension before change, after change, a preferred extension
before change and after change.
a
Table 4 concerns the structural properties for a change i .
a
Table 5 concerns the status-based properties for a change i .
These tables underline the fact that we have been able to identify sufficient conditions
(CS) under which some interesting properties hold (e.g., decisive, expansive, conservative,
monotonic, satisfying Priority to Recency). For the properties of changes that are less
desirable such as questioning, destructive, altering, we have focused our search on necessary
conditions (CN), allowing us to enunciate sufficient conditions in order to avoid them.

5. Discussion and Future Works
In this paper, we study change in argumentation. We propose properties to characterize
the impact of a change operation on the outcome of an argumentation framework. Then,
we focus on a particular type of change: the addition of a new argument that may interact
with previously introduced arguments10 . And we establish conditions under which a given
property is satisfied.
The study of change is an important issue in Artificial Intelligence, but it traditionally
concerns belief change. When an agent receives a new piece of information, she must
adapt her beliefs; this adaptation is not always easy because it may imply to drop some
previous knowledge. The seminal work of Alchourron, Gardenfors and Makinson (AGM)
(1985) has settled a formal framework for reasoning about belief change and introduced
10. We do not consider knowledge from which arguments and interactions could be built.

69

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

Properties of the change

ai

Grounded semantics

Preferred semantics

Decisive
(E =  or E = {{}} or |E| > 2)
and |E | = 1 and E 6= {{}}

CS and CN: E = {} and Z not
attacked. (Prop.9)

CS: E = {{}} and Z not attacked and no even-length cycle in G. (Prop.16)
if E = {{}} CN: Z attacks G.
(Prop.17)

Restrictive
|E| > |E | > 2

Never (Prop.12)

CN:  an even-length cycle in
G and Z attacks at least one Ei
(Prop.15)

Questioning
|E| < |E |

Never (Prop.12)

CN:  an even-length cycle in
G  and Z attacks G (Prop.17,
Prop.18)

CN and CS: E =
6 {} and Z attacks each unattacked argt in
G and Z is attacked (Prop.13)

CS: E 6= {{}} and Z is attacked and no even-length cycle in G  and Z attacks each
unattacked argt in G (Prop.21)
CN: E 6= {{}} and Z is attacked and  an odd-length cycle in G  and Z attacks each
unattacked argt in G (Prop.1.5,
Prop.2.2)

CS: E =
6 {} and Z does not
attack E and E defends Z
(Prop.11)

CS: E 6= {{}} and Z does not
attack G and i, Ei defends Z
(Prop.18)

CS: E = {} and Z attacked by
G (Prop.9)
CS : E =
6 {} and Z does not
attack E and E does not defend
Z (Prop.11)

CS: E = {{}} and Z does not
attack G (Prop.17)
CS : E 6= {{}} and Z does not
attack G and i, Ei does not defend Z (Prop.18)

CN: E =
6 {} and Z attacks E
(Prop.10)

CN: E 6= {{}} and Ei s.t. Z
attacks Ei (Prop.15)

Destructive
E 6=  and E 6= {{}} and (E =
 or E = {{}})

Expansive
|E| = |E | and Ej  E , Ei 
E, s.t. Ei  Ej
Conservative
E = E

Altering
|E| = |E | and Ei  E s.t. Ej 
E , Ei 6 Ej

Table 4: Synthesis of the necessary and sufficient conditions (CN and CS) for structural
a
properties  Case of i

70

fiChange in Argumentation Systems

a
Properties of the change i
Monotony
Ei  E, Ej  E , s.t. Ei  Ej

Grounded Semantics

Preferred Semantics

CS: E = {}

CS: Z does not attack any Ei
(Prop.19)

CS : E =
6 {} and Z does not
attack E (Prop.10)
Priority to Recency
|E |  1 and Ej  E , Z  Ej

Partial Monotony for X
If Ei  E s.t. X  Ei , then
Ej  E s.t. X  Ej
Skeptical Monotony
i1 Ei  j1 Ej

CS: Z not attacked (Prop.8)
CS : E =
6 {}, Z does not attack
E and E defends Z (Prop.11)

CS: Z not attacked (Prop.14),
CS : Ei  E, Z does not
attack Ei and Ei defends Z
(Corol.3)

CS: X  E and Z does not indirectly attack X (Prop.7)

cf. Monotony (because, X,
Partial Monotony for X is implied by Monotony)

cf. Monotony (because, for the
grounded semantics, Skeptical
Monotony is Monotony)

CS: no controversial argt in G
and Z does not attack i1 Ei
(Prop.20)

Table 5: Synthesis of the necessary and sufficient conditions (CN and CS) for status-based
a
properties  Case of i

the concept of belief revision together with two other types of belief change, namely
contraction and expansion. Expansion consists only in adding information without
checking its consistency with previous beliefs. Contraction is an operation designed for
removing information. Revision consists in adding information while preserving consistency.
This last operation is the most interesting one since, in belief theory, inconsistency leads to
unexploitable information.
Although the change operations defined in Section 3 could be thought of as being related
to the AGM theory11 , the comparison is not appropriate because of two main reasons:
 The basic underlying formalism is different: in standard belief revision, logical formulae are used for knowledge representation whereas, in this paper, an argumentation
framework represents the current knowledge. In the first case, the outcome is a new
set of logical formulae, whereas, in the second case, the outcome is a new argumentation framework which induces a new set of extensions, each extension being a set of
arguments.
 Revision is a task in knowledge representation which is strongly related to concepts
such as inference and consistency. The postulates for standard belief revision (AGM)
are built on the consistency notion, since revision aims at incorporating a new piece
11. Note that other important cognitive tasks linked to belief change theory have already been studied in
the field of argumentation, see for instance the work on merging of Coste-Marquis, Devred, Konieczny,
Lagasquie-Schiex, and Marquis (2007).

71

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

of information while preserving consistency. However, in the framework of argumentation, the notion of consistency has not a clear and standard accepted meaning (even
if some authors propose to take into account a kind of degree of inconsistency in
the argumentation context as in the works of Matt & Toni, 2008; Besnard & Hunter,
2008).
Moreover, revision has also been studied in the framework of non-monotonic theories (Witteveen & van der Hoek, 1997) and argumentation theory is linked to nonmonotony, but postulates for non-monotonic theories are also based on consistency
and inference notions that are not explicitly present in an abstract argumentation
system. So, these postulates are not suited for our problem. Some of the belief revision postulates can be restated (this is the case for the property called Priority to
Recency which has been inspired by the AGM Success postulate ), but other principles must be proposed (for instance, we have identified a property called Monotony
which checks a kind of preservation of the existing extensions by the change process).
Our work is an extension of a previous work (Cayrol et al., 2008) which presented a
preliminary step towards a formal characterization of the notion of change in argumentation
frameworks. In the work of Cayrol et al. (2008), a change was defined as the addition of
one argument and only one interaction and we studied only the structural properties and
Priority of Recency (called classicity by Cayrol et al., 2008). In the new version of this
work, proposed in the current paper, we are further taking into account the addition of
several interactions (so some properties given by Cayrol et al., 2008 do not hold here)
and defining new properties around the notion of Monotony. We also look further into
the connections between all the proposed properties and into the conditions (necessary or
sufficient) for obtaining or avoiding these properties.
There are many other approaches that deal with adding new pieces of information
within an argumentation system. The point of view adopted in this family of works is
different from ours because of the status of the new piece of information that is added. For
instance, Wassermann (1999), as well as Falappa, Garca, and Simari (2004) and Paglieri
and Castelfranchi (2005), define under which conditions, expressed in terms of arguments,
unjustified beliefs should become accepted. Pollock and Gilliess (2000) approach studies the
properties of knowledge revision under the argumentation point of view, i.e., the problem
is to generate a knowledge base in which each piece of information is justified by good
arguments. The same kind of problem is studied by Amgoud and Vesic (2009) in the context
of argument-based decision. Argument-based decision takes as input a set of options, a set of
arguments and a defeat relation among them, and returns a status for each option together
with a total preorder on the set of options. These authors study under which conditions an
option may change its status when a new argument is received and under which conditions
this new argument is useless.
Recently, Rotstein, Moguillansky, Falappa, Garca, and Simari (2008a) have proposed
a warrant-prioritized revision operation, which consists in adding an argument to a theory
in such a way that this argument is warranted afterwards. Even if the underlying ideas are
similar, this work differs from our approach in at least two points:
 First, in the work of Rotstein et al. (2008a), arguments are given a structure through
the sub-argument relation, and properties such as minimality, consistency and atom72

fiChange in Argumentation Systems

icity. And the definition of warranted arguments relies upon an evaluation of argumentation lines. In contrast, our approach remains at the most abstract level, and
our sets of accepted arguments are computed with the well-known extension-based
semantics.
 Secondly, the warrant-prioritized argument revision is designed in order to satisfy the
AGM Postulate, corresponding to our property of Priority to recency, since the
added argument must be warranted in the revised theory. Our work follows another
direction. We propose an extensive theoretical study of the impact of an addition
on the outcome of an abstract argumentation framework, which enables us to define
several properties for a change operation.
Concerning the more general question of handling dynamics in argumentation, our proposal can be related to recent works of Boella et al. (2009a, 2009b), and of Rotstein et al.
(2008b):
 The work of Boella et al. (2009a, 2009b) studies how the extensions of an argumentation system remain unchanged when the set of arguments or the attacks between
them are changed.
The four types of change we have proposed in Definition 7 have been introduced under
different names, respectively attack refinement, attack abstraction, argument refinement and argument abstraction. However, only the operations of attack refinement,
attack abstraction and argument abstraction have been studied, and from a more
restrictive point of view:
 Boella et al. (2009a, 2009b) consider only the case when the semantics provides
exactly one extension.
 The principles which are defined correspond to conditions under which a change
is conservative, in our terminology. No other property is considered.
As we focus on the addition of an argument and its interactions, the work of Boella
et al. (2009a, 2009b) can be viewed as complementary to our work.
 Rotstein et al. (2008b) introduce the notion of dynamics by considering arguments
built from evidence. Evidence is used to determine whether an argument is active (i.e.
can be used to draw inferences) or inactive. The question addressed by Rotstein et al.
(2008b) is: How a variation of the set of evidence affects the nature of arguments
(active or not)?. This question cannot be handled at a pure abstract level and
concerns internal dynamics. By contrast, we remain at an abstract level: we are
interested by the impact of a change of the abstract framework on the outcome of
that framework.
A promising application of our work could be to design dialogue strategies. Indeed, a
dialogue may be defined as an exchange (called move) of arguments between two or more,
human or artificial, agents under a given protocol. The protocol is a program that defines
the set of allowed moves at each step of the dialogue. Each agent has its own aim and
73

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

may develop its own strategy. Most of the works about dialogue strategies consider that
a strategy selects exactly one move (the move which must be done next). For instance,
Bench-Capon (1998) proposes a selection strategy (for agents) leading to more cooperative
dialogues. Other approaches study strategies in the context of persuasion dialogues, where
two agents argue in order to persuade each other that a given initial argument is true (or
false according to the agent opinion). In that case, a strategy helps to choose which argument must be defeated in order that the initial argument should be accepted (or rejected).
Amgoud and Maudet (2002) have proposed heuristics that select the less attackable arguments in a persuasion dialogue. In a similar way, Riveret, Prakken, Rotolo, and Sartor
(2008) have proposed an optimal strategy in order to win a debate based on the probability
of success of the argument and on the cost of this argument for the agent. Hunter (2004),
with a more global approach, has defined a strategy which builds an optimal subtree of
arguments maximizing the resonance with the agent goals and minimizing their cost.
Our approach takes another point of view. We do not define any protocol and we do not
restrict to a dialogue type. Given a set of arguments which may interact, we are interested
in the outcome of the argumentation system, that is the set of extensions under a given
semantics. In other words, we study the impact of the addition of an argument with respect
to two points of view: first, the structural modification induced on the set of extensions,
second, the impact on the acceptability of arguments. Although the concern of acceptability
evolution looks similar to the aim of the existing dialogue approaches presented above, our
proposal is more general, since in our work, we are not interested in finding strategies in
order to make accepted a precise argument but we are rather interested in establishing
general conditions for the preservation of acceptability. For instance, under the grounded
and preferred semantics, we provide a sufficient condition for maintaining an argument
accepted after the arrival of a new one (Monotony property) and a sufficient condition for
a new argument to be accepted (Priority to Recency).
The structural point of view of our analysis is completely original with respect to the
existing literature. Indeed, we analyze the impact of a new argument on the set of extensions
and this amounts to consider the addition of an argument as an operation performed in order
to modify the form of the change outcome (by doing an expansive change, or a decisive
change for instance). The work reported in this paper enables us to choose the right way of
changing (which argument must be affected by the change, with which kind of interaction)
in order to obtain the new outcome. This is why we plan to focus more on strategies for
directing a dialogue (i.e., to be integrated in a protocol) than on strategies for taking part
in it (i.e., concerning an agent). For instance, if a dialogue arbitrator wants the debate to
be more open then she should rather force the next speaker to use arguments appropriate
for an expansive change. If she wants the debate to be more focused then only arguments
appropriate for a restrictive (and even decisive) change should be accepted.
There are several directions of further research:

1. We plan to study the other change operations defined in this paper, corresponding to
the removal of one argument with its interactions and to the addition or the removal
of an interaction (for instance, by exploiting properties of symmetry between all the
change operations).
74

fiChange in Argumentation Systems

2. We would like to generalize our change operations to the case of the addition or the
removal of a subgraph of arguments (which would be a kind of iterated change).
3. We think that the decisive property is a desirable property for a change operation.
So, we intend to investigate the question How to make the minimal change12 to a
given argumentation framework so that it has a unique non-empty extension?.

Acknowledgments
We would like to thank the reviewers for their help and their very interesting suggestions.

Appendix A. Proofs
Lemma 1
 If X  G s.t. (Z, X)  Iz , the change operation

ai

introduces no new cycle in G  .

 If X  G s.t. (X, Z)  Iz , the change operation

ai

introduces no new cycle in G  .

In other words, if Z does not attack any argument of G, or if Z is not attacked by G,
a
the change operation i introduces no new cycle in G  .
Proof of Lemma 1: If follows directly from the fact that only one argument is added.



Proofs Related to Section 3.4 (Connections between Properties)
Proof of Proposition 3: It follows directly from the definitions of the properties (Definitions 8,
12, 13, 15).

Proof of Proposition 4: It follows directly from the definitions of the properties (Definitions 11,
9, 14, 15).

Proof of Proposition 5: It follows directly from the definitions of the properties (Definitions 11,
13, 17).

Proof of Proposition 6:
Grounded semantics: Let us show that if E ( E  then Z  E  . Assume that E ( E  and that
Z 6 E  . We are going to prove that E   E (which is in contradiction with the assumption
E ( E  ), by proving that F i ({})  F i ({}), by induction on i  1.
 Basic case (i = 1): Z 6 E  so Z is attacked by G. Thus, if X  F  ({}) then X is
in G and by definition X is unattacked in G  . Then X is also unattacked in G. So,
F  ({})  F({}).
 Induction hypothesis (for 1  i  p, F i ({})  F i ({})):
12. In terms of number of edges to add or to remove and/or in terms of number of arguments to add or to
remove.

75

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

 Let us first show that for any subset of arguments S in G such that F  (S)  G, we
have F  (S)  F(S): Let X  F  (S), it means that S defends X in G  and X  G.
If there exists Y in G which attacks X in G, then Y also attacks X in G  . And as
S defends X in G  , then S attacks Y . So S also defends X in G. So F  (S)  F(S).
 Let us compute F p+1 ({}) = F  (F p ({})). By induction hypothesis, F p ({}) 
F p ({}). As F  is monotonic, we have F p+1 ({})  F  (F p ({})). Now let S denote
the set F p ({}), S  E so S  G. We have assumed that E ( E  and Z 6 E 
so S  E   G. Then, as F  is monotonic, we have F  (S)  F  (E  ) = E   G
Due to the previous point, we can conclude that F  (S)  F(S). Then, we obtain
F p+1 ({})  F(F p ({})) = F p+1 ({}).
Preferred semantics: Given an expansive change of G by Z and Iz , let us suppose that there
exists an extension Ej of G  that does not contain Z. Then this extension is included in G.
As the change is expansive, there exists an extension Ei of G strictly included in Ej . Ei is a
maximal admissible set for inclusion. Since the inclusion of Ei inside Ej is strict, therefore
Ej is not admissible in G. Ej being an extension of G  , it has no conflict, hence Ej does not
defend each of its elements in G. It exists X  Ej that is attacked by Y in G (and thus in G  )
and that is not defended by Ej in G. This means that Ej does not attack Y . But, since Ej is
included in G it can not attack Y in G  . If there is no edge between an element of Ej and Y in
G, there is neither such an edge in G  . (Note that Y can be attacked by Z but Z is not in Ej )
Stable semantics: Assume that there exists an extension Ej of G  that does not contain Z. As
the change is expansive, there exists an extension Ei of G strictly included in Ej . Since the
inclusion is strict, there exists Y in Ej , which does not belong to Ei . And as we have assumed
that Ej does not contain Z, Y is in G. Ei is a stable extension of G, so Ei attacks Y . Then,
as Ei is included in Ej there is a conflict in Ej , which contradicts the fact that Ej is a stable
extension.


Proofs Related to Section 4.1 (Under the Grounded Semantics)
Proof of Proposition 7: E is the grounded extension of G. Due to the fact that R is finite, we
have E = i1 F i ({}). We prove by induction on i  1 that if X belongs to F i ({}) and Z does not
indirectly attack X, then X belongs to F i ({}).
 Basic case (i = 1): If X  F({}) then X is not attacked in G. Since Z does not attack X,
then X remains unattacked in G  and so belongs to F  ({}).
 Induction hypothesis (for 1  i  p, the proposition holds): Let X  F p+1 ({}). We have to
prove that X  F p+1 ({})(= F  (F p ({}))). Assume that X is attacked by Y in G  . As Z does
not attack X, Y is in G. As X  F p+1 ({}) = F(F p ({})), F p ({}) defends X by attacking
Y . So there exists W  F p ({}) which attacks Y , which in turn attacks X. As Z does not
indirectly attack X, we are sure that Z does not indirectly attack W . Using the induction
hypothesis for W , we have W  F p ({}). So, we have proved that F p ({}) defends X in G 
and so X  F p+1 ({}).

Proof of Proposition 8: If Z is not attacked by G, then Z is not attacked in G  . So, due to
Proposition 1.5, the grounded extension of G  contains Z.

Proof of Proposition 9:
76

fiChange in Argumentation Systems

 If E = {} then each argument of G is attacked. If Z is attacked by G, then each argument of
G  is attacked and due to Proposition 1.5 and Proposition 1.6, E  = {}. If Z is not attacked
by G, Z is not attacked in G  , then Z belongs to E  , which is not empty.
 If E = {} and Z is not attacked by G, Z  E  . As F  is monotonic,
and E  is a fixed point
S

i

of F , we have F ({Z})  E for each i  1, and then {Z}  i1 F i ({Z})  E  . Let S
S
denote {Z}  i1 F i ({Z}). Now, we have to prove that E   S. As E  is the least fixed


point of F  , it is sufficient to
S proveithat S is a fixed point of F . Obviously, F (S) = {X 

G s.t. X is not attacked}  i1 F ({Z}). Since E = {}, {X  G s.t. X is not attacked } =
S
{Z}, so F  (S) = S and S is a fixed point of F  . We have proved that E  = {Z} i1 F i ({Z}).

Proof of Corollary 1:
 It follows directly from Proposition 9. Due to Definition 8, under the grounded semantics, the
change is decisive when E = {} and E  6= {}.
 And as Z interacts with G, if Z is not attacked by G, Z must attack G.

Proof of Proposition 10: Due to the fact that R is finite, we have E = i1 F i ({}) and E  =
i1 F i ({}). We prove by induction on i  1 that F i ({})  F i ({}).
 Basic case (i = 1): If Y  F({}) then Y is not attacked in G and due to the fact that Z does
not attack E, Y is not attacked in G  and Y  F  ({}).
 Induction hypothesis (for 1  i  p, F i ({})  F i ({})): let S = F p ({}) and S  = F p ({}).
First, we prove that F(S)  F  (S). Let Y  F(S). Obviously, F(S)  E. So Y  E and
Z does not attack Y since Z does not attack E. So, if Y is attacked by A in G  then Y is
attacked by A in G. As Y  F(S), S defends Y by attacking A. And so S defends Y in G  ,
that is Y  F  (S).
Using the induction hypothesis, we have S  S  . Moreover, by definition F  is monotonic. So
F(S) = F p+1 ({})  F  (S)  F  (S  ) = F p+1 ({}). So, E  E  .

Proof of Proposition 11: E =
6 {} and Z does not attack E. Let us first notice that (1) If
Y  F  (E) and Y  G, then Y  F(E) = E. Indeed, Y  F  (E) means that E defends Y in G  . So,
if Y  G, E also defends Y in G, i.e., Y  F(E) = E.
 Due to Proposition 10, we have E  E  . So, we just have to prove that if E does not defend
Z, then E   E. Indeed, we will prove that F  (E) = E. Then, by definition of E  (least fixed
point), it will follow that E   E. Let Y  F  (E), as E does not defend Z, hence Y  G,
according to (1), Y  F(E) = E. Conversely, let Y  E = F(E), and let A be an argument
that attacks Y in G  . As Z does not attack E, A 6= Z, so A  G, and E defends Y by attacking
A. So, E defends Y in G  and Y  F  (E).
 First, we prove that if E defends Z then F  (E) = E {Z}. Due to (1), if Y  F  (E) and Y  G,
then Y  F(E) = E. Now, if E defends Z, we have also Z  F  (E). So, F  (E)  E  {Z}.
Conversely, let Y  F(E) = E. E defends Y in G. As Z does not attack E, Z does not attack
Y , and then E also defends Y in G  , so Y  F  (E). And Z  F  (E), so E  {Z}  F  (E).
In the particular case when Z does not attack G, Z cannot defend any argument. So, F  (E 
{Z}) = F  (E) and then F  (E  {Z}) = E  {Z}. That means that E  {Z} is a fixed point of
77

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

F  , and by definition of E  , we have E   E  {Z}. Due to Proposition 10, we have E  E  .
So, we have also E  {Z}  E  . Finally, E  reduces to E  {Z}.
S
In the general case, Z attacks G. Let S denote E  {Z}  i1 F i ({Z}). We will prove that
S
E  = S. Obviously, we have S  E  since E  {Z} = F  (E)  E  , and E  contains i1 F i (E  ),
S
hence contains i1 F i ({Z}), since F  is monotonic. Conversely, we will prove that S is a
fixed point of F  and by definition of E  (least fixed point), it will follow that E   S.
Since F  is monotonic, we have F  (E)  F  (S), F  ({Z})  F  (S) and for each i  2
F i ({Z})  F  (S). So, as F  (E) = E  {Z}, we have S  F  (S). Conversely, let Y  F  (S)
and assume that Y 
/ E  {Z} = F  (E). Then, there exists an
S attacker A of Y and A is not
attacked by E. As Y  F  (S), S must attack A. So {Z}  i1 F i ({Z}) attacks A, which
S
means that Y  i1 F i ({Z}). So, we have proved that if Y  F  (S), either Y  E  {Z} or
S
Y  i1 F i ({Z}), that is Y  S.

Proof of Corollary 2: It is a direct consequence of Proposition 11.



Proof of Proposition 12: It is a direct consequence of the definitions: the restrictive and questioning changes need a number of extensions strictly greater than one, and there exists only one
grounded extension.

Proof of Proposition 13: E =
6 {}, so there are unattacked arguments denoted by Ai in G. Ai ,

Ai is attacked in G and Z is attacked in G  . So there is no unattacked argument in G  , so i  1
F i ({}) = {} and E  = {}. So the change is destructive.
Conversely, if the change is destructive, by definition we have E =
6 {} and E  = {}. Then, due to

Proposition 1.5, there is no unattacked argument in G . So, Z is attacked and each Ai (unattacked
argument in G) is also attacked in G  .


Proofs Related to Section 4.2 (Under the Preferred Semantics)
Proof of Proposition 14: If Z is not attacked by G, then Z is not attacked in G  . So, due to
Proposition 1, each preferred extension of G  contains Z.

Proof of Proposition 15:
 Ei is conflict-free in G, so also in G  . Let A  Ei being attacked in G  . As Z does not attack
Ei , A is attacked in G and as Ei is admissible in G, Ei defends A. So, Ei remains admissible in
G.
 Z does not attack Ei , and Ei defends Z, so Ei does not attack Z and then Ei  {Z} is conflictfree in G  . Let A  Ei  {Z} being attacked in G  . Either A  Ei and as we have just proved
in the first item that Ei is admissible in G  , Ei defends A. Or A = Z, and we have assumed
that Ei defends Z. In each case, Ei  {Z} is admissible in G  .


Lemma 2 If Ei is an extension of G  not containing Z, then Ei is admissible in G.
Proof of Lemma 2:
78

fiChange in Argumentation Systems

As Ei does not contain Z, Ei  G. Ei is conflict-free in G  so Ei is also conflict-free in G. Let
Y  Ei being attacked by an argument A, A  G. As Ei is admissible, it defends Y . So, there is an
argument B  Ei attacking A. As Ei  G, B  G. So, we have proved that Ei is admissible in G.

Proof of Corollary 3: From Proposition 15, Ei  E, Ei  {Z} is admissible in G  . So, there exists
j  1 such that Ei  {Z}  Ej . From Lemma 2, if Ek is an extension of G  not containing Z, then Ek
is admissible in G. So, there exists i  1 such that Ek  Ei . So, we have Ek  Ei  Ei  {Z}  Ej . As
a consequence, there would be a strict inclusion between two extensions of G  , which is impossible.
So, there cannot exist Ek extension of G  not containing Z, and so each extension of G  contains Z.

Proof of Proposition 16: If Z is not attacked by G, then Z is not attacked in G  and Z belongs
to each preferred extension. Moreover, if there is no even-length cycle in G, due to Lemma 1, there
is no even-length cycle in G  . So, due to Proposition 2.5, G  has only one preferred extension which
is not empty (it contains at least Z).


Lemma 3 If Z attacks no argument of G and Ei is a non empty extension of G  , then
Ei \ {Z} is admissible in G.
Proof of Lemma 3:
 Ei is conflict-free in G  so Ei \ {Z} is also conflict-free in G  and in G.
 Let Y  Ei \ {Z}. Assume that there is an argument A attacking Y . Then A 6= Z since Z
attacks no argument of G. Ei is a non-empty preferred extension of G  , so there is an argument
B  Ei attacking A, and B 6= Z (always because Z attacks no argument of G). So, we have
B  Ei \ {Z}, and Ei \ {Z} defends Y . So, Ei \ {Z} is admissible in G.

Proof of Proposition 17: Suppose that Z attacks no argument of G and E = {{}}.
(reductio ad absurdum): Assume that there exists a non-empty extension of G  denoted by E  .
So there exists Y such that Y  E  . Either Y = Z, or Y  G. In both cases, Y is attacked, because
all arguments of G are attacked (since E = {{}}) and Z attacks no argument of G. So E  must
defend Y . If Y = Z, E  cannot be reduced to Y (because Z attacks no argument and cannot defend
itself). So E  \ {Z} 6= {}. If Y 6= Z, Y  E  \ {Z}, and E  \ {Z} =
6 {}. Due to Lemma 3, E  \ {Z} is
admissible in G and so E  \ {Z}  E with E being a preferred extension of G. So G has a non-empty
extension, which is in contradiction with the assumption.

Proof of Proposition 18:
 Z attacks no argument of G, so due to Proposition 15, i, Ei is admissible in G  . So there
exists a preferred extension Ej of G  including Ei . E 6= {{}}, so i, Ei 6= {} and so Ej 6= {}.
But Z 6 Ei , so Ei  Ej \ {Z}. Due to Lemma 3, Ej \ {Z} is admissible in G, so there exists
k  1 such that Ei  Ej \ {Z}  Ek . Using the definition of a preferred extension (-maximal
among the admissible sets), we can conclude that Ei = Ej \ {Z} = Ek . So, either Ej = Ei (if
Z 6 Ej ), or Ej = Ei  {Z} (if Z  Ej ). In the first case, Ei is an extension of G  . In the second
case, Ei  {Z} is an extension of G  . Moreover, if Z  Ej , Ej defends Z (which is attacked by
G, since it does not attack G) and as Z attacks no argument, Ei = Ej \ {Z} defends Z. So, if
79

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

Ei does not defend Z, Ei is an extension of G  . On the other hand, if Ei defends Z, Ei  {Z} is
conflict-free in G  . So, Ei  {Z} is admissible in G  and it is the case when Z  Ej and Ei  {Z}
is an extension of G  .
 Now, we prove that G and G  have the same number of extensions. From the first part of
the proof, we know that each extension of G is included in an extension of G  . Moreover, two
distinct extensions of G cannot be included in a same extension of G  . Indeed, the union of
two non-empty preferred extensions defends all its elements and strictly contains each of these
extensions. So the union of two extensions cannot be conflict-free.
So,we know that G  has at least as many extensions as G and that G  has at least one nonempty extension. So, Ej  E , Ej 6= {}. Due to Lemma 3, Ej \ {Z} is admissible in G. So,
for each Ej , there exists Ei , an extension of G such that Ej \ {Z}  Ei . From the first part of
the proof, we have:
 Either Ei defends Z, and Ei  {Z} is an extension of G  . As Ej \ {Z}  Ei , we have
Ej  Ei  {Z}, and as Ej is maximal admissible in G  , Ej = Ei  {Z}.
 Or Ei does not defend Z, Ei is an extension of G  . As Ej is maximal admissible in G  , we
have Z 6 Ej and Ej = Ej \ {Z} = Ei .
So, G and G  have the same number of extensions.

Proof of Proposition 19:
 If E = {{}}, obviously each change satisfies Monotony.
 If G has a non-empty extension, Proposition 15 can be applied. So each extension of G
remains admissible in G  and is included in a preferred extension of G  . So, the change satisfies
Monotony.

Proof of Proposition 20: Let E = i1 Ei and E  = i1 Ei .
Let Eg (resp. Eg ) denote the grounded extension of G (resp G  ). Due to Proposition 1.4, we know
that Eg  E and Eg  E  . Dung (1995) has proved that when there is no controversial argument,
the grounded extension is exactly the intersection of the preferred extensions. So, if G contains no
controversial argument, we have Eg = E.
Now, if Z does not attack i1 Ei , Z does not attack Eg , so due to Proposition 10, if E =
6 {}
then we have Eg  Eg when E = {} then i1 Ei = {} and the inclusion trivially holds. So, we have

E = Eg  Eg  E  , and then i1 Ei  i1 Ei .
Proof of Proposition 21: E 6= {{}} and there is no even-length cycle in G  so there is no evenlength cycle in G; as a consequence, according to Proposition 2.5 there is only one extension E in G;
moreover, E =
6 {}. Since there is no even-length cycle in G  , we know that there is only one extension


E in G . Assume that Z and each unattacked argument Ai in G are attacked in G  ; so there is no
unattacked argument in G  .
Assume that E  6= {}. Let X  E  . X is attacked in G  . Let Y1 denote an attacker of X. As E 
is admissible, E  defends X. So E  contains X2 which attacks Y1 . As there is no even-length cycle
in G  , we know that X2 6= X. And X2 is not unattacked.
So we are able to built an infinite sequence of distinct arguments:
X is attacked by Y1 attacked by X2 . . . Yp attacked by Xp+1 attacked by Yp+1 . . .
The Xi s (resp. Yi s) are distinct due to the absence of even-length cycles in G  .
It contradicts the assumption that A is finite. So E  = {} and the change is destructive.
80



fiChange in Argumentation Systems

Appendix B. Illustration of Properties for the Other Change Operations
The following examples illustrate the structural properties and the property of Monotony for
a
change operations distinct from i (let us recall that the property of Priority to Recency
does not make sense for these other change operations).
For

ai
a

a

First, we notice that if hA, Rii (Z, Iz ) = hA , R i then hA , R ii Z = hA, Ri. So in
a
each example of Section 3.2, a change i can also be illustrated.
 Example 4.1 show a decisive change
property of Monotony.

ai and a change ai which does not satisfy the
a

a

Example 5.2 shows a decisive change i and a change i which satisfies the property
of Monotony .
 Example 4.3 shows a restrictive change
the property of Monotony

ai and a change ai which does not satisfy

 Examples 2.3, 3.1, 5.1 show a questioning change
not satisfy the property of Monotony

ai

and a change

ai

which does

 Examples 2.1, 2.2, 4.2 show a destructive change
not satisfy the property of Monotony

ai

and a change

ai

which does

 Example 8.2 shows an expansive change
property of Monotony

ai

and a change

ai

which satisfies the
a

a

 Examples 7.1, 7.2, 7.3 show a conservative change i and a change i which satisfies
the property of Monotony
a

a

 Examples 6, 8.1 show an altering change i and a change i which does not satisfy
the property of Monotony
For

i

and

i

 With hA = {A, B, C}, R = {(A, B), (B, C), (C, A)}i, hA, Rii (A, C) is a decisive
change (before the change E = {{}}, after the change E = {{A}}); and the inverse
operation hA, R  {(A, C)}ii (A, C) is destructive.
In this example,

i satisfies the property of Monotony and i does not.

 With hA = {A, B, C}, R = {(A, B), (B, C)}i, hA, Rii (C, A) is a destructive change
(before the change E = {{A, C}}, after the change E = {{}}); and the inverse
operation hA, R  {(C, A)}ii (C, A) is decisive.
In this example,

i does not satisfy the property of Monotony and i satisfies it.

 With hA = {A, B, C}, R = {(A, B), (B, C)}i, hA, Rii (A, C) is an altering change
(before the change E = {{A, C}}, after the change E = {{A}}); and the inverse
operation hA, R  {(A, C)}ii (A, C) is expansive.
81

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

In this example,

i does not satisfy the property of Monotony and i satisfies it.

 With hA = {A, B, C}, R = {(A, B)}i, hA, Rii (C, B) is a conservative change (before the change E = {{A, C}}, after the change E = {{A, C}}); and the inverse
operation hA, R  {(A, C)}ii (A, C) is conservative.
In this example,

i and i satisfy the property of Monotony.

 With hA = {A, B, C, D}, R = {(A, B), (B, A), (B, C), (D, C)}i, hA, Rii (C, D) is
a questioning change (before the change E = {{A, D}, {B, D}}, after the change
E = {{A, D}, {B, D}, {A, C}}); and the inverse operation hA, R{(C, D)}ii (C, D)
is restrictive.
In this example,

i satisfies the property of Monotony and i does not.

 With hA = {A, B, C, D}, R = {(A, B), (B, A), (B, C), (D, C), (C, D)}i, the change
hA, Rii (A, D) is a restrictive one (before the change E = {{A, D}, {B, D}, {A, C}},
after the change E = {{B, D}, {A, C}}).
The inverse operation hA, R  {(A, D)}ii (A, D) is questioning.
In this example,

i does not satisfy the property of Monotony and i satisfies it.

 With hA = {A, B, C}, R = {(A, B), (B, C)}i, hA, Rii (A, B) is an altering change
(before the change E = {{A, C}}, after the change E = {{A, B}}).
In this example,

i does not satisfy the property of Monotony.

 With hA = {A, B, C, D}, R = {(A, B), (B, C), (C, A)}i, hA, Rii (D, A) is a expansive change (before the change E = {{D}}, after the change E = {{D, B}}).
In this example,

i satisfies the property of Monotony.

References
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). On the logic of theory change:
partial meet contraction and revision functions. Journal of Symbolic Logic, 50, 510
530.
Amgoud, L., & Cayrol, C. (2002). Inferring from inconsistency in preference-based argumentation frameworks. Journal of Automated Reasoning, 29, 125169.
Amgoud, L., & Maudet, N. (2002). Strategical considerations for argumentative agents
(preliminary report). In Proc. of NMR, pp. 409417.
Amgoud, L., Maudet, N., & Parsons, S. (2000). Modelling dialogues using argumentation.
In Proc. of ICMAS, pp. 3138.
Amgoud, L., & Vesic, S. (2009). On Revising Argumentation-Based Decision Systems. In
Proc. of ECSQARU, Vol. LNAI 5590, pp. 7182. Springer-Verlag.
Baroni, P., Giacomin, M., & Guida, G. (2005). Scc-recursiveness: a general schema for
argumentation semantics. Artifical Intelligence, 168, 162210.
Bench-Capon, T. (1998). Specification and implementation of Toulmin dialogue game. In
Proc. of JURIX, pp. 520.
82

fiChange in Argumentation Systems

Besnard, P., & Hunter, A. (2008). Elements of argumentation. MIT Press.
Boella, G., Kaci, S., & van der Torre, L. (2009a). Dynamics in argumentation with single
extensions: Abstraction principles and the grounded extension. In Proc. of ECSQARU
(LNAI 5590), pp. 107118.
Boella, G., Kaci, S., & van der Torre, L. (2009b). Dynamics in argumentation with single
extensions: Attack refinement and the grounded extension. In Proc. of AAMAS, pp.
12131214.
Bondarenko, A., Dung, P., Kowalski, R., & Toni, F. (1997). An abstract, argumentationtheoretic approach to default reasoning. Artificial Intelligence, 93, 63101.
Caminada, M. (2006). Semi-stable semantics. In Proc. of COMMA, pp. 121128.
Cayrol, C., Dupin de Saint-Cyr, F., & Lagasquie-Schiex, M. (2008). Revision of an argumentation system. In Proc. of KR 2008, pp. 124134. AAAI Press.
Chesnevar, C., Maguitman, A., & Loui, R. (2000). Logical models of argument. ACM
Computing surveys, 32 (4), 337383.
Coste-Marquis, S., Devred, C., Konieczny, S., Lagasquie-Schiex, M., & Marquis, P. (2007).
On the merging of Dungs argumentation systems. Artificial Intelligence, Argumentation in Artificial Intelligence, 171 (10-15), 730753.
Coste-Marquis, S., Devred, C., & Marquis, P. (2005). Prudent semantics for argumentation
frameworks. In Proc. of ICTAI, pp. 568572.
Dung, P. M. (1995). On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artificial Intelligence,
77, 321357.
Dung, P. M., Mancarella, P., & Toni, F. (2006). A dialectic procedure for sceptical
assumption-based argumentation. In Proc. of COMMA, pp. 145156.
Dunne, P., & Bench-Capon, T. (2001). Complexity and combinatorial properties of argument systems. Tech. report, U.L.C.S.
Dunne, P., & Bench-Capon, T. (2002). Coherence in finite argument system. Artificial
Intelligence, 141 (1-2), 187203.
Falappa, M., Garca, A., & Simari, G. (2004). Belief dynamics and defeasible argumentation
in rational agents. In Proc. of NMR, pp. 164170.
Hunter, A. (2004). Making argumentation more believable. In Proc. of AAAI, pp. 269274.
Kakas, A. C., & Moratis, P. (2003). Argumentation based decision making for autonomous
agents. In Proc. of AAMAS, pp. 883890.
Matt, P., & Toni, F. (2008). A game-theoretic measure of argument strength for abstract
argumentation. In Proc. of JELIA (LNAI 5293), pp. 285297.
Nute, D. (2003). Defeasible logic. In Proc. of INAP 2001, LNAI 2543, pp. 151169.
Paglieri, F., & Castelfranchi, C. (2005). Revising beliefs through arguments: Bridging the
gap between argumentation and belief revision in MAS. In Argumentation in MultiAgent Systems, pp. 7894. Springer.
83

fiCayrol, Dupin de Saint-Cyr & Lagasquie-Schiex

Pollock, J., & Gillies, A. (2000). Belief revision and epistemology. Synthese, 122 (1-2),
6992.
Pollock, J. L. (1992). How to reason defeasibly. Artificial Intelligence, 57, 142.
Prakken, H., & Vreeswijk, G. (2002). Logics for defeasible argumentation. In Handbook of
Philosophical Logic, Vol. 4, pp. 218319. Kluwer Academic.
Riveret, R., Prakken, H., Rotolo, A., & Sartor, G. (2008). Heuristics in argumentation: a
game-theoretical investigation. In Proc. of COMMA, pp. 324335.
Rotstein, N. D., Moguillansky, M. O., Falappa, M. A., Garca, A. J., & Simari, G. R. (2008a).
Argument theory change: revision upon warrant. In Proc. of COMMA, pp. 336347.
IOS Press.
Rotstein, N. D., Moguillansky, M. O., Garca, A. J., & Simari, G. R. (2008b). An abstract
argumentation framework for handling dynamics. In Proc. of NMR, pp. 131139.
Wassermann, R. (1999). Full acceptance through argumentation - A preliminary report. In
Proc. of IJCAI Workshop Practical Reasoning and Rationality.
Witteveen, C., & van der Hoek, W. (1997). A general framework for revising nonmonotonic
theories. In Proc. of LPNMR (LNAI 1265), pp. 258272. Springer.

84

fi