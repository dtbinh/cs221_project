Journal of Artificial Intelligence Research 17 (2002) 451-499

Submitted 12/00; published 12/02

Policy Recognition in the Abstract Hidden Markov Model
buihh@cs.curtin.edu.au
svetha@cs.curtin.edu.au
geoff@cs.curtin.edu.au

Hung H. Bui
Svetha Venkatesh
Geoff West
Department of Computer Science
Curtin University of Technology
PO Box U1987, Perth, WA 6001, Australia

Abstract

In this paper, we present a method for recognising an agent's behaviour in dynamic,
noisy, uncertain domains, and across multiple levels of abstraction. We term this problem
and view it generally as probabilistic inference on
the stochastic process representing the execution of the agent's plan. Our contributions in
this paper are twofold. In terms of probabilistic inference, we introduce the Abstract Hidden
Markov Model (AHMM), a novel type of stochastic processes, provide its dynamic Bayesian
network (DBN) structure and analyse the properties of this network. We then describe
an application of the Rao-Blackwellised Particle Filter to the AHMM which allows us to
construct an ecient, hybrid inference method for this model. In terms of plan recognition,
we propose a novel plan recognition framework based on the AHMM as the plan execution
model. The Rao-Blackwellised hybrid inference for AHMM can take advantage of the
independence properties inherent in a model of plan execution, leading to an algorithm for
online probabilistic plan recognition that scales well with the number of levels in the plan
hierarchy. This illustrates that while stochastic models for plan execution can be complex,
they exhibit special structures which, if exploited, can lead to ecient plan recognition
algorithms. We demonstrate the usefulness of the AHMM framework via a behaviour
recognition system in a complex spatial environment using distributed video surveillance
data.
on-line plan recognition under uncertainty

1. Introduction

Plan recognition is the problem of inferring an actor's plan by watching the actor's actions
and their effects. Often, the actor's behaviour follows a hierarchical plan structure. Thus,
in plan recognition, the observer needs to infer about the actor's plans and sub-plans at
different levels of abstraction in its plan hierarchy. The problem is complicated by the two
sources of uncertainty inherent in the actor's planning process: (1) the stochastic aspect of
plan refinement (a plan can be non-deterministically refined into different sub-plans), and
(2) the stochastic outcomes of actions (the same action can non-deterministically result in
different outcomes). Furthermore, the observer has to deal with a third source of uncertainty
arising from the noise and inaccuracy in its own observation about the actor's plan. In
addition, we would like our observer to be able to perform the plan recognition task \online" while the observations about the actor's plan are streaming in. We refer to this general
problem as on-line plan recognition under uncertainty.
c 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBui, Venkatesh & West
The seminal work in plan recognition (Kautz & Allen, 1986) considers a plan hierarchy,
but does not deal with the uncertainty aspects of the problem. As a result, the approach
can only postulate a set of possible plans for the actor, but is unable to determine which
plan is more probable. Since then, the important role of uncertainty reasoning in plan
recognition has been recognised (Charniak & Goldman, 1993; Bauer, 1994; van Beek, 1996),
and Bayesian probability has been argued as the appropriate model (Charniak & Goldman,
1993; van Beek, 1996). The dynamic, \on-line" aspect of plan recognition has only been
recently considered (Pynadath & Wellman, 1995, 2000; Goldman, Geib, & Miller, 1999;
Huber, Durfee, & Wellman, 1994; Albrecht, Zukerman, & Nicholson, 1998). All of this
recent work shares the view that online plan recognition is largely a problem of probabilistic
inference in a stochastic process that models the execution of the actor's plan. While this
view offers a general and coherent framework for modelling different sources of uncertainty,
the stochastic process that we need to deal with can become quite complex, especially if we
consider a large plan hierarchy. Thus, the main issue here is the computational complexity
for dealing with this type of stochastic processes, and whether the complexity is scalable to
more complex plan hierarchies.
1.1 Aim and Significance

In this paper, we demonstrate that the type of plan recognition problems described above
scales reasonably well with respect to the number of levels of abstraction in the plan hierarchy. This is in contrast to the common-sense analysis that more levels in the plan
hierarchy would introduce more variables to the stochastic process, which in turn, results
in exponential complexity w.r.t the number of levels in the hierarchy.
In order to achieve this, we first assume a general stochastic model of plan execution
that can model the three sources of uncertainty involved. The model for planning with
a hierarchy of abstraction under uncertainty has been developed recently by the abstract
probabilistic planning community (Sutton, Precup, & Singh, 1999; Parr & Russell, 1997;
Forestier & Varaiya, 1978; Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Dean
& Lin, 1995). To our advantage, we adopt their basic model, known as the abstract Markov
policies (AMP) 1 as our model for plan execution. The AMP is an extension of a policy
in Markov Decision Processes (MDP) that enables an abstract policy to invoke other more
refined policies and so on down the policy hierarchy. Thus, the AMP is similar to a contingent plan that prescribes which sub-plan should be invoked at each applicable state of the
world to achieve its intended goal, except that it can represent both the uncertainty in the
plan refinement and in the outcomes of actions. Since an AMP can be described simply in
terms of a state space and a Markov policy that selects among a set of other AMP's, using
the AMP as the model for plan execution also helps us focus on the structure of the policy
hierarchy.
The execution of an AMP leads to a special stochastic process which we called the
Abstract Markov Model (AMM). The noisy observation about the environment state (e.g.,
the effects of action) can then be modelled by making the state \hidden", similar to the
hidden state in the Hidden Markov Models (Rabiner, 1989). The result is an interesting and
novel stochastic process which we term the Abstract Hidden Markov Model. Intuitively, the
1. Also known as options, policies

, or supervisor's

of Abstract Markov Decision Processes

452

policies

.

fiPolicy recognition in the Abstract Hidden Markov Model
AHMM models how an AMP causes the adoption of other policies and actions at different
levels of abstraction, which in turn generate a sequence of states and observations. In the
plan recognition task, an observer is given an AHMM corresponding to the actor's plan
hierarchy, and is asked to infer about the current policy being executed by the actor at all
levels of the hierarchy, taking into account the sequence of observations currently available.
This amounts to reversing the direction of causality in the AHMM, i.e. to determine a set
of policies that can explain the sequence of observations at hand. We shall refer to this
problem as policy recognition.
Viewing the AHMM as a type of dynamic Bayesian network (Dean & Kanazawa, 1989;
Nicholson & Brady, 1992), it is known that the complexity of this kind of inferencing in the
DBN depends on the size of the representation of the so-called belief state, the conditional
joint distribution of the variables in the DBN at time t given the observation sequence up
to t (Boyen & Koller, 1998). Thus we can ask the following question: how does the policy
hierarchy affect the size of the belief state representation of the corresponding AHMM?
Generally, for a policy hierarchy with K levels, the belief state would have at least
K variables and thus the size of their joint distribution would be O(exp(K )). However,
the AHMM has a specific network structure that exhibits certain conditional independence
properties among its variables which can be exploited for eciency. We first identify these
useful independence properties in the AHMM and show that there is a compact representation of the special belief state in the case where the state sequence can be correctly observed
(full observability assumption) and the starting and ending time of each policy is known.
Consequently, policy recognition in this case can be performed very eciently by updating
the AHMM compact belief state. This partial result, although too restricted to be useful by
itself, leads to an important observation about the general belief state: although it cannot
be represented compactly, it can be approximated eciently by a collection of compact special belief states. This makes the inference problem in the AHMM particularly amenable
to a technique called Rao-Blackwellisation (Casella & Robert, 1996) which allows us to
construct hybrid inference methods that combine both exact inference and approximate
sampling-based inference for greater eciency. The application of Rao-Blackwellisation to
the AHMM structure reduces the sampling space that we need to approximate to a space
with fixed dimension that does not depend on K , ensuring that the hybrid inference algorithm scales well w.r.t K .
The contributions of the paper are thus twofold. In terms of stochastic processes and
dynamic Bayesian networks, we introduce the AHMM, a novel type of stochastic processes,
provide its DBN structure and analyse the properties of this network. We present an application of the Rao-Blackwellised Particle Filter to the AHMM which results in an ecient
hybrid inference method for this stochastic model. In terms of plan recognition, we propose
a novel plan recognition framework based on probabilistic inference using the AHMM as the
plan execution model. The complexity of the inference problem is addressed by applying
a range of recently developed techniques in probabilistic reasoning to the plan recognition
problem. Our work illustrates that while the stochastic models for plan execution can be
complex, they exhibit certain special structures that can be exploited to construct ecient
plan recognition algorithms.

453

fiBui, Venkatesh & West
1.2 Structure of the Paper

The main body of the paper is organised as follows. Section 2 introduces the background
material in dynamic Bayesian networks and probabilistic inference. Section 3 formally defines the abstract Markov policy and the policy hierarchy. Section 4 presents the AHMM,
its DBN representation and conditional independence properties. The algorithms for policy recognition are discussed in Section 5, first for the special tractable case and then for
the general case. Section 6 presents our experimental results with the AHMM framework,
including a real-time system for recognising people behaviour in a complex spatial environment using distributed video surveillance data. Section 7 provides a comparative review of
related work in probabilistic plan recognition. Finally, we conclude and discuss directions
for further research in Section 8.
2. Background in Probabilistic Inference

The aim of this section is to familiarise readers with some concepts in probabilistic inference
that will be used later on in the paper. In subsections 2.1 and 2.2, we discuss Bayesian
Networks (BN) and Dynamic Bayesian Networks (DBN) in general. In subsection 2.3,
we discuss the Sequential Importance Sampling (SIS) algorithm, a general approximate
sampling-based inference method for dynamic models. Subsections 2.4 and 2.5 introduce
Rao-Blackwellisation, a technique for improving sampling-based methods by utilising certain
special structures of the dynamic model. Later on, Rao-Blackwellisation will be used as our
key computational technique for performing policy recognition.
2.1 Bayesian Networks

The Bayesian network (BN) (Pearl, 1988; Jensen, 1996; Castillo, Gutierrez, & Hadi, 1997)
(also known as probabilistic network or belief network) is a well-established framework for
dealing with uncertainty. It provides a graphical and compact representation of the joint
probability distribution of a set of domain variables X1; : : : Xn in the form of a directed
acyclic graph (DAG) whose nodes correspond to the domain variables. For each node
Xi , the links from the parent nodes P a(Xi ) are parameterised by the conditional probability of that node given the parents Pr(Xi j P a(Xi )). The network structure together
with the parameters
encode a factorisation of the joint probability distribution (JPD)
Q
n
Pr(X1 ; : : : Xn) = i=1 Pr(Xi j P ai). Given a Bayesian network, conditional independence
statements of the form X ? Y j Z (X is independent of Y given Z, where X; Y; Z are variables or sets of variables) can be asserted if X is d-separated from Y by Z in the network
structure, where d-separation is a graph separation concept for DAGs (Pearl, 1988). The
network structure of a BN thus captures certain conditional independence properties among
the domain variables which can be exploited for ecient inference.
The main inference task on a Bayesian network is to calculate the conditional probability
of a set of variables given the values of another set of variables (the evidence). There are
two types of computation techniques for doing this. Exact inference algorithms (Lauritzen
& Spiegelhalter, 1988; Jensen, Lauritzen, & Olesen, 1990; D'Ambrosio, 1993) compute
the exact value of the conditional probability required based on analytical transformation
that exploits the conditional independence relationships of the variables in the network.

454

fiPolicy recognition in the Abstract Hidden Markov Model
Approximative inference algorithms (Pearl, 1987; York, 1992; Henrion, 1988; Fung & Chang,
1989; Shachter & Peot, 1989) compute only an approximation of the required probability,
usually obtained either through \forward" sampling (Henrion, 1988; Fung & Chang, 1989;
Shachter & Peot, 1989) (a variance of Bayesian Importance Sampling (Geweke, 1989)), or
through Gibbs (Monte-Carlo Markov-Chain) sampling (Pearl, 1987; York, 1992). These
algorithms have the advantages of simple implementation, can be applied to all types of
network, and can trade off the accuracy in the estimates for computation resources. It is
known that exact inference in BN is NP-hard with respect to the network size (Cooper,
1990), while approximate inference, although scales well with the network size, is NP-hard
with respect to the hard-bound accuracy of the estimates (Dagum & Luby, 1993). In the
light of these theoretical results, approximate inference can be useful in large networks when
exact computation is intractable, but a certain degree of error in the probability estimate
can be tolerated by the application.
2.2 Dynamic Bayesian Networks

To model the temporal dynamics of the environment, the Dynamic Bayesian Network
(DBN) (Dean & Kanazawa, 1989; Nicholson & Brady, 1992; Dagum, Galper, & Horvitz,
1992) is a special Bayesian network architecture for representing the evolution of the domain variables over time. A DBN consists of a sequence of time-slices where each time-slice
contains a set of variables representing the state of the environment at the current time.
A time-slice is in itself a Bayesian network, with the same network structure replicated at
each time-slice. The temporal dynamics of the environment is encoded via the network links
from one time-slice to the next. In addition, each time-slice can contain observation nodes
which model the (possibly noisy) observation about the current state of the environment.
Given a DBN and a sequence of observations, we might want to draw predictions
about the future state variables (predicting), or about the unobserved variables in the
past (smoothing) (Kjaerulff, 1992). This problem can be solved using an inference algorithm for Bayesian networks described above. However, if we want to revise the prediction
as the observations arrive over time, reapplying the inference algorithm each time the observation sequence changes could be costly, especially as the sequence grows. To avoid this,
we need to keep the joint distribution of all the variables in the current time-slice, given
the observation sequence up to date. This probability distribution is termed the belief state
(also known as the filtering distribution) and plays an important role in inferencing in the
DBN. All existing inference schemes for the DBN involve maintaining and updating the
belief state (i.e., filtering). When a new observation is received, the current belief state is
rolled over one time-slice ahead following the evolution model, then conditioned on the new
observation to obtain the updated belief state.
An obvious problem with this approach is the size of the belief state that we need to
maintain. It has been noted that while the interaction of the variables in the DBN is
localised, the variables in the belief state can be highly connected (Boyen & Koller, 1998).
This is because the marginalisation of the past time-slices usually destroys the conditional
independence of the current time-slice. When the size of the belief state is large, exact
inference methods like (Kjrulff, 1995) is intractable, and it becomes necessary to maintain
only an approximation of the actual belief state, either in the form of an approximate

455

fiBui, Venkatesh & West
distribution that can be represented compactly (Boyen & Koller, 1998), or in the form of
a set of weighted samples as in the Sequential Monte-Carlo Sampling methods (Doucet,
Godsill, & Andrieu, 2000b; Kanazawa, Koller, & Russell, 1995; Liu & Chen, 1998).
The most simple case of the DBN where, in each time-slice, there is only a single state
variable and an observation node, is the well-known Hidden Markov Model (HMM) (Rabiner, 1989). Filtering in this simple structure can be solved using dynamic programming in the discrete HMM (Rabiner, 1989), or Kalman filtering in the linear Gaussian
model (Kalman, 1960). More recently, extensions of the HMM with multiple hidden interacting chains such as the Coupled Hidden Markov Models (CHMM) and the Factorial
Hidden Markov Models (FHMM) have been proposed (Brand, 1997; Ghahramani & Jordan,
1997; Jordan, Ghahramani, & Saul, 1997). In these models, the size of the belief state is
exponential in the number of hidden chains. Therefore, the inference and parameter estimation problems become intractable if the number of hidden chains is large. For this reason,
approximate techniques are required. CHMM (Brand, 1997) employs a deterministic approximation that approximates full dynamic programming by keeping only a fixed number
of \heads" with highest probabilities. The \heads" are thus chosen deterministically rather
than randomly as in sampling-based methods. FHMM (Ghahramani & Jordan, 1997; Jordan et al., 1997) uses variational approximation (Jordan, Ghahramani, Jaakkola, & Saul,
1999) which approximates the full FHMM structure by a sparsified tractable structure. This
idea is similar to the structured approximation method in (Boyen & Koller, 1998).
Our AHMM can be viewed as a type of Coupled/Factorial HMM since the AHMM
also consists of a number of interacting chains. However the type of interaction in our
AHMM is different from the other types of interaction that have been considered (Brand,
1997; Jordan et al., 1997; Ghahramani & Jordan, 1997). This is because the main focus
of the AHMM is the dynamics of temporal abstraction among the chains, rather than the
correlation between them at the same time interval. In addition, each node in the AHMM
has a specific meaning (policy, state, or policy termination status), and the links have a
clear causal interpretation based on the policy selection and persistence model. This is in
contrast to the Coupled/Factorial HMM where the nodes and links usually do not have
any clear semantic/causal interpretation. The advantage is that prior knowledge about the
temporal decomposition of an abstract process can be incorporated in the AHMM more
naturally.
2.3 Sequential Importance Sampling (SIS)

Sequential Importance Sampling (SIS) (Doucet et al., 2000b; Liu & Chen, 1998), also
known as Particle Filter (PF), is a general Monte-Carlo approximation scheme for dynamic
stochastic models. In principle, the SIS method is the same as the so-called Bayesian
Importance Sampling (BIS) estimator
R in the static case (Geweke, 1989). Suppose that we
want to estimate the quantity f = f (x)p(x)dx, i.e., the mean of f (x) where x is a random
variable with density p. Note that if f is taken as the identity function of an event A
then f is simply Pr(A). Let q(x) be an arbitrary2 density function, termed the importance
distribution. Usually, the importance distribution q is chosen so that is it easy to obtain
2. For the weight to be properly defined, the support of q has to be a subset of the support of p.

456

fiPolicy recognition in the Abstract Hidden Markov Model
random samples from it. The expectation under estimation can then be rewritten as:
R [f (x)p(x)=q(x)]q(x)dx Eq f (x)p(x)=q(x)
f = R
[p(x)=q(x)]q(x)dx = Eq p(x)=q(x)
From this expression, the BIS estimator w.r.t q can be obtained:
f  f^BIS =

1

N

PNi=1 f (x(i))w_ (x(i) ) X
N
= f (x(i))w~(x(i) )
1 PN w_ (x(i) )
i=1
i=1

N

where fx(i) g are the N i.i.d samplesPtaken from q(x), w_ (x) = p(x)=q(x) and w~ is the
normalised weight w~(x(i) ) = w_ (x(i) )= i w_ (x(i) ). Note that the normalised weight can be
computed from any weight function w(x) / w_ (x), i.e., the weight function need only be
computed up to a normalising constant factor. R
In the dynamic case, we want to estimate f = x~t f (~xt)p(~xt jo~t ) where x~t = (x0 ; : : : ; xt )
and o~t = (o0; : : : ; ot ) are two sequences of random variables; ot represents the observation
available to us at time t. Often, (~xt ) is a Markov sequence and ot is the observation of xt
as in a HMM. In a DBN, xt corresponds to the set of state variables and ot corresponds to
the set of observations at time-slice t. The SIS method presented here however applies to
the most general case where (~xt ) can be non-Markov, and ot not only depends on xt.
We now can introduce the importance distribution q(~xt jo~t ) to obtain the estimator:
f  f^SIS

=

N
X
f (~
x(i) )w~ (~x(i) )
i=1

t

t

(1)

To ensure that we can obtain sample from q(~xtjo~t ) \online", i.e., to sample a new value
xt for the sequence x~t when the current observation ot arrives, q must be restricted to the
form:
q(~xt jo~t ) = q(~
xt 1 jo~t 1 )q(xt jx~t 1 ; o~t )
With this restriction on q, we can use the weight function w(~xt ) = p(~xt ; o~t )=q(~xt jo~t ) so that
the weight can also be updated \online" using:
w(~xt ) = w(~
xt 1 )p(xt ; ot jx~t 1 ; o~t 1 )=q(xt jx~t 1 ; o~t )
(2)
Let wt = w(~xt )=w(~xt 1 ) be the weight updating factor at time t, and qt = q(xt jx~t 1 ; o~t )
be the sampling distribution used at time t. From (2) we have
wt qt = p(xt ; ot jx~t 1 ; o~t 1 )
(3)
which means that p(xt ; ot jx~t 1 ; o~t 1 ) is factorised into two parts: wt and qt . By choosing different factorisations, we obtain different forms for qt and thus different important
distributions q. For example, when (~xt ; o~t ) is a HMM, qt can be chosen as p(xt jxt 1 )
with wt = p(ot jxt ) as in the likelihood weighting (LW) method, or qt can be chosen as
p(xt jxt 1 ; ot ) with wt = p(ot jxt 1 ) as in the likelihood weighting with evidence reversal (LW-ER) (Kanazawa et al., 1995). In general, the \forward" qt can be chosen as
p(xt jx~t 1 ; o~t 1 ) with the corresponding weight wt = p(ot jx~t ; o~t 1 ). The \optimal" qt , in

457

fiBui, Venkatesh & West
the sense discussed in (Doucet et al., 2000b), is chosen as qt = p(xtjx~t 1 ; o~t ) with the
associating wt = p(ot jx~t 1 ; o~t 1 ).
The general SIS approximation scheme is thus as follows. At time t 1, we maintain N
sample sequences fx~(ti)1 g and the N corresponding weight values fw(i) g. When the current
observation ot arrives, each sequence x~(ti)1 is lengthened by a new value x(ti) sampled from
the distribution q(xt jx~(ti)1 ; o~t ). The weight value for x~(ti) is then updated using (2). Once
the new samples and the new weights are obtained, the expectation of any functional f can
be estimated using (1). This procedure can be furthered enhanced with a re-sampling step
and a Markov-chain sampling step (see Doucet et al. (2000b), Doucet, de Freitas, Murphy,
and Russell (2000a)). We do not describe these important improvements of the SIS here.3
2.4 Rao-Blackwellisation

Rao-Blackwellisation is a general technique for improving the accuracy of sampling methods
by analytically marginalising some variables and only sampling the remainder (Casella &
Robert, 1996). In its simplest form, consider the problem of estimating the expectation
E f (x), where x is a joint product of two
P variables r; z. Using direct Monte-Carlo sampling, we obtain the estimator: f^ = N1 N1 f (r(i); z(i) ). Alternatively, a Rao-Blackwellised
estimator can be derived by sampling only the variable r, with the other variable z being
integrated out analytically:
N
X
E f (r; z) = Er h(r)  f^RB = N1 h(r(i) )

1

where h(r) = Ez [f (r; z)jr]. For our convenience, r will be referred to as the Rao-Blackwellising
variable.
The Rao-Blackwellised estimator f^RB is generally more accurate than f^ for the same
number of samples N . This is a direct consequence of the Rao-Blackwell theorem which
gives the relationship between unconditional and conditional variance:
VAR X = VAR[E[X jY ]] + E[VAR[X jY ]]
When applying to the problem of estimating E f (r; z), we have:
VAR f (r; z) = VAR[E[f (r; z)jr]] + E[VAR[f (r; z)jr]]
and thus VAR f (r; z)  VAR[E[f (r; z)jr]] = VAR h(r). This suggests that for direct MonteCarlo sampling, the error of RB-sampling (sample only r and marginalise z) is always
smaller than the error of sampling both r and z for the same number of samples, except in
the degenerated case. For Bayesian Importance Sampling, using the variance convergence
result from (Geweke, 1989), one can also easily prove that as the number of samples tend to
infinity, the RB-BIS would generally do better than BIS for the same number of samples.
3. Note that these improvements can be used orthogonal to the Rao-Blackwellisation procedure discussed
subsequently. Our implementation of the policy recognition algorithm in the later sections does include
a re-sampling step, which is crucial for keeping the error of SIS over time under control.

458

fiPolicy recognition in the Abstract Hidden Markov Model
2.5 SIS with Rao-Blackwellisation (RB-SIS)

Since SIS is a form of BIS, Rao-Blackwellisation can also be used to improve its performance (Liu & Chen, 1998; Doucet
et al., 2000b). Let us consider again the problem of
R

estimating the expectation f = f (~xt)p(~xt jo~t ), where each variable xt is the joint product
of two variables (zt ; rt ). We shall restrict ourselves to the case where x~t is Markov and
ot is an observation of xt , i.e., when (~
xt ; o~t ) can be represented by a DBN. In addition,
we only consider f that depends only on the current variable xt , i.e., f is an expectation
over the filtering distribution p(xt jo~t ). For example, if A is a \future" event, i.e., an event
that Rdepends on fxt0 jt0  tg, we can estimate p(Ajo~t ) by letting f (xt) = p(Ajxt ) so that
f = xt p(Ajxt )p(xt jo~t ) = p(Ajo~t ).
R f (zt; rt)p(ztjr~t; o~t),
Applying Rao-Blackwellisation
to
this
setting,
we
can
let
h
(~
r
)
=
t
zt
R
so that f = h = r~t h(~rt )p(~rt jo~t ). Thus, if we use SIS to estimate h , we also obtain an
estimator for f:
N
f  f^RBSIS = h^ SIS = X h(~rt(i) )w~(~rt(i) )
(4)
i=1

The benefit of doing this is the increase in the accuracy of the estimator, as we now
only need to sample the variables r~t . The down side is that for each sample r~t , we need
to compute h(~rt ) using some exact inference method. Furthermore, the SIS procedure to
estimate h might require some additional complexity since the sequence r~t is generally nonMarkov, and ot no longer depends only on rt . Overall, in comparison with the normal SIS
estimator f^SIS (Eq. 1), for the same number of samples N , f^RBSIS is more accurate but is
also more computationally demanding to compute.
To see more clearly what is involved in implementing the RB-SIS method, let us look
at the Rao-Blackwellised belief state, i.e., the belief state of the dynamic process when the
Rao-Blackwellising variables can be observed: Rt = p(zt ; rt ; ot jr~t 1 ; o~t 1 ) and its posterior
Rt+ = p(zt jr~t ; o~t ). All the entities needed in the RB-SIS procedure can be computed from
these two distributions. Indeed, the functional h can be rewritten in terms of Rt+ as:
h(~rt ) =

Z

zt

f (zt ; rt )p(zt jr~t ; o~t ) =

Z

zt

f (zt ; rt )Rt+ (zt )

In addition, while performing SIS to estimate h , from Eq. (3), the weight
sampling distribution qt can be computed from Rt :
wt qt = p(rt ; ot jr~t 1 ; o~t 1 ) = Rt (rt ; ot ) =

Z

zt

Rt (zt ; rt ; ot )

(5)
wt

and the
(6)

Thus, computing the RB belief state Rt and its posterior Rt+ is an essential step in
the RB-SIS method. Since we have to maintain an RB belief state for each sample of
the RB variables r~t, it is crucial that this can be done eciently using an exact inference
method. If xt is composed of many variables, as in the case of a DBN, our choice of the
Rao-Blackwellising variables should be so that the Rao-Blackwellised belief state can be
maintained in a tractable way. Hence, Rao-Blackwellisation is especially useful when the
set of variables in a DBN can be split into two parts such that conditioning on the first part
makes the structure of the second part tractable and amenable to exact inference.

459

fiBui, Venkatesh & West
Begin
For t = 0; 1; : : :
For each sample i = 1; : : : ; N
Sample rti from Rti (rt jot )
Update weight w i = w i Rti (ot)
Compute the posterior RB bel state Rti = Rti (ztjrti ; ot)
Compute the new RB belief state Rti from Rti
Compute h i from Rti
Compute the estimator f^RBSIS = PNi h i w~ i
End
( )

( )

( )

( )

( )

( )
+

( )

( )

( )
+1

( )
+

=1

( )

( )

( )
+

( )

Figure 1: RB-SIS for general DBN
The general RB-SIS algorithm is given in Fig. 1. For illustrating purpose, we assume
that the \optimal" qt and the corresponding wt are being used (qt = Rt (rt jot ) and wt =
Rt (ot )). At each time point, we need to maintain N samples r~t(i) , i = 1; : : : ; N . For each
sample, in addition to the sample weight w(i) , we also need to store a representation of the
RB belief state corresponding to that sample sequence: R(ti) = p(rt; zt ; ot jr~t(i)1 ; o~t 1 ) and
R(t+i) = p(zt jr~t(i) ; o~t ).
A number of applications of the RB-SIS method (also known as the Rao-Blackwellised
Particle Filter (RBPF)) have been discussed in the literature. A general framework for
using RB-SIS to do inference on DBNs has been presented by Doucet et al. (2000a), Murphy
(2000), Murphy and Russell (2001). However, these authors have mainly focused on the
case where the sequence of the Rao-Blackwellising variables (~rt ) is Markov (for example,
when the RB variables are the root nodes at each time slice). This assumption simplifies
the sampling step in the RB procedure since obtaining the sample for the RB variable
at time t + 1 is straightforward. In our previous work (Bui, Venkatesh, & West, 2000),
we introduced a hybrid-inference method for the AHMM in the special case of the statespace decomposition policy hierarchy, which is essentially an RB-SIS method. Note that
when applied to AHMMs, the sequence of Rao-Blackwellising variables that we use does not
satisfy the Markov property. In this case, care must be taken to design an ecient sampling
step, especially when the sampling distribution for the next RB variable does not have a
tractable form. The use of non-Markov RB variables also appears in other special models
such as the Bayesian missing data model (Liu & Chen, 1998), and the partially observed
Gaussian state space model (Andrieu & Doucet, 2000) where the RB belief state can be
maintained by a Kalman filter.
Since we have to make the Rao-Blackwellised belief state tractable, the context variables in the framework of context-specific independence (Boutilier, Friedman, Goldszmidt,
& Koller, 1996) can be used conveniently as Rao-Blackwellising variables (Murphy, 2000).
Indeed, since the context variable acts as a mixing gate for different Bayesian network structures, conditioning on these variables would simplify the structure of the remaining vari-

460

fiPolicy recognition in the Abstract Hidden Markov Model
ables. Because of this property of the context variables, Boutilier et al. (1996) have suggested
to use them as the cut-set variables in the cut-set conditioning inference method (Pearl,
1988). The cut-set variables play a similar role to the Rao-Blackwellising variables in which
they help to simplify the structure of the remaining network. In Rao-Blackwellised sampling, instead of summing over all the possible values of the cut-set variables which can be
intractable, only a number of representative sampled values are used.
The idea of combining both exact and approximate inference in RB sampling is also
similar to the hybrid inference scheme described by Dawid, Kjrulff, and Lauritzen (1995),
however it's unclear if RB sampling can be described using their model of communicating
belief universe. Also, Dawid et al. use hybrid inference mainly to do inference on networks
with a mixture of continuous and discrete variables, as opposed to RB whose goal is to
improve the sampling performance.
3. Abstract Markov Policies

In this section, we formally introduce the AMP concept as originating from the literature
of abstract probabilistic planning with MDPs (Sutton et al., 1999; Parr & Russell, 1997;
Forestier & Varaiya, 1978; Hauskrecht et al., 1998; Dean & Lin, 1995). The main motivation
in abstract probabilistic planning is to scale up MDP-based planning to problems with large
state space. It has been noted that a hierarchical organisation of policies can help reduce
the complexity of MDP-based planning, similar to the role played by the plan hierarchy
in classical planning (Sacerdoti, 1974). In comparison with a classical plan hierarchy, a
policy hierarchy can model different sources of uncertainty in the planning process such as
stochastic actions, uncertain action outcomes, and stochastic environment dynamics.
While the work in planning is concerned with finding the optimal policy given some
reward function, our work focuses on policy recognition which is the inverse problem, i.e., to
infer the agent's policies from watching the effects of the agent's actions. The two problems
however share a common element which is the model of a stochastic plan hierarchy. In policy
recognition, although it is possible to derive some information about the reward function
by observing the agent's behaviour, we choose not to do this, thus omitting from our model
the reward function and also the optimality notion. This leaves the model open to tracking
arbitrary agent's behaviours, regardless of whether they are optimal or not.
3.1 The General Model
3.1.1 Actions and Policies

In an MDP, the world is modelled as a set of possible states S , termed the state space. At
each state s, an agent has a set of actions A available, where each action a, if employed, will
cause the world to evolve to the next state s0 via a transition probability a (s; s0). An agent's
plan of actions is modelled as a policy that prescribes how the agent would choose its action
at each state. For a policy , this is modelled by a selection function  : S  A ! [0; 1]
where at each state s,  (s; a) is the probability that the agent will choose the action a. It
is easy to see that, given a fixed policy P
, the resulting state sequence is a Markov chain
0
with transition probabilities Pr(s j s) = a  (s; a)a (s; s0). Thus, a policy can also be
viewed as a Markov chain through the state space.

461

fiBui, Venkatesh & West
3.1.2 Local Policies

In the original MDP, behaviours are modelled at only two levels: the primitive action
level, and the plan level (policy). We would like to consider policies that select other
more refined policies and so on, down a number of abstraction levels. The idea is to form
intermediate-level abstract policies as policies defined over a local region of the state space,
having a certain terminating condition, and can be invoked and executed just like primitive
actions (Forestier & Varaiya, 1978; Sutton et al., 1999).
Definition 1 (Local policy). A local policy is a tuple  = hS; D; fi; i where:
 S is the set of applicable states.
 D is the set of destination states. fi : D ! (0; 1] is the stopping probabilities such
that fi (d) = 1; 8 d 2 D n S .
  : S  A ! [0; 1] is the selection function. Given the current state s, (s; a) is the
probability that the action a is selected by the policy  at state s.
The set S models the local region over which the policy is applicable. S will be called the
set of applicable states, since the policy can start from any state in S . We shall assume here
that S is discrete, and thus shall not be concerned with the technical details in generalising
the AHMM formulation to the continuous state space case. The stopping condition of the
policy is modelled by a set of possible destination states D and a set of positive stopping
probabilities fi (d); d 2 D where fi (d) is the probability that the policy will terminate when
the current state is d. It is possible to allow the policy to stop at some state outside of
S , however, for all d 2 D n S we enforce the condition that fi (d) = 1, i.e., d is a terminal
destination state. Sometimes, we might only want to consider policies with deterministic
stopping condition. In that case, every destination is a terminal destination: 8d 2 D,
fi (d) = 1. Thus, for a deterministically terminating policy, we can ignore the redundant
parameter fi , and need only specify the set of destinations D.
Given a starting state s 2 S , a local policy as defined above generates a Markov sequence of states according to its transition model. Each time a destination state d 2 D is
reached, the process stops with probability fi (d). Since the process starts from within S ,
but terminates only in one of the states in D, the destination states play the role of the
possible exits out of the local region S of the state space.
When we want to make clear which policy is currently being referred to, we shall use
the subscripted notations S , D , fi ,  to denote the elements of the policy .
Fig. 2 illustrates how a local policy  can be visualised. Fig. 2(a) shows the set of
applicable states S , the set of destinations D, and a chain starting within S and terminating
in D. The Bayesian network in Fig. 2(b) provides the detailed view of the chain from start
to finish. The Bayesian network in Fig. 2(c) is the abstract view of the chain where we are
only interested in its starting and stopping states.
3.1.3 Abstract Policies

The local policy as defined above selects among the set of primitive actions. Similarly, but
more generally, we can define higher level policies that select among a set of other policies.

462

fiPolicy recognition in the Abstract Hidden Markov Model




S
s
s
d

d
0

1

2

s

d

T

D

(a)

(b)

(c)

Figure 2: Visualisation of a policy
Definition 2 (Abstract Policy).
Let  be a set of abstract policies. An abstract
policy  over the policies in  is a tuple hS ; D ; fi ;  i where:
 S  [2S is the set of applicable states.
 D  [2D is the set of destination states. fi : D ! (0; 1] is the set of stopping
probabilities.
  : S   ! [0; 1] is the selection function where  (s; ) is the probability that
 selects the policy  at the state s.
Note the recursiveness in definition 2 that allows an abstract policy to select among a set
of other abstract policies. At the base level, primitive actions are viewed as abstract policies
themselves. Since primitive actions always stop after one time-step, Da  Sa and fi (d) =
1 8d 2 Da (Sutton et al., 1999). The idea that policies with suitable stopping condition
can be viewed just as primitive actions is first made explicit in (Sutton, 1995), which
also introduces the fi model for representing the stopping probabilities. Their subsequent
work (Sutton et al., 1999) introduces the abstract policy concept under the name options.
The execution of an abstract policy  is as follows. Starting from some state s, 
selects a policy  2  according to the distribution  (s; :). The selected policy  is then
executed until it is terminated in some state d 2 D . If d is also a destination state of 
(d 2 D ), the policy  stops with probability fi (d). If  still continues, a new policy
0 2  is selected by  at d, which will be executed until its termination and so on (Fig. 3).
Some remarks about the representation of an abstract policy are needed here. Let
s 2 [2 S , we denote the subset of policies in  which are applicable at s by (s) =
f 2  j s 2 S g. For an abstract policy  to be well-defined, we have to make sure that
at each state s,  only selects among the policies that are applicable at s. Thus, the
selection function has to be such that  (s; ) > 0 only if  2 (s). This helps to keep
the specification of the selection function to a manageable size, even when the set of all
policies  to be chosen from can be large. In addition, the specification of the selection
function and the stopping probabilities can make use of factored representations (Boutilier,
Dearden, & Goldszmidt, 2000) in the case where the state space is the composite of a set
of relatively independent variables. This ensures that we still have a compact specification

463

fiBui, Venkatesh & West

S

S
s


D
d
d



Figure 3: A chain generated by an abstract policy
of the probabilities conditioned on the state variable, even though the state space can be
of high dimension.
3.1.4 Policy Hierarchy

Using abstract policies as the building blocks, we can construct a hierarchy of abstract
policies as follows:
Definition 3 (Policy hierarchy). A policy hierarchy is a sequence H = (0 ; 1 ; : : : ; K )
where K is the number of levels in the hierarchy, 0 is a set of primitive actions, and for
k = 1; : : : ; K , k is a set of abstract policies over the policies in k 1 .
When a top-level policy K is executed, it invokes a sequence of level-(K-1) policies, each
of which invokes a sequence of level-(K-2) policies and so on. A level-1 policy will invoke
a sequence of primitive actions which leads to a sequence of states. Thus, the execution
of K generates an overall state sequence (s0; s1 ; : : : ; st ; : : :) that terminates in one of the
destination states in DK . When K = 1 this sequence is simply a Markov chain (with
suitable stopping conditions). However, for K  2, it will generally be non-Markovian,
despite the fact that all the policies are Markov, i.e., they select the lower level policies
based solely on the current state (Sutton et al., 1999). This is because knowing the current
state st alone does not provide information about the current intermediate-level policies,
which can affect the selection of the next state st+1. Intuitively, this means that an agent's
behaviour to achieve a given goal is usually non-Markovian, since its choice of actions
depends not only on the current state, but also on the current intermediate intentions of
the agent.
We term the dynamical process in executing a top-level abstract policy K the Abstract
Markov Model (AMM). When the states are only partially observable, the observation can
be modelled by the usual observation model Pr(ot j st ) = !(st; ot ). The resulting process is
termed the Abstract Hidden Markov Model (AHMM) since the states are hidden as in the
Hidden Markov Model (Rabiner, 1989).
The idea of having a higher level policy controlling the lower level ones in an MDP
can be traced back to the work by Forestier and Varaiya (1978), who investigated a two
layer structure similar to our 2-level policy hierarchy with deterministic stopping condition.
Forestier and Varaiya showed that that the sub-process, obtained by sub-sampling the state

464

fiPolicy recognition in the Abstract Hidden Markov Model

(a)

(b)

Figure 4: The environment and its partition
sequence at the time when the level-1 policy terminates, is also Markov, thus the policies
at level 1 simply play the role of an \extended" action. In our framework, given a policy
hierarchy, one can consider a \lifted" model where only the policies from level k up and the
observations at the time points when a policy at level k ends are considered. The level-k
policies can then be considered as primitive actions, and the lifted model can be treated
like a normal model.
3.2 State-Space Region-Based Decomposition

In some cases, the state space or some of its dimensions already exhibit a natural hierarchical
structure. For example, in the spatial domain, the set of ground positions can be divided
into small local spaces such as rooms, corridors, etc. A set of these local spaces can be
grouped together to form a larger space at the higher level (oors, buildings, etc). An
intuitive and often-used method for constructing the policy hierarchy in this case is via
the so-called region-based decomposition of the state space (Dean & Lin, 1995; Hauskrecht
et al., 1998). Here, the state space S is successively partitioned into a sequence of partitions
PK ; PK 1; :::P1 corresponding to the K levels of abstraction, where PK = fS g is the coarsest
partition, and P1 is the finest. For each region Ri of Pi, the periphery of Ri , P er(Ri) is
defined as the set of states not in Ri, but connected to some state in Ri. Let P eri be the
set of all peripheral states at level i: P eri = [Ri2Pi P er(Ri ). Fig. 4(b) shows an example
where the state space representing a building is partitioned into 4 regions corresponding to
the 4 rooms. The peripheral states for a region is shown in Fig 4(a), and Fig 4(b) shows all
such peripheral states.
To construct the policy hierarchy, we first define for each region R1 2 P1 a set of abstract
policies applicable on R1 , and having P er(R1) as the destination states. For example, for
each room in Fig 4, we can define a set of policies that model the agent's different behaviours
while it is inside the room, e.g., getting out through a particular door. These policies can
be initiated from inside the room, and terminate when the agent steps out of the room
(not necessarily through the target door since the policy might fail to achieve its intended

465

fiBui, Venkatesh & West
target). Note that since P er(R1 ) \ R1 = ;, all the policies defined in this manner have
deterministic stopping conditions.
Let the set of all policies defined be 1. At the higher level P2 , for each region R2 ,
we can define a set of policies that model the agent's behaviours inside that region with
applicable state space R2 , destination set P er(R2 ), and the constraint that these policies
must use the policies previously defined at level-1 to achieve their goals. An example is a
policy to navigate between the room-doors to get from one building gate to another. Let
the set of all policies defined at this level be 2. Continuing doing this at the higher levels,
we obtain the policy hierarchy H = (0 ; 1 ; 2 ; : : : ; K ). A policy hierarchy constructed
through State-space Region-based Decomposition is termed an SRD policy hierarchy.
An SRD policy hierarchy has the property that the set of applicable states of all the
policies at a given abstraction level forms a partition of the state space. Thus, from the state
sequence (s0 ; : : : ; st ; : : :) resulting from the execution of the top level policy, we can infer
the exact starting and terminating times of all intermediate-level policies. For example, at
level k, the starting/stopping times of the policies in this level are the time indices t's at
which the state sequence crosses over a region boundary: st 1 2 Rk and st 62 Rk for some
region Rk of the partition Pk . Later in section 5.1, we will show that this property helps to
simplify some of the complexity of the policy recognition problem.
3.3 A Policy Hierarchy Example

As an example, consider the task to monitor and predict the movement of an agent through
a building shown in Fig. 5(a). Each room is represented by a 5  5 grid, and two adjacent
rooms are connected via a door in the center of their common edge. The four entrances to
the building are labeled north (N), west (W), south (S) and east (E). In addition, the door
in the center of the building (C) acts like an entrance between the building's north wing
and south wing. At each state (cell), the agent can move in 4 possible directions except
when it is blocked by a wall.
The policy hierarchy to model the agent's behaviour in this environment can be constructed based on region-based decomposition at three levels of abstraction. Firstly, a region
hierarchy is constructed. The partition of the environment consists of the 8 rooms at level 1,
the two wings (north and south) at level 2, and the entire building at level 3. The behaviours
of the agent at level 1 (within each room) is represented by a set of level 1 policies. For
example, in each room, we use 4 level-1 policies to model the agent's behaviours of exiting
the room via the 4 different doors. These are essentially four Markov chains within the room
which terminate outside of the room. One way to represent these policies is to specify which
movement action the agent should take given the current position and the current heading.
At the higher level, the agent's behaviours within each wing are specified. For example,
we use 3 level-2 policies in each wing to model the agent's behaviours of exiting the wing
via the 3 wing exits. These policies are built on top of the set of level-1 policies already
defined. They specify which level-1 policies the agent should take to leave the wing at the
intended exit. Finally, at the top level, the agent's behaviours within the entire building
can be specified. For example, we use 4 top-level policies to model the agent's behaviours
of leaving the building via the four building exits N, W, S, E. A sample of these policies
and their parameters is given in Fig. 5(b).

466

fiPolicy recognition in the Abstract Hidden Markov Model
N
Level 1 Policy. (Destination is on the right)

Level 2 Policy (current state: W, destination : C)
0.8

W

Up
0.3

Go to rightdoor (level 1 policy)

0.2

0.1
0.5

Right

E

Go to backdoor (level 1 policy)
0.1

Down
C
W

Rightdoor
Backdoor

Level 3 (current state: W, destination: E)

S

0.9

Prior for toplevel policy

Go to C (level 2 policy)
N: 0.25, S: 0.25, E: 0.25, W: 0.25

W
0.1

Go to S (level 2 policy)
(a) The environment
(b) Parameters of the AHMM

Figure 5: An example policy hierarchy
3.4 AMM as a Plan Execution Model

Up to now, we have presented the AMM as a formal plan execution model to be used later
in the plan recognition process. In this subsection, we discuss the expressiveness of the
AMM as a formal plan specification language, and also the suitability of using the AMM
to encode plans in the context of plan recognition. Note that the discussion here focuses on
the representational aspect of the AMM alone. A discussion of the computational aspects
of the AMM/AHMM in comparison with other works in probabilistic plan recognition will
be presented in Section 7.
The AMM is particularly well-suited for representing goal-directed behaviours at different levels of abstraction. Each policy in the AMM can be viewed as a plan trying to
achieve a particular goal. However, unlike a classical plan, a policy specifies the course of
actions at all applicable states, and is more similar to a contingent plan. The ending of a
policy could either means that the goal has been achieved, or the attempt to achieve the
goal using the current policy has failed. This interpretation of the persistence of a policy
fits into the persistence model of intentions (Cohen & Levesque, 1990): when an intention
ends, there is no guarantee that the intended goal has been achieved. Thus, conceptually,
there are two types of destination states: one corresponds to the intended goal states, and
the other corresponds to unintended failure states resulting from the stochastic nature in
the execution of the plan. Due to its generality, the AMM does not need to distinguish between these two types; both the successful termination states and the unsuccessful ones are
treated the same as possible destination states, albeit with different reaching probabilities.4
4. One would expect that an agent would more likely to reach the intended destination state rather a
random failure state.

467

fiBui, Venkatesh & West
Using the AMM as a model of plan execution thus allows us to blur the difference
between planning and re-planning. At the same time, it moves from the recognition of
a classical plan towards the recognition of the agent's intention. Most of the existing
framework for probabilistic plan recognition does not explicitly represent the current state,
and thus, the relationship between states and the adoption and termination of current plans
is ignored (Goldman et al., 1999).5 Thus, it would be impossible to tell if the current plan
has failed and the new plan is an attempt to recover from this failure, or the current plan
has succeeded and the new plan is part of a new higher level goal.
A more expressive language for describing abstract probabilistic plan is the Hierarchical
Abstract Machines (HAM) proposed in (Parr & Russell, 1997; Parr, 1998). In a HAM, the
abstract policy is replaced by a stochastic finite automaton, which can call other machines
at the lower level. Our abstract policies can be written down as machines of this type. Such
a machine would choose one of the machines correspond to the policies at the lower level
and then go back to the start state after the called machines have terminated. The HAM
framework allows for machines with arbitrary finite number of machine states and transition
probabilities,6 thus can readily represent more complex plans such as concatenation of
policies, alternative policy paths, etc. It is possible to represent each machine in HAM
as a policy in our AMM, however with the cost of augmenting the state space to include
the machine states of all the machines in the current call stack. Thus, the size of the
AMM's new state space would be exponential with respect to the number of nested levels
in the HAM's call stack. While this shows in theory the expressiveness of HAM and our
policy hierarchy is the same, performing policy recognition on the HAM-equivalent policy
hierarchy is probably unwise since the state space becomes exponentially large after the
conversion. A better idea would be to represent the internal state of each machine as a
variable in a DBN and perform inference on this DBN structure directly.
The AMM is also closely related to a model for probabilistic plan recognition called the
Probabilistic State-Dependent Grammar (PSDG), independently proposed in (Pynadath,
1999; Pynadath & Wellman, 2000). The PSDG can be described as the Probabilistic
Context Free Grammar (PCFG) (Jelinek, Lafferty, & Mercer, 1992), augmented with a
state space, and a state transition probability table for each terminal symbol of the PCFG.
In addition, the probability of each production rule is made state dependent. As a result,
the terminal symbol now acts like primitive actions and the non-terminal symbol chooses its
expansion depending on the current state. Interestingly, the PSDG is directly related to the
HAM language described above, similar to the way production-rule grammars are related
to finite automata. Given a PSDG, we can convert it to an equivalent HAM by constructing
a machine for each non-terminating symbol, and modelling the production rules for each
non-terminating symbol by the automaton.
Our policy hierarchy is equivalent to a special class of PSDG where only production
rules of the form X ! Y X and X ! ; are allowed. The former rule models the adoption
of a lower level policy Y by a higher level policy X , while the latter models the termination
of a policy X . The PSDG model considered in (Pynadath, 1999; Pynadath & Wellman,
2000) allows for more general rules of the form X ! Y1 : : : YmX , i.e., the recursion symbol
5. with the exceptions of (Goldman et al., 1999; Pynadath & Wellman, 2000) which will be discussed in
detail in Section 7.
6. with the constraint that there is no recursion in the calling stack to keep the stack finite.

468

fiPolicy recognition in the Abstract Hidden Markov Model
must be located at the end of the expansion. Thus in a PSDG, a policy might be expanded
into a sequence of policies at the lower level which will be executed one after another before
control is returned to the higher level policy. The implicit assumption here is that when a
policy in the sequence terminates, it always does so at a state where the next policy in the
sequence is applicable. Given this assumption, in the language of the AHMM we can define
a compound policy k as a policy that simply and orderly executes a sequence of policies at
k 1 ; : : : ;  k 1 , independent of the current state. A PSDG is then equivalent
the lower level (1)
(m)
to an AHMM if compound policies of this form are allowed.
Since the AMM closely follows the models used in abstract probabilistic planning, it can
be used to model and recognise the behaviours of any autonomous agent whose decision
making process is equivalent to an abstract MDP. It is also useful as a formal language
for specifying contingent plans whose execution can then be monitored using the policy
recognition algorithm. The language is also rich enough to specify a range of useful human
behaviours, especially in domains where there is a natural hierarchical decomposition of
the state space. Section 6 presents an application of the AHMM framework to the problem
of recognising people behaviours in a complex spatial environment. Here, each policy of
the AHMM represents the evolution of possible trajectories of people movement while the
person performs a certain task in the environment such as heading towards a door, using
the computer at a certain location, etc. The policies at different levels would represent the
evolution of trajectories at different levels of abstraction. Due to the existing hierarchy in
the domain, the policies can be constructed using the region-based decomposition of the
state space. The environment is populated with multiple cameras divided into different
zones that can provide the current location of the tracking target, albeit a noisy one. The
noisy observations can be readily handled by the observation model in the AHMM. The
policy recognition algorithm can then be applied to infer the person's current policy at
different levels in the hierarchy.
One main restriction of the current AHMM model is that we consider only one toplevel policy at a time, thus are unable to model the inter-leaving of concurrent plans.
Another more subtle restriction is the assumption that a high level policy selects the lower
level policies depending only on the current state. If the state space is interpreted as the
states of the external environment, this assumption implies that the actor either has full
observation about the current state, or at least refines its intentions based on the actor's
observation about the current state only (and not the entire observation history). Note that
these restrictions of the AHMM also apply in the case of the PSDG model.
4. Dynamic Bayesian Network Representation

In this section, we describe the Dynamic Bayesian Network (DBN) representation of the
AHMM. The network serves two purposes: (1) as the tool to derive the probabilistic independence property of this stochastic model, and (2) as the computational framework for
the policy recognition algorithms in Section 5.
4.1 Network Construction

At time t, let st represent the current state, tk represent the current policy at level k
(k = 0; : : : ; K ), ekt represent the ending status of tk , i.e., a boolean variable indicating

469

fiBui, Venkatesh & West
k

k

k

ek

ek

ek =F

e k-1

e k-1 =T

ek-1 =F

s

s
(a)

s
(b)

(c)

Figure 6: Sub-network for policy termination
whether the policy tk terminates at the current time. These variables would make up the
current time-slice of the full DBN. For our convenience, the notation tall refers to the set of
all the current policies ftK ; : : : ; t0 g. Before presenting the full network, we first describe
the two sub-structures that model how policies are terminated and selected. The full DBN
can then be easily constructed from these sub-structures.
4.1.1 Policy Termination

From the definition of abstract policies, a level-k policy tk terminates only if the lower level
policy tk 1 terminates, and if so, tk terminates with probability fitk (st). In the Bayesian
network representation, the terminating status ekt therefore has three parent nodes: tk , st ,
and etk 1 (Fig. 6(a)).
The parent variable ekt 1 however plays a special role. If ekt 1 = T , meaning the lower
level policy terminates at the current time, Pr(ekt = T j tk ; st) = fitk (st) which gives the
conditional probability of ekt given the other two parent variables (Fig. 6(b)). However, if
ekt 1 = F , tk should not terminate and so ekt = F . Therefore, given that ekt 1 = F , ekt is
deterministically determined and is independent of the other two parent variables tk and
st . Using the notion of context-specific independence (CSI) (Boutilier et al., 1996), we can
then safely remove the links from the other two parents to ekt in the context that ekt 1 is
false (Fig. 6(c)).
At the bottom level, since the primitive action always terminates immediately, e0t = T
for all t. Since we are modelling the execution of a single top-level policy K , we can assume
that the top-level policy does not terminate and remains unchanged: eKt = F and tK = K
for all t. Also, note that elt = T ) ekt = T for all k  l, and elt = F ) ekt = F for all k  l.
Thus, at each time t, there exists 0  lt < K such that ekt = T for all k  lt , and ekt = F
for all k > lt . The variable lt is termed the highest level of termination at time t. Knowing
the value of lt is equivalent to knowing the terminating status of all the current policies.
4.1.2 Policy Selection

The current policy tk in general is dependent on the higher level policy tk+1, the previous
state st 1 , the previous policy at the same level tk 1 and its ending status ekt 1 . In the
Bayesian network, tk thus has these four variables as its parents (Fig. 7(a)). This depen-

470

fiPolicy recognition in the Abstract Hidden Markov Model
 k+1
k

 prev

k

k

eprev

 k+1
k

 prev

k

(a)

k
 prev

ek = F

ek =T

sprev

sprev

prev

sprev

 k+1


k

prev

(b)

(c)

Figure 7: Sub-network for policy selection
dency can be further broken down into two cases, depending on the value of the parent
node ekt 1 .
If the previous policy has not terminated (ekt 1 = F ), the current policy is the same as
the previous one: tk = tk 1 , and the variable tk is thus independent of tk+1 and st 1 .
Therefore, in the context ekt 1 = F , the two links from tk+1 and st 1 to the current policy
can be removed, and the two nodes tk and tk 1 can be merged together (Fig. 7(b)).
If the previous policy has terminated (ekt 1 = T ), the current policy is selected by the
higher level policy with probability Pr(tk j tk+1; st 1 ) = tk+1 (st 1 ; tk ). In this context, tk
is independent of tk 1 and the corresponding link in the Bayesian network can be removed
(Fig. 7(c)).
4.1.3 The Full DBN

The full dynamic Bayesian network can be constructed for all the policy, ending status, and
state variables by putting the sub-networks for policy termination and selection together
(Fig. 8). At the top level, since eKt = F , we can remove the ending status nodes and merge
all the tK into a single node K . At the base level, since e0t = T , we can remove the ending
status nodes and also the links from t0 to t0+1. To model the observation of the hidden
states, an observation layer can be attached to the state layer as shown in Fig. 8.
Suppose that we are given a context where each of the variable ekt is known. We can
then modify the full DBN using the corresponding link removal and node merging rules.
The result is a more intuitive tree-shaped network in Fig. 9, where all the policy nodes
corresponding to the same policy for its entire duration are grouped into one. The grouping
can be done since knowing the value of each ekt is equivalent to knowing the exact duration
of each policy in the hierarchy. One would expect that performing probabilistic inference
on this structure is more simple than that of the full DBN in Fig. 8. In particular, if the
state sequence is known, the remainder of the network in Fig. 9 becomes singly-connected,
i.e., a directed graph with no undirected cycles, allowing inference to be performed with
complexity linear to the size of the network (Pearl, 1988). The policy recognition algorithms
that follow later exploit extensively this particular tractable case of the AHMM.

471

fiBui, Venkatesh & West

K

Level K

Policy

2

Stop status

e2

Policy

1

Stop status

e1

Action

0

State

Observation

Figure 8: The DBN representation of the Abstract Hidden Markov Model

4

3

...

2

...

1
s

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

Figure 9: Simplified network if the duration of each policy is known (action nodes are
omitted for clarity)

472

fiPolicy recognition in the Abstract Hidden Markov Model
4.2 Conditional Independence in the Current Time-Slice

The above discussion identifies a tractable case for the AHMM, but it requires the knowledge
of the entire history of the state and the policy ending status variables. In this subsection,
we focus on the conditional independence property of the nodes in the current time-slice:
st ; t0 ; : : : ; tK . Since these nodes will make up the belief state of any future inference
algorithm for our AHMM, any independence properties among these variables, if exploited,
can provide a more compact representation of the belief state and reduce the inference
complexity.
Due to the way policies are invoked in the AMM, we can make an intuitive remark
that the higher level policies can only inuence what happens at the lower level through the
current level. More precisely, for a level k policy tk , if we know its starting state, the course
of its execution is fully determined, where being determined here means without inuence
from what is happening at the higher levels. Furthermore, if we also know how long the
policy has been executed, or equivalently its starting time, the current state of its execution
is also determined. Thus, the higher level policies can only inuence the current state of
execution of tk either through its starting state or starting time. In other words, if we know
tk together with its starting time and starting state, then the current higher level policies
are completely independent of the current lower level policies and the current state. The
theorem 1 below formally states this in a precise form. Note that the condition obtained
is the strictest: if one of the three conditional variables is unknown, there are examples of
AMMs in which the higher level policies can inuence the lower level ones.
Theorem 1. Let tk and bkt be two random variables representing the starting time and the

starting state, respectively, of the current level-k policy tk : tk = maxft0 < t j ekt0 = T g and
bkt = stk . Let t>k = ftk+1 ; : : : ; tK g denote the set of current policies from level k + 1 up
to K, and t<k = fst ; t0 ; : : : ; tk 1 g denote the set of current policies from level k 1 down
to 0 together with the current state. We have:
t>k ? t<k j tk ; bkt ; tk

(7)

Proof. We sketch here an intuitive proof of this theorem through the use of the Bayesian
network manipulation rules for context-specific independence which have been discussed in
4.1.1 and 4.1.2. An alternative proof that does not use CSI can be found in (Bui et al.,
2000).
We first note that the theorem is not obvious by looking at the full DBN in Fig. 8.
Therefore, we shall proceed by modifying the network structure in the context that we
know tk .
At time tk , all the policies at level k and below must terminate: eltk = T for all l  k.
Thus we can remove all the links from these policies to the new policies at time tk + 1.
On the other hand, from time tk + 1 until the current time t, all the policies at level k
and above must not terminate: elt0 = F for all l  k, tk + 1  t0 < t. Thus we can group all
the policies at level l  k between time tk + 1 and t into one node representing the current
policy at level l.
These two network manipulation steps result in a network with the structure shown in
Fig. 10. Once the modified network structure is obtained, we can observe that t>k and

473

fiBui, Venkatesh & West

k+1

t

k

t

k-1

t
...

...

k

...

State

bt

st

Time

k
t

t

...

Figure 10: Network structure after being conditioned on tk
are d-separated by tk and bkt in the new structure. Thus t>k and t<k are independent
given tk , bkt and tk .
t<k

5. Policy Recognition

In this section we begin to address the problem of policy recognition in the framework of
the AHMM. We assume that a policy hierarchy is given and is modelled by an AHMM,
however the top level policy and the details of its execution are unknown. The problem is
then to determine the top level policy and other current policies at the lower levels given
the current sequence of observations. In more concrete terms, we are interested in the
conditional probability:
Pr(tK ; : : : ; t0 j o~t 1 )
and especially, the marginals:
Pr(tk j o~t 1 ); for all levels k
Computing these probabilities gives us the information about the current policies at all
levels of abstraction, from the current action (k = 0), to the top-level policy (k = K ),
taking into account all the observations that we have up to date.
In typical monitoring situations, these probabilities need to be computed \online", as
each new observation becomes available. To do this, it is required to update the belief
state (filtering distribution) of the AHMM at each time point t. This problem is generally
intractable unless the belief state has an ecient representation that affords a closed form
update procedure. In our case, the belief state is a joint distribution of K + 3 discrete
variables: Pr(tK ; : : : ; t0 ; st ; lt j o~t ). Without any further structure imposed on the belief
state, the complexity for updating it is exponential in K .
To cope with this complexity, one generally has to resort to some form of approximation
to trade off accuracy for computational resources. On the other hand, the analysis of the

474

fiPolicy recognition in the Abstract Hidden Markov Model
AHMM network in the previous section suggests that the problem of inference in the AHMM
can be tractable in the special case when the history of the state and terminating status
variables is known. Motivated by this property of the AHMM, our main aim in this section
is to derive a hybrid inference scheme that combines both approximation and tractable
exact inference for eciency. We first treat the special case of policy recognition where
the belief state of the AHMM has a tractable structure in 5.1. We then present a hybrid
inference scheme for the general case using the Rao-Blackwellised Sequential Importance
Sampling (RB-SIS) method in 5.2.
5.1 Policy Recognition: the Tractable Case

Here, we address the policy recognition problem under two assumptions: (1) the state
sequence can be observed with certainty, and (2) the exact time when each policy starts
and ends is known. More precisely, our observation at time t includes the state history
s~t = (s0 ; : : : ; st ) and the policy termination history ~lt = (l0 ; : : : ; lt ). The belief state that
we need to compute in this case is Bt = Pr(tall ; st ; lt j s~t 1 ; ~lt 1 ) and its posterior after
absorbing the observation at time t: Bt+ = Pr(tall j s~t ; ~lt ).
The first assumption means that the observer always knows the true current state and is
often referred to as \full observability". When the states are fully observable, we can ignore
the observation layer fot g in the AHMM and thus only have to deal with the AMM instead.
The second assumption means that the observer is fully aware when the current policy
ends and a new policy begins. If the policy hierarchy is constructed from the region-based
decomposition of the state space (subsection 3.2), the termination status can be inferred
directly from the state sequence. Thus for SRD policy hierarchies, only the full observability
condition is needed since the second assumption is subsumed by the first and can be left
out. Except for SRD policy hierarchies, these two assumptions are usually too restrictive
for the policy recognition algorithm presented here to be useful by itself. However, the
algorithm for this special case will form the exact step in the hybrid algorithm presented in
subsection 5.2 for the general case.
5.1.1 Representation of the belief state

We first look at the conditional joint distribution Pr(tall ; st j s~t 1; ~lt 1 ). From the termination history ~lt 1 , we can derive precisely the starting time of the current level-k policy:
tk

= maxf0g [ ft0 < tj ekt0 = T g = maxf0g [ ft0 < tj lt0  kg

On the other hand, knowing the starting time together with the state history also gives
us the starting state bkt . Thus, both the starting time and the starting state of tk can be
derived from s~t 1 and ~lt 1 . From Theorem 1, we obtain for all level k:
t>k ? t<k j tk ; s~t 1 ; ~lt 1
In other words, given s~t 1 and ~lt 1 , the conditional joint distribution of ftK ; : : : ; t0 ; st g
can be represented by a Bayesian network with a simple chain structure. We denote this
chain network by Ct  Pr(tall ; st j s~t 1 ; ~lt 1 ) and term it the belief chain for the role it plays
in the representation of the belief state (Fig. 11(a)). If a chain is drawn so that all links

475

fiBui, Venkatesh & West

K

Bt

Ct



k+1
root

K

e

K

e

k+1

k+1



k



k



k-1

ek
k-1

e k-1
1



1



0

e1

0

s

s
(a)

(b)

Figure 11: Representation of the belief state
point away from the level-k node, we say that the chain has root at level k. The root of the
chain can be moved from k to another level k0 simply by reversing the links lying between
k and k0 using the standard link-reversal operation for Bayesian networks (Shachter, 1986).
Each node in the belief chain also has a manageable size. In principle, the domain of
tk is k , the set of all policies at level k, and the domain of st is S , the set of all possible
states. When K is large, we basically want to model a larger state space, and the set
of policies to cover this state space is also large. The sizes of these domains would most
likely grow exponential w.r.t. K . However, given a particular state, the number of policies
applicable at that state would remain relatively constant and independent of K . For each
policy tk , we know its starting state bkt , which implies that tk 2 k (bkt ), the set of all
level-k policies applicable at bkt . Thus k (bkt ) can be used as the \local" domain for tk
to avoid the exponential dependency on K . Similarly, the domain for st can be taken as
the set of neighbouring states of st 1 (reachable from st 1 by performing one primitive
action). For a given state, we term the maximum number of relevant objects (applicable
policies/actions, neighbouring states) at a single level the degree of connectivity N of the
domain being modelled. The size of the conditional probability table for each link of the
belief chain is then O(N 2), and the overall size of the belief chain is O(K N 2 ).
We now can construct the belief state Bt from Ct. Since the current terminating status
is solely determined by the current policies and the current state, the belief state Bt can be
factorised into:
Pr(tall ; st ; lt j s~t 1 ; ~lt 1 ) = Pr(lt j tall ; st ) Pr(tall ; st j s~t 1 ; ~lt 1 ) = Pr(lt j tall ; st)Ct
Note that the variable lt is equivalent to the set of variables feKt ; : : : ; e1t g. Thus, the full
belief state Bt can be realised by adding to Ct the links from the current policies and the
current state to the terminating status variables ekt (Fig. 11(b)). The size of the belief state

476

fiPolicy recognition in the Abstract Hidden Markov Model


K


e

K

k+1

K

e

k+1


e

k+1

k



k



k-1

ek


e
k+1




K

ek

k-1

e k-1


1



0

e1
(a)

(b)

Figure 12: Belief state updating: from Bt to Bt+
would still be O(K N 2 ). If the state is a composite of many orthogonal variables, a factored
representation can be used so that the size of the belief state representation does not depend
exponentially on the dimensionality of the state space. We discuss factored representations
further under subsection 5.2.2.
5.1.2 Updating the belief state

Since the belief state Bt can be represented by a simple belief network in Fig. 11(b), we can
expect that a general exact inference method for updating the belief state such as (Kjrulff,
1995) will work eciently. However, this general method works with undirected network
representation of the belief state distribution which can be inconvenient for us later on when
we want to sample from such a distribution. Here, we describe an algorithm that updates
the belief state in the closed form given by the directed network in Fig. 11(b).
Assuming that we have a complete specification of the belief state Bt , i.e., all the parameters for its Bayesian network representation, we need to compute the parameters for
the new network Bt+1 . This is done in two steps, as in the standard \roll-over" of the belief
state of a DBN: (1) absorbing the new evidence st , lt and (2) projecting the belief state
into the next time step.
The first step corresponds to the instantiation of the variables st , e1t ; : : : ; eKt in the
Bayesian network Bt to obtain Bt+ which is the conditional joint distribution of tK ; : : : ; t0 .
By checking the conditional independence relationships in Fig. 11(b), it is easy to see that
Bt+ again has a simple chain network structure. Thus, conceptually, the problem here is
to update the parameters of the chain Ct so as to absorb the given evidence to form a new
chain Bt+. This can be done by a number of link-reversal steps as follows.
To instantiate st, we first move the root of the chain Ct to st. The variable st then has
no parents and can be instantiated and deleted from the network (Fig. 12(a)).
To instantiate lt which is equivalent to the value assignment (eKt = F; : : : ; eltt +1 = F; eltt =
T; : : : e1t = T ), starting from k = 1, we iteratively reverse the links from tk 1 to tk and
from tk to ekt (Fig. 12(b)). In algebraic forms, the first link reversal operation corresponds

477

fiBui, Venkatesh & West
to computing the following probabilities:
Pr(tk j st; e1t ; : : : ; ekt 1 ) =
Pr(tk 1 j tk ; st; e1t ; : : : ; ekt 1 ) /

X Pr(k j k 1) Pr(k 1 j s ; e1; : : : ; ek 1)
t

t

t

t

t

tk 1
Pr(tk j tk 1) Pr(tk 1 j st ; e1t ; : : : ; ekt 1)

t

(8)
(9)

and the second link reversal corresponds to:
Pr(tk j st ; e1t ; : : : ; ekt ) / Pr(ekt j tk ; st; ekt 1 ) Pr(tk j st ; e1t ; : : : ; ekt 1)
(10)
Effectively, the k-th link reversal step positions the root of the chain Ct at tk and absorbs
the evidence ekt . By repeating this link reversal operations with k = 1; : : : ; lt + 1, we obtain
a new chain for Bt+ which has root at level lt +1. Note that there is no need to incorporate
the instantiations ekt = F for k > lt + 1 since they are the direct consequences of the
instantiation eltt+1 = F . The parameters of the chain Bt+ are given below. The upward
links remain the same as those of Ct , while the marginal at level lt + 1 and the downward
links are obtained as the results of the link reversal operations above:
Pr(tk+1 j tk ; st ; lt ) = Pr(tk+1 j tk ); k  lt + 1
Pr(tk j st ; lt ) = Pr(tk j st; e1t ; : : : ; ekt ); k = lt + 1
Pr(tk 1 j tk ; st ; lt ) = Pr(tk 1 j tk ; st ; e1t ; : : : ; ekt 1 ); k  lt
In the second step, we continue to compute Ct+1 from Bt+. Since all the policies at
levels higher than lt do not terminate, t>l+1t = t>lt , and we can retain this upper sub-chain
from Bt+ to Ct+1 . In the lower part, for k  lt, a new policy tk+1 is created by the policy
+1 at the state st , and thus a new sub-chain can be formed among the variables <lt with
tk+1
t+1
+1 ; st ) =  k+1 (st ; k ). Note that the domain of the newly-created
parameters Pr(tk+1 j tk+1
t+1
t+1
k
k
node t+1 is  (st ). The new chain Ct+1 is then the combination of these two sub-chains,
which will be a chain with root at level lt + 1 (see Fig. 13). Once we have the chain Ct+1 ,
the new belief state Bt+1 can be obtained by simply adding the terminating status variables
fekt+1 g to Ct+1 .
This completes the procedure for updating the belief state from Bt to Bt+1 , thus allowing
us to compute the belief state Bt at each time step. Although the belief state is the joint
distribution of all the current variables, due to its simple structure, the marginal distribution
of a single variable can be computed easily. For example, if we are only interested in the
current level-k policy tk , the marginal probability Pr(tk j s~t 1 ; ~lt 1 ) is simply the marginal
at the level-k node in the chain Ct, and can be readily obtained from the chain parameters.
The complexity of the belief state updating procedure at time t is proportional to lt
since it only needs to modify the bottom lt levels of the belief state. On the other hand, the
probability that the current policy at level l terminates can be assumed to be exponentially
P
small w.r.t. l. Thus, the average updating complexity at each time-step is O( l l=exp(l))
which is constant-bounded, and thus does not depend on the number of levels in the policy
hierarchy. In terms of the number of policies and states, the updating complexity is linear
to the size of a policy node in the belief chain, thus is linear to the degree of connectivity
of the domain.

478

fiPolicy recognition in the Abstract Hidden Markov Model

Kt

l +1
t

Ct+1
l

l

t+1

t

Bt+
1t
0

0

t+1

t

s

st

t+1

Figure 13: Belief state updating: from Bt+ to Ct+1
5.2 Policy Recognition: The General Case

We now return to the general case of policy recognition, i.e., without the two assumptions of
the previous subsection. This makes the inference tasks in the AHMM much more dicult.
Since neither the starting times nor the starting states of the current policies are known
with certainty, theorem 1 cannot be used. Thus, the set of current policies no longer forms
a chain structure as it did in Ct since the conditional independence properties of the current
time-slice no longer hold. We therefore cannot hope to represent the belief state by a simple
structure as we did previously. An exact method for updating the belief state will thus have
to operate on a structure with size exponential in K , and is bound to be intractable when
K is large.
To cope with this complexity, an approximation scheme such as sequential importance
sampling (SIS) (Doucet et al., 2000b; Liu & Chen, 1998; Kanazawa et al., 1995) can be
employed. In our previous work (Bui, Venkatesh, & West, 1999), we have applied an SIS
method known as the likelihood weighting with evidence reversal (LW-ER) (Kanazawa et al.,
1995) to an AHMM-like network structure. However the SIS method needs to sample in the
product space of all the layers of the AHMM and thus becomes less accurate and inecient
with large K . The key to get around this ineciency is to utilise the special structure of
the AHMM, particularly, its special tractable case, to keep the set of variables that need to
be sampled to a minimum.
The improvement of the SIS method to achieve this is has been presented in subsection 2.5 in the name of the Rao-Blackwellised SIS (RB-SIS) method. Rao-Blackwellisation
specifically allows the marginalisation of some variables analytically and only samples the
remaining variables. As a result, this reduces the averaged error, measured as the variance
of the estimator (Casella & Robert, 1996).

479

fiBui, Venkatesh & West
In order to apply RB-SIS to the AHMM, the main problem is to identify which variables should be used as the Rao-Blackwellising variables and should still be sampled, with
the remaining variables being marginalised analytically. The key to choosing the RaoBlackwellising variables, as we have shown in 2.5, is so that if those variables can be observed,
the Rao-Blackwellised belief state becomes tractable. In subsection 5.1, we have demonstrated that if the state history s~t and the terminating status history ~lt can be observed
then the belief state has a simple network structure and can be updated with constant average complexity. Thus, (st ; lt ) can be used conveniently as the Rao-Blackwellising variable
rt . Note that the variables ~lt are the context variables which help to simplify the network
structure of the AHMM, while the state variables s~t help to make the remaining network
singly-connected so that exact inference can operate eciently (see subsection 4.1.3).
5.2.1 RB-SIS for AHMM

We now discuss the specific application of RB-SIS to the problem of belief state updating
and policy recognition in the AHMM. Our main objective is to use RB-SIS to estimate the
conditional probability of the policy currently being executed at level-k given the current
sequence of observations Pr(tk+1 j o~t ).
Mapping the RB-SIS general framework in subsection 2.5 to the AHMM structure, the
set of all current variables xt is now the set of current policies, terminating status nodes,
and the current state: xt = (tall ; st; lt ). The probability under estimation Pr(tk+1 j o~t ) can
be viewed as an expectation by letting f (tall ; st; lt ) = Pr(tk+1 jtall ; st ; lt ) so that:
X Pr(k jall ; s ; l ) Pr(all ; s ; l jo~ ) = Pr(k j o~ )
f =
t t
t t t
t+1 t
t
t+1 t
tall ;st ;lt

Using RB-SIS to estimate this expectation, we shall split xt into two sets of variables:
the set of RB variables rt = (st; lt ), and the set of remaining variables zt = tall which is the
set of all the current policies. The functional h, which depends only on the RB variables
and is obtained from f by integrating out the remaining variables (Eq. (5)), now has the
form:
X Pr(k j all ; s ; l ) Pr(all j s~ ; ~l ; o~ ) = Pr(k j s~ ; ~l ) (11)
h(~rt ) = h(~st ; ~lt ) =
t t
t t t
t+1 t
t
t+1 t t
tall

which is the marginal Ct+1 (tk+1 ) from the belief chain at time t + 1.
The RB belief state, which is the belief state of the AHMM when the RB variables are
known, becomes:
Rt = Pr(tall ; st; lt ; ot j s~t 1 ; ~lt 1 ; o~t 1 ) = Pr(tall ; st ; lt ; ot j s~t 1 ; ~lt 1 )
(12)
and is identical to the special belief state Bt discussed in subsection 5.1, except a minor
modification to attach the observation variable ot .
From (11) and (12), both the h function and the RB belief state can be computed
very eciently using the exact inference techniques described in 5.1. Thus RB-SIS can be
implemented eciently with minimal overhead in exact inference.
The main RB-SIS algorithm for the AHMM is given in Fig. 14. Note that we only need
to sample the RB variables s~t and ~lt . For each sample i, in addition to the weights w(i) ,

480

fiPolicy recognition in the Abstract Hidden Markov Model
Begin
For t = 0; 1; : : :
For each sample i = 1; : : : ; N
Sample sti ; lti from Bti (st; ltj ot )
Update weight w i = w i Bti (ot )
Compute the posterior RB bel state Bti = Bti (tall jsti ; lti ; ot)
Compute the belief chain Ct i from Bti
Compute the new belief state Bti from Ct i
Compute h i = Ct i (tk )
Compute the estimator Pr(tk j o~t)  f^RBSIS = PNi h i w~ i
End
( )

( )

( )

( )

( )

( )

( )
+1

( )

( )
+1

( )
+1

( )
+
( )
+

( )

( )

( )

( )
+1

+1

+1

=1

( )

( )

Figure 14: RB-SIS for policy recognition
B er
t


K

e

K

e

k+1

k+1




k



k1

ek = F
e k1 = T


1



0

highest level
of termination lt

e1

s
o

Figure 15: Sampling the Rao-Blackwellising variables in AHMM
we also maintain a parametric representation of the Rao-Blackwellised belief state Bt(i), and
the value of the h function for that sample h(i) . The weights of the samples, together with
the values of the h function can then be combined to yield an approximation for f.
Some details on how we can obtain the new samples at each time step are worth noting
here. Since we are using the optimal sampling distribution qt = Bt(st ; lt j ot ) to sample the

481

fiBui, Venkatesh & West
RB variables st and lt , we need to perform the evidence reversal step.7 This can be done
by positioning the root of the belief chain Ct at st and reverse the link from st to ot . This
gives us the network structure for Bter = Bt (st; lt ; tall j ot ) which is exactly the same as Bt
(see Fig. 15), except that the evidence ot has been absorbed into the marginal distribution
of st. The weight wt = Bt(ot ) can also be obtained as a by-product of this evidence reversal
step. In order to sample st and lt from Bter without the need to compute the marginal
distribution for these two variables, we can use forward sampling to sample every variable
of Bter , starting from the root node st and proceeding upward. Since lt by definition is the
highest level of policy termination, the sampling can stop at the first level k where ekt = F .
We can then assign lt the value k 1. Any unnecessary samples for the policy nodes along
the way are discarded. Once we have the new samples for st and lt , the updating of the
RB belief state from Bt to Bt+1 is identical to the belief state updating procedure described
in 5.1. The h function can then be obtained by computing the corresponding marginal of
the new belief chain Ct+1.
At each time step, the complexity of maintaining a sample (sampling the new RB
variables and updating the RB belief state) is again O(lt ), and thus, on average, bounded
by a constant. The overall complexity of maintaining every sample is thus O(N ) on average.
If a prediction is needed, for each sample, we have to compute h by manipulating the chain
Ct+1 with the complexity O(K ). Thus the complexity at the time step when a prediction
needs to be made is O(NK ).
In comparison with the use of an SIS method such as LW-ER, the RB-SIS has the
same order of computational complexity (the SIS also has complexity O(NK )). However,
while the SIS method needs to sample every layers of the AHMM, the RB-SIS method only
needs to sample two sequences of variables s~t, ~lt , and avoids having to sample the K policy
sequences f~tk g. After Rao-Blackwellisation, the dimension of the sample space becomes
much smaller, and more importantly, does not grow with K . As a result, the accuracy of
the approximation by the RB-SIS method does not depend on the height of the hierarchy
K . In contrast, due to the problems of sampling in high dimensional space, the accuracy of
SIS methods tends to degrade, especially when K is large.
5.2.2 Performing Evidence Reversal with a Factored State Space

In many cases, the state space S is the Cartesian product of many state variables representing relatively independent properties of a state: st = (s1t ; s2t ; : : : ; sM
t ). Since the overall
state space is very large, specifying an action by the usual transition probability matrix is
problematic. It is advantageous in this case to represent the state information in a factored
form, i.e., representing each state variable smt in a separate node rather than lumping them
into a single node st. It has been shown that using factored representations, we can specify
the transition probability of each action in a compact form since an action is likely to affect
only a small number of state variables and the specification of the effects of actions has
many regularities (Boutilier et al., 2000).
7. The term evidence reversal is used in this paper to refer to a general procedure in which the link to
the observation node is reversed prior to sampling (Kanazawa et al., 1995), thus allowing us to sample
according to the optimal sampling distribution qt .

482

fiPolicy recognition in the Abstract Hidden Markov Model
The representation of the belief chain Ct and also the RB belief state Bt can take direct
advantage of this factored representation of actions. Indeed, the chain parameter Ct(st jt0 )
of the link from t0 to st is precisely the transition probability for the action t0 at the
previous state st 1 (note that st 1 is known due to Rao-Blackwellisation). This conditional
distribution can be extracted from the compact factored representation of t0 in the general
form of a Bayesian network of the variables fs1t ; s2t ; : : : ; sM
t g. For our convenience, let us
denote this Bayesian network by F (:jt0 ). This network is usually sparse enough so that exact inference can operate eciently. For example, in the special case where fs1t ; s2t ; : : : ; sMt g
are independent given t0 and st 1 , F will be factored completely into the product of M
marginals of smt .
Although factored representations can be used as part of the RB belief state, care must
be taken when performing evidence reversal, i.e. to reverse the link from the state variable
to the observation node. In the procedure for evidence reversal discussed previously (see
Fig. 15), we first position the root of Ct at the node st, thus need to compute and represent
the distribution Pr(st). In the factored state space case, this becomes a joint distribution
0
of all the state variables fs1t ; s2t ; : : : ; sM
t g. Without conditioning on the current action t ,
m
the factored representation of the state variables fst g cannot be utilised, thus resulting in
complexity exponential in M .
The key to get around this diculty is to always keep the specification of the distribution
of the current state conditioned on the current action, not vice versa. Thus, when computing
Bter = Bt (:jot ), we first position the root of the chain Ct at t0, and then reverse the evidence
from ot to both t0 and st . In algebraic form, we use the following factorisation of the joint
distribution of the current action and state given the current observation:
Pr(t0 ; st jot ) = Pr(stjt0 ; ot ) Pr(t0 jot )
(13)
Fig. 16 illustrates this evidence reversal procedure. In the model depicted here, F can
be an arbitrary Bayesian network. The observation model can be specified by attaching the
observation nodes fo1t ; o2t ; : : :g to the state variables. The overall network representing the
distribution Pr(st ; ot j t0 ) will be denoted by F obs(:jt0 ).
We first look at the first term in the RHS of (13). Let F er (:jt0 ; ot ) represent the
distribution Pr(st j t0 ; ot ). Note that F er can be obtained by conditioning F obs(:jt0 ) on the
observation ot . This can be achieved by applying an exact inference method such as the
clustering algorithm (Lauritzen & Spiegelhalter, 1988) on the network F obs(:jt0 ).
For the second term in the RHS of (13), we note that:
Pr(ot j t0 ) =

X Pr(s ; o j 0) = X F obs(s ; o j0)
st

t

t

t

st

t

t

t

This integration can be readily obtained as a by-product when performing the above clustering algorithm on F obs(:jt0 ). Once Pr(ot j t0 ) is known, we can compute Pr(t0 j ot ) by:
Pr(t0 j ot ) / Pr(ot j t0 ) Pr(t0 )
This shows that the belief state after evidence reversal Bter = Bt (:j ot ) still has a simple
structure that exploits the independence relationships between the state variables fsmt g
given the current action t0. Sampling the RB variables from this structure can proceed as

483

fiBui, Venkatesh & West



0



F
s 3t . .
.

2

st

F

0

obs

s 3t

2

st

1

F er
.
. .

1

st

st

2

1

o3t

...
o 4t
2

ot

1

ot

o3t

.. .
o 4t

ot

ot

Belief state before evidence reversal

Belief state after evidence reversal

Figure 16: Evidence reversal with factored state space
follows: Pr(t0 j ot ) is first used to sample t0; F er (stjt0 ; ot ) is then used to sample st. Once
we have obtained the sample for t0 and st , we can proceed to sample the remaining nodes
in the network Bter to obtain a sample for lt as usual. Finally, we note that the weight
wt = Pr(ot ) can also be computed eciently by:
Pr(ot ) =

X Pr(o j 0) Pr(0)
t

t0

t

t

In this evidence reversal procedure, for each value of t0 , we need to perform exact
inference on the structure of F obs (st; ot jt0 ). Thus the complexity of this procedure heavily
depends on the complexity of the network structure of F . However, as we have noted,
due to the nature of the factored representation, F usually has a sparse structure so that
exact inference can be performed eciently. For example, in the special case where F
is completely factored into the product of M independent state variables which are then
independently observed, the complexity becomes linear w.r.t. M .
6. Experimental Results

In this section, we present our experimental results with the policy recognition algorithm. In
subsection 6.1, we demonstrate the effectiveness of the Rao-Blackwellised sampling method
for policy recognition by comparing the performance of our Rao-Blackwellised procedure
against likelihood weighting sampling in a synthetic tracking task. In subsection 6.2, we
present an application of the AHMM framework to the problem of tracking human behaviours in a complex spatial environment using distributed video surveillance data.

484

fiPolicy recognition in the Abstract Hidden Markov Model
N
Rm 6

Rm 5

North Wing
25
Rm 4

36
50

Rm 7

62 E

16
Rm 0

W

7
Rm 3
Rm 1

Rm 2

South Wing

S

Figure 17: The environment and a sample trajectory
Destination probabilities
1
west
south
east
north

Probability

0.8

0.6

0.4

0.2

0
0

5

10

15

20

25

30 35
Time

40

45

50

55

60

Figure 18: Probabilities of top-level destinations over time
6.1 Effectiveness of Rao-Blackwellisation

To demonstrate the effectiveness of the Rao-Blackwellised inference method for AHMM, we
again consider the synthetic tracking task in which it is required to monitor and predict
the movement of an agent through the building environment previously discussed in subsection 3.3. The structure of the AHMM used is the same as the one shown in Fig. 5. The
parameters of the policies are chosen manually, and then used to simulate the movement of
the agent in the building. To simulate the observation noise, we assume that the observation of the agent's true position can be anywhere among its 8 neighbouring cells with the
probabilities given by a predefined observation model.

485

fiBui, Venkatesh & West

0.3
SIS
0.26/sqrt(x)
RB-SIS
0.055/sqrt(x)

std. deviation

0.25
0.2
0.15
0.1
0.05
0
10

20
30
40
Sample size (in 1000)

50

CPU Time (in second)

(a) Sample size and average error
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0

SIS
0.035*x
RB-SIS
0.08*x

10

20
30
40
Sample size (in 1000)

50

(b) Sample size and CPU time
0.25
SIS
RB-SIS

std. deviation

0.2

0.15

0.1

0.05

0
0

0.2

0.4

0.6
0.8
1
1.2
CPU time (in second)

1.4

1.6

(c) CPU time and average error
Figure 19: Performance profiles of SIS vs. RB-SIS

486

fiPolicy recognition in the Abstract Hidden Markov Model

0.003
SIS
0.00179375
RB-SIS
0.000235286

Efficiency coefficient

0.0025
0.002
0.0015
0.001
0.0005
0
0

5

10

15 20 25 30 35
Sample size (in 1000)

40

45

50

Figure 20: Eciency coecients of SIS and RB-SIS
We implement the RB-SIS method (with re-sampling) and use the policy hierarchy
specification and the simulated observation sequence as input to the algorithm. In a typical
run, the algorithm can return the probability of the main building exit, the next wing exit,
and the next room-door that the agent is currently heading to. An example track is shown
in Fig. 17. As the observations about the track arrive over time, the prediction probability
distribution of which main building exit the track is heading to is shown in Fig. 18.
To illustrate the advantage of RB-SIS, we also implement an SIS method without RaoBlackwellisation (LW with ER and re-sampling (Kanazawa et al., 1995)) and compare the
performance of the two algorithms. We run the two algorithms using different sample population sizes to obtain their performance profiles. For a given sample size N , the standard
deviation ((N )) over 50 runs in the estimated probabilities of the top-level policies is used
as the measure of expected error in the probability estimates. We also record the average
time taken in each update iteration.
Fig. 19(a) plots the standard deviation of the two algorithms for different
p sample sizes.
The behaviour of the error follows closely the theoretical curve (N ) = c= N , or 2(N ) =
c2 =N , with cSIS  0:26 and cRB SIS  0:055. As expected, for the same number of samples,
the RB-SIS algorithm delivers much better accuracy.
Fig. 19(b) plots the average CPU time (T ) taken in each iteration versus the sample
size. As expected, T (N ) is linear to N , with the RB-SIS taking about twice longer due to
the overhead in updating the RB belief state while processing each sample.
Fig. 19(c) plots the actual CPU time taken versus the expected error for the two algorithms. It shows that for the same CPU time spent, the RB-SIS method still significantly
reduces the error in the probability estimates.
Note that for each algorithm, the quantity  = 2 (N )T (N ) is approximately constant
since the dependency on N cancels one another out. Thus, this constant can be used as an
eciency coecient to measure the performance of the sampling algorithm independent of
the number of samples. For example, if an algorithm has a twice smaller coecient, it can
deliver the same accuracy with half CPU time, or half the variance for the same CPU time.
Fig. 20 plots the eciency coecients for both SIS and RB-SIS, with SIS  0:0018 and

487

fiBui, Venkatesh & West
RB SIS  0:000235. This indicates a performance gain of almost an order of magnitude
(8 folds) for RB-SIS.

6.2 Application to Tracking Human Behaviours

Using the policy recognition algorithm, we have implemented a real-time surveillance system
that tracks the behaviour of people in a complex indoor environment using surveillance video
data. The environment consists of a corridor, the Vision lab and two oces (see Fig. 21).
People enter/exit the scene via the left or the right entrance of the corridor. The system
has six static cameras with overlapping field of views which cover most of the ground plane
in the scene.
The entire environment is divided into a grid of cells, and the current cell position of
the tracked object acts like the current state in our AHMM. The cameras are calibrated so
that they can return the current position of the tracked object on the ground, however the
returned coordinates are unreliable as the cameras have to deal with noisy video frames and
occlusion of objects in the scene. For more information on how low-level tracking is done
with multiple cameras, readers are referred to (Nguyen, Venkatesh, West, & Bui, 2002).
We assume that the observation of a state can only be in the area surrounding it, thus the
observation model is a matrix specifying the observation likelihood for each cell within a
neighbourhood of the current state.
The policy hierarchy for behaviours in this environment is constructed as follows. First,
we construct the region hierarchy with three levels. At the bottom level, we identify 7
regions of special interest: the corridor, the two oces, the areas surrounding the Linux
server, NT server, printer, and the remaining free space in the Vision lab (Fig. 21). At the
higher level, all regions in the Vision lab are grouped together. The top level consists of the
entire environment. The policy hierarchy representing people's behaviors has three levels
corresponding to the three levels of the region hierarchy (see Fig. 23). At the bottom level,
we are interested in the behaviours that take place within each of the 7 regions of interest.
For example, near the Linux server, the person might be using the Linux machine, or simply
passing through that region, leading to two different policies. Similar policies are defined
for the NT server region, the printer region, and the two small oces. In the corridor
and inside the Vision lab (region 1 and 5), we construct different policies corresponding to
the different destinations that the person is heading to. Region 5 also has a special policy
representing the \walk-around" behaviour. At the middle level, three policies are defined
for the corridor and oce space representing a person's plan of exiting this space by the
left/right entrance or by the door of the Vision lab. We define only one policy for the Vision
lab to represent the typical behaviour of a lab user (e.g., go to Linux server, followed by
go to printer).8 Finally, for the top level region (the whole environment), we define two
policies representing a person's leaving the scene via the left/right entrance.
Fig. 21 and 22 show two concurrent trajectories of two different people in this environment. Some sample video frames captured by the different cameras in the system are shown
in Fig. 24.
With the AHMM model defined above, and a sequence of observations returned by
the cameras, we first determine the performance profiles of RB-SIS and SIS in this real
8. If we consider different groups of lab users, each group might give rise to a different policy at this level.

488

fiPolicy recognition in the Abstract Hidden Markov Model

Camera 2

Camera 5

Table

Book shelf

Table

Office 1

Office 2
Region 7

Region 2

Book shelf

Camera 3

Camera 4
Region 1

left entrance

right entrance

Corridor
Camera 0

Linux server

time slice 300
Region 3

time slice 150 > 180

time slice 50

time slice 60

Printer

1111
0000
0000
1111

Vision Lab

Table

Table

Region 5

Region 6
Region 4

NT server

Camera 1

Figure 21: The environment and the trajectory of person 1

489

fiBui, Venkatesh & West

Camera 2

Camera 5

Table

Table

Office 1

Book shelf

Office 2
Book shelf

Camera 3

Camera 4

Corridor

left entrance

right entrance

time slice 180

Camera 0
Linux region

time slice 260

Linux server

Table

Vision Lab
Table

(Region 5)

Printer

1111
0000
0000
1111

NT region
Printer region

NT server

Camera 1

Figure 22: The trajectory of person 2

Top level

The environment
2 policies

Middle level

Bottom level

Corridor & offices
3 policies

Corridor
5 policies

Office 1
2 policies

Vision lab
1 policy

Office 2
2 policies

Linux region Printer region Empty space
2 policies
2 policies
4 policies

Figure 23: The region and policy hierarchy

490

NT region
2 policies

fiPolicy recognition in the Abstract Hidden Markov Model

(a)
(b)
Figure 24: (a) Person 1 enters the scene and (b) Person 2 enters the scene.

0.6

0.2

std. deviation

0.5

Efficiency coefficient

RB-SIS
SIS

0.4
0.3
0.2
0.1
0

SIS
0.06
RB-SIS
0.011

0.15

0.1

0.05

0
0

0.2

0.4 0.6 0.8 1 1.2
CPU time (in second)

1.4

1.6

0

(a) Error vs. CPU time

0.2 0.4 0.6 0.8 1 1.2 1.4 1.6
CPU time (in second)

(b) Eciency coecients

Figure 25: Performance of RB-SIS and SIS with real tracking data

491

fiBui, Venkatesh & West
1.4
p(left_e)
p(right_ e)

1.2
Probability

1
0.8
0.6
0.4
0.2
0
0

50

100 150 200 250 300 350 400
Time

Figure 26: The probabilities that person 1 is leaving the scene via the entrances (top level
policies)
environment. The two algorithms behave in a similar way as in the previous experiment
with simulated data. Fig. 25 shows the error curve against the CPU time for the two
algorithms. The eciency co-ecient for RB-SIS in this case is RB SIS  0:011, and for
SIS is SIS  0:06. This shows that the RB-SIS still performs about 5 times better than
SIS in this domain.
In the surveillance system, the low level tracking module returns the observations at
the rate of approximately two per second. The observation is then passed to the RB-SIS
algorithm which produces the probability estimate of the current policy at different levels
in the hierarchy. At the moment, our surveillance system can run in real time using two
AMD 1G machines. Examples of the output returned by the system for the two trajectories
in Fig. 21 and Fig. 22 are given below.
Fig. 26 shows the probabilities that person 1 is exiting the environment by the left
or right entrance (denoted by pleft e and pright e respectively). At the beginning, pleft e
increases when person 1 is heading to the left entrance (see the trajectory in Fig. 21). Then,
pleft e is approximately constant from time slice 50 when person 1 is inside the Vision lab.
This is because only one middle level policy is defined for the Vision lab and his movement
inside the lab is independent of his final exit/entrance. At time slice 310, pleft e decreases
when person 1 is leaving the lab, turning right, and entering oce 2. Then, it increases and
approaches 1 when he is leaving oce 2, turning left, and going towards the left entrance.
In contrast, pright e falls quickly to zero during this time.
We now look at the results of querying of the bottom level policies. Fig. 27 shows the
distribution of the possible destinations of person 2 from time slice 180 to time slice 260,
when he is in region 5 (see the trajectory in Fig 22). The probabilities obtained show that
the system is able to correctly detect the \walk-around" behaviour.
The final result (Fig. 28) shows the inferred behaviours of person 1 when he is at the
Linux server region. Initially, the probabilities for \using Linux server" and for \passing
through" are the same. As the person stays in the same position for an extended period of
time, the system is able to identify the correct behaviour of person 1 as \using the Linux
server".

492

fiPolicy recognition in the Abstract Hidden Markov Model

1.4
p(v_Linux)
p(v_printer)
p(v_NT)
p(w_ around)

1.2
Probability

1
0.8
0.6
0.4
0.2

0
180 190 200 210 220 230 240 250 260
Time

Figure 27: Behaviours of person 2 inside the Vision lab

1.4
p(u_Linux)
p(pass)

1.2
Probability

1
0.8
0.6
0.4
0.2
0
50

100

150
200
Time

250

300

Figure 28: Behaviour of person 1 inside the Linux server region

493

fiBui, Venkatesh & West
7. Related Work in Probabilistic Plan Recognition

The case for using probabilistic inference for plan recognition has been argued convincingly
by Charniak and Goldman (1993). However, the plan recognition Bayesian network used
by Charniak and Goldman is a static network. Thus their approach would run into problems when they have to process on-line a stream of evidence about the plan. More recent
approaches (Pynadath & Wellman, 1995, 2000; Goldman et al., 1999; Huber et al., 1994;
Albrecht et al., 1998) have used dynamic stochastic models for plan recognition and thus
are more suitable for doing on-line plan recognition under uncertainty.
Among these, the most closely related model to the AHMM is the Probabilistic StateDependent Grammar (PSDG) (Pynadath, 1999; Pynadath & Wellman, 2000). A comparison of the representational aspect of the two models has been discussed under subsection 3.4.
In terms of algorithms for plan recognition, Pynadath and Wellman only offer an exact
method to deal with the case where the states are fully observable. When the states are
partially observable, a brute-force approach is suggested which amounts to summing over all
possible states. We note that even for the fully observable case, the belief state that we need
to deal with can still be large since the policy starting/ending times are unknown.9 Since
an exact method is used by Pynadath and Wellman, the complexity for maintaining the
belief state would most likely be exponential to the number of levels in the PSDG expansion
hierarchy (i.e., the height of our policy hierarchy). On the other hand, our RB-SIS policy
recognition algorithm can handle partially observable states and the Rao-Blackwellisation
procedure ensures that the sampling algorithm scales well with the number of levels in the
policy hierarchy. Furthermore, as we have noted in subsection 3.4, if we consider compound
policies, the PSDG can be converted to an AHMM. In our framework, a compound policy
k 1 ; : : : ;  k 1 can be represented just as a normal policy, with a slight modification
k = (1)
(m)
to let the variable ek take on values between 1 and m + 1, where the value m + 1 indicates
that the compound policy has terminated. The policy recognition algorithm can then be
modified to also work with this model.
Similar to our AHMM and the PSDG, the recent work by Goldman et al. (1999) also
makes use of a detailed model of the plan execution process. Using the rich language of
probabilistic Horn abduction, they are able to model more sophisticated plan structures
such as interleaved/concurrent plans, partially-ordered plans. However the work serves
mainly as a representational framework, and provides no analysis on the complexity of plan
recognition in this setting.
Other work in probabilistic plan recognition up to date has employed much coarser
models for plan execution. Most have ignored the important inuence of the state of the
world to the agent's planning decision (Goldman et al., 1999). To the best of our knowledge,
none of the work up to date has addressed the problem of partial and noisy observation
of the state. Most, except the PSDG, do not look at the observation of the outcomes of
actions, and assume that the action can be observed directly and accurately. We note that
this kind of simplifying assumptions is needed in previous work so that the computational
complexity of performing probabilistic plan recognition remains manageable. In contrary,
our work here illustrates that although the plan recognition dynamic stochastic model can
9. Of course, if an SRD policy hierarchy is considered then full observability alone is enough.

494

fiPolicy recognition in the Abstract Hidden Markov Model
be complex, they exhibit special types of conditional independence which, if exploited, can
lead to ecient plan recognition algorithms.
8. Conclusion and Future Work

In summary, we have presented an approach for on-line plan recognition under uncertainty
using the AHMM as the model for the execution of a stochastic plan hierarchy and its noisy
observation. The AHMM is a novel type of stochastic processes, capable of representing
a rich class of plans and the associating uncertainty in the planning and plan observation
process. We first analyse the AHMM structure and its conditional independence properties. This leads to the proposed hybrid Rao-Blackwellised Sequential Importance Sampling
(RB-SIS) algorithm for performing belief state updating (filtering) for the AHMM which
exploits the structure of the AHMM for greater eciency and scalability. We show that the
complexity of RB-SIS when applied to the AHMM only depends linearly on the number of
levels K in the policy hierarchy, while the sampling error does not depend on K .
In terms of plan recognition, these results show that while the stochastic process for
representing the execution of a plan hierarchy can be complex, they exhibit certain conditional independence properties that are inherent in the dynamics of the planning and acting
process. These independence properties, if exploited, can help to reduce the complexity of
performing inference on the plan execution stochastic model, leading to feasible and scalable
algorithms for on-line plan recognition in noisy and uncertain domains. The scalability of
the algorithm for policy recognition provides the possibility to consider more complex plan
hierarchies and more detailed models of the plan execution process. The key to achieve this
eciency, as we have shown in the paper, is a combination of recently developed techniques
in probabilistic inference: compact representations for Bayesian networks (context-sensitive
independence, factored representations), and hybrid DBN inference which can take advantage of these compact representations (Rao-Blackwellisation).
Several future research directions are possible. To further investigate the AHMM, we
would like to consider the problem of learning the parameters of an AHMM from a database
of observation sequences, e.g., to learn the plan execution model by observing multiple
episodes of an agent executing the same plan. The structure of the AHMM suggests that
we can try to learn the model of each abstract policy separately. Indeed, if we can observe
the execution of each abstract policy separately, the learning problem is reduced to HMM
parameter re-estimation for level-1 policies, and simple frequency counting for higher-level
policies. If the observation sequence is a long episode with no clear cut temporal boundary
between the policies, the problem becomes a type of parameter estimation for DBN with
hidden variables, and techniques for dealing with hidden variables such as EM (Dempster,
Laird, & Rubin, 1977) can be applied.
Extensions can be made to the AHMM to make the model more expressive and suitable
for representing more complex agents' plans. For example, a more expressive plan execution
model such as the HAM model (Parr, 1998) can be considered so that state-independent
sequences of policies can be represented. The current model can also be enriched to consider
a set of top-level policies which can be interleaved during their execution. We expect that
these new models would exhibit context-specific independence properties similar to the

495

fiBui, Venkatesh & West
AHMM, and Rao-Blackwellised sampling methods for policy recognition in these models
can be derived.
Acknowledgement

We would like to thank the anonymous reviewers for their insightful comments which have
helped improve both the presentation and the contents of this paper. Many thanks to Nam
Nguyen for his implementation of the distributed tracking system used in this paper.
References

Albrecht, D. W., Zukerman, I., & Nicholson, A. E. (1998). Bayesian models for keyhole
plan recognition in an adventure game. User Modelling and User-adapted Interaction,
8 (1{2), 5{47.
Andrieu, C., & Doucet, A. (2000). Particle filtering for partially observed Gaussian state
space models. Tech. rep. CUED-F-INFENG/TR. 393, Signal Processing Group, University of Cambridge, Cambridge, UK.
Bauer, M. (1994). Integrating probabilistic reasoning into plan recognition. In Proceedings
of the Eleventh European Conference on Artificial Intelligence.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
with factored representations. Artificial Intelligence, 121, 49{107.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence in Bayesian networks. In Proceedings of the Twelveth Annual Conference
on Uncertainty in Artificial Intelligence.
Boyen, X., & Koller, D. (1998). Tractable inference for complex stochastic processes. In
Proceedings of the Fourteenth Annual Conference on Uncertainty in Artificial Intelligence.

Brand, M. (1997). Coupled hidden Markov models for modeling interacting processes. Tech.
rep. 405, MIT Media Lab.
Bui, H. H., Venkatesh, S., & West, G. (1999). Layered dynamic Bayesian networks for
spatio-temporal modelling. Intelligent Data Analysis, 3 (5), 339{361.
Bui, H. H., Venkatesh, S., & West, G. (2000). On the recognition of abstract Markov policies.
In Proceedings of the National Conference on Artificial Intelligence (AAAI-2000), pp.
524{530.
Casella, G., & Robert, C. P. (1996). Rao-Blackwellisation of sampling schemes. Biometrika,
83, 81{94.
Castillo, E., Gutierrez, J. M., & Hadi, A. S. (1997). Expert systems and probabilistic network
models. Springer.
Charniak, E., & Goldman, R. (1993). A Bayesian model of plan recognition. Artificial
Intelligence, 64, 53{79.
Cohen, P. R., & Levesque, H. J. (1990). Intention is choice with commitment. Artificial
Intelligence, 42, 213{261.

496

fiPolicy recognition in the Abstract Hidden Markov Model
Cooper, G. F. (1990). The computational complexity of probabilistic inference using Baysian
belief networks. Artificial Intelligence, 42, 393{405.
Dagum, P., & Luby, M. (1993). Approximating probabilistic inference in Bayesian belief
networks is NP-hard. Artificial Intelligence, 60, 141{153.
Dagum, P., Galper, A., & Horvitz, E. (1992). Dynamic network models for forecasting. In
Proceedings of the Eighth Annual Conference on Uncertainty in Artificial Intelligence,
pp. 41{48.
D'Ambrosio, B. (1993). Incremental probabilistic inference. In Proceedings of the Ninth
Annual Conference on Uncertainty in Artificial Intelligence, pp. 301{308.
Dawid, A. P., Kjrulff, U., & Lauritzen, S. (1995). Hybrid propagation in junction trees. In
Zadeh, L. A. (Ed.), Advances in Intelligent Computing, Lecture Notes in Computer
Science, pp. 87{97.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation.
Computational Intelligence, 5 (3), 142{150.
Dean, T., & Lin, S.-H. (1995). Decomposition techniques for planning in stochastic domains. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence (IJCAI-95).
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical Society B, 39, 1{38.
Doucet, A., de Freitas, N., Murphy, K., & Russell, S. (2000a). Rao-Blackwellised particle filtering for dynamic Bayesian networks. In Proceedings of the Sixteenth Annual
Conference on Uncertainty in Artificial Intelligence.
Doucet, A., Godsill, S., & Andrieu, C. (2000b). On sequential Monte Carlo sampling methods for Bayesian filtering. Statistics and Computing, 10 (3), 197{208.
Forestier, J.-P., & Varaiya, P. (1978). Multilayer control of large Markov chains. IEEE
Transactions on Automatic Control, 23 (2), 298{305.
Fung, R., & Chang, K. C. (1989). Weighting and integrating evidence for stochastic simulation in bayesian networks. In Proceedings of the Fifth Conference on Uncertainty
in Artificial Intelligence.
Geweke, J. (1989). Bayesian inference in econometric models using Monte Carlo integration.
Econometrica, 57 (6), 1317{1339.
Ghahramani, Z., & Jordan, M. I. (1997). Factorial hidden Markov models. Machine Learning, 29, 245{273.
Goldman, R., Geib, C., & Miller, C. (1999). A new model of plan recognition. In Proceedings
of the Fifteenth Annual Conference on Uncertainty in Artificial Intelligence.
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchical
solution of Markov decision processes using macro-actions. In Proceedings of the
Fourteenth Annual Conference on Uncertainty in Artificial Intelligence.
Henrion, M. (1988). Propagating uncertainty in Bayesian networks by probabilistic logic
sampling. In Lemmer, J., & Kanal, L. (Eds.), Uncertainty in Artificial Intelligence 2,
Amsterdam. North-Holland.

497

fiBui, Venkatesh & West
Huber, M. J., Durfee, E. H., & Wellman, M. P. (1994). The automated mapping of plans
for plan recognition. In Proceedings of the Tenth Annual Conference on Uncertainty
in Artificial Intelligence.
Jelinek, F., Lafferty, J. D., & Mercer, R. L. (1992). Basic methods of probabilistic context free grammar. In Laface, P., & Mori, R. D. (Eds.), Recent Advances in Speech
Recognition and Understanding, pp. 345{360. Springer-Verlag.
Jensen, F. (1996). An Introduction to Bayesian Networks. Springer.
Jensen, F., Lauritzen, S., & Olesen, K. (1990). Bayesian updating in recursive graphical
models by local computations. Computational Statistics Quarterly, 4, 269{282.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to
variational methods for graphical models. Machine learning, 37 (2), 183{233.
Jordan, M. I., Ghahramani, Z., & Saul, L. K. (1997). Hidden Markov decision trees. In
Mozer, M. C., Jordan, M. I., & Petsche, T. (Eds.), Advances in Neural Information
Processing Systems 9, Cambridge, MA. MIT Press.
Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. Transactions of the American Society of Mechanical Engineering, Series D, Journal of Basic
Engineering, 82, 35{45.

Kanazawa, K., Koller, D., & Russell, S. (1995). Stochastic simulation algorithms for dynamic probabilistic networks. In Proceedings of the Eleventh Annual Conference on
Uncertainty in Artificial Intelligence, pp. 346{351.
Kautz, H., & Allen, J. F. (1986). Generalized plan recognition. In Proceedings of the Fifth
National Conference on Artificial Intelligence, pp. 32{38.
Kjaerulff, U. (1992). A computational scheme for reasoning in dynamic probabilistic networks. In Proceedings of the Eighth Annual Conference on Uncertainty in Artificial
Intelligence, pp. 121{129.
Kjrulff, U. (1995). dHugin: A computational system for dynamic time-sliced Bayesian
networks. International Journal of Forecasting, 11, 89{111.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical
Society B, 50, 157{224.
Liu, J. S., & Chen, R. (1998). Sequential Monte Carlo methods for dynamic systems.
Journal of the American Statistical Association, 93, 1032{1044.
Murphy, K., & Russell, S. (2001). Rao-blackwellised particle filtering for dynamic Bayesian
networks. In Doucet, A., de Freitas, N., & Gordon, N. J. (Eds.), Sequential Monte
Carlo Methods in Practice. Springer-Verlag.
Murphy, K. P. (2000). Bayesian map learning in dynamic environments. In Advances in
Neural Information Processing Systems 12, pp. 1015{1021. MIT Press.
Nguyen, N. T., Venkatesh, S., West, G., & Bui, H. H. (2002). Coordination of multiple
cameras to track multiple people. In Proceedings of the Asian Conference on Computer
Vision (ACCV-2002), pp. 302{307.

498

fiPolicy recognition in the Abstract Hidden Markov Model
Nicholson, A. E., & Brady, J. M. (1992). The data association problem when monitoring
robot vehicles using dynamic belief networks. In Proceedings of the Tenth European
Conference on Artificial Intelligence, pp. 689{693.
Parr, R. (1998). Hierarchical control and learning for Markov Decision Processes. Ph.D.
thesis, University of California, Berkeley.
Parr, R., & Russell, S. (1997). Reinforcement learning with hierarchies of machines. In
Advances in Neural Information Processing Sytems (NIPS-97).
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA.
Pearl, J. (1987). Evidential reasoning using stochastic simulation of causal models. Artificial
Intelligence, 32, 245{257.
Pynadath, D. V. (1999). Probabilistic grammars for plan recognition. Ph.D. thesis, Computer Science and Engineering, University of Michigan.
Pynadath, D. V., & Wellman, M. P. (1995). Accounting for context in plan recognition, with
application to trac monitoring. In Proceedings of the Eleventh Annual Conference
on Uncertainty in Artificial Intelligence.
Pynadath, D. V., & Wellman, M. P. (2000). Probabilistic state-dependent grammars for
plan recognition. In Proceedings of the Sixteenth Annual Conference on Uncertainty
in Artificial Intelligence.
Rabiner, L. R. (1989). A tutorial on Hidden Markov Models and selected applications in
speech recognition. Proceedings of the IEEE, 77 (2), 257{286.
Sacerdoti, E. (1974). Planning in a hierarchy of abstraction spaces. Artificial Intelligence,
5, 115{135.
Shachter, R. (1986). Evaluating inuence diagrams. Operations Research, 34, 871{882.
Shachter, R. D., & Peot, M. A. (1989). Simulation approaches to general probabilistic
inference on belief networks. In Proceedings of the Fifth Conference on Uncertainty
in Artificial Intelligence.
Sutton, R. S. (1995). Td models: Modelling the world at a mixture of time scales. In
Proceedings of the Internation Conference on Machine Learning (ICML-95).
Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDP and semi-MDPs: A framework
for temporal abstraction in reinforcement learning. Artificial Intelligence, 112, 181{
211.
van Beek, P. (1996). An investigation of probabilistic interpretations of heuristics in plan
recognition. In Proceedings of the Fifth International Conference on User Modeling,
pp. 113{120.
York, J. (1992). Use of Gibbs sampler in expert systems. Artificial Intelligence, 56, 115{130.

499

fi
	ff
fi 
			 ! #"$ % 
'&)(+*-,/.00.213( 42*+5'(+*
0

6789:  ;)( .2<
0!(=?>8	%&;A@2<
0.

B$CEDGFEHJI7CEKLB$CNMO-PRQEP3OSMTVUXWOZY\[VM]I7CEK_^`WTbaca]IdGO-P3afegI
M3hE[iFjMlkmP3TbDGI7CEK
n

H:HJFEP3TbHSoqpr[)PtsjIuvQXWIHJI
Mwoyx`kzhEO-[)P3OSM3I2H:TVW|{}OJaFXWMaS~
n

n

QXQEP3[JEIuLTM3I2[NC

WK)[iPRI
M]hXua~yTVCEDEQyO-PRIuLOCNMa

ff#$`|3

cRiJ$c$$$$$ $#ff

c
A
 2r?
 -$$ ' w2 2'2r+
 -ffEc
'?+rVw
!22
:Sff?
77

2]


r3
J727#rr2!]- +i?rr+b2/r 7+r?r++]rSrR7/?
 j+7r2+/r$7+
J27r++:N+)2E:r]7+)V++
'
r
7 ff+ r+7 +? +X+7?

mr-2/r ?+r  X7+|r
G?r++ Nr+? 77 X77m+ r? +7++?E2/r 77
 S2r
 
	
 !X?R:+7-rVrAj/+7 ):G+rjGr
7:+ff
 
r7
++| +r+ 
?/ !  +:ffi7 ?
E!?y

X?
fi
 S ++?/b?b/+G +G?+ ++/?G? 7?
r7 
  
 ]ffN7+b7+VffG+i+G+j?
!+br:+
+/2
 r72
r +
r
 yrr?2/r 77
 SXVffi?X
 7ff
 +
 :
r
 E+rm/
 !"crr7yrt+!27V7+)r+#!
 

7r+?iff
?r 
#
 '3/Ar
 
ff:!r+
 3? ]+Nr?
++A )r)/+i7
 
E!?
?r 
%$+Nr):/
Srff& 
  r+7S72/++2+r(
 '??S +ri7r+)rN7#) 

$rV rR) 
 2/r 
b:+ ff + '* 
rr
7+ ' ,i !)E& .
ff:
2+r
Effr+  2!+ :?++7rAr+! 7
/
 +ff
 
 
ff!r+ + ? rr+
 01+! r++77+!2 r  +! 
 rr ?rV?++/2/?V7r+7Arff +2
 
ff: br yff +G? 
?3
 
ffE+
ff +5
 476
?:7+
Nr|+ 8
 'rX ?/
 # r:/
 +!r r/?
 
)/ i+?N?9
 1+!+V+-+r: ffr
+
 r+r7+
r++!

;7<2=8> 3?@BA-3CD? >
E"FGHFI(JLKM#N8KIOGHFPRQSI2JT9FUPVJW9M#X-YZ7NYJQSI#[ffG]\SKPPQ_^`G]KaJQ_Y3IK\_[Yb/QSJT9cPdT9KNF	eObY3W#[3T(JdFc2fOQ_bQSG]K\#KI9M
JT#F]YbF]JQgG]K\bFPW9\SJPhG]\SFKab/\_XiP/T#YjUQgI#[kJT#FM9QSPGHbQScQSI9KaJQSY3IlfmYjnF]boYZBFIOPVFc!e`\_F2G]\SKPPQ_^OF]b/P2p.qnKW9F]b
rts Y3TOKNuQwv`xyyy{z#qnbFQScKIvOxyy|{z#}hQSF]JVJVF]b/QSG/Tv{~auzOhfOQ_JV r KG]\SQgI7vOxyyy{zOuG/TOKafOQ_bF r #QSI#[F]bv
xyy3k"T#FPVFcF]JT#Y{MOPoeOKPQSG]K\g\_XbF\SXY3INYJQgI#[lJT#FM#FG]QgPQ_Y3IYZLQSI9M9QSN{QSMOW9K\G]\SKP/PQ_^9F]b/PoQgI9PQSM#F
KI%FI9PVFcffeO\_FJnQgPBj	QSM#F\_X2KG]GHF]f9JVFM7v`KI9M%ZYb/cK\S\_Xf9bYNFIQSIGHF]b/JKQSIG]KPVFPpDuGT9KafOQ_bFvu#bFWOI9M7v
qLKabJ\_F]JVJ]v r F]Fvhxyy{zfiuG/T9Kaf`Q_bF r uQSI#[F]bvUxyy3vLJT9KaJ2JT#FQ_b2fmYjnF]bKGHJW9K\g\_XbF\gQ_FP-Y3IJT#F
KaeOQS\gQ_JX%JVYeOW9Qg\SMkfmYJVFIJQSK\g\_XHk\gKab[F!G]\SKPP/Q_^9F]b/P]R)JUTOKP"F]NFIieF]FIYeOPVF]bNFMFH{fmF]b/Qgc2FIJK\g\_X
JT9KaJPW9GTKI1FIOPVFc!e`\_FiG]KItPVY3cF]JQSc2FP%emFiKP\SKab[F5KPp:Ybl\SKab[F]b%JT9KI`JT#F5M9KaJKW9PVFM1JVY
eOW9Qg\SMJT#FFI9PFc!eO\SFp  Kab/[3QSI#FKI(JW r }hQSF]JVJVF]b/QSG/Tvxyy3lt"T9FI7vnKPQSc2f`\_Fl(W9FPVJQ_Y3IKab/QgPVFP]v
I9Kc2F\SX2jUT9KaJBQgPRJT#FfiQSI(JVF]bFPVJnKffG]W9PJVY3c2F]bG]KI%TOKNFoQSIWOPQSI#[!P/W9G/TkK!G]\gKPPQ_^9F]bv(QSI9PJVFKMYZP/QSc2fO\_F


.00. %%`!;V
;LA2
n3
!fi9J
R8!	%&%		r&2%%g2 ;

fi

\_Y{Y{W9fOPdQSIJT#F	M9KaJKuv#KI9MW9PQSI9[!K\_[Yb/Q_JTOcPRPWOG/T%KPBI#FKab/FPVJBI#FQS[3TemYbBG]\gKPPQ_^9F]bPUp  Kab/[3QSI#FKI(JW
r }fiQ_F]JVJVF]b/QgG/T7vxyy3
 ZJVF]b%PVY3c2FYZfiJT#Flc2Y3PJbFcKab aKaeO\_FkbFGHFI(JPVJWOM9Q_FP2QSINYJQSI9[G]\SKPPQS^OG]KaJQ_Y3IK\_[Yb/QSJT9cP]v
PVY3c2F"KW9JT#Yb/PdT9KNF"fmY3QSI(JVFMffY3W9JJT#F"QSI(JVF]bFPVJJVYe9bQSI#[hJT9QSP G]\SKPPQS^OG]KaJQ_Y3IfmYjnF]bdJVY!M9KaJKocQSI9QSI#[#v
KI9MlcYbFof9bFG]QSPF\_XJVYcK FffQSI(JVF]bf9bF]JKae`QS\SQ_J)XKG]\_FKabUQgPPW#FoQSIkNYJQSI9[G]\SKPPQS^OG]KaJQ_Y3IK\_[Yb/QSJT9cP
p.qLKW#F]b r s Y3T9KN{Qwv0xyyy{zE	QSM#[F]jLKXv  KM9Q_[3KIvOEUQSGT9Kab/M9PY3I7v r 	
  s KI9Fvxyy3U{Y3c2F!KW#JT9Yb/P
[YF]NFIZ.W#bJT#F]bv0KI9MKab[3W9FJT9KaJ!JT9FQSc2fmYbJKI9GHF2YZ"QgIJVF]bfObF]JKaeOQS\SQSJXiTOKPoemF]FIcKab[3QgI9K\SQ_]FM
QSIkJT#FM9FPQ_[3IkYZ JT#FPVFK\_[YbQ_JT9cP]v#KI9MlfOW#JLemFT9QSI9M%JT#FI#F]FMlJVYM#F]NuQSPVFoG]\SKPPQS^9F]b/PLjUQ_JTPJVbY3I#[
G]\SKPP/Q_^OG]KaJQ_Y3IfYjBF]bop.EUQgM#[F]jnKX2F]JnK\D_vxyy3RqLW#JnQSI(JVF]bf9bF]JKaeOQg\SQ_JXK\SPVYff[YNF]b/IOPBJT9F	{W9K\SQ_J)X-YZ
K-c2Y{M9F\Oe{Xf9bYN{QgM9QSI#[KI9PjBF]b/PnJVY2T#YjQSJBQSPBjBY
b {QSI9[#v{KI9Mv{c2Y3PJnQScfYb/JKIJ\_Xv(jUT(X  G]GHYb/MOQSI#[
JVYqLKW#F]b rs Y3T9KN{Qp+xyyy3vLPVJVb/QSN{QSI9[iZ:YbGHY3c2f9bFT9FI9PQ_eOQg\SQ_JX5QSINYJQSI#[c2YuM#F\SPQSP-Y3I#FYZ	JT#F
f9b/QgI9G]Q_fOK\f9bYeO\SFcPmbF{W9Q_b/QSI9[RZ.W#JW#bFdQSI(NFPVJQ_[3KaJQSY3I9P]0UT#F]XhK\gPVYbFcKab hJT9Kafi
J ff+NYJQgI#["JVFG/TOI9QS{W#FP
W9PWOK\S\_XbFP/W9\_JQSIQSI9GHY3cf9bFT#FI9P/Q_eO\_FhG]\SKPPQ_^OF]b/PLJT9KaJUG]KI9I9YJ"FKPQS\_X%emFoPT#Yj	IJVYW9PVF]bP u
 Y3cf9bFT#FI9P/Q_eOQS\SQSJXQgP]vY3IJT#FiYJT9F]blT9KI9M7v"KT9Kab/McQSI9QgI#[QSPP/W#Fp.qLW K r  F]Fvh~a#x
 
Q_J!M9F]fFIOM9PfiY3IfOKab/KcF]JVF]b/PffP/W9G/TKPffJT#FJ)X(fmFYZG]\SKP/PQ_^9F]b/PoWOPVFM7vJT9FK\_[Yb/Q_JTOc QSI9M9WOG]QSI#[kJT#F
G]\SKPP/Q_^9F]b/P]v{JT#FoW9PVF]bcQgI9QSI#[ffJT#FfiY3W#JVfOW#JP]vuF]JGa"dUT#Y3W#[3T%JT#F{W9KIJQS^OG]KaJQ_Y3I%YZ0QSI(JVF]bf9b/F]JKaeOQS\SQ_J)X
QSPPJQS\S\YfmFI#FMQgIJT#F[FI9F]b/K\RG]KPVFp.qLW K r  F]Fv ~a#xvJT9F]bFKabF%PVY3c2F%G]\SW#FPGHY3cQgI#[lZ:bY3c
JT#F]YbX-KI9M-f9b/KGHJQSGHFLYZcKG/T9QgI#F\_FKab/I9QSI9[ 8M9KaJK	cQSI9QSI#[	QSI9M9QSG]KaJQgI#[	PVY3cFnfmYJVFI(JQSK\S\_XffQSIJVF]b/FPVJQSI#[
bF{W9Q_bFcFIJPKI9MGHY3c2f9bY3cQgPVFPJVYM#F]NuQSPVFKIlF G]Q_FI(JU\_FKab/I9QgI#[ 8cQSI9QgI#[ffK\_[YbQ_JT9c
 ^9bPVJobF{W9Q_bFc2FI(JZ:Yb!JT#F%K\_[Yb/Q_JTOc QSPoYe{NuQ_Y3W9P\_X5Q_JP![FI#F]b/K\SQ_KaJQSY3IKaeOQS\gQ_JQ_FP j	Q_JT#Y3W#J
G]\SKPP/Q_^OG]KaJQ_Y3IkPVJVbFI9[JT7v9Q_JLQSPnfY3QgIJ\_FP/PBJVYPFKab/G/TkZ:Yb"QSI(JVF]bFPVJQSI#[c2YuM#F\SPnYZJT9FM9KaJKu  PFGHY3I9M
r  KPG]W#F\wvnxyy {z
bF{W9Q_bFcFIJ]vcYbFbF\SKaJVFMJVYcQSIOQSI#[#vQSPoJT9FPQ_]FYZLJT#FG]\SKP/PQ_^9F]b/Pp 	YuG  
r  Kaf9f{Xvxyy3)Z"KG]G]W#bKaJVFvRKiG]\gKPPQ_^9F]bffjUQ_JTbFPVJVb/QSGHJVFMPQ_]F%G]KI\_FKMJVY5Z.KPVJVF]b2KI9M
	Y{
G  
M#F]F]fmF]bLW9I9M#F]b/PJKI9M9QSI#[# "T9QSPBQSPBYe(NuQ_Y3W9P\SXI#YJnKIkKaeOPY3\SW#JVFUbW9\_Fv{b/KaJT#F]bLKI%KafOf9bY#QScKaJVFhf9bY{X
Z:YbQSIJVF]b/f9bF]JKaeOQS\gQ_J
X hfOKaJT#Y3\_Y[3QSG-G]KPVFPoFH#QSPVJoQSIijUT9QgG/T7vmZ:YbhFH#Kc2fO\SFvK%\gKab[F2KI9M5W9Ie`K\SKI9GHFM
JVbF]FG]KI emFNF]bXPQSc2fO\SF%JVYW9I9M#F]b/PJKI9M p.qLW VK r  F]Fv"~a#x
 UYJVFiJT9KaJQSIJTOQSPFHuKcfO\_Fv
JT#FoKW#JT#YbPLFH{fO\gKQSIJTOKaJJT#FfiJVbF]FQSPLPQSc2f`\_FUemFG]KW9PVFoK\S\mQ_JPI9Y{M#FPLG]KIkemFoM#FPGHb/QSeFM%W9PQgI#[!Z:F]j
!#"$&%  % "T#F]b/F]ZYbFv3P/QSc2fO\SQgG]Q_JXoQSP K\SPVYoKPPVYuG]QSKaJVFMJVYoKfiPT#YbJ M#FPGHbQ_f9JQ_Y3I7vaeOW9J W9P/QSI#[	KhfOKab/JQSG]W9\SKab
G]\SKPP"YZ GHY3IOGHF]f9J"bF]f9b/FPVFIJKaJQSY3I7
 JT9Q_bMfff`Kab/Kc2F]JVF]bBQg(
I 'OW#FI9G]QgI#[hGHY3cf9bFT#FI9P/Q_eOQS\SQSJXQSP JT#F	I9KaJW#bFYZmJT#F"K\_[YbQ_JT9)
c  PY3W9JVfOW#J]
+I9PQSM#FRJT#FLe9bY3KM-PGHYfmFBYZOPX{cffemY3\SQSGRG]\gKPPQ_^9F]bP]v8PY3c2FnG]\gKPPVFP YZ`GHY3I9GHF]fOJ b/F]f9bFPVFI(JKaJQ_Y3I9P Kaf9fmFKab
JVY5Y *F]b2Ki[bFKaJVF]bGHY3c2Z:YbJ-ZYb2QSI(JVF]bf9bF]JKaJQSY3I7i}hFG]QSPQSY3IJVbF]FPemF\_Y3I9[JVYJT9QSP-PVF]Jkp.qnbFQgcKI7v
#bFQgM9cKI7vo\SPT#FI7v r {JVY3I#Fv0xy +(vJT#Y3W#[3TJT#F]XK\SPVYbKQSPVF!PVY3cF!QSI(JVF]bf9bF]JKaeOQg\SQ_JXf9bYeO\SFc,
P 
s Y3T9KN{Q r {Y3cc2F]b/^9F\SMp+xyy3{W#YJVFJTOKaJ
ff+JT9FLG]\gQ_FIJP.- eOW9P/QSI#FPP0W9PVF]b/P0/uZ:Y3W9I9M-PVY3c2FLQSIJVF]b/FPVJQSI#[	fOKaJVJVF]b/I9P

SQ IffJT#FLM#FG]QSPQ_Y3IffJVbF]FP]v
e`W#JBJT9F]XM9QSMI#YJLZF]F\JT#FoPVJVbW9GHJW#bF	jnKPI9KaJW9b/K\ZYbLJT#FcdUT#F]XjnF]bFo\_Y(YuQSI#[ffZYb
JT9Y3PVF!J)jBYYb	JT#b/F]F!KaJVJVb/Q_eOW9JVFPUKI9MNaK\SW#FP-p214351LKGHY3c!eOQgI9KaJQ_Y3IYZ[F]Y[b/Kaf`T9QSGKI9M
QgI9M9W9PVJVbQ_FP0jUT#F]b/F	PVY3c2F]JTOQSI#[ffVQgIJVF]bFPJQSI#[6jLKPLT9Kaf9fmFI9QSI9[#+IKM9MOQ_JQ_Y3I7v3JT9F]XZF\SJ
QSJdjnKPBJVY{Y-\SQgcQ_JQSI#[fiJT9KaJBJT#F	I9Y{M#FPRQSIKffM#FG]QSPQ_Y3IJVbF]FUbF]fObFPVFI(JRb/W9\SFPJT9KaJLK\S\9PVJKabJ
j	Q_JTkJT#F!PKcF!KaJVJVb/Q_eOW9JVFP]7 
 \_JT#Y3W#[3TI9YJ\SQScQ_JQgI#[Z:bY3c KG]\SKPP/Q_^OG]KaJQ_Y3INuQ_F]j"fmY3QSI(J]vdJT#FYb/M9F]b/QSI#[YZhI9Y{M#FPfOb/Q_YbJVY
]G \SKPP/Q_^OG]KaJQ_Y3IG]KIJT9F]bF]ZYb/FcKF!Q_JW9I9GHY3cZYbJKae`\_FoJVYcQSI#FoK2M#FG]QgPQ_Y3IkJVbF]F8	YJQSGHFJTOKaJJT9QSP
f9bYe`\_Fc cQS[3TJT#Y3\SMZYb2KI(XG]\SKPPffYZUGHY3I9GHF]f9JbF]fObFPVFI(JKaJQ_Y3IQSIJVF][bKaJQSI#[iKIYb/M9F]b/QSI#[if9b/Q_Yb
JVYkG]\SKPPQ_^`G]KaJQ_Y39I BM9FG]QSPQ_Y3Ii\SQSPVJP!p.E	Q_NFPVJ]v xy3vK\_JVF]b/I9KaJQgI#[M#FG]QSP/Q_Y3IlJVbF]FPp.#bFWOI9M r  KPY3I7v
:<;>=

fi

iwc

	
ffff

xyyy3v7e9b/KIOG/T9QSI9[2f9bY[b/KcPp  KI9PVY3W#b r  G  \g\_FPVJVF]bv7~av7F]JGaU"T9F]bFFH#QSPVJP]vmT#YjnF]NF]bv7K
JX{fmFLYZ`G]\SKPP/Q_^9F]b/P Y3IjUT9QgG/T-bF\SKaJVFMfOKafmF]b/P Kaf9fmFKabJVYoemFB[FI9F]b/K\S\_X!WOI9KI9QSc2Y3WOPY3IJT#FQSb cQSI9QSI#[
KaeOQS\gQ_JQ_FP  M9QgP VWOI9GHJQ_NFBI9Yb/cK\{ZYbc ZYb/c-W9\SKP"p.} h P]v(KI9M-JT#FQSbdI(W9cF]bY3W9P FH{JVFIOPQ_Y3I9PvJT9KaJdQSP]v
M9QSP VW9IOGHJQ_Y3I9P"YZBGHY3I VWOI9GHJQ_Y3I9P]o+I(JVF]bFPVJQSI#[3\SXFI#Y3W#[3T7v7JT9QgP	QSPhJT#F2G]\SKP/P	j	T9QSG/Tc2YJQ_NaKaJVFM5FKab/\_X
jnYb {Pop.KI9MkK-[bFKaJKc2Y3W9I(JLYZjnYb {PLKaZ:JVF]bjLKab/M9PRY3I%JT#FhjBF\S\ fi {I#YjU
I    JT9F]YbXYZ0\SFKab/I9QSI#[
p dK\SQSKI(J]vUxy +#vUxy 3vBfOKabJ\_XeFG]KWOPVFYZUJT#FkJVFI9M#FI9GHXT{W9cKIOP-PF]Fc JVYT9KNFJVY5bF]f9bFPVFI(J
uI#YjU\SFM#[FW9PQgI#[PQScQS\gKab/\_XlPT9KafmFM5b/WO\_FPp dK\SQSKI(J]vnxy 3"TOQSPfiG]\SKPP!QgPoK\SPVYlJT#FM9W9K\ YZLJT#F
Y3I#FQScfO\SQSG]Q_J\SX%W9PFM5e(XqnW VK r  F]Fpw~a#xoJVYlG]KPVJoJT#FQ_bP/Q_]F2c2FKPW#bFZ:YbM#FG]QSPQSY3IiJVb/F]FPp:JVY
PVJKaJVFffjUT#F]JT#F]bJT#FffGHY3I9GHF]f9J"bF]fObFPVFI(JVFMQSPPQgc2fO\_FhYb	I#YJ
JUQSPLY3W#b	KQSc QSIJTOQSPLfOKafmF]bJVYf9b/YfY3PFhJT9F]YbF]JQSG]K\b/FPW9\_JP"KIOMlKaf9f9bYuQgcKaJQ_Y3IK\_[Yb/QSJT9cP
bF\SKaJVFMiJVY%JT#F-QgI9M9W9GHJQ_Y3IkYZRNF]bXfOKabJQSG]WO\SKabUNYJQSI9[%G]\SKP/PQ_^9F]b/PvOM#b/KjUQSI#[JT9FQ_bUbY{YJPhY3IiP/QSc2fO\_F
b/W9\SFUPVF]JPp.\S#Q Fh} 	dvujUQ_JTJT#FhYe +FGHJQSNFhJV	
Y F]F]flKffJVb/KM#F]Y *emF]JjnF]FIPQScfO\SQSG]Q_J)X-KIOMKG]G]W#bKGHX
fiW#bkKQSc QSP%K\SPVYJVYf9bYNFJT9KaJ]vfiQSI JT9F5I{W9c2F]b/Y3W9PkQSI9M9W9GHJQSY3I K\_[Yb/QSJT9cPkK\_bFKM#X f9bYfmY3PVFM
JT#bY3W9[3T#Y3W#JhJT#F-cKG/T9QSI9Fff\_FKab/IOQSI#[KIOMiM9KaJKkcQgI9QSI#[GHY3cc-W9I9QSJQ_FP]vmPVY3c2F-YZdJT#Fcvmf9bF]NuQ_Y3W9P/\_X
W9PVFMiQSIM9FG]QSPQ_Y3IJVb/F]FP	KI9MM#FG]QSP/Q_Y3Il\SQSPJP"QSI9M9WOGHJQ_Y3I7vOG]KIiemFFKPQS\_XKM9KafOJVFMJVY%GHYfFffjUQ_JTJT9QSP
Ye VFGHJQ_NFvaJT#F]bF]e{X\_FKM9QSI9[JVY"FKPVX fiDJVY fiQSc2fO\_FcFIJLp.KI9M!GHY3c2f`KabF7K\_[YbQ_JT9cP] "T#FdI#FHuJ0PFGHJQ_Y3I
f9bFPFIJP"KPVX{I(JT#FPQgPLYZ Y3W9bUGHY3IJVbQ_eOW#JQ_Y3Iv{j	T9QSG/TQgPM#F]JKQS\_FMlQgIkJT#FbFPVJYZ JT#Fof`KafF]b
 < AJ? >

C BA:3C?

>

 T9QgPBf`KafF]b"QgPBfOb/QSI9G]Q_f`K\S\_X2GHY3I9GHF]b/I#FMljUQ_JT%JT#FoJT#F]YbF]JQgG]K\KI9MkFHufF]bQSc2FI(JK\7PVJW9M#XYZ0KPVF]JYZ
"
NYJQSI#[G]\SKP/PQ_^9F]b/P	jUT9QgG/TjBFJT9QgI(QSPh\SQ#F\_XJVY%fObYNuQSM#FKIKG]G]W#b/KaJVFKI9PjBF]boJVYkJT#F2PQScfO\SQSG]Q_J)Xfi
KG]G]W#b/KGHXJVb/KM#F]Y *  M9FG]QSPQ_Y3IGHY3ccQ_JVJVF]FPfip.}  "p 	Y{
G  r  KP/G]W#F\wvmxyy 3R}  QSPRQSI#Z:Yb/cK\S\_XffJT#F
qnY(Y3\SFKIcffWO\_JQSG]\SKPPhFHuJVFI9PQ_Y3I5YZBfmY3\_X{I9Y3cQSK\ M9QSPGHb/QgcQSI9KI(JUZ.W9I9GHJQ_Y3IOP]  M#FG]QSPQ_Y3I5GHY3ccQ_JVJVF]F
GHY3I(JKQSI9PdbW9\_FP]v3FKGT%YZJT9FPVFUemFQSI#[KfOKQ_bhp.cY3I#Y3cQSK\wvNFGHJVYb
 RKGTcY3I#Y3cQSK\9QgPRKGHY3I9MOQ_JQ_Y3I
JT9KaJ]v	jUT#FI^ObFM7vbF]JW#bI9PQ_JPNFGHJVYb  ZJVF]bkFKG/TtcY3I#Y3cQSK\	T9KP%eF]FI1JVFPVJVFM7vUJT#FP/W9c YZ
JT#FbF]JW9b/I#FMNFGHJVYb/PQgP%WOPVFMJVYJK FJT#FM#FG]QSP/Q_Y3I7 "TOQSPkKM9M9Q_JQ_NFZ.KPT9Q_Y3IZ:YblGHY3c!eOQgI9QSI#[
b/W9\SFP!QSPffKaeOPVFI(JffZ:bY3c G]\SKPPQSG]K\BqnY(Y3\_FKIG]\SKP/PQ_^9F]b/P!PW9G/TKP}hFG]QSPQ_Y3Ib/F]FP%p.}fi	Yb-}hFG]QSP/Q_Y3I
 QgPVJP"p.}  9W9bJT#F]b/c2Yb/FvW9I9\SQ FBJT#FPFJjnY!\SKaJVJVF]bRG]\SKP/PVFP]v3JT#F"G]\gKPPQ_^9F]bGHY3I(JKQSI9PdKae`PVY3\SW#JVF\_X-I#Y
Yb/M#F]bQSI#[#vI#FQ_JT#F]b Y3I-NaKab/QSKaeO\_FPp.W9IO\S#Q FB}fi	v3I9Yb Y3Ic2Y3I9Y3cQSK\SPp.W9I9\g#Q FB}   T#FIP(W G]Q_FIJ\SX
PcK\g\}  PKabFeOWOQS\_JfiKI9MKM#F{W9KaJVFb/FPVJVb/QSGHJQ_Y3IOPoKabFJK FI7v KI#F]j M9Qgc2FI9PQ_Y3I5QSIQgIJVF]bfObF]JQSI#[
JT#FG]\SKPP/Q_^9F]b2QSP-Ye9JKQSI#FMvjUT9QSGTM9Y(FP2I#YJFH#QSPVJZ:Yb}h Yb2}   hKc2F\_XvnKIXFHuKcfO\_FG]KI
PKaJQSPZXcYbFfiJT9KI%Y3I#Ffib/W9\_FvuKI9MK}  G]KIkJT9F]bF]ZYb/FhemFoQSI(JVF]bf9bF]JVFM%e{Xc2FKI9PLYZ0N8KabQ_Y3W9PBb/W9\_F
%<$ %   % p.QSIK2IOKQ_NFoGHY3INF]b/P/Q_Y3IYZ K2}h YbUK2}  QSI(JVY2b/W9\_FfiPVF]JP]vOKI(X%FHuKc2f`\_FoPKaJQSPV^9FPLFH#KGHJ\_X
Y3I#FlbW9\_F }hFG]QSPQSY3IGHY3ccQ_JVJVF]FPbFPVFcffeO\_FYb[FI#F]b/K\SQS]FlYJT#F]bbW9\_FPVF]JPip  Y3T#FI r #QSI#[F]bv
xyyy3 +I1JT9QSPfOKafmF]bv"JT#FKW#JT#YbP%GHY3IOPQSM#F]bk} 	 fiPT9KafmFMZ:Yb/cffWO\SKP]vUQSI1jUT9QgG/T1JT#FiY3W9JVfOW#J
YZKcY3I#Y3cQSK\"QgPI#YJKG]\SKPPp.G]K\S\_FM ff+fmY3PQSJQ_N2F veOW#JKp.I#Y3!I fiI#F][3KaJQSNFGHY3I#^`M#FI9GHFiQgIJT#F
G]\SKPP/Q_^OG]KaJQ_Y3I2KPRfY3P/Q_JQ_NF  M9F]Z:KW9\SJ G]\gKPPf9bFMOQSGHJPJT#FYJT#F]bnG]\SKPP]v(G]K\S\_FM ffVI#F][3KaJQSN2F p:JT9QSPRQSPdK
PVF]JVJQSI9[ojUQ_JT-JjnY!G]\SKPPFP  Y3c2fOW#JQgI#[	JT9F"G]\SKPP YZmKI2YeOPVF]bNaKaJQ_Y3IemY3QS\SP M#YjUIJVYPW9ccQSI#[hJT#F
GHY3I#^OM9FI9GHFPRYZ7JT9FUb/W9\SFPdQ_JBP/KaJQSPV^9FP]v{KI9MJT9FIM#FG]QgM9QSI#[oJT#F	fmY3PQ_JQ_NFUG]\gKPPBQSZJT9F	PW9c QSPd[b/FKaJVF]b
JT9KI]F]bY#vdKI9MJT#FkI#F][3KaJQ_NFkG]\SKPP-YJT#F]bjUQgPVF}hFG]QSPQ_Y3IGHY3ccQSJVJVF]FP-Kab/F%Ki[FI#F]b/K\SQSKaJQ_Y3IYZ
JT#FPVFoZ:Yb/c-W9\SKP]vuQSI%jUT9QgG/T%jBFfibFcYNFJT#FoPVF]JVJQgI#([  P"GHY3I9PVJVbKQSIJp:J)jBYG]\gKPPVFPBKI9MKW#JT9Yb/Q_]FfiJT#F
c2FcffemF]b/PT9Q_ff9bFM9QSGHJQSY3IJVY2KabeOQ_JVbKabX2G]\SKPPVFP]vuJT#F]b/F]e(X\SFKM9QSI#[JVY2KffJVb/W#F	NYJQSI#[G]\SKPPQ_^OF]b"T9QSP
NYJQSI#[kZ.KPT9Q_Y3I5QSPfiK%Z:FKaJW#bFJT9KaJM#FG]QSP/Q_Y3IiGHY3ccQ_JVJVF]FPPTOKabFjUQ_JTM#FG]QSPQ_Y3IJKaeO\SFPp s Y3T9KNuQ r
{Y3ccF]b^9F\SM7v`xyy3#
 "	YjnF]NF]bv#M#FG]QSPQ_Y3IJKaeO\_FPnG]\SKPPQS^9F]b/PdKabF	eOKPVFMY3IcK VYb/Q_J)X2NYJQSI#[-YZ7JT#F
:<;%$

fi

FH#Kc2fO\_FP-p.KI9MI#YJUYZb/W9\_FPv#YNF]boK2bFPVJVb/QgGHJVFM ff+jUQSIOM#YjfiYZ JT9F!M#FPGHb/QSf9JQ_Y3INaKab/QSKaeO\SFP]R"T#F]X
I#FGHFPP/Q_JKaJVFJT#F2PVJVYb/QSI#[%YZRcKI(XlFH#Kc2fO\_FPvmKIOMiJT9F-QSI(JVF]bf9b/F]JKaJQ_Y3I9PhYZdJT#F2M9KaJKkG]KIY3I9\_XlemF
cKM#FJT#bY3W#[3TJT9QSP!jUQSI9M9Yjv KG]GHYb/M9QSI9[JVYJT9QSPfmYJVFI(JQSK\S\_X\SKab/[FPVF]J-YZ"FH#Kc2fO\_FP]l}hFG]QSP/Q_Y3I
GHY3ccQSJVJVF]FPb/KaJT#F]bbF]fObFPVFI(J KIF G]Q_FIJjLKX-JVYoFI9GHYuM#FKfiNYJQSI#[oc2F]JT#YuM2QSI(JVYoKoPcK\S\{I(WOc!emF]b
YZb/W9\_FP]v3KIOMJT#FjnKX-KoG]\gKPPdQSP [3Q_NFI2G]KIeFLe9b/Y3W#[3TJe`KG -JVYFKab/\_XffjBYb uPdQSIcKGT9QSI#F"\SFKab/I9QSI#[
p  \SKa
b  r qnY3PVjnF\S\wvxyyux  YbFZ:Yb/cK\7M#F]JKQS\gP"KabFf9b/YNuQSM#FMQSIkJT#FI9FH{JUPFGHJQ_Y3I7
 c2Y3I#[lY3W9boJT#F]YbF]JQSG]K\RbFPW9\_JPv7JT9KaJKabFf9b/FPVFIJVFMQSI5JT#F2Z:Y3\S\_Yj	QSI#[PVFGHJQ_Y3I7vjBFfObYNuQSM#F
Z:Yb/cK\9fObY(YZ.PJT9KaJBJT#F	P/QSc2fO\SQgG]Q_JX fiKG]G]W#b/KGHX-JVb/KM#F]Y *QSPRK\SPVYffTOKab/M2JVYffKGT9Q_F]NFhZYbn}  v(KPRjBF\S\`KP
Z:Yb0JT#FnGHY3I9PVJVb/WOGHJQ_Y3I!YZ`GHY3c2fO\_FHoNYJVFPQSI(NY3\_NuQSI#[U}fio8"T9QSP0\SKPVJb/FPW9\_JP/T#YjUP0JT9KaJ]vjUTOQS\_FdcQ #QSI#[
 +# jUQ_JTemY{Y3PVJQSI#[f9bYNuQSM#FPUY3I#F!YZJT9Fffc2Y3PVJ	fmYjnF]bZ:WO\G]\SKPP/Q_^OG]KaJQ_Y3IlK\S[Yb/Q_JT9cP-p.#b/Q_FMOcKI7v
"hKPVJQ_Fv r "Q_eOPTOQ_b/KI9Qwv~avOf9b/W9I9QgI#[oemY(Y3PVJQgI#[ffQSPnFPPFIJQSK\g\_XT#FW#b/QgPVJQSG!p  Kab[3QSI#FKI(JW r }fiQ_F]J fi
JVF]b/QSGT7vxyy3
"T9FK\_[YbQ_JT9c jBF5f9bYfmY3PVFZ:Yb%JT9F5QgI9M9W9GHJQ_Y3I YZ!}  
v  p:Z:Yb FK 1+I9M9WOGHJQ_Y3I YZ
}hFG]QSPQ_Y3I  Y3ccQ_JVJVF]FPvdT9KPffJT#F%ZY3\S\SYjUQSI9[ F]XZ:FKaJW#bFP]i)JW9PVFPffbFGHFI(J-b/FPW9\_JP!Y3IfOKab/JQ_JQ_Y3I
emY(Y3PJQSI#[#v3b/KI {QSI9[!\_Y3PPReY{Y3PVJQSI9[pDuG/T9Kaf`Q_bF r uQSI#[F]bvxyy3dKIOMPVY3c2FhKaemY3W#JRf9bW9I9QSI#[oqnY{Y3\_FKI
Z:Yb/cffWO\SKP-p s FKab/I9P r  KIOPVY3W#bvxyy3	
 
 ZY3\g\_YjUPhKkPG/T9Fc2F2G]\_Y3PVFJVY  +# & PfiZYboM#FG]QgPQ_Y3I
JVbF]FPfip fffiWOQSI9\SKI7v`xyy +(v9Yb  }   PnZYbM#FG]QSP/Q_Y3I\gQSPVJPfip UYuG  r  Kaf9f{Xv7xyy3	zuKPP/W9G/T7vuQ_JLM9Q *mF]b/P
Z:bY3c f9b/F]N{Q_Y3WOPUPVJW9MOQ_FP"QSINYJQSI#[G]\gKPPQ_^9F]bPffp:emY(Y3PJQSI#[#v9eOKa[[3QgI#[p.qnbFQScKI7v0xyy|3V	e{XZFKaJW9bFP
PW9GTffKP0JT#FnZ:KGHJ JT9KaJ I#Y	c2YuM9Q_^`G]KaJQ_Y3I!QSP0cKM#FBY3I!JT#FnFH#Kc2fO\_F  P M9QSPVJVbQ_eOW#JQ_Y3IfiM9W9b/QSI#["QSIOM9W9GHJQ_Y3I7
JBQSPBK\SPVY-Y3I#F	Q_Z7Q_JPnM9Q *mF]bFI9GHFPRjUQ_JTJT#Ffi   #BE b/WO\_FUQSI9MOW9GHJQ_Y3IKaf9fObY3KG/T5p  Y3T#FI r #QSI#[F]bv
xyyy3
fiIcffWO\_JQSG]\SKPPKIOMc-W9\_JQS\SKaemF\{f9bYe`\_FcP]v f9bYfmY3PVFPdK!NF]bX2Z:KPJRKI9MP/QSc2fO\_FPVY3\gW#JQ_Y3I
JVYob/K(I uQSI#[o\_Y3P/P0emY{Y3PVJQSI#[#vYfOJQScK\#QSI-Z.KQ_b/\_X[FI9F]b/K\#G]KPVFP]v{KI9M2KPVXuc2f9JVYJQSG]K\S\SX!Yf9JQScK\{QgI2c2Y3PVJ
YZ7JT#FhbFcKQSIOQSI#[oY3I#FP]dUT#FU[FI#F]bK\Of9b/YeO\_Fc YZ7bK(I {QgI#[!\_Y3PPBemY(Y3PJQSI#[jLKPnf9bF]NuQ_Y3W9P\_XGHY3I VF
G fi
JW#bF
M  fi "hKab/MpDuG/T9Kaf`Q_bF r uQgI#[F]bvxyy3"T#Y3W#[3TlY3W#b"b/K(I uQSI#[2\_Y3P/P"emY(Y3PJQSI#[2K\_[Yb/Q_JTOc QSP
I#YJLK\_jLKXuPLYf9JQScK\wv{jnFfiK\gPVY-P/T#YjJT9KaJLJT#Fh[FI#F]b/K\b/K(I uQSI#[-\_Y3PPnemY(Y3PJQSI#[!fObYeO\_Fc bF\SKaJVFM%JVY
uGT9KafOQ_bF r uQSI9[F]bffp+xyy3"QgPUKGHJW9K\S\SXkI9Y
J  fi "	KabM7v`KI9MG]KIemF!PVY3\SNFMQSIfmY3\_XuI#Y3cQgK\mJQSc2Fv
JT#Y3W#[3TQ_JRPVF]FcPdJVY!b/F(W9QSbFJT#FUW9PF"YZGHY3c2fO\_FH2KI9MJQSc2F fiDFHufFIOPQ_NFUK\_[YbQ_JT9cP]vbF\SKaJVFMJVYffJT#F
cQSIOQScQ_KaJQ_Y3IlYZ	p.PVXucc2F]JVb/QSGUPW#e`c2Y{MOW9\SKab"Z.W9I9GHJQ_Y3IOP]U"T9QgPUK\SPVY%fOKabJQgK\S\_X VWOPVJQ_^9FPUJT#F-WOPVFffYZ
Y3W#bUP/QSc2fO\_FfiKI9MZ.KPVJUKafOf9bY#QScKaJQSY3IK\_[Yb/Q_JT9cl
"T9F\SKPVJ-PVFGHJQ_Y3IYZJT9QSPfffOKafmF]b!fObFPVFI(JP!FHufmF]b/QSc2FI(JK\dbFP/W9\_JPffYe9JKQSI#FMjUQ_J
T  Y3I
JT9Q_b/JX fiDY3I#FM#Y3cKQSIOP]v	c2Y3PJlYZffj	T9QSG/TKabFbFKM9QS\_X1KNaKQS\SKaeO\_FKI9MtG]KIemFZ:Y3W9I9MY3ItJT9
F   
bF]fmY3PQ_JVYb/XYZcKG/TOQSI#F\_FKab/IOQSI#[2M9KaJKaeOKPVF%p.qn\gK Fv s F]Y[3T7v r  F]b/vxyy3
+IYb/M9F]b!JVY F]F]fJT#FfOKafmF]b!PVF\_Z fiGHY3IJKQgI#FMKI9MKPffGHY3IOG]QSPVFKI9MbFKM9KaeO\SFKP!fmY3PP/Q_eO\_Fv7jnF
T9KNFfiGT#Y3PVFI%JVYfOW#JnKIKaf9fmFI9MOQ KaJBJT#FhFI9MYZJT#FhfOKafmF]bnGHY3I(JKQSI9QgI#[ffK\S\`f9bY{YZ.PRYZY3W9bBbFP/W9\_JP]

 Cff   
 < #C aCD? > ?
 F]J"
! eF2JT9FI(W9cffemF]boYZG]\SKPPVFP]#	I9\SFPPfiYJT#F]bjUQgPVFPVfmFG]Q_^9FM7v0KIFHuKcfO\_F$QSPoKlGHY3W#fO\SF$%
p'&)(*!,]+ UjUT9F]bF"
& QSP	KIYeOPF]bN8KaJQSY3IiM#FP/GHb/Q_emFMlYNF]b.
- NaKab/QSKaeO\SFP]v`KI9M/!,
+ Q_JPhGHYbbFPfY3IOM9QSI#[G]\gKPP
Kc2Y3I#[10 2( x3(5464646(*!8
7 x:
9 z3JVYFKGTFH#Kc2fO\_Fp'&)(*!,+  QSPKPPVYuG]QSKaJVFMKfijnFQ_[3T(J<; pVp'&)(*!,+ Vvb/F]f9bFPVFI(JQSI#[
Q_JPfiKaf9fmFKab/KI9GHF-f9bYe`KaeOQS\SQ_J)XkjUQ_JTibFPVfmFGHJfiJVYkK%\_FKab/IOQSI#[P/Kc2fO\_F1=?> jUT9QgG/TijBF2M9QSPfY3PFYZ)@=A>
QSPhQ_JPVF\_ZK%PW#eOPF]JUYZRKjUT#Y3\_F-M#Y3cKQSIijUT9QSGTljnF-M9FI#YJVFCl
B fifie(NuQ_Y3W9P\SXvOjnF-M#Y%I#YJfiT9KNFFI(JQ_bF
KG]GHFPPnJVY1B p'=A>EDF%
B  QSI2[FI#F]bK\wvjnFUF]NFI%T9KNFG =?>HGJIKGLB#G p*GM4N3G M9FI#YJVFPRJT9FUG]Kab/M9QSIOK\SQ_JXz3jnF
PW#fOfY3PFhQSIkK\S\JT9KaJLZ:Y3\S\_YjUPLJT9KaJHB QgPnMOQSPGHbF]JVFfijUQ_JT^OI9QSJVFfiG]KabM9QSI9K\SQSJX9 +I%JT#FfifOKabJQSG]WO\SKabG]KPVF
:PORQ

fi

iwc

	
ffff

jUT#F]b/FA! %t~{v(JT#FLJjnY!G]\SKPPFPKabF"I#YJVFM ff fi p'!,+ % KI9M ff  p'!,+ % xvKIOM2G]K\S\_FMbFPVfmFGHJQ_NF\SX
JT#FkI#F][3KaJQ_NFKI9MfmY3PQ_JQ_NF%G]\SKPP]5"T#F%\_FKab/I9QSI9[iPKc2f`\_FQSPffJT#FkW9I9Q_Y3IYZ"JjnYPKc2fO\SFP]vI#YJVFM
=?>KI9
M =A>Bv(GHY3I(JKQSI9QSI#[!bFPVfmFGHJQ_NF\_XJT#FhI#F][3KaJQ_NF	KIOMfmY3PQ_JQ_NFFH#Kc2fO\SFP] )JnQSPjnYbJTj	T9QS\_F
JVY-JT9QS(I -JT#FUfmY3PQSJQ_NF"FH#Kc2fO\_FPBKPRemF\_Y3I#[3QgI#[oJVY-KffPW#eOPF]JdYZ B GHY3IJKQgI9QSI#[-K\S\9fmY3PPQSeO\_FLfmY3PQ_JQ_NF
FH#Kc2fO\_FP]v9WOPW9K\S\_XG]K\S\_FMJT#F  "  3{  	 ff 
 .
 PfifOKabJoYZBY3W#bo[Y3K\dQSI5cKGT9QSI#F2\_FKab/IOQSI#[#v7QSPhJT#FI#F]FM5JVYeOWOQS\SMKkb/F\SQSKaeO\_F-Kaf9f9bYuQgcKaJQ_Y3I
JVYJT#F!JVb/W#FG]\SKPP/Q_^OG]KaJQ_Y3IkYZ JT#FFH#Kc2fO\SFP"QS
I Blv9JT9KaJUQSPv9K[Y(YuMKaf9fObY#QScKaJQ_Y3IkYZ JT#FoJKab/[F]J
GHY3I9GHF]f9J]ve{X!W9P/QSI#["Y3I9\SXoJT#FBFH#Kc2fO\SFP0QS"
I =?>n  Y{Y{M-Kaf9f9b/Y#QScKaJQ_Y3I9P PT9K\g\T9KNFLKhT9Q_[3TffKG]G]W#b/KGHX
YNF]b Blv{K\_JT9Y3W#[3T2jnF	M#Y!I9YJRT9KNFfiKG]GHFPPBJVY!JT9QgPd(WOKIJQ_J)Xv(e`W#Jdb/KaJT#F]bBJVYffQ_JPdFPVJQScKaJVY2b Kffc2YbF
Yb\_FPPb/F\SQSKaeO\_F%KG]G]W#b/KGHXGHY3c2fOW9JKaeO\_FkYNF]b =?>n FlbF]Z:F]b2JT#FkbFKM#F]bJVYPVJKIOM9Kab/McKG/TOQSI#F
\_FKab/IOQSI#[emY(Y uPffp  Q_JG/T9F\S\wvxyy3"ZYb	Z:W9bJT#F]bUGHY3I9P/QSM#F]b/KaJQ_Y3IOP"KaemY3W#J"JT9QSPUQSPPW#F  }  GHY3IJKQgI9P
JjnYf`KabJP 

fi

0(pff (  P9 ( . j	T#F]bF"FKGT7QSPdK!cY3I#Y3cQSK\p.KffGHY3I!fi
(  . (5464646( (   9  p -emFQSI9[	JT#F"I{W9cffemF]b YZmM#FPGHbQ_f9JQ_Y3I
NaKab/QSKae`\_FP]v7FKG/T!#"QSPoKfmY3PQ_JQ_NF2\SQSJVF]b/K\ KI9MFKG/T #"QSPoKI#F][3KaJQ_NF%\SQ_JVF]b/K\vKIOM5FKG/T$  
QgPffKiNFGHJVYbQSI!% &(H' 5#YbJT#F%P/KF%YZUbFKM9KaeOQS\gQ_JXv JT9QSPffNFGHJVYb/QSK\LI#YJKaJQSY3IP/T9K\S\demF F]f9J

 PVF]JBYZ7W9I#Yb/M#F]b/FM2fOKQ_b/P	p:YbRb/W9\SFP
W9I9GHJQSY3IYZ`\SQ_JVF]bK\SPYNF]bH0 ( (  ( ( .

JT9bY3W#[3T#Y3W#JK\S\JT#FfifOKafmF]bvuF]NFIZ:Ybf9bYe`\_FcPBjUQ_JT%Y3I9\_XJ)jBYG]\SKPPVFP]BoI#FocQ_[3T(JLGT#Y{Y3PVF
JVYKM9MlKPQgI#[3\_FfibFK\7b/KaJT9F]b"JT9KIlK~ fiGHY3cfY3I9FIJ"NFGHJVYbfiQSIJTOKaJ"G]KPVF

}hF]Z:KW9\SJ FGHJVYb )  QSI - 2(x / '   [3KQSI7v7QSIiJT#FJjnYfiG]\SKPPG]KPFvQ_JfiQSPfiPW(G]Q_FI(JhJVY%b/F]fO\SKGHF
) e{XkK2M9F]Z:KW9\SJ"G]\SKPP"QSI0  ( 7 93
#YbffKIXiYeOPVF]bNaKaJQ_Y3I%
& KI9M5KIX5c2Y3I#Y3cQgK\*  v7JT#Ff9bYfmY3PQ_JQ_Y3I ff &kPKaJQSPV^9FP+  QgPhM9FI#YJVFM
e{X&-,.  "T#F2YfOfY3P/Q_JVFfffObYfmY3PQ_JQ_Y3Iff k
& M#Y{FPoI#YJPKaJQSPVZ:X/  QSPfiM#FI#YJVFMe{Xff &1,2
0   u2"T#F
G]\SKPP/Q_^OG]KaJQ_Y3IkYZKI(X%Ye`PVF]bNaKaJQ_Y3I 
& QSPcKM#FffQSIkJT#FoZ:Y3\S\_Yj	QSI#[jnKX nM#F]^OI#F 3  h
+ KP"Z:Y3\S\_Yj	P
3 + %
4
  4
p  (   
&5,6 

fi



"T9FG]\SKPP"KPP/Q_[3I#FMJVY&QSP"JT#FI9

fi Kab/[BcK8 " 3  + Q_ZAGKab[ncK8 " 3  + G % xavOKI9M
fi Kab/[BcK8 "87 
9J9:<;>A=? @ )  YJT#F]b/jUQSPVF

+ IYJT#F]bBjBYbM9P]v(QSZJT9FUcK8#QScK\9GHY3c2fmY3I#FI(JRYZ 3  + QSPRWOI9QS{W#FvJT#FIJT9F	QSI9M9FH-[3Q_NFPBJT#FUG]\gKPP
KPPQS[3I#FMJVYC&{BhJT#F]b/jUQSPVFv{jBFhJKFoJT#FfiQSI9M#FHYZJT#FocK8uQgcK\OGHY3c2fmY3I#FI(JLYZ )  GHYbb/FPVfmY3I9M9QSI#[
JVYkJT#F2cK8#QScK\ GHY3c2fmY3I#FIJfiYZ 3  +%p:JQ_FPfiKabF2PVY3\_NFM5e{XK%b/KI9M#Y3cG/T9Y3QSGHF2Kc2Y3I#[kJT#F2cK8#QScK\
GHY3c2fmY3I#FI(JP
}  GHY3IJKQSIOP-KPW#e7G]\SKP/PojUT9QSGTQSP-Kc2Y3I#[iJT#Fk\SKab[FPVJG]\SKPPVFPffYZ"qnY(Y3\SFKIZ:Yb/c-W9\SKPffJVYiemF
r  KPG]W#F\wv9xyy 3v{T#YjnF]NF]bJT9QSP G]\SKPP QSP \_FPP QSI(JVF]bFPVJQSI9[UZ:bY3c KUf9bKGHJQSG]K\
   fi\_FKab/IOKaeO\_Fop UYuG  
NuQ_F]j"fmY3QSI(JfiP/QSI9GHFb/W9\_FPoG]KIemFI(W9cF]bY3W9PoKI9MTOKab/MJVYQSI(JVF]bf9b/F]J] UF]NF]bJT9F\_FPP]v KlPW#e7G]\SKPPfiYZ
}  p UYuG  r  KP/G]W#F\wvRxyy 3hfObFPVFI(JPoKIQSI(JVF]bFPVJQSI#[kGHY3c2f9b/Y3cQSPVFemF]JjnF]FIbF]f9b/FPVFIJKaJQSY3I9K\
fmYjnF]bKI9M QgIJVF]bfObF]JKaeOQS\SQSJXfmYjnF]b +ItJT9QSPG]\SKP/P]vfijUT9QSGTQgPlW9PVFMe{X !vFKGT YZJT#F
NFGHJVYbLGHY3c2fmY3I#FIJPRKabF"bFPVJVbQSGHJVFM2JVC
Y 073x (2 (  :x 9fiKI9MFKG/TcY3I#Y3cQSK\#QSPdf9bFPVFI(JdKaJRc2Y3PVJRY3I9GHF
:PO5:

fi

"T#FkNaK\SW#FP 7xavnuv  xK\g\_Yj I9KaJW#bK\QSIJVF]b/f9bF]JKaJQ_Y3I9PYZ	JT#Fb/W9\SFP]vemFQSI#[FQ_JT#F]bQSIZ.KNYbYZ
JT#FGHYbbFPfY3IOM9QSI#[G]\SKPPlp  xvnI#FW#JVb/K\njUQ_JTbFPVfmFGHJJVYJT#FG]\SKPPlp.vBYbQSIM9QSPVZ.KNYb2YZ	JT#F
GHYbbFPfY3IOM9QSI#[%G]\SKPP2p 7x!"TOQSP	P/W#e7G]\SKPP]vJVYkjUT9QSGTjnFbF\SKaJVF2KPo}   ( 0  ( v7QSP]v7KPfijBF2I#Yj
f9bYNFvnP/W *F]bQSI#[JT#FPKc2FlK\_[Yb/QSJT9cQSG%M#b/Kj"eOKG uPKP2}fi p "	X3Ka^O\ r EUQSNFPVJ]v	xy3a|3KI9M} 
p 	Y{
G  r  Kaf9f{Xv7xyy3< RF]NFI%jUQ_JT9Y3W#JRbFPJVb/QSGHJQSI#[ffJT#FoGHY3c2fmY3I#FI(JPBYZJT#F	NFGHJVYbP]v9YbLjUQ_JT%KIX
bFPVJVbQSGHJQ_Y3IJVY5KPVF]J2GHY3I(JKQSI9QSI#[KaJ2\_FKPVJ-Y3I#Fb/FK\BNaK\SW#FvJT9F%GHY3I9PJVb/W9GHJQ_Y3IYZUPcK\g\RZ:Yb/c-W9\SKP
jUQ_JTP(W G]QSFIJ\_XT9QS[3TKG]G]W#b/KGHXQSP!TOKab/M7"TOQSP!QSPffKiG]\SFKabcYJQ_N8KaJQSY3IZ:Yb-WOPQSI#[T9FW#b/QSPVJQgG]PQSI
M#FG]QSP/Q_Y3IGHY3ccQ_JVJVF]F  P	QSI9M9WOGHJQ_Y3I7
 ?Cff    > @}C  C 
d< ABC@BC >
	fiff zN#AJ   # C aC? > %

(@

F	I9YjPT#Yj1JT9KaJRe`W9QS\SM9QgI#[	M#FG]QgPQ_Y3IGHY3ccQ_JVJVF]FPBQgPdK!T9KabMK\_[Yb/Q_JTOcQSGLJKP 2jUT9FI2Y3I#FUPVJVbQ_NFP

JVYYe9JKQgIkeYJTP/cK\S\mKI9MlKG]G]W#bKaJVFZYbcffW9\gKP]d"T#F]bFoKabFoJ)jBY%W9PW9K\mI#YJQSY3I9PLYZ P/Q_]FojUT9QSGTG]KI
I9KaJW#bK\S\_X%eFffW9PVFMlZ:YbfiM#FG]QSPQ_Y3IGHY3ccQ_JVJVF]FP]	"T#F!^Ob/PVJ"Y3I#F-QSP"JT9F!jUT#Y3\SF!I(WOc!emF]bUYZd\gQ_JVF]b/K\SPYZ
r  KPG]W9F\wvxyy {z
JT#FZYb/c-W9\SKp.Q_ZK\SQ_JVF]bK\dQSPfObFPVFI(
J hJQSc2FP]v Q_J!QSPffGHY3W9I(JVF
M hJQSc2FPp UYuG  
r  Kaf9f{Xvxyy3vRJT#FkPVFGHY3I9MY3I#FkQSPJT#FkI{W9c!emF]bffYZ"b/W9\_FP!YZ"JT#F%ZYbcffW9\gKp s FKab/I9Pv  Qwv
	Y{
G  
RQSJVJ]v r dK\SQSKI(J]v0xy3fiW#bUb/FPW9\_JP"Qgc2fO\_X%JT9KaJhbF][3Kab/M9\_FP/P"YZJT#FffbFPVJVb/QSGHJQSY3IlYNF]bfiJT#F-NaK\SW#FP
YZJT#FffNFGHJVYb/P2p.KP	\_Y3I9[KP	JT#F]XKabFffF\SFc2FIJP	YZdKPF]JUjUQ_JTG]Kab/M9QSI9K\gQ_J
X t~3vKI9MiK\_bFKM#XkZ:Yb
JjnY fiG]\SKPPFPfif9bYeO\_FcP]vOcQSIOQScQ_QSI9[-JT9FffPQ_]FffYZdKM9FG]QSPQ_Y3IiGHY3ccQ_JVJVF]F-ZYbhemYJTPQS]FffM#F]^OIOQ_JQ_Y3I9P
QSPoKPTOKab/MKPPVY3\_NuQSI#[%jnF\S\ fi uI#YjU
I  fi "hKab/M5f9bYe`\_FcP]"T#F]bF]Z:YbFvJT9F-JKP 5QgPoK\SPVYT9Kab/M5Z:Yb
}   ( 0  ( jUQ_JTkJT9FfOKabJQSG]WO\SKabLN8K\SW9F@
P 7xavOuv  xfiZYb"JT9FNFGHJVYb/P]

! #"%$3'&("*))%+-, '.0/1"2.43#5" 6.874.:9 <;8" =3?>@A"*)CBAD75"6;'.
#
" E"F> HGI    I  %KJML  ON ":PRQSJ %UT  "%<$ WV "%J  %YX I !   $ZT  ] [N !\J ] "! %K]^J  J % _
` " [V  ba  V I  %KT "! !  % cV dJ %eJ   fTgTgJ    (%KJ %    XhJ  I " %   [N ji "fT 
 !  % =?> 1
k36)jl {F]FoJT#F  f9fmFI9M9Q_

F%G]KIFKPQg\_XKMOKaf9J!"T#F]Yb/Fc x2JVYiJT#FG]KPVFjUT#F]bFJT9Fb/W9\_FP!KabFbF]fO\gKGHFMe{XjnFQ_[3T(JVFM
}fi KPKM#NYuG]KaJVFMQSIeY{Y3PVJVFM  +# pDuG/TOKafOQ_bF r uQgI#[F]bvUxyy3 "UF]bFvnFKG/T JVbF]FlbF]JW9b/I9P2K
G]\SKPPm 0  x3( 7x:93vKI9MFKG/T JVbF]FlQSP[3Q_NFIK5b/FK\LjBFQ_[3T(J2JVY\_F]NF]bKa[FlQ_JPNYJVF"T#FPQ_[3IYZ
JT#F\gQSI#FKab!GHY3cffeOQSI9KaJQSY3I[3Q_NFPJT9FG]\SKPPYZ"KIFHuKcfO\_Fk"T#F2Z:Y3\S\_Yj	QSI#[JT#F]YbFc T#Y3\SM9P!Ka[3KQSI
jUQ_JTkKI(X\SQgcQ_JKaJQ_Y3I9PBY3IJT#Fo\_F]NF]bKa[3QSI#[2GHY(F G]Q_FIJPffp.KP"\_Y3I#[2KPKaJ"\_FKPVJY3I9FfiI9Y3!I fiD]F]bY2NaK\SW#FoQSP
KW#JT#YbQ_]FM`v{YbRjUQ_JT9Y3W#JR\SQScQ_JKaJQ_Y3I2Y3IJT#F	GHY{F G]Q_FI(JP]dqnX2JT9QSPvjnF	c2FKIJT9KaJRZ:YbnFKG/T%YZ7JT#F
Kaf9fO\gQSG]KaeO\_F!\gQScQ_JKaJQ_Y3IOPffp:YbfijUQ_JT#Y3W9JvJT9Ffff9b/YeO\_Fc Qg
P  fi "hKab/M7o"T#FPQ_]FI#YJQ_Y3IQSP	JT#F2PW9cv
YNF]bfiK\S\mJVbF]FPv9YZ JT#FQ_b"I{W9c!emF]bYZI#YuM#FP]

#" E"F>onqp
 J % :_` " OV  a  V  I  %KT "! !  % 
%KJ %    XhJ  I " %   [N Wi "fT 
 !  % =?> ]vXhJ  I $  !wJ'TgJ  "
"	  
  %<%KJ  !  !wJ'TgJ  "  J 	2]."% ! 	 3 "% "  !  "%  	   

X  J 3 I jV !\J  "  fT rJ "  J 	(%-ONsPut 	 _
	  I  ! ] aH " 3 J  3  yx dJ    %j]+  N8 
 _ L H   "!4$  J % "$  I   JML jV61
T9QS\_FffQ_JUQSPUjBF\S\ {I9YjUIJT9KaJ	emY{Y3PVJQSI#[2b/FPW9\_JP	QSIlKb/KafOQSMlM9FGHbFKPQSI#[YZJT#FffF]b/bYbUYNF]b =A>

 

 J 	

jUT9QgG/TG]KI%FKPQg\_X2KI9MbKafOQSM9\_XM#bYf%M#Yj	IJVY-]F]bYp.KPn\SY3I#[ffKPnQ_JBQgPdfmY3PPQ_e`\_Fv"T#F]Yb/Fc ~!PT#YjUP
JT9KaJoKaJVJVFc2f9JPfiJVYkF G]QSFIJ\_XbFM9W9GHFJT#FPQ_]F-YZRJT#F-NYJVF2jUT#FIemY{Y3PVJQSI#[%}h QSP  fi "hKab/M7fi)Z
JT#F	f9bYeO\_Fc QSPdPQScfO\SQ_^9FM-JVYffJT#FUJVY-f9b/W9IOQSI#[hYZK!\SKab[FhGHY3I9PQSPJVFIJBNYJVF	YZ}fi p  Kab[3QSI9FKIJW r
}fiQ_F]JVJVF]b/QSGT7v7xyy3v9JVYYe9JKQSI%KPcK\S\_F]bnGHY3I9PQSPJVFIJp:YbLjUQ_JT%\SQScQSJVFMF]bbYbHNYJVFfijUQ_JTbFPVJVbQSGHJVFM
PQ_]Fv#Q_JQSPLKa[3KQSIkfmY3PP/Q_eO\_Fp.W9PQSI#[-JT#FoPKc2Fob/FM9W9GHJQ_Y3I`dJVYPT#YjtJT9KaJJT9QSPne9bQSI#[3?
P  fi "hKab/M9I9FPP]
:POEz

fi

 n

iwc

	
ffff

#"%$3'&("*))%+-, ' .0/1"2.43#5" 6 .874.G=3?>@A"*)) 3 ;'"F.


FffI9YjPVJKaJVF-KI9Mkf9bYNF!JT#FoF{W9Q_NaK\_FI(JYZ"T#F]YbFc

xfij	Q_JTkJT9QSP"I9F]jPQ_]FI#YJQSY3I7

#" E"F>  GI    I  %KJML  [N "-PuQ J %uT  "%<$ WV "%J  %  $ZT  ] [N  $ !  %j] J  J % _` " OV
= > 1 t I % %<$ ! 
 a  V  I  %KT "! !  % UV( J %KJ 	 fTgTgJ    (%KJ %    XhJ  I "%  [N Wi "fT 
 !  % A
I  ! V % H %X I    I  	 
  !#"  !\J  3  I %Wi "fT 
 !  %:J % "T 	     r_ P N8  	
T $ !#"E]  I "  J %j]fi"
V J % $   J 	 [N 	 $#  J 	
	'Pfffi5]  "6 I XhJ  I $    3 "  J a !\J H "! % 1
k36)jl {F]FoJT#F  f9fmFI9M9Q_


 

 fObF]N{QSY3W9PjnYb 1p s FKabI9P-F]JK\D_vLxy3fffObYNFP2KPQScQg\SKabJT#F]YbFc GHY3IOGHF]b/I9QSI#[iJT#FkcQSI9Q fi
cQ_KaJQSY3IkYZJT#FPQS]FhYZ K2} 	nO"T9F]YbFc-G]KIemFfiP/T#YjUI%JVYemFfic2YbFo[FI#F]bK\wv9KPLJT#FG]\SKPPLYZ
}   ( 0  ( jUQ_JTkJ)jBYb/W9\_FPPJVb/QSGHJ\_X%GHY3IJKQSIOP"JT9KaJ"YZ } 	j	Q_JTkJjnYc2Y3I9Y3cQSK\SP]
"T9F-PVJKaJVFcFIJfiYZBUT#F]YbFcP-xav~{v KPhYf9JQScQ_KaJQ_Y3Iif9bYeO\_FcPUjLKPfiG/T#Y3PVFI5Z:YbhfOW#bF-GHY3I!fi
NFI9Q_FIOGHFffzubF]fO\gKG]QSI#[JT#Fc e{XJT#FQ_bLKPPVYuG]QSKaJVFMkM#FG]QSP/Q_Y3If9b/YeO\_FcPhp.M#FG]QSM#FhjUT#F]JT#F]bnJT#F]bFhFH#QSPVJ
KGHY3I9PQSPJVFIJnZYbcffW9\gK!jUT9Y3PVFhP/Q_]FhQgPnI9Y-c2Yb/FhJTOKI%PY3c2F	^9{FM%JT#bFP/T#Y3\SM`jBY3W9\gMJVb/QSN{QSK\g\_X2cK F
JT#FfObYeO\_FcPLI#YJUY3I9\_#
X  fi "hKab/M7v9e`W#J"K\SPV
Y  fi  Y3c2f`\_F]JVF

 <   C  ?
IlK\_[Yb/Q_JTOcv#}  vOjnKPUf9bF]NuQ_Y3W9P\_Xf9bYfmY3PVFMp 	Y{
G  r  KPG]W9F\wvxyy 3"ZYb	eOW9QS\SMOQSI#[!M#FG]QgPQ_Y3I
HG Y3ccQSJVJVF]FP])Jf9bYuGHF]FM9P2QSIJ)jBYPJKa[FP]"T#Fk^9b/PVJPVJKa[Fle`W9QS\SM9PffK5fmYJVFIJQSK\g\_X\SKab[FlP/W#eOPVF]J
YZLM9Q *mF]bFIJfib/WO\_FP]vmFKG/TYZRjUT9QgG/T5QSPoKGHJW9K\S\SXiK}   ( 0  ( jUQ_JTY3I9\_XiY3I#F2b/W9\SFIKPFGHY3I9M
PVJKa[Fv Q_Jo[b/KM9WOK\S\_XiG]\gW9PVJVF]b/PoJT9FM#FG]QSPQ_Y3IGHY3ccQ_JVJVF]FP]v W9PQSI9[%JT9Ff9bYfmF]bJ)XiJT9KaJ!JT#FW9I9Q_Y3I5YZ
JjnY!}   ( 0  ( Pj	Q_JT2M9Q *mF]bFI(J bW9\_FPQSPPVJQg\S\uKo}   ( 0  (   JRJT#FFI9MYZmJT9QSP f9b/Y{GHFM9W9bFvJT#F
W9PVF]bnYe9JKQSIOPRK-PVF]JBYZ}  Pv{KI9MJT#Fhc2Y3PVJLKG]G]W#b/KaJVFfiY3I#F	QgPBGT#Y3PVFI%KI9Mb/F]JW#b/I#FM7 d{fmF]b/QScFIJK\
bFPWO\_JPM9QgPVfO\SKXhJT9FnKaeOQg\SQ_JXoYZ#+}  JVY	eOWOQS\SMPcK\S\}  P] +I!JT9KaJ fOKafmF]bv8jnFBf9b/YNuQSM#FBKIK\_[YbQ_JT9c
Z:YbR\_FKab/I9QgI#[oM#FG]QSPQ_Y3I2GHY3ccQ_JVJVF]FPBjUT9QSGT2T9KPRKMOQ *F]b/FIJdPVJVbW9GHJW#bF"PQgI9GHF"Q_JeOW9Qg\SM9P0Y3I9\_X-Y3I#FU}  
 YbFof9bFG]QSPF\_Xv QSPLKJT#b/F]FfiPJKa[F!K\_[Yb/Q_JTOc )JL^9b/PJne`W9QS\SM9PBKPVF]JYZb/WO\_FPLM#F]b/Q_NFM%Z:bY3c
bFPWO\_JPY3IlemY{Y3PVJQSI#[2M#FG]QgPQ_Y3IJVbF]FPpDuGT9KafOQ_bF r uQSI#[F]bvxyy3nJ"JT#FIiG]K\SG]W9\SKaJVFPUJT#FNFGHJVYb/P
W9PQgI#[KP/G/T#Fc2FkM#F]b/QSNFMZ:bY3c EUK(I uQSI#[\SY3PP!emY{Y3PVJQSI#[pDuG/T9Kaf`Q_bF r uQSI#[F]bvnxyy35J!^OI9K\g\_X
f9b/WOI#FPhJT#F^OI9K\ }   ( 0  ( W9PQSI#[%JjnYlfmY3PPQSeO\_FffP/G/T#Fc2F2P fiKIOKaJW#b/K\ f9b/W9IOQSI#[j	T9QSG/TjnFG]K\S\
ff+fmFPPQScQSPVJQSG%f9b/WOI9QSI#6
[ uvRKIOMf9b/WOI9QSI#[5W9PQSI9[\_Y{G]K\UGHY3INF]b[FIOGHFb/FPW9\_JPlp s FKabI9P r  KI9PY3W#bv
xyy3v jUT9QSGT5jBFG]K\g\ ff+Yf9JQScQgPVJQSG-f9b/W9IOQSI#6[ uffUT#FM#F]Z.KW9\_JfiNFGHJVYb-QgPoK\_jnKX{PffG/T#Y3PFIJVYlemF2JT#F
YeOPVF]b/NFMlM9QSPVJVbQ_eOW#JQ_Y3IYZKc!e`Q_[3W#Y3W9P\SXG]\SKPPQS^9FMkFHuKcfO\_FP]


 B&$*"=+"'.r'" ,j<> > 8 7d74"F" 3#.r#"($ kN$5787'"
! 3';S#"%$ 

6. 7#"($

3

u W#fOfY3PFUJT9KaJJT#FT(X{fYJT9FPQSPop.I#YJI#FGHFPPKabQS\_XKM#FG]QgPQ_Y3IkGHY3ccQ_JVJVF]Fv`Q_JcQ_[3T(JBemF214351BKM#FG]Q fi
PQ_Y3IJVbF]F jnFUe`W9QS\SMbFK\gQ_]FPRK!fOKabJQ_JQ_Y3IYZ7JT#FUM#Y3cKQgI B QSI(JVYffM9QSP+Y3QSI(JdPW#e`PVF]JP*) ( (+) . (5464646(+)-,ff
 Q KP - - . / /0JT#FoZ.W9I9GHJQ_Y3IlbF]JW#b/I9QgI#[ffJT9FJVb/W#JTkNaK\SW#FoYZKf9bFMOQSG]KaJV/
F . d}hF]^OI#F

0 "  1 %
0 "  1 %

4
; pVp'&)(*!,+V - -_p'&)(*!,+vm)5"76 ,! +A%98 / /	(
+,  ' @ 1 73254
4
; pVp'&)(*!,+V - -_'p &)(*!,+vm)5"76 ,! + %9
0 8 / /	4
+,  ' 2@ 1 37 254
:PO ;

fi

"

+IkYJT#F]b"jnYb/M9P]v 0   1 bF]f9bFPVFI(JPJT#FfiZ:b/KGHJQ_Y3IYZ FH#Kc2fO\_FPLYZ G]\SKPP 80f9bFPVFI(J"QSIkPW#e`PVF]J )5"v
"
KI9M 0  1 bF]fObFPVFI(JPBJT9FhZ:b/KGHJQ_Y3I%YZ0FHuKc2f`\_FPnYZ0G]\SKPPFP %9
0 8f9b/FPVFIJLQSI%PW#eOPF]J )5"3  G]GHYb/MOQSI#[

JVY%uGT9KafOQ_b/F r uQSI#[F]b!p+xyy3vmKjnFK l\_FKab/I#F]b"PT9Y3W9\SMcQSIOQScQ_]F	JT#F!GHb/Q_JVF]bQ_Y3I9

 %

~ 4

 0 " 1 0 " 1 4
 
" 1
4

p+x

+IJT#F%G]KPVF%YZLKM#FG]QSP/Q_Y3IJVbF]Fv JT#FfOKabJQ_JQ_Y3IQSPJT9KaJffjUT9QSGTQSPeOW9QS\_JKaJffJT#F\SFKNFP-YZJT#F
JVbF]FpfifffiWOQSI9\SKI7v xyy+(z0QSIJT#FG]KPVFYZnKM#FG]QSPQ_Y3I5\SQgPVJ]vmJT#F2fOKabJQ_JQSY3IQgP	JTOKaJojUT9QSGTQSPheOW9QS\_JhKaJ
FKG/TkbW9\_FvJVY-jUT9QgG/TjnFfiKM9MJT9F	PW#e`PVF]JBKPPY{G]QSKaJVFM%JVY-JT#FfiM#F]Z:KWO\_JRG]\SKPPop	Y{G
 r  Kaf9f{Xv7xyy3
uW#fOfY3PFJT9KaJhjBF-FI9GHYuM#F!JT#FM#FG]QSP/Q_Y3IlJVbF]FQSIJT#FffZ:Yb/c YZdKP/W#eOPVF]J	YZRc2Y3I#Y3cQgK\SP]vOe{XJK uQSI#[
Z:Yb"FKG/T\_FKaZ JT#F!\_Y[3QgG]K\ fi 6YZK\S\7KaJVJVb/Q_e`W#JVFP"Z:bY3c JT#F!bY(YJUJVYJT#F!\_FKaZ  FKPW#bQSI#
[ YNF]bhJT#F
JVbF]F  P\_FKNFPQSPnF(WOQ_N8K\SFIJBJVYcFKPW#b
F YNF]bJT#F	f`KabJQ_JQ_Y3IbFK\SQ_]FM%e{XJT#FfiPVF]JLYZc2Y3I#Y3cQSK\SP]
"	YjnF]NF]bvdJT#Fkc2Y3I#Y3cQgK\SP!KabF%M9QSP VY3QSI(JZ:bY3c FKGTYJT#F]bkp:FKGTFHuKcfO\_FP/KaJQSPV^9FPffFHuKGHJ\SXY3I#F
c2Y3I#Y3cQSK\ }hW9FiJVYJT9QSPf9bYfmF]bJ)Xv"Y3I9\_X PW#eOPVF]JP%G]KIemFbFK\gQ_]FM j	Q_JT c2Y3I#Y3cQSK\gP]vYb
F{W9Q_NaK\_FIJ\SXjUQ_JTKJVbF]F!T9KN{QSI9[ n\_FKNFP]
uW9f9fmY3PVF"JT9KaJBjBFh[FI#F]b/K\SQS]FUJT9QSPRYeOPVF]bNaKaJQ_Y3Ie(X2b/Fc2YNuQSI#[ffJT#FhMOQSP VY3QSIJI9FPPdGHY3I9M9QSJQ_Y3IYNF]b
JT#Fc2Y3I#Y3cQSK\SP] "T#FIKI(W9cffemF]bkYZ-PW9eOPVF]JP%YZffYbM#F]
b kpw~ )QSPI#Yj fmY3PP/Q_eO\_FijUQ_JTY3I9\_X 
c2Y3I#Y3cQSK\SP]v"KI9MQ_JkKaf9fmFKab/PJT9KaJkJT#FI(WOc!emF]b%YZbFK\SQ_]FMfOKab/JQ_JQ_Y3I9PG]KIeFFHufY3I9FIJQSK\g\_X
\SKab[F]bfiW9PQgI#[M#FG]QSP/Q_Y3IGHY3ccQ_JVJVF]FPfiJT9KIM#FG]QSPQ_Y3IJVbF]FP] "UYjBF]NF]bv7JT#FffFHufmFGHJVFMbW9I9I9QSI9[2JQSc2F
QSP%I#YJe`Q_[[F]bkjUT#FI WOPQSI#[M9FG]QSPQ_Y3I1GHY3ccQ_JVJVF]FP]vhPQSI9GHFiJT#F5I(W9cffemF]bYZf`KabJQ_JQ_Y3I9PQSP%QSIZ.KGHJ
emY3W9I9M#FMe{XJT#F	I{W9cffemF]bdYZ7FHuKcfO\_FP]v G =?> G"T{W9P]v(jBFhcKXFHufFGHJnPVY3c2F	bFM9W9GHJQ_Y3IQSIJT#F	PQ_]F
YZJT#F	ZYbcffW9\gKjnF	eOWOQS\SMjUT#FIW9PQSI9[!M#FG]QSPQSY3IGHY3ccQSJVJVF]Fv{j	T9QSG/TQSPBYZQSIJVF]b/FPVJBJVYQSI(JVF]bf9bF]JBJT#F
G]\SKPP/Q_^9F]bYe9JKQSI9FM7
 f9fO\SQgG]KaJQ_Y3IYZJT9QSPBf9b/QSI9G]QSfO\_F"QSI QgPBPVJVbKQ_[3TJVZ:YbjLKab/9
M K\SKab[FoM#FG]QSP/Q_Y3IGHY3ccQ_JVJVF]FQSP
eOW9Qg\_JBe{X%[bYjUQgI#[2Q_JVF]b/KaJQ_NF\_Xv`QSIkKJVYf fiM#Yj	IlZ.KPT9Q_Y3I7v#K2G]W#b/bFIJc2Y3I9Y3cQSK\w+IkJT9QSPc2Y3I9Y3cQSK\wv
JT#F"\SQSJVF]b/K\uKM9M#FM2KaJdJT#F"G]W9bbFI(J PJVF]fQSP JT#FY3I#Fj	T9QSG/T2cQSI9QScQS]FPJT#F"G]W9bbFI(	
J 1GHb/QSJVF]b/Q_Y3I7vYNF]b
K\S\mfmY3PPQSeO\_F	KMOM9Q_JQ_Y3I%YZ\SQ_JVF]b/K\gP]v{KIOMk[3Q_NFIJT9KaJ"JT9FI#F]j c2Y3I#Y3cQgK\7M#Y(FPUI#YJFHuQgPVJ"K\_bFKM#X%QSI
JT#FnG]W#bbFI(J0M#FG]QgPQ_Y3IGHY3ccQ_JVJVF]Ffip.QgIYb/M#F]b JVYUfObF]NFIJ c-W9\_JQ_fO\SFdKM9M9Q_JQ_Y3IOP7YZ9KUPQgI#[3\_FRc2Y3I9Y3cQSK\
"T#

F  GHb/Q_JVF]b/Q_Y3I QSPGHY3c2fOW9JVFM1W9P/QSI#[JT#Flf`KabJQ_JQ_Y3IQgI9M9W9GHFMYNF]b =A> e{XJT#FiG]W9bbFI(JPVF]JYZ
c2Y3I#Y3cQSK\SPeOW9Qg\_Jlp.Q_Z	J)jBYFHuKcfO\_FPPKaJQgPVZXJT#FP/Kc2Fc2Y3I#Y3cQSK\SP]vnJT#F]XeF\SY3I#[JVYJT#FiPKc2F
PW#e`PVF]JYZ	JT#FkfOKab/JQ_JQ_Y3I` T#FII9YZ:W#b/JT#F]b2KM9M9Q_JQ_Y3IYZhK\SQ_JVF]b/K\LM#FGHbFKPFP2JT#
F  N8K\SW9FvBK
I#F]j c2Y3I#Y3cQgK\0QSPhGHbFKaJVFMKI9M5QSIOQ_JQSK\SQ_]FMKa
J ff{v7KI9MiJT#FI5QSP	[bYjUI5W9P/QSI#[JT9F-PKcFfff9bQSI9G]Q_fO\SF
T#FII9YZ.W#bJT#F]b!GHb/FKaJQ_Y3IYZ"Kc2Y3I#Y3cQSK\RM#FGHb/FKPVFP-JT#fi
F  NaK\SW#Fv JT#FkK\_[Yb/Q_JT9c PVJVYfOP-KI9M
bF]JW#bI9P JT#F"G]W#bbFI(J]v\SKab/[FLM9FG]QSPQ_Y3I2GHY3ccQ_JVJVF]FjUQ_JTPVJQS\g\(Fcf9JX-NFGHJVYb/P]+IJT#FZ:Y3\S\_Yj	QSI#[fiPVJVF]fv

 G]K\SG]WO\SKaJVFPhJT#FPVF-NFGHJVYb/P]U+IKf9bF]NuQ_Y3W9P	Kaf9f9bY3KGTJVYeOWOQS\SM9QSI9[ffb/WO\_F!PVF]JPhZ:Yb	f9b/YeO\_FcP
jUQ_JT-J)jBYG]\SKP/PVFP"p  Y3T#FI r uQgI#[F]bv#xyyy3v{KI2QSJVF]b/KaJQ_NFL[bYj	QSI#[ fiDf9b/WOI9QSI#[hK\_[Yb/Q_JT9c QSP M#FP/Q_[3I#FM
pD   #REfi3"T#Fb/WO\_F fiD[bYj	QSI#[oKaf9f9bY3KGTYZ   #REQgPGHF]bJKQSI9\_XG]\_Y3PVFJVYffjUT9Ka<
J 
1M#Y{FP
Z:Yb[bYj	QSI#[lKl}  PQgI9GHFQ_JoYf9JQScQ_]FPo
K  GHb/Q_JVF]b/QSY3I7v7XF]JffKI#YJKae`\_FM9Q *mF]bFI9GHFQSPJT9KaJQ_JM#Y{FP
I#YJhGHY3c2fOW#JV
F  YNF]boKfOKabJQ_JQ_Y3IQSI9M9W9GHFMle(XlK%PVF]JUYZbW9\_FP]EUKaJT9F]bv`JT9FffG/T9Y3QSGHFffYZB   BE
QSPffJVY5[bYj KaJFKG/TPJVF]fK %eJ  3 ! lc2Y3I#Y3cQgK\wv f9b/W9I#FQ_J]vdKI9MJT#FI[bYj K5PVFGHY3I9Mc2Y3I9Y3cQSK\wv
f9b/WOI#FQ_J]vBKI9MPVY5Y3IW9I(JQS\BK5^`I9K\B} h fiPT9KafmFMZYbcffW9\gKQgP-GHY3cfO\_F]JVFKI9MbF]JW#b/I9FM7 	YJQSGHF
JT9KaJ   #RE K\SPYc2YuM9Q_^9FPJT#FjBFQ_[3T(JYZfiJT#FFHuKc2f`\_FP]vQSIKG]GHYb/M9KIOGHFijUQSJTqnY(Y3PVJQgI#([  P
PVJKI9MOKab/M9PpDuGT9KafOQ_bF r uQSI#[F]bv7xyy3
:PO

O

fi

 n ,j*; 3 ; 7#"($


iwc

3 ;'"U"(7rE.R3 .r#"%$

	
ffff

'" #"($ B6.d.

y R

*. 7#"($

3

uGT9KafOQ_bF r uQgI#[F]blp+xyy3T9KNFQSI(NFPVJQ_[3KaJVFMG]\gKPPQ_^OG]KaJQSY3If9bYeO\_FcPffj	T#F]bFkJT#FlKQSc YZhJT#F
f9bYuGHFM9W#b/F!QSPhI#YJUJVYkf9bYN{QgM#F!KIKG]G]W#b/KaJVF2G]\SKPP	ZYboPVY3cF!YeOPVF]b/N8KaJQ_Y3I	EUKaJT9F]bv`JT9F-K\_[YbQ_JT9c
Y3W#JVfOW9JPKlPVF]JYZLNaK\SW#FPp:Y3I9FZYb!FKG/TG]\SKPPhKI9MjBFFHufmFGHJJT#FG]\gKPPYZnJT#FYeOPF]bN8KaJQSY3IJVY
bFGHFQ_NF-JT#F\SKab[FPVJ	N8K\gW#F!YZdK\S\DvOJT{W9PUemFQSI9[b/K(I FMT9Q_[3T#F]b	JT9KIK\S\YJT#F]bP]	"TOQSP"Kaf9f9b/Y3KG/T5QSP
fOKabJQgG]W9\SKab/\_XWOPVF]Z:WO\Bj	T#FIK5[3Q_NFIFHuKcfO\_FlG]KIemF\_Y3I#[5JVYc2YbFJTOKIY3I#FG]\SKPPp.c-W9\_JQS\gKaeF\
f9bYe`\_FcPvfiKG]KPVFjUT#F]bFjnFFHufFGHJFKG/T YZffJT#FPFG]\SKPPVFPJVYbFGHFQ_NFJT#F[bFKaJVFPVJNaK\SW#FP
GHY3c2fOKab/FMJVYJT#FG]\gKPPVFPJT#F!FHuKc2f`\_FP"M#Y{FP"I#YJ"emF\_Y3I9[-JVY#
"T9F- "	EJ  3 ! %<% b/F]f9bFPVFI(JPQSI#Z:Yb/cK\S\SX-JT9FfiI{W9cffemF]bnYZ0JQSc2FPLJT#FoT(X(fmYJT#FP/QSPRZ.KQS\SPnJVYb/K(I 
JT#FLG]\SKPP YZ`KI-FH#Kc2fO\_FLT9Q_[3T9F]b0JT9KIKhG]\SKPP JVYfijUT9QSGTffQSJ M#Y{FP I#YJ eF\SY3I#[# qnF]Z:YbFL[Y3QSI#[hZ:W9bJT#F]bv
jnFh^Ob/PVJn[FI#F]b/K\SQ_]FhY3W#bG]\SKPP/Q_^OG]KaJQ_Y3I%PVF]JVJQSI#[#v9KIOMbF]f`\SKGHFfiJT#FhGHY3cc2Y3II#YJKaJQ_Y3I'p &)(*!,+dZ:YbKI
FH#Kc2fO\_F	e(XJT#Fhc2YbFh[FI#F]b/K\9Y3I9F'p &)(	!, + "UF]bF*
v !, + m/02 (:x 9 ' QSPRK!NFGHJVYbn[3QSN{QSI9[#vZ:YbBFKGT%G]\gKPP]v
JT#F!cFc!emF]b/PTOQ_fJVYJT#FffG]\SKPPffp ffV %QSP"I9YKI9M ff]x QSP"XFPLYZ JT#FffGHYb/bFPVfmY3I9M9QSI9[-Ye`PVF]bNaKaJQ_Y3
I &{
J!QSPQScfYb/JKIJoJVYI9YJVFJT9KaJJTOQSPPVF]JVJQSI9[QSPc2Yb/F[FI#F]b/K\RJT9KIJT#FWOPW9K\dqLKXFP/QSKIPF]JVJQSI#[#v
QSIjUT9QSGTJT#F]bFkG]KIFHuQgPVJ!FH#Kc2fO\_FPk'p &)(*!,+]!KI9M'p &N(*! + ; p.W9PQSI9[JT#F%I#Y3!I fiDNFGHJVYbI#YJKaJQ_Y3IoZ:Yb
jUT9QgG/
T &1% &  eOW#H
J !,+% 0 ! + ; UE	K(I uQSI#[\_Y3P/PU[FI#F]bK\SQ_]FPhqnKXFPfiJVYJT9Fffc-W9\_JQS\SKaemF\mf9b/YeO\_FcP]vKI9M
fmY3PVJW9\SKaJVFPnJT9KaJJT9F]bFoG]KIkeFfiPVY3c2FfiFH#Kc2fO\_FPnZ:YbjUT9QSGTjnFG]KI9I#YJLf9b/YNuQSM#FfiK-P/QSI#[3\_FhG]\SKPPKaJ"K
JQSc2FvuF]NFIQ_ZL2 14351dKI(XYZJT#FoG]\SKPPVFPLJVYjUT9QgG/TJT#FfiFHuKc2f`\_F	emF\_Y3I9[3PnKab/FhPWOPGHF]f9JQ_eO\SFUJVYKaf9fmFKab
QSI9M9F]fFIOM#FIJ\SX\SKaJVF]bUjUQSJTkJT#FPKc2FYe`PVF]bNaKaJQ_Y3I7
E	K(I {QgI#[ff\_Y3P/PBqnY{Y3PVJQSI#[-bF]fO\SKGHFPnFKGTkFHuKc2f`\_F-'p &)(	!, +e{XKPVF]JnYZRx ' = @ 'p ! 7x ' = @ dFHuKc2f`\_FP]v
jUT#F]b/Fx ' = @ M#FI#YJVFP%JT#F "	KccQSI#[jnFQ_[3T(JkYZ !
 +p J 1.2 1 JT9FI{W9cffeF]b%YZ!G]\SKP/PVFP%JVYjUTOQSG/T1JT#F
FH#Kc2fO\_FffeF\SY3I#[3P RKGTYZJT#FPF-I#F]j FHuKcfO\_FPUQgPUM#FI#YJVFM'p &)(	8(ff
(v7jUT9F]bfi
F 
kKI9
M lPVfOKIiK\S\
NaK\SW#FPQSE
I 02 (3x (5464646(*!@7t:x 9 . %UT#FM9QSPVJVbQ_eOW#JQ_Y3IiYZLJT#FI#F]j FHuKcfO\_FPQSPbFI#Yb/cK\gQ_]FM7vPVYJT9KaJ
;-pV'p &)(	8(ff
{V? % (	  @ ,, , + '  ' = @ (	11 @ 1 j	T#FI#F]NF]b !, +- 
/ % xKI9M !, +-/8%uvOKI9M2YJT#F]bjUQgPVF
 Y3I9Y3 c QSK\ LYe9JKQSI9FMlZ:bY3c JT#F-\SKab[F-}  v`KI9MlK\g\FH#Kc2fO\_FP	PKaJQSPVZ:X{QgI#[Q_J] F
 K F-PY3c2Fffc2
I#YjjBYb !jUQ_JTffJT9QSP bFPVJVb/QgGHJVFM-P/W#eOPVF]J YZ9FH#Kc2fO\_FP]vj	T9QS\_FRG]K\gG]W9\SKaJQSI#[hJT#FGHYbbFPfY3IOM9QSI#["NFGHJVYb
  YZ d#G/T9KafOQSbF r uQSI#[F]bfip+xyy3Bf9bYfmY3PVFUK-GHY3PVJnZ:W9IOGHJQ_Y3IjUTOQSG/TjBFfiPT#Y3W9\gMcQSI9QgcQ_]F"QSI2YbM#F]b
JVYcQgI9QScQ_]FhJT#FbK(I {QgI#[2\_Y3PP]d"TOQSPnZ:W9IOGHJQ_Y3IQSP

 %



4

+ "  

;-pVp'&)(	8(ff
(V

" 
 $   , =   =   1

4

pw~3

"	F]bFv QSPKfiJWOI9KaeO\_FfOKabKc2F]JVF]bdjUT9QgG/T7v3QSI(JW9Q_JQSNF\_XvbF]f9bFPFIJPJT9FUGHY3I#^OM#FIOGHF"QSIJT#F"G/T9Y3QSGHF

YZ(  v`KI9M\_F]NF]bKa[FP	Q_JP	(WOK\SQ_JXL"T#F!eF]JVJVF]b-  QgPUKaJUG]\SKP/PQ_Z:X{QSI9[2FHuKc2f`\_FP]v9JT#Fff\SKab[F]b	QgP"G ?G+I
Y3W#boG]KPVF2T#YjBF]NF]bv0KW#JT#Yb/Q_QgI#[! % 0 xQSPhF{W9Q_NaK\_FIJhJVYKW#JT#Yb/QSQSI#[GHY3cfY3I9FIJPhZ:Yb   QSI5PVF]JP
07  (2 ( 9ffZ:Yb"KabeOQSJVb/Kab
X 0dY2b/FK\S\_XGHY3IOPVJVb/KQSIkJT#FGHY3cfY3I9FIJPYZ   Qg
I 073x (2 (  :x 93vOjnF!T9KNF
G/T9Y3PVFIlJVYYfOJQScQ_]FoJT#FGHbQ_JVF]b/Q_Y3I

 %

4

+ "  

;-pVp'&)(	8(ff
(V

" 
 $  , =   =   1

p 3

p:JT#F]b/F]ZYbFoZ:Yb/G]QSI9["% xLuG/T9Kaf`Q_bF r uQSI#[F]bp+xyy3GHY3I+FGHJW#b/FoJT9KaJ^OI9M9QgI#[!JT#FoYfOJQScK\
NFGHJVYbcQSIOQScQ_QSI9[  QgIF`pw~3%p:jUT9QSGTQSP-PQScQS\SKaboJVY5KI  !\J  J $&% TX{fmYJT#FPQSPffKG]GHYb/M9QSI#[iJVY
JT#FQ_bhM#F]^OI9QSJQ_Y3I9Pv9Yb [3Q_NFIKf`KabJQSG]W9\SKabUN8K\SW9F!YZ#RvQSP fi "	Kab/MijUT#FI!QSP	I#YJ	^9{FM7vKI9M
jUT#FIJT#FhGHY3c2fmY3I#FIJPnYZ   Kab/FhQSIJT#FfiPVF]H
J 07!3x (  :x 93R"T#F	ZY3\g\_YjUQgI#[!PVFGHJQ_Y3IkKM9M9bFPPVFPnM9Q_bFGHJ\SX
JT#F-PVF]JVJQSI#[YZB#G/T9KafOQSbF r #QSI#[F]bffp+xyy3v7KI9Mf9bFPVFI(JPUGHY3c2f`\_FHuQSJX fiDJT#F]YbF]JQSGffbFPW9\SJP"PT#YjUQSI#[

$

:PO

fi

JT9KaJJT9FcQSI9QScQ_KaJQ_Y3IYZ  QSPoKGHJWOK\S\_XfY3\SX{I#Y3cQSK\wvmeOW#JT9QS[3T9\_XGHY3c2f`\SQSG]KaJVFMJVYKGT9Q_F]NFv K\S\
JT#F5c2YbFZYb%jUT9KaJkQ_J%QSPP/W#f9fmY3PVFMJVYe9bQSI#[JVYJT#F5cQSI9QScQ_KaJQ_Y3IYZ  QSI Y3W9b%PVF]JVJQgI#[# 
PVJVb/Q {QSI9[bFP/W9\_JojnFK\SPVYl[3Q_NFv I#YJobF\SKaJVFMJVYlJT#FfOW9bfmY3PVFYZBJT#F2f`KafF]bvQSPfiJT9KaJ!QSJQSPoKGHJW9K\S\_X
JT#F!cK8uQScQ_KaJQ_Y3IkY	
Z  j	T9QSG/TQgP  fi "	Kab/M
"T9FI7vujBFfObFPVFI(JJT#FKaf9f9bYuQgcKaJQ_Y3IK\_[Yb/QSJT9c jBFffT9KNFe`W9QS\_JLKI9MQSc2f`\_Fc2FI(JVFMJVYYf9JQ fi
cQ_]FBJT#FLGHY3c2fOW#JKaJQ_Y3IffYZ   QSI!Y3W#b0PVF]JVJQgI#[p.GHY3cfY3I9FIJP YZ   QSI!JT#FBPVF]J 073x (2 (  :x 98v{K\_Y3I#[	jUQ_JT
Q_JPBf9bYfmF]bJQ_FP T9QS\_F	jBFhZ:F]F\`JTOKaJBJT#FfiQSM#FKPnW9PVFMJVY2cQSI9QgcQ_]F QSIJT#FfiPVF]JVJQSI#[-YZ0uGT9KafOQ_bF r
uQSI9[F]b	p+xyy3nG]KIemFUKMOKaf9JVFMJVYffY3W#bLPVF]JVJQSI#[JVY-f9bYN{QgM#FUKIK\S[Yb/Q_JT9c JT9KaJnQgPdK\_jLKXuPBYfOJQScK\wv
Y3W#bK\_[YbQ_JT9c T9KPoJT9FKM#N8KI(JKa[FJVYemFPQgc2fO\_FvmZ.KPVJ]vKI9MK\gPVYkYf9JQScK\ Z:Yb!I{W9c2F]bY3W9PoG]KPFP]
+IlcKI(XYJT#F]bhG]KPVFP]v9jnF!PT9Yj JT9KaJ	Q_J"QSP"PVJQg\S\mKPVX{cf9JVYJQSG]K\S\_X%Yf9JQScK\7KP !oQSI9GHbFKPFP]

	fiff



fffiffff!"#$&%	fi'& r

fi')(*,+-+-.0/

+IkJT#FoG]KPVFjUT#F]b/F	FKGTGHY3c2fmY3I#FI(JYZ   QSPnbFPVJVb/QSGHJVFMkJVY2JT9FPVF]J 07x3(  :x 93v#G/T9KafOQSbF r #QSI#[F]b
p+xyy3[3Q_NFLK"jLKXJVYfiG/T#Y{Y3PV
F kJVY	cQSI9QScQS]F ZYb KIX!fY3P/PQ_eO\_FGT#Y3QSGHFnYZ   .p W9PQSI#[UY3W#b0I#YJKaJQSY3I`<


%

~

x

\SY[21

0 

0 43 (

p +(

jUQSJT9

0 
%

0 
%

4
+ 
4
+ 

"
"

;-pVp'&)(	8(ff
{V - - 

- /




7 


  -/

;-pVp'&)(	8(ff
{V - - 

- /




7

- /

%t~ / /"(

p 3

% 7U~ / /	4

pw|3

EUF]fO\SKG]QSI#[2JTOQSPLN8K\gW#FoYZ 
 QSIkF`	pw~3v`[3Q_NFP"JT#FoZ:Y3\S\_YjUQSI#[2I#F]jFH{f9b/FPPQ_Y3IkZ:Yb

 % 0

0

 ~,5 0  0  (




pD

jUQSJT 0 0 %76 +    " ;-pVp'&)(	8(ff
(V - -  - 
/87   -/ %t/ /n#G/T9KafOQSbF r uQSI#[F]bp+xyy3"b/KQSPVFfiJT#F!f9bYefi
\_Fc Y ZmcQSI9QScQ_QSI#[  K PM#F]^OI#FMQSI-F{W9KaJQ_Y3I9P"pw~3 KIOMlpD F	I#YjPT9YjJT9KaJdQ_J QSP fmY3\_XuI#Y3cQSK\w

#" E"F> 98 J 6J8TgJMLrJ 3  "% V( a  j V2 J  I H  J  ;: $ "  J 	(%/	=<fi4] 	?> fi5  	;@+fi J % 
 !   fTgJ"!6X I  
fT 
 	    % [N    " Vff % . J jV    I  %  <0 7!x3(  x:9 1
k36)jl {F]FoJT#F  f9fmFI9M9Q_
 

I 

/b KaJT9F]b-PVJVbQ#{QgI#[bFPW9\SJ[3Q_NFIJT#FGHY3IVFGHJW#bF%YZ	uGT9KafOQ_bF r uQgI#[F]bp+xyy3-QSP!JT9KaJffQ_J-QSP
JT#FcK8#QScQ_KaJQSY3IYZ !vRKI9MI#YJ2Q_JPcQSI9QgcQ_KaJQ_Y3I7v jUTOQSG/TQSP1 fi "	Kab/M TOQS\_F%JT9QSPQSPI#YJ
JT#Ff`W#bfmY3PVFYZLJT#Ff9bFPFIJofOKafmF]bp:jnFKab/FQSI(JVF]bFPVJVFMQSIcQSI9QScQ_QSI#[ hv0jBFT9KNF%G/T#Y3PVFIJVY
[3Q_NFoT#F]bFoK-e9b/QSF]Zf9bY{YZP F]JGTYZJT#FfibFP/W9\_J]v{jUT9QSGT%WOPVFPLG]\SKPPQSG]K\bFMOW9GHJQ_Y3I9PnZ:bY3c jBF\S\ fi {I#YjUI
 fi "	KabMf9bYeO\SFcP]


#" E"F> A8 " i 'J TgJ L J  3  "% V a  WV J  I H J  ;: $ "  J 	(% 	=<fi5] 	?> fi   B	 @ fisJ % :_` " O V X I  
 I  fT 
 	    % [N    " Vff % . J jV    I  %  <07!x3(  x:9 1
k36) ?. 6" 7rffAl uF]
F JT#F  f9fmFI9M9Q m
 

:PODC

fi

 
	fiff



!



iwc

	
ffff

' fiffff

P f9bF]NuQ_Y3W9P\_XKab/[3W#FM2QSI"T#F]YbFc +#v{cQSI9QgcQ_QSI#[  QSI-JT#FPVF]JVJQSI9[oYZmuGT9KafOQ_bF r uQgI#[F]bLp+xyy3
G]KIieF-M#Y3I#FffYf9JQScK\S\SXv9eOW#JUKaJhJT#FffFH{fmFI9PVFffYZdGHY3c2fO\_FHlYfOJQScQ_KaJQ_Y3If9bYuGHFM9W#bFPv9jUQ_JTi\SKab[F
GHY3c2fO\SFHuQ_JQSFP]BfiI#FoG]KIljnY3I9M#F]bUjUT#F]JT#F]b"PWOG/Tf9b/Y{GHFM9W9bFP]v#JVYYf9JQgcQ_]FfiY3I9\_X%JT#FGHY3c2fOW9JKaJQ_Y3I
YZ   p.KP/cK\S\mfOKabJYZ 
fivOKabFb/FK\S\_XjnF\S\7jnYbJTJT9FKM9Kaf9JKaJQ_Y3IJVYY3W#bUPVF]JVJQgI#[#v9QSIkjUT9QgG/T
c2YbF!N8K\SW9FPUKabF!KW9JT#Yb/Q_]FM7#
 F-Kab/F!I#Yj [Y3QSI#[JVY%PT#YjJT9KaJhKcffWOG/TP/QSc2fO\_F]b"GHY3cffeOQSIOKaJVYb/QSK\
f9bYuGHFM9W#b/Fv	jUQSJTtGHY3c2fOKab/KaJQ_NF\SXNF]b/X\SYj GHY3c2f`\_FHuQSJXvG]KIte9bQSI#[Yf9JQScK\fib/FPW9\_JPlQgIZ:KQ_b\_X
[FI#F]b/K\PQ_JWOKaJQ_Y3I9P]"T#Fhc2Y3PVJnPQSc2fO\SFjnKXJVYM#FPGHb/Q_emF"c2Y3PJBYZ7JT#FPFhPQSJW9KaJQ_Y3I9PBQSPdJVYcK FfiJT#F
Z:Y3\S\_Yj	QSI#[2KPPW9cf9JQ_Y3IkY3IJT#FoFH#Kc2fO\SFP 


p2BKG/TFHuKc2f`\_FW9PVFMJVYGHY3c2fOW#JVF   T9KPY3I9\SXY3I#Fff]xQSIQ_JP"G]\gKPPNFGHJVYb
]G Kab/F]Z:W9\ bFKMOQSI#[YZBKPPW9c2fOJQ_Y3IpUbF]NFK\SPhJT9KaJoQ_J	Qgc2fO\SQ_FP"JT9KaJfiFKG/T5FH#Kc2fO\_F-emF\_Y3I#[3P
JVYFHuKGHJ\SXY3I#FoG]\SKPPv $ RQ_JLM#Y{FPnI9YJnfObF]NFIJKI%YeOPVF]b/N8KaJQ_Y3I%JVYemF	F\SFc2FIJLYZ c2YbFhJT9KIkY3I#F
G]\SKPPvuKPU\_Y3I#[2KPM9Q *mF]bFI(JnFH#Kc2fO\SFPnP/T9Kab/QSI#[-JT#FPKcFhYe`PVF]bNaKaJQ_Y3IT9KNFffM9Q *mF]bFIJLG]\SKPPFPp:JT#F
ff]xYZmJT#F	G]\SKPPdNFGHJVYbPBQSPRQSI2M9Q *mF]bFI(JdfmY3PQ_JQ_Y3I9PKcY3I#[JT#FPVFUFHuKc2f`\_FP"T#F]bF]Z:YbFv(F]NFIQ_ZQ_J
M#Y{FPRI#YJBQSIJVF][bKaJVFUJT#FUcY3PVJR[FI#F]bK\9Z:FKaJW#bFPdYZmJT#F	b/K(I uQSI#[o\_Y3PPBPVF]JVJQSI#[#v(Y3W#bBKP/PW9c2f9JQSY3I2PVJQS\S\
KW#JT#YbQ_]FPdJVY-GHY3I9PQSM#F]bRf9bYeO\_FcP jUQ_JTI#Y3I]F]bYffqLKXFPBYfOJQScffWOc UT9QSPdQSPdbFK\S\_XQSI(JVF]bFPVJQSI#[#v{KP
cKI(X!GHY3cc2Y3IO\_XW9PVFM-M9KaJKPVF]JPZ.K\S\(QSIJVYhJT#FLG]KaJVF][YbX-YZ9Y3W#b KPPW9c2fOJQ_Y3I7vaKP ZYb FHuKcfO\_FLcKIX
M9KaJKPVF]JP	YZ JT#	
F   LbF]fmY3PQ_JVYbXYZ  KGT9QSI#F  FKab/I9QgI#[2M9KaJKaeOKPVFkp.qn\gK F!F]J	K\D_vxyy3 QgI9K\S\_Xv
F]NFIQSZJT#FKPP/W9c2f9JQ_Y3IM9Y(FPI#YJ	T#Y3\SM7vujnFPT#YjJT9KaJUQSIcKI(X%YZ0JT#F!bFcKQSI9QgI#[kp.QgIJVF]bFPJQSI#[(
G]KPVFP]vuY3W#bLKaf9f9b/Y#QScKaJQ_Y3IK\_[Yb/Q_JT9c QSPnKPVX{cf9JVYJQSG]K\S\_XYf9JQgcK\wv{JT9KaJBQSPv(^OIOM9PdPVY3\SW9JQ_Y3I9PRG]\SY3PVF]b
JVYJT#FffcQSI9QScK\ONaK\SW#FoYZ KH
P !oQSI9GHb/FKPVFP]
uW9f9fmY3PVFkZYbI#Yj JT9KaJp T#Y3\gM9P] fiW#b2Ye VFGHJQ_NFQgPJVYG]K\SG]W9\SKaJVFJT#FNFGHJVYb   YZoPVY3c2F
c2Y3I#Y3cQSK
\  FW9PF%JT9FlPT#YbJT9KIOM9P 0 0  ( 0 (  (5464646( 0  ( JVYM#FI9YJVFJT#FlPW9c YZhjBFQS[3TJP2YZ
JT#FFH#Kc2fO\SFP!PKaJQSPZXuQSI#[ oKI9MeF\SY3I#[3QSI#[kbFPVfmFGHJQ_NF\SX' JV YG]\SKPPVFPff2 (3x (5464646(*!@7txa FjLKIJffJVY
cQSIOQScQ_]
F  KPf9bYfmY3PVFMQSIlFO	p 3LuW#f9fmY3PVFfijUQ_JT9Y3W#J\_Y3PPYZ [FI#F]b/K\gQ_JX%JT9KaJ


  0 (
0 0


46464


0 '  ( (

YJT#F]bj	QSPVFvmbF]Yb/M#F]bfiJT#FG]\SKPPFPhPY%JTOKaJhJT9F]XlNF]b/Q_Z:XlJT9QSPhKPPVF]b/JQ_Y3I7  Q_NFIY3I9\_XJT#bF]F-fY3P/PQ_eO\_F
NaK\SW#FPoZ:Yb!FKGTGHY3c2fmY3I#FI(JYZ   v0JT#FJVFPVJQgI#[lYZK\S\  ' fmY3PP/Q_eOQS\SQSJQ_FP"Z:Yb   QSPoFHufmY3I#FIJQgK\dKI9M
JQSc2F fiGHY3IOPW9cQSI9[# qLW#JUjBF!G]KIlf9bYfmY3PVFK2NF]bXkZ.KPVJ"Kaf9f9b/Y3KG/T7 F-T9KNF-QSI9M#F]FM

B "F> >  
  x 
k36)jl {]F FoJT#F




	





!:(  




- /






- / 1

 f9fmFI9M9Q_
"T{W9P]v(JT#F"Yf9JQgcK\   M#Y{FPRI#YJBemF\_Y3I#[oJVYffKffPVF]JRYZG]KabM9QSI9K\SQSJX  ' v(eOW9JdJVYffKffPVF]JRYZG]Kab/MOQSI9K\SQ_J)X
kp'! . oW#bK\_[Yb/Q_JTOc QSPoJT#FIPVJVb/KQ_[3T(JVZ:YbjnKabM9ffPQgc2fO\_XlFHufO\_Yb/FJT9QSPoPVF]JffYZ kp'! . fiF\_Fc2FI(JP]v
KI9
M F]F]fJT#F-NFGHJVYbT9KNuQSI#[%JT#F\_YjnFPVJfiN8K\gW#F!YZ ! UYJVFJT9KaJhJT9QSPhGHY3cffeOQSI9KaJVYb/QgK\0K\_[YbQ_JT9c
T9KPnJT#FoKM#NaKIJKa[F!JVY-emFfiKM9Kaf9JKaeO\SFhJVY2c2Yb/Fh[FI9F]b/K\7PVF]JVJQSI#[3PLQSI%jUTOQSG/T 8f`KabJQSG]W9\SKabnNaK\SW#FPLKabF
KW#JT#YbQ_]FMkZYbJT9FfiGHY3cfY3I9FIJPY+
Z   vuZ:Yb"KIX%^#uF
M 80I9YJLI9FGHFPPKab/QS\SXF{W9K\JVY {dIkJTOKaJ"G]KPVFv
JT#F!GHY3cfO\_FH#Q_JXkQSP\SKab[F]bv#eOW#J"\SQScQ_JVFM%JVY k'p ! 1  ( 
"T9F]bFKabFP\SQ_[3T(J\_XcYbF[FI#F]bK\UPVF]JVJQSI9[3PQSIjUT9QSGTY3W#bK\_[YbQ_JT9c b/FcKQSI9P2YfOJQScK\wvLQSI
 
 0    fi p   % 0 
 (	 0 "   
 0    V pVp 0 "  
fOKabJQgG]W9\SKabUj	T#FIljnFffG]KI5GHF]bJQ_Z:X  
 (	v pVp 0 "ff
0    fi p   % 0 
J(	 0 "   
 0    V "	F]bF(v 0   M9FI#YJVFPfiJT#F2PW9c YZRjnFQ_[3T(JP	YZBJT#F-FHuKc2f`\_FP
emF\_Y3I#[3QSI9[	KaJd\_FKPVJJVYoG]\SKP/P vKI9M 0    M9FI#YJVFP JT#F"PW9c YZ`jBFQ_[3T(JP YZJT#FLFHuKc2f`\_FP emF\_Y3I#[3QSI#[
:PO

fi

KaJff\SFKPVJJVYiG]\gKPP 0v KI9M   fiemF\_Y3I#[3QgI#[lKaJff\_FKPJJVYiG]\SKP/
P %"TOQSPPT#YjUPJTOKaJ!F]NFIZYb-PVY3c2F
fOKabJQgG]W9\SKabc-W9\_JQg\SKaemF\G]KPVFP]vUY3W9b%Kaf9fObY#QScKaJQ_Y3I1K\_[Yb/Q_JTOc G]KI1bFcKQSI1Yf9JQScK\w fiI#FG]KI
jnY3I9M#F]b2Q_ZJT#FkYf9JQScK\SQ_JXQgPfffObFPVF]bNFMQSIJT#FkW9I#bFPVJVbQSGHJVFMc-W9\_JQg\SKaemF\dZ:b/Kc2F]jnYb` FlI#Yj
PT#YjtJT9KaJ]v#Q_ZYf9JQgcK\SQ_J)XQSPnI#YJLf9bFPVF]bNFMv{jnFoG]KIPVJQS\S\`f9bYNFfiJT#Fo(W9K\gQ_JXYZY3W#b"K\_[Yb/Q_JTOc Z:Yb
[FI#F]b/K\cffW9\SJQS\SKaemF\mG]KPVFP]vOPT9YjUQSI9[KPVX{cf9JVYJQSGfiYf9JQScK\SQ_JXkKP !oQSIOGHbFKPVFP]
fiW9b KafOf9bY#QScKaJQSY3I-K\S[Yb/Q_JT9c QSP b/W9IQSI-JT#Fc-W9\_JQS\gKaeF\(G]KPVFe{X!JVb/KI9PZYb/cQSI#[hJT#FLFHuKc2f`\_FP
KP"Z:Y3\S\_YjUP dFKGTFH#Kc2fO\SF'p &)(	!
 +ZYb"j	T9QSG/Tx ' = @ 
 xffQSPJVb/KI9PZYb/cFMlQSI(JVYx ' = @ FHuKcfO\_FP]vOTOKNuQSI#[
JT#FPKcFM#FPGHb/Q_fOJQ_Y3
I &{vKIOM5Y3I9\_XY3I9F ff]x lQSIJT#FQ_boNFGHJVYbv QSIPW9GTKkjnKX5JT9KaJojBFPfOKI5JT#F
x ' = @ 
 x ff]x !YZ`JT9F"Yb/Q_[3QSIOK\uFHuKc2f`\_F UT#FQ_bjnFQ_[3TJBQSPJT#FY3I#FUYZJT9F"Yb/Q_[3QSIOK\(FH#Kc2fO\SFv(M9Q_NuQSM#FM
e{Xx ' = @  F!JT#FIb/W9IkY3W9bUK\_[Yb/Q_JTOc Y3IJT9QSPI9F]jPVF]J"YZ FHuKcfO\_FP"PKaJQgPVZXuQSI#[2KPP/W9c2f9JQ_Y3Ip 
	YjvmPW#fOfY3PFJT9KaJhZYbhKIXlFH#Kc2fO\SF'p &)(	!  + vjnFffT9KNF%x @  Z:YbhPVY3cF U"T9F]bF!KabF-JjnY
'=
QSI(JVF]bFPVJQSI9[NFGHJVYb/P-jBFWOPVF"T9F2^9b/PVJoY3I#F%QSP   v JT#FYfOJQScK\NFGHJVYbp:YbffKIYf9JQScK\NFGHJVYbH
cQSIOQScQ_QSI9

[  YNF]b%JT#FYb/Q_[3QSIOK\PVF]JYZhFHuKc2f`\_FP]vnJT#FlPVFGHY3I9MY3I9FQSP   vRJT#FlNFGHJVYb%jnF^OI9M
cQSIOQScQ_QSI9[ YNF]bJT#FJVb/KI9PVZ:Yb/c2FM1PVF]J%YZFHuKcfO\_FP] T9KaJ%jnFijLKIJQSPJVYFPJQScKaJVF5JT#F
{W9K\SQ_J)XY+
Z   jUQ_JT%bFPfFGHJJVY2JT9FoYf9JQScK\mNaK\SW#FfiYZ YNF]bUJT#FfiYb/Q_[3QgI9K\mPVF]J"YZFH#Kc2fO\SFP]v -p    
W9PQgI#[ffY3W9bLI9YJKaJQ_Y3I7R"T9F	Z:Y3\S\_YjUQSI#[ffJT#F]YbFc [3Q_NFPKIKIOPVjBF]bJVY2JTOQSPnf9bYeO\_Fclv(e{X{W9KI(JQ_ZXuQSI#[
Q_JP"GHY3I(NF]b[FI9GHF-JVYjLKab/M9P -p    

#" E"F>  p  
 

k36)jl {F]FoJT#F



-p   8x  	'   ff
fi

1

 f9fmFI9M9Q_
"T9F]bF]ZYb/Fv"QSIJT#FPVF]JYZK\S\Uf9bYeO\SFcP2Z:YbjUTOQSG/TZ:YbkPVY3c2F  xav    !vjnFYeOJKQSI
-p  < % p+x  &#p+xV ffAp   v{KI9MY3W#bdemY3W9I9M2GHY3I(NF]b[FPBJVYJT#FYf9JQgcffW9c KP<!QSI9GHbFKPFPdQSIJT9QSPG]\gKPP
YZf9bYe`\_FcP]"qnXc2FKIOPUYZjnYb/M9P]vY3W#bfiPQSc2f`\_FKaf9f9bYuQgcKaJQ_Y3IK\S[Yb/Q_JT9c QSP	{W9Q_JVF!FG]Q_FI(J	Z:Yb
f9bYe`\_FcPjUQSJTl\SKab[F-I(W9cffemF]bUYZdG]\SKP/PVFP] 	YJVFffJT9KaJhW9PQSI9[KP\SQS[3TJ\_Xc2Yb/F!QSI(NY3\_NFMfObY(YZv9jnF
GHY3W9\SMT9KNF	bFM9W9GHFMJT#FUGHY3I9PVJKI(J ff*$>!Z.KGHJVYbLQSI2"T#F]YbFc |oJVYJT9F	P\SQS[3TJ\_X-PcK\S\SF]b ff*$ 7p+x 3$8 u
	Yjv JVYl^#5JT#FQSM9FKP]vJT#FZY3\S\SYjUQSI9[kP/W#eOPVFGHJQ_Y3IMOQSPVfO\SKX{PfiJT9FFH{f`\SQSG]Q_Jp.KI9MP/QSc2fO\_F	PVY3\gW#JQ_Y3I
jUT#FIkJT9F]bF!KabFY3IO\_XJ)jBY%G]\SKPPVFP]

	,$ff  ff !ff

   %   % 
0 0 % 0 
 KI9M 0 (  % 0 bF]f9bFPFIJQSI9[JT#F-ZbKGHJQ_Y3IYZ

#YbfiJT#FPK F-YZRPQSc2f`\SQSG]Q_J)Xv9bFI9Kc2F
FH#Kc2fO\_FPZ:bY3c JT#FffI#F][3KaJQ_NF!KI9MkfmY3PQSJQ_NFoG]\SKPPbFPfFGHJQSNF\_XvOPKaJQSPZXuQSI#[ B"T#Ffib/W9\_FfiJVYGT#Y{Y3PVF
  QSPLJT#FZ:Y3\S\_YjUQSI#[(

B "F> > %n t I  AN ! ! fXhJ 3  "!  3 J  %  I ff $&!    I  %  "
 !
pN
 I  X  I % 
#%$  ($ '
  % p 7x3(  x
#& # $
5 $  # &  $ '
  % p 7!x3(    % p.2(  x
*
#
$
) (   ##*&$  5 $   % p 7!x3( 7 x     % p.2(    % p  x3(  x
(
(

  % p.2( 7!x    % p  x3(
 ' #% $ #+ & ( ) 
  % p  x3( 7x
#&  '
k36)jl {F]FoJT#F  f9fmFI9M9Q_
:PO =

fi

iwc

	
ffff

 +-,
  kf3 "##"($ 

"T#FK\S[Yb/Q_JT9c QSPKP/QSI#[3\_F fiDfOKP/PK\_[Yb/Q_JT9c FKGT b/WO\_FQSPkJVFPVJVFM Y3IO\_X Y3I9GHFvoZ:bY3c JT#F^9b/PVJ
b/W9\SFffJVYlJT9F\gKPVJoY3I#F#Yb!FKGTfmY3PPQ_e`\_F!b/WO\_Fv7KGHb/Q_JVF]b/Q_Y3I	

fffipbF]JW#bI9P ffV"EA  Yb
ffV 	 !
 8-M#F]fmFI9M9QgI#[oY3Ij	T#F]JT#F]bRJT9F	b/WO\_FUPT#Y3WO\SM2emFUbFcYNFM%YbnI9YJ]d"T#F]bFhKabF	J)jBYNF]b/P/Q_Y3I9P
YZJT9QSP	GHb/Q_JVF]b/QSY3I7""T#Fff^9b/PVJ"Y3I9Fv`jUTOQSG/TjBFG]K\Sfi
\ ff+fmFPP/QScQSPVJQgG uvOQSP"e`KPVFMY3IGHY3I(NFIJQ_Y3IOK\0F]b/bYb
cQSIOQScQ_KaJQ_Y3I1"T#FiPVFGHY3I9MY3I9Fv"G]K\S\_FM ff+YfOJQScQSPVJQgG uvLQgPM#F]b/QSNFMZb/Y3c KfObF]N{QSY3W9P2jnYb Y3I
f9b/WOI9QSI#[M#FG]QSPQSY3!I fiDJVbF]FP!p s FKabI9P r  KI9PVY3W#bvxyy3





   ff$ '


&

 FPPQScQSPVJQSGhf9b/WOI9QSI#[-eOW9QS\gM9PBKPVF(W9FI9GHFYZ}  Zb/Y3c

J T#F!QSIOQ_JQSK\Y3I#F  J	FKG/TPVJVF]fv#jnFbFc2YNF
Y3I#F-b/W9\_FvmPW9GTJT9KaJoQ_JPUb/Fc2YNaK\ e9b/QSI#[3P	JT#F\_YjnFPVJfiF]bbYboKc2Y3I9[%K\S\0fmY3PPQ_e`\_FbFc2YN8K\gP	YZRb/W9\_F
QSIJT9FG]W#bb/FIJ!}   RKGTJQScFJT#FF]b/bYb!YZLJT#F%G]W#bbFI(J!}  QSPffI#YJ![b/FKaJVF]bffJT9KIJT#F\SYjnFPVJ
F]bbYbRZY3W9IOMK\_bFKM#Xv

fffipobF]JW#b/I9PdJVb/W#F"Z:YbRK\S\#bW9\_FPdK\_b/FKM#XffJVFPJVFMZ:YbdbFc2YN8K\Dd"T9QSP
f9b/WOI9QSI#[b/F]JW#b/I9P	JT#F2PcK\S\SFPVJ	}  T9KN{QSI9[JT#F2\_YjBFPJhF]b/bYbfiYZRJT#F2PVF{W#FI9GHF"TOQSPUfOb/W9I9QSI9[QSP
b/KaJT#F]bhI9KaJW#b/K\Bp.KI9MPQgc2fO\_Fv9KI9Mc2YJQ_NaKaJVFMe{XkJT#FZ.KGHJUJT9KaJ	JT#F!QSI9MOW9GHJQ_Y3IkYZ JT#Fff\gKab[F!} 
emF]ZYb/FBf9bW9I9QSI#[	M#Y(FP I#YJ \_FKM-JVYKhGHY3INFI(JQ_Y3I9K\#F]b/bYbcQSI9QScQ_KaJQ_Y3I70uWOG/TKhf9bYfmF]bJXffQSPbKaJT#F]b
PVF\SM9Y3c QSI ff+JVYffiM#YjUIkKI9Mf9bW9I#2F !QgI9M9W9GHJQ_Y3IK\_[Yb/Q_JT9cP] 9YbFHuKc2f`\_Fv{GHY3cc2Y3IM9FG]QSPQ_Y3IJVbF]F
QSI9MOW9GHJQ_Y3IK\S[Yb/Q_JT9cPBQSIJT9QSPnPG/T#FcFfiQgI9GHYbfmYb/KaJVFUNF]b/XPVYfOT9QgPVJQSG]KaJVFMf9b/W9I9QgI#[!GHb/Q_JVF]bQSKp   E 
p.qnbFQScKIlF]JUK\w_vxy +(v  +# lfip fffiW9QgI9\SKI7v7xyy +(V



 
	fiff ff$ '


&

s FKab/IOP r KI9PVY3W#blp+xyy3fff9bFPFIJKiI9YNF\K\_[Yb/QSJT9c JVYf9b/WOI#FM#FG]QgPQ_Y3IJVb/F]FP]vdeOKPVFMY3IK
VJ FPVJYNF]bk\_Y{G]K\g\_XYeOPF]bNFMF]bb/Yb/P]JPf9bQSI9G]Q_fO\SFQSPP/QSc2fO\_FFKGT QSI(JVF]b/I9K\UI#Y{M9FYZfiK}h QSP
JVFPVJVFMiY3I9\_XkY3I9GHF-QSIlKemYJVJVY3c fiW9fZ.KPT9Q_Y3I7v`KI9MljnF!FPVJQgcKaJVF!JT#Fff\_Y{G]K\F]bbYb	YNF]bfiJT#Fff\SFKab/I9QSI#[
FH#Kc2fO\_FPob/FKG/T9QSI9[lJT9QSPI9Y{M#Fv0emF]ZYbFKIOMKaZ:JVF]b!JT#Fb/Fc2YNaK\dYZLJT#FI9Y{M#F)ZnJT#F\SY{G]K\F]b/bYb
KaZ:JVF]bRbFc2YN8K\`QSPI#YJd[b/FKaJVF]bRJT9KI2JT9FU\_YuG]K\#F]bbYbdemF]Z:YbFv3fO\gW9P KofmFI9K\_JX-JVF]b/cv(JT#FI2jnFbFc2YNF
JT#F-I#Y{M9FKI9MQ_JP	PW#e9JVb/F]Fn"T9FfmFI9K\_JXkJVF]b/c cK FP	JT#F!f9bW9I9QSI#[FPPFIJQSK\g\_XYfOJQScQSPVJQgGav#JT9KaJ
QSP]v7jnF2JVFI9M5JVYYNF]bf9b/W9I9F2JT#F2M#FG]QSPQ_Y3IJVbF]F "	YjnF]NF]bv JT9K(I uPfiJVYl\_Y{G]K\WOI9Q_Z:Yb/cGHY3INF]b[FIOGHF
bFPWO\_JP]vmKI9M5M9W#F-JVYJT#FffZ.KGHJoJT9KaJoGHF]bJKQSIP/W#e fiG]\SKPPFPUYZnM#FG]QSPQ_Y3IiJVbF]FPKab/F-b/FKPVY3I9KaeO\_Xi\SKab[Fv
s FKab/IOP r  KI9PVY3W9bp+xyy3KabFKaeO\_F%JVYf9bYNFJT9KaJjUQ_JTT9Q_[3Tf9bYeOKae`QS\SQ_J)Xv JT9F%YNF]bf9b/WOI9QSI#[
jUQS\g\0I#YJoemFJVY{YlPVF]NF]bF2jUQ_JT5b/FPVfmFGHJhJVYJT9F2Yf9JQScK\PW9e9JVbF]FYZRJT#FQgI9Q_JQSK\ }fio F2bF]Z:F]bJT#F
bFKM#F]bhJVYJT9FQ_bUfOKafmF]b"Z:YbhZ:W#b/JT#F]b"JT#F]YbF]JQgG]K\bFP/W9\_JP]vOI9YJ	I#F]FM#FMT#F]bF"T9FfmY3QSIJ	QSP"JT9KaJ	e(X
W9PQgI#[JT#F%bFPWO\_JP!YZ s FKab/I9P r KIOPVY3W#blp+xyy3vRjnFG]KIYe9JKQSIKP/QScQS\SKab!JVFPVJZYb2}   F
Fc2fOTOKPQ_]F-JT9KaJfiY3W#b	emY3W9IOMicQS[3TJfiI#YJhFI +YXJT#F2PKcFffJT#F]Yb/F]JQSG]K\ f9bYfmF]bJQ_FPhKPhZYboM#FG]QgPQ_Y3I
JVbF]FP]vBemFG]KW9PVFkYZUJT#FG]Kab/MOQSI9K\SQ_J)XbFKPVY3I9PeOb/Q_F '9XY3W9J\SQSI#FMemF]ZYbF "UYjBF]NF]bvLPW9GTKJVFPVJQSP
QSI(JVF]bFPVJQSI9[!PQSI9GHF	Q_JBcKX\_FKMFPfFG]QgK\S\_X-JVYffNF]bXP/cK\S\9KI9M%QSI(JVF]bf9bF]JKae`\_F"M#FG]QSPQSY3IGHY3ccQSJVJVF]FP]v
jUQ_JTJT#FYe{N{Q_Y3WOP!T#YfmF2JT9KaJffJT#FQ_b!KG]G]W#bKGHXjUQS\S\ I#YJ-M#FGHbFKPVFJVY(YcffWOG/T7k9W#bJT9F]b/c2YbFv JT#F
fOKafmF]bYZ s FKabI9P r  KI9PVY3W9b%p+xyy3ffM#Y{FP!I#YJ!GHY3I(JKQSIFH{fmF]b/Qgc2FIJK\dbFPW9\_JP F%JT9QS(I iY3W#b
GHb/Q_JVF]bQ_Y3IKP"K2jLKXkJVYJVFPVJ	T#FW#b/QSPJQSG]K\S\_X2JT#FoFHufmF]b/QSc2FI(JK\mZ:FKPQ_eOQS\gQ_JXYZPVY3cFoYZ0JT#F!bFPW9\_JPLYZ
s FKab/IOP r  KI9PY3W#bhp+xyy3n"T#FUfOb/QSI9G]Q_f`\_FBYZ7Y3W9bBGHb/Q_JVF]bQ_Y3IQSPRFHuKGHJ\SX2JT#F	PKcFhKPBJT#FUYbQ_[3QSI9K\
JVFPVJ2YZ s FKab/I9P r KIOPVY3W#blp+xyy3  ffVG]KIjBFGHY3c2fOKab/FvRjUT#FIJVFPVJQSI9[5PY3c2Fkb/W9\_Fp (  ffKI9M
W9PQgI#[lJT#FFHuKc2f`\_FP!JTOKaJ-PKaJQgPVZXJT9Fb/W9\SFv0JT#FF]bbYb/PffemF]ZYb/FKI9MKaZJVF]b-bFc2YN{QgI#[JT#F%b/W9\SF25
 F]
J  ,    = 1 bF]f9b/FPVFIJnJT#FhF]bbYbnemF]ZYbF	bFc2YN{QSI9[ffJT#F	b/W9\_Fv(Y3I%JT#Fh\SY{G]K\PKc2fO\SH
F =?> ,    = 1 PKaJQSPZXuQSI#[
c2Y3I#Y3cQSK#\  }hFI#YJV
F nKP JT#FLF]bbYb emF]ZYb/FBbFcYNuQSI#[p (  vPVJQg\S\c2FKP/W#bFM-Y3IJT#F\_YuG]K\{P/Kc2fO\_F
:PO $

fi

=?> ,    = 1 ff"T#FIjnF2M#F]^OI#F-JT#F2T#FW#b/QgPVJQSG ff+fmFI9K\SJX p:f9bY{YZdY3cQ_JVJVFM9fiQ_JfiQSPfiKkbY3W#[3T5W#f9fmF]bemY3W9IOM
YZ s FKab/I9P r  KI9PY3W#bffp+xyy3v  FccKx 

 ,    = 1

pD{F]JpVp
(   V  ~3{\_Y[7p -0  \_Y[hx
4

%

pw3

G =A> ,    = 1 G

{F]JpVp
(   VM#FI9YJVFP	JT9FffcK8#QSc-W9c I{W9cffeF]b	YZd\SQ_JVF]b/K\gPYZdK\S\bW9\_FP"FH#GHF]f9J2p(  QgIlJT#F-G]W#b fi
bFI(Jn}  v(JT9KaJLKIKabe`Q_JVb/KabX2FH#Kc2fO\_FhGHY3W9\SMPKaJQSPVZ:Xd"T#F	Z:KPVJnG]K\SG]W9\SKae`QS\SQ_J)XffYZ   ,  1 QSPRYeOJKQSI#FM
KaJhJT#F!FHufmFI9PVF!YZBK[bFKaJVF]bob/QgP kYZYNF]bf9bW9I9QSI#[#v`jUT#Y3PVFffF *FGHJPhY3I5PVY3c2F-PcK\S\0 M9 = KaJKPVF]JPhjnF]bF
FHufF]bQSc2FI(JK\S\_XM#b/KcKaJQgGZ:YbJT#F%KG]G]W9b/KGHX+IY3W#b-FHufF]bQSc2FI(JP]vjUT9QSGTGHY3I(JKQSINF]bXP/cK\S\
M9KaJKPVF]JPvOjnF!T9KNFffG/T#Y3PVFIJVYJW9I9FKfOKab/Kc2F]JVF]bh\SQScQ_JQgI#[ffJT9FoF *FGHJP	YZ JT9QSPGHY3cffeOQSI9KaJVYb/QgK\W#f fi
fmF]bemY3W9I9M7  Yb/F2f9bFG]QSPVF\SX	v FT9KNFG/T#Y3PFI5JVYkWOI9Q_Z:Yb/c\_XkbFPKcfO\_"
F =A>QSI(JVY%Kk\SKab[F]bP/W#eOPVF]J
YZ aFH#Kc2fO\SFP]vLjUT#FIJT#FiQSI9Q_JQSK
\ =A> GHY3I(JKQSI#FM1\_FPPJTOKI aFH#Kc2fO\SFP]qnXJT9QSPvBjnF
KabJQ_^`G]QSK\S\_X%QSI9GHbFKPV#
F G =A> ,    = 1 GuKIOMcQScQSGhZYb	JT#F!PcK\g\M#Y3cKQSIOPI#F]j M#Y3cKQSI9PLjUQ_JTKIQSM#FI(JQ fi
G]K\d\SKab/[F]bPQ_]FvjUQSJTiJT9FKM9M9Q_JQ_Y3IOK\emFI#F]^9JPfiJTOKaJobFKPVY3I9KaeO\SFGHY3c2fOKab/QSPY3I9PficKXemFcKM#F2Y3I
f9b/WOI9QSI#[#
"T9FoN8K\SW9FoYZ 	
fffipVp (  VQSPJT#F]bF]Z:YbF ffV"A
E  8Q *  ,    = 1    ,  1    

 =

 
< 	

C

 `>



#Y3\S\SYjUQSI9[iKabF%JT#bF]F%FH{fmF]b/Qgc2FIJK\BPVFGHJQ_Y3I9PvdKQSc2FMKaJffJVFPVJQgI#
[  Y3IJT9bF]F%QgPPW#FP]i"T#F
^9b/PJBf9b/FPVFIJPLFHuJVFI9PQSNFhb/FPW9\_JPnY3I%JT#FfiJVb/KM#F]Y*PQSc2f`\SQSG]Q_J)XfiKG]G]W#bKGHX2Ye9JKQSI#FMke{X
!v9KI9M
GHY3c2fOKab/FP	JT9FffbFP/W9\_JP	jUQ_JTlJT9Y3PVFYe9JKQSI#FMiZYbfiPVJKaJVF fiDYZ fiDJT#F fiKabJ!K\S[Yb/Q_JT9cP	"T#F-PVFGHY3I9M[Y{FP
Y3I%QSIM9F]f9JTKI9K\SX(]FPnZ:YbBJT#FficQSIOQSI#[ 8QSI(JVF]bf9bF]JKae`QS\SQ_J)XffQSPP/W#Fv(KI9MJT#FhJT9Q_b/Mf9bFPVFI(JPnbFPW9\_JPRY3I
I#Y3QSPFoJVY3\_F]b/KI9GHF

 !

" !$3y>fi?;'87

$ ff


3

 $

ufmF]b/QSc2FI(JPijBF]bFG]Kabb/QSFM Y3W9JW9P/QSI#[ JT9bF]FN8Kab/QgKIJPYZ
 fijUQ_JT Yf9JQScQgPVJQSGfOb/W9I9QSI9[

:p Y(vhjUQ_JT1fFP/PQScQSPJQSGkf9b/W9IOQSI#[1p:fvhKI9M1jUQ_JT#Y3W#Jf9b/W9IOQSI#[1p ff3  KaeO\_Fxf9bFPVFI(JPkPVY3c2F5bF fi
PW9\SJPY3IN8Kab/QSY3W9P!M9KaJKPF]JP]vRc2Y3PVJffYZ"jUT9QgG/TjnF]bFJKFIZbY3c JT#F   ob/F]fY3P/Q_JVYbX5YZ"cKG/TOQSI#F
\_FKab/IOQSI#[M9KaJKae`KPVFp.qn\gK FF]JK\w_vffxyy3 #YbFKG/TtM9KaJKPF]J]vUJT#FF]NFIJWOK\oM9QSPGHbF]JQSKaJQ_Y3I YZ
KaJVJVb/Q_e`W#JVFPjLKPifF]b/ZYb/cFM Z:Y3\S\_Yj	QSI#[f9bF]NuQ_Y3W9PbFGHY3cc2FIOM9KaJQ_Y3I9PiKI9MFHufmF]b/QSc2FI(JK\!PF]JW#fOP
p.M#F  KabN8K\gT#Y  Y3c2FP r  KP/G]W#F\wv0xyy +("UT#FbFPWO\_JPjnF]bF!GHY3c2fOW9JVFMW9PQgI#[K2JVF!I fiDZ:Y3\SMPJVb/KaJQ fi
^9FMkGHbY3PP"N8K\SQgM9KaJQ_Y3If9bYuGHFM9W#bF2fip fffiW9QSIO\SKI7v`xyy|3n"T#Fo\_FKPVJLF]bb/Yb/PLZYb KabFW9IOM#F]b/\SQSI9FM
Z:Yb	FKGTM9Y3cKQSI7#YbhJT#FffP/K FffYZdGHY3c2f`Kab/QSPVY3I9Pv`GHY3\SW9cI ff/hJT#F]b/
P fmY3QSI(JPY3W#J	N8Kab/QSY3W9P"bFP/W9\_JP
Z:Yb-YJT9F]bK\_[Yb/Q_JT9cP]vdQSI(JVFI9M#FMJVYT#F\_f[F]JVJQSI#[K[FI#F]b/K\LfOQSGHJW9bFYZUjUT9KaJ2G]KIemFJT#FkfmF]b fi
Z:Yb/cKI9GHFP!YZBF G]QSFIJ-Kaf9f9bY3KGT#FPjUQSJTM9Q *mF]bFIJoY3W9JVfOW#JPp.M#FG]QgPQ_Y3I\SQSPJP]vJVbF]FP]v GHY3ccQSJVJVF]FP]v
F]JGavOQSIJVF]b/cPnYZF]b/bYb/Pop.KI9M7vujUT#FI%Kaf9fO\gQSG]KaeO\_Fv{PQ_]FPB{Y3cF	YZJT9FficY3PVJLbF\_F]NaKIJnbFPW9\SJPRZ:Yb

 KabFPWOccKab/Q_]FMQgIkJT#F!PG]KaJVJVF]bf`\_YJP"YZ 0Kae`\_F!~{
"T9FiQSI(JVF]bf9bF]JKaJQSY3I YZ KaeO\_FxW9P/QSI#[Y3I9\SXF]bbYb/P[3Q_NFP%JT#FiKM9N8KI(JKa[F5JVY jUQ_JT
fmFPPQScQSPVJQSGf9b/W9I9QgI#[#vUK\S\hJT#Fc2Yb/FKP p:f%T9KPkJT#FKM#NaKI(JKa[FYZ-f9bYNuQSM9QgI#[PQgc2fO\_F]b
Z:Yb/cffWO\SKPhJT9K/
I -p ff3v0KI9M5T9KPoKkcffW9GT5PQSc2fO\SF]b	f9bW9I9QSI#[PJKa[FJT9K/
I p:Y(ffE"FP/W9\_JP
K\SPVY5GHY3c2fOKab/FZ.KNYb/Kae`\_XJVY5JT#F ff/fiJT#F]
b lb/FPW9\_JP]v eOWOQS\SM9QSI9[%FQSJT#F]b-}  P]v}hUP]vYb-}  Pi"T#F]X
KabF!K\g\7JT#F!c2Yb/FffQSI(JVF]bFPVJQSI9[Q_Z0jnF!GHY3c2f`KabF!JT#F!F]bbYb/P"QgIJT#F!\SQS[3TJYZ JT#F-PQ_]FPYe9JKQgI#FM7B9Yb
JT#F ff RGT#6Y lM#Y3cKQSI7v  jUQ_JTifFP/PQScQSPJQSGf9bW9I9QSI#[eFKaJPoQgc2f9bYNFM  h~e{XJ)jBYlfmY3QSI(JP]v
:

$Q

fi

}hY3cKQSI
 W9PVJVb/K\gQSKI
qLK\SKI9GHF
qnbFKPVJ fi 
qLW#fOK
BG/T#Y
 \gKPP/~
"	FKabJ fi
"	FKabJ fi 
"	FKabJ fi "
"	F]fOKaJQ_JQSP
"	Yb/PVF
b/QSP
 KaeYb
 R}
 R}hF]NFI
 R}hF]NFIO~
 W9I#[
 Y3(I mx
 Y3(I #~
 Y3I(
BQScK
 Y3\_F
uT{W#JVJ\_F
"QgG]0KG]0Y(F
FT9QgG]\_F~
YJVF
YJVF3x
KNF]ZYb/c
QSI#F
fi}o|

F]bb
x2)4 3
~)~ 4 
 4 +|
)| 4 3
)~ 4Sx +
~u3x 4a|
~+ 4 
~)~ 4&ya
~)~ 4&|3
~a2 4 y
2x )4&~|
)4 



-p:Y(

2x )4 
ux34 3y
x )4Sx
a2 4 
+)
~ 4 a
2x )4 
~+ 4 + 
)4  +
~)y 4&|ux
)| 4&|3
)4&~3
~)~ 4 +(
~)| 4 +(
)| 4&ux
x]2 4&ya
a2 4 +y
x]2 4 
x)| 43 



x34Sx
S4 x
x34Sx
)4&~
3x 4&
x34 
)4Sx
~)4&y
)4&y

+

)4 +

x34
x34&y
~)4&y
|)4&y
~)4
+ 4Sx
x34 
+ 4Sx
)y 4 
)4&|
)~ 4&~
x34 
3x 4 
)4
)~ 4&
3x 4&y
)~ 4 
+ 4&
)4 
)4&~

iwc

8

3x 4&
x]24 
+ 4
x~)4 +

)4&y

4
)4&y
y)4Sx
x]24&y
) 4
)4&|
+ 4&|
)4 
)4 +
)| 4Sx
x)| 4 +
)4&
y)4 
) 4 +
+ 4&
)4&y
+ 4Sx
)~ 4 
x + 4+
 4&
)4 
+

)4 
)4&~
|)4&~

x + 4+


-p:f

]F b/b
x|)4 
x + 4a|
+ 4 3
3 4Sx +
~3 4&|
~u3x 4Sx
x)y 4 +
~u3x 4& 
~a2 4 +5
x)y 4&~ +
2x )4 3
)4 



x2)4 
~+ 4&~
x~)4 + 
~ )4S2x 
+)
~ 4 a
2x )4 
~u3x 4 +
)y 4&y
~)| 4Sx
 )4 ~
)4&~3
~a2 4Sx]
~)| 48
) 4 +3
)y 4&y
~ )4 +(
)y 4 +(
x 4 a



+

3x 4&|
y)4&y
)4 
4

4 
x34
)4 
|)4 
)4 +
 4
)&4 
~)&4 y
)4 
x|)&4 ~
)&4 
 S4 x
~)&4 |
)&4 ~
x)&4 ~
+ 4 
)4 
+ &4 ~
x34 
|)4 
+ 4
+ 4
 4
 4
)4 
|)&4 ~
+

	
ffff

8

+ 4Sx
~3 4 
u~ x34 
x)| 4&|
x3x 4Sx
)4 +

ux34 +

~3 4 +
~+ 4&~
x 4 
x]24 +
 S4 x
)| 4&|
~u3x 4 
)y 4&~
~)| 4Sx
 S4 x
x )4 
|u3x 4 
) 4&y
~)y 4 +
x)~ 4
)~ 4 
x 4&|
x3x 4&~
)4 
x + 4&y
x 4 
) 4Sx
x S4 x

-p ff3

]F bb
x)4Sx +
x + 4&~y
)| 4&ya
3 4Sx +
u3x 4 +~
~)| 4 +(
~u3x 4& 
~)4 +
~a2 4 
2x )4&~y
~a2 4&~|
~a2 4&|3
x)| 4&|3
~+ 43 
~+ 4&| 
~u3x 48
+)
~ 4 a
2x )4 
u3x 4&a
x)~ 4 a
)~ 4&yy
3 4&| +
+ 4 ux
~ )4 a
 )4Sx
x]2 4 
x)~ 4 a
~a2 4&~ +
 4&y
~)~ 4&|y

8

	

+ 4&
x)4
 4
 4 
~+ 4&|
+

x 4 
&4 y
~)y 4 
~) 4 +
) 4&
x~)4 
+32
 4&
+)
| 4&~

4

++

x~)4 
x)4 

x + 4
x x34 +
x)~ 4 
)4
)4&
x)y 4 
)y 4&y

~+ 4 +
)~ 4
y)4 +
~+ 4&
y)4 
~)~ 4&~
~+ 4 
)~ 4 
2x )4&y
x|)4 +
y)4 
x )4&|
+32
 4Sx
+ 4&~
x)y 4&

)4 

+

~|)4
u3x 4
 &4 y
)| 4

~)4 +
~u3x 4&y
 )4&
 &4 ~
x 4&y
)~ 4Sx
x~)4 
|) 4&y
|)4&
+ 4
+ )4
+)
| 4
x) 4&y
~)y 4
| )4 
) 4&y
)
~ 4

hJT#F]b
2x )4Sx 4ff
 0 )
~a2 4Sx @fffi 0 )
+ 4&y .!( @ )
3 4  42*  0 )
)~ 4  4ff  
~|)4  @ 0 )
~ux34 G
~~)4  . 0 
~u3x 4& fi0 4 
x)y 4&~ 4A 0 
2x )4 ( 4  )
)4 G
x)| 4 ux fi @ 
~ )43  ( . . 
x )4  (
 . )
~ )4Sx .ff  )
+)
| 4&
| "
x)| 4&||  0 
~)y 4 y ( @ 0 
)~ 4&|3 . 0 
~ )4&G
y 

)4  @!( fi )
x34 .ff
 @ )
x)4  ( 40 
 )
~)4&| 24 0 )
+ 4  
 fi 
x]24&y fi  

 )4  .!( @ @
~~)4&"



~ux34&~ @ 0 )

0Kae`\_Fx ufmF]b/QSc2FI(JK\mbFPWO\_JPW9PQSI9[1!
3Y I(NFIJQSY3I9P  8 QSP"JT#FffjUT#Y3\_FffI(W9cffemF]bUYZd\SQSJVF]b/K\SP"YZdK%}  v	QSPUQSJP	I{W9cffeF]bUYZdb/W9\_FPL9Yb
ff/hJT#F]bPuv#I(W9cffemF]b/PRKab/F	[3Q_NFI%Y3IJT#FhZYb/c F]bbYb PQ_]F v(j	T#F]bFikQSPBQSc2f9b/YNFM  h~%p  fi~ fi n L
,
1
eOW9Qg\SM9QSI#[l}  P]vdPQ_]FQSPffJT#FI(WOc!emF]b-YZU\SQ_JVF]b/K\gP%p.}hY3cQSI9[Y3P]vUxyy3 @ QSPff  }  eOWOQS\SM9QSI9[l}  P]v
r  Kaf9f(Xvxyy3 QgP  +# p.#b/KI9
G  r Q_JVJVFI7v"xyy3 tQSP+} 
I#YJKaJQ_Y3IOP!Z:Y3\S\_Yj p UYuG  
eOW9Qg\SM9QSI#[o}  P]vuI#YJKaJQ_Y3IOPRZ:Y3\S\_Y`
j 5p 	Y{
G  r  KPG]W#F\wv7xyy 3 "2QSPU
x fi 	FKabFPVJ 	FQ_[3T(eYbBb/W9\_FhKI9M
)!QgP  +# lp:f9b/WOI#FM7v#M#F]Z.KW9\_JLfOKab/Kc2F]JVF]bPneOW9Qg\SM9QSI#[ff}hUP]z9JT#FPQS]FoYZ KJVb/F]F!QSPQ_JPj	T#Y3\_FoI(WOc!emF]b
YZI#YuM#FP]


eOW#J	JT#Fff}  Ye9JKQSI9FMiGHY3I(JKQSI9P	bY3W#[3T9\SX%FQS[3TJhJQSc2FP	ZF]jnF]bo\SQ_JVF]b/K\gP"JT9KI  fi~ fi n fi
  P	M#FG]QgPQ_Y3I
\SQSPJ] Z-jBFFH#GHF]f9J ff YJVFuv-Y3I K\S\fiYJT9F]bf9bYe`\_FcPkY3ItjUTOQSG/TjnFM9QSPfY3PFiYZ  fi~ fi n fi P

$

: :

fi50

50

45

45

45

40

40

40

35

35

35

30
25
20
15

30
25
20
15

30
25
20
15

10

10

10

5

5

5

0

0
0

5

10

15

20 25 30 35
WIDC(p) err. (%)

40

45

50

0
0

5

10

15

20 25 30 35
WIDC(p) err. (%)

40

45

50

0

100

100

80

80

80

60
40
20

WIDC() #litterals

100

WIDC() #litterals

WIDC(o) #litterals

WIDC() err. (%)

50

WIDC() err. (%)

WIDC(o) err. (%)



60
40
20

0
20

40
60
WIDC(p) #litterals

80

100

10

15

20 25 30 35
WIDC(o) err. (%)

40

40
60
WIDC(o) #litterals

80

45

50

60
40
20

0
0

5

0
0

20

40
60
WIDC(p) #litterals

80

100

0

20

100

 KaeO\_F!~&huG]KaJVJVF]b/fO\_YJPPW9ccKab/Q_QSI9[	PVY3c2FLbFP/W9\_JP0YZ` KaeO\_FfixBZ:Yb JT#FLJT#bF]F.'OKNYb/P YZ 
!vQSI
JVF]b/cPLYZF]bbYbp:^9bPVJLbYjhBKI9MkPQS]F2#p 8nv#PVFGHY3I9MkbYjhv#Y3IkJT#FfiJT9Q_bJ)XM9KaJKPVF]JPRKGT
fmY3QSI(JoKaeYNF2JT#F  % \SQSI#FM#F]fOQSGHJPoKkM9KaJKPF]JZYboj	T9QSG/TJT#FK\S[Yb/Q_JT9cQSI5KaeOPG]QgPPK
fmF]bZ:Yb/cPLemF]JVJVF]b
bFPWO\_JP]v3jnF"Y3W#JVfmF]bZ:Yb/c  fi~ fi n Y3I2emYJTKG]G]W#bKGHXKI9MPQ_]F QSI9K\g\_Xv3Y3I ff YJVFuv9I#YJVFUJT9KaJ

 jUQ_JT5YfOJQScQSPVJQgGf9b/W9I9QgI#[%QgPoP\SQ_[3T(J\_XY3W9JVfF]b/ZYb/cFMe(X  fi~ fi n  e{X~)4 ux kv e`W#JoJT#F
}  Ye9JKQSI9FMlQSP a2N /  JQScFP"PcK\S\SF]bJT9KIkJT#F!M#FG]QgPQ_Y3Ik\SQSPVJLYZ  fi~ fi n U9)Z jnFM#jBF\g\mY3IJT#F
bFPWO\_JPUYZ  +# {vPQScQS\SKabUGHY3I9G]\gW9PQ_Y3I9PhG]KIemFffe9b/Y3W#[3TJ UY3Ix~Y3W9J	YZhxMOKaJKPVF]JPfiY3IjUT9QSGTjnF
b/KI  +# {v p:f	^OI9M9PfiPcK\S\_F]bfiZYbcffW9\gKP]vKI9MPVJQS\g\emFKaJP  +# & PffKG]G]W#b/KGHXY3Iy%YZnJT#Fc
 {W9KI(JQ_JKaJQ_NFGHY3c2f`Kab/QSPVY3IYZ 8 Ka[3KQgI9PVJoJT#F2I{W9c!emF]bfiYZBI9Y{M#FPhYZBJT#F}fi"PoPT#YjUPhJT9KaJoY3I
+M9KaJKPVF]JPfiY3W#JhYZRJT9F%x 5p  Y3\_Fv0uT{W#JVJ\_Fvm"QSG] KG]Y{Fv  W9PVJVb/K\SQgKI`vOJT#F2}  PfiKabFc2YbFJT9KI|
JQSc2FP	PcK\S\SF]bv9jUT9Qg\_FoJT#F]XY3I9\_XQSIOG]W#bUK\SY3PPUQSIKG]G]W#b/KGHXlZ:Ybfi~YZJT#FcvKI9M\SQgcQ_JVFMkJVY5xa& %
#Yb JT9QSP0\SKaJVJVF]b f9bYeO\_Fc p."QSG] KG]Y{Fv3K	[3\SQSc2fOPFdKaJ  KaeO\_FhxRPT#YjUP0JT9KaJ0JT9FB}  P]vajUQ_JTff\_FPPJT9KI
b/W9\_FP	Y3I5KNF]b/Ka[Fv F]F]fOPoGHY3c2fOKabKaJQ_NF\_XicY3PVJ	YZBJT#F2QSI#Z:Yb/cKaJQ_Y3IiGHY3IJKQgI#FMQSI}hUPhTOKNuQSI#[
c2YbFUJT9KIKT{W9I9M#b/FM-\SFKNFP]BfiIcKIXf9b/YeO\_FcP jUT#F]b/F"cQSI9QSI9[fiQgPPW#FPKabF	GHb/W9G]QSK\wv3P/W9G/TK!PQ_]F
bFM9WOGHJQ_Y3I%jBY3W9\gMemFfijBF\g\`jnYbJT%JT#Fp.GHY3c2fOKabKaJQ_NF\_XkP\SQ_[3T(Jd\SY3PPLQSIkKG]G]W#b/KGHXv9emFG]KW9PVFfijnF F]F]f
K!P/Q_[3I9Q_^OG]KI(JfOKabJRYZ7JT#FUQSI#Z:Yb/cKaJQSY3I2Y3INF]bXP/cK\S\#G]\SKPP/Q_^9F]b/P]vJT(W9PB\S#Q F\_X-JVY!emFUQgIJVF]bfObF]JKaeO\_F

  n"<74" fi#E" 72@y;'87 .d.53#"F.

+I5JT#F h}o|M#Y3cKQSI7v7FKG/TFHuKc2f`\_F2T9KPx]keOQSIOKabXlNaKab/QSKaeO\_FP!"T#FJVFI(JTQSPoQ_bb/F\_F]N8KI(JoQSIJT#F
PVJVbY3I9[FPVJPVFI9PVFp  Y3TOI7v s Y3T9KN{QDv r 8'9F][F]bvffxyy+( "T#FJKab[F]JlGHY3I9GHF]f9J
QSP%K  fi} h p.K
:



$4z

fi

iwc

7

 0 6  ( 6 
 4 6   6-
 fi 6  * 6-
 ( 6  
 0 6  .


	
ffff



fi/x
.

fi/x


x

fi/x
@



M#F]Z.KW9\_J ) 

x

fi/x

x

x
u&y|

(

x

fi/x
fi/x



u  3 

 Q_[3W#bFx  }  Ye9JKQSI#FMY3IJT9F fi}fi|!M#Y3cKQSIjUQ_JT-p:fdUT#F"^9b/PJdJT#bF]F	b/W9\_FPFH#KGHJ\_X
FI9GHY{M9FJT#FoJKab[F]JhGHY3I9GHF]f9J]v`KIOMkJT#F!Q_bb/F\_F]N8KI(JLN8KabQSKaeO\_FQSPKae`PVFIJUZbY3c JT#F}  






4

0

fi
*

	

@



@






*




@

(

fi




*




	
0

	



(



.

	

	


(






4




fi



(



.

	



@




	




fi

	

	

 Q_[3W#bF!~&  aK b/JUYZK}fiYeOJKQSI#FMY3IJT#F fi}fi|M#Y3cKQSIljUQ_JT  +# {  Y3PQ_JQSNF!\SQ_JVF]b/K\gP\SKaemF\7JT#F
QSIJVF]bI9K\uI#Y{M9FP] 0YoG]\SKPPQSZX!KIYe`PVF]bNaKaJQ_Y3I7vJT#F\_F]Z:J FM#[FLYZKfiI#YuM#FLQSP ZY3\S\SYjnFMjUT#FI
KI2YeOPVF]bNaKaJQ_Y3IGHY3I(JKQSI9P	p fffiff` %  JT9FfY3P/Q_JQ_NF"\SQ_JVF]bK\wv3KI9MJT#Fb/QS[3TJFM#[F	QSP ZY3\g\_YjnFM
YJT#F]bjUQSPFffp J 1.2 1dJT#F	\gQ_JVF]b/K\OQgPRI#F][3KaJQ_NFfiQSI2JT9FUYeOPVF]b/N8KaJQ_Y3Id"T#F"emY3\SMP/(W9Kab/F	QSPBW9PVFM
JVY%M9QgPVfO\SKXJT#Ffff9bFPVFIOGHF!YZJT#F-Q_bbF\_F]NaKI(J"N8KabQSKaeO\_FQSIJT#F!JVb/F]F  I9KQ_NF-GHY3INF]b/P/Q_Y3I
YZ JTOQSPLJVbF]F!QSIkbW9\_FPLZ:Yb"emYJTlG]\SKPPVFP"[FI#F]b/KaJVF
P a2b/W9\_FPv{Z:YbUK2JVYJK\YZxay2\SQ_JVF]b/K\gP]

$

: 2;

fi

} 	
 jUQ_JTFKG/Tc2Y3I#Y3cQSK\RGHY3I(JKQSI9QSI9[iKaJc2Y3PVJ-JT#bF]Fk\SQ_JVF]b/K\SPhYNF]b2JT#F^9b/PVJ-I9QSI#FN8Kab/QgKaeO\_FP
p  0 6  ( 6  . p 4 6   6    p fi 6  * 6  @ uW9G/TK2ZYbcffW9\gK-QgPJX{fOQSG]K\S\SX%T9KabMkJVYFI9GHYuM#F
W9PQgI#[K5PcK\S\BM#FG]QSPQSY3IJVbF]F5+IY3W9bffFHufmF]b/QSc2FI(JP!j	Q_JT 
-p:Y(-KI9
M p:fvjBFT9KNF
bFcKa
b FM5JT9KaJoJT#FJKab[F]JoZ:Yb/cffWO\SKQSJPVF\_ZRQSPfiK\ScY3PVJoK\_jnKX{P!KI5F\_Fc2FI(JhYZBJT#F2G]\SKPPQS^9F]bfieOW9QS\_J]v
KI9MkJT#FffQ_bbF\_F]NaKI(JLKaJVJVbQ_eOW#JVFQSPK\SjnKX{P"Kae`PVFIJ]B QS[3W#bFxoPT#YjUP"KIFH#Kc2fO\SFoYZ }  jUTOQSG/TkjLKP
Ye9JKQSI9FMkY3IlKb/W9I%YZ ! 	YJVFJT9KaJJT9FGHY3I9GHF]f9JbF]JW#bI#FMQSPK  fi}  9 Q_[3W#bF!~M9F]fOQSGHJPK
fOKabJYZ K2JVbF]FoYe9JKQgI#FMkY3IJT9QSPLM#Y3cKQSIkjUQ_JT  +# { T9QS\SFhJT9FoJVbF]FKaf9fmFKab/PLJVYemFo(WOQ_JVFo\SKab[F
Z:YbnJT9FhM#Y3cKQSI7v(I9YJVFhJT9F	f9b/FPVFI9GHFhYZJT#FfiQ_bbF\_F]NaKI(JRNaKab/QSKaeO\_F	QSIJT9F	JVbF]FvujUTOQSG/T%Q_JBGHY3I(JVb/Q_e`W#JVFP
JVY-FI9\SKab[F	jUT9QS\_F"cK {QSI9[!Q_JRT9KabM#F]bRJVYcQSI#FdfiI%cKI(X2YJT#F]bnM9Y3cKQSI9P]vjBFhYeOPVF]bNFMfmF]b/PQSPJVFIJ
b/W9\SFPYbhP/W#e7GHY3I9GHF]f9JP"JT9bY3W#[3TlJT9Fx]GHb/Y3PP fiDNaK\SQSM9KaJQ_Y3Ilb/W9I9P]"uQScQS\SKab/\_XJVY fi}o|{v`jUT9FI#F]NF]bUjnF
GHY3W9\SMkcQgI#FUJT#FfibFP/W9\_JPBjUQ_JTkKP(W G]Q_FI(J\_XKG]G]W#bKaJV,
F {I#YjU\_FM#[FfiYZ0JT#FoM#Y3cKQSI7v{JT#FPVFfifOKaJVJVF]bI9P
jnF]bF-cY3PVJ	QSI(JVF]bFPVJQgI#[#U#YbfiFH#Kc2fO\_Fv`JT#F-}  P	Ye9JKQSI9FMlY3IiJT#F  R}hF]NFI5M#Y3cKQSIiGHY3IJKQgI#FM
c2Y3PVJLYZJT9FhJQgc2FhKGHY3cffeOQSI9KaJQSY3IYZ0JjnY2b/W9\_FPBjUQ_JTY3I#Ffi\SQ_JVF]b/K\`FKG/T7v#j	T9QSG/TbF]f9b/FPVFIJVFMK-NF]bX
KG]G]W#b/KaJVF!jnKX%JVYG]\gKPPQ_Z:Xy-Y3W#JYZ JT#F2x]2fmY3PPQ_e`\_F	G]\SKP/PVFP]BfiIkJT#F YJVFKI9M YJVF3x!M#Y3cKQSI9P]v
jnFK\SPVYYeOPVF]b/NFMGHY3I9PVJKI(JfOKaJVJVF]b/IOP]v0PVY3cF2YZBjUTOQSG/TKabF2jnF\S\ uI#Yj	Ip.qn\gK FF]J!K\w_vRxyy3oJVY
f9bYN{QgM#FhKNF]b/X%KG]G]W#bKaJVFG]\SKPPQ_^`G]KaJQ_Y3IkZYb"K-JQSI(XP/Q_]F dNFIZ:Yb YJVF3xojUT9F]bFoG]\SKPPQSG]K\mPVJWOM9Q_FP
YZ:JVFIbF]fmYbJF]b/bYb/P2YNF]bx~ %vKI9M1K\Sc2Y3PVJI#F]NF]bKabY3WOI9M x] p "	Y3\_JVFvxyy 3vjnFYeOPF]bNFM
Y3IcY3PVJ!YZJT#Fb/W9I9P!K}  GHY3I(JKQSI9QgI#[KIKG]G]W9b/KaJVFbW9\_FjUQSJTJjnY\gQ_JVF]b/K\SPoY3IO\_Xv0j	Q_JTjUT9QgG/T

-p:fnf9b/YNuQSM#FMkY3IlKNF]b/Ka[F-KIlF]bbYbUW9IOM#F]bx] %

 jnKPfiK\SPVY%GHY3c2fOKabFMJVY  +# Y3IKbFK\0jBYb\SMM#Y3cKQSIY3IijUT9QgG/TcQSI9QSI#[QgPPW#FPhKabF
KP%GHb/W9G]QSK\"KPG]\SKPPQS^OG]KaJQ_Y3IPVJVb/FI#[J9T 5Ka[b/QSG]W9\_JW9bF  IFHufmF]b/QSc2FI(JQSP2emFQSI#[G]Kab/b/Q_FMY3W9JQSI
 KabJQSI9QS{W#F-e(XJT#F2}fi}   p.}hF]fOKabJc2FI(JK\R}fiQ_bFGHJQSY3I5YZ  [b/QSG]W9\SJW#bFKI9M#YbFPVJv0JVYKG/TOQ_F]NF
emF]JVJVF]b-WOI9M#F]b/PVJKIOM9QSI#[kYZJT#FeFTOKNuQ_Yb!YZZ.Kab/c2F]b/Pv QSIfOKabJQSG]WO\SKabbF][3Kab/MOQSI#[lJT#FQSb!jUQS\g\SQSI#[3I#FP/P
JVY2GHY3IJVbKGHJ"K   p.OKab/cQSI#[0F]bb/Q_JVYb/QgK\  Y3I(JVb/KGHJ hPW9K\`Z.Kab/cQSI#[-GHY3IJVbKGHJPjUQ_JTFQ_JT#F]bLJT#F
PVJKaJVFp.#b/KIOGHFBYb BW#bYfmFUM9QgMI#YJLGHY3IJKQgIGHY3ccQ_Jc2FI(JPnZYbnJT#FhZ:Kabc2F]bLJVY-PKaJQgPVZX+I%K   "v
FKG/TZ.Kab/c2F]b2GHY3ccQ_JP-JVY5KMOKaf9J-KIOM YbG/T9KI9[FkTOQSPffKa[bQSG]W9\_JW#bK\dJVFG/TOI9QS{W#FP-Yb-fObY{MOW9GHJQ_Y3I9P]v
JVYFIOPW#bFP/W9PVJKQSI9Kae`\_FM#F]NF\_YfOcFIJffZYb-\_Y{G]K\BKa[b/QSG]W9\SJW#bF%IFHuGT9KI#[F%ZYb-JT9QSP]v T#FbFGHFQSNFP
JT#F[3W9Kab/KI(JVF]F2JVYYe9JKQSI^OIOKI9G]QSK\ T#F\_f5ZYbfiJT9QgPhGHY3I(JVb/KGHJ]v KI9MJVYeF-JVb/KQSI9FMiJVYI#F]j Ka[bQSG]W9\ fi
JW#b/K\ JVFGT9I9QS{W#FP]uWOG/TKM#Y3cKQgIQSPfiK[Y(YuM5JVFPVJemFM5JVYF]NaK\SW9KaJVFKcF]JT#Y{M5Y3IJT#F2eOKPQSPfiYZ
f9bFMOQSGHJKaeOQS\SQSJXKI9MQSI(JVF]bf9bF]JKaeOQg\SQ_JXv7emFG]KW9PFYZLJT#FfO\SKGHFYZLWOI9GHF]bJKQSI(JXQSIKa[b/QgG]W9\_JW#bFv KI9M
JT#FkZ.KGHJ2JT9KaJYe9JKQSI9QSI9[iM9KaJKG]KIemFkKTOKab/MKI9M\_Y3I#[JKP  JT#F}fi}   TOKPffJVYemFkKP2K
G fi
G]W#b/KaJVFKPfifmY3PP/Q_eO\_F-QSIiQSJPfif9bFM9QSGHJQSY3I9P	KIOMQgIJVF]bfObF]JKaJQ_Y3I9P]v7JVYlcKI9Ka[FKPfiemFPVJoKPfifY3P/PQ_eO\_F-Q_JP
bF\SKaJQSY3I9PT9Q_f`PjUQ_JTZ:Kabc2F]b/P]v#KI9M%QSIJT#FoG]KPVFfiYZ   BP]v(JVY2cK FfiJT#FfiemFPVJnf9bY3c2YJQSY3IG]Kc2f`KQ_[3I
Z:YbUJT#FPVFffI#F]j GHY3I(JVb/KGHJP]  [b/QSG]W9\SJW#bFoQSP"K\SPYNF]bXlPVFI9P/Q_JQ_NFoJVYK ffVPT#YjLG]KPVF-F *FGHJ  Lf9b/YNuQSM#FM
F]NFIlZ:F]j bF]fObFPVFI(JKaJQ_NFZ.Kab/c2F]b/PLjUQg\S\mT9KNF!P/W#eOPGHb/QSeFMJVYJT#FGHY3I(JVb/KGHJP]vGHY3c2fOKab/KaJQSNF\_XcKIX
YJT#F]b/PUKabF!\S#Q F\SXJVYZ:Y3\S\_Yj
+I5JT9QSPoPVJWOM#XvZ:bY3cJT#FM#FPGHbQ_f9JQ_Y3IYZ ~kN8KabQSKaeO\_FPfiZ:YbKaemY3W#J!|abF]f9b/FPVFIJKaJQSNFZ:Kabc2F]b/P
PKaJQSPZXuQSI#[JT#FGHb/Q_JVF]b/QgK5JVYKM9T#F]b/FJVYK   "vRJT#FiKQSc QSPJVYM#F]NF\_Yf1cY{M#F\gP2ZYb%JT#Y3PVFjUT#Y
KabFKGHJW9K\g\_XjUQg\S\SQSI#[JVYlKM9T#F]bFv7JT#Y3PFI#YJojUQS\S\gQSI#[JVYlKM9T9F]bFvKI9MJT#Y3PVFG]W9bbFI(J\_XiW9IOGHF]bJKQSI7
dKab/QSKae`\_FP2KabFM9KaJKY3IFKG/T1Ka[b/QSG]W9\_JW9b/K\nFH{fO\SY3Q_JKaJQ_Y3Ip.PQS]FvBJVF]bbKQSII9KaJW#bFvn^OI9KIOG]QSK\LM9KaJKuv
JX{fmFYZ f9bYuM9W9GHJQ_Y3Iv{F]JGvOKP"jnF\S\KP	c2YbFfmF]b/PY3I9K\7M9KaJKY3IlJT#FZ.Kab/c2F]b/P!p:FMOW9G]KaJQ_Y3I7v`Z:KcQg\_X
PVJKaJW9PvYe VFGHJQ_NFP]v(fF]bPVY3I9K\#KI9PVjnF]bdJVY!Ko{W#FPVJQSY3I9I9KQ_bFvF]JGaB"T9QSP bF]f9b/FPVFIJPRKPcK\S\#MOKaJKPVF]J
JVYcQSI#Fv{eOW#J]vuQSI(JVF]bFPVJQSI#[3\SXvJT#FhbFPW9\SJPRYe9JKQgI#FMjnF]bFfiM9Q *mF]bFI(JRjUT#FIf9bYuGHFPPQgI#[!Q_JnjUQ_JT  +# 
YH
b 
-p:f
:

$5O

fi

iwc

	
ffff

p'& &:- &Efi-  J&
$ ! V6p'& $8!	 &:-0
p'& &:- &Efi-  J&
$ ! V6p'= $5- ff
( J&3!]6p'& fi
0 
 &38'$58 $  
M#F]Z.KW9\_J ) 

KMOT#F]bF
fi/x
x
u ~



fi/x

fi/x
u&|

KM9T9F]bF
x
x


 Q_[3W#bF/&	"T#F!}  Ye9JKQSI#FMY3IiJT#F-Ka[b/QSG]W9\_JW9b/K\7M9KaJKip.PF]FffJVFHuJUZ:YbUJT#F-QSI(JVF]bf9bF]JKaJQ_Y3IYZJT#F
N8Kab/QgKaeO\_FP
FbKI5eYJTK\S[Yb/Q_JT9cPfiQSI5Kx] fiDZ:Y3\SMPVJVbKaJQ_^9FMGHbY3PP fiDN8K\SQgM9KaJQ_Y3IFH{fmF]b/Qgc2FIJ]1
-p:f
Ye9JKQSI9FMffKfi~)4& KNF]b/Ka[FF]b/bYb I|"Y3W9J0YZx]hb/WOI9P]v8JT#FnP/Kc2Fn}  jLKP QgI9M9W9GHFM7)J QSP0f9bFPVFI(JVFM
QSI- Q_[3W#bF {qnKP/QSG]K\S\_XvaJT9QSP }  f9bYNFP JT9KaJ f9bFM9QSGHJQgI#[UJT#F ff KM9T9F]bF2fiG]\SKPP QSP JT#FnFKPQ_FPVJ JKP v
Z:Y3\S\_YjnFMle{XJT#FfObFM9QSGHJQ_Y3IlYZJT#
F ffVKM9T9F]b2F G]\SKP/P]"T#F ff
> p.W9I9GHF]bJKQgIlZ.Kab/c2F]b/PQSP"f9b/FM9QSGHJVFM
Y3I9\_Xe(XJT9FfiM9F]Z:KW9\SJBNFGHJVYbn"T9QSPnPVF]FcPnb/KaJT9F]b"I9KaJW#b/K\  jUT9F]bFKPnJT#FfiFH{JVbFcFoeFTOKNuQ_Yb/PBJVFI9M
JVYemFG]\_FKab"JVYM#F]JVF]b/cQSI9Fv#JT#F!W9I9GHF]b/JKQSIJ)X%QgPJT#FT9Kab/M9FPVJJVYf9bFMOQSGHJ]
 +# p.M#F]Z.KW9\_JfffOKab/Kc2F]JVF]b/PoQSI9MOW9GHFMKi}h jUT9QgG/TjLKPK\Sc2Y3PVJffJT#FFH#KGHJJVb/KI9PGHbQ_f9JQ_Y3IYZ
b/W9\SF-xav9K-b/W9\SF	jUTOQSG/T%PKXuPJT9KaJLZ.Kab/c2F]b/PLj	Q_JTkI#YffFMOW9G]KaJQ_Y3Ip:jUQ_JT9Y3W#JnKI(XkKa[b/QSG]W9\_JW9b/K\`MOQ_fO\_Y3cK
YbkJVb/KQSI#F]FP/T9Q_fOPKI9MI#YY3I#[Y3QSI#[f9bY VFGHJKabFI#YJj	QS\S\SQSI9[JVYKMOT#F]bF "T9QSPb/W9\SFiQSP%c2Y3PVJ\_X
QSI(JVF]bFPVJQSI9[%emFG]KW9PFQ_Jfif9bYNFPJT9KaJoFM9W9G]KaJQ_Y3IQgPoKPVJVbY3I#[Z.KGHJVYb!M#F]JVF]b/cQSI9QSI#[%JT#F ff KMOT#F]b2F 
KI9PVjnF]b"T#Fff}hUPUQSI9MOW9GHFMlK\SPVYGHY3IJKQSI9FMY3I#FffYbUJ)jBYkc2YbF!\gQ_JVF]b/K\SP"PF]fOKab/KaJQSI#[JT#F ffVKM9T9F]b2F 
KI9M ff
21G]\SKPPVFPp.KNF]bKa[FF]bbY2b ")| 4 vmeOW9JUY3I9\_XkZ:F]j YJT9F]bhJTOQSI#[3PUGHY3WO\SMlemFffcQSI#FMlZ:bY3c JT#F
JVbF]FPUYZ  +# {vQSIkJT#F\SQS[3TJYZ JT#Fof9b/YeO\_Fc KM9M#b/FPPVFM7
E	W9\_F~QgI1 Q_[3W#b/F M9QSMI#YJT9KNFJT#F5F(W9QSN8K\_FI(JkQSI JT9F5}fi"PkQSIOM9W9GHFM7 T9KaJQ_J%P/KXuP
QSP-QSI(JVF]bFPVJQSI#[iZYb-JT#Fk}fi}  nvemFG]KW9PVF%Q_J!eOb/QSI#[3P!JT#FZ:Y3\S\_YjUQSI#[iGHY3I9G]\SW9PQSY39I Z:Kab/cF]b/P!j	Q_JT#Y3W#J
Y3I#[Y3QSI9[2f9bY VFGHJP]vOKI9MI#YJ"PF\S\SQSI#[-JT#FQ_bfObY{MOW9GHJPnY3I9\_XJVYKjUT#Y3\_FPK\SF]bv#KabFY3IkJT#,
F uI9Q_Z:FfiFM#[F
Z:Yb2JT#FQ_b2c2FcffeF]bPT9Q_fp:FQ_JT#F]bQSI ffVKM9T#F]b/2F uvRYbQSI ff KM9T#F]b/2F  Q_JT#Y3W#J-[Y3QSI#[5Z.W#bJT#F]b2QSI(JVY
\_YuG]K\`Ka[bQSG]W9\_JW#bK\OGHY3I9PQgM#F]b/KaJQ_Y3I9PvJT9QSPBb/W9\SFvZ:YbnJT9Fh}fi}   RI#[3QSI9F]F]b/P]vb/F]f9bFPVFI(JPLKIKG]G]W#bKaJVF
NuQ_F]j YZ JT#FZ.Kab/c2F]b/P"KGHJWOK\S\_XkGHY3IJVbY3\g\SQSI#[JT#FQ_bFHufO\_Y3QSJKaJQ_Y3IlGHY3PVJP]vOeFQgI#[FQ_JT#F]b"Z:Yb"Yb	Ka[3KQgI9PVJ
  BP]v#KI9MJT9KaJFMOW9G]KaJQ_Y3IfOWOPT#FPnJVYjLKab/M9PUJT#Fc2FcffeF]bPT9Q_fp.GHY3c!e`QSI9KaJQ_Y3IYZ b/W9\SFPhx!KI9Ml~3v
f9bYe`KaeO\_XemFG]KW9PFQ_J"K\S\_YjUPJT#Fc JVYPVF]FJT#FZ.W#JW#b/FofYJVFI(JQSK\7emFI#F]^OJPLYZ JT9FGHY3IJVb/KGHJ]vemF]JVJVF]b
JT9KIlQSJP"G]W#bbFI(J"GHY3I9PVJVbKQSIJP

 

'.d"m "#?;'#"($

|c

p.qLKW#F]b r s Y3T9KNuQwvoxyyy{z!hfOQ_JV r  KG]\SQSI7v
x yyy3v3F]NFIGHY3I9PQgM#F]bFMkp.qnKW9F]b r1s Y3T9KN{QDvuxyyy30KP Q_JP0fmYJVFIJQSK\{cKQSI!f9bYeO\SFc d{fmF]b/QScFIJK\
PVJW9MOQ_FPUPT9Yj JT9KaJfiPW#eOPVJKI(JQSK\0I#Y3QSPVF-\_F]NF\SPhG]KIK\SJVF]b	JT#F-NYJVFJVYJT9FfffmY3QSI(JUJT9KaJfiQ_JPhKG]G]W#b/KGHX
QSP\_YjnF]bJTOKI JT9KaJ%YZKP/QSI#[3\_FlYZQSJPG]\SKP/PQ_^9F]bipDfifOQ_JV r  KG]\SQSI7voxyyy3 hfOQ_JV r  KG]\SQSI
p+xyyy3fifmY3QSI(JhY3W#JfiJT#FbF]jBFQS[3TJQSI9[kP/G/T#Fc2F2YZBJT#F2FH#Kc2fO\_FPfiQSIemY(Y3PJQSI#[kKPfieFQgI#[%K%fmYJVFI(JQSK\
bFKPVY3IZYbBJT9QSPemFT9KN{QSYb"T#Y3W#[3T2jnFUM#YffI#YJBW9PF"KIX2bF]jnFQ_[3T(JQSI#[!P/G/T#Fc2Fv{jnFUT9KNFfiG/T9Y3PVFIZ:Yb
JT#F-PK F!YZdGHY3c2f`\_F]JVFI#FPP	JVYKM9M9bFPP"JT9F!emFT9KNuQ_Yb"Y<
Z -p:f"Ka[3KQSI9PVJhI#Y3QSPVFv`KI9MGHY3c2f`KabF
Q_JPb/FPW9\_JP%jUQ_JTfmF]b/TOKafOPkJT#FcK VYbQSI9MOW9GHJQ_Y3IK\_[Yb/Q_JTOc jUQ_JTjUT9QgG/TjnFPT9KabFJT9F ff+JVYf fi
M#YjUIKI9M5f9b/WOI#2F kQSI9M9W9GHJQSY3IiPGT#Fc2F   +# fip ffoW9QSI9\SKIv xyy +(UT9QSPfiPVJW9M#XbF\SQ_FPfiY3I5JT#F fi}o|
M#Y3cKQgI7vnQgIjUT9QSGTjnFbF]fO\gKGHFJT#FlYbQ_[3QSI9K\x] G]\SKPPI#Y3QSPVFp.qnWOIJQSI9F r hQ_eO\_F]JVJ]voxyy~3e(X
NaKab/Q_Y3W9PQSIOGHbFKPQSI#[KcY3W9IJPdYZmG]\gKPPdI#Y3QSPFb/KI#[3QSI#[oZ:bY3c  JVY +3 e{X2PVJVF]fOPYZ~ %v3YbBN8KabQ_Y3W9P
	Y3QSPVFiTOKI9M9\SQSI9[5QgPKGHb/WOG]QSK\UQSP/PW#FZ:YbemY{Y3PVJQSI#[

:

$$

fi

0.5

0.35

WIDC
C4.5
Bayes

0.45
0.4

0.25

0.35
0.3

error (%)

error (%)

WIDC
C4.5
Bayes

0.3

0.25
0.2
0.15

0.2
0.15
0.1

0.1
0.05

0.05
0

0
0

0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4

0

0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4

class noise (%)
100

attribute noise (%)
110

WIDC
C4.5
Bayes (DC)
Bayes (DT)

90
80

90
80

70

70
size

60
size

WIDC
C4.5
Bayes (DC)
Bayes (DT)

100

50
40

60
50
40

30

30

20

20

10

10

0

0
0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0

class noise (%)

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

attribute noise (%)

 Q_[3W#bF,+( R\_YJPkYZffJT#F5F]bbYb/Pp.W#fmKIOMP/Q_]FPp.M#Yj	I`YZ 
-p:fv  +# KI9MqLKXFPb/W9\_F
Ka[3KQSI9PVJ"NaKab/Q_Y3WOPG]\SKPP"I#Y3QgPVF\_F]NF\SP!p.\SF]ZJLKIOMKaJVJVb/Q_eOW#JVFffI#Y3QSPVFo\_F]NF\SPffp:b/Q_[3T(J
SQ I9GHb/FKPQSI#["Kc2Y3WOIJPYZ9KaJVJVb/Q_eOW#JVFBI#Y3QSPVFdQSIJT#FBPKcFdb/KI#[F "T#F fi}o|"M#Y3cKQSI!T9KPJT#FRKM#NaKI(JKa[F
JT9KaJ2JT#FkJKab[F]JGHY3IOGHF]f9JQSP	{I9YjUI7vBKI9MQ_JT9KPemF]FIKMOM#bFPPVFMQSIKPW9eOPVJKI(JQSK\BKcY3W9IJ2YZ
f9bF]NuQ_Y3W9PhFHufmF]b/QSc2FI(JK\ jBY
b {P] FT9KNFPQSc-W9\SKaJVFMGHYb/bFPVfmY3I9M9QSI9[%M9KaJKPF]JPoY.
Z ux~FHuKc2f`\_FP
FKG/Tv#ZYbnFKG/TI#Y3QgPVFh\SF]NF\w RKG/TP/W9G/T%M9KaJKPVF]JLjLKPBfObY{GHFP/PVFM%e(
X 
-p:fBKI9M  +# {v#W9PQSI9[ffK
x] fiDZ:Y3\S!M fiGHbY3PP fiDN8K\SQgM9KaJQ_Y3If9b/Y{GHFM9W9bFR Q_[3W#b,
F +M#F]fOQSGHJPJT9FbFPW9\SJPYe9JKQSI#FMlZYb"JT9FF]bbYb/P	KI9M
Z:YbUJT#F!P/Q_]FP"YZ JT#F!G]\gKPPQ_^9F]bP]B"T9FPQ_]FYZK}  QSP"Q_JPj	T#Y3\_F!I{W9cffeF]bUYZ \gQ_JVF]b/K\SP]v9KIOMJT9KaJUYZ
K}fi QSPUQ_JPI(WOc!emF]bYZQSI(JVF]b/I9K\7I#YuM#FP]
T9QS\_F JT#FRbFPQSPVJKIOGHFdKa[3KQSI9PVJI#Y3QgPVFdPVF]FcPJVY"emF b/F\SKaJQ_NF\_XhjBF\g\M9QSPVJVb/QSeOW#JVFMhKc2Y3I#H
[ 
-p:f
KI9M  +# 'p 
-p:fUPVF]FcP	JVYfmF]bZ:Yb/c emF]JVJVF]bhZYbhG]\SKPPhI#Y3QSPVFv9j	T9QS\_F  +# PVF]FcP	JVYfmF]bZ:Yb/c
emF]JVJVF]b"Z:Yb"KaJVJVb/Q_eOW#JVFoI#Y3QgPVFv9KfOT#FI#Y3cFI#Y3Ic2YbFQSI(JVF]bFPVJQgI#[2GHY3c2FPZ:bY3c JT#FPQ_]FPLYZ JT#FoZ:Yb fi
c-W9\SKPQSI9MOW9GHFM7  QSb/PVJ]vuJT#F}  P"T9KNFoNF]bXkPcK\S\mPQ_],
F '`W9GHJW9KaJQ_Y3I9PLGHY3c2f`KabFMJVY2JT#F}fi"P Z:Yb
G]\SKPP!I#Y3QSPVFPo[b/FKaJVF]bffJT9KI~a %v0JT9F}fi"PT9KNFPQ_]FQgI9GHbFKPQSI9[%e{XKZ.KGHJVYb!YZoxa  fi)~{uFGHY3I9M7v
I#YJVFLJT9KaJ JT#Fnb/KaJQ_YheF]J)jBF]FIJT9FnI{W9cffeF]b YZOI#YuM#FP0YZ9JT#FnJKab[F]Jd}fiovaKI9M-JT#FnI{W9cffemF]bYZ`\gQ_JVF]b/K\SP

$

: C

fi

iwc

	
ffff

YZnJT#FJKab[F]J!}  QgP{#YbffKcK +Yb/QSJXYZLG]\gKPPfiYb!KaJVJVb/Q_e`W#JVFI#Y3QSPVF\SF]NF\SP]vJT#F2bKaJQ_YemF]JjnF]FI
JT#Fo}fi"PLeOW9Qg\SMKI9M%JT#Fo}  PLeOW9Qg\_JBQSP 
 {v#jUQSJTKfOKaJT9Y3\_Y[3QSGhG]KPVFoZYbx] KaJVJVbQ_eOW#JVFfiI#Y3QSPVFvuZ:Yb
jUT9QgG/TJT#Fffb/KaJQ_YQgP 
 |{"T9FPVFbFcKab
{P]v`K\_Y3I#[j	Q_JTJT#FZ.KGHJhJT9KaJ"JT#F-}  P"eOW9Qg\_JT9KNFKNF]bX
bFKPVY3IOKaeO\_FPQ_]FLjUT#FIGHY3c2f`KabFM2JVYoJT9KaJ YZJT#FLJKab[F]JR}  ZYbdKI(X-JX{fFKI9M2\SF]NF\uYZ`I#Y3QSPFvJVFI9M
JVYiP/T#Yj Kl[Y{Y{MI#Y3QSPVFT9KI9M9\SQgI#[kZY1
b p:f  f`KabJZ:bY3c JT#FPFGHY3I9PQgM#F]b/KaJQ_Y3I9Pv0[3\SQgc2fOPVFP
KaJoJT#F2}  PfiY3W#JVfOW9Jhe{
X -p:fhP/T#Yj JT9KaJfiF]NFIZ:Yb\SKab[F2I#Y3QgPVF2\_F]NF\SP]vQ_JocKIOKa[FPoJVYk^OI9M
GHY3I9GHF]f9JPhPVXuIJKGHJQgG]K\S\_XG]\_Y3PVFJVYJT#FJKab[F]Jfi}  O#YbhFH#Kc2fO\_Fv9Y3I9FYZJT#F!}  PUY3W#JVfOW#JUKaff
J a
G]\SKPPffI#Y3QSPVFQSPFH#KGHJ\_XJT#FJKab[F]J-}  z K\SPVY#v Q_J!QSP!Y3I9\_X5Z:YbffG]\SKPPffI#Y3QSPV=
F  x~ p.KI9MKaJVJVb/QSeOW#JVF
I#Y3QSP
F  x| nJT9KaJ	PVY3c2F!}  PZ:Y3W9I9MM9YI#YJUPVXuIJKGHJQgG]K\S\_XkQSI9G]\SW9M9F	JT#F!JKab[F]J	}  KI(X{cYbF
< ? >


AaCD? >

E FGHFI(JLKM#N8KIOGHFPRQSI2JT9FUPVJW9M#X-YZ7NYJQSI#[ffG]\SKPPQ_^`G]KaJQ_Y3IK\_[Yb/QSJT9cPdT9KNF	eObY3W#[3T(JdFc2fOQ_bQSG]K\#KI9M
"
JT#F]YbF]JQgG]K\LbFPW9\_JPG]\_FKab\_XPT#YjUQSI#[JT#FM9QSP/GHb/QScQSIOKaJQ_Y3IfmYjnF]b2YZUFI9PFc!eO\SF%G]\SKP/PQ_^9F]b/P"T9QSP
fOKafmF]bKM9M#bFP/PVFPnZbY3c K-JT#F]YbF]JQgG]K\7KI9M%Fc2fOQ_b/QgG]K\9fmY3QSI(JBYZNuQ_F]jJT#Fo{W#FPVJQ_Y3I%YZj	T#F]JT#F]bLY3I#F
cQ_[3T(JT9KNFJVYM9QSPVfmFI9PVFkjUQSJTQSIJVF]b/f9bF]JKaeOQS\gQ_JXQSIYb/M#F]bJVY F]F]fG]\SKPPQ_^`G]KaJQ_Y3IPVJVbFI9[JT71+I
Yb/M#F]bnJVYGHYfFhjUQ_JTJT9QSPRf9bYeO\_FclvjnFhT9KNFhGT#Y3PVFI%JVYPVJW9M#XK-G]\SKPPBYZGHY3I9GHF]f9JnbF]f9b/FPVFIJKaJQSY3I9P
bFPVFcffeO\SQgI#[c-W9\_JQS\gQSI#FKabUM9QgPGHb/QScQgI9KIJUfY3\SX{I#Y3cQSK\SP]vOKM9F(W9KaJVF2Z:YbocQSI9QgI#[QSPP/W#FPUj	T#FIiM9FK\SQSI#[
jUQ_JT5NYJQgI#[lf9bYuGHFM9W#b/FP]v7jUT9QSGT5jBFM#F]^OI#F2KP}hFG]QSPQSY3I  Y3ccQSJVJVF]FP]%fiW#boJT9F]YbF]JQSG]K\bFP/W9\_JP
PT#Yj1JTOKaJnPVJVbQ_N{QgI#[oZYbLPQScfO\SQSG]Q_J)X-QgP]v(\SQ FUZ:YbncKI(X2YJT#F]bG]\SKPPFPRYZGHY3I9GHF]fOJBbF]fObFPVFI(JKaJQ_Y3I9P]v#K
T9Kab/MffGHY3c2fOW#JKaJQSY3I9K\3f9b/YeO\_FctjUT9FI!M#FK\SQgI#[jUQ_JT}  Yb0YJT9F]b GHY3c2f`\_FHoNYJQSI#[UfObY{GHFMOW#bFP]vaKI9M
f9bYNFPhJT#F-T9FW#b/QSPVJQgGoI9KaJW#bFffYZdYJT#F]bhbFPW9\_JPUJVbXuQSI#[JVY%f9b/WOI#FKM9Kaf9JQ_NF-emY(Y3PJQSI#[#""T9QgP"fOKafmF]b
f9bYfmY3PVFP!JVYKMOKaf9J-Kif9bF]NuQ_Y3W9PffPG/T9Fc2FJVYeOW9Qg\SM5jBFK \_FKab/I#F]b/Pv P/W9G]GHFPPVZ.W9\dZ:Yb-JT#FQSIOM9W9GHJQ_Y3I
YZ"M#FG]QSP/Q_Y3IJVbF]FP-KI9MM#FG]QSPQSY3I\SQSPVJPv0JVYiJT#F%G]KPFYZ"}   "T9QSPQgP!KIYb/Q_[3QSI9K\RKaf9f9bY3KGTQSZBjnF
bF]Z:F]bJVYJT#FoPVJKaJVF fiDYZ fiDJT#F fiKabJfiK\_[Yb/Q_JTOcPLeOW9QS\gM9QSI#[GHY3c2f`\_FH%NYJVFPUf9bYuGHFM9W#bFPv{W9P/W9K\S\_XjnYb uQSI#[
QSIiJT#FPVJVbY3I#[k\_FKab/I9QgI#[Z:b/Kc2F]jnYb `fiW#bfiK\_[Yb/QSJT9cv !vbF\SQSFP"Y3IbFGHFI(JPfiYbfiI9F]j bFP/W9\_JP
KaemY3W#J fOKabJQ_JQSY3I!emY{Y3PVJQSI#[#vb/KI {QSI9[	\_Y3PP emY{Y3PVJQSI#[#v3KI9M-f9bW9I9QSI#[#)JGHY3c2FPj	Q_JT!J)jB,
Y '`KNYb/P]v(Y3I#F
jUQ_JT5YfOJQScQSPVJQgGfffOb/W9I9QSI9[%KI9M5Y3I9FjUQ_JT5fmFPPQgcQSPVJQSGfff9b/WOI9QSI#[#qnYJTYe9JKQSI9FMFH{fmF]b/Qgc2FIJK\g\_X
[Y{Y{M1bFPW9\SJPY3IJT9FPQgc2fO\SQSG]QSJX fiKG]G]W#b/KGHXJVb/KM#F]Y * v"eOW#Jj	T#F]bFKPYf9JQScQSPJQSGkf9b/W9IOQSI#[G]\_FKab/\SX
Y3W#JVfmF]bZ:Yb/cP	YJT#F]bK\_[Yb/QSJT9cPhQSIJT9F\gQ_[3TJ	YZRJT#FPQ_]F-YZRJT9F-Z:Yb/c-W9\SKP	Ye9JKQSI#FMvfmFPP/QScQSPVJQgG
f9b/WOI9QSI#[hJVFI9M9P JVY!KGT9Q_F]NFhKc2YbFbFKPY3I9KaeO\_FJVb/KM9F]Y * v{jUQ_JT2T9QS[3T2KG]G]W#b/KG]Q_FPRYe9JKQSI#FMY3IP/cK\S\
Z:Yb/cffWO\SKP]L"T9QSP"QgP"K\S\7JT#F-c2YbF-QSIJVF]b/FPVJQSI#[KP	fmFPPQScQgPVJQSGhf9b/W9IOQSI#[QSP"eOKPFMlY3IKI9KaJW#b/K\0KI9M
PQScfO\_Fhf9b/W9I9QgI#[fffObY{GHFMOW#bF

 <



 > ?    @ 	  O>



"T9KI {PhKabFM9W#F!JVYk}fi}    KabJQSIOQS(W9Fv	8 n  } p dJKaeO\SQgPPVFc2FI(JhKaJQ_Y3I9K\ M9 RIOPVFQ_[3I#Fc2FI(J
uW#
f F] bQ_FW#b  [bY3I9Y3cQS{W#FhM9Fh}fiQ VY3I`RKI9M  QSPVF  FKI!fi  Y3W9QSPnZ:YbLTOKNuQSI#[-f9bYNuQSM#FMJT#FoKa[b/QSG]WO\_JW#b/K\
M9KaJKuv0ZYb!PJQScffWO\SKaJQSI#[kM9QSPG]WOPPQ_Y3I9PhKabY3W9IOM5Y3W#bbFPWO\_JP]vKI9M5Z:Yb!T9KN{QSI9[kKW9JT#Yb/Q_]FMJT9F2fOW#eO\SQfi
G]KaJQ_Y3IYZ PVY3c2F!YZ0JT#Fob/FPW9\_JPLYe9JKQgI#FM7"T9K(I uPLJVY  T9c2FM  QSI#Y3W9GT#FoZ:Yb"T9KNuQSI#[2fmY3QSI(JVFMkY3W#J
JT#FUQgIJVF]bFPJBQSI2cQgI9QScQ_QgI#[hP/W#eOc2YuM9W9\SKab Z.W9I9GHJQ_Y3IOP]  QSIOK\S\_XvJT#FhKW#JT#YbdjUQgPT#FPJVYffJT9K(I   FM#bY
}hY3cQSI#[Y3PUKI9MJT#Fb/F]N{Q_F]jnF]b/PZ:Yb"JT#FQSbN8K\SWOKaeO\_FoPW#[[FPVJQSY3I9P]

:

$


fi



 `>

@RC 

k36)N6)



" 5"Z> 

 c

uQSIOGHF	JT#FoT9KabM9I#FPPBbFPW9\SJPBYZ "T#F]YbFcPhxfiKI9M-KabFoPVJKaJVFMZ:YbJT#FfiJjnYfiG]\SKPPFP"G]KPVFv#jnFPT9K\S\
9W PVFfiJT#FoI#YJKaJQSY3I ,  1 %   ,  1 - x / 7   ,  1 - /mZ:Yb"PVY3c2FoKabe`Q_JVb/KabXbW9\_F-p ,  1 (   ,  1 v#jUT9F]bF   ,  1 - /QSPnJT#F
NaK\SW#FZ:Yb-G]\gKPPff fi KI9M   ,  1 - x /QSPoJT#F%N8K\gW#FZ:YbffG]\SKPP ff  u  fmY3PQSJQ_NFNaK\SW#FZ:Yb ,  1 cFKI9P
JT9Ka(
J  ,  1 QSPfiQSIZ.KNYbYZnG]\SKPP)ff  kjUT9F]bFKPfiKI#F][3KaJQ_NF2NaK\SW#F-[3Q_NFPK  ,  1 QSIZ:KNYbYZBG]\gKPP)ff fi u
dK\SW#FhZ:Y
b ,  1 [3Q_NFPB5
K  ,  1 I9FW#JVb/K\9j	Q_JT2bFPVfmFGHJBJVYffJT#FhG]\SKPPVFP FhW9PFUK!b/FM9W9GHJQ_Y3I2Z:bY3c JT#F
 fi "	KabMf9bYeO\SFc ff  QSIOQScffWOc  YNF]
b ip  KabF]X r  Y3T9I9PVY3I7v7xy3ay3< 

6> "  ff  QSI9QSc-W9c  YNF]b
u
fi " . 7r'"("   GHY3\S\_FGHJQ_Y3IYZ0PW9eOPVF]JPBYZK!^`I9Q_JVF	PF]J>n  fY3P/Q_JQ_NF	QgIJVF][F]bv
fi
	 3 " .d7 '" R}hY(FPff GHY3I(JKQSIKGHYNF]b	YZPQS]FKaJUc2Y3PVJfiv#JT9KaJhQSP]v#KPW9eOPVF]J 
G  G  v9PW9G/TlJT9KaJUKI(X%F\SFc2FIJUYZ >emF\_Y3I9[3PJVYKaJU\_FKPJ"Y3I#Fc2FcffemF]b"YZ  
fi

|





GG
jUQ_JT

"T9Fhb/FM9W9GHJQ_Y3IQSPLGHY3I9PJVb/W9GHJVFMlKPZ:Y3\S\_YjUP Zb/Y3c K ff  QSIOQScffWOc  YNF]b
QgI9PVJKI9GHFfijnFoeOW9QS\SM%K
\_FKab/IOQSI#[ffP/Kc2fO\_F.=?>P/W9G/T%JT9KaJ"Q_ZJT#F]bFfiFHuQgPVJPKGHYNF]bUYZ PQ_]FG  G   YZ >Lv{JT9FIkJT#F]bFfiFHuQgPVJP
KM#FG]QgPQ_Y3IGHY3ccQSJVJVF]FjUQ_JT G  Gu\gQ_JVF]b/K\SPLGHY3I9PQgPVJVFIJUjUQ_JT =?>Lv9KI9M7v#bFG]Q_fObY{G]K\g\_Xv#Q_Z0JT9F]bFoFHuQgPVJP
KM9FG]QSPQ_Y3IGHY3ccQ_JVJVF]Fffj	Q_JT \SQSJVF]b/K\SP"GHY3I9P/QSPVJVFI(J"jUQ_JT =A>nvOJT9FIJT#F]bFffFHuQSPJPUKGHYNF]bhYZdP/Q_]F 
YH
Z >n "UFIOGHFv ^`I9M9QSI#[kJT#FPcK\S\_FPJM#FG]QSPQ_Y3IGHY3ccQ_JVJVF]FkGHY3I9PQSPJVFIJj	Q_J
T =?> QSPoF{W9Q_NaK\_FI(JJVY
^OI9MOQSI#[JT9FP/cK\S\_FPVff
J  ZYbojUTOQSG/TJT#F]bFFH#QSPVJPoKPVY3\gW#JQ_Y3IiJVY ff  QSIOQScffWOc  YNF]
b uvKIOM5JT9QSPfiQSP
QSI(JVb/KGHJKaeO\_FQSZ  % 0 2
 F]J !"M9FI#YJVF5JT#F 
  F\_Fc2FI(JkY
Z 2v	KI9M  "JT#F 
  F\_Fc2FI(JkY1
Z >n FM#F]^OI#F5KPVF]J
Y
Z GG"qnY(Y3\_FKI NaKab/QSKaeO\_FPiQSI Y3I9FJVYY3I#FGHYbb/FPVfmY3I9M#FI9GHFj	Q_JT JT#FF\_Fc2FI(JPY
Z 2vjUT9QgG/T
jnFWOPVFJVYtM#FP/GHb/Q_emFJT#FFH#Kc2fO\_FPiY#
Z =A>n "T#FGHYbbFPVfmY3I9M9QgI#[ PVF]J5YZ\SQSJVF]b/K\SPQSPiM9FI#YJVFM
0 ( (  ( ( . (  . (5464646(   (   M93R"T9FfiP/Kc2fO\_F =?>GHY3I(JKQSI9P"JjnYM9QSP VY3QSI(JnPW9eOPVF]JP JT#FPVF]JYZ fmY3PQ fi
JQ_NFhFHuKcfO\_F?
P =?>  v#KI9MJT#FhPF]JBYZI9F][3KaJQ_NFfiY3I#FA
P =?>  <
 =?>  GHY3IJKQgI9@
P GL> GFH#Kc2fO\SFP]v{M9FI#YJVFM



e{
X $ ( (P$ . (5464646(P$  4   FGHY3I9PVJVb/W9GHJoFKGTfmY3PQ_JQ_NF-FH#Kc2fO\_FPVYkJT9KaJQ_JhFI9GHYuM#FPfiJT#F2c2FcffeF]bPT9Q_f
YZ JT#FGHYbbFPfY3IOM9QSI#[-F\_Fc2FI(J"YZ >QSIkJT#F!F\_Fc2FI(JPY
Z   YbFf9bFG]QgPVF\_Xv


x





GL> GM(P$ 
%



 "#"$ 6
 "#"$ 4
" !7 ' :
"% !'&7 ' :

=A>  GHY3IJKQSIOPUKPQgI#[3\_FoI#F][3KaJQ_NFFH#Kc2fO\SFvOM#F]^OI#FMke{X
"   
$ %
" 4
" (

fi uW#f9fmY3PVF	JT#F]bFhFHuQgPVJPKGHYNF]b(  Y Z >PKaJQSPZXuQSI#[ G  G

pwy3



p+x]



FGHbFKaJVFKM#FG]QSPQSY3IGHY3ccQ_JVJVF]F
GHY3I9PQgPVJQSI#[ffYZ
cY3I#Y3cQSK\SPvFKG/TljUQ_JTY3I#Fo\SQ_JVF]b/K\OY3I9\_XKI9MkKPPY{G]QSKaJVFM%JVY2K-fmY3PQ_JQ_NF k R
 KGT
c2Y3I#Y3cQSK\dGHYuM#FP!Y3I9FYZJT#FPF]JP!QSI
"T9FM#F]Z.KW9\_J!G]\SKP/P!QSP ff fi ulUT9QSPM#FG]QgPQ_Y3IGHY3ccQ_JVJVF]F



* 



)

QSP	GHY3I9PQSPVJVFI(JUj	Q_JTlJT#FffFHuKcfO\_FP"YZ<=A>+ =A> vOYJT#F]b/jUQSPVF!PY3c2FffF\SFc2FIJ	YZ >jnY3W9\SMI#YJ	emF
GHYNF]b/FM7dZJT9F]bF	KabF	Y3I9\_XJjnY-N8K\SW9FPRKW#JT#YbQ_]FMZ:YbRJT#F	NFGHJVYb/PLKI9MJT9F]XKab/F  uv{jBFhPQSc2f`\_X
GHbFKaJVFK}  GHY3I9PQSPVJQgI#[YZnY3I#Fc2Y3I9Y3cQSK\ jUQ_JTI9F][3KaJQ_NF\SQ_JVF]bK\SPfiKPPVYuG]QSKaJVFMJVYKI#F][3KaJQSNF

$

: 2=

fi

iwc

	
ffff

c2Y3I9Y3cQSK\> 









 /





























h




 Q_[3W#bF &n"T#FoPQ kfmY3PPQ_eO\SFhG]KPVFPUYZ b/W9\_FP]
p:JT#F2NaK\SW#F-ZYboJT9FI#F][3KaJQ_NFG]\SKPPoQgP	[b/FKaJVF]bJT9KI5JT#F2Y3I#FYZBJT#F2fmY3PQ_JQ_NFG]\SKPPzFKG/TYZnJT#F
I#F][3KaJQ_NF-\SQ_JVF]b/K\gPnGHYuM#FPUY3I#FoYZ JT#F!PVF]JP"QgI   R"T#FM#F]Z.KW9\_JG]\SKP/P"QSP ff  u
fi uW#f9fmY3PVF!I9Yj JT9KaJhJT#F]bF-FHuQSPJPhK%M#FG]QSP/Q_Y3IGHY3ccQ_JVJVF]F jUQ_JTKaJfic2Y3PVJ \SQSJVF]b/K\SPhGHY3I9PQSPVJVFI(J
jUQ_JT =A>nd}hFI#YJV5
F  ( ( . (5464646(#  uFKG/Tic2Y3I#Y3cQSK\YZ v#QSIkI#YPVfmFG]Q_^OG	Yb/M#F]bv#KI9
M ( ( . (5464646(   
JT#FQ_bhKPPVYuG]QSKaJVFMN8K\gW#FPUZ:Y)
b kU"T#F-c2Y3I#Y3cQgK\SP"YZ G]KIiemF\_Y3I9[JVY%JT#bF]F!J)X(fmFPhYZdPW#e`PVF]JPUYZ
c2Y3I#Y3cQSK\SP







fi cY3I#YJVY3I#Y3W9P	c2Y3I#Y3cQSK\gPp:jUQ_JT#Y3W#J"I9F][3KaJQ_NF!\SQ_JVF]bK\SPv
fi cY3I#Y3cQSK\SPGHY3I(JKQSI9QgI#[2Y3I9\_XkI#F][3KaJQ_NF!\gQ_JVF]b/K\SP]v
fi cY3I#Y3cQSK\SPGHY3I(JKQSI9QgI#[2fmY3PQ_JQ_NFKI9MI9F][3KaJQ_NFff\SQSJVF]b/K\SP]
 F]JW9PoG]K\S\ bFPVfmFGHJQ_NF\_X v 
 v  JT9FPVFJT#bF]FG]\SKPPFP]

_Q NFI5JT9KaJoFKG/Tc2Y3I#Y3cQgK\0YZ
G]KI5eF2KPPY{G]QSKaJVFM5JVYlK%fmY3PQ_JQSNFffYbKI#F][3KaJQSNF kv7JT#F]bFFH#QSPVJPfiY3IJT#F2jUT#Y3\SF-PQ_iG]\SKPPVFPfiYZ
b/W9\SFP]vuf9bFPVFI(JVFMQSI Q_[3W9bF {
 IXc2Y3I#Y3cQgK\	YZ GHY3I(JKQSI9QgI#[KaJk\SFKPVJY3I#F5fmY3PQ_JQ_NF5\SQ_JVF]b/K\hG]KItY3I9\SXemFPKaJQgPV^9FMe(X
fmY3PQ_JQ_NFLFH#Kc2fO\_FP UT#F]bF]Z:YbFv(Q_Z`JT#F]bF"FH#QSPVJPdb/W9\_FP emF\_Y3I#[3QgI#[fiJVY!G]\SKPP/0Yb hv3jBF	G]KI2bFc2YNF
JT#Fc jUQSJT#Y3W#J \_Y3PQSI#[fiGHY3I9PQgPVJVFI9GHX9W#bJT9F]b/c2YbFvP/QSI9GHF $  GHY3IJKQgI9P Y3I9\_X-I#F][3KaJQ_NF"\SQ_JVF]bK\SP]vQ_Z9jnF
bFc2YNFffJT9FQ_b	I#F][3KaJQSNFff\SQ_JVF]bK\SPZ:bY3c K\S\b/W9\_FP"eF\SY3I#[3QSI#[2JVY%G]\SKPP  p.cK {QSI9[JT#Fc [YJVYG]\gKPP
v{jnFoM#Y2I#YJ\_Y3PVFfiGHY3I9PQSPJVFI9GHX  PK-GHY3IOPVF(W9FI9GHFv#jBFoG]KIPW9f9fmY3PVFUjUQSJT#Y3W#JL\_Y3PPnYZ[FI#F]bK\SQ_JX
JT9KaJUK\g\mb/W9\_FPLYZ KabF!QSIG]\gKPPv9vuYb" -
FI#Yj JVbFKaJoQSI9M#F]fmFI9M#FI(J\_X%JjnYlG]KPVFP]vM#F]fmFI9MOQSI#[Y3IjUT#F]JT9F]b	JT#F2M#F]Z.KW9\_JhG]\SKPPhYZ QSP
ff  2Yb ff fi u













xaUUT#FM#F]Z.KW9\_J-G]\SKPP-QSP ff fi u  IXfmY3PQ_JQSNFFH#Kc2fO\_F%PKaJQSPV^9FPffJT#F]bF]Z:YbF%Kic2Y3I9Y3cQSK\RQgI 
UT#F]bF2G]KIFH#QSPVJfiJjnYlJX{fmFPfiYZBfmY3PQSJQ_NFffFH#Kc2fO\SFP	JT9Y3PVFPKaJQSPVZ:XuQSI#[kKaJ\_FKPVJoY3I#F2b/WO\_FffYZ
G]\gKPPv9KI9MJT#Y3PVFI9YJUPKaJQSPVZ:XuQSI#[KIXG]\SKPP"Rb/W9\SF2p:JT9F]bF]ZYb/F!PKaJQSPVZ:XuQSI#[2KaJU\_FKPVJY3I9Fob/W9\_F
YZG]\SKP/P $  PKaJQSP^9FP"K\S\7G]\SKP/PnKI9M  b/WO\_FP]d"T#F]b/F]ZYbFv
,



 = 1 7

 ,

4

G]\gKPPG]\SKP/P 




E4

p+xx

1

UT9QSPRP/T#YjUPBJT9KaJ]vuQ_Z7K!fmY3PQSJQ_NF"FH#Kc2fO\_FhI#YJBPKaJQgPVZXuQSI#[ffKIXG]\gKPPRbW9\_FPjnY3W9\SMPKaJQSPVZ:XK\S\
G]\gKPPh  b/WO\_FP]vOJT9FIQSJUjnY3W9\SMlemF-cQSPG]\SKP/PQ_^9FM7vOjUT9QSGTiQSP	QSc2fmY3PPQSeO\_Foe{XJT#F-GHY3IOPQSPVJVFI9GHX
T(X{fYJT9FPQSP]RUT9QSPL[3Q_NFP	KIQSc2fmYbJKI(Jf9bYfmF]bJ)XvOI9Kc2F\SXJT9KaJ	KIXkfmY3PQ_JQ_NFoFH#Kc2fO\SF!I#YJ

$

: $

fi

P/KaJQSPVZ:X{QSI9[ffKI(XG]\SKPPndb/WO\_F	G]KI9I9YJLP/KaJQSPVZ:XK\S\`G]\gKPPn  bW9\_FP]  F]JLW9PLG]K\S\<k JT9QSPBf9bYfmF]bJ)X
QgIjUTOKaJ!Z:Y3\S\_Yj	P] FkI#Yj PT9Yj T#Yj JVY5eOW9QS\gMKNaK\SQSMPVY3\SW#JQ_Y3IJVY ff  QSIOQScffWOc  YNF]b<
j	Q_JTKaJUc2Y3PVJ kF\_Fc2FI(JP]R#YbhKIXkfmY3PQ_JQSNFhFH#Kc2fO\S	
F $  v



fi QSZ $  PKaJQSPV^9FPfiKaJo\_FKPVJfiY3I#F2G]\SKP/P	Ub/W9\_FvmG/T#Y{Y3PVFQSI

K PW#eOPVF]JhYZA>1GHYbb/FPVfmY3I9M9QSI#[
k
JVYK2fmY3PQ_JQ_NF\SQSJVF]b/K\YZ PY3c2F!PKaJQSP^9FMG]\SKPPnb/W9\SFd"T9QSPLPW#e`PVF]J"GHY3IJKQgI9P$  
fi QSZH$  M#Y(FPI#YJPKaJQSPZXKI(XG]\gKPP-b/W9\_Fv JT#F]b/FFH#QSPVJP-Z:bY3c k PY3c2FG]\SKPP-  b/W9\_F
j	T9QSG/TkQSPI9YJ"PKaJQSPV^9FM  c2Y3I#[K\g\I9F][3KaJQ_NF!\SQ_JVF]bK\SPnYZ0K2G]\SKPPL  b/W9\_FhjUT9QSGTkQSPI#YJ
P/KaJQSPV^9FMie(X$  vGT#Y(Y3PFffY3I#FffjUT9QSGTQSPUfmY3PQ_JQ_NF!Qg
I $  p.G]KW9PQSI#[%Q_J	I9YJUJVYkPKaJQSPVZ:XJT#F
bW9\_FvKI9MJT9FI2G/T#Y{Y3PVF"JT#F"GHYb/bFPVfmY3I9M9QSI9[	F\_FcFIJYZ "T9QgP P/W#eOPVF]J YZ >5GHY3IJKQgI9P
$  
JVF]b/KaJQSI9[%JT9FKaeYNF2f9bYuGHFM9W#bF-Z:YbK\S\fmY3PQSJQ_NFffFH#Kc2fO\SFP]v7jBF2YeOJKQSI5K%GHYNF]b!YZ>GHY3I!fi
P/QSPVJQSI#[YZKaJUc2Y3PJ PW#eOPF]JPYZ >L
~{UUT#FM#F]Z.KW9\_J"G]\SKPPUQSP ff  u?$  PKaJQSPV^9FPUK\S\7G]\SKPP/BKI9Ml  b/W9\_FPd"T#F]bF]Z:YbFv
,



 = 1 7

 ,

4

G]\gKPPG]\SKP/P 




E4

p+x~3

1

RNFI5Q_ZJT#F2QSI#F{W9K\SQ_J)XQSP	I9Yj

PVJVb/QSGHJ]v7Q_J	[3Q_NFPfiJT#FPKc2F-f9bYuGHFM9W#bFffZYbfiFG]QSFIJ\_XeOW9QS\SMfi
QgI#[JT#FkPVY3\SW9JQ_Y3IJVY ff  QSI9QSc-W9c  YNF]b
jUQSJTKaJc2Y3PVJ F\_Fc2FI(JP]ve(XW9PQSI#[JT#FPKc2F
Kab/[3W9c2FI(JUKPUQSIkJT9Fof9bFGHF]FM9QSI9[G]KPVF
"TOQSPLFI9M9PLJT#F!f9bY{YZYZ"T#F]YbFc xa

k36)N6)

" 5"Z> 

 c

FoW9PVFfiK-bFM9W9GHJQ_Y3IZ:bY3c

JT9F  fi "hKab/M%f9bYeO\SFc

ff~ fi 

 fi  Y3\SYb/KaeOQS\SQSJX kp s FKab/I9PnF]JK\w_v7xy3<

6> "  ff~ fi   fi  Y3\_Yb/KaeOQS\gQ_JX u
fi " . 7r'"("   ^OI9Q_JVF%PVF]J> % 0 ( ( . (5464646(  4  9kKIOMKGHY3\S\_FGHJQ_Y3IYZUGHY3I9PJVb/KQSI(JPffYNF]b#>nv
 % 0 ! ( (*! . (5464646(*!    93vOPW9GTJT9KaJ  ^m/0x3(/~)(5464646(RGG 9 (*!   >n
fi
	 3 " .d7 '" R}hY(FPJT9F]bFFH#QSPVJ"K~ fi   fi  Y3\SYb/KaJQ_Y3IlYZ JT#FoF\SFc2FIJP"YZ<>Lv J 1.21BKZ:WOI9GHJQ_Y3I
)
>  0 x3/( ~ 2
9 PW9GTJT9KaJ
fi

|

p  
m/0x3(/~)(5464646(RGG 98(p   ( 1 m! D  p   

%0

p 1 

"T9F"bFM9W9GHJQSY3IQSPRGHY3IOPVJVb/W9GHJVFM%KPdZ:Y3\S\_YjUP  Zb/Y3c K ff~ fi   fi  Y3\_Yb/KaeOQg\SQ_JX !QgI9PVJKI9GHFv(jBF	eOW9QS\SM
K \SFKab/I9QSI#[P/Kc2fO\_F =A>PWOG/TJTOKaJBQ_ZJT#F]bF	FHuQSPJPRKff~ fi   fi  Y3\_Yb/KaJQ_Y3IYZ7JT#F"F\SFc2FIJPBYZ >nv(JT#FI
!
JT#F]bFFHuQgPVJP!KiM#FG]QSPQ_Y3IGHY3ccQ_JVJVF]FjUQ_JTJ)jBYib/W9\_FP!GHY3I9PQSPVJVFI(J!j	Q_JT=?>Lv KIOM7vbFG]QSf9bYuG]K\S\_Xv0QSZ
JT#F]bF!FHuQSPJP"KM#FG]QSP/Q_Y3IlGHY3ccQSJVJVF]FjUQ_JTJ)jBYb/W9\_FPGHY3IOPQSPVJVFI(J"jUQ_J
T =?>nv#JT#FIJT#F]bFFH#QSPVJPUK~ fi
  fi  Y3\SYb/KaJQ_Y3IYZ JT#FoF\_Fc2FI(JPY<
Z >nR9W9bJT#F]b/c2Yb/Fv#JT#F]bFI#F]NF]b"FH#QSPVJPK2M9FG]QSPQ_Y3IGHY3ccQ_JVJVF]F
jUQ_JTY3I9\_XfiY3I#Fdb/WO\_FdGHY3I9PQSPJVFIJ0jUQ_J@
T =A>n "	FI9GHFv^OI9MOQSI#[LJT#FRM#FG]QgPQ_Y3IGHY3ccQ_JVJVF]FBj	Q_JToJT#FRP/cK\S\ fi
FPVJhI(W9cffemF]bUYZb/WO\_FPUGHY3I9P/QSPVJVFI(JUjUQ_J
T =?>QSPUKaJfi\_FKPVJhKPhTOKab/MlKPfiPVY3\_NuQSI#[ ff~ fi   fi  Y3\_Yb/KaeOQS\gQ_JX uv
KI9MJTOQSPQSPQSI(JVb/KGHJKaeO\_FffQ_Z  % 0 2
:BC

Q

fi

iwc

	
ffff

 F]J !"M9FI#YJVF5JT#F 
 F\_Fc2FI(JkYZ2v	KI9M "JT#F 
 F\_Fc2FI(JkYZ1>n FM#F]^OI#F5KPVF]J
YZ/GL>HG"qnY{Y3\_FKI N8KabQSKaeO\_FPQSI Y3I#FJVYY3I9FGHYb/bFPVfmY3I9M#FI9GHFjUQ_JT JT#FF\_Fc2FI(JPYZ >nv-jUT9QgG/T
jnFWOPVFJVYtM#FP/GHb/Q_emFJT#FFH#Kc2fO\_FPiYZ#=A>n "T#FGHYbbFPVfmY3I9M9QgI#[ PVF]J5YZ\SQSJVF]b/K\SPQSPiM9FI#YJVFM
0 ( (  ( ( . (  . (5464646( 4  (  4  93 oW#bbFM9W9GHJQSY3ItQSPcKM#FQSIJT#FJ)jBYfiG]\gKPPVFPlZ:b/Kc2F]jnYb "T#F
PKc2f`\_.
F =?>GHY3I(JKQSI9PJ)jBY%M9QSP VY3QSI(JnP/W#eOPVF]JP dJT9FPVF]J"YZ fY3P/Q_JQ_NFfiFHuKcfO\_F
P =?> Rv`KI9MkJT#FPVF]J"YZ
I#F][3KaJQ_NFY3I#F@
P =?> 
 =A>1GHY3IJKQSIOP GL> G`FH#Kc2fO\_FPvM#FI#YJVFMe{
X $ ( (P$ . (5464646(P$   4   FGHY3I9PJVb/W9GHJ
FKG/TifmY3PQ_JQ_NFfiFH#Kc2fO\_FPYJT9KaJUQ_JLbF]fObFPVFI(JPUKIF\_FcFIJ"Y<
Z >n  YbFfObFG]QSPVF\_Xv



" 4
p+x3
"  ( "  & 
A= >  GHY3I(JKQSI9PGGdFH#Kc2fO\_FP]v	M#FI#YJVFMe{XF$ ( (P$ . (5464646(P$      F5GHY3IOPVJVb/W9GHJ%FKG/T I9F][3KaJQ_NF
x









GL> GM(P$ 

%

 6

"   4

FH#Kc2fO\_FPVYJT9KaJUQ_JFI9GHYuM#FPFKGTlYZ JT#F!GHY3IOPVJVb/KQSI(JPYZ 
   YbFf9b/FG]QSPVF\_X 


x





GGM(P$  
%

 
 
#"%"$ 6
#""$ 4
"  : 7 '
"%  : 7& '

p+x +(

Q_JT#Y3W#J\SY3PP7YZ{[FI#F]b/K\SQ_J)XvjnFRcKFRZY3W#b0KPPW9c2fOJQ_Y3I9PY3IJT#FdQgI9PVJKI9GHFYZ ff~ fi   fi  Y3\_YbKaeOQS\SQ_J)X&

M9W#FoJVYJT#FoZ:KGHJ	JT9KaJUQ_J"QgPI#YJ"JVb/Q_NuQSK\ 
xaUUT#F]bF	M9Y(FPnI#YJnFHuQSPJBPVY3c2F	F\_Fc2FI(JBYZ >f9b/FPVFIJnQSIK\g\OGHY3I9PVJVbKQSIJPIJT9QSPBG]KPVFhQgI9M#F]FM7v
JT9FJVb/Q_NuQSK\LGHY3\_Yb/KaJQ_Y3IGHY3IOPQSPVJP2QSI[3QSN{QSI9[JVYY3I9FYZhPWOG/TF\_FcFIJP2Y3I9FlGHY3\_YbvLKI9MJT#F
YJT9F]b	GHY3\_Yb"JVYK\S\7YJT#F]b"F\SFc2FIJPUYZ >n
~{


p'*(ff
J(	8( D8 Cm/0x3(/~)(5464646(RGL> G 9  jUQ_JT  % 0 
KI9M % 0 8v
 & m0x3(/~)(5464646(RGG 9 (,0  ("390 !,+ 6 0  ( 1 9-0 !,+4
p+x23
fiJT#F]bjUQSPFQgI9M#F]FM7vJT9F]bF2jBY3WO\SM5FHuQgPVJp'*(ff
 (	8( 8w:m 0x3(/~)(5464646(RGL>HG 9  j	Q_JT % 0 
KI9M  % 0 8
P/W9G/TJTOKaJ
x (/)~ (5464646(RGG 9 (,0  ( "39  !,+  0  ( 1 9  !,+(
p+x|3
 & m03
KIOMlQSIJT9KaJ	G]KPVFv`K2JVbQ_N{QgK\7PVY3\SW#JQ_Y3IkJVY ff~ fi   fi  Y3\_YbKaeOQS\SQ_J)X&jBY3W9\gMGHY3I9PQSPVJ"QgI[3Q_NuQSI#[JVY
  Y3I#F!GHY3\_YbhKI9MkJVY  " JT#FoYJT#F]b	Y3I#FvOKI9MlJVY   Y3I#F!GHY3\_YbhKI9MkJVY  1 JT#FYJT9F]bUY3I#F

{ B KG/TF\_FcFIJ%YZ"> eF\SY3I#[3PJVYKaJk\_FKPVJ%Y3I#FGHY3IOPVJVb/KQSI(J%QgI  hJT#F]bj	QSPVFv"Q_J%G]KI1emF
b/Fc2YNFM7

+# BKG/TlGHY3IOPVJVb/KQSI(J"GHY3IJKQgI9PKaJ"\_FKPVJJ)jBY2F\_FcFIJPZ:bY3c

>nBfiJT#F]bjUQSPFhQ_JG]KIkemFfibFc2YNFM7

fi uW9f9fmY3PVFJT9F]bFFHuQgPVJPKPVY3\gW#JQ_Y3IJVY ff~ fi   fi  Y3\_Yb/KaeOQS\gQ_JX u FeOWOQS\SMJT#F} 	 j	Q_JTJjnY
c Y3I#Y3cQSK\SPnYZLp s FKab/I9PF]J"K\w_vxy3"GHY3I9PQSPJVFIJjUQSJTJT9FoFHuKc2f`\_FP]d"T#FIv{jnFoeOW9Qg\SMJ)jBYbW9\_FP
2
e{XiKPPVYuG]QSKaJQSI9[%JT9F-J)jBYic2Y3I#Y3cQSK\gP	JVYlPY3c2Flp.KabeOQ_JVb/Kab/X#UfmY3PQ_JQ_NF-NaK\SW#FffUT#F2M#F]Z:KWO\_JhG]\gKPPoQSP
ff fi uBUT9QSPL\_FKM9PJVYKM#FG]QSPQSY3IGHY3ccQ_JVJVF]Fj	Q_JTJjnYb/WO\_FPGHY3I9PQSPJVFIJ"j	Q_JT =?>n
fi uW9f9fmY3PVFfiJT9KaJUJT#F]b/FFHuQgPVJPUKM9FG]QSPQ_Y3IGHY3ccQ_JVJVF]F 5j	Q_JTKaJ	c2Y3PJ"JjnYb/WO\_FP"GHY3I9PQgPVJVFIJUjUQ_JT
=?>L
 FI#Yj PT#Yj JT9KaJ2JT#F]bFkFH#QSPVJP2K5NaK\SQSM~ fi   fi  Y3\_Yb/KaJQSY3IYZUJT9F%F\SFc2FIJPYZ >n F
^9b/PJP/T#Yj JT#b/F]Fl\_FccKPjUTOQSG/TPT9K\g\BemFW9PVFM\gKaJVF]bY3I7"T#FI7vdjnFlPT#Yj JT9KaJJT#FM#FG]QgPQ_Y3I
GHY3ccQSJVJVF]F%QSP!KGHJW9K\S\_XF{W9Q_NaK\_FI(JJVYiKi} h j	Q_JTJjnY5c2Y3I#Y3cQSK\gPGHY3I9PQSPJVFIJffjUQ_J
T =A>n F
GHY3I9G]\SWOM#FUe{XW9PQSI9[!f9bF]NuQ_Y3W9PRbFPW9\SJP	p s FKab/IOPRF]JK\w_vmxy3BY3IT9YjJVYffJVbKI9PVZ:Yb/c JTOQSPR} hQSI(JVY
K2NaK\SQSM~ fi   fi  Y3\SYb/KaJQ_Y3IiYZ0JT#F!F\_Fc2FI(JP"Y<
Z >n



:BC6:

fi

 p N "RT   fTgJ"! J %(   % "  J %8a WV   "	 (
 %KJ  J Wi f" T 
 !  ]
fi  J  I H J  	  "fJ(%	"  !  "%   X    3 "  J  !wJ H "! %j]  
fi J  J %  I  T 	  T J "! 	  "fJ 6J  3 "! ! 
  %KJ  J a !\J ] "! % !
"   4 

B "F> >



"

 "4

(

p RbY(YZ PVJVb/KQS[3TJVZ:YbjLKab/M`

B "F> >

jV   "! ! 
 %KJ  J aji "fT 
 !  %K]
J  J %  T 
 .61
p:+I9M#F]FM7v Z:Yb-KI(XN8KabQSKaeO\_Fv JT#F]bFFHuQSPJJjnYifmY3PQSJQ_NFFH#Kc2fO\_FP-T9KNuQSI#[JT#FGHYbb/FPVfmY3I9M9QSI#[
fmY3PQ_JQ_NFo\SQSJVF]b/K\wv#KI9MJT#FffGHYbbFPVfmY3I9M9QgI#[-I9F][3KaJQ_NFff\SQSJVF]b/K\


 pN R
" T   fTgJ"! J %

% "  J 8% a

B "F> >       "fJ(% Wi "6  !   X   $ !  % 1
k36)jl uW#f9fmY3PVF%JT9KaJ GHY3IJKQgI9PY3I#Fb/W9\SFvjUT#Y3PVFc2Y3I#Y3cQSK\LQSP2G]K\S\_FM  ( Z"JT#FlM#F]Z.KW9\_J
G]\SKPPnQSP ff fi uvuK\g\#fmY3PQ_JQ_NF"FH#Kc2fO\SFPBPKaJQgPVZX  ( v(jUTOQSG/TQSPdQSc2fmY3PP/Q_eO\_FLe{X  FccK +( JT#F	c2Y3I9Y3cQSK\
jnY3W9\SMemFFc2fOJXv KI9M GHY3WO\SMI#YJemFGHY3IOPQSPVJVFI(J]k)ZLJT#FM9F]Z:KW9\SJG]\SKPP!QgP ff  uv JT#FI9F][3KaJQ_NF
FH#Kc2fO\_FPhKabFffG]\gKPPQ_^9FMe(X  ( KIOMlJT#F]bF]Z:YbF ( uh"T{W9P]vOI#Y%fmY3PQ_JQ_NF!FHuKc2f`\_FffPKaJQgPV^9FP  ( 
"  4 
#bY3c  FccK {vFQSJT#F]b  ( % "  ( #"-KI9MI#YkI#F][3KaJQ_NFFH#Kc2fO\_F-G]KIP/KaJQSPVZ:XlQ_J-p.QScfY3P/PQ_eO\_Fv
Yb  ( GHY3IJKQSIOP-KaJ\_FKPVJ-JjnY5I#F][3KaJQ_NF\SQ_JVF]b/K\gP]v KIOMJT#F%GHY3I9PVJVb/KQgIJPK\S\BT9KNFQSIGHY3cc2Y3IJjnY
F\_Fc2FI(JP-YZn
> iUT(W9Pv JT9F%QSIOPVJKI9GHFYZ ff~ fi   fi  Y3\_Yb/KaeOQg\SQ_JX QSP!JVbQ_N{QgK\wvjUTOQSG/TQSP!Qgc2fmY3PPQ_eO\SF
"T9QgPLFI9M9PLJT#FfObY(YZ0YZ  FccK {
F-I#YjP/T#Yj JT9KaJ"JT#F!M9F]Z:KW9\SJLG]\gKPP"YZ 5QSP ff fi un#YbUJT#FffPKF!YZ PQSc2f`\SQSG]Q_J)Xv{jnFj"bQ_JVFoJT#F
JjnY5c2Y3I#Y3cQSK\gPoYZ e{/
X  ( KI91
M  . l"T9FM#F]Z.KW9\_J!G]\SKP/P!QSPffM#FI#YJVFM 1m 0 ff fi uv ff  )93  KuQSI#[
JT#FKPP/W9c2f9JQ_Y3I5JTOKa
J  % ff  lQScfO\SQ_FP	JT9KaJ!K\S\I9F][3KaJQ_NFFH#Kc2fO\_FPc-W9PVJP/KaJQSPVZ:XiKaJ!\_FKPJY3I#F
c2Y3I#Y3cQSK\7QSI 







fi #W#f9fmY3PVF2JT9KaJ (  KI9M .  u%"T9FI7v0I#YfY3P/Q_JQ_NF2FH#Kc2fO\_FG]KIPKaJQSPZXFQ_" J T#F]4 b   (
Yb  . 9bY3c JT#F%JjnYfmY3PP/Q_eOQS\SQSJQ_FPfiYZ  FccK {vY3IO\_XJT#F%^9b/PVJffY3I#FkQSP!NaK\SQSMp "  (  "
G]KIOI#YJoemFP/KaJQSPV^9FMe{XiKI(XI#F][3KaJQSNF2FHuKc2f`\_Fff"T{W9P]v  ( KI9M/ . GHY3I(JKQSI5FKG/TKaJ\_FKPVJ
J)jBY%I#F][3KaJQ_NF-\SQ_JVF]b/K\SP2

0   (  ":9
0   ( 19




( (

p+x
p+x3

. 4

FKabF2QSIJT#FPVFGHY3I9MG]KPVFYZRJVb/Q_NuQSK\SQSJXYZnJT#F2QSI9PVJKI9GHFYZ ff~ fi   fi  Y3\_YbKaeOQS\SQ_J)X&uv0PQSI9GHF



cK{QSI9[2JT#F!KPPWOc2f9JQ_Y3I%JT9KaJ 5QgPGHY3I9PQSPVJVFI(JUQScfO\SQ_FP

&

fi #W#f9fmY3PVFfiJT9KaJ

m0x3(/~)(5464646(RGG 9 (,0  ("39 0 !,+ 6

0  (  1 9- 0 !,+4

p+xy3

 KI9M . 
 u  \S\I#F][3KaJQ_NFFH#Kc2fO\_FPUcffW9PJUPKaJQSPVZ:X  (   ( QSPZ:Yb/GHFM

JVYkemFc2Y3I#YJVY3I#Y3W9PoP/QSI9GHF-YJT#F]bjUQSPFkp:[3Q_NFIJT9KaJE% ff  	K\S\ I#F][3KaJQ_NF2FH#Kc2fO\_FPhjnY3W9\SM
P/T9KabF!KGHY3cc2Y3II9F][3KaJQ_NF-\gQ_JVF]b/K\wv#JT{W9PUK\g\GHY3I9PVJVb/KQgIJPUjBY3W9\gMlPT9KabFffKGHY3cc2Y3IF\_Fc2FI(J
YZ	>nvKI9M JT#FQSIOPVJKI9GHFlY)
Z ff~ fi   fi  Y3\SYb/KaeOQS\SQSJX jBY3WO\SMemFJVb/Q_NuQSK\w  . emFQSI#[PKaJQgPV^9FM
( 

:BC5z

fi

iwc

	
ffff



e{XKaJ2\_FKPVJ-Y3I#FkfmY3PQ_JQ_NFFHuKcfO\_Fp:YJT9F]bjUQSPVFv jBY3WO\SMemF%F(WOQ_N8K\SFIJffJVY5KPQSI9[3\_F fiDb/W9\_F
M9FG]QSPQ_Y3IGHY3ccQSJVJVF]FvfiKIOM1jnFZ.K\S\fiQSIJT#FGHY3IJVb/KMOQSGHJQ_Y3IYZ  FccK 3vfiQ_JGHY3IJKQgI9PKaJ
cY3PVJ	Y3I9F-I#F][3KaJQSNF\gQ_JVF]b/K\wZdQ_J	GHY3I(JKQSI9PhFH#KGHJ\_XlY3I#FI#F][3KaJQ_NF\SQ_JVF]bK\wv`Q_JhQSPUP/KaJQSPV^9FMie(X
FH#KGHJ\_XffY3I#FLfY3P/Q_JQ_NFRFH#Kc2fO\SFv3KI9M-jBFLG]KIbF]fO\SKGHFLQ_J e(X!JT#Fc2Y3I#YJVY3I#Y3W9P c2Y3I9Y3cQSK\(jUQ_JT
GL>HG)7 xfmY3PQ_JQSNF\SQ_JVF]bK\SPp:jnF%\_FKNFFcf9JXJT9FfmY3PQ_JQ_Y3IYZ"JT#FQgI9Q_JQSK\I#F][3KaJQSNFk\gQ_JVF]b/K\
 Y3I9PVF{W#FIJ\SXv9PQScQS\gKab/\_X-ZYb  ( v#jBFG]KIPW9f9fmY3PVF	JTOKa
J  . QSPLc2Y3I#YJVY3I#Y3WOP] F!M9QSPJQSI#[3W9QSP/T
J)jBY%G]KPVFP]

"  4



Z G ( G 
 G . GvOI#Y2fmY3PQSJQ_NFfiFHuKc2f`\_FG]KIPKaJQgPVZX- ( dqnXkZ.KGHJ{v  ( % "  ( #"vOKI9M
I9YI#F][3KaJQ_NFFH#Kc2fO\SFG]KIPKaJQSPZXkQ_J]v9KGHY3I(JVb/KM9QSGHJQSY3Ip 5G]KI9I#YJ"emFGHY3I9P/QSPVJVFI(J
Z G ( G  G . G  . G]KI9I9YJeFFc2f9J)X`z!JT#F]bF]Z:YbFQ_JkGHY3IJKQSIOPKGHF]b/JKQSItI(WOc!emF]b
YZ-fmY3PQ_JQ_NF5\SQ_JVF]bK\SP] BKG/TtfmY3PQSJQ_NF5FHuKc2f`\_FPKaJQSPVZ:X{QgI#!
[  . cffWOPVJK\SPVYPKaJQSPZX  ( v
P/QSI9GHF5YJT#F]bjUQgPVF
QSPI#YJlGHY3I9PQgPVJVFIJ]z2uQSI9GHF  ( KI9M  . KabFc2Y3I#YJVY3I#Y3W9P+
v  . QSPK
[FI9F]b/K\SQ_KaJQ_Y3IYZ  ( v KI9MKI(XFHuKc2f`\_FPKaJQgPVZXuQSI#[  ( p.QgIfOKabJQgG]W9\SKabv0JT#F%I9F][3KaJQ_NF
FH#Kc2fO\SFPLc-W9PVJ"PKaJQSPZX  . v9KGHY3I(JVb/KM9QgGHJQ_Y3I7





"T9F]bF]ZYb/F%)ff fi un"TOQSPZ:Yb/GHFPUK\S\fY3P/Q_JQ_NFoFH#Kc2fO\_FPUJVYPKaJQSPZXkKaJ	\_FKPJ"Y3I#FffcY3I#Y3cQSK\7YZ
M .  uJffGHY3cFP
( 
 iKI9*
 ( % ffp  FccK +(  \S\I#F][3KaJQ_NFFHuKcfO\_FPc-W9PVJPKaJQgPVZ/
X  . v KI9M5jnFK\SPVYlT9KN/
F G ( G  G . G


"
4

	YfY3P/Q_JQ_NFFHuKcfO\_FG]KIPKaJQgPVZX  . vKIOM  FccK %[3QSNFPFQ_JT#F](
b  ( % "  (  "lp.PKaJQSP^9FM5e(X
I#YkFH#Kc2fO\_Fv7QSc2fmY3PP/Q_eO\_FnY(
b  . GHY3IJKQSIOPoKaJ\_FKPVJfiJjnYlI#F][3KaJQ_NF\SQSJVF]b/K\SP]vjUT#Y3PVF2GHYbb/FPVfmY3I9M9QSI#[
F\_Fc2FI(JPYH
Z >Kab/FPT9KabFMe(X5K\S\dGHY3I9PJVb/KQSI(JP]v0KI9MjBFYeOJKQSIKa[3KQSIJTOKaJJT#FQSIOPVJKI9GHF2YZ ff~ fi
  fi  Y3\SYb/KaeOQS\SQSJX 2QSPLJVb/Q_NuQSK\w
"T9F]bF]ZYb/Fv ( 
 KI9M . 
 uvKIOMlFKG/T5c2Y3I9Y3cQSK\QgPUPKaJQSP^9FMle{XlKaJh\SFKPVJUY3I#FfffmY3PQ_JQ_NF
FH#Kc2fO\_F
QgP!JT{W9PffF(W9QSN8K\_FI(J-JVYK5} 	 jUQ_JTJT#FPKc2F%JjnYc2Y3I#Y3cQSK\gP]vKI9MjnFG]KIW9PVF
Kkf9bF]NuQ_Y3W9PfiPVY3\gW#JQ_Y3Ip s FKab/I9PF]JK\w_vRxy3fiJVYe`W9QS\SMKkN8K\gQSM~ fi   fi  Y3\_Yb/KaJQSY3I72 QSb/PVJ]vjnFG]KI
PW#fOfY3PFdJT9KaJ 2QSP Ka[3KQSIffcY3I#YJVY3I#Y3W9PLp s FKab/I9P F]J K\w_v#xy3d"T#FI7vaPQSIOGHFRFKG/T-fmY3PQ_JQSNFdFHuKcfO\_F
PKaJQSP^9FP KaJd\_FKPVJ Y3I#Fc2Y3I#Y3cQSK\7p % ff fi v3JT#FIZ:YbK\S\NaKab/QSKae`\_FvJT#F]bFnFHuQgPVJPKfic2Y3I#Y3cQSK\(jUT9QgG/T
M#Y{FP"I#YJUGHY3I(JKQSIJT9F!GHYbbFPVfmY3I9MOQSI#[-fY3P/Q_JQ_NF\SQ_JVF]bK\w"T#F!~ fi  Y3\_YbKaJQ_Y3IlQSPJT9FI

 EUFG]K\S\RJTOKaJ GHY3I(JKQSI9P-J)jBYc2Y3I#Y3cQSK\SP]iuW9f9fmY3PVFJT9KaJ







"87 c(QSI  .j 0 
    m 0  "J9 4


m0x3(/~)(5464646(RGL> G 9 ( p    %


pw~a

 Y3WO\SMJT9QgP!emFQgINaK\SQSM  "T9KaJ-jBY3W9\gMc2FKIJT9KaJ-JT#F]bF%FHuQgPVJPKiGHY3I9PJVb/KQSI(J !UPWOG/TJT9KaJ
 "m !  ( p  "#%  % !  UT9QSPjnY3W9\SMc2FKI JT9KaJJT#FlGHYbb/FPVfmY3I9M9QSI#[I#F][3KaJQSNFlFHuKcfO\_F
PKaJQSP^9FP   vOK2GHY3I(JVb/KM9QgGHJQ_Y3Ip s FKabI9PF]JUK\w_vxy3L"T9QgPLFI9M9PLJT#FfObY(YZ YZ "T#F]YbFc {

k36)N6)

" 5"Z> 

 c

}hF]^OI#FJT9FoZ:W9IOGHJQ_Y3I





~

  '  ( 
0 (



% & P/W9G/TJTOKaJ

 02(x3(5464646(*!7x:9 ( 

- /

%

4 ; pVp'&)(	8(ff
(V"p 
 (	9 (
+   "

pw~ux

jUQSJT
np 
J(	9

% $  - - 
um  6



m0  / /  $ 


 m0
 --/


6  m
:BC>;

 / /  - -_p 
Rm 

6  m

 p 
 m0 
  

6  m0

  / /.4

fi


>
>

(

>

(

>

(

>
(

>

.

>
4

>


(

>
>

.

>

.

>

4

>

4

.

>

.

>

4

>

4

>
>



>

(

>

>



 -

8?m
m

.

>
4

>


>

(

>
.

>
4

>


>

(

>


>

.

>
>


4



GHY(FG]QSFIJ"YZ 
; pVp'& 	( 8(ff
{V	QSI
+ /  - 
- / 
- /
  /
~
~
$  x
$  x
$  x
$  x
~J$  
J~ $  
$  x
$  x
~
~
~
$  $
x  $  
x  $  
$  x
$  x
~
$   $	 
~
~
x  $  
x  $  
~J$ 
~J$ 
x  $
x  $
x  $
x  $
~
~







 KaeO\_F/&  3Y PPQSeO\_FiGHY{FG]Q_FI(JPkYZ@;pVp'& (	8(ff
{V 
 FT9KNF^#uFM Z:YbPT#YbJ >
:x 9up  +v > . %  %v8> 4 %  v8>  % 	 %
(

% 02(x3(5464646(*!7

[FI#F]b/K\SQ_]FPJT#FJT#bF]FFH{f9b/FPPQ_Y3I9PfiYZ  QSI5F{W9KaJQ_Y3I9Ppw~3vLp 3vKI9M
KM#F{W9KaJVFNaK\SW#FPZ:YbfiR8	Yjv`jBF!GT#FGkJT9KaJ 5
 PKaJQSPV^9FP"JT#FPW#eOcY{M9WO\SKabQSI#F{W9K\SQ_J)X 
 -  + /   -  /  -  /   - /@(
	YJVFJT9KaJ



pDfijUQ_JT
pw~~3

Z:YbK\S\LPW#eOPF]JP  
(  0 2( x3(5464646(*!1
7 x:39 "T#F F]XQSPJVYFH#KcQSI#FkJT#FGHY{FG]Q_FI(J2YZ	FKGT
;-pVp'&)(	8(ff
{VvRZ:Yb!FKG/TPVF]J02(x3(5464646(*!	7tx:9up  +ffv  %vfi  v  JVYjUT9QSGT 
iYb G]KI
emF\_Y3I#[#d KaeO\_F/fObFPVFI(JPJT#FPVF!GHY{FG]Q_FI(JP] Fff[F]J"Z:bY3c  KaeO\_F/ 

 -

+ /   - 	 / 7 p  -  /   - /: %
 ~@7 $  7 $    4 ; pVp'& (	8(ff
{V - -_p 
=m   6  m   5p 
um  6  m   / /	4
+   "
"TOQSPd\SKPVJn(WOKIJQ_J)XQSP !ZYbLKI(X2fY3P/PQ_eO\_FGT#Y3QSGHFhYZ d
 d"T9F]bF]ZYb/Fv(cQSIOQScQ_QSI9[ QSIKIX2YZ
Q_JP JT#bF]FLZ:Yb/cP YZOF`"pw~3v`p 3vKI9MpD0emY3QS\SP0M#Yj	IJVYficQSI9QScQSQSI#[ Y3IffJT9FLP/W#eOc2YuM9W9\SKab PVXuPVJVFc
p 02(x3(5464646(*! 7 x:9 ( op:jUQSJTJT#F!KM#F{W9KaJVFNaK\SW#FP"YZ  L"T9QSPLf9b/YeO\_Fc KM9cQSJPfmY3\_X{I9Y3cQSK\ fiDJQScF
PVY3\_NuQSI#[-K\_[Yb/Q_JTOcPhp  bY JPG/T9F\wv  YNK PVv r uGT#b/Q VNF]bv`xyuxaz fffiW#F]X{b/KI9I#Fvmxyy3T9KaJQSPncffW9GT
QSI(JVF]bFPVJQSI9[QSPJT9KaJdJT9FUK\_[Yb/Q_JTOcP {I9YjUIKab/FUT9Q_[3T9\SXffGHY3c2fO\gQSG]KaJVFMKI9M2JQgc2F"GHY3I9PW9cQSI#[fiZYbBJT#F
[FI#F]b/K\ncQSIOQScQ_KaJQ_Y3I5YZ  pfiffoW#F]X(bKI9I#FvLxyy3 "	YjnF]NF]bvjUT#FIW9PQSI#[JT9FNaK\SW#FYZ 
 KP-QSI
F`!p +("KI9M  KPhQSIF`ffpDv`JT9FffGHYbbFPfY3IOM9QSI#[2Z.W9I9GHJQ_Y3I 5emFGHY3c2FPhPW#eOc2YuM9W9\gKab"PVX{cc2F]JVb/QSG
p  -  / %  -M02(x3(5464646(*!"7 x:9  /:  P2P/W9G/T7vBc2YbFkFG]Q_FI(Jlp.KI9MPQSc2f`\_F]boK\S[Yb/Q_JT9cP-FH#QSPVJ-JVY
cQSIOQScQ_]F  #YbnFHuKc2f`\_FvJT#F]b/FUFH#QSPVJPRK!fmYjnF]bZ:WO\9GHY3c!e`QSI9KaJVYb/QSK\OK\_[Yb/Q_JT9c jBYb
{QSI9[QSI k
 p'!  




pfifffiW#F]X{b/KI9I9Fvxyy3 	YJVF!JT9KaJJTOQSP"QSPPVJQg\S\mK2NF]bX\SKab[FGHY3c2f`\_FHuQSJX
:BC

O

fi

iwc

	
ffff

" 5"Z> 

k36) .?6" 7rff * )

 c

"T#FRbFM9W9GHJQ_Y3IffQSPcKM9FRZ:bY3c JT#F? fi "	Kab/Mfff9bYe`\_Fc 3  fiffp.#FQ_[Fv9xyy|3d"TOQSPQSPJT#FBG]\SKP/PQSG]K\
3   f9bYeO\SFc p  KabF]X r  Y3TOI9PVY3I7vLxy3ay3vdeOW#JffFKG/TNaKab/QSKaeO\_FKaf9fmFKab/P!QSIFHuKGHJ\SX lG]\SKWOPVFP]
hPQSI#[!K!jBF\g\ fi {I9YjUIbFM9W9GHJQ_Y3Ip  KabF]X r  Y3TOI9PVY3I7vOxy3ay3v#f`Ka[F {vujUQ_JTKIKM9MOQ_JQ_Y3I9K\9P/QSc2fO\_F
[3KM#[F]J]vfijnF5G]KItcK FKb/FM9W9GHJQ_Y3IZ:bY3c 3  fiJVYNF]bJVFHtGHYNF]bp:JT{W9P]vhQSI9M#F]fmFI9M9FIJ%PVF]Jv
Ye9JKQSIOQSI#[	Kh[b/Kaf`
T QgI!jUTOQSG/T-K\S\{NF]bJQSGHFPT9KNF"M#F][bF]FLFQ_JT#F]b {vYbduv3KI9M-Z:YbjUT9QSGT!JT9FL\gKab[FPVJ
QSI9M9F]fFIOM#FIJdPF]Jhp:Z:YbnPKaJQgPV^OKaeO\_F	QSI9PVJKI9GHFPRY%
Z 3  fi
 3RT9KPRP/Q_]1
F G3 G a~{v{jUT#F]bC
F G3#G3QSPdJT#FhI(WOc!emF]b
YZNF]b/JQSGHFPY
Z d#bY3c JT9QgPBf`KabJQSG]W9\SKabn[b/KafOTv{jnFfieOW9QS\gMK2PQScfO\_F"bFMOW9GHJQ_Y3I%JVY2Y3W#bLf9bYeO\SFc YZ
cK8#QScQ_QgI#[  	YJVFRJT9KaJ PQSI9GHFdjnFBKab/FRPVFKab/GT9QSI#[	ZYb KIYeO\SQSN{Q_Y3WOP7TX{fmYJT#FPQSP]vJT#FdYeOPF]bN8KaJQSY3I9P
KabFI#YJ	Qgc2fmYbJKIJ-p:jBFG]KI5PW#f9fmY3PVFoJT9KaJfiK\S\FHuKc2f`\_FP	TOKNF-JT#FffPKcFffYeOPF]bN8KaJQSY3I`U"T9KaJhQSP
jUT(XfiJT#FBb/FM9W9GHJQ_Y3IoY3IO\_XfieOW9QS\SMOP7G]\SKPP0NFGHJVYb/Pp:YNF]
b G3 GG]\SKP/PVFPvaFI9GHYuM9QSI#["JT9FBG]\SKPP0c2FcffeF]bPT9Q_f
YZ`KI(X-YZOJT#FPVFQSM9FIJQSG]K\{YeOPF]bN8KaJQSY3I9P] "T#FQSM#FKhQSP JT9KaJJT#FG]\SKPPFPKabF"QSIffY3I#F fiDJVY fiDY3I#F	cKaf9fOQSI#[
jUQ_JTkJT9FNF]bJQSGHFP]v`KI9MJT#F]bFKab/FJjnYPVF]JPUYZ G]\gKPPNFGHJVYb/P	eOW9QS\SJBZ:bY3
c 	
T G3#GNFGHJVYb/P]v{FI9GHYuM9QSI#[oJT#F	NF]bJQSGHFPRYZ  RKG/TY3I#FUQgPdK!G]\SKP/PNFGHJVYbLjUQ_JT2Y3I9\SX
fi KffPVF]JdjUQSJ
Y3I9F ff]x GHYbb/FPVfmY3I9M9QSI#[!JVY-JT9F	NF]bJVFHmv9KI9M%JT#FhbFcKQSIOQSI#[!GHY3c2fmY3I#FI(JPLKabFfi]F]bY{FP] RKGT
YZ JT#FGHYb/bFPVfmY3I9M9QSI9[-FH#Kc2fO\SFP"T9KNF!jBFQ_[3T(ff
J 0  

fi KkPVF]JfijUQSJTG  GONFGHJVYb/PvjUT#F]bF G  GOQSPhJT#FI(WOc!emF]bhYZdFM#[FPfiYZ- B KG/TY3I#FFI9GHYuM#FPfiKI

FM9[Fv9KI9MJT9F]bF]ZYb/FhGHY3I(JKQSI9PnJjnY ff]xlp.KI9MJT#F	b/FcKQSI9QSI9[!KabFfi]F]bY{FPdGHYb/bFPVfmY3I9M9QSI9[JVY
JT9FJjnYNF]bJQgGHFP"YZ JT#FFM#[F
 BKG/TYZ JT9FGHYbbFPVfmY3I9M9QgI#[FHuKc2f`\_FPT9KNFffjBFQS[3TJff0  
 Y3IOPQSM#F]bhZYbcffW9\gKPpw~3vBp 3	ZYboFH#Kc2fO\SFff"T#F]XiKabF2JT#F2PW9c YZRJT9FGHY3IJVb/QSeOW#JQ_Y3IiJVY YZ
JT#FFH#Kc2fO\SFPT9KNuQSI#[jBFQ_[3T(J 0  v0KI9MJT#FFH#Kc2fO\_FPTOKNuQSI#[jnFQ_[3TJ 0  IJT#FPVFG]KPVFPv0jnF
G]KIlb/F]j"b/Q_JVF  W9PQSI9[-JT9F[FI#F]b/QSGfiFHuf9bFP/PQ_Y39I 

 %    (
pw~ 3
  % 0   $   7p*G3 G:7 9  7p 7x  p*G3 G37 9Hp*G3 G37 17 x  $  7p*G3 G:7 9  (Lpw~+(
  % 0   $   p*G3 G:7 9Hpw~     $  7pw~ 	   4
pw~3
"	F]bFv  QSPdJT#FfiI{W9c!emF]bBYZFM#[FPLT9KNuQSI#[ffJT#FQ_bBJjnY-NF]b/JQSGHFPLQSIJT#FfiPVF]JLGHYbbFPVfmY3I9MOQSI#[JVY-JT#F

 xNaK\SW#FPRQSI    v SQ PJT#FUI{W9cffemF]bdYZmFM#[FPRT9KN{QgI#[JT#FQ_bRJjnY!NF]bJQSGHFPnQSIJT#F	PVF]JBGHYbb/FPVfmY3I9M9QSI#[
JVYJT#FC
7 xfiNaK\SW#FPQgI    v9KI9M  QSPLJT#FI{W9c!emF]bLYZ FM#[FPUT9KN{QgI#[Y3I#FoYZ0JT9FQ_bNF]bJQSGHFP"QgIkJT#F  x
PVF]J]vOKIOMJT#FYJT#F]bUY3I#F!QSIkJT#F!
7 xPVF]J] QgPLJT#F!I{W9c!emF]bYZ  xoNaK\SW#FPQSI    
uW9f9fmY3PVF	JTOKaJff0 ff
 0  p214351 0  
 G3#G 4 0  R"T#FIkJT9FcK8uQgcQ_KaJQ_Y3IkYZ	 QSPLJT#FcK8#Q fi
cQ_KaJQSY3IYZ   v`ZY3\S\SYjnFMe(XkJT#FffcK8uQgcQ_KaJQ_Y3IYZ      KMOcQ_JP"KcK8uQSc-W9c Z:Yb #%G3 G a~{v

KI9MjUQSJTJTOQSP-N8K\SW9F%Z:Yb vBQSJG]KIeFkPT#YjUIJT9KaJcK8#QScQSQSI#[
  emY3QS\SP-M#YjUIJVYcK8#QScQ_]F
~
fi vhJT9KaJlQSP]v	JT#F p:jBFQS[3TJVFM`%I{W9cffemF]b%YZ-FM#[FPI#YJZ.K\S\SQgI#[FI(JQ_bF\_XQSI(JVYJT9F5PF]JlGHYb fi
bFPVfmY3I9MOQSI#[JVYJT#F  xkNaK\SW#FP]zLjUT9FI#F]NF]bJT#F 3  fi5QSI9PVJKIOGHFlQSP2PKaJQSPV^`KaeO\_Fp.KI9MW9P/QSI#[5JT#F
fOKabJQgG]W9\SKabLM#F][bF]FPLYZJT9F	NF]bJQgGHFPv9JT9QSPnPVF]JGHYbbFPfY3IOM9PRJVY2JT9Fh\SKab/[FPVJQSI9M#F]fmFI9M#FI(JRPVF]JY
Z -

k36)N6)bB^"Z> >





 T#F	f9bY{YZ7YZ7JT9QSPn\_FccKffQSPn(W9QSJVF	PVJVb/KQS[3TJVZ:YbjLKab/M7v{eOW#JdjnF	[3QSNFhQ_JnZ:YbnGHY3c2f`\_F]JVFI#FPP] t
"
 G]KIemF
bF]j"bQ_JVJVFIKP

 %

4
" &

$

:BC



 "  (

pw~|3

fi

jUQSJT

0    , "  1 0    , "  1
% 0 8 (  "   % !7" x $ 
(
pw~3
  !7 x $ 
jUT9F]bF p 
J(	9%   - 
/?7   -/D uW9f9fmY3PVFZ:YblGHY3IJVbKM9QSGHJQ_Y3IJT9KaJkZ:YblPVY3c2F 
  vff %
lp 
 (	9 
 u FPQgc2fO\_XfmF]b/c-W#JVFJT#F2JjnYlNaK\SW#FP   - 
/BKIOM   -/Dv0KIOM5jBFPT9Yj JT9KaJoJT9FI#F]j
NaK\SW#F!YZ  KaZJVF]bv   vQSP	I#YJh[bFKaJVF]bfiJT9KI  eF]Z:YbFfffF]bcffW#JQgI#[#v  @ 	UT#F!M9Q *mF]bFI9GHFffemF]JjnF]FI
  KIOM @ G]KIemFLFKPQS\_XM#FGHY3c2fmY3PVFM2W9P/QSI#[fiJT#FUI#YJKaJQSY3I ,   " 1 @ p'*ff( 
um/0 2( x3(5464646(*! l
7 x:9 (O5% 0 
{
KPhJT#F-N8K\SW9F!YZ    k
" p:F`pw~3VhQSI
 @ vmKIOM
 ,   " 1  p'*ff( 
=m 0 2( x3(5464646(*!.7 x:9 (O % 0 
{	KP	JT9F!NaK\SW#F
YZ    
" p:F`	pw~3VUQSI    F-K\SPVYM#F]^OI9F
( 
 	(  m/0 2( x3(5464646(*! 
7 x:9 (O % 0 
 % 0 8(  ,   "  1  %  ,   1     ,    1     , "  1     ,   " 1   4 pw~3
 *ff
F-M#F]^OI#FoQSIkJT#FffPKc2FjLKX  ,   "  1 @  F!YeOJKQSI
  7  @ %   , "  1  7  , "  1 @ 
  4 '   , "     1  7  , "     1 @ 
 4
pw~y3
 (  7& "  d
RbYNuQSI#[2JT9KaJ   7  @  2G]KIemFoYe9JKQSI#FMlKP"ZY3\S\SYjUP]d Q_bPVJ]v
0   7 0 "  
 $  
 , "  1  7  , "  1 @ %
 xH7 $ 
  E4
!A
7 x
F-K\SPVYT9KNF  
m0 x3/( ~)(5464646(*! 9 0 
 	(  69 
~ 0 
" $ 
: 7 ~ 0 "  $   : 7 ~ 0   $   
 , "     1  7  , "     1 @ % ~!A0 
$
  



7 x
!A
7 x
!A
7 x
!
7 x
~0   7 0 "  

%
,$   : 7 $    

!7x
~0   7 0 "  
 $   :
%
8x 7 $ 
  E4
!
7 x
"	F]bFUjnF	TOKNFfiW9PVFUJT9F"Z:KGHJnJT9KaJ    %
  "  k"T9QSPP/T#YjUPBJT9KaJ   7  @  uvuKI9MFI9M9P





JT#FfObY(YZ0YZ  FccKxa

k36)N6)

" 5"Z> 

 c

0YhKNY3QSMffGHY3I9Z:W9P/Q_Y3I7v8jBFLG]K\S\   JT#FnNaK\SW#FdYZ GHY3c2fOW#JVFMffYNF]b JT#FnJVb/KI9PVZ:Yb/c2FMffPVF]J YZ#FHuKc2f`\_FP]v
KI9M  p  Z:Yb  m0 (   9KI9M 
 m0    (   9KPhJT#FffNaK\SW#FffYZRGHb/Q_JVF]bQ_Y3I  WOPQSI#[NFGHJVYb/ h)JfiQSP
PQScfO\_FhJVYYe9JKQSIlK ffVP(W G]Q_FI(J 2emY3W9I9M%JVYGT#FGJT9FJT#F]YbFcF!T9KNF

-p   %   p   7
  p    % - p    

4

>  3
LG > G 
 x
4

>  3
GL> G 
 x

0 4  4 $  =    4 $    =  "  "$ (
GL>HG  734   "734   

p a

0 4  4 $  	=     4 $   =  "  "$ 4
 "87 4
  
GL> G  734 


p ux

:BC C

fi
"	F]bFv0 l
4 QSP"JT#FffPWOc

iwc

	
ffff

YZjBFQS[3TJP	YZ JT#F!FH#Kc2fO\_FP	QSIJT#F!Yb/Q_[3QSI9K\PVF]J]v`j	T#Y3PVFNFGHJVYb/PfiT9KNF
 wp      .p   v`PQSIOGHFoY3W#bUK\_[Yb/QSJT9c QSPYfOJQScK\wv

ff]xcKaJG/T9QSI9[JT#FF\_FcFIJPUYZ<>L 	YJVF!JT9KaJ

KI9MjnFYe9JKQgI

-p  


-p    

4

>  3
GL> G 
 x

0 4  4
GL> G



 734

$   =    

 "87 4 
4 



$     =   "  7 $   =   

 "734 4


$    =  " 





"$ 4

qnXkJK{QgI#[2Y3I9\_X%JT#FofmY3PQ_JQ_NFof`KabJYZ0JT9Fb/Q_[3T(J fiT9KI9MPQgM#Fv9KI9MkbFcKab
{QSI9[2JT9KaJ

fi

>  3 (RGL> G 
 3x (  bm >nv&6 "734
   $    =   "   $* ' 4    4 (  
 6 " 7 ? +4 $    =   "  :p JT#Fb/QS[3TJhPW9c
QgPUp'!7 GL> G  $   K I9MJT#Fff\_F]ZJY3I9FQSP  *p GL> JG 7x 5 $av
"

fi JT9F!GHY(FG]Q_FIJUYZ*04QSI   QSP 54% 6  734 $  	=   6 "7 ? +4 $   	=   v




jnF[F]J

-p  

-p    

$ GL> *p GGL>p'! G37 7GLx>  G  0 4 4

4



>  3
LG > G 
 x
-p     $ 7p p'C!7 7 x9 







-p   

Ox 

	

!7


$

4



>  3
GL>HG 
 x
ff
fi


04 54


(

KP	G]\SKQSc2FM7

k36)N6)bB^"Z> > %
 n

emFGHY3c2FP"QSIJTOKaJUG]KPVF




jUT9F]bF %   - x /

 % 0  $   0  $  (
p ~3
7   - /DB"T#F]bFfiKabFo^9NFoM9Q *mF]bFI(JLN8K\SW9FPBZ:Ybfikv#[3Q_NuQSI#[-b/QSPF	JVYI9QgI#F	M9Q#*F]bFI(J

%
%
%
%
%

, 
 x , 
 , 
7 x , 

7 ~ , 
U
 ~

% p 7x3(  x
% p
7 x3(  
% p
7 x3( 
7 x
% p.2( 
7 x 
% p  x3( 
7 x

(

 %
 
 %
4
:BC


p.2(  x (
% p.2(   % p  3x (  x (
p  3x ( (

fi

 Q
% 5jUT#F]bF  m 0 7U~)( 7x3(2(x3(/~ 93  m 07x3(2(x3(/~ 93vJT9FN8K\gW#F % 5
 PT#Y3W9\SM5emF
f9bF]Z:F]bbFMkJVYJT#FNaK\SW#F % 17 xQ *iJT9F!GHYbbFPVfmY3I9MOQSI#[QgPPcK\S\_F]bv#JT9KaJUQSP 

0   $    0   $   
EUFKabb/KI#[3QSI9[2JVF]b/cP[3Q_NFP0   0   (&


    O >  

0   $   &   0   $  &  4



p  3 

R"TOQSP\_FKM9PLJVYJT9Fb/W9\_FfiYZ JT#F\_FccKu



qLKW#F]bv"_v rs Y3T9KNuQwvaEup+xyyy3  IffFc2fOQSb/QSG]K\GHY3cfOKab/QSPVY3IffYZ9NYJQSI9[hG]\SKP/PQ_^OG]KaJQ_Y3I-K\_[Yb/QSJT9cP
qLKa[[3QSI9[#vOemY(Y3PJQSI#[#v#KI9MNaKab/QSKI(JP] 8 "5 I J  "  2J  3v > vx]6`xy{
qL\SK Fv    _v s F]Y[3T7v "_v r  F]bv  p+xyy3    bF]fmY3PQ_JVYb/X-YZcKG/T9QSI9F	\_FKab/IOQSI#[M9KaJKaeOKPFP]_
  	ff
fifi   	fi "!
fifi$#"%	& $ ff'  ff(  $ #
qnbFQScKIv  p+xyy|3LqnKa[[3QgI#[f9b/FM9QSGHJVYb/P] 8 "6 I J 
 0 "  2J 3v <) vx~ `x +3u
qnbFQScKIv  _vR#b/FQSM9cKI7v  
 "ff_vnfi\SPT9FI7vE  _v r {JVY3I9Fv    Up+xy +( Q !#"%<%KJ a 
"  J 	 "	 V
*U 3 %<%KJ 	 t V %  KM9PVjnYbJT7
qLW Kuv  _v r  F]F"v + fipw~a#xd}hKaJK2cQSI9QSI#[-GHb/Q_JVF]b/QgK!Z:YbLJVbF]F fiDeOKPVFMkbF][b/FPPQ_Y3IkKI9MkG]\SKPP/Q_^OG]K fi
JQSY3I7oI , 2 /j V J  3 %-[N  I  @  p  H  "  J 	 "!bQ dN ]V   	/.  fX ! W V23{ PJ %  a] J

P	" ""%  % v`f9fO~30 5|{
qLW9I(JQSI#Fv  _v r hQ_eO\_F]JVJ]vo`p+xyy~3  Z.W#bJT#F]bdGHY3c2f`Kab/QSPVY3IYZmPVfO\SQSJVJQSI#[hb/W9\_FP Z:YbR}hFG]QSPQ_Y3I fibF]F
QgI9M9W9GHJQ_Y3I 8 "6 I J  1
 0 "  2J 33
v 2v { {
 \SKa
b `v 0_v r qBY3PjBF\S\Dv`Ep+xyyuxoEUW9\SF!QSI9M9WOGHJQ_Y3IjUQ_JT  fi&
~ PVY3cFffbFGHFI(JoQSc2f9b/YNFc2FI(JP]U+I
, > /j V J 3 % [N  I   54 $   
# "	 G   EJ  376 %<%KJ 	J   "  2J  3vOfOf72x `x|uxa
 Y3T#FIv    _v r uQSI#[F]b
v +Rp+xyyy3  uQSc2f`\_Fv`9KPJ!KI9M  *mFGHJQ_NFE	W9\_F  FKabI#F]b-8
I ,  _
/j V J  3 % [N  I :
 9    "  J 	 "! Q	4N ]V    <; 
  J a J "! p   ! !wJ 3(  ]v#f9f  5+~{
M#F  Kab/N8K\ST9Y  Y3c2FP]vOn  _v r  KPG]W#F\Dv-p+xyy +(L#}  v9K2PVJVYuG/T9KPVJQgGK\_[Yb/Q_JTOc ZYbU\SFKab/I9QSI#[
M9FG]QSPQ_Y3I\SQgPVJPhjUQ_JT\SQScQ_JVFM5GHY3c2fO\_FH#Q_JX ;   "! %[N 8 "  I  T "  J % "	 V ;   J a J "! p   ! !\J _
3{  H=
v 9>8v`~u?x 5a3~{
}fiQ_F]JVJVF]b/QSGT7v0  npw~a  IFH{fmF]b/QScFIJK\dGHY3cfOKab/QSPVY3IYZLJT#bF]FcF]JT#Y{MOPoZYb-GHY3I9PVJVb/WOGHJQSI#[
FIOPVFc!e`\_FP YZ`M#FG]QSP/Q_Y3IJVbF]F2P 3qnKa[[3QgI#[#vaeY{Y3PVJQSI9[#v8KI9Mffb/KI9M9Y3cQ_KaJQ_Y3I7 8 "5 I J @
  "  2J 33v
>) 8v0x y `2x 3(
}hY3cQSI#[Y3Pv 0Rp+xyy3  RbY{GHFP/P fiDYb/Q_FI(JVFM "UFW9b/QSPVJQSG-Z:Yb  YuM#F\PVF\SFGHJQ_Y3I7-8
I , 2 /j V J  3 % [N
I :
 9	A  p  H  "  J 	 "!cQ	dN H   	 8 "6 I J 
  "  6J  33v9f9fx~30 `x {
#FQ_[Fv ffLp+xyy|3  JT#bFPT9Y3\SMYZ"\SI -ZYbKaf9f9b/Y#QScKaJQSI#[iPVF]J-GHYNF]b+B
I , > /j V J 3 % [N  I 
2<  ;:Q 8 69 T 
 %eJ $ZT 	  I  t I    [NRQ T 
 $  J 33v9f9f ux +C5ux{
#b/KIOG v U_v r Q_JVJVFI7v(p+xyy3 hPQSI#["K dF]b/c-W#JKaJQ_Y3IffFPVJZ:Yb  JVJVb/QSeOW#JVFdPVF\_FGHJQSY3I!QSI}hFG]QSP/Q_Y3I
"bF]FP]m7
I ,L 2 W V J 3 % [N  I 
 9	A  p  ]  "  J 	 "!AQ	dN H   	 8 "5 I J ff
 0 "  2J 3v{f9f
2x ~ `x|au
#bFWOI9M7D
v +-_v r  KPY3I7v  p+xyyy3 "T9FK\_JVF]bI9KaJQSI#[M#FG]QSPQSY3I JVbF]F\_FKabI9QSI#[K\_[Yb/Q_JT9cl +I
, > /j V J 3 % [N  I E
 9   p  ]  "  J 	 "!^Q	dN H   	 8 "6 I J F
  "  2J 33v{fOfx~ +C`x  {
:BC>=

fi

iwc

	
ffff

#b/QSFM9cKI7v  _v "hKPVJQ_Fvao_v r "QSeOPT9Q_bKI9QwvHE{pw~a  M9M9QSJQ_NF  Y[3QgPVJQSGREUF][bFPPQSY3I 8Kfi{JKaJQgPVJQSG]K\
fiQ_F]j YZqnY{Y3PVJQSI#[# ;+ "! % [N 6 "  J %  J % v < 23v' 3053 +#
 KabF]Xv  _v r  Y3T9I9PVY3I7v}-Lp+xy3ay3 Q T 
 $ ] %)"	 Vp  . "6  "rJ !wJ . ]," 3 $ZJ V   I  I    [N
 ,v_ QfT 
 !     %<% RqnF\S\0F\_F]fOT9Y3I#F  KaemYb/KaJVYb/QSFP]
 b Y JP/G/T#F\wv  _v  Y
N K PVv  _v r uG/T9b/Q VNF]bv  up+xyux`"T#FnF\S\SQ_fOPY3QSMoc2F]JT#YuMKI9MffQSJP0GHY3I9PF(W#FIOGHFP
QgIGHY3c!eOQgI9KaJVYb/QSK\7YfOJQScQ_KaJQ_Y3I Q T  J  "    J<" v 9avx|y `xy3(
"	Y3\_JVFvBE!	p+xyy 3 F]bXPQScfO\_FG]\SKPPQS^OG]KaJQ_Y3Ib/WO\_FP2fmF]bZ:Yb/c jnF\S\Y3I cY3PVJGHY3cc2Y3IO\_XW9PVFM
MOKaJKPVF]JP] 8 "6 I J  
 0 "  2J 33
v 9	9av| {yuxa
"	X3Ka^O\wv  _v r EUQSNFPVJ]v#Ep+xy3a|3  Y3I9PJVb/W9GHJQSI#[-Yf9JQScK\M9FG]QSPQ_Y3I%JVbF]FPQS.
P   fiGHY3c2f`\_F]JVF^
 p dN8 K _


,


3






H


v

a
A

v
x
2


`


x
(


T " J 	 2 %<%KJ
%
 Y3TOI7v  	
s
r
 "ff_v Y3TOKNuQwvOE_v  'OF][F]bv s p+xyy +(bbF\SF]N8KI(J"Z:FKaJW#bFPUKIOMlJT#F!P/W#eOPVF]J"PVF\SFGHJQ_Y3I
fObYeO\_FcRI , > /j V J 3 % [N  I :
 9	9  p  ]  "  J 	 "!vQ dN ]V   	 8 "6 I J 1
  "  2J 33v
fOfx~u?x `x~y{
s FKab/IOP]v    _v r  KI9PVY3W#b
v +Up+xyy3   KPVJ]vRqnYJVJVY3c fiW#f}hFG]QSPQ_Y3I"bF]F db/W9IOQSI#[5K\_[Y fi
bQ_JT9c jUQSJT UFKab fifif9JQScK\[FI9F]b/K\SQ_KaJQ_Y3I +I ,L 2 /W V J  3 %/[N  I  9	A  p  ]  "  J   "!
Q dN ]V   	 8 "6 I J   0 "  2J 3v`f9fO~|y {~3(
s FKab/IOP]v  _v  Qwv  _v BQ_JVJ]v  _v r dK\SQSKI(J]v  up+xy3OfiI!JT9FB\_FKab/IOKaeOQS\SQ_J)XUYZ#emY{Y3\_FKIZ:Yb/c-W9\SKaF+I
, > /j V J 3 % ON  I  9   ;Q 8 69 T 
 %eJ $ZT 	  I  t I    ONRQfT 
 $  J  3v9f9f#~ {~y {
s Y3T9KN{QDvL}-_v r {Y3cc2F]b^9F\SMvn}-!p+xyy3 0Kab/[F]JVJQSI#[qLW9PQSI9FPPkW9PVF]b/P%jUQ_JT}hFG]QSPQSY3It"KaeO\_F
 \SKPPQ_^OF]b/P]a:
I ,L 2 W V J 3 % ON  I  ) p  ]  "  J 	 "!#Q	dN H     .  fX ! W V23{ PJ %  a]
J P	"  ""%  % vOf9f#~ +y {~  {
 KI9PVY3W#bv +_v r  G  \S\_FPVJVF]bvu}-pw~adqBY{Y3PVJQSI9[WOPQSI#[ffe9b/KI9GT9QSI#[fff9bY[b/KcP+5
I , > /j V J 3 %
ON  I  9 > %p  H  "  J 	 "!Q dN ]V   	qQfT 
 $  "  J 	 "!  "  2J 3 t I   av f9f ~~a 
~~ +#
 Kab[3QSI#FKI(JW7v#}-_v r }fiQ_F]JVJVF]b/QgG/T7v#o  p+xyy3 db/WOI9QSI#[!KMOKaf9JQ_NFfiemY(Y3PVJQgI#[#+5
I , > /j V J 3 % [N
I :
 9 )  p  H  "  J 	 "!cQ	dN H   	 8 "6 I J 
  "  6J  33v9f9fO~ux?x {~ux{
 Q_JG/T#F\g\wv9op+xyy3 8 "6 I J  
  "  2J 33  G  b/Kj fi "hQS\S\D
	Y{
G `vaE_v r  KP/G]W#F\wva{p+xy
y 3OfiI!\SFKab/I9QSI#["M9FG]QSPQ_Y3IGHY3ccQ_JVJVF]FP]{:
I ,L 2 W V J 3 % ON  I  9 < 
p  H  "  J 	 "!cQ	dN H   	 8 "5 I J 1
  "  6J  33vOf9f( +9x 5+~au  Yb[3KI s KW#Z.cKI9I7
	Y{
G `vE!_v r  Kaf9f{Xv  Rp+xyy3fiI5JT#F2fmYjnF]boYZLM#FG]QSPQ_Y3I5\SQgPVJP]+8
I ,L 2 W V J 3 % [N  I 7
 9	A 
p  H  "  J 	 "!cQ	dN H   	 8 "5 I J 1
  "  6J  33vOf9f( +9x 5+~au  Yb[3KI s KW#Z.cKI9I7
hfOQSJVv9}ff_v r  KG]\gQSI7v9Ep+xyyy3  YfOW9\SKabUFI9PVFcffeO\_Fc2F]JT#YuM9P 9K2P/W#bNF]X
  $   "! [N1; 
  J a dJ"!
p   ! !wJ 3(  
 *" %  "  I v 9	9avx|y `xy{
fffiW#F]X{b/KI9I9Fv  np+xyy3  QgI9QScQ_QgI#[PVX{cc2F]JVb/QSG2PW9eOc2YuM9W9\SKabfiZ.W9I9GHJQ_Y3I9P 8 "  I  T "  J 
"! ,  _
33 " T TgJ 33
v 2 < v `x~{
fffiW9QgI9\SKI7v  9E!p+xyy +( Q) 1 A ! 
`  3 "fT	%vNA  T "6 I J  !  "  2J 3  Yb/[3KI s KW#Z.cKI9I
fffiW9QgI9\SKI7v   ELp+xyy|3qLKa[[3QSI#[#v qnY(Y3PJQSI#[iKIOM  +# {l+B
I , > /j V J 3 % [N  I 5
 9 >   "  J 	 "!
]

V







p



(
3

]

#
v
9
f

f
`

a

~



u


3
a

u


Q dN  	 ; J a dJ"!  ! !wJ 

:BC%$

fi

EUQgM#[F]jnKXv  _v  KM9Q_[3KI7v }-_v E	QSG/T9KabM9PVY3I7v_v r   s KI#Fv  np+xyy3I(JVF]bf9b/F]JKaeO\_FemY{Y3PVJVFM
IOKQ_NFe`KXFP5G]\SKPPQS^OG]KaJQ_Y3I7 +I ,L 2 WV J 3 % [N  I  )  p  H  "  J 	 "! Q	4N ]V    
.  fX ! j V23( PUJ %  a] J  P	"  "
"%  % vOfOfx]#x?`x]+#
EUQSNFPVJ]vOEp+xy3  FKabI9QSI#[M#FG]QgPQ_Y3I\SQSPJP] 8 "6 I J   0 "  2J 3v < v`~~y{~+|{
uGT9KafOQ_bFv E "_v #bFW9IOM7v +_v qLKabJ\SF]JVJ]v  _v r  F]Fv  dnp+xyy3qnY{Y3PVJQSI#[JT#F  Kab/[3QSI  K
I9F]j FHufO\SKI9KaJQ_Y3I ZYb%JT#FlF *mFGHJQ_NFI#FP/PYZ hYJQSI#[c2F]JT#YuM9P] ;+  "! %/ON)%  "  J %  J % v <" v
x| u?x `x||{
uGT9KafOQ_bFv#E!! U_v r uQSI#[F]bv +p+xyy3R+c2f9bYNFMkemY(Y3PVJQgI#[-K\S[Yb/Q_JT9cPLW9P/QSI#[GHY3I#^OM#FI9GHF fiDbKaJVFM
fObFM9QSGHJQ_Y3IOP]8
I , 2 /j V J  3 % [N  I 7
 9	9 -p  ]  "  J   "! Q dN ]V     Q T 
 $  "  J 	 "!
0 "  2J 3 t I   8v`f9fOa {yuxa
dK\SQSKI(J]v     p+xy +(  JT#F]Yb/XYZdJT#Fff\SFKab/I9KaeO\_F QfTgT	$ 2J 
"  J 	 % [N  I  ;:Q 8 v <@ vxx +C
xx +~{
dK\SQSKI(J]v    Up+xy 3  FKab/I9QgI#[M9QSP W9I9GHJQ_Y3IOPYZfiGHY3I VWOI9GHJQ_Y3I9P]I , > /j V J 3 % [N  I   
p  H  "  J 	 "!   J   Q	dN H   	 ;   J a J "! p   ! !\J 3{  Hv9fOf |a &||{

:

5Q


fiJournal of Artificial Intelligence Research 17 (2002) 333-361

Submitted 2/2002; published 11/2002

A New Technique for Combining Multiple Classifiers using
The Dempster-Shafer Theory of Evidence
Ahmed Al-Ani
Mohamed Deriche

a.alani@qut.edu.au
m.deriche@qut.edu.au

Signal Processing Research Centre
Queensland University of Technology
GPO Box 2434, Brisbane, Q 4001, Australia

Abstract
This paper presents a new classifier combination technique based on the DempsterShafer theory of evidence. The Dempster-Shafer theory of evidence is a powerful method
for combining measures of evidence from different classifiers. However, since each of the
available methods that estimates the evidence of classifiers has its own limitations, we
propose here a new implementation which adapts to training data so that the overall mean
square error is minimized. The proposed technique is shown to outperform most available
classifier combination methods when tested on three different classification problems.

1. Introduction
In the field of pattern recognition, the main objective is to achieve the highest possible classification accuracy. To attain this objective, researchers, throughout the past few decades,
have developed numerous systems working with different features depending upon the application of interest. These features are extracted from data and can be of different types
like continuous variables, binary values, etc. As such, a classification algorithm used with a
specific set of features may not be appropriate with a different set of features. In addition,
classification algorithms are different in their theories, and hence achieve different degrees
of success for different applications. Even though, a specific feature set used with a specific
classifier might achieve better results than those obtained using another feature set and/or
classification scheme, we can not conclude that this set and this classification scheme achieve
the best possible classification results (Kittler, Hatef, Duin, & Matas, 1998). As different
classifiers may offer complementary information about the patterns to be classified, combining classifiers, in an efficient way, can achieve better classification results than any single
classifier (even the best one).
As explained by Xu et al. (1992), the problem of combining multiple classifiers consists
of two parts. The first part, closely dependent on specific applications, includes the problems
of How many and what type of classifiers should be used for a specific application?, and
for each classifier what type of features should we use?, as well as other problems that are
related to the construction of those individual and complementary classifiers. The second
part, which is common to various applications, includes the problems related to the question
How to combine the results from different existing classifiers so that a better result can be
obtained?. In our work, we will be concentrating on problems related to the second issue.

c
2002
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiAl-Ani & Deriche

The output information from various classification algorithms can be categorized into
three levels: the abstract, the rank, and the measurement levels. In the abstract level,
a classifier only outputs a unique label, as in the case of syntactic classifiers. For the
rank level, a classifier ranks all labels or a subset of the labels in a queue with the label
at the top being the first choice. This type was discussed by Ho et al. (1994). For the
measurement level, a classifier attributes to each class a measurement value that reflects the
degree of confidence that a specific input belongs to a given class. Among the three levels,
the measurement level contains the highest amount of information while the abstract level
contains the lowest. For this reason, we adopted, in this work, the measurement level.
Kittler et al. (1998) differentiated between two classifier combination scenarios. In the
first scenario, all the classifiers use the same representation of the input pattern. On the
other hand, each classifier uses its own representation of the input pattern in the second
scenario. They illustrated that in the first case, each classifier can be considered to produce an estimate of the same a posteriori class probability. However, in the second case
it is no longer possible to consider the computed a posteriori probabilities to be estimates
of the same functional value, as the classification systems operate in different measurement systems. Kittler et al. (1998) focused on the second scenario, and they conducted
a comparative study of the performance of several combination schemes namely; product,
sum, min, max, and median. By assuming the joint probability distributions to be conditionally independent, they found that the sum rule gave the best results. A well known
approach that has been used in combining the results of different classifiers is the weighted
sum, where the weights are determined through a Bayesian decision rule (Lam & Suen,
1995). An alternative method was presented by Hashem & Schmeiser (1995), where a cost
function was used to minimize the mean square error (MSE) in order to calculate a linear
combination of the corresponding outputs from a number of trained artificial neural networks (ANNs). The expectation maximization algorithm was used by Chen & Chi (1998)
to perform the linear combination. The fuzzy integral has been used by Cho & Kim (1995a,
1995b) to combine multiple ANNs, while (Rogova, 1994; Mandler & Schurmann, 1988) have
used the Dempster-Shafer theory of evidence to combine the result of several ANNs. Many
other combination methods have also been used to combine classifiers, such as bagging and
boosting (Dietterich, 1999), which are powerful methods for diversifying and combining
classification results obtained using a single classification algorithm and a specific feature
set. In bagging, we get a family of classifiers by training on different portions of the training
set. The method works as follows. We first create N training bags. A single training bag
is obtained by taking a training set of size S and sampling this training set S times with
replacement. Some training instances will occur multiple times in a bag, while others may
not appear at all. Next, each bag is used to train a classifier. These classifiers are then
combined. Boosting, on the other hand, is based on multiple learning iterations. At each
iteration, instances that are incorrectly classified are given a greater weight in the next iteration. By doing so, in each iteration, the classifier is forced to concentrate on instances it
was unable to correctly classify in earlier iterations. In the end, all of the trained classifiers
are combined.
In this paper, we will focus on combining classification results obtained using N different
feature sets, f 1 ,    , f N . Each feature set will be used to train a classifier, and hence there
will be N different classifiers, c1 ,    , cN . For a specific input x, each classifier cn produces
334

fiA New Technique for Combining Multiple Classifiers

Feature
Extraction
1

f

1

Classifier

y

1

c1

Combination

x

Feature
Extraction
N

f

N

Classifier

z

yN

cN

Figure 1: A multi-classifier recognition system

a real vector yn = [y n (1),    y n (k),    y n (K)]T , where K is the number of class labels and
y n (k) corresponds to the degree that cn considers x has the label k. This degree could be
a probability, as in the Bayesian classifier, or any other scoring system. Fig. 1 shows the
block diagram of a multi-classifier recognition system.
Unlike statistical-based combination techniques, the Dempster-Shafer theory of evidence
has the ability to represent uncertainties and lack of knowledge. This is quite important for
the problem of classifier combination, because there is usually a certain level of uncertainty
associated with the performance of each of the classifiers. Since available classifier combination methods based on this theory do not accurately estimate the evidence of classifiers,
this paper attempts to solve this issue by proposing a new technique based on the gradient
descent learning algorithm, which aims at minimizing the MSE between the combined output and the target output of a given training set. Aha (1995) gave the following definition
for learning:
Learning denotes changes in the system that are adaptive in the sense that they
enable the system to do the same task or tasks drawn from the same population
more effectively the next time.
Based on the above, we show that instead of attempting to find an analytical formula which
accurately measures evidence, one can obtain a very good estimate of evidence by just using
appropriate learning procedures, as will be discussed later.
Some basic concepts of the Dempster-Shafer theory of evidence are presented in the
next section. Section three discusses the existing methods for computing evidence. The
proposed combination technique is presented in section four. Section five compares the
proposed algorithm to other conventional methods used by Kittler et al. (1998), the fuzzy
integral, and a previous implementation of the Dempster-Shafer theory. Section six provides
a conclusion to the paper.

2. The Dempster-Shafer Theory of Evidence
The Dempster-Shafer (D-S) theory of evidence (Shafer, 1976) is a powerful tool for representing uncertain knowledge. This theory has inspired many researchers to investigate
335

fiAl-Ani & Deriche

different aspects related to uncertainty and lack of knowledge and their applications to real
life problem. Today, the D-S theory covers several different models, such as the theory of
hints (Kohlas & Monney, 1995) and the transferable belief model (TBM) (Smets, 1998).
The latter will be adopted in this paper as it represents a powerful tool for combining
measures of evidence.
Let  = {1 , ....., K } be a finite set of possible hypotheses. This set is referred to as the
frame of discernment, and its powerset denoted by 2 . Following are the basic concepts of
the theory:
Basic belief assignment (BBA). A basic belief assignment m is a function that assigns
a value in [0, 1] to every subset A of  and satisfies the following:
X
m() = 0, and
m(A) = 1
(1)
A

It is worth mentioning that m() could be positive when considering unnormalized combination rule as will be explained later. While in probability theory a measure of probability is
assigned to atomic hypotheses i , m(A) is the part of belief that supports A, but does not
support anything more specific, i.e., strict subsets of A. For A 6= i , m(A) reflects some ignorance because it is a belief that we cannot subdivide into finer subsets. m(A) is a measure
of support we are willing to assign to a composite hypothesis A at the expense of support
m(i ) of atomic hypotheses i . A subset A for which m(A) > 0 is called a focal element.
The partial ignorant associated with A leads to the following inequality: m(A) + m(A)  1,
where A is the compliment of A. In other words, the D-S theory of evidence allows us
to represent only our actual knowledge without being forced to overcommit when we are
ignorant.
Belief function. The belief function, bel(.), associated with the BBA m(.) is a function
that assigns a value in [0, 1] to every nonempty subset B of . It is called degree of belief
in B and is defined by
X
bel(B) =
m(A)
(2)
AB

We can consider a basic belief assignment as a generalization of a probability density function whereas a belief function is a generalization of a probability function.
Combination rule. Consider two BBAs m1 (.) and m2 (.) for belief functions bel1 (.) and
bel2 (.) respectively. Let Aj and Bk be focal elements of bel1 and bel2 respectively. Then
m1 (.) and m2 (.) can be combined to obtain the belief mass committed to C   according
to the following combination or orthogonal sum formula (Shafer, 1976),
X
m1 (Aj )m2 (Bk )
m(C) = m1  m2 (C) =

j,k,Aj Bk =C

1

X
j,k,Aj Bk =

336

,
m1 (Aj )m2 (Bk )

C=
6 

(3)

fiA New Technique for Combining Multiple Classifiers

The denominator is a normalizing factor, which intuitively measures how much m1 (.) and
m2 (.) are conflicting. Smets (1990) proposed the unnormalized combination rule:
X

m1 (Aj )m2 (Bk ), C  
(4)
m1 m
2 (C) =
Aj Bk =C

This rule implies that m() could be positive, and in such case reflects some kind of contradiction in the belief state. In this work we will consider that m() = 0 and use the
normalized combination rule. A comparison between normalized and unnormalized combination rules for the problem of combining classifiers will be considered in the future.
Combining several belief functions. The combination rule can be easily extended to
several belief functions by repeating the rule for new belief functions. Thus the pairwise
orthogonal sum of n belief functions bel1 , bel2 ,    , beln , can be formed as
((bel1  bel2 )  bel3 )     beln =

n
M

beli

(5)

i=1

Notation. According to Smets (2000), the full notation for bel and its related functions is:
<
belY,t
[ECY,t ](w0  A) = x

where Y represents the agent, t the time,  the frame of discernment, < a boolean algebra
of subsets of , w0 the actual world, A a subset of , and ECY,t all what agent Y knows
at t. Thus, the above expression denotes that the degree of belief held by Y at t that w0
belongs to the set A of worlds is equal to x. The belief is based on the evidential corpus
ECY,t held be Y at t.
In practice, many indices can be omitted for simplicity sake. Usually < is the power set
of , which is 2 . When bel is defined on 2 , < is not explicitly stated. w0  A is denoted
as A. Y and/or t are omitted when the values of the missing elements are clearly defined
from the context. Furthermore, EC is usually just a conditioning event. So, bel(A) is one
of the most often used notations (Smets, 2000). In the proposed method, we will adopt the
following notation: beln (k ), where the agent is the classifier, and the subsets of concern
are the class labels.
It is important to mention that the combination rule given by Eq. 3 assumes that the
belief functions to be combined are independent. Consider that we have certain information
and would like to measure its belief, then we can think of this process as a mapping from
the original information level to the belief level. Liu & Bundy (1992) explained that
independence in the original information level would lead to independence in the belief level.
But, if two independent belief functions are rooted to the original information level, then
their original information may or may not be independent. For the problem of combining
multiple classifiers, the original information level consists of outputs of the classifiers to be
combined, while the belief level consists of the evidence of these classifiers (or their BBAs).
The assumption that these BBAs are independent, whether obtained from independent or
dependent original information, can hence justify the use of D-S theory. In fact, many

337

fiAl-Ani & Deriche

existing classifier combination methods assume the classification results of different classifiers to be independent (Mandler & Schurmann, 1988; Hansen & Salamon, 1990; Xu et al.,
1992). Since the classifiers evidence plays a crucial role in the combination performance,
there is an increased interest in the proper estimation of such evidence. In the next section,
we discuss how a number of existing classifier combination methods estimate evidence of
classifiers, and in section 4 we present our proposed method.

3. Existing Methods for Computing Evidence
Mandler & Schurmann (1988) proposed a method that transforms distance measures of the
different classifiers into evidence. This was achieved by first calculating a distance between
learning data sets and a number of reference points in order to estimate statistical distributions of intra- and interclass distances. For both, the a posteriori probability function
was estimated, indicating to which degree an input pattern belongs to a certain reference
point. Then, for each class label, the class conditional probabilities were combined into
evidence value ranging between 0 and 1, which was considered as the BBA of that class.
Finally, Dempsters combination rule was used to combine the BBAs of the different classifiers to give the final result. As explained by Rogova (1994), this method brought forward
questions about the choice of reference vectors and the distance measure. Moreover, approximations associated with estimation of parameters of statistical models for intra- and
interclass distances can lead to inaccurate measure of the evidence.
Xu et al. (1992) used K + 1 classes to perform the classification task, where for the
(K + 1)th class denotes that the classifier has no idea about which class the input comes
from. For each classifier cn , n = 1..N , recognition, substitution, and rejection rates (nr , ns ,
and 1  nr  ns ) were used as a measure of BBA, mn , on  as follows:
1. If the maximum output of a specific classifier belongs to K + 1, then mn has only a
focal element  with mn () = 1.
2. When the maximum output belongs to one of the K classes, mn has two focal elements
k and k with mn (k ) = nr , mn (k ) = ns . As the classifier says nothing about any
other propositions, mn () = 1  mn (k )  mn (k ).
The drawback of this method is again the way evidence is measured. There are two problems
associated with this method. Firstly, many classifiers do not produce binary outputs, but
rather probability like outputs. So, in the first case, it is inaccurate to assign 0 to both
mn (k ) and mn (k ). Secondly, this way of measuring evidence ignores the fact that classifiers
normally do not have the same performance with different classes. This had a clear impact
on the performance of this combination method when compared with other conventional
methods especially the Bayesian (Xu et al., 1992).
Rogova (1994) used several proximity measures between a reference vector and a classifiers output vector. The proximity measure that gives the highest classification accuracy
was later transformed into evidences. The reference vector used was the mean vector, nk , of
the output set of each classifier cn and each class label k. A number of proximity measures,
dnk , for nk and yn were considered. For each classifier, the proximity measure of each class

338

fiA New Technique for Combining Multiple Classifiers

is transformed into the following BBAs:
mk (k ) = dnk ,

mk () = 1  dnk
Y
Y
mk (k ) = 1  (1  dnl ), mk () =
(1  dnl )
l6=k

l6=k

The evidence of classifier cn and class k is obtained by combining the knowledge about
k , thus mk  mk . Finally, Dempsters combination rule was used to combine evidences
for all classifiers to obtain a measure of confidence for each class label. Note that the first
combination was performed with respect to the class label (Rogova used the notations k and
k), while in the second one the agent was n. This idea was a promising one. However, the
major drawback is the way the reference vectors are calculated, where the mean of output
vectors may not be the best choice. Also, trying several proximity measures and choosing
the one that gives the highest classification accuracy is itself questionable.

4. The Proposed Combination Technique
In this section we will estimate the value of mn (k ), which represents the belief in class label
k that is produced by classifier cn . In addition, we will also estimate mn (), which reflects
the ignorance associated with classifier cn . Since the ultimate objective is to minimize the
MSE between the combined classification results and the target output, mn (k ) and mn ()
will be estimated using an iterative procedure that aims at attaining this objective. We will
first compare yn , which is the output classification vector produced by classifier cn , to a
reference vector, wkn , and the obtained distance will be used to estimate the BBAs. These
BBAs will then be combined to obtain a new output vector, z, that represents the combined
confidence in each class label. wkn will be measured such that the MSE between z and the
target vector, t, of a training dataset is minimized. Note that there are two indices for wkn .
Thus, for class label k, we dont only consider the value assigned to it by classifier cn , but
rather the whole output vector (values assigned to each class label).
Let the frame of discernment  = {1 ,    k ,    , K }, where k is the hypothesis that
the P
input x is of class k. Considering a BBA, mn , such that mn (k )  0, mn () =
1 K
k=1 mn (k ), and mn is 0 elsewhere. Let dn (k ) be a distance measure and gn the
unnormalized ignorance of classifier cn , then mn (k ) and mn () will be estimated according
to the following formulas:
(6)
dn (k ) = exp(kwkn  yn k2 )
mn (k ) =

dn (k )
K
X

(7)

dn (k ) + gn

k=1

mn () =

gn
K
X

(8)

dn (k ) + gn

k=1

where mn (k ) and mn () are the normalized values of dn (k ) and gn respectively. Similar
to wkn , the minimized MSE will be used to estimate gn .
339

fiAl-Ani & Deriche

Evidences of all classifiers are combined according to the normalized combination rule
to obtain a measure of confidence of each class label. The k th element of the new combined
vector is given by:
M
z(k) = m(k ) = m1 (k )      mN (k ) =
mn (k )
(9)
nN

L
For a given classifier cn , let I = {1    N } \ {n}, mI = iI mi , then Eq. 9 can be written
as:
z(k) = mI (k )  mn (k )
(10)
where according to Eq. 3, the combination of two BBAs is:
mj (k )  ml (k ) =

mj (k )ml (k ) + mj (k )ml () + mj ()ml (k )
XX
1
mj (p )ml (q )
p

(11)

q
q6=p

wkn and gn will be initialized randomly, then their values will be adjusted according to a
training dataset so that the MSE of z is minimized.
Err = kz  tk2

(12)

The values of wkn and gn are adjusted according to the formulas:
Err
wkn [old]
Err
gn [new] = gn [old]  
gn [old]

wkn [new] = wkn [old]  

(13)
(14)

where  and  are the learning rates. The terms Err/wkn and Err/gn are derived as
follows:
Err
wkn
Err
gn

=
=

Err z(k) mn (k )
z(k) mn (k ) wkn
Err z(k) mn (k )
z(k) mn (k ) gn

340

(15)
(16)

fiA New Technique for Combining Multiple Classifiers

where,
Err
= 2[z(k)  t(k)]
(17)
z(k)
("
#
XX
z(k)
=
1
mn (p )mI (q ) [mI (k ) + mI ()] + [mn (k )mI (k ) +
mn (k )
p
q
q6=p

"
mn (k )mI () + mn ()mI (k )]

#),
X

mI (p )

p
p6=k

#2

"
1

XX
p

mn (p )mI (q )

(18)

q
q6=p

X
2 exp(kwkn  yn k2 )[wkn  yn ][
dn (p ) + gn ]
p

mn (k )
=
wkn

p6=k

X
[
dn (p ) + gn ]2

(19)

p

mn (k )
dn (k )
=X
gn
[
dn (p ) + gn ]2

(20)

p

Fig. 2 shows a flow chart of these learning procedures. It has been found that adjusting
the values of gn can be achieved during the first few iterations. By continuing the training
to fine-tune the values of wkn until there is no further improvement on the training set, or
we reach a pre-defined maximum number of epochs1 , the result could be further enhanced.
Note that the weight values are adjusted by each pattern (not batch training). We fix the
value of  = 106 , while  is first initialized to 5  104 , and is then changed according to
the value of MSE, as described in the flow chart.
Although the computational cost involved in implementing our technique is higher than
that of other combination methods2 , we only need to perform training once, which can be
done off-line. Then, with the optimal values of wkn and gn , we can perform the on-line
combination, which is comparable to other combination methods.
On the other hand, as indicated in the beginning of this section, we consider a reference
vector, wkn , for each class. This leads to an increase in training time as the number of classes
and/or classifiers increases. An alternative is to consider only using a reference value for
each class, wkn . This will save more than 50% of training time for the case of several
classifiers and classes. Note that the same learning formulas are applicable by replacing wkn
with wkn and yn with ykn . We will refer to these two alternative approaches as DS1 and DS2,
respectively. In the following section, we will compare DS1 and DS2 with other well-known
combination methods.
1. The maximum number of epochs is set to 50 in all experiments described in this paper
2. Training time of most of the experiments conducted in section 5 required less than 3 minutes on a
conventional PC

341

fiAl-Ani & Deriche

Start
Th
=
Errnew =
Errold =
It_no =
It_nomax=

Randomly initialize
wkn and g n
n =1: N, k =1: K
Initialize learning
rates
-4
-6
=510 , =10

error threshold
current MSE
previous MSE
current no. of iterations
max. no. of iterations

For each pattern

It_no=It_no+1
Errold = Errnew

Compute
mn(k ) and mn()

Adjust
wkn and g n

Compute
z and Errnew
IF
Yes
Errold -Errnew>Th
 = 1.03

No
 = 0.7

Yes

IF
It_no<It_nomax
and -4
 > 10
No
End

Figure 2: Training procedure of the proposed technique

It is worth mentioning that although the training procedures of both the proposed
method and the backpropagation algorithm of ANN are based on minimizing the MSE using
iterative approaches, the proposed method and ANN are not similar. The backpropagation
training operates by passing the weighted sum of its input through an activation function,
usually in a multi-layer architecture known as multi-layer perceptron (MLP). Extracting
rules from a trained MLP is a very challenging problem. On the other hand, the training of
the proposed method operates by measuring a distance between a classification vector and
a reference vector. This distance would later be used to measure the belief of each class
label for all classifiers. The final confidence of each class label is obtained by combining
the beliefs of all classifiers. Unlike MLP, the belief of a given class label for each classifier
indicates its contribution towards the final confidence. The reader may refer to (Denoeux,
2000) for a description of an ANN classifier based on the D-S theory.

342

fiA New Technique for Combining Multiple Classifiers

5. Performance Analysis of Different Combination Methods
The following three classification problems have been considered: texture classification,
classification of speech segments according to their manner of articulation, and speaker
identification. ANNs are used to perform classification for the three problems. For each
case, classifiers will be sorted according to their performance, such that the best classifier
is referred to as c1 , the 2nd best as c2 , and the worst one as cN .
For each problem, we will consider different number of classes, and combine the results of
different number of classifiers, where combining results of the best, the worst and mixtures
of best and worst classifiers will be investigated. For example, if we have five classifiers
and would like to combine two of these, then we will consider combining the best two,
{c1 , c2 }, best one and worst one, {c1 , c5 }, and worst two classifiers, {c4 , c5 }. The following
combination methods were tested: the weighted sum (WS)3 , average (Av), median (Md),
maximum (Mx), majority voting (MV), fuzzy integral (FI) (Cho & Kim, 1995a) 4 , Rogovas
D-S method (DS0) (Rogova, 1994), and our proposed method with its two alternatives (DS1
& DS2). The training set used to train the ANNs will be used to estimate the confusion
matrix for WS and FI, as well as to estimate the evidence of DS0, DS1, and DS2.
Two measures will be used to compare the performance of the different combination
methods, namely: overall performance and error reduction rate (ERR). The overall performance is the mean of classification accuracy obtained by combining all considered subsets
of 2,    , N classifiers. ERR is the percentage of error reduction obtained by combining
classifiers with reference to the best single classifier:
ERR =

ERBSC  ERCC
 100
ERBSC

(21)

where ERBSC is the error rate of the best single classifier and ERCC is the error rate obtained by combining the considered classifiers. Unlike classification accuracy, ERR clearly
shows how the performance of the combined classifiers improves or deteriorates compared
to the best single classifier. In other words, it shows the merit of performing the combination. We will specifically concentrate on the maximum ERR obtained by combining all
the considered subsets of 2,    , N classifiers. In addition, we will also investigate how the
value of ERR gets affected by increasing the number of combined classifiers.
5.1 Texture Classification
Several experiments have been carried out for the classification of texture images. The
textures considered here are: bark, brick, bubbles, leather, raffia, water, weave, wood and
wool (USC, 1981). In order to obtain a better comparison between the different combination
methods, we considered classifying the first two textures, then the first three, the first five
and finally all the nine textures. Additive Gaussian noise, with different signal-to-noise ratio,
has been added to (1024  1024) pixels image of each texture class to form the training and
testing sets. 961 patterns were obtained from each image using (64  64) windows with an
overlap of 32 pixels.
3. The weights of each classifier are determined according to the classification accuracy of each class label
using the training dataset
4. The reader may refer to Appendix A for a brief description of this method

343

fiAl-Ani & Deriche

No. of classes
2
3
5
9

SDH1
86.96
84.58
85.10
80.97

SDH2
85.73
84.52
84.62
77.44

SDH3
84.44
83.91
84.34
77.51

SDH4
85.45
86.24
83.46
75.72

En
91.14
89.72
88.84
83.65

Table 1: Texture classification accuracy of the five original classifiers for different number
of class labels

Four nine-feature vectors were calculated using statistics of sum and difference histogram
(SDH) of the co-occurrence matrix with different directions, vertical (SDH1 ), horizontal
(SDH2 ), and the two diagonals (SDH3 and SDH4 ) . For each direction, the features used
were: mean, variance, energy, correlation, entropy, contrast, homogeneity, cluster shade,
and cluster prominence. The fractal dimension (FD) has also been used to form the tenth
feature of each vector. The energy contents of texture images (En) has been used to form
another feature vector using 9 different masks. Again the tenth feature was FD.
Each of these five feature vectors has been used as input to an ANN. The numbers of
training and testing patterns depend upon number of classes considered, i.e. for the case
of two classes, 15376 patterns were used to train the networks and 5766 to test them. The
results obtained are shown in Table 1. Note that as the number of classes increases the
overall accuracy decreases. In addition, the performance of the En classifiers is found to be
better than that of the other four.
No. of classes
2
3
5
9

WS
89.16
88.52
89.60
84.96

Av
89.04
88.39
89.41
84.55

Md
87.66
87.41
87.99
83.37

Mx
90.12
88.86
89.23
82.90

MV
88.09
87.30
87.83
83.23

FI
90.08
88.71
90.28
86.76

DS0
88.70
88.40
89.52
84.87

DS1
90.66
90.21
92.69
89.83

DS2
90.72
90.08
91.50
86.79

Table 2: Overall performance of the various combination methods for different number of
class labels (texture classification)

The overall performance of the tested combination methods for different number of class
labels are shown in Table 2. For the case of 2 classes, it is clear that the overall performances
of DS1 and DS2 are better than that of the other combination methods. When mixtures of
good and bad classifiers are considered, the performance of combination methods, except
for DS1 and DS2, is closer to or worse than that of the best single classifier. This is shown
in Table 3 for the combination of {c1 , c3 , c4 , c5 }, {c1 , c4 , c5 }, {c1 , c5 }, etc5 . When 3 and 5
classes are considered, DS1 performs slightly better than DS2, and both outperform the
other methods. The gap between DS1 and other methods gets wider when all 9 classes are
considered. The superiority of DS1 reflects the advantage of using the whole output vector
in measuring evidences of classifiers.
5. The reader may refer to Appendix B for detailed results of other cases

344

fiA New Technique for Combining Multiple Classifiers

Classifiers
c1 , c2
c1 , c5
c4 , c5
c1 , c2 , c3
c1 , c2 , c5
c1 , c4 , c5
c3 , c4 , c5
c1 , c2 , c3 , c4
c1 , c2 , c3 , c5
c1 , c2 , c4 , c5
c1 , c3 , c4 , c5
c2 , c3 , c4 , c5
c1 , c2 , c3 , c4 , c5

WS
92.56
91.16
85.07
91.21
91.03
89.80
85.38
89.94
89.70
89.72
88.57
86.07
88.81

Av
92.59
91.12
85.07
91.21
90.81
89.59
85.38
89.70
89.42
89.49
88.45
86.11
88.54

Md
92.59
91.12
85.07
88.92
88.68
86.21
85.47
87.84
87.53
87.37
86.30
85.87
86.63

Mx
92.51
91.09
85.22
91.62
91.48
91.21
85.40
91.47
91.42
91.48
91.09
86.25
91.33

MV
92.51
91.09
85.22
88.92
88.71
86.21
85.48
89.13
89.04
88.94
87.03
86.26
86.59

FI
92.61
91.00
85.07
91.69
91.48
91.24
85.33
91.59
91.29
91.40
90.98
86.13
91.21

DS0
92.40
91.33
85.15
90.81
90.43
88.88
85.22
89.13
88.92
89.00
87.81
85.93
88.10

DS1
92.46
91.62
85.10
92.40
92.47
91.68
85.43
92.21
92.32
92.25
91.78
86.66
92.21

DS2
92.46
91.61
85.12
92.53
92.39
91.78
85.40
92.33
92.42
92.26
91.87
86.80
92.33

Table 3: Classification accuracy of texture images using different combination methods (2
textures)

The best ERR values of WS, FI, DS0, DS1 and DS2 are determined according to Eq.
21. Since WS has been widely used in the literature, and it outperforms other conventional
methods (Av, Md, Mx, and MV), as observed in Table 2, then we will use it as a representative of the conventional methods when performing the comparison with FI, DS0, DS1 and
DS2. Figure 3a shows the ERR values when 2 classes are considered. It is clear that the
maximum ERR values of these five combination methods are very close, ranging between
14% to 16%. They are obtained by combining the best two classifiers for WS, FI and DS0,
while DS1 and DS2 use three classifiers to obtain their maximum ERR. As mentioned
earlier, The performance of the first four individual classifiers is weaker than that of the
En. Notice that, for both DS1 and DS2, there is no significant degradation in ERR as the
number of combined classifiers increases.
For the case of 3 classes, both DS1 and DS2 outperform other combination methods
in terms of the maximum ERR. They achieve values of 17.3% and 19.6% respectively,
compared to 11.4% or less for other methods as shown in Figure 3b. In addition, ERR of
DS1 and DS2 are not affected as the number of combined classifiers increases.
For the case of 5 classes, the maximum ERR values sorted in a descending order are:
DS1 50.7%, DS2 40.2%, FI 31.6%, WS 28.1%, and DS0 23.8%, as shown in Figure 3c. In
addition, ERR values of DS1 improve as the number of combined classifiers increases, DS2
is the second best, while ERR values of other methods degrade as the number of combined
classifiers increases. For the case of 9 classes, the superiority of DS1 becomes clearer, where
as shown in Figure 3d, the maximum ERR value of DS1 is 54% compared to 37.5% or less
for other methods. It is worth mentioning that even though the maximum ERR values of
other methods degrade, they still perform better than the best single classifier. This leads
us to conclude that as the number of classes increases, the performance of most classifier
combination methods gets better overall.

345

fiAl-Ani & Deriche

20

30

10

20

0

10

10

0

20

10

(a) 2 classes

(b) 3 classes

20

WS
FI
DS0
DS1
DS2

ERR

30

50
50
40
40
30
30
20
20
10
10

(c) 5 classes
0

2

2.5

3

3.5

4

4.5

5

(d) 9 classes
2

2.5

3

3.5

4

4.5

5

No. of combined classifiers

Figure 3: ERR of different classifier combination methods obtained by considering different
number of classifiers for the cases: (a) 2 classes, (b) 3 classes, (c) 5 classes, and
(d) 9 classes

Taking all these facts into consideration, we can sort the methods in a descending order
as follows: DS1, DS2, FI, WS, DS0, and the other conventional methods. Thus, in summary,
for the problem of texture classification, our proposed technique with its two alternatives
(DS1 and DS2) clearly outperforms other standard combination methods with an increase
in classification accuracy of about 2  7%. For the cases of 2 and 3 classes, there is a little
difference in performance between DS1 and DS2. This is because using reference vectors of
small size, 2  1 and 3  1, does not make a big impact upon the estimation of evidence
compared to that obtained using a single reference value. As the size of the reference vector
increases, 5  1 and 9  1 for the other two cases, its impact on estimating the evidence
becomes clearer, which leads to better results, but at the cost of increasing computational
load.
5.2 Speech Segment Classification
Six different input feature sets have been used to classify speech segments according to their
manner of articulation, these were: 13 mel-frequency cepstral coefficients (MFC), 16 log
mel-filter bank (MFB), 12 linear predictive cepstral coefficients (LPC), 12 linear predictive

346

fiA New Technique for Combining Multiple Classifiers

No. of classes
3
6
9

MFC
88.21
83.16
78.48

MFB
90.98
85.50
83.24

LPC
81.64
74.77
71.64

LPR
80.69
74.06
70.03

WVT
90.64
84.33
81.33

ARP
70.87
62.90
56.66

Table 4: Speech segment classification accuracy of the six original classifiers for different
number of class labels

reflection coefficients (LPR), 10 wavelet energy bands (WVT), and 12 autoregressive model
parameters (ARP). For this experiment, speech was obtained from the TIMIT database
(MIT, SRI, & TI, 1990). Segments of 152 speakers (56456 segments) were used to train
the ANNs, and 52 speakers (19228 segments) to test them. Three cases were considered:
3 classes (vowel, consonant, and silence), 6 classes (vowel, nasal, fricative, stop, glide, and
silence), and finally 9 classes (vowel, semi-vowel, nasal, fricative, stop, closure, lateral,
rhotic, and silence). The classification results for these three cases are summarized in Table
4.
No. of classes
3
6
9

WS
90.80
85.54
83.05

Av
90.41
84.91
82.31

Md
90.20
84.62
81.93

Mx
86.15
81.16
75.63

MV
89.51
84.03
81.00

FI
90.65
85.29
82.73

DS0
90.90
85.18
82.86

DS1
91.57
87.18
85.20

DS2
91.31
86.37
84.22

Table 5: Overall performance of the various combination methods for different number of
class labels (speech segment classification)

The two best individual classifiers are MFB and WVT in all three cases, followed by
MFC then other methods. Unlike texture classifiers that had one good classifier and four,
relatively, weak classifiers, we have here three good classifiers (MFB, MFC and WVT) and
three weak classifiers (LPC, LPR and ARP).
The overall performance values of the various combination methods are displayed in
Table 5. For the case of the 3 classes, it can be seen that the overall performance of DS1 is
better than that of DS2 and they both outperform the other methods. This becomes even
clearer as the number of classes increases (with more than 2% increase in accuracy).
The ERR values for the case of 3 classes are shown in Figure 4a. The maximum ERR
value of DS1 is 23.4%, which is achieved by combining all six classifiers, compared to 20.3%
for DS2 and 19.6% or less for the other methods. The gap between DS1 and the other
methods gets wider when we consider 6 and 9 classes as shown in Figures 4b and 4c.
Because there are more good classifiers in this experiment compared to that of the texture
experiment, the variations of the ERR values when the number of classifiers increases are
found to be smaller. In addition, we can see that as the number classes increases DS1 keeps
its steady and superior performance in terms of ERR with more than 10% increase.
As a summary, DS1 outperforms other methods in terms of overall performance and
ERR measurements. It is followed by DS2, WS, and the rest of the methods.

347

fiAl-Ani & Deriche

25
25
20
20
15
15
10
10

(b) 6 classes

(a) 3 classes

5
2

3

4

5

6

2

3

4

5

6

25

ERR

20

WS
FI
DS0
DS1
DS2

15

10

(c) 9 classes

5
2

3

4

5

6

No. of combined classifiers

Figure 4: ERR of different classifier combination methods obtained by considering different
number of classifiers for the cases: (a) 3 classes, (b) 6 classes, and (c) 9 classes

5.3 Speaker Identification
Three limited-scope experiments were carried out to perform speaker identification using 2,
3, and 4 speakers. Speech data from the TIMIT database was also used (MIT et al., 1990).
The number of training patterns were 3232, 4481 and 5931 respectively, and the number of
testing patterns were 1358, 1921 and 2542 respectively. The same features used to classify
speech segments according to their manner of articulation were used to identify speakers.
Classification results of the six classifiers are shown in Table 6. The performance of the
individual classifiers are not quite similar to the speech segment problem, where the three
good classifiers are: MFB, MFC and LPC and the three weak classifiers are: LPR, WVT
and ARP.
The overall performance of the various combination methods are shown in Table 7. For
the case of 2 classes, it is clear that the overall performance of most combination methods
is very comparable. The superiority of DS1, and to a lesser degree DS2, becomes clear as
the number of classes increases (more patterns were included to estimate evidence).
Note that, because of the high performance of individual classifiers for the case of 2
classes, a small difference in the performance of combination methods will have great impact
on ERR, which explains the graphs fluctuations, as shown in Figure 5a. It can be seen

348

fiA New Technique for Combining Multiple Classifiers

No. of classes
2
3
4

MFC
94.58
85.84
85.01

MFB
96.17
87.25
85.96

LPC
92.49
82.20
80.84

LPR
89.60
81.00
77.97

WVT
87.80
74.39
70.93

ARP
84.55
73.03
64.59

Table 6: Speaker identification accuracy of the six original classifiers with different number
of speakers
No. of classes
2
3
4

WS
95.53
90.80
83.05

Av
95.50
90.41
82.31

Md
95.26
90.20
81.93

Mx
95.36
86.15
75.63

MV
95.21
89.51
81.00

FI
95.25
90.65
82.73

DS0
95.46
90.90
82.86

DS1
95.48
91.57
85.20

DS2
95.45
91.31
84.22

Table 7: Overall performance of the various combination methods for different number of
class labels (speaker identification)

that both maximum ERR and overall performance of most combination methods are close.
These results do not favor DS1 nor DS2, because they have an additional computational
cost. Lets now consider the case of 3 classes, Figure 5b shows that the maximum ERR
of DS2 is the highest followed by DS1, and they both outperform the other methods. For
the case of 4 classes, the maximum ERR of DS1 is 30%, compared to 27% or less for other
methods, as shown in Figure 5c. The figure also shows that ERR values of DS2 and WS
are close. However, as the overall performance of DS2 is better than that of WS, DS2 can
be considered as the second best method followed by WS, DS0 and finally FI.
The above results clearly show how the performance of DS1 and DS2 get affected by the
number of training patterns, which is crucial in achieving good estimation of the evidence of
each classifier. This is very clear for the case of 2 speakers. Their performance, however, get
better as the number of speakers and training patterns increase. In other words, DS1 and
DS2 require a larger number of patterns to work properly. Failing to provide such number
of patterns, other conventional methods, such as WS, can achieve similar performance.
The experiments of textures, speech segments and speaker classification show that our
proposed technique clearly outperforms the other methods in terms of overall performance
and ERR, providing that a sufficient number of patterns to estimate evidence of classifiers
exists. Also, among the different combination methods, DS1 and DS2 are the least effected
by the inclusion of weak classifiers. The experiments also show that the BBAs could be
better estimated using reference vectors rather than reference values, especially for large
number of classes.
It is worth mentioning that each one of the combination methods has its own merit.
For example, the MV is very useful combination method when dealing with classifiers that
produce results of the abstract level. When working in the measurement level, other combination methods could have better performance.
The Mx method can provide good results when the performance of the combined classifiers are close. In such case, the classifier with higher confidence can provide better results
349

fiAl-Ani & Deriche

40
20
35
15
30
10
25
5
20
0

(a) 2 classes

(b) 3 classes

15

5
2

3

4

5

6

2

3

4

5

6

30

ERR

25

WS
FI
DS0
DS1
DS2

20
15
10

(c) 4 classes

5
2

3

4

5

6

No. of combined classifiers

Figure 5: ERR of different classifier combination methods obtained by considering different
number of classifiers for the cases: (a) 2 classes, (b) 3 classes, and (c) 4 classes

than any individual classifier. This is shown in Tables 11-13 (refer to Appendix B), where
good results are achieved when combining the best two or three classifiers of the speech
segment experiment compared to the best individual classifier. However, if there is a clear
difference in the performance of classifiers, as in the case when considering mixtures of good
and bad classifiers, then using Mx to combine the classification results will not be a good
choice. In case we dont have any information about the performance of the classifiers, i.e.,
there is no training dataset, the Av and Md methods could provide an attractive choice.
Similar to the findings of (Kittler et al., 1998; Alkoot & Kittler, 1999), the performance of
these two methods are found to be close with slight favor of the Av method. If the classification accuracy of the different classifiers are available, then the WS method represents
a good choice, where it outperforms Av in almost all the conducted experiments. This is
expected, as associating each classifier with a weight that reflects its performance, would
make the better classifier contributes more towards the final decision. If the performance
of the combined classifiers are very close, then combining their results using both the Av
and WS methods would lead to very similar performance, as shown in Tables 11-13 for the
cases of combining the best two and three speech segment classifiers.
The FI and DS0 represent two non-linear combination methods. According to (Cho &
Kim, 1995a), the performance of FI was slightly better than the WS when tested using an
350

fiA New Technique for Combining Multiple Classifiers

optical character recognition database, which is similar to the results we obtained for the
texture experiments. However, for speech segment classification and speaker identification
experiments, the performance of FI was not as good as that of WS. On the other hand, the
experiments conducted here show that WS slightly outperforms DS0. Note that Rogova
(1994) only compared DS0 to the original classifiers. The main problem with both FI
and DS0 is the appropriate estimation of their parameters. For example, the desired sum
of fuzzy densities affects the combination results of FI, while the choice of the proximity
measure and reference vector plays an important role in the performance of DS0.
DS1 and DS2 differ from DS0 by the appropriate measure of the reference vectors,
and hence the accurate estimation of the evidence of each classifier. This will exploit
the complementary information provided by the different classifiers. In other words, the
accurate estimation of evidence of each classifier will lead to minimizing the MSE of the
combined results, and hence resolving the conflicts between classifiers.

6. Conclusion
We have developed in this work a new powerful classifier combination technique based
on the D-S theory of evidence. The technique, based on adjusting the evidence of different
classifiers by minimizing the MSE of training data, gave very good results in terms of overall
performance and error reduction rate. To test the algorithm, three experiments were carried
out: texture classification, speech segments classification, and speaker identification. All
of the experiments showed the superiority of the proposed technique when compared to
conventional methods, fuzzy integral, and another D-S implementation that uses a different
measure of evidence. We have shown that accurate estimation of the evidence from different
classifiers based on the whole output vectors (DS1) gives the best performance, especially
for higher number of class labels. The only drawback of the algorithm is that training
can be computationally expensive (this is used to accurately estimate the evidence of each
classifier). However, this can be executed off-line, and as such, has no major effect on the
performance of the algorithm. We have also shown that the proposed algorithm can easily
achieve an increase in classification accuracy of the order of 2% to 7% compared to other
combination methods. We believe that with more work on enhancing the technique, the
scheme can form a new framework for pattern classification in the future.

Acknowledgment
The authors wish to thank Dr. J. Chebil and Dr. M. Mesbah for their valuable comments
on the paper. The authors also acknowledge the support of Queensland University of
Technology for the work presented in this paper. Dr. Deriche acknowledges the support of
King Fahd University, Saudi Arabia, where he is currently on leave.

Appendix A. Classifier Combination Based on the Fuzzy Integral
Fuzzy integral is a non-linear combination method defined with respect to a fuzzy measure.
Detailed explanation of classifier combination based on the g fuzzy measure can be found
in the work of Cho & Kim (1995a, 1995b).

351

fiAl-Ani & Deriche

For a finite set of elements, Z, the g fuzzy measure (Sugeno, 1977) is defined as the set
function g: 2Z  [0, 1] that satisfies the following conditions:
1. g() = 0, g(Z) = 1,
2. g(A)  g(B) if A  B,
3. if {Ai }
i=1 is an increasing sequence of measurable sets, then limi g(Ai ) = g(limi Ai ),
4. g(A  B) = g(A) + g(B) + g(A)g(B)
for all A, B  Z and A  B = , and for some  > 1. Let h : Z  [0, 1] be a fuzzy
subset of Z. The fuzzy integral over Z of the function h with respect to a fuzzy measure g
is defined by



h(z)  g(.) = max min min h(z).g(E)
EZ

=

zE

max [min(, g(F ))],

where

[0,1]

F = {z|h(z)  }
Let Z = {z1 ,    zn }, and suppose that h(z1 )  h(z2 )      h(zn ), (if not, Z is rearranged
so that this relation holds). Then a fuzzy integral e, with respect to a fuzzy measure g over
Z can be computed by
n

e = max[min(h(zi ), g(Ai ))],
i=1

where

Ai = {z1 ,    zi }
g(A1 ) = g({z1 }) = g 1
g(Ai ) = g i + g(Ai1 ) + g i g(Ai1 ), for 1 < i  n
Q
 is given by solving:  + 1 = ni=1 (1 + g i ), where   (1, ) and  6= 0. This can be
calculated by solving an (n  1)st degree polynomial and finding the unique root greater
than 1.
For the problem of combining classifiers, Z represents the set of classifiers, A the object
under consideration for classification, and hk (zi ) is the partial evaluation of the object A for
class k . Corresponding to each classifier zi , the degree of importance, g i , that reflects how
good is zi in the classification of class k must be given. These densities can be induced
from a training dataset.

352

fiA New Technique for Combining Multiple Classifiers

Appendix B. Tables of Classification Accuracy for Different Combination
Methods
Classifiers
c1 , c2
c1 , c5
c4 , c5
c1 , c2 , c3
c1 , c2 , c5
c1 , c4 , c5
c3 , c4 , c5
c1 , c2 , c3 , c4
c1 , c2 , c3 , c5
c1 , c2 , c4 , c5
c1 , c3 , c4 , c5
c2 , c3 , c4 , c5
c1 , c2 , c3 , c4 , c5

WS
90.89
89.69
85.05
89.92
89.55
89.05
85.76
89.29
89.07
88.87
88.60
86.45
88.54

Av
90.79
89.57
85.05
89.83
89.39
88.80
85.73
89.07
88.83
88.78
88.38
86.44
88.47

Md
90.79
89.57
85.05
88.04
87.54
86.95
85.49
87.98
87.24
87.34
87.05
86.18
87.06

Mx
90.83
89.68
84.67
90.48
90.23
89.62
84.81
90.17
90.01
90.02
89.39
85.52
89.73

MV
90.17
89.10
84.81
87.77
87.09
86.62
85.49
87.91
87.50
87.61
87.52
86.31
87.03

FI
90.22
88.99
85.13
89.88
89.56
89.26
85.67
89.85
89.58
89.64
89.44
86.36
89.65

DS0
90.81
89.92
85.51
89.59
89.18
88.80
85.91
88.82
88.81
88.66
88.39
86.46
88.29

DS1
91.09
90.45
85.49
91.26
91.12
90.77
87.27
91.44
91.48
91.16
91.31
88.37
91.50

DS2
90.81
90.29
85.44
91.73
91.10
90.73
86.88
91.51
91.53
91.26
91.00
87.20
91.61

Table 8: Classification accuracy of texture images using different combination methods (3
textures)

Classifiers
c1 , c2
c1 , c5
c4 , c5
c1 , c2 , c3
c1 , c2 , c5
c1 , c4 , c5
c3 , c4 , c5
c1 , c2 , c3 , c4
c1 , c2 , c3 , c5
c1 , c2 , c4 , c5
c1 , c3 , c4 , c5
c2 , c3 , c5 , c6
c1 , c2 , c3 , c4 , c5

WS
91.98
91.95
85.14
91.49
90.77
90.61
86.29
90.26
90.14
89.73
90.32
86.41
89.65

Av
91.87
91.74
85.19
91.29
90.50
90.35
86.22
90.03
89.91
89.43
90.06
86.38
89.39

Md
91.87
91.74
85.19
88.78
87.92
87.67
85.40
88.37
88.17
87.62
87.91
85.99
87.27

Mx
90.28
90.72
84.77
90.39
90.34
90.29
85.99
90.12
90.25
90.19
90.50
85.98
90.11

MV
89.89
89.58
84.50
88.60
87.74
87.43
85.64
88.90
89.00
88.45
88.51
85.79
87.81

FI
92.21
91.30
84.94
92.35
91.54
91.41
85.91
91.86
91.83
91.11
91.83
86.15
91.22

DS0
91.45
91.55
85.82
91.08
90.32
90.25
86.98
89.93
90.09
89.41
90.20
87.21
89.51

DS1
93.40
93.27
86.28
94.37
93.66
93.69
89.16
94.48
94.41
93.82
94.50
89.47
94.46

DS2
92.81
92.72
85.36
93.19
92.96
92.90
87.29
93.19
93.26
92.56
93.33
86.97
93.01

Table 9: Classification accuracy of texture images using different combination methods (5
textures)

353

fiAl-Ani & Deriche

Classifiers
c1 , c2
c1 , c5
c4 , c5
c1 , c2 , c3
c1 , c2 , c5
c1 , c4 , c5
c3 , c4 , c5
c1 , c2 , c3 , c4
c1 , c2 , c3 , c5
c1 , c2 , c4 , c5
c1 , c3 , c4 , c5
c2 , c3 , c4 , c5
c1 , c2 , c3 , c4 , c5

WS
88.45
86.39
79.55
87.10
86.48
85.92
80.27
86.42
85.70
85.79
85.42
81.60
85.33

Av
88.13
85.97
79.40
86.53
85.85
85.32
80.21
85.95
85.11
85.39
84.92
81.44
84.88

Md
88.13
85.97
79.40
84.20
83.66
83.22
79.63
84.56
83.37
84.29
83.06
81.15
83.19

Mx
85.44
82.27
78.74
84.65
84.08
82.97
79.54
84.33
83.91
83.83
83.47
80.75
83.74

MV
85.91
83.00
78.53
84.60
84.17
83.67
79.38
85.16
84.26
84.91
84.08
81.03
83.35

FI
89.78
88.18
79.37
89.51
88.72
88.25
79.97
89.18
88.52
88.52
88.34
81.54
88.02

DS0
89.26
87.10
79.28
87.06
86.92
85.98
79.80
85.96
85.41
85.94
84.81
81.02
84.83

DS1
91.42
90.07
81.22
92.00
92.13
92.01
82.48
92.45
92.02
92.48
92.35
84.70
92.43

DS2
89.20
88.08
79.91
88.59
88.53
88.30
80.89
88.70
88.30
88.66
88.35
82.17
88.65

Table 10: Classification accuracy of texture images using different combination methods (9
textures)

Classifiers
c1 , c2
c1 , c6
c5 , c6
c1 , c2 , c3
c1 , c2 , c6
c1 , c5 , c6
c4 , c5 , c6
c1 , c2 , c3 , c4
c1 , c2 , c3 , c6
c1 , c2 , c5 , c6
c1 , c4 , c5 , c6
c3 , c4 , c5 , c6
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c5 ,
c1 , c2 , c4 , c5 ,
c1 , c3 , c4 , c5 ,
c2 , c3 , c4 , c5 ,
c1 , c2 , c3 , c4 ,

c5
c6
c6
c6
c6
c6
c5 , c6

WS
92.34
89.99
81.09
92.63
92.17
89.85
84.98
92.59
92.62
92.25
90.15
88.79
92.75
92.57
92.73
92.14
91.41
91.51
92.62

Av
92.34
88.11
80.65
92.61
91.79
88.79
84.96
92.57
92.47
91.93
89.51
88.42
92.64
92.48
92.50
91.70
91.04
90.98
92.36

Md
92.34
88.11
80.65
92.37
91.83
88.20
84.76
92.42
92.50
91.82
89.34
88.30
92.23
92.30
92.20
91.07
90.63
90.52
92.24

Mx
92.34
83.95
76.19
92.34
85.97
84.17
79.36
91.64
86.94
86.01
84.64
83.25
91.42
86.96
86.93
86.19
85.79
85.73
86.95

MV
92.22
82.64
76.10
92.27
91.60
87.50
84.08
92.04
92.02
91.96
89.84
88.46
92.13
91.92
91.91
90.97
90.41
90.60
91.97

FI
91.84
91.06
82.03
92.36
92.03
89.66
84.89
92.38
92.49
92.14
89.48
88.46
92.49
92.25
92.53
91.71
90.98
91.15
92.38

DS0
92.38
90.92
81.54
92.63
92.27
90.34
84.62
92.61
92.73
92.37
90.12
88.76
92.74
92.56
92.76
92.23
91.46
91.50
92.61

DS1
92.45
91.48
82.29
92.79
92.53
91.92
85.65
92.77
92.78
92.77
91.92
90.51
93.07
92.77
93.03
92.78
92.48
92.67
93.09

Table 11: Classification accuracy of speech segments using different combination methods
(3 classes)

354

DS2
92.29
91.42
82.01
92.67
92.23
91.79
85.36
92.64
92.54
92.49
91.84
89.94
92.81
92.56
92.72
92.46
92.14
92.40
92.64

fiA New Technique for Combining Multiple Classifiers

Classifiers
c1 , c2
c1 , c6
c5 , c6
c1 , c2 , c3
c1 , c2 , c6
c1 , c5 , c6
c4 , c5 , c6
c1 , c2 , c3 , c4
c1 , c2 , c3 , c6
c1 , c2 , c5 , c6
c1 , c4 , c5 , c6
c3 , c4 , c5 , c6
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c5 ,
c1 , c2 , c4 , c5 ,
c1 , c3 , c4 , c5 ,
c2 , c3 , c4 , c5 ,
c1 , c2 , c3 , c4 ,

c5
c6
c6
c6
c6
c6
c5 , c6

WS
86.97
84.30
74.60
88.09
86.48
84.33
79.27
88.03
87.64
86.51
84.61
83.84
87.90
87.57
87.69
86.71
86.36
86.70
87.67

Av
86.95
81.98
73.91
88.08
85.76
82.80
78.73
87.91
87.17
85.84
83.64
83.12
87.87
87.24
87.17
86.00
85.85
85.95
87.33

Md
86.95
81.98
73.91
87.77
86.01
81.88
77.84
87.81
87.26
85.90
83.28
82.84
87.44
86.98
86.91
85.54
85.26
85.18
87.02

Mx
86.46
77.97
70.38
87.11
80.38
78.64
74.07
86.58
82.59
80.54
79.47
79.28
86.01
82.76
82.64
81.21
81.50
81.63
82.76

MV
86.30
76.66
70.20
87.76
85.31
81.24
77.26
87.29
86.95
86.10
83.85
83.05
87.35
86.85
86.96
85.63
85.57
85.23
86.95

FI
86.67
85.01
75.00
87.51
86.49
83.88
78.46
87.67
87.37
86.69
83.97
83.22
87.72
87.36
87.52
86.46
86.10
86.06
87.35

DS0
86.64
84.52
74.20
87.49
86.45
84.56
78.51
87.48
87.25
86.59
84.29
83.56
87.34
87.08
87.32
86.39
85.95
85.81
86.98

DS1
88.12
86.71
76.47
88.73
88.19
87.16
80.17
88.98
88.61
88.47
87.38
86.00
89.15
88.95
88.93
88.64
88.26
88.37
89.14

Table 12: Classification accuracy of speech segments using different combination methods
(6 classes)

355

DS2
87.59
86.39
75.68
88.10
87.50
86.45
79.39
88.09
88.08
87.57
86.68
84.95
88.15
88.11
88.12
87.56
87.38
87.28
88.01

fiAl-Ani & Deriche

Classifiers
c1 , c2
c1 , c6
c5 , c6
c1 , c2 , c3
c1 , c2 , c6
c1 , c5 , c6
c4 , c5 , c6
c1 , c2 , c3 , c4
c1 , c2 , c3 , c6
c1 , c2 , c5 , c6
c1 , c4 , c5 , c6
c3 , c4 , c5 , c6
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c5 ,
c1 , c2 , c4 , c5 ,
c1 , c3 , c4 , c5 ,
c2 , c3 , c4 , c5 ,
c1 , c2 , c3 , c4 ,

c5
c6
c6
c6
c6
c6
c5 , c6

WS
84.58
81.85
70.48
85.41
84.20
82.18
75.94
85.44
85.34
84.48
82.41
81.02
85.52
85.53
85.54
84.60
83.97
84.14
85.32

Av
84.61
78.59
69.04
85.42
83.44
80.26
75.23
85.30
84.97
83.77
81.44
80.51
85.43
85.01
85.16
84.03
83.32
83.48
84.93

Md
84.61
78.59
69.04
85.00
83.53
79.19
74.43
85.20
84.99
83.77
81.03
80.15
84.94
84.41
84.76
83.34
82.74
82.32
84.70

Mx
83.77
71.48
62.87
83.81
74.18
72.32
67.23
83.41
76.49
74.61
73.61
72.74
82.93
77.04
76.62
75.51
75.80
75.40
77.08

MV
83.66
70.50
62.65
85.02
82.70
77.86
73.46
84.96
84.82
83.98
81.49
79.83
84.77
84.53
84.66
83.69
83.16
82.66
84.51

FI
84.06
82.53
71.22
85.00
84.11
81.20
75.30
85.11
85.19
84.58
81.61
80.06
85.45
84.96
85.38
84.17
83.67
83.29
85.07

DS0
84.15
82.51
69.97
85.34
84.01
82.53
75.54
85.15
85.14
84.53
82.44
80.41
85.42
85.00
85.25
84.42
83.91
83.44
85.19

DS1
86.11
84.41
73.24
86.85
86.05
85.10
78.21
87.05
86.79
86.67
85.65
83.86
87.32
87.09
87.11
86.82
86.59
86.55
87.26

Table 13: Classification accuracy of speech segments using different combination methods
(9 classes)

356

DS2
85.60
83.72
71.94
86.15
85.44
84.56
76.92
86.13
86.16
85.86
84.76
82.25
86.32
86.09
86.22
85.78
85.45
85.56
85.23

fiA New Technique for Combining Multiple Classifiers

Classifiers
c1 , c2
c1 , c6
c5 , c6
c1 , c2 , c3
c1 , c2 , c6
c1 , c5 , c6
c4 , c5 , c6
c1 , c2 , c3 , c4
c1 , c2 , c3 , c6
c1 , c2 , c5 , c6
c1 , c4 , c5 , c6
c3 , c4 , c5 , c6
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c5 ,
c1 , c2 , c4 , c5 ,
c1 , c3 , c4 , c5 ,
c2 , c3 , c4 , c5 ,
c1 , c2 , c3 , c4 ,

c5
c6
c6
c6
c6
c6
c5 , c6

WS
96.10
95.38
90.25
96.68
95.81
94.66
92.20
96.90
97.04
95.88
95.88
95.02
96.82
96.10
96.53
95.74
96.39
95.16
96.53

Av
96.10
94.95
90.69
96.68
95.74
94.73
92.13
96.90
96.82
95.88
95.81
94.95
96.75
96.10
96.46
95.67
96.32
95.23
96.61

Md
96.10
94.95
90.69
96.90
95.23
94.37
91.77
96.97
96.97
95.74
95.52
94.30
96.25
95.45
96.17
95.38
95.60
95.02
96.53

Mx
96.10
94.95
90.83
96.97
95.38
95.09
92.64
96.68
96.10
95.16
95.52
94.01
96.53
96.32
95.88
95.60
96.39
95.45
96.17

MV
96.10
94.95
90.83
96.90
95.23
94.51
91.70
96.61
96.53
95.96
95.45
94.30
96.25
95.45
96.10
95.31
95.52
94.95
96.25

FI
95.16
96.25
87.51
96.61
96.17
94.95
91.48
96.61
96.97
96.17
94.95
94.30
96.82
96.25
96.75
95.81
95.96
95.16
95.96

DS0
96.02
95.36
89.32
96.98
95.73
94.62
91.75
97.05
96.91
95.95
95.95
95.14
96.69
96.02
96.47
95.88
96.24
95.14
96.54

DS1
95.51
96.17
88.59
96.91
95.51
96.24
91.16
96.69
96.76
95.80
95.88
94.55
96.76
96.24
96.76
96.02
96.39
95.58
96.54

Table 14: Speaker identification accuracy using different combination methods (2 speakers)

357

DS2
95.80
96.17
88.66
96.54
95.88
96.17
91.16
96.69
96.69
95.80
96.24
94.40
96.61
96.10
96.54
95.95
96.24
95.58
96.39

fiAl-Ani & Deriche

Classifiers
c1 , c2
c1 , c6
c5 , c6
c1 , c2 , c3
c1 , c2 , c6
c1 , c5 , c6
c4 , c5 , c6
c1 , c2 , c3 , c4
c1 , c2 , c3 , c6
c1 , c2 , c5 , c6
c1 , c4 , c5 , c6
c3 , c4 , c5 , c6
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c5 ,
c1 , c2 , c4 , c5 ,
c1 , c3 , c4 , c5 ,
c2 , c3 , c4 , c5 ,
c1 , c2 , c3 , c4 ,

c5
c6
c6
c6
c6
c6
c5 , c6

WS
89.22
88.91
80.37
90.53
90.37
88.70
84.54
91.10
91.25
90.63
90.37
86.88
91.20
91.36
90.99
91.41
91.05
90.37
91.51

Av
89.17
88.55
80.90
90.47
90.58
88.60
84.17
91.10
91.51
90.47
90.32
86.78
91.20
91.31
91.15
91.67
90.99
90.27
91.36

Md
89.17
88.55
80.90
90.32
89.90
87.35
83.60
90.58
91.36
90.47
89.48
85.94
90.58
91.25
90.58
90.99
89.69
89.22
91.78

Mx
88.70
87.61
79.44
89.28
88.65
87.25
83.24
89.59
88.70
88.81
87.87
84.64
89.59
88.96
88.81
88.86
88.34
88.24
88.91

MV
88.91
85.58
77.93
90.06
90.06
86.93
83.34
90.32
90.06
88.39
88.55
84.54
89.59
90.32
90.06
90.47
89.59
88.81
90.89

FI
87.82
89.38
75.90
89.80
89.43
88.29
83.08
90.94
90.89
90.16
89.54
85.79
90.84
90.89
90.73
90.89
89.85
88.96
91.46

DS0
89.22
88.86
79.44
90.47
90.58
88.96
83.86
91.31
91.62
90.58
90.47
86.15
91.72
91.36
91.20
91.46
91.04
89.38
91.46

DS1
89.33
89.90
80.37
90.58
90.32
90.11
84.64
91.78
91.62
90.94
91.57
87.40
91.93
91.93
91.62
91.83
91.83
90.42
91.93

Table 15: Speaker identification accuracy using different combination methods (3 speakers)

358

DS2
89.17
89.95
79.85
91.20
90.21
90.58
85.06
91.51
92.04
90.21
91.46
87.35
91.78
92.40
91.88
92.09
91.41
90.32
91.98

fiA New Technique for Combining Multiple Classifiers

Classifiers
c1 , c2
c1 , c6
c5 , c6
c1 , c2 , c3
c1 , c2 , c6
c1 , c5 , c6
c4 , c5 , c6
c1 , c2 , c3 , c4
c1 , c2 , c3 , c6
c1 , c2 , c5 , c6
c1 , c4 , c5 , c6
c3 , c4 , c5 , c6
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c4 ,
c1 , c2 , c3 , c5 ,
c1 , c2 , c4 , c5 ,
c1 , c3 , c4 , c5 ,
c2 , c3 , c4 , c5 ,
c1 , c2 , c3 , c4 ,

c5
c6
c6
c6
c6
c6
c5 , c6

WS
87.45
84.78
74.00
89.18
87.96
85.60
80.84
89.77
89.26
87.84
87.06
84.74
89.61
89.77
89.26
89.10
88.00
88.99
89.42

Av
87.53
83.79
74.47
89.18
88.08
85.41
80.68
89.85
89.22
87.33
86.74
84.38
89.61
89.73
89.14
89.06
87.88
88.87
89.26

Md
87.53
83.79
74.47
88.24
87.53
83.32
79.58
89.65
88.71
86.90
86.23
84.30
89.02
88.83
88.36
88.20
86.98
87.21
88.91

Mx
87.29
83.40
72.03
88.16
86.35
83.52
77.18
88.04
87.29
86.35
83.99
80.72
88.00
87.45
87.29
86.74
85.21
85.80
87.37

MV
86.98
81.55
71.28
87.92
87.33
83.12
78.76
88.59
88.20
87.10
85.44
83.01
88.47
88.87
88.04
88.36
87.45
87.06
88.24

FI
86.55
85.48
69.83
88.32
87.25
84.66
80.29
89.65
88.75
87.37
86.82
84.07
89.18
89.06
88.55
88.36
87.92
88.00
89.22

DS0
87.69
83.87
71.99
89.10
87.65
84.62
78.80
89.61
88.91
87.65
86.66
83.83
89.61
89.61
89.18
88.95
87.88
87.92
89.54

DS1
87.73
85.37
73.29
89.38
88.08
86.15
81.51
90.17
89.50
88.32
88.16
85.68
90.01
89.93
89.73
89.65
89.38
89.26
89.89

Table 16: Speaker identification accuracy using different combination methods (4 speakers)

359

DS2
87.41
84.78
73.13
88.87
87.41
85.13
80.68
89.61
88.71
87.69
87.14
84.66
89.73
89.50
88.59
88.95
89.02
88.75
89.73

fiAl-Ani & Deriche

References
Aha, D. (1995). Machine learning. Tutorial presented at The 1995 Artificial Intelligence
and Statistics Workshop.
Alkoot, F., & Kittler, J. (1999). Experimental evaluation of expert fusion strategies. Pattern
Recognition Letters, 20, 13611369.
Chen, K., & Chi, H. (1998). A method of combining multiple classifiers through soft competition on different feature sets. Neurocomputing, 20, 227252.
Cho, S., & Kim, J. (1995a). Combining multiple neural networks by fuzzy integral for robust
classification. IEEE Transactions on Systems, Man and Cybernetics, 25, 380384.
Cho, S., & Kim, J. (1995b). Multiple networks fusion using fuzzy logic. IEEE Transactions
on Neural Networks, 6, 497501.
Denoeux, T. (2000). A neural network classifier based on DempsterShafer theory. IEEE
Transactions on Systems, Man and Cybernetics, 30, 131150.
Dietterich, T. (1999). An experimental comparison of three methods for constructing ensembles of decision trees: Bagging boosting and randomization. Machine Learning,
40, 139158.
Hansen, L., & Salamon, P. (1990). Neural network ensembles. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 12, 9931001.
Hashem, S., & Schmeiser, B. (1995). Improving model accuracy using optimal linear combinations of trained neural networks. IEEE Transactions on Neural Networks, 6,
792794.
Ho, T., Hull, J., & Srihari, S. (1994). Decision combination in multiple classifier system.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 16, 6675.
Kittler, J., Hatef, M., Duin, R., & Matas, J. (1998). On combining classifiers. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 20, 226239.
Kohlas, J., & Monney, P. (1995). A mathematical theory of hints. An approach to the
Dempster-Shafer theory of evidence. Berlin: SpringerVerlag.
Lam, L., & Suen, C. (1995). Optimal combinations of pattern classifiers. Pattern Recognition
Letters, 16, 945954.
Liu, W., & Bundy, A. (1992). The combination of different pieces of evidence using incidence
calculus. Tech. rep. RP 599, Dept. of Artificial Intelligence, Univ. of Edinburgh.
Mandler, E., & Schurmann, J. (1988). Combining the classification results of independent
classifiers based on the dempstershafer theory of evidence. In Gelsema, E., & Kanal,
L. (Eds.), Pattern recognition and artificial intelligence, pp. 381393. North-Holland.
MIT, SRI, & TI (1990). DARPA TIMIT acoustic-phonetic continuous speech corpus..
http://www.ldc.upenn.edu/doc/TIMIT.html.
Rogova, G. (1994). Combining the results of several neural network classifiers. Neural
Networks, 7, 777781.
Shafer, G. (1976). A mathematical theory of evidence. Princeton University Press.
360

fiA New Technique for Combining Multiple Classifiers

Smets, P. (1990). The combination of evidence in the transferable belief model. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 12, 447458.
Smets, P. (1998). The transferable belief model for quantified belief representation. In
Gabbay, D., & Smets, P. (Eds.), Handbook of defeasible reasoning and uncertainty,
pp. 267301. Kluwer.
Smets, P. (2000). Data fusion in transferable belief model. In 3rd Intl. Conf. Information
Fusion, pp. 2133.
Sugeno, M. (1977). Fuzzy measures and fuzzy integrals: a survey. In Gupta, M., Saridis, G.,
& Gaines, B. (Eds.), Fuzzy automata and decision processes, pp. 89102. Amsterdam:
North-Holland.
USC (1981). USC-SIPI image database.. http://sipi.usc.edu/services/database/.
Xu, L., Krzyzak, A., & Suen, C. (1992). Methods of combining multiple classifiers and their
applications to handwriting recognition. IEEE Transactions on Systems, Man and
Cybernetics, 22, 418435.

361

fi	ff
fiff 	 ff

 
  !ff#"%$'&)(+*,,*-/.,0$213.*&

45678 9;:<ff,*0=?>6
  "9%$!*<ff,*

@BADCFE)GHIJLKNMONE)OQPRKNSDT%GVUWKNCYXLMZ[8U]\WE)G_^F`
a

EbHVcDUdONUWeU%Tf^hgfiDOkjlCNCNe5M[8gmHVMUWi

npoqr/q/sutvq/wyx)z{or/z{o
]25m]'  	0fi
      8 5  0
 ff  D
d 5 '0]_' 	  

|/}~~/ ~/]{}/}/W} }

qqmqoq 
  m   	D      	
 
   	 { 		   

~/|~ ~}~W 5
	 d

ymV{
b+8?{5)'0!5ff)?N?'ffd'{5!!#+0D!'D+'	5'bf? N'5
?';!'+'	5'W'0'5'm ' R?!'d0'' ' 5fi+{5!3'0fi
5'+'+{0?!0?ffff? ' 'D_! v	5?ff?'' 50 ' ?)' D?  
 +ffd+?? fi50 ff? Vb]?'5 {!ff?b'5?'m'{!0!'d+'	5'
'0'%?''R'0' ?]ff	'+'+5?  v!  {5?%'{5!' 0f?]!5 0'0!'f?
!'Ffi+!?  0	' ff	'+'+5?  F!  {5?'{5!' 0?'!0'N5'+ffF'd0'' 
' 8?_ 0D!'5    ;'fi+!?  0	' D!  8!'R'{5!' 0b R';3'
?+'' ' 8?_'ffi%{5ff8?V? 08 D'%!'5b%?'ffR+8   'ff	'0
fi''0! ffbff? 03)W0+5?' 5?_0?' '0' R?_	' 58'+!d'5
VN 8]]{ 
f3/fi_	D??v	ff
fiW3	%5	_ff
$	3%#&f#3%

_	'&(F#3)mFfiF*	?+ff

$	3%#/)%fiD+%3+	#&.

!"#ff

-,-


/+*	?+0#,1 1#,fiD23fiD%$#	
?_ff;+<
 
_	'=f*	?+"68>7'
? +ff-,A@ffBBC5DFE_ffG,H@ffBBI*,KJ%LL@9M14NO P

 Q* 5#	 #B
 3d3
 !

N!R# $SRR?T]U_	WVX
 ff 
 
 &3
 	1;{
Y _ff	 Z	 #,	m
 
*R=ffm/fi5
N!5#[	ff##<f
 M ]\5
 3 #'<
 ff  ff
 53+ ;_
 ?
 ?fiM5 
#5#
4,M/!)ffff##
 5# 
 
 $
 5% 
 
U N

 	 (m
 ?#
 * !,* ^! ff #S
+%3+	##Z	*)_	
 ?F
 ff ff ffv
 
 /!{
, F
 ff_
 
 
 ]
 3
 3 #?[ % #

N!R-1`_T RV'	 
 #
 37( F
 $
 ff\5
 3 #'<N

 ?
 v3d
 ff m
 ffG
1 f
$ffS
N!R#N
 F_
 	ff
 fi	3 ff;+P ^N

 ff	 
 	 ff
a-! ff R
 S? 5#/
 
/%ff*)
 
 /A%
 # p#
 v53+b]: Wfi 	
 ! 	 c
 
 fip
 ?fi^3?]6XdAeTf79M1 $
fd/fi_	^
 
 
 #?[ % # 3%^:#%
 !1(_T#
 5# 

 (:
 W
 
 `! ff #y
 ff ff #-,
%5  fi; /

 y
  ffff* ^g R
U _	WVX
 ffy
 
 =3
 	=N

 ! R#
, fi
?
 5#_
 ffp={
Y ff #`J51){
Y ff #=D
h ?ff* !gP
 
 fi
 ?fi^3?53+b"
 
 `! ff #
	 ff
4,(
 <3 	ff /% ff*v
 53+bi
 
 /!,) ?p+ % 3+ 	 25 fi5 	5#?[ % #-,T
 3
 3 #?[ jV
#-,kN

 ff	 )
 * 	! % #
 !fi 
 % #-
, !F
 33!
 #`Q N
U _	!0  


ff34
/+/5m+%3+	7683+5#	#:925fi5	53+	;%#

lnmokp qnrMsXt uXvjw(xyZrMsRxGzj{jqMsXrnzj|F}~OuX~WOuXt rnFM%}xG~Wp ~WOuXt rnFv%+~{~W~WKt w(|jp ~Ww(~W!uX~+}kt uXvj~rnp t j~Ku'kTt jt jqff~OsX~Os
 MwH{~OsXqn~OsFff wHzjn5Mnffl'R|jzj{jp t Wp P+nMt p M{jp ~(uK!n	8M5Gn	!7vjt 'v+MP{~HuX~Wx8uX~+}7t }rnw)Mt jx
t uXv;zj|PuXr7nM2~OjMw(|jp ~WxWm^ow(rMsX~(xGrn|jvjt x8uXt +uX~+}t w(|jp ~Ww(~W!u'uXt rnrMy$uXvj~)Mp qnrMsXt uXvjw(xFt xgjrMug+nMt p M{jp ~(yZrMs
|jzj{jp t zjxG~nm


*,,*		9	' fff9A;ffHVff0 7)ffg{6
  "'
 
 "/9j

fi~}~ 

~/]{}/}



v3	R?!1<Y{ff#
?R RV'	 #37(F$ff
M \5
 3 #'`N

 ?
 :  
 &3	!yU_	5
Nff*
N/fi53+`%#y
ff ff
 ff`
 +WVX/
 ff`\5
 3 #'N

 ?
 1 b
 #,{
Y ff # R
 ?)3 
b H+S dff3 % ff4H
 b1


	

b )
B!fi

fiff

 ]]{ 

  

 /{


f3mff
 #4? 5#_
 ?F 2'HS

 v
 + 	/Hk T
 	5V
 
 
 3
 	
 R ?!)nV
#DNffff#*	?+/!1f?A'Hd+	/!,mffA%A?5#

 #F
 R ?fN

 d # 
U _	f
 *#
 ffN

 ff*!1

!#"$&% z $ (q '*),+-/.1032)z{o5476	t98;:z{=<38xr/o>+?82 @n :A'  +{z{o5B

DC

f;+b&mU_	WVXffy<3	?ff=32Hb& _	53#*#3


33	+b4ffp`Y{ff#@dp$ff5Q68>7'? +ff-,@ffBBC5DE_ffG,
@ffBBI9M1FNW
 + ?;
 W
 ;  

 N

 ?
 A:
 )+

 % ff
 
 D?fiM5 Nff ff #-,
	c
 7
 
 3A+
k 
U _	f
 p
 _	:
 M

  /
U #	 p _ff #
 R?fiM5 38
 ;
 
 
5
 =7  

 W
 + #
 !1 W
_ v
 ff\5
 ff ,^ &ff	 3# 7%
 5  
 
 /S%
 )_
 
ff ff
 ff3+/: M
  2
 V
 + #v
 3) !:A+S 
U _	!]
1 f2 +b d
 
 33
 	
#
 5# 
 3+
 ff
 %	 H (! ff #//
 M ?? 5#/
 f
 $ ff* 3#7
  

 
 
 
 /!,j%
 5 
fi^5
 fi5 	5#	[ ff]5; f _	' %F !N
 fi^
 ffg:
 H

 *; fi
 !H!3 g	 ?T6 /% ff* 
%#  
 fiB
 ?fi3?8
, < 
U N

 	 "

 
 ff Q 37H
 b
9  
 ffP: 
G,-
 P
/
 +cO,kRVX fi !P!3 7	 ?"#6 ?# *y
  ff n9MN
1 fi
 !P!3 c	 ?!ff *+
N
 
 fi7	  ff
A%
  cRVX fi
 !^!3 )	 ?%
 ff#F
 !ffF*+
 

 
 fi7	  ff
n1
fN/fi
 !3 fi
U _	WVX
 ff
 
 Q3
 	& +
b ?ff& 3cH
 y
b N

 ?
&3 %$ 5% # P  
U _	v
   <%  
 #
 +v
  	_?% ff# M
 B
  
 
 
3
 	p
 # 5# 
 %
 # p/
 kR
 M ?(? 5#/
 R
 $
 	3 % #v
 
 
 /A%
 5 -

9EGF5HIHKJMLON

QP

OC

SR

TNULGFVWHKJ>EGXUNUXAYVGE
Z7[M\ E*VHKJ>EGXUNUXAYVGE

S^"	ff*#"3fi7	5%,

]
]

ffR

S^"	ff*#43fiPM

ffN$##P/3)+;fiMTfi!c!3T	;	5%`6#ffNF

>_#3$##5%#59

$##

S^"	ff*#"#	5c:^!ff]
/+)7fi!%$	3%#-,
] 8Zj	U_	!0SR ff#g
N??!	%/3#'k	5+%/3#',
N	3	#'
]

fi

#%/3#'1

NW ?5  # 	5% #-,A ` +b 2  
 
  3
 	 #
 5# h

 3S+ 
 ?+"
 `

/R$ff*3#4
+#!1cY{ff#&J51 J?5#_?%Fff53+d?fiM5#5#
 YRf,
%5  	  _
 =
 ff +
 + M"

 * M ?S %F
 fi=
  

 )%
 #  ?_ff"+  
U _	
ff ff
 ffiff
 	53 #?[ % # /fi5N

 ! 	ff1 
Y  

 *T p ff M ?	 i_
 
 	 

 3 fiff,
$ y= 	M

 2g ?c 	5% F&Nff ff ff<?%
 ?!,-  2Y
Y #
 5# 
 ? 5#_
 ff 
Y ff #PJ51 h(	 7_
{
 F
 ff2+%
 ff ff
 f

 5
P_
 	-R3+ _
 M ?$ %
 fiF _	ff7+) 8
U _	
$
 ff* 3#  

 %
 + #
 !1 _T# 	M
 % #
 ff#
, 
 
  3
 	 	  _
 N

 	 ffN

 ff* ff
%#  `. m
 ?#
 * ff-0 	5
 # 5# 
 f <Y
, 3T S	 = D		3 !#p
 fffi 3 %	 ;
f % 

 "{
Y 	 	S6 
P_
 	 	
YR
%,-J%LL@9M,-%
 5 3
 ff
 	5% ?c
 `+" 	P_
 ?+W
 
 
 /f
!
 	p
 # 	5% #-1

a

`

c

edgf

ba

h*i>j

MC

fi } 
	

fiff 	

|/|

!  $&% z 3< 8xr/o>+?82 @n :A'  +{z{o5Br+/oM: 4 %

 	 

/}





30 {}

w

	

f(
 	$ %S3	N#5#
 YRf,3ffN b#7@%,3K+?fiM5]:)M?
%2
jU
#[	
  ,	%	$#&%lfiM$##?!,'$% fiP#3$##?!,)(3%
.- /+
/0 "! n14W
e #
 Q\5
 3 #' M ?7
 	;

 *& fi!!3dU
N	?dB3#
* ,+
5
P_
 	HkRVX fi !;
 
U N

 	 ?!8
1 f25
P_
 	g+ 	5% ff4RVX fi
 !(!3 A	 ?!
, ff3 % #
 T+
75
P_
 	( 	ff`fi !^!3 )	 ?!/
, 3(! 	M

 
 ff5F
 /fi5N

 ! 	1(	1 
 ^#2
 (<36 (4@9M,

 ffvM ?A%
 V
 ff R#
 p_ff	 Z	 #'<68## 3 3 fiM
 5% j
9  P
 	5S8
 !
 	p
  
RVX fi
 !!3 
U N

 	 ]3k

 ]ff3 % #
 ff#2
 	/. 
U _ff
 #
 0#6
1 527 g 	8-,5d
 ff ff 
p#
 7(3fi 
 336 (98 @!L: d

 

 
 n9M,
/
 ;
 ff
 	5b
 M ?T%
 k_
 
 ff
 	5% ff-,
 	5
3 +SRVX
 fi !T!3 f
 +  ?!1

V !VOL [M\ X *[NUX#J HK[ML [ V7N VOL

	

:;3<=?>@.ACBDFEHGJI"EHKLM<?>=KNfiGO@3PCQ=/RS>,T
UWV NMKAIYX[Zfi\^]O_a`3X 'ff?  !0b X b'ff?  !]! 5
\ {! '  ` ff?')+'dc ? 05 _ 5	'  ` ff?'8+'dc? 0dc
e !)?_? 030	''0 ` 	!'' '	? 0dc?fghe
y
ij > j DkSACS>/Il` 55ff?  0	' d?ff?'50 inm oqp l ? )	?  o c
rOsut vxwyyz,{,| `   ?/!{!838' 55ff?,c
}~, r s| ` 	 ?fi%{58?' 0b h ~, r ? _~   ~, r c
KAxNMKAI)ZC { l~ | f  v,vqz,tHn ` !)?' 03'0R? }~, r s| {0] '  qz,tH c
` o c  =?> ? ' 0b h ~, r ? _~   ~, r ` s Z o ' }~, r s| c6 =

  '+?  5 ' D?!8?';' ;'{5 qz,tH ` s c 
  '+?  5;' f?  nW ` s c  i
` j c  B@3;3S '5'?'; '	55ff h ~, r  =
`3 c
 =?> ? ' 0b h ~, r ` s Z o ' }~, r s| c6 =
`. c
 =?> ? fge9 =
`h c
3' W  ' ;fid3'    ' N8 ' R?'
 ' R3' ~, r ?30	''1f  qz,tH ` s c qz,tH ` s cHOf
`3 c
';'f?  d?_W  ' 6W Z 

`u c
@    " rOsut vxwyyz,{,| jnV @ Wf+b+?'5'?HN ?fiW ` s c  _~   ~, r
jnV @ B
 '  ' ;+8'5 5	? =
`3 c
'5+m'  ffb'   _~   ~, r   'R'  ' f?
'5ff5'' 0b  _~   ~, r   'D'0!{0b''5 )?  
`3 c
S V9 =?> 30	''0
` Go i c
S V^ =?> ' 03' ~, r
` oo c
 ~, r _~   ~, r
` Oo j c S V^ B@3;3S

	b#S@%ge%ff53+T_?
?fiM5Mc+M##5#



:%43	1

( ff %	 ?2 NU _	2+3 N  <3 	y
 &#&%&'$%
6 	#+ff]S3V!jU3n9K	ffF+P_^

#[	ff-,h#&% 6	#+ff]S
 V!jU3n92	ffc+3_S
jU
#[	ff-1Nf#&%&'$% /3
3fi2+3 57d 6 fff?#	h5WV
	5%d;fi5	53+
 m
9 / "u6 )
 +
ffA
 !+!,kJ%LL@9M;
1 fPN

 /fi53+ 57du
#&%&'$% /WS1(ff 53+ ;
 fiW
 #5	[ ffS! 
 "{
Y ff #
 HJ51 D {
, ?_ff #
 ff#1
fi4/fi;

'$%

/,/%5

`

d 	



h*i 

U`



Q`

fi~}~ 

~/]{}/}




ff
fififi

fififi  
#%$ & '(' ) # *
-, #%. /

_T#5#
 YRf %b?v#	/`
N	!35B!
i`?%! ),
%	T ?%
 ?
fi2#	$##H+Mff"
L%+5#	Tfi?A?5#/7
UN

 	 ?R S1 /3!&68	%!5	:9R%+5#	?!,?%?Dff"":M


c 

,:)5
N	5	_%+5#	?A!vff7c:M

8


1 k": M

 3 #	[ P ?%
 ;
 + M #-,$ !Tfi 
 ?
6 9 @
9)ff

 _	ff*Tfi
 
 ?Tm
 %+ 5#	 
 %

 fi_?fi` D$
 # #
 R
U N

 	 ?
6 k
@
9( 
 _	ff*^fi
 
 ?)
fi_?fi5
 4 ;
 !
 % #
 R
U N

 	 ?!1^_ !^F ?%
 ?
3)
 + M ff):#%
 !


	 fi	
  
fififi  

fififi"!  
+) # * DC
'(' ) #%0 ") #%0 DC


"
 #
1
 #

 , #%. fi7ff	5%ff-1
] 	T3!d%+5#	?2
 # ,?%?)c:M
3
 #  #%$ 
 # 4
] 	 )*5m%+5#	?
 # ,
3fig+ 	ff5]N'56W@ffBBJ9M,?%?HT:M



 # a
4 6  #%$65 , %# . 9%Jfi;?%ff=:Wk?#*$5/fi/5P6 #%$87 , #%. 9M,_`?%?

 # 8 6 #%$5 , #%. 9%J;:W-?#*$5R/576, #%.97#%$ 9M1
] 	N*!	;fiff %+5#	?:
 # ,K?%?Rfi"ff	5%ff R#2
 # m	]$3!
 *5!,%?#  ?%?P:4DC_	ff*":M
!	
 # 4
6 #%$+5 , #%. 9%J5,

 # 8 6 #%$5 , #%. 9%J5,;
 #  #%$ ,/<
 # 4
 , #%. 1
(N/fi5
N!	ff,R	;52

f	3H!!	$_	gfi^:A2	+Vff

fi	N+,

?+ff_	P3
#QRU	ff"5
P_	(5U
N	?!1

a <Y{		NfipF5_!'m	ffLR1@p @!LLR,g:
ff%LU
N	?!1]f75P (v_d>R+ff$4+]
#[	c7%!%4+Sd$#"b$##U
N	?f!!1
_T#5#
 YRf7'Hy##b/fi5
N!	5P%5Bfi]'{/	#<R>R+ff<5<
(Qfi?R4f%

?+ff

#/%!)3+"J

=<?> @ACBCB;DEfi

"F E

 	ff1Ff//5+d3

;68!##7fi3
%O S,%	J% 3c]5
P_	7Afi!
!3U
N	?f ;9H%5vff#]?(7

5
P_	(fi!^!3U
N	?
%5 4

 +(_
 7	ff`5F!	p18fdff3
68!##(fi3^J%L*9
 5 `
%
 ?) 5
P_
 	))
 + #
 )
b 	W
 
 ?5 y# 	5% #-f
1 f7 	^K R#
 5# 

3d!

 _	ff*
 M ?2%
 # B#
 ?+$
 /fi 
 ?!
1 fNM ?ff
 ] S:
 M
 
  #
 A ?%
 ?( 
 )1
5
 ffN

 '
 M H# #
 !1
f;#
 5# 
 
 # 33 #	[ ?5 ;M ?
L
f?\5
 3 #'2fi 
 ? 6 W
9 fim!+T	[ 	/#6 + 	4@9M1 ^ m
 # 33 #?[ % #
 3:#m
 ffP5d
 R
 R
 # 
#5 4#6 + 	/KJ @ffJ9k %8
 ++ /%
 ff-,%:
 b
 M ?P H_
 ?
4
, #b
 3T#
 	b$
 #	 F+T8
  	
N

  / ?c\5
 3 #'1 ^ ?2	 <_
 N

 
 ff&#35&  #
 ff# " ?%
 ?c 

)14_^: 	 //
 5+
 # 	5% #-b
, M S# #Q
 3+ 7f
  S ?%
 b
, %: 	 Fff
#
 	5% #Q
 Q+3'H` ?%
 ?!b
, B+3:
  -
1 f?fiM5  3d+
 + ff

 % 
 & ff
 S %7: 

 M ?f4 P_
 ?
 #6 + 	<h
9  ?%
 ?^ 
 a#6 + 	 5f
9 fiP ?+ ff 
 ?5 y# 	5% #-1  
!
 	"
 !u
 M ,R
 + M ff5]  #
 ff#
 R
  ?%
 2+N
 M ^$R`#6 + 	 9H\5
 3 #'
37N

 	 ff #6 + 	C9M1"NOH 
 $
 2A ]
 ! M 32
 ?% 	 

#
 # ]\5
 3 #'
 3/ ?% 	"   `\5
 3 #'
 * M 
<,( 3H
 5+
 M 
3m		3 
 ff"5S c
 ! M 8
1 fWM ?mfiW!
 M	ffv
!
 M+
 ?P\5

 3 #'^
 %1"_AP NffQm
 ?5  # 	5% #-,
37
 /ffB*+
#6 +
 	 @@9M1
E ff4 # 5# 
  	M

 
 % ?!, 2/
 5+
 M 
3A
 dM 2%
 # 4

 j
U 


 1
_
 ff ?fi"# #<#6 F
 + 	=I9g:
 RM T+;_
 !ff
3g
 %f
 #A

 +
 
_
/+ 8m
1 fc
 !u
 M 3ff !fi *
 #k 	
U 3+ DM 

5
 4 %( M
 
$
 # # ?k	 (
 ! M ;fi;/
 !	 M
 m$
 # #
 ?k
#3 m$
 # #
 ?k	 A
 ! M 

<

>R

"K 2G  = ,IHCfiJ DC
AP



2G  = ,IHCfiJ

3

"L  =



`

"MNO, G  =

`

>R

>R

Q



fiff

R=<?> A@ CBCB;DEfi
SMNO, G   =
TMNO, G   =
UMNO, G  =
VL  =
<L  =
MNO, G  =
6W TMNO, G  =
;W

MNO, G   =

L VO\ VOYM[ N

h*i 

	

fi } 
	

fiff 	

|/|

W;1K_

 	 

/}





30 {}

f

 ff{5 %,

P_		ff,M!46W@ffBB9M,3KF$^
N_)$3b/fi_	ff1K_^:	F^! M 3
!ffN
#H
fffi_ff/%;+
N^-%U3+dM?)
_ffN

 
ff!fi*(%#p?_ff^+S3^!M1(YR5M?ffidff3
%ff4

fi5
#!M	5Q6#+	9M1f;?+3mff!fiNff?2%
* 
 c#
 _	ff*fpff!fi*M ?!1
fiR_	5!g#3%$##?g

c

!ffF#3Fff!fi,?ff*ff"5

MNO, G   =

MNO, G   =
3MNO, G  =
MNO, G  =

DC

YRf,fiM(\53#'P
N?1	?K'Hd	$?!-/5+!,Mm!fi%#-,
 #
 -%
 # #
 $ ff* 3:
 k H
 + M #
D# \53#' M ?v /
 ff\5
 ff*p
 # 	5% #
 !L
1 f
 #
 3P 34\5
 3 #' N

 ?
 
Y{ff#
%4
 %3: 3 &/
 5+v
 	
 $
 
, iN

 ?
 B#
 i _	ff*4
 + 3+ # 3 
$##?;#3 c
 !
 % # ?)v_	:
 Ml

 ff\5
 #]m
 ffG,5	A:
 ( 	
 $
 T
 P 
?fiM537N

 ?
 d3fi* % !
 !1
NW/_T#5#


>R

ff-,fi!fi%#7?%?7?

=

DC

! 8 zT<38x;'?z 49<Vzz  4>:A+s
f3ff#?5#_?

+  ff]5
P_	P)ff	5%ff M?7+yff3%#ff#

5
P_	P^#	5FM?!1 fff	`FM!N3?5%	S_ff	U_ff U_	P+
!fi%D]3fiR!^8M?f3^ ?#	 
,  ff-,/
U _	5N

 ff* ^ff
/
 + 5% P%T	
fif/!	R
3fiM ?%
 5 ;
 
/
 + fN

 f%+ 5#	 Afi 
 ?bNff
 %

 3 fi
ff#v_	?!1

fi`

c

fcm
 ?#
 * ff` 	5
 fi
 5 
 $
 ff:
 ^R/M
%#pM/!fffff#6 
;V
 ff{5 %,-J%LLL*9)?)#	5dM?)P%^	c#	5d!^fi!T!3
_		

U N

 	?!1bfmfi5-,
N	ff
Nff*ff_T#5#
 2YY23ff b#AJ5,	;fi+V
*
 	+ % 3+ 	 /
 	_ffff A- %ff ff ffM ?!,	]#)ff?KT#	5#'-ff	5%ff
/
 ! !1

1d

f

`

Q	

:;3<=?>@.ACBD
	EE Iff	K;3SEHKLMPCSAEHS/;3S/Q AC@3= V
UWV NMKAI  !)?' 03b';ff?')+'
\ ff?')+'b? 0
ij > j DkSACS>/I t wr }~ { ` '0 '0fi%{5)?_!5 0'0R' 0b R'm! " c
KAxNMKAI" !8?_'5+	' 5  5{55]' 03b';ff?')+'
` o c @ V @.AC@ j ;3@ fiS$"    ` 5!)?V!5 0'0' 0dc
` j c  =?> 55' ~ g \  
=  `3~ c  o
`3 c >SNS j A t wr }~ { ' 0
`. c
PCS/;3S/Q A 3'  ''    'R' 0  5  o   `3~ c  5'!	' D+
\ ^\ ?ff?'8+'b? 08	5'0dfi'' 
	5'f! 
`h c
\  	5'0dfi'f!5 0'0' 
 =?> 55' ~ g 
=  `3~ c   `3~ c  o
 
`3 c
S/;3@3Dk@ Vj ACS 'f!5 0'0' m3' 
`u c
jnv
 'f!5 0'0'  '! "
`3 c S V >SNS j A

	b#PJ5ge%ff53+WM/!fffff##5#
41
h*i>h

fi~}~ 

~/]{}/}

`

WN 	^+_T#5#
 2YYpfi;D!T]kfi!c!3WU
N	?&%L`D!T)M?
H1KNO g	m3mffff!AVM? H,
H18fT	; +%	%/fi5
N!	
!
 	M

 
 ?

 *
 M ?-%
 j_
 ]ff ffffP:!#c	! H1 !	U
N	
%  	3d4* 	 6 j9M1]NW # 3#,$ S 	d
 !c
 ff ff ffBM ?3ffN

 ' #6 + 	 @9
 H* 	;fi
 
 ?Rfiv!;+@&#6 + 	 J9M1 %UR!]
, B
 ?5  # 	5% #) #5
  #6 + 	//h
+ 9M,
 DM D3ff&+ / 	
 !]#6 + 	I9M1 
 ! H, DM %
 # ` #
 ?+
( Hfi  W3mff ff ff-1  ;?5 vM , ( ;3HN

 	 ffF+ %c@  6 j9gfi 
 ?mfiff
: H fi !]!3 D
U N

 	 ?/ 	ff 5 3RM <#6 + 	 59M1_^: 	NM vff ff #-,g vM 
3ff3 

 
 % ff&
 !
#6 + 	C
9  6 j92fi 
 ?c:
 R fi
 !;!3 
U N

 	 ?P
 	ff5
ff ff ffBM Dfi ffN

 ff* ff&5@`#6 + 	 9M
1 f332 S ff* 5)
 /fi2A #
 5# 

%5 Fff
 
 ?H %
 " T/
 5+;
 # 	5% #v
  fi
 !)!3 ;
U N

 	 ?A* 5#	 2 N

 2fi 
 
@  6 j9  @)+P  ( n,R%
  / ^:#%
 
 # 	5% #
 F T* 5#	 #
 V
 
U N

 	 ?]fi
*
 	5 $  #
 5+c ?F 	5%
 25
 !
 #
 #
 ff ff ffM ?!1kNW; 3;
 ff; 
U N

 	 ?
 ?&
 	ffQ5=
 / P
/ ff ff ffM ?cff ?S ?
( ]%
  RM ?c
 	5

 *S !A 	ff"fi !(!3 m

U N

 	 ?g%
 
 ^m
 ?#
 * ;ff
 2H_
 	ff"ff ?ff"%
 ff 
/
 ?% 	T5  P+S_
 dff ff ff c:#%
 N
 # 	5% #
 !1
NW3 R		3 !#
 fffi 3 %	 P
f % 

 "{
Y 	
 	ff, 2Yp
Y 3%N

 	 ffN

 ff* ff
 ` 	2#5
 `: 
YRf1 b#
 ch7
 # ?A W/ffR2
 #
 5# 
 f <Y1RNWF
 # ;
 	H#5
 ,Rf <Y]	 3 )YRf 
ff ff
 ( 
 # (_
 ?
  d T_
 ?+
 M T+/_
 d!ffF*+; c 	f
 ! H)
1 kN
 ff
 %	 
YRf +D
  fP _	ff*]
 + #
 %]
 ?5 v# 	5% #-
, 
U N

 	 )m
 ?#
 *  6 j)
9 fiW* R ff
 ff 7\5

 3 #'N

 ?
 c%
 5 p3)
 ffp(:#%
 !

K
 
T

K K K K K

 



	

,6O 8J fi

,6O 8J fi
 

"K

 

>R

	

> =VGO

K

K K ;	

 

,

ff

,6O 8J fi

,6O 8J fi@

a

S	

!`
ba

 

DC

&
f383F%
N^\53#'/
N?%]"YRf



 
'$%

KK

	 ff
 '
5 (
$
( ff-

U	g%g)m?#*g$Mf$##U
N	?
$ ,-5]

#	5%#`+
( ff-

	 ff


fi;2+*ff\5k+<@P	^ff`5pU?#
#	5%#-1

ba

fc
v?+4:(7?5#_ffF
N	ff
Nff*%#p3H+ff2c#	5#']bff

/m!ff3-,_ff	cdUff!#4
N73
#H4		3!#fffi3%	

G  = ,IHCfiJ

/fi5N

 ! 	4fi 
 y _T#
 5# 
 YRf  +_
 y!Q6:<!##
	
 	ffW
, #
fi3(J%L*9M1gfW?/#c2#ff5%	7#	5#'SM?;5!ff4_T#5#
 f <Y,
fi
 5 Nb3 +
 +N

 A5ff)/
 
b !8
1 fg/
 5+5ff)/
 R
b 3k % N

 mM H	 ;_A!ffff
P _	ff*b
 # 	5% #
 k	_T# 5# 
f <Y,%?/# g A5 
 ?87  6 j9fi 
 ?!b
1 fH
/ 
N

 $  *P5ff)/
 B
b 37 %D
 ff
 53+ N
 ?fiM5 %
 #  

 
fi 
 "

 ff
 !
 ff*
/! ff #&m
 +N

 ;5R&\5
 3 #'
 
 
 /!
1 f	!:
 /5
 
U 	#
 5%+
 
 fi	3 	 % #
 !,
fi	#
 p
  "YRL
f 
U ff! #%
 # B`3 fi
 
:#m
 ffQ5
  NM< 2YY
fi_?fi5(+S_
 /_
 !+ 	Wfi
 5 -1

DC

ba

fi 

ba

1G  = ,IHCfiJ

`

G  = ,IHCfiJ

! <38xr/o>+?82 <Vz{qo  %

q/s

.WqU8qK4>:A+s :#s,4 % z 

q/s

`

$ < 2q

 z

57d / ;%
 5 y3T
 
 #
 ff=:!fi%4!3Z/	_	:M
,V&O#&%&'$%/;%5y32_?F?fiM5ff

5"dff53+W;YRf #5#
41
 
fi
 % #Q%
  ffB 
 /dQ  57d / <6 57dT fff ?#
 	J5W_	5% =;
d fi+V

 	53+ %, )
 +
ffA
 !+!,HJ%LL@d
9 %
 7 ?d_	:
 M

  < 	M

 7 #&% fi'$% R,
fg3ff#y3T+3!3fi5#:4Dff3%#=_!'m	ff&



d 	

`

h*i 

U`

`





fi } 
	

fiff 	

|/|

 	 

/}





30 {}

:;3<=?>@.ACBD 
G E IMG j A j [@ V @ V <)EHS>,RS>OPCKLM<?>=KN9Q= V PxAx>KQ AC@3= V
X Zfi\^]O_ `3X 'ff?  !0b X b'ff?  !)! 5
[
UWV NMKAI
\ {! '  ` ff?')+'dc? 05
_ 5	'  ` ff?')+'dc ? 0dc
e !)?_? 030	''0 ` 	!'' '	? 0dc ?fghe
y
ij > j DkSACS>/I t wr }~ { ` '0 '0fi%{5)?_!5 0'0R' 0
 D']! " c
l` 55ff?  0	' d?ff?'50 inm oqp l pGo i5i ? )	?  o c
rOsut vxwyyz,{,| `   ?/!{!838' 55ff?,c
}~, r s| ` fi%{5)?' 0b R'{0?$c
KAxNMKAI" !8?_'5+	' 5  5{55]' 03b';ff?')+'
` o c @ V @.AC@ j ;3@ fiS$"    ` 5!)?V!5 0'0' 0dc
` j c  =?> 55' ~ g \  
=  `3~ c  o
`3 c >SNS j A t wr }~ { ' 0
`. c
5?  :;3<=?>@.ACBDFEHG 'd'')W'    'N	 ?

?  W Z
 
 	 ff
fi
`h c
 =?> 55' ~ g \  	H5 '0dfi'f'''0' 
=  `3~ c   `3~ c  o
 
`3 c
jnv
 'f'''0' ; 'd! "
`u c S V >SNS j A

	b#Ph5FNO	5%#3+M#7f% a
E*V KEGXUNUXAY5XUN



]Y{		ff1

%	#&% 3/ ,+
4Q!3Z/	]
N?5&35#i$##3	?S%
fi;!3 Z/
 ffW$##,_ '$% 3^
C0 
N?5]5#`]ff#
    1g_$*"
!3 Z/
 ff!
 % # 7	 ?!#&% $
   ,)'$% 
57d / N%
 c!3 Z/
 	W_	: 
M

 "!  D=  	M

 2g# 3 D3 fi M
 ! 5%  h'$% <6 	#+ ff&& 
3!V jU 3n9K %
 S_WA# m$
 #	 
, vff
 # #
 #'O
 #&% "6 	#+ ff"]   !V j
U 3n9K %
"_d%#
 p$
 #	 /#6 	 b#
  N`{
Y ff #`h51 J9M1
f 57d /
 F3dfi 53% S:
 N

 ?
 54 ! ?P
 
 
 3
 	8
,  

 
 /K%
 
 1#&% ?'$% c 5! 3F!#
 ^+P T3%
 	 ]_
 ^3	 fiMffg
 * 	?+WV
1FdH*
 	5ff#
, * 	?+ W
 M ? 
 
 /fiA 
 f "	 ff*#P3+ * 
  (3%
 G1
f
 RM ?)%
 5  fi;
/
 +c3+ *T 
  ;3%
 
 P R$
 * W3  57
d / 
 

 5 4* U 5
%
 3c
 + M ff-1fNfi?3	2  57d !
 
 ]
 ff=5y
 V
 

 /c%
 # & S_
 ?+

 #&% ?'$% " 5!
 <_
	
 S
 ffB4\5
 3 #'&N

 ?
 /:
 N

 /fi5
 v! ?T _	ff* ?fiM

 	5P N
 
 

 
 	5!1`NW 
 
 
 + M #-,K % 

 #
 +2	 = 3+p
 5 ! ; ;?ff)#&% ?'$% ] 5! 53	
 D
 M ?T
 ] _	ff*
%
 P

 
 
 # 5# 

 !,5/ _	ff*8
 /fi5N

 ! 	;!+ 
 FV
 dff ff ff"% 7

 
 
 #
 5# 

 ^5v

 fi	#
 N
 S
 +W!V ff
 # # P% S

 
 N
 #
 5# 
  %^ %
b ?W*+/ !*T _	ff*

 3!3 Z	 % #

 + !1
f(N

 ?
 f; TYRf # 5# 
  %g
 	ffK+c_
 )

 j
U 

 #	[ ff-, 5?+cN
 
 /
 %

 fic# fi$
 #	  
  S3%
 A  57d / & Sff	 #=H S !:
 _	7 M

 	46:%
 # ^#&% pff\5
 F+@!LL
 '$% pff\5
 F+=L S9M1 ^ )
, m
 !
 	ff,K %P 

 RN

 	 % #-
, bN

 	 ffN

 ff* ff_T#
 5# 
 YRffi, 3
 _	:
 MN

 ffP 	M

  #&% %





`

`



&	
MC
G_





DC









eff

 




S^

`

`

MC





DC



1Z7[M\ E*V [M\ [ML

MC







h*i 

`

DC
DC







fi~}~ 

~/]{}/}

' % R,%
Nffe`57du#3!,R	5%	%	M
( #&% k'$% +%V'	ff
$
#&%&'$% /1FfN?+ 3c
NffQ
N	%#8O^"	ff&(
N	J
fi)%53Fffm?fiM5vff53+A:H
N/fi57T\53#';VM?K:;7#ff-,*Uff


 -1  R
U ff<

 -, #&%&'$%/ 3fi
 53% N7  57d / 7
57u
d / R3)
 N

 ff#F
 ff\5
 #fi  ff*)+] P
 M

 3 #	[ ff#&%&'$%l/ P%
 	$% R
,(pfi
 M

 3 #?[ % #`
 + * T:   7 j
U ?!_
, ?_ff #
 ff#%
1 f#&%&'$%L/ R` 57d
/
 fi
 + 5% ffv`{
Y ff #`h51 J/5 b#
 ?  5
, ?_ff #
 ff#1

	

`

`

	

   /fi)#

b



 ]]{ 

W

D@

VMN

&`

fiff

S

8 /fi

fR#]/!,	?5#_ff=Y{ff#&J51 J5,_	?ff*^
+	&K%V

$ff=?5##R?!1Wf3%+	&?$)+;%"
+	&F

+
 fiMRR ?bR^ ffA3
 	D
 % %/
 ?^68>7f2f79M
1 fA
 	5?5#;nV
#vR?!,$ffp3m/fi_	ff,3(
N/fi5%	2+Sd+fiM4>7f2f R?76 ff5-,
]3% ! +b5*VWY fi/, YR
P5 -,@ffBBC9M,k%
 #  +N

 R/fi !3 fi5# ?)g  +b`]&3WV
 	1
f
 $
 ffv
U _	WVX
 ffp
 
 33
 	v
 R ?(
 3+ ( c:#%
 N
 + 	/!

U	

d

@%1f	ff


	5+

J51^%S	5+Dp	/fi5%#
h51%3!ff#

1%/!fffff#
ff51%+%3+	5fi5	5#[?%#`8/
C51%433#[?%#
I*1%*	!%#
51%p!fi%#
Y{ff#`h51@%,+5%R+	/7@d`J5,?ff*fS
Nff	_	ff
 ffp%/	R+
:fi	#
 = p $
 ff? 5 #
 p #N

 ! R#1 k53 ;:$ V

 =! ff # ff ff #
 
 + 	/T
h  Sm
 	? 5#_
 ffp3! 
 ={
Y ff #
 TJ51 J
J51 h5,R%
  ( %?# F
 
U _	WVX
 ff
 
 "! ff #F
 Fff ff #F
 fi^3 
 ff"{
Y ffnV
#3h51 J51 ! Rv?# Hb
 + 	/
:
 ) 3A

 v
 fic3 
 ffF3{
Y ff #
 )h51 h Rh51 C5,
?_ff
 #
 ff#1
fD
 $
 ff<? 5 # D #
 R ?d3# 	5% #
 * 	5 #
 1/NOd
 3# 	5% # ,

 c

 *F
 + 	/(

 ff
 	ff+/_
 	_?% ff_!:
 N% 3+# +
 v
 + #v
 3H:-1NOf
 3;3 +
*
 	5 # {
, 

 W
 
U _	!0 )*#
 ffN

 ff*)
 /
/
 +K ;/?K 
 $
 ff;? 5 # 

 # R ?!1 fy
U _	!0 p B  B/% ff*p
 53+b 
 
  ! ff # fi	3 	 % # 3
?
 5#_
 ffv`{
Y ff #=h51 I*1

@

a

ffAP

AP

h*i 

fi } 
	

#"$&% z  >o +/x




zw

fiff 	

 	 

|/|

+-  Kq 4>:zs?4  A: '*)

/}

>+?82

tvo





30 {}

4 4>:A+s

npz z 

fiM#3! ff
 #&]
 fi 	
 !	;fi?fi23?46XdAeTf79%3Wy
N$*=#V
!#^
Nff		ff
41^dAeTf 53+b#+5W!dfi	!	R%+5#	?!,3#%/#!,
ff
/
 + % "# + 5!,F	#5R ?,8
N!%$3]#+536 T
 !NG1#,)@ffBBC9M1 f?

 	ff
 
 3;_	: MN

 ffp ff
 	5V
 5  75% S ff #
 4 	7 _	ff*f
 + %
 ?!1

c



DC

dHff
S?+d:M
%#p/*	VU
%#p?#!,!R53+b"#nV
+ 573 #b N% 
, $
 # # ;# 

 #
 3++
 ,-m
 ?#
 *!
, ?#
 *!,k	 #
 fi!+ 
/
b 
, 
 KRV
N

  #-,R	#5Rv?
 
, p!
 #
 ;?fi%
 4fi!3 fi^3??!1
dHffN?#)K3%$5%+4?+!,!N:M
%#p%$%53+b"#+5T3#bP3/
% ,
 c+ 	5 
, 
P$
 ff
 c# +
 5!1

 c



dHff
d
%p?+4?+p?#!,W!
N?ff
Nff*D?fip5%,c!:ffRV
 5!3 fiv*{_	/*,cY !
Nff*3	?#-,)	fiM3yfi5*5
3v #

3+
 /
  ?!1

52^;T++M%c?+^Rff!fi*

d?+(cHb,:f?5

*	?+F,		ff

+%,_P,T,/=dT,?_ff#ff#1

/%+	M

_ % %/
 S%
 # <Jhv
 /% ff* 	?ff* '{/	K
Nff	b5QdAeTf 3%3!,
ffffR%HNW+#F:Fd;fiM#fi!3fi )!ff*#R f?%/3#%#-,_%!,*dm%3R,
;]
 ffi: F 
  3 	1 f`% %/3 ?_ff<5R 	/ff
##	
dAeTfa% %/
  /ff 
 
 dAeTf R!!
 ff 
 /
 ff
 	5
 $
 	3 % #-
,  %$ %L
2
 % 	ff /% ff*
 ff M	?ff*dAeTf /% ff* !1 %!
 	 ff ?!,( `% %/
  3S
 	
fi
 %	 R D#
 !?fff MTH _	ff*T'{
 _?2g /3?1 `
 !
 	ff,k N!ff
 !

 % # &	 ?<6 /% ff* ]%
 F
 ff
 QdAeTf7
9 fi=
 5
S# ff ff ff _	5+
 S	
#
 3 A%
 # y+N

 N ff # D	 ff

 ^
 c 
 /
 	ff=5`
 ff
 	5b
 5 # #
 	5
$
 ff* 3^dAeTf /% ff* !m
,  ff ff*;:
 /8
  	N*
 ?+ #
 % #
 ;+< NW
 + # 1 f3
/3ff % B!F
 3fi 53% : 4dAeTf 53+b
 
  3
 	,A	
 #
 3
 fi
 53% ": 

 ?
N
 5] N! ?2(dAeTl
f 53+b`! ff #
 =:
 d
 
  _	:
 M

  R?+ 

 % #y
 
ff
 	5N

 ff	 V
 5  1

&`

OP

/ff 



DC

ea

SR






z

'58  54 ' +-/.1032)z{o5476	t98;:z{ <38xr/o>+?82

4 4>:A+s

npz z 

q/s

<Vzz  4>:A+s

fR?AU_	WVXffp43	";_	:M
Nffv(:#%!1

	


 %!
 	% 
 %M3:Tfi?1(35;LR1 S+&@!LLR,V 
;
Uff"5
P_	gffffff"	]M?)ff\5
 	+/h51)f%M?]ffS3m#	5%#%R?
m	^S+%U_	g:;ffff#FF* 	! % #-8
1 fW
 _ff #/F@ RJ%R
L M ?F: 
?5&%+%5#	ff38	%U_	5N

 ff* !1)dH ! R
 ?+ #
 )K N

 ff	 V
 
U _	
*#ff 3D+m	4+3 

 #P 35
P_
 	;T ?%
 ? pM $R +< 
+ff&;ff	5%#<]
 M ?T%
 
  ?%
 ?cHy*#
 D
U _ff
 #
 D 
 7
 ff3 3%	 
3%$5%+?+!1gdH
 ff\5
 ff*#,*m
 fff
 %_	:
 MN

 ff)8
  	b
U _	5N

 ff* 5N
 * ff* #
 #
3
#) A ?%
 f/ f(5
P_
 	
 # 	5% #
 8 (

 ;#5
 ; ^YRf #
 5# 

#6 +
 	/TJVM@ffJ/_T# 5# 
 YRf79M1
NW7 3# 	5% # )R ?!,j )
U _	
 ff ff ff7	
 ]* 	?+ 2dAeTf 53+bT
 
 /!
1 %	 ^@
%
 7 F ffB 
 /!,+ ! 	;%
 # < ]fi 
 ?7q
 (QM #
 Z	  1"NW
 N 

 <3 	= 	M

 #
 
 $
 ffy= 3W/fi_	ff,- ; ?%
 ?dfi_?fi5v
 = 
+%/_P,

ba<Y#5#


&dT,	f

ff

*ffAP

?_

h*i 

fi~}~ 

~/]{}/}

1H?LGX 7X HK[M\IZ7[ *NJML E

##HM?A?5#/Pd/mfi7	ff
+
 n1mfc?5#_ff
#
 	5% # ]R ?k;
 ! ?+8: % W%
 + %
 ? B/dT,ff	#Mff7k%
S?+
%
 3 # 7^ #6 + % 4_ % *
9 3PR
 :
 M

 % #
 ff
 Q:
 R	 p
 
 
 /!
, G1 1#b
, #
# ff/+P8Z5 %
U _	!0 g 5# 	532
 * 	?+ 
 ?!
1 52#D
 %: 	]ff
 
 		5c T

 -,5
	/fi5%
 7

  %/ff

  /% ff* !,*m
 	%* 	?+ d
 
 
 /K3
 	ff-1H{
Y 	T{
Y ff #"h51 I
: ^
/ P! 3 A4 d
U _	!0 f*#
 ffN

 ff*%
  3
 
 33
 	v
 R ?!1

	


j



	




 o

	




j

	




o

	




 NS>,AEHS/;3S/Q ACS  EHKLM<?>=KNMP
ff G
{! ' m?  +'' :fih

 o






 m	5
?
0?
{'b 	5 
?m	5
0?ff
'?ff?ff 0'5'	5

?m	5
0?ff fiff
{'b {5  

'?ff?ff 0'5'	5

' 5	5

 fiff
{'b {5  

 )5''++?bfifi{5!''fi



h

j5h

l r fi
: ffhG
 o rOrz f3e  :fiffhG
: hG
 i l r
h  rOrz f3e  fi: ffhG
 1l e : hG
i l
r 

l
o

 h 



 

oGi

   

oOj

   

oGi

   

`

{ 

%	S@%)NWffp/;7:M


>R


 M ?!1 ^ 7# #
 fi7  #
 Ab
 5RV
	/-#+5!1)YR3_@d3(:T
d/%ff*!,/`_2J;:^ff
d/%ff*!,
% W 
 / 2@%
, (J5
, 3d2
@ fi2:
 (

  "ff

  /% ff* !)
1 f
 
 /
fi
 ff7
  _	ff*b
 %+ 5#	 ;/
 ! k%
 # 
 ?$1
 (d/fi5N

 ! 	Kfi 
 ?
# ffD
 
S
 (	8
1 fA3 +
S
  * * 
 b:
 M

 % #
 %$ ;#
 Z	  
 M ?(N

 	 ff45  *  ?+!1

DC

X

Y 	/fi5% ff#": %?5`%+%,	mffR*?+#%ff3%5dffvM?fi7
{
_?+]/	M
F$ 57d /,G1 1#,*%5/$ff
 fi^ffS+P) 57di*UF5G1
_A
 + %  T,: d
 +,!ffBM?fiS<S*UB5b^ b#?  ":
#&%&'$%& 57d / 
, ?_ff #
 ff#N
1 (H4g ?NM ?!, 
@ ^&J5
, 	 % ff
`
 P	
 ?!_
, fi;3 3+ ff %	 /J51 ^  ; %2 R
U _	W!V ff ff ffy
 
 / 2D
@  (J
fi#
 Z	 *!,	f
 fi%
 
/S 
 #
 ;3 P*
 U 5G]
1 f?+3:
 Wff ff 

U #& 
 ]'H
 M ?%
 + %   fiS ?N

 	3 	 #'68
 3+ 4H 	S ?%
 ?7#9M,
?F ff
 	53 #'368 	5
 ff3 % # ff#;

 *D
 $
 # #
 )	 ?n)
9 /)# g %F T
 ff/?%
 ?
fi, 
 
 7N

 ff	 V
 $
 *A
 !7
, 
 
U _ff
 #
 23 %$
 5%+
 " ?+ !1

/`

	

`

 

<!4qK4>:A'74>:  q  %

f&URp+	u

/`

b

Tff



4 M:qK4>:A+s +- <38xr/o>+?82;'

qoq  z{o

y$ff

?5#y#

DC

R?v+fi


<3	ff

/!1NW<3+	,+%3+	F _	ff?<3+5#	#WfiS
N	ff<:7'H$V

3%#!,kSfi!DQ!	ff$	3%#-1fSfi!R$	3%#Q3+7AM
/%
 ff* !ff*+3 F
 #5	[ ff 
 
 $9M,k%
 	?7 ! 	ff 
$
 	3 % #v
 fiV
 fffi 3 %	 7RVX fi
 !T!3 
U N

 	 ?P#6 $ ?# *v
  ff n9M1
$##S	<6XdAeTf

SR

hoGi

fi } 
	


o

j

	




	




fiff 	

 	 

|/|

/}





30 {}

S/PxA UWV K QS  EHKLM<?>=KNMP
o
: hG
o  h rOrz f3e  :fiffhG
o  j5h rOrz 3f e  
j5h : hG
: ffhG
o  j5h rOrz f3e  fi
 i rOrz f3e fi
: ffhG
 rOrz f3e : hG
 1l e

l

 ;	5
?
0?ff fiff
''fi +5'+0{5  

 N5! d  {'?'5 D{5  
fiff
{'b R	5
 N5! d  {'?'5 D{5  
'++{5  

 !;{5  
 fiff
' 5R	5



%	PJ5%(Hc`_?+ff

#&%&'$%lQ`57d

/ffi:v+%

   

o

   

T1mf?$##   

9	b#?fiff5,?_ff#ff#1

/fiP
fibff

	b# %f7#&%&'$%

/
 ?ff*  
*U 5v/yff
  \53#' 
N?  
#&%&Ru6 '$% (9%%2%v+% T1  jV
 ff3  2@  (Jff $##
_
W
  
 /Dffffffi54
NffRV
	
 
 
U _	!v
,  ;@  cJ'H
 ! ff
  
 /H:
 M

 / 
#&%&'$% * 
U 5G1

	b#Qff5%fy
N/Fv 	b#%V

W
 ^c ` 57d/mRV
$#&%&'$%l/1f

+?`

5



{ 



ff\5#fiff=P?='H

	 _ ?# ff-1 NW 
57dy/](73Kff
$
 * 268LR, L*]
9 <6W@!LLR,@!LL*m
9 	V
ff*
 RM $
 # #
 c%
 #  #
 ZV
  dff\5
	
 -	[ 	1

`



DC

/?

Y5 % 3+ 	  _	ff ?W`3+ 5#	 #A:k?5+5;6#%+5#	?n9A_!'m	ff=?
'HF$	3%#3)?+ff&/fi * ?+^%#=B
RffR+%46  L L 9M1 T3
	
 $
 /5N

 	5	 b
 %+ 5#	 ?dff ]_
 	ffy/fi##ff<<+=h%L*	fi3d+4%y!	
*
 	fi A 	fi%S ?+ y
 +  ?!1_T
/& %+ 5#	 ?;%
 #  #
 Z	 *#Q _	ff*
3+
 5#	 #
 ^ 	fiF#;
 ff
 7 
 S %P:
 M
  S ?%
 ?? 5#/
 " 
 
 /S6: 
5	
 /# + 5n9M,%	k
 
 #) 	]fi]3 +) 	%+ 5#	 ?-%
 # #
 Z	 *#T _	ff*-fi 
 
3+
 5#	 #
 !)
1 f?R%+ 5#	 ?%fi	  ff
+*
n_
, 3P ?%
 ?^:
 MN

 ff3
?kfi 
 ?- %b
 fiH5 fi5 	53+ F: k H3
 	ffR
 
 /VfiH	  ff
+ *  n1

ff 

ff

B

8' Iff 1	

DC

EGF5HIHKJMLONUX [N#NULGXOFKN VGE
hoo

DC
EGF5HIHKJMLONUX Z7[ *NJML E

fi~}~ 

~/]{}/}

Y $/#+5fi	 
N$*)+p5!D/%+	M&?5#(%fiR?+RV
R
%	#Q
N	!v !	%	:/
Nff	]5,);
Nff	]U_	33#bM?
p! 	M ?!R
 ^
5 
 $
  #
 !
 ff df$
 #	 /68>c
 ffb,k@ffBBh9M1
NW` 3^H b,$ R ]
 + % 3+ 	 b
 
 #
 3W3^+! ff7N

 ?
 8
 
 $
  S# +
 5!,
%	?K ^ff	 3#;%
 ! 	F !%
 5_
 )
 ff/+
 
 $
 
 	ff0 FRff %; %
 
 
?
 5 #y
 3c !:c+3 N
U _	!1/NW& dAeTL
f fi	3 	 % #& N
U _	d
 7ff	 ff&%! 	
W $
 ff]#+ 5fi	ffF* 	?+ 
,  ff3 3%	 ) !F
 fiT
  ?#S !]	 _
 

 ?
N
 ff 
 5  1^NW %	 ;h5
, 
U _	W
 ff ff ffy
 $
  /# +
 5fi;3 3+ ff`URT+" 
#
 $dAeTf 53+b] 
 /!
, ?5 =? 5#_
 ff"5v
 S3 3+Ab
 5	 /	# +
 5!1

 o

j


 o


j
o

EHKNN=?>,AC@ V < j Q AC=?>P

i >@ V Q@.N j ; j Q AC=?>P
{! ' m?  +''
?;	5
0?

!ff!+?''0'
 ?'!';!fi 
fifi{5!'5! 
	5  5 
{! ' m?  +''
fifi{5!'5! 
!  ' d '0!0

 pff 0'5'
'?]0'0!0 
 pff 0'5'
 '0!0''  5'+0	? 



{'b 
 R	5
?;	5
0
 ?ff



j5h l r



 

 o O
r rz f3e 
 i l r

h rOrz f3e 
  rOrz f3e
 i l r

'?ff?/ff 0'5'{	5

?;	5
0?ff
{'b R{5  

'?ff?/ff 0'5'{	5

' 5R	5


{'b R{5  

 )5''++?bfifi{5!''fi

h



{! ' m?  +''

{! ' m?  +''
fifi{5!'5! 
+?{'0b5   '

%	Ph5)NWff`?5#P65	/-#+5n9f`?W+%3+	5fi5	5ZV

[?%#6#$#+5n9M1



 <38 >+?82@:A'58 U: K4>:A+
xr/o

q

q

s

_

ff	33#[?%#/
N!R]	]_)ffS+P33#[	A^	FV*]3	
 %3 < 	

 4 <:
 M
 R
 M ?%
 # u fi
 !`!3   ?
ff\5
 ff*!1gNO)	 
 3 +/_
 7
 ffp%SN

 ! R:
 ^
 3
 3 #?[ R
 + fiM3!3 Z	 % #v
 M ?!1
YR 
 4
 3
 3 #?[ % #-
, ^? 5#_
 ffp4 3fff #-/
, #%
 )
 )+]N

 /fi3+ 5#	 #
 
2 _	ff*N
  
 /!1 fpfi
 5  N

 ?; p
U 3+ ff 3
 %S ?+S
 35N

 	5<6: 
M	ffS3 ! jb
9 %+ 5#	 )_
 
U _	!0 )* 	?+F:
 m
 
 
 #
 3!b
1 ffff ff ff%+ 5#	 
3%	#+
 ff=& O3!V j
U 32F ;3%
 5
4
1 f  !V j
U 3W	?ff* !3 !,$
 c
/
 Dff	 3ff#,
S5
P_
 	2m
 
 +  ?2m
 
 # ff<!3 !1 H Qff #
 ^H   !V j
U 3]6  !   W
9 fi
 ff=+p

 	 % P /5
P_
 	T]
 
 +  ?!12NW b#
 /C5,-:
 
 +  ,$ O3!V j
U 3	?ff* 
( ,   ! !V jU 3Kff ?K!3   fiR
 ?fiK3?76XdAeTf7
9   ff ?K!3 H. ?# *0
#5#
4,Wff

DC

 8

	

hoOj

fi } 
	

fiff 	

|/|

 	 

/}





30 {}







86 RVWdAeTf79M152H:A5fi/m%A  ! ,*		?ff*;ff/26 @%, 2J
 /@9k-dAeTf /%ff*!,A:D%%^3+5#	#c	;ff*;$	3%#
FdAeTf #6 kdAeTf7m
9 /% ff* !)
1 fc 5fi/f%(     73+ 5#	 #]RVWdAeTf
#6 b
 ?# *f
9 /% ff* ` ; 5
 F
 !d
 =/3+ 5#	 #4m
 ?# *
  ff !ff
*+/
  
 3_2J368ff43
 j9M1



SR

	b#PC5^f3+5#	#c)5
P_	5P2dAeTf/%ff*46#)dAeTf79d ?#* SRff
6#]?#*9Q	M
7f%<6#Q?fi5n9M1 W
c 5fi/ 
@%, 
2J5,8/@	?ff*P
3+5#	#c7dAeTf
/%ff*;_ff#`+<4?$ /!1Bf
ff33W	?ff*%?#*vS
R fff!ffF<
2J51

	b#I*^f3+5#	#c)5
P_	5P2dAeTf/%ff*46#)dAeTf79d

SR

?#
 *  ff 
6#V?#*9M,Amff_(c3+5#	#F/%ff*(:f/ @ /JR
	M

 ^]
 % 3#6 3 ?fi5n9M
1 fff`3
 d	?ff* W?# *
  ffW!ffp

 
  @%1

SR

L

L






T	

<L

52B
 	
 $
 , ] 5fi/7f/ @ /@ b#C #<"	5%
f$##"	?36XdAeTf /%ff*n9M,]  b#
 3I45fi/% /J%#

	

ho 

L

fi~}~ 

~/]{}/}

	b#

5^f3+5#	#K-dAeTf/%ff*m"2?5#_ff/5/%+	MA_@W (J5,
 ffb
m
 db
 ?# *  ff &
 !ff*+p
 /%+ 	M (J<68ff<3j9f
$
 1
 	M

 (+ k5  ?+ 	fi 
 d

SR

T=<=D 

2


T	

T	

	L

"	5%4;$##]	?!,%	?P]5fi/7 2J b#"C @ b#4I
	
 %  %2 ;? 5 #
 ^]
 
 
 /T
 	d$
 ##	?]6XdAeTfl/%ff*n9%Tmffb
+N

 ^
 !
 % # 2	 ?2#6 ?# *N
 #
 3 n9M1 U 	F:
 g ^
 ffH
 3
 3 #?[ % #;V
 
 
 /
2
J  
@ 3 ff* /dAeTf `RVWdAeTf 3+ 5#	 #-, b#
 ?T
C  N
I ff
 _
 	ff
N

 	3 Z/
 ff M	+
 ff
 %	 2_
 !+ 	F	5+ ^ A
 3
 3 #?[ % #N

 ! R-,5R
 %
 

 +( 7 	5% ;
 $
 # # 7	 ?!1
NWN

 ff	 g

 
 cm
 ]'{
 /	 #<
 S   ! S+
 	?ff*P "5
P_
 	7
 $
 # # 
 ?46XdAeT
	
f /% ff* !)
, Q 3/fi_	
9 
 M	P+y
 !
 ?m
 
 _	 ?7f
  ffB/%+ 	M
 P: 

 
 /$ ?f/% ff* !
1 52/ ) 	m-,   f38?	
 ffS+d
 !
 ?/
 
 _	 ?
F ?DN

 R/%+ 	M
 /6: 2 	/%+ 	M
 n9(:
 2 ;
 !
 % #
 /	 ?/6 /% ff* T%
 # cdAeTf79M1
52 A	 ffi* % ?F   !   8 $ ffD%$ f3 %8 /  fiP!3 Z	 % #
	 ff

 ^ ;N

 /fi53+=F Nfi?	T ;
 5fi/=]
 v
 
 y=
 5fi/=g 
ff*
 $ 	3 % #;
 3
 3 #	[ ? ) 5 #
 K
  
 %F   ! P#6 ff # # #'$
 #&% *9M,



"

!
 	1% f
,(D+ ;: K 

 
 %K   c6:# 3 3 fiM
 5% '$% *9M,%
  ! # #  4! % # P	 ?% ff* $ 	3 % #-{, ?_ff # ff#1  W +  ,
5
P_
 	5H
 $
3
 
 3
 3 #?[ % #4]
  
  R
@  b#
 /I; Dfi?"	^ ;ff`3
 7`  
%	?ff*
 g 25
P_
 	5Fk

 3!3 Z/
 ff/5
 
 
 +  ?g
 
 
  @%1KNW] 3F;
 ff,
 Fff

 # #
 #'y
 Q#3 3 fiM
 5% 	 _
 ?+ 

 % ff:
 /%+ 	M 2@S 
 b#
 4I*p
1 f
N

 D: M

 % #4: W/%+ 	M (J]	 =_
 7: b#
  5V
, %
 
 
 
 /^_
@  (J

 	M

 (8
 %+ 5#	 ;. + k5  ?+ 	fi 
 0#1
f%
 $
 ff]3
 3 #?[ % #SN

 ! R"	"_
 Wfi ff"+
 3
 3 #	[ %
 
 /]3 +D
 S 	M

 
Tfi 
 33+ 5#	 #
 P23 !  j

 
 
 %+ 5#	 ?!1 _T fi
 5 i+ 5 i
 3
 3 #?[ % #
3?ff* ffB b#
 SB51]e^m
 ! 	ff,/+4/
 fiP5 fid
 	?ff* % #-
, #d
 3c
/
 S "!#T+
N

 /fi! 	5
  
 /;
 7
 3
 3 #?[ % #-1
NW= ff
 	5G
, #
 32c
 ff ?fi` %  !   ff /'H4
 $
 # ;!3 ?!1PNO]
 fi%V
53%
 , !3

 ff`ff D*3'H!3 ?!,
 !
 ffy
 *4'H4 _	ff*W
 %+ 5#	 Pfi 
 ?!,	%
 5 
 d

U _	(H43#b 2+"N

 /fi1






VL

S	

R












VL



D@

fi	

L

fi	

	

"MN

>_




	

	

D^

DC

ho 

fi } 
	

fiff 	

|/|

 	 

/}

	b#PB5^f3+5#	#4AdAeTf





30 {}

SR

/%
 ff* y?# *
  ff T%
 # ?_ffT+p
 + ?Tfi 
 ?
68#7,#-,;	N#9-:Fff*$	3%#R/A	fffR/%+	M!1

	b#S@!LR^f3+5#	#PdAeTf

SR

/%ff*)D?#*D ff%#R?_ffK+U	M	3

dc

Y u!N


 ff*^	?#p
 

 3 N

 ! 	526W@!
S
 
 ?$A+SLR1@7
  9M1  fi
 7#V
	ffT_!'m	ff"+_?#*F$	3%#F	"_^ff-,5	H _	ff?

/
 /%+ 	M
 fi4 	 

 G
1 b%+ 	M
 S_p
@  d2p
@ fiff ff ff NUR ffN

 
 ?!1 f"ff3
	
 ?ff* R?# *y
 _	5+
 
 ff#<? 5#_
 ff<5
 /%WV
	M&d2@

DC



<38xr/o>+?82 ?s?4z{o>2oz 4qK4>:A+s 4 % >o +?8r % @ :A'58qU:qK4>:A+s




YR333#[?%#p3)	4fi%	7:%U_	W*	!%#3)`3	pV


b#
 ?7v
C IF#c	 <_
 N	ff< %c 	32
 #
 Z	 *2 _	ff ;_
 !'m
 	ff

&	 g	

DC

#!1

hoOh

fi~}~ 

~/]{}/}

SR

dAeTfl/%ff*y?#*  ffW!fiM"?%,	T%2	NfiN#Z	*2#V
	ff ?F
/ =! ffff /%+	M!1 b# B+5%? 
3fiN _ff:]`+
  ?+ 	$fi 
5
 ?;# 
 p#;
 3HbR^%A+5?+	V3;p
N$*m53+b/#+):
^dAeTf 3?b
1 f3k/
 	fi % #D
 %
  % ;	 ff
-dAeTf 53+bc
 
 /! ff #S	 
'{
 /	 #/g_
 %+# ff]5/
 	5d
  ) ?%
 ?m]ff
/
 + 5% ?g Wfi
 53% ff
 ?
  ?+ ffvfi
 5 "%
 5 S 5?g+;
 ff
 	5% 
 
 "? 5 #
 K%
 5 fi#
 	 
  #8
 / ! ff3 % ff3?%
 ?!1
b#
 =@!L 33 + * 	?+ 8
,  F#
 3P
 	Q _	ff*P 
fi 	P	
 ?!1 ^  " %

U 	M	 3
d fi
Y !N

 ff*"	?#;
 ]]
 ff  %+ 5#	  3 5
 <% 
6:%
 5 S* 
 ff]#D
 %+ 5#	 ?F %m
 fi%fffi 3 %	 %%m
 + %
 ?H_P,  d)9M
D 
U 	M	 3 d 
Y 
!N

 ff*^	?#-,#; 	M

d
ff
 MN
 pff5 R5 fiM#
 5fi/*v
 fiPf
 fffi 3 %	 2: 
?fiM#
 53+b/ 
 4! ff #p
  ^ !"	 _
 T ff ff jN

 ?
 ff#
 F
 _ff	 33 #	[ ff"
Nff	 

 + # #
 !1 b#
 @!L! ?fiM#Sff
/
 + 5% ?;#
 Z	 *g _	ff ?g_
 !'m
 	ffv
 -dAeT
f F
?#
 *  ff D 	M

 ;f
 
U 	M	 3
d
Y  !N


 ff*S	?#fi 
 ?!,Fff
/
 + 5% 
%g 3KN

 ?
 ffN

 ff*!
, #_
 fffi 3 %	 
, 3)
 U ff ff*g3?%	 %+
 ff1 A]
 #]
 3 +d
 %
 F %!,
#
 
 #f
 3(bR^3 %f
 /%+ 	M
 ^_d
@ =d2@7
 	c _	ff*)3?d
 $
 	3 % #
 !,5 !
_?ff 7 	p
 

 3 fiM#
  	M

 ( 
U 	M	 3
d
Y u!N


 ff*T	?#v
 
 _	'1

>R

	

	

OC



DC

 c

 c

;	
SR

?_

 c



DC

DC

 c



 c

<38xr/o>+?82 .WqU8qK4>:A+s

NW3M	T+F!fi%3	ff 53+b"/!,	;
Nff	U_	W^?+ff=Rff

 
 /%+ 	M
 )
 
 	_ffff*!^gIjL_!	]6 %L4dAeTf /%ff*%=J%L]RVWdAeTf
 ?( 
  N
	

 
 /# :9M)
1 fW?# H:
 A ?/% ff*!,
S
fi5#[	ffFv%	 ,
%7 N/%+ 	M
 fi! ?+88
 <! ff &dAeTL
f /% ff* !1S_^$PB%L
)dAeTL
f /% ff* 
m	R!ff*+F
 %T ?+^
 K 7	
 /%+ 	M
 !%
1 fP! ff ffyff
 # #
 #'fi 
 ?36 #&% *9
: /%+ 	M
 c_@%, (J5
, Qd2N
@ fiN#
 Z	 *#
 #
 	2 & ;fi 
 ?cN

 	 ff&& N!

 /% ff* (
 ff": f 
 3 	1 
 ( 2 	('HN
 /%+ 	M
 A 2fi 
 ?A/( _	
#
 Z	 *#1 ^ ; %2 D!!
 5`fi 
 ?WfiDff3 % #
 ff#
 #
 -,$?/# P Rff3 % #
 ff#
 #
 
#3 d$
 # # 5% u6 '$% *9M;]#m
 	&'$% Sff
 P_
 	ff
 5 !
 ff`5p
 ff ff ]#m
 	Tfi 
 ?
F  ff
 	53 #?[ % # /fi5N

 ! 	&(	V
, %
 "
 +2g! ff v
 
 
 /)%
 # =#m
 	c
 	5%
 S
$
 # # c	 ?!1

#ff



	





#&%

_@
_2J
2@
(J
d2@
%	

5I*1 ff 
 51  
J51  
hJ51  
Jh51  

1



DC





5D!

'$%

JC51 
C51 I
B51
@ffJ51 C
51







ff ff



7FSL [  #&% 
ffB51  
>1  
R@%1 J 
 @%1 J 

I%J51 I 
hC51 L
CB51 J 
 J51 L
*
I 51 





J51 L

?+%!

'$%

I I*1 

JI*1 h
J%LR1 L
@ 51 L
LR1 L





*ff 
 





7FSL [ 

2YR
S
fiW?#;ffii45!i 
IjL _	5+=6#%ff LidAeTf /%ff*F J%LRVWdAeTf 	?"


N?ff	M
( #&% R,H'$% N
7FSL [ %1


ho 




ff


%LR1 L
IjLR1 L
R@%1 
I*1
h51 I

	_ffff*D!S
 
Ny/#:9M,

fi } 
	





fiff 	

|/|

 	 

/}





30 {}

$&% fiz .1032)z{o54 '  + z :#s <38xr/o>+?82 @n :A'  +{z{o5B


f)dAeTf

 ++5%?%bU_	WVXffR#3bD#	5%#;R?bP%5
	
U_	)	 35Pff\5?+ff4ff	53#'"dffv/d/!(
%+5#	?26: ?%
 ?n9K %m
 fi2

 Wfffi 3 %	 ):
 mM ^
 + M #-1NW] 3F;
 ff
 #]
 3)$
 #	 
+W K _	ff*V
 /%+ 	M
 H#6  
 /n9	 
  ]N

 g% %!!b
1 f)ff ff #7R
 F
 
/
 

 
 /)	?ff* P ^
 /
 + #F
 3g !:H+ W
U _	!DR 2ff	 3#]	_ffF$ ]
M Nff
 #
  _	 ?]683 #b S "5
P_
 	PA M
 $
 # #
 ?dQ]+ 	5% ff5
P_
 	P
#3 $
 # # ?n9M
, ^m
 ff
  ff # 
 _	 ?)3 #
b P 	5+ %/
 3 #',
 
U _ff ff?
y
 #
 %/
 3 #'3]
  ff  
 &? 5 #
 6X
Y #_
 	55 %+[
?
[ 3 -,k@ffBB 9M,k%
 5 
	_ff/S
 ^ ?%
 ?g
 ff/ T# #
 KV
  ffNM ?!1NWS %fi	3 	 % #S? 5#_
 ff
`
 3W/fi_	ff,$ /

 y
  ff # D! 	 %/
 3 #'= 5# 	53]m
 	S	5+ %/
 3 #'
, N

 	3 	 #'

 #
 %/
 3 #'1

DC

SR

d

ff

SR

P

bfi # # 4 dAeTf 53+b`   	 ff
 *+3 	]% + % ?_ da;P
N	!ff#
/ff&<NU_	!0 7	5+]H/'{/	K3%+RR?!1 
 S
5
 ?fiM
  $
 *;T
 !  3N _ff ] ff ff #
 /
 ! ;
 %+ 5#	 ?;%fi3ff 
 _	ff*
 
U _	5N

 ff* !1 ` ! 	ff8
, %P% + %
 "_  /fi # #
 "A N
U N

 	 !
 
_	ff`
 ff^m
 ffG1A_AT 3)% + %
  	RfiP#v
 S ! %+ 5#	 ?) %Tff
 _
 	ff
 ffS: ]M % #-

1 ff
U _	!0 g	5+ 2- T

 
 
 ?+ ff"%g cdAeTf
$
 	3 % #_
 d/fi # #
 ff*+]'Hv
 
 $
 	3 % #
 H/
 ff3` RU48
 /% ff* !,

 %
b 
 #
#
 Z	 *#
 ?	+
  * 	?+ 
 
 
 /!
1 f3/fi # #
 v
 ?# ff
 /%+ 	M
 
_d
@ 3_2J51

DC

a

	

>C

_T# 	M
 % # ff#,//fi##S	3_d_	:M
Nff3+F4/78_	:M
N+%3+	
5fi5	5#[?%# P3	ff /!,A5i8		3#+<&!ffff /
!
 	5
 /fi /6 1 1#, _	ff*3%]_!'m	ff<
R=ff
/%ff*T%fiMR$##
	?P: P  
 $%
9 QffQN

 /fi5
 %+ 5#	 /fi 
 ]3+ 5#	 #
 ^:
 P ?/fi !1
_T*
 #
 Z	 *^ _	ff R3 3^3+ 5#	 #

 ff3_
 $ ff* 3#v
 * 	?+ F
 %/fi^F 

 
 P? 5 #-1kW
_ ^/
 3: b
 
 d/fi # #
 (
 H

 ffP
 ]?# 	+N

 A! ff ff

 $  D
 53+b"# + T W*4 	W%+ 5#	 7
 W%+ 5#	 P
P/
 
 % #3%
 5 3$ ff* 3#
*
 	?+ //
 ff44 d
U 3+ D
 
U _	(bR^ ff1

DC

DC

OC$7!ff3+"+p+
%DR?T)/fi##U5V
_c3AHbQ6 YR
f 
%,?c 
P_		ff,?da>5+ 	f  %,-J%LL@9M1
	 
 7
N!R#	$*Ak!#3;*	?+;+Sc%(U_	fiV

f	DT_	ffy+
NR


N	!c54
N!R<(_	3ff`?fiM,-	d#?ff*%#B327AS



	3%ff<ffy/2	5`
*&fi!P!37	?"6:%#<MN$##D5%S
%c ?+7J%L S%
9 =%# `# 3 R$
 # #
 5% Nc# W$
 #	 ,	%
 # ` N* ff* #`+
b 		B#
_ff# @!L "1 Af
 p
 ff ff N
 NM 
, # ff #\5
 3 #'F
 ((_
 	ff4 7
/
 +f
 N

 $
  *
#+ ff8
1 fA
 ff ?fiP# #P: 8ff ff 
 WM g;
  % m
U _	;
 8%	 g+
 ff
 #	[ 

 ff #
 W
/" ?%
 ?T	
  RM 7 %
 fi;N

 ff	 #F
 ?+
 %	 1^NW3 3Wff
 ,

 R
 M ?Rfip#
 Z	 *#Q
/ p*
 # #
 ]
D #;	 _
   ff
 %	 &@" %N
 ]
 M ?
ff ff
 ff=53 
U _	
 ff D%c
/
 +T 	P ?%
 ?2
 S R5	 /_
 53+b# +
 5!
1 f
#g %F f
U _	g/]
 ff ff]
 
 
 /%
 # NS
  

 
 #&%&'$%5% #
 3)
 + 5% ff5
b#
 ?A@ffC @ 
 S{
Y ff # 1 J5,%
 5 Ri m$
 # #
 k ;/%+ 	M
 _@ d2
@ P  #&%&'$%
/
 <h#&%&'$% * U 53  ff&:
 % + %
 ?_ dT,k
 ff 
 $
 * 2%
 # 

	





AP

&

P

ho 

P

fi~}~ 

~/]{}/}

^
	5%_	?!1NOg	]_^ffS%H)-%U_	]ffffff/%+	M
3(#
 ;4 #&%&'$% * 
U 5$	A ff ff ff/%+ 	M
 ffiP!#
 7+S 7*
 
U 5G1
)



 

#)fi!vd    3>/ ) !2* Vfi)IB>/

 _ 


	

 

`

/)!!

NOv3]mff^bR^ 
  57d #3!,(%p M	+ 5!&=_?+F?#!,(
3
 	ffM ?);_%g!#W)$#	A+P^+V'!:gM	g- 57d /1)f3

 ?
N
 K %)
  q#&% ?'$% T 5! n
, #&% _F3 fi
 fb$
 #	 
, h'$% )

f$
 #	 1g
Y 

 3 fiM#{
, 4 
#&%&'$%l/ H
, #&% "_d^3 fi
 f$
 #	 
, k'$%l


 V
 $
 #	 1
NW 3H b 3\5
 3 #'N

 ?
 ) a#&%&Ru6 '$%
(9 = ff 	53 #?[ % # /fi5N

 ! 	
(y7_ 	ffQ ff-1f3ff #B
 
U 	3 
 2%
 *& 37\5
 3 #'&N

 ?
 7_
 	ffB
 ff ff ff-,b
N

 /fi53+%
 #  	F
/ f*
 # #
 A\5
 3 #'PN

 ?
 (3 #
b ;c
 +WVX/
 ffS
N?
 1 *#
 
. 
 +!/
0 /fi5N

 ! 	 j1



MC



/`





5

	



3#"  + 2 M:A'*+ ,+- 4 %
w

qo

s



z

q/s

S8 M:A'74>:  '

fi
	ff

z o

52U_	5ff]7DC_	ff*
Nff	fi	3	%#V	%?-%*##-3#b U_ff#
3!	fi'$%
ff#<	M
7## #&%
ff#7
S5Mpfi"!8
:2	5+;	ff
 FffSR?fiM5y4#&%&'$% /12YR$c%



	

^##;-+m/fi5
N!	 f3K/ffSS):#%dfi
Nff*! m!	##
'$%/,H M p 	
/ 3 p###&% U
N	?M	S+Q_4_!+	ff1 
;ff<
 5 y?+
 
, #
 3$
 #	7+3N\53#'=
N?h ,";:#%
#&%&'$%  5! 

MC

 3#'
N?$

	



 	  #&% 
  '$%

	

'

j1gNO

3?++]7_ff	c*##*	!%#4b/fi5
N!	

;3+N;W_	'S%ffff":3	K5"5;

^  2
j1

 mc	

fi


/R
#&%&'$%l/ffff)c
V$*A/ff4p/fi5
N!	
NWi_T# 5# 
 YRf,K `\5
 3 #'N

 ?
 k%,A
 
 < _	ff*O
 #&%&'$%  5! u3/ff& #&%&Ru6 '$% (9M,%
 	
(v3A 7
 ff
 	53 #?[ % #p
 /fi5N

 ! 	ff1
NOd
   
  3 	 # 5# 
 ffN

 	#
 
U 
 + #
  ?fiM5 a6:
 v#d
 f
 $
 * 
#&%&'$% / vfi"bR^  fi j9c ff "'H&N

 ?
 ?OF fiFff\5
 #fi  ff*
 pff

 " %D
 ! 	  

 m
 + ##
 ` 4*
 U 5g	 _
 ! ff ffi5
 
*
 7'H
 ff
 53+ 	 !DR#] 7fi 
 ?( %^

 +(_
 dff ff ff4:
 %/fi5N

 ! 	5q(v fi
 _	ff*!1KNW4
 3(	 
,  

 #
 *(_
 !
 ff3_
 !+ 	^_
 ff	 
 d# f* 	! % #
 3(
/
 *
 # # 1
e^m
 ! 	ff
,  &_T# 5# 
 YRf 3v ff
 53+ 4_
 ?
 ?fiM5 u#
 5# 
4,) #
 % # 3
 _	ff*!1 YR
 
  3 	 3 # 	5% #
 R ?!m
, _	:
 M

 &
 3
 "
/
 # 	5% #
 
6:'{
 /	 #`J 9^* -5RM ?%fi;
 + M ff=54:
 M

 ]  #
 )F ?%
 ?3 
M `$R1NW 
 3R ?!f
,  M `\5
 3 #' N

 ?
 y3"
 ff :
 vM ff ff #a6::
 %
 5 
 c'H]N


 ?
 ?1d) fidff\5
 #fi  ff*n]
9 (m
 ff_
 (:
 ) dff ff # ?%
 ?f?
  #
  %b
 ff ;#
 d$ ff* 3:
  H
 + M #P
 #
 P\5
 3 #'
 M ?
 /
 ff\5
 ff*

#
 	5% #
 !D: ) 3(
 
, M c\5
 3 #'"N

 ?
 d3A_
 !+ 	^ k
  1 !^
 ;
U 	3 %
 *1
YR
 $
 7 %Tm
 Dff N$
 *;#6 M j9 l` #&%&'$% / ,$%
 	#&%  '$%Lfi
#
 2 M
 &#3 N$
 # # ?!V
, ?_ff #
 ff#1 
 pff ff ff9(`fi 
 
, /	 <_
 S! 	M

 
 ff=: 

DC

5

MC

	

DC
DC

	

APSff

>R

>R

	

	

	

W

ho 



fi } 
	

fiff 	

 	 

|/|

/}





30 {}

W

3M ;1ANO^	`_^3%W$*(%WffR
N\53#'%M
S3 
 2ff45"c:#%;8#-

W

3c

#&%(  &
# %*69B 5 ( 9 '
9B
fi B) &#$' %
5
% 5 ( $
' % 5 (
' % 5 (
$

fiB

NW33(8#-,
	?ff*^5
P_	)KMd$##?))Mc%#3\53#')7%5
 	5d
U #
 Q!%#U
N	?!1 H ffffp4 _	ff* <fi,-/?$
3fi 
 2	 _
 c! 	M

 
 ff]5] 3H8#-18fc3,R!	M
ff]5]3H8#-,?

j
U 3%f
 $
 * #&% ,  #&% C( Ru6 '$%
(;
9 4 j
U 3%f
 $
 *fi
 1(	]
1 f3;3^p
 
b#
 2@@%8
1 f;# _H	 33 
 m3ff\5
 5+2 (\5
 3 #'c/
 M  ;,%
 5 ff\5
 3 #&%&Ru6 '$%
(9M1

fiB

	

9B

DC

fiB

9B

9B

5

W

5

tp

points with same
quality
qg=TP/(FP+g)

R

TP
TP0

-g

	b#S@@%

fp

FP

)_	?7;M?c%#<F
N
\53#' '

	b#S@ffJ5/`^?4%##?+3\53#'
!ff *+&"_?

#&%&Ru6 '$% (9M1

5

RV
:k

WN  )#&%&'$% /,$*/%# #	]\53#' fi %$`3S3,;i
ff# c=_	S!:"M	ff1 ^=%vik#&%&'$%
/=`+V'!:F3S
! 	ff /fi4B/ $*p %p/fip	?ff*pM?"%# &_?+#&%&'$%
5! F
1 f3W?+
 
 	 % ?c %d$*2%7%_!ffy&S_?
 
+d
3 D%$
 S S3 
 ]
 ff\5
 m
 ?#
 * $ 	ff
 %
 5 y3c
 ff=5= /3 +
 M D= /_?
41PNO
	?ff* ffS5fi/	
 #,%/
 5+
"5
P_
 	K_
 M ?!,:; 1#&%&'$% / )%
 ff

 % 2 (3 
 g 
 $
 *(68L % %
9 P (!#Rb5%
 3Aff #-,%%
 _
 ;!ffP A_
 ?
41
f7
 ff* 	^b
  % #
 3;$
 *76 1( L*9M;
1 f33
 + 5% ffv b#
 S@ffJ51
52P A 	8-,%:  q \5 3 #'7N

 ?
 A
 ffP5
   #&%  3'$%  ;#
 % #R
 3


 3 fiK	F]
 ff* 	 G6
1 
 * F%
 # / %N

 T\5
 3 #';3 (
 P3 
  )
6 fi'$%9 #&%/,
	;
 # # _3A
 + *f
 Fff\5
 $+ j
1 
 * A%
 # v#
 	A\5
 3 #'"3 W%$
 7 c3 
 W" 
ff
 #`F ; !:2
 _	T M
 	ff
1 fR$
 * ^ %T%
 _
 R!ff*+] ;_
 ?
 fi; 
/5+
p$
 * mS &#&%&'$% / T:]5
 
R
/
 ffN

 ff*(k 23 
 )%
 # 
#
 _ j
, + fi " 
 $
 */68L % %W
9 ` /ff #`+;
 fiM2 /#m
 	5#
 *2
 M
 	ff
1 f3
3
 + 5% ffp b#
 /@ffh51
 !
 P N

 S %Pm
 Ffi"#5
b 4:
 RQ
  

 )
 M /%
 5 B3c
 	B
 _ff	 Z%1"NW
 3F	 

, /fi5N

 ! 	 A%
 
 ff d#
 /fi 
 )%
  /fi5N

 ! 	(%
 
 ff
 7
 	N
 

 fi 
 1
fN*
 ff* #y
 32+`N

 ;  

 b
 M R= #&%&'$% / 1;_Ac ;/
 5+
 + %
 S
M ^
 + M #]#
  ) ?%
 ?;fi2
 	ffF"
/
 +m
 /
 %	# ?A\5
 3 #'
 H 



MC




1G  = ,IHCfiJ
7 D@
7

	

G  = ,IHCfiJ



9	

	



eHK[ML [M\A\ VO\

7 D@

9	

fi

ho 

fi B



 9B

5

fi~}~ 

~/]{}/}

	b#S@ffh5/`^?

	b#S@O

 #Q#?+ \53#'kRV
%
!ffv"c_?
 :  #&% 
 '$%/1

	



1	

g3ff
Nff*



*	?+

?];q#&%&'$%
c/
 5+f
 # 	5% #-1

 ?jV
/%%:	

O

+#3)5%	]$5ff1gY{	 b#P@ P:m7'{/	/	3ff
Nff*g_$ff*3#N*	?+
 ?%
 ?% 
#&%&'$%l/ 1
ff5

 fi8 #/- ?^?%?m3K+_)5RS	T	#Rb]+P%g5/ nV
 # ff#D

 2 	F ?%
 ?!{
, #
 ;\5
 3 #'R
 M ?	 S_
 (
 + M ff-1 HD
 c #!,
+
 #
 ^ ff
 	5#=
/ = /ff #`H / !:2#m
 	P
 M
 	ff
1 fD?+
 3T %cRV
 #
 T	  ff  /5
P_
 	T
 '$% ff #
 !12e^m
 !
 	ff, !
 ff  /5
P_
 	T
#&%/0 f)m
 ffG1AdH
 ff\5
 ff*#,5  #
 ff#v
 ; ?%
 ?)+
 M ?A %%
 fiP!#
 7+] 
 !:c#m
 	7 M
 	ff,- N# 5# 
 %
 k2_
 N%	 +`?_ff	 33 #?[ % #
 T
 ?fi	c+4 
 !:H
 _	H M
 	ff1 52#; WM ?F %;
 ff
 #
 h#&% fi 
 {
, vfiS 2
 _	)/fiH 
#&%&'$%l/ /, ff S5  P+S %b /fif
  7
 + M #48
 * 	?+ S
 !
 M ?!1
b#
 P@ d
 + 5% ? ^

 S _	ff )_
 !'m
 	ff"\5
 3 #';N

 ?
 ?% ):
 MN

 	
 ffF+

 ff ffH
/ ) ff
 	5 ?%
 ?g 
a %5#
 *F
 _	8/fiF- 1#&%&'$% / ;6 $
 * 

 +%V'	  ff. (R/ 0 9M,*%
  g (3 % 	(. ! 	5!
0 _ff	 Zg ?%
 ? 
  ( !:K#m
 	g
 M
 	
6 $
 * b f+%V'	  ff". ;/ 0 9M1KNW;	 ?K%
 ff m3
 	;3 fi
 O(R3
 	D
 

 G,% ; _ff
 <_
	
 +
 N

 $  *2 %
 #c

 ffy
 !
 ff*7 #
 5# 
  
 " /
  

 8
 + #
!
 ffS%
 # N73 fi (_
 ?
a%
 -1 ^  
, m
 !
 	ff,5 %g_T#
 5# 
 YRf 38ff
 53+ m
 N
 # F
 %
 
&p
 + % ffN

 ff* fi; M
 ;: d	 ?!D
1 f3TN

 ?
 2 %
 y
 +N

 ,	T
 	
 5fi/	 ?!
, 
\5
 3 #'N

 ?
 c/
 ff4p
 /fi5N

 ! 	 c

 ffv
 ?#
 p
 /_
 !+ 	(
 V
 + #-1

>R

>R

R

>R

	

*ff

DC





	





3  .1032 M: ?4 
. U8 K4>:A+ +- 4 %
)z{o #w

	

zs q

Wq

q

s

OC

z

ff

S8 M:A'74>:  '
z o

	

 ; 	
 $
 /)N

 /fi53 F  N

 ?
 ?!8
, )#&%&'$% *
 U 5K:
 R?5
P'H4
N??%^_	ff`+Mff-1%fRffc;W	_?%ff`:+%?2_ dT1f
#&%&'$%fi* U 53 ;: ]  3N

 ?
 `m
 	`
 + M ff +Q %]:
 " _	ff*h
 (fi?

 *  

 //m
 	&
 + M ff-1 _T
/ ff
  
 &#
 Q  &*
 U 5f
   
#&%&'$% / Sm
 	Fff ff ff-P3?# ffQ*
 U 53 ?ff* ffQ5= S b<3 
 ?
b#
 ?T@ffC @ 5m
1 fT ]3 
 ?]	?ff*H  #&%&'$% *
 p
U 53 K 
 ffvS N

 T;
 ff
: % 
 / ff"5"  N

 ?
 ,:
  2fi 
 ?(_
 !'m
 	ff`LR1d
@  %LR1
b#
 ?7@ffCVM@ /: W+ % ?^_ d ff
/
 + 5% 7 %)$ 3!
 
 ?f%
 	R P3 fi
 ?+
 /fi)
 #&%&'$%L/ ,-	^ %2: 


 
 '$% fi 
 ?T ;N

 ?
 D3W%	 P+" 
 
 /

	

	

AP

	
P

DC



h5j*i

fiff

P

fi } 
	

fiff 	

 	 

|/|

/}





30 {}

	b#S@*ff5%f;\53#'7 	

ff
N	#]

/fi5
N!	"ff]+



ffff/%WV

	M 6$*n9&%# 
h#&%
fi
 ?!,%A\53#'ff
N	#*V
2
  (/fi5N

 ! 	K%
 {
 !
3 +i

 * /%+ 	M
 %
 #  3 fi 
#&% fi  ?;6: 
  R5# *%
 /fi
- 1#&%&'$% / j9 %m
 ff 
 5
  + _
 Q
 ff 	
 RV
P 
 #
 KV
 #
 S\5
 3 #'
M ?!1

>R

 	5<
/ F$
 # # 
U N

 	 ?!1<_T!
 M=+& v
 #
 3R F!
 #
 Rff #-,K 3
;(dU_ffffp?#!1KNWM	)+]
%b77 _	ff7
/c5#A#"7!:/fi(
#&%&'$% / 3^
  ?c	
 ?!1

DC

	b#S@ffC5%f

!:

#&%&'$%

/fi=

  #&%&'$%

*U 53Q/ RV
ff  \53 #' N

 ??
#&%&R6u'$% (9;6: b=3  j9
 
 #&%  M'$% 6: 
3j9 %%  + %
  _P1  %_
 ff3 
_@ d2@7ff $
 # #
 Hb
 V

 /ff ff ff5< "N

 ff	 

U _	f
 f* 	?+ N
 53+bS
 
 
?
 5 #
 !1
/

P

	b#S@I*%f]!:/fi7A#&%&'$%

RV
U 53 	?ff*u/
ffp%^%+% T1

h5jo

	b#S@ 5%f

	

?ff*

5



 !: /fi=#&%&'$%
*U 53p	?ff* V

 /8ffR%F%+%2dT1

fi~}~ 

~/]{}/}

DC

#&%&'$%

	

 *U53: 
N??K
ffR	ff
 

 #Z	*!,	RB?3#' #R3D
 +1yf
 +5#'Q%*	?+ /"6:3
!3
 3$ffy3+45/%+	M7_@ d2N
@ ff ff ff<5= S

  
U _	n9Wfi/
%#R
#3%$##f5%T%5 ]3 %S W5
 S%
 5 J^H
 
b g_
 !+ 	ff1KNW
 # #-,
:N/7%#'$%  L` " M
 F$
 # #
 5% p
 R
U N

 	 ?;
 D%$'H& N

 ?
3fi	H:m/) ff/%# J^ S%
 # J 1 
  	M
/
 ,5 T %H:
 m+ %
 ?A
_ 
	ffi)'HPF		 %  /)68_2
J "d2@9%
 5 /3 / (
 fi]_
 !'m
 	ffS 1#&%&'$%
*U 53 !1/NOA J N

 ?
 
 + ?Q1
 SN

 ?
 Sm
 	"
 ffB< 
U _	5N

 ff* c%
 # 
dAeTfa

 -
, %^ ?+%
  
 3_2J;3f
 ff
 P_
 	ff3! ff ff-1
f( _	ff?_!'m	ff;

5R

P

	 
	

	

b

 / 

f3ff#R?
N/fi53+3b+Wff3%ffHbR3	,
N??

8*	?+?!,!fi%#3
N??f433#[?%#-1

!#" <38xr/o>+?82 @n :A'  +{z{o5B
fA	ff:K	b*	5##'RR3	R3?ffP57E_ffY1!8G16W@ffBBC9M,





>R



?
 5#/
 
 d+
 + ff
 !ff#_ff/^> YH5
_?N??fiM5 ff268>7^ff UR5nV
#i:"Y5%3+	&5 "?n9R ++ff


  5 ^_ 68>7'? +ff-,2@ffBBC9N `NWf$57Y
6:
E _
 ffG,$@ffBBI*,	J%LL@9M1 
  5 ^_ ?%A2?fiMc+b;Rfff3%#	ff
4,{G1 1#,

 ]% fiN

 ff<+3_
 fffi 3 %	 &
 S %	 `6 ff3 % #9M,k%
 	? `NW$
f 57Y UR ff
3^ +b`+4
# ZV ff3 % #`% %/
 ?!,-%
 5  3%ff3 % ff=+p
 "5
P_
 	Tg 	c ?fiM
 S +
b 
68W
f  %fiff W
f ?_,$@ffBBI*D

 3  k
 #
 ff-,-@ffBBC5D
E _
 ff f 	[ 	
 +
b G,-@ffBB 9M,
/
 +#
 ff{
"
 T/
 ff"kNW #   
 
 )
 5
S

 468f 	[ 	
 +
b 
 5 %,$J%LL@%D  ff{
 5 
f 	[ 	
 +
b G,
@ffBB 59M1
f2
/
 +
 N

 $  *g ?%
 ?H 
  5 ^
_  `NW$
f 57Y
, ff3 % ff"+; 3]/fi_	ff,R 	M
c
 2
 ff
 53+ 	 g: f 
 43
 	D cN

 ?
 ?A
 * 	?+ 
 ?;?fiM5 
ff
 53+ 	 fi`3 
 ff  	/fi5% ff #
 ]_
 ff#71 _ ff3 % ff fi
 5  +
 vfi
 5 
+
 M /
 !d
 ff ff #-
, ?ff* ff {
Y ff #J51 h5b
, 3 W!/
 fiM!0 ]6W@ffBBR@9cH
 b& 
 
 

 ?#-1
^ P %%
 +N

 fi
 5 ?)+F
 +R	 3% #p
 M  #	 
 3 +S_
 7
 ff:
 W
 
 
 ff{
3
 	1  
 +  ,) &&
_  )WN 5 )N'VW
d #
 5# 
 6 fi 
 +
b 
 5 %,7J%LL@9M,T%
 5 
fi	3 ?+R	
 3% #N
 M ; #P+7!3 Z	 % #R
 M ; #-,j 	 K!3 Z	 % #R
 M ?
%# 
 fi5* 	ff
 $ 
 /Rff (%
 # R?_ff+
 T fi
 !g!3 !1NO
 M ;% 3W/
 ?83 +
2
 	+V'
 ff#
 Z	  g ?-,fiD
  ffP_& )WN 5 )N'VWB
d M m3D
 	_ffff*K. 5 5b0
HbR^ ff%$2 / fi !P!3 !,%
 5 &	 <_
 ;
 !m
 ffB
 
 <? 5 #`%
 # 

 fi5* 	ffB#
 Z	  _
, 
 $ W
 &Rff 1;
Y 

 3 fiM#, /R/
 M

 % #
 M ; 	!,
*
 R ff;5 
P_
 	 	;  ff{
 5 ;6GJ%LLL*]
9 Sff]c/
 3K:
 g %
 
 ]3
 	
#
 5# 
 S 3)/fi_	ff,R 3 #	[ ?K 2

 
 

 
 
 $
 )
 ff\5
 ffN

 ff*]
 ;N

 ?
 T%
 5 ]

 +
_d% 3W/
 ff5F
 ! 	p
 M  M	)+S_
 d!ffF  ffR/
 M

 % #p
 M !!1
H %$ 7N

 ff* #
 ffvfi
 5 ?(+N
 
 
 43
 	
 
U 	#
 #H :
 M

 % #F
 %$
!3 cN

 ff
P_
 	5O
1 52
 H /

  ?+
 2%
 *` ?fi
 5 ?dfi;;
 * 	?+2:
 dV

 `3 	
 3A %!,3 #b 2 P!3 	 !3 Z	 % #p
 M d #v
 #
 5# 

 f5 
 ff{
d 2J6XdA3 fib
W#	 !+!,-@ffBB;
9 _  6 5 3 +
b G, `	[ !  %,e^,
 5 %,@ffBC9M, !"
A
 T 2 	5N
 # 5# 
41NW"
 	5N
 #
 5# 

 g#/ ^/
 5+g !u
  ffM ?g

 ff

/^

b`
>

d



b`



a

Qd



a

Qa

d

f

b`

b`

a

f

!d

f

&f d

&c



	



` b`

ed

` b`

f

c

fi

d 

f

Ua

Sa

h5j5j

f

d

f

ff

f

fi } 
	

fiff 	

|/|

 	 

/}





30 {}

S^

_c)*	?+%%3?5+5(%# "	ff*)	5%12YR/ff\5ff*#FffvM?
fi
 ff
 /
 3ff 
U N

 	 /
 ! !m
, 1 1#,f/!!`# $##pU
N	?
T 	ff=5v
 !
 #
 #v
  ffvM ?!;
1 f3(/
 3^
 + 5
 ( $
 	3%#:
3
 	
 N
 7;
 ff/ %m
 3F
 %
 5R:
 g W
 
 S3
 	N
 R ?F%
 5 N3!{
, /
 ff
 	5G,
N

 ff%^3 	5
 * 	?+ D
 
 _	 ?A8
 
 
 /H ff* d$
 	3 % #-1
fff
 ff*
 fi
 5 ?2+v
  
 &3
 	
 
 %2
 	M

  R	 ff
 F 3W
 fiV

 53% S/
 37( F+ fiM 	5 #
 5# 
4p
1 fff ff*#Q!
 ff#
 _ff 
 
 3WV
 	F
 # 5# 

 Ad 2JVWYRf 6  ff{
 5 %, 3 5 -,Rd
> ff !b,
kR
 +
b G,J%LLJ]
9  2YRf 6  ff{
 5 %,
ff ![ff  ,
3 5 -,J%LLJ9c
 ; +%V'	  ff<m?#
 * ffQ
 	5
 #
 5# 
4
, 

 3 fiT+3 /
 
N

 	 ffN

 ff* ffp4_T# 5# 
 f <Y4? 5#_
 ffv 3;/fi_	ff1
NW
 +  <m
 ?#
 * p	3 ff 
 N

 $
  *v
   $5
 +  6 ff
Y fi/,@ffBBCp

9 
#
 	M
 %  ff	 3#  	?6XYR5 fi/
Y 

 	ff,;@ffBBB9M1 NW
 +  <m
 ?#
 * ff
 Q_
 	ff 
 ff
3 +
 4fi fi53* (  	5
 # 5# 

 N

 	 ffN

 ff* ffp
 M c ?fiM
 
 fi
 5 ?%5 
Y  dN 
6XdH
 ff-,R@ffBBB9M,  6  	, A5 
 -, i%
_ 
 3!,5@ffBBV
9 7f)_^N  68W
e -,Y5R	M3 -,
+?[ #
 G,K@ffBB9M1;_ fi fi53*2g ;m
 ?#
 * ffQ
 	5p
 #
 5# 
 2_
 	ff&
 ffy3 +p
 = 
 ff{
* UR2FR/
 M

 % #
 M d/
 !W
 ff ff #6 
P_
 	
 	
 5 %,kJ%LLL*9M,
 ffWS/
 3
: ) dM /
 !f
 ff ff #
 # 5# 
 2YY"? 5#_
 ffp 3;/fi_	ff1

`

f

;d 	

fi

ba

f (d

`

U	

d

`

d 

f 	

I`

`

>d
c

f

d

`

ed

f

!  z{q('58oz ' +- ?s?4z{oz '74>:#srsz '*'




fi5#%Md!fi%#=
N??Wff53+	%ff_	ff


Nff

%"/3	Q

#[	=da6!	ff

+

#/3+5#	#g5?6!	ff+B/#+

+ff4:=3	,

(i5

]#+

B-9M1

>7'? +ff-,7@ffBBC9S%#

fv_	?T8#%

B
W 
     @E@


P/c?7'H]#+5%ff7_	ffpURff#ff#v+ffQ6:+%V'	ff V(V!/*9M1
Y 


 3 fiM#,! Km
 ?#
 * ffdff3 % # ]!!
 5d
 ff
 53+ %,
 ff
6

R96
6
R9 56 6
	
R9  6
%9+;
9 `ff35p
 kR
 +
b G, 3 5 -
,   ff{
 5 46GJ%LLL*9M,
 5?( < ff

 	53 #' dM S6 6
R9M/
, G1 1#
, M c
 	5%
 j%
9 pff3 % #
 d!!
 5
6
	
R9  6
%9M8
1 f3]ff
 53+ %
 3m!:
 M
3 % #]k
 Tk 2N

 ?
 ?A
 ffF

  5 ^_P1
m??N5  .  ff # 0gN

 ?
 ?;
 * 	?+ 
 ?!]
, +N

 &.  ff #
 0gN

 ?
 
 RV
 	?+ 

 ?;23 	ff /%+ 	M
 S	 i_
 " %
b ff *+& !*!;
, 5   #
 %/
 3 #'
6'. /%+ 	M 3* 	?+ 
 #T `
 	S	   +N

 ! <%
 #  #/+ 3;
 	fi* %
 0 9

 
U _ff ff?"6  /%+ 	M 3* 	?+ `+= 
 	R#
 #R
 3R
 53"+= 
 	 *9
6X
Y #_
 	55 %+[
?
[ 3 -,@ffBB 9M1

B

B



DE> H B   @E@ ED > H B
MC
  @E@ DE> H B   E@ @

b`

B

  @E@

?	

DE> H

SR

f

DE> H

SR

ed
ff
! <38xr/o>+?82 W. qU8qK4>:A+s  z{q('58oz '
 fi%#4)ffp/f4Q`57du/6u)+ d	ffA!+!,kJ%LL@9%%^!3WV
Z/	%_	:M
4	M
^K#3R3fiM

Z7[M\ E*V/HKJ>EGXUNUXAYV L [N V&'$% h
 !   6	#+ff`

3V!jU3n9%g	ffF+7_^

#[	ff-,ff###'1NULGFV HKJ>EGXUNUXAYVbL [N V6#&% $

6	#+ff  V!jU3n9c%/	ffP+&_"
jU
#[	ff-1yf
` 57dl/v3Rfi53%"]! :

N?5]D!?T]=3	,VD/)%$#&% ?'$% S5!MC 3
!#P+"3%k	=_P3	fiMffW#Z	*!1(_Tfi53%fi5`+F!fiRV
%R!gffN/)3F5/cWfi?	F1
` 57di*UF5ff/5
/F%#ST_?+#&% ?'$% 75!MC ;\53#'S
N?T:(
N/fi5P!?
KD_
C 	ff*(?fiM	5!1
h5j 

fi~}~ 

~/]{}/}

`

_ #	M%#?+=vfi?&	P 57d *U 5g
N	%# fi"	N+fiM
T
!fi%#i
N??;ff  M?fiM,)5 Dff#v!!5ff,m3	4

N  "	 ffQ
 + 5* P %
 	ffQ+=_
 S %
b ff *+`F!*!,K]5!

N??
f _ 68
> ff 	ff, b% 	5+-,
m		ff,(J%LLL*
9  _T 
 + ff %% # %% #
 =68_
2936 m5%[ffG,
Y5
 fi?!,
	?5R,	J%LL@9( %^
P/  d!! 5p
 4N

 c+
 ?fff3 % #
 d_	:
 M

  1
5W 

 #	[ ffd!!
 5R
 3!fi, m
 ! 	ff, H# 

 % g
 5
 
 
 P3
 	1NWR
 # #
+/ dfi?S	A  57d * 
U 5$\5
 3 #'"N

 ?
 , 	fN

 $
  *
 ! ?)N

 ?
 ?
fiM #
 Z	  768N

 ?
 52 )3+ 5#	 #
 *5
 
 
 ?k_
 
 
 $9M
, M (
 	5%
 
68N

 ?
 5
  3 fi 3%S3 	ff
 
 $9M
, M #	[ #	[ 78
 NM d!;68N

 ?
 5
DN

 	3 	 #'p
 =	5+ %/
 3 #'"g3
 	ff=bR^ ffj9Md
1 f?N

 ?
 ?Tm
 	/
 ff
+
 !fi 
 % 7 d?# A ;d 2JVWYRf 
 
 33
 	v
 #
 5# 
fi6  ff{
 5 d
 !W
 G1#,	J%LLJ9M1



>_ O^
?d

Wd

>R

`

MC
1`/`

b`

e`

fi

f

! <38xr/o>+?82@:A'58qU:qK4>:A+s

f % 7

 3
 3 #?[ % #/N

 ! R)ff ^_
 	ffN
 /fiF_
 + % 3+ 	 ]S% 
 #
 3b??fiM5 S:
 H

 *
?fi5!1]f3;??fiM5`ff*5%ff5
fi5#/p	#+/cT
/d	_ffff*Hfifi53%	?
%
 
 +
 Q	_ffff*;fi fi53%	 p 
 $
 ;W
 
U 	#
 5%+
 % B
 #
 34#6 
b !,c@ffBII*D  	,
52,   !b,@ffBB 5D T*%
 -,J%LLL*9M1
fP
 3
 3 #?[ % #3]
 
 #
 3f?# W!_
, m
 !
 	ff,-
 
 ff=#p
 ff ff*#
 +N

 D%+ ff* #
%#  F3 # 	5% #Q^% &

 
 6X;
d fiM-,

b 3 ff,
Y 

 ?	M

 -,H@ffBBB5D ff5
 -,
W5
 + ?-,
E 	5,J%LLJ5D
> ?
 
> 5!
 ffG,$@ffBBC5D
Y 
/ , ^
 5
SN

 V 5#
 ,
H{ ? ff-,
J%LL@9Mm
1 fT
 3
 3 #?[ % #S
 
 #
 3]?# ]5

 fi5#D
 	
 ?A:
 ;	
 $
 ?!k_!+ 	f
 + 5% 
/%+ 	M/+7 ffS	ff, ff
 %	 ( )N

 /fi53+;
 /%+ 	M
 !{
,  ?;/%+ 	M
 ! 	  ,
 ff
 %	 /%+ 	M ff#   
 $
 /:
  W%
 %W!V #7\5
 ?+ #
  R1 fff ff*p
 * 	?+F
 
]
 3
 3 #?[ % #&f
 
 #
 3?# 7;
 R/ff^
 ffQ5& ]: ff
 	%
 ff

 `5
P_
 	R
N

 	 
U #'"K% S

 
 R
 ?# !1
f?	5* 	?+ ffy`N

 /fi5" 
 3
 3 #?[ % #3N

 ! R 
 $
 ff` 3f/fi_	T%
 # 
 	] 
 /
 3
 3 #?[ % #;N

 ! RK	 ;;(
 3
 3 #?[ % #_
 
 
 /K_@ d2f
@ ; 
+
 *(H b5 
P_
 	 	ff,  ff{
 5 %_
, E!+ 5 	ffb6GJ%LLJ9M1

d

ff

c

d


a

d


d
MC 

	

`

R

@c

b

Sd

	

P

f

K

   !]fi  

f3/fi_	d?ff*3ffmQ3	B#5#


*!5%ff

*+3ffQ+ff

bR^ff(3	RR?!1bf(3!#RDff
N/5	?#$*f
N$

  _ff # %
U _	WVX
 ffN 
 S3
 	D
 ; q#&%&'$% / 1NO F

 
 fi* %
 ?mfi
$
 #/
 3 #';+
  TbR^ ff%^ _	ff*( !
 ff3 Ak
 ff
 	53 #?[ % #Q#6 5 !
 ff35]
 
k( /fi5N

 ! 	"c   
  3
 	 #
 5# 
"9;
 ff   M `\5
 3 #'iN

 ?
 
%N
 ff
 
 ?;  #
 #
 \5
 3 #' M ?D3 +B
  ff
 53+ F
 
 
 i3
 	
R
 ?!%
1 f/fi_	fi
 ?^ %W
 
U _	!0 W*#
 ffN

 ff*
 ` R #p
 R ?W3T
 ff ?fi
: %! ?+8V
  #
 %	 7bR^ ffc
 ff
 	5% #-1
f
 $
 ffv
U _	WVX
 ffp
 
 33
 	v
 R ?)
 3+ ( 7:#%
 N
 + 	/!
	 ff
 	5+
 ,%% c	5+ W
 D	/fi5% #-
, 
 
 /3
 	{
, 
 
 
/
 !%
 ff ff #-_
, + % 3+ 	 5 fi5 	5#?[ % #=)
 
 
 /!/
, 
 
 3
 3
 3 #?[ % #-, ?WRV
	! % #
 N!fi 
 % #-b
1 f)

 N
 + 	/!,*? 5#_
 ffD/! 
 ; 3b/fi_	ff
, fi%
 
 
3
 	N
 /fff ff #S_
 d/
 !K-#
 	5f
 
 /!,:#m
 ff/5 %+ % 3+ 	 5 fi+V

 	5#?[ % #
  
 / %b
 
 $
  (# +
 5+T m ffd
 
 ? 5 #
 !1

OC

DC

h5j 

fi } 
	

fiff 	

|/|

 	 

/}





30 {}

YR$;#+5W	?ff*fff*%:M
%#%$W/!,	!,3^/#-,
?A8 #v
 3mUR ffN

 ff#F
 N

 $  *;
 
 /%+ 	M4? 5 #-,5_
 ff	 
 2 !F
 ff " 
U _	 
+] 
 S
/ PN

 	 ! P5 fi5 	5#?[ % # 3_!+ 	T	5+ 8
 
 
 /!1 `
 V
 	ff, !
  ?7 d
U _	!0 TRff 7 %^ /%+ 	M
 3ffi
 53% 7:
 ^ 	 ff

%d
 32 
 +
 +# 1;NW
 # #-
, 
 
 &
 3
 3 #?[ % #y
 ff /<	5+ ] 
ff3 %
 #
 /]
/D
 /%+ 	M
 "#
 ?A
 3
 
 
 #
 * m*+; ?ff
 # #
 #'
 "#3 3 fiM

5%
 1
fD?ff* ffBfi
 5 <+`? 5 #
 D #&
 ?
U _	cbR^ ff%
 !
 	
 + 	1
52 * ff* #=;
 c2+4	
 v+
 + ff
  %c%
 
 		3  D
U _	 T	W
 5% 	c+p
 
 D

 ! R#& %P%
N
 8
 ff B
U _	 d< ]bR^ ff]3
 	y
 R ?!1"NWQ
 P
 !7,k 
$
 #/
 3 #'/
 ; d #F
 R ?f3f
 fi* %
 P 3fi
 5 -1


a

  
 



 

/

 

&a

f3$Hbc;$ffc5cF(dm%3


 3+^YR	ffmff5#,5YR#ff3
	%#-,HYR	ffv Y{$!,8
8ffB ff/f%
 
fWff	3#YR$ K: A?HdH
N_!##ff?!_
_?   *	53P6:NYAVM@ffBBBV
@@ *B 9M1 E fi& 5% !82+ T 5 
> 5+  	 B
  uk
33ff YR
=:4?43%$5%#
QFU_	5N

 ff*  
 fi ?fi;3?53+b&
 
 ! ff #-,K+^
 ! 	 3 5 :
 D3
3%$ 5% #
 S 
 #
 3Kk _	ff*H\5
 3 #'/N

 ?
 ?!
, ]+;
f ! 55 S
E !+ 5 	ffb: 

 R
 $
 * 	5(+S ff3 % ff4H
 p
b 
 3
 3 #?[ % #-1

a

3+&



O ff

c

DC

f





f

>R

a



	

    I  S/

Sd 	?5R, `716GJ%LL@9M1;`fff	5bK-!3Z/	5K5Nff3
%
ff*2!3Z/	5!17NWWL J * L VGE E X + 1LONUX  7X#[M\x+N VO\A\ X * V,+GV
	ffWL J GV VfiMX + * EJ Z NKV?V,+N
 JMLONUF * FVGE*V J/+*Z*VOL V,+GV J/+ 1LONUX  7X#[M\x+N VO\A\ X * V,+GV1Y{5	ff1
d;fiM-,Y15>/1#,Ka b3ff, 1Rf1#,(a
d Y?	M
-, T16W@ffBBB9M1 V [fiMX + * E X +fiX +*Z7JMLC0 [NUX#J/+Y5X EGF([M\ 
X.-*[NUX#/J +1 `a `>d8
-1
dA3fib,q1#,/d W#	!+!,c176W@ffBB9M1 fdfi2J # #5#
41T
[  X +!V V [MCL +(X + * ,,

m5%[ffG,n1#,Y5fi?!,RdT1#,





P

JCR@ RJh51


N	,#+!,O_
C ff#RMP?fiM	ff1ANWWL J GV VfiMX + * E J Z &+H+(F([M\
VOLGX [/+ bE E7J 7X#[NUX#J/+ Z7JML 1LONUX  7X#[M\ x+N VO\A\ X * V,+GV1
fWQ`%fiff!,  1#,?dafW?_,  1k6W@ffBBI9M1)dA3-3	1!T
[  X +!"V V [MCL +(X + * ,#%$j,BBAP@O*C51
f [	f 	+bG,-Y1#,d  ff{5 %f , P16GJ%LL@9M1& VO\ [NUX#/J +?[M(
\ '&[N)[  X +(X + * 1HY{5	ff1
	ff5-, P1;a1#, cW5+?-,c 1 c 1#, d E 	5,_P1H6GJ%LLJ9M1x+*Z7JMCL 0 [NUX#/J +,Y5X EGF([M\ .X -*[NUX#/J +,X +*fiI[N[
0 X +(X + * [/+ fi,++?.J -\ V fi * /
V fiMX E JMYVOL %1 `a `>d8
-1
	ff5-, P1!
a 1#,3d N'5G,	>/1 T1F6W@ffBBJ9M1O52`D3Sg*5WVXfiff %+5#	?W
ff	3#	7ff	5%#-1!T
[  X +!"V V [MCL +(X + * ,0, IOP@!LJ51
	ff5-, P1 
a 1#,]3%!+b5*VWYfi/,Sc 1#,Sd YR
P5-,?1$6W@ffBBC9M1	 
 %P
2+PbR^ff
3	_T	!71NW 1fiMYM/
[ +GVGEX +32+?.J -\ V fi * !V 'eX E JMYVOL &/[ + fi4'&[N4[  X +(X + * 1_T_T_^N

dHff-,	E

1	E

1K6W@ffBBB9M12_

 /J +*Z*VOL ,V +GVJ Z &0

)?!1









h5j5h

fi~}~ 

~/]{}/}

	ff-, ;1#,d Yfi/,
`71  126W@ffBBC9M1  U_	5
Nff*S%# ! $5+ #5#
41 NW
W L J GV VfiMX + * E/J Z NKV ( XALON V V,+Nx+N VOLC+?[NUX#J/+?[M\ J/+*Z*VOL V,+GV J/+/T[ X +!V V [MLC+(X + * 1Ma 5


?fiM1

c
P_		ff,Pf1#,&d  ff{5 %f ,P1]6GJ%LLL*9M1 dHR/M
%# M !!1 NW WL J GV VfiMX + * E J Z NKV
 JMFSLON WFSL J H(V [/+  J/+*Z*VOL V,+GVfiJ/+*WLGX +7X H?\ VGE@J Z,'&[N[ X +(X + * [/+ fi 2+?J.-\ Vfi * V 'eX E JMY 
VOL %1$Y{5	ff1
c
P_		ff,	f1#,  ff{5 %f , P1#,d E!+5	ffb,-f1k6GJ%LLJ9M1(YR33#[?%#-R_ 
N!R
Dfi	3	%#R$	3%#R	ff1RNW)WL J GV 
V fiMX + * EbJ Z N KV x+N VOCL +?[NUX#/J +?[M\fiJML +5E SJ H
/J + x+N VO\A\ X * ,V +ffN '&[N[ &+?[M\ >EGX EeX + V fiMX 7X +!V&/[ + fi3 S[MCL 0 [ JM\ J *   '  ! #	
 #1
c
P_		ff,f1#,Id YRf 
%,fic16GJ%LL@9M1qff +  \ X +!V '&[N4[  X +(X + *  VOLGYVOnL 1K^` >R	 H +f b )NW+#,
*+ _j
_ 
!1 1 ff1
cW!/fiM!,3H	 1g6W@ffBBR@9M1d;5p
/3
N_!4ff	53#[?%#!132+?.J -\ V fi * V fi7FSX EGXUNUX#/J +
 JMFSCL +?[M\ ,,hCR@PRh%LR1
cT
-,  1#,3c fi_	ff,_P1!
a 1#,Wc 	ff,kY1_P1#, d eT3%+b5,!
a 1$_P1K6W@ffBBC9M1dH+WVO_C ff#ff?c
?
Nff*]/
%ff
Nff*F53+bP#+5!1  JMFSC
L +?[M\?J Z &0 VOLGX /[ +  JM\A\ V * /V  [ML fiMX#JM\ J * %,
#! ,@!LJ%L P@!Lh%LR1
eW-,Af1#,^Y5R	M3-,AY1#,d  +[?#G,5126W@ffBB9M1 _ ff*"	5 #5#
Yfi	3ff
+3UR7!3Z	%#-1PNWWL J GV V fiMX + * EJ Z N KV
fiJML +5E SJ H /J + V [MCL +(X + * ZGL /J 0 ?VIN
 [N V * JMLG.X -*[NUX#/J +1
fi+bG,  1#,/d  ff{5 %f , P1P6GJ%LL@9M1 dA3Z	%# M&?fiM%# _&)` NW5b)` N'VWdT1 NW
WL J * L VGE EbX + 1LONUX  7X#[M\ x+N VO\A\ X * ,V +GV
	WL J GV V fiMX + * EeJ Z1N K,
V ?,V +N / JMLONUF * FVGE*)V  /J +*Z*VOL ,V +GV /J +
1LONUX  7X#[M\ x+N VO\A\ X * ,V +GV1Y{5	ff1
>?
4,f1	_P1#,a
d >5!ffG,eP11k6W@ffBBC9M1  33#[?%#ff5\5?(:^
;3fi7%%/?!
S
N/fi53+-1  KL /
[ +KE7[ *NUX#/J +KE /J + 2+?.J -\ V fi * V /[ + fi3'&[N[q + * X +!V VOLGX + * ,(0,BJhAPRBh51
>ff	ff,1#,?b%	5+-,*Nn1#, d m		ff,eP16GJ%LLL*9M1k_T*!5%ff]	g:g
#5#	53%5b
7%
#5#
!1 NW
 (   #	

fiJML +5E SJ H /J + V7N.[  V [MCL +(X + * 	 FSXA\ fiMX + *
1FKN/J 0 [NUX 1fiMY5X GV ?NUL [N V * XVGEZ7JM4L T
J fi VO\  VO\ V *NUX#/J +/[ + fi)V7N S
J fi /J 0 OX +?[NUX#/J +1


























>7'? +ff-,KE

1T6W@ffBBC9M1



U	#5RF_

1fiMYM[/+GVGEQX + 2+?J.-\ Vfi * 
V 'eX E



#/%+	M




#3+5%!3	
+ + * 1 `ffN )?!1

JMYVOL  [/+ fi)'&[N[3 X (X Ka



>cffb,Nn1k6W@ffBBh9M1HNW#d ;ff?3=?fiMN4
Nff	-3%3!1

x+
* ,+ , !,hR@I RhhI*1

3+*!1

HIH?\ XVfi 1LONUX

7X#[M\ N VO\A\ X V GV  OP
ff{5 %
d f [	f 	+bG,Y16W@ffBB>59M14x+ fiMF*NUXAYV"3J * X WL J * L [/00 X + * 	"?V ?+(Xfi7FVGE [/+ fi HIH?\ X [.
f , P1#,!a
NUX#/J +KnE 1  33He^H5R-1
ff{5 %
f , P1#,
	 35-,1#,F>dff f !b, T1#,d kR+bG,  1)6GJ%LLJ9M1i_Tfi&!3Z	%# M
#+d/3	1	NW,WL J GV VfiMX + * EbJ Z NKV  x+N VOLC+?[NUX#J/+?[M\ff J/+*Z*VOL V,+GV
/J + '&[N3
[  X +(X + * 1
ff{5 %
f , P1#, c 
P_		ff,	f1#,!d M!,H16W@ffBB9M1)_ ff!fi"#	):^+M#RnV
#-1  x+N VO\A\ X * ,V +N  >EON ,V 0&E (KVOXAL HIH?\ X [NUX#/J +KnE , ,!%ff L PS
ff C51
ff{5 %
f , P1#,/f ff![ff ,3H	 1#,3d 	 35-, 1F6GJ%LLJ9M1 2` YRf f` ff3%#b&3	`
/5+WVXM	c?%S+M#-1NWWL J GV V fiMX + * E J ZN KV - VO\ ZON x+N VOCL +?[NUX#/J +?[M4
\  /J +*Z*VOL 
,V +GVGE&/J + x+ fiMF *NUXAY4
V 3J * X WL J * L /[ 00 X + * 1Y{5	ff1














NW



h5j 

fi } 
	

fiff 	

 	 

|/|



/}



30 {}

d  !b,  1eP1g6W@ffBBff9M1  U	##"33#[?%# =bR^ffS3WV
WL J GV V MX + * E J ZeNKV  XAL EON x+N VOLC+?[NUX#J/+?[M\! J/+*Z*VOL V,+GV&J/+ 2+?J.-\ Vfi * V"'eX E JMYVOL 
[ &[N[ X (X
 	, ;1#, A5-, T1S
c 1#,Sd _%3!,1 a16W@ffBB9M1>7^ffVX/ffS?fiMDU	#5%+
	ff  ?fiMRM?H+NffmRff*(	fiM	ff	#'1T[ X +!V V [MLC+(X + * , j,	JR@IOP
J>
 LR1
a 3R,eP1#,!d k#ff-,eP16W@ffBBC9M1&52#5#
 :)D*	?+ff*ff?!1
NWWL J GV V fiMX + * EJ ,
Z    VOCL +!V7NUX OE&/[ + fi >EON ,V 0&E   $j1
a 53+bG,(7` 1-Y1#,`a [	! %f ,-Nn1#,	e^, 1#,?d  ff{5 %f , P16W@ffBC9M1%fP
#ZV	$ff
NffRV
H?fiM++ff
 _  @*
ff  #7?+ fi	3	%#<Q	"
Nff	g
!1"NW
WL J GV V fiMX + * E&J Z N KV  X ZON Q[NUX#/J +?[M
\  /J +*Z*VOL ,V +GV&/J + 1LONUX  7X#[M\ x+N VO\A\ X * ,V +GV1S`a 4>dV
;1#, 52,-eP1  1#,
	1FNW 
fi
/+ fi3' ) + + * 1



	,eP1




-1

)+!,>	H1#,Md
V [MLC(+ X + *

	ffA!+!,c156GJ%LL@9M1 `)	+!3Z	%#2:
Nff	3bff*
Nff*!1 T[ X +!V
#,$J%LhAPRJhR@%1


,

K`  d

YR5fi/, 71 1#,
Y	ff, ;1k6W@ffBBB9M1gNW
Nff$5+D#5#
A;RffV5%ff
ff#!1  + "
C+ + * , !,$JBI RhhC51

d



T[ X !V V [ML (X

OP

ff
SR
WL J GV V MX EfiJ Z N KV  XAL EON N VOL ?[NUX#J ?[M\ J *Z*VOL V GV@J ?J \ V V
eX E JMYVOL  [ &[N[ X (X
Y
/MC,*Y1 1#, ^5
S
NV 	5#,
a 1#, d H{?ff-,a1*eP1	6GJ%LL@9M1WL J GV VfiMX + * E J Z/NKV 2"'4'
#	
  fiJML +5E SJ H /J +3X EGF([M
\ '&[N)[  X +(X + * 1
kR+bG,  1#,S	35-, 1#,Kd  ff{5 %
f , P1-6GJ%LLL*9M16)ff#%_	:M
^m?#*ffFff3%#
!!51&NW WL J GV 
V fiMX + * EfiJ ZN KV  JMFSLON  WFSL J H(V /[ +  /J +*Z*VOL ,V +GV@/J + WLGX +7X H?\ VGE9J )Z '&[N[
 X +(X + * /[ + fi32+?.J -\ V fi * "
V 'eX E JMYVOL %1$Y{5	ff1
b!, 1E 16W@ffBII9M1  GH?\ JML [NJML  '&[N[ &+?[M\ >EGX n
E 1g_T3+"E?!1
T*%-,fi_P1 6GJ%LLL*9M1
3X EGF([M\ .X -*[NUX#/J + Z7JML fiI[N[ 0 X +(X + * 1
*+ _
_ ))P@%1 
%-1 ZV
+	1 >

_ 	j*%?j_ _T*+*R_%G_  3  fiff>c?fWffffJ%LLLR1 -O1
YRf 
%,c1#,?
c 
P_		ff,	f1#,?d >5+ 	f  %,3c 1k6GJ%LL@9M1)dH
P/;_	#[	ffFp_	#[	ff

5;?fiMF #3^F]dAeTf /%ff*c%%/1cNW WL J GV V fiMX + * E J Z WX *  N 
 /J +*Z*VOL ,V +GV/J + 1LONUX  7X#[M
\ x+N VO\A\ X * ,V +GVeX + V fiMX 7X +!VQX + WFSL J H(V 1-Y{5	ff1
E_ffG,Y1g6W@ffBBI9M13_TB#5#
 :
#ZVff3%#K3	&/!1/NW WL J GV V fi.
X + * E J Z&N KV  XAL EON WFSL J H(V /[ +  /0/HKJ>EGXAF 0 /J + WLGX +7X H?\ VGE J "Z '&[N,[  X +(X + * /[ + fi,2+?.J -\ V fi * V
'eX E JMYVOL %1$Y{5	ff1
Y#_	55%+[,_P1#, [?3-,%_P16W@ffBB 9M1 52 ff#^
N??FV*	?+?);bR^ZV
ff43	1=NW*
fi + *

1x+ C+
/+  /+ ,+ /+ 2+ .- fi *
'
/+ fi3' ) + + * 1_T_T_^N1)?!1







E_ffG,Y1K6GJ%LL@9M1NW#;#5
S
]:2bR^ff/3	  =%%/?!17NW

/+ (' 3 + + * 1	{Y 5 	ff1

VO\ [NUX#J ?[M\ &[N[ X (X
E_ffG,Y1#, d f [	f 	+bG,kY1K6W@ffBBff9M1fN   ?5#`?fiM	ff
4/k;fiM]ffRV
	5
/RffZV'!ffk##4g%"
3N  )1^NWWL J GV VfiMX + * E  [ * LGF5HIH(V,+NUL VfiffbV,+
T[>E  X +!VO\A\ VG4
E VOCL +!,V +1 T#1fT
-1
E_ff	Y1#,E!+5	ffb,Rf1#,  	bfi
/,*_P1Nn1#,5Y!_?!,%_P1#,Ia 3R,eP1#,>c;%b5bffG,IH
	 1#, d >7#{?WV
ff-,E 16W@ffBBC9M1 W	*	5##'4	33fiR	%]
1HNW WL J GV V fiMX + * E J Z
N K
V /VOCL 0 /[ + fiJML +5E SJ H /J + T[  X +!4V V [MCL +(X + * 1 T#1d;ff
S#+[1
h5j 

fiJournal of Artificial Intelligence Research 17 (2002) 309-332

Submitted 06/02; published 10/02

An Analysis of Phase Transition in NK Landscapes
Yong Gao
Joseph Culberson

ygao@cs.ualberta.ca
joe@cs.ualberta.ca

Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2H1

Abstract
In this paper, we analyze the decision version of the NK landscape model from the
perspective of threshold phenomena and phase transitions under two random distributions,
the uniform probability model and the fixed ratio model. For the uniform probability
model, we prove that the phase transition is easy in the sense that there is a polynomial
algorithm that can solve a random instance of the problem with the probability asymptotic
to 1 as the problem size tends to infinity. For the fixed ratio model, we establish several
upper bounds for the solubility threshold, and prove that random instances with parameters
above these upper bounds can be solved polynomially. This, together with our empirical
study for random instances generated below and in the phase transition region, suggests
that the phase transition of the fixed ratio model is also easy.

1. Introduction
The NK landscape is a fitness landscape model devised by Kauffman (1989). An appealing
property of the NK landscape is that the ruggedness of the landscape can be tuned
by changing some parameters. Over the years, the NK landscape model itself has been
studied from the perspectives of statistics and computational complexity (Weinberger, 1996;
Wright, Thompson, & Zhang, 2000). In the study of genetic algorithms, NK landscape
models have been used as a prototype and benchmark in the analysis of the performance of
different genetic operators and the effects of different encoding methods on the algorithms
performance (Altenberg, 1997; Hordijk, 1997; Jones, 1995).
In the field of combinatorial search and optimization, one of the interesting discoveries
is the threshold phenomena and phase transitions. Roughly speaking, a phase transition in
combinatorial search refers to the phenomenon that the probability that a random instance
of the problem has a solution drops abruptly from 1 to 0 as the order parameter of the
random model crosses a critical value called the threshold. Closely related to this phase
transition in solubility is the hardness of solving the problems. There has been strong empirical evidence and theoretical arguments showing that the hardest instances of the problems
usually occur around the threshold and instances generated with parameters far away from
the threshold are relatively easy. Since the seminal work of Cheeseman et al. (Cheeseman, Kanefsky, & Taylor, 1991), many NP-complete combinatorial search problems have
been shown to have the phase transition and the associated easy-hard-easy pattern (Cook
& Mitchell, 1997; Culberson & Gent, 2001; Freeman, 1996; Gent, MacIntyre, Prosser, &

c
2002
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiGao & Culberson

Walsh, 1998; Kirkpatrick & Selman, 1994; Mitchell, Selman, & Levesque, 1992; Vandegriend
& Culberson, 1998).
In this paper, we analyze the NK landscape model from the perspective of threshold
phenomena and phase transitions. We establish two random models for the decision problem
of NK landscapes and study the threshold phenomena and the associated hardness of the
phase transitions in these two models.
The rest of the paper is organized as follows. In Section 2, we introduce the NK fitness
landscape and our probabilistic models, the uniform probability model and the fixed ratio
model. In Section 3 and Section 4, the threshold phenomena and phase transitions in
NK landscapes are analyzed. For the uniform probability model, we prove that the phase
transition of the uniform probability model is easy in the sense that there is a polynomial
algorithm that can solve a random instance of the problem with the probability asymptotic
to 1 as the problem size tends to infinity. For the fixed ratio model, we establish two upper
bounds for the solubility threshold, and prove that random instances with parameters above
these upper bounds can be solved polynomially. This, together with our empirical study
for random instances generated below and in the phase transition region, suggests that
the phase transition of the fixed ratio model is also easy. In Section 5, we report our
experimental results on typical hardness of the fixed ratio model. In Section 6, we conclude
our investigation and discuss implications of our results.

2. NK Landscapes and their Probabilistic Models
An NK landscape f (x) =

n
P

fi (xi , (xi )), is a real-valued function defined on binary strings

i=1

of fixed length, where n > 0 is a positive integer and x = (x1 ,    , xn )  {0, 1}n . It is the sum
of n local fitness functions fi , 1  i  n. Each local fitness function fi (xi , (xi )) depends
on the main variable xi and its neighborhood (xi )  Pk ({x1 ,    , xn }\{xi }) where Pk (X)
denotes the set of all subsets of size k from X. The most important parameters of an NK
landscape are the number of variables n, and the size of the neighborhood k = |(xi )|.
In an NK landscape, the neighborhood (xi ) can be chosen in two ways: the random
neighborhood, where the k variables are randomly chosen from the set {x1 ,    , xn }\{xi },
and the adjacent neighborhood, where k variables with indices nearest to i (modulo n) are
chosen. For example, for any even integer k, the k variables in (xi ) can be defined as
x((n+i k ) mod n) ,    , x((n+i+ k ) mod n) . Once the variables in the neighborhood are deter2
2
mined, the local fitness function fi is determined by a fitness lookup table which specifies
the function value fi for each of the 2k+1 possible assignments to the variables xi and (xi ).
Throughout this paper, we consider NK landscapes with random neighborhoods. To
simplify the discussion, we further assume that the local fitness functions take on binary
values. Given an NK landscape f , the corresponding decision problem is stated as follows:
Is the maximum of f (x) equal to n? An NK landscape decision problem is insoluble if there
is no solution for it.
It has been proved that the NK landscape model is NP complete for k  2 (e.g.,
Weinberger, 1996; Wright et al., 2000). The proofs were based on a reduction from SAT to
the decision problem of NK landscapes. To study the typical hardness of the NK landscape
decision problems in the framework of thresholds and phase transitions, we introduce two

310

fiPhase Transition in NK Landscapes

random models. In both of the models defined below, the neighborhood set (xi ) of a
variable xi is selected by randomly choosing without replacement k = |(xi )| variables
from x\{xi }.
Definition 2.1. The Uniform Probability Model N (n, k, p): In this model, the fitness value
of the local fitness function fi (xi , (xi )) is determined as follows: For each assignment
y  Dom(fi ) = {0, 1}k+1 , let fi (y) = 0 with the probability p and fi (y) = 1 with the
probability 1  p, where this is done for each possible assignment and each local fitness
function independently.
Definition 2.2. The Fixed Ratio Model N (n, k, z): In this model, the parameter z takes
on values from [0, 2k+1 ]. If z is an integer, we specify the local fitness function fi (xi , (xi ))
by randomly choosing without replacement z tuples of possible assignments Y = (y1 ,    , yz )
from Dom(fi ) = {0, 1}k+1 , and defining the local fitness function as follows:

0, if y  Y ;
fi (y) =
1, else.
For a non-integer z = (1  )[z] + [z + 1] where [z] is the integer part of z, we choose
randomly without replacement [(1  )n] local fitness functions and determine their fitness
values according to N (n, k, [z]). The rest of the local fitness functions are determined according to N (n, k, [z] + 1).
In the theory of random graphs, there are two related random models G(n, p) where
each of the n(n1)
possible edges is included in the graph independently with probability
2
p, and G(n, m) where exactly m edges are chosen randomly and without replacement from
the set of n(n1)
possible edges. It is well known that for most of the monotone graph
2
properties, results proved in G(n, p) (or G(n, m)) also hold asymptotically for G(n, N p)
m
(correspondingly, G(n, N
)) where N = n(n1)
. However, we cannot expect that similar
2
relations exist between the two random models of NK landscapes defined above unless the
parameter k tends to infinity. As a result, the asymptotic behaviors of the two NK landscape
models are significantly different for fixed k.
We conclude this section by establishing a relation between the decision problem of NK
landscapes and the SAT problem. A decision problem of the NK landscape
f (x) =

n
X

fi (xi , (xi )),

i=1

is the maximum of f (x) equal or greater than n?, can be reduced to a (k+1)-SAT problem
as follows:
z
V
(1) For each local fitness function fi (xi , (xi )), construct a conjunction Ci =
Cij
j=1

of clauses with exactly k + 1 variable-distinct literals from the set of variables {xi , (xi )},
where z is the number of zero values that fi takes and Cij is such that for any assignment
yj  {0, 1}k+1 that falsifies Cij , we have fi (yj ) = 0.
n
V
(2) The (k+1)-SAT is the conjunction  =
Ci .
i=1

311

fiGao & Culberson

x
0
0
0
0
1
1
1
1

y
0
0
1
1
0
0
1
1

z
0
1
0
1
0
1
0
1

fi
0
1
1
0
1
0
0
1

Clauses
xyz

x  y  z
x  y  z
x  y  z

Table 1: A local fitness function and its equivalent 3-clauses.

Table 1 shows an example of the fitness assignment of a local fitness function fi = fi (x, y, z)
and its associated equivalent 3-SAT clauses. It is easy to see that for any assignment s to
the variables x, y, z, fi (s) = 1 if and only if the assignment satisfies the formula
x  y  z, x  y  z, x  y  z, x  y  z.

3. Analysis of The Uniform Probability Model
In the uniform probability model N (n, k, p), the parameter p determines how many zero
values a local fitness function can take. We are interested in how the solubility and hardness
of the NK landscape decision problem change as the parameter p increases from 0 to 1. It
turns out that for fixed p > 0, the decision problem is asymptotically trivially insoluble.
This is quite similar to the phenomena in the random models of the constraint satisfaction
problem observed by Achlioptas et al. (1997).
To gain some more insight into the problem, we consider the case where p = p(n) is a
function of the problem size n with lim p(n) = 0. Our analysis shows that the solubility of
n

the problem depends on how fast p(n) decreases:
(1) If
1

lim p(n)n 2k+1 = +,

(3.1)

n

the problem is still asymptotically trivially insoluble because with the probability asymptotic to 1, there is at least one local fitness function that always has a fitness value 0;
(2) On the other hand if p(n) decreases fast enough, i.e.,
1

lim p(n)n 2k+1 < +,

(3.2)

n

the problem can be decomposed into a set of independent sub-problems. In either case the
problem can be solved in polynomial time. The case of (3.1) is not difficult to prove, but
to prove the case of (3.2), we need to make use of the following concepts and results.
Definition 3.1. The connection graph of an NK landscape instance f (x) =

n
P
i=1

is a graph G = G(V, E) satisfying
312

fi (xi , (xi ))

fiPhase Transition in NK Landscapes

(1) Each vertex v  V corresponds to a local fitness function; and
(2)There is an edge between vi , vj if and only if the corresponding local fitness functions
fi , fj share variables, i.e., the neighborhoods (xi ) and (xj ) of xi and xj have a non-empty
intersection, and both of them have at least one zero value.
Definition 3.2. Let f (x) =

n
P

fi (xi , (xi )) be an NK landscape instance with the con-

i=1

nection graph G = G(V, E). Let G1 ,    , Gl be the connected components of G. Since the
vertices of G correspond to local fitness functions, we can regard Gi as a set of local fitness
functions. For each 1  i  l, let Ui  x = (x1 ,    , xn ) be the set of variables that appear
in the definition of the local fitness functions in Gi .
It is easy to see that (U1 ,    Ul ) excluding independent vertices forms a disjoint partition
of (a subset of) the variables x = (x1 ,    , xn ), and that the local fitness functions in Gi
only depend on the variables in Ui . Furthermore, the NK decision problem is soluble if and
only if for each 1  i  l, there is an assignment si  {0, 1}|Ui | to the variables in Ui such
that for each local fitness function g  Gi , g(s) = 1.
Theorem 3.1 summarizes the result on the uniform probability model.
1

Theorem 3.1. For any p(n) such that lim p(n)n 2k+1 exists, k fixed,there is a polynon

mial time algorithm that successfully solves a random instance of N (n, k, p) with probability
asymptotic to 1 as n tends to infinity.
1

1

Proof: We consider two cases: lim p(n)n 2k+1 = + and lim p(n)n 2k+1 < +.
n

(1) The case of lim p(n)n
n

1
2k+1

n

= +.

Let Ai be the event that fi (y) = 0 for each possible assignment y  {0, 1}k+1 and let
n
S
A=
Ai be the event that at least one of the Ai s occurs. We have
i=1

lim P r{A} = 1  lim P r{

n

n

n
\

Aci }

i=1

= 1  lim (1  p(n)2
n

k+1

)n .

1

It can be shown that if k is fixed and lim p(n)n 2k+1 = +, then lim P r{A} = 1. It follows
n
n
that with probability asymptotic to one, there is at least one local fitness function which
takes on values 0 for any possible assignments. We can therefore show that in this case, the
NK decision problem is insoluble by checking the local fitness functions one by one. And
this only takes linear time.
1
(2) The case of lim p(n)n 2k+1 < +.
n
Consider an algorithm that first finds the connected components Gi , 1  i  l of the
connection graph G of the NK model, and then uses brute force to find an assignment
si  {0, 1}|Ui | to the variables in Ui such that for each local fitness function g  Gi ,
g(s) = 1. The time complexity of this algorithm is O(n2 + n  2M(n,k,p) ) where M(n, k, p) =
max(|Ui |, 1  i  l) is the maximum size of the subsets (Ui , 1  i  l) associated with the
313

fiGao & Culberson

connected components of the connection graph. To prove the theorem, we only need to show
1
that M(n, k, p)  O(log n). In the following, we will show that for lim p(n)n 2k+1 < +,
n
we have
lim P r{M(n, k, p)  2k + 2} = 1
n

Consider the connection graph G = G(V, E) of the NK model. It is a random graph and
there is an edge between two nodes if and only if the two corresponding local fitness functions
share variables and both of the local fitness functions take at least one zero as their fitness
value. However, under this definition the edge probabilities are not independent. If vx  E
then we know that fx has at least one zero and so the probability that xw is in E is greater
than if there were no other edge on x.
To deal with this we resort to the following proof construction. Let Cm = {v1 , . . . , vm }
be a subset of V of size m. Let  be an ordering (permutation) of v1 . . . vm . We say that
Cm is variable connected with respect to the ordering , denoted as C(Cm , ), if for each
i, 2  i  m there is either
1. a j < i such that f(j) and f(i) share a variable; or
2. a j, 1  j  i such that the variable xj is one of the k random variables in fi .
Lemma If the induced subgraph G[Cm ] is connected then there exists at least one ordering
 of v1 . . . vm such that C(Cm , ).
As proof, consider the ordering of vertices of any depth first search of a connected
subgraph. In this case, the connections are all by case 1.
The expected number of permutations  for which C(Cm , ) is
Ec = E[|{ : C(Cm , )}|] = m!Pr{C(Cm , )}
We then observe
 that the expected number of connected induced graphs on m vertices is
n
less than pm
0 m Ec , where p0 is the probability that fi takes at least one value zero. We
show this value goes to zero in the limit if m  2k + 2. Finally, since if there is a connected
subgraph on m vertices then there must be one for each i < m, it follows that the largest
connected component has size at most 2k + 1.
For a randomly generated permutation  of Cm , let Ci be the set of the first i vertices
of the permutation. For i  2 define Pi to be the probability that f(i) shares at least one
variable with f(j) for some j < i given that C(Ci1 , /1,    , i  1). Let P1 = 1. (A one
vertex subgraph is always connected.)
For i > 1 we have Pi = Pr{j < i, f(i) and f(j) share variables, given C(Ci1 , ) or
one of the k random variables in f(i) is in {x1 . . . xm }  {xi }}.
Pr{C(Cm , )} =

m
Y

Pi .

i=2

Finally, for i > 1 we note that Ci1 has at most (i  1)k distinct other variables. If Ci1
is connected then the number of variables may be less than this. Thus,
nk(i1)m
k

n1
k

Pi  1 
314

.

fiPhase Transition in NK Landscapes

The combinatorial part reduces to
(n  k(i  1)  m) . . . (n  k(i  1)  m  k + 1)
(n  1) . . . (n  k)


n  ki  m + 1 k

.
n1
So, Pr{C(Cm , )} is


m
Y


1

i=2

n  ki  m + 1
n1

k !

 !m1
km + m  2 k

1 1
n1
 m1 !
1
 O
, m, k fixed.
n


 m 
Noting that pm

O
n 2k +1 , we see that the expected number of connected subgraphs of
0
size m is bounded by
 
 m1 !
m
1
m n
m 2k +1
p0
Ec  O n n
m
n
which goes to zero if m = 2k +2. It follows that M(n, k, p) is less than 2k +2 with probability
asymptotic to 1. This completes the proof.

4. Analysis of The Fixed Ratio Model
As has been discussed in the previous section, the uniform probability model N (n, k, p) of
NK landscapes is asymptotically trivial. Part of the cause of this asymptotic triviality lies in
the fact that if the parameter p does not decrease very quickly with n, then asymptotically
there will be at least one local fitness function that takes the value 0 for all the possible
assignments, making the whole decision problem insoluble. In this section, we study the
fixed ratio model N (n, k, z). In this model, we require that each local fitness function has
fixed number of zero values so that the trivially insoluble situation in the uniform probability
model is avoided. We note that the same idea has been used in the study of the flawless
CSP (Gent et al., 1998).
Recall that in the fixed ratio model, we choose the neighborhood structure for each local
fitness in the same way as in the uniform probability model N (n, k, p). To determine the
fitness value for a local fitness function fi , we randomly without replacement select exactly
z tuples {s1 ,    , sz } from {0, 1}k+1 , and let fi (sj ) = 0 for each 1  j  z and fi (s) = 1 for
every other s  {0, 1}k+1 .
For the fixed ratio model, we are interested in how the probability of an instance of
N (n, k, z) being soluble changes as the parameter z increases from 0 to 2k+1 . It is easy to
n
P
see that the property There exists an assignment x such that f (x) =
fi (xi , (xi )) = n
i=1

315

fiGao & Culberson

is monotone in the parameter z  the number of tuples at which a local fitness function takes
zero. Actually, we have the following Lemma on the property of the solubility probability
of the fixed ratio model:
Lemma 4.1. For the fixed ratio model, if z1 > z2 , then
P r{N (n, k, z1 )is soluble}  P r{N (n, k, z2 )is soluble}.
Furthermore, we have

P r{N (n, k, z)is soluble} =

1, if z  1;
0, if z = 2k+1 .

Based on the above Lemma and in parallel to the study of the threshold phenomena in
other random combinatorial structures such as 3-Coloring of random graphs and random
3-SAT, we suggest the following conjecture:
Conjecture 4.1. There exists a threshold zc such that

1, if z < zc ;
lim P r{N (n, k, z)is soluble} =
n
0, if z > zc .
Conjectures like this are the starting point of the study of phase transition in many
random combinatorial structures such as 3-coloring of random graphs and random SAT,
but the existence of the thresholds is still an open question (Achlioptas, 1999; Cook &
Mitchell, 1997). However, bounding the thresholds has been an important topic in the
study of phase transition (Achlioptas, 1999, 2001; Dubois, 2001; Franco & Gelder, 1998;
Franco & Paul, 1983; Frieze & Suen, 1996; Kirousis, P.Kranakis, D.Krizanc, & Y.Stamation,
1994). In this section, we will establish two upper bounds on the threshold of the parameter
zc , and theoretically prove that random instances generated with the parameter z above
these upper bounds can be solved with probability asymptotic to 1 by polynomial (even
linear) algorithms.
Characterizing the sharpness of the thresholds is also of great interest in the study of
the phase transition. After proving that every monotone graph property has a threshold
behavior (Friedgut & Kalai, 1996), Friedgut (1999) established a necessary and sufficient
condition for a monotone graph property to have sharp threshold, which has been used to
prove the sharpness of the thresholds of 3-colorability and 3-SAT problems (Friedgut, 1999;
Achlioptas, 1999). For the fixed ratio model discussed in this paper, we suspect that it will
exhibit a coarse threshold behavior, and would like to leave a detailed investigation into
this problem as a future research direction.
4.1 The Upper Bound of z = 3.0
The derivation of this upper bound is based on the concept of a conflicting pair of local
fitness functions. We say that two local fitness functions fi and fj conflict with each other
if
1. fi and fj share at least one variable x; and
316

fiPhase Transition in NK Landscapes

2. For any assignment s  {0, 1}n , we have fi (s)fj (s) = 0.
It is obvious that an instance of the NK decision problem is insoluble if there exists a pair
of conflicting local fitness functions.
Based on the second moment method in the theory of probability (Alon & Spencer,
1992), we can prove the following upper bound result. As it takes linear time to check if
there is a pair of conflicting local fitness functions, we can see that the fixed ratio model
N (n, 2, z) is linearly solvable when z > 3.0.
Theorem 4.1. Define A to be the event that there is a conflicting pair of local fitness
functions in N (n, 2, z). For the fixed ratio model N (n, 2, z) with z = 3.0 + , we have
lim P r{A} = 1
n

and thus the problem is insoluble with probability asymptotic to 1.
Proof: Without loss of generality, we may write f as where fi has 4 zeroes in its fitness
value assignment for 1  i  n, and 3 zeroes for n + 1  i  n. Let Iij be the indicator
function of the event that fi and fj conflicts with each other, i.e.,

1, if fi and fj conflicts with each other;
Iij =
0, else.
P
Iij . We claim that lim P r{S = 0} = 0.
and S =
1i,jn

n

By Chebyschevs inequality, we have
P r{S = 0}  P r{|S  E(S)|  E(S)}
V ar(S)

.
(E(S)2 )

(4.3)

Since for each 1  i  n, fi has exactly 4 zeros in its fitness value assignment, we know
that two local fitness function fi , fj , 1  i, j  n, conflict with each other if and only if
they have exactly one common variable x such that one of the following is true: (1)fi (s) =
0(or 1), fj (s) = 1(or 0) for all the assignments s such that x = 1(respectively x = 0); and
(2)fi (s) = 1(or 0), fj (s) = 0(or 1) for all the assignments s such that x = 1(respectively x =
0);
Since the probability that two local fitness functions share at least one variable is equal
to


n2 n4
1

2
 2 
n1 n1 ,
2
2

we have


P r{Iij = 1} =

1

1
= ( ),
n

!

n2 n4
2
 2 
n1 n1
2
2

2

1

8

!2

4

 > 0, 1  i, j  n,

317

(4.4)

fiGao & Culberson

and hence,
X

E(S) =

X

E(Iij ) =

1i,jn

P r{Iij = 1}  (n).

1i,jn

P

We now consider the variance of S. Since S =

Iij , we have

1i,jn

P
V ar(S) =

P

V ar(Iij ) + 2

i,j

[E{Iij Ilm }  E{Iij }E{Ilm }]

(i,j)6=(l,m)

.

(E(S))2

Let

P
A1 =

and

P

2
A2 =

V ar(Iij )

i,j

(E(S))2

[E{Iij Ilm }  E{Iij }E{Ilm }]

(i,j)6=(l,m)

(E(S))2

.

It is easy to see that lim A1 = 0. To prove lim A2 = 0, we consider two cases:
n
n
Case 1: i 6= j 6= m 6= l. In this case, the two random variables Iij and Ilm are actually
independent. It follows that E{Iij Ilm }  E{Iij }E{Ilm } = 0.
Case 2: (i, j) 6= (l, m), but they have one in common, say j = l. In this case, we have
 2 !
1
E{Iij Ilm }  E{Iij }E{Ilm } = P r{Iij = 1}P r{Ijm = 1|Iij = 1}  
n
 
 2 !
1
1
= 
P r{Ijm = 1|Iij = 1}  
n
n
Given that fi and fj conflict with each other, the conditional probability that fj and fm
conflict with each other is still in ( n1 ).
3 pairs of I and I
SincePthere are only Cn
ij
jm satisfying the condition in Case 2, we know
that
[E{Iij Ilm }  E{Iij }E{Ilm }] is in (n). And therefore, lim A2 = 0. It follows
n

(i,j)6=(l,m)

that

V ar(S)
= 0.
n (E(S)2 )

lim P r{S = 0}  lim

n

Since the event {S > 0} implies that there exists a conflicting pair of local fitness functions,
the theorem follows.
4.2 2-SAT Sub-problems in N (n, 2, z) and a Tighter Upper Bound
In this subsection, we establish a tighter upper bound z > 2.837 for the threshold of the fixed
ratio model N (n, 2, z) by showing that asymptotically N (n, 2, z) contains an unsatisfiable
2-SAT sub-problem with probability 1 for any value of z greater than 2.873. This also
gives us a polynomial time algorithm which determines that N (n, 2, z) is insoluble with
probability asymptotic to 1 for z > 2.837.
318

fiPhase Transition in NK Landscapes

Recall from Section 2 that each instance of N (n, 2, z) has an equivalent 3-SAT instance.
The idea is to show that with probability asymptotic to 1, an instance of N (n, 2, z) will
contain a set of specially structured 3-clauses, called a t-3-module (Definition 10.3, Franco
& Gelder, 1998):
M = {M1 , . . . , M3p+2 }, t = 3p + 2,
where
M1 = (u1  u2  z1 , u1  u2  z1 );

Mp1 = (up1  up  zp1 , up1  up  zp1 );
Mp = (up  u0  zp , up  u0  zp );
Mp+1 = (up+1  up+2  zp+1 , up+1  up+2  zp+1 );

M3p1 = (u3p1  u3p  z3p1 , u3p1  u3p  z3p1 );
M3p = (u3p  u0  z3p , u3p  u0  z3p )
M3p+1 = (u0  u1  z3p+1 , u0  u1  z3p+1 );
M3p+2 = (u0  up+1  z3p+2 , u0  up+1  z3p+2 );
and u1 ,    , u3p+1 , z1 ,    , z3p+1 are binary variables. Notice that a t-3-module can be reduced to a 2-SAT problem containing two contradictory cycles and hence is unsatisfiable.
The result is proved in two steps. In the first step, it is shown that for z > 2.837 the
average number of t-3-modules contained in N (n, 2, z) tends to infinity as n increases. In
the second step, we use a result established by Alon and Spencer (1992) on the second
moment method to prove that for z > 2.837 the probability that N (n, 2, z) contains at least
one t-3-module tends to 1.
Let us start with the first step to show that the average number of t-3-modules contained
in N (n, 2, z) tends to infinity as n increases.
Definition 4.1. Given a t-3-module M and an NK landscape instance f =

n
P

fi , k = 2, a

i=1

sequence of local fitness functions
g = (g1 ,    , gt )  (f1 ,    , fn )
is said to be a possible match(PM) if for each 1  m  t, the main variable of gm is one of
the three variables that occur in the 3-module Mm . A subsequence (h1 ,    , hl ) of a possible
match g is legal if for any 1  m < j  l, hm 6= hj .
Lemma 4.2. Let f (x) =

n
P

fi (xi , (xi )) be an instance of N (n, 2, z) and M be a t-3-

i=1

t
module. Then the number of possible matches
 for the t-3-module M is 3 . Further, the

number of legal possible matches is  ( 3+2 5 )t .

319

fiGao & Culberson

Proof: For each 1  m  t, there are exactly 3 possible choices for gm :
fi1 (xi1 , (xi1 )), fi2 (xi2 , (xi2 )), fi3 (xi3 , (xi3 )),
where xi1 , xi2 , and xi3 correspond to the three variables that occur in the 3-module Mm .
Therefore, there are 3t possible matches for the t-3-module.
To prove the second conclusion, we divide the t-3-module into 3 parts M = (M1 , M2 , M3 ),
where M1 = (Mm , 1  m  p), M2 = (Mm , p + 1  m  3p  1), and M3 =
(M3p , M3p+1 , M3p+2 ). Letting L1 , L2 , and L3 be the number of legal possible matches for
M1 , M2 , M3 respectively. Since the literals in M1 are variable-distinct from the literals in
M2 , we have that the number of legal possible matches, L, for the t-3-module M satisfies
L1 L2  L  27L1 L2 .
We now estimate the order of L1 . To this end, we consider the probability space (, P ),
where  is the set of sequences (g1 ,    , gp ) of local fitness functions that possibly match M1
and P is the uniform probability distribution. Then, the number of legal possible matches
is
L1 = ||  P r{a random sample from  is legal}
(4.5)
Let g = (g1 ,    , gp ) be a random sample from  and xgm denote the main variable of the
local fitness function gm , then we have
1
P r{xgm = |um |} = P r{xgm = |um+1 |} = P r{xgm = |zm |} = ,
3
where |u| denotes the variable corresponding to the literal u.
Let Bm , 0 < m  p be the event that the first m local fitness functions g1 ,    , gm in
the possible match g = (g1 ,    , gp ) are mutually distinct. Since in M1 only consecutive
3-modules share variables, we have
Bm = {(g1 ,    , gm ) : gi 6= gi+1 , 1  i  m  1}.
Let bm = P r{gm 6= gm1 | Bm1 }, m  2, and b1 = 1. Notice that B1 = . Then, we have
P r{g = (g1 ,    , gp )is legal} = P r{Bp }
= P r{g1 6= g2 , g2 6= g3 ,    , gp1 6= gp }
= P r{B1 }P r{g2 6= g1 | B1 }  P r{g3 6= g2 | B2 }    P r{gp 6= gp1 | Bp1 }

(4.6)

= b1 b2    bp
Recalling that xgm denotes the main variable of the local fitness function gm , we have
bp = P r{gp1 6= gp , xgp1 = |up | | Bp1 } + P r{gp1 6= gp , xgp1 6= |up | | Bp1 }
= P r{gp1 6= gp | Bp1 , xgp1 = |up |}  P r{xgp1 = |up | | Bp1 } +
P r{gp1 6= gp | Bp1 , xgp1 6= |up |}  P r{xgp1 6= |up | | Bp1 }
2
ap + (1  ap )
=
3
1
= 1  ap ,
3
320

(4.7)

fiPhase Transition in NK Landscapes

where ap = P r{xgp1 = |up | | Bp1 }. For ap , we have
P r{Bp1 , xgp1 = |up |}
P r{Bp1 }
1
=
(P r{Bp1 , xgp1 = |up |, xgp2 = |up1 |}
P r{Bp1 }

ap =

+ P r{Bp1 , xgp1 = |up |, xgp2 6= |up1 |})
(4.8)
1
=
(P r{xgp1 = |up | | Bp1 , xgp2 = |up1 |}  P r{Bp1 , xgp2 = |up1 |}
P r{Bp1 }
+ P r{xgp1 = |up | | Bp1 , xgp2 6= |up1 |}  P r{Bp1 , xgp2 6= |up1 |})


1
1
1
=
P r{Bp1 , xgp2 = |up1 |} + P r{Bp1 , xgp2 6= |up1 |}
P r{Bp1 } 2
3
The last equation in the above formula is because that given Bp1 and xgp2 = |up1 |
(or xgp2 6= |up1 |), we have two (three, respectively) choices in selecting the local fitness
function gp1 . Consider the two terms P r{Bp1 , xgp2 = |up1 |} and P r{Bp1 , xgp2 6=
|up1 |} in (4.8), we have
P r{Bp1 , xgp2 = |up1 |}
= P r{gp2 6= gp1 | Bp2 , xgp2 = |up1 |}  P r{Bp2 , xgp2 = |up1 |}
2
= Pr{xgp2 = |up1 | | Bp2 }  P r{Bp2 }
3
2
= ap1  P r{Bp2 }
3

(4.9)

and
P r{Bp1 , xgp2 6= |up1 |}
= P r{gp2 6= gp1 | Bp2 , xgp2 6= |up1 |}  P r{Bp2 , xgp2 6= |up1 |}
= P r{xgp2 6= |up1 | | Bp2 }  P r{Bp2 }

(4.10)

= (1  ap1 )  P r{Bp2 }
By plugging (4.9) and (4.10) into (4.8), we get


P r{Bp2 } 1
1
1
ap =
ap1 + (1  ap1 ) =
.
P r{Bp1 } 3
3
3bp1
This, together with (4.7), gives us
bp = 1 

1
.
9bp1

(4.11)

It is not difficult to show that the sequence {bp } is decreasing and lower bounded by 0.
Letting lim bp = b and taking the limit on both sides, we get
p

b=1
321

1
,
9b

(4.12)

fiGao & Culberson

and thus, b =
thus,


3 5
6 .

In our case, b =


3+ 5
6

b 1    bp 

since b1 = 1. It follows that bp  b =

To prove that the expected number of legal possible matches L1 for M1 is in 

3+ 5
6

and

 !p
3+ 5
.
6

From (4.5), we know that the number of legal possible matches is greater than
 !p
 !p
3
+
5
3
+
5
3p
=
.
6
2

let p = bp 


3+ 5
6



(4.13)
 p 
3+ 5
,
2

= bp  b. From (4.11) and (4.12), we have
p = bp  b =

which means that the series

p
P

bp1  b
 dp1 , 0 < d < 1,
9bbp1

m is convergent. It follows that

m=1

(1 +

p
1
)    (1 +
)
b
b

converges to a finite positive constant c. Therefore,
b1    bp = (b + 1 )    (b + p )


p 
1 
= bp 1 +
 1 +
b
b
 !p
3+ 5
c
6

(4.14)

for sufficient large p and some constant c.
we can show that the number of legal possible matches L2 for M2 is in
Similarly,
  2p+2 
3+ 5

. Recalling that the number of legal possible matches L for the t-3-module
2
satisfies L1 L2  L  27L1 L2 , the second conclusion follows.
The following Lemma calculates the probability that a matching local fitness function
implies the matched 3-module.
Lemma 4.3. Given a 3-module x  y  w, x  y  w, and a local fitness function g such
that the main variable xg of g is one of the three Boolean variables |x|, |y|, |w|, let z =
2 + , 0    1 be the parameter in the fixed ratio model N (n, 2, z). Then the probability
that g contains the 3-module is
!

1
1
6

p0 =
(1  ) + 
(4.15)
n1
28
56
2

322

fiPhase Transition in NK Landscapes

Proof: Since xg is already one of the variables in the 3-module, the probability that
1
the other two variables are also in the 3-module is n1
.
( 2 )
Now, assume that the variables of the local fitness function g are the same as the
variables in the 3-module. From the definition of the fixed ratio model, g has two zeros
in its fitness value assignment with probability (1  ), and has three zeros in its fitness
assignment with probability . Note that the local fitness function g implies the 3-module
x  y  w, x  y  w if and only if
g(x, y, w) = 0 and

g(x, y, w) = 0.

From the definition of the fixed ratio model, this happens with the probability
1
6


8 (1  ) + 8 
2

3

The Lemma follows.
With the above preparation, we can now prove that the average number of t-3-modules
contained in N (n, 2, z) tends to infinity.
Theorem 4.2. Let At be the number of t-3-modules contained in N (n, 2, z) and t =
(ln2 n). Then, if z = 2 +  > 2.837,
lim E{At } = .

n

(4.16)



Proof: From Lemma 4.2, there are more than ( 3+2 5 )t legal possible matches for a fixed
t-3-module. From Lemma 4.3, we know that each possible legal match g = {g1 ,    , gt }
implies the t-3-module with probability pt0 . From the proof of Theorem 10.1 in (Franco &
Gelder, 1998), there are
2t2 nt1 (n  t + 1)t
(4.17)

n!
1
6
possible t-3-modules, where nt1 = (nt+1)!
. Let r = 28
(1  ) + 56
 , and write p0 =
1
r. We have
(n1
2 )
E{At } =

=

=

=
=

!t

3+ 5
p0  2t2 nt1 (n  t + 1)t
2
 !t
1
3+ 5
r  2t2 nt1 (n  t + 1)t 
t
n1
2
2
 !t
 !t t t
n
2 n (n  t + 1)t
1
3+ 5
2

r 

n1
n t
2
4(n  t + 1)
2
2
 !t t t

t
3+ 5
4 n (n  t + 1)t
n
1
r 
4(n  t + 1)
2
(n(n  1))t
n2
 2

1
t
(2(3 + 5)r)t (1  O
),
4n
n
323

(4.18)

fiGao & Culberson

where the fourth equation in (4.18) is due to the fact that for any positive integer n and q
2
such that q < n2 , we have nq eq /2n  nq  nq . It follows that lim E{At } =  if
n

2(3 +



5)r > 1.

(4.19)

Solving the inequality (4.19) gives us  > 0.837, that is, z = 2 +  > 2.837. This proves
Theorem 4.2.
Based on the Chebychevs inequality, to prove that N (n, 2, z) contains t-3-modules with
probability 1, we need to show that the variance of At , the number of contained t-3-modules,
is o(E{At }). For this purpose, we follow Franco and Gelders approach (Lemma 4.1, Franco
& Gelder, 1998) to apply the second moment method (Alon & Spencer, 1992):
Lemma 4.4. (Alon & Spencer, 1992, Ch. 4.3 Cor 3.5) Given a random structure(e.g., a
random CNF formula), let W be the set of substructures under consideration, A(w) be the
set of substructures sharing some clauses with w  W . Let Iw = 1 when w is in the random
structure and 0 otherwise. If
(1) elementsP
of W are symmetric;
(2)  = E{
Iw }  ; and
wW
P
(3)
P r(w | w) = o(), for each w  W ,
wA(w)

then as n  , the probability that the random structure contains a substructure tends to
1.
To use the above Lemma to study the 2-SAT sub-problem in NK landscapes, we view
the random structure to be a random instance of N (n, 2, z), and W to be the set of all
t-3-modules which are symmetric by their definitions(Sections 5 and 10, Franco & Gelder,
1998).
Theorem 4.3. If z = 2 +  > 2.837, then N (n, 2, z) is asymptotically insoluble with
probability 1.
Proof:
Let At be the number of t-3-modules implied by N (n, 2, z) and t = O(ln2 n).
Theorem 4.2 shows that lim E{At } = . By Lemma 4.4, it is enough to show that for
n
each w  W ,
X
P r(w | w) = o(E{At }),
(4.20)
wA(w)

where P r(w | w) is the conditional probability that N (n, 2, z) implies the t-3-module w
given that it implies w, and A(w) is the set of all t-3-modules sharing some clauses with w.
Suppose that w shares Q, 1  Q  2t clauses with w, and that these Q clauses are
distributed among q 3-modules. Further, let q1 be the number of 3-modules whose two
clauses are both shared and q2 = q  q1 the number of 3-modules that only has one clause
shared.
Let T1 be a 3-module in w that shares exactly one clause with a 3-module T2 in w.
We claim that the conditional probability that T1 is implied by N (n, 2, z) given that w is
implied by N (n, 2, z), is
1
1
 + O( ).
(4.21)
6
n
324

fiPhase Transition in NK Landscapes

Without loss of generality, assume that T2 = {xyu, xy u} and T1 = {xyu, xy u}.
Since w is implied by N (n, 2, z), there is a local fitness function g = g(|x|, |y|, |u|) that implies
T2 . The conditional probability that T1 is implied, is less than or equal to P1 + P2 where
P1 is the conditional probability that g also implies the clause x  y  u given that g implies
T2 , and P2 is the conditional probability that the clause x  y  u is implied by other local
fitness functions. By the definition of N (n, 2, z), we have that P1 = 61 . Since a local fitness
function implies x  y  u only if it has the same variables with g = g(|x|, |y|, |u|), we have
that P2 = O( n1 ). The claim is proved. It follows that, for sufficiently large n,
!tq

 q 2
3+ 5
1
q1
P r{w | w}  c
p0
1 

(4.22)
2
6
where p0 is defined in Lemma 4.3 and c is a fixed constant.
Let AQ,q,q2 (w) be the set of t-3-modules that share Q clauses with w such that these Q
clauses are distributed over q different 3-modules. As before, q1 is the number of 3-modules
whose two clauses are both shared and q2 = q  q1 the number of 3-modules that only has
one clause shared. We claim that
|AQ,q,q2 (w)| = |A2q,q,0 (w)|6q2 .

(4.23)

where A2q,q,0 (w) is the set of t-3-modules that share all the 2q clauses in the q 3-modules
with w. Let M = {M1 ,    , Mt } be a t-3-module in which all the clauses Mi , 1  i  q
are shared with w. Let M = {M1 ,    , Mt } be a t-3-module in which all the clauses in
Mi , 1  i  q1 are shared and each of the 3-modules Mi , q1 + 1  q1 + q2 has only one
clause shared. Since for each of the q2 3-modules, we have 6 ways to choose the non-shared
clauses, there are 6q2 such t-3-modules M in AQ,q,q2 (w) that correspond to one t-3-module
M in A2q,q,0 . The claims follow. From formula (55) and (56) in (Franco & Gelder, 1998)
and (4.23), it follows that
(
O(t) tq 2(tq) q2
2 n
6
, q  p + 1,
n2
|AQ,q,q2 (w)| < O(1)
(4.24)
tq
2(tq)
q
2
n
6
, q > p + 1.
n 2

6
1
1
Let r = 28
(1  ) + 56
 , and write p0 = n1
r. Then, we have
( 2 )
|AQ,q,q2 (w)|P r{w | w}


O(t) tq 2(tq) q2 3 + 5 tq 1 q2
 2 2 n
6 (
p0 ) ( )
n
2
6
 !tq
O(t)
1
3+ 5
 2 2tq n2(tq)
r

n1 tq
n
2

(4.25)

2


O(t) 1
(2(3 + 5)r))tq
n 4n

O(t)

E{At }(2(3 + 5)r))q , q  p + 3
n



and
|AQ,q,q2 (w)|P r{w | w}  O(1)E{At }(2(3 +
325



5)r))q , q > p + 3.

(4.26)

fiGao & Culberson

Therefore,
X
X
P r(w | w) =
|AQ,q,q2 (w)|P r{w | w}
Q,q,q2

wA(w)

=

t
X

t
X
X X
X X O(t)


q
E{At }(2(3 + 5)r)) +
O(1)E{At }(2(3 + 5)r))q .
n
q
q

Q=1 qp+3

Q=1 q>p+3

2

2

(4.27)
Since 2(3 +



5)r) > 1 for z > 2.837, we have
X
wA(w)

P r(w | w) 

O(t4 )
E{At } + t3 E{At }(4r)(p+3)
n

(4.28)

= o(E{At }).
This completes the proof of Theorem 4.3.

5. Experiments
Our study of the threshold phenomena in NK landscapes started with an experimental
investigation. Many of the theoretical results in the previous section are motivated by
the observations made in our experiments. In this section, we describe the approach and
methods we used in the experimental study, and report the results and observations we
have made.
In our experiments, an instance of the NK landscape decision problem is converted to an
equivalent 3-SAT problem, and then the 3-SAT problem is solved using Robertos relsatan
enhanced version of the famous Davis-Putnam algorithm for SAT problems implemented in
C ++ . The source code of relsat can be found at http://www.cs.ubc.ca/ hoos/SATLIB/.
In the experiments, we generated random instances of the NK landscape decision problem from the random model N (n, 2, z). As a result, the equivalent SAT problem for each
random NK landscape instance is a 3-SAT problem with n variables and (on average) zn
clauses. By definition, the parameter z is between 0 and 8. For z  1, the 3-SAT instance
can be solved easily by setting the literals that correspond to the main variables of the
local fitness function to true. As z increases, we get more and more clauses and the 3-SAT
problem becomes more and more constrained. The aims of the experiments are threefold:(1)Investigating if there exists a threshold phenomenon in the random NK landscape
model; (2) Locating the threshold of the parameter z; and (3)Determining if there are any
hard instances around the threshold.
5.1 Experiments on the Fixed Ratio Model
In this part of the experiments, we generate 100 random instances of N (n, 2, z) for each
of the parameters n = 29    216 and z = 2.71, 2.72,    , 3.00. These instances are then
converted to 3-SAT instances and solved by relsat. Figure 1 shows the fraction of insoluble
instances as a function of the parameter z. It can be seen that there exists a threshold
phenomenon and the threshold is around 2.83. This shows that our upper bound z = 2.837
is very tight.
326

fiPhase Transition in NK Landscapes

1

0.9

0.8

0.7

n=512
n=1024
n=2048
n=4096
n=8192
n=16384
n=32768
n=65536
z=2.84

0.6

0.5

0.4

0.3

0.2

0.1

0
2.7

2.75

2.8

2.85

2.9

2.95

3

Figure 1: Fractions of insoluble instances(Y-axis) as a function of z (X-axis).

In Figure 2, we plot the square root of the average search cost as a function of the
parameter n. The figure indicates that the average search is in O(n2 ) for any parameter
z. We have also observed that more than 99 percent of the insoluble instances are solved
quickly in the preprocessing stage of relsat. This indicates that there must be some small
structures that make the instances insoluble. More detailed experimental results can be
found in Gaos thesis (Gao, 2001).
5.2 Experiments on the 2-SAT sub-Problem
This is the part of the experiments that motivated our theoretical analyses in Section 4.2.
The idea can be explained as follows. Let
f (x) =

n
X

fi (xi , (xi ))

i=1

be an instance of the decision problem of NK landscape and
^
^
 = C1 C2    Cn
the equivalent 3-SAT problem where Ci is the set of 3-clauses equivalent to the local fitness
function fi . For each i, there is a set of 2-clauses Di (possibly empty) implied by Ci . For
example, if Ci has three 3-clauses ((x, y, z), (x, y, z), (x, y, z)), then the set of 2-clauses Di
would be ((x, z), (x, y)). The conjunction of Di , denoted by , is a 2-SAT problem. It is
obvious that the original 3-SAT problem  is satisfiable only if the 2-SAT sub-problem  is
satisfiable. In the experiment, we generate instances of the NK landscape N (n, 2, z), convert
327

fiGao & Culberson

35
z=2.71
z=2.80
z=2.84
z=2.86
z=2.90

30

25

20

15

10

5

0

0

1

2

3

4

5

6

7
4

x 10

Figure 2: Square root of the average search cost (Y-axis, in seconds) as a function of n
(X-axis).

1

0.9

0.8

0.7

n=512
n=1024
n=2048
n=4096
n=8192
n=16384
n=32768
n=65536
z=2.84

0.6

0.5

0.4

0.3

0.2

0.1

0
2.7

2.75

2.8

2.85

2.9

2.95

3

Figure 3: Fractions of insoluble instances(Y-axis) as a function of z (X-axis) for 2-SAT
sub-problems.

them to the equivalent 3-SAT problems, and extract the 2-SAT sub-problems. These 2-SAT

328

fiPhase Transition in NK Landscapes

40
z=2.71
z=2.80
z=2.84
z=2.86
z=2.90

35

30

25

20

15

10

5

0

0

1

2

3

4

5

6

7
4

x 10

Figure 4: Square root of the average search cost (Y-axis, in seconds) as a function of n
(X-axis) for 2-SAT sub-problems.

problems are then solved by the relsat solver. If the 2-SAT problem is unsatisfiable, then
the original NK landscape instance is also insoluble.
The experimental settings are the same as those in the experiment on the original
problem. The results are shown in Figures 3-4, in parallel to the Figures 1-2 of the results
on the original 3-SAT problems in Section 5.1. We see that the patterns of insoluble fractions
and search cost are similar to those we found in the original 3-SAT problems. There is a
soluble-insoluble phase transition occurring around 2.83, but the fraction of unsatisfiable
instances is lower than the fraction in the original 3-SAT problems.
We also observed that the average search cost for the 2-SAT sub-problems remains the
same as that for the original 3-SAT problems. This tells us that the difficulty of solving a
soluble instance of NK landscape is almost the same as that of solving a 2-SAT problem, and
hence is easy. Therefore, on average the NK landscape N (n, 2, z) is also easy at parameters
below the threshold where almost all of the instances are soluble.

6. Implications and Conclusions
One of the questions that arises about this work is its implications to the design and analysis of genetic algorithms. NK landscapes were initially conceived as simplified models
of evolutionary landscapes which could be tuned with respect to ruggedness and epistatic
interactions (Kauffman, 1989). In the study of genetic algorithms, NK landscape models
have been used as a prototype and benchmark in the analysis of the performance of different genetic operators and the effects of different encoding methods on the algorithms

329

fiGao & Culberson

performance (Altenberg, 1997; Hordijk, 1997; Jones, 1995). Kauffman (1993) points out
that the parameters that primarily affect a number of ruggedness measures are n and k.
Nevertheless, the fact that for k  2 the discrete NK landscape is NP-complete (Wright
et al., 2000) when the neighbors are arbitrarily chosen could be construed as implying that
random landscapes with fixed k are in practice hard.
The results in this paper should serve as a cautionary note that this may not be the
case. Our analyses show that for fixed k the uniform probability model is trivially solvable
as the problem size tends to infinity. For the fixed ratio model, we have derived two upper
bounds for the threshold of the solubility phase transition, and proved that the problem
with the control parameter above the upper bounds can be solved in polynomial time with
probability asymptotic to 1 due to the existence of easy sub-problems such as 2-SAT. A
series of experiments has also been conducted to investigate the hardness of the problem
with the control parameters around and below the threshold. From the experiments, we
have observed that the problem is also easy around and below the threshold.
Our proofs hold only for the decision version of the problem where the component functions are discrete on {0, 1}. The proofs are obtained by noticing that the clustering of
functions, or clauses, on selected subsets of variables implies that the overall problem is
decomposable into independent subproblems, or that the problem contains small substructures that identify the solution. The subproblems are the components of the connection
graph defined in Section 3 and the 2-SAT sub-problems studied in Section 4.2. It is currently unclear to us to what extent our analysis can be extended to the optimization version
of the NK model, and we would like to study this problem further in the future.
In response to the question what are the implications for GAs? we suggest the following
speculative line of enquiry. For the discrete model we use, the soluble instances are readily
solved by a standard algorithmic approach based on recognizing the components of the
connection graph. (This should not be a surprise for us as it has been pointed out by
Heckendorn, Rana, and Whitley (1999) that Even relatively old algorithms such as DavisPutnam which are deterministic and exact are orders of magnitude faster than GAs.) 1 A
similar connectivity can be developed for real valued distributions, for example by capping
the minimum value which we allow a sub-function to take. We can speculate that the
clustering imposed by fixed values of k would also generate localized structures when real
values are applied and when considering optimization instead of decision, but perhaps with
fuzzy boundaries. In fact, this observation is just the flip side of limited epistasis. Genetic
algorithms, or their variants such as the probabilistic model-building algorithms (Larranaga
& Lozano, 2001), designed to mimic natural evolution, are supposed to take advantage of
this situation. So, to the extent that NK landscapes are an accurate reflection of the
features exploited by evolutionary algorithms, we pose the following question. Is it possible
to identify these fuzzy components if they exist, and in doing so design an algorithm that
exploits the same landscape features that the evolutionary algorithms do, but far more
efficiently, as we have done for the uniform discrete decision problem?
These landscapes were designed with the intent of studying limited interactions, and our
results can also be seen as a confirmation that indeed limited epistasis leads to easier problems. In another domain, that of the more traditional research into search and optimization,
1. We thanks an anonymous referee for pointing out to us the work of Heckendorn, et al. (Heckendorn
et al., 1999)

330

fiPhase Transition in NK Landscapes

there is a need for test bed problems with real world connections which are tunable with
respect to difficulty. NK landscapes might have been such a domain for generating 3-SAT
instances. It is disappointing that for restricted k the instances generated are easy with
high probability.

Acknowledgments
This research supported in part by Natural Sciences and Engineering Research Council
Grant No. OGP8053. We thank the anonymous reviewers for their comments.

References
Achlioptas, D. (1999). Threshold Phenomena in Random Graph Colouring and Satisfiability. Ph.D. thesis, Department of Computer Science, University of Toronto, Toronton,
Canada.
Achlioptas, D. (2001). A survey of lower bounds for random 3-sat via differential equations.
Theoretical Computer Science, 265, 159185.
Achlioptas, D., Kirousis, L., Kranakis, E., Krizanc, D., & Molloy, M. (1997). Random
constraint satisfaction: A more accurate picture. In Proceedings of CP97, pp. 107
120. Springer.
Alon, N., & Spencer, J. (1992). The Probabilistic Method. Wiley, New York.
Altenberg, L. (1997). Nk fitness landscapes. In Back, T., Fogel, D., & Michalewicz, Z.
(Eds.), Handbook of Evolutionary Computation. Oxford University Press, New York.
Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). Where the really hard problems are. In
Proceedings of the 12th International Joint Conference on Artificial Intelligence, pp.
331337. Morgan Kaufmann.
Cook, S., & Mitchell, D. (1997). Finding hard instances of the satisfiability problem: A
survey. In Du, Gu, & Pardalos (Eds.), Satisfiability Problem: Theory and Applications,
Vol. 35 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science.
American Mathematical Society.
Culberson, J., & Gent, I. (2001). Frozen development in graph coloring. Theoretical Computer Science, 265 (1-2), 227264.
Dubois, O. (2001). Upper bounds on the satisfiability threshold. Theoretical Computer
Science, 265 (1-2), 187197.
Franco, J., & Gelder, A. (1998). A perspective on certain polynomial time solvable classes
of satisfiability. Discrete Applied Mathematics, to appear.
Franco, J., & Paul, M. (1983). Probabilistic analysis of the davis-putnam procedure for
solving satisfiability. Discrete Applied Mathematics, 5, 7787.
Freeman, J. (1996). Hard random 3-sat problems and the davis-putman procedure. Artificial
Intelligence, 81, 183198.

331

fiGao & Culberson

Friedgut, E. (1999). Sharp thresholds of graph properties and the k-sat problem. J. Amer.
Math. Soc., 10171054.
Friedgut, E., & Kalai, G. (1996). Every monotone graph property has a sharp threshold.
Proc. Amer. Math. Soc., 29933002.
Frieze, A., & Suen, S. (1996). Analysis of two simple heuristics on a random instance of
k-sat. J. of Algorithm, 20 (2), 312355.
Gao, Y. (2001). Threshold phenomena in NK landscapes. Masters thesis, Department of
Computing Science, University of Alberta, Edmonton, Alberta, Canada.
Gent, I., MacIntyre, I., Prosser, P.and Smith, B., & Walsh, T. (1998). Random constraint
satisfaction: Flaws and structure. Tech. rep. APES-08-1998, APES Research Group.
Heckendorn, R., Rana, S., & Whitley, D. (1999). Polynomial time summary statistics for a
generalization of maxsat. In GECCO99: Proceedings of the Genetic and Evolutinary
Computation Conference, pp. 281288. Morgan Kaufmann.
Hordijk, W. (1997). A measure of landscapes. Evolutionary Computation, 4 (4), 335360.
Jones, T. (1995). Evolutionary Algorithms, Fitness Landscapes and Search. Ph.D. thesis,
University of New Mexico, Albuquerque, NM.
Kauffman, S. (1989). Adaptation on rugged fitness landscapes. In Stein, D. (Ed.), Lectures
in the Sciences of Complexity, Santa Fe Institute Studies in the Sciences of Complexity,
pp. 527618. Addison Wesley.
Kauffman, S. (1993). The Origins of Order: Self-organization and Selection in Evolution.
Oxford University Press, Inc.
Kirkpatrick, S., & Selman, B. (1994). Critical behavior in the satisfiability of random
boolean expressions. Science, 264, 12971301.
Kirousis, L., P.Kranakis, D.Krizanc, & Y.Stamation (1994). Approximating the unsatisfiability threshold of random formulas. Random Structures and Algorithms, 12 (3),
253269.
Larranaga, P., & Lozano, J. (2001). Estimation of Distribution Algorithms: A New Tool for
Evolutinary Computation. Kluwer Academic Publishers, New York.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of sat problems.
In Proceedings of the 10th Natl. Conf on Artificial Intelligence, pp. 459465. AAAI
Press.
Vandegriend, B., & Culberson, J. (1998). The Gn,m phase transition is not hard for the
Hamiltonian Cycle problem. Journal of Artificial Intelligence Research, 9, 219245.
Weinberger, E. D. (1996). Np completeness of kauffmans NK model, a tunable rugged
fitness landscape. Tech. rep. 96-02-003, Santa Fe Institute, Santa Fe.
Wright, A. H., Thompson, R. K., & Zhang, J. (2000). The computational complexity of NK
fitness functions. IEEE Transactions on Evolutionary Computation, 4 (4), 373379.

332

fiJournal of Artificial Intelligence Research 17 (2002) 229-264

Submitted 12/01; published 9/02

A Knowledge Compilation Map
Adnan Darwiche

darwiche@cs.ucla.edu

Computer Science Department
University of California, Los Angeles
Los Angeles, CA 90095, USA

Pierre Marquis

marquis@cril.univ-artois.fr

Universite dArtois
F-62307, Lens Cedex, France

Abstract
We propose a perspective on knowledge compilation which calls for analyzing different compilation approaches according to two key dimensions: the succinctness of the target compilation
language, and the class of queries and transformations that the language supports in polytime.
We then provide a knowledge compilation map, which analyzes a large number of existing target compilation languages according to their succinctness and their polytime transformations and
queries. We argue that such analysis is necessary for placing new compilation approaches within
the context of existing ones. We also go beyond classical, flat target compilation languages based
on CNF and DNF, and consider a richer, nested class based on directed acyclic graphs (such as
OBDDs), which we show to include a relatively large number of target compilation languages.

1. Introduction
Knowledge compilation has emerged recently as a key direction of research for dealing with the
computational intractability of general propositional reasoning (Darwiche, 1999; Cadoli & Donini,
1997; Boufkhad, Gregoire, Marquis, Mazure, & Sas, 1997; Khardon & Roth, 1997; Selman &
Kautz, 1996; Schrag, 1996; Marquis, 1995; del Val, 1994; Dechter & Rish, 1994; Reiter & de
Kleer, 1987). According to this direction, a propositional theory is compiled off-line into a target
language, which is then used on-line to answer a large number of queries in polytime. The key
motivation behind knowledge compilation is to push as much of the computational overhead into
the off-line phase, which is amortized over all on-line queries. But knowledge compilation can serve
other important purposes as well. For example, target compilation languages and their associated
algorithms can be very simple, allowing one to develop on-line reasoning systems for simple software
and hardware platforms. Moreover, the simplicity of algorithms that operate on compiled languages
help in streamlining the effort of algorithmic design into a single task: that of generating the smallest
compiled representations possible, as that turns out to be the main computational bottleneck in
compilation approaches.
There are three key aspects of any knowledge compilation approach: the succinctness of the
target language into which the propositional theory is compiled; the class of queries that can be
answered in polytime based on the compiled representation; and the class of transformations that
can be applied to the representation in polytime. The AI literature has thus far focused mostly on
target compilation languages which are variations on DNF and CNF formulas, such as Horn theories
and prime implicates. Moreover, it has focused mostly on clausal entailment queries, with very little
discussion of tractable transformations on compiled theories.
The goal of this paper is to provide a broad perspective on knowledge compilation by considering
a relatively large number of target compilation languages and analyzing them according to their
succinctness and the class of queries/transformations that they admit in polytime.

c
2002
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiDarwiche & Marquis

Instead of focusing on classical, flat target compilation languages based on CNF and DNF, we
consider a richer, nested class based on representing propositional sentences using directed acyclic
graphs, which we refer to as NNF. We identify a number of target compilation languages that have
been presented in the AI, formal verification, and computer science literature and show that they
are special cases of NNF. For each such class, we list the extra conditions that need to be imposed
on NNF to obtain the specific class, and then identify the set of queries and transformations that the
class supports in polytime. We also provide cross-rankings of the different subsets of NNF, according
to their succinctness and the polytime operations they support.
The main contribution of this paper is then a map for deciding the target compilation language
that is most suitable for a particular application. Specifically, we propose that one starts by identifying the set of queries and transformations needed for their given application, and then choosing
the most succinct language that supports these operations in polytime.
This paper is structured as follows. We start by formally defining the NNF language in Section 2,
where we list a number of conditions on NNF that give rise to a variety of target compilation languages.
We then study the succinctness of these languages in Section 3 and provide a cross-ranking that
compares them according to this measure. We consider a number of queries and their applications in
Section 4 and compare the different target compilation languages according to their tractability with
respect to these queries. Section 5 is then dedicated to a class of transformations, their applications,
and their tractability with respect to the different target compilation languages. We finally close in
Section 6 by some concluding remarks. Proofs of all theorems are delegated to Appendix A.

2. The NNF Language
We consider more than a dozen languages in this paper, all of which are subsets of the NNF language,
which is defined formally as follows (Darwiche, 1999, 2001a).
Definition 2.1 Let PS be a denumerable set of propositional variables. A sentence in NNFP S is
a rooted, directed acyclic graph (DAG) where each leaf node is labeled with true, false, X or X,
X  P S; and each internal node is labeled with  or  and can have arbitrarily many children.
The size of a sentence  in NNFP S , denoted |  |, is the number of its DAG edges. Its height is the
maximum number of edges from the root to some leaf in the DAG.
Figure 1 depicts a sentence in NNF, which represents the odd parity function (we omit reference
to variables PS when no confusion is anticipated). Any propositional sentence can be represented
as a sentence in NNF, so the NNF language is complete.
It is important here to distinguish between a representation language and a target compilation
language. A representation language is one which we expect humans to read and write with some
ease. The language of CNF is a popular representation language, and so is the language of Horn
clauses (especially when expressed in rules form). On other hand, a target compilation language does
not need to be suitable for human specification and interpretation, but should be tractable enough
to permit a non-trivial number of polytime queries and/or transformations. We will consider a
number of target compilation languages that do not qualify as representation languages from this
perspective, as they are not suitable for humans to construct or interpret. We will also consider a
number of representation languages that do not qualify as target compilation languages.1
A formal characterization of representation languages is outside the scope of this paper. But for
a language to qualify as a target compilation language, we will require that it permits a polytime
clausal entailment test. Note that a polytime consistency test is not sufficient here, as only one
consistency test on a given theory does not justify its compilation. Given this definition, NNF does
1. It appears that when proposing target compilation languages in the AI literature, there is usually an implicit
requirement that the proposed language is also a representation language. As we shall see later, however, the
most powerful target compilation languages are not suitable for humans to specify or interpret directly.

230

fiA Knowledge Compilation Map

Decomposability

(a)

(b)

or

or

or

and
A,B

Smoothness

(c)

Determinism

or
and

and

C,D

or

or

or

or

and

and
or

or

or

or

and
or

or

or

A,B
A,B

and

and

and

and

and

and

and

and

and

and

and

and

and

and

and

and

and

and

and

and

and

and

and

and

A

B

B

A

C

D

D

C

A

B

B

A

C

D

D

C

A

B

B

A

C

D

D

C

Figure 1: A sentence in NNF. Its size is 30 and height is 4.
not qualify as a target compilation language unless P=NP (Papadimitriou, 1994), but many of its
subsets do. We define a number of these subsets below, each of which is obtained by imposing
further conditions on NNF.
We will distinguish between two key subsets of NNF: flat and nested subsets. We first consider
flat subsets, which result from imposing combinations of the following properties:
 Flatness: The height of each sentence is at most 2. The sentence in Figure 3 is flat, but the
one in Figure 1 is not.
 Simple-disjunction: The children of each or-node are leaves that share no variables (the
node is a clause).
 Simple-conjunction: The children of each and-node are leaves that share no variables (the
node is a term). The sentence in Figure 3 satisfies this property.
Definition 2.2 The language f-NNF is the subset of NNF satisfying flatness. The language CNF is
the subset of f-NNF satisfying simpledisjunction. The language DNF is the subset of f-NNF satisfying
simpleconjunction.
CNF does not permit a polytime clausal entailment test (unless P=NP) and, hence, does not qualify
as a target compilation language. But its dual DNF does.
The following subset of CNF, prime implicates, has been quite influential in computer science:
Definition 2.3 The language PI is the subset of CNF in which each clause entailed by the sentence
is subsumed by a clause that appears in the sentence; and no clause in the sentence is subsumed by
another.
A dual of PI, prime implicants IP, can also be defined.
Definition 2.4 The language IP is the subset of DNF in which each term entailing the sentence
subsumes some term that appears in the sentence; and no term in the sentence is subsumed by
another term.
There has been some work on representing the set of prime implicates of a propositional theory in
a compact way, allowing an exponential number of prime implicates to be represented in polynomial
space in certain casessee for example the TRIE representation in (de Kleer, 1992), the ZBDD
representation used in (Simon & del Val, 2001), and the implicit representation based on metaproducts, as proposed in (Madre & Coudert, 1992). These representations are different from the
language PI in the sense that they do not necessarily support the same queries and transformations
231

fiDarwiche & Marquis

that we report in Tables 5 and 7. They also exhibit different succinctness relationships than the
ones we report in Table 3.
Horn theories (and renamable Horn theories) represent another target compilation subset of CNF,
but we do not consider it here since we restrict our attention to complete languages L only, i.e., we
require that every propositional sentence is logically equivalent to an element of L.
We now consider nested subsets of the NNF language, which do not impose any restriction on
the height of a sentence. Instead, these subsets result from imposing one or more of the following
conditions: decomposability, determinism, smoothness, decision, and ordering. We start by defining
the first three properties. From here on, if C is a node in an NNF, then Vars(C) denotes the set of
all variables that label the descendants of node C. Moreover, if  is an NNF sentence rooted at C,
then Vars() is defined as Vars(C).
 Decomposability (Darwiche, 1999, 2001a). An NNF satisfies this property if for each conjunction C in the NNF, the conjuncts of C do not share variables. That is, if C1 , . . . , Cn are
the children of and-node C, then Vars(Ci )  Vars(Cj ) =  for i 6= j. Consider the and-node
marked in Figure 1(a). This node has two children, the first contains variables A, B while the
second contains variables C, D. This and-node is then decomposable since the two children do
not share variables. Each other and-node in Figure 1(a) is also decomposable and, hence, the
NNF in this figure is decomposable.
 Determinism (Darwiche, 2001b): An NNF satisfies this property if for each disjunction C
in the NNF, each two disjuncts of C are logically contradictory. That is, if C1 , . . . , Cn are
the children of or-node C, then Ci  Cj |= false for i 6= j. Consider the or-node marked
in Figure 1(b), which has two children corresponding to sub-sentences A  B and B  A.
The conjunction of these two sub-sentences is logically contradictory. The or-node is then
deterministic and so are the other or-nodes in Figure 1(b). Hence, the NNF in this figure is
deterministic.
 Smoothness (Darwiche, 2001b): An NNF satisfies this property if for each disjunction C in the
NNF, each disjunct of C mentions the same variables. That is, if C1 , . . . , Cn are the children of
or-node C, then Vars(Ci ) = Vars(Cj ) for i 6= j. Consider the marked or-node in Figure 1(c).
This node has two children, each of which mentions variables A, B. This or-node is then
smooth and so are the other or-nodes in Figure 1(c). Hence, the NNF in this figure is smooth.
It is hard to ensure decomposability. It is also hard to ensure determinism while preserving decomposability. Yet any sentence in NNF can be smoothed in polytime, while preserving decomposability
and determinism. Preserving flatness, however, may blow-up the size of given NNF. Hence, smoothness is not that important from a complexity viewpoint unless we have flatness.
The properties of decomposability, determinism and smoothness lead to a number of interesting
subsets of NNF.
Definition 2.5 The language DNNF is the subset of NNF satisfying decomposability; d-NNF is the subset satisfying determinism; s-NNF is the subset satisfying smoothness; d-DNNF is the subset satisfying
decomposability and determinism; and sd-DNNF is the subset satisfying decomposability, determinism
and smoothness.
Note that DNF is a strict subset of DNNF (Darwiche, 1999, 2001a). The following decision property
comes from the literature on binary decision diagrams (Bryant, 1986).
Definition 2.6 (Decision) A decision node N in an NNF sentence is one which is labeled with true,
false, or is an or-node having the form (X  )  (X  ), where X is a variable,  and  are
decision nodes. In the latter case, dVar (N ) denotes the variable X.
Definition 2.7 The language BDD is the set of NNF sentences, where the root of each sentence is a
decision node.
232

fiA Knowledge Compilation Map

or

X1
and

and

X1  X1
or
and
 X2

or

X2
or
and

 X3

and
X3

true

and

and

X2

X2
and
 X2

X2

X3

X3

1

0

or
and

and
X3

 X3

false

Figure 2: On the left, a sentence in the BDD language. On the right, its corresponding binary decision
diagram.

The NNF sentence in Figure 2 belongs to the BDD subset.
The BDD language corresponds to binary decision diagrams (BDDs), as known in the formal
verification literature (Bryant, 1986). Binary decision diagrams are depicted using a more compact
notation though: the labels true and false are denoted by 1 and 0, respectively; and each decision
or
and

and

X

 . The BDD sentence on the left of Figure 2 corresponds to the
node X  X  is denoted by 
binary decision diagram on the right of Figure 2. Obviously enough, every NNF sentence that satisfies
the decision property is also deterministic. Therefore, BDD is a subset of d-NNF.
As we show later, BDD does not qualify as a target compilation language (unless P=NP), but the
following subset does.

Definition 2.8 FBDD is the intersection of DNNF and BDD.
That is, each sentence in FBDD is decomposable and satisfies the decision property. The FBDD language
corresponds to free binary decision diagrams (FBDDs), as known in formal verification (Gergov &
Meinel, 1994a). An FBDD is usually defined as a BDD that satisfies the read-once property: on
each path from the root to a leaf, a variable can appear at most once. FBDDs are also known as
read-once branching programs in the theory literature. Imposing the read-once property on a BDD
is equivalent to imposing the decomposability property on its corresponding BDD sentence.
A more influential subset of the BDD language is obtained by imposing the ordering property:
Definition 2.9 (Ordering) Let < be a total ordering on the variables PS. The language OBDD<
is the subset of FBDD satisfying the following property: if N and M are or-nodes, and if N is an
ancestor of node M , then dVar (N ) < dVar (M ).
Definition 2.10 The language OBDD is the union of all OBDD< languages.
The OBDD language corresponds to the wellknown ordered binary decision diagrams (OBDDs)
(Bryant, 1986).
Our final language definition is as follows:
Definition 2.11 MODS is the subset of DNF where every sentence satisfies determinism and smoothness.
233

fiDarwiche & Marquis

or
and

X

Y

and

Z

and

and

X Y Z

Figure 3: A sentence in language MODS.

NNF
CO,

d-NNF

s-NNF

CE, ME

DNNF

f-NNF

VA, IM, CT

BDD

d-DNNF

EQ?

VA, IM

FBDD

EQ?

DNF

sd-DNNF

CNF

EQ

OBDD
SE

OBDD<

EQ, SE

VA, IM, EQ, SE

MODS

IP

CO , CE, EQ, SE, ME

PI

Figure 4: The set of DAG-based languages considered in this paper. An edge L1  L2 means that
L1 is a proper subset of L2 . Next to each subset, we list the polytime queries supported
by the subset but not by any of its ancestors (see Section 4).

Figure 3 depicts a sentence in MODS. As we show later, MODS is the most tractable NNF subset we
shall consider (together with OBDD< ). This is not surprising since from the syntax of a sentence in
MODS, one can immediately recover the sentence models.
The languages we have discussed so far are depicted in Figure 4, where arrows denote set inclusion.
Table 1 lists the acronyms of all of these languages, together with their descriptions. Table 2 lists
the key language properties discussed in this section, together with a short description of each.

3. On the Succinctness of Compiled Theories
We have discussed more than a dozen subsets of the NNF language. Some of these subsets are well
known and have been studied extensively in the computer science literature. Others, such as DNNF
(Darwiche, 2001a, 1999) and d-DNNF (Darwiche, 2001b), are relatively new. The question now is:
What subset should one adopt for a particular application? As we argue in this paper, that depends

234

fiA Knowledge Compilation Map

Acronym
NNF
DNNF
d-NNF
s-NNF
f-NNF
d-DNNF
sd-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP
MODS

Description
Negation Normal Form
Decomposable Negation Normal Form
Deterministic Negation Normal Form
Smooth Negation Normal Form
Flat Negation Normal Form
Deterministic Decomposable Negation Normal Form
Smooth Deterministic Decomposable Negation Normal Form
Binary Decision Diagram
Free Binary Decision Diagram
Ordered Binary Decision Diagram
Ordered Binary Decision Diagram (using order <)
Disjunctive Normal Form
Conjunctive Normal Form
Prime Implicates
Prime Implicants
Models

Table 1: Language acronyms.

Property
Flatness
Simple Disjunction
Simple Conjunction
Decomposability
Determinism
Smoothness
Decision
Ordering

Short Description
The height of NNF is at most 2
Every disjunction is a clause, where literals share no variables
Every conjunction is a term, where literals share no variables
Conjuncts do not share variables
Disjuncts are logically disjoint
Disjuncts mention the same set of variables
A node of the form true, false, or (X    X  ),
where X is a variable and ,  are decision nodes
Decision variables appear in the same order on any path in the NNF

Table 2: Language properties.

235

fiDarwiche & Marquis

on three key properties of the language: its succinctness, the class of tractable queries it supports,
and the class of tractable transformations it admits.
Our goal in this and the following sections is to construct a map on which we place different
subsets of the NNF language according to the above criteria. This map will then serve as a guide to
system designers in choosing the target compilation language most suitable to their application. It
also provides an example paradigm for studying and evaluating further target compilation languages.
We start with a study of succinctness2 in this section (Gogic, Kautz, Papadimitriou, & Selman, 1995).
Definition 3.1 (Succinctness) Let L1 and L2 be two subsets of NNF. L1 is at least as succinct
as L2 , denoted L1  L2 , iff there exists a polynomial p such that for every sentence   L2 , there
exists an equivalent sentence   L1 where ||  p(||). Here, || and || are the sizes of  and ,
respectively.
We stress here that we do not require that there exists a function that computes  given  in
polytime; we only require that a polysize  exists. Yet, our proofs in Appendix A contain specific
algorithms for computing  from  in certain cases. The relation  is clearly reflexive and transitive,
hence, a pre-ordering. One can also define the relation <, where L1 < L2 iff L1  L2 and L2 6 L1 .
Proposition 3.1 The results in Table 3 hold.
An occurrence of  in the cell of row r and column c of Table 3 means that the fragment Lr
given at row r is at least as succinct as the fragment Lc given at column c. An occurrence of 6 (or
6 ) means that Lr is not at least as succinct as Lc (provided that the polynomial hierarchy does
not collapse in the case of 6 ). Finally, the presence of a question mark reflects our ignorance about
whether Lr is at least as succinct as Lc . Figure 5 summarizes the results of Proposition 3.1 in terms
of a directed acyclic graph.
A classical result in knowledge compilation states that it is not possible to compile any propositional formula  into a polysize data structure  such that:  and  entail the same set of clauses,
and clausal entailment on  can be decided in time polynomial in its size, unless NP  P/poly
(Selman & Kautz, 1996; Cadoli & Donini, 1997). This last assumption implies the collapse of the
polynomial hierarchy at the second level (Karp & Lipton, 1980), which is considered very unlikely.
We use this classical result from knowledge compilation in some of our proofs of Proposition 3.1,
which explains why some of its parts are conditioned on the polynomial hierarchy not collapsing.
We have excluded the subsets BDD, s-NNF, d-NNF and f-NNF from Table 3 since they do not
qualify as target compilation languages (see Section 4). We kept NNF and CNF though given their
importance. Consider Figure 5 which depicts Table 3 graphically. With the exception of NNF and
CNF, all other languages depicted in Figure 5 qualify as target compilation languages. Moreover, with
the exception of language PI, DNNF is the most succinct among all target compilation languageswe
know that PI is not more succinct than DNNF, but we do not know whether DNNF is more succinct
than PI.
In between DNNF and MODS, there is a succinctness ordering of target compilation languages:
DNNF <

d-DNNF <

FBDD <

OBDD <

OBDD<

< MODS.

DNNF is obtained by imposing decomposability on NNF; d-DNNF by adding determinism; FBDD by
adding decision; and OBDD and OBDD< by adding ordering (w.r.t. any total ordering on PS in the
first case and a specific one in the second case). Adding each of these properties reduces language
succinctness (assuming that the polynomial hierarchy does not collapse).
One important fact to stress here is that adding smoothness to d-DNNF does not affect its succinctness: the sd-DNNF and d-DNNF languages are equally succinct. It is also interesting to compare
2. A more general notion of space efficiency (model preservation for polysize reductions) exists (Cadoli, Donini,
Liberatore, & Schaerf, 1996), but we do not need its full generality here.

236

fiA Knowledge Compilation Map

L
NNF
DNNF
d-DNNF
sd-DNNF
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP
MODS

NNF

6
6
6
6
6
6
6
6
6
6
6

DNNF


6
6
6
6
6
6
6
6
6
6

d-DNNF




6
6
6
6
6
6
6
6

sd-DNNF




6
6
6
6
6
6
6
6

FBDD





6
6
6
6
6
6
6

OBDD






6
6
6
6
6
6

OBDD<







6
6
6
6
6

DNF


6
6
6
6
6

6
6
6
6

CNF

6
6
6
6
6
6
6

6
6
6

PI

?
?
?
6
6
6
6


6
6

IP


?
?
6
6
6

6
6

6

MODS









?



Table 3: Succinctness of target compilation languages.  means that the result holds unless the
polynomial hierarchy collapses.

NNF
DNNF
sd-DNNF

=

CNF

d-DNNF
DNF

FBDD

PI
OBDD
IP
OBDD<
MODS

Figure 5: An edge L1  L2 indicates that L1 is strictly more succinct than L2 : L1 < L2 , while
L1 = L2 indicates that L1 and L2 are equally succinct: L1  L2 and L2  L1 . Dotted
arrows indicate unknown relationships; for instance, the dotted arrow from DNNF to PI
means that we do not know whether DNNF is at least as succinct as PI. Some of the edges
are conditioned on the polynomial hierarchy not collapsingsee Table 3.

sd-DNNF (which is more succinct than the influential FBDD, OBDD and OBDD< languages) with MODS,
which is a most tractable language. Both sd-DNNF and MODS are smooth, deterministic and decomposable. MODS, however, is flat and obtains its decomposability from the stronger condition
of simple-conjunction. Therefore, sd-DNNF can be viewed as the result of relaxing from MODS the
flatness and simple-conjunction conditions, while maintaining decomposability, determinism and
smoothness. Relaxing these conditions moves the language three levels up the succinctness hierarchy, although it compromises only the polytime test for sentential entailment and possibly the one
for equivalence as we show in Section 4.

237

fiDarwiche & Marquis

4. Querying a Compiled Theory
In evaluating the suitability of a target compilation language to a particular application, the succinctness of the language must be balanced against the set of queries and transformations that it
supports in polytime. We consider in this section a number of queries, each of which returns valuable information about a propositional theory, and then identify target compilation languages which
provide polytime algorithms for answering such queries. We restrict our attention in this paper to
the existence of polytime algorithms for answering queries, but we do not present the algorithms
themselves. The interested reader is referred to (Darwiche, 2001a, 2001b, 1999; Bryant, 1986) for
some of these algorithms and to the proofs of theorems in Appendix A for others.
The queries we consider are tests for consistency, validity, implicates (clausal entailment), implicants, equivalence, and sentential entailment. We also consider counting and enumerating theory
models; see Table 4. One can also consider computing the probability of a propositional sentence,
assuming that all variables are probabilistically independent. For the subsets we consider, however,
this can be done in polytime whenever models can be counted in polytime.
From here on, L denotes a subset of language NNF.
Definition 4.1 (CO, VA) L satisfies CO (VA) iff there exists a polytime algorithm that maps
every formula  from L to 1 if  is consistent (valid), and to 0 otherwise.
One of the main applications of compiling a theory is to enhance the efficiency of answering
clausal entailment queries:
Definition 4.2 (CE) L satisfies CE iff there exists a polytime algorithm that maps every formula
 from L and every clause  from NNF to 1 if  |=  holds, and to 0 otherwise.
A key application of clausal entailmentVis in testing equivalence. Specifically, suppose we have a
designVexpressed as a set of clauses d = i i and a specification expressed also as a set of clauses
s = j j , and we want to test whether the design and specification are equivalent. By compiling
each of d and s to targets d and s that support a polytime clausal entailment test, we can test
the equivalence of d and s in polytime. That is, d and s are equivalent iff d |= j for all j
and s |= i for all i.
A number of the target compilation languages we shall consider support a direct polytime equivalent test:
Definition 4.3 (EQ, SE) L satisfies EQ (SE) iff there exists a polytime algorithm that maps every
pair of formulas ,  from L to 1 if    ( |= ) holds, and to 0 otherwise.
Note that sentential entailment (SE) is stronger than clausal entailment and equivalence. Therefore,
if a language L satisfies SE, it also satisfies CE and EQ.
For completeness, we consider the following dual to CE:
Definition 4.4 (IM) L satisfies IM iff there exists a polytime algorithm that maps every formula
 from L and every term  from NNF to 1 if  |=  holds, and to 0 otherwise.
Finally, we consider counting and enumerating models:
Definition 4.5 (CT) L satisfies CT iff there exists a polytime algorithm that maps every formula
 from L to a nonnegative integer that represents the number of models of  (in binary notation).
Definition 4.6 (ME) L satisfies ME iff there exists a polynomial p(., .) and an algorithm that
outputs all models of an arbitrary formula  from L in time p(n, m), where n is the size of  and
m is the number of its models (over variables occurring in ).

238

fiA Knowledge Compilation Map

Notation
CO
VA
CE
IM
EQ
SE
CT
ME

Query
polytime consistency check
polytime validity check
polytime clausal entailment check
polytime implicant check
polytime equivalence check
polytime sentential entailment check
polytime model counting
polytime model enumeration

Table 4: Notations for queries.

L
NNF
DNNF
d-NNF
s-NNF
f-NNF
d-DNNF
sd-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP
MODS

CO

















VA








CE































IM








SE












CT








ME








EQ





?
?

?

















































Table 5: Subsets of the NNF language and their corresponding polytime queries.
and  means does not satisfy unless P = NP.



means satisfies

Table 4 summarizes the queries we are interested in and their acronyms.
The following proposition states what we know about the availability of polytime algorithms for
answering the above queries, with respect to all languages we introduced in Section 2.
Proposition 4.1 The results in Table 5 hold.
The results of Proposition 4.1 are summarized in Figure 4. One can draw a number of conclusions
based on the results in this figure. First, NNF, s-NNF, d-NNF, f-NNF, and BDD fall in one equivalence
class that does not support any polytime queries and CNF satisfies only VA and IM; hence, none
of them qualifies as a target compilation language in this case. But the remaining languages all
support polytime tests for consistency and clausal entailment. Therefore, simply imposing either
of smoothness (s-NNF), determinism (d-NNF), flatness (f-NNF), or decision (BDD) on the NNF language does not lead to tractability with respect to any of the queries we considerneither of these
properties seem to be significant in isolation. Decomposability (DNNF), however, is an exception and
leads immediately to polytime tests for both consistency and clausal entailment, and to a polytime
algorithm for model enumeration.
239

fiDarwiche & Marquis

Recall the succinctness ordering DNNF < d-DNNF < FBDD < OBDD < OBDD< < MODS
from Figure 5. By adding decomposability (DNNF), we obtain polytime tests for consistency and
clausal entailment, in addition to a polytime model enumeration algorithm. By adding determinism
to decomposability (d-DNNF), we obtain polytime tests for validity, implicant and model counting,
which are quite significant. It is not clear, however, whether the combination of decomposability and
determinism leads to a polytime test for equivalence. Moreover, adding the decision property on top
of decomposability and determinism (FBDD) does not appear to increase tractability with respect to
the given queries3 , although it does lead to reducing language succinctness as shown in Figure 5. On
the other hand, adding the ordering property on top of decomposability, determinism and decision,
leads to polytime tests for equivalence (OBDD and OBDD< ) as well as sentential entailment provided
that the ordering < is fixed (OBDD< ).
As for the succinctness ordering NNF < DNNF < DNF < IP < MODS from Figure 5, note that
DNNF is obtained by imposing decomposability on NNF, while DNF is obtained by imposing flatness
and simple-conjunction (which is stronger than decomposability). What is interesting is that DNF is
less succinct than DNNF, yet does not support any more polytime queries; see Figure 4. However, the
addition of smoothness (and determinism) on top of flatness and simple-conjunction (MODS) leads to
five additional polytime queries, including equivalence and entailment tests.4
We close this section by noting that determinism appears to be necessary (but not sufficient) for
polytime model counting: only deterministic languages, d-DNNF, sd-DNNF, FBDD, OBDD, OBDD< and
MODS, support polytime counting. Moreover, polytime counting implies a polytime test of validity,
but the opposite is not true.

5. Transforming a Compiled Theory
A query is an operation that returns information about a theory without changing it. A transformation, on the other hand, is an operation that returns a modified theory, which is then operated
on using queries. Many applications require a combination of transformations and queries.
Definition 5.1 (C, C) Let L be a subset of NNF. L satisfies C (C) iff there exists a polytime
algorithm that maps every finite set of formulas 1 , . . . , n from L to a formula of L that is logically
equivalent to 1  . . .  n (1  . . .  n ).
Definition 5.2 (C) Let L be a subset of NNF. L satisfies C iff there exists a polytime algorithm
that maps every formula  from L to a formula of L that is logically equivalent to .
If a language satisfies one of the above properties, we will say that it is closed under the corresponding operator. Closure under logical connectives is important for two key reasons. First, it has
implications on how compilers are constructed for a given target language. For example, if a clause
can be easily compiled into some language L, then closure under conjunction implies that compiling
a CNF sentence into L is easy. Second, it has implications on the class of polytime queries supported
by the target language: If a language L satisfies CO and is closed under negation and conjunction,
then it must satisfy SE (to test whether  |= , all we have to do, by the Refutation Theorem,
is test whether    is inconsistent). Similarly, if a language satisfies VA and is closed under
negation and disjunction, it must satisfy SE by the Deduction Theorem.
3. Deciding the equivalence of two sentences in FBDD, d-DNNF, or in sd-DNNF, can be easily shown to be in coNP.
However, we do not have a proof of coNP-hardness, nor do we have deterministic polytime algorithms for deciding
these problems. Actually, the latter case is quite unlikely as the equivalence problem for FBDD has been intensively
studied, with no such algorithm in sight. Note, however, that the equivalence of two sentences in FBDD can be
decided probabilistically in polytime (Blum, Chandra, & Wegman, 1980), and similarly for sentences in d-DNNF
(Darwiche & Huang, 2002).
4. Given flatness, simple-conjunction and smoothness, we can obtain determinism by simply removing duplicated
terms.

240

fiA Knowledge Compilation Map

It is important to stress here that some languages are closed under a logical operator, only if the
number of operands is bounded by a constant. We will refer to this as bounded closure.
Definition 5.3 (BC, BC) Let L be a subset of NNF. L satisfies BC (BC) iff there exists
a polytime algorithm that maps every pair of formulas  and  from L to a formula of L that is
logically equivalent to    (  ).
We now turn to another important transformation:
Definition 5.4 (Conditioning) (Darwiche, 1999) Let  be a propositional formula, and let  be
a consistent term. The conditioning of  on , noted  | , is the formula obtained by replacing
each variable X of  by true (resp. false) if X (resp. X) is a positive (resp. negative) literal of .
Definition 5.5 (CD) Let L be a subset of NNF. L satisfies CD iff there exists a polytime algorithm
that maps every formula  from L and every consistent term  to a formula from L that is logically
equivalent to  | .
Conditioning has a number of applications, and corresponds to restriction in the literature on
Boolean functions. The main application of conditioning is due to a theorem, which says that   
is consistent iff  |  is consistent (Darwiche, 2001a, 1999). Therefore, if a language satisfies CO
and CD, then it must also satisfy CE. Conditioning also plays a key role in building compilers
that enforce decomposability. If two sentences 1 and 2 are both decomposable (belong to DNNF),
their conjunction 1  2 is not necessarily decomposable since the sentences may share variables.
Conditioning
can be used to ensure decomposability in this case since 1  2 is equivalent to
W
(
|
)

(
1
2 | )  , where  is a term covering all variables shared by 1 and 2 . Note that
W
(
|
)

(
1
2 | )   must be decomposable since 1 |  and 2 |  do not mention variables

in . The previous proposition is indeed a generalization to multiple variables of the well-known
Shannon expansion in the literature on Boolean functions. It is also the basis for compiling CNF into
DNNF (Darwiche, 1999, 2001a).
Another critical transformation we shall consider is that of forgetting (also referred to as marginalization, or elimination of middle terms (Boole, 1854)):
Definition 5.6 (Forgetting) Let  be a propositional formula, and let X be a subset of variables
from PS. The forgetting of X from , denoted X., is a formula that does not mention any variable
from X and for every formula  that does not mention any variable from X, we have  |=  precisely
when X. |= .
Therefore, to forget variables from X is to remove any reference to X from , while maintaining all
information that  captures about the complement of X. Note that X. is unique up to logical
equivalence.
Definition 5.7 (FO, SFO) Let L be a subset of NNF. L satisfies FO iff there exists a polytime
algorithm that maps every formula  from L and every subset X of variables from PS to a formula
from L equivalent to X.. If the property holds for singleton X, we say that L satisfies SFO.
Forgetting is an important transformation as it allows us to focus/project a theory on a set of
variables. For example, if we know that some variables X will never appear in entailment queries,
we can forget these variables from the compiled theory while maintaining its ability to answer
such queries correctly. Another application of forgetting is in counting/enumerating the instantiations of some variables Y, which are consistent with a theory . This query can be answered by
counting/enumerating the models of X., where X is the complement of Y. Forgetting also has
applications to planning, diagnosis and belief revision. For instance, in the SATPLAN framework,

241

fiDarwiche & Marquis

Notation
CD
FO
SFO
C
BC
C
BC
C

Transformation
polytime conditioning
polytime forgetting
polytime singleton forgetting
polytime conjunction
polytime bounded conjunction
polytime disjunction
polytime bounded disjunction
polytime negation

Table 6: Notations for transformations.

L
NNF
DNNF
d-NNF
s-NNF
f-NNF
d-DNNF
sd-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP
MODS

CD

















FO













SFO

























C


BC





C





BC




































































C





?
?











Table 7: Subsets of the NNF language and their polytime transformations.
means satisfies, 
means does not satisfy, while  means does not satisfy unless P=NP.

compiling away fluents or actions amounts to forgetting variables. In model-based diagnosis, compiling away every variable except the abnormality ones does not remove any piece of information
required to compute the conflicts and the diagnoses of a system (Darwiche, 2001a). Forgetting has
also been used to design update operators with valuable properties (Herzig & Rifi, 1999).
Table 6 summarizes the transformations we are interested in and their acronyms. The following
proposition states what we know about the tractability of these transformations with respect to the
identified target compilation languages.
Proposition 5.1 The results in Table 7 hold.
One can draw a number of observations regarding Table 7. First, all languages we consider satisfy
CD and, hence, lend themselves to efficient application of the conditioning transformation. As for
forgetting multiple variables, only DNNF, DNF, PI and MODS permit that in polytime. It is important
to stress here that none of FBDD, OBDD and OBDD< permits polytime forgetting of multiple variables.
This is noticeable since some of the recent applications of OBDD< to planningwithin the so-called
symbolic model checking approach to planning (A. Cimmati & Traverso, 1997)depend crucially
242

fiA Knowledge Compilation Map

on the operation of forgetting and it may be more suitable to use a language that satisfies FO in
this case. Note, however, that OBDD and OBDD< allow the forgetting of a single variable in polytime,
but FBDD does not allow even that. d-DNNF is similar to FBDD as it satisfies neither FO nor SFO.
It is also interesting to observe that none of the target compilation languages is closed under
conjunction. A number of them, however, are closed under bounded conjunction, including OBDD< ,
DNF, IP and MODS.
As for disjunction, the only target compilation languages that are closed under disjunction are
DNNF and DNF. The OBDD< and PI languages, however, are closed under bounded disjunction. Again,
the d-DNNF, FBDD and OBDD languages are closed under neither.
The only target compilation languages that are closed under negation are FBDD, OBDD and OBDD< ,
while it is not known whether d-DNNF or sd-DNNF are closed under this operation. Note that d-DNNF
and FBDD support the same set of polytime queries (equivalence checking is unknown for both)
so they are indistinguishable from that viewpoint. Moreover, the only difference between the two
languages in Table 7 is the closure of FBDD under negation, which does not seem to be that significant
in light of no closure under either conjunction or disjunction. Note, however, that d-DNNF is more
succinct than FBDD as given in Figure 5.
Finally, OBDD< is the only target compilation language that is closed under negation, bounded
conjunction, and bounded disjunction. This closure actually plays an important role in compiling
propositional theories into OBDD< and is the basis of state-of-the-art compilers for this purpose
(Bryant, 1986).

6. Conclusion
The main contribution of this paper is a methodology for analyzing propositional compilation approaches according to two key dimensions: the succinctness of the target compilation language, and
the class of queries and transformations it supports in polytime. The second main contribution
of the paper is a comprehensive analysis, according to the proposed methodology, of more than
a dozen languages for which we have produced a knowledge compilation map, which cross-ranks
these languages according to their succinctness, and the polytime queries and transformations they
support. This map allows system designers to make informed decisions on which target compilation
language to use: after the class of queries/transformations have been decided based on the application of interest, the designer chooses the most succinct target compilation language that supports
such operations in polytime. Another key contribution of this paper is the uniform treatment we
have applied to diverse target compilation languages, showing how they all are subsets of the NNF
language. Specifically, we have identified a number of simple, yet meaningful, properties, including
decomposability, determinism, decision and flatness, and showed how combinations of these properties give rise to different target compilation languages. The studied subsets include some well known
languages such as PI, which has been influential in AI; OBDD< , which has been influential in formal
verification; and CNF and DNF, which have been quite influential in computer science. The subsets
also include some relatively new languages such as DNNF and d-DNNF, which appear to represent
interesting, new balances between language succinctness and query/transformation tractability.

Acknowledgments
This is a revised and extended version of the paper A Perspective on Knowledge Compilation,
in Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI01), pp.
175-182, 2001. We wish to thank Alvaro del Val, Mark Hopkins, Jerome Lang and the anonymous
reviewers for some suggestions and comments, as well as Ingo Wegener for his help with some of
the issues discussed in the paper. This work has been done while the second author was a visiting
researcher with the Computer Science Department at UCLA. The first author has been partly
243

fiDarwiche & Marquis

supported by NSF grant IIS-9988543 and MURI grant N00014-00-1-0617. The second author has
been partly supported by the IUT de Lens, the Universite dArtois, the Nord/Pas-de-Calais Region
under the TACT-TIC project, and by the European Community FEDER Program.

Appendix A. Proofs
To simplify the proofs of our main propositions later on, we have identified a number of lemmas that
we list below. Some of the proofs of these lemmas are direct, but we include them for completeness.
Lemma A.1 Every sentence in d-DNNF can be translated to an equivalent sentence in sd-DNNF in
polytime.
Proof: Let  = 1  . . .  n be an or-node in a d-DNNF sentence . Suppose that  is not smooth
andWlet V = V
Vars(). Consider now the sentence s obtained by replacing in  each such node
n
by i=1 i  vV \Vars(i ) (v  v). Then s is equivalent to  and is smooth. Moreover, s can
be computed in time polynomial in the size of  and it satisfies decomposability and determinism. 2
Lemma A.2 Every sentence in FBDD can be translated to an equivalent sentence in FBDD  s-NNF
in polytime.
Proof: Let  be a sentence in FBDD and let  be a node in . We can always replace  with (Y )
(Y  ), for some variable Y , while preserving equivalence and the decision property. Moreover,
as long as the variable Y does not appear in  and is not an ancestor of , then decomposability is
also preserved (that is, the resulting sentence is in FBDD). Note here that ancestor is with respect
to the binary decision diagram notation of see left of Figure 2.
Now, suppose that (X  )  (X  ) is an or-node in . Suppose further that the or-node
is not smooth. Hence, there is some Y which appears in Vars() but not in Vars() (or the other
way around). Since  is decomposable, then Y cannot be an ancestor of  (since in that case it
would also be an ancestor of , which is impossible by decomposability of ). Hence, we can replace  with (Y  )  (Y  ), while preserving equivalence, decision and decomposability. By
repeating the above process, we can smooth  while preserving all the necessary properties. Finally,
note that for every or-node (X  )  (X  ) in , we need to repeat the above process at most
| Vars()  Vars() | + | Vars()  Vars() | times. Hence, the smoothing operation can be performed in polytime.
2
Lemma A.3 If a subset L of NNF satisfies CO and CD, then it also satisfies ME.
Proof: Let  be a sentence in L. First, we test if  is inconsistent (can be done in polytime). If
it is, we return the empty set of models. Otherwise, we construct a decision-tree representation of
the models of . Given an ordering of the variables x1 , . . . , xn of Vars(), we start with a tree
T consisting of a single root node. For i = 1 to n, we repeat the following for each leaf node 
(corresponds to a consistent term) in T :
a. If  |   xi is consistent, we add xi as a child to ;
b. If  |   xi is consistent, we add xi as a child to .
The key points are:
 Test (a) and Test (b) can be performed in time polynomial in the size of  (since L satisfies
CO and CD).

244

fiA Knowledge Compilation Map

 Either Test (a) or Test (b) above must succeed (since  is consistent).
Hence, the number of tests performed is O(mn), where m is the number of leaf nodes in the final
decision tree (bounded by the number of models of ) and n is the number of variables of .
2
Lemma A.4 If a subset of NNF satisfies CO and CD, then it also satisfies CE.
Proof: To test whether sentence  entails non-valid clause ,  |= , it suffices to test whether
 |  is inconsistent (Darwiche, 2001a).
2
Lemma A.5 Let  and  be two sentences that share no variables. Then    is valid iff  is valid
or  is valid.
Proof:    is valid iff    is inconsistent. Since  and  share no variables, then   
is inconsistent iff  is inconsistent or  is. This is true iff  is valid or  is valid.
2
Lemma A.6 Let  be a sentence in d-DNNF and let  be a clause. Then a sentence in d-DNNF
which is equivalent to    can be constructed in polytime in the size of  and .
Wn
Vi1
Proof: Let l1 , . . . , ln be the literals that appear in clause . Then  = i=1 (li  j=1 lj ) is
equivalent to clause , is in d-DNNF, and can be constructed in polytime in size of . Now let  be
the term equivalent to . We have that    is equivalent to (( | )  )  . The last sentence
is in d-DNNF and can be constructed in polytime in size of  and .
2
Lemma A.7 If a subset of NNF satisfies VA and CD, then it also satisfies IM.
Proof: To test whether a consistent term  entails sentence ,  |= , it suffices to test whether
   is valid. This sentence is equivalent to   (  ), to   (  ( | )), and to   ( | ).
Since  and  |  share no variables, the disjunction is valid iff  is valid or  |  is valid (by
Lemma A.5).  cannot be valid since  is consistent.  |  can be constructed in polytime since
the language satisfies CD and its validity can be tested in polytime since the language satisfies VA. 2
Lemma A.8 Every CNF or DNF formula can be translated to an equivalent sentence in BDD in
polytime.
Proof: It is straightforward to convert a clause or term into an equivalent sentence in BDD. In order
to generate a BDD sentence corresponding to the conjunction (resp. disjunction) of BDD sentences 
and , it is sufficient to replace the 1-sink (resp. 0-sink) of  with the root of .
2
Lemma A.9 If a subset of NNF satisfies EQ, then it satisfies CO and VA.
Proof: true and false belong to every NNF subset.  is inconsistent iff it is equivalent to false.  is
valid iff it is equivalent to true.
2
Lemma A.10 If a subset of NNF satisfies SE, then it satisfies EQ, CO and VA.
Proof: Sentences 1 and 2 are equivalent iff 1 |= 2 and 2 |= 1 . EQ implies CO and VA
(Lemma A.9).
2

245

fiDarwiche & Marquis

Lemma A.11 Let  be a sentence in d-DNNF and let  be a clause. The validity of    can be
tested in time polynomial in the size of  and .
Proof: Construct    in polytime as given in Lemma A.6 and check its validity, which can be
done in polytime too.
2
Lemma A.12 For every propositional formula  and every consistent term , we have | is
equivalent to
Vars().(  ).
Proof: Without loss of generality, assume that  is given by the disjunctively-interpreted set of
its models (over Vars()). Conditioning  on  leads (1) to removing every model of , then
(2) projecting the remaining models so that every variable of  is removed. Conjoining  with 
leads exactly to (1), while forgetting every variable of  in the resulting formula leads exactly to (2)
(Lang, Liberatore, & Marquis, 2000).
2
Lemma A.13 Each sentence  in f-NNF can be converted into an equivalent sentence  in polynomial time, where   CNF or   DNF.
Proof: We consider three cases for the sentence :
1. The root node of  is an and-node. In this case,  can be turned into a CNF sentence  in
polynomial time by simply ensuring that each or-node in  is a clause (that is, a disjunction
of literals that share no variables). Let C be an or-node in . Since  is flat and its root is
an and-node, C must be a child of the root of  and the children of C must be leaves. Hence,
we can easily ensure that C is a clause as follows:
 If we have one edge from C to some leaf X and another edge from C to X (C is valid),
we replace the edge from the root to C by an edge from the root to true.
 If we have more than one edge from C to the same leaf node X, we keep only one of these
edges and delete the rest.
2. The root of  is an or-node.  can be turned into a DNF sentence  in a dual way.5
3. The root of  is a leaf node.  is already a CNF sentence.
2
Lemma A.14  is a prime implicant (resp. an essential prime implicant) of sentence  iff  is
a prime implicate (resp. an essential prime implicate) of . 6
Proof: This is a folklore result, immediate from the definitions.

2

Proof of Proposition 3.1
The proof of this proposition is broken down into eight steps. In each step, we prove a number of
succinctness relationships between different languages, and then apply transitivity of the succinctness
relation to infer even more relationships. Associated with each step of the proof is a table in which

246

fiA Knowledge Compilation Map

L

NNF

DNNF

d-DNNF

FBDD

OBDD

OBDD<

DNF

CNF

PI

IP

MODS

sd-DNNF

NNF
DNNF
d-DNNF
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP
MODS
sd-DNNF































































Table 8:
L

NNF

DNNF

d-DNNF

FBDD

OBDD

OBDD<

DNF

CNF

PI

IP

MODS

sd-DNNF

NNF
DNNF
d-DNNF
FBDD
OBDD
OBDD<














































DNF

6



6

6





CNF

6

6

6





6

PI

6

6

6

6



6

IP
MODS
sd-DNNF

6

6

6

6

6







Table 9:
we mark all relationships that are proved in that stepwe dont show these marks in the very first
table though.
Table 8: Follows immediately from the language inclusions reported in Figure 4.
Table 9: We can prove both that DNF 6 PI and CNF 6 IP (this slightly generalizes the results
DNF 6 CNF and CNF 6 DNF given in (Gogic et al., 1995)).
Vn1
Let us consider the CNF formula n = i=0 (x2i  x2i+1 ). This formula is in prime implicates
form7 (and each clause in n is an essential prime implicate of it). Hence its negation n is in
prime implicants form (as an easy consequence of Lemma A.14).
Since Quines early work (Quine, 1959), we know that the number of essential prime implicants
(resp. prime implicates) of a formula is a lower bound of the number of terms (resp. clauses) that
can be found in any DNF (resp. CNF) representation of it (indeed, any such representation must
include the essential prime). n has 2n essential prime implicants. Indeed, this can be easily shown
by induction on n given that (i) every literal occurring in n occurs only once, (ii) the set of prime
implicants of any nontautological clause is the set of literals occurring in it (up to logical equivalence),
and (iii) the distribution property for prime implicants (see e.g., (dual of) Proposition 40 in (Marquis,
2000)) which states that IP ( ) = max({PI PI | PI  IP (), PI  IP ()}, |=) (up to logical
equivalence). Subsequently, n has 2n essential prime implicates (cf. Lemma A.14). Accordingly,
we obtain that both DNF 6 PI and CNF 6 IP. We also obtain PI 6 IP and IP 6 PI. Now, it is
wellknown that some DNF formulas have exponentially many prime implicants (see the proof of
Proposition 5.1 where we show that IP does not satisfy SFO). Hence, their negations are CNF
5. Note that f-NNF satisfies C and that the negation of a CNF sentence (resp. DNF sentence) can be turned into a
DNF (resp. CNF) in linear time.
6. A prime implicant (resp. a prime implicate)  of  is essential iff the disjunction (resp. conjunction) of all prime
implicants (resp. prime implicates) of  except  is not equivalent to .
7. The correctness of (the dual of) Quines consensus algorithm for computing prime implicants (Quine, 1955)
ensures it, since no clause of n is subsumed by another clause and no consensi can be performed since there are
no negated variables.

247

fiDarwiche & Marquis

L

NNF

DNNF

d-DNNF

FBDD

OBDD

OBDD<

DNF

CNF

PI

IP

MODS

sd-DNNF

NNF
DNNF
d-DNNF
FBDD
OBDD
OBDD<














































DNF

6

6

6

6

6

6



6

6





CNF

6

6

6

6

6

6

6





6

PI

6

6

6

6

6

6

6

6



6

IP
MODS
sd-DNNF

6

6

6

6

6

6

6

6

6







Table 10:
L

NNF

DNNF

d-DNNF

FBDD

OBDD

NNF
DNNF
d-DNNF


















OBDD<




FBDD

6

6

6







OBDD

6

6

6

6





OBDD<
DNF
CNF
PI
IP
MODS
sd-DNNF

6
6

6

6

6


6
6

6

6

6


6
6

6

6

6


6
6

6

6

6


6
6

6

6

6



6

6

6

6


DNF

CNF

PI

IP

MODS

sd-DNNF




















6

6

6


6

6

6


6


6



6

6










Table 11:
formulas having exponentially many prime implicates. Subsequently IP 6 DNF and PI 6 CNF. The
remaining results in this table follow fromL
the transitivity of .
n1
Table 10: The parity function On = i=0 xi has linear size OBDD< representations (Bryant,
1986) but only exponential size CNF and DNF representations. The reason is that On has 2n
essential prime implicants (resp. essential prime implicates) and the number of essential prime
implicants (resp. essential prime implicates) of a formula is a lower bound of the size of any of its
DNF (resp. CNF) representation. This easily shows that both CNF 6 OBDD and DNF 6 OBDD. The
remaining results in this table follow from the language inclusions reported in Figure 4.
Table 11: It is shown in (Darwiche, 2001b) that there is a sentence in d-DNNF which only
has exponential FBDD representations. Accordingly, we have FBDD 6 d-DNNF. In (Gergov & Meinel,
1994a), it is shownVthat OBDD 6 FBDD. Finally, it is easy to show that OBDD< 6 OBDD (for instance,
n
the formula n = i=1 (xi  yi ) has an OBDD< representation of size polynomial in n whenever <
satisfies x1 < y1 < x2 < . . . < xn < yn , while it has an OBDD< representation of size exponential in
n provided that < is s.t. x1 < x2 < . . . < xn < y1 < y2 < . . . < yn ). The remaining results in this
table follow from the language inclusions reported in Figure 4.
Table 12: L 6 L means that L 6 L unless the polynomial hierarchy PH collapses. The
results in this table follow since the existence of polysize knowledge compilation functions for clausal
entailment implies the collapse of the polynomial hierarchy PH (Selman & Kautz, 1996; Cadoli &
Donini, 1997). Now, if DNNF  CNF, then for each sentence  in CNF there exists a polysize equivalent
sentence  in DNNF. Therefore, we can test whether a clause is entailed by  in polytime by testing
whether the clause is entailed by . This proves the existence of polysize knowledge compilation
functions for clausal entailment, leading to the collapse of the polynomial hierarchy PH. The same
is true for d-DNNF and sd-DNNF since all these languages support a polytime clausal entailment test
(see Proposition 4.1).
Table 13: In (Wegener, 1987) (Theorem 6.2 pp. 436), a family of n2 -variable boolean functions
 is pointed out. Provided that every interpretation I over these n2 variables represents a n-vertices
digraph (for every 1  i, j  n, we have I(xi,j ) = 1 iff (i, j) is an arc of the digraph), (I) = 1 iff the
248

fiA Knowledge Compilation Map

L

NNF

DNNF

d-DNNF

FBDD

OBDD

OBDD<

DNF

CNF

PI

IP

MODS

sd-DNNF

NNF


6 



































6 












DNNF
d-DNNF
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP
MODS

6 


sd-DNNF

6

6
6

6

6

6

6

6



6

6

6

6

6

6

6


6
6

6

6

6

6

6




6

6

6

6

6

6





6

6

6

6

6


6 






6

6

6

6



6

6

6


6

6

6


6


6



6

6






6





Table 12:
L

NNF

DNNF

d-DNNF

FBDD

OBDD

CNF

PI

IP

MODS

sd-DNNF


6
6
















OBDD<




DNF

NNF
DNNF
d-DNNF





6
6














FBDD

6

6

6







6

6

6

6

OBDD

6

6

6

6





6

6

6

6

OBDD<
DNF
CNF
PI
IP
MODS
sd-DNNF

6
6

6

6

6


6
6

6

6

6


6
6

6

6

6


6
6

6

6

6


6
6

6

6

6



6

6

6

6


6

6

6

6


6
6


6

6


6
6



6


6

6

6



6






6



Table 13:
digraph represented by I contains a k-clique of a special kind (k is a parameter of the family). It is
shown that for certain values of k (depending on n), every FBDD representation of  has exponential
size. Moreover, it is shown that  has only a cubic number of prime implicants. This shows that
FBDD 6 IP, hence FBDD 6 DNF. Because FBDD satisfies C (see Proposition 5.1),8 it cannot be the
case that  has a polynomial size FBDD. Since  has only a cubic number of prime implicates, we
obtain that FBDD 6 PI, hence FBDD 6 CNF. The remaining results in this table follow since FBDD
OBDD  OBDD< .
Table 14: Assume that d-DNNF  DNF holds. As a consequence, every sentence  in DNF can be
compiled into an equivalent d-DNNF sentence  of polynomial size. Now, checking whether a clause
 is entailed by the CNF sentence  is equivalent to checking whether the DNF sentence    is
valid. Checking whether ()   is validwhen () is a d-DNNF sentence and  is a clausecan
be achieved in polynomial time by Lemma A.11. Therefore, () is a polysize compilation of the
8. That is, a sentence in FBDD can be negated in polytime to yield a sentence in FBDD too.

L

NNF

DNNF

d-DNNF

FBDD

OBDD

NNF
DNNF



6 











OBDD<



d-DNNF


6 

6

DNF

CNF

PI

IP

MODS

sd-DNNF


6 

6
















6 








FBDD

6

6

6











6

6

6

6

OBDD

6

6

6

6

6





6

6

6

6

OBDD<
DNF

6

6

6

6

6

6



6

6

6

6

6

6

6

6

6

6



6

6



CNF

6

6

6

6

6

6

6





6

PI

6

6

6

6

6

6

6

6



6

IP
MODS

6

6

6

6

6

6

6

6

6



sd-DNNF

6

6


6
6
6
6


6









Table 14:

249

6

6





fiDarwiche & Marquis

L

NNF

DNNF

d-DNNF

FBDD

OBDD

OBDD<

DNF

CNF

PI

IP

MODS

sd-DNNF

NNF
DNNF
d-DNNF


6 

6



6



















6


6 

6














FBDD

6

6

6







6

6

6

6



6

OBDD

6

6

6

6





6

6

6

6



6

OBDD<
DNF

6
6


6
6


6
6


6
6


6
6



6


6


6
6


6
6


6





6
6


CNF
PI

6
6


6
6


6
6


6
6


6
6


6
6


6
6



6





6
6




6
6


IP

6

6

6

6

6

6

6

6

6





6

MODS
sd-DNNF

6

6

6

6

6


6


6


6


6

6

6

6

6

6




6


Table 15:
CNF sentence , allowing clausal entailment to be achieved in polynomial time. The existence of
such () for every CNF sentence  implies the collapse of the polynomial hierarchy (Selman &
Kautz, 1996; Cadoli & Donini, 1997). Hence, we obtain that d-DNNF 6 DNF. As a consequence, we
also have d-DNNF 6 DNNF. Finally, since every d-DNNF sentence can be turned in polynomial time
into an equivalent sd-DNNF sentence by Lemma A.1, we have sd-DNNF  d-DNNF. Moreover, since
d-DNNF  sd-DNNF, we obtain sd-DNNF 6 DNF, sd-DNNF 6 DNNF, sd-DNNF  FBDD, sd-DNNF 
OBDD, sd-DNNF  OBDD< , FBDD 6 sd-DNNF, OBDD< 6 sd-DNNF, DNF 6 sd-DNNF, CNF 6 sd-DNNF, PI
6 sd-DNNF and IP 6 sd-DNNF.
Table 15: Let us now show that WMODS is not less succinct than PI, IP, sd-DNNF and OBDD.
n
First, let us consider the formula  = i=1 xi .  can be represented by PI, IP, sd-DNNF and OBDD
formulas of size polynomial in n. Contrastingly,  cannot be represented by a MODS formula of
size polynomial in n since  has 2n  1 models over Vars(). Now, it is well-known that the old
good Quine-McCluskeys algorithm for generating prime implicants from a MODS representation of
a propositional formula  runs in time polynomial in the number of models of  (Wegener, 1987).
This shows that IP  MODS. As to CNF and OBDD< , it is obvious that a decision tree (or Shannon tree)
for  that respects a given total ordering over Vars() can be generated in polynomial time from a
MODS representation of . Such a decision tree has m 1-leaves where m is the number of models of
 over Vars(). Accordingly, it has at most n  m 0-leaves where n = |Vars()|. Since the set of all
paths from the root of the tree to any 0-leaf can be read as a CNF representation of , we obtain that
CNF  MODS. On the other hand, since reducing a decision tree to derive a corresponding OBDD<
can be done in polynomial time, it follows that an OBDD< representation of  can also be generated
from a MODS representation of it. Hence, OBDD<  MODS. The remaining results in this table follow
from the language inclusions reported in Figure 4. 2
Proof of Proposition 4.1
The proof of this proposition is broken down into twelve steps. In each step, we prove a number
of results. Associated with each step of the proof is a table in which we mark all results that are
proved in that step. The table of the last step includes all results declared by this proposition.
Table 16: Every classical CNF or DNF formula can be translated in a straightforward way into
an equivalent f-NNF sentence (with a tree structure) in polytime. Moreover, every NNF sentence
can be translated into an equivalent s-NNF sentence in polytime (Lemma A.1). Given that CO
is NP-hard (resp. VA is coNP-hard) for classical CNF (resp. DNF) sentences, and the inclusion
between the various NNF subsets reported in Figure 4, we obtain the table.
Table 17: SE implies both CO and VA (Lemma A.10). Moreover, since CT implies both CO
and VA, IM implies VA (valid term), and CE implies CO (inconsistent clause), we obtain the
table.

250

fiA Knowledge Compilation Map

L

CO

VA

NNF





DNNF
d-NNF
d-DNNF
BDD
FBDD
OBDD
OBDD<

CE

IM

EQ

CT

SE

EQ

ME



DNF



CNF
PI
IP
MODS



s-NNF





f-NNF
sd-DNNF





Table 16:

L

CO

VA

CE

IM

CT

SE

NNF





























DNNF
d-NNF
d-DNNF
BDD
FBDD
OBDD
OBDD<



DNF



CNF
PI
IP
MODS





s-NNF













f-NNF
sd-DNNF













ME

Table 17:

L

CO

VA

CE

IM

CT

SE

NNF
DNNF
d-NNF
d-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP

























MODS
s-NNF
f-NNF
sd-DNNF
















EQ








Table 18:

251







ME

fiDarwiche & Marquis

L

CO

VA

CE

IM

CT

SE

NNF





























DNNF
d-NNF
d-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP
MODS
s-NNF
f-NNF
sd-DNNF







EQ

ME










































Table 19:
L

CO

VA

CE

IM

CT

SE

ME

NNF
























DNNF
d-NNF
d-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP





EQ















































































s-NNF















f-NNF


















MODS

sd-DNNF





Table 20:
Table 18: A sentence  is consistent (resp. valid) iff it has a model (resp. 2n models, where
n = |Vars()|). Moreover, the number of models of  is given by the number of edges outgoing
from the or-node in any MODS representation of . Accordingly, CO, VA and CT can be achieved
in polynomial time when  is given by a MODS formula which gives us the table.
Table 19:Because DNNF satisfies CE (Darwiche, 2001a), CE implies CO and MODS  DNF 
DNNF, IP DNF and
sd-DNNF  d-DNNF  DNNF, we obtain the table.
Table 20: We now use the following results:
CD and CO imply CE (Lemma A.4).
CD and VA imply IM (Lemma A.7).
CD and CO imply ME (Lemma A.3).
All considered NNF subsets satify CD (cf. Proposition 5.1).
If an NNF subset does not satisfy CO it cannot satisfy ME.
It is well-known that FBDD satisfies CO, VA and CT, and that OBDD< satisfies (in addition)
EQ (Gergov & Meinel, 1994a; Bryant, 1992).
Since  |=  holds iff    is inconsistent and since OBDD< satisfies CO, C and BC (cf.
Proposition 5.1), OBDD< also satisfies SE.
252

fiA Knowledge Compilation Map

L

CO

VA

CE

IM

CT

SE

ME

NNF
DNNF
d-NNF
d-DNNF
BDD
FBDD
OBDD
OBDD<
DNF





































CNF







PI
IP
MODS
s-NNF
f-NNF
sd-DNNF













EQ
























































Table 21:
L

CO

VA

CE

IM

CT

SE

ME

NNF
DNNF






















d-NNF
d-DNNF


















BDD
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP
MODS
s-NNF
f-NNF
sd-DNNF






































































EQ





















Table 22:
Obviously enough, any query concerning OBDD is equivalent to the corresponding query concerning OBDD< provided that only one DAG is brought into play. Together with the above
results, we conclude that OBDD satisfies CO, VA and CT. Since this fragment satisfies CD as
well, it satisfies CE, IM and ME in addition. It also satisfies EQ (see Theorem 8.11 from
(Meinel & Theobald, 1998)) but does not satisfy SE (unless P = NP). Indeed, it is known
that checking the consistency of two OBDD< formulas  and  (based on two different variable orderings <) is NP-complete (Lemma 8.14 from (Meinel & Theobald, 1998)). Since OBDD
satisfies C and since    is consistent iff  6|= , checking sentential entailment for OBDD
formulas is coNP-complete.
These results lead to the table.
Table 21: It is known that IM is satisfied by classical CNF formulas (hence, PI) (in order to
check whether a non-valid clause is implied by a consistent term, it is sufficient to test that they
share a literal). CNF (hence, PI) is also known to satisfy VA. We then obtain the table.
Table 22: Every sentence in CNF or DNF can be turned into an equivalent sentence in BDD in
polytime (Lemma A.8). Hence, a  in a CNF or DNF cell implies a  in the corresponding BDD cell.
Similarly, since BDD  d-NNF, a  in a BDD cell implies a  in the corresponding d-NNF cell. This
leads to the table.
Table 23: Since EQ implies CO and VA (Lemma A.9), a  in a CO or VA cell implies a  in
the corresponding EQ cell. This leads to the table.
Table 24: By definition, PI satisfies CE and IP satisfies IM. Since PI  CNF and IP  DNF,
this implies that both PI and IP satisfy SE. Now, SE implies EQ, hence both PI and IP satisfy EQ
(actually, two equivalent formulas share the same prime implicates and the same prime implicants
(both forms are canonical ones, provided that one representative per equivalence class is considered,
253

fiDarwiche & Marquis

L

CO

VA

CE

IM

EQ

CT

SE

ME

NNF




























d-NNF
d-DNNF




















BDD
FBDD
OBDD
OBDD<





















































DNNF

DNF
CNF
PI
IP
MODS































s-NNF

















f-NNF
sd-DNNF






















Table 23:

L

CO

VA

CE

IM

EQ

CT

SE

ME

NNF
DNNF
d-NNF
d-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF





























































PI
IP
MODS
s-NNF
f-NNF
sd-DNNF
































































Table 24:

254

































fiA Knowledge Compilation Map

L

CO

VA

CE

IM

EQ

CT

SE

ME

NNF
DNNF
d-NNF
d-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP














































































MODS
s-NNF
f-NNF
sd-DNNF


















































































Table 25:
L

CO

VA

CE

IM

EQ

CT

SE

ME

NNF
DNNF
d-NNF





















































































































d-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP
MODS
s-NNF
f-NNF
sd-DNNF









































Table 26:
only)). Since PI satisfies CE, it also satisfies CO. Since it satisfies CD as well (cf. Proposition 5.1),
it also satisfies ME (Lemma A.3). Contrastingly, the models counting problem for monotone Krom
formulas (i.e. conjunctions of clauses containing at most two literals and only positive literals)
is #P-complete (Roth, 1996). Such formulas can easily be turned into prime implicates form in
polynomial time (Marquis, 2000), hence PI does not satisfy CT. Now, since the negation of a
formula  in prime implicates form is a formula in prime implicants form (cf. Lemma A.14), and
since the number of models of  over Vars() is 2|Vars()| minus the number of models of  over
Vars(), we necessarily have that IP does not satisfy CT. This also imply that IP satisfies VA,
leading to the table.
Table 25: In the proof of Proposition 3.1, we have shown that the prime implicants of  can
be computed in polytime from a MODS representation of . As an immediate consequence, since IP
satisfies IM, EQ and SE, we obtain that MODS satisfies IM, EQ and SE, leading to the table.
Table 26: Since d-DNNF satisfies CT (Darwiche, 2001b), it also satisfies VA. Since it satisfies
CD (Proposition 5.1), it also satisfies IM as well (Lemma A.7). Since sd-DNNF  d-DNNF, these
results follow for sd-DNNF. Hence, we obtain the table.
Table 27: It is known that determining whether the conjunction of two FBDD formulas 1 and 2
is consistent is NP-complete (Gergov & Meinel, 1994b) Moreover, FBDD satisfies C. Since 1  2
is inconsistent iff 1 |= 2 , we can reduce the consistency test into an entailment test. Hence, FBDD
does not satisfy SE. Since FBDD  d-DNNF, d-DNNF does not satisfy SE either. Finally, since every
d-DNNF can be translated into an equivalent sd-DNNF sentence in polytime (Lemma A.1), sd-DNNF
does not satisfy SE either. This leads to the final table above. 2

255

fiDarwiche & Marquis

L

CO

VA

CE

IM

EQ

CT

SE

ME

NNF
DNNF
d-NNF












































d-DNNF
BDD
FBDD
OBDD
OBDD<
DNF
CNF
PI
IP
MODS
s-NNF
f-NNF
sd-DNNF



















































































































Table 27:
Proof of Proposition 5.1
The proof of this proposition is broken down into eight steps. Each step corresponds to one of the
transformations, where we prove all results pertaining to that transformation.
 CD. To show that a language L satisfies CD, we want to show that for any sentence   L
and any consistent term , we can construct in polytime a sentence which belongs to L and is
equivalent to  | .
 NNF, f-NNF, CNF and DNF. The property is trivially satisfied by these languages: If 
belongs to any of these languages, then replacing the literals of  by a Boolean constant
in  results a sentence in the same language. In the case of DNF (resp. CNF), some
inconsistent terms (valid clauses) may result through conditioning, but these can be
removed easily in polynomial time.
 DNNF. It is sufficient to prove that conditioning preserves decomposability. For every
propositional sentences ,  and every consistent term , if  and  do not share variables,
then | and | do not share variables either since Vars(|)  Vars() and Vars(|) 
Vars().
 d-NNF and d-DNNF. Since NNF and DNNF satisfy CD, it is sufficient to prove that conditioning preserves determinism, i.e. for every propositional formulas ,  and every consistent
term , if    |= false, then (|)  (|) |= false. If    |= false, then for every term
, we have (  )   |= false. Since (  )    ((  )|)  , this implies that
((  )|)   |= false. Since  is consistent and share no variable with (  )|, it must
be the case that (  )| is inconsistent. This is equivalent to state that (|)  (|) |=
false.
 s-NNF and sd-DNNF. Since NNF satisfies CD, and since conditioning preserves decomposability and determinism, all we have to show is that conditioning also preserves smoothness. This follows immediately since for two propositional sentences ,  and a consistent
term , we have Vars() = Vars() only if Vars( | ) = Vars( | ).
 BDD, FBDD, OBDD and OBDD< . It is wellknown that BDD satisfies CDthe conditioning
operation on binary decision diagrams is known as the restrict operation (Bryant, 1986).
To condition a sentence  in BDD on a consistent term , we replace every node labeled
by a variable in  by one of its two children, according to the sign of the variable in .
The resulting sentence is also a BDD and is equivalent to  | . The same applies to FBDD,
OBDD and OBDD< .
 PI. The prime implicates of    can be computed in polytime when  is in prime
implicates form and  is a term (see Proposition 36 in (Marquis, 2000)). Moreover, since
256

fiA Knowledge Compilation Map

PI satisfies FO (see below), the prime implicates of Vars().(  ) can be computed in
polytime. But these are exactly the prime implicates of  |  according to Lemma A.12.
Wn
 IP.
WnLet  = i=1 i be a formula in prime implicants form. It is clear that the formula
( i=1 i ) |  is a DNF formula equivalent to  | . Now, our claim is
Wnthat the formula 
obtained by keeping only the logically weakest terms i |  among ( i=1 i ) |  is a prime
implicants formula equivalent to  | . Removing such terms clearly is truth-preserving.
Since generating  requires only O(n2 ) entailment tests among terms, and since such
tests can be easily achieved in polynomial time, we obtain that IP satisfies CD. Now,
how to prove that  is in prime implicants form? Since any pair of different terms of
 cannot be compared w.r.t. logical entailment, the correctness of Quines consensus
algorithm for generating prime implicants shows that it is sufficient to prove that every
consensus among two terms of  is inconsistent or entails another term of . Lets
recall that consensus is to DNF formulas what resolution is to CNF formulas. Since 
is in prime implicants form, every consensus among two terms of  is inconsistent or
entails another term of . What happens to the terms (here, the prime implicants) of 
when conditioned by ? All those containing the negation of a literal of  are removed
and the remaining ones are shortened by removing from them every literal of . Hence,
for every pair of terms 1 , 2 of , if there is no consensus between 1 and 2 , then
there is no consensus between 1 | and 2 |: conditioning cannot create new consensus.
Now, it remains to prove that no unproductive consensus between terms of  can be
rendered productive through conditioning. Formally, let 1 = 10  l and 2 = 20  l be
two prime implicates of  s.t. l (resp. l) does not appear in 10 (resp. 20 ). There is a
consensus 10 20 between 1 and 2 . Let us assume that both 1 and 2 have survived the
conditioning: this means that both 1 | and 2 | are consistent. Especially, l belongs to
1 | and l belongs to 2 |. Accordingly, there is a consensus between 1 | and 2 |. By
construction, this consensus is equivalent to (10 |)(20 |), hence equivalent to (10 20 )|.
Now, if 10  20 is inconsistent, then (10  20 )| is inconsistent as well and we are done.
Otherwise, let us assume that there exists a prime implicant 3 of  s.t. 10  20 |= 3
holds. Necessarily, 3 is preserved by the conditioning of  by . Otherwise, 3 would
contain the negation of a literal of , but since every literal of 3 is a literal of 1 or a
literal of 2 , 2 and 3 would not have both survived the conditioning. Since 10  20 |= 3
holds, we necessarily have (10  20 )| |= 3 |. This completes the proof.
 MODS. Direct consequence of Lemma A.12 and the fact that MODS satisfies BC and FO
(see below).
 FO.
 DNNF and DNF. It is known that DNNF satisfies FO (Darwiche, 2001a). It is also known
that DNF satisfies FO (Lang et al., 2000).
 NNF, s-NNF, f-NNF, d-NNF, BDD and CNF. Let  be a sentence in CNF. We now show that
if any of the previous languages satisfies FO, then we can test the consistency of  in
polytime. Since CNF does not satisfy CO (see Proposition 4.1), it then follows that none
of the previous languages satisfy FO unless P = NP. First, we note that  must also
belong to NNF and f-NNF. Moreover,  can be turned into a sentence in BDD in polytime
(Lemma A.8) or a sentence in s-NNF in polytime (see the proof of Lemma A.1). We
also have that  can be turned into a sentence in d-NNF in polytime since BDD  d-NNF.
Suppose now that one of the previous languages, call it L, satisfy FO. We can test the
consistency of  in polytime as follows:
 Convert  into a sentence  in L in polytime (as shown above).
 Compute Vars()., which can be done in polytime by assumption.

257

fiDarwiche & Marquis

 Test the validity of Vars()., which can be done in polytime since the sentence
contains no variablesall we have to do is check whether the sentence evaluates to
true.
Finally, note that the definition of forgetting implies that a sentence  is consistent iff
Vars(). is valid, which completes the proof.
 d-DNNF and sd-DNNF. Follows immediately since none of these languages satisfies SFO
unless P = NP (see below).
 IP. Follows immediately since IP does not satisfy SFO.
 FBDD, OBDD and OBDD< . We will show that if FBDD (resp. OBDD, OBDD< ) satisfies FO,
then for every sentence  in DNF, there must exist an equivalent sentence  in FBDD (resp.
OBDD, OBDD< ), which size is polynomial in the size of . This contradicts the fact that
FBDD (resp. OBDD, OBDD< ) 6 DNFsee Table 3.
Given a DNF  consisting of terms 1 , ..., n , we can convert each of these terms into equivalent FBDD (resp. OBDD, OBDD< ) sentences 1 , . . . , n in polytime. Let {v1 , . . . , vn1 }
be a set of variables that do not belong to P S. Construct a new set of variables
P S 0 = P S  {v1 , . . . , vn1 }. In case of OBDD and OBDD< , we also assume that these
new variables are earlier than variables P S in the ordering. Consider now the sentence
 = {v1 , . . . , vn1 }.1 , with respect to variables P S 0 , where i is inductively defined
by:
 i = i , for i = n, and
 i = (i  vi )  (i+1  vi ), for i = 1, . . . , n  1.
Clearly enough, an FBDD (resp. OBDD, OBDD< ) sentence equivalentWto 1 can W
be computed
n
n
in time polynomial in the input size. Moreover, we have   i=1 i  i=1 i  .
Hence, if FBDD (resp. OBDD, OBDD< ) satisfies FO, then we can convert the DNF sentence 
into an equivalent FBDD (resp. OBDD, OBDD< ) which size is polynomial in the size of the
given DNF. This is impossible in general.
 PI. It is known that the prime implicates of X. are exactly the prime implicates of 
that do not contain any variable from X (see Proposition 55 in (Marquis, 2000)). Hence,
such prime implicates can be computed in time polynomial in the input size when  is
in prime implicates form.
 MODS. Given a MODS formula  and a subset X of P S, the formula obtained by removing
every leaf node (and the corresponding incoming edges) of  labeled by a literal x or x
s.t. x  X is a MODS representation of X.this is an easy consequence of Propositions
18 and 20 from (Lang et al., 2000). See also the polytime operation of forgetting on DNNF,
as defined in (Darwiche, 2001a), which applies to MODS, since
MODS  DNNF, and which can be easily modified so it guarantees that the output is in
MODS when the input is also in MODS.
 SFO.
 DNNF, DNF, PI and MODS. Immediate from the fact that each of these languages satisfies
FO (see above).
 NNF, d-NNF, s-NNF, f-NNF, BDD, OBDD< and CNF. Direct from the fact that x.  (|x) 
(|x) holds and the fact that any of these fragments satisfies CD and BC.
 OBDD. Direct from the fact that only one OBDD sentence is considered in the transformation
and OBDD< satisfies SFO.
 d-DNNF, sd-DNNF and FBDD. Let 1 and 2 be two FBDD formulas. Let x be a variable
not included in Vars(1 )  Vars(2 ). The formula  = (x  1 )  (x  2 ) is a FBDD
258

fiA Knowledge Compilation Map

formula since decomposability and decision are preserved by this construction. Since x.
is equivalent to 1  2 , if FBDD would satisfy SFO, it would satisfy BC as well, but
this is not the case unless P = NP (see below). The same conclusion can be drawn for
d-DNNF. Hence, FBDD and d-DNNF do not satisfy SFO unless P = NP. Since every d-DNNF
formula can be turned in polynomial time into an equivalent sd-DNNF formula, we obtain
that sd-DNNF does not satisfy SFO unless P = NP.
 IP. Let us show that the number of prime implicants of x. can be exponentially greater
than the number of prime implicants of . Let 0 be the following DNF formula:

0 = 

k _
m
_


(pi  qi,j ) 

i=1 j=1

k
^

pi .

i=1

0 has (m + 1)k + mk primes implicants (Chandra & Markowsky, 1978). Now, let  be
the formula:

=

k _
m
_


(x  pi  qi,j )  (x 

i=1 j=1

k
^

pi ).

i=1

Since 0 can be obtained from  by removing in every term of  every occurrence of x
and x, 0 is equivalent to {x}. (see (Lang et al., 2000)). Now,  has only mk + 1
prime implicants; indeed, every term of it is a prime implicant, and the converse holds
since every term is maximal w.r.t. logical entailment and every consensus of two terms
is inconsistent. This completes the proof.
 C.
 NNF, s-NNF, d-NNF, CNF. The property is trivially satisfied by these languages since determinism and smoothness are only concerned with or-nodes. Hence, if 1 , . . . , n belong to
one of these languages, so is 1  . . .  n .
 BDD. It is wellknown that the conjunction of two BDDs  and  can be easily computed
by connecting the 1-sink of  to the root of  (see proof of Lemma A.8). The size of the
resulting BDD is just the sum of the sizes of the respective BDDs of  and . Accordingly,
we can repeat this operation n times in time polynomial in the input size.
 f-NNF. Direct from the fact that f-NNF does not satisfy BC.
 FBDD, OBDD, OBDD< , DNF, PI and IP. It is straightforward to convert a clause into an
equivalent formula in any of these languages in polynomial time. In the proof of Proposition 3.1, we show specific CNF formulas which cannot be turned into an equivalent FBDD
(resp. OBDD, OBDD< , DNF, PI and IP) formulas in polynomial space (see Tables 9 and 10).
Hence, such conversion cannot be accomplished in polynomial time either. This implies
that none of FBDD, OBDD, OBDD< , DNF, PI and IP satisfies C.
 DNNF, d-DNNF and sd-DNNF. Direct from the fact that none of these languages satisfy
BC unless
P = NP.
Vn
 MODS. Let  = i=1 i , where i = (xi,1  xi,2 ), i  1..n. Each i has 3 models over
Vars(i ). Since  has 3n models, it does not have a MODS representation of size polynomial
in the input size.
 BC.

259

fiDarwiche & Marquis

 NNF, s-NNF, d-NNF, BDD and CNF. Immediate since each of these languages satisfy C (see
above).
 DNNF, d-DNNF, sd-DNNF, FBDD and OBDD. Checking whether the conjunction of two OBDD<
formulas 1 and 2 (w.r.t. two different variable orderings <) is consistent is NP-complete
(see Lemma 8.14 in (Meinel & Theobald, 1998)). Since OBDD satisfies CO, it cannot satisfy
BC unless P = NP. Since OBDD  FBDD  d-DNNF  DNNF, and d-DNNF and DNNF satisfy
CO, none of them can satisfy BC unless P = NP. Finally, since every d-DNNF formula
can be turned in polynomial time into an equivalent smoothed d-DNNF formula and since
sd-DNNF satisfies CO, it cannot be the case that sd-DNNF satisfy BC unless P = NP.
 OBDD< . Well-known fact (Bryant, 1986).
Vn1
Wn1
 f-NNF. Let 1 = i=0 (x2i  x2i+1 ) be a CNF formula and 2 = i=0 (x02i  x02i+1 ) a DNF
formula. 1 has 2n essential prime implicants and n essential prime implicates (see the
proof of Proposition 3.1, Table 9). By duality, 2 has n essential prime implicants and 2n
essential prime implicates. Now, 1 and 2 are two f-NNF formulas. By Lemma A.13, we
know that every f-NNF formula  can be turned in polynomial time into a CNF formula or
a DNF formula. If f-NNF would satisfy BC, then a f-NNF formula  s.t.   1 2 could
be computed in time polynomial in the input size. Hence, either a CNF formula equivalent
to 1  2 or a DNF formula equivalent to 1  2 could be computed in polytime. But
this is impossible since 1  2 has n + 2n essential prime implicates and n  2n essential
prime implicants. Hence every CNF (resp. DNF ) formula equivalent to 1  2 has a size
exponential in |1 | + |2 |.
Note that in the case where the two f-NNF formulas 1 and 2 into consideration can
be turned in polynomial time into either two CNF formulas or two DNF formulas, then a
f-NNF formula equivalent to 1  2 can be computed in time polynomial in the input
size (this is obvious when two CNF formulas are considered and the next item of the proof
shows how this can be achieved when two DNF formulas are considered).
 DNF and MODS. If 1 and 2 are sentences in one of these languages L, then we can construct
a sentence in L which is equivalent to 1  2 by simply taking all the conjunctions of one
term from 1 and one term from 2 , while removing redundant literals in the resulting
terms and removing any inconsistent terms in the result. The disjunction of all the
resulting terms is a sentence from L equivalent to 1  2 and it has been computed in
polynomial time.
Vk Vm
Wk
 PI. Let 1 = i=1 pi and 2 = i=1 j=1 (pi qi,j ). Sentence 1 has one prime implicate
and 2 has m  k prime implicates. But 1  2 has (m + 1)k + m  k prime implicates
(Chandra & Markowsky, 1978).
 IP. Let IP () be the set of prime implicants for . We have IP (1  2 ) = max({1 
2 | 1  IP (1 ), 2  IP (2 )}, |=) (up to logical equivalence). See e.g., (dual of) Proposition 40 in (Marquis, 2000).

260

fiA Knowledge Compilation Map

 C.
 NNF, s-NNF, DNNF and DNF. The property is trivially satisfied by these languages since
decomposability is only concerned with and-nodes, and since every NNF formula can be
turned in polynomial time into an equivalent smoothed NNF formula.
 d-NNF and BDD. Direct consequence from the fact that d-NNF and BDD satisfies both C
and C. Especially, it is well-known that the disjunction of two BDDs  and  can
be easily computed by connecting the 0-sink of  to the root of  (see the proof of
Lemma A.8). The size of the resulting BDD is just the sum of the sizes of the respective
BDDs of  and . Accordingly, we can repeat this operation n times in time polynomial
in the input size.
 f-NNF. Since f-NNF does not satisfy C but satisfies C, it cannot satisfy C (due to
De Morgans laws).
 FBDD, OBDD, OBDD< , CNF, PI, IP and MODS. It is straightforward to convert any term into
an equivalent formula from any of the previous languages in polynomial time. In the
proof of Proposition 3.1, we show specific DNF formulas which cannot be turned into
equivalent FBDD (resp. OBDD, OBDD< , CNF , PI, IP and MODS) formulas in polynomial space
(see Tables 9, 10 and 15). Hence, the conversion cannot be accomplished in polynomial
time either. This implies that none of FBDD, OBDD, OBDD< , CNF, PI, IP and MODS satisfies
C.
 d-DNNF and sd-DNNF. Immediate form the fact that none of these classes satisfies BC
unless P = NP (see below).
 BC.
 NNF, d-NNF, DNNF, s-NNF, BDD and DNF. Immediate since each of these languages satisfies
C.
 OBDD< . Well-known fact (Bryant, 1986).
 OBDD, FBDD, d-DNNF and sd-DNNF. Checking whether the conjunction of two OBDD< formulas 1 and 2 (w.r.t. two different variable orderings <) is consistent is NP-complete (see
Lemma 8.14 in (Meinel & Theobald, 1998)). Now, 1  2 is inconsistent iff 1  2
is valid. Since OBDD satisfies C, an OBDD formula equivalent to 1 (resp. 2 ) can
be computed in time polynomial in |1 | (resp. |2 |). Since OBDD  FBDD  d-DNNF, the
resulting formulas are also FBDD and d-DNNF formulas. If OBDD (resp. FBDD, d-DNNF)
would satisfy BC, then an OBDD (resp. FBDD, d-DNNF) formula equivalent to 1  2
could be computed in time polynomial in |1 | + |2 |. But since d-DNNF satisfies VA,
this is impossible unless P = NP. Finally, since every d-DNNF formula can be turned in
polynomial time into an equivalent sd-DNNF formula, sd-DNNF cannot satisfy BC unless
P = NP.
 f-NNF. Since f-NNF does not satisfy BC but satisfies C, it cannot satisfy BC (due
to De Morgans laws).
 CNF. If 1 and 2 are two CNF sentences, then we can construct a CNF sentence which
is equivalent to 1  2 by simply taking all the disjunctions of one clause from 1 and
one clause from 2 , while removing redundant literals inside the resulting clauses and
removing any valid clause in the result. The conjunction of all the resulting clauses is a
CNF sentence equivalent to 1  2 , and it has been computed in polynomial time.
 PI. Let PI () be the set of prime implicates for sentence . We have PI (1  2 ) =
min({1  2 | 1  PI (1 ), 2  PI (2 )}, |=). See Proposition 40 in (Marquis, 2000).

261

fiDarwiche & Marquis

Vk
Wk Wm
 IP. Let 1 = i=1 pi and 2 = i=1 j=1 (pi qi,j ). Sentence 1 has one prime implicant
and 2 has m  k prime implicants. But 1  2 has (m + 1)k + m  k prime implicants
(Chandra & Markowsky, 1978).
Vn
 MODS. Let 1 = i=1 xi and 2 = y. Sentence 1 has 1 model over Vars(1 ) and 2 has
1 model over Vars(2 ). But 1  2 has 2n + 1 models over Vars(1 )  Vars(2 ).
 C.
 NNF, s-NNF, f-NNF, BDD, FBDD, OBDD and OBDD< . The property is obviously satisfied by
NNF. s-NNF also satisfies C since every NNF formula can be turned in polynomial time
into an equivalent s-NNF formula. f-NNF satisfies C since applying De Morgans laws
on a f-NNF formula results in a f-NNF formula. Finally, for all the forms of BDDs, it is
sufficient to switch the labels of the sinks to achieve negation (Bryant, 1986).
 CNF. Because the negation of a DNF formula is a CNF formula that can be computed in
polynomial time, if CNF would satisfy C, then it would be possible to turn any DNF
formula into an equivalent CNF formula in polynomial time (by involution of negation).
But we know that it is not possible in polynomial space since CNF 6 DNF(see the proof of
Proposition 3.1). Hence, CNF does not satisfy C.
 DNF. Dual of the proof just above (just replace CNF by DNF and vice-versa).
Vn1
 PI. The formula n = i=0 (x2i  x2i+1 ) is in prime implicates form (see the proof of
Proposition 3.1, Table 9). This formula has exponentially many prime implicants, that
are just the negations of the prime implicates of n . Since n has exponentially many
prime implicates, it cannot be the case that PI satisfies C.
 IP. We just have to take the dual of the above proof (prime implicates case). The formula
Wn1
n = i=0 (x2i x2i+1 ) is in prime implicants form. This formula has exponentially many
prime implicates, that are just the negations of the prime implicants of n . Since n
has exponentially many prime implicants, it cannot be the case that IP satisfies C.
 DNNF. The negation of any CNF formula can be computed in polynomial time as a DNF
formula, hence as a DNNF formula. If DNNF would satisfy C, then it would be possible
to turn a CNF formula into an equivalent DNNF one (by involution of negation). Because
DNNF satisfies CO, we would have P = NP.
 d-NNF. Following is a procedure for negating a d-NNF sentence :9
 Traverse nodes in the DAG of , visiting the children of a node before you visit the
node itself. When visiting a node, construct its negation as follows:
 true is the negation of false.
 false is the negation of true.
 (N10 , . . . , Nk0 ) is the negation of (N1 , . . . , Nk ). Here, Ni0 is the node representing
the negation of Ni .
 ((N10 , M1 ), . . . , (Nk0 , Mk )) is the negation of (N1 , . . . , Nk ). Here, Ni0 is the
node representing the negation of Ni , and Mi is a node representing the conjunction N1  . . .  Ni1 .
 Return the negation of the root of d-NNF .
We can implement the above four steps so that we when we visit a node with k children,
we only construct O(k) nodes and O(k) edges.10 Hence, the procedure complexity is
9. Mark Hopkins pointed us to this procedure.
10. We assume that any or-node (resp. and-node) with less than two children is removed and replaced by its unique
child or by f alse (resp. true) if it has no children. This simplification process is equivalence-preserving and it
can be achieved in time linear in the size of the input DAG.

262

fiA Knowledge Compilation Map

linear in the size of the original d-NNF. It is easy to check that the result is equivalent to
the negation of the given d-NNF sentence and is also in d-NNF.
 sd-DNNF and d-DNNF. Unknown.
Vn
Sn
 MODS.  = Si=1 xi has only one model over i=1 {xi } but its negation  has 2n  1
n
models over i=1 {xi }. Hence MODS cannot satisfy C. 2

References
A. Cimmati, E. Giunchiglia, F. G., & Traverso, P. (1997). Planning via model checking: a decision
procedure for AR. In Proceedings of the 4th European Conference on Planning (ECP97), pp.
130142.
Blum, M., Chandra, A. K., & Wegman, M. N. (1980). Equivalence of free Boolean graphs can be
decided probabilistically in polynomial time. Information Processing Letters, 10 (2), 8082.
Boole, G. (1854). An investigation of the laws of thought. Walton and Maberley, London.
Boufkhad, Y., Gregoire, E., Marquis, P., Mazure, B., & Sas, L. (1997). Tractable cover compilations.
In Proc. of the 15th International Joint Conference on Artificial Intelligence (IJCAI97), pp.
122127, Nagoya.
Bryant, R. E. (1986). Graph-based algorithms for Boolean function manipulation. IEEE Transactions on Computers, C-35, 677691.
Bryant, R. E. (1992). Symbolic Boolean manipulation with ordered binary decision diagrams. ACM
Computing Surveys, 24 (3), 293318.
Cadoli, M., & Donini, F. (1997). A survey on knowledge compilation. AI Communications, 10,
137150. (printed in 1998).
Cadoli, M., Donini, F., Liberatore, P., & Schaerf, M. (1996). Comparing space efficiency of propositional knowledge representation formalisms. In Proc. of the 5rd International Conference on
Knowledge Representation and Reasoning (KR96), pp. 364373.
Chandra, A., & Markowsky, G. (1978). On the number of prime implicants. Discrete Mathematics,
24, 711.
Darwiche, A. (1999). Compiling knowledge into decomposable negation normal form. In Proceedings
of International Joint Conference on Artificial Intelligence (IJCAI99), pp. 284289. Morgan
Kaufmann, California.
Darwiche, A. (2001a). Decomposable negation normal form. Journal of the ACM, 48 (4), 608647.
Darwiche, A. (2001b). On the tractability of counting theory models and its application to belief
revision and truth maintenance. Journal of Applied Non-Classical Logics, 11 (1-2), 1134.
Darwiche, A., & Huang, J. (2002). Testing equivalence probabilistically. Tech. rep. D123, Computer
Science Department, UCLA, Los Angeles, Ca 90095.
de Kleer, J. (1992). An improved incremental algorithm for generating prime implicates. In Proc.
of the 10th National Conference on Artificial Intelligence (AAAI92), pp. 780785, San Jose,
California.
Dechter, R., & Rish, I. (1994). Directional resolution: the Davis-Putnam procedure, revisited. In
Proceedings of the Fourth International Conference on Principles of Knowledge Representation
and Reasoning (KR94), pp. 134145, Bonn.
del Val, A. (1994). Tractable databases: How to make propositional unit resolution complete through
compilation. In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning (KR94), pp. 551561. Morgan Kaufmann Publishers, Inc., San
Mateo, California.
263

fiDarwiche & Marquis

Gergov, J., & Meinel, C. (1994a). Efficient analysis and manipulation of obdds can be extended to
fbdds. IEEE Transactions on Computers, 43 (10), 11971209.
Gergov, J., & Meinel, C. (1994b). On the complexity of analysis and manipulation of Boolean
functions in terms of decision diagrams. Information Processing Letters, 50, 317322.
Gogic, G., Kautz, H., Papadimitriou, C., & Selman, B. (1995). The comparative linguistics of
knowledge representation. In Proc. of the 14th International Joint Conference on Artificial
Intelligence (IJCAI95), pp. 862869, Montreal.
Herzig, A., & Rifi, O. (1999). Propositional belief base update and minimal change. Artificial
Intelligence, 115 (1), 107138.
Karp, R., & Lipton, R. (1980). Some connections between non-uniform and uniform complexity
classes. In Proc. of the 12th ACM Symposium on Theory of Computing (STOC80), pp. 302
309.
Khardon, R., & Roth, D. (1997). Learning to reason. Journal of the ACM, 44 (5), 697725.
Lang, J., Liberatore, P., & Marquis, P. (2000). Propositional independencePart I: formulavariable
independence and forgetting. Submitted.
Madre, J. C., & Coudert, O. (1992). A new method to compute prime and essential prime implicants
of boolean functions. In Advanced research in VLSI and parallel systems, Proceedings of the
Brown/MIT conference, pp. 113128.
Marquis, P. (2000). Consequence finding algorithms, Vol. 5 of Handbook of Defeasible Reasoning
and Uncertainty Management Systems: Algorithms for Uncertain and Defeasible Reasoning.
Kluwer Academic Publishers.
Marquis, P. (1995). Knowledge compilation using theory prime implicates. In Proc. International
Joint Conference on Artificial Intelligence (IJCAI95), pp. 837843. Morgan Kaufmann Publishers, Inc., San Mateo, California.
Meinel, C., & Theobald, T. (1998). Algorithms and Data Structures in VLSI Design: OBDD Foundations and Applications. Springer.
Papadimitriou, C. (1994). Computational complexity. AddisonWesley.
Quine, W. (1955). A way to simplify truth functions. American Mathematical Monthly, 52, 627631.
Quine, W. (1959). On cores and prime implicants of truth functions. American Mathematical
Monthly, 66, 755760.
Reiter, R., & de Kleer, J. (1987). Foundations of assumption-based truth maintenance systems:
Preliminary report. In Proceedings of the Fifth National Conference on Artificial Intelligence
(AAAI), pp. 183188.
Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence, 82 (1-2), 273302.
Schrag, R. (1996). Compilation for critically constrained knowledge bases. In Proc. of the 13th
National Conference on Artificial Intelligence (AAAI96), pp. 510515, Portland, Oregan.
Selman, B., & Kautz, H. (1996). Knowledge compilation and theory approximation. Journal of the
Association for Computing Machinery, 43, 193224.
Simon, L., & del Val, A. (2001). Efficient consequence finding. In Proc. of the 17th International
Joint Conference on Artificial Intelligence (IJCAI01), pp. 359365, Seattle (WA).
Wegener, I. (1987). The complexity of boolean functions. Wiley-Teubner, Stuttgart.

264

fiJournal of Artificial Intelligence Research 17 (2002) 379-449

Submitted 4/02; published 12/02

Specific-to-General Learning for Temporal Events
with Application to Learning Event Definitions from Video
Alan Fern
Robert Givan
Jeffrey Mark Siskind

AFERN @ PURDUE . EDU
GIVAN @ PURDUE . EDU
QOBI @ PURDUE . EDU

School of Electrical and Computer Engineering
Purdue University, West Lafayette, IN 47907 USA

Abstract
We develop, analyze, and evaluate a novel, supervised, specific-to-general learner for a simple temporal logic and use the resulting algorithm to learn visual event definitions from video
sequences. First, we introduce a simple, propositional, temporal, event-description language called
AMA that is sufficiently expressive to represent many events yet sufficiently restrictive to support
learning. We then give algorithms, along with lower and upper complexity bounds, for the subsumption and generalization problems for AMA formulas. We present a positive-examplesonly
specific-to-general learning method based on these algorithms. We also present a polynomialtimecomputable syntactic subsumption test that implies semantic subsumption without being
equivalent to it. A generalization algorithm based on syntactic subsumption can be used in place of
semantic generalization to improve the asymptotic complexity of the resulting learning algorithm.
Finally, we apply this algorithm to the task of learning relational event definitions from video and
show that it yields definitions that are competitive with hand-coded ones.

1. Introduction
Humans conceptualize the world in terms of objects and events. This is reflected in the fact that
we talk about the world using nouns and verbs. We perceive events taking place between objects,
we interact with the world by performing events on objects, and we reason about the effects that
actual and hypothetical events performed by us and others have on objects. We also learn new
object and event types from novel experience. In this paper, we present and evaluate novel implemented techniques that allow a computer to learn new event types from examples. We show results
from an application of these techniques to learning new event types from automatically constructed
relational, force-dynamic descriptions of video sequences.
We wish the acquired knowledge of event types to support multiple modalities. Humans can
observe someone faxing a letter for the first time and quickly be able to recognize future occurrences
of faxing, perform faxing, and reason about faxing. It thus appears likely that humans use and
learn event representations that are sufficiently general to support fast and efficient use in multiple
modalities. A long-term goal of our research is to allow similar cross-modal learning and use of
event representations. We intend the same learned representations to be used for vision (as described
in this paper), planning (something that we are beginning to investigate), and robotics (something
left to the future).
A crucial requirement for event representations is that they capture the invariants of an event
type. Humans classify both picking up a cup off a table and picking up a dumbbell off the floor
as picking up. This suggests that human event representations are relational. We have an abstract

c 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiF ERN , G IVAN , & S ISKIND

relational notion of picking up that is parameterized by the participant objects rather than distinct
propositional notions instantiated for specific objects. Humans also classify an event as picking
up no matter whether the hand is moving slowly or quickly, horizontally or vertically, leftward or
rightward, or along a straight path or circuitous one. It appears that it is not the characteristics of
participant-object motion that distinguish picking up from other event types. Rather, it is the fact
that the object being picked up changes from being supported by resting on its initial location to
being supported by being grasped by the agent. This suggests that the primitive relations used to
build event representations are force dynamic (Talmy, 1988).
Another desirable property of event representations is that they be perspicuous. Humans can
introspect and describe the defining characteristics of event types. Such introspection is what allows us to create dictionaries. To support such introspection, we prefer a representation language
that allows such characteristics to be explicitly manifest in event definitions and not emergent consequences of distributed parameters as in neural networks or hidden Markov models.
We develop a supervised learner for an event representation possessing these desired characteristics as follows. First, we present a simple, propositional, temporal logic called AMA that is a
sublanguage of a variety of familiar temporal languages (e.g. linear temporal logic, or LTL Bacchus & Kabanza, 2000, event logic Siskind, 2001). This logic is expressive enough to describe a
variety of interesting temporal events, but restrictive enough to support an effective learner, as we
demonstrate below. We proceed to develop a specific-to-general learner for the AMA logic by giving algorithms and complexity bounds for the subsumption and generalization problems involving
AMA formulas. While we show that semantic subsumption is intractable, we provide a weaker syntactic notion of subsumption that implies semantic subsumption but can be checked in polynomial
time. Our implemented learner is based upon this syntactic subsumption.
We next show means to adapt this (propositional) AMA learner to learn relational concepts.
We evaluate the resulting relational learner in a complete system for learning force-dynamic event
definitions from positive-only training examples given as real video sequences. This is not the first
system to perform visual-event recognition from video. We review prior work and compare it to
the current work later in the paper. In fact, two such prior systems have been built by one of the
authors. H OWARD (Siskind & Morris, 1996) learns to classify events from video using temporal,
relational representations. But these representations are not force dynamic. L EONARD (Siskind,
2001) classifies events from video using temporal, relational, force-dynamic representations but
does not learn these representations. It uses a library of hand-code representations. This work adds
a learning component to L EONARD , essentially duplicating the performance of the hand-coded
definitions automatically.
While we have demonstrated the utility of our learner in the visual-eventlearning domain, we
note that there are many domains where interesting concepts take the form of structured temporal sequences of events. In machine planning, macro-actions represent useful temporal patterns of
action. In computer security, typical application behavior, represented perhaps as temporal patterns of system calls, must be differentiated from compromised application behavior (and likewise
authorized-user behavior from intrusive behavior).
In what follows, Section 2 introduces our application domain of recognizing visual events and
provides an informal description of our system for learning event definitions from video. Section 3
introduces the AMA language, syntax and semantics, and several concepts needed in our analysis
of the language. Section 4 develops and analyzes algorithms for the subsumption and generalization
problems in the language, and introduces the more practical notion of syntactic subsumption. Sec380

fiL EARNING T EMPORAL E VENTS

tion 5 extends the basic propositional learner to handle relational data and negation, and to control
exponential run-time growth. Section 6 presents our results on visual-event learning. Sections 7
and 8 compare to related work and conclude.

2. System Overview
This section provides an overview of our system for learning to recognize visual events from video.
The aim is to provide an intuitive picture of our system before providing technical details. A formal
presentation of our event-description language, algorithms, and both theoretical and empirical results appears in Sections 36. We first introduce the application domain of visual-event recognition
and the L EONARD system, the event recognizer upon which our learner is built. Second, we describe
how our positive-only learner fits into the overall system. Third, we informally introduce the AMA
event-description language that is used by our learner. Finally, we give an informal presentation of
the learning algorithm.
2.1 Recognizing Visual Events
L EONARD (Siskind, 2001) is a system for recognizing visual events from video camera input
an example of a simple visual event is a hand picking up a block. This research was originally
motivated by the problem of adding a learning component to L EONARDallowing L EONARD to
learn to recognize an event by viewing example events of the same type. Below, we give a high-level
description of the L EONARD system.
L EONARD is a three-stage pipeline depicted in Figure 1. The raw input consists of a video-frame
image sequence depicting events. First, a segmentation-and-tracking component transforms this
input into a polygon movie: a sequence of frames, each frame being a set of convex polygons placed
around the tracked objects in the video. Figure 2a shows a partial video sequence of a pick up event
that is overlaid with the corresponding polygon movie. Next, a model-reconstruction component
transforms the polygon movie into a force-dynamic model. This model describes the changing
support, contact, and attachment relations between the tracked objects over time. Constructing
this model is a somewhat involved process as described in Siskind (2000). Figure 2b shows a
visual depiction of the force-dynamic model corresponding to the pick up event. Finally, an eventrecognition component armed with a library of event definitions determines which events occurred
in the model and, accordingly, in the video. Figure 2c shows the text output and input of the
event-recognizer for the pick up event. The first line corresponds to the output which indicates
the interval(s) during which a pick up occurred. The remaining lines are the text encoding of the
event-recognizer input (model-reconstruction output), indicating the time intervals in which various
force-dynamic relations are true in the video.
The event-recognition component of L EONARD represents event types with event-logic formulas like the following simplified example, representing x picking up y off of z .

4

P ICK U P (x; y; z ) = (S UPPORTS (z; y ) ^ C ONTACTS (z; y )); (S UPPORTS (x; y ) ^ ATTACHED (x; y ))

This formula asserts that an event of x picking up y off of z is defined as a sequence of two states
where z supports y by way of contact in the first state and x supports y by way of attachment in
the second state. S UPPORTS , C ONTACTS , and ATTACHED are primitive force-dynamic relations.
This formula is a specific example of the more general class of AMA formulas that we use in our
learning.
381

fiF ERN , G IVAN , & S ISKIND

image
sequence

Segmentation
and Tracking

polygonscene
sequence

Model
Reconstruction

training
models
event
labels

model
sequence

Event
Learner

Event
Classification

event
labels

learned event
definitions

Figure 1: The upper boxes represent the three primary components of L EONARDs pipeline. The
lower box depicts the event-learning component described in this paper. The input to the
learning component consists of training models of target events (e.g., movies of pick up
events) along with event labels (e.g., P ICK U P (hand; red; green)) and the output is an
event definition (e.g., a temporal logic formula defining P ICK U P (x; y; z )).
2.2 Adding a Learning Component
Prior to the work reported in this paper, the definitions in L EONARD s event-recognition library
were hand coded. Here, we add a learning component to L EONARD so that it can learn to recognize
events. Figure 1 shows how the event learner fits into the overall system. The input to the event
learner consists of force-dynamic models from the model-reconstruction stage, along with event
labels, and its output consists of event definitions which are used by the event recognizer. We take
a supervised-learning approach where the force-dynamic model-reconstruction process is applied
to training videos of a target event type. The resulting force-dynamic models along with labels
indicating the target event type are then given to the learner which induces a candidate definition of
the event type.
For example, the input to our learner might consist of two models corresponding to two videos,
one of a hand picking up a red block off of a green block with label P ICK U P (hand; red; green) and
one of a hand picking up a green block off of a red block with label P ICK U P (hand; green; red)the
output would be a candidate definition of P ICK U P (x; y; z ) that is applicable to previously unseen
pick up events. Note that our learning component is positive-only in the sense that when learning
a target event type it uses only positive training examples (where the target event occurs) and does
not use negative examples (where the target event does not occur). The positive-only setting is of
interest as it appears that humans are able to learn many event definitions given primarily or only
positive examples. From a practical standpoint, a positive-only learner removes the often difficult
task of collecting negative examples that are representative of what is not the event to be learned
(e.g., what is a typical non-pickup event?).
The construction of our learner involves two primary design choices. First, we must choose an
event representation language to serve as the learners hypothesis space (i.e., the space of event definitions it may output). Second, we must design an algorithm for selecting a good event definition
from the hypothesis space given a set of training examples of an event type.
2.3 The AMA Hypothesis Space
The full event logic supported by L EONARD is quite expressive, allowing the specification of a
wide variety of temporal patterns (formulas). To help support successful learning, we use a more
382

fiL EARNING T EMPORAL E VENTS

(a)

Frame 0

Frame 1

Frame 2

Frame 13

Frame 14

Frame 20

Frame 0

Frame 1

Frame 2

Frame 13

Frame 14

Frame 20

(b)

(PICK-UP HAND RED GREEN)@{[[0,1],[14,22])}

(c)

(SUPPORTED? RED)@{[[0:22])}
(SUPPORTED? HAND)@{[[1:13]), [[24:26])}
(SUPPORTS? RED HAND)@{[[1:13]), [[24:26])}
(SUPPORTS? HAND RED)@{[[13:22])}
(SUPPORTS? GREEN RED)@{[[0:14])}
(SUPPORTS? GREEN HAND)@{[[1:13])}
(CONTACTS? RED GREEN)@{[[0:2]), [[6:14])}
(ATTACHED? RED HAND)@{[[1:26])}
(ATTACHED? RED GREEN)@{[[1:6])}

Figure 2: L EONARD recognizes a pick up event. (a) Frames from the raw video input with the automatically generated polygon movie overlaid. (b) The same frames with a visual depiction
of the automatically generated force-dynamic properties. (c) The text input/output of the
event classifier corresponding to the depicted movie. The top line is the output and the
remaining lines make up the input that encodes the changing force-dynamic properties.
GREEN represents the block on the table and RED represents the block being picked up.

383

fiF ERN , G IVAN , & S ISKIND

restrictive subset of event logic, called AMA, as our learners hypothesis space. This subset excludes
many practically useless formulas that may confuse the learner, while still retaining substantial
expressiveness, thus allowing us to represent and learn many useful event types. Our restriction to
AMA formulas is a form of syntactic learning bias.
The most basic AMA formulas are called states which express constant properties of time intervals of arbitrary duration. For example, S UPPORTS (z; y ) ^ C ONTACTS (z; y ) is a state which tells us
that z must support and be in contact with y . In general, a state can be the conjunction of any number
of primitive propositions (in this case force-dynamic relations). Using AMA we can also describe
sequences of states. For example, (S UPPORTS (z; y ) ^ C ONTACTS (z; y )) ; (S UPPORTS (x; y ) ^
ATTACHED (x; y )) is a sequence of two states, with the first state as given above and the second
state indicating that x must support and be attached to y . This formula is true whenever the first
state is true for some time interval, followed immediately by the second state being true for some
time interval meeting the first time interval. Such sequences are called MA timelines since they
are the Meets of Ands. In general, MA timelines can contain any number of states. Finally, we can
conjoin MA timelines to get AMA formulas (Ands of MAs). For example, the AMA formula

[(S UPPORTS (z; y) ^ C ONTACTS (z; y)) ; (S UPPORTS (x; y) ^ ATTACHED (x; y))] ^
[(S UPPORTS (u; v) ^ ATTACHED (u; v)) ; (S UPPORTS (w; v) ^ C ONTACTS (w; v))]
defines an event where two MA timelines must be true simultaneously over the same time interval.
Using AMA formulas we can represent events by listing various property sequences (MA timelines),
all of which must occur in parallel as an event unfolds. It is important to note, however, that the
transitions between states of different timelines in an AMA formula can occur in any relation to one
another. For example, in the above AMA formula, the transition between the two states of the first
timeline can occur before, after, or exactly at the transition between states of the second timeline.
An important assumption leveraged by our learner is that the primitive propositions used to construct states describe liquid properties (Shoham, 1987). For our purposes, we say that a property is
liquid if when it holds over a time-interval it holds over all of its subintervals. The force-dynamic
properties produced by L EONARD are liquide.g., if a hand S UPPORTS a block over an interval
then clearly the hand supports the block over all subintervals. Because primitive propositions are
liquid, properties described by states (conjunctions of primitives) are also liquid. However, properties described by MA and AMA formulas are not, in general, liquid.
2.4 Specific-to-General Learning from Positive Data
Recall that the examples that we wish to classify and learn from are force-dynamic models, which
can be thought of (and are derived from) movies depicting temporal events. Also recall that our
learner outputs definitions from the AMA hypothesis space. Given an AMA formula, we say that
it covers an example model if it is true in that model. For a particular target event type (such as
P ICK U P ), the ultimate goal is for the learner to output an AMA formula that covers an example
model if and only if the model depicts an instance of the target event type. To understand our
learner, it is useful to define a generality relationship between AMA formulas. We say that AMA
formula 	1 is more general (less specific) than AMA formula 	2 if and only if 	2 covers every
example that 	1 covers (and possibly more).1
1. In our formal analysis, we will use two different notions of generality (semantic and syntactic). In this section, we
ignore such distinctions. We note, however, that the algorithm we informally describe later in this section is based on
the syntactic notion of generality.

384

fiL EARNING T EMPORAL E VENTS

If the only learning goal is to find an AMA formula that is consistent with a set of positiveonly training data, then one result can be the trivial solution of returning the formula that covers
all examples. Rather than fix this problem by adding negative training examples (which will rule
out the trivial solution), we instead change the learning goal to be that of finding the least-general
formula that covers all of the positive examples.2 This learning approach has been pursued for a
variety of different languages within the machine-learning literature, including clausal first-order
logic (Plotkin, 1971), definite clauses (Muggleton & Feng, 1992), and description logic (Cohen &
Hirsh, 1994). It is important to choose an appropriate hypothesis space as a bias for this learning
approach or the hypothesis returned may simply be (or resemble) one of two extremes, either the
disjunction of the training examples or the universal hypothesis that covers all examples. In our
experiments, we have found that, with enough training data, the least-general AMA formula often
converges usefully.
We take a standard specific-to-general machine-learning approach to finding the least-general
AMA formula that covers a set of positive examples. The approach relies on the computation of two
functions: the least-general covering formula (LGCF) of an example model and the least-general
generalization (LGG) of a set of AMA formulas. The LGCF of an example model is the least general
AMA formula that covers the example. Intuitively, the LGCF is the AMA formula that captures the
most information about the model. The LGG of any set of AMA formulas is the least-general AMA
formula that is more general than each formula in the set. Intuitively, the LGG of a formula set is
the AMA formula that captures the largest amount of common information among the formulas.
Viewed differently, the LGG of a formula set covers all of the examples covered by those formulas,
but covers as few other examples as possible (while remaining in AMA).3
The resulting specific-to-general learning approach proceeds as follows. First, use the LGCF
function to transform each positive training model into an AMA formula. Second, return the LGG
of the resulting formulas. The result represents the least-general AMA formula that covers all of
the positive training examples. Thus, to specify our learner, all that remains is to provide algorithms for computing the LGCF and LGG for the AMA language. Below we informally describe
our algorithms for computing these functions, which are formally derived and analyzed in Sections 3.4 and 4.
2.5 Computing the AMA LGCF
To increase the readability of our presentation, in what follows, we dispense with presenting examples where the primitive properties are meaningfully named force-dynamic relations. Rather, our
examples will utilize abstract propositions such as a and b. In our current application, these propositions correspond exclusively to force-dynamic properties, but may not for other applications. We
now demonstrate how our system computes the LGCF of an example model.
Consider the following example model: fa@[1; 4]; b@[3; 6]; c@[6; 6]; d@[1; 3]; d@[5; 6]g . Here,
we take each number (1, . . . , 6) to represent a time interval of arbitrary (possibly varying with the
number) duration during which nothing changes, and then each fact p@[i; j ] indicates that proposition p is continuously true throughout the time intervals numbered i through j . This model can
be depicted graphically, as shown in Figure 3. The top four lines in the figure indicate the time
2. This avoids the need for negative examples and corresponds to finding the specific boundary of the version space
(Mitchell, 1982).
3. The existence and uniqueness of the LGCF and LGG defined here is a formal property of the hypothesis space and is
proven for AMA in Sections 3.4 and 4, respectively.

385

fiF ERN , G IVAN , & S ISKIND

1

2

a

3

4

a

a

b

b

5

6

b

b
c

d
a^d

d

d

d

; a^b^d ; a^b ; b^d ; b^c^d

Figure 3: LGCF Computation. The top four horizontal lines of the figure indicate the intervals over which the propositions a; b; c and d are true in the model given by
fa@[1; 4]; b@[3; 6]; c@[6; 6]; d@[1; 3]; d@[5; 6]g . The bottom line shows how the model
can be divided into intervals where no transitions occur. The LGCF is an MA timeline,
shown at the bottom of the figure, with a state for each of the no-transition intervals. Each
state simply contains the true propositions within the corresponding interval.
intervals over which each of the propositions a; b; c, and d are true in the model. The bottom line
in the figure shows how the model can be divided into five time intervals where no propositions
change truth value. This division is possible because of the assumption that our propositions are
liquid. This allows us, for example, to break up the time-interval where a is true into three consecutive subintervals where a is true. After dividing the model into intervals with no transitions, we
compute the LGCF by simply treating each of those intervals as a state of an MA timeline, where
the states contain only those propositions that are true during the corresponding time interval. The
resulting five-state MA timeline is shown at the bottom of the figure. We show later that this simple
computation returns the LGCF for any model. Thus, we see that the LGCF of a model is always an
MA timeline.
2.6 Computing the AMA LGG
We now describe our algorithm for computing the LGG of two AMA formulasthe LGG of m
formulas can be computed via a sequence of m 1 pairwise LGG applications, as discussed later.
Consider the two MA timelines: 1 = (a ^ b ^ c); (b ^ c ^ d); e and 2 = (a ^ b ^ e); a; (e ^ d).
It is useful to consider the various ways in which both timelines can be true simultaneously along
an arbitrary time interval. To do this, we look at the various ways in which the two timelines
can be aligned along a time interval. Figure 4a shows one of the many possible alignments of
these timelines. We call such alignments interdigitationsin general, there are exponentially many
interdigitations, each one ordering the state transitions differently. Note that an interdigitation is
allowed to constrain two transitions from different timelines to occur simultaneously (though this is
not depicted in the figure).4
4. Thus, an interdigitation provides an ordering relation on transitions that need not be anti-symmetric, but is reflexive,
transitive, and total.

386

fiL EARNING T EMPORAL E VENTS

(a)

a^b^e
(b)

a^b^c

b^c^d

a

e^d

e

a^b^c a^b^c a^b^c b^c^d
a^b^e
a^b ;

a

e^d

e^d

a

; true ;

d

e
e^d
;

e

Figure 4: Generalizing the MA timelines (a ^ b ^ c); (b ^ c ^ d); e and (a ^ b ^ e); a; (e ^ d). (a)
One of the exponentially many interdigitations of the two timelines. (b) Computing the
interdigitation generalization corresponding to the interdigitation from part (a). States are
formed by intersecting aligned states from the two timelines. The state true represents a
state with no propositions.

Given an interdigitation of two timelines, it is easy to construct a new MA timeline that must be
true whenever either of the timelines is true (i.e., to construct a generalization of the two timelines).
In Figure 4b, we give this construction for the interdigitation given in Figure 4a. The top two
horizontal lines in the figure correspond to the interdigitation, only here we have divided every state
on either timeline into two identical states, whenever a transition occurs during that state in the other
timeline. The resulting pair of timelines have only simultaneous transitions and can be viewed as
a sequence of state pairs, one from each timeline. The bottom horizontal line is then labeled by
an MA timeline with one state for each such state pair, with that state being the intersection of the
proposition sets in the state pair. Here, true represents the empty set of propositions, and is a state
that is true anywhere.
We call the resulting timeline an interdigitation generalization (IG) of 1 and 2 . It should be
clear that this IG will be true whenever either 1 or 2 are true. In particular, if 1 holds along a
time-interval in a model, then there is a sequence of consecutive (meeting) subintervals where the
sequence of states in 1 are true. By construction, the IG can be aligned relative to 1 along the
interval so that when we view states as sets, the states in the IG are subsets of the corresponding
aligned state(s) in 1 . Thus, the IG states are all true in the model under the alignment, showing
that the IG is true in the model.
In general, there are exponentially many IGs of two input MA timelines, one for each possible
interdigitation between the two. Clearly, since each IG is a generalization of the input timelines,
then so is the conjunction of all the IGs. This conjunction is an AMA formula that generalizes the
input MA timelines. In fact, we show later in the paper that this AMA formula is the LGG of the
two timelines. Below we show the conjunction of all the IGs of 1 and 2 which serves as their
LGG.
387

fiF ERN , G IVAN , & S ISKIND

[(a ^ b); b; e; true; e] ^
[(a ^ b); b; true; e] ^
[(a ^ b); b; true; true; e] ^
[(a ^ b); b; true; e] ^
[(a ^ b); b; true; d; e] ^
[(a ^ b); true; true; e] ^
[(a ^ b); true; e] ^
[(a ^ b); true; d; e] ^
[(a ^ b); a; true; true; e] ^
[(a ^ b); a; true; e] ^
[(a ^ b); a; true; d; e] ^
[(a ^ b); a; d; e] ^
[(a ^ b); a; true; d; e]
While this formula is an LGG, it contains redundant timelines that can be pruned. First, it is
clear that different IGs can result in the same MA timelines, and we can remove all but one copy
of each timeline from the LGG. Second, note that if a timeline 0 is more general than a timeline
, then  ^ 0 is equivalent to thus, we can prune away timelines that are generalizations of
others. Later in the paper, we show how to efficiently test whether one timeline is more general
than another. After performing these pruning steps, we are left with only the first and next to last
timelines in the above formulathus, [(a ^ b); a; d; e] ^ [(a ^ b); b; e; true; e] is an LGG of 1 and
2 .
We have demonstrated how to compute the LGG of pairs of MA timelines. We can use this
procedure to compute the LGG of pairs of AMA formulas. Given two AMA formulas we compute
their LGG by simply conjoining the LGGs of all pairs of timelines (one from each AMA formula)
i.e., the formula
m^
n
^
LGG(i ; 0j )
i j

is an LGG of the two AMA formulas 1 ^    ^ m and 01 ^    ^ 0n , where the i and 0j are
MA timelines.
We have now informally described the LGCF and LGG operations needed to carry out the
specific-to-general learning approach described above. In what follows, we more formally develop
these operations and analyze the theoretical properties of the corresponding problems, then discuss
the needed extensions to bring these (exponential, propositional, and negation-free) operations to
practice.

3. Representing Events with AMA
Here we present a formal account of the AMA hypothesis space and an analytical development of the
algorithms needed for specific-to-general learning for AMA. Readers that are primarily interested in
a high-level view of the algorithms and their empirical evaluation may wish to skip Sections 3 and 4
and instead proceed directly to Sections 5 and 6, where we discuss several practical extensions to
the basic learner and then present our empirical evaluation.
We study a subset of an interval-based logic called event logic (Siskind, 2001) utilized by
L EONARD for event recognition in video sequences. This logic is interval-based in explicitly rep388

fiL EARNING T EMPORAL E VENTS

resenting each of the possible interval relationships given originally by Allen (1983) in his calculus
of interval relations (e.g., overlaps, meets, during). Event-logic formulas allow the definition
of event types which can specify static properties of intervals directly and dynamic properties by
hierarchically relating sub-intervals using the Allen relations. In this paper, the formal syntax and
semantics of full event logic are needed only for Proposition 4 and are given in Appendix A.
Here we restrict our attention to a much simpler subset of event logic we call AMA, defined
below. We believe that our choice of event logic rather than first-order logic, as well as our restriction
to the AMA fragment of event logic, provide a useful learning bias by ruling out a large number of
practically useless concepts while maintaining substantial expressive power. The practical utility
of this bias is demonstrated via our empirical results in the visual-eventrecognition application.
AMA can also be seen as a restriction of LTL (Bacchus & Kabanza, 2000) to conjunction and
Until, with similar motivations. Below we present the syntax and semantics of AMA along with
some of the key technical properties of AMA that will be used throughout this paper.
3.1 AMA Syntax and Semantics
It is natural to describe temporal events by specifying a sequence of properties that must hold over
consecutive time intervals. For example, a hand picking up a block might become the block
is not supported by the hand and then the block is supported by the hand. We represent such
sequences with MA timelines5 , which are sequences of conjunctive state restrictions. Intuitively, an
MA timeline is given by a sequence of propositional conjunctions, separated by semicolons, and is
taken to represent the set of events that temporally match the sequence of consecutive conjunctions.
An AMA formula is then the conjunction of a number of MA timelines, representing events that
can be simultaneously viewed as satisfying each of the conjoined timelines. Formally, the syntax of
AMA formulas is given by,
state
MA
AMA

::= true j prop j prop ^ state
::= (state) j (state); MA
// may omit parens
::= MA j MA ^ AMA

where prop is any primitive proposition (sometimes called a primitive event type). We take this
grammar to formally define the terms MA timeline, MA formula, AMA formula, and state. A k MA formula is an MA formula with at most k states, and a k -AMA formula is an AMA formula
all of whose MA timelines are k -MA timelines. We often treat states as proposition sets with
true the empty set and AMA formulas as MA-timeline sets. We may also treat MA formulas as
sets of statesit is important to note, however, that MA formulas may contain duplicate states,
and the duplication can be significant. For this reason, when treating MA timelines as sets, we
formally intend sets of state-index pairs (where the index gives a states position in the formula).
We do not indicate this explicitly to avoid encumbering our notation, but the implicit index must be
remembered whenever handling duplicate states.
The semantics of AMA formulas is defined in terms of temporal models. A temporal model
M = hM; I i over the set PROP of propositions is a pair of a mapping M from the natural numbers
(representing time) to the truth assignments over PROP, and a closed natural-number interval I .
We note that Siskind (2001) gives a continuous-time semantics for event logic where the models
5. MA stands for Meets/And, an MA timeline being the Meet of a sequence of conjunctively restricted intervals.

389

fiF ERN , G IVAN , & S ISKIND

are defined in terms of real-valued time intervals. The temporal models defined here use discrete
natural-number time-indices. However, our results here still apply under the continuous-time semantics. (That semantics bounds the number of state changes in the continuous timeline to a countable number.) It is important to note that the natural numbers in the domain of M are representing
time discretely, but that there is no prescribed unit of continuous time represented by each natural
number. Instead, each number represents an arbitrarily long period of continuous time during which
nothing changed. Similarly, the states in our MA timelines represent arbitrarily long periods of time
during which the conjunctive restriction given by the state holds. The satisfiability relation for AMA
formulas is given as follows:




A state s is satisfied by a model hM; I i iff M [x] assigns P true for every x 2 I and P



An AMA formula 1 ^ 2 ^    ^ n is satisfied by M iff each i is satisfied by M.

2 s.

An MA timeline s1 ; s2 ; : : : ; sn is satisfied by a model hM; [t; t0 ]i iff there exists some t00
in [t; t0 ] such that hM; [t; t00 ]i satisfies s1 and either hM; [t00 ; t0 ]i or hM; [t00 + 1; t0 ]i satisfies
s2 ; : : : ; sn .

The condition defining satisfaction for MA timelines may appear unintuitive at first due to the
fact that there are two ways that s2 ; : : : ; sn can be satisfied. The reason for this becomes clear by recalling that we are using the natural numbers to represent continuous time intervals. Intuitively, from
a continuous-time perspective, an MA timeline is satisfied if there are consecutive continuous-time
intervals satisfying the sequence of consecutive states of the MA timeline. The transition between
consecutive states si and si+1 can occur either within an interval of constant truth assignment (that
happens to satisfy both states) or exactly at the boundary of two time intervals of constant truth
value. In the above definition, these cases correspond to s2 ; : : : ; sn being satisfied during the time
intervals [t00 ; t0 ] and [t00 + 1; t0 ] respectively.
When M satisfies  we say that M is a model of  or that  covers M. We say that AMA 	1
subsumes AMA 	2 iff every model of 	2 is a model of 	1 , written 	2  	1 , and we say that 	1
properly subsumes 	2 , written 	2 < 	1 , when we also have 	1 6 	2 . Alternatively, we may state
	2  	1 by saying that 	1 is more general (or less specific) than 	2 or that 	1 covers 	2 . Siskind
(2001) provides a method to determine whether a given model satisfies a given AMA formula.
Finally, it will be useful to associate a distinguished MA timeline to a model. The MA projection
of a model M = hM; [i; j ]i written as MAP(M) is an MA timeline s0 ; s1 ; : : : ; sj i where state sk
gives the true propositions in M (i + k ) for 0  k  j i. Intuitively, the MA projection gives
the sequence of propositional truth assignments from the beginning to the end of the model. Later
we show that the MA projection of a model can be viewed as representing that model in a precise
sense.
The following two examples illustrate some basic behaviors of AMA formulas:
Example 1 (Stretchability). S1 ; S2 ; S3 , S1 ; S2 ; S2 ; : : : ; S2 ; S3 , and S1 ; S1 ; S1 ; S2 ; S3 ; S3 ; S3 are
all equivalent MA timelines. In general, MA timelines have the property that duplicating any state
results in a formula equivalent to the original formula. Recall that, given a model hM; I i, we
view each truth assignment M [x] as representing a continuous time-interval. This interval can
conceptually be divided into an arbitrary number of subintervals. Thus if state S is satisfied by
hM; [x; x]i, then so is the state sequence S ; S ; : : : ; S .
390

fiL EARNING T EMPORAL E VENTS

Example 2 (Infinite Descending Chains). Given propositions A and B , the MA timeline  =
is subsumed by each of the formulas A; B , A; B ; A; B , A; B ; A; B ; A; B , . . . . This is
intuitively clear when our semantics are viewed from a continuous-time perspective. Any interval
in which both A and B are true can be broken up into an arbitrary number of subintervals where
both A and B hold. This example illustrates that there can be infinite descending chains of AMA
formulas where the entire chain subsumes a given formula (but no member is equivalent to the given
formula). In general, any AMA formula involving only the propositions A and B will subsume .

(A ^ B )

3.2 Motivation for AMA
MA timelines are a very natural way to capture stretchable sequences of state constraints. But
why consider the conjunction of such sequences, i.e., AMA? We have several reasons for this language enrichment. First of all, we show below that the AMA least-general generalization (LGG)
is uniquethis is not true for MA. Second, and more informally, we argue that parallel conjunctive constraints can be important to learning efficiency. In particular, the space of MA formulas
of length k grows in size exponentially with k , making it difficult to induce long MA formulas.
However, finding several shorter MA timelines that each characterize part of a long sequence of
changes is exponentially easier. (At least, the space to search is exponentially smaller.) The AMA
conjunction of these timelines places these shorter constraints simultaneously and often captures a
great deal of the concept structure. For this reason, we analyze AMA as well as MA and, in our
empirical work, we consider k -AMA.
The AMA language is propositional. But our intended applications are relational, or first-order,
including visual-event recognition. Later in this paper, we show that the propositional AMA learning algorithms that we develop can be effectively applied in relational domains. Our approach to
first-order learning is distinctive in automatically constructing an object correspondence across examples (cf. Lavrac, Dzeroski, & Grobelnik, 1991; Roth & Yih, 2001). Similarly, though AMA
does not allow for negative state constraints, in Section 5.4 we discuss how to extend our results to
incorporate negation into our learning algorithms, which is crucial in visual-event recognition.
3.3 Conversion to First-Order Clauses
We note that AMA formulas can be translated in various ways into first-order clauses. It is not
straightforward, however, to then use existing clausal generalization techniques for learning. In
particular, to capture the AMA semantics in clauses, it appears necessary to define subsumption and
generalization relative to a background theory that restricts us to a continuous-time first-order
model space.
For example, consider the AMA formulas 1 = A ^ B and 2 = A; B where A and B are
propositionsfrom Example 2 we know that 1  2 . Now, consider a straightforward clausal
translation of these formulas giving C1 = A(I ) ^ B (I ) and C2 = A(I1 ) ^ B (I2 ) ^ M EETS (I1 ; I2 ) ^
I = S PAN (I1 ; I2 ), where the I and Ij are variables that represent time intervals, M EETS indicates
that two time intervals meet each other, and S PAN is a function that returns a time interval equal
to the union of its two time-interval arguments. The meaning we intend to capture is for satisfying
assignments of I in C1 and C2 to indicate intervals over which 1 and 2 are satisfied, respectively.
It should be clear that, contrary to what we want, C1 6 C2 (i.e., 6j= C1 ! C2 ), since it is easy to
find unintended first-order models that satisfy C1 , but not C2 . Thus such a translation, and other
similar translations, do not capture the continuous-time nature of the AMA semantics.
391

fiF ERN , G IVAN , & S ISKIND

In order to capture the AMA semantics in a clausal setting, one might define a first-order theory
that restricts us to continuous-time modelsfor example, allowing for the derivation if property B
holds over an interval, then that property also holds over all sub-intervals. Given such a theory ,
we have that  j= C1 ! C2 , as desired. However, it is well known that least-general generalizations relative to such background theories need not exist (Plotkin, 1971), so prior work on clausal
generalization does not simply subsume our results for the AMA language.
We note that for a particular training set, it may be possible to compile a continuous-time background theory  into a finite but adequate set of ground facts. Relative to such ground theories,
clausal LGGs are known to always exist and thus could be used for our application. However,
the only such compiling approaches that look promising to us require exploiting an analysis similar to the one given in this paperi.e., understanding the AMA generalization and subsumption
problem separately from clausal generalization and exploiting that understanding in compiling the
background theory. We have not pursued such compilations further.
Even if we are given such a compilation procedure, there are other problems with using existing clausal generalization techniques for learning AMA formulas. For the clausal translations of
AMA we have found, the resulting generalizations typically fall outside of the (clausal translations
of formulas in the) AMA language, so that the language bias of AMA is lost. In preliminary empirical work in our video-event recognition domain using clausal inductive-logic-programming (ILP)
systems, we found that the learner appeared to lack the necessary language bias to find effective
event definitions. While we believe that it would be possible to find ways to build this language bias
into ILP systems, we chose instead to define and learn within the desired language bias directly, by
defining the class of AMA formulas, and studying the generalization operation on that class.
3.4 Basic Concepts and Properties of AMA
We use the following convention in naming our results: propositions and theorems are the key
results of our work, with theorems being those results of the most technical difficulty, and lemmas
are technical results needed for the later proofs of propositions or theorems. We number all the
results in one sequence, regardless of type. Proofs of theorems and propositions are provided in the
main textomitted proofs of lemmas are provided in the appendix.
We give pseudo-code for our methods in a non-deterministic style. In a non-deterministic language functions can return more than one value non-deterministically, either because they contain
non-deterministic choice points, or because they call other non-deterministic functions. Since a nondeterministic function can return more than one possible value, depending on the choices made at
the choice points encountered, specifying such a function is a natural way to specify a richly structured set (if the function has no arguments) or relation (if the function has arguments). To actually
enumerate the values of the set (or the relation, once arguments are provided) one can simply use
a standard backtracking search over the different possible computations corresponding to different
choices at the choice points.
3.4.1 S UBSUMPTION

AND

G ENERALIZATION

FOR

S TATES

The most basic formulas we deal with are states (conjunctions of propositions). In our propositional
setting computing subsumption and generalization at the state level is straightforward. A state S1
subsumes S2 (S2  S1 ) iff S1 is a subset of S2 , viewing states as sets of propositions. From this, we
derive that the intersection of states is the least-general subsumer of those states and that the union
of states is likewise the most general subsumee.
392

fiL EARNING T EMPORAL E VENTS

3.4.2 I NTERDIGITATIONS
Given a set of MA timelines, we need to consider the different ways in which a model could simultaneously satisfy the timelines in the set. At the start of such a model (i.e., the first time point),
the initial state from each timeline must be satisfied. At some time point in the model, one or more
of the timelines can transition so that the second state in those timelines must be satisfied in place
of the initial state, while the initial state of the other timelines remains satisfied. After a sequence
of such transitions in subsets of the timelines, the final state of each timeline holds. Each way of
choosing the transition sequence constitutes a different interdigitation of the timelines.
Viewed differently, each model simultaneously satisfying the timelines induces a co-occurrence
relation on tuples of timeline states, one from each timeline, identifying which tuples co-occur at
some point in the model. We represent this concept formally as a set of tuples of co-occurring states,
i.e., a co-occurrence relation. We sometimes think of this set of tuples as ordered by the sequence
of transitions. Intuitively, the tuples in an interdigitation represent the maximal time intervals over
which no MA timeline has a transition, with those tuples giving the co-occurring states for each
such time interval.
A relation R on X1      Xn is simultaneously consistent with orderings 1 ,. . . ,n, if,
whenever R(x1 ; : : : ; xn ) and R(x01 ; : : : ; x0n ), either xi i x0i , for all i, or x0i i xi , for all i. We say
R is piecewise total if the projection of R onto each component is totali.e., every state in any Xi
appears in R.
Definition 1 (Interdigitation). An interdigitation I of a set f1 ; : : : ; n g of MA timelines is a cooccurrence relation over 1      n (viewing timelines as sets of states6 ) that is piecewise total
and simultaneously consistent with the state orderings of each i . We say that two states s 2 i
and s0 2 j for i 6= j co-occur in I iff some tuple of I contains both s and s0 . We sometimes refer to
I as a sequence of tuples, meaning the sequence lexicographically ordered by the i state orderings.
We note that there are exponentially many interdigitations of even two MA timelines (relative to the
total number of states in the timelines). Example 3 on page 396 shows an interdigitation of two MA
timelines. Pseudo-code for non-deterministically generating an arbitrary interdigitation for a set of
MA timelines can be found in Figure 5. Given an interdigitation I of the timelines s1 ; s2 ; : : : ; sm
and t1 ; t2 ; : : : ; tn (and possibly others), the following basic properties of interdigitations are easily
verifiable:
1. For i < j , if si and tk co-occur in I then for all k 0
2.

< k, sj does not co-occur with tk

0

in I .

I (s1 ; t1 ) and I (sm ; tn ).

We first use interdigitations to syntactically characterize subsumption between MA timelines.
Definition 2 (Witnessing Interdigitation). An interdigitation I of two MA timelines 1 and 2
is a witness to 1  2 iff for every pair of co-occurring states s1 2 1 and s2 2 2 , we have that
s2 is a subset of s1 (i.e., s1  s2 ).
The following lemma and proposition establish the equivalence between witnessing interdigitations
and MA subsumption.
6. Recall, that, formally, MA timelines are viewed as sets of state-index pairs, rather than just sets of states. We ignore
this distinction in our notation, for readability purposes, treating MA timelines as though no state is duplicated.

393

fiF ERN , G IVAN , & S ISKIND

1:

an-interdigitation (f1 ; 2 ; : : : ; n g)

// Input: MA timelines 1 ; : : : ; n
// Output: an interdigitation of f1 ; : : : ; n g

2:
3:

S0 := hhead(1 ); : : : ; head(n )i;
if for all 1  i  n; ji j = 1
then return hS0 i;
0
T := fi such that ji j > 1g;
T 00 := a-non-empty-subset-of (T 0 );

4:
5:
6:
7:
8:

for i := 1 to n
if i 2 T 00
then 0i := rest(i )
else 0i := i ;

9:
10:
12:
12:

return extend-tuple (S0 ; an-interdigitation (f01 ; : : : ; 0n g));

13:

Figure 5: Pseudo-code for an-interdigitation(), which non-deterministically computes an interdigitation for a set f1 ; : : : ; n g of MA timelines. The function head() returns the first
state in the timeline . rest() returns  with the first state removed. extend-tuple(x,I )
extends a tuple I by adding a new first element x to form a longer tuple. a-non-emptysubset-of(S ) non-deterministically returns an arbitrary non-empty subset of S .
Lemma 1. For any MA timeline  and any model M, if M satisfies , then there is a witnessing
interdigitation for MAP(M)  .
Proposition 2. For MA timelines 1 and 2 , 1

1  2 .

 2 iff there is an interdigitation that witnesses

Proof: We show the backward direction by induction on the number of states n in timeline 1 . If
n = 1, then the existence of a witnessing interdigitation for 1  2 implies that every state in 2
is a subset of the single state in 1 , and thus that any model of 1 is a model of 2 so that 1  2 .
Now, suppose for induction that the backward direction of the theorem holds whenever 1 has n
or fewer states. Given an arbitrary model M of an n + 1 state 1 and an interdigitation W that
witnesses 1  2 , we must show that M is also a model of 2 to conclude 1  2 as desired.
Write 1 as s1 ; : : : ; sn+1 and 2 as t1 ; : : : ; tm . As a witnessing interdigitation, W must identify
some maximal prefix t1 ; : : : ; tm of 2 made up of states that co-occur with s1 and thus that are
subsets of s1 . Since M = hM; [t; t0 ]i satisfies 1 , by definition there must exist a t00 2 [t; t0 ] such
that hM; [t; t00 ]i satisfies s1 (and thus t1 ; : : : ; tm ) and hM; I 0 i satisfies s2 ; : : : ; sn+1 for I 0 equal to
either [t00 ; t0 ] or [t00 + 1; t0 ]. In either case, it is straightforward to construct, from W , a witnessing
interdigitation for s2 ; : : : ; sn+1  tm +1 ; : : : ; tm and use the induction hypothesis to then show that
hM; I 0 i must satisfy tm +1; : : : ; tm . It follows that M satisfies 2 as desired.
For the forward direction, assume that 1  2 , and let M be any model such that 1 =
MAP(M). It is clear that such an M exists and satisfies 1 . It follows that M satisfies 2 .
Lemma 1 then implies that there is a witnessing interdigitation for MAP(M)  2 and thus for
1  2 . 2
0

0

0

0

394

fiL EARNING T EMPORAL E VENTS

3.4.3 L EAST-G ENERAL C OVERING F ORMULA
A logic can discriminate two models if it contains a formula that satisfies one but not the other. It
turns out that AMA formulas can discriminate two models exactly when much richer internal positive event logic (IPEL) formulas can do so. Internal formulas are those that define event occurrence
only in terms of properties within the defining interval. That is, satisfaction by hM; I i depends only
on the proposition truth values given by M inside the interval I . Positive formulas are those that
do not contain negation. Appendix A gives the full syntax and semantics of IPEL (which are used
only to state and prove Lemma 3 ). The fact that AMA can discriminate models as well as IPEL
indicates that our restriction to AMA formulas retains substantial expressive power and leads to
the following result which serves as the least-general covering formula (LGCF) component of our
specific-to-general learning procedure. Formally, an LGCF of model M within a formula language
L (e.g. AMA or IPEL) is a formula in L that covers M such that no other covering formula in
L is strictly less general. Intuitively, the LGCF of a model, if unique, is the most representative
formula of that model. Our analysis uses the concept of model embedding. We say that model M
embeds model M0 iff MAP(M)  MAP(M0 ).
Lemma 3.

For any E

2 IP EL, if model M embeds a model that satisfies E , then M satisfies E .

Proposition 4. The MA projection of a model is its LGCF for internal positive event logic (and
hence for AMA), up to semantic equivalence.
Proof: Consider model M. We know that MAP(M) covers M, so it remains to show that
MAP(M) is the least general formula to do so, up to semantic equivalence.
Let E be any IPEL formula that covers M. Let M0 be any model that is covered by MAP(M)
we want to show that E also covers M0 . We know, from Lemma 1, that there is a witnessing
interdigitation for MAP(M0 )  MAP(M). Thus, by Proposition 2, MAP(M0 )  MAP(M)
showing that M0 embeds M. Combining these facts with Lemma 3 it follows that E also covers
M0 and hence MAP(M)  E . 2
Proposition 4 tells us that, for IPEL, the LGCF of a model exists, is unique, and is an MA
timeline. Given this property, when an AMA formula 	 covers all the MA timelines covered by
another AMA formula 	0 , we have 	0  	. Thus, for the remainder of this paper, when considering
subsumption between formulas, we can abstract away from temporal models and deal instead with
MA timelines. Proposition 4 also tells us that we can compute the LGCF of a model by constructing
the MA projection of that model. Based on the definition of MA projection, it is straightforward to
derive an LGCF algorithm which runs in time polynomial in the size of the model7 . We note that
the MA projection may contain repeated states. In practice, we remove repeated states, since this
does not change the meaning of the resulting formula (as described in Example 1).
3.4.4 C OMBINING I NTERDIGITATION

WITH

G ENERALIZATION

OR

S PECIALIZATION

Interdigitations are useful in analyzing both conjunctions and disjunctions of MA timelines. When
conjoining a set of timelines, any model of the conjunction induces an interdigitation of the timelines
such that co-occurring states simultaneously hold in the model at some point (viewing states as
sets, the the states resulting from unioning co-occurring states must hold). By constructing an
7. We take the size of a model M = hM; I i to be the sum over x 2 I of the number of true propositions in M (x).

395

fiF ERN , G IVAN , & S ISKIND

interdigitation and taking the union of each tuple of co-occurring states to get a sequence of states,
we get an MA timeline that forces the conjunction of the timelines to hold. We call such a sequence
an interdigitation specialization of the timelines. Dually, an interdigitation generalization involving
intersections of states gives an MA timeline that holds whenever the disjunction of a set of timelines
holds.
Definition 3. An interdigitation generalization (specialization) of a set  of MA timelines is an MA
timeline s1 ; : : : ; sm , such that, for some interdigitation I of  with m tuples, sj is the intersection
(respectively, union) of the components of the jth tuple of the sequence I . The set of interdigitation
generalizations (respectively, specializations) of  is called IG() (respectively, IS()).
Example 3. Suppose that s1 ; s2 ; s3 ; t1 ; t2 ; and t3 are each sets of propositions (i.e., states). Consider the timelines S = s1 ; s2 ; s3 and T = t1 ; t2 ; t3 . The relation

f hs1; t1 i ; hs2; t1 i ; hs3; t2 i ; hs3; t3 i g
is an interdigitation of S and T in which states s1 and s2 co-occur with t1 , and s3 co-occurs with
t2 and t3 . The corresponding IG and IS members are

s1 \ t1 ; s2 \ t1 ; s3 \ t2 ; s3 \ t3
s1 [ t1 ; s2 [ t1 ; s3 [ t2 ; s3 [ t3

2 IG(fS; T g)
2 IS(fS; T g):

If t1  s1 ; t1  s2 ; t2  s3 ; and t3  s3 , then the interdigitation witnesses S

 T.

Each timeline in IG() (dually, IS()) subsumes (is subsumed by) each timeline in this is
easily verified using Proposition 2. For our complexity analyses, we note that the number of states
in any member of IG() or IS() is bounded from below by the number of states in any of the
MA timelines in  and is bounded from above by the total number of states in all the MA timelines
in . The number of interdigitations of , and thus of members of IG() or IS(), is exponential in that same total number of states. The algorithms that we present later for computing LGGs
require the computation of both IG () and IS(). Here we give pseudo-code to compute these
quantities. Figure 6 gives pseudo-code for the function an-IG-member that non-deterministically
computes an arbitrary member of IG() (an-IS-member is the same, except that we replace intersection by union). Given a set  of MA timelines we can compute IG() by executing all possible
deterministic computation paths of the function call an-IG-member(), i.e., computing the set of
results obtainable from the non-deterministic function for all possible decisions at non-deterministic
choice points.
We now give a useful lemma and a proposition concerning the relationships between conjunctions and disjunctions of MA concepts (the former being AMA concepts). For convenience here,
we use disjunction on MA concepts, producing formulas outside of AMA with the obvious interpretation.
Lemma 5. Given an MA formula  that subsumes each member of a set  of MA formulas,  also
subsumes some member 0 of IG(). Dually, when  is subsumed by each member of , we have
that  is also subsumed by some member 0 of IS(). In each case, the length of 0 is bounded by
the size of .

396

fiL EARNING T EMPORAL E VENTS

an-IG-member (f1 ; 2 ; : : : ; n g)

// Input: MA timelines 1 ; : : : ; n
// Output: a member of IG(f1 ; 2 ; : : : ; n g)

return map (intersect-tuple ; an-interdigitation (f1 ; : : : ; n g));
Figure 6: Pseudo-code for an-IG-member, which non-deterministically computes a member of
IG(T ) where T is a set of MA timelines. The function intersect-tuple(I ) takes a tuple I
of sets as its argument and returns their intersection. The higher-order function map(f; I )
takes a function f and a tuple I as arguments and returns a tuple of the same length as I
obtained by applying f to each element of I and making a tuple of the results.
Proposition 6.

The following hold:

1. (and-to-or) The conjunction of a set  of MA timelines equals the disjunction of the timelines
in IS().
2. (or-to-and) The disjunction of a set  of MA timelines is subsumed by the conjunction of the
timelines in IG().
Proof: To prove or-to-and, recall that, for any  2  and any 0 2 IG(), we have that   0 .
W
V
From this it is immediate that ( )  ( IG()). Using a dual argument, we can show that
W
V
V
W
( IS())  ( ). It remains Vto show that ( )  ( ISW()), which is equivalent to showing
that any timeline subsumed by ( ) is also subsumed by ( IS()) (by Proposition 4). Consider
V
any MA timeline  such that   ( )this implies that each member of  subsumes . Lemma
W
5 then implies that there is some 0 2 IS() such that   0 . From this we get that   ( IS())
as desired. 2
Using and-to-or, we can now reduce AMA subsumption to MA subsumption, with an exponential increase in the problem size.
Proposition 7.
	2 ; 1  2 .

For AMA

	1

and

	2 , 	1

 	2 if and only if for all 1 2 IS(	1) and 2 2

Proof: For the forward direction we show the contrapositive. Assume there is a 1 2 IS(	1 ) and a
2 2 	2 such that W1 6 2 . Thus, there is an MA timeline
 such that   1 but  6 2 . This
W
tells us that   ( IS(	1 )) and that  6 	2 , thus ( IS(	1 )) 6 	2 and by and-to-or we get
that 	1 6 	2 .
For the backward direction assume that for all 1 2 IS(	1 ) and 2 2 	2 that 1  2 . This
W
tells us that for each 1 2 IS(	1 ), that 1  	2 thus, 	1 = ( IS(	1 ))  	2 . 2

4. Subsumption and Generalization
In this section we study subsumption and generalization of AMA formulas. First, we give a
polynomial-time algorithm for deciding subsumption between MA formulas and then show that
deciding subsumption for AMA formulas is coNP-complete. Second we give algorithms and complexity bounds for the construction of least-general generalization (LGG) formulas based on our
397

fiF ERN , G IVAN , & S ISKIND

MA-subsumes (1 ; 2 )
// Input: 1 = s1 ; : : : ; sm and 2
// Output: 1  2

= t1 ; : : : ; tn

1. if there is a path from v1;1 to vm;n in SG(1 ; 2 ) then return TRUE. For example,
(a)
(b)

(c)

Create an array Reachable(i,j ) of boolean values, all FALSE, for 0
0  j  n.
for i := 1 to m, Reachable(i; 0) := TRUE;
for j := 1 to n, Reachable(0; j ) := TRUE;
for i := 1 to m
for j := 1 to n
Reachable(i; j ) := (ti  sj ^ ( Reachable(i
Reachable(i; j
Reachable(i

if Reachable(m; n) then return TRUE;

 i  m and

1; j ) _
1) _
1; j 1));

2. Otherwise, return FALSE;
Figure 7: Pseudo-code for the MA subsumption algorithm.
defined in the main text.

SG(1 ; 2 ) is the subsumption graph

analysis of subsumption, including existence, uniqueness, lower/upper bounds, and an algorithm for
the LGG on AMA formulas. Third, we introduce a polynomial-timecomputable syntactic notion
of subsumption and an algorithm that computes the corresponding syntactic LGG that is exponentially faster than our semantic LGG algorithm. Fourth, in Section 4.4, we give a detailed example
showing the steps performed by our LGG algorithms to compute the semantic and syntactic LGGs
of two AMA formulas.
4.1 Subsumption
All our methods rely critically on a novel algorithm for deciding the subsumption question 1  2
between MA formulas 1 and 2 in polynomial-time. We note that merely searching the possible
interdigitations of 1 and 2 for a witnessing interdigitation provides an obvious decision procedure
for the subsumption questionhowever, there are, in general, exponentially many such interdigitations. We reduce the MA subsumption problem to finding a path in a graph on pairs of states
in 1  2 , a polynomial-time operation. Pseudo-code for the resulting MA subsumption algorithm is shown in Figure 7. The main data structure used by the MA subsumption algorithm is the
subsumption graph.
Definition 4. The subsumption graph of two MA timelines 1 = s1 ;    ; sm and 2 = t1 ;    ; tn
(written SG(1 ; 2 )) is a directed
graph G = hV; E i with V = fvi;j j 1  i  m; 1  j  ng	.

The (directed) edge set E equals hvi;j ; vi ;j i j si  tj ; si  tj ; i  i0  i + 1; j  j 0  j + 1 .
0

0

0

0

To achieve a polynomial-time bound one can simply use any polynomial-time pathfinding algorithm. In our case the special structure of the subsumption graph can be exploited to determine if
398

fiL EARNING T EMPORAL E VENTS

the desired path exists in O (mn) time, as the example method shown in the pseudo-code illustrates.
The following theorem asserts the correctness of the algorithm assuming a correct polynomial-time
path-finding method is used.
Lemma 8. Given MA timelines 1 = s1 ; : : : ; sm and 2 = t1 ; : : : ; tn , there is a witnessing
interdigitation for 1  2 iff there is a path in the subsumption graph SG(1 ; 2 ) from v1;1 to
vm;n .
Theorem 9.
mial time.

Given MA timelines 1 and 2 , MA-subsumes(1 ; 2 ) decides 1

 2 in polyno-

Proof: The algorithm clearly runs in polynomial time. Lemma 8 tells us that line 2 of the algorithm
will return TRUE iff there is a witnessing interdigitation. Combining this with Proposition 2 shows
that the algorithm returns TRUE iff 1  2 . 2
Given this polynomial-time algorithm for MA subsumption, Proposition 7 immediately suggests
an exponential-time algorithm for deciding AMA subsumptionby computing MA subsumption
between the exponentially many IS timelines of one formula and the timelines of the other formula.
Our next theorem suggests that we cannot do any better than this in the worst casewe argue that
AMA subsumption is coNP-complete by reduction from boolean satisfiability. Readers uninterested
in the technical details of this argument may skip directly to Section 4.2.
To develop a correspondence between boolean satisfiability problems, which include negation,
and AMA formulas, which lack negation, we imagine that each boolean variable has two AMA
propositions, one for true and one for false. In particular, given a boolean satisfiability problem
over n variables p1 ; : : : ; pn , we take the set PROPn to be the set containing 2n AMA propositions
Truek and Falsek for each k between 1 and n. We can now represent a truth assignment A to the pi
variables with an AMA state sA given as follows:

sA = fTruei j 1  i  n; A(pi ) = trueg [ fFalsei j 1  i  n; A(pi ) = falseg
As Proposition 7 suggests, checking AMA subsumption critically involves the exponentially
many interdigitation specializations of the timelines of one of the AMA formulas. In our proof, we
design an AMA formula whose interdigitation specializations can be seen to correspond to truth
assignments8 to boolean variables, as shown in the following lemma.
Lemma 10.

Given some n, let 	 be the conjunction of the timelines
n
[
i=1

f(PROPn; Truei; Falsei; PROPn); (PROPn; Falsei; Truei; PROPn)g:

We have the following facts about truth assignments to the Boolean variables p1 ; : : : ; pn :
1. For any truth assignment A, PROPn ; sA ; PROPn is semantically equivalent to a member
of IS(	).
2. For each  2 IS(	) there is a truth assignment A such that   PROPn ; sA ; PROPn .
8. A truth assignment is a function mapping boolean variables to true or false.

399

fiF ERN , G IVAN , & S ISKIND

With this lemma in hand, we can now tackle the complexity of AMA subsumption.
Theorem 11.

Deciding AMA subsumption is coNP-complete.

Proof: We first show that deciding the AMA-subsumption of 	1 by 	2 is in coNP by providing
a polynomial-length certificate for any no answer. This certificate for non-subsumption is an
interdigitation of the timelines of 	1 that yields a member of IS(	1 ) not subsumed by 	2 . Such
a certificate can be checked in polynomial time: given such an interdigitation, the corresponding
member of IS(	1 ) can be computed in time polynomial in the size of 	1 , and we can then test
whether the resulting timeline is subsumed by each timeline in 	2 using the polynomial-time MAsubsumption algorithm. Proposition 7 guarantees that 	1 6 	2 iff there is a timeline in IS(	1 )
that is not subsumed by every timeline in 	2 , so that such a certificate will exist exactly when the
answer to a subsumption query is no.
To show coNP-hardness we reduce the problem of deciding the satisfiability of a 3-SAT formula
S = C1 ^  ^ Cm to the problem of recognizing non-subsumption between AMA formulas. Here,
each Ci is (li;1 _ li;2 _ li;3 ) and each li;j either a proposition p chosen from P = fp1 ; : : : ; pn g or
its negation :p. The idea of the reduction is to construct an AMA formula 	 for which we view
the exponentially many members of IS(	) as representing truth assignments. We then construct an
MA timeline  that we view as representing :S and show that S is satisfiable iff 	 6 .
Let 	 be as defined in Lemma 10. Let  be the formula s1 ; : : : ; sm , where

si =

fFalsej j li;k = pj for some kg [
fTruej j li;k = :pj for some kg:

Each si can be thought of as asserting not Ci . We start by showing that if S is satisfiable
then 	 6 . Assume that S is satisfied via a truth assignment Awe know from Lemma 10
that there is a 0 2 IS(	) that is semantically equivalent to PROPn ; sA ; PROPn . We show that
PROPn ; sA ; PROPn is not subsumed by , to conclude 	 6  using Proposition 7, as desired.
Suppose for contradiction that PROPn ; sA ; PROPn is subsumed by then the state sA must be
subsumed by some state si in . Consider the corresponding clause Ci of S . Since A satisfies S
we have that Ci is satisfied and at least one of its literals li;k must be true. Assume that li;k = pj (a
dual argument holds for li;k = :pj ), then we have that si contains Falsej while sA contains Truej
but not Falsej thus, we have that sA 6 si (since si 6 sA ), contradicting our choice of i.
To complete the proof, we now assume that S is unsatisfiable and show that 	  . Using
Proposition 7, we consider arbitrary 0 in IS(	)we will show that 0  . From Lemma 10 we
know there is some truth assignment A such that 0  PROPn ; sA ; PROPn . Since S is unsatisfiable
we know that some Ci is not satisfied by A and hence :Ci is satisfied by A. This implies that
each primitive proposition in si is in sA . Let W be the following interdigitation between T =
PROPn ; sA ; PROPn and  = s1 ; : : : ; sm :

fhPROPn; s1 i hPROPn; s2 i    hPROPn; sii hsA; sii hPROPn; sii hPROPn; si+1i    hPROPn; smig

We see that in each tuple of co-occurring states given above that the state from T is subsumed by
the state from . Thus W is a witnessing interdigitation to PROPn ; sA ; PROPn  , which then
holds by Proposition 2combining this with 0  PROPn ; sA ; PROPn we get that 0  . 2
Given this hardness result we later define a weaker polynomial-timecomputable subsumption
notion for use in our learning algorithms.
400

fiL EARNING T EMPORAL E VENTS

4.2 Least-General Generalization.
An AMA LGG of a set of AMA formulas is an AMA formula that is more general than each
formula in the set and not strictly more general than any other such formula. The existence of
an AMA LGG is nontrivial as there can be infinite chains of increasingly specific formulas all of
which generalize given formulas. Example 2 demonstrated such chains for an MA subsumee and
can be extended for AMA subsumees. For example, each member of the chain P ; Q, P ; Q; P ; Q,
P ; Q; P ; Q; P ; Q; : : : covers 	1 = (P ^ Q); Q and 	2 = P ; (P ^ Q). Despite such complications,
the AMA LGG does exist.
Theorem 12. There is an LGG for any finite set  of AMA formulas that is subsumed by all other
generalizations of .
Proof: Let be the set 	 2 IS(	0 ). Let 	 be the conjunction of all the MA timelines that
generalize while having size no larger than . Since there are only a finite number of primitive
propositions, there are only a finite number of such timelines, so 	 is well defined9 . We show that
	 is a least-general generalization of . First, note that each timeline in 	 generalizes and thus
 (by Proposition 6), so 	 must generalize . Now, consider arbitrary generalization 	0 of .
Proposition 7 implies that 	0 must generalize each formula in . Lemma 5 then implies that each
timeline of 	0 must subsume a timeline  that is no longer than the size of and that also subsumes
the timelines of . But then  must be a timeline of 	, by our choice of 	, so that every timeline of
	0 subsumes a timeline of 	. It follows that 	0 subsumes 	, and that 	 is an LGG of  subsumed
by all other LGGs of , as desired. 2
S

0

Given that the AMA LGG exists and is unique we now show how to compute it. Our first step is to
strengthen or-to-and from Proposition 6 to get an LGG for the MA sublanguage.
Theorem 13. For a set  of MA formulas, the conjunction of all MA timelines in IG() is an AMA
LGG of .
Proof: Let 	 be the specified conjunction. Since each timeline of IG() subsumes all timelines
in , 	 subsumes each member of . To show 	 is a least-general such formula, consider an
AMA formula 	0 that also subsumes all members of . Since each timeline of 	0 must subsume all
members of , Lemma 5 implies that each timeline of 	0 subsumes a member of IG() and thus
each timeline of 	0 subsumes 	. This implies 	  	0 . 2
We can now characterize the AMA LGG using IS and IG.
Theorem 14.

S

IG( 	2 IS(	)) is an AMA LGG of the set  of AMA formulas.

Proof: Let  = f	1 ; : : : ; 	n g and E = 	1 _    _ 	n . We know that the AMA LGG of 
must subsume E , or it would fail to subsume one of the 	i . Using and-to-or we can represent
W
W
E as a disjunction of MA timelines given by E = ( IS(	1 )) _    _ ( IS(	n )). Any AMA
LGG must be a least-general formula that subsumes E i.e., an AMA LGG of the set of MA
S
timelines fIS(	)j	 2 g. Theorem 13 tells us that an LGG of these timelines is given by
S
IG( fIS(	)j	 2 g). 2
9. There must be at least one such timeline, the timeline where the only state is true

401

fiF ERN , G IVAN , & S ISKIND

1:
2:
3:
4:
5:
6:
7:
8:
9:

10:
11:
12:
13:
14:

15:

semantic-LGG(f	1 ; 	2 ; : : : ; 	m g)

// Input: AMA formulas 	1 ; : : : ; 	m
// Output: LGG of f	1 ; : : : ; 	m g

S := fg;
for i := 1 to m
for each  in all-values(an-IS-member (	i ))
if (80 2 S :  6 0 )
then S 0 := f00 2 S j 00  g;
S := (S S 0 ) [ fg;
G := fg;
for each  in all-values(an-IG-member(S ))
if (80 2 G : 0 6 )
then G0 := f00 2 G j   00 g;
G := (G G0 ) [ fg;
V

return (

G)

Figure 8: Pseudo-code for computing the semantic AMA LGG of a set of AMA formulas.
Theorem 14 leads directly to an algorithm for computing the AMA LGGFigure 8 gives
pseudo-code for the computation. Lines 4-9 of the pseudo-code correspond to the computation
S
of fIS(	)j	 2 g, where timelines are not included in the set if they are subsumed by timelines
already in the set (which can be checked with the polynomial time MA subsumption algorithm).
This pruning, accomplished by the if test in line 7, often drastically reduces the size of the timeline set for which we perform the subsequent IG computationthe final result is not affected by
the pruning since the subsequent IG computation is a generalization step. The remainder of the
S
pseudo-code corresponds to the computation of IG( fIS(	)j	 2 g) where we do not include
timelines in the final result that subsume some other timeline in the set. This pruning step (the if test
in line 12) is sound since when one timeline subsumes another, the conjunction of those timelines
is equivalent to the most specific one. Section 4.4.1 traces the computations of this algorithm for an
example LGG calculation.
Since the sizes of both IS() and IG() are exponential in the sizes of their inputs, the code in
Figure 8 is doubly exponential in the input size. We conjecture that we cannot do better than this,
but we have not yet proven a doubly exponential lower bound for the AMA case. When the input
formulas are MA timelines the algorithm takes singly exponential time, since IS(fg) =  when
 is in MA. We now prove an exponential lower bound when the input formulas are in MA. Again,
readers uninterested in the technical details of this proof can safely skip forward to Section 4.3.
For this argument, we take the available primitive propositions to be those in the set fpi;j j 1 
i  n; 1  j  ng, and consider the MA timelines
and

1 = s1; ; s2; ; : : : ; sn;
2 = s;1 ; s;2 ; : : : ; s;n ;
402

where

fiL EARNING T EMPORAL E VENTS

and

si; = pi;1 ^    ^ pi;n
s;j = p1;j ^    ^ pn;j :

We will show that any AMA LGG of 1 and 2 must contain an exponential number of timelines.
In particular, we will show that any AMA LGG is equivalent to the conjunction of a subset of
IG(f1 ; 2 g), and that certain timelines may not be omitted from such a subset.
Lemma 15. Any AMA LGG 	 of a set
timelines from IG() with j	0 j  j	j

 of MA timelines is equivalent to a conjunction 	0 of

Proof: Lemma 5 implies that any timeline  in 	 must subsume some timeline 0 2 IG(). But
then the conjunction 	0 of such 0 must be equivalent to 	, since it clearly covers  and is covered
by the LGG 	. Since 	0 was formed by taking one timeline from IG() for each timeline in 	,
we have j	0 j  j	j. 2 We can complete our argument then by showing that exponentially many
timelines in IG(f1 ; 2 g) cannot be omitted from such a conjunction while it remains an LGG.
Notice that for any i; j we have that si; \s;j = pi;j . This implies that any state in IG(f1 ; 2 g)
contains exactly one proposition, since each such state is formed by intersecting a state from 1 and
2 . Furthermore, the definition of interdigitation, applied here, implies the following two facts for
any timeline q1 ; q2 ; : : : ; qm in IG(f1 ; 2 g):
1. q1

= p1;1 and qm = pn;n.

2. For consecutive states qk
and not both i = i0 and j

= pi;j and qk+1 = pi ;j , i0 is either i or i + 1, j 0 is either j or j + 1,
= j0.
0

0

Together these facts imply that any timeline in IG(f1 ; 2 g) is a sequence of propositions starting
with p1;1 and ending with pn;n such that any consecutive propositions pi;j ; pi ;j are different with
i0 equal to i or i + 1 and j 0 equal to j or j + 1. We call a timeline in IG(f1 ; 2 g) square if
and only if each pair of consecutive propositions pi;j and pi ;j have either i0 = i or j 0 = j . The
following lemma implies that no square timeline can be omitted from the conjunction of timelines
in IG(1 ; 2 ) if it is to remain an LGG of 1 and 2 .
0

0

0

0

Lemma 16. Let 1 and 2 be as given above and let 	 = IG(f1 ; 2 g). For any
timelines are a subset of those in 	 that omits some square timeline, we have 	 < 	0 .
V

	0 whose

n 2)! and hence is exponenThe number of square timelines in IG(f1 ; 2 g) is equal to (n (21)!(
n 1)!
tial in the size of 1 and 2 . We have now completed the proof of the following result.

Theorem 17.

The smallest LGG of two MA formulas can be exponentially large.

Proof: By Lemma 15, any AMA LGG 	0 of 1 and 2 is equivalent to a conjunction of the same
number of timelines chosen from IG(f1 ; 2 g). However, by Lemma 16, any such conjunction
n 2)! timelines, and then so must 	0 , which must then be exponentially
must have at least (n (21)!(
n 1)!
large. 2
Conjecture 18.

The smallest LGG of two AMA formulas can be doubly-exponentially large.
403

fiF ERN , G IVAN , & S ISKIND

We now show that our lower-bound on AMA LGG complexity is not merely a consequence of
the existence of large AMA LGGs. Even when there is a small LGG, it can be expensive to compute
due to the difficulty of testing AMA subsumption:
Theorem 19. Determining whether a formula 	 is an AMA LGG for two given AMA formulas 	1
and 	2 is co-NP-hard, and is in co-NEXP, in the size of all three formulas together.
Proof: To show co-NP-hardness we use a straightforward reduction from AMA subsumption. Given
two AMA formulas 	1 and 	2 we decide 	1  	2 by asking whether 	2 is an AMA LGG of 	1
and 	2 . Clearly 	1  	2 iff 	2 is an LGG of the two formulas.
To show the co-NEXP upper bound, note that we can check in exponential time whether 	1  	
and 	2  	 using Proposition 7 and the polynomial-time MA subsumption algorithm. It remains
to show that we can check whether 	 is not the least subsumer. Since Theorem 14 shows that the
LGG of 	1 and 	2 is IG(IS(	1 ) [ IS(	2 )), if 	 is not the LGG then 	 6 IG(IS(	1 ) [ IS(	2 )).
Thus, by Proposition 7, if 	 is not a least subsumer, there must be timelines 1 2 IS(	) and
2 2 IG(IS(	1 ) [ IS(	2 )) such that 1 6 2 . We can then use exponentially long certificates
for No answers: each certificate is a pair of an interdigitation I1 of 	 and an interdigitation I2 of
IS(	1 ) [ IS(	2 ), such that the corresponding members 1 2 IS(	) and 2 2 IG(IS(	1 ) [ IS(	2 ))
have 1 6 2 . Given the pair of certificates I1 and I2 , 1 can be computed in polynomial time,
2 can be computed in exponential time, and the subsumption between them can be checked in
polynomial time (relative to their size, which can be exponential). If 	 is the LGG then 	 
IG(IS(	1 ) [ IS(	2 )), so that no such certificates will exist. 2
4.3 Syntactic Subsumption and Syntactic Least-General Generalization.
Given the intractability results for semantic AMA subsumption, we now introduce a tractable generality notion, syntactic subsumption, and discuss the corresponding LGG problem. The use of
syntactic forms of generality for efficiency is familiar in ILP (Muggleton & De Raedt, 1994)
where, for example,  -subsumption is often used in place of the entailment generality relation.
Unlike AMA semantic subsumption, syntactic subsumption requires checking only polynomially
many MA subsumptions, each in polynomial time (via Theorem 9).
Definition 5. AMA 	1 is syntactically subsumed by AMA 	2 (written 	1
timeline 2 2 	2 , there is an MA timeline 1 2 	1 such that 1  2 .

syn 	2) iff for each MA

Proposition 20. AMA syntactic subsumption can be decided in polynomial time.
Syntactic subsumption trivially implies semantic subsumptionhowever, the converse does not
hold in general. Consider the AMA formulas (A; B ) ^ (B ; A), and A; B ; A where A and B are
primitive propositions. We have (A; B ) ^ (B ; A)  A; B ; A; however, we have neither A; B 
A; B ; A nor B ; A  A; B ; A, so that A; B ; A does not syntactically subsume (A; B ) ^ (B ; A).
Syntactic subsumption fails to recognize constraints that are only derived from the interaction of
timelines within a formula.
Syntactic Least-General Generalization. A syntactic AMA LGG is a syntactically least-general
AMA formula that syntactically subsumes the input AMA formulas. Here, least means that no
404

fiL EARNING T EMPORAL E VENTS

formula properly syntactically subsumed by a syntactic LGG can syntactically subsume the input
formulas. Based on the hardness gap between syntactic and semantic AMA subsumption, one might
conjecture that a similar gap exists between the syntactic and semantic LGG problems. Proving such
a gap exists requires closing the gap between the lower and upper bounds on AMA LGG shown in
Theorem 14 in favor of the upper bound, as suggested by Conjecture 18. While we cannot yet
show a hardness gap between semantic and syntactic LGG, we do give a syntactic LGG algorithm
that is exponentially more efficient than the best semantic LGG algorithm we have found (that of
Theorem 14). First, we show that syntactic LGGs exist and are unique up to mutual syntactic
subsumption (and hence up to semantic equivalence).
Theorem 21. There exists a syntactic LGG for any AMA formula set  that is syntactically subsumed by all syntactic generalizations of .
Proof: Let 	 be the conjunction of all the MA timelines that syntactically generalize  while
having size no larger than . As in the proof of Theorem 12, 	 is well defined. We show that
	 is a syntactic LGG for . First, note that 	 syntactically generalizes  because each timeline
of 	 generalizes a timeline in every member of , by the choice of 	. Now consider an arbitrary
syntactic generalization 	0 of . By the definition of syntactic subsumption, each timeline  in
	0 must subsume some timeline ff in each member ff of . Lemma 5 then implies that there is a
timeline 0 of size no larger than  that subsumes all the ff while being subsumed by . By our
choice of 	, the timeline 0 must be a timeline of 	. It follows then that 	0 syntactically subsumes
	, and that 	 is a syntactic LGG of  subsumed by all other syntactic generalizations of . 2
In general, we know that semantic and syntactic LGGs are different, though clearly the syntactic
LGG is a semantic generalization and so must subsume the semantic LGG. For example, (A; B ) ^
(B ; A), and A; B ; A have a semantic LGG of A; B ; A, as discussed above; but their syntactic LGG
is (A; B ; true) ^ (true; B ; A), which subsumes A; B ; A but is not subsumed by A; B ; A. Even
so, for MA formulas:
Proposition 22.

For MA  and AMA 	,  syn

	 is equivalent to   	.

Proof: The forward direction is immediate since we already know syntactic subsumption implies
semantic subsumption. For the reverse direction, note that   	 implies that each timeline of 	
subsumes thus since  is a single timeline each timeline in 	 subsumes some timeline in 
which is the definition of syntactic subsumption. 2
Proposition 23.

Any syntactic AMA LGG for an MA formula set  is also a semantic LGG for .

Proof: Now, consider a syntactic LGG 	 for . Proposition 22 implies that 	 is a semantic
generalization of . Consider any semantic LGG 	0 of . We show that 	  	0 to conclude that 	
is a semantic LGG for . Proposition 22 implies that 	0 syntactically subsumes . It follows that
	0 ^ 	 syntactically subsumes . But, 	0 ^ 	 is syntactically subsumed by 	, which is a syntactic
LGG of it follows that 	0 ^ 	 syntactically subsumes 	, or 	 would not be a least syntactic
generalization of . But then 	  (	0 ^ 	), which implies 	  	0 , as desired. 2
We note that the stronger result stating that a formula 	 is a syntactic LGG of a set  of MA formulas if and only if it is a semantic LGG of  is not an immediate consequence of our results above. At
405

fiF ERN , G IVAN , & S ISKIND

first examination, the strengthening appears trivial, given the equivalence of   	 and  syn 	
for MA . However, being semantically least is not necessarily a stronger condition than being syntactically leastwe have not ruled out the possibility that a semantically least generalization 	 may
syntactically subsume another generalization that is semantically (but not syntactically) equivalent.
(This question is open, as we have not found an example of this phenomenon either.)
Proposition 23 together with Theorem 21 have the nice consequence for our learning approach
that the syntactic LGG of two AMA formulas is a semantic LGG of those formulas, as long as the
original formulas are themselves syntactic LGGs of sets of MA timelines. Because our learning approach starts with training examples that are converted to MA timelines using the LGCF operation,
the syntactic LGGs computed (whether combining all the training examples at once, or incrementally computing syntactic LGGs of parts of the training data) are always syntactic LGGs of sets of
MA timelines and hence are also semantic LGGs, in spite of the fact that syntactic subsumption is
weaker than semantic subsumption. We note, however, that the resulting semantic LGGs may be
considerably larger than the smallest semantic LGG (which may not be a syntactic LGG at all).
Using Proposition 23, we now show that we cannot hope for a polynomial-time syntactic LGG
algorithm.
Theorem 24.

The smallest syntactic LGG of two MA formulas can be exponentially large.

Proof: Suppose there is always a syntactic LGG of two MA formulas that is not exponentially large.
Since by Proposition 23 each such formula is also a semantic LGG, there is always a semantic LGG
of two MA formulas that is not exponentially large. This contradicts Theorem 17. 2
While this is discouraging, we have an algorithm for the syntactic LGG whose time complexity
matches this lower-bound, unlike the semantic LGG case, where the best algorithm we have is
doubly exponential in the worst case. Theorem 14 yields an exponential time method for computing
the semantic LGG of a set of MA timelines since for a timeline , IS() = , we can simply
conjoin all the timelines of IG(). Given a set of AMA formulas, the syntactic LGG algorithm uses
this method to compute the polynomially-many semantic LGGs of sets of timelines, one chosen
from each input formula, and conjoins all the results.
Theorem 25.
	1 ; : : : ; 	n .

The formula

 2	 IG(f1 ; : : : ; n g) is a syntactic LGG of the AMA formulas

V

i

i

Proof: Let 	 be i 2	i IG(f1 ; : : : ; n g). Each timeline  of 	 must subsume each 	i because
 is an output of IG on a set containing a timeline of 	i thus 	 syntactically subsumes each 	i .
To show that 	 is a syntactically least such formula, consider a 	0 that syntactically subsumes every
	i . We show that 	 syn 	0 to conclude. Each timeline 0 in 	0 subsumes a timeline Ti 2 	i ,
for each i, by our assumption that 	i syn 	0 . But then by Lemma 5, 0 must subsume a member
of IG(fT1 ; : : : ; Tn g)and that member is a timeline of 	so each timeline 0 of 	0 subsumes a
timeline of 	. We conclude 	 syn 	0 , as desired. 2
V

This theorem yields an algorithm that computes a syntactic AMA LGG in exponential time
pseudo-code for this method is given in Figure 9. The exponential time bound follows from the fact
that there are exponentially many ways to choose 1 ; : : : ; m in line 5, and for each of these there
are exponentially many semantic-LGG members in line 6 (since the i are all MA timelines)the
product of these two exponentials is still an exponential.
406

fiL EARNING T EMPORAL E VENTS

1:
2:
3:
4:
5:
6:

syntactic-LGG(f	1 ; 	2 ; : : : ; 	m g)

// Input: AMA formulas f	1 ; : : : ; 	m g
// Output: syntactic LGG of f	1 ; : : : ; 	m g

G := fg;

for each h1 ; : : : ; m i 2 	1      	m

for each  in semantic-LGG(f1 ; : : : ; m g)

7:
8:
9:
10:

V

return (

if (80 2 G : 0 6 )
then G0 := f00 2 G j   00 g;
G := (G G0 ) [ fg;

G)

Figure 9: Pseudo-code that computes the syntactic AMA LGG of a set of AMA formulas.
The formula returned by the algorithm shown is actually a subset of the syntactic LGG given
by Theorem 25. This subset is syntactically (and hence semantically) equivalent to the formula
specified by the theorem, but is possibly smaller due to the pruning achieved by the if statement in
lines 79. A timeline is pruned from the set if it is (semantically) subsumed by any other timeline in
the set (one timeline is kept from any semantically equivalent group of timelines, at random). This
pruning of timelines is sound, since a timeline is pruned from the output only if it subsumes some
other formula in the outputthis fact allows an easy argument that the pruned formula is syntactically equivalent to (i.e. mutually syntactically subsumed by) the unpruned formula. Section 4.4.2
traces the computations of this algorithm for an example LGG calculation. We note that in our empirical evaluation discussed in Section 6, there was no cost in terms of accuracy for using the more
efficient syntactic vs. semantic LGG. We know this because our learned definitions made errors in
the direction of being overly specificthus, since the semantic-LGG is at least as specific as the
syntactic-LGG there would be no advantage to using the semantic algorithm.
The method does an exponential amount of work even if the result is small (typically because
many timelines can be pruned from the output because they subsume what remains). It is still an
open question as to whether there is an output-efficient algorithm for computing the syntactic AMA
LGGthis problem is in coNP and we conjecture that it is coNP-complete. One route to settling
this question is to determine the output complexity of semantic LGG for MA input formulas. We
believe that problem also to be coNP-complete, but have not proven this; if that problem is in P,
there is an output-efficient method for computing syntactic AMA LGG based on Theorem 25.
A summary of the algorithmic complexity results from this section can be found in Table 3 in
the conclusions section of this paper.
4.4 Examples: Least-General Generalization Calculations
Below we work through the details of a semantic and a syntactic LGG calculation. We consider the
AMA formulas 	 = (A; B ) ^ (B ; A) and  = A; B ; A, for which the semantic LGG is A; B ; A
and the syntactic LGG is (A; B ; true) ^ (true; B ; A).

407

fiF ERN , G IVAN , & S ISKIND

4.4.1 S EMANTIC LGG E XAMPLE
The first step in calculating the semantic LGG, according to the algorithm given in Figure 8, is to
compute the interdigitation-specializations of the input formulas (i.e., IS() and IS(	)). Trivially,
we have that IS() =  = A; B ; A. To calculate IS(	), we must consider the possible interdigitations of 	, for which there are three,

f hA; B i ; hB; B i ; hB; Ai g
f hA; B i ; hB; Ai g
f hA; B i ; hA; Ai ; hB; Ai g
Each interdigitation leads to the corresponding member of IS(	) by unioning (conjoining) the states
in each tuple, so IS(	) is

f (A ^ B ); B ; (A ^ B );
(A ^ B );
(A ^ B ); A; (A ^ B ) g:
Lines 59 of the semantic LGG algorithm compute the set S , which is equal to the union of the
timelines in IS(	) and IS(), with all subsumed timelines removed. For our formulas, we see that
each timeline in IS(	) is subsumed by thus, we have that S =  = A; B ; A.
After computing S , the algorithm returns the conjunction of timelines in IG(S ), with redundant
timelines removed (i.e., all subsuming timelines are removed). In our case, IG(S ) = A; B ; A,
trivially, as there is only one timeline in S , thus the algorithm correctly computes the semantic LGG
of 	 and  to be A; B ; A.
4.4.2 S YNTACTIC LGG E XAMPLE
The syntactic LGG algorithm, shown in Figure 9, computes a series of semantic LGGs for MA
timeline sets, returning the conjunction of the results (after pruning). Line 5 of the algorithm, cycles
through timeline tuples from the cross-product of the input AMA formulas. In our case the tuples
in   	 are T1 = hA; B ; A; A; B i and T2 = hA; B ; A; B ; Aifor each tuple, the algorithm
computes the semantic LGG of the tuples timelines.
The semantic LGG computation for each tuple uses the algorithm given in Figure 8, but the
argument is always a set of MA timelines rather than AMA formulas. For this reason, lines 4
9 are superfluous, as for an MA timeline 0 , IS(0 ) = 0 . In the case of tuple T1 , lines 49
of the algorithm just compute S = fA; B ; A; A; B g. It remains to compute the interdigitationgeneralizations of S (i.e., IG(S )), returning the conjunction of those timelines after pruning (lines
1015 in Figure 8). The set of all interdigitations of S are,

f hA; Ai ; hB; Ai ; hB; B i ; hB; Ai g
f hA; Ai ; hB; B i ; hB; Ai g
f hA; Ai ; hA; B i ; hB; B i ; hB; Ai g
f hA; Ai ; hA; B i ; hB; Ai g
f hA; Ai ; hA; B i ; hA; Ai ; hB; Ai g
By intersecting states in interdigitation tuples we get IG(S ),

f A; true; B ; true; A; B ; true; A; true; B ; true; A; true; true; A; true; A; true g
408

fiL EARNING T EMPORAL E VENTS

Since the timeline A; B ; true is subsumed by all timelines in IG(S ), all other timelines will be
pruned. Thus the semantic LGG algorithm returns A; B ; true as the semantic LGG of the timelines
in T1 .
Next the syntactic LGG algorithm computes the semantic LGG of the timelines in T2 . Following
the same steps as for T1 , we find that the semantic LGG of the timelines in T2 is true; B ; A. Since
A; B ; true and true; B ; A do not subsume one another, the set G computed by lines 59 of the
syntactic LGG algorithm is equal to f A; B ; true; true; B ; A g. Thus, the algorithm computes the
syntactic LGG of  and 	 to be (A; B ; true) ^ (true; B ; A). Note that, in this case, the syntactic
LGG is more general than the semantic LGG.

5. Practical Extensions
We have implemented a specific-to-general AMA learning algorithm based on the LGCF and syntactic LGG algorithms presented earlier. This implementation includes four practical extensions.
The first extension aims at controlling the exponential complexity by limiting the length of the
timelines we consider. Second we describe an often more efficient LGG algorithm based on a
modified algorithm for computing pairwise LGGs. The third extension deals with applying our
propositional algorithm to relational data, as is necessary for the application domain of visual event
recognition. Fourth, we add negation into the AMA language and show how to compute the corresponding LGCFs and LGGs using our algorithms for AMA (without negation). Adding negation
into AMA turns out to be crucial to achieving good performance in our experiments. We end this
section with a review of the overall complexity of our implemented system.
5.1 k-AMA Least-General Generalization
We have already indicated that our syntactic AMA LGG algorithm takes exponential time relative
to the lengths of the timelines in the AMA input formulas. This motivates restricting the AMA
language to k -AMA in practice, where formulas contain timelines with no more than k states.
As k is increased the algorithm is able to output increasingly specific formulas at the cost of an
exponential increase in computational time. In the visual-eventrecognition experiments shown
later, as we increased k , the resulting formulas became overly specific before a computational bottleneck is reachedi.e., for that application the best values of k were practically computable and the
ability to limit k provided a useful language bias.
We use a k -cover operator in order to limit our syntactic LGG algorithm to k -AMA. A k -cover
of an AMA formula is a syntactically least general k -AMA formula that syntactically subsumes
the inputit is easy to show that a k -cover for a formula can be formed by conjoining all k -MA
timelines that syntactically subsume the formula (i.e., that subsume any timeline in the formula) .
Figure 10 gives pseudo-code for computing the k -cover of an AMA formula. It can be shown that
this algorithm correctly computes a k -cover for any input AMA formula. The algorithm calculates
the set of least general k -MA timelines that subsume each timeline in the inputthe resulting k -MA
formulas are conjoined and redundant timelines are pruned using a subsumption test. We note that
the k -cover of an AMA formula may itself be exponentially larger than that formula; however, in
practice, we have found k -covers not to exhibit undue size growth.
Given the k -cover algorithm we restrict our learner to k -AMA as follows: 1) Compute the
k-cover for each AMA input formula. 2) Compute the syntactic AMA LGG of the resulting kAMA formulas. 3) Return the k -cover of the resulting AMA formula. The primary bottleneck of
409

fiF ERN , G IVAN , & S ISKIND

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

12:
13:
14:
15:
17:
18:
19:
20:

V

k-cover(k; 1im i )
V
// Input: positive natural number k , AMA formula 1im i
V
// Output: k -cover of 1im i
G := fg;
for i := 1 to m

:= hP1 ; : : : ; Pn i in all-values(a-k-partition (k; i ))
T
T
 := ( P1 ); : : : ; ( Pn );
if (80 2 G : 0 6 )
then G0 := f00 2 G j   00 g;
G := (G G0 ) [ fg;
V
return ( G)
for each P

a-k-partition (k; s1 ; : : : ; sj )

// Input: positive natural number k , MA timeline s1 ; : : : ; sj
// Output: a tuple of  k sets of consecutive states that partitions s1 ; : : : ; sj

 k then return hfs1g; : : : ; fsj gi;
if k = 1 then return hfs1 ; : : : ; sj gi;
l := a-member-of(f1; 2; : : : ; j k + 1g);
P0 = fs1 ; : : : ; sl g;
if j

return extend-tuple (P0 ; a-k-partition (k

// pick next block size
// construct next block

1; sl+1 ; : : : ; sj ));

Figure 10: Pseudo-code for non-deterministically computing a k-cover of an AMA formula, along
with a non-deterministic helper function for selecting a  k block partition of the states
of a timeline.

the original syntactic LGG algorithm is computing the exponentially large set of interdigitationgeneralizationsthe k -limited algorithm limits this complexity as it only computes interdigitationgeneralizations involving k -MA timelines.
5.2 Incremental Pairwise LGG Computation
Our implemented learner computes the syntactic k-AMA LGG of AMA formula setshowever,
it does not directly use the algorithm describe above. Rather than compute the LGG of formula
sets via a single call to the above algorithm, it is typically more efficient to break the computation
into a sequence of pairwise LGG calculations. Below we describe this approach and the potential
efficiency gains.
It is straightforward to show that for both syntactic and semantic subsumption we have that
LGG(	1 ; : : : ; 	m ) = LGG(	1 ; LGG(	2 ; : : : ; 	m )) where the 	i are AMA formulas. Thus, by
recursively applying this transformation we can incrementally compute the LGG of m AMA formulas via a sequence of m 1 pairwise LGG calculations. Note that since the LGG operator is
410

fiL EARNING T EMPORAL E VENTS

commutative and associative the final result does not depend on the order in which we process the
formulas. We will refer to this incremental pairwise LGG strategy as the incremental approach and
to the strategy that makes a single call to the k-AMA LGG algorithm (passing in the entire formula
set) as the direct approach.
To simplify the discussion we will consider computing the LGG of an MA formula set the
argument can be extended easily to AMA formulas (and hence to k-AMA). Recall that the syntactic
LGG algorithm of Figure 9 computes LGG() by conjoining timelines in IG() that do not subsume any of the others, eliminating subsuming timelines in a form of pruning. The incremental
approach applies this pruning step after each pair of input formulas is processedin contrast, the
direct approach must compute the interdigitation-generalization of all the input formulas before any
pruning can happen. The resulting savings can be substantial, and typically more than compensates
for the extra effort spent checking for pruning (i.e. testing subsumption between timelines as the
incremental LGG is computed). A formal approach to describing these savings can be constructed
S
S
based on the observation that both 2IG(f1 ;2 g) IG(fg[ ) and 2LGG(1 ;2 ) IG(fg[ )
can be seen to compute the LGG of  [ f1 ; 2 g, but with the latter being possibly much cheaper
to compute due to pruning. That is, LGG(1 ; 2 ) typically contains a much smaller number of
timelines than IG(f1 ; 2 g).
Based on the above observations our implemented system uses the incremental approach to
compute the LGG of a formula set. We now describe an optimization used in our system to speedup
the computation of pairwise LGGs, compared to directly running the algorithm in Figure 9. Given a
pair of AMA formulas 	1 = 1;1 ^    ^ 1;m and 	2 = 2;1 ^    ^ 2;n , let 	 be their syntactic
LGG obtained by running the algorithm in Figure 9. The algorithm constructs 	 by computing
LGGs of all MA timeline pairs (i.e., LGG(1;i ; 2;j ) for all i and j ) and conjoining the results
while removing subsuming timelines. It turns out that we can often avoid computing many of these
MA LGGs. To see this consider the case when there exists i and j such that 1;i  2;j , we know
LGG(1;i ; 2;j ) = 2;j which tells us that that 2;j will be considered for inclusion into 	 (it may
be pruned). Furthermore we know that any other LGG involving 2;j will subsume 2;j and thus
will be pruned from 	. This shows that we need not compute any MA LGGs involving 2;j , rather
we need only to consider adding 2;j when constructing 	.
The above observation leads to a modified algorithm (used in our system) for computing the
syntactic LGG of a pair of AMA formulas. The new algorithm only computes LGGs between
non-subsuming timelines. Given AMA formulas 	1 and 	2 , the modified algorithm proceeds as
follows: 1) Compute the subsumer set S = f 2 	1 j 90 2 	2 s:t: 0  g [ f 2 	2 j 90 2
	1 s:t: 0  g. 2) Let AMA 	01 (	02 ) be the result of removing timelines from 	1 (	2 ) that are
in S . 3) Let 	0 be the syntactic LGG of 	01 and 	02 computed by running the algorithm in Figure 9
(if either 	0i is empty then 	0 will be empty). 4) Let S 0 be the conjunction of timelines in S that do
not subsume any timeline in 	0 . 5) Return 	 = 	0 ^ S 0 . This method avoids computing MA LGGs
involving subsuming timelines (an exponential operation) at the cost of performing polynomially
many MA subsumption tests (a polynomial operation). We have noticed a significant advantage to
using this procedure in our experiments. In particular, the advantage tends to grow as we process
more training examples. This is due to the fact that as we incrementally process training examples
the resulting formulas become more generalthus, these more general formulas are likely to have
more subsuming timelines. In the best case when 	1 syn 	2 (i.e., all timelines in 	2 are subsuming), we see that step 2 produces an empty formula and thus step 3 (the expensive step) performs no
workin this case we return the set S = 	2 as desired.
411

fiF ERN , G IVAN , & S ISKIND

5.3 Relational Data
L EONARD produces relational models that involve objects and (force dynamic) relations between
those objects. Thus event definitions include variables to allow generalization over objects. For
example, a definition for P ICK U P (x; y; z ) recognizes both P ICK U P (hand; block; table) as well as
P ICK U P (man; box; floor). Despite the fact that our k -AMA learning algorithm is propositional, we
are still able to use it to learn relational definitions.
We take a straightforward object-correspondence approach to relational learning. We view the
models output by L EONARD as containing relations applied to constants. Since we (currently)
support only supervised learning, we have a set of distinct training examples for each event type.
There is an implicit correspondence between the objects filling the same role across the different training models for a given type. For example, models showing P ICK U P (hand; block; table)
and P ICK U P (man; box; floor) have implicit correspondences given by hhand; mani, hblock; boxi,
and htable; floori. We outline two relational learning methods that differ in how much objectcorrespondence information they require as part of the training data.
5.3.1 C OMPLETE O BJECT C ORRESPONDENCE
This first approach assumes that a complete object correspondence is given, as input, along with
the training examples. Given such information, we can propositionalize the training models by
replacing corresponding objects with unique constants. The propositionalized models are then given
to our propositional k -AMA learning algorithm which returns a propositional k -AMA formula. We
then lift this propositional formula by replacing each constant with a distinct variable. Lavrac et al.
(1991) has taken a similar approach.
5.3.2 PARTIAL O BJECT C ORRESPONDENCE
The above approach assumes complete object-correspondence information. While it is sometimes
possible to provide all correspondences (for example, by color-coding objects that fill identical
roles when recording training movies), such information is not always available. When only a
partial object correspondence (or even none at all) is available, we can automatically complete the
correspondence and apply the above technique.
For the moment, assume that we have an evaluation function that takes two relational models
and a candidate object correspondence, as input, and yields an evaluation of correspondence quality. Given a set of training examples with missing object correspondences, we perform a greedy
search for the best set of object-correspondence completions over the models. Our method works
by storing a set P of propositionalized training examples (initially empty) and a set U of unpropositionalized training examples (initially the entire training set). For the first step, when P is empty, we
evaluate all pairs of examples from U , under all possible correspondences, select the pair that yields
the highest score, remove the examples involved in that pair from U , propositionalize them according to the best correspondence, and add them to P . For each subsequent step, we use the previously
computed values of all pairs of examples, one from U and one from P , under all possible correspondences. We then select the example from U and correspondence that yields the highest average
score relative to all models in P this example is removed from U , propositionalized according to
the winning correspondence, and added to P . For a fixed number of objects, the effort expended
here is polynomial in the size of the training set; however, if the number of objects b that appear in a
training example is allowed to grow, the number of correspondences that must be considered grows
412

fiL EARNING T EMPORAL E VENTS

as bb . For this reason, it is important that the events involved manipulate only a modest number of
objects.
Our evaluation function is based on the intuition that object roles for visual events (as well as
events from other domains) can often be inferred by considering the changes between the initial
and final moments of an event. Specifically, given two models and an object correspondence, we
first propositionalize the models according to the correspondence. Next, we compute ADD and
DELETE lists for each model. The ADD list is the set of propositions that are true at the final
moment but not the initial moment. The DELETE list is the set of propositions that are true at the
initial moment but not the final moment. These add and delete lists are motivated by STRIPS action
representations (Fikes & Nilsson, 1971). Given such ADDi and DELETEi lists for models 1 and 2,
the evaluation function returns the sum of the cardinalities of ADD1 \ ADD2 and DELETE1 \
DELETE2 . This heuristic measures the similarity between the ADD and DELETE lists of the two
models. The intuition behind this heuristic is similar to the intuition behind the STRIPS actiondescription languagei.e., that most of the differences between the initial and final moments of an
event occurrence are related to the target event, and that event effects can be described by ADD and
DELETE lists. We have found that this evaluation function works well in the visual-event domain.
Note, that when full object correspondences are given to the learner (rather than automatically
extracted by the learner), the training examples are interpreted as specifying that the target event
took place as well as which objects filled the various event roles (e.g., P ICK U P (a,b,c)). Rather,
when no object correspondences are provided the training examples are interpreted as specifying the
existence of a target event occurrence but do not specify which objects fill the roles (i.e., the training
example is labeled by P ICK U P rather than P ICK U P (a,b,c)). Accordingly, the rules learned when no
correspondences are provided only allow us to infer that a target event occurred and not which
objects filled the event roles. For example when object correspondences are manually provided the
learner might produce the rule,
"

4 (S UPPORTS (z; y) ^ C ONTACTS (z; y));
P ICK U P (x; y; z ) =
(S UPPORTS (x; y) ^ ATTACHED (x; y))

#

whereas a learner that automatically extracts the correspondences would instead produce the rule,
"

4 (S UPPORTS (z; y) ^ C ONTACTS (z; y));
P ICK U P =
(S UPPORTS (x; y) ^ ATTACHED (x; y))

#

Its worth noting, however, that upon producing the second rule the availability of a single training
example with correspondence information allows the learner to determine the roles of the variables,
upon which it can output the first rule. Thus, under the assumption that the learner can reliably
extract object correspondences, we need not label all training examples with correspondence information in order to obtain definitions that explicitly recognize object roles.
5.4 Negative Information
The AMA language does not allow negated propositions. Negation, however, is sometimes necessary to adequately define an event type. In this section, we consider the language AMA , which is a
superset of AMA, with the addition of negated propositions. We first give the syntax and semantics
of AMA , and extend AMA syntactic subsumption to AMA . Next, we describe our approach to
413

fiF ERN , G IVAN , & S ISKIND

learning AMA formulas using the above-presented algorithms for AMA. We show that our approach correctly computes the AMA LGCF and the syntactic AMA LGG. Finally, we discuss
an alternative, related approach to adding negation designed to reduce the overfitting that appears to
result from the full consideration of negated propositions.
AMA has the same syntax as AMA, only with a new grammar for building states with negated
propositions:
literal
state

::= true j prop j :3prop
::= literal j literal ^ state

where prop is any primitive proposition. The semantics of AMA
for state satisfaction.

are the same as for AMA except



A positive literal P (negative literal
true (false), for every x 2 I .10

:3P ) is satisfied by model hM; I i iff M [x] assigns P



A state l1 ^    ^ lm is satisfied by model hM; I i iff each literal li is satisfied by hM; I i.

Subsumption. An important difference between AMA and AMA is that Proposition 2, establishing the existence of witnessing interdigitations to MA subsumption, is no longer true for MA .
In other words, if we have two timelines 1 ; 2 2 AMA , such that 1  2 , there need not be an
interdigitation that witnesses 1  2 . To see this, consider the AMA timelines:

1 = (a ^ b ^ c); b; a; b; (a ^ b ^ :  c)
2 = b; a; c; a; b; a; :  c; a; b
We can then argue:
1. There is no interdigitation that witnesses 1  2 . To see this, first show that, in any such
witness, the second and fourth states of 1 (each just b) must interdigitate to align with
either the first and fifth, or the fifth and ninth states of 2 (also, each just b). But in either
of these cases, the third state of 1 will interdigitate with states of 2 that do not subsume it.
2. Even so, we still have that 1  2 . To see this, consider any model hM; I i that satisfies 1 .
There must be an interval [i1 ; i2 ] within I such that hM; [i1 ; i2 ]i satisfies the third state of 1 ,
that is the state a. We have two cases:
(a) The proposition c is true at some point in hM; [i1 ; i2 ]i. Then, one can verify that hM; I i
satisfies both 1 and 2 in the following alignment:

1
2

=
=

(a ^ b ^ c); b;
b;

a;
a; c; a;

b;
b;

(a ^ b ^ :  c)
a; :  c; a; b

10. We note that it is important that we use the notation :3P rather than just :P . In event-logic, the formula :P
is satisfied by a model whenever P is false as some instant in the model. Rather, event-logic interprets :3P as
indicating that P is never true in the model (as defined above). Notice that the first form of negation does not yield a
liquid propertyi.e., :P can be true along an interval but not necessarily during all subintervals. The second form of
negation, however, does yield a liquid property provided that P is liquid. This is important to our learning algorithms,
since they all assume states are built from liquid properties.

414

fiL EARNING T EMPORAL E VENTS

(b) The proposition c is false everywhere in hM; [i1 ; i2 ]i. Then, one can verify that hM; I i
satisfies both 1 and 2 in the following alignment:

1 =
(a ^ b ^ c);
2 =
b; a; c; a;
It follows that 1  2 .

b;
b;

a;
a; :  c; a;

b; (a ^ b ^ :  c)
b

In light of such examples, we conjecture that it is computationally hard to compute AMA
subsumption even between timelines. For this reason, we extend our definition of syntactic subsumption to AMA in a way that provides a clearly tractable subsumption test analogous to that
discussed above for AMA.
Definition 6. AMA 	1 is syntactically subsumed by AMA 	2 (written 	1 syn 	2 ) iff for
each timeline 2 2 	2 , there is a timeline 1 2 	1 such that there is a witnessing interdigitation
for 1  2 .
The difference between the definition here and the previous one for AMA is that here we only need
to test for witnessing interdigitations between timelines rather than subsumption between timelines.
For AMA formulas, we note that the new and old definition are equivalent (due to Proposition 2);
however, for AMA the new definition is weaker, and will result in more general LGG formulas. As
one might expect, AMA syntactic subsumption implies semantic subsumption and can be tested
in polynomial-time using the subsumption graph described in Lemma 8 to test for witnesses.
Learning. Rather than design new LGCF and LGG algorithms to directly handle AMA , we
instead compute these functions indirectly by applying our algorithms for AMA to a transformed
problem. Intuitively, we do this by adding new propositions to our models (i.e., the training examples) that represent the proposition negations. Assume that the training-example models are over the
set of propositions P = fp1 ; : : : ; pn g. We introduce a new set P = fp1 ; : : : ; pn g of propositions
and use these to construct new training models over P [ P by assigning true to pi at a time in a
model iff pi is false in the model at that time. After forming the new set of training models (each
with twice as many propositions as the original models) we compute the least general AMA formula
that covers the new models (by computing the AMA LGCFs and applying the syntactic AMA LGG
algorithm), resulting in an AMA formula 	 over the propositions P [ P . Finally we replace each pi
in 	 with :3pi resulting in an AMA formula 	0 over propositions in P it turns out that under
syntactic subsumption 	0 is the the least general AMA formula that covers the original training
models.
We now show the correctness of the above transformational approach to computing the AMA
LGCF and syntactic LGG. First, we introduce some notation. Let M be the set of all models over
P . Let M be the set of models over P [ P , such that at any time, for each i, exactly one of pi
and pi is true. Let T be the following mapping from M to M: for hM; I i 2 M, T [hM; I i] is the
unique hM 0 ; I i 2 M such that for all j 2 I and all i, M 0 (j ) assigns pi true iff M (j ) assigns pi
true. Notice that the inverse of T is a functional mapping from M to M. Our approach to handling
negation using purely AMA algorithms begins by applying T to the original training models. In
what follows, we consider AMA formulas over the propositions in P , and AMA formulas over
the propositions in P [ P .
Let F be a mapping from AMA to AMA where for 	 2 AMA , F [	] is an AMA formula
identical to 	 except that each :3pi in 	 is replaced with pi . Notice that the inverse of F is a func415

fiF ERN , G IVAN , & S ISKIND

tion from AMA to AMA and corresponds to the final step in our approach described above. The
following lemma shows that there is a one-to-one correspondence between satisfaction of AMA
formulas by models in M and satisfaction of AMA formulas by models in M.
Lemma 26. For any model hM; I i 2 M and any 	 2 AMA ,
T [hM; I i].

	 covers hM; I i

iff

F [	] covers

Using this lemma, it is straightforward to show that our transformational approach computes the
AMA LGCF under semantic subsumption (and hence under syntactic subsumption).
Proposition 27.

For any

hM; I i 2 M, let  be the AMA LGCF of the model T [hM; I i].
LGCF of hM; I i, up to equivalence.

F 1 [] is the unique AMA

Then,

Proof: We know that  covers T [hM; I i], therefore by Lemma 26 we know that F 1 [] covers
hM; I i. We now show that F 1[] is the least-general formula in AMA that covers hM; I i. For
the sake of contradiction assume that some 0 2 AMA covers hM; I i but that 0 < F 1 []. It
follows that there is some model hM 0 ; I 0 i that is covered by F 1 [] but not by 0 . By Lemma 26
we have that F [0 ] covers T [hM; I i] and since  is the unique AMA LGCF of T [hM; I i], up to
equivalence, we have that   F [0 ]. However, we also have that T [hM 0 ; I 0 i] is covered by 
but not by F [0 ] which gives a contradiction. Thus, no such 0 can exist. It follows that  is an
AMA LGCF. The uniqueness of the AMA LGCF up to equivalence follows because AMA is
closed under conjunction; so that if there were any two non-equivalent LGCF formulas, they could
be conjoined to get an LGCF formula strictly less than one of them. 2
Below we use the fact that the F operator preserves syntactic subsumption. In particular, given
two MA timelines 1 ; 2 , it is clear that any witnessing interdigitation of 1  2 can be trivially
converted into a witness for F [1 ]  F [2 ] (and vice versa). Since syntactic subsumption is defined
in terms of witnessing interdigitations, it follows that for any 	1 ; 	2 2 AMA , (	1 syn 	2 ) iff
(F [	1 ] syn F [	2 ]). Using this property, it is straightforward to show how to compute the syntactic
AMA LGG using the syntactic AMA LGG algorithm.
Proposition 28.

For any AMA

formulas

	1 ; : : : ; 	m ,

let

	

fF [	1 ]; : : : ; F [	m ]g. Then, F 1[	] is the unique syntactic AMA

be the syntactic AMA LGG of
LGG of f	1 ; : : : ; 	m g.

Proof: We know that for each i, F [	i ] syn 	thus, since F 1 preserves syntactic subsumption,
we have that for each i, 	i syn F 1 [	]. This shows that F 1 [	] is a generalization of the inputs.
We now show that F 1 [	] is the least such formula. For the sake of contradiction assume that
F 1 [	] is not least. It follows that there must be a 	0 2 AMA such that 	0 <syn F 1 [	] and for
each i, 	i syn 	0 . Combining this with the fact that F preserves syntactic subsumption, we get
that F [	0 ] <syn 	 and for each i, F [	i ]  F [	0 ]. But this contradicts the fact that 	 is an LGG;
so we must have that F 1 [	] is a syntactic AMA LGG. As argued elsewhere, the uniqueness of
this LGG follows from the fact that AMA is closed under conjunction. 2
These propositions ensure the correctness of our transformational approach to computing the
syntactic LGG within AMA . For the case of semantic subsumption, the transformational approach
does not correctly compute the AMA LGG. To see this, recall that above we have given two timelines 1 ; 2 2 AMA , such that 1  2 , but there is no witnessing interdigitation. Clearly under
416

fiL EARNING T EMPORAL E VENTS

semantic subsumption, the AMA LGG of 1 and 2 is 2 . However, the semantic AMA LGG of
F [1 ] and F [2 ] is not F [2 ]. The reason for this is that since there is no witness to F [1 ]  F [2 ]
(and the F [i ] are MA timelines), we know by Proposition 2 that F [1 ] 6 F [2 ]. Thus, F [2 ]
cannot be returned as the AMA LGG, since it does not subsume both input formulasthis shows
that the transformational approach will not return 2 = F 1 [F [2 ]]. Here, the transformational
approach will produce an AMA formula that is more general than 2 .
On the computational side, we note that, since the transformational approach doubles the number of propositions in the training data, algorithms specifically designed for AMA may be more
efficient. Such algorithms might leverage the special structure of the transformed examples that our
AMA algorithms ignorein particular, that exactly one of pi or pi is true at any time.
Boundary Negation. In our experiments, we actually compare two methods for assigning truth
values to the pi propositions in the training data models. The first method, called full negation,
assigns truth values as described above, yielding the syntactically least-general AMA formula that
covers the examples. We found, however, that using full negation often results in learning overly
specific formulas. To help alleviate this problem, our second method places a bias on the use of
negation. Our choice of bias is inspired by the idea that, often, much of the useful information for
characterizing an event type is in its pre- and post-conditions. The second method, called boundary
negation, differs from full negation in that it only allows pi to be true in the initial and final moments
of a model (and then only if pi is false). pi must be false at all other times. That is, we only allow
informative negative information at the beginnings and ends of the training examples. We have
found that boundary negation provides a good trade-off between no negation (i.e., AMA), which
often produces overly general results, and full negation (i.e., AMA ), which often produces overly
specific and much more complicated results.
5.5 Overall Complexity and Scalability
We now review the overall complexity of our visual event learning component and discuss some
scalability issues. Given a training set of temporal models (i.e., a set of movies), our system does the
following: 1) Propositionalize the training models, translating negation as descried in Section 5.4.
2) Compute the LGCF of each propositional model. 3) Compute the k -AMA LGG of the LGCFs.
4) Return a lifted (variablized) version of the LGG. Steps two and four require little computational
overhead, being linear in the sizes of the input and output respectively. Steps one and three are
the computational bottlenecks of the systemthey encompass the inherent exponential complexity
arising from the relational and temporal problem structure.
Step One. Recall from Section 5.3.2 that our system allows the user to annotate training examples with object correspondence information. Our technique for propositionalizing the models was
shown to be exponential in the number of unannotated objects in a training example. Thus, our
system requires that the number of objects be relatively small or that correspondence information
be given for all but a small number of objects. Often the event class definitions we are interested
in do not involve a large number of objects. When this is true, in a controlled learning setting we
can manage the relational complexity by generating training examples with only a small number (or
zero) irrelevant objects. This is the case for all of the domains studied empirically in this paper.
In a less controlled setting, the number of unannotated objects may prohibit the use of our
correspondence techniquethere are at least three ways one might proceed. First, we can try to
417

fiF ERN , G IVAN , & S ISKIND

develop efficient domain-specific techniques for filtering objects and finding correspondences. That
is, for a particular problem it may be possible to construct a simple filter that removes irrelevant
objects from consideration and then to find correspondences for any remaining objects. Second, we
can provide the learning algorithm with a set of hand-coded first-order formulas, defining a set of
domain-specific features (e.g., in the spirit of Roth & Yih, 2001). These features can then be used
to propositionalize the training instances. Third, we can draw upon ideas from relational learning to
design a truly first-order version of the k -AMA learning algorithm. For example, one could use
existing first-order generalization algorithms to generalize relational state descriptions. Effectively
this approach pushes the object correspondence problem into the k -AMA learning algorithm rather
than treating it as a preprocessing step. Since it is well known that computing first-order LGGs can
be intractable (Plotkin, 1971), practical generalization algorithms retain tractability by constraining
the LGGs in various ways (e.g., Muggleton & Feng, 1992; Morales, 1997).
Step Three. Our system uses the ideas of Section 5.2 to speedup the k -AMA LGG computation
for a set of training data. Nevertheless, the computational complexity is still exponential in k thus,
in practice we are restricted to using relatively small values of k . While this restriction did not limit
performance in our visual event experiments, we expect that it will limit the direct applicability
of our system to more complex problems. In particular, many event types of interest may not
be adequately represented via k -AMA when k is small. Such event types, however, often contain
significant hierarchical structurei.e., they can be decomposed into a set of short sub-event types.
An interesting research direction is to consider using our k -AMA learner as a component of a
hierarchical learning systemthere it could be used to learn k -AMA sub-event types. We note
that our learner alone cannot be applied hierarchically because it requires liquid primitive events,
but learns non-liquid composite event types. Further work is required (and intended) to construct a
hierarchical learner based perhaps on non-liquid AMA learning.
Finally, recall that to compute the LGG of m examples, our system uses a sequence of m 1
pairwise LGG calculations. For a fixed k , each pairwise calculation takes polynomial time. However, since the size of a pairwise LGG can grow by at least a constant factor with respect to the
inputs, the worst-case time complexity of computing the sequence of m 1 pairwise LGGs is exponential in m. We expect that this worst case will primarily occur when the target event type does not
have a compact k -AMA representationin which case a hierarchical approach as described above
is more appropriate. When there is a compact representation, our empirical experience indicates
that such growth does not occurin particular, each pairwise LGG tends to yield significant pruning. For such problems, reasonable assumptions about the amount of pruning11 imply that the time
complexity of computing the sequence of m 1 pairwise LGGs is polynomial in m.

6. Experiments
6.1 Data Set
Our data set contains examples of 7 different event types: pick up, put down, stack, unstack, move,
assemble, and disassemble. Each of these involve a hand and two to three blocks. For a detailed
description and sample video sequences of these event types, see Siskind (2001). Key frames from
sample video sequences of these event types are shown in Figure 11. The results of segmentation,
11. In particular, assume that the size of a pairwise k-AMA LGG is usually bounded by the sizes of the k-covers of the
inputs.

418

fiL EARNING T EMPORAL E VENTS

tracking, and model reconstruction are overlaid on the video frames. We recorded 30 movies for
each of the 7 event classes resulting in a total of 210 movies comprising 11946 frames.12 We
replaced one assemble movie (assemble-left-qobi-04), with a duplicate copy of another (assembleleft-qobi-11) because of segmentation and tracking errors.
Some of the event classes are hierarchical in that occurrences of events in one class contain occurrences of events in one or more simpler classes. For example, a movie depicting a
M OVE (a; b; c; d) event (i.e. a moves b from c to d) contains subintervals where P ICK U P (a; b; c)
and P UT D OWN (a; b; d) events occur. In our experiments, when learning the definition of an event
class only the movies for that event class are used in training. We do not train on movies for other
event classes that may also depict an occurrence of the event class being learned as a subevent.
However, in evaluating the learned definitions, we wish to detect both the events that correspond to
an entire movie as well as subevents that correspond to portions of that movie. For example, given a
movie depicting a M OVE (a; b; c; d) event, we wish to detect not only the M OVE(a; b; c; d) event but
also the P ICK U P (a; b; c) and P UT D OWN (a; b; d) subevents as well. For each movie type in our data
set, we have a set of intended events and subevents that should be detected. If a definition does not
detect an intended event, we deem the error a false negative. If a definition detects an unintended
event, we deem the error a false positive. For example, if a movie depicts a M OVE(a; b; c; d) event,
the intended events are M OVE(a; b; c; d), P ICK U P (a; b; c), and P UT D OWN (a; b; c). If the definition
for pick up detects the occurrence of P ICK U P (c; b; a) and P ICK U P (b; a; c), but not P ICK U P (a; b; c),
it will be charged two false positives as well as one false negative. We evaluate our definitions in
terms of false positive and negative rates as describe below.
6.2 Experimental Procedure
For each event type, we evaluate the k -AMA learning algorithm using a leave-one-movie-out crossvalidation technique with training-set sampling. The parameters to our learning algorithm are k
and the degree D of negative information used. The value of D is either P, for positive propositions
only, BN, for boundary negation, or N, for full negation. The parameters to our evaluation procedure
include the target event type E and the training-set size N . Given this information, the evaluation
proceeds as follows: For each movie M (the held-out movie) from the 210 movies, apply the k AMA learning algorithm to a randomly drawn training sample of N movies from the 30 movies of
event type E (or 29 movies if M is one of the 30). Use L EONARD to detect all occurrences of the
learned event definition in M . Based on E and the event type of M , record the number of false
positives and false negatives in M , as detected by L EONARD . Let FP and FN be the total number
of false positives and false negatives observed over all 210 held-out movies respectively. Repeat the
entire process of calculating FP and FN 10 times and record the averages as FP and FN.13
Since some event types occur more frequently in our data than others because simpler events
occur as subevents of more complex events but not vice versa, we do not report FP and FN directly.
Instead, we normalize FP by dividing by the total number of times L EONARD detected the target
event correctly or incorrectly within all 210 movies and we normalize FN by dividing by the total
12. The source code and all of the data used for these experiments are available as Online Appendix 1, and also from
ftp://ftp.ecn.purdue.edu/qobi/ama.tar.Z.
13. While we did not record the times for our experiments, the system is fast enough to give live demos when N = 29
and k = 3 with boundary negation, giving the best results we show here (though we dont typically record 29 training
videos in a live demo for other reasons). Some of the less favorable parameter settings (particularly k = 4 and full
negation) can take a (real-time) hour or so.

419

fiF ERN , G IVAN , & S ISKIND

pick up

put down

stack

unstack

move

assemble

disassemble

Figure 11: Key frames from sample videos of the 7 event types.

420

fiL EARNING T EMPORAL E VENTS

number of correct occurrences of the target event within all 210 movies (i.e., the human assessment
of the number of occurrences of the target event). The normalized value of FP estimates the probability that the target event did not occur given that it was predicted to occur, while the normalized
value of FN estimates the probability that the event was not predicted to occur given that it did
occur.
6.3 Results
To evaluate our k -AMA learning approach, we ran leave-one-movie-out experiments, as described
above, for varying k , D , and N . The 210 example movies were recorded with color-coded objects to
provide complete object-correspondence information. We compared our learned event definitions to
the performance of two sets of hand-coded definitions. The first set HD1 of hand-coded definitions
appeared in Siskind (2001). In response to subsequent deeper understanding of the behavior of
L EONARD s model-reconstruction methods, we manually revised these definitions to yield another
set HD2 of hand-coded definitions that gives a significantly better FN performance at some cost
in FP performance. Appendix C gives the event definitions in HD1 and HD2 along with a set of
machine-generated definitions, produced by the k -AMA learning algorithm, given all training data
for k = 30 and D = BN.
6.3.1 O BJECT C ORRESPONDENCE
To evaluate our algorithm for finding object correspondences, we ignored the correspondence information provided by color coding and applied the algorithm to all training models for each event
type. The algorithm selected the correct correspondence for all 210 training models. Thus, for this
data set, the learning results when no correspondence information is given will be identical to those
where the correspondences are manually provided, except that, in the first case, the rules will not
specify particular object roles, as discussed in section 5.3.2. Since our evaluation procedure uses
role information, the rest of our experiments use the manual correspondence information, provided
by color-coding, rather than computing it.
While our correspondence technique was perfect in these experiments, it may not be suited to
some event types. Furthermore, it is likely to produce more errors as noise levels increase. Since
correspondence errors represent a form of noise and our learner makes no special provisions for
handling noise, the results are likely to be poor when such errors are common. For example, in the
worst case, it is possible for a single extremely noisy example to cause the the LGG to be trivial (i.e.,
the formula true). In such cases, we will be forced to improve the noise tolerance of our learner.
6.3.2 VARYING k

The first three rows of Table 1 show the FP and FN values for all 7 event types for k 2 f2; 3; 4g ,
N = 29 (the maximum), and D = BN. Similar trends were found for D = P and D = N. The
general trend is that, as k increases, FP decreases or remains the same and FN increases or remains
the same. Such a trend is a consequence of our k -cover approach. This is because, as k increases,
the k -AMA language contains strictly more formulas. Thus for k1 > k2 , the k1 -cover of a formula
will never be more general than the k2 -cover. This strongly suggests, but does not prove, that FP
will be non-increasing with k and FN will be non-decreasing with k .
Our results show that 2-AMA is overly general for put down and assemble, i.e. it gives high
FP. In contrast, 3-AMA achieves FP = 0 for each event type, but pays a penalty in FN compared
421

fiF ERN , G IVAN , & S ISKIND

k D
2 BN

pick up

put down

stack

unstack

move

assemble

disassemble

FP
FN

0
0

0.14
0.19

0
0.12

0
0.03

0
0

0.75
0

0
0

3

BN

FP
FN

0
0

0
0.2

0
0.45

0
0.10

0
0.03

0
0.07

0
0.10

4

BN

FP
FN

0
0

0
0.2

0
0.47

0
0.12

0
0.03

0
0.07

0
0.17

3

P

FP
FN

0.42
0

0.5
0.19

0
0.42

0.02
0.11

0
0.03

0
0.03

0
0.10

3

BN

FP
FN

0
0

0
0.2

0
0.45

0
0.10

0
0.03

0
0.07

0
0.10

3

N

FP
FN

0
0.04

0
0.39

0
0.58

0
0.16

0
0.13

0
0.2

0
0.2

HD1

FP
FN

0.01
0.02

0.01
0.22

0
0.82

0
0.62

0
0.03

0
1.0

0
0.5

HD2

FP
FN

0.13
0.0

0.11
0.19

0
0.42

0
0.02

0
0.0

0
0.77

0
0.0

Table 1: FP and FN for learned definitions, varying both k and D , and for hand-coded definitions.
to 2-AMA. Since 3-AMA achieves FP = 0, there is likely no advantage in moving to k -AMA for
k > 3. That is, the expected result is for FN to become larger. This effect is demonstrated for
4-AMA in the table.
6.3.3 VARYING D

Rows four through six of Table 1 show FP and FN for all 7 event types for D 2 fP; BN; Ng, N = 29,
and k = 3. Similar trends were observed for other values of k . The general trend is that, as the
degree of negative information increases, the learned event definitions become more specific. In
other words, FP decreases and FN increases. This makes sense since, as more negative information
is added to the training models, more specific structure can be found in the data and exploited by
the k -AMA formulas. We can see that, with D = P, the definitions for pick up and put down are
overly general, as they produce high FP. Alternatively, with D = N, the learned definitions are
overly specific, giving FP = 0, at the cost of high FN. In these experiments, as well as others, we
have found that D = BN yields the best of both worlds: FP = 0 for all event types and lower FN
than achieved with D = N.
Experiments not shown here have demonstrated that, without negation for pick up and put down,
we can increase k arbitrarily, in an attempt to specialize the learned definitions, and never significantly reduce FP. This indicates that negative information plays a particularly important role in
constructing definitions for these event types.

422

fiL EARNING T EMPORAL E VENTS

6.3.4 C OMPARISON

TO

H AND -C ODED D EFINITIONS

The bottom two rows of table 1 show the results for HD1 and HD2 . We have not yet attempted to
automatically select the parameters for learning (i.e. k and D ). Rather, here we focus on comparing
the hand-coded definitions to the parameter set that we judged to be best performing across all event
types. We believe, however, that these parameters could be selected reliably using cross-validation
techniques applied to a larger data set. In that case, the parameters would be selected on a perevent-type basis and would likely result in an even more favorable comparison to the hand-coded
definitions.
The results show that the learned definitions significantly outperform HD1 on the current data
set. The HD1 definitions were found to produce a large number of false negatives on the current
data set. Notice that, although HD2 produces significantly fewer false negatives for all event types,
it produces more false positives for pick up and put down. This is because the hand definitions
utilize pick up and put down as macros for defining the other events.
The performance of the learned definitions is competitive with the performance of HD2 . The
main differences in performance are: (a) for pick up and put down, the learned and HD2 definitions
achieve nearly the same FN but the learned definitions achieve FP = 0 whereas HD2 has significant
FP, (b) for unstack and disassemble, the learned definitions perform moderately worse than HD2
with respect to FN, and (c) the learned definitions perform significantly better than HD2 on assemble
events.
We conjecture that further manual revision could improve HD2 to perform as well as (and perhaps better than) the learned definitions for every event class. Nonetheless, we view this experiment
as promising, as it demonstrates that our learning technique is able to compete with, and sometimes
outperform, significant hand-coding efforts by one of the authors.
6.3.5 VARYING N
It is of practical interest to know how training-set size affects our algorithms performance. For this
application, it is important that our method work well with fairly small data sets, as it can be tedious
to collect event data. Table 2 shows the FN of our learning algorithm for each event type, as N is
reduced from 29 to 5. For these experiments, we used k = 3 and D = BN. Note that FP = 0
for all event types and all N and hence is not shown. We expect FN to increase as N is decreased,
since, with specific-to-general learning, more data yields more-general definitions. Generally, FN
is flat for N > 20, increases slowly for 10 < N < 20, and increases abruptly for 5 < N < 10. We
also see that, for several event types, FN decreases slowly, as N is increased from 20 to 29. This
indicates that a larger data set might yield improved results for those event types.
6.3.6 P ERSPICUITY

OF

L EARNED D EFINITIONS

One motivation for using a logic-based event representation is to support perspicuityin this respect
our results are mixed. We note that perspicuity is a fuzzy and subjective concept. Realizing this,
we will say that an event definition is perspicuous if most humans with knowledge of the language
would find the definition to be natural. Here, we do not assume the human has a detailed knowledge of the model-reconstruction process that our learner is trying to fit. Adding that assumption
would presumably make the definitions qualify as more perspicuous, as many of the complex features of the learned definitions appear in fact to be due to idiosyncrasies of the model-reconstruction
process. In this sense, we are evaluating the perspicuity of the output of the entire system, not just
423

fiF ERN , G IVAN , & S ISKIND

of the learner itself, so that a key route to improving perspicuity in this sense would be to improve
the intuitive properties of the model-reconstruction output without any change to the learner.
While the learned and hand-coded definitions are similar with respect to accuracy, typically the
learned definitions are much less perspicuous. For our simplest event types, however, the learned
definitions are arguably perspicuous. Below we look at this issue in more detail. Appendix C gives
the hand-coded definitions in HD1 and HD2 along with a set of machine-generated definitions. The
learned definitions correspond to the output of our k -AMA learner when run on all 30 training
movies from each event type with k = 3 and D = BN (i.e., our best performing configuration with
respect to accuracy).
Perspicuous Definitions. The P ICK U P (x; y; z ) and P UT D OWN (x; y; z ) definitions are of particular interest here since short state sequences appear adequate for representing these event types
thus, we can hope for perspicuous 3-AMA definitions. In fact, the hand-coded definitions involve short sequences. Consider the hand-coded definitions of P ICK U P(x; y; z )the definitions
can roughly be viewed as 3-MA timelines of the form begin;trans;end.14 State begin asserts facts
that indicate y is on z and is not being held by x and end asserts facts that indicate y is being held by
x and is not on z . State trans is intended to model the fact that L EONARDs model-reconstruction
process does not always handle the transition between begin and end smoothly (so the definition
begin;end does not work well). We can make similar observations for P UT D OWN(x; y; z ).
Figure 15 gives the learned 3-AMA definitions of P ICK U P (x; y; z ) and P UT D OWN (x; y; z )
the definitions contain six and two 3-MA timelines respectively. Since the definitions consists of
multiple parallel timelines, they may at first not seem perspicuous. However, a closer examination
reveals that, in each definition, there is a single timeline that is arguably perspicuouswe have
placed these perspicuous timelines at the beginning of each definition. The perspicuous timelines
have a natural begin;trans;end interpretation. In fact, they are practically equivalent to the definitions
of P ICK U P (x; y; z ) and P UT D OWN (x; y; z ) in HD2 .15
With this in mind, notice that the HD2 definitions are overly general as indicated by significant
false positive rates. The learned definitions, however, yield no false positives without a significant
increase in false negatives. The learned definitions improve upon HD2 by essentially specializing
the HD2 definitions (i.e., the perspicuous timelines) by conjoining them with the non-perspicuous
timelines. While these non-perspicuous timelines are often not intuitive, they capture patterns in the
events that help rule out non-events. For example, in the learned definition of P ICK U P (x; y; z ) some
of the non-perspicuous timelines indicate that ATTACHED (y; z ) is true during the transition period
of the event. Such an attachment relationship does not make intuitive sense. Rather, it represents a
systematic error made by the model reconstruction process for pick up events.
In summary, we see that the learned definitions of P ICK U P (x; y; z ) and P UT D OWN (x; y; z ) each
contain a perspicuous timeline and one or more non-perspicuous timelines. The perspicuous timelines give an intuitive definition of the events, whereas the non-perspicuous timelines capture nonintuitive aspects of the events and model reconstruction process that are important in practice. We
note that, for experienced users, the primary difficulty of hand-coding definitions for L EONARD is
14. Note that the event-logic definition for P ICK U P(x; y; z ) in HD2 is written in a more compact form than 3-MA, but
this definition can be converted to 3-MA (and hence 3-AMA). Rather, HD1 cannot be translated exactly to 3-MA
since it uses disjunctionit is the disjunction of two 3-MA timelines.
15. The primary difference is that the HD2 definitions contain more negated propositions. The learner only considers a
proposition and its negation if the proposition is true at some point during the training movies. Many of the negated
propositions in HD2 never appear positively, thus they are not included in the learned definitions.

424

fiL EARNING T EMPORAL E VENTS

to determining which non-perspicuous properties must be included. Typically this requires many
iterations of trial and error. Our automated technique can relieve the user of this task. Alternatively,
we could view the system as providing guidance for this task.
Large Definitions. The S TACK (w; x; y; z ) and U NSTACK (w; x; y; z ) events are nearly identical
to put down and pick up respectively. The only difference is that now we are picking up from and
putting down onto a two block (rather than single block) tower (i.e., composed of blocks y and z ).
Thus, here again we might expect there to be perspicuous 3-AMA definitions. However, we see that
the learned definitions for S TACK (w; x; y; z ) and U NSTACK (w; x; y; z ) in Figures 16 and 17 involve
many more timelines than those for P ICK U P (w; x; y ) and P UT D OWN (w; x; y ). Accordingly, the
definitions are quite overwhelming and much less perspicuous.
Despite the large number of timelines, these definitions have the same general structure as those
for pick up and put down. In particular, they each contain a distinguished perspicuous timeline,
placed at the beginning of each definition, that is conjoined with many non-perspicuous timelines.
It is clear that, as above, the perspicuous timelines have a natural begin;trans;end interpretation
and, again, they are very similar to the definitions in HD 2 . In this case, however, the definitions
in HD2 are not overly general (committing no false positives). Thus, here the inclusion of the
non-perspicuous timelines has a detrimental effect since they unnecessarily specialize the definition
resulting in more false negatives.
We suspect that a primary reason for the large number of non-perspicuous timelines relative
to the definitions of pick up and put down stems from the increased difficulty of constructing
force-dynamic models. The inclusion of the two block tower in these examples causes the modelreconstruction process to produce more unintended results, particularly during the transition periods
of S TACK and U NSTACK . The result is that often many unintuitive and physically incorrect patterns
involving the three blocks and the hand are produced during the transition period. The learner
captures these patterns roughly via the non-perspicuous timelines. It is likely that generalizing the
definitions by including more training examples would filter out some of these timelines, making the
overall definition more perspicuous. Alternatively, it is of interest to consider pruning the learned
definitions. A straightforward way to do this is to generate negative examples. Then with these,
we could remove timelines (generalizing the definition) that do not contribute toward rejecting the
negative examples. It is unclear how to prune definitions without negative examples.
Hierarchical Events. M OVE(w; x; y; z ), A SSEMBLE (w; x; y; z ), and D ISASSEMBLE (w; x; y; z )
are inherently hierarchical, being composed of the four simpler event types. The hand-coded definitions leverage this structure by utilizing the simpler definitions as macros. In this light, it should
be clear that, when viewed non-hierarchically, (as our learner does) these events involve relatively
long state sequences. Thus, 3-AMA is not adequate for writing down perspicuous definitions. In
spite of this representational shortcoming, our learned 3-AMA definitions perform quite well. This
performance supports one of our arguments for using AMA from section 3.2. Namely, given that it
is easier to find short rather than long sequences, a practical approach to finding definitions for long
events is to conjoin the short sequences within those events. Examining the timelines of the learned
3-AMA definitions reveals what we might expect. Each timeline captures an often understandable
property of the long event sequence, but the conjunction of those timelines cannot be considered
to be a perspicuous definition. A future direction is to utilize hierarchical learning techniques to
improve the perspicuity of our definitions while maintaining accuracy.
425

fiF ERN , G IVAN , & S ISKIND

N

pick up

put down

stack

unstack

move

assemble

disassemble

29
25
20
15
10
5

0.0
0.0
0.01
0.01
0.07
0.22

0.20
0.20
0.21
0.22
0.27
0.43

0.45
0.47
0.50
0.53
0.60
0.77

0.10
0.16
0.17
0.26
0.36
0.54

0.03
0.05
0.08
0.14
0.23
0.35

0.07
0.09
0.12
0.20
0.32
0.57

0.10
0.10
0.12
0.16
0.26
0.43

Table 2: FN for k

= 3, D = BN, and various values of N .

We note, however, that, at some level, the learned definition of M OVE (w; x; y; z ) given in Figure 18 is perspicuous. In particular, the first 3-MA timeline is naturally interpreted as giving the
pre- and post-conditions for a move action. That is, initially x is supported by y and the hand w is
empty and finally x is supported by z and the hand w is empty. Thus, if all we care about is preand post-conditions, we might consider this timeline to be perspicuous. The remaining timelines in
the definition capture pieces of the internal event structure such as facts indicating that x is moved
by the hand. A weaker case can be made for assemble and disassemble. The first timeline in each
of the learned definitions in Figures 19 and 20 can be interpreted as giving pre- and post-conditions.
However, in these cases, the pre(post)-conditions for assemble(disassemble) are quite incomplete.
The incompleteness is due to the inclusion of examples where the model-reconstruction process did
not properly handle the initial(final) moments.

7. Related Work
Here we discuss two bodies of related work. First, we present previous work in visual event recognition and how it relates to our experiments here. Second, we discuss previous approaches to learning
temporal patterns from positive data.
7.1 Visual Event Recognition
Our system is unique in that it combines positive-only learning with a temporal, relational, and
force-dynamic representation to recognize events from real video. Prior work has investigated various subsets of the features of our systembut, to date, no system has combined all of these pieces
together. Incorporating any one of these pieces into a system is a significant endeavor. In this respect, there are no competing approaches to directly compare our system against. Given this, the
following is a representative list of systems that have common features with ours. It is not meant to
be comprehensive and focuses on pointing out the primary differences between each of these systems and ours, as these primary differences actually render these systems only very loosely related
to ours.
Borchardt (1985) presents a representation for temporal, relational, force-dynamic event definitions but these definitions are neither learned nor applied to video. Regier (1992) presents techniques for learning temporal event definitions but the learned definitions are neither relational, force
dynamic, nor applied to video. In addition the learning technique is not truly positive-onlyrather,
it extracts implicit negative examples of an event type from positive examples of other event types.
426

fiL EARNING T EMPORAL E VENTS

Yamoto, Ohya, and Ishii (1992), Brand and Essa (1995), Siskind and Morris (1996), Brand, Oliver,
and Pentland (1997), and Bobick and Ivanov (1998) present techniques for learning temporal event
definitions from video but the learned definitions are neither relational nor force dynamic. Pinhanez
and Bobick (1995) and Brand (1997a) present temporal, relational event definitions that recognize
events in video but these definitions are neither learned nor force dynamic. Brand (1997b) and Mann
and Jepson (1998) present techniques for analyzing force dynamics in video but neither formulate
event definitions nor apply these techniques to recognizing events or learning event definitions.
7.2 Learning Temporal Patterns
We divide this body of work into three main categories: temporal data mining, inductive logic
programming, and finite-statemachine induction.
Temporal Data Mining. The sequence-mining literature contains many general-to-specific (levelwise) algorithms for finding frequent sequences (Agrawal & Srikant, 1995; Mannila, Toivonen,
& Verkamo, 1995; Kam & Fu, 2000; Cohen, 2001; Hoppner, 2001). Here we explore a specific-togeneral approach. In this previous work, researchers have studied the problem of mining temporal
patterns using languages that are interpreted as placing constraints on partially or totally ordered
sets of time points, e.g., sequential patterns (Agrawal & Srikant, 1995) and episodes (Mannila et al.,
1995). These languages place constraints on time points rather than time intervals as in our work
here. More recently there has been work on mining temporal patterns using interval-based pattern
languages (Kam & Fu, 2000; Cohen, 2001; Hoppner, 2001).
Though the languages and learning frameworks vary among these approaches, they share two
central features which distinguish them from our approach. First, they all typically have the goal
of finding all frequent patterns (formulas) within a temporal data setour approach is focused
on finding patterns with a frequency of one (covering all positive examples). Our first learning
application of visual-event recognition has not yet required us to find patterns with frequency less
than one. However, there are a number of ways in which we can extend our method in that direction
when it becomes necessary (e.g., to deal with noisy training data). Second, these approaches all
use standard general-to-specific level-wise search techniques, whereas we chose to take a specificto-general approach. One direction for future work is to develop a general-to-specific level-wise
algorithm for finding frequent MA formulas and to compare it with our specific-to-general approach.
Another direction is to design a level-wise version of our specific-to-general algorithmwhere for
example, the results obtained for the k -AMA LGG can be used to more efficiently calculate the
(k + 1)-AMA LGG. Whereas a level-wise approach is conceptually straightforward in a general-tospecific framework it is not so clear in the specific-to-general case. We are not familiar with other
temporal data-mining systems that take a specific-to-general approach.
First-Order Learning In Section 3.3, we pointed out difficulties in using existing first-order
clausal generalization techniques for learning AMA formulas. In spite of these difficulties, it is still
possible to represent temporal events in first-order logic (either with or without capturing the AMA
semantics precisely) and to apply general-purpose relational learning techniques, e.g., inductive
logic programming (ILP) (Muggleton & De Raedt, 1994). Most ILP systems require both positive
and negative training examples and hence are not suitable for our current positive-only framework.
Exceptions include G OLEM (Muggleton & Feng, 1992), P ROGOL (Muggleton, 1995), and C LAU DIEN (De Raedt & Dehaspe, 1997), among others. While we have not performed a full evaluation
427

fiF ERN , G IVAN , & S ISKIND

Inputs
MA
AMA

Subsumption
Semantic
Syntactic
P
P
coNP-complete P

Semantic AMA LGG
Lower Upper Size
P
coNP EXP
coNP NEXP 2-EXP?

Syntactic AMA LGG
Lower Upper Size
P
coNP EXP
P
coNP EXP

Table 3: Complexity Results Summary. The LGG complexities are relative to input plus output size.
The size column reports the worst-case smallest correct output size. The ? indicates a
conjecture.
of these systems, our early experiments in the visual-event recognition domain confirmed our belief
that horn clauses, lacking special handling of time, give a poor inductive bias. In particular, many of
the learned clauses find patterns that simply do not make sense from a temporal perspective and, in
turn, generalize poorly. We believe a reasonable alternative to our approach may be to incorporate
syntactic biases into ILP systems as done, for example, in Cohen (1994), Dehaspe and De Raedt
(1996), Klingspor, Morik, and Rieger (1996). In this work, however, we chose to work directly in a
temporal logic representation.
Finite-State Machines Finally, we note there has been much theoretical and empirical research
into learning finite-state machines (FSMs) (Angluin, 1987; Lang, Pearlmutter, & Price, 1998). We
can view FSMs as describing properties of strings (symbol sequences). In our case, however, we are
interested in describing sequences of propositional models rather than just sequences of symbols.
This suggests learning a type of factored FSM where the arcs are labeled by sets of propositions
rather than by single symbols. Factored FSMs may be a natural direction in which to extend the
expressiveness of our current language, for example by allowing repetition. We are not aware of
work concerned with learning factored FSMs; however, it is likely that inspiration can be drawn
from symbol-based FSM-learning algorithms.

8. Conclusion
We have presented a simple logic for representing temporal events called AMA and have shown
theoretical and empirical results for learning AMA formulas. Empirically, weve given the first
system for learning temporal, relational, force-dynamic event definitions from positive-only input
and we have applied that system to learn such definitions from real video input. The resulting
performance matches that of event definitions that are hand-coded with substantial effort by human
domain experts. On the theoretical side, Table 3 summarizes the upper and lower bounds that
we have shown for the subsumption and generalization problems associated with this logic. In
each case, we have provided a provably correct algorithm matching the upper bound shown. The
table also shows the worst-case size that the smallest LGG could possibly take relative to the input
size, for both AMA and MA inputs. The key results in this table are the polynomial-time MA
subsumption and AMA syntactic subsumption, the coNP lower bound for AMA subsumption, the
exponential size of LGGs in the worst case, and the apparently lower complexity of syntactic AMA
LGG versus semantic LGG. We described how to build a learner based on these results and applied
it to the visual-event learning domain. To date, however, the definitions we learn are neither crossmodal nor perspicuous. And while the performance of the learned definitions matches that of hand428

fiL EARNING T EMPORAL E VENTS

coded ones, we wish to surpass hand coding. In the future, we intend to address cross-modality by
applying our learning technique to the planning domain. We also believe that addressing perspicuity
will lead to improved performance.

Acknowledgments
The authors wish to thank our anonymous reviewers for helping to improve this paper. This work
was supported in part by NSF grants 9977981-IIS and 0093100-IIS, an NSF Graduate Fellowship
for Fern, and the Center for Education and Research in Information Assurance and Security at
Purdue University. Part of this work was performed while Siskind was at NEC Research Institute,
Inc.

Appendix A. Internal Positive Event Logic
Here we give the syntax and semantics for an event logic called Internal Positive Event Logic
(IPEL). This logic is used in the main text only to motivate our choice of a small subset of this
logic, AMA, by showing, in Proposition 4, that AMA can define any set of models that IPEL can
define.
An event type (i.e., set of models) is said to be internal if whenever it contains any model
M = hM; I i, it also contains any model that agrees with M on truth assignments M [i] where i 2 I .
Full event logic allows the definition of non-internal events, for example, the formula 	 = 3< P
is satisfied by hM; I i when there is some interval I 0 entirely preceding I such that P is satisfied
by hM; I 0 i, thus 	 is not internal. The applications we are considering do not appear to require
non-internal events, thus we currently only consider events that are internal.
Call an event type positive if it contains the model M = hM; [1; 1]i where M (1) is the truth
assignment assigning all propositions the value true. A positive event type cannot require any proposition to be false at any point in time.
IPEL is a fragment of full propositional event logic that can only describe positive internal
events. We conjecture, but have not yet proven, that all positive internal events representable in the
full event logic of Siskind (2001) can be represented by some IPEL formula. Formally, the syntax
of IPEL formulas is given by

E ::= true j prop j E1 _ E2 j 3R E1 j E1 ^R E2 ;
0

where the Ei are IPEL formulas, prop is a primitive proposition (sometimes called a primitive event
type), R is a subset of the thirteen Allen interval relations fs,f,d,b,m,o,=,si,fi,di,bi,ai,oi g (Allen,
1983), and R0 is a subset of the restricted set of Allen relations fs,f,d,=g, the semantics for each
Allen relation is given in Table 4. The difference between IPEL syntax and that of full propositional
event logic is that event logic allows for a negation operator, and that, in full event logic, R0 can
be any subset of all thirteen Allen relations. The operators ^ and ; used to define AMA formulas
are merely abbreviations for the IPEL operators ^f=g and ^fmg respectively, so AMA is a subset of
IPEL (though a distinguished subset as indicated by Proposition 4).
Each of the thirteen Allen interval relations are binary relations on the set of closed naturalnumber intervals. Table 4 gives the definitions of these relations, defining [m1 ; m2 ] r [n1 ; n2 ] for
each Allen relation r . Satisfiability for IPEL formulas can now be defined as follows,
429

fiF ERN , G IVAN , & S ISKIND

I1
[m1 ; m2 ]
[m1 ; m2 ]
[m1 ; m2 ]
[m1 ; m2 ]
[m1 ; m2 ]
[m1 ; m2 ]
[m1 ; m2 ]

Relation
s
f
d
b
m
o
=

I2
[n1 ; n2 ]
[n1 ; n2 ]
[n1 ; n2 ]
[n1 ; n2 ]
[n1 ; n2 ]
[n1 ; n2 ]
[n1 ; n2 ]

English
starts
finishes
during
before
meets
overlaps
equals

Definition
m1 = n1 and m2
m1  n1 and m2
m1  n1 and m2

 n2
= n2
 n2

m2  n1
m2 = n1 or m2 + 1 = n1
m1  n1  m2  n2
m1 = n1 and m2 = n2

Inverse
si
fi
di
bi
mi
oi
=

Table 4: The Thirteen Allen Relations (adapted to our semantics).

 true is satisfied by every model.
 prop is satisfied by model hM; I i iff M [x] assigns prop true for every x 2 I .
 E1 _ E2 is satisfied by a model M iff M satisfies E1 or M satisfies E2.
 3RE is satisfied by model hM; I i iff for some r 2 R there is an interval I 0 such that I 0 r I
and hM; I 0 i satisfies E .
 E1 ^R E2 is satisfied by model hM; I i iff for some r 2 R there exist intervals I1 and I2 such
that I1 r I2 , S PAN (I1 ; I2 ) = I and both hM; I1 i satisfies E1 and hM; I2 i satisfies E2 .
where prop is a primitive proposition, E and Ei are IPEL formulas, R is a set of Allen relations, and
S PAN (I1 ; I2 ) is the minimal interval that contains both I1 and I2 . From this definition, it is easy to
show, by induction on the number of operators and connectives in a formula, that all IPEL formulas
define internal events. One can also verify that the definition of satisfiability given earlier for AMA
formulas corresponds to the one we give here.

Appendix B. Omitted Proofs
Lemma 1. For any MA timeline  and any model M, if M satisfies  then there is a witnessing
interdigitation for MAP(M)  .
Proof: Assume that M = hM; I i satisfies the MA timeline  = s1 ; : : : ; sn , and let 0 =
MAP(M). It is straightforward to argue, by induction on the length of , that there exists a mapping
V 0 from states of  to sub-intervals of I , such that

 for any i 2 V 0 (s), M [i] satisfies s,
 V 0(s1) includes the initial time point of I ,
 V 0(sn) includes the final time point of I , and
 for any i 2 [1; n 1], we have V 0(si ) meets V 0(si+1) (see Table 4).
430

fiL EARNING T EMPORAL E VENTS

Let V be the relation between states s 2  and members i 2 I that is true when i 2 V 0 (s). Note
that the conditions on V 0 ensure that every s 2  and every i 2 I appear in some tuple in V (not
necessarily together). Below we use V to construct a witnessing interdigitation W .
Let R be the total, one-to-one, onto function from time-points in I to corresponding states in 0 ,
noting that 0 has one state for each time-point in I , as 0 = MAP(hM; I i). Note that R preserves
ordering in that, when i  j , R(i) is no later than R(j ) in 0 . Let W be the composition V  R of
the relations V and R.
We show that W is an interdigitation. We first show that each state from  or 0 appears in a
tuple in W , so W is piecewise total. States from  must appear, trivially, because each appears in a
tuple of V , and R is total. States from 0 appear because each i 2 I appears in a tuple of V , and R
is onto the states of 0 .
It now suffices to show that for any states s before t from , W (s; s0 ) and W (t; t0 ) implies that
s0 is no later than t0 in 0 , so that W is simultaneously consistent. The conditions defining V 0 above
imply that every number in i 2 V (s) is less than or equal to every j 2 V (t). The order-preservation
property of R, noted above, then implies that every state s0 2 V  R(s) is no later than any state
t0 2 V  R(t) in 0 , as desired. So W is an interdigitation.
We now argue that W witnesses 0  . Consider s 2  and t 2 0 such that W (s; t). By the
construction of W , there must be i 2 V 0 (s) for which t is the ith state of 0 . Since 0 = MAP(M),
it follows that t is the set of true propositions in M [i]. Since i 2 V 0 (s), we know that M [i] satisfies
s. It follows that s  t, and so t  s. 2

2 IPEL, if model M embeds a model that satisfies E then M satisfies E .
Proof: Consider the models M = hM; I i and M0 = hM 0 ; I 0 i such that M embeds M0 , let
 = MAP(M) and 0 = MAP(M0 ). Assume that E 2 IPEL is satisfied by M0 , we will show that
E is also satisfied by M.
We know from the definition of embedding that   0 and thus there is a witnessing interdigitation W for   0 by Proposition 2. We know there is a one-to-one correspondence between
numbers in I (I 0 ) and states of  (0 ) and denote the state in  (0 ) corresponding to i 2 I (i0 2 I 0 )
Lemma 3. For any E

as si (ti ). This correspondence allows us to naturally interpret W as a mapping V from subsets of
I 0 to subsets of I as follows: for I10  I 0 , V (I10 ) equals the set of all i 2 I such that for some i0 2 I10 ,
si co-occurs with ti in W . We will use the following properties of V ,
0

0

1. If I10 is a sub-interval of I 0 , then V (I10 ) is a sub-interval of I .

2. If I10 is a sub-interval of I 0 , then hM; V (I10 )i embeds hM 0 ; I10 i.

3. If I10 and I20 are sub-intervals of I 0 , and r is an Allen relation, then I10 rI20 iff V (I10 )rV (I20 ).
4. If I10 and I20 are sub-intervals of I 0 , then V (S PAN (I10 ; I20 )) = S PAN (V (I10 ); V (I20 )).

5.

V (I 0 ) = I .

We sketch the proofs of these properties. 1) Use induction on the length of I10 , with the
definition of interdigitation. 2) Since V (I10 ) is an interval, MAP(hM; V (I10 )i) is well defined.
MAP(hM; V (I10 )i)  MAP(hM 0 ; I10 i) follows from the assumption that M embeds M0 . 3) From
Appendix A, we see that all Allen relations are defined in terms of the  relation on the natural
431

fiF ERN , G IVAN , & S ISKIND

number endpoints of the intervals. We can show that V preserves  (but not <) on singleton sets
(i.e., every member of V (fi0 g) is  every member of V (fj 0 g) when i0  j 0 ) and that V commutes with set union. It follows that V preserves the Allen interval relations. 4) Use the fact that
V preserves  in the sense just argued, along with the fact that S PAN (I10 ; I20 ) depends only on the
minimum and maximum numbers in I10 and I20 . 5) Follows from the definition of interdigitation and
the construction of V .
We now use induction on the number of operators and connectives in E to prove that, if M0
satisfies E , then so must M. The base case is when E = prop, where prop is a primitive proposition,
or true. Since M0 satisfies E , we know that prop is true in all M 0 [x0 ] for x0 2 I 0 . Since W witnesses
  0 , we know that, if prop is true in M 0 [x], then prop is true in all M [x], where x 2 V (x0 ).
Therefore, since V (I 0 ) = I , prop is true for all M 0 [x], where x 2 I , hence M0 satisfies E .
For the inductive case, assume that the claim holds for IPEL formulas with fewer than N operators and connectiveslet E1 ; E2 be two such formulas. When E = E1 _ E2 , the claim trivially
holds. When E = 3R E1 , R must be a subset of the set of relations fs,f,d,=g. Notice that E can
be written as a disjunction of 3r E1 formulas, where r is a single Allen relation from R. Thus, it
suffices to handle the case where R is a single Allen relation. Suppose E = 3fsg E1 . Since M0
satisfies E , there must be a sub-interval I10 of I 0 such that I10 s I 0 and hM 0 ; I10 i satisfies E1 . Let
I1 = V (I10 ), we know from the properties of V that V (I 0 ) = I , and, hence, that I1 s I . Furthermore, we know that hM; I1 i embeds hM 0 ; I10 i, and, thus, by the inductive hypothesis, hM; I1 i
satisfies E1 . Combining these facts, we get that E is satisfied by M. Similar arguments hold for
the remaining three Allen relations. Finally, consider the case when E = E1 ^R E2 , where R can
be any set of Allen relations. Again, it suffices to handle the case when R is a single Allen relation
r. Since M0 satisfies E = E1 ^r E2 , we know that there are sub-intervals I10 and I20 of I 0 such that
S PAN (I10 ; I20 ) = I 0 , I10 r I20 , hM 0 ; I10 i satisfies E1 , and hM 0 ; I20 i satisfies E2 . From these facts, and
the properties of V , it is easy to verify that M satisfies E . 2
Lemma 5. Given an MA formula  that subsumes each member of a set  of MA formulas, 
also subsumes some member 0 of IG(). Dually, when  is subsumed by each member of , we
have that  is also subsumed by some member 0 of IS(). In each case, the length of 0 can be
bounded by the size of .
Proof: We prove the result for IG(). The proof for IS() follows similar lines. Let

 =

f1 ; : : : ; ng,  = s1; : : : ; sm, and assume that for each 1  i  n, i  . From Proposition 2, for each i, there is a witnessing interdigitation Wi for i  . We will combine the Wi

into an interdigitation of , and show that the corresponding member of IG() is subsumed by
. To construct an interdigitation of , first notice that, for each sj , each Wi specifies a set of
states (possibly a single state but at least one) from i that all co-occur with sj . Furthermore, since
Wi is an interdigitation, it is easy to show that this set of states corresponds to a consecutive subsequence of states from i let j;i be the MA timeline corresponding to this subsequence. Now
let j = fj;i j 1  i  ng, and ffj be any interdigitation of j . We now take I to be the union of
all ffj , for 1  j  m. We show that I is an interdigitation of . Since each state s appearing in 
must co-occur with at least one state sj in  in at least one Wi , s will be in at least one tuple of ffj ,
and, hence, be in some tuple of I so I is piecewise total.
Now, define the restriction I i;j of I to components i and j , with i < j , to be the relation given
by taking the set of all pairs formed by shortening tuples of I by omitting all components except
432

fiL EARNING T EMPORAL E VENTS

the ith and the j th. Likewise define ffi;j
k for each k . To show I is an interdigitation, it now suffices
to show that each I i;j is simultaneously consistent. Consider states si and sj from timelines i and
j , respectively, such that I i;j (si ; sj ). Suppose that ti occurs after si in i, and for some tj 2 j ,
I i;j (ti ; tj ) holds. It suffices to show that sj is no later than tj in j . Since I i;j (si ; sj ) and I i;j (ti ; tj ),
i;j
0
0
we must have ffi;j
k (si ; sj ) and ffk (ti ; tj ), respectively, for some k and k . We know k  k because
0
si is before ti in i and Wi is simultaneously consistent. If k = k , then sj is no later than tj in j ,
because ffk must be simultaneously consistent, being an interdigitation. Otherwise, k < k 0 . Then sj
is no later than tj in j , as desired, because Wj is simultaneously consistent. So I is simultaneously
consistent, and an interdigitation of .
Let 0 be the member of IG() corresponding to I . We now show that 0  . We know that
each state s0 2 0 is the intersection of the states in a tuple of some ffj we say that s0 derives from
ffj . Consider the interdigitation I 0 between  and 0 , where I 0 (sj ; s0 ), for sj 2  and s0 2 0 , if and
only if s0 derives from ffj . I 0 is piecewise total, as every tuple of I 0 derives from some ffj , and no ffj
is empty. I 0 is simultaneously consistent because tuples of I 0 deriving from later ffk must be later in
the lexicographic ordering of I , given the simultaneous consistency of the Wk interdigitations used
to construct each ffj . Finally, we know that sj subsumes (i.e., is a subset of) each state in each tuple
of ffj , because each Wk is a witnessing interdigitation to k  , and, hence, subsumes (is a subset
of) the intersection of those states. Therefore, if sj 2  co-occurs with s0 2 0 in I 0 we have that
s0  sj . Thus, I 0 is a witnessing interdigitation for 0  , and by Proposition 2 we have 0  .
The size bound on 0 follows, since, as pointed out in the main text, the size of any member of
IG() is upper-bounded by the number of states in . 2
0

Lemma 8. Given MA timelines 1 = s1 ; : : : ; sm and 2 = t1 ; : : : ; tn , there is a witnessing
interdigitation for 1  2 iff there is a path in the subsumption graph SG(1 ; 2 ) from v1;1 to
vm;n .
Proof: Subsumption
graph SG(1 ; 2 ) is equal to hV; E i with V = fvi;j j 1	  i  m; 1  j  ng

and E = hvi;j ; vi ;j i j si  tj ; si  tj ; i  i0  i + 1; j  j 0  j + 1 . Note that there is a
correspondence between vertices and state tupleswith vertex vi;j corresponding to hsi ; tj i.
For the forward direction, assume that W is a witnessing interdigitation for 1  2 . We
know that, if the states si and tj co-occur in W , then si  tj since W witnesses 1  2 . The
vertices corresponding to the tuples of W will be called co-occurrence vertices, and satisfy the
first condition for belonging to some edge in E (that si  tj ). It follows from the definition of
interdigitation that both v1;1 and vm;n are both co-occurrence vertices. Consider a co-occurrence
vertex vi;j not equal to vm;n , and the lexicographically least co-occurrence vertex vi ;j after vi;j
(ordering vertices
 by ordering
the pair of subscripts). We show that i, j , i0 , and j 0 satisfy the
ff
requirements for vi;j ; vi ;j 2 E . If not, then either i0 > i + 1 or j 0 > j + 1. If i0 > i + 1, then
there can be no co-occurrence vertex vi+1;j , contradicting that W is piecewise total. If j 0 > j + 1,
then since W is piecewise total, there must be a co-occurrence vertex vi ;j +1 : but if i00 < i or
i00 > i0 , this contradicts the simultaneous consistency of W , and if i00 = i, this contradicts the
lexicographically least choice of vi ;j . It follows that every co-occurrence vertex but vm;n has an
edge to another co-occurrence vertex closer in Manhattan distance to vm;n , and thus that there is a
path from v1;1 to vm;n .
For the reverse direction assume there is a path of vertices in SG(1 ; 2 ) from v1;1 to vm;n
given by, vi1 ;j1 ; vi2 ;j2 ; : : : ; vir ;js with i1 = j1 = 1, ir = m; js = n. Let W be the set of state
0

0

0

0

0

0

0

00

00

0

0

433

0

fiF ERN , G IVAN , & S ISKIND

tuples corresponding to the vertices along this path. W must be simultaneously consistent with the
i orderings because our directed edges are all non-decreasing in the i orderings. W must be
piecewise total because no edge can cross more than one state transition in either 1 or 2 , by the
edge set definition. So W is an interdigitation. Finally, the definition of the edge set E ensures
that each tuple hsi ; tj i in W has the property si  tj , so that W is a witnessing interdigitation for
1  2 , showing that 1  2 , as desired. 2
Lemma 10. Given some n, let 	 be the conjunction of the timelines
n
[
i=1

f(PROPn; Truei; Falsei; PROPn); (PROPn; Falsei; Truei; PROPn)g:

We have the following facts about truth assignments to the Boolean variables p1 ; : : : ; pn :
1. For any truth assignment A, PROPn ; sA ; PROPn is semantically equivalent to a member
of IS(	).
2. For each  2 IS(	) there is a truth assignment A such that   PROPn ; sA ; PROPn .
Proof: To prove the first part of the lemma, we construct an interdigitation I of 	 such that the
corresponding member of IS(	) is equivalent to PROPn ; sA ; PROPn . Intuitively, we construct I
by ensuring that some tuple of I consists only of states of the form Truek or Falsek that agree with
the truth assignmentthe union of all the states in this tuple, taken by IS(	) will equal sA . Let
I = fT0 ; T1 ; T2 ; T3 ; T4 g be an interdigitation of 	 with exactly five state tuples Ti . We assign the
states of each timeline of 	 to the tuples as follows:
1. For any k , such that 1  k




 n and A(pk ) is true,

for the timeline s1 ; s2 ; s3 ; s4 = Q; T ruek ; F alsek ; Q, assign each state si to tuple Ti ,
and assign state s1 to T0 as well, and
for the timeline s01 ; s02 ; s03 ; s04 = Q; F alsek ; T ruek ; Q, assign each state s0i to tuple Ti 1 ,
and state s04 to tuple T4 as well.

2. For any k , such that 1  k  n and A(pk ) is false, assign states to tuples as in item 1 while
interchanging the roles of T ruek and F alsek .

It should be clear that I is piecewise total and simultaneously consistent with the state orderings
in 	, and so is an interdigitation. The union of the states in each of T0 , T1 , T3 , and T4 is equal to
PROPn , since PROPn is included as a state in each of those tuples. Furthermore, we see that the
union of the states in T2 is equal to sA . Thus, the member of IS(	) corresponding to I is equal to
PROPn ; PROPn ; sA ; PROPn ; PROPn , which is semantically equivalent to PROPn ; sA ; PROPn , as
desired.
To prove the second part of the lemma, let  be any member of IS(	). We first argue that
every state in  must contain either Truek or Falsek for each 1  k  n. For any k , since 	 contains PROPn ; Truek ; Falsek ; PROPn , every member of IS(	) must be subsumed by PROPn ; Truek ;
Falsek ; PROPn . So,  is subsumed by PROPn ; Truek ; Falsek ; PROPn . But every state in PROPn ;
Truek ; Falsek ; PROPn contains either Truek or Falsek , implying that so does , as desired.
434

fiL EARNING T EMPORAL E VENTS

Next, we claim that for each 1  k  n, either   Truek or   Falsek i.e., either all states
in  include Truek , or all states in  include Falsek (and possibly both). To prove this claim, assume,
for the sake of contradiction, that, for some k ,  6 Truek and  6 Falsek . Combining this assumption with our first claim, we see there must be states s and s0 in  such that s contains T ruek but
not F alsek , and s0 contains F alsek but not T ruek , respectively. Consider the interdigitation I of 	
that corresponds to  as a member of IS(	). We know that s and s0 are each equal to the union of
states in tuples T and T 0 , respectively, of I . T and T 0 must each include one state from each timeline
s1 ; s2 ; s3 ; s4 = PROPn ; Truek ; Falsek ; PROPn and s01 ; s02 ; s03 ; s04 = PROPn ; Falsek ; Truek ; PROPn .
Clearly, since s does not include Falsek , T includes the states s1 and s02 , and likewise T 0 includes
the states s2 and s01 . It follows that I is not simultaneously consistent with the state orderings in
s1 ; s2 ; s3 ; s4 and s01 ; s02 ; s03 ; s04 , contradicting our choice of I as an interdigitation. This shows that
either   Truek or   Falsek .
Define the truth assignment A such that for all 1  k  n, A(pk ) if and only if   Truek .
Since,for each k ,   Truek or   Falsek , it follows that each state of  is subsumed by
sA . Furthermore, since  begins and ends with PROPn , it is easy to give an interdigitation of
 and PROPn ; sA ; PROPn that witnesses   PROPn ; sA ; PROPn . Thus, we have that  
PROPn ; sA ; PROPn . 2
Lemma 16. Let 1 and 2 be as given on page 402, in the proof of Theorem 17, and let 	 =
V
IG(f1 ; 2 g). For any 	0 whose timelines are a subset of those in 	 that omits some square
timeline, we have 	 < 	0 .
Proof: Since the timelines in 	0 are a subset of the timelines in 	, we know that 	  	0 . It remains
to show that 	0 6 	. We show this by constructing a timeline that is covered by 	0 , but not by 	.
Let  = s1 ; s2 ; : : : ; s2n 1 be a square timeline in 	 that is not included in 	0 . Recall that each
si is a single proposition from the proposition set P = fpi;j j 1  i  n; 1  j  ng, and that,
for consecutive states si and si+1 , if si = pi;j , then si+1 is either pi+1;j or pi;j +1 . Define a new
timeline  = s2 ; s3 ; : : : ; s2n 2 with si = (P si ). We now show that  6  (so that  6 	), and
that, for any 0 in 	 fg,   0 (so that   	0 ).
For the sake of contradiction, assume that   then there must be a interdigitation W
witnessing   . We show by induction on i that, for i  2, W (si ; sj ) implies j > i. For the
base case, when i = 2, we know that s2 6 s2 , since s2 6 s2 , and so W (s2 ; s2 ) is false, since
W witnesses subsumption. For the inductive case, assume the claim holds for all i0 < i, and that
W (si ; sj ). We know that si 6 si , and thus i 6= j . Because W is piecewise total, we must have
W (si 1 ; sj ) for some j 0 , and, by the induction hypothesis, we must have j 0 > i 1. Since W is
simultaneously consistent with the sk and sk state orderings, and i 1 < i, we have j 0  j . It
follows that j > i as desired. Given this claim, we see that s2n 2 cannot co-occur in W with any
state in , contradicting the fact that W is piecewise total. Thus we have that  6 .
Let 0 = s01 ; : : : ; s0m be any timeline in 	 fg, we now construct an interdigitation that
witnesses   0 . Note that while  is assumed to be square, 0 need not be. Let j be the smallest
index where sj 6= s0j  since s1 = s01 = p1;1 , and  6= 0 , we know that such a j must exist, and is
in the range 2  j  m. We use the index j to guide our construction of an interdigitation. Let W
be an interdigitation of  and 0 , with exactly the following co-occurring states (i.e., state tuples):
0

0

1. For 1  i  j

1, si+1 co-occurs with s0i .
435

fiF ERN , G IVAN , & S ISKIND

 i  m, sj co-occurs with s0i.
For j + 1  i  2n 2, si co-occurs with s0m .

2. For j
3.

It is easy to check that W is both piecewise total and simultaneously consistent with the state
orderings in  and , and so is an interdigitation. We now show that W witnesses   0 by
showing that all states in  are subsumed by the states they co-occur with in W . For co-occurring
states si+1 and s0i corresponding to the first item above we have that s0i = si this implies that s0i
is contained in si+1 , giving that si+1  s0i . Now consider co-occurring states sj and s0i from the
second item above. Since  is square, choose k and l so that sj 1 = pk;l , we have that sj is either
pk+1;l or pk;l+1. In addition, since sj 1 = s0j 1 we have that s0j is either pk+1;l ; pk;l+1 or pk+1;l+1
but that sj 6= s0j . In any of these cases, we find that no state in 0 after s0j can equal sj this follows
by noting that the proposition indices never decrease across the timeline 0 16 . We therefore have
that, for i  j , sj  s0i . Finally, for co-occurring states si and s0m from item three above, we have
si  s0m , since s0m = pn;n, which is in all states of . Thus, we have shown that for all co-occurring
states in W , the state from  is subsumed by the co-occurring state in 0 . Therefore, W witnesses
  0 , which implies that   0 . 2
Lemma 26. For any model hM; I i 2 M and any 	 2 AMA ,
T [hM; I i].

	 covers hM; I i

iff

F [	] covers

Proof: Recall that M is the set of models over propositions in the set P = fp1 ; : : : ; pn g and that
we assume AMA uses only primitive propositions from P (possibly negated). We also have the
set of propositions P = fp1 ; : : : ; pn g, and assume that formulas in AMA use only propositions in
P [ P and that M is the set of models over P [ P , where for each i, exactly one of pi and pi is
true at any time. Note that F [	] is in AMA and that T [hM; I i] is in M. We prove the lemma via
straightforward induction on the structure of 	proving the result for literals, then for states, then
for timelines, and finally for AMA formulas.
To prove the result for literals, we consider two cases (the third case of true is trivial). First, 	
can be a single proposition pi , so that 	0 = F [pi ] = pi . Consider any model hM; I i 2 M and let
hM 0 ; I i = T [hM; I i]. The following relationships yield the desired result.

	 covers hM; I i

iff
iff
iff

for each i 2 I , M [i] assigns pi true
for each i 2 I , M 0 [i] assigns pi true
	0 = pi covers T [hM; I i]

(by definition of satisfiability)
(by definition of T )
(by definition of satisfiability)

The second case is when 	 is a negated proposition :3pi here, we get that 	0 = pi . Let
hM; I i 2 M and hM 0 ; I i = T [hM; I i]. The following relationships yield the desired result.

	 covers hM; I i

iff
iff
iff

for each i 2 I , M [i] assigns pi false
for each i 2 I , M 0 [i] assigns pi true
	0 = pi covers T [hM; I i]

(by definition of satisfiability)
(by definition of T )
(by definition of satisfiability)

This proves the lemma for literals.
16. Note that if
pk+1;l+1 .

 were not required to be square then it is possible for +1 to equal
0

sj

436

sj

i.e., they could both equal

fiL EARNING T EMPORAL E VENTS

To prove the result for states, we use induction on the number k of literals in a state. The base
case is when k = 1 (the state is a single literal) and was proven above. Now assume that the lemma
holds for states with k or fewer literals and let 	 = l1 ^    ^ lk+1 and hM; I i 2 M. From the
inductive assumption we know that  = l1 ^  ^ lk covers hM; I i iff F [] covers T [hM; I i]. From
our base case we also know that lk+1 covers hM; I i iff F [lk+1 ] covers T [hM; I i]. From these facts
and the definition of satisfiability for states, we get that 	 covers hM; I i iff F [] ^ F [lk+1 ] covers
T [hM; I i]. Clearly F has the property that F [] ^ F [lk+1 ] = F [	], showing that the lemma holds
for states.
To prove the result for timelines, we use induction on the number k of states in the timeline. The
base case is when k = 1 (the timeline is a single state) and was proven above. Now assume that the
lemma holds for timelines with k or fewer states. Let 	 = s1 ; : : : ; sk+1 and hM; [t; t0 ]i 2 M with
hM 0 ; [t; t0 ]i = T [hM; [t; t0 ]i]. We have the following relationships.

	 covers hM; [t; t0 ]i

iff
iff
iff
iff

there exists some t00 2 [t; t0 ], such that s1 covers hM; [t; t00 ]i and
 = s2 ; : : : ; sk+1 covers either hM; [t00 ; t0 ]i or hM; [t00 + 1; t0 ]i
there exists some t00 2 [t; t0 ], such that F [s1 ] covers hM 0 ; [t; t00 ]i and
F [] covers either hM 0 ; [t00 ; t0 ]i or hM 0 ; [t00 + 1; t0 ]i
F [s1 ]; F [] covers hM 0 ; [t; t0 ]i
F [	] covers hM 0 ; [t; t0 ]i

Where the first iff follows from the definition of satisfiability; the second follows from our inductive
hypothesis, our base case, and the fact that for I  [t; t0 ] we have T [hM; I i] = hM 0 ; I i; the third
follows from the definition of satisfiability; and the fourth follows from the fact that F [s1 ]; F [] =
F [	].
Finally, we prove the result for AMA formulas, by induction on the number k of timelines
in the formula. The base case is when k = 1 (the formula is a single timeline) and was proven
above. Now assume that the lemma holds for AMA formulas with with k or fewer timelines
and let 	 = 1 ^    ^ k+1 and hM; I i 2 M. From the inductive assumption, we know that
	0 = 1 ^    ^ k covers hM; I i iff F [	0 ] covers T [hM; I i]. From our base case, we also
know that k+1 covers hM; I i iff F [k+1 ] covers T [hM; I i]. From these facts and the definition of
satisfiability, we get that 	 covers hM; I i iff F [	0 ] ^ F [k+1 ] covers T [hM; I i]. Clearly F has the
property that F [	0 ] ^ F [k+1 ] = F [	], showing that the lemma holds for AMA formulas. This
completes the proof. 2

Appendix C. Hand-coded and Learned Definitions Used in Our Experiments
Below we give the two sets of hand-coded definitions, HD1 and HD2 , used in our experimental
evaluation. We also give a set of learned AMA event definitions for the same seven event types. The
learned definitions correspond to the output of our k -AMA learning algorithm, given all available
training examples (30 examples per event type), with k = 3 and D = BN. All the event definitions
are written in event logic, where :3p denotes the negation of proposition p.

437

fiF ERN , G IVAN , & S ISKIND

1

0

4

P ICK U P (x; y; z )

=

P UT D OWN(x; y; z )

=

S TACK (w; x; y; z )

=

U NSTACK (w; x; y; z )

=

M OVE(w; x; y; z )
A SSEMBLE(w; x; y; z )
D ISASSEMBLE(w; x; y; z )

4

4

4

4
4
=
4
=
=

:3x = y ^ :3z = x ^ :3z = y^
C
B S UPPORTED(y ) ^ :3ATTACHED(x; z )^
B 8 2
3 9 C
C
B >
:3ATTACHED(x; y) ^ :3S UPPORTS(x; y)^
>
>
C
B >
>
7
>
C
B >
>
6
S UPPORTS (z; y )^
>
7
>
C
B >
>
6
>
7
>
C
B >
>
6
:
3
S UPPORTED(x) ^ :3ATTACHED(y; z )^ 7 ; >
>
C
B >
>
6
>
>
C
B >
>
5
4
:3S UPPORTS(y; x) ^ :3S UPPORTS(y; z )^
>
>
C
B >
>
>
>
B >
>
:3S UPPORTS(x; z ) ^ :3S UPPORTS(z; x)
= C
C
B <
C
B
B > [2ATTACHED(x; y ) _ ATTACHED(y; z )] ;
3 > C
>
C
B >
ATTACHED(x; y ) ^ S UPPORTS(x; y )^
>
>
C
B >
>
>
6
7
>
C
B >
>
:3S UPPORTS(z; y)^
>
6
7
>
C
B >
>
>
6
7
>
C
B >
>
:
3
S
UPPORTED
(
x
)
^
:
3
A
TTACHED
(
y;
z
)
^
>
6
7
>
C
B >
>
>
>
>
4
5
A
@ >
:
3
S
UPPORTS
(
y;
x
)
^
:
3
S
UPPORTS
(
y;
z
)
^
>
>
>
>
;
:
:3S UPPORTS(x; z ) ^ :3S UPPORTS(z; x)
1
0
:3x = y ^ :3z = x ^ :3z = y^
C
B S UPPORTED(y ) ^ :3ATTACHED(x; z )^
B 8 2
3 9 C
C
B >
A
TTACHED
(
x; y ) ^ S UPPORTS (x; y )^
>
>
C
B >
>
>
C
7
B >
>
6
:
3
S
UPPORTS
(
z;
y
)
^
>
>
C
7
B >
>
6
>
>
C
B >
>
6
7
:
3
S
UPPORTED
(
x
)
^
:
3
A
TTACHED
(
y;
z
)
^
;
>
>
C
B >
>
6
7
>
>
C
B >
>
4
5
:
3
S
UPPORTS
(
y;
x
)
^
:
3
S
UPPORTS
(
y;
z
)
^
>
>
C
B >
>
>
>
B >
>
:
3
S
UPPORTS
(
x;
z
)
^
:
3
S
UPPORTS
(
z;
x
)
= C
C
B <
C
B
B > [2ATTACHED(x; y ) _ ATTACHED(y; z )] ;
3 > C
>
C
B >
A
TTACHED
(
x; y ) ^ :3 S UPPORTS(x; y )^
:
3
>
>
> C
B >
7 >
>
C
B >
>
6 S UPPORTS (z; y )^
>
>
7 > C
B >
6
>
C
B >
>
6 :3S UPPORTED(x) ^ :3ATTACHED(y; z )^ 7 >
>
>
7 > C
B >
6
>
>
A
@ >
4 :3S UPPORTS(y; x) ^ :3S UPPORTS(y; z )^ 5 >
>
>
>
>
;
:
:3S UPPORTS(x; z ) ^ :3S UPPORTS(z; x)
3
2
:3z = w ^ :3z = x ^ :3z = y^
4 P UT D OWN(w; x; y ) ^ S UPPORTS(z; y )^ 5
:ATTACHED(z; y)


:3z = w ^ :3z = x ^ :3z = y^
P ICK U P (w; x; y ) ^ S UPPORTS(z; y ) ^ :ATTACHED(z; y )
:3y = z ^ [P ICK U P(w; x; y); P UT D OWN(w; x; z )]
P UT D OWN(w; y; z ) ^f g S TACK (w; x; y; z )
U NSTACK(w; x; y; z ) ^f g P ICK U P (x; y; z )
<

<

Figure 12: The HD1 event-logic definitions for all seven event types.

438

fiL EARNING T EMPORAL E VENTS

0

1
:3x = y ^ :3z = x ^ :3z = y^
B
C
(y) ^ :3ATTACHED (x; z )^
B S UPPORTED
C
3
9 C
B 8 2
B >
C
A
TTACHED (x; y ) ^ :3S UPPORTS (x; y )^
:
3
>
>
B >
C
> 6
>
7
>
B >
C
> 6 S UPPORTS (z; y ) ^ C ONTACTS (z; y )^
>
7
>
B >
C
>
>
7
6
>
B >
C
> 6 :3S UPPORTED (x) ^ :3ATTACHED (y; z )^ 7 ^f<;mg >
>
B >
C
>
>
7
6
>
B >
C
>
> 4 :3S UPPORTS (y; x) ^ :3S UPPORTS (y; z )^ 5
>
4
B >
C
>
>
>
P ICK U P (x; y; z ) = B >
= C
<
B
C
S
UPPORTS (x; z ) ^ :3S UPPORTS (z; x)
:
3
3
B
2
C
B >
C
>
A
TTACHED
(
x;
y
)
^
S
UPPORTS
(
x;
y
)
^
>
B >
C
>
> 6
>
7
B >
C
>
>
:
3
S
UPPORTS
(
z;
y
)
^
>
6
7
B >
C
>
> 6
>
7
B >
C
>
>
>
:
3
S
UPPORTED
(
x
)
^
:
3
A
TTACHED
(
y;
z
)
^
6
7
B >
C
> 6
>
>
7
B >
C
>
>
>
4
5
@ >
A
:
3
S
UPPORTS
(
y;
x
)
^
:
3
S
UPPORTS
(
y;
z
)
^
>
>
>
>
;
:
:3S UPPORTS(x; z) ^ :3S UPPORTS(z; x)
0
1
:3x = y ^ :3z = x ^ :3z = y^
C
B
(y) ^ :3ATTACHED (x; z )^
C
B S UPPORTED
3
9 C
B 8 2
C
B >
A
TTACHED (x; y ) ^ S UPPORTS (x; y )^
>
>
C
B >
>
> 6
7
>
C
B >
>
>
S
UPPORTS
(
z;
y
)
^
:
3
7
6
>
C
B >
>
> 6
7
>
C
B >
>
>
:
3
S
UPPORTED
(
x
)
^
:
3
A
TTACHED
(
y;
z
)
^
^
7
6
>
f
<;
m
g
C
B >
>
>
7
> C
> 6
B >
>
5
4
>
:
3
S
UPPORTS
(
y;
x
)
^
:
3
S
UPPORTS
(
y;
z
)
^
4 B>
C
>
>
>
P UT D OWN (x; y; z ) = B >
= C
<
C
B
:
3
S
UPPORTS (x; z ) ^ :3S UPPORTS (z; x)
3
C
B
2
C
B >
>
:
3ATTACHED (x; y) ^ :3S UPPORTS(x; y)^
>
C
B >
>
>
>
6
7
C
B >
>
> 6 S UPPORTS (z; y ) ^ C ONTACTS (z; y )^
>
7
C
B >
>
>
>
6
7
B >
C
>
> 6 :3S UPPORTED (x) ^ :3ATTACHED (y; z )^ 7
>
B >
C
>
>
>
6
7
B >
> C
>
>
4 :3S UPPORTS (y; x) ^ :3S UPPORTS (y; z )^ 5
A
@ >
>
>
>
>
;
:
:3S UPPORTS(x; z) ^ :3S UPPORTS(z; x)

Figure 13: Part I of the HD2 event-logic definitions.

439

fiF ERN , G IVAN , & S ISKIND

0

1

:3w = x ^ :3y = w ^ :3y = x^
B :3z = w ^ :3z = x ^ :3z = y ^
C
B
C
B S UPPORTED (x) ^ :3ATTACHED(w; y )^
C
B 8 2
9 C
3
B >
C
ATTACHED(w; x) ^ S UPPORTS (w; x)^
>
B >
>
C
> 6
>
B >
7
>
C
:
3
S UPPORTS(y; x)^
>
B >
>
7
>
6
C
> 6
>
B >
7
>
C
S UPPORTS(z; y ) ^ C ONTACTS(z; y )^
>
B >
7
>
6
>
C
> 6
>
B >
7
C
>
:
3
ATTACHED(z; y )^
^
f
mg >
B >
7
6
C
>
>
> 6
>
7
B >
C
>
:
3
S UPPORTED(w) ^ :3ATTACHED(x; y )^ 7
>
B >
6
C
>
>
> 4
>
B >
C
>
5
:3S UPPORTS(x; w) ^ :3S UPPORTS(x; y)^
>
B >
>
>
= C
B <
C
B
C
2 :3S UPPORTS(w; y ) ^ :3S UPPORTS(y; w)
3
B >
C
:3ATTACHED(w; x) ^ :3S UPPORTS(w; x)^
>
B >
C
>
>
B >
C
6
7
>
>
S UPPORTS(y; x) ^ C ONTACTS (y; x)^
>
B >
C
6
7
>
>
>
B >
C
6
7
>
>
>
B >
C
6 S UPPORTS(z; y ) ^ C ONTACTS(z; y )^
7
>
>
>
>
B >
C
6
7
>
>
B >
C
7
> 6 :3ATTACHED(z; y )^
>
>
>
B >
C
6
7
>
>
B >
C
> 6 :3S UPPORTED(w) ^ :3ATTACHED(x; y )^ 7
>
>
>
>
@ >
A
4
5
:3S UPPORTS(x; w) ^ :3S UPPORTS(x; y)^
>
>
>
>
:
;
:3S UPPORTS(w; y) ^ :3S UPPORTS(y; w)
1
0
:3w = x ^ :3y = w ^ :3y = x^
C
B :3z = w ^ :3z = x ^ :3z = y ^
C
B
C
B S UPPORTED(x) ^ :3ATTACHED(w; y )^
9 C
B 8 2
3
C
B >
:3ATTACHED(w; x) ^ :3S UPPORTS(w; x)^
>
C
>
B >
>
C
>
6
B >
7
>
>
C
>
6 S UPPORTS (y; x) ^ C ONTACTS (y; x)^
B >
7
>
>
>
C
>
6
B >
7
>
C
>
B >
7
> 6 S UPPORTS (z; y ) ^ C ONTACTS (z; y )^
>
>
C
>
6
B >
7
C
>
6 :3ATTACHED(z; y )^
B >
7 ^f mg >
>
>
>
C
>
6
B >
7
>
C
>
B >
> 6 :3S UPPORTED(w) ^ :3ATTACHED(x; y )^ 7
>
>
C
>
B >
4
5
:3S UPPORTS(x; w) ^ :3S UPPORTS(x; y)^
>
C
>
B >
>
=
<
C
B
C
B
2 :3S UPPORTS(w; y ) ^ :3S UPPORTS(y; w) 3
C
B >
ATTACHED(w; x) ^ S UPPORTS(w; x)^
>
C
>
B >
>
>
6
C
7
>
B >
>
C
7
>
B >
> 6 :3S UPPORTS(y; x)^
>
>
6
C
7
>
B >
>
6 S UPPORTS (z; y ) ^ C ONTACTS (z; y )^
C
7
>
B >
>
>
>
6
C
7
>
B >
>
C
7
>
B >
> 6 :3ATTACHED(z; y )^
>
>
6
C
7
>
B >
>
C
>
B >
> 6 :3S UPPORTED(w) ^ :3ATTACHED(x; y )^ 7
>
>
>
4
A
5
@ >
:3S UPPORTS(x; w) ^ :3S UPPORTS(x; y)^
>
>
>
>
;
:
:3S UPPORTS(w; y) ^ :3S UPPORTS(y; w)
:3y = z ^ [P ICK U P(w; x; y); P UT D OWN(w; x; z )]
P UT D OWN(w; y; z ) ^f g S TACK (w; x; y; z )
U NSTACK (w; x; y; z ) ^f g P ICK U P (x; y; z )
<;

S TACK(w; x; y; z )

4

=

<;

U NSTACK(w; x; y; z )

M OVE(w; x; y; z )
A SSEMBLE(w; x; y; z )
D ISASSEMBLE(w; x; y; z )

4

=

4
4
=
4
=
=

<

<

Figure 14: Part II of the HD2 event-logic definitions.

440

fiL EARNING T EMPORAL E VENTS

3 9
0 8 2
S UPPORTED (y ) ^ S UPPORTS (z; y )^
>
>
>
>
>
B >
> 4 C ONTACTS (y; z ) ^ : S UPPORTS(x; y )^
5; >
>
>
B >
>
>
>
B >
>
: ATTACHED(x; y) ^ : ATTACHED(y; z )
=
B <
B
S UPPORTED(y );
B > 2
3 >^
>
B >
S UPPORTED (y ) ^ S UPPORTS (x; y )^
>
>
>
B >
>
>
B >
>
5
4
ATTACHED(x; y ) ^ : S UPPORTS(z; y )^
>
>
>
B >
;
:
B
: C ONTACTS(y; z ) ^ : ATTACHED(y; z9)
B 8
B > S UPPORTED(y );
>
B >

 >
=
B <
S UPPORTED(y ) ^ ATTACHED(x; y )^
B
;
^
B >
ATTACHED(y; z )
>
B >
>
:
;
B
B 8 [S UPPORTED(y ) ^ ATTACHED(x; y )] 9
B < [S UPPORTED(y ) ^ C ONTACTS (y; z )] ; =
B
B
B : [S UPPORTED(y ) ^ ATTACHED(y; z )] ; ; ^
B
B 8 [2S UPPORTED(y ) ^ ATTACHED(x; y )]
3 9
B >
S UPPORTED (y ) ^ S UPPORTS (z; y )^
>
>
B >
>
> 4 C ONTACTS (y; z ) ^ : S UPPORTS(x; y )^
B >
5; >
=
B <
B
:
A
TTACHED
(
x; y ) ^ : ATTACHED(y; z )
^
B >
>
>
B >
[
S
UPPORTED
(
y
)
^
S
UPPORTS
(
z;
y
)]
;
>
>
>
B >
;
B : [S UPPORTED(y ) ^ ATTACHED(x; y )]
9
B 8
B > [S UPPORTED(y ) ^ S UPPORTS (z; y )] ;
>
>
B >
>
>
> [S UPPORTED(y ) ^ ATTACHED(x; y )] ;
B >
B < 2
3 =
B
S
UPPORTED
(
y ) ^ S UPPORTS (x; y )^
B >
>
5 >
@ >
>
> 4 ATTACHED(x; y ) ^ : S UPPORTS(z; y )^
>
>
:
;

3

3

3

3

P ICK U P (x; y; z )

4

=

3

3

3

3

3

3

:3C ONTACTS(y; z ) ^ :3ATTACHED(y; z )

P UT D OWN(x; y; z )

4

=

1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A

3 9
0 8 2
S UPPORTED(y ) ^ S UPPORTS(x; y ) ^ ATTACHED(x; y )^
>
>
>
>
> 4 : S UPPORTS(z; y ) ^ : C ONTACTS(y; z )^
>
B >
5; >
>
B >
>
>
=
B <
: ATTACHED(y; z )
B
^
B > S UPPORTED (y );
>
>
B >


>
>
>
B >
S UPPORTED(y ) ^ S UPPORTS (z; y ) ^ C ONTACTS(z; y )^
>
B >
>
>
;
B :
:
S
UPPORTS
(
x; y ) ^ :
A
TTACHED
(
x; y )
B 8 
9

B <
B
 S UPPORTED (y ) ^ ATTACHED(x; y ) ;
 =
@
S UPPORTED (y ) ^ ATTACHED(x; y ) ^ ATTACHED(y; z ) ;
:
;

3
3

3

3

3

S UPPORTED (y )

Figure 15: The learned 3-AMA definitions for P ICK U P (x; y; z ) and P UT D OWN (x; y; z ).

441

1
C
C
C
C
C
C
C
C
C
C
C
C
A

fiF ERN , G IVAN , & S ISKIND

0 8
>
>
B <
B
B >
B >
B :
B (
B
B
B
B
B (
B
B
B
B
B (
B
B
B
B
B (
B
B
B
B
B (
B
B
B
B
B 8
B <
B
B
B :
B
B (
B
B
B
B
B (
B
B
B
B
B 8
B
B <
B
B
B :
B 8
B
B <
B
B
B :
B
B (
B
B
B
B
B 8
B <
B
B
B :
B
B 8
B >
B >
B <
B
@ >
>
:

h

^

^

^

^

S UPPORTED(y ) ATTACHED(w; x) S UPPORTS(z; y ) C ONTACTS(y; z )
S UPPORTS(x; y )
S UPPORTS(y; x)
C ONTACTS(x; y )
ATTACHED(x; y )

:3

^ :3

^ :3

^ :3

i

;

9
>
>
=

1

C
C
i >^ C
S UPPORTED(y ) ^ S UPPORTED(x) ^ S UPPORTS(y; x) ^ C ONTACTS(x; y ) ^ C ONTACTS(y; z )^
>
; C
C
:3S UPPORTS(x; y) ^ :3ATTACHED(w; x) ^ :3ATTACHED(x; y) ^ :3)ATTACHED(y; z)
C
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
C
[S UPPORTED(y) ^ ATTACHED(x; y)] ;
^
C
C
[S UPPORTED(y) ^ S UPPORTED(x) ^ S UPPORTS(y; x) ^ C ONTACTS(x; y)]
)
C
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
[S UPPORTED(y) ^ S UPPORTS(x; y) ^ ATTACHED(w; x) ^ ATTACHED(x; y) ^ ATTACHED(y; z)] ; ^ C
C
C
[S UPPORTED(y) ^ S UPPORTED(x)S UPPORTS(y; x)]
)
C
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
C
[S UPPORTED(y) ^ S UPPORTED(x) ^ S UPPORTS(x; y) ^ S UPPORTS(y; x) ^ ATTACHED(w; x)] ; ^
C
C
[S UPPORTED(y) ^ S UPPORTED(x) ^ S UPPORTS(y; x)]
)
C
C
[S UPPORTED(y) ^ ATTACHED(w; x) ^ S UPPORTS(z; y) ^ C ONTACTS(y; z)] ;
C
[S UPPORTED(y) ^ ATTACHED(y; z)] ;
^
C
C
[S UPPORTED(y) ^ S UPPORTED(x) ^ S UPPORTS(y; x) ^ C ONTACTS(y; z)] )
C
C
[S UPPORTED(y) ^ ATTACHED(w; x) ^ S UPPORTS(z; y) ^ C ONTACTS(y; z)] ;
C
[S UPPORTED(y) ^ ATTACHED(w; x) ^ ATTACHED(y; z)] ;
^
C
C
[hS UPPORTED(y) ^ S UPPORTED(x) ^ S UPPORTS(y; x)]
i 9
C
S UPPORTED(y ) ^ ATTACHED(w; x) ^ S UPPORTS(z; y ) ^ C ONTACTS(y; z )^
C
=
;
C
:3S UPPORTS(x; y) ^ :3S UPPORTS(y; x) ^ :3C ONTACTS(x; y) ^ :3ATTACHED(x; y)
C
^
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
;
C
[S UPPORTED(y) ^ S UPPORTED(x) ^ S UPPORTS(y; x)]
C
)
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
C
[S UPPORTED(y) ^ ATTACHED(w; x) ^ S UPPORTS(z; y) ^ C ONTACTS(y; z)] ; ^
C
C
[S UPPORTED(y) ^ S UPPORTED(x)]
C
)
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
C
[S UPPORTED(y) ^ ATTACHED(w; x) ^ S UPPORTS(z; y) ^ S UPPORTED(x)] ; ^
C
C
[S UPPORTED(y) ^ S UPPORTED(x)]
C
9
C
[hS UPPORTED(y) ^ ATTACHED(w; x)] ;
i =
C
S UPPORTED(y ) ^ C ONTACTS(y; z ) ^ S UPPORTS(z; y ) ^ S UPPORTED(x)^
C
;
^
C
:3S UPPORTS(x; y) ^ :3ATTACHED(x; y)
;
C
C
[S UPPORTED(y) ^ S UPPORTED(x)]
9
C
S UPPORTED(y );
C
h
i =
C
S UPPORTED(y ) ^ C ONTACTS(y; z ) ^ S UPPORTS(z; y ) ^ S UPPORTED(x)^
^
;
C
:3S UPPORTS(x; y) ^ :3ATTACHED(x; y) ^ :3ATTACHED(y; z)
C
;
C
[S UPPORTED(y) ^ S UPPORTED(x) ^ S UPPORTS(y; x)] )
C
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
C
[S UPPORTED(y) ^ C ONTACTS(y; z) ^ S UPPORTED(x)] ; ^
C
C
[S UPPORTED(y) ^ S UPPORTED(x) ^ S UPPORTED(y)x]
9 C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
= C
C
[hS UPPORTED(y) ^ S UPPORTED(x) ^ S UPPORTS(y; x)] ;
i
^C
S UPPORTED(y ) ^ S UPPORTED(x) ^ S UPPORTS(y; x) ^ C ONTACTS(x; y ) ^ C ONTACTS(y; z )^
; C
C
:3S UPPORTS(x; y) ^ :3ATTACHED(w; x) ^ :3ATTACHED(x; y) ^ :3ATTACHED(y; z)
9 C
S
UPPORTED(y );
> C
h
i
>
= C
S UPPORTED(y ) ^ S UPPORTED(x) ^ S UPPORTS(y; x) ^ S UPPORTS(z; y )^
C
;
C
C
ONTACTS(x; y ) ^ C ONTACTS(y; z )
h
i > A
S UPPORTED(y ) ^ S UPPORTED(x) ^ S UPPORTS(y; x) ^ C ONTACTS(x; y ) ^ C ONTACTS(y; z )^
>
;

[S UPPORTED(y)] ;
h

:3S UPPORTS(x; y) ^ :3ATTACHED(w; x) ^ :3ATTACHED(x; y) ^ :3ATTACHED(y; z)

Figure 16: The learned 3-AMA definition for S TACK (w; x; y; z ).

442

fiL EARNING T EMPORAL E VENTS

0 8
>
>
B >
>
B >
>
B >
>
B <
B
B >
B >
>
B >
B >
>
>
B >
B :
B
B (
B
B
B
B
B (
B
B
B
B
B
B (
B
B
B
B
B (
B
B
B
B
B 8
B
B >
>
B >
>
B >
B <
B
B >
B >
>
B >
:
B >
B 8
B
B >
>
B >
>
B >
B <
B
B >
B >
>
B >
>
B :
B
B (
B
B
B
B
B 8
B
B >
<
B
B
B >
B :
B
B (
B
B
B
B
B (
B
B
B
B
B
B 8
B >
B <
B
@
>
:

"

#

9

S UPPORTED(x) ^ S UPPORTED(y ) ^ S UPPORTS(y; x)^
>
>
>
;
C ONTACTS(x; y ) ^ C ONTACTS(y; z ) ^ :3S UPPORTS(w; x)^
>
>
>
>
:3S UPPORTS(x; y) ^ :3ATTACHED(w; x) ^ :3ATTACHED(x; y)
>
=
[2S UPPORTED(x) ^ S UPPORTED(y)] ;
3
^
S UPPORTED(x) ^ S UPPORTED(y ) ^ ATTACHED(w; x) ^ S UPPORTS(z; y )^
>
>
>
6 C ONTACTS(y; z ) ^ ATTACHED(w; x) ^ :3S UPPORTS(x; y )^
7 >
>
4
5 >
>
:3S UPPORTS(y; x) ^ :3C ONTACTS(x; y)^
>
;
:3ATTACHED(x; y) ^ :3ATTACHED(y; z)
)
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x)] ;
[S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(w; x) ^ ATTACHED(y; z)] ; ^
[S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(w; x) ^ C ONTACTS(y; z)] )
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x) ^ C ONTACTS(y; z)] ;
[S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(y; z)] ;
^
[S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(w; x) ^ C ONTACTS(y; z)] )
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x) ^ C ONTACTS(x; y)] ;
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x) ^ ATTACHED(x; y)] ; ^
[S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(w; x)] )
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x)] ;
[S UPPORTED(x) ^ S UPPORTED(y) ^ C ONTACTS(y; z)] ; ^
[S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(w; x)]
9
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x)] ;
>
>
>
>
[2S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(w; x)] ;
3 >
=
S UPPORTED(x) ^ S UPPORTED(y ) ^ ATTACHED(w; x) ^ S UPPORTS(z; y )^
^
7 >
6 C ONTACTS(y; z ) ^ ATTACHED(w; x) ^ :3S UPPORTS(x; y )^
5 >
4
>
>
:3S UPPORTS(y; x) ^ :3C ONTACTS(x; y)^
>
;
ATTACHED(x; y ) ^ :3ATTACHED(y; z )
:
3
2
3 9
S UPPORTED(x) ^ S UPPORTED(y ) ^ S UPPORTS(y; x)^
>
>
>
6 C ONTACTS(x; y ) ^ C ONTACTS(y; z )^
7 >
>
4
5; =
:3S UPPORTS(w; x) ^ :3S UPPORTS(x; y)^
^
:3ATTACHED(w; x) ^ :3ATTACHED(x; y)
>
>
>
>
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x)] ;
>
;
[S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(w; x)]
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x) ^ C ONTACTS(y; z)] ; )
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x) ^ ATTACHED(y; z)] ; ^
[S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(w; x)]
9
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x)] ;

 >
=
S UPPORTED(x) ^ S UPPORTED(y ) ^ S UPPORTS(y; x) ^ ATTACHED(y; z )^
^
;
S UPPORTS(x; y ) ^ ATTACHED(w; x) ^ ATTACHED(x; y )
>
;
[S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(w; x)]
)
[S UPPORTED(x) ^ S UPPORTED(y)] ;
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x) ^ ATTACHED(w; x)] ; ^
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(w; x) ^ ATTACHED(w; x)] )
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x)] ;
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(w; x) ^ ATTACHED(w; x)] ; ^
[S UPPORTED(x) ^ S UPPORTED(y) ^ ATTACHED(w; x)]
9
[S UPPORTED(x) ^ S UPPORTED(y) ^ S UPPORTS(y; x)] ;

 >
=
S UPPORTED(x) ^ S UPPORTED(y ) ^ C ONTACTS(y; z )^
;
:3S UPPORTS(x; y) ^ :3ATTACHED(x; y) ^ :3ATTACHED(y; z)
>
;
[S UPPORTED(x) ^ S UPPORTED(y)]

Figure 17: The learned 3-AMA definition for U NSTACK (w; x; y; z ).

443

1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A

fiF ERN , G IVAN , & S ISKIND

0 8
>
>
>
B >
>
B >
>
B >
>
B >
B <
B
B >
B >
>
B >
>
B >
>
B >
>
B >
B :
B 8
B >
B <
B
B
B >
B :
B 8
B >
B <
B
B
B >
B :
B 8
B >
B <
B
B
B >
B :
B 8
B >
B <
B
B
B >
B :
B 8
B >
B <
B
B
B >
B :
B 8
B >
B <
B
@
>
:

2
6
4

S UPPORTED (x) ^ S UPPORTS (y; x) ^ C ONTACTS (y; x)^
:3S UPPORTS(w; x) ^ :3S UPPORTS(z; x) ^ :3C ONTACTS(x; z)^
:3ATTACHED(w; x) ^ :3ATTACHED (y; x) ^ :3ATTACHED (x; z)

3
7
5

S UPPORTED (x);
S UPPORTED (x) ^ S UPPORTS (z; x) ^ C ONTACTS (x; z )^
6
:
4 3S UPPORTS (w; x) ^ :3S UPPORTS (y; x) ^ :3C ONTACTS (y; x)^
:3ATTACHED(w; x) ^ :3ATTACHED9(y; x) ^ :3ATTACHED (x; z)
[S UPPORTED (x) ^ S UPPORTS (y; x)] ; >
=
[S UPPORTED (x) ^ ATTACHED (w; x)] ; > ^
;
S UPPORTED (x)
9
>
S UPPORTED (x);
=
[S UPPORTED (x) ^ ATTACHED (w; x) ^ ATTACHED (x; z )] ; > ^
;
S UPPORTED (x)
9
>
[S UPPORTED (x)] ;
=
[S UPPORTED (x) ^ ATTACHED (x; z )] ; > ^
[S UPPORTED (x) ^ C ONTACTS (x; z )] ;
9
>
S UPPORTED (x);
=
[S UPPORTED (x) ^ ATTACHED (w; x) ^ S UPPORTS (w; x)] ; > ^
;
S UPPORTED (x)
9
>
S UPPORTED (x);
=
[S UPPORTED (x) ^ ATTACHED (w; x) ^ ATTACHED (y; x)] ; > ^
;
S UPPORTED (x)
9
[S UPPORTED (x) ^ C ONTACTS (y; x)] ; >
=
[S UPPORTED (x) ^ ATTACHED (y; x)] ; >
;
S UPPORTED (x)
2

Figure 18: The learned 3-AMA definition for M OVE (w; x; y; z ).

444

3
7
5

;

9
>
>
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
>
>
;

1

^

C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A

fiL EARNING T EMPORAL E VENTS

0 8
>
>
>
B >
>
>
B >
B >
>
>
B <
B
B
B >
B >
>
B >
>
B >
>
B >
>
B >
:
B
B 8
B >
B >
B >
>
B >
<
B
B
B >
B >
>
B >
>
B :
B 8
B
B >
B <
B
B >
B :
B 8
B
B >
B <
B
B >
B :
B 8
B
B >
B <
B
B >
B :
B 8
B
B >
B <
@
>
:

2

3

9

:3S UPPORTED (x) ^ :3S UPPORTS(z; y) ^ :3S UPPORTS(y; x)^ 7 >
>
>
6
>
:
4 3C ONTACTS (x; y ) ^ :3C ONTACTS (z; y )^
5; >
>
>
>
>
>
:3ATTACHED(w; x) ^ :3ATTACHED (z; y)
=

true
;
2
6
4
2
6
4

3

S UPPORTED (x) ^ S UPPORTED (y ) ^ S UPPORTS (z; y )^
7
S UPPORTS (y; x) ^ C ONTACTS (x; y )^
5
C ONTACTS (z; y ) ^ :3ATTACHED (w; y )
:3S UPPORTED (x) ^ :3S UPPORTS(z; y) ^ :3S UPPORTS(y; x)^
:3C ONTACTS(x; y) ^ :3C ONTACTS(z; y)^
:3ATTACHED(w; x) ^ :3ATTACHED (z; y)

ATTACHED (w; y );
S UPPORTED (y )

9
>
=

true;

3
7
5

;

>
>
>
>
>
>
>
>
>
>
;
9
>
>
>
>
>
=
>
>
>
>
>
;

[S UPPORTED (y) ^ :3ATTACHED (w; x) ^ :3ATTACHED (z; y)] ; > ^
;
S UPPORTED (y )
9
>
true;
=
[S UPPORTED (y) ^ ATTACHED (z; y)] ; > ^
[S UPPORTED (y) ^ C ONTACTS (z; y)] ;
true;
[S UPPORTED (y) ^ S UPPORTS (z; y)C ONTACTS (z; y) ^ ATTACHED (w; x)] ;
S UPPORTED (y )
9
>
true;
=
[S UPPORTED (y) ^ ATTACHED (w; y)ATTACHED (z; y)] ; >
;
S UPPORTED (y )
Figure 19: The learned 3-AMA definition for A SSEMBLE (w; x; y; z ).

445

1

^

^

9
>
=
>
;

^

C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A

fiF ERN , G IVAN , & S ISKIND

0 8
>
>
B >
>
B >
>
B >
>
B >
>
B <
B
B >
B >
B >
>
B >
>
B >
>
B >
>
B :
B 8
B >
B >
B <
B
B >
B >
B :
B 8
B >
B >
B <
B
B >
B >
B :
B 8
B >
B >
B <
B
B >
B >
B :
B 8
B >
B >
B <
B
B >
B >
B :
B 8
B <
B
B
B :
B
B 8
B <
B
@
:

3

2

9

S UPPORTED (x) ^ S UPPORTED(y ) ^ S UPPORTS(y; x) ^ S UPPORTS (z; y )^
>
>
7 >
>
6 C ONTACTS (x; y ) ^ C ONTACTS(z; y ) ^ : S UPPORTS(w; x)^
7; >
>
6
>
5 >
4 : S UPPORTS(w; y ) ^ : S UPPORTS(x; y ) ^ : ATTACHED(x; w)^
>
>
=
: ATTACHED(w; y) ^ : ATTACHED(x; y) ^ : ATTACHED(z; y)
^
S UPPORTED(y );
>
>
2
3
>
>
S UPPORTED (y ) ^ : S UPPORTED(x) ^ : S UPPORTS(w; x)^
>
>
>
>
4 : S UPPORTS(z; y ) ^ : S UPPORTS(y; x) ^ : C ONTACTS(x; y )^ 5 ;
>
>
;
: C ONTACTS(z; y) ^ : ATTACHED(x; w) ^ : ATTACHED9(z; y)
[ S UPPORTED (x) ^ S UPPORTED (y )] ;
>

 >
=
S UPPORTED(x) ^ S UPPORTED (y ) ^ S UPPORTS (w; x)^
^
;
S UPPORTS(z; y ) ^ C ONTACTS (z; y ) ^ ATTACHED(x; w)
>
>
;
S UPPORTED(y )
9


S UPPORTED(x) ^ S UPPORTED (y ) ^ S UPPORTS (z; y )^
>
>
;
=
S UPPORTS(y; x) ^ C ONTACTS (x; y ) ^ C ONTACTS(z; y )
^
[ S UPPORTED (x) ^ S UPPORTED (y ) ^ S UPPORTS (y; x) ^ ATTACHED (x; y )] ; >
>
;
S UPPORTED(y )
9
[ S UPPORTED (x) ^ S UPPORTED (y ) ^ S UPPORTS (y; x) ^ C ONTACTS (z; y )] ; >
>


=
S UPPORTED(x) ^ S UPPORTED (y ) ^ S UPPORTS (x; y )^
;
^
S UPPORTS(y; z ) ^ ATTACHED(x; y ) ^ ATTACHED(z; y )
>
>
;
S UPPORTED(y )
9
[ S UPPORTED (x) ^ S UPPORTED (y ) ^ S UPPORTS (y; x)] ;
>
 >

=
S UPPORTED(x) ^ S UPPORTED (y ) ^ S UPPORTS (x; y )^
;
^
S UPPORTS(y; z ) ^ ATTACHED(x; y ) ^ ATTACHED(z; y ) ^ ATTACHED(x; w)
>
>
;
S UPPORTED(y )
9
S UPPORTED(y );
=
[ S UPPORTED (y ) ^ ATTACHED (w; y ) ^ ATTACHED (z; y )] ;
^
;
S UPPORTED(y )
9
S UPPORTED(y );
=
[ S UPPORTED (y ) ^ S UPPORTS (w; y ) ^ ATTACHED (w; y )] ;
;
S UPPORTED(y )

3
3

3
3

3
3

3

3
3

3

3

3
3

3
3

Figure 20: The learned 3-AMA definition for D ISASSEMBLE (w; x; y; z ).

446

1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A

fiL EARNING T EMPORAL E VENTS

References
Agrawal, R., & Srikant, R. (1995). Mining sequential patterns. In Proceedings of the Eleventh
International Conference on Data Engineering, pp. 314.
Allen, J. F. (1983). Maintaining knowledge about temporal intervals. Communications of the ACM,
26(11), 832843.
Angluin, D. (1987). Learning regular sets from queries and counterexamples. Information and
Computation, 75, 87106.
Bacchus, F., & Kabanza, F. (2000). Using temporal logics to express search control knowledge for
planning. Artificial Intelligence, 16, 123191.
Bobick, A. F., & Ivanov, Y. A. (1998). Action recognition using probabilistic parsing. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
pp. 196202, Santa Barbara, CA.
Borchardt, G. C. (1985). Event calculus. In Proceedings of the Ninth International Joint Conference
on Artificial Intelligence, pp. 524527, Los Angeles, CA.
Brand, M. (1997a). The inverse Hollywood problem: From video to scripts and storyboards via
causal analysis. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pp. 132137, Providence, RI.
Brand, M. (1997b). Physics-based visual understanding. Computer Vision and Image Understanding, 65(2), 192205.
Brand, M., & Essa, I. (1995). Causal analysis for visual gesture understanding. In Proceedings of
the AAAI Fall Symposium on Computational Models for Integrating Language and Vision.
Brand, M., Oliver, N., & Pentland, A. (1997). Coupled hidden Markov models for complex action
recognition. In Proceedings of the IEEE Computer Society Conference on Computer Vision
and Pattern Recognition.
Cohen, P. (2001). Fluent learning: Elucidating the structure of episodes. In Proceedings of the
Fourth Symposium on Intelligent Data Analysis.
Cohen, W. (1994). Grammatically biased learning: Learning logic programs using an explicit antecedent description lanugage. Artificial Intelligence, 68, 303366.
Cohen, W., & Hirsh, H. (1994). Learning the CLASSIC description logic: Theoretical and experimental results. In Proceedings of the Fourth International Conference on Principles of Knowledge
Representation and Reasoning, pp. 121133.
De Raedt, L., & Dehaspe, L. (1997). Clausal discovery. Machine Learning, 26, 99146.
Dehaspe, L., & De Raedt, L. (1996). DLAB: A declarative language bias formalism. In Proceedings
of the Ninth International Syposium on Methodologies for Intelligent Systems, pp. 613622.
Fikes, R., & Nilsson, N. (1971). STRIPS: A new approach to the application of theorem proving to
problem solving. Artificial Intelligence, 2(3/4).
Hoppner, F. (2001). Discovery of temporal patternsLearning rules about the qualitative behaviour
of time series. In Proceedings of the Fifth European Conference on Principles and Practice
of Knowledge Discovery in Databases.
447

fiF ERN , G IVAN , & S ISKIND

Kam, P., & Fu, A. (2000). Discovering temporal patterns for interval-based events. In Proceedings
of the Second International Conference on Data Warehousing and Knowledge Discovery.
Klingspor, V., Morik, K., & Rieger, A. D. (1996). Learning concepts from sensor data of a mobile
robot. Artificial Intelligence, 23(2/3), 305332.
Lang, K., Pearlmutter, B., & Price, R. (1998). Results of the Abbadingo one DFA learning competition and a new evidence-driven state merging algorithm. In Proceedings of the Fourth
International Colloquium on Grammatical Inference.
Lavrac, N., Dzeroski, S., & Grobelnik, M. (1991). Learning nonrecursive definitions of relations
with LINUS. In Proceedings of the Fifth European Working Session on Learning, pp. 265
288.
Mann, R., & Jepson, A. D. (1998). Toward the computational perception of action. In Proceedings
of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp.
794799, Santa Barbara, CA.
Mannila, H., Toivonen, H., & Verkamo, A. I. (1995). Discovery of frequent episodes in sequences.
In Proceedings of the First International Conference on Knowledge Discovery and Data Mining.
Mitchell, T. (1982). Generalization as search. Artificial Intelligence, 18(2), 51742.
Morales, E. (1997). Pal: A pattern-based first-order inductive system. Machine Learning, 26, 227
252.
Muggleton, S. (1995). Inverting entailment and Progol. Machine Intelligence, 14, 133188.
Muggleton, S., & Feng, C. (1992). Efficient induction of logic programs. In Muggleton, S. (Ed.),
Inductive Logic Programming, pp. 281298. Academic Press.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory and methods. Journal
of Logic Programming, 19/20, 629679.
Pinhanez, C., & Bobick, A. (1995). Scripts in machine understanding of image sequences. In
Proceedings of the AAAI Fall Symposium Series on Computational Models for Integrating
Language and Vision.
Plotkin, G. D. (1971). Automatic Methods of Inductive Inference. Ph.D. thesis, Edinburgh University.
Regier, T. P. (1992). The Acquisition of Lexical Semantics for Spatial Terms: A Connectionist Model
of Perceptual Categorization. Ph.D. thesis, University of California at Berkeley.
Roth, D., & Yih, W. (2001). Relational learning via propositional algorithms: An information extraction case study. In Proeedings of the Seventeenth International Joint Conference on Artificial
Intelligence.
Shoham, Y. (1987). Temporal logics in AI: Semantical and ontological considerations. Artificial
Intelligence, 33(1), 89104.
Siskind, J. M. (2000). Visual event classification via force dynamics. In Proceedings of the Seventeenth National Conference on Artificial Intelligence, pp. 149155, Austin, TX.
Siskind, J. M. (2001). Grounding the lexical semantics of verbs in visual perception using force
dynamics and event logic. Journal of Artificial Intelligence Research, 15, 3190.
448

fiL EARNING T EMPORAL E VENTS

Siskind, J. M., & Morris, Q. (1996). A maximum-likelihood approach to visual event classification. In Proceedings of the Fourth European Conference on Computer Vision, pp. 347360,
Cambridge, UK. Springer-Verlag.
Talmy, L. (1988). Force dynamics in language and cognition. Cognitive Science, 12, 49100.
Yamoto, J., Ohya, J., & Ishii, K. (1992). Recognizing human action in time-sequential images using
hidden Markov model. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 379385.

449

fiJournal of Artificial Intelligence Research 17 (2002) 1-33

Submitted 8/01; published 7/02

A Critical Assessment of
Benchmark Comparison in Planning
Adele E. Howe
Eric Dahlman

Computer Science Department
Colorado State University, Fort Collins, CO 80523

howe@cs.colostate.edu
dahlman@cs.colostate.edu

Abstract
Recent trends in planning research have led to empirical comparison becoming commonplace. The field has started to settle into a methodology for such comparisons, which
for obvious practical reasons requires running a subset of planners on a subset of problems.
In this paper, we characterize the methodology and examine eight implicit assumptions
about the problems, planners and metrics used in many of these comparisons. The problem assumptions are: PR1) the performance of a general purpose planner should not be
penalized/biased if executed on a sampling of problems and domains, PR2) minor syntactic
differences in representation do not affect performance, and PR3) problems should be solvable by STRIPS capable planners unless they require ADL. The planner assumptions are:
PL1) the latest version of a planner is the best one to use, PL2) default parameter settings
approximate good performance, and PL3) time cut-offs do not unduly bias outcome. The
metrics assumptions are: M1) performance degrades similarly for each planner when run
on degraded runtime environments (e.g., machine platform) and M2) the number of plan
steps distinguishes performance. We find that most of these assumptions are not supported
empirically; in particular, that planners are affected differently by these assumptions. We
conclude with a call to the community to devote research resources to improving the state
of the practice and especially to enhancing the available benchmark problems.

1. Introduction
In recent years, comparative evaluation has become increasingly common for demonstrating
the capabilities of new planners. Planners are now being directly compared on the same
problems taken from a set of domains. As a result, recent advances in planning have
translated to dramatic increases in the size of the problems that can be solved (Weld,
1999), and empirical comparison has highlighted those improvements.
Comparative evaluation in planning has been significantly inuenced and expedited by
the Artificial Intelligence Planning and Scheduling (AIPS) conference competitions. These
competitions have had the dual effect of highlighting progress in the field and providing
a relatively unbiased comparison of state-of-the-art planners. When individual researchers
compare their planners to others, they include fewer other planners and fewer test problems
because of time constraints.
To support the first competition in 1998 (McDermott, 2000), Drew McDermott defined,
with contributions from the organizing committee, a shared problem/domain definition
language, PDDL (McDermott et al., 1998) (Planning Domain Definition Language). Using

c 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHowe & Dahlman

a common language means that planners' performance can be directly compared, without
entailing hand translation or factoring in different representational capabilities.
As a second benefit, the lack of translation (or at least human accomplished translation) meant that performance could be compared on a large number of problems and
domains1. In fact, the five competition planners were given a large number of problems
(170 problems for the ADL track and 165 for the STRIPS track) within seven domains,
including one domain that the planner developers had never seen prior to the competition.
So the first competition generated a large collection of benchmarks: seven domains used in
the competition plus 21 more that were considered for use. All 28 domains are available
at ftp://ftp.cs.yale.edu/pub/mcdermott/domains/. The second competition added three
novel domains to that set.
A third major benefit of the competitions is that they appear to have motivated researchers to develop systems that others can use. The number of entrants went from five in
the first competition to 16 in the second. Additionally, all of the 1998 competitors and six
out of sixteen of the 2000 competitors made their code available on web sites. Thus, others
can perform their own comparisons.
In this paper, we describe the current practice of comparative evaluation as it has evolved
since the AIPS competitions and critically examine some of the underlying assumptions
of that practice. We summarize existing evidence about the assumptions and describe
experimental tests of others that had not previously been considered. The assumptions
are organized into three groups concerning critical decisions in the experiment design: the
problems tested, the planners included and the performance metrics collected.
Comparisons (as part of competitions or by specific researchers) have proven to be enormously useful to motivating progress in the field. Our goal is to understand the assumptions
so that readers know how far the comparative results can be generalized. In contrast to the
competitions, the community cannot legislate fairness in individual researcher's comparative evaluations, but readers may be able to identify cases in which results should be viewed
either skeptically or with confidence. Thus, we conclude the paper with some observations
and a call for considerably more research into new problems, metrics and methodologies to
support planner evaluation.
Also in contrast to the competitions, our goal is not to declare a winner. Our goal is
also not to critique individual studies. Consequently, to draw attention away from such a
possible interpretation, whenever possible, we report all results using letter designators that
were assigned randomly to the planners.

2. Planning Competitions and Other Direct Comparisons

Recently, the AIPS competitions have spurred considerable interest in comparative evaluation. The roots of comparative planner evaluation go back considerably further, however.
Although few researchers were able to run side-by-side comparisons of their planners with
1. To solve a particular planning problem (i.e., construct a sequence of actions to transform an initial state to
a goal state), planners require a domain theory and a problem description. The domain theory represents
the abstract actions that can be executed in the environment; typically, the domain descriptions include
variables that can be instantiated to specific objects or values. Multiple problems can be defined for
each domain; problem descriptions require an initial state description, a goal state and an association
with some domain.

2

fiA Critical Assessment of Benchmark Comparison in Planning

others, they were able to demonstrate performance of their planner on well-known problems, which could be viewed as de facto benchmarks. Sussman's anomaly (Sussman, 1973)
in Blocksworld was the premier planning benchmark problem and domain for many years;
every planner needed to \cut its teeth" on it.
As researchers tired of Blocksworld, many called for additional benchmark problems
and environments. Mark Drummond, Leslie Kaelbling and Stanley Rosenschein organized
a workshop on benchmarks and metrics (Drummond, Kaelbling, & Rosenschein, 1990).
Testbed environments, such as Martha Pollack's TileWorld (Pollack & Ringuette, 1990) or
Steve Hanks's TruckWorld (Hanks, Nguyen, & Thomas, 1993), were used for comparing
algorithms within planners. By 1992, UCPOP (Penberthy & Weld, 1992) was distributed
with a large set of problems (117 problems in 21 domains) for demonstration purposes. In
1995, Barry Fox and Mark Ringer set up a planning and scheduling benchmarks web page
(http://www.newosoft.com/~benchmrx/) to collect problem definitions, with an emphasis
on manufacturing applications. Recently, PLANET (a coordinating organization for European planning and scheduling researchers) has proposed a planning benchmark collection
initiative (http://planet.dfki.de).
Clearly, benchmark problems have become well-established means for demonstrating
planner performance. However, the practice has known benefits and pitfalls; Hanks, Pollack
and Cohen (1994) discuss them in some detail in the context of agent architecture design.
The benefits include providing metrics for comparison and supporting experimental control.
The pitfalls include a lack of generality in the results and a potential for the benchmarks to
unduly inuence the next generation of solutions. In other words, researchers will construct
solutions to excel on the benchmarks, regardless of whether the benchmarks accurately
represent desired real applications.
To obtain the benefits just listed for benchmarks, the problems often are idealized or
simplified versions of real problems. As Cohen (1991) points out , most research papers in
AI, or at least at an AAAI conference, exploit benchmark problems; yet few of them relate
the benchmarks to target tasks. This may be a significant problem; for example, in a study
of owshop scheduling2 benchmarks, we found that performance on the standard benchmark set did not generalize to performance on problems with realistic structure (Watson,
Barbulescu, Howe, & Whitley, 1999). A study of just Blocksworld problems found that the
best known Blocksworld benchmark problems are atypical in that they require only short
plans for solution and optimal solutions are easy to find (Slaney & Thiebaux, 2001).
In spite of these diculties, benchmark problems and the AIPS competitions have considerably inuenced comparative planner evaluations. For example, in the AIPS 2000 conference proceedings (Chien, Kambhampati, & Knoblock, 2000), all of the papers on improvements to classical planning (12 out of 44 papers at the conference) relied heavily on
comparative evaluation using benchmark problems; the other papers concerned scheduling,
specific applications, theoretical analyses or special extensions to the standard paradigm
(e.g., POMDP, sensing). Of the 12 classical papers, six used problems from the AIPS98
competition benchmark set, six used problems from Kautz and Selman's distribution of
problems with blackbox (Kautz, 2002) and three added some of their own problems as
well. Each paper showed results on a subset of problems from the benchmark distributions
2. Scheduling is an area related to planning in which the actions are already known, but their sequence still
needs to be determined. Flowshop scheduling is a type of manufacturing scheduling problem.

3

fiHowe & Dahlman

(e.g., Drew McDermott's from the first competition) with logistics, blocksworld, rocket and
gripper domains being most popular (used in 11, 7, 5 and 5 papers, respectively). The availability of planners from the competition was also exploited; eight of the papers compared
their systems to other AIPS98 planners: blackbox, STAN, IPP and HSP (in 5, 3, 3 and 1
papers, respectively).

3. Assumptions of Direct Comparison
A canonical planner evaluation experiment follows the procedure in Table 1. The procedure
is designed to compare performance of a new planner to the previous state of the art and
highlight superior performance in some set of cases for the new planner. The exact form
of an experiment depends on its purpose, e.g., showing superiority on a class of problem or
highlighting the effect of some design decision.
1. Select and/or construct a subset of planner domains
2. Construct problem set by:
 running large set of benchmark problems
 selecting problems with desirable features
 varying some facet of the problem to increase diculty (e.g., number of blocks)
3. Select other planners that are:
 representative of the state of the art on the problems OR
 similar to or distinct from the new planner, depending on the point of the comparison or advance of the new planner OR
 available and able to parse the problems
4. Run all problems on all planners using default parameters and setting an upper limit
on time allowed
5. Record which problems were solved, how many plan steps/actions were in the solution
and how much CPU time was required to either solve the problem, fail or time out
Table 1: Canonical comparative planner evaluation experiment.
The protocol depends on three selections: problems, planners and evaluation metrics.
It is simply not practical or even desirable to run all available planners on all available
problems. Thus, one needs to make informed decisions about which to select. A purpose
of this paper is to examine the assumptions underlying these decisions to help make them
more informed. Every planner comparison does not adopt every one of these assumptions,
but the assumptions are ones commonly found in planner comparisons. For example, those
comparisons designed for a specific purpose (e.g., to show scale-up on certain problems
or suitability of the planner for logistics problems) will carefully select particular types of
problems from the benchmark sets.
4

fiA Critical Assessment of Benchmark Comparison in Planning

Problems Many planning systems were developed to solve a particular type of planning
problem or explore a specific type of algorithmic variation. Consequently, one would expect
them to perform better on the problems on which and for which they were developed. Even
were they not designed for a specific purpose, the test set used during development may have
subtly biased the development. The community knows that planner performance depends
on problem features, but not in general, how, when and why. Researchers tend to design
planners to be general purpose. Consequently, comparisons assume that
the performance of a general-purpose planner should not be penalized/biased if
executed on a sampling of problems and domains (problem assumption 1).

The community also knows that problem representation inuences planner performance.
For example, benchmark problem sets include many versions of Blocksworld problems, designed by different planner developers. These versions vary in their problem representation,
both minor apparently syntactic changes (e.g., how clauses are ordered within operators,
initial conditions and goals, and whether any information is extraneous) and changes reecting addition of domain knowledge (e.g., what constraints are included and whether
variables are typed). Consequently, comparisons assume that
syntactic representational modifications either do not matter or affect each planner equally (problem assumption 2).

PDDL includes a field, :requirements, for the capabilities required of a planner to solve
the problem. PDDL1.0 defined 21 values for the :requirements field; the base/default requirement is :strips, meaning STRIPS derived add and delete sets for action effects. :adl
(from Pednault's Action Description Language) requires variable typing, disjunctive preconditions, equality as a built-in predicate, quantified preconditions and conditional effects
in addition the :strips capability. Yet, many planners either ignore the :requirements
field or reject the problem only if it specifies :adl (ignoring many of the other requirements
that could also cause trouble). Thus, comparisons assume that
problems in the benchmark set should be solvable by a STRIPS planner unless
they require :adl (problem assumption 3).

Planners The wonderful trend of making planners publicly available has led to a dilemma
in determining which to use and how to configure them. The problem is compounded by the
longevity of some of these planner projects; some projects have produced multiple versions.
Consequently, comparisons tend to assume that
the latest version of the planner is the best (planner assumption 1).

These planners may also include parameters. For example, the blackbox planner allows the
user to define a strategy for applying different solution methods. Researchers expect that
parameters affect performance. Consequently, comparisons assume that
default parameter settings approximate good performance (planner assumption
2).
5

fiHowe & Dahlman

Experiments invariably use time cut-offs for concluding planning that has not yet found
a solution or declared failure. Many planners would need to exhaustively search a large space
to declare failure. For practical reasons, a time out threshold is set to determine when to
halt a planner, with a failure declared when the time-out is reached. Thus, comparisons
assume that
if one picks a suciently high time-out threshold, then it is highly unlikely that
a solution would have been found had slightly more time been granted (planner
assumption 3).

Metrics Ideally, performance would be measured based on how well the planner does

its job (i.e., constructing the `best' possible plan to solve the problem) and how eciently
it does so. Because no planner has been shown to solve all possible problems, the basic
metric for performance is the number or percentage of problems actually solved within the
allowed time. This metric is commonly reported in the competitions. However, research
papers tend not to report it directly because they typically test a relatively small number
of problems.
Eciency is clearly a function of memory and effort. Memory size is limited by the
hardware. Effort is measured as CPU time, preferably but not always on the same platform
in the same language. The problems with CPU time are well known: programmer skill
varies; research code is designed more for fast prototyping than fast execution; numbers in
the literature cannot be compared to newer numbers due to processor speed improvements.
However, if CPU times are regenerated in the experimenter's environment then one assumes
that
performance degrades similarly with reductions in capabilities of the runtime
environment (e.g., CPU speed, memory size) (metric assumption 1).

In other words, an experimenter or user of the system does not expect that code has been
optimized for a particular compiler/operating system/hardware configuration, but it should
perform similarly when moved to another compatible environment.
The most commonly reported comparison metric is computation time. The second most
is number of steps or actions (for planners that allow parallel execution) in a plan. Although
planning seeks solutions to achieving goals, the goals are defined in terms of states of the
world, which does not lend itself well to general measures of quality. In fact, quality is likely
to be problem dependent (e.g., resource cost, amount of time to execute, robustness), which
is why number of plan steps has been favored. Comparisons assume that
number of steps in a resulting plan varies between planner solutions and approximates quality (metric assumption 2).

Any comparison, competitions especially, has the unenviable task of determining how to
trade-off or combine the three metrics (number solved, time, and number of steps). Thus,
if number of steps does not matter, then the comparison could be simplified.
We converted each assumption into a testable question. We then either summarized the
literature on the question or ran an experiment to test it.
6

fiA Critical Assessment of Benchmark Comparison in Planning

3.1 Our Experimental Setup
Some of the key issues have been examined previously, directly or indirectly. For those,
we simply summarize the results in the subsections that follow. However, some are open
questions. For those, we ran seven well known planners on a large set of 2057 benchmark
problems. The planners all accept the PDDL representation, although some have built-in
translators for PDDL to their internal representation and others rely on translators that we
added. When several versions of a planner were available, we included them all (for a total
of 13 planners). The basic problem set comprises the UCPOP benchmarks, the AIPS98 and
2000 competition test sets and an additional problem set developed for a specific application.
With the exception of the permuted problems (see the section on Problem Assumption
2 for specifics), the problems were run on 440 MHz Ultrasparc 10s with 256 Megabytes
of memory running SunOS 2.8. Whenever possible, versions compiled by the developers
were used; when only source code was available, we compiled the systems according to the
developers' instructions. The planners written in Common Lisp were run under Allegro
Common Lisp version 5.0.1. The other planners were compiled with GCC (EGCS version
2.91.66). Each planner was given a 30 minute limit of wall clock time3 to find a solution;
however, all times reported are run times returned by the operating system.
3.1.1 Planners

The planners are all what have been called primitive-action planners (Wilkins & desJardins,
2001), planners that require relatively limited domain knowledge and construct plans from
simple action descriptions. Because the AIPS98 competition required planners to accept
PDDL, the majority of planners used in this study were competition entrants or are later
versions thereof 4 . The common language facilitated comparison between the planners without having to address the effects of a translation step. The two exceptions were UCPOP and
Prodigy; however, their representations are similar to PDDL and were translated automatically. The planners represent five different approaches to planning: plan graph analysis,
planning as satisfiability, planning as heuristic search, state-space planning with learning
and partial order planning. When possible, we used multiple versions of a planner, and not
necessarily the most recent. Because we conducted this study over some period of time (almost 1.5 years), we froze the set early on; we are not comparing the performance to declare
a winner and so did not think that the lack of recent versions undermined the results of
testing our assumptions.

IPP (Koehler, Nebel, Hoffmann, & Dimopoulos, 1997) extends the Graphplan (Blum &

Furst, 1997) algorithm to accept a richer plan description language. In its early versions,
this language was a subset of ADL that extends the STRIPS formalism of Graphplan
to allow for conditional and universally quantified effects in operators. Until version 4.0,
negation was handled via the introduction of new predicates for the negated preconditions
3. We used actual time on lightly loaded machines because occasionally a system would thrash due to
inadequate memory resulting in little progress over considerable time.
4. We used the BUS system as the manager for running the planners (Howe, Dahlman, Hansen, Scheetz, &
von Mayrhauser, 1999), which was implemented with the AIPS98 competition planners. This facilitated
the running of so many different planners, but did somewhat bias what was included.

7

fiHowe & Dahlman

and corresponding mutual exclusion rules; subsequent versions handle it directly (Koehler,
1999). We used the AIPS98 version of IPP as well as the later 4.0 version.
SGP (Sensory Graph Plan) (Weld, Anderson, & Smith, 1998) also extends Graphplan to
a richer domain description language, primarily focusing on uncertainty and sensing. As
with IPP, some of this transformation is performed using expansion techniques to remove
quantification. SGP also directly supports negated preconditions and conditional effects.
SGP tends to be slower (it is implemented in Common Lisp instead of C) than some of the
other Graphplan based planners. We used SGP version 1.0b.
STAN (STate ANalysis) (Fox & Long, 1999) extends the Graphplan algorithm in part by
adding a preprocessor (called TIM) to infer type information about the problem and domain.
This information is then used within the planning algorithm to reduce the size of the search
space that the Graphplan algorithm would search. STAN also incorporated optimized data
structures (bit vectors of the planning graph) that help avoid many of the redundant calculations performed by Graphplan. Additionally, STAN maintains a wave front during graph
construction to track remaining goals and so limit graph construction. Subsequent versions
incorporated further analyses (e.g., symmetry exploitation) and an additional simpler planning engine. Four versions of STAN were tested: the AIPS98 competition version, version
3.0, version 3.0s and a development snapshot of version 4.0.
blackbox (Kautz & Selman, 1998) converts planning problems into Boolean satisfiability
problems, which are then solved using a variety of different techniques. The user indicates
which techniques should be tried in what order. In constructing the satisfiability problem,
blackbox uses the planning graph constructed as in Graphplan. For blackbox, we used
version 2.5 and version 3.6b.
HSP (Heuristic Search Planner) (Bonet & Geffner, 1999) is based on heuristic search. The
planner uses a variation of hill-climbing with random restarts to solve planning problems.
The heuristic is based on using the Graphplan algorithm to solve a relaxed form of the
planning problem. In this study, we used version 1.1, which is an algorithmic refinement of
the version entered into the AIPS98 competition, and version 2.0.
Prodigy 5 (The Prodigy Research Group, 1992) combines state-space planning with backward chaining from the goal state. A plan under construction consists of a head-plan of
totally ordered actions starting from the initial state and a tail-plan of partially ordered
actions related to the goal state. Although not ocially entered into the competition, informal results presented at the AIPS98 competition suggested that Prodigy performed well
in comparison to the entrants. We used Prodigy version 4.0.
UCPOP (Barrett, Golden, Penberthy, & Weld, 1993) is a Partial Order Causal Link
planner. The decision to include UCPOP was based on several factors. First, it does
not expand quantifiers and negated preconditions; for some domains, the expansion from
grounding operators can be so great as to make the problem insolvable. Second, UCPOP
is based on a significantly different algorithm in which interest has recently resurfaced. We
used UCPOP version 4.1.
5. We thank Eugene Fink for code that translates PDDL to Prodigy.

8

fiA Critical Assessment of Benchmark Comparison in Planning

Source
# of Domains # of Problems
Benchmarks
50
293
AIPS 1998
6
202
AIPS 2000
5
892
Developers
1
13
Application
3
72
Table 2: Summary of problems in our testing set: source of the problems, the number of
domains and problems within those domains.
3.1.2 Test Problems

Following standard practice, our experiments require planners to solve commonly available
benchmark problems and the AIPS competition problems. In addition, to test our assumptions about the inuence of domains (assumption PR1) and representations of problems
(assumption PR2), we will also include permuted benchmark problems and some other application problems. This section describes the set of problems and domains in our study,
focusing on their source and composition.
The problems require only STRIPS capabilities (i.e., add and delete lists). We chose this
least common denominator for several reasons. First, more capable planners can still handle
STRIPS requirements; thus, this maximized the number of planners that could be included
in our experiment. Also, not surprisingly, more problems of this type are available. Second,
we are examining assumptions of evaluation, including the effect of required capabilities on
performance. We do not propose to duplicate the effort of the competitions in singling out
planners for distinction, but rather, our purpose is to determine what factors differentially
affect planners.
The bulk of the problems came from the AIPS98 and AIPS 2000 problem sets and
the set of problems distributed with the PDDL specification. The remaining problems
were solicited from several sources. The source and counts of problems and domains are
summarized in Table 2.
Benchmark Problems The preponderance of problems in planning test sets are \toy
problems": well-known synthetic problems designed to test some attribute of planners.
The Blocksworld domain has long been included in any evaluation because it is well known,
can have subgoal interactions and supports constructing increasingly complex problems
(e.g., towers of more blocks). A few benchmark problems are simplified versions of realistic
planning problems, e.g., the at tire, refrigerator repair or logistics domains. We used the
set included with the UCPOP planner. These problems were contributed by a large number
of people and include multiple encodings of some problems/domains, especially Blocksworld.
AIPS Competitions: 1998 and 2000 For the first AIPS competition, Drew McDermott solicited problems from the competitors as well as constructing some of his own, such
as the mystery domain, which had semantically useless names for objects and operators.
Problems were generated for each domain automatically. The competition included 155
problems from six domains: robot movement in a grid, gripper in which balls had to be
9

fiHowe & Dahlman

moved between rooms by a robot with two grippers, logistics of transporting packages, organizing snacks for movie watching, and two mystery domains, which were disguised logistics
problems.
The format of the 1998 competition required entrants to execute 140 problems in the
first round. Of these problems, 52 could not be solved by any planner. For round two, the
planners executed 15 new problems in three domains, one of which had not been included
in the first round.
The 2000 competition attracted 15 competitors in three tracks: STRIPS, ADL and
a hand-tailored track. It required performance on problems in five domains: logistics,
Blocksworld, parts machining, Freecell (a card game), and Miconic-10 elevator control.
These domains were determined by the organizing committee, with Fahiem Bacchus as the
chair, and represented a somewhat broader range. We chose problems from the Untyped
STRIPS track for our set.
From a scientific standpoint, one of the most interesting conclusions of both competitions was the observed trade-offs in performance. Planners appeared to excel on different
problems, either solving more from a set or finding a solution faster. In 1998, IPP solved
more problems and found shorter plans in round two; STAN solved its problems the fastest;
HSP solved the most problems in round one; and blackbox solved its problems the fastest
in round one. In 2000, awards were given to two groups of distinguished planners across
the different categories of planners (STRIPS, ADL and hand tailored), because according
to the judges, \it was impossible to say that any one planner was the best"(Bacchus, 2000);
TalPlanner and FF were in the highest distinguished planner group. The graphs of performance do show differences in computation time relative to other planners and to problem
scale-up. However, each planner failed to solve some problems, which makes these trends
harder to interpret (the computation time graphs have gaps).
The purpose of these competitions was to showcase planner technology at which they
succeeded admirably. The planners solved much harder problems than could have been
accomplished in years past. Because of this trend in planners handling increasingly dicult
problems, the competition test sets may become of historical interest for tracking the field's
progress.
Problems Solicited from Planner Developers We also asked planner developers what
problems she had used during development. One developer, Maria Fox, sent us a domain
(Sodor, which is a logistics application) and set of problems that they had used. We would
have included other domains and problems had we received any others.
Other Applications The Miconic elevator domain from the AIPS2000 competition was
derived from an actual planning application. The domain and problems were extremely
simplified (e.g., removing the arithmetic).
To add another realistic problem to the comparison, we included one other planning application to the set of test domains: generating cases to test a software interface. Because
of the similarities between software interface test cases and plans, we developed a system,
several years ago, for automatically generating interface test cases using an AI planner.
The system was designed to generate test cases for the user interface to Storage Technology's robot tape library (Howe, von Mayrhauser, & Mraz, 1997). The interface (i.e., the
commands in the interface) was coded as the domain theory. For example, the mount com10

fiA Critical Assessment of Benchmark Comparison in Planning

mand/action's description required that a drive be empty and had the effect of changing
the position of the tape being mounted and changing the status of the tape drive. Problems
described initial states of the tape library (e.g., where tapes were resident, what was the
status of the devices and software controller) and goal states that a human operator might
wish to achieve.
At the time, we found that only the simplest problems could be generated using the
planners available. We included this application in part because we knew it would be a
challenge. As part of the test set, we include three domain theories (different ways of
coding the application involving 8-11 operators) and twenty-four problems for each domain.
We included only 24 because we wanted to include enough problems to see some effect, but
not too many to overly bias the results. These problems were relatively simple, requiring
the movement of no more than one tape coupled with some status changes, but they were
still more dicult than could be solved in our original system.

3.2 Problem Assumptions
General-purpose planners exhibit differential capabilities on domains and sometimes even
problems within a domain. Thus, the selection of problem set would seem to be critical
to evaluation. For example, many problems in benchmark sets are variants of logistics
problems; thus, a general-purpose planner that was actually tailored for logistics may appear
to be better overall on current benchmarks. In this section, we will empirically examine
some possible problem set factors that may inuence performance results.

Problem Assumption 1: To What Extent Is Performance of General Purpose
Planners Biased Toward Particular Problems/Domains? Although most planners
are developed as general purpose, the competitions and previous studies have shown that
planners excel on different domains/problems. Unfortunately, the community does not yet
have a good understanding of why a planner does well on a particular domain. We studied
the impact of problem selection on performance in two ways.
First, we assessed whether performance might be positively biased toward problems
tested during development. Each developer6 was asked to indicate which domains they used
during development. We then compared each planner's performance on their development
problems (i.e., the development set) to the problems remaining in the complete test set
(rest). We ran 2x2 2 tests comparing number of problems solved versus failed in the
development and test sets. We included only the number solved and failed in the analysis
as timed-out problems made no difference to the results7.
The results of this analysis are summarized in Table 3; Figure 1 graphically displays the
ratio of successes to failures for the development and other problems. All of the planners
except C performed significantly better on their development problems. This suggests that
these planners have been tailored (intentionally or not) for particular types of problems and
that they will tend to do better on test sets biased accordingly. For example, one of the
6. We decided against studying some of the planners in this way because the representations for their
development problems were not PDDL.
7. One planner was the exception to this rule; in one case, the planner timed out far more frequently on
non-development problems.

11

fiHowe & Dahlman

Development
Planner Sol. Fail
A
48
56
B
42
34
C
30
0
G
43
35
H
52
9
I
113
20
J
114
24
K
37
56
L
63
32

Rest
Sol. Fail
2
P
207 1026 51.70 0.001
226 929 51.27 0.001
549 16
0.13 0.722
233 924 49.56 0.001
234 655 91.41 0.001
328 920 187.72 0.001
388 949 157.62 0.001
203 987 27.82 0.001
358 846 52.13 0.001

Table 3: 2 results comparing outcome on development versus other problems.
planners in our set, STAN, was designed with an emphasis on logistics problems (Fox &
Long, 1999).

Figure 1: Histogram of ratios of success/failures for development and other problems for
each of the planners.
The above analysis introduces a variety of biases. The developers tended to give us short
lists that probably were not really representative of what they actually used. The set used
is a moving target, rather than stationary as this suggests. The set of problems included
in experimentation for publication may be different still. Consequently, for the second
part, we broadened the question to determine the effect of different subsets of problems on
12

fiA Critical Assessment of Benchmark Comparison in Planning

n
5
10
20
30

0
0
0
0
0

1
1
3
0
0

2
2
0
0
0

3
0
0
0
0

Rank Dominance
4 5 6 7
5 7 10 4
4 10 6 7
1 3 8 7
1 1 9 6

8 9 10 Total Pairs
10 18 21
78
5 23 20
78
11 8 40
78
9 8 44
78

Table 4: Rank dominance counts for 10 samples of domains with domain sizes (n) of five
through 30.
performance. For each of 10 trials, we randomly selected n domains (and their companion
problems) to form the problem set. We counted how many of these problems could be
solved by each planner and then ranked the relative performance of each planner. Thus,
for each value of n, we obtained 10 planner rankings. We focused on rankings of problems
solved for two reasons: First, each domain includes a different number of problems, making
the count of problems variable across each of the trials. Second, relative ranking gets to the
heart of whether one planner might be considered to be an improvement over another.
We tested values of 5, 10, 20 and 30 for n (30 is half of the domains at our disposal).
To give a sense of the variability in size, at n = 5, the most problems solved in a trial
varied from 11 to 64. To assess the changes in rankings across the trials, we computed rank
dominance for all pairs of planners; rank dominance is defined as the number of trials in
which planner x's rank was lower than planner y's (note: ties would count toward neither
planner). The 13 planners in our study resulted in 78 dominance pairings. If the relative
ranking between two planners is stable, then one would expect one to always dominate the
other, i.e., have rank dominance of 10.
Table 4 shows the number of pairs having each value (0-10) of rank dominance for the
four values for n. For a given pair, we used the highest number as the rank dominance for
the pair, e.g., if one always has a lower rank, then the pair's rank dominance is 10 or if
both have five, then it is five. Because of ties, the maximum can be less than five. The
data suggest that even when picking half of the domains, the rankings are not completely
stable: in 56% of the pairings, one always dominates, but 22% have a 0.3 or greater chance
of switching relative ranking. The values degrade as n decreases with only 27% always
dominating for n = 5.

Problem Assumption 2: How Do Syntactic Representation Differences Affect
Performance? Although it is well known that some planners' performance depends on

representation (Joslin & Pollack, 1994; Srinivasan & Howe, 1995), two recent developments
in planner research suggest that the effect needs to be better understood. First, a common
representation, i.e., PDDL, may bias performance. Some planners rely on a pre-processing
step to convert PDDL to their native representation, a step that usually requires making
arbitrary choices about ordering and coding. Second, an advantage of planners based on
Graphplan is that they are supposed to be less vulnerable to minor changes in representa13

fiHowe & Dahlman

Planner
A
B
C
D
E
F
G
H
I
J
K
L
M

All None Subset
65 315
30
70 295
45
318 74
18
202 169
39
111 132
167
112 138
160
70 295
45
91 290
29
109 134
167
150 124
136
60 305
45
112 284
14
212 148
50

Table 5: The number of problems for which the planners were able to solve all, none or
only a subset of the permutations.
tion. Although the reasoning for the claim is sound, the exigencies of implementation may
require re-introduction of representation sensitivity.
To evaluate the sensitivity to representation, ten permutations of each problem in the
AIPS2000 set were generated, resulting in 4510 permuted problems. The permutations were
constructed by randomly reordering the preconditions in the operator definitions and the
order of the definitions of the operators within the domain definition.
We limited the number of problems in this study because ten permutations of all problems would be prohibitive. We selected the AIPS2000 problems for attention because this
was the most recently developed benchmark set. Even within that set, not all of the domains
were permuted because some would not result in different domains under the transformation we used. For the purposes of this investigation, we limited the set of modifications to
permutations of preconditions and operators because these were known to affect some planners and because practical considerations limited the number of permutations that could be
executed. Finally, for expediency, we ran the permutations on a smaller number of faster
platforms because it expedited throughput and computation time was not a factor in this
study.
To analyze the data, we divided the performance on the permutations of the problems
into three groups based on whether the planner was able to solve all of the permutations,
none of the permutations or only a subset of the permutations. If a planner is insensitive to
the minor representational changes, then the subset count should be zero. From the results
in Table 5, we can see that all of the planners were affected by the permutation operation.
The susceptibility to permuting the problem was strongly planner dependent (2 = 1572:16,
P < 0:0001), demonstrating that some planners are more vulnerable than others.
By examining the number in the Subset column, one can assess the degree of susceptibility. All of the planners were sensitive to reorderings, even those that relied on Graphplan
14

fiA Critical Assessment of Benchmark Comparison in Planning

0
0
165
166
152
145
0
0
138
130
0
13
169

35
8
216
196
199
185
8
46
169
160
8
24
212

0
0
163
139
157
150
0
0
138
130
0
16
149

0
0
2
0
0
0
0
0
0
0
0
0
2

255
268
561
279
384
376
276
285
441
502
240
421
372

8
0
197
180
168
165
0
17
139
130
0
13
180

Pre.
8

0
0
169
164
162
157
0
0
138
130
0
19
168

Pre.
Safety
Strips
Typing

0
0
5
3
1
0
0
0
0
0
0
0
0

9

A
B
C
D
E
F
G
H
I
J
K
L
M

Feature

Axioms
Cond. Eff.
Dis. Pre.
Equality

Planner

0
0
160
139
149
145
0
0
138
130
0
13
151

Table 6: The number of problems claiming to require each PDDL feature solved by each
planner.

methodology. The most sensitive were E, F, I and J (which included some Graphplan based
planners and in which 40% of the problems had mixed results on the permutations) with C
and L being least sensitive (3-4% were affected).

Problem Assumption 3: Does Performance Depend on PDDL Requirements
Features? The planners were all intended to handle STRIPS problems. Some of the

problems in the test set claim to require features other than STRIPS; one would expect
that some of the planners would not be able to handle those problems. In addition, those
planners that claim to be able to handle a given feature may not do as well as other planners.
Table 6 shows the effects of feature requirements on the ability to solve problems. The data
in this table are based on the features specified with the :requirements list in the PDDL
definition of the domain.
We did not verify that the requirements were accurate or necessary; thus, the problem
may be solvable by ignoring a part of the PDDL syntax that is not understood, or the
problem may have been mislabeled by its designer. This is evident in cases where a planner
that does not support a given feature still appears to be able to solve the corresponding
problem. Some planners, e.g., older versions of STAN, will reject any problem that requires
more than STRIPS without trying to solve it; an ADL problem that only makes use of
STRIPS features would not be attempted.
As guidance on which planner to use when, these results must be viewed with some
skepticism. For example, it would appear based on these results that planner I might be
15

fiHowe & Dahlman

a good choice for problems with conditional effects as it was able to solve many of these
problems. This would be a mistake, since that planner cannot actually handle these types
of problems. In these cases, the problems claim to require ADL, but in fact, they only make
use of the STRIPS subset.
Clearly, certain problems can only be solved by specific planners. For instance, C and
M are the only planners that are able to handle safety constraints, while based on the data,
only C, D and E appear to handle domain axioms. About half the planners had trouble
with the typed problems. Some of the gaps appear to be due to problems in the translation
to native representation.

3.3 Planners

Publicly available, general-purpose planners tend to be large programs developed over a
period of years and enhanced to include additional features over time. Thus, several versions
are likely to be available, and those versions are likely to have features that can be turned
on/off via parameter settings.
When authors release later versions of their planning systems, the general assumption is
that these newer versions will outperform their predecessors. However, this may not be the
case in practice. For instance, a planner could be better optimized toward a specific class
of problem which then in turn hurts its performance on other problems. Also, advanced
capabilities, even when unused, may incur overhead in the solution of all problems.
So for comparison purposes, should one use the latest version? First, we tested this
question in a study comparing multiple versions of four of the planners. Second, each
planner relies on parameter settings to tune its performance. Some, such as blackbox, have
many parameters. Others have none. Comparisons tend to use the default or published
parameter settings because few people usually understand the effects of the parameters
and tuning can be extremely time consuming. So does this practice undermine a fair
comparison?
Planner Assumption 1: Is the Latest Version the Best? In this study, we compared
performance of multiple versions of four planners (labeled for this section with W, X, Y and
Z, with larger version numbers indicating subsequent versions). We considered two criteria
for improvement: outcome of planning and computation time for solved problems. The
outcome of planning is one of: solved, failed or timed-out. On each criterion, we statistically
analyzed the data for superior performance of one of the versions. The outcome results for
all the planners are summarized in Table 7. As the table shows, rarely does a new version
result in more problems being solved. Only Z improved the number of our test problems
solved in subsequent versions.
To check for whether the differences in outcome are significant, we ran 2x3 2 tests with
planner version as independent variable and outcome as dependent. Table 8 summarizes
the results of the 2 analysis. For Z, we compared each version to its successor only. The
differences are significant except for Y and the transition from Z 2 to 3 (this was expected
because these two versions were extremely similar).
Another planner performance metric, which we evaluated, was the speed of solution. For
this analysis, we limited the comparison to just those problems that were solved by both
versions of the planner. We then classified each problem by whether the later version solved
16

fiA Critical Assessment of Benchmark Comparison in Planning

Planner Version Solved Failed Timeout  Solved?
W
1
286
664
533
W
2
255
1082
147
+
X
1
502
973
3
X
2
441
940
103
+
Y
1
387
750
339
Y
2
382
771
329
+
Z
1
240
1043
201
Z
2
276
959
248
*
Z
3
268
963
252
+
Z
4
421
878
184
*
Table 7: Version performance: counts of outcome and change in number solved.
old
new
Planner Version Version 2
P
W
1
2
320.96 .0001
X
1
2
98.84 .0001
Y
1
2
.46
.79
Z
1
2
10.96 .004
Z
2
3
.158 .924
Z
3
4
48.50 .0001
Table 8: 2 results comparing versions of the same planner.
the problem faster, slower, or in the same time as the preceding version. From the results
in Table 9, we see that all of the planners improved in the average speed of solution for
subsequent versions, with the exception of Z (transition from the 1 to 2 versions). However,
Z did increase the number of problems solved between those versions.
Planner Old New Faster Slower Same Total
W
1
2
161
61
30
252
X
1
2
295
126
0
421
Y
1
2
222
82
53
357
Z
1
2
84
121
30
235
Z
2
3
131
84
53
268
Z
3
4
115
92
21
228
Table 9: Improvements in execution speed across versions. The Faster column counts the
number of cases in which the new version solved the problem faster; Slower specifies
those cases in which the new version took longer to solve a given problem.
17

fiHowe & Dahlman

Planner Assumption 2: Do Parameter Settings Matter to a Fair Comparison?

In this planner set, only three have obvious, easily manipulable parameters: Blackbox, HSP
and UCPOP. blackbox has an extensive set of parameters that control everything from
how much trace information to print to the sequence of solver applications. HSP's function
can be varied to include (or not) loop detection, change the search heuristic and vary the
number paths to expand. For UCPOP, the user can change the strategies governing node
orderings and aw selection.
We did not run any experiments for this assumption because not all of the planners
have parameters and because it is clear from the literature that the parameters do matter.
Blackbox relies heavily on random restarts and trying alternative SAT solvers. In Kautz
and Selman (1999), the authors of blackbox carefully study aspects of blackbox's design and
demonstrate differential performance using different SAT solvers; they propose hypotheses
for the performance differences and are working on better models of performance variation.
At the heart of HSP is heuristic search. Thus, its performance varies depending on
the heuristics. Experiments with both HSP and FF (a planner that builds on some ideas
from HSP) have shown the importance of heuristic selection in search space expansion,
computation time and problem scale up (Haslum & Geffner, 2000; Hoffmann & Nebel,
2001).
As with HSP, heuristic search is critical to UCPOP's performance. A set of studies have
explored alternative settings to the aw selection heuristics employed by UCPOP (Joslin &
Pollack, 1994; Srinivasan & Howe, 1995; Genevini & Schubert, 1996), producing dramatic
improvements on some domains with some heuristics. As Pollack et al. (1997) confirmed,
a good default strategy could be derived, but its performance was not the best under some
circumstances.
Thus, because parameters can control fundamental aspects of algorithms, such as their
search strategies, the role of parameters in comparisons cannot be easily dismissed.

Planner Assumption 3: Are Time Cut-offs Unfair? Planners often do not admit

to failure. Instead, the planner stops when it has used the allotted time and not found
a solution. So setting a time threshold is a requirement of any planner execution. In
a comparison, one might always wonder whether enough time was allotted to be fair {
perhaps the solution was almost found when execution was terminated.
To determine whether our cut-off of 30 minutes was fair, we examined the distribution
of times for declared successes and failures8. Across the planners and the problem set, we
found that the distributions were skewed (approximately log normal with long right tails)
and that the planners were quick to declare success or failure, if they were going to do so.
Table 10 shows the max, mean, median and standard deviation for success and failure times
for each of the planners. The differences between mean and median indicate the distribution
skew, as do the low standard deviations relative to the observed max times. The max time
shows that on rare occasions the planners might make a decision within 2 minutes of our
cut-off.
8. We separated the two because we usually observed a significant difference in the distributions of time to
succeed and time to fail { about half the planners were quick to succeed and slow to fail, the other half
reversed the relationship.

18

fiA Critical Assessment of Benchmark Comparison in Planning

Planner
A
B
C
D
E
F
G
H
I
J
K
L
M

Successes
Max Mean Median
667.9 34.0
1.3
1608.5 38.5
0.5
1455.4 89.9
1.6
481.0 17.8
1.1
1076 26.2
0.1
1282.4 44.4
0.1
1456.2 44.6
0.7
657.7 29.58
1.4
1713.8 115.4
0.2
1596.5 43.6
4.3
1110.5 31.0
0.32
1611.9 54.4
2.0
1675.3 53.4
1.45

Sd
98.7
182.8
244.6
77.4
126.8
126.8
188.5
80.6
303.1
127.4
121.8
180.9
196.5

Failures
Max Mean Median
1116.4 44.9
4.9
1692.0 45.6
17.8
1.4
0.4
0.13
713.6 26.3
1.1
1622.8 286.9
260.6
1188.4 22.3
0.2
1196.5 43.8
16.7
1080.6 93.8
1.4
50.6
5.1
4.9
1796 11.0
11.0
1298.8 27.7
12.1
847.1 124.1
68.4
1.6
0.9
0.8

Sd
128.8
96.8
0.4
122.6
189.1
104.8
78.5
162.1
6.3
57.9
65.2
164.8
0.4

Table 10: Max, mean, median and standard deviations (Sd) for the computation times to
success and failure for each planner.

What this table does not show, but the observed distributions do show, is that very
few values are greater than half of the time until the cut-off. Figures 2 and 3 display
the distributions for planner F, which had means in the middle of the set of planners and
quite typical distributions. Consequently, at least for these problems, any cut-off above 15
minutes (900 seconds) would not significantly change the results.

300

200

100

0
0

104 208 312 416 520 624 728 832 936 1040 1144 1248
success.time

Figure 2: Histogram of times, in seconds, for planner F to succeed.
19

fiHowe & Dahlman

600

400

200

0
0

96

192 288 384 480 576 672 768 864 960 1056 1152
fail.time

Figure 3: Histogram of times, in seconds, for planner F to fail.

3.4 Performance Metrics

Most comparisons emphasize the number of problems solved and the CPU time to completion as metrics. Often, the problems are organized in increasing diculty to show scale-up.
Comparing based on these metrics leaves a lot open to interpretation. For example, some
planners are designed to find the optimal plan, as measured by number of steps in either
a parallel or sequential plan. Consequently, these planners may require more computation.
Thus, by ignoring plan quality, these planners may be unfairly judged. We also hypothesize
that the hardware and software platform for the tests can vary the results. If a planner is
developed for a machine with 1GB of memory, then likely its performance will degrade with
less. A key issue is whether the effect is more or less uniform across the set of planners.
In this section, we examine these two issues: execution platform and effect of plan
quality.

Metric Assumption 1: Does Performance Vary between Planners When Run
on Different Hardware Platforms? Often when a planner is run at a competition or

in someone else's lab, the hardware and software platforms differ from the platform used
during development. Clearly, slowing down the processor speed should slow down planning,
requiring higher cut-offs. Reduction in memory may well change the set of problems that
can be solved or increase the processing time due to increased swapping. Changing the
hardware configuration may change the way memory is cached and organized, favoring
some planners' internal representations over others. Changing compilers could also affect
the amount and type of optimizations in the code. The exact effects are probably unknown.
The assumption is that such changes affect all planners more or less equally.
To test this, we ran the planners on a less powerful, lower memory machine and compared
the results on the two platforms: the base Sun Ultrasparc 10/440 with 256mb of memory
and Ultrasparc 1/170 with 128mb of memory. The operating system and compilers were
the same versions for both machines. The same problems were run on both platforms. We
followed much the same methodology as in the comparison of planner versions: comparing
on both number of problems solved and time to solution. Table 11 shows the results as
measured by problems solved, failed or timed-out for each planner on the two platforms.
20

fiA Critical Assessment of Benchmark Comparison in Planning

Planner Platform Solved Failed Timed-Out 2
p % Reduction
A
Ultra 1
94
383
27
Ultra 10
95
389
20 1.09 .58
1
B
Ultra 1
121
346
37
Ultra 10
121
353
30 0.80 .67
0
C
Ultra 1
354
7
143
Ultra 10
367
7
130 0.85 .65
4
D
Ultra 1
218
59
227
Ultra 10
217
59
228 0.01 .998
-.4
E
Ultra 1
280
145
79
Ultra 10
284
150
70 0.66 .72
1
F
Ultra 1
277
155
72
Ultra 10
284
154
66 0.35 .84
2
G
Ultra 1
120
347
37
Ultra 10
121
352
31 0.57 .75
1
H
Ultra 1
116
350
38
Ultra 10
122
338
44 0.80 .67
7
I
Ultra 1
265
201
38
Ultra 10
274
201
29 1.36 .51
3
J
Ultra 1
280
220
4
Ultra 10
285
217
2 0.73 .69
2
K
Ultra 1
108
370
26
Ultra 10
108
368
28 0.08 .96
0
L
Ultra 1
149
339
16
Ultra 10
150
341
13 0.32 .85
1
M
Ultra 1
250
65
189
Ultra 10
258
66
180 0.35 .84
3
Table 11: Number of problems solved, failed and timed-out for each planner on the two
hardware platforms. Last column is the percentage reduction in the number
solved from the faster to slower platforms.

21

fiHowe & Dahlman

Planner
A
B
C
D
E
F
G
H
I
J
K
L
M

Faster
# Mean 
92
5.18
120
4.02
294
31.89
177
11.02
275
2.68
271
14.86
117
5.02
115
6.86
261
25.73
280
42.24
107
15.26
148
16.81
194
32.72

Slower
Sd  # Mean 
30.76 1
10.01 0
101.71 60
0.29
82.82 39
0.23
12.27 1
72.44 0
17.17 1
25.24 0
119.97 0
138.16 0
75.42 0
98.54 1
139.73 56
0.30

Sd 
0.14
0.14

0.18

Same Total
1
1
0
1
4
6
2
1
4
0
1
0
0

94
121
354
217
280
277
120
116
265
280
108
149
250

Table 12: Improvements in execution speed moving from slower to faster platform. Counts
only problems that were solved on both platforms. For faster and slower, the
mean and standard deviation (Sd) of difference is also provided.
As before, we also looked at change in time to solution. Table 12 shows how the time
to solution changes for each planner. Not surprisingly, faster processor and more memory
nearly always lead to better performance. Somewhat surprisingly, the difference is far less
than the doubling that might be expected; the mean differences are much less than the
mean times on the faster processor (see Table 10 for the mean solution times).
Also, the effect seems to vary between the planners. Based on the counts, the Lisp-based
planners appear to be less susceptible to this trend (the only ones that sometimes were faster
on the slower platform). However, the advantages are very small, affecting primarily the
smaller problems. We think that this effect is due to the need to load in a Lisp image
at startup from a centralized server; thus, computation time for small problems will be
dominated by any network delay. Older versions of planners appear to be less sensitive to
the switch in platform.
In this study, the platforms make little difference to the results, despite a more than
doubling of processor speed and doubling of memory. However, the two platforms are
underpowered when compared to the development platforms for some of the planners. We
chose these platforms because they differed in only a few characteristics (processor speed
and memory amount) and because we had access to 20 identically configured machines. To
really observe a difference, 1GB9 of memory or more may be needed.
Recent trends in planning technology have exploited cheap memory: translations to
propositional representations, compilation of the problems and built-in caching and memory
management techniques. Thus, some planners are designed to trade-off memory for time;
9. We propose this figure because it is the amount requested by some of the participants in the AIPS 2000
planning competition.

22

fiA Critical Assessment of Benchmark Comparison in Planning

these planners will understandably be affected by memory limitations for some problems.
Given the results of this study, we considered performing a more careful study of memory
by artificially limiting memory for the planners but did not do so because we did not have
access to enough suciently large machines to likely make a difference and because we could
not devise a scheme for fairly doing so across all the planners (which are implemented in
different languages and require different software run-time environments).
Another important factor may be memory architecture/management. Some planners
include their own memory managers, which map better to some hardware platforms than
to others (e.g., HSP uses a linear organization that appears to fit well with Intel's memory
architecture).

Metric Assumption 2: Do the Number of Plan Steps Vary? Several researchers

have examined the issue of measuring plan quality and directing planning based on it, e.g.,
(Perez, 1995; Estlin & Mooney, 1997; Rabideau, Englehardt, & Chien, 2000). The number
of steps in a plan is a rather weak measure of plan quality, but so far, it is the only one
that has been widely used for primitive-action planning.
We expect that some planners sacrifice quality (as measured by plan length) for speed.
Thus, ignoring even this measure of plan quality may be unfair to some planners. To
check whether this appears to be a factor in our problem set, we counted the plan length
in the plans returned in output and compared the lengths across the planners. Because
not all of the planners construct parallel plans, we adopted the most general definition:
sequential plan length. We then compared the plan lengths returned by each planner on
every successfully solved problem.
We found that 11% of the problems were solved by only one planner (not necessarily the
same one). The planners found equal length solutions for 62% of those that remained (493
problems). We calculated the standard deviation (SD) of plan length for solutions to each
problem and then analyzed the SDs. We found that the minimum observed SD was 0.30,
the maximum was 63.30, the mean was 2.43 and the standard deviation was 5.45. Thirteen
cases showed SDs higher than 20. Obviously, these cases involved fairly long plans (up to
165 steps); the cases were for problems from the logistics and gripper domains.
To check whether some planners favored minimal lengths, we counted the number of
cases in which each planner found the shortest length plan (ties were attributed to all
planners) when there was some variance in plan length. Table 13 lists the results. Most
planners find the shortest length plans on about one third of these problems. Planner F
was designed to optimize plan length, which shows in the results. With one exception, the
older planners rarely find the shortest plans.

4. Interpretation of Results and Recommendations
The previous section presented our summarization and analysis of the planner runs. In
this section, we reect on what those results mean for empirical comparison of planners; we
summarize the results and recommend some partial solutions. It is not possible to guarantee
fairness and we propose no magic formula for performing evaluations, but the state of the
practice in general can certainly be improved. We propose three general recommendations
and 12 recommendations targeted to specific assumptions.
23

fiHowe & Dahlman

Planner Count
A
178
B
169
C
0
D
161
E
5
F
319
G
171
H
176
I
222
J
0
K
159
L
151
M
283
Table 13: Number of plans on which each planner found the shortest plan. The data only
include problems for which different length plans were found.
Many of the targeted recommendations amount to requesting problem and planner developers to be more precise about the requirements for and expectations of their contributions.
Because the planners are extremely complex and time consuming to build, the documentation may be inadequate to determine how a subsequent version differs from the previous or
under what conditions (e.g., parameter settings, problem types) the planner can be fairly
compared. With the current positive trend in making planners available, it behooves the
developer to include such information in the distribution of the system.
The most sweeping recommendation is to shift the research focus away from developing
the best general-purpose planner. Even in the competitions, some of the planners identified
as superior have been ones designed for specific classes of problems, e.g., FF and IPP. The
competitions have done a great job of exciting interest and encouraging the development
and public availability of planners that incorporate the same representation.
However, to advance the research, the most informative comparative evaluations are
those designed for a specific purpose { to test some hypothesis or prediction about the
performance of a planner10. An experimental hypothesis focuses the analysis and often
leads naturally to justified design decisions about the experiment itself. For example, Hoffmann and Nebel, the authors of the Fast-Forward (FF) system, state in the introduction to
their JAIR paper that FF's development was motivated by a specific set of the benchmark
domains; because the system is heuristic, they designed the heuristics to fit the expectations/needs of those domains (Hoffmann & Nebel, 2001). Additionally, in part of their
evaluation, they compare to a specific system on which their own system had commonalities
and point out the various advantages or disadvantages of their design decisions on specific
10. Paul Cohen has advocated such an experimental methodology for all of artificial intelligence based on
hypotheses, predictions and models in considerable detail; see Cohen (1991, 1995).

24

fiA Critical Assessment of Benchmark Comparison in Planning

problems. Follow-up work or researchers comparing their own systems to FF now have a
well-defined starting point for any comparison.

Recommendation 1: Experiments should be driven by hypotheses. Re-

searchers should precisely articulate in advance of the experiments their expectations about how their new planner or augmentations to an existing planner add
to the state of the art. These expectations should in turn justify the selection
of problems, other planners and metrics that form the core of the comparative
evaluation.
A general issue is whether the results are accurate. We reported the results as they are
output by the planners. If a planner stated in its output that it had been successful, we
took it at face value. However, by examining some of the output, we determined that some
claims of successful solution were erroneous { the proposed solution would not work. The
only way to ensure that the output is correct is with a solution checker. Drew McDermott
used a solution checker in the AIPS98 competition. However, the planners do not all
provide output in a compatible format with his checker. Thus, another concern with any
comparative evaluation is that the output needs to be cross-checked. Because we are not
declaring a winner (i.e., that some planner exhibited superior performance), we do not think
that the lack of a solution checker casts serious doubt on our results. For the most part, we
have only been concerned with factors that cause the observed success rates to change.

Recommendation 2: Just as input has been standardized with PDDL, output
should be standardized, at least in the format of returned plans.

Another general issue is whether the benchmark sets are representative of the space of
interesting planning problems. We did not test this directly (in fact, we are not sure how
one could do so), but the clustering of results and observations by others in the planning
community suggest that the set is biased toward logistics problems. Additionally, many of
the problems are getting dated and no longer distinguish performance. Some researchers
have begun to more formally analyze the problem set, either in service of building improved
planners (e.g., Hoffmann & Nebel, 2001) or to better understand planning problems. For
example, in the related area of scheduling, our group has identified distinctive patterns in
the topology of search spaces for different types of classical scheduling problems and has
related the topology to performance of algorithms (Watson, Beck, Barbulescu, Whitley, &
Howe, 2001). Within planning, Hoffmann has examined the topology of local search spaces
in some of the small problems in the benchmark collection and found a simple structure
with respect to some well-known relaxations (Hoffmann, 2001). Additionally, he has worked
out a partial taxonomy, based on three characteristics, for the analyzed domains. Helmert
has analyzed the computational complexity of a subclass of the benchmarks, transportation
problems, and has identified key features that affect the diculty of such problems (Helmert,
2001).

Recommendation 3: The benchmark problem sets should themselves be eval-

uated and over-hauled. Problems that can be easily solved should be removed.
Researchers should study the benchmark problems/domains to classify them
25

fiHowe & Dahlman

into problem types and key characteristics. Developers should contribute application problems and realistic versions of them to the evolving set.
The remainder of this section describes other recommendations for improving the state
of the art in planner comparisons.

Problem Assumption 1: Are General Purpose Planners Biased Toward Particular Problems/Domains? The set of problems on which a planner was developed
can have a strong effect on the performance of the planner. This can be either the effect
of unintentional over-specialization or the result of a concerted effort on the part of the
developers to optimize their system to solve a specific problem. With one exception, every
planner fared better on the tailored subset of problems (training set). Consequently, we
must conclude that the choice of a subset of problems may well affect the outcome of any
comparison.
A fair planner comparison must account for likely biases in the problem set. Good
performance on a certain class of problems does not imply good performance in general.
A large performance differential for planners with a targeted problem domain (i.e., do well
on their focus problems and poorly on others) may well indicate that the developers have
succeeded in optimizing the performance of their planner.
Recommendation 4: Problem sets should be constructed to highlight the
designers' expectations about superior performance for their planner, and they
should be specific about this selection criteria.
On the other hand, if the goal is to demonstrate across the board performance, then
our results at randomly selecting domains suggests that biases can be mitigated.
Recommendation 5: If highlighting performance on \general" problems is
the goal, then the problem set should be selected randomly from the benchmark
domains.

Problem Assumption 2: How Do Syntactic Representation Differences Affect
Performance? Many studies, including this, have shown that planners may be sensitive

to representational features. Just because representations can be translated automatically
does not mean that performance will be unaffected. Just because an algorithm should
theoretically be insensitive to a factor does not mean that in practice it is. All of the
planners showed some sensitivity to permuted problems, and the degree of sensitivity varied.
This outcome suggests that translators and even minor variations on problem descriptions
impact outcome and should be used with care, especially when the sensitivity is not the
focus of the study and some other planner is more vulnerable to the effect.
Recommendation 6: Representation translators should be avoided by using
native versions of problems and testing multiple versions of problems if necessary.
With many planner developers participating in the AIPS competitions, this should become
less of an issue.
More importantly, researchers should be explicitly testing the effect of alternative phrasings of planning problems to determine the sensitivity of performance and to separate the
effects of advice/tuning from the essence of the problem.
26

fiA Critical Assessment of Benchmark Comparison in Planning

Recommendation 7: Studies should consider the role of minor syntactic vari-

ations in performance and include permuted problems (i.e., initial conditions,
goals, preconditions and actions) in their problem sets because they can demonstrate robustness, provide an opportunity for learning and protect developers
from accidentally over-fitting their algorithm to the set of test problems.

Problem Assumption 3: Does Performance Depend on PDDL Requirements
Features? The planners did not perform quite as advertised or expected given some

problem features. This discrepancy could have many possible causes: problems incorrectly
specified, planners with less sensitivity than thought, solutions not being correct, etc. For
example, many of the problems in the benchmark set were not designed for the competitions
or even intended to be widely used and so may not have been specified carefully enough.

Recommendation 8: When problems are contributed to the benchmark set,
developers should verify that the requirements stated in the description of each
problem correctly reect the subset of features needed. Planner evaluators
should then use only those problems that match a planner's capabilities.

Depending on the cause, the results can be skewed, e.g., a planner may be unfairly
maligned for being unable to solve a problem that it was specifically designed not to solve.
The above recommendation addresses gaps in the specification of the problem set, but some
mismatches between the capabilities specifiable in PDDL and those that planners possess
remain.

Recommendation 9: Planner developers should develop a vocabulary for
their planner's capabilities, as in the PDDL ags, and specify the expected
capabilities in the planner's distribution.

Planner Assumption 1: Is the Latest Version the Best? Our results suggest that

new versions run faster, but often do not solve more problems. Thus, the newest version may
not represent the \best" (depending on your definition) performance for the class of planner.
Some competitions in other fields, e.g., the automatic theorem proving community, require
the previous year's best performer to compete as well; this has the advantage of establishing
a baseline of performance as well as allowing a comparison to how the focus may shift over
time.

Recommendation 10: If the primary evaluation metric is speed, then a newer

version may be the best competition. If it is number of problems solved or if one
wishes to establish what progress has been made, then it may be worth running
against an older version as well. If recommendation 9 has been followed, then
evaluators should select a version based on this guidance.

Planner Assumption 2: What Are the Effect of Parameter Settings? Perfor-

mance of some planners does vary with the parameter settings. Unfortunately, it often is
dicult to figure out how to set the parameters properly, and changing settings makes it
dicult to compare results across experiments. Generally, this is not an issue because the
27

fiHowe & Dahlman

developers and other users tend to rely on the default parameter settings. Unfortunately,
sometimes the developers exploit alternative settings in their own experiments, complicating
later comparison.

Recommendation 11: If a planner includes parameters, the developer should
guide users in their settings. If they do not, then the default settings should be
used by both the developers and others in experiments to facilitate comparison.

Planner Assumption 3: Are Time Cut-offs Unfair? We found little benefit from
increasing time cut-offs beyond 15 minutes for our problems.

Recommendation 12: If total computation time is a bottleneck, then run the

problems in separate batches, incrementally increasing the time cut-off between
runs and including only unresolved problems in subsequent runs. When no
additional problems are solved in a run, stop.

Metric Assumption 1: Do Alternative Platforms Lead to Different Performance? In our experiments, performance did not vary as much as we expected. This
result suggests that researchers in general are not developing for specific hardware/software
configurations, but recent trends suggest otherwise, at least with regards to memory. Again,
because these systems are research prototypes, it behooves the developer to be clear about
his/her expectations and anyone subsequently using the system to accommodate those requests in their studies.

Recommendation 13: As with other factors in planner design, researchers

must clearly state the hardware/software requirements for their planners, if the
design is based on platform assumptions. Additionally, a careful study of memory versus time trade-offs should be undertaken, given the recent trends in memory exploitation.

Metric Assumption 2: Do the Number of Plan Steps Vary? They certainly can.

If one neglects quality measures, then some planners are being penalized in efforts to declare
a best planner.

Recommendation 14: To expedite generalizing across studies, reports should
describe performance in terms of what was solved (how many of what types),
how much time was required and what were the quality of the solutions. Tradeoffs should be reported, when possible, e.g., 12% increase in computation time
for 30% decrease in plan length. Additionally, if the design goal was to find an
optimal solution, compare to other planners with that as their design goal.

Good metrics of plan quality are sorely needed. The latest specification of the PDDL
specification supports the definition of problem-specific metrics (Fox & Long, 2002); these
metrics indicate whether total-time (a new concept supported by specification of action
durations) or specified functions should be minimized or maximized. This addition is an
excellent start, but general metrics other than just plan-length and total-time are also
needed to expedite comparisons across problems.
28

fiA Critical Assessment of Benchmark Comparison in Planning

Recommendation 15: Developing good metrics is a valuable research contri-

bution. Researchers should consider it a worthwhile project, conference organizers and reviewers should encourage papers on the topic, and planner developers
should implement their planners to be responsive to new quality metrics (i.e.,
support tunable heuristics or evaluation criteria).

5. Conclusions

Fair evaluation and comparison of planners is hard. Many apparently benign factors exert
significant effects on performance. Superior performance of one planner over another on
a problem that neither was intentionally designed to solve may be explained by minor
representational features. However, comparative analysis on general problems is of practical
importance as it is not practical to create a specialized solution to every problem.
We have analyzed the effects of experiment design decisions in empirical comparison of
planners and made some recommendations for ameliorating the effects of these decisions.
Most of the recommendations are common sense suggestions for improving the current
methodology.
To expand beyond the current methodology will require at least two substantive changes.
First, the field needs to question whether we should be trying to show performance on
planning problems in general. A shift from general comparisons to focused comparisons (on
problem class or mechanism or on hypothesis testing) could produce significant advances in
our understanding of planning.
Second, the benchmark problem sets require attention. Many of the problems should be
discarded because they are too simple to show much. The domains are far removed from
real applications. It may be time to revisit testbeds. For example, several researchers in
robotics have constructed an interactive testbed for comparing motion planning algorithms
(Piccinocchi, Ceccarelli, Piloni, & Bicchi, 1997). The testbed consists of a user interface for
defining new problems, a collection of well-known algorithms and a simulator for testing
algorithms on specific problems. Thus, the user can design his/her own problems and compare performance of various algorithms (including their own) on them via a web site. Such a
testbed affords several advantages over the current paradigm of static benchmark problems
and developer conducted comparisons, in particular, replicability and extendability of the
test set. Alternatively, challenging problem sets can be developed by modifying deployed
applications (Wilkins & desJardins, 2001; Engelhardt, Chien, Barrett, Willis, & Wilklow,
2001).
In recent years, the planning community has significantly improved the size of planning
problems that can be solved in reasonable time and has advanced the state of the art in
empirical comparison of our systems. To interpret the results of empirical comparisons
and understand how they should motivate further development in planning, the community
needs to understand the effects of the empirical methodology itself. The purpose of this
paper is to further that understanding and initiate a dialogue about the methodology that
should be used.

29

fiHowe & Dahlman

Acknowledgments
This research was partially supported by a Career award from the National Science
Foundation IRI-9624058 and by a grant from Air Force Oce of Scientific Research F4962000-1-0144. The U.S. Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright notation thereon. We are most
grateful to the reviewers for the careful reading of and well-considered comments on the
submitted version; we hope we have done justice to your suggestions.

References

Bacchus,
F.
(2000).
AIPS-2000
planning
competition.
http://www.cs.toronto.edu/aips2000/SelfContainedAIPS-2000.ppt.
Barrett, A., Golden, K., Penberthy, S., & Weld, D. (1993). UCPOP User's Manual. Dept.
of Computer Science and Engineering, University of Washington, Seattle, WA. TR
93-09-06.
Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence Journal, 90 (1-2), 225{279.
Bonet, B., & Geffner, H. (1999). Planning as heuristic search: New results. In Proceedings
of the Fifth European Conference on Planning (ECP-99) Durham, UK.
Chien, S., Kambhampati, S., & Knoblock, C. A. (Eds.)(2000). Proceedings of the Fifth
International Conference on Artificial Intelligence Planning and Scheduling (AIPS
2000). AAAI Press, Breckenridge, CO.
Cohen, P. R. (1991). A survey of the eighth national conference on artificial intelligence:
Pulling together or pulling apart? AI Magazine, 12 (1), 16{41.
Cohen, P. R. (1995). Empirical Methods for Artificial Intelligence. MIT Press.
Drummond, M. E., Kaelbling, L. P., & Rosenschein, S. J. (1990). Collected notes from the
benchmarks and metrics workshop. Artificial intelligence branch FIA-91-06, NASA
Ames Research Center.
Engelhardt, B., Chien, S., Barrett, T., Willis, J., & Wilklow, C. (2001). The data-chaser
and citizen explorer benchmark problem sets. In Proceedings of the Sixth European
Conference on Planning (ECP 01) Toledo, Spain.
Estlin, T. A., & Mooney, R. J. (1997). Learning to improve both ecicency and quality of
planning. In Proceedings of the Fifteenth International Joint Conference on Artificial
Intelligence, pp. 1227{1233, Nagoya, Japan.
Fox, M., & Long, D. (1999). The ecient implementation of the plan-graph in STAN.
Journal of Artificial Intelligence Research, 10, 87{115.
Fox, M., & Long, D. (2002). PDDL2.1: An extension to PDDL for expressing temporal
planning domains. Available at http://www.dur.ac.uk/d.p.long/pddl2.ps.gz.
30

fiA Critical Assessment of Benchmark Comparison in Planning

Genevini, A., & Schubert, L. (1996). Accelerating partial-order planners: Some techniques
for effective search control and pruning. Journal of Artificial Intelligence Research, 5,
95{137.
Hanks, S., Nguyen, D., & Thomas, C. (1993). A beginner's guide to the truckworld simulator. Dept. of Computer Science and Engineering UW-CSE-TR 93-06-09, University
of Washington.
Hanks, S., Pollack, M. E., & Cohen, P. R. (1994). Benchmarks, test beds, controlled
experimentation and the design of agent architectures. AI Magazine, 17{42.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Proceedings of the Fifth International Conference on Artificial Intelligence Planning and
Scheduling (AIPS 2000), pp. 140{149, Breckenridge, CO. AAAI Press.
Helmert, M. (2001). On the complexity of planning in transportation domains. In 6th
European Conference on Planning (ECP'01), Lecture Notes in Artificial Intelligence,
New York, Springer-Verlag.
Hoffmann, J. (2001). Local search topology in planning benchmarks: An empirical analysis.
In Proceedings of the 17th International Joint Conference on Artificial Intelligence
Seattle, WA, USA.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253{302.
Howe, A. E., Dahlman, E., Hansen, C., Scheetz, M., & von Mayrhauser, A. (1999). Exploiting competitive planner performance. In Proceedings of the Fifth European Conference
on Planning, Durham, UK.
Howe, A. E., von Mayrhauser, A., & Mraz, R. T. (1997). Test case generation as an AI
planning problem. Automated Software Engineering, 4 (1), 77{106.
Joslin, D., & Pollack, M. (1994). Least-cost aw repair: A plan refinement strategy for
partial-order planning. In Proceedings of the Twelfth National Conference on Artificial
Intelligence, pp. 1004{1009, Seattle, WA.
Kautz, H., & Selman, B. (1998). BLACKBOX: A new approach to the application of
theorem proving to problem solving. In Working notes of the AIPS98 Workshop on
Planning as Combinatorial Search, Pittsburgh, PA.
Kautz,
H.
blackbox:
a SAT technology planning system.
http://www.cs.washington.edu/homes/kautz/blackbox/index.html.
Kautz, H., & Selman, B. (1999). Unifying SAT-based and graph-based planning. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, Stockholm, Sweden.
Koehler, J. (1999). Handling of conditional effects and negative goals in IPP. Tech. rep.
128, Institute for Computer Science, Albert Ludwigs University, Freiburg, Germany.
31

fiHowe & Dahlman

Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphs
to an ADL subset. In Proceedings of the Fourth European Conference in Planning.
McDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., Weld, D., &
Wilkins, D. (1998). The Planning Domain Definition Language.
McDermott, D. (2000). The 1998 AI planning systems competition. AI Magazine, 21 (2),
35{56.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: a sound, complete, partial order planner
for adl. In Proceedings of the Third International Conference on Knowledge Representation and Reasoning, pp. 103{114.
Perez, M. A. (1995). Learning Search Control Knowledge to Improve Plan Quality. Ph.D.
thesis, Carnegie-Mellon University.
Piccinocchi, S., Ceccarelli, M., Piloni, F., & Bicchi, A. (1997). Interactive benchmark for
planning algorithms on the web. In Proceedings of IEEE International Conference on
Robotics and Automation.
Pollack, M. E., & Ringuette, M. (1990). Introducing the Tileworld: Experimentally evaluating agent architectures. In Proceedings of the Eight National Conference on Artificial
Intelligence, pp. 183{189, Boston, MA.
Pollack, M., Joslin, D., & Paolucci, M. (1997). Flaw selection strategies for partial-order
planning. Journal of Artificial Intelligence Research, 6, 223{262.
Rabideau, G., Englehardt, B., & Chien, S. (2000). Using generic prferences to incrementally
improve plan quality. In Proceedings of the Fifth International Conference on Artificial
Intelligence Planning and Scheduling (AIPS 2000), Breckenridge, CO.
Slaney, J., & Thiebaux, S. (2001). Blocks world revisited. Artificial Intelligence Journal,
125 (1-2), 119{153.
Srinivasan, R., & Howe, A. E. (1995). Comparison of methods for improving search eciency
in a partial-order planner. In Proceedings of the 14th International Joint Conference
on Artificial Intelligence, pp. 1620{1626, Montreal, Canada.
Sussman, G. A. (1973). A computational model of skill acquisition. Tech. rep. Memo no.
AI-TR-297, MIT AI Lab.
The Prodigy Research Group (1992). PRODIGY 4.0; the manual and tutorial. School of
Computer Science 92-150, Carnegie Mellon University.
Watson, J., Barbulescu, L., Howe, A., & Whitley, L. D. (1999). Algorithm performance and
problem structure for ow-shop scheduling. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence (AAAI-99), Orlando, FL.
Watson, J., Beck, J., Barbulescu, L., Whitley, L. D., & Howe, A. (2001). Toward a descriptive model of local search cost in job-shop scheduling. In Proceedings of Sixth
European Conference on Planning (ECP'01), Toledo, Spain.
32

fiA Critical Assessment of Benchmark Comparison in Planning

Weld, D., Anderson, C., & Smith, D. (1998). Extending graphplan to handle uncertainty
and sensing actions. In Proceedings of the Fifteenth National Conference on Artificial
Intelligence Madison, WI.
Weld, D. S. (1999). Recent advances in AI planning. AI Magazine, 20 (2), 93{122.
Wilkins, D. E., & desJardins, M. (2001). A call for knowledge-based planning. AI Magazine,
22 (1), 99{115.

33

fiJournal of Artificial Intelligence Research 17 (2002) 289-308

Submitted 6/02; published 10/02

A Unified Model of Structural Organization in
Language and Music
Rens Bod

RENS@ILLC.UVA .NL

Institute for Logic, Language and Computation
University of Amsterdam, Nieuwe Achtergracht 166
1018 WV Amsterdam, THE NETHERLANDS, and
School of Computing, University of Leeds
LS2 9JT Leeds, UK

Abstract
Is there a general model that can predict the perceived phrase structure in language and
music? While it is usually assumed that humans have separate faculties for language and
music, this work focuses on the commonalities rather than on the differences between these
modalities, aiming at finding a deeper "faculty". Our key idea is that the perceptual system
strives for the simplest structure (the "simplicity principle"), but in doing so it is biased by the
likelihood of previous structures (the "likelihood principle"). We present a series of dataoriented parsing (DOP) models that combine these two principles and that are tested on the
Penn Treebank and the Essen Folksong Collection. Our experiments show that (1) a
combination of the two principles outperforms the use of either of them, and (2) exactly the
same model with the same parameter setting achieves maximum accuracy for both language
and music. We argue that our results suggest an interesting parallel between linguistic and
musical structuring.

1. Introduction: The Problem of Structural Organization
It is widely accepted that the human cognitive system tends to organize perceptual information
into hierarchical descriptions that can be conveniently represented by tree structures. Tree
structures have been used to describe linguistic perception (e.g. Wundt, 1901; Chomsky,
1965), musical perception (e.g. Longuet-Higgins, 1976; Lerdahl & Jackendoff, 1983) and
visual perception (e.g. Palmer, 1977; Marr, 1982). Yet, little attention has been paid to the
commonalities between these different forms of perception and to the question whether there
exists a general, underlying mechanism that governs all perceptual organization. This paper
studies exactly that question: acknowledging the differences between the perceptual
modalities, is there a general model that can predict the perceived tree structure for sensory
input? In studying this question, we will use an empirical methodology: any model that we
might hypothesize will be tested against manually analyzed benchmarks such as the
linguistically annotated Penn Treebank (Marcus et al. 1993) and the musically annotated
Essen Folksong Collection (Schaffrath, 1995). While we will argue for a general model of
structural organization in language, music and vision, we will carry out experiments only with
linguistic and musical benchmarks, since no benchmark of visual tree structures is currently
available, to the best of our knowledge.
Figure 1 gives three simple examples of linguistic, musical and visual information with their
corresponding tree structures printed below (these examples are resp. taken from Martin et al.
1987, Lerdahl & Jackendoff, 1983, and Dastani, 1998).

2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBOD

List the sales of products in 1973
S
NP
NP
NP
V

DT

PP
N

P

PP
N

P

N

List the sales of products in 1973

Figure 1: Examples of linguistic, musical and visual input with their tree structures
Thus, a tree structure describes how parts of the input combine into constituents and how
these constituents combine into a representation for the whole input. Note that the linguistic
tree structure is labeled with syntactic categories, whereas the musical and visual tree
structures are unlabeled. This is because in language there are syntactic constraints on how
words can be combined into larger constituents (e.g. in English a determiner can be combined
with a noun only if it precedes that noun, which is expressed by the rule NP  DT N), while
in music (and to a lesser extent in vision) there are no such restrictions: in principle any note
may be combined with any other note.
Apart from these differences, there is also a fundamental commonality: the perceptual input
undergoes a process of hierarchical structuring which is not found in the input itself. The main
problem is thus: how can we derive the perceived tree structure for a given input? That this
problem is not trivial may be illustrated by the fact that the inputs above can also be assigned
the following, alternative tree structures:
S

NP
PP

NP
V

DT

N

P

PP
N

P

N

List the sales of products in 1973

Figure 2: Alternative tree structures for the inputs in Figure 1
These alternative structures are possible in that they can be perceived. The linguistic tree
structure in Figure 1 corresponds to a meaning which is different from the tree in Figure 2.
The two musical tree structures correspond to different groupings into motifs. And the two
visual structures correspond to different visual Gestalts. But while the alternative tree
structures are all possible, they are not plausible: they do not correspond to the structures that
are actually perceived by the human cognitive system.
The phenomenon that the same input may be assigned different structural organizations is
known as the ambiguity problem. This problem is one of the hardest problems in modeling
human perception. Even in language, where a phrase-structure grammar may specify which
words can be combined into constituents, the ambiguity problem is notoriously hard (cf.
Manning & Schtze, 1999). Charniak (1997: 37) argues that many sentences from the Wall
Street Journal have more than one million different parse trees. The ambiguity problem for
290

fiA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC

musical input is even harder, since there are virtually no constraints on how notes may be
combined into constituents. Talking about rhythm perception in music, Longuet-Higgins and
Lee (1987) note that "Any given sequence of note values is in principle infinitely ambiguous,
but this ambiguity is seldom apparent to the listener.".
In the following Section, we will discuss two principles that have traditionally been proposed
to solve ambiguity: the likelihood principle and the simplicity principle. In Section 3, we will
argue for a new integration of the two principles within the data-oriented parsing framework.
Our hypothesis is that the human cognitive system strives for the simplest structure generated
by the shortest derivation, but that in doing so it is biased by the frequency of previously
perceived structures. In Section 4, we go into the computational aspects of our model. In
Section 5, we discuss the linguistic and musical test domains. Section 6 presents an empirical
investigation and comparison of our model. Finally, in Section 7, we give a discussion of our
approach and go into other combinations of simplicity and likelihood that have been proposed
in the literature.

2. Two principles: Likelihood and Simplicity
How can we predict from the set of all possible tree structures the tree that is actually
perceived by the human cognitive system? In the field of visual perception, two competing
principles have traditionally been proposed to govern structural organization. The first, initiated
by Helmholtz (1910), advocates the likelihood principle: perceptual input will be organized
into the most probable structure. The second, initiated by Wertheimer (1923) and developed by
other Gestalt psychologists, advocates the simplicity principle: the perceptual system is
viewed as finding the simplest rather than the most probable structure (see Chater, 1999, for
an overview). These two principles have also been used in linguistic and musical structuring.
In the following, we briefly review these principles for each modality.
2.1 Likelihood
The likelihood principle has been particularly influential in the field of natural language
processing (see Manning and Schtze, 1999, for a review). In this field, the most appropriate
tree structure of a sentence is assumed to be its most likely structure. The likelihood of a tree
is computed from the probabilities of its parts (e.g. phrase-structure rules) which are in turn
estimated from a large manually analyzed language corpus, i.e. a treebank. State-of-the-art
probabilistic parsers such as Collins (2000), Charniak (2000) and Bod (2001a) obtain around
90% precision and recall on the Penn Wall Street Journal treebank (Marcus et al. 1993).
The likelihood principle has also been applied to musical perception, e.g. by Raphael (1999)
and Bod (2001b/c). As in probabilistic natural language processing, the most probable musical
tree structure can be computed from the probabilities of rules or fragments taken from a large
annotated musical corpus. A musical benchmark which has been used by some models is the
Essen Folksong Collection (Schaffrath, 1995).
Also in vision science, there is a huge interest in probabilistic models (e.g. Hoffman, 1998;
Kersten, 1999). Mumford (1999) has even seen fit to declare the Dawning of Stochasticity.
Unfortunately, no visual treebanks are currently available.
2.2 Simplicity
The simplicity principle has a long tradition in the field of visual perception psychology (e.g.
Restle, 1970; Leeuwenberg, 1971; Simon, 1972; Buffart et al. 1983; van der Helm, 2000). In
291

fiBOD

this field, a visual pattern is formalized as a constituent structure by means of a visual coding
language based on primitive elements such as line segments and angles. Perception is
described as the process of selecting the simplest structure corresponding to the "shortest
encoding" of a visual pattern.
The notion of simplicity has also been applied to music perception. Collard et al. (1981) use
the coding language of Leeuwenberg (1971) to predict the metrical structure for four preludes
from Bach's Well-Tempered Clavier. More well-known in music perception is the theory
proposed by Lerdahl and Jackendoff (1983). Their theory contains two kinds of rules: "wellformedness rules" and "preference rules". The role of well-formedness rules is to define the
kinds of formal objects (grouping structures) the theory employs. What grouping structures a
listener actually hears, is then described by the preference rules which describe Gestaltpreferences of the kind identified by Wertheimer (1923), and which can therefore also be seen
as an embodiment of the simplicity principle.
Notions of simplicity also exist in language processing. For example, Frazier (1978) can be
viewed as arguing that the parser prefers the simplest structure containing minimal
attachments. Bod (2000a) defines the simplest tree structure of a sentence as the structure
generated by the smallest number of subtrees from a given treebank.

3. Combining Likelihood and Simplicity
The key idea of the current paper is that both principles play a role in perceptual organization,
albeit rather different ones: the simplicity principle as a general cognitive preference for
economy, and the likelihood principle as a probabilistic bias due to previous perceptual
experiences. Informally stated, our working hypothesis is that the human cognitive system
strives for the simplest structure generated by the shortest derivation, but that in doing so it is
biased by the frequency of previously perceived structures (some other combinations of
simplicity and likelihood will be discussed in Section 7). To formally instantiate our working
hypothesis, we first need a model that defines the set of possible structures of an input. In this
paper, we have chosen for a model that defines the set of phrase-structures for an input on
the basis of a treebank of previously analyzed input, and which is known as the Data-Oriented
Parsing or DOP model (see Bod, 1998; Collins & Duffy, 2002). DOP learns a grammar by
extracting subtrees from a given treebank and combines these subtrees to analyze fresh input.
We have chosen DOP because (1) it uses subtrees of arbitrary size, thereby capturing nonlocal dependencies, and (2) it has obtained very competitive results on various benchmarks
(Bod, 2001a/b; Collins & Duffy, 2002). In the following, we first review the DOP model and
discuss the use of the likelihood and simplicity principles by this approach. Next, we show how
these two principles can be combined to instantiate our working hypothesis.
3.1 Data-Oriented Parsing
In this Section, we illustrate the DOP model with a linguistic example (for a rigorous definition
of DOP, the reader is referred to Bod, 1998). We will come back to some musical examples
in Section 5. Suppose we are given the following extremely small linguistic treebank of two
trees for resp. she wanted the dress on the rack and she saw the dog with the telescope
(actual treebanks contain tens of thousands of trees, cf. Marcus et al. 1993):

292

fiA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC

S

S

NP

VP

she V

NP

she

wanted

on the

PP
NP

saw the

NP

dress P

VP
V

PP

NP
the

VP

NP

NP

P

dog with the telescope

rack

Figure 3: An example treebank
The DOP model can parse a new sentence, e.g. She saw the dress with the telescope, by
combining subtrees from this treebank by means of a substitution operation (indicated as ):



S

the

VP

NP



NP

PP
NP

P

dress

S

=

with the telescope

she

VP

she

PP
NP

V

VP

NP

VP

PP
NP

V
saw the

saw

P

NP

dress with the telescope

Figure 4: Parsing a sentence by combining subtrees from Figure 3
Thus the substitution operation combines two subtrees by substituting the second subtree on
the leftmost nonlexical leaf node of the first subtree (the result of which may be combined
with a third subtree, etc.). A combination of subtrees that results in a tree structure for the
whole sentence is called a derivation. Since there are many different subtrees, of various
sizes, there are typically also many different derivations that produce, however, the same tree;
for instance:



S

NP
the

VP

NP
she

VP
V
saw

dress

P

VP

NP
she

PP
NP

S

=

NP

VP
V

with the telescope

PP
NP

P

NP

saw the dress with the telescope

Figure 5: A different derivation which produces the same parse tree
293

fiBOD

The more interesting case occurs when there are different derivations that produce different
parse trees. This happens when a sentence is ambiguous; for example, DOP also produces
the following alternative parse tree for She saw the dress with the telescope:



S
NP

VP

she V

V
saw


P

S

=

PP

NP

NP

with the telescope

NP

VP

she V

NP

saw
NP
the

PP

PP

NP
the

dress

dress

P

NP

with the telescope

Figure 6: A different derivation which produces a different parse tree
3.2 Likelihood-DOP
In Bod (1993), DOP is enriched with the likelihood principle to predict the perceived tree
structure from the set of possible structures. This model, which we will call Likelihood-DOP,
computes the most probable tree of an input from the occurrence-frequencies of the subtrees.
The probability of a subtree t, P(t), is computed as the number of occurrences of t, | t |, divided
by the total number of occurrences of treebank-subtrees that have the same root label as t.
Let r(t) return the root label of t. Then we may write:
P(t) =

|t|

 t': r(t')= r( t)

| t' |

The probability of a derivation t1...tn is computed by the product of the probabilities of its
subtrees ti:
P(t1...tn ) =

 i P(ti)

As we have seen, there may be different derivations that generate the same parse tree. The
probability of a parse tree T is thus the sum of the probabilities of its distinct derivations. Let
tid be the i-th subtree in the derivation d that produces tree T, then the probability of T is given
by
P(T) =

d i P(tid)

294

fiA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC

In parsing a sentence s, we are only interested in the trees that can be assigned to s, which we
denote by Ts. The best parse tree, Tbest, according to Likelihood-DOP is then the tree which
maximizes the probability of Ts:
Tbest = arg max P(Ts)
Ts

Thus Likelihood-DOP computes the probability of a tree as a sum of products, where each
product corresponds to the probability of a certain derivation generating the tree. This
distinguishes Likelihood-DOP from most other statistical parsing models that identify exactly
one derivation for each parse tree and thus compute the probability of a tree by only one
product of probabilities (e.g. Charniak, 1997; Collins, 1999; Eisner, 1997). Likelihood-DOP's
probability model allows for including counts of subtrees of a wide range of sizes: everything
from counts of single-level rules to counts of entire trees.
Note that the subtree probabilities in Likelihood-DOP are directly estimated from their
relative frequencies in the treebank-trees. While the relative-frequency estimator obtains
competitive results on several domains (Bonnema et al. 1997; Bod, 2001a; De Pauw, 2000), it
does not maximize the likelihood of the training data (Johnson, 2002). This is because there
may be hidden derivations which the relative-frequency estimator cannot deal with. 1 There
are estimation procedures that do take into account hidden derivations and that maximize the
likelihood of the training data. For example, Bod (2000b) presents a Likelihood-DOP model
which estimates the subtree probabilities by a maximum likelihood re-estimation procedure
based on the expectation-maximization algorithm (Dempster et al. 1977). However, since the
relative frequency estimator has so far not been outperformed by any other estimator (see
Bod et al. 2002b), we will stick to the relative frequency estimator for the current paper.
3.3 Simplicity-DOP
Likelihood-DOP does not do justice to the preference humans display for the simplest
structure generated by the shortest derivation of an input. In Bod (2000a), the simplest tree
structure of an input is defined as the tree that can be constructed by the smallest number of
subtrees from a treebank. We will refer to this model as Simplicity-DOP. Instead of
producing the most probable parse tree for an input, Simplicity-DOP thus produces the parse
tree generated by the shortest derivation consisting of the fewest treebank-subtrees,
independent of the probabilities of these subtrees. We define the length of a derivation d,
L(d), as the number of subtrees in d; thus if d = t1...tn then L(d) = n. Let d T be a derivation
which results in parse tree T, then the best parse tree, Tbest, according to Simplicity-DOP is
the tree which is produced by a derivation of minimal length:
Tbest = arg min L(d Ts )
Ts

As in Section 3.2, Ts is a parse tree of a sentence s. For example, given the treebank in Figure
3, the simplest parse tree for She saw the dress with the telescope is given in Figure 5, since
1 Only if the subtrees are restricted to depth 1 does the relative frequency estimator coincide with the

maximum likelihood estimator. Such a depth-1 DOP model corresponds to a stochastic context-free
grammar. It is well-known that DOP models which allow subtrees of greater depth outperform depth-1
DOP models (Bod, 1998; Collins & Duffy, 2002).

295

fiBOD

that parse tree can be generated by a derivation of only two treebank-subtrees, while the
parse tree in Figure 6 (and any other parse tree) needs at least three treebank-subtrees to be
generated. 2
The shortest derivation may not be unique: it can happen that different parse trees of a
sentence are generated by the same minimal number of treebank-subtrees (also the most
probable parse tree may not be unique, but this never happens in practice). In that case we
will back off to a frequency ordering of the subtrees. That is, all subtrees of each root label
are assigned a rank according to their frequency in the treebank: the most frequent subtree (or
subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.
Next, the rank of each (shortest) derivation is computed as the sum of the ranks of the
subtrees involved. The derivation with the smallest sum, or highest rank, is taken as the final
best derivation producing the final best parse tree in Simplicity-DOP (see Bod, 2000a).
We performed one little adjustment to the rank of a subtree. This adjustment averages the
rank of a subtree by the ranks of its own sub-subtrees. That is, instead of simply taking the
rank of a subtree, we compute the rank of a subtree as the (arithmetic) mean of the ranks of
all its sub-subtrees (including the subtree itself). The effect of this technique is that it
redresses a very low-ranked subtree if it contains high-ranked sub-subtrees.
While Simplicity-DOP and Likelihood-DOP obtain rather similar parse accuracy on the
Wall Street Journal and the Essen Folksong Collection (in terms of precision/recall -- see
Section 6), the best trees predicted by the two models do not quite match. This suggests that a
combined model, which does justice to both simplicity and likelihood, may boost the accuracy.
3.4 Combining Likelihood-DOP and Simplicity-DOP: SL-DOP and LS-DOP
The underlying idea of combining likelihood and simplicity is that the human perceptual system
searches for the simplest tree structure (generated by the shortest derivation) but in doing so it
is biased by the likelihood of the tree structure. That is, instead of selecting the simplest tree
per se, our combined model selects the simplest tree from among the n likeliest trees, where n
is our free parameter. There are of course other ways to combine simplicity and likelihood
within the DOP framework. A straightforward alternative would be to select the most
probable tree from among the n simplest trees, suggesting that the perceptual system is
searching for the most probable structure only from among the simplest ones. We will refer to
the first combination of simplicity and likelihood (which selects the simplest among the n
likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP, and to the second combination
(which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LSDOP. Note that for n=1, Simplicity-Likelihood-DOP is equal to Likelihood-DOP, since there is
only one most probable tree to select from, and Likelihood-Simplicity-DOP is equal to
Simplicity-DOP, since there is only one simplest tree to select from. Moreover, if n gets large,
SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP. By
varying the parameter n, we will be able to compare Likelihood-DOP, Simplicity-DOP and
several instantiations of SL-DOP and LS-DOP.

2 One might argue that a more straightforward metric of simplicity would return the parse tree with the

smallest number of nodes (rather than the smallest number of treebank-subtrees). But such a metric is
known to perform quite badly (see Manning & Schtze, 1999; Bod, 2000a).

296

fiA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC

4. Computational Issues
Bod (1993) showed how standard chart parsing techniques can be applied to Likelihood-DOP.
Each treebank-subtree t is converted into a context-free rule r where the lefthand side of r
corresponds to the root label of t and the righthand side of r corresponds to the frontier labels
of t. Indices link the rules to the original subtrees so as to maintain the subtree's internal
structure and probability. These rules are used to create a derivation forest for a sentence
(using a chart parser -- see Charniak, 1993), and the most probable parse is computed by
sampling a sufficiently large number of random derivations from the forest ("Monte Carlo
disambiguation", see Bod, 1998). While this technique has been successfully applied to parsing
the ATIS portion in the Penn Treebank (Marcus et al. 1993), it is extremely time consuming.
This is mainly because the number of random derivations that should be sampled to reliably
estimate the most probable parse increases exponentially with the sentence length (see
Goodman, 2002). It is therefore questionable whether Bod's sampling technique can be scaled
to larger domains such as the Wall Street Journal (WSJ) portion in the Penn Treebank.
Goodman (1996) showed how Likelihood-DOP can be reduced to a compact stochastic
context-free grammar (SCFG) which contains exactly eight SCFG rules for each node in the
training set trees. Although Goodman's method does still not allow for an efficient computation
of the most probable parse (in fact, the problem of computing the most probable parse in
Likelihood-DOP is NP-hard -- see Sima'an, 1996), his method does allow for an efficient
computation of the "maximum constituents parse", i.e. the parse tree that is most likely to have
the largest number of correct constituents. Unfortunately, Goodman's SCFG reduction method
is only beneficial if indeed all subtrees are used, while maximum parse accuracy is usually
obtained by restricting the subtrees. For example, Bod (2001a) shows that the "optimal"
subtree set achieving highest parse accuracy on the WSJ is obtained by restricting the
maximum number of words in each subtree to 12 and by restricting the maximum depth of
unlexicalized subtrees to 6. Goodman (2002) shows that some subtree restrictions, such as
subtree depth, may be incorporated by his reduction method, but we have found no reduction
method for our optimal subtree set.
In this paper we will therefore use Bod's subtree-to-rule conversion method for LikelihoodDOP, but we will not use Bod's Monte Carlo sampling technique from derivation forests, as
this turned out to be computationally prohibitive. Instead, we will use the well-known Viterbi
optimization algorithm for chart parsing (cf. Charniak, 1993; Manning & Schtze, 1999) which
allows for computing the k most probable derivations of an input in cubic time. Using this
algorithm, we will estimate the most probable parse tree of an input from the 10,000 most
probable derivations, summing up the probabilities of derivations that generate the same tree.
Although this approach does not guarantee that the most probable parse tree is actually found,
it is shown in Bod (2000a) to perform at least as well as the estimation of the most probable
parse by Monte Carlo techniques on the ATIS corpus. Moreover, this approach is known to
obtain significantly higher accuracy than selecting the parse tree generated by the single most
probable derivation (Bod, 1998; Goodman, 2002), which we will therefore not consider in this
paper.
For Simplicity-DOP, we also first convert the treebank-subtrees into rewrite rules just as
with Likelihood-DOP. Next, the simplest tree, i.e. the shortest derivation, can be efficiently
computed by Viterbi optimization in the same way as the most probable derivation, provided
that we assign all rules equal probabilities, in which case the shortest derivation is equal to the
most probable derivation. This can be seen as follows: if each rule has a probability p then the
probability of a derivation involving n rules is equal to p n, and since 0<p<1 the derivation with
297

fiBOD

the fewest rules has the greatest probability. In our experiments in Section 6, we give each
rule a probability mass equal to 1/R, where R is the number of distinct rules derived by Bod's
method. As mentioned in 3.3, the shortest derivation may not be unique. In that case we
compute all shortest derivations of an input and then apply our ranking scheme to these
derivations. The ranks of the shortest derivations are computed by summing up the ranks of
the subtrees they involve. The shortest derivation with the smallest sum of subtree ranks is
taken to produce the best parse tree.
For SL-DOP and LS-DOP, we compute either n likeliest or n simplest trees by means of
Viterbi optimization. Next, we either select the simplest tree among the n likeliest ones (for
SL-DOP) or the likeliest tree among the n simplest ones (for LS-DOP). In our experiments, n
will never be larger than 1,000.

5. The Test Domains
As our linguistic test domain we used the Wall Street Journal (WSJ) portion in the Penn
Treebank (Marcus et al. 1993). This portion contains approx. 50,000 sentences that have been
manually annotated with the perceived linguistic tree structures using a predefined set of
lexico-syntactic labels. Since the WSJ has been extensively used and described in the
literature (cf. Manning & Schtze, 1999; Charniak, 2000; Collins, 2000; Bod, 2001a), we will
not go into it any further here.
As our musical test domain we used the European folksongs in the Essen Folksong
Collection (Schaffrath, 1995; Huron, 1996), which correspond to approx. 6,200 folksongs that
have been manually enriched with their perceived musical grouping structures. The Essen
Folksong Collection has been previously used by Bod (2001b) and Temperley (2001) to test
their musical parsers. The current paper presents the first experiments with Likelihood-DOP,
Simplicity-DOP, SL-DOP and LS-DOP on this collection. The Essen folksongs are not
represented by staff notation but are encoded by the Essen Associative Code (ESAC). The
pitch encodings in ESAC resemble "solfege": scale degree numbers are used to replace the
movable syllables "do", "re", "mi", etc. Thus 1 corresponds to "do", 2 corresponds to "re", etc.
Chromatic alterations are represented by adding either a "#" or a "b" after the number. The
plus ("+") and minus ("-") signs are added before the number if a note falls resp. above or
below the principle octave (thus -1, 1 and +1 refer al to "do", but on different octaves).
Duration is represented by adding a period or an underscore after the number. A period (".")
increases duration by 50% and an underscore ("_") increases duration by 100%; more than
one underscore may be added after each number. If a number has no duration indicator, its
duration corresponds to the smallest value. Thus pitches in ESAC are encoded by integers
from 1 to 7 possibly preceded or followed by symbols for octave, chromatic alteration and
duration. Each pitch encoding is treated as an atomic symbol, which may be as simple as "1"
or as complex as "+2#_.". A pause is represented by 0, possibly followed by duration
indicators, and is also treated as an atomic symbol. No loudness or timbre indicators are used
in ESAC.
Phrase boundaries are indicated by hard returns in ESAC. The phrases are unlabeled (cf.
Section 1 of this paper). Yet to make the ESAC annotations readable for our DOP models,
we added three basic labels to the phrase structures: the label "S" to each whole song, the
label "P" to each phrase, and the label "N" to each atomic symbol. In this way, we obtained
conventional tree structures that could directly be employed by our DOP models to parse new
input. The use of the label "N" distinguishes our annotations from those in previous work (Bod,
298

fiA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC

2001b/c) where we only used labels for song and phrase ("S" and "P"). The addition of "N"
enhances the productivity and robustness of the musical parsing model, although it also leads
to a much larger number of subtrees.
As an example, assume a very simple melody consisting of two phrases, (1 2) (2 3), then its
tree structure is given in Figure 7.
S
P

P

N

N

N

N

1

2

2

3

Figure 7: Example of a musical tree structure consisting of two phrases
Subtrees that can be extracted from this tree structure include the following:
S
P
N

P
P

N

N

N

N

2

3

3

1

Figure 8: Some subtrees that can be extracted from the tree in figure 7
Thus the first subtree indicates a phrase starting with a note 1, followed by exactly one other
(unspecified) note, with the phrase itself followed by exactly one other (unspecified) phrase.
Such subtrees can be used to parse new musical input in the same way as has been explained
for linguistic parsing in Section 3.

6. Experimental Evaluation and Comparison
To evaluate our DOP models, we used the blind testing method which randomly divides a
treebank into a training set and a test set, where the strings from the test set are parsed by
means of the subtrees from the training set. We applied the standard PARSEVAL metrics of
precision and recall to compare a proposed parse tree P with the corresponding correct test
set parse tree T as follows (cf. Black et al. 1991):
# correct constituents in P

# correct constituents in P

Precision =

Recall =

# constituents in P

# constituents in T

A constituent in P is "correct" if there exists a constituent in T of the same label that spans the
same atomic symbols (i.e. words or notes).3 Since precision and recall can obtain rather
3 The precision and recall scores were computed by using the "evalb" program (available via

http://www.cs.nyu.edu/cs/projects/proteus/evalb/)

299

fiBOD

different results (see Bod, 2001b), they are often balanced by a single measure of
performance, known as the F-score (see Manning & Schtze, 1999):
F-score =

2  Precision  Recall
Precision + Recall

For our experiments, we divided both treebanks (i.e. the WSJ and the Essen Folksong
Collection) into 10 training/test set splits: 10% of the WSJ was used as test material each time
(sentences  40 words), while for the Essen Folksong Collection test sets of 1,000 folksongs
were used each time. For words in the test set that were unknown in the training set, we
guessed their categories by using statistics on word-endings, hyphenation and capitalization
(cf. Bod, 2001a); there were no unknown notes. As in previous work (Bod, 2001a), we limited
the maximum size of the subtrees to depth 14, and used random samples of 400,000 subtrees
for each depth > 1 and  14.4 Next, we restricted the maximum number of atomic symbols in
each subtree to 12 and the maximum depth of unlexicalized subtrees to 6. All subtrees were
smoothed by the technique described in Bod (1998: 85-94) based on simple Good-Turing
estimation (Good, 1953).
Table 1 shows the mean F-scores obtained by SL-DOP and LS-DOP for language and
music and for various values of n. Recall that for n=1, SL-DOP is equal to Likelihood-DOP
while LS-DOP is equal to Simplicity-DOP.

n
1
5
10
11
12
13
14
15
20
50
100
1,000

SL-DOP

LS-DOP

(simplest among n likeliest)

(likeliest among n simplest)

Language

Music

Language

Music

87.9%
89.3%
90.2%
90.2%
90.2%
90.2%
90.2%
90.2%
90.0%
88.7%
86.8%
85.6%

86.0%
86.8%
87.2%
87.3%
87.3%
87.3%
87.2%
87.2%
86.9%
85.6%
84.3%
84.3%

85.6%
86.1%
87.0%
87.0%
87.0%
87.0%
87.0%
87.0%
87.1%
87.4%
87.9%
87.9%

84.3%
85.5%
85.7%
85.7%
85.7%
85.7%
85.7%
85.7%
85.7%
86.0%
86.0%
86.0%

Table 1: F-scores obtained by SL-DOP and LS-DOP for language and music

4 These random subtree samples were not selected by first exhaustively computing the complete set of

subtrees (this was computationally prohibitive). Instead, for each particular depth > 1 we sampled
subtrees by randomly selecting a node in a random tree from the training set, after which we selected
random expansions from that node until a subtree of the particular depth was obtained. We repeated
this procedure 400,000 times for each depth > 1 and  14.

300

fiA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC

The Table shows that there is an increase in accuracy for both SL-DOP and LS-DOP if the
value of n increases from 1 to 11. But while the accuracy of SL-DOP decreases after n=13
and converges to Simplicity-DOP (i.e. LS-DOP at n=1), the accuracy of LS-DOP continues
to increase and converges to Likelihood-DOP (i.e. SL-DOP at n=1). The highest accuracy is
obtained by SL-DOP at 11  n  13, for both language and music. Thus SL-DOP outperforms
both Likelihood-DOP and Simplicity-DOP, and the selection of the simplest structure out of
the top likeliest ones turns out to be a more promising model than the selection of the likeliest
structure out of the top simplest ones. According to paired t-testing, the accuracy
improvement of SL-DOP at n=11 over SL-DOP at n=1 (when it is equal Likelihood-DOP) is
statistically significant for both language (p<.0001) and music (p<.006).
It is surprising that SL-DOP reaches highest accuracy at such a small value for n. But it is
even more surprising that exactly the same model (with the same parameter setting) obtains
maximum accuracy for both language and music. This model embodies the idea that the
perceptual system strives for the simplest structure but in doing so it only searches among a
few most probable structures.
To compare our results for language with others, we also tested SL-DOP at n=11 on the
now standard division of the WSJ, which uses sections 2 to 21 for training (approx. 40,000
sentences) and section 23 for testing (2416 sentences  100 words) (see e.g. Manning &
Schtze, 1999; Charniak, 2000; Collins, 2000). On this division, SL-DOP achieved an F-score
of 90.7% while the best previous models obtained an F-score of 89.7% (Collins, 2000; Bod,
2001a). In terms of error reduction, SL-DOP improves with 9.6% over these other models. It
is common to also report the accuracy for sentences  40 words on the WSJ, for which SLDOP obtained an F-score of 91.8%.
Our musical results can be compared to Bod (2001b/c), who tested three probabilistic
parsing models of increasing complexity on the same training/test set splits from the Essen
Folksong Collection. The best results were obtained with a hybrid DOP-Markov parser:
80.7% F-score. This is significantly worse than our best result of 87.3% obtained by SL-DOP
on the same splits from the Essen folksongs. This difference may be explained by the fact that
the hybrid DOP-Markov parser in Bod (2001b/c) only takes into account context from higher
nodes in the tree and not from any sister nodes, while the DOP models presented in the
current paper take any subtree into account of (almost) arbitrary width and depth, thereby
covering a larger amount of musical context. Moreover, as mentioned in Section 5, the models
in Bod (2001b/c) did not use the label "N" for notes; instead, a Markov approach was used to
parse new sequences of notes.
It would also be interesting to compare our musical results to the melodic parser of
Temperley (2001), who uses a system of preference rules similar to Lerdahl and Jackendoff
(1983), and which is also evaluated on the Essen Folksong Collection. But while we have
tested on several test sets of 1,000 randomly selected folksongs, Temperley used only one test
set of 65 folksongs that was moreover cleaned up by eliminating folksongs with irregular
meter (Temperley, 2001: 74). It is therefore difficult to compare our results with Temperley's;
yet, it is noteworthy that Temperley's parser correctly identified 75.5% of the phrase
boundaries. Although this is lower than the 87.3% obtained by SL-DOP, Temperley's parser is
not "trained" on previously analyzed examples like our model (though we note that
Temperley's results were obtained by tuning the optimal phrase length of his parser on the
average phrase length of the Essen Folksong Collection).
It should perhaps be mentioned that while parsing models trained on treebanks are widely
used in natural language processing, they are still rather uncommon in musical processing.
301

fiBOD

Most musical parsing models, including Temperley's, employ a rule-based approach where the
parsing is based on a combination of low-level rules -- such as "prefer phrase boundaries at
large intervals" -- and higher-level rules -- such as "prefer phrase boundaries at changes of
harmony". The low-level rules are usually based on the well-known Gestalt principles of
proximity and similarity (Wertheimer, 1923), which prefer phrase boundaries at larger
intervallic distances. However, in Bod (2001c) we have shown that the Gestalt principles
predict incorrect phrase boundaries for a number of folksongs, and that higher-level
phenomena cannot alleviate these incorrect predictions. These folksongs contain a phrase
boundary which falls just before or after a large pitch or time interval (which we have called
jump-phrases) rather than at such intervals -- as would be predicted by the Gestalt principles.
Moreover, other musical factors, such as melodic parallelism, meter and harmony, predict
exactly the same incorrect phrase boundaries for these cases (see Bod, 2001b/c for details).
We have conjectured that such jump-phrases are inherently memory-based, reflecting idiomdependent pitch contours (cf. Huron, 1996; Snyder, 2000), and that they can be best captured
by a memory-based model that tries to mimic the musical experience of a listener from a
certain culture (Bod, 2001c).

7. Discussion and Conclusion
We have seen that our combination of simplicity and likelihood is quite rewarding for linguistic
and musical structuring, suggesting an interesting parallel between the two modalities. Yet, one
may question whether a model which massively memorizes and re-uses previously perceived
structures has any cognitive plausibility. Although this question is only important if we want to
claim cognitive relevance for our model, there appears to be some evidence that people store
various kinds of previously heard fragments, both in language (Jurafsky, 2002) and music
(Saffran et al. 2000). But do people store fragments of arbitrary size, as proposed by DOP?
In his overview article, Jurafsky (2002) reports on a large body of psycholinguistic evidence
showing that people not only store lexical items and bigrams, but also frequent phrases and
even whole sentences. For the case of sentences, people not only store idiomatic sentences,
but also "regular" high-frequency sentences.5 Thus, at least for language there is some
evidence that humans store fragments of arbitrary size provided that these fragments have a
certain minimal frequency. And this suggests that humans need not always parse new input by
the rules of a grammar, but that they can productively re-use previously analyzed fragments.
Yet, there is no evidence that people store all fragments they hear, as suggested by DOP.
Only high-frequency fragments seem to be memorized. However, if the human perceptual
faculty needs to learn which fragments will be stored, it will initially need to keep track of all
fragments (with the possibility of forgetting them) otherwise frequencies can never
accumulate. This results in a model which continuously and incrementally updates its fragment
memory given new input -- which is in correspondence with the DOP approach, and also with
some other approaches (cf. Daelemans, 1999; Scha et al. 1999; Spiro, 2002). While we
acknowledge the importance of a rule-based system in acquiring a fragment memory, once a
substantial memory is available it may be more efficient to construct a tree by means of
already parsed fragments than constructing it entirely by means of rules. For many cognitive
5 These results are derived from differences in reaction times in sentence recognition where only the

frequency of the (whole) test sentences is varied, while all other variables, such as lexical frequency,
bigram frequency, plausibility, syntactic/semantic complexity, etc., are kept constant.

302

fiA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC

activities it is advantageous to store results, so that they can immediately be retrieved from
memory, rather than computing them each time from scratch. This has been shown, for
example, for manual reaches (Rosenbaum et al. 1992), arithmetic operations (Rickard et al.
1994), word formation (Baayen et al. 1997), to mention a few. And linguistic and musical
parsing may be no exception to this.
It should be stressed that the experiments reported in this paper are limited in at least two
respects. First, our musical test domain is rather restricted. While a wide variety of linguistic
treebanks is currently available (see Manning & Schtze, 1999), the number of musical
treebanks is extremely limited. There is thus a need for larger and richer annotated musical
corpora covering broader domains. The development of such annotated corpora may be timeconsuming, but experience from natural language processing has shown that it is worth the
effort, since corpus-based parsing systems dramatically outperform grammar-based parsing
systems. A second limitation of our experiments is that we have only evaluated the parse
results rather than the parse process. That is, we have only assessed how accurately our
models can mimic the input-output behavior of a human annotator, without investigating the
process by which an annotator arrived at the perceived structures. It is unlikely that humans
process perceptual input by computing 10,000 most likely derivations using random samples of
400,000 subtrees  as we did in the current paper. Yet, for many applications it suffices to
know the perceived structure rather than the process that led to that structure. And we have
seen that our combination of simplicity and likelihood predicts the perceived structure with a
high degree of accuracy.
There have been other proposals for integrating the principles of simplicity and likelihood in
human perception (see Chater, 1999 for a review). Chater notes that in the context of
Information Theory (Shannon, 1948), the principles of simplicity and likelihood are identical. In
this context, the simplicity principle is interpreted as minimizing the expected length to encode
a message i, which is log2 p i bits, and which leads to the same result as maximizing the
probability of i. If we used this information-theoretical definition of simplest structure in
Simplicity-DOP, it would return the same structure as Likelihood-DOP, and no improved
results would be obtained by a combination of the two. On the other hand, by defining the
simplest structure as the one generated by the smallest number of subtrees, independent of
their probabilities, we created a notion of simplicity which is provably different from the notion
of most likely structure, and which, combined with Likelihood-DOP, obtained improved results.
Another integration of the two principles may be provided by the notion of Minimum
Description Length or MDL (cf. Rissanen, 1978). The MDL principle can be viewed as
preferring the statistical model that allows for the shortest encoding of the training data. The
relevant encoding consists of two parts: the first part encodes the model of the data, and the
second part encodes the data in terms of the model (in bit length). MDL is closely related to
stochastic complexity (Rissanen, 1989) and Kolmogorov complexity (Li and Vitanyi, 1997),
and has been used in natural language processing for estimating the parameters of a stochastic
grammar (e.g. Osborne, 1999). We will leave it as an open research question as to whether
MDL can be successfully used for estimating the parameters of DOP's subtrees. However,
since MDL is known to give asymptotically the same results as maximum likelihood estimation
(MLE) (Rissanen, 1989), its application to DOP may lead to an unproductive model. This is
because the maximum likelihood estimator will assign the training set trees their empirical
frequencies, and assign 0 weight to all other trees (see Bonnema, 2002 for a proof). This
would result in a model which can only generate the training data and no other strings.
Johnson (2002) argues that this may be an overlearning problem rather than a problem with
303

fiBOD

MLE per se, and that standard methods, such as cross-validation or regularization, would seem
in principle to be ways to avoid such overlearning. We will leave this issue to future
investigation.
The idea of a general underlying model for language and music is not uncontroversial. In
linguistics it is usually assumed that humans have a separate language faculty, and Lerdahl and
Jackendoff (1983) have argued for a separate music faculty. This work does not propose that
these separate faculties do not exist, but wants to focus on the commonalities rather than on
the differences between these faculties, aiming at finding a deeper "faculty" which may hold
for perception in general. Our hypothesis is that the perceptual system strives for the simplest
structure but in doing so it only searches among the likeliest structures.

Acknowledgements
Thanks to Aline Honingh, Remko Scha, Neta Spiro, Menno van Zaanen and three anonymous
reviewers for their excellent comments. A preliminary version of this paper was presented as
a keynote talk at the LCG workshop ("Learning Computational Grammars", Tbingen, 2001).

References
Baayen, R. H., Dijkstra, T. & Schreuder, R. (1997). Singular and Plurals in Dutch: Evidence
for a Parallel Dual-Route Model. Journal of Memory and Language, 37, 94-117.
Black, E., Abney, S., Flickinger, D., Gnadiec, C., Grishman, R., Harrison, P., Hindle, D.,
Ingria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos, S., Santorini, B.
& Strzalkowski, T. (1991). A Procedure for Quantitatively Comparing the Syntactic
Coverage of English, In Proceedings DARPA Speech and Natural Language
Workshop, Pacific Grove, Morgan Kaufmann.
Bod, R. (1993). Using an Annotated Language Corpus as a Virtual Stochastic Grammar. In
Proceedings AAAI-93, Menlo Park, Ca.
Bod, R. (1998). Beyond Grammar: An Experience-Based Theory of Language. Stanford:
CSLI Publications (Lecture notes number 88).
Bod, R. (2000a). Parsing with the Shortest Derivation. In Proceedings COLING-2000,
Saarbrcken, Germany.
Bod, R. (2000b). Combining Semantic and Syntactic Structure for Language Modeling. In
Proceedings ICSLP-2000, Beijing, China.
Bod, R. (2001a). What is the Minimal Set of Fragments that Achieves Maximal Parse
Accuracy? In Proceedings ACL'2001, Toulouse, France.
Bod, R. (2001b). A Memory-Based Model for Music Analysis. In Proceedings International
Computer Music Conference (ICMC'2001), Havana, Cuba.
Bod, R. (2001c). Memory-Based Models of Melodic Analysis: Challenging the Gestalt
Principles. Journal of New Music Research, 31(1), 26-36. (available at
http://staff.science.uva.nl/~rens/jnmr01.pdf)
304

fiA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC

Bod, R., Hay, J. & Jannedy, S. (Eds.) (2002a). Probabilistic Linguistics. Cambridge, The
MIT Press. (in press)
Bod, R., Scha, R. & Sima'an, K. (Eds.) (2002b). Data-Oriented Parsing. Stanford, CSLI
Publications. (in press)
Bonnema, R. (2002). Probability Models for DOP. In Bod et al. (2002b).
Bonnema, R., Bod, R. & Scha, R. (1997). A DOP Model for Semantic Interpretation, In
Proceedings ACL/EACL-97, Madrid, Spain.
Buffart, H., Leeuwenberg, E. & Restle , F. (1983). Analysis of Ambiguity in Visual Pattern
Completion. Journal of Experimental Psychology: Human Perception and
Performance. 9, 980-1000.
Charniak, E. (1993). Statistical Language Learning, Cambridge, The MIT Press.
Charniak, E. (1997). Statistical Techniques for Natural Language Parsing, AI Magazine,
Winter 1997, 32-43.
Charniak, E. (2000). A Maximum-Entropy-Inspired Parser. In Proceedings ANLPNAACL'2000, Seattle, Washington.
Chater, N. (1999). The Search for Simplicity: A Fundamental Cognitive Principle? The
Quarterly Journal of Experimental Psychology, 52A(2), 273-302.
Chomsky, N. (1965). Aspects of the Theory of Syntax, Cambridge, The MIT Press.
Collard, R., Vos, P. & Leeuwenberg, E. (1981). What Melody Tells about Metre in Music .
Zeitschrift fr Psychologie. 189, 25-33.
Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing, PhDthesis, University of Pennsylvania, PA.
Collins, M. (2000). Discriminative Reranking for Natural Language Parsing, In Proceedings
ICML-2000, Stanford, Ca.
Collins, M. & Duffy, N. (2002). New Ranking Algorithms for Parsing and Tagging: Kernels
over Discrete Structures, and the Voted Perceptron. In Proceedings ACL'2002,
Philadelphia, PA.
Daelemans, W. (1999). Introduction to Special Issue on Memory-Based Language
Processing. Journal of Experimental and Theoretical Artificial Intelligence 11(3),
287-296.
Dastani, M. (1998). Languages of Perception. ILLC Dissertation Series 1998-05, University
of Amsterdam.
Dempster, A., Laird, N. & Rubin, D. (1977). Maximum Likelihood from Incomplete Data via
the EM Algorithm, Journal of the Royal Statistical Society, 39, 1-38.

305

fiBOD

De Pauw, G. (2000). Aspects of Pattern-matching in Data-Oriented Parsing, In Proceedings
COLING-2000, Saarbrcken, Germany.
Eisner, J. (1997). Bilexical Grammars and a Cubic-Time Probabilistic Parser, In Proceedings
Fifth International Workshop on Parsing Technologies, Boston, Mass.
Frazier, L. (1978). On Comprehending Sentences: Syntactic Parsing Strategies. PhD.
Thesis, University of Connecticut.
Good, I. (1953). The Population Frequencies of Species and the Estimation of Population
Parameters, Biometrika 40, 237-264.
Goodman, J. (1996). Efficient Algorithms for Parsing the DOP Model, In Proceedings
Empirical Methods in Natural Language Processing, Philadelphia, PA.
Goodman, J. (2002). Efficient Parsing of DOP with PCFG-Reductions. In Bod et al. 2002b.
von Helmholtz, H. (1910). Treatise on Physiological Optics (Vol. 3), Dover, New York.
Hoffman, D. (1998). Visual Intelligence. New York, Norton & Company, Inc.
Huron, D. (1996). The Melodic Arch in Western Folksongs. Computing in Musicology 10, 223.
Johnson, M. (2002). The DOP Estimation Method is Biased and Inconsistent. Computational
Linguistics, 28, 71-76.
Jurafsky, D. (2002). Probabilistic Modeling in Psycholinguistics: Comprehension and
Production. In Bod et al. 2002a. (available at http://www.colorado.edu/ling/jurafsky/
prob.ps)
Kersten, D. (1999). High-level vision as statistical inference. In Gazzaniga , S. (Ed.), The New
Cognitive Neurosciences, Cambridge, The MIT Press.
Leeuwenberg, E. (1971). A Perceptual Coding Language for Perceptual and Auditory
Patterns. American Journal of Psychology. 84, 307-349.
Lerdahl, F. & Jackendoff, R. (1983). A Generative Theory of Tonal Music. Cambridge, The
MIT Press.
Li, M. & Vitanyi, P. (1997). An Introduction to Kolmogorov Complexity and its
Applications (2nd ed.). New York, Springer.
Longuet-Higgins, H. (1976). Perception of Melodies. Nature 263, 646-653.
Longuet-Higgins, H. and Lee, C. (1987). The Rhythmic Interpretation of Monophonic Music.
Mental Processes: Studies in Cognitive Science, Cambridge, The MIT Press.
Manning, C. & Schtze, H. (1999). Foundations of Statistical Natural Language
Processing. Cambridge, The MIT Press.
Marcus, M., Santorini, B., & Marcinkiewicz, M. (1993). Building a Large Annotated Corpus
of English: the Penn Treebank, Computational Linguistics 19(2).
306

fiA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC

Marr, D. (1982). Vision. San Francisco, Freeman.
Martin, W., Church, K. & Patil, R. (1987). Preliminary Analysis of a Breadth-first Parsing
Algorithm: Theoretical and Experimental Results. In Bolc, L. (Ed.), Natural Language
Parsing Systems, Springer Verlag, Berlin.
Mumford, D. (1999). The dawning of the age of stochasticity. Based on a lecture at the
Accademia Nazionale dei Lincei. (available at http://www.dam.brown.edu/people/
mumford/Papers/Dawning.ps)
Osborne, M. (1999). Minimal description length-based induction of definite clause grammars
for noun phrase identification. In Proceedings EACL Workshop on Computational
Natural Language Learning. Bergen, Norway.
Palmer, S. (1977). Hierarchical Structure in Perceptual Representation. Cognitive
Psychology, 9, 441-474.
Raphael, C. (1999). Automatic Segmentation of Acoustic Musical Signals Using Hidden
Markov Models. IEEE Transactions on Pattern Analysis and Machine Intelligence,
21(4), 360-370.
Restle , F. (1970). Theory of Serial Pattern Learning: Structural Trees. Psychological
Review, 86, 1-24.
Rickard, T., Healy, A. & Bourne Jr., E. (1994). On the cognitive structure of basic arithmetic
skills: Operation, order and symbol transfer effects. Journal of Experimental
Psychology: Learning, Memory and Cognition, 20, 1139-1153.
Rissanen, J. (1978). Modeling by the shortest data description. Automatica, 14, 465-471.
Rissanen, J. (1989). Stochastic Complexity in Statistical Inquiry. Series in Computer
Science - Volume 15. World Scientific, 1989.
Rosenbaum, D., Vaughan, J., Barnes, H. & Jorgensen, M. (1992). Time course of movement
planning: Selection of handgrips for object manipulation. Journal of Experimental
Psychology: Learning, Memory and Cognition, 18, 1058-1073.
Saffran, J., Loman, M. & Robertson, R. (2000). Infant Memory for Musical Experiences.
Cognition, 77, B16-23.
Scha, R., Bod, R. & Sima'an, K. (1999). Memory-Based Syntactic Analysis. Journal of
Experimental and Theoretical Artificial Intelligence, 11(3), 409-440.
Schaffrath, H. (1995). The Essen Folksong Collection in the Humdrum Kern Format. D.
Huron (ed.). Menlo Park, CA: Center for Computer Assisted Research in the
Humanities.
Shannon, C. (1948). A Mathematical Theory of Communication. Bell System Technical
Journal. 27, 379-423, 623-656.
Sima'an, K. (1996). Computational Complexity of Probabilistic Disambiguation by means of
Tree Grammars. In Proceedings COLING-96, Copenhagen, Denmark.
307

fiBOD

Simon, H. (1972). Complexity and the Representation of Patterned Sequences as Symbols.
Psychological Review. 79, 369-382.
Snyder, B. (2000). Music and Memory. Cambridge, The MIT Press.
Spiro, N. (2002). Combining Grammar-based and Memory-based Models of Perception of
Time Signature and Phase. In Anagnostopoulou, C., Ferrand, M. & Smaill, A. (Eds.).
Music and Artificial Intelligence, Lecture Notes in Artificial Intelligence, Vol. 2445,
Springer-Verlag, 186-197.
Temperley, D. (2001). The Cognition of Basic Musical Structures. Cambridge, The MIT
Press.
Wertheimer, M. (1923). Untersuchungen zur Lehre von der Gestalt. Psychologische
Forschung 4, 301-350.
Wundt, W. (1901). Sprachgeschichte und Sprachpsychologie. Engelmann, Leipzig.

308

fiJournal of Artificial Intelligence Research 17 (2002) 57-81

Submitted 12/01; published 8/02

A Logic for Reasoning about Upper Probabilities
Joseph Y. Halpern
Riccardo Pucella

halpern@cs.cornell.edu
riccardo@cs.cornell.edu

Department of Computer Science
Cornell University
Ithaca, NY 14853
http://www.cs.cornell.edu/home/halpern

Abstract
We present a propositional logic to reason about the uncertainty of events, where the
uncertainty is modeled by a set of probability measures assigning an interval of probability
to each event. We give a sound and complete axiomatization for the logic, and show that
the satisfiability problem is NP-complete, no harder than satisfiability for propositional
logic.

1. Introduction
Various measures exist that attempt to quantify uncertainty. For many trained in the use
of probability theory, probability measures are an obvious choice. However, probability
cannot easiliy capture certain situations of interest. Consider a simple example: suppose
we have a bag of 100 marbles; we know 30 are red and we know the remaining 70 are
either blue or yellow, although we do not know the exact proportion of blue and yellow. If
we are modeling the situation where we pick a ball from the bag at random, we need to
assign a probability to three different events: picking up a red ball (red-event), picking up
a blue ball (blue-event), and picking up a yellow ball (yellow-event). We can clearly assign
a probability of .3 to red-event, but there is no clear probability to assign to blue-event or
yellow-event.
One way to approach this problem is to represent the uncertainty using a set of probability measures, with a probability measure for each possible proportion of blue and yellow
balls. For instance, we could use the set of probabilities P = { :   [0, .7]}, where
 gives red-event probability .3, blue-event probability , and yellow-event probability
.7  . To any set of probabilities P we can assign a pair of functions, the upper and lower
probability measure, that for an event X give the supremum (respectively, the infimum) of
the probability of X according to the probability measures in P. These measures can be
used to deal with uncertainty in the manner described above, where the lower and upper
probability of an event defines a range of probability for that event.1 (This example can
be viewed as giving a frequentist interpretation of upper probabilities. Upper probabilities
can also be given a subjective interpretation, for example, by considering the odds at which
someone would be willing to accept or reject a bet (Smith, 1961; Walley, 1991).)
1. Note that using sets of probability measures is not the only way to model this situation. An alternative
approach, using inner measures, is studied by Fagin and Halpern (1991).

c
2002
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHalpern & Pucella

Given a measure of uncertainty, one can define a logic for reasoning about it. Fagin,
Halpern and Megiddo (1990) (FHM from now on) introduce a logic for reasoning about
probabilities, with a possible-worlds semantics that assigns a probability to each possible
world. They provide an axiomatization for the logic, which they prove sound and complete
with respect to the semantics. They also show that the satisfiability problem for the logic,
somewhat surprisingly, is NP-complete, and hence no harder than the satisfiability problem
for propositional logic. They moreover show how their logic can be extended to other notions
of uncertainty, such as inner measures (Fagin & Halpern, 1991) and Dempster-Shafer belief
functions (Shafer, 1976).
In this paper, we describe a logic for reasoning about upper probability measures, along
the lines of the FHM logic. The logic allows reasoning about linear inequalities involving
upper probabilities measures. Like the logics considered in FHM, our logic is agnostic
as to the interpretation of upper probabilities, whether frequentist or subjectivist. The
main challenge is to derive a provably complete axiomatization of the logic; to do this, we
need a characterization of upper probability measures in terms of properties that can be
expressed in the logic. Many semantic characterizations of upper probability measures have
been proposed in the literature. The characterization of Anger and Lembcke (1985) turns
out to be best suited for our purposes. Even though we are reasoning about potentially
infinite sets of probability measures, the satisfiability problem for our logic remains NPcomplete. Intuitively, we need guess only a small number of probability measures to satisfy
any given formula, polynomially many in the size of the formula. Moreover, these probability
measures can be taken to be defined on a finite state space, again polynomial in the size of
the formula. Thus, we need to basically determine polynomially many valuesa value for
each probability measure at each stateto decide the satisfiability of a formula.
The rest of this paper is structured as follows. In Section 2, we review the required
material from probability theory and the theory of upper probabilities. In Section 3, we
present the logic and an axiomatization. In Section 4, we prove that the axiomatization is
sound and complete with respect to the natural semantic models expressed in terms of upper
probability spaces. Finally, in Section 5, we prove that the decision problem for the logic is
NP-complete. The proofs of the new, more technical results are given in Appendix A. To
make the paper self-contained, we also review Anger and Lembckes results in Appendix B.

2. Characterizing Upper Probability Measures
We start with a brief review of the relevant definitions. Recall that a probability measure
is a function  :   [0, 1] for  an algebra of subsets of  (that is  is closed under
complements and unions), satisfying () = 0, () = 1, and (A  B) = (A) + (B) for
all disjoint sets A, B in .2 A probability space is a tuple (, , ), where  is a set,  is
an algebra of subsets of  (the measurable sets), and  is a probability measure defined on
. Given a set P of probability measures, let P  be the upper probability measure defined
2. If  is infinite, we could also require that  be a -algebra (i.e., closed under countable unions) and that
 be countably additive. Requiring countable additivity would not affect our results, since we show that
we can take  to be finite. For ease of exposition, we have not required it.

58

fiA Logic for Reasoning about Upper Probabilities

by P  (X) = sup{(X) :   P} for X  .3 Similarly, P (X) = inf{(X) :   P} is
the lower probability of X  . A straightforward derivation shows that the relationship
P (X) = 1P  (X) holds between upper and lower probabilities, where X is the complement
of X in . Because of this duality, we restrict the discussion to upper probability measures
in this paper, with the understanding that results for lower probabilities can be similarly
derived. Finally, an upper probability space is a tuple (, , P) where P is a set of probability
measures on .
We would like a set of properties that completely characterizes upper probability measures. In other words, we would like a set of properties that allow us to determine if a
function f :   R (for an algebra  of subsets of ) is an upper probability measure,
that is, whether there exists a set P of probability measures such that for all X  ,
P  (X) = f (X).4
One approach to the characterization of upper probability measures is to adapt the
characterization of Dempster-Shafer belief functions; these functions are known to be the
lower envelope of the probability measures that dominate them, and thus form a subclass
of the class of lower probability measures. By the duality noted earlier, a characterization
of lower probability measures would yield a characterization of upper probability measures.
The characterization of belief functions is derived from a generalization of the following
inclusion-exclusion principle for probabilities (obtained by replacing the equality with an
inequality):
(

n
[

n
X
(1)i1 (

Ai ) =

i=1

i=1

X

(

J{1,... ,n}
|J|=i

\

Aj )).

jJ

It seems reasonable that a characterization of lower (or upper) probability measures
could be derived along similar lines. However, as is well known, most properties derivable
from the inclusion-exclusion principle (which include most of the properties reported in the
literature) are insufficient to characterize upper probability measures. Huber (1981, p. 257)
and Walley (1991, p. 85) give examples showing the insufficiencies of such properties.
To give a sense of the insufficiency of simple properties, consider the following inclusionexclusionstyle properties, some of which are taken from (Walley, 1991). To simplify the
statement of these properties, let P 1 = P  and P +1 = P .
P P
i T
(1) P  (A1      An )  ni=1 |I|=i (1)i+1 P (1) ( jI Aj ),
(2) P (A1      An ) 

Pn P
i=1

|I|=i (1)

i+1 P (1)i+1 (

T

jI

Aj ),

(3) P (A  B) + P (A  B)  P (A) + P  (B)  P  (A  B) + P  (A  B),
3. In the literature, the term upper probability is sometimes used in a more restricted sense than here. For
example, Dempster (1967) uses the term to denote a class of measures which were later characterized as
Dempster-Shafer belief functions (Shafer, 1976); belief functions are in fact upper probability measures
in our sense, but the converse is not true (Kyburg, 1987). In the measure theory literature, what we call
upper probability measures are a special case of upper envelopes of measures, which are defined as the
sup of sets of general measures, not just probability measures.
4. It is possible to define a notion of upper probability over an arbitrary set of subsets of , not necessearily
an algebra, by simply requiring that f coincides with P  on its domain, for some set P of probability
measures. See Walley (1991) for details.

59

fiHalpern & Pucella

(4) P (A) + P (B)  P (A  B) + P  (A  B)  P  (A) + P  (B),
(5) P (A) + P (B)  P (A  B) + P  (A  B)  P  (A) + P  (B).
Note that without the alternation between upper probabilities and lower probabilities,
(1) and (2) would just be the standard notions of subadditivity and superadditivity, respectively. While subadditivity and superadditivity hold for upper and lower probabilities,
respectively, (1) and (2) are stronger properties. It is easily verified that all five properties
hold for upper probability measures. The question is whether they completely characterize
the class of upper probability measures. We show the inherent incompleteness of these
properties by proving that they are all derivable from the following simple property, which
is by itself insufficient to characterize upper probability measures:
(6) If A  B = , then P  (A) + P (B)  P  (A  B)  P  (A) + P  (B).
Proposition 2.1: Property (6) implies properties (1)-(5).
Observe that our property (6) is already given by Walley (1991, p. 84), as properties (d)
and (e). The following example shows the insufficiency of Property (6). Let P be the set
of probability measures {1 , 2 , 3 , 4 } over  = {a, b, c, d} (with  containing all subsets
of ) defined on singletons by
1 (b) =

1
4

1 (c) =

1
4

1 (d) =

1
4

2 (a) = 0 2 (b) =

1
8

2 (c) =

3
8

2 (d) =

1
2

3
8

3 (c) = 0

3 (d) =

1
2

1 (a) =

1
4

3 (a) =

1
8

3 (b) =

4 (a) =

3
8

4 (b) = 0 4 (c) =

1
8

4 (d) = 12 ,

and extended by additivity to all of . This defines an upper probability measure P  over
. Consider the function  :   [0, 1] defined by
 
P (X) +  if X = {a, b, c}
 (X) =
P  (X)
otherwise.
We claim that the function  , for small enough  > 0, satisfies property (6), but cannot be
an upper probability measure.
Proposition 2.2: For 0 <  < 18 , the function  satisfies property (6), but is not an
upper probability measure. That is, we cannot find a set P 0 of probability measures such
that  = (P 0 ) .
This example clearly illustrates the need to go beyond the inclusion-exclusion principle
to find properties that characterize upper probability measures. As it turns out, various
complete characterizations have been described in the literature (Lorentz, 1952; Huber,
1976, 1981; Williams, 1976; Wolf, 1977; Giles, 1982; Anger & Lembcke, 1985; Walley, 1991).
Most of these characterizations are obtained by considering upper and lower expectations,
rather than working directly with upper and lower probabilities. Anger and Lembcke (1985)
60

fiA Logic for Reasoning about Upper Probabilities

give a characterization in terms of upper and lower probabilities. Since their characterization
is particularly well-suited to the logic presented in the next section, we review it here.
The characterization is based on the notion of set cover. A set A is said to be covered
n times by a multiset {{A1 , . . . , Am }} of sets if every element of A appears in at least
n sets from A1 , . . . , Am : for all x  A, there exists distinct i1 , . . . , in in {1, . . . , m} such
that for all j  n, x  Aij . It is important to note here that {{A1 , . . . , Am }} is a multiset, not a set; the Ai s are not necessarily distinct. (We use the {{ }} notation to denote
multisets.) An (n, k)-cover of (A, ) is a multiset {{A1 , . . . , Am }} that covers  k times
and covers A n + k times. For example, {{1, 2}, {2, 3}, {1, 3}} covers {1, 2, 3} 2 times, and
{{{1, 2}, {2, 3}, {1, 3}, {2}, {2}}} is a (2,2) cover of ({2}, {1, 2, 3}).
The notion of (n, k)-cover is the key concept in Anger and Lembckes characterization
of upper probability measures.
Theorem 2.3: (Anger & Lembcke, 1985) Suppose that  is a set,  is an algebra of subsets
of , and  :   R. Then there exists a set P of probability measures with  = P  if and
only if  satisfies the following three properties:
UP1. () = 0,
UP2. () = 1,
UP3. for all natural numbers m, n, k and all subsets A1P
, . . . , Am in , if {{A1 , . . . , Am }}
is an (n, k)-cover of (A, ), then k + n(A)  m
i=1 (Ai ).
Proof: We reproduce a proof of this result in Appendix B.
Note that UP1 is redundant in the presence of UP2 and UP3. Indeed, {{, }} is a
(0, 1)-cover of (, ), and applying UP3 yields () + () = 1. Since UP2 states that
() = 1, this means that () = 0. A further consequence of UP3 is that if A  B, then
(A)  (B), since {{B}} is a (1, 0)-cover of (A, ). Therefore, for all A  , (A)  [0, 1].
We need to strengthen Theorem 2.3 in order to prove the main result of this paper,
namely, the completeness of the axiomatization of the logic we introduce in the next section.
We show that if the cardinality of the state space  is finite, then we need only finitely
many instances of property UP3. Notice that we cannot derive this from Theorem 2.3
alone: even if || is finite, UP3 does not provide any bound on m, the number of sets to
consider in an (n, k) cover of a set A. Indeed, there does not seem to be any a priori reason
why the value of m, n, and k can be bounded. Bounding this value of m (and hence of n
and k, since they are no larger than m) is one of the key technical results of this paper, and
a necessary foundation for our work.
Theorem 2.4: There exist constants B0 , B1 , . . . such that if  is a finite set,  is an
algebra of subsets of , and  :   R, then there exists a set P of probability measures
such that  = P  if and only if  satisfies the following properties:
UPF1. () = 0,
UPF2. () = 1,
UPF3. for all integers m, n, k  B|| and all sets A1 , P
. . . , Am , if {{A1 , . . . , Am }}
is an (n, k)-cover of (A, ), then k + n(A)  m
i=1 (Ai ).
61

fiHalpern & Pucella

Property UPF3 is significantly weaker than UP3. In principle, checking that UP3
holds for a given function requires checking that it holds for arbitrarily large collections of
sets, even if the underlying set  is finite. On the other hand, UPF3 guarantees that if  is
finite, then it is in fact sufficient to look at collections of size at most B|| . This observation
is key to the completeness result.
Theorem 2.4 does not prescribe any values for the constants B0 , B1 , . . . . Indeed, the
proof found in Appendix A relies on a Ramsey-theoretic argument that does not even
provide a bound on the Bi s. We could certainly attempt to obtain such bounds, but
obtaining them is completely unnecessary for our purposes. To get completeness of the
axiomatization of the logic introduced in the next section, it is sufficient for there to exist
finite constants B0 , B1 , . . . .

3. The Logic
The syntax for the logic is straightforward, and is taken from FHM. We fix a set 0 =
{p1 , p2 , . . . } of primitive propositions. The set  of propositional formulas is the closure
of 0 under  and . We assume a special propositional formula true, and abbreviate
true as false. We use p to represent primitive propositions, and  and  to represent
propositional formulas. A term is an expression of the form 1 l(1 ) +    + k l(k ), where
1 , . . . , k are reals and k  1. A basic likelihood formula is a statement of the form t  ,
where t is a term and  is a real. A likelihood formula is a Boolean combination of basic
likelihood formulas. We use f and g to represent likelihood formulas. We use obvious
abbreviations where needed, such as l()  l()  a for l() + (1)l()  a, l()  l()
for l()  l()  0, l()  a for l()  a, l() < a for (l()  a) and l() = a for
(l()  a)  (l()  a). Define the length |f | of the likelihood formula f to be the number
of symbols required to write f , where each coefficient is counted as one symbol. Let LQU be
the language consisting of likelihood formulas. (The QU stands for quantitative uncertainty.
The name for the logic is taken from (Halpern, 2002).)
In FHM, the operator l was interpreted as either probability or belief (in the sense
of Dempster-Shafer). Under the first interpretation, a formula such as l() + l()  2/3
would be intereted as the probability of  plus the probability of  is at least 2/3. Here
we interpret l as upper probaiblity. Thus, the logic allows us to make statements about
inequalities involving upper probabilities.
To capture this interpretation, we assign a semantics to formulas in LQU using an upper
probability space, as defined in Section 2. Formally, an upper probability structure is a
tuple M = (, , P, ) where (, , P) is an upper probability space and  associates with
each state (or world) in  a truth assignment on the primitive propositions in 0 . Thus,
(s)(p)  {true, false} for s   and p  0 . Let [[p]]M = {s   : (s)(p) = true}.
We call M measurable if for each p  0 , [[p]]M is measurable. If M is measurable then
[[]]M is measurable for all propositional formulas . In this paper, we restrict our attention
to measurable upper probability structures. Extend (s) to a truth assignment on all
propositional formulas in a standard way, and associate with each propositional formula
the set [[]]M = {s   : (s)() = true}. An easy structural induction shows that [[]]M

62

fiA Logic for Reasoning about Upper Probabilities

is a measurable set. If M = (, , P, ), let
M |= 1 l(1 ) +    + k l(k )   iff 1 P  ([[1 ]]M ) +    + k P  ([[k ]]M )  
M |= f iff M 6|= f
M |= f  g iff M |= f and M |= g.
Note that LQU can express lower probabilities: it follows from the duality between upper
and lower probabilities that M |= l()    1 iff P ([[]]M )  .5
Consider the following axiomatization AXup of upper probability, which we prove sound
and complete in the next section. The key axioms are simply a translation into LQU of the
characterization of upper probability given in Theorem 2.3. As in FHM, AXup is divided
into three parts, dealing respectively with propositional reasoning, reasoning about linear
inequalities, and reasoning about upper probabilities.
Propositional reasoning
Taut. All instances of propositional tautologies in LQU (see below).
MP. From f and f = g infer g.
Reasoning about linear inequalities
Ineq. All instances of valid formulas about linear inequalities (see below).
Reasoning about upper probabilities
L1. l(false) = 0.
L2. l(true) = 1.
L3. l()  0.
W
V
L4. l(1 ) +    + l(m )  nl()  k if   J{1,... ,m}, |J|=k+n jJ j and
W
V
6
J{1,... ,m}, |J|=k jJ j are propositional tautologies.
L5. l() = l() if    is a propositional tautology.
The only difference between AXup and the axiomatization for reasoning about probability
given in FHM is that the axiom l(  ) + l(  ) = l() in FHM, which expresses
the additivity of probability, is replaced by L4. Although it may not be immediately
obvious, L4 is the logical analogue of SUP3. To see this,
T first note that {{A1 , . . . , Am }}
covers
A
m
times
if
and
only
if
A

J{1,... ,m}, |J|=n jJ Aj . Thus, the formula  
W
V

says
that

(more
J{1,... ,m}, |J|=k+n jJ j
Wprecisely, the setVof worlds where  is true)
is covered k +n times by {{1 , . . . , n }}, while J{1,... ,m}, |J|=k jJ j says that the whole
space is covered k times by {{1 , . . . , n }}; roughly speaking, the multiset {{1 , . . . , n }}
is an (n, k)-cover of (, true). The conclusion of L4 thus corresponds to the conclusion of
5. Another approach, more in keeping with FHM, would be to interpret l as a lower probability measure.
On the other hand, interpreting l as an upper probability measure is more in keeping with the literature
on upper probabilities.
6. Note that, according to the syntax of LQU , 1 , . . . , m must be propositional formulas.

63

fiHalpern & Pucella

UP3. Note that in the same way that UP1 follows from UP2 and UP3, axiom L1 (as
well as L3) follows from L2 and L4.
Instances of Taut include all formulas of the form f  f , where f is an arbitrary
formula in LQU . We could replace Taut by a simple collection of axioms that characterize
propositional reasoning (see, for example, (Mendelson, 1964)), but we have chosen to focus
on aspects of reasoning about upper probability.
As in FHM, the axiom Ineq includes all valid formulas about linear inequalities. An
inequality formula is a formula of the form a1 x1 +    + an xn  c, over variables x1 , . . . , xn .
An inequality formula is said to valid if it is true under every possible assignment of real
numbers to variables. To get an instance of Ineq, we replace each variable xi that occurs
in a valid inequality formula by a primitive likelihood term of the form l(i ) (naturally each
occurence of the variable xi must be replaced by the same primitive likelihood term l(i )).
As with Taut, we can replace Ineq by a sound and complete axiomatization for Boolean
combinations of linear inequalities. One such axiomatization is given in FHM.

4. Soundness and Completeness
A formula f is provable in an axiom system AX if f can be proven using the axioms and
rules of inferences of AX. AX is sound with respect to a class M of structures if every
formula provable in AX is valid in M (i.e., valid in every structure in M); AX is complete
with respect to M if every formula valid in M is provable in AX.
Our goal is to prove that AXup is a sound and complete axiomatization for reasoning
about upper probability (i.e., with respect to upper probability structures). The soundness
of AXup is immediate from our earlier disscussion. Completeness is, as usual, harder. Unfortunately, the standard technique for proving completeness in modal logic, which involves
considering maximal consistent sets and canonical structures (see, for example, (Popkorn,
1994)) does not work. We briefly review the approach, just to point out the difficulties.
The standard approach uses the following definitions. A formula  is consistent with
an axiom system AX if  is not provable from AX. A finite set of formulas {1 , . . . , n }
is consistent with AX if the formula 1      n is consistent with AX; an infinite set
of formulas is consistent with AX if all its finite subsets are consistent with AX. F is
a maximal AX-consistent set if F is consistent with AX and no strict superset of F is
consistent with AX. If AX includes Taut and MP, then it is not hard to show, using
only propositional reasoning, that every AX-consistent set of formulas can be extended to
a maximal AX-consistent set.
To show that AX is complete with respect to some class M of structures, we must show
that every formula that is valid in M is provable in AX. To do this, it is sufficient to show
that every AX-consistent formula is satisfiable in some structure in M. Typically, this is
done by constructing what is called a canonical structure M c in M whose states are the
maximal AX-consistent sets, and then showing that a formula  is satisfied in a world w
in M c iff  is one of the formulas in the canonical set associated with world w.
Unfortunately, this approach cannot be used to prove completeness here. To see this,
consider the set of formulas
F 0 = {l() 

1
, n = 1, 2, . . . }  {l() > 0}.
n
64

fiA Logic for Reasoning about Upper Probabilities

This set is clearly AXup consistent according to our definition, since every finite subset
is satisfiable in an upper probability structure and AXup is sound with respect to upper
probability structures. It thus can be extended to a maximal AXup consistent set F .
However, the set F 0 of formulas is not satisfiable: it is not possible to assign l() a value
that will satisfy all the formulas at the same time. Hence, F is not satisfiable. Thus, the
canonical model approach, at least applied naively, simply will not work.
We take a different approach here, similar to the one taken in FHM. We do not try to
construct a single canonical model. Of course, we still must show that if a formula f is
AXup -consistent then it is satisfiable in an upper probability structure. We do this by an
explicit construction, depending on f . We proceed as follows.
By a simple argument, we can easily reduce the problem to the case where f is a
conjunction of basic likelihood formulas and negations of basic likelihood formulas. Let
N
p1 , . . . , pN be the primitive propositions that appear in f . Observe that there are 22
inequivalent propositional formulas over p1 , . . . , pN . The argument goes as follows. Let an
atom over p1 , . . . , pN be a formula of the form q1  . . .  qN , where qi is either pi or pi .
There are clearly 2N atoms over p1 , . . . , pN . Moreover, it is easy to see that any formula
N
over p1 , . . . , pN can be written in a unique way as a disjunction of atoms. There are 22
such disjunctions, so the claim follows.
Continuing with the construction of a structure satisfying f , let 1 , . . . , 22N be some
canonical listing of the inequivalent formulas over p1 , . . . , pN . Without loss of generality, we assume that 1 is equivalent to true, and 22N is equivalent to false. Since every propositional formula over p1 , . . . , pN is provably equivalent to some , it follows
that f is provably equivalent to a formula f 0 where each conjunct of f 0 is of the form
1 l(1 ) +    + 22N l(22N )  . Note that the negation of such a formula has the form
1 l(1 ) +    + 22N l(22N ) <  or, equivalently, (1 )l(1 ) +    + (22N )l(22N ) > .
Thus, the formula f gives rise in a natural way to a system of inequalities of the form:
1,1 l(1 ) +    + 1,22N l(22N )
...
r,1 l(1 ) +    + r,22N l(22N )
0 l( ) +    +  0
1,1
1
N l(22N )
1,22
..
.
0
s,1 l(1 ) +    + 0 2N l(22N )
s,2

 1
..
..
.
.
 r
> 1
..
..
.
.
> s .

(1)

We can express (1) as a conjunction of inequality formulas, by replacing each occurrence
of l(i ) in (1) by xi . Call this inequality formula f .
If f is satisfiable in some upper probability structure M , then we can take xi to be the
upper probability of i in M ; this gives a solution of f . However, f may have a solution
without f being satisfiable. For example, if f is the formula l(p) = 1/2  l(p) = 0, then
f has an obvious solution; f , however, is not satisfiable in an upper probability structure,
because the upper probability of the set corresponding to p and the upper probability of the
set corresponding to p must sum to at least 1 in all upper probability structures. Thus,
we must add further constraints to the solution to force it to act like an upper probability.
UP1UP3 or, equivalently, the axioms L1L4, describe exactly what additional constraints are needed. The constraint corresponding to L1 (or UP1) is just x1 = 0, since
65

fiHalpern & Pucella

we have assumed 1 is the formula false. Similarly, the constraint corresponding to L2 is
N
x22N = 1. The constraint corresponding to L3 is xi  0, for i = 1, . . . , 22 . What about
L4? This seems to require an infinite collection of constraints, just as UP3 does.7
This is where UPF3 comes into play. It turns out that, if f is satisfiable at all, it
is satisfiable in a structure with at most 2N worlds, one for each atom over p1 , . . . , pN .
Thus, we need to add only instances of L4 where k, m, n < B2N and 1 , . . . , m ,  are all
among 1 , . . . , 22N . Although this is a large number of formulas (in fact, we do not know
exactly how large, since it depends on B2N , which we have not computed), it suffices for our
purposes that it is a finite number. For each of these instances of L4, there is an inequality
of the form a1 x1 +    + a22N x22N  k. Let f, the inequality formula corresponding to f ,
be the conjunction consisting of f , together with all the inequalities corresponding to the
relevant instances of L4, and the equations and inequalities x1 = 0, x22N = 1, and xi  0
N
for i = 1, . . . , 22 , corresponding to axioms L1L3.
Proposition 4.1: The formula f is satisfiable in an upper probability structure iff the
inequality formula f has a solution. Moreover, if f has a solution, then f is satisfiable in
an upper probability structure with at most 2|f | worlds.
Theorem 4.2: The axiom system AXup is sound and complete for upper probability structures.
Proof: For soundness, it is easy to see that every axiom is valid for upper probability
structures, including L4, which represents UP3.
For completeness, we proceed as in the discussion above. Assume that formula f is not
satisfiable in an upper probability structure; we must show that f is AXup inconsistent.
We first reduce f to a canonical form. Let g1      gr be a disjunctive normal form
expression for f (where each gi is a conjunction of basic likelihood formulas and their
negations). Using propositional reasoning, we can show that f is provably equivalent to this
disjunction. Since f is unsatisfiable, each gi must also be unsatisfiable. Thus, it is sufficient
to show that any unsatisfiable conjunction of basic likelihood formulas and their negations is
inconsistent. Assume that f is such a conjunction. Using propositional reasoning and axiom
N
L5, f is equivalent to a likelihood formula f 0 that refers to 22 propositional formulas, say
1 , . . . , 22N . Since f is unsatisfiable, so is f 0 . By Proposition 4.1, the inequality formula
f0 corresponding to f 0 has no solution. Thus, by Ineq, the formula f 00 that results by
replacing each instance of xi in f0 by l(i ) is AXup provable. All the conjuncts of f 00 that
are instances of axioms L1L4 are AXup provable. It follows that f 0 is AXup provable,
and hence so is f .

5. Decision Procedure
Having settled the issue of the soundness and completeness of the axiom system AXup ,
we turn to the problem of the complexity of deciding satisfiability. Recall the problem of
7. Although we are dealing with only finitely many formulas here, 1 , . . . , 22N , recall that the formulas
1 , . . . , m in L4 need not be distinct, so there are potentially infinitely many instances of L4 to deal
with.

66

fiA Logic for Reasoning about Upper Probabilities

satisfiability: given a likelihood formula f , we want to determine if there exists an upper
probability structure M such that M |= f . As we now show, the satisfiability problem is
NP-complete, and thus no harder than satisfiability for propositional logic.
For the decision problem to make sense, we need to restrict our language slightly. If
we allow real numbers as coefficients in likelihood formulas, we have to carefully discuss
the issue of representation of such numbers. To avoid these complications, we restrict our
language (in this section) to allow only integer coefficients. Note that we can still express
rational coefficients by the standard trick of clearing the denominator. For example, we
can express 32 l()  1 by 2l()  3 and l()  23 by 3l()  2. Recall that we defined
|f | to be the length of f , that is, the number of symbols required to write f , where each
coefficient is counted as one symbol. Define ||f || to be the length of the longest coefficient
appearing in f , when written in binary. The size of a rational number ab , denoted || ab ||,
where a and b are relatively prime, is defined to be ||a|| + ||b||.
A preliminary result required for the analysis of the decision procedure shows that if a
formula is satisfied in some upper probability structure, then it is satisfied in a structure
(, , P, ), which is small in terms of the number of states in , the cardinality of the
set P of probability measures, and the size of the coefficients in f .
Theorem 5.1: Suppose f is a likelihood formula that is satisfied in some upper probability
structure. Then f is satisfied in a structure (, , P, ), where ||  |f |2 ,  = 2 (every
subset of  is measurable), |P|  |f |, (w) is a rational number such that ||(w)|| is
O(|f |2 ||f || + |f |2 log(|f |)) for every world w   and   P, and (w)(p) = false for every
world w   and every primitive proposition p not appearing in f .
Theorem 5.2: The problem of deciding whether a likelihood formula is satisfiable in an
upper probability structure is NP-complete.
Proof: For the lower bound, it is clear that a given propositional formula  is satisfiable iff
the likelihood formula l() > 0 is satisfiable, therefore the satisfiability problem is NP-hard.
For the upper bound, given a likelihood formula f , we guess a small satisfying structure
M = (, , P, ) for f of the form guaranteed to exist by Theorem 5.1. We can describe
such a model M in size polynomial in |f | and ||f ||. (The fact that (w)(p) = false for
every world w   and every primitive proposition p not appearing in f means that we
must describe  only for propositions that appear in f .) We verify that M |= f as follows.
Let l() be an arbitrary likelihood term in f . We compute [[]]M by checking the truth
assignment of each s   and seeing whether this
P truth assignment makes  true. We then
replace each occurence of l() in f by maxP { s[[]]M (s)} and verify that the resulting
expression is true.

6. Conclusion
We have considered a logic with the same syntax as the logic for reasoning about probability, inner measures, and belief presented in FHM, with uncertainty interpreted as the upper
probability of a set of probability measures. Under this interpretation, we have provided
a sound and complete axiomatization for the logic. We further showed that the satisfiability problem is NP-complete (as it is for reasoning about probability, inner measures, and
67

fiHalpern & Pucella

beliefs), despite having to deal with probability structures with possibility infinitely many
states and infinite sets of probability measures. The key step in the axiomatization involves
finding a characterization of upper probability measures that can be captured in the logic.
The key step in the complexity result involves showing that if a formula is satisfiable at all,
it is satisfiable in a small structure, where the size of the state space, as well as the size
of the set of probability measures and the size of all probabilities involved, are polynomial
in the length of the formula.
Given the similarity in spirit of the results for the various interpretations of the uncertainty operator (as a probability, inner measure, belief function, and upper probability),
including the fact that the complexity of the decision problem is NP-complete in all cases,
we conjecture that there is some underlying result from which all these results should follow.
It would be interesting to make that precise.
In FHM, conditional probabilities as well as probabilities are investigated. We have not,
in this paper, discussed conditional upper probabilities. The main reason for this is that,
unlike probability, we cannot characterize conditional upper probabilities in terms of (unconditional) upper probabilities. Thus, our results really tell us nothing about conditional
upper probabilities. It might be of interest to consider a logic that allows conditional upper probabilities as primitive likelihood terms (that is, allows likelihood terms of the form
l( | )). While there is no intrinsic difficult giving semantics to such a language, it is far
from clear what an appropriate axiomatization would be, or the effect of this extension on
complexity.
Finally, it is worth noting that the semantic framework developed here and in FHM
is in fact rich enough to talk about gambles (that is, real-valued functions over the set
of states) and the expectation of such gambles. Expectation functions can be defined for
the different measures of uncertainty, including upper probabilities, and it is not difficult to
extend the FHM logic in order to reason about expectation. One advantage of working with
expectation functions is that they are typically easier to characterize than the corresponding
measures; for instance, the characterization of expected upper probabilities is much simpler
than that of upper probabilities (Huber, 1981; Walley, 1981, 1991). However, getting a
complete axiomatization is quite nontrivial. We refer the reader to (Halpern & Pucella,
2002) for more details on this subject. We remark that Wilson and Moral (1994) take as
their starting point Walleys notion of lower and upper previsions. They consider when
acceptance of one set of gambles implies acceptance of another gamble. Since acceptance
involves expectation, it cannot be expressed in the logic considered in this paper; however,
it can be expressed easiliy in the logic of (Halpern & Pucella, 2002).

Acknowledgments
A preliminary version of this paper appears in Uncertainty in Artificial Intelligence, Proceedings of the Seventeenth Conference, 2001. Thanks to Dexter Kozen, Jon Kleinberg, and
Hubie Chen for discussions concerning set covers. Vicky Weissman read a draft of this paper
and provided numerous helpful comments. We also thank the anonymous UAI and JAIR
reviewers for their useful comments and suggestions. This work was supported in part by
NSF under grants IRI-96-25901 and IIS-0090145, and ONR under grants N00014-00-1-03-

68

fiA Logic for Reasoning about Upper Probabilities

41, N00014-01-10-511, and N00014-01-1-0795. The first author was also supported in part
by a Guggenheim and a Fulbright Fellowship while on sabbatical leave; sabbatical support
from CWI and the Hebrew University of Jerusalem is also gratefully acknowledged.

Appendix A. Proofs
Proposition 2.1: Property (6) implies properties (1)-(5).
Proof: We introduce the following auxiliary properties to help derive the implications:
(7) P (A) + P (B)  P (A  B) + P  (A  B).
(8) P (A) + P (B)  P (A  B) + P  (A  B).
(9) P (A  B) + P (A  B)  P (A) + P  (B).
(10) If A  B = , then
P (A) + P (B)  P (A  B)  P (A) + P  (B)  P  (A  B)  P  (A) + P  (B).
Using these properties, we show the following chain of implications:

(6) = (10)

(10) = (9) = (3)
(10) = (7) = (4)
(10) = (8) = (5)

(4), (5) = (1), (2).

The implication (4), (5) = (1), (2) follows easily by mutual induction on n. The
base case is the following instances of properties (4) and (5): P (A  B)  P (A) + P (B) 
P  (A  B) and P  (A  B)  P  (A) + P  (B)  P (A  B). The details are left to the reader.
We now prove the remaining implications.
(9) = (3): Since (9) is already one of the inequalities in (3), it remains to show that
it implies the other inequality in (3), that is, P (A)+P  (B)  P  (AB)+P  (AB).
P  (A  B) + P  (A  B) = 1  P (A  B) + 1  P (A  B)
= 1  P (A  B) + 1  P (A  B)
= 2  (P (A  B) + P (A  B))
= 2  (P (B  A) + P (B  A))
 2  (P (B) + P  (A))
= 1  P (B) + 1  P  (A)
= P  (B) + P (A).

69

fiHalpern & Pucella

(7) = (4): Since (7) is already one of the inequalities in (4), it remains to show that
it implies the other inequality in (4), that is, P (AB)+P  (AB)  P  (A)+P  (B).
P  (A) + P  (B) = 1  P (A) + 1  P (B)
= 2  (P (A) + P (B))
 2  (P (A  B) + P  (A  B))
= 1  P (A  B) + 1  P  (A  B)
= 1  P (A  B) + 1  P  (A  B)
= P  (A  B) + P (A  B).
(8) = (5): Since (8) is already one of the inequalities in (5), it remains to show that
it implies the other inequality in (5), that is, P (AB)+P  (AB)  P  (A)+P  (B).
P  (A) + P  (B) = 1  P (A) + 1  P (B)
= 2  (P (A) + P (B))
 2  (P (A  B) + P  (A  B))
= 1  P (A  B) + 1  P  (A  B)
= 1  P (A  B) + 1  P  (A  B)
= P  (A  B) + P (A  B).
For the next implications, given A, B, let Z = A  B.
(10) = (9):
P (A  B) = P ((A  Z)  B)
 P (A  Z) + P  (B)

[since (A  Z)  B = ]

 P ((A  Z)  Z)  P (Z) + P  (B)
= P (A) + P  (B)  P (A  B).
(10) = (7):
P (A  B) = P ((A  Z)  B)
 P (A  Z) + P (B)
 P ((A  Z)  Z)  P  (Z) + P (B)
= P (A) + P (B)  P  (A  B).
(10) = (8):
P  (A  B) = P  ((A  Z)  B)
 P  (A  Z) + P (B)
 P ((A  Z)  Z)  P (Z) + P (B)
= P (A) + P (B)  P (A  B).
70

fiA Logic for Reasoning about Upper Probabilities

(6) = (10): Again, since (6) already comprises two of the inequalities in (10), it
remains to show that it implies the other two, that is, if A  B = , then
P (A) + P (B)  P (A  B)  P  (A) + P (B).
First, we show that P (A) + P (B)  P (A  B). Using (6), we know that
P  (A  B) + P (A)  P  ((A  B)  A) = P  (B).
In other words, P  (A  B)  P  (B) + P (A). From this, we derive that
P (A  B) = 1  P  (A  B)
= 1  P  (A  B)
 1  (P  (B)  P (A))
= 1  P  (B) + P (A)
= P (B) + P (A).
Second, we show that P (A  B)  P  (A) + P (B). Using (6), we know that
P  (A  B) + P  (A)  P  ((A  B)  A) = P  (B).
(The last equality follows from the fact that (A  B)  A = B when A  B = .) In
other words, P  (A  B)  P  (B)  P  (A). From this, we derive that
P (A  B) = 1  P  (A  B)
= 1  P  (A  B)
 1  (P  (B)  P  (A))
= 1  P  (B) + P  (A)
= P (B) + P  (A).
Proposition 2.2: For 0 <  < 18 , the function  satisfies property (6), but is not an
upper probability measure. That is, we cannot find a set P 0 of probability measures such
that  = (P 0 ) .
Proof: We are given 0 <  < 81 . It is easy to check mechanically that  satisfies (6).
We now show that there is no set P 0 such that  = (P 0 ) . By way of contradiction,
assume there is such a P 0 . By the properties of sup, this means that there is a   P 0 such
that ({a, b, c}) > 43 , since  ({a, b, c}) = 34 +  > 34 . Consider this  in detail. Since   P,
we must have for all X  , X 6= {a, b, c}, that (X)  (P 0 ) (X) = P  (X). In particular,
({a, b}), ({b, c}), ({a, c})  21 . Therefore,
3
({a, b}) + ({b, c}) + ({a, c})  .
2

(2)

However, from standard properties of probability, it follows that
({a, b}) + ({b, c}) + ({a, c}) = 2({a, b, c}) > 2 
71

3
3
= ,
4
2

fiHalpern & Pucella

which contradicts (2). Therefore, , and therefore P 0 cannot exist, and  is not an upper
probability measure.
Theorem 2.4: There exists constants B0 , B1 , . . . such that if  is an algebra of subsets
of  and  is a function  :   R, then there exists a set P of probability measures such
that  = P  if and only if  satisfies the following properties:
UPF1. () = 0,
UPF2. () = 1,
UPF3. for all integers m, n, k  B|| and all sets A1 , P
. . . , Am , if {{A1 , . . . , Am }}
is an (n, k)-cover of (A, ), then k + n(A)  m
i=1 (Ai ).
Proof: In view of Theorem 2.3, we need only show that there exist constant B0 , B1 , . . .
such that a function  satisfies UP3 iff it satisfies UPF3. Clearly, UP3 always implies
UPF3, so it is sufficient to show that there exists B0 , B1 , . . . such that UPF3 implies
UP3.
We need some terminology before proceeding. An exact (n, k)-cover of (A, ) is a cover
C of A with the property that every element of A appears in exactly n + k sets in C, and
every element of   A appears in exactly k sets in C. Thus, while an (n, k)-cover of (A, )
can have many extra sets, as long as the sets cover A at least n + k times and  k times,
an exact cover has only the necessary sets, with the right total number of elements. An
exact (n, k)-cover C of (A, ) is decomposable if there exists an exact (n1 , k1 )-cover C1 and
an exact (n2 , k2 )-cover C2 of (A, ) such that C1 and C2 form a nontrivial partition of C,
with n = n1 + n2 and k = k1 + k2 . Intuitively, an exact cover C is decomposable if it
can be split into two exact covers. It follows easily by induction that for any exact (n, k)cover, there exists a (not necessarily unique) finite set of nondecomposable exact covers
C1 , . . . , Cm
Pm(ni , ki )-cover, such that the Ci s a nontrivial partition of C
P, mwith Ci an exact
with n = i=1 ni and k = i=1 . (If C is itself nondecomposable, we can take m = 1 and
C1 = C.) One can easily verify that if C is an exact (n, k)-cover of (A, ) and C 0  C is
an exact (n0 , k 0 )-cover of (A, ) with n0 + k 0 < n + k, then C is decomposable.
The following lemma highlights the most important property of exact covers from our
perspective. It says that for any set A  , there cannot be a large nondecomposable
exact cover of (A, ).
Lemma A.1: There exists a sequence B10 , B20 , B30 , . . . such that for all A  , every exact
0
0
(n, k)-cover of (A, ) with n > B||
or k > B||
is decomposable.
Proof: It is clearly sufficient to show that for any finite  we can find a B|| with the
required properties. Fix a . Given A  , we first show that there exists NA such that
if n > NA or k > NA , every exact (n, k)-cover of (A, ) is decomposable. Suppose for the
sake of contradiction that this is not the case. This means that we can find an infinite
sequence C1 , C2 , . . . such that Ci is a nondecomposable exact (ni , ki )-cover of (A, ), with
either n1 < n2 < . . . or k1 < k2 < . . . .
To derive a contradiction, we use the following lemma, known as Dicksons Lemma
(Dickson, 1913).
72

fiA Logic for Reasoning about Upper Probabilities

Lemma A.2: Every infinite sequence of d-dimensional vectors over the natural
numbers contains a monotonically nondecreasing subsequence in the pointwise
ordering (where x  y in the pointwise ordering iff xi  yi for all i).
Proof: It is straightforward to prove by induction on k that if k  d, then
every infinite sequence of vectors x1 , x2 , . . . contains a subsequence xi1 , xi2 , . . .
such that xij1 , xij2 , . . . is a nondecreasing sequence of natural numbers for all
j  k. The base case is immediate from the observation that every infinite
sequence of natural numbers contains a nondecreasing subsequence. For the
inductive step, observe that if xi1 , xi2 , . . . is a subsequence such that xij1 , xij2 , . . .
is a nondecreasing sequence of natural numbers for all j  k, then the sequence
1
2
xik+1
, xik+1
, . . . of natural numbers must have a nondecreasing subsequence. This
determines a subsequence of the original sequence with the appropriate property
for all j  k + 1.
Let S1 , . . . , S2|| be an arbitrary ordering of the 2|| subsets of . We can associate
C
C
with any cover C a 2|| -dimensional vector xC = (xC
1 , . . . , x2|| ), where xi is the number
of times the subset Si of  appears in the multiset C. The key property of this association
0
is that if C 0 and C are multisets, then C 0  C iff xC  xC in the pointwise ordering.
Consider the sequence of vectors xC1 , xC2 , . . . associated with the sequence C1 , C2 , . . .
of nondecomposable exact covers of (A, ). By Lemma A.2, there is a nondecreasing subsequence of vectors, xCi1  xCi2     . But this means that Ci1  Ci2     . Since
n1 < n2 < . . . or k1 < k2 < . . . , every cover in the chain must be distinct. But any pair
of exact covers in the chain is such that Ci  Ci+1 , meaning Ci+1 is decomposable, contradicting our assumption. Therefore, there must exist an NA such that any exact (n, k)-cover
of A with n > NA or k > NA is decomposable.
0
= max{NA : A  {1, . . . , ||}}. It is easy to see that this choice
Now define B||
works.
0 , for N = 1, 2, . . . , where B 0 is as
To get the constants B1 , B2 , . . . , let BN = 2N BN
N
in Lemma A.1. We now show that UPF3 implies UP3 with this choice of B1 , B2 , . . . .
Assume that UPF3 holds. Fix . Suppose that C = {{A1 , .P
. . , Am }} is an (n, k)-cover
of (A, ) with |C| = m. We want to show that k + n(A)  m
i=1 (Ai ). We proceed as
follows.
The first step is to show that, without loss of generality, C is an exact (n, k)-cover of
(A, ). Let Bi consist of those states s  Ai such that either s  A and s appears in
more than n + k sets in A1 , . . . , Ai1 or s    A and s appears in more than k sets in
A1 , . . . , Ai1 . Let A0i = Ai  Bi . Let C 0 = {{A01 , . . . , A0m }}. It is easy to check that C 0 is
an exact (n, k)-cover of (A, ). For if s  A, then s appears in exactly n + k sets in C 0 (it
appears in A0j iff Aj is among the first n + k sets in C in which s appeared) and, similarly,
if s    A, then s appears in exactly k sets in C 0 . Clearly if UP3 holds for C 0 , then it
holds for C, since (A0i )  (Ai ) for i = 1, . . . , m. Thus, we can assume without loss of
generality that C is an exact (n, k)-cover of A.
We can also assume without loss of generality that no set in C is empty (otherwise,
we can simply remove the empty sets in C; the resulting set is still an (n, k)-cover of
(A, )). There are now two cases to consider. If max(m, n, k)  B|| , the desired result

73

fiHalpern & Pucella

follows from UPF3. If not, consider a decomposition of C into multisets C1 , . . . , Cp ,
where Ch is an exact (nh , kh )-cover of (A, ) and is not further decomposable. We claim
that max(|Ch |, nh , kh )  B|| for h = 1, . . . , p. If nh > B|| or kh > B|| , then it is
immediate from Lemma A.1 that Ch can be further decomposed, contradicting
the fact
P
that Ch is not decomposable. And if |Ch | > B|| , then observe that XCh |X|  |Ch |.
0 , there must be some s   which appears in at least 2B 0
Since |Ch | > B|| = 2||B||
||
0 or k > B 0 .
sets in Ch . Since Ch is an exact (nh , kh )-cover, it follows that either nh > B||
h
||
But then, by Lemma A.1, Ch is decomposable, again a contradiction.
Now we can apply UPF3 to each of C1 , . . . , Ck to get
X
(X)  nh (A)  kh .
XCh

Since the Ch s form a decomposition of C, we have


p
p
X
X
X


kh
(X)  nh (X) 
XCh

h=1





p
X
h=1



m
X

p
p
X
X
(Ai )  (
nh )(A) 
kh

i=1

Pp

h=1





X

XCh

(X) 

p
X

h=1

By decomposition, n = h=1 nh and k =
showing that UP3 holds, as desired.

nh (A) 

h=1

p
X

kh

h=1

h=1

Pp

h=1 kh ,

and therefore

Pm

i=1 (Ai )  n(A)

 k,

Proposition 4.1: The formula f is satisfiable in an upper probability structure iff the
inequality formula f has a solution. Moreover, if f has a solution, then f is satisfiable in
an upper probability structure with at most 2|f | worlds.
Proof: Assume first that f is satisfiable. Thus there is some upper probability structure
M = (, , P, ) such that M |= f . As in Section 4, let p1 , . . . , pN be the primitive propositions that appear in f , and let 1 , . . . , 22N be some canonical listing of the inequivalent
formulas over p1 , . . . , pN . Without loss of generality, we assume that 1 is equivalent to
true, and 22N is equivalent to false. Define the vector x by letting xi = P  ([[i ]]M ),
N
for 1  i  22 . Since M |= f , it is immediate that x is a solution to the inequality
2N

formula f . Moreover, since 1 = false and 2
= true, it follows that x1 = 0 (since
P  ([[false]]M ) = P  () = 0) and x2N = 1 (since P  ([[true]]M ) = P  () = 1). Final2
ly, consider a conjunct of f corresponding to an instance of L4; suppose it has the form
xi1 +    xW
Since this conjunct appears
in f, it must be the case that
im  nxim+1  k. V
W
V
(im+1  J{1,... ,m}, |J|=k+n J{1,... ,m}, jJ ij )  ( |J|=k jJ ij ) is a propositional tautology. Thus, it follows that [[i1 ]]M , . . . , [[im ]]M is an (n, k)-cover for ([[im+1 ]]M , [[true]]M ).
It follows from UP3 that
P  ([[i1 ]]M ) +    + P  ([[im ]]M )  nP  ([[]]M )  k.
74

fiA Logic for Reasoning about Upper Probabilities

Thus, x is a solution to the inequality formulas corresponding to L4. Hence, x is a solution
to f.
For the converse, assume that x is a solution to f. We construct an upper probability
structure M = (S, E, P, ) such that M |= f as follows. Let p1 , . . . , pN be the primitive
propositions appearing in f . Let S = {1 , . . . , 2N } be the atoms over p1 , . . . , pN . Let E be
the set of all subsets of S. As observed earlier, every propositional formula over p1 , . . . , pn
is equivalent to a unique disjunction of atoms. Thus, we can get a canonical collection
1 , . . . , 22N of inequivalent formulas over p1 , . . . , pn by identifying each formula i with a
different element of E, where 1 corresponds to the empty set and 22N corresponds to all
of S. Define a set function  by taking ({i1 , . . . , ij }) = xi if i is the disjunction of the
atoms i1 , . . . , ij . Let ()() = true iff   .
It is now sufficient to show that  is an upper probability (of a set P of probability
measures), since then it is clear that (S, E, P, ) |= f (since x is a solution to f, the system
of inequalities derived from formula f ). To do this, by Theorem 2.4, it suffices to verify
UPF1, UPF2, and UPF3, using B2N in UPF3, since |S| = 2N .
UPF1: () = x1 = 0.
UPF2: (S) = x2N = 1.
2

UPF3: Suppose that A and A1 , . . . , Am are in E and satisfy the premises of property UPF3, with k, m, n  B2N . Let i1 , . . . , im , im+1 be the canonical
to A1 , . . . , AmW, A, respectively. Clearly,
A 
V
S formulas corresponding
T
iff



A
i
i
i
m+1
j
J{1,...
J{1,... ,m}, |J|=k+n jJ
S ,m}, |J|=k+n TjJ j is a
propositional tautology and similarly   J{1,... ,m}, |J|=k jJ Aij iff
W
V
Pm
J{1,... ,m}, |J|=k jJ ij is a propositional tautology. Thus,
j=1 xij 

x
 k is one of the inequality formulas in f . Thus, it follows that
Pim+1
m


 k, as desired. By our definition of , we therefore have
j=1 xij  xiP
m+1
k + n(A)  m
i=1 (Ai ), and so UPF3 holds.
Theorem 5.1: Suppose f is a likelihood formula that is satisfied in some upper probability
structure. Then f is satisfied in a structure (, , P, ), where ||  |f |2 ,  = 2 (every
subset of  is measurable), |P|  |f |, (w) is a rational number such that ||(w)|| is
O(|f |2 ||f || + |f |2 log(|f |)) for every world w   and   P, and (w)(p) = false for every
world w   and every primitive proposition p not appearing in f .
Proof: The first step in the proof involves showing that if P is a set of probability measures
defined on an algebra  of a finite space , we can assume without loss of generality that
for each set X  , there is a probability measure X  P such that X (X) = P  (X)
(rather than P  (X) just being the sup of (X) for   P).
Lemma A.3: Let P be a set of probability measures defined on an algebra  over a finite set
. Then there exists a set P 0 of probability measures such that, for each X  , P  (X) =
(P 0 ) (X); moreover, there is a probability measure X  P 0 such that X (X) = P  (X). In
addition, for any interpretation , if M = (, , P, ) and M = (, , P 0 , ), then for all
likelihood formulas f , M |= f iff M 0 |= f .

75

fiHalpern & Pucella

Proof: Since  is finite, to show that P 0 exists, it clearly suffices to show that, for each
X  , there is a probability measure X such that X (X) = P  (X) and, if P 0 = P {X },
then P  (Y ) = (P 0 ) (Y ) for all Y  .
Given X, if there exists   P such that (X) = P  (X), then we are done. Otherwise,
we construct a sequence 1 , 2 , . . . of probability measures in P such that limi i (X) =
P  (X) and, for all Y  , the sequence i (Y ) converges to some limit. Let X1 , . . . , Xn
be an enumeration of the sets in , with X1 = X. We inductively construct a sequence of
measures m1 , m2 , . . . in P for m  n such that mi (Xj ) converges to a limit for i  k
and limi mi (X) = P  (X). For m = 1, we know there must be a sequence 11 , 12 , . . .
of measures in P such that 1i (X) converges to P  (X). For the inductive step, if m < n,
suppose we have constructed an appropriate sequence m1 , m2 , . . . . Consider the sequence
of real numbers mi (Xm+1 ). Using the Bolzano-Weierstrass theorem (Rudin, 1976) (which
says that every sequence of real numbers has a convergent subsequence), this sequence has
a convergent subsequence. Let (m+1)1 , (m+1)2 , . . . be the subsequence of m1 , m2 , . . .
which generates this convergent subsequence. This sequence of probability measures clearly
has all the required properties. This completes the inductive step.
Define X (Y ) = limi ni (Y ). It is easy to check that that X is indeed a probability
measure, that X (X) = P  (X), and if P 0 = P  {X }, that P  (Y ) = (P 0 ) (Y ) for all
Y  . This shows that an appropriate set P 0 exists.
Now, given , let M = (, , P, ) and M 0 = (, , P 0 , ). A straightforward induction
on the structure of f shows that M |= f iff M 0 |= f . For the base case:
(, , P, ) |= a1 l(1 ) +    + an l(n )  a
 a1 P  ([[1 ]]M ) +    + an P  ([[n ]]M )  a
 a1 (P 0 ) ([[1 ]]M 0 ) +    + an (P 0 ) ([[n ]]M 0 )  a
 (, , P 0 , ) |= a1 l(1 ) +    + an l(n )  a.
The others cases are trivial.
Just as in FHM, to prove Theorem 5.1, we make use of the following lemma which
can be derived from Cramers rule (Shores, 1999) and simple estimates on the size of the
determinant (see also (Chvatal, 1983) for a simpler variant):
Lemma A.4: If a system of r linear equalities and/or inequalities with integer coefficients
each of length at most l has a nonnegative solution, then it has a nonnegative solution with at
most r entries positive, and where the size of each member of the solution is O(rl +r log(r)).
Continuing with the proof of Theorem 5.1, suppose that f is satisfiable in an upper
probability structure. By Proposition 4.1, the system f of equality formulas has a solution,
so f is satisfied in a upper probability structure with a finite state space. Thus, by Lemma A.3, f is satisfied in a structure M = (, , P, ) such that for all X  , there exists
X  P such that X (X) = P  (X).
As in the completeness proof, we can write f in disjunctive normal form. Each disjunct
g is a conjunction of at most |f |  1 basic likelihood formulas and their negations. Since
M |= f , there must be some disjunct g such that M |= g. Suppose that g is the conjunction
of r basic likelihood formulas and s negations of basic likelihood formulas. Let p1 , . . . , pN
76

fiA Logic for Reasoning about Upper Probabilities

be the primitive formulas appearing in f . Let 1 , . . . , 2N be the atoms over p1 , . . . , pN .
As in the proof of completeness, we derive a system of equalities and inequalities from g.
It is a slightly more complicated system, however. Recall that each propositional formula
over p1 , . . . , pN is a disjunction of atoms. Let 1 , . . . , k be the propositional formulas that
appear in g. Notice that k < |f | (since there are some symbols in f , such as the coefficients,
that are not in the propositional formulas). The system of equations and inequalities we
construct involve variables xij , where i = 1, . . . , k and j = 1, . . . , 2N . Intuitively, xij
represents [[i ]]M ([[ j ]]M ), where [[i ]]M  P is such that [[i ]]M ([[i ]]M ) = P  ([[i ]]M ). Thus,
the system includes k < |f | equations of the following form,
xi1 +    + xi2N = 1,
for i = 1, . . . , k. Since [[i ]]M ([[i ]]M )  ([[i ]]M ) for all   P, if Ei is the subset of
W
{1, . . . , 2N } such that i = jEi j , the system includes k 2  k inequalities of the form
X
X
xij 
xi 0 j ,
jEi

i0

jEi

i0 .

for each pair i,
such that i 6=
For each conjunct in g of the form 1 l(1 ) +    +
n l(k )  , there is a corresponding inequalityP
where, roughly speaking, we replace l(i )
by [[i ]]M ([[]]M ).8 Since [[i ]]M corresponds to jEi xij , the appropriate inequality is
k
X

i

i=1

X

xij  .

jEi

Negations of such formulas correspond to a negated inequality formula; as before, this is
equivalent to a formula of the form
(

k
X
i=1

i

X

xij ) > .

jEi

Notice that there are at most |f | inequalities corresponding to the conjuncts of g. Thus,
altogether, there are at most k(k  1) + 2|f | < |f |2 equations and inequalities in the
system (since k < |f |). We know that the system has a nonnegative solution (taking
xij to be [[i ]]M ([[ j ]]M )). It follows from Lemma A.4 that the system has a solution
x = (x11 , . . . , x12N , . . . , xk1 , . . . , xk2N ) with t  |f |2 entries positive, and with each entry
of size O(|f |2 ||f || + |f |2 log(|f |)).
We use this solution to construct a small structure satisfying the formula f . Let I =
{i : xij is positive, for some j}; suppose that I = {i1 , . . . , it0 }, for some t0  t. Let
M = (S, E, P, ) where S has t0 states, say s1 , . . . , st0 , and E consists of all subsets of S. Let
(sh ) be the truth assignment corresponding to the formula ih , that is, (sh )(p) = true
if and only if ih  p (and where (sh )(p) = false if p does not appear in f ). Define
P = {j : 1  i  k}, where j (sh ) = xih j . It is clear from the construction that M |= f .
Since |P| = k < |f |, |S| = t0  t  |f |2 and j (sh ) = xih j , where, by construction, the size
of xih j is O(|f |2 ||f || + |f |2 log(|f |)), the theorem follows.
8. For simplicity here, we are implicitly assuming that each of the formulas i appears in each conjunct of
g. This is without loss of generality, since if i does not appear, we can put it in, taking i = 0.

77

fiHalpern & Pucella

Appendix B. Proof of the Characterization of Upper Probabilities
To make this paper self-contained, in this appendix we give a proof of Theorem 2.3. The
proof we give is essentially that of Anger and Lembcke (1985). Walley (1991) gives an
alternate proof along somewhat similar lines. Note that the functional g we define in our
proof corresponds to the construction in Walleys Natural Extension Theorem, which is
needed in his version of this result.
Theorem 2.3: Suppose that  is a set,  is an algebra of subsets of , and  :   R.
Then there exists a set P of probability measures with  = P  if and only if  satisfies the
following three properties:
UP1. () = 0,
UP2. () = 1,
UP3. for all integers m, n, k and all subsets A1 , . . . , A
Pm in , if {{A1 , . . . , Am }}
is an (n, k)-cover of (A, ), then k + n(A)  m
i=1 (Ai ).
Proof: The if direction of the characterization is straightforward. Given P = {i }iI a
set of probability measures, we show P  satisfies UP1-UP3.
UP1: P  () = sup{i ()} = sup{0} = 0
UP2: P  () = sup{i ()} = sup{1} = 1
S
T
UP3: GivenSA1 , . . . , Am and
T A such that A  J{1,... ,m},|J|=k+n jJ Aij and
  J{1,... ,m},|J|=k jJ Aij , then for any i we have ki () + ni (A) 
Pm
P
Pm
i (Aj ), that is kP+ ni (A)  m
j=1 i (Aj )  supi { j=1 i (Aj )} 
Pj=1
m
m
 (A ). But sup {k+n (A)} = k+n sup { (A)} =
j
i
i
i
i
j=1 supi {i (Aj )} =
j=1 P P
 (A ), as required.
k + nP  (A), so k + nP  (A)  m
P
j
j=1
As for the only if direction, we first prove a general lemma relating the problem to the
Hahn-Banach Theorem. Some general definitions are needed. Suppose that we are given a
space W and an algebra F of subsets of W . Let K be the vector space generated by the
indicator functions 1X defined by

0 if x 6 X
1X (x) =
1 if x  X,
for X  F. A sublinear functional on K is a mapping c : K  R such that c(h) = c(h)
for   0 and c(h1 + h2 )  c(h1 ) + c(h2 ) for all h1 , h2 . A sublinear functional is increasing
if h  0 implies c(h + h0 )  c(h0 ) for all h0  K. The following result is a formulation of the
well-known Hahn-Banach Theorem (see, for example, (Conway, 1990)).
Theorem (Hahn-Banach): Let K be a vector space over R, and let g be a sublinear
functional on K. If M is a linear subspace in K and  : M  R is a linear functional such
that (x)  g(x) for all x in M, then there is a linear functional 0 : K  R such that
0 |M =  and 0 (x)  g(x) for all x in K.
Lemma B.1: Let g : F  [0, 1] be such that g(W ) = 1 and suppose that there is an
increasing sublinear functional g on K such that
78

fiA Logic for Reasoning about Upper Probabilities

1. g(1K ) = g(K) for K  F;
2. g(h)  0 if h  0;
3. g(1)  1 (where g() is identified with g(1W )).
Then g is an upper probability measure.
Proof: We show that g is an upper probability by exhibiting a set {X : X  } of
probability measures, with the property that X (X) = g(X) and X (Y )  g(X) for Y 6= X.
Each probability measure X is constructed through an application of the Hahn-Banach
Theorem.
Given X  F, define the linear functional  on the subspace generated by 1X by
(1X ) = g(1X ). We claim that (h)  g(h) for all h in the subspace. Since the elements
of the subspace have the form 1X , there are two cases to consider:   0 and  < 0.
If   0, then (1X ) = g(1X ) = g(1X ), since g is sublinear. Moreover, 0 = g(0) =
g(1X + 1X )  g(1X ) + g(1X ), so g(1X )  g(1X ). Thus, if  > 0, then
(1X ) = g(1X )  g(1X ) = g(1X ).
Now, by the Hahn-Banach Theorem, we can extend  to a linear functional 0 on all of
K such that 0 (h)  g(h) for all h. We claim that (a) 0 (1Y )  0 for all Y  K and (b)
0 (1) = 1. For (a), note that 0 (1Y )  g(1Y )  0 by assumption, so 0 (1Y )  0. For
(b), note that 0 (1)  g(1) = g(W ) = 1 and that 0 (1) = 0 (1)  g(1)  1 (since
g(1)  1, by assumption).
Define X (Y ) = 0 (1Y ). Since 0 (1W ) = 1, X (W ) = 1. If Y and Y 0 are disjoint, it is
immediate from the linearity of  that X (Y  Y 0 ) = X (Y ) + X (Y 0 ). By construction,
X (Y )  g(1Y ) = g(Y ) for any Y 6= X, and X (X) = (1X ) = g(1X ) = g(X). Bottom
line: there is a probability measure X dominated by g such that X (X) = g(X).
Take P = {X : X  }. Since for any X we have that X (X) = g(X) and
X (Y )  g(X) (if Y 6= X), we have P  (X) = X (X) = g(X). Therefore, g = P  .
The main result follows by showing how to construct, from a function  satisfying the
properties of Theorem 2.3, a sublinear functional c on K with the required properties.
Suppose that g :   R is a function satisfying UP1-UP3. As we show in the discussion
after Theorem 2.3 in the text, UP1-UP3 show that the range of g is in fact [0, 1].PSince g
m
satisfies UP3, if {{K1 , . . . , Km }} is an (n, k)-cover
Pm of (K, ), we have k +ng(K)  i=1 Ki .
This is equivalent
to saying that k + n1K  Pi=1 1Ki . Hence, for all K1 , . . . , Km such that
P
m
k + n1K  m
1
i=1 Ki , we have k + ng(K) 
i=1 g(Ki ), or equivalently
m

k
1X
 +
g(Ki )  g(K).
n n

(3)

i=1

This observation motivates the following definition of the functional g : K  R 
{, }:
(
)
m
m
k
1X
k
1X
g(h) = inf  +
g(Ki ) : m, n, k  N, m, n > 0, K1 , . . . , Km  F,  +
1Ki  h .
n n
n n
i=1

i=1

Our goal now is to show that g satisfies the conditions of Lemma B.1.
79

fiHalpern & Pucella

 It is almost immediate from the definitions that g is increasing: if h  0 and  nk +
1 Pm
k
1 Pm
0
0
i=1 1Ki  h + h , then  n + n
i=1 1Ki  h .
n
 To see that g is sublinear, note that it is easy to see using the properties of inf that
g(h1 + h2 )  g(h1 ) + g(h2 ). To show that g(h) = g(h) for   0, first observe that
the definition of g is equivalent to
(
)
m
m
X
X
inf  +
i g(Ki ) : m  N, , i  R+ , K1 , . . . , Km  F   +
i 1Ki  h .
i=1

i=1

Consider first the case  > 0. Then
(
)
m
m
X
X
g(h) = inf  +
i g(Ki ) :   +
i 1Ki  h
i=1

= inf

(

 +

m
X
i=1

=  inf

(

i=1

)
m
1X

i 1Ki  h
i g(Ki ) :  +
 


1
 +
 

i=1

m
X
i=1

)
m

1X
i g(Ki ) :  +
i 1Ki  h
 
i=1

= g(h).
For  = 0, it is clear from the definition of g that g(1 )  g(). From (3) it follows
that g(1 )  g(), and hence g(0) = g(1 ) = g() = 0.
 It is immediate from the definition of g that g(1K )  g(K) for K  F; the fact that
g(1K ) = g(K) now follows from (3).
 It is immediate from the definition that g(1)  1.
 If h  0, then h  0; since g is increasing, g(h)  g(h + h) = g(0), and since g is
sublinear, g(0) = 0.
Since the conditions of Lemma B.1 are satisfied, g is an upper probability measure.

References
Anger, B., & Lembcke, J. (1985). Infinitely subadditive capacities as upper envelopes of
measures. Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebiete, 68, 403
414.
Chvatal, V. (1983). Linear Programming. W. Freeman and Co., San Francisco, Calif.
Conway, J. B. (1990). A Course in Functional Analysis (Second edition). No. 96 in Graduate
Texts in Mathematics. Springer-Verlag.
Dempster, A. P. (1967). Upper and lower probabilities induced by a multivalued mapping.
Annals of Mathematical Statistics, 38 (2), 325339.

80

fiA Logic for Reasoning about Upper Probabilities

Dickson, L. E. (1913). Finiteness of the odd perfect and primitive abundant numbers with
n distinct prime factors. American Journal of Mathematics, 35 (4), 413422.
Fagin, R., & Halpern, J. Y. (1991). Uncertainty, belief and probability. Computational
Intelligence, 7 (3), 160173.
Fagin, R., Halpern, J. Y., & Megiddo, N. (1990). A logic for reasoning about probabilities.
Information and Computation, 87 (1,2), 78128.
Giles, R. (1982). Foundations for a theory of possibility. In Gupta, M. M., & Sanchez, E.
(Eds.), Fuzzy Information and Decision Processes, pp. 183195. North-Holland.
Halpern, J. Y. (2002). Reasoning about uncertainty. Book manuscript.
Halpern, J. Y., & Pucella, R. (2002). Reasoning about expectation. In Proc. Eighteenth
Conference on Uncertainty in Artificial Intelligence (UAI 2002).
Huber, P. J. (1976). Kapazitaten statt Wahrscheinlichkeiten? Gedanken zur Grundlegung
der Statistik. Jber. Deutsch. Math.-Verein, 78, 8192.
Huber, P. J. (1981). Robust Statistics. Wiley Interscience.
Kyburg, Jr., H. E. (1987). Bayesian and non-Bayesian evidential updating. Artificial Intelligence, 31, 271293.
Lorentz, G. G. (1952). Multiply subadditive functions. Canadian Journal of Mathematics,
4 (4), 455462.
Mendelson, E. (1964). Introduction to Mathematical Logic. Van Nostrand, New York.
Popkorn, S. (1994). First Steps in Modal Logic. Cambridge University Press, Cambridge;
New York.
Rudin, W. (1976). Principles of Mathematical Analysis (Third edition). McGraw-Hill.
Shafer, G. (1976). A Mathematical Theory of Evidence. Princeton University Press, Princeton, NJ.
Shores, T. (1999). Applied Linear Algebra and Matrix Analysis (Second edition). McGrawHill.
Smith, C. A. B. (1961). Consistency in statistical inference and decision. Journal of the
Royal Statistical Society, Series B, 23, 125.
Walley, P. (1981). Coherent lower (and upper) probabilities. Manuscript, Dept. of Statistics,
University of Warwick.
Walley, P. (1991). Statistical Reasoning with Imprecise Probabilities. Chapman and Hall.
Williams, P. M. (1976). Indeterminate probabilities. In Przelecki, M., Szaniawski, K., &
Wojciki, E. (Eds.), Formal Methods in the Methodology of Empirical Sciences, pp.
229246.
Wilson, N., & Moral, S. (1994). A logical view of probability. In Proc. 11th European
Conference on Artificial Intelligence (ECAI-94), pp. 7195.
Wolf, G. (1977). Obere und Untere Wahrscheinlichkeiten. Doctoral dissertation, Eidgenossischen Technischen Hochschule, Zurich. (Diss. ETH 5884).

81

fiJournal of Artificial Intelligence Research 17 (2002) 171-228

Submitted 3/02; published 9/02

Towards Adjustable Autonomy for the Real World
scerri@isi.edu
pynadath@isi.edu
tambe@usc.edu

Paul Scerri
David V. Pynadath
Milind Tambe
Information Sciences Institute and Computer Science Department
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292 USA

Abstract

Adjustable autonomy refers to entities dynamically varying their own autonomy, transferring decision-making control to other entities (typically agents transferring control to
human users) in key situations. Determining whether and when such transfers-of-control
should occur is arguably the fundamental research problem in adjustable autonomy. Previous work has investigated various approaches to addressing this problem but has often
focused on individual agent-human interactions. Unfortunately, domains requiring collaboration between teams of agents and humans reveal two key shortcomings of these previous
approaches. First, these approaches use rigid one-shot transfers of control that can result in
unacceptable coordination failures in multiagent settings. Second, they ignore costs (e.g.,
in terms of time delays or effects on actions) to an agent's team due to such transfers-ofcontrol.
To remedy these problems, this article presents a novel approach to adjustable autonomy, based on the notion of a transfer-of-control strategy. A transfer-of-control strategy
consists of a conditional sequence of two types of actions: (i) actions to transfer decisionmaking control (e.g., from an agent to a user or vice versa) and (ii) actions to change an
agent's pre-specified coordination constraints with team members, aimed at minimizing
miscoordination costs. The goal is for high-quality individual decisions to be made with
minimal disruption to the coordination of the team. We present a mathematical model
of transfer-of-control strategies. The model guides and informs the operationalization of
the strategies using Markov Decision Processes, which select an optimal strategy, given an
uncertain environment and costs to the individuals and teams. The approach has been
carefully evaluated, including via its use in a real-world, deployed multi-agent system that
assists a research group in its daily activities.
1.

Introduction

Exciting, emerging application areas ranging from intelligent homes (Lesser et al., 1999), to
routine organizational coordination (Pynadath et al., 2000), to electronic commerce (Collins
et al., 2000a), to long-term space missions (Dorais et al., 1998) utilize the decision-making
skills of both agents and humans. These new applications have brought forth an increasing
interest in agents' adjustable autonomy (AA), i.e., in entities dynamically adjusting their own
level of autonomy based on the situation (Mulsiner & Pell, 1999). Many of these exciting
applications will not be deployed unless reliable AA reasoning is a central component. With
AA, an entity need not make all decisions autonomously; rather it can choose to reduce its
own autonomy and transfer decision-making control to other users or agents, when doing so
c 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiScerri, Pynadath & Tambe
is expected to have some net benefit (Dorais et al., 1998; Barber, Goel, & Martin, 2000a;
Hexmoor & Kortenkamp, 2000).
A central problem in AA is to determine whether and when transfers of decision-making
control should occur. A key challenge is to balance two potentially conicting goals. On the
one hand, to ensure that the highest-quality decisions are made, an agent can transfer control to a human user (or another agent) whenever that user has superior decision-making
expertise.1 On the other hand, interrupting a user has high costs and the user may be
unable to make and communicate a decision, thus such transfers-of-control should be minimized. Previous work has examined several different techniques that attempt to balance
these two conicting goals and thus address the transfer-of-control problem. For example,
one technique suggests that decision-making control should be transferred if the expected
utility of doing so is higher than the expected utility of making an autonomous decision
(Horvitz, Jacobs, & Hovel, 1999). A second technique uses uncertainty as the sole rationale
for deciding who should have control, forcing the agent to relinquish control to the user
whenever uncertainty is high (Gunderson & Martin, 1999). Yet other techniques transfer
control to a user if an erroneous autonomous decision could cause significant harm (Dorais
et al., 1998) or if the agent lacks the capability to make the decision (Ferguson, Allen, &
Miller, 1996).
Unfortunately, these previous approaches to transfer-of-control reasoning and indeed
most previous work in AA, have focused on domains involving a single agent and a single
user, isolated from interactions with other entities. When applied to interacting teams of
agents and humans, where interaction between an agent and a human impacts the interaction with other entities, these techniques can lead to dramatic failures. In particular,
the presence of other entities as team members introduces a third goal of maintaining coordination (in addition to the two goals already mentioned above), which these previous
techniques fail to address. Failures occur for two reasons. Firstly, these previous techniques
ignore team related factors, such as costs to the team due to incorrect decisions or due to
delays in decisions during such transfers-of-control. Secondly (and more importantly), these
techniques use one-shot transfers-of-control, rigidly committing to one of two choices: (i)
transfer control and wait for input (choice H ) or (ii) act autonomously (choice A). However,
given interacting teams of agents and humans, either choice can lead to costly failures if the
entity with control fails to make or report a decision in a way that maintains coordination.
For instance, a human user might be unable to provide the required input due to a temporary communication failure; this may cause an agent to fail in its part of a joint action,
as this joint action may be dependent on the user's input. On the other hand, forcing a
less capable entity to make a decision simply to avoid miscoordination can lead to poor
decisions with significant consequences. Indeed, as seen in Section 2.2, when we applied a
rigid transfer-of-control decision-making to a domain involving teams of agents and users,
it failed dramatically.
Yet, many emerging applications do involve multiple agents and multiple humans acting
cooperatively towards joint goals. To address the shortcomings of previous AA work in such
domains, this article introduces the notion of a transfer-of-control strategy. A transfer-ofcontrol strategy consists of a pre-defined, conditional sequence of two types of actions: (i)
1. While the AA problem in general involves transferring control from one entity to another, in this paper,
we will typically focus on interactions involving autonomous agents and human users.

172

fiTowards Adjustable Autonomy for the Real World
actions to transfer decision-making control (e.g., from an agent to a user or vice versa);
(ii) actions to change an agent's pre-specified coordination constraints with team members,
rearranging activities as needed (e.g., reordering tasks to buy time to make the decision).
The agent executes such a strategy by performing the actions in order, transferring control to
the specified entity and changing coordination as required, until some point in time when the
entity currently in control exercises that control and makes the decision. Thus, the previous
choices of H or A are just two of many different and possibly more complex transfer-ofcontrol strategies. For instance, an ADAH strategy implies that an agent initially attempts
to make an autonomous decision. If the agent makes the decision autonomously the strategy
execution ends there. However, there is a chance that it is unable to make the decision
in a timely manner, perhaps because its computational resources are busy with higher
priority tasks. To avoid miscoordination the agent executes a D action which changes the
coordination constraints on the activity. For example, a D action could be to inform other
agents that the coordinated action will be delayed, thus incurring a cost of inconvenience
to others but buying more time to make the decision. If it still cannot make the decision, it
will eventually take action H , transferring decision-making control to the user and waiting
for a response. In general, strategies can involve all available entities and contain many
actions to change coordination constraints. While such strategies may be useful in singleagent single-human settings, they are particularly critical in general multiagent settings, as
discussed below.
Transfer-of-control strategies provide a exible approach to AA in complex systems
with many actors. By enabling multiple transfers-of-control between two (or more) entities,
rather than rigidly committing to one entity (i.e., A or H ), a strategy attempts to provide the
highest quality decision, while avoiding coordination failures. In particular, in a multiagent
setting there is often uncertainty about whether an entity will make a decision and when it
will do so, e.g., a user may fail to respond, an agent may not be able to make a decision as
expected or a communication channel may fail. A strategy addresses such uncertainty by
planning multiple transfers of control to cover for such contingencies. For instance, with the
ADH strategy, an agent ultimately transfers control to a human to attempt to ensure that
some response will be provided in case the agent is unable to act. Furthermore, explicit
coordination-change actions, i.e., D actions, reduce miscoordination effects, for a cost, while
better decisions are being made. Finally, since the utility of transferring control or changing
coordination is dependent on the actions taken afterwards, the agent must plan a strategy
in advance to find the sequence of actions that maximizes team benefits. For example,
reacting to the current situation and repeatedly taking and giving control as in the strategy
ADHADH : : : may be more costly than planning ahead, making a bigger coordination
change, and using a shorter ADH strategy. We have developed a decision theoretic model
of such strategies, that allows the expected utility of a strategy to be calculated and, hence,
strategies to be compared.
Thus, a key AA problem is to select the right strategy, i.e., one that provides the
benefit of high-quality decisions without risking significant costs in interrupting the user and
miscoordination with the team. Furthermore, an agent must select the right strategy despite
significant uncertainty. Markov decision processes (MDPs) (Puterman, 1994) are a natural
choice for implementing such reasoning because they explicitly represent costs, benefits and
uncertainty as well as doing lookahead to examine the potential consequences of sequences
173

fiScerri, Pynadath & Tambe
of actions. In Section 4, a general reward function is presented for an MDP that results in an
agent carefully balancing risks of incorrect autonomous decisions, potential miscoordination
and costs due to changing coordination between team members. Detailed experiments were
performed on the MDP, the key results of which are as follows. As the relative importance
of central factors, such as the cost of miscoordination, was varied the resulting MDP policies
varied in a desirable way, i.e., the agent made more decisions autonomously if the cost of
transferring control to other entities increased. Other experiments reveal a phenomenon not
reported before in the literature: an agent may act more autonomously when coordination
change costs are either too low or too high, but in a \middle" range, the agent tends to act
less autonomously.
Our research has been conducted in the context of a real-world multi-agent system,
called Electric Elves (E-Elves) (Chalupsky, Gil, Knoblock, Lerman, Oh, Pynadath, Russ,
& Tambe, 2001; Pynadath et al., 2000), that we have used for over six months at the
University of Southern California, Information Sciences Institute. The E-Elves assists a
group of researchers and a project assistant in their daily activities, providing an exciting
opportunity to test AA ideas in a real environment. Individual user proxy agents called
Friday (from Robinson Crusoe's servant Friday) act in a team to assist with rescheduling
meetings, ordering meals, finding presenters and other day-to-day activities. Over the course
of several months, MDP-based AA reasoning was used around the clock in the E-Elves,
making many thousands of autonomy decisions. Despite the unpredictability of the user's
behavior and the agent's limited sensing abilities, the MDP consistently made sensible
AA decisions. Moreover, many times the agent performed several transfers-of-control to
cope with contingencies such as a user not responding. One lesson learned when actually
deploying the system was that sometimes users wished to inuence the AA reasoning, e.g.,
to ensure that control was transferred to them in particular circumstances. To allow users
to inuence the AA reasoning, safety constraints are introduced that allow users to prevent
agents from taking particular actions or ensuring that they do take particular actions.
These safety constraints provide guarantees on the behavior of the AA reasoning, making
the basic approach more generally applicable and, in particular, making it more applicable
to domains where mistakes have serious consequences.
The rest of this article is organized as follows. Section 2 gives a detailed description of the
AA problem and presents the Electric Elves as a motivating example application. Section
3 presents a formal model of transfer-of-control strategies for AA. (Readers not interested
in the mathematical details may wish to skip over Section 3.) The operationalization of
the strategies via MDPs is described in Section 4. In Section 5, the results of detailed
experiments are presented. Section 6 looks at related work, including how earlier AA work
can be analyzed within the strategies framework. Section 7 gives a summary of the article.
Finally, Section 8 outlines areas where the work could be extended to make it applicable to
more applications.
2.

Adjustable Autonomy { The Problem

The general AA problem has not been previously formally defined in the literature, particularly for a multiagent context. In the following, a formal definition of the problem is given
so as to clearly define the task for the AA reasoning. The team, which may consist entirely
174

fiTowards Adjustable Autonomy for the Real World
of agents or include humans, has some joint activity, ff. Each entity in the team works
cooperatively on the joint activity. The agent, A, has a role, , in the team. Depending
on the specific task, some or all of the roles will need to be performed successfully in order
for the joint activity to succeed. The primary goal of the agent is the success of ff which
it pursues by performing . Performing  requires that one or more non-trivial decisions
are made. To make a decision, d, the agent can draw upon n other entities from a set
E = fe1 : : : en g, which typically includes the agent itself. Each entity in E (e.g., a human
user) is capable of making decision d. The entities in E are not necessarily part of the
team performing ff. Different agents and users will have differing abilities to make decisions
due to available computational resources, access to relevant information, etc. Coordination
constraints, , exist between  and the roles of other members of the team. For example,
various roles might need to be executed simultaneously or in a certain order or with some
combined quality or total cost. A critical facet of the successful completion of the joint task
ff, given its jointness, is to ensure that coordination between team members is maintained,
i.e.,  are not violated. Thus, we can describe an AA problem instance with the tuple:
hA; ff; ; ; d; E i.
From an AA perspective, the agent can take two types of actions for a decision, d.
First, it can transfer control to an entity in E capable of making that decision. In general,
there are no restrictions on when, how often or for how long decision-making control can
be transferred to a particular entity. Typically, the agent can also transfer decision-making
control to itself. In general, we assume that when the agent transfers control, it does not
have any guarantee on the exact time of response or exact quality of the decision made by
the entity to which control is transferred. In fact, in some cases it will not know whether
the entity will be able to make a decision at all or even whether the entity will know it has
decision-making control, e.g., if control was transferred via email, the agent may not know
if the user actually read the email.
The second type of action that an agent can take is to request changes in the coordination constraints, , between team members. A coordination change gives the agent
the possibility of changing the requirements surrounding the decision to be made, e.g., the
required timing, cost or quality of the decision, which may allow it to better fulfill its responsibilities. A coordination change might involve reordering or delaying tasks or it may
involve changing roles, or it may be a more dramatic change where the team pursues ff in
a completely different way. Changing coordination has some cost, but it may be better
to incur this cost than violate coordination constraints, i.e., incur miscoordination costs.
Miscoordination between team members will occur for many reasons, e.g., a constraint that
limits the total cost of a joint task might be violated if one team member incurs a higher
than expected cost and other team members do not reduce their costs. In this article, we
are primarily concerned with constraints related to the timing of roles, e.g., ordering constraints or requirements on simultaneous execution. This in turn, usually requires that the
agent guards against delayed decisions although it can also require that a decision is not
made too soon.
Thus, the AA problem for the agent, given a problem instance, hA; ff; ; ; d; E i, is to
choose the transfer-of-control or coordination-change actions that maximizes the overall
expected utility of the team. In the remainder of this section we describe a concrete, real175

fiScerri, Pynadath & Tambe
world domain for AA (Section 2.1) and an initial failed approach that motivates our solution
(Section 2.2).

2.1 The Electric Elves
This research was initiated in response to issues that arose in a real application and the
resulting approach was extensively tested in the day-to-day running of that application.
The Electric Elves (E-Elves) is a project at USC/ISI to deploy an agent organization in
support of the daily activities of a human organization (Pynadath et al., 2000; Chalupsky
et al., 2001). We believe this application to be fairly typical of future generation applications involving teams of agents and humans. The operation of a human organization
requires the performance of many everyday tasks to ensure coherence in organizational
activities, e.g., monitoring the status of activities, gathering information and keeping everyone informed of changes in activities. Teams of software agents can aid organizations
in accomplishing these tasks, facilitating coherent functioning and rapid, exible response
to crises. A number of underlying AI technologies support the E-Elves, e.g., technologies
devoted to agent-human interactions, agent coordination, accessing multiple heterogeneous
information sources, dynamic assignment of organizational tasks, and deriving information
about organization members (Chalupsky et al., 2001). While these technologies are useful,
AA is fundamental to the effective integration of the E-Elves into the day-to-day running
of a real organization and, hence, is the focus of this paper.
The basic design of the E-Elves is shown in Figure 1(a). Each agent proxy is called
Friday (after Robinson Crusoes' man-servant Friday) and acts on behalf of its user in the
agent team. The design of the Friday proxies is discussed in detail in (Tambe, Pynadath,
Chauvat, Das, & Kaminka, 2000) (where they are referred to as TEAMCORE proxies).
Currently, Friday can perform several tasks for its user. If a user is delayed to a meeting,
Friday can reschedule the meeting, informing other Fridays, who in turn inform their users.
If there is a research presentation slot open, Friday may respond to the invitation to present
on behalf of its user. Friday can also order its user's meals (see Figure 2(a)) and track the
user's location, posting it on a Web page. Friday communicates with users using wireless
devices, such as personal digital assistants (PALM VIIs) and WAP-enabled mobile phones,
and via user workstations. Figure 1(b) shows a PALM VII connected to a Global Positioning
Service (GPS) device, for tracking users' locations and enabling wireless communication
between Friday and a user. Each Friday's team behavior is based on a teamwork model,
called STEAM (Tambe, 1997). STEAM encodes and enforces the constraints between roles
that are required for the success of the joint activity, e.g., meeting attendees should arrive at
a meeting simultaneously. When a role within the team needs to be filled, STEAM requires
that a team member is assigned responsibility for that role. To find the best suited person,
the team auctions off the role, allowing it to consider a combination of factors and assign
the best suited user. Friday can bid on behalf of its user, indicating whether its user is
capable and/or willing to fill a particular role. Figure 2(b) shows a tool that allows users
to view auctions in progress and intervene if they so desire. In the auction in progress, Jay
Modi's Friday has bid that Jay is capable of giving the presentation, but is unwilling to do
so. Paul Scerri's agent has the highest bid and was eventually allocated the role.

176

fiTowards Adjustable Autonomy for the Real World

Friday

Friday

Friday

Friday

(a)

(b)

Figure 1: (a) Overall E-Elves architecture, showing Friday agents interacting with users.
(b)Palm VII for communicating with users and GPS device for detecting their
location.

177

fiScerri, Pynadath & Tambe
AA is critical to the success of the E-Elves since, despite the range of sensing devices,
Friday has considerable uncertainty about the user's intentions and even location; hence,
Friday will not always have the appropriate information to make correct decisions. On the
other hand, while the user has the required information, Friday cannot continually ask the
user for input, since such interruptions are disruptive and time-consuming. There are four
decisions in the E-Elves to which AA reasoning is applied: (i) whether a user will attend a
meeting on time; (ii) whether to close an auction for a role; (iii) whether the user is willing
to perform an open team role; and (iv) if and what to order for lunch. In this paper, we
focus on the AA reasoning for two of those decisions: whether a user will attend a meeting
on time and whether to close an auction for a role. The decision as to whether a user will
attend a meeting on time is the most often used and most dicult of the decisions Friday
faces. We briey describe the decision to close an auction and later show how an insight
provided by the model of strategies led to a significant reduction in the amount of code
required to implement the AA reasoning for that decision. The decision to volunteer a user
for a meeting is similar to the earlier decisions, and omitted for brevity; the decision to order
lunch is currently implemented in a simpler fashion and is not (at least as yet) illustrative
of the full set of complexities.
A central decision for Friday, which we describe in terms of our problem formulation,
hA; ff; ; ; d; E i, is whether its user will attend a meeting at the currently scheduled meeting time. In this case, Friday is the agent, A. The joint activity, ff, is for the meeting
attendees to attend the meeting simultaneously. Friday acts as proxy for its user, hence
its role, , is to ensure that its user arrives at the currently scheduled meeting time. The
coordination constraint, , between Friday's role and the roles of other Fridays is that they
occur simultaneously, i.e., the users must attend at the currently scheduled time. If any
attendee arrives late, or not at all, the time of all the attendees is wasted; on the other
hand, delaying a meeting is disruptive to users' schedules. The decision, d, is whether the
user will attend the meeting or not and could be made by either Friday or the user, i.e.,
E = fuser; Fridayg. Clearly, the user will be often better placed to make this decision.
However, if Friday transfers control to the user for the decision, it must guard against miscoordination, i.e., having the other attendees wait, while waiting for a user response. Some
decisions are potentially costly, e.g., incorrectly telling the other attendees that the user
will not attend, and Friday should avoid taking them autonomously. To buy more time for
the user to make the decision or for itself to gather more information, Friday could change
coordination constraints with a D action. Friday has several different D actions at its disposal, including delaying the meeting by different lengths of time, as well as being able to
cancel the meeting entirely. The user can also request a D action, e.g., via the dialog box in
Figure 5(a), to buy more time to make it to the meeting. If the user decides a D is required,
Friday is the conduit through which other Fridays (and hence their users) are informed.
Friday must select a sequence of actions, either transferring control to the user, delaying or
cancelling the meeting or autonomously announcing that the user will or will not attend,
to maximize the utility of the team.
The second AA decision that we look at is the decision to close an auction for an open
role and assign a user to that role.2 In this case, the joint activity, ff, is the group research
2. There are also roles for submitting bids to the auction but the AA for those decisions is simpler, hence
we do not focus on them here.

178

fiTowards Adjustable Autonomy for the Real World

(a)

(b)

Figure 2: (a) Friday transferring control to the user for a decision whether to order lunch.
(b) The E-Elves auction monitoring tool.
meeting and the role, , is to be the auctioneer. Users will not always submit bids for the
role immediately; in fact, the bids may be spread out over several days, or some users might
not bid at all. The specific decision, d, on which we focus is whether to close the auction
and assign the role or continue waiting for incoming bids. Once individual team members
provide their bids, the auctioneer agent or human team leader decides on a presenter based
on that input (E = fuser; auctioneer agentg). The team expects a willing presenter to do
a high-quality research presentation, which means the presenter will need some time to
prepare. Thus, the coordination constraint,  is that the most capable, willing user must
be allocated to the role with enough time to prepare the presentation. Despite individually
responsible actions, the agent team may reach a highly undesirable decision, e.g., assigning
the same user week after week, hence there is advantage in getting the human team leader's
input. The agent faces uncertainty (e.g., will better bids come in?), costs (i.e., the later
the assignment, the less time the presenter has to prepare), and needs to consider the
possibility that the human team leader has some special preference about who should do
a presentation at some particular meeting. By transferring control, the agent allows the
human team leader to make an assignment. For this decision, a coordination-change action,
D, would reschedule the research meeting. However, relative to the cost of cancelling the
meeting, the cost of rescheduling is too high for rescheduling to be a useful action.

2.2 Decision-Tree Approach
One logical avenue of attack on the AA problem for the E-Elves was to apply an approach
used in a previously reported, successful meeting scheduling system, in particular CAP
(Mitchell, Caruana, Freitag, McDermott, & Zabowski, 1994). Like CAP, Friday learned
user preferences using C4.5 decision-tree learning (Quinlan, 1993). Friday recorded values
of a dozen carefully selected attributes and the user's preferred action (identified by asking
179

fiScerri, Pynadath & Tambe
the user) whenever it had to make a decision. Friday used the data to learn a decision
tree that encoded its autonomous decision making. For AA, Friday also asked if the user
wanted such decisions taken autonomously in the future. From these responses, Friday
used C4.5 to learn a second decision tree which encoded its rules for transferring control.
Thus, if the second decision tree indicated that Friday should act autonomously, it would
take the action suggested by the first decision tree. Initial tests with the C4.5 approach
were promising (Tambe et al., 2000), but a key problem soon became apparent. When
Friday encountered a decision for which it had learned to transfer control to the user, it
would wait indefinitely for the user to make the decision, even though this inaction caused
miscoordination with teammates. In particular, other team members would arrive at the
meeting location, waiting for a response from the user's Friday, but they would end up
completely wasting their time as no response arrived. To address this problem, if a user
did not respond within a fixed time limit (five minutes), Friday took an autonomous action.
Although performance improved, when the resulting system was deployed 24/7 it led to
some dramatic failures, including:
1. Example 1: Tambe's (a user) Friday incorrectly cancelled a meeting with the division
director because Friday over-generalized from training examples.
2. Example 2: Pynadath's (another user) Friday incorrectly cancelled the group's weekly
research meeting when a time-out forced the choice of an (incorrect) autonomous
action.
3. Example 3: A Friday delayed a meeting almost 50 times, each time by 5 minutes.
It was correctly applying a learned rule but ignoring the nuisance to the rest of the
meeting participants.
4. Example 4: Tambe's Friday automatically volunteered him for a presentation, but he
was actually unwilling. Again Friday over-generalized from a few examples and when
a timeout occurred it took an undesirable autonomous action.
Clearly, in a team context, rigidly transferring control to one agent (user) failed. Furthermore, using a time-out that rigidly transferred control back to the agent, when it was
not capable of making a high-quality decision, also failed. In particular, the agent needed
to better avoid taking risky decisions by explicitly considering their costs (example 1), or
take lower cost actions to delay meetings to buy the user more time to respond (example 2
and 4). Furthermore, as example 3 showed, the agent needed to plan ahead, to avoid taking
costly sequences of actions that could be replaced by a single less costly action (example
3). In theory, using C4.5 Friday might have eventually been able to learn rules that would
successfully balance costs and deal with uncertainty and handle all the special cases and so
on, but a very large amount of training data would be required.
3.

Strategies for Adjustable Autonomy

To avoid rigid one-shot transfers of control and allow team costs to be considered, we
introduce the notion of a transfer-of-control strategy, which is defined as follows:
180

fiTowards Adjustable Autonomy for the Real World
Definition 3.1 A transfer-of-control strategy is a pre-defined, conditional sequence of two

types of actions: (i) actions to transfer decision-making control (e.g., from an agent to
a user or other agents, or vice versa) and (ii) actions to change an agent's pre-specified
coordination constraints with team members, aimed at minimizing miscoordination costs.

The agent executes a transfer-of-control strategy by performing the specified actions in
sequence, transferring control to the specified entity and changing coordination as required,
until some point in time when the entity currently in control exercises that control and
makes the decision. Considering multi-step strategies allows an agent to exploit decisionmaking sources considered too risky to exploit without the possibility of retaking control.
For example, control could be transferred to a very capable but not always available decision
maker then taken back if the decision was not made before serious miscoordination occurred.
More complex strategies, potentially involving several coordination changes, give the agent
the option to try several decision-making sources or to be more exible in getting input from
high-quality decision makers. As a result, transfer-of-control strategies specifically allow an
agent to avoid costly errors, such as those enumerated in the previous section.3
Given an AA problem instance, hA; ff; ; ; d; E i , agent A can transfer decision-making
control for a decision d to any entity ei 2 E , and we denote such a transfer-of-control
action with the symbol representing the entity, i.e., transferring control to ei is denoted as
ei . When the agent transfers decision-making control, it may stipulate a limit on the time
that it will wait for a response from that entity. To capture this additional stipulation,
we denote transfer-of-control actions with this time limit, e.g., ei (t) represents that ei has
decision-making control for a maximum time of t. Such an action has two possible outcomes:
either ei responds before time t and makes the decision, or it does not respond and decision
d remains unmade at time t. In addition, the agent has some mechanism by which it
can change coordination constraints (denoted D) to change the expected timing of the
decision. The D action changes the coordination constraints, , between team members.
The action has an associated value, Dvalue , which specifies its magnitude (i.e., how much
the D has alleviated the temporal pressure), and a cost, Dcost , which specifies the price paid
for making the change. We can concatenate such actions to specify a complete transferof-control strategy. For instance, the strategy H (5)A would specify that the agent first
relinquishes control and asks entity H (denoting the H uman user). If the user responds
with a decision within five minutes, then there is no need to go further. If not, then the
agent proceeds to the next transfer-of-control action in the sequence. In this example, this
next action, A, specifies that the agent itself make the decision and complete the task.
No further transfers of control occur in this case. We can define the space of all possible
strategies with the following regular expression:

S = (E  R)((E  R) + D)

(1)

where (E  R) is all possible combinations of entity and maximum time.
For readability, we will frequently omit the time specifications from the transfer-ofcontrol actions and instead write just the order in which the agent transfers control among
3. In some domains, it may make sense to attempt to get input from more than one entity at once, hence
requiring strategies that have actions that might be executed in parallel. However, in this work, as a first
step, we do not consider such strategies. Furthermore, they are not relevant for the domains at hand.

181

fiScerri, Pynadath & Tambe
the entities and executes Ds (e.g., we will often write HA instead of H (5)A). If time
specifications are omitted, we assume the transfers happen at the optimal times,4 i.e.,
the times that lead to highest expected utility. If we consider strategies with the same
sequence of actions but different timings to be the same strategy, the agent has O(jE jk )
possible strategies to select from, where k is the maximum length of the strategy and jE j
is the number of entities. Thus, the agent has a wide range of options, even if practical
considerations lead to a reasonable upper bound on k and jE j. The agent must select the
strategy that maximizes the overall expected utility of ff.
In the rest of this section, we present a mathematical model of transfer-of-control strategies for AA and use that model to guide the search for a solution. Moreover, the model
provides a tool for predicting the performance of various strategies, justifying their use
and explaining observed phenomena of their use. Section 3.1 presents the model of AA
strategies in detail. Section 3.2 reveals key properties of complex strategies, including dominance relationships among strategies. Section 3.3 examines the E-Elves application in the
light of the model, to make specific predictions about some properties that a successful
AA approach reasoning for that application class will have. These predictions shape the
operationalization of strategies in Section 4.

3.1 A Mathematical Model of Strategies
The transfer-of-control model presented in this section allows calculation of the expected
utility (EU) of individual strategies, thus allowing strategies to be compared. The calculation of a strategy's EU considers four elements: the likely relative quality of different
entities' decisions; the probability of getting a response from an entity at a particular time;
the cost of delaying a decision; and the costs and benefits of changing coordination constraints. While other parameters might also be modeled in a similar manner, our experience
with the E-Elves and other AA work suggests that these parameters are the critical ones
across a wide range of joint activities.
The first element of the model is the expected quality of an entity's decision. In general,
we capture the quality of an entity's decision at time t with the functions EQ = fEQde (t) :
R ! Rg. The quality of a decision reects both the probability that the entity will make an
\appropriate" decision and the costs incurred if the decision is wrong. The expected quality
of a decision is calculated in a decision theoretic way, by multiplying the probability of each
outcome, i.e., each decision, by the utility of that decision, i.e., the cost or benefit of that
decision. For example, the higher the probability that the entity will make a mistake, the
lower the quality, even lower if the mistakes might be very costly. The quality of decision
an entity will make can vary over time as the information available to it changes or as it has
more time to \think". The second element of the model is the probability that an entity
will make a decision if control is transferred to it. The functions, P = fP>e (t) : R ! [0; 1]g,
represent continuous probability distributions over the time that
the entity e will respond.
R t0 ei
That is, the probability that ei will respond before time t0 is 0 P> (t)dt.
The third element of the model is a representation of the cost of inappropriate timing
of a decision. In general, not making a decision until a particular point in time incurs some
4. The best time to transfer control can be found, e.g., by differentiating the expected utility equation in
Section 3.1 and solving for 0.

182

fiTowards Adjustable Autonomy for the Real World
cost that is a function of both the time, t, and the coordination constraints, , between
team members. As stated earlier, we focus on cases of constraint violations due to delays
in making decisions. Thus, the cost is due to the violation of the constraints caused by
not making a decision until that point in time. We can write down a wait-cost function :
W = f (; t) which returns the cost of not making a decision until a particular point in time
given coordination constraints, . This miscoordination cost is a fundamental aspect of our
model given our emphasis on multiagent domains. It is called a \wait cost" because it models
the miscoordination that arises while the team \waits" for some entity to make the ultimate
decision. In domains like E-Elves, the team incurs such wait costs in situations where (for
example) other meeting attendees have assembled in a meeting room at the time of the
meeting, but are kept waiting without any input or decision from Friday (potentially because
it cannot provide a high-quality decision, nor can it get any input from its user). Notice
that different roles will lead to different wait cost functions, since delays in the performance
of different roles will have different effects on the team. We assume that there is some
point in time, , after which no more costs accrue, i.e., if t   then f (; t) = f (; ).
At the deadline, , the maximum cost due to inappropriate timing of a decision has been
incurred. Finally, we assume that, in general, until , the wait cost function is nondecreasing, reecting the idea that bigger violations of constraints lead to higher wait costs.
The final element of the model is the coordination-change action, D, which moves the agent
further away from the deadline and hence reduces the wait costs that are incurred. We
model the effect of the D by letting W be a function of t Dvalue (rather than t) after
the D action and as having a fixed cost, Dcost , incurred immediately upon its execution.
For example, in the E-Elves domain, suppose at the time of the meeting, Friday delays the
meeting by 15 minutes (D action). Then, in the following time period, it will incur the
relatively low cost of not making a decision 15 minutes before the meeting (t Dvalue ),
rather than the relatively high cost of not making the decision at the time of the meeting.
Other, possibly more complex, models of a D action could also be used.
We use these four elements to compute the EU of an arbitrary strategy, s. The utility
derived from a decision being made at time t by the entity in control is the quality of the
entity's decision minus the costs incurred from waiting until t, i.e., EUedc (t) = EQdec (t)
W (t). If a coordination-change action has been taken it will also have an effect on utility.
Until a coordination change of value Dvalue is taken at some time , the incurred wait cost
is W (). Then, between  and t, the wait cost incurred is W (t Dvalue ) W ( Dvalue ).
Thus, if a D action has been taken at time  for cost Dcost and with value Dvalue , the
utility from a decision at time t (t > ) is: EUedc (t) = EQdec (t) W () W ( Dvalue ) +
W (t Dvalue) Dcost. To calculate the EU of an entire strategy, we multiply the response
probability mass function's value at each instant by the EU of receiving a response at that
instant, and then integrate over the products. Hence, the EU for a strategy s given a
problem instance, hA; ff; ; ; d; E i , is:
Z 1
h
A;ff;;

;d;E
i
EUs
=
P (t)EUedc (t) :dt
(2)
0 >
If a strategy involves several actions, we need to ensure that the probability of response
function and the wait-cost calculation reect the control situation at that point in the
strategy. For example, if the user, H , has control at time t, P> (t) should reect H's
183

fiScerri, Pynadath & Tambe

W (0)

EUAd = EQdA (0)
EUed
d
EUeA

=

=



Z

0
Z T

0

EUedDeA =

P>(t)  (EQde (t)
P>(t)  (EQde (t)

W (t)):dt +
W (t)):dt +

1
P>(t)  (EQde (t)


Z

1

Z

T

R

(3)

W (D)):dt (4)

P>(t):dt  (EQda (T )

W (T )) (5)

d
0
0 P> (t)(EQe (t) W (t)):dt +
d
 P>(t)(EQe (t) W () + W ( Dvalue ) W (t Dvalue ) Dcost ):dt +
R1
d
T P> (t)(EQA (t) W () + W ( Dvalue ) W (T Dvalue ) Dcost ):dt

(6)

RT

Table 1: General AA EU equations for sample transfer of control strategies.
probability of responding at t, i.e., P>H (t0 ). To this end, we can break the integral from
Equation 2 into separate terms, with each term representing one segment of the strategy,
e.g., for a strategy UA there would be one term for when U has control and another for
when A has control.
Using this basic technique for writing down EU calculations, we can write down the
specific equations for arbitrary transfer-of-control strategies. Equations 3-6 in Table 1
show the EU equations for the strategies A, e , eA and e DeA respectively. The equations
assume that the agent, A, can make the decision instantaneously (or at least, with no delay
significant enough to affect the overall value of the decision). The equations are created by
writing down the integral for each of the segments of the strategy, as described above. T
is the time when the agent takes control from e , and  is the time at which the D occurs.
One can write down the equations for more complex strategies in the same way. Notice
that these equations make no assumptions about the particular functions.
Given that the EU of a strategy can be calculated, the AA problem for the agent reduces
to finding and following the transfer-of-control strategy that will maximize its EU. Formally,
the agent's problem is:

Axiom 3.1 For a problem hA; ff; ; ; d; E i , the agent must select s 2 S such that 8s0 2
S; s0 6= s; EUshA;ff;;;d;E i  EUsh0A;ff;;;d;E i

184

fiTowards Adjustable Autonomy for the Real World

5
0
-5
0.1

w 0.2

0.3

0.4

1.2
0.8p

Figure 3: Graph comparing the EU of two strategies, H DA (solid line) and H (dashed line)
given a particular instantiation of the model with constant expected decisionmaking quality, exponentially rising wait costs, and Markovian response probabilities. p is a parameter to the P>(t) function, with higher p meaning longer
expected response time. w is a parameter to the W (t) function with higher w
meaning more rapidly accruing wait costs.

3.2 Dominance Relationships among Strategies
An agent could potentially find the strategy with the highest EU by examining each and
every strategy in S, computing its EU, and selecting the strategy with the highest value. For
example, consider the problem for domains with constant expected decision-making quality,
exponentially rising wait costs, and Markovian response probabilities. Figure 3 shows a
graph of the EU of two strategies (H DA and H ) given this particular model instantiation.
Notice that, for different response probabilities and rates of wait cost accrual, one strategy
outperforms the other, but neither strategy is dominant over the entire parameter space.
The EU of a strategy is also dependent on the timing of transfers of control, which in turn
depend on the relative quality of the entities' decision making. Appendix I provides a more
detailed analysis.
Fortunately, we do not have to evaluate and compare each and every candidate in an
exhaustive search to find the optimal strategy. We can instead use analytical methods
to draw general conclusions about the relative values of different candidate strategies. In
particular, we present three Lemmas that show the domain-level conditions under which
particular strategy types are superior to others. The Lemmas also lead us to the, perhaps
surprising, conclusion that complex strategies are not necessarily superior to single-shot
strategies, even in a multi-agent context; in fact, no particular strategy dominates all other
strategies across all domains.
Let us first consider the AA subproblem of whether an agent should ever take back
control from another entity. If we can show that, under certain conditions, an agent should
always eventually take back control, then our strategy selection process can ignore any
strategies where the agent does not do so (i.e., any strategies not ending in A). The agent's
goal is to strike the right balance between not waiting indefinitely for a user response and not
185

fiScerri, Pynadath & Tambe
taking a risky autonomous action. Informally, the agent reasons that it should eventually
make a decision if the expected cost of continued waiting exceeds the difference between the
user's decision quality and its own. More formally, the agent should eventually take back
decision-making control iff, for some time t:
Z 
P> (t0 )W (t0 ):dt0 W (t) > EQdU (t) EQdA (t)
(7)
t

where the left-hand side calculates the future expected wait costs and the right-hand side
calculates the extra utility to be gained by getting a response from the user. This result
leads to the following general conclusion about strategies that end with giving control back
to the agent:
Lemma 1: If s 2 S isRa strategy ending with e 2 E , and s0 is sA, then EUsd0 > EUsd iff
8e 2 E; 9t <  such that t P>(t0 )W (t0 ):dt0 W (t) > EQde (t) EQdA(t)
Lemma 1 says that if, at any point in time, the expected cost of indefinitely leaving
control in the hands of the user exceeds the difference in quality between the agent's and
user's decisions, then strategies which ultimately give the agent control dominate those
which do not. Thus, if the rate of wait cost accrual increases or the difference in the
relative quality of the decision-making abilities decreases or the user's probability of response
decreases, then strategies where the agent eventually takes back control will dominate. A
key consequence of the Lemma (in the opposite direction) is that, if the rate that costs accrue
does not accelerate, and if the probability of response stays constant (i.e., Markovian), then
the agent should indefinitely leave control with the user (if the user had originally been
given control), since the expected wait cost will not change over time. Hence, even if the
agent is faced with a situation with potentially high total wait costs, the optimal strategy
may be a one-shot strategy of handing over control and waiting indefinitely, because the
expected future wait costs at each point in time are relatively low. Thus, Lemma 1 isolates
the condition under which we should consider appending an A transfer-of-control action to
our strategy.
We can perform a similar analysis to identify the conditions under which we should
include a D action in our strategy. The agent has incentive in changing coordination
constraints via a D action due to the additional time made available for getting a highquality response from an entity. However, the overall value of a D action depends on
a number of factors (e.g., the cost of taking the D action and the timing of subsequent
transfers of control). We can calculate the expected value of a D by comparing the EU of a
strategy with and without a D. The D is useful if and only if the increased expected value
of the strategy with it is greater than its cost, Dcost .
Lemma 2: if sR2 S has no D and s0 is s with a D included at t then EUsd0 > EUsd iff
R
P> (t0 )W (t):dt0
P>(t0 )W (tjD):dt0 > Dcost
We can illustrate the consequences of Lemma 2 by considering the specific problem model
of Appendix I (i.e., P> (t) =  exp t , W (t) = ! exp!t , EQde (t) = c, and candidate strategies
d iff ( ! )! exp ( !) (1 exp !Dvalue ) >
eA and e DA). In this case, EUedDA > EUeA
Dcost. Figure 4 plots the value of the D action as we vary the rate of wait cost accumulation,
w, and the parameter of the Markovian response probability function, p. The graph shows
186

fiTowards Adjustable Autonomy for the Real World

Value
0.16
0.12
0.08
0.04
0
-0.04
0.1 0.2
w0.3 0.4 0.5

1
0.75
0.5 p
0.25

Figure 4: The value of D action in a particular model (P> (t) =  exp
and EQde (t) = c).

t ,

W (t) = ! exp!t ,

that the benefit from the D is highest when the probability of response is neither too low
nor too high. When the probability of response is low, the user is unlikely to respond,
even given the extra time; hence, the agent will have incurred Dcost with no benefit. A
D also has little value when the probability of response is high, because the user will likely
respond shortly after the D, meaning that it has little effect (the effect of the D is on the
wait costs after the action is taken). Overall, according to Lemma 2, at those points where
the graph goes above Dcost , the agent should include a D action, and, at all other points, it
should not. Figure 4 demonstrates the value of a D action for a specific subclass of problem
domains, but we can extend our conclusion to the more general case as well. For instance,
while the specific model has exponential wait costs, in models where wait costs grow more
slowly, there will be fewer situations where Lemma 2's criterion holds (i.e., where a D will
be useful). Thus, Lemma 2 allows us to again eliminate strategies from consideration, based
on the evaluation of its criterion in the particular domain of interest.
Given Lemma 2's evaluation of adding a single D action to a strategy, it is natural to
ask whether a second, third, etc. D action would increase EU even further. In other words,
when a complex strategy is better than a simple one, is an even more complex strategy even
better? The answer is \not necessarily".

8K 2 N; 9W 2 W; 9P 2 P; 9EQ 2 EQ such that the optimal strategy
D actions.
Informally, Lemma 3 says that we cannot fix a single, optimal number of D actions,
because for every possible number of D actions, there is a potential domain (i.e., combination
Lemma 3:

has K

of a wait-cost, response-probability, and expected-quality functions) for which that number
of D actions is justified by being optimal. Consider a situation where the cost of a D was
a function of the number of Ds to date (i.e., the cost of the K th D is f (K )). For example,
in the E-Elves' meeting case, the cost of delaying a meeting for the third time is much
higher than the cost of the first delay, since each delay is successively more annoying to
other meeting participants. Hence, the test for the usefulness of the K th D in a strategy,
187

fiScerri, Pynadath & Tambe
given the specific model in Appendix I, is:

!

exp  exp! T )
(8)
f (K ) < !(exp Dvalue! 1)  ( exp T


Depending on the nature of f (K ), Equation 8 can hold for any number of Ds, so, for any
K , there will be some conditions for which a strategy with K Ds is optimal. For instance,
in Section 5.3, we show that the maximum length of the optimal strategy for a random
configuration of up to 25 entities is usually less than eight actions.
Equation 8 illustrates how the value of an additional D can be limited by changing Dcost ,
but Lemma 3 also shows us that other factors can affect the value of an additional D. For
example, even with a constant Dcost , the value of an additional D depends on how many
other D actions the agent performs. Figure 4 shows that the value of the D depends on the
rate at which wait costs accrue. If the rate of wait cost accrual accelerates over time (e.g.,
for the exponential model), a D action slows that acceleration, rendering a second D action
less useful (since the wait costs are now accruing more slowly). Notice also that Ds become
valueless after the deadline, when wait costs stop accruing.
Taken together, Lemmas 1-3 show that no particular transfer-of-control strategy dominates all others across all domains. Moreover, very different strategies, from single-shot
strategies to arbitrarily complex strategies, are appropriate for different situations, although
the range of situations where a particular transfer-of-control action provides benefit can be
quite narrow. Since a strategy might have very low EU for some set of parameters, choosing
the wrong strategy can lead to very poor results. On the other hand, once we understand
the parameter configuration of an intended application domain, Lemmas 1-3 provide useful
tools for focusing the search for an optimal transfer-of-control strategy. The Lemmas can
be used off-line to substantially reduce the space of strategies that need to be searched to
find the optimal strategy. However, in general there may be many strategies and finding
the optimal strategy may not be possible or feasible.

3.3 Model Predictions for the E-Elves
In this section, we use the model to predict properties of a successful approach to AA in
the E-Elves. Using approximate functions for the probability of response, wait cost, and
expected decision quality, we can calculate the EU of various strategies and determine the
types of strategies that are going to be useful. Armed with this knowledge, we can predict
some key properties of a successful implementation.
A key feature of the E-Elves is that the user is mobile. As she moves around the environment, her probability of responding to requests for decisions changes drastically, e.g., she is
most likely to respond when at her workstation. To calculate the EU of different strategies,
we need to know P>(t), which means that we need to estimate the response probabilities
and model how they change as the user moves around. When Friday communicates via a
workstation dialog box, the user will respond, on average, in five minutes. However, when
Friday communicates via a Palm pilot the average user response time is an hour. Users
generally take longer to decide whether they want to present at a research meeting, taking
approximately two days on average. So, the function P>(t) should have an average value
of 5 minutes when the user in her oce, an average of one hour when the user is contacted
via a Palm pilot and an average of two days when the decision is whether to present at a
188

fiTowards Adjustable Autonomy for the Real World
research meeting. It is also necessary to estimate the relative quality of the user, EQdU (t),
and Friday's decision making, EQdA (t). We assume that the user's decision-making EQdU (t)
is high with respect to Friday's, EQdA (t). The uncertainty about user intentions makes it
very hard for Friday to consistently make correct decisions about the time at which the user
will arrive at meetings, although its sensors (e.g., GPS device) give some indication of the
user's location. When dealing with more important meetings, the cost of Friday's errors
is higher. Thus, in some cases, the decision-making quality of the user and Friday will be
similar, i.e., EQUd (t)  EQAd (t); while in other cases, there will be an order of magnitude
difference, i.e., EQUd (t)  10  EQAd (t). The wait cost function, W (t), will be much larger for
big meetings than small and increase rapidly as other attendees wait longer in the meeting
room. Finally, the cost of delays, i.e., Dcost , can vary by about an order of magnitude. In
particular, the cost of rescheduling meetings varies greatly, e.g., the cost of rescheduling
small informal meetings with colleagues is far less than rescheduling a full lecture room at
5 PM Friday.
The parameters laid out above show how parameters vary from decision to decision. For
a specific decision, we use Markovian response probabilities (e.g., when the user is in her
oce, the average response time is five minutes), exponentially increasing wait costs, and
constant decision-making quality (though it changes from decision to decision) to calculate
the EU of interesting strategies. Calculating the EU of different strategies using the values
for different parameters shown above allows us to draw the following conclusions (Table 5
in Section 5.3 presents a quantitative illustration of these predictions):

 The strategy e should not be used, since for all combinations of user location and
meeting importance the EU of this strategy is very low.

 Multiple strategies are required, since for different user locations and meeting importance different strategies are optimal.

 Since quite different strategies are required when the user is in different locations, the
AA reasoning will need to change strategies when the user changes location.

 No strategy has a reasonable EU for all possible parameter instantiations, hence always
using the same strategy will occasionally cause dramatic failures.

 For most decisions, strategies will end with the agent taking a decision, since strategies
ending with the user in control generally have very low EU.

These predictions provide important guidance about a successful solution for AA in the
E-Elves. In particular, they make clear that the approach must exibly choose between
different strategies and adjust depending on the meeting type and user location.
Section 2.2 described the unsuccessful C4.5 approach to AA in E-Elves and identified
several reasons for the mistakes that occurred. In particular, rigidly transferring control to
one entity and ignoring potential team costs involved in an agent's decision were highlighted
as reasons for the dramatic mistakes in Friday's autonomy reasoning. Reviewing the C4.5
approach in the light of the notion of strategies, we see that Friday learned one strategy and
stuck with that strategy. In particular, originally, Friday would wait indefinitely for a user
response, i.e., it would follow strategy e , if it had learned to transfer control. As shown later
189

fiScerri, Pynadath & Tambe
in Table 5, this strategy has a very low EU. When a fixed-length timeout was introduced,
Friday would follow strategy e (5)A. Such a strategy has high EU when EQUd (t)  EQAd (t)
but very low EU when EQUd (t)  10  EQAd (t). Thus, the model explains a phenomenon
observed in practice.
On the other hand, we can use the model to understand that C4.5's failure in this case
does not mean that it will never be useful for AA. Different strategies are only required
when certain parameters (like probability of response or wait cost) change significantly. In
applications where such parameters do not change dramatically from decision to decision,
one particular strategy may always be appropriate. For such applications, C4.5 might learn
the right strategy just with a small amount of training data and perform acceptably well.
4.

Operationalizing Strategies with MDPs

We have formalized the problem of AA as the selection of the transfer-of-control strategy with the highest EU. We now need an operational mechanism that allows an agent to
perform that selection. One major conclusion from the previous section is that different
strategies dominate in different situations, and that applications such as E-Elves will require mechanism(s) for selecting strategies in a situation-sensitive fashion. In particular,
the mechanism must exibly change strategies as the situation changes. The required mechanism must also represent the utility function specified by our expected decision qualities,
EQ, the costs of violating coordination constraints, W, and our coordination-change cost,
Dcost. Finally, the mechanism must also represent the uncertainty of entity responses and
then look ahead over the possible responses (or lack thereof) that may occur in the future.
MDPs are a natural means of performing the decision-theoretic planning required to find
the best transfer-of-control strategy. MDP policies provide a mapping between the agent's
state and the optimal transfer of control strategy. By encoding the parameters of the model
of AA strategies into the MDP, the MDP effectively becomes a detailed implementation of
the model and, hence, assumes its properties. We can use standard algorithms (Puterman,
1994) to find the optimal MDP policy and, hence, the optimal strategies to follow in each
state.
To simplify exposition, as well as to illustrate the generality of the resulting MDP, this
section describes the mapping from AA strategies to the MDP in four subsections. In
particular, Section 4.1 provides a direct mapping of strategies to an abstract MDP. Section
4.2 fills in state features to enable a more concrete realization of the reward function, while
still maintaining a domain-independent view. Thus, the section completely defines a general
MDP for AA is potentially reusable across a broad class of domains. Section 4.3 illustrates
an implemented instantiation of the MDP in E-Elves. Section 4.4 addresses further practical
issues in operationalizing such MDPs in domains such as E-Elves.

4.1 Abstract MDP Representation of AA Problem
Our MDP representation's fundamental state features capture the state of control:

 controlling-entity is the entity that currently has decision-making control.
 ei -response is any response ei has made to the agent's requests for input.
190

fiTowards Adjustable Autonomy for the Real World
Original State Action
Destination State
ectrl time
ectrl ei -response
time
ej
tk
ei
ei
yes
tk+1
ej
tk
ei
ei
no
tk+1
ei
tk
wait
ei
yes
tk+1
ei
tk
wait
ei
no
tk+1
ei
tk
D
ei
no
tk Dvalue

Probability
1
1

R tk+1 ei
tkR P> (t)dt
tk+1 ei
tk P> (t)dt
R tk+1
ei
tkR P> (t)dt
tk+1 ei
tk P> (t)dt

1

Table 2: Transition probability function for AA MDP. ectrl is the controlling-entity.



time is the current time, typically discretized and ranging from 0 to our deadline,
 | i.e., a set ft0 = 0; t1 ; t2 ; : : : ; tn = g.

If ei -response is not null or if time = , then the agent is in a terminal state. In the former
case, the decision is the value of ei -response.
We can specify the set of actions for this MDP representation as = E [fD; waitg. The
set of actions subsumes the set of entities, E , since the agent can transfer decision-making
control to any one of these entities. The D action is the coordination-change action that
changes coordination constraints, as discussed earlier. The \wait" action puts off transferring control and making any autonomous decision, without changing coordination with the
team. The agent should reason that \wait" is the best action when, in time, the situation
is likely to change to put the agent in a position for an improved autonomous decision or
transfer-of-control, without significant harm. For example, in the E-Elves domain, at times
closer to a meeting, users can generally make more accurate determinations about whether
they will arrive on time, hence it is sometimes useful to wait when the meeting is a long
time off.
The transition probabilities (specified in Table 2) represent the effects of the actions as
a distribution over their effects (i.e., the ensuing state of the world). If, in a state with
time = tk , the agent chooses an action that transfers decision-making control to an entity,
ei , other than the agent itself, the outcome is a state with controlling-entity = ei and
time = tk+1 . There are two possible outcomes for ei -response: either the entity responds
with a decision during this transition (producing a terminal state), or it does not, and we
derive the probability distribution over the two from P. The \wait" action has a similar
branch, except that the controlling-entity remains unchanged. Finally, the D action occurs
instantaneously, so there is no time for the controlling entity to respond, but the resulting
state effectively moves to an earlier time (e.g., from tk to tk Dvalue ).
We can derive the reward function for this MDP in a straightforward fashion from
our strategy model. Table 3 presents the complete specification of this reward function.
In transitions that take up time, i.e., transferring control and not receiving a response
(Table 3, row 1) or \wait" (Table 3, row 2), the agent incurs the wait cost of that interval.
In transitions where the agent performs D, the agent incurs the cost of that action (Table 3,
row 3). In terminal states with a response from ei , the agent derives the expected quality of
that entity's decision (Table 3, row 4). A policy that maximizes the reward that an agent
expects to receive according to this AA MDP model will correspond exactly to an optimal
191

fiScerri, Pynadath & Tambe
controlling-entity time ei -response Action
ej
tk
no
ei
ei
tk
no
wait
ei
tk
no
D
ei
tk
yes

Reward

W (k + 1) W (k)
W (k + 1) W (k)
Dcost
EQdei (tk )

Table 3: Reward function for AA MDP.
transfer-of-control strategy. Note that this reward function is described in an abstract
fashion|for example, it does not specify how to compute the agent's expected quality of
decision, EQAd (t).

4.2 MDP Representation of AA Problem within Team Context
We have now given a high-level description of an MDP for implementing the notion of
transfer-of-control strategies for AA. The remainder of this section provides a more detailed
look at the MDP for a broad class of AA domains (including the E-Elves) where the agent
acts on behalf of a user who is filling a role, , within the context of a team activity, ff.
The reward function compares the EU of different strategies, finding the optimal one for
the current state. To facilitate this calculation, we need to represent the parameters used
in the model. We introduce the following state features to capture the aspects of the AA
problem in a team context:

 team-orig-expect- is what the team originally expected of the fulfilling of .
 team-expect- is the team's current expectations of what fulfilling the role  implies.
 agent-expect- is the agent's (probabilistic) estimation for how  will be fulfilled.
 \other ff attributes" encapsulate other aspects of the joint activity that are affected
by the decision.

When we add these more specific features to the generic AA state features already
presented, the overall state, within the MDP representation of a decision d, is a tuple:

hcontrolling-entity; team-orig-expect-; team-expect-; agent-expect-; ff-status;
ei -response; time; other ff attributesi
For example, for a meeting scenario, team-orig-expect- could be \Meet at 3pm", teamexpect- could be \Meet at 3:15pm" after a user requested a delay, and agent-expect- could
be \Meet at 3:30pm" if the agent believes its user will not make the rescheduled meeting.
The transition probability function for the AA MDP in a team context includes our
underlying AA transition probabilities from Table 3, but it must also include probabilities
over these new state features. In particular, in addition to the temporal effect of the
D action described in Section 4.1, there is the additional effect on the coordination of ff.
The D action changes the value of the team-expect- feature (in a domain-dependent but
192

fiTowards Adjustable Autonomy for the Real World
deterministic way). No other actions affect the team's expectations. The team-orig-expect-
feature does not change; we include it to simplify the definition of the reward function. The
transition probabilities over agent-expect- and other ff-specific features are domain-specific.
We provide an example of such transition probabilities in Section 4.3.
The final part of the MDP representation is the reward function. Our team AA MDP
framework uses a reward function that breaks down the function from Table 3 as follows:

R(s; a) = f (team-orig-expect-(s); team-expect- (s); agent-expect- (s);
ff-status (s); time(s); a)
X
=
EQde (time(s))  e -response

(9)

e 2E nfAg

1 f1 (k team-orig-expect- (s) team-expect- (s) k)
21 f21 (time(s))
22 f22 (k team-expect-(s) agent-expect-(s) k)
+3 f3 (ff-status (s)) + 4 f4 (a)

(10)

The first component of the reward function captures the value of getting a response from
a decision-making entity other than the agent itself. Notice that only one entity will actually
respond, so only one e -response will be non-zero. This corresponds to the EQed (t) function
used in the model and the bottom row of Table 3. The f1 function reects the inherent
value of performing a role as the team originally expected, hence deterring the agent from
taking costly coordination changes unless they can gain some indirect value from doing
so. This corresponds to Dcost from the mathematical model and the third row of Table 3.
The f21 corresponds to the second row of Table 3, so it represents the wait cost function,
W (t), from the model. This component encourages the agent to keep other team members
informed of the role's status (e.g., by making a decision or taking an explicit D action),
rather than causing them to wait without information. Functions f22 and f3 represent
the quality of the agent's decision, represented by QAd (t). The standard MDP algorithms
compute an expectation over the agent's reward, and an expectation over this quality will
produce the desired EQAd (t) from the fourth row of Table 3. The first quality function, f22 ,
reects the value of keeping the team's understanding of how the role will be performed in
accordance with how the agent expects the user to actually perform the role. The agent
receives most reward when the role is performed exactly as the team expects, but because of
the uncertainty in the agent's expectation, errors are possible. f22 represents the costs that
come with such errors. The second quality component, f3 , inuences overall reward based
on the successful completion of the joint activity, which encourages the agent to take actions
that maximize the likelihood that the joint activity succeeds. The desire to have the joint
task succeed is implicit in the mathematical model but must be explicitly represented in the
MDP. The component, f4 , augments the first row from Table 3 to account for additional
costs of transfer-of-control actions. In particular, f4 can be broken down further as follows:
(

f4 (a) =

q(e ) if a 2 E
0
otherwise
193

(11)

fiScerri, Pynadath & Tambe
The function q(e ) represents the cost of transferring control to a particular entity, e.g., the
cost of a WAP phone message to a user. Notice, that these detailed, domain-specific costs
do not appear directly in the model.
Given the MDP's state space, actions, transition probabilities, and reward function,
an agent can use value iteration to generate a policy P : S ! that specifies the optimal
action in each state (Puterman, 1994). The agent then executes the policy by taking the
action that the policy dictates in each and every state in which it finds itself. A policy
may include several transfers of control and coordination-change actions. The particular
series of actions depends on the activities of the user. We can then interpret this policy as
a contingent combination of many transfer-of-control strategies, with the strategy to follow
chosen depending on the user's status (i.e., agent-expect-).

4.3 Example: The E-Elves MDPs
An example of an AA MDP is the generic delay MDP, which can be instantiated for any
meeting for which Friday may act on behalf of its user. Recall the decision, d, is whether
to let other meeting attendees wait for a user or to begin their meeting. The joint activity,
ff, is the meeting in which the agent has the role, , of ensuring that its user attends the
meeting at the scheduled time. The coordination constraints, , are that the attendees
arrive at the meeting location simultaneously and the effect of the D action is to delay or
cancel the meeting.
In the delay MDP's state representation, team-orig-expect- is originally-scheduledmeeting-time, since attendance at the originally scheduled meeting time is what the team
originally expects of the user and is the best possible outcome. team-expect- is timerelative-to-meeting, which may increase if the meeting is delayed. ff-status becomes statusof-meeting. agent-expect- is not represented explicitly; instead, user-location is used as
an observable heuristic of when the user is likely to attend the meeting. For example, a
user who is away from the department shortly before a meeting should begin is unlikely to
be attending on time, if at all. With all the state features, the total state space contains
2800 states for each individual meeting, with the large number of states arising from a very
fine-grained discretization of time.
The general reward function is mapped to the delay MDP reward function in the following way.
(

g(N; ff) if N < 4
(12)
1
otherwise
where N is the number of times the meeting is rescheduled and g is a function that takes
into account factors like the number of meeting attendees, the size of the meeting delay and
the time until the originally scheduled meeting time. This function effectively forbids the
agent from ever performing 4 or more D actions.
In the delay MDP, the functions, f21 and f22 , both correspond to the cost of making the
meeting attendees wait, so we can merge them into a single function, f2 . We expect that
such a consolidation is possible in similar domains where the team's expectations relate to
f1 =

194

fiTowards Adjustable Autonomy for the Real World
the temporal aspect of role performance.
(

f2 =

h(late; ff) if late > 0
0
otherwise

(13)

where late is the difference between the scheduled meeting time and the time the user
arrives at the meeting room. late is probabilistically calculated by the MDP based on the
user's current location and a model of the user's behavior.
8
>
<

rff + ruser if the user attends
f3 = > rff
if the meeting takes place, but the user does not attend
: 0
otherwise

(14)

The value, rff , models the inherent value of ff, while the value ruser models the user's
individual value to ff.
f4 was given previously in Equation 11. The cost of communicating with the user
depends on the medium which is used to communicate. For example, there is higher cost
to communicating via a WAP phone than via a workstation dialog box.
When the users are asked for input, it is assumed that, if they respond, their response
will be \correct", i.e., if a user says to delay the meeting by 15 minutes, we assume the
user will arrive on time for the re-scheduled meeting. If the user is asked while in front of
his/her workstation, a dialog like the one shown in Figure 5 is popped up, allowing the user
to select the action to be taken. The expected quality of the agent's decision is calculated
by considering the agent's proposed decision and the possible outcomes of that decision.
For example, if the agent proposes delaying the meeting by 15 minutes, the calculation of
the decision quality includes the probability and benefits that the user will actually arrive
15 minutes after the originally scheduled meeting time, the probability and costs that the
user arrives at the originally scheduled meeting time, etc.

(a)

(b)

Figure 5: (a) Dialog box for delaying meetings. (b) A small portion of the delay MDP.
The delay MDP also represents probabilities that a change in user location (e.g., from
oce to meeting location) will occur in a given time interval. Figure 5(b) shows a portion
195

fiScerri, Pynadath & Tambe
of the state space, showing only the user-response, and user location features. A transition
labeled \delay n" corresponds to the action \delay by n minutes". The figure also shows
multiple transitions due to \ask" (i.e., transfer control to the user) and \wait" actions, where
the relative probability of each outcome is represented by the thickness of the arrow. Other
state transitions correspond to uncertainty associated with a user's response (e.g., when the
agent performs the \ask" action, the user may respond with specific information or may
not respond at all, leaving the agent to effectively \wait"). One possible policy produced
by the delay MDP, for a subclass of meetings, specifies \ask" in state S0 of Figure 5(b)
(i.e., the agent gives up some autonomy). If the world reaches state S3, the policy specifies
\wait". However, if the agent then reaches state S5, the policy chooses \delay 15", which
the agent then executes autonomously. In terms of strategies, this sequence of actions is
H D.
Earlier, we described another AA decision in the E-Elves, namely whether to close an
auction for an open team role. Here, we briey describe the key aspects of the mapping
of that decision to the MDP. The auction must be closed in time for the user to prepare
for the meeting, but with sucient time given for interested users to submit bids and for
the human team leader to choose a particular user. team-orig-expect- (s) is that a highquality presenter be selected with enough time to prepare. There is no D action, hence
team-expect- (s) = team-orig-expect- (s). agent-expect-(s) is whether the agent believes it
has a high-quality bid or believes such a bid will arrive in time for that user to be allocated
to the role. The agent's decision quality, EQdA (t), is a function of the number of bids that
have been submitted and the quality of those bids, e.g., if all team members have submitted
bids and one user's bid stands out, the agent can confidently choose that user to do the
presentation. Thus, ff-status is primarily the quality of the best bid so far and the difference
between the quality of that bid and the second-best bid. The most critical component of
the reward function from Equation 10 is the 2 component, which gives reward if the agent
fulfills the users' expectation of having a willing presenter do a high-quality presentation.

4.4 User-Specified Constraints
The standard MDP algorithms provide the agent with optimal policies subject to the encoded probabilities and reward function. Thus, if the agent designer has access to correct
models of the entities' (e.g., human users in the E-Elves) decision qualities and probabilities of response, then the agent will select the best possible transfer-of-control strategy.
However, it is possible that the entities themselves have more accurate information about
their own abilities than does the agent designer. To exploit this knowledge, an entity could
communicate its model of its quality of decision and probability of response directly to
the agent designer. Unfortunately, the typical entity is unlikely to be able to express its
knowledge in the form of our MDP reward function and transition probabilities. An agent
could potentially learn this additional knowledge on its own through its interactions with
the entities in the domain. However, learning may require an arbitrarily large number of
such interactions, all of which will take place without the benefit of the entities' inside
knowledge.
As an alternative, we can provide a language of constraints that allows the entities to
directly and immediately communicate their inside information to the agent. Our constraint
196

fiTowards Adjustable Autonomy for the Real World

Figure 6: Screenshot of the tool for entering constraints. The constraint displayed forbids
not transferring control (i.e., forces transfer) five minutes before the meeting if the
teammates have previously been given information about the user's attendance
at the meeting.
language provides the entities a simple way to inform the agent of their specific properties
and needs. An entity can use a constraint to forbid the agent from entering specific states or
performing specific actions in specific states. Such constraints can be directly communicated
by a user via the tool shown in Figure 6. For instance, in the figure shown the user is
forbidding the agent from autonomous action five minutes before the meeting. We define
such forbidden-action constraints to be a set, Cfa , where each element constraint is a
boolean function, cfa : S  A !ft; f g. Similarly, we define forbidden-state constraints to
be a set, Cfs , with elements, cfs : S !ft; f g. If a constraint returns t for a particular domain
element (either state or state-action pair, as appropriate), then the constraint applies to the
given element. For example, a forbidden-action constraint, cfa , forbids the action a from
being performed in state s if and only if cfa (s; a) = t.
To provide probabilistic semantics, suitable for an MDP context, we first provide some
notation. Denote the probability that the agent will ever arrive in state sf after following
 s jP ). Then, we define the semantics of
a policy, P , from an initial state si as Pr(si !
f
 s jP ) = 0. The semantics given to a
a forbidden-state constraint cfs as requiring Pr(si !
f
 s ^P (s )=ajP ) = 0
forbidden-action constraint, cfa , is a bit more complex, requiring Pr(si!
f
f
(i.e., cfa forbids the agent from entering state sf and then performing action a). In some
cases, an aggregation of constraints may forbid all actions in state sf . In this case, the
conjunction allows the agent to still satisfy all forbidden-action constraints by avoiding sf
(i.e., the state sf itself becomes forbidden). Once a state, sf , becomes indirectly forbidden
in this fashion, any action that potentially leads the agent from an ancestor state into
sf likewise becomes forbidden. Hence, the effect of forbidding constraints can propagate
backward through the state space, affecting state/action pairs beyond those which cause
immediate violations.
197

fiScerri, Pynadath & Tambe
The forbidding constraints are powerful enough for the entity to communicate a wide
range of knowledge about their decision quality and probability of response to the agent.
For instance, some E-Elves users have forbidden their agents from rescheduling meetings
to lunch time. To do so, the users provide a feature specification of the states they want
to forbid, such as meeting-time =12 PM. Such a specification generates a forbidden-state
constraint, cfs , that is true in any state, s, where meeting-time =12 PM in s. This constraint
effectively forbids the agent from performing any D action that would result in a state where
meeting-time =12PM. Similarly, some users have forbidden autonomous actions in certain
states by providing a specification of the actions they want to forbid, e.g., action 6=\ask".
This generates a forbidden-action constraint, cfa , that is true for any state/action pair,
(s; a), with a 6=\ask". For example, a user might specify such a constraint for states
where they are in their oce, at the time of a meeting because they know that they will
always make decisions in that case. Users can easily create more complicated constraints
by specifying values for multiple features, as well as by using comparison functions other
than = (e.g., 6=, >).
Analogous to the forbidding constraints, we also introduce required-state and requiredaction constraints, defined as sets, Crs and Cra , respectively. The interpretation provided
to the required-state constraint is symmetric, but opposite to that of the forbidden-state
 s jP ) = 1. Thus, from any state, the agent must eventually reach a
constraint: Pr(si !
f
 s ^P (s )=ajP ) = 1.
required state, sf . Similarly, for the required-action constraint, Pr(si!
f
f
The users specify such constraints as they do for their forbidding counterparts (i.e., by specifying the values of the relevant state features or action, as appropriate). In addition, the
requiring constraints also propagate backward. Informally, the forbidden constraints focus
locally on specific states or actions, while the required constraints express global properties
over all states.
The resulting language allows the agent to exploit synergistic interactions between its
initial model of transfer-of-control strategies and entity-specified constraints. For example,
a forbidden-action constraint that prevents the agent from taking autonomous action in a
particular state is equivalent to the user specifying that the agent must transfer control to
the user in that state. In AA terms, the user instructs the agent not to consider any transferof-control strategies that violate this constraint. To exploit this pruning of the strategy
space by the user, we have extended standard value iteration to also consider constraint
satisfaction when generating optimal strategies. Appendix II provides a description of a
novel algorithm that finds optimal policies while respecting user constraints. The appendix
also includes a proof of the algorithm's correctness.
5. Experimental Results

This section presents experimental results aimed at validating the claims made in the previous sections. In particular, the experiments aim to show the utility of complex transfer-ofcontrol strategies and the effectiveness of MDPs as a technique for their operationalization.
Section 5.1 details the use of the E-Elves in daily activities and Section 5.2 discusses the
pros and cons of living and working with the assistance of Fridays. Section 5.3 shows some
characteristics of strategies in this type of domain (in particular, that different strategies
198

fiTowards Adjustable Autonomy for the Real World
are used in practice). Finally, Section 5.4 describes detailed experiments that illustrate
characteristics of the AA MDP.

5.1 The E-Elves in Daily Use
The E-Elves system was heavily used by ten users in a research group at ISI, between June
2000 and December 2000.5 The Friday agents ran continuously, around the clock, seven
days a week. The exact number of agents running varied over the period of execution, with
usually five to ten Friday agents for individual users, a capability matcher (with proxy),
and an interest matcher (with proxy). Occasionally, temporary Friday agents operated on
behalf of special guests or other short-term visitors.
Daily Counts of Exchanged Messages
No. of Messages

300
250
200
150
100
50
0
Jun Jul Aug Sep Oct Nov Dec
Date

Figure 7: Number of daily coordination messages exchanged by proxies over a seven-month
period.
Figure 7 plots the number of daily messages exchanged by the Fridays over seven months
(June through December, 2000). The size of the daily counts reects the large amount of
coordination necessary to manage various activities, while the high variability illustrates
the dynamic nature of the domain (note the low periods during vacations and final exams).
Figure 8(a) illustrates the number of meetings monitored for each user. Over the seven
months, nearly 700 meetings where monitored. Some users had fewer than 20 meetings,
while others had over 250. Most users had about 50% of their meetings delayed (this includes
regularly scheduled meetings that were cancelled, for instance due to travel). Figure 8(b)
shows that usually 50% or more of delayed meetings were autonomously delayed. In this
graph, repeated delays of a single meeting are counted only once. The graphs show that the
5. The user base of the system was greatly reduced after this period due to personnel relocations and
student graduations, but it remains in use with a smaller number of users.

199

fiUser Delays vs. Autonomous Delays

Meetings Monitored vs. Meetings Delayed
400
350
300
250
200
150
100
50
0

140

Number of Meetings

ito

ramanan

tambe

nair

scerri

modi

pynadath

jungh

Total Delays
Human Delays

120

Monitored
Delayed

kulkarni

Number of Meetings

Scerri, Pynadath & Tambe

100
80
60
40
20
0
1

Users

2

3

4

5

6

7

8

Users

(a)

(b)

Figure 8: (a) Monitored vs. delayed meetings per user. (b) Meetings delayed autonomously
vs. by hand.
agents are acting autonomously in a large number of instances, but, equally importantly,
humans are also often intervening, indicating the critical importance of adjustable autonomy
in Friday agents.
For a seven-month period, the presenter for USC/ISI's TEAMCORE research group
presentations was decided using auctions. Table 4 shows a summary of the auction results.
Column 1 (\Date") shows the dates of the research presentations. Column 2 (\No. of
Bids") shows the total number of bids received before a decision. A key feature is that
auction decisions were made without all 9 users entering bids; in fact, in one case, only
4 bids were received. Column 3 (\Best bid") shows the winning bid. A winner typically
bid < 1; 1 >, i.e., indicating that the user it represents is both capable and willing to
do the presentation | a high-quality bid. Interestingly, the winner on July 27 made a
bid of < 0; 1 >, i.e., not capable but willing. The team was able to settle on a winner
despite the bid not being the highest possible, illustrating its exibility. Finally, columns
4 (\Winner") and 5 (\Method") show the auction outcome. An `H' in column 5 indicates
the auction was decided by a human, an `A' indicates it was decided autonomously. In five
of the seven auctions, a user was automatically selected to be presenter. The two manual
assignments were due to exceptional circumstances in the group (e.g., a first-time visitor),
again illustrating the need for AA.
Date
No. of bids Best bid Winner Method
Jul 6, 2001
7
1,1
Scerri
H
Jul 20, 2001
9
1,1
Scerri
A
Jul 27, 2001
7
0,1
Kulkarni
A
Aug 3, 2001
8
1,1
Nair
A
Aug 3, 2001
4
1,1
Tambe
A
Sept 19, 2001
6
-,Visitor
H
Oct 31, 2001
7
1,1
Tambe
A
Table 4: Results for auctioning research presentation slot.
200

fiTowards Adjustable Autonomy for the Real World
5.2 Evaluating the Pros and Cons of E-Elves Use
The general effectiveness of the E-Elves is shown by several observations. During the
E-Elves' operation, the group members exchanged very few email messages to announce
meeting delays. Instead, Fridays autonomously informed users of delays, thus reducing the
overhead of waiting for delayed members. Second, the overhead of sending emails to recruit
and announce a presenter for research meetings was assumed by agent-run auctions. Third,
a web page, where Friday agents post their users' location, was commonly used to avoid
the overhead of trying to track users down manually. Fourth, mobile devices kept users
informed remotely of changes in their schedules, while also enabling them to remotely delay
meetings, volunteer for presentations, order meals, etc. Users began relying on Friday so
heavily to order lunch that one local \Subway" restaurant owner even suggested: \. . . more
and more computers are getting to order food. . . so we might have to think about marketing
to them!!". Notice that this daily use of the E-Elves by a number of different users occurred
only after the MDP implementation of AA replaced the unreliable C4.5 implementation.
However, while the agents ensured that users spent less time on daily coordination (and
miscoordination), there was a price to be paid. One issue was that users felt they had
less privacy when their location was continually posted on the web and monitored by their
agent. Another issue was the security of private information such as credit card numbers
used for ordering lunch. As users adjusted to having agents monitor their daily activities,
some users adjusted their own behavior around that of the agent. One example of such
behavior was some users preferring to be a minute or two early for a meeting lest their
agent decide they were late and delay the meeting. In general, since the agents never made
catastrophically bad decisions most users felt comfortable using their agent and frequently
took advantage of its services.
The most emphatic evidence of the success of the MDP approach is that, since replacing
the C4.5 implementation, the agents have never repeated any of the catastrophic mistakes
enumerated in Section 2.2. In particular, Friday avoids errors such as error 3 from Section
2.2 by selecting a strategy with a single, large D action, because it has a higher EU than a
strategy with many small Ds (e.g., DDDD). Friday avoids error 1, because the large cost
associated with an erroneous cancel action significantly penalizes the EU of a cancellation.
Friday instead chooses the higher-EU strategy that first transfers control to a user before
taking such an action autonomously. Friday avoids errors such as errors 2 and 4 by selecting
strategies in a situation-sensitive manner. For instance, if the agent's decision-making
quality is low (i.e., high risk), then the agent can perform a coordination-change action to
allow more time for user response or for the agent itself to get more information. In other
words, it exibly uses strategies like e DeA, rather than always using the e (5)A strategy
discussed in Section 2.2. This indicates that a reasonably appropriate strategy was chosen
in each situation. Although the current agents do occasionally make mistakes, these errors
are typically on the order of transferring control to the user a few minutes earlier than may
be necessary. Thus, the agents' decisions have been reasonable, though not always optimal.6
6. The inherent subjectivity in user feedback makes a determination of optimality dicult.

201

fiScerri, Pynadath & Tambe
5.3 Strategy Evaluation
The previous section looked at the application of the MDP approach to the E-Elves but did
not address strategies in particular. In this section, we specifically examine strategies in the
E-Elves. We show that Fridays did indeed follow strategies and that the strategies followed
were the ones predicted by the model. We also show how the model led to an insight that,
in turn, led to a dramatic simplification in one part of the implementation. Finally, we show
that the use of strategies is not limited to the E-Elves application by showing empirically
that, for random configurations of entities, the optimal strategy will have more than one
transfer-of-control action in 70% of cases.
Figure 9 shows a frequency distribution of the number of actions taken per meeting
(this graph omits \wait" actions). The number of actions taken for a meeting corresponds
to the length of the part of the strategy followed (the strategy may have been longer, but
a decision was made so the actions were not taken). The graph shows both that the MDP
followed complex strategies in the real world and that it followed different strategies at
different times. The graph bears out the model's predictions that different strategies would
be required of a good solution to the AA problem in the E-Elves domain.
Table 5 shows the EU values computed by the model and the strategy selected by
the MDP. Recall that the MDP explicitly models the users' movements between locations,
while the model assumes that the users do not move. Hence, in order to do an accurate
comparison between the model and the MDP's results, we focus on only those cases when
the user's location does not change (i.e., where the probability of response is constant).
These EU values were calculated using the parameter values set out in Section 3.3. Notice,
that the MDP will often perform Ds before transferring control to buy time to reduce
uncertainty. The model is an abstraction of the domain, so such D actions, like changes
in user location, are not captured. Except for a slight discrepancy in the first case the
match between the MDP's behavior and the model's predictions is exact, provided that we
ignore the D actions at the beginning of some MDP strategies. Thus, despite the model
being considerably abstracted from the domain there is high correlation between the MDP
policies and the model's suggested strategies. Moreover, general properties of the policies
that were predicted by the model were borne out exactly. In particular, recall that the model
predicted different strategies would be required, that strategy e would not be used, and that
generally strategies ending in A would be best | all properties of the MDP policies.
The model predicts that if parameters do not vary greatly then it is sucient to find
a single optimal strategy and follow that strategy in each situation. The MDP for the
decision to close an auction is an instance of this for the E-Elves. The same pattern of
behavior is followed every time an open role needs to be filled by the team. This consistency
arises because the wait cost is the same (since the meetings are the same) and because
the pattern of incoming bids is reasonably consistent (variations in individuals' behavior
cancel each other out when we look at the team as a whole). The model predicts that
when parameters do not change, we can find the optimal strategy for those parameters
and execute that strategy every time. However, since the MDP had worked effectively for
the meeting AA, an MDP was also chosen for implementing the auction AA. When it was
realized that the parameters do not vary greatly, we concluded the MDP could be replaced
with a simple implementation of the optimal strategy. To verify this hypothesis, we replaced
202

fiTowards Adjustable Autonomy for the Real World

No. of meetings

No. of actions per meeting
200
180
160
140
120
100
80
60
40
20
0
0

2

4
6
8
No. of actions

10

12

Figure 9: The frequency distribution of the number of steps taken in an AA strategy for
the meeting scenario. If no actions were taken for a meeting, the meeting was
cancelled before Friday started AA reasoning.

Location
A
e
eA e DA MDP
Small meeting, active participant
oce
14.8 -277 41.9 42.05 DDe DA
not @ dept. 14.8 -6E7 31.4 28.0 DDeA
@ meet loc. 14.8 -2E5 39.2 39.1
eA
Large meeting, passive participant
oce
14.6 -7E12 30.74 30.65 DDeA
not @ dept. 14.6 -2E17 14.6 7.7
DDeA
@ meet loc. 14.5 -7E14 25.1 23.5
eA
Table 5: EU values for the simple strategies as calculated from the model. The last column
shows the strategy actually followed by the MDP.

203

fiScerri, Pynadath & Tambe
Date No. Bids MDP eA
7/20/00
9
25% 26%
7/27/00
7
14% 20%
8/3/00
8
29% 23%
Table 6: Auction results. The \MDP" column shows the percentage of available auction
time remaining when the MDP chose to close the auction. The \eA" column
shows the percentage of available auction time remaining when the strategy eA,
with EQde (t) proportional to the number of bids received (\No. Bids" column),
would have closed the auction.

the general MDP code with three simple lines of code implementing the eA strategy, which
we determined to be optimal for the particular parameters of the problem. Using log files
recorded during the actual auctions reported in (Scerri, Pynadath, & Tambe, 2001), we
experimentally verified that both the MDP and the eA strategy produced the same result.
Table 6 shows the percentage of available auction time remaining (e.g., if the auction was
opened four days before the role should be performed, closing the auction one day before
would correspond to 25%) when the MDP version and the eA version of the code closed
the auction. The number of bids is used to estimate the agent's expected decision quality.
The timing of the auction closing is close, certainly within just a few hours. The result
is not precisely the same for the MDP and strategy implementations, because the MDP
implementation was more reactive to incoming bids than the strategy implementation.
To confirm that the need for strategies was not a phenomenon unique to the particular
settings of the E-Elves, an experiment was run with randomly generated configurations
of entities. The wait cost for each configuration increased exponentially, with the rate of
accrual varying from configuration to configuration. The configurations contained between
3 and 25 entities, with randomly chosen Markovian response probabilities and randomly
chosen, but constant, decision-making quality. The cost and value of a D action was also
randomly selected. In each configuration, there was an agent that could respond instantly,
but with lower decision quality than any of the other entities. For each configuration, the
optimal transfer-of-control strategy was found. Figure 10(a) shows the percentage of optimal
strategies (z-axis) that were of each length (y-axis \jOpt. Strat.j"), separated according to
the rate at which wait costs accrued (x-axis, \Wait Cost Param"). The figure shows that
if the rate at which the wait cost accrues is very low, most optimal strategies are of length
one, with the agent just handing control to the entity with the highest decision-making
quality. When the rate of wait cost accrual is high, most strategies are of length two,
with the agent briey giving the best decision maker an opportunity to make the decision
but taking back control and acting before the wait costs became too high. For intermediate
values of the wait cost parameter, there was considerably more variation in the length of the
optimal strategy. Figure 10(b) shows the percentage of optimal strategies for each length
when the wait cost parameter is 0.12 (i.e., a slice through Figure 10(a)). Hence, strategies
often contained several transfers of control and several coordination changes. Thus, this
experiment shows that complex transfer-of-control strategies are useful, not only in E-Elves,
204

fiTowards Adjustable Autonomy for the Real World
but in a range of domains, especially those in which wait costs are neither negligible nor
accruing too fast.
Strategy Lengths for w = 0.12
35

% of Opt. Strats.

30
25
% of Opt. Strats.

100
90
80
70
60
50
40
30
20
10
01

2

20
15
10

3

|Opt. Strat.|

4

5

6

7

8 0

0.35 0.4
0.25 0.3
0.15 0.2
0.1
Wait
Cost
Param
0.05

5
0
1

(a)

2

3

4
5
|Opt. Strat.|

6

7

8

(b)

Figure 10: (a) Percentage of optimal strategies having a certain length, broken down according to how fast wait costs are accruing. (b) Percentage of optimal strategies
having certain length for wait cost parameter = 0.12.
Thus, we have shown that the MDP produces strategies and that Friday follows these
strategies in practice. Moreover, the strategies followed are the ones predicted by the model.
Of practical use, when we followed a prediction of the model, i.e., that an MDP was not
required for auctions, we were able to substantially reduce the complexity of one part of the
system. Finally, we showed that the need for strategies was not specifically a phenomenon
of the E-Elves domain.

5.4 MDP Experiments
Experience using the MDP approach to AA in the E-Elves indicates that it is effective
at making reasonable AA decisions. However, in order to determine whether MDPs are
a generally useful tool for AA reasoning, more systematic experiments are required. In
this section, we present such systematic experiments to determine important properties of
MDPs for AA. The MDP reward function is designed to result in the optimal strategy being
followed in each state.
In each of the experiments, we vary one of the  parameters that are the weights of the
different factors in Equation 10. The MDP is instantiated with each of a range of values
for the parameter and a policy produced for each value. In each case, the total policy is
defined over 2800 states. The policy is analyzed to determine some basic properties of that
policy. In particular, we counted the number of states in which the policy specifies to ask,
to delay, to say the user is attending and to say the user is not attending. The statistics
show broadly how the policy changes as the parameters change, e.g., whether Friday gives
up autonomy more or less when the cost of a coordination change is increased. The first
aim of the experiments is to simply confirm that policies change in the desired and expected
way when parameters in the reward function are changed. For instance, if Friday's expected
decision quality is increased, there should be more states where it makes an autonomous
205

fiScerri, Pynadath & Tambe
decision. Secondly, from a practical perspective it is critical to understand how sensitive the
MDP policies are to small variations in parameters, because such sensitivity would mean
that any small variations in parameter values can significantly impact MDP performance.
Finally, the experiments reveal some interesting phenomena.
The first experiment looks at the effect of the 1 parameter from Equation 10, represented in the delay MDP implementation by the team repair cost (function g from Equation
12), on the policies produced by the delay MDP. This parameter determines how averse Friday should be to changing coordination constraints. Figure 11 shows how some properties
of the policy change as the team repair cost value is varied. The x-axis gives the value of
the team repair cost, and the y-axis gives the number of times that action appears in the
policy. Figure 11(a) shows the number of times Friday will ask the user for input. The
number of times it will transfer control exhibits an interesting phenomenon: the number
of asks has a maximum at an intermediate value for the parameter. For the low values,
Friday can \confidently" (i.e., its decision quality is high) make decisions autonomously,
since the cost of errors is low, hence there is less value to relinquishing autonomy. For
very high team repair costs, Friday can \confidently" decide autonomously not to make a
coordination change. It is in the intermediate region that Friday is uncertain and needs
to call on the user's decision making more often. Furthermore, as the cost of delaying the
meeting increases, Friday will delay the meeting less (Figure 11(b)) and tell the team the
user is not attending more often (Figure 11(d)). By doing so, Friday gives the user less time
to arrive at the meeting, choosing instead to just announce that the user is not attending.
Essentially, Friday's decision quality has become close enough to the user's decision quality
that asking the user is not worth the risk that they will not respond and the cost of asking
for their input. Except for a jump between a value of zero and any non-zero value, the
number of times Friday says the user is attending does not change (Figure 11(c)). The
delay MDP in use in the E-Elves has the team repair cost parameter set at two. Around
this value the policy changes little, hence slight changes in the parameter do not lead to
large changes in the policy.
In the second experiment, we vary the 2 parameter from Equation 10, implemented
in the delay MDP by the variable team wait cost (function h from Equation 13). This is
the factor that determines how heavily Friday should weigh differences between how the
team expects the user will fulfill the role and how the user will actually fulfill the role. In
particular, it determines the cost of having other team members wait in the meeting room
for the user. Figure 12 shows the changes to the policy when this parameter is varied (again
the x-axis shows the value of the parameter and the y-axis shows the number of times the
action appears in the policy). The graph of the number of times the agent asks in the
policy (Figure 12(a)), exhibits the same phenomena as when the 1 parameter was varied,
i.e., increasing and then decreasing as the parameter increases. The graphs show that, as
the cost of teammates' time increases, Friday acts autonomously more often (Figure 12(bd)). Friday asks whenever the potential costs of asking are lower than the potential costs
of errors it makes { as the cost of time waiting for a user decision increases, the balance
tips towards acting. Notice that the phenomenon of the number of asks increasing then
decreasing occurs in the same way that it did for the 1 parameter; however, it occurs for a
slightly different reason. In this case, when waiting costs are low, Friday's decision-making
quality is high so it acts autonomously. When the waiting costs are high, Friday cannot
206

fiTowards Adjustable Autonomy for the Real World

Number of delays in policy

68
66
64
62
60
58
56
54
52
50
48

# delays

# asks

Number of asks in policy

0

2
4
6
8
"Team repair cost" weight

140
130
120
110
100
90
80
70
60
50
40
30

10

0

(a)

Number of Not Attending messages in policy
# Not Attending

# attending

140
135
130
125
120
115
110
105
100
95
90
2
4
6
8
"Team repair cost" weight

10

(b)

Number of Attending messages in policy

0

2
4
6
8
"Team repair cost" weight

10

70
60
50
40
30
20
10
0
0

(c)

2
4
6
8
"Team repair cost" weight

(d)

Figure 11: Properties of the MDP policy as team repair cost is varied.

207

10

fiScerri, Pynadath & Tambe
afford the risk that the user will not respond quickly, so it again acts autonomously (despite
its decision quality being low). Figure 12(b) shows that the number of delay actions taken
by Friday increases, but only in states in which the meeting has already been delayed twice.
This indicates that the normally very expensive third delay of the same meeting starts to
become worthwhile if the cost of having teammates wait in the meeting room is very high.
In the delay MDP, a value of 1 is used for 2 . The decision to transfer control (i.e., ask)
is not particularly sensitive to changes in the parameter around this value|again, slight
changes will not have a significant impact.
Number of Asks in policy

Number of Delays in policy

70
# delays

# asks

50
40
30

100
80
60
40
20
0

20
0

2

4

6

8

10

0

"Cost of teammates time" weight

4

6

8

10

(b)

Number of Not Attending messages in policy
30
# Not Attending

Number of Attending messages in policy
260
240
220
200
180
160
140
120
100
80
0

2

"Cost of teammates time" weight

(a)

# Attending

Total
1st Delay
2nd Delay
3rd Delay

120

60

2
4
6
8
10
"Cost of teammates time" weight

25
20
15
10
5
0
0

(c)

2
4
6
8
10
"Cost of teammates time" weight

(d)

Figure 12: Properties of the MDP policy as teammate time cost is varied. (b) shows the
number of times the meeting is delayed in states where it has not yet been
delayed, where it has been delayed once already, and where it has been delayed
twice already.
In the third experiment, the value of the 3 , the weight of the joint task, was varied
(Figure 13). In the E-Elves, the value of the joint task includes the value of the user to the
meeting and the value of the meeting without the user. In this experiment, the value of the
208

fiTowards Adjustable Autonomy for the Real World
meeting without the user is varied. Figure 13 shows how the policy changes as the value of
the meeting without the user changes (again the x-axis shows the value of the parameter
and the y-axis shows the number of times the action appears in the policy). These graphs
show significantly more instability than for the other  values. These large changes are a
result of the simultaneous change in both the utility of taking key actions and the expected
quality of Friday's decision making, e.g., the utility of saying the user is attending is much
higher if the meeting has very low value without that user. In the current delay MDP, this
value is set at 0.25, which is in a part of the graph that is very insensitive to small changes
of the parameter.
In the three experiments above, the specific E-Elves parameters were in regions of the
graph where small changes in the parameter do not lead to significant changes in the policy.
However, there were regions of the graphs where the policy did change dramatically for small
changes in a parameter. This indicates that in some domains, with parameters different to
those in E-Elves, the policies will be sensitive to small changes in the parameters.

180
160
140
120
100
80
60
40
20
0
-10

Number of delays in policy
120
100
# delays

# asks

Number of asks in policy

80
60
40

-8

-6
-4
-2
0
Joint activity weight

20
-10

2

-8

(a)

Number of Not Attending messages in policy
20
# not attending

# attending

180
160
140
120
-8

-6
-4
-2
0
Joint activity weight

2

(b)

Number of Attending messages in policy
200

100
-10

-6
-4
-2
0
Joint activity weight

15
10
5
0
-10

2

(c)

-8

-6
-4
-2
Joint activity weight

0

2

(d)

Figure 13: Properties of the MDP policy as the importance of a successful joint task is
varied.
209

fiScerri, Pynadath & Tambe
The above experiments show three important properties of the MDP approach to AA.
First, changing the parameters of the reward function generally lead to the changes in the
policy that are expected and desired. Second, while the value of the parameters inuenced
the policy, the effect on the AA reasoning was often reasonably small, suggesting that small
errors in the model should not affect users too greatly. Finally, the interesting phenomena of
the number of asks reaching a peak at intermediate values of the parameters was revealed.
The three previous experiments have examined how the behavior of the MDP changes
as the parameters of the reward function are changed. In another experiment, a central
domain-level parameter affecting the behavior of the MDP, i.e., the probability of getting a
user response and the cost of getting that response (corresponding to f4 ), is varied. Figure
14 shows how the number of times Friday chooses to ask (y-axis) varies with both the
expected time to get a user response (x-axis) and the cost of doing so (each line on the
graph represents a different cost). The MDP performs as expected, choosing to ask more
often if the cost of doing so is low and/or it is likely to get a prompt response. Notice
that, if the cost is low enough, Friday will sometimes choose to ask the user even if there
is a long expected response time. Conversely, if the expected response time is suciently
high, Friday will assume complete autonomy. This graph also shows that there is a distinct
change in the number of asks at some point (depending on the cost), but outside this change
point the graphs are relatively at. The key reason for the fairly rapid change in the number
of asks is that often the difference between the quality of Friday's and the user's decision
making is in a fairly small range. As the mean response time increases, the expected wait
costs increase, eventually becoming high enough for Friday to decide to act autonomously
instead of asking.

# Asks

Number of Asks in Policy
70
60
50
40
30
20
10
0
0.01

Cost = 0.0001
Cost = 0.2
Cost = 1.0

0.1
1
10
Mean Response Time

100

Figure 14: Number of ask actions in policy as the mean response time (in minutes) is varied.
The x-axis uses a logarithmic scale.
We conclude this section with a quantitative illustration of the impact constraints have
on strategy selection. In this experiment, we merged user-specified constraints from all the
E-Elves users, resulting in a set of 10 distinct constraints. We started with an unconstrained
210

fiTowards Adjustable Autonomy for the Real World

Figure 15: (a) Number of possible strategies (logarithmic). (b) Time required for strategy
generation.
instance of the delay MDP and added these constraints one at a time, counting the strategies
that satisfied the applied constraints. We then repeated these experiments on expanded
instances of the delay MDP, where we increased the initial state space by increasing the
frequency of decisions (i.e., adding values to the time-relative-to-meeting feature). This
expansion results in three new delay MDPs, which are artificial, but are inuenced by the
real delay MDP. Figure 15a displays these results (on a logarithmic scale), where line A
corresponds to the original delay MDP (2760 states), and lines B (3320 states), C (3880
states), and D (4400 states) correspond to the expanded instances. Each data point is a
mean over five different orderings of constraint addition. For all four MDPs, the constraints
substantially reduce the space of possible agent behaviors. For instance, in the original
delay MDP, applying all 10 constraints eliminated 1180 of the 2760 original states from
consideration, and reduced the mean number of viable actions per acceptable state from
3.289 to 2.476. The end result is a 50% reduction in the size (log10 ) of the strategy space.
On the other hand, constraints alone did not provide a complete strategy, since all of the
plots stay well above 0, even with all 10 constraints. Since none of the individual users were
able/willing to provide 10 constraints, we cannot expect anyone to add enough constraints
to completely specify an entire strategy. Thus, the MDP representation and associated
policy selection algorithms are still far from redundant.
The constraints' elimination of behaviors also decreases the time required for strategy
selection. Figure 15b plots the total time for constraint propagation and value iteration over
the same four MDPs as in Figure 15a (averaged over the same five constraint orderings).
Each data point is also a mean over five separate iterations, for a total of 25 iterations
per data point. The values for the zero-constraint case correspond to standard value iteration without constraints. The savings in value iteration over the restricted strategy space
dramatically outweigh the cost of pre-propagating the additional constraints. In addition,
the savings increase with the size of the MDP. For the original delay MDP (A), there is
a 28% reduction in policy-generation time, while for the largest MDP (D), there is a 53%
reduction. Thus, the introduction of constraints can provide dramatic acceleration of the
agent's strategy selection.

211

fiScerri, Pynadath & Tambe
6.

Related Work

We have discussed some related work in Section 1. This section adds to that discussion.
In Section 6.1, we examine two representative AA systems { where detailed experimental
results have been presented { and explain those results via our model. This illustrates the
potential applicability of our model to other systems. In Section 6.2, we examine other AA
systems and other areas of related work, such as meta-reasoning, conditional planning and
anytime algorithms.

6.1 Analyzing Other AA Work Using the Strategy Model
Goodrich, Olsen, Crandall, and Palmer (2001) report on tele-operated teams of robots,
where both the user's high-level reasoning and the robots' low-level skills are required to
achieve some task. Within this domain, they have examined the effect of user neglect on
robot performance. The idea of user neglect is similar to our idea of entities taking time
to make decisions; in this case, if the user \neglects" the robot, the joint task takes longer
to perform. In this domain, the coordination constraint is that user input must arrive so
that the robot can work out the low-level actions it needs to perform. Four control systems
were tested on the robot, each giving a different amount of autonomy to the robot, and the
performance was measured as user neglect was varied.
Although quite distinct from the E-Elves system, mapping Goodrich's team of robots
to our AA problem formulation provides some interesting insights. This system has the
interesting feature that the entity the robot can call on for a decision, i.e., the user, is also
part of the team. Changing the autonomy of the robot effectively changes the nature of
the coordination constraints between the user and robot. Figure 16 shows the performance
(y-axis) of the four control policies as the amount of user neglect was increased (x-axis).
The experiments showed that higher robot autonomy allowed the operator to \neglect" the
robot more without as serious an impact on its performance.
The notion of transfer-of-control strategies can be used to qualitatively predict the same
behavior as was observed in practice, even though Goodrich et al. (2001) did not use the
notion of strategies. The lowest autonomy control policy used by Goodrich et al. (2001)
was a pure tele-operation one. Since the robot cannot resort to its own decision making,
we represent this control policy with a strategy U , i.e., control indefinitely in the hands
of the user. The second control policy allows the user to specify waypoints and on-board
intelligence works out the details of getting to the waypoints. Since the robot has no highlevel decision-making ability, the strategy is simply to give control to the user. However,
since the coordination between the robot and user is more abstract, i.e., the coordination
constraints are looser, the wait cost function is less severe. Also the human is giving less
detailed guidance than in the fully tele-operated case (which is not as good according to
(Goodrich et al., 2001)), hence we use a lower value for the expected quality of the user
decision. We denote this approach Uw p to distinguish it from the fully tele-operated case.
The next control policy allows the robot to choose its own waypoints given that the user
inputs regions of interest. The robot can also accept waypoints from the user. The ability
for the robot to calculate waypoints is modeled as a D, since it effectively changes the
coordination between the entities, by removing the user's need to give waypoints. We model
this control policy as the strategy U DU . The final control policy is full autonomy, i.e., A.
212

fiTowards Adjustable Autonomy for the Real World
Performance

A

UDU
U wp

U

Neglect

(a)
Goodrich robot operation EU
60
40
EU

20
0
-20
-40
-60
2

1.5

(b)

1
p

0.5

0

Figure 16: Goodrich at al's various control strategies plotted against neglect. (a) Experimental results. Thinner lines represent control systems with more intelligence
and autonomy. (b) Results theoretically derived from model of strategies presented in this article (p is the parameter to the probability of response function).
Robot decision making is inferior to that of the user, hence the robot's decision quality is less
than the user's. The graphs of the four strategies, plotted against the probability of response
parameter (getting smaller to the right, to match \neglect" in the Goodrich et al graph) is
shown in Figure 16. Notice that the shape of the graph theoretically derived from our model,
shown in Figure 16(b), is qualitatively the same as the shape of the experimentally derived
graph, Figure 16(a). Hence, the theory predicted qualitatively the same performance as
was found from experimentation.
A common assumption in earlier AA work has been that if any entity is asked for a
decision it will make that decision promptly, hence strategies handling the contingency
213

fiScerri, Pynadath & Tambe
of a lack of response have not been required. For example, Horvitz's (1999) work using
decision theory is aimed at developing general, theoretical models for AA reasoning for a
user at a workstation. A prototype system, called LookOut, for helping users manage their
calendars has been implemented to test these ideas (Horvitz, 1999). Although such systems
are distinctly different from E-Elves, mapping them to our problem formulation allows us to
analyze the utility of the approaches across a range of domains without having to implement
the approach in those domains.
A critical difference between Horvitz's work and our work is that LookOut does not
address the possibility of not receiving a (timely) response. Thus, complex strategies are
not required. In the typical case for LookOut, the agent has three options: to take some
action, not to take the action, or to engage in dialog. The central factor inuencing the
decision is whether the user has a particular goal that the action would aid, i.e., if the user
has the goal, then the action is useful, but if he/she does not have the goal, the action is
disruptive. Choosing to act or not to act corresponds to pursuing strategy A.7 Choosing
to seek user input corresponds to strategy U . Figure 17(a) shows a graph of the different
options plotted against the probability the user has the goal (corresponds to Figure 6 in
Horvitz (1999)). The agent's expected decision quality, EQdA (t) is derived from Equation
2 in Horvitz (1999). (In other words, Horvitz's model performs more detailed calculations
of expected decision quality.) Our model then predicts the same selection of strategies as
Horvitz does, i.e., choosing strategy A when EQdA (t) is low, U otherwise (assuming that
only those two strategies are available). However, our model further predicts something
that Horvitz did not consider, i.e., that if the rate at which wait costs accrue becomes
non-negligible then the choice is not as simple. Figure 17(b) shows how the EU of the two
strategies changes as the rate of wait costs accruing is increased. The fact that the optimal
strategy varies with wait cost suggests that Horvitz's approach would not immediately be
appropriate for a domain where wait costs were non-negligible, e.g., it would need to be
modified in many multi-agent settings.

6.2 Other Approaches to AA
Several different approaches have been taken to the core problem of whether and when to
transfer decision-making control. For example, Hexmoor examines how much time the agent
has to do AA reasoning (Hexmoor, 2000). Similarly, in the Dynamic Adaptive Autonomy
framework, a group of agents allocates votes amongst themselves, hence defining the amount
of inuence each agent has over a decision and thus, by their definition, the autonomy of
the agent with respect to the decision (Barber, Martin, & Mckay, 2000b). For the related
application of meeting scheduling Cesta, Collia, and D'Aloisi (1998) have taken the approach
of providing powerful tools for users to constrain and monitor the behavior of their proxy
agents, but the agents do not explicitly reason about relinquishing control to the user.
While at least some of this work is done in a multiagent context, the possibility of multiple
transfers of control is not considered.
Complementing our work, other researchers have focused on issues of architectures for
AA. For instance, an AA interface to the 3T architecture (Bonasso, Firby, Gat, Kortenkamp,
7. We consider choosing not to act an autonomous decision, hence categorize it in the same way as autonomous action

214

fiTowards Adjustable Autonomy for the Real World

Horvitzs EU Calculations with Wait Cost
1

EU

0
-1
-2
0

0.2
0.4
0.6
0.8
Probability User has Goal

1

(a)

Horvitzs EU Calculations with Wait Cost
0.4

EU

0.2
0
-0.2
-0.4
-0.6
0

0.05 0.1 0.15 0.2 0.25 0.3

(b)

w

Figure 17: EU of different agent options. The solid (darkest) line shows the EU taking
an autonomous action, the dashed (medium dark) line shows the EU of autonomously deciding not to act and the dotted line shows the EU of transferring
control to the user. (a) Plotted against the probability of user having goal, no
wait cost. (b) plotted against wait cost, fixed probability of user having goal.
Miller, & Slack, 1997) has been implemented to solve human-machine interaction problems
experienced in a number of NASA projects (Brann, Thurman, & Mitchell, 1996). The
experiences showed that interaction with the system was required all the way from the
deliberative layer through to detailed control of actuators. The AA controls at all layers
are encapsulated in what is referred to as the 3T's fourth layer { the interaction layer
215

fiScerri, Pynadath & Tambe
(Schreckenghost, 1999). A similar area where AA technology is required is for safety-critical
intelligent software, such as for controlling nuclear power plants and oil refineries (Musliner
& Krebsbach, 1999). That work has resulted in a system called AEGIS (Abnormal Event
Guidance and Information System) that combines human and agent capabilities for rapid
reaction to emergencies in a petro-chemical refining plant. AEGIS features a shared task
representation that both the users and the intelligent system can work with (Goldman,
Guerlain, Miller, & Musliner, 1997). A key hypothesis of the work is that the model needs
to have multiple levels of abstraction so that the user can interact at the level they see fit.
Interesting work by Fong, Thorpe, and Baur (2002) has extended the idea of tele-operated
robotics by re-defining the relationship between the robot and user as a collaborative one,
rather than the traditional master-slave configuration. In particular, the robot treats the
human as a resource that can perform perceptual or cognitive functions that the robot
determines it cannot adequately perform. However, as yet the work has not looked at the
possibility that the user is not available to provide input when required, which would require
the robot perform more complex transfer-of-control reasoning.
While most previous work in AA has ignored complex strategies for AA, there is work
in other research fields that is potentially relevant. For example, the research issues addressed by fields such as mixed-initiative decision-making (Collins, Bilot, Gini, & Mobasher,
2000b), anytime algorithms (Zilberstein, 1996), multi-processor scheduling (Stankovic, Ramamritham, & Cheng, 1985), meta-reasoning (Russell & Wefald, 1989), game theory (Fudenberg & Tirole, 1991), and contingency plans (Draper, Hanks, & Weld, 1994; Peot &
Smith, 1992) all have, at least superficial, similarities with the AA problem. However, it
turns out that the core assumptions and focus of these other research areas are different
enough that the algorithms developed in these related fields are not directly applicable to
the AA problem.
In mixed-initiative decision making a human user is assumed to be continually available
(Collins et al., 2000b; Ferguson & Allen, 1998), negating any need for reasoning about the
likelihood of response. Furthermore, there is often little or no time pressure or coordination
constraints. Thus, while the basic problem of transferring control between a human and
agent is common to both mixed-initiative decision making and AA, the assumptions are
quite different leading to distinct solutions. Likewise, other related research fields make
distinctly different assumptions which lead to distinctly different solutions. For instance,
contingency planning (Draper et al., 1994; Peot & Smith, 1992) deals with the problem of
creating plans to deal with critical developments in the environment. Strategies are related
to contingency planning in that they are plans to deal with the specific contingency of an
entity not making a decision in a manner that maintains coordination. However, in contingency planning, the key diculty is in creating the plans. In contrast, in AA, creating
strategies is straightforward and the key diculty is choosing between those strategies. Our
contribution is in recognizing the need for strategies in addressing the AA problem, instantiating such strategies via MDPs, and the development of a general, domain-independent
reward function that leads to an MDP choosing the optimal strategy for a particular situation.
Similarly, another related research area is meta-reasoning (Russell & Wefald, 1989).
Meta-reasoning work looks at online reasoning about computation. A type of meta-reasoning,
most closely related to AA, chooses between sequences of computations with different ex216

fiTowards Adjustable Autonomy for the Real World
pected quality and running time, subject to the constraint that choosing the highest-quality
sequence of computations is not possible (because it takes too long) (Russell & Wefald,
1989). The idea is to treat computations as actions and \meta-reason" about the EU of
doing certain combinations of computation and (base-level) actions. The output of metareasoning is a sequence of computations that are executed in sequence. AA parallels metareasoning if we consider reasoning about transferring control to entities as reasoning about
selecting computations, i.e., we think of entities as computations. However, in AA, the
aim is to have one entity make a high-quality decision, while in meta-reasoning, the aim is
for a sequence of computations to have some high quality. Moreover, the meta-reasoning
assumption that computations are guaranteed to return a timely result if executed, does
not apply in AA. Finally, meta-reasoning looks for a sequence of computations that use a
fixed amount of time, while AA reasons about trading off extra time for a better decision
(possibly buying time with a D action). Thus, algorithms developed for meta-reasoning are
not applicable to AA.
Another research area with conceptual similarity to AA is the field of anytime algorithms (Zilberstein, 1996). An anytime algorithm quickly finds an initial solution and then
incrementally tries to improve the solution until stopped. The AA problem is similar when
we assume that the agent itself can make an immediate decision, because the problem then
has the property that a solution is always available (an important property of an anytime
algorithm). However, this will not be the case in general, i.e., the agent will not always have
an answer. Furthermore, anytime algorithms do not generally need to deal with multiple,
distributed entities, nor do they have the opportunity to change coordination (i.e., using a
D action).
Multi-processor scheduling looks at assigning tasks to nodes in order to meet certain
time constraints (Stankovic et al., 1985). If entities are thought of as \nodes", then AA
is also about assigning tasks to nodes. In multiprocessor scheduling, the quality of the
computation performed on each of the nodes is usually assumed to be equal, i.e., the nodes
are homogeneous. Thus, reasoning that trades off quality and time is not required, as it is in
AA. Moreover, deadlines are externally imposed for multi-processor scheduling algorithms,
rather than being exibly reasoned about as in AA. Multi-processor scheduling algorithms
can sometimes deal with a node rejecting a task because it cannot fulfill the time constraints
or network failures. However, while the AA problem focuses on failure to get a response
as a central issue and load balancing as an auxiliary issue, multi-processor scheduling has
the opposite focus. The difference in focus leads to algorithms being developed in the
multiprocessor scheduling community that are not well suited to AA (and vice versa).
7.

Conclusions

Adjustable autonomy is critical to the success of real-world agent systems because it allows
an agent to leverage the skills, resources and decision-making abilities of other entities,
both human and agent. Previous work has addressed AA in the context of single-agent
and single-human scenarios, but those solutions do not scale to increasingly complex multiagent systems. In particular, previous work used rigid, one-shot transfers of control that
did not consider team costs and, more importantly, did not consider the possibility of costly

217

fiScerri, Pynadath & Tambe
miscoordination between team members. Indeed, when we applied a rigid transfer-of-control
approach to a multi-agent context, it failed dramatically.
This article makes three key contributions to enable the application of AA in more
complex multiagent domains. First, the article introduces the notion of a transfer-of-control
strategy. A transfer-of-control strategy consists of a conditional sequence of two types
of actions: (i) actions to transfer decision-making control and (ii) actions to change an
agent's pre-specified coordination constraints with team members, aimed at minimizing
miscoordination costs. Such strategies allow agents to plan sequences of transfer-of-control
actions. Thus, a strategy allows the agent to transfer control to entities best able to make
decisions, buy more time for decisions to be made and still avoid miscoordination | even
if the entity to which control is transferred fails to make the decision. Additionally, we
introduced the idea of changing coordination constraints as a mechanism for giving the
agent more opportunity to provide high-quality decisions, and we showed that such changes
can, in some cases, be an effective way of increasing the team's expected utility.
The second contribution of this article is a mathematical model of AA strategies that
allows us to calculate the expected utility of such strategies. The model shows that while
complex strategies are indeed better than single-shot strategies in some situations, they are
not always superior. In fact, our analysis showed that no particular strategy dominates
over the whole space of AA decisions; instead, different strategies are optimal in different
situations.
The third contribution of this article is the operationalization of the notion of transferof-control strategies via Markov Decision Processes and a general reward function that
leads the MDP to find optimal strategies in a multiagent context. The general, domainindependent reward function should allow our approach to potentially be applied to other
multi-agent domains. We implemented, applied, and tested our MDP approach to AA reasoning in a real-world application supporting researchers in their daily activities. Daily use
showed the MDP approach to be effective at balancing the need to avoid risky autonomous
decisions and the potential for costly miscoordination. Furthermore, detailed experiments
showed that the policies produced by the MDPs have desirable properties, such as transferring control to the user less often when the probability of getting a timely response is low.
Finally, practical experience with the system revealed that users require the ability to manipulate the AA reasoning of the agents. To this end, we introduced a constraint language
that allows the user to limit the range of behavior the MDP can exhibit. We presented an
algorithm for processing such constraints, and we showed it to have the desirable property
of reducing the time it takes to find optimal policies.
8.

Future Work

The model of AA presented in this article is suciently rich to model a wide variety of
interesting applications. However, there are some key factors that are not modeled in the
current formulation that are required for some domains. One key issue is to allow an agent
to factor the AA reasoning of other agents into its own AA reasoning. For instance, in
the Elves domain, if one agent is likely to decide to delay a meeting, another agent may
wait until that decision and avoid asking its user. Conversely, if an agent about to take
back control of a decision knows another agent is going to continue waiting for user input,
218

fiTowards Adjustable Autonomy for the Real World
it might also continue to wait for input. Such interactions will substantially increase the
complexity of the reasoning an agent needs to perform. In this article, we have assumed
that the agent is finding a transfer-of-control strategy for a single, isolated decision. In
general, there will be many decisions to be made at once and the agent will not be able to
ignore the interactions between those decisions. For example, transferring control of many
decisions to a user, reduces the probability of getting a prompt response to any of them.
Reasoning about these interactions will add further complexity to the required reasoning of
the agent.
Another focus of future work will be generalizing the AA decision making to allow other
types of constraints | not just coordination constraints | to be taken into account. This
would in turn require generalization of the concept of a D action to include other types
of stop-gap actions and may lead to different types of strategies an agent could pursue.
Additionally, transfer-of-control actions could be generalized to allow parts of a decision
to be transferred, e.g., to allow input to be received from a user without transferring total
control to him/her, or allow actions that could be performed collaboratively. Similarly, if
actions were reversible, the agent could make the decision but allow the user to reverse
it. We hope that such generalizations would improve the applicability of our adjustable
autonomy research in more complex domains.
Acknowledgments

This research was supported by DARPA award no. F30602-98-2-0108. The effort is being
managed by Air Force Research Labs/Rome site. This article unifies, generalizes, and significantly extends approaches described in our previous conference papers (Scerri et al., 2001;
Scerri, Pynadath, & Tambe, 2002; Pynadath & Tambe, 2001). We thank our colleagues,
especially, Craig Knoblock, Yolanda Gil, Hans Chalupsky and Tom Russ for collaborating
on the Electric Elves project. We would also like to thank the JAIR reviewers for their
useful comments.

219

fiScerri, Pynadath & Tambe
Appendix A: An Example Instantiation of the Model

In this Appendix, we present a detailed look at one possible instantiation of the AA model.
We use that instantiation to calculate the EU of commonly used strategies and show how
that EU varies with parameters such as the rate of wait cost accrual and the time at which
transfers of control are performed. In this instantiation, the agent, A, has only one entity to
call on for a decision (i.e., the user U ), hence E = fA; U g. For W (t), we use the following
function:
(

!t t  
W (t) = !! exp
exp! otherwise

(15)

The exponential wait cost function reects the idea that a big delay is much worse than
a small one. A polynomial or similar function could have also been used but an exponential
was used since it makes the mathematics cleaner. For the probability of response we use:
P>(t) =  exp t . A Markovian response probability reects an entity that is just as likely
to respond at the next point in time as they were at the previous point. For users moving around a dynamic environment, this turns out to be a reasonable approximation. The
entities' decision-making quality is constant over time, in particular, EQdA (t) = ff and for
EQdU (t) = fi . Assuming constant decision-making quality will not always be accurate in a
dynamic environment since information available to an entity may change (hence inuencing
their ability to make the decision) however, for decisions involving static facts or preferences
decision-making quality will be relatively constant. The functions are a coarse approximation of a range of interesting applications, including the E-Elves. Table 7 shows the resulting
instantiated equations for the simple strategies (For convenience we let  =  !). Figures
18(a) and (b) show graphically how the EU of the eA strategy varies along different axes (w
is the parameter to the wait cost function, higher w means faster accruing wait costs and
p is the parameter to the response probability function, higher p means faster response).
Notice how the EU depends on the transfer time (T) as much as it does on fi (the user's
decision quality). Figure 18(d) shows the value of a D (as discussed earlier).
Figure 18(c) compares the EU of the e DeA and e strategies. The more complex the
transfer-of-control strategy (i.e., the more transfers of control it makes), the atter the
EU graph when plotted against wait cost (w) and response probability (p) parameters. In
particular, the fall-off when the wait costs are high and the probability of response low is
not so dramatic for the more complex strategy.

Appendix B: Constraint Propagation Algorithm and its Correctness

In Section 4.4, we examined the need for user-specified constraints in conjunction with
our MDP-based approach to strategies. We must thus extend the standard MDP policy
evaluation algorithms to support the evaluation of strategies while accounting for both the
standard quantitative reward function and these new qualitative constraints. This appendix
provides the novel algorithm that we developed to evaluate strategies while accounting for
220

fiTowards Adjustable Autonomy for the Real World

5
4.5
4
3.5
3
0

20
15
10
5
0
-5

1.5
0.1

w 0.2

0.3

0.5

1 p

0

10

(a)

T20

30

40

(b)
Value
0.16
0.12
0.08
0.04
0
-0.04

5
0
-5
0.1

w 0.2

0.3

4

20
16
12
beta
8

0.4

1.2
0.8p

0.1 0.2
w0.3 0.4 0.5

(c)

1
0.75
0.5 p
0.25

(d)

Figure 18: Equation 17, i.e., strategy eA plotted against (a) ! (i.e., w, the rate at which
wait costs accrue) and  (i.e., p the likelihood of response) and (b) T (transfer
time)and beta (the user's decision quality). (c) Comparing strategies e DeA and
e (dotted line is e ). (d) The value of a D.

221

fiScerri, Pynadath & Tambe


EUed t = exp  !(

d t = ! exp
EUeA

T ( 

1) + exp



1)

!
+fi


T (ff

fi)

(16)

!
+fi


(17)

EUedDeAt =
(18)
!
D
value
! (exp  1) + fi (1 exp  ) + ! exp
(exp T  exp  ) +


(Dcost fi )(exp T exp  ) + ! exp! (exp !Dvalue 1)(exp  exp T )
exp T (Dcost ff + !(exp! exp!( Dvalue ) + exp!(T Dvalue ) ))
Table 7: Instantiated AA EU equations for simple transfer of control strategies.
both. We also present a detailed proof that our algorithm's output is the correct strategy
(i.e., the strategy with the highest expected utility, subject to the user-specified constraints).
In the standard MDP value iteration algorithm, the value of a strategy in a particular
state is a single number, an expected utility U . With the addition of our two types of
constraints, this value is now a tuple hF; N; U i. F represents a strategy's ability to satisfy
the forbidding constraints; therefore, it is a boolean indicating whether the state is forbidden
or not. N represents a strategy's ability to satisfy the necessary constraints; therefore, it
is the set of requiring constraints that will be satisfied. As in traditional value iteration,
U is the expected reward. For instance, if the value of a state, V (s) = htrue; fcrs g; 0:3i,
then executing the policy from state s will achieve an expected value of 0.3 and will satisfy
required-state constraint crs . However, it is not guaranteed to satisfy any other requiredstate, nor any required-action, constraints. In addition, s is forbidden, so there is a nonzero
probability of violating a forbidden-action or forbidden-state constraint. We do not record
which forbidding constraints the policy violates, since violating any one of them is equally
bad. We do have to record which requiring constraints the policy satisfies, since satisfying
all such constraints is preferable to satisfying only some of them. Therefore, the size of the
value function grows linearly with the number of requiring constraints, but is independent
of the number of forbidding constraints.
Following the form of standard value iteration, we initialize the value function over
states by considering the immediate value of the strategy in the given state, without any
lookahead. More precisely:

V 0 (s)

*

_

c2Cfs

+

c(s); fc 2 Crs jc(s)g ; RS (s)

(19)

Thus, the state s is forbidden if any forbidden-state constraints immediately apply, and
it satisfies those required-state constraints that immediately apply. As in standard value
iteration, the expected utility is the value of the reward function in the state.
222

fiTowards Adjustable Autonomy for the Real World
In value iteration, we must define an updated value function V t+1 as a refinement
of the previous iteration's value function, V t . States become forbidden in V t+1 if they
violate any constraints directly or if any of their successors are forbidden according to V t .
States satisfy requirements if they satisfy them directly or if all of their successors satisfy
the requirement. To simplify the following expressions, we define S 0 to be the set of all
successors: fs0 2 S jMssa 0 > 0g. The following expression provides the precise definition of
this iterative step:
*

_
_
_
max
c(s) _
c(s; a) _
F 0;
a2A c2C
t
0
0
0
c2Cfa V (s )=hF ;N ;U 0 i;s0 2S 0
fs
\
fc 2 Crsjc(s)g [ fc 2 Cra jc(s; a)g [ N 0;
V t (s0 )=hF 0 ;N 0 ;U 0 i;s0 2S 0
+
X
RS (s) + R(s; a) + Mssa 0 U 0
(20)
V t (s0 )=hF 0 ;N 0 ;U 0 i;s0 2S 0
Just as in standard value iteration, this iterative step specifies a maximization over all possible choices of action. However, with our two additional components to represent the value
of the strategy with respect to the constraints, we no longer have an obvious comparison
function to use when evaluating candidate actions. Therefore, we perform the maximization
using the following preference ordering, where x  y means that y is preferable to x:
ht; N; U i  

f; N 0; U 0ff ff
hF; N; U i  
F; N 0  N; Uff0
hF; N; U i  F; N; U 0 > U

V t+1 (s)

In other words, satisfying a forbidden constraint takes highest priority, satisfying more
requiring constraints is second, and increasing expected value is last. We define the optimal
action, P (s), as the action, a, for which the final V (s) expression above is maximized.
Despite the various set operations in Equation 20, the time complexity of this iteration
step exceeds that of standard value iteration by only a linear factor, namely the number
of constraints, jCfs j + jCfa j + jCrsj + jCra j. The eciency derives from the fact that the
constraints are satisfied/violated independently of each other. The determination of whether
a single constraint is satisfied/violated requires no more time than that of standard value
iteration, hence the overall linear increase in time complexity.
Because expected value has the lowest priority, we can separate the iterative step of
Equation 20 into two phases: constraint propagation and value iteration. During the
constraint-propagation phase, we compute only the first two components of our value function, hF; N; i. The value-iteration phase computes the third component, h; ; U i, as in
standard value iteration. However, we can ignore any state/action pairs that, according
to the results of constraint propagation, violate a forbidding constraint (ht; N; i) or requiring constraint (hf; N  Crs [ Cra ; i). Because of the component-wise independence of
Equation 20, the two-phase algorithm computes an identical value function as the original,
single-phase version (over state/action pairs that satisfy all constraints).
In the rest of this Appendix we provide a proof of the correctness of the modified value
iteration policy. Given a policy, P , constructed according to the above algorithm, we must
223

fiScerri, Pynadath & Tambe
show that an agent following P will obey the constraints specified by the user. If the agent
begins in some state, s 2 S , we must prove that it will satisfy all of its constraints if and only
if V (s) = hf; Cra [ Crs ; U i. We prove the results for forbidding and requiring constraints
separately.

Theorem 1 An agent following policy, P , with value function, V , generated as in Section 4.4, from any state s 2 S will violate a forbidding constraint with probability zero if
and only if V (s) = hf; N; U i (for some U and N ).
Proof: We prove the theorem by induction over subspaces of the states, classified by

how \close" they are to violating a forbidding constraint. More precisely, we partition the
state space, S , into subsets, Sk , defined to contain all states that can violate a forbidding
constraint after a minimum of k state transitions. In other words, S0 contains those states
that violate a forbidding constraint directly; S1 contains those states that do not violate
any forbidding constraints themselves, but have a successor state (following the transition
probability function, P ) that does (i.e., a successor state in S0 ); S2 contains those states
that do not violate any forbidding constraints, nor have any successors that do, but who
have at least one successor state that has a successor state that does (i.e., a successor state
in S1 ); etc. There are at most jS j nonempty subsets in this mutually exclusive sequence. To
make this partition exhaustive, the special subset, S1 , contains all states from which the
agent will never violate a forbidding constraint by following P . We first show, by induction
over k, that 8s 2 Sk (0  k  jS j), V (s) = ht; N; U i, as required by the theorem.
Basis step (S0): By definition, the agent will violate a forbidding constraint in s 2 S0 .
Therefore, either 9c 2 Cfs such that c(s) = t or 9c 2 Cfa such that c(s; P (s)) = t, so we
know, from Equation 20, V (s) = ht; N; U i.
Inductive step (Sk ; 1  k  jS j): Assume, as the induction hypothesis, that 8s0 2
Sk 1 , V (s0 ) = ht; N 0 ; U 0 i. By the definition of Sk , each state, s 2 Sk , has at least one
successor state, s0 2 Sk 1 . Then, according to Equation 20, V (s) = ht; N; U i, because the
disjunction over S 0 must include s0 , for which F 0 = t.
Therefore, by induction, we know that for all s 2 Sk (0  k  jS j), V (s) = ht; N; U i.
We now show that 8s 2 S1 , V (s) = hf; N; U i. We prove, by induction over t, that, for any
state, s 2 S1, V t (s) = hf; N; U i.
Basis step (V 0 ): By definition, if s 2 
S1 , thereff cannot exist any c 2 Cfs such that
c(s) = t. Then, from Equation 19, V 0 (s) = f; N 0 ; U 0 .
Inductive step (V t ; t > 0): Assume, as the
 inductive
hypothesis, that, for any s0 2 S1 ,
ff
V t 1 (s0 ) = hf; N 0 ; U 0 i. We know that V t (s) = f; N t ; U t if and only if all three disjunctions
in Equation 20 are false. The first is false, as described in the basis step. The second term
is similarly false, since, by the definition of S1, there cannot exist any c 2 Cfa such that
c(s; P (s)) = t. In evaluating the third term, we first note that S 0  S1. In other words,
all of the successor states of s are also in S1 (if successor s0 2 Sk for some finite k, then
s 2 Sk+1). Since all of the successors are in S1 , we know, by the inductive hypothesis, that
the disjunction over V t 1 in all these successors
is fffalse. Therefore, all three disjunctive


terms in Equation 20 are false, so V t (s) = f; N t ; U t .
Therefore, by induction, we know that for all s 2 S1 , V (s) = hf; N; U i. By the definition
of the state partition, these two results prove the theorem as required. 2
224

fiTowards Adjustable Autonomy for the Real World
Theorem 2 An agent following policy, P , with value function, V , generated as described
in Section 4.4, from any state s 2 S will satisfy each and every requiring constraint with
probability one if and only if V (s) = hF; Cra [ Crs ; U i (for some U and F ).
Proof Sketch: The proof parallels that of Theorem 1, but with a state partition, Sk ,
where k corresponds to the maximum number of transitions before satisfying a requiring
constraint. However, here, states in S1 are those that violate the constraint, rather than

satisfy it. Some cycles in the state space can prevent a guarantee of satisfying a requiring
constraint within any fixed number of transitions, although the probability of satisfaction
in the limit may be 1. In our current constraint semantics, we have decided that such a
situation fails to satisfy the constraint, and our algorithm behaves accordingly. Such cycles
have no effect on the handling of forbidding constraints, where, as we saw for Theorem 1,
we need consider only the minimum -length trajectory. 2
The proofs of the two theorems operate independently, so the policy-specified action will
satisfy all constraints, if such an action exists. The precedence of forbidding constraints
over requiring ones has no effect on the optimal action in such states. However, if there
are conicting forbidding and requiring constraints in a state, then the preference ordering
causes the agent to choose a policy that satisfies the forbidding constraint and violates
a requiring constraint. The agent can make the opposite choice if we simply change the
preference ordering from Section 4.4. Regardless of the choice, from Theorems 1 and 2,
the agent can use the value function, V , to identify the existence of any such violation and
notify the user of the violation and possible constraint conict.
References

Barber, K., Goel, A., & Martin, C. (2000a). Dynamic adaptive autonomy in multi-agent
systems. Journal of Experimental and Theoretical Artificial Intelligence, 12 (2), 129{
148.
Barber, K. S., Martin, C., & Mckay, R. (2000b). A communication protocol supporting
dynamic autonomy agreements. In Proceedings of PRICAI 2000 Workshop on Teams
with Adjustable Autonomy, pp. 1{10, Melbourne, Australia.
Bonasso, R., Firby, R., Gat, E., Kortenkamp, D., Miller, D., & Slack, M. (1997). Experiences with an architecture for intelligent reactive agents. Journal of Experimental
and Theorectical Artificial Intelligence, 9 (1), 237{256.
Brann, D., Thurman, D., & Mitchell, C. (1996). Human interaction with lights-out automation: A field study. In Proceedings of the 1996 Symposium on Human Interaction and
Complex Systems, pp. 276{283, Dayton, USA.
Cesta, A., Collia, M., & D'Aloisi, D. (1998). Tailorable interactive agents for scheduling
meetings. In Lecture Notes in AI, Proceedings of AIMSA'98, No. 1480, pp. 153{166.
Springer Verlag.
Chalupsky, H., Gil, Y., Knoblock, C., Lerman, K., Oh, J., Pynadath, D., Russ, T., & Tambe,
M. (2001). Electric Elves: Applying agent technology to support human organizations.
In International Conference on Innovative Applications of AI, pp. 51{58.
225

fiScerri, Pynadath & Tambe
Collins, J., Bilot, C., Gini, M., & Mobasher, B. (2000a). Mixed-initiative decision-support
in agent-based automated contracting. In Proceedings of the International Conference
on Autonomous Agents (Agents'2000).
Collins, J., Bilot, C., Gini, M., & Mobasher, B. (2000b). Mixed-initiative decision support
in agent-based automated contracting. In Proceedings of the International Conference
on Autonomous Agents (Agents'2000), pp. 247{254.
Dorais, G., Bonasso, R., Kortenkamp, D., Pell, B., & Schreckenghost, D. (1998). Adjustable
autonomy for human-centered autonomous systems on mars. In Proceedings of the
First International Conference of the Mars Society, pp. 397{420.
Draper, D., Hanks, S., & Weld, D. (1994). Probabilistic planning with information gathering
and contingent execution. In Hammond, K. (Ed.), Proc. Second International Conference on Artificial Intelligence Planning Systems, pp. 31{37, University of Chicago,
Illinois. AAAI Press.
Ferguson, G., Allen, J., & Miller, B. (1996). TRAINS-95 : Towards a mixed-initiative
planning assistant. In Proceedings of the Third Conference on Artificial Intelligence
Planning Systems, pp. 70{77.
Ferguson, G., & Allen, J. (1998). TRIPS : An intelligent integrated problem-solving assistant. In Proceedings of Fifteenth National Conference on Artificial Intelligence(AAAI98), pp. 567{573, Madison, WI, USA.
Fong, T., Thorpe, C., & Baur, C. (2002). Robot as partner: Vehicle teleoperation with collaborative control. In Workshop on Multi-Robot Systems, Naval Research Laboratory,
Washington, D.C.
Fudenberg, D., & Tirole, J. (1991). Game Theory. The MIT Press, Cambridge, Massachusetts.
Goldman, R., Guerlain, S., Miller, C., & Musliner, D. (1997). Integrated task representation for indirect interaction. In Working Notes of the AAAI Spring Symposium on
Computational Models for Mixed-Initiative Interaction.
Goodrich, M., Olsen, D., Crandall, J., & Palmer, T. (2001). Experiments in adjustable
autonomy. In Hexmoor, H., Castelfranchi, C., Falcone, R., & Cox, M. (Eds.), Proceedings of IJCAI Workshop on Autonomy, Delegation and Control: Interacting with
Intelligent Agents.
Gunderson, J., & Martin, W. (1999). Effects of uncertainty on variable autonomy in maintainance robots. In Agents'99 Workshop on Autonomy Control Software, pp. 26{34.
Hexmoor, H. (2000). A cognitive model of situated autonomy. In Proceedings of PRICAI2000, Workshop on Teams with Adjustable Autonomy, pp. 11{20, Melbourne, Australia.
Hexmoor, H., & Kortenkamp, D. (2000). Introduction to autonomy control software. Journal
of Experiemental and Theoretical Artificial Intelligence, 12 (2), 123{128.
Horvitz, E. (1999). Principles of mixed-initiative user interfaces. In Proceedings of ACM
SIGCHI Conference on Human Factors in Computing Systems (CHI'99), pp. 159{166,
Pittsburgh, PA.
226

fiTowards Adjustable Autonomy for the Real World
Horvitz, E., Jacobs, A., & Hovel, D. (1999). Attention-sensitive alerting. In Proceedings of
Conference on Uncertainty and Artificial Intelligence (UAI'99), pp. 305{313, Stockholm, Sweden.
Lesser, V., Atighetchi, M., Benyo, B., Horling, B., Raja, A., Vincent, R., Wagner, T., Xuan,
P., & Zhang, S. (1999). The UMASS intelligent home project. In Proceedings of the
Third Annual Conference on Autonomous Agents, pp. 291{298, Seattle, USA.
Mitchell, T., Caruana, R., Freitag, D., McDermott, J., & Zabowski, D. (1994). Experience
with a learning personal assistant. Communications of the ACM, 37 (7), 81{91.
Mulsiner, D., & Pell, B. (1999). Call for papers: AAAI spring symposium on adjustable
autonomy. www.aaai.org.
Musliner, D., & Krebsbach, K. (1999). Adjustable autonomy in procedural control for
refineries. In AAAI Spring Symposium on Agents with Adjustable Autonomy, pp.
81{87, Stanford, California.
Peot, M. A., & Smith, D. E. (1992). Conditional nonlinear planning. In Hendler, J. (Ed.),
Proc. First International Conference on Artificial Intelligence Planning Systems, pp.
189{197, College Park, Maryland. Morgan Kaufmann.
Puterman, M. L. (1994). Markov Decision Processes. John Wiley & Sons.
Pynadath, D., Tambe, M., Arens, Y., Chalupsky, H., Gil, Y., Knoblock, C., Lee, H., Lerman,
K., Oh, J., Kamachandran, S., Rosenbloom, P., & Russ, T. (2000). Electric-elves:
Immersing and agent organization in a human organization. In Proceedings of the
AAAI Fall Symposium on Socially Intelligent Agents { The Human in the Loop.
Pynadath, D., & Tambe, M. (2001). Revisiting Asimov's first law: A response to the call to
arms. In Intelligent Agents VIII Proceedings of the International workshop on Agents,
Theories, Architectures and Languages (ATAL'01).
Quinlan, J. R. (1993). C4.5: Programs for machine learning. Morgan Kaufmann, San
Mateo, CA.
Russell, S. J., & Wefald, E. (1989). Principles of metareasoning. In Brachman, R. J.,
Levesque, H. J., & Reiter, R. (Eds.), KR'89: Principles of Knowledge Representation
and Reasoning, pp. 400{411. Morgan Kaufmann, San Mateo, California.
Scerri, P., Pynadath, D., & Tambe, M. (2001). Adjustable autonomy in real-world multiagent environments. In Proceedings of the Fifth International Conference on Autonomous Agents (Agents'01), pp. 300{307.
Scerri, P., Pynadath, D., & Tambe, M. (2002). Why the elf acted autonomously: Towards
a theory of adjustable autonomy. In First International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS'02).
Schreckenghost, D. (1999). Human interaction with control software supporting adjustable
autonomy. In Musliner, D., & Pell, B. (Eds.), Agents with Adjustable Autonomy,
AAAI 1999 Spring Symposium Series, pp. 116{119.
Stankovic, J., Ramamritham, K., & Cheng, S. (1985). Evaluation of a exible task scheduling algorithm for distributed hard real-time system. IEEE Transactions on Computers, 34 (12), 1130{1143.
227

fiScerri, Pynadath & Tambe
Tambe, M. (1997). Towards exible teamwork. Journal of Artificial Intelligence Research
(JAIR), 7, 83{124.
Tambe, M., Pynadath, D. V., Chauvat, N., Das, A., & Kaminka, G. A. (2000). Adaptive
agent integration architectures for heterogeneous team members. In Proceedings of
the International Conference on MultiAgent Systems, pp. 301{308.
Zilberstein, S. (1996). Using anytime algorithms in intelligent systems. AI Magazine, 17 (3),
73{83.

228

fiJournal of Artificial Intelligence Research 17 (2002) 363378

Submitted 5/02; published 11/02

Competitive Safety Analysis: Robust Decision-Making in
Multi-Agent Systems
moshet@ie.technion.ac.il

Moshe Tennenholtz
Faculty of Industrial Engineering and Management
Technion  Israel Institute of Technology
Haifa 32000, Israel

Abstract
Much work in AI deals with the selection of proper actions in a given (known or unknown) environment. However, the way to select a proper action when facing other agents
is quite unclear. Most work in AI adopts classical game-theoretic equilibrium analysis to
predict agent behavior in such settings. This approach however does not provide us with
any guarantee for the agent. In this paper we introduce competitive safety analysis. This
approach bridges the gap between the desired normative AI approach, where a strategy
should be selected in order to guarantee a desired payoff, and equilibrium analysis. We show
that a safety level strategy is able to guarantee the value obtained in a Nash equilibrium,
in several classical computer science settings. Then, we discuss the concept of competitive
safety strategies, and illustrate its use in a decentralized load balancing setting, typical to
network problems. In particular, we show that when we have many agents, it is possible
to guarantee an expected payoff which is a factor of 8/9 of the payoff obtained in a Nash
equilibrium. Our discussion of competitive safety analysis for decentralized load balancing
is further developed to deal with many communication links and arbitrary speeds. Finally,
we discuss the extension of the above concepts to Bayesian games, and illustrate their use
in a basic auctions setup.

1. Introduction
Deriving solution concepts for multi-agent encounters is a major challenge for researchers
in various disciplines. The most famous and popular solution concept in the economics
literature is the Nash equilibrium. Although Nash equilibrium and its extensions and modifications are powerful descriptive tools, and have been widely used in the AI literature
(Rosenschein & Zlotkin, 1994; Kraus, 1997; Sandholm & Lesser, 1995), their appeal from
a normative AI perspective is somewhat less satisfactory.1 We wish to equip an agent with
an action that guarantees some desired outcome, or expected utility, without relying on
other agents rationality.2 This paper shows that, surprisingly, the desire for obtaining a
guaranteed expected payoff, where this payoff is of the order of the value obtained in a
1. If we restrict ourselves to cases where there exists an equilibrium in dominant strategies, as is done in
some of the CS literature (Nisan & Ronen, 1999), then the corresponding equilibrium is appealing from
a normative perspective. However, such cases rarely exist.
2. Maximizing expected payoff when facing a set of possible environment behaviors is fundamental to AI.
In particular, it is discussed in the context of game trees, in the context of planning with incomplete
information, where we need to obtain a desired goal regardless of the initial configuration, as well as
in the context of reinforcement learning, where we wish to maximize expected payoff when the actual
model (selected from a set of possible models in adversarial way) is initially unknown. (Russell & Norvig,
1995).
c
2002
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiTennenholtz

Nash equilibrium, is achievable in various classical computer science settings. Our results
are inspired by several interesting examples for counter-intuitive behaviors obtained by following Nash equilibria and other solution concepts (Roth, 1980; Aumann, 1985). One of
the most interesting and challenging examples has been introduced by Aumann (Aumann,
1985). Aumann presented a 2-person 2-choice (2  2) game g, where the safety-level (probabilistic maximin) strategy of the game is not a Nash equilibrium of it, but it does yield
the expected payoff of a Nash equilibrium of g. This observation may have significant positive ramifications from an agents design perspective. If a safety-level strategy of an agent
guarantees an expected payoff that equals its expected payoff in a Nash equilibrium, then
it can serve as a desirable robust protocol for the agent! Given the above, we are interested
in whether an optimal safety level strategy leads to an expected payoff similar to the one
obtained in a Nash equilibrium of simple games that represent basic variants of classical
computer science problems. As we show, this is indeed the case for 2  2 games capturing
simple variants of the classical load balancing and leader election problems. A more general
question refers to more general 2  2 games. We show that if the safety-level strategy is a
(strictly) mixed one, then its expected payoff is identical to the expected payoff obtained in
a Nash equilibrium in any generic non-reducible 2  2 game. We also show that this is no
longer necessarily the case if we have a pure safety-level strategy. In addition, we consider
general 2-person set-theoretic games (which naturally extend 2  2 leader election games)
and show that if a set-theoretic game g possesses a strictly mixed strategy equilibrium then
the safety level value for a player in that game equals the expected payoff it obtains in
that equilibrium. Following this, we define the concept of C-competitive safety strategies.
Roughly speaking, a strategy will be called a C-competitive safety strategy, if it guarantees
an expected payoff that is C1 of the expected payoff obtained in a Nash equilibrium. We
show that in an extended decentralized load balancing setting a 9/8-competitive strategy
exists, when the number of players is large. We also discuss extensions of this result to more
general settings. In particular, we deal with the cases of arbitrary number of communication
lines, and arbitrary different speeds of communication. We show that a ratio of 4/3 can be
obtained when we allow arbitrary speeds in two communication lines connecting source to
target. We also consider the notion of a k-regular network, where k is the ratio between
the average communication speed and the lowest speed of communication (in a given set
of communication lines), and show that a k-competitive safety strategy exists for general
k-regular networks. Then, we discuss C-competitive strategies in the context of Bayesian
games. In particular we show the existence of an e-competitive safety strategy for a classical
first-price auctions setup.
Imagine an agent designed to deal with the communication of a user with different
targets. Selecting routes for messages in a multi-agent system is a non-trivial task. The
efficiency of the agent depends on the actions selected by other users (and their agents)
that try also to communicate with similar targets. In such cases, game-theoretic analysis
can identify the Nash equilibria that may emerge in that setting. However, adopting the
strategy prescribed by a Nash equilibrium may be quite dangerous for our agent. Other
agents may fail to choose strategies prescribed by that equilibrium, and as a result the
outcome of our agent can be quite poor. It would have been much better if the agent could
have guaranteed similar payoff (to the one obtained in a Nash equilibrium) without relying
on other agents behavior. In computational settings, where (machine and other) failures
364

fiCompetitive Safety Analysis

are possible, and rationality assumptions about participants behavior should be minimized,
a safety-level strategy has a special appeal, especially when it yields a value that is close to
the expected payoff obtained in a Nash equilibrium.
Previous work has been concerned with comparing the payoffs that can be obtained
by an optimal centralized (and Pareto-efficient) controller to the expected payoffs obtained
in the Nash-equilibria of the corresponding game (Koutsoupias & Papadimitriou, 1999).3
That work is in the spirit of competitive analysis, a central topic in theoretical computer
science (Borodin & El-Yaniv, 1998). Our work can be considered as suggesting a complementary approach, comparing the safety-level value to the agents expected payoff in a Nash
equilibrium.
The rest of this paper is organized as follows. In Section 2 we provide some basic
definitions and notations. In sections 3 and 4 we deal with simple variants of the load
balancing and the leader election problems. We use these as examples for showing that
safety-level strategies can be quite competitive and attractive, leading to the value of a
Nash equilibrium. This is generalized in section 5 to the context of general 2  2 games. A
discussion of another extension dealing with set-theoretic games is discussed in section 6. In
section 7 we deal with several settings of decentralized load balancing, with increasing level
of complexity. In particular we show the existence of desired competitive safety strategies
for settings with many agents and many possible routes. Section 8 illustrates the use of
competitive safety analysis in games with incomplete information.

2. Basic Definitions and Notations
A game is a tuple G = hN = {1, . . . , n}, {Si }ni=1 , {Ui }ni=1 i, where N is a set of n players,
Si is a finite set of pure strategies available to player i, and Ui : ni=1 Si  < is the payoff
function of player i. Given Si , we denote the set of probability distributions over the
elements of Si by (Si ). An element t  (Si ) is called a mixed strategy of player i. It
is called a pure strategy if it assigns probability 1 to an element of Si , and it is called a
strictly mixed strategy if it assigns a positive probability to each element in Si . A tuple
t = (t1 , . . . , tn )  ni=1 (Si ) is called a strategy profile. We denote by Ui (t) the expected
payoff of player i given the strategy profile t. A strategy profile t = (t1 , . . . , tn ) is a Nash
equilibrium if i  N , Ui (t)  Ui (t1 , t2 , . . . , ti1 , t0i , ti+1 , . . . , tn ) for every t0i  Si . The Nash
equilibrium t = (t1 , . . . , tn ) is called a pure strategy Nash equilibrium if ti is a pure strategy
for every i  N . The Nash equilibrium t = (t1 , . . . , tn ) is called a strictly mixed strategy
Nash equilibrium if for every i  N we have that ti is a strictly mixed strategy. Given a
game g and a mixed strategy of player i, t  (Si ), the safety level value obtained by i when
choosing t in the game g, denoted by val(t, i, g), is the minimal expected payoff that player
i may obtain when employing t against arbitrary strategy profiles of the other players. A
strategy t0 of player i for which val(., i, g) is maximal is called a safely-level strategy (or
a probabilistic maximin strategy) of player i. Hence, a safety-level strategy for agent i,
ssaf e  (Si ) satisfies that
ssaf e  argmaxs(Si ) min(s1 ,s2 ,...,si1 ,si+1 ,...,sn )j6=i Sj Ui (s1 , s2 , . . . , si1 , s, si+1 , . . . , sn )
3. This work has been extended in e.g. (Roughgarden, 2001; Roughgarden & Tardos, 2002).

365

fiTennenholtz

A strategy e  Si dominates a strategy f  Si if for every (s1 , s2 , . . . , si1 , si+1 , . . . , sn ) 
j6=i (Sj ) we have Ui (s1 , . . . , sj1 , e, sj+1 , . . . , sn ) 
Ui (s1 , . . . , sj1 , f, sj+1 , . . . , sn ), with a strict inequality for at least one such tuple. A
game is called non-reducible if there do not exist e, f  Si , for some i  N , such that
e dominates f . A game is called generic if for every i  N , pair of strategies e, f  Si ,
and (s1 , s2 , . . . , si1 , si+1 , . . . , sn )  j6=i Sj , we have that Ui (s1 , . . . , si1 , e, si+1 , . . . , sn ) =
Ui (s1 , . . . , si1 , f, si+1 , . . . , sn ) only if e and f coincide. In a generic game different strategies of player i, assuming a fixed strategy profile for the rest of the players, should lead
to different payoffs. This property simply says that in a fixed environment (captured by
a strategy profile of the rest of the players), different strategies of player i should lead to
somewhat different payoffs (e.g. as a result of their costs, outcomes, etc.) A game is called
a 2  2 game if n = 2 and |S1 | = |S2 | = 2.

3. Decentralized Load Balancing
In this section we consider decentralized load balancing, where two rational players need to
submit messages in a simple communication network: a network of two parallel communication lines e1 , e2 connecting nodes s and t. Each player has a message that he needs to
deliver from s to t, and he needs to decide on the route to be taken. The communication line
e1 is a faster one, and therefore the value of transmitting a single message along e1 is X > 0
while the value of transmitting a single message along e2 is X for some 0.5 <  < 1.4 Each
player needs to decide on the communication line to be used for sending its message from
s to t. If both players choose the same communication line then the value for each one of
them drops in a factor of two (a player will obtain X2 if both players choose e1 , and a player
will obtain X
2 if both players choose e2 ). In a matrix form, this game can be presented as
follows:

!
X/2, X/2
X, X
M=
X, X
X/2, X/2
Proposition 1 The optimal safety-level value for a player in the decentralized load balancing game equals its expected payoff in the strictly mixed strategy equilibrium of that game.
Proof: Consider the following equations for the probability to choose e1 in a symmetric
equilibrium, where each player selects e1 with probability p and e2 with probability 1  p.
This equation is derived from the fact that in a Nash equilibrium every strategy in the
support should lead to identical expected payoffs. Notice that by solving this equation we
will also prove the existence of a strictly mixed strategy Nash equilibrium.
p

X
X
+ (1  p)X = pX + (1  p)
2
2

Hence, p X2 + X  pX = pX +  X2  p X2 , and X   X2 = p X2 + p X2 . This implies that
p=

2
1+

4. Notice that here and later in the paper, X is a constant. The important factor is the ratio between the
payoffs.

366

fiCompetitive Safety Analysis

Notice that 0 < p < 1 as required. The safety level mixed strategy satisfies the following
equation. This equation is derived from the fact that the expected payoff of a (mixed)
safety-level strategy should be identical for any strategy of the other player.
p

X
X
+ (1  p)X = pX + (1  p)
2
2

Hence, p X2 +X pX = pX + X2 p X2 . This implies that pX p X2 +pX p X2 =
) = X
and therefore that p( X+X
2
2 . We get:
p=

X
2 ,


1+

Notice that the above Nash equilibrium is different from the safety level strategy. However,
consider the expected payoff obtained by the Nash equilibrium and by the safety level
strategy: The Nash value is:
2X
2  1
+
X
1+ 2
1+
The safety level value is:
1
 X
+
X
1+ 2
1+
We will show that these values coincide. It is enough to show that:
2  1

2
+
= 1.5
2(1 + )
1+
1+

The above however trivially holds since both sides equal 1.5 1+
2
Notice that the above proposition shows that an agent can guarantee itself an expected
payoff that equals its payoff in a Nash equilibrium of the decentralized load balancing
game. This is obtained using a strategy that differs from the agents strategies in the Nash
equilibria of that game (which do not provide that guarantee). Notice that if the players
could have used a mediator/correlation devise, and play the game repeatedly, then the
mediator could have directed them to the use of strategies leading to a payoff that is higher
than the one guaranteed by the safety-level strategy. The use of such mediator/correlation
devise, as well as the discussion of repeated games, is beyond the scope of this paper.

4. Leader Election: Decentralized Voting
In a leader election setting, the players vote about the identity of the player who will take
the lead on a particular task. A failure to obtain agreement about the leader is a bad
output, and can be modelled as leading to a 0 payoff. Assume that the players strategies
are either vote for 1 or vote for 2, denoted by a1 , a2 respectively, then Ui (aj , ak ) > 0,
where i, j, k  {1, 2}, and j = k. Notice that this setting captures various forms of leader
election, e.g. when a player prefers to be selected, when it prefers the other player to be
selected, etc. In a matrix form, this game can be presented as follows (where a, b, c, d > 0):


M=

a, b
0, 0
367

0, 0
c, d

!

fiTennenholtz

Proposition 2 The optimal safety-level value for a player in the leader election game equals
its expected payoff in the strictly mixed strategy equilibrium of that game.
Proof: In a strictly mixed Nash equilibrium we have that the probability q of choosing a1
by player 2 should satisfy:
qU1 (a1 , a1 ) = (1  q)U1 (a2 , a2 )
The above equality is implied by the fact that any pure strategy in the support of the
mixed strategy for an agent, in a Nash equilibrium, should yield the same expected payoff
(otherwise, deviation will be rational.) Hence, the above equality captures the fact that the
strategy of player 2 in equilibrium should be selected in a way that the utility for agent 1
when using either a1 or a2 will be the same.
Similarly, the probability p of choosing a1 by player 1 should satisfy
pU2 (a1 , a1 ) = (1  p)U2 (a2 , a2 )
Hence, a strictly mixed strategy Nash equilibrium exists, where q =

U1 (a2 ,a2 )
U1 (a1 ,a1 )+U1 (a2 ,a2 )

and

U2 (a2 ,a2 )
U2 (a1 ,a1 )+U2 (a2 ,a2 )

As can be seen from the above equations a strictly mixed strategy
p=
equilibrium exists. Consider now w.l.o.g player 1. The expected payoff it obtains in the
)U1 (a2 ,a2 )
above equilibrium is qU1 (a1 , a1 ) = UU11(a(a11,a,a11)+U
Player 1s safety level strategy satisfies
1 (a2 ,a2 )
0
the following, where p is the probability of choosing a1 :
p0 U1 (a1 , a1 ) = (1  p0 )U1 (a2 , a2 )
Hence, p0 =

U1 (a2 ,a2 )
U1 (a1 ,a1 )+U1 (a2 ,a2 ) Notice
)U1 (a2 ,a2 )
.
p0 U1 (a1 , a1 ) = UU11(a(a11,a,a11)+U
1 (a2 ,a2 )

that p0 = q. The safety level value will be there-

We get that the Nash equilibrium and safety level
fore:
strategies are different, but their expected payoffs for the players coincide.
2
Notice that the above proposition shows that a agent can guarantee itself an expected
payoff that equals its payoff in a Nash equilibrium of the leader election game.5 As in the
decentralized load balancing game, this is obtained using a strategy that differs from the
agents strategies in the Nash equilibria of that game (which do not provide that guarantee).

5. Safety Level in General 2  2 Games
The results presented in the previous sections refer to 2-person 2-choice variants of central problems occurring in computational contexts. Given the encouraging results in the
framework of these basic settings, we wish to consider two types of extensions:
1. Generalize the results to a broader family of simple games.
2. Generalize the results to more general CS-related settings, dealing in particular with
games with many players, as found in load-balancing settings.
5. The reader should not confuse the fact that p0 = q with similarity between safety-level and Nash equilibrium. Indeed, p0 refers to the probability of choosing a1 by player 1, while q refers to the probability
of choosing that action by player 2.

368

fiCompetitive Safety Analysis

In this section we deal with the first point. Later, and in particular in section 7, we will
deal with the second one. It is of interest to see whether our results in sections 3-4 can
be extended to other forms of 2  2 games. Notice that the load balancing and the leader
election settings can be represented as non-reducible generic 2  2 games. The same is true
with regard to the game presented by Aumann:


M=

2, 6
6, 0

4, 2
0, 4

!

Non-reducible generic games are an attractive concept. Having dominated strategies in the
game do not add to the understanding of the interaction, since these strategies can be safely
ignored. The fact a game is generic is also quite appealing: it is quite natural to assume that
a pair of actions should lead to different outcomes when we fix the rest of the environment.
We can show:
Theorem 1 Let G be a 2  2 non-reducible generic game. Assume that the optimal safety
level value of a player is obtained by a strictly mixed strategy, then this value coincides with
the expected payoff of that player in a Nash equilibrium of G.
Proof: Denote the strategies available to the players by a1 , a2 . Use the following notation: a = U1 (a1 , a1 ), b = U1 (a1 , a2 ), c = U1 (a2 , a1 ), d = U1 (a2 , a2 ), e = U2 (a1 , a1 ), f =
U2 (a1 , a2 ), g = U2 (a2 , a1 ), h = U2 (a2 , a2 )
In a matrix form, the above will be presented as:


M=

a, e
c, g

b, f
d, h

!

If a strictly mixed strategy Nash equilibrium exists then it should satisfy that:
qa + (1  q)b = qc + (1  q)d
and
pe + (1  p)g = pf + (1  p)h
where p and q are the probabilities for choosing a1 by players 1 and 2, respectively. We get
that we should have qa + b  qb = qc + d  qd, which implies that q(a  b  c + d) = d  b.
Similarly, we get that we should have pe + g  pg = pf + h  ph, which implies that
p(e  g  f + h) = h  g. Hence, in a strictly mixed strategy Nash equilibrium we should
have:
db
q=
abc+d
and
hg
p=
egf +h
Notice that since the game is generic then d 6= b. If d > b then if q is not strictly in between
0 and 1 then c > a which will contradict non-reducibility. If d < b then in if q is not strictly
in between 0 and 1 then a > c, which also contradicts non-reducibility. Similarly, since the
game is generic then h 6= g. If h > g then if p is not strictly in between 0 and 1 then f > e
369

fiTennenholtz

which will contradict non-reducibility. If h < g then in if p is not strictly in between 0 and
1 then e < f , which also contradicts non-reducibility. Given the above we get that p and q
define a strictly mixed strategy equilibrium of G. Consider now the safety level strategy of
player 1. If player 1 chooses a1 with probability p0 then it satisfies that:
p0 a + (1  p0 )c = p0 b + (1  p0 )d
This implies that we need to have p0 a+cp0 c = p0 b+dp0 d, which implies p0 (acb+d) =
d  c. Hence, we have
dc
p0 =
acb+d
and
ab
1  p0 =
acb+d
Compute now the expected payoff for player 1 in the strictly mixed Nash equilibrium, given
ac
, we have that:
that 1  q = abc+d
qa + (1  q)b =

da  cb
(d  b)a + (a  c)b
=
abc+d
abc+d

The expected payoff of the safety level strategy for player 1 will be:
p0 a + (1  p0 )c =

(d  c)a + (a  b)c
da  cb
=
abc+d
abc+d

Hence, we get that the expected payoffs of the Nash equilibrium and the safety level strategies for player 1 coincide. The computation for player 2 is similar.
2
5.1 The Case of Pure Safety-Level Strategies
The reader may wonder whether the previous result can be also proved for the case where
there are no restrictions on the structure of the safety-level strategy of the game g. In several
AI contexts, the discussion is on pure maximin strategies, where probabilistic behavior is
not considered. Of course, probabilistic maximin strategies are more powerful, and in many
cases the best safety level is obtained only by a mixed strategy and not by a pure one.
However, it will be of interest to consider the case where the safety-level strategy is a pure
one. As we now show, there exists a generic non-reducible 2  2 game g, where the optimal
safety level strategy for a player is pure, and the expected payoff for that player is lower than
the expected payoff for that player in all Nash equilibria of g. Consider a game g, where
U1 (1, 1) = 100, U1 (1, 2) = 40, U1 (2, 1) = 60, U1 (2, 2) = 50, and U2 (1, 1) = 100, U2 (1, 2) =
210, U2 (2, 1) = 200, U2 (2, 2) = 90. In a matrix form this game looks as follows:


M=

100, 100
60, 200

40, 210
50, 90

!

It is easy to check that g is generic and non-reducible. In particular, there are no dominated
strategies, and the payoffs obtained by each player for different strategy profiles are different
from one another. The game has no pure Nash equilibria. In a strictly mixed strategy
370

fiCompetitive Safety Analysis

equilibrium the probability q of choosing a1 by player 2 should satisfy 100q + 40(1  q) =
60q+50(1q), i.e. that 60q+40 = 10q+50, q = 0.2. In that equilibrium the probability that
player 1 will choose a1 is p = 0.5, and the expected payoff of player 1 is 100q+40(1q) = 52.
The safety-level strategy for player 1 is to perform a2 , guaranteeing a payoff of 50, given
that (a2 , a2 ) is a saddle point in a zero-sum game where the payoffs of player 2 are taken to
be the complement to 0 of player 1s original payoffs. Hence, the value of the safety level
strategy for player 1 is 50 < 52.
2

6. Beyond 2  2 Games
The leader election game is an instance of a more general set of games: set-theoretic games.
In a set theoretic game the sets of strategies available to the players are identical, and the
payoff of each player is uniquely determined by the set of strategies selected by each player.
For example, in a 2-person set-theoretic game we will have that U1 (s, t) = U1 (t, s), U2 (s, t) =
U2 (t, s) for every s, t  S1 = S2 . Notice that set-theoretic games are very typical to voting
contexts. In a typical voting context we care about the votes, but not about the indentity
of the voters. We can prove the following:
Proposition 3 Given a 2-person set theoretic game g with a strictly mixed strategy Nash
equilibrium, then the value of an optimal safety level strategy of a player equals its expected
payoff in that equilibrium.
Proof: Let S = S1 = S2 = {s1 , s2 , . . . , sl }. Let t = (t1 , t2 ) be a strictly mixed strategy
Nash equilibrium. Denote the tuple of probabilities associated with ti by (pi1 , . . . , pil ) (i 
{1, 2}, |S1 | = |S2 | = l). In a strictly mixed Nash equilibrium we have that the expected
payoff of player 1 is:
lj=1 p2j U1 (se , sj ) ()
for every 1  e  l. Consider now a strategy f of player 1 that assigns probability p2j to
strategy sj . Then, for every strategy se selected by player 2, the expected payoff of f is
given by
lj=1 p2j U1 (sj , se ) = lj=1 p2j U1 (se , sj ) = ()
This implies that the safety level strategy for player 1 yields an expected payoff that is
identical to the expected payoff for player 1 in the above equilibrium. Similar reasoning can
be applied for player 2.
2

7. Competitive Safety Strategies
Let S be a set of strategies. Consider a family of games (g1 , g2 , . . . , gj , . . .) where i is a
player at each of them, its set of strategies at each of these games is S, and there are j
players, in addition to i, in gj . As an example, consider a family of decentralized load
balancing settings. The (n  1)-th game in this extended load-balancing setting will consist
of n players, one of them is i. The players submit their messages along e1 and e2 . The
payoff for player i when participating in an n-person decentralized load balancing game is
X
X
k (resp. k ) if he has chosen e1 (resp. e2 ) and additional k  1 participants have chosen
371

fiTennenholtz

that communication line. A mixed strategy t  (S) will be called a C-competitive safety
strategy if there exists some constant C > 0, such that
nash(i, gj )
C
j val(t, i, gj )
lim

where nash(i, gj ) is the lowest expected payoff player i might obtain in some equilibrium
of gj , and val(t, i, gj ) is the expected payoff guaranteed for i by choosing t in the game gj .
The extended decentralized load balancing setting 6 is a typical and basic network problem.
If C is small, a C-competitive safety strategy for that context will provide a useful protocol
of behavior. We can show:
Theorem 2 There exists a 9/8-competitive safety strategy for the extended decentralized
load-balancing setting.
Proof: Consider the following strategy profile for the players in an n-person decentralized
1
load balancing game: players {1, 2, . . . , d 1+
ne} will choose e1 , and the rest will choose
e2 . W.l.o.g we assume that i = 1 is the player for which we will make the computation of
expected payoffs. It is easy to verify that the above strategy profile is an equilibrium of the
game, with an expected payoff for player i that is bounded above by
X(1 + )
()
n
Intuitively, this equilibrium is obtained by partitioning the players in a way where the payoff
for using the communication lines are (almost) equal. Consider now the following strategy

1
t for player i: select e1 with probability 1+
and select e2 with probability 1+
. Notice
that t (if adopted by all participants) is not a Nash equilibrium. However, we will show
that it is a competitive safety strategy for small C > 0 . Consider an arbitrary number of
participants n, where (n  1) of the other (i.e. excluding player i) n  1 participants use
e2 while the rest use e1 , for some arbitrary 0    1. The expected payoff obtained using
t will be:
1
X

X
+
1 +  (n  1) + 1 1 +  (1  )(n  1) + 1
This value is greater or equal to:
1
X

X
+
1 +  n + 1 1 +  (1  )n + 1
The above equals



1
1
X
+
1 +  n + 1 (1  )n + 1



Simplifying the above we get:
X
n+2
(  )
1 +  (1 + n)(n  n + 1)
6. Here and later the term extended load-balancing setting refers to a family of games as above.

372

fiCompetitive Safety Analysis

Dividing (**) by (***) we get that the ratio is:
(1 + )2 (   2 )n2 + n + 1

n(n + 2)
When n approaches infinity the above ratio approaches
(1 + )2
(   2 )

Given that 0.5   < 1 and 0    1 we get that the above ratio is bounded by 9/8 as
desired.
2
7.1 Extensions: Arbitrary Speeds and m Links
In this section we generalize the result obtained in the context of decentralized load balancing to the case where we have m parallel communication lines leading from source to target.
The value obtained by the agent (w.l.o.g. agent 1) when submitting its message along line
i
i, where ni agents have decided to submit their messages through that line is given by X
ni ,
where 1 = 1  2      m > 0. Our extension enables us to handle the general binary
case where 0 <  < 1, as well as to discuss cases where a safety level strategy can be very
effective in the general m-lines situation. Using the ideas developed for the case m = 2, we
can now show:
m  m 



j6=i j
Theorem 3 There exists a i=1mi2 i=1
competitive safety strategy for the extended
m 
i=1 j
decentralized load-balancing setting, when we allow m (rather than only 2) parallel communication lines, and arbitrary i s.

Proof: Following the ideas of the previous theorem, there exists an equilibrium where agent
1 obtains at most (X/n)m
i=1 i . Intuitively, in this equilibrium the players are distributed
in a way where the payoff for using the different communication lines are (almost) identical.
In particular, agents {1, 2, . . . , d m1 i ne}, where 1 = 1 will be assigned to communication
i=1
line 1, and hence agent 1s payoff will be as prescribed.
Consider the following strategy for each of the agents: choose communication line i with
probability
j6=i j
m
i=1 j6=i j
Given the above, the expected payoff of agent i can be minimized (using similar ideas
to the ones in the proof of Theorem 2), by splitting the other agents equally among the
communication lines. Hence, the expected payoff of the agent is at least:
m
i=1

i Xj6=i j
m2 X m
j=1 j
=
n
m
m
(1 + m )i=1 j6=i j
m + n i=1 j6=i j

Hence, the ratio between the expected payoff in the Nash equilibrium and the expected
payoff that can be guaranteed is bounded by:
m
m+n m
i=1 j6=i j
(

)
i=1 i
2
m n
m
j=1 j
373

fiTennenholtz

The above implies, when n is large the existence of an

m
m
i=1 i i=1 j6=i j
competitive
m2 m
j=1 j

strategy.
2
In the general binary case, where 1 = 1, and 2 = , where 0 <   1, the above
implies the existence of an
(1 + )2
4
competitive strategy.
Corollary 1 Given an extended load balancing setting, where m = 2, with arbitrary speeds
of the communication lines (0 <   1), there exists a 43 -competitive strategy.
2

2

Proof: To see the above notice that 1 +  < (1+)
if and only if  < 1/3 and that (1+)
4
4
is decreasing in the interval (0, 1]. Hence, by considering a strategy prescribed by the above
theorem when   1/3 and selecting e1 otherwise, we are guaranteed a ratio of at most
1 + 1/3 = 4/3.
2
Consider now the general m-links (i.e. m parallel communication lines) case. The
 m i
average network quality (or speed), Q, can be defined as i=1
m . A network will be called
Q
k-regular if m  k. Many networks are k-regular for small k. For example, if m  0.5 as
before, then the network is 2-regular regardless of the number of edges.
Corollary 2 Given a k-regular network, there exists a k-competitive safety strategy for the
extended decentralized load-balancing setting, when we allow m (rather than only 2) parallel
edges.
Proof: To show the above, observe that
m
m
Q m
i=1 i i=1 j6=i j
i=1 j6=i j
=
m2 m

m
m
j=1 j
j=1 j

The latter is smaller or equal to
Q m
=k
m m
as desired.
2
Together, Theorem 3 and corollaries 1 and 2 extend the results on decentralized load
balancing to the general case of m parallel communication lines.

8. Competitive Safety Analysis in Bayesian Games
The results presented in the previous sections refer to games with complete information.
The games we have studied in this context refer to fundamental settings in the AI and game
theory intersection, and deal with issues such as congestion. In this section, we show that
our ideas can be applied to games with incomplete information as well.
In a game with incomplete information the payoff for a player given the behavior of the
set of players is private information of that player. In order to illustrate competitive safety
analysis in games with incomplete information, we have chosen to consider a very basic
mechanism, the first-price auction. The selection of first-price auction is not an accident.
374

fiCompetitive Safety Analysis

Auctions are fundamental to the theory of economic mechanism design7 , and among the
auctions that do not possess a dominant strategy, assuming the independent private value
model, first-price auctions are probably the most common ones.
We consider a setting where a good g is put for sale, and there are n potential buyers.
Each such buyer has a valuation (i.e. maximal willingness to pay) for g that is drawn from
a uniform distribution on the interval of real numbers [0, 1]. This valuation is the private
information that the agent has. The exact valuation is known only to the agent, while the
distribution on agent valuations are commonly known. The valuations are assumed to be
independent from one another. In a first price auction, each potential buyer is asked to
submit a bid for the good g. We assume that the bids of a buyer with valuation v is a
number in the interval [0, v].8 The good will be allocated to the bidder who submitted the
highest bid (with a lottery to determine the winner in a case of a tie). The auction setup can
be defined using a Bayesian game.9 In this game the players are the potential bidders, and
the payoff of a player with valuation v is vp if he wins the good and pays p, and 0 if he does
not get the good. As the reader can see, the distinguished feature of such games is that the
players utility function depends on the agents private valuation, and therefore it is known
only to it. The equilibrium concept can be also extended to the context of Bayesian games.
In the auction setup an agents strategy is a function from its valuations to monetary bids.
A strategy profile will be in equilibrium if an agents strategy is the best response against
the other agents strategies given the distribution on these agents valuations. In particular,
in equilibrium of the above game the bid of a player with valuation v is (1  n1 )v.
n
Given the above, the expected payoff of an agent with valuation v, will be vn . As before,
the question is whether we can guarantee a payoff that is proportional to the expected payoff
in equilibrium.
Before discussing an appropriate strategy, we should emphasize a formal issue with
regard to competitive safety strategies in Bayesian games. Notice that in our definition
of competitive safety strategies, we assume that the players competitive action should be
independent of the number of players. On the other hand, as suggested by the equilibrium
analysis above, behavior in first-price auction may heavily depend on the number of players.
In order to address this issue, we make use of the revelation principle, discussed in the
economic mechanism design literature. The revelation principle tells us that one can replace
the above-mentioned first-price auction with the following auction: each bidder will be asked
to reveal his valuation, and the good will be sold to the bidder who reported the highest
valuation; if agent i who reported valuation v 0 will turn out to be the winner then he will
be asked to pay (1  n1 )v 0 . In this mechanism a player will submit bids in between 0 and
n
n1 v. It turns out that reporting the true valuation is an equilibrium of that auction, and
that it will yield (in equilibrium) the same allocation, payments, and expected utility to
the participants, as the original auction. It is convenient to consider the above revelation
mechanism, since when facing any number of participants, a bidders strategy in equilibrium
will always be the same.
7. For a general discussion of mechanism design see (Mas-Colell, Whinston, & Green, 1995), Chapter 23,
and (Fudenberg & Tirole, 1991), Chapter 7).
8. In general, buyers may submit bids that are higher than their valuations, but these strategies are dominated by other strategies, and their existence will not effect the equilibrium discussed in this paper.
9. A formal definition and exposition of Bayesian games can be found in (Fudenberg & Tirole, 1991).

375

fiTennenholtz

Given the above, a first-price auction setup will be identified with a family of (Bayesian)
games (g1 , g2 , . . .) where gj is the Bayesian game associated with (the revelation mechanism
of) first-price auction with j +1 potential buyers. The definition of C-competitive strategies
can now be applied to the above context as well.
Theorem 4 There exists an e-competitive strategy for the first-price auction setup.
Proof: When player 1 with valuation v submits the bid b in an auction with additional
n  1 players, its worst case payoff is
Z

n1
b
n

v2 =0

Z

dv2

n1
b
n

v3 =0

dv2   

Z

n1
b
n

vn =0

(v 

n1
b)dvn
n

The above says that in order to win, player is bid should be higher than the other players
n
bids. Each players bid in the revelation mechanism is however at most n1
times its
n1
valuation, and therefore we should integrate over valuations that are at most n times
player is bid. If agent i will be the winner then he will gain v  n1
n b when he bids b and
his valuation is v (given the rules of the revelation mechanism). The above is maximized
when
d
n  1 n  1 n1
(v 
b)(
b)
=0
db
n
n
n1 , i.e. when b = v.
Hence, the expected value is maximized when (n  1)vbn2 = n1
n nb
We therefore get that the safety-level strategy coincides in this case with the equilibrium
strategy. The expected payoff in equilibrium can be shown to be v n /n. The expected payoff
guaranteed by the above strategy will be

(

n  1 n1 1
v)
v
n
n

The ratio between the safety level value and the equilibrium value is therefore bounded
n
1
above by ( n1
n ) , which is greater or equal to e , and approaches it when the number of
players approaches infinity.
2
An interesting observation is that in the above theorem, the safety-level strategy is
identical to the equilibrium strategy. This connection occurs although the game is not a
0-sum game. It is interesting to observe that since we consider revelation mechanisms then
the safety-level strategy turns out to be independent of the number of participants. Our
result can also be obtained if we consider standard first-price auctions, rather than the
revelation mechanisms associated with them; nevertheless, this will require us to allow a
player to choose its action knowing the number of potential bidders (as in the corresponding
equilibrium analysis).

9. Discussion
Some previous work in AI has attempted to show the potential power of decision-theoretic
approaches that do not rely on classical game-theoretic analysis. In particular, work in theoretical computer science on competitive analysis has been extended to deal with rationality
constraints (Tennenholtz, 2001), in order to become applicable to multi-agent systems. We
376

fiCompetitive Safety Analysis

introduced competitive safety analysis, bridging the gap between the normative AI/CS approach and classical equilibrium analysis. We have shown that the observation, due to
Aumann, that safety-level strategies may yield the value of a Nash equilibrium in games
that are not zero-sum, provides a powerful normative tool for computer scientists and AI
researchers interested in protocols for non-cooperative environments. We have illustrated
the use and power of competitive safety analysis in various contexts. We have shown general
results about 2  2 games, as well as about games with many participants, and introduced
the use of competitive safety analysis in the context of decentralized load balancing, leader
election, and auctions. Notice that our work is concerned with a normative approach to
decision making in multi-agent systems. We make no claims as for the applicability of this
approach for descriptive purposes, i.e. for the prediction of how people will behave in the
corresponding situations. Although there exists much literature on the failure of Nash equilibrium, it is still the most powerful concept for action prediction in multi-agent systems.
The setting of decentralized load balancing discussed as part of this paper is central to game
theory and its applications.10 Given the importance of this setting from a CS perspective,
providing robust agent protocols for that setting is a major challenge to work in multiagent systems. In order however to build robust protocols, relying on standard equilibrium
analysis might not be satisfactory, and safety guarantees are required. Our work suggests
protocols and analysis for providing such guarantees, bridging the gap between classical
AI/decision-theoretic reasoning and equilibrium analysis in game theory.

Acknowledgements
This work has been carried out when the author was on a sabbatical leave with the computer
science department at Stanford university. A preliminary version of this paper appears in
the proceedings of AAAI-2002.

References
Aumann, R. (1985). On the non-transferable utility value: A comment on the Roth-Shaper
examples. Econometrica, 53 (3), 667677.
Borodin, A., & El-Yaniv, R. (1998). On-Line Computation and Competitive Analysis. Cambridge
University Press.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Koutsoupias, E., & Papadimitriou, C. (1999). Worst-Case Equilibria. In STACS.
Kraus, S. (1997). Negotiation and cooperation in multi-agent environments. Artificial Intelligence,
94, 7997.
Mas-Colell, A., Whinston, M., & Green, J. (1995). Microeconomic Theory. Oxford University Press.
Monderer, D., & L.S.Shapley (1996). Potential games. Games and Economic Behavior, 14, 124143.
Nisan, N., & Ronen, A. (1999). Algorithmic mechanism design. Proceedings of STOC-99.
10. See the literature on potential and congestion games, e.g. (Monderer & L.S.Shapley, 1996; Rosenthal,
1973).

377

fiTennenholtz

Rosenschein, J. S., & Zlotkin, G. (1994). Rules of Encounter. MIT Press.
Rosenthal, R. (1973). A class of games possessing pure-strategy nash equilibria. International
Journal of Game Theory, 2, 6567.
Roth, A. E. (1980). Values for games without side payments: Some difficulties with current concepts.
Econometrica, 48 (2), 457465.
Roughgarden, T. (2001). The price of anarchy is independent of the network topology. In Proceedings
of the 34th Annual ACM Symposium on the Theory of Computing, pp. 428437.
Roughgarden, T., & Tardos, E. (2002). How bad in selfish routing?. Journal of the ACM, 49 (2),
236259.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: A Modern Approach. Prentice Hall.
Sandholm, T. W., & Lesser, V. (1995). Equilibrium Analysis of the Possibilities of Unenforced
Exchange in Multiagent Syustems. In Proc. 14th International Joint Conference on Artificial
Intelligence, pp. 694701.
Tennenholtz, M. (2001). Rational Competitive Analysis. In Proc. of the 17th International Joint
Conference on Artificial Intelligence, pp. 10671072.

378

fiJournal of Artificial Intelligence Research 17 (2002) 265287

Submitted 10/01; published 9/02

When do Numbers Really Matter?
Hei Chan
Adnan Darwiche

hei@cs.ucla.edu
darwiche@cs.ucla.edu

Computer Science Department
University of California, Los Angeles
Los Angeles, CA 90095, USA

Abstract
Common wisdom has it that small distinctions in the probabilities (parameters) quantifying a belief network do not matter much for the results of probabilistic queries. Yet,
one can develop realistic scenarios under which small variations in network parameters can
lead to significant changes in computed queries. A pending theoretical question is then to
analytically characterize parameter changes that do or do not matter. In this paper, we
study the sensitivity of probabilistic queries to changes in network parameters and prove
some tight bounds on the impact that such parameters can have on queries. Our analytic
results pinpoint some interesting situations under which parameter changes do or do not
matter. These results are important for knowledge engineers as they help them identify
influential network parameters. They also help explain some of the previous experimental
results and observations with regards to network robustness against parameter changes.

1. Introduction
A belief network is a compact representation of a probability distribution (Pearl, 1988;
Jensen, 2001). It consists of two parts, one qualitative and the other quantitative. The
qualitative part of a belief network (called its structure) is a directed acyclic graph in
which nodes represent domain variables and edges represent direct influences between these
variables. The quantitative part of a belief network is a set of conditional probability tables
(CPTs) that quantify our beliefs in such influences. Figure 1 depicts the structure of a
belief network and Figure 2 depicts its CPTs.1
Automated reasoning systems based on belief networks have become quite popular recently as they have enjoyed much success in a number of real-world applications. Central to
the development of such systems is the construction of a belief network (hence, a probability distribution) that faithfully represents the domain of interest. Although the automatic
synthesis of belief networksbased on design information in certain applications and based
on learning techniques in othershas been drawing a lot of attention recently, mainstream
methods for constructing such networks continue to be based on traditional knowledge engineering (KE) sessions involving domain experts. One of the central issues that arise in
such KE sessions is the assessment of impact that changes in network parameters may have
on probabilistic queries of interest.
Consider for example the following common method for constructing belief networks in
medical diagnosis applications (Coupe, Peek, Ottenkamp, & Habbema, 1999). First, the
1. This specific network and its CPTs are distributed with the evaluation version of the commercial HUGIN
system at http://www.hugin.com/.
c
2002
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiChan & Darwiche

Fire

Smoke

Tampering

Alarm

Leaving

Report

Figure 1: A belief network structure.

Fire
true
false

x|u
.01
.99

Fire
true
true
false
false

Smoke
true
false
true
false

x|u
.9
.1
.01
.99

Alarm
true
true
false
false

Leaving
true
false
true
false

x|u
.88
.12
.001
.999

Fire
true
true
true
true
false
false
false
false

Tampering
true
false

x|u
.02
.98

Tampering
true
true
false
false
true
true
false
false

Alarm
true
false
true
false
true
false
true
false

x|u
.5
.5
.99
.01
.85
.15
.0001
.9999

Leaving
true
true
false
false

Report
true
false
true
false

x|u
.75
.25
.01
.99

Figure 2: The CPTs of the belief network shown in Figure 1.

network structure is developed. Next, parameters are estimated by non-experts using a
combination of statistical data and qualitative influences available from textbook materials.
Finally, medical experts are brought in to evaluate the network and fine-tune its parameters.
One method of evaluation is to pose diagnostic scenarios to the network, and compare the
results of such queries to those expected by the experts. For example, given some set of
symptoms e, and two potential diagnoses y and z, the network may give us the conclusion
that Pr (y | e)/Pr (z | e) = 2, while a domain expert may believe that the ratio should
be no less than 4. Assuming that the network structure is correct, a central question is
then: which network parameters should be changed to give us the correct ratio, and by how
much?
To automate the task of identifying such parameter changes, we have recently developed a belief network tool, called SamIam (Sensitivity Analysis, Modelling, Inference And
266

fiWhen do Numbers Really Matter?

Figure 3: A screen shot of SamIam performing sensitivity analysis on the belief network
shown in Figure 1.

More)2 . One of its feature is sensitivity analysis, which allows domain experts to fine-tune
network parameters in order to enforce constraints on the results of certain queries. Users
can specify the constraint that they want to enforce, and SamIam will automatically decide whether a given parameter is relevant to this constraint, and if it is, will compute the
minimum amount of change to that parameter which is needed to enforce the constraint.
The technical details of our approach to sensitivity analysis are the subject of Section 2.
As we experimented with SamIam, we ran into scenarios that we found to be surprising
at first glance. Specifically, there were many occasions in which queries would be quite
sensitive to small variations in certain network parameters. Consider the scenario in Figure 3
for one example, which corresponds to the network detailed in Figures 1 and 2. Here, we
have evidence e = report, smoke: people are reported to be evacuating a building, but there
is no evidence for any smoke. This evidence should make tampering more likely than fire,
and the given belief network does indeed reflect this with Pr (tampering | e) = .50 and
Pr (fire | e) = .03. We wanted, however, the probability of tampering to be no less than .65.
Hence, we asked SamIam to identify parameter changes that can enforce this constraint,
and it made two recommendations:
1. either decrease the probability of a false report, Pr (report | leaving), from its current
value of .01 to  .0047,
2. SamIam is developed by the UCLA Automated Reasoning Group.
http://reasoning.cs.ucla.edu/.

267

Its web page is at

fiChan & Darwiche

2. or increase the prior probability of tampering from its current value of .02 to  .036.
Therefore, the distinctions between .02 and .036, and the one between .01 and .0047, do
really matter in this case as each induces an absolute change of .15 on the probabilistic query
of interest. Note also that implicit in SamIams recommendations is that the parameters
of variables Fire, Smoke, Leaving, and Alarm are irrelevant to enforcing this constraint, i.e.
no matter how much we change any of these parameters, we would not be able to enforce
our desired constraint.
This example shows that the absolute change in a query can be much larger than the
absolute change in the corresponding parameters. Later, we will show an example where
an infinitesimal change to a network parameter leads to a change of .5 to a corresponding
query. We also show examples in which the relative change in the probability of a query is
larger than the corresponding relative change in a network parameter. One wonders then
whether there is a different method for measuring probabilistic change (other than absolute
or relative), which allows one to non-trivially bound the change in a probabilistic query in
terms of the corresponding change in a network parameter.
To answer this and related questions, we conduct in Section 3 an analytic study of the
partial derivative of a probabilistic query Pr (y | e) with respect to some network parameter
x|u . Our study leads us to three main results:
1. a bound on the derivative in terms of Pr (y | e) and Pr (x | u) only, which is independent of any other aspect of the given belief network;
2. a bound on the sensitivity of queries to infinitesimal changes in network parameters;
3. a bound on the sensitivity of queries to arbitrary changes in network parameters.
The last bound in particular shows that the amount of change in a probabilistic query can
be bounded in terms of the amount of change in a network parameter, as long as change is
understood to be the relative change in odds. This result has a number of practical implications. First, it can relieve experts from having to be too precise when specifying certain
parameters subjectively. Next, it can be important for approximate inference algorithms
that pre-process network parameters to eliminate small distinctions in such parameters, in
order to increase the efficiency of inference (Poole, 1998). Finally, it can be used to show
that automated reasoning systems based on belief networks are robust and, hence, suitable
for real-world applications (Pradhan, Henrion, Provan, Del Favero, & Huang, 1996).
Section 4 is indeed dedicated to exploring the implications of the above bounds, where
we provide an analytic explanation of why certain parameter changes dont matter. We
finally close in Section 5 with some concluding remarks. Proofs of all theorems are given in
Appendix A.

2. The Tuning of Network Parameters
We report in this section on a tool that we have been developing, called SamIam, for finetuning network parameters (Laskey, 1995; Castillo, Gutierrez, & Hadi, 1997; Jensen, 1999;
Kjrulff & van der Gaag, 2000; Darwiche, 2000). Given a belief network, some evidence
e, which is an instantiation of variables E in the belief network, and two events y and z of
variables Y and Z respectively, where Y, Z 6 E, our tool can efficiently identify parameter
changes needed to enforce the following types of constraints:
268

fiWhen do Numbers Really Matter?

Difference: Pr (y | e)  Pr (z | e)  ;
Ratio: Pr (y | e)/Pr (z | e)  .
These two constraints often arise when we debug belief networks. For example, we can
make event y more likely than event z, given evidence e, by specifying the constraint,
Pr (y | e)  Pr (z | e)  0, or we can make event y at least twice as likely as event z, given
evidence e, by specifying the constraint, Pr (y | e)/Pr (z | e)  2. We will discuss next how
one would enforce the two constraints, but we need to settle some notational conventions
and technical preliminaries first.
Variables are denoted by upper-case letters (A) and their values by lower-case letters (a).
Sets of variables are denoted by bold-face upper-case letters (A) and their instantiations
are denoted by bold-face lower-case letters (a). For a variable A with values true and false,
we use a to denote A = true and a to denote A = false. The CPT for variable X with
parents U defines a set of conditional probabilities of the form Pr (x | u), where x is a value
of variable X, u is an instantiation of parents U, and Pr (x | u) is a probability known as a
network parameter and denoted by x|u . We finally recall a basic fact about belief networks.
The probability of some instantiation x of all network variables X equals the product
of all network parameters that are consistent with that instantiation. For example, the
probability of instantiation fire, tampering, smoke, alarm, leaving, report in Figure 1 equals
.01  .98  .9  .99  .12  .01, which is the product of network parameters (from Figure 2)
that are consistent with this instantiation.
2.1 Binary Variables
We first consider the parameters of a binary variable X, with two values x and x and,
hence, two parameters x|u and x|u for each parent instantiation u. We assume that for
each variable X and parent instantiation u we have a meta parameter x|u , such that
x|u = x|u and x|u = 1  x|u . Therefore, our goal is then to determine the amount of
change to the meta parameter x|u which would lead to a simultaneous change in both x|u
and x|u . We use the meta parameter x|u because it is not meaningful to change only x|u
or x|u without changing the other since x|u + x|u = 1.
First we observe that the probability of an instantiation e, Pr (e), is a linear function
in any network parameter x|u in a belief network (Russell, Binder, Koller, & Kanazawa,
1995; Castillo et al., 1997). In fact, the probability is linear in any meta parameter x|u .
Theorem 2.1 The derivative of Pr (e) with respect to the meta parameter x|u is given by:
Pr (e)
Pr (e, x, u) Pr (e, x, u)
=

,
x|u
x|u
x|u

(1)

when x|u 6= 0 and x|u 6= 0.3 We will designate the derivative as constant e .
In Theorem 2.1, e = Pr (e, x, u)/x|u  Pr (e, x, u)/x|u is a constant in terms of both
x|u and x|u (and consequently, x|u ) since Pr (e, x, u) = Kx x|u and Pr (e, x, u) = Kx x|u ,
3. If either of the previous parameters is zero, we can use the differential approach by Darwiche (2000) to
compute the derivative directly.

269

fiChan & Darwiche

where Kx = Pr (u)Pr (e | x, u) and Kx = Pr (u)Pr (e | x, u) are constants in terms of both
x|u and x|u . By substituting y, e and z, e for e in Theorem 2.1, we get:
y,e =

Pr (y, e)
x|u

=

Pr (y, e, x, u) Pr (y, e, x, u)

;
x|u
x|u

(2)

z,e =

Pr (z, e)
x|u

=

Pr (z, e, x, u) Pr (z, e, x, u)

.
x|u
x|u

(3)

Now, if we want to enforce the Difference constraint, Pr (y | e)  Pr (z | e)  , it
suffices to ensure that Pr (y, e)  Pr (z, e)  Pr (e). Suppose that the previous constraint
does not hold, and we wish to establish it by applying a change of  to the meta parameter
x|u . Such a change leads to a change of e  in Pr (e). It also changes Pr (y, e) and Pr (z, e)
by y,e  and z,e , respectively. Hence, to enforce the Difference constraint, we need to
solve for  in the following inequality:
[Pr (y, e) + y,e ]  [Pr (z, e) + z,e ]  [Pr (e) + e ].
Rearranging the terms, we get the following result.
Corollary 2.1 To satisfy the Difference constraint, we need to change the meta parameter x|u by , such that:
Pr (y, e)  Pr (z, e)  Pr (e)  [y,e + z,e + e ],
where the  constants are defined by Equations 1, 2 and 3.
We can similarly solve for parameter changes  that enforce the Ratio constraint,
Pr (y | e)/Pr (z | e)  , in the following inequality:
[Pr (y, e) + y,e ]/[Pr (z, e) + z,e ]  .
Rearranging the terms, we get the following result.
Corollary 2.2 To satisfy the Ratio constraint, we need to change the meta parameter x|u
by , such that:
Pr (y, e)  Pr (z, e)  [y,e + z,e ],
where the  constants are defined by Equations 2 and 3.
For both the Difference and Ratio constraints, the solution of , if any, is always in
one of two forms:
   q, for some computed q < 0, in which case the new value of meta parameter x|u
must be in the interval [0, p + q].
   q, for some computed q > 0, in which case the new value of meta parameter x|u
must be in the interval [p + q, 1].
270

fiWhen do Numbers Really Matter?

Note that p is the current value of meta parameter x|u (before the change). For many
parameters, these intervals are empty and, therefore, there is no way we can change these
meta parameters to enforce the constraint.
The question now is how to solve these inequalities, efficiently, and for all meta parameters. Note that there may be more than one possible parameter change that would enforce
the given constraint, so we need to identify all such changes. With either Corollary 2.1
or 2.2, we can easily solve for the amount of change needed, , once we know the following
probabilities: Pr (e), Pr (y, e), Pr (z, e), Pr (e, x, u), Pr (e, x, u), Pr (y, e, x, u), Pr (y, e, x, u),
Pr (z, e, x, u), and Pr (z, e, x, u). This leads to the following complexity of our technique.
Corollary 2.3 If we have an algorithm that can compute Pr (i, x, u), for a given instantiation i, and all family instantiations x, u of every variable X, in time O(f ), then we can
solve for Corollaries 2.1 and 2.2 for all parameters in time O(f ). We do this by running
the algorithm three times, once with i = e, and then with i = y, e, and finally with i = z, e.
Recall that the family of a variable X is the set containing X, and its parents U in the
belief network.
The join-tree algorithm (Jensen, Lauritzen, & Olesen, 1990) and the differential approach (Darwiche, 2000) can both compute Pr (i, x, u), for a given instantiation i and all
family instantiations x, u of every variable X in O(n exp w) time. Here, n is the number of
variables in the belief network, and w is the width of a given elimination order. SamIam
uses the differential approach, and thus its running time to identify all possible parameter
changes in a network is also O(n exp w). Note that this is also the time needed to answer
one of the simplest queries, that of computing the probability of evidence e.
2.2 Multi-Valued Variables
Our results can be easily extended to multi-valued variables, as long as we assume a model
for changing co-varying parameters when one of them changes (Darwiche, 2000; Kjrulff &
van der Gaag, 2000). After the parameter x|u changes, we need to use a scheme to change
the other parameters, xi |u for all xi 6= x, in order to ensure the sum-to-one constraint.
The most common way to do this is to use the proportional scheme. In this scheme,
we change the other parameters so that the ratios between them remain the same. For
example, suppose we have three parameters x1 |u = .6, x2 |u = .3 and x3 |u = .1. After
x1 |u changes to .8, the other two parameter values will be changed to x2 |u = .3(.2/.4) = .15
and x3 |u = .1(.2/.4) = .05 accordingly. We now define the meta parameter x|u such that it
simultaneously changes all parameters according to the proportional scheme. We can then
obtain a linear relation between Pr (e) and x|u , and the partial derivative is given by:
Pr (e)
Pr (e, x, u)
=

x|u
x|u

P

xi 6=x Pr (e, xi , u)

P

xi 6=x xi |u

.

This is very similar to the result in Theorem 2.1, in the way that we have grouped all the
values xi 6= x into the value x. We can then use Corollaries 2.1 and 2.2 to solve for the
Difference and Ratio constraints.
We now present another example to illustrate how the results above are used in practice.

271

fiChan & Darwiche

Example 2.1 Consider again the network in Figure 3. Here, we set the evidence such that
we have smoke, but no report of people evacuating the building, i.e. e = smoke, report. We
then got the posteriors Pr (fire | e) = .25 and Pr (tampering | e) = .02. We thought in this
case that the posterior on fire should be no less than .5 and asked SamIam to recommend
the necessary changes to enforce the constraint, Pr(fire | e)  Pr (fire | e)  0. There were
five recommendations in this case, three of which could be ruled out based on qualitative
considerations:
1. increase the prior on fire to  .03 (from .01);
2. increase the prior on tampering to  .80 (from .02);
3. decrease Pr (smoke | fire) to  .003 (from .01);
4. increase Pr (leaving | alarm) to  .923 (from .001);
5. increase Pr (report | leaving) to  .776 (from .01).
Clearly, the only sensible change here is either to increase the prior on fire, or to decrease
the probability of having smoke without a fire.
This example and other similar ones suggest that identifying such parameter changes
and their magnitudes is inevitable for developing a faithful belief network, yet it is not trivial
for experts to accomplish this task by visual inspection of the belief network, often due to
its size and complexity. Sensitivity analysis tools such as SamIam can help facilitate this
by identifying important parameters that need to be fine-tuned in order to satisfy certain
constraints. Of course, if we are given multiple constraints, we need to be cautious when
implementing a recommendation made by SamIam due to one constraint, because this may
result in violating other constraints. In this case, the parameter changes recommended
by SamIam should be used to help experts in focusing their attention on the relevant
parameters.
Moreover, the previous examples illustrate the need to develop more analytic tools to
understand and explain the sensitivity of queries to certain parameter changes. There is
also a need to reconcile the sensitivities exhibited by our examples with previous experimental studies demonstrating the robustness of probabilistic queries against small parameter
changes in certain application areas, such as diagnosis (Pradhan et al., 1996). We address
these particular questions in the next two sections.

3. The Sensitivity of Probabilistic Queries to Parameters Changes
Our starting point in understanding the sensitivity of a query Pr (y | e) to changes in a
meta parameter x|u is to analyze the derivative Pr (y | e)/x|u . In our analysis, we
assume that X is binary, but Y and all other variables in the network can be multi-valued.
The following theorem provides a simple bound on this derivative, in terms of Pr (y | e)
and Pr (x | u) only. We then use this simple bound to study the effect of changes to meta
parameters on probabilistic queries.

272

fiWhen do Numbers Really Matter?

6
pd bound 4
0.8

2
0

0.6
0.4

0.2
0.4
Pr(x| u)

Pr(y|e)

0.2

0.6
0.8

Figure 4: The plot of the upper bound on the partial derivative Pr (y | e)/x|u , as given
in Theorem 3.1, against Pr (x | u) and Pr (y | e).

Theorem 3.1 If X is a binary variable in a belief network, then:4


 Pr (y | e) 
Pr (y | e)(1  Pr (y | e))


.


 x|u 
Pr (x | u)(1  Pr (x | u))

The bound in Theorem 3.1 is tight, and we will show later an example for which the
derivative assumes the above bound exactly. The main point to note about this bound is
that it is independent of any given belief network.5
The plot of this bound against Pr (x | u) and Pr (y | e) is shown in Figure 4. A number
of observations are in order about this plot:
 For extreme values of Pr (x | u), the bound approaches infinity, and thus a small
absolute change in the meta parameter x|u can have a big impact on the query
Pr (y | e).
 On the other hand, the bound approaches 0 for extreme values of the query Pr (y | e).
Therefore, a small absolute change in the meta parameter x|u will have a small effect
on the absolute change in the query.
One of the implications of this result is that if we have a belief network where queries
of interest Pr (y | e) have extreme values, then such queries will be robust against small
changes in network parameters. This of course assumes that robustness is understood to
4. This theorem and all results that follow requires that x|u 6= 0 and x|u 6= 1, since we can only use the
expression in Equation 2.1 under these conditions.
5. Note that we have an exact closed form for the derivative Pr (y | e)/x|u (Darwiche, 2000; Greiner,
Grove, & Schuurmans, 1997), but that form includes terms which are specific to the given belief network.

273

fiChan & Darwiche

X

Y

E

Figure 5: The network used in Example 3.1.

be a small change in the absolute value of the given query. Interestingly enough, if y is a
disease which is diagnosed by finding ethat is, the probability Pr (y | e) is quite high
then it is not surprising that such queries would be robust against small perturbations to
network parameters. This seems to explain some of the results by Pradhan et al. (1996),
where robustness have been confirmed for queries with Pr (y | e)  .9.
Another implication of the above result is that one has to be careful when changing
parameters that are extreme. Such parameters are potentially very influential and one
must handle them with care.
Therefore, the worst situation from a robustness viewpoint materializes if one has extreme parameters with non-extreme queries. In such a case, the queries can be very sensitive
to small variations in the parameters.
Example 3.1 Consider the network structure in Figure 5. We have two binary nodes, X
and Y with respective parameters x , x and y , y . We assume that E is a deterministic
binary node where the value of E is e iff X = Y . This dictates the following CPT for E:
Pr (e | x, y) = 1, Pr (e | x, y) = 1, Pr (e | x, y) = 0 and Pr (e | x, y) = 0. The conditional
probability Pr (y | e) can be expressed using the root parameters x and y as:
Pr (y | e) =

x y
.
x y + x y

Since x /x = 1 and x /x = 1, the derivative of Pr (y | e) with respect to the meta
parameter x is given by:
Pr (y | e)
x

=
=

(x y + x y )y  x y (y  y )
(x y + x y )2
y y
.
(x y + x y )2

This is equal to the upper bound given in Theorem 3.1:
Pr (y | e)(1  Pr (y | e))
Pr (x)(1  Pr (x))

=
=

(x y )(x y )
x x (x y + x y )2
y y
.
(x y + x y )2

Now, if we set x = y , the derivative becomes:
Pr (y | e)
1
=
,
x
4x x
274

fiWhen do Numbers Really Matter?

and as x (or x ) approaches 0, the derivative approaches infinity. Finally, if we set x =
y = , we have Pr (y | e) = .5, but if we keep y and y constant and change x from  to
0, we get the new result Pr (y | e) = 0.
Example 3.1 then illustrates three points. First, it shows that the bound in Theorem 3.1
is tight, i.e. we can construct a belief network that assumes the bound. Second, it gives an
example network for which the derivative Pr (y | e)/x|u tends to infinity, and therefore we
cannot bound the derivative by any constant. Finally, it shows that an infinitesimal absolute
change in a meta parameter (changing x from  to 0) can induce a non-infinitesimal absolute
change in some query (Pr (y | e) changes from .5 to 0). The following theorem, however,
shows that this is not possible if we consider a relative notion of change.
Theorem 3.2 Assume that x|u  .5 without loss of generality.6 Suppose that x|u is an
infinitesimal change applied to the meta parameter x|u , leading to a change of Pr (y | e)
to the query Pr (y | e). We then have:




  
 Pr (y | e) 
x|u




.
 Pr (y | e)   2  
x|u 

For a function f (x), the quantity:
(f (x)  f (x0 ))/f (x0 )
,
(x  x0 )/x0
(xx0 )0
lim

is typically known as the sensitivity of f to x at x0 . Therefore, Theorem 3.2 shows that the
sensitivity of Pr (y | e) to x|u is bounded.
As an example application of Theorem 3.2, consider Example 3.1 again. The change
of x from  to 0 amounts to a relative change |  /| = 1. The corresponding change of
Pr (y | e) from .5 to 0 amounts to a relative change of |  .5/.5| = 1. Hence, the relative
change in the query is not as great from this viewpoint.7
The relative change in Pr (y | e) may be greater than double the relative change in x|u
for non-infinitesimal changes because the derivative Pr (y | e)/x|u depends on the value
of x|u (Darwiche, 2000; Jensen, 1999). Going back to Example 3.1, if we set x = .5 and
y = .01, we obtain the result Pr (y | e) = .01. If we now increase x to .6, a relative change
of 20%, we get the new result Pr (y | e) = 0.0149, a relative change of 49%, which is more
than double of the relative change in x .
The question now is: Suppose that we change a meta parameter x|u by an arbitrary
amount (not an infinitesimal amount), what can we say about the corresponding change in
the query Pr (y | e)? We have the following result.
Theorem 3.3 Let O(x | u) denote the odds of x given u: O(x | u) = Pr (x | u)/(1  Pr (x |
u)), and let O(y | e) denote the odds of y given e: O(y | e) = Pr (y | e)/(1  Pr (y | e)).
Let O0 (x | u) and O0 (y | e) denote these odds after having applied an arbitrary change to
6. For a binary variable X, if x|u > .5, we can instead choose the meta parameter x|u without loss of
generality.
7. If we consider the meta parameter x = 1   instead, the relative change in x will then amount to
/(1  ). But Theorem 3.2 will not be applicable in this case (assuming that  is close to 0) since the
theorem requires that the chosen meta parameter be no greater than .5.

275

fiChan & Darwiche

the meta parameter x|u where X is a binary variable in a belief network. If the change is
positive, then:
O(x | u)
O0 (y | e)
O0 (x | u)


;
O0 (x | u)
O(y | e)
O(x | u)
or if it is negative, then:
O0 (x | u)
O0 (y | e)
O(x | u)

 0
.
O(x | u)
O(y | e)
O (x | u)
Combining both results, we have:
| ln(O0 (y | e))  ln(O(y | e))|  | ln(O0 (x | u))  ln(O(x | u))|.
Theorem 3.3 means that the relative change in the odds of y given e is bounded by the
relative change in the odds of x given u, if X is a binary variable.8 Note that the result
makes no assumptions whatsoever about the structure of the given belief network.
To illustrate this theorem, we go back to Example 2.1. We intend to increase the
posterior Pr (fire | e) from .25 to .5, for e = smoke, report. The log-odds change for the
query is thus lo(Pr (y | e)) = | ln(O0 (y | e))  ln(O(y | e))| = 1.1. There were five
recommendations made by SamIam and we can calculate the log-odds change, lo(x|u ) =
| ln(O0 (x | u))  ln(O(x | u))| for each parameter change:
1. increase the prior on fire to  .03 (from .01): lo(x|u ) = 1.1;
2. increase the prior on tampering to  .80 (from .02): lo(x|u ) = 5.3;
3. decrease Pr (smoke | fire) to  .003 (from .01): lo(x|u ) = 1.2;
4. increase Pr (leaving | alarm) to  .923 (from .001): lo(x|u ) = 9.4;
5. increase Pr (report | leaving) to  .776 (from .01): lo(x|u ) = 5.8.
Therefore, we can see that all the recommended parameter changes satisfy Theorem 3.3,
i.e. the log-odds change of the query is bounded by the log-odds change of the parameter.
An interesting special case of Theorem 3.3 is when X is a root node and X = Y . From
basic probability theory, we have:
O(x | e) = O(x)

Pr (e | x)
.
Pr (e | x)

As the ratio Pr (e | x)/Pr (e | x) is independent of Pr (x), the ratio O(x | e)/O(x) is also
independent of this prior. Therefore, we can conclude that:
O0 (x | e)
O0 (x)
=
.
O(x | e)
O(x)

(4)

This means we can find the exact amount of change needed for a meta parameter x in
order to induce a particular change on the query Pr (x | e). There is no need to use the
more expensive technique of Section 2 in this case.
8. We recently expanded our results to multi-valued variables, where we arbitrarily change parameters
0
x|u to new values x|u
, for all values x. The resulting bound is: | ln(O0 (y | e))  ln(O(y | e))| 
0
0
ln(maxx x|u /x|u )  ln(minx x|u
/x|u ) (Chan & Darwiche, 2002).

276

fiWhen do Numbers Really Matter?

Example 3.2 Consider the network in Figure 3. Suppose that e = report, smoke. Currently, Pr (tampering) = .02 and Pr (tampering | e) = .50. We wish to increase the conditional probability to .65. We can compute the new prior probability Pr 0 (tampering) using
Equation 4:
.65/.35
Pr 0 (tampering)/(1  Pr 0 (tampering))
=
,
.50/.50
.02/.98
giving us Pr 0 (tampering) = .036, which is equal to the result we obtained using SamIam
in Section 1. Both the changes to Pr (tampering) and Pr (tampering | e) bring a log-odds
difference of .616.
Theorem 3.3 has a number of implications. First, given a particular query Pr (y | e) and
a meta parameter x|u , it can be used to bound the effect that a change in x|u will have on
the query Pr (y | e). Going back to Example 3.2, we may wish to know what is the impact
on other conditional probabilities if we apply the change making Pr 0 (tampering) = .036.
The log-odds changes for all conditional probabilities in the network will be bounded by
.616. For example, currently Pr (fire | e) = .029. Using Theorem 3.3, we can find the range
of the new conditional probability value Pr 0 (fire | e):
 
!



.029 
Pr 0 (fire | e)

 ln
ln
  .616,

1  P r0 (fire | e)
.971 

giving us the range .016  Pr 0 (fire | e)  .053. The exact value of Pr 0 (fire | e), obtained
by inference, is .021, which is within the computed bounds.
Second, Theorem 3.3 can be used to efficiently approximate solutions to the Difference
and Ratio problems we discussed in Section 2. That is, given a desirable change in the
value of query Pr (y | e), we can use Theorem 3.3 to immediately compute a lower bound
on the minimum change to meta parameter x|u needed to induce the change. This method
can be applied in constant time and can serve as a preliminary recommendation, as the
method proposed in Section 2 is much more expensive computationally.
Third, suppose that SamIam was used to recommend parameter changes that would
induce a desirable change on a given query. Suppose further that SamIam returned a
number of such changes, each of which is capable of inducing the necessary change. The
question is: which one of these changes should we adopt? The main principle applied
in these situations is to adopt a minimal change. But what is minimal in this case? As
Theorem 3.3 reveals, a notion of minimality which is based on the amount of absolute change
can be very misleading. Instead, it suggests that one adopts the change that minimizes the
relative change in the odds, as other queries can be shown to be robust against such a
change in a precise sense.
For example, we are given two parameter changes, one from .1 to .15, and another from
.4 to .45. Both these changes give us the same absolute change of .05. However, the first
change has an log-odds change of .462, while the second one has an log-odds change of .205.
Therefore, two parameter changes that give us the same absolute change can have different
amounts of log-odds change.
On the other hand, two parameter changes that give us the same relative change can
also have different amounts of log-odds change. For example, we are given two parameter
changes, one from .1 to .2, and another from .2 to .4. Both these changes double the original
277

fiChan & Darwiche

parameter value. However, the first change has a log-odds change of .811, while the second
one has a log-odds change of .981.
Finally, the result can be used to obtain a better intuitive understanding of parameter
changes that do or do not matter, a topic which we will discuss in the next section.

4. Changes that (Dont) Matter
We now return to a central question: When do changes in network parameters matter and
when do they not matter? As we mentioned earlier, there have been experimental studies
investigating the robustness of belief networks against parameter changes (Pradhan et al.,
1996). But we have also shown very simple and intuitive examples where networks can
be very sensitive to small parameter changes. This calls for a better understanding of the
effect of parameter changes on queries, so one can intuitively sort out situations in which
such changes do or do not matter. Our goal in this section is to further develop such
an understanding by looking more closely into some of the implications of Theorem 3.3.
We start first by highlighting the difference between this theorem and previous results on
sensitivity analysis.
4.1 Network-Specific Sensitivity Analysis
One of the main differences between our results and other sensitivity analysis approaches
is that we do not need to know the belief network, and hence, do not need to perform
inference. To clarify this difference, we compare it with the sensitivity function approach
(van der Gaag & Renooij, 2001), which computes the sensitivity function that relates a
query, f (x), and a parameter, x, in the form:
f (x) =

ax+b
,
cx+d

where a, b, c, d are constants that depend on the given network and are computed by
performing inference as suggested by van der Gaag and Renooij (2001).
Going back to Example 2.1, we can express the query Pr (fire | smoke, report) as a
function of the parameter x = Pr (smoke | fire). The function is given by:
f (x) =

0.003165
,
0.9684  x + 0.003165

and we plot this function in Figure 6. We can see that at the current parameter value .01,
the query value is .25, but if we decrease it to .003, the query value increases to .5, which
is one of the suggested parameter changes by SamIam.
However, we can find a bound on the relations between the query and the parameter
using Theorem 3.3, without doing inference on the network (and without knowing the
network). For example, by changing the current parameter value from .01 to .003, the new
query value will be within the bounds of .09 and .53. On the other hand, if we want the
query value to increase to .5, we have to at least decrease the parameter value from .01 to
.003, or increase it to .03.
278

fiWhen do Numbers Really Matter?

___

___

Pr(fire|smoke, report)

Pr(fire| smoke, report)
1

0.5

0.8

0.4
0.6
0.3
0.4

0.2

0.2

0.1
___

0.2

0.4

0.6

0.8

1

___

Pr(smoke, fire)

0.005

0.01

0.015

0.02

Pr(smoke, fire)

Figure 6: The plot of the query Pr (fire | smoke, report) against the parameter Pr (smoke |
fire). The second graph shows a magnification of the first graph for the region
where Pr (smoke | fire) is between 0 and .02.

4.2 Assuring Query Robustness
One of the important issues we have yet to settle is: What does it mean for a parameter
change to not matter? One can think of at least three definitions. First, the absolute
change in the probability Pr (y | e) is small. Second, the relative change in the probability
Pr (y | e) is small. Third, relative change in the odds O(y | e) is small. The first notion is
the one most prevalent in the literature, so we shall adopt it in the rest of this section.
Suppose we have a belief network for a diagnostic application and suppose we are concerned about the robustness of the query Pr (y | e) with respect to changes in network
parameters. In this application, y is a particular disease and e is a particular finding which
predicts the disease, with Pr (y | e) = .9. Let us define robustness in this case to be an
absolute change of no more than .05 to the given query. Now, let X be a binary variable in
the network and let us ask: What kind of changes to the parameters on X are guaranteed
to keep the query within the desirable range? We can use Theorem 3.3 easily to answer this
question. First, if we are changing a parameter by , and if we want the value of the query
to remain  .95, we must ensure that:
| ln((p + )/(1  p  ))  ln(p/(1  p))|  | ln(.95/.05)  ln(.9/.1)| = .7472,
where p is the current value of the parameter. Similarly, if we want to ensure that the query
remains  .85, we want to ensure that:
| ln((p + )/(1  p  ))  ln(p/(1  p))|  | ln(.85/.15)  ln(.9/.1)| = .4626.
Figure 7 plots the permissible change  as a function of p, the current value of the
parameter. The main point to observe here is that the amount of permissible change
depends on the current value of p, with smaller changes allowed for extreme values of p.
It is also interesting to note that it is easier to guarantee the query to stay  .95 than to
guarantee that it stays  .85. In general, it is more likely for a parameter change to reduce
279

fiChan & Darwiche



0.15
0.1
0.05

0.2

0.4

0.6

0.8

1

p

0.05
0.1
0.15

Figure 7: The amount of parameter change  that would guarantee the query Pr (y | e) = .9
to stay within the interval [.85, .95], as a function of the current parameter value
p. The outer envelope guarantees the query to remain  .95, while the inner
envelope guarantees the query to remain  .85.



0.04

0.02

0.2

0.4

0.6

0.8

1

p

0.02

0.04

Figure 8: The amount of parameter change  that would guarantee the query Pr (y | e) = .6
to stay within the interval [.55, .65], as a function of the current parameter value
p. The outer envelope guarantees the query to remain  .65, while the inner
envelope guarantees the query to stay in  .55.

the value of a query which is close to 1 (and to increase the value of a query which is close
to 0). Finally, if we are increasing the parameter, then a parameter value close to .4 will
allow the biggest absolute change. But if we are decreasing the parameter, then a value
close to .6 will allow the biggest absolute change.
Now let us repeat the same exercise but assuming that the initial value of the query
is Pr (y | e) = .6, yet insisting on the same measure of robustness. Figure 8 plots the
280

fiWhen do Numbers Really Matter?

8
6

lo

4

0.8

2
0

0.6
Pr(x| u)

0.4

0.2
0.4
Pr(x| u)

0.2

0.6
0.8

Figure 9: The plot of the log-odd difference, lo = | ln(O0 (x | u))  ln(O(x | u))|, against
Pr (x | u) and Pr 0 (x | u).

lo

lo

lo

6

4

6

5

5

3

4
3

4
3

2

2

2

1

1
0.2

0.4

0.6

0.8

1

p

1
0.2

0.4

0.6

0.8

1

p

0.2

0.4

0.6

0.8

1

p

Figure 10: The plots of the log-odd difference, lo = | ln(O0 (x | u))  ln(O(x | u))|, against
the new parameter value p0 = Pr 0 (x | u). The figures correspond to different
initial values of the parameter, p = Pr (x | u) = .1, .5, .9, respectively.

permissible changes  as a function of p, the current value of the parameter. Again, the
amount of permissible change becomes smaller as the probability p approaches 0 or 1. The
other main point to emphasize is that the permissible changes are now much smaller than
in the previous example, since the initial value of the query is not as extreme. Therefore,
this query is much less robust than the previous one.
More generally, Figure 9 plots the log-odds difference, | ln(O0 (x | u))  ln(O(x | u))|,
against Pr (x | u) = p and Pr 0 (x | u) = p + , and Figure 10 shows cross-sections of Figure 9
for three different values of p. Again, the plots explain analytically why we can afford more
absolute changes to non-extreme probabilities (Pradhan et al., 1996; Poole, 1998).
281

fiChan & Darwiche

From Figure 10, we also notice that although the plot is symmetric for p = .5, it is not
for both p = .1 and p = .9, i.e. absolute changes of p and p give us different amounts
of log-odds change. For example, changing the parameter from .1 to .05 give us a larger
log-odds change than changing the parameter from .1 to .15. We also notice that the plots
for p = .1 and p = .9 are mirror images of each other. Therefore, the log-odds change is the
same for complementary parameter changes on x|u and x|u .
We close this section by emphasizing that the above figures identify parameter changes
that guarantee keeping queries within certain ranges. However, if the belief network has
specific properties, such as a specific topology, then it is possible for the query to be robust
against parameter changes that are outside the identified bounds.

5. Conclusion
In this paper, we presented an efficient technique for fine-tuning the parameters of a belief
network. The technique suggests minimal changes to network parameters which ensure
that certain constraints are enforced on probabilistic queries. Based on this technique, we
have experimented with some belief networks, only to find out that these networks are
more sensitive to parameter changes than previous experimental studies seem to suggest.
This observation leads us to an analytic study on the effect of parameter changes, with the
aim of characterizing situations under which parameter changes do or do not matter. We
have reported on a number of results in this direction. Our central result shows that belief
networks are robust in a very specific sense: the relative change in query odds is bounded
by the relative change in the parameter odds. A closer look at this result, its meaning,
and its implications provides interesting characterizations of parameter changes that do
or do not matter, and explains analytically some of the previous experimental results and
observations on this matter.

Acknowledgments
A shorter version of this paper appeared in Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence (UAI-01), pp. 6574. This work has been partially supported
by NSF grant IIS-9988543, MURI grant N00014-00-1-0617, and by DiMI grant 00-10065.

Appendix A. Proofs
Theorem 2.1 The derivative of Pr (e) with respect to the meta parameter x|u is given
by:
Pr (e, x, u) Pr (e, x, u)
Pr (e)
=

,
x|u
x|u
x|u

when x|u 6= 0 and x|u 6= 0.
282

fiWhen do Numbers Really Matter?

Proof From Russell et al. (1995), the semantics of the first derivative of Pr (e) with respect
to parameter x|u is given by:9
Pr (e)
Pr (e, x, u)
=
,
x|u
x|u
if x|u 6= 0, and:
Pr (e, x, u)
Pr (e)
=
,
x|u
x|u
if x|u 6= 0. Because x|u = x|u and x|u = 1  x|u , we have:
Pr (e)
x|u

=

Pr (e) Pr (e)

x|u
x|u

=

Pr (e, x, u) Pr (e, x, u)
,

x|u
x|u

if x|u 6= 0 and x|u 6= 0.2
Theorem 3.1 If X is a binary variable in a belief network, then:


 Pr (y | e) 
Pr (y | e)(1  Pr (y | e))


.


 x|u 
Pr (x | u)(1  Pr (x | u))

Proof From Darwiche (2000), the derivative Pr (y | e)/x|u is equal to:
Pr (y | e)
Pr (y, x, u | e)  Pr (y | e)Pr (x, u | e)
.
=
x|u
Pr (x | u)
Since:

Pr (y | e)
Pr (y | e) Pr (y | e)
,
=

x|u
x|u
x|u

we have:
Pr (y | e)
x|u
=
=

Pr (y, x, u | e)  Pr (y | e)Pr (x, u | e) Pr (y, x, u | e)  Pr (y | e)Pr (x, u | e)

Pr (x | u)
Pr (x | u)
Pr (y, x, u | e)  Pr (y | e)Pr (x, u | e)  Pr (x | u)(Pr (y, u | e)  Pr (y | e)Pr (u | e))
.
Pr (x | u)(1  Pr (x | u))

In order to find an upper bound on the derivative, we would like to bound the term
Pr (y, x, u | e)Pr (y | e)Pr (x, u | e). Since, Pr (y, x, u, e)  Pr (y, u, e) and Pr (y, x, u, e) 
Pr (x, u, e), we have:
Pr (y, x, u | e)  Pr (y | e)Pr (x, u | e)  Pr (y, x, u | e)  Pr (y | e)Pr (y, x, u | e)
= Pr (y, x, u | e)Pr (y | e)
 Pr (y, u | e)Pr (y | e).
9. We allow the notations Pr (e)/x|u and Pr (e)/x|u by assuming Pr (e) as functions of x|u and x|u ,
even though it is not allowed in belief networks to change only x|u or x|u .

283

fiChan & Darwiche

Therefore, the upper bound on the derivative is given by:
Pr (y | e)
Pr (y, u | e)Pr (y | e)  Pr (x | u)(Pr (y, u | e)  Pr (y | e)Pr (u | e))

,
x|u
Pr (x | u)(1  Pr (x | u))
which is equal to the following term:
Pr (y | e)Pr (y, u | e) Pr (y | e)Pr (y, u | e)
+
Pr (x | u)
1  Pr (x | u)
(1  Pr (x | u))Pr (y | e)Pr (y, u | e) + Pr (x | u)Pr (y | e)Pr (y, u | e)
=
Pr (x | u)(1  Pr (x | u))
Pr (y, u | e)Pr (y | e)  Pr (x | u)(P r(y, u | e)  Pr (y | e)Pr (u | e))
=
.
Pr (x | u)(1  Pr (x | u))
Since Pr (y, u | e)  Pr (y | e) and Pr (y, u | e)  Pr (y | e), the upper bound on the
derivative is given by:
Pr (y | e)
x|u



=

Pr (y | e)Pr (y, u | e) Pr (y | e)Pr (y, u | e)
+
Pr (x | u)
1  Pr (x | u)
Pr (y | e)Pr (y | e) Pr (y | e)Pr (y | e)
+
Pr (x | u)
1  Pr (x | u)
Pr (y | e)(1  Pr (y | e))
.
Pr (x | u)(1  Pr (x | u))

In order to find a lower bound on the derivative, we note that Pr (y | e) = 1  Pr (y | e),
and thus Pr (y | e)/x|u = Pr (y | e)/x|u . Therefore, we can get our lower bound by
finding the upper bound on the derivative Pr (y | e)/x|u and multiplying by 1:
Pr (y | e)
x|u

 

Pr (y | e)(1  Pr (y | e))
Pr (x | u)(1  Pr (x | u))

= 

Pr (y | e)(1  Pr (y | e))
.
Pr (x | u)(1  Pr (x | u))

Combining the upper bound and the lower bound, we have:


 Pr (y | e) 
Pr (y | e)(1  Pr (y | e))


.2


 x|u 
Pr (x | u)(1  Pr (x | u))

Theorem 3.2 Assume that x|u  .5 without loss of generality. Suppose that x|u is an
infinitesimal change applied to the meta parameter x|u , leading to a change of Pr (y | e)
to the query Pr (y | e). We then have:




  
 Pr (y | e) 
x|u




.
 Pr (y | e)   2  
x|u 

284

fiWhen do Numbers Really Matter?

Proof Because x|u is infinitesimal, from Theorem 3.1:


 Pr (y | e) 




 x|u 



 Pr (y | e) 


' 

 x|u 



Pr (y | e)(1  Pr (y | e))
.
Pr (x | u)(1  Pr (x | u))

Arranging the terms, we have:


 Pr (y | e) 


 Pr (y | e) 







1  Pr (y | e)  x|u 


1  Pr (x | u)  x|u 



1  x|u 


.5  x|u 



=





  
x|u 

2
,
 x|u 

since Pr (x | u) = x|u  .5.2
Theorem 3.3 Let O(x | u) denote the odds of x given u: O(x | u) = Pr (x | u)/(1Pr (x |
u)), and let O(y | e) denote the odds of y given e: O(y | e) = Pr (y | e)/(1  Pr (y | e)).
Let O0 (x | u) and O0 (y | e) denote these odds after having applied an arbitrary change to
the meta parameter x|u where X is a binary variable in a belief network. If the change is
positive, then:
O(x | u)
O0 (y | e)
O0 (x | u)


;
0
O (x | u)
O(y | e)
O(x | u)
or if it is negative, then:
O0 (x | u)
O0 (y | e)
O(x | u)

 0
.
O(x | u)
O(y | e)
O (x | u)
Combining both results, we have:
| ln(O0 (y | e))  ln(O(y | e))|  | ln(O0 (x | u))  ln(O(x | u))|.

Proof We obtain this result by integrating the bound in Theorem 3.1. In particular, if
0
we change x|u to x|u
> x|u , and consequently Pr (y | e) changes to Pr 0 (y | e), we can
separate the variables in the upper bound on the derivative in Theorem 3.1, integrate over
the intervals, and yield:
Z Pr 0 (y|e)
Pr (y|e)

dPr (y | e)

Pr (y | e)(1  Pr (y | e))

Z 0
x|u
x|u

dx|u
.
x|u (1  x|u )

This gives us the solution:
ln(Pr 0 (y | e))  ln(Pr (y | e))  ln(1  Pr 0 (y | e)) + ln(1  Pr (y | e))
0
0
 ln(x|u
)  ln(x|u )  ln(1  x|u
) + ln(1  x|u ),
285

fiChan & Darwiche

and after taking exponentials, we have:
0 /(1   0 )
x|u
Pr 0 (y | e)/(1  Pr 0 (y | e))
x|u

,
Pr (y | e)/(1  Pr (y | e))
x|u /(1  x|u )

which is equivalent to:

O0 (x | u)
O0 (y | e)

.
O(y | e)
O(x | u)

Similarly, we can separate the variables in the lower bound on the derivative in Theorem 3.1, integrate over the intervals, and yield:
Z Pr 0 (y|e)
Pr (y|e)

dPr (y | e)

Pr (y | e)(1  Pr (y | e))

Z 0
x|u
x|u

dx|u
.
x|u (1  x|u )

This gives us the solution:
ln(Pr 0 (y | e))  ln(Pr (y | e))  ln(1  Pr 0 (y | e)) + ln(1  Pr (y | e))
0
0
  ln(x|u
) + ln(x|u ) + ln(1  x|u
)  ln(1  x|u ),
and after taking exponentials, we have:
x|u /(1  x|u )
Pr 0 (y | e)/(1  Pr 0 (y | e))
 0
0 ),
Pr (y | e)/(1  Pr (y | e))
x|u /(1  x|u
which is equivalent to:

O0 (y | e)
O(x | u)
 0
.
O(y | e)
O (x | u)

0
Therefore, we have the following inequality if x|u
> x|u :

O(x | u)
O0 (y | e)
O0 (x | u)


.
O0 (x | u)
O(y | e)
O(x | u)
0
On the other hand, if we now change x|u to x|u
< x|u , we can instead integrate from
to x|u . The integrals will satisfy these two inequalities:

0
x|u

Z Pr (y|e)
Pr 0 (y|e)

Z Pr (y|e)
Pr 0 (y|e)

Z x|u

dPr (y | e)
Pr (y | e)(1  Pr (y | e))



dPr (y | e)
Pr (y | e)(1  Pr (y | e))

 

0
x|u

dx|u
;
x|u (1  x|u )

Z x|u
0
x|u

dx|u
.
x|u (1  x|u )

We can solve for the inequalities similarly and get the result:
O0 (x | u)
O0 (y | e)
O(x | u)

 0
.
O(x | u)
O(y | e)
O (x | u)
0
0
Combining the results for both x|u
> x|u and x|u
< x|u , we have:

| ln(O0 (y | e))  ln(O(y | e))|  | ln(O0 (x | u))  ln(O(x | u))|.2
286

fiWhen do Numbers Really Matter?

References
Castillo, E., Gutierrez, J. M., & Hadi, A. S. (1997). Sensitivity analysis in discrete Bayesian
networks. IEEE Transactions on Systems, Man, and Cybernetics, 27, 412423.
Chan, H., & Darwiche, A. (2002). A distance measure for bounding probabilistic belief
change. In Proceedings of the Eighteenth National Conference on Artificial Intelligence
(AAAI), pp. 539545.
Coupe, V. M. H., Peek, N., Ottenkamp, J., & Habbema, J. D. F. (1999). Using sensitivity analysis for efficient quantification of a belief network. Artificial Intelligence in
Medicine, 17, 223247.
Darwiche, A. (2000). A differential approach to inference in Bayesian networks. In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence (UAI), pp.
123132.
Greiner, R., Grove, A., & Schuurmans, D. (1997). Learning Bayesian nets that perform
well. In Proceedings of the 13th Conference on Uncertainty in Artificial Intelligence
(UAI), pp. 198207.
Jensen, F. V., Lauritzen, S., & Olesen, K. (1990). Bayesian updating in recursive graphical
models by local computation. Computational Statistics Quarterly, 4, 269282.
Jensen, F. V. (1999). Gradient descent training of bayesian networks. In Proceedings of the
Fifth European Conference on Symbolic and Quantitative Approaches to Reasoning
with Uncertainty (ECSQARU), pp. 190200.
Jensen, F. V. (2001). Bayesian Networks and Decision Graphs. Springer-Verlag, Inc., New
York.
Kjrulff, U., & van der Gaag, L. C. (2000). Making sensitivity analysis computationally
efficient. In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence
(UAI), pp. 317325.
Laskey, K. B. (1995). Sensitivity analysis for probability assessments in Bayesian networks.
IEEE Transactions on Systems, Man, and Cybernetics, 25, 901909.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers, Inc., San Mateo, California.
Poole, D. (1998). Context-specific approximation in probabilistic inference. In Proceedings
of the 14th Conference on Uncertainty in Artificial Intelligence (UAI), pp. 447454.
Pradhan, M., Henrion, M., Provan, G., Del Favero, B., & Huang, K. (1996). The sensitivity
of belief networks to imprecise probabilities: an experimental investigation. Artificial
Intelligence, 85, 363397.
Russell, S., Binder, J., Koller, D., & Kanazawa, K. (1995). Local learning in probabilistic
networks with hidden variables. In Proceedings of the Fourteenth International Joint
Conference on Artificial Intelligence (IJCAI), pp. 11461152.
van der Gaag, L. C., & Renooij, S. (2001). Analysing sensitivity data from probabilistic networks. In Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence
(UAI), pp. 530537.

287

fiJournal of Articial Intelligence Research 17 (2002) 83135

Submitted 10/01; published 08/02

Monitoring Teams by Overhearing:
A Multi-Agent Plan-Recognition Approach

galk@cs.biu.ac.il

Gal A. Kaminka
Computer Science Department
Bar Ilan University
Ramat Gan 52900, Israel

pynadath@isi.edu
tambe@usc.edu

David V. Pynadath
Milind Tambe
Computer Science Department and Information Sciences Institute
University of Southern California
4676 Admiralty Way
Los Angeles, CA 90292, USA

Abstract

Recent years are seeing an increasing need for on-line monitoring of teams of cooperating agents, e.g., for visualization, or performance tracking. However, in monitoring
deployed teams, we often cannot rely on the agents to always communicate their state
to the monitoring system. This paper presents a non-intrusive approach to monitoring by
overhearing, where the monitored team's state is inferred (via plan-recognition) from teammembers' routine communications, exchanged as part of their coordinated task execution,
and observed (overheard) by the monitoring system. Key challenges in this approach include the demanding run-time requirements of monitoring, the scarceness of observations
(increasing monitoring uncertainty), and the need to scale-up monitoring to address potentially large teams. To address these, we present a set of complementary novel techniques,
exploiting knowledge of the social structures and procedures in the monitored team: (i)
an ecient probabilistic plan-recognition algorithm, well-suited for processing communications as observations; (ii) an approach to exploiting knowledge of the team's social behavior to predict future observations during execution (reducing monitoring uncertainty);
and (iii) monitoring algorithms that trade expressivity for scalability, representing only
certain useful monitoring hypotheses, but allowing for any number of agents and their
dierent activities to be represented in a single coherent entity. We present an empirical
evaluation of these techniques, in combination and apart, in monitoring a deployed team
of agents, running on machines physically distributed across the country, and engaged in
complex, dynamic task execution. We also compare the performance of these techniques
to human expert and novice monitors, and show that the techniques presented are capable
of monitoring at human-expert levels, despite the diculty of the task.
1. Introduction
Recent years have seen tremendous growth of applications involving distributed multi-agent
teams, formed of agents that collaborate on a specic joint task (e.g., Jennings, 1995; Pechoucek, Marik, & Stepankova, 2000, 2001; Kumar & Cohen, 2000; Kumar, Cohen, &
Levesque, 2000; Horling, Benyo, & Lesser, 2001; Lenser, Bruce, & Veloso, 2001; Barber &
Martin, 2001). This growth has led to increasing need for monitoring techniques that allow a

c 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiKaminka, Pynadath, & Tambe
synthetic agent or human operator to monitor and identify the state of the distributed team.
Previous work has discussed the critical role of monitoring in visualization (e.g., Ndumu,
Nwana, Lee, & Collis, 1999), in identifying failures in execution (e.g., Horling et al., 2001),
in providing advice to improve performance (e.g., Aiello, Busetta, Dona, & Serani, 2001),
and in facilitating collaboration between the monitoring agent and the members of the team
(e.g., Grosz & Kraus, 1996).
This paper focuses on monitoring cooperative agent teams by overhearing their internal communications.

This allows a human operator or a synthetic agent to monitor the

coordinated execution of a task, by listening to the messages team-members exchange with
each other.

It contrasts with previous techniques that are impractical in settings where

direct observations of the team members are unavailable (e.g., when team-members are
physically distributed away from the observer), or in large-scale applications composed of

already-deployed

agents that are dynamically integrated to jointly execute a task.

For example, one common technique,

report-based monitoring, requires

each monitored

team-member to communicate its state to the monitoring agent at regular intervals, or at
least whenever the team-member changes its state. Such reporting provides the monitoring
agent with accurate information on the state of the team. Unfortunately, report-based monitoring suers from several diculties in monitoring large deployed teams of interest in the
real-world (see Section 2 for a detailed discussion): First, it requires intrusive modications
to the behavior of agents, such that they report their state as needed by the dierent monitoring applications. However, since agents are already deployed, such repeated modications
to the behavior of the agents are dicult to implement and complex to manage.

In par-

ticular, legacy and proprietary systems are notoriously expensive to modify (for instance,
consider the notorious modications to address the Year 2000 bug, also known as Y2K).
Second, the bandwidth requirements of report-based monitoring (which relies on communication channels) can be unrealistic (Jennings, 1993, 1995; Grosz & Kraus, 1996; Pechoucek
et

al., 2000, 2001; Vercouter, Beaune, & Sayettat, 2000). In addition, network delays and

unreliable or lossy communication channels are a key concern with report-based monitoring
approaches.
We therefore advocate an alternative monitoring approach, based on multi-agent keyhole
plan-recognition (Tambe, 1996; Huber & Hadley, 1997; Devaney & Ram, 1998; Intille &
Bobick, 1999; Kaminka & Tambe, 2000).

In this approach, the monitoring system infers

the unobservable state of the agents based on their observable actions, using knowledge of
the plans that give rise to the actions. This approach is non-intrusive, requiring no changes
to agents' behaviors; and it allows for changes in the requested monitoring information.
It assumes access to knowledge of plans that may explain observable actionhowever this
knowledge is readily available to the monitoring system as we assume it is deployed in a
collaborative environment. Indeed, in some cases, the monitoring system may be deployed
by the human operator of the team. An additional benet of a plan-recognition approach
is that it can rely on inference to compensate for occasional communication losses, and can
therefore be robust to communication failures.
In general, the only observable actions of agents in a distributed team are their

routine

communications, which the agents exchange as part of task execution (Ndumu et al., 1999).
Fortunately, the growing popularity of agent integration tools (Tambe, Pynadath, Chauvat,
Das, & Kaminka, 2000; Martin, Cheyer, & Moran, 1999) and agent communications (Finin,

84

fiMonitoring Teams by Overhearing
Labrou, & Mayeld, 1997; Reed, 1998) increases standardization of aspects of agent communications, and provides increasing opportunities for observing and interpreting inter-agent
communications.

We assume that monitored agents are truthful in their messages, since

they are communicating to their teammates; and that they are not attempting to deceive
the monitoring agent or prevent it from overhearing (as it is deployed by the human operator of the team).

Given a (possibly stochastic) model of the plans that the agents may

be executing, a monitoring system using plan-recognition can infer the current state of the
agents from such observed routine messages.
However, the application of plan-recognition techniques for overhearing poses signicant
challenges. First, a key characteristic of the overhearing task is the scarcity of observations.
Explanations for overheard messages (i.e., the observed actions) can sometimes be fairly easy
to disambiguate, but uncertainty arises because there are relatively few of them to observe:
team members cannot and do not in practice continuously communicate among themselves
about their state (Jennings, 1995; Grosz & Kraus, 1996). Thus team-members change their
state while keeping quiet. Another key characteristic of overhearing is that the observable
actions are inherently
agent that

listening.

multi-agent actions :

sends the messages.

When agents communicate, it is only a single

The others implicitly act their role in the communications by

Yet despite the scarcity of observable communications, and the multi-agent nature

of the observed actions, a monitoring system must infer the state of all agents in the team,
at all times. Previous investigations of multi-agent plan-recognition (Tambe, 1996; Devaney
& Ram, 1998; Intille & Bobick, 1999; Kaminka & Tambe, 2000) have typically made the
assumption that all changes to the state of agents have an observable eect: Uncertainty
resulted from ambiguity in the explanations for the observed actions. Furthermore, these
investigations have addressed settings where observable actions were individual (each action
is carried out by a single agent).
In addition to these challenges that are unique to overhearing, a monitoring system must
address additional challenges stemming from the use of monitoring in service of visualization.

The representation and algorithms must support soft real-time response; reasoning

must be done quickly to be useful for visualization.

Furthermore, real-world applications

demand techniques that can scale up as the number of agents increases, for monitoring large
teams. However, many current representations for plan-recognition are computationally intense (e.g., Kjrul, 1992), or only address single-agent recognition tasks (e.g., Pynadath
& Wellman, 2000). Multi-agent plan-recognition investigations have typically not explicitly
addressed scalability concerns (Devaney & Ram, 1998; Intille & Bobick, 1999).
This paper presents Overseer, an implemented monitoring system capable of monitoring large distributed applications composed of previously-deployed agents.

Overseer

builds on previous work in multi-agent plan-recognition (Tambe, 1996; Intille & Bobick,
1999; Kaminka & Tambe, 2000) by utilizing knowledge of the relationships between agents
to understand how their decisions interact. However, as previous techniques proved insucient, Overseer includes a number of novel multi-agent plan-recognition techniques that
address the scarcity of observations, as well as the severe response-time and scale-up requirements imposed by realistic applications. Key contributions include: (i) a

linear time

probabilistic plan-recognition representation and associated algorithms, which exploit the
nature of observed communications for eciency; (ii) a method for addressing unavailable
observations by exploiting knowledge of the

social procedures
85

of teams to eectively predict

fiKaminka, Pynadath, & Tambe
(and hence eectively monitor) future observations during normal and failed execution, thus
allowing inference from lack of such observations; and (iii) YOYO*, an algorithm that uses

team-hierarchy )

knowledge of the team organizational structure (

to model the agent team

(with all the dierent parallel activities taken by individual agents) using a single structure,
instead of modeling each agent individually. YOYO* sacrices some expressivity (the ability
to accurately monitor the team in certain coordination failure states) for signicant gains in
eciency and scalability.
We present a rigorous evaluation of Overseer's dierent monitoring techniques in one
of its application domains and show that the techniques presented result in signicant boosts
to Overseer's monitoring accuracy and eciency, beyond techniques explored in previous
work. We evaluate Overseer's capability to address lossy observations, a key concern with
report-based monitoring. Furthermore, we evaluate Overseer's performance in comparison
with human expert and novice monitors, and show that Overseer's performance is comparable to that of human experts, despite the diculty of the task, and Overseer's reliance
on computationally-simple techniques. One of the key lessons that we draw in Overseer
is that a combination of computationally-cheap multi-agent plan-recognition techniques, exploiting knowledge of the expected structures and interactions among team-members, can
be competitive with approaches which focus on accurate modeling of individual agents (and
may be computationally expensive).
This paper is organized as follows.

Section 2 presents the motivation for the design

of Overseer, using examples from an actual distributed application in which Overseer
was applied.

Section 3 presents a novel single-agent plan-recognition representation and

associated algorithms, particularly suited to monitoring an agent based on its observed
communications. Section 4 explores several methods Overseer uses to address uncertainty
in using this representation for monitoring a team of agents. Section 5 presents YOYO*,
which allows ecient reasoning using the methods previously discussed. Section 6 presents
an evaluation of the dierent techniques incorporated in YOYO*. Section 7 contrasts the
techniques presented with previous related investigations, and nally, Section 8 concludes
and presents our plans for future work. In addition, several appendices present all pseudocode for algorithms discussed in the text, and portions of the data used in our experiments,
for those readers who may wish to replicate the experiments.

2. Motivation and Illustrative Examples
Several considerations, based on our experience with actual distributed applications, have
directed us towards the plan-recognition approach we advocate in this paper. We present
these considerations in the context of an illustrative complex distributed application, which
we also use for evaluating Overseer in Section 6. In this application, a distributed team of
11 to 20 agents executes a simulation of an evacuation of civilians from a threatened location.
The integrated system allows a human commander to interactively provide locations of the
stranded civilians, safe areas for evacuation and other key points. Simulated helicopters then
y a coordinated mission to evacuate the civilians, relying on various information agents to
dynamically obtain information about enemy threats, (re)plan routes to avoid threats and
obstacles, etc. The distributed team is composed of diverse agents from four dierent research groups: A Quickset multi-modal command input agent (Cohen, Johnston, McGee,

86

fiMonitoring Teams by Overhearing
Oviatt, Pittman, Smith, Chen, & Clow, 1997), a Retsina route planner (Payne, Sycara,
Lewis, Lenox, & Hahn, 2000), the Ariadne information agent (Knoblock, Minton, Ambite, Ashish, Modi, Muslea, Philpot, & Tejada, 1998) and eight synthetic helicopter pilots
(Tambe, Johnson, Jones, Koss, Laird, Rosenbloom, & Schwamb, 1995).
The agents were not designed to work together on this taskthey were already built
and deployed prior to the creation of the team. The team is integrated using Teamcore
(Tambe et

al., 2000), which accomplishes integration by wrapping each agent with a

proxy that maintains collaboration with other agents (via their own proxies). The proxies
and agents form a team, jointly executing a distributed application described by a

oriented program.

team-

Such a program consists of:



A team hierarchy, where a team decomposes into subteams, and sub-subteams.



A plan hierarchy, which contains team plans that decompose into subteam plans



Assignment of teams from the team hierarchy to plans in the plan hierarchy.

As an example, Figure 1-a shows a part of the team/subteam hierarchy used in the
evacuation-domain (described below).

Here, for instance, TRANSPORT is a subteam of

FLIGHT-TEAM, itself a subteam of TASK-FORCE. Figure 1-b shows an abbreviated planhierarchy for the same domain.

High-level team plans, such as

compose into other team plans, such as

Process-Orders,

Evacuate,

typically de-

and, ultimately, into leaf-level

plans that are executed by individuals. Temporal transitions are used to constrain the order of execution of plans. There are teams assigned to execute the plans, e.g., the TASK
FORCE team jointly executes Evacuate, while only the TRANSPORT subteam executes
the Transport-Operations (Transport-Ops) step.

The team-oriented program for

this application consists of about 40 team-plans. Some plans may get executed repeatedly
though, so each agent may execute up to hundreds of plan steps as part of the execution of
a single team-oriented program.
To execute the team-oriented program, each proxy uses a domain-independent teamwork
model, called STEAM (Tambe, 1997).

The teamwork model automatically generates the

communication messages required to ensure appropriate coordination among the proxies.
For instance, STEAM requires that if an agent privately obtains a belief

bel

that terminates

a team plan, then that agent should send a message to the rest of the team to terminate that
team plan, along with the private belief

bel

that led to that termination. To avoid jamming

the communication channels with a ood of messages about every single plan, STEAM
chooses to communicate selectively. Thus, whereas communicating about the initiation and
termination of each and every plan would have led to 2000 or more messages generated in
one run, only about 100 messages get exchanged in any one run when using STEAM (Tambe
et

al., 2000).
Figure 2 displays some of the messages exchanged among team members in the evac-

uation application, through the use of STEAM. The rst message is sent from a proxy
called teamquickset

to members of a team TEAM-EVAC (another name for TASK

FORCE). The content of this message indicates that the team should terminate a plan
called

determine-number-of-helos.

The second message is sent from a proxy called

87

fiKaminka, Pynadath, & Tambe

TASK FORCE

EVACUATE [TASK FORCE]

.....
GET ORDERS
ROLE

FLIGHT
TEAM

ESCORT

ROUTE
PLANNER

TRANSPORT

ESCORT ESCORT

TRANSPORT ...

LEAD

DIVISION 1

FOLLOW

PROCESS
ORDERS
[TASK FORCE]

EXECUTE
MISSION
[TASK FORCE]

LANDING
ZONE
MANEUVERS
[FLIGHT TEAM]

FLY-FLIGHT
PLAN
GET
ORDERS
[FLIGHT TEAM]
[GET ORDERS]
FLY-CONTROL
ROUTE....
[FLIGHT TEAM]

(a)

.....

....
ESCORT
TRANSPORT
OPERATIONS OPERATIONS
[ESCORT]
[TRANSPORT]

(b)

Figure 1: Portions of the team-hierarchy (a) and plan-hierarchy (b) used in our domain.
Dotted lines show temporal transitions.

team_auto2 to members of a subteam TEAM-ESCORT-FOLLOW (a subteam of ESCORTS). The content of this message indicates that the subteam should establish commitment to a plan named

prepare-to-execute-mission.

The online appendix presents sample

logs of the overheard messages from complete runs, as well as the plan and team hierarchies
for the evacuation application.
As discussed in Section 1, the capability for automatically monitoring the progress of
the team is critical.

This need for team monitoring is further amplied in distributed

settings, since a human operator in one place cannot directly observe the agents executing
in a remote location.

For instance, in trial runs of the evacuation simulation application

described above, monitoring sometimes required a series of frantic phone calls among human
operators in dierent states, trying to verify the successful execution of the system as it was
operating.

And even when this agent team was co-located on multiple computers in one

room, the diversity of agents made it extremely dicult for an observer to automatically
monitor the state of the team just from observing the dierent agent output screens.
Overseer was built to provide such monitoring by tracking the routine communica-

tions among the agents (Figure 2).

Using plan-recognition, it allows humans and agents

to query about the present and future likely plans of the entire team, its subteams and
individualsto monitor progress, compute likelihoods of failure, etc. However, given that
the agent team communicates selectively about the plans being executed, Overseer's planrecognition faces signicant uncertainty. Furthermore, Overseer must be able to answer
queries on-line, and must therefore work eciently.

As discussed later, addressing these

challenges has required several novel team-based plan-recognition techniques to be developed.
Several considerations have led us away from report-based monitoring for this and other
Teamcore applications. First, report-based monitoring requires that agents' code be mod-

ied to communicate the reports needed for monitoring; as monitoring requirements change

88

fiMonitoring Teams by Overhearing

Log Message Received; Fri Sep 17 18:27:54 1999:
Logging Agent: teamquickset
Message==> tell
:content teamquickset terminate-jpg constant determine-number-of-helos
number-of-helos-determined *yes* 4 4 98 kqml_string
:receiver TEAM-EVAC 9 kqml_word
:reply-with nil 3 kqml_word
:team TEAM-EVAC 9 kqml_word
:sender teamquickset 12 kqml_word
:kqml-msg-id 21547+tsevet.isi.edu+7 22 kqml_word
Log Message Received; Fri Sep 17 18:30:35 1999:
Logging Agent: TEAM_auto2
Message==> tell
:content TEAM_auto2 establish-commitment prepare-to-execute-mission
58 kqml_string
:receiver TEAM-ESCORT-FOLLOW 18 kqml_word
:reply-with nil 3 kqml_word
:team TEAM-ESCORT-FOLLOW 18 kqml_word
:sender TEAM_auto2 10 kqml_word
:kqml-msg-id 20752+dui.isi.edu+16 20 kqml_word
Figure 2: Example KQML messages used as observations by Overseer.

89

fiKaminka, Pynadath, & Tambe
from one application to the next, so does the information needed about each agent.

Un-

fortunately, the agents and their proxies are already deployed in several government laboratories and universities.

Modifying the agents at each deployed location is problematic

and intrusivemodications interfere with carefully designed timing specications of given
tasks, requiring further modications by other agent developers. The distributed nature of
Teamcore implies that there is no centralized server which controls the behavior of the

agents, but instead changes are required in the dierent proxy types.

Indeed, in general,

modifying legacy and proprietary applications (including the integration architecture) is of
course known to be a dicult process, and so a solution that requires constant modications
to the agents and architecture will not scale up.
A second important consideration was the computational and bandwidth requirements of
report-based monitoring. As has been repeatedly noted in the literature, one cannot expect
agents to be able to communicate continuously and fully monitor all other agents (e.g.,
Jennings, 1993, 1995; Grosz & Kraus, 1996; Pechoucek et al., 2001; Vercouter et al., 2000).
In a team of 11 (used as an example in this paper), regularly scheduled state reports from the
agents at the required temporal resolution would require approximately 50,000 messages to
be sent during a 15-minute run, with the number nearly doubling when we reach 20 agents.
If we instead have the 11 agents only report on state changes, announcing plan initiation
and termination, approximately 2,000 messages have to be sent.

However, this is still an

order-of-magnitude more than the normal 100 messages or so that are exchanged by the
11 agents as part of routine execution. Even if the network could support the bandwidth
necessary for report-based monitoring, there is also a signicant computational burden on
the monitoring system to process all the incoming reports.
On the other hand, a plan-recognition approach seemed like a natural t for the task.
First, it doesn't require any changes in the behavior of the monitored agents, and is thus
very suitable for monitoring agents that are already deployed. Second, it doesn't add any
computational burdens to the monitored agents or the network, since it uses only what
observations are already available. Third, the main knowledge source plan-recognition systems typically rely ona plan libraryis in fact easily available in accessible form to the
monitoring system from the team-oriented program which is used to integrate the agents,
since the operator deploying the monitoring system is assumed to be the one to describe
the integration team-oriented program in the rst place. Thus plan-recognition's sometimes
criticized assumption of a correct plan-library is in fact satised fully in this monitoring
application.
Note that this assumption holds even if agents are not all using the same integration architecture: The only knowledge we rely on is a (possibly stochastic) model of how
components of execution t together, and the communications that are used to integrate
them. Therefore, while this paper focuses on team-oriented programs (described above), the
techniques introduced appear generalizable to other types of representation languages for
distributed systems, such as TMS (Decker, 1995), team-oriented programming (Tidhar,
1993a) and others. Furthermore, the plan-library need not contain implemetation details
only the names of the key steps. Thus even agents utilizing radically-dierent representations
than a plan-hierarchy can be monitored, as long as they have execution states corresponding
to the team-oriented program (which they have to have in any case in order to coordinate
with other team-members).

90

fiMonitoring Teams by Overhearing
Monitoring by overhearing poses unique challenges as previously discussed. However, it
also oers unique opportunities for plan recognition. We had earlier stated our assumption
that agents are truthful in their communications, and do not seek to deceive their teammates
or the monitoring system, nor prevent overhearing in any way (e.g., encryption).

This

assumption is justied as the monitoring system is deployed by the operator of the monitored
agents, or by an agent team-member. Failures of the team to coordinate (e.g., due to clock
asynchrony or unintentional erroneous messages) will therefore cause corresponding failures
in monitoring. However, we do not make additional assumptions about the messages beyond
those that are made by the monitored agents themselves.
This assumption allows a plan-recognition system to treat observations with certainty:
When a message is overheard terminating plan
certainty that indeed the plan
plan recognition ambiguity.

X

X,

the monitoring system can infer with

is no longer executed. However, this does not eliminate

First, multiple instantiations of plan

X

may exist, and the

message does not specify which one was terminated. Second, upon termination of the plan,
the monitored team-member must often choose between multiple alternative plan steps to
follow

X,

and yet this choice is not evident in the observations.

Indeed, the diculty

of monitoring by overhearing is demonstrated by human monitoring performance: Novice
human monitors have managed to only achieve approximately 60% accuracy on average.

3. Monitoring a Team of Agents as Separate Individuals
In this section, we present a representation and associated baseline algorithms to support
overhearing based on the plan-hierarchy and team-hierarchy.

We begin by making an as-

sumption of agent independence, where observations and beliefs about one agent's state of
execution have no bearing on our beliefs about another agent's state. This assumption can
be contrasted with another: If we assume instead that team-members are successful in their
coordination, then knowing that one agent has begun executing a joint plan would naturally increase the likelihood that its teammates have begun as well, as agents would not be
considered independent. In fact, successful teamwork

requires

interdependency among the

agents (Grosz, 1996).
However, an initial assumption of agent independence provides a baseline of comparison,
as it more closely follows current approaches to multi-agent plan recognition, which often
assume that observations about each individual agent are continuously available.

Later

sections (Sections 4 and 5) will highlight the unique challenges tackled in monitoring by
overhearing, and will take agent interdependencies into account.
We thus begin by maintaining a separate plan recognizer for each agent.

Each recog-

nizer observes only those messages that its respective agent sends. On the basis of these
observations, the recognizer maintains a probabilistic estimate of the state of execution of
the various plans the agent may be currently executing. Knowledge of the plans assigned to
agents and their team memberships is available in our application from the plan-hierarchy
and team-hierarchy of the team-oriented program used in constructing the monitored application.
Section 3.1 presents the language we use for the probabilistic representation of a teamoriented program.

We exploit various independence properties within team-oriented pro-

grams to achieve a compact representation of the possible plan states of the agents. Sec-

91

fiKaminka, Pynadath, & Tambe
tion 3.2 presents an algorithm for updating the recognizer's beliefs about the agents' plan
states upon the observation of a message.

This algorithm performs the update with an

eciency gained by exploiting the particular semantics of communicated messages, namely
that each such message is an observation that indicates the initiation/termination of a par-

with certainty.

ticular plan

Section 3.3 presents an algorithm for updating the recognizer's

beliefs about the agents' plan states when

no

message has been observed. In the absence

of any such evidence, this algorithm eciently updates the recognizer's beliefs by using a
temporal model of the agents' plan execution that makes a strong Markovian assumption.
Finally, Section 3.4 presents the overall recognition procedure, as well as an illustration and
complexity analysis of that procedure.

3.1 Plan-State Representation
We address uncertainty in monitoring through a probabilistic model that supports quantitative evaluation of the recognized plan hypotheses. Since we are monitoring these agents
through the duration of their execution, we use a time series of plan-state variables.

At

each point in time, the agent's plan state is the state of the team-oriented program that it
is currently executing, i.e., a path from root to leaf in the team-oriented program tree. We
represent the plans in the program by a set of boolean random variables,
variable

Xt

represent our beliefs about the agent's actual state at time
all variables

fXt g.

X

fXt g, where each

t. We then
t as a probability distribution over

is true if and only if the agent is actively executing plan

at time

The distribution takes into account dependencies among the dierent

plans in the team-oriented program (e.g., parent-child relationships), as well as the temporal dependencies between the plan state at times

t

and

t + 1.

To simplify the dependency

done(X; t), that are
1 and its execution has terminated at

structure, it is useful to introduce additional boolean random variables,
true if and only if plan
time

t.

X

was executed at time

t

There are a number of possible representations for capturing the distribution and performing inference over these variables.

However, the generality of the plan hierarchy, the

dynamic nature of the domain, and the requirements of the task eliminate most existing approaches from consideration. For instance, we could potentially generate a DBNDynamic
Belief Network (Kjrul, 1992)to represent the probabilistic distribution over the plan
variables. To do so, we include nodes representing all of the plan variables,
representing

done(X; t).

Xt ,

as well as

The links among these nodes represent the structure of the plan

hierarchy (e.g., parent-child relationships, temporal constraints), and we can ll in the conditional probability tables accordingly. We also represent the temporal progress of the team
by including nodes for the variables at the next time slice,
nodes to the

Xt+1

on those links.

Xt+1 .

We add links from the

Xt

nodes and represent the dynamics in the conditional probability tables

For each transition from a node

Xt

to a node

Yt+1 (X

6= Y ), we would

also add binary nodes indicating the observation of a message along that transition. Thus,
for a plan hierarchy with

O (4M

M

plan nodes, the corresponding DBN representation will have

+ M 2 ) = O(M 2 ) binary random variables.

The standard DBN inference algorithms maintain a belief state,
posterior probability distribution over the variables in time slice,

t,

bt ,

representing the

conditioned on all of

the observations made so far (from time 0t). These inference algorithms can update the

92

fiMonitoring Teams by Overhearing
belief state to incorporate new evidence about any variables,
the next time-tick's belief state,

bt+1 .

Xt ,

and they can also compute

We can extract the desired probability over plan-

state variables by examining the posterior probabilities stored in

bt .

Given the dependency

structure of our plan model, the space and time complexity of performing inference using
M 2 for a
this DBN (either incorporating a single observation, or computing bt+1 ) is O

(2 )

single agent.
This DBN method is not suciently ecient to support on-line monitoring in real-world
domains, since on each and every time step, the recognizer must perform an inferential
step of exponential computational complexity.

There exist

single-agent

plan-recognition

techniques that avoid the exponential complexity of DBNs by using a representation and
inference algorithms aimed at the particular properties of the plan-recognition task (e.g.,
Pynadath & Wellman, 2000).

Such specialized representations avoid the full generality

of DBNs, while still capturing a broad class of interesting planning agent models.

Given

a specialized representation, the single-agent plan-recognition algorithms can exploit the
particular structure of the plan models to achieve ecient online inference.
Drawing our inspiration from the success of this work in single-agent domains, we adopt
a similar methodology in our multi-agent domain.

In other words, we have developed a

novel plan-recognition representation more suited to capturing team-oriented programs. The
structural assumptions we make in this representation support ecient inference with our
specialized algorithms, as well as more naturally supporting an extension to represent interagent dependencies (as discussed in Section 4).
We represent the team-oriented plan as a directed graph, whose vertices are plans, and
whose edges signify temporal and hierarchical decomposition transitions between plans:
Children edges denote hierarchical decomposition of a plan into sub-plans.

Sibling edges

denote temporal orderings between plans. Following the structure of the plan hierarchy, the
variables

fXt g form a directed connected graph, such that each node Xt has at most one

hierarchical-decomposition incoming transition from a parent node (representing its parent
plan), and any number of temporal incoming transitions from plans that precede it in order

of execution. The graph may contain multiple nodes for a single plan, if the plan is the potential child of multiple parent plans. The node may have any number of temporal outgoing
transitions to immediate successor sibling nodes (representing plans that may follow it in
order of execution), and any number of hierarchical-decomposition outgoing transitions to
the node's
plan

Xt .

rst

children (i.e., those that will be executed rst by a decomposition of the

The graph forms a tree along hierarchical decomposition transitions, so that no

plan can have itself as a descendent. On the other hand, there may be cycles along temporal
transitions (to siblings). In other words, a plan may have an outgoing temporal transition
to itself (meaning that it can be selected for execution again upon termination), or to a
node that has a temporal path leading back to the plan (meaning that it is the rst node
in a temporal sequence of plans that may be executed repeatedly). It may also have two
alternative temporal paths leading indirectly from one node to another.
To perform inference with this representation, we borrow the standard DBN inference
algorithms' notion of a belief state,

bt .

As in the DBN case, the belief state represents the

posterior probability distribution over the variables in time slice, t, conditioned on all of the
observations made so far. In addition, for each plan, we distinguish between a state of actual
execution and a

blocked

state, indicating that execution has terminated, but execution of

93

fiKaminka, Pynadath, & Tambe
a successor has not yet begun (perhaps because the agent is in the process of sending a
message).

bt (X; block )

Thus,

is our belief that

X

has terminated, but the agent has not

bt (X; :block ) is then our belief at time t that the monitored
X , which has not yet terminated. More precisely, we dene
Pr(Xt ; done(X; t + 1)jE ) and bt (X; :block)  Pr(Xt ; :done(X; t + 1)jE ),

begun execution of a successor;
agent is currently executing


E again denotes all of the evidence we have received so far. If the recognizer observes

bt (X; block )
where

a message from an agent at time

t,

it updates its previous belief state,

bt+1 ,

the evidence into its new belief state,

bt+1 ,

by incorporating

according to the method described in Section

3.2. If it does not observe a message from an agent at time
new belief state,

bt ,

t,

it propagates belief into its

using the method described in Section 3.3 to simulate plan execution

over time.

3.2 Belief Update with Observed Message
While observing team communications, the recognizer can expect to occasionally receive
evidence in the form of messages (sent by an individual agent member) that identify either
plan initiation or termination.

In incorporating this evidence, we exploit the assumption

that the agents are truthful in their messages. In other words, if we observe an initiation
message for a plan,

X,

at time

t,

a termination message for a plan,

then

X,

Xt

is true with certainty.

at time

t,

then

Likewise, if we observe

done(X; t + 1)

is true with certainty.

More precisely, the algorithms presented in this section are specialized to exploit the prop-


, either Pr(Xt j
; E ) = 1 or
Pr(done(X; t)j
; E ) = 1, for any possible previously observed evidence, E .
erty of observed communications, where for any observation

Though messages are assumed truthful, there still remains ambiguity.
message uniquely species the relevant

First, while a

plan, it does not uniquely specify the relevant node.

In other words, the recognizer is still unsure about which particular
refers to, since the graph may contain multiple

Xt

Xt

node the message

nodes consistent with the message. Fur-

thermore, when a message announces termination of a plan (even with no ambiguity about
the corresponding node), there still remains ambiguity about the next plan selected by the
agent.
The observations available in the overhearing tasks of immediate interest to us fall into
this level of ambiguity. In our evacuation scenario example, there are two nodes corresponding to the plan

land-troops,

because there is one instance of

land-troops

for picking up

the people to be transported and another for dropping them o. If the recognizer observes
a message indicating that an agent has initiated execution of

land-troops,

then there is

ambiguity about which of the two instances is currently relevant. Furthermore, there may
exist ambiguity about which plan the agent will select after terminating

land-troops.

Algorithm 1 presents the pseudo-code for the complete procedure for incorporating evidence from observations.

Incorporating Evidence of an Observed Initiation Message (lines 38)

Suppose

t, we have observed a message, msg, that corresponds to initiation. If only one
X , is consistent with msg, then we know, with certainty, that the agent is executing X ,

that, at time
plan,

regardless of whatever evidence we have previously observed. Therefore, we can simply set
our belief that

Xt

is true to be 1.0. If multiple plans are consistent with

msg, we distribute

the unit probability over each consistent plan, weighted by our prior belief in seeing the given

94

fiMonitoring Teams by Overhearing
Algorithm 1 Incorporate-Evidence(msg m, beliefs b, plans M )
0
1: Initialize distributions b ; bt+1
0:0 for all plans in M
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

plans X 2 M consistent with m do
m is an initiation message then
for all plans W that precede X do
b0 (X; :block )
b0 (X; :block ) + bt (W; block )wx wx
else {m is a termination message}
for all plans Y 2 M that succeed X do
b0 (Y; :block )
b0 (Y; :block ) + bt (X; block )xy xy
Normalize distribution b0
for all plans X 2 M with b0 > 0 do
bt+1 (X; :block )
b0 (X; :block )
(X; b0(X; :block ); b; M )
tmp
X
while parent(tmp) 6= null do
bt+1 (parent(tmp); :block )
bt+1 (parent(tmp); :block ) + bt+1 (tmp; :block )
tmp
parent(tmp)
for all
if

Propagate-Down

message. This prior belief depends on all predecessor plans of

X

that may have terminated

prior to seeing this message.
To support the computation of the beliefs over transitions from predecessor plans to
successors, as well as the beliefs of seeing a message for a given transition, Overseer stores
two parameters:



and

.

The former is the probability of entering a successor plan,

given that predecessor plan,

W,

has just completed:

wx

X,

 Pr(Xt jWt; done(W; t + 1)).
+1

The latter is the probability of seeing a message, given that the agent took the specied
transition:

wx

 Pr(msgt jWt ; done(W; t + 1); Xt ).
+1

We can use previous runs to acquire

 and , by producing a frequency count over transitions
runs (see Section 4.2 for more discussion of the use of  in

suitable values for these parameters,
and messages seen during those
Overseer).

msg, at time t, we wish to
distribute the unit probability over all plans, X , (in the unblocked state) that are consistent
with msg. We can derive our new belief in plan X at time t + 1 as follows:
Therefore, given the observation of an initiation message,

msg; Xt+1 jE )
Pr(Xt+1 jmsg; E ) = Pr(Pr(
msg jE )
The denominator is simply a normalization factor, and it is the same for all candidate plans,

X.

Therefore, we ignore it in this derivation, and focus on only the numerator, which we

X Pr(
X
+ Pr(

can expand over all possible predecessor plans,

/

W,

and possible termination states of

msg; Xt+1 ; Wt ; done(W; t + 1)jE )

W

W

msg; Xt+1 ; Wt ; :done(W; t + 1)jE )

95

W:

fiKaminka, Pynadath, & Tambe
The second term is 0, since we cannot proceed from

W

to

X

if

W

has

not

terminated.

In the second term, we can expand the joint probability into its component conditional
probabilities:

/

X[Pr(
W

msg jWt ; done(W; t + 1); Xt+1 ; E )

 Pr(Xt jWt; done(W; t + 1); E )
+1

 Pr(Wt ; done(W; t + 1)jE )]
We assume that the probability of sending a message and the distribution over plan transitions obey a Markov property, so that they are independent of the plan history before
time

t,

given the current plan at time

t.

Thus, the rst two conditional probabilities are

independent of our previous history of observations. The third is exactly our previous belief
that

W

is blocked:

/

X[Pr(
W

msg jWt ; done(W; t + 1); Xt+1 ) Pr(Xt+1 jWt ; done(W; t + 1))

 bt(W; block)]

X

The rst two conditional probabilities are exactly our parameters,

/

W



and

:

wx wx bt (W; block )

(1)

Lines 45 of Algorithm 1 perform exactly the derived summation of Equation 1 (the
normalization step is carried out on line 9 (see below). A similar procedure is followed when
a message is observed indicating the termination of
that the agent was executing
successor. Thus, for each of

X

(lines 68). In such a case, we know

X in the previous time step but that it has moved
X 's potential successor plans Y , we set our belief

on to some
in

Y

to be

proportional to a transition probability, similar to that for the initiation message:

msg; Yt+1 jE )
Pr(Yt+1 jmsg; E ) = Pr(Pr(
msg jE )
The denominator is again a normalization factor that we ignore. We can expand the numerator over possible states of

X 's

execution:

/ Pr(msg; Yt

+1

; Xt ; done(X; t + 1)jE )

+ Pr(msg; Yt+1 ; :Xt ; done(X; t + 1)jE )
+ Pr(msg; Yt+1 ; Xt ; :done(X; t + 1)jE )
+ Pr(msg; Yt+1 ; :Xt ; :done(X; t + 1)jE )
96

fiMonitoring Teams by Overhearing
Only the rst term is nonzero, since the others correspond to states of execution that are
inconsistent with the observed message:

/ Pr(msg; Yt

+1

; Xt ; done(X; t + 1)jE )

We can rewrite this joint probability as a product of conditional probabilities:

/ Pr(msgjXt ; done(X; t + 1); Yt ; E )
 Pr(Yt jXt ; done(X; t + 1); E )
 Pr(Xt ; done(X; t + 1)jE )
+1

+1

We again use our Markovian assumptions to simplify the conditional probabilities, and we
rewrite the third probability using our belief state:

/ Pr(msgjXt ; done(X; t + 1); Yt ) Pr(Yt jXt ; done(X; t + 1))
 bt (X; block)
+1

+1

Finally, we rewrite the rst two conditional probabilities using our parameters,



and

/xy xy bt (X; block)

:
(2)

Lines 78 of Algorithm 1 perform exactly the derived summation of Equation 2.

Normalization of the sum (line 9).

Line 9 normalizes the sum to recapture a well-

formed probability distribution. Note that the normalization step must take into account
the fact that evidence may be incorporated for plan steps where one is an ancestor of
anotherin which case the evidence for the ancestor plan is probabilistically redundant.
The more specic evidence (for the descendent plan) will be more useful for visualization,
as it is more accurate.

Propagation of Evidence (lines 1016)

Finally, the recalculated beliefs are set (line

11) and then the changes are recursively propagated down the decomposition hierarchy to
the plan's children (line 12), via the call to Algorithm 2. In addition, the recalculated beliefs
are propagated up to the plan's ancestors in the decomposition hierarchy (lines 1316), since
evidence of a child plan being active is evidence of its parent being active as well. We assume
here that we have no knowledge about the relative likelihood of the child plans, so we treat
each as equally likely.

If we had additional knowledge about these likelihoods, we could

easily exploit it in our Propagate-Down algorithm.

Algorithm 2 Propagate-Down(plan Y , probability , beliefs b, plans M )
1:
2:
3:
4:
5:

fc j c 2 M; c rst child of Y g
= j C j
for all plans c 2 C do
bt (Y; :block )
bt (Y; :block ) + 0
C
0

+1

+1

Propagate-Down(c; 0; b; M )

97

fiKaminka, Pynadath, & Tambe
3.3 Belief Update with No Observation
In overhearing tasks, there is a great deal of uncertainty about when agents complete the
execution of their plan steps, since agents do not necessarily send messages upon every
termination or initiation of a plan. Therefore, if no messages are observed at time t, then the

t +1 must be calculated based on the possibility that the agents may

system's beliefs for time

have initiated or terminated plans without sending any messages. To support the necessary
belief update, we need a model of plan execution that provides us with a probability of plan
termination over time (i.e.,

Pr(done(X; t))).

In principle, this probability distribution can

be arbitrarily complex, and its structure may vary enormously from domain to domain, and
even from plan to plan within the same domain. In some domains, obtaining an accurate
model of this distribution requires complex knowledge acquisition from domain experts or
else a complex learning process on the part of the agent. In addition, an accurate model
may be too complex to support ecient online inference.
Overseer instead uses a temporal model that supports both ecient inference and

simple parameter estimation procedures. Overseer models the duration of a (leaf ) plan,

X , as an exponential random variable. In other words, the probability of the plan completing
 time units increases as 1 e X . The single parameter, X , corresponds
to 1/(mean duration of X ), which we can easily acquire from domain experts or previous
execution within

runs. As for inference, the exponential random variable has a Markovian property, in that
the probability of the plan's completion between times

t

Pr(done(X; t + 1)jXt )  1
independent

of how long the agent has been executing

and

t+1

is

e x ;

X

before time t. This strong assump-

tion may not fully hold in some real-world domains, but it is often a good approximation.
Also, the error associated with this approximation may be acceptable, given the enormous
gain in inferential eciency (as we show in the remainder of this section).
These eciency gains manifest themselves when Overseer rolls the model forward
in time to compute its belief state for the next time slice. Given the exponential random
variable as a model of plan duration, the probability of completion of a leaf plan is a constant,

1

e x ,

for each plan

X.

For plans with children, the probability of completion is exactly

the probability of completion of its last child (according to the temporal ordering of the
children).
Having computed the probability of plan termination, Overseer then evaluates which
plan the agent may execute next. It examines the possible successors and, for each, computes the probability of taking the corresponding transition, conditioned on the fact that no

1

xy ),

message was observed (

and on the prior probability of taking this message (xy ).

Again, as mentioned in Section 3.2, Overseer makes a Markovian assumption that the
plan history before time

t

does not aect the likelihood of the various transitions. Given

this assumption, it can combine the two parameters,

98



and

,

to get the desired conditional

fiMonitoring Teams by Overhearing
probability of the transition, given that we observed no message:

Pr(Yt+1 jXt ; done(X; t + 1); :msgt )
(X; t + 1); Yt+1 ) Pr(Yt+1 jXt ; done(X; t + 1))
= Pr(:msgt jXt ; done
Pr(:msgt jXt ; done(X; t + 1))
(1 xy )xy
=
Pr(:msgt jXt ; done(X; t + 1); Zt+1 ) Pr(Zt+1 jXt ; done(X; t + 1))

X
(1
=X
(1
Z

Z

= (1

xy )xy

xz )xz

xy )xy
X

(3)

The normalizing denominator,
sors,

Y,

X ,

is the sum of the numerator over all possible succes-

which we can pre-compute o-line. We can use the value of

likelihood that the agent will send a message upon terminating plan
special case when

X

require

X

= 0, Equation 3 is not well-dened, as all

X to determine
X at time t. In

the
the

possible transitions from

a message. In this case, the agent cannot have begun execution of any successor,

even though it has completed execution of

X . X

our belief that the agent is no longer executing
message (i.e., it is in a blocked state).
agent is executing one of

X 's

is therefore the probability mass signifying

X

at time

t + 1,

and is not waiting for a

In other words, it is our increased belief that the

immediate successors at time

t + 1,

given that we have seen

no message.
Algorithm 3 presents the pseudo-code for the process of propagating the probabilities
forward in time when a message is not observed. First, it initializes all the values to 0 (lines
15).

The process continues by going over all plans

X

2 M,

in post-order we

explore

children plans (i.e., plans reachable by hierarchical decomposition transitions) before their
parents, and sibling plans in order of execution. For each plan, the algorithm executes four
stages: (1) It determines the plan's outgoing probabilities (lines 710); (2) it determines

x ,

the outgoing probability mass that is propagated along the outgoing temporal transitions
without being blocked by waiting for a message (lines 1112); (3) it propagates

x

along

the non-blocked temporal outgoing transitions (lines 1320); and nally (4) it computes our
belief that the agent will execute the plan at the next time-tick

bt+1 (X; :block )

or will be

blocking (lines 2122). The remainder of this section explains these four stages in detail.

Calculating the outgoing probability outx (lines 710).

outx

In Algorithm 3, the variable

represents the total temporal outgoing probability from plan,

X,

given our belief that

X at time t. If a plan X is a leaf, then we derive its temporal
outx , from the temporal model discussed previously, given our belief
that the agent is currently executing X (lines 78). If X is a parent, lines 910 are, in fact,
redundant: They serve only to remind the reader that for a parent, Y , outy follows from
Y 's children when they execute line 20. This depends critically on the post-order traversal
of the plan-hierarchy: the outgoing probability of a parent Y is derived from the outgoing
the agent was executing
outgoing probability,

probabilities of its last hierarchical-decomposition children, and thus all children's outgoing
probabilities must be calculated before their parents'.

99

fiKaminka, Pynadath, & Tambe
Algorithm 3 Propagate-Forward(beliefs b, plans M )
plans X 2 M do
bt+1 (X; :block )
0:0
bt+1 (X; block )
0:0
outx
0:0
x
0:0
for all plans X 2 M in post-order
for all

1:
2:
3:
4:
5:
6:

if

7:
8:

X is a leaf

bt (X; :block )(1

outx

else

9:

then

{X is a parent}

do

{children in temporal order before parents}

e x ) {calculate probability of X terminating at time t}

outx is known { because post-order guarantees all children set it in line 20}
for all temporal outgoing transitions Tx!y from X do
x
x + (1 xy )xy
if x > 0 then {some transition can be taken}
for all temporal outgoing transitions Tx!y from X do

outx (1 xy )xy
if Tx!y leads to a successor plan Y then
bt+1 (Y; :block )
bt+1 (Y; :block ) + 
(Y; ; b; M )
else {Tx!y is a terminating transition}
outparent(x)
outparent(x) + (1 xy )xy {parent's outgoing probability is its chil-

10:
11:
12:
13:
14:
15:
16:
17:

Propagate-Down

18:
19:
20:

dren's}

bt+1 (X; block )
bt+1 (X; block ) + outx x
bt+1 (X; :block )
bt+1 (X; :block ) outx

21:
22:

Determining the non-blocked outgoing probability x (lines 1112).
ability,

x

is the sum over all possible values of the numerator in Equation 3 (i.e., over all

temporal outgoing transitions originating in
in line 21,

X;

The prob-

x

X ),

as illustrated in the derivation. As we see

is critical for calculating the belief that the agent has terminated execution of

but has not yet begun execution of a successor (i.e., the belief

bt+1 (X; block )

that the

agent is blocking).

Propagating x along temporal outgoing transitions (lines 1320).
key component in the propagation. For every temporal outgoing transition
seer calculates

,

This is the

Tx!y ,

Overseer's belief in the joint event of (i) the agent having completed execution of

the agent taking the transition

TX !Y ,

observable message. The calculation of



X,

(ii)

and (iii) the agent doing so without sending out an



is derived as follows:

= Probability that X is done ^ no message was observed ^ agent chose Tx!y
= Pr(done(X; t)jXt ) Pr(:msgjXt ; done(X; t)) Pr(Yt+1 jXt ; done(X; t); :msgt )
= outx  x  (1 xy )xy
x
= outx  (1 xy )xy
(4)

If the transition

Y 's

Over-

a temporary variable that holds the probability mass corresponding to

Tx!y

future state (at time

leads to a successor plan

t

+ 1)

Y

(lines 1618), then

as temporal incoming probability.



is added to

Since decomposition

is assumed to be immediate, this incoming probability is propagated (added) to

100

Y 's

rst

fiMonitoring Teams by Overhearing
children (Algorithm 2). If there are multiple rst children, then they denote alternative plan
decompositions for a single agent, and we compute the probability over them by dividing
the probability incoming to the parent among them. If any children have rst child plans
of their own, we distribute this new incoming probability in turn, using the same method.
Only in the next time-step does the algorithm propagate from rst children to the next
child, in order of execution.

The reason for this is that we assume that all plans take at

least a single time step to complete.

Tx!y

X has
 is added to X 's parent's outgoing probability outparent(x) so that it may be used when propagating parent(x)'s temporal
If the transition

is the special-case termination transition (line 1920), then

no successors. In this case, the outgoing temporal probability

outgoing probability along its own temporal outgoing transitions. Note again that the postorder traversal of the plan-hierarchy guarantees that all children are explored before their
parents, thus

outparent(x)

is fully computed by the time the algorithm reaches

parent(x).

Computing X 's new blocked and non-blocked probabilities (lines 2122).
that the outgoing probability mass has been propagated to

X 's

only steps remaining involve re-calculation of Overseer's belief in

X 's

blocked and non-

blocked states. The total temporal outgoing probability (whether blocked or not) is
must be subtracted from future belief that the agent is executing
that left

outx

bt (X; :block )

x .

Now

children and siblings, the

X.

outx ;

it

The probability mass

but is blocking on a message that was not observed by Overseer is

It is added to

X 's

future blocked state.

3.4 Discussion
The overhearing approach outlined in this section maintains a separate plan-recognition
mechanism for each agent, ignoring any inter-agent dependencies. Using an array of individual models (Figure 3) that are updated with the passage of time, or as messages are
observed, the state of a team is taken to be the combination of the most likely state of each
individual agent. Algorithm 4 embodies this approach: It is called every time tick, collects
all messages that are observed, and updates the state of the agents.
EVACUATE [TASK FORCE]

PROCESS
ORDERS
[TASK FORCE]

.....

EXECUTE
MISSION
[TASK FORCE]

PROCESS
ORDERS
[TASK FORCE]
LANDING
ZONE
MANEUVERS
[FLIGHT TEAM]

FLY-FLIGHT
PLAN
GET
ORDERS
[FLIGHT TEAM]
[GET ORDERS]
FLY-CONTROL
ROUTE....
[FLIGHT TEAM]

EVACUATE [TASK FORCE]

LANDING
ZONE
MANEUVERS
[FLIGHT TEAM]

FLY-FLIGHT
PLAN
GET
ORDERS
[FLIGHT TEAM]
[GET ORDERS]

....

FLY-CONTROL
ROUTE....
[FLIGHT TEAM]

ESCORT
TRANSPORT
OPERATIONS OPERATIONS
[ESCORT]
[TRANSPORT]

.....

EXECUTE
MISSION
[TASK FORCE]

....
ESCORT
TRANSPORT
OPERATIONS OPERATIONS
[ESCORT]
[TRANSPORT]

Figure 3: Array of single-agent recognizersone for each agent.
As an illustration of the operation of this algorithm, consider the example domain of
the evacuation scenario. Overseer begins with a belief that the agent is executing its top-

Process-Orders) at time 0 (i.e., b0 (Evacuate; :block ) = 1:0,
b0 (P rocessOrders; :block ) = 1:0). If Overseer observes a message about the initiation of
Fly-Flight-Plan by one of the helicopters, then it applies Incorporate-Evidence (Algolevel plan (and its rst child,

101

fiKaminka, Pynadath, & Tambe
Algorithm 4 Array-Overseer(beliefs b, plan-hierarchy
1:
2:
3:
4:
5:

Agents a 2 A do
if A message ma from a was observed then
Incorporate-Evidence(ma; b; M [a])
else {No message was sent by a }
Propogate-Forward(b; M [a])

array

M [],

agents

A)

for all

rithm 1). From the plan-hierarchy (Figure 1b) it is known that

Process-Orders cannot be a

possible current or future plan of the agent, and that the helicopter in question is executing

Fly-Flight-Plan,

i.e.,

bt (P rocessOrders; :block ) = 0, bt (F lyF lightP lan; :block ) = 1:0.
Fly-Flight-Plan's rst children, of which there is

This probability mass is propagated to

one, and thus the belief in this child is set to 1.0 as well.
After some time passes and no message is observed, there is uncertainty as to whether

Fly-Flight-Plan

Landing-Zone-Maneuvers are active, as both are possible future
states, and the duration of Fly-Flight-Plan is uncertain. Overseer would still assign
a probability of 1.0 to the top-level plan Evacuate. However, some probability mass from
Fly-Flight-Plan would be propagated every time-tick to Landing-Zone-Maneuvers by
and

Propagate-Forward (Algorithm 3). For each such propagation, the incoming temporal

probability mass being added to the belief in the execution of

Landing-Zone-Maneuvers

would be propagated to its rst children immediately. Assuming that the helicopter agent
is free to select either

Transport-Operations

or

Escort-Operations,

the incoming proba-

bility would be split evenly and added to the prior belief in each of the two rst children.
In the same temporal propagation step, any outgoing belief from these rst children would
be propagated via their own outgoing temporal transitions.
The inference procedure described by Algorithms 14 exploits the particular structure of
our representation in ways that more general existing algorithms cannot. The pseudo-code
demonstrates that for a single monitored agent, both types of belief updates have a time
complexity

linear

M,
O (M N ).

in the number of plans and transitions in

agents, the space and time complexity of Algorithm 4 is

i.e.,

O (M ).

Thus for

N

We gain this eciency (compared to an approach such as DBN) from two sources. First,
we make a Markovian assumption that the probability of observing a message depends on
only the relevant plan being active, independently of execution history. With this assumption, we can incorporate evidence, based on only our beliefs at time

t.

Second, we make

another Markovian assumption in the temporal model, allowing our propagation algorithm
to reason forward to time

t

+1

based on only our beliefs at time

t,

without regard for

previous history.

4. Monitoring a Team by Overhearing
The previous section has outlined an ecient plan-recognition mechanism that is particularly
suitable for monitoring a single agent based on its communications. Monitoring a team was
achieved by monitoring each member of the team independently of the others. Unfortunately,
although the time complexity of this approach is acceptable, its monitoring (recognition)
results are poor.

The evaluation in Section 6.1 provides more details, but, in short, the

average accuracy using this approach over all experiments was

102

less than 4%.

fiMonitoring Teams by Overhearing
The main cause for this low accuracy is the scarcity of observations, one of the identifying
characteristics of monitoring by overhearing. As previously discussed, agents often switch
their state unobservably (i.e., without sending a message). Therefore, the monitoring system
critically needs to estimate correctly the times at which agents switch state.

Since some

agents rarely communicate (i.e., there are very few observations about them), variance in
their temporal behavior (with respect to the system's predictions) tends to cause large errors
in monitoring.
To address this issue, we bring back for discussion the agent independence assumption
which we have made in the previous section. After all, team-members do not communicate
independently of each other:

Communication in a team is an action that is intended to

change the state of a listener (Cohen & Levesque, 1990).
message may still change their state upon

receiving

Agents that only rarely

a message.

send

a

In other words, although

observed messages are used in the previous section to update the belief in the state of
the sender, they could also be used to update the state of any listeners.

To do this, the

monitoring system must know about the relationships between the team-members.
Knowledge of the social structures enables additional sophisticated forms of monitoring.
For instance, in order to maintain their social structures, team-members communicate with
each other predictably, during particular points in the execution of a task. Such predictions
of future observable behaviorcommunicationscan be used to further reduce the uncertainty. However, it is often the case that while it can be dicult to correctly predict that
a specic agent will communicate at a specic point in task execution, it is easy to predict
that some team-member will. Knowledge of the procedures employed by a team to maintain
its social structures can be very useful allows a monitoring system to make such predictions.
To reason about the eects of communications on receivers, and about future observable behavior of team-members, a monitoring system must utilize knowledge of the social structures and social procedures used by team-members to maintain these structures.
Such exploitation of social knowledge for monitoring is called Socially-Attentive Monitoring
(Kaminka & Tambe, 2000). This section discusses these concepts in detail.

4.1 Exploiting Social Structures
While computationally cheap, the approach described earlier proved insucient in the evacuation domain.

In monitoring by overhearing tasks, the monitoring system must address

scarce observations, as agents rarely communicate all at the same time. Indeed, in the evacuation application, only a single message was observed (on average) for every 20 combined
individual state changes.
Under such challenging conditions, a system for monitoring by overhearing must come
to rely extensively on its ability to estimate when agents change their internal state without sending a message.

The representation presented earlier used a simple, but ecient,

temporal model to do this, based on the estimated average duration of plans. However, we
have found high variance in the actual duration of plan execution, compared to the duration
predicted by the average-duration model:



Plan execution times vary depending on the

external environment.

For instance, when

all the agents in the team are running on a local network, their response times to queries

103

fiKaminka, Pynadath, & Tambe
may be shorter than when communicating across continents. Indeed, latency times in
the Internet vary greatly, and are dicult to predict.



Plan execution times vary depending on
instance, the

traveling

when a plan-step is executed internally.

For

plans, used repeatedly within the given evacuation team-

oriented program, take anywhere from 15 seconds to almost two minutes to execute,
depending on the particular route being followed.



Plan execution times vary depending on

the outcome of a plan-step.

For instance, when

the route-planner is functioning correctly, it responds within a few seconds. However,
when it crashes it does not return an answer at all, and the other agents wait for a
relatively long time before relying on a time-out to decide that it had failed.

This problem can be addressed in principle by a more expressive model of execution duration,
for instance taking into account the internal execution context. However, in practice, such
a model would likely be much more expensive computationally, as it would need to rely
on knowledge of previous and future steps, breaking the Markovian assumption (e.g., to
determine duration based on

when a plan-step is executed,

an improved temporal model

would have to reason about the likelihood that a given instance of the plan-step is the
second instance, as opposed to a third). As applications grow in scale in the real world, an
increasingly more complex temporal model would have to be continuously rened to cover
the increasingly complex temporal behavior of agents.

Fortunately, a temporal model is

only one way in which a monitoring system can estimate the times in which agents change
their internal state unobservedly.
An alternative method for estimating unobserved state changes is to utilize known dependencies between agents to exploit evidence about the state of one agent to infer the state
of another. In particular, it is often true in team settings that one agent would send a message

intending

to aect the state of all its receivers in a particular way. Thus in principle,

under the assumption that the receivers do change their state predictably, an observation
of such a message can be used as evidence in the inference of the sender's state, as well as
all receivers', i.e., the state of all team-members. We can trade the agent independence assumption made earlier with an assumption of successful coordination. This is a reasonable
assumption in team settings, given that agents are actively attempting to maintain their
teamwork with such communications (Tambe, 1997; Kumar et

al., 2000; Dunin-Keplicz &

Verbrugge, 2001).
The eects of a message on a receiver are dependent on the relationship between the
sender and the receiver (where we take such a relationship to be described by a mathematical
relation between the possible states of the sender and the receiver). In principle, such relationships underly

social structures structures of interactions between agents that make the

decisions of one team-member dependent, to some predictable degree, on those of its teammates. Using knowledge of these dependencies, a monitoring agent may use observations of
a communication action by an agent to infer the possible state of another.
One simple example of such a structure is common in many teams (e.g., Jennings, 1993;
Kinny, Ljungberg, Rao, Sonenberg, Tidhar, & Werner, 1992), and indeed is present also
in our application:

roles

that govern which team-members undertake what tasks in service

104

fiMonitoring Teams by Overhearing
of the team goal. Such roles ideally bias the decision mechanism of the team-members towards making decisions that are appropriate for their roles. Thus knowledge of the roles of
team-members can be useful to counter the uncertainty faced by a monitoring agent. For
instance, suppose the monitoring agent knows that in the evacuation application, a particular team-member is to choose

Landing-Zone-Maneuvers

Transport-Ops,

rather than

Escort-Ops,

as a child of

(because the team-member belongs to the TRANSPORT team,

rather than the ESCORT team). This knowledge can reduce the uncertainty the monitoring agent hasunder the assumption that the team-member did not incorrectly choose an

inappropriate

plan for its role. Overseer in fact uses knowledge of roles in such a manner

to alleviate uncertainty. This monitoring use of role information has been used in previous
work (Tambe, 1996; Intille & Bobick, 1999), discussed in Section 7.
However, a much more important social structure exists in teams. Agents in teams work

together, as team-member are ideally in agreement

about their joint goals and plans (Cohen

& Levesque, 1991; Levesque, Cohen, & Nunes, 1990; Jennings, 1995; Grosz & Kraus, 1996,
1999; Tambe, 1997; Rich & Sidner, 1997; Lesh, Rich, & Sidner, 1999; Kumar & Cohen,
2000; Kumar et al., 2000). This phenomenonsometimes called

team coherence

(Kaminka

& Tambe, 2000)holds at dierent levels in the team. Agents in an atomic subteam work
together on the plans selected for the subteam, subteams work together with sibling subteams
on higher level joint plans, etc. Individual agents may still choose their own execution, but
they do so in service of agreed-upon joint plans. Provided the monitoring agent knows what
plans are to be jointly executed by which subteams, and what transitions are to be taken
together by which subteams, it can use coherence as a heuristic, preferring hypotheses in
which team-members are in agreement about their joint plans, over hypotheses in which
they are in disagreement.
For example, suppose that the entire team is known to be executing

Fly-Flight-Plan

(Figure 1-b). Now, a message from one member of the TRANSPORT subteam is observed,
indicating that it has begun execution of the

Transport-Ops plan step.

Since this plan step

is to be jointly executed by all members of the TRANSPORT subteam (and only them),
we can use coherence to prefer the hypothesis that the other subteam members have also

Transport-Ops. Furthermore, since this plan-step is in service of the
Landing-Zone-Maneuvers plan, which is to be jointly executed by the TRANSPORT and
initiated execution of

ESCORT subteams, we can prefer the coherent hypothesis that team-members of ESCORT
are executing

Landing-Zone-Maneuvers.

Now, based on their known role, we can now come

back down the plan-hierarchy and infer that members of the ESCORT subteam are executing

Escort-Ops,

etc.

This knowledge of the expected relationships, and in particular knowledge of which plans
are joint to team-members (i.e., are subject to coherence), is part of the specication of a
distributed applicationand can thus be provided to an overhearing system by the designer
or operator. In fact, it is often readily available, since it is used by the agents themselves
in their coordination.

For instance, we have earlier discussed the assumption that team-

oriented programs are available to the monitoring agent, and that these hold knowledge
about what plans in the hierarchy are to be executed by which (sub)teams is encoded in the
plan-hierarchy. The team hierarchy contains the knowledge about what subteam/agent is
part of another subteam.

105

fiKaminka, Pynadath, & Tambe
Coherence can be a very powerful heuristic. It assumes non-failing cases, where teammembers successfully maintain their joint execution of particular plans. Under this assumption, evidence about a decision made by one team-member inuences (through coherence),
our belief of what its team-mates have decided. And lacking such evidence, coherence prefers
hypotheses in which at least the team-members have made joint decisions. For instance, suppose a transition from a team plan is to be taken only by the TRANSPORT team. Under
non-failure circumstances, there are only two coherent hypotheses considering this transition: Either all members of TRANSPORT took the transition, or none did. Evidence for
one member, supporting one of these hypotheses, can be used to infer the state of the other
members.
The signicance of this property of coherence is that if the monitoring system can reduce
the uncertainty for even one agent, then this reduction will be amplied through the use
of the coherence heuristic to apply to the other agents as well.

The use of the coherence

heuristic can thus lead to a signicant boost in monitoring accuracy, since the number of
hypotheses underlying any further (probabilistic) disambiguation is cut down dramatically.
Section 6.1 provides an in-depth evaluation of the use of coherence and knowledge of roles
to select plan recognition hypotheses in Overseer.
The use of coherence signicantly increases the time complexity of the computation.
At the very least, it requires setting inter-agent links in the array of plan recognizers used
by Overseer (Section 3.4), such that these links represent a probabilistic association between plans that are to be executed jointly (in contrast with the temporal and hierarchic
decomposition transitions used thus far). For instance, if a specic plan
jointly by agents

A

(representing agent

and

A's

B,

N

N

execution of a plan

X)

and the variable

O (M N 2 )

N (N
2

XtB

1)

(representing

M,

XtA
agent B 's

such inter-agent links be-

agents, for each one of the joint plans (of which there are at most

agents, and the array of recognizers

is of size

is to be executed

then such a link would be constructed between the variable

execution of the same plan). In general, there would be
tween

X

M [],

M ).

Thus given

where each individual agent's plan-hierarchy

the run-time complexity of an exact-inference algorithm would be at least

and quite likely much worse (since in general there is an exponential number of

coherent and non-coherent hypotheses to select from).

In the next section (Section 5.1),

we describe a highly scalable (in the number of agents) representation for reasoning about
coherent hypotheses.

4.2 Exploiting Procedures that Maintain Social Structures
A monitoring system can exploit knowledge of the procedures agents use to maintain their
social structures to alleviate some of the uncertainty resulting from the scarceness of observations. For instance, if the monitoring system could accurately predict

future observable

behavior of monitored agents, then while it has not observed the predicted behavior, the
monitoring system may infer that the agents have not reached the state associated with the
predicted behavior. Thus such predictions can be used to eliminate monitoring hypotheses,
by setting an individual agent's

XY

probabilities to reect a prediction that a message will

be transmitted by the agent as its execution of

X

terminates and it initiates

Y.

For instance,

in our own application, the Ariadne information agent is queried for possible threats before
each route is followed in the evacuation. It may therefore be possible to predict that before

106

fiMonitoring Teams by Overhearing
each route is taken by the helicopters, a message will be sent by the Ariadne agent to its
teammates; thus while no such message is observed, the Ariadne agent can be inferred to
have not yet executed this step. Furthermore, under the assumption of coherence (discussed
above), the monitoring system may further infer that all team-members have not yet executed this step, i.e., a new route was not taken by the team.

Such inference is obviously

dependent on the system's observational capabilities, but we have found it to be useful even
under lossy observations by the monitoring system (see Section 6.2).
However, in general, such specic individual predictions can be dicult to make. Teammembers are often engaged in joint tasks, which require many agents to tackle a problem
together. In these settings, predicting individual communications may be impossible. For
instance, consider a distributed search problem in which a target solution is to be found
somewhere in the search-space; dierent areas of the search space are divided amongst the
agents, with the understanding that the rst to nd the target will communicate with the
others. It would be dicult to accurately predict which one of the agents will communicate
(nd the target), since if we could predict that, we could focus all agents' eorts on that area
alone. Yet it is easy to predict that at least one agent will nd the target and communicate.
Similarly, in the evacuation application, it may be dicult to predict which helicopter will
reach the civilians rstbut it is easy to predict that one of them will, and will then
communicate their location.
Indeed, teams utilize

social procedures

or

conventions

(Jennings, 1993) by which team-

members maintain their relationships with one another. Removal of the agent independence
assumption allows the monitoring system to exploit knowledge of such procedures, by making predictions as to the behavior of team-members in coordinating with one another. For
instance, knowledge of the failure-recovery procedures used by a team to recover from coordination failures allows the monitoring system to predict the future behavior of team-members
in case of failed execution. Similarly, knowledge of the communication procedures used by
the team (as part of its team-members' coordination) allows predicting future observable
messagesfuture interactions between team-memberswithout necessarily specifying a particular individual agent that will carry them out.
For example, suppose Overseer overhears a message indicating that the ight team has

Fly-Flight-Plan (Figure 1-b). After some time has passed, it is
now possible that the team is either still executing Fly-Flight-Plan, or it has terminated
it already and begun joint execution of Landing-Zone-Maneuvers. However, if Overinitiated joint execution of

seer knows that at least one team-member will explicitly communicate after terminating

Fly-Flight-Plan and before initiating Landing-Zone-Maneuvers, then while such communications are not observed, the monitoring system can eliminate the possibility that the team
is executing the latter, eliminating any uncertainty in this case (only

Fly-Flight-Plan

is

possible).
We leave discussion of how technically a social procedure of the form at least one teammember will communicate when its subteam will take this transition from
converted into
team-wide



XY

X

to

Y

can be

values to the next section, where we present a technique for representing

probabilities in a way that allows ecient reasoning. In the remainder of this

section, we address instead how knowledge of such social procedures may be acquired.
Social procedures of communications may be simple per-case rules, or may involve
complex algorithms.

For instance, Jennings (1993) suggests using heuristic application-

107

fiKaminka, Pynadath, & Tambe
dependent rules to determine communication decisions.

STEAM (Tambe, 1997) instead

uses a decision-theoretic procedure that considers the cost of communication and the cost
of miscoordination in the decision to communicate. Other procedures have been proposed
as well (e.g., Cohen & Levesque, 1991; Jennings, 1995; Rich & Sidner, 1997).

However,

regardless of their complexity, a key point is that a monitoring system does not necessarily
have to have full knowledge of these procedures in order to exploit them for predictions: it
only needs to approximate their outcome, since it can use a combination of techniques to
combat plan-recognition ambiguity, rather than relying just on one technique.
The decisions of social procedures can be acquired by learning from previous runs of
the system.

Although a detailed exploration of appropriate learning mechanisms is out-

side the scope of this paper, we provide a strict demonstration of the feasibility of learning
social procedures by simple rote-learning, which proved eective in generating a useful communications model that signicantly reduced the uncertainty in monitoring the evacuation
application.

This simple mechanism records during execution which plans are explicitly

communicated about, and whether they were initiated or terminated. The learned rules are
eective immediately, and are stored for future monitoring of the same task.
Figures 4ad present the results from using of this rote-learning mechanism in four
dierent runs on the same tasks.

The X-axis denotes observed communication message-

exchanges as the task progresses.

Overall, between 22 and 45 exchanges take place in a

run, each exchange including between one and a dozen broadcast messages in which agents
announce termination or initiation of a plan. The Y-axis shows the number of hypotheses
considered by Overseer after seeing each message, without using any probabilistic temporal
knowledge. Thus greater uncertainty about which hypothesis is correct would be reected
by higher values on the Y-axis. At the beginning of task execution, all possible plans are
considered possible, since we ignore temporal knowledge in this graph. As progress is made
on the task, less and less steps remain possible before the end is reached, and so we expect
to see a gradual (non-monotonic) decline as we move along the X-axis. A technique that
successfully eliminates hypotheses from considerations results in Y-axis values

lower

than

those of this baseline execution curve.
In Figure 4, the line marked

No Learning

shows this baseline (i.e., no predictions, and

with the learning component turned o ). The baseline shows that a relatively high level of
ambiguity exists, since the system cannot make any predictions about future states of the
agents, other than that they are possible. When the learning technique is applied on-line
(i.e., any message seen is immediately used for future predictions), some learned experience is
immediately useful, and ambiguity is reduced somewhat (the line marked

On-Line Learning ).

However, some exchanges are either encountered late during task execution, or are seen only
once. Those cannot be eectively used to reduce the ambiguity of the monitoring system on
the rst run. However, the third line (

After Learning )

presents the number of hypotheses

considered when a fully-learned model is used. Here, the model was learned on run G, then
applied without any modications in the other runs of the system. As can be seen, it shows
a signicantly reduction in the number of hypotheses considered by Overseer.

Further

evaluation of the use of communications predictions is presented in Sections 6.1 and 6.2;
however, a full exploration of the use of learning for this task is beyond the scope of this
paper.

108

fiMonitoring Teams by Overhearing

25

No learning
On-line learning
Using previously learned predictions

20

Number of Recognized Plans

Number of Recognized Plans

25

15
10
5
0

No learning
On-line learning
Using previously learned predictions

20
15
10
5
0

0

5

10
15
20
25
30
35
Observed Communication Exchanges

40

45

0

5

(a) Learning in experiment C

25

No learning
On-line learning
Using previously learned predictions

20

40

45

40

45

(b) Experiment E

Number of Recognized Plans

Number of Recognized Plans

25

10
15
20
25
30
35
Observed Communication Exchanges

15
10
5
0

No learning
On-line learning
Using previously learned predictions

20
15
10
5
0

0

5

10
15
20
25
30
35
Observed Communication Exchanges

40

45

0

(c) Experiment G

5

10
15
20
25
30
35
Observed Communication Exchanges

(d) Experiment I

Figure 4: Learning of communication decisions in dierent experiments.

109

fiKaminka, Pynadath, & Tambe
4.3 Discussion
A key characteristic of monitoring by overhearing tasks is the scarcity of observations available to the monitoring system.

Fortunately, the observations available to the monitoring

system can often be viewed as observations of

multi-agent actions :

The sender of the message

not only changes its own state, but often also intends to change the state of the recipients
(Cohen & Levesque, 1990).

Thus even a single observation can be used as evidence for

inferring the state of both sender and receivers. This stands in contrast to previous work,
which addressed monitoring of multiple

single-agent

actions.

In monitoring a team, the monitoring system can use knowledge of social structures and
procedures to exploit information about the activities of one team-member, in hypothesizing about the activities of another team-member. These techniques are not specic to the
representation presented earlier. For instance, an increased belief in one agent's execution
of a plan

X

based on evidence for a teammate's execution of

X

can be also used by con-

structing appropriate probabilistic links between nodes representing these beliefs in a large
DBN representing the two agents. If we start with the DBN representation as discussed in
Section 3.1, we can replicate the single-agent network (containing

N

separate agents. The number of nodes is then

O (M N ),
2

M

plans) for each of the

since we represent the plans and

transitions for each individual agent. We can also introduce the appropriate inter-agent links
to capture the inter-agent dependencies represented by our model of teamwork. However,
upon introducing such links, the computational complexity of performing DBN inference
M 2N .
explodes to O

(2

)

Obviously, such social reasoning can be computationally expensive, even with the efcient representation described earlier.

The next section provides details of an ecient

mechanism for reasoning about a team using information about role and coherence, and utilizing communications predictions. Using this mechanism, the techniques described in this
section have resulted in an accuracy of up to 97% (84% average across all experiments)
compared to average 4% without the use of social knowledge. Sections 6.1 and 6.2 present
a detailed discussion of these results.

5. Plan-Recognition for Overhearing
The previous section has outlined socially-attentive monitoring techniques, alleviating the
uncertainty in monitoring a team of agents by exploiting knowledge of the social structures
and social procedures of the monitored team. It discussed using

nance

coherence

and

role mainte-

to exploit knowledge of the ideal agreement of agents that specic plans be executed

together, and that other specic plans are assigned to agents fullling their roles. Furthermore, it discussed disambiguation based on predictions of future observable behavior, based
on knowledge of the social procedures employed by team-members. These disambiguation
heuristics eliminate many (incorrect) hypotheses from being considered. However, reasoning
using these techniques can be computationally expensive.
This section presents an ecient algorithm, building on the representation previously
presented, which facilitates scalable monitoring by overhearing of large teams. The key idea
here is to represent only those hypotheses which the heuristics would have considered valid,
eliminating from consideration plans and transitions that would be considered illegal with
the heuristics. Relying on the team-hierarchy for bookeeping, all coherent hypotheses are

110

fiMonitoring Teams by Overhearing
represented using a single recognizer instead of an array of recognizers, oering considerable
scalability in team monitoring. However, since the algorithm can no longer represent certain
hypotheses, this scalability comes at the expense of expressivity.

We discuss the scalable

representation and the trade-o it oers below.

5.1 Ecient Reasoning with Team Coherence
Coherence is a very strong constraint, since for a team of agents there are only a linear
number (O

(M ) where M

is the size of the plan-hierarchy) of coherent hypotheses, but an

exponential number of incoherent hypotheses (O

(M N ) where N is the number of agents; the

proof is in Appendix A). We can exploit this property by designing monitoring algorithms
that reason only about the linear number of coherent hypotheses, and therefore oer better
scalability as the number of agents increases. Such algorithms may not be able to reason
about incoherent hypotheses, and are therefore less expressive. However, Section 6 demonstrates that the level of accuracy even with such limited expressiveness is sucient for our
purposes.

Furthermore, algorithms that reason only about coherent hypotheses may still

be able to detect incoherent hypotheses, representing a failure state in which two or more
team-members are in disagreement with each other.
We begin by presenting the YOYO* algorithm, an ecient technique for reasoning about
coherent hypotheses (Algorithm 5). YOYO* replaces the array-based algorithm described
earlier (Algorithm 4). Similarly to it, YOYO* is called every time tick.

If no message is

observed, the state of the entire team is propagated forward in time. Otherwise, all observed
messages are collected together and used as evidence for the (dierent) plans implied by these
messages.
YOYO*'s key novelty is that it relies on a

single

plan-hierarchy that is used to represent

all team-members together (regardless of their number), instead of an array of such structures. In other words, each variable
teams associated with

X

at time

t.

X

Xt

represents Overseer's belief that

all agents

in the

(as described in the team-oriented program) are executing the plan

Thus YOYO* makes extensive use of the information associating plans and

transitions in

M

with teams and subteams in

H,

the team-hierarchy. The team hierarchy

plays a critical bookeeping role in this respect, since it maintains the knowledge critical for
correctly applying coherence in the single recognizer.
This key distinction between YOYO* and the array-based approach causes a subtle,
but critical, dierence in the way probabilities are propagated along transitions. In a planhierarchy

M

of an individual agent, part of an array of such models, each outgoing transition

represented a hierarchical decomposition or temporal step that the agent is allowed to take.
Alternative outgoing transitions therefore represent alternative paths of execution available
to the agent.

On the other hand, in a plan-hierarchy

M

used by YOYO*, alternative

outgoing transitions tagged by dierent subteams (that are not ancestors of one another)
represent not a decision point for the agent, but alternative paths of execution as decided
by the agents' roles and team-memberships.
This creates a critical dierence in how the values of
Where previously (in Section 3) the value of
agent will take a transition

X

!

Y

xy

XY

and

XY

are to be interpreted.

referred to the probability that a specic

(given that it has terminated execution of

YOYO* it refers to the probability that an entire team will take the transition

111

X ),

in

together.

fiKaminka, Pynadath, & Tambe

Algorithm 5 YOYO*(plan-hierarchy M , team-hierarchy H , beliefs b)
1:
2:
3:
4:
5:
6:
7:

8:
9:
10:
11:
12:
13:
14:

if

no new messages are observed then

Team-Propagate-Forward(b, M )

else

Initialize distributions b0 ; bt+1 0 for all plans U 2 M . ; Initialize I; E to be empty sets.
for all Messages mi do
I
I [ fX j X 2 M; mi is a an initiation message; X consistent with mi g
E
E [ fY j Y 2 M; mi is a termination message; Y consistent with mi g
plans X 2 I do
teammsg (X ) {T is the agent sending the message initiating X }
for all plans W 2 M that precede X , where the transition W ! X is allowed for T do
b0 (X; :block )
b0 (X; :block ) + bt (W; block )wx wx
for all plans X 2 E do
T
teamm sg (X ) {T is the agent sending the message terminating X }
for all plans Y 2 M; Y 2
= I that succeed X , where the transition X ! Y is allowed for T

for all

T

do

15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:

b0 (Y; :block )

b0 (X; :block ) + bt (X; block )xy xy
Normalize distribution b0 taking teams into account
for all plans X where b0 (X; :block ) > 0 do
bt+1 (X; :block )
b0 (X; :block )
(X; b0 (X; :block ); b; M )
T
team(X )
P
X
while parent(P ) 6= null do
bt+1 (parent(P ); :block )
bt+1 (P; :block )
if team(parent(P )) = parentteam (T ) then
(parent(P ); T; P; b)
T = parentteam (T )
P
parent(P )

Team-Propagate-Down

Scale

112

fiMonitoring Teams by Overhearing
YOYO* is unable to represent hypotheses in which some team-members take one transition,
and others do notunless these two dierent groups of members form dierent subteams
that are represented in the team-hierarchy, and the dierent transitions are tagged as being
allowed for the dierent subteams.
The value of

XY

is also interpreted dierently, in a very critical way.

Where in the

previous sections it was taken to represent the probability that a specic individual will
communicate when a transition
the probability that

X

!Y

is taken, in YOYO* its value represents instead

one or more team-members will communicate

when the transition is

taken by the team. Thus it no longer refers to individual agents, but to a (sub-)team. In
this way, YOYO* solves the issue of how to represent predictions of the type at least one
team-member will communicate when this step is reached, discussed previously.
For

example,

suppose

Landing-Zone-Maneuvers

YOYO*

sets

the

belief

that

the

is

executing

the

p. Landing-Zone-Maneuvers, in
Escort-Ops and Transport-Ops, to be executed by mem-

plan-step to some probability

YOYO*, has two (rst) children:

bers of th ESCORT and TRANSPORT subteams, respectively.
agent case, the probability

team

p

Unlike in the individual

should not be divided among these two children, but should

be duplicated to them: A belief that the entire team is executing

Landing-Zone-Maneuvers
Transport-Ops,

implies an equally-likely belief that the TRANSPORT subteam is executing
and that the ESCORT subteam is executing

Escort-Ops.

We explain YOYO*'s operation

in detail below:

No message is observed (lines 12).

Since no observations are available, the state of the

entire team is jointly propagated forward in time by calling Team-Propagate-forward
(Algorithm 7, Appendix A). This is a slightly modied version of the propagate-forward
(Algorithm 3) that takes dierent subteams into account in propagating beliefs: Given some
total outgoing probability (either to a sibling or child transition), if the outgoing transitions
are to be taken by dierent teams where one team is not an ancestor of another (such as
the TRANSPORT and ESCORT sub-teams), the same total probability would be used for
each transition, instead of splitting the outgoing probability between the transitions. Appropriately, Team-Propagate-forward relies on a modied version of the PropagateDown algorithm (Algorithm 2), called Team-Propagate-Down (Algorithm 6, Appendix

A).

This latter algorithm is also used in the incorporation of evidence (lines 327).

run-time complexity of the propagation process is

O (M ).

One or more messages are observed (lines 37).

The

If one or more messages are ob-

served (since YOYO* is a single algorithm monitoring multiple potential message senders,
more than one message may be observed at once), YOYO* begins to incorporate these observations into the maintained beliefs about the team. This process is somewhat similar to
the Incorporate-Evidence algorithm, described earlier (Algorithm 1), but takes into account multiple observations (since all

N

agents may have sent a message). Multiple messages

(from dierent agents) may all refer to the same plan, but YOYO* must not incorporate
evidence for them multiple times.
The simple loop (lines 57) builds the set

I

(of initialized plans) and

plans) by going over all incoming messages that have arrived at time
complexity of this process (in the worst case) is

113

O (N ).

E
t.

(of terminated
The run-time

Here, YOYO* does better than the

fiKaminka, Pynadath, & Tambe
array approach, since multiple messages always cause multiple updates in the array, but in
YOYO*, multiple messages may all refer to a single plan, thus triggering a single update.

Incorporating evidence about initiated and terminated plans (lines 815).
each one of these plans
prior belief in

X

in

I

(line 8), YOYO* now sets the new belief

b0 ,

For

weighted by any

X 's initiation (lines 1011), similarly to how this is done in the incorporate-

evidence algorithm (Algorithm 1), but taking into account the team implied by the sender

of the processed message (line 9).
(teammsg

This is done by a lookup into

M

using the sender

(mi )): Only transitions in M that T is allowed to take are followed.

any transition that is allowed to be taken by a super-team of

T

T

By denition,

is allowed for

T.

A similar

process is then done with any termination messages (lines 1215), but of course looking at
possible successors of any plans consistent with the messages.

However, since we do not

want to cause updates in both line 11 and line 15 in cases where a termination message and
an initiation message refer to the same transition, the loop over the plans

Y

(line 14) skips

any plans which have already been addressed in the previous step. Overall, the run-time
complexity of this process is

O (M ).

Normalizing the temporary distribution b0 (line 16).
b0

The temporary distribution

resulting from the processing of initiation and termination messages is normalized, in a

similar fashion to the analogous step in algorithm 1. However, the process must take into
account not only the plan-hierarchy in question, but also the team-hierarchy. Unlike a typical
normalization procedure, evidence for two dierent plans, selected by two dierent teams,
may not necessarily compete with each other, and therefore may not necessarily require
normalization. For instance, if two messages are observed, one implying that team

A

has

P , and another implying that team B has initiated execution
Q are both children of a joint parent J (executed jointly by the
then the same normalized likelihood (1.0) should be assigned to P and

initiated execution of plan
of plan

Q,

then if

(and

and

A; B ),
J but this will

two subteams

Q

P

be assigned to it by the propagation steps described below). The

run-time complexity of this process is

O (M ) .

Propagating the evidence up and down M (lines 1727).

First, the beliefs are set

for each plan implied by the observations, and its children (lines 1819). Then, the team
that is to execute this plan is determined by a lookup into

M

using

team(X )

T

(line 20). Now

YOYO* begins to propagate the evidence up to the plan's parents (lines 2126). Any belief
in the child plan is propagated and added to the belief in its parent (line 23).

However,

(P )) is to be executed by a super-team of the current team T ,

if the parent plan (parent

then any change to its probability must be propagated to its other children, that are to be
executed by other (subteams). Thus the upward propagation is alternated with downward

1

propagation along hierarchical decomposition transitions . This downward step is executed
whenever the team that is responsible for joint execution of the parent plan is no longer
the current subteam being considered (T ), but its parent team in the team-hierarchy
given by

parentteam (T )

H,

(lines 2426). When this condition is satised, any change in the

beliefs about the parent plan must be propagated down to any children it has that are to be
executed by other subteams. This is done via the Scale algorithm (Algorithm 8, Appendix
A).
1. This alternating upward-downward propagation is the origin for YOYO*'s name.

114

fiMonitoring Teams by Overhearing
The downward propagation (line 25) implements a subtle but critical step: It re-aligns
any beliefs YOYO* maintains about subteams other than those implied by the message so
that these beliefs are made coherent with existing evidence. The Scale procedure, which
re-distributes the new state probability of a parent among its children, such that each child
gets scaled based on its relative weight in the parent.

The end result is that the state

probabilities of the children are made to sum up to the state probability of the parent. The
process is recursive, but never re-visits a subtree, since it is only carried out for hierarchicaldecomposition transitions that were not previously updated.
Once this downward propagation is done, YOYO* updates the current team to be its

parentteam

parent in the team-hierarchy, in line 26. Note that the call to
in the team-hierarchy

H,

rather than the plan-hierarchy

M.

downward propagation took place, the temporary variable
hierarchical decomposition in

M

(line 27).

Each iteration through the loop begun on line 17 is

reects a lookup

Finally, regardless of whether

P

O (M

is updated to climb up the

+ H ) since in the worst case

both the plan-hierarchy and team-hierarchy are traversed. However, this loop many repeat
(in the worst case) for each of the plans in the plan-hierarchy, and thus overall, the run-time
complexity of this process is

O (M (M

An example run of YOYO*.

+ H )) = O(M 2 + M H ).

The following example illustrates YOYO*'s inference

upon an observation of a message. Suppose a single member of the TRANSPORT subteam
communicates that it is initiating the

Transport-Ops

plan. Upon observing this message,

YOYO* looks up the sender, to determine what transitions can be taken by it (line 8). It then
proceeds to determine the new beliefs in team

T 's execution of the Transport-Ops plan (lines

910, then 16), and incorporates these new beliefs to reect a much increased belief that the

Transport-Ops and its children (lines 1819). Since
Landing-Zone-Maneuvers, is not null, YOYO* enters the loop in lines

TRANSPORT subteam is executing
this plan's parent,

2227. First, it increases the belief in the execution of the parent (line 23). Then, it checks
the condition on line 24: Indeed, the team that is to execute

Landing-Zone-Maneuvers is
Landing-Zone-Maneuvers

TEAM-FLY-OUT, the parent of the TRANSPORT subteam (i.e.,

is to be executed jointly by the TRANSPORT and ESCORT subteams).
fore calls the Scale procedure (line 25) to re-adjust
children subtrees.

Transport-Ops

Landing-Zone-Maneuvers

YOYO* there-

Landing-Zone-Maneuvers'

other

has two hierarchical-decomposition children:

(which YOYO* has already updated) which is to be executed by the

Escort-Ops, which is to be executed by the ESCORT subteam.
Landing-Zone-Maneuvers to Escort-Ops, increasing YOYO*'s
team is executing the Escort-Ops plan. This process re-aligns
had about the likelihood that Escort-Ops was being executed

TRANSPORT subteam, and
Scale climbs

down

from

beliefs that the ESCORT
any prior beliefs YOYO*

with current evidence, in eect updating beliefs about the plans executed by the ESCORT
subteam, based on a single observation made of a member of the TRANSPORT team. The
process now repeats this loop until the entire set of beliefs is updated and aligned with
respect to the observed message.

5.2 Scalability in the Number of Agents
YOYO* oers signicant computational advantages when compared to the individual representation (array) approach. YOYO* requires only a

115

single, fully-expanded plan-hierarchy

fiKaminka, Pynadath, & Tambe
to represent the entire team.

This hierarchy is

a union

of all the individual agent plan-

hierarchies, containing all transitions and plans, tagged by the subteams that are allowed
to execute them.

M

In addition YOYO* uses a single copy of the team hierarchy.

is the size of the plan-hierarchy,

H

is the size of the team-hierarchy, and

N

Suppose

the number

of agents in the team. When agents are added to the monitored team, the team hierarchy
grows by one new node that represents the new agent, and is connected to the appropriate

+ H ). Since
grows with N , we could write it O (M + N ) (compare to the array approach: O (M N ),

sub-team in the team hierarchy. YOYO*'s space complexity is therefore

H

O (M

Algorithm 4).
To analyze YOYO*'s run-time complexity, we have to consider the behavior of Algorithm
5 separately in cases where no communications are observed, and in cases where at least
one message is observed.

If no messages are observed, then an update takes the form of

a single call to Team-Propagate-Forward (Algorithm 7), an

O (M )

process.

This is

clearly a best-case scenario for YOYO*. If one agent communicates, then YOYO* would
have to go through

O (M

M

and

+ H ) = O(M + N ).

H

in its upward-downward propagation process only once, thus

The worst case scenario for YOYO* occurs if all agents send messages, and each one of
these

N

messages refers to a dierent plan (messages about the same plans would be merged

in lines 57). In this case, there would be up to

M

dierent plans for which evidence exists,

and each one of them would require a separate update through lines 1727. Thus YOYO*'s
run-time complexity in this case is

O (N

+ M + M + M (M + H )) = O(N + M 2 + M H ) = O(N + M 2 + M N )

Clearly, this worst-case cannot be continuously sustained by a monitored team, since agents
cannot continuously communicate about their state. We thus believe that the average case
in real-world domains with

many

agents would be much closer to the

O (M

+ N)

case

presented earlier (see Section 6.4 for empiric evaluation). In any case, YOYO*'s complexity
compares favorably with a procedure reasoning about coherent hypotheses using an array
of recognizers, an

O (M N 2 )

process (at least), even if only one agent communicates (Section

4.1).

5.3 Discussion
YOYO* explicitly represents a team as a single coherent entity. Its space and run-time requirements are preferable to the array based approach when the number of agents grow, and
it considerably simplies reasoning about coherence and communications predictions. On
the other hand, YOYO* sacrices the capability to represent failing team activities (incoherent hypotheses), where one team-member is executing one team-plan while its teammate
is executing another.

This does

not

at all mean that individual actions taken by agents

are somehow locked together in synchronous execution, or that individual agents must all
execute the same individual action at the same time. For instance, two team-members

A; B

that are each executing a completely dierent path of execution at the same time (i.e., plan
steps

A1 ; :::; Ak

and

an overall joint plan

B1 ; :::; Bl ) can be easily represented by a plan hierarchy that includes
J , having two rst hierarchical decomposition children, A1 and B1 , to

116

fiMonitoring Teams by Overhearing
be selected by

A and B , respectively. A1 would have an outgoing temporal transition to A2 ,
B1 would have an outgoing temporal transition to B2 , etc. Since J is to

etc. and similarly

be executed by the two team-members jointly, any initial evidence for any one of the agent
executing any of its individual plans would be used by YOYO* as evidence for the other
team-member having begun its own parallel execution of its own individual execution path.
Further evidence about one agent executing its own individual actions would only increase
the likelihood that the other agent is continuing its own execution, at its own pace. However,
it would be impossible for YOYO* to correctly represent a monitoring hypothesis in which

A
of

is executing some child of

J.

J , Ai ,

while

B

is executing some plan that is not

J,

nor a child

Given the results of the evaluation we conducted (Section 6), which demonstrated the

importance of coherence in accurate visualization, the tradeo of expressivity vs. scalability
is justied: Overseer's accuracy was much improved due to the use of coherence.

Although YOYO* sacrices the capability to reason about certain failure (incoherence)
hypotheses, it is still capable of supporting failure-detection, an important secondary goal of
visualization. In earlier work, we have shown the merits of coherence in service of detecting
disagreements in a team, in particular demonstrating that coherent monitoring leads to
sound centralized disagreement detection, and may lead to sound and complete disagreement
detection under specic circumstances (Kaminka & Tambe, 2000). As YOYO* is in fact a
very ecient way to reason about coherent hypotheses, it provides a good basis for providing
sound disagreement detection results.

A concern about the generality of the technique may be raised based on YOYO*'s reliance on the team-hierarchy.

However, we believe it is reasonable to expect that large,

complex, real-world multi-agent systems of the type targeted by this paper would have
an organizational hierarchy of some sort associated with them (see, for instance,

Tidhar,

1993b). Human organizations certainly demonstrate the emergence of such hierarchies, especially as the organizations grow larger (e.g., big corporations, government organizations)
or tackle mission-critical tasks (e.g., military organizations). In addition, team-hierarchies
for computational agents are critical for planning, for maintaining network and system security, etc. Thus we believe our use of a team-hierarchy is not a weakness in our approach, as
organizational structures will become as wide-spread in computational multi-agent systems
as they already are in human multi-agent systems. Indeed, it may be possible to gradually
learn a team-hierarchy for a given coordinated team for the purpose of monitoring; however,
discussion of this possibility is outside the scope of this paper.

Indeed, using a team-hierarchy, we can apply our assumption of coherence to other representations and algorithms as well. For instance, if we start with the DBN representation
of the team from Section 4.3, we can unify the multiple random variables used to represent the separate agents into a single random variable for an overall team/subteam. As in
YOYO*, the size of the representation grows with the size of the plan hierarchy, and not
the number of agents. Thus, the number of nodes will be the same as for the single-agent
case,

O (M 2 ),

as discussed in Section 3.1.

However, again, the complexity of inference in
2
O M .

answering plan-recognition queries will still be exponential in the number of nodes,

117

(2 )

fiKaminka, Pynadath, & Tambe
6. Evaluation
This section presents a detailed evaluation of the dierent contributions contained within
Overseer. We begin by exploring the relative contribution of each technique to the success

of Overseer as a whole (Section 6.1).

We then focus on evaluating Overseer's use of

communications predictions with respect to lossless and lossy observations (Section 6.2).
We then present a comparison of Overseer's performance with that of human experts
and non-experts (Section 6.3). Finally, we empirically evaluate YOYO*'s scalability in our
application domain (Section 6.4).

6.1 Accuracy Evaluation
The rst part of the evaluation tests the contribution of the dierent techniques in Overseer to the successful recognition of the correct state of the team-members. Figure 5 com-

pares the average accuracy for a sample of our actual runs, marked A through J (X-axis).
In each such 1020-minute run, the team executed its task completely. At dierent points
during the execution, the

actual

state of the system was compared to the state

predicted

by Overseer, where the prediction was taken to be the current most-likely hypothesis.
Each run had 2245 such comparisons (data-points). The percentage of correct monitoring
hypotheses for each run across those comparisons is given in the 0-1 (0-100%) range, on the
Y-axis.
1
0.9

Temporal

Average Accuracy

0.8

Coherent
Coherent,
Temporal

0.7
0.6
0.5

Coherent, Comm

0.4

Coherent,
Temporal, Comm

0.3
0.2
0.1
0
A

B

C

D

E

F

G

H

I

J

Evaluation Run
Figure 5: Percent accuracy in sample runs.

The accuracy when using the individual models with no coherence (as in Section 3) is
presented in the leftmost bar (marked

Temporal )

in each group (Figure 5), and is clearly

very low. This approach is a straightforward attempt at monitoring multiple agents by monitoring each individual, without considering the interactions between them, as described in
Section 3.

The next bar presents the monitoring accuracy when only coherence is used

to rule out hypotheses (Section 5.1), with ties broken randomly. The next bar to the right

Coherent, Temporal ) presents the results of combining both coherence and the probabilistic
temporal model (Sections 3 and 5.1). Then, the bar marked (Coherent, Comm ) shows the
(

eects of combining the use of coherence with the use of predictions based on knowledge of

118

fiMonitoring Teams by Overhearing
the communication procedures used by the team (Section 4.2). Here, the communications
predictions were used to restrict the set of coherent hypotheses considered, with ties bro-

Coherence, Temporal, Comm )

ken randomly. The remaining bar (

presents the monitoring

accuracy in each run using the combination of all techniques.
The results presented in Figure 5 demonstrate the eectiveness of the socially-attentive
monitoring techniques we presented.

First, the results show that the coherence heuristic

brings the accuracy up by 1530% without using any probabilistic reasoning. This boost in
performance is a particularly interesting result, because of the relation between the coherence
technique and previous techniques explored in the literature (Tambe, 1996; Intille & Bobick,
1999). Previous work has successfully used the relationships between agents to increase the
accuracy of monitoring. The boost in Overseer's accuracy based on the use of role and
teamwork relationships conrms the results from previous investigations.

However, the

results also demonstrate that the technique is not sucient in this domain.
Overseer adds a number of novel techniques not addressed in previous work.

The

Coherent,

rst such technique combines coherence with a temporal model of plan-duration (

Temporal ),

and it results in signicant increases to the accuracy, because the probabilistic

temporal information now allows Overseer to better handle the lack of observations. A
possible alternative, which we explore in this evaluation, is to rely instead on the communications predictions to rule out hypotheses about future states that may or may not have

Coherent, Comm ).

been reached (

It is therefore interesting to compare the performance of

Coherent, Temporal ) and (Coherent, Comm ) bars.

these two techniques by comparing the (

In almost all runs the average accuracy when using coherence and communications predictions is signicantly higher than when using coherence and the temporal model. This is
despite the fact that the more eective coherence technique uses arbitrary (random) selection
among the available hypotheses: The reason for this is that in many cases the communication predictions are powerful enough to rule out all hypotheses but one or two, signicantly
decreasing the uncertainty of the agents' plan-horizons. Thus even a random selection stands
a better chance than a more informed (by a temporal model) selection among many more
(1020) hypotheses.
However, runs J and B show a reversal of this trend compared to the other runs. Figures
6ab show the accumulative number of errors as task execution progresses during run I
(Figure 6-a) and during run J (Figure 6-b). An error is dened as a failure to choose the
correct hypothesis as the most likely one (i.e., the most likely hypothesis does not reect
the true state of the agent/team). Each message exchange corresponds to one to a dozen
messages communicated by the agents, establishing or terminating a plan. In the two gures,
a lower slope means better performance (less errors). The line marked

Coherent

shows the

accumulative number of errors if only coherence is used to select the correct hypothesis
most such choices turn out to be erroneous since a random choice is made among the
competing hypotheses. The line marked

Coherent, Temporal

shows the results using both

coherence and the temporal model to choose the most likely hypothesis. Similarly, the line
marked

Coherent, Comm

shows the results using both coherence and the communications

predictions. Finally, the remaining line displays the results of using the combined technique,
using coherence, the temporal model, and the communications predictions.
In Figure 6-a, we see that the two techniques (

Coherent, Temporal and Coherent, Comm )

have almost equal slopes and result in almost equal number of errors at the end of run I,

119

fi

7HPSRUDO



&RKHUHQW

























&RKHUHQW

&RKHUHQW




2EVHUYHG 0HVVDJH ([FKDQJHV

7HPSRUDO
&RPP










7HPSRUDO

&RPP



&RKHUHQW

&RKHUHQW
















&RPP



&RKHUHQW

&RKHUHQW









$FFXPXODWLYH  (UURUV

&RKHUHQW








$FFXPXODWLYH  (UURUV

Kaminka, Pynadath, & Tambe

7HPSRUDO
&RPP

2EVHUYHG 0HVVDJH ([FKDQJHV

(a) Run I

(b) Run J

Figure 6: Accumulative number of errors in runs I and J.

though from Figure 5 we know that due to the alleviated uncertainty, the use of communications predictions leads to overall higher probability of success (i.e., the

Coherent, Comm

technique results in fewer alternative hypotheses, and thus has a better chance of being correct). However, in Figure 6-b we see that in run J the situation has changed dramatically.
First, we see that the two lines are no longer similar. The line marked

Coherent, Comm

has

greater slope than in run I, indicating that the communications predictions are not able to
reduce the uncertainty, resulting in lower average accuracy. Second, we see that the temporal model results in many less errors, as evidenced by the much slower-rising slope of the
line marked

Coherent, Temporal.

Thus in this case, the actual duration of plans matched

the temporal model more accurately than in other runs.
In trying to understand this dierence between runs J, B and the other runs of the
system, we discovered that runs J and B involved relatively more failures on the part of
team-members, including agents crashing or not responding at all.

The communications

predictions, however, were learned based on successful runsand thus did not correctly
predict the communication messages that would result as the team detected and recovered
from the failures. Thus the uncertainty was not alleviated, and the arbitrary selection was
made among relatively many hypotheses. This explains the relatively lower accuracy of the
(

Coherent, Comm)

technique in run J and B. This clearly shows a limitation of the simple

learning approach we took, and we intend to address it in future work.

However, there

are other factors that inuence the accuracy of the communication models, since this lower
accuracy did not occur in other runs where failures have occurred.
The results of the

Coherent, Temporal

technique vary as well.

We have been able to

determine that failures cause a relative increase in the relative accuracy of the

Temporal

Coherent,

technique. However, variance in the results is due to additional factors. In run C,

for instance, this technique results in relatively higher accuracy, but no failure has occurred.
Certainly, the mission specications themselves dier between runs, machine loads cause the
mission execution to run slower or faster, etc. The great variance in the temporal behavior
of the system was the principal reason for our using the communication prediction.
variance is obvious in the graphs.

120

This

fiMonitoring Teams by Overhearing
In summary, despite the variance in the results of the

Coherent, Temporal

technique (due

to variance in the temporal behavior of the system and the simplicity of the temporal model),
and the possible sensitivity of the

Coherent, Comm

technique to learned predictions, it is

clear that the two techniques work well in combination, building on the coherence heuristic,
and compensating for each other's weaknesses. In all runs, the combined technique

Temporal, Comm

Coherent,

was superior to either technique alone. Its performance varied between

72% accuracy (Run E) to 97% (Run I). The average accuracy across all runs of this allcombination technique was 84%, resulting in very signicant increases in accuracy compared
to the initial solution with which we began our investigation (less than 4%), and to human
novice performance (see Section 6.3).

Thus the communications predictions need not be

perfect, and the temporal knowledge need not be precise, in order to be useful.

6.2 Evaluating the Use of Communications Predictions
One key question about the use of the communications predictions is their sensitivity to loss
of observations. The ecacy of the technique (see Figure 5) stems from its capability to make
inferences based on an expected

future

observation. The predictions used in the previous

section assumed no observation loss, i.e., if a prediction stated that a particular message
was to be observed, than the probability assigned to this prediction was 1.0. But in settings
involving lossy observation streams, such inference will prove incorrect, as Overseer will
wait for the observation and will therefore not correctly monitor the actual state of teammembers.
To evaluate the predictions' sensitivity to observation loss, we chose three of the experimental runs, E, I, and J, which represent the extreme performance results of Overseer:
Run E had the lowest accuracy (72%), Run I had the highest (97%), and run J showed an
interesting reverse in relative performance of the
(see Figure 5).

Coherent, Temporal

and

Coherent, Comm

For each of these runs, we simulated observation loss at a rate of 10%,

repeating each trial three times with dierent random seeds. In other words, we ran a total
of 9 trials, in which a random 10% of the messages to be observed by Overseer were
not observable to Overseer (though they still reached the evacuation team-members
team-performance was identical to the original settings).

We then set the predictions to

appropriately use 90%10% settings: each expected message was predicted to appear with
0.9 probability (as opposed to 1.0 probability originally).
The results of these experiments are presented in Figure 7. For each of the three dierent
runs, two bars are presented. The left (shaded) bar shows the original results as presented in
the previous section (i.e., with no observation loss, and no treatment of possible loss in the
predictions). The right bar shows the average accuracy achieved by Overseer on the three
trials (for each run) in which 10% of the observations were not observable to Overseer.
The error-bars on the right bar mark the minimum and maximum accuracy values achieved
in the three trials for each run. Run I's error-bars are unseen since all three trials resulted
in the same accuracy.
There are a number of promising conclusions that can be drawn from these results.
First, in both runs E and I, Overseer's average accuracy dropped by less than 8%, i.e.,
the performance of Overseer dropped by less than the level of loss introduced. Indeed,
in run E, in which the original performance was the poorest, there was almost no change

121

fiKaminka, Pynadath, & Tambe

Average Accuracy

1
0.8
0.6
0.4
0.2
0

Run E

Run I

Run J

Figure 7: Comparison of average accuracy results with 0% and 10% observation losses.

in performance. Performance in run J did drop by slightly more than 10%, and that can
be at least partially explained by run J's previously discussed failures to exploit the communications predictions. Thus one promising conclusion to be drawn from these results is
that Overseer's performance can degrade gracefully, at a rate comparable to the rate of
degradation to Overseer's input.
A second conclusion is that Overseer's performance under observation-loss settings is
fairly invariant. Again, both run E and I, which can be considered normative, show very
little (if any) variance from one trial to the next, despite the change in the selection of
observations to be made unobserved from one trial to the next. Even run J, which is not
a representative of the normative runs, shows little variance with respect to its average
accuracy under observation loss.

This result suggests that while there may be a drop in

performance with observation loss (as expected), Overseer performs consistently under
varying lossy settings.

6.3

Overseer and Human Monitoring by Overhearing

Another important facet to the evaluation of Overseer examines its performance in comparison to that of novice and expert monitors of the evacuation application. This evaluation
sheds some light on the diculty of the monitoring task, and demonstrates that Overseer's performance is comparable (sometimes higher, sometimes lower) to human expert

performance, and signicantly better than that of novices.
To conduct this evaluation, we examined the same three runs representatives of Overseer's bounds on performance discussed above (runs E, I, and J). The rst author of this

paper served as an expert monitor, having as much experience in overhearing in the evacua-

122

fiMonitoring Teams by Overhearing
2

tion application as possible (and specically in the actual test runs E, I and J) . We established a group of novice monitors, made up from ve subjects who were generally familiar
with hierarchical control structures but unfamiliar with either monitoring by overhearing or
with the evacuation application or its component agents. Each subject was presented with
printed books (one for each run) containing the overheard messages (in human-readable
form), the same messages overheard by Overseer under optimal (lossless) conditions. As
reference material, each subject was given a copy of the plan-hierarchy, team-hierarchy, and
the same average duration information available to Overseer (the parameter

 for dierent

leaf plans). For each overheard message, a second line of print indicated the time passed
since overhearing the message, and the subject was asked to write down their best estimate
for the agents' current state (i.e., after the message was overheard and the specied time
passed). If they felt dierent agents or dierent sub-teams had dierent states, they were to
specify what each agent or subteam is doing. We emphasize that the subjects were presented
with exactly the same runs on which Overseer was evaluated.
The actual test process began with a short explanation of the task, with a full explanation of the plan-hierarchy (including answering any questions the subjects had about the
semantics of dierent transitions, etc.), and with a short test run which allowed each subject to use the plan-hierarchy and team hierarchy (but without providing any feedback as
to the subject's accuracy). Then, once all questions had been answered, the subjects were
presented with the test books and were given unlimited time to complete them, in any order
(though once started on a book, they were required to nish it). Subjects were not allowed to
look forward at the next message before completing their answer, nor to go back and change
their previous answers. They were encouraged to seek assistance in locating plans (in the
plan-hierarchy) consistent with messages, but no information was provided as to which plan
may be correct if two or more plans were consistent. After all subjects nished with all runs,
the test books were taken together and a score was computed. Each correct prediction was
awarded a 1, incorrect prediction a 0same standard used in scoring Overseer's accuracy. A non-specic answer (i.e., it was somewhere in the sub-tree of

Fly-Flight-Plan)

was considered to be an incorrect prediction, as subjects were repeatedly instructed that a
specic answer was required.
The results of the test are presented in Figure 8.

As in previous graphs, the Y-axis

denotes percentage of accurate monitoring hypotheses across all the data-points in a run
(2245, depending on the run). The X-axis has three categories, for the three dierent runs.
For each run, the left bar (marked Novices) presents the average accuracy achieved by
the novice monitors, the middle bar (marked Experts) presents the accuracy achieved by
the human expert monitor, and the nal bar (YOYO*) re-prints the results presented in
Figure 5 above. The results show that the average accuracy of the novices is clearly inferior
to that of the expert monitor and to Overseer. Overseer's performance is above that
of the human expert in runs I and J. However, the human expert does much better than
Overseer in run E.

We draw several conclusions from these results. First, the monitoring task Overseer
faced in the evacuation application is not trivial: The novices failed to achieve more than
2. We have had to settle for one expert since training an expert in this task is very time consuming and
requires much familiarity with the internals of the evacuation application as well as the TEAMCORE
architecture.

123

fiKaminka, Pynadath, & Tambe
70% on average (in their best run), and generally performed signicantly worse (by 15%
and more) than a human expert. Second, Overseer's performance in dierent runs was
comparable to that of the human expert (sometimes better, sometimes worse).

However,

Overseer's performance tended to follow the same trend as the novices. In other words,
Overseer's accuracy tended to go up and down on dierent runs in a similar manner

to that of the average novice human monitor, while the expert's accuracy remained fairly
constant across all runs.

1

0.8

Novices

0.6

Experts
0.4

YOYO*
0.2

0

I

J

E

Figure 8: Accuracy of human novice and expert monitors compared to Overseer.

6.4 Evaluating YOYO*'s Trading of Expressivity for Scalability
We examine a key trade-o between the expressivity and eciency involved in the planrecognition techniques we have presented. From the accuracy discussion above, it is clear
that coherence is a useful heuristic. YOYO* takes an extreme approach, strictly ruling out
reasoning about incoherences.

It is impossible for YOYO*, for instance, to represent an

incoherence in which two team-members are in disagreement about the plan executed by
the common team. It may thus be impossible for YOYO* to explicitly represent hypotheses associated with communication losses and delays, which cause such incoherences.

An

approach in which each individual is represented separately allows for such representation,
and in this respect is more expressive. However, with a few failure-checks in place,

is able to detect many incoherences, as previously discussed.

YOYO*

On the other hand, YOYO* oers signicant computational scalability with respect to
the number of agents monitored. Analysis of YOYO*'s complexity (in contrast to the array

124

fiMonitoring Teams by Overhearing
approach) was already presented in Section 5.2, and we follow it here with empirical evaluation. Figure 9 reports on the space requirement of YOYO* and the array-based approach
in three dierent domains: the evacuation domain, where YOYO* has been evaluated and
deployed, and two additional domains in which we have built multi-agent teamsModSAF
(Tambe et al., 1995; Calder, Smith, Courtemanche, Mar, & Ceranowicz, 1993) and RoboCup
(Tambe, Adibi, Al-Onaizan, Erdem, Kaminka, Marsella, & Muslea, 1999; Marsella, Adibi,
Al-Onaizan, Kaminka, Muslea, Tallis, & Tambe, 2001). YOYO* is currently being evaluated
in these domains, and while it has not yet been fully deployed there, we believe the partial
existing implementations are sucient to provide robust projections of the space savings
achieved in these domains.

We believe that such projected savings of implementation in

these two domains could provide a rough guide as to the savings that designers could expect
from deploying YOYO* in additional domains.
For each domain, Figure 9 compares the space requirements of the array-based approach
(left bar) with those of YOYO* (right bar).

In addition, the dark-shaded region on top

of each bar shows the space required for representing each additional agent in the two
approaches, under the assumption that no additional plans are added to the plan-hierarchy
as more agents are added. As discussed above, this assumption is favorable to the arraybased representation. The gure shows the signicant space savings achieved by YOYO*.
First, in representing the teams in their current size, YOYO*'s space requirements are
signicantly smaller.

Furthermore, YOYO*'s savings really shine when we examine the

scalability of the two approaches.

While the array-based approach requires at least the

amount of space shown in the gure as darkly-shaded area, YOYO*'s requirements grow
by one node with each additional agent. Its space requirements for representing additional
agents are so small, that they don't show in the gure.

Number of Nodes

900

600

Each
Additional
Agent

300

Current
Application

0

Array

YOYO*

Evacuation (11 Agents)

Array

YOYO*

RoboCup (11 Agents)

Array

YOYO*

ModSAF (3 Agents)

Figure 9: Empirical savings in applying YOYO* in the evacuation and other domains.
Earlier, in Section 5.2, we have analyzed YOYO*'s worst case run-time complexity, but
argued that this worst case behavior is very extreme, and cannot be sustained in practice
since it involves continuous communications among all agents, the infeasibility of which

125

fiKaminka, Pynadath, & Tambe
provided the motivation for exploring a plan-recognition approach.

As further evidence

for the average case, consider the evacuation application, where agents communicate on
average once every 20 state changes. In this application, agents communicate in parallel in
4 or 5 exchanges (out of dozens), but in all cases but one, such parallel communications all
referred to the same plan, thus still requiring only a single update in YOYO* (see discussion
in Section 5.2). Only once during task execution would 3 agents (out of 11) be expected to
communicate in parallel about dierent plans, a scenario still dierent than YOYO*'s worst
case scenario.
The average length of task execution in this domain is approximately 900 time-ticks.
The array approach would update the state of each agent, at each time tick, whether a
message would appear or not.

Thus its average complexity per time-tick is the same as

its worst-case, which is at least

O (M N 2 ).

For YOYO*, the average complexity would be

signicantly dierent: 899 out of 900 time-ticks it would result in an

O (M + H ) process, and

only one time (out of 900) it would be result in a process three times as expensive (updating
the state of 3 dierent agents). The worst case scenario did not occur at all in any of the
dierent runs.

7. Related Work
Aiello et al. (2001) present several benets to overhearing agent conversations. They suggest
that the overhearer may infer the intent of the agents engaged in conversations, and oer
specic suggestions for improving the agents' performance.

For instance, overhearing a

conversation between two agents about a keyword search on the web, the overhearer may
suggest alternative keywords to conduct the same search.

This work is closely related to

our research on Overseer, and indeed points out several potential additional benets of
overhearing technology. However, in contrast to our work, Aiello et al. do not address the
problem of intent- or plan-recognition. They do not present algorithms for inferring plans,
nor for disambiguating recognized plans.
Overseer diers from most previous work on plan-recognition in being focused on

monitoring multiple agents, not a single agent.

While previous work in multi-agent plan

recognition has either focused on exploiting explicit teamwork reasoning (e.g.,

Tambe,

1996), or explicitly reasoning about uncertainty when recognizing multi-agent plans (e.g.,
Devaney & Ram, 1998; Intille & Bobick, 1999), a key novelty in Overseer is that it
eectively blends these two threads together. We provide a detailed discussion below.
Like Overseer,
for inferring
heuristic.

RESCteam

team plans

RESCteam

(Tambe, 1996) reasons explicitly about team intentions

from observations, similarly to Overseer's use of the coherence

uses coherence to restrict the space requirements of the plan-library

used, similarly to YOYO*.

However, Overseer uses a more advanced teamwork model

(e.g., it can predict failure states and recovery actions), uses knowledge about procedures
used by a team (i.e., communication decisions), and also explicitly reasons about uncertainty
and time, allowing it to answer queries related to the likelihood of current and future team
plans (issues not addressed in

RESCteam ).

Indeed,

RESCteam

does not explicitly represent

ordering constraints between plans, and does not address scarce observations: It assumes
that observations are available that account for possible changes in the state of each of the
observed agents.

126

fiMonitoring Teams by Overhearing
Work such as (Devaney & Ram, 1998; Intille & Bobick, 1999) focuses on explicitly
addressing uncertainty in plan recognition in multi-agent contexts, but does not exploit
explicit notions of teamwork. Devaney and Ram (1998) use pattern matching to recognize
team-tactics in military operations. Their approach relies on team-plan libraries, veried by
domain experts, that combine the team- and plan-hierarchies; the organizational knowledge
is not explicitly represented in their technique.

Similarly, Intille and Bobick (1999) rely

entirely on coordination constraints among agents to recognize team-tactics in football, and
in this sense use a socially-attentive technique that prefers hypotheses in which agents are
maintaining their roles. Intille and Bobick's work uses a single structure for each dierent
recognized tactic. Both investigations use position trace data of the monitored human teams.
Our work diers from (Devaney & Ram, 1998; Intille & Bobick, 1999) in several ways.
First, these previous investigations have been applied in settings where observations are continuously available about each monitored agent. In contrast, Overseer is targeted towards

overhearing,

where limited observations are available, both in time, and in the number of

agents actually observed. Overseer introduces a number of novel techniques (such as the
communications predictions) which are useful in such settings. A second important dierence is the underlying representation used in reasoning. We introduce a novel representation
particularly suited for monitoring by overhearing, while Intille and Bobick rely on standard
belief networks, constructed in a particular way to support reasoning about spatial/temporal
coordination. Finally, the explicit use we make of teamwork and organizational structure
(the team-hierarchy) enables YOYO* in principle to reason about coordination and teamwork failures, where the previous monitoring techniques would fail to recognize the team's
actions (Intille & Bobick, 1999).
Huber (1996) reports on the use of probabilistic plan recognition in service of observationbased coordination in the Net-trek domain, and shows that agents using plan recognition
for coordination outperform agents using communications for coordination.

Huber takes

coordination to be cooperative actions on the part of the self- interested agents, e.g., joining
an agent in attacking a common enemy. Huber's work does not exploit any knowledge of
relationships between the agents to limit the computation or increase the accuracy. Huber's
system does allow for some uncertainty caused by missing observations, but in contrast to
our work, does not introduce specialized mechanisms (such as ours) to explicitly address
these.
Plan Recognition Bayesian Networks (PRBNs) (Charniak & Goldman, 1993) provide a
very general model for plan events, evidence, and inference. However, a PRBN is a static
Bayesian network, so it must include nodes for all plans and observations throughout the
execution of the plans. Therefore, instead of representing only the events of a single time
step (as in the DBNs described in Section 3.1), it must include nodes over all time steps.

M , over a nite time horizon
2
of T steps, the number of nodes in the network will be O (T N M ). Inference will have a
2
T
NM
space/time complexity exponential in the number of nodes, O (2
), which is prohibitive
over the lengths of execution found in our example domains (e.g., T = 900).
Therefore, for

N

agents, executing a plan hierarchy of size

The representation used by YOYO* is related to existing approaches to the modeling
of stochastic processes, in particular those used for probabilistic plan recognition.

The

representation we present perhaps most closely resembles Hidden Markov Models (HMMs)
(Rabiner, 1989), used for plan-recognition in (Han & Veloso, 1999). One could, in theory,

127

fiKaminka, Pynadath, & Tambe
represent the plan state of a team of agents within the unconstrained state space of an
HMM. However, the HMM state space would have to represent all possible combinations
of the individual plan states of the agents, so the size of the HMM state space would be
exponential in the number of agents and plans. Thus, the standard algorithms for HMM
inference would not be able to exploit the structure of the plan and team hierarchies, nor the
particular forms of evidence (as described in Section 3.2), in the way that we do in YOYO*.
Generalized versions of the HMM model (Ghahramani & Jordan, 1997; Jordan, Ghahramani,
& Saul, 1997) could more compactly represent the same state space as in YOYO*, but exact
inference is intractable for these models. These models have more ecient algorithms for
approximate inference, but these would have diculty with the determinism present in our
planning models.
Pynadath and Wellman report on the Probabilistic State-Dependent Grammar (PSDG)
model (2000) that avoids the full complexity of DBN inference by making simplifying assumptions appropriate for plan recognition. However, while PSDG can incorporate broader
classes of inference than YOYO*, it is intended for single-agent plan recognition, and does
not support concurrency in a general enough fashion for multi-agent plan recognition.
Goldman, Geib and Miller (1999) develop a conceptual model for Bayesian plan recognition which does include, as one of its key novelties, the ability to infer the plans of a single
agent from lack of observation of its action. However, Goldman et al. deal with a dierent
issue altogether than the one our communications predictions address.

Their framework

looks at a sequence of observations, in which an observation may be missing, but observations of actions following it appear. Their framework then allows inference that plans that
should have given rise to the missing observation can be ruled out as recognition hypotheses.

In contrast, our approach uses the communications predictions to make inference of

plan-steps that did not

yet

occur. Overseer probabilistically expects the predictions to

come true, and does not infer additional information from a missing (predicted) observation
that is followed by another. In addition, our approach is fully implemented and deployed in
multi-agent settings, rather than single agent.
A complementary line of work (in the context of the TEAMCORE architecture) has
focused on

intended

plan-recognition for monitoring, where team-members may adapt their

communications such that monitoring is made easier (Tambe et

al., 2000).

This work

(i) reduced, but did not eliminate uncertainty, and (ii) did not present any methods to
address uncertainty, as we do here, However, it presents an interesting future direction for
Overseer's development.

8. Summary and Future Work
This paper introduced monitoring by overhearing, a technique that will be increasingly
important with the growing need to monitor agent systems, particularly distributed or deployed. We presented Overseer, a system for monitoring teams by overhearing the routine
communications team-members exchange as part of the execution of their joint tasks. Monitoring by overhearing, while being a plan-recognition task, presents characteristic challenges
not previously addressed. These include the scarcity of observations compared to the rate
of change in agent's state, and the fact that agents are not individually observable, as the
observations are essentially of multi-agent actions. In addition to these, familiar challenges

128

fiMonitoring Teams by Overhearing
such as demanding response times and maintaining performance in face of a scale-up in the
number of monitored agents, are also present.
To address these challenges, Overseer employs a number of novel techniques, which
exploit knowledge of the relationships between the agents to alleviate uncertainty and increase eciency of monitoring: (i) An ecient probabilistic algorithm for plan-recognition,
particularly suited for monitoring communications; (ii) YOYO*, an approach for ecient
maintenance of recognition of coherent hypotheses; and (iii) use of social structures and
procedures, e.g., team coherence and communications to maintain coherence, to alleviate
uncertainty. To demonstrate the generality of these techniques, we have discussed the potential use of these techniques with representations other than a plan-hierarchy, in particular
DBNs (Kjrul, 1992).
We provided an in-depth empirical evaluation of these techniques in one of the domains
in which Overseer is applied. The evaluation carefully examines the contribution of each
technique to the overall recognition success, and demonstrates that these techniques work
best together, as they complement relative weaknesses of each other. The paper also presented an evaluation of the scalability of YOYO*, and its performance under conditions of
observation loss. Finally, we presented a comparison of Overseer's performance with that
of human expert and novice monitors, and demonstrated that Overseer performance is
comparable to that of human experts, despite the diculty of the monitoring task.
Several opportunities for future research directions arise from the experimental results.
First, the use of rote-learning to predict when messages will be observed (provided as feasibility demonstration), proved eective for normative runs. However, the simple mechanism
was damaging when rare patterns of communications arose, as some of the experiments have
shown. In-depth exploration of the role of learning is therefore one of the directions we hope
to pursue in the future.

In addition, learning mechanisms that can derive plan-hierarchy

and team-hierarchy structures from records of conversations are also of much interest.

Acknowledgements
This paper is based in part on an Agents-2001 paper by the same authors (Kaminka, Pynadath, & Tambe, 2001). Parts of this research were carried out while the rst author was
a Post Doctorate Fellow at the Computer Science Department, Carnegie Mellon University.
We thank Manuela Veloso for her enthusiastic support of this project at Carnegie Mellon
University, and we thank Yves Lesprance, Victor Lesser, George Bekey, Je Rickel, and
Dan O'Leary for useful comments. Oshra Kaminka deserves special thanks for her help in
analyzing and processing the data. This research was supported by DARPA awards F3060298-2-0108, F30602-98-2-0135, and F30602-00-2-0549, managed by the Air Force Research
Labs/Rome site.

Appendix A. Additional algorithms and proofs
This appendix contains the pseudo-code for all algorithms described in the paper, for which
pseudo-code was not provided in the body of the text itself. These include the modications
to the propagation procedures necessary for propagation within YOYO*. In addition, we

129

fiKaminka, Pynadath, & Tambe
provide a proof that the number of coherent hypotheses for
the plan-library

M.

N

agents is linear in the size of

A.1 The Number of Incoherent and Coherent Hypotheses
Let

Mi

be the monitoring plan-library for agent

i; 1

 i  N.

monitoring system reasons about monitoring hypotheses in

Mi

as the nite set of all possible plans agent

i

Mi .

When monitoring agent

i,

a

In other words, we can view

may be executing. Given a query as to the

agent's current state by the monitoring system, the plan-recognition algorithm picks some

ki

Mi as hypotheses
mi where jmi j = ki .

specic members of

of hypotheses

as to the current state of the agentcall these sets

To construct an overall team hypothesis, the monitoring system must combine the individual hypotheses to form a hypothesis for the team's state. For each agent i, the monitoring
system chooses one individual hypothesis

hi

2 mi. The combination of these forms the team

state hypothesis. If there is no uncertainty about the state of any of the agent, i.e.,
for all

i,

then one team hypothesis exists.

ki

=1

However, if uncertainty exists about the state

agents, then clearly, the process of selecting individual hypotheses becomes combinatorial
in nature, as all possible combinations of all individual hypotheses are possible in principle.
Let us consider how many coherent hypotheses exist. If we restrict ourselves to coherent
hypotheses, then the selection of individual hypotheses for each agent are constrained such
that the selections are in agreementthe same individual hypothesis is selected for each
agent. Given a selection of an individual state hypothesis
must choose

h1

=

h2

=

h2
h3

2m

=

:::

2 for the second agent,

=

hN .

h3

2m

h1

2m

1 for the rst agent, we

3 for the third agent, etc., such that

Since there are not more than

k1

 jM j individual
1

state

hypotheses for the rst agent, it follows that the number of coherent team-state hypotheses
is bounded by

jM j, i.e., the size of the plan library for the agents.
1

coherent hypotheses is bounded by

In fact, the number of

minki since only members of mmin ki can be matched

m. In contrast, by denition, all other
k1  k2  k3  :::  kN (min ki ) of these

with members of the other individual hypothesis sets,
team-state hypotheses are incoherent. There will be
hypotheses.

A.2 YOYO* Propagation Algorithms (Section 5.1)
The algorithms presented in this section support those presented in the main text of the
paper, and are provided here for completeness.

Some of them may contain a step which

iterates over all teams that can take an outgoing transition (e.g., line 1 of algorithm 6, or
line 13 of algorithm 7). This step requires some further clarication: When iterating over
all outgoing teams that meet the condition, the algorithm consults the team-hierarchy to
carry out the iteration only for the

topmost

teams (in terms of the team-hierarchy) that

meet the condition. For instance, in our application domain, the team TASK-FORCE has
(among others) two subteams TRANSPORTS and ESCORTS. If a transition is allowed
to be taken by TRANSPORTS only, then an iteration over all teams that are allowed to
take the transition will not consider either ESCORTS or TASK-FORCE. However, if the
transition allows TASK-FORCE, then the iteration step will take place only onceit will
be executed once for the team TASK-FORCE, which is the parent team for TRANSPORTS
and ESCORTS.

130

fiMonitoring Teams by Overhearing

Algorithm 6 Team-Propagate-Down(plan Y , probability , beliefs, b, plans M )
1:

for all

Y

2:
3:
4:
5:
6:

teams T who are allowed to take an outgoing hierarchical-decomposition transition from

do

fc j c 2 M; c rst child of Y; c is to be taken by team T g
= j CT j
for all plans c 2 CT do
bt+1 (Y; :block )
bt+1 (Y; :block ) + 0
(c; 0 ; b; M )

CT
0

Team-Propagate-Down

Algorithm 7 Team-Propagate-Forward(team-hierarchy H , beliefs b, plans M )
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

22:
23:

plans X 2 M do
bt+1 (X; :block )
0:0
bt+1 (X; block )
0:0
outx
0:0
x
0:0
for all plans X 2 M in post-order
for all

if

X is a leaf

outx

else

then

bt (X; :block )(1

{X is a parent}

do

{children in temporal order before parents}

e x ) {calculate probability of X terminating at time t}

outx is known { because post-order guarantees all children set it in line 21}
for all temporal outgoing transitions Tx!y from X do
x
x + (1 xy )xy
for all teams E who are allowed to take a temporal outgoing transition do
if x > 0 then {some transition can be taken}
for all temporal outgoing transitions Tx!y from X to be taken by E do

outx (1 xy )xy
if Tx!y leads to a successor plan Y then
bt+1 (Y; :block )
bt+1 (Y; :block ) + 
(Y; ; b; M )
else {Tx!y is a terminating transition}
outparent(x)
outparent(x) + (1 xy )xy {parent's outgoing probability is its chil-

Team-Propagate-Down
dren's}

bt+1 (X; block )
bt+1 (X; block ) + outx x
bt+1 (X; :block )
bt+1 (X; :block ) outx

131

fiKaminka, Pynadath, & Tambe
Algorithm 8 below may require some clarications. First, it is important to note that the
plans

Y

(line 1) are traversed in pre-orderparents before children. The scaling calculation

depends on the parent having the scaled probability. Second, the iteration over sub-plans

Y

essentially captures all plans in the subtree rooted in the parent plan

in the subtree rooted by

P 's

child

X,

X 's

except for those

which already has been adjusted by YOYO* prior to

the call to this algorithm. In fact, the use of
sure that any of

P,

X 's

team

siblings, that are alternatives to

X

T

to scale only other plans makes

for the team

T,

do not get scaled.

This is correct because this procedure is called when incorporating evidence for

X

(rather

than any of its siblings).

Algorithm 8 Scale(parent plan P , team T , child plan X , beliefs b)
1:
2:
3:

subplans Y of P , where team(Y ) 6= T , in pre-order do
bt (Y;:block)
b (parent(Y ); :block )
bt+1 (Y; :block )
bt+1 (Y; :block )+ bt (parent
(Y );:block) t+1
bt (Y;block)
bt+1 (Y; block )
bt+1 (Y; block )+ bt (parent(Y );:block) bt+1 (parent(Y ); :block )

for all

References
Ontological overhearing. In Intelligent Agents VIII, Proceedings of the international workshop on Agents, Theories,
Architectures, and Languages (ATAL-2001).

Aiello, M., Busetta, P., Dona, A., & Serani, L. (2001).

Barber, K. S., & Martin, C. E. (2001). Dynamic reorganization of decision-making groups. In

Proceedings of the Fifth International Conference on Autonomous Agents (Agents-01),
pp. 513520. ACM Press.

Calder, R. B., Smith, J. E., Courtemanche, A. J., Mar, J. M. F., & Ceranowicz, A. Z. (1993).

Modsaf behavior simulation and control. In Proceedings of the Third Conference on
Computer Generated Forces and Behavioral Reresentation Orlando, Florida. Institute
for Simulation and Training, University of Central Florida.

Charniak, E., & Goldman, R. P. (1993). A Bayesian model of plan recognition.

Intelligence, 64 (1), 5379.

Articial

Cohen, P. R., Johnston, M., McGee, D., Oviatt, S., Pittman, J., Smith, I., Chen, L., & Clow,
J. (1997). Quickset: Multimodal interaction for distributed applications. In

Proceedings

of the Fifth Annual International Multimodal Conference (Multimedia '97), pp. 3140.
Cohen, P. R., & Levesque, H. J. (1990). Rational interaction as the basis for communication.

In Cohen, P. R., Morgan, J., & Pollack, M. E. (Eds.),

Intentions in Commu-

nication, Systems Development Foundation Benchmark Series, chap. 12, pp. 221255.
MIT Press.

Nous, 35.
Decker, K. (1995). Environment Centered Analysis and Design of Coordination Mechanisms.

Cohen, P. R., & Levesque, H. J. (1991). Teamwork.

Ph.D. thesis, Department of Computer Science, University of Massachusetts, Amherst.
Devaney, M., & Ram, A. (1998). Needles in a haystack: Plan recognition in large spatial

Proceedings of the Fifteenth National Conference
on Articial Intelligence (AAAI-98), pp. 942947 Madison, WI.
domains involving multiple agents. In

132

fiMonitoring Teams by Overhearing
Dunin-Keplicz, B., & Verbrugge, R. (2001).

The role of dialogue in collective problem

Proceedings of fth International Symposium on the Logical Formalization
of Commonsense Reasoning (Commonsense 2001), pp. 89104.
solving. In

Finin, T., Labrou, Y., & Mayeld (1997). KQML as an agent communication language. In
Bradshaw, J. (Ed.),

Software Agents. MIT Press.

Ghahramani, Z., & Jordan, M. I. (1997). Factorial hidden Markov models.

29, 245275.

Goldman, R. P., Geib, C. W., & Miller, C. A. (1999).
In

Machine Learning,

A new model of plan recognition.

Proceedings of the Conference on Uncertainty in Articial Intelligence (UAI-1999)

Stockholm, Sweden.
Grosz, B. (1996). Collaborating systems.

AI Magazine, 17 (2).

Grosz, B. J., & Kraus, S. (1999). The evolution of SharedPlans. In Wooldridge, M., & Rao,
A. (Eds.),

Foundations and Theories of Rational Agency, pp. 227262.

Grosz, B. J., & Kraus, S. (1996). Collaborative plans for complex group actions.

Intelligence, 86, 269358.

Articial

Han, K., & Veloso, M. (1999). Automated robot behavior recognition applied to robotic soccer. In

Proceedings of the IJCAI-99 Workshop on Team Behavior and Plan-Recognition.

Also appears in Proceedings of the 9th International Symposium of Robotics Research
(ISSR-99).
Horling, B., Benyo, B., & Lesser, V. (2001).

Using self-diagnosis to adapt organizational

Proceedings of the Fifth International Conference on Autonomous Agents
(Agents-01), pp. 529536.
Huber, M. J. (1996). Plan-Based Plan Recognition Models for the Eective Coordination of
Agents Through Observation. Ph.D. thesis, University of Michigan.
structures. In

Huber, M. J., & Hadley, T. (1997). Multiple roles, multiple teams, dynamic environment:

Proceedings of the First International Conference on Autonomous Agents (Agents-97), pp. 332339 Marina del Rey,
Autonomous netrek agents. In Johnson, W. L. (Ed.),
CA. ACM Press.

Intille, S. S., & Bobick, A. F. (1999). A framework for recognizing multi-agent action from

In Proceedings of the Sixteenth National Conference on Articial
Intelligence (AAAI-99), pp. 518525. AAAI Press.
visual evidence.

Jennings, N. R. (1993). Commitments and conventions: the foundations of coordination in
multi-agent systems.

Knowledge Engineering Review, 8 (3), 223250.

Jennings, N. R. (1995). Controlling cooperative problem solving in industrial multi-agent
systems using joint intentions.

Articial Intelligence, 75 (2), 195240.

Jordan, M. I., Ghahramani, Z., & Saul, L. K. (1997).

Hidden Markov decision trees.

Mozer, M. C., Jordan, M. I., & Petsche, T. (Eds.),

Processing Systems, Vol. 9, p. 501. The MIT Press.

In

Advances in Neural Information

Kaminka, G. A., Pynadath, D. V., & Tambe, M. (2001). Monitoring deployed agent teams.

Proceedings of the Fifth International Conference on Autonomous Agents (Agents01), pp. 308315.
In

133

fiKaminka, Pynadath, & Tambe
Kaminka, G. A., & Tambe, M. (2000).
monitoring.

Robust multi-agent teams via socially-attentive

Journal of Articial Intelligence Research, 12, 105147.

Kinny, D., Ljungberg, M., Rao, A., Sonenberg, E., Tidhar, G., & Werner, E. (1992). Planned
team activity.

In Castelfranchi, C., & Werner, E. (Eds.),

Articial Social Systems,

Lecture notes in AI 830, pp. 227256. Springer Verlag, New York.
Kjrul, U. (1992).

A computational scheme for reasoning in dynamic probabilistic net-

Proceedings of the Conference on Uncertainty in Articial Intelligence (UAI1992), pp. 121129 San Mateo, CA. Morgan Kaufmann.

works. In

Knoblock, C. A., Minton, S., Ambite, J. L., Ashish, N., Modi, P. J., Muslea, I., Philpot,
A. G., & Tejada, S. (1998).

Modeling Web sources for information integration.

In

Proceedings of the Fifteenth National Conference on Articial Intelligence (AAAI-98).
Kumar, S., & Cohen, P. R. (2000). Towards a fault-tolerant multi-agent system architecture.

Proceedings of the Fourth International Conference on Autonomous Agents (Agents00), pp. 459466 Barcelona, Spain. ACM Press.
In

Kumar, S., Cohen, P. R., & Levesque, H. J. (2000). The adaptive agent architecture: Achiev-

Proceedings of the Fourth International Conference on Multiagent Systems (ICMAS-00), pp. 159166 Boston, MA.
ing fault-tolerance using persistent broker teams. In
IEEE Computer Society.

Lenser, S., Bruce, J., & Veloso, M. (2001).
tonomous legged soccer robots. In

Cmpack: A complete software system for au-

Proceedings of the Fifth International Conference

on Autonomous Agents (Agents-01), pp. 204211. ACM Press.

Lesh, N., Rich, C., & Sidner, C. L. (1999). Using plan recognition in human-computer collaboration. In

Proceedings of the Seventh International Conference on User Modelling

(UM-99) Ban, Canada.

Levesque, H. J., Cohen, P. R., & Nunes, J. H. T. (1990). On acting together. In

of the Eigth National Conference on Articial Intelligence (AAAI-90)

Proceedings

Menlo-Park,

CA. AAAI Press.
Marsella, C. S., Adibi, J., Al-Onaizan, Y., Kaminka, G. A., Muslea, I., Tallis, M., & Tambe,
M. (2001). On being a teammate: Experiences acquired in the design of robocup teams.

Journal of Autonomous Agents and Multi-Agent Systems, 4 (12).
Martin, D. L., Cheyer, A. J., & Moran, D. B. (1999).

The open agent architecture: A

framework for building distributed software systems.

13 (1-2), 92128.

Applied Articial Intelligence,

Ndumu, D. T., Nwana, H. S., Lee, L. C., & Collis, J. C. (1999). Visualizing and debugging

Proceedings of the Third International Conference
on Autonomous Agents (Agents-99). ACM Press.
distributed multi-agent systems. In

Payne, T. R., Sycara, K., Lewis, M., Lenox, T. L., & Hahn, S. (2000).

Varying the user

Proceedings of the Fourth International
Conference on Autonomous Agents (Agents-00), pp. 412418.
interaction within multi-agent systems.

In

Pechoucek, M., Marik, V., & Stepankova, O. (2000).

Role of acquaintance models in an

agent-based production planning. In Klusch, M., & Kerschberg, L. (Eds.),

134

Cooperative

fiMonitoring Teams by Overhearing
Information Agents IV, Proceedings of the Fourth International Workshop (CIA-2000),
No. 1860 in LNAI, pp. 179190. Springer Verlag.
Pechoucek, M., Marik, V., & Stepankova, O. (2001). Towards reducing communication trac
in multi-agent systems.

Journal of Applied System Studies.

Pynadath, D. V., & Wellman, M. P. (2000). Probabilistic state-dependent grammars for plan
recognition. In

Proceedings of the Conference on Uncertainty in Articial Intelligence

(UAI-2000), pp. 507514.

Rabiner, L. R. (1989). A tutorial on Hidden Markov Models and selected applications in

Proceedings of the IEEE, 77 (2), 257286.
Reed, C. (1998). Dialogue frames in agent communications. In Proceedings of the Third
International Conference on Multiagent Systems (ICMAS-98), pp. 246253.
speech recognition.

Rich, C., & Sidner, C. L. (1997).

COLLAGEN: When agents collaborate with people.

Proceedings of the First International Conference on Autonomous Agents (Agents-97), pp. 284291 Marina del Rey, CA. ACM Press.
Tambe, M. (1996). Tracking dynamic team activity. In Proceedings of the National Conference on Articial Intelligence (AAAI).
Tambe, M. (1997). Towards exible teamwork. Journal of Articial Intelligence Research,
7, 83124.
In Johnson, W. L. (Ed.),

Tambe, M., Adibi, J., Al-Onaizan, Y., Erdem, A., Kaminka, G. A., Marsella, S. C., &
Muslea, I. (1999). Building agent teams using an explicit teamwork model and learning.

Articial Intelligence, 111 (1), 215239.

Tambe, M., Johnson, W. L., Jones, R., Koss, F., Laird, J. E., Rosenbloom, P. S., & Schwamb,
K. (1995).

16 (1).

Intelligent agents for interactive simulation environments.

AI Magazine,

Tambe, M., Pynadath, D. V., Chauvat, N., Das, A., & Kaminka, G. A. (2000). Adaptive

In Proceedings of
the Fourth International Conference on Multiagent Systems (ICMAS-00), pp. 301308

agent integration architectures for heterogeneous team members.
Boston, MA.

Tidhar, G. (1993a). Team oriented programming: Preliminary report. Tech. rep. 41, Australian Articial Intelligence Institute, Melbourne, Australia.
Tidhar, G. (1993b). Team oriented programming: Social structures. Tech. rep. 47, Australian
Articial Intelligence Institute, Melbourne, Australia.
Vercouter, L., Beaune, P., & Sayettat, C. (2000).

Towards open distributed information

Working Notes of
the AAAI-2000 Workshop on Agent-Oriented Information Systems (AOIS-2000), pp.

systems by the way of a multi-agent conception framework.
2938.

135

In

fiJournal of Artificial Intelligence Research 17 (2002) 35-55

Submitted 12/01; published 8/02

Inferring Strategies for Sentence Ordering in Multidocument
News Summarization
Regina Barzilay
Noemie Elhadad
Kathleen R. McKeown

regina@cs.columbia.edu
noemie@cs.columbia.edu
kathy@cs.columbia.edu

Columbia University, Computer Science Department
1214 Amsterdam Ave
New York, 10027, NY, USA

Abstract
The problem of organizing information for multidocument summarization so that the
generated summary is coherent has received relatively little attention. While sentence
ordering for single document summarization can be determined from the ordering of sentences in the input article, this is not the case for multidocument summarization where
summary sentences may be drawn from different input articles. In this paper, we propose
a methodology for studying the properties of ordering information in the news genre and
describe experiments done on a corpus of multiple acceptable orderings we developed for
the task. Based on these experiments, we implemented a strategy for ordering information
that combines constraints from chronological order of events and topical relatedness. Evaluation of our augmented algorithm shows a significant improvement of the ordering over
two baseline strategies.

1. Introduction
Multidocument summarization poses a number of new challenges over single document summarization. Researchers have already investigated issues such as identifying repetitions or
contradictions across input documents and determining which information is salient enough
to include in the summary (Barzilay, McKeown, & Elhadad, 1999; Carbonell & Goldstein,
1998; Elhadad & McKeown, 2001; Mani & Bloedorn, 1997; McKeown, Klavans, Hatzivassiloglou, Barzilay, & Eskin, 1999; Radev & McKeown, 1998; White, Korelsky, Cardie, Ng,
Pierce, & Wagstaff, 2001). One issue that has received little attention is how to organize
the selected information so that the output summary is coherent. Once all the relevant
pieces of information have been selected across the input documents, the summarizer has
to decide in which order to present them so that the whole text makes sense. In single
document summarization, one possible ordering of the extracted information is provided by
the input document itself. However, Jing (1998) observed that, in single document summaries written by professional summarizers, extracted sentences do not always retain their
precedence orders in the summary. Moreover, in the case of multiple input documents, this
does not provide a useful solution: information may be drawn from different documents and
therefore, no single document can provide an ordering. Furthermore, the order between two
pieces of information can change significantly from one document to another.
In this paper, we provide a corpus based methodology for studying ordering. Our goal
was to develop a good ordering strategy in the context of multidocument summarization
c
2002
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBarzilay, Elhadad & McKeown

targeted for the news genre. The first question we addressed is the importance of ordering. We conducted experiments which show that ordering significantly affects the readers
comprehension of a text. Our experiments also show that although there is no single ideal
ordering of information, ordering is not an unconstrained problem; the number of good orderings for a given text is limited. The second question addressed was the analysis and use
of data to infer a strategy for ordering. Existing corpus based methods, such as supervised
learning, are not easily applicable to our problem in part because of lack of training data.
Given that there are multiple possible orderings, a corpus providing one ordering for each
set of information does not allow us to differentiate between sentences which must be together and sentences which happen to be together. This led us to develop a corpus of data
sets, each of which contains multiple acceptable orderings of a single text. Such a corpus
is expensive to construct and therefore, does not provide enough data for pure statistical
approaches. Instead, we used a hybrid corpus analysis strategy that first automatically identifies commonalities across orderings. Manual analysis of the resulting clusters led to the
identification of constraints on ordering. Finally, we evaluated plausible ordering strategies
by asking humans to judge the results.
Our set of experiments together suggests an ordering algorithm that integrates constraints from an approximation of the temporal sequence of the underlying events and
relatedness between content elements. Our evaluation of plausible strategies measured the
usefulness of a Chronological Ordering algorithm used in previous summarization systems
(McKeown et al., 1999; Lin & Hovy, 2001) as well as an alternative, original strategy,
Majority Ordering. Our evaluation showed that the two ordering algorithms alone do not
yield satisfactory results. The first, Majority Ordering, is critically linked to the level of
similarity of information ordering across the input texts. When input texts have different
orderings, however, the algorithm produces unpredictable and unacceptable results. The
second, Chronological Ordering produces good results when the information is event-based,
and therefore, is temporally sequenced. When texts do not refer to events, but describe
states or properties, this algorithm falls short.
Our automatic analysis revealed that topical relatedness is an important constraint;
groups of related sentences tend to appear together. Our algorithm combines Chronological
Ordering with constraints from topical relatedness. Evaluation shows that the augmented
algorithm significantly outperforms either of the simpler methods alone. This strategy can
be characterized as bottom-up since final ordering of the text emerges from how the data
groups together, whether by related content or by chronological sequence. This contrasts
with top-down strategies such as RST (Moore & Paris, 1993; Hovy, 1993), schemas (McKeown, 1985) or plans (Dale, 1992) which impose an external, rhetorically motivated ordering
on the data.
In the following sections, we first show that the way information is ordered in a summary
can critically affect its overall quality. We then give an overview of our summarization
system, MultiGen. We next describe the two naive ordering algorithms and evaluate
them, followed by a study of multiple orderings produced by humans. This allows us
to determine how to improve the Chronological Ordering algorithm using cohesion as an
additional constraint. The last section describes the augmented algorithm along with its
evaluation.
36

fiSentence Ordering in Multidocument News Summarization

2. Impact of Ordering on the Overall Quality of a Summary
Even though the problem of ordering information for multidocument summarization has
received relatively little attention, we hypothesize that good ordering is crucial to produce
summaries of quality. The consensus architecture of the state of the art summarizers consists
of a content selection module in which the salient information is extracted and a regeneration
module in which the information is reformulated into a fluent text. Ideally, the regeneration
component contains devices that perform surface repairs on the text by doing anaphora
resolution, introducing cohesion markers or choosing the appropriate lexical paraphrases.
Our claim in this paper is that the multidocument summarization architecture needs an
explicit ordering component. If two pieces of information extracted by the content selection
phase end up together but should not, in fact, be next one to another, surface devices will
not repair the impaired flow of information in the summary. An ordering strategy would
help avoid this situation.
It is clear that ordering cannot improve the output of earlier stages of a summarizer,
among them content selection1 ; however, finding an acceptable ordering can enhance user
comprehension of the summary and, therefore, its overall quality. Of course, surface devices
are still needed to smooth the output summary, but this is out of the scope of this paper (but
see (Schiffman, Nenkova, & McKeown, 2002)). In this section we show that the quality of
ordering has a direct effect on user comprehension of the summary. To verify our hypothesis,
we performed an experiment, measuring the impact of ordering on the users comprehension
of summaries.
We selected ten summaries produced by the Columbia Summarization system (McKeown, Barzilay, Evans, Hatzivassiloglou, Kan, Schiffman, & Teufel, 2001). It is composed
of a router and two underlying summarizers  MultiGen and DEMS (Difference Engine
for Multidocument Summarization). Depending on the type of input articles to be summarized, the router selects the appropriate summarizer. We evaluated this system through the
Document Understanding Conference 2001 (DUC) 2 evaluation, where summaries produced
by several systems were graded by human judges according to different criteria, among
them how well the information contained in the summary is ordered. To actually identify a
possible impact of ordering on comprehension, we selected only summaries where humans
judged the ordering as poor.3 For each summary, we manually reordered the sentences
generated by the summarizer, using the input articles as a reference. When doing so, we
did not change the content  all the sentences in the reordered summaries were the same
ones as in the originally produced summaries. This process yields ten additional reordered
summaries and thus, overall our collection contains twenty summaries.
Two subjects other than the authors participated in this experiment. Each summary
was read by one participant without having access to the input articles. We distributed the
summaries among the judges so that none of them read both an original summary and its
reordering. They were asked to grade how well the summary could be understood, using
the ratings Incomprehensible, Somewhat comprehensible or Comprehensible.
1. No information is added or deleted once the content selection is performed.
2. http://www-nlpir.nist.gov/projects/duc/
3. The selected summaries were produced by the DEMS system. We didnt select any summary produced
by MultiGen because it implemented our ordering algorithm at the time. DEMS on the other hand,
had no specific ordering strategy implemented and thus provided us with the appropriate type of data.

37

fiBarzilay, Elhadad & McKeown

The results are shown in Figure 14 . Seven original summaries were considered incomprehensible by their judge, two were somewhat comprehensible, and only one original summary
was fully comprehensible. The reordered summaries obtained better grades overall  five
summaries were fully comprehensible, two were somewhat comprehensible, while three remained incomprehensible. To assess the statistical significance of our results, we applied
the Fisher exact test to our data set, conflating Incomprehensible and Somewhat comprehensible summaries into one category to obtain a 2x2 table. This test is adapted to
our case because of the reduced size of our data set. We obtained a p-value of 0.07 (Siegal
& Castellan, 1988), which means that if reordering is not, in general, helpful, there is only
a 7% chance that doing reordering anyway would produce a result this different in quality
from the original ordering. This experiment indicates that a good ordering can improve the
overall comprehensibility of a summary.
Summary set
d13
d19
d24
d31
d32
d39
d45
d50
d54
d56

Original
Incomprehensible
Somewhat comprehensible
Incomprehensible
Somewhat comprehensible
Incomprehensible
Incomprehensible
Incomprehensible
Incomprehensible
Incomprehensible
Comprehensible

Reordered
Incomprehensible
Comprehensible
Comprehensible
Comprehensible
Somewhat comprehensible
Incomprehensible
Incomprehensible
Comprehensible
Somewhat comprehensible
Comprehensible

Figure 1: Impact of ordering on the user comprehension of summaries.

In the case of some low-scoring summaries, it is clear that poor ordering is the likely
culprit. For instance, readers can easily identify that grouping the two following sentences
is an unsuitable choice and could be misleading. Miss Taylors health problems started with
a fall from a horse when she was 13 and filming the movie National Velvet. The recovery of
Elizabeth Taylor, near death two weeks ago with viral pneumonia, was complicated by a yeast
infection, her doctors said Friday. But in other cases, when information in a summary is
poorly ordered and readers cannot make sense of the text, we observed through interviews
with the readers that they tend to blame it on content selection rather than on ordering,
even if the content is not the issue. Thus, the issue of ordering is not isolated; it can affect
the overall quality of a summary.

3. MultiGen Overview
Our framework is the MultiGen system (McKeown et al., 1999), a multidocument summarizer which has been trained and tested on news articles. MultiGen is part of the
Columbia Summarization System. It operates on a set of news articles describing the same
4. The set names are the ones used in the DUC evaluation.

38

fiSentence Ordering in Multidocument News Summarization

event, creating a summary which synthesizes common information across documents. The
system runs daily over real data within Newsblaster 5 , a tool which collects news articles
from multiple sources, organizes them into topical clusters and provides a summary for each
of the clusters.
In the case of multidocument summarization of articles about the same event, source
articles can contain both repetitions and contradictions. Extracting all the similar sentences would produce a verbose and repetitive summary, while extracting only some of the
similar sentences would produce a summary biased towards some sources. MultiGen uses
a comparison of extracted similar sentences to select the appropriate phrases to include in
the summary and reformulates them as new text.
MultiGen consists of an analysis and a generation component. The analysis component (Hatzivassiloglou, Klavans, & Eskin, 1999) identifies units of text which convey similar
information across the input documents using statistical techniques and shallow text analysis. Once similar text units are identified, we cluster them into themes. Themes are sets of
sentences from different documents that contain repeated information and do not necessarily contain sentences from all the documents (see two examples of themes in Figure 2). For
each theme, the generation component (Barzilay et al., 1999) identifies phrases which are
in the intersection of the theme sentences and selects them as part of the summary. The
intersection sentences are then ordered to produce a coherent text. At the end, for each
theme there will be a single corresponding generated output sentence in the summary. In
the following section, we describe different strategies for ordering the output sentences to
obtain a quality summary.
Theme 1
Mr. Salvi, 24, apparently killed himself in his prison cell last November.
The state wouldnt execute him for killing two abortion clinic workers in 1994, so John
C. Salvi III took his own life.
John C. Salvi III, who was convicted of killing two people in a shooting spree on two
abortion clinics in 1994, killed himself in prison.
Theme 2
His attorneys said he attempted suicide twice before in prison.
His lawyers said that he twice had tried to commit suicide in jail, a charge authorities
have denied.
Figure 2: Two themes with their corresponding sentences. Theme 2 contains sentences from only
two articles, while Theme 1 contains sentences from three input articles.

4. Naive Ordering Algorithms Are Not Sufficient
When producing a summary, any multidocument summarization system has to choose in
which order to present the output sentences. In this section, we describe two algorithms
5. http://www.cs.columbia.edu/nlp/newsblaster

39

fiBarzilay, Elhadad & McKeown

for ordering sentences suitable for multidocument summarization in the news genre. The
first algorithm, Majority Ordering (MO), relies only on the original orders of sentences
in the input documents. The second one, Chronological Ordering (CO), uses time-related
features to order sentences. This strategy was originally implemented in MultiGen and
followed by other summarization systems (Radev, Jing, & Budzikowska, 2000; Lin & Hovy,
2001). In the MultiGen framework, ordering sentences is equivalent to ordering themes,
and we describe the algorithms in terms of themes. This makes sense because, ultimately,
the summary will be composed of a sequence of sentences, each one constructed from the
information in one theme. Our evaluation shows that these methods alone do not provide
an adequate strategy for ordering.
4.1 Majority Ordering
4.1.1 The Algorithm
In single document summarization, the order of sentences in the output summary is typically
determined by their order in the input text. This strategy can be adapted to multidocument
summarization. Consider two themes, T h 1 and T h2 ; if sentences from T h1 precede sentences
from T h2 in all input texts, then presenting T h 1 before T h2 is likely to be an acceptable
order. To use the majority ordering algorithm when the order between sentences from T h 1
and T h2 varies from one text to another, we must augment the strategy. One way to define
the order between T h1 and T h2 is to adopt the order occurring in the majority of the
texts where T h1 and T h2 occur. This strategy defines a pairwise order between themes.
However, this pairwise relation is not necessarily transitive. For example, given the themes
T h1 , T h2 and T h3 and the following situation: T h1 precedes T h2 in a text, T h2 precedes
T h3 in the same text or in another text, and T h 3 precedes T h1 in yet another text; there is a
conflict between the orders (T h1 , T h2 , T h3 ) and (T h3 , T h1 ). Since transitivity is a necessary
condition for a relation to be called an order, this relation does not form an order.
We, therefore, have to expand this pairwise relation to provide a total order. In other
words, we have to find a linear ordering between themes which maximizes the agreement
between the orderings provided by the input texts. For each pair of themes, T h i and T hj ,
we keep two counts, Ci,j and Cj,i ; Ci,j is the number of input texts in which sentences from
T hi occur before sentences from T hj , and Cj,i is the same for the opposite order. The weight
of a linear order (T hi1 , . . . , T hik ) is defined as the sum of the counts for every pair C il ,im ,
such that il  im and l, m  {1 . . . k}. Stating this problem in terms of a directed graph
where nodes are themes, and a vertex from T h i to T hj has the weight Ci,j , we are looking
for a path with maximal weight which traverses each node exactly once (see Figure 3). We
call such a graph a precedence graph.
The problem of finding a path with maximal weight has been addressed by Cohen,
Schapire, and Singer (1999) in the task of learning orderings. They adopt a two-stage
approach. In the first stage, given a training corpus of ordered instances and a set of
features describing them, a binary preference function is learned. In the second stage, new
instances are ordered so that agreement with the learned preference function is maximized.
To do so, Cohen et al. (1999) represent the preference function as a directed, weighted
graph. Our precedence graph can be seen as such a graph where the preference function
40

fiSentence Ordering in Multidocument News Summarization

T h11  T h12  T h13
T h23  T h22  T h24
T h34  T h31  T h32  T h33

Th 1
2

2

1

Th 2
2
1

1

Th 3

1
1

1

Th 4
Figure 3: Three input theme orderings and their corresponding precedence graph. T h ji is the
sentence part of the theme T hi in the input ordering j.

between the nodes T hi and T hj is Ci,j . The orderings from the input articles provide us
directly with a preference function and, therefore, we do not need to learn it.
Unfortunately this problem is NP-complete; Cohen et al. (1999) prove it by reducing
from CYCLIC-ORDERING (Galil & Megido, 1977). However, using a modified version
of topological sort provides us with an approximate solution. For each node, we assign a
weight equal to the sum of the weights of its outgoing edges minus the sum of the weights
of its incoming edges. We first pick up the node with maximum weight, ordering it ahead
of the other nodes, delete it and its outgoing edges from the precedence graph and update
properly the weights of the remaining nodes in the graph. We then iterate through the
nodes until the graph is empty. Cohen et al. (1999) show that this algorithm produces a
tight approximation of the optimal solution. Currently MultiGen uses an implementation
of this algorithm for its ordering component.
Figures 4 and 5 show examples of produced summaries. One feature of this strategy is
that it can produce several orderings with the same weight. This happens when there is a
tie between two opposite orderings. In this situation, this strategy does not provide enough
constraints to determine one optimal ordering; an ordering is chosen randomly among the
orders with maximal weight.
4.1.2 Evaluation
We asked three human judges (not including ourselves) to classify the quality of the order
of information in 25 summaries produced using the MO algorithm into three categories
Poor, Fair and Good. We use an operational definition of a Poor summary as a text whose
41

fiBarzilay, Elhadad & McKeown

The man accused of firebombing two Manhattan subways in 1994 was convicted Thursday after the jury
rejected the notion that the drug Prozac led him to commit the crimes.
He was found guilty of two counts of attempted murder, 14 counts of first-degree assault and two counts
of criminal possession of a weapon.
In December 1994, Leary ignited firebombs on two Manhattan subway trains. The second blast injured 50
people  16 seriously, including Leary.
Leary wanted to extort money from the Transit Authority.
The defense argued that Leary was not responsible for his actions because of toxic psychosis caused by
the Prozac.

Figure 4: A summary produced using the Majority Ordering algorithm, graded as Good.

Hemingway, 69, died of natural causes in a Miami jail after being arrested for indecent exposure.
A book he wrote about his father, Papa: A Personal Memoir, was published in 1976.
He was picked up last Wednesday after walking naked in Miami.
He had a difficult life.
A transvestite who later had a sex-change operation, he suffered bouts of drinking, depression and drifting,
according to acquaintances.
Its not easy to be the son of a great man, Scott Donaldson, told Reuters.
At the time of his death, he lived in the Coconut Grove district where he was well-known to its Bohemian
crowd.
He had been due to appear in court later that day on charges of indecent exposure and resisting arrest.
He sometimes went by the name of Gloria and wore womens clothes.
The cause of death was hypertension and cardiovascular disease.
Taken to the Miami-Dade Womens Detention Center, he was found dead in his cell early on Monday,
spokeswoman Janelle Hall said.
He was booked into the womens jail because he had a sex-change operation, Hall added.

Figure 5: A summary produced using the Majority Ordering algorithm, graded as Poor.

readability would be significantly improved by reordering its sentences. A Fair summary
is a text which makes sense, but reordering of some sentences can yield a better readability. Finally, a summary which cannot be further improved by any sentence reordering is
considered a Good summary.
The judges were asked to grade the summaries taking into account only the order in
which the information is presented. To help them focus on this aspect of the texts, we
resolved dangling references beforehand. Figure 13 shows the grades assigned to the summaries  three summaries were graded as Poor, 14 were graded as Fair, and eight were
graded as Good. We are showing here the majority grade that is selected by at least two
judges. This was made possible because in our experiments, judges had strong agreement;
they never gave three different grades to a summary.
The MO algorithm produces a small number of Good summaries, but most of the
summaries were graded as Fair. For instance, the summary graded Good shown in Figure 4
orders the information in a natural way; the text starts with a sentence summary of the
event, then the outcome of the trial is given, a reminder of the facts that caused the trial
and a possible explanation of the facts. Looking at the Good summaries produced by
MO, we found that it performs well when the input articles follow the same order when
42

fiSentence Ordering in Multidocument News Summarization

presenting the information. In other words, the algorithm produces a good ordering if the
input articles orderings have high agreement.
On the other hand, when analyzing Poor summaries, we observed that the input texts
have very different orderings. By trying to maximize the agreement of the input texts
orderings, MO produces a new ordering that does not occur in any input text. The ordering
is, therefore, not guaranteed to be acceptable. An example of a new produced ordering is
given in Figure 5. The summary would be more readable if several sentences were moved
around. An example of a better ordering is given in Figure 6. In this summary, the
three sentences related to the fact that the subject had a sex-change operation are grouped
together, while in the one produced by the majority ordering algorithm, they are scattered
throughout the summary.

Hemingway, 69, died of natural causes in a Miami jail after being arrested for indecent exposure.
The cause of death was hypertension and cardiovascular disease.
He was picked up last Wednesday after walking naked in Miami.
He had been due to appear in court later that day on charges of indecent exposure and resisting arrest.
Taken to the Miami-Dade Womens Detention Center, he was found dead in his cell early on Monday,
spokeswoman Janelle Hall said.
He was booked into the womens jail because he had a sex-change operation, Hall added.
A transvestite who later had a sex-change operation, he suffered bouts of drinking, depression and drifting,
according to acquaintances.
He sometimes went by the name of Gloria and wore womens clothes.
He had a difficult life.
Its not easy to be the son of a great man, Scott Donaldson, told Reuters.
At the time of his death, he lived in the Coconut Grove district where he was well-known to its Bohemian
crowd.
A book he wrote about his father, Papa: A Personal Memoir, was published in 1976.

Figure 6: One possible better ordering for the summary graded as Poor.

This algorithm can be used to order sentences accurately if we are certain that the
input texts follow similar organizations. This assumption may hold in limited domains
where documents have a fixed organization of the information. However, in our case, the
input texts we are processing do not have such regularities. Looking at the daily statistics of
Newsblaster which collects clusters of related articles to be synthesized into one summary,
we notice that the typical cluster size is seven. But every day there are several clusters
which contain more than 20 and up to 70 articles to be summarized into single summaries 6 .
With such a big number of input articles, we cannot assume that they will all have similar
ordering of the information. MOs performance critically depends on the agreement of
orderings in the input texts; we, therefore, need an ordering strategy which can fit any
input data. From here on, we will focus only on the Chronological Ordering algorithm and
techniques to improve it.

6. These giant clusters correspond to the hot topics of the day in the news.

43

fiBarzilay, Elhadad & McKeown

4.2 Chronological Ordering
4.2.1 The Algorithm
Multidocument summarization of news typically deals with articles published on different
dates, and articles themselves cover events occurring over a wide range of time. Using
chronological order in the summary to describe the main events helps the user understand
what has happened. It seems like a natural and appropriate strategy. As mentioned earlier,
in our framework, we are ordering themes; using this strategy, we, therefore, need to assign
a date to themes. To identify the date an event occurred requires a detailed interpretation
of temporal references in articles. While there have been recent developments in disambiguating temporal expressions and event ordering (Wiebe, OHara, Ohrstrom-Sandgren, &
McKeever, 1998; Mani & Wilson, 2000; Filatova & Hovy, 2001), correlating events with the
date on which they occurred is a hard task. In our case, we approximate the theme time
by its first publication time; that is, the first time the theme has been reported in our set
of input articles (see Figure 7). It is an acceptable approximation for news events; the first
publication time of an event usually corresponds to its occurrence in real life. For instance,
in a terrorist attack story, the theme conveying the attack itself will have a date previous
to the date of the theme describing a trial following the attack.
Theme 5
Oct 5, 11:35am

Oct 6, 6:13am
Oct 5, 10:20am

Hours after the crash, U.S. officials said that the tragedy had been
caused by an S-200 missile fired by Ukraine during military exercises
on the Crimean Peninsula.
U.S. officials said immediately after the crash that they had evidence
the passenger jet was hit by a Ukrainian missile.
But U.S. officials said that the crash had been caused by an S-200
missile fired mistakenly by Ukrainian forces during military exercises
on the Crimean Peninsula.

Figure 7: A theme with its corresponding sentences. The time theme is shown underlined; it is the
earliest publication time of the sentences.

Articles released by news agencies are marked with a publication time, consisting of a
date and a time with two fields (hour and minutes). Articles from the same news agency
are thus guaranteed to have different publication times. This is also quite likely for articles
coming from different news agencies. During the development of MultiGen, we processed
hundreds of articles, and we never encountered two articles with the same publication time.
Thus, the publication time serves as a unique identifier over articles. As a result, when two
themes have the same publication time, it means that they both are reported for the first
time in the same article.
Our Chronological Ordering (CO) algorithm takes as input a set of themes and orders
them chronologically whenever possible. Each theme is assigned a date corresponding to
its first publication. To do so, we select for each theme the sentence that has the earliest
publication time. We call it the time stamp sentence and assign its publication time as
44

fiSentence Ordering in Multidocument News Summarization

the time stamp of the theme. This establishes a partial order over the themes. When two
themes have the same date (that is, they are reported for the first time in the same article)
we sort them according to their order of presentation in this article. This results in a total
order over the input themes. Figures 8 and 9 show examples of summaries produced using
CO.
One of four people accused along with former Pakistani Prime Minister Nawaz Sharif has agreed to testify
against him in a case involving possible hijacking and kidnapping charges, a prosecutor said Wednesday.
Raja Quereshi, the attorney general, said that the former Civil Aviation Authority chairman has already
given a statement to police.
Sharifs lawyer dismissed the news when speaking to reporters after Sharif made an appearance before a
judicial magistrate to hear witnesses give statements against him. Sharif has said he is innocent.
The allegations stem from an alleged attempt to divert a plane bringing army chief General Pervez Musharraf
to Karachi from Sri Lanka on October 12.

Figure 8: A summary produced using the Chronological Ordering algorithm graded as Good.

Thousands of people have attended a ceremony in Nairobi commemorating the first anniversary of the
deadly bombings attacks against U.S. Embassies in Kenya and Tanzania.
Saudi dissident Osama bin Laden, accused of masterminding the attacks, and nine others are still at large.
President Clinton said, The intended victims of this vicious crime stood for everything that is right about
our country and the world.
U.S. federal prosecutors have charged 17 people in the bombings.
Albright said that the mourning continues.
Kenyans are observing a national day of mourning in honor of the 215 people who died there.

Figure 9: A summary produced using the Chronological Ordering algorithm graded as Poor.

4.2.2 Evaluation
Following the same methodology we used for the MO algorithm evaluation, we asked three
human judges (not including ourselves) to grade 25 summaries generated by the system
using the CO algorithm applied to the same collection of input texts. The results are
shown in Figure 13: ten summaries were graded as Poor, eight were graded as Fair and
seven were graded as Good.
Our first suspicion was that our approximation deviates too much from the real chronological order of events and, therefore, lowers the quality of sentence ordering. To verify
this hypothesis, we identified sentences that broke the original chronological order and restored the ordering manually. Interestingly, the displaced sentences were mainly background
information. The evaluation of the modified summaries shows no visible improvement.
When comparing Good (Figure 8) and Poor (Figure 9) summaries, we notice two phenomena: first, many of the badly placed sentences cannot be ordered based on their temporal occurrence. For instance, in Figure 9, the sentence quoting Clinton is not one event
in the sequence of events being described, but rather, a reaction to the main events. A
tool assigning time stamps would assign to this sentence the date at which Clinton made
his statement. This is also true for the sentence reporting Albrights reaction. Assigning
45

fiBarzilay, Elhadad & McKeown

a date to a reaction, or more generally to any sentence conveying background information,
and placing it into the chronological stream of the main events does not produce a logical
ordering. The ordering of these themes is, therefore, not covered by the CO algorithm. Furthermore, some sentences cannot be assigned any time stamp. For instance, the sentence,
The vast, sparsely inhabited Xinjiang region, largely desert, has many Chinese military
and nuclear installations and civilian mining. describes a state rather than an event and,
therefore, trying to describe it in temporal terms is invalid. Thus the ordering cannot be
improved at the temporal level.
The second phenomenon we observed is that Poor summaries typically contain abrupt
switches of topics and are generally incoherent. For instance, in Figure 9, quotes from
US officials (third and fifth sentences) are split, and sentences about the mourning (first
and sixth sentences) appear too far apart in the summary. Grouping them together would
increase the readability of the summary. At this point, we need to find additional constraints
to improve the ordering.

5. Improving the Ordering: Experiments and Analysis
In the previous section, we showed that using naive ordering algorithms does not produce
satisfactory orderings. In this section, we investigate through experiments with humans
how to identify patterns of orderings that can improve the algorithm.
5.1 Collecting a corpus of multiple orderings
Sentences in a text can be ordered in a number of ways, and the text as a whole will still
convey the same meaning. But the majority of possible orders are likely to be unacceptable because they break conventions of information presentation. One way to identify these
conventions is to find commonalities among different acceptable orderings of the same information. Extracting regularities in several acceptable orderings can help us specify ordering
constraints for a given input type. There is no naturally occurring existing collection of
summaries for multiple documents that we aware of 7 . But even such a collection would not
be sufficient since we want to analyze a collection of multiple summaries over the same set
of articles. We created our own collection of multiple orderings produced by different humans. Using this collection, we studied common behaviors and mapped them to strategies
for ordering.
Our collection of multiple orderings, along with our test corpus is available at
http://www.cs.columbia.edu/~noemie/ordering/. We collected ten sets of articles for
this collection. Each set consisted of two to three news articles reporting the same event.
For each set, we manually selected the intersection sentences, simulating MultiGen 8 . On
average, each set contained 8.8 intersection sentences. The sentences were cleaned of explicit references (for instance, occurrences of the President were resolved to President
Clinton) and connectives, so that participants would not use them as clues for ordering.
Ten subjects participated in the experiment, and they each built one ordering per set of
7. In a recent attempt, NIST for the DUC conference collected sets of articles to summarize and one
summary per set.
8. We performed a manual simulation to ensure that ideal data was provided to the subjects of the experiments.

46

fiSentence Ordering in Multidocument News Summarization

intersection sentences. Each subject was asked to order the intersection sentences of a set
so that they form a readable text. Overall, we obtained 100 orderings, ten alternative
orderings per set. Figure 10 shows the ten alternative orderings collected for one set.
Participant
Participant
Participant
Participant
Participant
Participant
Participant
Participant
Participant
Participant

1
2
3
4
5
6
7
8
9
10

D
D
D
D
D
D
D
D
D
D

BGIHFCJAE
GBICFAJEH
BIGFJAEHC
CFGIBJAHE
GBIHFJACE
GIBFCEHJA
BGIFCHEJA
BCFGIEHAJ
GIBEHFAJC
BGICFAJEH

Figure 10: Multiple orderings for one set in our collection. A, B, . . . J stand for sentences. Underlined are automatically identified blocks.

We first observed that a surprisingly large portion of the orderings are different. Out
of the ten sets, only two sets had some identical orderings (in one set, two orderings were
identical while in the other set, there were two pairs of identical orderings). This variety
in the produced orderings can be interpreted as suggesting that not all the orderings were
actually valid or that the task was maybe too hard for the subjects to allow them to
produce reasonable orderings. In fact, all the subjects were satisfied with the orderings
they produced. Furthermore, we manually went through all the 100 orderings, and all
appeared to be valid. In other words, there are many acceptable orderings given one set of
sentences. This confirms the intuition that we do not need to look for a single ideal total
ordering but rather construct an acceptable one.
Looking at these various orderings, one might also conclude that any ordering would
do just as well as any other. One piece of evidence against this statement is that, as
shown in section 2, some orderings yield incomprehensible texts and thus should be avoided.
Furthermore, for a text with n sentences, there are n! possible orderings, but only a small
fraction of those are actually valid orderings. One way to validate this claim would be to
enumerate all the possible orderings of a single text and evaluate each one of them. This
would be doable for very small texts (a text of 5 sentences has 120 possible orderings)
but not for texts of a reasonable size. A more feasible way to validate our claim is to get
multiple orderings of the same text from a large number of subjects. We asked subjects to
order one text of eight sentences. There is a maximum of 40,320 possible orderings for these
sentences. While 50 subjects participated, we only obtained 21 unique orderings, showing
that the number of acceptable orderings does not grow as fast as the number of participants.
We can conclude that only a small fraction of all possible orderings of the information in a
text contains orderings that render a readable text.
47

fiBarzilay, Elhadad & McKeown

5.2 Analysis
The several alternative orderings produced for a single summary exhibit commonalities.
We noticed that, within the multiple orderings of a set, some sentences always appear
together. They do not appear in the same order from one ordering to another, but they
share an adjacency relation. From now on, we refer to them as blocks. For each set, we
identify blocks by automatically clustering sentences across orderings. We use as a distance
metric between two sentences, the average number of sentences that separate them over all
orderings. In Figure 10, for instance, the distance between sentences D and G is 2. The
blocks identified by clustering are: sentences B, D, G and I; sentences A and J; sentences
C and F; and sentences E and H.
We observed that all the blocks in the experiment correspond to clusters of topically
related sentences. These blocks form units of text dealing with the same subject. In
other words, all valid orderings contain blocks of topically related sentences. The notion
of grouping topically related sentences is known as cohesion. As defined by Hasan (1984),
cohesion is a device for sticking together different parts of the text. Studies show that
the level of cohesion has a direct impact on reading comprehension (Halliday & Hasan,
1976). Therefore, good orderings are cohesive; this is what makes the summary readable.
Conversely, the evaluation of the CO algorithm showed that the summaries that were judged
invalid contain abrupt switches of topic. In other words, orderings that are not cohesive
are graded poorly. There is a correlation between the quality of the ordering and cohesion.
Incorporating cohesion constraint into our ordering strategy by opportunistically grouping
sentences together would be beneficial. Cohesion is achieved by surface devices, such as
repetition of words and coreferences. We describe next how we include cohesion in the CO
algorithm based on these surface features.

6. The Augmented Algorithm
Disfluencies arise in the output of the CO algorithm when topics are distributed over the
whole text, violating cohesion properties (McCoy & Cheng, 1991). A typical scenario is
illustrated in Figure 11. The inputs are texts T 1 , T2 , T3 (ordered by publication time). A1 ,
A2 and A3 belong to the same theme, whose intersection sentence is A, and similarly for
B and C. The themes A and B are topically related, but C is not related. Summary S 1 ,
based only on chronological clues, contains two topical shifts; from A to C and back from
C to B. A better summary would be S2 , which keeps A and B together.
6.1 The Algorithm
Our goal is to remove disfluencies from the summary by grouping together topically related
themes. The main technical difficulty in incorporating cohesion in our ordering algorithm is
to identify and to group topically related themes across multiple documents. In other words,
given two themes, we need to determine if they belong to the same cohesion block. For a
single document, topical segmentation (Hearst, 1994) could be used to identify blocks, but
this technique is not a possibility for identifying cohesion between sentences across multiple
documents. Segmentation algorithms typically exploit the linear structure of an input text;
in our case, we want to group together sentences belonging to different texts.
48

fiSentence Ordering in Multidocument News Summarization

T1

T2

T3

S1

S2

A1
...
C1

C2
...
A2
B2

A3
B3
...
C3

A

A

C

B

B

C

Figure 11: Input texts T1 T2 T3 are summarized by the Chronological Ordering (S1 ) or by the Augmented algorithm (S2 ).

Our solution consists of the following steps. In a preprocessing stage, we segment each
input text (Kan, Klavans, & McKeown, 1998) based on word distribution and coreference
analysis, so that given two sentences within the same text, we can determine if they are
topically related. Assume the themes A and B exist, where A contains sentences (A 1 . . . An ),
and B contains sentences (B1 . . . Bm ). Recall that a theme is a set of sentences conveying
similar information drawn from different input texts. We denote #AB to be the number of
pairs of sentences (Ai , Bj ) which appear in the same text, and #AB + to be the number of
sentence pairs which appear in the same text and are in the same segment.
In the first stage, for each pair of themes A and B, we compute the ratio #AB + /#AB
to measure the relatedness of two themes. This measure takes into account both positive
and negative evidence. If most of the sentences in A and B that appear together in the same
texts are also in the same segments, it means that A and B are highly topically related. In
this case, the ratio is close to 1. On the other hand, if among the texts containing sentences
from A and B, only a few pairs are in the same segments, then A and B are not topically
related. Accordingly, the ratio is close to 0. A and B are considered related if this ratio is
higher than a predetermined threshold. We determined experimentally its value to be 0.6.
This strategy defines pairwise relations between themes. A transitive closure of this
relation builds groups of related themes and, as a result, ensures that themes that do not
appear together in any article but which are both related to a third theme will still be
linked. This creates an even higher degree of relatedness among themes. Because we use a
threshold to establish pairwise relations, the transitive closure does not produce elongated
chains that could link together unrelated themes. We are now able to identify topically
related themes. At the end of the first stage, they are grouped into blocks.
In a second stage, we assign a time stamp to each block of related themes using the
earliest time stamp of the themes it contains. We adapt the CO algorithm described in 4.2.1
to work at the level of the blocks. The blocks and the themes correspond to, respectively,
themes and sentences in the CO algorithm. By analogy, we can easily show that the
adapted algorithm produces a complete order of the blocks. This yields a macro-ordering
of the summary. We still need to order the themes inside each block.
In the last stage of the augmented algorithm, for each block, we order the themes it
contains by applying the CO algorithm to them. Figure 12 shows an example of a summary
produced by the augmented algorithm.
49

fiBarzilay, Elhadad & McKeown

This algorithm ensures that cohesively related themes will not be spread over the text
and decreases the number of abrupt switches of topics. Figure 12 shows how the Augmented
algorithm improves the sentence order compared with the order in the summary produced
by the CO algorithm in Figure 9; sentences quoting US officials are now grouped together,
and so are the descriptions of the mourning.
Thousands of people have attended a ceremony in Nairobi commemorating the first anniversary of the
deadly bombings attacks against U.S. Embassies in Kenya and Tanzania. Kenyans are observing a national
day of mourning in honor of the 215 people who died there.
Saudi dissident Osama bin Laden, accused of masterminding the attacks, and nine others are still at large.
U.S. federal prosecutors have charged 17 people in the bombings.
President Clinton said, The intended victims of this vicious crime stood for everything that is right about
our country and the world. Albright said that the mourning continues.

Figure 12: A summary produced using the Augmented algorithm. Related sentences are
grouped into paragraphs.

6.2 Evaluation
Following the same methodology used to evaluate the MO and the CO algorithms, we asked
the judges to grade 25 summaries produced by the Augmented algorithm. Results are shown
in Figure 13.
The manual effort needed to compare and judge system output is extensive considering
that each human judge had to read three summaries for each input set as well as skim the
input texts to verify that no misleading information was introduced in the summaries. We
collected a corpus of 25 sets of articles for evaluation. Overall, there were 75 summaries to
be evaluated. The size of our corpus is comparable with the collection used for the DUC
evaluation (30 sets of articles). This evaluation shows a significant improvement in the
quality of the orderings from the CO algorithm to the Augmented algorithm. To assess the
significance of the improvement, we used the Fisher exact test, conflating Poor and Fair
summaries into one category (p-value of 0.04). The augmented algorithm also shows an
improvement over the MO algorithm (p-value of 0.07).

Majority Ordering
Chronological Ordering
Augmented Ordering

Poor
3
10
3

Fair
14
8
8

Good
8
7
14

Figure 13: Evaluation of the the Majority Ordering, the Chronological Ordering and the Augmented Ordering.

50

fiSentence Ordering in Multidocument News Summarization

7. Related Work
Finding an acceptable ordering has not been studied before in domain independent text
summarization. In single document summarization, summary sentences are typically arranged in the same order that they were found in the full document, although Jing (1998)
reports that human summarizers do sometimes change the original order. In multidocument
summarization, the summary consists of fragments of text or sentences that were selected
from different texts. Thus, there is no complete ordering of summary sentences that can be
found in the original documents.
In domain dependent summarization, it is possible to establish possible orderings a
priori. A valid ordering is traditionally derived from a manual analysis of a corpus of
texts in the domain, and it typically operates over a set of semantic concepts. A semantic
representation of the information is usually available as input to the ordering component.
For instance, in the specific domain of news on the topic of terrorist attacks, summaries
can be constructed by first describing the place of the attack, followed by the number of
casualties, who the possible perpetrators are, etc.
Another alternative when ordering information, still in the domain dependent framework, is to use a more data driven approach, which produces a more flexible output. A
priori defined simple ordering strategies are combined together by looking at a set of features from the input. Elhadad and McKeown (2001) use such techniques to produce patient
specific summaries of technical medical articles. Examples of features which influence the
ordering are presence of contradiction or repetition, relevance to the patient characteristics,
or type of results being reported. A linear combination of these features assigns a weight to
each semantic predicate to be included in the output, allowing them to be ordered. In this
case, the features are domain dependent and have been identified through corpus analysis
and interviews with physicians. In the case of a domain independent system, it would be
an entire new challenge to define and compute such a set of features.
Producing a good ordering of information is also a critical task for the generation community, which has extensively investigated the issue (McKeown, 1985; Moore & Paris, 1993;
Hovy, 1993; Bouayad-Agha, Power, & Scott, 2000; Mooney, Carberry, & McCoy, 1990). One
approach is top-down, using schemas (McKeown, 1985) or plans (Dale, 1992) to determine
the organizational structure of the text. This approach postulates a rhetorical structure
which can be used to select information from an underlying knowledge base. Because the
domain is limited, an encoding can be developed of the kinds of propositional content that
match rhetorical elements of the schema or plan, thereby allowing content to be selected
and ordered. Rhetorical Structure Theory (RST) allows for more flexibility in ordering content by establishing relations between pairs of propositions. Constraints based on intention
(e.g., Moore & Paris, 1993), plan-like conventions (e.g., Hovy, 1993), or stylistic constraints
(e.g., Bouayad-Agha et al., 2000) are used as preconditions on the plan operators containing
RST relations to determine when a relation is used and how it is ordered with respect to
other relations. Another approach (Mooney et al., 1990) is bottom-up and is used to group
together stretches of text in a long, generated document by finding propositions that are
related by a common focus. Since this approach was developed for a generation system, it
finds related propositions by comparisons of proposition arguments at the semantic level.
51

fiBarzilay, Elhadad & McKeown

In our case, we are dealing with a surface representation, so we find alternative methods
for grouping text fragments.
A more recent approach by Duboue and McKeown (2001) has been implemented to
automatically estimate constraints on information ordering in the medical domain, at the
content planning stage. Using a collection of semantically tagged transcripts written by
domain experts, Duboue and McKeown (2001) identify basic adjacency patterns contained
within a plan, as well as their ordering. MultiGen generates summaries of news on any
topic. In such an unconstrained domain, it would be impossible to enumerate the semantics
for all possible types of sentences which could match the elements of a schema, a plan or
rhetorical relations. For instance, Duboue and McKeown build their content planner based
on a set of 29 semantic categories; in our case, there is no such regularity in the input
information. Furthermore, it would be difficult to specify a generic rhetorical plan for a
summary of news. Instead, content determination in MultiGen is opportunistic, depending
on the kinds of similarities that happen to exist between a set of news documents. Similarly,
we describe here an ordering scheme that is opportunistic and bottom-up, depending on
the cohesion and temporal connections that happen to exist between selected text.
Our ordering component takes place after the content selection of the information in
a pipeline architecture, in contrast to generation systems, where usually the ordering and
the content selection come in tandem. This separation might come at a cost  if there is
no good ordering to the given extracted information, it is not possible to go back to the
content selection to extract new information. In summarization, content selection is driven
by salience criteria. We believe that the same ordering strategy should work with different
content selectors, independently of their salience criteria. Therefore, we choose to keep the
two components, selection and ordering, as two separate modules.

8. Conclusion and Future Work
In this paper we investigated information ordering constraints in multidocument summarization in the news genre. We evaluated two alternative ordering strategies, Chronological
Ordering (CO) and Majority Ordering (MO). Our experiments show that MO performs well
only when all input texts follow similar organization of the information. In the domains
where this constraint holds, MO would be an appropriate and highly effective strategy. But
in the news genre we cannot make this assumption; thus it is not an appropriate solution.
The Chronological Ordering (CO) algorithm can provide an acceptable solution for many
cases, but is not sufficient when summaries contain information that is not event based.
Our experiments, using a corpus that we collected of multiple alternative summaries each of
multiple documents, show that cohesion is an important constraint contributing to ordering.
Moreover, they also show that appropriate ordering of information is critical to allow for
easy comprehension of the summary and that it is not the case that all possible orderings of
information are acceptable. We developed an operational algorithm that integrates cohesion
as part of the CO algorithm, and implemented it as part of the MultiGen summarization
system. Our evaluation of the system shows significant improvement in summary quality.
While in this paper we focused on augmenting the CO algorithm, we believe that MO
is a promising strategy and should not be neglected. It is clear that different forms of
summarization are useful in different situations, depending on the intended purpose of
52

fiSentence Ordering in Multidocument News Summarization

the summary and on the types of documents summarized. For our future work, we plan
to build on the approach we used for the DUC 2001 evaluation, where we developed a
summarizer that would use different algorithms for summary generation depending on the
type of input text. We suspect that ordering strategies may differ also, depending on
the type of summary. Our work will first investigate whether we can use our augmented
algorithm for other summary types. If the algorithm does not yield good orderings, we will
investigate through corpus analysis other summary type specific constraints. We suspect
that our augmented algorithm may apply, for instance, to biographical summaries, since
the information being summarized is a mixture of event-based information that can be
chronologically ordered along with descriptive information about the person. It is unclear
whether it can apply to other types of summaries such as summaries of different events,
since pieces of information may not be temporally related to each other. We also plan to
identify the types of summaries which would benefit from using the MO algorithm or an
augmented version of it (the same way the CO algorithm was augmented with the cohesion
constraint).

9. Acknowledgments
This work was partially supported by DARPA grant N66001-00-1-8919, a Louis Morin
scholarship and a Viros scholarship. We thank Eli Barzilay for providing help with the
experiments interface, Michael Elhadad for the useful discussions and comments, and all
the many voluntary participants in the experiments. Our initial work on the problem was
presented at the Human Language Technologies Conference (San Diego, 2001). We also
thank the anonymous reviewers of HLT and JAIR for their comments.

References
Barzilay, R., McKeown, K., & Elhadad, M. (1999). Information fusion in the context of
multi-document summarization. In Proc. of the 37th Annual Meeting of the Assoc. of
Computational Linguistics.
Bouayad-Agha, N., Power, R., & Scott, D. (2000). Can text structure be incompatible with
rhetorical structure?. In Proceedings of the First International Conference on Natural
Language Generation (INLG2000), Mitzpe Ramon, Israel.
Carbonell, J., & Goldstein, J. (1998). The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval.
Cohen, W., Schapire, R., & Singer, Y. (1999). Learning to order things. Journal of Artificial
Intelligence, 10, 243270.
Dale, R. (1992). Generating Referring Expressions: Constructing Descriptions in a Domain
of Objects and Processes. MIT Press, Cambridge, MA.
Duboue, P., & McKeown, K. (2001). Empirically estimating order constraints for content
planning in generation. In Proceedings of the ACL/EACL 2001.
53

fiBarzilay, Elhadad & McKeown

Elhadad, N., & McKeown, K. (2001). Generating patient specific summaries of medical
articles. In Proceedings of the NAACL 2001 Workshop on Automatic Summarization.
Filatova, E., & Hovy, E. (2001). Assigning time-stamps to event-clauses. In Proceedings of
the AACL/EACL 2001 Workshop on Temporal and Spatial Information Processing.
Galil, Z., & Megido, N. (1977). Cyclic ordering is np-complete. Theoretical Compter Science,
5, 179182.
Halliday, M., & Hasan, R. (1976). Cohesion in English. Longman.
Hasan, R. (1984). Reading Comprehension, chap. Coherence and Cohesive Harmony.
Hatzivassiloglou, V., Klavans, J., & Eskin, E. (1999). Detecting text similarity over short
passages: Exploring linguistic feature combinations via machine learning. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Hearst, M. (1994). Multi-paragraph segmentation of expository text. In Proceedings of the
32th Annual Meeting of the Association for Computational Linguistics.
Hovy, E. (1993). Automated discourse generation using discourse structure relations. Artificial Intelligence, 63. Special Issue on NLP.
Jing, H. (1998). Summary generation through intelligent cutting and pasting of the input
document. Tech. rep., Columbia University.
Kan, M.-Y., Klavans, J., & McKeown, K. (1998). Linear segmentation and segment
relevence. In Proceedings of 6th International Workshop of Very Large Corpora
(WVLC-6).
Lin, C.-Y., & Hovy, E. (2001). Neats: A multidocument summarizer. In Proceedings of the
Document Understanding Workshop (DUC).
Mani, I., & Bloedorn, E. (1997). Multi-document summarization by graph search and
matching. In Proceedings of the Fifteenth National Conference on Artificial Intelligence.
Mani, I., & Wilson, G. (2000). Robust temporal processing of news. In Proceedings of the
38th Annual Meeting of the Association for Computational Linguistics.
McCoy, K., & Cheng, J. (1991). Focus of attention: Constraining what can be said next.
In Paris, C., Swartout, W., & Mann, W. (Eds.), Natural Language Generation in
Artificial Intelligence and Computational Linguistics. Kluwer Academic Publishers.
McKeown, K. (1985). Text Generation: Using Discourse Strategies and Focus Constraints
to Generate Natural Language Text. Cambridge University Press, England.
McKeown, K., Barzilay, R., Evans, D., Hatzivassiloglou, V., Kan, M., Schiffman, B., &
Teufel, S. (2001). Columbia multi-document summarization: Approach and evaluation.
In Proceedings of the Document Understanding Workshop (DUC).
McKeown, K., Klavans, J., Hatzivassiloglou, V., Barzilay, R., & Eskin, E. (1999). Towards
multidocument summarization by reformulatin: Progress and prospects. In Proceedings of the Seventeenth National Conference on Artificial Intelligence.
54

fiSentence Ordering in Multidocument News Summarization

Mooney, D., Carberry, S., & McCoy, K. (1990). The generation of high-level structure for
extended explanations. In Proceedings of the International Conference on Computational Linguistics (COLING90), pp. 276281, Helsinki.
Moore, J., & Paris, C. (1993). Planning text for advisory dialogues: Capturing intentional
and rhetorical information. Journal of Computational Linguistics, 19 (4).
Radev, D., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies. In
Proceedings of the ANLP/NAACL 2000 Workshop on Automatic Summarization.
Radev, D., & McKeown, K. (1998). Generating natural language summaries from multiple
on-line sources. Computational Linguistics, 24(3), 469500.
Schiffman, B., Nenkova, A., & McKeown, K. (2002). Experiments in multidocument summarization. In Proceedings of the HLT Conference.
Siegal, S., & Castellan, N. J. (1988). Non-Parametric statistics for the behavioural sciences.
McGraw Hill.
White, M., Korelsky, T., Cardie, C., Ng, V., Pierce, D., & Wagstaff, K. (2001). Multidocument summarization via information extraction. In Proceedings of the HLT Conference.
Wiebe, J., OHara, T., Ohrstrom-Sandgren, T., & McKeever, K. (1998). An empirical
approach to temporal reference resolution. Journal of Artificial Intelligence, 9, 247
293.

55

fi