Journal of Artificial Intelligence Research 28 (2007) 517-557

Submitted 8/06; published 4/07

Consistency and Random Constraint Satisfaction Models
Yong Gao

yong.gao@ubc.ca

Irving K. Barber School of Arts and Sciences
University of British Columbia Okanagan, Kelowna, Canada V1V 1V7

Joseph Culberson

joe@cs.ualberta.ca

Department of Computing Science, University of Alberta
Edmonton, Alberta, Canada T6G 2E8

Abstract
In this paper, we study the possibility of designing non-trivial random CSP models by
exploiting the intrinsic connection between structures and typical-case hardness. We show
that constraint consistency, a notion that has been developed to improve the efficiency
of CSP algorithms, is in fact the key to the design of random CSP models that have
interesting phase transition behavior and guaranteed exponential resolution complexity
without putting much restriction on the parameter of constraint tightness or the domain size
of the problem. We propose a very flexible framework for constructing problem instances
with interesting behavior and develop a variety of concrete methods to construct specific
random CSP models that enforce different levels of constraint consistency.
A series of experimental studies with interesting observations are carried out to illustrate the effectiveness of introducing structural elements in random instances, to verify the
robustness of our proposal, and to investigate features of some specific models based on
our framework that are highly related to the behavior of backtracking search algorithms.

1. Introduction
The tale of random models of constraint satisfaction problems (CSP) gives us an excellent
example of the dramatic impact of structures on the typical-case hardness of randomlygenerated problem instances (Achlioptas et al., 1997; Gent et al., 2001; MacIntyre et al.,
1998; Molloy, 2002; Prosser, 1996; Smith, 2001). It also shows that the study of phase
transitions of random models of NP-complete problems is indeed a viable approach to
understanding the interplay between the structures and the efficiency of search algorithms.
Unlike the random Boolean satisfiability problem (SAT) where structures do not seem
to exist in randomly-generated instances, algorithmically-exploitable structures do exist
in the more general random constraint satisfaction problem. The chief reason for this
is that in random constraint satisfaction instances the uncontrolled existence of multiple
nogoods within the same constraint generates small scale structures that make the instance
unsatisfiable. Here small scale may mean only a few variables are involved, or that the
variables involved are very highly constrained in a local way. The first example of such
structures is the existence of flawed variables and flawed constraints in random instances
generated from the four classical random CSP models in most of the parameter regions
(Achlioptas et al., 1997), assuming that the domain size is fixed. The appearance of flawed
variables and constraints makes these models trivially unsatisfiable and excludes any phase

c
2007
AI Access Foundation. All rights reserved.

Gao & Culberson

transition behavior. As yet another example, the existence of embedded easy subproblems
(Gao & Culberson, 2003), also called “flowers” (Molloy & Salavatipour, 2003), in another
part of the parameter space of the models makes randomly-generated instances polynomially
sovable by constraint consistency algorithms.
Several new models have been proposed to overcome the trivial unsatisfiability. Gent
et al. (2001) proposed the flawless random binary CSP based on the notion of a flawless
conflict matrix. Instances of the flawless random CSP model are guaranteed to be arcconsistent, and thus do not suffer asymptotically from the problem of flawed variables.
Achlioptas et al. (1997) proposed a nogoods-based CSP model and showed that the model
has non-trivial asymptotic behaviors. Random CSP models with a (slowly) increasing
domain size have also been shown to be free from the problem of flawed variables and have
interesting threshold behavior (Xu & Li, 2000; Smith, 2001; Frieze & Molloy, 2003).
While all of these proposed models are guaranteed to have interesting phase transition
behavior (some of them are also guaranteed to generate hard instances at phase transitions), the fundamental relation between the structures and the typical-case hardness of
randomly-generated CSP instances has not been seriously addressed. As a consequence,
superficial conditions and restrictions on the parameter of constraint tightness in these
models frequently give the (false) impression that it is the constraint tightness and/or the
domain size that determines the behavior of random CSP instances. For the classic binary
random CSP, the constraint tightness has to be less than the domain size in order to have
a phase transition. The flawless random CSP does have a true solubility phase transition
at a high constraint tightness, but as we will show later, it still suffers from embedded easy
unsatisfiable subproblems when the constraint tightness is greater than the domain size. In
CSP models with increasing domain size, there is still an obvious restriction on the possible
values of the constraint tightness. In the nogood-based CSP model, it is impossible to have
a high constraint tightness without making the constraint (hyper)graph very dense.
In this paper, we study the possibility of designing non-trivial random CSP models
by exploiting the intrinsic connection between structures and typical-case hardness. For
this purpose, we show that consistency, a notion that has been developed to improve the
efficiency of CSP algorithms, is in fact the key to the design of random CSP models that have
interesting phase transition behaviors and guaranteed exponential resolution complexity
without putting much restriction on the parameter of constraint tightness or the domain
size.
In Section 4 we report a series of experimental studies. Algorithmic studies on random
instances are sometimes criticized for being more about the model than about the algorithms (Johnson, 2002). We believe that it is better to study the interaction of models with
algorithms. For example, in Section 4.2 we observe a double peak phenomenon where our
best guess suggests it is related to the (lack of) consistency enforcement in the algorithms
we use interacting with the degree of consistency enforced in the instances by the various
models. We propose a very flexible framework for constructing problem instances with interesting behavior and develop a variety of concrete methods to construct specific random
CSP models that enforce different levels of constraint consistency. We hope this framework
will be useful in helping researchers construct pseudo-random instances exhibiting various
kinds of structure, while avoiding trivialities that have sometimes tainted claimed results

518

Consistency and Random CSPs

in the past, or that may cause algorithm design to become too focused on a particular
structure to the detriment of a more general application.

2. Random Models for CSPs
Throughout this paper, we consider binary CSPs defined on a domain D with |D| = d. A
binary CSP C consists of a set of variables x = {x1 , · · · , xn } and a set of binary constraints
{C1 , · · · , Cm }. Each constraint Ci is specified by its constraint scope, a pair of variables in
x, and a constraint relation RCi that defines a set of incompatible value-tuples in D × D
for the scope variables. An incompatible value-tuple is also called a restriction, or a nogood.
Associated with a binary CSP is a constraint graph whose vertices correspond to the set
of variables and whose edges correspond to the set of constraint scopes. Throughout the
discussion, we assume that the domain size is a fixed constant. In the rest of the paper, we
will be using the following notation:
1. n, the number of variables;
2. m, the number of constraints;
3. d, the domain size; and
4. t, the constraint tightness, i.e., the size of the restriction set.
Given two variables, their constraint relation can be specified by a 0-1 matrix, called
the conflict matrix, where an entry 0 at (i, j) indicates that the tuple (i, j) ∈ D × D is
incompatible. Another way to describe a constraint relation is to use the compatibility
graph, a bipartite graph with the domain of each variable as an independent part, where an
edge signifies the corresponding value-tuple is compatible.
We now define our generic model for generating random binary CSPs. Recall that a
relation is a set of ordered pairs.
d,t
Definition 2.1. (Bn,m
[L], Restricted Random Binary CSP) Let d, t, n, m be integers
as specified above, all variables share a common domain D = {1, . . . , d}, and let L =
d,t
{L1 , L2 , . . . | Li ⊂ D × D} be a set of relations. Bn,m
[L] is a random CSP model such that

1. its constraint graph is the standard random graph G(n, m) where the m edges of the
graph are selected uniformly at random from all possible n2 edges; and
2. for each edge of G(n, m), a constraint relation on the corresponding scope variables is
specified as follows:
(a) Choose at random a relation L ∈ L.
(b) Choose t value-tuples from D × D \ L uniformly at random as its nogood set.
To the above definition, we typically want to add the condition that the set L satisfies
\
(a, a) ∈
/
Li , ∀a ∈ D.
i≥1

519

Gao & Culberson

If this condition is not met, then the instances generated by the CSP model are always
trivially satisfiable (and vacuously have exponential resolution complexity). This is a very
d,t
mild, but sufficient, condition to make sure that Bn,m
[L] has a linear solubility threshold.
In particular, all the existing models and all the models to be discussed in the present
paper satisfy the condition. In Appendix A.3, we provide a result showing that under
d,t
this condition, there is a constant c∗ such that Bn,m
[L] with m/n > c∗ is asymptotically
unsatisfiable with probability one.
By placing other restrictions on L we can subsume many of the previously existing
models as well as the new models we will define shortly.
d,t
d,t
Definition 2.2. (Model B: Random Binary CSP Bn,m
) If for Bn,m
[L] we have L = {∅},
d,t
we write Bn,m
, which is known in the literature as Model B.

An instance of a CSP is said to be k-consistent if and only if for any (k − 1) variables,
each consistent (k − 1)-tuple assignment to the (k − 1) variables can be extended to a
consistent k-tuple assignment to any other kth variable. A CSP instance is called strongly
k-consistent if and only if it is j-consistent for each j ≤ k. Of special interest are the
strong-k-consistencies for k = 1, 2, 3, also known as node-consistency, arc-consistency, and
path-consistency (Mackworth, 1977).
d,t
there are no restrictions on the construction of the constraint. It has
Note that in Bn,m
d,t
been shown that Bn,m is asymptotically trivial and unsatisfiable for t ≥ d, but has a phase
transition in satisfiability probability for t < d (Achlioptas et al., 1997; Gent et al., 2001).
Basically, the trivial cases arise because asymptotically with high probability there will be
a variable such that all of its values are forbidden due to constraints containing it that are
not arc-consistent. This motivates the introduction of the flawless conflict matrix to make
sure that the random model is arc-consistent (Gent et al., 2001).
d,t
d,t
Definition 2.3. (Flawless Random Binary CSP Bn,m
[M]) If for Bn,m
[L] each L ∈ L
d,t
is a bijection, L : D ↔ D, then we write Bn,m
[M]. This is the flawless random binary CSP
as defined by Gent et al. (2001).

The M in the notation is a reference to matchings. If we use the construction for the
generalized flawless random binary CSPs that we present in Section 3.2, but restrict the
underlying graphs to be matchings, then it reduces to the flawless model.
By specifying bijections we create a set of tuples such that there is one for each i ∈ D.
These will not be considered when choosing incompatible value-tuples, and so the resulting
model is guaranteed to be arc-consistent and consequently will not have flawed variables.
d,t
However, even though the flawless random binary CSP Bn,m
[M] does not suffer from the
d,t
problem of trivial unsatisfiability, it can be shown that Bn,m
[M] may asymptotically have
embedded easy subproblems for t ≥ d − 1.
∗
Theorem 2.1. For t ≥ d − 1, there is a constant c∗ > 0 such that for any m
n > c , with high
d,t
probability Bn,m
[M] is asymptotically unsatisfiable and can be solved in polynomial time.

Proof. We outline the proof here and provide a detailed proof in Appendix A.1. The idea is
d,t
to show that for sufficiently high m
n , Bn,m [M] contains structures (sub-instances) that (1)
cause the unsatisfiability of the whole instance and (2) can be detected in polynomial time.
520

Consistency and Random CSPs

One such structure is the so-called “flower” consisting of a collection of forbidden constraint
cycles which we explain below.
A binary constraint over two variables x1 and x2 is said to contain an (α, β)-forcer if
for the assignment x1 = α, the only compatible assignment to x2 is x2 = β. Consider a
CSP instance. Let {x0 , x1 , · · · , xr−1 } be a subset of r variables and α be a domain value.
Suppose that there is a cycle of constraints
C1 (x0 , x1 ), C2 (x1 , x2 ), . . . , Cr−1 (xr−2 , xr−1 ), and Cr (xr−1 , x0 )
such that C1 (x0 , x1 ) contains an (α, α1 )-forcer, Ci (xi−1 , xi ) contains an (αi−1 , αi )-forcer,
and Cr (xr−1 , x0 ) contains an (αr−1 , β)-forcer with α 6= β. Then, the assignment x0 = α
cannot be used in any satisfying assignment. We call such a cycle of constraints a forbidden
cycle (for the variable x0 and domain value α). Now, if there are d (the domain size)
forbidden cycles for the variable x0 , one for each domain value, then the CSP instance is
not satisfiable since none of its domain values can be in a satisfying assignment. We call a
variable together with forbidden cycles for each α a flower.
Using the second-moment method, it can be shown that there is a constant c∗ such
d,t
∗
that for any m
n > c , the probability that Bn,m [M] contains a flower is asymptotically one.
d,t
∗
So, with high probability, Bn,m
[M] is unsatisfiable if m
n > c . Furthermore, if a binary
CSP instance contains a flower, then any path-consistency algorithm (e.g., Mackworth,
1977) will produce a new CSP instance in which the variable of the flower has an empty
domain. Therefore, a CSP instance with an embedded flower sub-instance can be solved in
polynomial time.
d,t
It should be noted that Bn,m
[M] does have a non-trivial phase transition since it is
1
satisfiable with high probability if m
n < 2 . Theorem 2.1 does not exclude the possibility
d,t
∗
[M] will be able to generate hard instances when m
that Bn,m
n is below the upper bound c ,
in particular in the case of a large domain size. Further investigation is required to fully
d,t
understand the complexity of Bn,m
[M] in this regard.

3. Consistency, Resolution Complexity, and Better Random CSP Models
Propositional resolution complexity deals with the minimum length of resolution proofs for
an (unsatisfiable) CNF formula. As many backtracking-style complete algorithms can be
simulated by a resolution proof, the resolution complexity provides an immediate lower
bound on the running time of these algorithms. Since the work of Chvatal and Szemeredi
(1988), there have been many studies on the resolution complexity of randomly-generated
CNF formulas (Beame et al., 1998; Achlioptas et al., 2001).
Mitchell (2002b) developed a framework in which the notion of resolution complexity
is generalized to CSPs and the resolution complexity of randomly-generated random CSPs
can be studied. In this framework, the resolution complexity of a CSP instance is defined
to be the resolution complexity of a natural CNF encoding which we give below. Given an
instance of a CSP on a set of variables {x1 , · · · , xn } with a domain D = {1, 2, · · · , d}, its
CNF encoding is constructed as follows:
1. For each variable xi , there are d Boolean variables xi : 1, xi : 2, . . . , xi : d each of
which indicates whether or not xi takes on the corresponding domain value. There is
521

Gao & Culberson

a clause xi : 1 ∨ xi : 2 ∨ . . . ∨ xi : d on these d Boolean variables making sure that xi
takes at least one of the domain values;
2. For each restriction (α1 , · · · , αk ) ∈ Dk of each constraint C(xi1 , · · · , xik ), there is a
clause xi1 : α1 ∨ · · · ∨ xik : αk to respect the restriction.
This CNF encoding is equivalent to the original CSP problem in the sense that a CSP
instance is satisfiable if and only if the CNF encoding is satisfiable. Any satisfying assignment to the CSP variables translates immediately to a satisfying assignment of the CNF
encoding. On the other hand, any satisfying assignment to the CNF encoding can also be
translated to a satisfying assignment to the original CSP instance. If in the CNF assignment, more than one of the d boolean variables xi : 1, xi : 2, . . . xi : d are assigned TRUE,
we can just pick any one of them and assign the corresponding domain value to the CSP
variable xi . It is possible to add another set of 2-clauses for each constraint variable xi of
the form {xi : αj ∨ xi : αh | 1 ≤ j < h ≤ d} to ensure that any satisfying assignment to the
CNF encoding assigns a unique domain value to each CSP variable. However, these do not
in general change the complexity results, and make the analysis more complicated.
Mitchell (2002b) as well as Molloy and Salavatipour (2003) showed that random CSPs
will have an exponential resolution complexity if their constraint tightness t is less than
d,t
a certain value. For random binary CSP Bn,m
, the requirement is (1) t < d − 1; or (2)
t < d and m
is
sufficiently
small.
For
t
≥
d
−
1,
recent theoretical results (Gao & Culbern
son, 2003; Molloy & Salavatipour, 2003) indicate that it is still possible for these classical
models to have an asymptotic polynomial complexity due to the existence of embedded easy
subproblems.
In the following, we will show that it is not necessary to put restrictions on the constraint
tightness in order to have a guaranteed exponential resolution complexity. Based on similar
arguments as those in the literature (Mitchell, 2002b; Molloy & Salavatipour, 2003; Beame,
d,t
Culberson, Mitchell, & Moore, 2005), it can be shown that if in Bn,m
[L], the constraint
relation of each constraint is chosen in such a way that the resulting instances are always
d,t
strongly 3-consistent, then Bn,m
[L] has an exponential resolution complexity no matter how
large the constraint tightness is.
d,t
Theorem 3.1. Let L be such that Bn,m
[L] is strongly 3-consistent. Then for any constant
d,t
m
n = c > 0, the resolution complexity of Bn,m [L] is almost surely exponential.

Proof. See Appendix A.2.
Using the tool developed by Molloy and Salavatipour (2003), the requirement that CSP
instances be strongly 3-consistent to have an exponential resolution complexity can be
further relaxed. Recall that a constraint on x1 , x2 contains an (α, β)-forcer if for x1 = α
the only consistent assignment allowed by the constraint is x2 = β.
Definition 3.1. We call a CSP instance weakly 4-consistent if
1. it is arc-consistent,
2. it contains no forcer, and

522

Consistency and Random CSPs

3. for any 4 distinct variables x1 , x2 , x3 , x4 and constraints C1 (x1 , x2 ), C2 (x2 , x3 ), C3 (x3 , x4 )
and for any α1 , α4 ∈ D there exist α2 , α3 ∈ D such that the assignment x1 = α1 , x2 =
α2 , x3 = α3 , x4 = α4 is consistent with C1 , C2 and C3 .
Note that weak 4-consistency is weaker than strong 3-consistency since it does not
require an instance to be 3-consistent.
d,t
Theorem 3.2. Let L be such that Bn,m
[L] is weakly 4-consistent. Then for any constant
d,t
m
n = c > 0, the resolution complexity of Bn,m [L] is almost surely exponential.

Proof. See Appendix A.2.
The proofs of the above theorems are based on the structural properties of the underlying
constraint random graph of the CSP models and the relation between the size of a resolution
proof and the maximum size of a clause in the resolution proof. Below we briefly discuss
the key ideas of the proof of Theorem 3.1 to illustrate the role of constraint consistency.
First, in order for a CSP instance to have an exponential size proof, the size of a minimum
unsatisfiable sub-instance (MUS) must not be constant. Otherwise, an enumeration of all
the partial assignments to all the sub-instances of size less than that of the MUS gives us
d,t
[L], the minimum vertex degree of a MUS
a proof of polynomial size. In the case of Bn,m
must be larger than 2 since it is 3-consistent. Due to the local sparseness of a random
graph forced by the assumption m
n = c, the minimum size of a MUS has to be linear in the
problem size n.
Second, Ben-Sasson and Wigderson (2001) have shown that to establish an exponential
lower bound on the resolution complexity, it is sufficient to show that any resolution proof
must contain a clause whose size is linear in the problem size. Let s be the minimum size
of a MUS. It can be shown that any resolution proof must contain a clause C such that the
minimum size of the sub-instance J that implies C is at least 2s . Since the instance is 3
consistent and J is minimal, the clause C must contain one literal related to each of the
variables in J whose vertex degree is less than 3. A random graph argument shows that the
number of such variables in any sub-instance is linear in n. In summary, due to constraint
consistency and the local sparseness of a random constraint graph, any resolution proof
must contain a clause whose derivation involves a linear number of variables.
The question remaining to be answered is whether or not there are any natural random
CSP models that are guaranteed to be strongly 3-consistent or weakly 4-consistent. In fact,
the CSP-encoding of random graph k-coloring is strongly k-consistent. In the rest of this
section, we discuss how to generate random CSPs with a high tightness that are strongly
3-consistent or weakly 4-consistent.
3.1 The Generalized Flawless Random Binary CSP
We now introduce our generalized flawless model. We call this a generalized flawless model
since it requires even stronger conditions on L than the flawless model of Gent et al. (2001).
We assume a common domain D = {1, . . . , d}, d > 2.
Definition 3.2. We say that L is SC-inducing if
1. ∀L ∈ L and ∀α ∈ D there exist β, γ ∈ D such that (α, β) ∈ L and (γ, α) ∈ L and
523

Gao & Culberson

2. ∀L1 , L2 ∈ L and ∀α, γ ∈ D there exists β ∈ D such that (α, β) ∈ L1 and (β, γ) ∈ L2 .
The first condition in Definition 3.2 guarantees arc consistency in any constraint. The
second condition ensures 3-consistency. Given d > 2, these are the two conditions for strong
3-consistency.
Definition 3.3. We say that L is WC-inducing if
1. ∀L ∈ L and ∀α ∈ D there exist β1 , β2 , γ1 , γ2 ∈ D such that (α, β1 ), (α, β2 ) ∈ L and
(γ1 , α), (γ2 , α) ∈ L and
2. ∀L1 , L2 , L3 ∈ L and ∀α, δ ∈ D there exist β, γ ∈ D such that (α, β) ∈ L1 , (β, γ) ∈ L2
and (γ, δ) ∈ L3 .
The first condition in Definition 3.3 is to prevent forcers as well as to enforce arc consistency, which are the first two conditions required by the definition of weak 4-consistency.
Definition 3.4. (Generalized Flawless Random Binary CSPs)
d,t
d,t
For the model Bn,m
[L] if L is SC-inducing then we write Bn,m
[SC]. If L is WC-inducing
d,t
then we write Bn,m [WC]. In either case, we say we generate a generalized flawless random
binary CSP.
d,t
[SC] is strongly 3-consistent. A CSP constructed
Lemma 3.3. A CSP constructed by Bn,m
d,t
by Bn,m [WC] is weakly 4-consistent.

We should note that just as the flawless method could not generate every arc consistent
CSP (Gent et al., 2001) not every SC (WC) CSP will be generated by our system. For
example, if there is a constraint C on variables x1 and x2 then with respect to a third
variable x3 strong 3-consistency requires only that there be a consistent assignment to x3
for those pairs of values that are compatible in C. Our method ensures consistency on every
pair of values for x1 , x2 independently of whether or not there is a constraint on them. We
see no reasonable way to produce the relaxed version given that the constraints have to be
generated independently of G(n, m).
3.2 Constructing Consistent-Inducing Sets L
We now provide methods to generate a variety of sets of relations L which are SC-inducing
or WC-inducing. Although our CSP models require all variables to have a common domain,
we will start with a more general construction on varying domain sizes. This will enable
us to significantly increase the domain tightness on larger domains. As a side benefit, it
indicates how the models might be adapted to varying domains. However, to build a model
for varying domain sizes would require at least that we have distinct sets L for every ordered
pair of distinct domains. It would also complicate the selection step, 2(a) in Definition 2.1
as well as the analysis. We leave further development of such models for future research.
We start by constructing a set of bipartite graphs G, which we call the core graphs.
For each pair of domains D` and Dr with sizes d` = |D` |, dr = |Dr |, there will be one
or more core graphs G ∈ G. Each graph G ∈ G is of the form G = (V` ∪ Vr , E) where
V` = {v1 , . . . , vd` } and Vr = {w1 , . . . , wdr } are sets of vertices and E ⊆ V` × Vr . For each
524

Consistency and Random CSPs

graph G we create a set of size kG of triples of the form T (G) = {(G, π`i , πri ) | 1 ≤ i ≤ kG },
where π`i : V` ↔ D` and πri : Vr ↔ Dr are bijections labeling the vertices.
We define T = T (G) = ∪G∈G T (G). For each T ∈ T , where T = (G, π` , πr ), we let L(T )
be the set of pairs {(π` (v), πr (w)) | (v, w) ∈ E(G)}. We define L = L(T ) = {L(T ) | T ∈ T },
the set of relations induced by the triples.
Given a bipartite graph G = (V` ∪ Vr , E), define θi to be the minimum degree of a vertex
in the vertex subset Vi where i ∈ {`, r}.
We say that G is degree bound if for each graph θ` > d2r and θr > d2` .
Lemma 3.4. If G is degree bound then for any T1 , T2 ∈ T sharing the domain D = Dr1 =
D`2 , for any α ∈ D`1 , γ ∈ Dr2 , there exists β ∈ D such that (α, β) ∈ L1 = L(T1 ), (β, γ) ∈
L2 = L(T2 ).
Proof. Let v = π` −1
1 (α) be the vertex in G1 labeled by α. Since deg(v) ≥ θ`1 , the number
of pairs in L1 which have α as their first element must be greater than d2 . Similarly, the
pairs in L2 with γ as the second element must also cover more than 1/2 of D. Thus, there
must exist β such that (α, β) ∈ L1 , (β, γ) ∈ L2 .
Corollary 3.5. If G is degree bound on a common domain D then L(T ) is SC-inducing.
It is not hard to see that without restricting the bijections Lemma 3.4 provides the best
possible result.
Lemma 3.6. If T is not degree bound and arbitrary bijections π` and πr are allowed, L(T )
may not be SC-inducing.
Proof. Consider two relations L1 and L2 on a common domain D with graphs G1 , G2 such
that θ`1 ≤ d2 and θr2 ≤ d2 . Let α ∈ D1 and β ∈ D4 and note that the neighbors of the vertices
labeled by these values will each cover at most 1/2 of D. Since we can choose πr1 and π`2
arbitrarily, we can ensure that there is no value for x2 consistent with x1 = α, x3 = β.
Consider the graph on a domain with d = 9 shown in Figure 1. If we allow arbitrary
bijections π` , πr then L is not SC-inducing, but it is WC-inducing, which the reader may
readily verify. The degree of the vertices in this graph is θ = 4, which is less than that allowed
by Lemma 3.4. It remains WC-inducing because the structure of this graph distributes the
edges in a uniform way.
To generalize this observation, assume a common domain D and consider three constraints induced by L1 , L2 , L3 built from some core graph set G. If we have minimum
degree θ then for any value on the left of L1 there will be θ values on the right of L1 , and
if we allow arbitrary bijections, these may match any subset of size θ on the left of L2 .
Similarly, any value on the right of L3 will match an arbitrary set of θ values on the right
of L2 . To ensure that G generates an L that is WC-inducing, all we need is that any such
arbitrary pair of θ-sets induces at least one edge in any G ∈ G. If any pair of subsets fails
to induce an edge on some core graph, then the corresponding constraint set is not weakly
4-consistent.
Letting NG (v) be the neighbors of vertex v in graph G, this argument generalizes to the
formal statement

525

Gao & Culberson

Lemma 3.7. On a common domain D, if ∀G ∈ G the minimum left and right degree is
θ` = θr = θ, then G generates a WC-inducing
S L using arbitrary bijections π` , πr iff ∀G ∈ G,
for i ∈ {`, r} and for all S ⊂ Vi , |S| = θ, | v∈S NG (v)| > d − θ.
A graph is θ-regular if every vertex is of degree θ. A graph is connected if there is a
path between every pair of vertices. Notice that in a degree bound G each G is necessarily
connected. The following is a special case where G will generate WC-inducing but not
SC-inducing L.
Corollary 3.8. Suppose we have a common domain D with d = 2θ for some θ and G is
a set of θ-regular connected bipartite graphs. Let T be a set of triples generated by random
pairs of bijections on graphs in G. Then L(T ) is WC-inducing.
Proof. By Lemma 3.7, we must show that for an arbitrary subset S ⊂ V` where |S| = θ,
the neighbors of S contain more than θ vertices. Suppose not. Note that we must have θ2
edges on the vertices in S, so if there are θ other distinct endpoints (there cannot be fewer)
each of these has θ edges. It follows that these are the only edges on the neighbors, but
then G is not connected.
Lemma 3.7 can be used to show that θ2 ≥ d. It also follows easily that G must be
connected. These bounds are met at d = 4, θ = 2 using the eight cycle as illustrated in
Figure 1. For larger d we mostly leave the problem of optimizing θ as a research project
in combinatorial design, although we will provide some insight in the next section. A few
other WC-inducing graphs are shown in Figure 1.
3.3 Recursive Construction
By Lemmas 3.4 and 3.6 if we do not introduce additional restrictions on the bijections, the
2
best tightness we can achieve while maintaining strong 3-consistency is t < d2 . If we only
require weak 4-consistency, then Lemma 3.7 may allow significant improvement, although
it is unclear exactly how much in general. We will now show how to increase tightness by
reducing the number of edges in the graphs using a recursive construction for larger domains
that simultaneously restricts the set of bijections.
Given a domain D we partition it into a set of blocks Π = {Π1 , . . . , Πk } subject to k ≥ 3
and |Πj | ≥ 3, ∀j. These latter restrictions are in place because the only SC-inducing graphs
on pairs of size two are the complete bipartite graphs. The blocks Πj may vary in size.
Each variable xi must have exactly one partition Πi of its domain which will be used in
the construction of all constraints on xi . For problems with a common domain it is simplest
to have just one partition of the common domain for all variables, which is the construction
we will now assume.
Let V = {v1 , . . . , vd }. A bijection π : V ↔ D is said to respect the partition Π if there is
a bijection π ∗ : Π ↔ Π on the partition such that for each j, 1 ≤ j ≤ k and for each i ∈ Πj
we have π(vi ) ∈ π ∗ (Πj ). Note that to meet this condition, π ∗ must satisfy the condition
|Πj | = |π ∗ (Πj )| for each j. Thus, partitions with varying sizes of blocks will further restrict
the set of respectful bijections.
We construct a Π-bound graph G as follows. Let VΠ` = {v10 , . . . , vk0 } and VΠr =
{w10 , . . . , wk0 } be sets of vertices corresponding to the blocks of the partition. Let GΠ =
(VΠ` ∪ VΠr , EΠ ) be an SC-inducing graph.
526

Consistency and Random CSPs

d=4
d=5
d=6

d=9

Figure 1: Graphs that are WC-inducing but not SC-inducing for domains of size 4,5,6 and
9. Notice that for d = 5 the minimal graph is not regular. The boxes in the graph
for d = 9 indicate the recursive construction as described in Section 3.3. See also
Figure 2.

We assume as in the general construction two sets of vertices V` = {v1 , . . . , vd } and
Vr = {w1 , . . . , wd }. For each edge (v, w) ∈ EΠ we construct a degree bound graph Gvw =
(V` [Πv ] ∪ Vr [Πw ], Evw ), where V` [Πv ] = {vi : i ∈ Πv } and Vr [Πw ] = {wi : i ∈ Πw }. Recall
that degree bound graphs work even when the domains are of differing sizes. We construct
the Π-bound graph G = (V` ∪ Vr , E) by letting E = ∪(v,w)∈EΠ Evw .
Lemma 3.9. If G consists of Π-bound graphs and all bijections π` , πr respect Π, then L is
SC-inducing.
Proof. Consider two relations L1 , L2 . Since block graphs GΠ are SC-inducing, it follows
that there is a path between any block on the left graph and any block on the right under
the block permutations π ∗ implied by respectful bijections. But now focusing on the edges
and bijections restricted to the blocks on such a path, since these induce degree bound
graphs, the proof is completed by an argument similar to the proof of Lemma 3.4.
Figure 1 shows a graph generated by this method for d = 9. (For d = 3, there is only
one minimal degree bound graph up to isomorphism.) Figure 2 shows the construction of
527

Gao & Culberson

1

6

A 2

1

7

5 B C9

3

4

4

8
7 C

B 5

7

5

4

B6

5

4

6

1

7

2A A 2

8

3

9

1

C 8
9

X

3

8

9

6

2 A

3

Y

Y

L1

B

C

Z
L2

Figure 2: An illustration of a strongly 3-consistent pair constructed using a 3 × 3 recursive
construction on a domain of size 9. For simplicity, we use identity bijections on
the far left and right. In the center, the dashed lines represent the bijection on
the shared domain induced by πr1 and π`2 . πr1 and π`2 respect the blocks, but
rearrange the values at both the block and vertex levels. We highlight value 1 on
the left and the vertices connected to it up to the left side of L2 , where there are
2 vertices in each of two blocks. These are sufficient to connect to any value on
the right.

two constraints using two copies of this graph. The bijections respect the block structure,
and so the constraint is SC-inducing.
For very large domains, we notice that we may have many blocks in Π. Since GΠ is
SC-inducing, we can recursively use the construction to build GΠ . We call this construction
the recursive Π-bound construction. For example, similar to the graph for d = 9 in Figure 1,
we see that for domains of size d = 3i this recursive construction lets us use graphs with
degree 2i . Thus, tightness can be as high as d2 − d1+log3 2 on SC-inducing graphs.

528

Consistency and Random CSPs

We note that it is also possible to recursively partition very large blocks, resulting in
possibly non-uniform depth recursive constructions, provided all permutations are block
respecting in the appropriate sense. We leave this as an exercise as the notation gets messy,
and applications appear limited.
We also claim without proof that if all blocks in Π are of the same size, and we make
GΠ WC-inducing and replace the degree bound subgraphs with WC-inducing graphs, then
provided we use Π-respecting bijections, G is WC-inducing. We call such graphs weakly
Π-bound. As before, we can recursively construct GΠ to get recursive weakly Π-bound
graphs. For d = 4i using a recursion based on the d = 4 case in Figure 1, we can have weak
4-consistency where the core graphs have degree 2i ; that is, tightness in this case can be as
high as d2 − d3/2 .
Although there is increased tightness using these recursive constructions, there is a down
side in that we greatly reduce the set of allowed bijections. For example, to respect the
block structure of the graph on d = 9 shown in Figure 2, we have at most (3!)4 = 1296
distinct bijections on either side, compared to 9! = 362880 unrestricted bijections. We also
note that the recursive constructions have limited practical application since it is expensive
to express tight constraints in an explicit way for such large domains.
3.4 Limits to Tightness and Complexity Tradeoffs
Can we get even tighter constraints with exponential resolution complexity? The discussion
so far indicates we are near the limits using our consistency inducing constructions, so we will
need something else. The following construction illustrates that it is possible to maintain
exponential complexity while having even higher tightness, but it is trivial in the sense that
it is really only exponential in a linear sized embedded subproblem.
Let us embed 3-coloring in a domain of size d > 3. If we wish to construct a CSP version
of k-coloring for graphs, we start with a coloring domain K = {1, . . . , k}. We create one
graph G = (V` ∪ Vr , E) ∈ G, with edge set E = V` × Vr \ {(vi , wi ) | 1 ≤ i ≤ k}. We restrict
the bijections to π` (vi ) = i, πr (wi ) = i, 1 ≤ i ≤ k, which we refer to as the identity bijections
or identities, ι` , ιr for short. So there is only one relation in L = {L(G, ι` , ιr )}. To complete
the k-coloring implementation, we set t = k, the maximum possible value for t. To embed
3-coloring in a domain of size d, let G be constructed as in the 3-coloring construction on
the sub-domain {1, 2, 3} and then pad G with independent vertices {vi , wi | 4 ≤ i ≤ d}. We
construct one triple (G, ι` , ιr ) and again we have one relation in L. We set tightness to its
maximum possible value, t = d2 − 6.
We conjecture this to be the maximal possible tightness with exponential complexity,
d,t
but it is of little relevance for larger d. For this L, on any instance from Bn,m
[L] any
reasonable algorithm will simply reduce it to 3-coloring, and will thus exhibit the exponential
complexity of the 3-coloring threshold.
We see that maximizing tightness is certainly not the only consideration, and probably
not the most important. We feel it is more important to consider the kinds of structure that
one can embed in a model, while avoiding generating trivial instances. Our model expands
the framework wherein this can be done over previous models.

529

Gao & Culberson

3.5 Implementation Issues
The experiments conducted by Gao and Culberson (2004) and many of those in this paper
use a domain size of d = 4 and the 8 cycle WC-inducing graph shown in Figure 1. We
should point out that in the experiments conducted by Gao and Culberson (2004) we fixed
the bijection π` = ι` and randomly assigned only the right hand set πr . Although this is
adequate to satisfy the WC-inducing conditions, it does not include all possible L, since for
example for any L generated that way there will be an α such that both (1, α) and (2, α)
are in L. If during the construction of constraints the variables are ordered in a consistent
way, for example with the smaller index always on the left side of the constraint, then this
may introduce an overall asymmetry in the CSP.
In the following, we report a new series of experiments in which we used the randomly
paired bijection approach.
The best degree bound graphs, in the sense of maximizing t, are those in which all
vertices are of the minimum possible degree, that is θ-regular graphs. For our purposes,
there is no requirement that these graphs be uniformly selected from all possible such
graphs, and it is not too hard to find various techniques to get a variety of such graphs. For
graphs conforming to Corollary 3.8 for example, one can first generate a cycle alternating
between vertices of V` , Vr and then choose a set of maximum matchings from the remaining
unused pairs to reach the required degree. These can be found using any of a variety of
techniques, for example by finding a 1-factorization of a regular bipartite graph (Bondy &
Murty, 1976, Chapter 5). Should one desire to have a uniform sampling for some reason,
there is a significant literature on generating random regular graphs of various types, which
might serve as a starting point (Wormald, 1999).

4. Experiments
In this section, we discuss a series of experiments we have conducted to study the impact
of the structural elements introduced in the generalized flawless model on the typical case
hardness of problem instances of practical size. In Section 4.1, we discuss an experiment
on a Boolean 3-ary CSP model obtained from the widely-used random distribution of CNF
formula. In Section 4.2, we report detailed experimental results on the comparison of three
different random CSP models.
4.1 Randomly-generated 3-ary Boolean CSPs
The observation from this set of experiments on Boolean CSPs with different constraint
tightness partly motivated this study on the interplay between the performance of backtracking algorithms and the structural information in the seemingly “structureless” randomlygenerated instances. We include the results here since they provide a nice illustration of the
effect of an increase in the constraint tightness (hence an increase in the likelihood of the
existence of a forcer in a constraint) on the typical case hardness of random CSP instances.
To obtain an instance distribution for the ternary boolean CSP, we start with a random
3-CNF formula in which each clause is generated by selecting three variables uniformly
at random without replacement. A clause in a CNF formula is equivalent to a constraint
with constraint tightness t = 1 over the same set of variables. E.g., x ∨ y ∨ z corresponds

530

Consistency and Random CSPs

to the restriction {(0, 0, 1)}. We can therefore obtain random instances of Boolean-valued
CSPs with a larger constraint tightness by adding more clauses defined over the same set
of variables.
Let F(n, m) be a random 3-CNF formula with n variables and m clauses selected
uniformly at random without replacement. We construct a new random 3-CNF formula
F(n, m, a) as follows:
1. F(n, m, a) contains all the clauses in F(n, m);
2. For each clause C in F(n, m), we generate a random clause on the same set of variables
of C, and add this new clause to F(n, m, a) with probability a.
In fact, F(n, m, a) is the random Boolean CSP model with an average constraint tightness
1 + a and has been discussed by Gao and Culberson (2003). For a > 0, it is easy to see
that F(n, m, a) is always strongly 2-consistent, but is not 3-consistent asymptotically with
probability 1.

Figure 3: Thresholds for the solution probability in the model F(n, m, a) with n = 250.
The z-axis is the solution probability. The axis with the range 1.0 ... 2.0 is for
the parameter 1 + a and the axis with the range 1.0 ... 6.0 is for the clause density
m/n.
Figure 4 shows the median of the number of branches used by the SAT solver ZChaff
(Zhang et al., 2001) on 100 instances of F(n, m, a) with n = 250. Figure 3 shows the
solution probability of the same model. As expected, an increase in the tightness results
in a shift of the location of the hardness peak toward smaller m/n. More significant is the
magnitude of the decrease of the hardness as a result of a small increase in the constraint
tightness. The instances hardness varies so dramatically that it is hard to illustrate the
difference for all the constraint tightness values from t = 1.0 to t = 2.0 using either the
original scale or the log scale. This explains the scale scheme used in Figure 4.
The reason for such dramatic change can be explained as follows. If we randomly generate constraints with tightness t > 1 then for each constraint, there is a positive (fixed)
531

Gao & Culberson

5000

3000

branches

4000

2000

1000

1
0

1.2
1.4
5.5

5

1.6
4.5

m/n

4

3.5

3

1+a

1.8
2.5

Figure 4: Effects of an increase in the constraint tightness on the instance hardness for
F(n, m, a) with n = 250. The z-axis is the median number of branches with the
first three highest peaks scaled to 1/40, 1/10, and 1/4 respectively. The axis with
the range 1.0 ... 1.8 is for the parameter 1 + a, 0 ≤ a ≤ 0.8 and the axis with the
range 2.5 ... 6 is for the clause density m/n.

probability that the restriction set will be something such as {(1, 0, 0), (0, 0, 0)}. This
constraint is not 3-consistent, as the partial assignment y = 0, z = 0 has no consistent
extension to x. In effect, this constraint induces the binary constraint on y, z with the restriction {(0, 0)}. If there are enough such constraints, then the induced binary constraints
create an unsatisfiable subproblem, and since the domain is also Boolean, this subproblem can be detected in polynomial time. Upper bounds on m/n have been established for
the existence of the easy unsatisfiable subproblems in F(n, m, a). For example, we know
that the upper bounds on m/n for F(n, m, a) to have an exponential resolution complexity are respectively 23.3 if a = 0.1 and 11.7 if a = 0.2 (Gao & Culberson, 2003). Since
the ratio of constraints to variables m/n considered in the experiment are well below these
bounds above which embedded 2SAT subproblems appear with high probability, it seems
that the impact of forcers on the instance hardness goes beyond simply producing higlystructured 2-SAT-like embedded easy subproblems. We will see a similar effect in the next
subsection on non-Boolean valued CSP instances.
4.2 Random Binary CSP Models: the Significance of Structures
This set of experiments is designed to investigate whether introducing structural elements
that enforce constraint consistency in random CSPs leads to a significant increase in the
typical case hardness of instances of practical size. It should be mentioned that the purpose
of the experiments is not to compare and rank the relative merits of the solvers we have
used in the experiments. Neither is it our intention to exhaust all the available solvers and
implementation techniques to solve this set of problem instances.

532

Consistency and Random CSPs

d,t
d,t
The three random CSP models we consider are Bn,m
(the Model B), Bn,m
[M] (the
d,t
flawless model), and the generalized flawless model Bn,m [L] with different domain size and
different consistency-inducing core graphs. Randomly-generated instances from these models are encoded as CNF formulas and solved by the SAT solver ZChaff 1 . Also included
are some experiments on the comparison of ZChaff, SatZ (Li & Anbulagan, 1997) 2 , and a
CSP solver based on forward checking (FC) and maintaining arc consistency (MAC) with
a static variable order3 .
It looks unnatural that we have primarily tested random CSP instances by converting
them to SAT instances and using a SAT solver to solve them. This is justified by the
following considerations. First, all of the existing research on the resolution complexity
of random CSPs has been carried out by studying the resolution complexity of a SAT
encoding of CSPs as described in Section 3. We use the same encoding in the experiments.
Secondly, it has been shown that as far as the complexity of solving unsatisfiable CSP
instances is concerned, many of the existing CSP algorithms can be efficiently simulated by
the resolution system of the corresponding SAT encodings of the CSPs (Mitchell, 2002a).
Experimental comparisons were conducted for the three CSP models with domain size
(d = 4, 5, 6, and 9) and different values of constraint tightness. The generalized flawless
CSP models are constructed using the WC-inducing core graphs as described in Figure 1.
In the next two subsections, we focus on the results for the domain size d = 4 and 9. The
experiments were carried out on machines with AMD Athlon (tm) Processor (Model 3700,
Frequency 2.4GHz, Main Memory 1GB with 1MB Cache) running Linux. The following
setup was used in all of our experiments: the sample size for each parameter point is 100;
the cutoff time for all the solvers is 1800 seconds of CPU time.

4.2.1 The Case of d = 4
d,t
[WC] we use in the experiment is based on the WC-inducing
For the case of d = 4, the Bn,m
core graph shown in Figure 1, a connected 2-regular bipartite graph (or an 8-cycle). Note
that for this core graph, the maximum possible constraint tightness is 8.
To observe the detailed behavior of the models, we first fixed the constraint tightness
to t = 6 and the number of variables to n = 500. Figure 5 plots the solution probability
as a function of the ratio of constraints to variables for the three CSP models. From the
d,t
experimental data, we observed that the phase transition of Bn,m
[WC] is much sharper than
d,t
d,t 4
those of Bn,m
[M] and Bn,m
.
d,t
More importantly, instances of Bn,m
[WC] at the phase transition are clearly much harder
d,t
d,t
than those of Bn,m and Bn,m [M]. Figures 6, 7, and 8 show the 50th percentile, 20th
percentile, 10th percentile, and 5th percentile of the number of branches and running time
in seconds for ZChaff to solve randomly-generated CSP instances from the three models.
d,t
As can be seen, instances drawn from Bn,m
[WC] are consistently much harder than those

1.
2.
3.
4.

Available at http://www.princeton.edu/∼ chaff/zchaff.html
Available at http://www.laria.u-picardie.fr/∼ cli/EnglishPage.html
Available at http://ai.uwaterloo.ca/∼ vanbeek/software/software.html
The reader should be aware that this model doesn’t show a phase transition in the limit (as problem size
approaches infinity).

533

Gao & Culberson

1

0.9

0.8

Solution Probability

0.7

0.6

0.5

0.4

0.3
generalized flawless
flawless model
model B

0.2

0.1

0

0

0.5

1

1.5
2
2.5
Constraints−Variables Ratio

3

3.5

4

Figure 5: Solution probability as a function of the ratio of constraints to variables for the
three random CSP models with n = 500, t = 6. For the generalized flawless
model, L is the set of connected 2-regular bipartite graphs. The y-axis is for
the solution probability and x-axis for the ratio of constraints to variables m/n.
Sample size for each data point is 100.

d,t
d,t
from Bn,m
and Bn,m
[M] in terms of both the size of the search tree and the running time
and for satisfiable instances (Figure 8) as well as unsatisfiable instances.
d,t
Careful readers may have noticed that for the model Bn,m
[WC], the number of branches
has an obvious secondary peak before the phase transition (Figure 6 and Figure 8), an
“anomaly” that we have reported in our previous paper (Gao & Culberson, 2004). A detailed look at the experimental data reveals that the secondary peak, though persistent in a
statistical sense, appears only for the number of branches; We noticed that the measure of
running time does not have such a secondary peak. We have tried several approaches to understand such a secondary peak, and finally conclude that it is a solver-dependent behavior
caused the branching heuristics, the CNF encoding, and the level of the local consistency
enforced by the CSP models. We provide some further discussion of this phenomenon in
the next subsection.
For the case of d = 4, we also studied the behavior of the CSP models with different
constraint tightness values. The results are reported in Tables 1 and 2 where we show
the maximum over all the ratio of constraints to variables of the median of the number of
branches for the three CSP models with the constraint tightness ranging from t = 5 to
t = 8. The results are consistent with those observed in the experiments on Boolean CSPs,
showing that an increase in constraint tightness has a significant impact on the typical case
instance hardness. It is also clear that for all the constraint tightness values, instances from
d,t
the generalized flawless model Bn,m
[WC] are significantly harder than those from the two

534

Consistency and Random CSPs

5

6

10

median
Number of Branches

Number of Branches

10

4

10

3

10

2

1
2
3
Constraints−Variables Ratio

6

Number of Branches

4

10

3

10
10

4

0

1
2
3
Constraints−Variables Ratio

4

generalized flawless
flawless model6
model B 10

10

90th percentile

5

10

4

10

3

10

2

10

5

10

2

0

Number of Branches

10

95th percentile

80th percentile

5

10

4

10

3

10

2

0

1
2
3
Constraints−Variables Ratio

10

4

0

1
2
3
Constraints−Variables Ratio

4

Figure 6: Number of branches used by ZChaff to solve instances from the three random CSP
d,t
[WC],
models with n = 500, t = 6, d = 4. For the generalized flawless model Bn,m
the core graph is the connected 2-regular bipartite graph described in Figure 1.

d,t
classical models. In the case of t = 5, instances drawn from Bn,m
[WC] cannot be solved
within the time limit of 1800 seconds even before the solubility phase transition.
To further confirm that enforcing constraint consistency increases the typical case hardness, we tested two other backtracking solvers in our experiments: SatZ and the CSP solver
with FC + MAC and a static variable ordering.
In this set of experiments, the problem size is n = 300 and we run all the solvers on the
same instances to reduce variance. We summarize the results in Tables 3 and 4.
For problem size n = 300, ZChaff and SatZ have little difficulty in solving randomlygenerated instances from all the three models except that in some rare cases, SatZ can
get stuck on instances generated with the ratio of constraints to variables well below the
threshold. Although the CSP solver works directly on the CSP instances, it does not perform
as well as the SAT solvers working on the encoded instances. Given the long history of

535

Gao & Culberson

2

4

10

10

95th percentile

median
1
2

Seconds

Seconds

10

0

10

−1

10

−2

10

10

0

10

−2

1.5

2
2.5
3
3.5
Constraints−Variables Ratio

10

4

0

10

90th percentile

80th percentile

2

2

10

Seconds

Seconds

4

generalized flawless
flawless model
4
model B 10

4

0

10

−2

10

1
2
3
Constraints−Variables Ratio

10

0

10

−2

0

1
2
3
Constraints−Variables Ratio

10

4

1

2
3
Constraints−Variables Ratio

4

Figure 7: User CPU time in seconds to solve instances from the three random CSP models
d,t
with n = 500, t = 6, d = 4. For the generalized flawless model Bn,m
[WC], the core
graph is the connected 2-regular bipartite graph described in Figure 1.

536

Consistency and Random CSPs

6

6

10

10

95th percentile
Number of Branches

Number of Branches

median
5

10

4

10

3

10

2

0.5
1
1.5
2
Constraints−Variables Ratio

6

Number of Branches

4

10

3

10
10

2.5

0

0.5
1
1.5
2
Constraints−Variables Ratio

2.5

generalized flawless
flawless model6
model B 10

10

90th percentile

5

10

4

10

3

10

2

10

10

2

0

Number of Branches

10

5

80th percentile

5

10

4

10

3

10

2

0

0.5
1
1.5
2
Constraints−Variables Ratio

10

2.5

0

0.5
1
1.5
2
Constraints−Variables Ratio

2.5

Figure 8: Number of branches used by ZChaff to solve satisfiable instances from the three
random CSP models with n = 500, t = 6, d = 4. For the generalized flawless
d,t
model Bn,m
[WC], the core graph is the connected 2-regular bipartite graph described in Figure 1.

537

Gao & Culberson

(n, t)
(500, 5)
(500, 6)
(500, 7)
(500, 8)

d,t
Bn,m
branches
10673
7243
4440
1853

m/n
2.30
1.80
1.40
0.90

d,t
Bn,m
[M]
branches m/n
20271
2.90
7446
1.90
5123
1.50
4113
1.20

d,t
Bn,m
[WC]
branches
m/n
?
> 3.10
94281
2.44
13813
2.00
9163
1.60

Table 1: The maximum, over all m
n , of the median of the number of branches of ZChaff on
100 random instances of the three random CSP models , where the domain size
d,t
d = 4, tightness t = 5, 6, 7, 8. For the generalized flawless model Bn,m
[WC], the
core graph is the connected 2-regular bipartite graph described in Figure 1.

(n, t)
(500, 5)
(500, 6)
(500, 7)
(500, 8)

d,t
Bn,m
seconds m/n
0.08
2.80
0.02
2.00
0.01
1.40
0.00
0.90

d,t
Bn,m
[M]
seconds m/n
1.79
2.90
0.02
1.90
0.02
1.70
0.00
1.00

d,t
Bn,m
[WC]
seconds
m/n
> 1800 > 3.10
40.3
2.44
1.08
2.00
0.26
1.60

Table 2: The maximum, over all m
n , of the median running time in seconds of ZChaff on
random instances of the three random CSP models , where the domain size d = 4,
d,t
tightness t = 5, 6, 7, 8. For the generalized flawless model Bn,m
[WC], the core
graph is the connected 2-regular bipartite graph described in Figure 1.

development of SAT solvers, with significant branch selection heuristics of SatZ and clauselearning (nogood recording) in ZChaff, this should not be surprising. In particular, the
d,t
design of Bn,m
[WC] can be seen as rendering the consistency checking of the CSP solver
less effective. Still, the better performance of the SAT solvers indicates there is still some
structure to exploit.
4.2.2 The Case of d = 9
To further study the robustness of our method and to seek explanations on the double
peaks, we consider the case of domain size d = 9. It turns out that for d = 9 we are able to
construct a variety of random CSP models that have significantly different behavior. We
consider a collection of 5 random CSP models:
d,t
1. Bn,m
(model B),
d,t
2. Bn,m
[M] (the flawless model),

538

Consistency and Random CSPs

(n, t)
(300, 5)
(300, 6)
(300, 7)
(300, 8)

ZChaff
seconds m/n
4.16
3.10
0.33
2.50
0.11
2.00
0.03
1.70

SatZ
seconds m/n
2.50
3.10
0.31
2.50
0.11
2.00
0.03
1.70

CSP (FC + MAC)
seconds
m/n
90.21
3.10
12.11
2.50
3.10
2.00
0.60
1.70

Table 3: Maximum, over all m
n , of the median number of branches of ZChaff, SatZ, and a
d,t
CSP solver with FC + MAC on random instances of Bn,m
[WC] , where n = 300,
d,t
d = 4, t = 5, 6, 7, 8, and the core graph of Bn,m [WC] is the connected 2-regular
bipartite graph described in Figure 1.

(n, t)
(300, 5)
(300, 6)
(300, 7)
(300, 8)

ZChaff
seconds m/n
0.08
2.90
0.01
2.20
0.01
1.70
0.00
1.30

SatZ
seconds m/n
0.15
2.90
0.02
2.20
0.01
1.70
0.01
1.30

CSP (FC + MAC)
seconds
m/n
2.68
3.00
0.29
2.20
0.09
1.80
0.06
1.30

Table 4: Maximum, overall m
n , of the median running time in seconds of ZChaff, SatZ, and
d,t
a CSP solver with FC + MAC on random instances of Bn,m
[M], where n = 300,
d = 4, and t = 5, 6, 7, 8.

d,t
3. Bn,m
[L1 ] where L1 is constructed using an 18 cycle (i.e., a connected 2-regular bipartite
graph) with arbitrary bijections,
d,t
4. Bn,m
[L2 ] where L2 is constructed using the core graph shown in Figure 1 with arbitrary
bijections, and
d,t
5. Bn,m
[L3 ] where L3 is constructed using the same core graph in Figure 1 with blockrespecting bijections.
d,t
d,t
d,t
Recall that the model Bn,m
[L2 ] belongs to the class Bn,m
[WC], while Bn,m
[L3 ] belongs to
d,t
the class Bn,m [SC]. For all of the five models, we fix the constraint tightness to t = 45,
the maximum possible constraint tightness that can be achieved by a generalized flawless
model with the WC-inducing core graph.
These experiments show that the typical-case hardness of randomly generated instances
increases with the level of consistency enforced. In Figure 9, we plot the median of the
number of branches and the median running time in seconds for ZChaff to solve instances
d,t
d,t
d,t
from Bn,m
, Bn,m
[M], and Bn,m
[L1 ]. In Figure 10, we show the median of the number of

539

Gao & Culberson

6

3

10

10

18−cycle
flawless model
model B

18−cycle
flawless model
model B
2

10

5

Number of Branches

10

1

Seconds

10

0

10
4

10

−1

10

3

10

−2

1

1.5
2
2.5
Constraints−Variables Ratio

10

3

1

1.5
2
2.5
Constraints−Variables Ratio

3

d,t
d,t
d,t
Figure 9: Results on using ZChaff to solve instances from Bn,m
, Bn,m
[M], and Bn,m
[L1 ]
with L1 constructed by an 18 cycle (i.e., a connected 2-regular bipartite graph)
and arbitrary bijections. The other parameters used are n = 300, d = 9, and t =
d,t
45. For Bn,m
[L1 ] with the parameter m/n = 2.3, ZChaff failed to solve 45 of the
100 instances. The data (pointed to by solid triangles) for m/n = 2.3 are based
on the solved instances only.

branches and median of the running time in seconds for ZChaff to solve instances from
d,t
d,t
Bn,m
[L2 ] and Bn,m
[L3 ].
d,t
d,t
[M], ZChaff is able to solve all the instances within the time limit of
and Bn,m
For Bn,m
1800 seconds for all m/n. ZChaff starts to have frequent timeouts when m/n ≥ 2.3. For
d,t
[L1 ] with m/n = 2.3, ZChaff can solve 55 of the 100 instances within 1800 seconds,
Bn,m
and 10 of the 55 solved instances are unsatisfiable. At m/n = 2.4, 2.5, and 2.6, ZChaff can
solve 42 instances (respectively, 81 instances, 90 instances) all of which are unsatisfiable.
d,t
d,t
For the models Bn,m
[L2 ] andBn,m
[L3 ], we observed that out of 100 instances generated
d,t
with parameter m/n = 2.3, ZChaff can solve 90 instances from Bn,m
[L2 ] and 33 instances
d,t
d,t
from Bn,m
[L3 ]. For Bn,m
[L2 ] with m/n = 2.4, ZChaff only solved 5 instances. All the
d,t
d,t
solved instances from Bn,m
[L2 ] and Bn,m
[L3 ] are satisfiable, which is to be expected since
d,t
this region is well below their threshold. We didn’t conduct experiments on Bn,m
[L2 ] with
d,t
m/n = 2.5 and 2.6 and Bn,m
[L3 ] with m/n = 2.4, 2.5 and 2.6, but expect that instances
from these models will be even harder.
Having assumed that the secondary peak observed in the CSP models with d = 4 is
algorithm-independent and unique to our models, we had expected that using a larger
domain size with a variety of CSP models would be able to help provide a satisfactory
explanation of the phenomenon. We designed some experiments to empirically investigate
the relations between the appearance of the secondary peak and three characteristics of our
540

Consistency and Random CSPs

6

3

10

10
18−cycle
WC−inducing
SC−inducing

18−cycle
WC−inducing
SC−inducing
2

10
5

1

10

Seconds

Number of Branches

10

4

10

0

10

3

10

−1

10

2

10

1

−2

1.5
2
2.5
Constraints−Variables Ratio

10

3

1

1.5
2
2.5
Constraints−Variables Ratio

3

d,t
Figure 10: Results on using ZChaff to solve instances from Bn,m
[L1 ], weakly 4-consistent
d,t
d,t
model Bn,m [L2 ] and the strongly 3-consistent model Bn,m [L3 ]. The other parameters used are d = 9, n = 300, t = 45. ZChaff starts to have frequent timeouts
when m/n ≥ 2.3. Therefore for m/n = 2.3, the data (pointed to by solid trid,t
[L1 ], 45 of
angles) are based on the solved instances only. For the model Bn,m
the 55 solved instances are satisfiable and the remaining 10 solved instances are
d,t
unsatisfiable. For the weakly 4-consistent model Bn,m
[L2 ] and the strongly 3d,t
consistent model Bn,m [L3 ], Zchaff solved 90 instances (respectively 33 instances)
d,t
all of which are satisfiable. For the weakly 4-consistentmodel Bn,m
[L2 ] with
m/n = 2.4, ZChaff can only solve 5 of the 100 instances and these 5 instances
are all satisfiable.

541

Gao & Culberson

CSP models including (1) the number of derangements in the bijections in our constraint
construction, (2) the level of the enforced local consistency of the models, and (3) the CNF
encoding scheme. Based on a series of results, we conclude that the secondary peak is an
algorithm-dependent artifact that is influenced by the selection of the branch heuristics, the
level of local consistency enforced by the models, and the CNF encoding scheme. In the
following, we briefly summarize these experimental investigations which, we hope, also helps
illustrate the flexibility of our framework in constructing an ensemble of problem instances
to study the behavior of different algorithms.
1. The Number of Derangements. The number of derangements of a bijection π : V ↔ D
is the number of vertices vi ∈ V such that π(vi ) 6= i. If there is no derangement in
the bijections in our constraint construction, then the generated instances are always
satisfied by the assignments that assign a common domain value to all the variables.
Contrary to our initial assumption, we did not observe any significant impact of the
number of derangements on the behavior of the solvers except for the extreme cases
where we use bijections with no derangement with probability very close to one.
2. The Local Consistency Level. The main difference among the five models with d = 9
d,t
is the level of the enforced local constraint consistency. Bn,m
does not enforce any
d,t
d,t
[L1 ] guarantees that there is no
[M] enforces arc-consistency; Bn,m
consistency; Bn,m
d,t
forcer; Bn,m
[L2 ] generates instances that are weakly 4-consistent; and instances from
d,t
Bn,m [L3 ] are strongly 3-consistent. In addition to the obvious impact on instances
hardness, our experiments indicate that the local consistency level in the models
contributes to the appearance of the secondary peak of ZChaff. As depicted in Figures
9 and 10, for the problem size n = 300, the secondary peak exists in all of the first four
d,t
models which do not enforce strong 3-consistency, while in the fifth model Bn,m
[L3 ]
there is no such peak. However, we observed, on some additional trials not reported
d,t
[L3 ] as we increase the problem size.
here, that the secondary peak also exists in Bn,m
3. The CNF Encoding Scheme. In our experiments, the CNF encoding of the CSP
instances does not include the clauses that enforce a unique assignment to a CSP
variable. While the absence of these clauses does not affect theoretical exponential
lower bounds on the resolution complexity, we observed in our experiments mixed
impacts of these clauses on the behavior of the solvers including the appearance of
the secondary peak of ZChaff. When these clauses are included in the CNF encoding,
the number of branches and the running time of SatZ both increase, while for ZChaff
the running time has an obvious decrease. It is also interesting that adding these
clauses to the CNF encoding also makes the secondary peak of ZChaff even sharper.
We believe that this is largely due to the greedy branching heuristics in SatZ and the
tradeoff between the search and the inference achieved in ZChaff.
While most of the empirical studies on the phase transitions in the AI literature have
focused on the behavior of the solvers around the solubility threshold of the random
model, we would like to point out that all of the above observations are made at the
ratio of constraints to variables below the respective models’ threshold, a parameter region
where the selection of the branch heuristics and the no-good recording technique make a
big difference.
542

Consistency and Random CSPs

5. Conclusions and Future Directions
Random or semi-random binary constraint instances have been used to test various aspects
of CSP solvers by many researchers. To illustrate robustness of algorithms, it is desirable
that the instances generated under a model not be trivially unsolvable and a minimal
guarantee of interest is that the generators should produce instances that asymptotically
have exponential resolution proofs. In this paper we have shown that if we ensure that
the constraints are such that the instances are strongly 3-consistent or weakly 4-consistent
then this will be the case. In addition we have shown how to create such instances, while
allowing for a high constraint tightness, and allowing considerable flexibility in constraint
design. In the experimental sections we showed a significant increase in difficulty of instances
generated by our model over other models at similar densities and tightness. We also noticed
a double peak phenomenon and after further experiments identified it as an artifact of the
specific solver, but influenced by the choice of the generation model and the characteristics
of the instances. This exemplifies exactly the kind of algorithmic issue that we hope such
generators would help researchers identify and explore.
The generation of hard instances has two foci, one in the restriction of the constraints
and the other restricting the constraint graph (or hyper-graph). For specific problems, for
example independent set (Brockington & Culberson, 1996) or SAT (Bayardo Jr. & Schrag,
1996), it has been observed that techniques such as balancing the degree of the constraint
graph to reduce variance increases the difficulty of instances, whether in camouflaging a
hidden solution or at the phase transition of semi-random instances. We expect that modifying our generation model by also controlling the graph structure might lead to other harder
instances, or other interesting properties that would test the mettle of various algorithms.
For example, suppose we consider the domain D = {0, 1, 2, 3} and let a constraint
have the set of allowed value pairs {(0, 1), (0, 3), (1, 0), (3, 0), (1, 2), (2, 1), (2, 3), (3, 2)}. This
constraint could not be generated by our system using the eight cycle that enforces weak
4-consistency because this value set induces two four cycles on a pair of domains. Thus,
this constraint will not generate weakly 4-consistent CSPs. On the other hand, it is arc
consistent and does not contain a forcer, since each vertex in the induced constraint graph
has degree two. If we consider a constraint graph with a triangle, then applying this
constraint to all three edges will mean there is no satisfying assignment. Since in a random
graph with m/n = c there is a positive (bounded) expected number of triangles, there is
then a positive expectation that an instance generated allowing this constraint would be
trivially unsatisfiable. Thus, it appears that weak 4-consistency is a minimal requirement
for exponential complexity on random constraint graphs. Notice that weak 4-consistency
not only ensures that every triangle is satisfiable, but it also ensures (by induction basically)
that any larger cycle is satisfiable. Thus, speaking in general terms, the only way that a
sub-instance can be unsatisfiable is to have the subgraph contain more edges than vertices.
But random graph analysis shows this means asymptotically the minimal unsatisfiable subinstance is of size O(n), and this is of course a key ingredient of the complexity analysis.
Now, suppose we ensure that the constraint graph has girth g > 3. Again this technique has been used before on specific problems, for example on graph coloring problems
(Culberson, Beacham, & Papp, 1995). We wonder whether or not combining such a graph
restriction together with a weak g + 1-consistency (weaker than weak 4-consistency) might

543

Gao & Culberson

also produce instances with exponentially long resolution proofs. Note that one difficulty
in such an analysis is that we no longer have uniformly random graphs.
Once we start considering the larger picture, involving multiple variables, we naturally
must consider k-ary constraints. As part of future research we expect to consider extending
our model to such cases. Of course some of the experiments on the effects of increasing
tightness presented in this paper are on 3-ary constraints (SAT). In fact, our initial foray
into tightness started with variations of SAT.
As a final cautionary note, we point out that it is well known that it is possible for
CSP (i.e. SAT) instances to have exponential resolution proofs, while being resolvable in
polynomial time by other techniques, such as Gaussian elimination. We are not certain our
system can produce such instances, but see no explicit reason that prevents it from doing
so.

Acknowledgments
A preliminary version of this paper appeared in the Proceedings of the Tenth International
Conference on Principles and Practice of Constraint Programming (CP-2004). We thank
the referees for their helpful comments on the conference version and the journal version.
Thanks are also given to Dr. K. Xu for his comments on the conference version of the paper.
This research has been supported by Natural Sciences and Engineering Research Council
Grant No. OGP8053. Yong Gao is supported in part by a UBCO startup grant and an
NSERC Discovery Grant RGPIN 327587-06.

Appendix A: Proofs of the Theorems
In this section, we present more concepts related to the resolution complexity results stated
in this paper and prove Theorems 2.1, 3.1, and 3.2.
A.1 Proof of Theorem 2.1
Before providing the proof for Theorem 2.1, let us first formalize some definitions such as a
forcer, a forbidding cycle, and an r-flower.
Definition A.1 (Forcers, Molloy & Salavatipour, 2003). A constraint Cf with var(Cf ) =
{x1 , x2 } is called an (α, β)-forcer if its nogood set is
{(α, γ); ∀γ 6= β},
where α, β, and γ are domain values of the involved variables. We say that a constraint C
contains an (α, β)-forcer Cf defined on the same set of variables as C if the nogood set of
Cf is a subset of the nogood set of C.
Definition A.2 (Forbidding cycles and r-flowers, Molloy & Salavatipour, 2003). An αforbidding cycle for a variable x0 is a set of constraints
C1 (x0 , x1 ), C2 (x1 , x2 ), . . . , Cr−1 (xr−2 , xr−1 ), and Cr (xr−1 , x0 )

544

Consistency and Random CSPs

such that C1 (x0 , x1 ) is an (α, α1 )-forcer, Cr (xr−1 , x0 ) is an (αr−1 , αr )-forcer (αr 6= α), and
Ci (xi−1 , xi ) is an (αi−1 , αi )-forcer (2 ≤ i ≤ r − 1). We call x0 the center variable of the
α-forbidding cycle.
An r-flower R = {C1 , · · · , Cd } consists of d (the domain size) forbidding cycles each of
which has the length r such that
1. Ci , 1 ≤ i ≤ d, have the same center variable x;
2. each Ci is a distinct αi -forbidding cycle of the common center variable x; and
3. these forbidding cycles do not share any other variables.
The following facts are straightforward to establish:
1. An r-flower consists of s = d(r − 1) + 1 = dr − d + 1 variables and dr constraints;
2. The total number of r-flowers is
 
n
s!(d − 1)d dd(r−1) .
s
d,t
3. A constraint in Bn,m
[M] contains an (α, β)-forcer only if the pair (α, β) is one of the
tuples that are not considered when selecting the set of nogoods of the constraint.
√
In the following, we assume that r = o( n). The probability that a constraint contains
a forcer and the probability that a random instance of the flawless random CSP model
contains an r-flower are given in the following lemma.

Lemma A.1. Consider the flawless random CSP

d,t
Bn,m
[M]

and define fe =

(d

2 −d−d+1
t−d+1
d2 −d
t

(

)

)

.

1. The probability that a given constraint C(x1 , x2 ) contains an (α, β)-forcer is
1
fe .
d

(A.1)

2. Let R be an r-flower and let c = m/n,
d,t
P {R appears in Bn,m
[M]} = Θ(1)(2cfe )dr

1 1
.
ndr ddr

(A.2)

Proof. Equation (A.1) follows from the following two observations:
1.

1
d

is the probability that (α, β) is one of the tuples that are not considered when
d,t
selecting the set of nogoods of a constraint in Bn,m
[M] and

2. fe is the probability that the d − 1 tuples, (α, γ), γ 6= β, are in the set of t nogoods
selected uniformly at random from d2 − d tuples.

545

Gao & Culberson

d,t
To calculate the probability that a given r-flower R appears in Bn,m
[M], notice that the
probability of selecting all the constraint edges in R is

N −dr
cn(cn − 1) · · · (cn − dr + 1)
cn−dr
=

N
N (N − 1) · · · (N − dr + 1)
cn
 dr
2c
= Θ(1)
n

where N = n2 . Since for each fixed choice of dr constraint edges in the r-flower, the
probability for these constraints to contain the r-flower is ( d1 fe )dr , Equation (A.2) follows.

Proof of Theorem 2.1. Let c∗ =

d
2fe .

We will show that if c =

m
n

> c∗, then

d,t
lim P {Bn,m
[M]} contains an r-flower } = 1.

(A.3)

n→∞

d,t
Let IR be the indicator function of the event that the r-flower R appears in Bn,m
[M] and
let
X
X=
IR
R
d,t
where the sum is over all the possible r-flowers. Then, Bn,m
[M] contains an r-flower if and
only if X > 0.
By Lemma A.1 and the fact that s = dr − d + 1, we have
X
E[X] =
E[IR ]
R

 
n
1 1
= Θ(1)
s!(d − 1)d dd(r−1) (2cfe )dr dr dr
s
n d
1 1
= Θ(1)n(n − 1) · · · (n − s + 1)ddr (2cfe )dr dr dr
n d
= Θ(1)n1−d (2cfe )dr .
Therefore, if c > c∗ and r = λ log n with λ sufficiently large, we have lim E[X] = ∞.
n→∞

If we can show that E[X 2 ] ≤ E 2 [X](1 + o(1)), then an application of the Chebyshev
inequality will establish that lim P {X = 0} = 0. To get an upper bound on E[X 2 ], we
n→∞
need a counting argument to upper bound the number of r-flowers sharing a given number
of edges. This is done by considering how the shared edges form connected components
(Franco & Gelder, 2003; Gao & Culberson, 2003; Molloy & Salavatipour, 2003). Here, we

546

Consistency and Random CSPs

follow the method used by Molloy and Salavatipour (2003), from which we have


s X
i
X
X X
X
X
2
E[X 2 ] =
E[IA
]+
E[IA IB ] +
IA 
Nij (Pij )dr−i 
A

=

X

A B:B∩A=
2
E[IA
]+

X

A

X

E[IA ]E[IB ] +

X

A B:B∩A=


2

i=1 j=1

A

≤ E [X] +

X

IA 

s X
i
X



s X
i
X
IA 
Nij (Pij )dr−i 

A

i=1 j=1


Nij (Pij )dr−i 

(A.4)

i=1 j=1

A

where (1) Nij is the number of the r-flowers that share exactly i constraint edges with A
and these i constraints forms j connected components in the constraint graph of A; and
(2) (Pij )dr−i is the probability conditional on IA , that the random CSP contains the dr − i
constraints of a specific r-flower. In the work of Molloy and Salavatipour (2003), Nij is
upper bounded by
2

(2 + r2 )d (dr2 )j−1 j!ns−i−j ds−i−j+d−1 ,
where ((2 + r2 )d (dr2 )j−1 )2 j! upper bounds the number of ways to choose and arrange the j
shared connected components for two r-flowers; ns−i−j upper bounds the number of ways
of choosing the remaining non-shared variables since the number of variables in each of
the j shared connected components is at least one plus the number of edges in that shared
component; and ds−i−j+d−1 upper bounds the number of ways of choosing the forcing values
in these non-sharing variables. The shared variables have to take the same forcing values
as those in A due to the assumption that t < d (Molloy & Salavatipour, 2003).
Since in our case d−1 ≤ t ≤ d2 −d, it is possible for the shared variables to take different
forcing values in different r-flowers. Thus, an upper bound for Nij is


(2 + r2 )d (dr2 )j−1

2

j!ns−i−j ds .

But in our case, the probability corresponding to (Pij )dr−i is
N −dr−(dr−i)
1
cn−i−(dr−i)
( fe )dr−i

N −dr
d
cn−i

= Θ(1)(2cfe )dr−i

= Θ(1)(

1

1

ndr−i

ddr−i

547

.

cn − i dr−i 1 dr−i
)
( fe )
N − dr
d

Gao & Culberson

Therefore, with c∗ =

d
2fe ,

s X
i
X

we have
Nij (2cfe )dr−i

i=1 j=1

≤

s
X

1

1

ndr−i

ddr−i


(2 + r2 )2d r−4 ns−i ds (2cfe )dr−i

i=1

≤

s
X

O(r4d−4 )n1−d (2cfe )dr

i=1

1

1

ndr−i

ddr−i


i
2
4
X
d r j j
(
)
n
j=1

(2cfe )−i r4
O( )
d−i
n

s

≤ E[X]O(r

4d−4

r4 X d i
)
)O( )
(
n
2cfe
i=1

≤ E[X]O(

r4d
n

),

where the last inequality is because c >
completed.

d
2fe .

From this and formula (A.4), the proof is

Remark A.1. The relatively loose upper bound c∗ = 2fde in the above proof may be improved
by a factor of d by making a further distinction among the r-flowers that share forcing
values at a different number of shared variables. But for the purpose of showing that the
flawless random CSP also has potential embedded easy sub-problems, our upper bound for
the constraint-variable ratio c is sufficient since the domain size d is a constant.
A.2 Proof of Theorems 3.1 and 3.2
Given a CNF formula F, we use Res(F) to denote the resolution complexity of F, i.e., the
length of the shortest resolution refutation of F. The width of deriving a clause A from F,
denoted by w(F ` A), is defined to be the minimum over all the resolution refutations of
the maximum clause size in the resolution refutation. The width w(F) of a formula F is the
size of the largest clause in it. Ben-Sasson and Wigderson (2001) established a relationship
between Res(F) and w(F ` ∅):
Res(F) = eΩ(

(w(F `∅)−w(F ))2
)
n

.

This relationship indicates that to give an exponential lower bound on the resolution complexity, it is sufficient to show that every resolution refutation of F contains a clause whose
size is linear in n, the number of variables.
Let I be an instance of the CSP and let CNF(I) be the CNF encoding of I. Mitchell
(2002b) provided a framework within which one can investigate the resolution complexity of
I, i.e., the resolution complexity of the CNF formula CNF(I) that encodes I, by working
directly on the structural properties of I. Denote by var(I) the set of CSP variables in
I and var(CNF(I)) the set of encoding Boolean variables in CNF(I). A sub-instance J
of I is a CSP instance such that var(J ) ⊂ var(I) and J contains all the constraints of I
whose scope variables are in var(J ). The following crucial concepts make it possible to work
548

Consistency and Random CSPs

directly on the structural properties of the CSP instance when investigating the resolution
complexity of the encoding CNF formula.
Definition A.3 (Implies, Mitchell, 2002b). Let C be a clause over the encoding Boolean
variables in var(CNF(I)). We say that a sub-instance J of I implies C, denoted as J |= C,
if and only if for each assignment to the CSP variables satisfying J , the corresponding
assignment to the encoding Boolean variables satisfies C.
Definition A.4 (Clause Complexity, Mitchell, 2002b). Let I be a CSP instance. For each
clause C defined over the Boolean variables in var(CNF(I)), define
µ(C, I) = min{|var(J )|; J is a sub-instance and implies C}.
The following two concepts slightly generalize those used by Mitchell (2002b) and by
Molloy and Salavatipour (2003) and enable us to have a uniform treatment when establishing
resolution complexity lower bounds.
Definition A.5 (Boundary). The boundary B(J ) of a sub-instance J is defined to be the
set of CSP variables such that a variable x is in B(J ) if and only if the following is true:
If J minimally implies a clause C defined on some Boolean variables in var(CNF(I)), then
C contains at least one of the Boolean variables, x : α, α ∈ D, that encode the CSP variable
x.
Definition A.6 (Sub-critical Expansion, Mitchell, 2002b). Let I be a CSP instance. The
sub-critical expansion of I is defined as
e(I) =

max

min

|B(J )|

(A.5)

0≤s≤µ(∅,I) s/2≤|var(J )|≤s

where the minimum is taken over all the sub-instances of I such that s/2 ≤ |var(J )| ≤ s.
The following theorem relates the resolution complexity of the CNF encoding of a CSP
instance to the sub-critical expansion of the CSP instance.
Theorem A.2 (Mitchell, 2002b). For any CSP instance I, we have
w(CNF(I) ` ∅) ≥ e(I)

(A.6)

Proof. For any resolution refutation π of CNF(I) and s ≤ µ(∅, I), Lemma 1 of Mitchell
(2002b) shows that π must contain a clause C with
s/2 ≤ µ(C, I) ≤ s.
Let J be a sub-instance such that |var(J )| = µ(C, I) and J implies C. Since J minimally
implies C, according to the definition of the boundary, w(C) ≥ |B(J )|. Formula (A.6)
follows.
To establish an asymptotically exponential lower bound on Res(C) of a random CSP C,
it is enough to show that there is a constant β ∗ > 0 such that
lim P {e(C) ≥ β ∗ n} = 1.

n→∞

549

(A.7)

Gao & Culberson

For any α > 0, let Am (α) be the event {µ(∅, C) > αn} and As (α, β ∗ ) be the event
)
(
min

αn
≤|var(J )|≤αn
2

B(J ) ≥ β ∗ n .

Notice that
P {e(C) ≥ β ∗ n} ≥ P {Am (α) ∩ As (α, β ∗ )}
≥ 1 − P {Am (α)} − P {As (α, β ∗ )}.

(A.8)

We only need to find appropriate α∗ and β ∗ such that
lim P {Am (α∗ )} = 0

(A.9)

lim P {As (α∗ , β ∗ )} = 0.

(A.10)

n→∞

and
n→∞

Event Am (α∗ ) is about the size of minimally unsatisfiable sub-instances. For the event
As (α∗ , β ∗ ), a common practice is to identify a special subset of the boundaries and show
that this subset is large. For different random CSP models and under different assumptions
on the model parameters, there are different ways to achieve this. Following Beame et al.
(2005), we say a graph G is (r, q)-dense if there is a subset of r vertices that induces at least
q edges of G.
d,t
Proof of Theorem 3.1. Recall that the constraint graph of Bn,m
[SC] is the standard
random graph G(n, m).
d,t
Since each instance of Bn,m
[SC] is strongly k-consistent, variables in a minimal unsatisfiable sub-instance J with |var(J )| = r must have a vertex degree greater than or equal to
k, and consequently, the constraint sub-graph H(J ) must contain at least rk
2 edges. Thus,
d,t
[SC]) ≤ α∗ n}
P {Am (α∗ )} = P {µ(∅, Bn,m
( α∗ n
)
[
≤ P
{G(n, m) is (r, rk/2)-dense } .
r=k+1

Let B k (J ) be the set of the variables in var(J ) whose vertex degrees are less than k. Again,
d,t
since instances of Bn,m
[SC] are always strongly k-consistent, we have B k (J ) ⊂ B(J ) and
k
thus, |B(J )| ≥ |B (J )|. Therefore, the probability P {As (α∗ , β ∗ )} can be bounded by
P {As (α∗ , β ∗ )} ≤ P {Aks (α∗ , β ∗ )}
where Aks (α∗ , β ∗ ) is the event


k

min

α∗ n/2≤|var(J )|≤α∗ n

550

∗



B (J ) ≥ β n .

Consistency and Random CSPs

Random graph arguments (see, e.g., Beame et al., 2005) show that there exist constants
α∗ and β ∗ such that P {Am (α∗ } and P {Aks (α∗ , β ∗ )} both tend to 0. Indeed, let β ∗ be such
∗
n(n−1)
that (1−β2 )k > 1, c = m
. We have
n , and N =
2
(
P {Am (α∗ )} ≤ P

∗
α
[n

)
{G(n, m) is (r, rk/2)-dense }

r=k+1

≤

α∗ n
X

P {G(n, m) is (r,

r=k+1

rk
)-dense}
2

α∗ n   r(r−1) 
X
N−
n
2
≤
rk
r
m−
2
r=k+1

rk  −1
N
2
rk
m
2

∗

α n
X
en e(r − 1) rk 2c rk
)2( )2
≤
( )r (
r
k
n
r=k+1


α∗ n 
X
en 2ec(r − 1) k r
=
(
)2
r
kn
r=k+1


α∗ n 
X
k k k+2 k r k−2 r
=
( )2 e 2 c2 ( ) 2
2
n
r=k+1

blog nc 

≤

X

r=k+1

k k k+2 k log n k−2
( )2 e 2 c2 (
) 2
2
n
∗

α n
X

+

r=blog nc

Similarly, we have for β =





k k k+2 k ∗ k−2 log n
( ) 2 e 2 c 2 (α ) 2
2

2β ∗
α∗ ,

P {Aks (α∗ , β ∗ )} = P


∗n

 α[

{∃ a size-r sub-instance J s.t. |Bk (J )| ≤ β ∗ n}


r= α∗ n

∗n

 α[





r(1 − β)k
{G(n, m) is (r,
)-dense}


2
r= α∗ n

2

r

 (1−β)k
α∗ n
X
2
(1−β)k+2
(1−β)k−2
2c


≤
e 2 (α∗ ) 2
(1
−
β)k
α∗ n
r=







2

≤P

(A.11)

(A.12)

2

where the second inequality is because of the fact that for a sub-instance J with size r and
∗
|Bk (J )| ≤ β ∗ n, its constraint graph contains at least r − β ∗ n = r − α2 βn ≥ r − βr vertices
whose degree is at least k.

551

Gao & Culberson

∗

∗

(1−β )k
There exist α∗ and β ∗ be such that (1) 2β
> 1; and (3) the right hand
α∗ < 1; (2)
2
side of formula (A.11) and the right hand side of formula (A.12) both tend to zero. This
completes the proof of Theorem 3.1.
d,t
We now prove Theorem 3.2. First from the definition of Bn,m
[W C], we have the following
d,t
Lemma A.3. For the random CSP Bn,m
[W C], we have

1. Every sub-instance whose constraint graph is a cycle is satisfiable;
2. for any path of length ≥ 3, any compatible assignments to the two variables at the
ends of the path can be extended to assignments that satisfy the whole path.
In an effort to establish exponential lower bounds on the resolution complexity for a
classical random CSP models with a tightness higher than those established by Mitchell
(2002b), Molloy and Salavatipour (2003) introduced a collection of sub-instances, denoted
here as BM (J ), and used its size to give a lower bound on the size of the boundary. For
binary CSPs whose constraints are arc-consistent and contain no forcer, BM (J ) consists of
1 (J ) and B 2 (J ), defined respectively as follows:
two parts: BM
M
1 (J ) contains the set of single-edge sub-instances X , i.e., |var(X )| = 2, such that
1. BM
at least one of the variables has a vertex degree 1 in the original constraint graph;
2 (J ) contains the set of sub-instances X whose induced constraint graph is a pen2. BM
dant path of length 4, i.e., a path of length 4 such that no vertex other than the
endpoints has a vertex degree greater than 2 in the original constraint graph.

It can be shown that
Lemma A.4. For any weakly path-consistent CSP sub-instance J , we have
1
|B(J )| ≥ |BM
(J )| +

2 (J )|
|BM
.
4

1 (J ) has to be in B(J ); At
Proof. The variable with degree one in any sub-instance in BM
2
least one internal variable in any pendant path BM (J ) has to be in B(J ). It is possible
that several pendant paths of length 4 share a common internal variable that is in B(J ),
e.g., in a very long pendant path. But a variable can only appear in at most three pendant
paths of length 4.

With the above preparations, the proof provided for Theorem 1 of Molloy and Salavatipour
(2003) readily applies to our case. To make this report self-contained, we give the proof
below.
Proof of Theorem 3.2. By Lemma A.3, any minimally unsatisfiable sub-instance J is
such that (1) its constraint graph cannot be a single cycle; and (2) BM (J ) is empty since
1 (J )| = 0 and |B 2 (J )| = 0 for a minimally unsatisfiable sub-instance. According to
|BM
M
1
Lemma 11 of Molloy and Salavatipour, the constraint graph of J has at least (1+ 12
)var(J )

552

Consistency and Random CSPs

edges. Therefore, due to the locally sparse property of random graphs, there is a constant
α∗ > 0 such that formula (A.9) holds, i.e.,
lim P {Am (α∗ )} = 0.

n→∞

To establish formula (A.10), due to Lemma A.4 we have
P {As (α∗ , β ∗ )} ≥ P {As,M (α∗ , β ∗ )}
where As,M (α∗ , β ∗ ) is the event

min

α∗ n/2≤|var(J )|≤α∗ n


|BM (J )| ≥ β ∗ n .

Now suppose on the contrary that for any ζ > 0, there is a sub-instance J with α∗ n/2 ≤
1 (J )| + |B 2 (J )| ≤ ζn. Then, from Lemmas 10 and 11 of Molloy
|var(J )| ≤ α∗ such that |BM
M
and Salavatipour, the constraint graph of J contains only cycle components, where Lemma
11 of Molloy and Salavatipour asserts that the edges-to-vertices ratio of the constraint graph
of J has to be bigger than one. If we remove all the cycle components from the constraint
graph of J , the edges-to-vertices ratio of the remaining graph becomes even bigger. But
this is impossible because the constraint graph of J , and hence the remaining graph, has
less than α∗ n vertices.
It is well-known that w.h.p. a random graph has fewer than log n cycle components
of length at most 4; for the random graph G(m, n) with m/n = c being constant, the
number of cycle components with a fixed length has asymptotically a Poisson distribution
(Bollobas, 2001). Thus, the number of variables that are in cycle components of length
4 is at most 4 log n. Since any cycle component of length l > 4 contains l pendant paths
of length 4, the total number of variables in cycle components of length greater than 4 is
2 (J )| < ζn. Therefore, we have var(J ) < ζn + 4 log n < α∗ n/2 ≤ var(J ) for
at most |BM
sufficiently small ζ, a contradiction.
We, therefore, conclude that there is a β ∗ such that w.h.p, for any sub-instance J with
α∗ n/2 ≤ |var(J )| ≤ α∗ , |BM (J )| ≥ β ∗ n, i.e., formula (A.10) holds.
A.3 Upper Bound on the Threshold of the Random Restricted Binary CSP
d,t
Bn,m
[L]
In this subsection, we show that the condition mentioned at the end of Definition 2.1
d,t
guarantees the existence of a constant c∗ such that Bn,m
[L] with m/n > c∗ is asymptotically
unsatisfiable with probability one. We state and prove the result for the case of binary
constraints, but similar results also hold for the more general k-ary constraints.
Recall that the condition on the set of relations L = {L1 , L2 , . . . | Li ⊂ D × D} is
\
(a, a) ∈
/
Li , ∀a ∈ D.
(A.13)
i≥1

Since we assume that the domain size is fixed, for any a ∈ D the probability that a constraint
d,t
in Bn,m
[L] contains (a, a) as one of its nogoods is lower bounded by a constant p.
553

Gao & Culberson

d,t
Theorem A.5. If Bn,m
[L] is such that the set of relations L satisfies the condition A.13,
then it is unsatisfiable w.h.p. if m
n = n is such that

d(1 −

p c
) < 1.
d2

Proof. For any assignment A, there must be a subset S of nd variables that are assigned a
d,t
common domain value a. Then, A satisfies Bn,m
[L] only if A satisfies all the constraints
d,t
that only involve the variables in S. Let Hi be the event that Bn,m
[L] has i constraints that
only involve the variables in S. Then, we have
d,t
P {A satisfies Bn,m
[L]} ≤

cn
X

(1 − p)i P {Hi }.

i=0

Let N = 21 n(n − 1). Write
N1 =

1n n
1
( − 1) = 2 n(n − 1) + O(n)
2d d
2d

for the number of possible edges induced by S, and
1
1
N2 = N − N1 = (1 − 2 )n(n − 1) + O(n).
2
d
We have for any 1 ≤ i ≤ cn − 1
 N2 
N1
P {Hi } =

i

cn−i

N
cn

N1 (N1 − 1) · · · (N1 − i + 1) ∗ N2 (N2 − 1) · · · (N2 − cn + i + 1) (cn)!
N (N − 1) · · · (N − cn + 1)
i!(cn − i)!
 i 
cn−i  
N1
N2
cn
≤
N
N −i
i
 i 
cn−i  
N1
N2
cn
≤
i
N
N − cn
=

Similarly, we have
N2
cn

N
cn



P {H0 } =
and


≤

N1
cn

N
cn

N2
N − cn



P {Hcn } =

554


≤

N1
N

cn

cn

Consistency and Random CSPs

So,
d,t
P {A satisfies Bn,m
[L]} ≤

cn
X

(1 − p)i P {Hi }

i=0
cn
X

cn−i  
N2
cn
N1
≤
(1 − p)
N
N − cn
i
i=0




cn
cn−i  
X
N1 i N2 N
cn
≤
(1 − p)
N
N N − cn
i
i=0
cn

N1 N2 N
= (1 − p)
+
N
N N − cn


1
1 cn
= (1 − p) 2 + (1 − 2 )
O(1)
d
d

p cn
O(1)
= 1− 2
d
Therefore,

i



i 


p cn 
p n
d,t
P {Bn,m
[L] is satisfiable } ≤ dn 1 − 2
= d(1 − 2 )c
d
d

References
Achlioptas, D., Beame, P., & Molloy, M. (2001). A sharp threshold in proof complexity.
In Proceedings of the 33rd Annual ACM Symposium on Theory of Computing, pp.
337–346.
Achlioptas, D., L.M.Kirousis, E.Kranakis, D.Krizanc, M.Molloy, & Y.C.Stamation (1997).
Random constraint satisfaction: A more accurate picture. In Proceedings of Principles
and Practice of Constraint Programming (CP-1997), pp. 107–120. Springer.
Bayardo Jr., R. J., & Schrag, R. (1996). Using CSP look-back techniques to solve exceptionally hard SAT instances. In Principles and Practice of Constraint Programming
(CP-1996), pp. 46–60.
Beame, P., Culberson, J., Mitchell, D., & Moore, C. (2005). The resolution complexity of
random graph k-colorability. Discrete Applied Mathematics, 153 (1-3), 25–47.
Beame, P., Karp, R. M., Pitassi, T., & Saks, M. E. (1998). On the complexity of unsatisfiability proofs for random k-CNF formulas. In Proceedings of the 30th Annual ACM
Symposium on Theory of Computing, pp. 561–571.
Ben-Sasson, E., & Wigderson, A. (2001). Short proofs are narrow - resolution made simple.
Journal of the ACM, 49 (2), 149–169.
Bollobas, B. (2001). Random Graphs. Cambridge University Press.
Bondy, J. A., & Murty, U. S. R. (1976). Graph Theory with Applications. The MacMillan
Press Ltd.

555

Gao & Culberson

Brockington, M., & Culberson, J. C. (1996). Camouflaging independent sets in quasirandom graphs. In Johnson, D. S., & Trick, M. A. (Eds.), Cliques, Coloring, and
Satisfiability: Second DIMACS Implementation Challenge, Vol. 26, pp. 75–88. American Mathematical Society.
Chvatal, V., & Szemeredi, E. (1988). Many hard examples for resolution. Journal of the
Association for Computing Machinery, 35 (4), 759–768.
Culberson, J., Beacham, A., & Papp, D. (1995). Hiding our colors. In CP’95 Workshop on
Studying and Solving Really Hard Problems, pp. 31–42, Cassis, France.
Franco, J., & Gelder, A. V. (2003). A perspective on certain polynomial-time solvable
classes of satisfiability. Discrete Applied Mathematics, 125 (2-3), 177–214.
Frieze, A., & Molloy, M. (2003). The satisfiability threshold for randomly generated binary
constraint satisfaction problems. In 7th International Workshop on Randomization
and Approximation Techniques in Computer Science, RANDOM 2003, pp. 275–289.
Gao, Y., & Culberson, J. (2003). Resolution complexity of random constraint satisfaction
problems: Another half of the story. In LICS’03 Workshop on Typical Case Complexity
and Phase Transitions.
Gao, Y., & Culberson, J. (2004). Consistency and random constraint satisfaction models
with a high constraint tightness. In Proceedings of the Tenth International Conference
on Principles and Practice of Constraint Programming (CP-2004), pp. 17–31.
Gent, I., MacIntyre, E., Prosser, P., Smith, B., & Walsh, T. (2001). Random constraint
satisfaction: Flaws and structure. Constraints, 6 (4), 345–372.
Johnson, D. S. (2002). A theoretician’s guide to the experimental analysis of algorithms. In
Data Structures, Near Neighbor Searches, and Methodology: Fifth and Sixth DIMACS
Implementation Challenges, pp. 215–250. American Mathematical Society.
Li, C. M., & Anbulagan (1997). Heuristics based on unit propagation for satisfiability problems. In Proceedings of 15th International Joint Conference on Artificial Interlligence
(IJCAI’97), pp. 366–371.
MacIntyre, E., Prosser, P., Smith, B., & Walsh, T. (1998). Random constraint satisfaction: theory meets practice. In Proceedings of Principles and Practices of Constraint
Programming(CP-1998), pp. 325–339. Springer.
Mackworth, A. K. (1977). Consistency in networks of relations. Artificial Intelligence, 8,
99–118.
Mitchell, D. (2002a). The Resolution Complexity of Constraint Satisfaction. Ph.D. thesis,
Department of Computer Science, University of Toronto, Canada.
Mitchell, D. (2002b). Resolution complexity of random constraints. In Proceedings of Principles and Practices of Constraint Programming (CP-2002), pp. 295–309. Springer.
Molloy, M. (2002). Models and thresholds for random constraint satisfaction problems. In
Proceedings of the 34th ACM Symposium on Theory of Computing, pp. 209 – 217.
ACM Press.

556

Consistency and Random CSPs

Molloy, M., & Salavatipour, M. (2003). The resolution complexity of random constraint
satisfaction problems. In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science (FOCS-2003), pp. 330–339. IEEE Press.
Prosser, P. (1996). An empirical study of phase transitions in binary constraint satisfaction
problems. Artificial Intelligence, 81 (1-2), 81–109.
Smith, B. M. (2001). Constructing an asymptotic phase transition in random binary constraint satisfaction problems. Theoretical Computer Science, 265 (1-2), 265–283.
Wormald, N. C. (1999). Models of random regular graphs. In Surveys in Combinatorics,
London Mathematical Society Lecture Note Series, vol. 276, pp. 239–298. Cambridge
Univ. Press.
Xu, K., & Li, W. (2000). Exact phase transitions in random constraint satisfaction problems.
Journal of Artificial Intelligence Research, 12, 93–103.
Zhang, L., Madigan, C., Moskewicz, M., & Malik, S. (2001). Efficient conflict driven learning in a boolean satisfiability solver. In Proceedings of International Conference on
Computer Aided Design (ICCAD2001), pp. 530–535.

557

Journal of Artificial Intelligence Research 28 (2007) 157–181

Submitted 08/06; published 02/07

Junta Distributions and the Average-Case Complexity of
Manipulating Elections
Ariel D. Procaccia
Jeffrey S. Rosenschein

arielpro@cs.huji.ac.il
jeff@cs.huji.ac.il

School of Engineering and Computer Science,
The Hebrew University of Jerusalem,
Jerusalem 91904, Israel

Abstract
Encouraging voters to truthfully reveal their preferences in an election has long been
an important issue. Recently, computational complexity has been suggested as a means of
precluding strategic behavior. Previous studies have shown that some voting protocols are
hard to manipulate, but used N P-hardness as the complexity measure. Such a worst-case
analysis may be an insufficient guarantee of resistance to manipulation.
Indeed, we demonstrate that N P-hard manipulations may be tractable in the averagecase. For this purpose, we augment the existing theory of average-case complexity with
some new concepts. In particular, we consider elections distributed with respect to junta
distributions, which concentrate on hard instances. We use our techniques to prove that
scoring protocols are susceptible to manipulation by coalitions, when the number of candidates is constant.

1. Introduction
Multiagent environments are often inhabited by heterogeneous, selfish agents, continually
interacting but sharing few common goals. In such settings, agents may have diverse — or
even conflicting — preferences. Therefore, reaching consensus among agents has long been
an important issue.
A general, well-studied and well-understood scheme for preference aggregation is voting:
the agents reveal their preferences by ranking a set of candidates, and a winner is determined
according to a voting protocol. The candidates in the election can be beliefs, plans (Ephrati
& Rosenschein, 1997), schedules (Haynes, Sen, Arora, & Nadella, 1997), or indeed many
other less obvious entities, such as movies (Ghosh, Mundhe, Hernandez, & Sen, 1999).
Applications of voting, in place of other methods, are motivated by theoretical guarantees
provided by various voting protocols. For instance, Ghosh et al. (1999) present a movie
recommender system that relies on voting, and makes use of voting properties to generate
convincing explanations for different recommendations.
There is, however, an obstacle that has always plagued voting theory, and social choice
theory in general: strategic behavior on the part of voters. In our setting, a self-interested
agent may reveal its preferences untruthfully, if it believes this would make the final outcome
of the elections more favorable for it. Manipulation is generally regarded as a problem, since
it makes the actual ballot into a complex game, where the voters react and counter-react to
the strategies of others. Not only does this require a larger investment of (computational)
resources by voters, it may result in a socially undesirable alternative being chosen.
c
2007
AI Access Foundation. All rights reserved.

Procaccia & Rosenschein

The celebrated Gibbard-Satterthwaite Theorem (Gibbard, 1973; Satterthwaite, 1975)
establishes that in any deterministic voting protocol that is non-dictatorial,1 there are
elections where an agent is better off by voting untruthfully. Consequently, it is not possible
to design a nonmanipulable voting system so as to guarantee that voters act honestly.
Fortunately, it is reasonable to make the assumption that the agents are computationally bounded. Therefore, although in principle an agent may be able to manipulate
an election, the computation required may be infeasible. This has motivated researchers
to study the computational complexity of manipulating voting protocols. Indeed, it has
been demonstrated that several voting protocols are N P-hard to manipulate by a single
voter (Bartholdi, Tovey, & Trick, 1989a; Bartholdi & Orlin, 1991). Hereinafter we mainly
focus our attention on a setting in which multiple manipulators collude in order to achieve
a certain outcome. In this setting, manipulation is even harder: it is known that the
coalitional manipulation problem is N P-hard in numerous voting protocols, even when the
number of candidates is constant.
These results suggest that computational complexity may be the cure to the malady
called “Manipulation”. In Computer Science, though, the notion of hardness is usually
considered in the sense of worst-case complexity. Indeed, most results on the complexity
of manipulation use N P-hardness as the complexity measure. Therefore, it could still be
the case that most instances of the problem are easy to manipulate. To put it differently, a
strategic voter may usually succeed in finding a beneficial manipulation, and do so efficiently,
even when the problem is hard in the worst-case. If so, the truly significant issue is the
average-case complexity of manipulations.
Sadly, so far all attempts to design a voting protocol that is resistant to manipulations
in the average-case have failed. This suggests that the manipulation problem is inherently
easy in the average-case — and pushes us to analytically support this claim: we must
characterize settings and protocols that can easily be manipulated in the average-case.
A relatively little-known theory of average case complexity exists (Trevisan, 2002); that
theory introduces the concept of distributional problems, and defines what a reduction
between distributional problems is. It is also known that there are average-case complete
problems. However, the goal of the existing theory is to define when a problem is hard in
the average-case; it does not provide criteria for deciding when a problem is easy.
In this paper, we engage in a novel average-case analysis, based on criteria we propose.
Coming up with an “interesting” distribution of problem instances with respect to which
the average-case complexity is computed is a difficult task, and our solution may be controversial. We analyze problems whose instances are distributed with respect to a junta
distribution. Such a distribution must satisfy several conditions, which (arguably) guarantee that it focuses on instances that are harder to manipulate. We consider a protocol to
be susceptible to manipulation when there is a polynomial time algorithm that can usually
manipulate it: the probability of failure (when the instances are distributed according to a
junta distribution) must be inverse-polynomial. Such an algorithm is known as a heuristic
polynomial time algorithm.
We use these new methods to analytically establish the following result: an important
family of voting protocols, called scoring protocols, is susceptible to coalitional manipulation
1. In a dictatorial protocol, there is an agent that dictates the outcome regardless of the others’ choices.

158

Junta Distributions

when the number of candidates is constant. Specifically, we contemplate sensitive scoring
protocols, which include such well-known protocols as Borda and Veto. To accomplish this
task, we define a natural distribution µ∗ over the instances of a well-defined coalitional
manipulation problem, and show that this is a junta distribution. Furthermore, we present
the manipulation algorithm Greedy, and prove that it usually succeeds with respect to
µ∗ . The significance of this result stems from the fact that sensitive scoring protocols are
N P-hard to manipulate, even when the number of candidates is constant. We support
our claim that junta distributions provide a good benchmark by proving that Greedy also
usually succeeds with respect to the uniform distribution.
We also show that all protocols are susceptible to a certain setting of manipulation,
where the manipulator is unsure about the others’ votes. This result depends upon a basic
conjecture regarding junta distributions.
The paper proceeds as follows: in Section 2, we outline some important voting protocols,
and define the manipulation problems we shall discuss. In Section 3, we formally introduce
the tools for our average case analysis: junta distributions, heuristic polynomial time, and
susceptibility to manipulations. In Section 4 we prove our main result: sensitive scoring
protocols are susceptible to coalitional manipulation with few candidates. In Section 5,
we discuss the case when a single manipulator is unsure about the other voters’ votes. In
Section 6 we survey related work. Finally, in Section 7, we present our conclusions and
directions for future research.

2. Preliminaries
We first describe some common voting protocols and formally define the manipulation
problems with which we shall deal. Next, we introduce two useful lemmas from probability
theory.
2.1 Elections and Manipulations
An election consists of a set C = {c1 , c2 , . . .} of candidates and a set V = {v1 , v2 , . . . , } of
voters, who provide a total order on the candidates. An election also includes a winner
determination function from the set of all possible combinations of votes to C. We note
that throughout this paper the number of candidates is constant, so the complexity results
are in terms of the number of voters.
Different voting protocols are distinguished by their winner determination functions.
The protocols we shall discuss are:
• Scoring protocols: A scoring protocol is defined by vector α
~ = hα1 , α2 , . . . , α|C| i, such
that α1 ≥ α2 ≥ . . . ≥ α|C| and αi ∈ N ∪ {0}. A candidate receives αi points for each
voter which ranks it in the i’th place. Examples of scoring protocols are:
– Plurality: α
~ = h1, 0, . . . , 0, 0i.
– Veto: α
~ = h1, 1, . . . , 1, 0i.
– Borda: α
~ = h|C| − 1, |C| − 2, . . . , 1, 0i.
159

Procaccia & Rosenschein

• Copeland: For each possible pair of candidates, simulate an election; a candidate wins
such a pairwise election if more voters prefer it over the opponent. A candidate gets
1 point for each pairwise election it wins, and −1 for each pairwise election it loses.
• Maximin: A candidate’s score in a pairwise election is the number of voters that
prefer it over the opponent. The winner is the candidate whose minimum score over
all pairwise elections is highest.
• Single Transferable Vote (STV): The election proceeds in rounds. In each round, the
candidate’s score is the number of voters that rank it highest among the remaining
candidates; the candidate with the lowest score is eliminated.
Remark 1. We assume that tie-breaking is always adversarial to the manipulator.2
In the case of weighted votes, a voter with weight k ∈ N is naturally regarded as k voters
who vote unanimously. In this paper, we consider weights in [0, 1]. This is equivalent, since
any set of integer weights that are exponential in n can be scaled down to rational weights
in the segment [0, 1], represented using O(n) bits.
The main results of the paper focus on scoring protocols. We shall require the following
definition:



Definition 1. Let P be a scoring protocol with parameters α
~ = α1 , α2 , . . . , α|C| . We say
that P is sensitive iff α1 ≥ α2 ≥ . . . ≥ α|C|−1 > α|C| = 0 (notice the strict inequality on the
right).
In particular, Borda and Veto are sensitive scoring protocols.
Remark 2. Generally, from any scoring protocol with α|C|−1 > α|C| , an equivalent sensitive
scoring protocol can be obtained by subtracting α|C| on a coordinate-by-coordinate basis
from the vector α
~ . Moreover, observe that if a protocol is a scoring protocol but is not
sensitive, and α|C| = 0, then α|C|−1 = 0. In this case, for three candidates it is equivalent to
the plurality protocol, for which all interesting formulations of the manipulation problem
are tractable even in the worst-case. Therefore, it is sufficient to restrict our results to
sensitive scoring protocols.
We next consider some types of manipulations, state the appropriate complexity results,
and introduce some notations.
Remark 3. We discuss the constructive cases, where the goal is trying to make a candidate
win, as opposed to destructive manipulation, where the goal is to make a candidate lose.
Constructive manipulations are always at least as hard (in the worst-case sense) as their
destructive counterparts, and in some cases strictly harder (if one is able to determine
whether p can be made to win, one can also ask whether any of the other m − 1 candidates
can be made to win, thus making p lose).
Definition 2. In the Individual-Manipulation (IM) problem, we are given all the other
votes, and a preferred candidate p. We are asked whether there is a way for the manipulator
to cast its vote so that p wins.
2. This is a standard assumption, also made, for example, in the work of Conitzer and Sandholm (2002),
and Conitzer, Lang, and Sandholm (2003).

160

Junta Distributions

Bartholdi and Orlin (1991) show that IM is N P-complete in Single Transferable Vote,
provided the number of candidates is unbounded. However, the problem is in P for most
well-known voting schemes, and hence will not be studied here.
In the lion’s share of this paper, we consider the coalitional manipulation setting. In
this scenario, the set V of voters is partitioned into two subsets: the set V1 = {v1 , . . . , vn }
of manipulative, or untruthful, voters; and the set V2 = {vn+1 , . . . , vn+N } of nonmanipulative voters. The set of candidates is C = {c1 , . . . , cm , p}. The manipulators’ goal is
to make the distinguished candidate p win the election, by coordinating their rankings of
candidates. In the CWM and SCWM problems, the manipulators have full knowledge of
the nonmanipulators’ votes.
Definition 3. In the Coalitional-Weighted-Manipulation (CWM) problem, we are
given the set of voters V = V1 ] V2 , the set of candidates C, the weights of all voters, and
a preferred candidate p ∈ C. In addition, we are given the votes of the voters in V2 , and
assume the manipulators are aware of these votes. We are asked whether it is possible for
the manipulators in V1 to cast their votes in a way that makes the preferred candidate p
win the election.
We know (Conitzer & Sandholm, 2002; Conitzer et al., 2003) that CWM is N P-complete
in Borda, Veto, and Single Transferable Vote, even with 3 candidates, and in Maximin and
Copeland with at least 4 candidates.
The CWM version that we shall analyze, which is specifically tailored for scoring protocols, is a slightly modified version whose analysis is more straightforward:
Definition 4. In the Scoring-Coalitional-Weighted-Manipulation (SCWM) problem, we are given an initial score S[c] for each candidate c, the weights of the manipulators
in V1 , and a preferred candidate p. We are asked whether it is possible for the manipulators
in V1 to cast their votes in a way that makes the preferred candidate p win the election.
S[c] can be interpreted as c’s total score from the votes in V2 . However, we do not
require that there exist a combination of votes that actually induces S[c] for all c.
Another setting that we shall shortly discuss (in Section 5) is the scenario where the
manipulators are uncertain about the others’ votes.
Definition 5. In the Uncertain-Votes-Weighted-Evaluation (UVWE) problem, we
are given a weight for each voter, a distribution over all the votes, a candidate p, and a
number r ∈ [0, 1]. We are asked whether the probability of p winning is greater than r.
Definition 6. In the Uncertain-Votes-Weighted-Manipulation (UVWM) problem,
we are given a single manipulative voter with a weight, weights for all other voters, a
distribution over all the nonmanipulators’ votes, a candidate p, and a number r, where
r ∈ [0, 1]. We are asked whether the manipulator can cast its vote so that p wins with
probability greater than r.
If CWM is N P-hard for a protocol, then UVWE and UVWM are also N P-hard for
that protocol (Conitzer & Sandholm, 2002).
We make the assumption that the given distributions over the nonmanipulators’ votes
can be sampled in polynomial time. In other words, given a distribution over nonmanipulators’ votes, it is possible to obtain a specific instance in polynomial time.
161

Procaccia & Rosenschein

2.2 Probability Theory Tools
The following lemma will be of much use later on. Informally, it states that the average of
independent identically distributed (i.i.d.) random variables is almost always close to the
expectation.
Lemma 1 (Chernoff’s Bounds). (Alon & Spencer, 1992) Let X1 , . . . , Xt be i.i.d. random
variables such that a ≤ Xi ≤ b and E[Xi ] = µ. Then for any  > 0, it holds that:
• Pr[ 1t

Pt

−2t

2
(b−a)2

• Pr[ 1t

Pt

−2t

2
(b−a)2

i=1 Xi ≥ µ + ] ≤ e
i=1 Xi ≤ µ − ] ≤ e

Another tool that we shall require is the Central Limit Theorem. For our purposes, it
implies that the probability that a sum of random variables takes values in a very small
segment is very small.
Lemma 2 (Central Limit Theorem). (Feller, 1968) Let Xt , . . . , Xt be independent continuous random variables with common density function, having expected value µ and variance
σ 2 . Then for a < b:
"
#
Pt
Z b
x2
X i − tµ
1
t→∞
i=1√
√
Pr a <
< b −→
e− 2 dx.
tσ
2π a

3. Our Approach
In this section we lay the mathematical foundations required for an average-case analysis
of the complexity of manipulations. All of the definitions are as general as possible; they
can be applied to the manipulation of any mechanism, not merely to the manipulation of
voting protocols.
We describe a distribution over the instances of a problem as a collection of distributions
µ = {µn }n∈N , where µn is a distribution over the instances x such that |x| = n. We wish to
analyze problems whose instances are distributed with respect to a distribution that focuses
on hard-to-manipulate instances. Ideally, we would like to ensure that if one manages to
produce an algorithm that can usually manipulate instances according to this distinguished
“difficult” distribution, the algorithm would also usually succeed when the instances are
distributed with respect to most other reasonable distributions.
Definition 7. Let µ = {µn }n∈N be a distribution over the possible instances of an N Phard manipulation problem M . µ is a junta distribution if and only if µ has the following
properties:
1. Hardness: The restriction of M to µ is the manipulation problem whose possible
instances are only:
[
{x : |x| = n ∧ µn (x) > 0}.
n∈N

Deciding this restricted problem is still N P-hard.
162

Junta Distributions

2. Balance: There exist a constant c > 1 and N ∈ N such that for all n ≥ N :
1
1
≤ Prx∼µn [M (x) = “yes”] ≤ 1 − .
c
c
3. Dichotomy: for all n and instances x such that |x| = n:
µn (x) ≥ 2−polyn ∨ µn (x) = 0.
If M is a voting manipulation problem, we also require the following property:
4. Symmetry: Let v be a nonmanipulative voter, let c1 , c2 6= p be two candidates, and
let i ∈ {1, . . . , m}. The probability that v ranks c1 in the i’th place is the same as the
probability that v ranks c2 in the i’th place.
If M is a coalitional manipulation problem, we also require the following property:
5. Refinement: Let x be an instance such that |x| = n and µn (x) > 0; if all manipulators
voted identically, then p would not be elected.
The name “junta distribution” comes from the idea that in such a distribution, relatively few “powerful” and difficult instances represent all the other problem instances.
Alternatively, our intent is to have a few problematic distributions (the family of junta distributions) convincingly represent all other distributions with respect to the average-case
analysis.
The first three properties are basic, and are relevant to problems of manipulating any
mechanism. The definition is modular, and additional properties may be added on top of
the basic three, in case one wishes to analyze a mechanism which is not a voting protocol.
The exact choice of properties is of extreme importance (and, as we mentioned above,
may be arguable). We shall briefly explain our choices. Hardness is meant to ensure that the
junta distribution contains hard instances. Balance guarantees that a trivial algorithm that
always accepts (or always rejects) has a significant chance of failure. The dichotomy property helps in preventing situations where the distribution gives a (positive but) negligible
probability to all the hard instances, and a high probability to several easy instances.
We now examine the properties that are specific to manipulation problems. The necessity of symmetry is best explained by an example. Consider CWM in STV with m ≥ 3.
One could design a distribution where p wins if and only if a distinguished candidate loses
the first round. Such a distribution could be tailored to satisfy the other conditions, but
misses many of the hard instances. In the context of SCWM, we interpret symmetry in the
following way: for every two candidates c1 , c2 6= p and y ∈ R,
Pr [S[c1 ] = y] = Pr [S[c2 ] = y].

x∼µn

x∼µn

Refinement is less important than the other four properties, but seems to help in concentrating the probability on hard instances. Observe that refinement is only relevant to
coalitional manipulation; we believe that in the analysis of individual voting manipulation
problems, the first four properties are sufficient.
163

Procaccia & Rosenschein

Definition 8. (Trevisan, 2002) A distributional problem is a pair hL, µi where L is a decision
problem and µ is a distribution over the set {0, 1}∗ of possible inputs.
Informally, an algorithm is a heuristic polynomial time algorithm for a distributional
problem if it runs in polynomial time, and fails only on a small fraction of the inputs. We
now give a formal definition; this definition is inspired by Trevisan (2002) (there the same
name is used for a somewhat different definition).
Definition 9. Let M be a manipulation problem and let hM, µi be a distributional problem.
1. An algorithm A is a deterministic heuristic polynomial time algorithm for the distributional manipulation problem hM, µi if A always runs in polynomial time, and there
exists a polynomial p of degree at least 1 and N ∈ N such that for all n ≥ N :
Pr [A(x) 6= M (x)] ≤

x∼µn

1
.
p(n)

(1)

2. Let A be a probabilistic algorithm, which uses a random string s. A is a probabilistic heuristic polynomial time algorithm for the distributional manipulation problem
hM, µi if A always runs in polynomial time, and there exists a polynomial p of degree
at least 1 and N ∈ N such that for all n ≥ N :
Prn [A(x) 6= M (x)] ≤

x∼µ ,s

1
.
p(n)

(2)

Probabilistic algorithms have two potential sources of failure: an unfortunate choice of
input, or an unfortunate choice of random string s. The success or failure of deterministic
algorithms depends only on the choice of input.
We now combine all the definitions introduced in this section in an attempt to establish
when a mechanism is susceptible to manipulation in the average case. The following definition abuses notation a bit: M is used both to refer to the manipulation itself, and to the
corresponding decision problem.
Definition 10. We say that a mechanism is susceptible to a manipulation M if there
exists a junta distribution µ, such that there exists a deterministic/probabilistic heuristic
polynomial time algorithm for hM, µi.

4. Formulation, Proof, and Justification of Main Result
Recall (Conitzer & Sandholm, 2002; Conitzer et al., 2003) that in Borda and Veto, CWM is
N P-hard, even with 3 candidates. Since Borda and Veto are examples of sensitive scoring
protocols, we would like to know how resistant this family of protocols really is with respect
to coalitional manipulation. In this section we use the methods from the previous section
to prove our main result:
Theorem 1. Let P be a sensitive scoring protocol. If m = O(1) then P , with candidates
C = {p, c1 , . . . , cm }, is susceptible to SCWM.
164

Junta Distributions

Intuitively, the instances of CWM (or SCWM) which are hard are those that require a
very specific partitioning of the voters in V1 to subsets, where each subset votes unanimously.
These instances are rare in any reasonable distribution; this insight will ultimately yield the
theorem.
The following proposition generalizes Theorem 1 of Conitzer and Sandholm (2002) and
Theorem 2 of Conitzer, Lang and Sandholm (2003), and justifies our focus on the family
of sensitive scoring protocols. A stronger version of Proposition 1 has been independently
proven by Hemaspaandra and Hemaspaandra (2005). Nevertheless, we include our proof,
since it will be required in proving the hardness property of a junta distribution we shall
design.
Proposition 1. Let P be a sensitive scoring protocol. Then CWM in P is N P-hard, even
with 3 candidates.
Definition 11. In the Partition problem, we are given a set of integers {ki }i∈[t] , summing
to 2K, and are asked whether a subset of these integers sum to K.
It is well-known that Partition is N P-complete.
Proof of Proposition 1. We reduce an arbitrary instance of Partition to the following
CWM instance. There are 3 candidates, a, b, and p. In V2 , there are K(4α1 − 2α2 ) − 1
voters voting a  b  p, and K(4α1 − 2α2 ) − 1 voters voting b  a  p. In V1 , for
every ki there is a vote of weight 2(α1 + α2 )ki . Observe that from V2 , both a and b get
(K(4α1 − 2α2 ) − 1)(α1 + α2 ) points.
Assume first that a partition exists. Let the voters in V1 in one half of the partition
vote p  a  b, and let the other half vote p  b  a. By this vote, a and b each have
(K(4α1 − 2α2 ) − 1)(α1 + α2 ) + 2K(α1 + α2 )α2 = (α1 + α2 )(4Kα1 − 1)
votes, while p has (α1 + α2 )4Kα1 points; thus there is a manipulation.
Conversely, assume that a manipulation exists. Clearly there must exist a manipulation
where all the voters in V1 vote either p  a  b or p  b  a, because the manipulators do
not gain anything by not placing p at the top in a scoring protocol. In this manipulation, p
has (α1 + α2 )4Kα1 points, while a and b already have (K(4α1 − 2α2 ) − 1)(α1 + α2 ) points
from V2 . Therefore, a and b must gain less than (2α2 K + 1)(α1 + α2 ) points from the voters
in V1 . Each voter corresponding to ki contributes 2(α1 + α2 )α2 ki points; it follows that
the sum of the ki corresponding to the voters voting p  a  b is less than K + 2α1 2 , and
likewise for the voters voting p  b  a. Equivalently, the sum can be at most K, since
all ki are integers and α2 ≥ 1. In both cases the sum must be at most K; hence, this is a
partition.
Since an instance of CWM can be translated into an instance of SCWM in the obvious
way, we have:
Corollary 1. Let P be a sensitive scoring protocol. It holds that SCWM in P is N P-hard,
even with 3 candidates.
165

Procaccia & Rosenschein

4.1 A Junta Distribution
Let w(v) denote the weight of voter v, and let W denote the total weight of the votes in
V1 ; P is a sensitive scoring protocol. We denote |V1 | = n: the size of V1 is the size of the
instance.
Consider a distribution µ∗ = {µ∗n }n∈N over the instances of SCWM in P , with m + 1
candidates p, c1 , . . . , cm , where each µ∗n is induced by the following sampling algorithm:
1. Fix a polynomial q = q(n).
2. ∀v ∈ T : Randomly and independently choose w(v) ∈ [0, 1] (up to O(n) bits of precision, i.e., in intervals of 1/2q(n) ).
3. ∀i ∈ {1, . . . , m}: Randomly and independently choose S[ci ] ∈ [(α1 − α2 )W, α1 W ] (up
to O(n) bits of precision).
Remark 4. Although the distribution is in fact discrete — the weights, for example, are
uniformly distributed in {0, 1/2q(n) , 2/2q(n) , 3/2q(n) , . . . , 1} — we treat it below as continuous
for the sake of clarity.
We assume that S[p] = 0, i.e., all voters in S rank p last. This assumption is not a
restriction. If it holds for a candidate c that S[c] ≤ S[p], then candidate c will surely lose,
since the manipulators all rank p first. Therefore, if S[p] > 0, we may simply normalize
the scores by subtracting S[p] from the scores of all candidates. This is equivalent to our
assumption.
Remark 5. We believe that µ∗ is the most natural distribution with respect to which
coalitional manipulation in scoring protocols should be studied. Even if one disagrees with
the exact definition of a junta distribution, µ∗ should satisfy many reasonable conditions
one could produce.
We shall, of course, (presently) prove that the distribution possesses the properties of a
junta distribution.
Proposition 2. Let P be a sensitive scoring protocol. Then µ∗ is a junta distribution for
SCWM in P with C = {p, c1 , . . . , cm }, and m = O(1).
Proof. We first observe that symmetry is obviously satisfied, and dichotomy holds by Remark 4.
The proof of the hardness property relies on the reduction from Partition in Proposition 1. The reduction generates instances x of CWM in P with 3 candidates, where
W = 4(α1 + α2 )K, and
S[a] = S[b]
= (K(4α1 − 2α2 ) − 1)(α1 + α2 )
= (α1 − α2 /2)W − (α1 + α2 ),
166

Junta Distributions

for some K that originates in the Partition instance. These instances satisfy (α1 −α2 )W ≤
S[a], S[b] ≤ α1 W . It follows that µ∗ (x) > 0 (after scaling down the weights).3
We now prove that µ∗ has the balance property. If for all i, S[ci ] > (α1 − α2 /m)W ,
then clearly there is no manipulation, since at least α2 W points are given by the voters in
V1 to the undesirable candidates c1 , . . . , cm . This happens with probability at least m1m .
On the other hand, consider the situation where for all i,
S[ci ] < (α1 −

m2 − 1
α2 )W ;
m2

(3)

this occurs with probability at least (m12 )m . Intuitively, if the manipulators could distribute
their votes in such a way that each undesirable candidate is ranked last in exactly 1/mfraction of the votes, this would be a successful manipulation: each undesirable candidate
would gain at most an additional m−1
m α2 W points. Unfortunately, this is usually not the
case, but the following condition is sufficient for a successful manipulation (assuming condition (3) holds). Partition the manipulators to m disjoint subsets P1 , . . . , Pm (w.l.o.g. of
size n/m), and denote by WPi the total weight of the votes in Pi . The condition is that for
all i ∈ {1, . . . , m}:
(1 − 1/m) · 1/2 · n/m ≤ WPi ≤ (1 + 1/m) · 1/2 · n/m.

(4)

This condition is sufficient, because if the voters in Pi all rank ci last, the fraction of the
votes in V1 which gives ci points is at most:
(m − 1)(1 + 1/m)
m2 − 1
= 2
.
(m − 1)(1 + 1/m) + 1 − 1/m
m +m−2
Hence the number of points ci gains from the manipulators is at most:
m2 − 1
m2 − 1
α
W
≤
α2 W < α1 W − S[ci ].
2
m2 + m − 2
m2
Furthermore, by Lemma 1 and the fact that the expected total weight of n/m votes is
2n
1/2 · n/m, the probability that condition (4) holds is at least 1 − 2e− m3 . Since m is a
constant, this probability is larger than 1/2 for a large enough n.
Finally, it can easily be seen that µ∗ has the refinement property: if all manipulators
rank p first and candidate c second, then p gets α1 W points, and c gets α2 W + S[c] points.
But S[c] ≥ (α1 − α2 )W , and thus p surely loses.
4.2 A Heuristic Polynomial Time Algorithm
We now present our algorithm Greedy for SCWM, given as Algorithm 1. w
~ denotes the
vector of the weights of voters in V1 = {v1 , . . . , vn }.
3. It seems the reduction can be generalized for a larger number of candidates. The hard instances are
the ones where all undesirable candidates but two have approximately (α1 − α2 )W initial points, and
two problematic candidates have approximately (α1 − αm /2)W points. These instances have a positive
probability under µ∗ .

167

Procaccia & Rosenschein

Algorithm 1 Decides SCWM
1: procedure Greedy(S, w,
~ p)
2:
for all c ∈ C do
3:
S0 [c] ← S[c]
4:
end for
5:
for i = 1 to n do
6:
Let j1 , j2 , . . . , jm s.t. ∀l, Si−1 [cjl−1 ] ≤ Si−1 [cjl ]
7:
Voter vi votes p  cj1  cj2  . . .  cjm
8:
for l = 1 to m do
9:
Si [cjl ] ← Si−1 [cjl ] + w(ti )αl+1
10:
end for
11:
Si [p] ← Si−1 [p] + w(ti )α1
12:
end for
13:
if argmaxc∈C Sn [c] = {p} then
14:
return true
15:
else
16:
return false
17:
end if
18: end procedure

. Initialization

. All voters in V1

. Update score

. p wins

The voters in V1 , according to some order, each rank p first, and the rest of the candidates by their current score: the candidate with the lowest current score is ranked highest.
Greedy accepts if and only if p wins this election.
This algorithm, designed specifically for scoring protocols, is a realization of an abstract
greedy algorithm: at each stage, voter vi ranks the undesirable candidates in an order that
minimizes the highest score that any undesirable candidate obtains after the current vote.
If there is a tie among several permutations, the voter chooses the option such that the
second highest score is as low as possible, etc. In any case, every manipulator always ranks
p first.
Remark 6. This abstract scheme might also be appropriate for protocols such as Maximin
and Copeland. Similarly to scoring protocols, in these two protocols the manipulators are
always better off by ranking p first. In addition, the abstract greedy algorithm can be
applied to Maximin and Copeland since the result of an election is based on the score each
candidate has (unlike STV, for example).
Remark 7. Greedy can be considered a generalization of the greedy algorithm given by
Bartholdi et al. (1989a).
In the following lemmas, a stage in the execution of the algorithm is an iteration of the
for loop.
Lemma 3. If there exists a stage i0 during the execution of Greedy, and two candidates
a, b 6= p, such that
|Si0 [a] − Si0 [b]| ≤ α2 ,
(5)
then for all i ≥ i0 it holds that |Si [a] − Si [b]| ≤ α2 .
168

Junta Distributions

Proof. The proof is by induction on i. The base of the induction is given by equation (5).
Assume that |Si [a] − Si [b]| ≤ α2 , and without loss of generality: Si [a] ≥ Si [b]. By the
algorithm, voter vi+1 ranks b higher than a, and therefore:
Si+1 [b] − Si+1 [a] ≥ −α2 .

(6)

Since p is always ranked first, and the weight of each vote is at most 1, b gains at most α2
points. Therefore:
Si+1 [b] − Si+1 [a] ≤ α2 .
(7)
Combining equations (6) and (7) completes the proof.
Lemma 4. Let p 6= a, b ∈ C, and suppose that there exists a stage i0 such that Si0 [a] ≥
Si0 [b], and a stage i1 ≥ i0 such that Si1 [b] ≥ Si1 [a]. Then for all i ≥ i1 it holds that
|Si [a] − Si [b]| ≤ α2 .
Proof. Assume that there exists a stage i0 such that Si0 [a] ≥ Si0 [b], and a stage i1 ≥ i0
such that Si1 [b] ≥ Si1 [a]; w.l.o.g. i1 > i0 (otherwise at stage i0 it holds that Si0 [b] = Si0 [a],
and then we finish by Lemma 3). Then there must be a stage i2 such that i0 ≤ i2 < i1 and
Si2 [a] ≥ Si2 [b] but Si2 +1 [b] ≥ Si2 +1 [a]. Since the weight of each vote is at most 1, b gains at
most α2 points by voter vi2 +1 . Hence the conditions of Lemma 3 hold for stage i2 , which
implies that for all i ≥ i2 : |Si [a] − Si [b]| ≤ α2 . In particular, i1 ≥ i2 .
Lemma 5. Let P be a sensitive scoring protocol, and assume Greedy errs on an instance
of SCWM in P which has a successful manipulation. Then there is d ∈ {2, 3, . . . , m}, and
a subset of candidates D = {cj1 , . . . , cjd }, such that:
d
X
i=1

(α1 W − S[cji ]) −

d−1
X

(i · α2 ) ≤ W

i=1

d
X
i=1

αm+2−i ≤

d
X

(α1 W − S[cji ]).

(8)

i=1

Proof. For the right inequality, for any d candidates, even ifP
all voters in V1 rank them last
in every vote, the total points distributed among them is W di=1 αm+2−i . If this inequality
does not hold, there must be some candidate ci that gains at least α1 W − S[ci ] points
from the manipulators, implying that this candidate has at least α1 W points. However, p
also has at most α1 W points, and we assumed that there is a successful manipulation — a
contradiction.
For the left inequality, assume the algorithm erred. Then at some stage i0 , there is a
candidate cj0 who has a total of at least α1 W points (w.l.o.g. only one candidate passes this
threshold simultaneously). Denote V10 = {v1 , v2 , . . . , vi0 }, and let WV10 be the total weight
of the voters in V10 . Voter vi0 did not rank cj0 last, since αm+1 = 0, and thus ranking a
candidate last gives it no points. We have that there is another candidate cj1 , such that:
Si0 −1 [cj1 ] ≥ Si0 −1 [cj0 ]. By Lemma 4, Si0 [cj0 ] − Si0 [cj1 ] ≤ α2 , and thus Si0 [cj1 ] ≥ α1 W − α2 .
If these candidates were not always ranked last by the voters of V10 , there must be another
candidate cj2 who was ranked strictly higher by some voter in V10 , w.l.o.g. higher than cj1 .
Therefore, we have from Lemma 4 that: Si0 [cj1 ] − Si0 [cj2 ] ≤ α2 , and so cj2 has a total of at
least α1 W − 2α2 points. By inductively continuing this reasoning, we obtain a subset D of
d candidates (possibly d = m), who were always ranked in the d last places by the voters
169

Procaccia & Rosenschein

in V10 , and for the l’th candidate it holds that: Si0 [cjl ] ≥ α1 W − (l − 1)α2 . The total points
gained by this l’th candidate until stage i0 must be at least α1 W − (l − 1)α2 − S[cjl ]. Since
P
the total points distributed by the voters in V10 to the d last candidates is WV10 di=1 αm+2−i ,
we have:
d
X

(α1 W − S[cji ]) −

i=1

d−1
X

(i · α2 ) ≤ WV10

d
X

i=1

αm+2−i ≤ W

i=1

d
X

αm+2−i .

i=1

Lemma 6. Let M be SCWM in a sensitive scoring protocol P with C = {p, c1 , . . . , cm },
m=O(1). Then Greedy is a deterministic heuristic polynomial time algorithm for hM, µ∗ i.
Proof. It is obvious that if the given instance has no successful manipulation, then the
greedy algorithm would indeed answer that there is no manipulation, since the algorithm
is constructive (it actually selects specific votes for the manipulators).
We wish to bound the probability that there is a manipulation and the algorithm erred.
By Lemma 5, a necessary condition for this to occur is as specified in equation (8), or
equivalently:
W

d
X

α1 − W

i=1

d
X
i=1

d
d
d
X
X
X
d(d − 1)
αm+2−i −
α2 ≤
S[cji ] ≤ W
α1 − W
αm+2−i .
2
i=1

i=1

(9)

i=1

In this case the algorithm may err; but
Pdwhat is the probability of equation (9) holding? Fix
a subset D of size d ∈ {2, . . . , m}.
i=1 S[cji ] is a random variable that takes values in
[d(α1 − α2 )W, dα1 W ]. By conditioning on the values of S[cji ], i = 1, . . . , d − 1, we have that
P
the probability of di=1 S[cji ] taking values in some interval [a, b] is at most the chance of
b−a
S[cjd ] taking a value in an interval of size b − a, which is at most α1 W −(α
, since S[cjd ]
1 −α2 )W
n

is uniformly distributed. By Lemma 1, W < n/4 with probability at most (n) = e− 8 . On
the other hand, if W ≥ n/4, then (9) holds for D with probability at most
d(d−1)
α2
2

α1 W − (α1 − α2 )W

=

d(d − 1)
2d(d − 1)
1
≤
= D
,
2W
n
p (n)

for some polynomial pD . We complete the proof by showing that equation (1) holds:
Pr [Greedy(x) 6= M (x)] ≤ Pr[W ≥ n/4 ∧ (∃D ⊂ C s.t. |D| ≥ 2 ∧ (9) holds)]

x∼µ∗n

≤

+ Pr[W < n/4]
X
1
D⊂C:|D|≥2

≤

pD (n)

+ (n)

1
poly n

The last inequality follows from the assumption that m = O(1).
Clearly, Theorem 1 directly follows.
170

Junta Distributions

4.3 Algorithm 1 and the Uniform Distribution
In the previous subsection we have seen that Algorithm 1 is a heuristic polynomial time
algorithm with respect to our junta distribution µ∗ . We have argued that this suggests
that the algorithm also does well with respect to other distributions. In this subsection
we support this claim by showing that Algorithm 1 is also a heuristic polynomial time
algorithm with respect to the uniform distribution over instances of SCWM.
For the sake of consistency with previous results, we shall consider a uniform distribution
over votes which may produce unfeasible ballots. Nevertheless, equivalent results can be
obtained for feasible (discrete) distributions over votes. If so, in this subsection we assume
each voter vi ∈ V2 , where |V2 | = N , awards each candidate c ∈ C, including p, a score
independently and uniformly distributed in [0, α1 ]. Further, we assume that the votes are
unweighted; this does not limit the generality of our results, since we use lower bounds
that depend only on the total weight of the manipulators in V1 (where |V1 | = n) — the
individual weights are of no consequence.
We distinguish between two cases in our results, depending on the ratio between the
number of nonmanipulators N and the number of manipulators n:
√
1. n/ N < 1/p(n) for some polynomial p of degree at least 1.
√
2. n/ N > p(log n) for some polynomial p of degree at least 1.
The middle ground which is not covered by the two cases remains an open problem.
Before we tackle the first case, we require a lower bound of sorts on the probability that
an instance of SCWM is very easy to decide. Since the manipulators in V1 can award a
candidate at most α1 n points, the manipulators cannot make a candidate c beat another
candidate c0 if S[c0 ] − S[c] > α1 n. In particular, if for every two candidates c and c0 it holds
that |S[c] − S[c0 ]| > α1 n, then the manipulators cannot affect the outcome of the election.
Moreover, Algorithm 1 always decides such an instance correctly: if S[p] < S[c] for some
c, then the instance is a “no” instance, and in this case the algorithm never errs; and if
S[p] > S[c] for all c, then the instance is a “yes” instance, and any vote of the manipulators
is sufficient to make p win. We have obtained the following Lemma:
Lemma 7. Consider an instance of SCWM where for all c, c0 ∈ C, |S[c] − S[c0 ]| > α1 n.
Then the instance is a “yes” instance iff S[p] > S[c] for all candidates c 6= p, and the
instance is correctly decided by Algorithm 1.
This Lemma, together with the Central Limit Theorem, yields the first result.
Proposition 3. Algorithm 1 is a heuristic polynomial time algorithm
with respect to the
√
uniform distribution over instances of SCWM which satisfy n/ N < 1/p(n) for some
polynomial p(n) of degree at least 1.
Proof. By Lemma 7, it is sufficient to bound from below the probability that for all c, c0 ∈ C,
|S[c] − S[c0 ]| > α1 N .
Pr[∀c, c0 ∈ C, |S[c] − S[c0 ]| > α1 N ] = 1 − Pr[∃c, c0 ∈ C s.t. 0 ≤ S[c] − S[c0 ] ≤ α1 N ].
171

Procaccia & Rosenschein

By the union bound:
Pr[∃c, c0 ∈ C s.t. 0 ≤ S[c] − S[c0 ] ≤ α1 n] ≤

X

Pr[0 ≤ S[c] − S[c0 ] ≤ α1 n].

c,c0 ∈C

Fix c, c0 ∈ C, and let Xi be si [c] − si [c0 ], where si is the score given to a candidate by
voter vi ∈ V2 , i = n + 1, . . . , n + N . The Xi are i.i.d. continuous random variables with
expectation 0 and constant variance σ 2 = 2 · (α1 )2 /12 = (α1 )2 /6. Therefore, we can apply
Lemma 2:
"
#
n+N
X


Pr 0 ≤ S[c] − S[c0 ] ≤ α1 n = Pr 0 ≤
Xi ≤ α1 n
"
= Pr 0 ≤

i=n+1
Pn+N
i
i=n+1 X

√

Z
1
−→ √
2π 0
Z √α1 n
Nσ
≤
1 dx

N →∞

Nσ

α n
√1
Nσ

e−

x2
2

α1 n
≤√
Nσ

#

dx

0

α1 n
=√
Nσ


n
=O √
.
N
By our assumption regarding the ratio of manipulators and nonmanipulators, this is
inverse-polynomial in n. Rolling back, we obtain that the probability that the algorithm is
1
correct is at least 1 − (m + 1)m p(n)
, and the result follows from the fact that m = O(1).
Moving on to the second case, we require the following lemma:
α2
Lemma 8. Let  = 2(m+1)
, and consider an instance of SCWM where for all c, c0 ∈ C,
0
|S[c] − S[c ]| < n. Then this instance is a “yes” instance, and is correctly decided by
Algorithm 1.

Proof. Obviously, it is sufficient to prove that the algorithm constructively finds a successful
vote that makes p win. Let C 0 ⊆ C \ {p} be the set of undesirable candidates that had
maximal score among the candidates in C \ {p} at some stage during the execution of the
algorithm. By the algorithm, at any stage some candidate from C 0 is ranked last by a
voter in V1 , i.e., is given 0 points; the other candidates in C 0 receive at any stage at most
α2 points. Therefore, the total number of points the candidates in C 0 receive from the
manipulators is at most (d − 1)α2 n, where |C 0 | = d. Consequently, if S ∗ [c] is the score of
candidate c when the algorithm terminates,
X
X
S ∗ [c] ≤
S[c] + (d − 1)α2 n.
c∈C 0

c∈C 0

172

Junta Distributions

Let c∗0 = argmaxc∈C 0 S[c], and c∗1 = argmaxc∈C 0 S ∗ [c]. By Lemma 4, when the algorithm
terminates it holds that the scores of all candidates in C 0 are within α2 of one another.
Therefore:
X
X
S ∗ [c∗1 ] ≤
S[c] + (d − 1)α2 n −
S ∗ [c] ≤ dS ∗ [c∗0 ] + (d − 1)α2 n − (d − 1)(S ∗ [c∗1 ] − α2 ).
c∗1 6=c∈C 0

c∈C 0

Through some algebraic manipulations, we obtain:




d−1
m
d−1
m
S ∗ [c∗1 ] ≤ S[c∗0 ] + n
α2 +
α2 ≤ S[c∗0 ] + n
α2 +
α2 .
d
d
m+1
m+1
Now, we have that:
∗

S [p] − S

∗

[c∗1 ]



m
m
α2 n +
α2
≥ (S[p] + α1 n) −
+
m+1
m+1


α2
m
m
≥ α1 n −
n−
α2 n −
α2
2(m + 1)
m+1
m+1
m
α2
n−
α2
≥
2(m + 1)
m+1
> 0.


S[c∗0 ]



The second transition follows from the assumption that S[p] ≥ S[c∗0 ]−n, the third transition
from the fact that α1 ≥ α2 , and the last transition holds for a large enough n.
Proposition 4. Algorithm 1 is a heuristic polynomial time algorithm
with respect to the
√
uniform distribution over instances of SCWM which satisfy n/ N > p(log n) for some
polynomial p of degree at least 1.
Proof. Let  =
least:

α2
2(m+1) .

By Lemma 8, the probability that the algorithm does not err is at

Pr[∀c, c0 ∈ C, |S[c] − S[c0 ]| < n] = 1 − Pr[∃c, c0 ∈ C s.t. S[c] − S[c0 ] > n].
By the union bound:
Pr[∃c, c0 ∈ C s.t. S[c] − S[c0 ] > n] ≤

X

Pr[S[c] − S[c0 ] > n].

c,c0 ∈C

As before, fix c, c0 ∈ C, and let Xi be si [c] − si [c0 ], where si is the score given to a
candidate by voter vi . The Xi are i.i.d. random variables with expectation 0, which take
values in [−α1 , α1 ]. Applying Lemma 1 to these variables, we obtain:
2

n+N
( n )
n
1 X
0 n2
−2N N 2
0
(2α1 )
Xi ≥ E[Xi ] + ] ≤ e
= e− N ,
Pr[S[c] − S[c ] ≥ n] = Pr[
N
N
i=n+1

where 0 is some constant. The result follows from the fact that m is constant and our
assumption regarding the relation between n and N .
173

Procaccia & Rosenschein

5. The Case of Uncertainty about Votes
So far we have dealt with a setting where an entire coalition of manipulators is trying to
influence the outcome of the election, using complete knowledge of the nonmanipulators’
votes. This section is a short aside, in which we discuss a setting where there is a single
manipulator with uncertainty about others’ votes. We shall prove:
Theorem 2. Let P be a voting protocol such that there exists a junta distribution µP over
the instances of UVWM in P , with the following property: r is uniformly distributed in
[0, 1]. Then P , with candidates C = {p, c1 , . . . , cm }, m = O(1), is susceptible to UVWM.
Recall that in UVWM, we ask whether the manipulator can cast his vote so that p
wins with probability greater than r. The existence of a junta distribution with r uniformly distributed is a very weak requirement (it is even quite natural to have r uniformly
distributed). In fact, the following claim is very likely to be true:
Conjecture 1. Let P be a voting protocol for which UVWM is N P-hard. Then there exists
a junta distribution µP over the instances of UVWM in P , with r uniformly distributed in
[0, 1].
If this conjecture is indeed true, we have that all voting protocols are susceptible to
UVWM. If for some reason the conjecture is not true with respect to our definition of junta
distributions, then perhaps the definition is too restrictive and should be modified accordingly. We also remark that similar results can be derived for destructive manipulations by
analogous proofs.
To prove Theorem 2, we first present a helpful procedure, which decides UVWE. w
~
denotes the vector of given weights, and ν is the given distribution over all the votes. The
number of voters is |V | = n.
Sample(C = {p, c1 , . . . , cm }, w,
~ ν, r)
1: count = 0
2: for i = 1 to n3 do
3:
Sample the distribution ν over the votes
4:
Calculate the result of the election using the sampled votes
5:
if p won then
6:
count = count + 1
7:
end if
8: end for
9: if count/n3 > r then
10:
return 1
11: else
12:
return 0
13: end if
Sample samples the given distribution on the votes n3 times, and calculates the winner
of the election each time. If p won more than an r-fraction of the elections then the procedure
accepts, otherwise it rejects.
174

Junta Distributions

Lemma 9. Let P be a voting protocol, and E be UVWE in P with C = {p, c1 , . . . , cm }.
Furthermore, let µ be a distribution over the instances of E, with r uniformly distributed in
[0, 1]. Then there exists N such that for all n ≥ N :
Pr [Sample(x) 6= E(x)] ≤

x∼µn

1
.
polyn

3

Proof. Let {Xi }ni=1 be random variables, such that Xi = 1 if p won in the i’th iteration
of the for loop, and Xi = 0 otherwise. Let r0 be the probability that p wins in the given
instance. By Lemma 1 and the union bound:



3
 X

1 n

3 1
1
Pr  3
Xi − r0  ≥  ≤ 2e−2n n2 = 2e−2n .
n
 n
i=1

We deduce that if |r − r0 | > n1 , Sample will fail with an exponentially small probability.
By the assumption that r is uniformly distributed, the probability that |r − r0 | ≤ n1 is at
most 2/n. Thus, by the union bound it holds that:




1
1
0
0
Pr [Sample(x) 6= E(x)] ≤ Pr |r − r | ≤
+ Pr |r − r | > ∧ Sample(x) 6= E(x)
x∼µn
n
n
≤ 2/n + 2e−2n
1
.
≤
polyn

We now present an algorithm that decides UVWM. Here, w
~ denotes the weights of all
voters including the manipulator, and ν is the given distribution over the nonmanipulators’
votes.
Sample-and-manipulate(C = {p, c1 , . . . , cm }, w,
~ ν, r)
1: ans = 0
2: for i = 1 to (m + 1)! do
3:
π = next permutation of the m + 1 candidates
4:
ν ∗ = the manipulator always votes π, others’ votes are distributed with respect to ν
5:
if Sample(C, w,
~ ν ∗ , r) = 1 then
6:
ans = 1
7:
end if
8: end for
9: return ans
Given an instance of UVWM, Sample-and-Manipulate generates (m+1)! instances of
the UVWE problem, one for each of the manipulator’s possible votes, and executes Sample
on each instance. Sample-and-Manipulate accepts if and only if Sample accepts one of
the instances.
175

Procaccia & Rosenschein

Lemma 10. Let P be a voting protocol, and M be UVWM in P with C = {p, c1 , . . . , cm },
m = O(1). Furthermore, let µ be a distribution over the instances of UVWM, with r
uniformly distributed in [0, 1]. Then Sample-and-Manipulate is a probabilistic heuristic
polynomial time algorithm for hM, µi.
Proof. For each independent call to Sample, the chance of failure is inverse-polynomial.
By applying the union bound we have that the probability of Sample failing on any of the
1
(m + 1)! invocations is at most (m + 1)! polyn
, which is still inverse-polynomial since m is
constant. The lemma now follows from the fact that there is a manipulation if and only if
there is a permutation of candidates, such that if the manipulator votes according to this
permutation, the chance of p winning is greater than r.
Notice that Sample-and-Manipulate is indeed polynomial by the fact that m = O(1),
and we assumed that the given distribution over the votes can be sampled in polynomial
time.

6. Related Work
Computational aspects of voting have long been investigated. A pivotal issue is the problem
of winner-determination: voting protocols designed to satisfy theoretical desiderata may be
quite complex. Consequently, deciding who won an election governed by such protocols may
be a computationally hard problem (Bartholdi, Tovey, & Trick, 1989b). Another concern
is strategic behavior on the part of the officials conducting the election, who may add or
remove voters and candidates from the slate. The computational complexity of strategically
controlling an election has been analyzed by Bartholdi, Tovey and Trick (1992).
That said, the main issue with respect to strategic behavior in voting has always been
manipulation by voters. There is a growing body of work which deals with the worst-case
complexity of manipulating elections. A seminal paper is that of Bartholdi, Tovey and
Trick (1989a); the authors suggested, for the first time, that computational complexity
is an obstacle that strategic voters must overcome. Indeed, although it is shown that
many voting protocols can be efficiently manipulated, it is nevertheless proven that there
is a voting protocol, namely second-order Copeland, which is N P-hard to manipulate.
Bartholdi and Orlin (1991) have demonstrated that the prominent Single Transferable Vote
(STV) protocol is N P-hard to manipulate.
Even in voting protocols that are easy to manipulate, difficulty can be artificially introduced by adding a preround (Conitzer & Sandholm, 2003); the candidates are paired, and
in each pairing of two candidates, the candidate that loses the pairwise election between the
two is eliminated. Plurality, Borda and Maximin have been shown to be hard to manipulate
when augmented with a preround. In more detail, these protocols are N P-hard to manipulate when the scheduling of the preround precedes voting, #P-hard when voting precedes
scheduling, and PSPACE-hard when voting and scheduling are interleaved. Elkind and
Lipmaa (2005a) induce hardness of manipulation using a more general approach. Hybrid
voting protocols that are hard to manipulate are constructed by composing several base
protocols; the base protocols may be individually easy to manipulate.
Another case where manipulation may be hard, in the worst-case, is when the election
has multiple winners instead of a single winner, as is the case in elections to a parliament
176

Junta Distributions

or an assembly. Procaccia, Rosenschein, and Zohar (2007) demonstrate that manipulation
in Cumulative voting, a major protocol for multi-winner elections, is N P-hard.
The coalitional manipulation problem, which has been the focus of this paper, has first
been investigated by Conitzer and Sandholm (2002, 2003). In this setting, the computational
problem is made more difficult by the fact that numerous manipulators must coordinate
their strategy (and additionally, by the introduction of weighted voting). While the hardness
results in the abovementioned papers relied on the number of candidates being unbounded,
Conitzer and Sandholm present hardness results in the coalitional manipulation setting with
a constant number of candidates, with respect to several central voting protocols.
Elkind and Lipmaa (2005b) extend the preround approach presented by Conitzer and
Sandholm (2003) to the coalitional manipulation setting. In this context, they provide
an early impossibility result regarding the average-case complexity of manipulations: the
authors present a family of preference profiles in which a manipulator can always improve
the outcome by voting strategically, regardless of the preround schedule. This result applies
only when seeking to make manipulation hard by adding a preround. Further, one would
usually not expect distributions over the instances of the coalitional manipulation problem
to give this family of preference profiles significant probability, as it is extremely restricted.
A recent result regarding average-case complexity of manipulation, which complements
our own, was presented by Conitzer and Sandholm (2006). The authors put forward two
properties of instances of the coalitional manipulation problem, and demonstrate that any
instance that satisfies both properties is easy to manipulate. The first property is that the
instance satisfy a weaker form of monotonicity — this property seems very natural; the
second property is that manipulators be able to make one of exactly two candidates win
the election — and this property is much harder to accept. In order to justify the second
property, the authors show that in many voting protocols the property usually holds, but
only with respect to a specific family of distributions.
This result has two main shortcomings compared to ours. First, the arguments in favor
of the second property mentioned above are empirical rather than analytical; second, the
family of distributions considered is not “special” in any sense — which is not the case
here. In other words, the family of distributions in question is a priori not especially hard
to manipulate. On the other hand, their result has some advantages: unlike ours, it does
not depend on the number of candidates being constant (although in all experiments the
number of candidates and manipulators is extremely small compared to the number of
voters), and (arguably) does not require significant restrictions on the voting rule.

7. Conclusions
To date, all results on the complexity of manipulation only considered the worst case. Although better than nothing, such results are a weak guarantee of resistance to manipulation.
A truly worthy goal is to design a voting protocol that is hard to manipulate in the average
case while being plausible from a social choice point of view, but so far all attempts have
failed.
Motivated by this, we have presented a specific manipulation setting that is worst-case
hard but average-case tractable. We have first prepared the ground for our average-case
analysis by borrowing several concepts from the existing theory and introducing several
177

Procaccia & Rosenschein

new ideas. The key to our approach is junta distributions, which presumably concentrate
on hard instances of the coalitional manipulation problem. We have considered a voting
protocol to be susceptible to coalitional manipulation if there is an algorithm that almost
always correctly decides the problem, when the instances are distributed with respect to a
junta distribution.
Our main result states that sensitive scoring protocols are susceptible to coalitional manipulation when the number of candidates is constant, although they are hard to manipulate
even when the number of candidates is constant.
7.1 Discussion
Our results, first and foremost, suggest that worst-case hardness is indeed not a strong
enough barrier against manipulation. This motivates further research regarding averagecase complexity of manipulations, at the expense of future investigations into worst-case
complexity.
Moreover, in our view, our main result provides further evidence that a voting rule
that is average-case hard to manipulate does not exist. At the very least, it suggests that
scoring protocols cannot form the basis of a protocol which is usually strategy resistant.
Nevertheless, this negative result can be circumvented in many ways.
First, it can be circumvented via the voting protocol. Scoring protocols are among
the easiest voting systems to manipulate, as their structure is quite simple and they can be
concisely represented. Other protocols, say STV, are inherently harder to deal with. In fact,
recall that STV is worst-case hard to manipulate when there is only one manipulator (but
an unbounded number of candidates) (Bartholdi & Orlin, 1991), whereas scoring protocols
are most certainly not.
Second, it can be circumvented via the setting. Our results hold only when one contemplates coalitional manipulation with a constant number of candidates. A constant number
of candidates is known to guarantee worst-case hardness, but it may be the case that allowing for a large number of candidates would make the difference with respect to average-case
analysis.
Third, it can be circumvented via the distribution. Traditional average-case complexity
theory deals with hardness of distributional problems; in other words, a specific distribution
is considered. Junta distributions were chosen in a way that if one of them can usually be
manipulated by an algorithm, presumably the same algorithm would be successful with
other distributions. This view was supported by the results in Section 4.3, but at this point
there are no strong theoretical guarantees, and it may certainly be true that there is a
specific distribution over the instances of the manipulation problem which is average-case
hard to manipulate, even when a scoring protocol is considered.
Section 4.3 deserves an aside. The lemmas established there show that, with respect
to the uniform distribution, even a completely trivial algorithm can usually decide the
coalitional manipulation problem: if the number of manipulators is small (less than the
square root of the number of voters), the manipulators can rarely influence the outcome
of the election; therefore, if p was elected by the nonmanipulators as well, it is usually
correct to answer “yes”, and if not, it is usually correct to answer “no”. If the number
of manipulators is large, it is usually correct to answer “yes” — there is a manipulation.
178

Junta Distributions

Recent preliminary results in this direction imply that this is true for several families of
voting rules, and under a large variety of distributions. It is very important to note that
such a simple algorithm would not work well with respect to the junta distribution µ∗ .
7.2 Future Research
In our view, a central contribution of the paper is that it establishes a framework that
can be used to study the average-case complexity of manipulations in other protocols, and
even more generally, in other mechanisms. Indeed, voting is the most general method of
preference aggregation, but the same issues are also relevant when one considers mechanisms
in more specific settings. One such mechanism of which we are aware, whose manipulation is
N P-hard, is presented by Bachrach and Rosenschein (2006). All the definitions in Section 3
are sufficiently general to deal with different mechanisms for preference aggregation.
There is still room for debate as to the exact definition of a junta distribution. It may
also be the case that there are “unconvincing” distributions that satisfy all of the (current)
conditions of a junta distribution. It might prove especially fruitful to show that a heuristic
polynomial time algorithm with respect to a junta distribution is guaranteed to have the
same property with respect to some easy distributions, such as the uniform distribution.
An issue of great importance is coming up with natural criteria to decide when a manipulation problem is hard in the average-case. The traditional definition of average-case
completeness is very difficult to work with in general; is there a satisfying definition that
applies specifically to the case of manipulations? Once the subject is more fully understood, this understanding will surely shed more light on the great mystery: are there voting
protocols that are usually hard to manipulate?

Acknowledgments
This work was partially supported by grant #898/05 from the Israel Science Foundation.

References
Alon, N., & Spencer, J. H. (1992). The Probabilistic Method. Wiley and Sons.
Bachrach, Y., & Rosenschein, J. S. (2006). Achieving allocatively-efficient and strongly
budget-balanced mechanisms in the network flow domain for bounded-rational agents.
In The Seventh International Workshop on Agent-Mediated Electronic Commerce:
Designing Mechanisms and Systems, Utrecht, The Netherlands (AMEC 2005), No.
3937 in Lecture Notes in Artificial Intelligence, pp. 71–84. Springer-Verlag, Berlin.
Bartholdi, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social
Choice and Welfare, 8, 341–354.
Bartholdi, J., Tovey, C. A., & Trick, M. A. (1989a). The computational difficulty of manipulating an election. Social Choice and Welfare, 6, 227–241.
Bartholdi, J., Tovey, C. A., & Trick, M. A. (1989b). Voting schemes for which it can be
difficult to tell who won the election. Social Choice and Welfare, 6, 157–165.
179

Procaccia & Rosenschein

Bartholdi, J., Tovey, C. A., & Trick, M. A. (1992). How hard is it to control an election.
Mathematical and Computer Modelling, 16, 27–40.
Conitzer, V., Lang, J., & Sandholm, T. (2003). How many candidates are needed to make
elections hard to manipulate?. In Proceedings of the International Conference on
Theoretical Aspects of Reasoning about Knowledge, pp. 201–214.
Conitzer, V., & Sandholm, T. (2002). Complexity of manipulating elections with few candidates. In Proceedings of the National Conference on Artificial Intelligence, pp. 314–
319.
Conitzer, V., & Sandholm, T. (2003). Universal voting protocol tweaks to make manipulation hard. In Proceedings of the International Joint Conference on Artificial Intelligence, pp. 781–788.
Conitzer, V., & Sandholm, T. (2006). Nonexistence of voting rules that are usually hard
to manipulate. In Proceedings of the Twenty-First National Conference on Artificial
Intelligence, pp. 627–634.
Elkind, E., & Lipmaa, H. (2005a). Hybrid voting protocols and hardness of manipulation.
In 16th Annual International Symposium on Algorithms and Computation, Lecture
Notes in Computer Science, pp. 206–215. Springer-Verlag.
Elkind, E., & Lipmaa, H. (2005b). Small coalitions cannot manipulate voting. In International Conference on Financial Cryptography, Lecture Notes in Computer Science.
Springer-Verlag.
Ephrati, E., & Rosenschein, J. S. (1997). A heuristic technique for multiagent planning.
Annals of Mathematics and Artificial Intelligence, 20, 13–67.
Feller, W. (1968). Introduction to Probability Theory and its Applications (3rd edition).,
Vol. 1, p. 254. John Wiley.
Ghosh, S., Mundhe, M., Hernandez, K., & Sen, S. (1999). Voting for movies: the anatomy
of a recommender system. In Proceedings of the Third Annual Conference on Autonomous Agents, pp. 434–435.
Gibbard, A. (1973). Manipulation of voting schemes. Econometrica, 41, 587–602.
Haynes, T., Sen, S., Arora, N., & Nadella, R. (1997). An automated meeting scheduling system that utilizes user preferences. In Proceedings of the First International Conference
on Autonomous Agents, pp. 308–315.
Hemaspaandra, E., & Hemaspaandra, L. A. (2005). Dichotomy for voting systems. University of Rochester Department of Computer Science Technical Report 861.
Procaccia, A. D., Rosenschein, J. S., & Zohar, A. (2007). Multi-winner elections: Complexity
of manipulation, control and winner-determination. In The Twentieth International
Joint Conference on Artificial Intelligence (IJCAI 2007), Hyderabad, India. To appear.
Satterthwaite, M. (1975). Strategy-proofness and Arrow’s conditions: Existence and correspondence theorems for voting procedures and social welfare functions. Journal of
Economic Theory, 10, 187–217.
180

Junta Distributions

Trevisan, L. (2002). Lecture notes on computational complexity. Available from
http://www.cs.berkeley.edu/˜luca/notes/complexitynotes02.pdf. Lecture 12.

181

Journal of Artificial Intelligence Research 28 (2007) 393-429

Submitted 6/06; published 3/07

Bin Completion Algorithms for Multicontainer Packing,
Knapsack, and Covering Problems
Alex S. Fukunaga

fukunaga@aig.jpl.nasa.gov

Jet Propulsion Laboratory
California Institute of Technology
4800 Oak Grove Drive
Pasadena, CA 91108 USA

Richard E. Korf

korf@cs.ucla.edu

Computer Science Department
University of California, Los Angeles
Los Angeles, CA 90095

Abstract
Many combinatorial optimization problems such as the bin packing and multiple knapsack problems involve assigning a set of discrete objects to multiple containers. These problems can be used to model task and resource allocation problems in multi-agent systems
and distributed systms, and can also be found as subproblems of scheduling problems. We
propose bin completion, a branch-and-bound strategy for one-dimensional, multicontainer
packing problems. Bin completion combines a bin-oriented search space with a powerful
dominance criterion that enables us to prune much of the space. The performance of the
basic bin completion framework can be enhanced by using a number of extensions, including nogood-based pruning techniques that allow further exploitation of the dominance
criterion. Bin completion is applied to four problems: multiple knapsack, bin covering,
min-cost covering, and bin packing. We show that our bin completion algorithms yield
new, state-of-the-art results for the multiple knapsack, bin covering, and min-cost covering problems, outperforming previous algorithms by several orders of magnitude with
respect to runtime on some classes of hard, random problem instances. For the bin packing problem, we demonstrate significant improvements compared to most previous results,
but show that bin completion is not competitive with current state-of-the-art cutting-stock
based approaches.

1. Introduction
Many NP-hard problems involve assigning some set of discrete objects to multiple containers. In one class of problems, the objective is to pack some items into a set of containers
without exceeding the containers’ capacities. In a related class of problems, the goal is to
cover a set of containers by filling them up to at least some minimal level (quota) using a set
of items. When both the containers and the items are modeled as one-dimensional objects
(possibly with an associated cost/value function), we refer collectively to these problems as
one-dimensional, multicontainer packing and covering problems, or simply multicontainer
packing problems.
One example of a multicontainer packing problem is the bin packing problem: Given a
set of items (numbers), and a fixed bin capacity, assign each item to a bin so that the sum
of the items assigned to each bin does not exceed the bin capacity, and the number of bins
c
2007
AI Access Foundation. All rights reserved.

Fukunaga & Korf

used is minimized. For example, given the set of items 6, 12, 15, 40, 43, 82, and a bin
capacity of 100, we can assign 6, 12, and 82 to one bin, and 15, 40, and 43, to another, for
a total of two bins. This is an optimal solution to this instance, since the sum of all the
items, 198, is greater than 100, and hence at least two bins are required.
Multicontainer packing problems are ubiquitous. They model many important operations research problems such as cargo loading and transport, and also model many artificial
intelligence applications, such as the allocation and rationing of resources or tasks among
a group of agents. Multicontainer packing problems can often be found embedded as subproblems of more complex, real-world combinatorial optimization problems. For example
many constraint programming problems contain “bin packing” or “knapsack” constraints
as subproblems (e.g., Shaw, 2004). Such constraints are at the core of many scheduling and
resource allocation problems.
In this paper, we propose bin completion, a new algorithm for optimally solving multicontainer packing problems. We begin in Section 1.1 with an overview of four representative, strongly NP-complete, multicontainer problems: (1) the bin packing problem, (2)
the multiple knapsack problem, (3) the bin covering problem, and (4) the min-cost covering
problem.
In Section 2, we begin by describing the standard, “item-oriented” branch-and-bound
framework for these problems. In this traditional approach, items are considered one at a
time. Each node in the search corresponds to a decision regarding the assignment of an
item to some non-full container. Then, we describe bin completion, an alternative, “binoriented” branch-and-bound strategy with two key features: (1) the nodes in the search tree
correspond to complete assignments of items to a single bin, and (2) dominance criteria
between assignments of items to bins are used to prune the search. Section 3 describes
extensions to the basic bin completion framework that improve search efficiency, as well as
the runtime and memory usage at each node.
In Sections 4-7, we explore the application of the bin completion framework to four
specific, one-dimensional multicontainer packing problems. For each problem, we review
the previous work on algorithms that optimally solve the problem, detail our bin completion
algorithm for the problem, and provide an empirical comparison with the previous state-ofthe-art algorithm. We apply bin completion to the multiple knapsack problem in Section
4, and show that our bin completion solver significantly outperforms Mulknap (Pisinger,
1999), the previous state-of-the-art algorithm. The min-cost covering problem (also called
the liquid loading problem) was the first problem for which Christofides, Mingozzi, and
Toth (1979) proposed an early variant of the bin completion approach. In Section 5, we
show that our new bin completion algorithm significantly outperforms the earlier algorithm
by Christofides et al. In Section 6, we apply bin completion to the bin covering problem
(also known as the dual bin packing problem). We show that our bin completion algorithm
significantly outperforms the previous state-of-the-art algorithm by Labbé, Laporte, and
Martello (1995). In Section 7, we apply our extended bin completion algorithm to the
bin packing problem. Although our initial results were promising (Korf, 2002, 2003), our
best bin completion solver is not competitive with the current state of the art, which is a
recent branch-and-price approach based on a cutting-stock problem formulation. Section 8
concludes with a discussion and directions for future work.

394

Bin Completion Algorithms for Multicontainer Problems

1.1 One-Dimensional, Multicontainer Packing Problems
We consider the class of multicontainer packing problems: Given a set of items, which must
be assigned to one or more containers (“bins”), each item can be assigned to at most one
container. Each item j has a weight wj associated with it. Depending on the problem,
each item j may also have a profit or cost pj associated with it. We assume that item
weights and containers are one-dimensional. For some real-world applications, such as
continuous call double auctions (Kalagnanam, Davenport, & Lee, 2001), this model can be
applied directly. For other applications, such as the lot-to-order matching problem, the onedimensional model is an approximation (Carlyle, Knutson, & Fowler, 2001). We consider
two types of containers: (1) Containers with a capacity, where the sum of the weights of the
items assigned to a container cannot exceed its capacity, and (2) containers with a quota,
where the sum of the weights of the items assigned to a container must be at least as large
as the quota. When there is only a single container, we have the well-known 0-1 knapsack
problem. See the recent text by Kellerer, Pferschy, and Pisinger (2004) for an overview
of work on the 0-1 Knapsack problem and its variants. In this paper, we focus on four
one-dimensional, multicontainer packing problems (1) bin packing, (2) multiple knapsack,
(3) bin covering, and (4) min-cost covering.
1.1.1 The Bin Packing Problem
In the bin packing problem, the goal is to pack n items with weights w1 , ..., wn into bins of
capacity c such that all items are packed into the fewest number of bins, and the sum of the
weights of the items in each bin is no greater than the capacity. Classical applications of bin
packing include the classic vehicle/container loading problem (Eilon & Christofides, 1971),
as well as memory/storage allocation for data. The minimal number of agents required to
carry out a set of tasks in a multiagent planning problem can be modeled as a bin packing
problem.
More formally, the bin packing problem can be formulated as the integer program:

minimize

n
X

yi

subject to:

i=1
n
X

wj xij ≤ cyi ,

i = 1, ..., n

(2)

j=1
n
X

xij ≤ 1,

j = 1, ..., n

(3)

xij ∈ {0, 1}

i = 1, ..., n, j = 1, ..., n

(4)

yi ∈ {0, 1},

i = 1, ..., n

(5)

(1)

i=1

where yi represents whether the ith bin is used or not (yi = 1 if any items are assigned
to bin i, yi = 0 otherwise), and xij = 1 if item j is assigned to bin i, and 0 otherwise.
Constraint 2 ensures that the capacity is not violated for each bin that is instantiated, and
constraint 3 ensures that items are assigned to at most one bin.

395

Fukunaga & Korf

In this standard formulation, we assume that all bins have the same capacity. However,
this assumption is not restrictive, since instances where bins with different capacities (nonuniform bins) can be modeled by introducing additional items and constraints.
1.1.2 The 0-1 Multiple Knapsack Problem
Consider m containers with capacities c1 , ..., cm , and a set of n items, where each item has
a weight w1 , ..., wn and profit p1 , ..., pn . Packing the items in the containers to maximize
the total profit of the items, such that the sum of the item weights in each container does
not exceed the container’s capacity, and each item is assigned to at most one container is
the 0-1 Multiple Knapsack Problem, or MKP.
The MKP is a natural generalization of the 0-1 Knapsack Problem where there are m
containers of capacities c1 , c2 , ...cm . Let the binary decision variable xij be 1 if item j is
placed in container i, and 0 otherwise. Then the 0-1 Multiple Knapsack Problem can be
formulated as:
maximize

n
m X
X

subject to:

i=1 j=1
n
X

wj xij ≤ ci ,

i = 1, ..., m

(7)

j=1
m
X

xij ≤ 1,

j = 1, ..., n

(8)

xij ∈ {0, 1}

∀i, j.

(9)

pj xij

(6)

i=1

Constraint 7 encodes the capacity constraint for each container, and constraint 8 ensures
that each item is assigned to at most one container.
The MKP has numerous applications, including task allocation among a group of autonomous agents in order to maximize the total utility of the tasks executed (Fukunaga,
2005), continuous double-call auctions (Kalagnanam et al., 2001), multiprocessor scheduling (Labbé, Laporte, & Martello, 2003), vehicle/container loading (Eilon & Christofides,
1971), and the assignment of files to storage devices in order to maximize the number of
files stored in the fastest storage devices (Labbé et al., 2003). A special case of the MKP
that has been studied in its own right is the Multiple Subset-Sum Problem (MSSP), where
the profits of the items are equal to their weights, i.e., pj = wj for all j (e.g., Caprara,
Kellerer, & Pferschy, 2000b 2000a,2003). An application of the MSSP is the marble cutting
problem, where given m marble slabs, the problem is to decide how to cut the slabs into
sub-slabs (each sub-slab is then further processed into a product) in order to minimize the
total amount of wasted marble.
1.1.3 Bin Covering
Suppose we have n items with weights w1 , ..., wn , and an infinite supply of identical containers with quota q. The bin covering problem, also known as the dual bin packing problem
is to pack the items into containers such that the number of containers that contain sets of
items whose sums are at least q is maximized. That is, the goal is to distribute, or ration
396

Bin Completion Algorithms for Multicontainer Problems

the items among as many containers as possible, given that the containers have a specified
quota that must be satisfied. Note that the total weight of the items placed in a container
can be greater than q (we assume infinite capacity, although assigning an additional item
to a bin whose quota is already satisfies is clearly suboptimal).
More formally, the bin covering problem can be formulated as the integer program:

maximize

n
X

yi

subject to:

i=1
n
X

wj xij ≥ qyi ,

i = 1, ..., n

(11)

j=1
n
X

xij ≤ 1,

j = 1, ..., n

(12)

xij ∈ {0, 1}

i = 1, ..., n, j = 1, ..., n

(13)

yi ∈ {0, 1},

i = 1, ..., n

(14)

(10)

i=1

where yi represents whether the quota on the ith bin is satisfied (yi = 1) or not (yi = 0),
and xij = 1 if item j is assigned to bin i, and 0 otherwise. Constraint 11 ensures that the
quota is satisfied for each bin that is instantiated, and constraint 12 ensures that items are
assigned to at most one bin.
Bin covering is a natural model for resource or task allocation among multiple agents
where the goal is to maximize the number of agents who achieve some quota. It is also
models industrial problems such as: (1) packing peach slices into cans so that each can
contains at least its advertised net weight in peaches, and (2) breaking up monopolies into
smaller companies, each of which is large enough to be viable (Assmann, Johnson, Kleitman,
& Leung, 1984). Another application of bin covering is the lot-to-order matching problem
in the semiconductor industry, where the problem is to assign fabrication wafer lots to
customer orders of various sizes (Carlyle et al., 2001).
1.1.4 Min-Cost Covering Problem (Liquid Loading Problem)
We define the Min-Cost Covering Problem (MCCP) as follows. Given a set of m bins with
quotas {q1 , ..., qm }, and a set of n items with weights w1 , ..., wn and costs p1 , ..., pn , assign
some subset of the items to each bin such that (1) each item is assigned to at most one
bin, (2) the sum of the weights of the items assigned to each bin is at least the bin’s quota
(i.e., the bin is covered, as in bin covering), and (3) the total cost of all the items that are
assigned to a bin is minimized. This problem has also been called the liquid loading problem
(Christofides et al., 1979), because it was originally motivated by the following application:
Consider the disposal or transportation of m different liquids (e.g., chemicals) that cannot
be mixed. If we are given n tanks of various sizes, each with some associated cost, the
problem is to load the m liquids into some subset of the tanks so as to minimize the total
cost. Note that here, the “liquids” correspond to containers, and the “tanks” correspond
to the items.
Other applications of the MCCP include: (1) the storage of different varieties of grain
in different silos, where different types of grains cannot be mixed, (2) storage of food types
397

Fukunaga & Korf

in freezer compartments, (3) a trucking firm which distributes its trucks (of different sizes)
among customers with no mixing of different customer’s orders on any truck, and (4) storage
of information in storage devices (e.g., sensitive customer data that must be segregated
between physical filing cabinets or file servers). A closely related problem is the segregated
storage problem (Neebe, 1987; Evans & Tsubakitani, 1993).
More formally, the MCCP can be formulated as the integer program:

minimize

n
m X
X

subject to:

i=1 j=1
n
X

pj xij

(15)

wj xij ≥ qi ,

i = 1, ..., m

(16)

xij ≤ 1,

j = 1, ..., n

(17)

xij ∈ {0, 1}

∀i, j.

(18)

j=1
m
X
i=1

The binary variable xij represents whether item j is assigned to container i. Constraint
16 ensures that the quotas of all the bins are satisfied, and constraint 17 ensures that each
item is assigned to at most one bin.
1.1.5 A taxonomy of multicontainer problems
Table 1 summarizes the two key, defining dimensions of the four multicontainer problems
we study in this paper. One key dimension is whether the problem involves packing a set of
items into containers so that the container capacity is not exceeded (packing), or whether
the problem requires satisfying the quota associated with a container (covering). Another
key dimension is whether all of the items must be assigned to bins, or whether only a
subset of items are selected to be assigned to bins. A third dimension (not shown in the
table) is the set of item attributes (e.g., weight, profit/cost). Bin packing and bin covering
are single-attribute problems (items have weight only), while the multiple knapsack and
min-cost covering problems are two-attribute problems (items have weight and profit/cost).
We focus on the bin packing, bin covering, MKP, and MCCP problems because we
believe that these are in some sense the most “basic” multicontainer problems. Many
combinatorial optimization problems can be viewed as extensions of these problems with
additional constraints. For example, the generalized assignment problem can be considered
a generalization of a MKP with a more complex profit function.

packing
covering

assign all items to bins
bin packing
bin covering

assign subset of items to bins
multiple knapsack
min-cost covering

Table 1: Characterizing multicontainer problems.

398

Bin Completion Algorithms for Multicontainer Problems

2. Bin Completion
The multiple-knapsack problem, bin covering problem, min-cost covering problem, and bin
packing problem are all strongly NP-complete (proofs by reduction from 3-PARTITION,
e.g. Martello & Toth, 1990; Fukunaga, 2005). Thus, these problems cannot be solved by
a polynomial or pseudo-polynomial time algorithm unless P = N P , and the state-of-theart approach for optimally solving to these problems is branch-and-bound. In contrast,
the single-container 0-1 Knapsack problem and subset sum problem are only weakly NPcomplete, and can be solved in pseudopolynomial time using dynamic programming algorithms (Kellerer et al., 2004).
In this section, we begin by describing the standard item-oriented branch-and-bound approach to solving multi-container problems. We then describe bin completion, an alternate,
bin-oriented strategy. For clarity and simplicity of exposition, we detail the algorithms in
the context of the bin packing problem.
2.1 Item-Oriented Branch-and-Bound
The standard approach to solving multi-container problems is an item-oriented branch-andbound strategy. Suppose we have a bin packing problem instance where the bin capacity is
100, and we have 7 items with weights 83, 42, 41, 40, 12, 11, and 5. We perform a branchand-bound procedure to search the space of assignments of items to bins, where each node
in the branch-and-bound search tree corresponds to an item, and the branches correspond
to decisions about the bin in which to place the item. Assuming that we consider the items
in order of non-increasing weight, we first place the 83 in a bin, resulting in the set of
bins {(83)}. Next, we consider the 42. If we put it in the same bin as the 83, resulting
in {(83, 42)}, then we exceed the bin capacity. The other possibility is to put the 42 in
a new bin, resulting in {(83), (42)}. This is the only feasible choice at this node. Next,
the 41 is assigned to a bin. There are three possible places to assign the 41: (a) in the
same bin as the 83, resulting in {(83, 41), (42)}, (b) in the same bin as the 42, resulting in
{(83), (42, 41)}, and (c) in a new bin of its own, resulting in {(83), (42), (41)}. Option (a)
exceeds bin the capacity of the first bin and is infeasible. Options (b) and (c) are feasible
choices, so we branch on this item assignment, resulting in two subproblems, to which the
search procedure is recursively applied. Figure 1 illustrates a portion of this search space
(we show only the feasible nodes).
This branch-and-bound procedure is item-oriented because at each node, we decide
upon the placement of a particular item. Upper and lower bounding techniques specific to
the problem can be applied to make this basic depth-first strategy more efficient. Itemoriented branch-and-bound appears to be the most “natural” strategy for multicontainer
problems. The seminal paper by Eilon and Christofides (1971), which was the first paper to
address optimal solutions for multi-container problems, proposed an item-oriented branchand-bound strategy. Most of the work in the literature on algorithms for optimally solving
multi-container problems have relied upon this item-oriented strategy.

399

Fukunaga & Korf

(83)
(83)(42)

(83)(42)(41)

(83) (42,41)

(83)
(42,41) (40)

(83 12) (42 41) (40)

(83)(42 41 12)
(40)

(83)(42 41)
(40 12)

(83)(42)
(41,40)

(83)(42)
(41)(40)

...

...

(83)(42 41)
(40)(12)

Figure 1: Partial, item-oriented search space for a bin packing instance with capacity 100
and items {83,42,41,40,12,11,5}. Each node corresponds to a decision about which
existing or new bin an item is assigned to. Items are considered in nonincreasing
order of weight.

2.2 Bin Completion, a Bin-Oriented Branch-and-Bound Strategy
An alternate problem space for solving multi-container problems is bin-oriented, where
nodes correspond to decisions about which remaining item(s) to assign to the current bin.
A bin assignment B = (item1 , ..., itemk ) is a set of all the items that are assigned to
a given bin. Thus, a valid solution to a bin packing problem instance consists of a set of
bin assignments, where each item appears in exactly one bin assignment. A bin assignment
is feasible with respect to a given bin capacity c if the sum of its weights does not exceed
c. Otherwise, the bin assignment is infeasible. The definition of feasibility is the same for
the MKP; however, for bin covering and the MCCP, we define a bin assignment as feasible
with respect to a bin quota q if the sum of its weights is at least q. Given a set of k
remaining items, we say that a bin assignment S is maximal with respect to capacity c if
S is feasible, and adding any of the k remaining items would make it infeasible. Similarly,
for bin covering and the MCCP, a feasible bin assignment is minimal with respect to quota
q if removing any item would make it infeasible. For brevity, in the rest of this paper,
we omit the qualification “with respect to a given capacity/quota” when it is unnecessary
(for example, in bin packing and bin covering, all bins have the same capacity/quota so no
qualification is necessary).
Bin completion (for bin packing) is a bin-oriented branch-and-bound algorithm in which
each node represents a maximal, feasible bin assignment. Rather than assigning items one at
a time to bins, bin completion branches on the different maximal, feasible bin assignments.
The nodes at a given level in a bin completion search tree represent different maximal,
400

Bin Completion Algorithms for Multicontainer Problems

2
(83,12,5)
(42,41)

(42,40)

(83,11,5)
(42,11)

(40,11)
Figure 2: Bin-completion search space for a bin packing instance with capacity 100 and
items {83,42,41,40,12,11,5}. Each node represents a maximal, feasible bin assignment for a given bin. Bin assignments shown with a strikethrough, e.g., (83,11,5),
are pruned due to the dominance criterion described in Section 2.3.

feasible bin assignments that include the largest remaining item. The nodes at the next
level represent different maximal, feasible bin assignments that include the largest remaining
item, etc. The reason that we restrict sibling bin assignments in the bin packing search
tree to have the largest remaining number in common is to eliminate symmetries that are
introduced by the fact that all bins have the same capacity in our bin packing fomulation.
Thus, the depth of any branch of the bin completion search tree corresponds to the number
of bins in the partial solution to that depth. Figure 2 shows an example of a bin completion
search tree.
2.3 Dominance Criteria for Bin Assignments
The key to making bin completion efficient is the use of a dominance criterion on the feasible
bin assignments that requires us to only consider a small subset of them.
Bin comletion (for bin packing) only considers maximal, feasible assignments, because
it is clear that non-maximal assignments are dominated by maximal assignments, i.e., assigning a non-maximal assignment to a bin cannot lead to a solution with fewer bins than
assigning a maximal assignment. For example, if the current bin has 20 units of remaining
space and the remaining items are {15, 30, 40, 60}, it is always better to add the 15 to the
current bin, because we must eventually pack the 15 somewhere. We now formalize this
notion of dominance and describe more powerful dominance criteria that significantly prune
the search space.
Definition 1 (Dominance) Given two feasible bin assignments F1 and F2 , F1 dominates
F2 if the value of the optimal solution which can be obtained by assigning F1 to a bin is
no worse than the value of the optimal solution that can be obtained by assigning F2 to the
same bin.
If we can show that a bin assignment B is dominated by another bin assignment A, then
we can prune the search tree under B. Maximality is a trivial dominance criterion for bin
completion. If a feasible bin assignment B is not maximal, then by definition, it must be a

401

Fukunaga & Korf

subset of a maximal, feasible subset A, and B is clearly dominated by A. We now consider
more powerful dominance criteria.
Suppose we have a bin packing instance with bin capacity 100 and items {96,3,4,80,15,12}.
The sets (96,3) and (96,4) are both maximal, feasible bin assignments. If we choose the
bin assignment (96,3), the remaining subproblem has the unassigned items {80,15,12,4}.
On the other hand, if we choose the bin assignment (96,4), the remaining subproblem is
{80,15,12,3}. Clearly, the optimal solution to the subproblem {80,15,12,4} must use at least
as many bins as the optimal solution to the subproblem {80,15,12,3}. In other words, the
optimal solution in the subtree under the node (96,4) is at least as good as the optimal
solution in the subtree under (96,3), and therefore there is no need to search under (96,3)
because the bin assignment (96,4) dominates the bin assignment (96,3).
Christofides, Mingozzi, and Toth first proposed a more general form for this dominance
criterion in the context of the min-cost covering problem (Christofides et al., 1979). We
have reformulated their criterion in terms of the bin packing problem:
Proposition 1 (CMT Dominance for Bin Packing) Given two feasible sets A and B,
A dominates B if: (1) |A| ≥ |B| and (2) there exists a one-to-one (but not necessarily onto)
mapping π from B to A such that each item b ∈ B, is mapped to an element of A with a
weight that is greater than or equal to the weight of b, i.e., w(b) ≤ w(π(b)).1
In other words, if for each element b in a feasible bin assignment B, there is a corresponding item a of a feasible bin assignment A such that b ≤ a, then A dominates B. The
reason is as follows: consider a bin packing solution S, where a single bin is assigned the
items in B. All of the items must be assigned to some bin, so the items a ∈ A are assigned
to some other bin(s). For each item in B, we can swap the corresponding item in A, and the
resulting solution S ′ will be feasible, and have no more bins than S. For example, consider
a bin packing instance with the items {10,9,8,7,6} and bin capacity 20. The bin assignment
A = (10, 8) dominates the bin assignment B = (9, 7), because we can map the 9 to the 10
and map the 7 to the 8.
Martello and Toth (1990) proposed a more powerful dominance criterion that subsumes
the CMT criterion. Consider a bin packing instance with items {6,4,2,1,...} and capacity
10. The assignment (6,4) dominates the assignment (6,2,1) because given any solution with
the assignment (6,2,1), we can swap the 2 and 1 with the 4, resulting in a solution with the
assignment (6,4) and the same number of bins. The CMT criterion does not account for
this. More generally, consider two feasible bin assignments A and B. If all the elements of
B can be packed into bins whose capacities are the elements of A, then set A dominates set
B. For example, let A = (20, 30, 40) and let B = (5, 10, 10, 15, 15, 25). Partition B into the
subsets (5, 10),(25), and (10, 15, 15). Since 5 + 10 ≤ 20, 25 ≤ 30, and 10 + 15 + 15 ≤ 40,
set A dominates set B. More formally:
Proposition 2 (Martello-Toth Bin Packing Dominance Criterion) Let A and B be
two feasible bin assignments. A dominates B if B can be partitioned into i subsets B1 , ...Bi
such that each subset Bk is mapped one-to-one to (but not necessarily onto) an item ak ∈ A
such that the sum of the weights of the items in Bk is less than or equal to the weight of ak .
1. Recall that a function f : X → Y is one-to-one if for any two distinct elements x, x′ ∈ X, f (x) 6= f (x′ ).
A function f : X → Y is onto Y if each element of Y is the image under f of some element in X.

402

Bin Completion Algorithms for Multicontainer Problems

Proof: Suppose we have candidate solution s that assigns B to bin m, and all of the bin
assignments in s are currently feasible. Let A be a feasible bin assignment such that B can
be partitioned into i subsets B1 , ..., Bi , and each subset Bk can be mapped one-to-one to
ak ∈ A, such that the sum of the weights of the items in Bk is less than or equal to the weight
of ak . Consider swapping B with A. Each subset Bk is swapped with its corresponding
element ak (where ak is assigned to bin dk in the original solution s). Since A is feasible,
bin m remains feasible after A and B are swapped. Now consider each of the bins d1 , ...di
where the subsets B1 , ..., Bi end up after the swaps (in other words, d1 , ...di are the bins
which contain a1 , ..., ak , respectively, in the original solution s). Each such target bin dk
will remain feasible after the swap, since (1) the bin assignment of dk is feasible prior to the
swap, and (2) the sum of the weights of the items in Bk is less than or equal to the weight
of ak . Thus, the resulting solution s′ after the swap is (a) feasible (all bin assignments are
feasible) (b) no worse than the initial solution s, and (c) assigns A to bin m. Therefore, A
dominates B. 2
The Martello-Toth dominance criterion is a generalization of the CMT dominance criterion – the CMT dominance criterion is a special case of the Martello-Toth criterion where
we only consider partitions of B into single-element subsets. Thus, any node that would be
pruned by the CMT criterion is also pruned by the Martello-Toth criterion, but not vice
versa.
Similarly, we can define a dominance criterion for bin covering as follows:
For bin covering (and the MCCP), a bin assignment is feasible with respect to a given
bin if the sum of the item weights is greater than or equal to the bin quota q.
Proposition 3 (Bin Covering Dominance Criterion) Let A and B be two feasible assignments. A dominates B if B can be partitioned into i subsets B1 , ..., Bi such that each
item ak ∈ A is mapped one-to-one to (but not necessarily onto) a subset Bk , such that the
weight of each ak is less than or equal to the sum of the item weights of the corresponding
subset Bk (i.e., Bk “covers” ak ).
Proof: Suppose we have a solution s that assigns B to bin m, and all of the bin assignments
in s are currently feasible. Let A be a feasible bin assignment such that B can be partitioned
into i subsets B1 , ..., Bi , and each item ak ∈ A can be mapped one-to-one to a subset Bk ,
such that the weight of ak is less than or equal to the sum of the weights of the items in Bk .
Consider swapping B with A. Each subset Bk is swapped with its corresponding element ak
(where ak is assigned to bin dk in the original solution s). Since A is feasible, bin m remains
feasible after A and B are swapped. Now consider each of the bins d1 , ..., d|A| where the
subsets Bk end up after the swaps. In other words, d1 , ..., d|A| are the bins which contain
a1 , ..., ak , respectively, in the original solution s. Each such target bin dk will remain feasible
after the swap, since (1) the bin assignment of dk is feasible prior to the swap, and (2) the
sum of the weights of the items in Bk is greater than or equal to the weight of ak (and thus
the quota of bin dk will continue to be satisfied). Thus, the resulting solution s′ after the
swap is (a) feasible (all bin assignments are feasible) (b) no worse than the initial solution
s, and (c) assigns A to bin m. Therefore, A dominates B. 2
The dominance criteria for the MKP and MCCP are similar to the dominance criteria for
bin packing and bin covering, respectively, except that we must also take into consideration
the profits/costs. The proofs are similar to the proofs for bin packing and bin covering.
403

Fukunaga & Korf

Proposition 4 (MKP Dominance Criterion) Let A and B be two assignments that
are feasible with respect to capacity c. A dominates B if B can be partitioned into i subsets
B1 , ..., Bi such that each subset Bk is mapped one-to-one to (but not necessarily onto) ak ,
an element of A, and for all k ≤ i, (1) the weight of ak is greater than or equal the sum of
the item weights of the items in Bk , and (2) the profit of item ak is greater than or equal
to the sum of the profits of the items in Bk .
Proposition 5 (MCCP Dominance Criterion) Let A and B be two assignments that
are feasible with respect to quota q. A dominates B if B can be partitioned into i subsets
B1 , ..., Bi such that each item ak ∈ A is mapped one-to-one (but not necessarily onto) to a
subset Bk , and for each ak ∈ A and its corresponding subset Bk , (1) the weight of ak is less
than or equal to the sum of the item weights of the items in Bk , and (2) the cost of item ak
is less than or equal to the sum of the cost of the items in Bk .
The CMT dominance criterion for the MCCP originally proposed in (Christofides et al.,
1979) is a special case of Proposition 5, where |Bk | = 1 for all k.
Note that in the “packing” problems such as the MKP and bin packing, the dominance
criteria require that the subsets of the dominated assignment are packed into the items of the
dominating assignment. In contrast, with the “covering” problems such as the MCCP and
bin covering, the dominance criteria requires that the subsets of the dominated assignment
(B) cover the items in the dominating assignment (A).
2.3.1 Bin-Oriented Branch-and-Bound + Dominance = Bin Completion
At this point, we have defined the salient features of the bin completion approach to solving
multicontainer packing and knapsack problems:
• A bin-oriented branch-and-bound search where the nodes correspond to maximal (or
minimal), feasible bin assignments; and
• The exploitation of a dominance criterion among bin assignments to prune the search
space.
The first instance of a bin completion algorithm that we are aware of is the Christofides,
Mingozzi, and Toth algorithm for the min-cost covering problem in 1979 (Christofides et al.,
1979), which used the CMT criterion described above. However, as far as we know, no
further research was done with bin completion algorithms until our work on bin completion
for bin packing (Korf, 2002).
The Martello-Toth dominance criterion was proposed by Martello and Toth (1990), as a
component of the Martello-Toth Procedure (MTP), a branch-and-bound algorithm for bin
packing. However, the MTP branch-and-bound algorithm is item-oriented, and they only
exploit this dominance property in a limited way. In particular, they take each remaining
element x, starting with the largest element, and check if there is a single assignment of
x with one or two more elements that dominates all feasible sets containing x. If so, they
place x with those elements in the same bin, and apply the reduction to the remaining
subproblem. They also use dominance relations to prune some element placements as well.
Another earlier instance of a bin completion algorithm is the BISON algorithm for bin
packing by Scholl, Klein, and Jürgens (1997). BISON uses the following, very limited form
404

Bin Completion Algorithms for Multicontainer Problems

of the Martello-Toth dominance criterion: if a bin assignment has one or more items that
can be replaced by a single free item without decreasing the sum, then this assignment is
dominated. It is interesting that despite the fact that the basic idea of bin-oriented search
with dominance-based pruning was demonstrated by Christofides, Mingozzi, and Toth, both
the MTP and BISON only use a limited form of the Martello-Toth dominance criterion,
and the two ideas were not successfully integrated until our work.2 Presumably, the reason
is that it is not trivial to generate undominated bin assignments efficiently.
2.4 Generating Undominated Bin Assignments
A key component in bin completion is the efficient generation of undominated bin assignments. An obvious approach is to generate all feasible bin assignments, then apply the dominance tests to eliminate the dominated assignments. However, this is impractical, because
the number of feasible assignments is exponential in the number of remaining items, and
the memory and time required to generate and store all assignments would be prohibitive.
We now describe an algorithm that generates all and only undominated bin assignments,
which enables the efficient implementation of bin completion.
We can generate all subsets of n elements by recursively traversing a binary tree. Each
internal node corresponds to an item, where the left branch corresponds to the subsets that
include the item, and the right branch corresponds to the subsets that do not include the
item. Thus, each leaf node represents an individual subset. Note that this binary tree
is the search space for this subproblem (generating undominated bin assignments), and is
distinct from the search space of the higher level bin-completion algorithm (i.e.. the space
of undominated maximal assignments).
Given n elements and a container capacity c, the feasible bin assignments are the subsets of the n elements, the sum of whose weights do not exceed c. The recursive traversal
described above for generating all subsets can be modified to generate only feasible assignments as follows: At each node in the tree, we keep track of the sum of items that we have
committed to including in the subset (i.e., the sum of the weights of the items for which we
have taken the left branch). During the recursive traversal of the tree, we pass a parameter
representing the remaining capacity of the bin. Each time a left branch is taken (thereby
including the item), we reduce the remaining capacity by the item weight. If the remaining
capacity drops to zero or less, we prune the tree below the current node, since we have
equaled or exceeded the container capacity. Thus, this algorithm generates only feasible
bin assignments.
Now, we further extend the algorithm to generate only the undominated, feasible bin
assignments. Suppose that we have a feasible set A whose sum of weights is t. The excluded
items are all items that are not in A. Set A will be dominated (with respect to the MartelloToth dominance criterion) if and only if it contains any subset whose sum s is less than or
equal to an excluded item x, such that replacing the subset with x will not exceed the bin
capacity c. This will be the case if and only if there exists an excluded item x and a subset
with weight sum s such that t − s + x ≤ c. Therefore, to check if A is undominated, we
enumerate each possible subset of its items, and for each subset, we compare it against each
2. As shown in (Korf, 2002, 2003), bin-oriented search using the full Martello-Toth dominance criterion
results in dramatic speedups compared to the MTP.

405

Fukunaga & Korf

excluded number x to verify that t − s + x > c, where s is the sum of the item weights of the
subset. If so, then A is undominated, and we store A in a list of undominated assignments;
otherwise, A is dominated. Further optimizations can be found in (Korf, 2003).
This algorithm generates feasible bin assignments and immediately tests them for dominance, so it never stores multiple dominated bin assignments. Furthermore, the dominance
test is done by comparing the included elements to excluded elements, and does not involve
any comparison between a candidate bin assignment and previously generated bin assignments. Therefore, the memory required for dominance testing is linear in the number of
items. In contrast, a method that depends on comparisons between candidate sets, such
as the earlier algorithm described in (Korf, 2002), requires memory that is linear in the
number of undominated bin assignments, which is potentially exponential in the number
of items. The ability to incrementally generate undominated bin assignments using linear space without having to store all of the undominated assignments enables the hybrid
incremental branching strategy, described in Section 3.4.

3. Extensions to Bin Completion
We now describe some extensions to bin completion that significantly improve search efficiency. Again, for clarity, we describe the algorithms mostly in the context of bin packing,
but as with the basic bin completion algorithm, these extensions can be adapted straightforwardly to the multiple knapsack, bin covering, and min-cost covering problems.
3.1 Nogood Pruning (NP)
Nogood pruning prunes redundant nodes in the bin completion search tree by detecting
symmetries. Since we need to refer to specific bins, we extend our notation for bin assignment. Let (A)d denote a bin at depth d that is assigned the elements in A. Thus, (10, 8, 2)1
and (10, 7, 3)1 denote two possible bin assignments for the bin at depth 1.
Suppose we have an instance with the numbers {10,9,8,7,7,3,3,2,2}, and bin capacity
c=20. After exhausting the subproblem below the assignment (10, 8, 2)1 , and while exploring
the subproblem below the assignment (10, 7, 3)1 , assume we find a solution that assigns
(9, 8, 2)2 . We can swap the pair of items (8,2) from the assignment (9, 8, 2)2 with the pair
of items (7,3) from the assignment (10, 7, 3)1 , resulting in a solution with (10, 8, 2)1 and
(9, 7, 3)2 and the same number of bins. However, we have already exhausted the subtree
below (10, 8, 2)1 and we would have found a solution with the same number of bins as the
best solution in the subtree below (9, 7, 3)2 . Therefore, we can prune the branch below
(9, 8, 2)2 , because it is redundant (in other words, we have detected that the current partial
solution is symmetric to a partial state that has already been exhaustively searched).
More formally, let {N1 , N2 , ..., Nm } be a set of sibling nodes in the search tree, and
let {S1 , S2 , ..., Sm } be the bin assignments for each sibling node, excluding the first item
assigned to the bin, which is common to all the sibling nodes. When searching the subtree
below node Ni for i > 1, we exclude any bin assignment B that (1) includes all the items in
Sj , and (2) swapping the items in Sj from B with the items Si in Ni results in two feasible

406

Bin Completion Algorithms for Multicontainer Problems

bin assignments, for i > j. The items in Sj become a nogood with respect to nodes deeper
in the tree.3
If there exists such a bin assignment B, then we could swap the items Sj from B with
the items Si in Ni , resulting in a partial solution with the bin assignment Si in bin Ni .
However, we have already exhausted the subtree below Ni , so this is a redundant node and
can be pruned.
As the search progresses down the tree, a list of nogoods is maintained, which is the set
of bin assignments against which each candidate undominated bin assignment is compared.
Given a candidate bin assignment B, where the items are sorted according to weight, the
current implementation of the test for nogood pruning compares the items against each
nogood. Since the items in the nogood sets are also sorted by weight, each comparison
takes time linear in the cardinality of B. In the worst case, the number of nogoods that
we must compare with a candidate assignment at level d corresponds to the number of
undominated bin assignments at levels 1, ..., d − 1 that are currently on the stack (that is,
all of the ancestors and the siblings of the ancestors of the current node). Note that the list
of nogoods need not grow monotonically as we go down the search tree. If at any point, a
nogood N is no longer a subset of the set of remaining items, then N has been ’split’, and
can be removed from the nogood list that is passed down the search tree.
3.2 Nogood Dominance Pruning (NDP)
The following nogood dominance pruning (NDP) technique allows even more pruning: Suppose that after exhausting the subproblem below the assignment (10, 8, 2)1 , and while
exploring the subproblem below the assignment (10, 7, 3)1 , we consider the assignment
(9, 7, 2)2 . We can swap the pair of items (7,2) from bin 2 with the pair of items (7,3)
from bin 1 and end up with a solution with (10, 7, 2)1 and (9, 7, 3)2 . However, according
to the Martello-Toth dominance criterion, (10,7,2) is dominated by (10,8,2), and we have
already exhausted the search below the node (10, 8, 2)1 , so we can prune the search under
(9, 7, 2)2 because it is not possible to improve upon the best solution under (10, 8, 2)1 .
In general, given a node with more than one child, when searching the subtree of any
child but the first, we don’t need to consider assignments that are dominated by a bin
assignment in a previously explored child node. More precisely, let {N1 , N2 , ..., Nm } be a
set of sibling nodes in the search tree, and let {S1 , S2 , ..., Sm } be the corresponding sets of
items used in the bin assignment at each node. When searching the subtree below node
Ni for i > 1, we exclude any bin assignment A for which there exists an assignment Sj ,
j < i, such that (1) A is dominated by the items in Sj (note that an assignment dominates
itself), and (2) A can be swapped with Si , such that the resulting bin assignments are both
feasible. If there exists such a bin assignment A, then we could swap the items from A
with the items Si in Ni , resulting in a partial solution with the bin assignment A in bin Ni ,
and the same number of bins. However, since A is dominated by Sj , it means that we are
searching a node that is symmetric to one that is dominated by Sj , and therefore, it is not
possible to find a solution better than the best solution under Sj , so A can be pruned.
3. While the term nogood in the constraint programming literature often refers to an assignment of value(s)
to variable(s) that cannot lead to a solution, we use the term to mean an assignment that cannot lead
to any solution that is better than a previously discovered solution, similar to the usage in (Focacci &
Shaw, 2002).

407

Fukunaga & Korf

We can also describe nogood dominance pruning in terms of a more general constraint
programming formulation, where variables correspond to items and values denote the bins
to which they are assigned. Given a partial, j-variable solution x, nogood dominance
pruning tries to show, via swapping of items and dominance checks, that x is in the same
equivalence class as another partial solution x′ such that x̄′i , the subset of x′ including the
first i variables, is dominated by another partial solution q. Thus, if we have exhausted the
subtree below q in the search tree, we do not need to search the subtree below x′ .
Nogood dominance pruning is strictly more powerful than nogood pruning. Any node
pruned by nogood pruning will be pruned by nogood dominance pruning, but not vice
versa. Of course, since NDP must detect dominance relationships as opposed to equivalence relationships, NDP will incur more overhead per node compared to NP. Our current
implementation propagates a list of nogood sets along the tree. While generating the undominated completions for a given bin, we check each one to see if it is dominated by any
current nogood. If so, we ignore that bin assignment. Our current implementation uses
a brute-force algorithm to check for dominance between a candidate bin assignment and
each nogood, which in the worst case takes time exponential in the cardinality of the bin
assignment (for each nogood).
Since nogood pruning is much less expensive than nogood dominance pruning, we use
a combined pruning strategy. Whenever we apply NDP, we actually apply both NP and
NDP at each node. First, the candidate bin assignment is checked against a nogood, and
nogood pruning is applied. We apply nogood dominance pruning only if the bin assignment
was not pruned by nogood pruning. Thus, we never pay the full cost of applying NDP to
nodes that can be pruned quickly by NP. As with nogood pruning, NDP requires storing a
set of nogoods, and the number of possible nogoods at a particular search depth d in the
worst case is the number of undominated bin assignments considered at depths 1, ..., d − 1.
Note that when using NDP, we do not apply the optimization described above in Section
3.1 for removing nogoods from the nogood list that are passed down the tree when using
nogood pruning. The reason is that even if a nogood has been “split” and can no longer
prune a bin assignment due to nogood pruning, that nogood may still be able to prune a
bin assignment due to nogood dominance pruning.
The size of the nogood list increases with depth, and we compare each bin assignment
against each nogood. Therefore, the per-node overhead of NDP increases with depth. This
means that pruning at the bottom of the tree (where pruning has the lowest utility) is more
expensive than pruning at the top of the tree (where pruning has the highest utility). A
simple strategy which address this issue is depth-limited NDP, where NDP is applied only
at nodes up to the NDP depth limit L. At nodes below the depth limit, only the weaker
nogood pruning is applied. In the experiments described in this paper, we did not use this
depth-limited strategy because NDP consistently outperformed nogood pruning without
having to use depth limits.
3.3 Related Work in Constraint Programming
Nogood pruning identifies and prunes nodes by detecting whether the bin assignment for
the current node contains a nogood. This is related to other symmetry-breaking techniques
proposed in the constraint programming literature. A symmetry partitions the set of pos-

408

Bin Completion Algorithms for Multicontainer Problems

sible assignments of values to variables into equivalence classes (Gent & Smith, 2000). The
goal of symmetry breaking is to prune nodes that can be mapped to a previously explored
node via a symmetry function. Symmetry-breaking approaches introduce constraints during the search that prune symmetric variable assignments (e.g., Gent & Smith, 2000; Fahle,
Schamberger, & Sellmann, 2001; Focacci & Milano, 2001). Similarly, our nogood and nogood dominance pruning techniques dynamically introduce constraints that prune variable
assignments that cannot lead to a solution that is better than the best solution found so far.
Nogood dominance pruning uses the same dynamic nogood recording mechanism as nogood
pruning, but goes a step further in detecting dominance relationships based on the nogoods.
The general notion of dominance exploited by NDP is more powerful than symmetry, since
dominance can be asymmetric, i.e., all symmetries are dominance relationships, but not
vice versa.
Our NDP technique is similar to the pruning technique proposed by Focacci and Shaw
(2002) for constraint programming, who applied their technique to the symmetric and
asymmetric traveling salesperson problem with time windows. Both methods attempt to
prune the search by proving that the current node at depth j, which represents a partial jvariable (container) solution x, is dominated by some previously explored i-variable partial
solution (nogood), q. The main difference between the two methods is the approach used
to test for dominance. Focacci and Shaw’s method extends q to a j-variable partial solution
q ′ which dominates x. They apply a local search procedure to find the extension q ′ .
In contrast, our NDP method starts with a partial, j-variable solution x and tries to
transform it to a partial solution x′ such that x̄′i , the subset of x′ including the first i
variables, is dominated by q. We do this by swapping the values of the ith and jth variables
in x to derive x′ , and testing whether x̄′i is dominated by q.
For efficiency, the current implementations of both nogood dominance pruning methods
are weak, in the sense that if x is dominated by q, the procedures will not necessarily detect
the dominance. Focacci and Shaw rely on an incomplete, local search to find the extension
q ′ . Due to the cost of considering the transformations, we only consider transformations
involving two variables (containers), but to fully exploit the dominance criterion, we would
need to consider transformations involving all variables i, i + 1, ..., j.
3.4 Reducing the Computation Required Per Node
An issue with enumerating all undominated completions and applying value ordering is
that computing the undominated sets is itself NP-complete. For a multicontainer problem
instance where the items have weight wi andP
average container capacity is c, the average
number of items that fit in a container is x = ni=0 wi /c. The time to generate all undominated bin assignments increases with x. This is not an issue for bin packing, where problems
with large x tend to be easily solved using heuristics such as best-fit decreasing. The solution found by the heuristic often equals the lower bound. Therefore, such instances require
no search, and hence there is no need to compute undominated bin assignments. On the
other hand, for multiple knapsack and bin covering, we have observed experimentally that
it is much less likely that heuristics will match the optimistic bound and allow termination
without search, and we have found that for instances with high x, the algorithm would not
terminate within a reasonable time limit because it was spending an inordinate amount of

409

Fukunaga & Korf

2

b

a
e

d
h

i

f

g

c

k
l

j

Figure 3: Hybrid Incremental Branching Strategy
time computing the set of undominated completions at each node. In addition, generating
all undominated bin assignments may cause us to run out of memory.
An alternative approach is to start to go down the search tree and explore some of the
children of a node without first enumerating and sorting all the children. In cases where
(a) a good optimistic bound is available, and (b) it takes relatively little search to find an
optimal solution, this approach leads to dramatic speedups compared to the original scheme
of generating all undominated completions before going further down the search tree. The
price we pay for this strategy is that we lose the benefits of ordering all the candidate bin
assignments according to the value-ordering heuristic.
To solve this problem, we propose a hybrid incremental branching strategy that generates
h children at each node, applies the value-ordering heuristic to these, then recursively calls
bin completion on the remaining subproblems. For each node, we first generate h children,
and sort them according to the value-ordering heuristic. Then, we explore the subtrees under
each of these children. After the subtrees under the first h children are fully explored, then
the next h children are generated and their subtrees are explored, and so on.
For example, consider the tree in Figure 3, where the nodes correspond to undominated
bin assignments. Assume that this is the complete search tree, and assume for simplicity
that there is no value ordering heuristic. The standard bin completion algorithm first
generates the children of the root: a, b, and c. Then, bin completion selects one of the
children (say a), and expands a’s children (d,e,f ,g). It then generates the children of d,
and so on. Thus, the order of node generation is: a, b, c, d, e, f, g, h, i, j, k, l. This is a
pre-order traversal of the tree. Now consider hybrid incremental branching with width 1.
This corresponds to a standard postorder traversal of the tree, where the order in which
nodes are generated is a, d, h, i, j, e, f, g, b, k, l, c. Hybrid incremental branching with width
2 generates the nodes in the order a, b, d, e, h, i, j, f, g, k, l, c.

4. The Multiple Knapsack Problem (MKP)
Given a set of containers and a set of items (characterized by a weight and profit), the
objective of the multiple knapsack problem (defined formally in section 1.1.2) is to assign
items to containers such that the container capacities are not exceeded, and the sum of the
profits of the items assigned to the containers is maximized.
We compared bin completion to two standard algorithms: the state-of-the-art Mulknap
algorithm (Pisinger, 1999), as well as the MTM algorithm (Martello & Toth, 1981).
410

Bin Completion Algorithms for Multicontainer Problems

4.1 The MTM Algorithm
The MTM algorithm of Martello and Toth (1981) is an item-oriented branch-and-bound
algorithm. The items are ordered according to non-increasing efficiency (ratio of profit
to weight), so that the next item selected by the variable-ordering heuristic for the itemoriented branch-and-bound is the item with highest efficiency that was assigned to at least
one container by a greedy bound-and-bound procedure (see below). The branches assign
the selected item to each of the containers, in order of non-decreasing remaining capacity.
At each node, an upper bound is computed using a relaxation of the MKP, which is
obtained by combining all of the remaining
m containers in the MKP into a single conPm
tainer with aggregate capacity C = i=1 ci , resulting in the single-container, 0-1 knapsack
problem:

maximize

n
X

pj x′j

(19)

subject to

j=1
n
X

wj x′j ≤ C,

(20)

j=1
x′j ∈

{0, 1}, j = 1, ..., n.

(21)

where the variable x′j represents whether item j has been assigned to the aggregated bin.
This surrogate relaxed MKP (SMKP) can be solved by applying any algorithm for optimally
solving the 0-1 Knapsack problem, and the optimal value of the SMKP is an upper bound
for the original MKP. Thus, this upper bound computation is itself solving an embedded,
weakly NP-complete (single-container) 0-1 Knapsack problem instance as a subproblem.
The SMKP is currently the most effective upper bound for the MKP (details on how the
formulation above is derived from an initial surrogate relaxation are in Martello & Toth,
1981; Kellerer et al., 2004).
At each node, the MTM algorithm applies an extension to branch-and-bound called
bound-and-bound. Consider a branch-and-bound procedure for a maximization problem. If
the upper (optimistic) bound computed at a node is less than or equal to the best known
lower bound for the problem, then the node can be pruned. In the standard branch-andbound approach, the lower bound is simply the objective function value of the best solution
found so far.
In bound-and-bound, we attempt to validate the upper bound by applying a fast, heuristic algorithm in order to find a solution whose score equals the upper bound. If such a solution can be found, then it means that we have found the optimal solution under the current
node (i.e., the upper bound has been validated), and we can backtrack. On the other hand,
if no such solution is found, then we must continue to search under the node. The MTM
algorithm applies a greedy heuristic algorithm for the MKP, which involves solving a series
of m 0-1 Knapsack problems. First, container i = 1 is filled optimally using any of the
remaining items, and the items used to fill container i = 1 are removed. Then, container
i = 2 is filled using the remaining items. This process is iterated m times, at which point
all containers are filled.

411

Fukunaga & Korf

4.2 The Mulknap Algorithm
The previous state-of-the-art algorithm for the MKP is Mulknap (Pisinger, 1999). Like
MTM, Mulknap is an item-oriented branch-and-bound algorithm using the SMKP upper
bound and bound-and-bound. Mulknap differs from MTM in that it (1) uses a different
validation strategy for the bound-and-bound based on splitting the SMKP solution, (2)
applies item reduction at each node, and (3) applies capacity tightening at each node.
Like MTM, Mulknap uses a bound-and-bound strategy, but it uses a different approach
to validating the upper bound: As described above, the upper bound for a MKP instance
can be computed by solving the surrogate relaxation of the MKP, or SMKP, which is a 0-1
Knapsack instance. Suppose we have just computed the optimal solution to the SMKP.
Now, suppose we are able to partition the items used in the SMKP into the m containers
in the original MKP, such that each item used in the SMKP solution is assigned to some
container, and the capacity constraints are not violated. In this case, we have a solution to
the original MKP that achieves the upper bound. Details on the capacity tightening and
reduction procedures are given by Pisinger (1999).
4.3 Bin Completion Algorithm for the MKP
We now describe our bin completion algorithm for the MKP. We apply a depth-first, bin
completion branch-and-bound algorithm. Each node in the search tree represents a maximal, feasible bin assignment for a particular bin. Note that in the MKP, bin capacities
can vary, so at each node, we consider all undominated bin assignments for the bin. This
is in contrast to bin packing 2.2, where (assuming all bins have identical capacity), we can
restrict all nodes at the same level to have the same (largest) item.
At each node, an upper bound for the remaining subproblem is computed using the
surrogate relaxed MKP (SMKP) bound. The SMKP bound is computed using a straightforward branch-and-bound algorithm with the Martello-Toth U2 upper bound (Martello &
Toth, 1977). Pisinger’s R2 reduction procedure (Pisinger, 1999) is applied at each node
in order to possibly reduce the remaining problem. Then, we select the bin with smallest
remaining capacity (i.e., we use a smallest-bin-first variable ordering heuristic, with ties
broken randomly), and its undominated bin assignments are computed and explored using
our dominance criterion (Proposition 4). Nogood pruning and nogood dominance pruning
are applied as described in Sections 3.1 and 3.2.
The order in which we branch on the undominated children of the current node has a
significant impact on the algorithm’s performance. At each node, all of the undominated
children of the node are generated and ordered using a value ordering heuristic. We evaluated 11 different value ordering heuristics and found that the best performing heuristic
overall was the min-cardinality-max-profit ordering, where candidate bin assignments are
sorted in order of non-decreasing cardinality and ties are broken according to non-increasing
order of profit. Fukunaga (2005) provides further details on experiments evaluating various
value and variable ordering strategies. Figure 4 shows an outline of our bin completion
algorithm for the MKP.

412

Bin Completion Algorithms for Multicontainer Problems

MKP bin completion(bins,items)
bestProfit = −∞
search MKP(bins,items,0)
search MKP(bins, items,sumProfit)
if bins==∅ or items == ∅
/*we have a candidate solution*/
if sumProfit > bestProfit
bestProfit = sumProfit
return
/* Attempt to reduce problem. reducedBinAssignments are
maximal, feasible assignments of items to bins */
reducedBinAssignments = reduce(bins,items)
ri = get items(reducedBinAssignments) /* items eliminated by reduction */
rb = get bins(reducedBinAssignments) /* bins eliminated by reduction */
if reducedItems 6= ∅
P
search MKP(bins \ rb, items \ ri, sumProfit+ i∈ri profit(i))
return
/* Attempt to prune based on upper bound */
if (sumProfit + compute upper bound(items,bins)) < bestProfit
return
bin = choose bin(bins)
undominatedBinAssignments = generate undominated(items,capacity(bin))
foreach A ∈ sort assignments(undominatedBinAssignments)
if not(nogood(A)) and not(nogood dominated(A))
assign undominated bin assignment to bin
P
search MKP(bins \ bin, items \ items in(A),sumProfit+ i∈A profit(i))

Figure 4: Outline of bin completion for the multiple knapsack problem.
compute lower bound returns the SMKP upper bound on profit for the
remaining subproblem, and reduce applies Pisinger’s R2 reduction to eliminate
items and bins if possible. choose bin selects the bin with least remaining
capacity, and and sort assignments sorts the undominated bin assignments in
order of non-decreasing cardinality, and ties are broken in order of non-increasing
profit. generate undominated generates undominated bin assignments using
the algorithm in (Section 2.4). nogood and nogood dominated apply nogood
pruning (Section 3.1) and nogood dominance pruning (Section 3.2).

413

Fukunaga & Korf

4.4 Experimental Results
We evaluated our MKP algorithm using the same four classes of instances used by Pisinger
(1999). We considered:
• uncorrelated instances, where the profits pj and weights wj are uniformly distributed
in [min, max].
• weakly correlated instances, where the wj are uniformly distributed in [min,max] and
the pj are randomly distributed in [wj − (max − min)/10, wj + (max − min)/10] such
that pj ≥ 1,
• strongly correlated instances, where the wj are uniformly distributed in [min,max] and
pj = wj + (max − min)/10, and
• multiple subset-sum instances, where the wj are uniformly distributed in [min, max]
and pj = wj .
The bin capacities
follows: The first m − 1 capacities ci were uniformly
Pn were set asP
distributed in [0.4 j=1 wj /m, 0.6 nj=1 wj /m] for i = 1, ..., m − 1. The last capacity cm
P
Pm−1
is chosen as cm = 0.5 nj=1 wj − i=1
ci to ensure that the sum of the capacities is half
of the total weight sum. Degenerate instances were discarded as in Pisinger’s experiments
(1999). That is, we only used instances where: (a) each of the items fits into at least one of
the containers, (b) the smallest container is large enough to hold at least the smallest item,
and (c) the sum of the item weights is at least as great as the size of the largest container.
In our experiments, we used items with weights in the range [10,1000].
4.4.1 Comparison of Bin Completion With Previous Algorithms
We now compare bin completion (BC) with Mulknap and MTM. In all experiments described below involving Mulknap, we used Pisinger’s Mulknap code, available at his web
site4 , compiled using the gcc compiler with the -O3 optimization setting. Likewise, in all
experiments described below involving MTM, we used Martello and Toth’s Fortran implementation of MTM from (Martello & Toth, 1990), which was converted to C using f2c so
that we could add some instrumentation. Our bin completion code was implemented in
Common Lisp.5 We have shown experimentally that the choice of programming language
added an overhead of approximately a factor of two to our runtimes (see Fukunaga, 2005,
Appendix A) compared to GNU C 2.95 with -O3 optimization.
For each of the four problem classes, we generated test sets of 30 instances each, where
n = 20, and m was varied between 2 and 10. On each instance, we ran Mulknap, bin
completion, and bin completion with nogood dominance pruning. The purpose of this
experiment was to observe the behavior of the algorithms as the ratio n/m was varied. The
results are shown in Table 2. Each algorithm was given a time limit of 300 seconds to solve
each instance. The fail column indicates the number of instances (out of 30) which could
not be solved by the algorithm within the time limit. The time and nodes column show the
4. http://www.diku.dk/∼pisinger/
5. All of the bin completion solvers described in this paper for the MKP, MCCP, bin covering, and bin
packing problems are implemented in Common Lisp.

414

Bin Completion Algorithms for Multicontainer Problems

m

n

fail

# bins

# items

2
3
4
5
6
7
8
9
10

20
20
20
20
20
20
20
20
20

0
0
0
0
0
1
1
1
3

2
3
4
5
6
7
8
9
10

20
20
20
20
20
20
20
20
20

0
0
0
0
0
0
0
0
2

2
3
4
5
6
7
8
9
10

20
20
20
20
20
20
20
20
20

0
0
0
0
0
0
2
2
4

2
3
4
5
6
7
8
9
10

20
20
20
20
20
20
20
20
20

0
0
0
0
0
0
2
2
4

MTM
time

nodes

fail

Mulknap
time
nodes

Uncorrelated Instances
201
0
0.0000
1
697
0
0.0000
23
1530
0
0.0030
306
34800
0
0.0387
2977
100387
0
0.4063
36057
851591
1
3.8207
369846
3539729
1 23.7996 2493203
11294580
4 33.2138 2868979
13536735
5 52.0812 4848676
Weakly Correlated Instances
0.0040
781
0
0.0000
26
0.0127
2693
0
0.0167
638
0.0327
6626
0
0.0390
1708
0.2363
53324
0
0.2917
12701
0.2763
65216
0
0.4220
20165
0.6237
168974
0
1.3180
60890
11.0927
2922499
2
6.5793
40339
27.5807 11263928
3 22.5878 1274756
25.1029 10697165
5 72.3472 4478185
Strongly Correlated Instances
0.0723
5747
0
0.0037
32
0.1243
6748
0
0.0053
65
0.1237
8125
0
0.0120
156
0.1777
14823
0
0.0360
563
0.1807
18012
0
0.0550
734
5.1393
882080
1
0.0928
1339
11.2361
2616615
3 12.2581
225236
20.0136
3150848
4 33.5415
693398
38.8554
8141902
9 34.3291
634358
Subset-Sum Instances
0.0820
2061
0
0.0033
9
0.2057
7033
0
0.0337
77
0.1760
8371
0
0.0503
137
0.2507
17514
0
0.1780
830
0.2393
17546
0
0.1953
738
5.7657
852615
1
0.3221
1752
14.9261
2546834
3 19.5930
270841
25.9393
3124421
4 41.8962
677467
51.5027
7907407
9 39.7500
627688
0.0003
0.0017
0.0063
0.0767
0.2343
1.5721
6.3352
18.3314
22.5185

Bin Completion+NDP
fail
time
nodes

0
0
0
0
0
0
0
0
0

0.0050
0.0063
0.0023
0.0033
0.0050
0.0043
0.0023
0.0017
0.0010

55
123
112
212
403
399
207
136
84

0
0
0
0
0
0
0
0
0

0.0340
0.0117
0.0133
0.0107
0.0080
0.0050
0.0027
0.0013
0.0013

87
240
379
521
405
245
144
66
43

0
0
0
0
0
0
0
0
0

0.0743
0.0417
0.0353
0.0350
0.0217
0.0137
0.0073
0.0060
0.0057

118
278
331
359
194
109
42
25
21

0
0
0
0
0
0
0
0
0

0.0447
0.0310
0.0260
0.0257
0.0173
0.0113
0.0073
0.0063
0.0057

64
248
294
315
176
102
39
25
20

Table 2: MTM, Mulknap, and Bin Completion with Nogood Dominance Pruning on small
MKP instances with varying n/m ratio.The fail column indicates the number
of instances (out of 30) that were not solved within the time limit (300 seconds/instance). The time (seconds on 2.7GHz Pentium 4) and nodes columns
show average time spent and nodes generated on the successful runs, excluding
the failed runs.

415

Fukunaga & Korf

average time spent and nodes generated on the successful runs, excluding the failed runs.
All experiments described in this section were run on a 2.7GHz Pentium 4.
Our data confirms the observations of Pisinger (1999) and Martello and Toth (1981) that
the class of uniform, random instances that require the most search for previous branchand-bound solvers appear to be generated when n/m is relatively low. In other words,
the n/m ratio for the MKP appears to be a critical parameter that determines search
difficulty, similar to the clause-to-variable ratio for satisfiability problems (Mitchell, Selman,
& Levesque, 1992). Table 2 shows that Mulknap and MTM have great difficulty with
relatively small problems with small n/m ratios, while bin completion could solve these
problems very quickly.
Next, we investigated larger problem instances, where n/m varied between 2 and 4.
In this experiment, we also included bin completion with nogood pruning (BC+NP), in
addition to MTM, Mulknap, and BC+NDP.
As shown in Table 3, for problems where n/m = 2 or 3, both variants of bin completion
(BC+NP and BC+NDP) dramatically outperformed both MTM and Mulknap, with the
difference in performance becoming more pronounced as problem size was increased. Because the runs were truncated at 300 seconds per instance per algorithm, it is not possible
to compare the full runtimes for all of the sets. Note, for example, that on subset-sum
instances where n = 40 and m = 20, the mean runtime for BC+NDP is 0.06 seconds, while
neither MTM nor Mulknap solved any of the instances within 300 seconds each, so they are
both at least three orders of magnitude slower than BC+NDP on these instances. We tried
running the solvers longer in order to get actual runtimes (as opposed to a truncated, lower
bound on runtimes), but found that even allocating an hour per problem was insufficient
to allow MTM and Mulknap to solve any of these problems. These results suggest that
bin completion is asymptotically more efficient than MTM and Mulknap for this class of
problem instances. For the problems with n/m = 4, the results are similar, with the notable
exception that for the strongly-correlated instances, Mulknap outperformed bin completion
when n = 40 and m = 10.
For problems where n/m = 2 and 3, we observed that BC+NDP consistently outperforms BC+NP by a significant margin with respect to the number of nodes searched, and
significant improvements in success rate and execution times are observed for the larger
problems sets. However, when n/m = 4, BC+NDP still searches fewer nodes than BC+NP,
but the difference is much less significant, and in fact, the reduced search is not enough
to offset the increased overhead per node for NDP, so that the runtimes of BC+NP and
BC+NDP end up being comparable (Table 3).
We also ran some experiments with instances with larger n/m ratios. When n/m ≥ 5,
Mulknap clearly dominates all other current algorithms, solving instances with little or no
search in under a second, as first demonstrated by Pisinger (1999). At high n/m ratios,
the bound-and-bound approach of Mulknap has a very high probability of finding an upper
bound that matches the lower bound at each node, essentially eliminating the need for
search. In fact, Pisinger (1999) showed that for instances with n/m ≥ 10, there were
almost no instances that required searching more than one node.

416

MTM
time

n
# items

fail

15
20
25
10
15
5
10

30
40
50
30
45
20
40

30
30
30
12
30
0
7

47.77
0.08
99.13

15
20
25
10
15
5
10

30
40
50
30
45
20
40

30
30
30
16
30
0
29

68.80
0.10
205.99

15
20
25
10
15
5
10

30
40
50
30
45
20
40

30
30
30
1
30
0
13

23.48
0.06
115.38

15
20
25
10
15
5
10

30
40
50
30
45
20
40

30
30
30
0
18
0
5

25.58
0.0
0.05
35.20

nodes

fail

Mulknap
time
nodes

Bin Completion+NP
fail
time
nodes

Uncorrelated Instances
30
0
0.04
30
0
1.48
30
4
72.66
11
29.96
1649062
0
0.71
30
15
66.81
0
0.10
8487
0
0.01
4
51.66
1744911
0
11.58
Weakly Correlated Instances
30
0
0.07
30
0
0.79
30
0
34.39
8252155
15
69.65
1647576
0
0.94
30
19
52.42
21150
0
0.14
5758
0
0.01
12672965
27
131.41
2390152
0
21.10
Strongly Correlated Instances
30
0
0.04
30
0
3.46
30
9
52.26
2916858
0
7.02
88737
0
0.53
30
24
105.90
12490
0
0.02
486
0
0.01
5528847
0
44.55
361987
2
51.89
Subset-Sum Instances
30
0
0.01
30
0
0.13
30
0
4.37
1993648
0
4.74
43734
0
0.12
27.58
23
100.19
480014
2
45.07
5676
0
0.01
207
0
0.01
946934
0
10.48
58317
0
1.98
9240426
32602
15083247

Bin Completion + NDP
fail
time
nodes

1215
66681
2767939
48407
3369997
253
360567

0
0
3
0
15
0
0

0.02
0.72
32.43
0.61
60.54
0.01
13.47

887
26459
1060723
33141
2516681
237
297225

2123
22553
691645
34001
1126242
407
565036

0
0
0
0
18
0
0

0.05
0.35
15.80
0.79
67.17
0.01
20.63

1050
7473
231934
25764
1207099
396
490983

415
5407
58500
5662
964566
264
263462

0
0
4
0
23
0
2

0.03
2.29
58.72
0.50
118.72
0.01
50.51

194
2694
61900
5381
1150024
258
254094

171
1664
55334
1816
597581
140
26674

0
0
0
0
2
0
0

0.01
0.06
1.59
0.12
43.44
0.01
2.00

95
806
19491
1752
562129
137
26097

Table 3: Multiple knapsack problem results: Comparison of MTM, Mulknap, Bin Completion with Nogood Pruning (NP), and
Bin Completion with Nogood Dominance Pruning (NDP) on hard MKP instances. The fail column indicates the number
of instances (out of 30) that were not solved within the time limit (300 seconds/instance). The time (seconds on 2.7GHz
Pentium 4) and nodes show average time spent and nodes generated on the successful runs, excluding the failed runs.

Bin Completion Algorithms for Multicontainer Problems

417

m
# bins

Fukunaga & Korf

On the other hand, for n/m ≥ 5, bin completion tends to generate a very large (more
than 10000) number of undominated bin assignments at each decision node, and runs out of
memory while allocating the set of undominated bin assignments that are the candidates to
be assigned to the current bin. While hybrid incremental branching (Section 3.4) eliminates
this problem and allows the algorithm to run to completion, it is still not competitive with
Mulknap (runs do not complete within a 300-second time limit).
We conclude that uniform instances of the multiple knapsack problem exhibit a “bimodal” characteristic, for which bin completion and bound-and-bound are complementary
approaches. For n/m ≤ 4, uniform MKP instances require a significant amount of search
to solve, and our bin completion approach is clearly the current state of the art, as demonstrated by Table 3. On the other hand, for n/m ≥ 5, the bound-and-bound approach as
exemplified by Pisinger’s Mulknap algorithm is the state of the art, and the runtimes are
dominated by the computation of a single lower bound and a single upper bound at the
root node. There is a rather sharp “phase transition” around n/m = 4 where the dominant
approach changes between bin completion and bound-and-bound (see Fukunaga, 2005 for
details).
Because of the highly complementary nature of Mulknap and bin completion, the MKP
is a natural candidate for the application of an algorithm portfolio approach (Huberman,
Lukose, & Hogg, 1997; Gomes & Selman, 2001) where both Mulknap and bin completion
are run in parallel on the same problem instance. Even under a trivial portfolio resource
allocation scheme where both processes received equal time, the resulting total CPU usage
of the portfolio on each instance would be no worse than twice that of the faster algorithm.
Another way to combine bin completion with bound-and-bound is to add bound-and-bound
at each node of the bin completion search, and is an avenue for future work.

5. Min-Cost Covering Problem (MCCP)
Given a set of containers with quotas and a set of items (characterized by a weight and
cost), the objective of the MCCP is to assign items to containers such that all the container
quotas are satisfied, and the sum of the costs of the items assigned to the containers is
minimized (see 1.1.4 for a formal definition).
5.1 Christofides, Mingozzi, and Toth (CMT) Algorithm
The previous state-of-the-art algorithm for the min-cost covering problem is an early version
of bin completion by Christofides, Mingozzi, and Toth (1979). Their algorithm is a bin
completion algorithm which uses the CMT dominance criterion (see Section 2.3).
An effective lower bound can be computed by solving m independent minimization
problems (similar to the standard 0-1 Knapsack, except that the objective is to satisfy a
container’s quota while minimizing the cost of the items assigned to it), one for each of the
bins, and then summing the results. This L2 lower bound is a relaxation of the constraint
that each item can only be used once.
Christofides et al. (1979) proposed several more complex lower bounds, but they found
empirically that among all of the proposed lower bounds, the L2 bound resulted in the best
performance by far across a wide range of problem instances.

418

Bin Completion Algorithms for Multicontainer Problems

5.2 Bin Completion for the MCCP
Our new bin completion algorithm for the MCCP is similar to the Christofides et al. algorithm. The major difference is that we use a more powerful dominance criterion (Proposition
5). Each node in the depth-first, search tree represents a minimal, feasible bin assignment
for a particular bin. Our bin completion algorithm assigns bins in order of non-decreasing
size, i.e., a smallest-bin-first variable ordering heuristic. We evaluated eight different strategies for ordering the candidate undominated bin assignments, and found that a min-weight
strategy which sorts assignments in non-decreasing order of weight performed best. Fukunaga (2005) provides a detailed comparison of variable and value orderings for the MCCP.
We use the same L2 bound as the CMT algorithm.
5.3 Experimental Results
We compared the performance of bin completion variants and previous algorithms. We
implemented the following algorithms.
• CMT - The Christofides, Mingozzi and Toth algorithm described above. Our implementation used smallest-bin-first variable ordering, min-weight value ordering, and
the L2 lower bound.
• CMT+NP - The CMT algorithm extended with nogood pruning.
• BC - Bin completion using min-weight value ordering and smallest-bin-first variable
ordering.
• BC+NP - Bin completion with nogood pruning
• BC+NDP - Bin completion with nogood dominance pruning,
We used smallest-bin-first variable ordering for the CMT and BC variants after observing that this led to good performance compared with random or largest-bin-first variable
orderings.
In the experiment shown in Table 4, we compared CMT, CMT+NP, BC, BC+NP,
and BC+NDP. The purpose of this experiment was to evaluate the relative impact of
each component of bin completion, by comparing various combinations of: (1) the type of
dominance criterion used, (2) whether nogood pruning was used, and (3) whether nogood
dominance pruning was used.
We used the same four classes of test problems (uncorrelated, weakly correlated, strongly
correlated, and subset-sum) as for the multiple knapsack problem experiments (Section 4.4),
with item weights and costs in the range [10,1000]. For each of the four problem classes,
30 instances were generated for various values of m and n. We ran each algorithm on each
instance. All experiments were run on a 2.7GHz Pentium 4. Each algorithm was given a
time limit of 300 seconds to solve each instance. The fail column indicates the number of
instances (out of 30) which could not be solved by the algorithm within the time limit. The
time and nodes column show the total time spent and nodes generated, excluding the failed
runs.

419

n
# items

fail

5
10
5
10
15
20
5

15
30
10
20
30
40
20

0
30
0
1
30
30
0

5
10
5
10
15
20
5

15
30
10
20
30
40
20

5
10
5
10
15
20
5
5
10
5
10
15
20
5

CMT
time

nodes

fail

0.30
0.01
47.43
13.66

34094
256
5136873
1041966

0
30
0
0
11
30
0

0
30
0
0
28
30
1

0.21
0.01
3.22
227.63
54.11

10239
46
189874
16471570
975879

0
30
0
0
2
18
0

15
30
10
20
30
40
20

0
30
0
0
6
30
0

0.08
0.01
0.24
93.33
18.91

4226
30
19496
5178573
446419

0
30
0
0
0
18
0

15
30
10
20
30
40
20

0
30
0
0
10
30
0

0.08
0.01
0.17
74.57
18.11

4606
31
13286
3812510
384584

0
30
0
0
0
18
0

CMT+NP
time
nodes

fail

BC
time

nodes

Uncorrelated Instances
0.21
12641
0
0.02
678
20
184.79
12816074
0.01
48
0
0.01
127
0.17
10645
0
7.24
801863
80.04
3121178
30
30
0.10
4665
0
0.29
15979
Weakly Correlated Instances
0.13
4665
0
0.01
347
26
208
4242634
0.01
21
0
0.01
25
0.07
3979
0
0.13
11216
46.49
1803180
20
106.70
5367215
97.58
2874351
30
17.64
203251
0
0.65
15566
Strongly Correlated Instances
0.05
2518
0
0.01
68
0
54.65
1340276
0.01
16
0
0.01
10
0.02
1064
0
0.01
1064
5.18
241654
0
2.45
204069 0
97.19
2844301
17
52.50
2933474
20.78
253291
0
0.09
2474
Subset-Sum Instances
0.07
2899
0
0.01
65
0
32.71
810471
0.01
17
0
0.01
10
0.03
1573
0
0.01
986
17.48
789928
0
2.34
177683
97.90
2874340
17
55.18
2933474
17.66
203251
0
0.08
2270

fail

BC+NP
time

nodes

fail

BC+NDP
time
nodes

0
2
0
0
0
26
0

0.01
49.66
0.01
0.02
11.58
184.59
0.25

385
1952236
31
2149
705748
8356961
9339

0
0
0
0
0
10
0

0.01
23.82
0.01
0.01
2.81
97.67
0.15

238
635035
28
1149
136000
3145263
5193

0
16
0
0
0
12
0

0.01
168.21
0.01
0.01
3.19
112.08
0.54

197
2843080
12
428
182830
3871911
10032

0
5
0
0
0
2
0

0.01
97.16
0.01
0.01
0.56
44.84
0.45

150
1436142
12
236
30669
1704351
7698

0
0
0
0
0
1
0

0.01
17.60
0.01
0.01
0.12
27.17
0.07

53
333450
7
139
7532
1039810
1893

0
0
0
0
0
0
0

0.01
6.20
0.01
0.01
0.02
2.13
0.06

40
114786
7
79
1354
86685
1546

0
0
0
0
0
2
0

0.01
11.32
0.01
0.01
0.10
26.37
0.07

49
213975
8
146
6168
1160452
1605

0
0
0
0
0
0
0

0.01
3.97
0.01
0.01
0.02
2.47
0.06

42
70774
8
73
1272
125697
1242

Table 4: Min-cost covering problem results: Comparison of (a) Christofides, Mingozzi, and Toth (CMT) algorithm, (b) CMT algorihm with our Nogood
Pruning (NP), (c) Bin Completion, (d) Bin Completion with Nogood Pruning (NP), and (e) Bin Completion with Nogood Dominance Pruning
(NDP). The fail column indicates the number of instances (out of 30) that were not solved within the time limit. The time (seconds on 2.7GHz
Pentium 4) and nodes columns show average time spent and nodes generated on the successful runs, excluding the failed runs.

Fukunaga & Korf

420

m
# bins

Bin Completion Algorithms for Multicontainer Problems

As shown in Table 4, each component of bin completion has a significant impact. Although our dominance criterion requires much more computation per node than the simpler
CMT criterion, the search efficiency is dramatically improved. Thus, BC performed much
better than CMT, and BC+NP performed much better than CMT+NP.
Furthermore, the nogood pruning (NP) strategy significantly improves performance for
both the algorithm based on the CMT dominance criterion, as well as our dominance
criterion, as evidenced by the improvement of CMT+NP over CMT and the improvement
of BC+NP over BC. In fact, nogood pruning is sufficiently powerful that it allows CMT+NP
to sometimes outperform pure BC (without nogood pruning). Thus, this data illustrates
the power of nogood pruning, confirming similar results for bin packing reported in (Korf,
2003), as well as in some preliminary experiments for the MKP and MCCP.
Finally, BC+NDP results in the best performance, significantly outperforming BC+NP
for the larger instances with respect to the number of problems solved within the time limit,
as well as the runtimes and nodes on the problems that were solved.
We also implemented two baseline algorithms: a straightforward integer programming
model using the freely available GNU glpk integer programming solver, as well as an itemoriented branch-and-bound algorithm which uses the L2 lower bound, but these baselines
performed very poorly compared to the CMT and bin completion algorithms (Fukunaga,
2005).

6. The Bin Covering Problem
Given a set of identical containers with quota q and a set of n items, each with weight
wi , the bin covering problem, sometimes called dual bin packing, is to assign all items to
containers such that the number of containers whose quotas are satisfied (i.e., the sum of
item weights assigned to the container equal or exceed the quota) is maximized (see 1.1.3
for formal definition). Although there has been considerable interest in the bin covering
problem in the algorithm and operations research communities, most of the previous work
on bin covering has been theoretical, focusing on approximation algorithms or heuristic
algorithms (e.g., Assmann et al., 1984; Foster & Vohra, 1989; Csirik, Frenk, Galambos, &
Kan, 1991; Csirik, Johnson, & Kenyon, 2001), and analysis of the properties of some classes
of instances (e.g., Rhee & Talagrand, 1989).
6.1 The Labbé, Laporte, Martello Algorithm
The state-of-the-art algorithm for opimally solving bin covering is an item-oriented branchand-bound algorithm by Labbé, Laporte, and Martello (1995). We refer to this as the
LLM algorithm. The items are sorted in non-increasing order of size. Each node represents
a decision as to which bin to put an item into. At each node, upper bounds based on
combinatorial arguments are computed, and the remaining subproblem is reduced using
two reduction criteria. At the root node, a set of heuristics is applied in order to compute
an initial solution and lower bound. The LLM upper and lower bounds are described in
(Labbé et al., 1995).

421

Fukunaga & Korf

6.2 Bin Completion for Bin Covering
Our bin completion algorithm for bin covering works as follows. First, we use the LLM
upper bounding heuristics at the root node to find an initial solution and lower bound.
Then, we apply a bin completion branch-and-bound algorithm, using our new dominance
criterion (Proposition 3). Each node in the depth-first, search tree represents a minimal,
feasible bin assignment for a particular bin that include the largest remaining item. At each
node, we apply the same upper bounding procedures as the LLM algorithm to compute an
upper bound. In addition, we apply the LLM reduction criteria at each node. We evaluated
eight different strategies for ordering undominated bin assignments, and found that the mincardinality-min-sum strategy (sort bin assignments in order of non-decreasing cardinality,
breaking ties by non-decreasing sum) performed best overall (Fukunaga, 2005).
6.3 Empirical Results
In order to evaluate our bin covering algorithm, we considered the class of uniform, random
problem instances previously studied by Labbé, Laporte, and Martello. This is a simple
model in which n items are chosen uniformly in the range [min, max], where max is less
than the bin quota q. In their experimental evaluations, Labbé, Laporte, and Martello used
items with weights in the range 1 and 100, and bin quotas ranging between 100 and 500.
However, many instances in this class can be solved without any search. If the LLM
lower bound heuristics find a solution with the same number of bins as the LLM upper
bound, then we have found the optimal solution and can terminate without any search. We
say that an instance is trivial if it is solved without any search, and nontrivial otherwise.
To illustrate the prepondrance of trivial instances, we generated 10000 uniform, random
instances with bin quota 100000 and 120 items with weights in the range [1, 99999]. Of
these, 9084 were solved at the root node. This shows that most uniform random bin
covering instances are, in fact, trivial given powerful upper and lower bounds. We have
previously observed a similar phenomenon for bin packing – most uniform, random bin
packing instances are solved at the root node by the combination of the best-first-decreasing
heuristic and the Martello-Toth L2 lower bound (Korf, 2002).
It is well-known that the number of significant digits of precision in the weights of the
items significantly affects the difficulty of one-dimensional packing problems such as the
0-1 Knapsack problem (Kellerer et al., 2004). In general, problem difficulty increases with
precision. This property extends to multicontainer, one-dimensional packing problems as
well (e.g., Pisinger, 1999). We confirmed that problem difficulty was highly correlated with
the number of significant digits of item weights (Fukunaga, 2005).
Therefore, in the experiments described below, we only used nontrivial instances, in
order to highlight the impact of differences in search strategy. That is, when generating
test instances, we filtered out all trivial instances by testing whether the LLM upper bound
is matched by its lower bound heuristics. Furthermore, we use high precision problems
(quotas of 10000 or more) in order to focus on the most difficult instances.
6.3.1 Hybrid Incremental Branching
In Section 3.4, we proposed hybrid incremental branching, a strategy for avoiding the runtime and memory overheads imposed by completely enumerating all undominated children
422

Bin Completion Algorithms for Multicontainer Problems

of a node. In our earlier experiments with the multiple knapsack and min-cost covering
problems, (Sections 4–5), the problem instances used in our experiments had fewer than 50
items, and we have not observed more than a few hundred assignments generated at each
node in our MKP and MCCP experiments. Thus, the number of candidate undominated
bin assignments generated per node did not become a bottleneck, and hybrid incremental branching was unnecessary. However, as (1) the average number of items that fit in a
container increases, and (2) the number of items increases, the number of candidate undominated bin assignments per node increases. Therefore, the benchmark comparisons below
for bin covering, hybrid incremental branching becomes much more relevant.
To illustrate this, we performed the following experiment. We generated 20 nontrivial
instances with bin quota q=20000 and 100 items in the range [1,9999]. We applied bin
completion + NDP to these problem instances, using hybrid incremental branching with
various values for the parameter h, which limits the number of children that are generated
at once at every node. For h= 2000, 200, 20, and 2, the instances were solved on average
in 4.998 seconds, 0.150 seconds, 0.0139 seconds, and 0.0079 seconds, respectively. The
average number of nodes expanded was 20.2 for all h ∈ {2, 20, 200, 2000}. In other words,
these instances were easily solved by relying on the leftmost children at each node in the
bin completion search tree – generating additional children was an entirely unnecessary
but very expensive overhead. Thus, as h increases, the same number of nodes are being
explored, but each node requires much more computation to enumerate and sort up to h
undominated children, resulting in more than two orders of magnitude difference in runtime
between h = 2 and h = 2000. To see how large h would have to be in order to be able to
enumerate all of the undominated children of a node, we have experimented with h up to
10000, but found it insufficient (the statically allocated array of size h overflowed during
the runs).
We experimented with several values of h on several classes of problems, but found
that the optimal value of h varied significantly depending on the problem instance. In
our experiments below, we use hybrid incremental branching with h=100, in order to set
a balance between minimizing the number of nodes expanded (exploiting value ordering
among the children of each node) and minimizing the computational overhead per node (by
minimizing the number of children generated).
6.3.2 Comparing LLM and Bin Completion
Next, we compared LLM, bin completion+NP, and bin completion+NDP using larger instances. We also implemented a straightforward integer linear programming model using
GNU glpk for bin covering, but found that it performed much worse than the LLM and
bin completion algorithms (Fukunaga, 2005).
For each n ∈ {60, 80, 100}, we generated 2000 non-trivial, uniform random instances
where the items were chosen uniformly in the range [1,99999], and the bin quota was 100000.
We also generated 2000 non-trivial instances where n = 100, q = 200000, and items were in
the range [1,99999]. We ran our implementations of the three algorithms on each instance,
with a time limit of 180 seconds per instance. As shown in Table 5, the bin completion
algorithms significantly outperformed LLM. On the harder problems, bin completion + NDP
significantly outperforms bin completion + NP. For the problems with n = 100, q = 200000,

423

Fukunaga & Korf

n (# of items)

q (quota)

60
80
100

100000
100000
100000

100

200000

Labbé et al.
Bin Completion+NP
fail time
nodes fail time
nodes
2000 nontrivial instances per set
10 0.39
25196
6 0.08
33287
36 1.02
62093
15 0.18
39740
45 1.98 113110
15 0.42
152113
100 nontrivial instances per set
100
3 0.11
25

Bin Completion+NDP
fail time
nodes
2
12
8

0.11
0.09
0.31

13735
10517
31107

3

0.11

25

Table 5: Bin Covering results: Comparison of Labbé, Laporte, and Martello algorithm, Bin
Completion with Nogood Pruning, and Bin Completion with Nogood Dominance
Pruning. The fail column indicates the number of instances (out of 30) that were
not solved within the time limit. The time (seconds on 2.7GHz Pentium 4) and
nodes columns show average time spent and nodes generated on the successful
runs, excluding the failed runs.

our results indicate that while these instances are extremely difficult for the LLM algorithm,
which failed to solve any of the 100 instances within the time limit, these problems are
actually relatively easy for bin completion. Since little or no search is being performed by
bin completion, bin completion+NDP does not improve upon bin completion+NP.

7. Bin Packing
The bin completion approach was originally developed for the bin packing problem, and
was shown to significantly outperform the Martello-Toth Procedure on randomly generated
instances (Korf, 2002, 2003). We implemented an extended bin completion algorithm for the
bin packing problem, and summarize our results below. For further details, see (Fukunaga,
2005). Our bin completion based solver incorporated nogood dominance pruning and a
min-cardinality-max-weight value ordering strategy, in which (1) completions are sorted in
order of non-decreasing cardinality, and (2) ties are broken according to non-increasing
weight.
We evaluated our bin completion solver using the set of standard OR-LIB instances.6
This test set consists of 80 “triplet” instances (where the elements are generated three at
a time so that the sum of the elements add exactly to the bin capacity) and 80 “uniform”
instances (where items sizes are chosen from a uniform random distribution). The bin
completion solver (executed on a 1.3GHz Athlon) was given a time limit of 15 minutes on
each instance from the benchmark set. It solved all 80 of the triplet instances from ORLIB within 1500 seconds combined. It solved all 40 of the uniform instances with 120 and
250 items in under 4 seconds combined. It solved 18 (out of 20) of the 500-item uniform
instances in 11.13 seconds combined, but failed to solve 2 instances. Bin completion solved
19 (out of 20) of the 1000-item uniform instances in 44 seconds combined, but failed to
solve one of the instances.
6. available at http://www.brunel.ac.uk/depts/ma/research/jeb/info.html

424

Bin Completion Algorithms for Multicontainer Problems

However, our bin completion solver was not competitive with the state of the art, which
is a recently developed branch-and-price integer linear programming solver by Belov and
Scheithauer (2006). Belov and Scheithauer provide a new benchmark set of 28 very hard
bin packing instances; their solver solved most of these within seconds, although some took
hours. Our current bin completion code could not solve any of the 28 instances, given
15 minutes per instance. They also report that the largest triplet instances from OR-LIB
(Triplet-501) were solved in an average of 33 seconds per instance (660 seconds total) on a
1GHz AMD Athlon XP. Furthermore, Belov was kind enough to run their solver on a set
of one hundred, 80-item, uniform, random instances that we generated with items in the
range [1,1000000] and a bin capacity of 1000000. Their solver solved all of these instances
in less than 1 second each at the root node (i.e., without any search), using rounding
heuristics based on the linear programming LP solution, whereas our best bin completion
solver required 534 seconds and searched 75,791,226 nodes.
7.1 Branch-and-Price vs. Bin Completion
Recent branch-and-price approaches for bin packing such as the solver by Belov and Scheithauer use a bin-oriented branching strategy, where decisions correspond to the instantiation of one or more maximal bin assignments (see Valério de Carvalho, 2002, for a survey
of branch-and-price approaches). At each node, a column generation procedure is used
to compute the LP lower bound. They derive much of their power from a very accurate
LP lower bound based on a cutting-stock formulation of bin packing (Gilmore & Gomory,
1961), which has been observed to almost always give the optimal value as the lower bound,
and has never been observed to give a value that is more than one greater than the optimal
value (e.g., Wäscher & Gau, 1996). In addition, rounding heuristics applied to fractional
LP-solutions often yield the optimal, integral solution. The combination of a very tight
LP lower bound and good upper bounding procedure results in very little search being
performed for almost all problem instances.
This branch-and-price LP-based approach does not seem to generalize straightforwardly
to the MKP and MCCP, in part due to the differences in the granularity of the objective
function. The objective function for bin packing counts the number of bins used. On the
other hand, the objective functions for the MKP and MCCP sums the profits of the items
assigned to the containers. Thus, the number of possible, distinct objective function values
for the MKP and MCCP is much larger than the number of distinct objective function values
for a bin packing problem of comparable size. Therefore, even if we assume the existence of
some formulation analogous to the cutting-stock problem, it is not likely that rounding up
the LP solution for the MKP, MCCP, or other problems with objective functions that are
fine-grained compared to bin packing will result in an optimistic bound that is as accurate
as for bin packing. This suggests that it may be difficult to develop a branch-and-price
solver that is competitive for these problems. On the other hand, since the granularity of
the objective function for bin covering is the same as that for bin packing, it is possible that
a branch-and-price approach could be applied to bin covering. However, we are unaware of
any such approach in the literature.

425

Fukunaga & Korf

8. Conclusions
We studied bin completion, a branch-and-bound approach for multi-container packing,
knapsack, and covering problems. While previous work focused on item-oriented, branchand-bound strategies that assign one item at a time to containers, bin completion is a
bin-oriented branch-and-bound algorithm that uses a dominance relationship between bin
assignments to prune the search. We presented a general framework for this approach,
and showed its general utility and applicability to multicontainer problems. We proposed
several extensions to bin completion that improve its efficiency, including nogood pruning,
nogood dominance pruning, variable and value ordering heuristics, and hybrid incremental
undominated completion generation. We demonstrated the power of the bin completion
approach by developing new, state-of-the-art algorithms for three fundamental multicontainer problems. We showed that bin completion algorithms significantly outperform the
Mulknap (Pisinger, 1999) and MTM (Martello & Toth, 1981) and algorithms on hard MKP
instances. We developed a new, state-of-the-art algorithm for the MCCP based on bin
completion. We showed that by exploiting a more powerful dominance criterion, our new
bin completion algorithms significantly outperform an early bin completion algorithm by
Christofides, Mingozzi, and Toth (1979). We developed a new, state-of-the-art algorithm for
bin covering based on bin completion, and showed that our bin completion algorithm significantly outperforms the item-oriented branch and bound algorithm by Labbé, Laporte, and
Martello (1995). However, our results for bin packing were not competitive with the stateof-the-art solver based on the cutting-stock approach. We showed that for all four of the
problems studied, nogood dominance pruning consistently improves upon the performance
of bin completion with nogood pruning.7
While we have focused on four particular multicontainer problems in this paper, there
are many similar problems involving the assignment of objects to multiple containers where
similar dominance relationships between candidate bin assignments can be exploited. Examples include the generalized assignment problem, a widely studied generalization of the
MKP with many applications where the weight and profit of an item is a function of the container to which it is assigned (e.g., Cattrysse & Wassenhove, 1992; Martello & Toth, 1990),
multiprocessor scheduling, which is equivalent to k-way number partitioning, (Dell’Amico
& Martello, 1995), and the segregated storage problem (Neebe, 1987; Evans & Tsubakitani,
1993). In addition, there are variants of the problems we studied with additional constraints,
such as the class-constrained multiple knapsack problem (Shachnai & Tamir, 2001a, 2001b;
Kellerer et al., 2004) which has applications in multimedia file storage. Exploiting powerful dominance criteria in a bin completion framework appears to be a promising future
direction for such multicontainer problems.
One issue with bin completion is that as the number of unique items grows, the number
of undominated bin assignments grows rapidly. While we showed that hybrid incremental
branching can significantly alleviate this problem (Section 6.3.1), a drawback is that it limits
the utility of value-ordering heuristics that are applied to sort undominated bin assignments
after they are generated. Thus, an algorithm that generates undominated assignments in
7. We previously showed that nogood pruning significantly improves performance over bin-completion without pruning in (Korf, 2003). This was also confirmed for the MCCP (Table 4), and for the MKP and
bin covering in preliminary experiments.

426

Bin Completion Algorithms for Multicontainer Problems

an order that conforms to a desired heuristic value ordering, rather than relying on sorting
the assignments after they are all generated, is an area for future work.

Acknowledgments
Thanks to Gleb Belov for running his solver on some of our bin packing test instances. The
anonymous reviewers provided many helpful comments and suggestions that improved this
paper. This research was supported by NSF under grant No. EIA-0113313, and by the
Jet Propulsion Laboratory, California Institute of Technology, under a contract with the
National Aeronautics and Space Administration.

References
Assmann, S., Johnson, D., Kleitman, D., & Leung, J. (1984). On a dual version of the
one-dimensional binpacking problem. Journal of Algorithms, 5, 502–525.
Belov, G., & Scheithauer, G. (2006). A branch-and-cut-and-price algorithm for onedimensional stock cutting and two-dimensional two-stage cutting. European Journal
of Operational Research, 171, 85–106.
Caprara, A., Kellerer, H., & Pferchy, U. (2000a). A PTAS for the multiple-subset sum
problem with different knapsack capacities. Information Processing Letters, 73, 111–
118.
Caprara, A., Kellerer, H., & Pferschy, U. (2000b). The multiple subset sum problem. SIAM
Journal of Optimization, 11, 308–319.
Caprara, A., Kellerer, H., & Pferschy, U. (2003). A 3/4-approximation algorithm for multiple
subset sum. Journal of Heuristics, 9, 99–111.
Carlyle, M., Knutson, K., & Fowler, J. (2001). Bin covering algorithms in the second stage
of the lot to order matching problem. Journal of the Operational Research Society,
52, 1232–1243.
Cattrysse, D., & Wassenhove, L. V. (1992). A survey of algorithms for the generalized
assignment problem. European Journal of Operational Research, 60, 260–272.
Christofides, N., Mingozzi, A., & Toth, P. (1979). Loading problems. In Christofides, N.,
Mingozzi, A., Toth, P., & Sandi, C. (Eds.), Combinatorial Optimization. John Wiley
& Sons.
Csirik, J., Frenk, J., Galambos, G., & Kan, A. R. (1991). Probabilistic analysis of algorithms
for dual bin packing problems. Journal of Algorithms, 12, 189–203.
Csirik, J., Johnson, D., & Kenyon, C. (2001). Better approximation algorithms for bin
covering. In Proc. of the 12th ACM/SIAM Symposium on Discrete Algorithms, pp.
557–566.
Dell’Amico, M., & Martello, S. (1995). Optimal scheduling of tasks on identical parallel
processors. ORSA Journal on Computing, 7 (2), 181–200.
Eilon, S., & Christofides, N. (1971). The loading problem. Management Science, 17 (5),
259–268.
427

Fukunaga & Korf

Evans, J., & Tsubakitani, S. (1993). Solving the segregated storage problem with Benders’
partitioning. Journal of the Operational Research Society, 44 (2), 175–184.
Fahle, T., Schamberger, S., & Sellmann, M. (2001). Symmetry breaking. In Proceedings of
the International Conference on Constraint Programming, pp. 93–107.
Focacci, F., & Milano, M. (2001). Global cut framework for removing symmetries. In
Proceedings of the International Conference on Constraint Programming, pp. 77–92.
Focacci, F., & Shaw, P. (2002). Pruning sub-optimal search branch brances using local
search. In Proc. Fourth International Workshop on Integration of AI and OR Techniques in Constraing Programming for Combinatorial Optimisation Problems (CPAI-OR), pp. 181–189.
Foster, D., & Vohra, R. (1989). Probabilistic analysis of a heuristic for the dual bin packing
problem. Information Processing Letters, 31, 287–290.
Fukunaga, A. (2005). Bin-Completion Algorithms for One Dimensional, Multicontainer
Packing Problems. Ph.D. thesis, UCLA.
Gent, I., & Smith, B. (2000). Symmetry breaking during search in constraint programming.
In Proc. European Conference on Artificial Intelligence, pp. 599–603.
Gilmore, P., & Gomory, R. (1961). A linear programming approach to the cutting stock
problem. Operations Research, 9, 849–859.
Gomes, C., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126, 43–62.
Huberman, B., Lukose, R., & Hogg, T. (1997). An economic approach to hard computational
problems. Science, 265, 51–54.
Kalagnanam, J., Davenport, A., & Lee, H. (2001). Computational aspects of clearing continuous call double auctions with assignment constraints and indivisible demand. Electronic Commerce Research, 1, 221–238.
Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack Problems. Springer-Verlag.
Korf, R. (2002). A new algorithm for optimal bin packing. In Proceedings of AAAI, pp.
731–736.
Korf, R. (2003). An improved algorithm for optimal bin packing. In Proceedings of the
International Joint Conference on Artificial Intelligence, pp. 1252–1258.
Labbé, M., Laporte, G., & Martello, S. (1995). An exact algorithm for the dual bin packing
problem. Operations Research Letters, 17, 9–18.
Labbé, M., Laporte, G., & Martello, S. (2003). Upper bounds and algorithms for the
maximum cardinality bin packing problem. European Journal of Operational Research,
149, 490–498.
Martello, S., & Toth, P. (1977). An upper bound for the zero-one knapsack problem and a
branch and bound algorithm. European Journal of Operational Research, 1, 169–175.
Martello, S., & Toth, P. (1981). A bound and bound algorithm for the zero-one multiple
knapsack problem. Discrete Applied Mathematics, 3, 275–288.
Martello, S., & Toth, P. (1990). Knapsack problems: algorithms and computer implementations. John Wiley & Sons.
428

Bin Completion Algorithms for Multicontainer Problems

Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of SAT
problems. In Proceedings of AAAI, pp. 459–65.
Neebe, A. (1987). An improved, multiplier adjustment procedure for the segregated storage
problem. Journal of the Operational Research Society, 38 (9), 815–825.
Pisinger, D. (1999). An exact algorithm for large multiple knapsack problems. European
Journal of Operational Research, 114, 528–541.
Rhee, W., & Talagrand, M. (1989). Optimal bin covering with items of random size. SIAM
Journal on Computing, 18, 487–498.
Scholl, A., Klein, R., & Jürgens, C. (1997). BISON: A fast hybrid procedure for exactly
solving the one-dimensional bin packing problem. Computers in Operations Research,
24 (7), 627–645.
Shachnai, H., & Tamir, T. (2001a). On two class-constrained versions of the multiple
knapsack problem. Algorithmica, 29, 442–467.
Shachnai, H., & Tamir, T. (2001b). Polynomial time approximation schemes for classconstrained packing problems. Journal of Scheduling, 4, 313–338.
Shaw, P. (2004). A constraint for bin packing. In Proceedings of 10th International Conference on the Principles and Practice of Constraint Programming 2004 (CP-2004),
Lecture Notes in Computer Science Vol. 3258, pp. 648–662. Springer.
Valério de Carvalho, J. (2002). LP models for bin packing and cutting stock problems.
European Journal of Operational Research, 141, 253–273.
Wäscher, G., & Gau, T. (1996). Heuristics for the integer one-dimensional cutting stock
problem: A computational study. OR Spektrum, 18 (3), 131–144.

429

Journal of Artificial Intelligence Research 28 (2007) 349–391

Submitted 06/06; published 03/07

Closed-Loop Learning of Visual Control Policies
Sébastien Jodogne
Justus H. Piater

Jodogne@Montefiore.ULg.ac.be
Justus.Piater@ULg.ac.be

Montefiore Institute (B28)
University of Liège, B-4000 Liège, Belgium

Abstract
In this paper we present a general, flexible framework for learning mappings from images to actions by interacting with the environment. The basic idea is to introduce a
feature-based image classifier in front of a reinforcement learning algorithm. The classifier
partitions the visual space according to the presence or absence of few highly informative local descriptors that are incrementally selected in a sequence of attempts to remove
perceptual aliasing. We also address the problem of fighting overfitting in such a greedy
algorithm. Finally, we show how high-level visual features can be generated when the
power of local descriptors is insufficient for completely disambiguating the aliased states.
This is done by building a hierarchy of composite features that consist of recursive spatial
combinations of visual features. We demonstrate the efficacy of our algorithms by solving
three visual navigation tasks and a visual version of the classical “Car on the Hill” control
problem.

1. Introduction
Designing robotic controllers quickly becomes a challenging problem. Indeed, such controllers face a huge number of possible inputs that can be noisy, must select actions among a
continuous set, and should be able to automatically adapt themselves to evolving or stochastic environmental conditions. Although a real-world robotic task can often be solved by
directly connecting the perceptual space to the action space through a given computational
mechanism, such mappings are usually hard to derive by hand, especially when the perceptual space contains images. Evidently, automatic methods for generating such mappings
are highly desirable, because many robots are nowadays equipped with CCD sensors.
In this paper, we are interested in reactive systems that learn to couple visual perceptions
and actions inside a dynamic world so as to act reasonably. This coupling is known as a
visual (control) policy. This wide category of problems will be called vision-for-action tasks
(or simply visual tasks). Despite about fifty years years of active research in artificial
intelligence, robotic agents are still largely unable to solve many real-world visuomotor
tasks that are easily performed by humans and even by animals. Such vision-for-action
tasks notably include grasping, vision-guided navigation and manipulation of objects so as
to achieve a goal. This article introduces a general framework that is suitable for building
image-to-action mappings using a fully automatic and flexible learning protocol.
1.1 Vision-for-Action and Reinforcement Learning
Strong neuropsychological evidence suggests that human beings learn to extract useful information from visual data in an interactive fashion, without any external supervisor (Gibson
c
2007
AI Access Foundation. All rights reserved.

Jodogne & Piater

& Spelke, 1983). By evaluating the consequence of our actions on the environment, we
learn to pay attention to visual cues that are behaviorally important for solving the task.
This way, as we interact with the outside world, we gain more and more expertise on our
tasks (Tarr & Cheng, 2003). Obviously, this process is task driven, since different tasks do
not necessarily need to make the same distinctions (Schyns & Rodet, 1997).
A breakthrough in modern artificial intelligence would be to design an artificial system
that would acquire object or scene recognition skills based only on its experience with
the surrounding environment. To state it in more general terms, an important research
direction would be to design a robotic agent that could autonomously acquire visual skills
from its interactions with an uncommitted environment in order to achieve some set of
goals. Learning new visual skills in a dynamic, task-driven fashion so as to complete an a
priori unknown visual task is known as the purposive vision paradigm (Aloimonos, 1990).
One plausible framework to learn image-to-action mappings according to purposive vision is Reinforcement Learning (RL) (Bertsekas & Tsitsiklis, 1996; Kaelbling, Littman,
& Moore, 1996; Sutton & Barto, 1998). Reinforcement learning is a biologically-inspired
computational framework that can generate nearly optimal control policies in an automatic
way, by interacting with the environment. RL is founded on the analysis of a so-called
reinforcement signal . Whenever the agent takes a decision, it receives as feedback a real
number that evaluates the relevance of this decision. From a biological perspective, when
this signal becomes positive, the agent experiences pleasure, and we can talk about a reward .
Conversely, a negative reinforcement implies a sensation of pain, which corresponds to a
punishment. The reinforcement signal can be arbitrarily delayed from the actions which
are responsible for it. Now, RL algorithms are able to map every possible perception to an
action that maximizes the reinforcement signal over time. In this framework, the agent is
never told what the optimal action is when facing a given percept, nor whether one of its
decisions was optimal. Rather, the agent has to discover by itself what the most promising
actions are by constituting a representative database of interactions, and by understanding the influence of its decisions on future reinforcements. Schematically, RL lies between
supervised learning (where an external teacher gives the correct action to the agent) and
unsupervised learning (in which no clue about the goodness of the action is given).
RL has had successful applications, for example turning a computer into an excellent
Backgammon player (Tesauro, 1995), solving the Acrobot control problem (Yoshimoto,
Ishii, & Sato, 1999), making a quadruped robot learn progressively to walk without any
human intervention (Huber & Grupen, 1998; Kimura, Yamashita, & Kobayashi, 2001; Kohl
& Stone, 2004), riding a bicycle (Randløv & Alstrøm, 1998; Lagoudakis & Parr, 2003) or
controlling a helicopter (Bagnell & Schneider, 2001; Ng, Coates, Diel, Ganapathi, Schulte,
Tse, Berger, & Liang, 2004). The major advantages of the RL protocol are that it is fully
automatic, and that it imposes very weak constraints on the environment.
Unfortunately, standard RL algorithms are highly sensitive to the number of distinct
percepts as well as to the noise that results from the sensing process. This general problem
is often referred to as the Bellman curse of dimensionality (Bellman, 1957). Thus, the high
dimensionality and the noise that is inherent to images forbid the use of basic RL algorithms
for direct closed-loop learning of image-to-action mappings according to purposive vision.
350

Closed-Loop Learning of Visual Control Policies

1.2 Achieving Purposive Vision through Reinforcement Learning
There exists a variety of work in RL on specific robotic problems involving a perceptual
space that contains images. For instance, Schaal (1997) uses visual feedback to solve a
pole-balancing task. RL has been used to control a vision-guided underwater robotic vehicle (Wettergreen, Gaskett, & Zelinsky, 1999). More recently, Kwok and Fox (2004) have
demonstrated the applicability of RL to learning sensing strategies using Aibo robots.
Reinforcement learning can also be used to learn strategies for view selection (Paletta &
Pinz, 2000) and sequential attention models (Paletta, Fritz, & Seifert, 2005). Let us also
mention the use of reinforcement learning in other vision-guided tasks such as ball kicking (Asada, Noda, Tawaratsumida, & Hosoda, 1994), ball acquisition (Takahashi, Takeda,
& Asada, 1999), visual servoing (Gaskett, Fletcher, & Zelinsky, 2000), robot docking (Weber, Wermter, & Zochios, 2004; Martı́nez-Marı́n & Duckett, 2005) and obstacle avoidance (Michels, Saxena, & Ng, 2005). Interestingly, RL is also used as a way of tuning the high-level parameters of image-processing applications. For example, Peng and
Bhanu (1998) introduce RL algorithms for image segmentation, whereas Yin (2002) proposes algorithms for multilevel image thresholding, and uses entropy as a reinforcement
signal.
All of these applications preprocess the images to extract some high-level information
about the observed scene that is directly relevant to the task to be solved and that feeds the
RL algorithm. This requires prior assumptions about the images perceived by the sensors
of the agent, and about the physical structure of the task itself. The preprocessing step
is task specific and is coded by hand. This contrasts with our objectives, which consist in
introducing algorithms able to learn how to directly connect the visual space to the action
space, without using manually written code and without relying on prior knowledge about
the task to be solved. Our aim is to develop general algorithms that are applicable to any
visual task that can be formulated in the RL framework.
A noticeable exception is the work by Iida et al. (2002) who apply RL to seek and reach
targets, and to push boxes (Shibata & Iida, 2003) with real robots. In this work, raw visual
signals directly feed a neural network that is trained by an actor-critic architecture. In these
examples, the visual signal is downscaled and averaged into a monochrome (i.e. two-color)
image of 64 × 24 = 1536 pixels. The output of four infrared sensors are also added to this
perceptual input. While this approach is effective for the specific tasks, this process can
only be used in a highly controlled environment. Real-world images are much richer and
could not undergo such a strong reduction in size.
1.3 Local-Appearance Paradigm
In this paper, we propose algorithms that rely on the extraction of visual features as a
way to achieve more compact state spaces that can be used as an input to traditional RL
algorithms. Indeed, buried in the noise and in the confusion of visual cues, images contain
hints of regularity. Such regularities are captured by the important notion of visual features.
Loosely speaking, a visual feature is a representation of some aspect of local appearance,
e.g. a corner formed by two intensity edges, a spatially localized texture signature, or a
color. Therefore, to analyze images, it is often sufficient for a computer program to extract
only useful information from the visual signal, by focusing its attention on robust and highly
351

Jodogne & Piater

percepts

Image Classifier

reinforcements

detected visual class
informative visual features

Reinforcement Learning

actions

Figure 1: The structure of Reinforcement Learning of Visual Classes.

informative patterns in the percepts. The program should thereafter seek the characteristic
appearance of the observed scenes or objects.
This is actually the basic postulate behind local-appearance methods that have had
much success in computer vision applications such as image matching, image retrieval and
object recognition (Schmid & Mohr, 1997; Lowe, 2004). They rely on the detection of
discontinuities in the visual signal thanks to interest point detectors (Schmid, Mohr, &
Bauckhage, 2000). Similarities in images are thereafter identified using a local description
of the neighborhood around the interest points (Mikolajczyk & Schmid, 2003): If two images
share a sufficient number of matching local descriptors, they are considered to belong to
the same visual class.
Local-appearance techniques are at the same time powerful and flexible, as they are
robust to partial occlusions, and do not require segmentation or 3D models of the scenes.
It seems therefore promising to introduce, in front of the RL algorithm, a feature-based
image classifier that partitions the visual space into a finite set of distinct visual classes
according to the local-appearance paradigm, by focusing the attention of the agent on
highly distinctive local descriptors located at interest points of the visual stimuli. The
symbol corresponding to the detected visual class could then be given as the input of a
classical, embedded RL algorithm, as shown in Figure 1.
This preprocessing step is intended to reduce the size of the input domain, thus enhancing the rate of convergence, the generalization capabilities as well as the robustness
of RL to noise in visual domains. Importantly, the same family of visual features can be
applied to a wide variety of visual tasks, thus the preprocessing step is essentially general
and task-independent. The central difficulty is the dynamic selection of the discriminative
visual features. This selection process should group images that share similar, task-specific
properties together in the same visual class.
1.4 Contributions
The key technical contribution of this paper consists in the introduction of reinforcement
learning algorithms that can be used when the perceptual space contains images. The
developed algorithms do not rely on a task-specific pre-treatment. As a consequence, they
can be used in any vision-for-action task that can be formalized as a Markov Decision
Problem. We now review the three major contributions that are discussed in this paper.
352

Closed-Loop Learning of Visual Control Policies

1.4.1 Adaptive Discretization of a Visual Space
Our first contribution is to propose a new algorithm called Reinforcement Learning of Visual
Classes (RLVC) that combines the aforementioned ideas. RLVC is an iterative algorithm
that is suitable for learning direct image-to-action mappings by taking advantage of the
local-appearance paradigm. It consists of two simultaneous, interleaved learning processes:
Reinforcement learning of a mapping from visual classes to actions, and incremental building
of a feature-based image classifier.
Initially, the image classifier contains one single visual class, so that all images are
mapped to this class. Of course, this introduces a kind of perceptual aliasing (or hidden
state) (Whitehead & Ballard, 1991): The optimal decisions cannot always be made, since
percepts requiring different reactions are associated with the same class. The agent then
isolates the aliased classes. Since there is no external supervisor, the agent can only rely on
a statistical analysis of the earned reinforcements. For each detected aliased class, the agent
dynamically selects a new visual feature that is distinctive, i.e. that best disambiguates the
aliased percepts. The extracted local descriptor is used to refine the classifier. This way,
at each stage of the algorithm, the number of visual classes in the classifier grows. New
visual features are learned until perceptual aliasing vanishes. The resulting image classifier
is finally used to control the system.
Our approach is primarily motivated by strong positive results of McCallum’s U-Tree
algorithm (McCallum, 1996). In essence, RLVC is an adaptation of U-Tree to visual spaces,
though the internals of the algorithms are different. The originality of RLVC lies in its
exploitation of successful local-appearance features. RLVC selects a subset of such highly
relevant features in a fully closed-loop, purposive learning process. We show that this
algorithm is of practical interest, as it can be successfully applied to several simulated
visual navigation tasks.
1.4.2 Compacting Visual Policies
Because of its greedy nature, RLVC is prone to overfitting. Splitting one visual class can
potentially improve the control policy for all the visual classes. Therefore, the splitting
strategy can get stuck in local minima: Once a split is made that subsequently proves
useless, it cannot be undone in the original description of RLVC. Our second contribution
is to provide RLVC with the possibility of aggregating visual classes that share similar
properties. Doing so has at least three potential benefits:
1. Useless features are discarded, which enhances generalization capabilities;
2. RLVC can reset the search for good features; and
3. the number of samples that the embedded RL algorithm has at its disposal for each
visual class is increased, which results in better visual control policies.
Experiments indeed show an improvement in the generalization abilities, as well as a reduction of the number of visual classes and selected features.
353

Jodogne & Piater

1.4.3 Spatial Combinations of Visual Features
Finally, the efficacy of RLVC clearly depends on the discriminative power of the visual
features. If their power is insufficient, the algorithm will not be able to completely remove
the aliasing, which will produce sub-optimal control policies. Practical experiments on
simulated visual navigation tasks exhibit this deficiency, as soon as the number of detected
visual features is reduced or as features are made more similar by using a less sensitive
metric. Now, most objects encountered in the world are composed of a number of distinct
constituent parts (e.g. a face contains a nose and two eyes, a phone possesses a keypad).
These parts are themselves recursively composed of other sub-parts (e.g. an eye contains an
iris and eyelashes, a keypad is composed of buttons). Such a hierarchical physical structure
certainly imposes strong constraints on the spatial disposition of the visual features.
Our third contribution is to show how highly informative spatial combinations of visual
features can be iteratively constructed in the framework of RLVC. This result is promising
for it permits the construction of features at increasingly higher levels of discriminative
power, enabling us to tackle visual tasks that are unsolvable using individual point features
alone. To the best of our knowledge, this extension to RLVC appears to be the very
first attempt to build visual feature hierarchies in a closed-loop, interactive and purposive
learning process.

2. An Overview of Reinforcement Learning
Our framework relies on the theory of RL, which is introduced in this section. In RL, the
environment is traditionally modeled as a Markov Decision Process (MDP). An MDP is
a tuple hS, A, T , Ri, where S is a finite set of states, A is a finite set of actions, T is a
probabilistic transition function from S × A to S, and R is a reinforcement function from
S × A to R. An MDP obeys the following discrete-time dynamics: If at time t, the agent
takes the action at while the environment lies in a state st , the agent perceives a numerical
reinforcement rt+1 = R(st , at ), then reaches some state st+1 with probability T (st , at , st+1 ).
Thus, from the point of view of the agent, an interaction with the environment is defined
as a quadruple hst , at , rt+1 , st+1 i. Note that the definition of Markov decision processes
assumes the full observability of the state space, which means that the agent is able to
distinguish between the states of the environment using only its sensors. This allows us to
talk indifferently about states and percepts. In visual tasks, S is a set of images.
A percept-to-action mapping is a fixed probabilistic function π : S 7→ A from states to
actions. A percept-to-action mapping tells the agent the probability with which it should
choose an action when faced with some percept. In RL terminology, such a mapping is called
a stationary Markovian control policy. For an infinite sequence of interactions starting in a
state st , the discounted return is
Rt =

∞
X

γ i rt+i+1 ,

(1)

i=0

where γ ∈ [0, 1[ is the discount factor that gives the current value of the future reinforcements. The Markov decision problem for a given MDP is to find an optimal percept-to-action
mapping that maximizes the expected discounted return, whatever the starting state is. It is
354

Closed-Loop Learning of Visual Control Policies

possible to prove that this problem is well-defined, in that such an optimal percept-to-action
mapping always exists (Bellman, 1957).
Markov decision problems can be solved using Dynamic Programming (DP) algorithms
(Howard, 1960; Derman, 1970). Let π be a percept-to-action mapping. Let us call the
state-action value function Qπ (s, a) of π, the function giving for each state s ∈ S and each
action a ∈ A the expected discounted return obtained by starting from state s, taking action
a, and thereafter following the mapping π:
Qπ (s, a) = Eπ {Rt | st = s, at = a} ,

(2)

where Eπ denotes the expected value if the agent follows the mapping π. Let us also define
the H transform from Q functions to Q functions as
X
(HQ)(s, a) = R(s, a) + γ
T (s, a, s0 ) max
Q(s0 , a0 ),
(3)
0
s0 ∈S

a ∈A

for all s ∈ S and a ∈ A. Note that the H transform is equally referred to as the Bellman
backup operator for state-action value functions. All the optimal mappings for a given MDP
share the same Q function, denoted Q∗ and called the optimal state-action value function,
that always exists and that satisfies Bellman’s so-called optimality equation (Bellman, 1957)
HQ∗ = Q∗ .

(4)

Once the optimal state-action value function Q∗ is known, an optimal deterministic perceptto-action mapping π ∗ is easily derived by choosing
π ∗ (s) = argmax Q∗ (s, a),

(5)

a∈A

for each s ∈ S. Another very useful concept from the DP theory is that of optimal value
function V ∗ . For each state s ∈ S, V ∗ (s) corresponds to the expected discounted return
when the agent always chooses the optimal action in each encountered state, i.e.
V ∗ (s) = max Q∗ (s, a).
a∈A

(6)

Dynamic Programming includes the well-known Value Iteration (Bellman, 1957), Policy Iteration (Howard, 1960) and Modified Policy Iteration (Puterman & Shin, 1978) algorithms. Value Iteration learns the optimal state-action value function Q∗ , whereas Policy
Iteration and Modified Policy Iteration directly learn an optimal percept-to-action mapping.
RL is a set of algorithmic methods for solving Markov decision problems when the
underlying MDP is not known (Bertsekas & Tsitsiklis, 1996; Kaelbling et al., 1996; Sutton
& Barto, 1998). Precisely, RL algorithms do not assume the knowledge of T and R. The
input of RL algorithms is basically a sequence of interactions hst , at , rt+1 , st+1 i of the agent
with its environment. RL techniques are often divided in two categories:
1. Model-based methods that first build an estimate of the underlying MDP (e.g. by
computing the relative frequencies that appear in the sequence of interactions), and
then use classical DP algorithms such as Value or Policy Iteration;
2. Model-free methods such as SARSA (Rummery & Niranjan, 1994), T D(λ) (Barto,
Sutton, & Anderson, 1983; Sutton, 1988), and the popular Q-learning (Watkins,
1989), that do not compute such an estimate.
355

Jodogne & Piater

3. Reinforcement Learning of Visual Classes
As discussed in the Introduction, we propose to insert an image classifier before the RL
algorithm. This classifier maps the visual stimuli to a set of visual classes according to
the local-appearance paradigm, by focusing the attention of the agent on highly distinctive
local descriptors detected at the interest points of the images.
3.1 Incremental Discretization of the Visual Space
Formally, let us call D, the infinite set of local descriptors that can be spanned through the
chosen local description method. The elements of D will be equivalently referred to as visual
features. Usually, D corresponds to Rn for some n ≥ 1. We assume the existence of a visual
feature detector , that is a Boolean function D : S × D 7→ B testing whether a given image
exhibits a given local descriptor at one of its interest points (Schmid et al., 2000). Any
suitable metric can be used to test the similarity of two visual features, e.g. Mahalanobis
or Euclidean distance.
The image classifier is iteratively refined. Because of this incremental process, a natural
way to implement the image classifiers is to use binary decision trees. Each of their internal
nodes is labeled by the visual feature, the presence of which is to be tested in that node.
The leaves of the trees define a set of visual classes, which is hopefully much smaller than
the original visual space, and upon which it is possible to apply directly any usual RL
algorithm. To classify an image, the system starts at the root node, then progresses down
the tree according to the result of the feature detector D for each visual feature found during
the descent, until reaching a leaf.
To summarize, RLVC builds a sequence C0 , C1 , C2 , . . . of growing decision trees, in a
sequence of attempts to remove perceptual aliasing. The initial classifier C0 maps all of its
input images in a single visual class V0,1 . At any stage k, the classifier Ck partitions the
visual perceptual space S into a finite number mk of visual classes {Vk,1 , . . . , Vk,mk }.
3.2 Learning Architecture
The resulting learning architecture has been called Reinforcement Learning of Visual Classes
(RLVC) (Jodogne & Piater, 2005a). The basic idea behind our algorithms, namely the
iterative learning of a decision tree, is primarily motivated by adaptive-resolution techniques
that have been previously introduced in reinforcement learning, and notably by McCallum’s
U-Tree algorithm (McCallum, 1996). In this section, this idea is showed to be extremely
fruitful when suitably adapted to visual spaces. The links between RLVC and adaptiveresolution techniques will be more thoroughly discussed in Section 3.6.
The components of RLVC are depicted in Figure 2. An in-depth discussion of each of
those components will be given in the next sections. For the time being, we review each of
them:
RL algorithm: For each classifier in the sequence, an arbitrary, standard RL algorithm
is applied. This provides information such as the optimal state-action function, the
optimal value function or the optimal policy that are induced by the current classifier
Ck . For the purpose of these computations, either new interactions can be acquired,
356

Closed-Loop Learning of Visual Control Policies

Figure 2: The different components of the RLVC algorithm.
or a database of previously collected interactions can be exploited. This component
is covered in Sections 3.3.1 and 3.3.2.
Aliasing detector: Until the agent has learned the visual classes required to complete
its task, from the viewpoint of the embedded RL algorithm, the input space is only
partially observable. The aliasing detector extracts the classes in which perceptual
aliasing occurs, through an analysis of the Bellman residuals. Indeed, as explained
in Section 3.3.3, there exist tight relations between perceptual aliasing and Bellman
residuals. If no aliased class is detected, RLVC stops.
Feature generator: After having applied the RL algorithm, a database of interactions
hst , at , rt+1 , st+1 i is available. The feature generator component produces a set F of
candidate visual features for each aliased class Vk,i . The features that are used to
refine a classifier will be chosen among this set of candidates. This step is further
exposed in Sections 3.4 and 5.
Feature selector: Once the set of candidate features F is built for the aliased visual
class Vk,i , this component selects the visual feature f ∗ ∈ F that best reduces the
perceptual aliasing. If no candidate feature is discriminant, the component returns
the conventional bottom symbol ⊥. The feature selector of RLVC is described in
Section 3.4.
Classifier refinement: The leaves that correspond to the aliased classes in the featurebased image classifier are replaced by an internal node testing the presence or absence
of the selected visual features.
Post-processing: This optional component is invoked after every refinement, and corresponds to techniques for fighting overfitting. Details are given in Section 4.
The general outline of RLVC is described in Algorithm 1. Note that in all the experiments that are contained in this paper, model-based RL algorithms were applied to static
357

Jodogne & Piater

Algorithm 1 — General structure of RLVC
1: k ← 0
2: mk ← 1
3: Ck ← binary decision tree with one leaf
4: repeat
5:
Collect N interactions hst , at , rt+1 , st+1 i
6:
Apply an arbitrary RL algorithm on the sequence that is mapped through Ck
7:
Ck+1 ← Ck
8:
for all i ∈ {1, . . . , mk } do
9:
if aliased(Vk,i ) then
10:
F ← generator ({st | Ck (st ) = Vk,i })
11:
f ∗ ← selector (Vk,i , F )
12:
if f ∗ 6= ⊥ then
13:
In Ck+1 , refine Vk,i by testing f ∗
14:
mk+1 ← mk+1 + 1
15:
end if
16:
end if
17:
end for
18:
k ←k+1
19:
post-process(Ck )
20: until Ck = Ck−1
databases of interactions at the fifth step of Algorithm 1. These databases were collected
using a fully randomized exploration policy. This choice was only guided by the ease of
implementation and presentation; any other way of collecting experience could be used as
well, for example by re-sampling a new database of interactions at each iteration of RLVC.
The crucial point here is that RLVC generates a representation for visual control policies
only from a set of collected visuomotor experience, which makes RLVC interactive. The following sections describe the remaining algorithms, namely aliased, generator, selector
and post-process.
3.3 Detection of the Aliased Visual Classes
We now discuss how aliasing can be detected in a classifier Ck .
3.3.1 Projection of an MDP through an Image Classifier
Formally, any image classifier Ck converts a sequence of N interactions
hst , at , rt+1 , st+1 i,
to a mapped sequence of N quadruples
hCk (st ), at , rt+1 , Ck (st+1 )i,
upon which the embedded RL algorithm is applied. Let us define the mapped MDP Mk as
the MDP
hSk , A, Tk , Rk i,
358

Closed-Loop Learning of Visual Control Policies

that is obtained from the mapped sequence, where Sk is the set of visual classes that are
known to Ck , and where Tk and Rk have been computed using the relative frequencies in
the mapped sequence, as follows.
Consider two visual classes V, V 0 ∈ {Vk,1 , . . . , Vk,mk } and one action a ∈ A. We define
the following functions:
• δt (V, a) equals 1 if Ck (st ) = V and at = a, and 0 otherwise;
• δt (V, a, V 0 ) equals 1 if Ck (st ) = V , Ck (st+1 ) = V 0 and at = a, and 0 otherwise;
• η(V, a) is the number of t’s such that δt (V, a) = 1.
Using this notation, we can write:
• Sk = {Vk,1 , . . . , Vk,mk };
P
0
• Tk (V, a, V 0 ) = N
t=1 δt (V, a, V ) /η(V, a);
P
• Rk (V, a) = N
t=1 rt δt (V, a)/η(V, a).
3.3.2 Optimal Q Function for a Mapped MDP
Each mapped MDP Mk induces an optimal Q function on the domain Sk × A that will be
0∗
denoted Q0∗
k . Computing Qk can be difficult: In general, there may exist no MDP defined
on the state space Sk and on the action space A that can generate a given mapped sequence,
since the latter is not necessarily Markovian anymore. Thus, if some RL algorithm is run
on the mapped sequence, it might not converge toward Q0∗
k , or not even converge at all.
However, when applied on a mapped sequence, any model-based RL method (cf. Section 2)
can be used to compute Q0∗
k if Mk is used as the underlying model. Under some conditions,
Q-learning also converges to the optimal Q function of the mapped MDP (Singh, Jaakkola,
& Jordan, 1995).
In turn, the function Q0∗
k induces another Q function on the initial domain S ×A through
the relation:
Q∗k (s, a) = Q0∗
(7)
k (Ck (s), a) ,
In the absence of aliasing, the agent would perform optimally, and Q∗k would correspond to
Q∗ , according to Bellman theorem that states the uniqueness of the optimal Q function (cf.
Section 2). By Equation 4, the function
Bk (s, a) = (HQ∗k )(s, a) − Q∗k (s, a)

(8)

is therefore a measure of the aliasing induced by the image classifier Ck . In RL terminology,
Bk is Bellman residual of the function Q∗k (Sutton, 1988). The basic idea behind RLVC is
to refine the states that have a non-zero Bellman residual.
3.3.3 Measuring Aliasing
Consider a time stamp t in a database of interactions hst , at , rt+1 , st+1 i. According to
Equation 8, the Bellman residual that corresponds to the state-action pair (st , at ) equals
X
Bk (st , at ) = R(st , at ) + γ
T (st , at , s0 ) max
Q∗k (s0 , a0 ) − Q∗k (st , at ).
(9)
0
a ∈A

s0 ∈S

359

Jodogne & Piater

Algorithm 2 — Aliasing Criterion
1: aliased(Vk,i ) :–
2:
for all a ∈ A do
3:
∆ ← {∆t | Ck (st ) = Vk,i ∧ at = a}
4:
if σ 2 (∆) > τ then
5:
return true
6:
end if
7:
end for
8:
return false
Unfortunately, the RL agent does not have access to the transition probabilities T and to
the reinforcement function R of the MDP modeling the environment. Therefore, Equation 9 cannot be directly evaluated. A similar problem arises in the Q-learning (Watkins,
1989) and the Fitted Q Iteration (Ernst, Geurts, & Wehenkel, 2005) algorithms. These
algorithms solve this problem by considering the stochastic version of the time difference
that is described by Equation 9: The value
X
T (st , at , s0 ) max
Q∗k (s0 , a0 )
(10)
0
a ∈A

s0 ∈S

can indeed be estimated as
max
Q∗k (s0 , a0 ),
0
a ∈A

(11)

if the successor s0 is chosen with probability T (st , at , s0 ). But following the transitions of
the environment ensures making a transition from st to st+1 with probability T (st , at , st+1 ).
Thus
∆t = rt+1 + γ max
Q∗k (st+1 , a0 ) − Q∗k (st , at )
a0 ∈A

0
0∗
= rt+1 + γ max
Q0∗
k Ck (st+1 ) , a − Qk (Ck (st ), a)
0
a ∈A

(12)
(13)

is an unbiased estimate of the Bellman residual for the state-action pair (st , at ) (Jaakkola,
Jordan, & Singh, 1994).1 Very importantly, if the system is deterministic and in the absence
of perceptual aliasing, these estimates are equal to zero. Therefore, a nonzero ∆t potentially
indicates the presence of perceptual aliasing in the visual class Vt = Ck (st ) with respect to
action at . Our criterion for detecting the aliased classes consists in computing the Q0∗
k
function, then in sweeping again all the interactions hst , at , rt+1 , st+1 i to identify nonzero
∆t . In practice, we assert the presence of aliasing if the variance of the ∆t exceeds a given
threshold τ . This is summarized in Algorithm 2, where σ 2 (·) denotes the variance of a set
of samples.
3.4 Generation and Selection of Distinctive Visual Features
Once aliasing has been detected in some visual class Vk,i ∈ Sk with respect to an action a,
we need to select a local descriptor that best explains the variations in the set of ∆t values
1. It is worth noticing that αt ∆t corresponds to the updates that would be applied by Q-learning, where
αt is known as the learning rate at time t.

360

Closed-Loop Learning of Visual Control Policies

Algorithm 3 — Canonical feature generator
1: generator({s1 , . . . , sn }) :–
2:
F ← {}
3:
for all i ∈ {1, . . . , n} do
4:
for all (x, y) such that (x, y) is an interest point of si do
5:
F ← F ∪ {symbol(descriptor(si , x, y))}
6:
end for
7:
end for
8:
return F
corresponding to Vk,i and a. This local descriptor is to be chosen among a set of candidate
visual features F .
3.4.1 Extraction of Candidate Features
Informally, the canonical way of building F for a visual class Vk,i consists in:
1. identifying all collected visual percepts st such that Ck (st ) = Vk,i ,
2. locating all the interest points in all the selected images st , then
3. adding to F the local descriptor of all those interest points.
The corresponding feature generator is detailed in Algorithm 3. In the latter algorithm,
descriptor(s, x, y) returns the local description of the point at location (x, y) in the image s,
and symbol(d) returns the symbol that corresponds to the local descriptor d ∈ F according
to the used metric. However, more complex strategies for generating the visual features can
be used. Such a strategy that builds spatial combinations of individual point features will
be presented in Section 5.
3.4.2 Selection of Candidate Features
The problem of choosing the candidate feature that most reduces the variations in a set of
real-valued Bellman residuals is a regression problem, for which we suggest an adaptation of
a popular splitting rule used in the CART algorithm for building regression trees (Breiman,
Friedman, & Stone, 1984).2
In CART, variance is used as an impurity indicator: The split that is selected to refine
a particular node is the one that leads to the greatest reduction in the sum of the squared
differences between the response values for the learning samples corresponding to the node
and their mean. More formally, let S = {hxi , yi i} be a set of learning samples, where xi ∈ Rn
are input vectors of real numbers, and where yi ∈ R are real-valued outputs. CART selects
the following candidate feature:


v
v
f ∗ = argmin pv⊕ · σ 2 S⊕
+ pv	 · σ 2 S	
,

(14)

v∈F

2. Note that in our previous work, we used a splitting rule that is borrowed from the building of classification
trees (Quinlan, 1993; Jodogne & Piater, 2005a).

361

Jodogne & Piater

Algorithm 4 — Feature Selection
1: selector(Vk,i , F ) :–
2:
f∗ ← ⊥
{Best feature found so far}
3:
r∗ ← +∞
{Variance reduction induced by f ∗ }
4:
for all a ∈ A do
5:
T ← {t | Ck (st ) = Vk,i and at = a}
6:
for all visual feature f ∈ F do
7:
S⊕ ← {∆t | t ∈ T and st exhibits f }
8:
S	 ← {∆t | t ∈ T and st does not exhibit f }
9:
s⊕ ← |S⊕ |/|T |
10:
s	 ← |S	 |/|T |
11:
r ← s⊕ · σ 2 (S⊕ ) + s	 · σ 2 (S	 )
12:
if r < r∗ and the distributions (S⊕ , S	 ) are significantly different then
13:
f∗ ← f
14:
r∗ ← s
15:
end if
16:
end for
17:
end for
18:
return f ∗

where pv⊕ (resp. pv	 ) is the proportion of samples that exhibit (resp. do not exhibit) the
v (resp. S v ) is the set of samples that exhibit (resp. do not exhibit)
feature v, and where S⊕
	
the feature v. This idea can be directly transferred in our framework, if the set of xi
corresponds to the set of interactions hst , at , rt+1 , st+1 i, and if the set of yi corresponds to
the set of ∆t . This is written explicitly in Algorithm 4.
Our algorithms exploit the stochastic version of Bellman residuals. Of course, real environments are in general non-deterministic, which generates variations in Bellman residuals
that are not a consequence of perceptual aliasing. RLVC can be made somewhat robust to
such a variability by introducing a statistical hypothesis test: For each candidate feature,
a Student’s t−test is used to decide whether the two sub-distributions the feature induces
are significantly different. This approach is also used in U-Tree (McCallum, 1996).
3.5 Illustration on a Simple Navigation Task
We have evaluated our system on an abstract task that closely parallels a real-world scenario
while avoiding any unnecessary complexity. As a consequence, the sensor model we use may
seem unrealistic; a better visual sensor model will be exploited in Section 4.4.
RLVC has succeeded at solving the continuous, noisy visual navigation task depicted
in Figure 3. The goal of the agent is to reach as fast as possible one of the two exits of
the maze. The set of possible locations is continuous. At each location, the agent has four
possible actions: Go up, right, down, or left. Every move is altered by a Gaussian noise,
the standard deviation of which is 2% the size of the maze. Glass walls are present in the
maze. Whenever a move would take the agent into a wall or outside the maze, its location
is not changed.
362

Closed-Loop Learning of Visual Control Policies

Figure 3: A continuous, noisy navigation task. The exits of the maze are indicated by boxes
with a cross. Walls of glass are identified by solid lines. The agent is depicted at
the center of the figure. Each one of the four possible moves is represented by an
arrow, the length of which corresponds to the resulting move. The sensors return
a picture that corresponds to the dashed portion of the image.

The agent earns a reward of 100 when an exit is reached. Any other move, including
the forbidden ones, generates zero reinforcement. When the agent succeeds in escaping the
maze, it arrives in a terminal state in which every move gives rise to a zero reinforcement.
In this task, γ was set to 0.9. Note that the agent is faced with the delayed reward problem,
and that it must take the distance to the two exits into consideration for choosing the most
attractive one.
The maze has a ground carpeted with a color image of 1280 × 1280 pixels that is a
montage of pictures from the COIL-100 database (Nene, Nayar, & Murase, 1996). The
agent does not have direct access to its (x, y) position in the maze. Rather, its sensors
take a picture of a surrounding portion of the ground. This portion is larger than the
blank areas, which makes the input space fully observable. Importantly, the glass walls are
transparent, so that the sensors also return the portions of the tapestry that are behind
them. Therefore, there is no way for the agent to directly locate the walls. It is obliged to
identify them as the regions of the maze in which an action does not change its location.
363

Jodogne & Piater

Figure 4: The deterministic image-to-action mapping that results from RLVC, sampled at
regularly-spaced points. It manages to choose the correct action at each location.

In this experiment, we have used color differential invariants as visual features (Gouet
& Boujemaa, 2001). The entire tapestry includes 2298 different visual features. RLVC
selected 200 features, corresponding to a ratio of 9% of the entire set of possible features.
The computation stopped after the generation of 84 image classifiers (i.e. when k reached
84), which took 35 minutes on a 2.4GHz Pentium IV using databases of 10,000 interactions.
205 visual classes were identified. This is a small number, compared to the number of
perceptual classes that would be generated by a discretization of the maze when the agent
knows its (x, y) position. For example, a reasonably sized 20×20 grid leads to 400 perceptual
classes.
Figure 4 shows the optimal, deterministic image-to-action mapping that results from
the last obtained image classifier Ck :
π ∗ (s) = argmax Q∗k (s, a) = Q0∗
k (Ck (s), a) .
a∈A

364

(15)

Closed-Loop Learning of Visual Control Policies

(a)

(b)

Figure 5: (a) The optimal value function, when the agent has direct access to its (x, y)
position in the maze and when the set of possible locations is discretized into a
50 × 50 grid. The brighter the location, the greater its value. (b) The final value
function obtained by RLVC.

Figure 5 compares the optimal value function of the discretized problem with the one obtained through RLVC. The similarity between the two pictures indicates the soundness of
our approach. Importantly, RLVC operates with neither pretreatment, nor human intervention. The agent is initially not aware of which visual features are important for its task.
Moreover, the interest of selecting descriptors is clear in this application: A direct, tabular
representation of the Q function considering all the Boolean combinations of features would
have 22298 × 4 cells.
The behavior of RLVC on real-word images has also been investigated. The navigation
rules were kept identical, but the tapestry was replaced by a panoramic photograph of
3041 × 384 pixels of a subway station, as depicted in Figure 6. RLVC took 101 iterations to
compute the mapping at the right of Figure 6. The computation time was 159 minutes on a
2.4GHz Pentium IV using databases of 10,000 interactions. 144 distinct visual features were
selected among a set of 3739 possible ones, generating a set of 149 visual classes. Here again,
the resulting classifier is fine enough to obtain a nearly optimal image-to-action mapping
for the task.

3.6 Related Work
RLVC can be thought of as performing adaptive discretization of the visual space on the
basis of the presence of visual features. Previous reinforcement learning algorithms that
exploit the presence of perceptual features in various contexts are now discussed.
365

Jodogne & Piater

(a)

(b)

Figure 6: (a) A navigation task with a real-world image, using the same conventions than
Figure 3. (b) The deterministic image-to-action mapping computed by RLVC.

366

Closed-Loop Learning of Visual Control Policies

3.6.1 Perceptual Aliasing
As explained above, the incremental selection of a set of informative visual features necessarily leads to temporary perceptual aliasing, which RLVC tries to remove. More generally,
perceptual aliasing occurs whenever an agent cannot always take the right on the basis of
its percepts.
Early work in reinforcement learning has tackled this general problem in two distinct
ways: Either the agent identifies and then avoids states where perceptual aliasing occurs
(as in the Lion algorithm, see Whitehead & Ballard, 1991), or it tries to build a short-term
memory that will allow it to remove the ambiguities on its percepts (as in the predictive
distinctions approach, see Chrisman, 1992). Very sketchily, these two algorithms detect the
presence of perceptual aliasing through an analysis of the sign of Q-learning updates. The
possibility of managing a short-term memory has led to the development of the Partially
Observable Markov Decision Processes (POMDP) theory (Kaelbling, Littman, & Cassandra,
1998), in which the current state is a random variable of the percepts.
Although these approaches are closely related to the perceptual aliasing RLVC temporarily introduces, they do not consider the exploitation of perceptual features. Indeed,
they tackle a structural problem in a given control task, and, as such, they assume that
perceptual aliasing cannot be removed. As a consequence, these approaches are orthogonal
to our research interest, since the ambiguities RLVC generates can be removed by further
refining the image classifier. In fact, the techniques above tackle a lack of information inherent to the used sensors, whereas our goal is to handle a surplus of information related
to the high redundancy of visual representations.
3.6.2 Adaptive Resolution in Finite Perceptual Spaces
RLVC performs an adaptive discretization of the perceptual space through an autonomous,
task-driven, purposive selection of visual features. Work in RL that incrementally partitions
a large (either discrete or continuous) perceptual space into a piecewise constant value
function is usually referred to as adaptive-resolution techniques. Ideally, regions of the
perceptual space with a high granularity should only be present where they are needed,
while a lower resolution should be used elsewhere. RLVC is such an adaptive-resolution
algorithm. We now review several adaptive-resolution methods that have been previously
proposed for finite perceptual spaces.
The idea of adaptive-resolution techniques in reinforcement learning goes back to the
G Algorithm (Chapman & Kaelbling, 1991), and has inspired the other approaches that
are discussed below. The G Algorithm considers perceptual spaces that are made up of
fixed-length binary numbers. It learns a decision tree that tests the presence of informative
bits in the percepts. This algorithm uses a Student’s t-test to determine if there is some bit
b in the percepts that is mapped to a given leaf, such that the state-action utilities of states
in which b is set are significantly different from the state-action utilities of states in which
b is unset. If such a bit is found, the corresponding leaf is split. The process is repeated
for each leaf. This method is able to learn compact representations, even though there is
a large number of irrelevant bits in the percepts. Unfortunately, when a region is split,
all the information associated with that region is lost, which makes for very slow learning.
367

Jodogne & Piater

Concretely, the G Algorithm can solve a task whose perceptual space contains 2100 distinct
percepts, which corresponds to the set of binary numbers with a length of 100 bits.
McCallum’s U-Tree algorithm builds upon this idea by combining a “selective attention”
mechanism inspired by the G Algorithm with a short-term memory that enables the agent
to deal with partially observable environments (McCallum, 1996). Therefore, McCallum’s
algorithms are a keystone in reinforcement learning, as they unify the G Algorithm (Chapman & Kaelbling, 1991) with Chrisman’s predictive distinctions (Chrisman, 1992).
U-Tree incrementally grows a decision tree through Kolmogorov-Smirnov tests. It has
succeeded at learning behaviors in a driving simulator. In this simulator, a percept consists
of a set of 8 discrete variables whose variation domains contain between 2 and 6 values,
leading to a perceptual space with 2, 592 possible percepts. Thus, the size of the perceptual
space is much smaller than a visual space. However, this task is difficult because the
“physical” state space is only partially observable through the perceptual space: The driving
task contains 21, 216 physical states, which means that several physical states requiring
different reactions can be mapped to the same percept through the sensors of the agent.
U-Tree resolves such ambiguities on the percepts by testing the presence of perceptual
features in the percepts that have been encountered previously in the history of the system.
To this end, U-Tree manages a short-term memory. In this paper, partially observable
environments are not considered. Our challenge is rather to deal with huge visual spaces,
without hand-tuned pre-processing, which is in itself a difficult, novel research direction.
3.6.3 Adaptive Resolution in Continuous Perceptual Spaces
It is important to notice that all the methods for adaptive resolution in large-scale, finite
perceptual spaces use a fixed set of perceptual features that is hard-wired. This has to be
distinguished from RLVC that samples visual features from a possibly infinite visual feature
space (e.g. the set of visual features is infinite), and that makes no prior assumptions
about the maximum number of useful features. From this point of view, RLVC is closer to
adaptive-resolution techniques for continuous perceptual spaces. Indeed, these techniques
dynamically select new relevant features from a whole continuum.
The first adaptive-resolution algorithm for continuous perceptual spaces is the Darling algorithm (Salganicoff, 1993). This algorithm, just like all the current algorithms
for continuous adaptive resolution, splits the perceptual space using thresholds. For this
purpose, Darling builds a hybrid decision tree that assigns a label to each point in the
perceptual space. Darling is a fully on-line and incremental algorithm that is equipped
with a forgetting mechanism that deletes outdated interactions. It is however limited to
binary reinforcement signals, and it only takes immediate reinforcements into account, so
that Darling is much closer to supervised learning than to reinforcement learning.
The Parti-Game algorithm (Moore & Atkeson, 1995) produces goal-directed behaviors in
continuous perceptual spaces. Parti-Game also splits regions where it deems it important,
using a game-theoretic approach. Moore and Atkeson show that Parti-Game can learn
competent behavior in a variety of continuous domains. Unfortunately, the approach is
currently limited to deterministic domains where the agent has a greedy controller and
where the goal state is known. Moreover, this algorithm searches for any solution to a
given task, and does not try to find the optimal one.
368

Closed-Loop Learning of Visual Control Policies

The Continuous U-Tree algorithm is an extension of U-Tree that is adapted to continuous perceptual spaces (Uther & Veloso, 1998). Just like Darling, Continuous U-Tree
incrementally builds a decision tree that splits the perceptual space into a finite set of hypercubes, by testing thresholds. Kolmogorov-Smirnov and sum-of-squared-errors are used
to determine when to split a node in the decision tree. Pyeatt and Howe (2001) analyze
the performance of several splitting criteria for a variation of Continuous U-Tree. They
conclude that Student’s t-test leads to the best performance, which motivates the use of
this statistical test in RLVC (cf. Section 3.4).
Munos and Moore (2002) have proposed Variable Resolution Grids. Their algorithm
assumes that the perceptual space is a compact subset of Euclidean space, and begins with
a coarse, grid-based discretization of the state space. In contrast with the other abstract
algorithms in this section, the value function and policy vary linearly within each region.
Munos and Moore use Kuhn triangulation as an efficient way to interpolate the value function within regions. The algorithm refines its approximation by refining cells according to
a splitting criterion. Munos and Moore explore several local heuristic measures of the importance of splitting a cell including the average of corner-value differences, the variance of
corner-value differences, and policy disagreement. They also explore global heuristic measures involving the influence and variance of the approximated system. Variable Resolution
Grids are probably the most advanced adaptive-resolution algorithm available so far.
3.6.4 Discussion
To summarize, several algorithms that are similar in spirit to RLVC have been proposed
over the years. Nevertheless, our work appears to be the first that can learn direct image-toaction mappings through reinforcement learning. Indeed, none of the reinforcement learning
methods above combines all the following desirable properties of RLVC: (1) The set of relevant perceptual features is not chosen a priori by hand, as the selection process is fully
automatic and does not require any human intervention; (2) visual perceptual spaces are explicitly considered through appearance-based visual features; and (3) the highly informative
perceptual features can be drawn out of a possibly infinite set.
These advantages of RLVC are essentially due to the fact that the candidate visual
features are not selected only because they are informative: They are also ranked according
to an information-theoretic measure inspired by decision tree induction (Breiman et al.,
1984). Such a ranking is required, as vision-for-action tasks induce a large number of visual
features (a typical image contains about a thousand of them). This kind of criterion that
ranks features, though already considered in Variable Resolution Grids (Munos & Moore,
2002), seems to be new in discrete perceptual spaces.
RLVC is defined independently of any fixed RL algorithm, which is similar in spirit to
Continuous U-Tree (Uther & Veloso, 1998), with the major exception that RLVC deals with
Boolean features, whereas Continuous U-Tree works in a continuous input space. Furthermore, the version of RLVC presented in this paper uses a variance-reduction criterion for
ranking the visual features. This criterion, though already considered in Variable Resolution
Grids, seems to be new in discrete perceptual spaces.
369

Jodogne & Piater

4. Compacting Visual Policies
As written in Section 1.4.2, this original version of RLVC is subject to overfitting (Jodogne &
Piater, 2005b). A simple heuristic to avoid the creation of too many visual classes is simply
to bound the number of visual classes that can be refined at each stage of the algorithm,
since splitting one visual class potentially has an impact on the Bellman residuals of all the
visual classes. In practice, we first try to split the classes that have the most samples before
considering the others, since there is more evidence of variance reduction for the first. In
our tests, we systematically apply this heuristics. However, it is often insufficient if taken
alone.
4.1 Equivalence Relations in Markov Decision Processes
Since we apply an embedded RL algorithm at each stage k of RLVC, properties like the
optimal value function Vk∗ (·), the optimal state-action value function Q∗k (·, ·) and the optimal
control policy πk∗ (·) are known for each mapped MDP Mk . Using those properties, it is easy
to define a whole range of equivalence relations between the visual classes. For instance,
given a threshold ε ∈ R+ , we list hereunder three possible equivalence relations for a pair
of visual classes (V, V 0 ):
Optimal Value Equivalence:
|Vk∗ (V ) − Vk∗ (V 0 )| ≤ ε.
Optimal Policy Equivalence:
|Vk∗ (V ) − Q∗k (V 0 , πk∗ (V ))| ≤ ε ∧
|Vk∗ (V 0 ) − Q∗k (V, πk∗ (V 0 ))| ≤ ε.
Optimal State-Action Value Equivalence:
(∀a ∈ A) |Q∗k (V, a) − Q∗k (V 0 , a)| ≤ ε.
We therefore propose to modify RLVC so that, periodically, visual classes that are
equivalent with respect to one of those criteria are merged together. We have experimentally
observed that the conjunction of the first two criteria tends to lead to the best performance.
This way, RLVC alternatively splits and merges visual classes. The compaction phase should
not be done too often, in order to allow exploration. To the best of our knowledge, this
possibility has not been investigated yet in the framework of adaptive-resolution methods
in reinforcement learning.
In the original version of RLVC, the visual classes correspond to the leaves of a decision
tree. When using decision trees, the aggregation of visual classes can only be achieved by
starting from the bottom of the tree and recursively collapsing leaves, until dissimilar leaves
are found. This operation is very close to post-pruning in the framework of decision trees
for machine learning (Breiman et al., 1984). In practice, this means that classes that have
similar properties, but that can only be reached from one another by making a number of
hops upwards then downwards, are extremely unlikely to be matched. This greatly reduces
the interest of exploiting the equivalence relations.
This drawback is due to the rather limited expressiveness of decision trees. In a decision
tree, each visual class corresponds to a conjunction of visual feature literals, which defines a
370

Closed-Loop Learning of Visual Control Policies

path from the root of the decision tree to one leaf. To take full advantage of the equivalence
relations, it is necessary to associate, to each visual class, an arbitrary union of conjunctions
of visual features. Indeed, when exploiting the equivalence relations, the visual classes are
the result of a sequence of conjunctions (splitting) and disjunctions (aggregation). Thus, a
more expressive data structure that would be able to represent general, arbitrary Boolean
combinations of visual features is required. Such a data structure is introduced in the next
section.
4.2 Using Binary Decision Diagrams
The problem of representing general Boolean functions has been extensively studied in the
field of computer-aided verification, since they can abstract the behavior of logical electronic
devices. In fact, a whole range of methods for representing the state space of richer and
richer domains have been developed over the last few years, such as Binary Decision Diagram
(BDD) (Bryant, 1992), Number and Queue Decision Diagrams (Boigelot, 1999), Upward
Closed Sets (Delzanno & Raskin, 2000) and Real Vector Automata (Boigelot, Jodogne, &
Wolper, 2005).
In our framework, BDD is a particularly well-suited tool. It is a acyclic graph-based
symbolic representation for encoding arbitrary Boolean functions, and has had much success in the field of computer-aided verification (Bryant, 1992). A BDD is unique when the
ordering of its variables is fixed, but different variable orderings can lead to different sizes
of the BDD, since some variables can be discarded by the reordering process. Although the
problem of finding the optimal variable ordering is coNP-complete (Bryant, 1986), automatic heuristics can in practice find orderings that are close to optimal. This is interesting
in our case, since reducing the size of the BDD potentially discards irrelevant variables,
which correspond to removing useless visual features.
4.3 Modifications to RLVC
To summarize, this extension to RLVC does not use decision trees anymore, but assigns
one BDD to each visual class. Two modifications are to be applied to Algorithm 1:
1. The operation of refining, with a visual feature f , a visual class V that is labeled by
the BDD B(V ), consists in replacing V by two new visual classes V1 and V2 such that
B(V1 ) = B(V ) ∧ f and B(V2 ) = B(V ) ∧ ¬f .
2. Given an equivalence relation, the post-process(Ck ) operation consists in merging
the equivalent visual classes. To merge a pair of visual classes (V1 , V2 ), V1 and V2 are
deleted, and a new visual class V such that B(V ) = B(V1 ) ∨ B(V2 ) is added. Every
time a merging operation takes place, it is advised to carry on variable reordering, to
minimize the memory requirements.
4.4 Experiments
We have applied the modified version of RLVC to another simulated navigation task. In
this task, the agent moves between 11 spots of the campus of the University of Liège (cf.
Figure 7). Every time the agent is at one of the 11 locations, its body can aim at four possible
371

Jodogne & Piater

N

(c) Google Map

Figure 7: The Montefiore campus at Liège. Red spots corresponds to the places between
which the agent moves. The agent can only follow the links between the different
spots. Its goal is to enter the Montefiore Institute, that is labeled by a red cross,
where it gets a reward of +100.

orientations: North, South, West, East. The state space is therefore of size 11 × 4 = 44.
The agent has three possible actions: Turn left, turn right, go forward. Its goal is to enter
a specific building, where it will obtain a reward of +100. Turning left or right induces a
penalty of −5, and moving forward, a penalty of −10. The discount factor γ is set to 0.8.
The optimal control policy is not unique: One of them is depicted on Figure 8.
The agent does not have direct access to its position and its orientation. Rather, it
only perceives a picture of the area that is in front of it (cf. Figure 9). Thus, the agent
has to connect an input image to the appropriate reaction without explicitly knowing its
geographical localization. For each possible location and each possible viewing direction,
a database of 24 images of size 1024 × 768 with significant viewpoint changes has been
collected. Those 44 databases have been randomly divided into a learning set of 18 images
and a test set of 6 images. In our experimental setup, both versions of RLVC learn an
image-to-action mapping using interactions that only contain images from the learning set.
Images from the test set are used to assess the accuracy of the learned visual control policies.
The SIFT keypoints have been used as visual features (Lowe, 2004). Thresholding on a
Mahalanobis distance gave rise to a set of 13,367 distinct features. Both versions of RLVC
have been applied on a static database of 10,000 interactions that has been collected using
a fully randomized exploration policy. The same database is used throughout the entire
algorithm, and this database only contains images that belong to the learning set.
The results of the basic version of RLVC and of the version that is extended by BDDs
are reported in Figures 10 and 11. The original version of RLVC has identified 281 visual
classes by selecting 264 SIFT features. The error rate on the computed visual policy (i.e. the
proportion of sub-optimal decisions when the agent is presented all the possible stimuli) was
372

Closed-Loop Learning of Visual Control Policies

(c) Google Map

Figure 8: One of the optimal, deterministic control policies for the Montefiore navigation
task. For each state, we have indicated the optimal action (the letter “F” stands
for “move forward”, “R” for “turn right” and “L” for “turn left”). This policy
has been obtained by applying a standard RL algorithm to the scenario in which
the agent has direct access to the (p, d) information.

0.1% on the learning set and 8% when the images of the test set are used, with respect to
the optimal policy when the agent has direct access to its position and viewing direction.
The modified version RLVC was then applied, with one compacting stage every 10 steps.
The results are clearly superior. There is no error on the learning set anymore, while the
error rate on the test set is 4.5%. The number of selected features is reduced to 171.
Furthermore, the resulting number of visual classes becomes 59, instead of 281. Thus, there
is a large improvement in the generalization abilities, as well as a reduction of the number
of visual classes and selected features. Interestingly enough, the number of visual classes
(59) is very close to the number of physical states (44), which tends to indicate that the
algorithm starts to learn a physical interpretation for its percepts.
To summarize, compacting visual policies is probably a required step to deal with realistic visual tasks, if an iterative splitting process is applied. The price to pay is of course a
373

Jodogne & Piater

(c) Google Map

Figure 9: The percepts of the agent. Four possible different percepts are shown, that correspond to the location and viewing direction marked in yellow on the top image.

374

Closed-Loop Learning of Visual Control Policies

60

60
RLVC
RLVC + BDD
50

40

40

30

0

20

40

60

80
Iterations (k)

100

120

140

Error rate (%)

50

30

20

20

10

10

0
160

0

20

40

60

80
Iterations (k)

100

120

140

Error rate (%)

RLVC
RLVC + BDD

0
160

Figure 10: Comparison of the error rates between the basic and extended versions of RLVC.
The error of the computed policy as a function of the step counter k on the images
of the learning set (resp. test set) is reported on the left of the figure (resp. on
the right).

higher computational cost. Future work will focus on a theoretical justification of the used
equivalence relations. This implies bridging the gap with the theory of MDP minimization (Givan, Dean, & Greig, 2003).

5. Learning Spatial Relationships
As motivated in the Introduction (Section 1.4.3), we propose to extend RLVC by constructing a hierarchy of spatial arrangements of individual point features (Jodogne, Scalzo, &
Piater, 2005). The idea of learning models of spatial combinations of features takes its
roots in the seminal paper by Fischler and Elschlager (1973) about pictorial structures,
which are collections of rigid parts arranged in deformable configurations. This idea has
become increasingly popular in the computer vision community over the 90’s, and has led to
a large literature about the modeling and the detection of objects (Amit & Kong, 1996; Burl
& Perona, 1996; Forsyth, Haddon, & Ioffe, 1999). Crandall and Huttenlocher (2006) provide pointers to recent resources. Among such recent techniques, Scalzo and Piater (2006)
propose to build a probabilistic hierarchy of visual features that is represented through an
acyclic graph. They detect the presence of such a model through Nonparametric Belief
Propagation (Sudderth, Ihler, Freeman, & Willsky, 2003). Other graphical models have
been proposed for representing articulated structures, such as pictorial structures (Felzenszwalb & Huttenlocher, 2005; Kumar, Torr, & Zisserman, 2004). Similarly, the constellation
model represents objects by parts, each modeled in terms of both shape and appearance by
Gaussian probability density functions (Perona, Fergus, & Zisserman, 2003).
Our work contrasts with these approaches in that the generation of so-called composite
features is driven by the task to be solved. This should be distinguished from the techniques
for unsupervised learning of composite features, since the additional information that is
375

Jodogne & Piater

300

300
RLVC
RLVC + BDD
250

200

200

150

0

20

40

60

80
Iterations (k)

100

120

140

Number of classes

250

150

100

100

50

50

0
160

0

20

40

60

80
Iterations (k)

100

120

140

Number of selected features

RLVC
RLVC + BDD

0
160

Figure 11: Comparison of the number of generated classes and selected visual features between the basic and extended versions of RLVC. The number of visual classes
(resp. selected features) as a function of the step counter k is plotted on the left
of the figure (resp. on the right).

embedded inside the reinforcement signal drives the generation of composite features by
focusing the exploration on task-relevant spatial arrangements.
In this extension of RLVC, a hierarchy of visual features is built simultaneously with
the image classifier. As soon as no sufficiently informative visual feature can be extracted,
the algorithm tries to combine two visual features in order to construct a higher level of
abstraction, which is hopefully more distinctive and more robust to noise. This extension
to RLVC assumes the co-existence of two different kinds of visual features:
Primitive Features: They correspond to the individual point features, i.e. to the localappearance descriptors (cf. Section 1.3).
Composite Features: They consist of spatial combinations of lower-level visual features.
There is no a priori bound on the maximal height of the hierarchy. Therefore, a
composite feature can be potentially combined with a primitive feature, or with a
composite feature.
5.1 Detection of Visual Features
A natural way to represent such a hierarchy is to use a directed acyclic graph G = (V, E), in
which each vertex v ∈ V corresponds to a visual feature, and in which each edge (v, v 0 ) ∈ E
models the fact that v 0 is a part of the composite feature v. Thus, G must be binary,
i.e. any vertex should have either no child, or exactly two children. The set VP of the leaves
of G corresponds to the set of primitive features, while the set VC of its internal vertexes
represents the set of composite features.
Each leaf vertex vP ∈ VP is annotated with a local descriptor D(vP ). Similarly, each
internal vertex vC ∈ VC is annotated with constraints on the relative position between its
parts. In this work, we consider only constraints on the distances between the constituent
376

Closed-Loop Learning of Visual Control Policies

visual features of the composite features, and we assume that they should be distributed
according to a Gaussian law G(µ, σ) of mean µ and standard deviation σ. Evidently, richer
constraints could be used, such as taking the relative orientation or the scaling factor between the constituent features into consideration, which would require the use of multivariate Gaussians.
More precisely, let vC be a composite feature, the parts of which are v1 and v2 . In order
to trigger the detection of vC in an image s, there should be an occurrence of v1 and an
occurrence of v2 in s such that their relative Euclidean distance has a sufficient likelihood ν
of being generated by a Gaussian of mean µ(vC ) and standard deviation σ(vC ). To ensure
symmetry, the location of the composite feature is then taken as the midpoint between the
locations of v1 and v2 .
The occurrences of a visual feature v in a percept s can be found using the recursive
Algorithm 5. Of course, at steps 6 and 7 of Algorithm 4, the test “does st exhibit v?” can
be rewritten as a function of Algorithm 5, by checking if occurrences(v, st ) 6= ∅.
5.2 Generation of Composite Features
The cornerstone of this extension to RLVC is the way of generating composite features.
The general idea behind our algorithm is to accumulate statistical evidence from the relative
positions of the detected visual features in order to find “conspicuous coincidences” of visual
features. This is done by providing a more evolved implementation of generator(s1 , . . . , sn )
than the one of Algorithm 3.
5.2.1 Identifying Spatial Relations
We first extract the set F of all the (primitive or composite) features that occur within the
set of provided images {s1 , . . . , sn }:
F = {v ∈ V | (∃i) si exhibits v} .

(16)

We identify the pairs of visual features the occurrences of which are highly correlated within
the set of provided images {s1 , . . . , sn }. This simply amounts to counting the number of
co-occurrences for each pair of features in F , then only keeping the pairs the corresponding
count of which exceeds a fixed threshold.
Let now v1 and v2 be two features that are highly correlated. A search for a meaningful
spatial relationship between v1 and v2 is then carried out in the images {s1 , . . . , sn } that
contain occurrences of both v1 and v2 . For each such co-occurrence, we accumulate in a set
Λ the distances between the corresponding occurrences of v1 and v2 . Finally, a clustering
algorithm is applied on the distribution Λ in order to detect typical distances between v1
and v2 . For the purpose of our experiments, we have used hierarchical clustering (Jain,
Murty, & Flynn, 1999). For each cluster, a Gaussian is fitted by estimating a mean value
µ and a standard deviation σ. Finally, a new composite feature vC is introduced in the
feature hierarchy, that has v1 and v2 as parts and such that µ(vC ) = µ and σ(vC ) = σ.
In summary, in Algorithm 1, we replace the call to Algorithm 3 by a call to Algorithm 6.
377

Jodogne & Piater

Algorithm 5 — Detecting Composite Features
1: occurrences(v, s) :–
2:
if v is primitive then
3:
return {(x, y) | (x, y) is an interest point of s, the local descriptor of which corresponds to D(v)}
4:
else
5:
O ← {}
6:
O1 ← occurrences(subfeature1 (v), s)
7:
O2 ← occurrences(subfeature2 (v), s)
8:
for all p
(x1 , y1 ) ∈ O1 and (x2 , y2 ) ∈ O2 do
9:
d ← (x2 − x1 )2 + (y2 − y1 )2
10:
if G(d − µ(v), σ(v)) ≥ ν then
11:
O ← O ∪ {((x1 + x2 )/2, (y1 + y2 )/2)}
12:
end if
13:
end for
14:
return O
15:
end if

Algorithm 6 — Generation of Composite Features
1: generator({s1 , . . . , sn }) :–
2:
F = {v ∈ V | (∃i) si exhibits v}
3:
F 0 = {}
4:
for all (v1 , v2 ) ∈ F × F do
5:
if enough co-occurrences of v1 and v2 in {s1 , . . . , sn } then
6:
Λ ← {}
7:
for all i ∈ {1, . . . , n} do
8:
for all occurrences (x1 , y1 ) of v1 in si do
9:
for all occurrences
(x2 , y2 ) of v2 in si do
p
10:
Λ ← Λ ∪ { (x2 − x1 )2 + (y2 − y1 )2 }
11:
end for
12:
end for
13:
end for
14:
Apply a clustering algorithm on Λ
15:
for each cluster C = {d1 , . . . dm } in Λ do
16:
µ = mean(C)
17:
σ = stddev(C)
18:
Add to F 0 a composite feature vC composed of v1 and v2 , annotated with a
mean µ and a standard deviation σ
19:
end for
20:
end if
21:
end for
22:
return F 0

378

Closed-Loop Learning of Visual Control Policies

H(p)
0.4

N

u
0.2

mg
−1

−.5

.5

1

p

−0.2

Figure 12: The “Car on the Hill” control problem.

5.2.2 Feature Validation
Algorithm 6 can generate several composite features for a given visual class Vk,i . However,
at the end of Algorithm 4, at most one generated composite feature is to be kept. It is
important to notice that the performance of the clustering method is not critical for our
purpose. Indeed, irrelevant spatial combinations are automatically discarded, thanks to the
variance-reduction criterion of the feature selection component. In fact, the reinforcement
signal helps to direct the search for a good feature, which is an advantage over unsupervised
methods of building feature hierarchies.
5.3 Experiments
We demonstrate the efficacy of our algorithms on a version of the classical “Car on the Hill”
control problem (Moore & Atkeson, 1995), where the position and velocity information is
presented to the agent visually.
In this episodic task, a car (modeled by a mass point) is riding without friction on a
hill, the shape of which is defined by the function:

H(p) =

p2 p
+p
if p < 0,
2
p/ 1 + 5p if p ≥ 0.

The goal of the agent is to reach as fast as possible the top of the hill, i.e. a location such
that p ≥ 1. At the top of the hill, the agent obtains a reward of 100. The car can thrust left
or right with an acceleration of ±4 Newtons. However, because of gravity, this acceleration
is insufficient for the agent to reach the top of the hill by always thrusting toward the right.
Rather, the agent has to go left for while, hence acquiring potential energy by going up the
left side of the hill, before thrusting rightward. There are two more constraints: The agent
is not allowed to reach locations such that p < −1, and a velocity greater than 3 in absolute
value leads to the destruction of the car.
379

Jodogne & Piater

5.3.1 Formal Definition of the Task
Formally, the set of possible actions is A = {−4, 4}, while the state space is S = {(p, s) |
|p| ≤ 1 ∧ |s| ≤ 3}. The system has the following continuous-time dynamics:
ṗ = s
ṡ =

M

a
p

1 + H 0 (p)2

−

gH 0 (p)
,
1 + H 0 (p)2

where a ∈ A is the thrust acceleration, H 0 (p) is the first derivative of H(p), M = 1 is the
mass of the car, and g = 9.81 is the acceleration due to gravity. These continuous-time
dynamics are approximated by the following discrete-time state update rule:
st+1 = st + hṗt + h2 ṡt /2
st+1 = ṗt + hṡt ,
where h = 0.1 is the integration time step. The reinforcement signal is defined through this
expression:

100 if st+1 ≥ 1 ∧ |st+1 | ≤ 3,
R((st , st ), a) =
0
otherwise.
In our setup, the discount factor γ was set to 0.75.
This definition is actually a mix of two coexistent formulations of the “Car on the Hill”
task (Ernst, Geurts, & Wehenkel, 2003; Moore & Atkeson, 1995). The major differences
with the initial formulation of the problem (Moore & Atkeson, 1995) is that the set of
possible actions is discrete, and that the goal is at the top of the hill (rather than on a given
area of the hill), just like in the definition from Ernst et al. (2003). To ensure the existence
of an interesting solution, the velocity is required to remain less than 3 (instead of 2), and
the integration time step is set to h = 0.1 (instead of 0.01).
5.3.2 Inputs of the Agent
In previous work (Moore & Atkeson, 1995; Ernst et al., 2003), the agent was always assumed
to have direct access to a numerical measure of its position and velocity. The only exception
is Gordon’s work in which a visual, low-resolution representation of the global scene is given
to the agent (Gordon, 1995). In our experimental setup, the agent is provided with two
cameras, one looking at the ground underneath, the second at a velocity gauge. This way,
the agent cannot directly know its current position and velocity, but has to suitably interpret
its visual inputs to derive them.
Some examples of the pictures the sensors can return are presented in Figure 13. The
ground is carpeted with a color image of 1280 × 128 pixels that is a montage of pictures
from the COIL-100 database (Nene et al., 1996). It is very important to notice that using
individual point features is insufficient for solving this task, since the set of features in the
pictures of the velocity gauge are always the same. To know its velocity, the agent has to
generate composite features sensitive to the distance of the primitive features on the cursor
with respect to the primitive features on the digits.
380

Closed-Loop Learning of Visual Control Policies

(a)

(b)
Figure 13: (a) Visual percepts corresponding to pictures of the velocity gauge when s = −3,
s = 0.5 and s = 1.5. (b) Visual percepts returned by the position sensor. The
region framed with a white rectangle corresponds to the portion of the ground
that is returned by the sensor when p = 0.1. This portion slides back and forth
as the agent moves.

5.3.3 Results
In this experimental setup, we used color differential invariants (Gouet & Boujemaa, 2001)
as primitive features. Among all the possible visual inputs (both for the position and the
velocity sensors), there were 88 different primitive features. The entire image of the ground
includes 142 interest points, whereas the images of the velocity gauge include about 20
interest points.
The output of RLVC is a decision tree that defines 157 visual classes. Each internal
node of this tree tests the presence of one visual feature, taken from a set of 91 distinct,
highly discriminant features selected by RLVC. Among the 91 selected visual features, there
were 56 primitive and 26 composite features. Two examples of composites features that
were selected by RLVC are depicted in Figure 15. The computation stopped after k = 38
refinement steps in Algorithm 1.
To show the efficacy of our method, we compare its performance with the scenario in
which the agent has a direct perception of its current (p, s) state. In the latter scenario, the
state space was discretized in a grid of 13 × 13 cells. The number 13 was chosen since it
approximately corresponds to the square root of 157, the number of visual classes that were
produced by RLVC. This way, RL is provided an equivalent number of perceptual classes in
the two scenarios. Figure 14 compares the optimal value function of the direct-perception
381

Jodogne & Piater

Velocity

3

0

−3
−1

0

Position

1

(a)

Velocity

3

0

−3
−1

0

Position

1

(b)

Figure 14: (a) The optimal value function, when the agent has a direct access to its current
(p, s) state, and when the input space is discretized in a 13 × 13 grid. The
brighter the location, the greater its value. (b) The value function obtained by
RLVC.

382

Closed-Loop Learning of Visual Control Policies

Figure 15: Two composite features that were generated, in yellow. The primitive features
of which they are composed are marked in yellow. The first feature triggers for
velocities around 0, whereas the second triggers around −2.

problem with the one obtained through RLVC. Here also, the two pictures are very similar,
which indicates the soundness of our approach.
We have also evaluated the performance of the optimal image-to-action mapping
π ∗ = argmax Q∗ ((p, s), a)

(17)

a∈A

obtained through RLVC. For this purpose, the agent was placed randomly on the hill, with
an initial velocity of 0. Then, it used the mapping π ∗ to choose an action, until it reached
a final state. A set of 10,000 such trials were carried out at each step k of Algorithm 1.
Figure 16 compares the proportion of trials that missed the goal (either because of leaving
the hill on the left, or because of acquiring a too high velocity) in RLVC and in the directperception problem. When k became greater than 27, the proportion of missed trials was
always smaller in RLVC than in the direct-perception problem. This advantage in favor
of RLVC is due to the adaptive nature of its discretization. Figure 17 compares the mean
lengths of the successful trials. The mean length of RLVC trials clearly converges to that
of the direct-perception trials, while staying slightly larger.
To conclude, RLVC achieves a performance close to the direct-perception scenario. However, the mapping built by RLVC directly links visual percepts to the appropriate actions,
without considering explicitly the physical variables.

6. Summary
This paper introduces Reinforcement Learning of Visual Classes (RLVC). RLVC is designed
to learn mappings that directly connect visual stimuli to output actions that are optimal
for the surrounding environment. The framework of RLVC is general, in the sense that it
can be applied to any problem that can be formulated as a Markov decision problem.
The learning process behind our algorithms is closed-loop and flexible. The agent
takes lessons from its interactions with the environment, according to the purposive vision
paradigm. RLVC focuses the attention of an embedded reinforcement learning algorithm
on highly informative and robust parts of the inputs by testing the presence or absence of
local descriptors at the interest points of the input images. The relevant visual features
383

Jodogne & Piater

25
RLVC
Direct perception (13x13 grid)

Missed goal (%)

20

15

10

5

0

0

5

10

15

20

25

30

35

40

Iterations (k)

Figure 16: Evolution of the number of times the goal was missed over the iterations of
RLVC.

19

RLVC
Direct perception (13x13 grid)

18

Mean length of an interaction

17

16

15

14

13

12

0

5

10

15

20

25

30

35

40

Iterations (k)

Figure 17: Evolution of the mean lengths of the successful trials over the iterations of RLVC.

384

Closed-Loop Learning of Visual Control Policies

are incrementally selected in a sequence of attempts to remove perceptual aliasing: The
discretization process targets zero Bellman residuals and is inspired from supervised learning algorithms for building decision trees. Our algorithms are defined independently of any
interest point detector (Schmid et al., 2000) and of any local description technique (Mikolajczyk & Schmid, 2003). The user may choose these two components as he sees fit.
Techniques for fighting overfitting in RLVC have also been proposed. The idea is to
aggregate visual classes that share similar properties with respect to the theory of Dynamic
Programming. Interestingly, this process enhances the generalization abilities of the learned
image-to-action mapping, and reduces the number of visual classes that are built.
Finally, an extension of RLVC is introduced that allows the closed-loop, interactive
and purposive learning of a hierarchy of geometrical combinations of visual features. This
is to contrast to most of the prior work on the topic, that uses either a supervised or
unsupervised framework (Piater, 2001; Fergus, Perona, & Zisserman, 2003; Bouchard &
Triggs, 2005; Scalzo & Piater, 2006). Besides the novelty of the approach, we have shown
its practical value in visual control tasks in which the information provided by the individual
point features alone is insufficient for solving the task. Indeed, spatial combinations of visual
features are more informative and more robust to noise.

7. Future Work
The area of applications of RLVC is wide, since nowadays robotic agents are often equipped
with CCD sensors. Future research includes the demonstration of the applicability of our
algorithms in a reactive robotic application, such as grasping objects by combining visual
and haptic feedback (Coelho, Piater, & Grupen, 2001). This necessitates the extension of
our techniques to continuous action spaces, for which no fully satisfactory solutions exist to
date. RLVC could also be potentially be applied to Human-Computer Interaction, as the
actions need not be physical actions.
The closed-loop learning of a hierarchy of visual feature also raises interesting research
directions. For example, the combination of RLVC with techniques for disambiguating
between aliased percepts using a short-term memory (McCallum, 1996) could solve visual
tasks in which the percepts of the agent alone do not provide enough information for solving
the task. Likewise, the unsupervised learning of other kinds of geometrical models (Felzenszwalb & Huttenlocher, 2005) could potentially be embedded in RLVC. On the other hand,
spatial relationships do not currently take into consideration the relative angles between the
parts of a composite feature. Doing so would further increase the discriminative power of the
composite features, but requires non-trivial techniques for clustering in circular domains.

Acknowledgments
The authors thank the associate editor Thorsten Joachims and the three anonymous reviewers for their many suggestions for improving the quality of the manuscript. Sébastien
Jodogne gratefully acknowledge the financial support of the Belgian National Fund for
Scientific Research (FNRS).
385

Jodogne & Piater

References
Aloimonos, Y. (1990). Purposive and qualitative active vision. In Proc. of the 10th International Conference on Pattern Recognition, pp. 436–460.
Amit, Y., & Kong, A. (1996). Graphical templates for model registration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18 (3), 225–236.
Asada, M., Noda, S., Tawaratsumida, S., & Hosoda, K. (1994). Vision-based behavior acquisition for a shooting robot by using a reinforcement learning. In Proc. of IAPR/IEEE
Workshop on Visual Behaviors, pp. 112–118.
Bagnell, J., & Schneider, J. (2001). Autonomous helicopter control using reinforcement
learning policy search methods. In Proc. of the International Conference on Robotics
and Automation. IEEE.
Barto, A., Sutton, R., & Anderson, C. (1983). Neuronlike adaptive elements that can
solve difficult learning control problems. IEEE Transactions on Systems, Man and
Cybernetics, 13 (5), 835–846.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D., & Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific.
Boigelot, B. (1999). Symbolic Methods for Exploring Infinite State Spaces. Ph.D. thesis,
University of Liège, Liège (Belgium).
Boigelot, B., Jodogne, S., & Wolper, P. (2005). An effective decision procedure for linear
arithmetic with integer and real variables. ACM Transactions on Computational Logic
(TOCL), 6 (3), 614–633.
Bouchard, G., & Triggs, B. (2005). Hierarchical part-based visual object categorization. In
IEEE Conference on Computer Vision and Pattern Recognition, Vol. 1, pp. 710–715,
San Diego (CA, USA).
Breiman, L., Friedman, J., & Stone, C. (1984).
Wadsworth International Group.

Classification and Regression Trees.

Bryant, R. (1986). Graph-based algorithms for boolean function manipulation. IEEE Transactions in Computers, 8 (35), 677–691.
Bryant, R. (1992). Symbolic boolean manipulation with ordered binary decision diagrams.
ACM Computing Surveys, 24 (3), 293–318.
Burl, M., & Perona, P. (1996). Recognition of planar object classes. In Proc. of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 223–230, San Francisco
(CA, USA).
Chapman, D., & Kaelbling, L. (1991). Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. In Proc. of the 12th International
Joint Conference on Artificial Intelligence (IJCAI), pp. 726–731, Sydney.
Chrisman, L. (1992). Reinforcement learning with perceptual aliasing: The perceptual
distinctions approach. In National Conference on Artificial Intelligence, pp. 183–188.
386

Closed-Loop Learning of Visual Control Policies

Coelho, J., Piater, J., & Grupen, R. (2001). Developing haptic and visual perceptual categories for reaching and grasping with a humanoid robot. Robotics and Autonomous
Systems, special issue on Humanoid Robots, 37 (2–3), 195–218.
Crandall, D., & Huttenlocher, D. (2006). Weakly supervised learning of part-based spatial
models for visual object recognition. In Proc. of the 9th European Conference on
Computer Vision.
Delzanno, G., & Raskin, J.-F. (2000). Symbolic representation of upward closed sets. In
Tools and Algorithms for the Construction and Analysis of Systems, Lecture Notes in
Computer Science, pp. 426–440, Berlin (Germany).
Derman, C. (1970). Finite State Markovian Decision Processes. Academic Press, New York.
Ernst, D., Geurts, P., & Wehenkel, L. (2003). Iteratively extending time horizon reinforcement learning. In Proc. of the 14th European Conference on Machine Learning, pp.
96–107, Dubrovnik (Croatia).
Ernst, D., Geurts, P., & Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6, 503–556.
Felzenszwalb, P., & Huttenlocher, D. (2005). Pictorial structures for object recognition.
International Journal of Computer Vision, 61 (1), 55–79.
Fergus, R., Perona, P., & Zisserman, A. (2003). Object class recognition by unsupervised
scale-invariant learning. In IEEE Conference on Computer Vision and Pattern Recognition, Vol. 2, pp. 264–271, Madison (WI, USA).
Fischler, M., & Elschlager, R. (1973). The representation and matching of pictorial structures. IEEE Transactions on Computers, 22 (1), 67–92.
Forsyth, D., Haddon, J., & Ioffe, S. (1999). Finding objects by grouping primitives. In Shape,
Contour and Grouping in Computer Vision, pp. 302–318, London (UK). SpringerVerlag.
Gaskett, C., Fletcher, L., & Zelinsky, A. (2000). Reinforcement learning for visual servoing
of a mobile robot. In Proc. of the Australian Conference on Robotics and Automation,
Melbourne (Australia).
Gibson, E., & Spelke, E. (1983). The development of perception. In Flavell, J. H., & Markman, E. M. (Eds.), Handbook of Child Psychology Vol. III: Cognitive Development
(4th edition)., chap. 1, pp. 2–76. Wiley.
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions and model minimization in
markov decision processes. Artificial Intelligence, 147 (1–2), 163–223.
Gordon, G. (1995). Stable function approximation in dynamic programming. In Proc. of
the International Conference on Machine Learning, pp. 261–268.
Gouet, V., & Boujemaa, N. (2001). Object-based queries using color points of interest. In
IEEE Workshop on Content-Based Access of Image and Video Libraries, pp. 30–36,
Kauai (HI, USA).
Howard, R. (1960). Dynamic Programming and Markov Processes. Technology Press and
Wiley, Cambridge (MA) and New York.
387

Jodogne & Piater

Huber, M., & Grupen, R. (1998). A control structure for learning locomotion gaits. In 7th
Int. Symposium on Robotics and Applications, Anchorage (AK, USA). TSI Press.
Iida, M., Sugisaka, M., & Shibata, K. (2002). Direct-vision-based reinforcement learning
to a real mobile robot. In Proc. of International Conference of Neural Information
Processing Systems, Vol. 5, pp. 2556–2560.
Jaakkola, T., Jordan, M., & Singh, S. (1994). Convergence of stochastic iterative dynamic
programming algorithms. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural Information Processing Systems, Vol. 6, pp. 703–710. Morgan Kaufmann Publishers.
Jain, A. K., Murty, M. N., & Flynn, P. J. (1999). Data clustering: A review. ACM Computing
Surveys, 31 (3), 264–323.
Jodogne, S., & Piater, J. (2005a). Interactive learning of mappings from visual percepts
to actions. In De Raedt, L., & Wrobel, S. (Eds.), Proc. of the 22nd International
Conference on Machine Learning (ICML), pp. 393–400, Bonn (Germany). ACM.
Jodogne, S., & Piater, J. (2005b). Learning, then compacting visual policies (extended abstract). In Proc. of the 7th European Workshop on Reinforcement Learning (EWRL),
pp. 8–10, Napoli (Italy).
Jodogne, S., Scalzo, F., & Piater, J. (2005). Task-driven learning of spatial combinations
of visual features. In Proc. of the IEEE Workshop on Learning in Computer Vision
and Pattern Recognition, San Diego (CA, USA). IEEE.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning and acting in partially
observable stochastic domains. Artificial Intelligence, 101 (1-2), 99–134.
Kaelbling, L., Littman, M., & Moore, A. (1996). Reinforcement learning: A survey. Journal
of Artificial Intelligence Research, 4, 237–285.
Kimura, H., Yamashita, T., & Kobayashi, S. (2001). Reinforcement learning of walking
behavior for a four-legged robot. In Proc. of the 40th IEEE Conference on Decision
and Control, Orlando (FL, USA).
Kohl, N., & Stone, P. (2004). Policy gradient reinforcement learning for fast quadrupedal
locomotion. In Proc. of the IEEE International Conference on Robotics and Automation, pp. 2619–2624, New Orleans.
Kumar, M., Torr, P., & Zisserman, A. (2004). Extending pictorial structures for object
recognition. In Proc. of the British Machine Vision Conference.
Kwok, C., & Fox, D. (2004). Reinforcement learning for sensing strategies. In Proc. of the
IEEE International Conference on Intelligent Robots and Systems.
Lagoudakis, M., & Parr, R. (2003). Least-squares policy iteration. Journal of Machine
Learning Research, 4, 1107–1149.
Lowe, D. (2004). Distinctive image features from scale-invariant keypoints. International
Journal of Computer Vision, 60 (2), 91–110.
Martı́nez-Marı́n, T., & Duckett, T. (2005). Fast reinforcement learning for vision-guided
mobile robots. In Proc. of the IEEE International Conference on Robotics and Automation, pp. 18–22, Barcelona (Spain).
388

Closed-Loop Learning of Visual Control Policies

McCallum, R. (1996). Reinforcement Learning with Selective Perception and Hidden State.
Ph.D. thesis, University of Rochester, New York.
Michels, J., Saxena, A., & Ng, A. (2005). High speed obstacle avoidance using monocular
vision and reinforcement learning. In Proc. of the 22nd International Conference in
Machine Learning, pp. 593–600, Bonn (Germany).
Mikolajczyk, K., & Schmid, C. (2003). A performance evaluation of local descriptors. In
Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, Vol. 2,
pp. 257–263, Madison (WI, USA).
Moore, A., & Atkeson, C. (1995). The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. Machine Learning, 21.
Munos, R., & Moore, A. (2002). Variable resolution discretization in optimal control. Machine Learning, 49, 291–323.
Nene, S., Nayar, S., & Murase, H. (1996). Columbia object image library (COIL-100). Tech.
rep. CUCS-006-96, Columbia University, New York.
Ng, A., Coates, A., Diel, M., Ganapathi, V., Schulte, J., Tse, B., Berger, B., & Liang, E.
(2004). Inverted autonomous helicopter flight via reinforcement learning. In Proc. of
the International Symposium on Experimental Robotics.
Paletta, L., Fritz, G., & Seifert, C. (2005). Q-learning of sequential attention for visual object
recognition from informative local descriptors.. In Proc. of the 22nd International
Conference on Machine Learning (ICML), pp. 649–656, Bonn (Germany).
Paletta, L., & Pinz, A. (2000). Active object recognition by view integration and reinforcement learning. Robotics and Autonomous Systems, 31 (1–2), 71–86.
Peng, J., & Bhanu, B. (1998). Closed-loop object recognition using reinforcement learning.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 20 (2), 139–154.
Perona, P., Fergus, R., & Zisserman, A. (2003). Object class recognition by unsupervised
scale-invariant learning. In Conference on Computer Vision and Pattern Recognition,
Vol. 2, p. 264.
Piater, J. (2001). Visual Feature Learning. Ph.D. thesis, University of Massachusetts,
Computer Science Department, Amherst (MA, USA).
Puterman, M., & Shin, M. (1978). Modified policy iteration algorithms for discounted
Markov decision problems. Management Science, 24, 1127–1137.
Pyeatt, L., & Howe, A. (2001). Decision tree function approximation in reinforcement
learning. In Proc. of the Third International Symposium on Adaptive Systems, pp.
70–77, Havana, Cuba.
Quinlan, J. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers
Inc., San Francisco (CA, USA).
Randløv, J., & Alstrøm, P. (1998). Learning to drive a bicycle using reinforcement learning
and shaping. In Proc. of the 15th International Conference on Machine Learning, pp.
463–471, Madison (WI, USA). Morgan Kaufmann.
389

Jodogne & Piater

Rummery, G., & Niranjan, M. (1994). On-line Q-learning using connectionist sytems. Tech.
rep. CUED/F-INFENG-TR 166, Cambridge University.
Salganicoff, M. (1993). Density-adaptive learning and forgetting. In Proc. of the 10th
International Conference on Machine Learning, pp. 276–283, Amherst (MA, USA).
Morgan Kaufmann Publishers.
Scalzo, F., & Piater, J. (2006). Unsupervised learning of dense hierarchical appearance
representations. In Proc. of the 18th International Conference on Pattern Recognition,
Hong-Kong.
Schaal, S. (1997). Learning from demonstration. In Mozer, M. C., Jordan, M., & Petsche,
T. (Eds.), Advances in Neural Information Processing Systems, Vol. 9, pp. 1040–1046.
Cambridge, MA, MIT Press.
Schmid, C., & Mohr, R. (1997). Local greyvalue invariants for image retrieval. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 19 (5), 530–535.
Schmid, C., Mohr, R., & Bauckhage, C. (2000). Evaluation of interest point detectors.
International Journal of Computer Vision, 37 (2), 151–172.
Schyns, P., & Rodet, L. (1997). Categorization creates functional features. Journal of
Experimental Psychology: Learning, Memory and Cognition, 23 (3), 681–696.
Shibata, K., & Iida, M. (2003). Acquisition of box pushing by direct-vision-based reinforcement learning. In Proc. of the Society of Instrument and Control Engineers Annual
Conference, p. 6.
Singh, S., Jaakkola, T., & Jordan, M. (1995). Reinforcement learning with soft state aggregation. In Advances in Neural Information Processing Systems, Vol. 7, pp. 361–368.
MIT Press.
Sudderth, E., Ihler, A., Freeman, W., & Willsky, A. (2003). Nonparametric belief propagation. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 605–612.
Sutton, R. (1988). Learning to predict by the methods of temporal differences. Machine
Learning, 3 (1), 9–44.
Sutton, R., & Barto, A. (1998). Reinforcement Learning, an Introduction. MIT Press.
Takahashi, Y., Takeda, M., & Asada, M. (1999). Continuous valued Q-learning for visionguided behavior acquisition. In Proc. of the International Conference on Multisensor
Fusion and Integration for Intelligent Systems, pp. 255–260.
Tarr, M., & Cheng, Y. (2003). Learning to see faces and objects. Trends in Cognitive
Sciences, 7 (1), 23–30.
Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications of
the ACM, 38 (3), 58–68.
Uther, W., & Veloso, M. (1998). Tree based discretization for continuous state space reinforcement learning. In Proc. of the 15th National Conference on Artificial Intelligence
(AAAI), pp. 769–774, Madison (WI, USA).
390

Closed-Loop Learning of Visual Control Policies

Watkins, C. (1989). Learning From Delayed Rewards. Ph.D. thesis, King’s College, Cambridge (UK).
Weber, C., Wermter, S., & Zochios, A. (2004). Robot docking with neural vision and
reinforcement. Knowledge-Based Systems, 17 (2–4), 165–172.
Wettergreen, D., Gaskett, C., & Zelinsky, A. (1999). Autonomous guidance and control for
an underwater robotic vehicle. In Proc. of the International Conference on Field and
Service Robotics, Pittsburgh (USA).
Whitehead, S., & Ballard, D. (1991). Learning to perceive and act by trial and error.
Machine Learning, 7, 45–83.
Yin, P.-Y. (2002). Maximum entropy-based optimal threshold selection using deterministic
reinforcement learning with controlled randomization. Signal Processing, 82, 993–
1006.
Yoshimoto, J., Ishii, S., & Sato, M. (1999). Application of reinforcement learning to balancing acrobot. In Proc. of the 1999 IEEE International Conference on Systems,
Man and Cybernetics, pp. 516–521.

391

Journal of Artificial Intelligence Research 28 (2007) 233–266

Submitted 05/06; published 3/07

Auctions with Severely Bounded Communication
Liad Blumrosen

liadbl@microsoft.com

Microsoft Research
1065 La Avenida
Mountain View, CA 94043

Noam Nisan

noam.nisan@gmail.com

School of Computer Science and Engineering
The Hebrew University
Jerusalem 91904, Israel

Ilya Segal

ilya.segal@stanford.edu

Department of Economics
Stanford University
Stanford, CA 94305

Abstract
We study auctions with severe bounds on the communication allowed: each bidder
may only transmit t bits of information to the auctioneer. We consider both welfare- and
profit-maximizing auctions under this communication restriction. For both measures, we
determine the optimal auction and show that the loss incurred relative to unconstrained
auctions is mild. We prove non-surprising properties of these kinds of auctions, e.g., that
in optimal mechanisms bidders simply report the interval in which their valuation lies
in, as well as some surprising properties, e.g., that asymmetric auctions are better than
symmetric ones and that multi-round auctions reduce the communication complexity only
by a linear factor.

1. Introduction
Recent years have seen the emergence of the Internet as a platform of multifaceted economic
interaction, from the technical level of computer communication, routing, storage, and computing, to the level of electronic commerce in its many forms. Studying such interactions
raises new questions in economics that have to do with the necessity of taking computational considerations into account. This paper deals with one such question: how to design
auctions optimally when we are restricted to use a very small amount of communication.
This paper studies the effect of severely restricting the amount of communication allowed
in a single-item auction. Each bidder privately knows his real-valued willingness to pay for
the item, but is only allowed to send k possible messages to the auctioneer, who must
then allocate the item and determine the price on the basis of the messages received. (For
example, a bidder may only be able to send t bits of information, in which case k = 2t ). The
simplest case is k = 2, i.e., each bidder sends a single bit of information. This is in contrast
to the usual auction design formulation, in which bidders communicate real numbers.
While communicating a real number may not seem excessively burdensome, there are
several motivations for studying auctions with such severe restrictions on the communication. First, if auctions are to be used for allocating low-level computing resources, they
c
°2007
AI Access Foundation. All rights reserved.

Blumrosen, Nisan & Segal

should use only a very small amount of computational effort. For example, an auction for
routing a single packet on the Internet must require very little communication overhead,
certainly not a whole real number. Ideally, one would like to “waste” only a bit or two on the
bidding information, perhaps “piggy-backing” on some unused bits in the packet header of
existing networking protocols (such as IP or TCP). Second, the amount of communication
also measures the extent of information revelation by the bidders. Usually, bidders will be
reluctant to reveal their exact private data (e.g., Rothkopf, Teisberg, & Kahn, 1990). This
work studies the tradeoff between the amount of revealed data and the optimality of the
auctions. We show that auctions can be close to optimal even using a single Yes/No question per each bidder. Our results can also be applied to various environments where there
is a need for discretize the bidding procedure; one example is determining the optimal bid
increment in English auctions (Harstad & Rothkopf, 1994). Finally, a restriction on communication may sometimes be viewed as a proxy for other simplicity considerations, such
as simple user interface or small number of possible payments to facilitate their electronic
handling. A recent paper (Blumrosen & Feldman, 2006) shows that the ideas illustrated
in this work extend to general mechanism-design frameworks where the requirement for a
small number of “actions” per each player are natural and intuitive.
We examine the effect of severe communication bounds on both the problem of maximizing social welfare and that of maximizing the seller’s expected profits (the latter under
the restrictions of Bayesian incentive compatibility and interim individual rationality of
the bidders, and under a standard regularity condition on the distribution of bidders’ valuations). We study both simultaneous mechanisms, in which the bidders send their bids
without observing any actions of the other bidders, and sequential mechanisms where messages may depend on previous messages. We find that single-item auctions may be very
close to fully optimal despite the severe communication constraints. This is in contrast to
combinatorial auctions, in which exact or even approximate efficiency is known to require
an exponential amount of communication in the number of goods (Nisan & Segal, 2006).1
Both for welfare maximization and revenue maximization, we show that the optimal
2-bidder auction takes the simple form of a ”priority game” in which the player with the
highest bid wins, but ties are broken asymmetrically among the players (i.e., some players
have a pre-defined priority over the others when they send the same message). We show
how to derive the optimal values for the parameters of the priority game. These optimal
mechanisms are asymmetric by definition, although the players are a priori identical. The
asymmetry is in contrast with optimal mechanisms with unconstrained communication,
where symmetric mechanisms achieve optimal welfare (the second-price auction, Vickrey,
1961) and optimal profit (the Myerson auction, Myerson, 1981, for symmetric bidders).
Furthermore, we show that for any number of players, as the allowed number of messages
grows, the loss due to bounded communication is in order of O( k12 ). The bound is tight
for some distributions of valuations (e.g., for the uniform distribution). In addition, we
consider the case in which the number of players grows while each player has exactly two
1. There have been several other studies considering various computational considerations in auction design: timing (e.g., Lavi & Nisan, 2004; Roth & Ockenfels, 2002), unbounded supply (e.g., Feigenbaum,
Papadimitriou, & Shenker, 2001; Goldberg, Hartline, & Wright, 2001; Bar-Yossef, Hildrum, & Wu, 2002),
computational complexity in combinatorial auctions (see the survey by Cramton, Shoham, & Steinberg,
2006) and more.

234

Auctions with Severely Bounded Communication

possible messages. We show that priority games are optimal for this case as well, and we
also characterize the parameters for the optimal mechanisms and show that they can be
generated from a simple recursive formula. We offer an asymptotic bound on the welfare
and profit losses due to bounded communication as the number of players grows (it is O( n1 )
for the uniform distribution). All the optimal mechanisms in this paper are deterministic,
but they are optimal even if the auctioneer is allowed to randomize.2
Our analysis implies some expected as well as some unexpected results:
• Low welfare and profit loss: Even severe bounds on communication result in
only a mild loss of efficiency. We present mechanisms in which the welfare loss and
the profit loss decrease exponentially in the number of the communication bits (and
quadratically in the number k of the allowed bids). For example, with two bidders
whose valuations are uniformly distributed on [0, 1], the optimal 1-bit auction brings
expected welfare 0.648, compared to the first-best expected welfare 0.667.
• Asymmetry helps: Asymmetric auctions are better than symmetric ones with the
same communication bounds. For example, with two bidders whose valuations are
uniformly distributed in [0, 1], symmetric 1-bit auctions only achieve expected welfare
of 0.625, compared to 0.648 for asymmetric ones. We prove that both welfare- and
profit-maximizing auctions must be discriminatory in both allocation and payments.
• Dominant-strategy incentive compatibility is achieved at no additional
cost: The auctions we design have dominant-strategy equilibria and are ex-post individually rational3 , yet are optimal even without any incentive constraints (for welfare
maximization), or among all Bayesian-Nash incentive-compatible and interim individually rational auctions (for profit maximization). This generalizes well-known results
for the case without any communication constraints.
• Bidding using “mutually-centered” thresholds is optimal: We show that in the
optimal auctions with k messages, bidders simply partition the range of valuations into
k interval ranges and announce their interval. In 2-bidder mechanisms, each threshold
will have the interesting property of being the average value of the other bidder in the
respective interval. We denote such threshold vectors as “mutually centered”.
• Sequential mechanisms can do better, but only up to a linear factor: Allowing players to send messages sequentially rather than simultaneously can achieve
a higher payoff than in simultaneous mechanisms. However, the payoff in any such
multi-round mechanism among n players can be achieved by a simultaneous mechanism in which the players send messages which are longer only by a factor of n. This
result is surprising in light of the fact that in general the restriction to simultaneous
communication can increase communication complexity exponentially. Our results
2. The mechanisms are optimal even when the auctions are run repeatedly, as long as the bidders’ values
are uncorrelated over time.
3. A mechanism is ex-post individually rational if a player never pays more than her value. Interim individual rationality is a weaker property, in which a player will not pay more than his value on average.
Individual rationality constraints are essential for the study of revenue maximization (otherwise, the
potential revenue is unbounded).

235

Blumrosen, Nisan & Segal

for sequential mechanisms are very robust in several aspects. They allow the players
to send messages of various sizes and in any order, and they allow the auctioneer to
adaptively determine the order and the size of the messages based on the history of
the messages. The auctioneer may also use randomized decisions.
Although the welfare-maximizing mechanisms are asymmetric, symmetric mechanisms
can also be close to optimal: we show that as the number k of possible messages grows,
while the number of players is fixed, the loss in optimal symmetric mechanisms converges
to zero at the same rate as the loss in efficient “priority games”. The optimal loss in
symmetric and asymmetric mechanisms, however, differs by a constant factor. On the other
hand, when we fix the number of messages, we show that the optimal loss in asymmetric
mechanisms converges to zero asymptotically faster than in optimal symmetric mechanisms
(O( n1 ) compared to O( logn
n ), for the uniform distribution).
We now demonstrate the properties above with an example for the simplest case: a
2-bidder mechanism where each player has two possible bids (i.e., 1 bit) and the values are
distributed uniformly.
Example 1. Consider two players, Alice and Bob, with values uniformly distributed between
[0, 1]. A 1-bit auction among these players can be described by a 2x2 matrix, where Alice
chooses a row, and Bob chooses a column. Each entry of the matrix specifies the allocation
and payments given a combination of bids. The mechanism is allowed to toss coins to
determine the allocations. Figure 1 describes an example for such a mechanism, and we
denote this mechanism by g1 .
A strategy defines how a player determines his bid according to his private value. We
first note that in g1 , both players have dominant strategies, i.e., strategies that are optimal
regardless of the actions of the other players. Consider the following threshold strategy:
“ bid 1 if your valuation is greater than 13 , else bid 0”. Clearly, this strategy is dominant
for Alice in g1 : when her valuation is smaller than 13 she will gain a negative utility if she
bids “1”; When her valuation is greater than 31 , bidding “0” gives her a utility of zero, but
she can get positive utility by bidding “1”. Similarly, a threshold strategy with the threshold
2
3 is dominant for Bob.
The social welfare in a mechanism measures the total happiness of the players from
the outcome, or in our case, the value of the player that receives the item. The expected
welfare in g1 , given that the players follow their dominant strategies, is easily calculated to
35
be 54
= 0.648: both players will bid “0” with probability 13 · 23 , and the expected welfare in
this case equals the expected value of Bob, 21 · 23 . Similar computations show that the expected
welfare is indeed:
¡ ¢
¡
¢
¡
¢
¡
¢
1 2 23
1
2 1 + 23
1 2 1 + 13
1
2 1 + 23
35
+ (1 − )
+ (1 − )
+ (1 − )(1 − )
=
33 2
3
3
2
3 3
2
3
3
2
54
We see that despite restricting the communication from an infinite number of bits to
1
a single bit only, a relatively small welfare loss of 54
was incurred. Of course, a random
allocation that can be implemented without communication at all will result in an expected
welfare of 12 , and this may be regarded as our naive benchmark.
236

Auctions with Severely Bounded Communication

It turns out that the mechanism described in Figure 1 maximizes the expected welfare:
no other 1-bit mechanism achieves strictly higher expected welfare with any pair of bidders’
strategies (that is, regardless of the concept of equilibrium we use). We note that the optimal
mechanism is asymmetric (a “priority game”) – ties are always broken in favor of Bob, and
that this mechanism is optimal even when randomized decisions are allowed. Note that
the optimal symmetric 1-bit mechanism uses randomization, but only achieves an expected
welfare of 0.625 (the mechanism is illustrated in Appendix A.3 and see also Footnote 20).
Finally, we note that the optimal thresholds of the players are “mutually centered”. That
is, Alice’s value 13 is the average value of Bob when he bids 0 and Bob’s value 32 is the average
value of Alice when she bids 1. The intuition is simple: given that Bob bids “0”, his average
value is 12 · 23 = 13 . For which values of Alice should an efficient mechanism give her the
item? Clearly when her value is greater than the average value of Bob. Therefore, Alice
should use the threshold 13 .

The most closely related work in the economic literature is by Harstad and Rothkopf
(1994), who considered similar questions in cases of restricting the bid levels in oral auctions
to discrete levels, and by Wilson (1989) and McAfee (2002) who analyzed the inefficiency
caused by discrete priority classes of buyers. In particular, Wilson showed that as the number k of priority classes grows, the efficiency loss is asymptotically proportional to k12 . While
in the work of Wilson the buyers’ aggregate demand is known while supply is uncertain,
in our model the demand is uncertain. Both Wilson and Harstad and Rothkopf restrict
attention to symmetric mechanisms, while we show that creating endogenous asymmetry
among ex ante identical buyers is beneficial. Another related work is by Bergemann and
Pesendorfer (2001), where the seller can decide on the accuracy by which bidders know their
private values. This problem is different than ours, since the bidders in our model know
their valuations. The work by Parkes (2005) is also related. He compared the efficiency
of simultaneous and sequential auctions under uncertainties on the values of the players.
Recent work also studied similar discrete-bid model in the context of ascending auctions
and auctions that use take-it-or-leave-it offers (Kress & Boutilier, 2004; Sandholm & Gilpin,
2006; David, Rogers, Schiff, Kraus, & Jennings, 2005).
The organization of the paper is as follows: Section 2 presents our model definition and
introduces our notations and Section 3 presents a characterization of the welfare- and profitoptimal 2-bidder auctions. Section 4 characterizes optimal mechanisms with an arbitrary
number of bidders, but 2 possible bids for each player. In Section 5 we give an asymptotic
analysis of the minimal welfare and profit losses in the optimal mechanisms. Finally, Section
6 compares simultaneous and sequential mechanisms with bounded communication.

2. The Model
This section presents our formal model and the notations we use.
237

Blumrosen, Nisan & Segal

B
A
0
1

0

1

B wins and pays 0
A wins and pays 13

B wins and pays 0
B wins and pays 23

Figure 1: (g1 ) A 2-bidder 1-bit game that achieves maximal expected welfare. For example, when
Alice (the rows bidder) bids “1” and Bob bids “0”, Alice wins and pays 13 .

2.1 The Bidders and the Mechanism
We consider single item, sealed bid auctions among n risk-neutral players. Player i has a
private valuation for the object vi ∈ [a, b].4 The valuations are independently drawn from
cumulative probability functions Fi . In some parts of our analysis5 , we assume the existence
of an always-positive probability density function fi . We will sometime treat the seller as
one of the bidders, numbered 0. The seller has a constant valuation v0 for the item. We
consider a normalized model, i.e., bidders’ valuations for not having the item are a.
The novelty in our model, compared to the standard mechanism-design settings, is that
each bidder i can send a message of ti = lg(ki ) bits to the mechanism, i.e., player i can
choose one of possible ki bids (or messages). Denote the possible set of bids for bidder i
as βi = {0, 1, 2, ..., ki − 1}. In each auction, bidder i chooses a bid bi ∈ βi . A mechanism
should determine the allocation and payments given a vector of bids b = (b1 , ..., bn ):
Definition 1. A mechanism g is composed of a pair of functions (a, p) where:
• a : (β1 × ... × βn ) → [0, 1]n+1 is the allocation scheme (not necessarily deterministic).
We denote the ith coordinate of a(b) by ai (b), which is bidder i’s probability
for winning
Pn
the item when the bidders bid b. Clearly, ∀i ∀b ai (b) ≥ 0 and ∀b
a
i
i=0 (b) = 1. If
a0 (b) > 0, the seller will keep the item with a positive probability.
• p : (β1 × ... × βn ) → <n is the payment scheme. pi (b) is the payment of the ith bidder
given a vector of bids b.6
Definition 2. In a mechanism with k possible bids, for every bidder i, |βi | = ki = k.
We denote the set of all the mechanisms with k possible bids among n bidders by Gn,k .
We denote the set of all the n-bidder mechanisms in which |βi | = ki for each bidder i, by
Gn,(k1 ,...,kn ) .
A strategy si for bidder i in a game g ∈ Gn,(k1 ,...,kn ) describes how a bidder determines
his bid according to his valuation, i.e., it is a function si : [a, b] → {0, 1, ..., ki − 1}. Let
4. For simplicity, we use the range [0, 1] in some parts of the paper. Using the general interval will be
required, though, for the characterization of the optimal mechanisms, mainly due to the reduction we
use for maximizing the revenue that translates the original support to their virtual valuations that are
drawn from another interval.
5. That is, in the characterization of the optimal mechanisms in Sections 3.2 and 4 and when using the
concept of virtual valuation in Sections 3.3 and 5.2
6. Note that we allow non-deterministic allocations, but we ignore non-deterministic payments (since we
are interested in expected values, using lottery for the payments has no effect on our results).

238

Auctions with Severely Bounded Communication

s−i denote the strategies of the bidders except i, i.e., s−i = (s1 , ..., si−1 , si+1 , ..., sn ). We
sometimes use the notation s = (si , s−i ).
Definition 3. A real vector (t0 , t1 , ..., tk ) is a vector of threshold values if t0 ≤ t1 ≤ ... ≤ tk .
Definition 4. A strategy si is a threshold strategy based on a vector of threshold values
(t0 , t1 , ..., tk ), if for every bid j ∈ {0, ..., ki − 1} and for every valuation vi ∈ [tj , tj+1 ), bidder
i bids j when his valuation is vi , i.e., si (vi ) = j (and for every vi , vi ∈ [t0 , tk ]). We say
that si is a threshold strategy, if there exists a vector t of threshold values such that si is a
threshold strategy based on t.
2.2 Optimality Criteria
The bidders aim to maximize their (quasi-linear) utilities. The utility of bidder i is a when
he loses (and pay nothing), and vi − pi when he wins and pay pi . Let ui (g, s) denote the
expected utility of bidder i from a game g when the bidders use the vector of strategies s
(implicit here is that this utility depends on the value vi ).
Definition 5. A strategy si for bidder i is dominant in a mechanism g ∈ Gn,(k1 ,...,kn ) if
regardless of the other bidders’ strategies s−i , i cannot increase his expected utility by a
deviation to another strategy, i.e.,
∀sei ∀s−i ui (g, (si , s−i )) ≥ ui (g, (sei , s−i ))
Definition 6. A profile of strategies s = (s1 , ..., sn ) forms a Bayesian-Nash equilibrium
(BNE) in a mechanism g ∈ Gn,(k1 ,...,kn ) , if for every bidder i, si is the best response for the
strategies s−i of the other bidders, i.e.,
∀i ∀sei ui (g, (si , s−i )) ≥ ui (g, (sei , s−i ))
We use standard participation constraints definitions: We say that a profile of strategies
s = (s1 , ..., sn ) is ex-post individually rational in a mechanism g, if every bidder never pays
more than his actual valuation (for any realization of the valuations); we will assume a
strong version of this definition that holds even in randomized mechanisms. We say that a
strategies profile s = (s1 , ..., sn ) is interim individually rational in a mechanism g if every
bidder i achieves a non-negative expected utility, given any valuation he might have, when
the other bidders play with s−i .
Our goal is to find optimal, communication-bounded mechanisms. As the mechanism
designers, we will try to optimize “social” criteria such as welfare (efficiency) and the seller’s
profit.
The expected welfare from a mechanism g, when bidders use the strategies s, is the
expected social surplus. Because the item is indivisible, the social surplus is actually the
valuation of the bidder who receives the item. If the seller keeps the item, the social welfare
is v0 .
Definition 7. Let w(g, s) denote the expected welfare (or expected efficiency) in the n-bidder
game g when the bidders’ strategies are s, i.e., the expected value of the player (possibly the
opt
seller) who receives the item in g. Let wn,(k
denote the maximal possible expected
1 ,...,kn )
239

Blumrosen, Nisan & Segal

welfare from any n-bidder game where each bidder i has ki possible bids, with any vector of
strategies allowed, i.e.,
opt
wn,(k
=
max
w(g, s)
1 ,...,kn )
g∈Gn,(k1 ,...,kn ) , s

opt
opt
When all bidders have k possible bids we use the notation wn,k
= wn,(k,...,k)

Actually, the optimal welfare should have been defined as the maximum expected welfare
that can be obtained in equilibrium. Since we later show that the optimal welfare without
strategic considerations is dominant-strategy implementable, we use the above definition
for simplicity. Note that even in the absence of communication restrictions, optimizing
the welfare objective is obtained by a first-best solution (using the VCG scheme); profit
maximization, on the other hand, is obtained by a second-best solution (incentive constraints
bind in Myerson’s auction, Myerson, 1981).
Definition 8. The seller’s profit is the payment received from the winning bidder, or v0
when the seller keeps the item.7 Let r(g, s) denote the expected profit in the n-bidder game
opt
g where the bidders’ strategies are s. Let rn,k
denote the maximal expected profit from an
n-bidder mechanism with k possible bids and some vector of interim individually-rational
strategies s that forms a Bayesian-Nash equilibrium in g:
opt
rn,k
=

max
r(g, s)
g ∈ Gn,k
s is interim IR and in BNE in g

Note that we define the optimal welfare as the maximal welfare among all mechanisms
and strategies, not necessarily in equilibria, and we define the optimal profit as the maximal
profit achievable in interim-IR Bayesian-Nash equilibria in any mechanism. Yet, the optimal
mechanisms (for both measures) that we present in this paper implement these optimal
values with dominant strategies and ex-post IR.8
Definition 9. We say that a mechanism g ∈ Gn,k achieves the optimal welfare (resp.
profit), if g has an interim-IR Bayesian-Nash equilibrium s for which the expected welfare
opt
opt
(resp. profit) is w(g, s) = wn,k
(resp. r(g, s) = rn,k
).
We say that a mechanism g ∈ Gn,k incurs a welfare loss (resp. profit loss) of L, if it
achieves an expected welfare (resp. profit) which is additively smaller than the optimal welfare (resp. profit) with unbounded communication by L (the optimal results with unbounded
communications are the best results achievable with interim-IR Bayesian-Nash equilibria).

3. Optimal Mechanisms for Two Bidders
In this section we present 2-bidder mechanisms with bounded communication that achieve
optimal welfare and profit. In Section 4 we will present the characterization of the welfareoptimal and profit-optimal n-bidder mechanisms with 2 possible bids for each bidder. The
7. When v0 = 0, the expected profit is equivalent to the seller’s expected revenue.
8. Note that ex-ante IR, i.e., when bidders do not know their type when choosing their strategies, is noninteresting in this model, since the auctioneer can then simply ask each bidder to pay her expected
valuation.

240

Auctions with Severely Bounded Communication

characterization of the optimal mechanisms in the most general case (n bidders and k possible bids) remains an open question. Anyway, our asymptotic analysis of the optimal welfare
loss and the profit loss (in Section 5) holds for the general case, and shows asymptotically
optimal mechanisms.
We first show that the allocation rules in efficient mechanisms have a certain structure
we call priority games. The term priority game means that the allocation rule uses an
asymmetric tie breaking rule: the winning player is the player with the highest priority
among the bidders that bid the highest. One consequence is that the bidder with the lowest
priority will win only when his bid is strictly higher than all other bids. Note that the
term “priority game” refers to the asymmetry in the mechanism’s allocation function, but
additional asymmetry will also appear in the payment scheme. A modified priority game
has a similar allocation, except the item is not allocated when all the bidders bid their
lowest bid.9 We will mostly be interested in such mechanisms when the players have the
same bid space βi .
Definition 10. A game is called a priority game if it allocates the item to the bidder i that
bids the highest bid (i.e., when bi > bj for all j 6= i, the allocation is ai (b) = 1 and aj (b) = 0
for j 6= i), with ties consistently broken according to a pre-defined order on the bidders.
A game is called a modified priority game if it has an allocation as of a priority game,
except when all bidders bid 0, the seller keeps the item.
It turns out to be useful to build the payment scheme of such mechanisms according to
a given profile of threshold strategies:
Definition 11. An n-bidder priority game based on a profile of threshold values’ vectors
−
→
t = (t1 , ..., tn ) ∈ ×ni=1 <k+1 (where for every i, ti0 ≤ ti1 ≤ ... ≤ tik ) is a mechanism
whose allocation is a priority game and its payment scheme is as follows: when bidder j
wins the item for a vector of bids b she pays the smallest valuation she might have and
still win the item, given that she uses the threshold strategy sj based on tj , i.e, pj (b) =
−
→
min{vj |aj (sj (vj ), b−j ) = 1}. We denote this mechanism as P Gk ( t ). A modified priority
game with a similar payment rule is called a modified priority game based on a profile of
−
→
threshold-value vectors, and is denoted by M P Gk ( t ).
For 2-bidder games, we may use the notations P Gk (x, y), M P Gk (x, y) (where x, y are
some vectors of threshold values). The mechanisms P Gk (x, y) and M P Gk (x, y) are presented in Figure 2. Note P Gk (x, y) and M P Gk (x, y) differ only when bidder A bids “0”
(i.e., the first line of the game’s matrix).
We now observe that priority games and the modified priority games, with the payments
schemes that were described above, have two desirable properties: they admit a dominantstrategy equilibrium, and they are ex-post individually rational when the players follow
these dominant strategies.
As for the dominant strategies, a well known result in mechanism design (see Mookherjee
& Reichelstein, 1992 and also Lemma 1 in Segal, 2003) states that for any monotone10
9. Modified priority games can be viewed as priority games that treat the seller as one of the bidders with
the lowest “priority” (then, the seller always bids his second-lowest bid).
10. A mechanism is monotone if the probability that some bidder wins increases as he raises his bid, fixing
the bids of the other bidders. See Definition 12 below for our model.

241

Blumrosen, Nisan & Segal

allocation rule there is some transfer (i.e., payment) rule that would implement the desired
allocation in dominant strategies. For deterministic auctions, to support this equilibrium,
each winning bidder should pay the smallest valuation for which she still wins (fixing the
behavior of the other bidders). The payments in Definition 11 are defined in this way, and
therefore they support the dominant-strategy implementation. It follows that the threshold
−
→
−
→
strategies based on the threshold values vector t are dominant in both P Gk ( t ) and
→
−
M P Gk ( t ). It is clear from the definition of priority games and modified priority games
that, when playing their dominant threshold strategies, winning players will never pay more
than their value, and losing players will pay zero. Ex-post IR follows.
Actually, the observation about the payments that lead to dominant strategies is even
more general. We observe that monotone mechanisms reveal enough information, despite
the communication constraints, to find transfer rules that support the dominant-strategy
implementation. Therefore, when characterizing the optimal mechanisms we can focus on
defining monotone allocation schemes under the communication restrictions, and the transfers that lead to dominant-strategy equilibria can be concluded “for free”. In other words,
we can use the 2-stage approach that is widely used in the mechanism-design literature also
for bounded-communication settings: first solve the optimal allocation rule, and then construct transfers that satisfy the desired incentive-compatibility and individual-rationality
constraints.
Remark 1. This argument holds for more general environments: in environments in which
each player has a one-dimensional private value and a quasi-linear utility, if a non-monetary
allocation rule can be implemented in dominant strategies with some transfers, then any
communication protocol11 realizing this rule also reveals enough information to construct
supporting transfers for the dominant strategies. To see this, recall that in direct-revelation
mechanisms (i.e., with unbounded communication), if the allocation rule proves to be
monotonic, there are transfers that support a dominant-strategy equilibrium. The transfers
will be defined according to some allocation-dependent thresholds, e.g., for a deterministic
allocation rule every bidder should pay the smallest valuation for which she still wins. By
standard revelation-principle arguments, any monotonic allocation rule in bounded communication mechanisms, can be viewed as a monotonic direct-revelation mechanism with
unbounded communication, and therefore such supporting transfers exist. The supporting
transfers are determined by the changes in the allocation rule as the valuation of each bidder
increases, so the transfers change as the allocation rule changes. Thus, with the same communication protocol that is used for determining the allocation, we can reveal the transfers
that support a dominant-strategy implementation.
3.1 The Efficiency of Priority Games
The characterization of the welfare-maximizing mechanism is done in two steps: we first
show that the allocation scheme in 2-bidder priority games is optimal12 . Afterwards, we
will characterize the strategies of the players that lead to welfare maximization in priority
11. Here we deal with simultaneous communication, i.e., where all bidders send their messages simultaneously. Our observation is not true for sequential mechanisms (see Section 6).
12. We assume, w.l.o.g., throughout this paper that in 2-bidder priority games B Â A, i.e., the mechanism
allocates the item to A if she bids higher than B, and otherwise to B.

242

Auctions with Severely Bounded Communication

0
1
2
...
k-2
k-1

0
B, y0
A, x1
A, x1
...
A, x1
A, x1

1
B, y0
B, y1
A, x2
...
A, x2
A, x2

...
...
...
...
...
...
...

k-2
B, y0
B, y1
B, y2
...
B, yk−2
A, xk−1

k-1
B, y0
B, y1
B, y2
...
B, yk−2
B, yk−1

0
1
2
...
k-2
k-1

0
φ
A, x1
A, x1
...
A, x1
A, x1

1
B, y1
B, y1
A, x2
...
A, x2
A, x2

...
...
...
...
...
...
...

k-2
B, y1
B, y1
B, y2
...
B, yk−2
A, xk−1

k-1
B, y1
B, y1
B, y2
...
B, yk−2
B, yk−1

Figure 2: A priority game (left) and a modified priority game (right) both based on the threshold
values’ vectors x, y. In each entry, the left argument denotes the winning bidder, and the
right argument is the price she pays. The mechanisms differ in the allocation for all-zero
bids, and the payments in the first row.

games; this will complete the description of the outcome of the mechanism for every profile
of bidder valuations. These two stages do not take strategic behavior of the bidders into
account. Yet, as observed before, since the allocation scheme is proved to be monotone,
there exists a payment scheme for which these strategies are dominant.
Definition 12. A mechanism g ∈ Gn,k is monotone if for any vector of bids b and for
any bidder i, the probability that bidder i wins the item cannot decrease when only his bid
increases, i.e.,
0
0
∀b ∀i ∀bi > bi ai (bi , b−i ) ≤ ai (bi , b−i )
In the following theorem we prove that priority games are welfare maximizing. The
proof is composed of four steps: We first show that we can assume that the bidders in the
optimal mechanisms use threshold strategies. Then, we show that the allocation in the optimal mechanisms is, w.l.o.g., monotone and deterministic. We then show that the optimal
mechanisms do not “waste” communication, i.e., no two “rows” or two “columns” in the
allocation matrix of the optimal mechanism are identical. Finally, we use these properties,
together with several combinatorial arguments, to derive the optimality of priority games.
Theorem 3.1. (Priority games’ efficiency) For every pair of distribution functions of
opt
the bidders’ valuations, and for every v0 , the optimal welfare (i.e., w2,k
) is achieved in
either a priority game or a modified priority game (with some pair of threshold strategies).
Proof. We first prove the theorem given that the seller has a low reservation value, i.e.,
v0 ≤ a. Recall that at this point we aim to find the welfare-maximizing allocation scheme,
without taking the incentives of the bidders into account. The proof uses the following
three claims. For a later use, Claims 3.2 and 3.3 are proved for n players.
Claim 3.2. (Optimality of threshold strategies) Given any mechanism g ∈ Gn,(k1 ,...,kn ) ,
there exists a vector of threshold strategies s that achieve the optimal welfare in g among
all possible strategies, i.e., w(g, s) = maxse w(g, se).
Proof. (sketch - a formal proof is given in Appendix A.1)
Given a profile of welfare-maximizing strategies in g, we can modify the strategy of each
bidder (w.l.o.g., bidder 1) to be a threshold strategy maintaining at least the same expected
welfare. The idea is that fixing the strategies s−1 of the other bidders, the expected welfare
243

Blumrosen, Nisan & Segal

achieved when bidder 1 bids some bid b1 is a linear function in bidder i’s value v1 . The
maximum of all these linear functions is a piecewise-linear function, and it specifies the
optimal welfare as a function of v1 . Bidder 1 can use a threshold strategy according to the
breaking points of this piecewise-linear function that choose the welfare-maximizing linear
function at each segment. Clearly, there are at most k − 1 breaking points.
Claim 3.3. (Optimality of deterministic, monotone mechanisms) For every n and
k1 , ..., kn , there exists a mechanism g ∈ Gn,(k1 ,...,kn ) with optimal welfare (i.e., there exists
opt
a profile s of strategies such that w(g, s) = wn,(k
) which is monotone, deterministic
1 ,...,kn )
(i.e., the winner is fixed for each combination of bids) and in which the seller never keeps
the item.
Proof. Consider a mechanism g ∈ Gn,(k1 ,...,kn ) and a profile s of strategies that maximize
opt
the expected welfare, that is, w(g, s) = wn,(k
. A social planner, aiming to maximize
1 ,...,kn )
the welfare, will always allocate the item to the bidder with the highest expected valuation.
That is, for each combination of bids b = (b1 , .., bn ) we will allocate the item (i.e., ai (b) = 1)
to a bidder i such that i ∈ argmaxj (E(vj |sj (vj ) = bj )). The expected welfare clearly did
not decrease. In addition, we always allocate the item (we assume that v0 ≤ a), and the
allocation is deterministic. Finally, we can assume, w.l.o.g., that for each bidder i the bids’
names (i.e., “0”,”1” etc.) are ordered according to the expected value this bidder has.
Then, the mechanism will also be monotone: if a winning bidder i increases his bid, his
expected valuation will also increase, while the expected welfare of all the other bidders will
not change. Thus, bidder i will still have the maximal expected valuation.
Claim 3.4. (Additional bids strictly help) Consider a deterministic, monotone mechanism g ∈ G2,k in which the seller never keeps the item. If g achieves the optimal expected
welfare, then in the matrix representation of g no two rows (or columns) have an identical
allocation scheme.
Proof. The idea that an optimal protocol exploits all its communication resources is intuitive, although it does not hold in all settings (a trivial example is calculating the parity of
two binary numbers, more involved examples can be found in Kushilevitz & Nisan, 1997).
We do not have a simple proof for this statement in our model, and the proof is based on
Lemma A.1 in the appendix in the following way: Consider such an optimal mechanism
g ∈ G2,k with two identical rows. This mechanism achieves the optimal welfare when the
players use some profile of strategies s. g’s monotonicity implies that the two identical rows
are adjacent. Thus, there is a mechanism with ge ∈ G2,(k−1,k) with k − 1 possible bids for
the rows bidder that achieves exactly the same expected welfare as g (when the identical
rows are united to one). This welfare is achieved with the same strategies s of the bidders,
where the rows player bids the united row instead of the two identical rows. The claim will
now follow from Lemma A.1 in the appendix. According to this lemma, the optimal welfare
from a game where both bidders have k possible bids cannot be achieved when one of the
opt
opt
bidders has only k − 1 possible bids (i.e., w2,k
> w2,(k−1,k)
).
Now, due to Claim 3.3, there is a deterministic, monotone game in which the item
opt
must be sold that achieves w2,k
. In such games, the allocation scheme in some row i looks
244

Auctions with Severely Bounded Communication

like [A, ..., A, B...B]. Due to Claim 3.4, in the matrix representation of this optimal game,
there are no two rows with the same allocation scheme. There are k+1 possible monotone
rows for the game matrix (with prefix of 0 to k A’s), but our mechanism has only k rows.
Similarly, we have k different columns (of possible k+1) in the mechanism. Assume that
the row [B, B, ..., B] is in g. Then, the column [A, A, ..., A] is clearly not in g. Therefore,
our game matrix consists of all the columns except [A, A, ..., A], which compose the priority
game where B Â A. If the row [B, B, ..., B] is not in g, then g is the priority game where
A Â B.
Next, we complete the proof for any seller’s valuation v0 . Consider a mechanism h ∈ G2,k
and a pair of threshold strategies based on some threshold-value vectors x
e, ye that achieve
the optimal welfare among all mechanisms and strategies (due to Claim 3.2, such strategies
exist). We will modify h, such that the expected welfare (with x
e, ye) will not decrease. Let
a be the smallest index such that E(vA |f
xa ≤ vA ≤ xg
)
≥
v
.
Let
b be the smallest index
a+1
0
such that E(vB |yeb ≤ vB ≤ yg
b+1 ) ≥ v0 . If a = 0 or b = 0, the item is never allocated to the
seller, and the efficient mechanism is as if v0 ≤ a.
When a, b > 0, consider some vector of bids (i, j). When i < a and j < b, the expected
valuations of both A and B are smaller than v0 . Thus, the seller should keep the item for
optimal welfare. When i < a and j ≥ b, the expected welfare of bidder B is above v0 , and
A’s expected welfare is below v0 , thus we can allocate the item to B and the welfare will not
decrease. Similarly, we should allocate the item to A when i ≥ a and j < b. When i < a,
the allocation is done regardless to i, thus we can assume that xa is the first threshold (i.e.,
a = 1), and similarly b = 1.
Now, we show the optimal allocation for combinations of bids (i, j) such that i ≥ a and
j ≥ b. Here, the item will not be allocated to the seller, so we actually perform an auction
with k − 1 possible bids for each bidder, when the bidders’ valuation are in the range [f
x1 , 1],
[ye1 , 1]. Note that the proof (above) for the case of v0 ≤ a holds for such ranges, so the
optimal welfare is achieved in a priority game. Altogether, the optimal mechanism turns
out to be a modified priority game.
3.2 Efficient 2-bidder Mechanisms with k Possible Bids
Now, we can finally characterize the efficient mechanisms in our model. It turns out that
the optimal threshold values for priority games are mutually centered, i.e., each threshold
is the expected valuation of the other bidder, given that the valuation of the other bidder
lies between his two adjacent thresholds.
Definition 13. The threshold values x = (x0 , x1 , ..., xk−1 , xk ), y = (y0 , y1 , ..., yk−1 , yk ) for
bidders A, B respectively are mutually centered, if the following constraints hold:
R yi
yi−1 fB (vB ) · vB dvB
∀1 ≤ i ≤ k − 1 xi = E(vB |yi−1 ≤ vB ≤ yi ) =
F (y ) − FB (yi−1 )
R xBi+1 i
fA (vA ) · vA dvA
xi
∀1 ≤ i ≤ k − 1 yi = E(vA |xi ≤ vA ≤ xi+1 ) =
FA (xi+1 ) − FA (xi )
→
→
It is easy to see that given any pair of distribution functions, a pair −
x,−
y of mutuallycentered vectors is uniquely defined (when xk = yk and, w.l.o.g., y1 ≥ x1 ). The basic
245

Blumrosen, Nisan & Segal

idea is that if x1 is known, we can clearly calculate y1 (the smallest value that solves
x1 = EvB (vB |y0 ≤ vB ≤ y1 )). Similarly, it is easy to see that all the variables xi and yi can
be considered as continuous, monotone functions of x1 . Now, let z be the solution for the
equation yk−1 = E(vA |xk−1 ≤ vA ≤ z). For satisfying all the 2(k − 1) equations, z must
equal xk . Since z is also a continuous monotone function of x1 , there is only a single value
of x1 for which all the equations hold.
The following intuition shows why the optimal thresholds in priority games must be
mutually centered: Assume that Alice bids i, that is, her value is in the range [xi , xi+1 ]. In
a monotone mechanism, the mechanism designer has to decide what is the minimal value for
which Bob wins when Alice bids i. If the value of Bob is at least the average value of Alice,
given that she bids i, then Bob should clearly receive the item. Therefore, Bob’s threshold
will be exactly this expected value of Alice. The proof has to handle few subtleties for
which the intuition above does not suffice (like the characterization of the first thresholds in
the optimal modified priority games, see below), thus we will derive the mutually-centered
condition from the solution of the optimization problem.
w
w
w
w = (a = y w , y w , ..., y w , y w = b) be
Let xw = (a = xw
0 , x1 , ..., xk−1 , xk = b) and y
0
1
k−1 k
w
mutually-centered threshold values (w.l.o.g., y1 ≥ xw
1 ).
Let x = (a = x0 , x1 , ..., xk−1 , xk = b) and y = (a = y0 , y1 , ..., yk−1 , yk = b) be two thresholdvalue vectors for which the following constraints hold:
• (x1 , ..., xk−1 , b) and (y1 , ..., yk−1 , b) are mutually-centered vectors13 .
³
´
Rx
• x1 = v0 and y1 = FA 1(x2 ) · v0 FA (v0 ) + x12 vA fA (vA )dvA
The following theorem says that if the valuation of the seller for the item (v0 ) is small
enough (e.g., a), the efficient mechanism is a priority game based on xw and y w (which are
mutually centered). Otherwise, the optimal welfare can be achieved in a modified priority
game based on x and y.
Theorem 3.5. For any pair of distribution functions of the bidders’ valuations, and for
any seller’s valuation v0 for the item, the mechanism P Gk (xw , y w ) or the mechanism
opt
M P Gk (x, y) achieves the optimal welfare (i.e., w2,k
). In particular, P Gk (xw , y w ) achieves
the optimal welfare when v0 = a.
Proof. First, we prove that P Gk (xw , y w ) is optimal when v0 = a. According to Theorem
3.1 there is a pair of threshold values’ vectors x = (x0 , x1 , ..., xk ),y = (y0 , y1 , ..., yk ) such
that P Gk (x, y) achieves the optimal welfare. Note that x0 = y0 = a and xk = yk = b, so we
have 2(k − 1) variables to optimize.
We will calculate the total expected welfare by summing first the expected welfare in
the entries of the game matrix where B wins the item, then summing the entries where A
is the winner.
R yi
k
X
fB (vB )vB dvB
y
w(g, s) =
(FB (yi ) − FB (yi−1 )) · (FA (xi ) − FA (x0 )) · i−1
FB (yi ) − FB (yi−1 )
i=1

13. Again, a unique solution exists when, w.l.o.g., y2 ≥ x2

246

Auctions with Severely Bounded Communication

+

k
X

R xi
(FA (xi ) − FA (xi−1 )) · (FB (yi−1 ) − FB (y0 )) ·

i=2

=

k
X

Z
FA (xi ) ·

i=1

yi

yi−1

fB (vB )vB dvB +

k
X

Z
FB (yi−1 ) ·

i=2

xi−1

fA (vA )vA dvA

FA (xi ) − FA (xi−1 )

xi

xi−1

fA (vA )vA dvA

We assume here that a probability density function exists for each bidder. Thus, we can
express the partial derivatives with respect to all variables:
ÃZ
!
yi

0

(w(g, s))xi =

yi−1

µZ

0

(w(g, s))yi =

fB (vB )vB dvB

xi+1

xi

· fA (xi ) + fA (xi ) · xi · FB (yi−1 ) − fA (xi ) · xi · FB (yi ) = 0

¶
fA (vA )vA dvA · fB (yi ) + fB (yi ) · yi · FA (xi ) − fB (yi ) · yi · FA (xi+1 ) = 0

Rearranging the terms derives that yi = EvA (vA |xi ≤ vA ≤ xi+1 ) and that
xi = EvB (vB |yi−1 ≤ vB ≤ yi ) and therefore, x, y should be mutually centered for optimal
efficiency.
Now, we no longer assume v0 = a: According to Theorem 3.1, if the optimal welfare is
not achieved in the priority game above, it will be achieved in a modified priority game.
For some threshold values’ vectors x, y, the expected welfare in M P Gk (x, y) is given by the
formula:
Z b
Z b
vB fB (vB )dvB + FB (y1 )
vA fA (vA )dvA
FA (x1 ) · FB (y1 ) · v0 + FA (x1 )
+
+

k
X
i=2
k
X
i=3

Z
(FA (xi ) − FA (x1 ))

y1

yi

yi−1

Z
(FB (yi−1 ) − FB (y1 ))

x1

vB fB (vB )dvB

xi

xi−1

vA fA (vA )dvA

First-order condition similarly derive the constraints on x1 and y1 given in the above definition of x, y, and that (x1 , ..., xk−1 , xk ) and (y1 , ..., yk−1 , yk ) should be mutually-centered14 .
We demonstrate the characterization given above by showing an explicit solution for the
case of uniformly-distributed valuations in [0, 1].
Corollary 3.6. When the bidders’ valuations are distributed uniformly on [0, 1] and v0 = 0,
the mechanism P Gk (x, y) achieves the optimal welfare where
x = (0,

1
3
2k − 3
,
, ...,
, 1) ,
2k − 1 2k − 1
2k − 1

y = (0,

2
4
2k − 2
,
, ...,
, 1)
2k − 1 2k − 1
2k − 1

14. The results are not surprising, since except for the case when one of the bidders bids 0, we have a priority
game’s allocation for which the optimal threshold values must be mutually centered (due to the first
part of the proof).

247

Blumrosen, Nisan & Segal

Proof. According to Theorem 3.5 optimal welfare is achieved with P Gk (x, y), when x, y are
mutually centered. With uniform distributions, this derives the following constraints, for
i+1
which the given vectors x, y are the unique solution: ∀1≤i≤k−1 xi = yi−12+yi
yi = xi +x
2
To see how the above constraints are implied, note that the conditional expectation of the
second player’s value, given that his value is uniformly distributed between yi−1 and yi , is
exactly yi−12+yi .
1
and y1 = x12+1 , implying
For example, when k = 2 we have the constraints x1 = 0+y
2
that x1 = 1/3 and y1 = 2/3 as in the optimal 1-bit mechanism from Example 1. The
optimal mutually-centered thresholds for k = 4 are, for instance, x = (0, 71 , 37 , 75 , 1) and
y = (0, 72 , 47 , 76 , 1).

3.3 Profit-Optimal 2-bidder Mechanisms with k Possible Bids
Now, we present profit-maximizing 2-bidder mechanisms. Most results in the literature on
profit-maximizing auctions assume that the distribution functions of the bidders’ valuations
are regular (as defined below). When the valuations of all bidders are distributed with the
same regular distribution function, it is well known that Vickrey’s 2nd-price auction, with
an appropriately chosen reservation price, is profit-optimal (Vickrey, 1961; Myerson, 1981;
Riley & Samuelson, 1981) with unbounded communication.
Definition 14. (Myerson, 1981) Let f be a probability density function, and let F be its
cumulative function. We say that f is regular, if the function
ve(v) = v −

1 − F (v)
f (v)

is monotone, strictly increasing function of v. We call the function ve(·) the virtual valuation
of the bidder.
For example, when the bidders valuations are distributed uniformly on [0, 1], a bidder
with a valuation v has a virtual valuation of ve(v) = 2v − 1.
Definition 15. The virtual surplus in a game is the virtual valuation of the bidder (including the seller15 ) who receives the item.
A key observation in the work of Myerson (1981), which we also use, is that in a
Bayesian-Nash equilibrium, the expected profit equals the expected virtual surplus (in interim individually-rational equilibria where losing bidders are not getting any surplus). We
use this property to reduce the profit-optimization problem to a welfare-optimization problem, for which we have already given a full solution. Myerson’s observation was originally
proved for direct-revelation mechanisms. We observe here that Myerson’s observation also
holds for auctions with bounded communication. That is, given a k-bid mechanism, the
expected profit in every Bayesian-Nash equilibrium equals the expected virtual surplus.
Proposition 3.7. Let g ∈ Gn,k be a mechanism with a Bayesian Nash equilibrium s =
(s1 , ..., sn ) and interim individual rationality. Then, the expected revenue achieved by s in
g is equal to the expected virtual surplus of s in g.
15. The seller’s virtual valuation is defined to be his “original” valuation (v0 ).

248

Auctions with Severely Bounded Communication

Proof. Consider the following direct-revelation mechanism gd : each player i bids her true
valuation vi . The mechanism calculates si (vi ) for every i, and determines the allocation
and payments according to g. An easy observation is that gd is incentive compatible (i.e.,
truthful bidding is a Bayesian-Nash equilibrium for the players) and interim individually
rational. According to Myerson observation for direct revelation mechanisms, the expected
revenue in gd is equal to the expected virtual surplus. However, for every combination
of bids, both mechanism output identical allocations and payments. Thus, the expected
revenue and the expected virtual surplus are equal in both mechanisms.
According to Theorem 3.5, the optimal welfare is achieved in either a priority game
or a modified priority game. In a model where bidders consider their virtual valuations as
their valuations, let M P G(e
x, ye) or P G(x, y) be the mechanisms which are the candidates to
achieve the optimal welfare (see Theorem 3.5 for a full characterization). Now, consider the
same mechanisms, except each payment e
c in them is replaced by the respective “true” valuation c = ve−1 (e
c) (i.e., e
c = ve(c) ). Denote these mechanisms by P Gk (xR , y R ), M P Gk (xr , y r ).
These mechanisms achieve the optimal profit in our (original) model. Note that the distribution functions must be regular (but not necessarily identical) for this reduction to
work.
Theorem 3.8. When both bidders’ valuations are distributed with regular distribution functions, the mechanism M P Gk (xr , y r ) or the mechanism P Gk (xR , y R ) (see definitions above)
achieve the optimal expected profit among all profits achievable in an interim-IR Bayesianopt
Nash equilibrium of a mechanism in G2,k (i.e., r2,k
).
x, ye) and (x, y) defined above. The mechanism
Proof. Consider the threshold-value vectors (e
M P G(e
x, ye) is efficient in the model where the bidders consider their virtual valuations as
their valuation (the same proof holds if P G(x, y) is the efficient mechanism). The density
function f is regular, and therefore the virtual valuation ve(·) is strictly increasing. Thus,
M P Gk (xr , y r ) (when the bidders use their original valuations) will have exactly the same
allocation for every combination of bids as M P Gk (e
x, ye) (when the bidders consider their
virtual valuations as their valuations). We conclude that M P Gk (xr , y r ) achieves the optimal
expected virtual surplus and thus also the optimal profit.
As in the case of welfare optimization, we give an explicit solution for the case of
uniform distribution functions. This is a direct corollary of Theorem 3.8. Note that the
optimal profit is achieved in a modified priority game. This holds since for the uniform
distribution the bidders’ expected virtual valuation is negative when they bid “0”, so an
efficient mechanism will not sell the item when all bidders bid “0”.
Corollary 3.9. When the bidders’ valuations are distributed uniformly on [0, 1] and v0 = 0,
the modified priority game M P Gk (x, y) achieves the optimal expected profit among all the
profits achievable in interim-IR Bayesian-Nash equilibria of mechanisms in G2,k , where
1
1 · (1 − θ)
(2k − 5) · (1 − θ)
x = (0, , θ +
, ..., θ +
, 1)
2
2k − 3
2k − 3
y = (0, θ, θ +

2 · (1 − θ)
(2k − 4) · (1 − θ)
, ..., θ +
, 1)
2k − 3
2k − 3
249

Blumrosen, Nisan & Segal

and θ =

√
−2α+ 1+3α
2(1−α)

for α =

1
(2k−3)2

(θ =

5
8

when k=2).

4. Optimal Mechanisms for n Bidders with Two Possible Bids
In this section we consider games among n bidders where each bidder has 2 possible bids
(i.e., they can send only 1 bit to the mechanism). We give the characterization of the optimal
mechanisms for general distribution functions. The characterization of the optimal n-bidder
mechanism with k possible bids seems to be harder, and it remains an open question. The
difficulty stems from the fact that the monotonicity of the allocation rule does not dictate
the exact allocation rule in the general case. Rather, there are many possible allocation
schemes that we cannot rule out before we know the strategies of the bidders.16 Therefore,
it seems that one should solve the involved combinatorial problem of finding the optimal
allocation rule together with finding the optimal payments. Priority games with 2 possible
bids per player can be interpreted as a sequence of take-it-or-leave-it offers; the player with
the highest priority in this interpretation is the first player to be offered, if he accepts the
offer (i.e., bids “1”) he will receive it. See the work by Sandholm and Gilpin (2006) for the
analysis of such take-it-or-leave-it mechanisms. Moreover, 1-bit priority games are actually
a solution to a full-information version of the secretary problem (e.g., Gilbert & Mosteller,
1966; Ajtai, Megiddo, & Waarts, 2001). A decision maker meets the players (or potential
secretaries to be hired), one after another, and should search for the player with the highest
value. The decision maker knows the underlying probability distributions. The decision,
however, should be made ”online” – a player that does not receive the item upon arrival
will never receive the item.
4.1 The Characterization of the Optimal Mechanisms
We first observe that priority games also maximize the welfare in n-bidder games with 2 possible bids. This is easier to see than in the k-possible-bids case. By Claim 3.2 in Theorem 3.1,
the bidders will use threshold strategies. An efficient mechanism will allocate the item, for
−
→
each combination of bids b , to the bidder with the highest
¯ he bids bi¢.
¢
¡when
¡ ¯expected welfare
Given that the distributions are i.i.d, if xi ≥ xj then E vi ¯vi ∈ [xi , b] ≥ E vj ¯vj ∈ [xj , b]
and E (vi |vi ∈ [a, xi ] ) ≥ E (vj |vj ∈ [a, xj ] ). Therefore, ties will be broken according to the
order of the thresholds. If the seller’s reservation price v0 is high enough, the efficient
mechanism will be a modified priority game.17
We now show the characterization of the optimal thresholds for the priority games. We
show that the optimal mechanisms use fully discriminatory payments: the bidder with the
highest priority in the priority game pays the highest payment when she wins, and so forth.
The optimal modified priority game is given by a simple recursive formula. When the seller
allocates the item for when all bids are zero, the constraints become cyclic.
16. Consider, for example, a 3-bidder 3-bid priority game, where the item is allocated to the player with
the second-highest priority when all the players bid their highest bid. This mechanism is also monotone
with no identical actions for the players.
17. To see this, we must note that in an efficient mechanism the seller will never keep the item when one of
the bidder bids 1 (then, a threshold higher than v0 for this bidder will gain a higher welfare).

250

Auctions with Severely Bounded Communication

→
→
Let −
x = (x1 , ..., xn ) and −
y = (y1 , ..., yn ) be the profiles of threshold values for the n
bidders such that the following constraints hold:

∀1≤m≤n−2

∀1≤m≤n−2

x1 = E (v |a ≤ v ≤ xn )
¡ ¯
¢
xm+1 = (1 − F (xm )) · E v ¯v ∈ [xm , b] + F (xm ) · xm
¡ ¯
¢
Pn−1 Qn−1
¯
i=1 ( j=i+1 F (xj )) (1 − F (xi )) E v v ∈ [xi , b]
xn =
Q
1 − n−1
i=1 F (xi )
y1 = v 0
¡ ¯
¢
ym+1 = (1 − F (ym )) · E v ¯v ∈ [ym , b] + F (ym ) · ym

(1)
(2)
(3)
(4)
(5)

→
→
We will now prove that either the mechanism P G2 (−
x ) or the mechanism M P G2 (−
y ) achieve
the optimal welfare. As the thresholds description shows, the thresholds for the modified
priority game (i.e., when the seller keeps the item when all bids are zero) are defined by
a simple, easy-to-compute recursive formula. The optimality of these thresholds can be
shown by the following intuitive argument: Consider a new bidder i that joins a set of i − 1
bidders. An efficient auction will allocate the item to bidder i if and only if his value is
greater than the optimal welfare achievable from the first i − 1 bidders. Therefore, the
threshold for each bidder will equal the optimal welfare gained from the preceding bidders;
and indeed, with probability of 1 − F (yi−1 ), bidder i’s valuation will be greater than the
expected
from the other bidders (yi−1 ) and his average contribution will
¢
¡ ¯ welfare attained
be E v ¯v ∈ [ym , b] ); with probability of F (yi−1 ) he will not contribute to the optimal
welfare which remains yi−1 . This intuition shows why the profit-maximizing thresholds
above (Equations 4,5) are independent of the number of players, what enables an “online”
implementation of the profit-maximizing auctions.
Theorem 4.1. When the bidders’ valuations are distributed with the same distribution func−
−
tion, the mechanism P G2 (→
x ) or the mechanism M P G2 (→
y ) achieves the optimal expected
→
welfare. In particular, when v0 = a, P G2 (−
x ) is the efficient mechanism.
Proof. We already observed that there exists a priority game that achieves the optimal
welfare with threshold strategies. Consider a priority game among n bidders, indexed
by their priorities (i.e., 1 ≺ 2... ≺ n ). Every bidder wins the item if he bids 1 and
all
³Qthe bidders´with higher priorities bid 0. Thus, the probability that bidder i wins is
n
j=i+1 F (xj ) · (1 − F (xi )). When all bidders bid 0, either bidder n wins or the seller
keeps the item for herself. The expected welfare from this game, where the bidders use
threshold strategies x1 , ..., xn is:


Ã n
!
Rb
n
n
Y
Y
X
f (vi )vi dvi
x
i

F (xj ) (1 − F (xi ))
+
F (xi ) E0
w(g, s) =
(1 − F (xi ))
i=1

j=i+1

i=1

Where E0 = E(vn |vn ∈ [a, xn ]) in the priority game and Eo = v0 in a modified priority
game (the second term relates to the case when all the bidders bid 0). For maximum, the
partial derivatives with respect to x1 , ..., xn should equal zero, resulting a characterization
of the optimal solution.
251

Blumrosen, Nisan & Segal

For bidders 1 ≤ m ≤ n − 1 we get that xm equals (both in the priority game and in the
modified priority game):




m−1
m−1
X m−1
Y
Y
¯
¢
¡

F (xj ) (1 − F (xi )) E vi ¯vi ∈ [xi , b] + 
F (xi ) E (vn |vn ∈ [a, xn ] )
i=1

j=i+1

i=1,i6=m

The recursive formula is reached by calculating xm+1 − xm , from which Equations 2 and
5 follow. For bidder n in the priority game the first order conditions yield the constraint
in Equation 3. When m = 1, we have x1 = E (vn |a ≤ vn ≤ xn ) (in the priority game) and
x1 = v0 (in the modified priority game).
As in Section 3, we characterize the profit optimal mechanism by a reduction to the
welfare optimizing problem. Again, the reduction can be performed only for regular distributions. Consider the model where bidders take their virtual valuations as their valuations.
Let P G2 (e
u) or M P G2 (e
z ) be the mechanisms that achieve the optimal welfare in this model
(see Theorem 4.1 above). Let P G2 (u) and M P G2 (z) be similar mechanisms respectively,
except each payment e
c is replaced with its respective “original” valuation c = ve−1 (e
c).
Theorem 4.2. When the bidders’ valuations are distributed with the same regular distribution function, the mechanism P G2 (u) or the mechanism M P G2 (z) achieves the optimal
expected profit among all the profits achievable with a Bayesian-Nash equilibrium and interim IR.
Proof. This is a corollary of Theorem 4.1. The reduction is done as in Theorem 3.8, and it
is possible due to the regularity of the distribution function.
Again, the optimal thresholds for the modified priority game can be given by a simple
recursive formula with an intuitive meaning. The recursion is identical to the welfare optimizing formula (Equation 2), and the only difference is in the value of the first threshold
that should hold y1 = ve−1 (v0 ). The intuition is that given that the best revenue achievable
from the first i − 1 bidders is yi−1 , with probability F (yi−1 ) a new player i will not be
able to pay a higher price (due to the individual-rationality restriction) and therefore the
optimal revenue remains yi−1 . When his value is greater than yi−1 , he cannot be charged
more than his average value (E(v|yi−1 ≤ v ≤ b)).
Now, we give explicit solutions for the uniform distribution on the support [0, 1]. The
following recursive constraints characterize the efficient and profit-optimal mechanisms –
these are the constraints given in Theorems 4.1 and 4.2 for uniform distributions.
Let (x1 , ..., xn ) ∈ [0, 1]n be threshold values for which the following constraints hold:
x1 =
∀m ∈ {1, ..., n − 2}

xm+1 =
xn =

252

xn
2
1 x2m
+
2
2
Pn−1 ³Qn−1

´¡
¢
x
1 − x2i
j=i+1 j
i=1
³
Qn−1 ´
2 1 − i=1
xi

(6)
(7)
(8)

Auctions with Severely Bounded Communication

Let y = (y1 , ..., yn ) ∈ [0, 1]n be threshold values where y1 =
∀m ∈ {1, ..., n − 2}

ym+1 =

1
2

and:

2
1 ym
+
2
2

−
→
Corollary 4.3. Consider the threshold values →
x = (x1 , ..., xn ) and −
y = (y1 , ..., yn ) defined
→
above. When the bidders’ valuations are distributed uniformly in [0, 1] and v0 = 0, P G2 (−
x)
−
→
achieves the optimal welfare and M P G2 ( y ) achieves optimal profit.
For example, when n = 5 we have y = (0.5, 0.625, 0.695, 0.741, 0.775). The above
description is simpler than any closed-form formulae we could find.

5. Asymptotic Analysis of the Welfare and Profit Losses
In this section, we measure the performance of the optimal mechanisms presented in earlier
sections. Although we did not present a characterization of the optimal mechanisms in the
general model of k possible bids and n bidders, we present here mechanisms for this general
case that are asymptotically optimal. For simplicity, we assume that the valuations’ range
is [0, 1] (all the results apply for a general range [a, b] which only changes the constants in
our analysis).
We analyze the welfare loss (Subsection 5.1), the profit loss (Subsection 5.2), and finally,
in Subsection 5.3 we measure the profit loss and the welfare loss in 1-bit mechanism with
n bidders. All the result are asymptotic with respect to the amount of the communication,
except in Section 5.3 where it is with respect to the number of bidders.
5.1 Asymptotic Bounds on the Welfare Loss
The next theorem shows that no matter how the bidders’ valuations are distributed, we can
always construct mechanisms such that the welfare loss they incur diminishes quadratically
in k. This is true for any number of bidders we fix (when k > 2n). In particular, the efficient
mechanism presented in Theorem 3.5 incurs a welfare loss of O( k12 ). The intuition behind
the proof: given the distribution functions of the bidders, we construct a certain threshold
strategy, which will be dominant for all bidders. When using this strategy, each bidder will
bid any bid i with probability smaller than k1 . This way, the probability that a welfare loss
may occur is O( k1 ) (for two players, for instance, a welfare loss will be possible only on the
diagonal of the game’s matrix). The average welfare loss will also be O( k1 ), resulting in a
total expected loss of O( k12 ).
Theorem 5.1. For any (fixed) number of bidders n, and for any set of distribution functions
of the bidders’ valuations, there exist a set of mechanisms gk ∈ Gn,k (k = 2n + 1, 2n + 2, ...),
that incur an expected welfare loss of O( k12 )). These results are implemented in dominant
strategies with ex-post individual rationality.
The requirement that k > 2n (here and in Proposition 5.3 below) is due to the construction of the symmetric mechanism in the following proof. These results hold even without
this requirement, as shown by an asymmetric construction for a more general setting in the
work of Blumrosen and Feldman (2006).
253

Blumrosen, Nisan & Segal

Proof. The proof’s idea: we construct a priority game in which all bidders have the same
dominant threshold strategy, such that the probability for a bidder to bid each bid is smaller
than nk . This is done by dividing the density functions of all the bidders to nk intervals with
equal mass, then combining these thresholds to a vector of k threshold values. Because the
bidders use the same threshold strategy, a welfare loss is possible only when more than one
bidder bids the highest bid. This observation leads to the upper bound.
¥ ¦
P
Let α1 , ..., αn be integers such that ni=1 αi = k − 2, and for every i, αi ≥ nk − 1
(clearly such numbers exist). For every bidder i, let Y i = (y1i , ..., yαi i ) be a set of threshold
values that divide her distribution function fi to αi + 1 segments with the same mass (when
y0i = 0, yαi i +1 = 1), i.e., for every bid j, Fi (yj+1 ) − Fi (yj ) = αi1+1 .
S
S
Let X = { ni=1 Y i } {v0 }, |X| = k − 1, be the union of all the threshold values (we add
arbitrary threshold values if the size of X is smaller than k − 1). Let x = (0, x1 , ..., xk−1 , 1)
be a threshold-value vector created by ordering the threshold values in X from smallest to
largest. Now, consider the n-bidder mechanism M P Gk (e
t) where e
t = (x, .., x). The threshold
strategy based on x is dominant for all the bidders, with ex-post IR. By the construction
18
of the sets Y 1 , ..., Y n , every bidder will bid any particular bid w.p. ≤ 2n
k .
Next, we will bound the welfare loss. We divide the possible cases according to the
number of bidders who bid the highest bid. Since all the bidders use the same threshold
strategy, if only one bidder bids the highest bid, no welfare loss is incurred (he will definitely
have the highest valuation). If more than 1 bidder bid the highest bid i, the expected welfare
loss will not exceed xi+1 − xi . For a set of bidders T ⊆ N , denote the probability that all
the bidders in T bid i by P r(T = i), and the probability that all the bidders not in T have
bids smaller than i by P r(N \ T < i). Thus, the expected welfare loss is smaller than (when
2n < k):
n
X

X

k
X

P r(T = i)P r(N \ T < i) (xi+1 − xi )

j=2 T ⊆N, |T |=j i=1

≤

n
X

X

k
X

P r(T = i) (xi+1 − xi )

j=2 T ⊆N, |T |=j i=1

≤

(nj) k µ ¶j
n X
X
X 2n
j=2

1

i=1

k

(xi+1 − xi )

=

¶
n µ ¶µ
X
n
2n j
j=2

j

k

<

2n · 4n2 ·

1
k2

When the valuations of all the bidders are smaller than v0 , there is no welfare loss (it is
easy to see that we can assume, w.l.o.g., that x1 = v0 ). Note that despite the coefficient of
1
is exponential in n, we consider it as a constant because n is fixed. For Example, when
k2
n = 2 a similar proof shows a welfare loss smaller than k82 (when k > 3).
Asymptotic quadratic bounds were also given by Wilson (1989), which studied similar
settings regarding the effect of discrete priority classes of customers. In the work of Wilson
the uncertainty was about the supply, while in this paper the demand is uncertain as well.
Both results are illustrations for the idea that the deadweight loss is second order in the
18. For every bidder i, and every bid j, Fi (xj+1 ) − Fi (xj ) ≤

254

1
kc
bn

≤

2n
k

Auctions with Severely Bounded Communication

price distortion. (The price distortion in our model is the maximum difference between the
prices that different bidders are facing for the item given the others’ bids, and it can be
bounded above by k1 .) Indeed, a small price distortion ensures both that the probability of
an inefficient allocation is small and that the inefficiency is small when it does occur.
Theorem 5.1 is related to proposition 4 in the paper by Nisan and Segal (2006). Nisan
and Segal showed that discretizing an exactly efficient continuous protocol communicating
d real numbers yields a “truly polynomial” approximation scheme that is proportional to
d (i.e., for any ² > 0 we can realize an approximation factor of 1 − ² using a number of
bits which is polynomial in log(²−1 ) ). Here, we discretize a continuous efficient auction
(e.g., first-price auction), where d is the number of bidders. Discretization then achieves an
approximation error that is exponential in the (minus) number of bits sent per bidder, i.e.,
asymptotically proportional to k1 . However, here we care about average-case approximation
which is even closer, because worst-case approximation within an error of ² ensures an
average case approximation within ²2 (the probability that an error is made is itself in the
order of ²).
We now show that the asymptotic upper bound above is tight, i.e., for some distribution
functions (and in particular, for the uniform distribution) the minimal welfare loss is exactly
proportional to k12 . We show this for any constant number of bidders.
Theorem 5.2. Assume that the bidders’ valuations are uniformly distributed and that v0 =
0. Then, the efficient 2-bidder mechanism P Gk (x, y) described in Corollary 3.6 incurs a
1
welfare loss of exactly 6·(2k−1)
2 . Moreover, for any (fixed) number of bidders n and for any
v0 , there exists a positive constant c such that any mechanism g ∈ Gn,k incurs a welfare
loss ≥ c · k12 .
Proof. We first prove the first part of the theorem, regarding 2-bidder mechanisms. Note
that the given mechanism can make non-optimal allocation only for combinations of bids
that are on the diagonal or on the lower secondary diagonal in the matrix representation
of the 2-bidder game (i.e., when bA = bB or when bA = bB + 1). For such bids (i, j), the
1
overlapping segment of [xi , xi+1 ] and [yj , yj+1 ] is of size 2k−1
. Given such vector of bids
(i, j), if one of the valuations is not in this overlapping segment, the allocation is optimal
(note that we allocate the item to B on the main diagonal, and to A on the secondary
1
diagonal). The probability that both valuation are in this overlapping range is (2k−1)
2.
The expected valuation in our priority game (when both valuation are in this overlapping
segment) is exactly in the middle of this segment. The expected valuation in the optimal
auction (with unbounded communications), restricted to this overlapping interval, will be
1
in the 23 point of this range. Thus, the welfare loss is 16 of the segment, i.e., 61 · 2k−1
. Thus,
for every vector of bids on the main diagonal or on the secondary-diagonal the expected
1
welfare loss is 61 (2k−1)
3 . There are (2k − 1) such vector of bids, thus the total welfare loss
is exactly

1
.
6(2k−1)2

A similar argument shows that even when the seller’s valuation v0 is non zero, the welfare
1
loss is asymptotically greater than (2k−1)
2 : let z1 , ..., zm be the sizes of the overlapping
segments (only when the valuations of both bidders are greater than v0 ). Clearly, m ≤ 2k−1
255

Blumrosen, Nisan & Segal

and

Pm

i=1 zi

≤ 1. Then, the welfare loss from the game is at least

(1 − v0 )2 ·

m
X
i=1

zi2 ·

19 :

m
zi
(1 − v0 )2 X 3 (1 − v0 )2 2k − 1
(1 − v0 )2
1
=
·
zi ≥
≥
3
6
6
6
(2k − 1)
6
(2k − 1)2
i=1

The proof of the second statement is easily derived: Consider only the case where bidders
1 and 2 have valuations above 12 , and the rest of the bidders have valuations below 12 . This
occurs with the constant probability of 21n . The best a mechanism can do is to always
allocate the item to one of 1 or 2. But due to the first part of the theorem, in any 2-bidder
mechanism a welfare loss of proportional to k12 will be incurred (the fact that the valuation
range is [ 12 , 1] and not [0, 1] only changes the constant c). This will hold for any opportunity
cost v0 of the seller. Thus, any mechanism will incur a welfare loss of Ω( k12 ).
Note that the same asymptotic results hold even if we restrict attention to symmetric
mechanisms. Actually, we prove the upper bound in Theorem 5.1 by constructing a symmetric mechanism (we can allocate the item to all the bidders who bid the highest bid with
equal probabilities). However, asymmetric mechanisms do incur a strictly smaller welfare
loss than symmetric mechanisms. For example, when the valuations are distributed uni1
formly, the optimal welfare loss is 6(2k−1)
2 (by Theorem 5.2) compared with an optimal

welfare loss of 6k12 attained by symmetric mechanisms20 (i.e., the welfare loss in asymmetric
mechanisms is about 4 times better). This observation is interesting in light of the results of
Harstad and Rothkopf (1994) and Wilson (1989). Harstad and Rothkopf studied symmetric
English auctions, and analyzed the optimal price-jumps in such auctions. Our results show
that non-anonymous prices (i.e., different jumps for each bidder) can achieve better results
than symmetric (or anonymous) jumps. We also characterize the optimal price-jumps for
such auctions (mutually centered threshold values). Wilson also studied only symmetric
priority classes in his model, and also gives a convergence rate of n12 for the efficiency loss
(where n is the number of priority classes). We show that asymmetric mechanisms can
incur smaller efficiency loss, although the asymptotic convergence rate is the same.
One obvious drawback of our characterization of the optimal mechanisms is that the
design is not “detail-free” (as in Wilson’s doctrine) – we must know the priors of the
bidders for designing the mechanisms. But can we design a mechanism that regardless of
the distribution functions, will always incur a low welfare loss? The answer is that we
can, but they will not be as efficient as in the commonly-known priors case. We observe
that a simple, symmetric mechanism that use equally spaced thresholds (i.e., P Gk (x, ..., x),
1
x = (0, k1 , k2 , ..., k−1
k , 1) ), incurs a welfare loss not greater than k for all possible distribution
functions. It is not hard to verify that this is actually the best that can be done by
“detail-free” mechanism: for any mechanism there exist distribution functions for which
the expected welfare loss is at least in order of k1 . For severely low communication, the
difference between the “detail-free” mechanisms and prior-aware mechanisms (with loss of
19. In
we use the fact that when z = (z1 , ..., zm ) is in the m’th dimensional simplex,
Pmthe 3left inequality
1
i=1 zi ≥ m2 .
20. It is easy to show that efficient symmetric mechanisms are similar to priority games, except the item
is allocated with equal probabilities in cases of ties. The thresholds of the bidders simply divide the
valuations’ range to identical segments. Then, it is straightforward to show that the welfare loss is
exactly 6k12 .

256

Auctions with Severely Bounded Communication

O( k12 ) ) may be substantial. Note that without communication constraints, socially-efficient
results can be achieved by “detail-free” mechanisms – second-price (Vickrey) auctions.
5.2 Asymptotic Bounds on the Profit Loss
As done in Theorem 3.8, the profit optimization problem can be reduced to a welfareoptimization problem by maximizing the expected virtual surplus.
Proposition 5.3. Assume that the bidders’ valuations are distributed with regular distribution functions. Then, for any number of bidders n, there exist a set of mechanisms gk ∈ Gn,k
(k = 2n + 1, 2n + 2, ...) that incur a profit loss of O( k12 ). The profit loss is compared with
the optimal, individually-rational mechanism that is unconstrained in communication.
Proof. Consider the model where bidders consider their virtual valuations vei (vi ) as their
valuations. As the range of the valuations in this model, we take the union of the ranges
of all the bidders’ virtual valuations. Denote this range as [α, β]. Let ge ∈ Gn,k be the
mechanism that achieves the maximal welfare in this model. Due to Theorem 5.1, ge incurs
a welfare loss smaller than c · k12 , for some positive constant c (the constant takes into
account the size of the virtual valuations’ range β − α). Let g be the mechanism with the
same allocation as in ge, only each payment qei for bidder i in ge is replaced with qi = vei −1 (qei )
in g, i.e., qei = vei (qi ). Since each vei is non-decreasing (by their regularity), the allocation
rules in g and ge are identical for every bids’ combination. Thus, g achieves the maximal
expected virtual surplus, and the loss of expected virtual surplus is smaller than c · k12 . The
proposition follows.
Again, this upper bound is asymptotically tight: with the uniform distribution, any
mechanism incurs a profit loss of Ω( k12 ). This result is derived from Theorem 5.2 using
similar arguments as in Proposition 5.3.
Proposition 5.4. Assume that the bidders’ valuations are distributed uniformly. Then, for
any (fixed) number of bidders n, there exists a positive constant c such that any mechanism
g ∈ Gn,k incurs a profit loss ≥ c · k12 .
So far, we assumed that the bidders’ valuations are drawn from statistically independent distributions. We now point out that the relaxation to general joint distributions is
non-interesting in our model. Specifically, we can show that the trivial priority game for
which all the bidders use the threshold strategy based on the vector x = (0, k1 , k2 , ..., k−1
k , 1)
always incurs an expected welfare loss smaller than k1 , and no mechanism can do asymptotically better. In other words, there exists some joint distribution function for which any
mechanism incurs a welfare loss proportional to k1 .
5.3 Asymptotic Bounds for a Growing Number of Bidders
In this subsection, we fix the size of communication allowed (to two possible bids), and we
show asymptotic bounds as a function of the number of bidders rather than the amount
of communication. Unfortunately, we have been able to prove such bounds only for the
uniform distribution.
257

Blumrosen, Nisan & Segal

When we restrict our attention to symmetric mechanisms, the solution is simple. Using
1
the threshold x = n− n−1 (for all bidders) achieves the maximal expected welfare, and we
21
have the exact formula showing that the optimal welfare loss is O( logn
n ).
We now show that optimal asymmetric mechanisms incur asymptotically smaller welfare
and profit losses of O( n1 ). These mechanisms fully discriminate between the agents.
−
→
Theorem 5.5. Consider the mechanisms P G2 (→
x ) and M P G2 (−
y ) described in Corollary
4.3 (in Section 4.1). When the bidders’ valuations are distributed uniformly, both the welfare
→
−
loss in P G2 (−
x ) and the profit loss in M P G2 (→
y ) are smaller than ≤ n9 .
Proof. Let x be the revenue-optimizing thresholds from Corollary 4.3. We will bound the
welfare loss in P G2 (x), the efficient mechanism will incur even a smaller loss. We assume,
w.l.o.g., that in g, bidders are indexed according to their priorities (i.e., 1 ≺ 2... ≺ n ).
When a bidder wins after bidding “1”, the maximal welfare loss is 1 − xi . When all bidders
bid “0”, we use the trivial upper bound of 1 for the welfare loss . Therefore, we can bound
the welfare loss with:


n
n
n
X
Y
Y


xj (1 − xi ) (1 − xi ) +
xi
(9)
i=1

j=i+1

i=1

The following two claims can be easily verified by induction:
Claim 5.6. ∀n

1 − xn ≤

Claim 5.7. ∀n≥15

xn ≤

2
n
2n−3
2n

Now, we prove by induction on n that the first summand in Equation 9 is ≤ n8 . Denote
this first term by wln . Note that wln+1 = (1 − xn+1 )2 + xn+1 wln . Assuming that wln ≤ n8 ,
8
and using the two claims above, it is easy to prove that wln+1 ≤ n+1
for n > 14. (the
reader can verify that this also holds for n ≤ 14.)
Next, we prove (again by induction on n) that the second expression is smaller than n1 .
Q
Q
1
We assume ni=1 xi ≤ n1 and prove that n+1
i=1 xi ≤ n+1 (using Claim 5.7 ) :
n+1
Y
i=1

xi = xn+1

n
Y

xi ≤ xn+1

i=1

2n − 1 1
2n − 1 1
1
1
1
≤
· <
· +
=
n
2n + 2 n
2n + 2 n 2n(n + 1)
n+1

Thus, the expected welfare loss is smaller than n8 + n1 = n9
The statement about the profit loss can be derived from the result about the welfare
loss (again, by reducing profit optimization to welfare optimization). Nevertheless, a direct
proof is easy given the above claims: with the same thresholds x from above, the profit
21. The expected welfare then is given by: xn ·

x
2

+ (1 − xn ) ·

1+x
. A
2
n
− 12
n+1

maximum
is achieved

 (first order
1
n
1 − n− n−1 ( n1 − 1) ( n+1
is the
1

maximal welfare with unbounded communication). It is easy to see that if 1 − n1 n converges to logn n
1
log n
then the welfare loss also converges to logn n . And indeed, 1− n1 n = 1−e− n ≈ logn n (since 1−e−x ≈ x
for small x’s).
conditions) with: x = n

1
− n−1

. The welfare loss is thus:

258

Auctions with Severely Bounded Communication

B
A

0
1

0

1

A, 0
A, 13

B, 14
B, 34

Figure 3: (h1 ) This sequential game (when A bids first) attains a higher expected welfare than
any simultaneous mechanism with the same communication requirement (2 bits). This
outcome is achieved with Bayesian-Nash equilibrium.

loss is bounded from above by
smaller than

Pn ³Qn
i=1

´
x
j=i+1 j · (1 − xi ) · (1 − xi ) that was proved to be

8 22
n.

6. Sequential Auctions
In sequential mechanisms, bidders split their bids into smaller messages and send them
in an alternating order. In this section, we show that sequential mechanisms can achieve
better results. However, the additional gain (in the amount of communication) is only up
to a linear factor in the number of bidders.
A sequential mechanism is a mechanism in which each bidder may send several separate
messages, in some order (not necessarily in a round-robin fashion). At each stage, each
bidder knows what messages the other bidders have sent so far. After all the messages
are sent, the mechanism determines the allocation and payments. We study a general
framework where the auctioneer can adaptively determine the order of the messages and
their sizes according to the message history. The auctioneer can also use randomization in
its decisions. We measure the communication volume in a mechanism by the number of
bits actually transmitted.
Definition 16. The communication requirement of the mechanism is the maximal amount
of bits which may be transmitted by the bidders in this mechanism.
A strategy for a bidder in a sequential mechanism is a threshold strategy if in each stage
i of the game the bidder determines the message she sends by comparing her valuation to
some threshold values x1 , ...xαi (where this bidder has αi + 1 possible bids in stage i).
Example 2. The following sequential mechanism has a communication requirement of 2
(see Figure 3 ): Alice sends one bit to the mechanism first. Bob, knowing Alice’s bid, also
sends one bit. When Alice bids 0: Bob wins if he bids 1 and pays 14 ; If he bids zero Alice
wins and pays zero. When Alice bids 1: Bob also wins when he bids 1, but now he pays 34 ;
If he bids zero, Alice wins again, but now she pays 13 .
It is easy to see that this mechanism has a Bayesian-Nash equilibrium23 that achieves
an expected welfare of 0.653. We saw that the efficient simultaneous mechanism with a
→
22. In the priority games based on the thresholds −
y , if bidder i wins the item, he pays yi . Thus, the maximal
profit loss when bidder i wins is 1 − yi .
23. The following strategies are in Bayesian-Nash equilibrium: Alice uses the threshold 12 , and Bob uses the
threshold 41 when Alice bids 0 and 34 when Alice bids 1.

259

Blumrosen, Nisan & Segal

communication requirement of 2 bits is 0.648 (see Section 1). We conclude that sequential
mechanisms can gain more efficiency than simultaneous mechanisms.
Note that throughout the paper we searched for optimal mechanisms among all the
mechanisms with Bayesian-Nash equilibria, but we managed to implement this optimum
in dominant strategy. In sequential mechanisms it is less likely to find dominant-strategy
implementations, thus our above example uses Bayesian-Nash implementation. Our result below, however, do not assume any particular equilibrium concept in the sequential
mechanisms.
How significant is the extra gain from sequential mechanisms over simultaneous mechanisms? The following theorem states for every sequential mechanism with a communication
requirement of m there exists a simultaneous mechanism that achieves at least the same
welfare with a communication requirement of nm (where n is the number of bidders)24 .
Note that in general (e.g., Kushilevitz & Nisan, 1997), multi-round protocols can reduce
the communication by an exponential factor. We observe that the gain from sequential
mechanism is actually even smaller. In many environments, all messages are sent to a centralized authority (auctioneer); therefore, extra bits of communication will be required to
inform the bidders about the previous messages of the other bidders. The following theorem
holds for any order of transmission and any size of the sub-messages, even if these values
are adaptively determined according to previous messages.
The goal of this section is to show that the gain from sequential auctions, compared
to simultaneous auctions, is mild. We do not offer a comprehensive analysis of this case,
not present welfare-maximizing and revenue-maximizing auctions. Several recent papers
studied different aspects of sequential auctions with similar constraints. Sandholm and
Gilpin (2006) analyzed sequential auctions designed as sequences of take-it-or-leave-it offers.
A paper by Kress and Boutilier (2004) studied sequential single-item auctions with discrete
price increment, where information can be used in subsequent stages. The work of Parkes
(2006) studied information elicitation in simultaneous and sequential auctions when the
values are uncertain.
First, we observe that we can assume that the welfare-maximizing strategies of the
bidders are threshold strategies. Again, we show that for each message chosen by bidder
i, the welfare is a linear function in vi . To show this we should use a backward-induction
argument: in the last message, the bidders will clearly use thresholds. Therefore, in previous
stages the welfare (as a function of vi fixing the strategies of all the other bidders) is a linear
combination of linear functions which is itself a linear function. The maximum over linear
function is a piecewise linear function and the thresholds will be its crossing points.
Theorem 6.1. Let h be an n-bidder sequential mechanism with a communication requirement m. Then, there exists a simultaneous mechanism g that achieves, with dominant
strategies, at least the same expected welfare as h, with a communication requirement smaller
than nm.
Proof. Consider an n-bidder mechanism h with a Bayesian-Nash equilibrium, and with
communication requirement m (for simplicity, assume n divides m, i.e., each bidder sends
24. Note that in sequential mechanisms the bidders must be informed about the bits the other bidders sent
(we do not take this into account in our analysis), so the total gain in communication can be very mild.

260

Auctions with Severely Bounded Communication

m
n

bits). There exists a profile s = (s1 , ..., sn ) of threshold strategies that achieves the
optimal welfare in h. First, we give an upper bound for the total number of thresholds
each bidder uses in the game. For a bidder i, let α1i , ..., αki i be the (positive) sizes of the ki
messages she sends in h. Let γji (1 ≤ j ≤ ki ) be the number of bits that were sent by all
the bidders (including i), before bidder i sends his jth message. When choosing a message
i
of size αji , the bidder uses up to 2αj − 1 thresholds. In each stage, every bidder can use a
different set of thresholds, for every
history of the game. Thus, for sending her jth
³ i possible
´
γji
αj
message she can use up to 2
2 − 1 different thresholds. Summing up, bidder i uses
´
Pki γ i ³ αi
at most T (i) = j=1 2 j 2 j − 1 thresholds. Now, assume, w.l.o.g., that the bidders are
numbered according to the order they send their last messages (i.e., γk11 > γk22 > ... > γknn ).
Recall that the total number
³ of1bits sent
´ by the bidders is m. When sending the last message,
m−α1k
αk
1
1
bidder 1 thus uses 2
2
− 1 < 2m different thresholds. Since each messages have
³ 2
´
m−1−α2k
α
2
a non-zero size, bidder 2 will have at most 2
2 k2 − 1 < 2m−1 different thresholds
for the last stage. Similarly, every bidder i can use at most 2m−i+1 thresholds for his last
message. But therefore, for her before-last message bidder i uses at most 2m−i−1 different
thresholds (the worst case occurs when one bidder sends one bit between bidder i’s 2 last
messages). It follows that the maximal number of different thresholds for bidder i is:
T (i) =

ki
X

´
³ i
i
2γj 2αj − 1

2m−i+1 + 2m−i−1 +

<

´
³ i
i
2γj 2αj − 1

j=1

j=1

< 2m−i+1 + 2m−i−1 +

kX
i −2

m−i−2
X

2j

<

2m−i+1 + 2m−i−1 + 2m−i−1

<

2m−i+2

j=1

Now, let g be a simultaneous mechanisms in which each bidder simply “informs” the
mechanism between which of the thresholds he uses in h his valuation lies. Clearly, for every
set of valuations of the bidders, this allocation in g and h is identical. Due to the inequality
above, m − i + 2 bits suffice for bidder i to express this P
number. We conclude that the
number of bits sent by all the bidders in g is smaller than: ni=1 (m − i + 2) = nm − n(n−3)
.
2
Finally, we mention that we can set the allocation scheme and the payment scheme in
g such that the threshold-strategies based on the thresholds in s will be an equilibrium and
the expected welfare will not decrease. As shown in Section 3, we turn this mechanism to be
monotone by allocating the item deterministically to the bidder with the highest expected
value, in each combination of bids. A dominant-strategy equilibrium follows.
This analysis holds for any order and sizes of the bidder messages, even when they
depend on the history of the messages, since counting the number of thresholds can be still
done in the same way.

7. Future Work
This paper concerns single-item auctions that are severely limited in their ability to elicit
information from the bidders: only a few possible bids are available for each player although
each player may have a continuum of types. We give a comprehensive analysis of such
261

Blumrosen, Nisan & Segal

auctions, and present welfare- and revenue-maximizing mechanisms under these restrictions.
We asymptotically analyze the losses in these optimal mechanisms compared to auctions
with unrestricted communication, and we also compare them to auctions where the bidders’
messages are sent sequentially.
We leave several questions open. The most obvious problem is the exact characterization
of the optimal mechanisms for an arbitrary number of players and an arbitrary number of
possible bids. This paper fully characterized the optimal 2-bidder k-bid auctions and the
optimal n-bidder 2-bid auctions, but only presented asymptotically optimal results for the
general case of n players and k bids. Also, future work may provide an asymptotic analysis
of the welfare- and the revenue loss as a function of both k and n (we provided a separate
asymptotic analysis by each of these variables).
An additional interesting question is regarding the gain from allocating the bits of communication non-uniformly among the agents. While in simple domains (like 2-bidder simultaneous auctions) uniform distribution of the communication seems to be the best option,
this would be unclear, and probably untrue, in more general settings. In addition, it seems
that the concepts and methods presented in this work extend to more general frameworks,
like general single-parameter mechanism-design settings and mechanism design with interdependent values (some extensions are given in the recent work by Blumrosen & Feldman,
2006).
Finally, this work presented a partial study of sequential auctions with communication
restrictions. This kind of auctions captures many reasonable real-life settings, and seems
to be analytically challenging. We did not present a characterization of the optimal sequential mechanisms in this paper, nor a direct comparison of simultaneous and sequential
mechanisms with the same communication requirement. Future work may also compare
prior-aware sequential mechanisms and “detail-free” sequential mechanisms (a similar comparison for simultaneous mechanisms showed that detail-free mechanism can only achieve
trivial results). Another possible extension would be to take an integrated approach and
study settings with partially-known priors.
Acknowledgment. We thank Ron Lavi, Daniel Lehmann, Ahuva Mua’lem and Motty
Perry for helpful discussions. We also thank several anonymous referees for valuable remarks, suggestions and insights. The first two authors were supported by grants from
the Israeli Academy of Sciences. The third author was supported by the National Science
Foundation.

Appendix A. Missing proofs
In this section we present formal proofs for some of the results given in the body of the
paper.

262

Auctions with Severely Bounded Communication

A.1 Optimality of Threshold Strategies
Proof of Claim 3.2 in Theorem 3.1:
Proof. Given a vector of strategies s∗ which achieve optimal welfare in g (i.e.,
maxse∈×k ϕk w(g, se) ), we will show that for every player i we can modify s∗i to be a threshold
i=1
i
strategy, and the welfare will not decrease.
Assume s∗i is not a threshold strategy. Therefore, there must be α, β, γ ∈ [a, b], α < β <
γ such that s∗i (α) = s∗i (γ) = m but s∗i (β) 6= m (where m is some bid of player i). We will
show that a strategy vector s identical to s∗ , except that for every such β si (β) = m, we
have that w(g, s) ≥ w(g, s∗ ).
Denote the probability that all players except i bids b−i as P r(b−i ). Thus, the expected
welfare from a game g given that bidder i with valuation vi bids m and that the other
players use strategies s∗−i is:


X
X
¯ ∗
P r(b−i ) ai (m, b−i ) · vi +
aj (m, b−i ) · E(vj ¯sj (vj ) = bj )
b−i

j6=i

Note that this expected welfare is a linear function of vi , and we denote it by h(m) · vi +
t(m) (the constants h(m) and t(m) depend on the bid m).
We know that s∗ achieve optimal welfare in g and that s∗i (α) = m. Therefore, there is
no other bid l such that if s∗i (α) = l, the expected welfare will increase, i.e.:
∀l 6= m

h(m) · α + t(m) ≥ h(l) · α + t(l)

(10)

h(m) · γ + t(m) ≥ h(l) · γ + t(l)

(11)

Similarly, because s∗i (γ) = m:
∀l 6= m

Because β is a convex combination of α and γ, and due to Equations 10 and 11:
∀l 6= m h(m) · β + t(m) ≥ h(l) · β + t(l)
Thus, the expected welfare for player i, given vi = β, is maximal when she bids m. Therefore, when modifying s∗i such that s∗i (β) = m the total expected welfare will not decrease.
We can repeat this process until s∗i becomes a threshold strategy.25
A.2 Optimal Mechanisms Use All the Possible Bids
opt
opt
Lemma A.1. w2,(k,k)
> w2,(k−1,k)
for every k > 1.

Proof. Let g ∈ G2,(k−1,k) be a deterministic, monotone mechanism that achieves the optimal
welfare with threshold strategies based on the vectors (x, y). Each row in g is of the form
[A, ..., A, B, ..., B], and let li ∈ {0, ..., k} be the first index in row i in which B wins. We will
modify g to ge ∈ G2,(k,k) by adding some missing row, and change the threshold strategy x to
x
e ∈ <k+1 , such that the welfare strictly improves. We assume, w.l.o.g., that the thresholds
are unique (i.e., 0 < x1 < ... < xk−1 < 1, 0 < y1 < ... < 1).
25. See analysis of similar problems, e.g., by Athey (2001).

263

Blumrosen, Nisan & Segal

Case 1. The row [B, ..., B] is in the game’s matrix.
0

E

(v |0≤v ≤y )

0

Let x1 = vB B 2 B 1 , and let x
e = (0, x1 , x1 , x2 , ..., xk−2 , 1). We will create a new
game ge by adding the line [B, ..., B] as the first line. It is easy to see that the allocation
0
in g and ge is identical in all rows except the new one. When vA ∈ [0, x1 ] and vB ∈ [0, y1 ]
ge allocates the item to B where g allocated the item to A. The distribution functions are
0
always positive, hence this will occur with a positive probability. Since E(vA |vA ∈ [0, x1 ]) <
0
x1 < EvB (vB |0 ≤ vB ≤ y1 ), the expected welfare has strictly increased. For higher bids of
bidder B, the allocation in the first row will clearly be efficient now, therefore no welfare
loss was incurred.
Case 2. The row [B, ..., B] does not appear in g’s game matrix.
Due to the monotonicity, g must have two rows i and i + 1 and two columns j and j + 1
such that we allocate the item to B when the bids are (i, j), (i, j + 1) and to A when the
bids are (i + 1, j), (i + 1, j + 1). We will create a mechanism ge by adding a row i0 identical
to row i + 1 except that B wins in index j + 1. The new threshold is constructed as follows:
If E(vB |yj ≤ vB ≤ yj+1 ) < xi+1 :
0
0
Let xi+1 = E(vB |yj ≤ vB ≤ yj+1 ), and let x
e = (0, x1 , ..., xi , xi , xi+1 , ..., 1). As in previous
cases, the welfare in all entries hasn’t changed, except a strictly positive improvement in
the (i0 , j) entry.
If E(vB |yj ≤ vB ≤ yj+1 ) ≥ xi+1 :
0
0
Let xi = E(vB |yj+1 ≤ vB ≤ yj+2 ) and let x
e = (0, x1 , ..., xi , xi , xi+1 , ..., 1). We show that
0
since g is efficient xi+1 < xi < xi+2 : First, E(vB |yj+1 ≤ vB ≤ yj+2 ) > E(vB |yj ≤ vB ≤
yj+1 ) ≥ xi+1 ; Also, since A wins for the bids (i + 1, j + 1), we have E(vB |yj+1 ≤ vB ≤
yj+2 ) ≤ E(vA |xi+1 ≤ vA ≤ xi+2 ) < xi+2 . It follows that the expected welfare has strictly
increased in the entry (i0 , j + 1), and has not decreased in all other entries.
A.3 Optimal Symmetric 1-bit Mechanisms
Following are the optimal 1-bit 2-bidder mechanisms assuming independent uniform distribution for all values. The socially-efficient symmetric 1-bit mechanism achieves an expected
welfare of 0.625 compared to 0.648 that is achieved in an asymmetric 1-bit mechanism and
2/3 that is achieved with unrestricted communication. Similarly, the revenue-maximizing
symmetric 1-bit mechanism below achieves an expected profit of 0.385 compared to 0.39 with
1-bit symmetric mechanisms and 5/12 = 0.417 with unrestricted communication (secondprice auction with a reserve price).
The following mechanism achieves the optimal welfare among all the symmetric 1-bit
mechanisms:
0
1

0
w.p. 12 A wins, pays 0
w.p. 12 B wins, pays 0
A wins and pays 14

1
B wins and pays
w.p.
w.p.

1
2
1
2

1
4

A wins, pays 12
B wins, pays 12

Proving the social efficiency of the mechanism can be done by the following idea: First
note that a symmetric, efficient mechanism will clearly allocate the item to the player that
264

Auctions with Severely Bounded Communication

bids 1 when the other player bids 0, and allocate with equal probabilities of 12 when the
bids are equal. With threshold strategies (x, y) the expected welfare is:
¡
¢
w(x,
y) = x · y · 12 · ´x2 + 12 · y2 + x · (1 − y) · (1+y)
+ (1 − x) · y · (1+x)
+ (1 − x) · (1 − y) ·
2
2
³
1
2

·

(1+x)
2a

+

1
2

·

(1+y)
2

Maximum is achieved when (x, y) = ( 12 , 12 ).
The mechanism below is the revenue-maximizing mechanism:
0
1

0
No allocation
A wins, pays √13

1
B wins and pays √13
w.p. 12 A wins, pays √13
w.p. 12 B wins, pays √13

The idea behind the optimality of the above mechanism over all the 1-bit symmetric
mechanisms: in the profit-maximizing symmetric mechanism if a player bids 0 and the other
bids 1, the latter wins and pays x. When both players bid 1, they will pay x with equal
probabilities. It is easy to see that under the ex-post IR assumption, x = x. The expected
profit is thus: r(x) = x(1 − x)x + (1 − x)xx + (1 − x)(1 − x)( 21 x + 12 x). Maximum is achieved
(x ∈ [0, 1]) when x = √13 .

References
Ajtai, M., Megiddo, N., & Waarts, O. (2001). Improved algorithms and analysis for secretary
problems and generalizations. SIAM Journal on Discrete Mathematics, 14 (1), 1–27.
Athey, S. (2001). Single crossing properties and the existence of pure strategy equilibria in
games of incomplete information. Econometrica, 69 (4), 861–89.
Bar-Yossef, Z., Hildrum, K., & Wu, F. (2002). Incentive-compatible online auctions for digital goods. In 13th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
pp. 964–970.
Bergemann, D., & Pesendorfer, M. (2001). Information structures in optimal auctions. Tech.
rep. 2991, C.E.P.R. Discussion Papers.
Blumrosen, L., & Feldman, M. (2006). Implementation with a bounded action space. In
Proceedings of the 7th ACM conference on Electronic commerce, pp. 62–71.
Cramton, P., Shoham, Y., & Steinberg, R. (2006). Combinatorial Auctions. MIT Press.
David, E., Rogers, A., Schiff, J., Kraus, S., & Jennings, N. (2005). Optimal design of
English auctions with discrete bid levels. In The Sixth ACM Conference on Electronic
Commerce, pp. 98–107.
Feigenbaum, J., Papadimitriou, C. H., & Shenker, S. (2001). Sharing the cost of muliticast
transmissions. Journal of Computer and System Sciences., 63, 21–41.
Gilbert, J. P., & Mosteller, F. (1966). Recognizing the maximum of a sequence. Journal of
the American Statistical Association, 61, 35–73.
Goldberg, A. V., Hartline, J. D., & Wright, A. (2001). Competitive auctions and digital
goods. In Symposium on Discrete Algorithms, pp. 735–744.
265

Blumrosen, Nisan & Segal

Harstad, R. M., & Rothkopf, M. H. (1994). On the role of discrete bid levels in oral auctions.
European Journal of Operations Research, 4, 572–581.
Kress, A., & Boutilier, C. (2004). A study of limited precision, incremental elicitation in
auctions. In the 3rd international joint conference of autonomous agents and multiagent systems.
Kushilevitz, E., & Nisan, N. (1997). Communication Complexity. Cambridge University
Press.
Lavi, R., & Nisan, N. (2004). Competitive analysis of incentive compatible on-line auctions.
Theoretical Computer Science., 310 (1), 159–180.
McAfee, P. (2002). Coarse matching. Econometrica, 70 (5), 2025–2034.
Mookherjee, D., & Reichelstein, S. (1992). Dominant strategy implementation of Bayesian
incentive compatible allocation rules. Journal of Economic Theory, 56 (2), 378–399.
Myerson, R. B. (1981). Optimal auction design. Mathematics of Operations Research, 6 (1),
58–73.
Nisan, N., & Segal, I. (2006). The communication requirements of efficient allocations and
supporting prices. Journal of Economic Theory, 129 (1), 192–224.
Parkes, D. C. (2005). Auction design with costly preference elicitation. Annals of Mathematics and Artificial Intelligence, 44 (3), 269–302.
Parkes, D. C. (2006). Iterative combinatorial auctions. In Cramton, P., Shoham, Y., &
Steinberg, R. (Eds.), Combinatorial Auctions, chap. 2. MIT Press.
Riley, J. G., & Samuelson, W. F. (1981). Optimal auctions. American Economic Review,
71 (3), 381–392.
Roth, A. E., & Ockenfels, A. (2002). Late-minute bidding and the rules for ending secondprice auctions: Evidence from eBay and Amazon on the Internet. American Economic
Review, 92 (4), 1093–1103.
Rothkopf, M. H., Teisberg, T. J., & Kahn, E. P. (1990). Why are Vickrey auctions rare?.
Journal of Political Economy, 98 (1), 94–109.
Sandholm, T., & Gilpin, A. (2006). Sequences of take-it-or-leave-it offers: Near-optimal
auctions without full valuation revelation. In Fifth International Joint Conference on
Autonomous Agents and Multiagent Systems, pp. 1127–1134.
Segal, I. (2003). Optimal pricing mechanisms with unknown demand. American Economic
Review, 93 (3), 509–529.
Vickrey, W. (1961). Counterspeculation, auctions and competitive sealed tenders. Journal
of Finance, 16, 8–37.
Wilson, R. (1989). Efficient and competitive rationing. Econometrica, 57, 1–40.

266

Journal of Artificial Intelligence Research 28 (2007) 453-515

Submitted 08/06; published 4/07

Abstract Reasoning for Planning and Coordination
Bradley J. Clement

BRAD . CLEMENT @ JPL . NASA . GOV

Jet Propulsion Laboratory, Mail Stop: 126-347,
Pasadena, CA 91109 USA

Edmund H. Durfee

DURFEE @ UMICH . EDU

University of Michigan, EECS Department,Ann Arbor, MI 48109 USA

Anthony C. Barrett

TONY. BARRETT @ JPL . NASA . GOV

Jet Propulsion Laboratory, Mail Stop: 126-347,
Pasadena, CA 91109 USA

Abstract
The judicious use of abstraction can help planning agents to identify key interactions between
actions, and resolve them, without getting bogged down in details. However, ignoring the wrong
details can lead agents into building plans that do not work, or into costly backtracking and replanning once overlooked interdependencies come to light. We claim that associating systematicallygenerated summary information with plans’ abstract operators can ensure plan correctness, even for
asynchronously-executed plans that must be coordinated across multiple agents, while still achieving valuable efficiency gains. In this paper, we formally characterize hierarchical plans whose
actions have temporal extent, and describe a principled method for deriving summarized state and
metric resource information for such actions. We provide sound and complete algorithms, along
with heuristics, to exploit summary information during hierarchical refinement planning and plan
coordination. Our analyses and experiments show that, under clearcut and reasonable conditions,
using summary information can speed planning as much as doubly exponentially even for plans
involving interacting subproblems.

1. Introduction
Abstraction is a powerful tool for solving large-scale planning and scheduling problems. By abstracting away less critical details when looking at a large problem, an agent can find an overall solution to the problem more easily. Then, with the skeleton of the overall solution in place, the agent
can work additional details into the solution (Sacerdoti, 1974; Tsuneto, Hendler, & Nau, 1998).
Further, when interdependencies are fully resolved at abstract levels, then one or more agents can
flesh out sub-pieces of the abstract solution into their full details independently (even in parallel) in
a “divide-and-conquer” approach (Korf, 1987; Lansky, 1990; Knoblock, 1991).
Unfortunately, it is not always obvious how best to abstract large, complex problems to achieve
these efficiency improvements. An agent solving a complicated, many-step planning problem, for
example, might not be able to identify which of the details in earlier parts will be critical for later
ones until after it has tried to generate plans or schedules and seen what interdependencies end up
arising. Even worse, if multiple agents are trying to plan or schedule their activities in a shared
environment, then unless they have a lot of prior knowledge about each other, it can be extremely
difficult for one agent to anticipate which aspects of its own planned activities are likely to affect,
and be affected by, other agents.

c
2007
AI Access Foundation. All rights reserved.

C LEMENT, D URFEE , & BARRETT

In this paper, we describe a strategy that balances the benefits and risks of abstraction in largescale single-agent and multi-agent planning problems. Our approach avoids the danger of ignoring
important details that can lead to incorrect plans (whose execution will fail due to overlooked interdependencies) or to substantial backtracking when abstract decisions cannot be consistently refined.
Meanwhile, our approach still achieves many of the computational benefits of abstraction so long
as one or more of a number of reasonable conditions (listed later) holds.
The key idea behind our strategy is to annotate each abstract operator in a plan hierarchy with
summary information about all of its potential needs and effects under all of its potential refinements. While this might sound contrary to the purpose of abstraction as reducing the number of
details, in fact we show that it strikes a good balance. Specifically, because all of the possibly
relevant conditions and effects are modeled, the agent or agents that are reasoning with abstract
operators can be absolutely sure that important details cannot be overlooked. However, because
the summary information abstracts away details about under which refinement choices conditions
and effects will or will not be manifested, and information about the relative timing of when conditions are needed and effects achieved, it still often results in an exponential reduction in information
compared to a flat representation.
Based on the concept of summary information, this paper extends the prior work summarized
below and in Section 8 to make the following contributions:
A formal model of hierarchical plans with temporal extent, and of their execution. While
many planning systems have sophisticated temporal models (e.g., Laborie & Ghallab, 1995; Muscettola, 1994) and some additionally use hierarchical representations of alternative courses of action
(Allen, Kautz, Pelavin, & Tenenberg, 1991; Currie & Tate, 1991; Chien, Knight, Stechert, Sherwood, & Rabideau, 2000a; Castillo, Fdez-Olivares, Garcı́a-Pérez, & Palao, 2006), we know of no
other work that extends the hierarchical task network (HTN) formalization (Erol, Hendler, & Nau,
1994a; Erol, Nau, & Hendler, 1994b) to include temporal extent. We need such a formalism in order
to clarify the semantics of summary information and concurrently executing agents.
Algorithms for deriving summary information about propositional and metric resource conditions and effects, and for using such information to determine potential and definite interactions between abstract tasks. We prove that our summarization techniques are guaranteed to
correctly capture all of the conditions and effects associated with an abstract operator appropriately, augmented with modal information about whether conditions must or may hold and whether
they hold during the entire operation or only for some of the time. Because summary information
captures all conditions and effects, our algorithms can reason with operators at different levels of
abstraction to predict and often resolve operator interactions without fully detailing task hierarchies,
even for operators that are executing asynchronously at different agents.
Sound and complete algorithms for hierarchical refinement planning and centralized plan coordination for actions with temporal extent, supporting flexible plan execution systems. An
agent can reduce backtracking during planning by selectively interleaving the refinement of its plan
with predicting and resolving potential interdependencies between its evolving plan and the plans
that will be asynchronously executed by other agents. Other research has also found benefit in
guiding refinement with conditions specified at higher levels in the plan hierarchy to guide refinement (Sacerdoti, 1974; Young, Pollack, & Moore, 1994; Tsuneto et al., 1998). We show that our
algorithms improve on these capabilities by exploiting the hierarchical structure using summary

454

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

information to more efficiently converge on coordinated plans, which can then be further refined
individually and in parallel by the participating agents.
This ability to coordinate at abstract levels rather than over detailed plans allows each of the
agents to retain some local flexibility to refine its operators as best suits its current or expected
circumstances without jeopardizing coordination or triggering new rounds of renegotiation. In this
way, summary information supports robust execution systems such as PRS (Georgeff & Lansky,
1986), UMPRS (Lee, Huber, Durfee, & Kenny, 1994), RAPS (Firby, 1989), JAM (Huber, 1999), etc.
that interleave the refinement of abstract plan operators with execution.
Our approach also extends plan coordination (plan merging) techniques (Georgeff, 1983; Lansky, 1990; Ephrati & Rosenschein, 1994) by utilizing plan hierarchies and a more expressive temporal model. Prior techniques assume that actions are atomic, meaning that an action either executes
before, after, or at exactly the same time as another. In contrast, we use interval point algebra (Vilain & Kautz, 1986) to represent the possibility of several actions of one agent executing during
the execution of one action of another agent. Because our algorithms can choose from alternative
refinements in the HTN dynamically in the midst of plan coordination, they support interleaved local
planning, multiagent coordination, and concurrent execution.
Search techniques and heuristics, including choose-fewest-threats-first (CFTF) and expandmost-threats-first (EMTF), that take advantage of summary information to prune the search
space. When interdependencies run more deeply in agents’ plans, resolving them at abstract levels, if possible at all, can lead to unacceptable losses in parallel activity. Fortunately, even when
agents need to delve into the details of their plans to tease out interdependencies, summary information can still enable exponential speedups by guiding decomposition and by pruning refinement
choices. The search efficiency of using summary information comes from ignoring irrelevant information, which in a distributed planning system also reduces communication overhead exponentially.
Complexity analyses and experiments showing potential doubly-exponential speedups in refinement and local search planning/scheduling using summary information. Our algorithms
demonstrate that exploiting summary information to guide hierarchical planning and scheduling can
achieve exponential speedups, and resolving interdependencies at abstract levels can improve the
performance of plan coordination algorithms doubly exponentially. While others have shown that
abstraction can exponentially reduce search space size (Korf, 1987; Knoblock, 1991) when subproblem independence properties hold, we show that our techniques lead to exponential improvements
if any of these broader conditions hold for the problem:
• solutions can be found at abstract levels;
• the amount of summary information is less at higher levels than at lower levels; or
• choices of decompositions lead to varying numbers of plan threats.
When none of these conditions hold, we show that generating and using summary information
provides no benefit and can increase computation and communication overhead. Thus, care must be
taken when deciding to use summary information, though it has proven to be extremely worthwhile
in the types of problem domains we have examined, an example of which we next describe.

455

C LEMENT, D URFEE , & BARRETT

M1

M2
D
E

transport1

transport2

A

B

C

tool

bin1

bin2

bin3

bin4

dock

Figure 1: A simple example of a manufacturing domain
produce H
produce G

produce H from G

produce G
on M2

produce G
on M1

move A&B
to M2
move A to M2

move G
to M2
build H

move H
to bin1

build G

move B to M2

Figure 2: The production manager’s hierarchical plan
1.1 Manufacturing Example
As a running example to motivate this work, consider a manufacturing plant where a production
manager, a facilities manager, and an inventory manager each have their own goals with separately
constructed hierarchical plans to achieve them. However, they still need to coordinate over the use
of equipment, the availability of parts used in the manufacturing of other parts, storage for the parts,
and the use of transports for moving parts around. The state of the factory is shown in Figure 1. In
this domain, agents can produce parts using machines M1 and M2, service the machines with a tool,
and move parts to and from the shipping dock and storage bins on the shop floor using transports.
Initially, machines M1 and M2 are free for use, and the transports (transport1 and transport2), the
tool, and all of the parts (A through E) shown in their storage locations are available.
The production manager is responsible for creating a part H using machines M1 and M2. Either M1 and M2 can consume parts A and B to produce G, and M2 can produce H from G. The
production manager’s hierarchical plan for manufacturing H involves using the transports to move
the needed parts from storage to the input trays of the machines, manufacturing G and H, and transporting H back to storage. This plan is shown in Figure 2. Arcs through subplan branches mean
that all subplans must be executed. Branches without arcs denote alternative choices to achieving
the parent’s goal. The decomposition of produce G on M1 is similar to that of produce G on M2.
The facilities manager services each machine by equipping it with a tool and then calibrating it.
The machines are unavailable for production while being serviced. The facilities manager’s hierarchical plan branches into choices of servicing the machines in different orders and uses the transports

456

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

maintenance
service M1 M2

service M1

service M2

service M2 M1

move tool
to dock

move tool equip M1 tool calibrate M1
to M1

Figure 3: The facilities manager’s hierarchical plan
move_parts
move C to dock

move D&E

move D to bin3

move E to bin4

Figure 4: The inventory manager’s hierarchical plan
for getting the tool from storage to the machines (Figure 3). The decomposition of service M2M1
is similar to that of service M1M2.
The parts must be “available” on the space-limited shop floor in order for an agent to use them.
Whenever an agent moves or uses a part, it becomes unavailable. The inventory manager’s goal is
just to move part C to the dock and move D and E into bins on the shop floor (shown in Figure 4).
To accelerate the coordination of their plans, each factory manager can analyze his hierarchical
plan to derive summary information on how each abstract plan operator can affect the world. This
information includes the summary pre-, post-, and in-conditions that intuitively correspond to the
externally required preconditions, externally effective postconditions, and the internally required
conditions, respectively, of the plan based on its potential refinements. Summary conditions augment state conditions with modal information about whether the conditions must or may hold and
when they are in effect. Examples are given at the end of Section 3.2.
Once summary information is computed, the production and inventory managers each could
send this information for their top-level plan to the facilities manager. The facilities manager could
then reason about the top-level summary information for each of their plans to determine that if
the facilities manager serviced all of the machines before the production manager started producing
parts, and the production manager finished before the inventory manager began moving parts on and
off the dock, then all of their plans can be executed (refined) in any way, or CanAnyWay. Then the
facilities manager could instruct the others to add communication actions to their plans so that they
synchronize their actions appropriately.
This top-level solution maximizes robustness in that the choices in the production and facilities managers’ plans are preserved, but the solution is inefficient because there is no concurrent
activity—only one manager is executing its plan at any time. The production manager might not
want to wait for the facilities manager to finish maintenance and could negotiate for a solution with
more concurrency. In that case, the facilities manager could determine that they could not overlap

457

C LEMENT, D URFEE , & BARRETT

their plans in any way without risking conflict (¬CanAnyWay). However, the summary information
could tell them that there might be some way to overlap their plans (MightSomeWay), suggesting
that a search for a solution with more concurrency (at the cost of perhaps committing to specific
refinement choices) has hope of success. In this case, the facilities manager could request the production manager for the summary information of each of produce H’s subplans, reason about the
interactions of lower level actions in the same way, and find a way to synchronize the subplans for
a more fine-grained solution where the plans are executed more concurrently. We give an algorithm
for finding such solutions in Section 5.
1.2 Overview
We first formally define a model of a concurrent hierarchical plan, its execution, and its interactions
(Section 2). Next, we describe summary information for propositional states and metric resources,
mechanisms determining whether particular interactions must or may hold based on this information, and algorithms for deriving the information (Section 3). Built upon these algorithms are others
for using summary information to determine whether a set of CHiPs must or might execute successfully under a set of ordering constraints (Section 4). These in turn are used within a sound and
complete multilevel planning/coordination algorithm that employs search techniques and heuristics
to efficiently navigate and prune the search space during refinement (Section 5). We then show how
planning, scheduling, or coordinating at abstract levels can exponentially improve the performance
of search and execution (Section 6). We provide experimental results demonstrating that the search
techniques also greatly reduce the search for optimal solutions (Section 7). Finally, in Section 8 we
differentiate our approach from related work that we did not mention elsewhere and conclude.

2. A Model of Hierarchical Plans and their Concurrent Execution
A representation of temporal extent in an HTN is important not only for modeling concurrently
executing agents but also for performing abstract reasoning with summary information. If an agent
is scheduling abstract actions and can only sequentially order them, it will be severely restricted
in the kinds of solutions it can find. For example, the agent may prefer solutions with shorter
makespans, and should seek plans with subthreads that can be carried out concurrently.
In this section we define concurrent hierarchical plans (CHiPs), how the state changes over time
based on their executions, and concepts of success and failure of executions in a possible world, or
history. Because we later define summary information and abstract plan interactions in terms of the
definitions and semantics given in this section, the treatment here is fairly detailed (though for an
even more comprehensive treatment, see Clement, 2002). However, we begin by summarizing the
main concepts and notation introduced, to give the reader the basic gist.
2.1 Overview
A CHiP (or plan p) is mainly differentiated from an HTN by including in its definition inconditions,
in(p), (sometimes called “during conditions”) that affect (or assert a condition on) the state just after
the start time of p (ts (p)) and must hold throughout the duration of p. Preconditions (pre(p)) must
hold at the start, and postconditions (post(p)) are asserted at the finish time of p (t f (p)). Metric
resource (res) consumption (usage(p, res)) is instantaneous at the start time and, if the resource is
defined as non-consumable, is instantaneously restored at the end. The decompositions of p (d(p))

458

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

is in the style of and/or tree, having either a partial ordering (order(p)) or a choice of child tasks
that each can have their own conditions.
An execution e of p is an instantiation of its start time, end time, and decomposition. That is, an
execution nails down exactly what is done and when. In order to reason about plan interactions, we
can quantify over possible histories, where each history corresponds to a combination of possible
executions of the concurrently-executing CHiPs for a partial ordering over their activities and in the
context of an initial state. A run (r(h,t)) specifies the state at time t for history h.
Achieve, clobber, and undo interactions are defined in terms of when the executions of some
plans assert a positive literal ` or negative literal ¬` relative to when ` is required by another plan’s
execution for a history. By looking at the literals achieved, clobbered, and undone in the set of
executions in a history, we can identify the conditions that must hold prior to the executions in the
history as external preconditions and those that must hold after all of the executions in the history
as external postconditions.
The value of a metric resource at time t (r(res, h,t)) is calculated by subtracting from the prior
state value the usage of all plans that start executing at t and (if non-consumable) adding back usages
of all that end at t. An execution e of p fails if a condition that is required or asserted at time t is not
in the state r(h,t) at t, or if the value of a resource (r(res, h,t)) used by the plan is over or under its
limits during the execution.
In the remainder of this section, we give more careful, detailed descriptions of the concepts
above, to ground these definitions in firm semantics; the more casual reader can skim over these
details if desired. It is also important to note that, rather than starting from scratch, our formalization
weaves together, and when necessary augments, appropriate aspects of other theories, including
Allen’s temporal plans (1983), Georgeff’s theory for multiagent plans (1984), and Fagin et al.’s
theory for multiagent reasoning about knowledge (1995).
2.2

CH i P s

A concurrent hierarchical plan p is a tuple hpre, in, post, usage, type, subplans, orderi. pre(p),
in(p), and post(p) are sets of literals (v or ¬v for some propositional variable v) representing the
preconditions, inconditions, and postconditions defined for plan p.1
We borrow an existing model of metric resources (Chien, Rabideu, Knight, Sherwood, Engelhardt, Mutz, Estlin, Smith, Fisher, Barrett, Stebbins, & Tran, 2000b; Laborie & Ghallab, 1995).
A plan’s usage is a function mapping from resource variables to an amount used. We write
usage(p, res) to indicate the amount p uses of resource res and sometimes treat usage(p) as a set
of pairs (res, amount). A metric resource res is a tuple hmin value, max value, typei. The min
and max values can be integer or real values representing bounds on the capacity or amount available. The type of the resource is either consumable or non-consumable. For example, fuel and
battery energy are consumable resources because, after use, they are depleted by some amount. A
non-consumable resource is available after use (e.g. vehicles, computers, power).
Domain modelers typically only specify state conditions and resource usage for primitive actions in a hierarchy. Thus, the conditions and usage of a CHiP are used to derive summary conditions,
as we describe in Section 3.4, so that algorithms can reason about any action in the hierarchy. In
order to reason about plan hierarchies as and/or trees of actions, the type of plan p, or type(p), is
1. Functions such as pre(p) are used for referential convenience throughout this paper. Here, pre and pre(p) are the
same, and pre(p) is read as “the preconditions of p.”

459

C LEMENT, D URFEE , & BARRETT

given a value of either primitive, and, or or. An and plan is a non-primitive plan that is accomplished by carrying out all of its subplans. An or plan is a non-primitive plan that is accomplished
by carrying out exactly one of its subplans. So, subplans is a set of plans, and a primitive plan’s
subplans is the empty set. order(p) is only defined for an and plan p and is a consistent set of
temporal relations (Allen, 1983) over pairs of subplans. Plans left unordered with respect to each
other are interpreted to potentially execute concurrently.
The decomposition of a CHiP is in the same style as that of an HTN as described by Erol et al.
(1994a). An and plan is a task network, and an or plan is an extra construct representing a set of all
methods that accomplish the same goal or compound task. A network of tasks corresponds to the
subplans of a plan.
For the example in Figure 2, the production manager’s highest level plan produce H (Figure 2)
is the tuple
h{}, {}, {}, {}, and, {produce G, produce H f rom G}, {be f ore(0, 1)}i.
In be f ore(0,1), 0 and 1 are indices of the subplans in the decomposition referring to produce G and
produce H f rom G respectively. There are no conditions defined because produce H can rely on
the conditions defined for the primitive plans in its refinement. The plan for moving part A from
bin1 to the first input tray of M1 using transport1 is the tuple
h{}, {}, {}, {}, and, {start move, f inish move}, {meets(0, 1)}i.
This plan decomposes into two half moves which help capture important intermediate effects. The
parent orders its children with the meets relation to bind them together into a single move. The
start move plan is
h{at(A, bin1), available(A), f ree(transport1), ¬ f ull(M1 tray1)},
{¬at(A, bin1), ¬available(A), ¬ f ull(bin1), ¬ f ull(M1 tray1), f ree(transport1)},
{¬at(A, bin1), ¬available(A), ¬ f ree(transport1), ¬ f ull(bin1), ¬ f ull(M1 tray1)},
{}, primitive, {}, {}i.
The f inish move plan is
h{¬at(A, bin1), ¬available(A), ¬ f ree(transport1), ¬ f ull(bin1), ¬ f ull(M1 tray1)},
{¬at(A, bin1), ¬available(A), ¬ f ree(transport1), ¬ f ull(bin1), f ull(M1 tray1)},
{¬at(A, bin1), at(A, M1 tray1), available(A), f ree(transport1), ¬ f ull(bin1), f ull(M1 tray1)},
{}, primitive, {}, {}i.
We split the move plan into these two parts in order to ensure that no other action that executes
concurrently with this one can use transport1, part A, or the input tray to M1. It would be incorrect
to instead specify ¬ f ree(transport1) as an incondition to a single plan because another agent could,
for instance, use transport1 at the same time because its ¬ f ree(transport1) incondition would agree
with the ¬ f ree(transport1) incondition of this move action. However, the specification here is still
insufficient since two pairs of (start move, f inish move) actions could start and end at the same
time without conflict. We can get around this by only allowing the planner to reason about the
move plan and its parent plans, in effect, hiding the transition between the start and finish actions.
So, by representing the transition from f ree to ¬ f ree without knowing when that transition will
460

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

take place the modeler ensures that another move plan that tries to use transport1 concurrently with
this one will cause a conflict.2
A postcondition is required for each incondition to specify whether the incondition changes.
This clarifies the semantics of inconditions as conditions that hold only during plan execution
whether because they are caused by the action or because they are necessary conditions for successful execution.
2.3 Executions
Informally, an execution of a CHiP is recursively defined as an instance of a decomposition and an
ordering of its subplans’ executions. Intuitively, when executing a plan, an agent chooses the plan’s
start time and how it is refined, determining at what points in time its conditions must hold, and
then witnesses a finish time. The formalism helps us reason about the outcomes of different ways
to execute a group of plans, describe state transitions, and define summary information.
An execution e of CHiP p is a tuple hd,ts ,t f i. ts (e) and t f (e) are positive, non-zero real numbers
representing the start and finish times of execution e, and ts < t f . Thus, instantaneous actions are not
explicitly represented. d(e) is a set of subplan executions representing the decomposition of plan p
under this execution e. Specifically, if p is an and plan, then it contains exactly one execution from
each of the subplans; if it is an or plan, then it contains only one execution of one of the subplans;
and it is empty if it is primitive. In addition, for all subplan executions, e0 ∈ d, ts (e0 ) and t f (e0 ) must
be consistent with the relations specified in order(p). Also, the first subplan(s) to start must start
at the same time as p, ts (e0 ) = ts (e), and the last subplan(s) to finish must finish at the same time
as p, t f (e0 ) = t f (e). The possible executions of a plan p is the set E (p) that includes all possible
instantiations of an execution of p, meaning all possible values of the tuple hd,ts ,t f i, obeying the
rules just stated.
For the example in Section 1.1, an execution for the production manager’s top-level plan
produce H would be some e ∈ E (produce H). e might be h{e1 , e2 }, 2.0, 9.0 i where e1 ∈
E (produce G), and e2 ∈ E (produce H f rom G). This means that the execution of produce H
begins at time 2.0 and ends at time 9.0.
For convenience, the subexecutions of an execution e, or subex(e), is defined recursively as the
set of subplan executions in e’s decomposition unioned with their subexecutions.
2.4 Histories and Runs
An agent reasoning about summary information to make planning decisions at abstract levels needs
to first be able to reason about CHiPs. In this section we complete the semantics of CHiPs by
describing how they affect the state over time. Because an agent can execute a plan in many different
ways and in different contexts, we need to be able to quantify over possible worlds (or histories)
where agents fulfill their plans in different ways. After defining a history, we define a run as the
transformation of state over time as a result of the history of executions. The formalization of
histories and runs follows closely to that of Fagin et al. (1995) in describing multiagent execution.
A state of a world, s, is a truth assignment to a set of propositions, each representing an aspect
of the environment. We will refer to the state as the set of true propositional variables. A history,
2. Using universal quantification (Weld, 1994) a single plan could have a ∀agent, agent 6= productionManager →
¬using(transport1, agent) condition that would exclude concurrent access to the transport. We could have also
simply specified transport1 as a non-consumable resource with maximum capacity of one.

461

C LEMENT, D URFEE , & BARRETT

h, is a tuple hE, sI i. E is the set of all plan executions of all agents occurring in h, and sI is the
initial state of h before any plan begins executing. So, a history h is a hypothetical world that begins
with sI as the initial state and where only executions in E(h) occur. In particular, a history for the
manufacturing domain might have an initial state as shown in Figure 1 where all parts and machines
are available, and both transports are free. The set of executions E would contain the execution of
produce H, maintenance, move parts, and their subexecutions.
A run, r, is a function mapping a history and time point to states. It gives a complete description
of how the state of the world evolves over time, where time ranges over the positive real numbers.
Axiom 1
r(h, 0) = sI
Axiom 2
v ∈ r(h,t > 0) ⇔(v ∈ r(h,t − ε)∨
∃p, e p ∈ E(h), (v ∈ in(p) ∧ ts (e p ) = t − ε) ∨ (v ∈ post(p) ∧ t f (e p ) = t))∧
(6 ∃p0 , e p0 ∈ E(h), (¬v ∈ in(p0 ) ∧ ts (e p0 ) = t − ε) ∨ (¬v ∈ post(p0 ) ∧ t f (e p0 ) = t))
Axiom 1 states that the world is in the initial state at time zero. Axiom 2 states that a predicate
v is true at time t if it was already true beforehand, or a plan asserts v with an incondition or
postcondition at t, but (in either case) no plan asserts ¬v at t. If a plan starts at t, then its inconditions
are asserted right after the start, t +ε, where ε is a small positive real number. Axiom 2 also indicates
that both inconditions and postconditions are effects.
The state of a resource is a level value (integer or real). For consumable resource usage, a task
that depletes a resource is modeled to instantaneously deplete the resource (subtract usage from the
current state) at the start of the task by the full amount. For non-consumable resource usage, a task
also depletes the usage amount at the start of the task, but the usage is restored (added back to the
resource state) at the end of execution. A task can replenish a resource by having a negative usage.
We will refer to the level of a resource res at time t in a history h as r(res, h,t). Axioms 3 and 4
describe these calculations for consumable and non-consumable resources, respectively.
Axiom 3
r(consumable res, h,t) = r(consumable res, h,t − ε) − ∑e p ∈E(h),ts (e p )=t usage(p, consumable res)
Axiom 4
r(nonconsumable res, h,t) =r(nonconsumable res, h,t − ε)−
∑e p ∈E(h),ts (e p )=t usage(p, nonconsumable res)+
∑e p ∈E(h),t f (e p )=t usage(p, nonconsumable res)
Now that we have described how CHiPs change the state, we can specify the conditions under
which an execution succeeds or fails. As stated formally in Definition 1, an execution succeeds if:
the plan’s preconditions are met at the start; the postconditions are met at the end; the inconditions
are met throughout the duration (not including the start or end); all used resources stay within their
value limits throughout the duration; and all executions in the decomposition succeed. Otherwise,
the execution fails.
462

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

Definition 1
succeeds(e p , h) ≡pre(p) ⊆ r(h,ts (e p ))∧
post(p) ⊆ r(h,t f (e p ))∧
∀t, res,ts (e p ) < t < t f (e p ) ∧ usage(p, res) 6= 0 →
in(p) ⊆ r(h,t)∧
min value(res) <= r(res, h,t) <= max value(res)∧
∀e ∈ d(e p ), succeeds(e, h)
2.5 Asserting, Clobbering, Achieving, and Undoing
Conventional planning literature often speaks of clobbering and achieving preconditions of plans
(Weld, 1994). In CHiPs, these notions are slightly different since inconditions can clobber and
be clobbered, as seen in the previous section. Formalizing these concepts and another, undoing
postconditions, helps us define summary conditions (in Section 3.2). However, it will be convenient
to define first what it means to assert a condition. Figure 5 gives examples of executions involved
in these interactions, and we define these terms as follows:
Definition 2
asserts(e p , `,t, h) ≡(e p ∈ E(h))∧
(` ∈ in(p) ∧ t = ts (e p ) + ε∨
` ∈ post(p) ∧ t = t f (e p ))∧
(r(t, h) ` `)
Definition 2 states that an execution e p in a history h asserts a literal at time t if that literal is an
effect of p that holds in the state at t. Note that that from this point on, beginning in Definition 3, we
use brackets [ ] as a shorthand when defining similar terms and procedures. For example, saying “[a,
b] implies [c, d]” means a implies c, and b implies d. This shorthand will help us avoid repetition,
at the cost of slightly more difficult parsing.
Definition 3
[achieves, clobbers] precondition(e p , `, e p0 ,t, h) ≡
e p , e p0 ∈ E(h)∧
asserts(e p , [`, ¬`],t, h) ∧ ` ∈ pre(p0 ) ∧ t < ts (e p0 )∧
6 ∃e p00 ,t 00 , (asserts(e p00 , `,t 00 , h) ∨ asserts(e p00 , ¬`,t 00 , h)) ∧ t < t 00 ≤ ts (e p0 )
Definition 4
clobbers [in, post]condition(e p , `, e p0 ,t, h) ≡
e p , e p0 ∈ E(h)∧
asserts(e p , ¬`,t, h) ∧ ` ∈ [in(p0 ), post(p0 )] ∧ [ts (e p0 ) < t < ts (e p0 ),t = t f (e p0 )]
Definition 5
undoes(e p , `, e p0 ,t, h) ≡
e p , e p0 ∈ E(h)∧
asserts(e p , ¬`,t, h) ∧ ` ∈ post(p0 ) ∧ t f (e p0 ) > t∧
6 ∃e p00 ,t 00 , (asserts(e p00 , `,t 00 , h) ∨ asserts(e p00 , ¬`,t 00 , h)) ∧ t f (e p0 ) ≤ t 00 < t
463

C LEMENT, D URFEE , & BARRETT

Figure 5: Interval interactions of plan steps
So, an execution achieves or clobbers a precondition if it is the last (or one of the last) to assert
the condition or its negation (respectively) before it is required. Likewise, an execution undoes a
postcondition if it is the first (or one of the first) to assert the negation of the condition after the
condition is asserted. An execution e clobbers an incondition or postcondition of e0 if e asserts the
negation of the condition during or at the end (respectively) of e0 . Achieving effects (inconditions
and postconditions) does not make sense for this formalism, so it is not defined. Figure 5 shows
different ways an execution e achieves, clobbers, and undoes an execution e0 . ` and ¬` point to
where they are asserted or required to be met.
2.6 External Conditions
As recognized by Tsuneto et al. (1998), external conditions are important for reasoning about potential refinements of abstract plans. Although the basic idea is the same, we define them a little
differently and call them external preconditions to differentiate them from other conditions that we
call external postconditions. Intuitively, an external precondition of a group of partially ordered
plans is a precondition of one of the plans that is not achieved by another in the group and must
be met external to the group. External postconditions, similarly, are those that are not undone by
plans in the group and are net effects of the group. Definition 6 states that ` is an external [pre,
post]condition of an execution e p if ` is a [pre, post]condition of a subplan for which it is not
[achieved, undone] by some other subplan.
Definition 6
external [pre, post]condition(`, e p ) ≡
∃h, E(h) = {e p } ∪ subex(e p ) →
(∃e p0 ∈ E(h), ` ∈ [pre(p0 ), post(p0 )]∧
6 ∃e p00 ∈ E(h),t,[achieves pre, undoes post]condition(e p00 , `, e p0 ,t, h))

464

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

For the example in Figure 2, available(G) is not an external precondition because, although G
must exist to produce H, G is supplied by the execution of the produce G plan. Thus, available(G)
is met internally, making available(G) an internal condition. available(M1) is an external precondition, an internal condition, and an external postcondition because it is needed externally and
internally; it is an effect of produce G on M1 which releases M1 when it is finished; and no other
plan in the decomposition undoes this effect.

3. Plan Summary Information
Summary information can be used to find abstract solutions that are guaranteed to succeed no matter
how they are refined because the information describes all potential conditions of the underlying
decomposition. Thus, some commitments to particular plan choices, whether for a single agent or
between agents, can be made based on summary information without worrying that deeper details
lurk beneath that will doom the commitments. While HTN planners have used abstract conditions
to guide search (e.g., Sacerdoti, 1974; Tsuneto et al., 1998), they rely on a user-defined subset of
constraints that can only help detect some potential conflicts. In contrast, summary information can
be used to identify all potential conflicts.
Having the formalisms of the previous section, we can now define summary information and
describe a method for computing it for non-primitive plans (in Section 3.4). Because there are
many detailed definitions and algorithms in this section, we follow the same structure here as in the
previous section, where we first give a more informal overview of the key concepts and notation,
into which we then subsequently delve more systematically.
3.1 Overview
The summary information of plan p consists of summary pre-, in-, and postconditions (presum (p),
insum (p), postsum (p)), summary resource usage (usagesum (p, res)) for each resource res, and whether
the plan can be executed in any way successfully (consistent).
A summary condition (whether pre, post, or in) specifies not only a positive or negated literal,
but additional modal information. Each summary condition has an associated existence, whose
value is either must or may depending on whether it must hold for all possible decompositions of
the abstract operator or just may hold depending on which decomposition is chosen. The timing of
a summary condition is either f irst, last, always, or sometimes, specifying when the condition must
hold in the plan’s interval of execution. A plan p1 must [achieve, clobber] summary precondition
c2 of p2 if the execution of p1 (or that of any plan with the same summary information) would
[achieve, clobber] a condition summarized by c2 (or any plan with the same summary information
as p2 ).
The algorithm for deriving summary conditions for plan p takes as input the summary conditions of the immediate subplans of p and the conditions defined for the CHiP p. The pre-, in-, and
postconditions of p become must first, must always, and must last summary conditions, respectively. The algorithm retains the existence and timing of subplan summary conditions in the parent
depending on whether the conditions are achieved, clobbered, or undone by siblings, whether the
decomposition is and or or, whether the subplan is ordered first or last, and whether all subplans
share the same condition. Subplan first, always, and last conditions can become sometimes conditions in the parent. The parent is computed as consistent as long as all subplans are consistent,

465

C LEMENT, D URFEE , & BARRETT

no subplan may clobber a summary condition of another, and summarized resources do not violate
limits.
We represent summary resource usage as three value ranges, hlocal min, local max, persisti,
where the resource’s local usage occurs within the task’s execution, and the persistent usage represents the usage that lasts after the task terminates for depletable resources. The summarization
algorithm for an abstract task takes the summary resource usages of its subtasks, considers all legal orderings of the subtasks, and all possible usages for all subintervals within the interval of the
abstract task, to build multiple usage profiles. These profiles are combined with algorithms for
computing parallel, sequential, and disjunctive usages to give the summary usage of the parent task.
3.2 Summary Conditions
The summary information for a plan p, psum , is a tuple hpresum , insum , postsum , usagesum , consistenti,
whose members are sets of summary conditions, summarized resource usage, and a consistent flag
indicating whether the plan will execute consistently internally. presum (p) and postsum (p) are summary pre- and postconditions, which are the external pre- and postconditions of p, respectively. The
summary inconditions of p, insum (p), contain all conditions that must hold within some execution
of p for it to be successful. A condition c in one of these sets is a tuple h`, existence,timingi. `(c)
is the literal of c. The existence of c can be must or may. If existence(c) = must, then c is called a
must condition because ` must hold for every successful plan execution. For convenience we usually
write must(c). c is a may condition (may(c) is true) if `(c) must hold for some successful execution.
The timing of a summary condition c can either be always, sometimes, f irst, or last. timing(c)
is always for c ∈ insum if `(c) is an incondition that must hold throughout all potential executions of p
(` holds always); otherwise, timing(c) = sometimes meaning `(c) holds at one point, at least, within
an execution of p. So, an always condition is must, and we do not define may always inconditions
because whether it is may because of existence or timing, it is not significantly different from may
sometimes in how a planner reasons about it. Whether a condition is may always (however defined)
or may sometimes, another plan can only have a may clobber relationship with the condition (as
defined in Section 3.3). Note also that the incondition of a CHiP has the restricted meaning of a
must always summary incondition. The timing is f irst for c ∈ presum if `(c) holds at the beginning
of an execution of p; otherwise, timing = sometimes. Similarly, timing is last for c ∈ postsum if `(c)
is asserted at the end of a successful execution of p; otherwise, it is sometimes. Although existence
and timing syntactically only take one value, semantically must(c) ⇒ may(c), and always(c) ⇒
sometimes(c).
We considered using modal logic operators to describe these concepts. While a mix of existing
temporal logic and dynamic logic (Pratt, 1976) notation could be forced to work, we found that
using our own terminology made definitions much simpler. We discuss this more at the end of
Section 8.
Definitions 7, 8, and 9 give the formal semantics of existence and timing for a few representative
condition types. Summary conditions of a plan are defined recursively in that they depend on the
summary conditions of the plan’s immediate subplans instead of its complete decomposition. Because a single description of summary information could represent many different plan hierarchies,
we quantify over plans p0 , whose subplans have the same summary information as those of the
plan p being summarized. We could have defined the existence and timing properties of conditions
based on the entire hierarchy, but in doing so, deriving summary conditions would be as expensive

466

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

as solving the planning problem, and one of the main purposes of summary information is to reduce
the computation of the planning problem. The reason why it would be so expensive is that in the
worst case all legal orderings of all plan steps must be explored to determine whether a condition is
must or may. We will discuss an example of this at the end of this subsection.
Definition 7
[must, may] f irst precondition(`, p) ≡
∀p0 = hpre(p), in(p), post(p), {},type(p), subplans(p0 ), order(p)i∧
summary in f ormation o f subplans(p0 ) = summary in f ormation f or subplans(p) →
∀h,e p0 , E(h) = {e p0 } ∪ subex(e p0 ) ∧ [true, external precondition(`, e p0 )] →
∃e p00 ∈ E(h),ts (e p00 ) = ts (e p0 ) ∧ ` ∈ pre(p00 )

Definition 8
must always incondition(`, p) ≡
∀p0 = hpre(p), in(p), post(p), {},type(p), subplans(p0 ), order(p)i∧
summary in f ormation o f subplans(p0 ) = summary in f ormation f or subplans(p) →
∀h, e p0 ,E(h) = {e p0 } ∪ subex(e p0 ),t,ts (e p0 ) < t < t f (e p0 ) →
∃e p00 ∈ E(h),ts (e p00 ) < t < t f (e p00 ) ∧ ` ∈ in(p00 )

Definition 9
[must, may] sometimes incondition(`, p) ≡
[∀, ∃]p0 = hpre(p), in(p), post(p), {},type(p), subplans(p0 ), order(p)i∧
summary in f ormation o f subplans(p0 ) = summary in f ormation f or subplans(p) [→, ∧]
[∀, ∃]h, e p0 ,E(h) = {e p0 } ∪ subex(e p0 ), ∃t,ts (e p0 ) < t < t f (e p0 )[→, ∧]
∃e p00 ∈ E(h), t = ts (e p00 ) ∧ ` ∈ pre(p00 )∨
ts (e p00 ) < t < t f (e p00 ) ∧ ` ∈ in(p00 )∨
t = t f (e p00 ) ∧ ` ∈ post(p00 )
Definition 7 states that a f irst precondition of p is an external precondition that is always required at the beginning of the execution of any p0 that has the same conditions as p and the same
summary information and ordering for its subplans as p. A last postcondition is always asserted at
the end of the execution (substitute “pre” with “post” and ts with t f in the last two lines of Definition 7). A [must,may] sometimes precondition is a [must,may] external precondition that is not a
f irst precondition. A sometimes postcondition is defined similarly. Definition 8 states that a literal
` is a must, always incondition of a plan p if at any time during any isolated execution of any p0
with the same summary information as p, some executing plan p00 has incondition `. Definition 9
states that a [must, may] sometimes incondition of plan p is a condition that is required during [any,
some] execution of [any, some] plan p0 that has the same summary information and ordering for its
subplans as p.
The consistent flag is a boolean indicating whether the plan (or any plan with the same summary information and ordering for subplans) would execute successfully no matter how it was decomposed and no matter when its subplans were executed. Definition 10 says that all possible
467

C LEMENT, D URFEE , & BARRETT

executions will succeed for a consistent plan. This is very similar to the CanAnyWay relation that
will be defined in Section 4. We do not include whether the plan will definitely not succeed in the
summary information because it requires an exponential computation to see whether all conflicts in
the subplans can be resolved. This computation can wait to be done during planning after summary
information is fully derived.
Definition 10
consistent(p) ≡
∀p0 = hpre(p), in(p), post(p), usage(p),type(p), subplans(p0 ), order(p)i∧
summary in f ormation o f subplans(p0 ) = summary in f ormation f or subplans(p) →
∀h, e p0 ∈ E (p0 ), e p0 succeeds
We show a subset of the summary conditions for the production manager’s top-level plan (of
Figure 2) below. Following each literal are modal tags for existence and timing information. “Mu”
is must; “Ma” is may; “F” is f irst; “L” is last; “S” is sometimes; and “A” is always.
Production manager’s produce H plan:
Summary preconditions:
available(A)MuF, available(M1)MaS, available(M2)MaS
Summary inconditions:
¬available(A)MuS, available(M1)MaS, available(M2)MuS, available(G)MuS,
available(A)MuS, ¬available(M1)MaS, ¬available(M2)MuS, ¬available(G)MuS,
available(H)MuS, ¬available(H)MuS
Summary postconditions:
¬available(A)MuS, available(M1)MaS, available(M2)MuS, ¬available(G)MuS,
available(H)MuL

The available(M1) summary precondition is a may condition because the production manager
may end up not using M1 if it chooses to use M2 instead to produce G. available(A) is a f irst summary precondition because part A must be used at the beginning of execution when it is transported
to one of the machines. Because the machines are needed sometime after the parts are transported,
they are sometimes (and not first) conditions: they are needed at some point in time after the beginning of execution.
Because the production manager may use M1 to produce G, ¬available(M1) is a summary
incondition of produce H. Having both available(M1) and ¬available(M1) as inconditions is
consistent because they are sometimes conditions, implying that they hold at different times during
the plan’s execution. In contrast, these conditions would conflict if they were both must and always
(meaning that they must always hold throughout every possible execution of the plan).
The summary condition ¬available(A) is a must postcondition of the top-level plan because A
will definitely be consumed by make G and is not produced by some other plan in the decomposition
of produce H f rom G. Even though available(G) is an effect of produce G, it is not an external
postcondition of produce H because it is undone by produce H f rom G, which consumes G to
make H. available(H) is a last summary postcondition because the production manager releases
H at the very end of execution. available(M2) is not last because the manager finishes using M2
before moving H into storage.
Notice that available(M2) is a may summary precondition. However, no matter how the hierarchy is decomposed, M2 must be used to produce H, so available(M2) must be established
468

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

externally to the production manager’s plan. Because summary information is defined in terms of
the summary information of the immediate subplans, in the subplans of produce H, we only see
that produce G has an available(M2)MaS precondition and an available(M2)MaS postcondition
that would achieve the available(M2)MuF precondition of produce H f rom G. This summary
information does not tell us that the precondition of produce G exists only when the postcondition
exists, a necessary condition to determine that the derived precondition of produce H is a must
condition. Thus, it is may. If we augmented summary information with which subsets of conditions
existed together, hunting through combinations and temporal orderings of condition subsets among
subplans to derive summary conditions would basically be an adaptation of an HTN planning algorithm, which summary information is intended to improve. Instead, we derive summary information
in polynomial time and then use it to improve HTN planning exponentially as we explain in Section 6. This is the tradeoff we made at the beginning of this section in defining summary conditions
in terms of just the immediate subplans instead of the entire hierarchy. Abstraction involves loss of
information, and this loss enables computational gains.
3.3 Summary condition relationships and algorithms
In order to derive summary conditions according to their definitions, we need to be able to recognize
achieve, clobber, and undo relationships based on summary conditions as we did for basic CHiP
conditions. We give definitions and algorithms for these, which build on constructs and algorithms
for reasoning about temporal relationships, described in Appendix A.
Achieving and clobbering are very similar, so we define them together. Definition 11 states that
plan p1 must [achieve, clobber] summary precondition c2 of p2 if and only if for all executions of
any two plans, p01 and p02 , with the same summary information and ordering constraints as p1 and
p2 , the execution of p01 or one of its subexecutions would [achieve, clobber] an external precondition
`(c2 ) of p02 .
Definition 11
must [achieve, clobber] precondition(p1 , c2 , p2 , Psum , order) ≡
∀h ∈H(Psum , order), p01 , p02 , e p01 , e p02 ,
(p01 and p02 have same summary and ordering in f ormation as p1 and p2 ) →
∃t,e p001 ∈ subex(e p01 ), e p002 ∈ subex(e p02 ),
[achieve, clobber] precondition(e p001 , `(c2 ), e p002 ,t, h) ∧
external precondition(`(c2 ), e p02 )
Achieving and clobbering in- and postconditions are defined the same as Definition 11 but substituting “in” and “post” for “pre” and removing the last line for inconditions. Additionally substituting ∃ for ∀ gives the definitions of may achieve and clobber. Furthermore, the definitions of
must/may-undo are obtained by substituting “post” for “pre” and “undo” for “achieve” in Definition 11. Note that, as mentioned in Section 2.5, achieving inconditions and postconditions does not
make sense for this formalism.
Algorithms for these interactions are given in Figure 6 and Figure 7. These algorithms build
on others (detailed in Appendix B) that use interval point algebra to determine whether a plan must
or may assert a summary condition before, at, or during the time another plan requires a summary
condition to hold. Similar to Definition 3 of must-achieve for CHiP conditions, Figure 6 says that p0
469

C LEMENT, D URFEE , & BARRETT

Algorithm: Must-[achieve, clobber]
Input: plan p0 , summary condition c of plan p, Psum , and order
Output: true or f alse, whether p0 must-[achieve, clobber] c
begin function
for each c0 ∈ in(p0 ) ∪ post(p0 )
if `(c0 ) ⇔ [`(c), ¬`(c)] ∧ must(c0 ) then
if c ∈ insum (p) ∧ p0 must-assert c0 in c then return [unde f ined, true]
if c ∈ postsum (p) ∧ p0 must-assert c0 when c then return [unde f ined, true]
if c ∈ presum (p) ∧ p0 must-assert c0 by c then
set assertion inbetween = f alse
for each c00 ∈ in(p00 ) ∪ post(p00 ), p00 ∈ Psum while assertion inbetween = f alse
if (p0 may-assert c0 before c00 ∧
p00 may-assert c00 by c ∧
`(c00 ) ⇔ [¬`(c), `(c)]) ∨
(p0 must-assert c0 before c00 ∧
p00 must-assert c00 by c ∧
`(c00 ) ⇔ [`(c), ¬`(c)] ∧ must(c00 )) then
set assertion inbetween = true
if ¬assertion inbetween then return true
return f alse
end function

Figure 6: Algorithm for whether a plan must achieve or clobber a summary condition
achieves summary condition c if it must asserts the condition before it must hold, and there are no
other plans that may assert the condition or its negative in between. The algorithm for may-achieve
(in Figure 7) mainly differs in that p0 may assert the condition beforehand, and there is no plan that
must assert in between. The undo algorithms are the same as those for achieve after swapping c and
c0 in all must/may-assert lines.
The complexity of determining must/may-clobber for inconditions and postconditions is simply
O(c) to check c conditions in p0 . If the conditions are hashed, then the algorithm is constant time.
For the rest of the algorithm cases, the complexity of walking through the summary conditions
checking for p00 and c00 is O(nc) for a maximum of c summary conditions for each of n plans
represented in Psum . In the worst case, all summary conditions summarize the same propositional
variable, and all O(nc) conditions must be visited.
Let’s look at some examples of these relationships. In Figure 8a, p0 = equip M2 tool mayclobber c = available(M2)MaS in the summary preconditions of p = produce G because there is
some history where equip M2 tool ends before produce G starts, and calibrate M2 starts after
produce G starts. In Figure 8b, p0 = build H must-achieve c = available(H)MuF in the summary preconditions of p = move H. Here, c0 is available(H)MuL in the summary postconditions
of build H. In all histories, build H attempts to assert c0 before the move H requires c to be
met, and there is no other plan execution that attempts to assert a condition on the availability
of H. equip M2 tool does not may-clobber c = available(M2)MuF in the summary preconditions
of build H even though equip M2 tool asserts c0 = ¬available(M2)MuL before c is required to
be met. This is because calibrate M2 must assert ¬available(M2)MuA between the time that
equip M2 tool asserts c0 and when c is required. Thus, calibrate M2 must-undo equip M2 tool ’s

470

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

Algorithm: May-[achieve, clobber]
Input: plan p0 , summary condition c of plan p
Output: true or f alse, whether p0 may-[achieve, clobber] c
begin function
for each c0 ∈ in(p0 ) ∪ post(p0 )
if `(c0 ) ⇔ [`(c), ¬`(c)] then
if c ∈ insum (p) ∧ p0 may-assert c0 in c then return [unde f ined, true]
if c ∈ postsum (p) ∧ p0 may-assert c0 when c then return [unde f ined, true]
if c ∈ presum (p) ∧ p0 may-assert c0 by c then
set assertion inbetween = f alse
for each c00 ∈ in(p00 ) ∪ post(p00 ), p00 ∈ Psum while assertion inbetween = f alse
if p0 must-assert c0 before c00 ∧
p00 must-assert c00 by c ∧
`(c00 ) ⇔ `(c) or ¬`(c) ∧ must(c00 )) then
set assertion inbetween = true
if ¬assertion inbetween then return true
return f alse
end function

Figure 7: Algorithm for whether a plan may achieve or clobber a summary condition
a)

produce H
produce G

produce H from G
move G

build H

move H

service M2
move

equip M2

tool

calibrate M2

tool

b)

produce H
produce H from G

produce G

move G
=
move
tool

build H

move H

service M2

equip M2

calibrate M2

tool

Figure 8: The production and facilities managers’ plans partially expanded. a) The managers’ plans
unordered with respect to each other. b) equip M2 tool must clobber available(M2)MaL
of produce G, and calibrate M2 must clobber available(M2)MuF of build H.

summary postcondition. Because calibrate M2 cannot assert its postcondition available(M2)MuL
before build H requires available(M2)MuF, calibrate M2 must-clobber the summary precondition.

471

C LEMENT, D URFEE , & BARRETT

3.4 Deriving Summary Conditions
Now that we have algorithms that determine interactions of abstract plans based on their summary
conditions, we can create an algorithm that derives summary conditions according to their definitions in Section 3.2. Figure 9 shows pseudocode for the algorithm. The method for deriving the
summary conditions of a plan p is recursive. First, summary information is derived for each of p’s
subplans. Then conditions are added based on p’s own conditions. Most of the rest of the algorithm
derives summary conditions from those of p’s subplans. Whether p is consistent depends on the
consistency of its subplans and whether its own summary conditions and resource usages are in
conflict. The braces ’{’ ’}’ used here have slightly different semantics than used before with the
brackets. An expression {x,y} can be interpreted simply as (x or y, respectively).
Definitions and algorithms for temporal relationships such as always- f irst and covers are in
Appendix A. When the algorithm adds or copies a condition to a set, only one condition can exist
for any literal, so a condition’s information may be overwritten if it has the same literal. In all cases,
must overwrites may; and f irst, last, and always overwrite sometimes; but, not vice-versa. Further,
because it uses recursion, this procedure is assumed to work on plans whose expansion is finite.
3.5 Summary Resource Usage
In this section, we define a representation for capturing ranges of usage both local to the task interval and the depleted usage lasting after the end of the interval. Based on this we introduce a
summarization algorithm that captures in these ranges the uncertainty represented by decomposition choices in or plans and partial temporal orderings of and plan subtasks. This representation
allows a coordinator or planner to reason about the potential for conflicts for a set of tasks. We will
discuss this reasoning later in Section 4.2. Although referred to as resources, these variables could
be durations or additive costs or rewards.
3.5.1 R EPRESENTATION
We start with a new example for simplicity that motivates our choice of representation. Consider
the task of coordinating a collection of rovers as they explore the environment around a lander on
Mars. This exploration takes the form of visiting different locations and making observations. Each
traversal between locations follows established paths to minimize effort and risk. These paths combine to form a network like the one mapped out in Figure 10, where vertices denote distinguished
locations, and edges denote allowed paths. Thinner edges are harder to traverse, and labeled points
have associated observation goals. While some paths are over hard ground, others are over loose
sand where traversal is harder since a rover can slip.
Figure 11 gives an example of an abstract task. Imagine a rover that wants to make an early
morning trip from point A to point B on the example map. During this trip the sun slowly rises
above the horizon giving the rover the ability to progressively use soak rays tasks to provide more
solar power (a non-consumable resource3 ) to motors in the wheels. In addition to collecting photons,
the morning traverse moves the rover, and the resultant go tasks require path dependent amounts of
power. While a rover traveling from point A to point B can take any number of paths, the shortest
three involve following one, two, or three steps.
3. It is important not to confuse power with battery energy. A power source (e.g. battery, solar panels) makes a fixed
amount of power in Watts available at any point in time. A battery’s energy (in Watt-hours) is reduced by the integral
of the total use of this power over time.

472

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

Algorithm: Derive summary information
Input: plan p
Output: psum
begin function
derive summary information
for each p0 ∈ d(p)
V
set consistent(p) = p0 ∈d(p) consistent(p0 )
for each ` ∈ pre(p) add h`, must, f irsti to presum (p)
for each ` ∈ in(p) add h`, must, alwaysi to insum (p)
for each ` ∈ post(p) add h`, must, lasti to postsum (p)
for each summary condition c0 of p0 ∈ d(p)
set c = c0
if c0 ∈ {presum (p0 ),postsum (p0 )} and
c0 is not must-{achieved,undone} or must-clobbered within d(p), then
if type(p) = and and (p0 is always not the { f irst,last}
temporally ordered subplan according to order(p) or
there is a sometimes- { f irst,last} subplan p0 that
does not have a { f irst, last} `(c0 ) condition in {presum (p0 ),postsum (p0 )}), then
set timing(c) = sometimes
if c0 is may-{achieved,undone} or may-clobbered by each of P ⊂ d(p) and
not all p00 ∈ P have a must `(c0 ) condition in {presum (p00 ),postsum (p00 )}, then
set existence(c) = may
copy c to {presum (p),postsum (p)}
if c0 ∈ insum (p0 ) or p0 is not-always { f irst,last} according to order(p), then
if must(c0 ) and c0 is always-not- { f irst,last} according to order(p), then
set existence(c) = must
set P = 0/
set allAlways = true
for each p00 ∈ d(p), c00 ∈ insum (p00 )
if `(c00 ) ⇔ `(c)
if always(c00 ) then add p00 to P
else set allAlways = f alse
else allAlways = f alse
if always(c) and ((type(p) = and and P covers p according to order(p)) or
(type(p) = or and allAlways)), then
set timing(c) = always
add c to insum (p)
if c0 is may-clobbered, then set consistent = f alse
usagesum (p) = SummarizeResourceUsage(p) (in Section 3.5.2)
if consistent(usagesum (p)) = f alse, then set consistent(p) = f alse
end function

Figure 9: Algorithm for deriving summary information
A summarized resource usage consists of ranges of potential resource usage amounts during
and after performing an abstract task, and we represent this summary information for a plan p on a
resource res using the structure
usagesum (p, res) = hlocal min(p, res), local max(p, res), persist(p, res)i,

473

C LEMENT, D URFEE , & BARRETT

A

D
B
C
F
E

Figure 10: Example map of established paths between points in a rover domain
morning activities
move(A,B)
soak rays soak rays soak rays
use -4w use -5w use -6w
20 min
20 min
20 min
go(A,1)
use 3w
10 min

take low path

go(1,2)
use 3w
10 min

high path
middle path
go(A,B)
go(2,B) use 4w go(A,3) go(3,B)
use 6w 50 min use 4w use 6w
20 min
15 min 25 min

Figure 11: and/or tree defining a rover’s tasks and their resource usages
where the resource’s local usage occurs within p’s execution, and the persistent usage represents
the usage that lasts after the execution terminates for consumable resources.
Definition 12
usagesum (p, res) ≡
h[minh∈H,e p ∈E(h) (mints (e p )<t<t f (e p ) (−r(res, h,t))), maxh∈H,e p ∈E(h) (mints (e p )<t<t f (e p ) (−r(res, h,t)))]
[minh∈H,e p ∈E(h) (maxts (e p )<t<t f (e p ) (−r(res, h,t))), maxh∈H,e p ∈E(h) (maxts (e p )<t<t f (e p ) (−r(res, h,t)))]
[minh∈H,e p ∈E(h) (−r(res, h,t f (e p ))),
maxh∈H,e p ∈E(h) (−r(res, h,t f (e p )))]
i
The context for Definition 12 is the set of all histories H where the value of res is 0 in the initial
state, and E(h) only contains the execution of p and its subexecutions. Thus, the −r(res, h,t) term
is the combined usage of res at time t of all executions in the hierarchy as defined in Section 2.4. So,
the maximum of the local min is the highest among all histories of the lowest point of usage during
p’s execution. The usage ranges capture the multiple possible usage profiles of a task with multiple
decomposition choices and timing choices among loosely constrained subtasks. For example, the
high path task has a h[4,4],[6,6],[0,0]i summary power use over a 40 minute interval. In this case
the ranges are single points due to no uncertainty – the task simply uses 4 watts for 15 minutes
followed by 6 watts for 25 minutes. The move(A,B) task provides a slightly more complex example
due to its decompositional uncertainty. This task has a h[0,4],[4,6],[0,0]i summary power use over
a 50 minute interval. In both cases the persist is [0,0] because solar power is a non-consumable
resource.
As an example of reasoning with resource usage summaries, suppose that only 3 watts of power
were available during a move(A,B) task. Given the [4,6] local max, we know that there is not
enough power no matter how the task is decomposed. Raising the available power to 4 watts makes
the task executable depending on how it gets decomposed and scheduled, and raising to 6 or more
watts makes the task executable for all possible decompositions.
474

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

This representation of abstract (or uncertain) metric resource usage can be seen as an extension
of tracking optimistic and pessimistic resource levels (Drabble & Tate, 1994). Computing only
the upper and lower bounds on resource usage for an abstract plan gives some information about
whether lower or upper bound constraints on a resource may, must, or must not be violated, but
it is not complete. By representing upper and lower bounds as ranges of these bounds over all
potential histories, we can certainly know whether bounds may, must, or must not be violated over
all histories. For the example above, if we only tracked one range for the local usage, [0,6], we
would not know that there is definitely a conflict when only 3 watts are available. Knowing this
extra information can avoid exploration of an infeasible search space.
3.5.2 R ESOURCE S UMMARIZATION A LGORITHM
The state summarization algorithm in Section 3.4 recursively propagates summary conditions upwards from an and/or hierarchy’s leaves, and the algorithm for resource summarization takes the
same approach. Starting at the leaves, the algorithm finds primitive tasks that use constant amounts
of a resource. The resource summary of a task using x units of a resource is h[x,x],[x,x],[0,0]i or
h[x,x],[x,x],[x,x]i over the task’s duration for non-consumable or consumable resources respectively.
Moving up the and/or tree, the summarization algorithm either comes to an and or an or branch.
For an or branch the combined summary usage comes from the or computation
h [minc∈children (lb(local min(c))), maxc∈children (ub(local min(c)))],
[minc∈children (lb(local max(c))), maxc∈children (ub(local max(c)))],
[minc∈children (lb(persist(c))),
maxc∈children (ub(persist(c)))]
i,

where lb() and ub() extract the lower bound and upper bound of a range respectively. The children
denote the branch’s children with their durations extended to the length of the longest child. This
duration extension alters a child’s resource summary information because the child’s usage profile
has a zero resource usage during the extension. For instance, in determining the resource usage
for move(A,B), the algorithm combines two 40 minute tasks with a 50 minute task. The resulting
summary information describes a 50 minute abstract task whose profile might have a zero watt
power usage for 10 minutes. This extension is why move(A,B) has a [0,4] for a local min instead
of [3,4]. Planners that reason about variable durations could use [3,4] for a duration ranging from
40 to 50.
Computing an and branch’s summary information is a bit more complicated due to timing
choices among loosely constrained subtasks. The take x path examples illustrate the simplest subcase, where subtasks are tightly constrained to execute serially. Here profiles are appended together,
and the resulting summary usage information comes from the SERIAL-AND computation
h [minc∈children (lb(local min(c)) + Σlb (c)), minc∈children (ub(local min(c)) + Σub (c))],
pre
pre
[maxc∈children (lb(local max(c)) + Σlb (c)), maxc∈children (ub(local max(c)) + Σub (c))],
[Σc∈children (lb(persist(c))),
Σc∈children (ub(persist(c)))]
i,
pre

pre

pre
pre
where Σlb
(c) and Σub
(c) are the respective lower and upper bounds on the cumulative persistent usages of children that execute before c. These computations have the same form as the Σ
computations for the final persist.
The case where all subtasks execute in parallel and have identical durations is slightly simpler.
Here the usage profiles add together, and the branch’s resultant summary usage comes from the

475

C LEMENT, D URFEE , & BARRETT

move(A,B)
soak rays
<[-4,-4],[-4,-4],[0,0]>

<[0,4],[4,6],[0,0]>

soak rays
<[-5,-5],[-5,-5],[0,0]>

soak rays
<[-6,-6],[-6,-6],[0,0]>

Figure 12: Possible task ordering for a rover’s morning activities, with resulting subintervals.
PARALLEL-AND computation
h [Σc∈children (lb(local min(c))),
maxc∈children (ub(local min(c)) + Σnon
ub (c))],
non
[minc∈children (lb(local max(c)) + Σlb (c)), Σc∈children (ub(local max(c)))],
[Σc∈children (lb(persist(c))),
Σc∈children (ub(persist(c)))]
i,
non
where Σnon
ub (c) and Σlb (c) are the respective sums of the local max upper bounds and the local min
lower bounds for all children except c.
To handle and tasks with loose temporal constraints, we consider all legal orderings of child
task endpoints. For example, in the rover’s early morning tasks, there are three serial solar energy collection subtasks running in parallel with a subtask to drive to location B. Figure 12 shows
one possible ordering of the subtask endpoints, which breaks move(A,B) into three pieces, and
two of the soak rays children in half. Given an ordering, the summarization algorithm can (1)
use the endpoints of the children to determine subintervals, (2) compute summary information for
each child task/subinterval combination, (3) combine the parallel subinterval summaries using the
PARALLEL-AND computation, and then (4) chain the subintervals together using the SERIALAND computation. Finally, the and task’s summary is computed by combining the summaries for
all possible orderings using an or computation.
Here we describe how step (2) generates different summary resource usages for the subintervals
of a child task. A child task with summary resource usage h[a,b],[c,d],[e, f ]i contributes one of two
summary resource usages to each intersecting subinterval4 :

h[a, b], [c, d], [0, 0]i, h[a, d], [a, d], [0, 0]i.

While the first usage has the tighter [a,b],[c,d] local ranges, the second has looser [a,d],[a,d] local
ranges. Since the b and c bounds only apply to the subintervals containing the subtask’s minimum
and maximum usages, the tighter ranges apply to one of a subtask’s intersecting subintervals. While
the minimum and maximum usages may not occur in the same subinterval, symmetry arguments
let us connect them in the computation. Thus one subinterval has tighter local ranges and all other
intersecting subintervals get the looser local ranges, and the extra complexity comes from having
to investigate all subtask/subinterval assignment options. For instance, there are three subintervals
intersecting move(A,B) in Figure 12, and three different assignments of summary resource usages
to the subintervals: placing [0,4],[4,6] in one subinterval with [0,6],[0,6] in the other two. These
placement options result in a subtask with n subintervals having n possible subinterval assignments.
So if there are m child tasks each with n alternate assignments, then there are nm combinations
of potential subtask/subinterval summary resource usage assignments. Thus propagating summary
information through an and branch is exponential in the number of subtasks with multiple internal
4. For summary resource usages of the last interval intersecting the child task, we replace [0, 0] with [e, f ] in the persist.

476

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

subintervals. However since the number of subtasks is controlled by the domain modeler and is
usually bounded by a constant, this computation is tractable. In addition, summary information can
often be derived offline for a domain. The propagation algorithm takes on the form:
• For each consistent ordering of endpoints:
– For each consistent subtask/subinterval summary usage assignment:
∗ Use PARALLEL-AND computations to combine subtask/subinterval summary
usages by subinterval.
∗ Use a SERIAL-AND computation on the subintervals’ combined summary usages
to get a consistent summary usage.
• Use or computation to combine all consistent summary usages to get and task’s summary
usage.
Now that we have described how to derive summary information, we can discuss how to use it.

4. Identifying Abstract Solutions
Up to this point, we have detailed algorithms for deriving summary conditions and for reasoning
about potential (may) and definite (must) interactions between tasks based on their summary information. In addition, we have outlined algorithms for deriving summarized resource usage but
have not yet discussed how to identify solutions at abstract levels. In this section, we show how
the interactions of summary conditions and summarized metric resource usages identify potentially
resolvable threats and unresolvable conflicts among the plans of a group of agents.
4.1 Threats on Summary Conditions
Agents can attempt to resolve conflicts among their plans by considering commitments to particular
decompositions and ordering constraints. In order to do this, the agents must be able to identify
remaining conflicts (threats) among their plans. Here we present simple algorithms for reasoning
about threats between abstract plans and their required conditions.
Formally, for a set of CHiPs P with ordering constraints order, a threat between an abstract plan
p ∈ P and a summary condition c0 of another plan p0 ∈ P exists iff p may-clobber c0 . We say that the
threat is unresolvable if p must-clobber c0 and must(c0 ) because there are no decomposition choices
or ordering constraints that could be added to resolve the threat.
So, a simple algorithm for identifying threats is to check to see if each of the O(nc) summary
conditions of n plans in Psum is must- or may-clobbered by any other plan. Since the complexity
of checking to see if a particular condition is must- or may-clobbered is O(nc), this algorithm’s
complexity is O(n2 c2 ).
In many coordination tasks, if agents could determine that under certain temporal constraints
their plans can be decomposed in any way (CanAnyWay) or that under those constraints there is
no way they can be successfully decomposed (¬MightSomeWay), then they can make coordination
decisions at abstract levels without entering a potentially costly search for valid plan merges at lower
levels. Here are the formal definitions of CanAnyWay and MightSomeWay:

477

C LEMENT, D URFEE , & BARRETT

a)

produce H

b)

maintenance

produce H
maintenance
move_parts

move_parts
c)

produce H
produce G

produce H from G
maintenance
service M1 M2
service

service

M1

M2

move
tool

move_parts

Figure 13: The top-level plans of each of the managers for the manufacturing domain
Definition 13
[CanAnyWay, MightSomeWay](order, Psum ) ≡
[∀, ∃]h, P with summary in f ormation = Psum ∧ h ∈ H(P, order) →
[∀, ∃]e ∈ E(h), succeeds(e, h)
Definition 13 states that the plans with summary information Psum under ordering constraints
can execute in any way if and only if all sets of plans P that have summary information Psum
will execute successfully in any history. MightSomeWay is true if there is some set of plans
that could possibly execute successfully. We could also describe CanSomeWay(order,Psum ) and
MightAnyWay(rel,Psum ) in the same fashion, but it is not obvious how their addition could further
influence search. Exploring these relations may be an interesting topic for future research.
In Figure 13a, the three top-level plans of the managers are unordered with respect to each other.
The leaf plans of the partially expanded hierarchies comprise Psum . Arrows represent the constraints
in order. CanAnyWay({},{produce G, maintenance, move parts}) is false because there are several conflicts over the use of machines and transports that could occur for certain executions of
the plans as described in Section 3.3 for Figure 8. However, MightSomeWay({}, {produce G,
maintenance, move parts}) is true because the plans might in some way execute successfully
as shown in Figure 13b. With the ordering constraints in Figure 13b, CanAnyWay({before(1,0),
before(0,2)},{produce G, maintenance, move parts}) is true because the plans can execute in any
way consistent with these ordering constraints without conflict. Figure 8b is an example where
MightSomeWay is false because calibrate M2 must-clobber the available(M2)MuF summary precondition of build H.
As shown in Figure 14, the algorithm for determining CanAnyWay for summary conditions
is simple in that it only needs to check for threats. MightSomeWay is more complicated because
just checking for an unresolvable threat is not enough. As shown in Figure 15, it is not the case
that plan p must clobber p0 because p00 could come between and achieve the precondition ` of p0 .
Thus, p may-clobbers ` in p and in p00 . However, obviously p will clobber one or the other, so
478

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

Algorithm: [CanAnyWay, MightSomeWay]
Input: order, Psum
Output: true or f alse
begin function
for each psum ∈ Psum
if [¬consistent(psum ), f alse] then return f alse
for each p0sum ∈ Psum
for each summary condition c of psum
if p0 [may-clobber, must-clobber] c, and
c is [may or must, must], then
return f alse
for each resource res
if ¬[CanAnyWay, MightSomeWay](order, Psum , res) (see Section 4.2) then
return false
return true
end function

Figure 14: Algorithm determining whether plans with the given summary information CanAnyWay
or MightSomeWay execute successfully.
p’
l

p
-l

l

l

p’’
l

l

l

Figure 15: MightSomeWay is false even though there is no must-clobber relationship.
MightSomeWay is false. In order to determine MightSomeWay is f alse, an agent must exhaustively
search through an exponential number of schedules to see if not all conflicts can be resolved. Instead
of performing an exponential search to determine MightSomeWay, we use the simple algorithm in
Figure 14 that just checks for must-clobber relationships. In Section 5.1 we describe a more flexible
search to find conflict-free abstract plans than just scheduling at an abstract level.
Thus, while the CanAnyWay algorithm is sound and complete, the MightSomeWay algorithm
is complete but not sound. This also means that determining ¬MightSomeWay is sound but not
complete. We will still make use of both of these algorithms in a sound and complete planning/coordination algorithm in Section 5.1. The complexity of these algorithms is O(n2 c2 ) since
the O(nc) procedures for determining must/may-clobber must be run for each of nc conditions (c
summary conditions in each of n plans represented by Psum ).
4.2 Summary Resource Usage Threats
Planners detect threats on resource constraints in different ways. If the planner reasons about partially ordered actions, it must consider which combinations of actions can overlap and together
exceed (or fall below) the resource’s maximum value (or minimum value). A polynomial algorithm

479

C LEMENT, D URFEE , & BARRETT

does this for the IxTeT planner (Laborie & Ghallab, 1995). Other planners that consider total order plans can more simply project the levels of the resource from the initial state through the plan,
summing overlapping usages, to see if there are conflicts (e.g., Chien et al., 2000b).
Finding conflicts involving summarized resource usages can work in the same way. For the
partial order planner, the resultant usage of clusters of actions are tested using the PARALLELAND algorithm in Section 3.5. For the total order planner, the level of the resource is represented
as a summarized usage, initially h[x, x], [x, x], [x, x]i for a consumable resource with an initial level
x and h[x, x], [x, x], [0, 0]i for a non-consumable resource. Then, for each subinterval between
start and end times of the schedule of tasks, the summary usage for each is computed using the
PARALLEL-AND algorithm. Then the level of the resource is computed for each subinterval while
propagating persistent usages using the SERIAL-AND algorithm.
We can decide CanAnyWay and MightSomeWay as defined in Section 4.1, in terms of the summary usage values resulting from invocations of PARALLEL-AND and SERIAL-AND in the propagation algorithm at the end of Section 3.5.2. CanAnyWay(order, Psum , res) is true if and only if
there are no potential threats. These algorithms discover a threat if they ever compute an interval i
such that
lb(local min(i)) < min value(res) ∨ lb(persist(i)) < min value(res) ∨
ub(local max(i)) > max value(res) ∨ ub(persist(i)) > max value(res).

MightSomeWay(order, Psum , res) is true if and only if there is a possible run with potentially no
threats. SERIAL-AND discovers such a run if it returns a summary usage where
ub(local min(i)) ≥ min value(res) ∧ lb(persist(i)) ≥ min value(res) ∧
lb(local max(i)) ≤ max value(res) ∧ ub(persist(i)) ≤ max value(res).

Now that we have mechanisms for deriving summary information and evaluating plans based
on their summarizations, we will discuss how to exploit them in a planning/coordination algorithm.

5. Hierarchical Planning and Coordination Algorithm
With the earlier defined algorithms for reasoning about a group of agents’ plans at multiple levels
of abstraction, we now describe how agents can efficiently plan and coordinate based on summary
information. We describe a coordination algorithm that searches for ways to restrict the decomposition and ordering of the collective actions of the agent(s) in order to resolve conflicts while
maximizing the utilities of the individual agents or the global utility of the group.
Our approach starts by making planning decisions at the most abstract level and, as needed,
decomposes the agents’ plans in a top-down fashion. The idea is to introduce only the information that is needed. Introducing irrelevant details complicates search and increases communication.
After describing the top-down planning/coordination algorithm, we describe search techniques and
heuristics that the algorithm can use to further exploit summary information.
5.1 Top-Down Hierarchical Planning and Coordination
The formalism of summary conditions culminated in Section 4 in algorithms that determine if a set
of plans (abstract or primitive) under a partial set of ordering constraints is definitely conflict-free
(CanAnyWay) or has unresolvable conflicts (¬MightSomeWay). Here we integrate these algorithms
into one that searches for a consistent plan for one or more agents. The particular algorithm we
describe here is shown to be sound and complete (Clement, 2002). The search starts out with
the top-level plans of each agent. A solution is one where there are no possible conflicts among the
480

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

agents’ plans. The algorithm tries to find a solution at this top level and then expands the hierarchies
deeper and deeper until the optimal solution is found or the search space has been exhausted. A
pseudocode description of the algorithm is given in Figure 16.
A state of the search is a partially elaborated plan that we represent as a set of and plans (one for
each agent), a set of temporal constraints, and a set of blocked plans. The subplans of the and plans
are the leaves of the partially expanded hierarchies of the agents. The set of temporal constraints
includes synchronization constraints added during the search in addition to those dictated by the
agents’ individual hierarchical plans. Blocked subplans keep track of pruned or subplans.
Decisions can be made during search in a decentralized fashion. The agents can negotiate over
ordering constraints to adopt, over choices of subplans to accomplish higher level plans, and over
which decompositions to explore first. While the algorithm described here does not specify (or
commit to) any negotiation technique, it does provide the mechanisms for identifying the choices
over which the agents can negotiate. Although agents can make search decisions in a decentralized fashion, we describe the algorithm given here as a centralized process that requests summary
information from the agents being coordinated.
In the pseudocode in Figure 16, the coordinating agent collects summary information about the
other agents’ plans as it decomposes them. The queue keeps track of expanded search states. If the
CanAnyWay relation holds for the search state, the Dominates function determines if the current
solutions are better for every agent than the solution represented by the current search state and
keeps it if the solution is not dominated. If MightSomeWay is false, then the search space rooted at
the current search state can be pruned; otherwise, the coordinator applies operators to generate new
search states.
The operators for generating successor search states are expanding non-primitive plans, blocking or subplans, and adding temporal constraints on pairs of plans. When an agent expands one
of its plans, each of the plan’s summary conditions are replaced with only the original conditions
of the parent plan. Then the subplans’ summary information and ordering constraints are added to
the search state. A subplan of an or plan is added (or selected) only when all other subplans are
blocked. When ApplyOperator is called for the select and block operators, search states are
generated for each selectable and blockable subplan, respectively. Blocking an or subplan can be
effective in resolving a constraint in which the other or subplans are not involved. For example, if
the inventory manager plans to only use transport2, the production manager could block subplans
using transport2, leaving subplans using transport1 that do not conflict with the inventory manager’s
plan. This can lead to least commitment abstract solutions that leave the agents flexibility in selecting among the multiple applicable remaining subplans. The agents can take another approach by
selecting a subplan (effectively blocking all of the others) to investigate a preferred choice or one
that more likely avoids conflicts.
When the operator is to add a temporal constraint, a new search state is created for each alternative temporal constraint that could be added. These successor states are enqueued so that if
backtracking is needed, each alternative can be tried. Adding temporal constraints should only generate new search states when the ordering is consistent with the other global and local constraints.
In our implementation, we only add constraints that will help resolve threats as determined by the
must/may achieves and clobbers algorithms. When a plan is expanded or selected, the ordering
constraints must be updated for the subplans that are added.
The soundness and completeness of the coordination algorithm depends on the soundness and
completeness of identifying solutions and the complete exploration of the search space. Soundness
481

C LEMENT, D URFEE , & BARRETT

Concurrent Hierarchical Coordination Algorithm
Input: set of top-level plans, initial state
Output: set of solutions, each a pair of order constraints and blocked plan choices
begin function
summarized plans = 0/
for each plan p ∈ plans
p0 = get summary information for plan p
summarized plans = summarized plans ∪ { p0 }
end for
threats = { (p, p0 ) | p, p0 ∈ summarized plans, MayClobber(p, p0 ) }
/ 0,
/ threats) }
queue = { (0,
solutions = 0/
loop
if queue == 0/
return solutions
(order, blocked, threats) = Pop(queue)
if CanAnyWay(initial state, summarized plans, order, blocked)
solution = (order, blocked)
solutions = solutions ∪ {solution}
for each sol1 and sol2 in solutions
if Dominates(sol1 , sol2 )
solutions = solutions - { sol2 }
if MightSomeWay(initial state, summarized plans, order, blocked)
operator = Choose({expand, select, block, constrain})
queue = queue ∪ ApplyOperator(operator, summarized plans, order, blocked)
return solutions
end function

Figure 16: A concurrent hierarchical coordination algorithm.
and completeness is not defined with respect to achieving particular goal predicates but resolving
conflicts in the plan hierarchies. A domain modeler may represent goals as abstract CHiPs that
decompose into possible plans that accomplish them or as a series of actions for an agent to execute
successfully.
Consider how the algorithm would find coordinated plans for the manufacturing agents. At the
beginning of the search, a coordinating agent gathers the summary information for the top-level
plans of the three agents in plans. At first, there are no ordering constraints, so order is empty
in the first search state (shown in Figure 13a) popped from the queue. CanAnyWay is false, and
MightSomeWay is true for this state as described earlier in this section, so the coordinator chooses
an operator to apply to the search state. It could choose constrain and order the maintenance
plan before produce H to resolve all conflicts between those two plans. The order is updated with
the new constraint, and the new search state is inserted into the queue by according to some ranking
function. On the next iteration of the loop, the only search state in the queue that was just inserted
is popped. The coordinator again finds that CanAnyWay is false, and MightSomeWay is true since
move parts may still conflict with other plans over the use of transports. It can choose to constrain
produce H before move parts to resolve the remaining conflicts. This is detected on the next cycle
of the search loop where CanAnyWay is found to be true for this search state (shown in Figure 13b).

482

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

The plans, the two constraints in order, and the empty set of blocked plans are added as a solution
since there is no previously found solution that Dominates it. The Dominates function uses domain
specific criteria for determining when a solution has value as an alternative and should be kept or
is inferior compared to another and should be dropped. In this manufacturing domain, one solution
dominates another if the finish time for at least one agent is earlier and no finish times are later for
any agents. The search then continues to find alternative or superior solutions, although the agents
may decide to terminate the search in the interest of time.
5.2 Search Techniques and Heuristics
Although summary information is valuable for finding conflict free or coordinated plans at abstract
levels, this information can also be valuable in directing the search to avoid branches in the search
space that lead to inconsistent or suboptimal coordinated plans. A coordinator can prune away
inconsistent coordinated plans at the abstract level by doing a quick check to see if MightSomeWay
is false. For example, if the search somehow reached the state shown in Figure 8b, the coordinator
could backtrack before expanding the hierarchies further and avoid reasoning about details of the
plans where they must fail.
Another strategy is to first expand plans involved in the most threats. For the sake of completeness, the order of plan expansions does not matter as long as they are all expanded at some point
when the search trail cannot be pruned. But, employing this “expand on most threats first” (EMTF)
heuristic aims at driving the search down through the hierarchy to find the subplan(s) causing conflicts with others so that they can be resolved more quickly. This is similar to a most-constrained
variable heuristic often employed in constraint satisfaction problems. For example, if the facilities
and inventory managers wished to execute their plans concurrently as shown in Figure 17a, at the
most abstract level, the coordinator would find that there are conflicts over the use of transports for
moving parts. Instead of decomposing produce H and reasoning about plan details where there
are no conflicts, the EMTF heuristic would choose to decompose either maintenance or move parts
which have the most conflicts. By decomposing maintenance the agents can resolve the remaining
conflicts and still execute concurrently.
Another heuristic that a coordinator can use in parallel with EMTF is “choose fewest threats
first” (CFTF). Here the search orders states in the search queue by ascending numbers of threats
left to resolve. In effect, this is a least-constraining value heuristic used in constraint satisfaction
approaches. As mentioned in Section 4.1, threats are identified by the CanAnyWay algorithm. By
trying to resolve the threats of coordinated plan search states with fewer conflicts, it is hoped that
solutions can be found more quickly. So, EMTF is a heuristic for ordering and subplans to expand,
and CFTF, in effect, orders or subplan choices. For example, if the production manager chooses to
use machine M1 instead of M2 to produce G, the coordinator is likely closer to a solution because
there are fewer conflicts to resolve. This heuristic can be applied not only to selecting or subplan
choices but also to choosing temporal constraints and variable bindings or any search operator from
the entire set of operators.
In addition, in trying to find optimal solutions in the style of a branch-and-bound search, the
coordinator can use the cost of abstract solutions to prune away branches of the search space whose
minimum cost is greater than the maximum cost of the current best solution. This is the role of the
Dominates function in the description of the coordination algorithm in Section 5.1. This usually

483

C LEMENT, D URFEE , & BARRETT

a)

maintenance
produce H
move_parts

b)

maintenance
service M1 M2
service

service

M1

M2

move
tool

produce H

move_parts

Figure 17:

EMTF

heuristic resolving conflicts by decomposing the maintenance plan

assumes that cost/utility information is decomposable over the hierarchy of actions, or the cost of
any abstract action is a function of its decompositions.

6. Complexity Analyses
Even though the planner or coordinator can use the search techniques described in the Section 5.2 to
prune the search space, just being able to find solutions at multiple levels of abstraction can reduce
the computation as much as doubly exponentially. In this section, we give an example of this and
then analyze the complexity of planning and scheduling to characterize this cost reduction and the
conditions under which it occurs.
An agent that interleaves execution with planning/coordination often must limit the total computation and execution cost required to achieve its goals. The planning algorithm described in Section
5.1 is able to search for solutions at different levels of abstraction. For the manufacturing example,
our implementation of a centralized coordinator uses this algorithm to find in 1.9 CPU seconds a solution at the top level of the agents’ plans as shown in Figure 13b. If we define the cost of execution
as the makespan (completion time) of the coordinated plan, the cost of this solution is 210 where the
makespan of the production manager’s plan is 90, the facilities manager’s is 90, and the inventory
manager’s is 30. For the solution in Figure 13c, the coordinator required 667 CPU seconds, and
the makespan of the coordinated plan is 170. Another solution is found at an intermediate level of
abstraction, taking 69 CPU seconds and having a makespan of 180. So, with a little more effort,
the algorithm expanded the hierarchy to an intermediate level where the cost of the solution was
reduced by 30. Thus, overall cost can be reduced by coordinating at intermediate levels.
For this problem, coordinating at higher levels of abstraction is less costly because there are
fewer plan steps. But, even though there are fewer plans at higher levels, those plans may have
greater numbers of summary conditions to reason about because they are collected from the much
greater set of plans below. Here we argue that even in the worst case where the number of summary
conditions per plan increases exponentially up the hierarchy, finding solutions at abstract levels is
expected to be exponentially cheaper than at lower levels. We first analyze the complexity of the
484

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

summarization algorithm to help the reader understand how the summary conditions can collect in
greater sets at higher levels.
6.1 Complexity of Summarization
Consider a hierarchy with n total plans, b subplans for each non-primitive plan, and depth d, starting
with zero at the root, as shown in Figure 18. The procedure for deriving summary conditions works
by basically propagating the conditions from the primitives up the hierarchy to the most abstract
plans. Because the conditions of any non-primitive plan depend only on those of its immediate subplans, deriving summary conditions can be done quickly if the number of subplans is not large. The
derivation algorithm mainly involves checking for achieve, clobber, and undo interactions among
subplans for all possible total orderings of the subplans (as described in Section 3.4). Checking for
one of these relations for one summary condition of one subplan is O(bs) for b subplans, each with
s summary conditions (as discussed in Section 3.3). Since there are O(bs) conditions that must be
checked in the set of subplans, deriving the summary conditions of one plan from its subplans is
O(b2 s2 ).
However, the maximum number of summary conditions for a subplan grows exponentially up
the hierarchy since, in the worst case, no summary conditions merge during summarization. This
happens when the conditions of each subplan are on completely different propositions/variables
than those of any sibling subplan. In this case, a separate summary condition will be generated for
each summary condition of each subplan. If the children share conditions on the same variable, this
information is collapsed into a single summary condition in the parent plan.
As shown in the third column of the table in Figure 18, a plan at the lowest level d has s = c
summary conditions derived from its c pre-, in-, and postconditions. A plan at level d − 1 derives c
summary conditions from its own conditions and c from each of its b subplans giving c + bc summary conditions, or s = O(bc). So, in this worst case s = O(bd−i c) for a plan at level i in a hierarchy
for which each plan has c (non-summary) conditions. Thus, the complexity of summarizing a plan
at level i (with subplans at level i + 1) is O(b2 b2(d−(i+1)) c2 ) = O(b2(d−i) c2 ). There are bi plans at
level i (second column in the figure), so the complexity of summarizing the set of plans at level i is
O(bi b2(d−i) c2 ) = O(b2d−i c2 ) as shown in the fourth column in the figure. Thus, the complexity of
i 2(d−i) c2 ). In this summation i = 0
summarizing the entire hierarchy of plans would be O(∑d−1
i=0 b b
2d
2
dominates, so the complexity can be simplified to O(b c ). If there are n = O(bd ) plans in the
hierarchy, we can write this simply as O(n2 c2 ), which is the square of the size of the hierarchy.
In the best case where all conditions are on the same variable, each plan will have c summary
i 2 2
conditions. Thus, the complexity for summarizing the hierarchy will be O(∑d−1
i=0 b b c ), which
simplifies to O(bd+1 c2 ) = O(nbc2 ). In any case, the summarization of conditions is tractable, and
as we discussed in Section 3.5.2, the summarization of resources is also tractable.
6.2 Complexity of Finding Abstract Solutions
In order to resolve conflicts (and potentially arrive at a solution) at a particular level of expansion
of the hierarchy, the coordination algorithm checks for threats between the plans under particular
ordering constraints at that level. Checking for threats involves finding clobber relations among the
plans and their summary conditions. The complexity of finding threats among n plans each with s
summary conditions is O(n2 s2 ) as shown in Section 4.1 for the MightSomeWay algorithm. For a
hierarchy expanded to level i, there are n = O(bi ) plans at the frontier of expansion, and each plan
485

C LEMENT, D URFEE , & BARRETT

level #plans #conds / #operations to #test operations / solution
plan derive summ. info. solution candidate space

1

2

...

b

......
.................................
..........
............
1

2

...

b

1

2

...

b

.......

0

1

O(bdc)

O(b2(bd-1c)2)
= O(b2dc2)

O(1)

1

1

b

O(bd-1c)

O(bb2(bd-2c)2)
= O(b2d-1c2)

O(b2(b(d-1)c)2)
= O(b2dc2)

O(kb)

2

b2

O(bd-2c) O(b2b2(bd-3c)2)
= O(b2d-2c2)

O(b4(b(d-2)c)2)
= O(b2dc2)

O(kb )

d-2

bd-2

O(b2c)

O(bd-2b2(bc)2)
= O(bd+2c2)

O(b2(d-2)(b2c)2) O(kb )
= O(b2dc2)

d-1

bd-1

3c+b3c
= O(bc)

O(bd-1b2c2)
= O(bd+1c2)

O(b2(d-1)(bc)2) O(kb )
= O(b2dc2)

d

bd

3c

O(1)

O(b2dc2)

O(kb )

i

bi

O(bd-ic)

O(b2d-ic2)

O(b2dc2)

O(kbi)

2

d-2

d-1

d

Figure 18: Complexity of threat identification and resolution at abstract levels
has s = O(bd−i c) summary conditions. So, as shown in the fifth column of the table in Figure 18,
the worst case complexity of checking for threats for one synchronization of a set of plans at level i
is O(b2i (bd−i c)2 ) = O(b2d c2 ). Notice that i drops out of the formula, meaning that the complexity
of checking a candidate solution is independent of the depth level. In the best case where summary
conditions fully merge, each plan has s = c summary conditions, so the complexity of checking a
candidate solution is O(b2i c2 ), a factor of O(b2(d−i) )faster than the worst case.
However, the algorithm may check many synchronizations at a particular level before finding
a solution or exhausting the search space. In fact this search complexity grows exponentially with
the number of plans.5 Thus, as shown in the last column of the table in Figure 18, the search space
i
is O(kb ) for bi plans at level i and constant k.6 Thus, the search space grows doubly exponentially
down the hierarchy based on the number of plan steps.
In our refinement coordination and planning algorithm, the conflict detection is a basic operation
that is done for resolving conflicts. So, to also include the effect of the size of conditions (in
addition to plan steps) on the complexity of the planning/coordination algorithm, we must multiply
i
by the complexity to check threats. Thus, the complexity is O(kb b2d c2 ) when summary information
i
does not merge at all and O(kb b2i c2 ) when summary information fully merges. The complexity
d
of resolving conflicts at the primitive level is O(kb b2d c2 ), so resolving conflicts at an abstract
d
i
level speeds search doubly exponentially, a factor of O(kb −b ) even when summary information
does not merge during summarization. Now, if it completely merges, the speedup is a factor of
d
i
O(kb −b b2(d−i) ).
5. In fact, it is NP-complete (Clement, 2002).
6. This is why Georgeff chose to cluster multiple operators into “critical regions” and synchronize the (fewer) regions
since there would be many fewer interleavings to check (1983). By exploiting the hierarchical structure of plans, we
use the “clusters” predefined in the hierarchy to this kind of advantage without needing to cluster from the bottom
up.

486

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

level
0
1
d

branching
factor b

...
1

2

n
c constraints
per hierarchy

v
variables

Figure 19: Schedule of n task hierarchies each with c constraints on v variables
There are only and plans in this analysis. In the case that there are or plans, being able to prune
branches at higher levels based on summary information will also greatly improve the search despite
the overhead of deriving and using summary conditions. Pruning effectively reduces the branching
factor since the branch is eliminated before investigating its details. Thus, the complexity based on
d
the number of plan steps becomes O(k(b−p) ) when a fraction of p/b branches can be pruned. Thus,
pruning can also create an exponential reduction in search.
6.3 Scheduling Complexity
A local search planner (e.g. ASPEN, Chien et al., 2000b) does not backtrack, but the problem to be
solved is the same, so one might expect that complexity advantages are the same as for the refinement planner. However, the search operations for the local search planner can be very different. A
previous study of a technique called aggregation eliminates search inefficiencies at lower levels of
detail in task hierarchies by operating on hierarchies as single tasks (Knight, Rabideau, & Chien,
2000). Thus, it is not immediately clear what additional improvements a scheduler could obtained
using summary information. We will show that the improvements are significant, but first we must
provide more background on aggregation.
Moving tasks is a central scheduling operation in iterative repair planners. A planner can more
effectively schedule tasks by moving related groups of tasks to preserve constraints among them.
Hierarchical task representations are a common way of representing these groups and their constraints. Aggregation involves moving a fully detailed abstract task hierarchy while preserving the
temporal ordering constraints among the subtasks. Moving individual tasks independently of their
parent, siblings, and subtasks is shown to be much less efficient (Knight et al., 2000). Valid placements of the task hierarchy in the schedule are computed from the state and resource usage profiles
for the hierarchy and for the other tasks in the context of the movement. A hierarchy’s profile represents one instantiation of the decomposition and temporal ordering of the most abstract task in the
hierarchy.
Consider a schedule of n task hierarchies with a maximum branching factor b expanded to a
maximum depth of d as shown in Figure 19. Suppose each hierarchy has c constraints on each of v
variables (states or metric resources). To move a hierarchy of tasks using aggregation, the scheduler

487

C LEMENT, D URFEE , & BARRETT

must compute valid intervals for each resource variable affected by the hierarchy.7 The scheduler
then intersects these intervals to get valid placements for the abstract tasks and their children. The
complexity of computing the set of valid intervals for a resource is O(cC) where c is the number
of constraints (usages) an abstract task has with its children for the variable, and C is the number
of constraints of other tasks in the schedule on the variable (Knight et al., 2000). With n similar
task hierarchies in the entire schedule, then C = (n − 1)c, and the complexity of computing valid
intervals is O(nc2 ). But this computation is done for each of v resource variables (often constant for
a domain), so moving a task will have a complexity of O(vnc2 ). The intersection of valid intervals
across variables does not increase the complexity. Its complexity is O(tnr) because there can be at
most nr valid intervals for each timeline; intersecting intervals for a pair of timelines is linear with
the number of intervals; and only t −1 pairs of timelines need to be intersected to get the intersection
of the set.
The summary information of an abstract task represents all of the constraints of its children, but
if the children share constraints over the same resource, this information is collapsed into a single
summary resource usage in the abstract task. Therefore, when moving an abstract task, the number
of different constraints involved may be far fewer depending on the domain. If the scheduler is
trying to place a summarized abstract task among other summarized tasks, the computation of valid
placement intervals can be greatly reduced because the c in O(vnc2 ) is smaller. We now consider
the two extreme cases where constraints can be fully collapsed and where they cannot be collapsed
at all.
In the case that all tasks in a hierarchy have constraints on the same variable, the number of
constraints in a hierarchy is O(bd ) for a hierarchy of depth d and branching factor (number of child
tasks per parent) b. In aggregation, where hierarchies are fully detailed first, this means that the
complexity of moving a task is O(vnb2d ) because c = O(bd ). Now consider using aggregation
for moving a partially expanded hierarchy where the leaves are summarized abstract tasks. If all
hierarchies in the schedule are decomposed to level i, there are O(bi ) tasks in a hierarchy, each with
one summarized constraint representing those of all of the yet undetailed subtasks beneath it for
each constraint variable. So c = O(bi ), and the complexity of moving the task is O(vnb2i ). Thus,
moving an abstract task using summary information can be a factor of O(b2(d−i) ) times faster than
for aggregation. Because the worst case number of conflicts increases with the number of plan
steps (just as with the refinement planner), the worst case complexity of resolving conflicts based
i
on the number of plan steps at level i is O(kb ). Thus (as with refinement planning) using summary
d−i
information can make speedups of O(kb b2(d−i) ) when summary information fully collapses.
The other extreme is when all of the tasks place constraints on different variables. In this case,
c = 1 because any hierarchy can only have one constraint per variable. Fully detailed hierarchies
contain v = O(bd ) different variables, so the complexity of moving a task in this case is O(nbd ).
If moving a summarized abstract task where all tasks in the schedule are decomposed to level i, v
is the same because the abstract task summarizes all constraints for each subtask in the hierarchy
beneath it, and each of those constraints are on different variables such that no constraints combine
when summarized. Thus, the complexity for moving a partially expanded hierarchy is the same as
for a fully expanded one. In this case, the number of conflicts also does not change with the depth of
the hierarchy because the conflicts are always between pairs of the n hierarchies. So, for this other
7. The analysis also applies to state constraints, but we restrict the discussion to resource usage constraints for simplicity.

488

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

extreme case, summary information does not reduce the complexity of scheduling and would only
incur unnecessary overhead.
Other complexity analyses have shown that different forms of hierarchical problem solving, if
they do not need to backtrack from lower to higher levels because there are no interacting subproblems, can reduce the size of the search space by an exponential factor (Korf, 1987; Knoblock, 1991).
A planner or scheduler using summary information can witness exponential improvements without
this assumption. Backtracking across abstraction levels occurs within the planner/coordinator described in Section 5.1 when the current search state is ¬MightSomeWay and another or subplan
on the same or higher level can be selected. We demonstrated that the search space grows doubly
exponentially down the hierarchy because the number of plans grows exponentially, and resolving
conflicts grows exponentially with the number of plans. Thus, as long as the planner or coordinator does not have to fully expand all abstract plans to the primitive level and summary information
d
i
merges at higher levels, the search complexity is reduced at least by a factor of kb −b where i is the
level where the search completed, and d is the depth of the hierarchy. Yang (1997) also suggests
ways exponential speedups can be obtained when subplans interact based on hierarchy structure.
Our speedups are complementary to these because summary information limits the decomposition
of task hierarchies and compresses the information manipulated by a planner or scheduler.

7. Experiments
Now we experimentally evaluate the use of summary information in planning and coordination for
three different domains: an evacuation domain, the manufacturing domain described in Section 1.1,
and a multi-rover domain. In these domains, we define performance in different ways to show a
range of benefits that abstract reasoning offers.
We evaluate the algorithm described in Section 5.1. Our implementation orders search states
in the queue such that those generated by synchronization operators precede those generated by
expansion and selection operators. Thus, before going deeper into a part of the hierarchy, the implementation of the algorithm explores all orderings of the agents’ plans before digging deeper into
the hierarchy. Investigating heuristics for choosing between synchronization and decomposition
operators is a topic for future research.
In the next section we report experiments for an evacuation domain that show how abstract
reasoning using summary information can find optimal coordination solutions more quickly than
conventional search strategies. Optimal solutions in the evacuation domain have minimal global execution times because evacuees must be transported to safety as quickly as possible. In Section 7.2,
we show that summary information improves local search performance significantly when tasks
within the same hierarchy have constraints over the same resource, and when solutions are found at
some level of abstraction. We also evaluate the benefits of using the CFTF and EMTF heuristics for
iterative repair and show where summary information can slow search.
In some domains, computation time may be insignificant to communication costs. These costs
could be in terms of privacy for self-interested agents, security for sensitive information that could
obtained by malicious agents, or simply communication delay. In Section 7.3, we show how multilevel coordination fails to reduce communication delay for the manufacturing domain example but,
for other domains, can be expected to reduce communication overhead exponentially.

489

C LEMENT, D URFEE , & BARRETT

1
s0

2

3

0

s3
t2

t1
4

5

Figure 20: Evacuation problem
7.1 Coordinated Planning Experiments
In this section, we describe experiments that evaluate the use of summary information in coordinating a group of evacuation transports that must together retrieve evacuees from a number of locations
with constraints on the routes. In comparing the EMTF and CFTF search techniques described in Section 5.2 against conventional HTN approaches, the experiments show that reasoning about summary
information finds optimally coordinated plans much more quickly than prior HTN techniques.
We compare different techniques for ordering the expansion of subplans of both and and or
plans to direct the decomposition of plan hierarchies in the search for optimal solutions. These
expansion techniques are the expand (for and subplans) and select (for or subplans) operators of
the algorithm described in Section 5.1.
We compare EMTF’s expansion of and plans to the ExCon heuristic and to a random selection
heuristic. The ExCon heuristic (Tsuneto et al., 1998) first selects plans that can achieve an external
precondition, or if there are no such plans, it selects one that threatens the external precondition.
In the case that there are neither achieving or threatening plans, it chooses randomly. Note that
EMTF will additionally choose to expand plans with only threatened external preconditions but has
no preference as to whether the plan achieves, threatens, or is threatened. For the expansion of or
plans, we compare CFTF to a depth-first (DFS) and a random heuristic.
We also compare the combination of CFTF and EMTF to an FAF (“fewest alternatives first”)
heuristic and to the combination of DFS and ExCon. The FAF heuristic does not employ summary
information but rather chooses to expand and and select or plans that have the fewest subplans
(Currie & Tate, 1991; Tsuneto, Hendler, & Nau, 1997). Since no summary information is used,
threats are only resolved at primitive levels. While it has been shown that the FAF heuristic can
be effectively used by an HTN planner (Tsuneto et al., 1997), the combination of DFS and ExCon
has been shown to make great improvements over FAF in a domain with more task interactions
(Tsuneto et al., 1998). We show in one such domain that the CFTF and EMTF heuristics can together
outperform combinations of FAF, DFS, and ExCon.
The problems were generated for an evacuation domain where transports are responsible for
visiting certain locations along restricted routes to pick up evacuees and bring them back to safety
points. Transports are allowed to be at the same location at the same time, but the coordinator must
ensure that transports avoid collisions along the single lane routes. In addition, in order to avoid the
risk of oncoming danger (from a typhoon or enemy attack), the transports must accomplish their
goals as quickly as possible.
Suppose there are two transports, t1 and t2, located at safety points s0 and s3 respectively, and
they must visit the locations 0, 1, and 2 and 2, 3, and 4 respectively and bring evacuees back to safe
490

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

evacuate
move s0-0

make rounds

one switch

no switch
clockwise

first route

cw0-1

second route

cw0-2

move 0-1

counterclockwise

ccw1-2

ccw2-0

go back

ccw2-0

goto safe loc

no move

move 0-s0

move 3-s3

move 1-2

Figure 21: The plan hierarchy for transport t1
locations as shown in Figure 20. Because of overlap in the locations they must visit, the coordinator
must synchronize their actions in order to avoid collision. The coordinator’s goal network includes
two unordered tasks, one for each transport to evacuate the locations for which it is responsible.
As shown in Figure 21, the high-level task for t1 (evacuate) decomposes into a primitive action of
moving to location 0 on the ring and an abstract plan to traverse the ring (make rounds). t1 can
travel in one direction around the ring without switching directions, or it can switch directions once.
t1 can then either go clockwise or counterclockwise and, if switching, can switch directions at any
location ( f irst route) and travel to the farthest location it needs to visit from where it switched
(second route). Once it has visited all the locations, it continues around until it reaches the first
safety point in its path (go back and goto sa f e loc). The no move plan is for the case where t1 is
already at location 0. The task for t2 can be refined similarly.
Suppose the coordinator gathers summary information for the plan hierarchy and attempts to
resolve conflicts. Looking just at the summary information one level from the top, the coordinator
can determine that if t1 finishes evacuating before t2 even begins, then there will be no conflicts
since the external conditions of t1’s evacuate plan are that none of the routes are being traversed.
This solution has a makespan (total completion time) of 16 steps. The optimal solution is a plan of
duration seven where t1 moves clockwise until it reaches location s3, and t2 starts out clockwise,
switches directions at location 4, and then winds up at s0. For this solution t1 waits at location 2 for
one time step to avoid a collision on the route from location 2 to location 3.
We generated problems with four, six, eight, and twelve locations; with two, three and four
transports; and with no, some, and complete overlap in the locations the transports visit. Performance was measured as the number of search states expanded to find the optimal solution or (if the
compared heuristics did not both find the optimal solution) as the number of states each expanded
to find solutions of highest common quality within memory and time bounds. We chose this instead of CPU time as the measure of performance in order to avoid fairness issues with respect to
implementation details of the various approaches.

491

C LEMENT, D URFEE , & BARRETT

Search States Expanded
100000

CFTF-RAND

10000
1000
100
10
1
1

10

100

1000 10000 1E+05

CFTF-EMTF

Figure 22: Comparing EMTF to random expansion in searching for optimal solutions

Figure 23: Comparing EMTF to ExCon in searching for optimal solutions
The scatter plot in Figure 22 shows the relative performance of the combination of CFTF and
and the combination of CFTF and random and expansion (CFTF-Rand). We
chose scatterplots to compare results because they capture the results more simply than trying to
plot against three dimensions of problem size/complexity. Note that for all scatter plots, the axes are
scaled logarithmically. Points above the diagonal line mean that EMTF (x-axis) is performing better
than Rand (y-axis) because fewer search states were required to find the optimal solution. While
performance is similar for most problems, there are a few cases where CFTF-EMTF outperformed
CFTF -Rand by an order of magnitude or more. Figure 23 exhibits a similar effect for CFTF - EMTF
and CFTF-ExCon. Note that runs were terminated after the expansion of 3,500 search states. Data
points at 3,500 (the ones forming a horizontal line at the top) indicate that no solution was found
within memory and time constraints. While performance is similar for most problems, there are four
points along the top where CFTF-ExCon finds no solution. Thus, although EMTF does not greatly
EMTF ( CFTF - EMTF )

492

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

Figure 24: Comparing CFTF and DFS in searching for optimal solutions
improve performance for many problems, it rarely performs much worse, and almost always avoids
getting stuck in fruitless areas of the search space compared to the ExCon and the random heuristic.
This is to be expected since EMTF focuses on resolving conflicts among the most problematic plans
first and avoids spending a lot of time reasoning about the details of less problematic plans.
The combination of CFTF with EMTF, pruning inconsistent abstract plan spaces, and branchand-bound pruning of more costly abstract plan spaces (all described in Section 5.2) much more
dramatically outperforms techniques that do not reason at abstract levels. Figure 24 shows DFSRand expanding between one and three orders of magnitude more states than CFTF-Rand. Runs
were terminated after the expansion of 25,000 search states. Data points at 25,000 (forming the
horizontal line at the top) indicate that no solution was found within memory and time constraints.
By avoiding search spaces with greater numbers conflicts, CFTF finds optimal or near-optimal solutions much more quickly. In Figures 25 and 26, CFTF-EMTF outperforms FAF-FAF (FAF for both
selecting and and or plans) and DFS-ExCon by one to two orders of magnitude for most problems.
These last two comparisons especially emphasize the importance of abstract reasoning for finding
optimal solutions. Within a maximum of 3,500 expanded search states (the lowest cutoff point in
the experiments), CFTF-EMTF and CFTF-Rand found optimal solutions for 13 of the 24 problems.
CFTF -ExCon and FAF- FAF found 12; and DFS -ExCon and DFS -Rand only found three.
A surprising result is that FAF-FAF performs much better than DFS-ExCon for the evacuation
problems contrary to the results given by Tsuneto et al. (1998) that show DFS-ExCon dominating
for problems with more goal interactions. We believe that this result was not reproduced here
because those experiments involved hierarchies with no or plans. The experiments show that the
selection of or subplans more greatly affects performance than the order of and subplans to expand.
So, we believe DFS-ExCon performed worse than FAF-FAF not because FAF is better at choosing
and subplans than ExCon but because FAF is stronger at selecting or subplans than DFS.
However, the main point of this section is that each of the heuristic combinations that use summary information to find solutions and prune the search space at abstract levels (CFTF-EMTF, CFTFExCon, and CFTF-Rand) greatly outperform all of those that do not (FAF-FAF, DFS-ExCon, and
DFS -Rand) when searching for optimal solutions.

493

C LEMENT, D URFEE , & BARRETT

Figure 25: Comparing the use of summary information to the FAF heuristic

Figure 26: Comparing the use of summary information to the algorithm using external conditions
7.2 Scheduling Experiments
The experiments we describe here show that summary information improves performance significantly when tasks within the same hierarchy have constraints over the same resource, and solutions
are found at some level of abstraction. At the same time, there are cases where abstract reasoning
incurs significant overhead when solutions are only found at deeper levels. However, in domains
where decomposition choices are critical, we show that this overhead is insignificant because the
CFTF heuristic chooses decompositions that more quickly lead to solutions at deeper levels. These
experiments also show that the EMTF heuristic outperforms a simpler heuristic depending on the
decomposition rate, raising new research questions. We use the ASPEN Planning System (Chien
et al., 2000b) to coordinate a rover team for the problem described next.

494

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

Figure 27: Randomly generated rectangular field of triangulated waypoints

Figure 28: Randomly generated waypoints along corridors
7.2.1 P ROBLEM D OMAINS
The domain involves a team of rovers that must resolve conflicts over shared resources. We generate
two classes of maps within which the rovers move. For one, we randomly generate a map of triangulated waypoints (Figure 27). For the other, we generate corridor paths from a circle of locations
with three paths from the center to points on the circle to represent narrow paths around obstacles
(Figure 28). This “corridor” map is used to evaluate the CFTF heuristic. We then select a subset of
the points as science locations (where the rovers study rocks/soil) and use a simple multiple traveling salesman algorithm to assign routes for the rovers to traverse and perform experiments. The
idea is that a map of the area around a lander is constructed from an image taken upon landing on
Mars.
Paths between waypoints are assigned random capacities such that either one, two, or three
rovers can traverse a path simultaneously. Only one rover can be at any waypoint, and rovers may
not traverse paths in opposite directions at the same time. These constraints are modeled as metric
resources. State variables are also used to ensure rovers are at locations from which they are about
to leave. In addition, rovers must communicate with the lander for telemetry using a shared channel
of fixed bandwidth (metric resource). Depending on the terrain between waypoints, the required
bandwidth varies. 80 problems were generated for two to five rovers, three to six science locations
per rover, and 9 to 105 waypoints. In general, problems that contain fewer waypoints and more
science goals are more difficult because there are more interactions among the rovers.
Schedules consist of an abstract task for each rover that have an and decomposition into tasks for
visiting each assigned science location. Those tasks have an or decomposition into the three shortest
paths through the waypoints to the target science location. The paths have an and decomposition
into movements between waypoints. Additional levels of hierarchy were introduced for longer paths
in order to keep the offline resource summarization tractable. Schedules ranged from 180 to 1300
tasks.

495

C LEMENT, D URFEE , & BARRETT

7.2.2 E MPIRICAL R ESULTS FOR M ARS ROVERS
We compare ASPEN using aggregation with and without summarization for three variations of the
rectangular field domain. When using summary information, ASPEN also uses the EMTF and CFTF
decomposition heuristics. One domain excludes the communications channel resource (no channel);
one excludes the path capacity restrictions (channel only); and the other excludes neither (mixed).
Since all of the movement tasks reserve the channel resource, greater improvement in performance
is expected when using summary information according to the complexity analyses in Section 6.3.
This is because constraints on the channel resource collapse in the summary information derived
at higher levels such that any task in a hierarchy only has one constraint on the resource. When
ASPEN does not use summary information, the hierarchies must be fully expanded, and the number
of constraints on the channel resource is equivalent to the number of leaf movement tasks.
However, tasks within a rover’s hierarchy rarely place constraints on the other path variables
more than once, so the no channel domain corresponds to the worst case where summarization
collapses no constraints. Here the complexity of moving an abstract task is the same without summary information for the fully expanded hierarchy as it is with summary information for a partially
expanded hierarchy.
Figure 29 (top) exhibits two distributions of problems for the no channel domain. In most of the
cases (points above the x=y diagonal), ASPEN with summary information finds a solution quickly at
some level of abstraction. However, in many cases, summary information performs notably worse
(points below the x=y diagonal). We discovered that for these problems finding a solution requires
the planner to dig deeply into the rovers’ hierarchies, and once it decomposes the hierarchies to
the level of the solution, the difference in the additional time to find a solution between the two
approaches is negligible (unless the use of summary information found a solution at a slightly higher
level of abstraction more quickly). Thus, the time spent reasoning about summary information at
higher levels incurred unnecessary overhead.
But this is the worst case in the analysis in Section 6.3 where we showed that summary information had no advantage even if it found abstract solutions. So, why did summary information perform
better when abstract solutions were found? It was not because of the CFTF heuristic since or branch
choices result in small differences in numbers of conflicts. It actually results from the stochastic nature of ASPEN’s iterative repair. Although moving the most abstract tasks using aggregation without
summary information would have enabled ASPEN to find solutions more quickly for fully expanded
hierarchies, ASPEN must sometimes move lower level tasks independently of their parents and siblings in order to resolve conflicts at lower levels. The problem is that ASPEN has no heuristic to tell
it at what level it needs to move activities, and it sometimes chooses to move activities at detailed
levels unnecessarily. This search at lower levels is where the search space explodes. Using summary
information to search at higher levels below lower levels of abstraction better protects ASPEN from
unnecessary search.
Figure 29 (middle) shows significant improvement for summary information in the mixed domain compared to the no channel domain. Adding the channel resource rarely affects the use of
summary information because the collapse in summary constraints incurs insignificant additional
complexity. However, the channel resource makes the scheduling task noticeably more difficult for
ASPEN when not using summary information. In the channel only domain (Figure 29 bottom), summary information finds solutions at the abstract level almost immediately, but the problems are still
complicated when ASPEN does not use summary information. These results support the complexity

496

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

a)

b)

c)
Figure 29: Plots for the a) no channel, b) mixed, and c) channel only domains
analysis in Section 6.3 that argues that summary information exponentially improves performance
when tasks within the same hierarchy have constraints over the same resource and when solutions
are found at some level of abstraction.
Because summary information is generated offline, the domain modeler knows up front whether
or not constraints are significantly collapsed. Thus, an obvious approach to avoiding cases where
reasoning about summary information causes unnecessary overhead is to fully expand at the start
of scheduling the hierarchies of tasks where summary information does not collapse. Because the
complexity of moving a task hierarchy is the same in this case whether fully expanded or not, ASPEN
does not waste time by duplicating its efforts at each level of expansion before reaching the level at
which it finds a solution. Evaluating this approach is a subject of future work.
Earlier we mentioned that the CFTF heuristic is not effective for the rectangular field problems.
This is because the choice among different paths to a science location usually does not make a

497

C LEMENT, D URFEE , & BARRETT

Figure 30: Performance using the CFTF heuristic

significant difference in the number of conflicts encountered—if the rovers cross paths, all path
choices usually still lead to conflict. For the set of corridor problems, path choices always lead
down a different corridor to get to the target location, so there is usually a path that avoids a conflict
and a path that causes one depending on the path choices of the other rovers. When ASPEN uses the
CFTF heuristic, the performance dominates that of when it chooses decompositions randomly for all
but two problems (Figure 30). This reflects experiments for the coordination algorithm in Section 7
that show that CFTF is crucial for reducing the search time required to find solutions.
In order to evaluate the EMTF heuristic for iterative repair planning, we compared it to a simple
alternative. This alternative strategy (that we refer to as level decomposition) is to interleave repair
with decomposition as separate steps. Step 1) The planner repairs the current schedule until the
number of conflicts cannot be reduced. Step 2) It decomposes all abstract tasks one level down
and returns to Step 1. By only spending enough time at a particular level of expansion that appears
effective, the planner attempts to find the highest decomposition level where solutions exist without
wasting time at any level. The time spent searching for a solution at any level of expansion is
controlled by the rate at which abstract tasks are decomposed. The EMTF heuristic is implemented
as a repair method to give priority to detailing plans that are involved in more conflicts.
Figure 31 shows the performance of EMTF vs. level decomposition for different rates of decomposition for three problems from the set with varied performance. The plotted points are averages
over ten runs for each problem. Depending on the choice of rate of decomposition (the probability
that a task will decompose when a conflict is encountered), performance varies significantly. However, the best decomposition rate can vary from problem to problem making it potentially difficult
for the domain expert to choose. For example, for problem A in the figure, all tested decomposition
rates for EMTF outperformed the use of level decomposition. At the same time, for problem C using
either decomposition technique did not make a significant difference while for problem B choosing
the rate for EMTF made a big difference in whether to use EMTF or level decomposition. Although
these examples show varied performance, results for most other problems showed that a decompo-

498

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

1200

A
A level-decomp
1000

B
B level decomp
C

CPU seconds

800

C level decomp
600

400

200

0
0

5

10

15

20

25

30

35

EMTF Decomposition Rate

Figure 31: Performance of EMTF vs. level-decomposition heuristics
sition rate of around 15% was most successful. This suggests that a domain modeler may be able to
choose a generally successful decomposition rate by running performance experiments for a set of
example problems.8
We have demonstrated many of the results of the complexity analyses in Section 6. Scheduling
with summary information gains speedups (over aggregation) by resolving conflicts at appropriate levels of abstraction. When summary information collapses, the scheduler gains exponential
speedups. In addition, the CFTF heuristic enables exponential speedups when or decomposition
choices have varying numbers of conflicts.
7.3 Communication Overhead
Here we show that, depending on bandwidth, latency, and how summary information is communicated among the agents, delays due to communication overhead vary. If only communication costs
are a concern, then at one extreme where message delay dominates cost, sending the plan hierarchy
without summary information makes the most sense. At the other extreme where bandwidth costs
dominate, it makes sense to send the summary information for each task in a separate message as
each is requested. Still, there are cases when sending the summary information for tasks in groups
makes the most sense. This section will explain how a system designer can choose how much
summary information to send at a time in order to reduce communication overhead exponentially.
Consider a simple protocol where agents request coordination from a central coordinating agent.
During the search for a feasible solution, whenever it decomposes a task, the coordinator requests
summary information for the subtasks that it has not yet received. For the manufacturing domain,
the coordinator may already have summary information for a task to move a part, but if it encounters
a different instantiation of the same task schema, it still must request the parameters for the new task.
If a coordinator needs the subplans of an or plan, the client agent sends the required information for
all subplans, specifying its preferences for each. The coordinator then chooses the most preferred
8. For other experiments, we used a decomposition rate of 20% since it seemed to work well.

499

C LEMENT, D URFEE , & BARRETT

a)

b)

Figure 32: Delay of communicating different granularities of summary information with varying a)
latency and b) bandwidth

subplan, and in the case it must backtrack, it chooses the next most preferred subplan. Once the
coordinator finds a feasible solution, it sends modifications to each agent specifying which or subplans are blocked and where the agent must send and wait for synchronization messages. An agent
can choose to send summary information for some number of levels of expansion of the requested
task’s hierarchy.
For the manufacturing problem described in Section 1.1, communication data in terms of numbers of messages and the size of each was collected up to the point that the coordinator found the
solution in Figure 13c. This data was collected for cases where agents sent summary information
for tasks in their hierarchies, one at a time, two levels at a time, and all at once. The two levels
include the requested task and its immediate subplans. The following table below summarizes the
numbers and total sizes of messages sent for each granularity level of information:

one task at a time
two levels at a time
all at once

number of messages
9
4
3

total size (bytes)
8708
10525
16268

Assuming that the coordinator must wait for requested information before continuing its search
and can request only one task of one agent at a time, the coordination will be delayed for an amount
of time depending on the bandwidth and latency of message passing. The total delay can be calculated as (n − 2)` + s/b, where n is the number of messages sent; ` is the latency in seconds; s is the
total size of all messages; and b is the bandwidth in bytes per second. We use n − 2 instead of n
because we assume that the agents all transmit their first top-level summary information message at
the same time, so three messages actually only incur a delay of ` instead of 3`.
Figure 32a shows how the communication delay varies for the three granularities of information
for a fixed bandwidth of 100 bytes/second. (We will address the lack of realism in this example
shortly.) When the latency is less than 3 seconds, sending summary information for each task in
separate messages results in the smallest communication overhead. For latencies greater than 58
seconds, sending the entire hierarchy is best; and in between sending summary information two
levels at a time is best. If the latency is fixed at 100 seconds, then the communication delay varies
500

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

a)

b)

Figure 33: Delay with varying a) latency and b) bandwidth for hypothetical example
with bandwidth as shown in Figure 32b. When the bandwidth is less than 3 bytes/second, sending
one at a time is best; sending it all at once is best for bandwidths greater than 60 bytes/second; and
sending two levels at a time is best for bandwidths in between.
Admittedly, these values are unrealistic for the manufacturing domain. The manufacturing problem itself is very simple and provided mainly as an interesting domain for coordination. More realistic problems involving the manufacturing domain could have much larger hierarchies and require
much larger scales of data to be sent. In that case more realistic bandwidth and latency values would
exhibit similar tradeoffs.
To see this, suppose that the manufacturing managers’ hierarchies had a common branching
factor b and depth d. If tasks generally had reservations on similar resources throughout the hierarchies, the amount of total summary information at a particular level would grow exponentially
down the hierarchy just as would the number of tasks. If the agents agreed on a feasible solution at
depth level i in the hierarchy, then the table for messages and size would appear as follows:

one task at a time
two levels at a time
all at once

number of messages
O(bi )
3i/2
3

total size
O(bi )
O(bi )
O(bd )

Now suppose that the branching factor b is 3; the depth d is 10; the solution is found at level
i = 5; and the summary information for any task is 1 Kbyte. Then the table would look like this:

one task at a time
two levels at a time
all at once

number of messages
363
9
3

total size (Kbytes)
1089
3276
246033

Now, if we fixed the bandwidth at 100 Kbyte/second and varied the latency, more realistic
tradeoffs are seen in Figure 33a. Here, we see that unless the latency is very small, sending summary
information two levels at a time is best. As shown in Figure 33b, if we fix latency to be one second
and vary the bandwidth, for all realistic bandwidths sending summary information two levels at a
time is again best.

501

C LEMENT, D URFEE , & BARRETT

This simple protocol illustrates how communication can be minimized by sending summary
information at a particular granularity. If the agents chose not to send summary information but
the unsummarized hierarchies instead, they would need to send their entire hierarchies. As the
experiment shows, as hierarchies grow large, sending the entire hierarchy (“all at once”) would take
a long time, even with a high bandwidth. Thus, using summary information (as opposed to not using
it) can reduce communication exponentially when solutions can be found at abstract levels.
At the other extreme, if the agents sent summary information one task at a time, the latency
for sending so many messages can grow large for larger task hierarchies. If solutions could only
be found at primitive levels, then sending summary information one task at a time would cause an
exponential latency overhead compared to sending the entire hierarchy at once. But, if solutions
can be found at intermediate levels, being able to send summary information at some intermediate
granularity can minimize total delay.
However, this argument assumes that summary information collapses at higher levels in the hierarchy. Otherwise, sending summary information at some intermediate level could be almost as
expensive as sending the entire hierarchy and cause unnecessary overhead. For the actual manufacturing domain, tasks in the agents’ hierarchies mostly have constraints on different resources, and
summarization is not able to reduce summary information significantly because constraints do not
collapse. The result is that it is better, in this case, to send the entire hierarchy at once to minimize
delay (unless there are unusual bandwidth and latency constraints, as shown in the experiment).
Even so, the coordination agent can still summarize the hierarchies itself to take advantage of the
computational advantages of abstract reasoning.
This section showed how a domain modeler can minimize communication overhead by communicating summary information at the proper level of granularity. If bandwidth, latency, and a
common depth for coordination solutions is known, the domain modeler can perform a hypothetical
experiment like the one above for varying granularities of summary information to determine which
granularity is optimal. If summary information collapses up the hierarchy, and solutions can be
found at intermediate levels, then communication can be exponentially reduced in this manner.

8. Other Related Work
The approach we have taken for abstract reasoning was originally inspired by earlier work involving
a hierarchical behavior-space search where agents represent their planned behaviors at multiple
levels of abstraction (Durfee & Montgomery, 1991). Distributed protocols are used to decide at what
level of abstraction coordination is needed and to resolve conflicts there. This approach capitalizes
on domains where resources can be abstracted naturally. This earlier work can be viewed as a very
limited, special case of the work presented here. It is justified only intuitively and with limited
experiments and analyses.
Corkill studied interleaved planning and merging in a distributed version of the NOAH planner
(1979). He recognized that, while most of the conditions affected by an abstract plan operator
might be unknown until further refinement, those that deal with the overall effects and preconditions
that hold no matter how the operator is refined can be captured and used to identify and resolve
some conflicts. He recognized that further choices of refinement or synchronization choices at
more abstract levels could lead to unresolvable conflicts at deeper levels, and backtracking could be
necessary. Our work is directed toward avoiding such backtracking by using summary information
to guide search.

502

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

In closer relation to our approach, Pappachan shows how to interleave hierarchical plan coordination with plan execution for cooperative agents using an online iterative constraint relaxation
(OICR) algorithm (Pappachan, 2001). Like our approach, coordination can be achieved at higher
levels of abstraction for more flexible execution, or the agents can decompose their tasks to lower
levels for tighter coordination that can improve plan quality. The OICR approach is tailored toward
interleaving coordination and flexible execution at the price of completeness while the coordination
algorithm presented here is aimed at complete interleaved coordination and planning at the price of
potentially delaying execution due to backtracking.
In planning research, hierarchical plans have often been represented as Hierarchical Task Networks (HTNs, Erol et al., 1994a), which planners such as NOAH (Sacerdoti, 1977), NonLin (Tate,
1977), SIPE-2 (Wilkins, 1990), O-Plan (Currie & Tate, 1991), UMCP (Erol et al., 1994b), and SHOP 2
(Nau, Au, Ilghami, Kuter, Murdock, Wu, & Yaman, 2003) use to search through combinations of
alternative courses of action to achieve goals within a particular context. These actions may be partially ordered, giving timing flexibility during execution (Wilkins, 1990; Currie & Tate, 1991). Our
CH i P representation extends HTN s to include temporal extent and partial orderings can be expressed
as constraints on the starting and ending timepoints of the action.
Yang presented a method (similar to our summarization) for preprocessing a plan hierarchy in
order to be able to detect unresolvable conflicts at an abstract level so that the planner could backtrack from inconsistent search spaces (Yang, 1990). This corresponds to the use of ¬MightSomeWay
in Section 5.2. However, his approach requires that the decomposition hierarchy be modeled so that
each abstract operator have a unique main subaction that has the same preconditions and effects as
the parent. We avoid this restriction by analyzing the subplans’ conditions and ordering constraints
to automatically compute the parent’s summary conditions.
While our approach has focused on resolving conflicts among agents, Cox and Durfee (2003)
have used summary information to exploit synergistic interactions. The idea is that using summary information to identify overlapping effects can help agents skip actions whose effects are
achieved by others. Thangarajah, Padgham, and Winikoff (2003) have used summary information
in rescheduling during execution. Their representations are actually subsumed by ours, and their
work significantly postdates our first reporting of work in this paper (Clement & Durfee, 1999).
DSIPE (desJardins & Wolverton, 1999) is a distributed version of the SIPE -2 (Wilkins, 1990)
hierarchical planning system. In the same way agents can use summary information to reduce
communication to just those states for which they have common constraints, DSIPE filters conditions
communicated among planners using irrelevance reasoning (Wolverton & desJardins, 1998).
The DPOCL (Decompositional Partial-Order Causal-Link) planner (Young et al., 1994) adds
action decomposition to SNLP (McAllester & Rosenblitt, 1991). Like other HTN planners, preconditions and high level effects can be added to abstract tasks in order to help the planner resolve
conflicts during decomposition. In addition, causal links can be specified in decomposition schemas
to isolate external preconditions that DPOCL must satisfy. However, because these conditions and
causal links do not necessarily capture all of the external conditions of abstract tasks, the planner
does not find solutions at abstract levels and requires that all tasks be completely decomposed. In addition, DPOCL cannot determine that an abstract plan has unresolvable conflicts (¬MightSomeWay)
because there may be effects hidden in the decompositions of yet undetailed tasks that could achieve
open preconditions. By deriving summary conditions automatically and using algorithms for determining causal link information (e.g. must-achieve), our planning/coordination algorithm can find

503

C LEMENT, D URFEE , & BARRETT

and reject abstract plans during search without adding burden to the domain expert to specify redundant conditions or causal links for abstract tasks.
Like DPOCL, TÆMS (a framework for Task Analysis, Environment Modeling, and Simulation)
allows the domain modeler to specify a wide range of task relationships (Decker, 1995). This
work offers quantitative methods for analyzing and simulating agents as well as their interactions.
While only some of these interactions can be represented and discovered using summary conditions,
we discover this information through analysis rather than depending on the model developer to
predefine the interactions.
Grosz’s shared plans model of collaboration (1996) presents a theory for modeling multiagent
belief and intention. While the shared plans work is directed toward cooperative agents, it represents
action hierarchies and provides mental models at a higher level than represented in this article.
However, our use and analysis of summary information complements Grosz’s work by providing a
way to automatically represent and efficiently reason about the intentions of agents at multiple levels
of abstraction. Future work is needed to understand how summary information can be bridged with
mental states of agents to exploit the techniques employed in shared plans and other work based on
BDI (belief-desire-intention) models of agents (Rao & Georgeff, 1995).
An analysis of hierarchical planning (Yang, 1997) explains that, in the case of interacting subgoals, certain structures of the hierarchy that minimize these interactions can reduce worst case
planning complexity exponentially. However, the complexity analyses in Section 6 explain how using summary information can achieve exponential performance gains in addition to those achieved
by restructuring plan hierarchies according to Yang’s analysis by limiting the decomposition of task
hierarchies and compressing the information manipulated by a coordinator, planner, or scheduler.
SHOP 2 (Nau et al., 2003) is an HTN planner that uses a domain translation technique to reason
about durative action. This however does not express temporal extent in the same way as the planner
given here. Our model differs in that it supports ordering relationships on endpoints as well as
conditions and effects during an action’s execution. While there may be some domain translation
that could achieve the expression of similar constraints and solutions for other systems, ours is the
only formal model of such expressions in HTN planning.
SIADEX (Castillo et al., 2006) is another HTN planner that handles temporal extent in the use
of more expressive simple temporal networks (Dechter, Meiri, & Pearl, 1991). The performance
improvement techniques reported for SIADEX are in temporal reasoning and not specific to HTNs.
Thus, this work is complementary to ours. However, more work is needed to understand how
summary information can be exploited in conjunction with the forward expansion approach that
both SHOP 2 and SIADEX use to perform competitively on planning competition problems.
Another class of hierarchical planners based on ABSTRIPS (Sacerdoti, 1974) introduces conditions at different levels of abstraction so that more critical conflicts are handled at higher levels
of abstraction and less important (or easier) conflicts are resolved later at lower levels. While this
approach similarly resolves conflicts at abstract levels, the planning decisions may not be consistent
with conditions at lower levels resulting in backtracking. Summary information provides a means
to make sound and complete decisions at abstract levels without the need to decompose and check
consistency with lower levels. However, resolving conflicts based on criticality can still improve
performance in complement to our approach.
Allen’s temporal planner (1991) uses hierarchical representations of tasks and could be applied
to reasoning about the concurrent actions of multiple agents. However, it does not exploit hierarchy
by reasoning about abstraction levels separately and generates a plan by proving the consistency of
504

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

the collective constraints. Allen’s model of temporal plans (1983) and subsequent work on interval
point algebra (Vilain & Kautz, 1986) strongly influenced our hierarchical task representation and
algorithms that reason about them.
There are also many, many models and theories of concurrency. Some older examples include
automata representations, Petri nets and Hoare’s theory of communicating sequential processes
(Glabbeek, 1997). There are also many temporal logics such as computational tree logic (CTL,
Emerson & Halpern, 1985) that allow modal expressions about a proposition holding in some or all
possible worlds some of the time, all of the time, in the next state, eventually, or until some other
proposition holds. Another language for specifying manufacturing processes has been in the process
of being standardized over 10 years (Bock, 1996; Schlenoff, Knutilla, & Ray, 2006). Many of these
logics could have been used to define summary conditions and relations like MightSomeWay. However, we found that these logics were awkward for representing inconditions and defining summary
conditions and that the terminology used in this article simplifies the definitions.
Model checking uses temporal logics to verify different properties of system models, software,
and hardware (such as correctness, deadlock-free, and convergence). In fact, model checking
and planning algorithms can be used interchangeably on the same problems (e.g., Giunchiglia &
Traverso, 1999). In the context of model checking, summary information is a set of properties
(akin to those specifiable in CTL) of a system model (as a planning domain) that summarize system
variable requirements (conditions) and assignments (effects). Thus, a model checking algorithm
could use this summary information to efficiently identify and resolve potential requirement violations/bugs (condition conflicts) or deadlock (resource conflicts) in a system model or its operation
(planning/scheduling problem instantiations).

9. Conclusion
This article provides a formalization of Hierarchical Task Network planning that, unlike the UMCP
formalism (Erol et al., 1994b), includes actions with temporal extent. We introduce a sound and
complete algorithm that can be used to generate a plan, coordinate a group of agents with hierarchical plans, and interleave planning and coordination.
The algorithms for summarizing propositional state and metric resource conditions and effects
at abstract levels and the mechanisms that reason about this summary information can facilitate the
construction of other planning and coordination systems that reason about plans at multiple levels
of abstraction. These mechanisms for reasoning about summary information determine whether a
task (at any level of abstraction) must or may achieve, clobber, or undo a condition of another task
under partial order constraints on endpoints of tasks. Built on these mechanisms, other mechanisms
determine whether a group of agents can decompose and execute a set of partially ordered abstract
tasks in any way (CanAnyWay), might decompose and execute them in some way (MightSomeWay),
or cannot execute them consistently in any way (¬MightSomeWay).
These algorithms enable a planning system to find solutions at multiple levels of abstraction
without needing to fully detail the task hierarchy. These abstract solutions support flexible execution
by remaining uncommitted about which of the alternative methods will be selected at runtime, based
on the circumstances, to achieve plan subgoals.
Our complexity analyses and experiments in different problem domains have quantified the benefits of using summary information for a refinement planning and local search scheduling algorithm.
d
i
There is a potential doubly exponential speedup of O(kb −b b2(d−i) ) for k ways to resolve a conflict,

505

C LEMENT, D URFEE , & BARRETT

a hierarchy branching factor b, a depth of the hierarchy d, and an abstract solution depth i. An exponential speedup is obtained if abstract solutions are found, if there are fewer summary conditions
at abstract levels, or if alternative decomposition choices lead to varying numbers of threats. These
conditions for exponential improvement are a significant relaxation compared to prior work, and the
performance improvement is greater.
A domain modeler can run the summarization algorithms offline for a library of plan hierarchies
so that summary information is available for the coordination and planning of any set of goal tasks
supported by the library. Using algorithms for reasoning about summary information, agents can
discover with whom they should coordinate and over which states and resources they must coordinate/negotiate. Communicating summary information at different levels of abstraction reduces
communication costs exponentially under conditions similar to those reducing computation time.
The use of summary information in a local search planner (like ASPEN, Section 6.3) is another
contribution of this work. The strength of local search algorithms is their ability to efficiently reason
about large numbers of tasks with constraints on metric resources, state variables, and other complex
resource classes. By integrating algorithms for reasoning about summarized propositional state and
metric resource constraints into a heuristic local search planner/scheduler, we enable such scalable
planning systems to scale to even larger problem domains. This use of summary information in
a different style of planner demonstrates the applicability of abstract reasoning in improving the
performance of different kinds of planning (and plan coordination) systems.
Future work is needed to evaluate the use of summary information in other planning and
scheduling systems and for wider classes of problems requiring more expressive representations
for resources and temporal constraints. Already, an approach for exploiting cooperative action
among agents based on summary information has been developed (Cox & Durfee, 2003). Other
promising approaches include abstracting other plan information, such as probabilistic conditions
and effects and classes of resources and states (e.g. location regions and sub-regions). More work
is also needed to understand how and when to communicate summary information in a distributed
planning system.

Acknowledgments
The authors wish to thank Pradeep Pappachan, Gregg Rabideau, and Russell Knight for help with
implementation. We also thank our anonymous reviewers for their many valuable suggestions. This
work was performed at the Jet Propulsion Laboratory, California Institute of Technology, under
contract with the National Aeronautics and Space Administration, and at the University of Michigan
supported in part by DARPA (F30602-98-2-0142).

Appendix A: Algorithms for Computing Interval Relations
The algorithms for determining whether the defined relations hold between summary conditions
for plans in P use a point algebra constraint table (Vilain & Kautz, 1986). This point algebra
table is constructed for the interval endpoints corresponding to the executions of the plans in P;
a row and column for both p− ≡ ts (e) (start endpoint of execution e of p) and p+ ≡ t f (e) (finish
endpoint) are added for each plan p ∈ P. Each cell of the table gives a time point constraint of
the row to the column that can be <, ≤, =, ≥, >, 6=, <=>, or empty. <=> means that the

506

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

p−
p+
p0 −
p0 +

p−
=
>
>
>

p+
<
=
<
<

p0 −
<
>
=
>

p0 +
<
>
<
=

Table 1: Point algebra table for p contains p0
p−
p+
p0 −
p0 +

p−
=
>
≥
>

p+
<
=
<=>
<=>

p0 −
≤
<=>
=
>

p0 +
<
<=>
<
=

Table 2: Point algebra table for p− before or at p0 −
points are unconstrained. If a cell is empty, then there are no allowed temporal relations, indicating
inconsistency. Table 1 shows a point algebra table for plans p and p0 where they are constrained such
that p’s execution contains that of p0 . Table 2 shows a table where just the start of p is constrained
to be earlier than the start of p0 . Both are transitive closures of these constraint relations. Table 1
can be computed from Table 2 by constraining p+ < p0 + (by putting < in the cell of row p+ and
column p0 + ) and then computing the transitive closure, an O(n2 ) algorithm for n points (Vilain &
Kautz, 1986). After the transitive closure is computed, the constraints of any point on any other
point can be looked up in constant time.
Similarly, the constraints in order for P can be added to the table, and the transitive closure can
be computed to get all constraints entailed from those in order. This only needs to be done once for
any P and order to determine achieve and clobber relationships defined in the next section.
We determine that a plan q in p’s subplans is temporally ordered always-[ f irst,last] if and only
if [q− , q+ ] is constrained [before, after] or equal to all other points in the point algebra table for
p’s subplans. This is done by looking at each entry in the row for [q− , q+ ] and checking to see
that the constraint is [<, >], =, or [≤, ≥]. If this is not the case, then q is not-always-[ f irst,last].
q is always-not-[ f irst,last] if and only if in the row for [q− , q+ ] there is an entry with the [>, <]
constraint; otherwise, it is sometimes-[ f irst,last].
An interval i0 is covered by a set of intervals I = {i1 , i2 , . . . , ik } if and only no interval can be
found that intersects i0 and intersects nothing in I. Our particular covering problem describes the
intervals in terms of a partial order over endpoints, so we represent these intervals in a point algebra
table. An algorithm for the covering problem is to check to see if i0 is covered by looking at all
pairs of intervals to see if they overlap. i0 is not covered if (1) either no intervals in I meet either
+
i−
0 or i0 , (2) there are any intervals that have an endpoint that is contained only by i0 and do not
meet the opposite endpoint of another interval in I or an endpoint of i0 , or (3) there are no intervals
overlapping i0 . Otherwise, i0 is covered. Examples are given in Figure 34.

507

C LEMENT, D URFEE , & BARRETT

a)
=

=

B

=
C

c)

E

b)

A

F
=
G

D

H

I

Figure 34: a) Interval A is covered by B, C, and D. b) E is not covered by F, G, and H. c) I is not
covered.

Appendix B: Algorithms for Must/May Asserting Summary Conditions
Here we describe algorithms for determining temporal plan relationships based on summary information. They are used to build other algorithms that determine whether plan must or may achieve,
clobber, or undo the condition of another under particular ordering constraints.
The definitions and algorithms throughout this section are given within the context of a set of
plans P with a corresponding set of summary information Psum , a set of ordering constraints order,
and a set of histories H including all histories where E(h) only includes an execution e of each
plan in P and e’s subexecutions, and E(h) satisfies all constraints in order. They are all concerned
with the ordering of plan execution intervals and the timing of conditions. By themselves, they do
not have anything to do with whether conditions may need to be met or must be met for a plan
execution.
First, in order to determine whether abstract plan executions can achieve, clobber, or undo
conditions of others, an agent needs to be able to reason about how summary conditions are asserted
and required to be met. Ultimately, the agent needs to be able to determine whether a partial ordering
of abstract plans can succeed, so it may be the case that an agent’s action fails to assert a summary
condition that is required by the action of another agent. Therefore, we formalize what it means
for an action to attempt to assert a summary condition and to require that a summary condition be
met. These definitions rely on linking the summary condition of a plan to the CHiP conditions it
summarizes in the subplans of the plan’s decompositions. Thus, we first define what it means for a
summary condition to summarize these conditions.
Definition 14
A summary condition c summarizes a condition ` in condition set conds
of plan p iff c was added by the procedure for deriving summary information to a
summary condition set of p0 ; ` = `(c); and either c was added for ` in a condition
set conds of p = p0 , or c was added for a summary condition of a subplan of p0 that
summarizes ` in conds of p.
For example, at(bin1, A) is a precondition of the start move plan for moving part A from bin1
to machine M1 (as given in Section 2.2). When deriving the summary conditions for start move,

508

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

at(bin1, A) is added to the summary preconditions. Thus, the summary precondition at(bin1,
A)MuF summarizes at(bin1, A) in the preconditions of start move.
Definition 15
An execution e of p requires a summary condition c to be met at t iff c is
a summary condition in p’s summary information; there is a condition ` in a condition
set conds of p0 that is summarized by c; if f irst(c), t = ts (e); if last(c), t = t f (e);
if always(c), t is within (ts (e),t f (e)); and if sometimes(c), there is an execution of
a subplan of p in d(e) that requires a summary condition c0 to be met at t, and c0
summarizes ` in conds of p0 .
So, basically, an execution requires a summary condition to be met whenever the conditions it
summarizes are required. The execution of build G has a summary precondition at(A,M1 tray1).
This execution requires this summary condition to be met at ts (build G) because at(A, M1 tray1) is
a precondition of build G’s first subplan that is summarized by build G’s summary precondition.
Definition 16
An execution e of p attempts to assert a summary condition c at t iff c is
a summary condition in p’s summary information; there is a condition ` in a condition
set conds of p0 that is summarized by c; ¬ f irst(c); if always(c), t is in the smallest
interval after ts (e) and before the start or end of any other execution that follows ts (e);
if last(c), t = t f (e); and if sometimes(c), there is an execution of a subplan of p in d(e)
that attempts to assert a summary condition c0 at t; and c0 summarizes ` in conds of p0 .
We say that an execution “attempts” to assert a summary condition because asserting a condition
can fail due to a simultaneous assertion of the negation of the condition. Like the example above
for requiring a summary condition, the executions of build G, produce G on M1, and produce H
all assert summary postconditions that M1 becomes available at t f (build G).
In order for agents to determine potential interactions among their abstract plans (such as clobbering or achieving), they need to reason about when a summary condition is asserted by one plan in
relation to when it is asserted or required by another. Based on interval or point algebra constraints
over a set of abstract plans, an agent specifically would need to be able to determine whether a plan
would assert a summary condition before or by the time another plan requires or asserts a summary
condition on the same state variable. In addition, to reason about clobbering inconditions, an agent
would need to determine if a summary condition would be asserted during the time a summary incondition c was required (asserted in c). Agents also need to detect when a summary postcondition
would be asserted at the same time as another summary postcondition c (asserted when c).
We do not consider cases where executions attempt to assert a summary in- or postcondition
at the same time an incondition is asserted because in these cases, clobber relations are already
detected because executions always require the summary inconditions that they attempt to assert.
For example, if equip M1 attempted to assert the incondition that M1 was unavailable at the same
time that build G attempted to assert the postcondition that M1 was available, the incondition would
be clobbered by the postcondition.
In the case that the ordering constraints allow for alternative synchronizations of the abstract
plans, the assertions of summary conditions may come in different orders. Therefore, we formalize
must-assert and may-assert to determine when these relationships must or may occur respectively.
As mentioned at the beginning of Section 9, this use of “must” and “may” is based only on disjunctive orderings and not on the existence of summary conditions in different decompositions. For
509

C LEMENT, D URFEE , & BARRETT

1
2
3
4

5
6
7

8
9

10
11

12
13
14
15

16
17
18
19

c0 ∈ post(p0 )
last
T
T
F
?
c0 ∈ in(p0 )
always
T
T
F
c0 ∈ post(p0 )
last
T
F
c0 ∈ in(p0 )
always
T
F
c0 ∈ post(p0 )
last
T
T
F
F
c0 ∈ in(p0 )
always
T
T
F
F

p0 must-assert c0 by c
order must impose
these constraints

p0 must-assert c0 before c
order must impose
these constraints

p0 + ≤ p−
p0 + ≤ p−
p0 + ≤ p−
p0 + ≤ p−

p0 + < p−
p0 + < p−
p0 + < p−
p0 + < p−

T
F
?
c ∈ in(p)
always
?
?

p0 − < p−
p0 − ≤ p−
f alse

p0 − < p−
p0 − ≤ p−
f alse

p0 + ≤ p−
p0 + ≤ p−

p0 + ≤ p−
p0 + ≤ p−

?
?
c ∈ post(p)
last
T
F
T
F

p0 − ≤ p−
f alse

p0 − < p−
f alse

p0 + ≤ p+
p0 + ≤ p−
p0 + ≤ p+
p0 + ≤ p−

p0 + < p+
p0 + ≤ p−
p0 + ≤ p+
p0 + ≤ p−

T
F
T
F

p0 − < p+
p0 − ≤ p−
f alse
f alse

p0 − < p+
p0 − ≤ p−
f alse
f alse

c ∈ pre(p)
f irst
T
F
?
?

Table 3: Table for must-assert by/before algorithm
the following definitions and algorithms of must- and may-assert, we assume c and c0 are summary
conditions of plans in P.
Definition 17
p0 ∈ P must-assert c0 [by, before] c iff for all histories h ∈ H and all t
where e is the top-level execution in E(h) of some p ∈ P that requires c to be met at t,
and e0 is the top-level execution of p0 in E(h), there is a t 0 where e0 attempts to assert c0
at t 0 , and [t 0 ≤ t, t 0 < t].
The must-assert algorithm is described in Table 3. p0 must-assert c0 by c iff order entails the
relationship given for the row corresponding to the type and timing of the two conditions. Rows of
the table indicate the timing of both summary conditions and the constraints that order must dictate
for must-assert to be true. ’T’ and ’F’ in the table indicate whether the timing in the column is true
or false for the condition. ’?’ means that timing doesn’t matter for that condition in this case. For
example, row 9 says that for the case where c0 is a sometimes (¬last) postcondition of p0 , and c is
an incondition of p with any timing, order must require that the end of p0 be before or at the start
of p in order for p0 to must-assert c0 by the time c is asserted or required.

510

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

1
2
3
4

5
6

7
8
9
10

11
12

13
14
15
16

17
18

c0 ∈ post(p0 )
last
T
T
F
F
c0 ∈ in(p0 )
always
?
?
c0 ∈ post(p0 )
last
T
T
F
F
c0 ∈ in(p0 )
always
?
?
c0 ∈ post(p0 )
last
T
T
F
F
c0 ∈ in(p0 )
always
?
?

p0 may-assert c0 by c
order cannot impose
these constraints

p0 may-assert c0 before c
order cannot impose
these constraints

p0 + > p−
p0 + ≥ p+
p0 − ≥ p−
p0 − ≥ p+

p0 + ≥ p−
p0 + ≥ p+
p0 − ≥ p−
p0 − ≥ p+

T
F
c ∈ in(p)
always
T
F
T
F

p0 − ≥ p−
p0 − ≥ p+

p0 − ≥ p−
p0 − ≥ p+

p0 + > p−
p0 + ≥ p+
p0 − ≥ p−
p0 − ≥ p+

p0 + > p−
p0 + ≥ p+
p0 − ≥ p−
p0 − ≥ p+

T
F
c ∈ post(p)
last
T
F
T
F

p0 − > p−
p0 − ≥ p+

p0 − ≥ p−
p0 − ≥ p+

p0 + > p+
p0 + ≥ p+
p0 − ≥ p+
p0 − ≥ p+

p0 + ≥ p+
p0 + ≥ p+
p0 − ≥ p+
p0 − ≥ p+

T
F

p0 − ≥ p+
p0 − ≥ p+

p0 − ≥ p+
p0 − ≥ p+

c ∈ pre(p)
f irst
T
F
T
F

Table 4: Table for may-assert by/before algorithm

1
2
3
4

5
6
7
8

c0 ∈ post(p0 )
last
T
T
F
F
c0 ∈ in(p0 )
always
T
T
F
F

p0 must-assert c0 in c
order must impose
these constraints

c ∈ in(p)
always
T
F
T
F

p0 + > p− and p0 + < p+
f alse
p0 − ≥ p− and p0 + ≤ p+
f alse

T
F
T
F

p0 − ≥ p− and p0 − < p+
f alse
f alse
f alse

c0 ∈ post(p0 )
last
T
T
F
F
c0 ∈ in(p0 )
always
T
T
F
F

c ∈ in(p)
always
T
F
T
F

p0 + ≤ p−
p0 + ≤ p−
p0 + ≤ p−
p0 + ≤ p−

or
or
or
or

p0 + ≥ p+
p0 + ≥ p+
p0 − ≥ p+
p0 − ≥ p+

T
F
T
F

p0 + ≤ p−
p0 + ≤ p−
p0 + ≤ p−
p0 + ≤ p−

or
or
or
or

p0 − ≥ p+
p0 − ≥ p+
p0 − ≥ p+
p0 − ≥ p+

Table 5: Table for must/may-assert in algorithm

511

p0 may-assert c0 in c
order cannot impose
these constraints

C LEMENT, D URFEE , & BARRETT

1
2
3
4

c0 ∈ post(p0 )
last
T
T
F
F

c ∈ post(p)
last
T
F
T
F

p0 must-assert c0 when c
order must impose
these constraints
p0 + = p+
f alse
f alse
f alse

c0 ∈ post(p0 )
last
T
T
F
F

c ∈ post(p)
last
T
F
T
F

p0 may-assert c0 when c
order cannot impose
these constraints
p0 + 6= p+
≤ p− or p0 + ≥ p+
p0 + ≤ p+ or p0 − ≥ p+
p0 + ≤ p− or p0 − ≥ p+
p0 +

Table 6: Table for must/may-assert when algorithm
The definitions and algorithms for the other assert relationships are similar. Tables 4-6 describe
the logic for the other algorithms. For may relationships, the algorithm returns true iff none of the
corresponding ordering constraints in the table are imposed by (can be deduced from) order.
We illustrate these relationships for the example in Figure 8. In Figure 8a the agents’ plans
are unordered with respect to each other. Part G is produced either on machine M1 or M2 depending on potential decompositions of the produce G plan. produce G must-assert c0 = must,
last available(G) before c = must, f irst available(G) in the summary preconditions of move G
because no matter how the plans are decomposed (for all executions and all histories of the plans
under the ordering constraints in the figure), the execution of produce G attempts to assert c0 before the execution of move G requires c to be met. The algorithm verifies this by finding that the
end of produce G is ordered before the start of move G (row 1 in Table 3). It is also the case that
equip M2 tool may-assert c0 = must, last ¬available(M2) by c = may, sometimes available(M2)
in the summary preconditions of produce G because the two plans are unordered with respect to
each other, and in some history equip M2 tool can precede produce G. The algorithm finds that
this is true since equip M2 is not constrained to start after the start of produce G (row 2 in Table 4).
In Figure 8b, move tool may-assert c0 = must, last f ree(transport1) in c = may, sometimes
¬ f ree(transport1) in produce G’s summary inconditions because in some history move tool attempts to assert c0 during the time that produce G is using transport1 to move part A to machine
M2. In addition, equip M2 tool must-assert c0 = must, last ¬available(M2) when c = may, last
available(M2) in produce G’s summary postconditions because equip M2 tool attempts to assert
c0 at the same time that produce G requires c to be met. The end of Section 3.3 gives other examples.

References
Allen, J., Kautz, H., Pelavin, R., & Tenenberg, J. (1991). Reasoning about plans. Morgan Kaufmann.
Allen, J. F. (1983). Maintaining knowledge about temporal intervals. Communications of the ACM,
26(11), 832–843.
Allen, J. F., & Koomen, J. A. (1983). Planning using a temporal world model. In Proceedings of
the International Joint Conference on Artificial Intelligence, pp. 741–747.
Bock, C. (1996). Unified process specification language: Requirements for modeling process. Tech.
rep. NISTIR 5910, National Institute of Standards and Technology.
Castillo, L., Fdez-Olivares, J., Garcı́a-Pérez, O., & Palao, F. (2006). Efficiently handling temporal
knowledge in an HTN planner. In 16th International Conference on Automated Planning and
512

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

Scheduling (ICAPS-06), pp. 63–72. AAAI.
Chien, S., Knight, R., Stechert, A., Sherwood, R., & Rabideau, G. (2000a). Using iterative repair to
improve the responsiveness of planning and scheduling. In Proceedings of the International
Conference on AI Planning and Scheduling, pp. 300–307.
Chien, S., Rabideu, G., Knight, R., Sherwood, R., Engelhardt, B., Mutz, D., Estlin, T., Smith, B.,
Fisher, F., Barrett, T., Stebbins, G., & Tran, D. (2000b). Automating space mission operations
using automated planning and scheduling. In Proc. SpaceOps.
Clement, B. (2002). Abstract Reasoning for Multiagent Coordination and Planning. Ph.D. thesis,
University of Michigan, Ann Arbor.
Clement, B., & Durfee, E. (1999). Top-down search for coordinating the hierarchical plans of
multiple agents. In Proceedings of the International Conference on Autonomous Agents.
Corkill, D. (1979). Hierarchical planning in a distributed environment. In Proceedings of the
International Joint Conference on Artificial Intelligence, pp. 168–175.
Cox, J. S., & Durfee, E. H. (2003). Discovering and exploiting synergy between hierarchical planning agents. In Proceedings of the International Joint Conference on Autonomous Agents and
MultiAgent Systems, pp. 281–288.
Currie, K., & Tate, A. (1991). O-Plan: The open planning architecture. Artificial Intelligence, 52,
49–86.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49,
61–95.
Decker, K. (1995). Environment centered analysis and design of coordination mechanisms. Ph.D.
thesis, University of Massachusetts.
desJardins, M., & Wolverton, M. (1999). Coordinating a distributed planning system. AI Magazine,
20(4), 45–53.
Drabble, B., & Tate, A. (1994). The use of optimistic and pessimistic resource profiles to inform
search in an activity based planner. In Artificial Intelligence Planning Systems, pp. 243–248.
Durfee, E. H., & Montgomery, T. A. (1991). Coordination as distributed search in a hierarchical
behavior space. IEEE Transactions of Systems, Man and Cybernetics, 21(6), 1363–1378.
Emerson, E., & Halpern, J. Y. (1985). Decision procedures and expressiveness in the temporal logic
of branching time. Journal of Computer and System Sciences, 30(1), 1–24.
Ephrati, E., & Rosenschein, J. (1994). Divide and conquer in multi-agent planning. In Proceedings
of the National Conference on Artificial Intelligence, pp. 375–380.
Erol, K., Hendler, J., & Nau, D. (1994a). Semantics for hierarchical task-network planning. Tech.
rep. CS-TR-3239, University of Maryland.
Erol, K., Nau, D., & Hendler, J. (1994b). UMCP: A sound and complete planning procedure for
hierarchical task-network planning.. In Proceedings of the International Conference on AI
Planning and Scheduling.
Fagin, R., Halpern, J., Moses, Y., & Vardi, M. (1995). Reasoning about knowledge. MIT Press.
Firby, J. (1989). Adaptive Execution in Complex Dynamic Domains. Ph.D. thesis, Yale University.
513

C LEMENT, D URFEE , & BARRETT

Georgeff, M. P. (1983). Communication and interaction in multiagent planning. In Proceedings of
the National Conference on Artificial Intelligence, pp. 125–129.
Georgeff, M. P. (1984). A theory of action for multiagent planning. In Proceedings of the National
Conference on Artificial Intelligence, pp. 121–125.
Georgeff, M. P., & Lansky, A. (1986). Procedural knowledge. Proceedings of IEEE, 74(10), 1383–
1398.
Giunchiglia, F., & Traverso, P. (1999). Planning as model checking. In Proceedings of the 5th
European Conference on Planning, pp. 1–20, London, UK. Springer-Verlag.
Glabbeek, R. v. (1997). Notes on the methodology of CCS and CSP. Theoretical Computer Science,
177(2), 329–349. Originally appeared as Report CS-R8624, CWI, Amsterdam, 1986.
Grosz, B., & Kraus, S. (1996). Collaborative plans for complex group action. Artificial Intelligence,
86, 269–358.
Huber, M. (1999). JAM: A BDI-theoretic mobile agent architecture. In Proceedings of the International Conference on Autonomous Agents, pp. 236–243.
Knight, R., Rabideau, G., & Chien, S. (2000). Computing valid intervals for collections of activities with shared states and resources. In Proceedings of the International Conference on AI
Planning and Scheduling, pp. 600–610.
Knoblock, C. (1991). Search reduction in hierarchical problem solving. In Proceedings of the
National Conference on Artificial Intelligence, pp. 686–691.
Korf, R. (1987). Planning as search: A quantitative approach. Artificial Intelligence, 33, 65–88.
Laborie, P., & Ghallab, M. (1995). Planning with sharable resource constraints. In Proceedings of
the International Joint Conference on Artificial Intelligence, pp. 1643–1649.
Lansky, A. (1990). Localized search for controlling automated reasoning. In Proceedings of the
DARPA Workshop on Innovative Approaches to Planning, Scheduling and Control, pp. 115–
125.
Lee, J., Huber, M. J., Durfee, E. H., & Kenny, P. G. (1994). UMPRS: An implementation of the
procedural reasoning system for multirobot applications. In Proceedings of the AIAA/NASA
Conference on Intelligent Robotics in Field, Factory, Service, and Space, pp. 842–849.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings of the
National Conference on Artificial Intelligence, pp. 634–639.
Muscettola, N. (1994). HSTS: Integrating planning scheduling. Intelligent Scheduling, 169–212.
Nau, D., Au, T., Ilghami, O., Kuter, U., Murdock, J., Wu, D., & Yaman, F. (2003). SHOP2: An
HTN planning system. Journal of Artificial Intelligence Research, 20, 379–404.
Pappachan, P. (2001). Coordinating Plan Execution in Dynamic Multiagent Environments. Ph.D.
thesis, University of Michigan, Ann Arbor.
Pratt, V. R. (1976). Semantical considerations on floyd-hoare logic. In 17th Annual IEEE Symposium on Foundations of Computer Science, pp. 109–121.
Rao, A. S., & Georgeff, M. P. (1995). BDI-agents: From theory to practice. In Proceedings of the
International Conference on Multi-Agent Systems, San Francisco.
514

A BSTRACT R EASONING FOR P LANNING AND C OORDINATION

Sacerdoti, E. (1974). Planning in a hierarchy of abstraction spaces. Artificial Intelligence, 5(2),
115–135.
Sacerdoti, E. D. (1977). A Structure for Plans and Behavior. Elsevier-North Holland.
Schlenoff, C., Knutilla, A., & Ray, S. (2006). Interprocess communication in the process specification language. Tech. rep. NISTIR 7348, National Institute of Standards and Technology.
Tate, A. (1977). Generating project networks. In Proceedings of the International Joint Conference
on Artificial Intelligence, pp. 888–893.
Thangarajah, J., Padgham, L., & Winikoff, M. (2003). Detecting & avoiding interference between
goals in intelligent agents. In Proceedings of the International Joint Conference on Artificial
Intelligence, pp. 721–726.
Tsuneto, R., Hendler, J., & Nau, D. (1997). Space-size minimization in refinement planning. In
Proceedings of the European Conference on Planning.
Tsuneto, R., Hendler, J., & Nau, D. (1998). Analyzing external conditions to improve the efficiency
of HTN planning. In Proceedings of the National Conference on Artificial Intelligence, pp.
913–920.
Vilain, & Kautz, H. (1986). Constraint propagation algorithms for temporal reasoning. In Proceedings of the National Conference on Artificial Intelligence, pp. 377–382.
Weld, D. (1994). An introduction to least commitment planning. AI Magazine, 15(4), 27–61.
Wilkins, D. E. (1990). Can AI planners solve practical problems?. Computational Intelligence,
6(4), 232–246.
Wolverton, M., & desJardins, M. (1998). Controlling communication in distributed planning using
irrelevance reasoning. In Proceedings of the National Conference on Artificial Intelligence,
pp. 868–874.
Yang, Q. (1990). Formalizing planning knowledge for hierarchical planning. Computational Intelligence, 6(1), 12–24.
Yang, Q. (Ed.). (1997). Intelligent Planning: A Decomposition and Abstraction Based Approach.
Springer.
Young, M., Pollack, M., & Moore, J. (1994). Decomposition and causality in partial-order planning.
In Proceedings of the International Conference on AI Planning and Scheduling, pp. 188–193.

515

Journal of Artificial Intelligence Research 28 (2007) 1-48

Submitted 4/06; published 1/07

Cutset Sampling for Bayesian Networks
Bozhena Bidyuk
Rina Dechter

bbidyuk@ics.uci.edu
dechter@ics.uci.edu

School of Information and Computer Science
University Of California Irvine
Irvine, CA 92697-3425

Abstract
The paper presents a new sampling methodology for Bayesian networks that samples
only a subset of variables and applies exact inference to the rest. Cutset sampling is a
network structure-exploiting application of the Rao-Blackwellisation principle to sampling
in Bayesian networks. It improves convergence by exploiting memory-based inference algorithms. It can also be viewed as an anytime approximation of the exact cutset-conditioning
algorithm developed by Pearl. Cutset sampling can be implemented efficiently when the
sampled variables constitute a loop-cutset of the Bayesian network and, more generally,
when the induced width of the network’s graph conditioned on the observed sampled variables is bounded by a constant w. We demonstrate empirically the benefit of this scheme
on a range of benchmarks.

1. Introduction
Sampling is a common method for approximate inference in Bayesian networks. When
exact algorithms are impractical due to prohibitive time and memory demands, it is often
the only feasible approach that offers performance guarantees. Given a Bayesian network
over the variables X = {X1 , ..., Xn }, evidence e, and a set of samples {x(t) } from P (X|e), an
estimate fˆ(X) of the expected value of function f (X) can be obtained from the generated
samples via the ergodic average:
1X
f (x(t) ) ,
(1)
E[f (X)|e] ≈ fˆ(X) =
T t
where T is the number of samples. fˆ(X) can be shown to converge to the exact value as T
increases. The central query of interest over Bayesian networks is computing the posterior
marginals P (xi |e) for each value xi of variable Xi , also called belief updating. For this
query, f (X) equals a δ-function, and the above equation reduces to counting the fraction
of occurrences of Xi = xi in the samples,
P̂ (xi |e) =
(t)

T
1X
δ(xi |x(t) ) ,
T

(2)

t=1

where δ(xi |x(t) )=1 iff xi = xi and δ(xi |x(t) )=0 otherwise. Alternatively, a mixture estimator can be used,
T
1X
(t)
P (xi |x−i ) ,
(3)
P̂ (xi |e)] =
T
t=1

c
2007
AI Access Foundation. All rights reserved.

Bidyuk & Dechter

(t)

where x−i = x(t) \xi .
A significant limitation of sampling, however, is that the statistical variance increases
when the number of variables in the network grows and therefore the number of samples
necessary for accurate estimation increases. In this paper, we present a sampling scheme for
Bayesian networks with discrete variables that reduces the sampling variance by sampling
from a subset of the variables, a technique also known as collapsing or Rao-Blackwellisation.
The fundamentals of Rao-Blackwellised sampling were developed by Casella and Robert
(1996) and Liu, Wong, and Kong (1994) for Gibbs sampling and MacEachern, Clyde,
and Liu (1998) and Doucet, Gordon, and Krishnamurthy (1999) for importance sampling.
Doucet, de Freitas, Murphy, and Russell (2000) extended Rao-Blackwellisation to Particle
Filtering in Dynamic Bayesian networks.
The basic Rao-Blackwellisation scheme can be described as follows. Suppose we partition the space of variables X into two subsets C and Z. Subsequently, we can re-write any
function f (X) as f (C, Z). If we can generate samples from distribution P (C|e) and compute E[f (C, Z)|c, e], then we can perform sampling on subset C only, generating samples
c(1) , c(2) , ..., c(T ) and approximating the quantity of interest by
E[f (C, Z)|e] = EC [EZ [f (C, Z)|c, e]] ≈ fˆ(X) =

T
1X
EZ [f (C, Z)|c(t) , e] .
T

(4)

t=1

The posterior marginals’ estimates for the cutset variables can be obtained using an expression similar to Eq.(2),
1X
δ(ci |c(t) ) ,
(5)
P̂ (ci |e) =
T t

or using a mixture estimator similar to Eq.(3),
P̂ (ci |e) =

1X
(t)
P (ci |c−i , e) .
T t

(6)

For Xi ∈ X\C, E, E[P (Xi |e)] = EC [P (Xi |c, e)] and Eq.(4) becomes
P̂ (Xi |e) =

1X
P (Xi |c(t) , e) .
T t

(7)

Since the convergence rate of Gibbs sampler is tied to the maximum correlation between
two samples (Liu, 2001), we can expect an improvement in the convergence rate when
sampling in a lower dimensional space since 1) some of the highly-correlated variables may be
marginalized out and 2) the dependencies between the variables inside a smaller set are likely
to be weaker because the variables will be farther apart and their sampling distributions will
be smoothed out. Additionally, the estimates obtained from sampling in a lower dimensional
space can be expected to have lower sampling variance and therefore require fewer samples
to achieve the same accuracy of the estimates. On the other hand, the cost of generating
each sample may increase. Indeed, the principles of Rao-Blackwellised sampling have been
applied only in a few classes of probabilistic models with specialized structure (Kong, Liu,
& Wong, 1994; Escobar, 1994; MacEachern, 1994; Liu, 1996; Doucet & Andrieu, 2001;
Andrieu, de Freitas, & Doucet, 2002; Rosti & Gales, 2004).
2

Cutset Sampling for Bayesian Networks

The contribution of this paper is in presenting a general, structure-based scheme which
applies the Rao-Blackwellisation principle to Bayesian networks. The idea is to exploit the
property that conditioning on a subset of variables simplifies the network’s structure, allowing efficient query processing by exact algorithms. In general, exact inference by variable
elimination (Dechter, 1999a, 2003) or join-tree algorithms (Lauritzen & Spiegelhalter, 1988;
Jensen, Lauritzen, & Olesen, 1990) is time and space exponential in the induced-width w
of the network. However, when a subset of the variables is assigned (i.e., conditioned upon)
the induced-width of the conditioned network may be reduced.
The idea of cutset sampling is to choose a subset of variables C such that conditioning
on C yields a sparse enough Bayesian network having a small induced width to allow exact
inference. Since a sample is an assignment to all cutset variables, we can efficiently generate
a new sample over the cutset variables in the conditioned network where the computation of
P (c|e) and P (Xi |c, e) can be bounded. In particular, if the sampling set C cuts all the cycles
in the network (i.e., it is a loop-cutset), inference over the conditioned network becomes
linear. In general, if C is a w-cutset, namely a subset of nodes such that when assigned, the
induced-width of the conditioned network is w, the time and space complexity of computing
the next sample is O(|C| · N · dw+2 ) where d is the maximum domain size and N = |X|.

The idea of exploiting properties of conditioning on a subset of variables was first proposed for exact belief updating in the context of cutset-conditioning (Pearl, 1988). This
scheme requires enumerating all instantiations of the cutset variables. Since the number of
instances is exponential in the size of the cutset |C|, sampling over the cutset space may
be the right compromise when the size of the cutset is too big. Thus, sampling on a cutset
can also be viewed as an anytime approximation of the cutset-conditioning approach.
Although Rao-Blackwellisation in general and cutset sampling in particular can be applied in the context of any sampling algorithm, we will introduce the principle in the context of Gibbs sampling (Geman & Geman, 1984; Gilks, Richardson, & Spiegelhalter, 1996;
MacKay, 1996), a Markov Chain Monte Carlo sampling method for Bayesian networks.
Extension to any other sampling approach or any other graphical models, such as Markov
networks, should be straight forward. We recently demonstrated how the idea can be incorporated into importance sampling (Bidyuk & Dechter, 2006).
The paper defines and analyzes the cutset sampling scheme and investigates empirically
the trade-offs between sampling and exact computation over a variety of randomly generated
networks and grid structure networks as well as known real-life benchmarks such as CPCS
networks and coding networks. We show that cutset sampling converges faster than pure
sampling in terms of the number of samples, as dictated by theory, and is also almost always
time-wise cost effective on all the benchmarks tried. We also demonstrate the applicability of
this scheme to some deterministic networks, such as Hailfinder network and coding networks,
where the Markov Chain is non-ergodic and Gibbs sampling does not converge.
Section 2 provides background information. Specifically, section 2.1 introduces Bayesian
networks, section 2.2 reviews exact inference algorithms for Bayesian networks, and section 2.3 provides background on Gibbs sampling. The contribution of the paper presenting
the cutset sampling starts in section 3. Section 4 presents the empirical evaluation of cutset
sampling. We also present an empirical evaluation of the sampling variance and the resulting standard error based on the method of batch means (for more details, see Geyer, 1992).
3

Bidyuk & Dechter

In section 5, we review previous application of Rao-Blackwellisation and section 6 provides
summary and conclusions.

2. Background
In this section, we define essential terminology and provide background information on
Bayesian networks.
2.1 Preliminaries
We use upper case letters without subscripts, such as X, to denote sets of variables and
lower case letters without subscripts to denote an instantiation of a group of variables (e.g.,
x indicates that each variable in set X is assigned a value). We use an upper case letter
with a subscript, such as Xi , to denote a single variable and a lower case letter with a
subscript, such as xi , to denote an instantiated variable (e.g., xi denotes an arbitrary value
in the domain of Xi and means Xi = xi ). D(Xi ) denotes the domain of the variable Xi . A
superscript in a subscripted lower case letter would be used to distinguish different specific
values for a variable, i.e., D(Xi ) = {x1i , x2i , ...}. We will use x to denote an instantiation of
a set of variables x = {x1 , ..., xi−1 , xi , xi+1 , ..., xn } and x−i = x\xi to denote x with element
xi removed. Namely, x−i = {x1 , x2 , ..., xi−1 , xi+1 , ..., xn }.
Definition 2.1 (graph concepts) A directed graph is a pair D=<V ,E>, where V =
{X1 , ..., Xn } is a set of nodes and E = {(Xi , Xj )|Xi , Xj ∈ V } is the set of edges. Given
(Xi , Xj ) ∈ E, Xi is called a parent of Xj , and Xj is called a child of Xi . The set of
Xi ’s parents is denoted pa(Xi ), or pai , while the set of Xi ’s children is denoted ch(Xi ), or
chi . The family of Xi includes Xi and its parents. The moral graph of a directed graph
D is the undirected graph obtained by connecting the parents of all the nodes in D and
removing the arrows. A cycle-cutset of an undirected graph a subset of nodes that, when
removed, yields a graph without cycles. A loop in a directed graph D is a subgraph of D
whose underlying graph is a cycle. A directed graph is acyclic if it has no directed loops. A
directed graph is singly-connected (also called a poly-tree), if its underlying undirected
graph has no cycles. Otherwise, it is called multiply-connected.
Definition 2.2 (loop-cutset) A vertex v is a sink with respect to a loop L if the two
edges adjacent to v in L are directed into v. A vertex that is not a sink with respect to a
loop L is called an allowed vertex with respect to L. A loop-cutset of a directed graph D
is a set of vertices that contains at least one allowed vertex with respect to each loop in D.
Definition 2.3 (Belief Networks) Let X = {X1 , ..., Xn } be a set of random variables
over multi-valued domains D(X1 ), ..., D(Xn ). A belief network (BN) (Pearl, 1988) is
a pair <G, P > where G is a directed acyclic graph whose nodes are the variables X and
P = {P (Xi |pai )|i = 1, ..., n} is the set of conditional probability tables (CPTs) associated
with each Xi . The BN represents a joint probability distribution having the product form:
P (x1 , ...., xn ) =

n
Y
i=1

P (xi |pa(Xi ))

An evidence e is an instantiated subset of variables E ⊂ X.
4

Cutset Sampling for Bayesian Networks

The structure of the directed acyclic graph G reflects the dependencies between the
variables using d-separation criterion. The parents of a variable Xi together with its children
and parents of its children form a Markov blanket, denoted markovi , of node Xi . We
will use xmarkovi to denote x restricted to variables in markovi . We know that the node
Xi is independent of the rest of the variables conditioned on its Markov blanket. Namely,
P (xi |x−i ) = P (xi |xmarkovi ).
The most common query over belief networks is belief updating which is the task of
computing the posterior distribution P (Xi |e) given evidence e and a query variable Xi ∈
X. Reasoning in Bayesian networks is NP-hard (Cooper, 1990). Finding approximate
posterior marginals with a fixed accuracy is also NP-hard (Dagum & Luby, 1993; Abdelbar
& Hedetniemi, 1998). When the network is a poly-tree, belief updating and other inference
tasks can be accomplished in time linear in size of the input. In general, exact inference is
exponential in the induced width of the network’s moral graph.
Definition 2.4 (induced-width) The width of a node in an ordered undirected graph
is the number of the node’s neighbors that precede it in the ordering. The width of an
ordering d, denoted w(d), is the maximum width over all nodes. The induced width
of an ordered graph, w∗ (d), is the width of the ordered graph obtained by processing the
nodes from last to first as follows: when node X is processed, all its preceding neighbors are
connected. The resulting graph is called induced graph or triangulated graph.
The task of finding the minimal induced width of a graph (over all possible orderings) is
NP-complete (Arnborg, 1985).
2.2 Reasoning in Bayesian Networks
Belief propagation algorithm, which we introduce in Section 2.2.2 below, performs belief
updating in singly-connected Bayesian networks in time linear in the size of the input
(Pearl, 1988). In loopy networks, the two main approaches for belief updating are cutset
conditioning and tree clustering. These algorithms are often referred to as “inference”
algorithms. We will briefly describe the idea of clustering algorithms in Section 2.2.1 and
the conditioning method in Section 2.2.3.
2.2.1 Variable Elimination and Join-Tree Clustering (JTC)
The join-tree clustering approach (JTC) refers to a family of algorithms including jointree propagation (Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990) and bucket-tree
elimination (Dechter, 2003, 1999a). The idea is to first obtain a tree-decomposition of
the network into clusters of functions connected as a tree and then propagate messages
between the clusters in the tree. The tree-decomposition is a singly-connected undirected
graph whose nodes, also called clusters, contain subsets of variables and input functions
defined over those variables. The tree-decomposition must contain each function once and
satisfy running intersection property (Maier, 1983). For a unifying perspective of treedecomposition schemes see (Zhang & Poole, 1994; Dechter, 1999b; Kask, Dechter, Larrosa,
& Dechter, 2005).
Given a tree-decomposition of the network, the message propagation over this tree can
be synchronized. We select any one cluster as the root of the tree and propagate messages
5

Bidyuk & Dechter

up and down the tree. A message from cluster Vi to neighbor Vj is a function over the
separator set Vi ∩ Vj that is a marginalization of the product of all functions in Vi and
all messages that Vi received from its neighbors besides Vj . Assuming that the maximum
number of variables in a cluster is w + 1 and maximum domain size is d, the time and
space required to process one cluster is O(d(w+1) ). Since the maximum number of clusters
is bounded by |X| = N , the complexity of variable-elimination algorithms and cluster-tree
propagation schemes is O(N · d(w+1) ). The parameter w, the maximum cluster size minus
1, is called the tree-width of the tree decomposition. The minimal tree-width is identical to
the minimal induced width of a graph.
2.2.2 Iterative Belief Propagation (IBP)
Belief propagation (BP) is an iterative message-passing algorithm that performs exact inference for singly-connected Bayesian networks (Pearl, 1988). In each iteration, every node Xi
sends a πj (Xi ) message to each child j and receives a λj (Xi ) message from each child. The
message-passing order can be organized so that it converges in two iterations. In essence the
algorithm is the same as the join-tree clustering approach applied directly to the poly-tree.
Applied to Bayesian networks with loops, the algorithm usually iterates longer (until it may
converge) and hence, is known as Iterative Belief Propagation (IBP) or loopy belief propagation. IBP provides no guarantees on convergence or quality of approximate posterior
marginals but was shown to perform well in practice (Rish, Kask, & Dechter, 1998; Murphy,
Weiss, & Jordan, 1999). It is considered the best algorithm for inference in coding networks
(Frey & MacKay, 1997; Kschischang & Frey, 1998) where finding the most probable variable
values equals the decoding process (McEliece, MacKay, & Cheng, 1997). Algorithm IBP
requires linear space and usually converges fast if it converges. In our benchmarks, IBP
converged within 25 iterations or less (see Section 4).
2.2.3 Cutset Conditioning
When the tree-width w of the Bayesian network is too large and the requirements of inference
schemes such as bucket elimination and join-tree clustering (JTC) exceed available memory,
we can switch to the alternative cutset conditioning schemes (Pearl, 1988; Peot & Shachter,
1992; Shachter, Andersen, & Solovitz, 1994). The idea of cutset conditioning is to select
a subset of variables C ⊂ X\E, the cutset, and obtain posterior marginals for any node
Xi ∈ X\C, E using:
X
P (xi |e) =
P (xi |c, e)P (c|e)
(8)
c∈D(C)

Eq.(8) above implies that we can enumerate all instantiations over C, perform exact inference for each cutset instantiation c to obtain P (xi |c, e) and P (c|e) and then sum up the
results. The total computation time is exponential in the size of the cutset because we have
to enumerate all instantiations of the cutset variables.
If C is a loop-cutset, then, when the nodes in C are assigned, the Bayesian network can
be transformed into an equivalent poly-tree and P (xi |c, e) and P (c|e) can be computed via
BP in time and space linear in the size of the network. For example, the subset {A, D} is a
loop-cutset of the belief network shown in Figure 1, left, with evidence E = e. On the right,
6

Cutset Sampling for Bayesian Networks

A
B

C

D

F

E

G

D

A

A

B

C

D

F

E

G

D

Figure 1: When nodes A and D in the loopy Bayesian network (left) are instantiated, the
network can be transformed into an equivalent singly-connected network (right).
In the transformation process, a replica of an observed node is created for each
child node.

Figure 1 shows an equivalent singly-connected network resulting from assigning values to A
and D.
It is well-known that the minimum induced width w∗ of the network is always less than
the size of the smallest loop-cutset (Bertele & Brioschi, 1972; Dechter, 2003). Namely,
w∗ + 1 ≤ |C| for any C. Thus, inference approaches (e.g., bucket elimination) are never
worse and often are better than cutset conditioning time-wise. However, when w∗ is too
large we must resort to cutset conditioning search in order to trade space for time. Those
considerations yield a hybrid search and inference approach. Since observed variables can
break down the dependencies in the network, a network with an observed subset of variables
C often can be transformed into an equivalent network with a smaller induced width, wC ,
which we will term the adjusted induced width. Hence, when any subset of variables C ⊂ X
is observed, complexity is bounded exponentially by the adjusted induced width of the
graph wC .
Definition 2.5 (adjusted induced width) Given a graph G=<X,E>, the adjusted
induced width of G relative to C, denoted wC , is its induced width once C is removed
from its moral graph.
Definition 2.6 (w-cutset) Given a graph G=<X,E>, a subset of nodes C ⊂ X is a
w-cutset of G if its adjusted induced width equals w.
If C is a w-cutset, the quantities P (xi |c, e) and P (c|e) can be computed in time and
space exponential in w, which can be much smaller than the tree-width of the unconditioned
network. The resulting scheme requires memory exponential in w and time O(d|C| ·N ·d(w+1) )
where N is the size of the network and d is the maximum domain size. Thus, the performance
can be tuned to the available system memory resource via the bounding parameter w.
Given a constant w, finding a minimal w-cutset (to minimize the cutset conditioning
time) is also a hard problem. Several greedy heuristic approaches have been proposed by
Geiger and Fishelson (2003) and by Bidyuk and Dechter (2003, 2004). We elaborate more
in Section 3.5.
7

Bidyuk & Dechter

2.3 Gibbs Sampling
Since the complexity of inference algorithms is memory exponential in the network’s induced
width (or tree-width) and since resorting to the cutset-conditioning scheme may take too
much time when the w-cutset size is too large, we must often resort to approximation
methods. Sampling methods for Bayesian networks are commonly used approximation
techniques. This section provides background on Gibbs sampling, a Markov Chain Monte
Carlo method, which is one of the most popular sampling schemes and is the focus of this
paper. Although the method may be applied to the networks with continuous distributions,
we limit our attention in this paper to discrete random variables with finite domains.
2.3.1 Gibbs Sampling for Bayesian Networks
Ordered Gibbs Sampler
Input: A belief network B over X={X1 , ..., Xn } and evidence e={(Xi = ei )|Xi ∈ E ⊆ X}.
Output: A set of samples {x(t) }, t = 1...T .
(0)
1. Initialize: Assign random value xi to each variable Xi ∈ X\E from D(Xi ). Assign
evidence variables their observed values.
2. Generate samples:
For t = 1 to T, generate a new sample x(t) :
(t)
For i = 1 to N, compute a new value xi for variable Xi :
(t)
(t)
(t)
Compute distribution P (Xi |xmarkovi ) and sample xi ← P (Xi |xmarkovi ).
(t)

Set Xi = xi .
End For i
End For t

Figure 2: A Gibbs sampling Algorithm
Given a Bayesian network over the variables X = {X1 , ..., Xn }, and evidence e, Gibbs
sampling (Geman & Geman, 1984; Gilks et al., 1996; MacKay, 1996) generates a set of
(t)
(t)
samples {x(t) } where each sample x(t) = {x1 , ..., xn } is an instantiation of all the variables.
(t)
The superscript t denotes a sample index and xi is the value of Xi in sample t. The first
(t)
sample can be initialized at random. When generating a new sample from sample xi , a
(t)
new value for variable Xi is sampled from probability distribution P (Xi |x−i ) (recall that
(t)

(t+1)

P (Xi |x−i ) = P (Xi |x1

(t+1)

(t+1)

(t)

(t)

(t)

, ..., xi−1 , xi+1 , ..., xn )) which we will denote as xi ← P (Xi |x−i ).
(t)

The next sample xi
is generated from the previous sample xi following one of two
schemes.
Random Scan Gibbs Sampling. Given a sample x(t) at iteration t, pick a variable
(t)
Xi at random and sample a new value xi from the conditional distribution xi ← P (Xi |x−i )
leaving other variables unchanged.
Systematic Scan (Ordered) Gibbs Sampling. Given a sample x(t) , sample a new
value for each variable in some order:
(t)

(t)

x1 ← P (X1 |x2 , x3 , ..., x(t)
n )
8

Cutset Sampling for Bayesian Networks

(t+1)

x2 ← P (X2 |x1

(t)

, x3 , ..., x(t)
n )

...

(t+1)

xi ← P (Xi |x1

(t+1)

(t)

, ..., xi−1 , xi+1 , ..., x(t)
n )

...

(t+1)

xn ← P (Xn |x1

(t+1)

, x2

(t+1)

, ..., xn−1 )
(t)

In Bayesian networks, the conditional distribution P (Xi |x−i ) is dependent only on the
(t)

(t)

assignment to the Markov blanket of variable Xi . Thus, P (Xi |x−i )=P (Xi |xmarkovi ) where
(t)

xmarkovi is the restriction of x(t) to markovi . Given a Markov blanket of Xi , the sampling
probability distribution is given explicitly by Pearl (1988):
Y
(t)
(t)
P (xj |x(t)
(9)
P (xi |xmarkovi ) = αP (xi |x(t)
)
paj )
pai
{j|Xj ∈chj }

Thus, generating a complete new sample can be done in O(n · r) multiplication steps
where r is the maximum family size and n is the number of variables.
The sequence of samples x(1) , x(2) , ... can be viewed as a sequence of states in a Markov
(t+1)
(t+1) (t) (t)
(t)
chain. The transition probability from state {x1
, ..., xi−1 , xi , xi+1 , ..., xn } to state
(t+1)

(t+1)

(t+1)

(t)

(t)

(t)

{x1
, ..., xi−1 , xi
, xi+1 , ..., xn } is defined by the sampling distribution P (Xi |x−i ).
By construction, a Markov chain induced by Gibbs sampling has an invariant distribution
P (X|e). However, since the values assigned by the Gibbs sampler to variables in a sample
x(t+1) depend on the assignment of values in the previous sample x(t) , it follows that the
sample x(n) depends on the initial state x(0) . The convergence of the Markov chain is
defined by the rate at which the distance between the distribution P (x(n) |x(0) , e) and the
stationary distribution P (X|e) (i.e., variational distance, L1 -distance, or χ2 ) converges to 0
as a function of n. Intuitively, it reflects how quickly the inital state x(0) can be “forgotten.”
The convergence is guaranteed as T → ∞ if Markov chain is ergodic (Pearl, 1988; Gelfand
& Smith, 1990; MacKay, 1996). A Markov chain with a finite number of states is ergodic if
it is aperiodic and irreducible (Liu, 2001). A Markov chain is aperiodic if it does not have
regular loops. A Markov chain is irreducible if we can get from any state Si to any state
Sj (including Si ) with non-zero probability in a finite number of steps. The irreducibility
guarantees that we will be able to visit (as number of samples increases) all statistically
important regions of the state space. In Bayesian networks, the conditions are almost always
satisfied as long as all conditional probabilities are positive (Tierney, 1994).
To ensure that the collected samples are drawn from distribution close to P (X|e), a
“burn-in” time may be allocated. Namely, assuming that it takes ≈ K samples for the
Markov chain to get close to the stationary distribution, the first K samples may not be included into the computation of posterior marginals. However, determining K is hard (Jones
& Hobert, 2001). In general, the “burn-in” is optional in the sense that the convergence of
the estimates to the correct posterior marginals does not depend on it. For completeness
sake, the algorithm is given in Figure 2.
P
When convergence conditions are satisfied, an ergodic average fT (X) = T1 t f (xt ) for
any function f (X) is guaranteed to converge to the expected value E[f (X)] as T increases.
9

Bidyuk & Dechter

In other words, |fT (X) − E[f (X)]| → 0 as T → ∞. For a finite-state Markov chain that is
irreducible and aperiodic, the following result applies (see Liu, 2001, Theorem 12.7.2):
√
T |fT (X) − E[f (X)]| → N (0, σ(f )2 )
(10)
for any initial assignment to x(0) . The variance term σ(f )2 is defined as follows:
σ(f )2 = 2τ (f )σ 2
where σ 2 = var[f (X)] and τ (h) is the integrated autocorrelation time.
Our focus is on computing the posterior marginals P (Xi |e) for each Xi ∈X\E. The
posterior marginals can be estimated using either a histogram estimator:
P̂ (Xi = xi |e) =

T
1X
δ(xi |x(t) )
T

P̂ (Xi = xi |e) =

T
1X
(t)
P (xi |x−i )
T

or a mixture estimator:

(11)

t=1

(12)

t=1

The histogram estimator corresponds to counting samples where Xi = xi , namely δ(xi |x(t) ) =
(t)
1 if xi = xi and equals 0 otherwise. Gelfand and Smith (1990) have pointed out that since
mixture estimator is based on estimating conditional expectation, its sampling variance
is smaller due to Rao-Blackwell theorem. Thus, mixture estimator should be preferred.
(t)
(t)
Since P (xi |x−i ) = P (xi |xmarkovi ), the mixture estimator is simply an average of conditional
probabilities:
T
1X
(t)
P̂ (xi |e) =
P (xi |xmarkovi )
(13)
T
t=1

As mentioned above, when the Markov chain is ergodic, P̂ (Xi |e) will converge to the exact
posterior marginal P (Xi |e) as the number of samples increases. It was shown by Roberts and
Sahu (1997) that random scan Gibbs sampler can be expected to converge faster than the
systematic scan Gibbs sampler. Ultimately, the convergence rate of Gibbs sampler depends
on the correlation between two consecutive samples (Liu, 1991; Schervish & Carlin, 1992;
Liu et al., 1994). We review this subject in the next section.
2.4 Variance Reduction Schemes
The convergence rate of the Gibbs sampler depends on the strength of the correlations
between the samples (which are also the states of the Markov chain). The term correlation
is used here to mean that the samples are dependent, as mentioned earlier. In the case of a
finite-state irreducible and aperiodic Markov chain, the convergence rate can be expressed
through maximal correlation between states x(0) and x(n) (see Liu, 2001, ch. 12). In practice,
the convergence rate can be analyzed through covariance cov[f (x(t) ), f (x(t+1) )], where f is
some function, also called auto-covariance.
The convergence of the estimates to the exact values depends on both the convergence
rate of the Markov chain to the stationary distribution and the variance of the estimator.
10

Cutset Sampling for Bayesian Networks

Both of these factors contribute to the value of the term σ(f )2 in Eq.(10). The two main
approaches that allow to reduce correlation between samples and reduce sampling variance
of the estimates are blocking (grouping variables together and sampling simultaneously)
and collapsing (integrating out some of the random variables and sampling a subset), also
known as Rao-Blackwellisation.
Given a joint probability distribution over three random variables X, Y , and Z, we can
depict the essence of those three sampling schemes as follows:
1. Standard Gibbs:
x(t+1) ← P (X|y (t) , z (t) )
y

(t+1)

z

(t+1)

(t+1)

← P (Y |x

(t+1)

← P (Z|x

(14)

(t)

,z )

,y

(15)

(t+1)

)

(16)

2. Collapsed (variable Z is integrated out):
x(t+1) ← P (X|y (t) )
y

(t+1)

(t+1)

← P (Y |x

(17)
)

(18)

3. Blocking by grouping X and Y together:
(x(t+1) , y (t+1) ) ← P (X, Y |z (t) )
z

(t+1)

(t+1)

← P (Z|x

,y

(19)
(t+1)

)

(20)

The blocking reduces the correlation between samples by grouping highly correlated
variables into “blocks.” In collapsing, the highly correlated variables are marginalized out,
which also results in the smoothing of the sampling distributions of the remaining variables
(P (Y |x) is smoother than P (Y |x, z)). Both approaches lead to reduction of the sampling
variance of the estimates, speeding up their convergence to the exact values.
Generally, blocking Gibbs sampling is expected to converge faster than standard Gibbs
sampler (Liu et al., 1994; Roberts & Sahu, 1997). Variations on this scheme have been
investigated by Jensen et al. (1995) and Kjaerulff (1995). Given the same number of samples,
the estimate resulting from collapsed Gibbs sampler is expected to have lower variance
(converge faster) than the estimate obtained from blocking Gibbs sampler (Liu et al., 1994).
Thus, collapsing is preferred to blocking. The analysis of the collapsed Gibbs sampler can
be found in Escobar (1994), MacEachern (1994), and Liu (1994, 1996).
The caveat in the utilization of the collapsed Gibbs sampler is that computation of the
probabilities P (X|y) and P (Y |x) must be efficient time-wise. In case of Bayesian networks,
the task of integrating out some variables is equivalent to posterior belief updating where
evidence variables and sampling variables are observed. Its time complexity is therefore
exponential in the adjusted induced width, namely, in the effective width of the network
after some dependencies are broken by instantiated variables (evidence and sampled).
11

Bidyuk & Dechter

2.5 Importance Sampling
Since sampling from the target distribution is hard, different sampling methods explore
different trade-offs in generating samples and obtaining estimates. As we already discussed,
Gibbs sampling generates dependent samples but guarantees convergence of the sampling
distribution to the target distribution. Alternative approach, called importance sampling,
is to generate samples from a sampling distribution Q(X) that is different from P (X|e) and
include the weight w(t) = P (x(t) |e)/Q(x(t) ) of each sample x(t) in the computation of the
estimates as follows:
T
1X
f (xt )w(t)
fˆT (X) =
T

(21)

t=1

The convergence of fˆT (X) to E[f (X)] is guaranteed as long as the condition P (x|e) 6=
0 ⇒ Q(x) 6= 0 holds. The convergence speed depends on the distance between Q(X) and
P (X|e).
One of the simplest forms of importance sampling for Bayesian networks is likelihood
weighting (Fung & Chang, 1989; Shachter & Peot, 1989) which processes variables in topological order, sampling root variables from their priors and the remaining variables from
conditional distribution P (Xi |pai ) defined by their conditional probability table (the evidence variables are assigned their observed values). Its sampling distribution is close to the
prior and, as a result, it usually converges slowly when the evidence is concentrated around
the leaf nodes (nodes without children) and when the probability of evidence is small. Adaptive (also called dynamic) importance sampling is a method that attempts to speed up the
convergence by updating the sampling distribution based on the weight of previously generated samples. Adaptive importance sampling methods include self-importance sampling,
heuristic importance sampling (Shachter & Peot, 1989), and, more recently, AIS-BN (Cheng
& Druzdzel, 2000) and EPIS-BN (Yuan & Druzdzel, 2003). In the empirical section, we
compare the performance of the proposed cutset sampling algorithm with AIS-BN which
is considered a state-of-the-art importance sampling algorithm to date (although EPIS-BN
was shown to perform better in some networks) and, hence, describe AIS-BN here in more
detail.
AIS-BN algorithm is based on the observation that if we could sample each node in
topological order from distribution P (Xi |pai , e), then the resulting sample would be drawn
from the target distribution P (X|e). Since this distribution is unknown for any variable
that has observed descendants, AIS-BN initializes the sampling distributions P 0 (Xi |pai , e)
equal to either P (Xi |pai ) or a uniform distribution and then updates each distribution
P k (Xi |pai , e) every l samples so that the next sampling distribution P k+1 (Xi |pai , e) will be
closer to P (Xi |pai , e) than P k (Xi |pai , e) as follows:
P k+1 (xi |pai , e) = P k (xi |pai , e) + η(k) · (P ′ (xi |pai , e) − P k (xi |pai , e))
where η(k) is a positive function that determines the learning rate and P ′ (xi |pai , e) is an
estimate of P (xi |pai , e) based on the last l samples.
12

Cutset Sampling for Bayesian Networks

3. Cutset Sampling
This section presents the cutset sampling scheme. As we discussed above, sampling on a
cutset is guaranteed to be more statistically efficient. Cutset sampling scheme is a computationally efficient way of sampling from a “collapsed” variable subset C ⊂ X, tying the
complexity of sample generation to the structure of the Bayesian network.
3.1 Cutset Sampling Algorithm
The cutset sampling scheme partitions the variable set X into two subsets C and X\C.
The objective is to generate samples from space C={C1 , C2 , ..., Cm } where each sample c(t)
is an instantiation of all the variables in C. Following the Gibbs sampling principles, we
(t)
wish to generate a new sample c(t) by sampling a value ci from the probability distribution
(t)
(t+1) (t+1)
(t+1) (t)
(t)
P (Ci |c−i ) = P (Ci |c1
, c2
, ..., ci−1 , ci+1 , ..., cm ). We will use left arrow to denote that
(t)

value ci is drawn from distribution P (Ci |c−i ):

(t)

ci ← P (Ci |c−i , e)

(22)
(t)

If we can compute the probability distribution P (Ci |c−i , e) efficiently for each sampling
variable Ci ∈ C, then we can generate samples efficiently. The relevant conditional distributions can be computed by exact inference whose complexity is tied to the network structure. We denote by JT C(B, Xi , e) a generic algorithm in the class of variable-elimination
or join-tree clustering algorithms which, given a belief network B and evidence e, outputs
the posterior probabilities P (Xi |e) for variable Xi ∈ X (Lauritzen & Spiegelhalter, 1988;
Jensen et al., 1990; Dechter, 1999a). When the network’s identity is clear, we will use the
notation JT C(Xi , e).
Cutset Sampling
Input: A belief network B, a cutset C = {C1 , ..., Cm }, evidence e.
Output: A set of samples ct , t = 1...T .
1. Initialize: Assign random value c0i to each Ci ∈ C and assign e.
2. Generate samples:
For t = 0 to T-1, generate a new sample c(t+1) as follows:
(t)
For i = 1 to m, compute new value ci for variable Ci as follows:
(t)
a. Compute JT C(Ci , c−i , e).
(t)

(t)

b. Compute P (Ci |c−i , e) = αP (Ci , c−i , e).
c. Sample:
(t+1)
(t)
ci
← P (Ci |c−i , e)
End For i
End For t

(23)

Figure 3: w-Cutset sampling Algorithm
Therefore, for each sampling variable Ci and for each value ci ∈ D(Ci ), we can compute
(t)
(t)
(t)
(t)
P (Ci , c−i , e) via JT C(Ci , c−i , e) and obtain P (Ci |c−i , e) via normalization: P (Ci |c−i , e) =
(t)

αP (Ci , c−i , e).

13

Bidyuk & Dechter

Cutset sampling algorithm that uses systematic scan Gibbs sampler is given in Figure 3.
Clearly, it can be adapted to be used with the random scan Gibbs sampler as well. Steps
(a)-(c) generate sample (t + 1) from sample (t). For every variable Ci ∈ C in sequence, the
(t)
main computation is in step (a), where the distribution P (Ci , c−i , e) over Ci is generated.
This requires executing JT C for every value ci ∈ D(Ci ), separately. In step (b), the
conditional distribution is derived by normalization. Finally, step (c) samples a new value
(t)
from the obtained distribution. Note that we only use P (Ci |c−i , e) as a short-hand notation
(t+1)

(t+1)

(t)

(t)

for P (Ci |c1
, ..., ci−1 , ci+1 , ..., ck , e). Namely, when we sample a new value for variable
Ci , the values of variables C1 through Ci−1 have already been updated.
We will next demonstrate the process using the special case of loop-cutset (see Definition 2.1).
Example 3.1 Consider the belief network previously shown in Figure 1 with the observed node
E = e and loop-cutset {A, D}. We begin the sampling process by initializing sampling variables to
a(0) and d(0) . Next, we compute new sample values a(1) , d(1) as follows:
P (A|d(0) , e)

=

αPJT C (A, c(0) , e)
(0)

(24)

(1)

a
P (D|a(1) , e)

←
=

P (A|d , e)
αPJT C (D, a(1) , e)

(25)
(26)

d(1)

←

P (D|a(1) , e)

(27)

The process above corresponds to two iterations of the inner loop in Figure 3. Eq. (24)-(25), where
we sample a new value for variable A, correspond to steps (a)-(c) of the first iteration. In the second
iteration, Eq.(26)-(27), we sample a new value for variable D. Since the conditioned network is a
poly-tree (Figure 1, right), computing probabilities PJT C (A|d(t) , e) and PJT C (D|a(t+1) , e) via JT C
reduces to Pearl’s belief propagation algorithm and the distributions can be computed in linear time.

3.2 Estimating Posterior Marginals
Once a set of samples over a subset of variables C is generated, we can estimate the posterior
marginals of any variable in the network using mixture estimator. For sampling variables,
the estimator takes the form similar to Eq.(12):
T
1X
(t)
P̂ (Ci |e) =
P (Ci |c−i , e)
T

(28)

t=1

For variables in X\C, E, the posterior marginal estimator is:
T
1X
P̂ (Xi |e) =
P (Xi |c(t) , e)
T

(29)

t=1

We can use JT C(Xi , c(t) , e) to obtain the distribution P (Xi |c(t) , e) over the input Bayesian
network conditioned on c(t) and e as shown before.
(t)
If we maintain a running sum of the computed distributions P (Ci |c−i , e) and P (Xi |c(t) , e)
during sample generation, the sums in the right hand side of Eq.(28)-(29) will be readily
available. As we noted before, the estimators P̂ (Ci |e) and P̂ (Xi |e) are guaranteed to converge to their corresponding exact posterior marginals as T increases as long as the Markov
14

Cutset Sampling for Bayesian Networks

chain over the cutset C is ergodic. While for the cutset variables the estimator is a simple
ergodic average, for Xi ∈ X\C, E the convergence can also be derived directly from first
principles:
Theorem 3.2 Given a Bayesian network B over X, evidence variables E ⊂ X, and cutset
C ⊂ X\E, and given a set of T samples c(1) , c(2) , ..., c(T ) obtained via Gibbs sampling from
P (C|e), and assuming the Markov chain corresponding to sampling from C is ergodic, then
for any Xi ∈ X\C, E assuming P̂ (Xi |E) is defined by Eq.(29), P̂ (Xi |e) → P (Xi |e) as
T →∞.
Proof. By definition:
T
1X
P (Xi |c(t) , e)
P̂ (Xi |e) =
T

(30)

t=1

Instead of summing over samples, we can rewrite the expression above to sum over all
possible tuples c ∈ D(C) and group together the samples corresponding to the same tuple
instanceP
c. Let q(c) denote the number of times a tuple C = c occurs in the set of samples
so that c∈D(C) q(c) = T . It is easy to see that:
P̂ (Xi |e) =

The fraction

q(c)
T

X

c∈D(C)

P (Xi |c, e)

q(c)
T

(31)

is a histogram estimator for the posterior marginal P̂ (c|e). Thus, we get:
X
P̂ (Xi |e) =
P (Xi |c, e)P̂ (c|e)
(32)
c∈D(C)

Since the Markov chain formed by samples from C is ergodic, P̂ (c|e) → P (c|e) as T → ∞
and therefore:
X
P̂ (Xi |e) →
P (Xi |c, e)P (c|e) = P (Xi |e)
c∈D(C)



3.3 Complexity
The time and space complexity of generating samples and estimating the posterior marginals
via cutset sampling is dominated by the complexity of JT C in line (a) of the algorithm
(Figure 3). Only linear amount of additional memory is required to maintain the running
(t)
sums of P (Ci |c−i , e) and P (Xi |c(t) , e) used in the posterior marginal estimators.
3.3.1 Sample Generation Complexity
Clearly, when JT C is applied to the network B conditioned on all the cutset variables C
and evidence variables E, its complexity is time and space exponential in the induced width
w of the conditioned network. It is O(N · d(w+1) ) when C is a w-cutset (see Definition 2.6).
15

Bidyuk & Dechter

Using the notion of a w-cutset, we can balance sampling and exact inference. At one end
of the spectrum we have plain Gibbs sampling where sample generation is fast, requiring
linear space, but may have high variance. At the other end, we have exact algorithm
requiring time and space exponential in the induced width of the moral graph. In between
these two extremes, we can control the time and space complexity using w as follows.
Theorem 3.3 (Complexity of sample generation) Given a network B over X, evidence E, and a w-cutset C, the complexity of generating a new sample is time and space
O(|C| · N · d(w+2) ) where d bounds the variable’s domain size and N = |X|.
Proof. If C is a w-cutset and d is the maximum domain size, then the complexity of
(t)
computing joint probability P (ci , c−i , e) over the conditioned network is O(N · d(w+1) ).
Since this operation must be repeated for each ci ∈ D(Ci ), the complexity of processing one
(t)
variable (computing distribution P (Ci |c−i , e)) is O(N · d · d(w+1) ) = O(N · d(w+2) ). Finally,
since ordered Gibbs sampling requires sampling each variable in the cutset, generating one
sample is O(|C| · N · d(w+2) ). 

3.3.2 Complexity of Estimator Computation
The posterior marginals for any cutset variable Ci ∈ C are easily obtained at the end of
sampling process without incurring additional computation overhead. As mentioned earlier,
(t)
we only need to maintain a running sum of probabilities P (ci |c−i , e) for each ci ∈ D(Ci ).
Estimating P (Xi |e), Xi ∈ X\C, E, using Eq.(29) requires computing P (Xi |c(t) , e) once a
sample c(t) is generated. In summary:
Theorem 3.4 (Computing Marginals) Given a w-cutset C, the complexity of computing posteriors for all variables Xi ∈ X\E using T samples over the cutset variables is
O(T · [|C| + d] · N · d(w+1) ).
Proof. As we showed in Theorem 3.3, the complexity of generating one sample is O(|C| ·
N · d(w+2) ). Once a sample c(t) is generated, the computation of the posterior marginals
for the remaining variables requires computing P (Xi |c(t) , e) via JT C(Xi , c(t) , e) which is
O(N · d(w+1) ). The combined computation time for one sample is O(|C| · N · d(w+2) +
N · d(w+1) ) = O([|C| + d] · N · d(w+1) ). Repeating the computation for T samples, yields
O(T · [|C| + d] · N · d(w+1) ). 
Note that the space complexity of w-cutset sampling is bounded by O(N · d(w+1) ).

3.3.3 Complexity of Loop-Cutset
When the cutset C is a loop-cutset, algorithm JT C reduces to belief propagation (Pearl,
(t)
1988) that computes the joint distribution P (Ci , c−i , e) in linear time. We will refer to the
special case as loop-cutset sampling and to the general as w-cutset sampling.
A loop-cutset is also a w-cutset where w equals the maximum number of unobserved
parents (upper bounded by the maximum indegree of a node). However, since processing
poly-trees is linear even for large w, the induced width does not capture its complexity
16

Cutset Sampling for Bayesian Networks

properly. The notion of loop-cutset could be better captured via the hyperwidth of the
network (Gottlob, Leone, & Scarello, 1999; Kask et al., 2005). The hyperwidth of a polytree is 1 and therefore, a loop-cutset can be defined as a 1-hypercutset. Alternatively, we
can express the complexity via the network’s input size M which captures the total size of
conditional probability tables to be processed as follows:
Theorem 3.5 (Complexity of loop-cutset sample generation) If C is a loop-cutset,
the complexity of generating each sample is O(|C| · d · M ) where M is the size of the input
network.
Proof. When a loop-cutset of a network is instantiated, belief propagation (BP) can
(t)
compute the joint probability P (ci , c−i , e) in linear time O(M ) (Pearl, 1988) yielding total
time and space of O(|C| · d · M ) for each sample. 

3.4 Optimizing Cutset Sampling Performance
Our analysis of the complexity of generating samples (Theorem 3.3) is overly pessimistic in
assuming that the computation of the sampling distribution for each variable in the cutset
is independent. While all variables may change a value when moving from one sample to
the next, the change occurs one variable at a time in some sequence so that much of the
computation can be retained when moving from one variable to the next .
We will now show that sampling all the cutset variables can be done more efficiently
reducing the factor of N · |C| in Theorem 3.3 to (N + |C| · δ) where δ bounds the number of
clusters in the tree decomposition used by JT C that contains any node Ci ∈ C. We assume
that we can control the order by which cutset variables are sampled.

X1
X1X2Y1

Y1

Y2

Yn-2

Yn-1

X2

X3

Xn-1

Xn

X2X3Y2

X3X4Y3

Xn-1XnYn-1

Figure 4: A Bayesian network (top) and corresponding cluster-tree (bottom).
Consider a simple network with variables X={X1 , ....Xn }, Y ={Y1 , ..., Yn−1 } and CPTs
P (Xi+1 |Xi , Yi ) and P (Yi+1 |Xi ) defined for every i as shown in Figure 4, top. The join-tree
of this network is a chain of cliques of size 3 given in Figure 4, bottom. Since Y is a loopcutset, we will sample variables in Y . Let’s assume that we use the ordering Y1 , Y2 , ...Yn−1 to
generate a sample. Given the current sample, we are ready to generate the next sample by
applying JT C (or bucket-elimination) to the network whose cutset variables are assigned.
17

Bidyuk & Dechter

This makes the network effectively singly-connected and leaves only 2 actual variables in
each cluster. The algorithm sends a message from the cluster containing Xn towards the
cluster containing X1 . When cluster (X1 , X2 , Y1 ) gets the relevant message from cluster
(X2 , X3 , Y2 ) we can sample Y1 . This can be accomplished by d linear computations in clique
(X1 , X2 , Y1 ) for each yi ∈ D(Yi ) yielding the desired distribution P (Y1 |.) (we can multiply
all functions and incoming messages in this cluster, sum out X1 and X2 and normalize). If
the cutset is a w-cutset, each computation in a single clique is O(d(w+1) ).
Once we have P (Y1 |·), Y1 is sampled and assigned a new value, y1 . Cluster (X1 , X2 , Y1 =
y1 ) then sends a message to cluster (X2 , X3 , Y2 ) which now has all the information necessary
to compute P (Y2 |.) in O(d(w+2) ). Once P (Y2 |.) is available, a new value Y2 = y2 is sampled.
The cluster than computes and sends a message to cluster (X3 , X4 , Y3 ), and so on. At the
end, we obtain a full sample via two message passes over the conditioned network having
computation complexity of O(N · d(w+2) ). This example can be generalized as follows.
Theorem 3.6 Given a Bayesian network having N variables, a w-cutset C, a tree-decomposition
Tr , and given a sample c1 , ..., c|C| , a new sample can be generated in O((N + |C| · δ) · d(w+2) )
where δ is the maximum number of clusters containing any variable Ci ∈ C.
Proof. Given w-cutset C, by definition, there exists a tree-decomposition Tr of the network
(that includes the cutset variables) such that when the cutset variables C are removed, the
number of variables remaining in each cluster of Tr is bounded by w + 1. Let’s impose
directionality on Tr starting at an arbitrary cluster that we call R as shown in Figure 5. Let
TCi denote the connected subtree of Tr whose clusters include Ci . In Figure 5, for clarity,
we collapse the subtree over Ci into a single node. We will assume that cutset nodes are
sampled in depth-first traversal order dictated by the cluster tree rooted in R.

TC1 R
TC2
TCk
TC3

TC4

TC6

TC5
Figure 5: A cluster-tree rooted in cluster R where a subtree over each cutset node Ci is
collapsed into a single node marked TCi .

18

Cutset Sampling for Bayesian Networks

Given a sample c(t) , JT C will send messages from leaves of Tr towards the root cluster.
We can assume without loss of generality that R contains cutset node C1 which is the first to
be sampled in c(t+1) . JTC will now pass messages from root down only to clusters restricted
(t)
to TC1 (note that R ∈ TC1 ). Based on these messages P (C1 = c1 , c−1 ) can be computed
in O(d(w+1) ). We will repeat this computation for each other value of C1 involving only
clusters in TC1 and obtain the distribution P (C1 |·) in O(d(w+2) ) and sample a new value
for C1 . Thus, if C1 appears in δ clusters, the number of message passing computations
(after the initial O(N ) pass) is O(δ) and we can generate the first distribution P (C1 |·) in
O(δ · d(w+2) ).
The next node in the depth-first traversal order is TC2 and thus, the second variable to
be sampled is C2 . The distance between variables C1 and C2 , denoted dist1,2 , is the shortest
path along Tr from a cluster that contains C1 to a cluster that contains C2 . We apply JTC’s
mesage-passing along that path only which will take at most O(dist1,2 · d(w+1) ). Then, to
obtain the conditional distribution P (C2 |·), we will recompute messages in the subtree of
TC2 for each value c2 ∈ D(C2 ) in O(δ · d(w+2) ). We continue the computation in a similar
manner for other cutset nodes.
If JT C traverses the tree in the depth-first order, it only needs to pass messages along
P|C|
each edge twice (see Figure 5). Thus, the sum of all distances traveled is i=2 disti,i−1 =
O(N ). What may be repeated is the computation for each value of the sampled variable.
This, however, can be accomplished via message-passing restricted to individual variables’
subtrees and is bounded by its δ. We can conclude that a new full sample can be generated
in O((N + |C| · δ) · d(w+2) ). 
It is worthwhile noting that the complexity of generating a sample can be further reduced
by a factor of d/(d−1) (which amounts to a factor of 2 when d = 2) by noticing that whenever
(t+1)
(t+1) (t)
(t)
we move from variable Ci to Ci+1 , the joint probability P (c1
, ..., ci
, ci+1 , ..., ck ) is
already available from the previous round and should not be recomputed. We only need
(t+1)
(t+1)
(t)
(t)
to compute P (c1
, ..., ci
, ci+1 , ..., ck ) for ci+1 6= ci+1 . Buffering the last computed
joint probability, we only need to apply JT C algorithm d − 1 times. Therefore, the total
complexity of generating a new sample is O((N + |C| · δ) · (d − 1) · d(w+1) ).
Example 3.7 Figure 6 demonstrates the application of the enhancements discussed. It
depicts the moral graph (a), already triangulated, and the corresponding join-tree (b) for the
Bayesian network in Figure 1. With evidence variable E removed, variables B and D form
a 1-cutset. The join-tree of the network with cutset and evidence variables removed is shown
in Figure 6 (c). Since removing D and E from cluster DF E leaves only one variable, F ,
we combine clusters BDF and DF E into one cluster, F G. Assuming that cutset variables
have domains of size 2, we can initialize B = b0 and D = d0 .
Selecting cluster AC as the root of the tree, JT C first propagates messages from leaves
to the root as shown in Figure 6 (c) and then computes P (b0 , d0 , e) in cluster AC. Next, we
set B = b1 ; updating all functions containing variable B, and propagating messages through
the subtree of B consisting of clusters AC and CF (Figure 6 (d)), we obtain P (b1 , d0 , e).
Normalizing the two joint probabilities, we obtain distribution P (B|d0 , e) and sample a new
value of B. Assume we sampled value b1 .
19

Bidyuk & Dechter

ABC
P(B|A),P(C|A),
P(A)

A

AC
P(b0|A),P(C|A),
P(A)

AC
P(b1|A),P(C|A),
P(A)

AC
P(b1|A),P(C|A),
P(A)

AC
P(b1|A),P(C|A),
P(A)

CF
P(F|b0,C),P(d0|b0)

CF
P(F|b1,C),P(d0|b1)

CF
P(F|b1,C),P(d1|b1)

CF
P(F|b1,C),P(d0|b1)

FG
P(e|d0,F),P(G|d0,F)

FG
P(e|d0,F),P(G|d0,F)

FG
P(e|d1,F),P(G|d1,F)

FG
P(e|d0,F),P(G|d0,F)

B
C

BCF
P(F|B,C)

F
G

DFG
P(D|B), P(G|D,F)

D

E

DFE
P(E|D,F)

B=b0, D=d0, E=e
(a)

(b)

(c)

B=b1

D=d1

D=d0

(d)

(e)

(f)

Figure 6: A join-tree of width 2 (b) for a moral graph (a) is transformed into a join-tree of
width 1 (c) when evidence variable E and cutset variables B and D are instantiated (in the process, clusters BDF and BCF are merged into cluster CF ). The
clusters contain variables and functions from the original network. The cutset
nodes have domains of size 2, D(B) = {b0 , b1 }, D(D) = {d0 , d1 }. Starting with a
sample {b0 , d0 }, messages are propagated in (c)-(e) to first, sample a new value
of variable B (d) and then variable D (e). Then messages are propagated up the
tree to compute posterior marginals P (·|b1 , d0 , e) for the rest of the variables (f).

Next, we need to compute P (D|b1 , e) to sample a new value for variable D. The joint
probability P (d0 , b1 , e) is readily available since it was computed for sampling a new value of
B. Thus, we set D = d1 and compute the second probability P (d1 , b1 , e) updating functions
in clusters CF and F G and sending an updated message from CF to F G (Figure 6 (e)).
We obtain distribution P (D|b1 , e) by normalizing the joint probabilities and sample a new
value d0 for D. Since the value has changed from latest computation, we update again
functions in the clusters CF and F G and propagate updated messages in the subtree CD
(send message from CF to F G).
In order to obtain the distributions P (·|b1 , d0 , e) for the remaining variables A, C, F ,
and G, we only need to send updated messages up the join-tree, from F G to CF and then
from CF to AC as shown in Figure 6 (f ). The last step also serves as the initialization
step for the next sample generation.
In this example the performance of cutset sampling is significantly better than its worst
case. We have sent a total of 5 messages to generate a new sample while the worst case
suggests at least N · |C| · d = 3 · 2 · 2 = 12 messages (here, N equals the number of clusters).
20

Cutset Sampling for Bayesian Networks

3.5 On finding A w-Cutset
Clearly, w-cutset sampling will be effective only when the w-cutset is small. This calls for
the task of finding a minimum size w-cutset. The problem is NP-hard; yet, several heuristic
algorithms have been proposed. We next briefly survey some of those proposals.
Larossa and Dechter (2003) obtain w-cutset when processing variables in the elimination
order. The next node to be eliminated (selected using some triangulation heuristics) is added
to the cutset if its current induced width (or degree) is greater than w. Geiger and Fishelson
(2003) agument this idea with various heuristics.
Bidyuk and Dechter (2003) select the variables to be included in the cutset using greedy
heuristics based on the node’s basic graph properties (such as the degree of a node). One
scheme starts from an empty w-cutset and then heuristically adds nodes to the cutset until
a tree-decomposition of width ≤ w can be obtained. The other scheme starts from a set
C = X\E containing all nodes in the network as a cutset and then removes nodes from the
set in some order. The algorithm stops when removing the next node would result in a tree
decomposition of width > w.
Alternatively, Bidyuk and Dechter (2004) proposed to first obtain a tree-decomposition
of the network and then find the minimal w-cutset of the tree-decomposition (also an NPhard problem) via a well-known greedy algorithm used for set cover problem. This approach
is shown to yield a smaller cutset than previously proposed heuristics and is used for finding
w-cutset in our experiments (section 4.4) with a modification that a tree-decomposition is
re-computed each time a node is removed from the tree and added to the w-cutset.

4. Experiments
In this section, we present empirical studies of cutset sampling algorithms for several classes
of problems. We use the mean square error of the posterior marginals’ estimates as a
measure of accuracy. We compare with traditional Gibbs sampling, likelihood weighting
(Fung & Chang, 1989; Shachter & Peot, 1989), and the state of the art AIS-BN adaptive
importance sampling algorithm (Cheng & Druzdzel, 2000). We implemented AIS-BN using
the parameters specified by Cheng and Druzdzel (2000). By using our own implementation,
we made sure that all sampling algorithms used the same data access routines and the
same error measures providing a uniform framework for comparing their performance. For
reference we also report the performance of Iterative Belief Propagation (IBP) algorithm.
4.1 Methodology
In this section we detail describe methodology used and the implementation decisions made
that apply to the collection of the empirical results.
4.1.1 Sampling Methodology
In all sampling algorithms we restarted the Markov chain every T samples. The samples
from each chain (batch) m are averaged separately:
P̂m (xi |e) =

T
1X
P (xi |c(t) , e)
T
t=1

21

Bidyuk & Dechter

The final estimate is obtained as a sample average over M chains:
M
1 X
P̂m (xi |e)
P̂ (xi |e) =
M
m=1

Restarting a Markov chain is known to improve the sampling convergence rate. A single
chain can become “stuck” generating samples from a single high-probability region without
ever exploring large number of other high-probability tuples. By restarting a Markov chain
at a different random point, a sampling algorithm can achieve a better coverage of the
sampling space. In our experiments, we did not observe any significant difference in the
estimates obtained from a single chain of size M · T or M chains of size T and therefore,
we only choose to report the results for multiple Markov chains. However, we rely on the
independence of random values P̂m (xi |e) to estimate 90% confidence interval for P̂ (xi |e).
In our implementation of Gibbs sampling schemes, we use zero “burn-in” time (see
section 2.3.1). As we mentioned earlier, the idea of burn-in time is to throw away the
first K samples to ensure that the remaining samples are drawn from distribution close
to target distribution P (X|e). While conservative methods for estimating K through drift
and minorization conditions were proposed by Rosenthal (1995) and Roberts and Tweedie
(1999, 2001), the required analysis is beyond the scope of this paper. We consider our
comparison between Gibbs sampling and cutset sampling, which is the objective, fair in the
sense that both schemes use K=0. Further, our experimental results showed no positive
indication that burn-in time would be beneficial. In practice, burn-in is the “pre-processing”
time used by the algorithm to find the high-probability regions in the distribution P (C|e)
in case it initially spends disproportionally large period of time in low probability regions.
Discarding a large number of low-probability tuples obtained initially, the frequency of the
remaining high-probability tuples is automatically adjusted to better reflect their weight.
cpcs360b, N=360, |E|=32, w*=21

cpcs360b, N=360, |E|=32, |LC|=26, w*=21

LCS
800

1.40E-04

700

# unique samples

1.60E-04

1.20E-04

MSE

1.00E-04
8.00E-05
6.00E-05
4.00E-05

LCS

600
500
400
300
200
100

2.00E-05

0

0.00E+00

0

2000

4000

6000

8000

10000

0

2000

4000

6000

8000

10000

# samples

# samples

Figure 7: Comparing loop-cutset sampling MSE vs. number of samples (left) and and number of unique samples vs. number of samples (right) in cpcs360b. Results are
averaged over 10 instances with different observations.

In our benchmarks, we observed that both full Gibbs sampling and cutset sampling
were able to find high probability tuples fast relative to the number of samples generated.
For example, in one of the benchmarks, cpcs360b, the rate of generating unique samples,
22

Cutset Sampling for Bayesian Networks

namely, the ratio of cutset instances that have not been seen to the number of samples,
decreases over time. Specifically, loop-cutset sampling generates 200 unique tuples after the
first 1000 samples, an additional 100 unique tuples while generating the next 1000 samples,
and then the rate of generating unique tuples slows to 50 per 1000 samples in the range
from 2000 to 10000 samples as shown in Figure 7, right. That means that after the first
few hundred samples, the algorithm spends most of the time revisiting high-probability
tuples. In other benchmarks, the number of unique tuple instances generated increases
linearly (as in cpcs54) and, thus, the tuples appear to be distributed nearly uniformly. In
this case, there is no need for burn-in because there are no strongly-expressed heavy-weight
tuples. Instead of using burn-in times, we sample initial variable values from the posterior
marginal estimates generated by IBP in all of our experiments. Our sampling time includes
the pre-processing time of IBP.
All experiments were performed on 1.8 GHz CPU.
4.1.2 Measures of Performance
For each problem instance defined by a Bayesian network B having variables X = {X1 , ..., Xn }
and evidence E ⊂ X, we derived the exact posterior marginals P (Xi |e) using bucket-tree
elimination (Dechter, 2003, 1999a) and computed the mean square error (MSE) of the
approximate posterior marginals P̂ (Xi |e) for each algorithm where MSE is defined by:
X X
1
[P (xi |e) − P̂ (xi |e)]2
M SE = P
|D(X
)|
i
Xi ∈X\E
Xi ∈X\E D(Xi )

While the mean square error is our primary accuracy measure, the results are consistent
across other well-known measures such as average absolute error, KL-distance, and squared
Hellinger’s distance which we show only for loop-cutset sampling. The absolute error ∆ is
averaged over all values of all unobserved variables:
X X
1
∆= P
|P (xi |e) − P̂ (xi |e)|
Xi ∈X\E |D(Xi )|
Xi ∈X\E D(Xi )

KL-distance DK between the distribution P (Xi |e) and the estimator P̂ (Xi |e) is defined as
follows:
X
P (xi |e)
DK (P (Xi |e), P̂ (Xi |e)) =
P (xi |e) lg
P̂ (xi |e)
D(X )
i

For each benchmark instance, we compute the KL-distance for each variable Xi ∈ X\E and
then average the results:
X
1
DK (P (Xi |e), P̂ (Xi |e))
DK (P, P̂ ) =
|X\E|
Xi ∈X\E

The squared Hellinger’s distance DH between the distribution P (Xi |e) and the estimator
P̂ (Xi |e) is obtained as:
q
X p
DH (P (Xi |e), P̂ (Xi |e)) =
[ P (xi |e) − P̂ (xi |e)]2
D(Xi )

23

Bidyuk & Dechter

The average squared Hellinger’s distance for a benchmark instance is the average of the
distances between posterior distributions of one variable:
DH (P, P̂ ) =

1
|X\E|

X

Xi ∈X\E

DH (P (Xi |e), P̂ (Xi |e))

The average errors for different network instances are then averaged over all instances of
the given network (typically, 20 instances).
We also report the confidence interval for the estimate P̂ (xi |e) using an approach similar
to the well-known batch means method (Billingsley, 1968; Geyer, 1992; Steiger & Wilson,
2001). Since chains are restarted independently, the estimates P̂m (xi |e) are independent.
Thus, the confidence interval can be obtained by measuring the variance in the estimators
P̂ (Xi |e). We report results in Section 4.5.
4.2 Benchmarks
We experimented with four classes of networks:
CPCS. We considered four CPCS networks derived from the Computer-based Patient
Case Simulation system (Parker & Miller, 1987; Pradhan, Provan, Middleton, & Henrion,
1994). CPCS network representation is based on INTERNIST 1 (Miller, Pople, & Myers,
1982) and Quick Medical Reference (QMR) (Miller, Masarie, & Myers, 1986) expert systems. The nodes in CPCS networks correspond to diseases and findings and conditional
probabilities describe their correlations. The cpcs54 network consists of N =54 nodes and
has a relatively large loop-cutset of size |LC|=16 (> 25% of the nodes). Its induced width
is 15. cpcs179 network consists of N =179 nodes. Its induced width is w∗ =8. It has a
small loop-cutset of size |LC|=8 but with a relatively large corresponding adjusted induced
width wLC =7. The cpcs360b is a larger CPCS network with 360 nodes, adjusted induced
width of 21, and loop-cutset |LC|=26. Exact inference on cpcs360b averaged ∼ 30 minutes.
The largest network, cpcs422b, consisted of 422 nodes with induced width w∗ =22 and
loop-cutset of size 47. The exact inference time for cpcs422b is about 50 minutes.
Hailfinder network. It is a small network with only 56 nodes. The exact inference in
Hailfinder network is easy since its loop-cutset size is only 5. Yet, this network has some
zero probabilities and, therefore, is a good benchmark for demonstrating the convergence
of cutset sampling in contrast to Gibbs sampling.
Random networks. We experimented with several classes of random networks: random networks, 2-layer networks, and grid networks. The random networks were generated
with N =200 binary nodes (domains of size 2). The first 100 nodes, {X1 , ..., X100 }, were
designated as root nodes. Each non-root node Xi , i > 100, was assigned 3 parents selected
randomly from the list of predecessors {X1 , ..., Xi−1 }. We will refer to this class of random
networks as multi-partite random networks to distinguish from bi-partite (2-layer) random
networks. The random 2-layer networks were generated with 50 root nodes (first layer)
and 150 leaf nodes (second layer), yielding a total of 200 nodes. A sample 2-layer random
network is shown in Figure 8, left. Each non-root node (second layer) was assigned 1-3
parents selected at random among the root nodes. All nodes were assigned a domain of size
2, D(Xi ) = {x0i , x1i }.
24

Cutset Sampling for Bayesian Networks

Figure 8: Sample random networks: 2-layer (left), grid (center), coding (right).

For both 2-layer and multi-partite random networks, the root nodes were assigned uniform priors while conditional probabilities were chosen randomly. Namely, each value
P (x0i |pai ) was drawn from uniform distribution over interval (0, 1) and used to compute
the complementary probability value P (x1i |pai ) = 1 − P (x0i |pai ).
The directed grid networks (as opposed to grid-shaped undirected Markov Random
Fields) of size 15x30 with 450 nodes were also constructed with uniform priors (on the single
root node) and random conditional probability tables (as described above). A sample grid
network is shown in Figure 8, center. Those networks had an average induced width of
size 20 (exact inference using bucket elimination required about 30 minutes). They had the
most regular structure of all and the largest loop-cutset containing nearly a half of all the
unobserved nodes.
Coding networks. We experimented with coding networks with 50 code bits and 50
parity check bits. The parity check matrix was randomized; each parity check bit had three
parents. A sample coding network with 4 code bits, 4 parity checking bits, and total of 8
transmitted bits is shown in Figure 8, center. The total number of variables in each network
in our experiments was 200 (50 code bits, 50 parity check bits, and 1 transmitted bit for
each code or parity check bit). An average loop-cutset size was 26 and induced width was
21. The Markov chain produced by Gibbs sampling over the whole coding network is not
ergodic due to the deterministic parity check function. As a result, Gibbs sampling does
not converge. However, the Markov chain corresponding to sampling the subspace of coding
bits only is ergodic and, thus, all of the cutset sampling schemes have converged as we will
show in the next two sections.
In all networks, except coding and grid networks, evidence nodes were selected at random
among the leaf nodes (nodes without children). Since a grid network has only one leaf
node, the evidence in the grid networks was selected at random among all nodes. For each
benchmark, we report on the chart title the number of nodes in the network N , average
number of evidence nodes |E|, size of loop-cutset |LC|, and average induced width of the
input instance denoted w∗ to distinguish from the induced width w of the network adjusted
for its w-cutset.
25

Bidyuk & Dechter

4.3 Results for Loop-Cutset Sampling
In this section we compare loop-cutset sampling with pure Gibbs sampling, likelihood
weighting, AIS-BN, and IBP. In all benchmarks, the cutset was selected so that the evidence
and sampling nodes together constitute a loop-cutset of the network using the algorithm
proposed by Becker et al. (2000). We show the accuracy of Gibbs and loop-cutset sampling
as a function of the number of samples and time.
CPCS networks. The results are summarized in Figures 9-12. The loop-cutset curve
in each chart is denoted LCS (for Loop Cutset Sampling). The induced width of the network
wLC when loop-cutset nodes are observed is specified in the caption. It is identical to the
largest family size in the poly-tree generated when cutset variables are removed. We plot
the time on the x-axis and the accuracy (MSE) on the y-axis. In the CPCS networks, IBP
always converged and converged fast (within seconds). Consequently, IBP curve is always
a straight horizontal line as the results do not change after the convergence is achieved.
The curves corresponding to Gibbs sampling, loop-cutset sampling, likelihood weighting,
and AIS-BN demonstrate the convergence of the sampling schemes with time. In the three
CPCS networks loop-cutset sampling converges much faster than Gibbs sampling. The only
exception is cpcs422b (Figure 12, right) where the induced width of the conditioned singlyconnected network remains high (wLC = 14) due to large family sizes and thus, loop-cutset
sampling generates samples very slowly (4 samples/second) compared to Gibbs sampling
(300 samples/second). Since computing sampling distribution is exponential in w, sampling
a single variable is O(214 ) (all variables have domains of size 2). As a result, although loopcutset sampling shows a significant reduction in MSE as a function of the number of samples
(Figure 12, left), it is not enough to compensate for the two orders of magnitude difference
in the loop-cutset rate of sample generation. For cpcs54 (Figure 9), cpcs179 (Figure 10),
and cpcs360b (Figure 11) loop-cutset sampling achieves greater accuracy than IBP within
10 seconds or less.
In comparison with importance sampling schemes, we observe that the AIS-BN algorithm consistently outperforms likelihood weighting and AIS-BN is slightly better than loopcutset sampling in cpcs54, where the probability of evidence P (e)=0.0928 is relatively high.
In cpcs179, where probability of evidence P (e)=4E-05 is smaller, LCS outperforms AIS-BN
while Gibbs sampling curves falls in between AIS-BN and likelihood weighting. Both Gibbs
sampling and loop-cutset sampling outperform AIS-BN in cpcs360b and cpcs422b where
probability of evidence is small. In cpcs360b average P (e)=5E-8 and in cpcs422b the probability of evidence varies from 4E-17 to 8E-47. Note that likelihood weighting and AIS-BN
performed considerably worse than either Gibbs sampling or loop-cutset sampling in all of
those benchmarks as a function of the number of samples. Consequently, we left them off
the charts showing the convergence of Gibbs and loop-cutset sampling as a function of the
number of samples in order to zoom in on the two algorithms which are the focus of the
empirical studies.
Coding Networks. The results for coding networks are shown in Figure 13. We
computed error measures over all coding bits and averaged over 100 instances (10 instances,
with different observed values, of each of the 10 networks with different coding matrices). As
we noted earlier, the Markov chains corresponding to Gibbs sampling over coding networks
are not ergodic and, as a result, Gibbs sampling does not converge. However, the Markov
26

Cutset Sampling for Bayesian Networks

Gibbs

cpcs54, N=54, |LC|=16, w*=15, |E|=8

LW

cpcs54, N=54, |LC|=16, w*=15, |E|=8

LCS

4.0E-04

AIS-BN

3.0E-04

Gibbs

IBP

3.5E-04

2.5E-04

LCS

2.0E-04

IBP

3.0E-04

MSE

MSE

2.5E-04
2.0E-04
1.5E-04

1.5E-04
1.0E-04

1.0E-04

5.0E-05

5.0E-05

0.0E+00

0.0E+00
0

5000

10000

15000

20000

25000

0

30000

2

4

6

# samples
LW

cpcs54, N=54, |LC|=16, w*=15, |E|=8

10

12

14

Hellinger-distance

LCS

1.0E-05

AIS-BN

7.0E-06

Gibbs

1.2E-05

LW

cpcs54, N=54, |LC|=16, w*=15, |E|=8

AIS-BN

1.4E-05

KL-distance

8

Time (sec)

IBP

8.0E-06
6.0E-06
4.0E-06
2.0E-06

Gibbs

6.0E-06

LCS

5.0E-06

IBP

4.0E-06
3.0E-06
2.0E-06
1.0E-06
0.0E+00

0.0E+00
0

2

4

6

8

10

12

0

14

2

4

6

10

12

14

Time (sec)

Time (sec)
LW

cpcs54, N=54, |LC|=16, w*=15, |E|=8

AIS-BN

2.1E-03

Absolute Error

8

Gibbs

1.8E-03

LCS

1.5E-03

IBP

1.2E-03
9.0E-04
6.0E-04
3.0E-04
0.0E+00
0

2

4

6

8

10

12

14

Time (sec)

Figure 9: Comparing loop-cutset sampling (LCS), wLC =5, Gibbs sampling (hereby referred
to as Gibbs), likelihood weighting (LW), AIS-BN, and IBP on cpcs54 network,
averaged over 20 instances, showing MSE as a function of the number of samples
(top left) and time (top right) and KL-distance (middle left), squared Hellinger’s
distance (middle right), and an average error (bottom) as a function of time.

27

Bidyuk & Dechter

cpcs179, N=179, |LC|=8, w*=8, |E|=17

LW

cpcs179, N=179, |LC|=8, w*=8, |E|=17

LW

1.0E-02

AIS-BN

1.0E-01

AIS-BN

Gibbs

Gibbs
LCS

IBP

1.0E-02

IBP

1.0E-03

MSE

Absolute Error

LCS

1.0E-04

1.0E-03

1.0E-05

1.0E-04
0

2

4

6

8

10

12

0

14

2

4

6

8

10

12

cpcs179, N=179, |LC|=8, w*=8, |E|=17

LW

cpcs179, N=179, |LC|=8, w*=8, |E|=17

LW

1.0E+00

AIS-BN

AIS-BN

1.0E-01

Gibbs

1.0E-01

Hellinger-distance

Gibbs

KL-distance

14

Time (sec)

Time (sec)

LCS
IBP

1.0E-02
1.0E-03
1.0E-04
1.0E-05

LCS

1.0E-02

IBP

1.0E-03

1.0E-04

1.0E-05
0

2

4

6

8

10

12

14

0

2

4

6

Time (sec)

8

10

12

14

Time (sec)
LW

cpcs179, N=179, |LC|=8, w*=8, |E|=17

AIS-BN

1.0E-01

Gibbs
Absolute Error

LCS
IBP

1.0E-02

1.0E-03

1.0E-04
0

2

4

6

8

10

12

14

Time (sec)

Figure 10: Comparing loop-cutset sampling (LCS), wLC =7, Gibbs sampling, likelihood
weighting (LW), AIS-BN, and IBP on cpcs179 network, averaged over 20 instances, showing MSE as a function of the number of samples (top left) and time
(top right) and KL-distance (middle left), squared Hellinger’s distance (middle
right), and an average error (bottom) as a function of time.

28

Cutset Sampling for Bayesian Networks

cpcs360b, N=360, |LC|=26, w*=21, |E|=15

cpcs360b, N=360, |LC|=26, w*=21, |E|=15
1.E-02

Gibbs

2.5E-04

LW
AIS-BN

LCS

2.0E-04

Gibbs

IBP

1.5E-04

LCS

MSE

MSE

1.E-03

1.0E-04

IBP

1.E-04

5.0E-05

1.E-05

0.0E+00
0

5000

10000

15000

20000

0

25000

2

4

6

# samples

cpcs360b, N=360, |LC|=26, w*=21, |E|=15

10

12

14

cpcs360b, N=360, |LC|=26, w*=21, |E|=15
LW

1.E-02

AIS-BN

Hellinger-distance

Gibbs

1.E-03

LCS
IBP

1.E-04

LW

1.E-02

AIS-BN

KL-distance

8

Time (sec)

1.E-05
1.E-06

Gibbs

1.E-03

LCS

1.E-04

IBP

1.E-05
1.E-06
1.E-07

0

2

4

6

8

10

12

14

0

2

4

6

Time (sec)

8

10

12

14

Time (sec)

cpcs360b, N=360, |LC|=26, w*=21, |E|=15
1.E-01

LW

Absolute Error

AIS-BN
Gibbs

1.E-02

LCS
IBP

1.E-03

1.E-04
0

5

10

15

20

25

30

Time (sec)

Figure 11: Comparing loop-cutset sampling (LCS), wLC =3, Gibbs sampling, likelihood
weighting (LW), AIS-BN, and IBP on cpcs360b network, averaged over 20 instances, showing MSE as a function of the number of samples (top left) and time
(top right) and KL-distance (middle left), squared Hellinger’s distance (middle
right), and an average error (bottom) as a function of time.

29

Bidyuk & Dechter

cpcs422b, N=422, |LC|=47, w*=22, |E|=28

Gibbs

cpcs422b, N=422, |LC|=47, w*=22, |E|=28

LCS

4.0E-04

LW
AIS-BN

1.0E-02

Gibbs

IBP

3.5E-04

LCS

3.0E-04

MSE

2.5E-04

MSE

IBP

1.0E-03

2.0E-04
1.5E-04

1.0E-04

1.0E-04
5.0E-05
1.0E-05

0.0E+00
0

1000

2000

3000

4000

5000

0

6000

10

20

LW

cpcs422b, N=422, |LC|=47, w*=22, |E|=28

40

50

60

LW

cpcs422b, N=422, |LC|=47, w*=22, |E|=28

AIS-BN

1.0E+00

AIS-BN

Gibbs

1.0E-01

LCS

1.0E-02

IBP

Gibbs

1.0E-01

Hellinger-distance

KL-distance

30

Time (sec)

# samples

1.0E-03
1.0E-04
1.0E-05
1.0E-06

LCS
1.0E-02

IBP

1.0E-03
1.0E-04
1.0E-05
1.0E-06

0

10

20

30

40

50

60

0

10

20

Time (sec)

30

40

50

60

Time (sec)
cpcs422b, N=422, |LC|=47, w*=22, |E|=28

LW
AIS-BN

1.0E-01

Gibbs

Absolute Error

LCS
IBP

1.0E-02

1.0E-03

1.0E-04
0

10

20

30

40

50

60

Time (sec)

Figure 12: Comparing loop-cutset sampling (LCS), wLC =14, Gibbs sampling, likelihood
weighting (LW), AIS-BN sampling, and IBP on cpcs422b network, averaged
over 10 instances, showing MSE as a function of the number of samples (top
left) and time (top right) and KL-distance (middle left), squared Hellinger’s
distance (middle right), and an average error (bottom) as a function of time.

30

Cutset Sampling for Bayesian Networks

1.0E-01

1.0E-02

1.0E-02

1.0E-03
1.0E-04

1.0E-03

1.0E-05

1.0E-04
0

2

4

6

8

0

10

2

4

Time (sec)

6

8

10

Time (sec)
LW
AIS-BN
Gibbs
LCS
IBP

1.0E+00

coding, N=200, P=3, |LC|=26, w*=21

LW
AIS-BN
Gibbs
LCS
IBP

1.0E+00

Hellinger-distance

coding, N=200, P=3, |LC|=26, w*=21
1.0E+01

KL-distance

LW
AIS-BN
Gibbs
LCS
IBP

1.0E-01

MSE

Absolute Error

coding, N=200, P=3, |LC|=26, w*=21

LW
AIS-BN
Gibbs
LCS
IBP

coding, N=200, P=3, |LC|=26, w*=21
1.0E+00

1.0E-01
1.0E-02
1.0E-03
1.0E-04
1.0E-05

1.0E-01
1.0E-02
1.0E-03
1.0E-04
1.0E-05

0

2

4

6

8

0

10

Time (sec)

2

4

6

8

10

Time (sec)

Figure 13: Comparing loop-cutset sampling (LCS), wLC =3, Gibbs sampling, likelihood
weighting (LW), AIS-BN, and IBP on coding networks, σ=0.4, averaged over
10 instances of 10 coding networks (100 instances total). The graphs show average absolute error ( top left), MSE (top right), KL-distance (bottom left), and
squared Hellinger’s distance (bottom right) as a function of time.

chain corresponding to sampling the subspace of code bits only is ergodic and therefore,
loop-cutset sampling, which samples a subset of coding bits, converges and even achieves
higher accuracy than IBP with time. In reality, IBP is certainly preferable for coding
networks since the size of the loop-cutset grows linearly with the number of code bits.
Random networks. In random multi-part networks (Figure 14, top) and random
2-layer networks (Figure 14, middle), loop-cutset sampling always converged faster than
Gibbs sampling. The results are averaged over 10 instances of each network type. In
both cases, loop-cutset sampling achieved accuracy of IBP in 2 seconds or less. In 2-layer
networks, Iterative Belief Propagation performed particularly poorly. Both Gibbs sampling
and loop-cutset sampling obtained more accurate results in less than a second.
Hailfinder network. We used this network (in addition to coding networks) to compare the behavior of cutset sampling and Gibbs sampling in deterministic networks. Since
Hailfinder network contains many deterministic probabilities, the Markov chain corresponding to Gibbs sampling over all variables is non-ergodic. As expected, Gibbs sampling fails
while loop-cutset sampling computes more accurate marginals (Figure 15).
31

Bidyuk & Dechter

random, N=200, |E|=20, |C|=30, w*=22

2-layer, R=50, P=3, N=200, |E|=16, |LC|=17, w*=16

Gibbs

1.8E-04

LCS

1.5E-04

IBP

1.0E-01

Gibbs
LCS

1.0E-02

IBP

MSE

MSE

1.2E-04
9.0E-05
6.0E-05

1.0E-03
1.0E-04

3.0E-05
1.0E-05

0.0E+00
0

5

10

15

20

25

0

30

2

4

Time (sec)

6

8

10

12

Time (sec)

Figure 14: Comparing loop-cutset sampling (LCS), Gibbs sampling, and IBP on random
networks (left) and 2-layer random networks (right), wLC =3 in both classes of
networks, averaged over 10 instances each. MSE as a function of time.

Hailfinder, N=56, |LC|=5, w*=5, |E|=4

Gibbs

1.0E-01

LCS
IBP

MSE

1.0E-02

1.0E-03

1.0E-04
0

1

2

3

4

5

6

7

Time (sec)

Figure 15: Comparing loop-cutset sampling (LCS), wLC =7, Gibbs sampling, and IBP on
Hailfinder network, 10 instances. MSE as a function of time.

In summary, the empirical results demonstrate that loop-cutset sampling is cost-effective
M
time-wise and superior to Gibbs sampling. We measured the ratio R = Mgc of the number
of samples Mg generated by Gibbs to the number of samples Mc generated by loop-cutset
sampling in the same time period (it is relatively constant for any given network and only
changes slightly between problem instances that differ with observations). For cpcs54,
cpcs179, cpcs360b, and cpcs422b the ratios were correspondingly 2.5, 3.75, 0.7, and 75
(see Table 2 in section 4.4). We also obtained R=2.0 for random networks and R=0.3 for
random 2-layer networks. The ratio values > 1 indicate that the Gibbs sampler generates
32

Cutset Sampling for Bayesian Networks

samples faster than loop-cutset sampling which is usually the case. In those instances,
variance reduction compensated for the increased computation time because fewer samples
are needed to converge resulting in the overall better performance of loop-cutset sampling
compared to Gibbs sampling. In some cases, however, the reduction in the sample size
also compensates for the overhead computation in the sampling of one variable value. In
such cases, loop-cutset sampling generated samples faster than Gibbs yielding ratio R < 1.
Then, the improvement in the accuracy is due both to larger number of samples and to
faster convergence.
4.4 w-Cutset Sampling
In this section, we compare the general w-cutset scheme for different values of w against
Gibbs sampling. The main goal is to study how the performance of w-cutset sampling
varies with w. For completeness sake, we include results of loop-cutset sampling shown in
section 4.3.
In this empirical study, we used the greedy algorithm for set cover problem, mentioned
in section 3.5, for finding minimal w-cutset. We apply the algorithm in such a manner that
each (w + 1)-cutset is a proper subset of a w-cutset and, thus, can be expected to have a
lower variance and converge faster than sampling on w-cutset in terms of number of samples
required (following the Rao-Blackwellisation theory). We focus the empirical study on the
trade-offs between cutset size reduction and the associated increase in sample generation
time as we gradually increase the bound w.
We used the same benchmarks as before and included also grid networks. All sampling
algorithms were given a fixed time bound. When sampling small networks, such as cpcs54
(w∗ =15) and cpcs179 (w∗ =8), where exact inference is easy, sampling algorithms were
allocated 10 seconds and 20 seconds respectively. For larger networks we allocated 100-200
seconds depending on the complexity of the network which was only a fraction of exact
computation time.
Table 1 reports the size of the sampling set used by each algorithm where each column
reports the size of the corresponding w-cutset. For example, for cpcs360b, the average
size of Gibbs sample (all nodes except evidence) is 345, the loop-cutset size is 26, the size
of 2-cutset is 22, and so on. Table 2 shows the rate of sample generation by different
algorithms per second. As we observed previously in the case of loop-cutset sampling,
in some special cases cutset sampling generated samples faster than Gibbs sampler. For
example, for cpcs360b, loop-cutset sampling and 2-cutset sampling generated 600 samples
per second while the Gibbs sampler was able to generate only 400 samples. We attribute
this to the size of cutset sample (26 nodes or less as reported in Table 1) compared to the
size of the Gibbs sample (over 300 nodes).
CPCS networks. We present two charts. One chart demonstrates the convergence
over time for several values of w. The second chart depicts the change in the quality
of approximation (MSE) as a function of w for two time points, at the half of the total
sampling time and at the end of total sampling time. The performance of Gibbs sampling
and cutset sampling for cpcs54 is shown in Figure 16. The results are averaged over 20
instances with 5-10 evidence variables. The graph on the left in Figure 16 shows the mean
square error of the estimated posterior marginals as a function of time for Gibbs sampling,
33

Bidyuk & Dechter

cpcs54
cpcs179
cpcs360b
cpcs422b
grid15x30
random
2layer
coding

Gibbs
51
162
345
392
410
190
185
100

LC
16
8
26
47
169
30
17
26

w=2
17
11
22
65
163
61
22
38

Sampling Set Size
w=3 w=4 w=5 w=6
15
11
9
8
9
7
5
19
16
15
14
57
50
45
40
119
95
75
60
26
25
24
18
15
13
13
11
23
18
18
-

w=7
13
35
50
17
-

w=8
13
-

Table 1: Markov chain sampling set size as a function of w.

cpcs54
cpcs179
cpcs360b
cpcs422b
grid15x30
random
2layer
coding

Gibbs
5000
1500
400
300
2000
2000
200
2400

LC
2000, w= 5
400, w= 7
600, w= 3
4, w=14
500, w= 2
1000, w= 3
700, w= 3
1000, w= 3

No. of Samples
w=2 w=3 w=4 w=5
3000 2400
800
500
400
150
40
10
600
400
160
100
200
150
90
50
300
260
150
105
1400
700
450
300
900
320
150
75
1000
400
200
120

w=6
300
40
30
60
140
40
100

w=7
20
15
35
75
-

w=8
20
-

Table 2: Average number of samples generated per second as a function of w.
loop-cutset sampling, and w-cutset sampling for w=2, 3, 4, and 5. The second chart shows
accuracy as a function of w. The first point corresponds to Gibbs sampling; other points
correspond to loop-cutset sampling and w-cutset sampling with w ranging from 2 to 6. The
loop-cutset result is embedded with the w-cutset values at w=5. As explained in section 3.3,
the loop-cutset corresponds to the w-cutset where w is the maximum number of parents in
the network. Initially, the best results were obtained by 2- and 3-cutset sampling followed
by the loop-cutset sampling. With time, 2- and 5-cutset sampling become the best.
The results for cpcs179 are reported in Figure 17. Both charts show that loop-cutset
sampling and w-cutset sampling for w in range from 2 to 5 are superior to Gibbs sampling.
The chart on the left shows that the best of the cutset sampling schemes, having the lowest
MSE curves, are 2- and 3-cutset sampling. The loop-cutset curve falls in between 2- and
3-cutset at first and is outperformed by both 2- and 3-cutset after 12 seconds. Loop-cutset
sampling and 2- and 3-cutset sampling outperform Gibbs sampling by nearly two orders of
magnitude as their MSE falls below 1E-04 while Gibbs MSE remains on the order of 1E02. The 4- and 5-cutset sampling results fall in between, achieving the MSE ≈1E-03. The
curves corresponding to loop-cutset sampling and 2-, 3- and 4-cutset sampling fall below
the IBP line which means that all four algorithms outperform IBP in the first seconds of
execution (IBP converges in less than a second). The 5-cutset outperforms IBP after 8
seconds. In Figure 17 on the right, we see the accuracy results for all sampling algorithms
34

Cutset Sampling for Bayesian Networks

Gibbs

2.5E-04

MSE

2.0E-04
1.5E-04

IBP

cpcs54, N=54, |LC|=16, w*=15, |E|=8

IBP
LCS,w=5

2.5E-04

|C|=16,w=2
|C|=15,w=3

2.0E-04

|C|=11,w=4
|C|=9,w=5

1.5E-04

1.0E-04

MSE

cpcs54, N=54, |LC|=16, w*=15, |E|=8
3.0E-04

5.0E-05

Cutset, 5 sec
Cutset, 10 sec

1.0E-04
5.0E-05

0.0E+00

0.0E+00
0

2

4

6

8

10

12

14

Gibbs

w=2

w=3

w=4

Time (sec)

LCS,
w=5

w=5

w=6

Figure 16: MSE as a function of time (left) and w (right) in cpcs54, 20 instances, time
bound=12 seconds.

MSE

1.0E-03

1.0E-04

IBP

cpcs179, N=179, |LC|=8, w*=8, |E|=17

Cutset, 10 sec

1.0E-01

Cutset, 20 sec
1.0E-02

MSE

Gibbs
IBP
LCS,w=7
|C|=11,w=2
|C|=9,w=3
|C|=7,w=4
|C|=5,w=5

cpcs179, N=179, |LC|=8, w*=8, |E|=17
1.0E-02

1.0E-03
1.0E-04
1.0E-05

12

14

Time (sec)

LC
S,
w=
7

10

w=
5

8

w=
4

6

w=
3

4

w=
2

2

G

0

ib
bs

1.0E-05

Figure 17: MSE as a function of time (left) and w (right) in cpcs179, 20 instances, time
bound=12 seconds. Y-scale is exponential due to large variation in performance
of Gibbs and cutset sampling.

after 10 seconds and 20 seconds. They are in agreement with the convergence curves on the
left.
In cpcs360b (Figure 18), loop-cutset sampling and 2- and 3-cutset sampling have similar
performance. The accuracy of the estimates slowly degrades as w increases. Loop-cutset
sampling and w-cutset sampling substantially outperform Gibbs sampling for all values w
and exceed the accuracy of IBP within 1 minute.
On the example of cpcs422b, we demonstrate the significance of the adjusted induced
width of the conditioned network in the performance of cutset sampling. As we reported
in section 4.3, its loop-cutset is relatively small |LC|=47 but wLC =14 and thus, sampling
just one new loop-cutset variable value is exponential in the big adjusted induced width.
As a result, loop-cutset sampling computes only 4 samples per second while the 2-, 3and 4-cutset, which are only slightly larger having 65, 57, and 50 nodes respectively (see
Table 1), compute samples at rates of 200, 150, and 90 samples per second (see Table 2).
35

Bidyuk & Dechter

cpcs360b, N=360, |LC|=26, w*=21, |E|=15

cpcs360b, N=360, |E|=18, |LC|=26, w*=15

Gibbs

1.E-03

1.E-04

|C|=23,w=2
|C|=19,w=3

8.E-05

|C|=16,w=4
|C|=15,w=5

6.E-05

IBP
cutset,t=30sec
cutset,t=60sec

MSE

MSE

1.E-04

IBP
LCS,w=3

4.E-05

1.E-05

2.E-05

G

=7
w

w=
6

=5
w

Time (sec)

=4

70

w

60

=3

50

w

40

=3

30

LC
,w

20

s

10

ib
b

0

w=
2

0.E+00

1.E-06

Figure 18: MSE as a function of time (left) and w (right) in cpcs360b, 20 instances, time
bound=60 seconds. Y-scale is exponential due to large variation in performance
of Gibbs and cutset sampling.

Gibbs
IBP
LCS,w=2
|C|=65,w=2
|C|=57,w=3
|C|=50,w=4
|C|=45,w=5

1.8E-04

MSE

1.5E-04
1.2E-04

cpcs422b, N=422, |LC|=47, w*=22, |E|=28
1.0E-01

IBP
Cutset, 100 sec
Cutset, 200 sec

1.0E-02

MSE

cpcs422b, N=422, |LC|=47, w*=22, |E|=28
2.1E-04

9.0E-05

1.0E-03

6.0E-05

1.0E-04

3.0E-05
0.0E+00
0

20

40

60

80

100

120

140

1.0E-05

Gibbs

Time (sec)

w=2

w=3

w=4

w=5

w=6

w=7

Figure 19: MSE as a function of time (left) and w (right) in cpcs422b, 10 instances, time
bound=200 seconds. Y-scale is exponential due to large variation in performance
of Gibbs and cutset sampling.

The 5-cutset that is closest to loop-cutset in size, |C5 | = 45, computes 50 samples per
second which is an order of magnitude more than loop-cutset sampling. The results for
cpcs422b are shown in Figure 19. The loop-cutset sampling results are excluded due to its
poor performance. The chart on the right in Figure 19 shows that w-cutset performed well
in range of w = 2 − 7 and is far superior to Gibbs sampling. When allowed enough time,
w-cutset sampling outperformed IBP as well. The IBP converged in 5 seconds. The 2-, 3-,
and 4-cutset improved over IBP within 30 seconds, and 5-cutset after 50 seconds.
Random networks. Results from 10 instances of random multi-partite and 10 instances of 2-layer networks are shown in Figure 20. As we can see, w-cutset sampling
substantially improves over Gibbs sampling and IBP reaching optimal performance for
w = 2 − 3 for both classes of networks. In this range, its performance is similar to that of
loop-cutset sampling. In case of 2-layer networks, the accuracy of both Gibbs sampling and
36

Cutset Sampling for Bayesian Networks

random, R=50, N=200, P=3, |LC|=30, w*=22

random, R=50, N=200, P=3, |LC|=30, w*=22

1.5E-04
1.0E-04

IBP
cutset,t=30sec
cutset,t=60sec

1.5E-04

1.0E-04

5.0E-05

5.0E-05
0.0E+00

G

2layer, R=50, N=200, P=3, |LC|=17, w*=16

1.0E-04

w=
7

IBP

1.0E-01

cutset,t=10sec
cutset,t=20sec
1.0E-02

MSE

MSE

1.0E-03

w=
6

2layer, R=50, N=200, P=3, |LC|=17, w*=16

Gibbs
IBP
|LC|=17,w*=3
|C|=22,w*=2
|C|=15,w*=3
|C|=13,w*=4
|C|=12,w*=5

1.0E-02

w=
5

Time (sec)

w
=4

50

=3

40

,w

30

LC

20

ib
bs

10

w=
3

0.0E+00
0

w=
2

MSE

2.0E-04

2.0E-04

MSE

Gibbs
IBP
|LC|=30,w*=3
|C|=61,w*=2
|C|=26,w*=3
|C|=25,w*=4
|C|=24,w*=5

2.5E-04

1.0E-03

1.0E-04

1.0E-05
0

5

10

15

20

25

Time (sec)

1.0E-05
Gibbs

w =2

LC,w =3

w =3

w =4

w =5

w =6

Figure 20: Random multi-partite networks (top) and 2-layer networks (bottom), 200 nodes,
10 instances. MSE as a function of the number of samples (left) and w (right).

37

Bidyuk & Dechter

IBP is an order-of-magnitude less compared to cutset sampling (Figure 20, bottom right).
The poor convergence and accuracy of IBP on 2-layer networks was observed previously
(Murphy et al., 1999).
grid, 15x30, |E|=60, |LC|=169, w*=15, MSE
2.5E-04

1.5E-04

IBP
|LC|=169,w*=2

2.0E-04

1.2E-04

|C|=163,w*=2

100

120

cutset,t=100sec

Time (sec)

w*
=8

80

w*
=7

60

w*
=6

40

ibb
s

20

cutset,t=50sec

0.0E+00
G

0

IBP

3.0E-05

w*
=5

5.0E-05

6.0E-05

w*
=4

|C|=75,w*=5

w*
=3

|C|=95,w*=4

1.0E-04

9.0E-05

w*
=2
LC
,w
*=
2

|C|=119,w*=3

1.5E-04

MSE

MSE

grid, 15x30, |E|=40, |LC|=169, w*=20

Gibbs

Figure 21: Random networks, 450 nodes, 10 instances. MSE as a function of the number
of samples (left) and w (right).

Grid networks. Grid networks having 450 nodes (15x30) were the only class of benchmarks where full Gibbs sampling was able to produce estimates comparable to cutsetsampling (Figure 21). With respect to accuracy, the Gibbs sampler, loop-cutset sampling,
and 3-cutset sampling were the best performers and achieved similar results. Loop-cutset
sampling was the fastest and most accurate among cutset sampling schemes. Still, it generated samples about 4 times more slowly compared to Gibbs sampling (Table 2) since
loop-cutset is relatively large. The accuracy of loop-cutset sampling was closely followed
by 2-, 3- and 4-cutset sampling slowly degrading as w increased. Grid networks are an
example of benchmarks with regular graph structure (that cutset sampling cannot exploit
to its advantage) and small CPTs (in a two-dimensional grid network each node has at most
2 parents) where Gibbs sampling is strong.
coding 50x50, N=200, P=3, |LC|=26, w*=19

coding, 50x50, N=200, P=3, |LC|=26, w*=19

IBP
2.5E-04

cutset,t=5sec

|C|=38,w*=2

2.5E-04

cutset,t=10sec
2.0E-04

|C|=21,w*=3

2.0E-04

|C|=18,w*=4

MSE

MSE

IBP

|LC|=26,w*=3

3.0E-04

1.5E-04
1.0E-04

1.5E-04
1.0E-04

5.0E-05

5.0E-05
0.0E+00
0

2

4

6

8

10

0.0E+00
w=2

Time (sec)

w=3

LC,w=3

w=4

Figure 22: Coding networks, 50 code bits, 50 parity check bits, σ=0.4, 100 instances, time
bound=6 minutes.

38

Cutset Sampling for Bayesian Networks

cpcs54
cpcs179
cpcs360b
cpcs422b
grid15x30
random
2layer
coding

Time
20 sec
40 sec
100 sec
200 sec
100 sec
50 sec
20 sec
20 sec

Gibbs
4500
1500
2000
3000
2000
2000
200
650

Markov Chain Length
LC w=2 w=3 w=4
2200 4000 2400
800
400
400
150
40
3000 3000 2000
800
20 2000 1500
900
500
300
260
150
1000 1400
700
450
700
900
320
150
450
800
600
250

T
w=5
500
10
500
500
105
300
75
150

w=6
200
250
60
140
40
100

Table 3: Individual Markov chain length as a function of w. The length of each chain M was
adjusted for each sampling scheme for each benchmark so that the total processing
time across all sampling algorithms was the same.

Coding Networks. The cutset sampling results for coding networks are shown in
Figure 22. Here, the induced width varied from 18 to 22 allowing for exact inference.
However, we additionally tested and observed that the complexity of the network grows
exponentially with the number of coding bits (even after a small increase in the number of
coding bits to 60 yielding a total of 240 nodes after corresponding adjustments to the number
of parity-checking bits and transmitted code size, the induced width exceeds 24) while the
time for each sample generation scales up linearly. We collected results for 10 networks
(10 different parity check matrices) with 10 different evidence instantiations (total of 100
instances). In decoding, the Bit Error Rate (BER) is a standard error measure. However,
we computed MSE over all unobserved nodes to evaluate the quality of approximate results
more precisely. As expected, Gibbs sampling did not converge (because the Markov chain
was non-ergodic) and was left off the charts. The charts in Figure 22 show that loop-cutset
is an optimal choice for coding networks whose performance is closely followed by 2-cutset
sampling. As we saw earlier, cutset sampling outperforms IBP.
4.5 Computing an Error Bound
Second to the issue of convergence of sampling scheme is always the problem of predicting
the quality of the estimates and deciding when to stop sampling. In this section, we compare
empirically the error intervals for Gibbs and cutset sampling estimates.
Gibbs sampling and cutset sampling are guaranteed to converge to the correct posterior
distribution in ergodic networks. However, it is hard to estimate how many samples are
needed to achieve a certain degree of convergence. It is possible to derive bounds on the
absolute error based on sample variance for any sampling method if the samples are independent. In Gibbs and other MCMC methods, samples are dependent and we cannot apply
the confidence interval estimate directly. In case of Gibbs sampling, we can apply the batch
means method that is a special case of standardized time series method and is used by the
BUGS software package (Billingsley, 1968; Geyer, 1992; Steiger & Wilson, 2001).
39

Bidyuk & Dechter

The main idea is to “split” a Markov chain of length M · T into M chains of length
T . Let P̂m (xi |e) be an estimate derived from a single chain m ∈ [1, ..., M ] of length T
(meaning, containing T samples) as defined in equations (28)-(29). The estimates P̂m (x|e)
are assumed approximately independent for large enough M . Assuming that convergence
conditions are satisfied and the central limit theorem holds, the P̂m (x|e) is distributed
according to N (E[P (xi |e)], σ 2 ) so that the posterior marginal P̂ (Xi |e) is obtained as an
average of the M results obtained from each chain, namely:
P̂ (x|e) =

M
1 X
P̂m (x|e)
M

(33)

m=1

and the sampling variance is computed as usually:
σ2 =

M
X
1
(P̂m (x|e) − P̂ (x|e))2
M −1
m=1

An equivalent expression for the sampling variance is:
PM
P̂ 2 (x|e) − M P̂ 2 (x|e)
2
σ = m=1 m
M −1

(34)

where σ 2 is easy to compute incrementally storing only the running sums of P̂m (x|e) and
2 (x|e). Therefore, we can compute the confidence interval in the 100(1 − α) percentile
P̂m
used for random variables with normal distribution for small sampling set sizes. Namely:
"
r #
σ2
=1−α
(35)
P P (x|e) ∈ [P̂ (x|e) ± t α2 ,(M −1)
M
where t α2 ,(M −1) is a table value from t distribution with (M − 1) degrees of freedom.
We used the batch means approach to estimate the confidence interval in the posterior
marginals with one modification. Since we were working with relatively small sample sets
(a few thousand samples) and the notion of “large enough” M is not well defined, we
restarted the chain after every T samples to guarantee that the estimates P̂m (x|e) were
truly independent. The method of batch means only provides meaningful error estimates
assuming that the samples are drawn from the stationary distribution. We assume that in
our problems the chains mix fast enough so that the samples are drawn from the target
distribution.
We applied this approach to estimate the error bound in the Gibbs sampler and the
cutset sampler. We have computed a 90% confidence interval for the estimated posterior
marginal P (xi |e) based on the sampling variance of Pm (xi |e) over 20 Markov chains as
described above. We computed sampling variance σ 2 from Eq.(34) and the 90% confidence
interval ∆0.9 (xi ) from Eq.(35) and averaged over all nodes:
X X
1
∆0.9 = P
∆0.9 (xi )
N i |D(Xi )|
i

xi ∈D(Xi )

The estimated confidence interval can be too large to be practical. Thus, we compared ∆0.9
with the empirical average absolute error ∆:
40

Cutset Sampling for Bayesian Networks

cpcs54
cpcs179
cpcs360b
cpcs422b
random
2layer
coding
grid15x30

∆
∆0.9
∆
∆0.9
∆
∆0.9
∆
∆0.9
∆
∆0.9
∆
∆0.9
∆
∆0.9
∆
∆0.9

Average Error
LC
w=2
0.00036 0.00030
0.00076 0.00064
0.00086 0.00074
0.00148 0.00111
0.00011 0.00010
0.00022 0.00023
- 0.00018
- 0.00033
0.00039 0.00119
0.00080 0.00247
0.00066 0.00063
0.00145 0.00144
0.00014 0.00019
0.00030 0.00035
0.00099 0.00119
0.00214 0.00247

Gibbs
0.00056
0.00119
0.01577
0.02138
0.00051
0.00113
0.00055
0.00119
0.00091
0.00199
0.00436
0.00944
0.00108
0.00248

and Confidence Interval
w=3
w=4
w=5
0.00030
0.00040 0.00036
0.00063
0.00098 0.00112
0.00066
0.00113 0.00178
0.00164
0.00235 0.00392
0.00008
0.00014 0.00012
0.00021
0.00030 0.00028
0.00020
0.00018 0.00027
0.00035
0.00043 0.00060
0.00091
0.00099 0.00109
0.00205
0.00225 0.00222
0.00082
0.00117 0.00134
0.00185
0.00235 0.00302
0.00019 0.000174
0.00034 0.000356
0.00091
0.00099 0.00109
0.00205
0.00225 0.00222

w=6
0.00067
0.00116
0.00022
0.00046
0.00037
0.00074
0.00113
0.00239
0.00197
0.00341
0.00113
0.00239

Table 4: Average absolute error ∆ (measured) and estimated confidence interval ∆0.9 as a
function of w over 20 Markov Chains.

∆=

N

X
1
i |D(Xi )|

P

i

X

xi ∈D(Xi )

|P̂ (xi |e) − P (xi |e))

The objective of this study was to observe whether the computed confidence interval ∆0.9
(estimated absolute error) accurately reflects the true absolute error ∆, namely, to verify
that ∆ < ∆0.9 , and if so, then investigate empirically whether confidence interval for cutsetsampling estimates will be smaller compared to Gibbs sampling as we would expect due to
variance reduction.
Table 4 presents the average confidence interval and average absolute error for our
benchmarks. For each benchmark, the first row of results (row ∆) reports the average
absolute error and the second row of results (row ∆0.9 ) reports the 90% confidence interval.
Each column in Table 4 corresponds to a sampling scheme. The first column reports results
for Gibbs sampling. The second column reports results for loop-cutset sampling. The
remaining columns report results for w-cutset sampling for w in range 2−6. The loop-cutset
sampling results for cpcs422b are not included due to statistically insignificant number of
samples generated by loop-cutset sampling. The Gibbs sampling results for coding networks
are left out because the network is not ergodic (as mentioned earlier) and Gibbs sampling
does not converge.
We can see that for all the networks ∆ < ∆0.9 which validates our method for measuring
confidence interval. In most cases the estimated confidence interval ∆0.9 is no more than
2-3 times the size of average error ∆ and is relatively small. In case of cutset sampling, the
largest confidence interval max ∆0.9 = 0.00247 is reported in grid networks for loop-cutset
41

Bidyuk & Dechter

sampling. Thus, the confidence interval estimate could be used as a criteria reflecting the
quality of the posterior marginal estimate by the sampling algorithm in practice. Subsequently, comparing the results for Gibbs sampling and cutset sampling, we observe not
only a significant reduction in the average absolute error, but also a similar reduction in the
estimated confidence interval. Across all benchmarks, the estimated confidence interval of
the Gibbs sampler remains ∆0.9 > 1E-3. At the same time, for cutset sampling we obtain
∆0.9 < 1E-3 in 5 out of 8 classes of networks (excluded are the cpcs179, grid, and 2-layer
networks).
4.6 Discussion
Our empirical evaluation of the performance of cutset sampling demonstrates that, except
for grid networks, sampling on a cutset usually outperforms Gibbs sampling. We show that
convergence of cutset sampling in terms of number of samples dramatically improves as
predicted theoretically.
The experiments clearly show that there exists a range of w-values where w-cutset
sampling outperforms Gibbs sampler. The performance of w-cutset sampling deteriorates
when increase in w yields only a small reduction in the cutset size. An example is cpcs360b
network where starting with w=4, increasing w by 1 results in the reducing the sampling
set by only 1 node (shown in Table 1).
We observe that the loop-cutset is a good choice of cutset sampling as long as the
induced width of network wLC conditioned on loop-cutset is reasonably small. When wLC
is large (as in cpcs422b), loop-cutset sampling is computationally less efficient then w-cutset
sampling for w < wLC .
We also showed in Section 4.3 that both Gibbs sampling and loop-cutset sampling
outperform the state-of-the-art AIS-BN adaptive importance sampling method when the
probability of evidence is small. Consequently, all the w-cutset sampling schemes in Section 4.4 that outperformed Gibbs sampler in cpcs360b and cpcs422b would also outperfrom
AIS-BN.

5. Related Work
We mention here some related work. The idea of marginalising out some variables to improve
efficiency of Gibbs sampling was first proposed by Liu et al. (1994). It was successfully
applied in several special classes of Bayesian models. Kong et al. (1994) applied collapsing
to the bivariate Gaussian problem with missing data. Liu (1994) defined a collapsed Gibbs
sampling algorithm for finding repetitive motifs in biological sequences applies by integrating
out two parameters from the model. Similarly, Gibbs sampling set is collapsed in Escobar
(1994), MacEachern (1994), and Liu (1996) for learning the nonparametric Bayes problem.
In all of the instances above, special relationships between problem variables have been
exploited to integrate several variables out resulting in a collapsed Gibbs sampling approach.
Compared to this previous research work, our contribution is in defining a generic scheme
for collapsing Gibbs sampling in Bayesian networks which takes advantage of the network’s
graph properties and does not depend on the specific form of the relationships between
variables.
42

Cutset Sampling for Bayesian Networks

Jensen et al. (1995) combined sampling and exact inference in a blocking Gibbs sampling
scheme. Groups of variables were sampled simultaneously using exact inference to compute
the needed conditional distributions. Their empirical results demonstrate a significant improvement in the convergence of the Gibbs sampler over time. Yet, in proposed blocking
Gibbs sampling, the sample contains all variables in the network. In contrast, cutset sampling reduces the set of variables that are sampled. As noted previously, collapsing produces
lower variance estimates than blocking and, therefore, cutset sampling should require fewer
samples to converge.
A different combination of sampling and exact inference for join-trees was described
by Koller et al. (1998) and Kjaerulff (1995). oller et al. and Kjaerulff proposed to sample
the probability distribution in each cluster for computing the outgoing messages. Kjaerulff
used Gibbs sampling only for large clusters to estimate the joint probability distribution
P (Vi ), Vi ⊂ X in cluster i. The estimated P̂ (Vi ) is recorded instead of the true joint
distribution to conserve memory. The motivation is that only high-probability tuples will
be recorded while the remaining low-probability tuples are assumed to have probability 0.
In small clusters, the exact joint distribution P (Vi ) is computed and recorded. However, the
paper does not analyze the introduced errors or compare the performance of this scheme
with standard Gibbs sampler or the exact algorithm. No analysis of error is given nor
comparison with other approaches.
Koller et al. (1998) used sampling used to compute messages sent from cluster i to
cluster j and the posterior joint distributions in a cluster-tree that contains both discrete
and continuous variables. This approach subsumes the cluster-based sampling proposed
by Kjaerulff (1995) and includes rigorous analysis of the error in the estimated posterior
distributions. The method has difficulties with propagation of evidence. The empirical
evaluation is limited to two hybrid network instances and compares the quality of the
estimates to those of likelihood weighting, an instance of importance sampling that does
not perform well in presence of low-probability evidence.
The effectiveness of collapsing of sampling set has been demonstrated previously in the
context of Particle Filtering method for Dynamic Bayesian networks (Doucet, Andrieu, &
Godsill, 2000a; Doucet, deFreitas, & Gordon, 2001; Doucet, de Freitas, Murphy, & Russell,
2000b). It was shown that sampling from a subspace combined with exact inference (RaoBlackwellised Particle Filtering) yields a better approximation than Particle Filtering on
the full set of variables. However, the objective of the study has been limited to observation
of the effect in special cases where some of the variables can be integrated out easily. Our
cutset sampling scheme offers a generic approach to collapsing a Gibbs sampler in any
Bayesian network.

6. Conclusion
The paper presents the w-cutset sampling scheme, a general scheme for collapsing Gibbs
sampler in Bayesian networks. We showed theoretically and empirically that cutset sampling improves the convergence rate and allows sampling from non-ergodic network that
has ergodic subspace. By collapsing the sampling set, we reduce the dependence between
samples by marginalising out some of the highly correlated variables and smoothing the
sampling distributions of the remaining variables. The estimators obtained by sampling
43

Bidyuk & Dechter

from a lower-dimensional space also have a lower sampling variance. Using the induced
width w as a controlling parameter, w-cutset sampling provides a mechanism for balancing
sampling and exact inference.
We studied the power of cutset sampling when the sampling set is a loop-cutset and,
more generally, when the sampling set is a w-cutset of the network (defined as a subset of
variables such that, when instantiated, the induced width of the network is ≤ w). Based
on Rao-Blackwell theorem, cutset sampling requires fewer samples than regular sampling
for convergence. Our experiments showed that this reduction in number of samples was
time-wise cost-effective. We confirmed this over a range of randomly generated and real
benchmarks. We also demonstrated that cutset sampling is superior to the state of the art
AIS-BN importance sampling algorithm when the probability of evidence is small.
Since the size of the cutset and the correlations between the variables are two main
factors contributing to the speed of convergence, w-cutset sampling may be optimized further with the advancement of methods for finding minimal w-cutset. Another promising
direction for future research is to incorporate the heuristics for avoiding selecting stronglycorrelated variables into a cutset since those correlations are driving factors in the speed
of convergence of Gibbs sampling. Alternatively, we could combine sample collapsing with
blocking.
In summary, w-cutset sampling scheme is a simple yet powerful extension of sampling
in Bayesian networks that is likely to dominate regular sampling for any sampling method.
While we focused on Gibbs sampling with better convergence characteristics, other sampling
schemes can be implemented with the cutset sampling principle. In particular, it was
adapted for use with likelihood weighting (Bidyuk & Dechter, 2006).

References
Abdelbar, A. M., & Hedetniemi, S. M. (1998). Approximating maps for belief networks is
NP-hard and other theorems. Artificial Intelligence, 102, 21–38.
Andrieu, C., de Freitas, N., & Doucet, A. (2002). Rao-Blackwellised particle filtering via
data augmentation. In Advances in Neural Information Processing Systems. MIT
Press.
Arnborg, S. A. (1985). Efficient algorithms for combinatorial problems on graphs with
bounded decomposability - a survey. BIT, 25, 2–23.
Becker, A., Bar-Yehuda, R., & Geiger, D. (2000). Random algorithms for the loop cutset
problem. Journal of Artificial Intelligence Research, 12, 219–234.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press.
Bidyuk, B., & Dechter, R. (2003). Empirical study of w-cutset sampling for Bayesian networks. In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence
(UAI), pp. 37–46. Morgan Kaufmann.
Bidyuk, B., & Dechter, R. (2004). On finding minimal w-cutset problem. In Proceedings
of the 20th Conference on Uncertainty in Artificial Intelligence (UAI), pp. 43–50.
Morgan Kaufmann.
44

Cutset Sampling for Bayesian Networks

Bidyuk, B., & Dechter, R. (2006). Cutset Sampling with Likelihood Weighting. In Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence (UAI), pp.
39–46. Morgan Kaufmann.
Billingsley, P. (1968). Convergence of Probability Measures. John Wiley & Sons, New York.
Casella, G., & Robert, C. P. (1996). Rao-Blackwellisation of sampling schemes. Biometrika,
83 (1), 81–94.
Cheng, J., & Druzdzel, M. J. (2000). AIS-BN: An adaptive importance sampling algorithm
for evidenctial reasoning in large baysian networks. Journal of Aritificial Intelligence
Research, 13, 155–188.
Cooper, G. (1990). The computational complexity of probabilistic inferences. Artificial
Intelligence, 42, 393–405.
Dagum, P., & Luby, M. (1993). Approximating probabilistic inference in Bayesian belief
networks is NP-hard. Artificial Intelligence, 60 (1), 141–153.
Dechter, R. (1999a). Bucket elimination: A unifying framework for reasoning. Artificial
Intelligence, 113, 41–85.
Dechter, R. (1999b). Bucket elimination: A unifying framework for reasoning. Artificial
Intelligence, 113 (1–2), 41–85.
Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.
Doucet, A., & Andrieu, C. (2001). Iterative algorithms for state estimation of jump Markov
linear systems. IEEE Trans. on Signal Processing, 49 (6), 1216–1227.
Doucet, A., Andrieu, C., & Godsill, S. (2000a). On sequential Monte Carlo sampling methods for Bayesian filtering. Statistics and Computing, 10 (3), 197–208.
Doucet, A., de Freitas, N., Murphy, K., & Russell, S. (2000b). Rao-Blackwellised particle
filtering for dynamic Bayesian networks. In Proceedings of the 16th Conference on
Uncertainty in Artificial Intelligence (UAI), pp. 176–183.
Doucet, A., deFreitas, N., & Gordon, N. (2001). Sequential Monte Carlo Methods in Practice.
Springer-Verlag, New York, Inc.
Doucet, A., Gordon, N., & Krishnamurthy, V. (1999). Particle filters for state estimation of jump markov linear systems. Tech. rep., Cambridge University Engineering
Department.
Escobar, M. D. (1994). Estimating normal means iwth a dirichlet process prior. Journal of
the American Statistical Aasociation, 89, 268–277.
Frey, B. J., & MacKay, D. J. C. (1997). A revolution: Belief propagation in graphs with
cycles. In Neural Information Processing Systems, Vol. 10.
Fung, R., & Chang, K.-C. (1989). Weighing and integrating evidence for stochastic simulation in Bayesian networks. In Proceedings of the 5th Conference on Uncertainty in
Artificial Intelligence (UAI), pp. 209–219. Morgan Kaufmann.
Geiger, D., & Fishelson, M. (2003). Optimizing exact genetic linkage computations. In Proceedings of the 7th Annual International Conf. on Computational Molecular Biology,
pp. 114–121. Morgan Kaufmann.
45

Bidyuk & Dechter

Gelfand, A., & Smith, A. (1990). Sampling-based approaches to calculating marginal densities. Journal of the American Statistical Association, 85, 398–409.
Geman, S., & Geman, D. (1984). Stochastic relaxations, Gibbs distributions and the
Bayesian restoration of images. IEEE Transaction on Pattern analysis and Machine
Intelligence, 6, 721–742.
Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 7, 473–483.
Gilks, W., Richardson, S., & Spiegelhalter, D. (1996). Markov chain Monte Carlo in practice.
Chapman and Hall.
Gottlob, G., Leone, N., & Scarello, F. (1999). A comparison of structural CSP decomposition
methods. In Proceedings of the 16th International Joint Conference on Artificial
Intelligence (IJCAI), pp. 394–399. Morgan Kaufmann.
Jensen, C., Kong, A., & Kjærulff, U. (1995). Blocking Gibbs sampling in very large probabilistic expert systems. Int. J. of Human Computer Studies. Special Issue on RealWorld Applications of Uncertain Reasoning, 42 (6), 647–666.
Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. (1990). Bayesian updating in causal
probabilistic networks by local computation. Computational Statistics Quarterly, 4,
269–282.
Jones, G., & Hobert, J. P. (2001). Honest exploration of intractable probability distributions
via Markov Chain Monte Carlo. Statist. Sci., 16 (4), 312–334.
Kask, K., Dechter, R., Larrosa, J., & Dechter, A. (2005). Unifying cluster-tree decompositions for reasoning in graphical models. Artificial Intelligence, 166, 165–193.
Kjærulff, U. (1995). HUGS: Combining exact inference and Gibbs sampling in junction
trees. In Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence
(UAI), pp. 368–375. Morgan Kaufmann.
Koller, D., Lerner, U., & Angelov, D. (1998). A general algorithm for approximate inference
and its application to hybrid Bayes nets. In Proceedings of the 14th Conference on
Uncertainty in Artificial Intelligence (UAI), pp. 324–333.
Kong, A., Liu, J. S., & Wong, W. (1994). Sequential imputations and Bayesian missing
data problems. J. of the American Statistical Association, 89 (425), 278–288.
Kschischang, F. R., & Frey, B. J. (1998). Iterative decoding of compound codes by probability propagation in graphical models. IEEE Journal on Selected Areas in Communications, 16, 219–230.
Larrosa, J., & Dechter, R. (2003). Boosting search with variable elimination in constraint
optimization and constraint satisfaction problems. Constraints, 8 (3), 303–326.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computation with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical
Society, Series B, 50(2), 157–224.
Liu, J. (1991). Correlation Structure and Convergence Rate of the Gibbs Sampler, Ph.D.
Thesis. University of Chicago.
46

Cutset Sampling for Bayesian Networks

Liu, J. (1994). The collapsed Gibbs sampler in Bayesian computations with applications to
a gene regulation problem. Journal of the American Statistical Association, 89 (427),
958–966.
Liu, J., Wong, W., & Kong, A. (1994). Covariance structure of the Gibbs sampler with
applications to the comparison of estimators and augmentation schemes. Biometrika,
81 (1), 27–40.
Liu, J. S. (1996). Nonparametric hierarchical bayes via sequential imputations. Annals of
Statistics, 24 (3), 911–930.
Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer-Verlag, New
York, Inc.
MacEachern, S., Clyde, M., & Liu, J. (1998). Sequential importance sampling for nonparametric bayes models: The next generation. The Canadian Journal of Statistics, 27,
251–267.
MacEachern, S. N. (1994). Estimating normal means with a conjugate style dirichlet process
prior. Communications in Statistics-Simulation and Computation, 23 (3), 727–741.
MacKay, D. (1996). Introduction to Monte Carlo methods. In Proceedings of NATO Advanced Study Institute on Learning in Graphical Models. Sept 27-Oct 7, pp. 175–204.
Maier, D. (1983). The theory of relational databases. In Computer Science Press, Rockville,
MD.
McEliece, R., MacKay, D., & Cheng, J.-F. (1997). Turbo decoding as an instance of Pearl’s
belief propagation algorithm. IEEE J. Selected Areas in Communication, 16, 140–152.
Miller, R., Masarie, F., & Myers, J. (1986). Quick medical reference (QMR) for diagnostic
assistance. Medical Computing, 3 (5), 34–38.
Miller, R., Pople, H., & Myers, J. (1982). Internist-1: An experimental computerbased
diagnostic consultant for general internal medicine. New English Journal of Medicine,
307 (8), 468–476.
Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999). Loopy belief propagation for approximate
inference: An empirical study. In Proceedings of the 15th Conference on Uncertainty
in Artificial Intelligence (UAI), pp. 467–475. Morgan Kaufmann.
Parker, R., & Miller, R. (1987). Using causal knowledge to create simulated patient cases:
the CPCS project as an extension of INTERNIST-1. In Proceedings of the 11th Symp.
Comp. Appl. in Medical Care, pp. 473–480.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Peot, M. A., & Shachter, R. D. (1992). Fusion and propagation with multiple observations
in belief networks. Artificial Intelligence, 48, 299–318.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering for
large belief networks. In Proceedings of 10th Conference on Uncertainty in Artificial
Intelligence, Seattle, WA, pp. 484–490.
Rish, I., Kask, K., & Dechter, R. (1998). Empirical evaluation of approximation algorithms
for probabilistic decoding. In Proceedings of the 14th Conference on Uncertainty in
Artificial Intelligence (UAI), pp. 455–463. Morgan Kaufmann.
47

Bidyuk & Dechter

Roberts, G. O., & Sahu, S. K. (1997). Updating schemes; correlation structure; blocking
and parameterization for the Gibbs sampler. Journal of the Royal Statistical Society,
Series B, 59 (2), 291–317.
Roberts, G. O., & Tweedie, R. L. (1999). Bounds on regeneration times and convergence
rates for Markov chains. Stochastic Processes and Their Applications, 80, 211–229.
Roberts, G. O., & Tweedie, R. L. (2001). Corregendum to bounds on regeneration times and
convergence rates for Markov chains. Stochastic Processes and Their Applications, 91,
337–338.
Rosenthal, J. S. (1995). Convergence rates for Markov Chains. SIAM Review, 37 (3), 387–
405.
Rosti, A.-V., & Gales, M. (2004). Rao-Blackwellised Gibbs sampling for switching linear
dynamical systems. In IEEE International Conference on Acoustics, Speech, and
Signal Processing (ICASSP 2004), pp. 809–812.
Schervish, M., & Carlin, B. (1992). On the convergence of successive substitution sampling.
Journal of Computational and Graphical Statistics, 1, 111–127.
Shachter, R. D., Andersen, S. K., & Solovitz, P. (1994). Global conditioning for probabilistic
inference in belief networks. In Proceedings of the 10th Conference on Uncertainty in
Artificial Intelligence (UAI), pp. 514–522.
Shachter, R. D., & Peot, M. A. (1989). Simulation approaches to general probabilistic
inference on belief networks. In Proceedings of the 5th Conference on Uncertainty in
Artificial Intelligence (UAI), pp. 221—231.
Steiger, N. M., & Wilson, J. R. (2001). Convergence properties of the batch means method
for simulation output analysis. INFORMS Journal on Computing, 13 (4), 277–293.
Tierney, L. (1994). Markov chains for exploring posterior distributions. Annals of Statistics,
22 (4), 1701–1728.
Yuan, C., & Druzdzel, M. (2003). An importance sampling algorithm based on evidence
pre-propagation. In Proceedings of the 19th Conference on Uncertainty in Artificial
Intelligence (UAI), pp. 624–631.
Zhang, N., & Poole, D. (1994). A simple algorithm for Bayesian network computations. In
Proceedings of the 10th Canadian Conference on Artificial Intelligence, pp. 171–178.

48

Journal of Artificial Intelligence Research 28 (2007) 299-348

Submitted 07/06; published 03/07

Supporting Temporal Reasoning by Mapping
Calendar Expressions to Minimal Periodic Sets
Claudio Bettini
Sergio Mascetti

bettini@dico.unimi.it
mascetti@dico.unimi.it

Dipartimento di Informatica e Comunicazione, Università di Milano
Via Comelico, 39, 20135, Milan, Italy

X. Sean Wang

Sean.Wang@uvm.edu

Department of Computer Science, University of Vermont
33 Colchester Avenue, Burlington, VT, 05405 USA

Abstract
In the recent years several research efforts have focused on the concept of time granularity and its applications. A first stream of research investigated the mathematical models
behind the notion of granularity and the algorithms to manage temporal data based on
those models. A second stream of research investigated symbolic formalisms providing a set
of algebraic operators to define granularities in a compact and compositional way. However, only very limited manipulation algorithms have been proposed to operate directly
on the algebraic representation making it unsuitable to use the symbolic formalisms in
applications that need manipulation of granularities.
This paper aims at filling the gap between the results from these two streams of research,
by providing an efficient conversion from the algebraic representation to the equivalent
low-level representation based on the mathematical models. In addition, the conversion
returns a minimal representation in terms of period length. Our results have a major
practical impact: users can more easily define arbitrary granularities in terms of algebraic
operators, and then access granularity reasoning and other services operating efficiently
on the equivalent, minimal low-level representation. As an example, we illustrate the
application to temporal constraint reasoning with multiple granularities.
From a technical point of view, we propose an hybrid algorithm that interleaves the
conversion of calendar subexpressions into periodical sets with the minimization of the period length. The algorithm returns set-based granularity representations having minimal
period length, which is the most relevant parameter for the performance of the considered reasoning services. Extensive experimental work supports the techniques used in the
algorithm, and shows the efficiency and effectiveness of the algorithm.

1. Introduction
According to a 2006 research by Oxford University Press, the word time has been found
to be the most common noun in the English language, considering diverse sources on the
Internet including newspapers, journals, fictions and weblogs. What is somehow surprising
is that among the 25 most common nouns we find time granularities like day, week, month
and year. We are pretty sure that many other time granularities like business day, quarter,
semester, etc. would be found to be quite frequently used in natural languages. However,
the way computer applications deal with these concepts is still very naive and mostly hidden in program code and/or based on limited and sometimes imprecise calendar support.

c
2007
AI Access Foundation. All rights reserved.

Bettini, Mascetti & Wang

Temporal representation and reasoning has been for a long time an AI research topic aimed
at providing a formal framework for common sense reasoning, natural language understanding, planning, diagnosis and many other complex tasks involving time data management.
Despite the many relevant contributions, time granularity representation and reasoning
support has very often been ignored or over-simplified. In the very active area of temporal
constraint satisfaction, most proposals implicitly assumed that adding support for granularity was a trivial extension. Only quite recently it was recognized that this is not the
case and specific techniques were proposed (Bettini, Wang, & Jajodia, 2002a). Even the
intuitively simple task of deciding whether a specific instant is part of a time granularity
can be tricky when arbitrary user-defined granularities like e.g., banking days, or academic
semesters are considered.
Granularities and periodic patterns in terms of granularities are playing a role even
in emerging application areas like inter-organizational workflows and personal information
management (PIM). For example, inter-organizational workflows need to model and monitor
constraints like: Event2 should occur no later than two business days after the occurrence
of Event1. In the context of PIM, current calendar applications, even on mobile devices,
allow the user to specify quite involved periodical patterns for the recurrence of events. For
example, it is possible to schedule an event every last Saturday of every two months. The
complexity of the supported patterns has been increasing in the last years, and the current
simple interfaces are showing their limits. They are essentially based on a combination of
recurrences based on one or two granularities taken from a fixed set (days, weeks, months,
and years). We foresee the possibility for significant extensions of these applications by
specifying recurrences over user-defined granularities. For example, the user may define (or
upload from a granularity library) the granularity corresponding to the academic semester
of the school he is teaching at, and set the date of the finals as the last Monday of each
semester. A bank may want to define its banking days granularity and some of the bank
policies may then be formalized as recurrences in terms of that granularity. Automatically
generated appointments from these policies may appear on the devices of bank employees
involved in specific procedures. We also foresee the need to show a user preferred view of
the calendar. With current standard applications the user has a choice between a businessday limited view and a complete view, but why not enabling a view based on the users’s
consulting-days, for example? A new perspective in the use of mobile devices may also result
from considering the time span in which activities are supposed to be executed (expressed
in arbitrary granularities), and having software agents on board to alert about constraints
that may be violated, even based on contextual information like the user location or traffic
conditions. This scenario highlights three main requirements: a) a sufficiently expressive
formal model for time granularity, b) a convenient way to define new time granularities,
and c) efficient reasoning tools over time granularities.
Consider a). In the last decade significant efforts have been made to provide formal
models for the notion of time granularity and to devise algorithms to manage temporal
data based on those models. In addition to logical approaches (Montanari, 1996; Combi,
Franceschet, & Peron, 2004), a framework based on periodic-set representations has been
extensively studied (Bettini, Wang, & Jajodia, 2000), and more recently an approach based
on strings and automata was introduced (Wijsen, 2000; Bresolin, Montanari, & Puppis,
2004). We are mostly interested in the last two approaches because they support the effective
300

Mapping Calendar Expressions to Minimal Periodic Sets

computation of basic operations on time granularities. In both cases the representation of
granularities can be considered as a low-level one, with a rather involved specification in
terms of the instants of the time domain.
Consider requirement b) above. Users may have a hard time in defining granularities
in formalisms based on low-level representations, and to interpret the output of operations.
It is clearly unreasonable to ask users to specify granularities by linear equations or other
mathematical formalisms that operate directly in terms of instants or of granules of a fixed
time granularity. Hence, a second stream of research investigated more high-level symbolic
formalisms providing a set of algebraic operators to define granularities in a compact and
compositional way. The efforts on this task started even before the research on formal
models for granularity (Leban, McDonald, & Forster, 1986; Niezette & Stevenne, 1992) and
continued as a parallel stream of research (Bettini & Sibi, 2000; Ning, Wang, & Jajodia,
2002; Terenziani, 2003; Urgun, Dyreson, Snodgrass, Miller, Soo, Kline, & Jensen, 2007).
Finally, let us consider requirement c) above. Several inferencing operations have been
defined on low-level representations, including equivalence, inclusion between granules in
different granularities, and even complex inferencing services like constraint propagation
(Bettini et al., 2002a). Even for simple operations no general method is available operating
directly on the high level representation. Indeed, in some cases, the proposed methods
cannot exploit the structure of the expression and require the enumeration of granules,
which may be very inefficient. This is the case, for example, of the granule conversion
methods presented by Ning e at. (2002). Moreover, we are not aware of any method to
perform other operations, such as equivalence or intersection of sets of granules, directly in
terms of the high level representation.
The major goal of this paper is to provide a unique framework to satisfy the requirements
a), b), and c) identified above, by adding to the existing results a smart and efficient
technique to convert granularity specifications from the high-level algebraic formalism to the
low-level one, for which many more reasoning tools are available. In particular, in this paper
we focus on the conversion from the high-level formalism called Calendar Algebra (Ning
et al., 2002) to the low-level formalism based on periodical sets (Bettini et al., 2000, 2002a).
Among the several proposals for the high-level (algebraic) specification of granularities, the
choice of Calendar Algebra has two main motivations: first, it allows the user to express
a large class of granularities; For a comparison of the expressiveness of Calendar Algebra
with other formalisms see (Bettini et al., 2000). Second, it provides the richest set of
algebraic operations that are designed to reflect the intuitive ways in which users define
new granularities. A discussion on the actual usability of this tool and on how it could
be enhanced by a graphical user interface can be found in Section 6.2. The choice of
the low-level formalism based on periodic-sets also has two main motivations: first, an
efficient implementation of all the basic operations already exists and has been extensively
experimented (Bettini, Mascetti, & Pupillo, 2005); second, it is the only one currently
supporting the complex operations on granularities needed for constraint satisfaction, as it
will be illustrated in more detail in Section 6.1.
The technical contribution of this paper is a hybrid algorithm that interleaves the conversion of calendar subexpressions into periodical sets with a step for period minimization.
A central phase of our conversion procedure is to derive, for each algebraic subexpression,
the periodicity of the output set. This periodicity is used to build the periodical represen301

Bettini, Mascetti & Wang

tation of the subexpression that can be recursively used as operand of other expressions.
Given a calendar algebra expression, the algorithm returns set-based granularity representations having minimal period length. The period length is the most relevant parameter
for the performance both of basic operations on granularities and of more specialized ones
like the operations used by the constraint satisfaction service. Extensive experimental work
reported in this paper validates the techniques used in the algorithm, by showing, among
other things, that (1) even large calendar expressions can be efficiently converted, and (2)
less precise conversion formulas may lead to unacceptable computation time. This latter
property shows the importance of carefully and accurately designed conversion formulas.
Indeed, conversion formulas may seem trivial if the length of periodicity is not a concern.
In designing our conversion formulas, we made an effort to reduce the period length of the
resulting granularity representation, and thus render the whole conversion process computationally efficient.
In the next section we define granularities; several interesting relationships among them
are highlighted and the periodical set representation is formalized. In Section 3 we define
Calendar Algebra and present its operations. In Section 4 we describe the conversion
process: after the definition of the three steps necessary for the conversion, for each algebraic
operation we present the formulas to perform each step. In Section 5 we discuss the period
minimality issue, and we report experimental results based on a full implementation of
the conversion algorithm and of its extension ensuring minimality. In Section 6 we further
motivate our work by presenting a complete application scenario. Section 7 reports the
related work, and Section 8 concludes the paper.

2. Formal Notions of Time Granularities
Time granularities include very common ones like hours, days, weeks, months and years,
as well as the evolution and specialization of these granularities for specific contexts or
applications. Trading days, banking days, and academic semesters are just few examples
of specialization of granularities that have become quite common when describing policies
and constraints.
2.1 Time Granularities
A comprehensive formal study of time granularities and their relationships can be found
in (Bettini et al., 2000). In this paper, we only introduce notions that are essential to
show our results. In particular, we report here the notion of labeled granularity which was
proposed for the specification of a calendar algebra (Bettini et al., 2000; Ning et al., 2002);
we will show later how any labeled granularity can be reduced to a more standard notion of
granularity, like the one used by Bettini et al. (2002a).
Granularities are defined by grouping sets of instants into granules. For example, each
granule of the granularity day specifies the set of instants included in a particular day. A
label is used to refer to a particular granule. The whole set of time instants is called time
domain, and for the purpose of this paper the domain can be an arbitrary infinite set with
a total order relationship, ≤.

302

Mapping Calendar Expressions to Minimal Periodic Sets

Definition 1 A labeled granularity G is a pair (LG , M ), where LG is a subset of the
integers, and M is a mapping from LG to the subsets of the time domain such that for each
pair of integers i and j in LG with i < j, if M (i) 6= ∅ and M (j) 6= ∅, then (1) each element
in M (i) is less than every element of M (j), and (2) for each integer k in LG with i < k < j,
M (k) 6= ∅.
The former condition guarantees the “monotonicity” of the granularity; the latter is
used to introduce the bounds (see Section 2.2).
We call LG the label set and for each i ∈ LG we call G(i) a granule; if G(i) 6= ∅ we call
it a non-empty granule. When LG is exactly the integers, the granularity is called “fullinteger labeled”. When LG = Z+ we have the same notion of granularity as used in several
applications, e.g., (Bettini et al., 2002a). For example, following this labeling schema, if
we assume to map day(1) to the subset of the time domain corresponding to January 1,
2001, day(32) would be mapped to February 1, 2001, b-day(6) to January 8, 2001 (the
sixth business day), and month(15) to March 2002. The generalization to arbitrary label
sets has been introduced mainly to facilitate conversion operations in the algebra, however
our final goal is the conversion of a labeled granularity denoted by a calendar expression
into a “positive-integer labeled” one denoted by a periodic formula.
2.2 Granularity Relationships
Some interesting relationships between granularities follows. The definitions are extended
from the ones presented by Bettini et al. (2000) to cover the notion of labeled granularity.
Definition 2 If G and H are labeled granularities, then G is said to group into H, denoted
G / H, if for each non-empty
granule H(j), there exists a (possibly infinite) set S of labels
S
of G such that H(j) = i∈S G(i).
Intuitively, G / H means that each granule of H is a union of some granules of G. For
example, day / week since a week is composed of 7 days and day / b-day since each business
day is a day.
Definition 3 If G and H are labeled granularities, then G is said to be finer than H,
denoted G  H, if for each granule G(i), there exists a granule H(j) such that G(i) ⊆ H(j).
For example business-day is finer than day, and also finer than week.
We also say that G partitions H if G / H and G  H. Intuitively G partitions H if
G / H and there are no granules of G other than those included in granules of H. For
example, both day and b-day group into b-week (business week, i.e., the business day in a
week), but day does not partition b-week, while b-day does.
Definition 4 A labeled granularity G1 is a label-aligned subgranularity of a labeled granularity G2 if the label set LG1 of G1 is a subset of the label set LG2 of G2 and for each i in
LG1 such that G1 (i) 6= ∅, we have G1 (i) = G2 (i).
Intuitively, G1 has a subset of the granules of G2 and those granules have the same label in
the two granularities.
303

Bettini, Mascetti & Wang

Granularities are said to be bounded when LG has a first or last element or when G(i) = ∅
for some i ∈ LG . We assume the existence of an unbounded bottom granularity, denoted
by ⊥ which is full-integer labeled and groups into every other granularity in the system.
There are time domains such that, given any set of granularities, it is always possible
to find a bottom one; for example, it can be easily proved that this property holds for each
time domain that has the same cardinality as the integers. On the other hand, the same
property does not hold for other time domains (e.g. the reals). However, the assumption
about the existence of the bottom granularity is still reasonable since we address problems
in which granularities are defined starting from a bottom one. The definition of a calendar
as a set of granularities that have the same bottom granularity (Bettini et al., 2000) captures
this idea.
2.3 Granularity Conversions
When dealing with granularities, we often need to determine the granule (if any) of a
granularity H that covers a given granule z of another granularity G. For example, we
may wish to find the month (an interval of the absolute time) that includes a given week
(another interval of the absolute time).
This transformation is obtained with the up operation. Formally, for each label z ∈ LG ,
H
0
0
dzeG is undefined if @z 0 ∈ LH s.t. G(z) ⊆ H(z 0 ) ; otherwise, dzeH
G = z , where z is the
0
0
unique index value such that G(z) ⊆ H(z ). The uniqueness of z is guaranteed by the
monotonicity 1 of granularities. As an example, dzemonth
second gives the month that includes
month
the second z. Note that while dzesecond is always defined, dzemonth
week is undefined if week
z falls between two months. Note that if G  H, then the function dzeH
G is defined for
week
each index value z. For example, since day  week, dzeday is always defined, i.e., for
each day we can find the week that contains it. The notation dzeH is used when the source
granularity can be left implicit (e.g., when we are dealing with a fixed set of granularities
having a distinguished bottom granularity).
Another direction of the above transformation is the down operation: Let G and H
H
be granularities such that
S G / H, and z an2 integer. Define bzcG as the set S of labels of
granules of G such that j∈S G(j) = H(z). This function is useful for finding, e.g., all the
days in a month.
2.4 The Periodical Granules Representation
A central issue in temporal reasoning is the possibility of finitely representing infinite granularities. The definition of granularity provided above is general and expressive but it may
be impossible to provide a finite representation of some of the granularities. Even labels
(i.e., a subset of the integers) do not necessarily have a finite representation.
A solution has been first proposed by Bettini et al. (2000). The idea is that most of the
commonly used granularities present a periodical behavior; it means that there is a certain
pattern that repeats periodically. This feature has been exploited to provide a method for
1. Condition (1) of Definition 1.
2. This definition is different from the one given by Bettini et al (2000) since it also considers non contiguous
granules of G.

304

Mapping Calendar Expressions to Minimal Periodic Sets

finitely describing granularities. The formal definition is based on the periodically groups
into relationship.
Definition 5 A labeled granularity G groups periodically into a labeled granularity H
(G /¯ H) if G / H and there exist positive integers N and P such that
(1) for each label i of H, i + N is a label of H unless i + N is greater than the greatest
label of H, and
S
(2) for each label i of H, if H(i) = kr=0 G(jr ) and H(i + N ) is a non-empty granule of
S
H then H(i + N ) = kr=0 G(jr + P ), and
(3) if H(s) is the first non-empty granule in H (if exists), then H(s + N ) is non-empty.
The groups periodically into relationship is a special case of the group into characterized
by a periodic repetition of the “grouping pattern” of granules of G into granules of H. Its
definition may appear complicated but it is actually quite simple. Since G groups into H,
any granule H(i) is the union of some granules of G; for instance assume it is the union of
the granules G(a1 ), G(a2 ), . . . , G(ak ). Condition (1) ensures that the label i + N exists (if
it not greater than the greatest label of H) while condition (2) ensures that, if H(i + N ) is
not empty, then it is the union of G(a1 + P ), G(a2 + P ), . . . , G(ak + P ). We assume that
∀r = 0 . . . k, (jr + P ) ∈ LG ; if not, the conditions are considered not satisfied. Condition
(3) simply says that there is at least one of these repetitions.
We call each pair P and N in Definition 5, a period length and its associated period
label distance. We also indicate with R the number of granules of H corresponding to each
groups of P consecutive granules of ⊥. More formally R is equal to the number of labels of
H greater or equal than i and smaller than i + N where i is an arbitrary label of H. Note
that R is not affected by the value of i.
The period length and the period label distance are not unique; more precisely, we
G the period label
indicate with PHG the period length of H in terms of G and with NH
distance of H in terms of G; the form PH and NH is used when G = ⊥. Note that the
period length is an integer value. For simplicity we also indicate with one period of a
granularity H a set of R consecutive granules of H.
In general, the periodically groups into relationship guarantees that granularity H can
be finitely described (in terms of granules of G).
Definition 6 If G /¯ H, then H can be finitely described by providing: (i) a value for P
P
and N ; (ii) the set LP of labels of H in
S one period of H; (iii) for each a ∈ L , the finite set
Sa of labels of G, such that H(a) = i∈Sa G(i); (iv) the labels of first and last non-empty
granules in H, if their values are not infinite.
In this representation, the granules that have labels in LP are the only ones that need
to be explicitly represented; we call these granules the explicit granules.
If a granularity H can be represented as a periodic set of granules of a granularity G,
G ) for which the periodically groups
then there exists an infinite number of pairs (PHG , NH
into relation is satisfied. If the relation is satisfied for a pair (P, N ), then it can be proved
that it can also be satisfied for each pair (αP, αN ) with α ∈ N+ .

305

Bettini, Mascetti & Wang

Definition 7 A periodic representation of a granularity H in terms of G is called minimal
if the period length P used in the representation has the smallest value among the period
G ) for which H periodically groups into G.
lengths appearing in all the pairs (PHG , NH
If H is fully characterized in terms of G, it is possible to derive the composition, in
terms of G, of any granule of H. Indeed, if LP is the set of labels of H with values in
G − 1}, and we assume H to be unbounded, the description of an arbitrary
{b, . . . , b + NH
G] + 1
granule H(j) can be obtained by the following formula. Given j 0 = [(j − 1) mod NH
and
k
k
j
 j
b−1
b−1
G + j0
G + j0 ≥ b

·
N
· NH
if

G
G
H

NH
NH
k=
k

j


b−1

+ 1 · N G + j 0 otherwise
G
NH

H

we have
H(j) =

[


G

PHG

i∈Sk




k−1
j−1
G
+ i − PH ·
.
·
G
G
NH
NH


Example 1 Figure 1 shows granularities day and week parts i.e., the granularity that,
for each week, contains a granule for the working days and a granule for the weekend. For
the sake of simplicity, we denote day and week parts with D and W respectively. Since
D /¯ W , W is fully characterized in terms of D. Among different possible representations,
D = 7, N D = 2, LP = {3, 4},
in this example we decide to represent W in terms of D by PW
W
W
S3 = {8, 9, 10, 11, 12} and S4 = {13, 14}. The composition of each granule of W can then
be easily computed; For example the composition of W (6) is given by the formula presented
above with j 0 = 2 and k = 4. Hence W (6) = D(7 · 2 + 13 − 7 · 1) ∪ D(7 · 2 + 14 − 7 · 1) =
D(20) ∪ D(21).
D
-1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
j

b

k

2

3

4

j

W
1

5

6

7

8

Figure 1: Periodically groups into example

3. Calendar Algebra
Several high-level symbolic formalisms have been proposed to represent granularities (Leban
et al., 1986; Niezette & Stevenne, 1992).
In this work we consider the formalism proposed by Ning et al. (2002) called Calendar
Algebra. In this approach a set of algebraic operations is defined; each operation generates
a new granularity by manipulating other granularities that have already been generated.
The relationships between the operands and the resulting granularities are thus encoded in
the operations. All granularities that are generated directly or indirectly from the bottom
granularity form a calendar, and these granularities are related to each other through the
306

Mapping Calendar Expressions to Minimal Periodic Sets

operations that define them. In practice, the choices for the bottom granularity include day,
hour, second, microsecond and other granularities, depending on the accuracy required in
each application context.
In the following we illustrate the calendar algebra operations presented by Ning et al.
(2002) together with some restrictions introduced by Bettini et al. (2004).
3.1 The Grouping-Oriented Operations
The calendar algebra consists of the following two kinds of operations: the grouping-oriented
operations and the granule-oriented operations. The grouping-oriented operations group
certain granules of a granularity together to form new granules in a new granularity.
3.1.1 The Grouping Operation
Let G be a full-integer labeled granularity, and m a positive integer. The grouping operation
Groupm (G) generates a new granularity G0 by partitioning the granules of G into m-granule
groups and making each group a granule of the resulting granularity. More precisely, G0 =
Groupm (G) is the granularity such that for each integer i,
i·m
[

G0 (i) =

G(j).

j=(i−1)·m+1

For example, given granularity day, granularity week can be generated by the calendar
algebra expression week = Group7 (day) if we assume that day(1) corresponds to Monday,
i.e., the first day of a week.
3.1.2 The Altering-tick Operation
Let G1 , G2 be full-integer labeled granularities, and l, k, m integers, where G2 partitions
m (G , G ) generates a new granularity
G1 , and 1 ≤ l ≤ m. The altering-tick operation Alterl,k
2
1
by periodically expanding or shrinking granules of G1 in terms of granules of G2 . Since G2
partitions G1 , each granule of G1 consists of some contiguous granules of G2 . The granules
of G1 can be partitioned into m-granule groups such that G1 (1) to G1 (m) are in one group,
G1 (m + 1) to G1 (2m) are in the following group, and so on. The goal of the altering-tick
operation is to modify the granules of G1 so that the l-th granule of every m-granule group
will have |k| additional (or fewer when k < 0) granules of G2 . For example, if G1 represents
30-day groups (i.e., G1 = Group30 (day)) and we want to add a day to every 3-rd month
(i.e., to make March to have 31 days), we may perform Alter12
3,1 (day, G1 ).
The altering-tick operation can be formally described as follows. For each integer i such
i
that G1 (i) 6= ∅, let bi and ti be the integers such that G1 (i) = ∪tj=b
G2 (j) (the integers bi
i
m
0
and ti exist because G2 partitions G1 ). Then G = Alterl,k (G2 , G1 ) is the granularity such
that for each integer i, let G0 (i) = ∅ if G1 (i) = ∅, and otherwise let
0

0

G (i) =

ti
[
j=b0i

307

G2 (j),

Bettini, Mascetti & Wang

where
b0i


=

bi + (h − 1) · k, if i = (h − 1) · m + l,
bi + h · k,
otherwise,
t0i = ti + h · k,

and


i−l
+ 1.
h=
m


Example 2 Figure 2 shows an example of the Alter operation. Granularity G1 is defined
by G1 = Group5 (G2 ) and granularity G0 is defined by G0 = Alter22,−1 (G2 , G1 ), which means
shrinking the second one of every two granules of G1 by one granule of G2 .
G2
-9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

G1
-1

0

1

2

3

4

G
-

1

0

1

2

3

4

Figure 2: Altering-tick operation example
The original definition of altering-tick given by Ning et al. (2002) as reported above,
has the following problems when an arbitrary negative value for k is used: (1) It allows
the definition of a G0 that is not a full-integer labeled granularity and (2) It allows the
definition of a G0 that does not even satisfy the definition of granularity. In order to avoid
this undesired behavior, we impose the following restriction:
k > −(mindist(G1, 2, G2) − 1)
where mindist() is formally defined by Bettini et al. (2000).
Intuitively, mindist(G1, 2, G2) represents the minimum distance (in terms of granules
of G2) between two consecutive granules of G1.
3.1.3 The Shift Operation
Let G be a full-integer labeled granularity, and m an integer. The shifting operation
Shiftm (G) generates a new granularity G0 by shifting the labels of G by m positions. More
formally, G0 = Shiftm (G) is the granularity such that for each integer i, G0 (i) = G(i − m).
Note that G0 is also full-integer labeled.
3.1.4 The Combining Operation
Let G1 and G2 be granularities with label sets LG1 and LG2 respectively. The combining
operation Combine(G1 , G2 ) generates a new granularity G0 by combining all the granules
of G2 that are included in one granule of G1 into one granule of G0 . More formally, for each
i ∈ L1 , let s(i) = ∅ if G1 (i) = ∅, and otherwise let s(i) = {j ∈ LG2 |∅ =
6 G2 (j) ⊆ G1 (i)}.

308

Mapping Calendar Expressions to Minimal Periodic Sets

Then G0 = Combine(G1 , G2 ) is the granularity
with the label set LG0 = {i ∈ LG1 |s(i) 6= ∅}
S
such that for each i in LG0 , G0 (i) = j∈s(i) G2 (j).
As an example, given granularities b-day and month, the granularity for business months
can be generated by b-month = Combine(month, b-day).
3.1.5 The Anchored Grouping Operation
Let G1 and G2 be granularities with label sets LG1 and LG2 respectively, where G2 is a
label-aligned subgranularity of G1 , and G1 is a full-integer labeled granularity. The anchored
grouping operation Anchored-group(G1 , G2 ) generates a new granularity G0 by combining
all the granules of G1 that are between two granules of G2 into one granule of G0 . More
formally, G0 = Anchored-group(G1 , G2 ) is the granularity with the label set LG0 = LG2
S 0 −1
G1 (j) where i0 is the next label of G2 after i.
such that for each i ∈ LG0 , G0 (i) = ij=i
For example, each academic year at a certain university begins on the last Monday in
August, and ends on the day before the beginning of the next academic year. Then, the
granularity corresponding to the academic years can be generated by AcademicY ear =
Anchored-group(day, lastMondayOfAugust).
3.2 The Granule-Oriented Operations
Differently from the grouping-oriented operations, the granule-oriented operations do not
modify the granules of a granularity, but rather enable the selection of the granules that
should remain in the new granularity.
3.2.1 The Subset Operation
Let G be a granularity with label set LG , and m, n integers such that m ≤ n. The subset
operation G0 = Subsetnm (G) generates a new granularity G0 by taking all the granules of
G whose labels are between m and n. More formally, G0 = Subsetnm (G) is the granularity
with the label set LG0 = {i ∈ LG | m ≤ i ≤ n}, and for each i ∈ LG0 , G0 (i) = G(i).
For example, given granularity year, all the years in the 20th century can be generated by
0
20CenturyYear = Subset1999
1900 (year). Note that G is a label-aligned subgranularity of G,
and G0 is not a full-integer labeled granularity even if G is. We also allow the extensions of
setting m = −∞ or n = ∞ with semantics properly extended.
3.2.2 The Selecting Operations
The selecting operations are all binary operations. They generate new granularities by
selecting granules from the first operand in terms of their relationship with the granules of
the second operand. The result is always a label-aligned subgranularity of the first operand
granularity.
There are three selecting operations: select-down, select-up and select-by-intersect. To
facilitate the description of these operations, the ∆lk (S) notation is used. Intuitively, if
S is a set of integers, ∆lk (S) selects l elements starting from the k-th one (for a formal
description of the ∆ operator see (Ning et al., 2002)).
Select-down operation. For each granule G2 (i), there exits a set of granules of G1 that
is contained in G2 (i). The operation Select-downlk (G1 , G2 ), where k 6= 0 and l > 0 are
309

Bettini, Mascetti & Wang

integers, selects granules of G1 by using ∆lk (·) on each set of granules (actually their labels)
of G1 that are contained in one granule of G2 . More formally, G0 = Select-downlk (G1 , G2 )
is the granularity with the label set
LG0 = ∪i∈LG2 ∆lk ({j ∈ LG1 | ∅ 6= G1 (j) ⊆ G2 (i)}),
and for each i ∈ LG0 , G0 (i) = G1 (i). For example, Thanksgiving days are the fourth
Thursdays of all Novembers; if Thursday and November are given, it can be generated by
Thanksgiving = Select-down14 (Thursday, November).
Select-up operation. The select-up operation Select-up(G1 , G2 ) generates a new granularity
G0 by selecting the granules of G1 that contain one or more granules of G2 . More formally,
G0 = Select-up(G1 , G2 ) is the granularity with the label set
6 G2 (j) ⊆ G1 (i)), }
LG0 = {i ∈ LG1 |∃j ∈ LG2 (∅ =
and for each i ∈ LG0 , G0 (i) = G1 (i). For example, given granularities Thanksgiving
and week, the weeks that contain Thanksgiving days can be defined by ThanxWeek =
Select-up(week, Thanksgiving).
Select-by-intersect operation. For each granule G2 (i), there may exist a set of granules of G1 ,
each intersecting G2 (i). The Select-by-intersectlk (G1 , G2 ) operation, where k 6= 0 and l > 0
are integers, selects granules of G1 by applying ∆lk (·) operator to all such sets, generating
a new granularity G0 . More formally, G0 = Select-by-intersectlk (G1 , G2 ) is the granularity
with the label set
LG0 = ∪i∈LG2 ∆lk ({j ∈ LG1 | G1 (j) ∩ G2 (i) 6= ∅}),
and for each i ∈ LG0 , G0 (i) = G1 (i). For example, given granularities week and month, the
granularity consisting of the first week of each month (among all the weeks intersecting the
month) can be generated by FirstWeekOfMonth = Select-by-intersect11 (week, month).
3.2.3 The Set Operations
In order to have the set operations as a part of the calendar algebra and to make certain
computations easier, we restrict the operand granularities participating in the set operations
so that the result of the operation is always a valid granularity: the set operations can be
defined on G1 and G2 only if there exists a granularity H such that G1 and G2 are both
label-aligned subgranularities of H. In the following, we describe the union, intersection,
and difference operations of G1 and G2 , assuming that they satisfy the requirement.
Union. The union operation G1 ∪ G2 generates a new granularity G0 by collecting all the
granules from both G1 and G2 . More formally, G0 = G1 ∪ G2 is the granularity with the
label set LG0 = LG1 ∪ LG2 , and for each i ∈ LG0 ,

G1 (i), i ∈ L1 ,
0
G (i) =
G2 (i), i ∈ L2 − L1 .
For example, given granularities Sunday and Saturday, the granularity of the weekend days
can be generated by WeekendDay = Sunday ∪ Saturday.
310

Mapping Calendar Expressions to Minimal Periodic Sets

Intersection. The intersection operation G1 ∩ G2 generates a new granularity G0 by taking
the common granules from both G1 and G2 . More formally, G0 = G1 ∩ G2 is the granularity
with the label set LG0 = LG1 ∩ LG2 , and for each i ∈ LG0 , G0 (i) = G1 (i) (or equivalently
G2 (i)).
Difference. The difference operation G1 \ G2 generates a new granularity G0 by excluding
the granules of G2 from those of G1 . More formally, G0 = G1 \ G2 is the granularity with
the label set LG0 = LG1 \ LG2 , and for each i ∈ LG0 , G0 (i) = G1 (i).

4. From Calendar Algebra to Periodical Set
In this section we first describe the overall conversion process and then we report the
formulas specific for the conversion of each calendar algebra operation. Finally, we present
a procedure for relabeling the resulting granularity, a sketch complexity analysis and some
considerations about the period length minimality.
4.1 The Conversion Process
Our final goal is to provide a correct and effective way to convert calendar expressions
into periodical representations. Under appropriate limitations, for each calendar algebra
operation, if the periodical descriptions of the operand granularities are known, it is possible
to compute the periodical characterization of the resulting granularity.
This result allows us to calculate, for any calendar, the periodical description of each
granularity in terms of the bottom granularity. In fact, by definition, the bottom granularity is fully characterized; hence it is possible to compute the periodical representation of
all the granularities that are obtained from operations applied to the bottom granularity.
Recursively, the periodical description of all the granularities can be obtained.
The calendar algebra presented in the previous section can represent all the granularities
that are periodical with finite exceptions (i.e., any granularity G such that bottom groups
periodically with finite exceptions into G). Since with the periodical representations defined
in Section 2 it is not possible to express the finite exceptions, we need to restrict the calendar
algebra so that it cannot represent them. This implies allowing the Subset operation to
be only used as the last step of deriving a granularity. Note that in the calendar algebra
presented by Ning et al. (2002) there was an extension to the altering-tick operation to allow
the usage of ∞ as the m parameter (i.e., G0 = Alter∞
l,k (G2 , G1 )); the resulting granularity
has a single exception hence is not periodic. This extension is disallowed here in order to
generate periodical granularities only (without finite exceptions).
The conversion process can be divided into three steps: in the first one the period length
and period label distance are computed; in the second we derive the set LP of labels in one
period, and in the last one the composition of the explicit granules is computed. For each
operation we identify the correct formulas and algorithms for the three steps.
The first step consists in computing the period length and the period label distance of
the resulting granularity. Those values are calculated as a function of the parameters (e.g.
the “grouping factor” m, in the Group operation) and the operand granularities (actually
their period lengths and period label distances).

311

Bettini, Mascetti & Wang

The second step in the conversion process is the identification of the label set of the
resulting granularity. In Section 2.4 we pointed out that in order to fully characterize a
granularity it is sufficient to identify the labels in any period of the granularity. In spite
of this theoretical result, to perform the computations required by each operation we need
the explicit granules of the operand granularities to be “aligned”. There are two possible
approaches: the first one consist in computing the explicit granules in any period and
then recalculate the needed granules in the correct position in order to eventually align
them. The second one consists in aligning all the periods containing the explicit granules
with a fixed granule in the bottom granularity. After considering both possibilities, for
performance reasons, we decided to adopt the second approach. We decided to use ⊥(1) as
the “alignment point” for all the granularities. A formal definition of the used formalism
follows.
Let G be a granularity and i be the smallest positive integer such that dieG is defined.
We call lG = dieG and LG the set of labels of G contained in lG . . . lG +NG −1. Note that this
definition of LG is an instance of the definition of LP given in Section 2.4. The definition
of LG provided here is useful for representing G and actually the final goal of this step is to
compute LG ; however LG is not suitable for performing the computations. The problem is
that if G(lG ) starts before ⊥(1) (i.e., min(blG cG ) < 1) then the granule G(lG + NG ) begins
at PG or before PG , and hence G(lG + NG ) is necessary for the computations; however
lG + NG ∈
/ LG .
To solve the problem we introduce the symbol L̂G to represent the set of all labels of
granules of G that cover one in ⊥(1) . . . ⊥(PG ). It is easily seen that if G(lG ) does not cover
⊥(0), then L̂G = LG , otherwise L̂G = LG ∪ {lG + NG }. Therefore the conversion between
L and L̂ and vice versa is immediate.
The notion of L̂ is still not enough to perform the computations. The problem is that
when a granularity G is used as an operand in an operation, the period length of the
resulting granularity G0 is generally bigger than the period length of G. Therefore it is
necessary to extend the notion of L̂G to the period length PG0 of G0 using PG0 in spite of
P 0
PG in the definition of L̂. The symbol used for this notion is L̂GG .
P 0
The idea is that when G is used as the operand in an operation that generates G0 , L̂GG is
computed from LG . This set is then used by the formula that we provide below to compute
LG0 .
The computation of LG0 is performed as follows: if G0 is defined by an operation that
0 .
returns a full-integer labeled granularity, then it is sufficient to compute the value of lG
0
0
0
Indeed it is easily seen that LG0 = {i ∈ Z|lG ≤ i ≤ lG + NG0 − 1}. If G is defined by
any other algebraic operation, we provide the formulas to compute L̂G0 ; from L̂G0 we easily
derive LG0 .
Example 3 Figure 3 shows granularities ⊥, G and H; it is clear that PG = PH = 4 and
NG = NH = 3. Moreover, lG = lH = 6 and therefore LG = LH = {6, 7}. Since 0 ∈
/ b6cG
H
then L̂G = LG . On the other hand, since 0 ∈ b6c , then L̂H = LH ∪ {6 + 3}.
P0
Suppose that a granularity G0 has period length PG0 = 8; then L̂GG = {6, 7, 9, 10} and
P 0
L̂HG = {6, 7, 9, 10, 12}.

312

Mapping Calendar Expressions to Minimal Periodic Sets

^

-6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10

G
1

3

4

6

7

9

10

12

H
1

3

4

6

7

9

10

12 13

Figure 3: L, l, L̂ and L̂PG0 examples
The third (and last) step of the conversion process is the computation of the composition of the explicit granules. Once LG0 has been computed, it is sufficient to apply, for each
label of LG0 the formulas presented in Chapter 3.
In Sections 4.3 to 4.10 we show, for each calendar algebra operation, how to compute
the first and second conversion steps.
4.2 Computability Issues
In some of the formulas presented below it is necessary to compute the set S of labels of a
granularity G such that ∀i ∈ S G(i) ⊆ H(j) where H is a granularity and j is a specific label
of H. Since LG contains an infinite number of labels, it is not possible to check, ∀i ∈ LG
if G(i) ⊆ H(j). However it is easily seen that ∀i ∈ S ∃k s.t. G(dkeG ) ⊆ H(j). Therefore
∀i ∈ S ∃k s.t. G(dkeG ) is defined and k ∈ bjcH .
Therefore we compute the set S by considering all the labels i of LG s.t. ∃n ∈ bjcH s.t.
dneG = i and G(i) ⊆ H(j). Since the set bjcH is finite3 , the computation can be performed
in a finite time. The consideration is analogous if S is the set such that ∀i ∈ S G(i) ⊇ H(j)
or ∀i ∈ S (G(i) ∩ H(j) 6= ∅).
4.3 The Group Operation
Proposition 1 If G0 = Groupm (G), then:
PG ·m
1. PG0 = GCD(m,N
and NG0 =
G)
j
k

lG −1
+
1
;
2. lG0 =
m

3. ∀i ∈ LG0 G0 (i) =

NG
GCD(m,NG ) ;

Si·m

j=(i−1)·m+1 G(j).

Example 4 Figure 4 shows an example of the group operation: G0 = Group3 (G). Since
PG = 1 and NG = 1, then PG0 = 3 and NG = 1. Moreover, since LG = {−7}, then lG = −7
and therefore lG0 = −2 and LG0 = {−2}. Finally G0 (−2) = G(−8) ∪ G(−7) ∪ G(−6) i.e.,
G0 (−2) = ⊥(0) ∪ ⊥(1) ∪ ⊥(2).
3. With the calendar algebra it is not possible to define granularities having granules that maps to an
infinite set of time instants.

313

Bettini, Mascetti & Wang

^

-6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

G
-14-13-12-11-10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12

G
-4

-3

-2

-1

0

1

2

3

4

Figure 4: Group operation example
4.4 The Altering-tick Operation
Proposition 2 If G0 = Alterm
l,k (G2 , G1 ) then:
1.


N

G0

PG2 · NG1
NG2 · m
= lcm NG1 , m,
,
GCD(PG2 · NG1 , PG1 ) GCD(NG2 · m, |k|)

and


PG0 =

NG0 · k
NG0 · PG1 · NG2
+
NG1 · PG2
m


·



PG2
NG2

0

2. lG0 = dlG2 eG
G2 ;
3. ∀i ∈ LG0 G0 (i) =

St0i

j=b0i

G(j) where b0i and t0i are defined in Section 3.1.2.

Referring to step 2., note that when computing lG0 the explicit characterization of the
0
granules of G0 is still unknown. To perform the operation dlG2 eG
G2 we need to know at least
the explicit granules of one of its periods. We choose to compute the granules labeled by
1 . . . NG0 . When lG0 is derived, the granules labeled by lG0 . . . lG0 + NG0 − 1 will be computed
so that the explicit granules are aligned to ⊥(1) as required.
Example 5 Figure 5 shows an example of the altering-tick operation: G0 = Alter32,1 (G2 , G1 ).
Since PG1 = 4, NG1 = 1, PG2 = 4 and NG2 = 2, then NG0 = 6 and PG0 = 28.
G0 = −4
Moreover, since LG2 = {−10, −9}, then lG2 = −10 and therefore lG0 = d−10eG
2
and hence LG2 = {−4, −3, . . . , 0, 1}. Finally G0 (−4) = G1 (−11) ∪ G1 (−10) ∪ G1 (−9) =
⊥(−1) ∪ ⊥(0) ∪ ⊥(1) ∪ ⊥(3) ∪ ⊥(4); analogously we derive G0 (−3), G0 (−2), G0 (−1), G0 (0)
and G0 (1).

4.5 The Shift Operation
Proposition 3 If G0 = Shiftm (G), then:
1. PG0 = PG1 and NG0 = NG1 ;
2. lG0 = lG + m;
3. ∀i ∈ LG0 G0 (i) = G(i − m).
314

Mapping Calendar Expressions to Minimal Periodic Sets

^

-3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41

G1
-5

-4

-3

-2

-1

0

1

2

3

4

5

G2
-11 -10

-9

-8

-7

-6

-5

-4

-3

-2

-1

0

1

2

3

4

5

6

7

8

9

10

G
-4

-3

-2

-1

0

1

2

3

4

Figure 5: Alter operation example
Example 6 The shifting operation can easily model time differences. Suppose granularity
USEast-Hour stands for the hours of US Eastern Time. Since the hours of the US Pacific
Time are 3 hours later than those of US Eastern Time, the hours of US Pacific Time can
be generated by USPacific-Hour= Shift−3 (USEast-Hour).
USEast-Hour
-6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8

USPacific-Hour
-9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5

Figure 6: Shift operation example

4.6 The Combining Operation
Proposition 4 Given G0 = Combining(G1 , G2 ), then:
1. PG0 = lcm(PG1 , PG2 ) and NG0 =
P

0

P

lcm(PG1 ,PG2 )NG1
;
PG1
P

0

0

2. ∀i ∈ L̂GG1 let be se(i) = {j ∈ L̂GG2 |∅ =
6 G2 (j) ⊆ G1 (i)}; then L̂G0 = {i ∈ L̂GG1 |e
s(i) 6= ∅};
S
3. ∀i ∈ LG0 G0 (i) = j∈s(i) G2 (j).
Example 7 Figure 7 shows an example of the combining operation: G0 = Combine(G1 , G2 ).
Since PG1 = 6, NG1 = 2, PG2 = 4 and NG2 = 2, then PG0 = 12 and NG0 = 4. Moreover,
P 0
since LG1 = {1} and 0 ∈ b1cG1 , then L̂G1 = {1, 3} and hence L̂GG1 = {1, 3, 5}. Since
0
s̃(i) 6= ∅ for i ∈ {1, 3, 5}, then L̂G0 = {1, 3, 5}; moreover, since 0 ∈ b1cG , then LG0 = {1, 3}.
Finally s(1) = {−1, 0} and s(3) = {2, 3}; consequently, G0 (1) = G2 (−1) ∪ G2 (0) i.e.,
G0 (1) = ⊥(−1) ∪ ⊥(0) ∪ ⊥(1) and G0 (3) = G2 (2) ∪ G2 (3) i.e., G0 (3) = ⊥(4) ∪ ⊥(5) ∪ ⊥(7).

4.7 The Anchored Grouping Operation
Proposition 5 Given G0 = Anchored-group(G1 , G2 ), then:
1. PG0 = lcm(PG1 , PG2 ) and NG0 =

lcm(PG1 ,PG2 )·NG2
;
PG2

315

Bettini, Mascetti & Wang

^

-6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

G1
1

3

5

7

G2
-3

-2

-1

0

1

2

3

4

5

6

7

8

9

G
1

3

5

7

Figure 7: Combine operation example
2.
(
L̂G0 =

P

0

L̂GG2 ,
if lG2 = lG1 ,
PG0
0
{lG2 } ∪ L̂G2 , otherwise,

0
where lG
is the greatest among the labels of LG2 that are smaller than lG2 .
2

3. ∀i ∈ LG0 G0 (i) =

Si0 −1
j=i

G1 (j) where i0 is the next label of G2 after i.

Example 8 Figure 8 shows an example of the anchored grouping operation: the USweek
(i.e., a week starting with a Sunday) is defined by the operation Anchored-group(day,
Sunday). Since Pday = 1 and PSunday = 7, then the period length of USweek is 7. MorePUSweek
over since lday = 11, lSunday = 14 and L̂Sunday
= {14}, then L̂USweek = {7} ∪ {14}.
S
Clearly, since 0 ∈ b7cUSweek then LUSweek = {7}. Finally, USweek(7) = 13
j=7 day(j) =
S3
k=−3 ⊥(k).

^

-18-17-16-15-14-13-12-11-10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12

day
-8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22

Sunday
-7

0

7

14

21

USweek
-7

0

7

14

Figure 8: Anchored Grouping operation example

4.8 The Subset Operation
The Subset operation only modifies the operand granularity by introducing the bounds.
The period length, the period label distance, L and the composition of the explicit granules
are not affected.

316

Mapping Calendar Expressions to Minimal Periodic Sets

4.9 The Selecting Operations
4.9.1 The Select-down Operation
Proposition 6 Given G0 = Select-downlk (G1 , G2 ), then:
1. PG0 = lcm(PG1 , PG2 ) and NG0 =
2. ∀i ∈ LG2 let

lcm(PG1 ,PG2 )·NG1
;
PG1

6 G1 (j) ⊆ G2 (i)}) .
A(i) = ∆lk ({j ∈ LG1 |∅ =

Then

o
[ n
P 0
a ∈ A(i)|a ∈ L̂GG1 ;

L̂G0 =

P 0

i∈L̂GG
2

3. ∀i ∈ LG0 G0 (i) = G1 (i).
Example 9 Figure 9 shows an example of the Select-down operation in which granularity
G0 is defined as: G0 = Select-down12 (G1 , G2 ). Since PG1 = 4, NG1 = 2 and PG2 = 6
then PG0 = 12 and NG0 = 6. Moreover, since LG2 = {−3} and 0 ∈ b−3cG2 , then L̂G2 =
P 0
{−3, −2} and L̂GG2 = {−3, −2, −1}. Intuitively, A(−3) = {−5}, A(−2) = {−2} and
0
A(−1) = {1}. Hence L̂G0 = {−5, −2, 1} and therefore, since 0 ∈ b−5cG , LG0 = {−5, −2}.
Finally G0 (−5) = G1 (−5) = ⊥(0) ∪ ⊥(1) and G0 (−2) = G1 (−2) = ⊥(6).
^

-9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

G1
-9

-8

-7

-6

-5

-4

-3

-2

-1

0

1

2

3

4

5

6

7

G2
-4

-3

-2

-1

0

1

G
-8

-5

-2

1

4

Figure 9: Select-down operation example

4.9.2 The Select-up Operation
Proposition 7 Given G0 = Select-up(G1 , G2 ), then:
1. PG0 = lcm(PG1 , PG2 ) and NG0 =

lcm(PG1 ,PG2 )·NG1
;
PG1

2.
P

0

L̂G0 = {i ∈ L̂GG1 |∃j ∈ LG2 s.t. ∅ =
6 G2 (j) ⊆ G1 (i)};
3. ∀i ∈ LG0 G0 (i) = G1 (i).

317

7

Bettini, Mascetti & Wang

Example 10 Figure 10 shows an example of the Select-up operation: G0 = Select-up(G1 , G2 ).
Since PG1 = 6, NG1 = 3 and PG2 = 4 then PG0 = 12 and NG0 = 6. Moreover, since LG1 =
P0

{−3, −2, −1} and 0 ∈ b−3cG2 , then L̂G1 = {−3, −2, −1, 0} and L̂GG1 = {−3, −2, −1, 0, 1, 2, 3}.
Since G1 (−3) ⊇ G2 (−6), G1 (−1) ⊇ G2 (−4) and G1 (3) ⊇ G2 (0) then L̂G0 = {−3, −1, 3}
0
and, since 0 ∈ b−3cG , then LG0 = {−3, 1} Finally G0 (−3) = G1 (−3) = ⊥(0) ∪ ⊥(1) and
G0 (−1) = G1 (−1) = ⊥(4).
^

-9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

G1
-8 -7

-6

-5 -4

-3

-2 -1

0

1 2

3

4 5

G2
-10

-8

-6

-4

-2

0

2

G
-7

-3

-1

3

5

Figure 10: Select-up operation example

4.9.3 The Select-by-intersect Operation
Proposition 8 Given G0 = Select-by-intersectlk (G1 , G2 ), then:
1. PG0 = lcm(PG1 , PG2 ) and NG0 =

lcm(PG1 ,PG2 )NG1
;
PG1

2. then ∀i ∈ LG2 let
A(i) = ∆lk ({j ∈ LG1 |G1 (j) ∩ G2 (i) 6= ∅}) .
then
L̂G0 =

o
[ n
P 0
a ∈ A(i)|a ∈ L̂GG1 .
P 0

i∈L̂GG
2

3. ∀i ∈ LG0 G0 (i) = G1 (i).
Example 11 Figure 11 shows an example of the Select-by-intersect operation in which
G0 = Select-by-intersect12 (G1 , G2 ). Since PG1 = 4, NG1 = 2 and PG2 = 6 then PG0 = 12
and NG0 = 6. Moreover, since LG2 = {−3} and 0 ∈ b−3cG2 , then L̂G2 = {−3, −2} and
P 0
L̂GG2 = {−3, −2, −1}. Intuitively, A(−3) = {−6}, A(−2) = {−2} and A(−1) = {0}. Hence
0
L̂G0 = {−2, 0} and therefore, since 0 ∈
/ b−5cG , then LG0 = {−2, 0}. Finally G0 (−2) =
G1 (−2) = ⊥(6) and G0 (0) = G1 (0) = ⊥(10).

318

Mapping Calendar Expressions to Minimal Periodic Sets

^

-9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

G1
-9

-8

-7

-6

-5

-4

-3

-2

-1

0

1

2

3

4

5

6

7

G2
-4

-3

-2

-1

0

1

G
-8

-6

-2

0

4

6

Figure 11: Select-by-intersect operation example
4.10 The Set Operations
Since a set operation is valid if the granularities used as argument are both labeled aligned
granularity of another granularity, the following property is used.
Proposition 9 If G is a labeled aligned subgranularity of H, then

NG
PG

=

NH
PH .

Proposition 10 Given G0 = G1 ∪ G2 , G00 = G1 ∩ G2 and G000 = G1 \ G2 , then:
1. PG0 = PG00 = PG000 = lcm(PG1 , PG2 ) and
lcm(PG1 ,PG2 )NG1
=
NG0 = NG00 = NG000 =
PG
1

P

0

P

0

P

00

P

lcm(PG1 ,PG2 )NG2
;
PG2

00

P

000

P

000

2. L̂G0 = L̂GG1 ∪ L̂GG2 ; L̂G00 = L̂GG1 ∩ L̂GG2 ; L̂G000 = L̂GG1 \ L̂GG2 ;

G1 (i), i ∈ LG1
0
3. ∀i ∈ LG0 G (i) =
G2 (i), otherwise,
∀i ∈ LG00 G00 (i) = G1 (i) and ∀i ∈ LG000 G000 (i) = G1 (i)
Example 12 Figure 12 shows an example of the set operations. Note that both G1 and
G2 are labeled aligned subgranularities of H. Then G0 = G1 ∪ G2 , G00 = G1 ∩ G2 and
G000 = G1 \ G2 . Since PG1 = PG2 = 6 and NG1 = NG2 = 6 then PG0 = PG00 = PG000 = 6
and NG0 = NG00 = NG000 = 2. Moreover, since L̂G1 = {1, 2} and L̂G2 = {2, 3}, then
L̂G0 = {1, 2, 3}, L̂G00 = {2} and L̂G000 = {1}. Finally G0 (1) = G1 (1), G0 (2) = G1 (2) and
G0 (3) = G2 (3); G00 (2) = G1 (2) and G000 (1) = G1 (1).

4.11 Relabeling
Granularity processing algorithms are much simpler if restricted to operate on full-integer
labeled granularities. Moreover, a further simplification is obtained by using only the positive integers as the set of labels (i.e., L = Z+ ).
In this section we show how to relabel a granularity G to obtain a full-integer labeled
granularity G0 . A granularity G00 such that LG00 = Z+ can be obtained by using G00 =
0
Subset∞
1 (G )
Note that with the relabeling process some information is lost: for example, if G is
a labeled aligned subgranularity of H and G 6= H, then, after the relabeling, G is not a
319

Bettini, Mascetti & Wang

^

-9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25

H
-3

-2

-1

-2

-1

0

1

2

1

2

3

4

5

4

5

6

7

8

7

8

9

10

G1
10

G2
-3

-1

0

-1

0

2

3

2

3

5

6

5

6

8

9

8

9

G
-3

-2

1

4

7

10

G
-1

2

5

8

G
-2

1

4

7

10

Figure 12: Set operations example
labeled aligned subgranularity of H. The lost information is semantically meaningful in the
calendar algebra, and therefore the relabeling must be performed only when the granularity
will not be used as an operator in an algebraic operation.
Let G be a labeled granularity, i and j integers with i ∈ LG s.t. G(i) 6= ∅. The
relabeling operation Relabelji (G) generates a full-integer labeled granularity G0 by relabeling
G(i) as G0 (j) and relabel the next (and previous) granule of G by the next (and previous,
respectively) integer. More formally, for each integer k, if k = j, then let G0 (k) = G(i),
and otherwise let G0 (k) = G(i0 ) where G(i0 ) is the |j − k|-th granule of G after (before,
respectively) G(i). If the required |j − k|-th granule of G does not exist, then let G0 (k) = ∅.
Note the G0 is always a full-integer labeled granularity.
The relabeling procedure can be implemented in the periodic representation we adopted
by computing the value of lG0 . It is easily seen that once lG0 is known, the full characterization of G0 can be obtained with: PG0 = PG ; NG0 = RG0 = RG and LG0 =
{lG0 , lG0 + 1, . . . , lG0 + NG0 − 2, lG0 + NG0 − 1}. It is clear that the explicit representation of
the granules is not modified.
j
k
i−lG
0
0
To compute lG consider the label i = i − NG · NG ; i0 represents the label of LG such
that i − i0 is a multiple of NG . jTherefore
it is clear that the label j 0 ∈ LG0 s.t. G0 (j 0 ) = G(i0 )
k
G
can be computed by j 0 = j − i−l
· NG0 . Finally lG0 is obtained with lG0 = j 0 − |δ| where
NG
δ is the distance, in terms of number of granules of G, from G(lG ) to G(i0 ).
4
Example 13 Figure 13 shows an example of the Relabel operation: G0 = Relabel
 33 (G).
33−6
0
Since PG = 4and R
i = 33 −
·5 = 8
5
 G = 2 then PG0 = 4 and NG0 0 = 2. Moreover,
0 ) is the next granule of G
and j 0 = 4 − 33−6
·
2
=
−6.
Since
l
=
6
and
i
=
8
then
G(i
G
5
after G(lG ). Then δ = 1 and hence lG0 = −6 − 1 = −7. It follows that LG0 = {−7, −6}.
Finally G0 (−7) = G(6) and G0 (−6) = G(8).

The GSTP constraint solver imposes that the first non-empty granule of any granularity
(⊥ included) is labeled with 1. Therefore, when using the relabeling operation for producing
320

Mapping Calendar Expressions to Minimal Periodic Sets

Figure 13: Relabeling example
granularities for GSTP, the parameter j must be set to 1. The parameter i has to be equal
to the smallest label among those that identify granules of G covering granules of ⊥ that
are all labeled with positive values. By definition of lG , i = lG if min(blG cG ) > 0; otherwise
i is the next label of G after lG .
4.12 Complexity Issues
For each operation the time necessary to perform the three conversion steps, depends on
the operation parameters (e.g. the “grouping factor” m, in the Group operation) and on
the operand granularities (in particular the period length, the period label distance and the
number of granules in one period).
A central issue is that if an operand granularity is not the bottom granularity, then its
period is a function of the periods of the granularities that are the operands in the operation
that defines it. For most of the algebraic operations, in the worst case the period of the
resulting granularity is the product of the periods of the operands granularity.
For all operations, the first step in the conversion process can be performed in a
constant or logarithmic time. Indeed the formulas necessary to derive the period length and
the period label distance involve (i) standard arithmetic operations, (ii) the computation of
the Greatest Common Divisor and (iii) the computation of the least common multiple. Part
(i) can be computed in a constant time while (ii) and (iii) can be computed in a logarithmic
time using Euclid’s algorithm.
For some operations, the second step can be performed in constant time (e.g. Group,
Shift or Anchored-group) or in linear time (e.g. set operations). For the other operations it
is necessary to compute the set S of labels of a granularity G such that ∀i ∈ S G(i) ⊆ H(j)
where H is a granularity and j ∈ LH (analogously if S is the set such that ∀i ∈ S G(i) ⊇
H(j) or ∀i ∈ S (G(i) ∩ H(j) 6= ∅)). This computation needs to be performed once for each
P 0
granule i ∈ PHG . The idea of the algorithm for solving the problem has been presented
in Section 4.2. Several optimizations can be applied to that algorithm, but in the worst
case (when H covers the entire time domain) it is necessary to perform a number of d·eG
operations linear in the period length of the resulting granularity. If an optimized data
structure is used to represent the granularities, the d·eG operation can be performed in
constant time 4 , then the time necessary to perform the second step is linear in the period
length of the resulting granularity (O(PG0 )).
The last step in the conversion process is performed in linear time with respect to the
number of granules in a period of G0 .
4. If a non-optimized data structure is used, d·eG requires logarithmic time.

321

Bettini, Mascetti & Wang

The complexity analysis of the conversion of a general algebraic expression needs to
consider the composition of the operations and hence their complexity. Finally, relabeling,
can be done in linear time.
A more detailed complexity analysis is out of the scope of this work.

5. Minimal Representation and Experimental Results
In this section we address the problem of guaranteeing that the converted representation
is minimal in terms of the period length. As we will show in Example 14 the conversion
formulas proposed in this paper do not guarantee a minimal representation of the result and
it is not clear if conversion formulas ensuring minimality exist. Our approach is to apply a
minimization step in the conversion.
The practical applicability of the minimization step depends on the period length of the
representation that is to be minimized. Indeed, in our tests we noted that the minimization
step is efficient if the conversion formulas proposed in Section 4 are adopted, while it is
impractical when the conversion procedure returns a period that is orders of magnitude
higher than the minimal one as would be the case if conversion formulas were constructed
in a naive way.
5.1 Period Length Minimization
As stated in Section 2, each granularity can have different periodical representations and,
for a given granularity, it is possible to identify a set of representations that are minimal
i.e. adopting the smallest period length.
Unfortunately, the conversions do not always return a minimal representation, as shown
by Example 14.
Example 14 Consider a calendar that has day as the bottom granularity. We can define
week as week = Group7 (day); by applying the formulas for the Group operation we obtain
Pweek = 7 and Nweek = 1.
We can now apply the Altering-tick operation to add one day to every first week every
two weeks. Let this granularity be G1 = Alter21,1 (day, week); applying the formulas for the
Altering-tick operation we obtain PG1 = 15 and NG1 = 2.
We can again apply the Altering-tick operation to create a granularity G2 by removing
one day from every first granule of G1 every two granules of G1 : G2 = Alter21,−1 (day, G1 ).
Intuitively, by applying this operation we should get back to the granularity week, however
using the formulas for the Altering-tick operation we obtain PG2 = 14 and NG2 = 2; Hence
G2 is not minimal.
In order to qualitatively evaluate how close to the minimal representations the results
of our conversions are, we performed a set of tests using an algorithm (Bettini & Mascetti,
2005) for minimality checking. In our experimental results the conversions of algebraic
expressions defining granularities in real-world calendars, including many user-defined nonstandard ones, always returned exactly minimal representations. Non-minimal ones could
only be obtained by artificial examples like the one presented in Example 14.
Although a non-minimal result is unlikely in practical calendars, the minimality of the
granularity representation is known to greatly affect the performance of the algorithms for
322

Mapping Calendar Expressions to Minimal Periodic Sets

granularity processing, e.g., granularity constraint processing (Bettini et al., 2002a), calendar calculations (Urgun et al., 2007), workflow temporal support (Combi & Pozzi, 2003).
Hence, we considered an extension of the conversion algorithm by adding a minimization
step exploiting the technique illustrated by Bettini et al. (2005) to derive a minimal representation.
The choice of using only the conversion algorithm or the extended one with minimizations, should probably be driven by performance considerations. In Section 5.3 we report
the results of our experiments showing that generally it is advantageous to apply the minimization step. In our implementation, presented in Section 5.2, it is possible to specify if
the minimization step should be performed.
5.2 Implementation of the CalendarConverter Web Service
The conversion formulas presented in Section 4 have been implemented into the CalendarConverter web service that converts Calendar Algebra representations into the equivalent
periodical ones. More precisely, given a calendar in which granularities are expressed by Calendar Algebra operations, the service converts each operation into an equivalent periodical
representation.
The service first rewrites each calendar algebra expression in order to express it only
in terms of the bottom granularity. For example, if the bottom granularity is hour, the
expression Monday = Select-down11 (day, week) is changed to
Monday = Select-down11 (Group24 (hour), Group7 (Group24 (hour)))
Then, Procedure 1 is run for each granularity’s expression. The idea is that the periodical
representation of each subexpression is recursively computed starting from the expressions
having the bottom granularity as operand. Once each operand of a given operation has been
converted to periodical representation, the corresponding formula presented in Section 4 is
applied. We call this step the ConvertOperation procedure.
A trivial optimization of Procedure 1 consists in caching the results of the conversions
of each subexpression so that it is computed only once, even if the subexpression appears
several times (like Group24 (hour) in the above Monday definition).
5.3 Experimental Results
Our experiments address two main issues: first, we evaluate how the conversion formulas
impact on the practical applicability of the conversion procedure and, second, we evaluate
how useful is the minimization step.
For the first issue, we execute the conversion procedure with two different sets of conversion formulas and compare the results. The first set is laid out in Section 4. The other,
that is less optimized, is taken from the preliminary version of this paper (Bettini et al.,
2004).
Table 1 shows that when converting calendars having granularities with small minimal
period length (first two rows), using the formulas in Section 4 improves the performance
by one order of magnitude; However, conversions and minimizations are almost instantaneous with both approaches. On the contrary, when the minimal period length is higher,
323

Bettini, Mascetti & Wang

Procedure 1 ConvertExpression
• Input: a calendar algebra expression ex; a boolean value minimize that is set to
true if the minimization step is to be executed;
• Output: the periodical representation of ex;
• Method:
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

if (ex is the bottom granularity) then
return the periodical representation of the bottom granularity
end if
operands := ∅
for (each operand op of ex) do
add ConvertExpression(op, minimize) to operands;
end for
result :=ConvertOperation(ex.getOperator(), operands)
if (minimize) then
minimize the periodical representation of result
end if
return result;

Table 1: Impact of the conversion formulas on the performance of the conversion and minimization procedures (time in milliseconds).

Calendar
Period
Bot
1 year
day
4 years
day
1 year
hour
4 years
hour
100 years day

Section 4 formulas
Conv. Min. Tot.
4
2
6
7
2
9
9
2
11
16
4
20
127
9 136

Less optimized formulas
Conv.
Min.
Tot.
62
32
94
76
55
131
2,244
126,904
129,148
4,362
908,504
912,866
3,764 1,434,524 1,438,288

(last three rows) the time required to minimize the periodical representation is up to five
orders of magnitude larger if the formulas proposed by Bettini et al. (2004) are used; as
a consequence, the entire conversion may require several minutes while, using the formulas
presented in Section 4, it still requires only a fraction of a second. If the period length is
even larger, the conversion procedure is impractical if the formulas presented by Bettini et
al. (2004) are used, and indeed in our experiments we did not obtain a result in less than
thirteen hours.
For the second issue, we perform a set of three experiments. In the first one we compare
the performance of the conversion procedure with the performance of the minimization step.

324

Mapping Calendar Expressions to Minimal Periodic Sets

In the experiment we consider the case in which the conversion procedure produces minimal
representations. In this case the minimization step is always an overhead since it cannot
improve the performance of the conversion procedure.
Figure 14 shows the result of the experiment. Four calendars are considered, each one
containing a set of granularities of the Gregorian calendar. The four calendars differs in the
values of two parameters: the bottom granularity (it is second for cal-1 and cal-3 while it
is minute for cal-2 and cal-4) and the period in which leap years and leap years exceptions
are represented (it is 1, 4, 100 and 400 years for cal-1, cal-3, cal-2 and cal-4 respectively);
As a consequence, the minimal period length of the granularities month and year is about
3 · 107 for cal-1, 5 · 107 for cal-2, 108 for cal-3 and 2 · 108 for cal-4.

Figure 14: Impact of minimization over conversion; minimal conversions case.
As can be observed in Figure 14, the ratio between the time required to perform the
conversions and the time required for the minimization step varies significantly from a
minimum of 3% for cal-4 to a maximum of 23% for cal-3. The reason is that the complexity
of the conversion procedure is mainly affected by the period length of the granularity having
the largest period length. On the other hand, the complexity of the minimization step is
affected also by other features of the granularities such as their internal structure and the
number of integers that can divide at the same time the period label distance, the period
length and the number of granules in one period; For more details see (Bettini & Mascetti,
2005).
In the second experiment we consider the case in which the conversion procedure produces a non-minimal representation for a granularity in the input calendar; in this case it is
possible to benefit from the minimization step. For example, suppose that a granularity G
is converted and that it is then used as an argument of another Calendar Algebra operation
that defines a granularity H. The time required to compute the periodical representation
of H strongly depends on the period length of G; If the period length of G is reduced by
the execution of the minimization step, the conversion of H can be executed faster.
We produced this situation using a technique similar to the one of Example 14; we
created Calendar Algebra definitions of the Gregorian calendar in which the granularity
day is converted into a granularity having a non-minimal representation. Figure 15 shows
the performance obtained converting the same granularities that were used in Figure 14.
325

Bettini, Mascetti & Wang

The difference was that in this case the definition of the granularity day is such that, after
the conversion procedure, its period is twice as large as the minimal one (i.e., 48 hours or
2880 minutes or 172800 seconds depending on the bottom granularity that is used). It can
be easily seen that in this case the use of the minimization step can improve the performance
of the entire algorithm. Indeed, when the minimization step is performed, the conversion
procedure requires about one half of the time that is required when no minimization is
performed.

Figure 15: Impact of minimization over conversion; non-minimal case.
In the third experiment we evaluate the impact of the minimal representation on the
performance of applications involving intensive manipulations of granularities. In the test we
use the GSTP solver as such an application; it computes solutions of temporal constraints
with granularities. A description of the architecture of the GSTP system is provided in
Section 6.1.
Figure 16 shows our experiments performed on four temporal constraint networks with
granularities. The four networks differs in the number of variables, in the number of constraints and in the granularities used to express the constraints. The networks labeled as
“non-minimal” use granularities definitions that are obtained with a technique similar to
the one used in Example 14, and have a period that is twice as large as the minimal one.
Figure 16 shows that the use of minimal representations greatly improves the performance of the GSTP solver. Indeed in our experiments the ratio between the time required
to solve the network using a non-minimal representation and a minimal one is between
three and five. Moreover, the more time required to solve the network, the greater the
improvement obtained using the minimal representation; this means that for very complex
temporal networks we expect the improvement to be even higher.
Considering the results of our experiments, we conclude that, in general, it is advisable
to perform the minimization step. In particular, it is very advantageous in the specific case
of GSTP, based on the following considerations: i) the time required to perform the minimization step is only a fraction of the time required to perform the conversion procedure, ii)
the conversions are performed off-line in most cases, with respect to granularity processing,
and conversion results are cached for future use, and iii) the period length strongly influ-

326

Mapping Calendar Expressions to Minimal Periodic Sets

Figure 16: Impact of minimal representations on the performance of the GSTP solver.
ences the GSTP processing time that is in most cases much longer than the time needed
for conversion.

6. Applications
In this section we complement the motivations for this work with a sketch of the applications
enabled by the proposed conversion. Firstly we describe the GSTP system, as an example
of applications involving intensive manipulation of time granularities. GSTP is used to
check the consistency and to find solutions of temporal constraint satisfaction problems
with granularities5 ; It has also been applied to check the consistency of inter-organizational
workflow models (Bettini, Wang, & Jajodia, 2002b). Then, we discuss the use of Calendar
Algebra to define new granularities that may later be part of the input of reasoning services,
such as GSTP.
6.1 The GSTP System
The GSTP system has been developed at the University of Milan with the objective of
providing universal access to the implementation of a set of algorithms for multi-granularity
temporal constraint satisfaction (Bettini et al., 2002a). It allows the user to specify binary
constraints of the form Y − X ∈ [m, n]G where m and n are the minimum and maximum
values of the distance from Y to X in terms of granularity G. Variables take values in the
positive integers, and unary constraints can be applied on their domains. For example, the
constraint: Event2 should occur 2 to 4 business days after the occurrence of Event1 can be
modeled by OccE2 − OccE1 ∈ [2, 4]BDay. This problem is considered an extension of STP
(Dechter, Meiri, & Pearl, 1991) to multiple and arbitrary granularities. To our knowledge,
GSTP is the only available system to solve this class of temporal constraint satisfaction
problems.
Figure 17 shows the general architecture of the GSTP system. There are three main
modules: the constraint solver; the web service, which enables external access to the solver;
5. For a detailed description of the system, see (Bettini et al., 2005).

327

Bettini, Mascetti & Wang

and a user interface that can be used locally or remotely to design and analyze constraint
networks.

Figure 17: The GSTP Architecture
The constraint solver is the C implementation of the ACG algorithm which has been
proposed by Bettini et al. (2002a), and it runs on a server machine. Following the approach of Bettini et al. (2002a), the solver uses the representation of granularities based on
periodical sets. This representation makes it possible to efficiently compute the core operations on granularities that are required to solve the constraint satisfaction problem. These
operations involve, for example, the union and the intersection of periodical sets. While
we cannot exclude that these operations may be computed in terms of alternative low level
representations, it seems much harder to obtain similar results if a high level representation,
such as Calendar Algebra, is used.
The second module of the system is the Web Service that defines, through a WSDL
specification, the parameters that can be passed to the constraint solver, including the
XML schema for the constraint network specification.
The third module is a remote Java-based user interface, which allows the user to easily
edit constraint networks, to submit them to the constraint solver, and to analyze results. In
particular, it is possible to have views in terms of specific granularities, to visualize implicit
constraints, to browse descriptions of domains, and to obtain a network solution. Fig. 18
shows a screenshot from the interface.
6.2 Defining New Granularities
While the GSTP solver can handle arbitrary granularities, new granularities must be added
by editing their explicit periodical representation. This is true in general for any multi328

Mapping Calendar Expressions to Minimal Periodic Sets

Figure 18: The GSTP User Interface
granularity reasoning service based on a low-level representation of granularities, and it is
a painful task when the granularities have a large period. For example, in the experimental
results illustrated in Figure 16, we used a representation of the granularity month that
considers leap years and leap years exceptions in a period of 400 years. In this case, the
users have to specify the representation of 4800 granules i.e., the number of months in 400
years.
Because the period length of real world granularities is generally high, a graphical interface does not help if it only supports the user to individually select the explicit granules. An
effective solution requires the use of implicit or explicit operations on granules. Among the
various proposals, Calendar Algebra provides the richest set of such operators. A question
arises: is the definition of granularities in terms of Calendar Algebra really simpler than the
specification of the periodical representation? Calendar Algebra does not seem to be user
friendly: the exact semantics of each operator may not be immediate for an inexperienced
user and some time is required in order to learn how to use each operator.
In practice, we do not think that it is reasonable to ask an unexperienced user to
define granularities by writing Calendar Algebra expressions. Nevertheless, we do think
that Calendar Algebra can be used by specialized user interfaces to guide the user when
specifying granularities. In this sense, we believe that Calendar Algebra plays the same
role that SQL does in the definition of databases queries. Similarly to Calendar Algebra,
SQL is an abstraction tool that can be directly exploited in all its expressive power by an
advanced user, but can also be used by a less experienced user through a graphical user
interface, possibly with a reduced expressiveness.
As mentioned above, in the case of periodical representations, graphical user interfaces
are not sufficient for making the specification of new granularities practical. On the contrary, in the case of Calendar Algebra, user interfaces can strongly enhance the usability
of Calendar Algebra, making its practical use possible also for the definition of involved
granularities. There are at least two reasons for this difference. Firstly, the main difficulty
of Calendar Algebra is the understanding of the semantics of the operators and the choice
of the most appropriate one for a given task. An effective user interface can hide the existence of the algebraic operators to the user showing only how the operators modify existing

329

Bettini, Mascetti & Wang

(a) Step 1.

(b) Step 2.

(c) Step 3.

Figure 19: A 3-steps wizard for visually defining a granularity using Calendar Algebra
granularities (i.e., the semantics of the operators). Secondarily, Calendar Algebra allows
the compact definition of granularities. This is due to the fact that the Calendar Algebra
operations are specifically designed to reflect the intuitive ways in which users define new
granularities.
Example 15 shows how a graphical user interface can be effectively used to define a new
granularity in terms of Calendar Algebra expression.
Example 15 This example shows how a graphical user interface can be used to support the
user in the definition of the granularity final as the set of days, each one corresponding to
330

Mapping Calendar Expressions to Minimal Periodic Sets

the last Monday of every academic semester. We assume that the granularities Monday and
academicSemester have already been defined. The graphical user interface that we use in
this example is a wizard that guides the user step by step. In the first step (Figure 19(a)) the
user chooses the kind of operation he wants to perform. In the second step (Figure 19(b))
the user can provide more details about how he wants to modify the operand granularity
(Monday, in the example). The results of this choice is a Calendar Algebra expression that
is shown in the third step (Figure 19(c)); in this last window the user can also give a name
to the granularity that has been defined.
6.3 The Global Architecture

Figure 20: Integration of GSTP and CalendarConverter web services
Figure 20 shows a possible architecture for the integration of GSTP, the interface for
new granularity definitions and the CalendarConverter web service. A granularity repository
collects the Calendar Algebra definitions. Upon request by the GSTP system definitions are
converted in low-level representation by the CalendarConverter web service to be efficiently
processed. Clearly, caching techniques can be used to optimize the process.

7. Related Work
Several formalisms have been proposed for symbolic representation of granularities and
periodicity. Periodicity and its application in the AI and DB area have been extensively
investigated (Tuzhilin & Clifford, 1995; Morris, Shoaff, & Khatib, 1996; Kabanza, Stevenne,
& Wolper, 1990; Ladkin, 1986). Regarding symbolic representation, it is well known the
formalism proposed by Leban et al. (1986), that is based on the notion of collection, and
it is intended to represent temporal expressions occurring in natural language. A collection
is a structured set of time intervals where the order of the collection gives a measure of
the structure depth: an order 1 collection is an ordered list of intervals, and an order n
(n > 1) collection is an ordered list of collections having order n − 1. Two operators,
331

Bettini, Mascetti & Wang

called slicing and dicing are used to operate on collections by selecting specific intervals
or sub-collections, and by further dividing an interval into a collection, respectively. For
example, Weeks:during:January2006 divides the interval corresponding to January2006
into the intervals corresponding to the weeks that are fully contained in that month. This
formalism has been adopted with some extensions by many researchers in the AI (Koomen,
1991; Cukierman & Delgrande, 1998) and Database area (Chandra, Segev, & Stonebraker,
1994; Terenziani, 2003). In particular, the control statements if-then-else and while
have been introduced by Chandra et al. (1994) to facilitate the representation of certain
sets of intervals. For example, it is possible to specify: the fourth Saturday of April if not
an holiday, and the previous business day otherwise.
As for the deductive database community, a second influential proposal is the slice
formalism introduced by Niezette et al. (1992). A slice denotes a (finite or infinite) set of not
necessarily consecutive time intervals. For example, the slice all.Years + {2,4}.Months
+ {1}.Days . 2.Days denotes a set of intervals corresponding to the first 2 days of February
and April of each year.
A totally different approach is the calendar algebra described by Ning et al. (2002), and
considered in this paper. The representation is based on a rich set of algebraic operators
on periodic sets as opposed to slicing and dicing over nonconvex intervals.
None of the above cited papers provide a mapping to identify how each operator changes
the mathematical characterization of the periodicity of the argument expressions. The
problem of finding these mappings is not trivial for some operators.
In (Bettini & Sibi, 2000) the expressive power of the algebras proposed by Leban et
al. (1986) and Niezette et al. (1992) is compared and an extension to the first is proposed
in order to capture a larger set of granularities. Since the periodical representation is
used to compare expressiveness, a mapping from calendar expressions in those formalisms
to periodical representations can be found in the proofs of that paper. However, since
minimality is not an issue for the purpose of comparing expressiveness, in many cases the
mapping returns non-minimal representations.
Regarding alternative approaches for low-level representation, we already mentioned
that the ones based on strings (Wijsen, 2000) and automata (Dal Lago, Montanari, &
Puppis, 2003; Bresolin et al., 2004) may be considered as an alternative for the target of
our conversion. As a matter of fact, an example of the conversion of a Calendar Algebra
expression into a string based representation can be found in (Dal Lago & Montanari,
2001). A complete conversion procedure appeared during the revision process of this paper
in the PhD Dissertation by Puppis (2006). The aim of the conversion is to prove that the
granspecs formalism, used to represent granularities in terms of automata, has at least the
same expressiveness as the Calendar Algebra. Hence, obtaining minimal representations was
not the goal. Moreover, in their case minimization is not in terms of the period length, but in
terms of the automaton size and automaton complexity. About the complexity of reasoning,
given an automaton M , the worst case time complexity of the operations analogous to our up
and down depends linearly on ||M ||, a value computed from M itself and called complexity
of M . In this sense ||M || has the same role of our period length (P ), even if a precise
relationship between the two values is hard to obtain. In our approach we compute up in
logarithmic time with respect to P and down in linear time with respect to the dimension
of the result (that is bounded by P ). Other operations, like checking for equivalence, seem
332

Mapping Calendar Expressions to Minimal Periodic Sets

to be more complex using automata (Bresolin et al., 2004). Techniques for minimization
in terms of automaton complexity are presented by Dal Lago et al. (2003), and the time
complexity is proved to be polynomial, even if the exact bound is not explicitly given. In
3
our approach, the worst case time complexity for the minimization is O(P 2 ) (Bettini &
Mascetti, 2005). Overall, the automata approach is very elegant and well-founded, but,
on one side it still misses an implementation in order to have some experimental data to
compare with, and on the other side only basic operations have been currently defined;
it would be interesting to investigate the definition on that formalism of more complex
operations like the ones required by GSTP.

8. Conclusion and Future Work
We have presented an hybrid algorithm that interleaves the conversion of Calendar Algebra
subexpressions into periodical sets with the minimization of the period length. We have
proved that the algorithm returns set-based granularity representations having minimal
period length, which is extremely important for the efficiency of operations on granularities. Based on the technical contribution of this paper, a software system is being developed
allowing users to access multi-granularity reasoning services by defining arbitrary time granularities with a high-level formalism. Our current efforts are mainly devoted to completing
and refining the development of the different modules of the architecture shown in Section 6.3.
As a future work, we intend to develop effective graphical user interfaces to support the
definition of Calendar Algebra expressions in a user friendly way. Example 15 described one
of the possible interfaces. Another open issue is how to convert a periodical representation
of a granularity into a “user friendly” Calendar Algebra expression. This conversion could
be useful, for example, to present the result of a computation performed using the periodical
representation. However, a naive conversion may not be effective since the resulting calendar
algebra expression could be as involved as the periodical representation from which it is
derived. For example, a conversion procedure is presented by Bettini et al. (2000) to prove
that the Calendar Algebra is at least as expressive as the periodical representation; however,
the resulting Calendar Algebra expression is composed by a number of Calendar Algebra
operations that is linear in the number of granules that are in one period of the original
granularity. On the contrary, an effective conversion should generate Calendar Algebra
expressions that are compact and easily readable by the user. This problem is somehow
related to the discovery of calendar-based association rules (Li, Ning, Wang, & Jajodia,
2001). Finally, we intend to investigate the usage of the automaton-based representation
as a low-level granularity formalism. It would be interesting to know whether, using this
representation, it is possible to compute the same operations that can be computed with
the periodical representation and if any performance gain could be achieved.

Acknowledgments
We thank the anonymous referees for their useful comments and suggestions. The work of
Bettini and Mascetti was partially supported by Italian MIUR InterLink project N.II04C0EC1D.
The work of Wang was partially supported by the US NSF grant IIS-0415023.

333

Bettini, Mascetti & Wang

Appendix A. Proofs
A.1 Transitivity of the Periodically Groups Into Relationship
In order to prove the correctness of the conversions of algebraic expressions into periodical
sets, it is useful to have a formal result about the transitivity of the periodically groups into
relation. In addition to transitivity of /¯ , Theorem 1 also says something about period
length values.
Theorem 1 Let G and H be two unbounded granularities such that G is periodic in terms
of the bottom granularity (i.e., ⊥ /¯ G) and H is periodic in terms of G (i.e., G /¯ H). Let
G be the period length and the period label distance of H in terms of granules of
PHG and NH
G, and NG the period label distance of G in terms of ⊥. Then, if PHG = αNG for some
positive integer α, then H is periodic in terms of the bottom granularity (i.e., ⊥ /¯ H) and
PH = αPG .
G
¯
Proof. Since
Sni by hypothesis G / H and PH = αNG , ∀i if H(i) =
G
NH ) = r=0 G(ir + αNG ). This can be also written as follows:
if

Sni

r=0 G(ir ),

then H(i +

H(i) = G(i0 ) ∪ ... ∪ G(ini )

(1)

G
H(i + NH
) = G(i0 + αNG ) ∪ ... ∪ G(ini + αNG )

(2)

then ∃β ∈ Ns.t.:

Since ⊥ /¯ G, if
τ

G(ij ) =

ij
[

⊥(ij,k )

(3)

⊥(ij,k + PG )

(4)

k=0

then
τ

G(ij + NG ) =

ij
[

k=0

This can be clearly extended using αNG instead of NG .
τ

G(ij + αNG ) =

ij
[

⊥(ij,k + αPG )

(5)

k=0

Rewriting (1) substituting G(ij ) according to (3) and rewriting (2) substituting G(ij +
αNG ) according to (5), we obtain:
if H(i) = ⊥(i0,0 ) ∪ . . . ∪ ⊥(i0,τi0 ) ∪ . . . ∪ ⊥(ini ,0 ) ∪ ... ∪ ⊥(ini ,τin )
i
|
{z
}
|
{z
}
G(i0 )

G(ini )

334

Mapping Calendar Expressions to Minimal Periodic Sets

G ) = ⊥(i
then H(i + NH
0,0 + αPG ) ∪ . . . ∪ ⊥(i0,τi0 + αPG ) ∪ . . .
{z
}
|
G(i0 +αNG )

∪ ⊥(ini ,0 + αPG ) ∪ . . . ∪ ⊥(ini ,τin + αPG )
i
|
{z
}
G(ini +αNG )

Hence the second condition of Definition 5 is satisfied. The third one is always satisfied for
unbounded granularities. The first one is satisfied too; in fact since G /¯ H with a period
G , then for each label i of H, i + N G is a label of H. Hence, by definition
label distance of NH
H
G.
of periodically-groups-into ⊥ /¯ H with PH = αPG and NH = NH

A.2 Proof of Proposition 1
A.2.1 Part 1
From the definition of the Group operation, for all i ∈ N:
0

G (i) =

im
[

G(j) = G(im − m + 1) ∪ . . . ∪ G(im) = G(λ) ∪ . . . ∪ G(λ + m − 1)

j=(i−1)m+1

with λ = im − m + 1. Furthermore, ∀k ∈ N:
(i+k)m

[

0

G (i + k) =

G(j) = G(im + km − m + 1) ∪ . . . ∪ G(im + km) =

j=(i+k−1)m+1

= G(λ + km) ∪ . . . ∪ G(λ + km + m − 1)
Hence,
0

0

If G (i ) =

m−1
[

0

m−1
[

0

G(λ + r) then G (i + k) =

r=0

G(λ + r + km).

(6)

r=0

G
This holds for each k. If we use k = GCMN(m,N
(note that k ∈ N), then all the hypotheses
G)
of Theorem 1 are satisfied: (i) ⊥ /¯ G (by hypothesis); (ii) G /¯ G0 (since G / G0 , LG0 = Z,
m·NG
NG
and (6) holds); (iii) PGG0 = GCM
(m,NG ) (since we use k = GCM (m,NG ) and, from (6) we

know that PGG0 = km). Therefore, by Theorem 1, ⊥ /¯ G0 with PG0 =
NG0 =

mPG
GCM (m,NG )

NG
GCM (m·NG ) .

A.2.2 Part 2
From the definition of the Group

j

k

“j

G0





lG −1
+1
m
0
operation, G (i) =

By definition of l, we need to show that G0



lG − 1
+1
m



lG −1
m

k ”
+1 ·m

[

=
j=

335

S
= tj=b G(j) with b ≤ lG ≤ t.
Si·m
j=(i−1)·m+1 G(i) ; hence:

j

lG −1
m

k
·m+1

G(j)

and

Bettini, Mascetti & Wang

We prove the thesis showing that (1)
lG .
(1) Since

j

lG −1
m

k

≤

lG −1
m ,

j

lG −1
m

lG −1
G
≥ lm
m
lG −1−[(lG −1)mod m]
G
≥ lm
−1; it
m

(2) First we prove that
prove that

hence
k
j

j

lG −1
m

k

·m+1 ≤ lG and that (2)

j

lG −1
m

k


+ 1 ·m ≥

k

· m + 1 ≤ lG
j
k
− 1. Since lGm−1 =

lG −1−[(lG −1)mod m]
m

we have to

is equivalent to thej inequality
− [(lG − 1)mod m] ≥
k

−m + 1 that is true since (lG − 1)mod m ≤ m − 1. Since
k

j
lG −1
+
1
· m ≥ lG .
m

lG −1
m

≥

lG
m

− 1 it is trivial that

A.3 Proof of Proposition 2
A.3.1 Part 1
Proof sketch
We show that G2 /¯ G0 with PGG02 = αNG2 and then we apply Theorem 1 to obtain the thesis.
In particular we use


PG2 · NG1
NG2 · m
∆ = lcm NG1 , m,
,
GCD(PG2 · NG1 , PG1 ) GCD(NG2 · m, |k|)
and


PG2
∆ · PG1 · NG2
∆·k
·
α=
+
NG1 · PG2
m
NG2
S
S
such that, for each i, if ∃j, k : G0 (i) = kr=0 G2 (j + r), then G0 (i + ∆) = kr=0 G2 (j + r +
αNG2 ).
Given an arbitrary granule G0 (i), we show that G0 (i + ∆) is the union of granules that
can be obtained by adding αNG2 to the index of each granule of G2 contained in G0 (i). Note
that i + ∆ ∈ LG0 since G0 is full-integer labeled. In order to show that this is correct we
consider the way granules of G0 are constructed by definition of altering-tick. More precisely,
we compute the difference between the label b0i+∆ of the first granule of G2 included in
G0 (i + ∆) and the label b0i of the first granule of G2 included in G0 (i); we show that this
difference is equal to the difference between the label t0i+∆ of the last granule of G2 included
in G0 (i + ∆) and the label t0i of the last granule of G2 included in G0 (i). This fact together
with the consideration that G2 is a full-integer labeled granularity, leads to the conclusion
that G0 (i) and G0 (i + ∆) have the same number of granules. It is then clear that the above
computed label differences are also equal to the difference between the label of an arbitrary
n-th granule of G2 included in G0 (i + ∆) and the label of the n-th granule
of G2 included
S
in G0 (i). If this difference is b0i+∆ − b0i , then we have: if ∃j, k : G0 (i) = kr=0 G2 (j + r), then

S
G0 (i + ∆) = kr=0 G2 j + r + b0i+∆ − b0i . By showing that b0i+∆ − b0i is a multiple of NG2
the thesis follows.
Proof details


336

Mapping Calendar Expressions to Minimal Periodic Sets

Si
Sti+∆
Assume G1 (i) = tj=b
G2 (j) and G1 (i + ∆) = j=b
G2 (j). We need to compute
i
i+∆
0
0
bi+∆ − bi . From the definition of the the altering-tick operation:
b0i

=

 

k
 bi + i−l
m


bi +

 i−l 
m

if i =

 i−l 
m

m + l,
(7)


+ 1 k otherwise.

and
b0i+∆

=




k
 bi+∆ + i+∆−l
m


bi+∆ +

 i+∆−l 
m

if i + ∆ =

 i+∆−l 
m

m + l,
(8)


+ 1 k otherwise.

 




Note that if i = i−l
m + l, then i + ∆ = i+∆−l
Indeed, i+∆−l
+l =
m
m  m + l. 
m  m
 i−l ∆ 
∆
i−l
∆
i−l
+
m+l
and,
since
∆
is
a
multiple
of
m,
then
+
m+l
=
+
m+
m
m
m
m
m
m

i−l
l = ∆ + m m + l.
Hence, to compute b0i+∆ − b0i we should consider two cases:
b0i+∆ − b0i =



 
 

k − bi − i−l
k if i = i−l
m+l
 bi+∆ + i+∆−l
m
m
m


bi+∆ +

 i+∆−l 
m

(9)


 

+ 1 k − bi − i−l
m + 1 k otherwise.

In both cases (again considering the fact that ∆ is a multiple of m):
b0i+∆ − b0i = (bi+∆ − bi ) +

∆·k
m

(10)

We are left to compute bi+∆ −bi , i.e., the distance in terms of granules of G2 , between G2 (bi )
Sti+∆
Si
G2 (j),
G2 (j) and G1 (i + ∆) = j=b
and G2 (bi+∆ ). Since, by hypothesis, G1 (i) = tj=b
i
i+∆
then the first granule of ⊥ making G2 (bi ) and the first granule of ⊥ making G1 (i) is the
same granule. The same can be observed for the first granule of ⊥ making G2 (bi+∆ ) and
the first granule of ⊥ making G1 (i + ∆). More formally:
min bbi cG2 = min bicG1
and
min bbi+∆ cG2 = min bi + ∆cG1
Hence, we have:
min bbi+∆ cG2 − min bbi cG2 = min bi + ∆cG1 − min bicG1

(11)

We have shown that the difference between the index of the first granule of ⊥ making
G2 (bi+∆ ) and the index of the first granule of ⊥ making G2 (bi ) is equal to the difference
between the index of the first granule of ⊥ making G1 (i + ∆) and the index of the first
granule of ⊥ making G1 (i). Then, we need to compute the difference between the index
of the first granule of ⊥ making G1 (i + ∆) and the index of the first granuleSof ⊥ making
G1 (i). Since ⊥ /¯ G1 and ∆ is a multiple of NG1 , for each i, if ∃j, τ : G1 (i) = τr=0 ⊥(j + r),
337

Bettini, Mascetti & Wang

S
∆·P
∆·P
then G1 (i + ∆) = τr=0 ⊥(j + NGG1 ). Hence, this difference has value NGG1 , and for what
1
1
shown above this is also the value of the difference between the index of the first granule
of ⊥ making G2 (bi+∆ ) and the index of the first granule of ⊥ making G2 (bi ). Then, since
∆·P
⊥ /¯ G2 with period length PG2 and since NGG1 is a multiple of PG2 , we have that, if:
1

⊥(j) ⊆ G2 (i)
then:
⊥(j +
Thus, bi+∆ − bi =
Reconsidering 10:

∆ · PG1
∆ · PG1 · NG2
)
) ⊆ G2 (i +
NG1
NG1 · PG2

∆·PG1 ·NG2
NG1 ·PG2 .

b0i+∆ − b0i =

∆ · PG1 · NG2
∆·k
+
.
NG1 · PG2
m
∆·P

·N

Analogously we can compute t0i+∆ − t0i = NGG1·PGG2 + ∆·k
m .
1
2
0
0
0
0
Thus, bi+∆ − bi = ti+∆ − ti ; hence ti+∆ − bi+∆ = ti − bi . Since G2 is a full integer labeled
granularity, then G0 (i) and G0 (i + ∆) are formed by the same number of granules.
St0i+∆
St0i
0
0
0
0
Since we now know G0 (i+∆) = j=b
G2 (j) = j=b
0 G2 (j +(bi+∆ −bi )) and (bi+∆ −bi )
0
i

i+∆

/¯ G0 ,

PGG02

∆·PG1 ·NG2
NG1 ·PG2

is a multiple of NG2 , we have G2
=
and ⊥ /¯ G2 . Hence, all the
hypothesis of Theorem 1 hold, and its application leads the thesis of this proposition.
A.3.2 Part 2
0

Since G2 partitions G0 (see table 2.2 of (Bettini et al., 2000)), then (1) dlG2 eG
G2 is al+
ways defined and (2) min({n ∈ N |∃i ∈ LG2 s.t. ⊥(n) ⊆ G2 (i)}) = min({m ∈ N+ |∃j ∈
LG0 s.t. ⊥(m) ⊆ G0 (j)}). Therefore lG0 is the label of the granule of G0 that covers the
0
granule of G2 labeled with lG2 ; by definition of d·e operation, lG0 = dlG2 eG
G2 .
A.4 Proof of Proposition 3
A.4.1 Part 2
By definition of the Shift operation, G0 (i) = G(i−m). Hence G0 (lG +m) = G(lG +m−m) =
G(lG ).
A.5 Proof of Proposition 4
A.5.1 Part 1
The thesis will follow from the application of Theorem 1. Indeed, we know that ⊥ /¯ G2 and
we show that G2 /¯ G0 with PGG02 multiple
∆ and α s.t., for
S of NG2 . For this we0 need to identify
S
0
each i, if there exists s(i) s.t. G (i) = j∈s(i) G2 (j), then G (i + ∆) = j∈s(i) G2 (j + αNG2 ).
lcm(P

,P

)N

G1 G2
G1
Consider an arbitrary i ∈ N and ∆ =
. By definition of the combining
PG1
S
S
0
0
operation, we have G (i) = j∈s(i) G2 (j) and G (i + ∆) = j∈s(i+∆) G2 (j) with

s(i) = {j ∈ LG2 |∅ =
6 G2 (j) ⊆ G1 (i)}
338

Mapping Calendar Expressions to Minimal Periodic Sets

and
s(i + ∆) = {j ∈ LG2 |∅ =
6 G2 (j) ⊆ G1 (i + ∆)} .
We now show that s(i + ∆) is composed by all and only the elements of s(i) when the
lcm(PG1 ,PG2 )NG2
quantity ∆0 =
is added. For this purpose we need:
PG
2

∀j ∈ s(i) ∃(j + ∆0 ) ∈ s(i + ∆)

(12)


∀ j + ∆0 ∈ s(i + ∆) ∃j ∈ s(i)

(13)

and

About 12, note that if j ∈ s(i), then G2 (j) ⊆ G1 (i). Since ⊥ /¯ G2 , if
G2 (j) =

k
[

⊥(jr )

r=0

then
0



G2 j + ∆ =

k
[

⊥(jr + lcm(PG1 , PG2 ))

(14)

r=0

Since G1 (i) ⊇ G2 (j) =

Sk

r=0 ⊥(jr ),

and since ⊥ /¯ G1 , then

G1 (j + ∆) ⊇

k
[

⊥(jr + lcm(PG1 , PG2 ))

(15)

r=0

From 14 and 15 we derive G1 (i + ∆) ⊇ G2 (j + ∆0 ), and hence (j + ∆0 ) ∈ s(i + ∆).
Analogously
validitySof 13; Hence, for each i, if there exists s(i) s.t.
S can be proved the
0
0
G (i) = j∈s(i) G2 (j), then G (i + ∆) = j∈s(i) G2 (j + ∆0 ). Hence, considering the fact that
G2 / G0 , we can conclude G2 /¯ G0 . Finally, since PGG02 is a multiple of NG2 , by Theorem 1
we obtain the thesis.
A.5.2 Part 2
Let

P 0
s(i) 6= ∅}
LeG0 = {i ∈ L̂GG1 |e
P

0

P

0

where ∀i ∈ L̂GG1 se(i) = {j ∈ L̂GG2 |∅ =
6 G2 (j) ⊆ G1 (i)};
We show that LeG0 = L̂G0 by proving that: (1) LeG0 ⊇ L̂G0 and (2) LeG0 ⊆ L̂G0 .
(1) Suppose by contradiction that exists k ∈ L̂G0 \ LeG0 . Since k ∈ L̂G0 and since G0
is derived by the Combine operation,
then ∃q ∈ LG2 |G2 (q) ⊆ G1 (k). By definition of the
S
0
Combine operation G (k) = j∈s(k) G2 (j); since q ∈ s(k), then G2 (q) ⊆ G0 (k). Hence (a)
∃q ∈ LG2 |G2 (q) ⊆ G0 (k).
P 0
Moreover, since k 6∈ LeG0 , then se(k) = ∅; therefore @j ∈ L̂GG2 |G2 (j) ⊆ G1 (k). By
definition of the Combine operation it is easily seen that G0  G1 . Using this and the
P 0
previous formula, we derive that (b) @j ∈ L̂GG2 |G2 (j) ⊆ G0 (k).
339

Bettini, Mascetti & Wang

P

0

From (a) and (b) it follows that ∃q ∈ LG2 \ L̂GG2 |G2 (q) ⊆ G0 (k). We show that this
leads to a contradiction.
P 0
P 0
P 0
P 0
Since q 6∈ L̂GG2 and labels of L̂GG2 are contiguous (i.e., @i ∈ LG2 \ L̂GG2 s.t. min(L̂GG2 ) <
P

P

0

P

0

0

i < max(L̂GG2 )), then q < min(L̂GG2 ) or q > max(L̂GG2 ). We consider the first case, the
proof for the second is analogous.
P 0
P 0
If q < min(L̂GG2 ) then max(bqcG2 ) < 1 (otherwise q ∈ L̂GG2 ).
0
0
Let be α = min(bmin(L̂G0 )cG ). Since k ∈ L̂G0 , then α ≤ bkcG .
If α ≥ 1, then G0 (k) ∩ G2 (q) = ∅ contradicting G0 (k) ⊇ G2 (q).
If α < 1, then G0 (lG0 ) ⊇ ⊥(0) and we show that lG0 ∈ LeG0 . Indeed, by definition of
P 0
P 0
Combine, ∃j ∈ L̂GG2 |G2 (j) ⊆ G0 (LG0 ). Since G0  G1 we also have ∃j ∈ L̂GG2 |G2 (j) ⊆
G1 (LG0 ); hence j ∈ se(lG0 ) and then lG0 ∈ LeG0 .
Since 0 ∈ G0 (lG0 ) and max(bqcG2 ) ≤ 0, then max(bqcG2 ) < α (otherwise G2 (q) ⊆
0
0
G0 (lG0 )). Therefore, since min(bkcG ) ≥ α, then bqcG2 ∩ blG0 cG = ∅, in contradiction with
G2 (q) ⊆ G0 (k).
e
(2) Suppose by contradiction that ∃k ∈ LeG0 \ L̂G0 . Since k ∈ LeG0 , by definition of L,
P

P

0

0

k ∈ L̂GG1 and se(k) 6= ∅; Therefore, by definition of se, ∃j ∈ L̂GG2 |G2 (j) ⊆ G1 (k).
P

0

Since j ∈ L̂GG2 , by definition of L̂, ∃h with 0 < h ≤ PG0 s.t. dheG2 = j. Since
0
G2 (j) ⊆ G1 (k), then dheG1 = k. By definition of the combine operation, dheG = k.
0
Moreover, since 0 < h ≤ PG0 , by definition of L̂, dheG = k ∈ L̂G0 , contradicting the
hypothesis.
A.6 Proof of Proposition 5
A.6.1 Part 1
The thesis will follow from the application of Theorem 1. Indeed, we show that G1 /¯ G0 with
PGG01 multiple of NG1 . For this we need to identify ∆ and α s.t., for each i, if there exists s(i)
S
S
lcm(PG1 ,PG2 )NG2
s.t. G0 (i) = j∈s(i) G1 (j), then G0 (i+∆) = j∈s(i) G1 (j +αNG1 ). Let ∆ =
.
PG2
0
S
S
0
(i+∆) −1
−1
G1 (j) and G0 (i + ∆) = j=i+∆ G1 (j)
By definition of anchored grouping, G0 (i) = ij=i
where i0 is the first label of G2 after i and (i + ∆)0 is the first label of G2 after i + ∆. By
periodicity of G2 , (and since ∆ is a multiple of NG2 ) the difference between the label of
the granule following G2 (i + ∆) and the label of the granule following G2 (i) is
Sk∆. More
0
0
0
0
0
formally, (i + ∆) − i = ∆, hence (i + ∆) = i + ∆. Then, for each i, if G (i) = j=i G1 (j),
S 0 +∆−1
S 0 −1
then G0 (i + ∆) = ij=i+∆
G1 (j) = ij=i
G1 (j + ∆). By this result and considering G1 / G0 ,
we conclude G1 /¯ G0 with PGG01 = ∆. Note that by Proposition 9, NG1 =
PGG01

is a multiple of ∆. Then, by Theorem 1, we have the thesis.

A.6.2 Part 2
Let

(
LeG0 =

P

0

if lG2 = lG1 ,
L̂GG2 ,
PG0
0
{lG2 } ∪ L̂G2 , otherwise,

340

PG1 ·NG2
PG2 ,

hence

Mapping Calendar Expressions to Minimal Periodic Sets

0
where lG
is the greatest among the labels of LG2 that are smaller than lG2 . We show that
2
e
LG0 = L̂G0 by proving that (1) LeG0 ⊆ L̂G0 and (2) L̂G0 ⊆ LeG0 .
P 0
(1) Suppose by contradiction that ∃k ∈ LeG0 \ L̂G0 . Then, since k ∈ LeG0 , then k ∈ L̂GG2
0 .
or k = lG
2
P

P

0

0

If k ∈ L̂GG2 , then, by definition of L̂GG2 , ∃h with 0 < h ≤ PG0 s.t. dheG2 = k. By
S 0 −1
definition of Anchored-group, G0 (k) = kj=k
G1 (j) where k 0 is the first label of G2 after
0
k. Therefore G (k) ⊇ G1 (k). Since G2 is a labeled aligned subgranularity of G1 and since
0
k ∈ LG2 , then k ∈ LG1 and G1 (k) = G2 (k). Hence G0 (k) ⊇ G2 (k). It follows that dheG = k
and therefore, by definition of L̂, k ∈ L̂G0 in contrast with the hypothesis.
0 , then, by definition of L
eG0 , lG 6= lG . Therefore, since G2 is a labeled aligned
If k = lG
2
1
2
0 <l
;
then
∃h
with
0 < h < min(blG2 cG2 ) s.t. dheG1 = lG1 .
<
l
subgranularity of G1 lG
G
G
2
1
2
S
lG2 −1
0 ) =
0
Since, by definition of Anchored-group, G0 (lG
G1 (j) and since lG
< lG1 < lG2 ,
j=l0
2
2
G2

0

0 ) ⊇ G (l ). Hence dheG = l0
0
then G0 (lG
1 G1
G2 and therefore, by definition of L̂, lG2 = k ∈ L̂G0
2
in contrast with the hypothesis.
P 0
(2) Suppose by contradiction that ∃k ∈ L̂G0 \ LeG0 . If k ∈ L̂GG2 then, by definition of
LeG0 , k ∈ LeG0 , in contrast with the hypothesis.
P 0
P 0
P 0
P 0
P 0
If k ∈
/ L̂GG2 , since @q ∈ LG2 \L̂GG2 s.t. min(L̂GG2 ) ≤ q ≤ max(L̂GG2 ), then k > max(L̂GG2 )
P

0

or k < min(L̂GG2 ).

P

0

If k > max(L̂GG2 ) then, by definition of L̂, min(bkcG2 ) > PG0 . Since G2 is a labeled
aligned subgranularity of G1 then G2 (k) = G1 (k) and hence min(bkcG1 ) > PG0 . Since
S 0 −1
0
G0 (k) = kj=k
G1 (j) then min(bkcG ) > PG0 in contrast with the hypothesis k ∈ L̂G0 .
P

0

0 , k < l0
0
If k < min(L̂GG2 ) then, by definition of lG
G2 or k = lG2 .
2
0
0
If k < lG
then, let k 0 be the next label of G2 after k. Since k < lG
then, by definition
2
2
0
0
0
0
0
G
lG2 , k ≤ lG2 . By definition of lG2 then max(blG2 c 2 ) ≤ 0. Since G2 is a labeled aligned
0 ) = G (l0 ); therefore max(bl0 cG1 ) ≤ 0. Since G0 (k) =
subgranularity of G1 then G1 (lG
2 G2
G2
2
Sk0 −1
0
0
G0 ) ≤ 0 in contrast with the hypothesis
G
(j)
and
k
≤
l
,
follows
that
max(bkc
1
j=k
G2
k ∈ L̂G0 .
SlG2 −1
0
0 ) =
0
Finally if k = lG
then G0 (lG
G1 (j). Since k = lG
∈ L̂G0 then ∃h with
j=l0
2
2
2
0

G2

0 . Since G0 is the composition of granules of G , dheG1 is
0 < h ≤ PG0 s.t. dheG = lG
1
2
P

0

defined. Let q = dheG1 . By definition of L̂, q ∈ L̂GG1 and therefore q ≥ lG1 . Since, by
0
0
definition of Anchored-group, G0 is the composition of granules of G1 and since dheG = lG
2
S
lG2 −1
0 ). Therefore since G0 (l0 ) =
and dheG1 = q, then G1 (q) ⊆ G0 (lG
G
(j)
then
q
<
l
1
G2 .
G2
j=l0
2
G2

0
It follows that lG1 ≤ q < lG2 and hence lG1 6= lG2 . By definition of LeG0 , lG
= k ∈ LeG0 in
2
contrast with the hypothesis.

A.7 Selecting operations
The selecting operations have a common part in the proof for the computation of the period
length and the period label distance.

341

Bettini, Mascetti & Wang

lcm(P

,P

)N

G1 G2
G1
Let be Γ =
. The proof is divided into two steps: first we show that for
PG1
each select operation if i ∈ LG0 then i + Γ ∈ LG0 (details for Select-down, Select-up and
Select-by-intersect operations can be found below). The second step is the application of
Theorem 1. Indeed, for each Select operation, the following holds: ∀i ∈ LG0 G0 (i) = G1 (i);
this implies G1 / G0 . From step 1 follows that i + Γ ∈ LG0 , hence G0 (i + Γ) = G1 (i + Γ).
By this result and considering G1 / G0 , we conclude that G1 /¯ G0 with PGG01 = Γ which is a
multiple of NG1 by definition. Then, by Theorem 1 we have the thesis.

A.8 Proof of Proposition 6
A.8.1 Part 1
See Section A.7.
We prove that if λ ∈ LG0 then λ0 = λ + Γ ∈ LG0 .
By definition of the select-down operation, if λ ∈ LG0 then ∃i ∈ LG2 s.t. λ ∈ ∆lk (S(i))
where S(i) is an ordered set defined as follows: S(i) = {j ∈ LG1 |∅ =
6 G1 (j) ⊆ G2 (i)}.
In order to prove the thesis we need to show that ∃i0 ∈ LG2 |λ0 ∈ ∆lk (S(i0 )). Consider
i0 = i+

lcm(PG1 PG2 )NG2
PG2

we will note that i0 ∈ LG2 (this is trivially derived from the periodicity

of G2 ). To prove that λ0 ∈ ∆lk (S(i0 )) we show that S(i0 ) is obtained from S(i) by adding Γ
to each of its elements.
Indeed note that from periodicity of G1 , ∀j ∈ S(i) if:
τj
[

⊥(jr )

(16)

⊥(jr + lcm(PG1 PG2 ))

(17)

G1 (j) =

r=0

then:
G1 j

0



=

τj
[
r=0

Since j ∈ S(i), G1 (j) ⊆ G2 (i) then, from (16), G2 (i) ⊇
periodicity of G2 :

0



G2 i ⊇

τj
[

⊥(jr + lcm(PG1 PG2 ))

Sτj

r=0 ⊥(jr ).

Moreover, from

(18)

r=0

Since (17) and (18), G2 (i0 ) ⊇ G1 (j 0 ); hence ∀j ∈ S(i), j 0 = (j + Γ) ∈ S(i0 ). Analogously
we can prove that ∀j 0 ∈ S(i0 ), j = (j 0 − Γ) ∈ S(i).
Thus S(i0 ) is obtained from S(i) by adding Γ to each of its elements; therefore if j ∈ S(i)
has position n in S(i), so j 0 ∈ S(i0 ) has position n in S(i0 ). Hence it is trivial that if λ has
position between k and k + l − 1 in S(i), then λ0 has position between k and k + l − 1 in
S(i0 ). Hence if λ ∈ LG0 , then λ0 ∈ LG0 .

342

Mapping Calendar Expressions to Minimal Periodic Sets

A.8.2 Part 2
Let
LeG0 =

o
[ n
P 0
a ∈ A(i)|a ∈ L̂GG1 ;
P 0

i∈L̂GG
2

where ∀i ∈ LG2 :

6 G1 (j) ⊆ G2 (i)}) .
A(i) = ∆lk ({j ∈ LG1 |∅ =

We show that LeG0 = L̂G0 by proving that (1) LeG0 ⊆ L̂G0 and (2) LeG0 ⊇ L̂G0 .
P 0
(1)Suppose by contradiction that ∃q ∈ LeG0 \ L̂G0 . By definition of LeG0 , q ∈ L̂GG1 ;
therefore ∃h with 0 < h ≤ PG0 s.t. dheG1 = q. Moreover, by definition of LeG0 and by
definition of Select-down, LeG0 ⊆ LG0 hence q ∈ LG0 . Since, by definition of Select-down
0
G0 (q) = G1 (q), then dheG = q; hence, by definition of L̂, q ∈ L̂G0 in contradiction with
hypothesis.
(2)Suppose by contradiction that ∃q ∈ L̂G0 \ LeG0 . Since q ∈ L̂G0 then, by definition of
Select-down
6 G1 (j) ⊆ G2 (i)})
∃i ∈ LG2 s.t. q ∈ ∆lk ({j ∈ LG1 |∅ =
therefore, by definition of A(i), q ∈ A(i).
0
Since q ∈ L̂G0 then ∃h with 0 < h ≤ PG0 s.t. dheG = q. By definition of Select-down,
P 0
G0 (q) = G1 (q), then dheG1 = q and therefore q ∈ L̂GG1 . Moreover, since G1 (q) ⊆ G2 (i),
P

P

0

0

P

0

then dheG2 = i and therefore i ∈ L̂GG2 . Since q ∈ A(i), q ∈ L̂GG1 and i ∈ L̂GG2 then, by
definition of LeG0 , q ∈ LeG0 , in contrast with the hypothesis.
A.9 Proof of Proposition 7
A.9.1 Part 1
See Section A.7. We prove that if i ∈ LG0 then i + Γ ∈ LG0 . From the periodicity of G1 ,
i+Γ ∈ LG1 (this is trivially derived from the periodicity of G1 ). Hence we only need to show
that ∃j 0 ∈ LG2 |∅ =
6 G2 (j) ⊆ G1 (i + Γ). Since i ∈ LG0 then ∃j ∈ LG2 |∅ =
6 G2 (j) ⊆ G1 (i).
From the periodicity of G2 , if:
G2 (j) =

τj
[

⊥(jr )

(19)

r=0

then:

 [
τj
lcm(PG1 PG2 )NG2
G2 j +
=
⊥(jr + lcm(PG1 PG2 ))
PG2
r=0

Moreover, from the (19) and since G1 (i) ⊇ G2 (j):
G1 (i) ⊇

τj
[
r=0

From the periodicity of G1 :
343

⊥(jr )

(20)

Bettini, Mascetti & Wang

G1 (i + Γ) ⊇

τj
[

⊥(jr + lcm(PG1 PG2 ))

(21)

r=0


From (20) and (21) follows that G1 (i + Γ) ⊇ G2 j +

lcm(PG1 PG2 )NG2
PG2



, that is the thesis.

A.9.2 Part 2
Let

P 0
6 G2 (j) ⊆ G1 (i)};
LeG0 = {i ∈ L̂GG1 |∃j ∈ LG2 s.t. ∅ =

We show that LeG0 = L̂G0 by proving that (1) LeG0 ⊆ L̂G0 and (2) LeG0 ⊇ L̂G0 .
P 0
(1) Suppose by contradiction that ∃k ∈ LeG0 \ L̂G2 . Since k ∈ LeG0 , then k ∈ L̂GG1 ;
therefore ∃h with 0 < h ≤ PG0 s. t. dheG1 = k. Moreover, by definition of LeG0 and
by definition of Select-down, LeG0 ⊆ LG0 hence q ∈ LG0 . Since, by definition of Select-up,
0
G0 (k) = G1 (k), then dheG = k. Hence, by definition of L̂, k ∈ L̂G0 , in contrast with the
hypothesis.
(2) Suppose by contradiction that ∃k ∈ L̂G0 \ LeG0 . Since k ∈ L̂G0 , then ∃h with 0 <
0
h ≤ PG0 s.t. dheG = k. Since, by definition of Select-up, G0 (k) = G1 (k), then dheG1 = k;
P 0
Therefore, by definition of L̂, k ∈ L̂GG1 . Moreover, since k ∈ L̂G0 and L̂G0 ⊆ LG0 , by
definition of the Select-up operation, then ∃j ∈ LG2 s.t. ∅ 6= G2 (j) ⊆ G1 (k). Hence by
definition of LeG0 , k ∈ LeG0 , in contradiction with hypothesis.
A.10 Proof of Proposition 8
A.10.1 Part 1
See Section A.7. We prove that if λ ∈ LG0 , then λ0 = λ + Γ ∈ LG0 .
By definition of the select-by-intersect operation, if λ ∈ LG0 , then ∃i ∈ LG2 : λ ∈
∆lk (S(i)) where S(i) is an ordered set defined as follows: S(i) = {j ∈ LG1 |G1 (j)∩G2 (i) 6= ∅}.
In order to prove the thesis we need to show that ∃i0 ∈ LG2 : λ0 ∈ ∆lk (S(i0 )). Consider
i0 = i +

lcm(PG1 PG2 )NG2
note that i0 ∈ LG2 (this
PG2
prove that λ0 ∈ ∆lk (S(i0 )) we show that

is trivially derived from the periodicity of

G2 ). To
S(i0 ) is obtained from S(i) by adding Γ to
each of its elements.
Indeed note that ∀j if j ∈ S(i), then G1 (j) ∩ G2 (i) 6= ∅. Hence ∃l ∈ Z : ⊥(l) ⊆ G1 (j)
and ⊥(l) ⊆ G2 (i). From the periodicity of G1 , G1 (j + Γ) ⊇ ⊥(l + lcm(PG1 PG2 )). From
the periodicity of G2 , G2 (i0 ) ⊇ ⊥(l + lcm(PG1 PG2 )). So G1 (j + Γ) ∩ G2 (i0 ) 6= ∅, therefore
∀j ∈ S(i), (j + Γ) ∈ S(i0 ).
Analogously we can prove that ∀j 0 ∈ S(i0 ), (j 0 − Γ) ∈ S(i). Hence S(i0 ) is obtained from
S(i) by adding Γ to each of its elements. Therefore, if j ∈ S(i) has position n in S(i), then
j + Γ ∈ S(i0 ) has position n in S(i0 ); hence if j has position between k and k + l − 1 in S(i),
then also j + Γ has position between k and k + l − 1 in S(i0 ) and so j + Γ ∈ LG0 .
A.10.2 Part 2
The proof is analogous to the ones of Proposition 6.
344

Mapping Calendar Expressions to Minimal Periodic Sets

A.11 Set Operations
A.11.1 Proof of Proposition 9
Given the periodical granularities H and G with G label aligned subgranularity of H, we
NH
G
prove that N
PG = PH . The thesis is proved by considering the common period length of H
and G i.e. Pc = lcm(PG , PH ).
Let NG0 be the difference between the label of the ith granule of one period of G and
the label of the ith granule of the next period, considering Pc as the period length of G.
0 is defined.
Analogously NH
S
S
By periodicity of G, if G(i) = kr=0 ⊥(ir ) then G(i + NG0 ) = kr=0 ⊥(ir + Pc ); since G is
S
an aligned subranularity of H, ∀i ∈ LH H(i) = G(i) = kr=0 ⊥(ij ) and, since H is periodic,
Sk
0 )=
0
0
H(i + NH
r=0 ⊥(ij + Pc ); from which we can easily derive that i + NG = i + NH , hence
0
0
NG = NH .
0 = N 0 , then
From the definition of Pc , ∃α, β ∈ N s. t. αPH = βPG . Moreover, since NH
G
PG
PH
=
.
αNH = βNG . Therefore N
NG
H
A.11.2 Property used in the proofs for set operations
lcm(P

,P

)N

lcm(P

,P

)N

G1 G2
G1
G1 G2
G2
Let Γ1 be
and Γ2 be
. Since G1 and G2 are aligned subgranPG1
PG2
ularity of a certain granularity H, from Proposition 9 we can easily derive that Γ1 = Γ2 .

A.12 Proof of Proposition 10
A.12.1 Part 1
Union. Let Γ1 be

lcm(PG1 ,PG2 )NG2
lcm(PG1 ,PG2 )NG1
and Γ2 be
. The thesis
PG1
PG2
S
Sk
0
0
LG0 if, G (i) = r=0 ⊥(ir ), then G (i + ∆) = kr=0 ⊥(ir

will be proved by

+ lcm(PG1 , PG2 ))
showing that ∀i ∈
0
with ∆ = Γ1 = Γ2 . Since LG = LG1 ∪ LG2 , two cases will be considered:
S
• ∀i ∈ LG1 G0 (i) = G1 (i) = kr=0 ⊥(ir ). From the periodicity of G1 , G1 (i + Γ1 ) =
Sk
Sk
0
r=0 ⊥(ir + lcm(PG1 , PG2 )); hence G (i + Γ1 ) =
r=0 ⊥(ir + lcm(PG1 , PG2 )).
S
• ∀i ∈ LG2 − LG1 G0 (i) = G2 (i) = kr=0 ⊥(ir ). From the periodicity of G2 , G2 (i + Γ2 ) =
Sk
Sk
0
r=0 ⊥(ir + lcm(PG1 , PG2 )); hence G (i + Γ2 ) =
r=0 ⊥(ir + lcm(PG1 , PG2 )).
S
Since Γ1 = Γ2 , then ∀i ∈ LG0 if G0 (i) = kr=0 ⊥(ir ), then G0 (i + Γ1 ) = G0 (i + Γ2 ) =
Sk
r=0 ⊥(ir + lcm(PG1 , PG2 )). Hence, by definition of /¯ , we have the thesis.
S
Intersect. ∀i ∈ LG0 = LG1 ∩ LG2 G0 (i) = G1 (i) = kr=0 ⊥(ir ). From the periodicity of
G1 and G2 , i + Γ1 ∈ LG1 eSi + Γ2 ∈ LG2 ; since Γ1 = Γ2 , then i + Γ1 ∈ LG0 . Moreover
G0 (i + Γ1 ) = G1 (i + Γ1 ) = kr=0 ⊥(ir + lcm(PG1 , PG2 )); hence, by the definition of /¯ , we
have the thesis.
S
Difference. ∀i ∈ LG0 = LG1 − LG2 G0 (i) = G1 (i) = kr=0 ⊥(ir ). Since i ∈ LG1 from the
periodicity of G1 i + Γ1 ∈ LG1 . Since i ∈
/ LG2 , from the periodicity of G2 , i + Γ2 ∈
/ LG2
(if it would exists i + Γ2 ∈ LG2 , from periodicity of G2 would exists i ∈ LG2 that is
0
0
not
Sk possible for hypothesis). Hence i + Γ1 ∈ LG . Moreover G (i + Γ1 ) = G1 (i + Γ1 ) =
r=0 ⊥(ir + lcm(PG1 , PG2 )); hence, by the definition of /¯ , we have the thesis.
345

Bettini, Mascetti & Wang

A.12.2 Part 2
P 0
P 0
Let LeG0 = L̂GG1 ∪ L̂GG2 .
We show that LeG0 = L̂G0 by proving that (1) LeG0 ⊆ L̂G0 and (2) LeG0 ⊇ L̂G0 .
P 0
(1) Suppose by contradiction that ∃k ∈ LeG0 \ L̂G0 . Since k ∈ LeG0 then k ∈ L̂GG1 or
P

0

P

P

0

0

P

0

k ∈ L̂GG2 . Suppose that k ∈ L̂GG1 (the proof is analogous if k ∈ L̂GG2 ). Since k ∈ L̂GG1 , then
0
∃ 0 < h < PG0 s.t. dheG = k. Since, by definition of the Union operation G0 (k) = G1 (k),
0
then dheG = k. Hence, by definition of L̂, k ∈ L̂G0 in contrast with the hypothesis.
(2) Suppose by contradiction that ∃k ∈ L̂G0 \ LeG0 . Since k ∈ L̂G0 , then, by definition of
0
L̂, ∃ 0 < h < PG0 s.t. dheG = k. Moreover, by definition of the Union operation, k ∈ LG1
P 0
P 0
or k ∈ LG2 . Suppose that k ∈ L̂GG1 (the proof is analogous if k ∈ L̂GG2 ). By definition of the
P

0

Union operation, G0 (k) = G1 (k) therefore dheG1 = k and so, by definition of L̂, k ∈ L̂GG1 .
e k ∈ LeG0 in contradiction with the hypothesis.
Hence, by definition of L,

References
Bettini, C., & Mascetti, S. (2005). An efficient algorithm for minimizing time granularity
periodical representations. In Proc. of the 12th International Symposium on Temporal
Representation and Reasoning (TIME), pp. 20–25. IEEE Computer Society.
Bettini, C., Mascetti, S., & Pupillo, V. (2005). A system prototype for solving multigranularity temporal csp. In Recent Advances in Constraints, Revised selected papers from the Workshop on Constraint Solving and Constraint Logic Programming
(CSCLP), volume 3419 of Lecture Notes in Computer Science, pp. 142–156. Springer.
Bettini, C., Mascetti, S., & Wang., X. S. (2004). Mapping calendar expressions into periodical granularities. In Proc. of the 11th International Symposium on Temporal
Representation and Reasoning (TIME), pp. 96–102. IEEE Computer Society.
Bettini, C., & Sibi, R. D. (2000). Symbolic representation of user-defined time granularities.
Annals of Mathematics and Artificial Intelligence, 30 (1-4), 53–92.
Bettini, C., Wang, X. S., & Jajodia, S. (2000). Time Granularities in Databases, Data
Mining, and Temporal Reasoning. Springer.
Bettini, C., Wang, X. S., & Jajodia, S. (2002a). Solving multi-granularity temporal constraint networks. Artificial Intelligence, 140 (1/2), 107–152.
Bettini, C., Wang, X. S., & Jajodia, S. (2002b). Temporal reasoning in workflow systems.
Distributed and Parallel Databases, 11 (3), 269–306.
Bresolin, D., Montanari, A., & Puppis, G. (2004). Time granularities and ultimately periodic
automata. In Proc. of the 9th European Conference on Logics in Artificial Intelligence
(JELIA) volume 3229 of Lecture Notes in Computer Science, pp. 513–525. Springer.
Chandra, R., Segev, A., & Stonebraker, M. (1994). Implementing calendars and temporal
rules in next generation databases. In Proc. of the 10th International Conference on
Data Engineering (ICDE), pp. 264–273. IEEE Computer Society.
Combi, C., Franceschet, M., & Peron, A. (2004). Representing and reasoning about temporal
granularities. Journal of Logic and Computation, 14 (1), 51–77.
346

Mapping Calendar Expressions to Minimal Periodic Sets

Combi, C., & Pozzi, G. (2003). Temporal conceptual modelling of workflows. In Proc. of the
22nd International Conference on Conceptual Modeling (ER) volume 2813 of Lecture
Notes in Computer Science, pp. 59–76. Springer.
Cukierman, D., & Delgrande, J. P. (1998). Expressing time intervals and repetition within
a formalization of calendars. Computational Intelligence, 14, 563–597.
Dal Lago, U., & Montanari, A. (2001). Calendars, time granularities, and automata. In Proc.
of the 7th International Symposium on Spatial and Temporal Databases (SSTD),
volume 2121 of Lecture Notes in Computer Science, pp. 279–298. Springer.
Dal Lago, U., Montanari, A., & Puppis, G. (2003). Towards compact and tractable
automaton-based representations of time granularities. In Proc. of the 8th Italian
Conference on Theoretical Computer Science (ICTCS), volume 2841 of Lecture Notes
in Computer Science, pp. 72–85. Springer.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49 (1-3), 61–95.
Kabanza, F., Stevenne, J. M., & Wolper, P. (1990). Handling infinite temporal data. In Proc.
of the 9th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database
Systems (PODS), pp. 392–403. ACM Press.
Koomen, J. (1991). Reasoning about recurrence. International Journal of Intelligent Systems, 6, 461–496.
Ladkin, P. B. (1986). Primitives and units for time specification. In Proc. of the 5th National
Conference on Artificial Intelligence (AAAI), pp. 353–359. Morgan Kaufmann.
Leban, B., McDonald, D., & Forster, D. (1986). A representation for collections of temporal
intervals. In Proc. of the 5th National Conference on Artificial Intelligence (AAAI),
pp. 367–371. Morgan Kaufmann.
Li, Y., Ning, P., Wang, X. S., & Jajodia, S. (2001). Discovering calendar-based temporal
association rules. In Proc. of the 8th International Symposium on Temporal Representation and Reasoning (TIME), pp. 111–118. IEEE Computer Society.
Montanari, A. (1996). Metric and Layered Temporal Logic for Time Granularity. Ph.D.
thesis, ILLC Dissertation Series 1996-02, University of Amsterdam.
Morris, R., Shoaff, W., & Khatib, L. (1996). Domain-independent temporal reasoning with
recurring events. Computational Intelligence, 12, 450–477.
Niezette, M., & Stevenne, J. M. (1992). An efficient symbolic representation of periodic
time. In Proc. of the first International Conference on Information and Knowledge
Management (CIKM) volume 725 of Lecture Notes in Computer Science, pp. 161–168.
Springer.
Ning, P., Wang, X. S., & Jajodia, S. (2002). An algebraic representation of calendars.
Annals of Mathematics and Artificial Intelligence, 36 (1-2), 5–38.
Puppis, G. (2006). Automata for Branching and Layered Temporal Structures. Ph.D. thesis,
Università degli Studi di Udine.
Terenziani, P. (2003). Symbolic user-defined periodicity in temporal relational databases.
IEEE Transactions of Knowledge and Data Engineering, 15 (2), 489–509.
347

Bettini, Mascetti & Wang

Tuzhilin, A., & Clifford, J. (1995). On periodicity in temporal databases. Information
Systems, 20 (8), 619–639.
Urgun, B., Dyreson, C. E., Snodgrass, R. T., Miller, J. K., Soo, M. D., Kline, N., & Jensen,
C. S. (2007). Integrating multiple calendars using TauZaman. Software-Practice
Experience, to appear.
Wijsen, J. (2000). A string-based model for infinite granularities. In Spatial and Temporal
Granularity: Papers from the AAAI Workshop. Technical Report WS-00-08, pp. 9–16.
AAAI Press.

348

Journal of Artificial Intelligence Research 28 (2007) 107-118

Submitted 9/05; published 2/07

Generating Hard Satisfiable Formulas by Hiding
Solutions Deceptively
Haixia Jia

hjia@cs.unm.edu

Computer Science Department
University of New Mexico

Cristopher Moore

moore@santafe.edu

Computer Science Department
University of New Mexico

Doug Strain

doug.strain@gmail.com

Computer Science Department
University of New Mexico

Abstract
To test incomplete search algorithms for constraint satisfaction problems such as 3SAT, we need a source of hard, but satisfiable, benchmark instances. A simple way to do
this is to choose a random truth assignment A, and then choose clauses randomly from
among those satisfied by A. However, this method tends to produce easy problems, since
the majority of literals point toward the “hidden” assignment A. Last year, Achlioptas,
Jia and Moore proposed a problem generator that cancels this effect by hiding both A
and its complement A (Achlioptas, Jia, & Moore, 2004). While the resulting formulas
appear to be just as hard for DPLL algorithms as random 3-SAT formulas with no hidden
assignment, they can be solved by WalkSAT in only polynomial time.
Here we propose a new method to cancel the attraction to A, by choosing a clause with
t > 0 literals satisfied by A with probability proportional to q t for some q < 1. By varying
q, we can generate formulas whose variables have no bias, i.e., which are equally likely to be
true or false; we can even cause the formula to “deceptively” point away from A. We present
theoretical and experimental results suggesting that these formulas are exponentially hard
both for DPLL algorithms and for incomplete algorithms such as WalkSAT.

1. Introduction
To evaluate search algorithms for constraint satisfaction problems, we need good sources
of benchmark instances. Real-world problems are the best benchmarks by definition, but
each such problem has structures specific to its application domain; in addition, if we
wish to study how the running times of search algorithms scale, we need entire families of
benchmarks with varying size and density.
One way to fill this need is to generate random instances. For instance, for 3-SAT we
can generate instances
 with n variables and m clauses by choosing each clause uniformly
from among the 8 n3 possibilities. We can then vary these formulas according to their
size and their density r = m/n. While such formulas lack much of the structure of realworld instances, they have been instrumental in the development and study of new search
methods such as simulated annealing (Johnson, Aragon, McGeoch, & Shevon, 1989), the
c
2007
AI Access Foundation. All rights reserved.

Jia, Moore, & Strain

breakout procedure (Morris, 1993), WalkSAT (Selman, Kautz, & Cohen, 1996), and Survey
Propagation (Mézard & Zecchina, 2002).
However, if we wish to test incomplete algorithms such as WalkSAT and Survey Propagation (SP), we need a source problems that are hard but satisfiable. In contrast, above a critical density r ≈ 4.27, the random formulas defined above are almost certainly unsatisfiable.
Random formulas at this threshold appear to be quite hard for complete solvers (Cheeseman, Kanefsky, & Taylor, 1991; Mitchell, Selman, & Levesque, 1992; Hogg, Huberman, &
Williams, 1996); but for precisely this reason, it is not feasible to generate large problems
at the threshold and then filter out the unsatisfiable ones. While other classes of satisfiable
CSPs have been proposed, such as the quasigroup completion problem (Shaw, Stergiou, &
Walsh, 1998; Kautz, Ruan, Achlioptas, Gomes, Selman, & Stickel, 2001; Achlioptas, Gomes,
Kautz, & Selman, 2000), we would like to have problems generators that are “native” to
3-SAT.
A natural way to generate random satisfiable 3-SAT formulas is to choose a random
truth assignment
A ∈ {0, 1}n , and then choose m clauses uniformly and independently from
n
among the 7 3 clauses satisfied by A. The problem with this is that simply rejecting clauses
that conflict with A causes an unbalanced distribution of literals; in particular, on average
a literal will agree with its value in the hidden assignment 4/7 of the time. Thus, especially
when there are many clauses, a simple majority heuristic or local search will quickly find
A. More sophisticated versions of this “hidden assignment” scheme (Asahiro, Iwama, &
Miyano, 1996; Van Gelder, 1993) improve matters somewhat but still lead to biased samples.
Thus the question is how to avoid this “attraction” to the hidden assignment,
One approach (Achlioptas et al., 2004) is to choose clauses uniformly from among those
that are satisfied by both A and its complement A. This is inspired by recent work on
random k-SAT and Not-All-Equal SAT (Achlioptas & Moore, 2002b), in which symmetry
with respect to complementation reduces the variance of the number of solutions; the idea
is that A and A cancel each others’ attractions out, making either one hard to find. Indeed,
the resulting formulas appear to take DPLL solvers exponential time and, in general, to
be just as hard as random 3-SAT formulas with no hidden assignment. On the other
hand, WalkSAT solves these formulas in polynomial time, since after a few variables are set
in a way that agrees with one of the hidden assignments, neighboring variables develop
correlations consistent with these (Barthel, Hartmann, Leone, Ricci-Tersenghi, Weigt, &
Zecchina, 2002).
In this paper, we pursue an alternate approach, inspired by Achlioptas and Preres, who
reweighted the satisfying assignments in a natural way (Achlioptas & Peres, 2003). We hide
just one assignment, but we bias the distribution of clauses as follows:
1. Predefine a constant q < 1 and generate a random truth assignment A ∈ {0, 1}n
2. Do rn times: choose a random k-tuple of variables, and choose from among the clauses
in which t > 0 literals are satisfied by A with probability proportional to q t .
This penalizes the clauses which are “more satisfied” by A, and reduces the extent to which
variable occurrences are more likely to agree with A. (Note that the naive formulas discussed
above amount to the case q = 1.) As we will see below, by choosing q appropriately we can
rebalance the distribution of literals, so that each variable is as likely to appear positively
108

Generating Hard Satisfiable Formulas by Hiding Solutions Deceptively

as often as negatively and no longer points toward its value in A. By reducing q further,
we can even make it more likely that a variable occurrence disagrees with A, so that the
formula becomes “deceptive” and points away from the hidden assignment.
We call these formulas “q-hidden,” to distinguish them from the naive “1-hidden” formulas discussed above, the “2-hidden” formulas studied by Achlioptas, Jia and Moore (Achlioptas et al., 2004), and the “0-hidden” formulas consisting of random 3-SAT formulas with no
hidden assignment. Like these other families, our q-hidden formulas are readily amenable to
all the mathematical tools that have been developed for studying random k-SAT formulas,
including moment calculations and the method of differential equations. Below we calculate
the expected density of satisfying assignments as a function of their distance from A, and
analyze the behavior of the Unit Clause (UC) algorithm on q-hidden formulas. We then
present experiments on several complete and incomplete solvers. For certain values of q, we
find that our q-hidden formulas are just as hard or harder for DPLL algorithms as 0-hidden
formulas or 2-hidden formulas, and are much harder than naive 1-hidden formulas. In addition, we find that local search algorithms like WalkSAT find our formulas much harder than
any of these other families, taking exponential as opposed to polynomial time. Moreover,
the running time of WalkSAT increases sharply as our formulas become more deceptive.

2. The Expected Density of Solutions and the Bias of Local Search
For α ∈ [0, 1], let Xα be the number of satisfying truth assignments in a random q-hidden
k-SAT formula that agree on a fraction α of the variables with the hidden assignment A;
that is, their Hamming distance from A is (1 − α)n. We wish to calculate the expectation
E[Xα ].
By symmetry, we can take A to be the all-true assignment. In that case, a clause with
t > 0 positive literals is chosen with probability q t /((1 + q)k − 1) (here we normalize the
probabilities by summing over the kt clauses for all t > 0). Let B be a truth assignment
where αn of the variables are true and (1−α)n are false. Then, analogous to the calculation
by Achlioptas, Jia and Moore (Achlioptas et al., 2004), we use linearity of expectation, independence between clauses, the selection of the literals in each clause with replacement, and
Stirling’s approximation for the factorial to obtain (where ∼ suppresses terms polynomial
in n):

n
Pr[B satisfies a random clause]m
αn
!m
 
k   t
X
k q (1 − α)t αk−t
n
1−
=
t (1 + q)k − 1
αn
t=1

E[Xα ] =



∼ fk,r,q (α)n

where
f (α) =

1
αα (1 − α)1−α



1−

(q(1 − α) + α)k − αk
(1 + q)k − 1

r

.

Looking at Figure 1, we see that the behavior of f near α = 1/2 changes dramatically
as we vary q. For q = 1 (i.e., naive 1-hidden formulas), f 0 (1/2) is positive. On the other
109

Jia, Moore, & Strain

Density of solutions with q=0.5

Density of solutions with r=6
1.1

1.4
r=3

1

1.3

q=0.5

0.9

1.2

r=4

q=0.618
0.8

1.1

0.7

1

r=5

0.6

r=5.6

q=1

0.9
r=6

0.5

0.8

0.4

0.7

r=7

0.3
0

0.2

0.4

α

0.6

0.8

0.6
0

1

0.2

0.4

α

0.6

0.8

1

Figure 1: The nth root f (α) of the expected number of solutions which agree with the
hidden assignment on a fraction α of the variables. Here k = 3. The left part of
the figure shows f (α) for q = 1, q = 0.618 and q = 0.5 at r = 6. The right part
shows f (α) for q = 0.5 and varying r. Note that at r = 5.6, we have f (α) < 1 for
all α ≤ 1/2.
hand, if q is the positive root q ∗ of
1 − (1 − q)(1 + q)k−1 = 0

(1)

∗
∗
then f 0 (1/2) =
√ 0. We call the resulting q -hidden formulas balanced; for k = 3, q is the
golden ratio ( 5 − 1)/2 = 0.618...
This choice of q affects local search algorithms such as WalkSAT in the following way.
If we start with a random assignment B, a step of WalkSAT chooses a random unsatisfied
clause, and satisfies a literal ` chosen randomly from that clause. The expected change
in the Hamming distance d(A, B) is then the probability that ` agrees with A, minus the
probability that it doesn’t. Since all the clauses are equally likely to be unsatisfied by a
random assignment, this is the expectation of 2t/k − 1 in a random clause, namely

E[∆d(A, B)] =

k   t
X
k q (2t/k − 1)
t=1

t (1 + q)k − 1

=

1 − (1 − q)(1 + q)k−1
.
(1 + q)k − 1

Thus is zero when (1) holds, in which case WalkSAT is equally likely to move toward or away
from A. Thus, analogous to the calculation by Achlioptas and Peres (Achlioptas & Peres,
2003), when q = q ∗ a given literal is equally likely to agree or disagree with A, and WalkSAT
has no information about in which direction the hidden assignment lies. (This argument
applies to the first o(n1/2 ) steps of WalkSAT, since until then it is unlikely to have seen any
variable twice).
110

Generating Hard Satisfiable Formulas by Hiding Solutions Deceptively

For smaller values of q such as q = 0.5 shown in Figure 1, f 0 (1/2) becomes negative,
and we expect a local search algorithm starting at a random assignment to move away from
A. Indeed, f (α) has a local maximum at some α < 1/2, and for small r there are solutions
with α < 1/2. When r is sufficiently large, however, f (α) < 1 for all α < 1/2, and as
n → ∞ the probability any of these “alternate” solutions exist is exponentially small. We
conjecture that for each q ≤ q ∗ there is a threshold rc (q) at which with high probability
the only solutions are those close to A. Setting max{f (α) | α ≤ 1/2} = 1 yields an upper
bound on rc (q), which we show in Figure 4 below. For instance, the dotted line in Figure 1
shows that rc (0.5) ≤ 5.6.
We call such formulas deceptive, since local search algorithms such as WalkSAT, DPLL
algorithms such as zChaff that use a majority heuristic in their splitting rule, and messagepassing algorithms such as SP will presumably search in the wrong direction, and take exponential time to cross the local minimum in f (α) to find the hidden assignment. Our
experiments below appear to confirm this intuition. In addition, all three types of algorithms appear to encounter the most difficulty at roughly the same density rc (q), where we
conjecture the “alternate” solutions disappear.

3. Unit Clause Heuristic and DPLL Algorithms
Unit Clause (UC) is a linear-time heuristic which permanently sets one variable in each
step as follows: if there are any unit clauses, satisfy them; otherwise, pick a random literal
and satisfy it. For random 3-SAT formulas, UC succeeds with constant probability for
r < 8/3, and fails with high probability for r > 8/3 (Chao & Franco, 1986). UC can
be thought as the first branch of a simple DPLL algorithm S, whose splitting rule takes
a random unset variable and tries its truth values in random order; thus UC succeeds if S
succeeds without backtracking. On the other hand, it was showed that S’s expected running
time is exponential in n for any r > 8/3 (Cocco & Monasson, 2004; Cocco, Monasson,
Montanari, & Semerjian, 2005); also Achlioptas, Beame and Molloy used lower bounds
on resolution complexity to show that S takes exponential time with high probability if
r > 3.81 (Achlioptas, Beame, & Molloy, 2001). In general, it appears that simple DPLL
algorithms begin to take exponential time at exactly the density where the corresponding
linear-time heuristic fails.
In this section, we analyze the performance of UC on our q-hidden formulas. Specifically,
we show that in the balanced case where q = q ∗ , UC fails for r > 8/3 just as it does for
0-hidden formulas. Based on this, we conjecture that the running time of S, and other
simple DPLL algorithms, is exponentially large for our formulas at the same density as for
0-hidden ones.
We analyze the behavior of UC on arbitrary initial distributions of 3-SAT clauses using
the method of differential equations. For simplicity we assume that A is the all-true assignment. A round of UC consists of a “free step,” in which we satisfy a random literal, and the
ensuing chain of unit-clause propagations. For 0 ≤ i ≤ 3 and 0 ≤ j ≤ i, let Si,j = si,j n be
the number
of clauses of length i with j positive literals and i − j negative ones, and let
P
si = j si,j . Let X = xn be the number of variables set so far, and let mT and mF be the
expected number of variables set true and false in a round. Then we can model the discrete
111

Jia, Moore, & Strain

stochastic process of the Si,j with the following differential equations for the si,j :
ds3,j
dx
ds2,j
dx

3s3,j
1−x
2s2,j
mF (j + 1)s3,j+1 + mT (3 − j)s3,j
= −
+
1−x
(mT + mF )(1 − x)
= −

(2)

The unit clauses are governed by a two-type branching process, with transition matrix


1
s2,1 2s2,0
M=
.
1 − x 2s2,2 s2,1
As in the calculation by Achlioptas and Moore (Achlioptas & Moore, 2002a), as long as the
largest eigenvalue of M is less than 1, the branching process is subcritical, and summing
over the round gives




1/2
mF
−1
= (I − M ) ·
.
1/2
mT
We then solve the equation (2) with the initial conditions s3,0 = 0 and
 
qj
3
s3,j =
j (1 + q)3 − 1
for 0 < j ≤ 3. In the balanced case q = q ∗ , we find that UC succeeds on q-hidden formulas
with constant probability if and only if r < 8/3, just as for 0-hidden formulas. The reason
is that, as for 2-hidden formulas, the expected number of positive and negative literals are
the same throughout the process. This symmetry causes UC to behave just as it would on
random 3-SAT formulas without a hidden assignment.
We note that for q < q ∗ , UC succeeds at slightly higher densities, at which it can find
one of the “alternate” solutions with α < 1/2. At higher densities where these alternate
solutions disappear, our experimental results below show that these “deceptive” formulas
take DPLL algorithms exponential time, and for r > rc (q) they are harder than 0-hidden
formulas of the same density.

4. Experimental Results
4.1 DPLL
In this section we discuss the behavior of DPLL solvers on our q-hidden formulas. We focus
on zChaff (Zhang, 2002); the behavior of OKsolver (Kullmann, 2002) is similar. Figure 2
shows zChaff’s running time on 0-hidden, 1-hidden, 2-hidden, and q-hidden formulas for
various values of q.
Balanced formulas, i.e. with q = q ∗ = 0.618..., appear to be about as hard as 0-hidden
ones, including above the satisfiability threshold r ≈ 4.27 where 0-hidden formulas become
unsatisfiable. Like 0-hidden formulas, these q ∗ -hidden formulas appear to peak in complexity near the satisfiability threshold. This is consistent with the picture given in the previous
two sections: namely, that these “balanced” formulas make it impossible for algorithms to
feel the attraction of the hidden assignment. In contrast, naive 1-hidden formulas are far
easier, since the attraction to the hidden assignment is strong.
112

Generating Hard Satisfiable Formulas by Hiding Solutions Deceptively

zChaff performance with n=200

zChaff performance with r=5.5

5

10

4

10

Median number of Decisions over 49 trials

Median number of Decisions over 49 trials

6

10

q=0.2
q=0.3
q=0.4
q=0.5
q=0.618
1−hidden
2−hidden
0−hidden

3

10

2

10

1

10

4

5

10

q=0.3
0−hidden
q=0.618
2−hidden
1−hidden

4

10

3

10

2

10

1

4.5

5

5.5

6
r

6.5

7

7.5

10
50

8

100

150

200

250

300

N

Figure 2: The left part of the figure shows zChaff’s median running time over 49 trials on
0-hidden, 1-hidden, 2-hidden and q-hidden formulas with n = 200 and r ranging
from 4.0 to 8.0. The right part shows the median running time with r = 5.5
and n ranging from 50 to 300. Note that 0-hidden formulas are almost always
unsatisfiable for r > 4.27.

Deceptive formulas, i.e. with q < q ∗ , appear to have two phases. At low density they
are relatively easy, and their hardness peaks at a density rc (q). Above rc (q) they take
exponential time; as for 0-hidden formulas, as r increases further the coefficient of the
exponential decreases as the clauses generate contradictions more quickly.
We believe that this peak rc (q) is the same threshold density defined earlier (see Figure 4
below) above which the only solutions are those close to the hidden assignment. The
situation seems to be the following: below rc (q), there are “alternate” solutions with α <
1/2, and zChaff is led to these by its splitting rule. Above rc (q), these alternate solutions
disappear, and zChaff takes exponential time to find the vicinity of the hidden assignment,
since the formula deceptively points in the other direction. Moreover, for a fixed r above
rc (q) these formulas become harder as q decreases and they become more deceptive.
To illustrate this further, the right part of Figure 2 shows zChaff’s median running
time on 0-hidden formulas, 1-hidden formulas, 2-hidden formulas, and q-hidden formulas
for q = q ∗ (balanced) and q = 0.3 (deceptive). We fix r = 5.5, which appears to be above
rc (q) for both these values of q. At this density, the 0-hidden, 2-hidden, and balanced
q-hidden formulas are all comparable in difficulty, while 1-hidden formulas are much easier
and the deceptive formulas appear to be somewhat harder.
4.2 SP
Survey Propagation or SP (Mézard & Zecchina, 2002) is a recently introduced incomplete
solver based on insights from the replica method of statistical physics and a generalization of
belief propagation. We tested SP on 0-hidden formulas and q-hidden formulas for different
values of q, using n = 104 and varying r. For 0-hidden formulas, SP succeeds up to r = 4.25,
113

Jia, Moore, & Strain

quite close to the satisfiability threshold. For q-hidden formulas with q = q ∗ , SP fails at
4.25 just as it does for 0-hidden formulas, suggesting that it finds these formulas exactly as
hard as 0-hidden ones even though they are guaranteed to be satisfiable. For naive 1-hidden
formulas, SP succeeds at a significantly higher density, up to r = 5.6.
Presumably the naive 1-hidden formulas are easier for SP since the “messages” from
clauses to variables, like the majority heuristic, tend to push the algorithm towards the
hidden assignment. In the balanced case q = q ∗ , this attraction is successfully suppressed,
causing SP to fail at essentially the same density as for 0-hidden formulas, close to the satisfiability threshold, even though our q-hidden formulas continue to be satisfiable at all densities. In contrast, the 2-hidden formulas proposed by Achlioptas, Jia and Moore (Achlioptas
et al., 2004) are solved by SP up to a somewhat higher density r ≈ 4.8. Thus it seems that
the reweighting approach of q-hidden formulas does a better job of confusing SP than hiding
two complementary assignments does.
For q < q ∗ , SP succeeds up to somewhat higher densities, each of which matches quite
closely the value rc (q) at which zChaff’s running time peaks (see Figure 4 below). Building
on our conjecture that this is the density above which the only solutions are those close
to the hidden assignment, we guess that SP succeeds for r < rc (q) precisely because the
local gradient in the density of solutions pushes it towards the “alternate” solutions with
α < 1/2. Above rc (q), these solutions no longer exist, and SP fails because the clauses send
deceptive messages, demanding that variables be set opposite to the hidden assignment.
4.3 WalkSAT
We conclude with a local search algorithm, WalkSAT. For each formula, we did up to 104
restarts, with 104 steps per attempt, where each step does a random or greedy flip with
equal probability. In the left part of Figure 3 we measure WalkSAT’s performance on 1hidden, 2-hidden, and q-hidden formulas with various values of q. We use n = 200 and r
range from 4 to 8. Even for these relatively small formulas, we see that for the three most
deceptive values of q, there is a density at which the median running time jumps to 108
flips. For instance, q-hidden formulas with q = 0.4 appear to be unfeasible for WalkSAT for,
say, r > 5.
We believe that, consistent with the discussion above, local search algorithms like
WalkSAT greedily follow the gradient in the density of solutions f (α). For q < q ∗ , this
gradient is deceptive, and lures WalkSAT away from the hidden assignment. At densities
below rc (q), there are many alternate solutions with α < 1/2 and WalkSAT finds one of
them very easily; but for densities above rc (q), the only solutions are those near the hidden
assignment, and WalkSAT’s greed causes it to wander for an exponentially long time in the
wrong region. This picture is supported by the fact that, as Figure 4 shows below, the density at which WalkSAT’s running time jumps upward closely matches the thresholds rc (q)
that we observed for zChaff and SP.
The right part of Figure 3 looks at WalkSAT’s median running time at a fixed density
as a function of n. We compare 1-hidden and 2-hidden formulas with q-hidden ones with
q = q ∗ and two deceptive values, 0.5 and 0.3. We choose r = 5.5, which is above rc (q)
for all three values of q. The running time of 1-hidden and 2-hidden formulas is only
polynomial (Achlioptas et al., 2004; Barthel et al., 2002). In contrast, even in the balanced
114

Generating Hard Satisfiable Formulas by Hiding Solutions Deceptively

case q = q ∗ , the running time is exponential, and the slope of this exponential increases
dramatically as we decrease q and make the formulas more deceptive. We note that it
might be possible to develop a heuristic analysis of WalkSAT’s running time in the deceptive
case (Semerjian & Monasson, 2003; Cocco et al., 2005).

WalkSAT performance with n=200

8

8

7

10

6

10

5

10

q=0.2
q=0.3
q=0.4
q=0.5
q=0.618
1−hidden
2−hidden

4

10

3

10

2

10

WalkSAT performance with r=5.5

10
Median number of flips over 49 trials

Median number of flips over 49 trials

10

4

7

10

q=0.3
q=0.5
q=0.618
2−hidden
1−hidden

6

10

5

10

4

10

3

10

2

4.5

5

5.5

6
r

6.5

7

7.5

10
50

8

100 150 200 250 300 350 400 450 500 550 600
N

Figure 3: The left part of the figure shows WalkSAT’s median running time over 49 trials
with n = 200 and r ranging from 4 to 8; the right part shows the median running
time with r = 5.5 and n ranging from 50 to 600.

5. The Threshold Density
As we have seen, there appears to be a characteristic density rc (q) for each value of q ≤ q ∗ at
which the running time of DPLL algorithms like zChaff peaks, at which WalkSAT’s running
time becomes exponential, and at which SP ceases to work. We conjecture that in all three
cases, the key phenomenon at this density is that the solutions with α < 1/2 disappear,
leaving only those close to the hidden assignment. Figure 4 shows our measured values of
rc (q), and indeed they are quite close for the three algorithms. We also show the analytic
upper bound on rc (q) resulting from setting max{f (α) | α ≤ 1/2} = 1, above which the
expected number of solutions with α ≤ 1/2 is exponentially small.

6. Conclusions
We have introduced a simple new way to hide solutions in 3-SAT problems that produces
instances that are both hard and satisfiable. Unlike the 2-hidden formulas proposed by
Achlioptas, Jia and Moore (Achlioptas et al., 2004) where the attraction of the hidden
assignment is cancelled by also hiding its complement, here we eliminate this attraction
by reweighting the distribution of clauses as proposed by Achlioptas and Peres (Achlioptas
& Peres, 2003). Indeed, by going beyond the value of the parameter q that makes our
q-hidden formulas balanced, we can create deceptive formulas that lead algorithms in the
wrong direction. Experimentally, our formulas are as hard or harder for DPLL algorithms
115

Jia, Moore, & Strain

12
Upper bound
zChaff
SP
WalkSAT

11
10

rc(q)

9
8
7
6
5
4
0.2

0.3

0.4
q

0.5

0.6

Figure 4: The density rc (q) at which the running time of zChaff peaks, WalkSAT peaks or
exceeds 108 flips, and SP stops working. We conjecture all of these events occur
because at this density the alternate solutions with α < 1/2 disappear, leaving
only those close to the hidden assignment. Shown also is the analytic upper
bound described in the text.

as 0-hidden formulas, i.e., random 3-SAT formulas without a hidden assignment; for local
search algorithms like WalkSAT, they are much harder than 0-hidden or 2-hidden formulas,
taking exponential rather than polynomial time. Our formulas are also amenable to all the
mathematical tools developed for the study of random 3-SAT; here we have calculated their
expected density of solutions as a function of distance from the hidden assignment, and
used the method of differential equations to show that UC fails for them at the same density
as it does for 0-hidden formulas.
We close with several exciting directions for future work:
1. Confirm that there is a single threshold density rc (q) at which a) the alternate solutions
far from the hidden assignment disappear, b) the running time of DPLL algorithms
is maximized, c) SP stops working, and d) the running time of WalkSAT becomes
exponential;
2. Prove that simple DPLL algorithms take exponential time for r > rc (q), in expectation
or with high probability;
3. Calculate the variance of the number of solutions as a function of α, and giving
improved upper and lower bounds on the distribution of solutions and rc (q).

Acknowledgments
H.J. is supported by an NSF Graduate Fellowship. C.M. and D.S. are supported by NSF
grants CCR-0220070, EIA-0218563, and PHY-0200909. C.M. thanks Tracy Conrad and
Rosemary Moore for their support.
116

Generating Hard Satisfiable Formulas by Hiding Solutions Deceptively

References
Achlioptas, D., Beame, P., & Molloy, M. (2001). A sharp threshold in proof complexity. In
Proc. STOC, pp. 337–346.
Achlioptas, D., Gomes, C., Kautz, H., & Selman, B. (2000). Generating satisfiable problem
instances. In Proc. AAAI, pp. 256–261.
Achlioptas, D., Jia, H., & Moore, C. (2004). Hiding satisfying assignments: two are better
than one. In Proc. AAAI, pp. 131–136.
Achlioptas, D., & Moore, C. (2002a). Almost all graphs with average degree 4 are 3colorable. In Proc. STOC, pp. 199–208.
Achlioptas, D., & Moore, C. (2002b). The asymptotic order of the random k-SAT threshold.
In Proc. FOCS, pp. 779–788.
Achlioptas, D., & Peres, Y. (2003). The threshold for random k-SAT is 2k (ln 2 − o(k)). In
Proc. STOC, pp. 223–231.
Asahiro, Y., Iwama, K., & Miyano, E. (1996). Random generation of test instances with
controlled attributes. DIMACS Series in Disc. Math. and Theor. Comp. Sci., 26.
Barthel, W., Hartmann, A., Leone, M., Ricci-Tersenghi, F., Weigt, M., & Zecchina, R.
(2002). Hiding solutions in random satisfiability problems: A statistical mechanics
approach. Phys. Rev. Lett., 88 (188701).
Chao, M., & Franco, J. (1986). Probabilistic analysis of two heuristics for the 3-satisfiability
problem. SIAM J. Comput., 15 (4), 1106–1118.
Cheeseman, P., Kanefsky, R., & Taylor, W. (1991). Where the really hard problems are. In
Proc. IJCAI, pp. 163–169.
Cocco, S., & Monasson, R. (2004). Heuristic average-case analysis of the backtrack resolution of random 3-satisfiability instances. Theor. Comp. Sci., 320, 345–372.
Cocco, S., Monasson, R., Montanari, A., & Semerjian, G. (2005). Approximate analysis of
search algorithms with “physical” methods. In Percus, A., Istrate, G., & Moore, C.
(Eds.), Computational Complexity and Statistical Physics. Oxford University Press.
Hogg, T., Huberman, B., & Williams, C. (1996). Phase transitions and complexity. Artificial
Intelligence, 81.
Johnson, D., Aragon, C., McGeoch, L., & Shevon, C. (1989). Optimization by simulated
annealing: an experimental evaluation. Operations Research, 37 (6), 865–892.
Kautz, H., Ruan, Y., Achlioptas, D., Gomes, C., Selman, B., & Stickel, . (2001). Balance
and filtering in structured satisfiable problems. In Proc. IJCAI, pp. 351–358.
Kullmann, O. (2002). Investigating the behaviour of a SAT solver on random formulas.
Tech. rep. CSR 23-2002, University of Wales Swansea.
Mézard, M., & Zecchina, R. (2002). Random k-satisfiability: from an analytic solution to a
new efficient algorithm. Phys. Rev. E, 66, 056126.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of SAT
problems. In Proc. AAAI, pp. 459–465.
117

Jia, Moore, & Strain

Morris, P. (1993). The breakout method for escaping from local minima. In Proc. AAAI,
pp. 40–45.
Selman, B., Kautz, H., & Cohen, B. (1996). Local search strategies for satisfiability testing.
In Proc. 2nd DIMACS Challange on Cliques, Coloring, and Satisfiability.
Semerjian, G., & Monasson, R. (2003). A study of pure random walk on random satisfiability
problems with “physical” methods. LNCS, 2919, 120–134.
Shaw, P., Stergiou, K., & Walsh, T. (1998). Arc consistency and quasigroup completion. In
Proc. ECAI, workshop on binary constraints.
Van Gelder, A. (1993). Problem generator mkcnf.c. In Proc. DIMACS. Challenge archive.
Zhang, L. (2002). zChaff.. ee.princeton.edu/˜chaff/zchaff.php.

118

Journal of Artificial Intelligence Research 28 (2007) 183–232

Submitted 5/06; published 3/07

Proactive Algorithms for Job Shop Scheduling with
Probabilistic Durations
J. Christopher Beck

jcb@mie.utoronto.ca

Department of Mechanical & Industrial Engineering
University of Toronto, Canada

Nic Wilson

n.wilson@4c.ucc.ie

Cork Constraint Computation Centre
University College Cork, Ireland

Abstract
Most classical scheduling formulations assume a fixed and known duration for each activity. In this paper, we weaken this assumption, requiring instead that each duration can
be represented by an independent random variable with a known mean and variance. The
best solutions are ones which have a high probability of achieving a good makespan. We
first create a theoretical framework, formally showing how Monte Carlo simulation can be
combined with deterministic scheduling algorithms to solve this problem. We propose an
associated deterministic scheduling problem whose solution is proved, under certain conditions, to be a lower bound for the probabilistic problem. We then propose and investigate
a number of techniques for solving such problems based on combinations of Monte Carlo
simulation, solutions to the associated deterministic problem, and either constraint programming or tabu search. Our empirical results demonstrate that a combination of the use
of the associated deterministic problem and Monte Carlo simulation results in algorithms
that scale best both in terms of problem size and uncertainty. Further experiments point
to the correlation between the quality of the deterministic solution and the quality of the
probabilistic solution as a major factor responsible for this success.

1. Introduction
Proactive scheduling techniques seek to produce an off-line schedule that is robust to execution time events. In this paper, we assume that we do not have perfect knowledge of
the duration of each activity: the durations are determined at execution time when it is
observed that an activity has finished. However, we do have partial knowledge in the form
of a known probability distribution for each duration. At execution time, the activities will
be dispatched according to the sequences defined by the off-line schedule and our measure of
robustness is the probability with which a given quality will be achieved. More specifically,
in this paper, we address the problem of job shop scheduling (and related generalizations)
when the durations of the activities are random variables and the objective is to find a
solution which has a high probability of having a good (ideally, minimal) makespan. This
is a challenging problem as even evaluating a solution is a hard problem.
To address this problem, we develop a theoretical framework within which we formally
define the problem and (a) construct an approach, based on Monte Carlo simulation, for
evaluating both solutions and partial solutions, and (b) show that solving a carefully defined
deterministic job shop scheduling problem results in a lower bound of the probabilistic
c
2007
AI Access Foundation. All rights reserved.

Beck & Wilson

minimum makespan of the probabilistic job shop scheduling problem. We use this framework
to define a number of algorithms embodying three solution approaches:
1. Branch-and-bound search with Monte Carlo simulation: at each search node, the
search is pruned if we can be almost certain (based on the Monte Carlo simulation)
that the partial solution cannot be extended to a solution better than our current best
solution.
2. Iterative deterministic search with a descending lower bound: the deterministic job
shop problem whose solution is a lower bound on the probabilistic job shop problem
is defined using a parameter, q. The lower bound proof depends on q being less than
or equal to q ∗ (I), a problem-instance-dependent threshold value for problem instance
I that is difficult to compute. Starting with a high q value, we use tree search and
Monte Carlo simulation to solve a sequence of deterministic problems with decreasing
q values. When q is large, the problems are highly constrained and easy to solve (if
any solutions exist). As q descends, the best probabilistic makespan from previous
iterations is used to restrict the search. If we are able to reach a value of q with
q ≤ q ∗ (I) within the CPU time limit, then the search is approximately complete
subject to the sampling error.
3. Deterministic filtering search: deterministic scheduling algorithms based on constraint
programming and tabu search are used to define a number of filter-based algorithms.
All these algorithms operate by generating a series of solution candidates that are
evaluated by Monte Carlo simulation.
Our empirical results indicate that the Monte Carlo based branch-and-bound is only
practical for very small problems. The iterative search based on descending q values is as
good as, or better than, the branch-and-bound algorithm on small problems, and performs
significantly better on larger problems. However, even for medium-sized problems, both of
these techniques are inferior to the heuristic approaches based on deterministic filtering.
Contributions.

The main contributions of this paper are:

• the introduction of the problem of finding proactive schedules with probabilistic execution guarantees for a class of problems where the underlying deterministic scheduling
problem is NP-hard;
• the development of a method for generating a lower bound on the probabilistic minimum makespan;
• the development of a particular Monte Carlo approach for evaluating solutions;
• the design and empirical analysis of a number of approximately complete and heuristic solution techniques based on either constraint-based constructive search or tabu
search; and
• the identification of the correlation between deterministic and probabilistic solution
quality as a key factor in the performance of the filter-based algorithms.
184

Proactive Algorithms for JSP

Plan of Paper. In the next section we define the probabilistic job shop scheduling problem, illustrating it with an example. Section 3 discusses related work. In Section 4, we
present our theoretical framework: we formally define the problem, derive our approach
for generating a lower bound based on an associated deterministic job shop problem, and
show how Monte Carlo simulation can be used to evaluate solutions and partial solutions.
Six search algorithms are defined in Section 5 and our empirical investigations and results
appear in Section 6. In Section 7, it is shown how the results of this paper apply to much
more general classes of scheduling problems. Directions for future work based on theoretical
and algorithmic extensions are also discussed.

2. Probabilistic Job Shop Scheduling Problems
The job shop scheduling problem with probabilistic durations is a natural extension of the
standard (deterministic) job shop scheduling problem (JSP).
2.1 Job Shop Scheduling Problems
A JSP involves a set A of activities, where each Ai ∈ A has a positive duration di . For
each instance of a JSP, it is assumed that either all the durations are positive integers, or
they are all positive real numbers.1 A is partitioned into jobs, and each job is associated
with a total ordering on that set of activities. Each activity must execute on a specified
unary capacity resource. No activities that require the same resource can overlap in their
execution, and once an activity is started it must be executed for its entire duration. We
represent this formally by another partition of A into resource sets: two activities are in
the same resource set if and only if they require the same resource.
A solution consists of a total ordering on each resource set, which does not conflict with
the jobs ordering, i.e., the union of the resource orderings and job orderings is an acyclic
relation on A. Thus, if Ai and Aj are in the same resource set, a solution either orders Ai
before Aj (meaning that Aj starts no sooner than the end of Ai ), or Aj before Ai . The set
of solutions of a job shop problem will be labeled S. A partial solution consists of a partial
ordering on each resource set which can be extended to a solution.
Let s be a (partial) solution. A path in s (or an s-path) is a sequence of activities such
that if Ai immediately precedes Aj in the sequence, then either (i) Ai and Aj are in the
same job, and Ai precedes Aj in that job, or (ii) Ai and Aj are in the same resource set
and s orders Ai before Aj . The length, len(π), of a path
P π (of a solution) is equal to the
sum of the durations of the activities in the path, i.e., Ai ∈π di . The makespan, make(s),
of a solution s is defined to be the length of a longest s-path. An s-path, π, is said to be a
critical s-path if the length of π is equal to the makespan of the solution s, i.e., it is one of
the longest s-paths. The minimum makespan of a job shop scheduling problem is defined
to be the minimum value of make(s) over all solutions s.
The above definitions focus on solutions rather than on schedules. Here, we briefly indicate how our definitions relate to, perhaps more immediately intuitive, definitions focusing
on schedules. A schedule assigns the start time of each activity, and so can be considered as
1. Our empirical investigations examine the integer case. As shown below, the theoretical results hold also
for the case of positive real number durations.

185

Beck & Wilson

a function from the set of activities A to the set of time-points, defining when each activity
starts. The set of time-points is assumed to be either the set of non-negative integers or
the set of non-negative real numbers. Let starti be the start time of activity Ai ∈ A with
respect to a particular schedule, and let endi , its end time, be starti + di . For Ai , Aj ∈ A,
write Ai ≺ Aj for the constraint endi ≤ startj . A schedule is defined to be valid if the
following two conditions hold for any two different activities Ai , Aj ∈ A: (a) if Ai precedes
Aj in the same job, then Ai ≺ Aj ; and (b) if Ai and Aj are in the same resource set, then
either Ai ≺ Aj or Aj ≺ Ai (since Ai and Aj are not allowed to overlap).
Let Z be a valid schedule. Define make(Z), the makespan of Z, to be maxAi ∈A endi ,
the time at which the last activity has been completed. The minimum makespan is defined
to be the minimum value of make(Z) over all valid schedules.
Each solution s defines a valid schedule sched(s), where each activity is started as soon
as its immediate predecessors (if any) have finished, and activities without predecessors are
started at time-point 0 (so sched(s) is a non-delay schedule given the precedence constraints
expressed by s). An immediate predecessor of activity Aj with respect to a particular
solution is defined to be an activity which is an immediate predecessor of Aj either with
respect to the ordering on the job containing Aj , or with respect to the ordering (associated
with the solution) on the resource set containing Aj . It can be shown that the makespan
of sched(s) is equal to make(s) as defined earlier, hence justifying our definition.
Conversely, given a valid schedule Z, we can define a solution, which we call sol(Z),
by ordering each resource set with the relation ≺ defined above. If Z is a schedule, then
the makespan of sched(sol(Z)), which is equal to make(sol(Z)), is less than or equal to the
makespan of Z. This implies that the minimum makespan over all solutions is equal to the
minimum makespan over all valid schedules. Therefore, if we are interested in schedules
with the best makespans, we need only consider solutions and their associated schedules.
To summarize, when aiming to find the minimum makespan for a JSP, we can focus on
searching over all solutions, rather than over all schedules, because (i) for any schedule Z,
there exists a solution s = sol(Z) such that Z is consistent with s (i.e., satisfies the precedence constraints expressed by s); and (ii) for any solution s, we can efficiently construct
a schedule sched(s) which is optimal among schedules consistent with s (and furthermore,
the makespan of sched(s) is equal to make(s)).
JSP Example. Consider a job shop scheduling problem involving two jobs and five activities as shown in Figure 1. The first job consists of the sequence (A1 , A2 , A3 ) of activities;
the second job consists of the sequence (A4 , A5 ). There are three resources involved. A1 and
A4 require the first resource; hence activities A1 and A4 cannot overlap, and so either (i)
A1 precedes A4 , or (ii) A4 precedes A1 . Activities A3 and A5 require the second resource;
A2 requires the third resource. Hence, the resource sets are {A1 , A4 }, {A2 } and {A3 , A5 }.
There are four solutions:
• sa involves the orderings A1 ≺ A4 and A3 ≺ A5 ;
• sb is defined by A1 ≺ A4 and A5 ≺ A3 ;
• sc by A4 ≺ A1 and A3 ≺ A5 ; and
• sd by A4 ≺ A1 and A5 ≺ A3 .
186

Proactive Algorithms for JSP

A1

A2

A3

A4

A1

A2

A5

A3

A4

A1

A5

A2

A4

A5

Solution Sa
A1

A2

Solution Sb
A3

A4

A3

A1

A5

A2

A3

A4

Solution Sc

A5
Solution Sd

Figure 1: The example JSP with its four solutions.
The duration of activity Ai is di . The sequence (A1 , A4 , A5 ) is an sa -path, whose length
is d1 +d4 +d5 . Also, if π is the sa -path (A1 , A2 , A3 , A5 ), then len(π) = d1 +d2 +d3 +d5 . The
only other sa -paths are subsequences of one of these two. Hence, make(sa ), the makespan
of solution sa , is equal to max(d1 + d4 + d5 , d1 + d2 + d3 + d5 ) = d1 + d5 + max(d4 , d2 + d3 ).
In particular, if d1 = 1, d2 = 2, d3 = 3, d4 = 4 and d5 = 5, then make(sa ) = 11 time units.
We then also have make(sb ) = 13, make(sc ) = 15 and make(sd ) = 12. Hence, the minimum
makespan is make(sa ) = 11.
Let Z = sched(sa ) be the schedule associated with solution sa . This is generated as
follows. A1 has no predecessors, so we start A1 at the beginning, setting Z(A1 ) = 0; hence
activity A1 starts at time-point 0 and ends at time-point d1 . The only predecessor of A4
is A1 , so we set Z(A4 ) = d1 . Similarly, we set Z(A2 ) = d1 , and so activity A2 ends at
time-point d1 + d2 . Continuing, we set Z(A3 ) = d1 + d2 . Activity A5 has two immediate
predecessors (for this solution, sa ), A3 and A4 , and so A5 is set to start as soon as both
of these activities have been completed, which is at time-point max(d1 + d2 + d3 , d1 + d4 ).
All activities have been completed when A5 has been completed, which is at time-point
max(d1 +d2 +d3 , d1 +d4 )+d5 = d1 +d5 +max(d4 , d2 +d3 ). This confirms that the makespan
make(sa ) of solution sa is equal to the makespan of its associated schedule sched(sa ).
2.2 Independent and General Probabilistic Job Shop Scheduling Problems
An independent probabilistic job shop scheduling problem is defined in the same way as a
JSP, except that the duration di associated with an activity Ai ∈ A is a random variable;
we assume that in each instance of a probabilistic JSP, either all the durations are positive
integer-valued random variables, or they are all positive real-valued random variables. d i
has (known) distribution Pi , expected value µi = E[di ] and variance σi2 = Var[di ]. These
187

Beck & Wilson

random variables are fully independent. The length of a path π of a solution s is now
a random variable, which we write as len(π). The makespan make(s) of solution s (the
length of the longest path in s) is therefore also a random variable, which we will sometimes
refer to as the random makespan of s.
We can generalize this to the non-independent case. In the probabilistic job shop scheduling problem we have a joint probability measure P over the durations vectors. (The intention
is that we can efficiently sample with the joint density function. For example, a Bayesian
network might be used to represent P .) Here, for activity Ai , distribution Pi is defined to
be the appropriate marginal distribution, with expected value µi and variance σi2 .
Loosely speaking, in a probabilistic job shop scheduling problem, we want to find as
small a value of D as possible such that there is a solution whose random makespan is, with
high probability, less than D (the “deadline” for all activities to finish). This time value D
will be called the probabilistic minimum makespan.
Evaluating a solution for a deterministic JSP, i.e., finding the associated makespan given
a duration for each activity, can be achieved in low degree polynomial time using a longest
path algorithm. Without the ordering on each resource set, the disjunctions of resource
constraints that must be satisfied to find a solution turn this very easy problem into the
NP-complete JSP (Garey & Johnson, 1979). PERT networks, on the other hand, generalize
the simple longest-path problem by allowing durations to be independent random variables,
leading to a #P-complete problem (Hagstrom, 1988). The probabilistic JSP makes both
these generalizations. Consequently, finding the optimal solutions of a probabilistic JSP
appears to be very hard, and we focus on methods for finding good solutions instead.
Evaluating (approximately) a solution of a probabilistic JSP can be done relatively
efficiently using Monte Carlo simulation: for each of a large number of trials we randomly
sample the duration of every activity and generate the makespan associated with that
trial. Roughly speaking, we approximately evaluate the solution by evaluating the sampled
distribution of these makespans. This approach is described in detail in Section 4.3.
Almost all of our solution techniques involve associating a deterministic job shop problem
with the given probabilistic job shop problem, by replacing, for some number q, each random
duration by the mean of its distribution plus q times its standard deviation. Hence, we set
the duration di of activity Ai in the associated deterministic problem to be µi +q ×σi for the
case of continuous time. For the case when time-points are integers, we set d i = bµi +q×σi c.
For certain values of q, this leads to the minimum makespan of the deterministic problem
being a lower bound for the probabilistic minimum makespan, as shown in Section 4.2. This
lower bound can be useful for pruning in a branch-and-bound algorithm. More generally,
we show how solving the associated deterministic problem can be used to help solve the
probabilistic problem.
Our assumptions about the joint probability are somewhat restrictive. For example, the
model does not allow an activity’s duration to depend on its start time; however, it can be
extended to certain situations of this kind.2 Despite these restrictions (which are common in
related literature—see Section 3), our model does apply to an interesting class of problems
2. We could allow the duration of each activity to be probabilistically dependent only on its start time, given
the additional (very natural) coherence condition that for any time-point t0 , the conditional probability
that endi ≥ t0 , given starti = t, is monotonically increasing in t, i.e., Pr(endi ≥ t0 |starti = t1 ) ≤
Pr(endi ≥ t0 |starti = t2 ) if t1 ≤ t2 . This condition ensures that, for any given solution, there is no

188

Proactive Algorithms for JSP

that has not been previously addressed. Extending our model to richer representations by
relaxing our assumptions remains for future work.
Probabilistic JSP Example. We consider an independent probabilistic job shop scheduling problem with the same structure as the JSP example in Figure 1. The durations of
activities A2 , A3 and A4 are now independent real-valued random variables (referred to as
d2 , d3 and d4 , respectively) which are all approximately normally distributed with standard deviation 0.5 (σ2 = σ3 = σ4 = 0.5) and with means µ2 = 2, µ3 = 3 and µ4 = 4. The
durations of activities A1 and A5 are deterministic, being equal to 1 and 5, respectively.
Let π be the sa -path (A1 , A2 , A3 , A5 ). The length len(π) of π is an approximately
normally distributed random variable√with mean 1+2+3+5 = 11 and variance 0.5 2 +0.52 =
0.5 and hence standard deviation 1/ 2.
The length of sa -path π 0 = (A1 , A4 , A5 ) is an approximately normal random variable
with mean 10 and standard deviation 0.5. The (random) makespan make(sa ) of solution
sa is a random variable equaling the maximum of random variables len(π) and len(π 0 ).
In general, the maximum of two independent normally distributed random variables is not
normally distributed; however, π is, with high probability, longer than π 0 , so the distribution
of make(sa ) is approximately equal to the distribution of len(π).

3. Previous Work
There has been considerable work on scheduling with uncertainty in a variety of fields
including artificial intelligence (AI), operations research (OR), fault-tolerant computing,
and systems. For surveys of the literature, mostly focusing in AI and OR, see the work of
Davenport and Beck (2000), Herroelen and Leus (2005), and Bidot (2005).
At the highest level, there are two approaches to such problems: proactive scheduling,
where some knowledge of the uncertainty is taken into account when generating an off-line
schedule; and reactive scheduling where decisions are made on-line to deal with unexpected
changes. While there is significant work in reactive scheduling and, indeed, on techniques
that combine reactive and proactive scheduling such as least commitment approaches (see
the surveys noted above), here our interest is on pure proactive scheduling. Three categories
of proactive approaches have been identified: redundancy-based techniques, probabilistic
techniques, and contingent/policy-based techniques (Herroelen & Leus, 2005). We briefly
look at each of these in turn.
3.1 Redundancy-based Techniques
Redundancy-based techniques generate a schedule that includes the allocation of extra
resources and/or time in the schedule. The intuition is that these redundant allocations will
help to cushion the impact of unexpected events during execution. For example, extra time
can be “consumed” when an activity takes longer than expected to execute. Because there
is a clear conflict between insertion of redundancy and common measures of schedule quality
(e.g., makespan), the focus of the work tends to be the intelligent insertion of redundancy
in order to achieve a satisfactory trade-off between schedule quality and robustness.
advantage in delaying starting an activity when its predecessors have finished. Allowing such a delay
would break the assumptions underlying our formulation.

189

Beck & Wilson

It is common in fault-tolerant scheduling with real-time guarantees to reserve redundant
resources (i.e., processors) or time. In the former case, multiple instantiations of a given
process are executed in parallel and error detection can be done by comparing the results
of the different instantiations. In contrast, in time redundancy, some time is reserved for
re-execution of a process that fails. Given a fault model, either technique can be used to
provide real-time guarantees (Ghosh, Melhem, & Mossé, 1995; Ghosh, 1996).
A similar approach is used in the work of Gao (1995) and Davenport, Gefflot and Beck
(2001) in the context of job shop scheduling. Statistical information about the mean time
between failure and the mean repair time of machines is used to either extend the duration
of critical activities in the former work or to require that any solution produced must respect
constraints on the slack of each activity. Given a solution, the slack is the room that an
activity has to “move” without breaking a constraint or increasing the cost. Typically, it is
formalized as the difference between an activity’s possible time window in a solution (i.e., its
latest possible end time less its earliest possible start time) and the duration of the activity.
The advantage of Gao’s approach is that it is purely a modeling approach: the problem is
changed to incorporate extended durations and any scheduling techniques can be used to
solve the problem. However, Davenport et al. show that reasoning about the slack shared
amongst a set of activities can lead to better solutions at the cost of specialized solving
approaches.
Leon, Wu and Storer (1994) present an approach to job shop scheduling where the
objective function is modified to be a linear combination of the expected makespan and
expected delay assuming that machines can break down and that, at execution time, disruptions are dealt with by shifting activities later in time while maintaining the sequence
in the original schedule. While this basic technique is more properly seen as a probabilistic
approach, the authors show that an exact calculation of this measure is intractable unless a
single disruption is assumed. When there are likely to be multiple disruptions, the authors
present a number of surrogate measures. Empirically, the best surrogate measure is the
deterministic makespan minus the mean activity slack. Unlike, Gao and Davenport et al.,
Leon et al. provide a more formal probabilistic foundation, but temporal redundancy plays
a central role in the practical application of their approach.
3.2 Probabilistic Techniques
Probabilistic techniques use representations of uncertainty to reason about likely outcomes
when the schedule is executed.3 Rather than explicitly inserting redundancy in an attempt
to create a robust schedule, probabilistic techniques build a schedule that optimizes some
measure of probabilistic performance. Performance measures typically come in two forms:
an expected value such as expected makespan or expected weighted tardiness, and a probabilistic guarantee with respect to a threshold value of a deterministic optimization measure.
An example of the latter measure, as discussed below, is the probability that the flow time
of a schedule will be less than a particular value.
Optimal expected value scheduling problems have been widely studied in OR (Pinedo,
2003). In many cases, the approach takes the form of dispatch rules or slightly more
complicated polynomial time algorithms that will find the optimal schedule for tractable
3. Alternative representations of uncertainty such as fuzzy sets can also be used (Herroelen & Leus, 2005).

190

Proactive Algorithms for JSP

problems (e.g., 1 and 2 machine problems) and which serve as heuristics for more difficult
problems. One example of such work in the AI literature is that of Wurman and Wellman
(1996) which extends decision theoretic planning concepts to scheduling. The problem
studied assumes a single machine, stochastic processing time and stochastic set-up time,
and has as its objective the minimization of the expected weighted number of tardy jobs.
The authors propose a state-space search and solve the problem of multi-objective stochastic
dominance A*. Critical aspects of this work are the use of a number of sophisticated path
pruning rules and relaxation-based heuristics for the evaluation of promising nodes.
A threshold measure is used by Burns, Punnekkat, Littlewood and Wright (1997) in a
fault-tolerant, single processor, pre-emptive scheduling application. The objective is to find
the minimum fault arrival rate such that all tasks can be scheduled to meet their deadlines.
Based on a fault-model, the probability of observing that fault arrival rate is calculated
and used as a measure of schedule quality. The optimization problem, then, is to find the
schedule that maximizes the probability of all tasks meeting their deadlines under the fault
arrival process.
In a one-machine manufacturing context with independent activities, Daniels and Carrillo (1997) define a β-robust schedule as the sequence that maximizes the probability that
the execution will achieve a flow time no greater than a given threshold. While the underlying deterministic scheduling problem is solvable in polynomial time and, indeed, the
minimum expected flow time schedule can be found in polynomial time, it is shown that
finding the β-robust schedule is NP-hard. Daniels and Carrillo present branch-and-bound
and heuristic techniques to solve this problem.
3.3 Contingent and Policy-based Approaches
Unlike the approaches described above, contingent and policy-based approaches do not
generate a single off-line schedule. Rather, what is produced is a branching or contingent
schedule or, in the extreme, a policy, that specifies the actions to be taken when a particular
set of circumstances arises. Given the importance of having an off-line schedule in terms
of coordination with other entities in the context surrounding the scheduling problem, this
difference can have significant practical implications (see Herroelen & Leus, 2005, for a
discussion).
An elegant example of a contingent scheduling approach is the “just-in-case” work of
Drummond, Bresina and Swanson (1994). Given an initial, deterministic schedule for a
single telescope observation problem, the approach identifies the activity most likely to fail
based on the available uncertainty information. At this point, a new schedule is produced
assuming the activity does, indeed, fail. Repeated application of the identification of the
most-likely-to-fail activity and generation of a new schedule results in a branching schedule
where a number of the most likely contingencies are accounted for in alternative schedules.
At execution time, when an activity fails, the execution switches to the alternative schedule
if one exists. If an alternative does not exist, on-line rescheduling is done. Empirical results
demonstrate that a significantly larger portion of an existing (branching) schedule can be
executed without having to revert to rescheduling as compared to the original deterministic
schedule.
191

Beck & Wilson

One of the weaknesses of the just-in-case scheduling surrounds the combinatorics of
multiple resources. With multiple inter-dependent telescopes, the problem quickly becomes
intractable. Policy-based approaches such as Markov Decision Processes (MDPs) (Boutilier,
Dean, & Hanks, 1999) have been applied to such problems. Here, the objective is to
produce a policy mapping states to actions that will direct the on-line execution of the
schedule: when a given state is encountered, the corresponding action is taken. Meuleau et
al. (1998) apply MDPs to a stochastic military resource allocation problem where weapons
must be allocated to targets. Given a limited number of weapons and uncertainty about the
effectiveness of a given allocation, an MDP is used to derive an optimal policy where the
states are represented by the number of remaining weapons and targets, and the actions are
the weapon allocation decisions. The goal is to minimize the expected number of surviving
targets. Empirical results demonstrated the computational challenges of such an approach
as a 6 target, 60 weapon problem required approximately 6 hours of CPU time (albeit on
now-outdated hardware).
In the OR literature, there has been substantial work (cited in Brucker, Drexl, Möhring,
Neumann and Pesch, 1999, and Herroelen and Leus, 2005) on stochastic resource-constraint
project scheduling, a generalization of job shop scheduling. The general form of these
approaches is a multi-stage stochastic programming problem, with the objective of finding a
scheduling policy which will minimize the expected makespan. In this context, a scheduling
policy makes decisions on-line about what activities to execute. Decisions need to be made at
the beginning of the schedule and at the end time of each activity, and the information used
for such decisions must be only that which has become known before the time of decision
making. A number of different classes of policy have been investigated. For example, a
minimal forbidden subset of activities, F , is a set such that the activities in F cannot
be executed simultaneously due to resource constraints, but that any subset of F can be
so executed. A pre-selective policy identifies such a set F and a waiting activity, j ∈ F ,
such that j cannot be started until at least one activity i ∈ F − {j} has been executed.
During execution, j can be started only when at least one other activity in F has finished.
The proactive problem, then, is to identify the waiting activity for each minimal forbidden
subset such that the expected makespan is minimized. The computational challenges of
pre-selective policies (in particular, due to the number of minimal forbidden subsets) have
led to work on different classes of policy as well as heuristic approaches.
3.4 Discussion
The work in this paper falls within the probabilistic scheduling approaches and is most
closely inspired by the β-robustness work of Daniels and Carrillo (1997). However, unlike
Daniels and Carrillo, we address a scheduling model where the deterministic problem that
underlies the probabilistic job shop scheduling problem is, itself, NP-hard. This is the
first work of which we are aware that seeks to provide probabilistic guarantees where the
underlying deterministic problem is computationally difficult.

4. Theoretical Framework
In this section, we develop our theoretical framework for probabilistic job shop problems.
In Section 4.1, we define how we compare solutions, using what we call α-makespans. If the
192

Proactive Algorithms for JSP

α-makespan of solution s is less than time value D, then there is at least chance 1 − α that
the (random) makespan of s is less than D. As it can be useful to have an idea about how far
a solution’s α-makespan is from the optimum α-makespan (i.e., the minimum α-makespan
over all solutions), in Section 4.2, we describe an approach for finding a lower bound for the
optimum α-makespan. Section 4.3 considers the problem of evaluating a given solution, s,
by using Monte Carlo simulation to estimate the α-makespan of s.
In order to separate our theoretical contributions from our empirical analysis, we summarize the notation introduced in this section in Section 5.1. Readers interested primarily
in the algorithms and empirical results can therefore move directly to Section 5.
This section makes use of notation introduced in Section 2: the definitions in Section
2.1 of a JSP, a solution, paths in a solution, the makespan of a solution, and the minimum
makespan; and the definitions in Section 2.2 of a probabilistic JSP and the random makespan
of a solution.
4.1 Comparing Solutions and Probabilistic Makespan
In a standard job shop problem, solutions can be compared by considering the associated
makespans. In the probabilistic case, the makespan of a solution is a random variable, so
comparing solutions is less straight-forward. We map the random makespan to a scalar
quantity, called the α-makespan, which sums up how good it is; solutions are compared by
comparing their associated α-makespans. A simple idea is to prefer solutions with smaller
expected makespan. However, there may be a substantial probability that the makespan
of the solution will be much higher than its expected value. Instead, we take the following
approach: if we can be confident that the random makespan for solution s is at most D,
but we cannot be confident that the makespan for solution s0 is at most D, then we prefer
solution s to solution s0 .
We fix a value α, which is used to bound probabilities. Although we imagine that in
most natural applications of this work, α would be quite small (e.g., less than 0.1) we
assume only that α is in the range (0, 0.5]. If the probability of an event is at least 1 − α,
then we say that the event is sufficiently certain. The experiments described in Section 6
use a value of α = 0.05, so that “sufficiently certain” then means “occurs with at least 95%
chance”.
Let D be a time value, and let s be a solution. D is said to be α-achievable using
s if it is sufficiently certain that all jobs finish by D when we use solution s; that is, if
Pr(make(s) ≤ D) ≥ 1 − α, where make(s) is the random makespan of s.
D is said to be α-achievable if there is some solution s such that D is α-achievable using
s, i.e., if there exists some solution s making it sufficiently certain that all jobs finish by D.
Time value D is α-achievable if and only if maxs∈S Pr(make(s) ≤ D)) ≥ 1 − α, where the
max is over all solutions s.
Define Achα (s) to be the set of all D which are α-achievable using s. We define Dα (s),
the α-makespan of s, to be the infimum4 of Achα (s). Then Dα , the α-minimum makespan,
is defined to be the infimum of Achα , which is the set of all D which are α-achievable, so
4. That is, the greatest lower bound of Achα (s); in fact, as shown by Proposition 1(i), Dα (s) is the smallest
element of Achα (s). Hence, Achα (s) is equal to the closed interval [Dα (s), ∞), i.e., the set of time-points
D such that D ≥ Dα (s).

193

Beck & Wilson

Dα = inf {D : (maxs∈S Pr(make(s) ≤ D)) ≥ 1 − α}. We will also sometimes refer to Dα (s)
as the probabilistic makespan of s, and refer to Dα as the probabilistic minimum makespan.5
We prefer solutions which have better (i.e., smaller) α-makespans. Equivalently, solution
s is considered better than s0 if there is a time value D which is α-achievable using s but
not α-achievable using s0 . Optimal solutions are ones whose α-makespan is equal to the
α-minimum makespan.
We prove some technical properties of α-makespans and α-achievability relevant for
mathematical results in later sections. In particular, Proposition 1(ii) states that the αminimum makespan Dα is α-achievable: i.e., there exists some solution which makes it
sufficiently certain that all jobs finish by Dα . Dα is the smallest value satisfying this
property.
Lemma 1 With the above notation:
(i) Achα =

S

s∈S

Achα (s);

(ii) there exists a solution s such that Achα = Achα (s) and Dα = Dα (s);
(iii) Dα = mins∈S Dα (s), the minimum of Dα (s) over all solutions s.
Proof:
(i) D is α-achievable
if and only if for some solution s, D ∈ Achα (s), which is true if and
S
only if D ∈ s∈S Achα (s).
(ii) Consider the following property (∗) on set of time values A: if D ∈ A and D 0 is a time
value greater than D (i.e., D 0 > D), then D 0 ∈ A; that is, A is an interval with no upper
bound. Let A and B be two sets with property (∗); then either A ⊆ B or B ⊆ A. (To show
this, suppose otherwise, that neither A ⊆ B nor B ⊆ A; then there exists some x ∈ A − B
and some y ∈ B − A; x and y must be different, and so we can assume, without loss of
generality, that x < y; then by property (∗), y ∈ A which is the contradiction required.)
Hence, A ∪ B is either equal to A or equal to B. By using induction, it follows that the
union of a finite number of sets S
with property (∗) is one of the sets. Each set Ach α (s)
satisfies property (∗); therefore, s∈S Achα (s) = Achsα0 for some solution s0 , so, by (i),
Achα = Achsα0 . This implies also Dα = Dα (s0 ).
(iii) Let s be any solution and let D be any time value. Clearly, if D is α-achievable using
s, then D is α-achievable. This implies that Dα ≤ Dα (s). Hence, Dα ≤ mins∈S Dα (s). By
(ii), Dα = Dα (s) for some solution s, so Dα = mins∈S Dα (s), as required.
2

Proposition 1
(i) Let s be any solution. Dα (s) is α-achievable using s, i.e., Pr(make(s) ≤ Dα (s)) ≥
1 − α.
(ii) Dα is α-achievable, i.e., there exists some solution s with Pr(make(s) ≤ D α ) ≥ 1−α.
5. Note that the probabilistic makespan is a number (a time value), as opposed to the random makespan
of a solution, which is a random variable.

194

Proactive Algorithms for JSP

Proof:
In the discrete case, when the set of time values is the set of non-negative integers, then
the infimum in the definitions of Dα (s) and Dα is the same as minimum. (i) and (ii) then
follow immediately from the definitions.
We now consider the case when the set of time values is the set of non-negative real
numbers.
1
1
(i): For m, n ∈ {1, 2, . . . , }, let Gm = Pr(0 < make(s)−Dα (s) ≤ m
), and let gn = Pr( n+1
<
1
make(s) − Dα (s) ≤ n ). By the countable additivity axiom of probability measures, Gm =
P∞
P
gn . This means that l−1
n=m
n=m gn tends to Gm as l tends to infinity, and hence Gl =
Pl−1
P∞
n=m gn tends to 0. So, we have limm→∞ Gm = 0. For all m > 0, we
l gn = G m −
1
) ≥ 1 − α, by definition of Dα (s). Also Pr(make(s) ≤
have Pr(make(s) ≤ Dα (s) + m
1
). So, for all m = 1, 2, . . ., Pr(make(s) ≤
Dα (s)) + Gm = Pr(make(s) ≤ Dα (s) + m
Dα (s)) ≥ 1 − α − Gm , which implies Pr(make(s) ≤ Dα (s)) ≥ 1 − α, because Gm tends to
0 as m tends to infinity.

(ii): By part (ii) of Lemma 1, for some solution s, Dα = Dα (s). Part (i) then implies that
Pr(make(s) ≤ Dα ) ≥ 1 − α.
2
Probabilistic JSP Example continued. We continue the example from Section 2.1
and Section 2.2. Set α to 0.05, corresponding to 95% confidence. A value of D = 12.5 is
α-achievable using solution sa , since there is more than 95% chance that both paths π and
π 0 are (simultaneously) shorter than length 12.5, and so the probability that the random
makespan make(sa ) is less than 12.5 is more than 0.95.
Now consider a value of√D = 12.0. Since len(π) (the random length of π) has mean 11
and standard deviation 1/ 2, the chance that
√ len(π) ≤ 12.0 is approximately the chance
that a normal distribution is no more than 2 standard deviations above its mean; this
probability is about 0.92. Therefore, D = 12.0 is not α-achievable using solution s a , since
there is less than 0.95 chance that the random makespan make(sa ) is no more than D.
The α-makespan (also referred to as the “probabilistic makespan”) of solution s a is
therefore between 12.0 and 12.5. In fact, the α-makespan Dα (sa ) is approximately equal
to 12.16, since there is approximately 95% chance that the (random) makespan make(s a )
is at most 12.16. It is easy to show that D = 12.16 is not α-achievable using any other
solution, so Dα , the α-minimum makespan, is equal to Dα (sa ), and hence about 12.16.
4.2 A Lower Bound For α-Minimum Makespan
In this section we show that a lower bound for the α-minimum makespan Dα can be found
by solving a particular deterministic JSP.
A common approach is to generate a deterministic problem by replacing each random
duration by the mean of the distribution. As we show, under certain conditions, the minimum makespan of this deterministic JSP is a lower bound for the probabilistic minimum
makespan. For instance, in the example, the minimum makespan of such a deterministic
JSP is 11, and the probabilistic minimum makespan is about 12.16. However, an obvious
weakness with this approach is that it does not take into account the spreads of the distributions. This is especially important since we are typically considering a small value of α,
195

Beck & Wilson

such as 0.05. We can generate a stronger lower bound by taking into account the variances
of the distributions when generating the associated deterministic job shop problem.
Generating a Deterministic JSP from a Probabilistic JSP and a Value q. From
a probabilistic job shop problem, we will generate a particular deterministic job shop problem, depending on a parameter q ≥ 0. We will use this transformation for almost all the
algorithms in Section 5. The deterministic JSP is the same as the probabilistic JSP except
with each random duration replaced by a particular time value. Solving the corresponding
deterministic problem will give us information about the probabilistic problem. The deterministic JSP consists of the same set A of activities, partitioned into the same resource sets
and the same jobs, with the same total order on each job. The duration of activity A i in the
deterministic problem is defined to be µi + qσi , where µi and σi are respectively the mean
and standard deviation of the duration of activity Ai in the probabilistic job shop problem.
Hence, if q = 0, the associated deterministic problem corresponds to replacing each random
duration by its mean. Let makeq (s) be the deterministic makespan of solution s, i.e., the
makespan of s for the associated deterministic problem (which is defined to be the length of
the longest s-path—see Section 2.1). Let makeq be the minimum deterministic makespan
over all solutions.
Let s be a solution. We say that s is probabilistically optimal if Dα (s) = Dα . Let π
be an s-path. (π is a path in both the probabilistic and deterministic problems.) π is said
to be a (deterministically) critical path if it is a critical path in the deterministic problem.
The length of π in the deterministic
problem, lenq (π), is P
equal to the sum
P
P of the durations
of activities in the path:
Ai ∈π (µi + qσi ), which equals
Ai ∈π µi + q
Ai ∈π σi .
We introduce the following rather technical definition whose significance is made clear
by Proposition 2: q is α-sufficient if there exists a (deterministically) critical path π in
some probabilistically optimal solution s with Pr(len(π) > lenq (π)) > α, i.e., there is more
than α chance that the random path length is greater than the deterministic length.
The following result shows that an α-sufficient value of q leads to the deterministic
minimum makespan makeq being a lower bound for the probabilistic minimum makespan
Dα . Therefore, a lower bound for the deterministic minimum makespan is also a lower
bound for the probabilistic minimum makespan.
Proposition 2 For a probabilistic JSP, suppose q is α-sufficient. Then, for any solution
s, Pr(make(s) ≤ makeq ) < 1 − α. Therefore, makeq is not α-achievable, and is a strict
lower bound for the α-minimum makespan Dα , i.e., Dα > makeq .
Proof: Since q is α-sufficient, there exists a (deterministically) critical path π in some (probabilistically) optimal solution so with Pr(len(π) > lenq (π)) > α. We have lenq (π) =
makeq (so ), because π is a critical path, and, by definition of makeq , we have makeq (so ) ≥
makeq . So, Pr(len(π) > makeq ) > α. By the definition of makespan, for any sample
of the random durations vector, make(so ) is at least as a large as len(π). So, we have
Pr(make(so ) > makeq ) > α. Hence, Pr(make(so ) ≤ makeq ) = 1 − Pr(make(so ) >
makeq ) < 1 − α. This implies Dα (so ) > makeq since Pr(make(so ) ≤ Dα (so )) ≥ 1 − α,
by Proposition 1(i). Since so is a probabilistically optimal solution, Dα = Dα (so ), and so
Dα > makeq . Also, for any solution s, we have Dα (s) ≥ Dα > makeq , so Dα (s) > makeq ,
which implies that makeq is not α-achievable using s, i.e., Pr(make(s) ≤ makeq ) < 1 − α. 2
196

Proactive Algorithms for JSP

4.2.1 Finding α-Sufficient q-Values
Proposition 2 shows that we can find a lower bound for the probabilistic minimum makespan
if we can find an α-sufficient value of q, and if we can solve (or find a lower bound for) the
associated deterministic problem. This section looks at the problem of finding α-sufficient
values of q, by breaking down the condition into simpler conditions.
In the remainder of Section 4.2, we assume an independent probabilistic JSP.
Let π be some path of some solution. Define µπ to be E[len(π)],
P the expected value
of the length of π (in the probabilistic JSP), which is equal to PAi ∈π µi . Define σπ2 to
be Var[len(π)], the variance of the length of π, which is equal to Ai ∈π σi2 , since we are
assuming that the durations are independent.
P
Defining α-adequate B. For B ≥ 0, write θB (π) for µπ + Bσπ , which equals Ai ∈π µi +
qP
2
B
Ai ∈π σi . We say that B is α-adequate if for any (deterministically) critical path π of
any (probabilistically) optimal solution, Pr(len(π) > θB (π)) > α, i.e., there is more than α
chance that π is more than B standard deviations longer than its expected length.
If each duration is normally distributed, then len(π) will be normally distributed, since
it is the sum of independent normal distributions. Even if the durations are not normally
distributed, len(π) will often be close to being normally distributed (cf. the central limit
theorem and its extensions). So, Pr(len(π) > θB (π)) will then be approximately 1 − Φ(B),
where Φ is the unit normal distribution. A B value of slightly less than Φ−1 (1 − α) will be
α-adequate, given approximate normality.
Defining B-adequate Values of q. We say that q is B-adequate if there exists a
(deterministically) critical path π in some (probabilistically) optimal solution such that
lenq (π) ≤ θB (π).
The following proposition shows that the task of finding α-sufficient values of q can be
broken down. It follows almost immediately from the definitions.
Proposition 3 If q is B-adequate for some B which is α-adequate, then q is α-sufficient.
Proof: Since q is B-adequate, there exists a (deterministically) critical path π in some
(probabilistically) optimal solution s such that lenq (π) ≤ θB (π). Since B is α-adequate,
Pr(len(π) > θB (π)) > α, and hence Pr(len(π) > lenq (π)) > α, as required.
2
Establishing B-adequate Values of q. A value q is B-adequate if and only if there
exists a (deterministically) critical path π in some (probabilistically) optimal solution
qP such
P
P
P
2
that lenq (π) ≤ θB (π), equivalently:
Ai ∈π σi ,
Ai ∈π µi + q
Ai ∈π σi ≤
Ai ∈π µi + B
qP

q

Mean{σi2 : Ai ∈π}
, where Mπ is
MeanP
{σi : Ai ∈π}
Ai
the number of activities in path π, and Mean{σi : Ai ∈ π} = M1π Ai ∈π σi .
If any activity Ai is not uncertain (i.e., its standard deviation σi equals 0), then it can
be omitted from the summations and means. Mπ then becomes the number of uncertain
activities in path π.
that is, q ≤ B

P

Ai ∈π

σi2

∈π σi

. This can be written as: q ≤

197

√B
Mπ

Beck & Wilson

As is well known (and quite easily shown), the root qmean square of a collection of
Mean{σ 2 : Ai ∈π}
numbers is always at least as large as the mean. Hence, Mean{σ i: A ∈π} is greater than
i
i
or equal to 1. Therefore, a crude sufficient condition for q to be B-adequate is: q ≤ √BM ,
where M is an upper bound for the number of uncertain activities in any path π for any
probabilistically optimal solution (or we could take M to be an upper bound for the number
of uncertain activities in any path π for any solution). In particular, we could generate Badequate q by choosing q = √BM .
An α-sufficient Value of q. Putting the two conditions together and using Proposition
−1
3, we have that a q-value of a little less than Φ √(1−α)
will be α-sufficient, given that the
M
lengths of the paths are approximately normally distributed, where M is an upper bound
for the number of uncertain activities in any path π for any optimal solution. Hence, by
Proposition 2, the minimum makespan makeq of the associated deterministic problem is
then a strict lower bound for the α-minimum makespan Dα . For example, with α = 0.05,
we have Φ−1 (1 − α) ≈ 1.645 (since there is about 0.05 chance that a normal distribution is
more than 1.645 standard deviations above its mean), and so we can set q to be a little less
√ .
than 1.645
M
One can sometimes generate a larger α-sufficient value of q, and hence a stronger lower
bound makeq , by focusing only on the significantly uncertain activities. Choose value ε
between
P 0 and 1. For any path π, say
P that that activity Aj is ε-uncertain (with respect to
π) if
{σi : Ai ∈ π, σi ≤ σj } > ε {σi : Ai ∈ π}; then the sum of the durations of the
activities which are not ε-uncertain is at most a fraction ε of the sum of all the durations in
the path. Hence, the activities in π which are not ε-uncertain have relatively small standard
deviations. If we define Mε to be an upper bound on the number of ε-uncertain activities
involved in any path of any (probabilistically) optimal solution, then it can be shown, by
√
will be B-adequate,
a slight modification of the earlier argument, that a q-value of (1−ε)B
M
and hence a q-value of a little less than

(1−ε)Φ−1 (1−α)
√
Mε

ε

will be α-sufficient.

The experiments described in Section 6 use, for varying n, problems with n jobs and n
activities per job). Solutions which have paths involving very large numbers of activities
are unlikely to be good solutions. In particular, one might assume that, for such problems,
there will be an optimal solution s and a (deterministically) critical s-path π involving no
more than 2n activities. Given this assumption, the following value of q is α-sufficient,
−1
, e.g.,
making makeq a lower bound for the probabilistic minimum makespan: q = Φ √(1−α)
2n
q=

1.645
√
2n

when α = 0.05. This motivates the choice of q1 in Table 2 in Section 6.1.

Probabilistic JSP Example continued. The number of uncertain activities in our
running example (see Section 2.2, Figure 1 and Section 4.1) is 3, so one
√ can set M = 3.
Using α = 0.05, this leads to a choice of q slightly less than 1.645/ 3 ≈ 0.950. By
Proposition 3 and the above discussion, such a value of q is α-sufficient. The durations of
the associated deterministic problem are given by setting di = µi + qσi , and so are d1 = 1,
d2 = 2 + q/2, d3 = 3 + q/2, d4 = 4 + q/2 and d5 = 5. Solution sa is the best solution
with makespan makeq (sa ) = 1 + 5 + (2 + q/2) + (3 + q/2) = 11 + q. Hence, the minimum
198

Proactive Algorithms for JSP

deterministic makespan makeq equals approximately 11.95, which is a lower bound for the
probabilistic minimum makespan Dα ≈ 12.16, illustrating Proposition 2.
However, sc is clearly a poor solution, so we could just consider the other solutions:
{sa , sb , sd }. No (deterministically) critical path of these solutions involves more than two
uncertain activities (within
√ the range of interest of q-values), so we can then set M = 2,
and q = 1.16 ≈ 1.645/ 2. This leads to the stronger lower bound of 11 + 1.16 = 12.16,
which is a very tight lower bound for the α-minimum makespan Dα .
4.2.2 Discussion of lower bound
In our example, we were able to use our approach to construct a very tight lower bound
for the probabilistic minimum makespan. However, this situation is rather exceptional.
Two features of the example which enable this to be a tight lower bound are (a) the best
solution has a path which is almost always the longest path; and (b) the standard deviations
of the uncertain durations are all equal. In the above analysis, the root mean square is
approximated (from below) by the mean. This is a good approximation when the standard
deviations are fairly similar, and in an extreme case when the (non-zero) standard deviations
of durations are all the same (as in the example), the root mean square is actually equal to
the mean.
More generally, there are a number of ways in which our lower bound will tend to be
conservative. In particular,
• the choice of M will often have to be conservative for us to be confident that it is
a genuine upper bound for the number of uncertain activities in any path for any
optimal solution;
• we are approximating a root mean square of standard deviations by the average of the
standard deviations: this can be a very crude approximation if the standard deviations
of the durations vary considerably between activities;
• we are approximating the random variable make(s) by the random length of a particular path.
The strength of our lower bound method, however, is that it is computationally feasible for
reasonably large problems as it uses existing well-developed JSP methods.
4.3 Evaluating a Solution Using Monte Carlo Simulation
For a given time value, D, we want to assess if there exists a solution for which there is
a chance of at most α that its random makespan is greater than D. Our methods will all
involve generating solutions (or partial solutions), and testing this condition.
As noted earlier, evaluating a solution amounts to solving a PERT problem with uncertain durations, a #P-complete problem (Hagstrom, 1988). As in other #P-complete
problems such as the computation of Dempster-Shafer Belief (Wilson, 2000), a natural approach to take is Monte Carlo simulation (Burt & Garman, 1970); we do not try to perform
an exact computation but instead choose an accuracy level δ and require that with a high
chance our random estimate is within δ of the true value. The evaluation algorithm then
199

Beck & Wilson

has optimal complexity (low-degree polynomial) but with a potentially high constant factor
corresponding to the number of trials required for the given accuracy.
To evaluate a solution (or partial solution) s using Monte Carlo simulation we perform
a (large) number, N , of independent trials assigning values to each random variable. Each
trial generates a deterministic problem, and we can check very efficiently if the corresponding
makespan is greater than D; if so, we say that the trial succeeds. The proportion of trials
that succeed is then an estimate of Pr(make(s) > D), the chance that the random makespan
of s is more than D. For the case of independent probabilistic JSPs, we can generate the
random durations vector by picking, using distribution Pi , a value for the random duration
di for each activity Ai . For the general case, picking a random durations vector will still
be efficient in many situations; for example, if the distribution is represented by a Bayesian
network.
4.3.1 Estimating the Chance that the Random Makespan is Greater than D
Perform N trials: l = 1, . . . , N .
For each (trial) l:
— Pick a random durations vector using the joint density function.
— Let Tl = 1 (the trial succeeds) if the corresponding (deterministic) makespan is greater
than D. Otherwise, set Tl = 0.
P
Let T = N1 N
l=1 Tl be the proportion of trials that succeed. T is then an estimate of p,
where p = Pr(make(s) > D), the chance that a randomly generated durations vector leads
to a makespan (for solution s) greater than D. The expected value of T is equal
q to p, since
1 PN
E[Tl ] = p and so E[T ] = N l=1 E[Tl ] = p. The standard deviation of T is p(1−p)
N , which
can be shown as follows: V ar[Tl ] = E[(Tl )2 ] − (E[Tl ])2 = p − p2 = p(1 − p). The variables
P
p(1−p)
1
Tl are independent so V ar[T ] = N12 N
≤ 4N
. The random variable N T
i=1 V ar[Tl ] =
N
is binomially distributed, and so (because of the deMoivre-Laplace limit theorem (Feller,
1968)) we can use a normal distribution to approximate T .
This means that, for large N , generating a value of T with the above algorithm will, with
high probability, give a value close to Pr(make(s) > D). We can choose any accuracy level
δ > 0 and confidence level r (e.g., r = 0.95), and choose N such that Pr(|T − p| < δ) > r;
in particular, if r = 0.95 and using a normal approximation, choosing a number N of trials
more than δ12 is sufficient. For fixed accuracy level δ and confidence level r, the number
of trials N is a constant: it does not depend on the size of the problem. The algorithm
therefore has excellent complexity: the same as the complexity (low-order polynomial) of a
single deterministic propagation, and so must be optimal as we clearly cannot hope to beat
the complexity of deterministic propagation. However, the constant factor δ12 can be large
when we require high accuracy.
4.3.2 When is the Solution Good Enough?
Let D be a time value and let s be a solution. Suppose, based on the above Monte-Carlo
algorithm using N trials, we want to be confident that D is α-achievable using s (i.e., that
200

Proactive Algorithms for JSP

Pr(make(s) > D) ≤ α). We therefore need the observed T to be at least a little smaller
than α, since T is (only) an estimate of Pr(make(s) > D).
To formalize this, we shall use a confidence interval-style approach. Let K ≥ 0. Recall
that p = Pr(make(s) > D) is an unknown quantity that we want to find information
about. We say that “p ≥ α is K-implausible given the result T ” if the following condition
holds: p ≥ αp
implies that T is at least K standard deviations below the expected value, i.e.,
T ≤ p − √KN p(1 − p).
If it were the case that p ≥ α, and “p ≥ α is K-implausible given T ”, then an unlikely
event would have happened. For example, with K = 2, (given the normal approximation),
such an event will only happen about once every 45 experiments; if K = 4 such an event
will only happen about once every 32,000 experiments.
If Pr(make(s) > D) ≥ α is K-implausible given the result T , then we can be confident
that Pr(make(s) > D) < α: D is α-achievable using s, so that D is an upper bound
of Dα (s) and hence of the α-minimum makespan Dα . The confidence level, based on a
normal approximation of the binomial distribution, is Φ(K), where Φ is the unit normal
distribution. For example, K = 2 gives a confidence of around 97.7%.
Similarly, for any α between 0 and 0.5, we say that p ≤ α is K-implausible given the
result T if the following condition holds: p ≤ α implies
that T is at least K standard
p
deviations above the expected value, i.e., T ≥ p + √KN p(1 − p).
The above definitions of K-implausibility are slightly informal. The formal definitions
are as follows. Suppose α ∈ (0, 0.5], K ≥ 0, T ∈ [0, 1] and N ∈ {1, 2, . . . , }. We define:
p ≥ α is K-implausible given p
T if and only if for all p such that α ≤ p ≤ 1, the following
condition holds: T ≤ p − √KN p(1 − p). Similarly, p ≤ α is K-implausible given T if and
p
only if for all p such that 0 ≤ p ≤ α, the following condition holds: T ≥ p + √KN p(1 − p).
These K-implausibility conditions cannot be tested directly using the definition since
p is unknown. Fortunately, we have the following result, which gives equivalent conditions
that can be easily checked.
Proposition 4 With the above definitions:
√K
N

p
α(1 − α).
p
(ii) p ≤ α is K-implausible given T if and only if T ≥ α + √KN α(1 − α).
(i) p ≥ α is K-implausible given T if and only if T ≤ α −

p
Proof: (i): If p ≥ α is K-implausible given T , then setting p to α gives T ≤ α− √KN α(1 − α)
p
as required. Conversely, suppose T ≤ α − √KN α(1 − α). The result follows if K = 0,
2

so we can assume that K > 0. Write f (x) = (x − T )2 − K x(1−x)
. Now, since T ≤
N
p
K 2 α(1−α)
K
2
α − √N α(1 − α), we have α > T and (α − T ) ≥
so, f (α) ≥ 0. Also, f (T ) ≤ 0.
N
Since f (x) is a quadratic polynomial with a positive coefficient of x2 , this implies that T is
either a solution of the equation f (x) = 0, or is between the two solutions. Since f (α) ≥ 0
and α > T , it follows that α must either be a solution of f (x) = 0, or be greater than the
2
. Since p > T ,
solution(s). This implies, for all p > α, f (p) > 0, and so (p − T )2 > K p(1−p)
N
q
p(1−p)
we have for all p ≥ α that T ≤ p − K
N , that is, p ≥ α is K-implausible given T ,
proving (i).
201

Beck & Wilson

q

. Con(ii) If p ≤ α is K-implausible given T , then setting p to α gives T ≥ α + K α(1−α)
q N
q
, then (since α ≤ 0.5) p ≤ α implies T ≥ p + K p(1−p)
since
versely, if T ≥ α + K α(1−α)
N
N
the right-hand-side is a strictly increasing function of p, so p ≤ α is K-implausible given T ,
as required.
2
Part (i) of this result shows us how to evaluate a solution s with respect
p to a bound
K
√
D: if we generate T (using a Monte Carlo simulation) which is at least N α(1 − α) less
than α, then we can have confidence that p < α, i.e., Pr(make(s) > D) < α, and so we
can have confidence that D is α-achievable using solution s, i.e., that D is an upper bound
for the probabilistic makespan Dα (s). Part (ii) is used in the branch-and-bound algorithm
described in Section 5.2.1, for determining if we can backtrack at a node.
4.3.3 Generating an upper approximation of the probabilistic makespan of a
solution
Suppose that, given a solution s, we wish to find a time value D which is just large enough
such that we can be confident that the probabilistic makespan of s is at most D, i.e., that
D is an upper bound for the α-makespan Dα (s). The Monte Carlo simulation can be
adapted for this purpose. We simulate the values of the random makespan make(s) and
record the distribution of these. We decide on a value of K, corresponding to the desired
degree of confidence (e.g., K = 2 corresponds to about 97.7% confidence) and choose D
minimal suchpthat the associated T value (generated from the simulation results) satisfies
T ≤ α − √KN α(1 − α). Then by Proposition 4(i), Pr(make(s) > D) ≥ α is K-implausible
given T . We can therefore be confident that Pr(make(s) > D) < α, so we can have
confidence that D is an upper bound for the α-makespan Dα (s) of s. In the balance of this
paper, we will use the notation D(s) to represent our (upper) estimate of D α (s) found in
this way.

5. Searching for Solutions
The theoretical framework provides two key tools that we use in building search algorithms.
First, we can use Monte Carlo simulation to evaluate a solution or a partial solution (see
Section 4.3). Second, with the appropriate choice of a q value, we can solve an associated
deterministic problem to find a lower bound on the α-minimum makespan for a problem
instance (see Section 4.2). In this section, we make use of both these tools (and some
variations) to define a number of constructive and local search algorithms. Before describing
the algorithms, we recall some of the most important concepts and notation introduced in
these earlier sections.
For all of our algorithms, we explicitly deal only with the case of independent probabilistic JSPs where durations are positive integer random variables. Given our approach,
however, these algorithms are all valid:
• for the generalized probabilistic case, with the assumptions noted in Section 4, provided we have an efficient way to sample the activity durations;
202

Proactive Algorithms for JSP

• for continuous random variables, provided we have a deterministic solver that can
handle continuous time values.
5.1 Summary of Notation
The remainder of the paper makes use of notation and concepts from earlier sections, which
we briefly summarize below.
For a JSP or probabilistic JSP: a solution s totally orders activities requiring the same
resource (i.e., activities in the same resource set), so that if activity Ai and Aj require the
same resource, then s either determines that Ai must have been completed by the time
Aj starts, or vice versa (see Section 2.1). A partial solution partially orders the set of
activities in each resource set. Associated with a solution is a non-delay schedule (relative
to the solution), where activities without predecessors are started at time 0, and other
activities are started as soon as all their predecessors have been completed. The makespan
of a solution is the time when all jobs have been completed in this associated non-delay
schedule. For a probabilistic JSP (see Section 2.2), the makespan make(s) of a solution s
is a random variable, since it depends on the random durations.
The quantity we use to evaluate a solution s is Dα (s), the α-makespan of s (also known as
the probabilistic makespan of s), defined in Section 4.1. The probability that the (random)
makespan of s is more than Dα (s) is at most α, and approximately equal to α. (More
precisely, Dα (s) is the smallest time value D such that Pr(make(s) > D) is at most α.)
Value α therefore represents a degree of confidence required. The α-minimum makespan
Dα (also known as the probabilistic minimum makespan) is the minimum of Dα (s) over all
solutions s.
A time value D is α-achievable using solution s if and only if there is at most α chance
that the random makespan is more than D. D is α-achievable using s if and only if
D ≥ Dα (s) (see Section 4.1).
Solutions of probabilistic JSPs are evaluated by Monte Carlo simulation (see Section
4.3). A method is derived for generating an “upper approximation” of D α . We use the
notation D(s) to represent this upper approximation, which is constructed so that D(s)
is approximately equal to Dα (s), and there is a high chance that Dα (s) will be less than
D(s)—see Section 4.3.3. D(s) thus represents a probable upper bound for the probabilistic
minimum makespan.
With a probabilistic job shop problem we often associate a deterministic JSP (see Section
4.2). This mapping is parameterized by a (non-negative real) number q. The associated
deterministic JSP has the same structure as the probabilistic JSP; the only difference is
that the duration of an activity Ai is equal to µi + qσi , where µi and σi are the mean and
standard deviation (respectively) of the duration of Ai in the probabilistic problem. We
write makeq (s) for the makespan of a solution s with respect to this associated deterministic
JSP, and makeq for the minimum makespan: the minimum of makeq (s) over all solutions s.
In Section 4.2, it is shown, using Propositions 2 and 3 and the further analysis in Section
4.2.1, that for certain values of q, the time value makeq is a lower bound for Dα .
203

Beck & Wilson

5.2 Constructive Search Algorithms
Four constructive-search based algorithms are introduced here. Each of them uses constraintbased tree search as a core search technique, incorporating simulation and q values in different ways. In this section, we define each constructive algorithm in detail and then provide
a description of the heuristics and constraint propagation building blocks used by each of
them.
5.2.1 B&B-N: An Approximately Complete Branch-and-Bound Algorithm
Given the ability to estimate the probabilistic makespan of a solution, and the ability to
test a condition that implies that a partial solution cannot be extended to a solution with
a better probabilistic makespan, an obviously applicable search technique is branch-andbound (B&B) where we use Monte Carlo simulation to derive both upper- and lower-bounds
on the solution quality. If we are able to cover the entire search space, such an approach is
approximately complete (only “approximately” because there is always a small probability
that we miss an optimal solution due to sampling error).
The B&B tree is a (rooted) binary tree. Associated with each node e in the tree is a
partial solution se , which is a solution if the node is a leaf node. The empty partial solution
is associated with the root node. Also associated with each non-leaf node e is a pair of
activities, Ai , Aj , j 6= i, in the same resource set, whose sequence has not been determined
in partial solution se . The two nodes below e extend se : one sequences Ai before Aj , the
other adds the opposite sequence. The heuristic used to choose which sequence to try first
is described in Section 5.2.5.
The value of global variable D ∗ is always such that we have confidence (corresponding to
the choice of K—see Section 4.3.2) that there exists a solution s whose α-makespan, D α (s),
is at most D ∗ . Whenever we reach a leaf node, e, we find the upper estimate D 0 = D(se )
of the probabilistic makespan Dα (s), by Monte Carlo simulation based on the method of
Section 4.3.3. We set D ∗ := min(D ∗ , D0 ). Variable D ∗ is initialized to some high value.
At non-leaf nodes, e, we check to see if it is worth exploring the subtree below e. We
perform a Monte Carlo simulation for partial solution, se , using the current value of D ∗ ;
this generates a result T . We use Proposition 4(ii) to determine if Pr(make(s e ) > D∗ ) ≤ α
is K-implausible given T ; if it is, then we backtrack, since we can be confident that there
exists no solution extending the partial solution se that improves our current best solution.
If K is chosen sufficiently large, we can be confident that we will not miss a good solution. 6
We refer to this algorithm as B&B-N as it performs B ranch-and-B ound with simulation
at each N ode.
5.2.2 B&B-DQ-L: An Approximately Complete Iterative Tree Search
For an internal node, e, of the tree, the previous algorithm used Monte Carlo simulation
(but without strong propagation within each trial) to find a lower bound for the probabilistic
makespans of all solutions extending partial solution se . An alternative idea for generating
6. Because we are doing a very large number of tests, we need much higher confidence than for a usual
confidence interval; fortunately, the confidence associated with K is (based on the normal approximation
2
1
of a binomial, and the approximation of a tail of a normal distribution) approximately 1 − K √1 2π e− 2 K ,
and so tends to 1 extremely fast as K increases.

204

Proactive Algorithms for JSP

B&B-DQ-L():
Returns the solution with lowest probabilistic makespan
1
2
3
4
5
6

7
8

(s∗ , D∗ ) ← findFirstB&BSimLeaves(∞, 0)
q ← qinit
while q ≥ 0 AND not timed-out do
(s, D) ← findOptB&BSimLeaves(D ∗ , q)
if s 6= N IL then
s∗ ← s; D∗ ← D
end
q ← q − qdec
end
return s∗
Algorithm 1: B&B-DQ-L: An Approximately Complete Iterative Tree Search

such a lower bound is to use the approach of Section 4.2: we find the minimum makespan,
over all solutions extending se , of the associated deterministic problem based on a q value
that is α-sufficient. This minimum makespan is then (see Proposition 2) a lower bound for
the probabilistic makespan. Standard constraint propagation on the deterministic durations
enables this lower bound to be computed much faster than the simulation of the previous
algorithm. At each leaf node, simulation is used as in B&B-N to find the estimate of the
probabilistic makespan of the solution.
This basic idea requires the selection of a q value. However, rather than parameterize
this algorithm (as we do with some others below), we choose to perform repeated tree
searches with a descending q value.
The algorithm finds an initial solution (line 1 in Algorithm 1) and therefore an initial
upper bound, D ∗ , on the probabilistic makespan with q = 0. Subsequently, starting with
a high q value (one that does not result in a deterministic lower bound), we perform a
tree search. When a leaf, e, is reached, simulation is used to find D(se ). With such a
high q value, it is likely that the deterministic makespan makeq (se ) is much greater than
D(se ). Since we enforce the constraint that makeq (se ) ≤ D(se ), finding D(se ) through
simulation causes the search to return to an interior node, i, very high in the tree such that
makeq (Si ) ≤ D(se ) where Si represents the set of solutions in the subtree below node i, and
makeq (Si ) is the deterministic lower bound on the makespan of those solutions. With high
q values, we commonly observed in our experiments that there are only a very few nodes
that meet this criterion and, therefore, search is able to very quickly exhaust the search
space. When this happens, we reduce the q value by a small amount, qdec (e.g., 0.05), and
restart the tree search. Eventually, and often very quickly, we reach a q value such that
there exists a full solution, se , such that makeq (se ) ≤ D(se ). That solution is stored as the
current best and we set D ∗ = D(se ). As in B&B-N, D ∗ is used as an upper bound on all
subsequent search.
Algorithm 1 presents pseudocode for the basic algorithm. We make use of two functions
not defined using pseudocode:
• findFirstB&BSimLeaves(c, q): creates a JSP with activity durations defined based on
the q value passed in and conducts a branch-and-bound search where Monte Carlo
205

Beck & Wilson

simulation is used for each leaf node and standard constraint propagation is used at
interior nodes. The first solution that is found whose probabilistic makespan is less
than c is returned with the value of its probabilistic makespan. When c is set very
high as in line 1, no backtracking is needed to find a solution and therefore only one
leaf node is visited and only one simulation is performed.
• findOptB&BSimLeaves(c, q): the same as findFirstB&BSimLeaves(c, q) except the
solution with lowest probabilistic makespan is returned rather than the first one found.
If no solution is found, a NIL value is returned. Unless the q value is low enough
that the deterministic makespan is a lower bound on the probabilistic makespan, this
function does not necessarily return the globally optimal solution.
We find a starting solution with q = 0 to serve as an initial upper bound on the optimal
probabilistic makespan. In practice, B&B-DQ-L is run with a limit on the CPU time. If
q = 0 is reached within the time limit, this algorithm is approximately complete.
As noted above, it is possible, especially with a high q value, that for a solution, se ,
makeq (se ) is much larger than D(se ), and therefore the search will backtrack to the deepest
interior node such that makeq (Si ) ≤ D(se ). In fact, the assignment of the D(se ) value is a
“global cut” as it is an upper bound on the probabilistic makespan. For technical reasons
beyond the scope of this paper, standard constraint-based tree search implementations do
not automatically handle such global cuts. We therefore modified the standard behavior to
repeatedly post the upper bound constraint on makeq (Si ) causing a series of backtracks up
to the correct interior node.
We refer to this algorithm as B&B-DQ-L as it does a series of B ranch-and-B ound
searches with Descending q values and where simulation is used at the Leaves of the tree.
B&B-DQ-L is an example of a novel constraint-based search technique that might be
useful in a wider context. When a problem has a cost function that is expensive to evaluate
but has an inexpensive, parameterizable lower bound calculation, a search based on overconstraining the problem (i.e., by choosing a parameter value that will not lead to a lower
bound) and then iteratively relaxing the bounding function, may be worth investigating.
We discuss such an approach in Section 7.
5.2.3 B&B-TBS: A Heuristic Tree Search Algorithm
Previous results with an algorithm similar to B&B-N (Beck & Wilson, 2004) indicated that
simulation was responsible for a large percentage (e.g., over 95%) of the run-time. We can
reduce the number of times we require simulation by only simulating solutions that have
a very good deterministic makespan. This deterministic filtering search is the central idea
for the rest of the algorithms investigated in this paper.
A simple method of filtering solutions is to first spend some fixed amount of CPU time
to find a solution, s0 , with a low deterministic makespan, makeq (s0 ), using a fixed q value
and standard constructive tree search. Then, search can be restarted using the same q value
and whenever a solution, si , is found such that makeq (si ) ≤ makeq (s0 ), a simulation is run
to evaluate D(si ), our estimate of the probabilistic makespan, Dα (si ). If the probabilistic
makespan found is better than the lowest probabilistic makespan so far, the solution is
stored. Search is continued until the entire tree has been explored or the maximum allowed
CPU time has expired. Algorithm 2 contains the pseudocode.
206

Proactive Algorithms for JSP

B&B-TBS(q):
Returns the solution with lowest probabilistic makespan found
1
2
3
4
5
6
7

8

(s∗ , Dinitial ) ← findOptB&B(∞, q, tinitial )
D∗ ← ∞
while solutions exist AND not timed-out do
(s, D) ← findNextB&B(Dinitial + 1, q, time-remaining)
D0 ← simulate(s)
if D0 < D∗ then
s∗ ← s; D∗ ← D0
end
end
return s∗
Algorithm 2: B&B-TBS: A Heuristic Tree Search Algorithm
As with Algorithm 1, we make use of a number of functions not defined with pseudocode:
• findOptB&B(c, q, t): creates a JSP with activity durations defined based on the q
value passed in and conducts a deterministic branch-and-bound search for up to t CPU
seconds using c as an upper bound on the deterministic makespan. When the search
tree is exhausted or the time-limit is reached, the best deterministic solution found
(i.e., the one with minimum makespan), together with its deterministic makespan are
returned. No Monte Carlo simulation is done.
• findNextB&B(c, q, t): this function produces a sequence of solutions (one solution
each time it is called) whose deterministic makespan is less than c. The problem is
defined using the q value and t is the CPU time limit. The solutions produced are the
leaves of the B&B search tree in the order encountered by the algorithm. Note that in
Algorithm 2, the c value does not change. Given enough CPU time, the algorithm will
evaluate the probabilistic makespan of all solutions whose deterministic makespan is
less than or equal to Dinitial .
• simulate(s): our standard Monte Carlo simulation is run on solution s and D(s), the
estimate of its probabilistic makespan, Dα (s), is returned.

The algorithm is not complete, even if the choice of q value results in deterministic
makespans that are lower bounds on the probabilistic makespan. This is because there is
no guarantee that the optimal probabilistic solution will have a deterministic makespan less
than Dinitial and therefore, even with infinite CPU time, it may not be evaluated.
The algorithm is called B&B-TBS for B ranch-and-B ound-T imed B etter S olution: a
fixed CPU time is spent to find a good deterministic solution, and then any deterministic
solution found that is as good as or better than the initial solution is simulated.
5.2.4 B&B-I-BS: An Iterative Heuristic Tree Search Algorithm
A more extreme filtering algorithm first finds an optimal deterministic solution and uses
the deterministic makespan as a filter for choosing the solutions to simulate. Using a fixed
207

Beck & Wilson

B&B-I-BS(q):
Returns the solution with smallest probabilistic makespan found
1
2
3
4
5
6
7
8
9

10
11

(s∗ , Dinitial ) ← findOptB&B(∞, q, t0 − 1)
D∗ ← simulate(s∗ )
i←0
while not timed-out do
while search is not complete do
(s, makeq ) ← findNextB&B(Dinitial × (1 + i/100) + 1, q, time-remaining)
D ← simulate(s)
if D < D ∗ then
s∗ ← s; D∗ ← D
end
end
i←i+1
end
return s∗
Algorithm 3: B&B-I-BS: An Iterative Heuristic Tree Search Algorithm

q value, an optimal solution is found and then simulated. If there is CPU time remaining,
the search does a series of iterations starting by using the optimal deterministic makespan
as the bound. All solutions with a deterministic makespan as good as (or, in general, better
than) the current bound are found and simulated. In subsequent iterations, the bound
on the deterministic makespan is increased, resulting in a larger set of solutions being
simulated. The solution with the lowest estimated probabilistic makespan is returned. On
larger problems, an optimal deterministic makespan may not be found within the CPU
limit. In such a case, the best deterministic solution that is found is simulated and returned
(i.e., only one simulation is done).
More formally, after finding an optimal deterministic solution with makespan, make q ∗ ,
a series of iterations beginning with i = 0 is executed. For each iteration, the bound on
deterministic makespans is set to makeq ∗ ×(1+i/100). All solutions, se , whose deterministic
makespans, makeq (se ) ≤ makeq ∗ × (1 + i/100), are simulated and the one with the lowest
probabilistic makespan is returned. Algorithm 3 presents pseudocode which depends on
functions defined above.
The algorithm is complete. When i is large enough so that the cost bound is greater
than the deterministic makespan of all activity permutations, they will all be simulated.
However, i may have to grow unreasonably large and therefore we treat this algorithm as,
practically, incomplete.
We refer to this algorithm as B&B-I-BS for B ranch-and-B ound-I terative-B est S olution.
5.2.5 Heuristic and Constraint Propagation Details
The algorithms described above use texture-based heuristics to decide on the pair of activities to sequence and which sequence to try first. The heuristic builds resource profiles that
combine probabilistic estimates of the contention that each activity has for each resource
and time-point. The maximum point in the resource profiles is selected and an activity
208

Proactive Algorithms for JSP

pair that contends for the resource at the selected time-point is heuristically chosen. The
sequence chosen is the one that maximizes the remaining slack. The intuition is that a
pair of activities that is contending for a highly contended-for resource and time-point is
a critical pair of activities that should be sequenced early in the search. Otherwise, via
constraint propagation from other decisions, the time windows of these activities may be
pruned to the point that neither sequence is possible. The texture-based heuristics have a
complexity at each search node of O(mn2 ) where m is the number of resources and n is the
number of activities on each resource.
For a detailed description and analysis of the texture-based heuristic see the work of
Beck and Fox (2000) and Beck (1999).
When constraint propagation is used (i.e., all algorithms above except B&B-N), we
use the strong constraint propagation techniques in constraint-based scheduling: temporal
propagation, timetables (Le Pape, Couronné, Vergamini, & Gosselin, 1994), edge-finder
(Nuijten, 1994), and the balance constraint (Laborie, 2003).
5.3 Local Search Algorithms
There is no reason why a deterministic filtering search algorithm needs to be based on
branch-and-bound. Indeed, given our approach of finding and simulating only solutions
with low deterministic makespans, algorithms based on local search may perform better
than constructive search algorithms.
In this section, we present two deterministic filtering algorithms based on tabu search. 7
We define each algorithm and then discuss the details of the tabu search procedure itself.
5.3.1 Tabu-TBS: A Tabu Search Analog of B&B-TBS
The central idea behind using tabu search for deterministic filtering search is to generate a
sequence of promising deterministic solutions which are then simulated. It seems reasonable
to create an analog of B&B-TBS using tabu search. For a fixed q and for a fixed amount
tinitial of CPU time, at the beginning of a run, a solution with the lowest possible deterministic makespan, Dinitial , is sought. Search is then restarted and whenever a solution, s,
is found that has a deterministic makespan makeq (s) ≤ Dinitial , Monte Carlo simulation is
used to approximate the probabilistic makespan. The solution with the lowest estimated
probabilistic makespan is returned.
Algorithm 4 presents the pseudocode for this simple approach. We use the following
functions (pseudo-code not given):
• findBestTabu(c, q, t): this function is analogous to findOptB&B(c, q, t). Tabu search is
run for up to t CPU seconds and the solution with the lowest deterministic makespan
(based on the q value) that is less than c is returned.
• findNextTabu(c, q, t): this function is analogous to findNextB&B(c, q, t). A sequence
of solutions (one solution each time it is called) whose deterministic makespan is less
7. Early experiments explored an even simpler way of using tabu search to solve the probabilistic JSP by
incorporating simulation into the neighborhood evaluation. Given a search state, the move operator (see
Section 5.3.3 for details) defines the set of neighboring states. For each neighbor, we can run a Monte
Carlo simulation and choose the neighbor with the lowest probabilistic makespan. This technique, not
surprisingly, proved impractical as considerable CPU time was spent to determine a single move.

209

Beck & Wilson

Tabu-TBS(q):
Returns the solution with lowest probabilistic makespan found
1
2
3
4
5
6
7

8

(s∗ , Dinitial ) ← findBestTabu(∞, q, tinitial )
D∗ ← ∞
while termination criteria unmet do
(s, D) ← findNextTabu(Dinitial + 1, q, time-remaining)
D0 ← simulate(s)
if D0 < D∗ then
s∗ ← s; D∗ ← D0
end
end
return s∗
Algorithm 4: Tabu-TBS: A Local Search Filtering Algorithm
than c is returned. The problem is defined using the q value and t is the CPU time
limit. The solution produced is the next solution found by the tabu search that meets
the makespan requirement.

We call this algorithm Tabu-TBS for Tabu-T imed B etter S olution.
As with B&B-TBS, the c value is not updated in each iteration. The initial search (line
1) is used to find a good deterministic solution and simulation is done on solutions whose
deterministic makespan is better than that of the solution found by the initial search.
5.3.2 Tabu-I-BS: An Iterative Tabu Search Algorithm
The core tabu search implementation for fixed durations does not necessarily use the entire
CPU time (see Section 5.3.3) and, in fact, especially on small instances often terminates
very quickly. We can therefore create an iterative tabu-based solver for the probabilistic
JSP similar to B&B-I-BS.
In the first phase, using a time limit that is one second less than the overall time limit,
tabu search is used to find a very good deterministic solution, based on a fixed q value.
That solution is then simulated. Because the tabu search may terminate before the time
limit has expired, any remaining time is spent generating solutions with a deterministic
makespan within a fixed percentage of the initial solution’s deterministic makespan. As
with B&B-I-BS, iterations are run with increasing i value starting with i = 0. In each
iteration, we simulate solutions found by the tabu search whose deterministic makespan is
at most (1 + i/100)Dinitial , where Dinitial is the value of the deterministic makespan found
in phase 1. The solution with the lowest probabilistic makespan is returned.8
The algorithm is termed Tabu-I-BS for Tabu-I terative-B est S earch. The pseudocode
for this algorithm is presented in Algorithm 5.
5.3.3 Tabu Search Details
The tabu search used to find solutions to problems with deterministic durations is the TSAB
algorithm due to Nowicki and Smutnicki (1996). A very restricted move operator (termed
8. The Tabuf algorithm proposed in Beck and Wilson (2004) corresponds to the first iteration of Tabu-I-BS.

210

Proactive Algorithms for JSP

Tabu-I-BS(q):
Returns the solution with smallest probabilistic makespan found
1
2
3
4
5
6
7
8
9

10
11

(s∗ , Dinitial ) ← findBestTabu(∞, q, t0 − 1)
D∗ ← simulate(s∗ )
i←0
while not timed-out do
while termination criteria unmet do
(s, makeq ) ← findNextTabu(Dinitial × (1 + i/100) + 1, q, time-remaining)
D ← simulate(s)
if D < D ∗ then
s∗ ← s; D∗ ← D
end
end
i←i+1
end
return s∗
Algorithm 5: Tabu-I-BS: An Iterative Tabu-based Filtering Algorithm

N 5 by Blazewicz, Domschke and Pesch, 1996) produces a neighborhood by swapping a
subset of the pairs of adjacent activities in the same resource of a given solution. A standard
tabu list of ten moves done in the immediate past is kept so as to escape local minima. We
use the standard aspiration criteria of accepting move on the tabu list only if the resulting
solution is better than any solution found so far.
One of the important additions to the basic tabu search mechanism in TSAB is the
maintenance of an elite pool of solutions. These are a small set (i.e., 8) of the best solutions that have been encountered so far that is updated whenever a new best solution is
encountered. When the standard tabu search stagnates (i.e., it has made a large number of
moves without finding a new best solution), search returns to one of the elite solutions and
continues search from it. That solution is removed from the set of elite solutions. Search is
terminated when either the maximum CPU time is reached or when the elite solution pool
is empty.
5.4 Summary of Algorithms
Table 1 summarizes the algorithms introduced above.

6. Empirical Investigations
Our empirical investigations address two main issues: the scaling behavior of both the
approximately complete and heuristic methods as problem size and uncertainty increase
and whether using deterministic methods, which represent the uncertainty through duration
extensions, is a useful approach. With respect to scaling, there are two interesting subquestions: first, how do the approximately complete techniques compare with each other
and, second, is there a cross-over point in terms of problem size above which the heuristic
techniques out-perform the approximately complete techniques.
211

Beck & Wilson

Deterministic
Algorithm
B&B

Complete
Yes

B&B-DQ-L

B&B

Yes

B&B-TBS

B&B

No

B&B-I-BS

B&B

Yes

Tabu-TBS

Tabu

No

Tabu-I-BS

Tabu

No

Name
B&B-N

Description
B&B with simulation at each node to find upper
and lower bounds
B&B with deterministic durations used for lower
bounds and simulation is done at each leaf node.
The durations decrease in each iteration.
Find a good deterministic solution, s, and
restart search, simulating whenever a
deterministic solution as good as s is found.
Find an optimal deterministic solution, s.
Restart search simulating whenever a
deterministic solution within i% of s is found
Repeat with increasing i.
Find a good deterministic solution, s, and
restart search simulating whenever a
deterministic solution as good as s is found.
Find as good a deterministic solution, s, as
possible. Restart search simulating whenever a
deterministic solution within i% of s is
found. Repeat with increasing i.

Table 1: A summary of the algorithms introduced to find the probabilistic makespan for an
instance of the job shop scheduling problem with probabilistic durations.

For the heuristic techniques it is necessary to assign fixed durations to each activity.
A standard approach is to use the mean duration. However, in such cases there is no
representation of the uncertainty surrounding that duration, and this does not take into
account that we want a high probability (1 − α) of execution. A more general approach is to
heuristically use the formulation for the lower bound on α-minimum makespans presented
in Section 4.2: the duration of activity Ai is defined to be µi + qσi , where q is a fixed
non-negative value, and µi and σi are (respectively) the mean and standard deviation of
the duration of Ai . Since we are no longer limited to producing a lower bound, we have
flexibility in selecting q. Intuitively, we want a q-value that leads to a situation where
good deterministic solutions also have low values of the probabilistic makespan D α (s). We
experiment with a number of q-values based on the analysis in Section 4.2 as shown in Table
2. In all cases, we set B = 1.645 (see Section 4.2) corresponding to α = 0.05. Value q 3 was
generated for each problem instance by Monte Carlo simulation: simulating 100000 paths
of n activities.
6.1 Experimental Details
Our empirical investigations examine four sets of probabilistic JSPs of size {4 × 4, 6 × 6, 10 ×
10, 20 × 20} (where a 10 × 10 problem has 10 jobs each consisting of 10 activities), and for
each set, three uncertainty levels uj ∈ {0.1, 0.5, 1} were considered. A deterministic problem
is generated using an existing generator (Watson, Barbulescu, Whitley, & Howe, 2002) with
212

Proactive Algorithms for JSP

q0
0

q1
1.645
√
2n

q2
q1 +q3
2

1.645
√
n

q3
MeanAi ∈π σi2
MeanAi ∈π σi

q

Table 2: The q-values used in the experiments. The choices of q1 and q3 are motivated by
the analysis in Section 4.2.1.

integer durations drawn uniformly from the interval [1, 99]. Three probabilistic instances
at different levels of uncertainty are then produced by setting each mean duration µ i to
be the deterministic duration of activity Ai , and by randomly drawing (using a uniform
distribution) the standard deviation σi of the duration of activity Ai from the interval [0,
uj µi ]. The distribution of each duration is approximately normal. For each problem size, we
generate 10 deterministic problems which are transformed into 30 probabilistic instances.
The problem sizes were chosen to elicit a range of behavior, from the small problems,
where the approximately complete algorithms were expected to be able to find and prove
(approximate) optimality, to the larger problems, where even the underlying deterministic
problems could not be solved to optimality within the time limit used. We chose to use
an existing generator rather than, for example, modifying existing benchmark problems,
because it allowed us to have full control over the problem structure. The three levels of
uncertainty are simply chosen to have low, medium, and high uncertainty conditions under
which to compare our algorithms.
Given the stochastic nature of the simulation and the tabu search algorithm, each algorithm is run 10 times on each problem instance with different random seeds. Each run has
a time limit of 600 CPU seconds. Each Monte Carlo simulation uses N = 1000 independent
trials.
The hardware used for the experiments is a 1.8GHz Pentium 4 with 512 MB of main
memory running Linux RedHat 9. All algorithms were implemented using ILOG Scheduler
5.3.
Recall that for the B&B-DQ-L algorithm, we employ a descending sequence of q values.
For all problems except the 20 × 20 problems, the initial q value, qinit , was set to 1.25, and
the decrement, qdec , to 0.05. For the 20 × 20 problems, a qinit value of 0.9 was used. This
change was made after observing that with qinit = 1.25, the initial tree search for the 20×20
problems would often fail to find a solution or prove that none existed within a reasonable
amount of time. We believe this is due to problem instances of that size not having any
solution with q = 1.25 that satisfied the constraint that the simulated makespan must be
less than or equal to the deterministic approximation (i.e., that makeq (se ) ≤ D(se )—see
Section 5.2.2), but yet having a search space that is sufficiently large to require a significant
amount of search to prove it. Reducing qinit to 0.9 results in an initial solution being found
quickly for all instances.
Our primary evaluation criterion is the mean normalized probabilistic makespan (MNPM )
that each algorithm achieved on the relevant subset of problem instances (we display the
data for different subsets to examine algorithm performance for different problem sizes and
uncertainty levels). The mean normalized probabilistic makespan is defined as follows:
213

Beck & Wilson

MNPM (a, L) =

1 X D(a, l)
|L|
Dlb (l)

(1)

l∈L

where L is a set of problem instances, D(a, l) is our mean estimate of the probabilistic
makespan found by algorithm a on l over 10 runs, Dlb (l) is the lower bound on the probabilistic makespan for l. For all problems except 20 × 20, the Dlb is found by solving the
deterministic problems using q1 , a simple, very plausibly α-sufficient q-value (see Section
4.2 and Table 2). Each instance was solved using constraint-based tree search incorporating
the texture-based heuristics and the global constraint propagation used above. A maximum
time of 600 CPU seconds was given. All (deterministic) problems smaller than 20 × 20 were
easily solved to optimality. However, none of the 20×20 problems were solved to optimality.
Because of this, the Dlb values were chosen to represent the best solutions found, and so
are not true lower bounds.
6.2 Results and Analysis
Table 3 presents an overview of the results of our experiments for each problem size and
uncertainty level. The results for q = q2 are shown for each heuristic algorithm. There was
not a large performance difference among the non-zero q-values (q 1 , q2 and q3 ). We return
to this issue in Section 6.2.2. Each cell in Table 3 is the mean value over 10 independent
runs of each of 10 problems. Aside from the 4 × 4 instances, all runs reached the 600 CPU
second time limit. Therefore, we do not report CPU times.

Problem
Size
4×4
6×6
10 × 10
20 × 20†

Unc.
Level
0.1
0.5
1
0.1
0.5
1
0.1
0.5
1
0.1
0.5
1

B&B Complete
N
DQ-L
1.027* 1.023*
1.060* 1.049*
1.151*
1.129
1.034
1.021
1.113
1.073
1.226
1.170
1.185
1.028
1.241
1.115
1.346
1.234
1.256
1.142
1.326
1.233
1.482
1.388

Algorithms
B&B Heuristic
TBS
I-BS
1.026
1.026
1.064
1.059
1.154
1.149
1.022
1.022
1.083
1.077
1.178
1.174
1.024 1.024
1.101 1.101
1.215 1.215
1.077
1.071
1.177
1.181
1.334
1.338

Tabu
TBS
I-BS
1.027 1.023
1.063 1.046
1.153 1.128
1.027
1.023
1.074
1.074
1.185 1.168
1.035
1.028
1.121
1.112
1.244
1.223
1.029 1.027
1.136 1.137
1.297 1.307

Table 3: The mean normalized probabilistic makespans for each algorithm. ‘*’ indicates a
set of runs for which we have, with high confidence, found approximately optimal
makespans. ‘†’ indicates problem sets for which normalization was done with
approximate lower bounds. The lowest MNPM found for each problem set are
shown in bold.

214

Proactive Algorithms for JSP

An impression of the results can be gained by looking at the bold entries that indicate the
lowest mean normalized probabilistic makespan (MNPM) that was found for each problem
set. B&B-N and B&B-DQ-L find approximately optimal solutions only for the smallest
problem set, while the B&B-DQ-L and Tabu-I-BS find the lowest probabilistic makespans
for both the 4 × 4 and 6 × 6 problems. Performance of the complete B&B techniques,
especially B&B-N, degrade on the 10 × 10 problems where the heuristic B&B algorithms
find the lowest probabilistic makespans. Finally, on the largest problems, the tabu-based
techniques are clearly superior.
One anomaly in the overall results in Table 3 can be seen in the B&B-N and B&B-DQ-L
entries for the 4 × 4 problems. On two of the three uncertainty levels both algorithms terminate before the limit on CPU time resulting in approximately optimal solutions. However,
the mean normalized probabilistic makespans are lower for the B&B-DQ-L algorithm. We
conjecture that this is an artifact of the B&B-DQ-L algorithm that biases the simulation
toward lower probabilistic makespan values. In B&B-N, a particular solution, s, is only
simulated once to find D(s). In B&B-DQ-L, the same solution may be simulated multiple
times leading to the bias. As an illustration, assume B&B-DQ-L finds an approximately
optimal solution s∗ while searching the tree corresponding to q = q 0 > 0. On a subsequent
iteration with q = q 00 < q 0 , provided that the deterministic makespan is less than the previously identified probabilistic makespan (i.e., makeq (s∗ ) < D(s∗ )), solution s∗ will be found
again and simulated again. The actual identity of the current best solution is not used to
determine which solutions to simulate. At each subsequent simulation, if a lower value for
D(s∗ ) is generated, it will replace the previous lowest probabilistic makespan value. This
leads to a situation where we may re-simulate the same solution multiple times, keeping the
lowest probabilistic makespan that is found in any of the simulations. Similar re-simulation
is possible with the Tabu-I-BS algorithm.
To test the statistical significance of the results in Table 3, we ran a series of randomized
paired-t tests (Cohen, 1995) with p ≤ 0.005. The results of these statistical tests are
displayed in Table 4 for the different problem sizes. The different uncertainty levels have
been collapsed so that, for example, the 4 × 4 statistics are based on all of the 4 × 4
instances. The informal impression discussed above is reflected in these tests with B&BDQ-L and Tabu-I-BS dominating for the two smallest problem sizes, the branch-and-bound
heuristic approaches performing best for the 10×10 problems, and the tabu-based techniques
delivering the best results for the 20 × 20 problems.
Overview. Our primary interpretation of the performance of the algorithms in these
experiments is as follows. For the smaller problems (4×4 and 6×6), the complete techniques
are able to cover the entire search space or at least a significant portion of it. Though in the
case of B&B-DQ-L, the solutions which are chosen for simulation are heuristically driven
by deterministic makespan values, the lower bound results of Section 4.2 ensure that very
good solutions will be found provided that iterations with small q values can be run within
the CPU time limit. On the 10 × 10 problems, the complete techniques are not able to
simulate a sufficient variety of solutions as, especially for B&B-N, the heuristic guidance
is poor. Note, however, that B&B-DQ-L is competitive with, and, for many problems
sets, better than the tabu-based algorithms on the 10 × 10 problems. We believe that
the 10 × 10 results stem from the ability of the B&B heuristic algorithms to quickly find
215

Beck & Wilson

Problem
Size
4×4
6×6
10 × 10
20 × 20

Statistical Significance
(p ≤ 0.005)
{B&B-DQ-L, Tabu-I-BS} < {B&B-TBS, B&B-I-BS, Tabu-TBS, B&B-N}
{B&B-DQ-L, Tabu-I-BS} < {B&B-I-BS} < {B&B-TBS} < {Tabu-TBS} < {B&B-N}
{B&B-TBS, B&B-I-BS} < {Tabu-I-BS, B&B-DQ-L, Tabu-TBS}∗ < {B&B-N}
{Tabu-TBS, Tabu-I-BS} < {B&B-TBS, B&B-I-BS} < {B&B-DQ-L} < {B&B-N}

Table 4: The statistically significant relationships among the algorithms for the results
shown in Table 3. Algorithms within a set show no significant difference. The
‘<’ relation indicates that the algorithms in the left-hand set have a significantly
lower MNPM than the algorithms in the right-hand set. The set indicated by ∗
represents a more complicated relationship amongst the algorithms: Tabu-I-BS <
Tabu-TBS but all other pairs in the set show no significant performance differences.

the optimal deterministic solution and then to systematically simulate all solutions with
deterministic makespans that are close to optimal. In contrast, the tabu-based algorithms
do not systematically enumerate these solutions. Finally, for the largest problems, we
hypothesize that tabu search techniques result in the best performance as they are able to
find better deterministic solutions to simulate.
Problem Size. As the size of the problems increase, we see the not-unexpected decrease
in the quality of the probabilistic makespans found. A simple and reasonable explanation
for this trend is that less of the search space can be explored within the given CPU time for
the larger problems. There are likely to be other factors that contribute to this trend (e.g.,
the quality of the lower bound may well systematically decrease as problem size increases).
Uncertainty Level. The normalized makespan values also increase within a problem size
as the uncertainty level rises. As these results are calculated by normalization against a
lower bound, it is possible that the observed decrease in solution quality is actually due to
a decrease in the quality of the lower bound rather than a reduction in the quality of the
solutions found by the algorithms as uncertainty increases. To test this idea, in Table 5 we
normalized the 4 × 4 results using the optimal probabilistic makespans found by B&B-N
rather than the deterministic lower bound. The table shows that for the algorithms apart
from B&B-DQ-L and Tabu-I-BS, the trend of increasing mean normalized probabilistic
makespan is still evident. For these algorithms, at least, the putative decreasing quality
of the lower bound cannot be the entire explanation for the trend of worse performance
results for higher levels of uncertainty. In Section 6.2.2, we revisit this question and provide
evidence that could explain why the algorithms perform worse when uncertainty is increased.
These results also lend credibility to the conjecture that the observed “super-optimal”
performance of B&B-DQ-L and Tabu-I-BS on the small problems is due to repeatedly
simulating the same solution. At low levels of uncertainty, repeated simulations of the truly
best solution will not vary greatly, resulting in a MNPM value of about 1. With higher levels
of uncertainty, the distribution of simulated makespans is wider and, therefore, repeated
simulation of the same solution biases results toward smaller probabilistic makespan values.
This is what we observe in the results of B&B-DQ-L and Tabu-I-BS in Table 5.
216

Proactive Algorithms for JSP

Unc.
Level
0.1
0.5
1

B&B Complete
N
DQ-L
1.004
1.000
1.008
0.998
1.015
0.996

Algorithms
B&B Heuristic
TBS
I-BS
1.003
1.002
1.012
1.008
1.018
1.013

Tabu
TBS
I-BS
1.003 0.999
1.011 0.995
1.017 0.996

Table 5: The mean normalized probabilistic makespans for each algorithm on the 4 × 4
problem set normalized by the optimal probabilistic makespans found by B&B-N.

In the balance of this section, we turn to more detailed analysis of the algorithms.
6.2.1 Analysis: B&B Complete Algorithms
The performance of B&B-N is poor when it is unable to exhaustively search the branchand-bound tree. The high computational cost of running simulation at every node and the
relatively weak lower bound that partial solutions provide9 conspire to result in a technique
that does not scale beyond very small problems.
Problem
Size
4×4
6×6
10 × 10
20 × 20

Uncertainty
0.1
0.5
0
0
0
0.5
0.95 0.85
0.9
0.9

Level
1
0
0.75
0.9
0.9

Table 6: The lowest q value used for each problem size and uncertainty level for the B&BDQ-L. For all problems except 20 × 20, the initial q value is 1.25. For the 20 × 20
problems, the initial q value is 0.9

B&B-DQ-L is able to perform somewhat better than B&B-N on larger problems even
when it is not able to exhaustively search each tree down to q = 0. Table 6 shows the
minimum q values attained for each problem size and uncertainty level. The deterministic
durations defined by the q value serve to guide and prune the search for each iteration and,
therefore, as with the heuristic algorithms (see below), the search is heuristically guided
to the extent that solutions with low deterministic makespans also have low probabilistic
makespans. However, the characteristics of the solutions found by the search are unclear.
Recall that B&B-DQ-L starts with a high q value that, in combination with the constraint
that the deterministic makespan must be less than or equal to the best simulated probabilis9. One idea for improving the lower bound that we did not investigate is to incorporate the resourcebased propagators (e.g., edge-finding) into the evaluation of a partial solution. In a single trial at an
internal node, the deterministic makespan is found by sampling from the distributions and then finding
the longest path in the temporal network. After the sampling, however, it is possible to apply the
standard propagation techniques which might insert additional edges into the precedence graph and
thereby increase the makespan, improving the lower bound.

217

Beck & Wilson

tic makespan found so far, significantly prunes the search space. Ideally, we would like the
search with high q to find solutions with very good probabilistic makespans both because
we wish to find good solutions quickly and because the simulated probabilistic makespan
values are used to prune subsequent search with lower q values. Therefore, in an effort
to better understand the B&B-DQ-L search, we examine the characteristics of the initial
solutions it finds.
Some idea of the quality of the solutions produced by high q values can be seen by
comparing the probabilistic makespan found with high q (the first solution found) with
the best solution found in that run. Table 7 presents this comparison in the form of D f ,
the mean normalized makespans for the initial solutions found by B&B-N and B&B-DQL. These data indicate that the first solution found by B&B-DQ-L is much better than
that found by B&B-N. When B&B-N searches for its initial solution, the upper bound
on the deterministic makespan does not constrain the problem: a solution is therefore
very easy to find (i.e., with no backtracking) but there is little constraint propagation or
heuristic information available to guide the search to a solution with a small makespan. In
contrast, when B&B-DQ-L searches for an initial solution, the high q value means that it
is searching in a highly constrained search space because the deterministic makespan must
be less than the probabilistic makespan. Therefore, there is a very tight upper bound on
the deterministic makespan (relative to the durations that incorporate the q values). In
many cases, the initial iterations fail to find any feasible solutions, but do so very quickly.
Eventually, the q value is low enough to allow a feasible solution, however the search for that
solution is strongly guided by propagation from the problem constraints. In summary, the
initial search for B&B-N has no guidance from the constraint propagation toward a good
solution while that of B&-DQ-L is guided by constraint propagation in an overly constrained
problem. Table 7 shows that, in these experiments, such guidance tends to result in better
initial solutions. We believe that this observation may be useful more generally in constraint
solving (see Section 7).
To provide a fuller indication of the performance differences, Table 7 also presents the
improvement over the first solution that is achieved: the difference between the first solution
and the last solution (Dl ) found by each algorithm (Dl is the value reported in Table 3).
On the larger problem sets, the improvement made on the first solution by B&B-DQ-L
is greater. For the smaller problem sets, the improvement by B&B-N is greater than for
B&B-DQ-L, however, we suspect a ceiling effect reduces the amount that B&B-DQ-L can
improve (i.e., the initial solutions are already quite close to optimal).
6.2.2 Analysis: Heuristic Algorithms
We now turn to the performance of the heuristic algorithms. We first examine the hypothesis
that their performance is dependent on two factors: the ability of the algorithms to find
solutions with low deterministic makespans and the correlation between good deterministic
and probabilistic makespans. Then we turn to an analysis of the effect of the differing q
values on the heuristic algorithm performance.
Finding Good Deterministic Makespans. It was argued above that the performance
of the heuristic techniques (and B&B-DQ-L) is dependent upon the ability to find solutions
with good deterministic makespans. To provide evidence for this argument, we looked
218

Proactive Algorithms for JSP

Problem
Size
4×4
6×6
10 × 10
20 × 20

Unc.
Level
0.1
0.5
1
0.1
0.5
1
0.1
0.5
1
0.1
0.5
1

B&B-N
Df
Df − D l
1.089
0.062
1.119
0.059
1.227
0.076
1.106
0.072
1.163
0.050
1.301
0.075
1.191
0.006
1.258
0.017
1.369
0.005
1.259
0.003
1.332
0.004
1.494
0.008

B&B-DQ-L
Df
Df − D l
1.028
0.005
1.078
0.029
1.165
0.036
1.067
0.046
1.108
0.035
1.221
0.051
1.069
0.045
1.151
0.050
1.269
0.054
1.168
0.026
1.242
0.009
1.404
0.016

Table 7: The mean normalized makespan for the first solutions found by each algorithm
(Df ) and the difference between the mean normalized makespans of the first and
last solutions (Df − Dl ).

at the quality of the best deterministic solutions found by B&B-I-BS and Tabu-I-BS. We
hypothesize that the better performing algorithm will also have found better deterministic
solutions than the worse performer.
Table 8 presents results for each algorithm on the two largest problem sets. 10 The mean
normalized deterministic makespan (MNDM ) is calculated as follows:
MNDM (a, L) =

makeq (a, l)
1 X
|L|
makeq,min (l, B&B − I − BS)

(2)

l∈L

where L is a set of problem instances, makeq (a, l) is the mean deterministic makespan found
by algorithm a on l over 10 runs, makeq,min (l, B&B − I − BS) is the lowest deterministic
makespan found by the B&B-I-BS algorithm over all runs on problem l. MNDM, therefore,
provides a relative measure of the quality of the average deterministic makespans from the
two algorithms: the higher the value, the worse the average makespan found relative to
B&B-I-BS.
Table 8 is consistent with our hypothesis. On the 10 × 10 problems, where B&B-I-BS
outperforms Tabu-I-BS, the former is able to find solutions with a lower mean deterministic
makespan. For the 20 × 20 problems the results are reversed with Tabu-I-BS finding both
better mean deterministic makespans and better probabilistic makespans.
This result lends support to the original motivation for the deterministic filtering algorithms: the performance of these algorithms in terms of probabilistic solution quality is
positively related to the quality of the deterministic solutions they are able to find. The
next section addresses the question of why this performance relationship is observed.
10. We show only the 10 × 10 and 20 × 20 problems sets as they are not influenced by the conjectured
repeated simulation behavior of Tabu-I-BS.

219

Beck & Wilson

Problem
Size
10 × 10
20 × 20

Uncertainty
Level
0.1
0.5
1
0.1
0.5
1

MNDM
B&B-I-BS Tabu-I-BS
1.000
1.002
1.000
1.004
1.000
1.004
1.045
1.002
1.041
0.998
1.037
1.002

Table 8: The mean normalized deterministic makespan (MNDM) for B&B-I-BS and TabuI-BS.

The Correlation Between Deterministic and Probabilistic Makespan. The ability of the algorithms to find good deterministic makespans would be irrelevant to their
ability to find good probabilistic makespans without some correlation between the two. It
is reasonable to expect that the level of uncertainty in a problem instance has an impact on
this correlation: at low uncertainty the variations in duration are small, meaning that we
can expect the probabilistic makespan to be relatively close to the deterministic makespan.
When the uncertainty level is high, the distribution of probabilistic makespans for a single
solution will be wider, resulting in less of a correlation. We hypothesize that this impact of
uncertainty level contributes to the observed performance degradation (see Tables 3 and 5)
of the heuristic techniques with higher uncertainty levels as problem size is held constant.
To examine our hypothesis we generated 100 new 10 × 10 deterministic JSP problem
instances with the same generator and parameters used above. The standard deviations
for the duration of each activity in the 100 instances were generated independently for
each of five uncertainty levels uj ∈ {0.1, 0.5, 1, 2, 3} resulting in a total of 500 problem
instances (100 for each uncertainty level). For each instance and for each of the four q
values (as in Table 2), we then randomly generated 100 deterministic solutions which were
then simulated. Using the R statistical package (R Development Core Team, 2004), we
measured the correlation coefficient for each problem set. Each cell in Table 9 is the result
of 10000 pairs of data points: the deterministic and probabilistic makespans for 100 random
deterministic solutions for each of 100 problem instances.
Uncertainty Level
0.1
0.5
1
2
3

q0
0.9990
0.9767
0.9176
0.8240
0.7381

q1
0.9996
0.9912
0.9740
0.9451
0.9362

q2
0.9996
0.9917
0.9751
0.9507
0.9418

q3
0.9995
0.9909
0.9736
0.9517
0.9423

Table 9: The correlation coefficient (r) comparing pairs of deterministic and probabilistic makespans for a set of 10 × 10 probabilistic JSPs. Each cell represents the
correlation coefficient for 10000 deterministic, probabilistic pairs.

220

Proactive Algorithms for JSP

Table 9 supports our explanation for the performance of the heuristic techniques. As
the uncertainty level increases, the correlation between the deterministic makespan and
corresponding probabilistic makespan lessens. The strength of the correlation is somewhat
surprising: even for the highest uncertainty level where the standard deviation of the duration of an activity is uniformly drawn from between 0 and 3 times its mean duration,
the correlation is above 0.94 for q2 and q3 . This is a positive indication for the heuristic
algorithms as it suggests that they may scale well to higher uncertainty levels provided a
reasonable q value is used. We examine the impact of the q values in the original experiments and the implications of the deterministic/probabilistic makespan correlation in the
next section.
It should be emphasized that these results are based on correlations between deterministic and probabilistic makespans for randomly generated solutions. We have not addressed
how these correlations might change for high-quality solutions, which might be considered
as a more appropriate population from which to sample. One technical difficulty for the
design of an experiment to examine this, is to ensure a sufficiently randomized sample from
the population of “good” solutions; also, the result could depend strongly on the (rather
arbitrary) particular choice of quality cutoff for solutions.
The Effect of the q Values. Each of the heuristic algorithms requires a fixed q value.11
We experimented with four different values (see Table 2). Table 10 displays the significant pairwise differences among the q values for each heuristic as measured by randomized
paired-t tests (Cohen, 1995) with p ≤ 0.005. As can be observed, there are almost no
significant differences for low levels of uncertainty (0.1 or 0.5) or for the smallest problem
set. For higher levels of uncertainty and larger problems, using q0 is never better than
using one of the higher q values and in many cases, q0 results in the worst mean makespan.
Among the other q-values, for the majority of the problem sets and algorithms there are no
significant differences. For a given algorithm, it is never the case that a lower q value leads
to significantly better results than a higher q value.
The correlation results in Table 9 provide an explanation for these differences. For the
10 × 10 problems, the performance of the q0 algorithms is competitive when there is not
a large difference in the correlations between deterministic and probabilistic solutions (i.e.,
at uncertainty levels 0.1 and 0.5). When the uncertainty level is 1, there is a significant
reduction in the correlation coefficient for q0 and a corresponding reduction in the mean
normalized probabilistic makespans found by the algorithms using q0 .
6.3 Summary
The results of our experiments can be summarized as follows:
• The most principled use of simulation (B&B-N) is only useful on small problems.
The simulation time is a major component of the run-time resulting in very little
exploration of the search space.
• Algorithm B&B-DQ-L, based on the idea of iteratively reducing a parameter that determines the validity of the lower bound, results in equal performance on small prob11. We are not addressing the behavior of B&B-DQ-L, where the q descends during the run of the algorithm.
We are only examining the algorithms with fixed q values.

221

Beck & Wilson

Problem
Size
4×4

6×6

10 × 10

20 × 20

Unc.
Level
0.1
0.5
1
ALL
0.1
0.5
1
ALL
0.1
0.5
1
ALL
0.1
0.5
1
ALL

B&B
TBS
I-BS
q2 < {q1 , q3 } < q0
q2 < {q0 , q1 }
{q1 , q2 , q3 } < q0
{q1 , q2 , q3 } < q0
{q1 , q2 , q3 } < q0
{q1 , q2 , q3 } < q0
{q1 , q2 } < q0
{q2 , q3 } < q0
q2 < q 1
q2 < q1 < q0
{q2 , q3 } < q0
q3 < q 0

Tabu
TBS
I-BS
q 2 < q1
{q1 , q2 , q3 } < q0
{q1 , q2 , q3 } < q0
q1 < q 0
{q1 , q3 } < q0
{q2 , q3 } < q0
q1 < q 0
{q1 , q2 , q3 } < q0
q 2 < q0
{q1 , q2 , q3 } < q0 {q1 , q2 , q3 } < q0
{q1 , q2 , q3 } < q0

{q1 , q2 , q3 } < q0

Table 10: The results of pair-wise statistical tests for each algorithm and problem set. The
notation a < b indicates that the algorithm using q = a achieved a significantly
better solution (i.e., lower probabilistic makespan) than when it used q = b. ‘-’
indicates no significant differences. All statistical tests are randomized paired-t
tests (Cohen, 1995) with p ≤ 0.005.

lems and much better performance on larger problems when compared to B&B-N.
More work is needed to understand the behavior of the algorithm, however preliminary evidence indicates that it is able to find good solutions quickly in the current
application domain.
• A series of heuristic algorithms were proposed based on using deterministic makespan
to filter the solutions which would be simulated. It was demonstrated that the performance of these algorithms depends on their ability to find good deterministic
makespans and the correlation between the quality of deterministic and probabilistic solutions. It was shown that even for problems with a quite high uncertainty
level, deterministic problems can be constructed that lead to a strong deterministic/probabilistic makespan correlation.
• Central to the success of the heuristic algorithms was the use of a q value that governed
the extent to which duration uncertainty was represented in the durations of activities
in deterministic problems. It was shown that such an incorporation of uncertainty
data leads to a stronger correlation between deterministic and probabilistic makespans
and the corresponding ability to find better probabilistic makespans.
222

Proactive Algorithms for JSP

7. Extensions and Future Work
In this section, we look at three kinds of extensions of this work. First, we show that our
theoretical framework in fact applies to far more general probabilistic scheduling problems
than just job shop scheduling. In Section 7.2, we discuss ways in which the algorithms
for probabilistic JSP presented in this paper might be improved. Finally, we discuss the
possibility of developing the central idea in the B&B-DQ-L algorithm into a solving approach
for general constraint optimization problems.
7.1 Generalization to Other Scheduling Problems
The results in this paper have been derived for the important case of job shop scheduling
problems. In fact, they are valid for a much broader class of scheduling problems, including
resource-constrained project scheduling problems of a common form (e.g., a probabilistic
version of the deterministic problems studied in the work of Laborie, 2005). In this section,
we describe how to extend our framework and approaches.
Our approach relies on the fact that in the job shop scheduling problem, one can focus
on orderings of activities, rather than directly on assignments of start times for activities;
specifically, the definition of minimum makespan based on orderings is equivalent to the
one based on start time assignments; this equivalence holds much more generally.
First, in 7.1.1, we give some basic definitions and properties which are immediate extensions of those defined in Section 2. Then, in 7.1.2, we characterize a class of scheduling
problems which have the properties we require, by use of a logical expression to represent
the constraints of the problem. In 7.1.3 we give the key result relating the schedule-based
minimum makespan with the ordering-based minimum makespan. Section 7.1.4 discusses
the extended class of probabilistic scheduling problems, and Section 7.1.5 considers different
optimization functions.
7.1.1 Schedules, Orderings and Makespans
As in Section 2, we are given a set A of activities, where activity Ai ∈ A has an associated positive duration di (for the deterministic case). A schedule (for A) is defined to
be a function from the set of activities to the set of time-points (which are non-negative
numbers), defining when each activity starts. Let Z be a schedule. The makespan make(Z)
of schedule Z is defined to be the time at which the last activity has been completed, i.e.,
maxAi ∈A (Z(Ai ) + di ). We say that Z orders Ai before Aj if and only if Aj starts no earlier
than Ai ends, i.e., Z(Ai ) + di ≤ Z(Aj ).
An essential aspect of job shop problems and our approach is that one can focus on
orderings of the activities rather than on schedules; in Section 2 we use the term solution
for an ordering that satisfies the constraints of a given JSP. Define an ordering (on A) to be
a strict partial order on A, i.e., an irreflexive and transitive relation on the set of activities.
Hence, for ordering s, for all Ai ∈ A, (Ai , Ai ) ∈
/ s, and if (Ai , Aj ) ∈ s and (Aj , Ak ) ∈ s,
then (Ai , Ak ) ∈ s. If (Ai , Aj ) ∈ s, then we say that s orders Ai before Aj ; we also say that
Ai is a predecessor of Aj . A path in s (or an s-path) is a sequence of activities such that
if Ai precedes Aj in the sequence, then s orders Ai before Aj . The length len(π) of a path
π (in an ordering) is defined to be the sum of the durations of the activities in the path,
223

Beck & Wilson

P
i.e., Ai ∈π di . The makespan, make(s), of an ordering s is defined to be the length of a
longest s-path. An s-path π is said to be a critical s-path if the length of π is equal to the
makespan of the ordering s, i.e., it is one of the longest s-paths.
Any schedule has an associated ordering. For schedule Z define the ordering sol(Z) as
follows: sol(Z) orders activity Ai before Aj if and only if Z orders Ai before Aj .
Conversely, from an ordering one can define a non-delay schedule, which is optimal
among schedules compatible with the ordering, by starting each activity as soon as its
predecessors finish. Let s be an ordering. We inductively define schedule Z = sched(s) as
follows: if Ai has no predecessor, then we start Ai at time 0, i.e., Z(Ai ) = 0. Otherwise,
we set Z(Ai ) = maxAj ∈pred(Ai ) (Z(Aj ) + dj ), where pred(Ai ) is the set of predecessors of Ai .
The fact that s is acyclic guarantees that this defines a schedule. As in Section 2.1, we have
the following two important properties. The first states that the makespan of an ordering
is equal to the makespan of its associated schedule. The second states that the makespan
of a schedule is no better than the makespan of its associated ordering.
Proposition 5
(i) For any ordering s, make(sched(s)) = make(s).
(ii) For any schedule Z, make(sol(Z)) ≤ make(Z).
The proof of these is straight-forward. It follows easily by induction that if a schedule Z
respects the precedence constraints expressed by an ordering s, then the last activity of any
s-path can end no earlier in Z than the length of the path; applying this to a critical path
implies (ii) make(sol(Z)) ≤ make(Z), and implies half of (i): make(sched(s)) ≥ make(s). By
working backwards from an activity that finishes last in sched(s), and choosing an immediate
predecessor at each stage, one generates (in reverse order) a path in s whose length is equal
to make(sched(s)), hence showing that make(sched(s)) ≤ make(s), and proving (i).
7.1.2 Positive Precedence Expressions
We will define a class of scheduling problems, using what we call positive precedence expressions (PPEs) to represent the constraints. Each of these scheduling problems assumes
no preemption (so activities cannot be interrupted once started) and we will again use
makespan as the cost function.
For activities Ai and Aj , the expression before(i, j) is interpreted as the constraint (on
possible schedules) that activity Aj starts no earlier than the end of activity Ai . Such
expressions are called primitive precedence expressions. A positive precedence expression is
defined to be a logical formula built from primitive precedence expressions, conjunctions and
disjunctions. (The term “positive” is used since they do not involve negations.) Formally,
the set E of positive precedence expressions (over A) is defined to be the smallest set such
that (a) E contains before(i, j) for each Ai and Aj in A, and (b) if ϕ and ψ are in E, then
(ϕ ∧ ψ) and (ϕ ∨ ψ) are both in E.
Positive precedence expressions over A are interpreted as constraining schedules on A.
Let ϕ ∈ E be a PPE and let Z be a schedule. We define “Z satisfies ϕ” recursively as
follows:
224

Proactive Algorithms for JSP

• Z satisfies primitive precedence expression before(i, j) if and only if Z orders A i before
Aj , i.e., if Z(Ai ) + di ≤ Z(Aj );
• Z satisfies the conjunction of two constraint expressions if and only if it satisfies both
of them;
• Z satisfies the disjunction of two constraint expressions if and only if it satisfies at
least one of them.
Similarly, for ordering s and positive precedence expression ϕ we can recursively define
“s satisfies ϕ” in the obvious way: s satisfies before(i, j) if and only if s orders A i before
Aj . Ordering s satisfies (ϕ ∧ ψ) if and only if it satisfies both ϕ and ψ. Ordering s satisfies
(ϕ ∨ ψ) if and only if it satisfies either ϕ or ψ.
Positive precedence expressions are powerful enough to represent the constraints of a
job shop scheduling problem, or of a resource-constrained project scheduling problem.
JSPs as Positive Precedence Expressions. Resource constraints in a job shop scheduling problem give rise to disjunctions of primitive precedence expressions: for each pair of
activities Ai and Aj which require the same resource, the expression before(i, j) ∨ before(j, i)
which expresses that Ai and Aj do not overlap (one of them precedes the other). The
ordering of activities in a job can be expressed in terms of primitive expressions: before(i, j)
when Ai precedes Aj within some job. Hence, the constraints in a job shop problem can be
expressed as a positive precedence expression in conjunctive normal form, i.e., a conjunction
of disjunctions of primitive precedence expressions.
RCPSPs as PPEs. The constraints in a resource-constrained project scheduling problem
(RCPSP) (Pinedo, 2003; Brucker et al., 1999; Laborie & Ghallab, 1995; Laborie, 2005) can
also be expressed as a positive precedence expression in conjunctive normal form. In an
RCPSP, we have precedence constraints between activities, each of which can be expressed
as a primitive precedence expression; let ϕ be the conjunction of these. In a RCPSP, there
are again a set of resources, each with a positive capacity. Associated with each activity A i
and resource r is the rate of usage Ai (r) of resource r by activity Ai . We have the following
resource constraints on a schedule: for each resource r, at any time-point t, the sum of
Ai (r) over all activities Ai which are in progress at t (i.e., which have started by t but not
yet ended) must not exceed the capacity of resource r.
Define a forbidden set (or conflict set) to be a set of activities whose total usage of
some resource exceeds the capacity of the resource. Let F be the set of forbidden sets. (If
we wished, we could delete from F any set which is a superset of any other set in F; we
could also delete any set H which contains elements Ai and Aj such that Ai precedes Aj
according to ϕ.) The resource constraints can be expressed equivalently as: for all H ∈ F,
there exists no time at which every activity in H is in progress. This holds if and only if
for each H ∈ F, there exist two activities in H which do not overlap (since if all pairs of
activities in H overlap then all activities in H are in progress at the latest start time of
activities in H), i.e., there exists Ai , Aj ∈ H with before(i, j). Hence, a schedule satisfies the
resource constraints if and only if it satisfies the positive precedence expression ψ defined
225

Beck & Wilson

to be
^

H∈F

_

before(i, j).

Ai ,Aj ∈H
i6=j

Therefore, expression (ϕ ∧ ψ) represents the RCPSP, i.e., a schedule satisfies the constraints
of the RCPSP if and only if it satisfies (ϕ ∧ ψ).
Another class of scheduling problems, each of which can be represented by a positive
precedence expression, is the class based on AND/OR precedence constraints (Gillies &
Liu, 1995; Möhring, Skutella, & Stork, 2004).
7.1.3 Solutions and Minimum Makespan
For a fixed positive precedence expression ϕ over A, we say that schedule Z is valid if it
satisfies ϕ. We say that ordering s is a solution if it satisfies ϕ. If ordering s satisfies
before(i, j), then, by construction, sched(s) satisfies before(i, j). It also follows immediately
that schedule Z satisfies before(i, j) if and only if sol(Z) satisfies before(i, j). The following
result can then be proved easily by induction on the number of connectives in ϕ.
Lemma 2 For any PPE ϕ over A, if s is a solution, then sched(s) is a valid schedule. If
Z is a valid schedule, then sol(Z) is a solution.
The minimum makespan (for ϕ) is defined to be the infimum makespan over all valid
schedules, i.e., the infimum of make(Z) over all valid schedules Z. The minimum solution
makespan is defined to be the minimum makespan over all solutions, i.e., the minimum
of make(s) over all solutions s. The following is the key result which links the schedulebased definition of minimum makespan with the solution-based definition. It follows from
Proposition 5 and Lemma 2, since for any solution s there is a valid schedule (i.e., sched(s))
with the same value of makespan, and for any valid schedule Z there is a solution (i.e.,
sol(Z)) with at least as good a value of makespan.
Proposition 6 Let ϕ be any positive precedence expression over A. Then the minimum
makespan for ϕ is equal to the minimum solution makespan.
7.1.4 Probabilistic Scheduling Problems based on PPEs
The probabilistic versions of the scheduling problems are defined in just the same way as
for JSPs. The duration of each activity Ai is now a random variable. A positive precedence
expression is used to represent the constraints.
The further definitions in Sections 2 and 4 can all be immediately extended to this much
more general setting. All the results of the paper still hold, with exactly the same proofs.
In particular, with a probabilistic problem one associates a corresponding deterministic
problem in just the same way; the lower bound results in Section 4.2 are based on the longest
path characterization of makespan; the Monte Carlo approach (or at least its usefulness)
relies on the fact that the makespan of a solution is equal to the makespan of the associated
schedule. Furthermore, the algorithms in Section 5 extend, given that one has a method of
solving the corresponding deterministic problem.
226

Proactive Algorithms for JSP

The ordering-based policies that we use (based on fixing a partial ordering of activities,
irrespective of the sampled values of the durations) are known as Earliest Start policies
(Radermacher, 1985). These and other policies have been studied for RCPSPs (see e.g.,
Stork, 2000, however the aim in that work is to minimize expected makespan, whereas we
are attempting to minimize α-makespan).
7.1.5 Different Optimization Functions
Because our approach for evaluating and comparing solutions is based on the use of Monte
Carlo simulation to generate a sample distribution, our techniques are quite general.
Much of the work in the paper also generalizes immediately to other regular cost functions, where “regular” means that the function is monotonic in the sense that increasing the
end of any activity in a schedule will not decrease the cost. A regular function based on any
efficiently computable measurement of the sample distributions can be accommodated. For
example, we could easily adapt to situations where the probability of extreme solutions is
important by basing the optimization function on the maximum sampled makespan. Conversely, we could use measures of the tightness of the makespan distribution for situations
where minimizing variance as a measure of the accuracy of a schedule is important. Furthermore, weighted combinations of such functions (e.g., the α-makespan plus a measure of
distribution tightness) could be easily incorporated.
We can also modify our approach to account for other ways of comparing solutions
based on the sample distributions. For example, we could perform t-tests using the sample
distributions to determine if one solution has a significantly lower expected makespan.
7.2 Toward Better Algorithms for Probabilistic JSPs
There are two directions for future work on the algorithms presented in this paper. First,
B&B-N could be improved to make more use of deterministic techniques and/or to incorporate probabilistic reasoning into existing deterministic techniques. For example, a number
of deterministic lower bound formulations for PERT networks exist in the operations research literature (Ludwig, Möhring, & Stork, 2001) that may be used to evaluate partial
solutions. Similarly, perhaps the dominance rules presented by Daniels and Carrillo (1997)
for the one-machine β-robustness problem can be generalized to multiple resources. Another
approach to improving the B&B-N performance is to incorporate explicit reasoning about
probability distributions into standard constraint propagation techniques. Techniques such
as the longest path calculations and edge-finding make inferences based on the propagation
of minimum and maximum values for temporal variables. We believe that many of these
techniques can be adapted to reason about probabilistic intervals; this is related to work
done, for example, on simple temporal networks with uncertainty (Morris, Muscettola, &
Vidal, 2001; Tsamardinos, 2002).
A second direction for future work is the improvement of the heuristic algorithms. The
key advantage of these algorithms is that they make use of deterministic techniques for
scheduling: by transforming probabilistic problems into deterministic problems, we bring a
significant set of existing tools to bear on the problem. Further developments of this approach include adaptively changing q-values during the search in order to find those that lead
to solutions with better values of probabilistic makespan (Dα (s)). A deeper understanding
227

Beck & Wilson

of the relationship between good deterministic solutions and good probabilistic solutions,
building on the work here, is necessary to pursue this work in a principled fashion.
Of course, proactive techniques are not sufficient. In practice, schedules are dynamic
and need to be adapted as new jobs arrive or existing jobs are canceled. At execution time, a
reactive component is necessary to deal with unexpected (or sufficiently unlikely) disruptions
that, nonetheless, can occur. A complete solution to scheduling under uncertainty needs to
incorporate all these elements to reason about uncertainty at different levels of granularity
and under different time pressures. See the work of Bidot, Vidal, Laborie and Beck (2007)
for recent work in this direction.
7.3 Exploiting Unsound Lower Bounds in Constraint Programming
The B&B-DQ-L algorithm may represent a problem-solving approach that can be applied
beyond the current application area. If we abstract away the probabilistic JSP application,
the central idea of B&B-DQ-L is to exploit an unsound lower bound to (over)constrain the
search and then to run subsequent searches with a gradually relaxed unsound lower bound.
Such an approach may play to the strengths of constraint programming: searching within
highly constrained spaces.
For example, the assignment problem (AP) is a well-known lower bound for the traveling
salesman problem (TSP) and has been used as a cost-based constraint in the literature
(Focacci, Lodi, & Milano, 2002; Rousseau, Gendreau, Pesant, & Focacci, 2004). Given a
TSP, P , let AP (P, q) be the corresponding assignment problem with the travel distances
multiplied by q. That is, let dij be the distance between cities i and j in P and let d0ij be
the distance between cities i and j in AP (P, q). Then d0ij = dij × q for q ≥ 1. An approach
similar to that of the B&B-DQ-L algorithm can now be applied to solve the TSP.
It would be interesting to investigate how the approach compares with the traditional
optimization approach in constraint programming. It may be particularly useful in applications where the evaluation of partial solutions is very expensive but where there exists a
parameterizable, inexpensive lower bound.

8. Conclusion
In this paper, we addressed job shop scheduling when the durations of the activities are
independent random variables. A theoretical framework was created to formally define this
problem and to prove the soundness of two algorithm components: Monte Carlo simulation
to find upper bounds on the probabilistic makespan of a solution and a partial solution;
and a carefully defined deterministic JSP whose optimal makespan is a lower bound on the
probabilistic makespan of the corresponding probabilistic JSP.
We then used these two components together with either constraint programming or
tabu search to define a number of algorithms to solve probabilistic JSPs. We introduced
three solution approaches: a branch-and-bound technique using Monte Carlo simulation to
evaluate partial solutions; an iterative deterministic search using Monte Carlo simulation
to evaluate the solutions from a series of increasingly less constrained problems based on
a parameterizable lower bound; and a number of deterministic filtering algorithms which
generate a sequence of solutions to a deterministic JSP, each of which is then simulated
using Monte Carlo simulation.
228

Proactive Algorithms for JSP

Our empirical evaluation demonstrated that the branch-and-bound technique is only
able to find approximately optimal solutions for very small problem instances. The iterative
deterministic search performs as well as, or better than, the branch-and-bound approach
for all problem sizes. However, for medium and large instances, the deterministic filtering
techniques perform much more strongly while providing no optimality guarantees. Further
experimentation demonstrated that for the techniques using deterministic methods, the
correlation between the deterministic makespan and probabilistic makespan is a key factor
in algorithm performance: taking into account the variance of the duration in a deterministic
problem led to strong correlations and good algorithmic performance.
Proactive scheduling techniques seek to incorporate models of uncertainty into an offline, predictive schedule. The goal of such techniques is to increase the robustness of the
schedules produced. This is important because a schedule is not typically generated or
executed in isolation. Other decisions such as when to deliver raw materials and how to
schedule up- and down-stream factories are all affected by an individual schedule. Indeed, a
schedule can be seen as a locus of competing constraints from across a company and supply
chain (Fox, 1983). Differences between a predictive schedule and its execution can be a
significant source of disruption leading to cascading delays across widely separated entities.
The ability, therefore, to develop schedules that are robust to uncertainty is very important.
This paper represents a step in that direction.

Acknowledgments
This work has received support from the Science Foundation Ireland under grants 00/PI.1/C075
and 05/IN/I886, the Natural Sciences and Engineering Research Council of Canada, and
ILOG, SA. The authors would like to thank Daria Terekhov and Radoslaw Szymanek for
comments on previous versions of the paper. Preliminary versions of the work reported in
this paper have been published in Beck and Wilson (2004, 2005).

References
Beck, J. C. (1999). Texture measurements as a basis for heuristic commitment techniques
in constraint-directed scheduling. Ph.D. thesis, University of Toronto.
Beck, J. C., & Fox, M. S. (2000). Dynamic problem structure analysis as a basis for
constraint-directed scheduling heuristics. Artificial Intelligence, 117 (1), 31–81.
Beck, J. C., & Wilson, N. (2004). Job shop scheduling with probabilistic durations. In
Proceedings of the Sixteenth European Conference on Artificial Intelligence (ECAI04),
pp. 652–656.
Beck, J. C., & Wilson, N. (2005). Proactive algorithms for scheduling with probabilistic durations. In Proceedings of the Nineteenth International Joint Conference on Artificial
Intelligence (IJCAI05), pp. 1201–1206.
Bidot, J. (2005). A General Framework Integrating Techniques for Scheduling under Uncertainty. Ph.D. thesis, Ecole Nationale d’Ingéieurs de Tarbes.
229

Beck & Wilson

Bidot, J., Vidal, T., Laborie, P., & Beck, J. C. (2007). A general framework for scheduling
in a stochastic environment. In Proceedings of the Twentieth International Joint
Conference on Artificial Intelligence (IJCAI07), pp. 56–61.
Blazewicz, J., Domschke, W., & Pesch, E. (1996). The job shop scheduling problem: Conventional and new solution techniques. European Journal of Operational Research,
93 (1), 1–33.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research,
11, 1–94.
Brucker, P., Drexl, A., Möhring, R., Neumann, K., & Pesch, E. (1999). Resource-constrained
project scheduling: Notation, classification, models and methods. European Journal
of Operational Research, 112, 3–41.
Burns, A., Punnekkat, S., Littlewood, B., & Wright, D. (1997). Probabilistic guarantees for fault-tolerant real-time systems. Tech. rep. DeVa TR No. 44, Design for Validation, Esprit Long Term Research Project No. 20072. Available at
http://www.fcul.research.ec.org/deva.
Burt, J. M., & Garman, M. B. (1970). Monte Carlo techniques for stochastic network analysis. In Proceedings of the Fourth Annual Conference on Applications of Simulation,
pp. 146–153.
Cohen, P. R. (1995). Empirical Methods for Artificial Intelligence. The MIT Press, Cambridge, Mass.
Daniels, R., & Carrillo, J. (1997). β-robust scheduling for single-machine systems with
uncertain processing times. IIE Transactions, 29, 977–985.
Davenport, A. J., Gefflot, C., & Beck, J. C. (2001). Slack-based techniques for robust
schedules. In Proceedings of the Sixth European Conference on Planning (ECP-2001).
Davenport, A., & Beck, J. C. (2000). A survey of techniques for scheduling with uncertainty.
Tech. rep.. Available at: http://www.tidel.mie.utoronto.ca/publications.php.
Drummond, M., Bresina, J., & Swanson, K. (1994). Just-in-case scheduling. In Proceedings
of the Twelfth National Conference on Artificial Intelligence (AAAI-94), pp. 1098–
1104, Menlo Park, CA. AAAI Press/MIT Press.
Feller, W. (1968). An Introduction to Probability Theory and Its Applications (Third edition). John Wiley and Sons, New York, London.
Focacci, F., Lodi, A., & Milano, M. (2002). A hybrid exact algorithm for the TSPTW.
INFORMS Journal on Computing, 14 (4), 403–417.
Fox, M. S. (1983). Constraint-Directed Search: A Case Study of Job-Shop Scheduling. Ph.D.
thesis, Carnegie Mellon University, Intelligent Systems Laboratory, The Robotics Institute, Pittsburgh, PA. CMU-RI-TR-85-7.
Gao, H. (1995). Building robust schedules using temporal protection—an empirical study
of constraint based scheduling under machine failure uncertainty. Master’s thesis,
Department of Industrial Engineering, University of Toronto.
230

Proactive Algorithms for JSP

Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-Completeness. W.H. Freeman and Company, New York.
Ghosh, S. (1996). Guaranteeing fault tolerance through scheduling in real-time systems.
Ph.D. thesis, University of Pittsburgh.
Ghosh, S., Melhem, R., & Mossé, D. (1995). Enhancing real-time schedules to tolerate
transient faults. In Real-Time Systems Symposium.
Gillies, D. W., & Liu, J. W.-S. (1995). Scheduling tasks with AND/OR precedence constraints. SIAM J. Comput., 24, 797–810.
Hagstrom, J. N. (1988). Computational complexity of PERT problems. Networks, 18,
139–147.
Herroelen, W., & Leus, R. (2005). Project scheduling under uncertainty: Survey and research
potentials. European Journal of Operational Research, 165 (2), 289–306.
Laborie, P. (2003). Algorithms for propagating resource constraints in AI planning and
scheduling: Existing approaches and new results. Artificial Intelligence, 143, 151–188.
Laborie, P. (2005). Complete MCS-Based Search: Application to Resource-Constrained
Project Scheduling. In Proceedings of the Nineteenth International Joint Conference
on Artificial Intelligence (IJCAI05), pp. 181–186.
Laborie, P., & Ghallab, M. (1995). Planning with sharable resource constraints. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence
(IJCAI’95).
Le Pape, C., Couronné, P., Vergamini, D., & Gosselin, V. (1994). Time-versus-capacity
compromises in project scheduling. In Proceedings of the Thirteenth Workshop of the
UK Planning Special Interest Group.
Leon, V. J., Wu, S. D., & Storer, R. H. (1994). Robustness measures and robust scheduling
for job shop. IIE Transactions, 26 (5), 32–43.
Ludwig, A., Möhring, R., & Stork, F. (2001). A computational study on bounding the
makespan distribution in stochastic project networks. Annals of Operations Research,
102, 49–64.
Meuleau, N., Hauskrecht, M., Kim, K., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.
(1998). Solving very large weakly coupled markov decision processes. In Proceedings
of the Fifteenth National Conference on Artificial Intelligence (AAAI-98).
Möhring, R., Skutella, M., & Stork, F. (2004). Scheduling with AND/OR precedence constraints. SIAM J. Comput, 33 (2), 393–415.
Morris, P., Muscettola, N., & Vidal, T. (2001). Dynamic control of plans with temporal
uncertainty. In Proceedings of the Seventeenth International Joint Conference on
Artificial Intelligence (IJCAI-01).
Nowicki, E., & Smutnicki, C. (1996). A fast taboo search algorithm for the job shop problem.
Management Science, 42 (6), 797–813.
Nuijten, W. P. M. (1994). Time and resource constrained scheduling: a constraint satisfaction approach. Ph.D. thesis, Department of Mathematics and Computing Science,
Eindhoven University of Technology.
231

Beck & Wilson

Pinedo, M. (2003). Scheduling: Theory, Algorithms, and Systems (2nd edition). PrenticeHall.
R Development Core Team (2004). R: A language and environment for statistical computing.
R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0.
Radermacher, F. J. (1985). Scheduling of project networks. Annals of Operations Research,
4, 227–252.
Rousseau, L., Gendreau, M., Pesant, G., & Focacci, F. (2004). Solving VRPTWs with
constraint programming based column generation. Annals of Operations Research,
130, 190–216.
Stork, F. (2000). Branch-and-bound algorithms for stochastic resource-constrained project
scheduling. Tech. rep. 702/2000, Technische Universität Berlin, Department of Mathematics.
Tsamardinos, I. (2002). A probabilistic approach to robust execution of temporal plans with
uncertainty. In Methods and Applications of Artificial Intelligence: Proceedings of the
Second Hellenic Conference on Artificial Intelligence, Vol. 2308 of Lecture Notes in
Artificial Intelligence, pp. 97–108.
Watson, J.-P., Barbulescu, L., Whitley, L., & Howe, A. (2002). Contrasting structured
and random permutation flow-shop scheduling problems: search-space topology and
algorithm performance. INFORMS Journal on Computing, 14 (1).
Wilson, N. (2000). Algorithms for Dempster-Shafer Theory. In: Kohlas, J., Moral, S.,
(eds.) Algorithms for Uncertainty and Defeasible Reasoning, Volume 5, Handbook of
Defeasible Reasoning. Kluwer Academic Publishers.
Wurman, P., & Wellman, M. (1996). Optimal factory scheduling using stochastic dominance
A*. In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence
(UAI-96).

232

Journal of Artificial Intelligence Research 28 (2007) 431-451

Submitted 07/06; published 04/07

Discovering Classes of Strongly Equivalent Logic Programs
Fangzhen Lin

flin@cs.ust.hk

Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong

Yin Chen

gzchenyin@gmail.com

Department of Computer Science
South China Normal University
Guangzhou, P.R. China

Abstract
In this paper we apply computer-aided theorem discovery technique to discover theorems about strongly equivalent logic programs under the answer set semantics. Our discovered theorems capture new classes of strongly equivalent logic programs that can lead
to new program simplification rules that preserve strong equivalence. Specifically, with
the help of computers, we discovered exact conditions that capture the strong equivalence
between a rule and the empty set, between two rules, between two rules and one of the two
rules, between two rules and another rule, and between three rules and two of the three
rules.

1. Introduction
In this paper we apply computer-aided theorem discovery technique to discover theorems
about strongly equivalent logic programs under the answer set semantics. Our discovered
theorems capture new classes of strongly equivalent logic programs that can lead to new
program simplification rules that preserve strong equivalence.
Theorem discovery is a highly creative human process. Generally speaking, we can
divide it into two steps: (i) conjecture formulation, and (ii) conjecture verification, and
computers can help in both of these two steps. For instance, machine learning tools can
be used in the first step, i.e. in coming up with reasonable conjectures, and automated
deduction tools can be used in the second step, i.e. in verifying the correctness of these
conjectures.
While theorem discovery may make use of learning, these two tasks are fundamentally different. Theorem discovery starts with a theory, and aims at finding interesting
consequences of the theory, while learning is mostly about induction, i.e. it starts with
examples/consequences, and aims at finding a theory that would explain the given examples/consequences.
Using computers to discover theorems is an old aspiration. There have been some
success stories. For instance, AM (Lenat, 1979) was reported to be able to come up with
some interesting concepts and theorems in number theory, and the remarkable systems
described by Petkovsek, Wilf, and Zeilberger (1996) can discover many identities, especially
hypergeometric identities involving sums of binomial coefficients that are important for the
analyses of algorithms. Yet another example where “interesting” theorems can be discovered
c
2007
AI Access Foundation. All rights reserved.

Lin & Chen

almost fully automatically is a recent work by Lin (2004) on discovering state invariants in
planning domains. Lin showed that there are ways to classify many state constraints that
are useful in planning according to their syntactic properties, and enumerate them easily
for many domains. Furthermore, for many of these constraints whether they are invariants
can be checked automatically. As a result, the system described by Lin (2004) can discover
many common constraints in planning domains, and for the logistics domain, it could even
discover a set of “complete” state invariants.
Following this line of research, in this paper, we consider the problem of discovering
classes of strongly equivalent sets of logic program rules under answer set semantics. As
noted by Lifschitz, Pearce, and Valverde (2001), if two sets of rules are strongly equivalent,
then we can replace one by the other in any logic program without changing the semantics
of the program. Thus identifying strongly equivalent sets of logic program rules is a useful
exercise that may have applications in program simplification.
This paper is organized as follows. In the next section, we briefly review the basic
concepts of logic programming under answer set semantics. Then in section 3 we state in
more precise terms the type of theorems that we want to discover. In section 4 we prove
some general theorems that will help us prove these theorems, and in section 5, we describe
some of the theorems that we discovered. We then discuss an application to logic program
simplification in section 6, and finally we conclude this paper in section 7.

2. Answer Set Programming
Traditional logic programming systems like Prolog solve problems by query answering. The
user encodes knowledge about a domain by a set of rules, and solves a problem by issuing
queries to the set of rules. In contrast, Answer Set Programming (ASP) (Niemelä, 1999;
Lifschitz, 1999; Marek & Truszczynski, 1999) is a constraint-based programming paradigm.
It is based on logic programming with answer set semantics (Gelfond & Lifschitz, 1988,
1991). To solve a problem, the user encodes the domain knowledge as a logic program in
such a way that the answer sets of the program will correspond to the solutions to the original
problem. Compared to other constraint-based programming paradigms, ASP allows natural
encodings of recursive relations, and has built-in facilities for default reasoning. Several ASP
solvers have been developed (Niemelä, Simons, & Syrjänen, 2000; Leone, Pfeifer, Faber,
Eiter, Gottlob, Perri, & Scarcello, 2006; Lin & Zhao, 2004; Lierler & Maratea, 2004). To
date, ASP has been used in space shuttle planning (Nogueira, Balduccini, Gelfond, Watson,
& Barry, 2001), evolutional linguistics (Erdem, Lifschitz, Nakhleh, & Ringe, 2003), and
others. In the following, we briefly review some basic notions in ASP.
Let L be a propositional language, i.e. a set of atoms. In this paper we shall consider
logic programs with rules of the following form:
h1 ; · · · ; hk ← p1 , · · · , pm , not pm+1 , · · · , not pn

(1)

where hi ’s and pi ’s are atoms in L. So a logic program here can have default negation (not ),
constraints (when k = 0), and disjunctions in the head of its rules. In the following, if r is a
rule of the above form, we write Hdr to denote the set {h1 , ..., hk }, Psr the set {p1 , ..., pm },
and Ngr the set {pm+1 , ..., pn }. Thus a rule r can also be written as Hdr ← Psr , not Ngr .
The semantics of these programs are given by answer sets (Gelfond & Lifschitz, 1991), which
432

Discovering Classes of Strongly Equivalent Logic Programs

are defined by a fixed-point operator through what has been known as Gelfond-Lifschitz
transformation. Let X be a subset of L, and P a logic program. The Gelfond-Lifschitz
transformation of P on X, written P X , is the set of rules obtained from P according to the
following two rules:
1. If a rule of the form (1) is in P , and pi ∈ X for some m + 1 ≤ i ≤ n, then delete this
rule.
2. Delete all literals of the form not pi in the bodies of the remaining rules.
For instance, if P is the set of following rules:
a; b ←
c ← not a
then P {a} is {a; b ←}, and P {b} is {(a; b ←), (c ←)}.
Clearly, for any X and P , P X is a set of rules which do not have the “not ” operator.
Now a set X is an answer set of P if X is a minimal set of atoms that satisfies every rule
in P X , where X satisfies a rule of the form
h 1 ; · · · ; h k ← p1 , · · · , p m
if for some 1 ≤ i ≤ k, hi ∈ X whenever {p1 , ..., pm } ⊆ X. For instance, for the above
program, both {a} and {b, c} are answer sets, and they are the only answer sets of the
program.
Two logic programs P1 and P2 are said to be equivalent if they have the same answer sets,
and strongly equivalent (Lifschitz et al., 2001) (under the language L), written P1 'se P2 ,
if for any logic program P in L, P ∪ P1 and P ∪ P2 are equivalent (thus we write P1 6'se P2
when P1 and P2 are not strongly equivalent). For example, {a ← b} and {a ← c} are
equivalent, but not strongly equivalent. It can be shown that {a ← not a} 'se {← not a}.
As in the abstract, we also say that a rule r is strongly equivalent to another rule r0 , written
r 'se r0 , if {r} 'se {r0 }, and two rules r1 and r2 are strongly equivalent to a rule r, written
{r1 , r2 } 'se r, if {r1 , r2 } 'se {r}, and so on.
The notion of strong equivalence is important for ASP for several reasons. First of all, it
helps us understand the answer set semantics. For instance, Turner (2003) showed that the
disjunctive rule (a; b ←) is not strongly equivalent to any set of normal rules. This implies
that there cannot be a modular translation from disjunctive logic programs to normal logic
programs. However,
{(a; b ←), (← a, b)}
is strongly equivalent to
{(a ← not b), (b ← not a), (← a, b)}.
This means that under the constraint (← a, b), the disjunctive rule (a; b ←) can be replaced
by two rules without disjunction. Secondly, as we mentioned in the introduction, if P1 and
P2 are strongly equivalent, then they are interchangeable regardless of where they occur.
Thus if we have a large repertoire of pairs of strongly equivalent logic programs, we could
433

Lin & Chen

use them to transform a given program into one that is most suitable to the need in hand.
In particular, it could help us simplify a program for the purpose of computing its answer
sets. As we shall see, our discovered theorems will contribute significantly to this repertoire.
Lifschitz et al. (2001) showed that checking for strong equivalence between two logic
programs can be done in the logic of here-and-there, a three-valued non-classical logic
somewhere between classical logic and intuitionistic logic. Lin (2002) provided a mapping
from logic programs to propositional theories and showed that two logic programs are
strongly equivalent iff their corresponding theories in propositional logic are equivalent.
This result will be used here both for generating example pairs of strongly equivalent logic
programs, and for verifying a conjecture. We repeat it here.
Let P1 and P2 be two finite logic programs, and L the set of atoms in them.
Theorem 1 (Lin, 2002) P1 'se P2 iff in propositional logic, the following sentence is valid:
^
^
^
p ⊃ p0 ) ⊃ [
(
δ(r) ≡
δ(r)],
(2)
p∈L

r∈P1

r∈P2

where for each p ∈ L, p0 is a new atom, and for each rule r of the form (1), δ(r) is the
conjunction of the following two sentences:
p1 ∧ · · · ∧ pm ∧ ¬p0m+1 ∧ · · · ∧ ¬p0n ⊃ h1 ∨ · · · ∨ hk ,

(3)

p01

(4)

∧ ··· ∧

p0m

∧

¬p0m+1

∧ ··· ∧

¬p0n

⊃

h01

∨ ··· ∨

h0k .

Notice that if m = n = 0, then the left sides of the implications in (3) and (4) are considered
to be true, and if k = 0, then the right sides of the implications in (3) and (4) are considered
to be f alse.
In general checking if two sets of rules are strongly equivalent is coNP-complete (c.f.
Turner, 2001; Pearce, Tompits, & Woltran, 2001; Lin, 2002).

3. The Problem
As we mentioned above, one possible use of the notion of strongly equivalent logic programs
is in program simplification. For instance, given a logic program, for each rule r in it,
we may ask whether it can be deleted without knowing what other rules are in P , i.e.
whether {r} is strongly equivalent to the empty set. Or we may ask whether a rule r in
P can be deleted if one knows that another rule r0 is already in P , i.e. whether {r, r0 }
is strongly equivalent to {r0 }. In general, we may ask the following k-m-n question: Is
{r1 , ..., rk , u1 , ..., um } 'se {r1 , ..., rk , v1 , ..., vn }? Thus our theorem discovery task is to come
up, for a given k-m-n problem, a computationally effective condition that holds if and only
if the answer to the k-m-n question is positive.
Now suppose we have such a condition C, and suppose that when
{r1 , ..., rk , u1 , ..., um } 'se {r1 , ..., rk , v1 , ..., vn },
it is better to replace {u1 , ..., um } by {v1 , ..., vn } in the presence of r1 , ..., rk for the purpose
of, say computing the answer sets of a program. One way to use this result to simplify a
given program P is to first choose k rules in P , and for any other m rules in it, try to find
434

Discovering Classes of Strongly Equivalent Logic Programs

n rules so that the condition C holds, and then replace the m rules in P by the simpler n
rules.
However, even if checking whether C holds would take a negligible constant time, using
the above procedure to simplify a given logic program will be practical only when k, m, n
are all very small or when k is almost the same as the number of the rules in the given
program, and m and n are very small. Thus it seems to us that it is worthwhile to solve
the k-m-n problem only when k, m, n are small. In particular, in this paper, we shall
concentrate on the 0-1-0 problem (whether a rule can always be deleted), the 0-1-1 problem
(whether a rule can always be replaced by another one), the 1-1-0 problem (in the presence
of a rule, whether another rule can be deleted), the 2-1-0 problem (in the presence of two
rules, whether a rule can always be deleted), and the 0-2-1 problem (if a pair of rules can
be replaced by a single rule).
An example of theorems that we want to discover about these problems is as follows:
For any rule r, r 'se ∅ iff (Hdr ∪ Ngr ) ∩ Psr 6= ∅.

(5)

4. Some General Theorems
In this section, we prove some general theorems that will help us verify whether an assertion
like (5) above is true.
Let L be a propositional language, i.e. a set of atoms. From L, construct a first-order
language FL with equality, two unary predicates H1 and H2 , three unary predicates Hdr ,
Psr , and Ngr for each logic program rule r in L (we assume that each rule in L has a unique
name), and three unary predicates Xi , Yi , and Zi for each positive number i.
Notice that we have used Hdr , Psr , and Ngr to denote sets of atoms previously, but
now we overload them as unary predicates. Naturally, the intended interpretations of these
unary predicates are their respective sets.
Definition 1 Given a set L of atoms, an intended model of FL is one whose domain is L,
and for each rule r in L, the unary predicates Psr , Hdr , and Ngr are interpreted by their
corresponding sets of atoms, Psr , Hdr , and Ngr , respectively.
Conditions on rules in L, such as Psr ∩ Ngr 6= ∅, will be expressed by special sentences
called properties in FL .
Definition 2 A sentence of FL is a property about n rules if it is constructed from equality
and predicates Xi , Yi , and Zi , 1 ≤ i ≤ n. A property Φ about n rules is true (holds) on a
sequence P = [r1 , ..., rn ] of n rules if Φ[P ] is true in an intended model of FL , where Φ[P ]
is obtained from Φ by replacing each Xi by Hdri , Yi by Psri , and Zi by Ngri .
Notice that since Φ[P ] does not mention predicates Xi , Yi , Zi , H1 , and H2 , if it is true in
one intended model, then it is true in all intended models.
As we have mentioned above, we are interested in capturing the strong equivalence
between two programs by a computationally effective condition. More specifically, for some
small k, m, and n, we are interested in finding a property Φ about k + m + n rules such
that for any sequence of k + m + n rules, P = [r1 , ..., rk , u1 , ..., um , v1 , ..., vn ],
{r1 , ..., rk , u1 , ..., um } 'se {r1 , ..., rk , v1 , ..., vn } iff Φ is true on P .
435

(6)

Lin & Chen

We shall now prove some general theorems that can help us verify the above assertion
for a class of formulas Φ.
First of all, Theorem 1 can be reformulated in FL as follows by reading H1 (p) as “p
holds”, and H2 (p) as “p0 holds”:
Theorem 2 P1 'se P2 in L iff the following sentence
^
^
∀x(H1 (x) ⊃ H2 (x)) ⊃ [
γ(r) ≡
γ(r)]
r∈P1

(7)

r∈P2

is true in all intended models of FL , where γ(r) is the conjunction of the following two
sentences:
[∀x(Psr (x) ⊃ H1 (x)) ∧ ∀x(Ngr (x) ⊃ ¬H2 (x))] ⊃ ∃x(Hdr (x) ∧ H1 (x)),

(8)

[∀x(Psr (x) ⊃ H2 (x)) ∧ ∀x(Ngr (x) ⊃ ¬H2 (x))] ⊃ ∃x(Hdr (x) ∧ H2 (x)).

(9)

In first order logic, if a prenex formula of the form ∃~x∀~y B is satisfiable, then it is
satisfiable in a structure with n elements, where B is a formula that contains no quantifiers,
constants, or function symbols, and n is the length of ~x if it is non-empty, and 1 when ~x
is empty. We can prove a similar result for our first-order languages and their intended
models here.
Definition 3 A sentence of FL is an extended property about n rules if it is constructed
from equality and predicates Xi , Yi , and Zi , 1 ≤ i ≤ n, and H1 and H2 . An extended
property Φ about n rules is true (holds) on a sequence P = [r1 , ..., rn ] of n rules in a model
M if Φ[P ] is true in M , where Φ[P ] is obtained from Φ by replacing each Xi by Hdri , Yi by
Psri , and Zi by Ngri .
Definition 4 In the following, if P = [r1 , ..., rn ] is a tuple of rules in L, and L0 is a subset
of L, then we define the restriction of P on L0 to be [r10 , ..., rn0 ], where ri0 is
Hdri ∩ L0 ← Psri ∩ L0 , not (Ngri ∩ L0 ).
Lemma 1 Let Φ be an extended property in FL about n rules, and of the form ∃~x∀~y Q,
where ~x is a tuple of w variables, and Q a formula that does not have any quantifiers. If
Φ holds on a sequence P of n rules in an intended model M of FL , then there is a subset
L0 of L such that L0 has at most w atoms (or one atom when w = 0), and Φ holds on the
restriction of P on L0 in an intended model of FL0 .
Proof: Suppose M is an intended model of FL such that M |= Φ[P ]. Thus there is a tuple
p~ of w (or one when w = 0) atoms in L such that M |= ∀~y Q[P ](~x/~
p). Now let L0 be the set
0
of atoms in p~, and M defined as follows:
• Each of the predicates H1 , H2 , Xi , Yi , and Zi , i ≥ 1, is interpreted as the restriction
of its interpretation in M on L0 .
• For each rule r in L0 , the predicates Hdr , Psr , and Ngr are interpreted the same as
they are in M . This is well-defined as r is also a rule in L,
436

Discovering Classes of Strongly Equivalent Logic Programs

Then M 0 is an intended model of FL0 . Let P 0 be the restriction of P on L0 . Then P 0 is a
tuple of rules in L0 . Since Q has no quantifiers (and the language has no function symbols),
for any instantiation ~u of ~y in L0 , M |= Q[P ](~x/~
p)(~y /~u) iff M 0 |= Q[P 0 ](~x/~
p)(~y /~u). Since
0
0
M |= ∀~y Q[P ](~x/~
p), we have M |= ∀~y Q[P ](~x/~
p), Thus M 0 |= ∃~x∀~y Q[P 0 ]. 
Using Theorem 2 and this lemma, we can show the following theorem which will enable
us to automate the verification of the “if” part of (6) when the property Φ is in the prenex
format.
Theorem 3 Without loss of generality, suppose m ≥ n. If Φ is a property about k+m+n
rules of the form ∃~x∀~y Q, where ~x is a tuple of w variables, and Q a formula that does not
have any quantifiers, then the following two assertions are equivalent:
(a) For any sequence of k + m + n rules, P = [r1 , ..., rk , u1 , ..., um , v1 , ..., vn ], if Φ is true
on P , then {r1 , ..., rk , u1 , ..., um } 'se {r1 , ..., rk , v1 , ..., vn }.
(b) (b.1) If n > 0, then for any sequence P = [r1 , ..., rk , u1 , ..., um , v1 , ..., vn ] of rules with
at most w + 2(k + m) atoms, if Φ is true on P , then
{r1 , ..., rk , u1 , ..., um } 'se {r1 , ..., rk , v1 , ..., vn }.
(b.2) If n = 0, then for any sequence P = [r1 , ..., rk , u1 , ..., um ] of rules with at most
K atoms, if Φ is true on P , then
{r1 , ..., rk , u1 , ..., um } 'se {r1 , ..., rk },
where K is w + 2k if w + 2k > 0, and K = 1 otherwise.
Proof: If (a) then (b) is obvious. We assume that (b) is true, and show that (a) holds as
well. Suppose first that n > 0. Suppose P = [r1 , ..., rk , u1 , ..., um , v1 , ..., vn ] is a sequence of
k + m + n rules in a language L such that Φ is true on P , and
{r1 , ..., rk , u1 , ..., um } 6'se {r1 , ..., rk , v1 , ..., vn }.
Thus there is an intended model of FL that satisfies Φ[P ], and an intended model M of FL
that satisfies the following sentence:
^
^
(∀x)H1 (x) ⊃ H2 (x) ∧ ¬[
γ(r) ≡
γ(r)],
r∈P1

r∈P2

where P1 = {r1 , ..., rk , u1 , ..., um }, and P2 = {r1 , ..., rk , v1 , ..., vn }. As we noted after Definition 2, M will also satisfy Φ[P ]. Thus M satisfies the following sentence
^
^
^
^
Φ[P ] ∧ (∀x)H1 (x) ⊃ H2 (x) ∧ {[
γ(r) ∧ ¬
γ(r)] ∨ [
γ(r) ∧ ¬
γ(r)]}, (10)
r∈P1

r∈P3

r∈P2

r∈P4

where P3 = {v1 , ..., vn }, and P4 = {u1 , ..., um }.
Now for any rule r, there is an extended property ϕ(x, y) of one rule that does not
mention any quantifiers such that γ(r) is equivalent to ∃x, y.ϕ[r]. Thus for any tuple Q of t
rules,Vthere is an extended property ϕ of t rules that does not mention any quantifiers such
that r∈Q γ(r) is equivalent to ∃~y .ϕ[Q], where ~y is a tuple of 2t variables.
Thus there is
437

Lin & Chen

• a tuple z~1 of 2(k + m) variables, a tuple z~2 of variables, an extended property ϕ1 of
k + m + n rules that does not have any quantifiers, and whose free variables are in z~1
and z~2 ; and
• a tuple z~3 of 2(k + n) variables, a tuple v~4 of variables, and an extended property ϕ2
of k + m + n rules that does not have any quantifiers, and whose free variables are in
z~3 and z~4
such that v~1 , v~2 , v~3 , v~4 do not have common variables in them, and (10) is equivalent to the
following sentence:
{Φ ∧ ∀x(H1 (x) ⊃ H2 (x)) ∧ (∃z~1 ∀z~2 ϕ1 ∨ ∃z~3 ∀z~4 ϕ2 )}[P ].
Since we have assumed that m ≥ n, thus there is an extended property ϕ3 about k + m + n
rules that does not mention any quantifiers and function symbols, and whose free variables
are among z~1 , z~2 , and z~4 such that the above sentence is equivalent to the following sentence:
(Φ ∧ ∀x(H1 (x) ⊃ H2 (x)) ∧ ∃z~1 (∀z~2 , z~4 )ϕ3 )[P ].
Now given the form of Φ assumed in the theorem, there is a tuple z~5 of w + 2(k + m)
variables, a tuple z~6 of variables, and an extended property Ψ of k + m + n rules that does
not mention any quantifiers, and whose free variables are among z~5 , and z~6 such that the
above sentence is equivalent to (∃z~5 )(∀z~6 )Ψ[P ].
By Lemma 1, there is a subset L0 of L that has at most w + 2(k + m) atoms such that
(∃z~5 )(∀z~6 )Ψ holds on P 0 , where P 0 is the restriction of P on L0 . If
P 0 = [r10 , ..., rk0 , u01 , ..., u0m , v10 , ..., vn0 ],
then this will mean that Φ is true on P 0 , and {r10 , ..., rk0 , u01 , ..., u0m } 6'se {r10 , ..., rk0 , v10 , ..., vn0 }.
This shows that if (b.1), then (a).
The proof that if (b.2) then (a) is exactly the same except now that
^
^
[
γ(r) ≡
γ(r)]
r∈P1

r∈P2

is equivalent to
[

^

r∈P2

^

γ(r) ⊃

γ(r)].

r∈P1


The “only if” part of (6) can often be proved with the help of the following theorem.
Theorem 4 Let L1 and L2 be two languages, and f a function from L1 to L2 . If P1 and P2
are two programs in L1 that are strongly equivalent, then f (P1 ) and f (P2 ) are two programs
in L2 that are also strongly equivalent. Here f (P ) is obtained from P by replacing each
atom p in it by f (p).
Proof: By Theorem 1 and the fact that in propositional logic, if ϕ is a tautology, and f a
function from L1 to L2 , then f (ϕ) is also a tautology, where f (ϕ) is the formula obtained
from ϕ by replacing each atom p in it by f (p). 
For an example of using the theorems in this section for proving assertions of the form
(6), see Section 5.1.
438

Discovering Classes of Strongly Equivalent Logic Programs

5. Computer-Aided Theorem Discovery
Given a k-m-n problem, our strategy for discovering theorems about it is as follows:
1. Choose a small language L;
2. Generate all possible triples
({r1 , ..., rk }, {u1 , ..., um }, {v1 , ..., vn })

(11)

of sets of rules in L such that {r1 , ..., rk , u1 , ..., um } 'se {r1 , ..., rk , v1 , ..., vn } in L;
3. Formulate a conjecture on the k-m-n problem that holds in the language L, i.e. a
condition that is true for a triple of the form (11) iff it is generated in Step 2;
4. Verify the correctness of this conjecture in the general case.
This process may have to be iterated. For instance, a conjecture formulated in Step 3 may
fail to generalize in Step 4, so we either need to formulate a new conjecture or start all over
again in step 1 using a larger language.
Ideally, we would like this process to be automatic. However, it is difficult to automate
Steps 3 and 4 - the number of possible patterns that we need to examine in order to come
up with a good conjecture in Step 3 is huge, and we do not have a general theorem that
enables us to automate the verification part in Step 4. While Theorem 3 enables us to
automate the proof of the sufficient part of the assertion (6) for a class of formulas Φ, we
do not have a similar result for the necessary part - as we shall see below, Theorem 4 helps
a lot here, but it does not provide an automated procedure. Nonetheless, computers play
a crucial role in all steps, and in the following we report some of the theorems discovered
using the above procedure.
5.1 The 0-1-0 Problem
This problem asks if a given rule is strongly equivalent to the empty set, thus can always
be deleted from any program. We have the following experimental result:
Lemma 2 If a rule r mentions at most three distinct atoms, then r 'se ∅ iff
(Hdr ∪ Ngr ) ∩ Psr 6= ∅.
Using Theorem 4, we can show the following result:
Lemma 3 If there is a rule r of the form (1) such that r 'se ∅ and (Hdr ∪ Ngr ) ∩ Psr 6= ∅
is not true, then there is such a rule that mentions at most three atoms.
Proof: Suppose r 'se ∅, Hdr ∩ Psr = ∅, and Psr ∩ Ngr = ∅. Suppose L is the set of atoms
in r, and a, b, c are three new atoms. Let

p ∈ Hdr
 a
b
p ∈ Psr
f (p) =

c
otherwise
439

Lin & Chen

By Theorem 4, we also have f (r) 'se ∅. By the construction of f , we also have
Hdf (r) ∩ Psf (r) = ∅, and Psf (r) ∩ Ngf (r) = ∅, and that f (r) mentions at most three distinct
atoms. 
Theorem 5 (The 0-1-0 problem) Lemma 2 holds in the general case, i.e. without any
restriction on the number of atoms in r.
Proof: We notice that the condition in Lemma 2, (Hdr ∪ Ngr ) ∩ Psr 6= ∅, is equivalent to
the following property
∃x.(X1 (x) ∨ Z1 (x)) ∧ Y1 (x)
being true on [r]. Thus the “if” part follows from Theorem 3 and Lemma 2. The “only if”
part follows from Lemma 2 and Lemma 3. 
The “if” part of the theorem is already well-known, first proved by Osorio et. al. (2001).
The “only if” part has also been proved recently by Inoue and Sakama (2004). While we
did not discover anything new in this case, it is reassuring that the methodology works.
We notice here that there is no need to consider the 0-n-0 problem for n > 1, because
for any n, {r1 , ..., rn } is strongly equivalent to ∅ iff for each 1 ≤ i ≤ n, {ri } is strongly
equivalent to ∅.
5.2 The 1-1-0 and the 0-1-1 Problems
The 1-1-0 problem asks if a rule can always be deleted in the presence of another rule, and
the 0-1-1 problem asks if a rule can always be replaced by another one. We first solve the
1-1-0 problem, and the solution to the 0-1-1 problem will come as a corollary.
We have the following experimental result for the 1-1-0 problem:
Lemma 4 For any two rules r1 and r2 that mentions at most three atoms, {r1 , r2 } and
{r1 } are strongly equivalent iff one of the following two conditions is true:
1. r2 'se ∅.
2. Psr1 ⊆ Psr2 , Ngr1 ⊆ Ngr2 , and Hdr1 ⊆ Hdr2 ∪ Ngr2 .
Lemma 5 If there are two rules r1 and r2 such that {r1 , r2 } 'se {r2 }, but none of the
two conditions in Lemma 4 hold, then there are two such rules that mention at most three
atoms.
Proof: Suppose there are two rules r1 , r2 such that {r1 , r2 } 'se {r2 }, and none of the two
conditions in Lemma 4 hold. Let L be the set of atoms in r1 , r2 .
Without loss of generality, suppose a1 is an atom that makes the condition (2) in
Lemma 4 false. If Psr2 \ {a1 } is not empty, let a2 be an atom in it. Let L0 = {a1 , a2 , a3 },
where a3 is a new atom, and f be a function from L to L0 as following:

a = a1
 a1
a2
a ∈ Psr2 \ {a1 }
f (a) =

a3
otherwise
440

Discovering Classes of Strongly Equivalent Logic Programs

clearly, f (r1 ) and f (r2 ) mention at most three distinct atoms, and by Theorem 4,
{f (r1 ), f (r2 )} 'se f (r1 ).
We show that none of the two conditions in Lemma 4 hold for f (r1 ) and f (r2 ) either.
We show first that f (r2 ) 6'se ∅. By Theorem 5, we need to show that
S = Psf (r2 ) ∩ (Hdf (r2 ) ∪ Ngf (r2 ) )
is empty. If a1 ∈ S, then by the construction of f , a1 ∈ Psr2 ∩ (Hdr2 ∪ Ngr2 ), a contradiction
with the assumption that r2 is not strongly equivalent to ∅. Similarly, if a2 ∈ S, then by
the construction of f , a2 ∈ Psr2 ∩ (Hdr2 ∪ Ngr2 ), a contradiction with the assumption that
r2 is not strongly equivalent to ∅. But then a3 cannot be in S as a3 cannot be in Psf (r2 ) .
Thus S must be empty.
We now show that it is not the case that Psf (r1 ) ⊆ Psf (r2 ) , Ngf (r1 ) ⊆ Ngf (r2 ) , and
Hdf (r1 ) ⊆ Hdf (r2 ) ∪Ngf (r2 ) . By our assumption, a1 is an atom that makes either Psr1 ⊆ Psr2 ,
Ngr1 ⊆ Ngr2 , or Hdr1 ⊆ Hdr2 ∪ Ngr2 false. There are three cases here. Suppose a1 makes
Psr1 ⊆ Psr2 false, i.e. a1 ∈ Psr1 but a1 6∈ Psr2 . Then by our construction of f , we also have
that a1 ∈ Psf (r1 ) but a1 6∈ Psf (r2 ) . The other two cases are similar. 
Theorem 6 (The 1-1-0 problem) Lemma 4 holds in the general case, without any restriction on the number of atoms in r1 and r2 .
Proof: The condition in Lemma 4 is equivalent to the following property
[∃x.(X2 (x) ∨ Z2 (x)) ∧ Y2 (x)] ∨
{[∀x.Y1 (x) ⊃ Y2 (x)] ∧ [∀x.Z1 (x) ⊃ Z2 (x)] ∧ [∀x.X1 (x) ⊃ (X2 (x) ∨ Z2 (x))]}
being true on [r1 , r2 ]. Thus the “if” part follows from Theorem 3 and Lemma 4, by noticing
that the above property can be written as ∃x∀~y .Q as required by Theorem 3. The “only
if” part follows from Lemma 4 and Lemma 5. 
Thus if a rule r2 cannot be deleted on its own but can be deleted in the presence of
another rule r1 , then it must be the case that r2 is redundant given r1 : if the body of r2 is
satisfied, then the body of r1 is satisfied as well; furthermore, r2 can entail no more than
what can be entailed by r1 (Hdr1 ⊆ Hdr2 ∪ Ngr2 ).
Osorio et al. (2001) proved that {r1 , r2 } 'se r1 if either Psr1 ∪Ngr1 = ∅ and Hdr1 ⊆ Ngr2
or Psr1 ⊆ Psr2 , Ngr1 ⊆ Ngr2 , and Hdr1 ⊆ Hdr2 . More recently, Eiter et al. (2004) showed
that {r1 , r2 } 'se r1 if r1 s-implies r2 (Wang & Zhou, 2005), i.e. if there exists a set
A ⊆ Ngr2 such that Hdr1 ⊆ Hdr2 ∪ A, Ngr1 ⊆ Ngr2 \ A, and Psr1 ⊆ Psr2 .
As one can see, these are all special cases of the “if” part of Theorem 6. Our result is
actually more general. For instance, these special cases do not apply to
{(c ← b, not c), (← b, not c)}
and
{c ← b, not c},
but one can easily show that these two sets are strongly equivalent using our theorem.
¿From our solution to the 1-1-0 problem, we can derive a solution to the 0-1-1 problem.
441

Lin & Chen

Theorem 7 (The 0-1-1 problem) For any two rules r1 and r2 , r1 'se r2 iff one of the
following two conditions is true:
1. r1 'se r2 'se ∅.
2. Psr1 = Psr2 , Ngr1 = Ngr2 , and Hdr1 ∪ Ngr1 = Hdr2 ∪ Ngr2 .
Proof: By Theorem 1, it is easy to see that r1 'se r2 iff {r1 , r2 } 'se r1 and {r1 , r2 } 'se r2 .

Thus two rules r1 and r2 can always be interchanged if either both of them can be
deleted (strongly equivalent to the empty set) or they have the same body, and the same
consequences when the body is true. For instance, we have {a ← B, not a} 'se {← B, not a}
no matter what B is, because the two rules have the same body, and when the body is true,
the same consequence - a contradiction. As another example, we have
{a; b ← not a} 'se {b ← not a},
because the two rules have the same body, and, when the body is true, the same consequence,
b.
5.3 The 2-1-0, 0-2-1, and 0-2-2 Problems
The 2-1-0 problem asks if a rule can be deleted in the presence of another two rules, the
0-2-1 problem asks if two rules can be replaced by a single rule, and the 0-2-2 problem asks
if two rules can be replaced by another two rules. Similar to the previous subsection, the
solution to the 0-2-1 and 0-2-2 problems will follow from a solution to the 2-1-0 problem.
The experiment on the 2-1-0 problem was more difficult because as it turned out, we
have to consider a language with six atoms in this case. In principle, given a language L,
every subset of L can be the Hd, Ps, or Ng of a rule. Thus when the size of L is six, there
are in principle (26 )3 − 1 = 262, 143 possible rules, and 262, 1433 triples of them. However,
we can cut down the numbers significantly with the results that we already have proved.
First, we only have to consider rules that do not have common elements in any of the
two sets in {Hd, Ps, Ng}: if either Hd and Ps or Ps and Ng have a common element, then by
Theorem 5, this rule can be deleted; if Hd and Ng have common elements, then according
to Theorem 7, we obtain a strongly equivalent rule by deleting the common elements in Hd.
In the following, we call such rules canonical, that is, a rule r is canonical if
Hdr ∩ Psr = Hdr ∩ Ngr = Psr ∩ Ngr = ∅.
Secondly, we do not have to consider isomorphic rules: if there is a one-to-one onto
function from L to L that maps {r1 , r2 , r3 } to {r10 , r20 , r30 }, then these two sets of rules are
essentially the same except for the names of atoms in them.
Thus by considering only canonical rules and using a certain normal form for triples of
rules that avoids isomorphic rules, we ended up with roughly 120 million triples of rules to
consider for verifying the following result, which took about 10 hours on a Solaris server
consisting of 8 Sun Ultra-SPARC III 900Mhz CPUs with 8GB RAM.
For more details on the experiment on 2-1-0 problem, please refer to (Chen, Lin, & Li,
2005).
442

Discovering Classes of Strongly Equivalent Logic Programs

Lemma 6 For any three canonical rules r1 , r2 and r3 that mention at most six atoms,
{r1 , r2 , r3 } 'se {r1 , r2 } iff one of the following three conditions is true:
1. {r1 , r3 } 'se r1 .
2. {r2 , r3 } 'se r2 .
3. There is an atom p such that:
3.1 p ∈ (Psr1 ∪ Psr2 ) ∩ (Hdr1 ∪ Hdr2 ∪ Ngr1 ∪ Ngr2 )
3.2 Hdri \ {p} ⊆ Hdr3 ∪ Ngr3 and Psri \ {p} ⊆ Psr3 and Ngri \ {p} ⊆ Ngr3 , where
i = 1, 2
3.3 If p ∈ Psr1 ∩ Ngr2 , then Hdr1 ∩ Hdr3 = ∅
3.4 If p ∈ Psr2 ∩ Ngr1 , then Hdr2 ∩ Hdr3 = ∅
The following lemma is the reason why we need to consider a language with six atoms
for this problem.
Lemma 7 If there are three canonical rules r1 ,r2 and r3 such that {r1 , r2 , r3 } 'se {r1 , r2 },
but none of the three conditions in Lemma 6 hold, then there are three such rules that
mention at most six atoms.
Proof: The proof of this lemma is tedious as we have to consider several cases. Consider
the following statements about any three canonical rules r1 , r2 , r3 :
(I) {r1 , r2 , r3 } 'se {r1 , r2 }.
(II) {r1 , r3 } 6'se {r1 }, i.e. Psr1 6⊆ Psr3 or Ngr1 6⊆ Ngr3 or Hdr1 ∪ Ngr1 6⊆ Hdr3 ∪ Ngr3
(III) {r2 , r3 } 6'se {r2 }, i.e. Psr2 6⊆ Psr3 or Ngr2 6⊆ Ngr3 or Hdr2 ∪ Ngr2 6⊆ Hdr3 ∪ Ngr3
(IV) (Psr1 ∪ Psr2 ) ∩ (Hdr1 ∪ Hdr2 ∪ Ngr1 ∪ Ngr2 ) = ∅
(V) There is an atom p in the set (Psr1 ∪ Psr2 ) ∩ (Hdr1 ∪ Hdr2 ∪ Ngr1 ∪ Ngr2 ), and another
different atom q such that one of the following three conditions is true:
1. q ∈ Hdr1 ∪ Ngr1 and q 6∈ Hdr3 ∪ Ngr3 .
2. q ∈ Psr1 and q 6∈ Psr3 .
3. q ∈ Ngr1 and q 6∈ Ngr3 .
Notice that this is the negation of condition (3.2) in Lemma 6.
(VI) Hdr1 ∩ Hdr3 6⊆ Ngr3 , and there is an atom p ∈ Psr1 ∩ Ngr2 such that for i = 1, 2,
Hdri \ {p} ⊆ Hdr3 ∪ Ngr3 , Psri \ {p} ⊆ Psr3 , and Ngri \ {p} ⊆ Ngr3 .
Since r1 and r2 are symmetric in the conditions in Lemma 6, to prove this lemma, we need
only to prove the following three assertions:
(a) If there are three canonical rules r1 , r2 , r3 which satisfy (I)-(IV), then there are three
canonical rules r10 , r20 , r30 which mention at most six atoms, and satisfy (I)-(IV) as well.
443

Lin & Chen

(b) If there are three canonical rules r1 , r2 , r3 which satisfy (I)-(III)(V), then there are
three canonical rules r10 , r20 , r30 which mention at most six atoms, and satisfy (I)-(III)(V)
as well.
(c) If there are three canonical rules r1 , r2 , r3 which satisfy (I)-(III)(VI), then there are
three canonical rules r10 , r20 , r30 which mention at most six atoms, and satisfy (I)(III)(VI) as well.
We now prove the above three assertions one by one.
(a) Let a1 , a2 be two atoms that make (II) and (III) true. If (Psr3 ∩(Psr1 ∪Psr2 ))\{a1 , a2 }
is not empty, let a3 be an atom in it. If Psr3 \(Psr1 ∪Psr2 ∪{a1 , a2 }) is not empty, let a4
be an atom in it. If (Psr1 ∪Psr2 )\(Psr3 ∪{a1 , a2 }) is not empty, let a5 be an atom in it.
Finally let a6 be a new atom different from a1 to a5 , and L0 = {a1 , a2 , a3 , a4 , a5 , a6 }.
Let f be a function from L to L0 defined as following:

a1
a = a1




a
a = a2

2


a3
a ∈ (Psr3 ∩ (Psr1 ∪ Psr2 )) \ {a1 , a2 }
f (a) =
a
a ∈ Psr3 \ (Psr1 ∪ Psr2 ∪ {a1 , a2 })


 4


a
a ∈ (Psr1 ∪ Psr2 ) \ (Psr3 ∪ {a1 , a2 })

 5
a6
otherwise
For each 1 ≤ i ≤ 3, let ri0 be as follows:
Psri0 = Psf (ri ) , Ngri0 = Ngf (ri ) , Hdri0 = Hdf (ri ) \ Ngf (ri ) .

(12)

We have that
– For each 1 ≤ i ≤ 3, ri0 is a canonical rule, and ri0 'se f (ri ). For this, we only
need to show f (ri ) 6'se ∅ for each 1 ≤ i ≤ 3. To see this, notice that from
the definition of f , atoms other than a1 and a2 in Psr3 are mapped to {a3 , a4 },
and atoms other than a1 and a2 in Hdr3 ∪ Ngr3 are mapped to {a5 , a6 }. Thus
Psf (r3 ) ∩ (Hdf (r3 ) ∪ Ngf (r3 ) ) = ∅. By Theorem 5, f (r3 ) 6'se ∅. Now f (r1 ) 6'se ∅
and f (r2 ) 6'se ∅, because (II) and (III) hold for f (r1 ), f (r2 ), f (r3 ) by definition
of f .
– (I) holds for r10 , r20 , r30 . This is because by Theorem 4,
{f (r1 ), f (r2 ), f (r3 )} 6'se {f (r1 ), f (r2 )},
and for each 1 ≤ i ≤ 3, ri0 'se f (ri ).
– (II) and (III) hold for r10 , r20 , r30 . As we mentioned, from the definition of f , (II)
and (III) hold for f (r1 ), f (r2 ), f (r3 ).
– (IV) holds for r10 , r20 , r30 . Again, we need only to show that (IV) holds for
f (r1 ), f (r2 ), f (r3 ). To see this, notice that atoms other than a1 and a2 in
Psr1 ∪ Psr2 are mapped to {a3 , a5 }, and atoms other than a1 and a2 in
Hdr1 ∪ Hdr2 ∪ Ngr1 ∪ Ngr2 are mapped to {a4 , a6 }.
444

Discovering Classes of Strongly Equivalent Logic Programs

(b) Again let a1 , a2 be two atoms that make (II) and (III) true. Let p, q be the two witness
atoms in (V). If P os(r3 ) \ {a1 , a2 , p, q} is not empty, let a3 be an atom in it. Let a4
be a new atom, and L0 = {a1 , a2 , a3 , a4 , p, q}. Define f as follows:

a = a1

 a1


a2
a = a2



p
a=p
f (a) =
q
a=q





a
a ∈ Psr3 \ {a1 , a2 , p, q}
3


a4
otherwise
Define ri0 by (12) as well for each 1 ≤ i ≤ 3.
– For each 1 ≤ i ≤ 3, ri0 is a canonical rule, and ri0 'se f (ri ). This can be seen in
the same way as for (a) above.
– By Theorem 4, {f (r1 ), f (r2 ), f (r3 )} 'se {f (r1 ), f (r2 )}, and thus
{r10 , r20 , r30 } 'se {r10 , r20 }.
So (I) holds for r10 , r20 , r30 .
– From definition of f , (II) and (III) hold for f (r1 ), f (r2 ), f (r3 ), and thus they hold
for r10 , r20 , r30 as well.
– Again from the definition of f , (V) holds for f (r1 ), f (r2 ), f (r3 ): there is an atom
p in the set (Psf (r1 ) ∪ Psf (r2 ) ) ∩ (Hdf (r1 ) ∪ Hdf (r2 ) ∪ Ngf (r1 ) ∪ Ngf (r2 ) ), and another
different atom q such that one of the following three conditions is true:
1. q ∈ Hdf (r1 ) ∪ Ngf (r1 ) and q 6∈ Hdf (r3 ) ∪ Ngf (r3 ) .
2. q ∈ Psf (r1 ) and q 6∈ Psf (r3 ) .
3. q ∈ Ngf (r1 ) and q 6∈ Ngf (r3 ) .
(V) holds for r10 , r20 , r30 as well because for each 1 ≤ i ≤ 3,
Psri0 = Psf (ri ) , Ngri0 = Ngf (ri ) , Hdri0 ∪ Ngri0 = Hdf (ri ) ∪ Ngf (ri ) .
(c) Let a1 , a2 be two atoms that make (II) and (III) true. Let p be the witness atom in
(VI), and let q ∈ Hdr1 ∩ Hdr3 but q 6∈ Ngr3 . If P os(r3 ) \ {a1 , a2 , p, q} is not empty, let
a3 be an atom in it. Let a4 is a new atom, and Let L0 = {a1 , a2 , a3 , a4 , p, q}, Define f
as follows:

a1
a = a1




a
a = a2

2


p
a=p
f (a) =
q
a=q





a
a ∈ Psr3 \ {a1 , a2 , p, q}

 3
a4
otherwise
Again define ri0 by (12) as well for each 1 ≤ i ≤ 3.
– For each 1 ≤ i ≤ 3, ri0 is a canonical rule, and ri0 'se f (ri ). This can be seen in
the same way as for (a) above.
445

Lin & Chen

– Again by Theorem 4, {f (r1 ), f (r2 ), f (r3 )} 'se {f (r1 ), f (r2 )}, and thus
{r10 , r20 , r30 } 'se {r10 , r20 }.
So (I) holds for r10 , r20 , r30 .
– Again from definition of f , (II) and (III) hold for f (r1 ), f (r2 ), f (r3 ), thus they
hold for r10 , r20 , r30 as well.
– By the definition of f , (VI) holds for f (r1 ), f (r2 ), f (r3 ): Hdf (r1 ) ∩ Hdf (r3 ) 6⊆
Ngf (r3 ) , and there is an atom p ∈ Psf (r1 ) ∩ Ngf (r2 ) such that for i = 1, 2, Hdf (ri ) \
{p} ⊆ Hdf (r3 ) ∪ Ngf (r3 ) , Psf (ri ) \ {p} ⊆ Psf (r3 ) , and Ngf (ri ) \ {p} ⊆ Ngf (r3 ) . (VI)
holds for r10 , r20 , r30 as well because
Psri0 = Psf (ri ) , Ngri0 = Ngf (ri ) , Hdri0 ⊆ Hdf (ri ) .

Theorem 8 (The 2-1-0 problem) Lemma 6 holds in the general case, without any restriction on the number of atoms in r1 , r2 , r3 .
Proof: The assertion that r1 , r2 , and r3 are canonical rules and satisfy one of the three
conditions in Lemma 6 is equivalent to the following property
[∀x.((¬(X1 (x) ∧ Y1 (x))) ∧ (¬(X1 (x) ∧ Z1 (x))) ∧ (¬(Y1 (x) ∧ Z1 (x))))] ∧
[∀x.((¬(X2 (x) ∧ Y2 (x))) ∧ (¬(X2 (x) ∧ Z2 (x))) ∧ (¬(Y2 (x) ∧ Z2 (x))))] ∧
[∀x.((¬(X3 (x) ∧ Y3 (x))) ∧ (¬(X3 (x) ∧ Z3 (x))) ∧ (¬(Y3 (x) ∧ Z3 (x))))] ∧
{[(∀x.Y1 (x) ⊃ Y3 (x)) ∧ (∀x.Z1 (x) ⊃ Z3 (x)) ∧ (∀x.X1 (x) ⊃ (X3 (x) ∨ Z3 (x)))] ∨
[(∀x.Y2 (x) ⊃ Y3 (x)) ∧ (∀x.Z2 (x) ⊃ Z3 (x)) ∧ (∀x.X2 (x) ⊃ (X3 (x) ∨ Z3 (x)))] ∨
[∃x.CON 1(x) ∧ CON 2(x) ∧ CON 3(x) ∧ CON 4(x)]}
being true on [r1 , r2 , r3 ], where CON 1(x) stands for
(Y1 (x) ∨ Y2 (x)) ∧ (X1 (x) ∨ X2 (x) ∨ Z1 (x) ∨ Z2 (x))
CON 2(x) for
∀y.(x 6= y) ⊃ [(X1 (y) ⊃ (X3 (y) ∨ Z3 (y))) ∧ (Y1 (y) ⊃ Y3 (y)) ∧ (Z1 (y) ⊃ Z3 (y)) ∧
(X2 (y) ⊃ (X3 (y) ∨ Z3 (y))) ∧ (Y2 (y) ⊃ Y3 (y)) ∧ (Z2 (y) ⊃ Z3 (y))]
CON 3(x) for
Y1 (x) ∧ Z2 (x) ⊃ ∀y.(¬(X1 (y) ∧ X3 (y))),
and CON 4(x) for
Y2 (x) ∧ Z1 (x) ⊃ ∀y.(¬(X2 (y) ∧ X3 (y))).
Thus the “if” part follows from Theorem 3 and Lemma 6, by noticing that the above
property can be written as ∃x∀~y .Q as required by Theorem 3. The “only if” part follows
from Lemma 6 and Lemma 7. 
446

Discovering Classes of Strongly Equivalent Logic Programs

The conditions in Lemma 6 (Theorem 8) are rather complex, and the reason why it
is difficult to automate Step 3 of the procedure at the beginning of the section. These
conditions capture all possible cases when r3 is “subsumed” by r1 and r2 , and are difficult
to describe concisely by words. We give some examples.
Consider the following three rules:
r1 : (a2 ← a1 )
r2 : (a3 ← not a1 )
r3 : (a3 ← not a2 ).
We have that {r1 , r2 , r3 } 'se {r1 , r2 } because the condition (4) in Lemma 6 holds.
However, if we change r3 into r30 : a2 ← not a3 , then P1 = {r1 , r2 , r30 } and P2 = {r1 , r2 }
are not strongly equivalent: one could check that condition (4.3) in Lemma 6 does not hold,
and indeed, while P2 ∪ {a1 ← a2 } has a unique answer set {a3 }, P1 ∪ {a1 ← a2 } has two
answer sets {a3 } and {a1 , a2 }.
It is also easy to show by Theorem 8 that a3 ← not a2 is “subsumed” by
{(a1 ; a2 ; a3 ←), (a2 ; a3 ← a1 )},
and a2 ; a3 ← is “subsumed” by
{(a2 ← a1 ), (a3 ← not a1 )}.
With the results that we have, the following theorem will yield a solution to the 0-2-1
problem.
Theorem 9 (the 0-2-1 problem) For any three rules r1 , r2 and r3 , {r1 , r2 } and {r3 } are
strongly equivalent iff the following three conditions are true:
1. {r1 , r2 , r3 } 'se {r1 , r2 }.
2. {r1 , r3 } 'se {r3 }.
3. {r2 , r3 } 'se {r3 }.
For example, We have
{(a2 ← a1 , not a3 ), (a1 ; a2 ← not a3 )} 'se {a2 ← not a3 }.
While we have
{(← a2 , a3 ), (← a3 , not a2 )} 'se {← a3 },
we have
{(a1 ← a2 , a3 ), (a1 ← a3 , not a2 )} 6'se {a1 ← a3 }.
Similarly, we have the following theorem
Theorem 10 (the 0-2-2 problem) For any four rules r1 , r2 , r3 , r4 , {r1 , r2 } and {r3 , r4 }
are strongly equivalent iff the following four conditions are true:
447

Lin & Chen

1. {r1 , r2 , r3 } 'se {r1 , r2 }.
2. {r1 , r2 , r4 } 'se {r1 , r2 }.
3. {r3 , r4 , r1 } 'se {r3 , r4 }.
4. {r3 , r4 , r2 } 'se {r3 , r4 }.

6. Program Simplification
We have mentioned that one possible use of the notion of strongly equivalent logic programs
is in simplifying logic programs: if P 'se Q, and that Q is “simpler” than P , we can then
replace P in any program that contains it by Q.
Most answer set programming systems perform some program simplifications. However,
only Smodels (Niemelä et al., 2000) has a stand-alone front-end called lparse that can be
used to ground and simplify a given logic program. It seems that lparse simplifies a grounded
logic program by computing first its well-founded model. It does not, however, perform any
program simplification using the notion of strong equivalence. For instance, lparse-1.0.13,
the current version of lparse, did nothing to the following set of rules:
{(a ← not b), (b ← not a), (a ← a)}. Nor does it replace the first rule in the following
program {(a ← not a), (a ← not b), (b ← not a)} by the constraint ← not a.
It is unlikely that anyone would be intentionally writing rules like a ← a or b ← a, not a.
But these type of rules can arise as a result of grounding some rules with variables. For
instance, the following is a typical recursive rule used in logic programming encoding of the
Hamiltonian Circuit problem (Niemelä, 1999; Marek & Truszczynski, 1999):
reached(X) ← arc(Y, X), hc(Y, X), reached(Y ).
When instantiated on a graph with cyclic arcs like arc(a, a), this rule generates cyclic rules
of the form reached(X) ← hc(X, X), reached(X). Unless deleted explicitly, these rules will
slow down many systems, especially those based on SAT. For instance, none of the graphs
tested using ASSAT have self-cycles consisting of an arc from a node to itself (Lin & Zhao,
2004). If these cycles are included, ASSAT would run significantly longer.
It is thus useful to consider using the results that we have here for program simplification.
Indeed, transformation rules such as deleting those that contain common elements in their
heads and positive bodies have been proposed (Brass & Dix, 1999), and studied from the
perspective of strong equivalence (Osorio et al., 2001; Eiter et al., 2004). Our results
add new such transformation rules. For instance, by Theorem 7, we can delete those
elements in the head of a rule that also appear in the negation-as-failure part of the rule.
Theorems 6, 8, and 9 can also be used to define some new transformation rules.

7. Concluding Remarks and Future Work
Donald Knuth, in his Forward to (Petkovsek et al., 1996), said
“Science is what we understand well enough to explain to a computer. Art is
everything else we do. ...Science advances whenever an Art becomes a Science.
448

Discovering Classes of Strongly Equivalent Logic Programs

And the state of the Art advances too, because people always leap into new
territory once they have understood more about the old.”
We hope that with this work, we are one step closer to making discovering classes of strongly
equivalent logic programs a Science.
We have mentioned that the methodology used in this paper is similar to that in (Lin,
2004). In both cases, plausible conjectures are generated by testing them in domains of
small sizes, and general theorems are proved to aid the verification of these conjectures
in the general case. However, while plausible conjectures are generated automatically in
(Lin, 2004), they are done manually here. While the verifications of most conjectures in
(Lin, 2004) are done automatically as well, they are done only semi-automatically here.
Overcoming these two weaknesses is the focus of our future work. Specifically, we would
like to make Step 3 of the procedure in Section 5 automatic, and prove a theorem similar
to Theorem 3 to automate the proofs of the “only if” parts of theorems like Theorems 5 8, in the same way that Theorem 3 makes the proofs of the “if” parts of these theorems
automatic. This way, we would be able to discover more interesting theorems in this area,
and more easily!

Acknowledgments
An extended abstract of this paper appeared in Proceedings of IJCAI’2005. We thank Yan
Zhang for his comments on an earlier version of the paper. We also thank the anonymous
reviewers for their useful comments, especially one of them for pointing out an error in
Lemma 4 in an earlier version of the paper. This work was supported in part by the
Research Grants Council of Hong Kong under Competitive Earmarked Research Grant
HKUST6170/04E. Part of the second author’s work was done when he was a student at Sun
Yat-Sen University, Guangzhou, China, and a visiting scholar in Department of Computer
Science and Engineering, Hong Kong University of Science and Technology, Hong Kong.

References
Brass, S., & Dix, J. (1999). Semantics of (disjunctive) logic programs based on partial
evaluation. Journal of Logic Programming, 40 (1), 1–46.
Chen, Y., Lin, F., & Li, L. (2005). SELP - a system for studying strong equivalence
between logic programs. In Proceedings of the 8th International Conference on Logic
Programming and Nonmonotonic Reasoning(LPNMR 2005), pp. 442–446.
Eiter, T., Fink, M., Tompits, H., & Woltran, S. (2004). Simplifying logic programs under
uniform and strong equivalence. In Proceedings of the 7th International Conference
on Logic Programming and Nonmonotonic Reasoning(LPNMR 2004), pp. 87–99.
Erdem, E., Lifschitz, V., Nakhleh, L., & Ringe, D. (2003). Reconstructing the evolutionary
history of indo-european languages using answer set programming. In Proceedings of
the 5th International Symposium on Practical Aspects of Declarative Languages(PADL
2003), pp. 160–176.
449

Lin & Chen

Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming. In
Proceedings of the 5th International Conference and Symposium on Logic Programming(ICLP/SLP), pp. 1070–1080.
Gelfond, M., & Lifschitz, V. (1991). Classical negation in logic programs and disjunctive
databases. New Generation Computing, 9 (3/4), 365–386.
Inoue, K., & Sakama, C. (2004). Equivalence of logic programs under updates. In Proceedings of the 9th European Conference on Logics in Artificial Intelligence(JELIA), pp.
174–186.
Lenat, D. B. (1979). On automated scientific theory formation: A case study using the AM
program. In Machine Intelligence 9, pp. 251–283. Jean Hayes, Donald Michie, and L.
I. Mikulich, eds. Ellis Horwood.
Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., Perri, S., & Scarcello, F. (2006).
The DLV system for knowledge representation and reasoning. ACM Transactions on
Computational Logic, 7 (3).
Lierler, Y., & Maratea, M. (2004). Cmodels-2: SAT-based answer set solver enhanced
to non-tight programs. In Proceedings of the 7th International Conference on Logic
Programming and Nonmonotonic Reasoning(LPNMR 2004), pp. 346–350.
Lifschitz, V. (1999). Action languages, answer sets and planning. In The Logic Programming
Paradigm: A 25-Year Perspective. K.R. Apt, V.W. Marek, M. Truszczynski, D.S.
Warren, eds, Springer-Verlag.
Lifschitz, V., Pearce, D., & Valverde, A. (2001). Strongly equivalent logic programs. ACM
Transactions on Computational Logic, 2 (4), 526–541.
Lin, F. (2002). Reducing strong equivalence of logic programs to entailment in classical
propositional logic. In Proceedings of the 8th International Conference on Principles
of Knowledge Representation and Reasoning(KR2002), pp. 170–176.
Lin, F. (2004). Discovering state invariants. In Proceedings of the 9th International Conference on Principles of Knowledge Representation and Reasoning(KR2004), pp. 536–
544.
Lin, F., & Zhao, Y. (2004). ASSAT: computing answer sets of a logic program by sat solvers.
Artificial Intelligence, 157 (1-2), 115–137.
Marek, V. W., & Truszczynski, M. (1999). Stable logic programming - an alternative logic
programming paradigm. In The Logic Programming Paradigm: A 25-Year Perspective.
K.R. Apt, V.W. Marek, M. Truszczynski, D.S. Warren, eds, Springer-Verlag.
Niemelä, I., Simons, P., & Syrjänen, T. (2000).
Smodels: a system for answer
set programming. In Proceedings of the 8th International Workshop on NonMonotonic Reasoning. Breckenridge, Colorado, USA. (CoRR: arXiv:cs.AI/0003033)
http://www.tcs.hut.fi/Software/smodels/.
Niemelä, I. (1999). Logic programs with stable model semantics as a constraint programming
paradigm. Annals of Mathematics and Artificial Intelligence, 25 (3-4), 241–273.
450

Discovering Classes of Strongly Equivalent Logic Programs

Nogueira, M., Balduccini, M., Gelfond, M., Watson, R., & Barry, M. (2001). An A-Prolog
decision support system for the space shuttle. In Proceedings of the 3rd International
Symposium on Practical Aspects of Declarative Languages(PADL 2001), pp. 169–183.
Osorio, M., Navarro, J. A., & Arrazola, J. (2001). Equivalence in answer set programming. In
Selected Papers of the 11th International Workshop on Logic Based Program Synthesis
and Transformation(LOPSTR 2001), pp. 57–75.
Pearce, D., Tompits, H., & Woltran, S. (2001). Encodings for equilibrium logic and logic
programs with nested expressions. In Proceedings of 10th Portuguese Conference on
Artificial Intelligence(EPIA 2001), pp. 306–320.
Petkovsek, M., Wilf, H. S., & Zeilberger, D. (1996). A = B. Wellesley, Mass. : A K Peters.
Turner, H. (2001). Strong equivalence for logic programs and default theories (made easy).
In Proceedings of the 6th International Conference on Logic Programming and Nonmonotonic Reasoning(LPNMR 2001), pp. 81–92.
Turner, H. (2003). Strong equivalence made easy: nested expressions and weight constraints.
Theory and Practice of Logic Programming, 3 (4-5), 609–622.
Wang, K., & Zhou, L. (2005). Comparisons and computation of well-founded semantics
for disjunctive logic programs. ACM Transactions on Computational Logic, 6 (2),
295–327.

451

Journal of Artificial Intelligence Research 28 (2007) 267-297

Submitted 05/06; published 03/07

Anytime Heuristic Search
Eric A. Hansen

hansen@cse.msstate.edu

Department of Computer Science and Engineering
Mississippi State University
Mississippi State, MS 39762 USA

Rong Zhou

rzhou@parc.com

Palo Alto Research Center
3333 Coyote Hill Road
Palo Alto, CA 94304 USA

Abstract
We describe how to convert the heuristic search algorithm A* into an anytime algorithm
that finds a sequence of improved solutions and eventually converges to an optimal solution.
The approach we adopt uses weighted heuristic search to find an approximate solution
quickly, and then continues the weighted search to find improved solutions as well as to
improve a bound on the suboptimality of the current solution. When the time available
to solve a search problem is limited or uncertain, this creates an anytime heuristic search
algorithm that allows a flexible tradeoff between search time and solution quality. We
analyze the properties of the resulting Anytime A* algorithm, and consider its performance
in three domains; sliding-tile puzzles, STRIPS planning, and multiple sequence alignment.
To illustrate the generality of this approach, we also describe how to transform the memoryefficient search algorithm Recursive Best-First Search (RBFS) into an anytime algorithm.

1. Introduction
A widely-used framework for problem-solving in artificial intelligence is heuristic search for a
minimum-cost solution path in a graph. For large and complex problems, finding an optimal
solution path can take a long time and a suboptimal solution that can be found quickly
may be more useful. Various techniques for modifying a heuristic search algorithm such
as A* to allow a tradeoff between solution quality and search time have been studied. One
approach is to weight an admissible evaluation function to make it non-admissible (Pohl,
1970a, 1970b; Pearl, 1984). In the substantial literature on weighted heuristic search, the
assumption is that the search stops as soon as the first solution is found. Analysis has
focused on characterizing the tradeoff between the time it takes to find the first solution and
its quality. For example, it has been shown that the cost of the first solution found will not
exceed the optimal cost by a factor greater than 1+², where ² depends on the weight (Pearl,
1984; Davis, Bramanti-Gregor, & Wang, 1988). There have also been empirical studies of
the tradeoff between search time and the quality of the first solution found (Gasching, 1979;
Korf, 1993).
The starting point for this paper is the simple observation that there is no reason to
stop a non-admissible search after the first solution is found. By continuing the search,
a sequence of improved solutions can be found that eventually converges to an optimal
solution. The possibility of continuing a non-admissible A* search after the first solution
c
°2007
AI Access Foundation. All rights reserved.

Hansen & Zhou

is found was suggested by Harris (1974), although he did not consider weighted heuristic
search but a somewhat related approach called bandwidth heuristic search. We are not
aware of any other mention of this idea before we proposed it as a strategy for creating an
Anytime A* algorithm (Hansen, Zilberstein, & Danilchenko, 1997; Zhou & Hansen, 2002).
In this paper, we discuss this strategy at length and evaluate it analytically and empirically.
We refer to this strategy as anytime heuristic search. Anytime algorithms are useful for
problem-solving under varying or uncertain time constraints because they have a solution
ready whenever they are stopped (with the possible exception of an initial time period
before the first solution is found) and the quality of the solution improves with additional
computation time (Zilberstein, 1996). Because heuristic search has many applications, a
general method for transforming a heuristic search algorithm such as A* into an anytime
algorithm could prove useful in many domains where good anytime algorithms are not
otherwise available.
The paper is organized as follows. Section 2 presents our approach for transforming a
weighted heuristic search algorithm into an anytime algorithm, and shows how to transform
Weighted A* into an Anytime A* algorithm. To illustrate the generality of this approach,
Section 3 considers Recursive Best-First Search (RBFS), which is a memory-efficient version
of A*, and shows how to transform Weighted RBFS into an Anytime RBFS algorithm.
Section 4 discusses related work, including a related approach to creating an Anytime A*
algorithm that has been recently proposed.

2. Anytime A*
We begin this section with a brief review of the standard A* and Weighted A* algorithms.
Then we describe how to transform Weighted A* into an Anytime A* algorithm. We analyze
the theoretical properties of the resulting algorithm and evaluate its performance in three
test domains; sliding-tile puzzles, STRIPS planning, and multiple sequence alignment.
2.1 A*
The A* algorithm (Hart, Nilsson, & Raphael, 1968) uses two lists, an Open list and a Closed
list, to manage a systematic search for a minimum-cost path from a start node to a goal
node in a graph. Initially, the Open list contains the start node and the Closed list is empty.
At each cycle of the algorithm, the most promising open node is expanded, moved to the
Closed list, and its successor nodes are inserted into the Open list. Thus, the Closed list
contains those nodes that have been expanded, by generating their successor nodes, and the
Open list contains those nodes that have been generated, but not yet expanded. The search
terminates when a goal node is selected for expansion. A solution path can be extracted by
tracing node pointers backwards from the goal node to the start node.
The order in which nodes are expanded is determined by the node evaluation function
f (n) = g(n) + h(n), where g(n) is the cost of the best path currently known from the start
node to node n, and h(n) is a heuristic estimate of h∗ (n), the cost of the best path from n
to a goal node. The behavior of A* depends in large part on the heuristic h(n) that guides
the search. If h(n) is admissible, that is, if it never overestimates h∗ (n), and if nodes are
expanded in order of f (n), then the first goal node selected for expansion is guaranteed to
be optimal. A heuristic is said to be consistent if h(n) ≤ c(n, n0 ) + h(n0 ) for all n and n0 ,
268

Anytime Heuristic Search

where c(n, n0 ) is the cost of an edge from node n to node n0 . If h(n) is consistent and nodes
are expanded in order of f (n), the g-cost of a node is guaranteed to be optimal when the
node is selected for expansion, and a node is never expanded more than once. Note that
consistency implies admissibility, and non-admissibility implies inconsistency.
If h(n) is not consistent, it is possible for A* to find a better path to a node after the
node is expanded. In this case, the improved g-cost of a node needs to be propagated
to its descendants. The way that A* usually does this is by reopening nodes, that is, by
moving a node from the Closed list to the Open list when its g-cost is improved. When the
node is eventually reexpanded, the improved g-cost is propagated to its successor nodes,
which may need to be reopened also. As a result, the same node can be expanded multiple
times. Although rarely used in practice, various techniques have been introduced to bound
the worst-case number of node reexpansions (Bagchi & Mahanti, 1983; Bagchi & Srimani,
1985).
2.2 Weighted A*
For difficult search problems, A* may take too long to find an optimal solution, and an
approximate solution that is found relatively quickly can be more useful. Beginning with
Pohl (1970a, 1970b), many researchers have explored the effect of weighting the terms g(n)
and h(n) in the node evaluation function differently, in order to allow A* to find a boundedoptimal solution with less computational effort. In this approach, called Weighted A*
(WA*), the node evaluation function is defined as f 0 (n) = g(n) + w × h(n), where the
weight w ≥ 1.0 is a parameter set by the user. Sometimes the node evaluation function is
defined as f 0 (n) = (1−w0 )×g(n)+w0 ×h(n), but this is equivalent to f 0 (n) = g(n)+w×h(n)
w
. We use the notation f 0 (n) to distinguish a weighted evaluation function
when w0 = 1+w
from the unweighted f (n). If w > 1.0, the search is not admissible and the (first) solution
found may not be optimal, although it is usually found much faster. If h(n) is admissible, the
suboptimality of the solution found by weighted heuristic search is bounded: the solution
cost cannot be greater than the optimal cost by more than a factor of w (Davis et al., 1988).
Such a solution is said to be ²-admissible where ² = w −1.0. A weighted heuristic accelerates
search for a solution because it makes nodes closer to a goal seem more attractive, giving
the search a more depth-first aspect and implicitly adjusting a tradeoff between search effort
and solution quality. Weighted heuristic search is most effective for search problems with
close-to-optimal solutions, and can often find a close-to-optimal solution in a small fraction
of the time it takes to find an optimal solution.
Some variations of weighted heuristic search have been studied. An approach called
dynamic weighting adjusts the weight with the depth of the search (Pohl, 1973; Koll &
Kaindl, 1992). Another approach uses a weighted heuristic to identify a subset of open nodes
that can be expanded without loss of ²-admissibility; from this subset, it selects the node
to expand next based on other criteria (Pearl & Kim, 1982; Davis et al., 1988). Weighted
heuristic search has been used with other search algorithms besides A*, including memoryefficient versions of A* such as IDA* and RBFS (Korf, 1993), as well as Learning RealTime A* (LRTA*) (Shimbo & Ishida, 2003), and heuristic search algorithms for AND/OR
graphs (Chakrabarti, Ghosh, & DeSarkar, 1988; Hansen & Zilberstein, 2001).
269

Hansen & Zhou

2.3 Anytime Weighted A*
We now consider how Weighted A* can be transformed into an anytime algorithm that
finds a sequence of improved solutions and eventually converges to an optimal solution.
The transformation is an example of a more general approach for transforming a search
algorithm that explores nodes in best-first order, such as A*, into an anytime algorithm.
This approach consists of the following three changes.
1. A non-admissible evaluation function, f 0 (n) = g(n) + h0 (n), where the heuristic h0 (n)
is not admissible, is used to select nodes for expansion in an order that allows good,
but possibly suboptimal, solutions to be found quickly.
2. The search continues after a solution is found, in order to find improved solutions.
3. An admissible evaluation function (i.e., a lower-bound function), f (n) = g(n) + h(n),
where h(n) is admissible, is used together with an upper bound on the optimal solution
cost (given by the cost of the best solution found so far), in order to prune the search
space and detect convergence to an optimal solution.
In this paper, we use a weighted heuristic to create the non-admissible evaluation function that guides the search. That is, we assume that we have an admissible heuristic h(n),
and use it to create a weighted heuristic h0 (n) = w × h(n). But this three-step approach
for creating an anytime heuristic search algorithm can use any non-admissible heuristic
that helps A* find an approximate solution quickly; it is not limited to weighted heuristic
search. When the general approach is used to transform A* into an anytime algorithm, we
call the resulting algorithm Anytime A*. In the special case in which the non-admissible
evaluation function is a weighted heuristic, we call the algorithm Anytime Weighted A* or
Anytime WA*.
Algorithm 1 shows high-level pseudocode for Anytime WA*. (Some details that are
unaffected by the transformation of WA* into an anytime algorithm, such as extracting the
solution path, are omitted.) Note that our implementation of Anytime A* tests whether
a node is a goal node as soon as the node is generated and not when it is selected for
expansion, as in A*, since this can improve the currently available solution more quickly.
Besides continuing the search after the first solution is found, Anytime WA* uses bounds
to prune the search space. The sequence of improved solutions found by Anytime WA*
provides a sequence of improved upper bounds on the optimal solution cost. Anytime WA*
tests whether the f -cost of each newly-generated node is less than the current upper bound.
If not, the node is not inserted in the Open list since it cannot lead to an improved solution.
By not inserting suboptimal nodes into the Open list, the memory requirements of the
algorithm are reduced.1 Each time an improved solution is found and the upper bound
decreases, it is possible that some nodes already in the Open list may have an f -cost equal
to or greater than the new upper bound. Although these nodes could be immediately
1. The possibility of using bounds on the optimal solution cost to reduce the number of nodes stored in
the Open list has been suggested at least twice before in the literature. Harris (1974, p. 219) points out
that this can be done when a bandwidth heuristic is used to guide the search, which is a heuristic with
error bounded by an additive constant. Such heuristics may not be easy to obtain, however. Ikeda and
Imai (1994) describe an Enhanced A* algorithm that uses a previously-computed upper bound to limit
the number of nodes stored in the Open list. We compare Enhanced A* to Anytime WA* in Section 2.4.3.

270

Anytime Heuristic Search

Algorithm 1: Anytime-WA*
Input: A start node start
Output: Best solution found and error bound
begin
g(start) ← 0, f (start) ← h(start), f 0 (start) ← w × h(start)
OP EN ← {start}, CLOSED ← ∅, incumbent ← nil
while OP EN 6= ∅ and not interrupted do
n ← arg minx∈OP EN f 0 (x)
OP EN ← OP EN \ {n}
if incumbent = nil or f (n) < f (incumbent) then
CLOSED ← CLOSED ∪ {n}
foreach ni ∈ Successors(n) such that g(n) + c(n, ni ) + h(ni ) < f (incumbent) do
if ni is a goal node then
f (ni ) ← g(ni ) ← g(n) + c(n, ni ), incumbent ← ni
else if ni ∈ OP EN ∪ CLOSED and g(ni ) > g(n) + c(n, ni ) then
g(ni ) ← g(n) + c(n, ni ), f (ni ) ← g(ni ) + h(ni ), f 0 (ni ) ← g(ni ) + w × h(ni )
if ni ∈ CLOSED then
OP EN ← OP EN ∪ {ni }
CLOSED ← CLOSED \ {ni }
else
g(ni ) ← g(n) + c(n, ni ), f (ni ) ← g(ni ) + h(ni ), f 0 (ni ) ← g(ni ) + w × h(ni )
OP EN ← OP EN ∪ {ni }
if OP EN = ∅ then error ← 0
else error ← f (incumbent) − minx∈OP EN f (x)
output incumbent solution and error bound
end

removed from the Open list, this incurs overhead for searching the Open list every time
the upper bound decreases, and so this step is not included in the pseudocode. (Of course,
if Anytime WA* is close to running out of memory, the overhead for searching through
the Open list for sub-optimal nodes that can be deleted might be justified by the need to
recover memory.) The algorithm shown in the pseudocode simply tests the f -cost of each
node before expanding it. If the f -cost is equal to or greater than the current upper bound,
the node is not expanded. This implies a related test for convergence to an optimal solution:
if the Open list is empty, the currently available solution must be optimal.
Anytime WA* requires more node expansions than A* to converge to an optimal solution, for two reasons. First, use of a weighted heuristic allows it to expand more distinct
nodes than A*. Second, a weighted heuristic is inconsistent and this means it is possible
for a node to have a higher-than-optimal g-cost when it is expanded. If a better path to
the same node is later found, the node is reinserted in the Open list so that the improved
g-cost can be propagated to its descendants when the node is reexpanded. This means that
Anytime WA* can expand the same node multiple times.
Before considering the empirical performance of Anytime WA*, we discuss two important
theoretical properties of the algorithm: convergence to an optimal solution, and a bound
on the suboptimality of the currently available solution.
271

Hansen & Zhou

2.3.1 Convergence
An admissible evaluation function, f (n), gives a lower bound on the cost of any solution
path that is an extension of the current best path to node n. Let incumbent denote the
goal node corresponding to the best solution found so far. Then f (incumbent) is an upper
bound on the cost of an optimal solution. Clearly, there is no reason to expand a node
that has an f -cost (i.e., a lower bound) greater than or equal to the current upper bound,
f (incumbent), since it cannot lead to an improved solution. Thus we have the following
convergence test for Anytime WA*, and, more generally, for any anytime version of A*: the
best solution found so far is optimal if there are no unexpanded nodes on the search frontier
(i.e., in the Open list) with an f -cost less than f (incumbent).
We prove the following theorem under the standard assumptions that a search graph
has a finite branching factor and a minimal edge cost c > 0. We also assume that a solution
exists and that h(n) ≥ 0 for all nodes n.
Theorem 1 Anytime WA* always terminates and the last solution it finds is optimal.
Proof: First we show that the algorithm cannot terminate before an optimal solution is
found. Suppose that the algorithm terminates before finding an optimal solution which
has cost f ∗ . The sequence of upper bounds used during execution of the algorithm is
b0 , b1 , ...., bk , where b0 = ∞ (the upper bound before any solution is found), b1 is the cost
of the first solution found, and bk is the cost of the last solution found. We know that,
b0 > b1 > ... > bk > f ∗ ,
where the last inequality holds under the assumption that the algorithm terminates before
finding an optimal solution, that is, with a suboptimal solution.
Now consider an optimal path leading from the initial state to a goal state. Under the
assumption that this optimal solution path was not found, there must be some node n along
this path that was generated but not expanded. That is only possible if g(n) + h(n) ≥ bk .
But by the admissibility of h, we know that
g(n) + h(n) ≤ f ∗ ,
and therefore
∀i : g(n) + h(n) ≤ f ∗ < bi .
From this contradiction, it follows that the algorithm cannot terminate before an optimal
solution is found.
Next we show that the algorithm always terminates. We have already proved that
before an optimal solution is found, the Open list must include some node n for which
g(n) + h(n) ≤ f ∗ . Hence,
f 0 (n) = g(n) + w × h(n) ≤ w × (g(n) + h(n)) ≤ w × f ∗ .
This establishes an upper bound on the f -cost of any node that can be expanded by Anytime
WA* before an optimal solution is found. Because there are a finite number of nodes for
which f (n) ≤ w × f ∗ , the algorithm must run a bounded number of steps before an optimal
solution is found. Once an optimal solution is found, the algorithm will not expand any
node with an f -cost that is greater than or equal to f ∗ . Because the number of nodes for
which f (n) ≤ f ∗ is also finite, the algorithm must eventually terminate. ¤
272

Anytime Heuristic Search

2.3.2 Error bound
An important property of this approach to creating an Anytime A* algorithm is that it
refines both an upper and lower bound on the optimal cost of a solution. The upper bound
is the f -cost of the best solution found so far, and is decreased when an improved solution
is found. The lower bound is the least f -cost of any currently open node, and is increased
when all nodes with the smallest f -cost are expanded.
Although it is obvious that the cost of the best solution found so far is an upper bound,
the claim that the least f -cost of any currently open node is a lower bound on the optimal
solution cost requires some justification. First note that if the currently available solution
is not optimal, an optimal solution path must pass through some currently open node.
Although the f -cost of an open node is not necessarily a lower bound on the best solution
path that passes through that node, since the g-cost of an open node may be suboptimal,
it is a lower bound on the cost of any solution path that is an extension of the current path
to that node. Since any improved path to an open node (resulting in an improved g-cost)
must be an extension of some already-found path to another currently open node with lower
f -cost, the least f -cost of any currently open node must be a lower bound on the cost of
an optimal solution path. In other words, the least f -cost of any currently open node is a
lower bound on the optimal solution cost for the same reason that it is a lower bound on
the optimal solution cost in branch-and-bound tree search.
These upper and lower bounds approach each other as the search progresses until they
meet upon convergence to an optimal solution. (Figure 3 shows an example of how the
bounds approach each other.) Before an optimal solution is found, a bound on the difference
between the cost of the currently available solution and the optimal solution cost is given
by the difference between the upper and lower bounds. This error bound can be expressed
≤ f (incumbent)
as an approximation ratio, such that f (incumbent)
, where f L denotes the lower
f∗
fL
bound on the optimal solution cost, f (incumbent) denotes the upper bound, and f ∗ denotes
the optimal solution cost. Thus, Anytime A* can be viewed as an anytime algorithm in two
respects. It improves a solution over time, and it also improves a bound on the suboptimality
of the currently available solution.
2.4 Performance and evaluation
We next consider the empirical performance of Anytime WA* in solving a range of search
problems. Its effectiveness depends on the weight used, and how the weight affects search
performance depends on characteristics of both the problem and the heuristic. We set the
weight based on a combination of knowledge of the search problem and trial and error. We
use the same weight from start to end of the search, which has the advantage of simplicity.
It also shows that the technique in its simplest form leads to good performance. It is
possible to change the weight during the search, and weight adjustment together with other
methods of search control has the potential to improve performance further. We postpone
discussion of this to Section 4.1 where we discuss a variant of Anytime A* that has recently
been proposed.
For search problems with unit edge costs, many nodes have the same f -cost and the tiebreaking rule used by a systematic search algorithm such as A* has a significant effect on the
number of nodes actually expanded. It is well-known that A* achieves best performance
273

Hansen & Zhou

Figure 1: (a) Performance profiles for Anytime WA* using three different weights, averaged
over all instances of the Eight Puzzle. (b) Average number of nodes stored and
expanded by Anytime WA* over all instances of the Eight Puzzle.

when it breaks ties in favor of nodes with the least h-cost. In all of the experimental
comparisons reported in this paper, A* uses this tie-breaking rule. We note that Anytime
WA* can achieve similar tie-breaking behavior without applying the same rule because
using even a very small weight has the effect of breaking ties in favor of nodes with the
least h-cost. Moreover, Anytime WA* usually finds an optimal solution before it can prove
that it is optimal (that is, before it expands all nodes with an f -cost less than the optimal
f -cost). As a result, it usually does not expand non-goal nodes with an f -cost equal to the
optimal solution cost. For consistency in experimental comparison, our implementation of
Anytime WA* uses the same rule that A* uses of breaking ties in favor of nodes with the
least h-cost. In practice, this tie-breaking rule can be omitted when implementing Anytime
WA* in order to reduce run-time overhead.
2.4.1 Sliding-tile puzzle
The first test domain we consider is a traditional benchmark that lets us illustrate this
technique on a simple and well-understood example. Figure 1(a) shows performance profiles
for Anytime WA* using three different weights, averaged over all instances of the Eight
Puzzle.2 (Performance profiles are commonly used to model the performance of anytime
algorithms, and show how expected solution quality improves as a function of computation
∗
time. For these problems, we define the quality of a solution as 1 − f −f
f ∗ .) A weight of 1.3
seems to result in the best overall performance among these three weights, although it does
not dominate the other performance profiles for all running times.
2. By all instances, we mean all possible starting states and a fixed goal state. The goal state has the blank
in the upper left corner and the tiles arranged in numerical order, left-to-right and then top-down. We
use the Manhattan distance heuristic.

274

Anytime Heuristic Search

A*
Instance
Blocks-8
Logistics-6
Satellite-6
Freecell-3
Psr-46
Depots-7
Driverlog-11
Elevator-12

Len
14
25
20
18
34
21
19
40

Stored
426,130
364,846
3,270,195
5,992,688
7,464,850
21,027,257
22,344,515
12,748,119

Exp
40,638
254,125
2,423,373
2,693,235
7,141,461
7,761,661
6,693,096
12,734,334

Secs
5.2
4.0
151.5
170.0
343.2
367.8
407.0
560.6

Stored
41,166
254,412
2,423,547
2,695,321
7,148,048
7,773,830
6,702,570
12,734,636

AWA* (weight
Exp
41,099
254,748
2,423,566
2,705,421
7,175,275
7,772,091
6,699,143
12,829,775

= 2)
Opt %
0.2%
6.2%
14.3%
1.7%
69.0%
0.5%
1.4%
98.6%

Secs
3.8
3.7
138.8
146.2
348.0
249.1
281.6
569.7

Table 1: Comparison of A* and AWA* on eight benchmark problems from the biennial
Planning Competitions.

Figure 1(b) shows how many nodes Anytime WA* stores and expands before it converges to an optimal solution, using different weights. (Again, by “converges to an optimal
solution”, we mean that the lower and upper bounds meet and the algorithm has proved
that the solution is optimal.) Using a weight of 1.3, the average increase in the number of
nodes expanded by Anytime WA* is very slight compared to the number of nodes expanded
by unweighted A*. Figure 1(b) also shows that Anytime WA* using a weight of 1.3 or 1.5
stores fewer nodes than unweighted A*. For these weights, the reduction in memory requirements due to using an upper bound to prune the Open list is greater than the increase
in memory requirements due to expanding more distinct nodes.
2.4.2 STRIPS planning
In recent years, there has been considerable interest in using heuristic search for domainindependent STRIPS planning. Influential examples of this approach are the HSP and
HSPr planners of Bonet and Geffner (2001), which have performed well in the biennial
planning competitions sponsored by the International Conference on Automated Planning
and Scheduling (Long & Fox, 2003). HSP solves STRIPS planning problems using A* to
search forward from the start state to the goal, and HSPr uses A* to search backwards
from the goal to the start state, which has the advantage that it allows the heuristic to be
computed more efficiently. Because many of the benchmark planning problems used in the
planning competition are difficult to solve optimally, WA* is often used to find suboptimal
solutions in a reasonable amount of time.
Using Bonet’s publicly-available implementation of HSPr, we compared the performance
of A* and Anytime WA* on benchmark problems from previous planning competitions that
could be solved optimally by A*, using the domain-independent and admissible max-pair
heuristic described by Haslum and Geffner (2000). We used a weight of 2.0 in our experiments. For all instances, Anytime WA* converged to an optimal solution using less
memory than A*. For most (but not all) instances, it also took less time. Table 1 compares the performance of A* and Anytime WA* (AWA*) on the hardest solvable instances
275

Hansen & Zhou

Instance
Blocks-8
Logistics-6
Satellite-6
Freecell-3
Psr-46
Depots-7
Driverlog-11
Elevator-12

AWA* (weight = 5)
Exp Opt %
Secs
42,293
0.2%
3.9
274,047
4.6%
3.8
2,458,452
8.9%
138.7
> 35,639,419
N/A > 2,207.6
7,310,349 10.0%
350.6
7,902,183
0.4%
250.7
6,814,696
1.2%
281.1
12,851,075 76.0%
557.5

AWA* (weight = 10)
Exp Opt %
Secs
42,293
0.2%
3.9
312,726 11.0%
4.3
2,585,074 13.2%
144.8
> 73,712,127
N/A > 4,550.5
7,623,007
4.8%
365.1
8,115,603
1.9%
254.7
7,674,956 18.0%
322.4
13,145,547 21.4%
568.0

Table 2: Performance of AWA* with weights 5 and 10 on eight benchmark problems from
the biennial Planning Competitions.

of eight of these planning problems.3 The CPU time is relatively long for the number of
nodes generated and stored due to significant overhead for generating a node and computing its heuristic in a domain-independent way. The Blocks and Driverlog domains have the
largest branching factors, and thus the space savings from using an upper bound to prevent
insertion of suboptimal nodes in the Open list are greatest in these domains. In no domain
did Anytime WA* expand as many as 1% more nodes than A*, and usually the increased
percentage of node expansions is a fraction of this.
The column labeled “Opt %” shows how soon Anytime WA* finds what turns out to be
an optimal solution. The percentage is the number of node expansions before finding the
optimal solution out of the total number of node expansions until convergence. This provides
a very rough measure of the anytime performance of the algorithm. It shows that in most
domains, Anytime WA* finds what turns out to be an optimal solution very quickly and
spends most of its search time proving that the solution is optimal. However in two domains
(Psr-46 and Elevator-12), Anytime WA* did not find any solution until relatively late. In
both of these domains, solutions were found sooner when the weight was increased. Table 2
shows the performance of Anytime WA* using weights of 5 and 10. Using these higher
weights, anytime performance is better for the last two problems, especially Elevator-12,
although worse for some of the others. Even with weights of 5 and 10, Anytime WA*
tends to outperform A* in solving the first five problems. The sixth problem, Freecell-3, is
different. With a weight of 5, Anytime WA* cannot find any solution before running out of
memory. With a weight of 10, the number of stored nodes is the same (since it exhausts the
same amount of memory) but the number of expanded nodes (and the CPU time) more than
doubles because there are more node reexpansions as the weight increases. These results
clearly show that the effect of the weight on search performance can vary with the domain.
Given this variability, some trial and error in selecting the weight appears inevitable. But
if an appropriate weight is used, Anytime Weighted A* is consistently beneficial.

3. All our experiments were performed on an Intel Xeon 3.0GHz processor with 2MB of L2 cache and 2GB
of RAM

276

Anytime Heuristic Search

2.4.3 Multiple sequence alignment
Alignment of multiple DNA or protein sequences plays an important role in computational
molecular biology. It is well-known that this problem can be formalized as a shortest-path
problem in an n-dimensional lattice, where n is the number of sequences to be aligned (Carillo & Lipman, 1988; Yoshizumi, Miura, & Ishida, 2000). A* can outperform dynamic programming in solving this problem by using an admissible heuristic to limit the number of
nodes in the lattice that need to be examined to find an optimal alignment (Ikeda & Imai,
1999). However, a challenging feature of this search problem is its large branching factor,
which is equal to 2n − 1. When A* is applied to this problem, the large branching factor
means the Open list can be much larger than the Closed list, and the memory required to
store the Open list becomes a bottleneck of the algorithm.
Two solutions to this problem have been proposed in the literature. Yoshizumi et
al. (2000) describe an extension of A* called A* with Partial Expansion (PEA*). Instead
of generating all successors of a node when it is expanded, PEA* inserts only the most
promising successors into the Open list. The “partially expanded” node is then reinserted
into the Open list with a revised f -cost equal to the least f -cost of its successors that have
not yet been generated, so that the node can be reexpanded later. Use of this technique
significantly reduces the size of the Open list, and PEA* can solve larger multiple sequence
alignment problems than A*. Unfortunately, the reduced space complexity of PEA* is
achieved at the cost of node reexpansion overhead. The tradeoff between space and time
complexity is adjusted by setting a “cutoff value” C, which implicitly determines how many
successor nodes to add to the Open list at a time.
Another way to reduce the size of the Open list is not to insert nodes into the Open list
if their f -cost is equal to or greater than a previously established upper bound on the cost
of an optimal solution, since such nodes will never be expanded by A*. This approach was
proposed by Ikeda and Imai (1999), who call it Enhanced A* (EA*). They suggest that one
way to obtain an upper bound is to use the solution found by Weighted A* search with a
weight w > 1, although they did not report experimental results using this technique.
Our anytime algorithm provides a third approach to reducing the size of the Open list.
We also use Weighted A* to quickly find a solution that provides an upper bound for pruning
the Open list. But because the first solution found may not be optimal, the weighted search
is continued in order to find a sequence of improved solutions that eventually converges to
an optimal solution. This provides a sequence of improved upper bounds that can further
reduce the size of the Open list.
Figure 2 compares the performance of Anytime WA* (AWA*) to the performance of A*
with Partial Expansion and Enhanced A* in aligning five sequences from a set of dissimilar
(and thus difficult to align) sequences used in earlier experiments (Kobayashi & Imai, 1998).
The cost function is Dayhoff’s PAM-250 matrix with a linear gap cost of 8. The admissible
heuristic is the standard pairwise alignment heuristic, and the (almost negligible) time
needed to compute the heuristic is included in the running time of the search.
All three algorithms require much less memory than standard A* in solving this problem.
We found that a good weight for Anytime WA* in solving this test problem is 100
99 , that is,
the g-cost is weighted by 99 and the h-cost is weighted by 100. (Because the cost function
for multiple sequence alignment is integer-valued, we use a weighting scheme that preserves
277

Hansen & Zhou

Figure 2: Average performance of search algorithms in aligning sets of 5 sequences from
Kobayashi and Imai (1998).

integer f -costs to allow more efficient integer-valued arithmetic.) To create an upper bound
for Enhanced A*, we ran Weighted A* with the same weight of 100
99 and used the cost of
the first solution found as the upper bound.
Figure 2 shows that Anytime WA* runs more than seven times faster and stores about
the same number of nodes as PEA* with a cutoff of C = 0. When PEA* uses a cutoff
of C = 50, it stores 44% more nodes than Anytime WA* and still runs 65% slower on
average. Although Enhanced A* runs about as fast as Anytime WA*, it stores 36% more
nodes. Anytime WA* stores fewer nodes because continuation of weighted search results in
discovery of improved solutions that provide tighter upper bounds for pruning the Open list.
In summary, Anytime WA* not only outperforms standard A* in solving this problem, it
performs better than two state-of-the-art enhancements of A* that were specifically designed
for this problem.
Figure 3 illustrates the behavior of Anytime WA* by showing how the upper and lower
bounds gradually converge. Notice that Anytime WA* finds an optimal solution after only
10% of the total search time, and spends the remaining 90% of the time proving that
the solution is optimal, at which point it converges. Compared to Partial Expansion A*
and Enhanced A*, an important advantage of Anytime WA* is that it finds a suboptimal
alignment quickly and then continues to improve the alignment with additional computation
time. Thus, it offers a tradeoff between solution quality and computation time that can
prove useful when finding an optimal alignment is infeasible.
The weight that we found worked well for this problem may seem surprisingly small,
and one might suspect that a weight this small has little or no effect on the order in which
nodes with different f -costs are expanded, and serves primarily as a tie-breaking rule for
nodes with the same f -cost but different h-costs. Because our implementations of A* and
Anytime WA* both break ties in favor of nodes with the least h-cost, however, the weight
has no effect on tie breaking in our experiments.
There are a couple of reasons why such a small weight is effective for this search problem.
First, the search graph for multiple sequence alignment has non-uniform edge costs. As a
278

Anytime Heuristic Search

Figure 3: Convergence of bounds for Anytime WA* in aligning sets of 5 sequences from
Kobayashi and Imai (1998).

result, the range of f -costs is much greater than for our other test problems, which have
unit edge costs. Second, the f -costs and h-costs are much larger for this problem than
for our other test problems – in part, because the edge costs are larger (given the cost
function we used), and, in part, because the search space is deeper. (The protein sequences
being aligned have an average length of about 150, and this means the search is at least
this deep.) For this search problem, the optimal f -costs are around 50, 000. Because the
pairwise alignment heuristic used in solving this problem is very accurate, the largest h-costs
are also around 50, 000. Given h-costs this large and a wide range of f -costs, a weight of 100
99
can have a significant effect on the order of node expansions. This serves to illustrate how
the appropriate weight for Anytime WA* depends on characteristics of the search problem.
2.4.4 Discussion
Our results show that Anytime WA* is effective for a wide range of search problems. In
general, it is effective for a search problem whenever Weighted A* is effective. As others have
observed, Weighted A* can usually find a solution much faster than A* because A* spends
most of its time discriminating between close-to optimal solutions in order to determine
which is optimal (Pearl, 1984, p. 86). Indeed, our test results show that Anytime WA*
often finds what turns out to be an optimal solution relatively quickly, and spends most of
its search time proving that the solution is optimal.
One of the surprising results of our experiments is that Anytime WA* using an appropriate weight can sometimes converge to an optimal solution using less memory and even
less time than A*. This is surprising because it is well-known that A* using a consistent
heuristic is “optimally efficient” in terms of the number of nodes expanded (Dechter &
Pearl, 1985). However it is not necessarily optimally efficient by other measures of search
complexity, including memory requirements and running time. Anytime WA* is sometimes
more efficient by these other measures of search performance, even though it requires more
node expansions to find a provably optimal solution. The reason for this is that the improved solutions found by the anytime approach provide upper bounds that can be used to
279

Hansen & Zhou

reduce the number of nodes stored in the Open list. The resulting savings, both in memory
and in time overhead for managing the Open list, are sometimes greater than the additional
overhead of increased node expansions.
In our experiments, we used relatively low weights that result in fast convergence as
well as good anytime performance. This shows that A* can be transformed into an anytime
algorithm in exchange for little or no delay in convergence to an optimal solution. This
does not mean we recommend that the weight used by Anytime WA* should always be
set low enough to minimize memory use or the time it takes to find a provably optimal
solution. For some search problems, it could be an advantage to use higher weights in an
attempt to find approximate solutions more quickly. In most cases, increasing the weight
used by Anytime WA* allows an approximate solution to be found sooner, but increases
the number of node expansions before convergence to an optimal solution. In the end, the
“best” weight depends on preferences about time-quality tradeoffs.
We have focused on how to use weighted heuristic search to create an anytime heuristic
search algorithm. But in fact, any non-admissible heuristic could be used to guide an
Anytime A* algorithm, as pointed out at the beginning of Section 2.3. It is possible (and
even seems likely) that a more informative, but inadmissible, heuristic could sometimes lead
to better anytime search performance than a weighted admissible heuristic. In this case,
Anytime A* would use two heuristics – a non-admissible heuristic to select the order of
node expansions, and another, admissible heuristic, to prune the search space and detect
convergence to an optimal solution. This is an interesting direction for further exploration.
Our contribution in this paper is to show that an approach as simple as weighting an
admissible heuristic creates a very effective anytime algorithm for many search problems.

3. Anytime RBFS
It is well-known that the scalability of A* is limited by the memory required to store the
Open and Closed lists. This also limits the scalability of Anytime A*. Several variants of
A* have been developed that use less memory, including algorithms that require only linear
space in the depth of the search. We now show how to transform one of them, Recursive
Best-First Search, or RBFS (Korf, 1993), into an anytime algorithm. Besides showing
how to create a linear-space anytime heuristic search algorithm, this helps to illustrate the
generality of our approach by showing that another weighted heuristic search algorithm can
be transformed into an anytime heuristic search algorithm in a similar way, by continuing
the weighted search after the first solution is found.
We begin this section with a brief review of the RBFS algorithm. Then we consider
two approaches to using a weighted evaluation function with RBFS, one that has been
studied before and an alternative that we show has some advantages. Finally, we discuss
how to transform Weighted RBFS into an Anytime Weighted RBFS algorithm. We use
the Fifteen Puzzle as a test domain, which is a larger version of the sliding-tile puzzle that
A* cannot solve optimally because of memory limitations. Because RBFS saves memory
by not storing all generated nodes, it is slowed by excessive node regenerations in solving
graph-search problems with many duplicate paths. As a result, RBFS is not effective (in
terms of time efficiency) for either STRIPS planning or multiple sequence alignment.
280

Anytime Heuristic Search

3.1 Recursive Best-First Search (RBFS)
Recursive best-first search, or RBFS (Korf, 1993), is a general heuristic search algorithm
that expands frontier nodes in best-first order, but saves memory by determining the next
node to expand using stack-based backtracking instead of by selecting nodes from an Open
list. The stack contains all nodes along the path from the start node to the node currently
being visited, plus all siblings of each node on this path. Thus the memory complexity of
RBFS is O(db), where d is the depth of the search and b is the branching factor.
RBFS is similar to a recursive implementation of depth-first search, with the difference
that it uses a special condition for backtracking that ensures that nodes are expanded
(for the first time) in best-first order. Instead of continuing down the current path as far
as possible, as in ordinary depth-first search, RBFS keeps track of the f -cost of the best
alternative path available from any ancestor of the current node, which is passed as an
argument to the recursive function. If the f -cost of the current path exceeds this threshold,
called the local cost threshold, the recursion unwinds back to the alternative path. As the
recursion unwinds, RBFS keeps track of the f -cost of the best unexpanded node on the
frontier of the forgotten subtree by saving it in the stored value F (n). These stored values,
one for each node n on the stack, are used by RBFS to decide which path to expand next
at any point in the search. Because F (n) is the least f -cost of any unexpanded node on the
frontier of the subtree rooted at node n, these stored values can be propagated to successor
nodes during successor generation. If a node has been previously expanded, its (propagated)
stored value will be greater than its static evaluation, and RBFS uses this fact to detect
previously expanded nodes and regenerate subtrees efficiently.
Among the advantages of RBFS, Korf points out that it expands nodes in best-first
order even when the evaluation function is nonmonotonic. To illustrate a nonmonotonic
evaluation function, he considers RBFS using a weighted evaluation function.
3.2 Weighted RBFS
Like A*, RBFS can use a weighted heuristic to trade off solution quality for search time.
Algorithm 2 gives the pseudocode for the recursive function of RBFS using a weighted
evaluation function. This is the same RBFS algorithm described by Korf, although the
notation is slightly adjusted to show that the weighted values F 0 are stored on the stack
instead of the unweighted values F , and the local cost threshold B 0 is a weighted value.
When RBFS is initially invoked, its three arguments are the start node, the (weighted)
evaluation of the start node, and a cost threshold of infinity. Using a weighted evaluation
function, RBFS expands nodes (for the first time) in order of the weighted evaluation
function, f 0 , instead of in order of the unweighted evaluation function, f . Korf (1993)
considers this approach to Weighted RBFS and presents an empirical study of the tradeoff
it offers between search time and solution quality.
To motivate another approach to weighted heuristic search using RBFS, we introduce a
distinction between two search frontiers maintained by RBFS. The stored values, denoted
F (n) or F 0 (n), keep track of the best unexpanded node on the frontier of the subtree rooted
at a node n on the stack. We call this a virtual frontier because RBFS does not actually
store this frontier in memory, but uses these stored values to represent and regenerate the
frontier. We introduce the term stack frontier to refer to the frontier that RBFS actually
281

Hansen & Zhou

Algorithm 2: RBFS (using a weighted evaluation function)
Input: A node n, F 0 (n), and a threshold B 0
begin
if n is a goal node then output solution path and exit algorithm
if Successors(n) = ∅ then return ∞
foreach ni ∈ Successors(n), i = 1, 2, · · · , |Successors(n)| do
g(ni ) ← g(n) + c(n, ni ), f 0 (ni ) ← g(ni ) + w × h(ni )
if f 0 (n) < F 0 (n) then F 0 (ni ) ← max{F 0 (n), f 0 (ni )}
else F 0 (ni ) ← f 0 (ni )
sort ni in increasing order of F 0 (ni )
if |Successors(n)| = 1 then F 0 (n2 ) ← ∞
while F 0 (n1 ) < ∞¡and F 0 (n1 ) ≤ B 0 do
¢
F 0 (n1 ) ←RBFS n1 , F 0 (n1 ), min{B 0 , F 0 (n2 )}
insert n1 in sorted order of F 0 (ni )
return F 0 (n1 )
end

stores in memory. The stack frontier consists of the nodes on the stack that do not have
successor nodes on the stack.
In weighted heuristic search, the weighted evaluation function is used to determine the
order in which to expand nodes on the frontier of the search. In the approach to Weighted
RBFS shown in Algorithm 2, which is the approach adopted by Korf (1993), the weighted
evaluation function is used to select the order in which to expand nodes on the virtual
frontier. Because this virtual frontier is the same frontier that is maintained in memory by
Weighted A*, using an Open list, this approach to Weighted RBFS expands nodes in the
same order as Weighted A* (disregarding tie breaking and node regenerations).
Algorithm 3 shows the pseudocode of an alternative approach to weighted heuristic
search using RBFS. Like the approach shown in Algorithm 2, it uses a weighted evaluation
function and continues to expand a solution path as long as the weighted evaluation of the
currently-expanding node is not greater than the weighted evaluation of any sibling of one
of the nodes along this path. The difference is that instead of backing up the least weighted
evaluation f 0 of any unexpanded node in the subtree rooted at node n and storing it in
F 0 (n), Algorithm 3 backs up the least unweighted evaluation f of any unexpanded node,
and stores it in F (n). If f (n) is an admissible evaluation function, then F (n) is a lower
bound on the cost of the best solution that can be found in the subtree rooted at n. It
follows that H(n) = F (n) − g(n) is an improved admissible heuristic for node n. Therefore,
Algorithm 3 can use the weighted evaluation g(n) + w × H(n) = g(n) + w × (F (n) − g(n)) to
determine the order in which to expand nodes. In this approach to Weighted RBFS, nodes
are expanded in best-first order of the weighted evaluation of nodes on the stack frontier,
instead of in order of the weighted evaluation of nodes on the virtual frontier.
RBFS is a general algorithmic scheme that can use different evaluation functions. Thus,
even when it uses a weighted evaluation function, Korf refers to it as RBFS. To make it
easier to distinguish between these algorithms, we introduce the name WRBFS to refer
to the alternative approach to weighted heuristic search based on RBFS that we propose.
WRBFS expands nodes on the stack frontier in best-first order of the evaluation function
282

Anytime Heuristic Search

Algorithm 3: WRBFS
Input: A node n, F (n), and a threshold B 0
begin
if n is a goal node then output solution path and exit algorithm
if Successors(n) = ∅ then return ∞
foreach ni ∈ Successors(n), i = 1, 2, · · · , |Successors(n)| do
g(ni ) ← g(n) + c(n, ni ), f (ni ) ← g(ni ) + h(ni )
if f (n) < F (n) then F (ni ) ← max{F (n), f (ni )}
else F (ni ) ← f (ni )
sort ni in increasing order of F (ni )
if |Successors(n)| = 1 then F (n2 ) ←
¡ g(n2 ) ← ∞ ¢
while F (n1 ) < ∞ and
g(n
)
+
w
×
F (n1 ) − g(n1 ) ¡≤ B 0 do
1
¡
¢ ¢
F (n1 ) ←WRBFS n1 , F (n1 ), min{B 0 , g(n2 ) + w × F (n2 ) − g(n2 ) }
insert n1 in sorted order of F (ni )
return F (n1 )
end

F 0 (n) = g(n) + w × H(n), instead of expanding nodes on the virtual frontier in order of
the evaluation function f 0 (n) = g(n) + w × h(n). Since F 0 (n) = g(n) + w × H(n) improves
during the search, it is not a static evaluation function, and this is another reason for using
the name WRBFS. Note that when w = 1, as in unweighted RBFS, there is no difference in
the behavior of these two algorithms; expanding nodes in best-first order of their evaluation
on the stack frontier is equivalent to expanding nodes in best-first order of their evaluation
on the virtual frontier. There is only a difference when one considers whether to apply a
weight greater than 1 to the heuristic on the stack frontier or the virtual frontier.
Figure 4 compares the performance of these two approaches to Weighted RBFS in solving
Korf’s (1985) 100 random instances of the Fifteen Puzzle. Figure 4(a) shows the average
length of the solutions found by each algorithm, using weights ranging from 1.0 to 5.0
in increments of 0.1. WRBFS finds better-quality solutions than RBFS using a weighted
evaluation function and the same weight, and the difference increases with the weight. But
since WRBFS can also take longer to find a solution, we consider the time-quality tradeoff
offered by each algorithm. Figure 4(b), which is similar to Figure 10 in the article by
Korf (1993), plots solution length versus time (measured by the number of recursive calls)
in solving the same Fifteen Puzzle examples, using solution lengths ranging from 53 to 85
for both algorithms. The time-quality tradeoff offered by the two algorithms is similar, with
a modest advantage for WRBFS. What is striking is that WRBFS offers a smooth timequality tradeoff, whereas the tradeoff offered by RBFS using a weighted evaluation function
is irregular. Sometimes, increasing the weight used by RBFS causes it to take longer to find
a solution, instead of less time. A dramatic example is that increasing the weight from 1.0
to 1.1 causes RBFS with a weighted evaluation function to take three times longer to find a
(potentially suboptimal) solution than unweighted RBFS takes to find an optimal solution.
Korf (1993) gives the reason for this irregular time-quality tradeoff. The node regeneration overhead of RBFS grows with the number of iterations of the algorithm, which is the
number of times the local cost threshold increases, since each iteration requires regeneration
of subtrees. There is one iteration for each distinct f -cost, or, in the case of RBFS using a
283

Hansen & Zhou

Figure 4: Comparison of RBFS using a weighted evaluation function and WRBFS in solving
Korf’s 100 random instances of the Fifteen Puzzle. Panel (a) shows solution
quality as a function of heuristic weight. Panel (b) shows the time-quality tradeoff
of each algorithm by plotting the number of recursive calls against solution quality.

weighted evaluation function, for each distinct f 0 -cost. The irregular time-quality tradeoff is
caused by fluctuation in the number of distinct f 0 -costs as the weight increases, which leads
to fluctuation in the number of iterations. The number of distinct f 0 -costs can be as many
as the number of distinct pairs of g-cost and h-cost, but the actual number depends on the
weight, since different pairs of g-cost and h-cost may sum to the same f 0 -cost, depending
on the weight. Increasing the weight from 1.0 to 1.1, for example, significantly increases
the number of distinct f 0 -costs, and this explains why RBFS using a weighted evaluation
function takes longer to find a solution in this case. An advantage of using WRBFS is
that the stored value of each node on the stack is the minimum f -cost on the frontier of
the subtree rooted at that node, instead of the minimum f 0 -cost, and thus the number of
iterations is not affected by any variation in the number of distinct f 0 -costs as the weight
is increased. As a result, adjusting the weight creates a smoother time-quality tradeoff.
Both approaches to Weighted RBFS are well-motivated and both offer a useful tradeoff
between search time and solution quality. The original approach expands frontier nodes in
best-first order of the weighted evaluation function f 0 (n) = g(n) + w × h(n), and, in this
respect, it is closer to Weighted A*. But as we have seen, the alternate approach has the
advantage that it allows a smoother time-quality tradeoff. As we consider how to transform
each of these two approaches to Weighted RBFS into an anytime heuristic search algorithm,
we will see that the alternate approach has other advantages as well.
3.3 Anytime Weighted RBFS
It is straightforward to transform either approach to Weighted RBFS into an anytime
algorithm. Algorithm 4 shows the pseudocode of the recursive function of Anytime WRBFS.
There are just a couple differences between a Weighted RBFS algorithm such as WRBFS
284

Anytime Heuristic Search

Algorithm 4: Anytime WRBFS
Input: A node n, F (n), and a threshold B 0
begin
if n is a goal node then return f (n)
if Successors(n) = ∅ then return ∞
foreach ni ∈ Successors(n), i = 1, 2, · · · , |Successors(n)| do
g(ni ) ← g(n) + c(n, ni ), f (ni ) ← g(ni ) + h(ni )
if ni is a goal node and f (ni ) < f (incumbent) then
incumbent ← ni
save (or output) incumbent solution path
if f (ni ) ≥ f (incumbent) then F (ni ) ← ∞
else if f (n) < F (n) then F (ni ) ← max{F (n), f (ni )}
else F (ni ) ← f (ni )
sort ni in increasing order of F (ni )
if |Successors(n)| = 1 then F (n2 ) ← g(n2 ) ← ∞
¡
¢
while F (n1 ) < f (incumbent)¡and g(n1 ) + w × F (n1 ) − g(n1¡) ≤ B 0 do
¢ ¢
F (n1 ) ←Anytime-WRBFS n1 , F (n1 ), min{B 0 , g(n2 ) + w × F (n2 ) − g(n2 ) }
insert n1 in sorted order of F (ni )
return F (n1 )
end

and an Anytime Weighted RBFS algorithm. Most importantly, the condition for termination is different. After the anytime algorithm finds a solution, and each time it finds
an improved solution, it saves (or outputs) the solution and continues the search. As in
Anytime Weighted A*, the algorithm checks whether a node is a goal node when it is generated, instead of waiting until it is expanded. It also checks whether the f -cost of a node
is greater than or equal to an upper bound given by the f -cost of the incumbent solution.
If so, this part of the search space is pruned. (Note that before the first solution is found,
f (incumbent) should be set equal to infinity, since there is not yet a finite upper bound on
the optimal solution cost.) Convergence to an optimal solution is detected when the stack
is empty. At this point, backtracking has determined that all branches of the tree have been
searched or pruned. Proof of termination with an optimal solution follows similar logic as
for Theorem 1. The suboptimality of the currently available solution is bounded by using
f (incumbent) as an upper bound on optimal solution cost and the least F -cost of any node
on the stack frontier as a lower bound. (Again, the stack frontier consists of the node at
the end of the current best path, plus every sibling of a node along this path.)
Figure 5(a) shows performance profiles for Anytime WRBFS, averaged over Korf’s 100
random instances of the Fifteen Puzzle. Although weights of 2.0 and 1.5 offer a better timequality tradeoff for short amounts of search time, a weight of 1.3 provides better long-term
performance. Figure 5(b) shows the time (measured by the average number of recursive
calls) taken by Anytime WRBFS to find optimal solutions for the Fifteen puzzle, using
weights from 1.0 to 2.0 in increments of 0.1. Using weights from 1.2 to 1.4, it converges
to an optimal solution more quickly than unweighted RBFS. In fact, using a weight of 1.3,
Anytime WRBFS converges to an optimal solution after an average of 25% fewer recursive
calls than unweighted RBFS. Although it always expands as many or more distinct nodes
than unweighted RBFS, reliance on stack-based backtracking to reduce memory use means
285

Hansen & Zhou

Figure 5: Performance of Anytime WRBFS. Panel (a) shows performance profiles using
three different weights, averaged over Korf’s 100 random instances of the Fifteen
Puzzle. Panel (b) shows the average number of recursive calls required to converge
to an optimal solution, using weights from 1.0 to 2.0 in increments of 0.1, and
averaged over the same 100 random instances of the Fifteen Puzzle.

that both algorithms can revisit the same nodes multiple times. Figure 5(b) shows that
the weighted heuristic used by Anytime WRBFS can reduce the number of recursive calls;
intuitively, this occurs because the greedier search strategy of weighted heuristic search
tends to delay and reduce backtracking. Of course, if the weight is increased enough, the
number of distinct node expansions increases and eventually the number of recursive calls
also increases, as Figure 5(b) shows. Nevertheless, the demonstration that a small weight
can sometimes improve efficiency in finding optimal solutions is interesting.
For comparison, Figure 6(a) shows performance profiles for a version of Anytime Weighted
RBFS that is based on RBFS using a weighted evaluation function, which is the original
approach to Weighted RBFS. In this case, the performance profile of Anytime Weighted
RBFS using a weight of 1.3 is dominated by its performance profiles using weights of 1.5
and 2.0. Figure 6(b) shows the average number of recursive calls taken by this version of
Anytime Weighted RBFS to find optimal solutions for the Fifteen puzzle, using the same
range of weights from 1.0 to 2.0. To ensure fair comparison, we implemented this version
of Anytime Weighted RBFS so that it saves the admissible F (n) values in addition to the
non-admissible F 0 (n) values, and uses the F (n) values to prune branches of the search tree
and detect convergence to an optimal solution, instead of using the static evaluation f (n).
Nevertheless, this version of Anytime Weighted RBFS converges much more slowly. The
scale of the y-axis in Figure 6(b) is an order of magnitude greater than in Figure 5(b),
and this reflects the fact that Anytime Weighted RBFS based on this version of Weighted
RBFS takes an order of magnitude longer to converge to an optimal solution than Anytime
WRBFS, using the same weight.
Fluctuations in the length of time until convergence in Figure 6(b) are caused by differences in the number of distinct f 0 -costs as the weight increases, causing differences in
286

Anytime Heuristic Search

Figure 6: Performance of Anytime Weighted RBFS based on RBFS using a weighted evaluation function. Panel (a) shows performance profiles using three different weights,
averaged over Korf’s 100 random instances of the Fifteen Puzzle. Panel (b) shows
the average number of recursive calls to converge to an optimal solution, using
weights from 1.0 to 2.0 in increments of 0.1, averaged over the same instances.
The scale of the y-axis is an order of magnitude greater than for Figure 5(b).

the number of iterations and resulting fluctuations in node regeneration overhead. This is
similar to what we observed earlier of the performance of this approach to Weighted RBFS.
But it does not explain the very large difference in the time it takes each algorithm to converge. There are at least two reasons why Anytime WRBFS converges much faster. One
is more efficient backtracking behavior. Because Anytime WRBFS expands nodes in order
of a weighted evaluation function on the stack frontier, instead of in order of a weighted
evaluation function on the virtual frontier, it searches more greedily at deeper levels on the
stack before backtracking to shallower levels. Since it is more computationally expensive
to regenerate the large subtrees that are rooted at shallower nodes on the stack than the
smaller subtrees that are rooted at deeper nodes, this bias towards backtracking at deeper
levels before backtracking to shallower levels contributes to improved convergence time.
Another reason that Anytime WRBFS converges much faster is that it is more effective
in improving the lower bound on optimal solution cost. As we pointed out earlier, anytime
heuristic search often finds what turns out to be an optimal solution relatively quickly,
and spends most of its time proving that the solution is optimal, which corresponds to
improving a lower bound. In both versions of Anytime Weighted RBFS, the lower bound is
the minimum of the F (n) values stored on the stack frontier. Although an anytime search
algorithm based on the original version of Weighted RBFS is guaranteed to improve the
F 0 (n) value of a subtree rooted at node n each iteration, it may or may not improve the F (n)
value (which we assume it also stores). By contrast, an anytime search algorithm based
on WRBFS is guaranteed to improve the F (n) value each iteration. Because it improves
admissible F (n) values, instead of weighted F 0 (n) values, Anytime WRBFS is more effective
in improving the lower bound on optimal solution cost, leading to faster convergence.
287

Hansen & Zhou

4. Related Work
In this section, we consider some closely-related work. We begin by considering a variant
of Anytime A* that has been recently proposed. Then we discuss the relationship between
Anytime A* and other variants of A* that, directly or indirectly, also allow a tradeoff
between search time and solution quality.
4.1 Anytime Repairing A*
Likhachev, Gordon, and Thrun (2004) have recently introduced an interesting variant of
Anytime A*, called Anytime Repairing A* (or ARA*), and have shown that it is very
effective for real-time robot path planning. Their approach follows our approach to creating
an Anytime A* algorithm in many respects. It uses Weighted A* to find an approximate
solution relatively quickly, and continues the weighted search to find a sequence of improved
solutions until convergence to an optimal solution. However, it introduces two extensions
to improve performance. First, after each solution is found, it decreases the weight before
continuing the search. Second, it uses a technique to limit node reexpansions. The first
of these extensions, decreasing the weight as new solutions are found, is easy to consider
independently of the other, and can also be easily combined with Anytime Weighted A*
(AWA*), and so we consider it first.
Decreasing the weight In our experiments, we used a weighted heuristic with a weight
that did not change during the search. We chose this approach because of its simplicity.
Likhachev et al. (2004) argue that better performance is possible by gradually decreasing
the weight during search. After each new solution is found, ARA* decreases the weight
by a small, fixed amount, and continues the search. Experimental results show that this
approach leads to improved performance in their robot path-planning domains.
Of course, relative performance can depend on the initial weight and not simply on
whether the weight remains fixed or decreases. Likhachev et al. report three experimental
comparisons of AWA* and ARA*. In one, they set the initial weight to 3; in another, they
set the initial weight to 10; in the third, they set the initial weight to 30. These weights
are higher than those we found worked well for our test problems. In their experiments,
AWA* never changes this initial weight whereas ARA* decreases it as new solutions are
found. If the initial weight is set too high, this might explain why decreasing it improves
performance. It could also be that setting the initial weight high and gradually decreasing
it is the most effective approach for the robot path-planning problems they consider, and for
similar problems. Even so, it does not follow that it is the best approach for all problems.
We compared Anytime Weighted A* and Anytime Repairing A* in solving the STRIPS
planning problems used as a testbed in Section 2.4.2. Although Likhachev et al. did not
use upper bounds to reduce the size of the Open list in their implementation of ARA*,
it is easy to do so and we included this enhancement in our implementation of ARA* in
order to ensure fair comparison. In addition to decreasing the weight, ARA* uses a special
“repairing” technique to limit node reexpansions. Since this is an independent idea, we
implemented a version of AWA* that decreases the weight during search but does not use
this special technique for limiting node reexpansions. By itself, decreasing the weight during
search only requires recalculating f 0 -costs and reordering the Open list whenever the weight
288

Anytime Heuristic Search

Instances
Blocks-8
Logistics-6
Satellite-6
Freecell-3
Psr-46
Depots-7
Driverlog-11
Elevator-12

AWA* (weight = 2, step = 0.1)
Stored
Exp Opt %
Secs
41,166
40,727
0.2%
3.9
254,412
254,390
6.2%
3.6
2,423,547
2,423,489 14.3% 138.6
2,695,321
2,698,596
1.7% 155.3
7,148,048
7,171,557 69.0% 345.7
7,773,830
7,762,783
0.5% 247.4
6,763,379
6,693,441
4.7% 283.3
12,734,636 12,825,980 98.7% 561.3

ARA* (weight = 2, step = 0.1)
Stored
Exp Opt %
Secs
41,166
42,141
0.2%
3.9
312,438
364,840 87.1%
5.0
2,423,547
2,428,325 14.2% 138.4
4,115,032
5,911,849 68.7% 317.2
7,143,912 11,888,700 35.3% 567.7
7,771,780
7,823,005
0.5% 247.1
6,698,404
6,771,651
1.3% 281.3
12,736,328 12,843,441 97.8% 559.7

Table 3: Comparison of AWA* (with decreasing weight) and ARA* on eight benchmark
problems from the biennial Planning Competitions.

is changed. Table 3 compares the performance of AWA* and ARA* when both use an initial
weight of 2.0 and decrease the weight by 0.1 after each new solution is found. For all planning
instances except Logistics-6, Freecell-3 and Psr-46, there is no significant difference in their
performance or any significant difference between their performance and the performance
of AWA* with a fixed weight of 2.0 in solving the same instances. (See Table 1.) For
these STRIPS planning problems and using this initial weight, gradually decreasing the
weight does not improve performance. Of course, it could improve performance for other
problems. In that case, we note that it is easy to decrease the weight used by AWA* without
implementing the full ARA* algorithm.
Another potential advantage of decreasing the weight, as Likhachev et al. point out, is
that it provides a different way of bounding the suboptimality of a solution. For any solution
found by Weighted A* using a weight of w, one has the error bound, f (incumbent)
≤ w. Note
f∗
that decreasing this bound requires decreasing the weight during the search.
In Section 2.3.2, we defined a different error bound, f (incumbent)
≤ f (incumbent)
, where
f∗
fL
L
f denotes the least f -cost of any currently open node on the frontier. An advantage of
this error bound is that it decreases even if the weight remains fixed during the search.
An additional advantage is that it is a tighter bound. Let nL denote an open node with
f (nL ) = f L . Because h(incumbent) = 0 and incumbent was expanded before nL , we know
that
f (incumbent) = f 0 (incumbent) ≤ g(nL ) + w × h(nL ).
Therefore,
f (incumbent)
g(nL ) + w × h(nL )
w × (g(nL ) + h(nL ))
≤
<
= w,
fL
g(nL ) + h(nL )
g(nL ) + h(nL )
where the strict inequality follows from the assumptions that w > 1 and g(nL ) > 0.
Although ARA* performs about the same as AWA* in solving five of the eight planning
problems, it performs worse in solving the other three: Logistics-6, Freecell-3, and Psr-46.
Comparing ARA* to AWA* when both have the same initial weight and decrease the weight
in the same way shows that this deterioration in performance is not caused by decreasing
the weight. We consider next the technique used by ARA* for limiting node reexpansions.
289

Hansen & Zhou

Figure 7: Comparison of AWA* and ARA* in solving all instances of the Eight Puzzle.
Panel (a) shows the average number of nodes stored and panel (b) shows the
average number of node expansions, both as a function of the initial weight.

Limiting node reexpansions As discussed before, a complication that Anytime Weighted
A* inherits from Weighted A* is that a weighted heuristic is typically inconsistent. This
means it is possible for a node to have a higher-than-optimal g-cost when it is expanded. If
a better path to the same node is later found, the node is reinserted in the Open list so that
the improved g-cost can be propagated to its descendants when the node is reexpanded. As
a result, both Weighted A* and Anytime WA* can expand the same node multiple times.
Likhachev et al. note that the error bound for Weighted A* remains valid even if node
reexpansions are not allowed. Since their ARA* algorithm performs a series of Weighted A*
searches with decreasing weights, they reason that if ARA* postpones node reexpansions
until the current iteration of Weighted A* finishes and the weight is decreased, this will
create a more efficient Anytime A* algorithm. When ARA* finds a better path to an
already-expanded node, it inserts the node into a list called INCONS in order to delay node
reexpansion. When a solution is found and the weight is decreased, ARA* moves all nodes
in the INCONS list to the Open list and resumes the search.
This technique for limiting node reexpansions may improve search performance for robot
path planning and similar search problems, especially when using large weights. But the
relative performance of AWA* and ARA* in solving the Logistics-6, Freecell-3, and Psr-46
planning instances raises a question about whether it always improves performance. For
further comparison, Table 7 shows the average performance of AWA* and ARA* in solving
all instances of the Eight Puzzle. Each algorithm has the same initial weight. AWA* never
changes the initial weight while ARA* reduces it in increments of 0.1 as new solutions are
found. A larger weight makes the heuristic more inconsistent and increases the likelihood
that AWA* will reexpand nodes. Yet the results show that the larger the initial weight, the
more nodes ARA* expands relative to AWA*, and the difference is dramatic. When the
initial weight is set to 3.0, ARA* expands more than four times more nodes than AWA*.
290

Anytime Heuristic Search

In many cases, it takes longer for ARA* to find an initial suboptimal solution than it takes
unweighted A* to find an optimal solution.
One reason for this result is that limiting node reexpansions can cause ARA* to expand
more distinct nodes. (The fact that ARA* stores as well as expands more nodes, as shown
in Figure 7, indicates that it expands and generates more distinct nodes.) Limiting node
reexpansions can lead to expansion of more distinct nodes because it blocks improvement of
all paths that pass through any node stored in the INCONS list. By blocking improvement
of these paths, it can prevent better solutions from being found. One possibility is that
the solution found by Weighted A* passes through a node that is stored in the INCONS
list, which means that reexpansion and propagation of its improved g-cost is postponed. In
that case, the f 0 -cost of the solution is greater than it would be if reexpansion of the node
was allowed. Another possibility is that a potentially better solution than the one found
by Weighted A* passes through a node in the INCONS list, and therefore is not discovered
because its improvement is blocked. Either way, the solution found by Weighted A* when
it does not allow node reexpansions can have a greater f 0 -cost than if node reexpansions
are allowed. Because Weighted A* must expand all nodes with an f 0 -cost less than the
f 0 -cost of the solution it finds, more distinct nodes can be expanded whenever limiting node
reexpansions prevents Weighted A* from finding a better solution. As Figure 7 shows, this
effect becomes more pronounced as increasing the weight increases the likelihood that the
first time ARA* expands a node, its g-cost is suboptimal.
Our results show that this effect does not occur for all search problems, at least to
the same degree. It seems to occur primarily for search problems with relatively sparse
solutions, such as the sliding-tile puzzle and the Logistics and Freecell planning domains.
When solutions are sparse, it is easier for all nodes that lead to a good solution to be
expanded with a higher-than-optimal g-cost, and thus more likely for Weighted A* to find a
solution that is worse than it would have found if it allowed node reexpansions. For search
problems with a huge number of solutions of equal or almost equal cost, limiting node
reexpansions in this way is less likely to cause the same problem. The robot path-planning
problems considered by Likhachev et al. are examples of this kind of search problem, and
thus the impressive results they report are not inconsistent with our observations.
There is yet another way in which limiting node reexpansions sometimes makes search
performance worse. So far, we have considered search problems where ARA* expands more
distinct nodes than AWA*. But for the Psr-46 planning instance, ARA* expands many
more nodes than AWA*, but does not store more nodes. This indicates that ARA* does
not expand more distinct nodes than AWA*. Instead, it performs more node reexpansions.
How is this possible when ARA* explicitly limits node reexpansions? It turns out that
limiting node reexpansions in the way that ARA* does can sometimes lead to more node
reexpansions. By the time ARA* decreases its weight and reexpands a node to propagate
improved path information, the reexpanded node can have many more descendants in the
explicit search graph than it did when an improved path to the node was originally found.
As a result, many more nodes may need to be reexpanded to propagate the improved path
information. Again, this does not always happen. But the behavior of ARA* in solving
Psr-46 illustrates this possibility.
Figure 7(b) compares the average number of nodes expanded by ARA* and AWA* in
solving all instances of the Eight Puzzle, but it does not show CPU time. With an initial
291

Hansen & Zhou

weight of 3, ARA* expands about 4.5 times more nodes than AWA*. The difference in
CPU time is actually greater. ARA* takes 7 times longer to solve these problems than
AWA*, on average. One reason for this is the extra time overhead for recalculating f 0 -costs
and reordering the Open list every time the weight is decreased. This time overhead is
negligible for the STRIPS planning problems compared to the much greater overhead for
domain-independent node generation and heuristic calculation. But for the sliding-tile
puzzle, node generation and heuristic calculation are so fast that the time overhead for
recalculating f 0 -costs and reordering the Open list has a noticeable effect in slowing the
search. This is another example of how the relative performance of ARA* and AWA* can
vary with the search problem.
In summary, the idea of decreasing the weight during search can be used independently
of the technique for limiting node reexpansions. Although gradually decreasing the weight
did not improve performance for our test problems, it could improve performance for other
problems. However, the additional overhead for recalculating f 0 -costs and reordering the
Open list should be considered. The technique for limiting node reexpansions can also help,
but should be used with caution. For some problems, we have shown that it can actually
cause significantly more node reexpansions or expansion of more distinct nodes. For other
problems, it does not have a negative effect. Although it did not show a clear benefit in our
test domains, it could improve performance for robot path planning and similar problems
with many close-to-optimal solutions, especially when using a large weight.
4.2 Real-time A*
An anytime approach to heuristic search is effective for real-time search problems where not
enough time is available to search for an optimal solution. Previous work on time-limited
heuristic search adopts the model of Korf’s Real-time A* algorithm (RTA*) which assumes
that search is interleaved with execution (Korf, 1990). After searching for a bounded amount
of time, the best next action is chosen and the search-act cycle repeats until the goal state
is reached. Similar examples of this real-time search strategy include DTA* (Russell &
Wefald, 1991), BPS (Hansson & Mayer, 1990) and k-best (Pemberton, 1995). Because realtime search algorithms commit to actions before finding a complete solution, they cannot
find optimal solutions. In contrast, we assume that a search phase precedes an execution
phase and that the output of the search is a complete solution. In other words, real-time
search algorithms try to find the best next decision under a time constraint, whereas our
anytime approach tries to find the best complete solution under a time constraint.
4.3 Depth-first branch and bound and Iterative-Deepening A*
Depth-first branch-and-bound (DFBnB) search algorithms are very effective for tree-search
problems, especially those that have many solutions at the same depth, such as the traveling
salesman problem. For such problems, DFBnB has the behavior of an anytime algorithm.
It quickly finds a solution that is suboptimal, and then continues to search for improved
solutions until an optimal solution is found. It even uses the cost of the best solution found
so far as an upper bound to prune the search space.
A search technique that combines elements of DFBnB and A* is Iterative-deepening
A* or IDA* (Korf, 1985). It is well-known that IDA* performs poorly on problems with
292

Anytime Heuristic Search

real-valued edge costs such as the traveling salesman where almost all nodes have distinct
f -costs. For such problems, it may expand only one new node each iteration. To prevent
excessive iterations and node regenerations, several variants of IDA* have been developed
that set successive thresholds higher than the minimum f -cost that exceeded the previous
threshold (Sarkar, Chakrabarti, Ghose, & DeSarkar, 1991; Rao, Kumar, & Korf, 1991; Wah
& Shang, 1994). As a result, the first solution found is not guaranteed to be optimal,
although it has bounded error. After finding an initial solution, these algorithms revert to
DFBnB search to ensure eventual convergence to an optimal solution. This approach to
reducing node regenerations in IDA* has the side-effect of creating an anytime algorithm,
although one that is only effective for problems for which IDA* and DFBnB are effective,
which are typically tree-search problems.
4.4 Bidirectional A*
Another search technique that has the side-effect of creating an anytime algorithm is Bidirectional A* (Kaindl & Kainz, 1997). In this approach, two simultaneous A* searches are
performed, one from the start state to the goal, and the other from the goal to the start
state. When the two search frontiers meet at a node, the two partial solutions are combined
to create a complete solution. Typically, the first solution found is suboptimal and the
search must be continued to find an optimal solution. Thus, a bidirectional search strategy
has the side-effect of finding a succession of improved solutions before convergence to an
optimal solution. In fact, the convergence test used by Bidirectional A* to detect an optimal
solution is similar to the convergence test used by Anytime WA*: an incumbent solution
of cost f (incumbent) is optimal if there is no unexpanded node with an f -cost less than
f (incumbent) in one of the two directions of the search, that is, if one of the two Open lists
is empty. An interesting possibility for improving bidirectional search is to use Anytime A*
(instead of standard A*) to search in both directions.
4.5 Local-search variants of A*
An important class of anytime search algorithms relies on local search in some form to iteratively improve a solution. Although local-search algorithms cannot guarantee convergence
to an optimal solution, they scale much better than systematic search algorithms. There
have been a couple attempts to improve the scalability of A* by integrating it with some
form of local search. Ratner and Pohl (1986) propose two local-search variants of A* that
improve a suboptimal solution path by making local searches around segments of the path.
For example, Joint A* divides an initial suboptimal solution path into segments, and, for
each segment, uses A* to search for a better path between the start and end states of the
segment, to reduce overall solution cost. Ikeda et al. (1999) propose a K-group A* algorithm for multiple sequence alignment that performs A* on groups of sequences, instead of
individual sequences, in order to reduce search complexity when the number of sequences
is too large to find optimal alignments. By varying the groupings, a local-search algorithm
is created that gradually improves an alignment in a computationally feasible way (Zhou &
Hansen, 2004). But, like other local-search algorithms, these local-search variants of A* do
not guarantee convergence to an optimal solution.
293

Hansen & Zhou

5. Conclusion
We have presented a simple approach for converting a heuristic search algorithm such as A*
into an anytime algorithm that offers a tradeoff between search time and solution quality.
The approach uses weighted heuristic search to find an initial, possibly suboptimal solution,
and then continues to search for improved solutions until convergence to a provably optimal
solution. It also bounds the suboptimality of the currently available solution.
The simplicity of the approach makes it very easy to use. It is also widely applicable.
Not only can it be used with other search algorithms that explore nodes in best-first order,
such as RBFS, we have shown that it is effective in solving a wide range of search problems.
As a rule, it is effective whenever a suboptimal solution can be found relatively quickly
using a weighted heuristic, and finding a provably optimal solution takes much longer.
That is, it is effective whenever weighted heuristic search is effective. If the weight is
chosen appropriately, we have shown that this approach can create a search algorithm
with attractive anytime properties without significantly delaying convergence to a provably
optimal solution. We conclude that anytime heuristic search provides an attractive approach
to challenging search problems, especially when the time available to find a solution is
limited or uncertain.
Acknowledgments
We are grateful to Shlomo Zilberstein for encouragement of this work, especially in its
early stages. We appreciate the very helpful comments and suggestions of the anonymous
reviewers, which led to several improvements of the paper. We also thank Rich Korf for
helpful feedback about the RBFS algorithm. This research was supported in part by NSF
grant IIS-9984952 and NASA grant NAG-2-1463.

References
Bagchi, A., & Mahanti, A. (1983). Search algorithms under different kinds of heuristics – a
comparative study. Journal of the ACM, 30 (1), 1–21.
Bagchi, A., & Srimani, P. (1985). Weighted heuristic search in networks. Journal of Algorithms, 6, 550–576.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (1),
5–33.
Carillo, H., & Lipman, D. (1988). The multiple sequence alignment problem in biology.
SIAM Journal of Applied Mathematics, 48 (5), 1073–1082.
Chakrabarti, P., Ghosh, S., & DeSarkar, S. (1988). Admissibility of AO* when heuristics
overestimate. Artificial Intelligence, 34 (1), 97–113.
Davis, H., Bramanti-Gregor, A., & Wang, J. (1988). The advantages of using depth and
breadth components in heuristic search. In Ras, Z., & Saitta, L. (Eds.), Methodologies
for Intelligent Systems 3, pp. 19–28.
294

Anytime Heuristic Search

Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies and the optimality
of A*. Journal of the ACM, 32 (3), 505–536.
Gasching, J. (1979). Performance measurement and analysis of certain search algorithms.
Ph.D. thesis, Carnegie-Mellon University. Department of Computer Science.
Hansen, E., Zilberstein, S., & Danilchenko, V. (1997). Anytime heuristic search: First
results. Tech. rep. 97-50, Univ. of Massachusetts/Amherst, Dept. of Computer Science.
Hansen, E., & Zilberstein, S. (2001). LAO*: A heuristic search algorithm that finds solutions
with loops. Artificial Intelligence, 129 (1–2), 35–62.
Hansson, O., & Mayer, A. (1990). Probabilistic heuristic estimates. Annals of Mathematics
and Artificial Intelligence, 2, 209–220.
Harris, L. (1974). The heuristic search under conditions of error. Artificial Intelligence,
5 (3), 217–234.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination of
minimum cost paths. IEEE Transactions on Systems Science and Cybernetics (SSC),
4 (2), 100–107.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Proceedings
of the 5th International Conference on Artifial Intelligence Planning and Scheduling
(AIPS-00), pp. 140–149. AAAI Press.
Ikeda, T., & Imai, T. (1994). Fast A* algorithms for multiple sequence alignment. In
Genome Informatics Workshop 94, pp. 90–99.
Ikeda, T., & Imai, H. (1999). Enhanced A* algorithms for multiple alignments: Optimal
alignments for several sequences and k-opt approximate alignments for large cases.
Theoretical Computer Science, 210 (2), 341–374.
Kaindl, H., & Kainz, G. (1997). Bidirectional heuristic search reconsidered. Journal of
Artificial Intelligence Research, 7, 283–317.
Kobayashi, H., & Imai, H. (1998). Improvement of the A* algorithm for multiple sequence
alignment. In Proceedings of the 9th Workshop on Genome Informatics (GIW’98),
pp. 120–130. Universal Academy Press, Inc.
Koll, A., & Kaindl, H. (1992). A new approach to dynamic weighting. In Proceedings of
the 10th European Conference on Artificial Intelligence (ECAI-92), pp. 16–17. John
Wiley and Sons.
Korf, R. (1985). Depth-first iterative deepening: An optimal admissible tree search. Artificial Intelligence, 27 (1), 97–109.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42 (2–3), 197–221.
Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62 (1), 41–78.
295

Hansen & Zhou

Likhachev, M., Gordon, G., & Thrun, S. (2004). ARA*: Anytime A* with provable bounds
on sub-optimality. In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference (NIPS-03). MIT Press.
Long, D., & Fox, M. (2003). The 3rd international planning competition: Results and
analysis. Journal of Artificial Intelligence Research, 20, 1–59.
Pearl, J. (1984). Heuristics: Intelligent search strategies for computer problem solving.
Addison-Wesley.
Pearl, J., & Kim, J. (1982). Studies in semi-admissible heuristics. IEEE Transactions on
Pattern Analysis and Machine Intelligence, PAMI-4 (4), 392–399.
Pemberton, J. (1995). k-best: A new method for real-time decision making. In Proceedings
of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95), pp.
227–233. Morgan Kaufmann.
Pohl, I. (1970a). First results on the effect of error in heuristic search. Machine Intelligence,
5, 219–236.
Pohl, I. (1970b). Heuristic search viewed as path finding in a graph. Artificial Intelligence,
1 (3), 193–204.
Pohl, I. (1973). The avoidance of (relative) catastrophe, heuristic competence, genuine dynamic weighting and computational issues in heuristic problem-solving. In Proceedings
of the 3rd International Joint Conference on Artificial Intelligence (IJCAI-73), pp.
12–17. Morgan Kaufmann.
Rao, V., Kumar, V., & Korf, R. (1991). Depth-first vs. best-first search. In Proceedings
of the 9th National Conference on Artificial Intelligence (AAAI-91), pp. 434–440.
AAAI/MIT Press.
Ratner, D., & Pohl, I. (1986). Joint and LPA*: Combination of approximation and search.
In Proceedings of the 5th National Conference on Artificial Intelligence (AAAI-86),
pp. 173–177. AAAI/MIT Press.
Russell, S., & Wefald, E. (1991). Do the Right Thing: Studies in Limited Rationality. MIT
Press.
Sarkar, U., Chakrabarti, P., Ghose, S., & DeSarkar, S. (1991). Reducing reexpansions in
iterative-deepening search by controlling cutoff bounds. Artificial Intelligence, 50 (2),
207–221.
Shimbo, M., & Ishida, T. (2003). Controlling the learning process of real-time heuristic
search. Artificial Intelligence, 146 (1), 1–41.
Wah, B., & Shang, Y. (1994). A comparative study of IDA*-style searches. In Proceedings
of the 6th International Conference on Tools with Artificial Intelligence (ICTAI-94),
pp. 290–296. IEEE Computer Society.
296

Anytime Heuristic Search

Yoshizumi, T., Miura, T., & Ishida, T. (2000). A* with partial expansion for large branching factor problems. In Proceedings of the 17th National Conference on Artificial
Intelligence (AAAI-2000), pp. 923–929. AAAI/MIT Press.
Zhou, R., & Hansen, E. (2002). Multiple sequence alignment using Anytime A*. In Proceedings of the 18th National Conference on Artificial Intelligence (AAAI-02), pp. 975–6.
AAAI/MIT Press.
Zhou, R., & Hansen, E. (2004). K-group A* for multiple sequence alignment with quasinatural gap costs. In Proceedings of the 16th IEEE International Conference on Tools
with Artificial Intelligence (ICTAI-04), pp. 688–695. IEEE Computer Society.
Zilberstein, S. (1996). Using anytime algorithms in intelligent systems. AI Magazine, 17 (3),
73–83.

297

Journal of Artificial Intelligence Research 28 (2007) 119–156

Submitted 04/06; published 02/07

Marvin: A Heuristic Search Planner with
Online Macro-Action Learning
Andrew Coles
Amanda Smith

ANDREW. COLES @ CIS . STRATH . AC . UK
AMANDA . SMITH @ CIS . STRATH . AC . UK

Department of Computer and Information Sciences,
University of Strathclyde,
26 Richmond Street, Glasgow, G1 1XH, UK

Abstract
This paper describes Marvin, a planner that competed in the Fourth International Planning
Competition (IPC 4). Marvin uses action-sequence-memoisation techniques to generate macroactions, which are then used during search for a solution plan. We provide an overview of its
architecture and search behaviour, detailing the algorithms used. We also empirically demonstrate
the effectiveness of its features in various planning domains; in particular, the effects on performance due to the use of macro-actions, the novel features of its search behaviour, and the native
support of ADL and Derived Predicates.

1. Introduction
One of the currently most successful approaches to domain-independent planning is forwardchaining heuristic search through the problem state space. Search is guided by a heuristic function
based on an appropriate relaxation of the planning problem. Different relaxations have been explored (Bonet & Geffner, 2000; McDermott, 1996; Hoffmann & Nebel, 2001; Helmert, 2004) and
have been shown to result in more or less informative heuristic functions. A common relaxation
is to ignore the delete lists of actions in the problem domain, resulting in an abstracted problem
domain comprised of relaxed actions. A given state can then be evaluated by counting the number
of relaxed actions needed to reach the goal state from the given state. Hoffmann and Nebel (2001)
present a search strategy called Enforced Hill-Climbing (EHC) which, coupled with a relaxation of
this kind, has been proven empirically to be an effective strategy in many planning domains. Their
planner, FF, performed with great success in the Second and Third International Planning Competitions (Bacchus, 2001; Long & Fox, 2003). In this paper we present our planner, Marvin, which
builds upon this search approach.
The EHC search strategy performs gradient-descent local search, using breadth-first search to
find action sequences leading to strictly-better states should no single-action step be able to reach
one. This embedded exhaustive-search step is one of the bottlenecks in planning with this approach.
We present an approach that, through memoising the plateau-escaping action sequences discovered
during search, can form macro-actions which can be applied later when plateaux are once-again
encountered. In doing so, the planner can escape from similar plateaux encountered later, without
expensive exhaustive search. The resulting planner is called Marvin.
We begin this paper with a brief review of FF’s search behaviour to provide the background for
our approach. We then introduce the main features of Marvin, explaining how its search behaviour
differs from that of FF. We describe the three main contributions made by Marvin, detailing the
c
2007
AI Access Foundation. All rights reserved.

C OLES & S MITH

key algorithms and their effects on performance. Marvin can plan in STRIPS and ADL domains,
and it can also handle the derived predicates of PDDL2.2. We describe the way in which domains
containing derived predicates and ADL are handled without first being reduced to STRIPS domains.
Finally, we discuss the results obtained by Marvin in the Fourth International Planning Competition
(IPC 4) (Hoffmann & Edelkamp, 2005), and provide additional ablation studies to assess the impact
of its various features on planning performance across a selection of domains.

2. Background
In this section, we give an overview of the background for this work. First, forward-chaining heuristic planning is defined, and existing work in this area described; with particular attention paid to the
planner FF. This is followed by an introduction to macro-actions.
2.1 Forward-Chaining Planning
Formally, forward-chaining planning can be described as search through a landscape where each
node is defined by a tuple < S, P >. S is a world state comprised of predicate facts and P is the
plan (a series of ordered actions) used to reach S from the initial state. Search begins from the initial
problem state, corresponding to a tuple < S0 , {} >.
Edges between pairs of nodes in the search landscape correspond to applying actions to lead
from one state to another. When an action A is applied to a search space node < S, P > the node
< S 0 , P 0 > is reached, where S 0 is the result of applying the action A in the state S and P 0 is
determined by appending the action A to P . Forward-chaining search through this landscape is
restricted to only considering moves in a forwards direction: transitions are only ever made from a
node with plan P to nodes with a plan P 0 where P 0 can be determined by adding (or ‘chaining’)
actions to the end of P .
As unguided search in this manner is prohibitively expensive in all but the smallest problems,
heuristics are used to guide search. Commonly, a heuristic value is used to provide a goal distance
estimate from a node < S, P > to a node < S 0 , P 0 > in which S 0 is a goal state.
2.2 Heuristics for Forward-Chaining Planning
Many of the heuristics used to guide forward-chaining planners are based around solving an abstraction of the original, hard, planning problem with which the planner is presented. The most widely
used abstraction involves planning using ‘relaxed actions’, where the delete effects of the original
actions are ignored. FF, HSP (Bonet & Geffner, 2000) and UnPOP (McDermott, 1996) use relaxed
actions as the basis for their heuristic estimates, although FF was the first to count the number of
relaxed actions in a relaxed plan connecting the goal to the initial state. Although ignoring delete
lists turns out to be a powerful relaxation, at least for a class of planning domains, other relaxations
are possible. More recently, work has been done using an abstraction based on causal graph analysis
(Helmert, 2004).
The initial approaches for calculating goal distance estimates, taken by planners such as HSP,
calculated the cost of reaching the goal state from the state to be evaluated by doing a forwards
reachability analysis from the state until the given goal appears. Two heuristics can be derived from
this information: either the maximum of the steps-to-goal values—an admissible heuristic; or the
sum of the steps-to-goal values—an inadmissible heuristic, which in practice is more informative.
120

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

Procedure: EHCSearch
open list = [initial state];
best heuristic = heuristic value of initial state;
while open list not empty do
current state = pop state from head of open list;
successors = the list of states visible from current state;
while successors is not empty do
next state = remove a state from successors;
h = heuristic value of next state;
if next state is a goal state then
return next state;
end if
if h better than best heuristic then
clear successors;
clear open list;
best heuristic = h;
end if
place next state at back of open list;
end while
end while

Figure 1: Enforced Hill-Climbing Search

The disadvantage of the latter of these approaches is that it ignores any positive interactions (shared
actions) between the action sequences for each goal: it is this problem which was addressed by the
heuristic used in FF. In FF, a planning graph (Blum & Furst, 1995) is built forward from the current
state using relaxed actions—this is known as a relaxed planning-graph (RPG). A relaxed plan (one
using relaxed actions) to achieve the goal state can be extracted from the RPG in polynomial time;
the number of actions in this plan can be used to provide the heuristic value. As Graphplan does not
provide a guarantee that the plan found will contain the optimum number of sequentialised actions
(only that it will have the optimum makespan) the heuristic returned is inadmissible, but in practice
the heuristic is more informative than any of those used previously.
2.3 Enforced Hill Climbing Search
Along with a heuristic based on relaxed planning graphs, FF introduced the Enforced Hill Climbing
(EHC) algorithm, illustrated in Figure 1. EHC is based on the commonly used hill-climbing algorithm for local search, but differs in that breadth-first search forwards from the global optimum is
used to find a sequence of actions leading to a heuristically better successor if none is present in the
immediate neighbourhood.
The key bottleneck in using EHC is where the search heuristic cannot provide sufficient guidance to escape a plateau1 in a single action step, and breadth-first search is used until a suitable
action sequence is found. Characteristically, EHC search consists of prolonged periods of exhaustive search, bridged by relatively quick periods of heuristic descent.
1. In this work, a plateau is defined to be a region in the search space where the heuristic values of all successors is
greater than or equal to the best seen so far.

121

C OLES & S MITH

In practice, EHC guided by the RPG heuristic is an effective search strategy in a number of
domains. Work has been done (Hoffmann, 2005) on analysing the topology of the local-search
landscape to investigate why it is an effective heuristic, as well as identifying situations in which it
is weak.
2.4 Exploiting the Structure of a Relaxed Plan
The actions in the relaxed plan to the goal from a given state can be used to provide further search
advice. YAHSP (Vidal, 2004), a planner that produced interesting results in the Fourth International
Planning Competition (IPC 4), makes use of the actions of the relaxed plan to suggest actions to add
to the current plan to reach the goal. In FF, the notion of ‘helpful actions’ is defined—those that add
a fact added by an action chosen at the first time unit in the relaxed plan. In each state encountered
during search, a number of actions are applicable, some of which are irrelevant; i.e. they make no
progress towards the goal. By only considering the helpful actions when determining the successors
to each state, when performing EHC, the number of successor states to be evaluated will be reduced.
Restricting the choice of actions to apply only to those that are ‘helpful’ further reduces the
completeness of EHC, beyond what would be the case if all applicable actions were considered. In
practice it is observed, however, that the cases where EHC using only helpful actions is unable to
find a plan correlate with the cases where EHC with all the applicable actions would be unable to
find a plan.
2.5 Guaranteeing Completeness in FF
FF first attempts to search for a solution plan by performing Enforced Hill-Climbing (EHC) search
from the initial state towards the goal state. As discussed earlier, EHC uses hill-climbing local
search guided by the RPG heuristic while a strictly-better successor can be found. As soon as no
strictly better successor can be found, FF has entered a plateau, and breadth-first search is used
until an improving state is found. In directed search spaces, EHC can lead the search process in the
wrong direction and to dead-ends; i.e. the open list is empty, but no goal state has been found. In
these cases FF resorts to best-first search from the initial state, thereby preserving completeness.
2.6 Macro-Actions in Planning
A macro-action, as used in planning, is a meta-action built from a sequence of action steps. In
forward-chaining planning, applying a macro-action to a state produces a successor corresponding to the application of a series of actions. In this way, the use of macro-actions can be thought
of as extending the neighbourhood of successors visible from each state to selectively introduce
states which hitherto would only have been visible after the application of several steps. If the additional states introduced are chosen effectively, an increase in planner performance can be realised;
whereas if the additional states are chosen poorly, the performance of the planner decreases due to
the increased branching factor.
The use of macro-actions in planning has been widely explored. Most techniques use an off-line
learning approach to generate and filter macro-actions before using them in search. Early work on
macro-actions began with a version of the STRIPS planner—known as ABSTRIPS (Fikes & Nilsson, 1971)—which used previous solution plans (and segments thereof) as macro-actions in solving
subsequent problems. MORRIS (Minton, 1985) later extended this approach by adding some filter122

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

ing heuristics to prune the generated set of macro-actions. Two distinct types of macro-actions were
identified in this approach: S-macros—those that occur frequently during search—and T-macros—
those that occur less often but model some weakness in the heuristic. Minton observed that the
T-macros, although used less frequently, offered a greater improvement in search performance. The
REFLECT system (Dawson & Siklossy, 1977) took the alternative approach of forming macroactions based on preprocessing of the domain. All sound pairwise combinations of actions were
considered as macro-actions and filtered through some basic pruning rules. Due to the small size of
the domains with which the planner was reasoning, the number of macro-actions remaining following this process was sufficiently small to use in planning.
More recent work on macro-actions includes that on Macro-FF (Botea, Enzenberger, Muller,
& Schaeffer, 2005). Macro-actions are extracted in two ways: from solution plans; and by the
identification of statically connected abstract components. An offline filtering technique is used
to prune the list of macro-actions. Another recent approach to macro-action generation (Newton,
Levine, & Fox, 2005) uses a genetic algorithm to generate a collection of macro-actions, and then
filters this collection through an offline filtering technique similar to that used by Macro-FF.

3. Marvin’s Search Behaviour
Marvin’s underlying search algorithm is based on that used by FF: forward-chaining heuristic search
using the RPG heuristic. However, Marvin includes some important modifications to the basic FF
algorithm. These are: a least-bad-first search strategy for exploring plateaux, a greedy best-first
strategy for searching when EHC fails and the development and use of plateau-escaping macroactions.
As in FF the first approach to finding a solution plan is to perform EHC search using only helpful
actions. The first successor with a heuristic strictly better than the best so far is taken, should one be
found. If one is not found, then a plateau has been encountered, and a form of best-first search using
helpful actions is used (instead of the breadth-first search of FF) to try to find an action sequence to
escape from it. Because the states on a plateau can never improve on the heuristic value of the node
at the root of the plateau, we call this least-bad-first search.
If the EHC approach is unable to find a plan, Marvin resorts to a modified form of best-first
search using all the actions applicable in each state. This expands the first strictly better successor
whilst keeping the current state for further expansion later if necessary. We call this strategy greedy
best-first search. As can be seen in the graphs in Section 6.2, in some of the IPC 4 domains our
modifications enable the solution of problems that are unsolvable for best-first search.
During the EHC search strategy, Marvin uses plateau-escaping macro-actions learned from
previous searches of similar plateaux. These can be applied in the same way as atomic actions to
traverse plateaux in a single step. Plateau-escaping macro-actions are learned online and the planner
must decide which ones are likely to be applicable at which points during search. In Section 6.5
we show that plateau-escaping actions can yield performance benefits. Their power depends on the
structure of the search space and the ability of the planner to learn re-usable macro-actions.
Least-bad-first search on plateaux, greedy best-first search and plateau-escaping macro-actions
are the three main features of Marvin distinguishing its basic search strategy from that of other
forward heuristic search-based planners. We now discuss these three features in more detail before going on to describe how they can be exploited in the context of ADL domains and domains
involving derived predicates.
123

C OLES & S MITH

5
6

5
5

5
6
5
5

6

4
4

4
4

3

4

Figure 2: Least-bad-first search versus breadth-first search on a plateau. Black nodes are those
expanded by breadth-first search. Dotted blue/grey nodes are those expanded by both
breadth-first and least-bad-first search. Solid blue/grey nodes are those expanded by only
least-bad-first search. It can be seen that least-bad-first search leads to a better exit node
than does breadth-first search.

3.1 Least-Bad-First Search on Plateaux
A plateau is encountered when all of the successor nodes of a given current node have a heuristic
value that is the same as, or worse than, that of the current node. The notion of best in this context
relates to the successor with the heuristic value closest to that of the parent state. This is called
least-bad-first search because no chosen successor can make heuristic progress, but some choices
are less negative than others. The successor chosen in least-bad-first search will have least negative
impact on the current state and therefore is more likely to be on the best path to the goal. When
breadth-first search is used, the exit state that is reached might be further from the goal than the exit
state reached when the state with the least negative impact is always expanded next.
In Figure 2 we show the order in which states are explored using least-bad-first search relative
to breadth-first search. It can be observed that, using least-bad-first search, the exit state reached
has a better heuristic value than that reached using the breadth-first search in FF. It can be expected
that this sometimes leads to better quality plans being found. Our results in Section 6.3 show that,
indeed, using least-bad-first search we sometimes find shorter plans than FF finds using its standard
breadth-first strategy on plateaux.
3.2 Greedy Best-First Search when EHC Fails
As in FF, Marvin resorts to best-first search if EHC proves unable to find a solution. This approach
maintains the completeness of the planner in the cases where the use of EHC with helpful actions
would otherwise render the search incomplete. Two other planners in IPC 4 used variations on
the best-first search algorithm, YAHSP (Vidal, 2004) and Fast-Downward (Helmert, 2006). Unlike
Marvin, however, in these two planners there is no incomplete search step (such as EHC) before
using a modified best-first search algorithm. In YAHSP, conventional WA* search is used but within
the search queue, states reached by applying a helpful action in their parent state are ordered before
those which were not. In Fast-Downward, a ‘deferred heuristic evaluation’ strategy is used, where
124

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

states are inserted into the search queue with the heuristic value of their parent state; the actual
heuristic cost of the state is then only calculated when the state is expanded.
In Marvin the best-first strategy is modified by greedily expanding the first successor found with
a better heuristic than its parent state, but retaining the parent so that its remaining children can be
evaluated later if necessary. The effect of this is similar to the approach taken in Fast-Downward,
and would lead to the nodes in the search space being visited in the same order. The approach taken
in Marvin, however, allows a smaller search queue to be maintained, as nodes are not necessarily
inserted into the search queue for each successor node reachable from a state.
Whenever a successor state is generated and evaluated (by calculating its heuristic value), one
of two things happens:
• If the successor has a heuristic better than its parent, the successor is placed at the front of
the search queue, with its parent state behind it (along with a counter variable, noting how
many successors have already been evaluated); and the search loop is then restarted from the
successor state.
• If the successor has a heuristic no better than its parent, the successor is inserted into the
search queue in its appropriate place (stable priority-queue insertion, ordered by heuristic
value). The process then carries on evaluating the successors of the parent state.
The pseudo-code for this can be seen in Figure 3. The approach is inspired by the idea of taking
the first strictly-better successor when performing EHC search, with the benefit that the number of
heuristic evaluations to be performed is potentially reduced by considering fewer successors to each
state. It differs from EHC in that, to maintain completeness, the parent state is not discarded—it is
placed back in the queue to have its other successors evaluated later if necessary. Theoretically, if
EHC search on a given problem does not encounter any plateaux, and any pruning from selecting
only the helpful actions is ignored, then using greedy best-first search on that problem would visit
the same number of nodes and evaluate the same number of successors. If a plateau was encountered, however, the search behaviour would differ as EHC would only consider states reachable
from the state at the start of the plateau.
Another effect of the greedy best-first search is that the search focusses on exploring in a given
direction. As has been described, as soon as a successor node is found with a heuristic value better
than that of its parent, then the further expansion of the parent node is postponed and the successor
node is expanded. The practical consequence of this is that as the search queue does not contain
the other equally good successors, any search forward from a successor state will not be sidetracked
by also having to search forward from its sibling states. The parent node will be re-visited, and
the other sibling nodes added, but only if it proves heuristically wise to do so—that is, if searching
forward from the successor node is not making heuristic progress.
3.3 Plateau-Escaping Macro-Actions
Due to the nature of the relaxed problem used to generate the RPG heuristic there are aspects of
the original problem that are not captured. Thus, when the RPG heuristic is used to perform EHC,
plateaux are often encountered. On plateaux, the RPG heuristic value of all successor states is
the same as, or worse than, the heuristic value of the current state. The nature of the plateaux
encountered, and whether EHC is able to find a path to escape from them, is influenced by the
properties of the planning domain (Hoffmann, 2001).
125

C OLES & S MITH

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:

Procedure: GreedyBFS
insert (state=initial state, h=initial heuristic, counter=0) into search queue;
while search queue not empty do
current queue entry = pop item from front of search queue;
current state = state from current queue entry;
current heuristic = heuristic from current queue entry;
starting counter = counter from current queue entry;
applicable actions = array of actions applicable in current state;
for all index ?i in applicable actions ≥ starting counter do
current action = applicable actions[?i];
successor state = current state.apply(current action);
if successor state is goal then
return plan and exit;
end if
successor heuristic = heuristic value of successor state;
if successor heuristic < current heuristic then
insert (current state, current heuristic, ?i + 1) at front of search queue;
insert (successor state, successor heuristic, 0) at front of search queue;
break for;
else
insert (successor state, successor heuristic, 0) into search queue;
end if
end for
end while
exit - no plan found;

Figure 3: Pseudo-code for greedy best-first search

126

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

Ignoring the delete effects of the pickup action in the Gripper domain creates a problem in
which, in a given state, it is possible to pick up many balls using one gripper, so long as the gripper
is initially available: the delete effect of the action, marking the gripper as no longer available, is
removed. The relaxed plan in the initial problem state is pickup all the balls with one gripper,
move to the next room, then drop them all. The length of this plan, the heuristic value of the
initial state, is n + 1 + n, that is 2n + 1 (where n is the number of balls). If, in the initial state, a ball
is picked up using one of the grippers, the relaxed plan for the resulting state will be to pickup the
remaining balls in the other gripper, move to the second room and then drop them all; this has a
length of (n − 1) + 1 + n, that is 2n, which is less than the heuristic value of the initial state so this
action will be chosen as the one to apply.
The next state, however, is at the start of a plateau. The actions applicable (those for which
all the preconditions are satisfied) are either to drop the ball that has been picked up, pickup
another ball or move to the next room. The ‘correct’ action would be to pickup another ball: the
relaxed plan to the goal state for the resulting state would be to drop one of the balls, pickup
all the remaining balls in the newly-freed gripper, move to the next room, and drop all the balls.
However, the heuristic value of this state would be 1 + (n − 2) + 1 + n, or 2n, the same value
as the state in which the action is applied. Moving to the next room would produce a state with
the heuristic value of 2n (move to the initial room, pickup remaining (n − 1) balls, drop all
balls in the final room—no move action is required to move back to any room the robot has already
visited). Dropping one of the balls would also produce a state with a heuristic value of 2n (pickup
all remaining (n − 1) balls in newly-freed gripper, move to next room, drop all balls). As all
successor states have the same RPG heuristic value as their parent state, the heuristic is unable to
provide useful guidance as to which action to apply.
With some exhaustive search forward from this point, an improvement in heuristic value can
be made in two ways: either move to the next room then drop a ball, or pickup a ball then
move to the next room—both of these lead to heuristic values of (2n − 1). The plateau will,
however, be encountered each time the robot is in the first room, holding one ball, and the action
choices are either to pickup another ball or move to the next room (or drop a ball). Each time
the plateau is encountered, the action sequence to escape the plateau is identical—move-drop or
pickup-move (in EHC the actual sequence chosen will depend on the order in which the actions
are considered by the planner). Having to discover one of these action sequences by exhaustive
search each time the plateau is encountered is a considerable bottleneck in the search process: this
is true in general for many domains.
In order to address the overhead caused by recurrent plateaux in the search space, Marvin memoises the action sequences used to escape the previously encountered plateaux; these action sequences are used to form what are called ‘Plateau-Escaping Macro-Actions’. A macro-action is
generated from the action sequence using the code presented in Figure 4. Each step of the action
sequence is considered in turn, and an abstract action step is made for it by replacing the entities
given as parameters to the action with placeholder identifiers—one for each distinct entity. These
placeholder identifiers then form the parameter list of the macro-action; and the recorded abstract
action steps dictate the component actions from which the macro-action is built.
Returning to the pickup-move action sequence, the action sequence:
0: pickup robot1 ball2 room1
1: move robot1 room1 room2
127

C OLES & S MITH

would form a macro-action:
pickup-move (?a - robot) (?b - ball) (?c - room) (?d - room)
0: pickup ?a ?b ?c
1: move ?a ?c ?d
This macro-action can then be instantiated by specifying the parameters ?a to ?d, resulting in a
sequence of actions. For example, (pickup-move robot1 ball3 room1 room2) would give
an action sequence:
0: pickup robot1 ball3 room1
1: move robot1 room1 room2
In Marvin, the preconditions of the steps within the macro-action are not collected to give a
single precondition formula for the macro-action. Instead, an instantiated macro-action is said to
be applicable in a given state if the first component action of the macro-action is applicable, and
subsequent actions are applicable in the relevant resulting states.
Having now built macro-actions from the plateau-escaping action sequences, when the search is
later attempting to escape a plateau, these macro-actions are available for application. If the plateau
arose due to the same weakness in the heuristic that led to an earlier plateau, then a macro-actions
will be able to lead the search to a strictly better state by skipping over the intermediate states. The
plateau-escaping macro-actions are only used when the search is attempting to escape a plateaux—
this avoids slowing down search when the RPG heuristic is able to provide effective guidance using
only single-step actions.
To reduce the number of macro-actions considered, and the blow-up in the size of the explored
search space that would otherwise occur, the only macro-actions considered are those containing
actions at the first time step that are helpful actions in the current state.
3.4 Macro-Actions in Use
The structure and reusability of macro-actions depends on the underlying topology of the problem
space under the given heuristic function. When a problem space contains many similar occurrences
of the same plateaux (which happens when a problem contains much repeating structure) the effort
involved in learning macro-actions to escape these plateaux efficiently can be richly rewarded. In
principle, the most benefit is obtained when the problem space features large, frequently recurring
plateaux, since large plateaux are the most time-consuming to explore and the effort would need
to be repeated on every similar occurrence. Short macro-actions (of two or three actions) indicate
that the problem space contains small plateaux (although these might arise frequently enough for
learned macro-actions to still be beneficial).
Problems with repeating structure include: transportation problems, where the same basic sequences of actions have to be repeated to move groups of objects from their sources to their destinations; construction problems, in which many similar components need to be built and then combined
into a finished artifact; and configuration problems, in which multiple components of an architecture
need to go through very similar processes to complete their functions, etc. The Dining Philosophers
and Towers of Hanoi problems are good examples of problems with repeating structure.
Although using macro-actions during search has advantages—they can offer search guidance
and allow many actions to be planned in one step—considering them during the expansion of each
128

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:

Procedure: BuildMacro
parameters = [];
parameter types = [];
abstract steps = [];
parameter count = 0;
for all action ?a in the action sequence used to escape a plateau do
abstract parameters = [];
for all parameter ?p of ?a do
if ?p ∈ parameters then
index = parameter index of ?p in parameters;
append (index) to abstract parameters;
else
parameters[parameter count] = ?p;
parameter types[parameter count] = type of ?p;
append (parameter count) to abstract parameters;
increment parameter count;
end if
end for
append (action type of ?a, abstract parameters) to abstract steps;
end for
return parameter types and abstract steps as a macro-action

Figure 4: Pseudo-code for building macro-actions from plan segments

129

C OLES & S MITH

state increases the branching factor. Thus, if a large number of unproductive macro-actions are
generated the search space will become larger, making the problem harder, not easier, to solve.
Whilst many of the plateau-escaping sequences are helpful in planning, some are specific to the
situation in which they were derived, a situation which might not occur again in the plan. As macroactions are learnt during the planning process—and there is no human intuition, or large test suite,
to allow reusable macro-actions to be identified—care must be taken when deciding the points at
which to consider their use in the planning process.
Plateau-escaping macro-actions are generated from situations in which the heuristic has broken
down; therefore, the heuristic can be used as an indicator of when they are likely to be useful again
during planning. As areas of repeating structure within the solution plan involve the application
of similar (or identical) sequences of actions, they are likely to have similar heuristic profiles. In
the case of plateau-escaping action sequences, the heuristic profile of the search landscape at their
application is an initial increase (or no-change) of heuristic value, eventually followed by a fall to
below the initial level—the profile occurring at a local minimum. If the plateau-escaping macroactions are to be reusable, it is likely that the re-use will occur when the planning process is in a
similar situation. As such, they are only considered for application in the exhaustive search step
used to escape plateaux (both at the start or at any other point on a plateau).
Situations may arise where the use of macro-actions increases the makespan of the resulting
plan due to redundant action sequences. For example, if in a simple game domain—with actions
to move up, down, left or right— a macro-action is formed for ‘left, left, left, left’ and the optimal
action sequence to escape a given plateau is ‘left, left, left’ then ‘{left, left, left, left}, right’ may be
chosen if the state reached by moving left four times is heuristically better than the one reached by
applying a single-step ‘left’ action. Thus, macro-actions can have an adverse effect on plan quality.
Within the problem domains presented in IPC 4 (Hoffmann & Edelkamp, 2005) was the encoding of the Dining Philosophers problem, translated from Promela into a PDDL encoding. When
solving this problem, two important macro-actions are formed: an eleven-step macro-action upon
completion of the first period of exhaustive search; and a three-step macro-action upon completion
of the second. The solution plan requires these macro-actions to be repeated many times, something which now—as a result of the macro-actions—involves simply applying a single action that
results in a strictly better state. Without the macro-actions, the planning process consists of repeated
episodes of exhaustive search to find the same two sequences of actions each time.
This behaviour can be seen in Figure 5 depicting the heuristic values of states generated with
and without macro-actions, across the solution plan for the IPC 4 Dining Philosophers problem
involving 14 philosophers. Initially, no macro-actions have been learnt so the search done by both
approaches is identical. For the first 14 action choices the value of the heuristic, shown by the line
in the graph, moves monotonically downwards as the planner is able to find actions to apply that
lead to strictly better states.
After time step 14, the heuristic value begins to oscillate, at this point the planner has reached a
plateau: there is no state with a strictly better heuristic value that can be reached by the application
of just one action. As this is the first plateau reached, no macro-actions have been generated so
the heuristic profiles are identical for both configurations. At time step 25 a state is reached that
has a better heuristic value than that at time step 14. It is at this time that the plateau-escaping
macro-action will be generated, memoising a lifted version of the sequence of actions that was used
to escape the plateau. A brief period of search in which a strictly better state can be found at each
choice point follows before the planner again hits a plateau.
130

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

40

35

30

Heuristic

25

20

15

10

5
Without Macro-Actions
With Macro-Actions
0
0

20

40

60

80

100

120

Plan Time Step

Figure 5: Heuristic landscape over makespan, with and without macro-actions.
The subsequent six plateaux consist of applying the same sequence of actions to six further
pairs of philosophers; it can be seen that the heuristic fingerprints of the plateaux are identical.
The version of Marvin in which macro-actions have been disabled repeats the expensive exhaustive
search at each plateau: the heuristic value again goes through the process of increasing and then
decreasing again before reaching a strictly-better state. The version using the plateau-escaping
macro-actions, however, now has a single action to apply that achieves a strictly better state and
search continues, stepping over the subsequent plateaux through the selection of macro-actions that
yield strictly-better states.
When all of the larger plateaux have been overcome, a series of smaller plateaux are encountered. Again, it can be seen that for the first of these, both versions must complete a stage of
exhaustive search; however, after the first of the smaller plateaux has been completed, the macroaction formed allows the subsequent plateaux to be bypassed. Finally, the plan finishes with a short
previously unseen sequence of actions, where both versions must do exhaustive search.

4. Handling ADL
PDDL (McDermott, 2000) (the Planning Domain Definition Language) was first defined for use in
the First International Planning Competition (IPC 1) at AIPS-98. Over the subsequent competitions,
modifications have been made to the language as planning technology has evolved.
In the first three competitions, domains were available in which only STRIPS (Fikes & Nilsson,
1971) actions were used. STRIPS actions have conjunctive predicate preconditions, add effects, and
delete effects defined in terms of action schema parameters and constant entities. To determine the
preconditions and effects of a given ground action instance (an action whose parameters have been
bound to specific entities) the action’s parameters are substituted into the schema. For the action to
be applicable in a given state, all of the precondition predicates must hold in that state; if the action
is applied, a new state is generated from the previous state by removing all the predicates present in
the delete effect list and adding those in the add effect list..
131

C OLES & S MITH

ADL action schemata (Pednault, 1989) extend the syntax of STRIPS action schemata. In ADL
domains the language used to describe the preconditions of an action is extended to allow disjunctive, quantified and negative preconditions as well as the conjunctive preconditions that can be used
in STRIPS domains. The syntax for describing the effects of actions is also extended to allow
conditional effects—effects which are applied whenever a given condition holds.
The extended syntax provided by ADL not only increases the convenience with which a domain
can be encoded, but can also reduce the size of the domain descriptions needed. For example, if
an action schema has, as a precondition (or A B C) then, without ADL, three copies of the action
schema would need to be made: one with a precondition (A), one with a precondition (B) and one
with a precondition (C). If one is willing to tolerate such increases in domain-description size, and
the number of objects in the domain is finite, it is possible to compile a given ADL domain and
problem-instance pair into a domain-problem pair containing only STRIPS actions: in general, this
compilation must be done once for each problem instance, not just once for each ADL domain. The
ability to compile ADL domains into STRIPS domains was first demonstrated by the compilation
procedure devised by Gazen and Knoblock (1997). Using these techniques in a preprocessing stage,
FF is able to handle domains containing ADL actions whilst only reasoning about STRIPS actions
internally. The output from FF’s preprocessor stage was made available in IPC 4 to allow planners
which could not handle ADL directly to solve compiled STRIPS formulations of the problems
by loading a compiled domain-problem pair for each of the original problem instances in a given
domain.
Whereas in previous competitions the ADL domains were simplified or manually reformulated
to produce STRIPS domains, the STRIPS domains in IPC 4 were compiled automatically from
ADL. The compilation used, based on the preprocessor of FF, results in a compiled domain-problem
pair for each original problem instance. This compilation explicitly grounds many of the original
actions, producing one compiled action schema (with preconditions and effects whose parameters
refer to PDDL constants) per ground action that could arise in the original ADL problem. Whilst
these compilations produce STRIPS domains that allow planning to be performed, they replace
general action schemata with sets of specific action instances.
To allow the new features in Marvin to be used in the competition, Marvin was extended to
include native support for ADL. By reasoning with the original ADL domain it is able to effectively
abstract macro-actions from action sequences.
4.1 The Preconditions of ADL Actions
The preconditions of STRIPS actions consist of one structural feature - an ‘and’ clause, allowing
conjunctive preconditions and predicates with constant or parameterised bindings. ADL actions
have a far greater range of structural features in their preconditions. They allow ‘or’, ‘imply’,
‘not’, ‘forall’ and ‘exists’, which can be combined in any well-formed manner. In Marvin, the ADL
preconditions are processed using two steps. First, all quantified preconditions are fully enumerated.
Second, the resulting precondition formula is translated into Negation Normal Form (NNF) using
the standard procedure: by replacing (a ⇒ b) with (¬a ∨ b), and using De Morgan’s laws to
eliminate negations of clauses. Further reductions are then applied to eliminate redundancy, such as
replacing (and A (and B C)) with (and A B C), and (or A (or B C)) with (or A B C).
Internally, within Marvin, the NNF precondition formula forms the basis of a ‘satisfaction tree’,
the nodes of which are the ‘and’ and ‘or’ elements of formula and the literals (negated and non132

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

negated) form the leaves. The structure of the satisfaction tree for a given action schema is fixed,
although the propositions at the leaves vary between groundings.
To determine which ground ADL action instances are applicable in a given state based on their
preconditions, the algorithm shown by the pseudo-code fragment in Figure 6 is used. Initially, the
satisfaction counters associated with each ground action’s satisfaction tree nodes are reset using the
following rules:
• Each ‘and’ nodes has its counter set to denote the number of children it has.
• Each ‘or’ node has its counter set to 1.
• Negative preconditions are assumed to be true, and the satisfaction counters of their parents
decremented accordingly.
As these values are state-independent, for reasons of efficiency the values used to reset the satisfaction counters are computed once and cached for later use.
Having reset the satisfaction counters, each proposition in the current state is considered, and
the satisfaction trees updated accordingly:
• The satisfaction counters of parent nodes that have the current proposition as a negative precondition are incremented.
• The satisfaction counters of parent nodes that have the current proposition as a positive precondition are decremented.
Then, by propagating the effects of truth value changes upwards through the tree, any action
whose root node has sufficiently many children satisfied is applicable.
4.2 The Effects of ADL Actions
ADL extends the action definitions of STRIPS actions by allowing quantified and conditional effects. As in preconditions, the former are dealt with by enumeration; the latter are dealt with depending on their conditions.
If a conditional effect is dependent only on static predicates it is possible to determine when
grounding an action whether or not it applies for that instance: the static information does not
change from state to state. If the effect depends on dynamic predicates, it is necessary to consider,
in each state, whether the effect applies. To achieve this, the effect and its conditions are used
to form a sub-action. The sub-action has the conditional effect’s condition as its preconditions,
and the conditional effect itself as its effects. As conditional effects can be nested in the original
operator schemata, the sub-actions themselves may also have conditional effects; in which case the
sub-action-creation step is applied recursively, creating nested sub-actions as necessary.
The applicability of ground sub-actions in a given state is performed in the same manner as
normal actions. When an action is applied, any sub-actions that are also applicable are applied
alongside it, thereby preserving the conditional effects of the original operator.
4.3 Modifying the Relaxed Planning Graph
It is necessary to modify the Relaxed Planning Graph expansion and plan-extraction phases to make
it possible to apply the heuristic when the domain contains ADL actions. Work has been done on
133

C OLES & S MITH

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:
45:

Procedure: test-action-applicability
reset satisfaction counters();
for all predicate ?p in the state do
for all (ground action ?a, tree node ?c) pair having ?p as a negative precondition child node do
tree node to update = ?c;
while tree node to update is still valid do
old value = value in tree node to update;
value in tree node to update = old value + 1;
if value in tree node to update > 0 && old value = 0 then
tree node to update = parent of tree node to update;
else
tree node to update = invalid;
end if
end while
end for
for all (ground action ?a, tree node ?c) pair having ?p as a positive precondition child node do
tree node to update = ?c;
while tree node to update is still valid do
old value = value in tree node to update;
value in tree node to update = old value -1;
if value in tree node to update = 0 && old value > 0 then
tree node to update = parent of tree node to update;
else
tree node to update = invalid;
end if
end while
end for
end for
applicable actions = ∅;
for all ground action ?a do
if root tree node is satisfied then
add ?a to applicable actions;
end if
end for

Figure 6: Pseudo-code for action applicability testing

134

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

extending full graphplan planning graphs to reason with a subset of ADL actions (Koehler, Nebel,
Hoffmann, & Dimopoulos, 1997); the approach taken in Marvin extends the relaxed planning graph
structure to handle all of the available ADL constructs. The effect of the modifications is that the
same heuristic estimate is obtained as if a precompiled STRIPS domain formulation was used.
When building a conventional relaxed planning graph the assumption is made that, in the first
fact layer, all the facts present in the state to be evaluated are true and all other facts are, implicitly,
false. Facts are then gradually accumulated by the application of actions, add effects adding facts to
the spike (Long & Fox, 1999). Actions become applicable when their preconditions are all present;
i.e. they have all been accumulated. The STRIPS actions used to build a conventional relaxed
planning graph necessarily have no negative preconditions, so it is sufficient to consider when facts
have a positive truth value and determine action applicability from this. ADL actions, however, can
also have negative preconditions, corresponding to facts which must be false. Within a conventional
relaxed planning graph, no record is made of whether it is possible for a given fact to have a negative
truth value.
To handle negative facts within the relaxed planning graph used in Marvin, a second spike is
added. As with the positive-fact spike, all the facts present in the state to be evaluated are true and
all other facts are, implicitly, false. However, unlike the positive-fact spike, facts are then gradually
eroded by the applications of actions; with their delete effects marking the fact in the negativefact spike as having been deleted. The inherent relaxation on which the relaxed planning graph
is founded is still preserved, though: delete effects have no effect on the positive-fact spike; and,
similarly, add effects have no effect on the negative-fact spike.
If a precompiled STRIPS domain formulation was used, additional complimentary propositions
are added to denote when each proposition is not true. These accumulate alongside the original
domain propositions, and in this way are able to satisfy negative preconditions. The negative fact
spike, as discussed, has the same effect, although rather than recording which propositions are
available in a negated form at each layer, it records which propositions are not available in a negated
form.
As discussed, ADL action preconditions are preprocessed such that negation is only applied
to the leaves of the satisfaction tree; i.e. only applied to unit facts forming part of the actions’
precondition structures. Within the relaxed planning graph a given fact leaf can now be marked as
satisfied if either one of the following holds:
• It is a positive fact leaf, and the fact contained therein has been added to the positive-fact
spike.
• It is a negative fact leaf, and the fact contained therein has either never been in the negativefact spike or has since been marked as deleted.
Plan graph construction proceeds in a manner similar to that used to build a conventional relaxed
planning graph. Each of the newly present or newly deleted facts are considered in turn, and their
effects on the applicability of all available actions noted. Should the updating of the satisfaction tree
of an action lead to it becoming applicable:
• The action is added to the action spike, available at the next fact layer.
• Previously unseen add effects are added to the positive-fact spike, available at the next fact
layer.
135

C OLES & S MITH

• Delete effects deleting a fact still present in the negative-fact spike mark the fact as being
deleted and available to satisfy negative preconditions from the next fact layer.
For efficiency, the first action to achieve each fact is stored when it is added to the positive-fact
spike, along with the earliest layer at which that action is applicable. Similarly, the first action
that deletes each fact that has ever been in the negative-fact spike is noted. Relaxed plan extraction
consists of regressing through the layers of the relaxed planning graph, selecting actions that achieve
the goals that are to be achieved at each layer. Initially, each proposition in the goal state is added to
the layer of goals for the layer in which it first appears (or disappears, in the case of negative goals).
To extract a plan, the next goal is repeatedly taken from the deepest action layer with outstanding
goals. Its first achieving action is added to the plan and its preconditions, taken from its satisfaction
tree, are added to the goals for the first layer in which they appear. The process finishes when there
are no more outstanding goals at any layer. If a sub-action (that is, an action created to represent
the conditional effect of an ADL action, see Section 4.2) is chosen to achieve a given proposition,
the preconditions of its parent action(s) are also added to the goals for the first layer in which they
appear.
When considering adding the preconditions of an achieving action to the layer in which they
appear, a collection of disjunctive preconditions may arise. In this situation, the first satisfied precondition or negative precondition in the disjunction is added as a subgoal in an earlier layer. This
avoids adding many redundant actions to satisfy each of a the disjunctive preconditions, where only
one needs to be satisfied. The precondition chosen to be satisfied from each collection of disjunctive preconditions is the first for which an achiever was found when building the relaxed planning
graph, thus providing the same heuristic estimate as if the compiled STRIPS domain formulation
was used. In the compiled STRIPS domain formulation, the disjunctive precondition would give
rise to several action instantiations; the first applicable of these would be chosen as the achiever for
the desired fact.
At the start of the planning process, a relaxed planning graph is constructed forward from the
initial state. However, rather than stopping when the goal literals appear, graph construction stops
when no more ground actions become applicable. The actions and propositions appearing in this
relaxed planning graph are a superset of all the actions and propositions appearing in later relaxed
planning graphs: these actions and propositions discovered are then used to form a cache detailing
the proposition–action dependencies. Using this cached information, the code shown in Figure 6
can be used to determine the actions applicable in a given state, and the relaxed planning graphs
used to calculate heuristic values can be extracted more efficiently.

5. Handling Derived Predicates
In IPC 4, PDDL was further extended with the addition of Derived Predicates (Hoffmann &
Edelkamp, 2005). Derived Predicates, used in three of the competition domains, allow higherlevel concepts to be recursively derived from other propositions. These derived predicates can then
be present in the preconditions of actions, and allow higher-level concepts in the domain to be reasoned with. For example, in the BlocksWorld domain, the derivation rule for the ‘above’ predicate
is as follows:
(:derived (above ?x ?y)
(or (on ?x ?y) (exists (?z) (and (on ?x ?z) (above ?z ?y)))))
136

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

Should a planner not include native support for derived predicates, it is possible to compile
domains containing derived predicates into “flattened” domains that do not. However, it is not possible to do this without a super-polynomial increase in the size of the domain and the solution plan
(Nebel, Hoffmann, & Thiebaux, 2003). At IPC 4, compiled versions of the domains that contained
derived predicates were made available for competitors who could not support derived predicates.
However, the sizes of the problems that could be compiled were restricted by the concomitant sizes
of the PDDL files produced by the compilation process and the computational effort necessary to
solve the compiled problems.
IPC 4 was the first planning competition to make use of derived predicates in its domains. As
it has been shown that derived predicates cannot be reasoned about efficiently through compilation
(Nebel et al., 2003) steps were taken to provide native support for them in Marvin.
It is also possible to compile derived predicates appearing in domains by adding actions to
instantiate the derived predicates on an as-needed basis (Gazen & Knoblock, 1997). Using this
compilation, the ‘above’ derivation rule from the blocksworld problem described above would be
compiled to the following action:
confirm above ?x ?y
pre: (or (on ?x ?y) (exists (?z) (and (on ?x ?z) (above ?y ?z))))
add: (above ?x ?y)
If this is to be used as a domain compilation, each of the original actions in the domain must be
extended to delete all of the ‘above’ propositions, forcing the confirm above actions to be used to
re-achieve the ‘above’ preconditions for any action that requires them. In this case, each action is
given the additional effect:
(forall (?x ?y) (not (above ?x ?y)))
Although effective in STRIPS domains, it is not possible to use such a compilation for domains
making use of negative preconditions as the re-derivation of derived predicates occurring as negative
preconditions of actions is not enforced. For example, an action could be applied that modifies the
‘on’ propositions, leading to a state from which a number of additional ‘above’ properties could
be derived. Deleting the ‘above’ propositions is a necessary step, as the confirm actions should
re-assert any derived predicate for any action that needs it. However, when (above ?x ?y) is deleted,
(not (above ?x ?y)) is true, and can be used as an action precondition. To deal with this issue it is
necessary to prevent any non-confirm actions from being applied until all possible derived predicates
have been re-asserted; this prevents actions from being applied when a given ‘not above’ is only
temporarily true, i.e. whilst it has not yet been re-derived. To force the re-derivation of derived
predicates, further dummy predicates and actions must be added to the domain. The necessary
compilation results in a large increase in the size of the search space explored, and the additional
dummy actions affect the usefulness of the relaxed-planning-graph heuristic.
The problems with using the Gazen & Knoblock compilation arise solely because, in its original
form, it does not force all applicable confirm actions to be applied after each original action is
applied. As such, if a planner generates the confirm actions internally and then deals with them
appropriately, the compilation can still form the basis of an effective means for handling derived
predicates.
137

C OLES & S MITH

To this end, when presented with a domain containing derived predicates, Marvin machinegenerates the confirm actions and extends each (original) action to delete the derived predicates, as
described. After each action is applied, all propositions that can be directly or recursively derived
from the resulting state are instantiated by applying all applicable confirm actions. Along with
avoiding an unwieldy compilation in domains with negative preconditions, handling the confirm
actions internally in this manner provides performance improvements for two further reasons:
• As the confirm actions are automatically applied when appropriate, Marvin does not have to
do search and perform heuristic evaluation to discover that the next action required will be a
confirm action.
• Confirm actions are included alongside normal actions in the relaxed planning graph built for
each state, but if used in the relaxed plan they do not contribute towards the heuristic value
taken from its length, eliminating any noise they would otherwise add.

6. Results
The planning competition offers a great opportunity for assessing the relative performance of various
techniques used in planning over a wide range of problems. Inevitably there will, however, be
features that are not tested by the set of domains used in the competition. There will also be some
domains in which many of the features of a planner collaborate to produce good results, rather than
the results being directly attributable to one individual feature. Here we discuss the results from the
competition and present further results to clarify which of the features of Marvin contribute to the
performance in each particular case.
It is important to note that when we refer to macro-actions generated and used by Marvin these
are all generated during the planning process for that specific problem. No additional learning time
or knowledge gained from solving other problems was used by Marvin in the competition, or in
producing the additional results presented in this paper. Although some planners can use additional
‘learning time’ when solving a series of problems, a satisfactory way to incorporate this extra time
into the time taken to solve each problem, as measured in the planning competition, has yet to
be found. In the planning competition the planners are compared based on their performance on
isolated problem instances, which is still an interesting comparison to make.
The results presented were produced on two machines: a machine at the University of Strathclyde (with a 3.4GHz Pentium 4 processor) and the IPC 4 competition machine (with a 3GHz Xeon
processor). In both cases, the planner was subjected to a 30 minute time limit and a 1Gb memory
usage limit. All results that are directly compared against each other (i.e. appear on the same graph)
are produced on the same machine. The domains used for evaluation are taken from IPC 3 and IPC
4, and are described in detail in the papers giving an overview of each of the two competitions (Long
& Fox, 2003; Hoffmann & Edelkamp, 2005).
6.1 Plateau-Escaping Macro-Actions
To assess the effect of plateau-escaping macro-actions on planner performance, tests were run across
a range of planning domains with macro-actions enabled and disabled. The results of these tests are
shown in Figures 7 and 8, illustrating the time taken to find a solution plan and the makespan of the
plan found respectively.
138

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING
Airport

Philosophers

1000

1000
With Macro-Actions
No Macro-Actions

100

100

10

10

Time (sec.)

Time (sec.)

With Macro-Actions
No Macro-Actions

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25
30
Problem Instance

35

40

45

50

5

10

15

20
25
30
Problem Instance

Depots

35

40

45

Driverlog

10000

1000
With Macro-Actions
No Macro-Actions

With Macro-Actions
No Macro-Actions

1000
100

Time (sec.)

Time (sec.)

100

10

10

1
1

0.1
0.1

0.01

0.01
5

10
Problem Instance

15

20

2

4

6

8

10
12
Problem Instance

Pipes Tankage Non-Temporal

14

16

18

20

Satellite

10000

1000
With Macro-Actions
No Macro-Actions

With Macro-Actions
No Macro-Actions

1000
100

Time (sec.)

Time (sec.)

100

10

10

1
1

0.1
0.1

0.01

0.01
5

10

15

20

25
30
Problem Instance

35

40

45

50

5

10

FreeCell

15
20
Problem Instance

30

35

Pipes No-Tankage Non-Temporal

10000

10000
With Macro-Actions
No Macro-Actions

With Macro-Actions
No Macro-Actions

1000

1000

100

100
Time (sec.)

Time (sec.)

25

10

10

1

1

0.1

0.1

0.01

0.01
2

4

6

8

10
12
Problem Instance

14

16

18

20

5

10

15

20

25
30
Problem Instance

35

40

45

50

Figure 7: CPU time showing the results of planning with and without plateau-escaping macroactions on a range of domains (from left to right: Airport, Philosophers, Depots, Driverlog, Pipestankage-nontemporal, Satellite, FreeCell, Pipesnotankage-nontemporal).

139

C OLES & S MITH
Airport

Philosophers

500

450
With Macro-Actions
No Macro-Actions

With Macro-Actions
No Macro-Actions

450

400

400

350

350
300
Makespan

Makespan

300
250

250
200

200
150
150
100

100

50

50
0

0
5

10

15

20

25
30
Problem Instance

35

40

45

50

5

10

15

20
25
30
Problem Instance

Depots

35

40

45

Driverlog

250

200
With Macro-Actions
No Macro-Actions

With Macro-Actions
No Macro-Actions
180

200

160
140

Makespan

120

Makespan

150

100

100
80
60

50

40
20

0

0
5

10
Problem Instance

15

20

2

4

6

8

10
12
Problem Instance

Pipes Tankage Non-Temporal

14

16

18

20

Satellite

250

450
With Macro-Actions
No Macro-Actions

With Macro-Actions
No Macro-Actions
400

200

350
300
Makespan

Makespan

150
250
200

100
150
100

50

50
0

0
5

10

15

20

25
30
Problem Instance

35

40

45

50

5

10

FreeCell

15
20
Problem Instance

25

30

35

Pipes No-Tankage Non-Temporal

250

250
With Macro-Actions
No Macro-Actions

With Macro-Actions
No Macro-Actions

150

150
Makespan

200

Makespan

200

100

100

50

50

0

0
2

4

6

8

10
12
Problem Instance

14

16

18

20

5

10

15

20

25
30
Problem Instance

35

40

45

50

Figure 8: Makespan of the solution plans found when planning with and without plateau-escaping
macro-actions on a range of domains (from left to right: Airport, Philosophers, Depots,
Driverlog, Pipestankage-nontemporal, Satellite, FreeCell, Pipesnotankage-nontemporal).

140

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

In the Airport domain the time taken to find plans and the makespans of the plans found were
almost identical. A strictly better successor can usually be found to each state when using EHC,
and it is clear in this domain that the addition of macro-actions from the occasional plateau has
not degraded the performance of the planner. The performance of the two configurations deviates
at problem 47, where planning with macro-actions was able to find a solution plan but planning
without macro-actions was not. Closer inspection of the output from the planner reveals that in this
case, some way into EHC search, a plateau is encountered and escaped; in the configuration using
macro-actions, this leads to the formation of a macro-action. Later in search, another plateau is
encountered. At this point, the earlier macro-action can be used to lead to a strictly better state,
from which a solution plan can ultimately be found using EHC. If the macro-action is not available,
however, the sequence of actions found to escape the plateau leads to a different exit point, from
which a solution plan cannot be found using EHC.
In the Philosophers domain neither the makespans of the plans found nor the coverage differs between the two configurations tested. Using macro-actions, however, leads consistently to improved
performance as the plateaux encountered during search require the application of the same action
sequence. Consistently, across the problems, searching with macro-actions is faster by a factor of
two; and furthermore, the factor is increasing with problem size, suggesting it has better scalability.
In the Depots domain, using macro-actions improves coverage, allowing 18 problems to be
solved within the time limit rather than 15. Further, in many cases, the time taken to find a plan is
reduced. In one case, problem file 6, planning without macro-actions is able to find a plan where
planning with macro-actions cannot. Here, planning without macro-actions is unable to find an exit
point from one of the plateaux encountered later in search, and resorts to best-first search. Planning
with macro-actions, however, is able to reach a greater number of successor states from the nodes
on the plateau and is unable to exhaust the reachable possibilities and terminate EHC search within
the 30-minute time limit.
In the Driverlog domain, using macro-actions generally increases the time taken to find plans
and has an adverse effect on the makespan. In this domain, macro-actions containing varying-length
action sequences consisting of repeated walk or drive actions are inferred. In practice, these are
detrimental in two ways: they have a large number of instantiations and dramatically increase the
branching factor, reducing performance; and they are only usefully reusable in situations where
the prescribed number of walk or drive actions are needed. Despite this, planning with macroactions is able to find solution plans in 18 of the problems, whereas planning without the macroactions is only able to solve 17 of the problems. In the problem in question, problem 17, the
increased number of successor states visible from the nodes on plateaux due to the presence of
macro-actions allows EHC to find a solution plan rather than resorting to best-first search, which
would ultimately fail within the time limit set.
In the Pipestankage-nontemporal domain, it is not clear at first whether macro-actions are beneficial or not. The number of problems solved by both configurations is the same, 34, and the impact
on makespan appears to be insignificant, improving it in some cases but making it worse in others.
However, looking at the harder problems from problem 25 upwards, planning with macro-actions
is able to solve 13 rather than 11 problems, suggesting it is able to scale better to larger problems
compared to searching without macro-actions.
In the Satellite domain both configurations exhibit similar performance, in terms of both the
time taken to find a solution plan and the makespan of the plan found, as the relaxed planning graph
heuristic is generally able to provide good search guidance. The exception is problem 36: here,
141

C OLES & S MITH

the inference of a macro-action allows search to be completed using EHC rather than resorting to
best-first search, reducing the time taken to find a plan.
In the FreeCell domain, macro-actions appear to lead to improved makespans and have negligible impact on the time taken to find solution plans. Intuitively, however, in a strongly directed
search space (such as that in FreeCell, where it is possible to move a card from one location to
another but often not to move it back) using a non-backtracking search strategy such as EHC should
reduce the effectiveness of macro-actions, as the introduction of redundant action steps as part of a
macro-action instantiations can lead search towards unpredicted dead-ends. The illustrated results,
contradicting this intuition, can be ascribed to the nature of the FreeCell problems used in IPC 3.
The problem files all have the four suits of cards, and from problem file 7 upwards have four free
cells. The number of cards in each suit and the number of columns are gradually increased from 2
to 13 and 4 to 8 respectively. The effect of this, however, is that all but the hardest problems have a
favourable free cells to cards ratio. When macro-actions are used, the impact of needlessly moving
a card into a free cell is not significant as there is a generous allocation of free cells compared to the
number of cards that might need to be stored there.
To provide a more reasonable test of whether macro-actions are beneficial in the FreeCell domain, twenty full-sized problem instances were generated and tests run to compare the performance
of Marvin with and without macro-actions on these problems. The results of these tests can be seen
in Figure 9 - clearly, the number of problems solvable within the 30 minute time limit and, generally,
the time taken to find a solution plan is improved when macro-actions are not used.
In the Pipesnotankage-nontemporal domain the results obtained do not show a significant advantage or disadvantage to using macro-actions: the planner is faster on some of the problems when
using macro-actions, but is slower on others; similarly, the planner produces plans with shorter
makespans on some problems when using macro-actions, but longer makespans on others. Two
results are obtained when macro-actions are not used that are very close to the 30-minute cut-off.
The first of these is solved in around 10 seconds when macro-actions are used; the second can be
solved using macro-actions if an extra 5 minutes of CPU time are allowed, or if a slightly faster
computer is used.
Overall, it can be seen that the effect of plateau-escaping macro-actions on the execution time
of the planner varies depending on the domain in question:
• In the Philosophers, Depots, Driverlog and Pipestankage-nontemporal domains, the use of
macro-actions improves the performance of the planner, either in terms of coverage or a
general reduction in the time taken to find solution plans.
• In the FreeCell domain, worse performance is observed when macro-actions are used.
• In the Airport, Pipesnotankage-nontemporal and Satellite domains the difference in performance is minimal.
Furthermore, with the exception of the Driverlog and FreeCell domains (where the makespan of
solution plans is generally increased when using macro-actions) the use of macro-actions does not
significantly affect the makespan.
6.2 Greedy Best-First Search versus Best-First Search
To assess how the performance of greedy best-first search compares to conventional best-first search,
we ran tests across a range of planning domains with EHC and macro-actions disabled to isolate the
142

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

FreeCell
10000
With Macro-Actions
No Macro-Actions
1000

Time (sec.)

100

10

1

0.1

0.01
5

10
Problem Instance

15

20

Figure 9: Time taken to solve twenty full-sized problems in the FreeCell domain, with and without
plateau-escaping macro-actions.

effect of the greedy best-first search approach. Overall, when analysing the results, it was observed
that the choice of best-first search algorithm had little impact on the performance of the planner.
6.3 Least-Bad-First Search versus Breadth-First Search
To assess the effect of using least-bad-first search rather than breadth-first search to escape plateaux
in EHC search, we ran tests across a range of planning domains using each of the two search
algorithms. The results of these tests are shown in Figures 10 and 11, illustrating the time taken to
find a solution plan, and the makespan of the plan found.
In the Airport domain, plateaux arise in one of two cases:
• An unforeseen deadend has been reached; as no backtracking is made over action choices
exhaustively searching the plateau is inexpensive, and EHC terminates rapidly.
• A short plateau has been reached, requiring two actions to be applied to reach a state with a
strictly better heuristic value—here, the two actions found by both least-bad-first and breadthfirst search were identical.
As can be seen from the planning time and makespan graphs, using least-bad-first search rather
than breadth-first search has no impact on planning time or solution plan quality in the Airport
domain: the time spent searching plateaux is negligible, and the escape paths found are identical
under the two plateau-search approaches.
143

C OLES & S MITH
Airport

Philosophers

1000

10000
Least-Bad-First Search
Breadth-First Search

Least-Bad-First Search
Breadth-First Search
1000

100

Time (sec.)

Time (sec.)

100
10

10

1
1

0.1
0.1

0.01

0.01
5

10

15

20

25
30
Problem Instance

35

40

45

50

5

10

15

20
25
30
Problem Instance

Depots

35

40

45

Driverlog

10000

1000
Least-Bad-First Search
Breadth-First Search

Least-Bad-First Search
Breadth-First Search

1000
100

Time (sec.)

Time (sec.)

100

10

10

1
1

0.1
0.1

0.01

0.01
5

10
Problem Instance

15

20

2

4

6

8

10
12
Problem Instance

Pipes Tankage Non-Temporal

18

20

10000
Least-Bad-First Search
Breadth-First Search

Least-Bad-First Search
Breadth-First Search
1000

1000

100

100
Time (sec.)

Time (sec.)

16

Satellite

10000

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25
30
Problem Instance

35

40

45

50

5

10

FreeCell

15
20
Problem Instance

25

30

35

Pipes No-Tankage Non-Temporal

10000

10000
Least-Bad-First Search
Breadth-First Search

Least-Bad-First Search
Breadth-First Search

1000

1000

100

100
Time (sec.)

Time (sec.)

14

10

10

1

1

0.1

0.1

0.01

0.01
2

4

6

8

10
12
Problem Instance

14

16

18

20

5

10

15

20

25
30
Problem Instance

35

40

45

50

Figure 10: CPU time showing a comparison between using breadth-first and least-bad-first search
on plateau search on a range of domains (from left to right: Airport, Philosophers,
Depots, Driverlog, Pipestankage-nontemporal, Satellite, FreeCell, Pipesnotankagenontemporal). These results were generated without using macro-actions.

144

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING
Airport

Philosophers

350

450
Least-Bad-First Search
Breadth-First Search

Least-Bad-First Search
Breadth-First Search
400

300
350
250

Makespan

Makespan

300
200

150

250
200
150

100
100
50
50
0

0
5

10

15

20

25
30
Problem Instance

35

40

45

50

5

10

15

20
25
30
Problem Instance

Depots

35

40

45

Driverlog

250

180
Least-Bad-First Search
Breadth-First Search

Least-Bad-First Search
Breadth-First Search
160

200

140
120
Makespan

Makespan

150
100
80

100
60
40

50

20
0

0
5

10
Problem Instance

15

20

2

4

6

8

10
12
Problem Instance

Pipes Tankage Non-Temporal

14

16

18

20

Satellite

250

300
Least-Bad-First Search
Breadth-First Search

Least-Bad-First Search
Breadth-First Search
250
200

200
Makespan

Makespan

150
150

100
100

50
50

0

0
5

10

15

20

25
30
Problem Instance

35

40

45

50

5

10

FreeCell

15
20
Problem Instance

25

30

35

Pipes No-Tankage Non-Temporal

250

140
Least-Bad-First Search
Breadth-First Search

Least-Bad-First Search
Breadth-First Search
120

200
100

Makespan

Makespan

150

100

80

60

40
50
20

0

0
2

4

6

8

10
12
Problem Instance

14

16

18

20

5

10

15

20

25
30
Problem Instance

35

40

45

50

Figure 11: Makespan of plans produced using breadth-first and least-bad-first search during
plateau search on a range of domains (from left to right: Airport, Philosophers,
Depots, Driverlog, Pipestankage-nontemporal, Satellite, FreeCell, Pipesnotankagenontemporal). These results were generated without using macro-actions.

145

C OLES & S MITH

In the Philosophers domain, search time is dramatically reduced by using least-bad-first search
rather than breadth-first search on plateaux. Using least-bad-first search, all 48 problems are solved;
using breadth-first search, only the first 14 are solved. The plans found in the first 14 have identical
makespans, although the actions occur in differing orders in the two plans.
The search landscape provides some insights into why least-bad-first search is suited to this
problem domain. At the start of the largest plateaux encountered, each action leads to a state with
a strictly worse heuristic value; each of these corresponds to applying the action ‘queue-write’ to a
philosopher. From each of these, a state with a less-bad heuristic is visible. When using least-badfirst search, this less-bad state is considered before the others in the queue, avoiding the redundant
search that would otherwise be performed by breadth-first search. Adding more philosophers to the
problem causes a dramatic increase in the amount of redundant search performed when breadth-first
search is used, leading to the observed performance improvement when a least-bad-first approach
is taken.
In the Depots domain, we can observe the effect of differing exit points to plateaux when using
least-bad-first and breadth-first search. When solving problem 18, least-bad-first search is able to
solve the problem in substantially less time: EHC search is able to escape all the plateau encountered, and find a solution plan. Breadth-first search, however, leads to the termination of EHC, and
exhaustive best-first search being used. On problem 15, however, breadth-first search is able to find
a solution plan where least-bad-first search cannot; also, problem 5 is solved in much less time. In
these two cases, it is not the success of breadth-first search on plateaux which leads to the improved
performance, but its failure; EHC search terminates and resorts to best-first search in less time when
breadth-first search is used than when least-bad-first search is used.
In the Driverlog domain, one additional problem, number 18, can be solved when least-badfirst search is used instead of breadth-first search. EHC using breadth-first search leads to a plateau
which cannot be escaped, and EHC aborts without a solution plan; the resulting exhaustive best-first
search cannot be completed within the allowed 30 minutes. The makespans of the plans found by
the two approaches do not differ significantly.
In the Pipestankage-nontemporal domain, it can be seen that the use of least-bad-first search
generally reduces the time taken to find solution plans. 34 problems are solved when using leastbad-first search compared to 30 when using breadth-first search and, in the majority of cases, the
time taken to find a solution plan is reduced. The makespans of the resulting solution plans are
generally increased when least-bad-first search is used, though, as the suboptimal exit paths found
in this domain are often longer than the (optimal-length) paths found when breadth-first search is
used.
In the Satellite domain using least-bad-first search leads to a reduction in planning time and, in
many cases, a reduction of the makespan. In particular, the performance on problems 19 and 20 is
substantially improved. The makespans on problems from 28 to 30 inclusive are also improved.
On the twenty standard benchmark FreeCell problems using least-bad-first search allows one
additional problem to be solved within the 30 minute time limit. As with the results obtained when
assessing the impact of macro-actions on planner performance, we obtained a more interesting and
useful set of data. Figure 12 shows the results of these experiments: it can be seen that although
least-bad-first search often improves the time taken to solve problems, the coverage overall is reduced, and no additional problems are solved where they previously were not.
In the Pipesnotankage-nontemporal domain, one additional problem can be solved using
breadth-first search rather than least-bad-first search. Also, in many cases, the use of least-bad146

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

FreeCell
10000
Least-Bad-First Search
Breadth-First Search
1000

Time (sec.)

100

10

1

0.1

0.01
5

10
Problem Instance

15

20

Figure 12: Time taken to solve twenty full-sized problems in the FreeCell domain, with least-badfirst and breadth-first search on plateaux (without macro-actions).

first search increases the makespan of the solution plan found. Overall, although time reductions
can occur when solving some problems when using least-bad-first search, the use of breadth-first
search provides better overall performance both in terms of planning time and makespan.
Overall, it can be seen across the evaluation domains that the performance of the planner when
using least-bad- or breadth-first search varies, in terms of planner execution time and plan quality:
• In the Philosophers domain, the use of least-bad-first search provides a substantial improvement in planner performance.
• In the Satellite, Driverlog and Pipestankage-nontemporal domains, the execution time of the
planner is generally improved by the use of least-bad-first search (with some reduction in plan
quality in the latter of these).
• In the Airport and Depots domain, the impact on performance is minimal, either in terms of
execution time or solution plan quality.
• In the FreeCell and Pipesnotankage-nontemporal domains, performance of the planner is degraded, both in terms of execution time and plan quality.
6.4 Handling Derived Predicates
It is possible to reason with domains involving derived predicates by precompiling the domain,
adding additional actions to support the derived predicates, and then planning in the usual manner
147

C OLES & S MITH

PSR
10000
Original Domain: ADL with Derived Predicates
Compiled Domain: ADL
1000

Time (sec.)

100

10

1

0.1

0.01
10

20

30
Problem Instance

40

50

Figure 13: Time taken to solve problems in the PSR domain with and without Derived Predicates.

(see Section 5). The necessary compilation, however, causes a large increase in the size of the
domain. If the planner performs the compilation itself, generating the confirm actions and segregating them from the normal actions internally, it can avoid the search overhead the compiled domain
would incur.
Three IPC 4 domains make use of derived predicates: PSR (Power Supply Restoration), Philosophers and Optical Telegraph. To assess the impact the native support of derived predicates was
having on planner performance, tests were run in these domains using the original domains containing derived predicates, and using the compiled domains. The results of these tests are shown in
Figures 13, 14 and 15.
In the PSR domain, the support of derived predicates substantially reduces the time taken to
find solution plans. This improvement in efficiency allows 23 rather than 12 problems to be solved
within the 30 minute time limit.
Marvin is only able to solve a few of the problems in the promela/optical-telegraph domain. On
the smaller problems, the performance is better without derived predicates; nonetheless, two of the
larger problems (problems 8 and 9) can be solved when working with the original domain where
previously they could not, and overall one additional problem is solved with derived predicates.
In the Philosophers domain, supporting derived predicates natively yields substantial reductions
in planning time. Using the compiled ADL domain formulation, only the first nine problems can be
solved. With native derived predicate support, all 48 problems can be solved.
148

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

Optical Telegraph
10000
Original Domain: ADL with Derived Predicates
Compiled Domain: ADL
1000

Time (sec.)

100

10

1

0.1

0.01
2

4

6
Problem Instance

8

10

Figure 14: Time taken to solve problems in the Optical Telegraph domain with and without Derived
Predicates.

6.5 Native ADL Support
The native support of ADL in Marvin provides two benefits, arising from the ability to use noncompiled domain formulations:
• Potentially improved efficiency, due to a more-efficient representation.
• The ability to infer reusable, parameterised macro-action sequences from the original ADL
actions, whose parameters are lost as a side-effect of the process used to compile ADL to
STRIPS domains.
6.5.1 T HE E FFECTS OF U SING

A

N ON -C OMPILED D OMAIN

To assess the effect of native support for ADL constructs on the performance of Marvin, we ran
a series of tests comparing the planner’s performance when given both the STRIPS and ADL domain encodings. Macro-actions were disabled in both cases to isolate the effect the encoding itself
was having on performance. In IPC 4, ADL was used to encode four of the domains: Airport,
Philosophers, Optical Telegraph and PSR. STRIPS compilations were made available for each of
these domains, in which each ground action that could arise when using the original ADL domain
was made into a fixed-parameter STRIPS action. In the Philosophers, Optical Telegraph and PSR
domains, the domain formulations making use of Derived Predicates were used.
149

C OLES & S MITH

Philosophers
10000
Original Domain: ADL with Derived Predicates
Compiled Domain: ADL
1000

Time (sec.)

100

10

1

0.1

0.01
5

10

15

20
25
30
Problem Instance

35

40

45

Figure 15: Time taken to solve problems in the Philosophers Domain with and without Derived
Predicates.

In the Airport, Optical Telegraph and PSR domains, the performance of Marvin (with macroactions disabled) was unaffected by the use of either the ADL or STRIPS domain encoding. The
ADL domain encodings did not give rise to inefficient compiled STRIPS encodings.
In the Philosophers domain, the use of the ADL domain encoding resulted in a reduction in
planning time when compared to the use of the compiled STRIPS encoding. As can be seen in
Figure 17, more problems can be solved within the 30 minute time-limit if the ADL encoding rather
than the STRIPS encoding is used, even disregarding the improvements in performance provided
by the use of macro-actions.
6.5.2 T HE E FFECTS OF I NFERRING M ACRO -ACTIONS
Supporting ADL natively in Marvin allows lifted macro-action schemata to be inferred during
search: in the compiled STRIPS domain formulations presented in IPC 4, the actions in the plateauescaping action sequences have few or no parameters, removing the opportunity to infer parameterised action sequences to use as the basis for macro-actions. Reusable macro-actions can be
inferred in STRIPS domains, as in many of the domains discussed in Section 6.1; but the compilation from ADL to STRIPS produces a domain in which the macro-actions cannot, in practice, ever
be reused.
To assess the effects of plateau-escaping macro-actions when using the ADL domain formulation, tests were run in the Philosophers, Optical Telegraph and PSR domains using the ADL domain
formulation with macro-actions enabled and disabled. Results for the Airport domain are presented
in Section 6.1, and results in the other domains will now be discussed.
150

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

0:
1:
2:
3:
4:

(activate-trans philosopher-1 philosopher forks–pid-wfork state-1 state-6) [1]
(activate-trans philosopher-2 philosopher forks–pid-wfork state-1 state-6) [1]
(activate-trans philosopher-3 philosopher forks–pid-wfork state-1 state-6) [1]
(activate-trans philosopher-4 philosopher forks–pid-wfork state-1 state-6) [1]
(activate-trans philosopher-0 philosopher forks–pid-wfork state-1 state-6) [1]

5: Macro-Action A Derived Here, using philosopher-4, philosopher-3, forks-4 and forks-3
16: (activate-trans philosopher-3 philosopher forks–pid-rfork state-6 state-3) [1]
17: Macro-Action A, using philosopher-2, philosopher-1, forks-2- and forks-1
28: (activate-trans philosopher-1 philosopher forks–pid-rfork state-6 state-3) [1]
29: Macro-Action B Derived Here, using philosopher-3 and -forks-332: (activate-trans philosopher-3 philosopher forks- -pidp1 11 -rfork state-3 state-4) [1]
33: Macro-Action B, using philosopher-1 and -forks-136: (activate-trans philosopher-1 philosopher forks- -pidp1 11 -rfork state-3 state-4) [1]
37:
38:
39:
40:

(queue-write philosopher-0 forks–pid-wfork forks-0- fork) [1]
(advance-empty-queue-tail forks-0- queue-1 qs-0 qs-0 fork empty zero one) [1]
(perform-trans philosopher-0 philosopher forks–pid-wfork state-1 state-6) [1]
(activate-trans philosopher-0 philosopher forks–pid-rfork state-6 state-3) [1]

41: Macro-Action B, using philosopher-0 and -forks-044: (activate-trans philosopher-0 philosopher forks- -pidp1 5 -rfork state-3 state-4) [1]

Figure 16: Plan for the Philosophers problem before macro-action expansion.
The plan shown in Figure 16 was produced by Marvin for problem four in the Philosophers
domain (before the translation of the macro-actions back into sequences of single-step actions).
The first five steps are found easily through guidance from the heuristic; the following eleven are
found during a period of exhaustive search which are, upon exiting the plateau, used to form a
macro-action, macro-action A. Macro-action B is formed in a similar manner later in the planning
process, and is subsequently used to avoid further exhaustive search. In solution plans for problems
involving more philosophers, the two macro-actions are used several times: macro-action A is used
once for each consecutive pair of philosophers, and macro-action B once for each odd-numbered
philosopher (and once for philosopher-0). The graph in Figure 17 shows the performance of Marvin
when the macro-actions are not inferred during search compared to that when the macro-actions are
inferred; both configurations produce identical solution plans. It can be seen that the performance
is consistently improved when the macro-actions are used, as exhaustive plateau search is avoided.
As can be seen in Figure 18, using macro-actions provides improved performance in the PSR
domain: 23 rather than 15 problems can be solved, and in the majority of the cases solved by the
two configurations, a solution can be found in less time when macro-actions are used.
151

C OLES & S MITH

Philosophers
10000
ADL Domain, Macro-Actions Enabled
ADL Domain, Macro-Actions Disabled
STRIPS Domain
1000

Time (sec.)

100

10

1

0.1

0.01
5

10

15

20
25
30
Problem Instance

35

40

45

Figure 17: Time taken to find a solution plan in the Philosophers domain with the STRIPS domain
encoding and the ADL domain encoding, with and without macro-actions.

PSR
10000
ADL Domain, Macro-Actions Enabled
ADL Domain, Macro-Actions Disabled
1000

Time (sec.)

100

10

1

0.1

0.01
10

20

30
Problem Instance

40

50

Figure 18: Time taken to solve problems in the PSR domain with and without macro-actions.

152

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

Optical Telegraph
10000
ADL Domain, Macro-Actions Enabled
ADL Domain, Macro-Actions Disabled
1000

Time (sec.)

100

10

1

0.1

0.01
2

4

6
Problem Instance

8

10

Figure 19: Time taken to solve problems in the Optical Telegraph domain with and without macroactions.

As shown in Figure 19, Marvin is only able to solve a few of the first 10 problems in the
promela/optical-telegraph domain. Nonetheless, when macro-actions are enabled, a net of two additional problems can be solved.

7. Future Work
The macro-action strategy adopted by Marvin in IPC 4 was to generate its macro-actions on a perproblem basis. It is possible, however, to build libraries of macro-actions on a per-domain basis;
this approach was taken by Macro-FF (Botea et al., 2005). Marvin’s macro-actions could also be
cached for use when solving all the problem instances in a given domain. If this were done, then the
knowledge encapsulated in the plateau-escaping macro-actions that allows heuristic imperfections
in the search landscape to be bypassed could be made available across all the problems in a given
domain without needing exhaustive search to re-discover this knowledge on each problem instance.
In contrast to existing systems that use off-line learning to generate and test macro-actions, caching
Marvin’s plateau-escaping macro-actions across solving a problem suite in this manner would allow
for online learning to take place. Further work is being undertaken in this area, to investigate
effective caching strategies and to manage the large number of macro-actions found.
The idea of using plateau-escaping macro-actions is not restricted to search under the relaxed
planning graph heuristic. Currently, the effect of using the macro-actions in search under other
heuristics is being investigated, including the causal-graph heuristic (Helmert, 2004) used by FastDownward .
153

C OLES & S MITH

At present, the macro-actions used in Marvin are restricted to those used to escape plateaux.
Work is currently in progress exploring ways of extending the macro-learning capabilities of Marvin
to include more general macro-action structures of the kind being explored by Botea and Schaeffer
(Botea et al., 2005).

8. Conclusions
We have presented a forward search heuristic planner called Marvin, which introduces several modifications to the search strategy of FF. These are:
• The use of learned macro-actions for escaping plateaux.
• A least-bad-first search strategy for search on plateaux.
• A greedy best-first search strategy when EHC fails.
• The addition of native support for both ADL and derived predicates, without relying on a
domain preprocessor.
Results presented indicate that the effects of these modifications varies depending on the domain
with which the planner is presented, but can be summarised as:
• The inference and use of plateau-escaping macro-actions:
– Provides improved performance in the Philosophers, Depots, Driverlog and
Pipestankage-nontemporal domains, in terms of planner execution time.
– Although performance did not improve in the other domains, it did not significantly
degrade, with the exception of FreeCell.
– The makespan of the plans found in the majority of domains was not degraded by the
use of macro-actions.
• The use of least-bad-first search:
– Provides substantial improvements in planner performance in the Philosophers domain.
– Reduces planner execution time in the Satellite, Driverlog and Pipestankagenontemporal domains, sometimes at the expense of increased solution plan makespans.
– Provides worse performance in the FreeCell and Pipesnotankage-nontemporal domains.
• Greedy best-first search does not perform significantly differently from best-first search in the
evaluation domains considered.
• Other than in the Airport domain, where no difference in performance is observed, the native
support for derived predicates and ADL improves the performance of the planner; either by
allowing a more-compact higher-level domain formulation to be used, or by improving the
effectiveness of the macro-actions inferred.
154

M ARVIN : A H EURISTIC S EARCH P LANNER WITH O NLINE M ACRO -ACTION L EARNING

Acknowledgments
We would like to thank the anonymous referees for their comments, and Maria Fox for her help in
revising this manuscript. We also thank Derek Long for supporting us in entering Marvin into IPC
4 and Jörg Hoffmann and Stefan Edelkamp for their hard work in organising the competition.

References
Bacchus, F. (2001). The aips ’00 planning competition.. AI Magazine, 22(3), 47–56.
Blum, A., & Furst, M. (1995). Fast planning through planning graph analysis. In Proceedings of
the Fourteenth International Joint Conference on Artificial Inteligence (IJCAI-95), pp. 1636–
1642.
Bonet, B., & Geffner, H. (2000). HSP: Heuristic search planner. AI Magazine, 21(2).
Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI planning
with automatically learned macro-operators. Journal of Artificial Intelligence Research, 24,
581–621.
Dawson, C., & Siklossy, L. (1977). The role of preprocessing in problem solving systems. In
Proceedings of the Fifth International Joint Conference on Artificial Intelligence, (IJCAI-77),
pp. 465–471.
Fikes, R., & Nilsson, N. (1971). STRIPS: A new approach to the application of theorem proving
to problem solving. In Proceedings of the 2nd International Joint Conference on Artificial
Intelligence (IJCAI-71), pp. 608–620.
Gazen, B., & Knoblock, C. (1997). Combining the expressivity of UCPOP with the efficiency of
Graphplan. In Proceedings of the Fourth European Conference on Planning (ECP-97), pp.
221–233.
Helmert, M. (2004). A planning heuristic based on causal graph analysis. In Proceedings of the
Fourteenth International Conference on Automated Planning and Scheduling (ICAPS-04),
pp. 161–170.
Helmert, M. (2006). The fast downward planning system. Journal of Artificial Intelligence Research, 26, 191–246.
Hoffmann, J., & Edelkamp, S. (2005). The deterministic part of IPC-4: An overview. Journal of
Artificial Intelligence Research, 24, 519–579.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic
search. Journal of Artificial Intelligence Research, 14, 253–302.
Hoffmann, J. (2001). Local search topology in planning benchmarks: An empirical analysis. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence (IJCAI01), pp. 453–458.
Hoffmann, J. (2005). Where ‘ignoring delete lists’ works: Local search topology in planning benchmarks. Journal of Artificial Intelligence Research, 24, 685–758.
Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphs to an
ADL sub-set. In Proceedings of the Fourth European Conference on Planning (ECP-97), pp.
275–287.
155

C OLES & S MITH

Long, D., & Fox, M. (2003). The 3rd International Planning Competition: Results and Analysis.
Journal of Artificial Intelligence Research, 20, 1–59.
Long, D., & Fox, M. (1999). Efficient implementation of the plan graph in STAN. Journal of
Artificial Intelligence Research, 10, 87–115.
Long, D., & Fox, M. (2003). The third international planning competition: Results and analysis.
Journal of Artificial Intelligence Research, 20, 1–59.
McDermott, D. (1996). A heuristic estimator for means ends analysis in planning. In Drabble, B.
(Ed.), Proceedings of the Third International Conference on Artificial Intelligence Planning
Systems (AIPS-96), pp. 142–149. AAAI Press.
McDermott, D. (2000). The 1998 AI planning systems competition. AI Magazine, 21(2), 35–55.
Minton, S. (1985). Selectively generalizing plans for problem-solving. In Proceedings of the Ninth
International Joint Conference on Artificial Intelligence (IJCAI-85).
Nebel, B., Hoffmann, J., & Thiebaux, S. (2003). In defense of PDDL axioms. In Proceedings of the
Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), pp. 961–966.
Newton, M., Levine, J., & Fox, M. (2005). Genetically evolved macro-actions in A.I. planning
problems. In Tuson, A. (Ed.), Proceedings of the 24th UK Planning and Scheduling SIG, pp.
163–172.
Pednault, E. (1989). ADL: Exploring the middle ground between STRIPS and the situation calculus.
In Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, pp. 324–332.
Vidal, V. (2004). A lookahead strategy for heuristic search planning. In Proceedings of the Fourteenth International Conference on Automated Planning and Scheduling (ICAPS-04), pp.
150–160.

156

Journal of Artificial Intelligence Research 28 (2007) 49-105

Submitted 2/06; published 02/07

The Strategy-Proofness Landscape of Merging
Patricia Everaere
Sébastien Konieczny
Pierre Marquis

everaere@cril.fr
konieczny@cril.fr
marquis@cril.fr

CRIL – CNRS
Faculté des Sciences, Université d’Artois
62300 Lens, France

Abstract
Merging operators aim at defining the beliefs/goals of a group of agents from the
beliefs/goals of each member of the group. Whenever an agent of the group has preferences
over the possible results of the merging process (i.e., the possible merged bases), she can
try to rig the merging process by lying on her true beliefs/goals if this leads to a better
merged base according to her point of view. Obviously, strategy-proof operators are highly
desirable in order to guarantee equity among agents even when some of them are not
sincere. In this paper, we draw the strategy-proof landscape for many merging operators
from the literature, including model-based ones and formula-based ones. Both the general
case and several restrictions on the merging process are considered.

1. Introduction
Merging operators aim at defining the beliefs/goals of a group of agents from the beliefs/goals of each member of the group. Though beliefs and goals are distinct notions,
merging operators can typically be used for merging either beliefs or goals. Thus, most of
the logical properties from the literature (Revesz, 1993, 1997; Konieczny & Pino Pérez, 1998,
2002) for characterizing rational belief merging operators can be used for characterizing as
well rational goal merging operators.
Whatever beliefs or goals are to be merged, there are numerous situations where agents
have preferences on the possible results of the merging process (i.e., the merged bases). As
far as goals are concerned, an agent is surely satisfied when her individual goals are chosen
as the goals of the group. In the case of belief merging, an agent can be interested in
imposing her beliefs to the group (i.e., “convincing” the other agents), especially because
the result of a further decision stage at the group level may depend on the beliefs of the
group.
So, as soon as an agent participates to a merging process, the strategy-proofness problem
has to be considered. The question is: is it possible for a given agent to improve the result of
the merging process with respect to her own point of view by lying on her true beliefs/goals,
given that she knows (or at least she assumes) the beliefs/goals of each agent of the group
and the way beliefs/goals are merged?
As an illustration, let us consider the following scenario of goal merging (that will be
used as a running example in the rest of the paper):

c
2007
AI Access Foundation. All rights reserved.

Everaere, Konieczny & Marquis

Example 1 Three friends, Marie, Alain and Pierre want to spend their summer holidays
together. They have to determine whether they will go to the seaside and/or to the mountains, or to stay at home, and also to determine whether they will take a long period of
vacations or not. The goals of Marie are to go to the seaside and to the mountains if it is
for a long period; otherwise she wants to go to the mountains, only, or to stay at home. The
goals of Alain are to go to the seaside if it is for a long period; or to go to the mountains
if it is for a short period. Finally, Pierre is only interested in going to the seaside for a
long period, otherwise he prefers to stay at home. If one uses a common merging operator
for defining the choice of the group,1 then the goals of the group will be either to go to the
seaside for a long period, or to go to the mountains or to stay at home for a short period.
Accordingly, the group may choose to go to the seaside, only, for a long period, which is not
among the goals of Marie. However, if Marie lies and claims that, for a short period, she
wants to go to the mountains only, or to stay at home, then the result of the merging process
will be different. Indeed, in this case, the goals of the group will be to go to the mountains
for a short period, or to stay at home, which corresponds to the goals of Marie.
Similarly, the strategy-proofness issue has to be considered in many belief merging scenarios, just because rational decision making typically takes account for the “true” state
of the world. When agents have conflicting beliefs about it, belief merging can be used
to determine what is the “true” state of the world for the group; manipulating the belief
merging process is a way for an agent to change the resulting beliefs at the group level so as
to make them close to her own beliefs. As a consequence, the decisions made by the group
may also change and become closer to those the agent would made alone. For instance,
assume that the three friends agree that the mountains must be avoided when the weather
is bad. If the beliefs of the group is that the weather is bad, then the decision to go to
the mountains will be given up. If Pierre believes that the weather is bad, then he may
be tempted to make the weather is bad accepted at the group level. Therefore, a collective
decision will be not to go to the mountains.
There are several multi-agent settings in which some agents exchange information and
must make individual decisions based on their beliefs. In many scenarios, agents are tempted
to get an informational advantage over other agents, which can be achieved by gathering as
much information as possible and by hiding their own ones. Indeed, being better informed
may help an agent making better decisions than the other agents of the group. For instance,
Shoham and Tennenholtz (2005) investigate non-cooperative computation: each agent delivers some piece of information (truthfully or not), all such pieces are used to compute
the value of a (commonly-known) function, and this value is given back to the agents; the
aim of each agent is to get the true value of the function, and if possible to be the only
one to get it. In the work of Shoham and Tennenholtz (2005), information is considered at
an abstract level; assuming that information represent beliefs and the function is a belief
merging operator, each agent wants to get the true merged base and possibly to be the only
one to get it. Contrastingly, in other scenarios, where decisions have to be made collectively
and are based on the beliefs of the group, agents are satisfied if the beliefs of the group
1. Formally, the model-based operator 4dµH ,Σ , using the Hamming distance and the sum aggregation function, defined in Section 3.1.

50

The Strategy-Proofness Landscape of Merging

are close to their own beliefs. In this paper, we focus on such an issue, which has to be
addressed in many everyday life situations. Let us illustrate this on an example:
Example 2 There is a position available in some university. The committee in charge
of the recruitment has to determine the right profile for this position. Four criteria are
considered: research skills, teaching skills, relationship skills, and the past positions of the
candidate. Suppose that a member of the committee believes that the important criteria for
the job are research skills and relationship skills, and that it is better to recruit a candidate
who got a good position in the past. She will be pleased by the recruitment if the group shares
her beliefs about the right profile. So she can be tempted to manipulate the merging process
in order to achieve such a situation.
Determining whether a belief/goal merging operator is strategy-proof, and in the negative case, identifying restrictions under which strategy-proofness can be ensured is thus an
important issue. Indeed, merging operators are intended to characterize the beliefs/goals
of a group of agents, from the beliefs/goals of each agent from the group; obviously, this
objective cannot be reached if the agents do not report their “true” beliefs/goals, which
can easily be the case when manipulable merging operators are used (since agents will be
tempted to manipulate the process in such a case).
Since merging operators are typically used in artificial systems, one can wonder whether
strategy-proofness is really a relevant issue in this context. The answer actually depends
on the sophistication of the agents under consideration. Thus, in a distributed database
setting, the (low-level) agents (i.e., the databases) have typically no evaluation/preference
on the merged base, and the strategy-proofness issue does not make sense. It is not the
same story when the agents have goals and reasoning capacities. In this case, it cannot be
discarded that some agents will be able to foresee weakenesses in the aggregation process
and to exploit them for their own benefits. When high-level artificial agents are involved,
the strategy-proofness problem is even more stricking than in the case of human agents
because of the superior computational abilities of artificial systems.
The strategy-proofness issue has been studied for years in the domain of Social Choice
Theory (Arrow, Sen, & Suzumura, 2002). An important objective is to design preference
aggregation procedures (and, in particular, voting procedures) which are strategy-proof. A
very famous result, known as Gibbard-Sattherwaite theorem, is that this objective cannot
be reached in an absolute manner: under a number of sensible requirements, no strategyproof voting procedure may exist (Gibbard, 1973; Satterthwaite, 1975). Strategy-proofness
can only be achieved by relaxing some of those requirements, which is enough to escape
from Gibbard-Sattherwaite theorem. We shall return to this topic in Section 7 where the
connections between belief merging and preference aggregation will be considered in more
depth.
The very objective of this paper is to draw the strategy-proofness landscape for many
merging operators from the literature, including model-based ones and formula-based ones.
We focused on operators for merging bases that are sets of propositional formulas, where no
priorities between the bases are available. The (classical) propositional logic framework can
be argued as a representation setting expressive enough for many AI scenarios; furthermore,
it is natural to investigate first the key problems raised by aggregation and manipulation in
this simple setting, before considering more sophisticated logics. For each operator under
51

Everaere, Konieczny & Marquis

consideration, we aim at determining whether it is strategy-proof in the general case, and
under some restrictions on the merging process (including the number of agents and the
presence of integrity constraints) and on the set of available strategies for the agents.
The rest of the paper is organized as follows. In Section 2, some formal preliminaries are
provided. In Section 3, the definitions of the main propositional merging operators from the
literature are recalled. Several definitions of strategy-proofness based on a general notion of
satisfaction index are given in Section 4 and our strategy-proofness results are reported in
Section 5. They are discussed in Section 6. Then, connections with Social Choice Theory
and other related works are pointed out in Section 7, just before the conclusion. Proofs are
reported at the end of the paper.

2. Formal Preliminaries
We consider a propositional language L defined from a finite (and non-empty) set of propositional variables P and the standard connectives, including >, the Boolean constant always
true, and ⊥, the Boolean constant always false.
An interpretation (or world) ω is a total function from P to {0, 1}, denoted by a bit
vector whenever a strict total order on P is specified. The set of all interpretations is noted
W. An interpretation ω is a model of a formula φ ∈ L if and only if it makes it true in the
usual truth functional way.
[φ] denotes the set of models of formula φ, i.e., [φ] = {ω ∈ W | ω |= φ}. In order to
avoid too heavy notations, we identify each interpretation ω with the canonical term on P
which has ω as its unique model. For instance, if P = {a, b} and ω(a) = 1, ω(b) = 0, ω is
identified with the term a ∧ ¬b.
A formula φ from L is consistent if and only if [φ] 6= ∅. φ is a logical consequence of a
formula ψ, noted ψ |= φ if and only if [ψ] ⊆ [φ]. Two formulas are logically equivalent (≡)
if and only if they share the same models.
A belief/goal base K denotes the set of beliefs/goals of an agent. It is a finite and
consistent set of propositional formulas, interpreted conjunctively.VWhen K is a belief/goal
b denotes the singleton base containing the conjunction K of all formulas from
base, K
K. A base is said to be complete if and only if it has exactly one model. Each belief/goal
base K characterizes a bipartition of the set of all interpretations: the models of K are the
interpretations which are acceptable for the agent, and the countermodels are not. When
K is a belief base, an interpretation ω is acceptable when there is enough evidence that ω is
the “true” state of the world; when K is a goal base, ω is acceptable when it is sufficiently
desired. Such a bipartition can be considered as an approximation of the full belief/goal
preference structure of the corresponding agent: in the belief case, ω is at least as preferred
as ω 0 means that the fact that ω is the “true” state of the world is at least as plausible as
the fact that ω 0 is the “true” state of the world; in the goal case, ω is at least as preferred as
ω 0 means that the fact that ω would be the “true” state of the world is at least as desired
as the fact that ω 0 would be the “true” state of the world.
A belief/goal profile E is associated with the group of n agents involved in the merging
process. It is a non-empty multi-set (bag) of belief/goal bases E = {K1 , . . . , Kn } (hence
different agents are allowed to exhibit identical bases). Note that profiles are non-ordered
(multi-)sets; thus, the profile representation of groups of agents induces an anonymity prop52

The Strategy-Proofness Landscape of Merging

erty: each agent has the same importance as the other agents of the group and the result
of the merging process only depends on the bases themselves (i.e., exchanging the bases of
two agents gives the same profile, hence the same merged base).
V
V
V
WeVdenote by E the conjunction
of
bases
of
E
=
{K
,
.
.
.
,
K
},
i.e.,
E
=
(
1
n
W
W
V K1 ) ∧
. . . ∧ (V Kn ), and we denote by E the disjunction of bases of E, i.e., E = ( K1 ) ∨
. . . ∨ ( Kn ).
V
A profile E is said to be consistent if and only if E is consistent. The multi-set union
is noted t and the multi-set containment relation is noted v. The cardinal of a finite set
(or a finite multi-set) A is noted #(A). ⊆ will denote set containment and ⊂ strict set
containment, i.e., A ⊂ B if and only if A ⊆ B and A 6= B.
If ≤E denotes a pre-order on W (i.e., a reflexive and transitive relation), then <E denotes
the associated strict ordering defined by ∀ω, ω 0 ∈ W, ω <E ω 0 if and only if ω ≤E ω 0 and
ω 0 6≤E ω.
The result of the merging of (the bases from) a profile E with the merging operator 4,
under the integrity constraints µ is the base, noted 4µ (E), called the merged base. The
integrity constraints consist of a consistent formula (or, equivalently, a (finite) consistent
conjunction of formulas) the merged base has to satisfy (it may represent some physical laws,
some norms, etc.); in other words, models of the merged base are models of the integrity
constraints.

3. Merging Operators
We recall in this section the two main families of merging operators from the literature. The
first family is defined by a selection of some interpretations (model-based operators). The
second family is defined by a selection of some formulas in the union of the bases (formulabased operators). For more details on those two families, see for example (Konieczny, Lang,
& Marquis, 2004).
3.1 Model-Based Operators
The first family is based on the selection of some models of the integrity constraints, the
“closest” ones to the profile. Closeness is usually defined from a notion of distance and an
aggregation function (Revesz, 1997; Konieczny & Pino Pérez, 1998, 1999; Lin & Mendelzon,
1999; Liberatore & Schaerf, 1998).
Definition 1 (pseudo-distances)
• A pseudo-distance between interpretations is a total function d : W × W 7→ R+ s.t.
for any ω, ω 0 , ω 00 ∈ W: d(ω, ω 0 ) = d(ω 0 , ω), and d(ω, ω 0 ) = 0 if and only if ω = ω 0 .
• A distance between interpretations is a pseudo-distance that satisfies the triangular
inequality: ∀ω, ω 0 , ω 00 ∈ W, d(ω, ω 0 ) ≤ d(ω, ω 00 ) + d(ω 00 , ω 0 ).
Two widely used distances between interpretations are Dalal distance (Dalal, 1988),
denoted dH , that is the Hamming distance between interpretations (the number of propositional atoms on which the two interpretations differ); and the drastic distance, denoted
53

Everaere, Konieczny & Marquis

dD , that is one of the simplest pseudo-distances one can define: it gives 0 if the two interpretations are the same one, and 1 otherwise.
Definition 2 (aggregation functions) An aggregation function f is a total function associating a nonnegative real number to every finite tuple of nonnegative real numbers s.t.
for any x1 , . . . , xn , x, y ∈ R+ :
• if x ≤ y, then f (x1 , . . . , x, . . . , xn ) ≤ f (x1 , . . . , y, . . . , xn ).

(non-decreasingness)

• f (x1 , . . . , xn ) = 0 if and only if x1 = . . . = xn = 0.
• f (x) = x.

(minimality)
(identity)

Widely used functions are the max (Revesz, 1997; Konieczny & Pino Pérez, 2002), the
sum Σ (Revesz, 1997; Lin & Mendelzon, 1999; Konieczny & Pino Pérez, 1999), or the
leximax GM ax (Konieczny & Pino Pérez, 1999, 2002).
The chosen distance between interpretations induces a “distance”2 between an interpretation and a base, which in turn gives a “distance” between an interpretation and a profile,
using the aggregation function. This latter distance gives the needed notion of closeness represented by a pre-order on W induced by E, noted ≤E . Such a pre-order can be interpreted
as a plausibility ordering associated with the merged base.
Definition 3 (distance-based merging operators) Let d be a pseudo-distance between
interpretations and f be an aggregation function. The result 4d,f
µ (E) of the merging of E
given the integrity constraints µ is defined by:
0
0
[4d,f
µ (E)] = min([µ], ≤E ) = {ω ∈ [µ] | @ω ∈ [µ], ω <E ω}

where the pre-order ≤E on W induced by E is defined by:
• ω ≤E ω 0 if and only if d(ω, E) ≤ d(ω 0 , E), where
• d(ω, E) = fK∈E (d(ω, K)), where
• d(ω, K) = minω0 |=K d(ω, ω 0 ).
Observe that dd,f (ω, E) would be a more correct notation than d(ω, E); however, since
there is no ambiguity in the choice of the function f and the distance between interpretations
d in the following, we prefer the lighter notation d(ω, E).
Let us step back to the example given in the introduction in order to illustrate modelbased merging operators:
Example 3 Consider the set P with three propositional variables l(long period), s(easide)
and m(ountains), taken in this order. The goals of the three agents are then given by the
following bases: [K1 ] = {000, 001, 111} (Marie’s wishes), [K2 ] = {001, 110} (Alain’s wishes)
and [K3 ] = {000, 110} (Pierre’s wishes). There is no integrity constraint (µ ≡ >).
2. Abusing words since it is not a distance from the mathematical standpoint.

54

The Strategy-Proofness Landscape of Merging

We have [∆dµH ,Σ ({K1 , K2 , K3 })] = {000, 001, 110}. Table 1 gives some details of the
computation. The first column gives all possible words. The Ki (i = 1 . . . 3) columns give
for each interpretation ω the value dH (ω, Ki ). Finally, the rightmost column gives for
each interpretation ω the value of dH (ω, {K1 , K2 , K3 }). The interpretations ω for which
dH (ω, {K1 , K2 , K3 }) is minimal (in bold) are the models of the merged base ∆dµH ,Σ ({K1 , K2 ,
K3 }).
ω
000
001
010
011
100
101
110
111

K1
0
0
1
1
1
1
1
0

K2
1
0
1
1
1
1
0
1

d

∆>H

K3
0
1
1
2
1
2
0
1

,Σ

({K1 , K2 , K3 })
1
1
3
4
3
4
1
2

Table 1: Merging with ∆dµH ,Σ .

3.2 Formula-Based Operators
The other main family of merging operators gather the so-called “formula-based operators”
or “syntax-based operators”. Formula-based operators are based on the selection of consistent subsets of formulas in the union of the bases of the profile E. Several operators are
obtained by letting vary the selection criterion. The result of the merging process is the
set of consequences that can be inferred from all selected subsets (Baral, Kraus, Minker, &
Subrahmanian, 1992; Rescher & Manor, 1970; Konieczny, 2000). For these operators, the
syntactic form of the bases may easily influence the result of the merging process: replacb = {ϕ1 ∧ . . . ∧ ϕn } may
ing a base K = {ϕ1 , . . . , ϕn } by the (logically equivalent) base K
lead to change the corresponding merged base (while this is not the case for model-based
operators).
Definition 4 (maximal consistent subsets) Let K be a base and µ be an integrity constraint. maxcons(K, µ) is the set of all the maximal (w.r.t. set inclusion) consistent
subsets (maxcons for short) of K ∪ {µ} that contains µ, i.e., maxcons(K, µ) is the set of
all consistent M that satisfy:
• M ⊆ K ∪ {µ}, and
• µ ∈ M , and
• If M ⊂ M 0 ⊆ K ∪ {µ}, then M 0 is not consistent.
When maximality must be taken with respect to cardinality (instead of set inclusion), we
shall use the notation maxconscard (K, µ).
Now, for any profile E and integrity constraint µ, we set
[
maxcons(E, µ) = maxcons(
Ki , µ)
Ki ∈E

55

Everaere, Konieczny & Marquis

Observe that set-theoretic union (and not multi-set union) is used here.
The following operators have been defined so far (Baral, Kraus, & Minker, 1991; Baral
et al., 1992; Konieczny, 2000):
Definition 5 (formula-based merging operators) Let E be a profile and let µ be an
integrity constraint:
W
V
• 4C1
µ (E) ≡
M ∈maxcons(E,µ) ( M ).
V
W
• 4C3
µ (E) ≡
M |M ∈maxcons(E,>) and M ∪{µ} consistent ( M ).
W
V
• 4C4
µ (E) ≡
M ∈maxconscard (E,µ) ( M ).
V
 W
 M ∈maxcons(E,>) and M ∪{µ} consistent ( {M ∪ {µ}})
• 4C5
if consistent,
µ (E) ≡

µ otherwise.
Those operators clearly select as much formulas as they can from the union of the bases,
under the consistency requirement. The differences between them lie in the meaning of
“as much”. Those operators were defined by Baral et al. (1992), except 4C5 which is a
modification of 4C3 that ensures consistency. Indeed, unlike the other operators listed here,
C2 operator
4C3
µ may generate inconsistent merged bases (as the empty disjunction). A 4
has also been introduced by Baral et al. (1992), and shown equivalent to 4C1 (this is why
it is not listed above). An important drawback of those operators is that they do not take
account for the sources from which the formulas are issued.3 Nevertheless, they have an
appealing, simple definition.
To illustrate the behaviour of formula-based operators, let us step back to the example
given in the introduction. Since the absence of constraints makes the operators 4C1 , 4C3
and 4C5 coincide, we shall add the following constraint: µ = l ∧ ¬s, i.e., it turns out that
the group will have to take holidays for a long period and that they cannot go to the seaside.
Example 4 Suppose that Marie, Alain and Pierre’s goals are encoded by the following
bases: K1 = {l ↔ s, l → m}, K2 = {l ↔ s, s ↔ ¬m}, and K3 = {l ↔ s, ¬m}. With the
integrity constraints µ = l ∧ ¬s, maxcons(E, µ) contains two sets: {l → m, s ↔ ¬m} and
C3
C4
C5
{¬m}. We get 4C1
µ (E) ≡ µ, 4µ (E) ≡ ⊥, 4µ (E) ≡ l ∧ ¬s ∧ m, and 4µ (E) ≡ µ.
The “syntax sensitivity” of those operators is due to the fact that the comma symbol
“,” which appears in the expression of the bases, is a specific, yet not truth-functional,
connective (Konieczny, Lang, & Marquis, 2005) that is usually not equivalent to standard
conjunction in the formula-based framework. Such operators may easily lead to merged
bases which differ from their counterparts where commas are replaced by conjunctions in
the input bases. For example, with µ = >, K1 = {a ∧ b}, K2 = {¬(a ∧ b)} and K10 =
C1
0
{a, b}, the fact that K10 ≡ K1 does not imply that 4C1
µ ({K1 , K2 }) ≡ 4µ ({K1 , K2 }), since
C1
C1
0
4µ ({K1 , K2 }) ≡ > and 4µ ({K1 , K2 }) ≡ a ∨ b. See (Konieczny et al., 2005) for a more
3. It is possible to avoid it by taking advantage of a further selection function (Konieczny, 2000).

56

The Strategy-Proofness Landscape of Merging

detailed discussion on the meaning of the comma connective in frameworks for reasoning
under inconsistency.
b the singleton base containing the
Clearly enough, if one replaces each base K by K,
b
conjunction of its elements before making the union, the resulting operators, noted 4C
µ , are
not any longer sensitive to the syntactic presentation of the bases (replacing every base by
a logically equivalent one leads to the same merged base). Formally, we have:
Definition 6 (other formula-based merging operators) Let E = {K1 , . . . , Kn } be a
profile and let µ be an integrity constraint:
c
Ci c
c
4Ci
µ (E) = 4µ ({K1 , . . . , Kn }).

Remark 1 Observe that 4C4 is equivalent to the model-based operator ∆dD ,Σ = ∆dD ,GM ax .
c
Indeed, 4C4 returns the disjunction of the maximal (for cardinality) consistent subsets of
the profile under the constraints. This is exactly what the operator ∆dD ,Σ = ∆dD ,GM ax
achieves since it amounts to define the set of models of the merged base as the set of interpretations that satisfy the constraints and a maximal number of bases of the profile, i.e.,
the interpretations that are models of a maximal (for cardinality) consistent subset of the
profile under the constraints.
c

4. Strategy-Proofness
The strategy-proofness issue for a merging operator can be stated as follows: is it possible
for a given agent to improve the result of the merging process with respect to her own
point of view by lying on her true beliefs/goals, given that she knows the beliefs/goals of
each agent of the group and the way beliefs/goals are merged? If this question can be
answered positively, then the operator is not strategy-proof (the agent may benefit from
being untruthful). Thus, a merging operator is not strategy-proof if one can find a profile
E = {K1 , . . . , Kn } which represents the bases of the other agents, an integrity constraint µ,
and two bases K and K 0 s.t. the result of the merging of E and K 0 is better for the agent
than the result of the merging of E with her true base K (called the initial base).
Definition 7 (strategy-proofness) Let i be a satisfaction index, i.e., a total function
from L × L to IR.
• A profile E is said to be manipulable by a base K for i given a merging operator ∆ and
an integrity constraint µ if and only if there exists a base K 0 such that i(K, ∆µ (E t
{K 0 })) > i(K, ∆µ (E t {K})).
• A merging operator ∆ is strategy-proof for i if and only if there are no integrity
constraint µ and profile E = {K1 , . . . , Kn } such that E is manipulable for i.
Given two bases (interpreted conjunctively) K, K∆ , the value of i(K, K∆ ) is intended to
indicate how much a base K is close to the merged base K∆ . The need for such satisfaction
indexes comes from the fact that the only information given by each agent is her own base
K: if the full preference structure over sets of interpretations were available for each agent
57

Everaere, Konieczny & Marquis

as an additional input (e.g., under the form of a utility function), then one could use it to
define strategy-proofness directly for a given agent (as it is done in Social Choice Theory,
Arrow et al., 2002) and not in a uniform way for all agents. This explains why we call i a
satisfaction index and not a utility function.
Clearly, there are many different ways to define the satisfaction of an agent given a
merged base. While many ad hoc definitions can be considered, we consider the following
three indexes which are, according to us, the most meaningful ones when no additional
information about the agents are available. As far as we know, this is the first time such
indexes are considered in the context of pure propositional merging.
The first two indexes are drastic ones: they range to {0, 1}, so the agent is either fully
satisfied or not satisfied at all.
Definition 8 (weak drastic index)

1
idw (K, K∆ ) =
0

if K ∧ K∆ is consistent,
otherwise.

This index takes value 1 if the result of the merging process (noted K∆ in the definition) is consistent with the agent’s base (K), and 0 otherwise. It means that the agent is
considered fully satisfied as soon as its beliefs/goals are consistent with the merged base.
Definition 9 (strong drastic index)

ids (K, K∆ ) =

1
0

if K∆ |= K,
otherwise.

This index takes value 1 if the agent’s base is a logical consequence of the result of the
merging process, and 0 otherwise. In order to be fully satisfied, the agent must impose her
beliefs/goals to the whole group.
The last index is not a Boolean one, leading to a more gradual notion of satisfaction.
The more compatible the merged base with the agent’s base the more satisfied the agent.
The compatibility degree of K with K∆ is the (normalized) number of models of K that
are models of K∆ as well:
Definition 10 (probabilistic index) If #([K∆ ]) = 0, then ip (K, K∆ ) = 0, otherwise:
ip (K, K∆ ) =

#([K] ∩ [K∆ ])
.
#([K∆ ])

ip (K, K∆ ) is the probability to get a model of K from a uniform sampling in the models
of K∆ . This index takes its minimal value when no model of K is in the models of the
merged base K∆ , and its maximal value when each model of the merged base is a model of
K.
Strategy-proofness for these three indexes are not independent notions:
Theorem 1
1. If a merging operator is strategy-proof for ip , then it is strategy-proof for idw .
58

The Strategy-Proofness Landscape of Merging

2. Consider a merging operator ∆ that generates only consistent bases.4 If it is strategyproof for ip , then it is strategy-proof for ids .
On the other hand, it is easy to prove that strategy-proofness for idw and strategyproofness for ids are logically independent in the general case (an operator can be strategyproof for one of them without being strategy-proof for the other, and it can be strategy-proof
for both of them or for neither).
Let us conclude this section with our running example, and give formal arguments
explaining how Marie can manipulate the merging process:
Example 5 We consider three bases [K1 ] = {000, 001, 111} (Marie’s wishes), [K2 ] =
{110, 001} (Alain’s wishes) and [K3 ] = {110, 000} (Pierre’s wishes). There is no constraint
(µ ≡ >).
dH ,Σ
({K1 , K2 , K3 })] = {000, 001, 110} and ids (K1 , ∆d>H ,Σ ({K1 , K2 , K3 })) = 0.
[∆>
If Marie reports [K10 ] = {000, 001} instead of K1 , then [∆d>H ,Σ ({K10 , K2 , K3 })] = {000,
001} and ids (K1 , ∆d>H ,Σ ({K10 , K2 , K3 })) = 1.
See Table 2 for details of the computations.
ω
000
001
010
011
100
101
110
111

K1
0
0
1
1
1
1
1
0

K10
0
0
1
1
1
1
2
2

K2
1
0
1
1
1
1
0
1

K3
0
1
1
2
1
2
0
1

d

∆>H

,Σ

({K1 , K2 , K3 })
1
1
3
4
3
4
1
2

d

∆>H

,Σ

({K10 , K2 , K3 })
1
1
3
4
3
4
2
4

Table 2: ∆dH ,Σ is not strategy-proof for ids .
In the rest of this paper, we shall focus on those three indexes, idw , ids and ip . Note
that investigating other indexes can be interesting. In particular, the probabilistic index
can be viewed as a rough measure of similarity between the bases. Could more complex
similarity measures between sets (see e.g., Tversky, 2003) also prove useful to define other
sensible indexes is an interesting question that we let for further research.

5. Strategy-Proofness Results
In the general case, both the family of model-based operators and the family of formulabased operators are not strategy-proof for the three indexes we consider. This means that
there are operators from those families which are not strategy-proof. However, imposing
further restrictions may lead to some strategy-proofness results. Considering them in a
systematic way allows us to draw the strategy-proofness landscape for both families.
In the following, we consider four natural restrictions for the merging process, as listed
below:
4. I.e., ∆µ (E) is always consistent.

59

Everaere, Konieczny & Marquis

• A first restriction concerns the number of bases to be merged. The question is the
following: does the number of bases involved in the merging process have an influence
on the strategy-proofness of an operator? In general, we can answer positively to
this question. More precisely, there is a distinction between the cases #(E) = 2 and
#(E) > 2. In some situations, no manipulation is possible with two bases, while
with more bases, it becomes possible. Since a base {>} typically plays the role of a
“neutral element” for all the operators we consider (i.e., 4µ (E) ≡ 4µ (E t{>})), if an
operator is not strategy-proof for profiles with n bases, then it is not strategy-proof
for profiles with m > n bases.
• A second parameter is the completeness of the beliefs/goals of the agent who aims at
manipulating. In some cases, having such strong beliefs/goals renders any manipulation impossible. Working with complete bases (i.e., singleton sets of models) makes
the merging process close to a uninominal vote, i.e., a vote for a unique interpretation.
• A third significant parameter is the presence of integrity constraints. On the one hand,
having nontrivial integrity constraints (µ 6≡ >) can make a manipulation possible,
while it is not the case when no integrity constraints are considered, and the converse
also holds.
• Another restriction bears on the available manipulations. In the general case the untruthful agent is free to reporting any base, even if it is “quite far” from her true base.
However, there are numerous situations where the other agents participating to the
merging process have some information about her true base. In these cases, the agents
have to report some bases close to the real ones. Two particular manipulations will
be studied: erosion manipulation when the agent pretends to believe/desire more that
she does (the agent gives only some parts of its models); and dilatation manipulation
when the agent pretends to believe/desire less that she does (the agent gives only
parts of its countermodels).
5.1 Model-Based Operators
The first result is that there is no general strategy-proofness result (i.e., for any aggregation
function and any distance) for model-based operators. This is not very surprising when
one reminds the existence of Gibbard-Satterthwaite theorem, which states that there is no
“good” strategy-proof preference aggregation method (see Section 7).
However, some quite general strategy-proofness results can be obtained. The following
theorem gives some of them, organized from the most general one (for any aggregation
function when the drastic distance dD is considered), to more specific ones (for any distance
d when the aggregation function is Σ):
Theorem 2
• Let f be any aggregation function. ∆dµD ,f is strategy-proof for ip , idw and ids .
• Let d be any distance. Provided that only two bases are to be merged, ∆d,Σ
> is strategyproof for the indexes idw and ids .
60

The Strategy-Proofness Landscape of Merging

• For any distance d, ∆d,Σ
is strategy-proof for the indexes ip , idw and ids when the
µ
initial base K is complete.
It is interesting to note that ∆dµD ,Σ (which coincides with ∆dµD ,GM ax ) is close to a voting
procedure called approval voting (Brams & Fishburn, 1983), where each agent can vote for
(approve) as many candidates as she wants, and the elected candidates are the ones who
get the greatest number of votes.
As shown by the running example, the family of merging operators ∆dµH ,f obtained
by considering the Hamming distance and letting the aggregation function f vary is not
strategy-proof. Let us now focus on this family, and consider successively the two operators
obtained by considering Σ and GM ax as aggregation functions.
As to ∆dµH ,Σ , the number of bases, the presence of integrity constraints and the completness of the bases are significant. For this operator, the next theorem makes precise the
boundaries between strategy-proofness and manipulation (in the following properties, K
represents the initial base and #(E) the number of bases in the profile E):
Theorem 3
• ∆dµH ,Σ is strategy-proof for idw or ids if and only if (µ ≡ > and #(E) = 2) or K is
complete.
• ∆dµH ,Σ is strategy-proof for ip if and only if K is complete.
In contrast to ∆dµH ,Σ , ∆dµH ,GM ax is not strategy-proof even in very restricted situations:
Theorem 4
• ∆dµH ,GM ax is not strategy-proof for the satisfaction indexes idw and ip (even if µ ≡ >,
K is complete and #(E) = 2).
• ∆dµH ,GM ax is strategy-proof for the satisfaction index ids if and only if µ ≡ >, K is
complete and #(E) = 2.
5.2 Formula-Based Operators
C3
C4
For the probabilistic index, none of the formula-based operators 4C1
µ , 4µ , 4µ , and
C5
4µ is strategy-proof. But, for the two drastic indexes, there are some situations where
strategy-proofness can be ensured:

Theorem 5
C3
C4
C5
• 4C1
µ , 4µ , 4µ , and 4µ are not strategy-proof for ip (even if µ ≡ >, K is complete
and #(E) = 2).

• 4C1
µ is strategy-proof for idw and ids .
• 4C3
µ is strategy-proof for idw and ids if and only if µ ≡ >.
• 4C4
µ is not strategy-proof for idw and ids (even if µ ≡ >, K is complete and #(E) = 2).
61

Everaere, Konieczny & Marquis

• 4C5
µ is strategy-proof for idw if and only if µ ≡ > or K is complete, and is strategyproof for ids if and only if µ ≡ >.
C3
C4
C5
For the other formula-based merging operators 4C1
µ , 4µ , 4µ , 4µ , the results are
more balanced, with more strategy-proofness results:
c

c

c

c

Theorem 6
• 4C1
µ is strategy-proof for idw and ids , and is strategy-proof for ip if and only if
#(E) = 2.
c

• 4C3
µ is strategy-proof for idw and ids if and only if µ ≡ >, and is strategy-proof for ip
if and only if #(E) = 2 and µ ≡ >.
c

• 4C4
µ is strategy-proof for ip , idw and ids .
c

• 4C5
µ is strategy-proof for idw if and only if #(E) = 2 or µ ≡ > or K is complete.
c

C5
4C5
µ is strategy-proof for ids if and only if #(E) = 2 or µ ≡ >. Finally, 4µ is
strategy-proof for ip if and only if #(E) = 2.
c

c

5.3 Ensuring Strategy-Proofness: The Case of Complete Bases
Let us now focus on a very specific case: the situation where every base is complete. While
this situation is rather infrequent when dealing with usual belief bases, it can be imposed
in a goal merging setting, especially if it guarantees strategy-proofness. This explains why
we consider such a case in this paper. As said above, it is also interesting because of the
relationship with uninominal voting systems if one interprets each complete base as a vote
for the corresponding interpretation.
Theorem 7 The strategy-proofness results reported in Table 3 hold, under the restriction
that each base is complete (f stands for any aggregation function, and d for any distance).
sp means “strategy-proof ”, sp means “non strategy-proof ” even if #(E) = 2 and µ ≡ >,
sp∗ means “non strategy-proof ” even if either #(E) = 2 or µ ≡ >, but “strategy-proof ” if
both #(E) = 2 and µ ≡ >. Finally, sp> means “non strategy-proof ” even if #(E) = 2, but
“strategy-proof ” whenever µ ≡ >.
As Theorem 7 shows, no operator among the ∆dµH ,GM ax and the 4C
µ ones ensures
full strategy-proofness in the restricted case where two complete bases are merged and
no integrity constraint is considered. Contrastingly, all the other operators offer strategyproofness for the three indexes whenever every base is complete.
5.4 Dalal Index
As explained before, the fact that ip is based on model counting allows some form of
graduality in the corresponding notion of satisfaction, and this contrasts with the drastic
indexes. Actually, other non drastic indexes can be defined. In particular, in cases where
the agent knows that the result could not fit her beliefs/goals (e.g., if her beliefs/goals are
62

The Strategy-Proofness Landscape of Merging

∆

ip

idw

ids

∆dµD ,f
∆d,Σ
µ
dH ,GM ax
∆µ
4C1
µ
4C3
µ
4C4
µ
4C5
µ
c
4C1
µ
c
4C3
µ
c
4C4
µ
c
4C5
µ

sp
sp
sp
sp
sp
sp
sp

sp
sp
sp
sp
sp>
sp
sp

sp
sp
sp∗
sp
sp>
sp
sp>

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

Table 3: Strategy-proofness: complete bases
.
not consistent with the integrity constraints), she still can be interested in achieving a result
that is close to her beliefs/goals. Closeness can be captured by a notion of distance, and a
possible satisfaction index is the following “Dalal index”:
Definition 11 (Dalal index) iDalal (K, K∆ ) = 1 −

min({dH (ω,K∆ ) | ω|=K})
.
#(P)

As far as we know, this index has never been carried out. For the sake of homogeneity
with previous indexes, the greater iDalal (K, K∆ ) the more satisfied the agent associated
with K. iDalal grows antimonotonically with the Hamming distance between the two bases
under consideration, i.e., the minimal distance between a model of the first base and a
model of the second one. Thus, this index takes its minimal value when every variable must
be flipped to obtain a model of K∆ from a model of K, while it takes its maximal value
whenever K is consistent with K∆ (no flip is required).
A direct observation is that iDalal (K, K∆ ) ≥ idw (K, K∆ ), whatever the bases K and
K∆ . Investigating the strategy-proofness of a profile E for the Dalal index given a merging
operator ∆ and an integrity constraint µ makes sense only in the situation K ∧ ∆µ (E) is
inconsistent. Indeed, in the remaining case, iDalal (K, ∆µ (E)) takes the maximal value 1
and no manipulation is possible.
In contrast to the three previous indexes we have considered, merging operators are not
strategy-proof for iDalal , even in very restricted situations.
Theorem 8 None of ∆dµD ,Σ , ∆dµD ,GM ax , ∆dµH ,Σ or ∆dµH ,GM ax is strategy-proof for iDalal ,
even in the restricted case where E consists of two complete bases.
C
Theorem 9 None of the 4C
µ operators (hence, none of the 4µ operators) is strategy-proof
for iDalal , even in the restricted case where E consists of two complete bases.
b

63

Everaere, Konieczny & Marquis

5.5 Restricted Strategies
There are situations where the other agents participating to the merging process have some
information about the bases of the other agents. For instance, in cooperative problem
solving, it can be decided that whenever an agent is able to answer a query within a
limited amount of time, she has to communicate it to the other agents. Contrastingly, the
communication protocol may force an agent to inform the other agents that she is definitely
not able to answer the query. Such information exchanges allow the other agents to get a
partial view of the models or the countermodels of the true beliefs/goals of the agent, and if
this conflicts with the reported beliefs/goals, the untruthful agent can be unmasked. That
is clearly a wrong thing for the untruthful agent since her opinion could then be ignored;
she can even be punished for her guilty behaviour.
We consider here two restrictions on available manipulations (and the corresponding
notions of strategy-proofness): erosion manipulation is when the agent pretends to believe/desire more that she does (the agent gives only some parts of its models); and dilatation manipulation is when the agent pretends to believe/desire less that she does (the agent
gives only parts of its countermodels).
Definition 12 (erosion and dilation)
• Erosion manipulation holds when the reported base K 0 is logically stronger than the
true one K: K 0 |= K
• Dilation manipulation holds when the reported base K 0 is logically weaker than the
true one K: K |= K 0 .
Erosion (resp. dilation) manipulation is safe for the untruthful agent when the other
agents may only have access to a subset of the countermodels (resp. models) of her true
beliefs/goals, while it is unsafe in general when the other agents may have access to a subset
of the models (resp. countermodels).
The next theorem gives the dilation strategy-proofness of model-based operators:
Theorem 10 Let d be a pseudo-distance and let f be an aggregation function. ∆d,f
is
µ
dilation strategy-proof for the indexes ip , idw and ids .
This result has to be compared with the ones in the unrestricted case (previous sections),
where most of the operators are not strategy-proof.
It is not the same story for erosion. One can easily find profiles that can be manipulated
using erosion manipulation (see the running example). Interestingly, focusing on erosion
strategy-proofness proves sufficient in some situations. Indeed, when d is any distance, Σ
is the aggregation function and any drastic index id are considered, ∆d,Σ
is strategy-proof
µ
for id if and only if it is erosion strategy-proof for id :
Theorem 11 Let d be any distance. If ∆d,Σ
is not strategy-proof for the index idw (resp.
µ
ids ), then it is not erosion strategy-proof for idw (resp. ids ).
This result has a corollary, showing that it it enough to focus on each complete base
that implies K to determine whether a profile E is manipulable by a base K for idw :
64

The Strategy-Proofness Landscape of Merging

Corollary 12 A profile E is manipulable by K for idw (resp. ids ) given ∆d,Σ
µ and µ if and
only if the manipulation is possible using a complete base Kω |= K, i.e., there exists Kω |= K
d,Σ
d,Σ
s.t. idw (K, ∆d,Σ
µ (E t {Kω })) > idw (K, ∆µ (E t {K})) (resp. ids (K, ∆µ (E t {Kω })) >
d,Σ
ids (K, ∆µ (E t {K}))).

6. Discussion
In this paper, we have drawn the strategy-proofness landscape for many merging operators,
including model-based ones and formula-based ones. While both families are not strategyproof in the general case, we have shown that several restrictions on the merging framework
or on the available strategies may lead to strategy-proofness.
As to model-based operators, the choice of a “right” distance appears crucial. Thus,
model-based operators are strategy-proof when based on the drastic distance, while they
are typically not strategy-proof when based on Dalal distance.
Among formula-based merging operators 4C1
µ achieves the highest degree of strategyproofness in the sense that it is strategy-proof for the drastic indexes.
Most of the results are summed up in Table 4 (sp means “strategy-proof” and sp means
“not strategy-proof”). For space reasons, the results on restricted strategies are not reported
there (see Section 5.5), as well as the ones concerning complete bases (see Table 3).
From the derived results, it appears that strategy-proofness is easier to achieve with
formula-based operators than with model-based ones, especially when the bases are singlec
tons (i.e., with the 4Ci operators). This could be explained by the fact that the latter
operators obey an all-or-nothing principle – a base is either selected as a whole (and included as such in a maxcons) or it is not selected at all – and this may forbid some subtle
manipulations.
We have also exhibited some restricted strategies that constrain the agent who wants
to manipulate. For example, all model-based operators are strategy-proof for dilation.
Most of the results of this paper are based on three satisfaction indexes, that are,
according to us, the most natural ones when no additional information about the merging
process is available. These three indexes share the property that if two bases are jointly
inconsistent, then the satisfaction is minimal. We call it the consistency property. In
order to handle scenarios where the consistency property is not discriminant enough, we
introduced the Dalal index. It turns out that none of the operators considered in the paper
is strategy-proof for this index.
The choice of a satisfaction index has (by definition) a major impact on the existence
of manipulation. As explained before, if the full preferences of all the agents over sets of
interpretations were available, strategy-proofness could be easily defined as in Social Choice
Theory: a manipulation occurs if and only if the merged base obtained when the agent lies
is strictly preferred (w.r.t. her own preference) to the the merged base obtained when she
reports her “true” base. When such information are not available, several choices for an
index are possible, capturing different intuitions.
When beliefs are to be merged, indexes satisfying the consistency property seem more
suited than the remaining ones; indeed, for the latter indexes, even if a merged base is “close”
to the agent’s beliefs, it is still not compatible with them. Drawing such a conclusion is not
so easy when goals are to be merged, since for instance, it can be the case that for some
65

Everaere, Konieczny & Marquis

i

#(E)

K

µ

d

∆µD

,f

d

∆µH

,Σ

d

∆µH

,Gmax

4C1
µ

4C3
µ

4C4
µ

4C5
µ

4C1
µ

4C3
µ

4C4
µ

4C5
µ

c

c

c

c

≡>

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>
not
complete
6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>
not
complete
6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>
not
complete
6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>
not
complete
6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>
not
complete
6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

≡>
not
complete
6≡ >

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

complete
=2

ip
complete
>2

complete
=2

idw
complete
>2

complete
=2

ids
complete
>2

Table 4: Synthesis of the results.

66

The Strategy-Proofness Landscape of Merging

agents, the more goals satisfied, the better. Nevertheless, our three satisfaction indexes idw ,
ids and ip are still meaningful in such scenarios, since an agent is typically more satisfied if
the goals of the group are compatible with her own ones than if it is not the case.
In light of our study, strategy-proofness appears as a property independent of the computational complexity of query answering from a merged base (see Konieczny et al., 2004).
It means that having a low/high complexity does not prevent/imply strategy-proofness.
Strategy-proofness appears also independent of the fact that the operator satisfies the
rationality postulates given by Konieczny and Pino Pérez (1999, 2002). Indeed, as a direct
consequence of Theorems 2 and 3, we have that some majority (resp. arbitration) merging
operators (Konieczny & Pino Pérez, 2002) are strategy-proof, while others are not. Thus,
satisfying the rationality postulates for merging proves not sufficient to ensure strategyproofness or manipulability. Nevertheless, we can note that arbitration operators, like
∆d,GM ax , are more sensitive to manipulation than majority operators, like ∆d,Σ . This is
easily explained by the fact that arbitration operators are egalitarist ones: they aim at
giving a result that is close to each base of the profile. Intuitively, a small change in a
base can heavily change the whole result. Contrastingly, majority operators, that listen to
majority wishes for defining the merged base, often do not take into account bases that are
“far” from the majority. So, when using majority operators, it is likely that a small change
in a base has no impact on the merged base.
Thus, strategy-proofness can be viewed as a further dimension that can be used to
compare merging operators, besides the computational complexity and rationality criteria.
Its independence to the latter criteria may also explain why no strategy-proofness results
for wide families of operators seem to exist.

7. Related Work
As explained through this paper, the manipulation problem has been studied extensively
in Social Choice Theory for years. In the next subsection we will relate our work to this
stream of research. In a second subsection we mention some other related work concerning
the strategy-proofness issue for weighted bases.
7.1 Social Choice Theory
In the propositional merging framework considered in the paper, the beliefs/goals K of each
agent induce a two-strata partition of the interpretations: all the models of K are equally
preferred, and strictly preferred to its countermodels, which are equally disliked. When
agents report full preference relations (that can be encoded in various ways, e.g., explicitly,
or by a prioritized base, an ordinal conditional function, etc.), the aggregation problem
consists in defining a global preference relation from individual preference relations. This
problem has been addressed for centuries in Social Choice Theory. This can be traced back
at least to Condorcet (1785) and Borda (1781).
In Social Choice Theory (Arrow et al., 2002), the strategy-proofness problem has received great attention. In this framework, an agent A is untruthful when she reports a
preference relation (a complete pre-order over the set of alternatives) that is not the true
one. A social choice function (associating an alternative to a profile of such preference
relations) is not strategy-proof when the alternative chosen by the function when A lies
67

Everaere, Konieczny & Marquis

is ranked higher for A than the alternative chosen when she reports her true preferences.
One of the most famous result in Social Choice Theory is that there is no “good” strategyproof preference aggregation procedure. This result is known as Gibbard-Satterthwaite
impossibility theorem (Gibbard, 1973; Satterthwaite, 1975; Moulin, 1988).
Formally, consider a set of agents (individuals) N = {1, . . . , n}, and a set of alternatives
A = {a, b, . . .}. Each agent i has a preference relation on those alternatives, that is supposed
to be a complete, reflexive and transitive binary relation, noted ≥i . A preference profile
P = (≥1 , . . . , ≥n ) assigns a preference relation to each agent. Let us note P the set of
all possible preference profiles. A given preference profile can be noted P = (P−i , ≥i ),
i ∈ N , where P−i denotes the profile P without (the preferences of) individual i. A social
choice function f is a mapping from P to A. A social choice function is manipulable if
there is an individual i ∈ N , a preference relation ≥0 , and a preference profile P such that
f (P−i , ≥0i ) >i f (P ), i.e., when there is an agent i that is best satisfied with the result when
she claims her preferences are ≥0i instead of her true preference ≥i . If a social choice function
is not manipulable, it is said to be strategy-proof. A social choice function is dictatorial if
there is an individual i ∈ N (the dictator), such that f (P ) ≥i a for all a ∈ A and for all
P ∈ P. A social choice function is onto if for each alternative a ∈ A there is a preference
profile P ∈ P such that f (P ) = a. Then Gibbard-Satterthwaite theorem (Gibbard, 1973;
Satterthwaite, 1975) can be stated as:
Theorem 13 (Gibbard, 1973; Satterthwaite, 1975) If A contains at least three alternatives, then there is no social choice function f that is onto, strategy-proof and non-dictatorial.
Since this result has been stated, there has been a lot of work for deriving strategyproofness results under some restrictions (see Kelly, 1988; Arrow et al., 2002). In some
sense, our work is relevant to such approaches. Nonetheless, our work is original - as far as
we know - from two points of view: on the one hand, the preference relations considered
here are two-strata total pre-orders, and not arbitrary pre-orders; on the other hand, the
result of a merging process is usually not a single interpretation but still a two-strata total
pre-order (and the number of models of the merged base is not constrained a priori). This
leads to more complex notions of strategy-proofness where different definitions are possible,
depending on an index which formalizes one of the various intuitive notions of “how satisfied
an agent is by the result of the merging process”.
In social choice theory, there are also some works on social choice correspondences, that
are mapping from P to 2A , and that are closer to our framework than social choice functions. The data coming from individuals are preference relations on A, and the problem is
to shift them to preference relations on 2A . The standard way to achieve it is to consider
that even if a set is chosen by the correspondence, then ultimately only one alternative will
be realized, and to suppose that each individual has a subjective probability measure on
this realization. Then a social choice correspondance is strategy-proof if it is not possible
for an individual to increase her expected utility of the result. In this case, results similar to Gibbard-Satterthwaite theorems can be derived (see e.g., Barberà, Dutta, & Sen,
2001; Chin & Zhou, 2002; Duggan & Schwartz, 2000). These works are related to the one
conducted in this paper, but they all suppose that each agent makes available not only its
full preference relation on alternatives under the form of a utility function – typically not
reducible to a two-strata complete pre-order – but also its subjective probability measure
68

The Strategy-Proofness Landscape of Merging

on alternatives. Contrastingly, in this work, the only information coming from each agent
is the corresponding base, which typically approximates her full preference relation, and is
of pure ordinal nature.
7.2 Strategy-Proofness for Weighted-Bases Merging
A study of strategy-proofness of some merging operators has been carried out by Meyer,
Ghose, and Chopra (2001). The framework they consider is distinct from the one used
in our work. On the one hand, agents may report full preference relations (encoded as
ordinal conditional functions, also called κ-functions, see Spohn, 1987). On the other hand,
the merging operators under consideration escape Gibbard-Satterthwaite theorem since
Meyer et al. (2001) make a commensurability assumption between the agents’ preference
relations (the same remark applies also to possibilistic base merging as defined by Benferhat,
Dubois, Kaci, & Prade, 2002). Roughly, this commensurability assumption amounts to
consider that the weights (or the levels) associated to formulas have the same meaning
for all the agents, i.e., that a weight 3 for agent 1 is the same that a weight 3 for agent
2. The commensurability assumption is sensible in many situations, but when dealing
with agents’ preferences, commensurability must be used carefully. For human agents, it
is commonly accepted in Social Choice Theory that this assumption is very strong. Arrow
(1963) illustrates this idea by quoting Bentham:
“ This is vain to talk of adding quantities which after the addition will continue
distinct as they were before, one man’s happiness will never be another man’s
happiness: a gain to one man is no gain to another; you might as well pretend
to add 20 apples to 20 pears...”
The notion of strategy-proofness and the merging operators of Meyer et al. (2001) and
Chopra, Ghose, and Meyer (2006) are defined in the framework of ordinal conditional functions. In this section, we study the corresponding operators in the pure propositional
framework, i.e., when the profile contains “flat” belief/goal bases, in order to compare them
with our approach.
An ordinal conditional function (OCF) κ is a total function from the set of interpretations W to the set of non-negative integers (originally, an OCF maps an interpretation
to the class of ordinals, and is such that at least one interpretation is mapped to zero,
but considering integers is sufficient here). Intuitively, the greater the number, the less
credible the interpretation. To each OCF κ one can associate a base Bel(κ) defined as
[Bel(κ)] = {ω ∈ W | κ(ω) = minω0 ∈W (κ(ω 0 ))}. The aim of OCF merging operators is,
from a profile of OCFs E = {κ1 , . . . , κn } to define an OCF κ∆ (E) that best represents the
profile. The operators studied by Meyer et al. (2001) are the following ones:
• κ∆max (E)(ω) = maxκi ∈E κi (ω),

2κ1 (ω) if κi (ω) = κj (ω) for all κi , κj ∈ E
• κ∆min1 (E)(ω) =
2 minκi ∈E κi (ω) + 1 otherwise,

κ1 (ω) if κi (ω) = κj (ω) for all κi , κj ∈ E
• κ∆min2 (E)(ω) =
minκi ∈E κi (ω) + 1 otherwise,
69

Everaere, Konieczny & Marquis

• κ∆Σ (E)(ω) =

P

κi ∈E

κi (ω).

The straightforward way to translate the framework of propositional merging into ordinal conditional functions is to consider a propositional base as a special case of OCF: a
propositional base is a two-strata OCF, with the models of the bases having rank 0 and the
countermodels having rank 1. If we consider only two-strata OCFs κi and note Ki = Bel(κi )
and ∆ = Bel(κ∆ ), the previous definitions of merging operators give:
V
• ∆max (E) ≡ ∆min2 (E) ≡ E if consistent and ∆max (E) ≡ ∆min2 (E) ≡ > otherwise.
V
W
• ∆min1 (E) ≡ E if consistent and ∆min1 (E) ≡ E otherwise.
• ∆Σ (E) ≡ ∆dD ,Σ (E).
The resulting propositional merging operators ∆max , ∆min1 , ∆min2 , and ∆Σ are quite
simple and well-known. ∆max (or equivalently ∆min2 ) is the so-called basic merging operator (in absence of integrity constraints) (Konieczny & Pino Pérez, 1999). ∆min1 is the
1-quota operator defined by Everaere, Konieczny, and Marquis (2005) (without integrity
constraints). ∆Σ corresponds to the intersection operator defined by Konieczny (2000).
All those operators are strategy-proof for our indexes:
Theorem 14 ∆max , ∆min1 , ∆min2 , and ∆Σ are strategy-proof for idw , ids and ip .
Besides those operators, Meyer, Chopra and Ghose also proposed general definitions of
strategy-proofness for OCF merging. More precisely, they have studied two properties. The
first one is the (IP) property (Meyer et al., 2001):
Definition 13 (IP) An OCF merging operator κ∆ satisfies the (IP) property if and only
if for every OCF profile E, for every agent i, we have whatever the OCF κ
∀ω ∈ W, |κ∆ (E)(ω) − κi (ω)| ≤ |κ∆ (rep(E, {i}, κ)(ω) − κi (ω)|
where rep(E, {i}, κ) is the profile identical to E except that the OCF κi is replaced by κ.
Focusing on two-strata OCFs, we say that a merging operator ∆(= Bel(κ∆ )) is strategyproof for (IP) if and only if κ∆ satisfies the (IP) property for every agent given any profile.
We have obtained the following characterization:
Theorem 15 ∆ is strategy-proof for (IP) if and only if for every profile E and every pair
of bases K and K 0 :
• K ∧ ¬∆(E t {K}) |= ¬∆(E t {K 0 }), and
• ¬K ∧ ∆(E t {K}) |= ∆(E t {K 0 }).
The second strategy-proofness property that Meyer, Chopra and Ghose have investigated is (WIP):
70

The Strategy-Proofness Landscape of Merging

Definition 14 (WIP) An OCF merging operator κ∆ satisfies the (WIP) property if and
only if for every profile E, for every agent i, we have whatever the OCF κ:
Σω∈W |κ∆ (E)(ω) − κi (ω)| ≤ Σω∈W |κ∆ (rep(E, {i}, κ)(ω) − κi (ω)|.
(WIP) is weaker than (IP) in the sense that if an OCF merging operator κ∆ satisfies
(IP) for an agent i, then κ∆ satisfies (WIP) for i (but the converse does not always hold).
Again, focusing on two-strata OCFs, we say that a merging operator ∆(= Bel(κ∆ )) is
strategy-proof for (WIP) if and only if κ∆ satisfies the (WIP) property for every agent
given any profile.
V
V
V
V
Let us note ⊕ the exclusive or operator, i.e., K ⊕ K 0 = ( K ∧ ¬ K 0 ) ∨ (¬ K ∧ K 0 ).
Then (WIP) can be characterized in our framework by :
Theorem 16 Let iwip (K, K∆ ) =
if it is strategy-proof for iwip .

1
#([K⊕K∆ ])+1 .

∆ satisfies the (WIP) property if and only

Note that the “wip index” iwip is very close to the probabilistic index ip . The probabilistic index measures the closeness of the merged base to the agent base, whereas the ”wip
index” measures the difference between the merged base and the agent base.
However, the corresponding notion of strategy-proofness (and a fortiori the one induced
by (IP)) appears too strong in the pure propositional setting. Consider the following belief
merging scenario:
Example 6 Consider K = a and K1 = b.We have ∆dH ,Σ ({K, K1 }) ≡ a ∧ b. If the
agent gives K 0 = {a ∧ ¬b} instead of K, then the merged base is ∆dH ,Σ ({K 0 , K1 }) ≡ a.
Accordingly, this an example of manipulation for (WIP) (iwip (∆dH ,Σ ({K, K1 })) = 12 <
iwip (∆dH ,Σ ({K 0 , K1 })) = 1).
In this example, the untruthful agent actually manages to change the merged base to
one which is more similar to her initial base (with respect to iwip ). This is because she is
not fully satisfied by the merged base equivalent to a ∧ b but still strictly prefers her initial
base {a}, despite the fact that a ∧ b refines her own beliefs. Accordingly, such an agent
wants to preserve both her beliefs and her ignorance. In many scenarios when an agent
participates to a merging process in order to get new information, this is counter-intuitive.
Chopra et al. (2006) give more general definitions of strategy-proofness, by considering other
similarity relations. In the propositional case, they all suffer from the same above-mentioned
drawback. This explains why we did not investigate the strategy-proofness of the purely
propositional model-based operators and formula-based operators for criteria like (WIP)
or (IP).

8. Conclusion
Investigating the strategy-proofness of merging operators is important from a multi-agent
perspective whenever some agents can get the information conveyed by the other agents
participating to the merging process. When strategy-proofness is not guaranteed, it may be
questioned whether the result of the merging process actually represents the beliefs/goals
of the group.
71

Everaere, Konieczny & Marquis

In this paper, we have drawn the strategy-proofness landscape for many existing merging
operators, including model-based ones and formula-based ones, both in the general case and
under several natural restrictions. Strategy-proofness appears as independent of complexity
and rationality aspects, and can be used as such, as a further criterion to evaluate merging
operators. All those results have been discussed in Section 6.
This work calls for a number of perspectives. A first perspective is to identify the complexity of determining whether a profile can be manipulated by a base given an operator.
Indeed, using a merging operator that is not strategy-proof is not necessarily harmful if finding out a strategy is computationally hard. Such a complexity issue has been investigated for
voting schemes (Conitzer & Sandholm, 2003; Conitzer, Lang, & Sandholm, 2003; Conitzer
& Sandholm, 2002a, 2002b) when individual preferences are given explicitly (which is not
the case in our framework). A first result follows easily from Theorem 11: if the distance d
between interpretations can be computed in polynomial time in the input size (which is not
a strong assumption), determining whether a given profile can be manipulated by a base
p
for a drastic index given ∆d,Σ
µ and µ is in Σ2 .
In Social Choice Theory, the Gibbard-Sattertwhaite theorem states that every “sensible”
social choice function is manipulable. Taking into account the fact that the agents are
tempted to manipulate transforms the aggregation process into a game between agents.
For ensuring strategy-proofness, it can prove sufficient to build a game where telling the
truth is an optimal strategy for each agent. How to achieve it is the aim of implementation
theory (also called mechanism design), see e.g., Maskin & Sjostrom, 2002. A perspective
is to determine whether building such mechanisms is possible in a belief merging setting in
order to force the agents to tell the truth. Most of the work on mechanism design assume
transferable utility, and use payments as part as the process. Importing such ideas in a
fully qualitative framework surely is a hard task.
Another interesting perspective is to study the strategy-proofness problem when coalitions are allowed. Instead of considering manipulation by single agents, one can be interested in manipulation by coalition of agents who coordinate to improve the result for the
coalition. See (Meyer et al., 2001; Chopra et al., 2006) for such a definition in a different
framework. Since manipulation by a single agent is a particular case of manipulation by a
coalition, and since we have seen that many operators are not strategy-proof for a single
agent, it is clear that strategy-proofness results for coalitions will be very hard to achieve.

Acknowledgements
The authors would like to thank the anonymous referees for their thoughful comments which
helped us a lot to improve the paper. The authors have been supported by the Université
d’Artois, the Région Nord/Pas-de-Calais, the IRCICA Consortium, and by the European
Community FEDER Program.

72

The Strategy-Proofness Landscape of Merging

Appendix A. Proofs
Theorem 1
1. If a merging operator is strategy-proof for ip , then it is strategy-proof for idw .
2. Consider a merging operator ∆ that generates only consistent bases.5 If it is strategyproof for ip , then it is strategy-proof for ids .
Proof:
1. Assume that ∆µ is not strategy-proof for idw . Then there exists a profile E, a base
K, a base K 0 and an integrity constraint µ s.t. (1) ∆µ (E t {K}) ∧ K is inconsistent,
#([K]∩[∆µ (Et{K})])
= 0. (2)
and (2) ∆µ (E t {K 0 }) ∧ K is consistent. (1) implies that #([∆µ (Et{K})])
implies that

#([K]∩[∆µ (Et{K 0 })])
#([∆µ (Et{K 0 })])

> 0. Hence, ∆µ is not strategy-proof for ip .

2. Assume that ∆µ is not strategy-proof for ids . Then there exists a profile E, a base
K, a base K 0 and an integrity constraint µ s.t. (1) ∆µ (E t {K}) 6|= K, and (2)
#([K]∩[∆µ (Et{K})])
∆µ (E t {K 0 }) |= K. (1) implies that #([∆µ (Et{K})])
6= 1. (2) implies that
#([K]∩[∆µ (Et{K 0 })])
#([∆µ (Et{K 0 })])

= 1 if ∆µ (E t {K 0 }) is consistent. Hence, ∆µ is not strategy-

proof for ip .


Theorem 2
• Let f be any aggregation function. ∆dµD ,f is strategy-proof for ip , idw and ids .
• Let d be any distance. Provided that only two bases are to be merged, ∆d,Σ
> is strategyproof for the indexes idw and ids .
• For any distance d, ∆d,Σ
is strategy-proof for the indexes ip , idw and ids when the
µ
initial base K is complete.
Proof:
• Let f be any aggregation function. ∆dµD ,f is strategy-proof for ip , idw and ids .
The proof is organized in three steps: by reduction ad absurdum, we show that the
minimal drastic distance between a model of µ and E t {K} is equal to the minimal
drastic distance between a model of µ and E t {K 0 }. Then, it is easy to show that
the number of K’s models is greater in E t {K} than in E t {K 0 }. Finally, we prove
that the number of countermodels of K is greater in E t {K 0 } than in E t {K}, which
entails a contradiction.
5. I.e., ∆µ (E) is always consistent.

73

Everaere, Konieczny & Marquis

From Theorem 1, we know that if any operator ∆dµD ,f is strategy-proof for ip , it is also
strategy-proof for both idw and ids (indeed, ∆dµD ,f (E) is always consistent). So it is
sufficient to prove the strategy-proofness of ∆dµD ,f for ip to prove its strategy-proofness
for the three indexes.
Let us prove it by reductio ad absurdum: assume that there is an operator ∆dµD ,f , where
dD is the drastic distance and f is any aggregation function, that is not strategy-proof
for ip . Then there exist an integrity constraint µ, a profile E, and two bases K and
K 0 s.t. ip (K, ∆dµD ,f ({K} t E)) < ip (K, ∆dµD ,f ({K 0 } t E)), which is equivalent to
#([K] ∩ [E 4dµD ,f K])
#([E 4dµD ,f K])

<

#([K] ∩ [E 4dµD ,f K 0 ])
#([E 4dµD ,f K 0 ])

where E 4dµD ,f K is a light notation for ∆dµD ,f ({K} t E)). Let us note dmin (E tdµD ,f
{K}) = min({dD (ω, E t {K}) | ω |= µ}, ≤). We now show that dmin (E tdµD ,f {K}) =
dmin (E tdµD ,f {K 0 }):
– Let us first notice that we have ip (K, E 4dµD ,f K) 6= 1: if ip (K, E 4dµD ,f K) = 1,
then the probabilistic satisfaction index takes its maximal value, so it is impossible to increase it.
Since ip (K, E 4dµD ,f K) < 1, we have that #([K]∩[E 4dµD ,f K]) < #([E 4dµD ,f K]),
so at least one model of E 4dµD ,f K does not belong to K:
∃ω1 |= (¬K) ∧ µ, dD (ω1 , E t {K}) = dmin (E tdµD ,f {K}).
Since ω1 |= (¬K) ∧ µ, we have dD (ω1 , K) = 1 and this distance is maximal
(because we use the drastic distance). We get immediately that dD (ω1 , K) ≥
dD (ω1 , K 0 ). Hence dD (ω1 , E t{K}) ≥ dD (ω1 , E t{K 0 }) (because the aggregation
function f satisfy non-decreasingness). Since dD (ω1 , E t {K}) = dmin (E tdµD ,f
{K}), we get dmin (E tdµD ,f {K}) ≥ dD (ω1 , E t {K 0 }). Since dD (ω1 , E t {K 0 }) ≥
dmin (E tdµD ,f {K 0 }) by definition of min and since ω1 |= µ, we have
dmin (E tdµD ,f {K}) ≥ dmin (E tdµD ,f {K 0 }) (∗).
– We can also conclude that ip (K, E 4dµD ,f K 0 ) 6= 0: if ip (K, E 4dµD ,f K 0 ) = 0, then
ip (K, E 4dµD ,f K 0 ) is minimal, so the value taken by ip has not increased, and
this contradicts the assumption (manipulation).
If ip (K, E 4dµD ,f K 0 ) 6= 0, then we can find at least one model of K ∧ µ in
E 4dµD ,f K 0 : ∃ω1 |= K ∧µ, dD (ω1 , E t{K 0 }) = dmin (E tdµD ,f {K 0 }). Since ω1 |= K,
we have dD (ω1 , K) = 0 and since this distance is minimal, we get dD (ω1 , E t
{K}) ≤ dD (ω1 , E t {K 0 }), and then dD (ω1 , E t {K}) ≤ dmin (E tdµD ,f {K 0 }),
because dD (ω1 , E t {K 0 }) = dmin (E tdµD ,f {K 0 }). Furthermore, since dD (ω1 , E t
{K}) ≥ dmin (E tdµD ,f {K}) by definition of min and because ω1 |= µ, we have:
dmin (E tdµD ,f {K}) ≤ dmin (E tdµD ,f {K 0 }) (∗∗).
74

The Strategy-Proofness Landscape of Merging

From the inequations (*) and (**), we get:
dmin (E tdµD ,f {K}) = dmin (E tdµD ,f {K 0 }).

(1)

Let us show now that we can only increase the number of countermodels of K in
E 4dµD ,f K 0 , and decrease the number of models of K in E 4dµD ,f K 0 .
– Let ω be a countermodel of K which is a model of E 4dµD ,f K: ω |= (¬K) ∧
(E 4dµD ,f K).
Since ω |= ¬K, we have dD (ω, K) = 1 and this distance is maximal. Hence
dD (ω, K) ≥ dD (ω, K 0 ). So:
dD (ω, E t {K}) ≥ dD (ω, E t {K 0 })

(2)

because the aggregation function f satisfies non-decreasingness.
Since ω |= E 4dµD ,f K, we have dD (ω, E t {K}) = dmin (E tdµD ,f {K}). With (2),
we get dmin (E tdµD ,f {K}) ≥ dD (ω, E t {K 0 }).
Since dmin (E tdµD ,f {K}) = dmin (E tdµD ,f {K 0 }) with (1), we obtain: dmin (E tdµD ,f
{K 0 }) ≥ dD (ω, E t {K 0 }). By definition of min and since ω |= µ (because ω |=
E 4dµD ,f K), we deduce that ω is a model of E 4dµD ,f K 0 . We can conclude that
every model of E 4dµD ,f K which is not a model of K is a model of E 4dµD ,f K 0 .
Hence: [¬K] ∩ [E 4dµD ,f K] ⊆ [¬K] ∩ [E 4dµD ,f K 0 ].
– Finally, let ω be a model of K which is a model of E 4dµD ,f K 0 : ω |= K ∧ (E 4dµD ,f
K 0 ). Since ω |= K, we have dD (ω, K) = 0 and this distance is minimal. Hence
dD (ω, K) ≤ dD (ω, K 0 ). So:
dD (ω, E t {K}) ≤ dD (ω, E t {K 0 })

(3)

because the aggregation function is non-decreasing.
Since ω |= E 4dµD ,f K 0 , we have dD (ω, E t {K 0 }) = dmin (E tdµD ,f {K 0 }). With
(3), we get dD (ω, E t {K} ≤ dmin (E tdµD ,f {K 0 }). Since dmin (E tdµD ,f {K}) =
dmin (E tdµD ,f {K 0 }) with (1), we obtain dD (ω, E t {K} ≤ dmin (E tdµD ,f {K}). By
definition of min and since ω |= µ (because ω |= E 4dµD ,f K 0 ), we deduce that ω
is a model of E 4dµD ,f K. We can conclude that every model of E 4dµD ,f K 0 which
is a model of K is a model of E 4dµD ,f K. It follows that [K] ∩ [E 4dµD ,f K 0 ] ⊆
[K] ∩ [E 4dµD ,f K].
Since we can only increase the number of countermodels of K in E 4dµD ,f K 0 and
decrease the number of models of K in E 4dµD ,f K 0 , the proportion of models of K in
E 4dµD ,f K 0 is smaller than in E 4dµD ,f K. This contradicts the assumption and shows
that ∆dµD ,f is strategy-proof for ip .
• Let d be any distance. Provided that only two bases are to be merged, ∆d,Σ
is
>
strategy-proof for the indexes idw and ids .
75

Everaere, Konieczny & Marquis

In this proof, we first show that the merging of two bases is consistent with each base.
Then, the property follows directly.
Strategy-proofness for the two drastic indexes is a direct consequence of the following
property:
d,Σ
Lemma 1 If E = {K1 , K2 }, then ∆d,Σ
> (E) ∧ K1 and ∆> (E) ∧ K2 are consistent.

Proof:
We show that ∆d,Σ
> (E) ∧ K1 is consistent (the remaining case is similar
by symmetry). Reductio ad absurdum. Let us suppose that for two bases K1 and K2 ,
∆d,Σ
> ({K1 , K2 }) is inconsistent with K1 . We can deduce that:
∃ω 0 |= ¬K1 , ∀ω |= K1 , d(ω, K1 4d,Σ K2 ) > d(ω 0 , K1 4d,Σ K2 ),
where K1 4d,Σ K2 is a light notation for ∆d,Σ
> ({K1 , K2 }).
Since ∀ω |= K1 , d(ω, K1 ) = 0, we get that ∃ω 0 |= ¬K1 , ∀ω |= K1 , d(ω, K2 ) >
d(ω 0 , K1 ) + d(ω 0 , K2 ). In particular, if we consider ω1 |= K1 s.t. d(ω 0 , ω1 ) = d(ω 0 , K1 )
(such an ω1 exists by definition of d(ω 0 , K1 )), we have: d(ω1 , K2 ) > d(ω 0 , ω1 ) +
d(ω 0 , K2 ). Similarly, if we consider ω2 |= K2 s.t. d(ω 0 , ω2 ) = d(ω 0 , K2 ), we get:
d(ω1 , K2 ) > d(ω 0 , ω1 ) + d(ω 0 , ω2 ) (∗).
By definition of d, we have ∀ω |= K2 , d(ω1 , K2 ) ≤ d(ω1 , ω); in particular, d(ω1 , K2 ) ≤
d(ω1 , ω2 ). By transitivity of ≤, and with (*), we get d(ω1 , ω2 ) > d(ω 0 , ω1 ) + d(ω 0 , ω2 ).
This contradicts the triangular inequality.

Let us now prove the main theorem:
Weak drastic index. For two bases K1 and K2 , we always have idw (K1 , K1 4 K2 ) = 1,
because ∆d,Σ
> ({K1 , K2 }) ∧ K1 is consistent (Lemma 1), so no manipulation is possible
(idw is maximal).
0
Strong drastic index. If ∆d,Σ
> is not strategy-proof, then we can find K1 s.t.:
d,Σ
0
ids (K1 , ∆d,Σ
> ({K1 , K2 }) < ids (K1 , ∆> ({K1 , K2 }).

For the strong drastic index, this means exactly that:
∆d,Σ
> ({K1 , K2 }) 6|= K1

(4)

0
∆d,Σ
> ({K1 , K2 }) |= K1 .

(5)

and:
0
Since ∆d,Σ
> ({K1 , K2 }) ∧ K2 is consistent (Lemma 1), we can find ω2 |= K2 s.t. ω2 |=
0
∆d,Σ
> ({K1 , K2 }). With (5), we can conclude that ω2 |= K1 as well.

Since we have ω2 |= K1 ∧K2 , we can conclude that for every model ω of ∆d,Σ
> ({K1 , K2 }),
d,Σ
we have d(ω, {K1 , K2 }) = 0. So ∀ω |= ∆> ({K1 , K2 }), d(ω, K1 ) = d(ω, K2 ) = 0.
Hence ∀ω |= ∆d,Σ
> ({K1 , K2 }), ω |= K1 ∧ K2 . This contradicts (4), so no manipulation
is possible.
76

The Strategy-Proofness Landscape of Merging

• For any distance d, ∆d,Σ
is strategy-proof for the indexes ip , idw and ids when the
µ
initial base K is complete.
For the drastic indexes, the result is a consequence of Theorem 11, showing that if a
manipulation occurs with an initial base K, then a manipulation with a complete base
Kω |= K is possible. If K is complete, no such manipulation not possible.
For the probabilistic index, the result is a consequence of the triangular inequality.
Drastic indexes. The property is a direct consequence of Theorem 11, showing that
if ∆d,Σ
µ is manipulable for idw and ids by a base K, then it is manipulable by erosion.
But manipulation by erosion is impossible whenever K is complete.
Probabilistic index. By reductio ad absurdum: let us suppose that there is an operator
∆d,Σ
µ , where d is any distance, that is manipulable for ip given a complete base K =
{ω1 }. So, there exists an integrity constraint µ, a profile E, and a base K 0 s.t.:
d,Σ
0
ip ({ω1 }, ∆d,Σ
µ ({ω1 } t E)) < ip ({ω1 }, ∆µ ({K } t E)).
d,Σ
If ip ({ω1 }, ∆d,Σ
µ ({ω1 } t E)) = 0, then idw ({ω1 }, ∆µ ({ω1 } t E)) = 0 too. In that case,
manipulation for ip implies manipulation for idw and we have seen that no manipulation is possible for idw . As a consequence, we can suppose that ip ({ω1 }, ∆d,Σ
µ ({ω1 } t
E)) 6= 0. Equivalently:
#({ω1 } ∩ [E 4Σ
µ {ω1 }])
6= 0
Σ
#([E 4µ {ω1 }])
d,Σ
(where E 4Σ
µ {ω1 } is a light notation for ∆µ ({ω1 } t E)).
This statement allows us to infer that ω1 is a model of E 4Σ
µ {ω1 }. In order to
d,Σ
0
0
increase ip ({ω1 }, ∆µ (K t E)), we have to reduce the number of models of E 4Σ
µ K
Σ
Σ
0
compared to E 4µ {ω1 }, without removing ω1 from [E 4µ K ]. So we have to find
Σ
0
ω2 6= ω1 s.t. ω2 |= E 4Σ
µ {ω1 } and ω2 6|= E 4µ K . So, ω2 |= µ and we have d(ω2 , E t
0
{ω1 }) = d(ω1 , E t {ω1 }) and: d(ω2 , E t {K }) > d(ω1 , E t {K 0 }) (because ω1 is a
Σ
0
model of both E 4Σ
µ {ω1 } and E 4µ K ). With the aggregation function Σ, we get:
d(ω2 , ω1 ) + d(ω2 , E) = d(ω1 , E) and d(ω2 , K 0 ) + d(ω2 , E) > d(ω1 , K 0 ) + d(ω1 , E).

Replacing d(ω1 , E) by d(ω2 , ω1 )+d(ω2 , E), we obtain d(ω2 , K 0 )+d(ω2 , E) > d(ω1 , K 0 )+
d(ω2 , ω1 ) + d(ω2 , E), so d(ω2 , K 0 ) > d(ω1 , K 0 ) + d(ω2 , ω1 ). If ω10 is a model of K 0 s.t.
d(ω1 , K 0 ) = d(ω1 , ω10 ), then we have d(ω2 , K 0 ) > d(ω1 , ω10 ) + d(ω2 , ω1 ). Furthermore by
definition of min, we have d(ω2 , ω10 ) ≥ d(ω2 , K 0 ), so d(ω2 , ω10 ) > d(ω1 , ω10 ) + d(ω2 , ω1 )
which contradicts the triangular inequality.

Theorem 3
• ∆dµH ,Σ is strategy-proof for idw or ids if and only if (µ ≡ > and #(E) = 2) or K is
complete.
• ∆dµH ,Σ is strategy-proof for ip if and only if K is complete.
77

Everaere, Konieczny & Marquis

Proof:
Theorem 2 entails straightforwardly the ⇐ part of the proof, taking the
Hamming distance dH for d.
For the ⇒ part of the proof, we shall show by examples of manipulation that ∆dµH ,Σ is not
strategy-proof in other cases.
• The first examples show that ∆dµH ,Σ is not strategy-proof for idw or ids if (µ 6≡ > or
#(E) 6= 2), and if K is not complete.
Weak drastic index.
– idw and µ 6≡ > (K is not complete)
We consider the constraint µ = a∨b and the two bases K1 and K2 defined by their
set of models: [K1 ] = {00, 01} and [K2 ] = {10}. We have [∆dµH ,Σ ({K1 , K2 })] =
{10} and idw (K1 , ∆dµH ,Σ ({K1 , K2 })) = 0. On the other hand, if the agent whose
base is K1 gives K10 , with [K10 ] = {01} instead of K1 , we obtain [∆dµH ,Σ ({K10 ,
K2 })] = {01, 10, 11} and idw (K1 , ∆dµH ,Σ ({K10 , K2 }) = 1. This example shows the
manipulability of ∆dµH ,Σ if µ 6≡ >, even if there are only two bases in the profile.
Computations are detailed in Table 5. Interpretations that do not satisfy the
constraint are shaded.
ω
00
01
10
11

dH (ω, K1 )
0
0
1
1

dH (ω, K10 )
1
0
2
1

dH (ω, K2 )
1
2
0
1

d

∆µH

,Σ

d

∆µH

({K1 , K2 })
1
2
1
2

,Σ

({K10 , K2 })
2
2
2
2

Table 5: Manipulability of ∆dµH ,Σ for idw with µ 6≡ >.
– idw and #(E) 6= 2 (K is not complete)
Let us consider the three bases [K1 ] = {00, 10}, [K2 ] = {01, 10, 11} and [K3 ] =
{01}. Then ∆d>H ,Σ ({K1 , K2 , K3 }) has a unique model 01 and idw (K1 , ∆d>H ,Σ ({K1 ,
K2 , K3 }) = 0. If we consider now [K10 ] = {10} instead of K1 , then [∆d>H ,Σ ({K10 ,
K2 , K3 })] = {01, 10, 11} and idw (K1 , ∆dH ,Σ ({K10 , K2 , K3 }) = 1. See Table 6.
ω
00
01
10
11

K1
0
1
0
1

K10
1
2
0
1

K2
1
0
0
0

K3
1
0
2
1

d

∆>H

,Σ

({K1 , K2 , K3 })
2
1
2
2

d

∆>H

,Σ

({K10 , K2 , K3 })
3
2
2
2

Table 6: Manipulability of ∆d>H ,Σ for idw with #(E) 6= 2.
Strong drastic index.
78

The Strategy-Proofness Landscape of Merging

– ids and µ 6≡ > (K is not complete)
We consider the constraint µ = (a ∧ b) ∨ (a ∧ ¬b ∧ ¬c) and the two bases K1 and
K2 defined by their sets of models: [K1 ] = {000, 111} and [K2 ] = {000, 001}. We
have [∆dµH ,Σ ({K1 , K2 })] = {111, 100} and ids (K1 , ∆dµH ,Σ ({K1 , K2 })) = 0. On the
other hand, if the agent whose base is K1 gives K10 , with [K10 ] = {111} instead
of K1 , we obtain [∆dµH ,Σ ({K10 , K2 })) = {111} and ids (K1 , ∆dµH ,Σ ({K10 , K2 }) = 1.
This example shows the manipulability of ∆dµH ,Σ for ids if µ 6≡ >, even if there
are only two bases in the profile. Details of the computation are reported in
Table 7.
ω
000
001
010
011
100
101
110
111

K1
0
1
1
1
1
1
1
0

K10
3
2
2
1
2
1
1
0

K2
0
0
1
1
1
1
2
2

d

∆µH

,Σ

({K1 , K2 })
0
1
2
2
2
2
3
2

d

∆µH

,Σ

({K10 , K2 })
3
2
3
2
3
2
3
2

Table 7: Manipulability of ∆dµH ,Σ for ids with µ 6≡ >.
– ids and #(E) 6= 2 (K is not complete)
Let us consider the three bases [K1 ] = {000, 001, 111}, [K2 ] = {110, 001} and
[K3 ] = {110, 000}. Then [∆d>H ,Σ ({K1 , K2 , K3 })] = {000, 001, 110} and
ids (K1 , ∆d>H ,Σ ({K1 , K2 , K3 }) = 0.
If we consider [K10 ] = {000, 001} instead of K1 , then [∆d>H ,Σ ({K1 , K2 , K3 })] =
{000, 001} and ids (K1 , ∆d>H ,Σ ({K10 , K2 , K3 }) = 1. See Table 8.
ω
000
001
010
011
100
101
110
111

K1
0
0
1
1
1
1
1
0

K10
0
0
1
1
1
1
2
2

K2
1
0
1
1
1
1
0
1

K3
0
1
1
2
1
2
0
1

d

∆>H

,Σ

({K1 , K2 , K3 })
1
1
3
4
3
4
1
2

d

∆>H

,Σ

({K10 , K2 , K3 })
1
1
3
4
3
4
2
4

Table 8: Manipulability of ∆dH ,Σ for ids with #(E) 6= 2.
• The following example shows that ∆dµH ,Σ is not strategy-proof for ip if K is not complete. Table 9 shows the manipulability of ∆d>H ,Σ for ip (even if there are only two bases
in the profile and if µ ≡ >). Let us consider the two bases K1 and K2 defined by their
sets of models: [K1 ] = {000, 001, 010, 100} and [K2 ] = {110, 011, 101, 111}. We have
79

Everaere, Konieczny & Marquis

[∆d>H ,Σ ({K1 , K2 })] = {001, 010, 100, 110, 011, 101} and ip (K1 , ∆d>H ,Σ ({K1 , K2 })) = 21 .
On the other hand, if the agent whose base is K1 gives K10 , with [K10 ] = {000} instead of K1 , we obtain [∆d>H ,Σ ({K10 , K2 })] = {000, 001, 010, 100, 110, 011, 101} and
ip (K1 , ∆d>H ,Σ ({K10 , K2 }) = 47 .
ω
000
001
010
011
100
101
110
111

K1
0
0
0
1
0
1
1
2

K10
0
1
1
2
1
2
2
3

d

∆>H

K2
2
1
1
0
1
0
0
0

,Σ

({K1 , K2 })
2
1
1
1
1
1
1
2

d

∆>H

,Σ

({K10 , K2 })
2
2
2
2
2
2
2
3

Table 9: Manipulability of ∆d>H ,Σ for ip if K is not complete.


Theorem 4
• ∆dµH ,GM ax is not strategy-proof for the satisfaction indexes idw and ip (even if µ ≡ >,
K is complete and #(E) = 2).
• ∆dµH ,GM ax is strategy-proof for the satisfaction index ids if and only if µ ≡ >, K is
complete and #(E) = 2.
Proof:
• Table 10 shows the manipulability of ∆dH ,GM ax for the weak satisfaction index idw
even if µ ≡ >, K is complete and #(E) = 2. We consider K1 s.t. [K1 ] = {001}, K2
with [K2 ] = {111}, and µ ≡ >. We have [∆dµH ,GM ax ({K1 , K2 })] = {011, 101}, so no
model of K1 belongs to [∆dµH ,GM ax ({K1 , K2 })] and idw (K1 , ∆dµH ,GM ax ({K1 , K2 }) = 0.
If agent 1 gives K10 with [K10 ] = {000} instead of K1 , then [∆dµH ,GM ax {K10 , K2 })] =
{001, 010, 011, 100, 101, 110} and idw (K1 , ∆dµH ,GM ax ({K10 , K2 }) = 1.
ω
000
001
010
011
100
101
110
111

K1
1
0
2
1
2
1
3
2

K10
0
1
1
2
1
2
2
3

K2
3
2
2
1
2
1
1
0

d

∆µH

,GM ax

({K1 , K2 })
(3, 1)
(2, 0)
(2, 2)
(1, 1)
(2, 1)
(1, 1)
(3, 1)
(2, 0)

d

∆µH

,GM ax

({K10 , K2 })
(3, 0)
(2, 1)
(2, 1)
(2, 1)
(2, 1)
(2, 1)
(2, 1)
(3, 0)

Table 10: Manipulability of ∆dµH ,GM ax for idw .
80

The Strategy-Proofness Landscape of Merging

Since manipulability for idw holds, manipulability for ip holds as well (cf. Theorem 1).
• As to ids , we first show that ∆dµH ,GM ax is strategy-proof for this index if µ ≡ >,
#(E) = 2 and K is complete. Then, we give examples of manipulation if µ 6≡ >, or
#(E) 6= 2, or K is not complete.
– ∆d>H ,GM ax is strategy-proof when E = {K1 , K2 } and µ ≡ >, if K1 is complete.
We consider E 0 = {K10 , K2 } with K10 = Kω0 1 complete (thanks to the forthcoming
Lemma 2, we know that if the operator is manipulable, it is manipulable for
a complete base), and µ ≡ >. Let #(P) = n and let d(K10 , K2 ) = m ≤ n.
Then there exists a model ω2 of K2 s.t. dH (Kω0 1 , ω2 ) = m. By definition of the
Hamming distance, ω2 can be generated from ω1 by flipping m variables (since
Kω0 1 and ω2 differ on the truth values of m variables x1 , . . . , xm ).
If m = 2k + 1 (m odd), then d(>, E 0 ) = (k + 1, k); otherwise m = 2k (m
even) and d(>, E 0 ) = (k, k). In the first case (m odd), there exist at least two
interpretations ω and ω 0 s.t. d(ω, E 0 ) = d(ω 0 , E 0 ) = d(>, E 0 ) (for instance, ω is
generated from ω1 by flipping x1 , . . . , xk and ω 0 is generated from ω2 by flipping
xk+1 , . . . , xm ).
A similar conclusion can be derived in the second case (m even) as soon as k ≥ 1.
In these two cases, ∆d>H ,GM ax (E 0 ) has at least two models, hence we cannot have
∆d>H ,GM ax (E 0 ) ≡ K1 with K1 complete: E cannot be manipulated by K1 for ids .
The remaining case is d(>, E 0 ) = (0, 0). It imposes that Kω0 1 ∧ K2 is consistent.
Since Kω0 1 is complete, we have ∆d>H ,GM ax (E 0 ) ≡ Kω0 1 , hence no manipulation is
dH ,GM ax
possible for ids (since ∆>
(E 0 ) ≡ K1 if and only if K1 ≡ Kω0 1 if and only if
dH ,GM ax
∆>
({K1 , K2 }) ≡ K1 ).
– For showing the manipulability for ids , we consider the following scenarios:
∗ µ 6≡ >, even if #(E) = 2 and K is complete.
Let us consider [K1 ] = {01}, [K2 ] = {11}, µ = ¬a ∧ b. Then [∆dµH ,GM ax ({K1 ,
K2 }] = {01, 11}, and ids (K1 , ∆dµH ,GM ax ({K1 , K2 }) = 0. If agent 1 gives K10
with [K10 ] = {00} instead of K1 , then the result is [∆dµH ,GM ax ({K10 , K2 })] =
{01} and ids (K1 , ∆µdH ,GM ax ({K10 , K2 }) = 1. (see Table 11).
ω
00
01
10
11

K1
1
0
2
1

K10
0
1
1
2

K2
2
1
1
0

d

∆µH

,GM ax

({K1 , K2 })
(2, 1)
(1, 0)
(2, 1)
(1, 0)

d

∆µH

,GM ax

({K10 , K2 })
(2, 0)
(1, 1)
(1, 1)
(2, 0)

Table 11: Manipulability of ∆dµH ,GM ax for ids if µ 6≡ >
∗ #(E) 6= 2, even if µ ≡ > and K is complete.
Let us consider [K1 ] = {01}, [K2 ] = {11}, and [K3 ] = {00, 01, 11}. Then
[∆d>H ,GM ax ({K1 , K2 , K3 }] = {01, 11}, so ids (K1 , ∆d>H ,GM ax ({K1 , K2 , K3 }) =
81

Everaere, Konieczny & Marquis

0. If agent 1 gives K10 with [K10 ] = {00} instead of K1 , then [∆d>H ,GM ax ({K10 ,
K2 , K3 })] = {01} and ids (K1 , ∆d>H ,GM ax ({K10 , K2 , K3 }) = 1. (see Table 12).
ω
00
01
10
11

K1
1
0
2
1

K10
0
1
1
2

K2
2
1
1
0

K3
0
0
1
0

d

∆µH

,GM ax

d

∆µH

({K1 , K2 , K3 })
(2, 1, 0)
(1, 0, 0)
(2, 1, 1)
(1, 0, 0)

,GM ax

({K10 , K2 , K3 })
(2, 0, 0)
(1, 1, 0)
(1, 1, 1)
(2, 0, 0)

Table 12: Manipulability of ∆d>H ,GM ax for ids if #(E) 6= 2.
∗ K is not complete, even if µ ≡ > and #(E) = 2.
The example given Table 13 shows that manipulation is possible if the initial base is not complete. Consider [K1 ] = {01, 10}, [K2 ] = {11}, and µ ≡
>. Then [∆dµH ,GM ax ({K1 , K2 }] = {01, 10, 11}, and ids (K1 , ∆dµH ,GM ax ({K1 ,
K2 }) = 0. If agent 1 gives K10 with [K10 ] = {00} instead of K1 , then
[∆dµH ,GM ax ({K10 , K2 })] = {01, 10} and ids (K1 , ∆dµH ,GM ax ({K10 , K2 }) = 1.
ω
00
01
10
11

K1
1
0
0
1

K10
0
1
1
2

K2
2
1
1
0

d

∆µH

,GM ax

({K1 , K2 })
(2, 1)
(1, 0)
(1, 0)
(1, 0)

d

∆µH

,GM ax

({K10 , K2 })
(2, 0)
(1, 1)
(1, 1)
(2, 0)

Table 13: Manipulability of ∆dµH ,GM ax for ids if K is not complete.



Theorem 5
C3
C4
C5
• 4C1
µ , 4µ , 4µ , and 4µ are not strategy-proof for ip (even if µ ≡ >, K is complete
and #(E) = 2).

• 4C1
µ is strategy-proof for idw and ids .
• 4C3
µ is strategy-proof for idw and ids if and only if µ ≡ >.
• 4C4
µ is not strategy-proof for idw and ids (even if µ ≡ >, K is complete and #(E) = 2).
• 4C5
µ is strategy-proof for idw if and only if µ ≡ > or K is complete, and is strategyproof for ids if and only if µ ≡ >.
Proof:
82

The Strategy-Proofness Landscape of Merging

• We first give an example of manipulation of 4C1
µ for ip , with #(E) = 2, a complete
base K1 , and µ ≡ >.
Consider E = {K1 , K2 }, with K1 = {a ∧ b} and K2 = {¬(a ∧ b)}. Then 4C1
> (E) ≡ >,
1
0 = {a, b} instead of K , then
and ip (K1 , 4C1
(E))
=
.
But
if
agent
1
gives
K
1
1
>
4
1
0
C1
0
4C1
> ({K1 , K2 }) ≡ a ∨ b, and ip (K1 , 4> ({K1 , K2 })) = 3 . So E is manipulable by K1
C1
C3
for ip . The same example holds for 4C4
µ . It remains to note now that 4> = 4> =
4C5
> to conclude the first point of the proof.
• 4C1
µ is strategy-proof for idw and ids .
Weak drastic index.
For any K ∈ E there are two cases:
– K
subset M of
S ∧ µ is consistent. Then there is at least one maximal consistent
C1
Ki ∈E Ki which contains µ and all the formulas of K. So 4µ (E t{K}) ≡ M ∨R
(where R denotes the disjunction of the other maxcons) is consistent with K ∧ µ.
So idw (K, 4C1
µ (E t {K})) = 1 and no manipulation is possible.
0
– K ∧ µ is not consistent. Since for any K 0 , we have 4C1
µ (E t {K })) |= µ, we also
C1
0
have idw (K, 4µ (E t {K })) = 0 and no manipulation is possible.

Strong drastic index.
By reductio ad absurdum. Assume that 4C1
µ is not strategy-proof for ids . It means
that
∃K s.t. 4C1
(6)
µ (E t {K}) 6|= K,
0
∃K 0 s.t. 4C1
µ (E t {K }) |= K.

(7)

From statement (7) we get that ∀M ∈ maxcons(E t {K 0 }, µ), M |= K. So if we
0
0
0
consider 4C1
µ (E t {K } t {K}), every M ∈ maxcons(E t {K } t {K}, µ) is of the
0
C1
0
form M ∪ {K}, so M |= K and 4µ (E t {K } t {K}) |= K (∗).
From statement (6) we get that ∃M ∈ maxcons(E t {K}, µ), M 6|= K. Since M is a
maximal subset, it means that M ∧ K is not consistent. So if we consider 4C1
µ (E t
{K 0 } t {K}), then M ⊆ M 0 , with M 0 ∈ maxcons(E t {K 0 } t {K}, µ). So M 0 ∧ K
0
is not consistent. Hence M 0 6|= K and 4C1
µ (E t {K} t {K }) 6|= K, which contradicts
(∗).
• 4C3
µ is strategy-proof for idw and ids if and only if µ ≡ >.
Weak drastic index.
C3
C1
C3
Since 4C1
> = 4> , it follows immediately from the above proof for 4µ that 4> is
strategy-proof for idw .

For showing that manipulation is possible for 4C3
µ if µ 6≡ >, even with two bases and a
complete initial base K1 , consider the following example: let K1 = {a∧b}, K2 = {¬a},
µ = ¬b and K10 = {a}. We have ∆C3
µ ({K1 , K2 }) ≡ ¬a, which is inconsistent with K1 .
0 , K }) ≡ >, which is consistent with K .
We also have ∆C3
({K
2
1
µ
1
83

Everaere, Konieczny & Marquis

Strong drastic index.
C3
C3
Since 4C1
> = 4> , it follows immediately from the point above that 4> is strategyproof for ids .

For showing that a manipulation is possible for 4C3
µ for ids if µ 6≡ >, even with two
bases and a complete initial base K1 , we consider the following example: let K1 = {a∧
b}, K2 = {¬a}, µ = ¬a ∧ ¬b and K10 = {¬a ∧ b}. We have ∆C3
µ ({K1 , K2 }) ≡ ¬a, hence
C3
C3
0
0
∆µ ({K1 , K2 }) 6|= K1 . We also have ∆µ ({K1 , K2 }) ≡ ⊥, hence ∆C3
µ ({K1 , K2 }) |=
K1 .
• 4C4
µ is not strategy-proof for idw and ids (even if µ ≡ >, K is complete and #(E) = 2).
Weak drastic index.
For showing that manipulation is possible for 4C4
µ with two bases, a complete initial base K1 and µ ≡ >, we consider the following example: let K1 = {a}, K2 =
{¬a, ¬a ∧ >}, µ = > and K10 = {a, a ∧ >}. We have ∆C4
µ ({K1 , K2 }) ≡ ¬a, hence
C4
0
∆µ ({K1 , K2 }) ∧ K1 is not consistent. We also have ∆C4
µ ({K1 , K2 }) ≡ >, hence
C4
0
∆µ ({K1 , K2 }) ∧ K1 is consistent.
Strong drastic index.
For showing that 4C4
µ is not strategy-proof for ids with two bases, a complete initial
base K1 and µ ≡ >, consider the following example: let K1 = {a}, K2 = {¬a}, and
C4
K10 = {a, a ∧ >}. We have ∆C4
µ ({K1 , K2 }) ≡ >, hence ∆µ ({K1 , K2 }) 6|= K1 . We also
0
C4
0
have ∆C4
µ ({K1 , K2 }) ≡ a, hence ∆µ ({K1 , K2 }) |= K1 .
• 4C5
µ is strategy-proof for idw if and only if µ ≡ > or K is complete, and is strategyproof for ids if and only if µ ≡ >.
Weak drastic index.
C5
C1
C5
Since 4C1
> = 4> , it follows from the above proof for 4µ that 4µ is strategy-proof
for idw if µ ≡ >.

If the initial base K1 is complete, 4C5
µ is also strategy-proof. There are two cases:
S
– K1 |= µ. Let S
M = {φ ∈ K∈E K | K1 |= φ}. By construction, M is an element
of maxcons( K∈E K, >). Since K1 |= µ, M is consistent with µ (K1 is a model
of each of them). Since we have both K1 |= M and M |= ∆C5
µ (E) (by definition
C5 (E) ∧ K is consistent,
of the operator), we also have K1 |= ∆C5
(E).
Hence
∆
1
µ
µ
and this prevents E from being manipulable by K1 for idw given ∆C5
µ and µ.
– K1 |= ¬µ. By definition of the operator, for any base K10 and any profile E 0
0
0
(especially the profile obtained by removing K1 from E), we have ∆C5
µ ({K1 }tE )
0
0
C5
0
0
is consistent and ∆C5
µ ({K1 } t E ) |= µ. This implies that ∆µ ({K1 } t E ) ∧ K1
is inconsistent, and no manipulation is possible for idw .
For showing that manipulation is possible for 4C5
µ if µ 6≡ > and if the initial base K1
is not complete, we consider the following example:
84

The Strategy-Proofness Landscape of Merging

let K1 = {a}, K2 = {b, ¬a}, µ = ¬a∨¬b and K10 = {a∧¬b}. We have ∆C5
µ ({K1 , K2 }) ≡
C5
C5
b ∧ ¬a, hence ∆µ ({K1 , K2 }) ∧ K1 is not consistent. We also have ∆µ ({K10 , K2 }) ≡
0
(a ∧ ¬b) ∨ (b ∧ ¬a), hence ∆C4
µ ({K1 , K2 }) ∧ K1 is consistent.
Strong drastic index.
C5
C1
C5
Since 4C1
> = 4> , it follows immediately from the above proof for 4µ that 4µ is
strategy-proof for ids if µ ≡ >.

For showing that manipulation is possible for 4C5
µ if µ 6≡ >, even with two bases
and a complete initial base K1 , we consider the following example: let K1 = {a ∧ b},
K2 = {¬b}, µ = a and K10 = {a ∧ b, b ∨ ¬a}. We have ∆C5
µ ({K1 , K2 }) ≡ a, hence
C5
C5
0
0
∆µ ({K1 , K2 }) 6|= K1 . We also have ∆µ ({K1 , K2 }) ≡ a∧b. Hence ∆C5
µ ({K1 , K2 }) |=
K1 .

Theorem 6
• 4C1
µ is strategy-proof for idw and ids , and is strategy-proof for ip if and only if #(E) =
2.
c

• 4C3
µ is strategy-proof for idw and ids if and only if µ ≡ >, and is strategy-proof for ip
if and only if #(E) = 2 and µ ≡ >.
c

• 4C4
µ is strategy-proof for ip , idw and ids .
c

• 4C5
µ is strategy-proof for idw if and only if #(E) = 2 or µ ≡ > or K is complete.
c

C5
4C5
µ is strategy-proof for ids if and only if #(E) = 2 or µ ≡ >. Finally, 4µ is
strategy-proof for ip if and only if #(E) = 2.
c

c

Proof:
• 4C1
µ is strategy-proof for idw and ids , and is strategy-proof for ip if and only if #(E) =
2.
c

Drastic indexes. The strategy-proofness of 4C1
µ comes from the strategy-proofness of
c

C1
C1
4C1
µ as reported in Theorem 5, because 4µ is a specialization of 4µ . Furthermore,
C1
in the examples given in the proof of Theorem 5 concerning 4µ , every base is a
c

singleton, so these examples hold for 4C1
µ too.
c

Probabilistic index.
The proof for the probabilistic index and a profile E s.t. #(E) = 2 is based on the fact
c
that the merging of two bases with 4C1
µ is either the conjunction of the two bases, or
their disjunction. In both cases, we shall show that no manipulation can occur.
For the ⇐ part of the proof, suppose that #(E) = 2. we shall show that 4C1
µ and
c

4C5
µ (we group here the two cases, because their proofs are similar) is strategy-proof
for ip . By case analysis:
c

85

Everaere, Konieczny & Marquis

– If K1 is consistent with µ, then there are two cases:
C5
∗ 4C1
µ ({K1 , K2 }) ≡ 4µ ({K1 , K2 }) ≡ K1 ∧ K2 ∧ µ if it is consistent.
c

c

C5
∗ 4C1
µ ({K1 , K2 }) ≡ 4µ ({K1 , K2 }) ≡ (K1 ∨ K2 ) ∧ µ otherwise.
c

c

C5
In the first case, 4C1
µ ({K1 , K2 }) |= K1 and 4µ ({K1 , K2 }) |= K1 so ip takes its
maximal value, and no manipulation is possible.
c

c

C5
Let us consider the second case for 4C1
µ (the case for 4µ is similar): we assume
c

c

that 4C1
µ ({K1 , K2 }) ≡ (K1 ∨ K2 ) ∧ µ.
c

C1
0
Since for every base K10 , the definition of 4C1
µ requires that we have 4µ ({K1 ,
c

c

K2 }) |= µ and since we have K1 ∧ µ |= 4C1
µ ({K1 , K2 }), the following inequation
0
holds for every base K1 :
c

0
C1
#([K1 ] ∩ [4C1
µ ({K1 , K2 })]) ≤ #([K1 ] ∩ [4µ ({K1 , K2 })]).
c

c

0
0
If K10 ∧ K2 ∧ µ is consistent, then 4C1
µ ({K1 , K2 }) ≡ K1 ∧ K2 ∧ µ, hence
c

0
#([K1 ]∩[4C1
µ ({K1 , K2 })]) = 0 is minimal since we have assumed that K1 ∧K2 ∧µ
is inconsistent.
If K10 ∧ K2 ∧ µ is inconsistent, then there are two cases:
c

0
∗ K10 ∧ µ is inconsistent and K2 ∧ µ is inconsistent. We have 4C1
µ ({K1 , K2 }) ≡
c

µ. Since we have assumed K1 ∧ µ consistent, we also have 4C1
µ ({K1 , K2 }) ≡
K1 ∧ µ. Hence:
c

c

#([K1 ] ∩ [µ]
,
#([µ])

ip (K1 , 4C1
µ ({K1 , K2 })) =

#([K1 ] ∩ [K1 ∧ µ]
.
#([K1 ∧ µ])

0
ip (K1 , 4C1
µ ({K1 , K2 })) =

and
c

Since the numerators of the two fractions coincide while #([K1 ∧µ]) ≤ #([µ]),
no manipulation is possible in this case.
0
∗ K10 ∧ µ is consistent or K2 ∧ µ is consistent. We have 4C1
µ ({K1 , K2 }) ≡
0
(K1 ∨ K2 ) ∧ µ. Since K1 ∧ K2 ∧ µ is inconsistent, we have
c

0
ip (K1 , 4C1
µ ({K1 , K2 })) =

#([K1 ∧ K10 ∧ µ])
.
#([(K10 ∨ K2 ) ∧ µ])

ip (K1 , 4C1
µ ({K1 , K2 })) =

#([K1 ∧ µ])
.
#([(K1 ∨ K2 ) ∧ µ])

c

We also have:
c

Now, since K10 ∧ K2 ∧ µ is inconsistent, we have #([(K10 ∨ K2 ) ∧ µ]) =
#([K10 ∧ µ]) + #([K2 ∧ µ]). Similarly, since K1 ∧ K2 ∧ µ is inconsistent, we
have #([(K1 ∨ K2 ) ∧ µ]) = #([K1 ∧ µ]) + #([K2 ∧ µ]). Subsequently, suppose
86

The Strategy-Proofness Landscape of Merging

0
C1
that we have ip (K1 , 4C1
µ ({K1 , K2 })) > ip (K1 , 4µ ({K1 , K2 })). This is the
case if and only if #([K1 ∧ K10 ∧ µ])(#([K1 ∧ µ]) + #([K2 ∧ µ])) > #([K1 ∧
µ])(#([K10 ∧ µ]) + #([K2 ∧ µ])).
If we note a = #([K1 ∧ K10 ∧ µ]) and b = #([K2 ∧ µ]), then there exist two
natural integers a0 and a00 such that #([K1 ∧ µ]) = a + a0 and #([K10 ∧ µ]) =
a + a00 . Replacing in the previous inequation, it comes:
c

c

a(a + a0 + b) > (a + a0 )(a + a00 + b)
which simplifies to 0 > aa00 + a0 a00 + a0 c, which is impossible. Hence no
manipulation is possible in this case as well.
0
C5
0
– If K1 is not consistent with µ, then, since ∀E 0 , 4C1
µ (E ) |= µ and 4µ (E ) |= µ,
c

c

0
C5
0
we have: ∀E 0 , ip (K1 , 4C1
µ (E )) = 0 and ip (K1 , 4µ (E )) = 0, so no manipulation
is possible.
c

c

For the ⇒ part of the proof, the following example shows that strategy-proofness
c
c
C5
for ip does not hold any longer for 4C1
µ or 4µ when #(E) 6= 2, even if the initial base is complete and µ ≡ >. We consider K1 = {a ∧ b}, K2 = {a ∧ b} and
K3 = {¬a}, with the integrity constraint µ = >. There are two maximal consistent sets in maxcons({K1 , K2 , K3 }, µ): M1 = {a ∧ b, >} and M2 = {¬a, >}. Hence
c
c
1
C1
4C1
µ ({K1 , K2 , K3 }) ≡ (a ∧ b) ∨ (¬a). We get ip (K1 , 4µ ({K1 , K2 , K3 })) = 3 . If
agent 1 gives K10 = {¬a ∧ b} instead of K1 , then there are two maximal consistent sets M10 = {a ∧ b, >}, M20 = {¬a ∧ b, ¬a, >} in maxcons({K10 , K2 , K3 }, µ), so
c
c
1
0
C1
0
4C1
µ ({K1 , K2 , K3 }) ≡ (a ∧ b) ∨ (¬a ∧ b). We get ip (K1 , 4µ ({K1 , K2 , K3 })) = 2 , so
this is an example of manipulation of 4C1
> for ip .
c

C5
C5
Since 4C1
> = 4> , this example shows the manipulability of 4µ as well.
c

c

c

• 4C3
µ is strategy-proof for idw and ids if and only if µ ≡ >, and is strategy-proof for ip
if and only if #(E) = 2 and µ ≡ >.
c

Drastic indexes.
C3
Since 4C3
> is strategy-proof for idw and ids , its specialization 4µ is also strategy-proof
for idw and ids .
c

In the example showing the manipulation for 4C3
µ with two bases and a complete
initial base K1 , every base is a singleton, so this example holds for 4C3
µ too.
c

Probabilistic index.
C3
C1
C3
Since 4C1
> = 4> , it follows immediately from the above proof for 4µ that 4> is
strategy-proof for ip when two bases are considered, and it is not the case when three
agents or more are taken into account.
c

c

For showing that manipulation is possible for 4C3
µ with two bases and a complete initial base K1 , when the integrity constraint µ is not true, consider the following examc
ple: let K1 = {a ∧ b}, K2 = {¬a}, µ = ¬b and K10 = {a}. We have 4C3
µ ({K1 , K2 }) ≡
c

87

Everaere, Konieczny & Marquis

¬a, which is inconsistent with K1 , so ip (K1 , 4C3
µ ({K1 , K2 })) = 0. We also have
c

0
C3
0
4C3
µ ({K1 , K2 }) ≡ >, which is consistent with K1 , so ip (K1 , 4µ ({K1 , K2 })) > 0.
c

c

• 4C4
µ is strategy-proof for ip , idw and ids . This is a direct consequence of Remark 1
and Theorem 2.
c

• 4C5
µ is strategy-proof for idw if and only if #(E) = 2 or µ ≡ > or K is complete.
c

C5
4C5
µ is strategy-proof for ids if and only if #(E) = 2 or µ ≡ >. Finally, 4µ is
strategy-proof for ip if and only if #(E) = 2.
c

c

Weak drastic index.
As 4C5
µ is strategy-proof for ip with two bases, it is also strategy-proof for idw in that
case.
c

C5
Since 4C5
> is strategy-proof for idw , we have that 4 > is also strategy-proof for idw .
c

Finally, as no manipulation is possible for 4C5
µ when the initial base K1 is complete
(Theorem 5), no manipulation exists for 4C5
µ in this cas.
c

Contrastingly, a manipulation example exists for 4C5
µ if #(E) 6= 2 and µ 6≡ >
and the initial base K1 not complete. We consider the three (singleton) bases:
c
K1 = {b}, K2 = {¬a}, K3 = {a ∧ ¬b}, and µ = a. We have 4C5
µ ({K1 , K2 , K3 }) ≡
c

0
a ∧ ¬b, so idw (K1 , 4C5
µ ({K1 , K2 , K3 })) = 0. And with K1 = {b ∧ a}, we have
c

0
C5
0
4C5
µ ({K1 , K2 , K3 }) ≡ a, and idw (K1 , 4µ ({K1 , K2 , K3 })) = 1.
c

c

Strong drastic index.
C5
As 4C5
> is strategy-proof for ids , 4> is also strategy-proof for ids .
c

C5
As 4C5
µ is strategy-proof for ip when there are two bases in E, 4µ is also strategyc

c

proof for ids in this case, since for any profile E, 4C5
µ (E) is consistent.
c

A manipulation example exists for 4C5
µ if #(E) 6= 2 and µ 6≡ >, even if the initial
base K1 is complete: K1 = {a ∧ b}, K2 = {a ∧ b}, K3 = {¬a ∨ ¬b}, and µ = a.
There are two maximal consistent sets in maxcons({K1 , K2 , K3 }, >): M1 = {a ∧ b}
c
and M2 = {¬a ∨ ¬b}. Hence 4C5
µ ({K1 , K2 , K3 }) ≡ (a ∧ b) ∨ (a ∧ ¬b). We get
c

0
ids (K1 , 4C5
µ ({K1 , K2 , K3 })) = 0. With K1 = {¬a ∧ ¬b}, there are two maximal
consistent sets in maxcons({K1 , K2 , K3 }, >): M10 = {a ∧ b} and M20 = {¬a ∧ ¬b}.
c
c
0
C5
0
Hence 4C5
µ ({K1 , K2 , K3 }) ≡ (a ∧ b), and ids (K1 , 4µ ({K1 , K2 , K3 })) = 1.
c

Probabilistic index.
C1
The proof for 4C5
µ is similar to the proof for 4µ .
c

c


Theorem 7 The strategy-proofness results reported in Table 14 hold, under the restriction
that each base is complete (f stands for any aggregation function, and d for any distance).
88

The Strategy-Proofness Landscape of Merging

f is any aggregation function, d is any distance, sp means “strategy-proof ”, sp means “non
strategy-proof ” even if #(E) = 2 and µ ≡ >, sp∗ means “non strategy-proof ” even if either
#(E) = 2 or µ ≡ >, but “strategy-proof ” if both #(E) = 2 and µ ≡ >. Finally, sp> means
“non strategy-proof ” even if #(E) = 2, but “strategy-proof ” whenever µ ≡ >.

∆

ip

idw

ids

∆dµD ,f
∆d,Σ
µ
dH ,GM ax
∆µ
4C1
µ
4C3
µ
4C4
µ
4C5
µ
c
4C1
µ
c
4C3
µ
c
4C4
µ
c
4C5
µ

sp
sp
sp
sp
sp
sp
sp

sp
sp
sp
sp
sp>
sp
sp

sp
sp
sp∗
sp
sp>
sp
sp>

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

sp

Table 14: Strategy-proofness: complete bases.
Proof:

The first line of the table (∆dµD ,f ) is a direct consequence of Theorem 2.

The second line of the table (∆d,Σ
µ ) is a direct consequence of Theorem 2.
The third line of the table (∆dµH ,GM ax ) comes from the proof of Theorem 4.
The first column of the fourth line (4C1
µ and ip ) comes from the following example. Let
K1 = {a, b}, K2 = {¬a, ¬b}, K10 = {a ∧ b} and µ = >. We have 4C1
µ ({K1 , K2 }) ≡ >, hence
1
C1
0
C1
ip (K1 , 4µ ({K1 , K2 })) = 4 , while we have 4µ ({K1 , K2 }) ≡ (a ∧ b) ∨ (¬a ∧ ¬b), showing
1
0
C1
that ip (K1 , 4C1
µ ({K1 , K2 })) = 2 . The rightmost columns of the fourth line (4µ and idw ,
ids ) come directly from Theorem 5.
The first column of the fifth line (4C3
µ and ip ) comes from the first column of the fourth
line given that the example is s.t. µ ≡ > (in that case both operators coincide). Similarly
C3
for the second and the third columns (4C3
µ and idw , ids ) in the case µ ≡ > (4> coincides
C3
with 4C1
> ). In the remaining case, 4µ is not strategy-proof for idw even if #(E) = 2 as
the following example shows: take E = {K1 , K2 } with K1 = {a ∧ b ∧ c}, K2 = {¬a ∧ ¬b, c}
and µ = ¬b; we have 4C3
µ (E) ≡ ¬a ∧ ¬b ∧ c, which is inconsistent with K1 ; if the agent
0
gives K10 = {a, b ∧ ¬c} instead of K1 , we obtain 4C3
µ ({K1 , K2 }) ≡ (a ∧ c) ∨ (¬a ∧ ¬b ∧ c),
which is consistent with K1 . Finally, 4C3
µ is not strategy-proof for ids even if #(E) = 2
when µ 6= >; let K1 = {a, ¬b}, K2 = {¬a, b}, K10 = {a ∧ ¬b} and µ = ¬b. We have
C3
4C3
µ ({K1 , K2 }) ≡ (a ∧ ¬b) ∨ (¬a ∧ ¬b), hence ids (K1 , 4µ ({K1 , K2 })) = 0, while we have
0
C3
0
4C3
µ ({K1 , K2 }) ≡ a ∧ ¬b, showing that ids (K1 , 4µ ({K1 , K2 })) = 1.
The sixth line (4C4
µ ) comes from the proof of Theorem 5.
89

Everaere, Konieczny & Marquis

The first column of the seventh line (4C5
µ and ip ) comes from the first column of the fourth
line given that the example is s.t. µ ≡ > (in that case both operators coincide). The second
C5
column (4C5
µ and idw ) comes directly from Theorem 5. The third column (4µ and ids ) in
C5
the case µ ≡ > comes from the third column of the fourth line (4> coincides with 4C1
> ).
C5
Finally, 4µ is not strategy-proof for ids even if #(E) = 2 when µ 6≡ >; let K1 = {a, ¬b},
K2 = {¬a, b}, K10 = {a ∧ ¬b} and µ = ¬b. We have 4C5
µ ({K1 , K2 }) ≡ (a ∧ ¬b) ∨ (¬a ∧ ¬b),
C5
C5
hence ids (K1 , 4µ ({K1 , K2 })) = 0, while we have 4µ ({K10 , K2 }) ≡ a ∧ ¬b, showing that
0
ids (K1 , 4C5
µ ({K1 , K2 })) = 1.
C4
Finally, it remains to consider the 4C
µ operators. As to 4µ , we know from Theorem 6 that
it is strategy-proof for ip (hence for idw and ids ) since 4C4
µ is strategy-proof for ip when all
c

b

C3
C5
bases are singletons. Let us focus on 4C1
µ , 4µ and 4µ . Since each base is complete and
can be assumed to be a singleton without loss of generality, we have
c

c
4C1
µ ({K1 , . . . , Kn })

≡

c

c

c
4C5
µ ({K1 , . . . , Kn })

≡(

n
_

Ki ) ∧ µ if consistent,

i=1
C5
4C1
µ ({K1 , . . . , Kn }) ≡ 4µ ({K1 , . . . , Kn }) ≡ µ otherwise.
c

c

We also have:
4C3
µ ({K1 , . . . , Kn }) ≡ (
c

n
_

Ki ) ∧ µ if consistent,

i=1
c
4C3
µ ({K1 , . . . , Kn })

≡ ⊥ otherwise.

C1
C3
C5
Let 4C
µ be any operator among 4µ , 4µ and 4µ . There are two cases:
b

c

c

c

• 4C
µ consistent. There are again two cases:
b

0
C5
0
– K1 |= ¬µ. Since for every profile E 0 we have 4C1
µ (E ) |= µ and 4µ (E ) |= µ,
c

c

0
C5
0
we also have 4C1
µ (E ) ∧ K1 inconsistent and 4µ (E ) ∧ K1 inconsistent, showing
that no manipulation is possible for ip , hence for idw and ids . In the specific case
we consider (all bases are singletons and are complete), we also have for every
c
0
0
profile E 0 , 4C3
µ (E ) |= µ since each base from E that is kept as an element from
c

c

0
maxcons(E 0 , >) must satisfy µ and if no base is kept, 4C3
µ (E ) is inconsistent,
c

0
hence 4C3
µ (E ) |= µ trivially holds. The previous argument can be used to show
c

that no manipulation is possible with 4C3
µ for ip , hence for idw (and ids under
c

the assumption 4C3
µ (E) consistent).
c

– K1 |= µ. We necessarily have #([K1 ] ∩ [4C
µ ({K1 , . . . , Kn })]) = 1. By reductio
ad absurdum. A manipulation for ip is possible only if we can find a complete
b
b
0
C
0
base K10 s.t. (1) K1 |= 4C
µ ({K1 , . . . , Kn }) and (2) #([4µ ({K1 , . . . , Kn })]) <
b

0
#([4C
µ ({K1 , . . . , Kn })]). (2) requires that K1 |= ¬µ. (1) imposes that K1 |=
K10 ∨ K2 ∨ . . . ∨ Kn . Since K1 |= µ while K10 |= ¬µ, we have K1 6≡ K10 . Subsequently, there exists Kj (j ∈ 2, . . . , n) s.t. K1 ≡ Kj . Since Kj is a model of
b

90

The Strategy-Proofness Landscape of Merging

0
C
4C
µ ({K1 , . . . , Kn }), inequation (2) cannot be satisfied. Hence, 4µ is strategyb

b

proof for ip , hence for idw . Since 4C
µ (E) is assumed consistent, no manipulation
is possible for ids .
b

C
C3
• 4C
µ (E) inconsistent. This is only possible for 4µ = 4µ and requires that each Ki
b

b

c

C3
(i ∈ 1, . . . , n) is s.t. Ki |= ¬µ. Since 4C3
µ (E) is inconsistent, we have ip (K1 , 4µ (E)) =
c

c

0
0. Since for every K10 complete, K1 is not a model of 4C3
µ ({K1 , . . . , Kn }), we have
c

C3
0
C3
ip (K1 , 4C3
µ (4µ ({K1 , . . . , Kn }))) = 0 as well. This shows that 4µ is strategy-proof
c

c

C3
for ip , hence for idw . Finally, when 4C3
µ (E) is inconsistent, we have 4µ (E) |= K1 ,
showing that no manipulation is possible for ids as well.
c

c


Theorem 8 None of ∆dµD ,Σ , ∆dµD ,GM ax , ∆dµH ,Σ or ∆dµH ,GM ax is strategy-proof for iDalal ,
even in the restricted case E consists of two complete bases.
Proof:
• ∆dµD ,Σ = ∆dµD ,GM ax . Let us consider [K1 ] = {000}, [K2 ] = {110} and µ = a ∧ b ∧ c
where P = {a, b, c}. We have [∆dµD ,Σ ({K1 , K2 })] = {110} and iDalal (K1 , ∆dµD ,Σ ({K1 ,
K2 })) = 1 − 23 . With [K10 ] = {001}, we get [∆dµD ,Σ ({K10 , K2 })] = {110, 001} and
iDalal (K1 , ∆dµD ,Σ ({K10 , K2 })) = 1 − 13 , showing the manipulation (details are reported
in Table 15).
ω
000
001
010
011
100
101
110
111

K1
0
1
1
1
1
1
1
1

K10
1
0
1
1
1
1
1
1

K2
1
1
1
1
1
1
0
1

d

∆µD

,Σ

({K1 , K2 })
1
2
2
2
2
2
1
2

d

∆µD

,Σ

({K10 , K2 })
2
1
2
2
2
2
1
2

Table 15: Manipulation of ∆dµD ,Σ for iDalal with two complete bases.
• ∆dµH ,Σ . Let us consider [K1 ] = {000}, [K2 ] = {110} and µ = ¬((¬a ∧ ¬b ∧ ¬c) ∨ (¬a ∧
b ∧ ¬c) ∨ (a ∧ ¬b ∧ ¬c)) where P = {a, b, c}. We have [∆dµH ,Σ ({K1 , K2 })] = {110} and
iDalal (K1 , ∆dµH ,Σ ({K1 , K2 })) = 1− 23 . With [K10 ] = {001}, we get [∆dµH ,Σ ({K10 , K2 })] =
{110, 001, 011, 111} and iDalal (K1 , ∆dµH ,Σ ({K10 , K2 })) = 1 − 13 , showing the manipulation (details are reported in Table 16).
• ∆dµH ,GM ax . Let us consider [K1 ] = {0001}, [K2 ] = {0111} and µ = (¬a ∧ ¬b ∧
¬c ∧ ¬d) ∨ (¬a ∧ b ∧ c) ∨ (a ∧ ¬b ∧ ¬c) ∨ (a ∧ ¬b ∧ c ∧ ¬d) ∨ (a ∧ b ∧ ¬c ∧ ¬d) ∨
(a ∧ b ∧ c ∧ d) where P = {a, b, c, d}. We have [∆dµH ,GM ax ({K1 , K2 })] = {0111} and
91

Everaere, Konieczny & Marquis

ω
000
001
010
011
100
101
110
111

K10
1
0
2
1
2
1
3
2

K1
0
1
1
2
1
2
2
3

d

∆µH

K2
2
3
1
2
1
2
0
1

,Σ

({K1 , K2 })
2
4
2
4
2
4
2
4

d

∆µH

,Σ

({K10 , K2 })
3
3
3
3
3
3
3
3

Table 16: Manipulation of ∆dµH ,Σ for iDalal with two complete bases.
iDalal (K1 , ∆dµH ,GM ax ({K1 , K2 })) = 1− 42 . With [K10 ] = {1000}, we get [∆dµH ,GM ax ({K10 ,
K2 })] = {0000, 0110, 1001, 1010, 1100, 1111} and iDalal (K1 , ∆dµH ,GM ax ({K10 , K2 })) =
1 − 41 , showing the manipulation (details are reported in Table 17).
ω
0000
0001
0010
0011
0100
0101
0110
0111
1000
1001
1010
1011
1100
1101
1110
1111

K1
1
0
2
1
2
1
3
2
2
1
3
2
3
2
4
3

K10
1
2
2
3
2
3
3
4
0
1
1
2
1
2
2
3

K2
3
2
2
1
2
1
1
0
4
3
3
2
3
2
2
1

d

∆µH

,GM ax

({K1 , K2 })
(3, 1)
(2, 0)
(2, 2)
(1, 1)
(2, 2)
(1, 1)
(3, 1)
(2, 0)
(4, 2)
(3, 1)
(3, 3)
(2, 2)
(3, 3)
(2, 2)
(4, 2)
(3, 1)

d

∆µH

,GM ax

({K10 , K2 })
(3, 1)
(2, 2)
(2, 2)
(3, 1)
(2, 2)
(3, 1)
(3, 1)
(4, 0)
(4, 0)
(3, 1)
(3, 1)
(2, 2)
(3, 1)
(2, 2)
(2, 2)
(3, 1)

Table 17: Manipulation of ∆dµH ,GM ax for iDalal with two complete bases.

C
Theorem 9 None of the 4C
µ operators (hence, none of the 4µ operators) is strategy-proof
for iDalal , even in the restricted case E consists of two complete bases.
b

Proof:
Let us consider the complete bases K1 = {a ∧ b} and K2 = {¬a ∧ ¬b}, with µ =
¬(a∧b). We have maxcons({K1 , K2 }, µ) = {{¬a∧¬b, ¬(a∧b)}} = maxconscard ({K1 , K2 },
c
µ) and maxcons({K1 , K2 }, >) = {{¬a ∧ ¬b, >}, {a ∧ b, >}}, hence 4C1
µ ({K1 , K2 }) ≡
C4
C5
4C3
µ ({K1 , K2 }) ≡ 4µ ({K1 , K2 }) ≡ 4µ ({K1 , K2 }) ≡ ¬a ∧ ¬b.
c

c

c

2
We get iDalal (K1 , 4C
µ ({K1 , K2 })) = 1 − 2 = 0.
With K10 = {¬a ∧ b}, we have maxcons({K10 , K2 }, µ) = {{¬a ∧ ¬b, ¬(a ∧ b)}, {¬a ∧ b, ¬(a ∧
b)}} = maxconscard ({K10 , K2 }, µ) and maxcons({K10 , K2 }, >) = {{¬a ∧ ¬b, >}, {¬a ∧
b

92

The Strategy-Proofness Landscape of Merging

0
C3
0
C4
0
C5
0
b, >}}, hence 4C1
µ ({K1 , K2 }) ≡ 4µ ({K1 , K2 }) ≡ 4µ ({K1 , K2 }) ≡ 4µ ({K1 , K2 }) ≡
(¬a ∧ ¬b) ∨ (¬a ∧ b) ≡ ¬a.
b
1
0
Thus iDalal (K1 , 4C

µ ({K1 , K2 })) = 1 − 2 , showing the manipulation.
c

c

c

c

Theorem 10 Let d be a pseudo-distance and let f be an aggregation function. ∆d,f
is
µ
dilation strategy-proof for the indexes ip , idw and ids .
Proof:
The idea is that if the untruthful base K 0 contains all the models of the “true”
base K, then the merged base when K 0 is provided contains at most the same models of K
as those appearing in the merged base when K is reported, and more countermodels of K.
So no manipulation is possible.
From Theorem 1, it is sufficient to show that ∆d,f
µ is strategy-proof for ip . By reductio ad
absurdum. Let us suppose that there is an operator ∆d,f
µ , where d and f are respectively a
pseudo-distance and an aggregation function, which is manipulable by dilation for ip . Under
this assumption, we can find an integrity constraint µ, a profile E, two bases K and K 0
d,f
0
with K |= K 0 , s.t. ip (K, ∆d,f
µ ({K} t E)) < ip (K, ∆µ ({K } t E)). Using the light notation
E 4µ K instead of ∆d,f
µ ({K} t E), we have:
#([K] ∩ [E 4µ K 0 ])
#([K] ∩ [E 4µ K])
<
.
#([E 4µ K])
#([E 4µ K 0 ])
Since K |= K 0 , for any pseudo-distance d, we have ∀ω ∈ W, d(ω, K) ≥ d(ω, K 0 ). So, for any
aggregation function f (that satisfies non-decreasingness):
∀ω ∈ W, d(ω, E t {K}) ≥ d(ω, E t {K 0 }).

(8)

Let us note dmin (E tµ {K}) = min({d(ω, E t {K}) | ω |= µ}, ≤). With (8), we can
immediately infer: dmin (E tµ {K}) ≥ dmin (E tµ {K 0 }). Two cases have to be considered:
• dmin (E tµ {K}) > dmin (E tµ {K 0 }) (*).
If ω1 is a model of K ∧ µ then, since K |= K 0 , d(ω1 , K) = d(ω1 , K 0 ) = 0, so d(ω1 , E t
{K}) = d(ω1 , E t {K 0 }) . If furthermore ω1 is a model of E 4µ K 0 , then d(ω1 , E t
{K 0 }) = dmin (E tµ {K 0 }), and d(ω1 , E t {K}) = dmin (E tµ {K 0 }). By definition of
min, we have d(ω1 , E t {K}) ≥ dmin (E tµ {K}), because ω1 |= µ. So we can conclude
that dmin (E tµ {K}) ≤ dmin (E tµ {K 0 }), but this contradicts (*). So, no model of
K ∧ µ is a model of E 4µ K 0 . Consequently, ip (K, E 4µ K 0 ) = 0 and is minimal, and
this prevents from any manipulation for ∆d,f
µ . So, we can exclude case (*).
• dmin (E tµ {K}) = dmin (E tµ {K 0 }) (**).
If ω is a model of E 4µ K, then we have both ω |= µ and d(ω, E t {K}) = dmin (E tµ
{K}). So, d(ω, E t {K}) = dmin (E tµ {K 0 }) with the equation (**). Furthermore,
with the inequation (8), we infer that d(ω, E t {K}) ≥ d(ω, E t {K 0 }). Hence,
d(ω, E t {K 0 }) ≤ dmin (E tµ {K 0 }). Since ω is a model of µ, we can finally infer that
93

Everaere, Konieczny & Marquis

ω is a model of E 4µ K 0 as well. So we have that any model of E 4µ K is a model of
E 4µ K 0 , and then:
#([E 4µ K]) ≤ #([E 4µ K 0 ]).
(9)
We can deduce as well that any model of E 4µ K which is a model of K ∧ µ is a model
of E 4µ K 0 (and of K), so:
#([K] ∩ [E 4µ K]) ≤ #([K] ∩ [E 4µ K 0 ]).
Furthermore, if ω1 |= K ∧ µ is a model of E 4µ K 0 , then we have both:
– d(ω1 , E t {K 0 }) = dmin (E tµ {K 0 }) = dmin (E tµ {K}) with (**), and
– d(ω1 , E t {K}) = d(ω1 , E t {K 0 }) because K |= K 0 : since d(ω1 , K) = 0, we have
d(ω1 , K 0 ) = 0 too.
We obtain: d(ω1 , E t{K}) = dmin (E tµ {K}) and ω1 |= µ, so ω1 is a model of E 4µ K.
Then we can state #([K] ∩ [E 4µ K]) ≥ #([K] ∩ [E 4µ K 0 ]). So we get:
#([K] ∩ [E 4µ K]) = #([K] ∩ [E 4µ K 0 ]).

(10)

With (9) and (10), we get immediately that:
#([K] ∩ #([E 4µ K 0 ])
#([K] ∩ [E 4µ K])
≥
.
#([E 4µ K])
#([E 4µ K 0 ])

(11)

d,f
0
As a consequence, ip (K, ∆d,f
µ ({K} t E) ≥ ip (K, ∆µ ({K } t E)). This inequation
d,f
shows that ∆µ is not manipulable for ip , which contradicts the assumption. So case
(**) has to be excluded as well, and this concludes the proof.


Theorem 11 Let d be any distance. If ∆d,Σ
is not strategy-proof for the index idw (resp.
µ
ids ), then it is not erosion strategy-proof for idw (resp. ids ).
Proof:

We first need the following lemma:

Lemma 2 Let d be a pseudo-distance and let f be an aggregation function. If a profile E
is manipulable by K for idw (resp. ids ) given ∆d,f
µ and µ, then one can find a complete base
K 0 – the base the agent gives instead of her true base K – s.t. idw (K, ∆µ (E t {K 0 })) >
idw (K, ∆µ (E t {K})) (resp. ids (K, ∆µ (E t {K 0 })) > ids (K, ∆µ (E t {K}))).
Proof:
This lemma is mainly a consequence of the definition of the distance between an
interpretation and a base K 0 , as the minimal distance between an interpretation and a model
ω 00 of the base; the complete base whose unique model is ω 00 allows as well a manipulation
Weak drastic index.
94

The Strategy-Proofness Landscape of Merging

We suppose that ∆d,f
µ is manipulable for idw , i.e., we can find an integrity constraint µ, a
profile E = {K1 , . . . , Kn }, and two bases K and K 0 s.t.:
d,f
0
idw (K, ∆d,f
µ ({K} t E)) < idw (K, ∆µ ({K } t E)).

(12)

This is equivalent to: idw (K, E 4µ K) = 0 and idw (K, E 4µ K 0 ) = 1, where ∆d,f
µ ({K} t E)
is noted E 4µ K for simplifying notations.
Statement (13) states that idw (K, E 4µ K) = 0: no model of K ∧ µ is a model of E 4µ K;
statement (14) states that idw (K, E 4µ K 0 ) = 1: there is as least one model of K ∧ µ in
E 4µ K 0 :
∀ω |= K ∧ µ, ∃ω 0 |= (¬K) ∧ µ, d(ω 0 , E t {K}) < d(ω, E t {K}).

(13)

∃ω1 |= K ∧ µ, ∀ω |= µ, d(ω1 , E t {K 0 }) ≤ d(ω, E t {K 0 }).

(14)

Since in (13) the choice of ω 0 can be made apart from ω, (13) is equivalent to:
∃ω 0 |= (¬K) ∧ µ, ∀ω |= K ∧ µ, d(ω 0 , E t {K}) < d(ω, E t {K}).

(15)

Let ω 00 |= K 0 s.t. d(ω1 , K 0 ) = d(ω1 , ω 00 ). We consider the complete base K 00 s.t. [K 00 ] = {ω 00 }.
we shall show in the rest of the proof that ∆d,f
µ is manipulable with that base. If the agent
whose beliefs/goals are K gives K 00 as a base instead of K, then, since d(ω1 , K 00 ) = d(ω1 , K 0 ),
we have:
d(ω1 , E t {K 00 }) = d(ω1 , E t {K 0 }),
(16)
and then:
∀ω |= µ, d(ω1 , E t {K 00 }) ≤ d(ω, E t {K 0 }),

(17)

with (14) and (16).
Furthermore, since the aggregation function f is non-decreasing (by definition) and since
K 00 |= K 0 , we have ∀ω |= µ, d(ω, E t {K 0 }) ≤ d(ω, E t {K 00 }), so we get immediately with
(17):
∀ω |= µ : d(ω1 , E t {K 00 }) ≤ d(ω, E t {K 00 }).
(18)
d,f
00
00
So ω1 is a model of ∆d,f
µ (E t {K }), and we have idw (K, ∆µ ({K } t E)) = 1 and
d,f
d,f
00
idw (K, ∆d,f
µ ({K} t E)) < idw (K, ∆µ ({K } t E)). This shows that ∆µ is manipulable
00
for a complete base K .

Strong drastic index. Let us assume that an operator ∆d,f
µ , where d is any pseudo-distance
and f any aggregation function, is manipulable for the strong drastic index ids .Then we
can find an integrity constraint µ, a profile E and bases K and K 0 s.t. ids (K, ∆d,f
µ (E t
0 }). This implies that i (K, ∆d,f (E t {K}) = 0, and i (K,
{K}) < ids (K, ∆d,f
(E
t
{K
µ
µ
ds
ds
d,f
0
∆d,f
µ (E t {K }) = 1. This means, by definition of the index, that ∆µ (E t {K}) 6|= K, and
0
∆d,f
µ (E t {K }) |= K.
0
0
0
Given a model ω1 of ∆d,f
µ (E t {K }) and a model ω2 of K s.t. d(ω1 , K ) = d(ω1 , ω2 ),
we define K 00 = {ω2 }. Then we have d(ω1 , K 00 ) = d(ω1 , K 0 ), and: d(ω1 , E t {K 00 }) =
d(ω1 , E t {K 0 }).

95

Everaere, Konieczny & Marquis

0
0
Let us note dmin (E td,f
µ {K }) = min({d(ω, E t {K }) | ω |= µ}, ≤). Since ω1 is a model of
d,f
0
∆µ (E t {K 0 }), we have d(ω1 , E t {K 0 }) = dmin (E td,f
µ {K }). Hence:
0
d(ω1 , E t {K 00 }) = dmin (E td,f
µ {K }).

(19)

00
By definition of min and since ω1 |= µ, we also have: d(ω1 , E t {K 00 }) ≥ dmin (E td,f
µ {K }).
So we get:
0
d,f
00
dmin (E td,f
(20)
µ {K }) ≥ dmin (E tµ {K }).

On the other hand, since K 00 |= K 0 , we have: ∀ω ∈ W, d(ω, K 0 ) ≤ d(ω, K 00 ). Since the
aggregation function f is non-decreasing, we get:
∀ω ∈ W, d(ω, E t {K 0 }) ≤ d(ω, E t {K 00 }),

(21)

d,f
d,f
0
00
0
and then dmin (E td,f
µ {K }) ≤ dmin (E tµ {K }). With (20) we get dmin (E tµ {K }) =
d,f
d,f
00
00
00
dmin (E tµ {K }). Then, with (19), we obtain d(ω1 , E t {K }) = dmin (E tµ {K }). Since
00
ω1 |= µ, we have that ω1 is a model of ∆d,f
µ (E t {K }) too.
d,f
00
00
00
Let ω be a model of ∆d,f
µ (E t{K }). Then, ω |= µ and d(ω, E t{K }) = dmin (E tµ {K }).
d,f
d,f
0
00
00
Then, since dmin (E td,f
µ {K }) = dmin (E tµ {K }), we have d(ω, E t {K }) = dmin (E tµ
0
{K 0 }). With (21), we get d(ω, E t {K 0 }) ≤ dmin (E td,f
µ {K }).
0
Then, by definition of min we have d(ω, E t {K 0 }) = dmin (E td,f
µ {K }).
0
This implies that ω is a model of ∆d,f
µ (E t {K }) too (because ω |= µ), so we can write:
d,f
d,f
00
0
0
∆d,f
µ (E t {K }) |= ∆µ (E t {K }). Since we have ∆µ (E t {K }) |= K, we can infer that
d,f
00
00
∆d,f
µ (E t {K }) |= K, and then ids (K, ∆µ (E t {K }) = 1.

We get then a manipulation for ids with a complete base K 00 , and this completes the proof
of the lemma.

Let us now give the proof of the main theorem:
Weak drastic index. By reductio ad absurdum. We assume that ∆d,Σ
is a manipulable
µ
operator and that it is not manipulable by erosion. Then we can find an integrity constraint
µ, a profile E, and two bases K and K 0 with K 0 6|= K s.t.:
d,Σ
0
idw (K, ∆d,Σ
µ (E t {K})) < idw (K, ∆µ (E t {K })).

Lemma 2 shows that that we can assume [K 0 ] = {ω10 } complete; it comes:
d,Σ
0
idw (K, ∆d,Σ
µ (E t {K})) < idw (K, ∆µ (E t {ω1 })).

This implies that:
idw (K, ∆d,Σ
µ (E t {K}) = 0
0
idw (K, ∆d,Σ
µ (E t {ω1 }) = 1.

96

(22)

The Strategy-Proofness Landscape of Merging

This means there is no model of K ∧ µ which is a model of ∆d,Σ
µ (E t {K}), and that there
d,Σ
is at least one model ω1 of K ∧ µ which is a model of ∆µ (E t {ω10 }). We can express those
facts by two statements:
∀ω |= K ∧ µ, ∃ω 0 |= ¬K ∧ µ, d(ω 0 , E t {K}) < d(ω, E t {K})
and:
∃ω1 |= K ∧ µ, ∀ω |= µ, d(ω1 , E t {ω10 }) ≤ d(ω, E t {ω10 }).
Hence:
∃ω1 |= K ∧ µ, ∀ω |= µ, d(ω1 , ω10 ) + d(ω1 , E) ≤ d(ω, ω10 ) + d(ω, E)

(23)

Let us now define a new base K 00 = {ω1 }. Since we have supposed the operator not
manipulable by erosion and since K 00 |= K, we can then state that it is strategy-proof for
d,Σ
00
idw with K 00 : idw (K, ∆d,Σ
µ (E t {K})) ≥ idw (K, ∆µ (E t {K })). This implies that:
• either idw (K, ∆d,Σ
µ (E t {K})) = 1,
d,Σ
00
• or idw (K, ∆d,Σ
µ (E t {K})) = idw (K, ∆µ (E t {K })) = 0.
00
From equation (22), we can infer that idw (K, ∆d,Σ
µ (E t {K })) = 0, and we have ∀ω |=
0
0
00
00
K ∧ µ, ∃ω |= ¬K ∧ µ, d(ω , E t {K }) < d(ω, E t {K }).

Since K 00 = {ω1 } and the choice of ω 0 can be made independently of ω1 , we have ∃ω2 |=
(¬K) ∧ µ, ∀ω |= K ∧ µ, d(ω2 , E t {ω1 }) < d(ω, E t {ω1 }), that is:
∃ω2 |= ¬K ∧ µ, ∀ω |= K ∧ µ, d(ω2 , ω1 ) + d(ω2 , E) < d(ω, ω1 ) + d(ω, E).
In particular, this statement holds for ω = ω1 , because ω1 |= K ∧ µ. Hence:
d(ω2 , ω1 ) + d(ω2 , E) < d(ω1 , ω1 ) + d(ω1 , E).
Since d(ω1 , ω1 ) = 0, we obtain finally:
d(ω2 , ω1 ) + d(ω2 , E) < d(ω1 , E).

(24)

On the other hand, since ω2 |= µ, with (23), we get:
d(ω1 , ω10 ) + d(ω1 , E) ≤ d(ω2 , ω10 ) + d(ω2 , E).
Summing (24) and (25) side by side, we get:
d(ω2 , ω1 ) + d(ω2 , E) + d(ω1 , ω10 ) + d(ω1 , E) < d(ω1 , E) + d(ω2 , ω10 ) + d(ω2 , E).
Simplifying by d(ω1 , E) and d(ω2 , E), we obtain:
d(ω2 , ω1 ) + d(ω1 , ω10 ) < d(ω2 , ω10 ).
97

(25)

Everaere, Konieczny & Marquis

This contradicts the triangular inequality. So, if manipulation is possible, then it is possible
by erosion with a complete base K 00 = {ω1 }.
Strong drastic index. Let us assume that ∆d,Σ
is manipulable for ids . So we can find a
µ
profile E, an integrity constraint µ and two bases K and K 0 s.t.:
d,Σ
0
ids (K, ∆d,Σ
µ (E t {K})) < ids (K, ∆µ (E t {K })).

This implies that:
ids (K, ∆d,Σ
µ (E t {K}) = 0

(26)

and
0
ids (K, ∆d,Σ
µ (E t {K }) = 1.

This means that there is at least one model of ∆d,Σ
µ (E t {K}) which is not a model of K,
0 }) is a model of K ∧ µ. Let us consider a model ω
(E
t
{K
and that every model of ∆d,Σ
µ
1
0
of ∆d,Σ
µ (E t {K }). We can write that ω1 |= K ∧ µ and:
∀ω |= µ, d(ω1 , E t {K 0 }) ≤ d(ω, E t {K 0 }).
So we have:
∀ω |= µ, d(ω1 , K 0 ) + d(ω1 , E) ≤ d(ω, K 0 ) + d(ω, E).
Let us define K 00 = {ω1 } and show that there is manipulation with this base. Let us assume
d,Σ
00
00
00
that we can find ω 00 |= ∆d,Σ
µ (E t {K }) s.t. ω 6|= K. Since ω |= ∆µ (E t {ω1 }), we have
ω 00 |= µ and
d(ω”, E t {ω1 }) ≤ d(ω1 , E t {ω1 }),
and then:
d(ω”, ω1 ) + d(ω”, E) ≤ d(ω1 , E)
(as d(ω1 , ω1 ) = 0).
0
We know that ω 00 6|= K and ω 00 |= µ, so we can infer that ω 00 6|= ∆d,Σ
µ (E t {K }) (else we
d,Σ
00
0
should have ω |= K). Since ω1 is a model of ∆µ (E t {K }), we have:
d(ω”, E t {K 0 }) > d(ω1 , E t {K 0 }).
Equivalently:
d(ω”, K 0 ) + d(ω”, E) > d(ω1 , K 0 ) + d(ω1 , E).
Since d(ω1 , E) ≥ d(ω”, ω1 ) + d(ω”, E), we obtain:
d(ω”, K 0 ) + d(ω”, E) > d(ω1 , K 0 ) + d(ω”, ω1 ) + d(ω 00 , E).
Simplifying this equation by d(ω”, E), we get:
d(ω”, K 0 ) > d(ω1 , K 0 ) + d(ω”, ω1 ).
If ω2 is a model of K 0 s.t. d(ω1 , K 0 ) = d(ω1 , ω2 ), we have:
d(ω”, K 0 ) > d(ω1 , ω2 ) + d(ω”, ω1 ),
98

The Strategy-Proofness Landscape of Merging

and finally, since d(ω”, ω2 ) ≥ d(ω”, K 0 ) by definition of min, we get:
d(ω”, ω2 ) > d(ω1 , ω2 ) + d(ω”, ω1 ).
This contradicts the triangular inequality.
d,Σ
00
We have shown that every model of ∆d,Σ
µ (E t{K }) is a model of K. Hence, ids (K, ∆µ (E t
{K 00 }) = 1. Since ids (K, ∆d,Σ
µ (E t {K}) = 0 with (26), we have:
d,Σ
00
ids (K, ∆d,Σ
µ (E t {K})) < ids (K, ∆µ (E t {K })),

and a manipulation by erosion with a complete base is possible.

Corollary 12 A profile E is manipulable by K for idw (resp. ids ) given ∆d,Σ
µ and µ if and
only if the manipulation is possible using a complete base Kω |= K, i.e., there exists Kω |= K
d,Σ
d,Σ
s.t. idw (K, ∆d,Σ
µ (E t {Kω })) > idw (K, ∆µ (E t {K})) (resp. ids (K, ∆µ (E t {Kω })) >
ids (K, ∆d,Σ
µ (E t {K}))).
Proof:
(⇒): This is a consequence of Theorem 11 and of Lemma 2, which enable to
state that if K is manipulable for i = idw or i = ids given ∆d,Σ
µ and E, then one can find a
d,f
0
0
0
base K = {ω } |= K complete such that i(K, ∆µ (E t {ω })) > i(K, ∆d,f
µ (E t {K})).
(⇐): If K is strategy-proof for i = idw or i = ids given ∆d,Σ
and E, then ∀[K 0 ] ⊆
µ
d,Σ
d,Σ
0
W, i(K, ∆µ (E t {K }) ≤ i(K, ∆µ (E t {K})). This is in particular true when K 0 is
reduced to a singleton.

Theorem 14 ∆max , ∆min1 , ∆min2 , and ∆Σ are strategy-proof for idw , ids and ip .
V
Proof:
∆max (or equivalently ∆min2 ) is strategy-proof for ip . Indeed, if E is
consistent, then all the models
V of the merged base are models of K1 ∈ E, thus the satisfaction
of K1 is maximal for ip . If E is not consistent,
V then the merged base is valid. Assume that
agent 1 reports K10 instead of K1 . If K10 ∧ {K2 , . . . , Kn } isVconsistent, then no model of
the resulting merged base is a model of K1 . In the case K10 ∧ {K2 , . . . , Kn } is inconsistent,
then the resulting merged base is still valid. From Theorem 1, ∆max (or equivalently ∆min2 )
is also strategy-proof for the two drastic indexes.
V
V
E
is
consistent
is
as
above.
If
E
∆min1 is also strategy-proof for ip . The case when
W
is not consistent, the result of the merging is E and all the models of K1 are models
of the merged base. So, in order to increase the value of the ip index, it is not possible
to increase the number of models of K1 in the result of the merging. Hence one needs to
decrease the number of countermodels of K1 in the merged base. But we shall
show that
V
it is not possible: assume that agent 1 reports K10 instead of K1 . If K10 ∧ {KV
2 , . . . , Kn }
is consistent, then no model of the resulting merged base is a model of K1 (as E is not
consistent).
99

Everaere, Konieczny & Marquis

Then K10 ∧

V

{K2 , . . . , Kn } is not consistent, and we have:
ip (K1 , ∆min1 (K1 t {K2 , . . . , Kn })) =

and
ip (K1 , ∆min1 (K10

#([K1 ∨

#([K1 ])
W
{K2 , . . . , Kn }])

W
#([K1 ∧ (K10 ∨ {K2 , . . . , Kn })])
W
.
t {K2 , . . . , Kn })) =
#([K10 ∨ {K2 , . . . , Kn }])

The numerator of ip (K1 , ∆min1 (K1 t {K2 , . . . , Kn })) is maximal, so in order to increase
ip (K1 , ∆min1 (K10 t{K2 , . . . ,W
Kn })), we have to decrease the denominator of ip (K1 , ∆min1 (K10
0
t{K2 , . . . , Kn })), #([K1 ∨ {K2 , . . . , Kn }]).
We can write the following equality:
#([K10 ∨

_

_
{K2 , . . . , Kn }]) = #([K10 ∧ K1 ∧ ¬( {K2 , . . . , Kn })])+
_
_
#([K10 ∧ ¬K1 ∧ ¬( {K2 , . . . , Kn })]) + #([ {K2 , . . . , Kn }]).

W
In this sum, #([ {K2 , . . . , Kn }]) cannot
be changed. We have to decrease the two other
W
0 ∧¬K ∧¬( {K , . . . , K })]) is minimal if K 0 is such that #([K 0 ∧
terms of this
sum:
#([K
1
2
n
1
1
1
W
¬KW1 ∧ ¬( {K2 , . . . , Kn })]) = 0. In the following, we suppose then that #([K10 ∧ ¬K1 ∧
¬( {K2 , . . . , Kn })]) = 0.
W
For the first term of the sum, #([K10 ∧ K1 ∧ ¬( {K2 , . . . , Kn })]), we can write:
_
#([K10 ∧ K1 ∧ ¬( {K2 , . . . , Kn })]) =
_
_
#([K1 ∧ ¬( {K2 , . . . , Kn })]) − #([K1 ∧ ¬K10 ∧ ¬( {K2 , . . . , Kn })])
So we obtain for the probabilistic index:
ip (K1 , ∆min1 (K10 t {K2 , . . . , Kn })) =
W
#([K1 ]) − #([K1 ∧ ¬K10 ∧ ¬( {K2 , . . . , Kn })])
W
W
W
.
#([K1 ∧ ¬( {K2 , . . . , Kn })]) − #([K1 ∧ ¬K10 ∧ ¬( {K2 , . . . , Kn })]) + #([ {K2 , . . . , Kn }])
a
Now remark that if k is an integer and if a ≤ b, then a−k
b−k ≤ b . If we subtract the same
W
0
integer #([K1 ∧ ¬K1 ∧ ¬( {K2 , . . . , Kn })]) from the numerator and the denominator, then
we get the following inequation:

W
#([K1 ]) − #([K1 ∧ ¬K10 ∧ ¬( {K2 , . . . , Kn })])
W
W
W
≤
#([K1 ∧ ¬( {K2 , . . . , Kn })]) − #([K1 ∧ ¬K10 ∧ ¬( {K2 , . . . , Kn })]) + #([ {K2 , . . . , Kn }])
#([K1 ])
W
W
.
#([K1 ∧ ¬( {K2 , . . . , Kn })]) + #([ {K2 , . . . , Kn }])

So:
ip (K1 , ∆min1 (K10 t {K2 , . . . , Kn })) ≤

#([K1 ])
W
W
#([K1 ∧ ¬( {K2 , . . . , Kn })]) + #([ {K2 , . . . , Kn }])
100

The Strategy-Proofness Landscape of Merging

then
ip (K1 , ∆min1 (K10 t {K2 , . . . , Kn })) ≤ ip (K1 , ∆min1 (K1 t {K2 , . . . , Kn })).
No manipulation is possible, and ∆min1 is strategy-proof for ip . From Theorem 1, ∆min1 is
also strategy-proof for the two drastic indexes.
Finally, ∆Σ is strategy-proof for the three indexes (see Theorem 2).



Theorem 15 ∆ satisfies the (IP) property if and only if for every profile E and every pair
of bases K and K 0 :
• K ∧ ¬∆(E t {K}) |= ¬∆(E t {K 0 }), and
• ¬K ∧ ∆(E t {K}) |= ∆(E t {K 0 }).
Proof:
Suppose that a merging operator ∆ = Bel(κ∆ )) does not satisfy the (IP)
property. Then there is a profile E, an agent i, an OCF κ, an interpretation ω ∈ W s.t.
|κ∆ (E)(ω) − κi (ω)| > |κ∆ (rep(E, {i}, κ)(ω) − κi (ω)|
where rep(E, {i}, κ) is the profile identical to E except that the OCF κi is replaced by κ. In
the following we note ∆(E t {K}) for Bel(κ∆ (E)): it is the initial merged base when agent
i reports her true base K ≡ Bel(κi ), and we note ∆(E t {K 0 }) for Bel(κ∆ (rep(E, {i}, κ)):
it is the merged base obtained by replacing the base K of agent i by another K 0 ≡ Bel(κ).
Focusing on two-strata OCFs, this inequation entails that |κ∆ (E)(ω) − κi (ω)| = 1 and
|κ∆ (rep(E, {i}, κ)(ω) − κi (ω)| = 0. Since |κ∆ (rep(E, {i}, κ)(ω) − κi (ω)| = 0, ω is a model
of both ∆(E t {K 0 }) and of K (*), or ω is a countermodel of both ∆(E t {K 0 }) and of K
(**).
To get |κ∆ (E)(ω) − κi (ω)| = 1, there are also two cases:
• either κ∆ (E)(ω) = 1 and κi (ω) = 0: then ω is a model of K and a countermodel
of ∆(E t {K}). Since ω is a model of K, with (*), we know that ω is a model of
∆(E t {K 0 }). Then ω is a model of K, of ¬(∆(E t {K})) and not of ¬∆(E t {K 0 }):
it entails that
K ∧ ¬∆(E t {K}) 6|= ¬∆(E t {K 0 }).
• or κ∆ (E)(ω) = 0 and κi (ω) = 1: then ω is a countermodel of K and a model of ∆(E t
{K}). Since ω is a countermodel of K, with (**), we know that ω is a countermodel
of ∆(E t {K 0 }). Then ω is a model of ¬K, of ∆(E t {K}) and not of ∆(E t {K 0 }):
it entails that
¬K ∧ ∆(E t {K}) 6|= ∆(E t {K 0 }).
Hence it is not the case that both
• K ∧ ¬∆(E t {K}) |= ¬∆(E t {K 0 }), and
101

Everaere, Konieczny & Marquis

• ¬K ∧ ∆(E t {K}) |= ∆(E t {K 0 }).
are satisfied.
As to the converse, suppose that there is a profile E, an agent i with a base K, and another
base K 0 s.t.:
K ∧ ¬∆(E t {K}) 6|= ¬∆(E t {K 0 })
or
¬K ∧ ∆(E t {K}) 6|= ∆(E t {K 0 }).
In the first case, there is a model ω of K ∧ ¬∆(E t {K}) which is a countermodel of
¬∆(E t {K 0 }) :
κ∆ (E)(ω) = 1, κi (ω) = 0, κ∆ (rep(E, {i}, κ)(ω) = 0.
So
|κ∆ (E)(ω) − κi (ω)| > |κ∆ (rep(E, {i}, κ)(ω) − κi (ω)|.
In the second case, there is a model ω of ¬K ∧ ∆(E t {K}) which is a countermodel of
∆(E t {K 0 }):
κ∆ (E)(ω) = 0, κi (ω) = 1, κ∆ (rep(E, {i}, κ)(ω) = 1.
So
|κ∆ (E)(ω) − κi (ω)| > |κ∆ (rep(E, {i}, κ)(ω) − κi (ω)|.
In both cases, ∆ does not satisfy the (IP) property.
1
#([K⊕K∆ ]))+1 .

Theorem 16 Let iwip (K, K∆ ) =
only if it is strategy-proof for iwip .
Proof:



∆ satisfies the (WIP) property if and

By definition K ⊕ K∆ ≡ (¬K ∧ K∆ ) ∨ (K ∧ ¬K∆ ) So we can write:
iwip (K, K∆ ) =

1
#([(¬K ∧ K∆ ) ∨ (K ∧ ¬K∆ )]) + 1

if and only if
iwip (K, K∆ ) =

1
.
#([¬K ∧ K∆ ]) + #([K ∧ ¬K∆ ]) + 1

Suppose that a merging operator ∆ = Bel(κ∆ )) does not satisfy the (WIP) property. Then
there is a profile E, an agent i, an OCF κ s.t.
Σω∈W |κ∆ (E)(ω) − κi (ω)| > Σω∈W |κ∆ (rep(E, {i}, κ)(ω) − κi (ω)|

(∗)

In the following we note ∆(E t {K}) for Bel(κ∆ (E)), that is the initial merged base when
agent i reports her true base K ≡ Bel(κi ), and we note ∆(E t {K 0 }) for Bel(κ∆ (rep(E, {i},
κ)), i.e., the merged base obtained by replacing the base K of the agent i by another
K 0 ≡ Bel(κ).
In the two-strata case, |κ∆ (E)(ω) − κi (ω)| is equal to 0 or 1. In fact, |κ∆ (E)(ω) − κi (ω)| = 1
if and only if:
102

The Strategy-Proofness Landscape of Merging

• either κ∆ (E)(ω) = 1 and κi (ω) = 0: this is equivalent to ω |= K ∧ ¬∆(E t {K}).
• or κ∆ (E)(ω) = 0 and κi (ω) = 1: this is equivalent to ω |= ¬K ∧ ∆(E t {K}).
We deduce the following equation:
Σω∈W |κ∆ (E)(ω) − κi (ω)| = #([K ∧ ¬∆(E t {K})]) + #([¬K ∧ ∆(E t {K})]).
Similarly, we get:
Σω∈W |κ∆ (rep(E, {i}, κ)(ω) − κi (ω)| = #([K ∧ ¬∆(E t {K 0 })]) + #([¬K ∧ ∆(E t {K 0 })])
The inequation (*) is then equivalent to:
#([K ∧ ¬∆(E t {K})]) + #([¬K ∧ ∆(E t {K})]) > #([K ∧ ¬∆(E t {K 0 })]) + #[¬K ∧ ∆(E t {K 0 })])

which is equivalent to
1
<
#([K ∧ ¬∆(E t {K})]) + #([¬K ∧ ∆(E t {K})]) + 1
1
#([K ∧ ¬∆(E t {K 0 })]) + #([¬K ∧ ∆(E t {K 0 })]) + 1
which is equivalent to
iwip (K, ∆(E t {K})) < iwip (K, ∆(E t {K 0 }))
which is equivalent to the fact that ∆ is not strategy-proof for iwip .


References
Arrow, K. J. (1963). Social choice and individual values (second edition). Wiley, New York.
Arrow, K., Sen, A. K., & Suzumura, K. (Eds.). (2002). Handbook of Social Choice and
Welfare, Vol. 1. North-Holland.
Baral, C., Kraus, S., & Minker, J. (1991). Combining multiple knowledge bases. IEEE
Transactions on Knowledge and Data Engineering, 3 (2), 208–220.
Baral, C., Kraus, S., Minker, J., & Subrahmanian, V. S. (1992). Combining knowledge
bases consisting of first-order theories. Computational Intelligence, 8 (1), 45–71.
Barberà, S., Dutta, B., & Sen, A. (2001). Strategy-proof social choice correspondences.
Journal of Economic Theory, 101 (2), 374–394.
Benferhat, S., Dubois, D., Kaci, S., & Prade, H. (2002). Possibilistic merging and distancebased fusion of propositional information. Annals of Mathematics and Artificial Intelligence, 34 (1–3), 217–252.
Borda, J. (1781). Mémoire sur les élections au srutin. Histoire de l’Académie Royale des
Sciences.
103

Everaere, Konieczny & Marquis

Brams, S. J., & Fishburn, P. C. (1983). Approval voting. Springer Verlag.
Chin, S., & Zhou, L. (2002). Multi-valued strategy-proof social choice rules. Social Choice
and Welfare, 19 (3), 569–580.
Chopra, S., Ghose, A., & Meyer, T. (2006). Social choice theory, belief merging and strategyproofness. Information Fusion, 7 (1), 61–79.
Condorcet, M. (1785). Essai sur l’application de l’analyse à la probabilité des décisions
rendues à la pluralité des voix. Paris.
Conitzer, V., Lang, J., & Sandholm, T. (2003). How many candidates are needed to make
elections hard to manipulate?. In Proceedings of the Ninth Conference on Theoretical
Aspects of Rationality and Knowledge (TARK’03), pp. 201–214.
Conitzer, V., & Sandholm, T. (2002a). Complexity of manipulating elections with few candidates. In Proceedings of the Eighteenth National Conference on Artificial Intelligence
(AAAI’02), pp. 314–319.
Conitzer, V., & Sandholm, T. (2002b). Vote elicitation: complexity and strategyproofness. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI’02), pp. 392–397.
Conitzer, V., & Sandholm, T. (2003). Universal voting protocol tweaks to make manipulation hard. In Proceedings of the Eighteenth International Joint Conference on
Artificial Intelligence (IJCAI’03), pp. 781–788.
Dalal, M. (1988). Investigations into a theory of knowledge base revision: preliminary report.
In Proceedings of the Seventh American National Conference on Artificial Intelligence
(AAAI’88), pp. 475–479.
Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness or shared
beliefs: Gibbard-satterthwaite generalized. Social Choice and Welfare, 17, 85–93.
Everaere, P., Konieczny, S., & Marquis, P. (2005). Quota and gmin merging operators. In
Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence
(IJCAI’05), pp. 424–429.
Gibbard, A. (1973). Manipulation of voting schemes. Econometrica, 41, 587–602.
Kelly, J. S. (1988). Social Choice Theory : An Introduction. Springer-Verlag.
Konieczny, S. (2000). On the difference between merging knowledge bases and combining them. In Proceedings of the Seventh International Conference on Principles of
Knowledge Representation and Reasoning (KR’00), pp. 135–144.
Konieczny, S., Lang, J., & Marquis, P. (2004). DA2 merging operators. Artificial Intelligence,
157 (1-2), 49–79.
Konieczny, S., Lang, J., & Marquis, P. (2005). Reasoning under inconsistency: the forgotten connective. In Proceedings of the Nineteenth International Joint Conference on
Artificial Intelligence (IJCAI’05), pp. 484–489.
Konieczny, S., & Pino Pérez, R. (1998). On the logic of merging. In Proceedings of the Sixth
International Conference on Principles of Knowledge Representation and Reasoning
(KR’98), pp. 488–498.
104

The Strategy-Proofness Landscape of Merging

Konieczny, S., & Pino Pérez, R. (1999). Merging with integrity constraints. In Proceedings of
the Fifth European Conference on Symbolic and Quantitative Approaches to Reasoning
with Uncertainty (ECSQARU’99), LNAI 1638, pp. 233–244.
Konieczny, S., & Pino Pérez, R. (2002). Merging information under constraints: a logical
framework. Journal of Logic and Computation, 12 (5), 773–808.
Liberatore, P., & Schaerf, M. (1998). Arbitration (or how to merge knowledge bases). IEEE
Transactions on Knowledge and Data Engineering, 10 (1), 76–90.
Lin, J., & Mendelzon, A. O. (1999). Knowledge base merging by majority. In Dynamic
Worlds: From the Frame Problem to Knowledge Management. Kluwer.
Maskin, E., & Sjostrom, T. (2002). Handbook of Social Choice and Welfare, Vol. 1, chap.
Implementation Theory, pp. 237–288. North-Holland.
Meyer, T., Ghose, A., & Chopra, S. (2001). Social choice, merging and elections. In Proceedings of the Sixth European Conference on Symbolic and Quantitative Approaches
to Reasoning with Uncertainty (ECSQARU’01), pp. 466–477.
Moulin, H. (1988). Axioms of cooperative decision making, chap. 9. Econometric society
monographs. Cambridge University Press.
Rescher, N., & Manor, R. (1970). On inference from inconsistent premises. Theory and
Decision, 1, 179–219.
Revesz, P. Z. (1993). On the semantics of theory change: arbitration between old and
new information. In Proceedings of the Twelfth ACM SIGACT-SIGMOD-SIGART
Symposium on Principles of Databases, pp. 71–92.
Revesz, P. Z. (1997). On the semantics of arbitration. International Journal of Algebra and
Computation, 7 (2), 133–160.
Satterthwaite, M. (1975). Strategy-proofness and Arrow’s conditions. Journal of Economic
Theory, 10, 187–217.
Shoham, Y., & Tennenholtz, M. (2005). Non-cooperative computation: Boolean functions
with correctness and exclusivity. Theoretical Computer Science, 343 (1-2), 97–113.
Spohn, W. (1987). Ordinal conditional functions: a dynamic theory of epistemic states.
In Harper, W. L., & Skyrms, B. (Eds.), Causation in Decision, Belief Change, and
Statistics, Vol. 2, pp. 105–134.
Tversky, A. (2003). Preference, Belief, and Similarity. MIT Press.

105

