Journal of Artificial Intelligence Research 40 (2011) 523-570

Submitted 9/10; published 2/11

Efficient Planning under Uncertainty with Macro-actions
Ruijie He

RUIJIE @ CSAIL . MIT. EDU

Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139 USA

Emma Brunskill

EMMA @ CS . BERKELEY. EDU

Electrical Engineering and Computer Science Department
University of California, Berkeley
Berkeley, CA 94709 USA

Nicholas Roy

NICKROY @ CSAIL . MIT. EDU

Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139 USA

Abstract
Deciding how to act in partially observable environments remains an active area of research.
Identifying good sequences of decisions is particularly challenging when good control performance
requires planning multiple steps into the future in domains with many states. Towards addressing
this challenge, we present an online, forward-search algorithm called the Posterior Belief Distribution (PBD). PBD leverages a novel method for calculating the posterior distribution over beliefs
that result after a sequence of actions is taken, given the set of observation sequences that could be
received during this process. This method allows us to efficiently evaluate the expected reward of a
sequence of primitive actions, which we refer to as macro-actions. We present a formal analysis of
our approach, and examine its performance on two very large simulation experiments: scientific exploration and a target monitoring domain. We also demonstrate our algorithm being used to control
a real robotic helicopter in a target monitoring experiment, which suggests that our approach has
practical potential for planning in real-world, large partially observable domains where a multi-step
lookahead is required to achieve good performance.

1. Introduction
Consider an autonomous helicopter tasked with protecting ships anchored in a busy harbor. At each
time step, the helicopter must know if anything is moving too close to the ships it is guarding, but
due to its sensor limits, the helicopter cannot observe the whole harbor at once. The only way to
keep its ships safe is to keep moving continuously throughout the harbor, keeping track of all the
other moving agents. The helicopter does well when it senses that another boat has moved too close
to one of its charges, but false alarms are costly. The helicopter’s controller must decide how to
move around, what to report and when, in order to maximize its own performance.
This problem requires decision-making in an uncertain, partially observable domain, a common challenge for any agent operating in a real-world environment. The helicopter problem just
described is an example of a general class of problems that are particularly difficult for two reasons.
First, to make a decision, the agent must take into consideration its present estimate of the location and orientation of each of the targets. All of these quantities will typically be real-valued. In
c
2011
AI Access Foundation. All rights reserved.

H E , B RUNSKILL , & ROY

the standard terminology of Markov decision processes (MDPs), the state space consists of a large
number of continuous variables. Second, to make a decision now, the agent must reason about how
its estimate of the state of the world may change many time steps into the future, under different
possible helicopter and target actions. Any problem with many variables to consider and a long
time horizon to plan over suffers from the curse of dimensionality and the curse of history (Pineau,
Gordon, & Thrun, 2003a). We refer to such problems as large and long.
In this paper we present a new planning algorithm for large, long, partially observable MDPs
(POMDPs), such as the target monitoring example. Beyond target monitoring, there are numerous
other problems, such as scientific exploration of extreme environments and autonomous management of retirement portfolios, which may be posed as large, long POMDPs.
Though there has been substantial progress in POMDP planning over the last decade, most
approaches still struggle to scale to large domains described by many state variables, where each
variable may take on a large or infinite number of potential values. Symbolic Perseus (Poupart,
2005) was used to find a good solution to a hand-washing domain with 11 state variables, but
each variable took on a relatively small number of values (at most 10 values). Recently online
forward search approaches have been used to achieve encouraging performance on some large1
POMDPs, such as the work by Ross, Chaib-draa and Pineau (2008b) and Paquet, Tobin and Chaibdraa (2005). However, the cost of performing a generic forward search scales exponentially with the
search horizon. The target monitoring example described above not only is too large to be solved by
offline approaches, but, as we will demonstrate later, also requires a long horizon search to achieve
good performance, limiting the effectiveness of standard forward search for long problems.
As an effort towards scaling to large, long, partially observable decision making, we introduce the Posterior Belief Distribution (PBD) algorithm. PBD leverages the insight that for certain
environments which have specific structure, the distribution of belief states (which in turn are distributions over states) that arise from a fixed sequence of actions can be computed efficiently and
analytically. This distribution over beliefs, or posterior belief distribution, allows us to scale to large,
long POMDP problems using efficient forward search with temporally-extended action sequences,
which we refer to as macro-actions. PBD selects an action for the current belief by planning over
a restricted policy space defined by the input macro-action set, and then re-plans after the selected
action is taken and a new observation is received. Note that this implies that the policy executed
does not necessarily equal the policy space used for planning, since only the first step of a macroaction is executed before re-planning is performed. This characteristic of PBD is very similar to
receding horizon controllers (RHC) (such as Mayne, Rawlings, Rao, & Scokaert, 2000; Kuwata
& How, 2004). RHCs consider a finite-horizon policy space when performing planning, but can
execute over a much longer horizon by repeatedly re-planning.
In this paper we demonstrate that our PBD algorithm achieves good performance on large, long
POMDP problems which are either outside the scope of prior approaches, or on which prior approaches fail to find good quality policies. Our experimental results demonstrate that PBD performs
well with an attractive computational cost on several large, long simulation problems, including a
variant of the ROCKSAMPLE POMDP benchmark problem (Smith & Simmons, 2005) and a simulated target monitoring example. We also demonstrate the PBD algorithm on a real-world version
of the target monitoring problem, where we use a robotic helicopter platform to monitor multiple
ground vehicles (Section 6.4). This demonstration suggests that PBD has practical potential for real
1. Unless otherwise specified, when we describe a domain as “large” we will be referring to a domain described by the
values of a number of state variables, where each variable can take on many or an infinite number of values.

524

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

robotic domains. In this paper, the macro-actions are assumed to be provided by a domain expert2 ;
however, to decouple the impact of our specific choice of macro-actions, we also provide experimental results where we modify alternate approaches (including a state-of-the-art planner) to use
macro-actions, and still find performance advantages for our presented methods.
The rest of the paper is organized as follows. Section 2 first provides a brief background on
planning under uncertainty using forward search. We then introduce our PBD algorithm in Section 3, and consider a slight variant of PBD that is applicable to a larger set of domains in Section 4.
In Section 5 we provide a formal analysis of the PBD algorithm, and then in Section 6 we present
experimental results. We present related work in Section 7 and finally conclude in Section 8.

2. Background: Planning under Uncertainty using Forward Search
Formally, we assume that our decision-making under state-uncertainty problem consists of the following known components:
• S is a set of states. Each state s ∈ S consists of an assignment of values to each of L state
variables, sl . The domain of each state variable may be either discrete or continuous.
• A is a set of actions (controls) a ∈ A, which can be either discrete or continuous.
• Z is a set of observations z ∈ Z, which can be either discrete or continuous.
• p(s′ |s, a) is a transition function (also known as a dynamics model) which encodes the probability of transitioning to state s′ after taking action a from state s. We assume the dynamics
satisfy the Markov assumption that the new state is only a function of the immediately prior
state and action.
• p(z|s) is an observation function (also known as a measurement or sensor model) that encodes
the probability of receiving observation z in state s.3
• b0 is a distribution over possible initial states, where b0 (s) is the probability that the initial
state is s. This distribution is known as the initial belief state, and is a well-formed distribution
that sums to one across all states.
• r(s, a) is a reward (or cost) function that describes the utility the agent receives for taking
action a in state s. Slightly abusing notation, r(b, a) is the expected reward for taking action
a given a distribution over current states (belief) b.
• γ is a discount factor that determines the weights of immediate rewards relative to the rewards
that will be received at a later time step.
The states S are not fully observable. Instead, at every time step, the agent receives an observation after taking an action. The agent must therefore make decisions based on the prior history
of observations it has received, z1:t , and actions it has taken, a1:t , up to time t. As the world states
are assumed to be Markov, instead of maintaining an ever-expanding list of past observations and
2. In other work we have demonstrated that we can automatically construct good macro-actions for smaller
POMDPs (He, Brunskill, & Roy, 2010b). Integrating these two lines of work is an interesting area for future work
but is outside the scope of this paper.
3. It is easy to extend our framework to allow the observation to depend on the prior state, action, and posterior state.

525

H E , B RUNSKILL , & ROY

actions, a sufficient statistic, known as a belief bt (s), is used to summarize the probability of the
world being in each state given its past history,
bt (s) = P r(st = s|a0 , z1 , . . . , zt−1 , at−1 , zt ).

(1)

The agent can therefore plan based only on the current belief state, rather than on all past actions
and observations (Smallwood & Sondik, 1973). For example, in the target monitoring problem
introduced in Section 1, the agent maintains a belief over the possible locations of each target. The
agent updates its belief at each step, after taking an action a and receiving an observation z (such as
a camera image of a far off target), using the Bayes filter:
Z
p(s′ |s, a)b(s)ds
(2)
b′ (s′ ) = τ (b, a, z) = η p(z|a, s′ )
s∈S

where τ (b, a, z) represents the belief update function and η is a normalization constant.
The planning problem is to compute a policy π : b → a, which is a mapping from belief states
to actions, that maximizes the expected sum of future4 discounted utilities:
"∞
#
X
i
π = argmax
γ E[r(bi )] ,
(3)
i=1

where E[r(bi )] denotes the expected reward at time step i given the actions specified by π and
possible observations received.
Many POMDP solvers, such as those by Smith and Simmons (2005), Porta, Vlassis, Spaan,
and Poupart (2006) and Kurniawati, Hsu, and Lee (2008), perform POMDP planning offline by
calculating a value function over the belief space V : b → R. V (b) is the expected total reward of
starting from any belief state b and following an optimal policy5 ,
Z


p(z|b, a)V (τ (b, a, z)) ,
(4)
V (b) = max r(b, a) + γ
a∈A

z∈Z

R
where p(z|b, a) = s p(z|s, a)b(s)ds. Given a value function over the belief space, a policy π can
be extracted by finding the action a which maximizes Equation 4.
Instead of computing a value function over the entire belief space in advance of acting, we take
an alternate approach of planning online, only explicitly computing a policy (that is, an action) for
the current belief. In particular, an action is selected by performing a fixed-horizon forward search
which is used to estimate the values of each of the possible action choices starting from the current
belief. This action-selection approach is closely related to methods from the controls community,
including Model Predictive/Receding Horizon Control, and forward search has also received recent
attention in the AI POMDP community (see the recent survey in Ross, Pineau, Paquet, & Chaibdraa, 2008a).
To select an action for the current belief, generic forward search approaches compute a lookahead AND-OR tree (Figure 1). The goal of the tree is to estimate the value of taking each of the
4. We will assume in this paper that we are interested in problems with an infinite horizon. If the problem has a finite
horizon, the discount factor γ can be set to 1, and our forward search process (which we will shortly describe) will
search out to a depth of at most the problem’s finite horizon.
5. This is often intractable to compute, so in practice the value function is often approximate.

526

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

Figure 1: A forward search tree. a are actions, z are observations, and b are beliefs. b0 is the initial
belief, while bi,j refers to the jth belief leaf node at depth i.

possible actions from the current belief b, in order to take the action with the greatest value. Given
the root belief b, the tree is constructed by first branching on all possible actions from the root. After
each action, the tree then branches on possible observations. For each distinct action-observation
combination, we can compute the resulting internal belief that would occur if that action-observation
trajectory were followed using Equation 2. This process of alternately branching on actions and observations is repeated out to a finite depth. This depth, known as the search horizon, determines
how far into the future the effects of actions are considered when selecting a possible action for the
root (current) belief state.
Once the tree has been constructed, the value of the actions at the root are computed by propagating the rewards from the beliefs at the leaf nodes back to the root. Starting at the leaf node
rewards, we take an expectation over observations. We then add in the expected immediate reward
from taking the parent action, and next take the maximum reward across all sibling action nodes.
This process is repeated all the way up to the root node. The expected rewards are maximized across
actions but summed across observations because the agent can choose which action to take, but must
optimize over the expected distribution of observations.
After the planning phase, the forward search procedure executes the action at the root with
the largest value, and then receives an observation. Given the previous belief, action taken, and
observation received, a new belief is computed using Equation 2. The forward search planning
process then repeats, with the new belief as the root node. Re-planning after every time step enables
the agent to condition on the action selected and the actual observation received.
There are a number of attractive characteristics of an online, forward-search framework. First,
computational effort is directed only towards belief states that are reachable from the current belief
under different actions. This property enables a forward search planner to compute a meaningful
policy in an arbitrarily large environment, since only a subset of the environment is relevant at
any point. Second, online, forward-search fits well into systems that need good, time constrained
solutions where a large amount of advance computation is not possible. Lastly, forward search does
527

H E , B RUNSKILL , & ROY

not have to compute an explicit representation of the value function, which can be an advantage in
factored domains where belief updating and immediate expected reward calculations are relatively
simple, but the value function itself is complex to represent.6
However, the computational cost of generic forward search will still scale with the cost of the belief updating and immediate expected reward calculations, multiplied by the number of tree nodes
which grows exponentially with the search horizon. The costs of belief updating and calculating
the immediate expected reward typically scale either linearly or exponentially with the number of
state variables and the size of their respective domains, depending on the independence relations
among the state variables. When the state variables are continuously-valued, and therefore take
on an infinite number of values, we will typically need to employ some parametric or compressed
representation in order to make these calculations tractable. The number of tree nodes scales exponentially with the horizon according to O((|A||Z|)H ), where |A| and |Z| are the number of actions
and observations respectively and H is the search horizon. Therefore, standard forward search approaches will typically struggle when there are many state variables and/or state variables with large
domains and when a large H-step lookahead is necessary to achieve good performance.
One approach to accelerating planning over large, long horizon problems is to use temporally
extended macro-actions, a technique that has been used successfully in fully observable settings for
some years (Sutton, Precup, & Singh, 1999). There has been limited exploration of these ideas for
partially observable settings (exceptions include those by Theocharous & Kaelbling, 2003; Hsiao,
Lozano-Pérez, & Kaelbling, 2008; Kurniawati, Du, Hsu, & Lee, 2009). In our work we define a
macro-action as a finite open-loop sequence of primitive actions that is executed without regard to
the observations received during the execution of this action sequence. For example, in our target
monitoring problem, one macro-action could be for the helicopter to travel to a key region, which
might involve a sequence of individual turns and straight line moves. By restricting the action space
to a set of length L macro-actions, the number of expanded nodes due to the action branching factor
can be reduced from|A|H to |Ã|H̃ where Ã is the set of length L (or longer) macro-actions, and
7
H̃ = H
L is the macro-action horizon or depth .
2.1 Macro-action Construction
If only a small set of macro-actions are evaluated during the search, the restricted action space will
result in significant computational savings due to the smaller exponent H̃ (vs. H) in the computational complexity expression. However, this restriction can also result in poor algorithmic performance if all the macro-actions being evaluated are unsuitable. In this paper, we assume that
macro-actions are provided by a domain expert as part of a comprehensive strategy to scaling up
to large problems with a multi-step lookahead. The macro-actions we use in our experimental results consist of open-loop policies which are a function of properties of the belief state at which
the macro-action is originated, and can be either computed and stored offline or computed online at
every timestep. Further details are provided in the experimental section.
Our reliance on domain knowledge in this paper is similar to prior work in the fully observable
community that separately investigated the potential advantage of macro-actions before turning to
6. An example of such a domain is one in which the state space is a set of independent variables, but the reward is an
aggregate function of these variables.
7. The macro-action depth refers to the number of macro-actions that are executed in sequence from the root belief node
to the leaves.

528

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

the challenge of learning these macro-actions (see the work by Sutton et al., 1999 for an overview of
one particular formalism). Although constructing macro-actions automatically is beyond the scope
of this paper, we have presented in related work a domain-independent algorithm (PUMA) that automatically generates macro-actions for planning in partially observable domains (He et al., 2010b).
Borrowing the notion of sub-goal states from the fully-observable planning literature (McGovern,
1998; Stolle & Precup, 2002), PUMA uses a heuristic that macro-actions can be designed to take
the agent, under the fully-observable model, from a possible start state under the current belief to
a sub-goal state. The PUMA algorithm was tested on variations of the experimental domains that
are used in this paper, and we encourage the reader to refer to the above-mentioned paper for more
details.
Regardless of how the set of macro-actions are generated, several key computational challenges
remain to scale macro-action forward-search to large, long environments. First, recall the number
of nodes in generic forward search scales as O(|A|H |Z|H ). Using macro-actions reduces the first
term in the product, but does not directly change the second term, so the number of tree nodes still
is an exponential function of the search horizon H. Second, using macro-actions does not directly
alleviate the cost of performing belief updates and expected reward computations at each tree node,
and these computational costs can be substantial in large domains. The central contribution of our
paper is a method for efficiently and analytically computing the result of a macro-action given any
possible observation sequence received during its execution. This will allow us to use temporallyextended actions to scale to certain types of large, long POMDPs.

3. The Posterior Belief Distribution Algorithm
To plan with macro-actions in a forward search manner, we must compute the expected reward received during a macro-action, as well as the expected future value after taking that macro-action.
The reward the planner can expect to receive from a macro-action is the expected sum of the rewards under each of the posterior beliefs the agent will reach after each action in the macro-action.
However, the process is complicated by the fact that posterior the belief is also a result of receiving
an observation. As the agent does not know which observations will be received during the macroaction, it cannot compute a single posterior belief reached during the macro-action, and therefore
cannot compute the expected reward.
Of course, an easy solution is to consider all possible observations, and compute the expected
reward of all possible beliefs that can result from all possible observations that could be received
during a macro-action. By computing the expected reward at each observation node, the AND-OR
tree constructed during forward search implicitly computes this expectation over all possible observation sequences. But, if computing the expected reward of a macro-action requires enumerating
all possible observation sequences that could be experienced during execution, the evaluation of a
macro-action will grow intractable quickly (see Figure 2(a)). The number of observation sequences
to be considered will grow exponentially with the length of the macro-action, and enumerating all
possible observations may not even be feasible in domains with continuous observations. One alternative may be to sample observation sequences for a given macro-action (Figure 2(b)), but sampling
is likely to still be computationally intensive due to the per-sample cost of performing a belief update
and expected reward calculation at each step of each sampled observation sequence.
We can avoid this computational burden by realizing that it is sometimes possible to analytically
represent the distribution over posterior beliefs. For a given sequence of actions, what we need is the
529

H E , B RUNSKILL , & ROY

(a) Exhaustive

(b) Sampled

(c) Analytic

Figure 2: Three methods to represent the resulting set of beliefs after a single macro-action. (a)
All possible observations are expanded. (b) A subset of possible observation trajectories
are sampled. (c) Compute an analytic distribution over the posterior beliefs, which could
have been generated via an exhaustive enumeration of all possible observation sequences.
b0 is the initial belief, while bi,j refers to the j th belief leaf node at depth i.

expected reward for those actions; if we cannot compute the distribution over states ahead of time,
but can compute a distribution over state distributions, we can still compute the expected reward. A
graphical depiction of this process is shown in Figure 2(c). By analytically computing a distribution
over beliefs, we avoid not only the exponential explosion of potential observation sequences (as a
function of the macro-action length), but also the costly step of performing many individual belief
updates along the possible observation sequences.
We define bdist as the posterior distribution over beliefs after a macro-action. We will show
in the next subsection (3.1) that when the parametric form of the model is such that the belief
is always Gaussian, then the distribution over posterior beliefs is itself a Gaussian over Gaussian
beliefs, as illustrated in Figure 3. This property follows from the fact that all future beliefs are
Gaussian. The random variables described by the distribution over posterior beliefs are therefore
the means and covariances of the posterior beliefs. In this case, bdist consists of an expression
for the distribution over belief means and an expression for the distribution over the covariances
after a macro-action. We will show that the means are distributed according to a Gaussian and
the covariances are a delta function over a single covariance, allowing us to represent the entire
distribution over beliefs as a Gaussian distribution over beliefs means and a single belief covariance.
In Section 3.2 we will further show that we can analytically compute the expected reward of the
distribution over beliefs resulting from a macro-action for certain classes of reward functions. Given
the ability to analytically compute a distribution over posterior beliefs, we will show in Section 5 the
computational complexity of forward search is reduced to a function of the macro-action horizon
H̃: for macro-actions of length 2 or more (L ≥ 2) we will see that it is significantly faster to search
to long horizons.
530

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

Figure 3: Distribution of posterior beliefs. a) A single Gaussian posterior belief is the result of
incorporating an observation sequence. b) Over all possible observation sequences, the
distribution of posterior means is a Gaussian (black line), and for each posterior mean, a
Gaussian (blue curve) describes the agent’s posterior belief.

3.1 Exact Computation of Posterior Belief Distribution
Let us assume for the moment that the agent’s belief can be exactly represented as a Gaussian
distribution over a continuous state space, and that the observation and transition models are both
linear-Gaussian. Formally, the state transition and observation models can be represented as follows:
εt ∼ N (0, P )

st = Ast−1 + Bat + εt ,

δ ∼ N (0, Q)

zt = Cst + δ,

(5)
(6)

where A and B are dynamics matrices, C is the observation matrix, P is the covariance of the
Gaussian dynamics process and Q is the covariance of the measurement noise.
When the state-transition and observation models are normally distributed and linear functions
of the state, the Kalman filter (1960) provides a closed-form solution for the posterior belief over
states, N (µt , Σt ) given a prior belief over states, N (µt−1 , Σt−1 ),
µt = µt + Kt (zt − Cµt )

µt = Aµt−1 + Bat
Σt = AΣt−1 AT + P

Σt = (C T Q−1 C +

−1
Σt )−1 ,

(7)
(8)

where N (f, F ) is a D-dimensional Gaussian with mean f and covariance matrix F ,
Kt = Σt C T (CΣt C T + Q)−1 is the Kalman gain and µt and Σt are the mean and covariance after
an action is taken but before incorporating the measurement.
Our key interest is to represent the distribution over possible beliefs that could result after taking
a particular action, but receiving any of the possible observations. Note that in the current setup,
all posterior beliefs are Gaussians, and can therefore be completely characterized by their mean and
covariance. We now derive an expression for the distribution over the posterior belief means, under
any possible observation, when the prior distribution over beliefs is simply a delta function over a
single belief. We first re-express the observation model as
zt ∼ N (Cst , Q)
531

(9)

H E , B RUNSKILL , & ROY

which we can use to compute an expression for the probability of an observation given the belief
mean, p(zt |µt ), by marginalizing over st ∼ N (µt , Σt ), as
R
(10)
p(zt |µt ) = p(zt |st )p(st |µt )dst
= N (Cµt , CΣt C T + Q).

(11)

We can perform further linear transformations to obtain an expression for the distribution of posterior means, under any potential observation:
zt ∼ N (Cµt , CΣt C T + Q)

zt − Cµt ∼ N (0, CΣt C T + Q)

Kt (zt − Cµt ) ∼

µt + Kt (zt − Cµt ) ∼
µt ∼
µt ∼

T

N (0, Kt (CΣt C + Q)KtT )
N (µt , Kt (CΣt C T + Q)KtT )
N (µt , Kt (CΣt C T + Q)KtT )
N (µt , Σt C T KtT )

(12)
(13)
(14)
(15)
(16)
(17)

where Equation 17 is computed by substituting the definition of the Kalman gain.
At this point, a somewhat unusual change has occurred, in that µt , the mean of the distribution
itself, is now a random variable. Without knowing the value of the particular observation that
occurs after a primitive action, we cannot deterministically predict the posterior mean of the belief.8
However, we can model the probability of any specific belief state, which effectively means that
we will compute a distribution over the belief means µ and covariances Σ. Equation 17 shows
that the distribution over the belief means is normally distributed about µt , with a covariance that
depends on the prior covariance Σt and the observation model parameters. Sampling a mean from
this distribution is equivalent to selecting a particular observation.
We have just presented a formula for calculating the posterior distribution over belief means
after one action, and any possible observation. We now wish to show that the posterior distribution
over beliefs means after a sequence of actions remains a Gaussian distribution. This will allow us
to compute an analytic expression for the posterior distribution over beliefs that could result from
a macro-action. We therefore require a method to iteratively use Equation 17 in order to compute
the posterior distribution over beliefs for a complete macro-action and any possible observation
sequence.
We first combine the process and measurement updates for a single primitive action belief update in order to get an expression for the posterior belief means in terms of the prior belief mean.
We marginalize over µt , theR posterior belief after the transition update but before the observation
update, using p(µt |µt−1 ) = p(µt |µt )p(µt |µt−1 )dµt . As µt is a deterministic function of µt−1 (see
Equation 7a), then p(µt |µt−1 ) is simply a delta function, which means that p(µt |µt−1 ) is identical
to Equation 17 after substituting µt using Equation 7a:
p(µt |µt−1 ) = N (Aµt−1 + Bat , Σt C T KtT ).

(18)

In a one-step belief update, the belief mean at the prior time step, µt−1 , is assumed to be a known
value. However, for a macro-action, once the first primitive action has been taken, the posterior be8. Note that we will show later in this section that we can deterministically predict the posterior belief covariance. Its
distribution is a Dirac delta that is independent of the specific observation received.

532

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

lief mean will depend on the received observation. In absence of the knowledge of that received observation, we will instead have a distribution over the belief means. Therefore, for the second primitive action in the macro-action, the prior belief is now given as a Gaussian µt−1 ∼ N (mt−1 , Σµt−1 )
where mt−1 and Σµt−1 are random variables. In order to compute the probability distribution over
µt , we must integrate over this distribution of prior belief means µt−1 :
Z
(19)
p(µt |µt−1 )p(µt−1 |mt−1 , Σµt−1 )dµt−1 .
p(µt |mt−1 , Σµt−1 ) =
µt−1

Since both terms inside the integral are Gaussian distributions, we can analytically combine these
two Gaussians, one of which is independent of µt−1 and one of which is dependent on µt−1 . Integrating over µt−1 , as we had done in Equations 9-11, we find that the mean of the posterior belief
means is conveniently still a Gaussian distribution over a function of the prior mean of the belief
means and covariance:
µt ∼ N (Amt−1 + Bat , AΣµt−1 AT + Σt C T KtT )

(20)

µt ∼ N (mt , Σµt )

(21)

or

where mt = Amt−1 + Bat and Σµt = AΣµt−1 AT + Σt C T KtT . Equation 20 can now be used to
predict the posterior mean distribution after a multi-step action sequence. Assuming that the agent
is currently at time t and has a particular prior mean µt (which we can also express as a Gaussian
with zero covariance, N (µt , 0)), the posterior mean after an action sequence of D time steps is
distributed as follows:
µt+D ∼ N (mt+D , Σµt:t+D )

(22)

where
mt+D = f (µt−1 , A, B, at+1:t+D )
= A mt+D−1 + B at+D
D
X
AD−i Bat+i ,
= AD mt +

(23)
(24)
(25)

i=1

and
Σµt:t+D

=

t+D
X

At+D−i Σi CiT DiT (At+D−i )T .

(26)

i=t

Note that mt+D does not depend on observations; it gives the mean of the distribution of beliefs that
might result from the received observations. mt+D is dependent only on the state-transition model
parameters and can be calculated via a recursive update along the action sequence.
We now consider the covariance of the posterior beliefs that may result after taking a macroaction. Recall that for a single belief, the posterior covariance after taking a primitive action and
receiving a particular observation can be calculated using Equation 8. Note that this formula is independent of the actual received observation zt , and the prior µt−1 or posterior mean µt . Formally, this
533

H E , B RUNSKILL , & ROY

property exists because the Fisher information associated with the observation model is independent
of the specific observations. Therefore, the posterior covariance after any observation sequence of
known length can be calculated in closed form given the prior covariance, without needing to know
the observations received along the way.
We can now specify the form of bdist , the posterior distribution over beliefs after a macro-action:
bdist (µt+T , Σ) = N (f (µt−1 , A, B, at:t+T ), Σµt:T ) · δ(Σ, Σ′ )

(27)

where bdist (µt+T , Σ) is the probability of arriving in posterior belief b = N (µt+T , Σ) after taking a
particular macro-action, Equation 22 defines the distribution over belief means, and Σ′ is computed
by iteratively applying Equation 8. This expression shows that for problems with linear-Gaussian
state-transition and observation models, we can exactly calculate the distribution of posterior beliefs
associated with a macro-action.
3.2 Calculating the Expected Reward
The prior section outlined a procedure for calculating the posterior set of beliefs after a macroaction. The reason to compute this distribution is in turn to be able to calculate the expected reward
of each macro-action, which will be used to compute the best action for the current belief.
To calculate the expected reward of a macro-action, we start by considering the expected reward
of starting in a particular belief state b0 and executing a L-length macro-action ã consisting of
actions a1 , a2 , . . . , aL . This may be expressed as
Z
r(b0 , ã1:L ) = r(b0 , a1 ) + γ
p(z1 |b0 , a)Q(ba1 ,z1 , ã2:L )
(28)
z1

ba1 ,z1

to represent the updated belief after taking action a1 and receiving obserwhere we have used
vation z1 from b0 , ã2:L to represent the macro-action consisting of the second through L-th primitive
actions of the macro-action ã, and Q(ba1 ,z1 , ã2:L ) to represent the future expected reward of taking
the remaining actions from belief ba1 ,z1 . Recursively expanding the second term in Equation 28 we
obtain the following expression
Z
p(z1 |b0 , a1 )r(ba1 ,z1 , a2 ) +
r(b0 , ã1:L ) = r(b0 , a1 ) + γ
z1
Z
2
γ
p(z1 |b0 , a1 )p(z2 |ba1 ,z1 , a2 )r(ba1 ,z1 ,a2 ,z2 , a3 ) + · · ·
(29)
z1 ,z2

γ

L−1

Z

z1 ,...,zL

"L−1
Y
i=1

p(zi |b

a1 ,z1 ,...,ai−1 ,zi−1

#

, ai ) r(ba1 ,...aL−1 ,zL−1 , aL ). (30)

The first term in Equation 29 represents the expected reward from taking the first primitive action
in the macro-action from the initial belief state. The remaining terms each represent the expected
reward at the i-th primitive action of the macro-action, where the expectation is taken over all
possible i − 1 length sequences of observations that could have been received up to that point (as
well as the standard integration over the state space). From Equation 27 we have a closed form
expression for the distribution over belief states possible after a sequence of primitive actions. We
can use this to re-express Equation 29 as a function of the distributions over beliefs:
r(b0 , ã1:L ) = r(b0 , a1 ) +

L
X
i=2

534

γ i−1 r(bi−1
dist , ai )

(31)

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

where bi−1
dist is used to represent the posterior distribution over beliefs that results after taking the
first i − 1 primitive actions in macro action ã. Slightly abusing notation, r(bdist , ai ) represents
the expected reward for taking action ai given the posterior distribution over beliefs bdist , and is
expressed as
Z Z
b(s)bdist (b)r(s, ai )dsdb.
(32)
r(bdist , ai ) =
b

s

Combining Equations 31 and 32, we can see that the expected reward of a macro-action can be
calculated from the sum of the expected reward of taking a primitive action from the posterior
distribution of beliefs at each step along the macro-action.
Recall from the prior section that the posterior distribution over beliefs can be factored into a
Gaussian distribution over the belief means µ (Equation 22), and a Dirac delta distribution over the
belief covariances Σ (since all beliefs will have identical covariances):
bdist (µ, Σ) = N (µ|ma , Σµa )δ(Σ, Σa )

(33)

where ma is the mean of the belief means after primitive action a, Σµa is the covariance of the belief
means after primitive action a, and Σa is the covariance of a belief state after primitive action a.
As the belief state itself is a Gaussian,
b(s) = N (s|µ, Σ),
we can re-express the reward as
Z Z
r(s, a)N (s|µ, Σ)N (µ|ma , Σµa )δ(Σ, Σa )dsdµdΣ
r(bdist , a) =
s µ,Σ
Z Z
r(s, a)N (s|µ, Σa )N (µ|ma , Σµa )dµds,
=
s

(34)

(35)
(36)

µ

where the second line follows due to the Dirac delta distribution on the belief covariances. Expanding out the formula for N (s|µ, Σ) we see it is identical to the formula for N (µ|s, Σ):
1
1
exp(− (s − µ)Σ−1 (s − µ)T )
N
/2
d
2
2π|Σ|
1
1
exp(− (µ − s)Σ−1 (µ − s)T )
= √
2
2π|Σ|Nd /2
= N (µ|s, Σ).
√

N (s|µ, Σ) =

Therefore, we can substitute the equivalent expression to yield
Z Z
r(s, a)N (µ|s, Σa )N (µ|ma , Σµa )dµds.
r(bdist , a) =
s

(37)
(38)
(39)

(40)

µ

Completing the square in the exponent, we re-express the product of the above two Gaussians as
Z Z
r(s, a)N (s|ma , Σa + Σµa )N (µ|ĉ, Ĉ)dµds,
(41)
r(bdist , a) =
s

µ

535

H E , B RUNSKILL , & ROY

µ −1 −1
where Ĉ = (Σ−1
and ĉ = Ĉ(ma (Σµa )−1 + µΣ−1
a + (Σa ) )
a ). We then integrate over µ to get
Z
r(s, a)N (s|ma , Σa + Σµa )ds.
(42)
r(bdist , a) =
s

If the reward model itself is a weighted sum of Nr Gaussians,
r(s, a) =

Nr
X
j=1

wj N (s|ζj , Υj ),

(43)

then the integral in Equation 42 can be evaluated in closed form as
Z X
Nr
r(bdist , a) =
wj N (s|ζj , Υj )N (s|ma , Σa + Σµa )ds

(44)

s j=1

=

Nr
X
j=1

wj N (ζj |ma , Υj + Σa +

Σµa )

Z

s

N (s|c1 , C1 ),

(45)

where we have again completed the square in the exponent, and defined new constants C1 = (Υ−1
j +
µ −1 −1
µ −1
−1
(Σa + Σa ) ) and c1 = C1 (ζj Υj + ma (Σa + Σa ) ). Integrating we obtain an analytic
expression for the expected reward of a primitive action under a distribution of beliefs:
r(bdist , a) =

Nr
X
j=1

wj N (ζj |ma , Υj + Σa + Σµa ).

(46)

A similar closed-form expression is available if the reward model is a polynomial function of
the state,
r(s, a) =

Nr
X

wj sj ,

(47)

j=1

instead of a weighted sum of Gaussians. Substituting Equation 47 into Equation 42 yields
Z X
Nr
r(bdist , a) =
wj sj N (s|ma , Σa + Σµa )ds
s j=1

=

Nr
X
j=1

wj

Z

s

sj N (s|ma , Σa + Σµa )ds.

(48)

Therefore, evaluating the expected reward involves calculating the first Nr moments of a Gaussian
distribution. Each of these moments is an analytic expression of the Gaussian mean and covariance.9 So, for reward models that are either a weighted sum of Gaussians, or which are polynomial
functions of the state space, the expected reward of a macro-action (Equation 28) can be computed
analytically.
For other arbitrary reward models it may not be possible to analytically compute the expected
reward of taking a primitive action in a particular distribution over beliefs. In such cases, we can
approximate the expectation in Equation 42 by sampling.
9. The Gaussian distribution is completely described by its first two moments; all higher order moments are simply
functions of the first two moments.

536

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

Figure 4: In PBD, individual beliefs b are sampled from the posterior distribution over beliefs bdist ,
implicitly sampling a particular observation trajectory. Then the best macro-action is
selected for each sampled posterior belief. A sum is taken over all the sampled beliefs,
again corresponding to a sum over the implicitly sampled observation sequences. Here,
bi refers to beliefs at macro-action depth i.

3.3 Branching on Posterior Beliefs
So far we have discussed how to compute the posterior distribution over beliefs that can arise after
executing a single macro-action, and how to compute the expected reward associated with that
distribution. But during planning we wish to compute the value of not taking just a single macroaction, but sequences of macro-actions. This allows us to consider scenarios much further in the
future, which can be useful in selecting the best action to take for the current belief. For example,
consider a large office space domain where a robot is trying to navigate to a goal location, and
macro-actions are to go to the end of a hallway and turn left or right. Assuming the robot starts
far from the goal location, a series of macro-actions will most likely be needed in order to reach
the goal, and therefore it will be important during forward search to consider a search horizon of
multiple macro-actions.
However, when constructing the forward search tree, it is not immediately clear how to evaluate
each branch in the three at the end of each macro-action. We have a closed form expression for
the posterior distribution over beliefs at the end of the macro-action. This posterior set represents
the distribution of beliefs possible given all possible observation sequences that could be received
during the macro-action’s execution. However, different individual posterior beliefs, or different
subsets of the posterior belief distribution, may be associated with different best subsequent macroactions in the tree, because different individual posterior beliefs are implicitly the result of receiving
a different set of observations during the macro-action execution and may reveal important information about the environment that result in different best subsequent macro-actions. Though the
motivation behind macro-actions is that it is reasonable to act in an open-loop fashion for a limited
537

H E , B RUNSKILL , & ROY

Algorithm 1 Forward Search with Macro-Actions
Require: Initial belief b0 , Discount factor γ, Macro-action search depth H̃, Sampling number Ns
1: t ← 0
2: loop
3:
Compute set of macro-actions Ã
4:
for each macro-action ãi ∈ Ã do
5:
Q(bt , ãi ) = E XPAND(ãi , bt , γ, H̃, Ns ) {See Algorithm 2}
6:
end for
7:
Execute first action a1 of ã = argmaxã Q(bt , ã)
8:
Obtain new observation zt and reward rt
9:
bt+1 = τ (bt , at , zt )
10:
t←t+1
11: end loop

time period, the received observation sequence does provide information about the underlying belief
that is likely to be useful for selecting future macro-actions.
Since we do not know in advance which subsets of posterior beliefs are associated with the same
best subsequent macro-action, we instead sample from the posterior belief distribution, and then
evaluate future macro-actions for each of these samples (see Figure 4 for an illustration). Sampling a
posterior belief is equivalent to implicitly sampling an observation sequence for the planned macroaction, without having to actually perform belief updates along the action-observation trajectory.
Note that the potential space of observation sequences grows exponentially with the macro-action
length. As the posterior distribution over beliefs is a Gaussian, its properties can be completely
described by its mean and covariance, which means that the posterior distribution over beliefs will
typically be of much lower dimension than the observation sequence space. Experimentally we will
see much better performance sampling from the posterior belief distribution than from sampling
from the space of observation sequences. The sampled beliefs essentially form a non-parametric,
particle estimate of the posterior distribution of beliefs that is present after taking the macro-action.
As the number of samples Ns goes to infinity, the sampled distribution will become an arbitrarily
good approximation of the full posterior distribution of beliefs. As the covariance is a Dirac delta
distribution, sampling is needed only for the posterior mean distribution, generating posterior belief
samples by associating each posterior mean sample with the posterior covariance Σt+T .
3.4 The PBD Algorithm Summary
We are now ready to present our PBD forward search algorithm (Algorithm 1). Given the current
belief, we select an action by constructing a macro-action forward search tree. Placing the current
belief at the root, we expand each possible macro-action (Algorithm 2), computing the expected
reward and the resulting posterior set of beliefs. We then sample a fixed number of posterior beliefs.
Forward search then proceeds from each of these sampled beliefs. We repeat this process out to a
fixed horizon depth and then select an action for the current belief by estimating its value, starting
from the search leaf nodes. After executing this action, an observation is received, and the new
belief state is computed. The whole process then repeats for this new belief state. Note that PBD
will only ever select actions that are the first action of a macro-action. If all primitive actions are to
538

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

Algorithm 2 E XPAND – Expand Macro-actions via PBD
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

Input: Macro-action ã, Belief state bt , Discount factor γ, Macro-action search depth H̃,
No. posterior belief samples per macro-action Ns
if H̃ = 0 then
return 0
else {Expand Macro-action ã={a1 , . . . , aL }}
Rã = 0
bdist = bt
for j = 1 to L do
Rã = Rã + γ ∗ r(bdist , aj )
Update the posterior distribution of beliefs bdist
end for
for i = 1 to Ns do
Sample posterior mean ni according to N (mt+T , Σµt+T )
bi ← N (ni , Σt+T )
Generate next set of macro-actions Ãnext
∈ Ãnext do
for ãnext
i
next
Q(bi , ãi ) = E XPAND(ãnext
,bi ,γ,H̃ − 1,Ns )
i
end for
V = Rã + N1s γ L maxãnext
Q(bi , ãnext
))
i
i
end for
return V
end if

be considered, the number of macro-actions that are evaluated for the root belief at every timestep
must be at least the same as the size of the primitive action space, and each primitive action must be
the first action of at least one macro-action.

4. Approximate Computation of Posterior Belief Distributions
The PBD algorithm described so far assumes that the transition and observation functions are linear functions of the state with Gaussian noise. When these functions are non-linear, the traditional
Kalman filter model no longer provides an exact belief update, and for the PBD algorithm, the distribution of posterior beliefs cannot be calculated exactly. In this section we briefly describe an
extension to the PBD algorithm to handle a wider class of observation models, namely parametric models that are members of the exponential family of distributions (Barndorff-Nielsen, 1979).
For non-linear transition models, there exist techniques such as the extended Kalman filter to approximate the posterior with a Gaussian; however, we do not formally consider incorporating such
techniques into our PBD algorithm here.
We choose to consider exponential family observation models since this family includes a wide
array of distributions, such as Gaussian, Bernoulli, and Poisson distributions, and has certain appealing mathematical properties. In particular, we leverage work by West, Harrison and Migon (1985)
who constructed linear-Gaussian models that approximate the non-Gaussian exponential family observation model in the neighborhood of the conditional mode, st |zt . They then used the approximate
539

H E , B RUNSKILL , & ROY

linear-Gaussian observation mode in a traditional Kalman filter, to maintain a closed-form Gaussian
representation of the posterior belief, creating an exponential family Kalman Filter (efKF). For completeness we include West et al.’s derivation of the filter in Appendix A, and we present the main
equations here.
Constructing the approximate linear-Gaussian observation model requires computation of the
first two moments of the distribution and the linearization around the mean estimate at every time
step. An exponential family observation model can be represented as follows,
p(zt |θt ) = exp(ztT θt − βt (θt ) + κt (zt )),

θt = W (st )

(49)

where st is the hidden state of the system, θt and βt (θt ) are the canonical parameter and normalization factor of the distribution, and W (.) maps the states to canonical parameter values. W (.) is
also known as the canonical link function, and depends on the particular member of the exponential
family.
The first two moments of the distribution (West et al., 1985) are
∂βt (θt ) 
E(zt |θt ) = β̇t =

∂θt θt =W (µt )

∂ 2 βt (θt ) 
V ar(zt |θt ) = β̈t =

∂θt ∂θtT θt =W (µt )

(50)

where β̇t and β̈t are the derivatives of the exponential family distribution’s normalization factor,
both linearized about θt = W (µt ).
Given an action-observation sequence, the posterior mean of the agent’s belief in the efKF can
then be updated according to
µt = Aµt−1 + Bat
Σt = AΣt−1 AT + P

µt = µt + K̃t (z̃t − W (µt )),

(51)

Σt =

(52)

−1
(Σt

+ YtT β̈t Yt )−1 ,

where K̃t = Σt Yt (Yt Σt YtT + β̈t−1 )−1 is the efKF Kalman gain, and z̃t = θt − β̈t−1 · (β̇t − zt )
is the projection of the observation onto the parameter space of the exponential family observation
t
model. Yt = ∂θ
∂st st =µt is the gradient of the exponential family distribution’s canonical parameter,
linearized about µt .
We can now incorporate these results to compute a modified form for the posterior belief mean
and covariance distributions, which were represented by Equations 8 and 22 when the observation
model was linear Gaussian. Now, for exponential family observation models, the posterior belief
covariance comes from Equation 52. The expression for the distribution of the posterior means can
be modified based on the efKF equations:

µt+T ∼ N (f (µt−1 , At:t+T , Bt:t+T , at:t+T ),

t+T
X

Σi YiT K̃iT ).

(53)

i=t

It is worth noting that in contrast to our prior expressions for the posterior belief distribution
(Equations 8 and 22), which are exact and completely independent of the received observations,
Equations 52 and 53 are no longer independent of the observations obtained because the observation model parameters are linearized about the prior mean µt . Hence while the parameters are
independent of the observation that will be obtained for a macro-action sequence of length 1, for a
540

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

longer macro-action, the observation model parameters depend on the prior observations obtained.
We approximate this update by linearizing about the mean of the prior mean distribution mt at each
step along the action sequence, rather than the true prior belief mean µt . We will shortly see that we
still obtain good experimental results using this approximation.
An alternate popular approach for non-Gaussian systems is to use a particle filter to represent the
system state. However, in high dimensional, continuous environments similar to the ones considered
in this paper, particle filters often suffer from particle depletion, or require a very large number of
particles to accurately capture the posterior. The costs of belief updating and expected reward
calculations scale with the number of particles. In contrast, our approximate PBD computation has
the same computational complexity as our exact PBD computation, which we will demonstrate in
later sections to scale polynomially with the number of state dimensions.
This approximate method for computing the posterior distribution over beliefs can be used as a
substitute for exactly calculating the posterior distribution over beliefs in the PBD algorithm.

5. Analysis
Here we provide a formal analysis of the accuracy and computational complexity of our PBD algorithm. Throughout this section we assume belief states can be represented exactly as Gaussian
distributions: in other words, we assume a linear-Gaussian system. In the following sections we
will demonstrate experimentally that the PBD algorithm is useful in a wider variety of problems
using an EKF or the efKF described in Section 4, but incorporating the error of these approximate
filtering techniques into an analysis of the algorithm is a topic for future research.
5.1 Performance
PBD selects actions by performing a limited-horizon forward search using a restricted policy space
induced by the macro-actions. However, during execution, only the first step of the macro-action
is taken. After an observation is received, the belief state is updated, and then planning is repeated
from the resulting belief. By only taking the first primitive action, the system may take sequences
of actions that do not correspond to any of the known macro-actions, effectively expanding the
considered policy space. As a result, the performance will be at least as good as actually executing
the entire macro-action. However, it would be useful to determine if any claims can be made about
the belief-action values calculated as part of the PBD algorithm. Obviously, the received rewards
of the executed policy will always be less than or equal to the optimal policy’s rewards, since the
policy space considered during planning is smaller than the full policy space. However, the values
calculated by the PBD algorithm are only approximate values due to the approximations (such as
sampling a subset of the posterior beliefs) made during the computation process. We now prove that
for linear-Gaussian systems, the values computed by PBD, minus an additional epsilon term due to
the approximations incurred by sampling a subset of the posterior beliefs after each macro-action,
are probabilistically guaranteed to be a lower bound on the true optimal values. For the purpose of
this analysis we will assume that all rewards are scaled to lie between 0 and 1. M is the maximum
number of macro-actions.
Theorem 5.1 Given a linear-Gaussian system, an initial belief b, and any δ > 0, and for any
reward model which is either a weighted sum of Gaussians, or a polynomial function, the following
541

H E , B RUNSKILL , & ROY

lower bound on the optimal value of b holds
VP BD (b) − ǫH̃ ≤ V ∗ (b)
q

2

H̃

Vmax
(M Ns )
with probability at least 1 − δ, where ǫH̃ =
)), Vmax is a
max +
Ns log(
δ
10
bound on the maximum value , and VP BD (b) is the best value computed for b by the PBD planning
algorithm.

γH V

1
1−γ (

Proof First recall in the PBD algorithm that after each macro-action, a subset of the possible posterior beliefs are sampled from the posterior belief distribution, before the tree is further expanded.
Note that this is equivalent to implicitly sampling a subset of the observation trajectories that might
have been received during that macro-action: each sampled posterior belief corresponds to the belief
that would result by following the macro-action and receiving a particular sequence of observations.
Consider an alternate variant of a macro-action forward search in which observation sequences are
exhaustively enumerated11 : that is, for each macro-action of length L, all |Z|L possible observation
sequences are expanded. In this case, the forward search tree constructed is precisely a subset of
a full POMDP forward search tree, since the macro-actions mean that only a subset of actions are
expanded. Therefore, the computed values of this alternate algorithm are directly a lower bound
on the optimal finite-horizon value, since the policy space considered is a strict subset of the full
optimal finite-horizon policy space.
However, for computational reasons, at each macro-action tree node, only a subset of observation sequences are sampled, and the results are averaged across the observation sequences. As
observation sequences that happen to lead to higher rewards may be, by chance, disproportionately
sampled, the resulting VP BD value could be an upper bound to the true optimal value. However, we
can now probabilistically bound this error induced by observation sampling,
Prior work by Kearns, Mansour and Ng (2002) proved bounds on the MDP state values computed using a sampled-states forward search given certain constraints on the number of samples, and
the horizon of the forward search. McAllester and Singh (1999) extended these ideas to POMDPs,
showing that similar bounds on the calculated values of a POMDP belief state could be computed
if a sufficient number of observations were sampled, and forward search was computed out to a
sufficiently large horizon. These results can be applied with little modification to our PBD algorithm. Essentially we can consider a new meta-POMDP in which the only available actions are
macro-actions, and observations are sequences of primitive observations. Since we can compute
the expected reward of macro-actions analytically (due to the assumed form of the reward model),
the only errors in evaluating the root belief node values for a macro-action policy come from limited sampling of the observation trajectories, and performing a finite horizon lookahead. The prior
results of McAllester and Singh directly apply to our meta-POMDP, and therefore, the values computed by PBD.
To obtain our final result, we depart slightly from the presentation of Kearns, Mansour and Ng
who sought to compute the number of samples required, and the horizon required, to ensure the
resulting root state-action values were within a specified ǫ bound of the true value. In contrast, we
seek to compute the resulting error from an input number of samples Ns and fixed horizon H̃.
10. The maximum value can be trivially upper bounded by maxs,a r(s, a)/(1 − γ).
11. This is possible only if there are a finite number of observations.

542

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

In the proof of Kearns, Mansour and Ng, they show that the error between the calculated H̃horizon state-action value QH̃ (b, a) and the true infinite-horizon policy value Q(b, a) is
|QH̃ (b, a) − Q(b, a)| ≤ γ H̃ Vmax +

ǫ
1−γ

(54)

with probability at least 1 − δ if
2
δ ≥ (M Ns )H̃ exp(−ǫ2 Ns /Vmax
).

We can solve Equation 55 for ǫ, to yield
s
2
Vmax
(M Ns )H̃ 
.
log
ǫ≤
Ns
δ

(55)

(56)

Substituting Equation 56 into Equation 54 and re-arranging yields the desired result.
If the reward of a macro-action cannot be analytically computed, we can approximate its value
by sampling Nr samples at each primitive action along the length-L macro-action. For an input
δ ′ we can compute a probabilistic bound on the resulting error of the approximate value at each
primitive action using Chernoff’s bound. Using the union bound, the probability that the true error
will exceed this threshold at any primitive action along the macro-action is no more than Lδ ′ , and
the resulting error is at most the sum of the error at each primitive action. This error (and probability
of error) can be easily incorporated to extend Theorem 1 to the case of generic reward models.
Note that Theorem 1 only states that with high probability that VP BD − ǫH̃ is a lower bound
on the optimal value: it does not provide a tight bound on how close the computed VP BD is to
the optimal value. To state this in an alternate way, ǫH̃ provides a bound on the error introduced
by sampling observation sequences, but PBD still is designed to only search over a limited policy
space, that defined by the macro-actions chosen and used in the forward search. Therefore in general
the computed values, even when a large number of observation sequences are sampled, may be
substantially less than the value under the optimal policy.
5.2 Computational Complexity
One of the central contributions of our work is providing an efficient macro-action forward search
algorithm that can scale to long horizons and large problems. We now analyze the computational
complexity of our approach. The computational cost will be a function of two operations: computing the posterior distribution over beliefs, and computing the expected reward of a distribution over
beliefs. As we will shortly see, the computational complexity of these operations is a polynomial
function of the state space dimension.12 This low order relationship is possible due to the particular parametric representation employed for the posterior distribution over beliefs: representing the
posterior distribution over beliefs as a Gaussian requires a number of parameters that scales only
quadratically with the number of state dimensions.13 PBD is therefore able to scale to large domains. Our computational complexity results are summarized in Table 1. Throughout this analysis
12. If there are multiple independent state variables, or factors, the complexity increases linearly with the number of
independent factors.
13. To represent a Gaussian in X dimensions requires an X-dimensional vector to specify the mean, and O(X 2 ) parameters to specify the covariance.

543

H E , B RUNSKILL , & ROY

we presume that the macro-actions themselves were selected or computed in advance; in general,
the cost of computing domain-relevant macro-actions will depend on the particular domain, and
we do not here analyze the possible additional computational cost incurred during macro-action
construction.
5.2.1 C OMPLEXITY

OF

G AUSSIAN B ELIEF U PDATING

FOR A

L ENGTH L M ACRO - ACTION

The computation for the posterior distribution over beliefs resulting from a macro-action was presented in Equation 53, and consists of a set of matrix multiplications and inversions. Matrix multiplication is an O(D2 ) computation, where D is the state space dimension. Matrix inversion can
be done in O(D3 ) time. Therefore the computational cost of performing a single update of the posterior over belief states is an O(D3 ) operation. This update must be performed for each primitive
action in a length-L macro-action ã, resulting in a computational cost of
O(LD3 )

(57)

for a single macro-action.
In Section 4 we presented a set of equations (Equations 50- 53) that we use to approximately
compute the posterior distribution over beliefs when the observation model is not Gaussian, but is
an exponential family. These equations again consist of a set of matrix multiplications, and the cost
of a single update, and cost of updating over a length-L macro-action will again be O(D3 ) and
O(LD3 ), respectively.14
5.2.2 C OMPLEXITY OF A NALYTICALLY C OMPUTING
L M ACRO - ACTION

THE

E XPECTED R EWARD

OF A

L ENGTH

The second component of the computational cost comes when we evaluate the expected reward of
a macro-action. If the reward is a weighted sum of Nr Gaussians, as specified by Equation 43,
this operation involves evaluating the value of Nr L Gaussians at particular fixed points. Evaluating
a D-dimensional Gaussian at a single point is an O(D3 ) operation, due to the inverse covariance
that must be computed. The cost for performing this operation Nr L times is simply O(Nr LD3 ).
Therefore the total cost for evaluating the expected reward of a macro-action when the reward model
is a weighted sum of Nr Gaussians is:
O(LD3 (Nr + 1)).

(58)

If instead the reward model is a Nr -th degree polynomial function of the state, then the expected
reward calculation consists of the cost of calculating the Nr -moments of a D-dimensional Gaussian
distribution (Equation 48). Assume without loss of generality that we are computing the Nr -th
central moment of a D-dimensional Gaussian: a non-central moment can always be converted into
a central moment by adding and subtracting a mean term. Let the Nr -th central moment denote
moments of the form E[(s1 − E[s1 ])2 (s2 − E[s2 ]) . . . (sD − E[sD ])] or E[(s2 − E[s2 ])Nr ], and
σij denote the ij-th entry of the covariance matrix. From the work by Triantafyllopoulos (2003) we
know that if Nr is odd, the central Nr -th moments are zero, and if Nr is even (Nr = 2k) any Nr -th
14. The actual computational cost will be higher for the efKF filter since additional operations must be performed to link
the observation and the parameter space, but these operations will similarly be cubic or lower functions of the state
space dimension.

544

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

central moments can be decomposed into a sum over products of k covariance terms. For example,
for a four-dimensional Gaussian, one of the fourth central moments (k = 2, 4 = 2k) is
X
σij σkl (59)
E[(s1 − µ1 )(s2 − µ2 )(s3 − µ3 )(s4 − µ4 )] = σ12 σ34 + σ14 σ23 + σ13 σ24 =
1,2,3,4

where the sum is taken over all permutations of product pairs (in this case, 12/34, 14/23, 13/24).
For any 2k-th central moment,
X
E[(si1 − E[si1 ])(sj1 − E[sj1 ]) . . . (sik − E[sik ])(sjk − E[sjk ])] =
σi1 j1 σi2 j2 . . . σik jk (60)

where the sum is again taken over all permutations of product pairs. This sum yields (Nr −
1)!/(2k−1 (k − 1)!) terms which consist of covariance elements to the power of at most k. For
a particular central moment, this cost is independent of the dimension of the state space. Therefore
the cost is dominated by the number of terms, which grows at slightly less than O(Nr !). There will
also be an additional cost if the original polynomial was not a central moment calculation, which
will involve at most Nr D-dimensional matrix multiplications, yielding a cost of O(Nr D2 ). In
summary, the cost of computing the expected reward when the reward is a polynomial function will
be
O(L(D3 + Nr ! + Nr D2 )).
5.2.3 C OMPLEXITY

OF

(61)

C ONDITIONAL M ACRO - ACTION P LANNING (PBD)

Sampling beliefs from the posterior distribution over beliefs requires sampling from a multivariate
Gaussian over the distribution of belief means, which we accomplish by computing the Cholesky
decomposition of the covariance matrix, Σ = AAT , an O(D3 ) operation. Each belief mean is
generated by first constructing a D-dimensional vector q, consisting of D independent samples
from a standard (scalar) normal distribution. A sample from the desired multivariate Gaussian
N (s|µ, Σ) is simply µ + Aq. Sampling Ns times involves the one-time cost of computing the
Cholesky decomposition plus the matrix-vector multiplication for each sample, yielding a cost of
O(D3 + Ns D2 ).

(62)

This procedure is performed at every branch point in the forward search tree (in other words, at all
macro-action nodes except those at the tree leaves). For concreteness, consider a horizon of two
macro-actions (H̃ = 2). After expanding out each of the |Ã| macro-actions, we will sample Ns
beliefs. From each resulting belief state, we will again expand each of the |Ã| macro-actions: refer
back to Figure 4 for an illustration. The computational complexity is now the sum of the cost at
horizon one and two:
O(|Ã|(LD3 Nr + Ns D2 + D3 ) + |Ã|2 Ns LD3 Nr ) = O(|Ã|(Ns D2 + D3 ) + |Ã|2 Ns LD3 C), (63)
where the second expression is derived by considering only the higher order terms. In general, the
computational complexity of selecting an action using PBD when considering a future horizon of
H̃ macro-actions is
O(|Ã|H̃−1 NsH̃−2 (Ns D2 + D3 ) + |Ã|H̃ NsH̃−1 LD3 C).
545

(64)

H E , B RUNSKILL , & ROY

Algorithm
PBD with Analytic Expected Reward
PBD with Arbitrary Reward Model

Computational Complexiy
O(|Ã|H̃−1 NsH̃−2 (Ns D2 + D3 ) + |Ã|H̃ NsH̃−1 LD3 C) (Eqn.
O(|Ã|H̃ NsH̃ LD3 + |Ã|H̃ NsH̃ LD2 ) (Eqn. 66)

Table 1: Computational complexity of selecting an action using PBD algorithm and closely related
alternatives. D is the number of state dimensions, H̃ is the macro-action forward search
horizon, and Ns is the number of sampled beliefs. Slightly abusing notation, we also use
Ns to represent the number of sampled states, in the case of arbitrary reward models.

5.2.4 C OMPLEXITY

OF

PBD

WITH

A RBITRARY R EWARD M ODELS

For arbitrary reward models it will not be possible to analytically compute the expected reward.
Instead the expected reward for each primitive action a within the macro-action ã can be approximated by sampling D-dimensional states and estimating the expected reward by averaging the
reward of each sampled state.15 The cost of sampling Ns states from a multivariate Gaussian is
an O(D3 + Ns D2 ) operation (from Equation 62). Assuming that calculating the reward for each
sample takes time linear in the state dimension, then sampling rewards adds an additional
O(D3 + Ns D2 D) = O(D3 (Ns + 1))

(65)

cost to each primitive action within a macro-action, yielding a total complexity of PBD planning
with reward sampling of:
O(|Ã|H̃ NsH̃ LD3 + |Ã|H̃ NsH̃ LD2 ).

(66)

6. Experimental Results
In this section we test our algorithm on planning under uncertainty problems. The PBD algorithm
assumes that the transition models of the problem domains can be approximated as linear Gaussians.
Our results on problems inspired by two different research communities, scientific exploration from
the POMDP literature (Smith & Simmons, 2005) and target monitoring from the sensor resource
management domain, suggest that numerous domains do satisfy this assumption. More generally,
using a linear Gaussian dynamics models is a common approximation in the controls community,
and has been used to approximate even very complex dynamics such as the physiological changes
involved in glucose control for diabetics (Patek, Breton, Chen, Solomon, & Kovatchev, 2007).
Despite the different origins and state space representations of the two problems that we will
shortly present results for, they both involve reasoning multiple steps into the future in order to
make good decisions in a very large domain. Our PBD algorithm outperforms existing approaches
in both settings. We also demonstrate our algorithm in a target monitoring problem on an actual
15. Note that if the rewards are bounded, for a given ǫ and δ, sampling a sufficient number of samples Ns = f (ǫ, δ),
guarantees the estimate of the expected reward of a primitive action is is ǫ-close to the true expected value, with
probability at least 1 − δ. The proof of this is a simple application of Hoeffding’s inequality (1963). If Ns is set
such that the estimated reward of each primitive action is Lǫ close to the true expected primitive action reward with
probability at least 1− δǫ , then the triangle inequality and union bound guarantee that the expected reward of the entire
length-L macro-action is ǫ-close to the true expected reward for the macro-action with probability at least 1 − δ.

546

64)

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

helicopter platform, underscoring the applicability of our algorithm to real-world domains. In all
results the macro-action search horizon H̃ was chosen empirically given computational constraints,
as is common in forward search approaches. We explicitly explore the performance changes as the
search horizon is varied in Table 3. We did not use a domain-specific estimate of the future node
value of search tree leaf nodes: in some domains it may be easier to specify macro-actions than a
heuristic value function, and a side benefit of PBD is to be able to efficiently search to sufficient
depths such that a heuristic is not required.
6.1 Generic Baselines
In both problems we compare the PBD algorithm to state-of-the-art approaches from the relevant
research community — POMDP planners and sensor resource management algorithms for the scientific exploration and target monitoring problems respectively.
To fully examine the impact of analytically computing the posterior distribution over beliefs,
we also constructed a variety of algorithms that do not currently exist in the literature. These algorithms are given access to the same hand-coded macro-actions as those used by the PBD algorithm.
We first constructed comparison algorithms which use a macro-action forward search but sample
observation trajectories rather than working with a posterior distribution over beliefs. Sampling
observation sequences produces a particle approximation of the resulting distribution over beliefs,
thereby providing a baseline algorithm that does not use an analytic representation of the posterior
belief distribution. These algorithms are referred to as the macro-action discrete (MAD) algorithm
when the underlying state space is discrete, and the macro-action continuous (MAC) algorithm
when the state space is continuous.
We also implemented an offline point-based POMDP solver that was given access to the macroactions used by the forward search algorithms.16 Specifically, we modified the state-of-the-art
POMDP planner SARSOP (Kurniawati et al., 2008) algorithm from the Approximate POMDP Planning (APPL) Toolkit17 and incorporated macro-actions to guide the sampling of belief points that
are used for the point-based value backups. Instead of the SARSOP algorithm using performance
bounds to guide the sampling of the point-based beliefs, the modified SARSOP algorithm uses a
macro-action and a sampled, same-length observation sequence to generate additional point-based
belief samples. This implementation is also a modified version of the MiGS (Kurniawati et al.,
2009) by the same authors. However, due to the offline, point-based nature of this modified algorithm, we were only able to evaluate the algorithm on two of the five problem domains used in this
paper.
Finally, we considered an experimental comparison to an open-loop version of PBD, in which no
conditioning on the received observations is ever performed; however, initial experiments suggested
that this variant performed very poorly in our domains of interest, and so we did not explore it
further.
6.2 Rocksample
The scientific exploration ROCKSAMPLE problem is a benchmark POMDP problem proposed by
Smith and Simmons (2005), and subsequently extended to the FieldVisionRockSample (FVRS)
16. For a formal discussion of the differences between the offline point-based and online forward search POMDP algorithms, we refer the reader to the survey paper by Ross et al. (2008a).
17. Approximate POMDP Planning Toolkit. http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/

547

H E , B RUNSKILL , & ROY

(a) ISRS(8,5)

(b) SARSOP policy

(c) PBD policy

Figure 5: Information Search Rocksample (ISRS) problem. (a) Initial (hidden) problem state. An
agent (pink square) explores and samples rocks (circles) in the world. White circles correspond to rocks with positive value, black otherwise. Yellow squares indicate locations
of the rock information beacons. The blue sidebar is the exit region. Red lines indicate
paths taken by an agent executing the (b) SARSOP and (c) PBD policies. We see that
the SARSOP policy only explores rocks and not the beacons; it cannot search far enough
ahead to model the value of the beacons. In contrast, the PBD plan visits the beacons and
then heads directly for the high-value rocks.

problem by Ross and Chaib-draa (2007). Initial experiments in these domains revealed that searching only to a shallow depth was sufficient to obtain good policies. As our interest is in domains
which require long-horizon lookahead, we created a new variant of the ROCKSAMPLE problem
called the Information Search Rocksample (ISRS) problem, shown in Figure 5(a). In ISRS an agent
explores and samples k rocks in a n × n grid world. The positions of the agent (pink square) and
the rocks (circles) are fully observable, but the value of each rock (good or bad) is unknown to the
agent. At every time step, the agent receives a binary observation of the value of each rock. The
accuracy of this observation depends not on the agent’s proximity to the rocks themselves but on
the agent’s proximity to rock information beacons (yellow squares), each of which correspond to
a particular rock (for example, information beacons could be mountain tops that offer a particularly good view of a far off geologic formation). A key characteristic of ISRS that is not present in
ROCKSAMPLE or FVRS is that the rock information beacons are not at the same locations as the
rock themselves. Unlike previous ROCKSAMPLE formulations, information gathering and reward
exploitation require different actions in ISRS.
The agent gets a fixed positive reward for collecting a good rock (white circle), a negative reward
for collecting a bad rock (black circle), and a smaller positive reward for exiting the problem (the
blue sidebar on the right). A discount factor γ = 0.99 encourages the agent to collect rewards
sooner. All other actions have zero rewards.
The observation model is a Bernoulli distribution with the noise of the distribution scaled with
the distance to the beacon, such that:
p(zi,t |si , rt , RBi ) =

(

0.5 + (si − 0.5)2
0.5 − (si − 0.5)2
548

−krt −RBi k2
D0
−krt −RBi k2
D0

zi,t = 1
zi,t = 0

(67)

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

where zi,t
si
rt
RBi
D0

is a binary {0 or 1} observation for the value of the rock i at time t,
is the true value {0 or 1} of the rock,
is the agent’s position at time t,
is the location of the information beacon associated with rock i,
is a tuning parameter that controls how quickly the accuracy of the observations
decrease with greater distance between the agent and the beacon.

For example, at an information beacon, the agent, with absolute certainty, receives an observation
that matches the true value of the corresponding rock, whereas when the distance between the agent
and the beacon is infinite, the agent receives an “accurate” observation with 0.5 probability.
All variants of the ROCKSAMPLE problem, including our new ISRS problem, are formulated
with discrete state, action and observation sets. To allow the use of our PBD and MAC algorithms,
we approximate the agent’s belief of each rock’s value as a Gaussian distribution over the [0,1]
state space, and take advantage of the efKF presented in Section 4 to represent the ROCKSAMPLE
problem’s Bernoulli observation model (Equation 67: see Appendix B for details).
Each macro-action is a finite, open-loop sequence of primitive actions. For the ROCKSAMPLE
problem, there are five primitive actions: single steps in the four cardinal directions and the rock
sampling action. Recall that the agent’s position is fully observable and its actions are deterministic.
Using domain knowledge, the macro-actions considered from a particular belief state are macroactions that, given the agent’s current position, consist of a sequence of actions that enables the
agent to move to each rock, each information beacon, or to the nearest exit. This results in 2k + 1
macro-actions being considered for forward search at every belief node. As the agent operates in
a grid world, there may be multiple action sequences with the same, shortest distance between two
grid squares: the macro-action considered is the one where the agent would move as diagonally as
possible, so as to replicate the agent’s shortest path movement in a continuous map. In addition,
if the agent is currently on a rock (which is fully observable), additional macro-actions where the
agent first collects the rock before executing one of the 2k + 1 default macro-actions are considered,
resulting in twice as many macro-actions. The set of macro-actions therefore varies with every
belief node.18 For an ISRS problem with 5 rocks in a 8 × 8 grid world, the average macro-action
length was 4.76, with a minimum and maximum macro-action length of 1 and 12 respectively.
As the ROCKSAMPLE family of problems originates from the POMDP literature, we compared
our macro-action algorithms to existing state-of-the-art POMDP solvers: the fast upper-bound of
QMDP (Littman, Cassandra, & Kaelbling, 1995), the point-based offline value-iteration techniques
HSVI2 (Smith & Simmons, 2005) and SARSOP (Kurniawati et al., 2008), as well as RTBSS (Paquet, Chaib-draa, & Ross, 2006), an online, factored, forward search algorithm. We also evaluated
a modified version of the SARSOP algorithm that was given access to the same macro-actions used
by the forward search macro-action algorithms. Since all approaches, including our own, are approximations, we also include as an upper bound the value of the fully observable problem.
Table 2 compares the performance of the different algorithms in the ISRS problem. Each algorithm was tested on 10 different initial conditions (which rocks were high valued and which were
low valued), and each scenario was tested 20 times. The HSVI2 and SARSOP algorithms were executed offline for a range of durations,19 while the forward search algorithms were allowed to search
18. However, if two belief nodes have the same agent position, their macro-actions will be identical.
19. The offline execution durations for both HSVI2 and SARSOP were chosen empirically. HSVI2 was able to search
for solutions to the ISRS[8,5] problem for 1,000s offline before running out of memory. It was found that the values
computed by SARSOP remained constant after 25,000s.

549

H E , B RUNSKILL , & ROY

Avg rewards
QMDP
HSVI2
SARSOP
SARSOP(macros)
RTBSS (d5, s10)
RTBSS (d7, s2)
RTBSS (d10, s1)
MAC (d3,s50)
MAD (d3,s50)
PBD (d3,s50)
Fully observable

1.11 ± 0.15
6.78 ± 0.62
8.46 ± 0.70
18.78 ± 1.59
9.78 ± 0.49
12.41 ± 0.46
15.39 ± 0.45
13.68 ± 0.65
15.88 ± 0.54
14.76 ± 0.57
21.37

ISRS[8,5]
Online
time (s)
0.0001
0.051
0.070
0.015
17.64
3.28
7.0357
15.39
4.81
1.26
N.A.

Offline
time (s)
3.03
1000
25000
1000
0
0
0
0
0
0
N.A.

Table 2: ISRS results. HSVI2 and SARSOP were executed offline for a range of durations. For
the forward search algorithms, the numbers in brackets represent the search depth (d) and
number of posterior beliefs obtained (s) at the end of each action/macro-action. Online
time indicates the average time taken by the planner to return a decision at every time step.
Standard error values are shown.

out to pre-defined depths. Here, depth refers to the primitive action depth in the RTBSS algorithm,
and the macro-action depth in the macro-action algorithms (MAC, PBD and MAD). In addition, a
pre-defined number of samples were used to obtain posterior beliefs after every action/macro-action.
We abuse notation here slightly by using samples to refer to observations in the RTBSS algorithm,
observation sequences in the MAD and MAC algorithms, and to samples from the posterior belief
distribution in the PBD algorithm.
We also attempted to allow the RTBSS algorithm to search to the same primitive action search
depth as the macro-action algorithms do on average, i.e. 4.76 × 3 ≈ 14, by reducing the number of
observations that are sampled per action. We found that even if only 1 observation was sampled per
action, RTBSS could only achieve a search depth of 10 in reasonable computation time.
The macro-action algorithms do significantly better than most of the other benchmark solvers.
Figure 5(b) and 5(c) compare the policies generated by the SARSOP algorithm and the PBD algorithm in the ISRS problem. Both SARSOP and HSVI2 explore parts of the belief space guided by
an upper bound on belief-action values. A long lookahead is required to realize that visiting beacons
and then rocks has a higher value that visiting rocks, so many iterations and therefore substantial
computation time is required for SARSOP and HSVI2 to sample the beliefs that will lead to them
computing a higher-value policy. In the considerable offline computation time provided, both SARSOP and HSVI2 did not discover that it is valuable for the agent to make a detour to the information
beacons before approaching the rocks. Instead, they directly approach the rocks and make decisions
based on the noisy observations that are obtained due to the large distance from the information
beacons.
The RTBSS algorithm does reasonably well when it is able to search deep enough, once again
emphasizing the need for planning under uncertainty algorithms to search far into the future in order
to perform well. Nevertheless, when the same amount of online planning time is available, the MAD
algorithm still outperforms the RTBSS algorithm. Macro-actions allow the algorithms to uncover
550

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

MAC
MAD
PBD

Depth 1, Samples 50
Avg
Online
rewards
time(s)
4.61±0
0
4.61±0
0
4.61±0
0

Depth 2, Samples 50
Avg
Online
rewards
time(s)
9.63±0.77
0.022
7.51±0.84
0.0083
7.73±0.77
0.002

Depth 3, Samples 50
Avg
Online
rewards
time(s)
13.68±0.65 15.39
15.88±0.54 4.81
14.76±0.57 1.26

Depth 4, Samples 20
Avg
Online
rewards
time(s)
15.07±1.62 660.50
17.43±0.78 225.74
15.82±0.77 75.06

Table 3: Performance of macro-action algorithms with different macro-action depth on ISRS. At
depth 4, a smaller number of posterior beliefs were sampled for computational reasons.

the potential value of moving to an information beacon without incurring the computational cost of
primitive-action forward search; this allows our macro-action forward search approaches to perform
better than prior primitive-action approaches. Figure 5(c) shows that a PBD agent’s policy involves
visiting some of the information beacons to gather information about which of the rocks are good
(white circles), before traveling to those rocks to sample them. In this domain, MAD does better
than the PBD algorithm since the problem specification is made up of discrete states, whereas the
parametric approaches must approximate the world models during planning. In addition, the fullyfactored nature of the problem domain, where the state of each rock value is independent, keeps the
computational requirements of the MAD algorithm relatively small.
Similarly, when the SARSOP algorithm was modified to incorporate the hand-coded macroactions, this offline, point-based algorithm performed much better than existing offline approaches,
including the SARSOP algorithm without access to macro-actions. This result re-emphasizes that
well-designed macro-actions can be very valuable in generating good policies in partially observable
domains. However, not all problem domains, especially those with large, factored domains that are
of interest in this paper, can be represented and solved in an offline manner, and we shall shortly see
the benefit of PBD for such settings.
We also performed additional analysis on the three macro-action forward search algorithms.
Table 3 compares the different rewards obtained by the macro-action algorithms for different macroaction depths, as well as the time taken by the planner to return a decision at every time step. The
sharp performance jump that occurs when the macro-action search depth is increased from 2 to 3
emphasizes the need to search to a longer horizon in the ISRS problem before a good policy can
be generated. However, the computational cost of the algorithms also increases exponentially with
the macro-action search depth. This table also illustrates the small loss in performance induced by
approximating the discrete problem with the continuous representation of either MAC or PBD, and
the substantial increase in computational speed using PBD.
Next we examine the relative performance and computational cost of PBD, MAC and MAD, as
the number of samples changes (Table 4) up to a search depth of 3. Recall that the PBD algorithm
samples from the posterior belief at each node in the search tree, and evaluates the expected future
reward of subsequent macro-actions for each sample. Different regions of the posterior belief space
may plan to use different subsequent macro-actions, allowing the planner to implicitly condition
its plans on the received observations. However, the sampling used to partition the posterior belief
space and assign different actions to different beliefs introduces a source of approximation error
and additional computational complexity. As predicted by our earlier computational complexity
analysis, PBD scales best of the three algorithms as the number of samples increases, since it does
551

H E , B RUNSKILL , & ROY

MAC
MAD
PBD

5 Samples
Avg
Online
rewards
time(s)
12.76±0.54 0.15
15.31±0.52 0.056
12.92±0.57 0.035

50 Samples
Avg
Online
rewards
time(s)
13.68±0.65 15.39
15.88±0.54 4.81
14.76±0.57 1.26

100 Samples
Avg
Online
rewards
time(s)
12.47±0.70 58.90
15.57±0.66 20.72
14.56±0.59 4.52

500 Samples
Avg
Online
rewards
time(s)
12.94±2.57 1732.52
16.32±2.18 552.64
15.36±1.15 108.64

Table 4: Performance of macro-action algorithms in ISRS up to depth 3 with different numbers of
samples.

(b) ISRS(100,30)

(a) ISRS(15,6)

Avg rewards
SARSOP
SARSOP(macros)
RTBSS(d7,s2)
RTBSS(d10,s1)
MAC(d3,s20)
MAD(d3,s20)
PBD(d3,s20)
Fully obs.

9.43 ± 1.03
11.42 ± 0.49
8.37 ± 0.55
9.35 ± 0.65
15.94 ± 0.92
17.57 ± 0.82
17.00 ± 0.83
30.95

Online
time (s)
0.00006
0.00006
4.98
10.91
7.01
2.74
0.58
N.A.

Offline
time (s)
10000
900
0
0
0
0
0
N.A.

Avg rewards
SARSOP
SARSOP(macros)
MAC(d3,s5)
MAD(d3,s5)
PBD(d3,s5)
Fully obs.

N.A.
N.A.
42.64 ± 3.78
51.70 ± 3.46
43.68 ± 2.00
66.61

Online
time (s)
N.A.
N.A.
310.05
101.92
60.81
N.A.

Table 5: Performance on larger ISRS problems

not have to perform belief updates along each sampled trajectory explicitly. In general, performance
improves with more samples, although the improvement was not statistically significant in the ISRS
problem. However, when a decision-making under uncertainty problem requires a large number of
posterior beliefs to be sampled after every macro-action, the PBD algorithm results in consistently
faster performance for the same number of samples. Once again, MAD has a slight performance
edge due to the approximation of the discrete ISRS problem with continuous variables implicit in
PBD, but the difference is again not significant.
The macro-action forward search nature of our algorithm also allows us to scale to much larger
versions of the ROCKSAMPLE problem, since unlike offline techniques, it is unnecessary to generate
a policy that spans the entire belief space. We compared the algorithms on two additional ISRS
problems — a 16 by 16 grid with 6 rocks, and a 100 by 100 grid with 30 rocks.
Both problem domains were too large for most of the benchmark solvers that were originally
used for comparison, though the SARSOP and RTBSS algorithms could be implemented for the
ISRS[15,6] problem domain. Table 5(a) shows the performance of SARSOP and the forward search
algorithms for the ISRS[15,6] problem domain. The modified SARSOP algorithm that incorporates macro-actions ran out of memory after computing a policy offline for 900s. Because the forward search macro-action algorithms are better able to concentrate computational resources on the
reachable belief space from the agent’s current belief, the forward search macro-action algorithms
perform much better than both the SARSOP algorithm and the modified version that incorporates
macro-actions. Similarly, while the forward search single-action RTBSS algorithm performed reasonably well on the ISRS[8,5] problem if the search depth was sufficiently large, the algorithm was
552

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

Figure 6: TARGET M ONITOR problem. A helicopter must track multiple targets moving with noisy
dynamics. The field-of-view of the agent’s sensor (shaded circle) increases with the
agent’s altitude.

unable to search sufficiently deep in reasonable time on the larger ISRS[15,6] problem, resulting in
poorer performance than the forward search macro-action algorithms.
We further implemented the macro-action algorithms on a ISRS[100,30] problem domain, which
far exceeds any problem that can be solved by a traditional POMDP solver, including the modified
SARSOP algorithm that incorporates macro-actions. Table 5(b) compares the results of the three
macro-action algorithms to the fully observable value, which provides a strict upper bound of the
maximum possible reward for the problem. Such large problems also underscore the value of having
macro-actions to limit the branching factor of the forward search.

6.3 Target Monitoring
We next consider a target monitoring problem related to those studied in the sensor resource management literature (Scott, Harris, & Chong, 2009). In this problem (Figure 6), a helicopter agent
has to track multiple targets that are moving independently with noisy dynamics. The helicopter
operates in 3D space, while the targets move on the 2D ground plane. The helicopter is equipped
with a downward-facing camera for monitoring the environment, and if a target is within the fieldof-view of the camera sensor, the agent receives a noisy observation of the location and orientation
of the target. We assume for simplicity that the observations of each target are unique, allowing us
to ignore the data association problem that has been addressed elsewhere.
The noise associated with the agent’s observation of a target depends on the agent’s position
relative to the target. When the helicopter is close to the ground it can only observe a small region, but can determine the position of objects within that small region to a high level of accuracy.
When the helicopter flies at a higher altitude, it can view a wider region of the environment, but its
553

H E , B RUNSKILL , & ROY

Greedy
WT-Single
WT-Macro
NBO
MAC(d2,s10)
MAD(d2,s3)
PBD(d2,s10)
Greedy
WT-Single
WT-Macro
NBO
MAC(d2,s10)
MAD(d2,s3)
PBD(d2,s10)

1 Target
Avg rewards
Online time (s)
-21.50 ± 7.65
0.0657
65.14 ± 8.64
0.00075
64.64 ± 8.28
0.00076
-5.80 ± 7.92
0.051
41.73 ± 6.96
4.73
1.27 ± 5.23
1.66
36.21 ± 6.52
0.89
3 Targets
-18.00 ± 7.15
0.46
-23.52 ± 10.89
0.00080
-10.53 ± 17.12
0.00037
-8.27 ± 8.84
0.63
37.89 ± 12.49
70.91
-1.86 ± 5.19
26.96
55.78 ± 13.84
13.02

2 Targets
Avg rewards
Online time (s)
-26.50 ± 5.00
0.19
-27.03 ± 8.06
0.00068
-19.05 ± 7.64
0.00042
-10.78 ± 6.95
0.21
46.67 ± 18.91
22.13
0.97 ± 5.82
8.46
68.00 ± 16.65
4.33
8 Targets
-95.00 ± 23.37
2.01
-71.17 ± 14.53
0.00063
-52.98 ± 21.74
0.00025
-5.98 ± 18.00
5.78
83.86 ± 25.65
711.67
27.36 ± 14.74
432.13
120.80 ± 25.77 132.50

Table 6: TARGET M ONITOR Results. Run for 200 time steps.
measurements will be less precise. Similarly, the closer the helicopter is to a particular target, the
more accurate the helicopter’s observation of that target is expected to be. Reflecting this intuition,
we use a Gaussian observation model where the noise covariance is a function of the position of
the helicopter and target: details of this sensor model are provided in Appendix C. One desirable
attribute of our sensor model is that if the helicopter is very uncertain about a target’s location, even
if the helicopter is close to the target’s mean location, a single observation is unlikely to localize the
target. If the target location is very uncertain, there is a low probability that the target is within the
helicopter’s field of view.
The agent’s pose is fully observable, though the actions that it takes are subject to a small amount
of additive Gaussian noise. As a result, unlike the ROCKSAMPLE domains, the open-loop nature of
macro-actions means that the planner cannot perfectly predict the vehicles’ pose at the end of the
macro-action. Each target’s motion is determined by its translational and rotational velocities. The
model provides the agent with a prior over these velocities, but at every time step, the target’s true
velocities are additive functions of these fixed input controls and Gaussian noise. In the parametric
formulation, the agent maintains a Gaussian belief over each target’s state, and in order to compare
MAD, we discretize the continuous state spaces of the agent’s and targets’ positions, and maintain
a probability distribution over each discrete target state. Due to computational memory constraints,
for a 100m by 100m by 20m target monitoring problem in the x, y and z directions, we were limited
to a discretization with 10m resolution in the x, y directions, 5m in the z direction, and 45◦ angular
resolution.
We focus on a particular decision-theoretic version of the sensor resource management problem,
where at each time step the agent must decide if each of the targets is inside an area of interest.
These areas of interest are indicated by the yellow squares in Figure 6. The agent receives a positive
reward if it successfully reports that a target is in an interest region, a negative reward if it wrongly
decides that the target is in the region, and no reward if it decides that the target is not in the region,
regardless of the target’s actual state. Small costs are incurred for the agent’s motion. We call this
the TARGET M ONITOR problem.
554

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

Given the current location of the agent, macro-actions were generated by computing the sequence of actions that will enable the agent to move to a particular altitude over the means of each
target belief. For a particular desired destination, a macro-action is constructed by first computing the shortest path between the agent’s current and desired location, and then dividing this path
into primitive actions based on the maximum length of each primitive action. We also included a
hovering macro-action that consists of hovering at the agent’s current location for four time steps.
Note that the agent’s current location is fully-observable, and for the purpose of generating macroactions, we assume that the primitive actions are noise-free. Hence, for each primitive action, the
helicopter is assumed to move by the mean expected change. Similar to the ROCKSAMPLE problem,
although the macro-actions are generated according to a policy that relies on domain knowledge, the
macro-actions themselves are evaluated in the forward-search algorithms as open-loop sequences
of primitive actions. We compare the forward search macro-action algorithms to a range of intuitive strategies and prior approaches. The first algorithm is the greedy strategy, which returns the
primitive action that results in the largest expected reward in the next step. The next two approaches
are the Worst Target (WT) policies, which are hand-coded policies of traveling to the target that
has the largest uncertainty of all the targets being tracked. The intuition is that the agent’s goal in
general is to localize the targets in the environment. The two algorithms differ based on whether
the agent chooses a new target to travel to after each time step (WT-single), or re-plans only after
it has reached the target it had initially chosen (WT-macro). Finally, we compared our algorithm to
the nominal belief optimization (NBO) algorithm proposed by Scott, Harris and Chong (2009). The
NBO algorithm also assumes a Kalman filter model for the target monitoring problem, but rather
than considering the entire distribution of posterior beliefs, only the most likely posterior belief after an action is considered. In this algorithm, the most likely posterior belief for a Gaussian belief
update is given by the posterior mean without incorporating any observations, and the covariance
given by linearizing about the most likely mean at each step. Although the original algorithm uses
an optimization approach to search for action sequences, here we modify the NBO algorithm by
adopting a forward search approach, evaluating each macro-action based on the most likely posterior belief.20
Table 6 presents results for the TARGET M ONITOR problem, comparing the algorithms in scenarios with different number of targets. These results demonstrate that the PBD algorithm, with
its closed form representation of the distribution of posterior beliefs after an action, finds a significantly better policy than alternate approaches. Figure 7 demonstrates a typical policy executed by
the PBD algorithm. The agent begins in the middle of the grid world, and approaches a target at a
high altitude (Figure 7(b)), maximizing the likelihood of localizing that target. If none of the targets
seem to be approaching a region of interest, the agent hovers in the same position to conserve energy
(Figure 7(c)). When one of the targets may potentially be entering a region of interest, the agent
focuses on that target, tracking it carefully to ensure that it knows when the target is exactly in the
region of interest (Figure 7(d),(e)). The agent subsequently travels to a high altitude and repeats the
process of localizing another target with potential rewards (Figure 7(f)).
Considering the entire distribution of posterior beliefs, rather than just the maximum likelihood
posterior belief, is valuable because the agent is able to reason that there is a possibility that the
20. As noted by the authors, the NBO algorithm focuses on a new method for approximating the Q-value, rather than on
the optimization techniques. While they adopt a generic search approach for performing the optimization, the authors
also point to forward-search POMDP algorithms as good search techniques in which their Q-value approximations
could be incorporated. Our use of forward search with the NBO Q-value approximation does not affect the results.

555

H E , B RUNSKILL , & ROY

(a)

(b)

(c)

(d)

(e)

(f)

Figure 7: Snapshots of the PBD policy being executed. The black circle indicates the field-of-view
of the agent’s sensor, which is directly proportionate to the agent’s height. The size of
the error ellipses indicate the agent’s uncertainty associated with each target at each time
step. The agent alternates between flying at a high altitude to maximize the likelihood of
observing targets (b),(f) and focusing on a single target that is near/has entered an area of
interest (e).

target could be within a region of interest. In contrast, the NBO approach only considers the most
likely posterior belief, and will seek to localize the target only if the mean of its belief appears to
be heading into a region of interest. While the consideration of the entire distribution of posterior
beliefs necessarily incurs greater computational cost, we demonstrate in Section 6.4 that we are able
to track two targets in real-time using an implementation of the PBD algorithm that has not been
optimized for speed.
Table 6 also shows that because the PBD algorithm directly computes the distribution of posterior beliefs after a macro-action, the computational cost of the PBD algorithm is significantly lower
than the MAC algorithm. The MAC algorithm suffers a greater computational cost as it generates
the set of posterior beliefs after a macro-action by sampling observation sequences and explicitly
performing belief updating along each sample trajectory. In addition, because the TARGET M ON ITOR problem has a state space that is fundamentally continuous, the resolution of the state space
556

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

(a) Our Quadrotor helicopter

(b) Multiple cars being tracked. (c) Helicopter tracking car through an
area of interest

Figure 8: TARGET M ONITOR demonstration with helicopter. The helicopter has to simultaneously
track two cars and report whenever either car enters an area of interest.

discretization that was achievable given computational memory constraints was still unable to capture the inherent characteristics of the target monitoring problem, resulting in the poor performance
of MAD in the TARGET M ONITOR problem.
In the single-target case, we also observed the result that the PBD algorithm does worse than the
hand-coded policy of the agent traveling to the target with the largest uncertainty (WT-single). When
the problem only involves a single target, such a policy equates to having the agent hover over the
sole target at every step, which is the optimal policy in the single target case. In contrast, we observe
that the MAC and PBD algorithms return policies that result in the agent periodically leaving the
target to fly to a higher altitude, resulting in greater noise in the observations and corresponding loss
of rewards on average. By restricting the MAC and PBD algorithms to planning with macro-actions,
we restrict the set of plans the agent can consider in order to search deeper, rather than a shorter
conditional plan that is conditioned on the observations after each primitive action. Even though
the agent re-plans after every time step, without this conditional plan, an agent executing the MAC
or PBD algorithms will execute the “safe” policy and fly to a higher altitude, which maximizes the
likelihood of keeping the target well-localized when it is unable to condition its actions based on
subsequent observations. This example highlights the trade-off we make by considering a smaller
class of policies (those that can be expressed as chains of macro-actions) compared to the full
policy set. While in simple problems, such as a single-target TARGET M ONITOR problem, the policy
restriction can clearly be a limitation, our macro-action algorithms perform significantly better than
the other benchmark approaches when there are multiple targets, in scenarios that are arguably more
complicated and require more sophisticated planning algorithms.
6.4 Real-world Helicopter Experiments
Finally, as a proof of concept, we demonstrate the PBD algorithm on a live instantiation of the
TARGET M ONITOR problem. A motivating application for this monitoring problem is our involvement (He et al., 2010a) in the 1st US-Asian Demonstration and Assessment of Micro Aerial Vehicle
(MAV) and Unmanned Ground Vehicle (UGV) Technology (MAV’08 competition). The mission
was a hostage rescue scenario, where an aerial vehicle had to guide ground units to a hostage building while avoiding an enemy guard vehicle. Our aerial vehicle therefore had to plan paths in order
557

H E , B RUNSKILL , & ROY

Figure 9: The helicopter (blue/red cross) uses an onboard laser scanner to localize itself. A downward pointing camera is used to observe the ground targets. In this figure, the camera
image from the onboard camera is projected onto the ground plane.

to be able to monitor the different ground objects and report whenever any of them arrived at an
area of interest.
We demonstrate this scenario on an actual helicopter platform monitoring multiple ground vehicles in an indoor environment (Figure 8b). In previous work (He, Prentice, & Roy, 2008; Bachrach,
He, & Roy, 2009), we developed a quadrotor helicopter (Figure 8a) that is capable of autonomous
flight in unstructured and unknown indoor environments. The helicopter uses a laser rangefinder to
localize itself in the environment.
We mounted a downward-facing camera to make observations of the target. Since target detection is not the focus of this paper, each of the ground vehicles had a known, distinctive color, to
be detected and distinguished easily with a simple blob detection algorithm. Given the helicopter’s
position in the world and the image coordinates of the detected object, we were able to recover an
estimate of the position and orientation of a target observation in global coordinates. The helicopter
only received an observation of the target when the target was within the camera’s field-of-view,
and although the helicopter platform hovered relatively stably, slight oscillations persisted, which
resulted in noisier observations when the helicopter was flying at higher altitudes. Hence, the helicopter had to choose actions that balanced between obtaining more accurate observations at low
altitudes and a larger field-of-view by flying high.
Two ground vehicles were driven autonomously in the environment with open-loop control,
and the helicopter had to plan actions that would accurately localize both targets. To replicate
the TARGET M ONITOR problem, we marked out three areas of interest where the helicopter had to
558

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

(a)

(b)

(c)

(d)

Figure 10: Bird’s eye-view snapshots of the helicopter’s trajectory (red), based on policy generated
by the PBD algorithm. The helicopter (blue/red cross) alternates between observing the
white (b,d) and blue (c) cars in order to accurately report when either car is in an area of
interest. The area of the field-of-view of the agent’s camera sensor varies directly with
the height that the agent is flying at.

WT-Single
NBO
PBD

# Target entry detections
1
1
4

# True target entries
7
4
6

Flight time (s)
484.15
435.25
474.64

Dist. traveled (m)
243.36
247.01
282.51

Table 7: Performance of algorithms on real-world helicopter experiment. Ground truth was found
using an overhead video camera.

predict at every time step if the targets were within those areas (Figure 8c). We applied the PBD
algorithm to plan paths for the helicopter that maximized the likelihood that it could accurately
report whenever a target is in an area of interest. However, rather than sending open-loop control
actions to the helicopter, as we did in the simulation experiments, for safety reasons we closed the
loop around the position of the helicopter, sending desired waypoints that we wanted the helicopter
to navigate to. The helicopter’s true state in the world was actually partially observable, and the
helicopter had to rely on an onboard laser scanner to localize its position in the environment.
Figure 9 shows a 3D view of the helicopter as it monitors and reports on the locations of the
ground targets. As the helicopter flew around the environment, it obtained observations of the target,
which were then used to update the agent’s belief of the targets. Figure 10 provides snapshots of
the helicopter executing a plan that is computed online by the PBD algorithm. The helicopter exhibited similar behaviors to those that were observed in the simulation experiments. The helicopter
alternated between the two targets in the environment to report when either target was in an area of
interest. When the agent had a large uncertainty over a particular target’s location, it would also fly
to a higher altitude in order to increase its sensor field-of-view, thereby maximizing the likelihood
that it will be able to re-localize the targets. A video of the complete system in action is available
at: http://groups.csail.mit.edu/rrg/index.php?n=Main.Videos.
559

H E , B RUNSKILL , & ROY

As a coarse measure of achieved reward, we evaluated how well the helicopter running PBD
did at monitoring when a target entered an area of interest, and compared it to the WT-Single and
NBO algorithms. The ground truth of the number of times the targets actually entered the areas of
interests in each trial was found by using a video camera mounted overhead above the environment.
Table 7 indicates that the PBD algorithm did a much better job of monitoring the targets’ positions
than both the WT-Single and NBO algorithms. In particular, we observed that both the WT-Single
and NBO algorithms seldom took advantage of the ability to increase the agent’s sensor field-ofview by having the agent fly to a higher altitude. An agent applying these two algorithms therefore
had a higher probability of losing track of the targets completely.

7. Related Work
Decision-making under uncertainty when the states are partially observable is most commonly discussed under the Partially Observable Markov Decision Process (POMDP) framework, though this
problem has also been analyzed in other research domains under similar assumptions. While it is
beyond the scope of this paper to provide a comprehensive survey of POMDP techniques, pointbased methods such as HSVI2 (Smith & Simmons, 2005) and SARSOP (Kurniawati et al., 2008) are
often considered state-of-the-art offline methods, leveraging the piece-wise and convex aspects of
the value function to perform value updates at selected beliefs. These approaches assume a discretestate representation, but offline approaches that use parametric representations have been proposed
for continuous-valued state spaces (Brooks, Makarenko, Williams, & Durrant-Whyte, 2006; Brunskill, Kaelbling, Lozano-Perez, & Roy, 2008; Porta et al., 2006). Hoey and Poupart (2005) have
also addressed continuous observation spaces by finding lossless partitions of the observation space.
Recent work by Bonet and Geffner (2009) suggests that alternate point-based approaches that use
tabular representations of the value function may also be competitive with prior point-based approaches which used α-vector representations, and this alternate representation may be useful for
continuous domains. The ideas in this paper are more closely related to the body of online, forward
search POMDP techniques that only compute an action for the current belief, which were recently
surveyed by Ross et al. (2008a).
Macro-actions have been considered in depth within the fully observable Markov decision process community, and are typically known as “options” (Sutton et al., 1999), or posed as part of
a semi-Markov decision process (Mahadevan, Marchalleck, Das, & Gosavi, 1997). These prior
formalisms for temporally-extended actions include closed-loop policies that persist until a termination state is achieved. It would be interesting to explore in the future how these richer notions of
macro-actions could be incorporated into our approach.
Several offline POMDP approaches use macro-actions such as those of Pineau, Gordon, and
Thrun (2003b), Hansen and Zhou (2003), Charlin, Poupart, and Shioda (2007), Foka and Trahanias (2007), Theocharous and Kaelbling (2003) and Kurniawati et al. (2009). Pineau et al.’s
PolCA+ (2003b) algorithm uses a hierarchical approach to solving discrete-state POMDPs. Similarly, Hansen and Zhou (2003) propose hierarchical controllers that exploit a user-specified hierarchy for planning, while Charlin et al. (2007) provide a method for automatically discovering a
problem hierarchy. Yu, Chuang, Gerkey, Gordon and Ng (2005) provide an optimal algorithm for
planning if no observations were available. Foka and Trahanias’s (2007) solution involves building
a hierarchy of nested representations and solutions. Their focus is on discrete-state problems, particularly navigation applications. Theocharous and Kaelbling’s (2003) discrete-state reinforcement
learning approach samples observation trajectories and solves for the expected reward of a discrete
560

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

set of belief points using function approximation. Kurniawati et al. (2009) recently used macroactions to guide the sampling of belief points for use in an offline point-based POMDP solver.
However, these prior macro-action POMDP approaches compute a value function off-line, are
not aimed at scaling to very large domains, and will struggle in the environments considered in this
paper. An exception to this is the work by Hsiao and colleagues (2008, 2010) who used a form
of macro-actions for those robot manipulation tasks that involve a large state space. The focus of
their work is on robust manipulation under uncertainty, and their work only considers a very short
horizon of action trajectories. Except for the work by Kurniawati et al. (2009), all these macroaction POMDP approaches, like our PBD algorithm, assume the macro-actions are provided by a
domain expert.
In the sensor resource management domain, planning under uncertainty techniques are used in
the context of planning sensor placements to track single or multiple targets. Existing algorithms
often adopt a myopic, or greedy strategy when it comes to planning (Krause & Guestrin, 2007),
but notable exceptions include the work by Scott et al. (2009) and Kreucher, Hero III, Kastella,
and Chang (2004). Kreucher et al. describe a multi-target tracking problem, where non-myopic
sensor management is necessary for multi-target tracking. The authors use a particle filter approach
to represent the agent’s belief of the target’s location, and seek to find paths that will result in the
greatest KL divergence in density before and after the measurement. To look ahead more than
one action, this algorithm uses Monte Carlo sampling to generate possible observation outcomes.
They also provide an information-directed path searching scheme to reduce the complexity of the
Monte Carlo sampling, as well as value heuristics that will help direct the search. It is possible
that some of their insights could be used in combination with our macro-action formulation to
strengthen both approaches. In the experimental section we compared our approach to the work by
Scott et al. (2009), who directly formulated target tracking as a POMDP, and proposed the Nominal
Belief Optimization (NBO) algorithm that computes the most likely belief after an action for deeper
forward search. In contrast, our algorithm explicitly computes the entire set of possible posterior
beliefs after a macro-action. Recently two groups (Erez & Smart, 2010; Platt, Tedrake, LozanoPerez, & Kaelbling, 2010) have independently proposed an approach that lies in the middle of this
spectrum: beliefs are updated by assuming that the most likely observation is received, but the
variance is increased. In contrast, our approach represents that each resulting belief may be fairly
peaked, but the mean of the beliefs may be spread out. This more complete representation may be
advantageous if there are sharp changes in the reward function.
As stated in the introduction, the finite-horizon forward search, act, and re-plan strategy PBD
follows can be seen as an instance of the Model Predictive Control/Receding Horizon Control
(MPC/ RHC) framework from the controls community. Examples of MPC and RHC include the
work by Kuwata and How (2004), Bellingham, Richards, and How (2002), and Richards, Kuwata,
and How (2003). A special case of RHC control is Certainty Equivalence Control, or CEC (see Bertsekas, 2007 for an overview). In fully observable systems, CEC first assumes all stochastic operations (such as transitions) take on their expected value, and then solves a finite-horizon deterministic
control problem. CEC may be applied in partially observable environments by first sampling an
initial state from the belief state. Though CEC can be very efficient in large domains, a key limitation of its use in partially observable environments is that a CEC-style controller will never take
information-gathering actions. Returning to the generic class of MPC approaches, to our knowledge
no prior model predictive controllers have used macro-actions nor developed the notion of a pos561

H E , B RUNSKILL , & ROY

terior distribution of beliefs, which enables our PBD approach to scale to large uncertain domains
where a multi-step lookahead is required.

8. Conclusion
In this paper we have presented the Posterior Belief Distribution algorithm. PBD is a forwardsearch algorithm for large (consisting of many variables, each of which can take on many values)
partially observable domains. PBD analytically and efficiently computes the resulting distribution
of posterior belief states possible after a sequence of actions. This allows the computational cost
of evaluating the reward associated with a macro-action to be tractable, which we leverage to enable longer horizon lookahead search during online planning. We have presented theoretical and
experimental results evaluating the performance and computational cost of our macro-action algorithms. Our algorithms were applied to problem domains that span multiple research communities,
and consistently performed better than prior approaches in large domains which require multi-step
lookahead for good performance. Finally, we demonstrated our algorithm on a real robotic helicopter, underscoring the applicability of our algorithm for planning in real-world, long-horizon,
partially observable domains.

9. Acknowledgments
Ruijie He, E. Brunskill and N. Roy were supported by the National Science Foundation (NSF)
Division of Information and Intelligent Systems (IIS) under Grant #0546467 and by the Office
of Naval Research under the “Decentralized Reasoning in Reduced Information Spaces” project,
Contract # N00014-09-1-1052.
We wish to thank Finale Doshi-Velez, Alborz Geramifard, Josh Joseph, Brandon Luders, Javier
Velez, and Matthew Walter for valuable discussions and feedback. Daniel Gurdan, Jan Stumpf
and Markus Achtelik provided the quadrotor helicopter and the support of Ascending Technologies. Abraham Bachrach, Anton De Winter, Garrett Hemann, Albert Huang, and Samuel Prentice
assisted with the development of the software and hardware for the helicopter demonstration. We
also appreciate early POMDP forward search discussions with Leslie Pack Kaelbling and Tomas
Lozano-Perez.

Appendix A: Exponential Family Kalman Filter
Building on statistical economics research for time-series analysis of non-Gaussian observations (Durbin
& Koopman, 2000), we present the Kalman filter equivalent for systems with linear-Gaussian statetransitions and observation models that belong to the exponential family of distributions.
The state-transition and observation models can be represented as follows:
st = At st−1 + Bt at + εt ,
p(zt |θt ) =

exp(ztT θt

− βt (θt ) + κt (zt )),

st−1 ∼ N (µt−1 , Σt−1 ),
θt = W (st ).

εt ∼ N (0, Pt )

(68)
(69)

For the state-transition model, st is the system’s hidden state, at is the control actions, At and Bt
are the linear transition matrices, and ǫt is the state-transition Gaussian noise with covariance Pt .
The observation model belongs to the exponential family of distributions. θt and βt (θt ) are
the canonical parameter and normalization factor of the distribution, and W (.) maps the states to
canonical parameter values. W (.) depends on the particular member of the exponential family. For
562

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

ease of notation, we let
υt (zt |θt ) = − log p(zt |θt ) = −ztT θt + βt (θt ) + κt (zt ).

(70)

Following the traditional Kalman filter, the process update can be written as
Σt = At Σt−1 ATt + Pt ,

µt = At µt−1 + Bt at ,

(71)

where µt and Σt are the mean and covariances of the posterior belief after the process update but
before the measurement udpate. For the measurement update, we seek to find the conditional mode
µt = arg max p(st |zt )

(72)

st

= arg max p(zt |st )b(st )

(Bayes rule)

st

(73)

= arg max p(zt |θt )b(st )

(74)

st

⇒

1
−1
= arg max exp(−Jt ), where Jt = − log p(zt |θt ) + (st − µt )T Σt (st − µt )
st
2
∂Jt 
∂υt (zt , θt ) ∂θt
−1
0=
=
+ Σt (µt − µt ).

∂st st =µt
∂θt
∂st

Taking the derivative of θt = W (st ) about the prior mean µt , we let

∂W (st ) 
.
Yt =
∂st st =µt

(76)

(77)

(zt |θt )
Similarly, performing a Taylor expansion on ∂υt∂θ
about θt = W (µt ),
t


∂ 2 υt (zt |θt ) 
∂υt (zt |θt ) ∂υt (zt |θt ) 
=
+
(θt − θt )

∂θt
∂θt
∂θt ∂θtT θt =θt
θt =θt

∂υt (zt |θt )
=υ̇t + ϋt (θt − θt )
∂θt


∂
T
(−zt θt + βt (θt ) − κt (zt ))
,
where
υ̇t =
∂θt
θt =θt

∂βt (θt ) 
−zt
=
∂θt 

(75)

(Eqn. 70)

(78)
(79)
(80)
(81)

θt =θt

υ̇t =β̇t − zt

and


∂ 2 βt (zt |θt ) 
ϋt =
∂θ ∂θT 
t

t

θt =θt

(θt − θt ).

(82)
(83)

ϋt =β̈t

(84)

Plugging Equations 82 and 84 into Equation 79, and then into Equation 76,
−1

YtT (β̇t − zt + β̈t (θt − θt )) = − Σt (µt − µt )
−1

YtT β̈t (β̈t−1 (β̇t − zt ) − θt + θt ) = − Σt (µt − µt )
−1

YtT β̈t ((θt − β̈t−1 (β̇t − zt )) − θt ) =Σt (µt − µt )
−1

YtT β̈t (z̃t − W (st )) =Σt (µt − µt ),
563

(85)
(86)
(87)
(88)

H E , B RUNSKILL , & ROY

where z̃t = (θt − β̈t−1 (β̇t − zt )) is the projection of the observation onto the parameter space of
the exponential family distribution, and is independent of st . In Equation 88 we substituted θt using
Equation 69.
Mean Update
Using Equation 88 and substituting µt for st ,
−1

Σt (µt − µt ) = YtT β̈t (z̃t − W (µt ))
=

=
Linearizing W (st ) about µt ,

YtT
YtT

(89)

β̈t (z̃t − W (µt )) + W (µt ) − W (µt )

β̈t (z̃t − W (µt )) − YtT β̈t (W (µt ) − W (µt )).

W (st ) = W (µt ) + W ′ (st )st =µt (st − µt )
⇒

−1
Σt (µt

= W (µt ) + Yt (µt − µt )

− µt ) = YtT β̈t (z̃t − W (µt )) − YtT β̈t Yt (µt − µt )

YtT β̈t (z̃t − W (µt )) =
=

⇒

µt − µt =

−1
(Σt + YtT β̈t Yt )(µt −
Σ−1
t (µt − µt )
Σt YtT β̈t (z̃t − W (µt )),

µt )

(90)
(91)
(92)
(93)
(94)
(95)
(96)
(97)

where Σt YtT β̈t = K̃t is the Kalman gain for non-Gaussian exponential family distributions. Via a
standard transformation, the Kalman gain can be written in terms of covariances other than Σt ,

and

K̃t = Σt YtT (Yt Σt YtT + β̈t−1 )−1

(98)

µt = µt + K̃t (z̃t − W (µt )).

(99)

Covariance Update
Given a Gaussian posterior belief,

∂2J
∂s2t

is the inverse of the covariance of the agent’s belief

∂2J
∂s2t
∂
−1
=
(Σt (st − µt ) − YtT β̈t (z̃t − W (st )))
∂x
−1
= Σt + YtT β̈t Yt

Σ−1
t =

⇒ Σt =

−1
(Σt

+ YtT β̈t Yt )−1 .

(100)
(101)
(102)
(103)

Appendix B. Rock Sample Observation Model
In the Rocksample problem, the Bernoulli observation function can be written as follows. Recall
that rt is the agent’s position at time t, RBi is the location of the information beacon associated
with rock i, zi,t is a binary observation of the value of rock i at time t, and si,t is the true value of
564

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

rock i at time t. Then if we let di,t =k rt − RBi k2 , then
p(zi,t |RVi,t = si,t , rt , RBi )

(104)

= (0.5 + (si,t − 0.5)2−di,t /D0 )zi,t (0.5 − (si,t − 0.5)2−di,t /D0 )1−zi,t

(105)

= exp(zi,t ln

(106)

0.5)2−di,t /D0

0.5 + (si,t −
+ ln(0.5 − (si,t − 0.5)2−di,t /D0 ))
0.5 − (si,t − 0.5)2−di,t /D0

= exp(zi,t θt − βt (θt )).

(107)

We therefore have the parameters of the exponential family observation model
θi,t = W (si,t , rt , RBi )
= ln

(108)

0.5 + (si,t − 0.5)2−di,t /D0
0.5 − (si,t − 0.5)2−di,t /D0

(109)

βi,t = − ln(0.5 − (si,t − 0.5)2−di,t /D0 )
= ln(exp(θi,t ) + 1).

We can then derive the derivatives Yi,t and β̈i,t

∂W (si,t , rt , RBi ) 
Yt =

∂si,t
si,t =m̂i,t

(111)

(112)


0.5 + (si,t − 0.5)2−di,t /D0 
∂
=
ln
∂si,t 0.5 − (si,t − 0.5)2−di,t /D0 si,t =m̂i,t
=

(110)

(113)

1
2−di,t /D0
·
−d
/D
0.5 + (m̂i,t − 0.5)2 i,t 0 0.5 − (m̂i,t − 0.5)2−di,t /D0

(114)

where ŝi,t is the mean of the belief used for linearization. Since
⇒

βi,t = ln(exp(θi,t ) + 1),

(115)

then
β̈i,t


∂ 2 bi,t 
=
2 
∂θi,t
θi,t =θ̂i,t

=

exp(θ̂i,t )

exp(θ̂i,t ) + 1

(116)
−

exp(2θ̂i,t )
(exp(θ̂i,t ) + 1)2

.

(117)

Appendix C. Target Tracking Observation Model
We adopt an observation model for target tracking where the target observation obtained has Gaussian noise and the noise covariance Σzi is a function of the position of the helicopter and target
i:




xi
zxi
 zyi  = f  yi  + N (0, Σzi )
θi
zθi
Σzi = g(xi , yi , xa , ya , ha ),
565

H E , B RUNSKILL , & ROY

Figure 11: The observation noise covariance is a function of the height of helicopter, the distance
between the helicopter and the mean of the target belief, and the covariance of the target
belief. At lower altitudes, the helicopter can make better observations of targets close to
it, but has a limited field of vision. At higher heights, the helicopter can see a larger area
but even close targets are more noisily observed.

where xi , yi , θi is the pose of target i, while xa , ya , ha correspond to the agent’s position and height
in the environment. zxi , zyi , zθi is the observation of target i in image coordinates.
The covariance function itself is specified as

 
 
 
T
xi
xa
xi
xa
−
−
yi
ya
yi
ya
g(xi , yi , xa , ya , ha ) = C1 ha + C2
+ C3 ,
ha
where C1 , C2 and C3 are constants.
In the generic belief update expression where the target position, si = [xi ; yi ; θi ], is unknown,
Z
Z
′ ′
′
′
b (si ) ∝ p(z|si , a, Σzi )
p(si |si , a)b(si )dsi s.t.
b′ (s′i )ds′i = 1,
s′i

si

which means that each possible s′i would be associated with a different covariance Σzi . Performing
this integration exactly would not keep the distribution Gaussian. Instead, we approximate the observation model by computing a single expected covariance Σ̂zi given the current belief distribution:
Z
b(si )Σzi (si )dsi .
Σ̂zi = E[Σzi ] =
si

Substituting in the exact expressions for the covariance function and the belief after an action is
taken but before incorporating the measurement, ba (s) ∼ N (si |µ, Σ), we get:
!

      T
Z   
C2
xi 
xa
xi
xa
xi
+ C3 dxi dyi .
E[Σzi ] = N
µ ,Σ
−
−
C1 ha −
yi  xy xy
ya
yi
ya
yi
ha
566

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

and by adding and subtracting µxy from the second term, reduces to
C2
E[Σzi ] = C1 ha +
ha




 
T
C2
xa
xa
µxy −
µxy −
Σxy
+
ya
ya
ha

where µxy , Σxy refer to the translational components of the agent’s belief.
In contrast to simpler observation models, our observation model has the desirable characteristic
that if a target’s location is very uncertain, namely its covariance Σxy is very large, then even if the
target’s mean location is close to the helicopter’s mean location, the expected benefit of receiving
an observation (in terms of reducing the target’s uncertainty) is still small. This property comes out
automatically from the above derivation, since E[Σzi ] includes the current target covariance Σxy .
Figure 11 provides an illustration of the expected covariance for different locations of the target
relative to the agent, agent heights, and target belief covariances.

References
Bachrach, A., He, R., & Roy, N. (2009). Autonomous flight in unstructured and unknown indoor
environments. In Proceedings of the European Micro Aerial Vehicle (EMAV) Conference.
Barndorff-Nielsen, O. (1979). Information and exponential families in statistical theory. Bulletin of
the American Mathematics Society, 273, 667–668.
Bellingham, J., Richards, A., & How, J. (2002). Receding horizon control of autonomous aerial
vehicles. In Proceedings of the American Control Conference (ACC), Vol. 5, pp. 3741–3746.
Bertsekas, D. (2007). Dynamic Programming and Optimal Control, vol. 1 & 2, 2nd. Athena Scientific.
Bonet, B., & Geffner, H. (2009). Solving POMDPs: RTDP-Bel vs. point-based algorithms. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 1641–
1646.
Brooks, A., Makarenko, A., Williams, S., & Durrant-Whyte, H. (2006). Parametric POMDPs for
planning in continuous state spaces. Robotics and Autonomous Systems, 54(11), 887–897.
Brunskill, E., Kaelbling, L., Lozano-Perez, T., & Roy, N. (2008). Continuous-state POMDPs with
hybrid dynamics. In Proceedings of the International Symposium on Artificial Intelligence
and Mathematics (ISAIM).
Charlin, L., Poupart, P., & Shioda, R. (2007). Automated hierarchy discovery for planning in
partially observable environments. In Advances in Neural Information Processing Systems
(NIPS).
Durbin, J., & Koopman, S. (2000). Time series analysis of non-Gaussian observations based on state
space models from both classical and Bayesian perspectives. Journal of the Royal Statistical
Society: Series B (Methodological), 62(1), 3–56.
Erez, T., & Smart, W. (2010). A Scalable Method for Solving High-Dimensional Continuous
POMDPs Using Local Approximation. In Proceedings of the Conference on Uncertainty
in Artificial Intelligence (UAI).
Foka, A., & Trahanias, P. (2007). Real-time hierarchical POMDPs for autonomous robot navigation.
Robotics and Autonomous Systems, 55(7), 561–571.
567

H E , B RUNSKILL , & ROY

Hansen, E., & Zhou, R. (2003). Synthesis of hierarchical finite-state controllers for POMDPs. In
Proceedings of the Thirteenth International Conference on Automated Planning and Scheduling (ICAPS).
He, R., Bachrach, A., Achtelik, M., Geramifard, A., Gurdan, D., Prentice, S., Stumpf, J., & Roy,
N. (2010a). On the design and use of a micro air vehicle to track and avoid adversaries.
International Journal of Robotics Research, 29(5), 529–546.
He, R., Brunskill, E., & Roy, N. (2010b). PUMA: planning under uncertainty with macro-actions.
In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI).
He, R., Prentice, S., & Roy, N. (2008). Planning in information space for a quadrotor helicopter in
GPS-denied environments. In Proceedings of the International Conference of Robotics and
Automation (ICRA), pp. 1814–1820.
Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal of
the American Statistical Association, 58(301), 13–30.
Hoey, J., & Poupart, P. (2005). Solving POMDPs with continuous or large discrete observation
spaces. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).
Hsiao, K., Kaelbling, L., & Lozano-Pérez, T. (2010). Task-driven tactile exploration. In Proceedings
of Robotics: Science and Systems (RSS).
Hsiao, K., Lozano-Pérez, T., & Kaelbling, L. (2008). Robust belief-based execution of manipulation programs. In Proceedings of the Workshop on the Algorithmic Foundations of Robotics
(WAFR).
Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. Transactions of
the ASME–Journal of Basic Engineering, 82(Series D), 35–45.
Kearns, M., Mansour, Y., & Ng, A. (2002). A sparse sampling algorithm for near-optimal planning
in large Markov decision processes. Machine Learning, 49(2-3), 193–209.
Krause, A., & Guestrin, C. (2007). Near-optimal observation selection using submodular functions.
In Proceedings of the National Conference on Artificial Intelligence (AAAI), Vol. 22, pp.
1650–1654.
Kreucher, C., Hero III, A., Kastella, K., & Chang, D. (2004). Efficient methods of non-myopic
sensor management for multitarget tracking. In Proceedings of the IEEE Conference on
Decision and Control (CDC), Vol. 1, pp. 722–727.
Kurniawati, H., Du, Y., Hsu, D., & Lee, W. (2009). Motion planning under uncertainty for robotic
tasks with long time horizons. In Proceedings of the International Symposium of Robotics
Research (ISRR).
Kurniawati, H., Hsu, D., & Lee, W. (2008). SARSOP: Efficient point-based POMDP planning by
approximating optimally reachable belief spaces. In Proceedings of the Robotics: Science
and Systems (RSS).
Kuwata, Y., & How, J. (2004). Three dimensional receding horizon control for UAVs. In Proceedings of the AIAA Guidance, Navigation, and Control Conference and Exhibit (GNC), pp.
16–19.
568

E FFICIENT P LANNING UNDER U NCERTAINTY WITH M ACRO - ACTIONS

Littman, M., Cassandra, A., & Kaelbling, L. (1995). Learning policies for partially observable
environments: scaling up. In Proceedings of the Twlfth International Conference on Machine
Learning (ICML), pp. 362–370.
Mahadevan, S., Marchalleck, N., Das, T., & Gosavi, A. (1997). Self-improving factory simulation
using continuous-time average-reward reinforcement learning. In Proceedings of the International Conference on Machine Learning (ICML), pp. 202–210.
Mayne, D. Q., Rawlings, J. B., Rao, C. V., & Scokaert, P. O. M. (2000). Constrained model predictive control: Stability and optimality. Automatica, 36, 789–814.
McAllester, D., & Singh, S. (1999). Approximate planning for factored POMDPs using belief state
simplification. In Proceedings of the Conference on Uncertainty in Artificial Intelligence
(UAI), pp. 409–416.
McGovern, A. (1998). acQuire-macros: An algorithm for automatically learning macro-actions. In
NIPS 98 Workshop on Abstraction and Hierarchy in Reinforcement Learning.
Paquet, S., Chaib-draa, B., & Ross, S. (2006). Hybrid POMDP algorithms. In Workshop on MultiAgent Sequential Decision Making in Uncertain Domains (MSDM), pp. 133–147.
Paquet, S., Tobin, L., & Chaib-draa, B. (2005). An online POMDP algorithm for complex multiagent environments. In Proceedings of the Conference on Autonomous agents and Multiagent
systems (AAMAS), pp. 970–977.
Patek, S., Breton, M., Chen, Y., Solomon, C., & Kovatchev, B. (2007). Linear quadratic gaussianbased closed-loop control of type 1 diabetes. Journal of Diabetes Science and Technology,
1.
Pineau, J., Gordon, G., & Thrun, S. (2003a). Point-based value iteration: An anytime algorithm
for POMDPs. In Proceedings of the International Joint Conference on Artificial Intelligence
(IJCAI), Vol. 18, pp. 1025–1032.
Pineau, J., Gordon, G., & Thrun, S. (2003b). Policy-contingent abstraction for robust robot control.
In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI).
Platt, R., Tedrake, R., Lozano-Perez, T., & Kaelbling, L. (2010). Belief space planning assuming
maximum likelihood observations. In Proceedings of Robotics: Science and Systems (RSS).
Porta, J., Vlassis, N., Spaan, M., & Poupart, P. (2006). Point-based value iteration for continuous
POMDPs. Journal of Machine Learning Research, 7, 2329–2367.
Poupart, P. (2005). Exploiting Structure to Efficiently Solve Large Scale Partially Observable
Markov Decision Processes. Ph.D. thesis, University of Toronto.
Richards, A., Kuwata, Y., & How, J. (2003). Experimental demonstrations of real-time MILP control. In Proceeding of the AIAA Guidance, Navigation, and Control Conference (GNC).
Ross, S., & Chaib-draa, B. (2007). AEMS: An anytime online search algorithm for approximate
policy refinement in large POMDPs. In Proceedings of the International Joint Conference in
Artificial Intelligence (IJCAI), pp. 2592–2598.
Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008a). Online planning algorithms for POMDPs.
Journal of Artificial Intelligence Research, 32(1), 663–704.
569

H E , B RUNSKILL , & ROY

Ross, S., Chaib-draa, B., & Pineau, J. (2008b). Bayesian reinforcement learning in continuous
POMDPs with application to robot navigation. In Proceedings of the International Conference on Robotics and Automation (ICRA). IEEE.
Scott, A., Harris, Z., & Chong, E. (2009). A POMDP framework for coordinated guidance of
autonomous UAVs for multitarget tracking. EURASIP Journal on Advances in Signal Processing, 2009, 1–17.
Smallwood, R., & Sondik, E. (1973). The optimal control of partially observable Markov processes
over a finite horizon. Operations Research, 21(5), 1071–1088.
Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: Improved analysis and implementation. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI).
Stolle, M., & Precup, D. (2002). Learning options in reinforcement learning. Lecture Notes in
Computer Science, 212–223.
Sutton, R., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for
temporal abstraction in reinforcement learning. Artificial Intelligence, 112, 181–211.
Theocharous, G., & Kaelbling, L. (2003). Approximate planning in POMDPs with macro-actions.
In Advances in Neural Processing Information Systems (NIPS).
Triantafyllopoulos, K. (2003). On the central moments of the multidimensional Gaussian distribution. The Mathematical Scientist, 28, 125–128.
West, M., Harrison, P., & Migon, H. (1985). Dynamic generalized linear models and Bayesian
forecasting. Journal of the American Statistical Association, 80(389), 73–83.
Yu, C., Chuang, J., Gerkey, B., Gordon, G., & Ng, A. (2005). Open-loop plans in multi-robot
POMDPs. Tech. rep., Stanford University.

570

Journal of Artificial Intelligence Research 40 (2011) 657-676

Submitted 12/10; published 03/11

The Complexity of Integer Bound Propagation
Lucas Bordeaux

lucasb@microsoft.com

Microsoft Research, 7 J J Thomson Avenue, CB30FB
Cambridge, UNITED KINGDOM

George Katsirelos

gkatsi@gmail.com

LRI, Université Paris-Sud 11
Paris, FRANCE

Nina Narodytska

ninan@cse.unsw.edu.au

NICTA Neville Roach Laboratory and
University of New South Wales
223 Anzac Parade Kensington NSW 2052, AUSTRALIA

Moshe Y. Vardi

vardi@cs.rice.edu

Rice University, P. O. Box 1892
Houston, TX 77251-1892, U.S.A.

Abstract
Bound propagation is an important Artificial Intelligence technique used in Constraint
Programming tools to deal with numerical constraints. It is typically embedded within a
search procedure (”branch and prune”) and used at every node of the search tree to narrow
down the search space, so it is critical that it be fast. The procedure invokes constraint
propagators until a common fixpoint is reached, but the known algorithms for this have
a pseudo-polynomial worst-case time complexity: they are fast indeed when the variables
have a small numerical range, but they have the well-known problem of being prohibitively
slow when these ranges are large. An important question is therefore whether stronglypolynomial algorithms exist that compute the common bound consistent fixpoint of a set
of constraints. This paper answers this question. In particular we show that this fixpoint
computation is in fact NP-complete, even when restricted to binary linear constraints.

1. Introduction and Overview of the Main Results
Constraint solvers typically solve problems by interleaving search and propagation. Propagation is an iterative procedure which, at each iteration, propagates every constraint in
the problem to narrow the domains of its variables. The iteration stops when no constraint
changes the domains of its variables. In this case, propagation has reached a common fixpoint for all constraints. This iterative algorithm is guaranteed to compute the fixpoint in
polynomial time if propagating each constraint takes polynomial time and the domains of
the variables are defined as lists of values. Very often, however, it is inconvenient or infeasible to list all values explicitly: instead the domains are defined by lower and upper bounds.
We focus on this representation, and on variables taking integer values. In this setting,
computing a fixpoint by the iterative algorithm may require exponential time even if each
constraint can be propagated in polynomial time. We show that this exponential behaviour
is not simply due to the iterative algorithm being suboptimal; rather it is intrinsic to the
c
2011
AI Access Foundation. All rights reserved.

Bordeaux, Katsirelos, Narodytska, & Vardi

problem of computing a fixpoint, as it is NP-complete even when the system of constraints
is restricted to binary linear inequality constraints.
1.1 Bound Propagation and Slow Convergence
We illustrate the behaviour of the iterative fixpoint algorithm using a system of two constraints:
x + y = 7, x + 1 ≥ 2y,

with initial bounds:

x ∈ [0, 5], y ∈ [0, 10]

A possible trace of the fixpoint computation is the following. The lower bound of y is
initially 0 but from the constraint x + y = 7 we deduce that y cannot take values 0 or 1:
if it does, then the sum is < 7, even if we fix x to its highest allowed value. Therefore the
intervals can be narrowed down to x ∈ [0, 5], y ∈ [2, 10]. Similarly:
from
from

x + y = 7,
x + 1 ≥ 2y,

back to

x + y = 7,

we deduce:
we deduce:
and:
we now deduce:

x ∈ [0, 5], y
x ∈ [3, 5], y
x ∈ [3, 5], y
x ∈ [4, 5], y

∈ [2, 7];
∈ [2, 7];
∈ [2, 3];
∈ [2, 3].

At this point we have reached a common fixpoint for both constraints, because we cannot
deduce that the domains need to be narrowed any further.
This algorithm, however, exhibits slow convergence behaviour even in deceivingly simple
examples such as:
x < y, y < x

with initial bounds:

x ∈ [0, 108 ], y ∈ [0, 108 ]

(1)

The iterative algorithm for fixpoint computation shrinks the bounds by one unit in each
iteration, which means that 108 iterations will be required to reach the fixpoint, which in
this case is empty. This slow convergence is in fact exponential in the size of the problem
representation, as log(108 ) bits are enough to represent each bound. This behaviour is not
limited to artificial examples as the previous one but in fact happens time and again when
solving problems with large numerical ranges. This severely limits the application of CP in
areas such as software verification or theorem proving where large ranges are needed (e.g.,
the whole 32-bit integer range).
Due to the importance of the problem, efforts have been made to alleviate slow convergence, notably Jaffar, Maher, Stuckey, and Yap (1994), Lhomme, Gottlieb, Rueher, and
Taillibert (1996), Lebbah and Lhomme (2002), Leconte and Berstel (2006); but all proposed
algorithmic improvements prevent slow convergence only in specific cases. Fully addressing
the slow convergence problem would require a strongly polynomial algorithm for fixpoint
computation. Therefore the question is: does such an algorithm exist, or is bound propagation in fact intractable?
1.2 Prior Complexity Results on Propagation
Standard propagation algorithms are iterative processes that apply ”propagators”, i.e., narrowing functions associated with each constraint, until reaching a fixpoint. Their complexity
is therefore determined by two complementary questions:
658

The Complexity of Integer Bound Propagation

Q1: How hard is it to compute each propagator?
Q2: How hard is it to find a common fixpoint of the propagators?
The complexity of constraint propagation has in a sense been extremely well-studied, but
all the results we are aware of for bound propagation deal with Question 1 only. Those prior
hardness results showed that for some complex constraints we cannot have polynomial-time
propagators reaching certain levels of consistency. Two such results are:
• Given a linear equality it has been observed (Yuanlin & Yap, 2000; Choi, Harvey,
Lee, & Stuckey, 2006) that any propagator that reaches arc consistency or bound(Z)
consistency1 needs to solve a knapsack problem, which is NP-complete in the weak
sense. For this reason propagators for linear constraints used in practice either reach
a weaker consistency such as bound(R) consistency, or are restricted to very small
domains, as proposed for instance in Trick (2001).
• Results by Bessière (2006) prove that even bounded-arity (two-variable) constraints
can be constructed for which checking bound(Z) consistency is NP-complete.
Question 2 only makes sense, of course, in the common case where the propagators are
polynomial-time computable (if they are not, computing their common fixpoint cannot be
easy in general). The only known fact in this case is that the standard, iterative propagation
algorithms often take an exponential number of steps to reach a fixpoint in practice, as
mentioned and illustrated in Section 1.1. This leaves open the question whether better
algorithms exist or fixpoint computation is, in fact, intrinsically hard.
1.3 Our Main Results
In this paper we consider very simple, common propagators and address Question 2. We
show that in general even surprisingly simple propagators can lead to a fixpoint computation problem that is NP-hard. This not only explains why all standard, iterative fixpointcomputation algorithms have an exponential worst-case in practice, but also shows it is unlikely that the there exists an algorithm with better worst case. In particular an important
class of simple propagators whose fixpoint computation is NP-hard is the bound(R) consistency propagators for linear constraints (Proposition 1). These are ubiquitous constraints,
and very weak and widely used propagators for these constraints. Many problems that
use numerical computations and large domains tend to include at least linear constraints,
therefore there are few cases where slow convergence will be avoidable. We nevertheless
identify one such case: if the coefficients of the linear constraints are all unit (1 in absolute
value), then bound(R) consistency can be obtained in polynomial time by a non-standard
propagation algorithm based on Linear Programming. We also study other types of basic
numerical constraints: multiplication and max.
1.4 Outline
In Section 2 we summarize the required material on Constraint Satisfaction Problems
and bound propagation. Section 3 then focuses on linear constraints. We prove the
1. We give the formal definitions of bound(R) and bound(Z) consistency in Section 2.2.

659

Bordeaux, Katsirelos, Narodytska, & Vardi

aforementioned Proposition 1, then identify restricted forms of linear constraints for which
propagation is tractable. Section 4 presents results for some other basic propagators: for
quadratic constraints, the hardness result can be strengthened and holds even for a fixed
number of variables; for max constraints, fixpoint computation has an interesting complexity
(between P and NP-complete) and is proved equivalent to an important open problem; we
last comment on max-closed constraints. We conclude in Section 5.

2. Formal Background
In this section we summarize the required material on Constraint Satisfaction Problems and
bound propagation. More details on this material can be found in papers by e.g. Schulte
and Carlsson (2006), and Bessière (2006).
2.1 Constraint Satisfaction Problems
A Constraint Satisfaction Problem (CSP) is a triple hX, D, Ci, where: X = {x1 · · · xn } is
a set of variables, D = {D1 · · · Dn } is a set of finite domains (finite sets of values), one for
each variable, and C is a set of constraints. In this paper we consider discrete domains: all
the elements in D are integers. For the moment we simply define constraints very generally
as logical predicates over subsets of X; later in the paper we consider specific types of
constraints, for instance linear ones. An assignment is a function τ that assigns a value
τ (xi ) ∈ Di to every variable xi . A solution to the CSP is an assignment that satisfies the
constraints. Throughout the paper, we keep the following conventions:
• n = |X| denotes the number of variables;
• m = |C| is the number of constraints;
• d = maxi∈1..n |Di | is the size of the largest domain.
It is important to note that Di may be represented as an interval, rather than an explicit
set of values. In this work, we only consider domains represented as intervals: each domain
is of the form Di = [li , ui ], where li and ui are the lower and upper bounds of the domain.
2.2 Propagators and Notions of Bound Consistency
The constraints of the problem are associated with propagators. (In our setting there will
be, in general, several propagators per constraint.) We follow the classical presentation of
propagators as operators on a lattice, initiated in work by Benhamou (1996) and on which
more details can be found in papers by Apt (1999), and Schulte and Carlsson (2006): each
propagator is a function that can narrow the domains of (some of) the variables, removing
values that cannot appear in any solution. Thus, we talk about the current domain of a
variable xi , as the result of it being narrowed by the application of one or more propagators.
+
−
We denote by x−
i the current lower bound of xi and by xi its current upper bound. xi
+
and xi are initially set to the initial bounds li , ui and remain afterwards constrained by
+
li ≤ x−
i ≤ xi ≤ ui . We denote by D the Cartesian product of the intervals [li , ui ] for all
i ∈ 1..n.
660

The Complexity of Integer Bound Propagation

Definition 1 (Propagator) A propagator for a constraint k ∈ 1..m is a function f :
P(D) → P(D), that is:
• monotone, i.e., A0 ⊆ A → f (A0 ) ⊆ f (A);
• contracting, i.e., f (A) ⊆ A;
• correct, i.e., no point in A \ f (A) satisfies the constraint.
We restrict ourselves to propagators that are polynomial-time computable. Bound consistency propagators are additionally restricted to elements of P(D) that are representable
as Cartesian products of intervals, plus the special value ∅.
Several types of propagators can be used for numerical constraints; these propagators
are characterized by the level of consistency they enforce. Since we have restricted our
focus to interval domains, we present only bound consistency. The two main variants are
bound(Z) and bound(R) consistency:
Definition 2 (Bound(Z|R) support) A bound(Z) (bound(R)) support of a constraint k
+
is an assignment τ of integer (real) values to the variables X such that x−
i ≤ τ (xi ) ≤ xi
for i ∈ 1..n and τ satisfies the constraint k.
Definition 3 (Bound(Z|R) consistency) A constraint k is bound(Z) (bound(R)) consistent iff for every variable xi ∈ X, there exists a bound(Z) (bound(R)) support τ − with
+ with τ + (x ) = x+ .
τ − (xi ) = x−
i
i
i and a bound(Z) (bound(R)) support τ
The difference between the two is easily understood on an example:
Example 1 Consider the constraint 2x + 2y + 3z = 4.
• The intervals x, y, z ∈ [0, 1] are bound(R) consistent since each of the integer bounds
has a real-valued support: x = 0 is supported by the tuple (x = 0, y = 1, z = 2/3);
z = 1 and y = 0 by the tuple (x = 1/2, y = 0, z = 1); x = 1, y = 1 and z = 0 by the
tuple (x = 1, y = 1, z = 0).
• These intervals are, however, not bound(Z) consistent: the only integer solution is
(x = 1, y = 1, z = 0), which means that bound(Z) consistency would reduce the bounds
further to x ∈ [1, 1], y ∈ [1, 1], z ∈ [0, 0].
Bound(Z) consistency requires that we check the existence of an integer-valued support,
and for some classes of constraints such as linear equalities each propagator would need to
solve an NP-complete problem. Since our focus is on the computation of common fixpoint of
simple operators, we only consider bound(R) consistency in this paper. As noted previously
in the literature (Schulte & Stuckey, 2005), bound(R) consistency it is in fact “the bound
consistency implemented for most primitive constraints”, precisely because it is often the
only one for which propagators are easy to compute in general for large domains. In the
rest of the paper we focus on several of the main basic types of numerical constraints (in
particular linear ones), and give further details on the bound(R) consistency propagators
obtained for these constraints. In all the cases we consider the propagators are very simple
indeed.
661

Bordeaux, Katsirelos, Narodytska, & Vardi

A := D
change := true
while change do
change := false
foreach f ∈ F do
oldA := A
A = f (A)
if A 6= oldA then change := true
done
done
Figure 1: A simple fixpoint computation algorithm.
2.3 Fixpoints
Propagators are monotone narrowing operators, thus we may consider the problem of identifying the greatest common fixpoint of a set of propagators.
Definition 4 (Greatest Common Fixpoint) The greatest common fixpoint gfp(F ) of a
set of propagators F is the largest Cartesian product of intervals A ⊆ D such that for each
operator f ∈ F , we have f (A) = A.
There are two computational problems related to fixpoints:
• Function Problem: Effectively compute gfp(F );
• Decision Problem: Decide whether gfp(F ) 6= ∅, i.e., whether there exists a (nonempty) fixpoint. (Note that our definition of propagators implies that f (∅) = ∅ for all
f ∈ F . Therefore ∅ is always a common fixpoint.) In other words: do the propagators
stabilize to non-empty domains?
As often in complexity work we mostly focus on the Decision problem in this paper.
The reason is that the basic complexity classes (NP in particular) are defined for decision
problems, and that hardness results on the decision problem also imply that the function
problem is hard. The only place where we refer to the function problem is this section,
where we describe the basic greatest fixpoint computation algorithm.
An algorithm for computing gfp(F ) is specified in Fig. 1. It is presented in its simplest
form, which excludes several possible optimizations related, in particular, to the fact that
not all constraints necessarily deal with all variables. (These optimizations are well-known
and orthogonal to our discussion in this paper.) In this algorithm we initialize the Cartesian
+
product of domains to D, in other words we initially have x−
i = li and xi = ui , for
all i ∈ 1..n; and we simply apply all propagators until a stable state is reached, i.e., no
propagator shrinks any domain further. The reader can verify that this algorithm specifies
formally the reasoning that we presented informally in our introductory example (Sec. 1.1).
662

The Complexity of Integer Bound Propagation

2.4 Complexity Upper Bound of Fixpoint Propagation
The worst-case time upper bound of fixpoint computation can be analyzed as follows2 .
Let p = |F | be the number of propagators. (Note that we have in general one or more
propagators per constraint, i.e., p ≥ m.) We enter the while loop at most nd times since
at every new iteration we must reduce at least one bound by one unit, and each time the
foreach loop is entered at most p times. Overall the algorithm therefore terminates after a
number of propagator applications of:
O(npd).
In other words, it is in fact exponential in the number of bits of the encoding: this complexity
can be written O(np·2b ), where b is the number of bits of the bound encoding. This is despite
the fact that each propagator is polynomial in the size of the encoding. Such algorithms are
called pseudo-polynomial. In contrast algorithms that are truly polynomial in the number
of bits of the encoding, i.e., whose worst-case time complexity is O(π(n, m, log d)), for
some polynomial π, are called strongly polynomial (Papadimitiou, 1994). The problem of a
pseudo-polynomial algorithm for this problem is that it scales linearly with the size of the
domains, which may themselves be exponentially large. Since the propagators we consider
take strongly polynomial time, the analyis of the upper bound is summarized as follows:
Observation 1 The naive fixpoint computation algorithm (Fig. 1) always terminates in
pseudo-polynomial-time.
The question is whether strongly polynomial algorithms exist. The rest of the paper
focusses on this question, for several classes of propagators.

3. Linear Constraints
In this section we consider linear inequalities, i.e., our set of constraints C contains m
inequalities of the form:
X
ai,k xi ≥ ck ,
k ∈ 1...m
(2)
i∈1...n

where each ck and ai,k are integers. It is convenient to introduce some extra notation: we
denote by si,k the sign of the ith term in constraint k, i.e.,:
si,k =

(

+ if ai,k ≥ 0
− if ai,k < 0

(3)

Moreover, given a sign s ∈ {−, +}, the sign −s is defined as + if s = − and as −
−s
otherwise. The sign +s will simply denote s. With this notation the terms ai,k xi i,k and
+s
+
ai,k xi i,k simply represent the smallest and largest elements of the set {ai,k v | v ∈ [x−
i , xi ]}.
2. Few papers give explicit upper bounds on the complexity of computing a fixpoint of a set of bound
consistency propagators. The earliest reference we are aware of is the work of Lhomme (1993); it
considers constraints on the reals but assumes finite precision (floating points), and its analysis directly
adapts to discrete intervals.

663

Bordeaux, Katsirelos, Narodytska, & Vardi

3.1 Bound(R) Consistency Propagators for Linear Inequalities
We briefly summarize the material we need on bound(R) consistency in the case of linear
inequalities. We refer the reader to the literature for more details, in particular the papers
by Harvey and Stuckey (2003), Schulte and Carlsson (2006), Bessière (2006), and Apt and
Zoeteweij (2007) have substantial material on bound(R) consistency and linear constraints.
Also of interest are works that show how to improve bound propagation for long linear
constraints (Harvey & Schimpf, 2002; Katriel, Sellmann, Upfal, & Van Hentenryck, 2007).
−s
Consider a variable xi . Its bound xi i,k is bound(R) inconsistent w.r.t. the kth inequality
of the system iff: even when we fix the other terms to their maximum, we obtain something
lower than ck . It is bound consistent if the opposite is true i.e., iff:
+s1,k

a1,k x1

+s

−si,k

+ . . . + ai−1,k xi−1i−1,k + ai,k xi

+sn,k

+s

+ ai+1,k xi+1i+1,k + . . . + an,k xn

≥ ck

(4)

We call this the bound consistency inequality of variable xi w.r.t. constraint k. The
bound consistency propagator for a linear inequality simply shrinks the bounds of each
variable xi . Let:
X
+s
aj,k xj j,k
qi,k = ck −
j∈[1,n], j6=i

−s

be the minimal quantity that has to be reached by ai,k xi i,k to satisfy the bound consistency
−s
inequality (in other words: xi is bound consistent w.r.t. constraint k iff ai,k xi i,k ≥ qi,k ).
The (bound(R) consistency) propagator associated with constraint k ∈ 1..m and variable
i ∈ 1..n is the function that reduces the bound of xi to the closest bound consistent value.
It is defined by the following pseudo-code:
if ai,k > 0 then x−
:=
i
Li,k :
if ai,k < 0 then

x+
i

:=



l



j

max x−
i ,
min

x+
i ,

qi,k
ai,k

qi,k
ai,k

m

(5)

k

(The propagator does nothing if ai,k = 0.)
3.2 NP-completeness of Integer Fixpoint Computation
We now prove that the propagators Li,k introduced in the previous sub-section (Eq. 5),
although very simple when considered independently, give rise to complex fixpoints. More
precisely, we show the NP-completeness of the following decision problem:
Decision Problem 1 (Bound(R)-Consistency for Linear Constraints)
INPUT: a CSP whose set of constraints C are linear inequalities.
QUESTION: Let F = {Li,k : i ∈ 1..n, k ∈ 1..m} be the set of bound(R) consistency propagators associated with the CSP. Do the propagators in F have a non-empty common fixpoint?
3.2.1 Characterising the Fixpoints by Inequalities
Our first observation is that the bounds obtained when a fixpoint is reached are characterized
by the bound consistency conditions of Eq. 4. In other words a fixpoint is reached iff the
664

The Complexity of Integer Bound Propagation

+
lower and upper bounds x−
i and xi satisfy the following inequalities, for each variable i and
constraint k:


+s
+s1,k
+s
−s
+s


+ .. + ai−1,k xi−1i−1,k + ai,k xi i,k + ai+1,k xi+1i+1,k + .. + an,k xn n,k ≥ ck
 a1,k x1




li ≤

x−
i

≤

x+
i

∀k ∈ 1 . . . m, i ∈ 1 . . . n
∀i ∈ 1 . . . n

≤ ui

(6)

It is clear that Decision Problem 1 is answered positively iff there are integer values for
+
the bounds x−
i and xi , i ∈ 1..n, that satisfy the Linear Program 6. (If a fixpoint exists then
the bounds given by this fixpoint satisfy the inequalities and are within the initial bounds
li , ui . Conversely if the inequalities are satisfied we have a fixpoint.)
A first consequence for Decision Problem 1 is that its membership in NP is straightforward since it is solvable by Integer Programming.
3.2.2 Linear Inequalities with Two-Variables-Per-inequality
The key to understanding why Decision Problem 1 is hard is to connect fixpoint computation
to the special case of Integer (Linear) Programming where all constraints have Two Variables
Per Inequality (TVPI in the LP terminology, see Bar-Yehuda & Rawitz, 2001):
Definition 5 A TVPI instance with m constraints and n variables is an Integer Linear
Program of the following form:
(

ak xik + bk xjk ≥ ck ∀k ∈ 1 . . . m
li ≤ xi ≤ ui
∀i ∈ 1 . . . n

where a, b, c are vectors of arbitrary (possibly negative) integers.
The feasibility of TVPI constraints is NP-complete3 but can be decided in pseudopolynomial time. An early pseudo-polynomial time algorithm can be found in work by
Aspvall and Shiloach (1980); this algorithm essentially reduces the problem to a 2-SAT
instance of size m · d, which is solvable in linear time (the overall algorithm therefore
runs in pseudo-polynomial time, but also with a pseudo-polynomial space requirement).
A particularly relevant algorithm for TVPI constraints is proposed in the work of BarYehuda and Rawitz (2001). This algorithm has pseudo-polynomial time complexity with
low, strongly polynomial space requirements. Interestingly, this algorithm essentially uses
bound propagation (in fact, precisely bound(R) consistency), and embeds it in what amounts
to a backtrack-free search with a “parallel” improvement that allows to amortize its overall
runtime.
This seems to suggest a strong relation between propagation and TVPI constraints; in
particular one could easily be mistaken to believe that propagation is a decision procedure
for systems of TVPI constraints. We say that propagation provides a decision procedure for
a class of constraints if propagation fails exactly when the constraints are unsatisfiable (in
3. Here we focus on feasibility only. The optimization problem, i.e., optimizing a linear function under
TVPI constraints, is strongly NP-hard, i.e., NP-hard even for bounded domain sizes (in fact domains
{0, 1} are enough), because it trivially encodes Max-2SAT (Bar-Yehuda & Rawitz, 2001).

665

Bordeaux, Katsirelos, Narodytska, & Vardi

other words: the existence of a bound consistent state suffices to guarantee the existence
of a solution). This is the usual condition that guarantees a backtrack-free search; but
propagation rarely achieves this in the general case and it is in fact not a decision procedure
for TVPI constraints:
Example 2 Consider the problem x + y = 1, x = y with x, y ∈ [0, 1]. The problem is
inconsistent yet it is bound(R) consistent (and also, in fact, bound(Z) consistent).
To prove our main result we need to identify a restricted case of TVPI constraints
for which fixpoint computation is indeed a decision procedure. This particular case is
monotone TVPI constraints, in which the two variables in each inequality have coefficients
with opposite signs, i.e., the problem is of the following form:
Definition 6 A monotone TVPI instance with m constraints and n variables is an Integer Linear Program of the following form:
(

ak xik − bk xjk ≥ ck ∀k ∈ 1 . . . m
li ≤ xi ≤ ui
∀i ∈ 1 . . . n

where ak ≥ 0, bk ≥ 0, ∀k ∈ 1 . . . m.
We can now prove our NP-hardness result from monotone TVPI constraints, using the
following result:
Theorem 1 (Lagarias, 1985) The feasibility of Two-Variable-Per-Inequality monotone Integer Programming is NP-complete.
3.2.3 NP-hardness
We now prove that Decision Problem 1 is NP-hard. We already know that it is in NP,
therefore we can state our main result for bound(R) consistency for linear constraints as:
Proposition 1 Decision problem 1 is NP-complete.
Proof. We show that fixpoint computation decides systems of monotone TVPI constraints.
Consider a monotone TVPI instance Q of the form given by Def. 6. We want to show
the equivalence: Q has an integer solution iff the set of bound(R) consistency propagators
obtained for Q have a non-empty common fixpoint.
• “Q has an integer solution” means that there exist integer values vi for each variable
xi satisfying li ≤ vi ≤ ui and, ∀k ∈ 1 . . . m:
ak vik − bk vjk ≥ ck

(7)

+
• “There exists a common fixpoint” means that bounds x−
i , xi , can be found for all
+
−
i ∈ 1 . . . n, satisfying li ≤ xi ≤ xi ≤ ui and, ∀k ∈ 1 . . . m:
−
ak x −
ik − bk xjk ≥ ck

+
ak x +
ik − bk xjk ≥ ck

(8)

(These are simply the constraints of Eq. 6 for variable xik (left) and xjk (right),
rewritten by taking into account a > 0, b > 0.)
666

The Complexity of Integer Bound Propagation

We prove the two directions of the iff:
• Consider an integer solution to Q in which each variable xi takes value vi . It is easy
+
−
+
to verify that the bounds x−
i = xi = vi satisfy li ≤ xi ≤ xi ≤ ui and Eq. 8.
+
• Consider a bound consistent state described by the bounds x−
i , xi . It is easy to verify
that the solution v defined by vi = x+
i , i ∈ 1 . . . n satisfies li ≤ vi ≤ ui and Eq. 7.

This means that we can reduce the problem of monotone TVPI feasibility to the existence
of a fixpoint, and that Decision Problem 1 is therefore NP-hard. 2
Note that the NP-hardness result for Decision Problem 1 holds even for (monotone)
TVPI constraints, while the pseudo-polynomial upper bound of Section 2.4 holds for general
linear constraints of unbounded sizes; as said earlier the membership in NP is also valid for
general linear constraints.
3.3 A Comment on Linear Equalities
From the beginning of this section we have focused on linear inequalities for reasons that
should become clear in this sub-section. Readers may wonder whether considering equalities
would make any difference. The short answer is no.
P
A first observation is that an inequality i∈1...n ai xi ≥ c can directly be encoded into the
P
P
equality i∈1...n ai xi − y = c where y is a new variable ranging over [0, u] for u > i ai xi+si ,
so that there is a bijection between solutions of the two constraints. Therefore the problem
of propagating inequalities reduces to the problem of propagating equalities, and the NPcompleteness result still holds for problems whose linear constraints are all equalities (or
for any mix of equalities / inequalities).
A second observation is the following: because we focus on bound(R) consistency, the
P
propagation obtained for an equality i∈1...n ai xi = c is the same as the one obtained using
P
P
two constraints i∈1...n ai xi ≤ c and i∈1...n ai xi ≥ c. For this reason it is convenient
to assume that constraints are of homogeneous form, and to restrict ourselves to linear
inequalities4 .
3.4 Tractable Classes of Linear Constraints
Intractable problems often become tractable when additional restrictions are imposed on
the topology of the constraint graph, or on the constraints themselves. In this subsection
we identify one significant class of linear constraints that can be propagated in strongly
polynomial time, based on a restriction on the coefficients of the constraints.
Our initial observation is that one source of complexity of the propagators Li,k of Eq.
5 is that they use rounding: when we update a variable’s bounds, we obtain from the other
variables a real value that is rounded upwards for lower bounds and downwards for upper
bounds. The effects of rounding were noticed by previous authors and used to optimize
4. Note also that in the case of inequalities the propagators for bound(Z) consistency are the same as for
bound(R) consistency. Since we want propagators to be polynomial-time computable, the case we want
to avoid however is bound(Z) consistency for linear equalities, where we cannot define polynomial-time
computable propagators unless P=NP.

667

Bordeaux, Katsirelos, Narodytska, & Vardi

propagation (Harvey & Stuckey, 2003). Rounding effectively means that propagation stabilizes to integral solutions of Linear Programs. The Linear Programs in question have a very
specific form, but the intractability is due to the integrality. Therefore in this sub-section
(1) we observe that if we remove the rounding, the problem becomes tractable; (2) we use
this observation to show that if the coefficients are unit (i.e., belong to {−1, 0, +1}), there
is effectively no rounding, which means that the same tractability result holds.
3.4.1 Linear Propagators without Rounding
We now consider operators similar to those of Eq. 5 but without rounding, in other words
we now associate to the linear constraints the following operators:
if ai,k > 0 then x−
:=
i



Si,k :
if ai,k < 0 then

x+
i

:=

q

i,k
max x−
i , ai,k

min



qi,k
x+
i , ai,k



(9)



Even when the initial bounds are integers as assumed throughout this paper, these
operators will in general reduce these bounds to real-values. Note that such propagators
can effectively be used to deal with variables with a real-valued domain, indeed they are used
both in the Constraint Programming community (Behamou & Granvilliers, 2006) and in the
Operations Research community, where a different terminology is used (Feasibility-Based
Bounds Tightening, see e.g. Belotti, Cafieri, Lee & Liberti, 2010).
The decision problem we focus on is now whether there exist real-valued bounds that
are a fixpoint. We note that this problem is tractable; a similar result has been reported
independently in the work of Belotti, Cafieri, Lee, and Liberti (2010).
Decision Problem 2 (Fixpoint of Continuous Linear Propagators)
INPUT: a CSP whose set of constraints C are linear inequalities.
QUESTION: Does the set of real-valued propagators F = {Si,k : i ∈ 1..n, k ∈ 1..m} associated to C have a common fixpoint?
Observation 2 Decision problem 2 can be decided by Linear Programming.
It is easy to see that the fixpoints of operators Si,k are exactly the real-valued solutions to
the system of linear constraints of Eq. 6. Note that we have been careful in the statement
of Observation 2: whether Linear Programming is strongly polynomial is in fact a longstanding open question (Smale, 1998). The best “polynomial-time” LP algorithms are,
encouragingly, of time complexity O(π(n, m, b)) for some polynomial π, where b is the
number of bits of the number encoding—this looks strongly polynomial (Khachian, 1979).
But there is a catch: the complexity is counted in number of operations, and operations
on the rationals can in principle expand the size of the numbers (repeated multiplications
can blow-up the representation exponentially). However, for practical purposes, typical LP
implementations prevent the blow-up of number representation by limiting the precision
to b bits throughout the execution; solvability by Linear Programming is widely regarded
as synonymous to strong tractability, and provably sub-exponential LP algorithms exist
(Matousek, Sharir, & Welzl, 1996). In other words, Observation 2 should really be read as
a carefully phrased way to say that Problem 2 is efficiently solvable in practice.
668

The Complexity of Integer Bound Propagation

3.4.2 Linear Constraints with Unit Coefficients
A unit linear constraint is of the usual form i∈1...n ai,k xi ≥ ck but with the additional
restriction that each coefficient ai,k is chosen in {−1, 0, +1}. Our introductory example of
slow convergence (Eq. 1) was a (particularly simple) example of unit linear constraints, and
the slow convergence could in this particular case be avoided. Note that we are considering
linear unit constraints of any number of variables. A special case of unit constraints that
have been widely studied is the class of unit-TVPI constraints (i.e., both unit and TVPI).
This is perhaps the most important class of linear constraints whose integer feasibility can
be solved in strongly polynomial time, see for instance work by Jaffar et al. (1994).
P

Proposition 2 When all constraints have unit coefficients, Decision Problem 1 can be
decided by Linear Programming.
Proof. The LP is, of course, of the form given in Eq. 6. The observation is, in short,
that no rounding is needed when the coefficients are unit.
T
More precisely, for any Cartesian product of intervals A, let L(A) = i,k Li,k (A) and
T
S(A) = i,k Si,k (A). We show that when all coefficients are unit and when (as defined) the
bounds of the initial Cartesian product D are integral, then we have Lt (D) = S t (D), for
all t ≥ 0. We first note that the bounds of Lt (D) are integral for all t since the original
state D has integral bounds and that each operator in L applies rounding. The equality
Lt (D) = S t (D) is now proved by induction on t. For t = 0, Lt (D) = S t (D) = D. If
the induction hypothesis holds at step t, then S t+1 (D) = S(S t (D)) = S(Lt (D)). Now
S t+1 (D) = L(Lt (D)) = Lt+1 (D) because Lt (D) has integral bounds, hence applying S or L
on this Cartesian product gives the same result. (In Eq. 5 all qi,k s are integral in this case
and all ai,k s are unit therefore the division qi,k /ai,k gives an integer, which means that the
propagators Li,k with rounding return the same result as the non-rounded propagators Si,k
of Eq. 9.)
Now having Lt (D) = S t (D), for all t ≥ 0 it is easy to see that gfp{Li,k } = gfp{Si,k }.
Because the domains are finite Lt (D) stabilizes for a finite t. For this particular t, Lt (D) is
the greatest fixpoint of L and the same greatest Cartesian product S t (D) is also the greatest
fixpoint of S. 2
Note that in general Linear Programming does not necessarily find integer solutions to
the system of Eq. 6; what the result shows is that LP will find a solution iff an integer one
+
exists. If we want to actually compute the largest consistent bounds x−
i and xi of a certain
−
+
variable xi , we can simply minimize xi or maximize xi under the constraints of Eq. 6.
The previous proof shows that these extremal values are integral.
3.4.3 Are There Other Tractable Cases?
It is interesting to consider whether other properties make the propagation solvable in
strongly polynomial time. With respect to restrictions on the constraint graph, there are
nevertheless reasons to be pessimistic: we note that the feasibility of monotone TVPI Integer
Programming remains NP-complete under strict restrictions on the constraint graph, as
shown in the work of Hochbaum and Naor (1994). This suggests that such restrictions are
unlikely to lead to interesting tractable classes of fixpoint computation.
669

Bordeaux, Katsirelos, Narodytska, & Vardi

Regarding the restrictions on coefficients, we note that in general the NP-completeness
of (monotone) TVPI constraints assumes that the coefficients ak , bk , ck are arbitrary. The
Unit restriction imposes, on the contrary, the strongest restriction on coefficients: that their
absolute value be ≤ 1. If we impose a more general bound β on these absolute values then
one may wonder whether the problem exhibits some form of fixed-parameter tractability.
We leave this question open for future work.

4. Generalizations and Non-Linear Constraints
By Proposition 1, fixpoint computation for numerical constraints as basic and common
as linear constraints is intractable. Several cases of non-linear constraints are nevertheless of interest. First, we show that if the simplest possible type of polynomials (a single
squaring operation) is added to linear constraints, then our general hardness result can be
strengthened. Second, it is interesting to note that if we enrich unit linear constraints with
simple min or max constraints, then fixpoint computation is equivalent to a puzzling open
problem discussed recently in the theorem-proving literature. Last, we briefly comment on
connections between our results and the tractability of max-closed constraints.
4.1 Quadratic Constraints
For the purposes of this section it is sufficient to enrich our linear constraint language
(constraints of the form given by Eq. 2) with squaring constraints of the form:
xi = x2j
It is also sufficient to restrict ourselves to non-negative values for variables xi and xj , i.e.,
0 ≤ li ≤ ui and 0 ≤ lj ≤ uj . In this setting the bound(R) consistency propagators are
defined by the following instructions:


−
−
x−
i := max(xi , xj

x−
j

:=

max(x−
j ,

2

q

x−
i



)

+
+
x+
i := min(xi , xj



x+
j

)

:=

min(x+
j ,

q

2

x+
i

)



)

In other words the fixpoints are integer solutions to the following bound consistency
inequalities:


−
x−
i ≥ xj

2

;



+
x+
i ≤ xj

2

;

x−
j ≥

q

x−
i ;

x+
j ≤

q

x+
i

(10)

When these simple quadratic constraints are added to the language of linear constraints,
our NP-completeness result can be strengthened: the problem is NP-complete even when
considering a bounded(!) number of variables and constraints; in fact one TVPI constraint
and one squaring constraint. This is due to the fact that fixpoint computation converges to
a state that encodes a complex number-theoretic problem.
Proposition 3 Given a CSP with 3 variables and 2 constraints a1 x1 + a2 x2 = c, x1 = x23 ,
determining whether their associated bound(R) consistency propagators have a fixpoint is
NP-complete.
670

The Complexity of Integer Bound Propagation

Proof. Membership in NP is straightforward. We show the hardness result for the special
case where ai ≥ 0, i ∈ {1, 2, 3} and focus, as said, on positive intervals. We first note
that the bound consistency inequalities of (Eq. 10) for the squaring constraint x1 = x23 are
− 2
+
+ 2
satisfied iff x−
1 = (x3 ) and x1 = (x3 ) since we focus on integer bounds. (This property of
the squaring propagator is noticed in a slightly different form in Schulte & Stuckey, 2005).
From a propagation viewpoint the equality a1 x1 + a2 x2 = c is seen as two inequalities
a1 x1 + a2 x2 ≥ c and −a1 x1 − a2 x2 ≥ c whose bound consistent inequalities (Eq. 4) are
−
+
+
effectively satisfied iff a1 x−
1 + a2 x2 = c and a1 x1 + a2 x2 = c.
We rely on a theorem (Manders & Adleman, 1978) which shows that deciding whether
an equation of the form a1 x23 + a2 x2 = c has integer solutions, where a1 , a2 and c are
non-negative integers, is NP-complete. We reduce this problem to the existence of bound
consistent bounds for the conjunction a1 x1 + a2 x2 = c, x1 = x23 with initial bounds l1 = l2 =
l3 = 0 and u1 = u2 = u3 = c. We just need to show that fixpoint computation is complete
for this system—a bound consistent state is found iff the original equation has a solution:
• If the original equation has a solution, i.e., a pair of non-negative integer values
−
+
+
2
hv2 , v3 i satisfying a1 v32 + a2 v2 = c, then we define x−
1 = x1 = v3 , x2 = x2 = v2 , and
−
+
−
+
x3 = x3 = v3 . These bounds are such that 0 ≤ xi ≤ xi ≤ c and satisfy the bound
consistency conditions of Eq. 10 and Eq. 4.

2

+
• If the conjunction has a bound consistent state, i.e., bounds x−
i , xi such that Eq. 10
−
and Eq. 4 are satisfied, then the solution v defined by v2 = x+
2 and v3 = x3 satisfies
2
the original equation a1 v3 + a2 v2 = c.

4.2 Connections to the Max-Atom Problem
Another common type of primitive non-linear constraints is of the form:
xh = max(xi , xj )
The bound(R) consistency propagators for this constraint are the following (Schulte &
Stuckey, 2005):
+ +
x+
i := min(xi , xh )

− − −
x−
h := max(xh , xi , xj )

+ +
+
+ +
+
x+
h := min(xh , max(xi , xj )) xj := min(xj , xh )

(In fact to strictly reach bound(R) consistency one would need to additionally check
whether the bounds of xh have an empty intersection with the bounds of one of the max-ed
variables, say xi , in which case we can essentially impose the constraint xj = xh ; for the
purposes of this section the simpler formulation above is equivalent.) In other words the
fixpoints are characterized by the following inequalities:
+ +
x+
h ≤ max(xi , xj )

+
x+
i ≤ xh

+
x+
j ≤ xh

−
x−
h ≥ xi

−
x−
h ≥ xj

The fixpoint computation of max constraints mixed with unit linear constraints is interesting
because its complexity is an open problem. Note that there is no rounding or use of
671

Bordeaux, Katsirelos, Narodytska, & Vardi

coefficients in the definition of the bound consistency inequalities, therefore the complexity
arising from rounding in all our NP-complete variants of propagation does not arise here.
The open problem we connect to is called Max-Atom in the work of Bezem, Nieuwenhuis,
and Rodrı́guez-Carbonell (2008); see this reference for prior problems of interest that are
shown equivalent to Max-Atom. A max-atom constraint is of the form: max(xi , xj )+c ≥ xh .
The work reported by Bezem et al. (2008) shows a number of results on the feasibility of
conjunctions of max-atom constraints: (1) There is no significant complexity difference
between integer and real feasibility; (2) The problem can be decided in pseudo-polynomial
time using what amounts to a fixpoint computation algorithm; (3) The problem has short
proofs of unsatisfiability and is therefore in NP∩coNP; which means that it is of a very
different nature from our other NP-complete variants. In fact, a recent result (Atserias &
Maneva, 2010) shows that the complexity of Max-Atom is equivalent to well-known open
problems called mean-payoff games, which have in turn connections to some important open
questions in model-checking: parity games, a class of games reducible to mean-payoff games,
are equivalent to the model-checking problem of µ-calculus (Emerson, Jutla, & Sistla, 1993;
Jurdzinski, 1998).
Here we draw a simple connection that follows from the observation that the bound
+ +
consistency inequalities for the upper bounds include the constraint x+
h ≤ max(xi , xj )
which encode max-atom constraints almost directly.
Proposition 4 Bound(R) consistency for a combination of unit linear and max constraints
can be solved in polynomial time only if Max-Atom can be also be solved in polynomial time.
Proof. To reduce a Max-Atom instance with variables xi , i ∈ 1 . . . n and m constraints to a
fixpoint computation problem we simply introduce one fresh variable yk , for each k ∈ 1 . . . m.
Let the kth constraint be of the form max(xik , xjk ) + ck ≥ xhk , it rewrites to the conjunction
max(xik , xjk ) = yk , yk + ck ≥ xhk . The lower bounds of all variables are fixed to 0 and the
P
upper bounds need only be set to k∈1...m ck by the small model property (Lemma 2) of the
paper by Bezem et al. (2008). The bound consistency equations for the upper bounds directly
encode the problem. 2

4.3 Max-Closed Constraints
We last note a connection between our results and the class of max-closed constraints
introduced by Jeavons and Cooper (1995) (more on this in, e.g., Petke & Jeavons, 2009).
A constraint R(x1 , . . . , xn ) is max-closed if whenever we have two solutions hv1 . . . vn i and
hw1 . . . wn i, their maximum defined as hmax(v1 , w1 ), . . . , max(vn , wn )i is also a solution.
Results by Jeavons and Cooper (1995) show that max-closed constraints are tractable: if
a system of constraints is max-closed, then its feasibility can be determined in polynomial
time. However note that this result essentially assumes an explicit (or table) representation
of the constraint, i.e., it is assumed that each constraint is defined by explicitly listing the
tuples that are solutions to it. In contrast some important types of constraints such as the
numerical constraints considered in this paper are implicitly defined: we do not know the
list of solutions to R(x1 , . . . , xn ) but can only verify efficiently whether a particular tuple
is accepted by the constraint.
672

The Complexity of Integer Bound Propagation

Implicitly-defined max-closed constraints played an important role in this paper: both
the monotone TVPI constraints, considered in Section 3.2 and the Max-atom constraints
considered in Section 4.2, are max-closed, as shown respectively by Hochbaum and Naor
(1994), and Bezem et al. (2008). In sharp contrast to the case of explicitly-defined constraints, the resolution of implicitly-defined max-closed constraints is therefore only pseudopolynomial and it is in fact intractable, as shown by the special case of monotone TVPI
constraints:
Observation 3 The feasibility of ”implicitly-defined” max-closed constraints is NP-complete.
As shown in Section 3.2 with the particular example of monotone TVPI constraints,
even the fixpoint computation of implicitly defined max-closed constraints is, in fact, NPcomplete in general.

5. Conclusion
Reasoning about intervals was introduced in the AI literature by the works of Cleary (1987),
and Davis (1987)5 . A substantial body of AI work has ensued (see, e.g. Hyvönen, 1992);
bound computation is now used by most finite-domain CP solvers (Schulte & Carlsson,
2006).
In this paper we have theoretically investigated the complexity of computing the common
fixpoint of a set of bound consistency propagators. We have shown that even when the
propagators are themselves very simple, the fixpoint computation used in these algorithms
can be complex, it is indeed NP-complete even for a very restricted constraint class – linear
monotone inequalities with two variables per inequality. We also considered some special
classes of constraints, like quadratic constraints and max constraints. Finally, we identified a
class of constraints, namely, linear inequalities with unit coefficients, that allows a tractable
fixpoint computation algorithm.
Bound propagation is a successful and widely used technique in Constraint Programing.
There is a large literature on propagating single constraints (Van Hoeve & Katriel, 2006;
Bessière, 2006; Rossi, van Beek, & Walsh, 2006) and it is perhaps a surprise that no prior
study exists on the complexity of the fixpoint computation. The NP-completeness of fixpoint
computation for simple types of constraints is a fundamental and somewhat surprising
result, and one that sheds light on slow convergence phenomena.
This result also “puts bound propagation on the map” of AI computational problems:
together with knapsack constraints and some forms of learning in neural nets (Schaeffer &
Yannakakis, 1991), it is one of the few important AI problems we are aware of that have a
pseudo-polynomial complexity and yet are intractable.
Acknowledgments
Preliminary results of Bordeaux, Hamadi, and Vardi (2007) showed the NP-completeness
of propagation in the case where quadratic constraints are considered (Prop. 3). This
5. As often, a good case can be made that similar ideas are already present in earlier work, in particular the
work of Laurière (1978). Interval computations are of course also used in other areas and the fixpoint
computation methods we consider relate to the broader theme of interval arithmetic pioneered by Moore
(1966).

673

Bordeaux, Katsirelos, Narodytska, & Vardi

paper is a thoroughly revised version whose central result for linear constraints is new
and more general. Part of this work was done while G. Katsirelos, N. Narodytska and
M. Vardi were visiting Microsoft Research, Cambridge. Part of this work was done while
G. Katsirelos was employed by NICTA, Australia. NICTA is funded by the Australian
Governments Department of Broadband, Communications, and the Digital Economy and
the Australian Research Council. This work was partially supported by the ANR UNLOC
project: ANR 08-BLAN-0289-01. Discussions with Youssef Hamadi and Claude-Guy Quimper are gratefully acknowledged. Thanks also to the anonymous reviewers whose feedback
helped improve the paper.

References
Apt, K. R. (1999). The essence of constraint propagation. Theoretical Computer Science
(TCS), 221 (1-2), 179–210.
Apt, K. R., & Zoeteweij, P. (2007). An analysis of arithmetic constraints on integer intervals.
Constraints, 12 (4), 429–468.
Aspvall, B., & Shiloach, Y. (1980). A polynomial time algorithm for solving systems of
linear inequalities with two variables per inequality. SIAM J. on Computing, 9 (4),
827–845.
Atserias, A., & Maneva, E. (2010). Mean-payoff games and the max-atom problem. Tech.
rep., Universitat Politecnica de Catalunya.
Bar-Yehuda, R., & Rawitz, D. (2001). Efficient algorithms for integer programs with two
variables per constraint. Algorithmica, 29 (4), 595–609.
Behamou, F., & Granvilliers, L. (2006). Continuous and interval constraints. In Rossi, F.,
van Beek, P., & Walsh, T. (Eds.), Handbook of Constraint Programming, chap. 16.
Elsevier.
Belotti, P., Cafieri, S., Lee, J., & Liberti, L. (2010). Feasibility-based bounds tightening via
fixed points. In Proc. of Int. Conf. on Combinatorial Optimization and Applications
(COCOA), p. To Appear.
Benhamou, F. (1996). Heterogeneous constraint solving. In Proc. of Int. Conf. on Algebraic
and Logic Programming (ALP), pp. 62–76.
Bessière, C. (2006). Constraint propagation. In Rossi, F., van Beek, P., & Walsh, T. (Eds.),
Handbook of Constraint Programming, chap. 3. Elsevier.
Bezem, M., Nieuwenhuis, R., & Rodrı́guez-Carbonell, E. (2008). The max-atom problem
and its relevance. In Proc. of Int. Conf. on Logic for Programming, Artificial Intelligence and Reasoning (LPAR), pp. 47–61.
Bordeaux, L., Hamadi, Y., & Vardi, M. Y. (2007). An analysis of slow convergence in
interval propagation. In Proc. of Int. Conf. on Principles and Practice of Constraint
Programming (CP), pp. 790–797.
674

The Complexity of Integer Bound Propagation

Choi, C. W., Harvey, W., Lee, J. H. M., & Stuckey, P. J. (2006). Finite domain bounds
consistency revisited. In Australian Conf. on Artificial Intelligence, pp. 49–58.
Cleary, J. G. (1987). Logical arithmetic. Future Computing Systems, 2 (2), 125–149.
Davis, E. (1987). Constraint propagation with interval labels. Artificial Intelligence, 32 (3),
281–331.
Emerson, E. A., Jutla, C. S., & Sistla, A. P. (1993). On model-checking for fragments of
µ-calculus. In Proc. of Int. Conf. on Computer-Aided Verification (CAV), pp. 385–396.
Harvey, W., & Schimpf, J. (2002). Bound consistency techniques for long linear constraints.
In Proc. of CP workshop on Techniques for Implementing Constraint Propgramming
Systems (TRICS).
Harvey, W., & Stuckey, P. J. (2003). Improving linear constraint propagation by changing
constraint representation. Constraints, 8 (2), 173–207.
Hochbaum, D. S., & Naor, J. (1994). Simple and fast algorithms for linear and integer
programs with two variables per inequality. SIAM J. on Computing, 23 (6), 1179–
1192.
Hyvönen, E. (1992). Constraint reasoning based on interval arithmetic: The tolerance
propagation approach. Artificial Intelligence, 58 (1-3), 71–112.
Jaffar, J., Maher, M. J., Stuckey, P. J., & Yap, R. H. C. (1994). Beyond finite domains.
In Proc. of Int. Workshop on Principles and Practice of Constraint Programming
(PPCP), pp. 86–94.
Jeavons, P., & Cooper, M. C. (1995). Tractable constraints on ordered domains. Artificial
Intelligence, 79 (2), 327–339.
Jurdzinski, M. (1998). Deciding the winner in parity games is in UP ∩ co-UP. Inf. Process.
Lett., 68 (3), 119–124.
Katriel, I., Sellmann, M., Upfal, E., & Van Hentenryck, P. (2007). Propagating knapsack
constraints in sublinear time. In Proc. of (North Amer.) Nat. Conf. on Artificial
Intelligence (AAAI), pp. 231–236.
Khachian, L. (1979). A polynomial algorithm for linear programming. Doklady Akad. USSR,
244, 1093–1096.
Lagarias, J. C. (1985). The computational complexity of simultaneous diophantine approximation problems. SIAM J. on Computing, 14 (1), 196–209.
Laurière, J.-L. (1978). A language and a program for stating and solving combinatorial
problems. Artificial Intelligence, 10 (1), 29–127.
Lebbah, Y., & Lhomme, O. (2002). Accelerating filtering techniques for numeric CSPs.
Artificial Intelligence, 139 (1), 109–132.
675

Bordeaux, Katsirelos, Narodytska, & Vardi

Leconte, M., & Berstel, B. (2006). Extending a CP solver with congruences as domains for
program verification. In CP Workshop on Software Testing, Verification and Analysis,
pp. 22–33.
Lhomme, O. (1993). Consistency techniques for numeric CSPs. In Proc. of Int. Joint. Conf.
on Artificial Intelligence (IJCAI), pp. 232–238.
Lhomme, O., Gottlieb, A., Rueher, M., & Taillibert, P. (1996). Boosting the interval
narrowing algorithm. In Proc.of Joint Int. Conf. and Symp. on Logic Programming
(JICSLP), pp. 378–392. MIT Press.
Manders, K. L., & Adleman, L. M. (1978). NP-complete decision problems for binary
quadratics. J. of Computer and System Sciences, 16 (2), 168–184.
Matousek, J., Sharir, M., & Welzl, E. (1996). A subexponential bound for linear programming. Algorithmica, 16 (4/5), 498–516.
Moore, R. E. (1966). Interval Analysis. Prentice-Hall.
Papadimitiou, C. (1994). Computational Complexity. Addison Wesley.
Petke, J., & Jeavons, P. (2009). Tractable benchmarks for constraint programming. Tech.
rep. CS-RR-09-07, Oxford University Computing Laboratory.
Rossi, F., van Beek, P., & Walsh, T. (2006). Handbook of Constraint Programming. Elsevier.
Schaeffer, A. A., & Yannakakis, M. (1991). Simple local search problems that are hard to
solve. SIAM J. on Computing, 20 (1), 56–87.
Schulte, C., & Carlsson, M. (2006). Finite domain constraint programming. In Rossi, F.,
van Beek, P., & Walsh, T. (Eds.), Handbook of Constraint Programming, chap. 14.
Elsevier.
Schulte, C., & Stuckey, P. J. (2005). When do bounds and domain propagation lead
to the same search space?. ACM Trans. on Programming Languages and Systems
(TOPLAS), 27 (3), 388–425.
Smale, S. (1998). Mathematical problems for the next century. Mathematical Intelligencer,
20, 7–15.
Trick, M. A. (2001). A dynamic programming approach for consistency and propagation for
knapsack constraints. In Proc. of Int. Conf. on Integration of AI and OR Techniques
in CP for Combinatorial Optimisation Problems (CP-AI-OR).
Van Hoeve, W.-J., & Katriel, I. (2006). Global constraints. In Rossi, F., Van Beek, P., &
Walsh, T. (Eds.), Handbook of Constraint Programming, chap. 6. Elsevier.
Yuanlin, Z., & Yap, R. H. C. (2000). Arc consistency on n-ary monotonic and linear constraints. In Proc. of Int. Conf. on Principles and Practice of Constraint Programming
(CP), pp. 470–483.

676

Journal of Artificial Intelligence Research 40 (2011) 143–174

Submitted 07/10; published 01/11

Automated Search for Impossibility Theorems
in Social Choice Theory: Ranking Sets of Objects
Christian Geist
Ulle Endriss

cgeist@gmx.net
ulle.endriss@uva.nl

Institute for Logic, Language and Computation
University of Amsterdam
Postbus 94242
1090 GE Amsterdam
The Netherlands

Abstract
We present a method for using standard techniques from satisfiability checking to automatically verify and discover theorems in an area of economic theory known as ranking
sets of objects. The key question in this area, which has important applications in social
choice theory and decision making under uncertainty, is how to extend an agent’s preferences over a number of objects to a preference relation over nonempty sets of such objects.
Certain combinations of seemingly natural principles for this kind of preference extension
can result in logical inconsistencies, which has led to a number of important impossibility
theorems. We first prove a general result that shows that for a wide range of such principles, characterised by their syntactic form when expressed in a many-sorted first-order
logic, any impossibility exhibited at a fixed (small) domain size will necessarily extend to
the general case. We then show how to formulate candidates for impossibility theorems at
a fixed domain size in propositional logic, which in turn enables us to automatically search
for (general) impossibility theorems using a SAT solver. When applied to a space of 20
principles for preference extension familiar from the literature, this method yields a total
of 84 impossibility theorems, including both known and nontrivial new results.

1. Introduction
The area of economic theory known as ranking sets of objects (Barberà, Bossert, & Pattanaik, 2004; Kannai & Peleg, 1984) addresses the question of how to extend a preference relation defined over certain individual objects to a preference relation defined over nonempty
sets of those objects. This question has important applications. For instance, if an agent
is uncertain about the effects of two alternative actions, she may want to rank the relative
desirability of the two sets of possible outcomes corresponding to the two actions. In the
absence of a probability distribution over possible outcomes, the principles and methods
developed in the literature on ranking sets of objects can guide this kind of decision making under (“complete”) uncertainty (e.g., see Gravel, Marchant, & Sen, 2008; Ben Larbi,
Konieczny, & Marquis, 2010). A second example for applications is voting theory. When
we want to analyse the incentives of a voter to manipulate an election, i.e., to obtain a
better election outcome for herself by misrepresenting her true preferences on the ballot
sheet, we need to be able to reason about her preferences in case that election will produce
a tie and return a set of winners (e.g., see Gärdenfors, 1976; Duggan & Schwartz, 2000;
c
2011
AI Access Foundation. All rights reserved.

Geist & Endriss

Taylor, 2002). In both scenarios, decision making under uncertainty and the manipulation
of elections with sets of tied winners, agents are assumed to have preferences over simple
“objects” (states of the world, election outcomes) which they then need to extend to sets
of such objects to be able to use these extended preferences to guide their decisions.
One line of research has applied the axiomatic method, as practised in particular in social
choice theory (Gaertner, 2009), to the problem of ranking sets of objects and formulated
certain principles for extending preferences to set preferences as axioms. For instance, the
dominance axiom states that you should prefer set A ∪ {x} to set A whenever you prefer
the individual object x to any element in A (and A to A ∪ {x} in case x is worse than any
element in A); and the independence axiom states that if you prefer set A to set B, then
that preference should not get inverted when we add a new object x to both sets. Against
all intuition, Kannai and Peleg (1984) have shown that, for any domain with at least six
objects, it is impossible to rank sets of objects in a manner that satisfies both dominance
and independence. This is the original and seminal result in the field, and since 1984 a
small number of additional impossibility theorems have been established.
In this paper we develop a method to automatically search for impossibility theorems
like the Kannai-Peleg Theorem, to enable us to both verify the correctness of known results
and to discover new ones. There are a number of reasons why this is useful. First, the
ability to discover new theorems is clearly useful whenever those theorems are (potentially)
interesting. But also the verification of known results has its merits: verification can increase
our confidence in the correctness of a result (the manual proof of which may be tedious and
prone to errors); verification forces us to fully formalise the problem domain, which will
often result in a deeper understanding of its subtleties; and verification of theorems in new
fields (here the social and economic sciences) can help advance the discipline of automated
reasoning itself by providing new test cases and challenges.
This is not the first time that techniques from logic and automated reasoning have been
applied to modelling and verifying results from economic theory. Here we briefly review a
number of recent contributions applying such tools to social choice theory, which is closely
related to the problem domain we focus on in this paper. A lot of this work has concentrated
on Arrow’s Theorem, which establishes the impossibility of aggregating preferences of a
group of agents in a manner that satisfies certain seemingly natural principles (Arrow, 1963;
Gaertner, 2009). Ågotnes, van der Hoek, and Wooldridge (2010), for instance, introduce
a modal logic for modelling preferences and their aggregation, while Grandi and Endriss
(2009) show that Arrow’s Theorem is equivalent to the statement that a certain set of
sentences in classical first-order logic does not possess a finite model.
Besides formally modelling the problem domain and the theorem, there have also been
a number of attempts to automatically re-prove Arrow’s Theorem. One approach has been
to encode the individual steps of known proofs into higher-order logic and to then verify
the correctness of these proofs with a proof checker. Examples for this line of work are
the contributions of Wiedijk (2007), who has formalised a proof of Arrow’s Theorem in the
Mizar proof checker, and that of Nipkow (2009), who did the same using the Isabelle
system. A particularly interesting approach is due to Tang and Lin (2009). These authors
first prove two lemmas that reduce the general claim of Arrow’s Theorem to a statement
pertaining to the special case of just two agents and three alternatives. They then show
that this statement can be equivalently modelled as a (large) set of clauses in propositional
144

Automated Search for Impossibility Theorems in Social Choice Theory

logic. The inconsistency of this set of clauses can be verified using a SAT solver, which
in turn (together with the lemmas) proves the full theorem. Tang and Lin have been able
to extend their method to the verification of a number of other results in social choice
theory and they have also shown that their method can serve as a useful tool to support the
(semi-automatic) testing of hypotheses during the search for new results (Tang & Lin, 2009;
Tang, 2010).1 All of these contributions neatly fit under the broad heading of computational
social choice, the discipline concerned with the study of the computational aspects of social
choice, the application of computational techniques to problems in social choice theory, and
the integration of methods from social choice theory into AI and other areas of computer
science (Chevaleyre, Endriss, Lang, & Maudet, 2007).
Our starting point for developing a method for automatically proving impossibility theorems in the area of ranking sets of objects has been the work of Tang and Lin (2009). We
have adapted and extended their approach as follows. Rather than proving a new lemma
reducing a general impossibility to an impossibility in a small domain for each and every
theorem that we want to verify, our first contribution is a broadly applicable result, the
Preservation Theorem, which entails that for any combinations of axioms satisfying certain
syntactic conditions, any impossibility that can be established for a domain of (small) fixed
size n will unravel into a full impossibility theorem for all domains of size ≥ n. To be able
to formulate this result, we introduce a many-sorted first-order logic for expressing axioms
relating preferences over individual objects to preferences over sets of objects. We were
able to express most axioms from the literature in this language, which facilitates a fully
automated search for impossibility theorems within the space of these axioms.
We then show how impossibility theorems regarding the extension of preferences can be
modelled in propositional logic, provided the size of the domain is fixed. Given the Preservation Theorem, any inconsistency found with the help of a SAT solver now immediately
corresponds to a general impossibility theorem. We have implemented this kind of automated theorem search as a scheduling algorithm that exhaustively searches the space of all
potential impossibility theorems for a given set of axioms and a given critical domain size n.
Together with a number of heuristics for pruning the search space, this approach represents
a practical method for verifying existing and discovering new impossibility theorems.
Finally, we have applied our method to a search space defined by 20 of the most important preference extension axioms from the literature, and we have exhaustively searched
this space (of around one million possible combinations) for domains of up to eight objects.
This search has resulted in 84 (minimal) impossibility theorems. Each theorem states, for a
particular n ≤ 8 and a particular set of axioms ∆, that there exists no preference ordering
over nonempty sets of objects that satisfies all the axioms in ∆ when there are n or more
objects in the domain. These 84 theorems are minimal in the sense that no strict subset of
∆ would still result in an impossibility (at the given domain size n) and in the sense that
all the axioms in ∆ can be satisfied for any domain with fewer than n objects.
The 84 impossibility theorems found include known results (such as the Kannai-Peleg
Theorem), simple consequences of known results, as well as new and nontrivial theorems
that constitute relevant contributions to the literature on ranking sets of objects. One of
these is the impossibility of combining independence and a weakened form of dominance
1. See the work of Lin (2007) for an outline of their general methodology as well as several examples for
applications in domains other than social choice theory.

145

Geist & Endriss

with two axioms known as simple uncertainty aversion and simple top monotonicity (see
Appendix A). This is particularly interesting, because (in the context of a characterisation
of a particular type of set preference orders) this very same set of axioms had previously
been claimed to be consistent (Bossert, Pattanaik, & Xu, 2000). This has later been found
to be a mistake, which has been corrected by Arlegi (2003), but even his work does not establish the actual impossibility theorem. This certainly demonstrates the nontrivial nature
of the problem. Other interesting theorems discovered by our method include variants of
the Kannai-Peleg Theorem involving weakened versions of the independence axiom, impossibility theorems that do not rely on the dominance axiom (which is an integral component
of most other results in the field), and impossibility theorems for which the critical domain
size n is different from those featuring in any of the known results in the literature.
The remainder of the paper is organised as follows. Section 2 introduces the formal
framework of ranking sets of objects and recalls the seminal result in the field, the KannaiPeleg Theorem. In Section 3 we prove our Preservation Theorem, which allows us to reduce
general impossibilities to small instances. Section 4 then shows how to model those small
instance as sets of clauses in propositional logic. Building on these insights, Section 5
finally presents our method to automatically search for impossibility theorems, as well as
the 84 impossibility theorems we have been able to obtain using this method. Section 6
concludes with a brief summary and a discussion of possible directions for future work.
Appendix A provides a list of the 20 axioms used in our automated theorem search. The
reader can find additional detail, regarding both the method and the impossibility theorems
discovered, in the Master’s thesis of the first author (Geist, 2010).

2. Ranking Sets of Objects
Ranking sets of objects deals with the question of how an agent should rank sets of objects,
given her preferences over individual objects. Answers to this question will depend on the
concrete interpretation assigned to sets (Barberà et al., 2004):
• Complete uncertainty. Under this interpretation, sets are considered as containing
mutually exclusive alternatives from which the final outcome is selected at a later
stage, but the agent does not have any influence on the selection procedure.
• Opportunity sets. Here, again, sets contain mutually exclusive alternatives, but this
time the agent can pick a final outcome from the set herself.
• Sets as final outcomes. In this setting, sets contain compatible objects that are assumed to materialise simultaneously (i.e., the agent will receive all of them together).
Suppose an agent prefers object x over object y. Then under the first interpretation
it is reasonable to assume that she will rank {x} (receiving x with certainty) over {x, y}
(receiving either x or y). Under the second interpretation she might be indifferent between
{x} and {x, y}, as she can simply pick x from the latter set. Under the third interpretation,
finally, she might prefer {x, y}, as this will give her y on top of x. In this paper, we will focus
on the idea of complete uncertainty, which is the most studied of the three. An important
application of this interpretation is voting theory: when we want to analyse whether a voter
has an incentive to manipulate an election we often have to reason about her preferences
146

Automated Search for Impossibility Theorems in Social Choice Theory

between alternative outcomes, each producing a set of tied winning candidates (Gärdenfors,
1976; Duggan & Schwartz, 2000; Taylor, 2002).
Next, we introduce the notation and mathematical framework usually employed to treat
problems in the field of ranking sets of objects (e.g., see Barberà et al., 2004), and we then
present the aforementioned Kannai-Peleg Theorem in some detail (Kannai & Peleg, 1984).
2.1 Formal Framework
˙
Let X be a (usually finite) set of alternatives (or objects), on which a (preference) order ≥
˙
is defined. The order ≥ is assumed to be linear, i.e., it is a reflexive, complete, transitive
˙ by >,
˙ i.e., x >
˙ y
and antisymmetric binary relation. We denote the strict component of ≥
˙ x. The interpretation of ≥
˙ y and y 
˙ will be such that x ≥
˙ y if and only if x is
if x ≥
considered at least as good as y by the decision maker.
Similarly, we have a binary relation  on the set of nonempty subsets of X (denoted by
X := 2X \{∅}). This relation will for now be assumed to be a weak order (reflexive, complete,
transitive); later on, however, our proof method will allow to explore weaker assumptions
regarding , too. Like above, we use  for the strict component of . Additionally, we
also define an indifference relation ∼ by setting A ∼ B if and only if A  B and B  A.
˙ and
For any A ∈ X we write max(A) for the maximal element in A with respect to ≥
˙
min(A) for the minimal element in A with respect to ≥.
2.2 The Kannai-Peleg Theorem
Kannai and Peleg (1984) were probably the first to treat the specific problem of extending
preferences from elements to subsets as a problem in its own right and in an axiomatic
fashion. In previous work, other authors had regarded this problem as more of a side issue
of other problems, particularly in the analysis of the manipulation of elections (e.g., see
Fishburn, 1972; Gärdenfors, 1976), or had merely axiomatised specific methods of extension
without considering the general problem (e.g., see Packard, 1979).
The Kannai-Peleg Theorem makes use of two axioms, both of which are very plausible
under the interpretation of complete uncertainty. First, there is the Gärdenfors principle
(Gärdenfors, 1976, 1979), also known as dominance. This principle consists of two parts
and it requires that
˙ than all the elements in a given set, to
(1) adding an element, that is strictly better (>)
that given set produces a strictly better set with respect to the order ,
˙ than all the elements in a given set, to
(2) adding an element, that is strictly worse (<)
that given set produces a strictly worse set with respect to the order .
Formally, the Gärdenfors principle (GF) can be written as the following two axioms:
(GF1)
(GF2)

˙ a) ⇒ A ∪ {x}  A
((∀a ∈ A)x >
˙ a) ⇒ A ∪ {x} ≺ A
((∀a ∈ A)x <

for all x ∈ X and A ∈ X ,
for all x ∈ X and A ∈ X .

Second, we have a monotonicity principle called independence, which states that, if a
set is strictly better than another one, then adding the same alternative (which was not
contained in either of the sets before) to both sets simultaneously does not reverse this strict
147

Geist & Endriss

order. An equivalent way of stating this (in the light of the completeness of the order) is to
require that at least a non-strict preference remains of the original strict preference (such
that  becomes ). The formal statement reads as follows:
(IND)

A  B ⇒ A ∪ {x}  B ∪ {x}

for all A, B ∈ X and x ∈ X \ (A ∪ B).

Before we get to the main theorem, we present a lemma, also due to Kannai and Peleg
(1984). It says that only very specific rankings can satisfy the conditions (GF) and (IND).
Lemma 1. If  satisfies the Gärdenfors principle (GF) and independence (IND), then A ∼
{max(A), min(A)} for all A ∈ X .
Proof. Let A be a nonempty subset of X. If |A| ≤ 2 then the lemma holds trivially
by reflexivity of  since then A = {max(A), min(A)}. So suppose |A| ≥ 3 and define
A− := A\max(A). Note that, because |A| ≥ 3, the set A− is nonempty and thus {min(A)} =
{min(A− )}. By a repeated application of (GF1) we get {min(A)} = {min(A− )} ≺ A− . We
can then add max(A) to both sides, showing that {min(A), max(A)}  A by (IND). In a
completely analogous way we get {min(A), max(A)}  A+ ∪ {min(A)} = A from (GF2) and
(IND), where A+ := A \ min(A).
That is, Lemma 1 shows that the ranking of subsets is completely determined by their
worst and best elements. We are now ready to state and prove the theorem.
Theorem 1 (Kannai and Peleg, 1984). Let X be a linearly ordered set with |X| ≥ 6. Then
there exists no weak order  on X satisfying the Gärdenfors principle (GF) and independence (IND).
Proof. Let xi , i ∈ {1, 2, . . . , 6} denote six distinct elements of X such that they are ordered
˙ with respect to their index, i.e., x1 >
˙ x2 >
˙ x3 >
˙ x4 >
˙ x5 >
˙ x6 . By way of contraby >
diction, suppose there exists a weak order  on X satisfying the Gärdenfors principle (GF)
and independence (IND). We first claim that then
{x2 , x5 }  {x3 }.

(1)

In order to prove this claim, suppose that the contrary is the case, which by completeness
of  is {x3 }  {x2 , x5 }. We can then, by (IND), include x6 , which yields {x3 , x6 } 
{x2 , x5 , x6 }. Note now that together with Lemma 1 (and transitivity) this implies
{x3 , x4 , x5 , x6 }  {x2 , x3 , x4 , x5 , x6 },
contradicting (GF1). Thus, claim (1) must be true and it follows from {x3 }  {x3 , x4 } 
{x4 } (which is a consequence of the Gärdenfors principle) together with transitivity that
{x2 , x5 }  {x4 }. Using (IND) again, we can add (the so far unused) x1 and get {x1 , x2 , x5 } 
{x1 , x4 }. As before, we again fill in the intermediate elements to both sets and obtain,
by Lemma 1 and transitivity, that {x1 , x2 , x3 , x4 , x5 }  {x1 , x2 , x3 , x4 }, which this time
contradicts (GF2).
148

Automated Search for Impossibility Theorems in Social Choice Theory

Put differently, the Kannai-Peleg Theorem says that the Gärdenfors principle (GF) and
independence (IND) are inconsistent for a domain of six or more elements. Thus, there is
no way of extending a linear order on a set of at least six objects to a weak order on the
collection of all nonempty sets of these objects. This is why the Kannai-Peleg Theorem is
also referred to as an impossibility theorem.
Many more axioms are discussed in the literature and we are aware of two more impossibility theorems regarding choice under complete uncertainty (Barberà et al., 2004). A
selection of 20 of the most important axioms can be found in Appendix A.

3. Reduction of Impossibilities to Small Instances
While the Kannai-Peleg Theorem applies to any set X with at least six elements, the
proof we have given (which closely follows the original proof of Kannai and Peleg) works
by exhibiting the case with exactly six elements. The fact that the impossibility theorem
applies to larger domains as well is very clear in this particular case. Our goal in this
section is to prove that this approach can be elevated to a general proof technique: to
prove a general impossibility theorem it is sufficient to establish impossibilities for a small
instance. Specifically, we will prove what we call the Preservation Theorem, which says that
certain axioms are preserved in specific substructures. A corollary to this theorem then is
a universal reduction step, which says that the non-existence of a satisfying relation on a
small domain shows that no larger satisfying relation can exist either.2
We will work in the framework of mathematical logic in order to have access to the syntactic as well as semantic features of axioms. In Section 3.1, we first describe a many-sorted
language for our specific problem of ranking sets of objects, before we apply techniques from
model theory to prove the Preservation Theorem, in Section 3.2, which has the universal
reduction step as a corollary. This universal step is then powerful enough to cater for all
the axioms from the literature that we were able to formalise in our language, including all
of those listed in Appendix A.
3.1 Many-Sorted Logic for Set Preferences
A natural and well-understood language for our problem domain is many-sorted (first-order)
logic, which has, compared to first-order logic, more and different quantifiers (allowing for
quantification over different domains containing the elements of a respective sort), but is
still reducible to first-order logic. Apart from the quantifiers, many-sorted logic is practically equivalent to first-order logic and thus many results (e.g., soundness, completeness,
compactness, Löwenheim-Skolem properties, etc.) can be transferred from first-order logic
or can be directly proven (Manzano, 1996; Enderton, 1972).
Many-sorted logic is characterised by the use of a set S of different sorts s ∈ S. A
structure (or model ) A for many-sorted logic is just like one for first-order logic, but with
2. The universal reduction step plays a similar role as the inductive lemmas of Tang and Lin (2009) play
in their work on a computer-aided proof of Arrow’s Theorem and other theorems in social choice theory.
An important difference is that for the domain of ranking sets of objects we are able to prove a single
such result, which allows us to perform reductions for a wide range of problems, while Tang and Lin had
to prove new (albeit similar) lemmas for every new result tackled.

149

Geist & Endriss

separate domains doms (A) for each sort s ∈ S instead of one single domain. We then have
corresponding quantifiers ∀s and ∃s for each sort s, equipped with the intuitive semantics:
• A |= ∀s xϕ(x) if and only if A |= ϕ(a) for all a ∈ doms (A),
• A |= ∃s xϕ(x) if and only if A |= ϕ(a) for some a ∈ doms (A).
Other than that, many-sorted logic is analogous to first-order logic with the slight difference of having separate variable, function, and relation symbols for the different sorts or
combinations of sorts.
In our case, we will have two sorts (S = {ε, σ}): elements (ε) and sets (σ). We further
˙ and  of type
demand that there is a relation ∈ of type hε, σi as well as two relations ≥
hε, εi and hσ, σi, respectively. These will then later be interpreted as the usual membership
relation and our linear and weak orders, respectively. But one can have many more relations
and functions in the signature, and we will use the following time and again:
• Relations:
– ⊆, type hσ, σi (intuitively: set inclusion)
– disjoint, type hσ, σi (intuitively: true iff sets are disjoint)
– evencard, type hσi (intuitively: true iff the cardinality of a set is even)
– equalcard, type hσ, σi (intuitively: true if sets have the same cardinality)
• Functions:
– ∪, type hσ, σ, σi (intuitively: set union)
– {·}, type hε, σi (intuitively: transforms an element into the singleton set)
– replaceInBy, type hε, σ, εi (intuitively: replace an element in a set by another
element; e.g., (A \ {a}) ∪ {b})
We call this language of many-sorted logic (with the two sorts ε and σ and the signature
containing exactly the above relations and functions) MSLSP (Many-Sorted Logic for Set
Preferences). Notation-wise we will sometimes use (the more common) infix notation for
certain relations and functions. We will for instance write A ∪ B, a ∈ A, A ⊆ B and
{x} instead of ∪(A, B), ∈ (a, A), ⊆ (A, B) and {·}(x), respectively. Furthermore, we will
sometimes use negated symbols like x ∈
/ A for ¬(x ∈ A) as well as the strict relation symbols
˙ y ∧ ¬(y ≥
˙ x), respectively.
˙ y when we mean A  B ∧ ¬(B  A) and x ≥
A  B and x >
Generally, we will use the (standard model-theoretic) notation of Hodges (1997).
MSLSP is expressive enough to formulate many axioms in the literature (including the
20 axioms of Appendix A) and as an example we give the representations of the principle
of independence and the Gärdenfors principle (see Section 2.2):
Example 1. (IND) can be formulated in MSLSP:
∀σ A∀σ B∀ε x [(x ∈
/ (A ∪ B) ∧ A  B) → A ∪ {x}  B ∪ {x}]
150

Automated Search for Impossibility Theorems in Social Choice Theory

Example 2. (GF) can be formulated in MSLSP:
˙ a)) → A ∪ {x}  A]
∀σ A∀ε x [(∀ε a(a ∈ A → x >
˙ x)) → A  A ∪ {x}]
∀σ A∀ε x [(∀ε a(a ∈ A → a >
Some axioms, however, do not have such straightforward representations in MSLSP.
The concept of weak preference dominance, proposed (in a slightly stronger form) by Sen
(1991), is an example of such an axiom:
(WPD)

[(|A| = |B| and there exists a bijective function ϕ : A → B such that


˙ ϕ(a) for all a ∈ A ⇒ A  B for any two sets A, B ∈ X .
a≥

Even though there is no obvious way to express this axiom in MSLSP, Puppe (1995)
showed that (WPD) is actually equivalent to an axiom much closer to our formalism, which
he calls preference-basedness and which is easily seen to be expressible in MSLSP.3


˙ a ⇒ (A \ {a}) ∪ {b}  A for all A ∈ X , a ∈ A, b ∈
(PB)
b≥
/ A,
It might, however, be the case that some axioms are not expressible in MSLSP at all. We
have, for instance, not been able to translate the axiom of neutrality (Nitzan & Pattanaik,
1984; Pattanaik & Peleg, 1984), which says that the manner in which the ranking is lifted
from objects to sets of objects does not depend on the names of the objects. This axiom
is usually defined in terms of a function ϕ from objects to objects and postulates that
˙ is invariant under ϕ, then so is .
whenever ≥


˙ y ⇐⇒ ϕ(x) ≥
˙ ϕ(y) and y ≥
˙ x ⇐⇒ ϕ(y) ≥
˙ ϕ(x)
(NEU)
x≥
for all x ∈ A, y ∈ B] ⇒
[A  B ⇐⇒ ϕ(A)  ϕ(B) and B  A ⇐⇒ ϕ(B)  ϕ(A)]
for any two sets A, B ∈ X and any injective mapping ϕ : A ∪ B → X.
3.2 Preservation Theorem and Universal Reduction Step
The famous Loś-Tarski Theorem of classical model theory offers a weak version of the result
we are going to prove.4 It does, however, not cover certain axioms and therefore we have
to find a stronger result than what classical model theory can offer. The idea is to be able
to preserve a larger class of axioms by making use of our problem-specific features: like,
for instance, the element-set framework. Thus, we define the concepts of a structure for set
preferences as well as subset-consistent substructures:
Definition 1. An MSLSP-structure B is a structure for set preferences if it fulfils the
following criteria:
ˆ`
´
˜
˙ a → replaceInBy(a, A, b)  A
3. ∀σ A∀ε a∀ε b a ∈ A ∧ b ∈
/ A∧b≥
4. The exact statement of the theorem (for first-order logic) can, for example, be found in Hodges’ book
(1997) as Corollary 2.4.2 and the proof idea is relatively simple: by contradiction it suffices to show that
∃1 -formulas are preserved by embeddings (Hodges, 1997, Thm 2.4.1). The proof of the latter proceeds
by induction on the complexity of the formula and the critical case of the existential quantifier does not
cause any trouble as witnesses are not “lost” when moving to a larger structure.

151

Geist & Endriss

1. domσ (B) ⊆ 2domε (B) , i.e., the domain of sort σ contains only sets of elements from
the domain of sort ε.
2. The relation symbol ∈ of type hε, σi is interpreted in its natural way.
If a substructure A of a structure for set preferences B is a structure for set preferences,
too, then it is called a subset-consistent substructure.
Note that in a substructure A of a structure for set preferences B we have
∈A =∈B |dom(A) ,
i.e., the symbol ∈ must be interpreted as the restriction of its interpretation in B. Hence,
it is sufficient to fulfill the first condition for being a subset-consistent substructure of B.
These two semantic conditions suffice for extending the Loś-Tarski Theorem to a larger
class of axioms. But what are the axioms that we can now treat? Let us look at the
following (purely syntactic) definition first and then explain our reasons for choosing this
particular class.
Definition 2. The class of existentially set-guarded (ESG) formulas is the smallest class of
MSLSP-formulas recursively defined as follows:
• all quantifier-free formulas are ESG,
• if ψ(x̄) and ψ 0 (x̄) are ESG, then ϕ(x̄) := (ψ ∧ ψ 0 )(x̄) as well as ϕ0 (x̄) := (ψ ∨ ψ 0 )(x̄)
are ESG,
• if ψ(y, x̄) is ESG, then ϕ(x̄) := ∀s yψ(y, x̄) is ESG for any sort s ∈ {ε, σ},
• if ψ(y, x̄) is ESG, then ϕ(x̄) := ∃ε y(y ∈ t(x̄) ∧ ψ(y, x̄)), where t is a term of sort σ and
y does not occur in x̄, is ESG.
The atomic formulas y ∈ t(x̄) of the last condition are called the set-guards of the respective
quantifiers.
The class of ESG formulas consists of all MSLSP-formulas that only contain set-guarded
existential quantifiers ∃ε of sort ε, and no existential quantifiers ∃σ of sort σ at all.
Note that when we write ϕ(x̄), we do not necessarily mean that ϕ contains all the
variables in the sequence x̄ = (x0 , x1 , x2 , . . . ), but just that all (free) variables of ϕ are
among those in x̄. We will also use the notation ϕ[ā], with ā being a sequence of elements,
which will mean that the elements a0 , a1 , a2 , . . . are assigned to the variables x0 , x1 , x2 , . . . .
Intuitively, we will do the following: in axioms we allow existential quantifiers (but only
for elements, i.e., of sort ε) as long as they are “guarded” by sub-formulas saying that the
respective witness belongs to some set. The sets can also be unions of sets or formed in a
different way by the term t. The important part is that when moving from a structure to a
substructure this set-guard now guarantees that the witness of the existential quantifier is
not lost. This is because the witness has to be within a set (as required by the set-guard)
that will be situated in the substructure.
Before we explain this further and give the formal proof of this claim, let us look at
examples of ESG and non-ESG sentences:
152

Automated Search for Impossibility Theorems in Social Choice Theory

Example 3. The axiom (GF1) (and similarly (GF2)) is an ESG sentence:

∃ε a(a ∈ A ∧
∃ε a(a ∈ A ∧
∀σ A∀ε x[∃ε a(a ∈ A ∧

˙ a
x≯
˙ a)
x≯

(quantifier-free)
(adding ∃ε )

˙ a) ∨ A ∪ {x}  A
x≯
˙ a) ∨ A ∪ {x}  A]
x≯

(∨ with quantifier-free)
(adding ∀s ).

Considering the last line of the above example, one can understand why removing elements from X does not affect this axiom. For universal quantifiers a restriction of the
domain is no problem anyway. But also the existential witness is not lost: if we suppose it
had been removed for some set A, then so would have been the set A itself, as the σ-domain
can only contain sets of elements from the ε-domain (by Definition 1). But with A removed
there is no need for a witness anymore.
That one cannot just allow arbitrary existential quantifiers without set-guards can be
seen when considering the following example, which shows a very simple sentence (with
unguarded existential quantifiers) that is not preserved in substructures.
Example 4. The MSLSP-sentence (axiom)
∃ε x∃ε y∃ε z [x 6= y ∧ x 6= z ∧ y 6= z] ,
which says that there are at least three distinct elements in the ε-domain of a structure
for set preferences, is clearly not preserved in substructures: it holds in all structures for
set preferences B with at least three elements in domε (B), but fails to hold in any of its
substructures A with less than three elements in domε (A).
After these examples the reader should have developed some understanding of why ESG
sentences are preserved in substructures and why we cannot allow much more. The formal
proof of our Preservation Theorem will explain the first part further.
Note that, apart from the case of the existential quantifier, the proof is essentially
identical to one direction of the proof of the Loś-Tarski Theorem for many-sorted logic,
which can be carried out on model-theoretic grounds alone. It is just the last part of this
proof (the induction step for the existential quantifier) that requires the syntactic restriction
(to ESG sentences) as well as the semantic restriction (to subset-consistent substructures),
the latter of which we can allow because of our particular problem domain.
Theorem 2 (Preservation Theorem). ESG sentences are preserved in subset-consistent
substructures, i.e., if A is a subset-consistent substructure of a structure for set preferences
B then B |= ϕ implies A |= ϕ for any ESG sentence ϕ.
Proof. We prove a stronger statement for ESG formulas (instead of sentences) by induction
on the complexity of the formula:
If A is a subset-consistent substructure of a structure for set preferences B then
B |= ϕ[ā] implies A |= ϕ[ā] for any ESG formula ϕ(x̄) and any tuple ā of
elements from dom(A) (matching the types of x̄).
153

Geist & Endriss

So let B be a structure for set preferences with a subset-consistent substructure A, let ϕ(x̄)
be an ESG formula and, furthermore, let ā be a tuple of elements from dom(A) (matching
the types of x̄).
Quantifier-free Formulas: If ϕ(x̄) is quantifier-free, a routine but tedious proof leads to
the desired results. One has to carry out a few nested inductions on the complexity of terms
and formulas, and examples of such proofs can be found in any textbook on Model Theory
(e.g., see Hodges, 1997, Theorem 1.3.1). First, one shows by one induction that terms are
interpreted in the substructure A as they are interpreted in its superstructure B, i.e.,
tA [ā] = tB [ā]

(2)

for all terms t(x̄). This practically immediately follows from the definition of a substructure.
Then one proceeds by another induction proving that atomic formulas hold in A if and
only if they hold in B, i.e.,
A |= ψ[ā] ⇐⇒ B |= ψ[ā]
(3)
for all atomic formulas ψ(x̄). As a typical example, suppose that ϕ(x̄) is of the form
R(s(x̄), t(x̄)), where R is a relation symbol and s(x̄) as well as t(x̄) are terms (matching
the type of R). Assume A |= R(s[ā], t[ā]), i.e., it holds that RA (sA [ā], tA [ā]). By (2) this
is equivalent to RA (sB [ā], tB [ā]). Since furthermore RA = RB |dom(A) , we even have an
equivalence with RB (sB [ā], tB [ā]), which is just another way of saying B |= R(s[ā], t[ā]).
Finally, one proves the claim for any quantifier-free formula by carrying out induction
steps for conjunction ∧, disjunction ∨ and negation ¬. Note that the step for ¬ is why we
required both directions in (3).
Conjunction and Disjunction: We only show the part for conjunction here; the one for
disjunction is completely analogous. If ϕ(x̄) is of the form ψ(x̄) ∧ ψ 0 (x̄) and furthermore
B |= ϕ[ā], then both ψ[ā] and ψ 0 [ā] must be true in B. By the induction hypothesis, this
carries over to A and we get A |= ψ[ā] ∧ ψ 0 [ā].
Universal Quantification: If ϕ(x̄) is of the form ∀s yψ(y, x̄) with sort s ∈ {σ, ε} and
furthermore B |= ϕ[ā], then for all b of sort s in doms (B) we have that B |= ψ(b, ā). Since
doms (A) ⊆ doms (B) we can use the induction hypothesis and obtain A |= ψ(b, ā) for any
b ∈ doms (A). This is the same as saying A |= ∀s yψ(y, ā), i.e., A |= ϕ[ā].
Existential Quantification: If ϕ(x̄) is of the form ∃ε y[y ∈ t(x̄) ∧ ψ(y, x̄)], where t(x̄) is a
term of sort σ and y does not occur in x̄, and furthermore B |= ϕ[ā], then there must exist
an element b in domε (B) such that
B |= (y ∈ t(x̄) ∧ ψ(y, x̄)) [b, ā], i.e., B |= y ∈ t(x̄)[b, ā] and B |= ψ[b, ā].
Hence, if we can show that b is in the ε-domain of A and not just of B, then it follows
by the induction hypothesis that also
A |= ψ[b, ā],
since then (b, ā) is a tuple of elements of A.
As ∈ is interpreted naturally in the structure for set preferences B and, additionally,
y cannot occur in x̄, the statement B |= y ∈ t(x̄)[b, ā] boils down to b ∈ tB [ā], which is
equivalent to
b ∈ tA [ā]
(4)
154

Automated Search for Impossibility Theorems in Social Choice Theory

since tA [ā] = tB [ā], as stated in (2).
The fact that b is an element of domε (A) (and not just of domε (B)) is now implied by
A
t [ā] being in domσ (A), together with A being a subset-consistent substructure:
(∗)

b ∈ tA [ā] ∈ domσ (A) ⊆ 2domε (A)
=⇒ b ∈ tA [ā] ∈ 2domε (A)
=⇒ b ∈ tA [ā] ⊆ domε (A),
where (∗) marks the point where the subset-consistency of A is used.
Hence, we can, as indicated before, apply the induction hypothesis to B |= ψ[b, ā] and
obtain A |= ψ[b, ā]. Together with b ∈ tA [ā] it follows that
A |= ∃ε y(y ∈ t(x̄) ∧ ψ(y, x̄))[ā].
This way we are done with the proof of the stronger claim (about formulas), which
implies the claim of the theorem (about sentences).
We are now almost ready to apply this theorem to our setting. Note first, however,
that the above theorem does not only hold for axioms that are ESG, but also for axioms
that are equivalent to an ESG sentence in all structures for set preferences (the reason is
that they have the same truth value in any such structure). We refer to these axioms as
ESG-equivalent axioms. In particular this applies to sentences that are logically equivalent
(i.e., equivalent in all structures) to an ESG sentence.
Now, we finally state and prove the corollary applying our general result to the particular
problem domain of ranking sets of objects.
Corollary 1 (Universal Reduction Step). Let Γ be a set of ESG (or ESG-equivalent) axioms
and let n ∈ N be a natural number. If, for any linearly ordered set Y with n elements, there
exists no binary relation on Y = 2Y \ {∅} satisfying Γ, then also for any linearly ordered set
X with more than n elements there is no binary relation on X = 2X \ {∅} that satisfies Γ.
Proof. Let Γ be a set of ESG (or ESG-equivalent) axioms and let n ∈ N be a natural
number. Assume that for any linearly ordered set Y with n elements, there exists no binary
relation on Y = 2Y \ {∅} satisfying Γ. By way of contradiction, suppose X is a linearly
ordered set with |X| > n and there is a binary relation on X = 2X \ {∅} that satisfies Γ. We
can view X ∪X as a structure for set preferences and define a subset-consistent substructure
by restricting X ∪ X to a domain Y ∪ Y with Y ⊆ X, |Y | = n and Y := 2Y \ {∅}. By the
Preservation Theorem all ESG(-equivalent) axioms are preserved in this subset-consistent
substructure Y ∪ Y. Hence, Y must be a linearly ordered set (as all order axioms are ESG)
and, furthermore, there is a binary relation on Y satisfying Γ. Contradiction!
Remark. When we say “a given set of ESG axioms”, then this deserves some further explanation. We mean axioms that are ESG in MSLSP. One can consider additional relations
and functions to be added to this signature, but this holds some hidden challenges. For
instance, it is not possible to include a predicate isWholeSet, which is true of the whole
domain only, a function (·)c for the complement, or even just the constant symbol Ẋ referring to the whole domain, since all three would (in their natural interpretation) prevent
155

Geist & Endriss

Y ∪ Y from being a substructure: for example, for any Y ⊂ X, while isWholeSet(Y ) (or
equivalently, Y = Ẋ) is false in X ∪ X , it is true in Y ∪ Y. Similarly, we run into problems
when including functions like ∩, \ or (·)c , which are (in their natural interpretation) not
functions in the strict sense in a structure like X ∪ X as they can produce the empty set,
which is not in X . Therefore, some attention has to be paid when adding new relation or
function symbols to the language in order to capture more axioms.
On the basis of Corollary 1 we can finally do what we had been hoping for: in order to
prove new impossibility theorems and check existing ones, we only have to look at their base
cases (as long as all axioms involved are expressible in MSLSP and are ESG-equivalent).
As we shall see next, these small instances can be efficiently checked on a computer.

4. Representing Small Instances in Propositional Logic
The small instances, which we can reduce impossibilities to, now need to be checked on
a computer. This requires a clever approach as a direct check is far too expensive. We
therefore modify and extend a technique due to Tang and Lin (2009).
It is remarkable that Tang and Lin (2009) were able to formulate the base case of
Arrow’s Theorem in propositional logic, even though some of the axioms intuitively are
second-order statements. The trick they used was to introduce “situations” as names for
preference profiles, which transforms the second-order axioms into first-order statements,
which can then (because of the finiteness of the base case) be translated into propositional
logic. We are going to use a similar approach since the axioms for ranking sets of objects
are stated in a (somewhat enriched)5 second-order format, too. The setting of ranking sets
of objects will, however, require a different treatment (which we are going to discuss in the
sequel) since we also have to apply functions like union (∪) and singleton set ({·}) to sets
and elements, respectively, whereas no functions needed to be applied to Tang and Lin’s
situations. Instead of coding these operations on sets within the propositional language, we
let the program that generates the final formula handle them.
In this section, we first show how to translate axioms for ranking sets of objects into
propositional logic and then explain how to instantiate instances of these axioms for fixed
domain sizes on a computer.
4.1 Conversion to Propositional Logic
As an example, consider again the Kannai-Peleg Theorem (Theorem 1). In light of the
universal reduction step (Corollary 1), proving this theorem reduces to proving a small base
case of exactly six elements:
Lemma 2 (Base case of the Kannai-Peleg Theorem). Let X be a linearly ordered set with
exactly 6 elements. Then there exists no weak order  on X satisfying the Gärdenfors
principle (GF) and independence (IND).
It might seem tempting to perform a direct check of the involved axioms on all weak
orders over the nonempty subsets of a six-element space. This, however, can be seen to be
practically impossible as there are around 1.525 · 1097 such orderings (Sloane, 2010, integer
5. There is also an order (i.e., a relation) on sets.

156

Automated Search for Impossibility Theorems in Social Choice Theory

sequence A000670). Therefore, we should stick to the idea of transforming the axioms of
the Kannai-Peleg Theorem into propositional logic such that they can be checked by a SAT
solver, which usually operates on propositional formulas in conjunctive normal form (CNF)
only. We will describe in the following how instances of axioms, like the ones stated in
Appendix A, can be converted to that language.
It will be sufficient for our formalisation to have two kinds of propositions only: w(A, B)
and l(x, y) (corresponding to propositional variables wA,B and lx,y ) with intended meanings
A is ranked at least as high as B by the weak order  (or short: A  B), and x is ranked
˙ (or short: x ≥
˙ y), respectively. For example,
at least as high as y by the linear order ≥
for the base case of the Kannai-Peleg Theorem this leads to a maximum of |X |2 + |X|2 =
(26 − 1)2 + 62 = 4005 different propositional variables.
As indicated earlier, the axioms for linear and weak orders on X and X , respectively, are
entirely unproblematic as they only contain first-order quantifications and, thus, they can
easily be transformed. As an example, we include here the transformation of the transitivity
axiom for orders on sets:
(TRANSσ )

(∀A ∈ X )(∀B ∈ X )(∀C ∈ X ) [A  B ∧ B  C → A  C]
^ ^ ^
∼
[¬wA,B ∨ ¬wB,C ∨ wA,C ] .
=
A∈X B∈X C∈X

Note, that, due to the finiteness of X (and thus X ), the derived formulas are actually
finite objects and can therefore be instantiated by hand (requiring a lot of effort) or using
a computer. Furthermore, only very little or no work is needed to convert them into CNF.
Other axioms, like (GF) and (IND), appear to be more difficult to transform because
of functions like singleton set {·} : X → X and set union ∪ : X × X → X , which occur
within these axioms. In fact, however, the same simple conversion technique as above can
be applied since we are going to take care of those (and similar) functions automatically in
our computer program for the instantiation of the axioms. How we do this will be briefly
described in Section 4.2 and for now we treat terms like A ∪ B as if they were just the
corresponding objects from the functions range, i.e., images under the respective functions.
For example, this leads to the following conversion of (GF1):
(GF1)

˙ a) → A ∪ {x}  A]
(∀A ∈ X )(∀x ∈ X)[((∀a ∈ A)x >
"
!
#
^ ^
^

∼
lx,a ∧ ¬la,x → wA∪{x},A ∧ ¬wA,A∪{x}
=
A∈X x∈X

a∈A

"
≡

!

^ ^

_

A∈X x∈X

a∈A

¬lx,a ∨ la,x

!
∨ wA∪{x},A

!
∧

_

¬lx,a ∨ la,x

!#
∨ ¬wA,A∪{x}

,

a∈A

where the last step only serves the purpose of converting into CNF.
The remaining problematic parts of the formula are the propositional variable index
A ∪ {x}, and the disjunction domain criterion a ∈ A. In order to write out the formula
explicitly (which we need to do to be able to feed it to a SAT solver) we have to determine
which set is represented by A∪{x} and also decide whether a ∈ A for any a ∈ X. In different
words, what we need is explicit access to the elements of a set and also we have to be able
157

Geist & Endriss

to manipulate them. This would theoretically be possible by hand; practically, however,
the instantiation of the formula is far too large to be written out manually. Therefore, we
need a computer program for the final conversion step, which we are going to describe in
the following section.
As a second example, consider the axiom of independence (IND), which can also be
transformed in the above fashion by first using finiteness to replace the quantifiers, and
then normalising the formula into CNF:
(IND)

(∀A, B ∈ X )(∀x ∈ X \ (A ∪ B)) [A  B → A ∪ {x}  B ∪ {x}]
^
^ 

∼
(wA,B ∧ ¬wB,A ) → wA∪{x},B∪{x}
=
≡

A,B∈X

x∈X
x∈(A∪B)
/

^

^

A,B∈X

x∈X
x∈(A∪B)
/



¬wA,B ∨ wB,A ∨ wA∪{x},B∪{x} .

The problematic terms here are x ∈
/ (A ∪ B), A ∪ {x} and B ∪ {x}. But, as we will see,
they can —just like the critical terms mentioned before— be handled by our program.
The same method of translation easily extends to other axioms, in particular all those
listed in Appendix A (Geist, 2010).
4.2 Instantiation of the Axioms on a Computer
As indicated above, we make use of a computer program in order to write out the formulas
derived above explicitly. We now briefly discuss the ideas of our implementation and, in
particular, the methods employed to cater for previously problematic expressions, such as
A ∪ {x} and a ∈ A. Full details of the implementation are given by Geist (2010).
The most widely used SAT solvers work on input files written according to the DIMACS
CNF format (DIMACS, 1993). In a few words, this format requires the propositional variables to be represented by natural numbers (starting from 1, since 0 is used as a separator)
with a minus (-) in front for negated literals. Furthermore, the whole file needs to be in
CNF; it has to contain exactly one clause per line.
To achieve a formulation of the axioms in this target format, the main idea is to fix
an enumeration of all propositional variables (of type lx,y and wA,B with x, y ∈ X and
A, B ∈ X ) by first enumerating sets and elements, and subsequently combining pairs of
these using a pairing function. Functions and relations like union and element of can then
be defined to operate on numbers directly, and quantifiers can be translated to conjunctive
or disjunctive iterations over their respective domains. All in all, easily readable code can
be used to instantiate the axioms.
Since the numberings of the items under consideration (here: elements, sets and later
propositional variables) form the core of our implementation, we start the translation process by first fixing an (arbitrary) numbering of the n elements x ∈ X, i.e., a bijective
function cn : X → {0, 1, . . . , n − 1}.6 For the Kannai-Peleg Theorem with its six elements,
for instance, the codes will hence range from 0 to 5.
6. In contrast to the propositional variables, there are no numbering constraints for the elements and so
their numbers are allowed to start from 0.

158

Automated Search for Impossibility Theorems in Social Choice Theory

We can then specify the corresponding numbering of sets in X . This requires special
attention because we want to define it in such a way that treating the problematic terms
(as mentioned above) is as easy as possible. A natural way to do this is by looking at a
set as its characteristic function and converting the corresponding finite string of zeros and
ones to a natural number. This allows us to perform operations on the codes of sets directly
and hence it is straightforward to instantiate the formulas from Appendix A automatically.
All that needs to be done is translating them to our specific source code style. Quantifiers
then correspond to for -loops over all elements or sets, respectively, and restrictions of the
quantification domain as well as operations on elements and sets can be taken care of by
functions operating on the “codes” of sets and elements directly.

5. Automated and Exhaustive Theorem Search
In Section 4 we described how to generate a (long) formula in propositional logic representing
small instances of impossibility theorems for ranking sets of objects. Let us denote such a
formula by ϕ. The formula ϕ describes a model with a linear order on its universe of a
given number of elements and a weak order satisfying a given set of axioms on the set of the
nonempty subsets of its universe. If such a model exists, ϕ has a satisfying assignment (the
explicit description of both orders) and, thus, a (complete) SAT solver will discover this
(assuming that there are no time or memory bounds). If, conversely, such a model does not
exist —which is exactly the statement of the impossibility theorem— then ϕ is unsatisfiable
and, again, a SAT solver will be able to detect this (assuming, again, that there are no time
or memory bounds).
By our universal reduction step (Corollary 1), a full impossibility theorem is therefore
equivalent to a lemma of the form “the formula ϕ is unsatisfiable”.
For example, feeding ϕKP (a description of the base case of the Kannai-Peleg Theorem
as generated by our program) to the SAT solver zChaff (SAT Research Group, Princeton
University, 2007) returns the correct result (‘UNSAT’) in about 5 seconds and thus the
automatic verification of this theorem is complete.
Using this technique for single theorems is likely to produce good results and might be a
helpful tool from a practical perspective, but we can do more: we are now going to present
a method for a fully automated and exhaustive theorem search for impossibility theorems.
Our theorem search will, for a given set of axioms, systematically check which of its subsets
are inconsistent and from which smallest domain size onwards these impossibilities do occur, thereby automatically identifying all impossibility theorems that the given axioms can
produce.7 In this sense, the search method is exhaustive on the space of given axioms.
To test the fruitfulness of this approach we ran the search on a set of 20 axioms from
the literature (Barberà et al., 2004), which we list and describe in Appendix A. Our search
algorithm returned a total of 84 impossibilities, of which a few were known already (and
are hence now automatically verified), others are immediate consequences of known results,
while others again are surprising and new. The very same search method can also be
run with arbitrary other ESG axioms in the field of ranking sets of objects, including, for
instance, the ones of the interpretation of sets as opportunity sets from which the decision
7. Since for practical reasons we can only check base cases up to a certain domain size |X| = n, there could
theoretically be more impossibilities hidden that only occur from larger domain sizes onwards.

159

Geist & Endriss

maker can himself select his favourite outcome, or, similarly, with axioms for the case of
assuming that the agent receives the whole set of alternatives (see Section 2).
In the remainder of this section, we first describe our method for automated theorem
search and then list and discuss the impossibilities that were found.
5.1 Approach
Our search method systematically decides whether combinations of given axioms are compatible or incompatible. We will therefore in the following refer to axiom subsets as problems,
and for a particular domain size we will speak of a problem instance.
After the generation of a problem instance by our computer program (as described
in Section 4.2) the instance is then passed to a SAT solver, which returns whether it is a
“possible” or an “impossible” one. We implemented interfaces for the commonly used solvers
PrecoSAT (Biere, 2010) and zChaff (SAT Research Group, Princeton University, 2007).
The latter provides an additional layer of verification by generating a proof trace that can
be checked using external tools, while the former is usually faster in practice, but does not
have this extra feature.
We could now just run this program on all possible problem instances from a given set
of axioms and a maximal domain size individually and collect the results. Note, however,
that on a space of 20 axioms with a maximal domain size of eight elements, we already
have to deal with a total of (220 − 1) · 8 ≈ 8, 400, 000 problem instances. If each of them
just requires a running time of one second,8 the whole job would take roughly 100 days.
Therefore, we designed a scheduler that makes sure all axiom subsets are treated for all
domain sizes in a sensible order. The order in which we check the problem instances has a
big effect on the overall running time because one can make use of a combination of four
different effects:
(1) if a set of axioms is inconsistent at domain size |X| = n, then it will also be inconsistent
for all larger domain sizes |X| > n (Corollary 1),
(2) if a set of axioms is inconsistent at domain size |X| = n, then also all its (axiom)
supersets are inconsistent at this domain size |X| = n,
(3) if a set of axioms is consistent at domain size |X| = n, then it will also be consistent
for all smaller domain sizes |X| < n. (Theorem 2), and
(4) if a set of axioms is consistent at domain size |X| = n, then also all its (axiom) subsets
are consistent at this domain size |X| = n.
Since larger instances require exponentially more time (there are exponentially more
variables in the satisfiability problem due to exponentially more subsets in X ), we start our
search at the smallest domain size and then after completely solving this “level” move on
to the next domain size.9 On a new level, only problems have to be considered that still
have the status “possible” because of condition (1) above.
8. In our tests, especially larger instances required much more time to be solved on average.
9. It is for this reason that condition (3) is not very helpful in practice.

160

Automated Search for Impossibility Theorems in Social Choice Theory

On each level, as soon as we find an impossibility, we can, by condition (2), mark all
axiom supersets as impossible at the current domain size (if they had not been found to be
impossible at a smaller domain size already). In order to use this mechanism as efficiently
as possible, we must check small axiom sets first. But also the dual approach of starting
from large axiom sets and marking all axiom subsets as “compatible” as soon as we find a
possibility (condition 4), is an option. In experiments, we found that the best performance
is achieved when combining these two approaches and so we decided to run the search in
alternating directions (switching every 15 minutes):10 from large axiom sets to small ones
and the other way around.
From a practical point of view, our implementation comes with the limitations of only
being able to treat at most 21 axioms at the same time (stack overflows occurred for larger
axiom sets) and at most a domain size of eight elements (due to memory limits in the SAT
solvers). But with better memory management and improved versions of the SAT solvers,
these (practical) boundaries should be extendable further.
5.2 Results
Our theorem search (checking problem instances up to a domain size of eight) yields a total
of 84 minimal impossibility theorems on the space of the 20 selected axioms. The results
are minimal in two senses:
• the corresponding axiom set is minimal with respect to set inclusion, i.e., all proper
subsets are compatible at the given domain size; and
• the domain size is minimal, i.e., for all smaller domain sizes the given axiom set is
still compatible.
Counting the total number of incompatible axiom sets (i.e., including all supersets), we find
312,432 inconsistent axiom sets out of about one million possible combinations.
The whole experiment required a running time of roughly one day for handling all of the
nearly 8.5 million instances.11 In order to externally verify as many of the impossibilities
as possible, we used the solver zChaff, which can create a computer-verifiable proof trace,
for all instances up to domain size 7, and switched to the faster solver PrecoSAT, which
does not have this feature, for instances with (exponentially larger) domain size 8.12
In Table 1 we list all minimal impossibilities that our search method was able to find (and
hence all that there are) for domain sizes up to 8. Recall again that, by Corollary 1, these all
directly correspond to full impossibility results (from the given domain size upwards). The
results are presented in ascending order by minimal domain size, and in ascending order by
the number of axioms involved as a second criterion, so as to have stronger and easier to
grasp impossibilities higher up in table. The axioms and their abbreviated names are listed
in Appendix A.
10. Switching every 15 minutes turned out to result in good performance, but we have not attempted to
systematically optimise this parameter.
11. The experiment was performed on an Intel Xeon 2,26 GHz octo-core machine using only one core and
5GB of the available 24GB memory. The machine is part of the Dutch national compute cluster Lisa.
12. The five impossibilities occurring only from domain size 8 onwards have therefore not been verified
externally. Using zChaff for these instances (and subsequently verifying them) would have also been
possible, but slower by about a factor of 10.

161

REFLσ

COMPLσ

TRANSσ

EXT

SDom

GF1

GF2

IND

strictIND

SUAv

SUAp

STopMon

SBotMon

topIND

botIND

disIND

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·

·
·
·
X
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·

·
·
·
·
·
·
·
·
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

·
·
·
·
X
·
·
·
·
·
X
·
·
X
·
·
·
·
·
·
X
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·

X
·
·
·
·
·
·
·
·
X
·
·
X
·
·
X
·
·
X
·
·
·
·
·
·
·
·
·
X
·
X
·
X
·
X
·
X
·
X
·
X
·

·
·
X
·
·
·
·
·
X
X
X
X
·
·
X
·
X
·
·
·
·
·
·
·
·
·
·
X
X
X
·
X
·
·
·
·
·
X
·
X
X
·

·
·
X
X
X
X
X
X
X
·
·
X
X
X
·
·
·
X
·
X
·
X
X
·
·
·
·
X
·
X
X
·
·
X
·
X
·
·
·
X
·
X

·
·
·
·
·
·
·
·
X
X
·
X
X
·
X
X
·
X
X
·
·
X
·
X
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·

X
·
X
X
X
X
X
X
·
·
X
·
·
X
·
·
X
·
·
X
X
·
X
·
X
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·

·
X
·
·
·
X
·
·
X
X
X
·
·
·
X
X
X
·
·
·
X
·
·
X
X
X
X
X
X
·
·
X
X
·
·
·
·
X
X
X
X
·

·
X
·
·
·
·
·
·
·
·
·
X
X
X
·
·
·
X
X
X
·
X
X
·
·
·
·
·
·
X
X
·
·
X
X
X
X
·
·
·
·
X

·
·
·
·
·
·
·
X
·
·
·
·
·
·
X
X
X
·
·
·
·
·
·
X
X
X
X
·
·
·
·
X
X
·
·
·
·
X
X
·
·
·

·
·
·
·
·
·
X
·
·
·
·
·
·
·
·
·
·
X
X
X
·
·
·
·
·
·
·
·
·
·
·
·
·
X
X
X
X
·
·
·
·
·

·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
X
X
X
X
X
X
X
X
·
·
X
X
·
·
X

·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
X
·
X
X
X
X
X
X
X
X
X
X
·
·
X
X
X

· · · ·
· · · ·
· · · ·
· · · X
· · · X
· · · X
· · · X
· · · X
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · X
· · · X
· · · X
· · · X
· · · X
· · · X
X · · X
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
· · · ·
X · · ·
X · · ·
X · · ·
X · · ·
· · · X
· · · X
· · · X
Continued. . .

Table 1: Results of our automated and exhaustive theorem search
on a space of 20 axioms (including orders).

162

MC

LINε

3
3
3
3
3
3
3
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4

evenExt

Size

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

intIND

No.

Geist & Endriss

No.

Size

LINε

REFLσ

COMPLσ

TRANSσ

EXT

SDom

GF1

GF2

IND

strictIND

SUAv

SUAp

STopMon

SBotMon

topIND

botIND

disIND

intIND

evenExt

MC

Automated Search for Impossibility Theorems in Social Choice Theory

43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84

4
5
5
5
5
5
5
5
5
5
5
5
5
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
7
7
8
8
8
8
8

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·

·
·
·
·
·
·
·
·
·
X
·
·
·
·
X
X
X
X
X
·
·
·
·
·
·
X
·
X
X
X
X
X
X
X
X
·
·
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·

·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
X
X
X
·
·
X
·
·
X
·
·
·
X
X
X
·
·
·
X
X
·
·
·
·
X
X
·

·
X
X
·
·
X
·
·
X
·
·
X
X
·
X
·
X
·
X
X
X
X
X
·
·
·
·
·
X
·
·
X
·
X
·
X
·
X
X
X
·
·

X
·
·
X
X
·
X
X
·
·
·
·
·
·
X
X
·
·
X
X
·
·
X
X
X
X
X
X
·
·
X
X
X
·
·
·
X
X
X
·
X
X

·
X
·
X
·
·
·
·
·
·
·
·
·
·
X
X
X
X
·
·
·
·
·
·
·
X
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·

·
·
X
·
X
·
·
·
·
X
X
·
·
X
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·

·
X
X
·
·
X
·
·
X
X
X
X
X
X
·
·
·
·
·
X
X
X
·
·
·
·
·
·
·
·
·
·
·
·
·
X
·
·
·
·
·
·

X
·
·
X
X
·
X
X
·
·
·
·
·
·
·
·
·
·
·
·
·
·
X
X
X
·
X
·
·
·
·
·
·
·
·
·
X
·
·
·
·
·

·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
X
·
X
·
·
·
X
·
·
·
X
·
X
·
X
X
·
X
·
X
·
·
·
·
·
X
X

·
·
·
·
·
·
·
·
·
·
X
·
·
·
·
·
X
X
·
·
·
·
·
·
X
·
·
·
X
X
·
·
·
X
X
·
·
·
·
X
·
·

·
·
·
·
·
X
X
·
X
·
·
·
·
·
·
·
·
·
X
·
·
·
X
X
X
·
X
X
X
X
X
·
·
·
·
·
X
·
X
·
X
X

X
·
·
·
·
X
X
X
·
·
·
X
·
·
·
·
·
·
X
X
X
X
·
·
·
·
·
X
X
X
X
X
X
X
X
X
·
X
·
X
·
·

X
·
·
·
·
·
·
X
X
·
·
·
X
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
X
X
X
X
·
·
X
X
X
X
X

·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
X
X
X
X
X
X
·
X
·
·
·
·
·
·
·
·
X
X
X
X
X
X
X

·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·
·

X
·
·
·
·
·
·
·
·
X
X
X
X
X
·
·
·
·
·
·
·
·
·
·
·
X
X
·
·
·
X
X
X
X
X
·
·
·
·
·
·
X

Table 1: Results of our automated and exhaustive theorem search
on a space of 20 axioms (including orders).

163

Geist & Endriss

Observing these results, we first note that impossibilities can occur from all of the
tested domain sizes larger than 2 onwards. This is novel in its own right since until now
only impossibilities with |X| ≥ k, k ∈ {3, 4, 6} had been known.
The results themselves differ much in their level of appeal and interestingness. We
can find impossibilities of at least five (potentially overlapping) categories, namely known
results, variations of known results, direct consequences of other results, straightforward
results, and, most importantly, new results.
Some previously known results we can easily recognise among the ones in our list: the
Kannai-Peleg Theorem corresponds to our Impossibility No. 57; an impossibility theorem
by Barberà and Pattanaik (1984) can be found as Impossibility No. 1. Other than that
we are only aware of one more known impossibility under the interpretation of complete
uncertainty, which we could unfortunately not encode in our framework since it uses the
axiom of neutrality (see Section 3.1). It is a variant of the Kannai-Peleg Theorem presented
by Barberà et al. (2004), in which the number of elements has been lowered to four by
adding the aforementioned axiom.
Variations of known results are also easy to spot by just keeping some axioms fixed and
browsing for results involving these. Impossibilities No. 80 and No. 10, for instance, are
variations of the Kannai-Peleg Theorem, where in the former a weakening of the axioms
makes the impossibility occur only at a larger domain size. The latter is a variation in the
other direction: the additional axiom (SUAv) causes an impossibility at a domain size of 4
elements already. And many more such variations of known theorems can be found (e.g.,
No. 33, 37, 40, etc.).
As we used a set of axioms in which certain axioms imply others, we had to expect
results that are just direct consequences of others. In particular, every result involving
some (weak) form of independence will also occur with the standard or strict independence
only, and similarly for simple dominance, which is a weaker form of the Gärdenfors principle.
Examples of such results are the Impossibilities No. 3 (implied by No. 1) and Impossibility
No. 9 (implied by No. 28).
Straightforward results we could only find one: Impossibility No. 2 says that a binary
relation cannot fulfill both (SUAv) and (SUAp), which reflect the contradictory principles of
uncertainty aversion and uncertainty appeal. This is immediate (especially when examining
the exact statement of the axioms).
What we are left with are the new, i.e., previously unknown, results. There are quite
a few of them, but they differ in how interesting they are. For instance, it is not very
reasonable to only postulate (GF1) but not (GF2), which makes the new Impossibility No. 11
not so fascinating after all. But we can also find results like Impossibilities No. 52 or No. 56,
where the combination of axioms appears to be reasonable and yet leads to an impossibility.
We will return to some of these results below. But let us now for a moment shift our
perspective from problems, i.e., combinations of axioms, to the special role of individual
˙ on
axioms with respect to all results. On the one hand, the axiom (LINε ) of a linear order ≥
X occurs in all impossibilities. This means that there is no impossibility without this axiom
(on the given axiom space and up to domain size 8). This could have been anticipated:
˙ then most axioms do not say anything about 
if we use the empty relation on X for ≥,
anymore and hence cannot be incompatible. Also note that the only impossibility without
any form of independence is the (straightforward) Impossibility No. 2. On the other hand,
164

Automated Search for Impossibility Theorems in Social Choice Theory

the axioms (evenExt) and (REFLσ ) do not occur in any impossibility. Therefore, we can
conclude that these must be particularly well-compatible with the other axioms. Or put
differently, that adding them to a given set of axioms does not cause an impossibility. The
axiom (intIND) of intermediate independence is contained in all discovered impossibilities
at domain sizes 7 and 8 (and does not cause any impossibilities at sizes 5 and below). That
this axiom is involved in somewhat larger instances makes a great deal of sense intuitively:
for each application of the axiom we have to add two elements (one above, one below the
set we apply it to), and so it was to be expected that larger domain sizes are necessary for
a contradiction.
In the following we discuss some of the obtained impossibilities and also provide an
example of a manual proof. That we were able to quickly construct manual proofs for all
theorems discussed below underpins the usefulness of our theorem search as a heuristic,
even for a sceptic who may not be willing to accept the output of a SAT solver as a rigorous
proof.13 Knowing the impossible axiom sets and critical domain sizes beforehand simplified
the construction of the manual proofs significantly. Additionally, one can run the search
program again with slightly modified axioms, not only to get an even better understanding
about where the borderline lies between the possible and the impossible, but also to have
some assistance in choosing the right steps when proving the results by hand. And there is
even one more application of our program when searching for a manual proof: one can run
it on instances with single axioms left out and inspect the orders satisfying the remaining
axioms in order to understand which structural properties these imply.
5.2.1 An Unintuitive Impossibility
Let us start with our most striking result. Theorem 3 of an important paper by Bossert
et al. (2000) states that the axioms (SDom), (IND), (SUAv) and (STopMon) characterise the
so-called min-max ordering, which is defined by


˙ max(B) .
˙ min(B) ∨ min(A) = min(B) ∧ max(A) ≥
A mnx B ⇐⇒ min(A) >
The same theorem also covers a dual result for the max-min ordering (characterized by
the axioms (SDom), (IND), (SUAp), (SBotMon)).
The reader can now check that this contradicts the results of our theorem search since
both of these axiom sets are among the impossibility theorems in Table 1 (Impossibilities
No. 16 and 19). Indeed, it turns out that the proofs of Bossert et al. (2000) were flawed
as Arlegi (2003) pointed out three years later. Arlegi, however, only notes that the minmax and max-min orderings do not satisfy the axiom of independence (IND), i.e., that these
orders cannot be characterized by the axioms (SDom), (IND), (SUAv), (STopMon), and (SDom),
(IND), (SUAp), (SBotMon), respectively. This shows the unintuitiveness of our findings (as
the contrary was believed for some time), which yield more than just a counterexample
to the original publication: we additionally get that the four axioms under consideration
are inconsistent (in the presence of transitivity) and hence no transitive binary relation
whatsoever can satisfy them. We now give a manual proof for this result.
13. We only included one example of a manual proof here. The complete set is given by Geist (2010).

165

Geist & Endriss

Theorem 3 (Impossibility No. 16). Let X be a linearly ordered set with |X| ≥ 4. Then there
exists no transitive binary relation  on X satisfying simple dominance (SDom), independence (IND), simple uncertainty aversion (SUAv), and simple top monotonicity (STopMon).
˙
Proof. Let xi , i ∈ {1, 2, 3, 4} denote four distinct elements of X such that are ordered by >
˙ x4 . By way of contradiction, suppose there
˙ x3 >
˙ x2 >
with respect to their index, i.e., x1 >
exists a transitive binary relation  on X satisfying simple dominance (SDom), independence
(IND), simple uncertainty aversion (SUAv), and simple top monotonicity (STopMon).
˙ x3
˙ x2 >
On the one hand, it follows from simple uncertainty aversion applied to x1 >
that {x2 }  {x1 , x3 }, and adding x4 to both sets yields (by independence):
{x2 , x4 }  {x1 , x3 , x4 }.

(5)

˙ x4 ) to show {x3 , x4 } 
On the other hand, we can use simple dominance (applied to x3 >
{x4 }, from which
{x1 , x3 , x4 }  {x1 , x4 }
(6)
˙ x2 >
˙ x4
follows by independence. Furthermore, simple top monotonicity applied to x1 >
directly gives {x1 , x4 }  {x2 , x4 }, which we are able to combine with (6) by transitivity.
We thus obtain
{x1 , x3 , x4 }  {x2 , x4 },
which directly contradicts (5).
Note that all four axioms are not only used in the proof above, but they are necessary
for the result and also logically independent from each other as the following automatically
˙ x2 >
˙ x3 >
˙ x4 ):
constructed examples of weak orders show (let X = {x1 , x2 , x3 , x4 } and x1 >
1. The weak order  given by
{x1 }  {x2 }  {x3 }  {x4 }  {x1 , x2 }  {x1 , x3 }  {x2 , x3 }  {x1 , x4 }  {x2 , x4 } 
{x3 , x4 }  {x1 , x2 , x3 }  {x1 , x2 , x4 }  {x1 , x3 , x4 }  {x2 , x3 , x4 }  {x1 , x2 , x3 , x4 }
satisfies (IND), (SUAv), (STopMon), but not (SDom).
2. The weak order  given by
{x1 }  {x1 , x2 }  {x2 }  {x1 , x3 }  {x2 , x3 }  {x3 }  {x1 , x2 , x3 }  {x1 , x4 } 
{x2 , x4 }  {x1 , x2 , x4 }  {x3 , x4 }  {x4 }  {x1 , x3 , x4 }  {x2 , x3 , x4 }  {x1 , x2 , x3 , x4 }
satisfies (SDom), (SUAv), (STopMon), but not (IND).
3. The weak order  given by
{x1 }  {x1 , x2 }  {x1 , x3 } ∼ {x1 , x2 , x3 }  {x2 }  {x2 , x3 }  {x3 }  {x1 , x4 } ∼
{x1 , x2 , x4 } ∼ {x1 , x3 , x4 } ∼ {x1 , x2 , x3 , x4 }  {x2 , x4 } ∼ {x2 , x3 , x4 }  {x3 , x4 }  {x4 }
satisfies (SDom), (IND), (STopMon), but not (SUAv).
4. The weak order  given by
{x1 }  {x1 , x2 }  {x2 }  {x1 , x3 } ∼ {x1 , x2 , x3 }  {x2 , x3 }  {x3 }  {x1 , x4 } ∼
{x1 , x3 , x4 } ∼ {x2 , x4 } ∼ {x1 , x2 , x4 } ∼ {x2 , x3 , x4 } ∼ {x1 , x2 , x3 , x4 }  {x3 , x4 }  {x4 }
satisfies (SDom), (IND), (SUAv), but not (STopMon).
166

Automated Search for Impossibility Theorems in Social Choice Theory

It can now also be seen that no subset of these four axioms suffices to characterise the
min-max ordering. Any subset containing (IND) can be rejected immediately since (IND) is
violated by the min-max ordering (as we have noted earlier), and any subset not containing
it cannot suffice for a characterisation either, since example 2 differs from the min-max
ordering mnx (in which {x2 , x3 , x4 } ≺mnx {x1 , x2 , x3 , x4 }).
Finally, we emphasise the fact that neither reflexivity nor completeness of  are used
in the proof of Theorem 3 (as also indicated by Table 1). Thus, the impossibility already
holds for arbitrary transitive binary relations instead of weak orders.
5.2.2 Variations of the Kannai-Peleg Theorem
Impossibility No. 9 offers an interesting variation of the Kannai-Peleg Theorem that trades
an additional axiom (simple uncertainty aversion) for the impossibility occurring at a domain size of 4 rather than 6 elements.
Theorem 4 (Impossibility No. 9). Let X be a linearly ordered set with |X| ≥ 4. Then
there exists no transitive binary relation  on X satisfying the Gärdenfors principle (GF),
independence (IND) and simple uncertainty aversion (SUAv).
The same impossibility result also holds with simple uncertainty appeal (SUAp) in place
of simple uncertainty aversion (SUAv); this is Impossibility No. 12.
When we have an even closer look at Table 1, then we can see that there is an even
stronger form of Theorem 4: Impossibility No. 28 corresponds to the the axioms (GF),
(SUAv), (botIND), (topIND) and is impossible from domain size 4 on. In contrast to (IND),
the axioms (botIND) and (topIND) allow the principle of independence in certain situations
only: the element to be added has to be ranked below or above all the elements in both sets,
respectively.14 Therefore, we immediately have the following stronger version of Theorem 4.
Theorem 5 (Impossibility No. 28). Let X be a linearly ordered set with |X| ≥ 4. Then there
exists no transitive binary relation  on X satisfying the Gärdenfors principle (GF), bottom
(botIND) as well as top independence (topIND), and simple uncertainty aversion (SUAv).
An interesting insight can be obtained from comparing Impossibility No. 48 to the
previous result. It shows us that we can drop the second Gärdenfors axiom if we add just
one element to the domain, i.e., we have |X| ≥ 5. The exact result is the following:
Theorem 6 (Impossibility No. 48). Let X be a linearly ordered set with |X| ≥ 5. Then
there exists no transitive binary relation  on X satisfying the first axiom of the Gärdenfors
principle (GF1), bottom (botIND) as well as top independence (topIND), and simple uncertainty aversion (SUAv).
Alternatively, we could have replaced (botIND) by (disIND) (No. 51), or (topIND) by
(intIND), then, however, requiring at least seven elements in the domain (No. 78).
Two further variants of the Kannai-Peleg Theorem can be found in Impossibilities No. 80
and 81, which can be considered strengthenings of the original theorem as they contain
weaker versions of independence only. The strengthening, however, comes at the cost of
14. Actually, already in the original proof of the Kannai-Peleg Theorem (Kannai & Peleg, 1984) only these
weaker forms of (IND) are used (cf. also Impossibility No. 61).

167

Geist & Endriss

the impossibility starting from a domain size of eight elements instead of six. The form of
independence that remains is a combination of intermediate, disjoint, and bottom or top
independence, respectively, which (even together) are weaker than standard independence.
5.2.3 Impossibilities without Dominance
All existing impossibilities in the literature we are aware of involve the Gärdenfors principle
(GF) or at least simple dominance (SDom). So let us now consider what kinds of results we
can obtain without any dominance principle.
A striking impossibility without any principle of dominance —i.e., without either (GF)
or (SDom)— is Impossibility No. 52: the axioms of strict independence (strictIND), simple uncertainty aversion (SUAv), and monotone consistency (MC) are incompatible in the
presence of completeness and transitivity from domain size 5 on.
Theorem 7 (Impossibility No. 52). Let X be a linearly ordered set with |X| ≥ 5. Then there
exists no weak order  on X satisfying strict independence (strictIND), simple uncertainty
aversion (SUAv) and monotone consistency (MC).
One might be tempted to think that this impossibility is mostly due to problems between
(SUAv) and (MC) since they seem to express contrary ideas: whereas (SUAv) favours small sets
over large ones, (MC) tells us that unions of two sets should be preferred to at least one of the
sets. But actually there is even a characterisation result of the min-max ordering by Arlegi
(2003) involving both axioms (SUAv) and (MC), demonstrating that this natural ordering
fulfils these two axioms. Therefore, we can see that it should not at all be considered
unreasonable to have both axioms act together.
We have found quite a few variants of this impossibility. According to our results,
completeness could be replaced by simple bottom monotonicity (Impossibility No. 53) or
even be dropped at the price of having one more element in the domain (Impossibility
No. 56). Alternatively, one can weaken strict independence to either bottom or disjoint
independence and shrink the domain by one element, at the price of adding the axiom of
simple top monotonicity (Impossibilities No. 26 and 27, respectively). A seemingly further
variant can be obtained from trading the axiom of extension (EXT) for a smaller domain. It
is, however, a direct consequence of Impossibilities No. 26 and 27, respectively, since (EXT)
and (strictIND) together imply (STopMon).
Since strict independence can be considered a relatively strong axiom, Impossibility
No. 26 (and the corresponding No. 27) is worth emphasising as well, as it does only postulate
a very weak form of independence.
Theorem 8 (Impossibility No. 26). Let X be a linearly ordered set with |X| ≥ 4. Then
there exists no transitive binary relation  on X satisfying bottom independence (botIND),
simple uncertainty aversion (SUAv), simple top monotonicity (STopMon) and monotone consistency (MC).
This result comes as quite a surprise since Arlegi (2003) characterises the min-max
ordering by an axiom set including (SUAv), (STopMon), and (MC) (as well as two further
axioms). It follows that adding just a tiny bit of independence to these three axioms turns
their possibility into a general impossibility.
168

Automated Search for Impossibility Theorems in Social Choice Theory

6. Conclusion
We have presented a method for automatically verifying and discovering theorems in a
subarea of economic theory concerned with the problem of formulating principles for lifting
preferences over individual objects to preferences over nonempty sets of those objects. The
theorems in question are impossibility theorems that establish that certain combinations of
those principles (called axioms) are inconsistent. Our method has three components:
• A general result, the universal reduction step (a corollary of our Preservation Theorem), that shows that if a combination of axioms, meeting certain conditions, is
inconsistent for a fixed domain size n, then it is also inconsistent for any domain with
more than n objects. The conditions on axioms for the applicability of this result are
purely syntactic: any axiom that is (equivalent to) an existentially set-guarded (ESG)
sentence in the many-sorted logic for set preferences (MSLSP) qualifies.
• A method for translating axioms into propositional formulas in CNF, and a method
for instantiating those axioms on a computer for a fixed domain size. This allows us
to verify small instances of an impossibility theorem using a SAT solver. Together
with the universal reduction step, this then constitutes a proof of the respective impossibility theorem also for all larger domain sizes.
• A scheduling algorithm to search a large space of axiom combinations for different
domain sizes. This, finally, allows us to systematically search for and discover new
impossibility theorems.
We have applied our method to a set of 20 axioms that have been proposed in the
literature as a means of formalising various principles for ranking sets of objects when those
sets are interpreted as representing mutually exclusive alternatives from which an object will
be selected in a manner that cannot be influenced by the decision maker (so-called complete
uncertainty). This did yield a total of 84 (minimal) impossibility theorems, including both
known results and new theorems. We have commented on the most interesting of these in
the previous section. These results clearly demonstrate the power of our method.
This work can be extended in a number of ways. First, our method can be applied to
other sets of axioms (including axioms for order types other than linear and weak orders).
Implementing further axioms can be done quickly, and as long as they are covered by our
universal reduction step, results can be read off after a short computation. Especially
for opportunity sets, for which to our knowledge no impossibility results are known, the
potential for success is very high.
Second, the method itself and its implementation can be refined further. It would
be attractive to integrate a parser that can read our language MSLSP so that axioms
no longer have to be transformed and coded by hand. A further idea is to implement
dependencies between the axioms. This would make sure that only absolutely minimal
results are returned, whereas now some of our results are trivial consequences of others
(since some of our axioms are immediately implied by others).
Third, in case a particular combination of axioms does not lead to an impossibility, it
may be possible to use the output of the SAT solver to infer useful information about the
class of set preference orderings satisfying those axioms. Some preliminary steps in this
direction have already been taken (Geist, 2010).
169

Geist & Endriss

Finally, and this is our most tentative suggestion, it would be interesting to explore to
what extent our method can be adapted to different disciplines and problem domains. A
starting point might be our Preservation Theorem, which potentially can still be strengthened to a larger class of axioms. One could try to find out where the exact borderline lies
between formulas that are preserved in certain substructures and those that are not. For
arbitrary first-order models this has been done in the famous Loś-Tarski Theorem, but for
our class of structures for set preferences it is still an open question.

Acknowledgments
We would like to thank Umberto Grandi and three anonymous JAIR reviewers for a host
of helpful comments and suggestions on earlier versions of this paper.

Appendix A. List of Axioms
In this appendix we provide the complete list of the axioms used in the theorem search
presented in Section 5. These axioms (or variations thereof) and further references can all
be found in the survey by Barberà et al. (2004).
The first axioms given here are the order axioms. For one, there are the axioms describ˙ on X, for another, the ones describing a weak order  on X = 2X \{∅}.
ing the linear order ≥
The former will just be denoted by (LINε ), whereas the latter are split up into their three
components reflexivity (REFLσ ), completeness (COMPLσ ) and transitivity (TRANSσ ), which are
then treated as separate axioms in order to investigate which parts are actually necessary
for impossibilities. The axioms in their intuitive form are:
(LINε )

(REFLσ )

˙ x for all x ∈ X
x≥
˙ y∨x≤
˙ y for all x 6= y ∈ X
x≥
˙ y∧y ≥
˙ z⇒x≥
˙ z for all x, y, z ∈ X
x≥
˙ y∧y ≥
˙ x ⇒ x = y for all x, y ∈ X
x≥
A  A for all A ∈ X

(reflexivity)
(completeness)
(transitivity)
(antisymmetry)
(reflexivity)

(COMPLσ )

A  B ∨ A  B for all A 6= B ∈ X

(TRANSσ )

A  B ∧ B  C ⇒ A  C for all A, B, C ∈ X

(completeness)
(transitivity)

Next we have the axiom of extension, which is a very natural requirement and thus also
implied by some other axioms (e.g., the Gärdenfors principle):
(EXT)

˙ y ⇐⇒ {x}  {y} for all x, y ∈ X
x≥

A further set of axioms we included in our search is the one dealing with the concept of
dominance, i.e., the idea that adding an object x to a set of objects A that are all dominated
by (or dominating) the object x produces a better (or worse) set, respectively. We chose for
the well-known Gärdenfors principle (GF), which was introduced in Section 2.2 already, as
170

Automated Search for Impossibility Theorems in Social Choice Theory

well as a weaker version by Barberà (1977) called simple dominance (SDom), which restricts
(GF) to small sets:
(GF1)
(GF2)
(SDom)

˙ a) ⇒ A ∪ {x}  A for all x ∈ X and A ∈ X
((∀a ∈ A)x >
˙ a) ⇒ A ∪ {x} ≺ A for all x ∈ X and A ∈ X
((∀a ∈ A)x <
˙ y ⇒ ({x}  {x, y} ∧ {x, y}  {y}) for all x, y ∈ X
x>

Independence axioms are also commonly postulated and especially their weaker variants
or versions thereof, like bottom, top, disjoint and intermediate independence, frequently
play a role in characterisation results (e.g., see Pattanaik & Peleg, 1984; Nitzan & Pattanaik, 1984). We decided to include standard independence (as already introduced in
Section 2.2), a stronger version (strictIND), which implies strict preferences, and a few
weaker versions, viz. bottom (botIND), top (topIND), disjoint (disIND) and intermediate
independence (intIND), which only apply to certain combinations of sets and elements.
(IND)

A  B ⇒ A ∪ {x}  B ∪ {x} for all A, B ∈ X and x ∈ X \ (A ∪ B)

(strictIND)

A  B ⇒ A ∪ {x}  B ∪ {x} for all A, B ∈ X and x ∈ X \ (A ∪ B)

(botIND)

A  B ⇒ A ∪ {x}  B ∪ {x} for all A, B ∈ X
˙ x for all y ∈ A ∪ B
and x ∈ X \ (A ∪ B) such that y >

(topIND)

A  B ⇒ A ∪ {x}  B ∪ {x} for all A, B ∈ X
˙ y for all y ∈ A ∪ B
and x ∈ X \ (A ∪ B) such that x >

(disIND)

A  B ⇒ A ∪ {x}  B ∪ {x} for all A, B ∈ X ,
such that A ∩ B = ∅, and for all x ∈ X \ (A ∪ B)

(intIND)

A  B ⇒ A ∪ {x, y}  B ∪ {x, y} for all A, B ∈ X and x, y ∈ X \ (A ∪ B)
˙ z and z >
˙ y for all z ∈ A ∪ B
such that x >

Bossert (1997) introduced axioms describing the attitude of the decision maker towards
uncertainty. We formalise weakenings of these axioms that apply to small sets only, since
these are sufficient for characterisation results like those of Arlegi (2003). Uncertainty
aversion postulates that the decision maker will, for any alternative x, (strictly) prefer this
alternative to a set containing both a better and a worse alternative. Uncertainty appeal,
on the other hand, says that the ranking has to be just the other way around: the set with
a better and a worse element is (strictly) preferred to the single element x.
(SUAv)
(SUAp)

˙ y∧y
(x >
˙ y∧y
(x >

˙ z) ⇒ {y}  {x, z} for all x, y, z ∈ X
>
˙ z) ⇒ {x, z}  {y} for all x, y, z ∈ X
>

Arlegi (2003) also uses two monotonicity axioms, called simple top and bottom monotonicity. The underlying idea is simple: given two alternatives, it is better to get the better
one of the two together with some third element (instead of the worse one with the same
third element). The two variants of the axiom then only apply to alternatives that are
ranked higher (top) than the third alternative, or ranked lower (bottom), respectively.
(STopMon)
(SBotMon)

˙ y ⇒ {x, z}  {y, z} for all x, y, z ∈ X such that x >
˙ z and y >
˙ z
x>
˙ z ⇒ {x, y}  {x, z} for all x, y, z ∈ X such that x >
˙ y and x >
˙ z
y>
171

Geist & Endriss

A rather odd axiom is the principle of even-numbered extension of equivalence. It says
that, for all sets with an even number of elements, if the decision maker is indifferent about
whether this set is added to each of two distinct singleton sets, then she should also be
indifferent about whether it is added to the union of the two singleton sets. Even though it
lacks intuitive support, this axiom is useful because (together with a few other principles)
it characterises a median-based ordering proposed by Nitzan and Pattanaik (1984).
(evenExt)

(A ∪ {x} ∼ {x} ∧ A ∪ {y} ∼ {y}) ⇒ A ∪ {x, y} ∼ {x, y}
for all A ∈ X , such that |A| is even, and for all x, y ∈ X \ A

The final axiom in our list is monotone consistency (MC), which was put forward by
Arlegi (2003) to characterise (in connection with other axioms) the min-max ordering (see
also Section 5.2). (MC) expresses that if a set of objects A is at least as good as another
set B, then the union of the two is at least as good as the latter. This implies —and for
complete binary relations is equivalent to— the potentially worse set B not being strictly
better than the union of the two. Intuitively, it means that after adding the alternatives of
the (weakly preferred) set A to the set B, the decision maker maintains the alternatives she
had in B plus the ones that were contained in A, which was weakly preferred to B. Thus,
this process should not produce a set that is strictly worse than B.
(MC)

A  B ⇒ A ∪ B  B for all A, B ∈ X

Although (MC) appears to be similar to the first axiom of the Gärdenfors principle, it is in
fact quite different since it does not dictate the existence of any strict preferences.

References
Ågotnes, T., van der Hoek, W., & Wooldridge, M. (2010). On the logic of preference and
judgment aggregation. Journal of Autonomous Agents and Multiagent Systems. (In
press)
Arlegi, R. (2003). A note on Bossert, Pattanaik and Xu’s “Choice under complete uncertainty: axiomatic characterization of some decision rules”. Economic Theory, 22 (1),
219–225.
Arrow, K. J. (1963). Social choice and individual values. Yale University Press, New Haven.
Cowles Foundation Monograph 12.
Barberà, S. (1977). The manipulation of social choice mechanisms that do not leave “too
much” to chance. Econometrica, 45 (7), 1573–1588.
Barberà, S., Bossert, W., & Pattanaik, P. K. (2004). Ranking sets of objects. In S. Barberà,
P. J. Hammond, & C. Seidl (Eds.), Handbook of utility theory (Vol. II: Extensions,
pp. 893–977). Kluwer Academic Publishers, Dordrecht.
Barberà, S., & Pattanaik, P. K. (1984). Extending an order on a set to the power set:
Some remarks on Kannai and Peleg’s approach. Journal of Economic Theory, 32 (1),
185–191.
Ben Larbi, R., Konieczny, S., & Marquis, P. (2010). A characterization of optimality criteria
for decision making under complete ignorance. In Proceedings of the 12th International
Conference on the Principles of Knowledge Representation and Reasoning (KR-2010).
AAAI Press.
172

Automated Search for Impossibility Theorems in Social Choice Theory

Biere, A. (2010). PrecoSAT. Available from http://fmv.jku.at/precosat/.
Bossert, W. (1997). Uncertainty aversion in nonprobabilistic decision models. Mathematical
Social Sciences, 34 (3), 191–203.
Bossert, W., Pattanaik, P. K., & Xu, Y. (2000). Choice under complete uncertainty:
Axiomatic characterizations of some decision rules. Economic Theory, 16 (2), 295–
312.
Chevaleyre, Y., Endriss, U., Lang, J., & Maudet, N. (2007). A short introduction to
computational social choice. In Proceedings of the 33rd Conference on Current Trends
in Theory and Practice of Computer Science (SOFSEM-2007) (pp. 51–69). SpringerVerlag.
DIMACS. (1993). DIMACS satisfiability suggested format. Available from ftp://
dimacs.rutgers.edu/pub/challenge/satisfiability/doc/satformat.dvi. Center for Discrete Mathematics & Theoretical Computer Science.
Duggan, J., & Schwartz, T. (2000). Strategic manipulation without resoluteness or shared
beliefs: Gibbard-Satterthwaite generalized. Social Choice and Welfare, 17 (1), 85–93.
Enderton, H. B. (1972). A mathematical introduction to logic. Academic Press.
Fishburn, P. C. (1972). Even-chance lotteries in social choice theory. Theory and Decision,
3 (1), 18–40.
Gaertner, W. (2009). A primer in social choice theory: Revised edition. Oxford University
Press, USA.
Gärdenfors, P. (1976). Manipulation of social choice functions. Journal of Economic
Theory, 13 (2), 217–228.
Gärdenfors, P. (1979). On definitions of manipulation of social choice functions. In J. J. Laffont (Ed.), Aggregation and revelation of preferences (pp. 29–36). North-Holland.
Geist, C. (2010). Automated search for impossibility theorems in choice theory: Ranking sets
of objects. M.Sc. thesis. Institute of Logic, Language and Computation. University of
Amsterdam.
Grandi, U., & Endriss, U. (2009). First-order logic formalisation of Arrow’s Theorem. In
Proceedings of the 2nd International Workshop on Logic, Rationality and Interaction
(LORI-2009) (pp. 133–146). Springer-Verlag.
Gravel, N., Marchant, T., & Sen, A. (2008). Ranking completely uncertain decisions by the
uniform expected utility criterion. Presented at the 3rd World Congress of the Game
Theory Society, Evanston, IL.
Hodges, W. (1997). A shorter model theory. Cambridge University Press.
Kannai, Y., & Peleg, B. (1984). A note on the extension of an order on a set to the power
set. Journal of Economic Theory, 32 (1), 172–175.
Lin, F. (2007). Finitely-verifiable classes of sentences. Presented at the 8th International
Symposium on Logical Formalizations of Commonsense Reasoning, Stanford, CA.
Manzano, M. (1996). Extensions of first-order logic. Cambridge University Press.
Nipkow, T. (2009). Social choice theory in HOL. Journal of Automated Reasoning, 43 (3),
289–304.
Nitzan, S. I., & Pattanaik, P. K. (1984). Median-based extensions of an ordering over a
set to the power set: An axiomatic characterization. Journal of Economic Theory,
34 (2), 252–261.
173

Geist & Endriss

Packard, D. J. (1979). Preference relations. Journal of Mathematical Psychology, 19 (3),
295–306.
Pattanaik, P. K., & Peleg, B. (1984). An axiomatic characterization of the lexicographic
maximin extension of an ordering over a set to the power set. Social Choice and
Welfare, 1 (2), 113–122.
Puppe, C. (1995). Freedom of choice and rational decisions. Social Choice and Welfare,
12 (2), 137–153.
SAT Research Group, Princeton University. (2007). zChaff. Available from http://
www.princeton.edu/∼chaff/zchaff.html.
Sen, A. (1991). Welfare, preference and freedom. Journal of Econometrics, 50 (1-2), 15–29.
Sloane, N. J. A. (Ed.). (2010). The on-line encyclopedia of integer sequences (OEIS).
Published electronically at http://www.research.att.com/∼njas/sequences/.
Tang, P. (2010). Computer-aided theorem discovery — a new adventure and its application
to economic theory. Ph.D. thesis. Hong Kong University of Science and Technology.
Tang, P., & Lin, F. (2009). Computer-aided proofs of Arrow’s and other impossibility
theorems. Artificial Intelligence, 173 (11), 1041–1053.
Taylor, A. D. (2002). The manipulability of voting systems. The American Mathematical
Monthly, 109 (4), 321–337.
Wiedijk, F. (2007). Arrow’s Impossibility Theorem. Formalized Mathematics, 15 (4), 171–
174.

174

Journal of Artificial Intelligence Research 40 (2011) 375–413

Submitted 06/10; published 02/11

Evaluating Temporal Graphs Built from Texts via Transitive Reduction
Xavier Tannier

XTANNIER @ LIMSI . FR

LIMSI-CNRS and Univ. Paris-Sud
B.P. 133
91403 ORSAY Cedex, France

Philippe Muller

MULLER @ IRIT. FR

ALPAGE-INRIA and Toulouse University
IRIT, Univ. Paul Sabatier
118 Route de Narbonne
F-31062 Toulouse Cedex 04, France

Abstract
Temporal information has been the focus of recent attention in information extraction, leading to
some standardization effort, in particular for the task of relating events in a text. This task raises the
problem of comparing two annotations of a given text, because relations between events in a story
are intrinsically interdependent and cannot be evaluated separately. A proper evaluation measure
is also crucial in the context of a machine learning approach to the problem. Finding a common
comparison referent at the text level is not obvious, and we argue here in favor of a shift from eventbased measures to measures on a unique textual object, a minimal underlying temporal graph, or more
formally the transitive reduction of the graph of relations between event boundaries. We support it by
an investigation of its properties on synthetic data and on a well-know temporal corpus.

1. Introduction
Temporal processing of texts is a somewhat recent field from a methodological point of view, even
though temporal semantics has a long tradition, dating back at least to the 1940’s (Reichenbach, 1947).
While theoretical and formal linguistic approaches to temporal interpretation at the discourse level
have been very active in the late 1980s and early 1990s (Kamp & Reyle, 1993; Asher & Lascarides,
1993; Steedman, 1995; Webber, 1988), empirical approaches were less frequent, and very few natural
language processing systems were evaluated beyond a few instances (Grover, Hitzeman, & Moens,
1995; Kameyama, Passonneau, & Poesio, 1993; Passonneau, 1988; Song & Cohen, 1991).
Temporal information being essential to the interpretation of a text and thus crucial in applications
such as summarization or information extraction, it has received growing attention in the 2000s (Mani,
Pustejovsky, & Gaizauskas, 2005) and has lead to some standardization effort through the TimeML
initiative (Saurí, Littman, Knippen, Gaizauskas, Setzer, & Pustejovsky, 2006). We address here a central part in this task, namely evaluating the extraction of the network of temporal relations between
events described in a text. Since temporal information is not easily broken down into local bits of information, there are many equivalent ways to express the same ordering of events. Human annotation
is thus notoriously difficult (Setzer, Gaizauskas, & Hepple, 2006) and comparisons between annotations cannot rely on simple precision/recall-type measures. The given practice nowadays has been
to compute some sort of transitive closure over the network/graph of constraints on temporal events
(usually expressed in the well-known Allen algebra (Allen, 1983), or a sub-algebra), and then either
to compare the sets of simple temporal relations that are deduced from it with standard precision and
recall, or to measure the agreement between all relations, including disjunctions of information (Verhagen, Gaizauskas, Schilder, Hepple, Katz, & Pustejovsky, 2007). This reasoning model is also used
c
2011
AI Access Foundation. All rights reserved.

TANNIER & M ULLER

before

X
Y
X
during

Y

X
meets

Y
starts

X
Y

overlaps X

Y

X
equals

X
finishes

Y

Y

Figure 1: Allen relations. Each relation r has an inverse relation ri.
to help build representations of temporal situations by imposing global constraints on top of local
decision problems (Chambers & Jurafsky, 2008a; Tatu & Srikanth, 2008; Bramsen, Deshpande, Lee,
& Barzilay, 2006).
We take a different route here, by extracting a single referent graph, a minimal graph of constraints. There are a number of ways of doing this and we argue for basing it on the graph of relations
between event boundaries. We aim to accomplish two things by doing so: to find a graph that is
easy to compute, and to eliminate a bias introduced by measures that do not take into account the
combinatorial aspect of agreement on transitive closure graphs.
The next section presents in more detail the usual way of comparing annotation graphs between
temporal entities extracted from a text, and the problems it raises. Then we argue for comparing event
boundaries instead of events and define two new metrics that apply to that type of information. We
focus on convex relations, a tractable sub-algebra of Allen relations which covers human annotations.
Finally, we present an empirical study of the behavior of these measures on generated data and on
the TimeBank Corpus (Pustejovsky, Hanks, Saurí, See, Gaizauskas, Setzer, Radev, Sundheim, Day,
Ferro, & Lazo, 2003) to support our claim of the practicality of this methodology.

2. Comparing Temporal Constraint Networks
Work on temporal annotation of texts strongly relies on Allen’s interval algebra. Allen represents
time and events as intervals, and states that 13 basic relations can hold between these intervals (see
Figure 1 and Table 1), by considering every possible ordering of the interval endpoints. These binary
relations, existing amongst all intervals of a collection (in our case, corresponding to temporal entities
in a text), define a graph where nodes are the intervals and where edges are labelled with the set of
relations which may hold between a pair of nodes. The relations are mutually exclusive. The TimeML
specification for linguistic temporal annotation uses Allen relations under different names, and other
projects either use a subset or groupings of these relations (see below).
We are interested in this paper in evaluating systems annotating texts by temporal relations holding
between events or between temporal expressions and events. For example, consider the following text,
extracted from the TimeBank corpus:

376

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

Relation
I bJ
I mJ
I oJ
I sJ
I dJ
IfJ
I = J

Meaning
I before J
I meets J
I overlaps J
I starts J
I during J
I finishes J
I equals J

Endpoint relations
I2 < J1
I2 = J1
(I1 < J1 ) ∧ (I2 < J2 ) ∧ (J1 < I2 )
(I1 = J1 ) ∧ (I2 < J2 )
(J1 < I1 ) ∧ (I2 < J2 )
(J1 < I1 ) ∧ (I2 = J2 )
(I1 = J1 ) ∧ (I2 = J2 )

Inverse relation
bi
mi
oi
si
di
fi

Table 1: Allen relations. Each relation r has an inverse relation ri. An interval I starts at I1 and ends
at I2 .

(1)

It wasn’t until twenty years after the first astronauts were chosene1 that NASA finally includede2 six women, and they were all scientists, not pilots. No woman has actually been in
charge of a missione3 until nowt1 .

A correct annotation of temporal relations could be given by the graph shown in Figure 2. Other
relations could be explicited, i.e. e1 bt1 , and a complete evaluation could consider all possible edges.

chosen_e1

b

included_e2

b
fi

now_t1

in charge of a mission_e3

Figure 2: Example annotation for example 1.
Precision and recall evaluations are often not performed on graphs of relations between all events
in a text however, but on the subproblem of ordering pairs of successively described events (Mani &
Schiffman, 2005; Verhagen et al., 2007) or even same-sentence events (Lapata & Lascarides, 2006)
(in our example, only e1 b e2 and e3 f i t1 1 ). The main reason of this choice is the difficulty of the task,
even for human beings, of assigning temporal relations in a large text (Setzer et al., 2006). Another
issue is that evaluation of full temporal graphs is an open question. As will be further discussed in
this section, metrics traditionally used for the task, namely recall and precision metrics (either strict
or relaxed), raise specific problems that are still to be addressed.
We detail now important notions concerning temporal networks and the comparison of these networks. All example relations given in this section are expressed in terms of Allen algebra, whose
set of relations and their abbreviations are recalled in Table 1. Here, we use the classical symbols <
and > for order on temporal points.
2.1 Temporal Closure
Temporal closure is an inferential closure mechanism that consists in composing known pairs of temporal relations in order to obtain new relations, up to a fixed point. E.g.: if A b B and C d B, then
1. Exceptions exist, as in the work of Mani et al. (2006) and Mani et al. (2007).

377

TANNIER & M ULLER

%
b
bi
d
di

b
b
all
b
{b, o, m, di, fi}

bi
all
bi
bi
{bi, oi, di, mi, si}

d
{b, o, m, d, s }
{bi, oi, mi, d, f}
d
{o, oi, d, s, f, di, si, fi, =}

di
b
bi
all
di

Table 2: Composition between a few Allen relations.
A b C; the operation can lead to a disjunction of relations, for example if A b B and B d C then
A b C ∨ A o C ∨ A m C ∨ A d C ∨ A s C. This can also be noted as A{b, o, m, d, s}C.
If we consider generalized relations, i.e. the set R of disjunctions of basic temporal relations,
each seen as a set of base relations, then set union and intersection and the composition of relations
define an algebra on R (the algebra of the subsets of the set of all Allen relations). Composition of
relations is the operation that generalizes inferences from basic relations to sets of relations. Taking
the previous example, if now A T B and B S C, with T = {t1 , t2 , ...tk } and S = {s1 , s2 , ...sm }, ti ,
si being base relations:
[
T ◦ S = {t1 , t2 , ...tk } ◦ {s1 , s2 , ...sm } = (ti ◦ sj )
i,j

So any composition of relations can be computed from the 13x13 compositions of base relations.
A table of all composition rules in Allen algebra can be found in the work of Allen (1983) or Rodriguez et al. (2004), and a sample for a few basic relations is given in Table 2. These new relations
do not express new intrinsic constraints, but make the temporal situation more explicit. They can also
make some information more precise, as disjunctions inferred about the same edge are intersected to
combine inferences from different compositions.
A constraint propagation algorithm ensures that all existing temporal relations are added to the
network, labelling an inconsistency with ∅ (Allen, 1983). This path-consistency algorithm is sound,
but not complete, as it does not detect all cases of inconsistency. See the simple version presented in
Algorithm 1. More efficient versions have also been developed (Vilain, Kautz, & van Beek, 1990),
and will be put to use in our experiments, but it is not our main focus here.
It is not desirable to compare temporal graphs without performing a temporal closure on them.
Indeed, there are several ways to encode the same temporal information in a graph, as shown in the
(very simple) example of Figure 3. Closure can be seen as a computationally simple way of expliciting
the temporal information from an annotation, allowing for more precise comparisons. But temporal
closure also produces redundant information, which can lead to evaluation issues, as will be explained
in Section 2.4.
In this paper, we call G∗ the temporal closure (also called a saturated graph) of a graph G.
=

A
<

B
<

C

=

A

↔

<

B
<

C

=

A

↔

<

B
<

C

Figure 3: Three identical annotations. The last one is the result of temporal closure.

378

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

Algorithm 1 Temporal closure
Let U = the disjunction of all 13 Allen relations,
Rm,n = the current relation between nodes m and n
procedure CLOSURE(G)
A=G.edges()
N=G.vertices()
changed = True
while changed do
changed = False
for all pairs of nodes (i, j) ∈ N × N do
for all k ∈ N such that ((i, k) ∈ A ∧ (k, j) ∈ A) do
. composition via k
R1i,j = (Ri,k ◦ Rk,j )
. to find info about (i,j)
if no edge (a relation R2i,j ) existed before between i and j then R2i,j = U
end if
Ri,j = R1i,j ∩ R2i,j
. intersect new with already known
if Ri,j = ∅ then error
. inconsistency detected
else if Ri,j = U then do nothing
. no new information
else
update edge (i,j)
changed = True
end if
end for
end for
end while
end procedure

379

TANNIER & M ULLER

A1

<

A2
<

=

=

C1

<

C2

<
B1

<

B2

Figure 4: Endpoint graph (same temporal information as Figure 3).
2.2 Time Point Algebra and Convex Relations
Interval graphs can be converted easily into graphs between points (Vilain et al., 1990), where an event
is split into a beginning and an ending point; the mapping between Allen relations and point relations
is given at Table 1. This leads to a smaller set of simple relations: equality (=) and precedence
(< and >), and a simpler algebra, with only 7 consistent sets ({<},{<, =},{=},{<, =, >},{>},{>=
}, {<, >} where a set denotes a disjunction of relations). The point algebra is obtained by the four
relations r1 ... r4 that hold between the endpoints of the two intervals I and J started by Ib and Jb
and ended by Ie and Je respectively. These four relations are Ib r1 Jb , Ie r2 Je , Ib r3 Je and Ie r4 Jb .
Converting an Allen graph into an endpoint graph is thus straightforward. Figure 4 shows a point
graph equivalent to the interval graph of Figure 3.
As with the interval algebra, pairs of point relations can be combined and the temporal closure
can be computed in the same way. The relation between two time points is continuous if the assigned
set of simple relations is convex (Vilain et al., 1990).
A so-called convex relation corresponds to cases where relations r1 to r4 are assigned to one of
the 6 possible relations {<}, {<, =}, {=}, {<, =, >}, {>}, {>=}2 , considered as conceptual neighbors (Freksa, 1992). Using only these relations between endpoints restricts interval relations to sets
of Allen relations that are conceptual neighbors. This means that they encode relations that may
be vague but in which intervals endpoints can only be in convex subsets of the time-line. Figure 5
shows which Allen relations are conceptual neighbors. Another useful way of seeing these conceptual neighbors is by considering continuous transformations of an interval endpoints on the time-line:
when a relation r holds between two intervals I1 and I2 , moving continuously their endpoints can
only change the relation to a conceptual neighbor of r. For instance, such a conceptual transformation
cannot change a situation where I1 starts I2 to a situation where I1 < I2 without going through (at
least) intermediary situations I1 overlaps I2 and I1 meets I2 .
Finally, instead of 213 possible disjunctive relations in Allen algebra, the set of corresponding
interval convex relations is reduced to 82. The corresponding sub-algebra is tractable, that is the problem of the satisfiability of a set of constraints has a sound and complete polynomial time algorithm3 .
Moreover, it will ensure the uniqueness of our minimal graphs, that will be defined and described in
this paper.
It is important to note that a temporal graph built from an annotated text contains only convex
relations, since the graph is generated from a finite set of base relations (annotators are not allowed to
2. The 7 relations described above, except {<, >}, also noted 6=. These 6 relations form a sub-algebra: all compositions
of disjunctions of these relations are disjunctions of these relations.
3. See the work by Schilder (1997) for a more complete presentation within a natural language processing perspective.

380

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

d

s

m

<

o

f

oi

eq

fi

di

mi

>

si

Figure 5: Temporal relations which are conceptual neighbors
use disjunctions), and since the set of all convex relations is stable under composition and thus forms
a sub-algebra.
2.3 Strict and Relaxed Recall and Precision
In the general case, both humans and systems may assign disjunctions of atomic relations between
two events (i.e. A b B ∨ A m B), directly or indirectly (after saturation). This is a way to reduce
vagueness even if the exact relation is not known.
The presence of disjunctions raises the question of how to score relations that are only partly
correct, like A b B ∨ A m B instead of A b B or the reverse. In response to that issue, different
variations of usual precision and recall measures have been proposed.
A strict measure only counts exact matching as success, and will for example score 0 for the latter
example.
However, it can be argued that an evaluation measure should take better account of “close matches”.
For example, suppose that the gold standard relation between A and B is A b B. If the system chooses
the disjunction A b B ∨ A m B, it must be rewarded less than A b B but more than A > B or nothing.
The system is vaguer but correct, as its annotation is a logical consequence of the standard annotation.
We proposed (Muller & Tannier, 2004) such a gradual measure that we might call “temporal”
precision and recall. If Si,j is the (possibly disjunctive) relation between i and j given by the system
and Ki,j the (possibly disjunctive) gold standard, then:
Ptemp i,j =

Card( Si,j ∩ Ki,j )
Card( Ki,j )

Rtemp i,j =

Card( Si,j ∩ Ki,j )
Card( Si,j )

With Card(Gi,j ) = the number of atomic relations present in the disjunction. Thus, in our example, the system S1 that answered before ∨ overlaps between i and j will get:
Ptemp i,j,S1 =

Card({b, o} ∩ {b} )
=1
Card( {b} )

Rtemp i,j,S1 =

Card({b, o} ∩ {b} )
1
=
Card({b, o})
2

While S2 that answered after will get Ptemp i,j,S2 = Rtemp i,j,S2 = 0.

381

TANNIER & M ULLER

The final precision (resp. recall) is the average on the number of relations given by the system
(resp. by the reference):
j=n
i=n X
X

Ptemp =

j=n
i=n X
X

Ptemp i,j
i=1 j=i+1
Card(S)

Rtemp =

Rtemp i,j
i=1 j=i+1
Card(K)

Similar measures have also been used during the TempEval evaluation campaign in 2007 (Verhagen et al., 2007), with a reduced set of relations : “before”, “overlaps” and “before or overlaps”.
These measures were called relaxed recall and precision. We will use the words strict and relaxed to
designate these two ways to score temporal relations.
2.4 Relative Importance of Relations
As shown above, temporal closure is necessary in order to be able to compare properly two temporal
graphs. But in a temporal graph, relations do not have all the same importance. Applying basic recall
and precision scores (either strict or relaxed) on closed temporal graphs is not enough. Consider the
very simple graph examples of Figure 6, in which the first graph K is the gold standard. S1 contains
only two relations, against six in K. But it seems unfair to consider a recall score of 62 , since adding
only one relation (B b C) would be enough to infer all others. An intuitive recall would be around 23 .
What counts here is how many relations are missing in order to recover the whole annotation graph.
Still, even if we suppose that we have a way to distinguish unambiguously “major” relations (solid
lines in K) from “minor” (deducible) relations (dashed lines), it would not be enough. Indeed, graph
S2 finds the relation “B b D”. This relation is minor in K, because it can be found by composing other
relations; but in S2 , it is not the case, this relation actually carries a piece of information and must
then be rewarded. However, even if the amount of temporal information brought by S2 and S3 seems
equivalent, S3 should get a higher score. Indeed, the amount of missing relations (needed to infer the
full graph) is much lower in S3 (only “C b D” is missing) than in S2 . Finally, S4 should get a better
recall than any other one. General cases involving all relations are obviously much more complex.
The same kind of problems could be found in the co-reference task of MUC-6, where co-reference
links define an equivalence relation. It is thus not necessary to specify all pair-wise co-reference
relations to retrieve them, and this has consequences for the evaluation of recall. This was addressed
by considering that a spanning tree of the graph of co-reference is enough to evaluate the recall of
such links (Vilain, Burger, Aberdeen, Connolly, & Hirschman, 1995). For each equivalence class
considered, consisting of n entities, n − 1 links are enough to define the class, and recall error depends
on the number of missing links m needed to reconnect an equivalence class. Recall on a class is then
m
1 − n−1
. Precision was then defined symmetrically. In a much simpler framework, this was a similar
problem of redundancy.4
2.5 “Minimal” Graphs
As said in previous section, a good, but insufficient way to deal with the relative importance of graph
relations would be to work on what we called “major” relations, or “minimal graph”.
We consider that a temporal graph is a “minimal graph” of a graph G if:
4. The MUC measures has other problems that were later addressed by the measures B3 and CEAF, but which have no
relevance to the evaluation of a temporal graph. The main issue is a tendency to favor the prediction of a co-reference
link for every pair of mentions; this has no counterpart in the temporal case since event pairs can be linked with different
relations and with different inferential properties, as opposed to only one equivalence relation.

382

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

K

A

B

C

D

S1

A

B

C

D

S2

A

B

C

D

S3

A

B

C

D

S4

A

B

C

D

Figure 6: What a metric should deal with. K is the reference annotations, Si are candidate annotations. S4 is better than S1 and S3 , which are better than S2 . Solid lines indicate annotated
relations, dashed lines indicate relations inferred from the annotated relations. All events
are related with the “before” relation here.

1. Its temporal closure leads to the same temporal information as G.
2. No relation can be removed from this graph without breaking the first property.
Unfortunately, a unique minimal graph does not exist in the general case, and in particular for
Allen relations. Rodriguez et al. (2004) propose a way to find all minimal graphs for a given temporal
graph. Their algorithm first finds core relations, relations that are in every minimal graph, by intersecting all derivations, and then computes all possible remaining combinations in order to find those
composing a minimal graph.
For example, for the relation RA,B between A and B, derivations are RA,C ◦ RC,B , RA,D ◦ RD,B ,
RA,E ◦ RE,B , etc. If the intersection of all these derived relations equals RA,B , it means that RA,B is
not a core relation, since it can be obtained by composing some other relations. Otherwise, the relation
is a core relation, since removing it always leads to a loss of information. The way this “kernel” is
obtained ensures its uniqueness.
However, the second part of the procedure (compute remaining combinations) is computationally
impractical, even for medium-sized graphs, since every subset of relations must be considered to
determine a minimal graph on top of core relations. The authors do not detail much their empirical
investigations, offering no support for the usability of this method. Moreover, it does not lead to a
unique graph that could be compared to a reference.
Going back to evaluation, Tannier and Muller (2008) suggest the comparison of graphs through
core relations, which are easy to compute and give a good idea of how important the relations are in
a same graph. But core relations do not contain all the information provided by closed graphs, and
383

TANNIER & M ULLER

Figure 7: Recall behavior when removing information vs. ideal behavior.
measures on core graphs are only an approximation of what should be assessed. In this paper, we
propose a method to obtain a unique graph respecting the constraints mentioned at Section 2.4.
2.6 Behavior of Existing Metrics
As stated in the work by Tannier and Muller (2008), a recall measure is expected to decrease in a
linear way when the amount of information decreases. Otherwise, evaluation measures could have
non-gradual changes that complicate comparisons of models. Besides, we expect the amount of information to grow roughly proportionally to the number of events in the text.
This behavior can be evaluated by comparing a given annotation with the same annotation where
some of the temporal information is taken out.
Figure 7 shows how recall measures evolve when removing relations from a temporal graph, until
no relations are present. It shows the different values of strict recall according to the proportion of
relations kept in the graph, as well as the ideal y = x line. This can be considered as an illustration
of the consequences of an annotator “forgetting” to annotate some relations, with respect to an ideal
reference.
This slope reveals two major drawbacks, both leading to a lack of stability of the metric:
• A non-linear progression of the curve does intuitively not correspond to what we expect from
a good metric: for example, if a system A provides 60% more correct information than a system B, this system should get a 60% better recall.
• As will be shown later, the parabolic shape is due to the irregular redundancy and sparseness of
human annotation. Then, two systems providing the same amount of correct information can
get different recall values, depending on whether the human annotation of this information was
redundant or not.
The Figure presents an idealized case; more thorough experiments were done on the whole TimeBank corpus (Pustejovsky et al., 2003) and explained in details in Section 6. For now, it is enough to
note that the measure decreases in a parabolic way, since annotators roughly tag O(n) relations where
n is the number of events, while inferred relations are in O(n2 ), the number of edges from n nodes.
384

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

A1
B1

<

A2

<

C1

<

C2

B2

Figure 8: Endpoint graph with merges (same temporal information as Figure 4). Gray dashed arcs
are trivial relations coming from the definitions of endpoints (C1 starts C, C2 ends C).

Full graphs contain redundant information, and recall thus decreases in artificial, irregular ways
when some of it is removed. Our hypothesis is that working on minimal graphs will suppress redundancy and lead to a more controlled behavior.
Moreover, the fact that reference graphs contain O(n2 ) relations after closure biases the evaluation
towards the larger texts (or at least texts containing large clusters of related events).

3. Proposed New Metric
When confronting a graph to a gold standard, a similarity measure is necessary. Many similarity
measures exist between two graphs once a many-to-many correspondence is found between the nodes
of both graphs (Sorlin, 2006).
Node matching, which is a major problem in graph comparison in general, is not difficult in our
case, since we consider that both graphs annotate the same events or expressions5 .
A traditional similarity function between two graphs is the following (Sorlin, 2006):
sim(K, G) =

f (K um G) − g(splits(m))
f (K ∪ G)

where K um G is the set of relations shared by both graphs according to the node matching function m,
K ∪ G is the union between K and G relations, and splits(m) the number of node splits imposed by
the matching to obtain a graph mapped to the other (see examples later). Functions f and g depend
on the types of graphs and applications.
But this kind of metrics is not appropriate for temporal relations, because the transitivity of relations implies different features; also, these metrics are symmetrical, whereas two distinct recall- and
precision-like values are more desirable. We adapted the general idea of two functions for split nodes
and relation similarity, and arrived at the algorithm described below.
3.1 Transitive Reduction of Endpoint Graph
To address the problem of finding minimal graphs and to take into account the relative importance of
relations, we take inspiration from the work of Dubois and Schwer (2000) in two main ideas. First,
graphs are saturated (i.e. temporal closure is applied), and are converted into endpoint graphs. Second,
two nodes linked by an equality relation are merged together (this will help guarantee the uniqueness
of the minimal graph, see below), a useful procedure on point-based graph (van Beek, 1992). Figure 8
presents the graph of Figure 3 after this transformation. The resulting point graph is saturated, by
definition of the composition of event relations in Allen’s algebra.
Recall that the graph we consider are built from “convex” annotations, i.e. there cannot be a “<
or >” relation between two points. We can keep relations < and ≤ without loss of information, since
all > and ≥ can be obtained by symmetry.
5. If this is not the case, creating fictitious unlinked nodes in one graph or both is enough.

385

TANNIER & M ULLER

With these specifications, the graph boils down to the directed graph of a transitive relation where
an edge between two points x and y means x ≤ y. A coherent graph will thus be acyclic, since we
collapse equal points into single nodes. It is important to note that as a consequence, no edge in the
transitive closure can be labelled with the equality relation only. Thus we can see our problem as
searching for the transitive reduction of a graph labelled with the transitive relation ≤ (but for which
we keep the additional information that some edges can be more precisely labelled as < instead of
the disjunctive ≤). This is important because the minimal graph is the transitive reduction of the
graph, and the transitive reduction of a directed acyclic graph is unique (Aho, Garey, & Ullman, 1972;
La Poutré & van Leeuwen, 1988). The transitive reduction of a graph G is by definition the subgraph
corresponding to the minimal set of edges (with respect to inclusion) that has the same transitive
closure as G, i.e. the minimal graph G0 such that G0 is a subgraph of G and G0∗ = G∗ where G∗ is
the transitive closure of G. It is simply determined by G∗ /(G∗ ◦ G∗ ). Algorithm 2 details a simple
computation of the transitive reduction while Figure 9 shows an illustration of the procedure on a
simple transitive graph. Figure 10 shows the process from the initial endpoint graph with both <
and ≤ labels, to the minimal graph, via transitive reduction of the unlabelled graph.
Algorithm 2 Transitive reduction simple computation
procedure COMPOSE(G)
. find relations inferable from others
newRels={}
base_rels= { x for x in G.edges() if x.relation() in {<, ≤}}
for all one in base_rels do
related ={x for x in G.edges() if x.source()=one.target() and x.relation() in {<, ≤} }
for all other in related do
relation = compose(one.relation(),other.relation())
newRels.add(Edge(one.source(),other.target(),relation))
end for
end for
return newRels
end procedure
procedure T RANSITIVE - REDUCTION(G)
G = closure(G)
non_min=compose(G)
for all one in non_min do
G.edges().remove(one)
end for
for all one in G.edges() do

. remove relations deduced by composition

if one!=before and one!=before_or_equals then
G.edges().remove(one)
. keep only <, ≤ and remove their symmetric relations
end if
end for
end procedure

386

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

1+4
1+5

4

2
3

5+7

7

5
6

1

8

6+7
(b)

(a)

(c)

Figure 9: Transitive reduction of an acyclic graph; (a) is the initial closure of a transitive relation
graph; (b) is the set of edges that can be obtained by composition of edges in (a), with examples of composition for each edge; (c) is the transitive reduction, the difference between
(a) and (b).

387

TANNIER & M ULLER

<
<

<
<=

<
<=

<
<
(b)

(a)

<=

<
<=

<

(d)

(c)

Figure 10: Transitive reduction of a point-based graph; (a) is the initial annotation transformed into a
graph on events endpoints; (b) is the corresponding graph as if every label was ≤, (c) its
transitive reduction and (d) the final minimal graph where the more precise initial information is reported.

388

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

We call:
• Major relations, the relations of the transitive reduction, or Gmaj .
• Minor relations, the relations of temporal closure which are not present in the transitive reduction, i.e. G∗ − Gmaj .
Formally:
Let G = {(x, y, R)/R ∈ {<, ≤}}, the temporal point graph, saturated with respect to the relation < and ≤. So G∗ = G
Let E(G) = {(x, y)/∃R, (x, y, R) ∈ G}, the unlabelled corresponding graph. The function f
which associates (x, y, R) in G to (x, y) in E(G) is an obvious bijection, as the original graph G has
at most one relation holding between any two vertices. Here E(G) = f (G). Since G is closed, so is
E(G).
Let P roj(G0 , G) = {(x, y, R) ∈ G/(x, y) ∈ G0 } the “projection” of an unlabelled graph into a
labelled one. The function associating an edge (x, y) in G0 to (x, y, R) in G is the inverse of f , f −1 .
P roj(G0 , G) = f −1 (G0 ), and obviously P roj(E(G), G)) = G.
It is enough to prove that E(G) (or G) is the graph of a transitive, acyclic relation to prove that
E(G) has a unique transitive reduction (Aho et al., 1972).
First, E(G) is transitive: let (x, y) ∈ E(G) and (y, z) ∈ E(G) then we have (x, y, <) or (x, y, ≤
) ∈ G and (y, z, <) or (y, z, ≤) in G in any of the four possibilities we can infer either (x, z, <) or
(x, z, ≤) is in G (because < is transitive, ≤ is transitive and x < y ≤ z or x ≤ y < z both imply
x < z); in other words any composition of < and ≤ is <, i.e. ((< ◦ ≤) = (≤ ◦ <) = (<)) so
(x, z) is also in E(G) and E(G) is the graph of a transitive relation. This is the reason why we said
that keeping a record of all < relations while considering G as a graph of ≤ does not change the graph
property.
Second, G is acyclic since < is irreflexive, and x < y ≤ z implies x < z so that the only way to
have cycles in G is if there are paths such as x ≤ y ≤ z ≤ ... ≤ x. But in that case we can infer that
x = y = z = ... and the nodes would have been merged beforehand. So E(G) is also acyclic (it has
exactly the same edges as G).
So, E(G) is acyclic and transitive and thus admits a unique transitive reduction E(G)tr .
Since the graphs G and E(G) have exactly the same edges, they necessarily have the same reductions, and thus both have a unique transitive reduction. We can project back the original relations of
G on E(G)tr , (P roj(E(G)tr , G)), to have a properly labelled reduction for G.
3.2 Temporal Recall and Precision
The idea is not to compare only minimal graphs. Temporal closure should be used as well. As we
showed in Section 2.4, reference minor relations should still be rewarded if they are not redundant in
the evaluated graph. However, they must carry a lower weight.
Minor relations are only to be considered in temporal recall, and not in precision. The reason is
that recall evaluates the proportion of reference relations found by the system, and this system can
find some minor relations without the major relations that produced them in the reference (see B b D
in S4 example, Figure 6). On the opposite, precision evaluates the proportion of system relations that
are in the reference graph, and minor (i.e. deducible) relations found by the system are by definition
redundant.
Our recall-like measure is then a combination of two values. Given K the reference graph and G
the evaluated graph:

389

TANNIER & M ULLER

• The major temporal recall is the rate of reference major relations (Kmaj ) found in G∗ .
• The minor temporal recall is the rate of reference minor relations (K ∗ − Kmaj ) found in Gmaj .
In the first case, the temporal closure is applied to G, since there is no reason to restrain the search
of good relations in the evaluated graph. In the second case, only the transitive reduction Gmaj is
considered; reference minor relations must be rewarded only if they are not minor in G (case of B b D
in example S2 , Figure 6). G minor relations have already been assessed through their major relations
(case of A b C in example S4 ).
The final value of temporal recall is a weighted sum of the two figures.
The precision-like measure is a single value corresponding to the ratio between correct relations
in Gmaj and its total number of relations. G minor relations must not be considered at all for precision. Unlike for recall where reference major relations may not be retrieved by the system, precision
necessarily considers major relations from the system. Minor relations are then all redundant.
In precision as well as in recall, a merge must be considered as a relation in some way, because it
corresponds to an ‘=’ relation.
3.3 Notations
We note:
• G and K the event graphs, K being the reference (key);
• Gpt and K pt , the corresponding endpoint graphs, where equals points are merged.
• (N, R) the set of nodes and relations of the graph G, and mutatis mutandis for the others.
• in the examples, only non-trivial relations are listed. Trivial relations are those involving two
points of a same interval. For example, A1 < A2 in Figure 8 is trivial, and thus not considered.
In all figures, trivial relations are dashed gray lines. Non-trivial relations and the node list are
enough to find the full graph.
• The temporal closure of G is noted G∗ .
• The transitive reduction of G is now noted Gmaj .
3.4 Temporal Recall and Precision
Our new metrics of temporal recall and precision rely on the notions defined above. With respect to
the recall value, we have shown it was important to distinguish “major” relations, i.e. relations that
belong to the minimal graph, from the other, “minor” relations. That is why we suggest to compute
recall in two steps. Precision is not concerned by this issue.
3.4.1 G RAPH VALUES
Let’s consider now Gpt a set of merged nodes and relations (N pt , Rpt ) evaluated with respect to the
pt
pt
reference K pt = (NK
, RK
). We need to take into account the number of relations expressed in the
original point graph before nodes are merged (called value(Gpt )), that is the number of equalities expressed in the merged nodes, node_values(Gpt ), plus the number of relations relation_value(Gpt ).
Then:
pt )
value(Gpt ) = node_value(G
+ relation_value(Gpt )
P
pt
=
ni ∈N pt (|m(ni )| − 1) + |R |
= |N | × 2 − |N pt |
+ |Rpt |
390

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

Here, m(ni ) the number of points merged into a node (m points into a node correspond to m − 1
merges). Similarly, we can compute the value of the reference graph.
The pairing of nodes in the evaluated graph and the reference graph must be then taken into
account. Let’s call a “split” the operation of mapping one node of the reference graph to many
nodes in the evaluated one, and a “conflation”, the converse operation. To map both graphs, we first
need to split any node of the reference that is not in the evaluated graph into nodes that are in it and
maybe some extra nodes, and then conflate, if necessary, the remaining nodes to evaluated nodes (see
Sections 4 and 5 for illustration). A split is like a ‘=’ relation provided by the reference but not by the
evaluated graph. A conflation is a ‘=’ wrongly predicted by the evaluated graph.
Therefore, the number of correct answers in an evaluated graph, or correct value vc , is the number
of correct ‘=’ and ‘<’ relations6 . The number of correct ‘=’ is “node value - nb of conflations”, the
number of correct ‘<’ is “relation value - wrongly predicted relations (errors)”.
A simple way of calculating the number of splits necessary to match two graphs G1 and G2 is to
count for each node x of G1 the number i of different nodes of G2 that intersect with x. If i is one, the
node is mapped directly, otherwise i − 1 splits are needed. Thus:
P
|split(K pt , Gpt )| = x∈N pt |{y ∈ N pt /x ∩ y 6= ∅}|
K

The number of conflation necessary to match G1 to G2 is also the number of splits necessary to match
G2 to G1, so:
|conf lation(K pt , Gpt )| = |split(Gpt , K pt )|
vc (Gpt ) =
=
=
=

correct equalities + correct precedences
(node_value(Gpt ) − conflations)) + (relation_value(Gpt ) − errors)
v(Gpt ) − (|conf lation(K pt , Gpt )| + errors)
v(Gpt ) − (|split(Gpt , K pt )| + |R − RK |/|R|)

3.4.2 T EMPORAL R ECALL AND P RECISION
A precision value deals with errors (incorrect relations) and conflations through the correct value vc .
On the other hand, a recall value must take into account misses (reference relations missed by the
pt
the point-based reference, and G = Gpt the
graph) and splits. For simplicity, let’s note now K = KK
graph to evaluate. Kmaj and Gmaj are the transitive reductions of these. We define:
• Major temporal recall Rt (G) is the number of reference major relations found by G∗ .

Rt (G) =
Rt (G) =

v(Kmaj ) − (splits + misses)
v(Kmaj )
v(Kmaj ) − (|split(K, G)| + |(RK − R∗ )/RK |)
v(Kmaj )

where misses is the number of relations in Kmaj missed by G∗ and splits the number of splits
(a split is a missed ‘=’ relation).
6. We consider of course only non trivial relations.

391

TANNIER & M ULLER

• Symmetrically, Temporal precision T P (G) is the ratio between the correct value of Gmaj ,
vc (Gmaj ), and its full value v(Gmaj ).
T P (G) =
T P (G) =

v(Gmaj ) − (conf lations + errors)
v(Gmaj )
vc (Gmaj )
v(Gmaj )

But, as we already stated, minor relations should also be taken into account. If we consider only
major recall, systems that find only minor (but correct) relations are disadvantaged. This is the case
of system S2 in Figure 6 (B b D is correct and minor). That is why we add a minor temporal recall:
• Minor temporal recall rt (G) is the proportion of reference minor relations (K ∗ − Kmaj ) found
by Gmaj . Minor relations are seeked in the minimal evaluated graph. Indeed, as we already
said, comparing two relations from non-minimal graphs is redundant, since major relations that
“produced” them have already been taken into account.
• Full temporal recall T R could be defined as a value pair (Rt (G), rt (G)), or preferably as a
combination:
T R(G) = Rt (G) +

1
rt (G)
v(Kmaj )

(2)

With this formula, we ensure that one single major relation is better than all minor ones and that
the recall can not exceed 1 (see next Section).
The symmetrical precision property, i.e. the proportion of system minor relations existing in the
reference, is always null. System minor relations are, by definition, always redundant. That is why
temporal precision does not need a two-fold figure.
A tool for computing different precision and recall values (strict, relaxed, core, as well as this
new one) is made available with this paper7 . It accepts several input formats, including TimeML. All
computed data are also available, with instructions to reproduce the experiments presented at the end
of this paper.
3.4.3 S YNTHESIS
Evaluating an interval graph G against a gold reference K is achieved through the following steps:
1. Perform transitive closure on both G and K.
2. Convert both graphs into endpoint graphs.
3. Merge equality relations into single nodes.
4. Perform transitive reduction.
5. Compute values of temporal recall and precision.
7. http://www.irit.fr/~Philippe.Muller/resources.html

392

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

The most costly operation here is the first one, and is common to the standard evaluation procedure; its time is in O(|N |3 ) in the worst case with the algorithm 1 page 379, N being the set of events
of the largest graph between G and K. The other are straightforwardly linear in the number of relations
of the graph (conversion to endpoints, merge equalities in the closed graph), or at worse in O(|N |2 )
for both the transitive reduction (one composition of relations in the graph and set difference), and the
computation of the number of splits and conflations. Overall, the whole procedure is dominated by
the first step, which is common to all evaluation procedures for this task. Our proposition thus does
not change the worst-case complexity of the evaluation procedure.
3.4.4 B OUNDARIES
Temporal precision. Temporal precision is a value between 0 and 1 since vr (G) ≤ v(G) (errors
and conflations are zero or positive).
Temporal recall.

Major recall is between 0 and 1.

• If 1, then minor recall rt (G) = 0, because all relations in Gmaj are already in Kmaj (and then
cannot be in K ∗ − Kmaj ). In this case temporal recall cannot exceed 1.
• If not 1,
Rt (G) ≤ (v(Kmaj ) − 1)/v(Kmaj ). Yet,
stays below 1.

1
v(Kmaj ) rt (G)

<

1
v(Kmaj ) ,

and the full temporal recall

Thus 0 ≤ T R(G) ≤ 1.
In order to make the measure clearer, we will present the metric in detail by developing a basic
example with a transitive closure made only of simple relations (the 13 basic Allen relations), then we
will turn to the more general case of a closure made only of convex temporal relations (disjunctions
of neighboring Allen relations).
3.4.5 S IMPLE E XAMPLES
Temporal recall and precision as described above lead to the expected values for the sample graphs
pictured in Figure 6 and analyzed in Section 2.4.
Figure 11 recalls these graphs and details the temporal recall values for each of them (given that
v(Kmaj ) = 3). As for precision, for each graph Si , T P (Si ) = 1.
An attentive reader would note that a system providing only minor relations, A < C, A < D and
B < C, i.e. half of all deducible relations, would get a recall of only 0.33, which could seem unfair.
However, even if three relations are present, three others are still missing (the same number of missing
relations as if the graph had no relation at all). Moreover, the fact that all minor relations does not get
a better score than one major relation is the condition for the boundaries not to go over 1. Finally, this
case is rare and does not affect the general behavior of the metric.
More sophisticated examples are provided in the following sections.

4. First Simple Case: Non-Disjunctive Allen Relations
Consider the sample graph K1 made of Allen non-disjunctive relations (see also the graph in Figure 12):

393

TANNIER & M ULLER

K

B

A

D

C

S1

A

B

C

D

T R(S1 ) = 3−(1+0)
+ 03 = 0.67
3
(2 major relations, 1 missing)

S2

A

B

C

D

1
T R(S2 ) = 3−(2+0)
+ 3×3
= 0.44
3
(1 major and 1 minor relations)

D

T R(S3 ) = 3−(1+0)
+ 03 = 0.67
3
(2 major and 1 redundant minor relations)

D

1
T R(S4 ) = 3−(1+0)
+ 3×3
= 0.77
3
(2 major and 1 non-redundant minor
relations)

S3

S4

A

A

B

C

B

C

Figure 11: Temporal recall of simple graphs, compared to reference K.

A B C D E
A
s bi b s
B
bi m s
K1 =
C
b b
D
f
E

F
b
m
b
e
fi

The conversion into relations between endpoints leads to the following graph, where an event A
is split into A1 and A2. Edges are not labelled since only relation ‘<’ is considered at this point.

A1

K1maj

C1

C2

B1
E1

A2

B2

E2

D1

D2

F1

F2

Figure 12: Reference with simple relations, K1maj .
Note that merging equal nodes is not equivalent to labelling arcs with ‘=’, because in the latter
case the minimal graph would not be unique any more. For example, the necessary choice between
A1 < A2, B1 < A2 and E1 < A2 would lead to three equivalent but different graphs.
Consider now that the reference K1 is compared to the following graph G1 (Figure 13; bold edges
are correct relations, thin edges are wrong relations, dashed edges are trivial relations, i.e. involving
two endpoints of the same interval).
Following the notations defined in Section 3.3:
• The list of nodes for graphs K1maj and G1maj are:

394

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

G1maj

C1

B1

C2

A1
B2

D1
A2

E1
F1

E2
F2

D2

Figure 13: Evaluated graph with simple relations, G1maj .
– for K1maj : N 1 = {C1, C2, (A1, B1, E1), A2, (B2, D1, F 1), (E2, D2, F 2)} : 6 nodes
– for G1maj : N 2 = {C1, B1, C2, (A1, B2), A2, (D1, E1, F 1), (E2, F 2), D2} : 8 nodes
• Non-trivial relations are:
– for K1maj : R1 = {[C2; (A1, B1, E1)], [A2; (B2, D1, F 1)]} : 2 relations
– for G1maj : R2 = {[C1; B1], [B1; C2], [C2; (A1, B2)], [A2; (D1, E1, F 1)],
[(E2, F 2); D2]}] : 5 relations
• The temporal closures Gi∗ are computed and listed below (bold relations are those added from
the minimal graph, i.e. K1∗ −K1maj and G1∗ −G1maj ). Relations on the same line share their
arguments (at least one point in common for each argument). Figures 14 and 15 also represent
the closures.
K1∗
[C2; (A1, B1, E1)]
[A2; (B2, D1, F 1)]
[C1; (A1, B1, E1)]
[C1; A2]
[C1; (B2, D1, F1)]
[C1; (E2, D2, F2)]
[C2; A2]
[C2; (B2, D1, F1)]
[A2; (E2, D2, F2)]
[C2; (E2, D2, F2)]

G1∗
[C2; (A1, B2)]
[A2; (D1, E1, F 1)]
[C1; B1]
[C1; A2]
[C1; (A1, B2)]
[C1; (D1, E1, F1)]
[C1; D2]
[C1; (E2, F2)]
[C2; A2]
[C2; (D1, E1, F1)]
[A2; (E2, F2)]
[A2; D2]
[C2; (E2, F2)]
[C2; D2]
[B1; C2]
[(E2, F 2); D2]
[B1; A2]
[B1; (D1, E1, F1)]
[B1; (E2, F2)]
[B1; D2]
[(A1, B2); (D1, E1, F1)]
[(A1, B2); (E2, F2)]
[(A1, B2); D2]

• The pairing of nodes and the list of splits and conflations needed to match both sets of nodes is
also computed (see also Figure 16):
395

TANNIER & M ULLER

K1∗

A1
C1

C2

A2

B1
E1

B2

E2

D1

D2

F1

F2

Figure 14: Temporal closure K1∗ . Dotted relations represent K1∗ − K1maj .

G1∗

C1

B1

A1

C2

B2

D1
A2

E1
F1

E2
F2

D2

Figure 15: Temporal closure G1∗ . Dotted relations represent G1∗ − Gmaj .
– (A1, B1, E1)K1 : 2 splits (“breaking” of 2 equality relations); (B2, D1, F 1)K1 : 1 split
(D1 and F 1 stay together in G1); (E2, D2, F 2)K1 : 1 split
– (A1, B2)G1 : 1 conflation (joining two nodes); (D1, E1, F 1)G1 : 1 conflation (joining E1
to the two others); (E2, F 2)G1 : nothing (already together in K1)
– Total: 4 splits and 2 conflations.
As stated above, an edge is correct if the relation is correct for at least one pair of points from
both nodes. For example, relation {A2}G1 → {D1, E1, F 1}G1 is correct because {A2}K1 →
{B2, D1, F 1}K1 , even if the sub-relation A2 < E1 is not true. This latter relation will be penalized anyway because a split is necessary for matching the graphs.
These definitions lead to the following values;
• Graph values:
– v(K1maj ) = node value + relation value = 6 + 2 = 8
– v(G1maj ) = 4 + 5 = 9
• Correct value of G1:
vc (G1maj ) = (node value − conf lations) + (relation value − errors)
= v(G1maj ) − (conf lations + errors)
= 9 − (2 + 2) = 5
• Major temporal recall:
Rt (G1) =
=

v(K1maj ) − (misses + splits)
v(K1maj )
8 − (0 + 4)
= 0.5
8

• Minor temporal recall: rt (G1) = 28 = 0.25
(this corresponds to [C1; {A1, B1, E1}] and [C2; (B2, D1, F 1)]).
396

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

A1

K1

C1

C2

A2

B1
E1

split

after
splits

C1

B1

C2

split

A1

B2

A2

merg.

G1
(after
conﬂations)

C1

B1

E2
D2

F1

F2

split

split

E1

D1

E2

F1

F2

D2

merg.

D1

A1

C2

B2
D1

A2

B2

E1
F1

E2
F2

D2

Figure 16: Split and conflation operations between K1 and G1.
• Full temporal recall:
T R(G1) = (R(G1), r(G1))
= (0.5, 0.25)
or T R(G1) = Rt (G1) +
= 0.5 +

1
rt (G1)
v(Kmaj )

0.25
= 0.53
8

• Temporal precision
T P (G1) =
=

vc (Gmaj )
v(Gmaj )
5
= 0.56
9

5. Disjunctive Convex Relations
We now apply the metric to sets of convex relations. Convex relations are a set of relations that are
conceptual neighbors, that is they encode relations that may be vague but in which intervals endpoints
can only be in convex subsets of the time-line (see Section 2.2).
Building the minimal graph follows the same procedure as explained above (transitive reduction).
The measures do not differ at all if we choose a strict scoring scheme (see Section 2.3). But in a
relaxed scheme, with disjunctions, it becomes necessary to apply a weighting procedure; a response
is no longer assigned a binary value (0 or 1), but for example one of the values in Table 3, in a manner
similar to what was done in TempEval (Verhagen et al., 2007).
As shown in that table, the relaxed measure has an effect on misses values and on vc (a relation
can get a half-point), but also on conflation and split values (where half-points are also possible).
397

TANNIER & M ULLER

%
Rel.

A

Repres.

A

<

B

1

0

A

≤

A

>

B

≥

B

<

<, =

B

0.5

B

<

A

≥

>
B

<

A

B

<, =

A

B

A

=

≤

<

=

0

(“ 12 -split”)

0

B

1

0.5

0.5
(“ 12 -split”)

0

0

(disjunction)
<, =

<

B

0.5

0.5

(“ 12 -confl.”)

(disjunction)

0

0

A

1

0

0.5
(disjunction)

0

1

0.5
(disjunction)

<, =

A

0.5

0

(“ 12 -confl.”)

0.5

0.5

(disjunction)

(disjunction)

1

Table 3: Example weights for relaxed measures.
Consider a new reference K2 with convex relations (see also Figure 17):
A B
C
D
E
F
A
s {bi, mi, oi, f i, =, f, di, si} b
s
b
B
{di, si, oi, mi, bi}
m
s
m
C
b {d, s, o, m, b }
b
D
f
{s, =, si}
E
{di, f i, o}
The characteristics of K2 are:
• 7 nodes for 6 events (5 equality relations)
• 2 relations: [C2 ≤ A2], [A2 < {B2, D1, F 1}]
• K2∗ =
[C2 ≤ A2], [A2 < {B2, D1, F 1}], C1 < A2, C1 < {B2, D1, F1}, C1 < {E2, D2},
C1 < F2, C2 < {B2, D1, F1}, C2 < {E2, D2}, C2 < F2, {A1, B1, E1} < F2,
A2 < {E2, D2}, A2 < F2
• v(K2maj ) = (12 − 7) + 2 = 7.
Let us consider an evaluated graph G2 (Figure 18) which has:
• 10 nodes for 6 events (2 equality relations)
• 5 relations: [C1 ≤ D1], [D2 < C2], [C2 ≤ A2], [{A1, B1} ≤ E1], [A2 < {B2, F 1}]
• G2∗ = [C1 ≤ D1], [D2 < C2], [C2 ≤ A2], [{A1, B1} ≤ E1], [A2 < {B2, F 1}],
[C1 < D2], [C1 < A2], [C1 < {B2, F1}], [C1 < F2], [D1 < C2], [D1 < A2],
[D1 < {B2, F1}],
[D1 < F2],
[D2 < A2],
[D2 < {B2, F1}],
[D2 < F2],
[C2 < {B2, F1}], [C2 < F2], [A2 < F2], [{A1, B1} < F2], [{A1, B1} < E2]
• v(G2maj ) = (12 − 10) + 5 = 7.
398

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

A1
B1
E1

K2maj
C1

<

E2
<
<,=

<

A2

B2

<

D1

<

F1

D2
F2

C2

Figure 17: Reference with convex relations, K2maj .

C1

<,=

D1

<

D2

<

C2
<,=

A1

G2maj

<

B1

A2

<

B2
F1

<

F2

<,= (1/2-split)
E1

<

E2

Figure 18: Evaluated graph with convex relations, G2maj .
There is no conflation and the number of splits is 2.5. The “half-split” comes from the fact that
E1 may be equal to {A1, B1}, because of the ≤ edge.
The application of the same measures leads to the following values:
Rt (G2) =

7 − (0 + 2.5)
= 0.64
7
rt (G2) = 0

T R(G2) = 0.64
vc (G2) = (12 + 2 ∗ 1/2) + 2 = 15
The two half-points in vc (G2) hold for relations C1 ≤ D1 (instead of C1 < D1 in the reference)
and B1 ≤ E1 (instead of B1 = E1 in the reference).
As an example, and following Table 3, we can see that E1 leaving the group {A1, B1, E1} costs
a half-point to the recall value, while relation B1 ≤ E1, correct but imprecise, gets a half-point of
precision. Both results seem logical behaviors of the measure.

6. Experiments
Our push for a change of evaluation measures of temporal annotation graphs was motivated in the
beginning of the paper by a few examples where we show some undesirable side effects of common
399

TANNIER & M ULLER

evaluation procedures. We will now present a more thorough methodology, aiming to evaluate the
evaluation procedure itself. We used two kinds of data for that purpose. Obviously, we can test
and compare measures on the freely available temporal annotated data provided in the TimeBank
corpus. We also introduce a set of artificially built temporal graphs, on which we can control a few
relevant parameters. TimeBank annotations are rather heterogenous, because human annotators make
mistakes, forget relations, or introduce inconsistencies, thus creating a fair amount of noise. Besides,
we wanted to test other aspects of temporal annotation that are easier to generate from scratch. One of
this factor is the amount of information that is present in an annotation. We have seen that the result of
an annotation can be more or less underspecified: a relation between events can be a simple, precise
relation, or a disjunction of simple relations. The size of a graph is also probably important, but it
is somewhat hidden in some human annotations, because the really relevant object when considering
inferred information is a connected component. Small subgraphs with only a few nodes do not allow
for a lot of inferences, and some of the bias we want to study probably happens only above a certain
threshold. In human annotations, the size of subgraphs can vary a lot, and it is common for events to
be related only to one or two other events, even after the graph has been enriched through inference
procedures.
The main experiment we performed to study the behavior of commonly adopted measures and our
own proposal is based on a comparison of an annotation to a “weakened” version of itself. For each
graph, we remove a portion of event-event relations at random, and we use the measures to estimate
the loss of information. Obviously, this is only interesting for recall-like measures, precision is not
affected. We vary the amount of removed information in steps, until all relations are vague: eventually,
the universal disjunction hold between any two events of the text. We designed another experiment to
watch also the behavior of precision measures : instead of removing relations, we disturbed the graph
by changing a simple relation to another one (simulating an “error” in the annotation, in a way). This
is meaningful only if we keep the graph consistent at the same time.
In what follows, we present our results on the synthetic graphs (6.1) and then on the TimeBank
data (6.2).
6.1 Artificial Graphs
In order to control the amount of information present in a temporal graph, we have built a set of
artificial temporal graphs in the following way:
• For a given number of events E, a temporal graph is built by randomly choosing a set of E
integer pairs (bi , ei ), within a given range N , and with a level of indeterminacy I, that is a
width around the boundaries bi and ei .
• For each pair of events (i, j), we determine a temporal relation between their intervals, considering that the endpoints of i can lie anywhere in the interval [bi − I/2, bi + I/2] and
[ei − I/2, ei + I/2], and the endpoints of j can lie anywhere in the interval [bj − I/2, bj + I/2]
and [ej − I/2, ej + I/2].
• The graph is closed, leading to N = (E × (E − 1))/2 relations.
Events are considered as intervals with some uncertainty about their endpoints, to be comparable
to a graph built from a text, and to give rise to possibly disjunctive relations between the generated
events. This graph can thus contain any kind of convex (possibly disjunctive) Allen relations. An
example of such generated events is provided by Figure 19. By varying N and I, we can control the
400

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

beginning
of E5

end of
E5

E5
E4
E3
E2
E1
0

1

5

10

15

20

Figure 19: Example of random artificial graph with N = 20 (X-axis), E = 5 and I = 3 (maximum
uncertainty for each endpoint). The indeterminacy leads to disjunctive relations, for example ((E5 < E3) ∨ (E5 meets E3) ∨ (E5 overlaps E3)). When building graphs from
this representation, relations are disjunctive but the graph is fully connected, which is not
the case with real data like TimeBank.

amount of vagueness of the information put in the graph. The smaller N is, the tighter the model we
generate, and the most likely it is that vague boundaries intersect, creating disjunctive relations. A
larger I also contributes to a vaguer representation. We quantify the amount of vagueness v by how
much the relations are disjunctive in the graph:
P
1
r∈edges(G) 1 − |r|
v=
|edges(G)|
where r is an edge in the graph and |r| is the number of simple Allen relations in the disjunction
(i.e.: for r = {b, o}, |r| = 2).
Figure 20 shows the different values of strict recall and our temporal recall (called point recall)
according to the proportion of relations kept in the graph, with lines joining values derived from the
same graphs.
It is interesting to note that curves based on the minimal graphs are almost linear, while the simpler measures decrease slowly at first then sharply, in a more parabolic way. We analyze below
(Section 6.2.2) the difference between being above or below “y=x”.
This shows that the overall effect is observable roughly on every graph considered. The effect is
even more marked when the number of events is higher.
We can also compare our point-based measure to the relaxed recall measure, similar to the measure
used in the TempEval campaign and which computes an overlap between all simple and disjunctive
relations on every edge of the graph. We also want to see the behavior of a measure based on the “core”
set of relations extracted from the annotation as in our previous work (Tannier & Muller, 2008).
We can see in Figure 21 that the only measure that behaves linearly is the point-based one. Here,
we have plotted the estimates of a parabolic regression on each data set, for clarity. The others measures show clearly parabolic behaviors, with different undesirable effects: relaxed recall is much too
permissive when no information is actually provided (since the disjunction of all relations, which

401

TANNIER & M ULLER

experiment : remove

1.0

experiment : remove

1.0

strict recall
id

0.6

0.6
score

0.8

score

0.8

recall/point graph
id

0.4

0.4

0.2

0.2

0.00.0

0.2

0.4

% modified

0.6

0.8

0.00.0

1.0

0.2

0.4

% modified

0.6

0.8

1.0

Figure 20: Behaviors for each graph with 30 events, strict recall (left) and point-based recall (right).
does not carry any information, gets a non-zero score), recall with respect to core relations is similar
to strict recall, and is surprisingly even less linear. This effect is again stronger when the number of
events increases.

experiment : remove

1.2

recall/core
strict recall
recall/point graph
relaxed recall
id

1.0

score

0.8
0.6
0.4
0.2
0.00.0

0.2

0.4

% modified

0.6

0.8

1.0

Figure 21: Parabolic regressions for all recall measures considered (30 event graphs).
Finally, Figure 22 shows the influence of different levels of “vagueness” of the annotation (and
thus the underlying temporal description) on the measures used. We plotted the same experiment as
above (recall with respect to the quantity of information removed) with the vagueness of the considered graphs as a third parameter. The surface shown is the departure from the “y=x” line, for
readability’s sake.
Here we can observe that less vague temporal descriptions have values mainly above the “y=x”
line for “parabolic” measures, while vaguer data show a less obvious parabolic behavior, sometimes
402

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

below that line. The curves for point-based recall seems insensitive to that aspect, which is consistent
with the average behavior we already discussed.

0.8

0.8

recall/point graph

0.6
strict recall

0.4
0.2

d0.2
isjunc 0.4
tion le 0.6
vel

0.8

0.8

0.6

0.2
0.4 oved
em
r
%

0.6
0.4
0.2

dis0.2
junct 0.4
ion le 0.6
vel

0.8

0.8

0.6

0.2
0.4 ved
o
% rem

Figure 22: Influence of the vagueness of the annotation on the behavior of different measures (30
event graphs), with strict recall on the left and point recall on the right.

6.2 TimeBank Corpus
Before presenting the results of our evaluations on the real annotated data of the TimeBank corpus, we
show some characteristics of the temporal graphs induced by the annotations. This helps to understand
some of the differences between the behavior we observe on the synthetic temporal graphs and the
more ecological ones.
6.2.1 A NALYSIS OF THE T IME BANK C ORPUS
The TimeBank corpus consists of 186 news report document (for about 65, 000 tokens). It is made
from news articles from the Associated Press, the Wall Street Journal, the L.A. Times and the San
José Mercury News, and transcripts from broadcast news by CNN, ABC, VOA. These documents
were initially collected for the DUC and ACE evaluation campaigns. The corpus in version 1.1 is
available from http://www.timeml.org/site/timebank/timebank.html.8
Documents are annotated using the TimeML standard for tagging events and states, dates, times,
and durations, and their temporal relations as well as various aspectual modalities. These entities are
also tagged with some attributes: tense, aspect for verbs, values for dates and durations, normalized
according to the ISO standard 8601.
Eventualities can be denoted by verbs, nouns, adjectives and some prepositional phrases. The
temporal relations encode topological information between the time intervals of occurring eventualities, using relations that are equivalent to Allen relations, although they have different names. The
TimeML standard specifies the relations between events, the anchoring of events to “times” (dates,
hours) and the relations between times. There are about 7000 annotated relations between temporal
entities. Additions to this set have been proposed (Bethard, Martin, & Klingenstein, 2007), in order
to complete the annotations. As was noted by Setzer (2001), tagging temporal relations is hard for
8. A somewhat cleaned-up version, 1.2, is available via the Linguistic Data Consortium. We used the freely available
version, while checking that the most recent version does not exhibit significant differences in our experiments.

403

TANNIER & M ULLER

Figure 23: Frequencies of TimeBank texts with respect to their number of temporal entities.
Mode
Consistent annotations
Average number of relations
Average nb of components/text
Component Average size
Max component average size

Raw
186
43
5.28
7.94
19.64

+time-time relations
186
58
7.15
5.89
27.04

Saturated
131
134
5.28
7.94
19.64

Saturated + t-t
146
281
7.15
5.89
27.04

Table 4: Timebank1.1 statistics (Average number of temporal entities=42)
the annotators, who tend to miss relations, or produce inconsistent annotations. This is all the more
obvious as the sizes of the texts increase, since then the number of possible links grows in the square
of the number of temporal entities. Figure 23 shows the distribution of events among the texts. While
annotators could be expected to keep track of all possible relations between events in small texts, the
task was probably different for the numerous larger texts, and it has been noted that they tend to produce sets of disconnected temporal subgraphs on the majority of the corpus (Chambers & Jurafsky,
2008b).
Table 4 shows two important consequences when considering relations inferred from an annotation: (a) a lot of annotations in TimeBank 1.1 are actually inconsistent when the temporal graph is
saturated using procedures introduced earlier in the paper and (b) each text gives rise to several connected components of various sizes. It is worse if we compute some missing (but obvious) relations
between dates whose values are fully specified (noted t-t in the table). As we have seen, the procedure
for checking consistency can only say that a set of relations is globally inconsistent, without a hint of
which relation(s) should be isolated to repair that situation. These texts have thus been ignored in our
evaluations.
The fact that the temporal graphs of some texts are actually split between components of different
sizes has consequences when considering the size of the referent graph: a fully connected graph of
n entities can have up to n × (n − 1)/2 non-vague edges after saturation, while a text with scattered
annotations might yield no other relation and a much smaller graph.
We have compared the number of relations present in a minimal graph obtained by transitive
reduction with the number in the temporal closure of the interval-based graph, with respect to the
number of events present in the text, on the whole TimeBank Corpus 1.1. (Figure 24). On this Figure,
each point corresponds to a text, with its number of events along the x-axis and the number of relations

404

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

in the reference used for evaluation with a given measure along the y-axis. It appears that minimal
graph grows roughly linearly as expected (the variance being due to variable multiple branching when
there is a lot of uncertainty). On the other hand, the temporal closure of the annotation is larger, much
more irregular, with greater variance when the number of events grows. This is even worse for the
“relaxed” measures which necessarily consider every possible edge between two events, even though
it might only bear non-informative vague relations consisting of the disjunction of many relations.
Note however that the reference size is not quadratic in the number of events, due to the high number
of small self-connected components, as noted in the previous paragraph.
This has to be taken into account when considering an evaluation of a an entire corpus, and deciding what the contribution should be made by one text, or by a set of temporal relations. In the case of
strict recall, the practice has been to add all edges from the temporal closures, possibly giving a given
text a weight proportional to the square of its number of events. Micro-averaging the results on each
text is probably not desirable either, giving too much importance to small texts with few relations.
Having a reference with a size linear in the number of events solves the problem, providing a sort of
smoothing with respect to that factor.

influence of size on reference graph size

1200

strict recall
recall/point graph
relaxed recall
y=x
id

1000

size of reference

800

600

400

200

00

50

100

150

200
nb of events

250

300

350

400

Figure 24: Number of relations considered in the reference vs. number of events in the texts, according to the measure used.

405

TANNIER & M ULLER

6.2.2 M EASURES ON T IME BANK
Recall We performed the same experiments on TimeBank as we did on the synthetic data, and we
can observe the same phenomenon (a linear decrease of point recall), while raw results are much more
irregular (Figure 25), with a larger unstability when almost all relations are removed.
Note that while synthetic graph measures had values mainly above the “y=x” line, with some
variation, annotated data show the same kind of parabolic behavior below that line.
The main difference here is how the reference is structured. The way generated graphs are built
ensures that they are fully connected, and all relations from the saturated graphs are considered as
the reference, since we have no reason to distinguish any subset in particular. On the other hand, in
the human-annotated corpus, we only removed relations from the set of initial relations tagged by the
annotators.
In the first case (artificial data), the reference graph is more resistant to the removal of random
relations, since there is more redundancy, but in an extreme manner: it is only when enough relations
are removed that some information is actually lost. This leads to a parabolic curve above the “y=x”
line, showing that redundancy is improperly assessed by regular recall. The point graph is immune to
this effect since the relevant relations are isolated first.
On the natural graphs on the contrary, the annotation is very “fragile”: annotators tend not to put
too much redundant information in their annotation. Then, removing some of them removes also a
lot of inferred relations at the same time (in quadratic quantity). This would not be a problem if the
evaluation could stick to the annotated relations and if everyone tagged the same event pairs. But, as
we already noticed, it is not the case. If we consider a system trying to build a temporal graph on
a text from scratch, there is no reason to provide it with the same event pairs as the reference. For
this reason, as soon as some redundancy is broken by removing relations, the recall improperly falls
faster than “y=x” does. Again, the point-based graphs isolate the likely underlying models and their
behavior is thus more controlled.
Remember also that the level of vagueness influences the shape of the curves, and that human
annotations are less specified and thus highly disjunctive.
We can also assume than human choices, when annotating, are important. Annotated relations
are probably regarded as central by the annotators, hence close to an ideal “core” set, especially since
annotators tend to minimize the number of relations they tag. This assumption should be verified by
more experiments on human assessments.
Precision To estimate the behavior of precision measures, we slightly changed the above experiment
by switching more and more relations to different ones, thus “disturbing” the initial graph, while trying
to keep it consistent. Again, we did this a number of times, and averaged the results on points with
similar rates of undisturbed relations. The result, shown Figure 26, is restricted to smaller values of
change, since as we increase the proportion of changed relations, the graph gets closer to a purely
random annotation. So we arbitrarily kept values between 0 and 40% of relations changed at random.
Of course, without any more control on the kind of change allowed, we generate a lot of different
types of modifications, some minor as well as some which totally change the inferences drawn from
the annotation. This is reflected in the huge variance of the results we observed on the detailed data
(not shown here), even close to the origin, i.e. the unchanged graph. Still, the experiment seems to
confirm that the point-based measure follows more closely the ideal “y=x” function, so it would be
more stable than the others, with the previous caveat about the variance.
System prediction Finally, as a last indication of the importance of the evaluation methodology,
we made a comparison of the behavior of the measures on the predictions of a real system. This is a

406

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

experiment : remove

1.0
0.8

experiment : remove

1.0

strict recall
strict recall
id

recall/point graph
recall/point graph
id

0.8

score

0.6

score

0.6
0.4

0.4

0.2

0.2

0.00.0

0.2

0.4

% modified

0.6

0.8

0.00.0

1.0

0.2

0.4

% modified

0.6

0.8

1.0

Figure 25: Behavior of recall measures on TimeBank according to the amount of temporal information removed, (left) strict recall and (right) recall on the point-based graph, with solid lines
showing the parabolic regression.

experiment : disturb
precision/point graph
relaxed precision
simple precision
id

1.0
0.8

score

0.6
0.4
0.2
0.00.0

0.2

0.4

% modified

0.6

0.8

1.0

Figure 26: Linear regression of precision measures according to the amount of temporal information
disturbed from the reference (TimeBank) in the range 0-40%

407

TANNIER & M ULLER

rather tricky issue, since we want to see which measure distinguishes the better method from others,
while the only way we have of evaluating the better method is the measure we want to assess. This
was the motivation for the set of experiments we presented above. Still, it is legitimate to wonder if
we can at least observe some differences in the context of a real system. One must then be cautious
about the conclusions that can be drawn from this. In order to do so, we took an implementation
of a temporal relation classifier reproducing standard work, as for instance in the work by Mani et
al. (2006), that was used by one of the authors in another study (Denis & Muller, 2010). We applied
it to TimeBank 1.1, and checked the correlations between the usual (strict) precision/recall and the
counterparts based on transitive reduction. Figure 27 shows the relations between the scores on each
text, with a subfigure for precision and one for recall. The spearman correlation, which estimates if
one variable is monotonically related to the other without any other assumption, is 0.86 for precision,
and 0.61 for recall, with very high significance levels (p < 10−20 ). The tentative interpretation we
can make is that while they are obviously related, both types of measures show a lot of variance
on the different texts, and are obviously sensitive to differences in the predictions. Texts are indeed
ordered very differently by the different sets of measures. This is especially true for recall. We also
note that point-based scores are generally higher, which can be easily explained since relations on an
event are correct only when relations about endpoints are correct at the same time. Inferences can still
muddy the waters and yield different results in that respect, so it could be interesting to investigate the
differences in more detail.
Correlation between precision and point precision for base classifier on TB 1.1
1.0

Correlation between recall and point recall for base classifier on TB 1.1
0.9

0.8
0.8

0.7

pt_recall

pt_precision

0.6
0.6
0.4

0.5
0.4
0.3
0.2

0.2

0.1
0.00.0

0.2

0.4

precision

0.6

0.8

0.00.0

1.0

0.2

0.4

recall

0.6

0.8

1.0

Figure 27: Correlations between classical measures and our proposal. Precisions are on the left, recalls on the right.

7. Conclusion
Comparing temporal constraints graphs is crucial in the task of extracting temporal information from
texts, both from an evaluation point of view and in the perspective of incorporating global constraints
in statistical learning procedures.
We argue here for comparison measures devoid of some of the biases inherent in the commonly
used comparisons of closures of Allen-based temporal graphs. The measure is defined on the transitive
reductions of the graph of (partially) ordered interval endpoints. Transitive reduction is conceptually
intuitive, easy to compute and is unique in the cases considered. We have shown empirically that the
behavior of this kind of measure is appropriate with the goals we had in mind.
408

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

We do not claim that ordering interval endpoints should be considered as the annotation provided
by humans, only that the translation is possible and useful. It remains unclear if this could also be an
acceptable way of presenting temporal information to humans, or how the resulting minimal graphs
could be meaningfully re-translated into interval-based relations. We do not claim either that point
relations should be the target of automated procedures to extract temporal information. The bulk of
the work done on event relation classification deals primarily with extended intervals, as does the
literature on temporal semantics (Steedman, 1997; Kamp & Reyle, 1993).
We plan to check our assumption that the procedure of translating interval constraints into endpoint constraints could be useful in the task of learning temporal constraints by integration of global
constraints (for instance as a good indication of how close two temporal situations may be). It can
also be useful when designing distances between structures in order to make structured predictions, in
a manner similar to what is done on tree kernels for syntactic parsing (Collins & Duffy, 2002).
So far automated attempts only aim at predicting relations on given event-pairs, and most of them
rely on a local learning strategy that does not take into account temporal constraints on the whole
text. The few exceptions (Chambers & Jurafsky, 2008b; Bramsen et al., 2006) make use of subsets of
relations, for instance a subset {before, after}, and any other relation is considered as “vague”. Then
they are able to use integer linear programming using transitivity constraints on the temporal order.
This simplicity could be preserved in an endpoint translation of the full algebra.
The issues presented here are relevant also for other graph-based representations in natural language processing tasks, as long as there are inference issues: for instance discourse representations
are often build as a set of rhetorical relations between segments (Marcu & Echihabi, 2002). Inference
properties of such relations remain to be investigated, and automated processes are still quite rare (but
see works by Sagae, 2009; Wellner, Pustejovsky, Havasi, Rumshisky, & Saurí, 2006; Subba & Di Eugenio, 2009), however some widely adopted structures behave like partial orders (narrative chains,
topic elaborations) and thus should follow some of the patterns we have investigated. In case of partial annotations or partial agreement, finding a minimally equivalent representation could be used for
comparison.
The limitations of this methodological study are also the limitations of our knowledge of the
way human readers process the temporal information, as a whole, that can be found in texts. To the
best of our knowledge, the psycholinguistics literature has little interest in that specific question.
Work exists on local temporal interpretation, i.e. the way temporal order between two events is
determined, with respect to linguistic and extra-linguistic factors (Zwaan & Razdvansky, 2001). In
a more global context, studies have focussed on the way a reader builds a situation corresponding to
a text (Speer, Zacks, & Reynolds, 2007; Zwaan, 2008), with temporal, spatial, causal aspects, and it
seems events are grouped in time-frames, or sets of events with causal relations, and comprehension
shifts from one time-frame representation to another. If these approaches are right, readers build
mental models that correspond to the situation for a given time-frame, and do not explicitly record
relations between time-frames. This could explain the structure of a lot of annotations (a set of small
connected components), but does not say much about the nature of the knowledge represented. It
remains to be seen how investigations into the process of human comprehension could be beneficial
to computationally oriented approaches of such a process.

8. Acknowledgments
This work has benefited from tremendously helpful feedback from several colleagues, mainly Pierre
Zweigenbaum and Pascal Denis, and Nicholas Asher. Pascal Denis also contributed to the development of the temporal tools used in this work, and allowed us to use some of the results of the system
409

TANNIER & M ULLER

he developed with Philippe Muller for temporal relation identification. We also thank Michel Gagnon,
who suggested the comparison of an annotation with a degraded version of itself as an evaluation of a
measure, a long time ago.

References
Aho, A., Garey, M., & Ullman, J. (1972). The Transitive Reduction of a Directed Graph. SIAM
Journal on Computing, 1(2), 131–137.
Allen, J. (1983). Maintaining Knowledge about Temporal Intervals. Communications of the ACM,
832–843.
Asher, N., & Lascarides, A. (1993). Temporal interpretation, discourse relations, and commonsense
entailment. Linguistics and Philosophy, 16, 437–493.
Bethard, S., Martin, J. H., & Klingenstein, S. (2007). Timelines from text: Identification of syntactic temporal relations. In International Conference on Semantic Computing, pp. 11–18, Los
Alamitos, CA, USA. IEEE Computer Society.
Bramsen, P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pp.
189–198, Sydney, Australia.
Chambers, N., & Jurafsky, D. (2008a). Unsupervised Learning of Narrative Event Chains. In Proceedings of ACL-08: HLT, pp. 789–797, Columbus, Ohio. Association for Computational Linguistics, Morristown, NJ, USA.
Chambers, N., & Jurafsky, D. (2008b). Jointly combining implicit constraints improves temporal ordering. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing, pp. 698–706, Honolulu, Hawaii. Association for Computational Linguistics, Morristown, NJ, USA.
Collins, M., & Duffy, N. (2002). New ranking algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Proceedings of 40th Annual Meeting of the
Association for Computational Linguistics, pp. 263–270, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.
Denis, P., & Muller, P. (2010). Comparison of different algebras for inducing the temporal structure
of texts. In Proceedings of Coling 2010, pp. 250–258, Beijing.
Dubois, M. F., & Schwer, S. R. (2000). Classification topologique des ensembles convexes de Allen.
In Proceedings of Reconnaissance des Formes et Intelligence Artificielle (RFIA), Vol. III, pp.
59–68.
Freksa, C. (1992). Temporal reasoning based on semi-intervals. Artificial Intelligence, 54(1-2), 199–
227.
Grover, C., Hitzeman, J., & Moens, M. (1995). Algorithms for analysing the temporal structure of
discourse. In Sixth International Conference of the European Chapter of the Association for
Computational Linguistics. ACL.
Kameyama, M., Passonneau, R., & Poesio, M. (1993). Temporal centering. In Proceedings of ACL
1993, pp. 70–77.
Kamp, H., & Reyle, U. (1993). From Discourse to Logic. Kluwer Academic Publishers.

410

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

La Poutré, J., & van Leeuwen, J. (1988). Maintenance of transitive closures and transitive reductions
of graphs. In Göttler, H., & Schneider, H.-J. (Eds.), Graph-Theoretic Concepts in Computer
Science, Vol. 314 of Lecture Notes in Computer Science, pp. 106–120. Springer Berlin / Heidelberg.
Lapata, M., & Lascarides, A. (2006). Learning Sentence-internal Temporal Relations. Journal of
Artificial Intelligence Research, 27, 85–117.
Mani, I., Pustejovsky, J., & Gaizauskas, R. (Eds.). (2005). The Language of Time: A Reader. Oxford
University Press.
Mani, I., & Schiffman, B. (2005). Temporally Anchoring and Ordering Events in News. In Pustejovsky, J., & Gaizauskas, R. (Eds.), Time end Event Recognition in Natural Language. John
Benjamin.
Mani, I., Verhagen, M., Wellner, B., Lee, C. M., & Pustejovsky, J. (2006). Machine learning of
temporal relations. In Proceedings of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pp.
753–760, Sydney, Australia. Association for Computational Linguistics.
Mani, I., Wellner, B., Verhagen, M., & Pustejovsky, J. (2007). Three Approaches to Learning TLINKs
in TimeML. Tech. rep., Computer Science Department, Brandeis University. Waltham, USA.
Marcu, D., & Echihabi, A. (2002). An unsupervised approach to recognizing discourse relations.
In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pp.
368–375, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.
Muller, P., & Tannier, X. (2004). Annotating and measuring temporal relations in texts. In Proceedings
of the 20th International Conference on Computational Linguistics (Coling 04), pp. 50–56,
Geneva, Switzerland. COLING.
Passonneau, R. J. (1988). A computational model of the semantics of tense and aspect. Computational
Linguistics, 14(2), 44–60.
Pustejovsky, J., Hanks, P., Saurí, R., See, A., Gaizauskas, R., Setzer, A., Radev, D., Sundheim, B.,
Day, D., Ferro, L., & Lazo, M. (2003). The TIMEBANK Corpus. In Proceedings of Corpus
Linguistics, pp. 647–656, Lancaster University, UK.
Reichenbach, H. (1947). Elements of Symbolic Logic. McMillan, New York.
Rodríguez, A., de Weghe, N. V., & Maeyer, P. D. (2004). Simplifying Sets of Events by Selecting
Temporal Relations. In Geographic Information Science, Third International Conference, GIScience 2004, Vol. 3234/2004 of Lecture Notes in Computer Science, pp. 269–284, Adelphi,
MD, USA. Springer Berlin / Heidelberg.
Sagae, K. (2009). Analysis of discourse structure with syntactic dependencies and data-driven shiftreduce parsing. In Proceedings of the 11th International Conference on Parsing Technologies
(IWPT’09), Paris, France.
Saurí, R., Littman, J., Knippen, R., Gaizauskas, R., Setzer, A., & Pustejovsky, J. (2006). TimeML
Annotation Guidelines, Version 1.2.1.. http://timeml.org/site/.
Schilder, F. (1997). A hierarchy for convex relations. In Proceedings of the 4th International Workshop
on Temporal Representation and Reasoning (TIME ’97), pp. 86–93, Washington, DC, USA.
IEEE Computer Society.

411

TANNIER & M ULLER

Setzer, A. (2001). Temporal Information in Newswire Articles: an Annotation Scheme and Corpus
Study. Ph.D. thesis, University of Sheffield, UK.
Setzer, A., Gaizauskas, R., & Hepple, M. (2006). The Role of Inference in the Temporal Annotation
and Analysis of Text. Language Resources and Evaluation, 39, 243–265.
Song, F., & Cohen, R. (1991). Tense interpretation in the context of narrative. In Proceedings of
AAAI’91, pp. 131–136.
Sorlin, S. (2006). Mesurer la similarité de graphes. Ph.D. thesis, Université Claude Bernard, Lyon I.
Speer, N., Zacks, J., & Reynolds, J. (2007). Human brain activity time-locked to narrative event
boundaries. Psychological Science, 18(5), 449.
Steedman, M. (1997). Temporality. In van Benthem, J., & ter Meulen, A. (Eds.), Handbook of Logic
and Language. Elsevier.
Steedman, M. (1995). Dynamic semantics for tense and aspect. In Proceedings of IJCAI’95, pp.
1292–1298.
Subba, R., & Di Eugenio, B. (2009). An effective discourse parser that uses rich linguistic information.
In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computational Linguistics, pp. 566–574, Boulder,
Colorado. Association for Computational Linguistics.
Tannier, X., & Muller, P. (2008). Evaluation Metrics for Automatic Temporal Annotation of Texts.
In ELRA (Ed.), Proceedings of the Sixth International Language Resources and Evaluation
(LREC’08).
Tatu, M., & Srikanth, M. (2008). Experiments with Reasoning for Temporal Relations between
Events. In Proceedings og the 22nd International Conference on Computational Linguistics
(Coling 2008), pp. 857–864, Manchester, UK.
van Beek, P. (1992). Reasoning about qualitative temporal information. Artificial Intelligence, 58(13), 297–326.
Verhagen, M., Gaizauskas, R., Schilder, F., Hepple, M., Katz, G., & Pustejovsky, J. (2007). SemEval2007 - 15: TempEval Temporal Relation Identification. In Proceedings of SemEval workshop
at ACL 2007, Prague, Czech Republic. Association for Computational Linguistics, Morristown,
NJ, USA.
Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). A model-theoretic coreference scoring scheme. In MUC6 ’95: Proceedings of the 6th conference on Message understanding, pp. 45–52, Columbia, Maryland, USA. Association for Computational Linguistics,
Morristown, NJ, USA.
Vilain, M., Kautz, H., & van Beek, P. (1990). Constraint propagation algorithms for temporal reasoning: a revised report. In Readings in qualitative reasoning about physical systems, pp. 373–381.
Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.
Webber, B. L. (1988). Tense as discourse anaphor. Computational Linguistics, 14(2), 61–73.
Wellner, B., Pustejovsky, J., Havasi, C., Rumshisky, A., & Saurí, R. (2006). Classification of discourse
coherence relations: An exploratory study using multiple knowledge sources. In Proceedings
of the 7th SIGdial Workshop on Discourse and Dialogue, pp. 117–125, Sydney, Australia. Association for Computational Linguistics.

412

E VALUATING T EMPORAL G RAPHS VIA T RANSITIVE R EDUCTION

Zwaan, R. A., & Razdvansky, G. A. (2001). Time in narrative comprehension. In Schram, D., &
Steen, G. (Eds.), Psychology and Sociology of Literature. John Benjamins, Amsterdam.
Zwaan, R. (2008). Time in language, situation models, and mental simulations. Language Learning,
58(s1), 13–26.

413

Journal of Artiﬁcial Intelligence Research 40 (2011) 571-598

Submitted 11/10; published 03/11

Multiagent Learning in Large Anonymous Games
Ian A. Kash

kash@seas.harvard.edu

Center for Research on Computation and Society
Harvard University

Eric J. Friedman

ejf27@cornell.edu

Department of Operations Research
and Information Engineering
Cornell University

Joseph Y. Halpern

halpern@cs.cornell.edu

Department of Computer Science
Cornell University

Abstract
In large systems, it is important for agents to learn to act eﬀectively, but sophisticated
multi-agent learning algorithms generally do not scale. An alternative approach is to ﬁnd
restricted classes of games where simple, eﬃcient algorithms converge. It is shown that
stage learning eﬃciently converges to Nash equilibria in large anonymous games if bestreply dynamics converge. Two features are identiﬁed that improve convergence. First,
rather than making learning more diﬃcult, more agents are actually beneﬁcial in many
settings. Second, providing agents with statistical information about the behavior of others
can signiﬁcantly reduce the number of observations needed.

1. Introduction
Designers of distributed systems are frequently unable to determine how an agent in the
system should behave, because optimal behavior depends on the user’s preferences and
the actions of others. A natural approach is to have agents use a learning algorithm.
Many multiagent learning algorithms have been proposed including simple strategy update
procedures such as ﬁctitious play (Fudenberg & Levine, 1998), multiagent versions of Qlearning (Watkins & Dayan, 1992), and no-regret algorithms (Cesa-Bianchi & Lugosi, 2006).
Our goal in this work is to help the designers of distributed systems understand when
learning is practical. As we discuss in Section 2, existing algorithms are generally unsuitable
for large distributed systems. In a distributed system, each agent has a limited view of the
actions of other agents. Algorithms that require knowing, for example, the strategy chosen
by every agent cannot be implemented. Furthermore, the size of distributed systems requires
fast convergence. Users may use the system for short periods of time and conditions in the
system change over time, so a practical algorithm for a system with thousands or millions
of users needs to have a convergence rate that is sublinear in the number of agents. Existing
algorithms tend to provide performance guarantees that are polynomial or even exponential.
Finally, the large number of agents in the system guarantees that there will be noise. Agents
will make mistakes and will behave in unexpectedly. Even if no agent changes his strategy,
there can still be noise in agent payoﬀs. For example, a gossip protocol will match diﬀerent

c
⃝2011
AI Access Foundation. All rights reserved.

Kash, Friedman, & Halpern

agents from round to round; congestion in the underlying network may eﬀect message delays
between agents. A learning algorithm needs to be robust to this noise.
While ﬁnding an algorithm that satisﬁes these requirements for arbitrary games may
be diﬃcult, distributed systems have characteristics that make the problem easier. First,
they involve a large number of agents. Having more agents may seem to make learning
harder—after all, there are more possible interactions. However, it has the advantage that
the outcome of an action typically depends only weakly on what other agents do. This
makes outcomes robust to noise. Having a large number of agents also make it less useful
for an agent to try to inﬂuence others; it becomes a better policy to try to learn an optimal
response. In contrast, with a small number of agents, an agent can attempt to guide learning
agents into an outcome that is beneﬁcial for him.
Second, distributed systems are often anonymous; it does not matter who does something, but rather how many agents do it. For example, when there is congestion on a link,
the experience of a single agent does not depend on who is sending the packets, but on how
many are being sent. Anonymous games have a long history in the economics literature
(e.g., Blonski, 2001) and have been a subject of recent interest in the computer science
literature (Daskalakis & Papadimitriou, 2007; Gradwohl & Reingold, 2008).
Finally, and perhaps most importantly, in a distributed system the system designer
controls the game agents are playing. This gives us a somewhat diﬀerent perspective than
most work, which takes the game as given. We do not need to solve the hard problem
of ﬁnding an eﬃcient algorithm for all games. Instead, we can ﬁnd algorithms that work
eﬃciently for interesting classes of games, where for us “interesting” means “the type of
games a system designer might wish agents to play.” Such games should be “well behaved,”
since it would be strange to design a system where an agent’s decisions can inﬂuence other
agents in pathological ways.
In Section 3, we show that stage learning (Friedman & Shenker, 1998) is robust, implementable with minimal information, and converges eﬃciently for an interesting class of
games. In this algorithm, agents divide the rounds of the game into a series of stages. In
each stage, the agent uses a ﬁxed strategy except that he occasionally explores. At the end
of a stage, the agent chooses as his strategy for the next stage whatever strategy had the
highest average reward in the current stage. We prove that, under appropriate conditions, a
large system of stage learners will follow (approximate) best-reply dynamics1 despite errors
and exploration.
For games where best-reply dynamics converge, our theorem guarantees that learners
will play an approximate Nash equilibrium. In contrast to previous results, where the convergence guarantee scales poorly with the number of agents, our theorem guarantees convergence in a ﬁnite amount of time with an inﬁnite number of agents. While the assumption
that best-reply dynamics converge is a strong one, many interesting games converge under best-reply dynamics, including dominance-solvable games, games with monotone best
replies, and max-solvable games (Nisan, Schapira, & Zohar, 2008). The class of max-solvable
games in particular includes many important games such as Transmission Control Protocol
(TCP) congestion control, interdomain routing with the Border Gateway Protocol (BGP),
cost-sharing games, and stable-roommates games (Nisan, Schapira, Valiant, & Zohar, 2011).
1. In this paper, we consider best-reply dynamics where all agents update their strategy at the same time.
Some other results about best-reply dynamics assume agents update their strategy one at a time.

572

Multiagent Learning in Large Anonymous Games

Marden, Arslan, and Shamma (2007a) have observed that convergence of best-reply dynamics is often a property of games that humans design (although their observation was for a
slightly diﬀerent notion of best-reply dynamics). Moreover, convergence of best-reply dynamics is a weaker assumption than a common assumption made in the mechanism design
literature, that the games of interest have dominant strategies (each agent has a strategy
that is optimal no matter what other agents do).
Simulation results, presented in Section 4, show that convergence is fast in practice: a
system with thousands of agents can converge in a few thousand rounds. Furthermore, we
identify two factors that determine the rate and quality of convergence. One is the number
of agents: having more agents makes the noise in the system more consistent so agents can
learn using fewer observations. The other is giving agents statistical information about the
behavior of other agents; this can speed convergence by an order of magnitude. Indeed,
even noisy statistical information about agent behavior, which should be relatively easy to
obtain and disseminate, can signiﬁcantly improve performance.
While our theoretical results are limited to stage learning, they provide intuition about
why other “well behaved” learning algorithms should also converge. Our simulations, which
include two other learning algorithms, bear this out. Furthermore, to demonstrate the
applicability of stage learning in more realistic settings, we simulate the results of learning
in a scrip system (Kash, Friedman, & Halpern, 2007). Our results demonstrate that stage
learning is robust to factors such as churn (agents joining and leaving the system) and
asynchrony (agents using stages of diﬀerent lengths). However, stage learning is not robust
to all changes. We include simulations of games with a small number of agents, games
that are not anonymous, and games that are not continuous. These games violate the
assumptions of our theoretical results; our simulations show that, in these games, stage
learning converges very slowly or not at all.
Finally, not all participants in a system will necessarily behave as expected. For learning
to be useful in a real system, it needs to be robust to such behavior. In Section 5, we show
that the continuity of utility functions is a key property that makes stage learning robust
to Byzantine behavior by a small fraction of agents.

2. Related Work
One approach to learning to play games is to generalize reinforcement learning algorithms
such as Q-learning (Watkins & Dayan, 1992). One nice feature of this approach is that it
can handle games with state, which is important in distributed systems. In Q-learning, an
agent associates a value with each state-action pair. When he chooses action 𝑎 in state 𝑠,
he updates the value 𝑄(𝑠, 𝑎) based on the reward he received and the best value he can
achieve in the resulting state 𝑠′ (max𝑎′ 𝑄(𝑠′ , 𝑎′ )). When generalizing to multiple agents, 𝑠
and 𝑎 become vectors of the state and action of every agent and the max is replaced by a
prediction of the behavior of other agents. Diﬀerent algorithms use diﬀerent predictions;
for example, Nash-Q uses a Nash equilibrium calculation (Hu & Wellman, 2003). See the
work of Shoham, Powers, and Grenager (2003) for a survey.
Unfortunately, these algorithms converge too slowly for a large distributed system. The
algorithm needs to experience each possible action proﬁle many times to guarantee convergence. So, with 𝑛 agents and 𝑘 strategies, the naive convergence time is 𝑂(𝑘𝑛 ). Even with
573

Kash, Friedman, & Halpern

a better representation for anonymous games, the convergence time is still 𝑂(𝑛𝑘 ) (typically
𝑘 ≪ 𝑛). There is also a more fundamental problem with this approach: it assumes information that an agent is unlikely to have. In order to know which value to update, the
agent must learn the action chosen by every other agent. In practice, an agent will learn
something about the actions of the agents with whom he directly interacts, but is unlikely
to gain much information about the actions of other agents.
Another approach is no-regret learning, where agents choose a strategy for each round
that guarantees that the regret of their choices will be low. Hart and Mas-Colell (2000)
present such a learning procedure that converges to a correlated equilibrium 2 given knowledge of what the payoﬀs of every action would have been in each round. They also provide a variant of their algorithm that requires only information about the agent’s actual
payoﬀs (Hart & Mas-Colell, 2001). However, to guarantee convergence to within 𝜖 of a
correlated equilibrium requires 𝑂(𝑘𝑛/𝜖2 log 𝑘𝑛), still too slow for large systems. Furthermore, the convergence guarantee is that the distribution of play converges to equilibrium;
the strategies of individual learners will not converge. Many other no-regret algorithms
exist (Blum & Mansour, 2007). In Section 4, we use the Exp3 algorithm (Auer, CesaBianchi, Freund, & Schapire, 2002). They can achieve even better convergence in restricted
settings. For example, Blum, Even-Dar, and Ligett (2006) showed that in routing games
a continuum of no-regret learners will approximate Nash equilibrium in a ﬁnite amount of
time. Jafari, Greenwald, Gondek, and Ercal (2001) showed that no-regret learners converge
to Nash equilibrium in dominance solvable, constant sum, and general sum 2 × 2 games.
Foster and Young (2006) use a stage-learning procedure that converges to Nash equilibrium for two-player games. Germano and Lugosi (2007) showed that it converges for generic
𝑛-player games (games where best replies are unique). Young (2009) uses a similar algorithm without explicit stages that also converges for generic 𝑛-player games. Rather than
selecting best replies, in these algorithms agents choose new actions randomly when not in
equilibrium. Unfortunately, these algorithms involve searching the whole strategy space,
so their convergence time is exponential. Another algorithm that uses stages to provide a
stable learning environment is the ESRL algorithm for coordinated exploration (Verbeeck,
Nowé, Parent, & Tuyls, 2007).
Marden, Arslan, and Shamma (2007b) and Marden, Young, Arslan, and Shamma (2009)
use an algorithm with experimentation and best replies but without explicit stages that
converges for weakly acyclic games, where best-reply dynamics converge when agents move
one at a time, rather than moving all at once, as we assume here. Convergence is based
on the existence of a sequence of exploration moves that lead to equilibrium. With 𝑛
agents who explore with probability 𝜖, this analysis gives a convergence time of 𝑂(1/𝜖𝑛 ).
Furthermore, the guarantee requires 𝜖 to be suﬃciently small that agents essentially explore
one at a time, so 𝜖 needs to be 𝑂(1/𝑛).
Adlakha, Johari, Weintraub, and Goldsmith (2010) have independently given conditions
for the existence of an “oblivious equilibrium,” or “mean ﬁeld equilibrium,” in stochastic
games. Just as in our model they require that the game be large, anonymous, and continuous. In an oblivious equilibrium, each player reacts only to the “average” states and
2. Correlated equilibrium is a more general solution concept than Nash equilibrium (see Osborne & Rubenstein, 1994); every Nash equilibrium is a correlated equilibrium, but there may be correlated equilibria
that are not Nash equilibria.

574

Multiagent Learning in Large Anonymous Games

strategies of other players rather than their exact values. However, this model assumes that
a player’s payoﬀ depends only on the state of other players and not their actions. Adlakha
and Johari (2010) consider stochastic games with strategic complementarities and show that
mean ﬁeld equilibria exist, best-reply dynamics converge, and “myopic learning dynamics”
(which require only knowledge of the aggregate states of other players) can ﬁnd them.
There is a long history of work examining simple learning procedures such as ﬁctitious
play (Fudenberg & Levine, 1998), where each agent makes a best response assuming that
each other player’s strategy is characterized by the empirical frequency of his observed
moves. In contrast to algorithms with convergence guarantees for general games, these algorithms fail to converge in many games. But for classes of games where they do converge,
they tend to do so rapidly. However, most work in this area assumes that the actions of
agents are observed by all agents, agents know the payoﬀ matrix, and payoﬀs are deterministic. A recent approach in this tradition is based on the Win or Learn Fast principle,
which has limited convergence guarantees but often performs well in practice (Bowling &
Veloso, 2001). Hopkins (1999) showed that many such procedures converge in symmetric
games with an inﬁnite number of learners, although his results provide no guarantees about
the rate of convergence.
There is also a body of empirical work on the convergence of learning algorithms in
multiagent settings. Q-learning has had empirical success in pricing games (Tesauro &
Kephart, 2002), 𝑛-player cooperative games (Claus & Boutilier, 1998), and grid world
games (Bowling, 2000). Greenwald at al. (2001) showed that a number of algorithms,
including stage learning, converge in a variety of simple games. Marden et al. (2009) found
that their algorithm converged must faster in a congestion game than the theoretical analysis
would suggest. Our theorem suggests an explanation for these empirical observations: bestreply dynamics converge in all these games. While our theorem applies directly only to
stage learning, it provides intuition as to why algorithms that learn “quickly enough” and
change their behavior “slowly enough” rapidly converge to Nash equilibrium in practice.

3. Theoretical Results
In this section we present the theoretical analysis of our model. We then provide support
from simulations in the following section.
3.1 Large Anonymous Games
We are interested in anonymous games with countably many agents. Assuming that there
are countably many agents simpliﬁes the proofs; it is straightforward to extend our results
to games with a large ﬁnite number of agents. Our model is adapted from that of Blonski (2001). Formally, a large anonymous game is characterized by a tuple Γ = (ℕ, 𝐴, 𝑃, Pr).
∙ ℕ is the countably inﬁnite set of agents.
∙ 𝐴 is a ﬁnite set of actions from which each agent can choose (for simplicity, we assume
that each agent can choose from the same set of actions).
∙ Δ(𝐴), the set of probability distributions over 𝐴, has two useful interpretations. The
ﬁrst is as the set of mixed actions. For 𝑎 ∈ 𝐴 we will abuse notation and denote the
575

Kash, Friedman, & Halpern

mixed action that is 𝑎 with probability 1 as 𝑎. In each round each agent chooses one
of these mixed actions. The second interpretation of 𝜌 ∈ Δ(𝐴) is as the fraction of
agents choosing each action 𝑎 ∈ 𝐴. This is important for our notion of anonymity,
which says an agent’s utility should depend only on how many agents choose each
action rather than who chooses it.
∙ 𝐺 = {𝑔 : ℕ → Δ(𝐴)} is the set of (mixed) action proﬁles (i.e. which action each
agent chooses). Given the mixed action of every agent, we want to know the fraction
of agents that end up choosing action 𝑎. For 𝑔 ∈ 𝐺, let 𝑔(𝑖)(𝑎) denote the probability
with which agent 𝑖 plays 𝑎 according to 𝑔(𝑖) ∈ Δ(𝐴).∑We can then express the fraction
of agents in 𝑔 that choose action 𝑎 as lim𝑛→∞ (1/𝑛) 𝑛𝑖=0 𝑔(𝑖)(𝑎), if this limit exists. If
the limit exists for all actions 𝑎 ∈ 𝐴, let 𝜌𝑔 ∈ Δ(𝐴) give the value of the limit for each
𝑎. The proﬁles 𝑔 that we use are all determined by a simple random process. For such
proﬁles 𝑔, the strong law of large numbers (SLLN) guarantees that with probability
1 𝜌𝑔 is well deﬁned. Thus it will typically be well deﬁned (using similar limits) for us
to talk about the fraction of agents who do something.
∙ 𝑃 ⊂ ℝ is the ﬁnite set of payoﬀs that agents can receive.
∙ Pr : 𝐴 × Δ(𝐴) → Δ(𝑃 ) denotes the distribution over payoﬀs that results when the
agent performs action 𝑎 and other agents follow action proﬁle 𝜌. We use a probability
distribution over payoﬀs rather than a payoﬀ to model the fact that agent payoﬀs may
change even if no agent changes his strategy. The expected utility of an agent who
performs
∑
∑ mixed action 𝑠 when other agents follow action distribution 𝜌 is 𝑢(𝑠, 𝜌) =
𝑎∈𝐴
𝑝∈𝑃 𝑝𝑠(𝑎) Pr𝑎,𝜌 (𝑝). Our deﬁnition of Pr in terms of Δ(𝐴) rather than 𝐺
ensures the game is anonymous. We further require that Pr (and thus 𝑢) be Lipschitz
continuous.3 For deﬁniteness, we use the L1 norm as our notion of distance when
specifying continuity (the L1 distance between two vectors is the sum of the absolute
values of the diﬀerences in each component). Note that this formulation assumes all
agents share a common utility function. This assumption can be relaxed to allow
agents to have a ﬁnite number of types, which we show in Appendix A.
An example of a large anonymous game is one where, in each round, each agent plays a
two-player game against an opponent chosen at random. Such random matching games are
common in the literature (e.g., Hopkins, 1999), and the meaning of “an opponent chosen
at random” can be made formal (Boylan, 1992). In such a game, 𝐴 is the set of actions of
the two-player game and 𝑃 is the set of payoﬀs of the game. Once every agent chooses an
action, the distribution over opponent actions is characterized by some 𝜌 ∈ Δ(𝐴). Let 𝑝𝑎,𝑎′
denote the payoﬀ for the agent if he plays 𝑎 and the other agent plays 𝑎′ . Then the utility
of mixed action 𝑠 given distribution 𝜌 is
∑
𝑢(𝑠, 𝜌) =
𝑠(𝑎)𝜌(𝑎′ )𝑝𝑎,𝑎′ .
𝑎,𝑎′ ∈𝐴2

3. Lipschitz continuity imposes the additional constraint that there is some constant 𝐾 such that ∣ Pr(𝑎, 𝜌)−
Pr(𝑎, 𝜌′ )∣/∣∣𝜌 − 𝜌′ ∣∣1 ≤ 𝐾 for all 𝜌 and 𝜌′ . Intuitively, this ensures that the distribution of outcomes does
not change “too fast.” This is a standard assumption that is easily seen to hold in the games that have
typically been considered in the literature.

576

Multiagent Learning in Large Anonymous Games

3.2 Best-Reply Dynamics
Given a game Γ and an action distribution 𝜌, a natural goal for an agent is to play the
action that maximizes his expected utility with respect to 𝜌: argmax𝑎∈𝐴 𝑢(𝑎, 𝜌). We call
such an action a best reply to 𝜌. In a practical amount of time, an agent may have diﬃculty
determining which of two actions with close expected utilities is better, so we will allow
agents to choose actions that are close to best replies. If 𝑎 is a best reply to 𝜌, then 𝑎′ is
an 𝜂-best reply to 𝜌 if 𝑢(𝑎′ , 𝜌) + 𝜂 ≥ 𝑢(𝑎, 𝜌). There may be more than one 𝜂-best reply; we
denote the set of 𝜂-best replies ABR 𝜂 (𝜌).
We do not have a single agent looking for a best reply; every agent is trying to ﬁnd a one
at the same time. If agents start oﬀ with some action distribution 𝜌0 , after they all ﬁnd a
best reply there will be a new action distribution 𝜌1 . We assume that 𝜌0 (𝑎) = 1/∣𝐴∣ (agents
choose their initial strategy uniformly at random), but our results apply to any distribution
used to determine the initial strategy. We say that a sequence (𝜌0 , 𝜌1 , . . .) is an 𝜂-bestreply sequence if the support of 𝜌𝑖+1 is a subset of ABR𝜂 (𝜌𝑖 ); that is 𝜌𝑖+1 gives positive
probability only to approximate best replies to 𝜌𝑖 . A 𝜂 best-reply sequence converges if
there exists some 𝑡 such that for all 𝑡′ > 𝑡, 𝜌𝑡′ = 𝜌𝑡 . Note that this is a particularly
strong notion of convergence because we require the 𝜌𝑡 to converge in ﬁnite time and not
merely in the limit. A game may have inﬁnitely many best-reply sequences, so we say that
approximate best-reply dynamics converge if there exists some 𝜂 > 0 such that every 𝜂-bestreply sequence converges. The limit distribution 𝜌𝑡 determines a mixed strategy that is an
𝜂-Nash equilibrium (i.e. the support of 𝜌𝑡 is a subset of 𝐴𝐵𝑅𝜂 (𝜌𝑡 )).
Our main result shows that learners can successfully learn in large anonymous games
where approximate best-reply dynamics converge. The number of stages needed to converge
is determined by the number of best replies needed before the sequence converges. It is possible to design games that have long best-reply sequences, but in practice most games have
short sequences. One condition that guarantees this is if 𝜌0 and all the degenerate action
distributions 𝑎 ∈ 𝐴 (i.e., distributions that assign probability 1 to some 𝑎 ∈ 𝐴) have unique
best replies. In this case, there can be at most ∣𝐴∣ best replies before equilibrium is reached,
because we have assumed that all agents have the same utility function. Furthermore, in
such games the distinction between 𝜂-best replies and best replies is irrelevant; for suﬃciently small 𝜂, a 𝜂-best reply is a best reply. It is not hard to show that the property that
degenerate strategies have unique best replies is generic; it holds for almost every game.
3.3 Stage Learners
An agent who wants to ﬁnd a best reply may not know the set of payoﬀs 𝑃 , the mapping
from actions to distributions over payoﬀs Pr, or the action distribution 𝜌 (and, indeed, 𝜌
may be changing over time), so he will have to use some type of learning algorithm to learn
it. Our approach is to divide the play of the game into a sequence of stages. In each stage,
the agent almost always plays some ﬁxed action 𝑎, but also explores other actions. At the
end of the stage, he chooses a new 𝑎′ for the next stage based on what he has learned. An
important feature of this approach is that agents maintain their actions for the entire stage,
so each stage provides a stable environment in which agents can learn. To simplify our
results, we specify a way of exploring and learning within a stage (originally described in
Friedman & Shenker, 1998), but our results should generalize to any “reasonable” learning
577

Kash, Friedman, & Halpern

algorithm used to learn within a stage. (We discuss what is “reasonable” in Section 6.) In
this section, we show that, given a suitable parameter, at each stage most agents will have
learned a best reply to the environment of that stage.
Given a game Γ, in each round 𝑡 agent 𝑖 needs to select a mixed action 𝑠𝑖,𝑡 . Our
agents use strategies that we denote 𝑎𝜖 , for 𝑎 ∈ 𝐴, where 𝑎𝜖 (𝑎) = 1 − 𝜖 and 𝑎𝜖 (𝑎′ ∕= 𝑎) =
𝜖/(∣𝐴∣ − 1). Thus, with 𝑎𝜖 , an agent almost always plays 𝑎, but with probability 𝜖 explores
other strategies uniformly at random. Thus far we have not speciﬁed what information an
agent can use to choose 𝑠𝑖,𝑡. Diﬀerent games may provide diﬀerent information. All that
we require is that an agent know all of his previous actions and his previous payoﬀs. More
precisely, for all 𝑡′ < 𝑡, he knows his action 𝑎𝑡′ (𝑖) (which is determined by 𝑠𝑖,𝑡′ ) and his
payoﬀs 𝑝𝑡′ (𝑖) (which is determined by Pr(𝑎𝑖,𝑡′ , 𝜌𝑡′ ), where 𝜌𝑡′ is the action distribution for
round 𝑡′ ; note that we do not assume that the agent knows 𝜌𝑡′ .) Using this information, we
can express the average value of an action over the previous 𝜏 = ⌈1/𝜖2 ⌉ rounds (the length
of a stage).4 Let 𝐻(𝑎, 𝑖, 𝑡) = {𝑡 − 𝜏 ≤ 𝑡′ < 𝑡 ∣ 𝑎𝑡′ (𝑖) = 𝑎} be ∑
the set of recent rounds in
which 𝑎 was played by 𝑖. Then the average value is 𝑉 (𝑎, 𝑖, 𝑡) = 𝑡′ ∈𝐻(𝑎,𝑖,𝑡) 𝑝𝑡′ (𝑖)/∣𝐻(𝑎, 𝑖, 𝑡)∣
if ∣𝐻(𝑎, 𝑖, 𝑡)∣ > 0 and 0 otherwise. While we need the value of 𝐻 only at times that are
multiples of 𝜏 , for convenience we deﬁne it for arbitrary times 𝑡.
We say that an agent is an 𝜖-stage learner if he chooses his actions as follows. If 𝑡 = 0,
𝑠𝑡 is chosen at random from {𝑎𝜖 ∣ 𝑎 ∈ 𝐴}. If 𝑡 is a nonzero multiple of 𝜏 , 𝑠𝑖,𝑡 = 𝑎(𝑖, 𝑡)𝜖 where
𝑎(𝑖, 𝑡) = argmax𝑎∈𝐴 𝑉 (𝑎, 𝑖, 𝑡). Otherwise, 𝑠𝑖,𝑡 = 𝑠𝑖,𝑡−1 . Thus, within a stage, his mixed
action is ﬁxed; at the end of a stage he updates it to use the action with the highest average
value during the previous stage.
The evolution of a game played by stage learners is not deterministic; each agent chooses
a random 𝑠𝑖,0 and the sequence of 𝑎𝑡 (𝑖) and 𝑝𝑡 (𝑖) he observes is also random. However, with
a countably inﬁnite set of agents, we can use the SLLN to make statements about the overall
behavior of the game. Let 𝑔𝑡 (𝑖) = 𝑠𝑖,𝑡. A run of the game consists of a sequence of triples
(𝑔𝑡 , 𝑎𝑡 , 𝑝𝑡 ). The SLLN guarantees that with probability 1 the fraction of agents who choose
a strategy 𝑎 in 𝑎𝑡 is 𝜌𝑔𝑡 (𝑎). Similarly, the fraction of agents who chose 𝑎 in 𝑎𝑡 that receive
payoﬀ 𝑝 will be Pr(𝑎, 𝜌𝑔𝑡 )(𝑝) with probability 1.
To make our notion of a stage precise, we refer to the sequence of tuples
(𝑔𝑛𝜏 , 𝑎𝑛𝜏 , 𝑝𝑛𝜏 ) . . . (𝑔(𝑛+1)𝜏 −1 , 𝑎(𝑛+1)𝜏 −1 , 𝑝(𝑛+1)𝜏 −1 ) as stage 𝑛 of the run. During stage 𝑛 there
is a stationary action distribution that we denote 𝜌𝑔𝑛𝜏 . If 𝑠𝑖,(𝑛+1)𝜏 = 𝑎𝜖 and 𝑎 ∈ ABR 𝜂 (𝑔𝑛𝜏 ),
then we say that agent 𝑖 has learned an 𝜂-best reply during stage 𝑛 of the run. As the following lemma shows, for suﬃciently small 𝜖, most agents will learn an 𝜂-best reply.
Lemma 3.1. For all large anonymous games Γ, action proﬁles, approximations 𝜂 > 0, and
probabilities of error 𝑒 > 0, there exists an 𝜖∗ > 0 such that for 𝜖 < 𝜖∗ and all 𝑛, if all
agents are 𝜖-stage learners, then at least a 1 − 𝑒 fraction of agents will learn an 𝜂-best reply
during stage 𝑛.
Proof. (Sketch) On average, an agent using strategy 𝑎𝜖 plays action 𝑎 (1 − 𝜖)𝜏 times during
a stage and plays all other actions 𝜖𝜏 /(𝑛 − 1) times each. For 𝜏 large, the realized number
of times played will be close to the expectation value with high probability. Thus, if 𝜖𝜏 is
suﬃciently large, then the average payoﬀ from each action will be exponentially close to the
4. The use of the exponent 2 is arbitrary. We require only that the expected number of times a strategy is
explored increases as 𝜖 decreases.

578

Multiagent Learning in Large Anonymous Games

true expected value (via a standard Hoeﬀding bound on sums of i.i.d. random variables), and
thus each learner will correctly identify an action with approximately the highest expected
payoﬀ with probability at least 1 − 𝑒. By the SLLN, at least a 1 − 𝑒 fraction of agents will
learn an 𝜂-best reply. A detailed version of this proof in a more general setting can be found
in the work by Friedman and Shenker (1998).
3.4 Convergence Theorem
Thus far we have deﬁned large anonymous games where approximate best-reply dynamics
converge. If all agents in the game are 𝜖-stage learners, then the sequence 𝜌ˆ0 , 𝜌ˆ1 , . . . of
action distributions in a run of the game is not a best-reply sequence, but it is close. The
action used by most agents most of the time in each 𝜌ˆ𝑛 is the action used in 𝜌𝑛 for some
approximate best reply sequence.
In order to prove this, we need to deﬁne “close.” Our deﬁnition is based on the error rate
𝑒 and exploration rate 𝜖 that introduces noise into 𝜌ˆ𝑛 . Intuitively, distribution 𝜌ˆ is close to
𝜌 if, by changing the strategies of an 𝑒 fraction of agents and having all agents explore an 𝜖
fraction of the time, we can go from an action proﬁle with corresponding action distribution
𝜌 to one with corresponding distribution 𝜌ˆ. Note that this deﬁnition will not be symmetric.
In this deﬁnition, 𝑔 identiﬁes what (pure) action each agent is using that leads to 𝜌, 𝑔′
allows an 𝑒 fraction of agents to use some other action, and 𝑔ˆ incorporates the fact that
each agent is exploring, so each strategy is an 𝑎𝜖 (the agent usually plays 𝑎 but explores
with probability 𝜖).
Deﬁnition 3.2. Action distribution 𝜌ˆ is (𝑒, 𝜖)-close to 𝜌 if there exist 𝑔, 𝑔′ , and 𝑔ˆ ∈ 𝐺 such
that:
∙ 𝜌 = 𝜌𝑔 and 𝜌ˆ = 𝜌𝑔ˆ ;
∙ 𝑔(𝑖) ∈ 𝐴 for all 𝑖 ∈ ℕ;
∙ ∣∣𝜌𝑔 − 𝜌𝑔′ ∣∣1 ≤ 2𝑒 (this allows an 𝑒 fraction of agents in 𝑔′ to play a diﬀerent strategy
from 𝑔);
∙ for some 𝜖′ ≤ 𝜖, if 𝑔′ (𝑖) = 𝑎 then 𝑔ˆ(𝑖) = 𝑎𝜖′ .
The use of 𝜖′ in the ﬁnal requirement ensures that if two distributions are (𝑒, 𝜖)-close
then they are also (𝑒′ , 𝜖′ )-close for all 𝑒′ ≥ 𝑒 and 𝜖′ ≥ 𝜖. As an example of the asymmetry of
this deﬁnition, 𝑎𝜖 is (0, 𝜖) close to 𝑎, but the reverse is not true. While (𝑒, 𝜖)-closeness is a
useful distance measure for our analysis, it is an unnatural notion of distance for specifying
the continuity of 𝑢, where we used the L1 norm. The following simple lemma shows that
this distinction is unimportant; if 𝜌ˆ is suﬃciently (𝑒, 𝜖)-close to 𝜌 then it is close according
to the L1 measure as well.
Lemma 3.3. If 𝜌ˆ is (𝑒, 𝜖)-close to 𝜌, then ∣∣ˆ
𝜌 − 𝜌∣∣1 ≤ 2(𝑒 + 𝜖).
Proof. Since 𝜌ˆ is (𝑒, 𝜖)-close to 𝜌, there exist 𝑔, 𝑔′ , and 𝑔ˆ as in Deﬁnition 3.2. Consider the
distributions 𝜌𝑔 = 𝜌, 𝜌𝑔′ , and 𝜌𝑔ˆ = 𝜌ˆ. We can view these three distributions as vectors, and
calculate their L1 distances. By Deﬁnition 3.2, ∣∣𝜌𝑔 − 𝜌𝑔′ ∣∣1 ≤ 2𝑒. ∣∣𝜌𝑔′ − 𝜌𝑔ˆ ∣∣1 ≤ 2𝜖 because
an 𝜖 fraction of agents explore. Thus by the triangle inequality, the L1 distance between 𝜌
and 𝜌ˆ is at most 2(𝑒 + 𝜖).
579

Kash, Friedman, & Halpern

We have assumed that approximate best reply sequences of 𝜌𝑛 converge, but during
a run of the game agents will actually be learning approximate best replies to 𝜌ˆ𝑛 . The
following lemma shows that this distinction does not matter if 𝜌 and 𝜌ˆ are suﬃciently close.
Lemma 3.4. For all 𝜂 there exists a 𝑑𝜂 such that if 𝜌ˆ is (𝑒, 𝜖)-close to 𝜌, 𝑒 > 0, 𝜖 > 0, and
𝑒 + 𝜖 < 𝑑𝜂 then ABR (𝜂/2) (ˆ
𝜌) ⊆ ABR 𝜂 (𝜌).
Proof. Let 𝐾 be the maximum of the Lipschitz constants for all 𝑢(𝑎, ⋅) (one constant for each
𝑎) and 𝑑𝜂 = 𝜂/(8𝐾). Then for all 𝜌ˆ that are (𝑒, 𝜖)-close to 𝜌 and all 𝑎, ∣𝑢(𝑎, 𝜌ˆ) − 𝑢(𝑎, 𝜌)∣ ≤
∣∣ˆ
𝜌 − 𝜌∣∣1 𝐾 ≤ 2𝜂/(8𝐾)𝐾 = 𝜂/4 by Lemma 3.3.
Let 𝑎 ∈
/ ABR 𝜂 (𝜌) and 𝑎′ ∈ argmax𝑎′ 𝑢(𝑎′ , 𝜌). Then 𝑢(𝑎, 𝜌) + 𝜂 < 𝑢(𝑎′ , 𝜌). Combining
this with the above gives 𝑢(𝑎, 𝜌ˆ) + 𝜂/2 < 𝑢(𝑎′ , 𝜌ˆ). Thus 𝑎 ∈
/ ABR𝜂/2 (ˆ
𝜌).
Lemmas 3.1 and 3.4 give requirements on (𝑒, 𝜖). In the statement of the theorem, we call
(𝑒, 𝜖) 𝜂-acceptable if they satisfy the requirements of both lemmas for 𝜂/2 and all 𝜂-best-reply
sequences converge in Γ.
Theorem 3.5. Let Γ be a large anonymous game where approximate best-reply dynamics
converge and let (𝑒, 𝜖) be 𝜂-acceptable for Γ. If all agents are 𝜖-stage learners then, for all
runs, there exists an 𝜂-best-reply sequence 𝜌0 , 𝜌1 , . . . such that in stage 𝑛 at least a 1 − 𝑒
fraction will learn a best reply to 𝜌𝑛 with probability 1.
Proof. 𝜌0 = 𝜌ˆ0 (both are the uniform distribution), so 𝜌ˆ0 is (𝑒, 𝜖)-close to 𝜌. Assume 𝜌ˆ𝑛 is
(𝑒, 𝜖)-close to 𝜌. By Lemma 3.1 at least a 1 − 𝑒 fraction will learn a 𝜂/2-best reply to 𝜌ˆ𝑛 .
By Lemma 3.4, this is a 𝜂-best reply to 𝜌𝑛 . Thus 𝜌ˆ𝑛+1 will be (𝑒, 𝜖)-close to 𝜌𝑛+1 .
Theorem 3.5 guarantees that after a ﬁnite number of stages, agents will be close to
an approximate Nash equilibrium proﬁle. Speciﬁcally, 𝜌ˆ𝑛 will be (𝑒, 𝜖)-close to an 𝜂-Nash
equilibrium proﬁle 𝜌𝑛 . Note that this means that 𝜌ˆ𝑛 is actually an 𝜂 ′ -Nash equilibrium for
a larger 𝜂 ′ that depends on 𝜂,𝑒,𝜖, and the Lipschitz constant 𝐾.
Our three requirements for a practical learning algorithm were that it require minimal
information, converge quickly in a large system, and be robust to noise. Stage learning
requires only that an agent know his own payoﬀs, so the ﬁrst condition is satisﬁed. Theorem 3.5 shows that it satisﬁes the other two requirements. Convergence is guaranteed in a
ﬁnite number of stages. While the number of stages depends on the game, in Section 3.2 we
argued that in many cases it will be quite small. Finally, robustness comes from tolerating
an 𝑒 fraction of errors. While in our proofs we assumed these errors were due to learning,
the analysis is the same if some of this noise is from other sources such as churn or agents
making errors. We discuss this issue more in Section 6.

4. Simulation Results
In this section, we discuss experimental results that demonstrate the practicality of learning
in large anonymous games. Theorem 3.5 guarantees convergence for a suﬃciently small
exploration probability 𝜖, but decreasing 𝜖 also increases 𝜏 , the length of a stage. Our
ﬁrst set of experiments shows that the necessary values of 𝜖 and 𝜏 are quite reasonable in
practice. While our theorem applies only to stage learning, the analysis provides intuition as
580

Multiagent Learning in Large Anonymous Games

to why a reasonable algorithm that changes slowly enough that other learners have a chance
to learn best replies should converge as well. To demonstrate this, we also implemented two
other learning algorithms, which also quickly converged.
Our theoretical results make two signiﬁcant predictions about factors that inﬂuence
the rate of convergence. Lemma 3.1 tells us that the length of a stage is determined by
the number of times each strategy needs to be explored to get an accurate estimate of its
value. Thus, the amount of information provided by each observation has a large eﬀect
on the rate of convergence. For example, in a random matching game, an agent’s payoﬀ
provides information about the strategy of one other agent. On the other hand, if he
receives his expected payoﬀ for being matched, a single observation provides information
about the entire distribution of strategies. In the latter case the agent can learn with many
fewer observations. A related prediction is that having more agents will lead to faster
convergence, particularly in games where payoﬀs are determined by the average behavior of
other agents, because variance in payoﬀs due to exploration and mistakes decreases as the
number of agents increases. Our experimental results illustrate both of these phenomena.
The game used in our ﬁrst set of experiments, like many simple games used to test
learning algorithms, is symmetric. Hopkins (1999) showed that many learning algorithms
are well behaved in symmetric games with large populations. To demonstrate that our
main results are due to something other than symmetry, we also tested stage learning on
an asymmetric game, and observed convergence even with a small population.
To explore the applicability of stage learning in a more practical setting that violates a
number of the assumptions of our theorem, we implemented a variant of stage learning for
a game based on a scrip system (Kash et al., 2007). To demonstrate the applicability of
this approach to real systems, we included experiments where there is churn (agents leaving
and being replaced by new agents) and agents learning at diﬀerent rates.
Finally, we give examples of games that are not large, not anonymous, and not continuous, and provide simulations showing that stage learners learn far more slowly in these
games than in those that satisfy the hypotheses of Theorem 3.5, or do not learn to play
equilibrium at all. These examples demonstrate that these assumptions are essential for
our results.
4.1 A Contribution Game
In our ﬁrst set of experiments, agents play a contribution game (also called a Diamondtype search model in the work by Milgrom & Roberts, 1990). In the contribution game,
two agents choose strategies from 0 to 19, indicating how much eﬀort they contribute to a
collective enterprise. The value to an agent depends on how much he contributes, as well
as how much the other agent contributes. If he contributes 𝑥 and the contribution of the
other agent is 𝑦, then his utility is 4𝑥𝑦 − (𝑥 − 5)3 . In each round of our game, each agent is
paired with a random agent and they play the contribution game. In this game, best-reply
dynamics converge within 4 stages from any starting distribution.
We implemented three learning algorithms to run on this game. Our implementation of
stage learners is as described in Section 3.3, with 𝜖 = 0.05. Rather than taking the length of
stage 𝜏 to be 1/𝜖2 , we set 𝜏 = 2500 to have suﬃciently long stages for this value of 𝜖, rather
than decreasing 𝜖 until stages are long enough. Our second algorithm is based on that of

581

Kash, Friedman, & Halpern

6
2 Agents
10 Agents
100 Agents

Distance from Equilibrium

5

4

3

2

1

0

0

1

2

3

4

Time

5
4

x 10

Figure 1: Stage learners with random matching.
3.5
2 Agents
10 Agents
100 Agents

Distance from Equilibrium

3

2.5

2

1.5

1

0.5

0

1

2

3
Time

4

5
4

x 10

Figure 2: Hart and Mas-Colell with random matching.
Hart and Mas-Colell (2001), with improvements suggested by Greenwald, Friedman, and
Shenker (2001). This algorithm takes parameters 𝑀 and 𝛿 (the exploration probability).
We used 𝑀 = 16 and 𝛿 = 0.05. Our ﬁnal learning algorithm is Exp3 (Auer et al., 2002).
We set 𝛾, the exploration probability, to 0.05. This algorithm requires that payoﬀs be
normalized to lie in [0, 1]. Since a few choices of strategies lead to very large negative
payoﬀs, a naive normalization leads to almost every payoﬀ being close to 1. For better
performance, we normalized payoﬀs such that most payoﬀs fell into the range [0, 1] and any
that were outside were set to 0 or 1 as appropriate.
The results of these three algorithms are shown in Figures 1, 2, and 3. Each curve
shows the distance from equilibrium as a function of the number of rounds of a population
of agents of a given size using a given learning algorithm. The results were averaged over
ten runs. Since the payoﬀs for nearby strategies are close, we want our notion of distance
to take into account that agents playing 7 are closer to equilibrium (8) than∑those playing
zero. Therefore, we consider the expected distance of 𝜌 from equilibrium:
𝑎 𝜌(𝑎)∣𝑎 − 8∣.
To determine 𝜌, we counted the number of times each action was taken over the length of
582

Multiagent Learning in Large Anonymous Games

5
2 Agents
10 Agents
100 Agents

4.5

Distance from Equilibrium

4
3.5
3
2.5
2
1.5
1
0.5

0

1

2

3

4

Time

5
4

x 10

Figure 3: Exp3 with random matching.
6
2 Agents
10 Agents
100 Agents

Distance from Equilibrium

5

4

3

2

1

0

0

1

2

3
Time

4

5
4

x 10

Figure 4: Stage learning with average-based payoﬀs.
a stage, so in practice the distance will never be zero due to mistakes and exploration. For
ease of presentation, the graph shows only populations of size up to 100; similar results
were obtained for populations up to 5000 agents.
For stage learning, increasing the population size has a dramatic impact. With two
agents, mistakes and best replies to the results of these mistakes cause behavior to be quite
chaotic. With ten agents, agents successfully learn, although mistakes and suboptimal
strategies are quite frequent. With one hundred agents, all the agents converge quickly to
near equilibrium strategies and signiﬁcant mistakes are rare.
Despite a lack of theoretical guarantees, our other two algorithms also converge, although
somewhat more slowly. The long-run performance of Exp3 is similar to stage learning.
Hart and Mas-Colell’s algorithm only has asymptotic convergence guarantees, and tends
to converge slowly in practice if tuned for tight convergence. So to get it to converge in a
reasonable amount of time we tuned the parameters to accept somewhat weaker convergence
(although for the particular game shown here the diﬀerence in convergence is not dramatic).

583

Kash, Friedman, & Halpern

1.8
2 Agents
10 Agents
100 Agents

1.6

Distance from Equilibrium

1.4
1.2
1
0.8
0.6
0.4
0.2
0

0

0.5

1

1.5
Time

2

2.5
4

x 10

Figure 5: Stage learners in a congestion game.
Convergence of stage learning in the random-matching game takes approximately 10,000
rounds, which is too slow for many applications. If a system design requires this type of
matching, this makes learning problematic. However, the results of Figure 4 suggest that
the learning could be done much faster if the system designer could supply agents with
more information. This suggests that collecting statistical information about the behavior
of agents may be a critical feature for ensuring fast convergence. To model such a scenario,
consider a related game where, rather than being matched against a random opponent, all
agents contribute to the same project and their reward is based on the average contribution
of the other agents. The results of stage learning in this game are shown in Figure 4. With
so much more information available to agents from each observation, we were able to cut
the length of a stage by a factor of 10. The number of stages needed to reach equilibrium
remained essentially the same. Convergence was tighter as well; mistakes were rare and
almost all of the distance from equilibrium is due to exploration.
4.2 A Congestion Game
For a diﬀerent game, we tested the performance of stage learners in a congestion game.
This game models a situation where two agents share a network link. They gain utility
proportional to their transmission rate over the link, but are penalized based on the resulting
congestion they experience. The game is asymmetric because the two diﬀerent types of
agents place diﬀerent values on transmission rate. The game is described in detail by
Greenwald, Friedman, and Shenker (2001), who showed that no-regret learners are able to
ﬁnd the equilibrium of this game. An extension of our theoretical results to games with
multiple types is presenting in Appendix A.
Figure 5 shows that stage learners were able to learn very quickly in this game, using
stages of length 250 even though they were being randomly matched against a player of
the other type. Because the diﬀerent types of agents had diﬀerent equilibrium strategies,
the distance measure we use is to treat the observed distribution of strategies and the
equilibrium distribution as vectors and compute their L1 distance.

584

Multiagent Learning in Large Anonymous Games

1

Fraction Playing Equilibrium Strategy

0.9
0.8
0.7
0.6
0.5
0.4
0.3

Capacity 2
Capacity 4
Capacity 5

0.2
0.1
0

0

5

10
Number of Stages

15

20

Figure 6: Stage learners in a TCP-like game.
4.3 A TCP-like Game
In the previous example, we considered a random-matching game where two agents of
diﬀerent types share a link. We now consider a game where a large number of agents share
several links (this game is a variant of the congestion control game studied in Nisan et al.,
2011).
There are three types of agents using a network. Each agent chooses an integer rate at
which to transmit between 0 and 10. Links in the network have a maximum average rate at
which agents can transmit; if this is exceeded they share the capacity evenly among agents.
An agent’s utility is his overall transmission rate through the network minus a penalty for
traﬃc that was dropped due to congestion. If an agent attempts to transmit at a rate of 𝑥
and has an actual rate of 𝑦 the penalty is 0.5(𝑥 − 𝑦).5
All agents share a link with an average capacity of 5. One third of agents are further
constrained by sharing a link with an average capacity of 2 and another third share a link
with average capacity of 4. This game has the unique equilibrium where agents in the ﬁrst
third choose a rate of 2, agents in the second third choose a rate of 4, and agents in the
ﬁnal third choose a rate of 9 (so that the overall average rate is 5). This results in a game
where best-reply dynamics converge in ﬁve stages from a uniform starting distribution.
Figure 6 shows the results for 90 learners (30 of each type) with 𝜏 = 50000 and 𝜖 = 0.01,
averaged over ten runs. Agents constrained by an average capacity of two quickly learn their
equilibrium strategy, followed by those with an average capacity of four. Agents constrained
by an average capacity of ﬁve learn their equilibrium strategy, but have a “sawtooth” pattern
where a small fraction alternately plays 10 rather than 9. This is because, with exploration,
it is actually optimal for a small number of agents to play 10. Once a noticeable fraction
does so, 9 is uniquely optimal. This demonstrates that, strictly speaking, this game does
not satisfy our continuity requirement. In equilibrium, the demand for bandwidth is exactly
equal to the supply. Thus, small changes in the demand of other agents due to exploration
can have a large eﬀect on the amount that can actually be demanded and thus on the payoﬀs
5. This penalty is not used in the work by Nisan et al. (2011); using it avoids the tie-breaking issues they
consider.

585

Kash, Friedman, & Halpern

1

Fraction Playing Equilibrium Strategy

0.9
0.8
0.7
0.6
0.5
0.4
0.3
Type 1
Type 2
Type 3

0.2
0.1
0

0

5

10
Number of Stages

15

20

Figure 7: Stage learners in random TCP-like games.
of various strategies. However, the structure of the game is such that play still tends to
remain “close” to the equilibrium in terms of the rates agents choose.
In addition to the speciﬁc parameters mentioned above, we also ran 100 simulations
where each of the three capacities was a randomly chosen integer between 0 and 10. Figure 7
shows that, on average, the results were similar. All three types of agents share a common
constraint; type 1 and type 2 each have an additional constraint. Unsurprisingly, since these
two types are symmetric their results are almost identical. All three types demonstrate the
sawtooth behavior, with type 3 doing so in more runs due to examples like Figure 6 where
having fewer constraints gives agents more ﬂexibility. This primarily comes from runs where
type 1 and type 2 have constraints that are larger than the overall constraint (i.e. only the
overall constraint matters). Thus all three types have the ability to beneﬁt from resources
not demanded when other agents explore.
4.4 A Scrip System Game
Our motivation for this work is to help the designers of distributed systems understand
when learning is practical. In order to demonstrate how stage learning could be applied in
such a setting, we tested a variant of stage learners in the model of a scrip system used by
Kash et al. (2007). In the model, agents pay other agents to provide them service and in
turn provide service themselves to earn money to pay for future service. Agents may place
diﬀerent values on receiving service (𝛾), incur diﬀerent costs to provide service (𝛼), discount
future utility at diﬀerent rates (𝛿), and have diﬀerent availabilities to provide service (𝛽).
We used a single type of agent with parameters 𝛾 = 1.0, 𝛼 = 0.05, 𝛿 = 0.9, 𝛽 = 1, average
amount of money per agent 𝑚 = 1, and stages of 200 rounds per agent (only one agent
makes a request each round).
This model is not a large anonymous game because whether an agent should provide
service depends on how much money he currently has. Thus, stage learning as speciﬁed does
not work, because it does not take into account the current state of the (stochastic) game.
Despite this, we can still implement a variant of stage learning: ﬁx a strategy during each
stage and then at the end of the stage use an algorithm designed for this game to determine
586

Multiagent Learning in Large Anonymous Games

1.6
10 Agents
100 Agents

1.4

Distance from Equilibrium

1.2
1
0.8
0.6
0.4
0.2
0
0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

Time

2
4

x 10

Figure 8: Stage learners in a scrip system.
1.6
10 Agents
100 Agents

1.4

Distance from Equilibrium

1.2
1
0.8
0.6
0.4
0.2
0
0.2

0.4

0.6

0.8

1

1.2
Time

1.4

1.6

1.8

2
4

x 10

Figure 9: A scrip system with churn.
a new strategy that is a best reply to what the agent observed. Our algorithm works by
estimating the agent’s probabilities of making a request and being chosen as a volunteer
in each round, and then uses these probabilities to compute an optimal policy. Figure 8
shows that this is quite eﬀective. The distance measure used is based on directly measuring
the distance of the agent’s chosen (threshold) strategy from the equilibrium strategy, since
unlike the previous games it is impossible to directly infer the agent’s strategy in each round
solely from his decision whether or not to volunteer. Note that the number of rounds has
been normalized based on the number of agents in Figure 8 and later ﬁgures; stages actually
lasted ten times as long with 100 agents.
Real systems do not have a static population of learning agents. To demonstrate the
robustness of stage learning to churn, we replaced ten percent of the agents with new agents
with randomly chosen initial strategies at the end of each period. As Figure 9 shows, this
has essentially no eﬀect on convergence.

587

Kash, Friedman, & Halpern

1.6
10 Agents
100 Agents

1.4

Distance from Equilibrium

1.2
1
0.8
0.6
0.4
0.2
0
0.2

0.4

0.6

0.8

1

1.2
Time

1.4

1.6

1.8

2
4

x 10

Figure 10: A scrip system with diﬀerent stage lengths.
Finally, in a real system it is often unreasonable to expect all agents to be able to update
their strategies at the same time. Figure 10 shows that having half the agents use stages of
222 rounds per agent rather than 200 did not have a signiﬁcant eﬀect on convergence.6
4.5 Learning Counterexamples
At ﬁrst glance, Theorem 3.5 may seem trivial. In a game where best-reply dynamics are
guaranteed to converge, it seems obvious that agents who attempt to ﬁnd best replies
should successfully ﬁnd them and reach equilibrium. However, as we show in this section,
this fact alone is not suﬃcient. In particular, all three of the key features of the games we
study—that they are large, anonymous, and continuous—are required for the theorem to
hold.
First, if the game has only a small number of agents, a mistake made by a single
agent could be quite important, to the point where learning essentially has to start over.
So, while our results can be converted into results about the probability that none of a
ﬁnite number of agents will make a mistake in a given stage, the expected time to reach
equilibrium following this algorithm can be signiﬁcantly longer than the best-reply dynamics
would suggest. The following is an example of a game where the number of best replies
needed to reach equilibrium is approximately the number of strategies, but our experimental
results show that the number of stages needed by stage learners to ﬁnd the equilibrium is
signiﬁcantly longer. (We conjecture that in fact the learning time is exponentially longer.)
In contrast, Theorem 3.5 guarantees that, for games satisfying our requirements, the number
of stages needed is equal to the number of best replies.
Consider a game with three agents, where 𝐴, the set of actions, is {0, 1, . . . , 𝑘}. The
utility functions of the agents are symmetric; the ﬁrst agent’s utility function is given by
the following table:
6. In general we expect that small variations in stage lengths will not aﬀect convergence; however large
enough diﬀerences can result in non-Nash convergence. See the work by Greenwald et al. (2001) for some
simulations and analysis.

588

Multiagent Learning in Large Anonymous Games

actions
(0, 𝑦, 𝑧)
(𝑥, 𝑦, 𝑧)
(0, 1, 0)
(0, 0, 1)
(1, 1, 0)
(1, 0, 1)
(𝑥, 𝑦, 𝑦)
(𝑥, 𝑦, 𝑦)
(𝑘, 𝑘, 𝑘)
(𝑥, 𝑘, 𝑘)

payoﬀ
1
0
0
0
1
1
1
0
1
0

conditions
if 𝑦 ∕= 𝑧 and either 𝑦 > 1 or 𝑧 > 1
if 𝑦 ∕= 𝑧 and 𝑥 > 0 and either 𝑦 > 1 or 𝑧 > 1

if 𝑥 = 𝑦 + 1
if 𝑥 ∕= 𝑦 + 1 and 𝑦 < 𝑘
if 𝑥 ∕= 𝑘

Agents learning best replies can be viewed as “climbing a ladder.” The best reply to
(𝑥, 𝑥, 𝑥) is (𝑥 + 1, 𝑥 + 1, 𝑥 + 1) until agents reach (𝑘, 𝑘, 𝑘), which is a Nash equilibrium.
However, when a mistake is made, agents essentially start over. To see how this works,
suppose that agents are at (3, 3, 3) and for the next stage one makes a mistake and they
select (5, 4, 4). This leads to the best reply sequence (5, 0, 0), (1, 0, 0), (1, 1, 1), at which
point agents can begin climbing again. The somewhat complicated structure of payoﬀs
near 0 ensures that agents begin climbing again from arbitrary patterns of mistakes. In a
typical run, 𝑘 + 2 stages of best replies are needed to reach equilibrium: one stage with the
initial randomly-chosen strategies, one stage where all three agents switch to strategy 0,
and 𝑘 stages of climbing. The exact number of stages can vary if two or more agents choose
the same initial strategy, but can never be greater than 𝑘 + 3.
The following table gives the number of rounds (averaged over ten runs) for stage learners
in this game to ﬁrst reach equilibrium. As the number of strategies varies, the length of a
stage is 𝜏 = 100(𝑘 + 1), with exploration probability 𝜖 = 0.05.
𝑘 rounds to reach 𝑘
4
7.0
9
19.3
14
25.8
19
39.5
24
37.3
29
102.7
34
169.4
39
246.6
With 𝑘 = 4, stage learners typically require 𝑘 + 2 stages, with an occasional error raising
the average slightly. With 𝑘 between 9 and 24, a majority of runs feature at least one agent
making a mistake, so the number of stages required is closer to 2𝑘. With 𝑘 = 29 and up,
there are many opportunities for agents to make a mistake, so the number of stages required
on average is in the range of 3𝑘 to 6𝑘. Thus learning is slower than best-reply dynamics,
and the disparity grows as the number of strategies increases.
A small modiﬁcation of this example shows the problems that arise in games that are
not anonymous. In a non-anonymous game with a large number of agents, payoﬀs can
depend entirely on the actions of a small number of agents. For example, we can split the
set 𝑁 of agents into three disjoint sets, 𝑁0 , 𝑁1 , and 𝑁2 , and choose agents 0 ∈ 𝑁0 , 1 ∈ 𝑁1 ,
589

Kash, Friedman, & Halpern

1
0.9
0.8

Fraction Playing 0

0.7
0.6
0.5
10 agents
100 agents
1000 agents

0.4
0.3
0.2
0.1
0

0

5

10
Number of Stages

15

20

Figure 11: Stage learners in a discontinuous game.
and 2 ∈ 𝑁2 . Again, each agent chooses an action in {0, . . . , 𝑘}. The payoﬀs of agents 0, 1,
and 2 are determined as above; everyone in 𝑁0 gets the same payoﬀ as 0, everyone in 𝑁1
gets the same payoﬀ as 1, and everyone in 𝑁2 gets the same payoﬀ as 2. Again, convergence
to equilibrium will be signiﬁcantly slower than with best-reply dynamics.
Finally, consider the following game, which is large and anonymous, but does not satisfy
the continuity requirement. The set of actions is 𝐴 = {0, 1}, and each agent always receives
a payoﬀ in 𝑃 = {0, 1, 10}. If an agent chooses action 0, his payoﬀ is always 1 (Pr0,𝜌 (1) = 1).
If he chooses action 1, his payoﬀ is 10 if every other agent chooses action 1, 10 if every
other agent chooses action 0, and 0 otherwise (Pr1,(1,0) (10) = 1, Pr1,(0,1) (10) = 1, and
Pr1,𝜌 (0) = 1) for 𝜌 ∕∈ {(1, 0), (0, 1)}).
In this game, suppose approximate best-reply dynamics start at (0.5, 0.5) (each action
is chosen by half of the agents). As they have not coordinated, the unique approximate best
reply for all agents is action 0, so after one best reply, the action distribution will be (1, 0).
Since agents have now coordinated, another round of approximate best replies leads to the
equilibrium (0, 1). If the agents are stage learners, after the ﬁrst stage they will learn an
approximate best reply to (0.5, 0.5) (exploration does not change the action proﬁle in this
case), so most will adopt the mixed action 0𝜖 : playing 0 with probability 1 − 𝜖 and 1 with
probability 𝜖. Thus, even if no agents make a mistake, the action distribution for the next
stage will have at least an 𝜖 fraction playing action 1. Thus the unique approximate best
reply will be action 0; stage learners will be “stuck” at 0, and never reach the equilibrium
of 1.
Figure 11 shows the fraction of times strategy 0 was played during each stage (averaged
over ten runs) for 10, 100, and 1000 agents (𝜏 = 100 and 𝜖 = 0.05). With ten agents,
some initial mistakes are made, but after stage 10 strategy 0 was played about 2.5% of
the time in all runs, which corresponds to the fraction of time we expect to see it simply
from exploration. With 100 agents we see another “sawtooth” pattern where most agents
are stuck playing 0, but in alternating rounds a small fraction plays 1. This happens
because, in rounds where all are playing 0, a small fraction are lucky and explore 1 when
no other agents explore. As a result, they adopt strategy 1 for the next stage. However,
most do not, so in the following stage agents return to all playing 0. Such oscillating
590

Multiagent Learning in Large Anonymous Games

behavior has been observed in other learning contexts, for example among competing myopic
pricebots (Kephart, Hanson, & Greenwald, 2000). With 1000 agents, such lucky agents are
quite rare, so essentially all agents are constantly stuck playing 0.

5. Learning with Byzantine Agents
In practice, learning algorithms need to be robust to the presence of other agents not
following the algorithm. We have seen that stage learning in large anonymous games is
robust to agents who do not learn and instead follow some ﬁxed strategy in each stage.
In the analysis, these agents can simply be treated as agents who made a mistake in the
previous stage. However, an agent need not follow some ﬁxed strategy; an agent attempting
to interfere with the learning of other for malicious reasons or personal gain will likely adapt
his strategy over time. However, as we show in this section, stage learning can also handle
such manipulation in large anonymous games.
Gradwohl and Reingold (2008) examined several classes of games and introduced the
notion of a “stable” equilibrium as one in which a change of strategy by a small fraction
of agents only has a small eﬀect on the payoﬀ of other agents. Their deﬁnition is for
games with a ﬁnite number of agents, but it can easily be adapted to our notion of a large
anonymous game. We take this notion a step further and characterize the game, rather
than an equilibrium, as stable if every strategy is stable.
Deﬁnition 5.1. A large anonymous game Γ is (𝜂, 𝛽)-stable if for all 𝜌, 𝜌′ ∈ Δ(𝐴) such that
∣∣𝜌 − 𝜌′ ∣∣1 ≤ 𝛽 and all 𝑎 ∈ 𝐴, ∣𝑢(𝑎, 𝜌) − 𝑢(𝑎, 𝜌′ )∣ ≤ 𝜂.
One class of games they consider is 𝜆-continuous games. 𝜆-continuity is essentially a
version of Lipschitz continuity for ﬁnite games, so it easy to show that large anonymous
games are stable, where the amount of manipulation that can be tolerated depends on the
Lipschitz constants for agents’ utility functions.
Lemma 5.2. For all large anonymous games Γ, there exists a constant 𝐾Γ such that for
all 𝜂, Γ is (𝜂, 𝜂/𝐾Γ )-stable
Proof. For all 𝑎, 𝑢(𝑎, ⋅) is Lipschitz continuous with a constant 𝐾𝑎 such that ∣𝑢(𝑎, 𝜌) −
𝑢(𝑎, 𝜌′ )∣/∣∣𝜌 − 𝜌′ ∣∣1 ≤ 𝐾𝑎 . Take 𝐾Γ = max𝑎 𝐾𝑎 . Then for all 𝜌 and 𝜌′ such that ∣∣𝜌 − 𝜌′ ∣∣1 ≤
𝜂/𝐾Γ ,
∣𝑢(𝑎, 𝜌) − 𝑢(𝑎, 𝜌′ )∣ ≤ 𝐾𝑎 ∣∣𝜌 − 𝜌′ ∣∣1
≤ 𝐾Γ ∣∣𝜌 − 𝜌′ ∣∣1
(
)
𝜂
≤ (𝐾Γ )
𝐾Γ
≤ 𝜂.

Gradwohl and Reingold (2008) show that stable equilibria have several nice properties.
If a small fraction of agents deviate, payoﬀs for the other agent will not decrease very much
relative to equilibrium. Additionally, following those strategies will still be an approximate
591

Kash, Friedman, & Halpern

equilibrium despite the deviation. Finally, this means that the strategies still constitute
an approximate equilibrium even if asynchronous play causes the strategies of a fraction of
agents to be revealed to others.
We show that, if the game is stable, then learning is also robust to the actions of a small
fraction of Byzantine agents. The following lemma adapts Lemma 3.1 to show that, in each
stage, agents can learn approximate best replies despite the actions of Byzantine agents.
Thus agents can successfully reach equilibrium, as shown by Theorem 3.5.
To state the lemma, we need to deﬁne the actions of a Byzantine agent. If there were
∗ and
no Byzantine agents, then in stage 𝑛 there would be some stationary strategy 𝑔𝑛𝜏
∗
corresponding fraction of agents choosing each action 𝜌𝑛𝜏 . A 𝛽 fraction of Byzantine agents
can change their actions arbitrarily each round, but doing so will have no eﬀect on the
actions of the other agents. Thus, the Byzantine agents can cause the observed fraction of
agents choosing each strategy in round 𝑡 to be any 𝜌𝑡 such that ∣∣𝜌∗𝑛𝜏 − 𝜌𝑡 ∣∣1 < 2𝛽. We refer
to a sequence 𝜌𝑛𝜏 , . . . , 𝜌𝑛(𝜏 +1)−1 such that this condition holds for each 𝑡 as a consistent
sequence. When we say that agents learn an 𝜂-best reply during stage 𝑛, we mean that the
∗ , the actions that the players
strategy that they learn is an approximate best reply to 𝑔𝑛𝜏
would have used had there been no Byzantine players, not the actual action proﬁle, which
includes the strategies used by the Byzantine players.
Lemma 5.3. For all large anonymous games Γ, action distributions 𝜌𝑔𝜏 𝑛 , approximations
𝜂 > 0, probabilities of error 𝑒 > 0, and fractions of agents 𝛽 < 𝜂/6𝐾Γ , there exists an 𝜖∗ > 0
such that for 𝜖 < 𝜖∗ , all 𝑛, and all consistent sequences 𝜌𝑛𝜏 , . . . , 𝜌𝑛(𝜏 +1)−1 , if all agents are
𝜖-stage learners, then at least a 1 − 𝑒 fraction of agents will learn an 𝜂-best reply during
stage 𝑛 despite a 𝛽 fraction of Byzantine agents.
Proof. Consider an agent 𝑖 and round 𝑡 in stage 𝑛. If all agents were stage learners then
the action distribution would be 𝜌∗ = 𝜌𝑔𝜏 𝑛 . However, the Byzantine agents have changed it
such that ∣∣𝜌∗ − 𝜌𝑡 ∣∣ ≤ 2𝛽. Fix an action 𝑎. By Lemma 5.2,
∣𝑢(𝑎, 𝜌∗ ) − 𝑢(𝑎, 𝜌𝑡 )∣ ≤ 𝐾Γ 2𝛽 <

2𝜂
𝜂
𝐾Γ =
6𝐾Γ
3

This means that Byzantine agents can adjust an agent’s expected estimate of the value
of an action by at most 𝜂/3. Let 𝑎∗ be a best reply to 𝑔𝜏 𝑛 (the action used by stage learners
during stage 𝑛). In each round 𝑡 of stage 𝑛,
𝑢(𝑎∗ , 𝜌∗ ) − 𝑢(𝑎∗ , 𝜌𝑡 ) <

𝜂
.
3

For any action 𝑎 that is not an 𝜂-best reply,
𝑢(𝑎∗ , 𝜌∗ ) − 𝑢(𝑎, 𝜌𝑡 ) = (𝑢(𝑎∗ , 𝜌∗ ) − 𝑢(𝑎, 𝜌∗ ) + (𝑢(𝑎, 𝜌∗ ) − 𝑢(𝑎, 𝜌𝑡 )) > 𝜂 −

𝜂
2𝜂
=
.
3
3

Thus, regardless of the actions of the 𝛽 fraction of Byzantine agents, agent 𝑖’s expected
estimate of the value of 𝑎∗ exceeds his expected estimate of the value of 𝑎 by at least 𝜂/3.
Using Hoeﬀding bounds as before, for suﬃciently large 𝜖, 𝑖’s estimates will be exponentially
close to these expectations, so with probability at least 1 − 𝑒, he will not select as best any
action that is not an 𝜂-best reply. By the SLLN, this means that at least a 1 − 𝑒 fraction
of agents will learn an 𝜂-best reply.
592

Multiagent Learning in Large Anonymous Games

Thus, as Lemma 5.3 shows, not only can stage learners learn despite some agents learning incorrect values, they can also tolerate a suﬃciently small number of agents behaving
arbitrarily.

6. Discussion
While our results show that a natural learning algorithm can learn eﬃciently in an interesting class of games, there are many further issues that merit exploration.
6.1 Other Learning Algorithms
Our theorem assumes that agents use a simple rule for learning within each stage: they
average the value of payoﬀs received. However, there are certainly other rules for estimating the value of an action; any of these can be used as long as the rule guarantees that
errors can be made arbitrarily rare given suﬃcient time. It is also not necessary to restrict
agents to stage learning. Stage learning guarantees a stationary environment for a period
of time, but such strict behavior may not be needed or practical. Other approaches, such
as exponentially discounting the weight of observations (Greenwald et al., 2001; Marden
et al., 2009) or Win or Learn Fast (Bowling & Veloso, 2001) allow an algorithm to focus
its learning on recent observations and provide a stable environment in which other agents
can learn.
6.2 Other Update Rules
In addition to using diﬀerent algorithms to estimate the values of actions, a learner could
also change the way he uses those values to update his behavior. For example, rather than
basing his new strategy on only the last stage, he could base it on the entire history of
stages and use a rule in the spirit of ﬁctitious play. Since there are games where ﬁctitious
play converges but best-reply dynamics do not, this could extend our results to another
interesting class of games, as long as the errors in each period do not accumulate over time.
Another possibility is to update probabilistically or use a tolerance to determine whether
to update (see, e.g., Foster & Young, 2006; Hart & Mas-Colell, 2001). This could allow
convergence in games where best-reply dynamics oscillate or decrease the fraction of agents
who make mistakes once the system reaches equilibrium.
6.3 Model Assumptions
Our model makes several unrealistic assumptions, most notably that there are countably
many agents who all share the same utility function. Essentially the same results holds
with a large, ﬁnite number of agents, adding a few more “error terms.” In particular, since
there is always a small probability that every agent makes a mistake at the same time, we
can prove only that no more than a 1 − 𝑒 fraction of the agents make errors in most rounds,
and that agents spend most of their time playing equilibrium strategies.
We have also implicitly assumed that the set of agents is ﬁxed. As Figure 9 shows,
we could easily allow for churn. A natural strategy for newly arriving agents is to pick
a random 𝑎𝜖 to use in the next stage. If all agents do this, it follows that convergence is
unaﬀected: we can treat the new agents as part of the 𝑒 fraction that made a mistake in the
593

Kash, Friedman, & Halpern

last stage. Furthermore, this tells us that newly arriving agents “catch up” very quickly.
After a single stage, new agents are guaranteed to have learned a best reply with probability
at least 1 − 𝑒.
Finally, we have assumed that all agents have the same utility function. Our results can
easily be extended to include a ﬁnite number of diﬀerent types of agents, each with their
own utility function, since the SLLN can be applied to each type of agent. This extension
is discussed in Appendix A. We believe that our results hold even if the set of possible
types is inﬁnite. This can happen, for example, if an agent’s utility depends on a valuation
drawn from some interval. However, some care is needed to deﬁne best-reply sequences in
this case.
6.4 State
One common feature of distributed systems not addressed in the theoretical portion of this
work is state. As we saw with the scrip system in Section 4.4, an agent’s current state is
often an important factor in choosing an optimal action.
In principle, we could extend our framework to games with state: in each stage each
agent chooses a policy to usually follow and explores other actions with probability 𝜖. Each
agent could then use some oﬀ-policy algorithm (one where the agent can learn without
controlling the sequence of observations; see Kaelbling, Littman, & Moore, 1996 for examples) to learn an optimal policy to use in the next stage. One major problem with
this approach is that standard algorithms learn too slowly for our purposes. For example, Q-learning (Watkins & Dayan, 1992) typically needs to observe each state-action pair
hundreds of times in practice. The low exploration probability means that the expected
∣𝑆∣∣𝐴∣/𝜖 rounds needed to explore each pair even once is large. Eﬃcient learning requires
more specialized algorithms that can make better use of the structure of a problem. However, the use of specialized algorithms makes providing a general guarantee of convergence
more diﬃcult. Another problem is that, even if an agent explores each action for each of
his possible local states, the payoﬀ he receives will depend on the states of the other agents,
and thus the actions they chose. We need some property of the game to guarantee that this
distribution of states is in some sense “well behaved.” Adlakha and Johari’s (2010) work on
mean ﬁeld equilibria gives one such condition. In this setting, the use of publicly available
statistics might provide a solution to these problems.
6.5 Mixed Equilibria
Another restriction of our results is that our agents only learn pure strategies. One way
to address this is to discretize the mixed strategy space (see, e.g., Foster & Young, 2006).
If one of the resulting strategies is suﬃciently close to an equilibrium strategy and bestreply dynamics converge with the discretized strategies, then we expect agents to converge
to a near-equilibrium distribution of strategies. We have had empirical success using this
approach to learn to play rock-paper-scissors.

594

Multiagent Learning in Large Anonymous Games

7. Conclusion
Learning in distributed systems requires algorithms that are scalable to thousands of agents
and can be implemented with minimal information about the actions of other agents. Most
general-purpose multiagent learning algorithms fail one or both of these requirements. We
have shown here that stage learning can be an eﬃcient solution in large anonymous games
where approximate best-reply dynamics lead to approximate pure strategy Nash equilibria.
Many interesting classes of games have this property, and it is frequently found in designed
games. In contrast to previous work, the time to convergence guaranteed by the theorem
does not increase with the number of agents. If system designers can ﬁnd an appropriate
game satisfying these properties on which to base their systems, they can be conﬁdent that
nodes can eﬃciently learn appropriate behavior.
Our results also highlight two factors that aid convergence. First, having more learners often improves performance. With more learners, the noise introduced into payoﬀs by
exploration and mistakes becomes more consistent. Second, having more information typically improves performance. Publicly available statistics about the observed behavior of
agents can allow an agent to learn eﬀectively while making fewer local observations. Our
simulations demonstrate the eﬀects of these two factors, as well how our results generalize
to situations with other learning algorithms, churn, asynchrony, and Byzantine behavior.
7.1 Acknowledgments
Most of the work was done while IK was at Cornell University. EF, IK, and JH are supported
in part by NSF grant ITR-0325453. JH is also supported in part by NSF grant IIS-0812045
and by AFOSR grants FA9550-08-1-0438 and FA9550-05-1-0055. EF is also supported in
part by NSF grant CDI-0835706.

Appendix A. Multiple Types
In this section, we extend our deﬁnition of a large anonymous game to settings where
agents may have diﬀerent utility functions. To do so, we introduce the notion of a type.
Agents’ utilities may depend on their type and the fraction of each type taking each action.
As our results rely on the strong law of large numbers, we restrict the set of types to
be ﬁnite. Formally, a large anonymous game with types is characterized by a tuple Γ =
(ℕ, 𝑇, 𝜏, 𝐴, 𝑃, Pr). We deﬁne ℕ, 𝐴, 𝐺, and 𝑃 as before. For the remaining terms:
∙ 𝑇 is a ﬁnite set of agent types.
∙ 𝜏 : ℕ → 𝑇 is a function mapping each agent to his type.
∙ As before, Δ(𝐴) is the set of probability distributions over 𝐴, and can be viewed as
the set of mixed actions available to an agent. But now, to describe the fraction of
agents of each type choosing each action, we must use element of Δ(𝐴)𝑇 .
∙ Pr : 𝐴×𝑇 ×Δ(𝐴)𝑇 → Δ(𝑃 ) determines the distribution over payoﬀs that results when
an agent of type 𝑡 performs action 𝑎 and other agents follow action proﬁle 𝜌. The
expected utility of an agent of type 𝑡 who performs mixed action 𝑠 when other agents

595

Kash, Friedman, & Halpern

∑
∑
follow action distribution 𝜌 is 𝑢(𝑠, 𝑡, 𝜌) = 𝑎∈𝐴 𝑝∈𝑃 𝑝𝑠(𝑎) Pr𝑎,𝑡,𝜌 (𝑝). As before, we
further require that Pr (and thus 𝑢) be Lipschitz continuous.
The revised deﬁnitions of an 𝜂-best reply, an 𝜂-Nash equilibrium, an 𝜂-best-reply sequence, convergence of approximate best-reply dynamics, and (𝑒, 𝜖)-close follow naturally
from the revised deﬁnitions of 𝜌 and 𝑢. Lemma 3.1 now applies to each type of agent separately, and shows that all but a small fraction of each type will learn an approximate best
reply in each stage. Lemma 3.3 and Lemma 3.4 hold given the revised deﬁnitions of 𝜌 and
𝑢. Thus Theorem 3.5, which combines these, also still holds.

References
Adlakha, S., & Johari, R. (2010). Mean ﬁeld equilibrium in dynamic games with complementarities. In IEEE Conference on Decision and Control (CDC).
Adlakha, S., Johari, R., Weintraub, G. Y., & Goldsmith, A. (2010). Mean ﬁeld analysis
for large population stochastic games. In IEEE Conference on Decision and Control
(CDC).
Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (2002). The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32 (1), 48–77.
Blonski, M. (2001). Equilibrium characterization in large anonymous games. Tech. rep.,
U. Mannheim.
Blum, A., Even-Dar, E., & Ligett, K. (2006). Routing without regret: on convergence
to Nash equilibria of regret-minimizing algorithms in routing games. In 25th ACM
Symp. on Principles of Distributed Computing (PODC), pp. 45–52.
Blum, A., & Mansour, Y. (2007). Learning, regret minimization, and equilibria. In Nisan,
N., Roughgarden, T., Tardos, É., & Vazirani, V. (Eds.), Algorithmic Game Theory,
pp. 79–102. Cambridge University Press.
Bowling, M. H. (2000). Convergence problems of general-sum multiagent reinforcement
learning. In 17th Int. Conf. on Machine Learning (ICML 2000), pp. 89–94.
Bowling, M. H., & Veloso, M. M. (2001). Rational and convergent learning in stochastic
games. In 17th Int. Joint Conference on Artiﬁcial Intelligence (IJCAI 2001), pp.
1021–1026.
Boylan, R. T. (1992). Laws of large numbers for dynamical systems with randomly matched
indviduals. Journal of Economic Theory, 57, 473–504.
Cesa-Bianchi, N., & Lugosi, G. (2006). Prediction, Learning and Games. Cambridge University Press.
Claus, C., & Boutilier, C. (1998). The dynamics of reinforcement learning in cooperative
multiagent systems. In AAAI-97 Workshop on Multiagent Learning, pp. 746–752.
Daskalakis, C., & Papadimitriou, C. H. (2007). Computing equilibria in anonymous games.
In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2007),
pp. 83–93.

596

Multiagent Learning in Large Anonymous Games

Foster, D. P., & Young, P. (2006). Regret testing: Learning to play Nash equilibrium without
knowing you have an opponent. Theoretical Economics, 1, 341–367.
Friedman, E. J., & Shenker, S. (1998). Learning and implementation on the internet. Tech.
rep., Cornell University.
Fudenberg, D., & Levine, D. (1998). Theory of Learning in Games. MIT Press.
Germano, F., & Lugosi, G. (2007). Global Nash convergence of Foster and Young’s regret
testing. Games and Economic Behavior, 60 (1), 135–154.
Gradwohl, R., & Reingold, O. (2008). Fault tolerance in large games. In Proc. 9th ACM
Conference on Electronic Commerce (EC 2008), pp. 274–283.
Greenwald, A., Friedman, E. J., & Shenker, S. (2001). Learning in networks contexts:
Experimental results from simulations. Games and Economic Behavior, 35 (1-2), 80–
123.
Hart, S., & Mas-Colell, A. (2000). A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68 (5), 1127–1150.
Hart, S., & Mas-Colell, A. (2001). A reinforecement learning procedure leading to correlated
equilibrium. In Debreu, G., Neuefeind, W., & Trockel, W. (Eds.), Economic Essays,
pp. 181–200. Springer.
Hopkins, E. (1999). Learning, matching, and aggregation. Games and Economic Behavior,
26, 79–110.
Hu, J., & Wellman, M. P. (2003). Nash Q-learning for general-sum stochastic games. Journal
of Machine Learning Research, 4, 1039–1069.
Jafari, A., Greenwald, A. R., Gondek, D., & Ercal, G. (2001). On no-regret learning,
ﬁctitious play, and nash equilibrium. In Proc. Eighteenth International Conference on
Machine Learning (ICML), pp. 226–233.
Kaelbling, L. P., Littman, M. L., & Moore, A. P. (1996). Reinforcement learning: A survey.
J. Artif. Intell. Res. (JAIR), 4, 237–285.
Kash, I. A., Friedman, E. J., & Halpern, J. Y. (2007). Optimizing scrip systems: Eﬃciency,
crashes, hoarders and altruists. In Eighth ACM Conference on Electronic Commerce
(EC 2007), pp. 305–315.
Kephart, J. O., Hanson, J. E., & Greenwald, A. R. (2000). Dynamic pricing by software
agents. Computer Networks, 32 (6), 731–752.
Marden, J. R., Arslan, G., & Shamma, J. S. (2007a). Connections between cooperative
control and potential games. In Proc. 2007 European Control Conference (ECC).
Marden, J. R., Arslan, G., & Shamma, J. S. (2007b). Regret based dynamics: convergence in
weakly acyclic games. In 6th Int. Joint Conf. on Autonomous Agents and Multiagent
Systems (AAMAS), pp. 42–49.
Marden, J. R., Young, H. P., Arslan, G., & Shamma, J. S. (2009). Payoﬀ-based dynamics
for multi-player weakly acyclic games. SIAM Journal on Control and Optimization,
48 (1), 373–396.

597

Kash, Friedman, & Halpern

Milgrom, P., & Roberts, J. (1990). Rationalizability, learning, and equilibrium in games
with strategic complement- arities. Econometrica, 58 (6), 1255–1277.
Nisan, N., Schapira, M., Valiant, G., & Zohar, A. (2011). Best-response mechanisms. In
Proc. Second Syposium on Innovations in Computer Science (ICS). To Appear.
Nisan, N., Schapira, M., & Zohar, A. (2008). Asynchronous best-reply dynamics. In
Proc. 4th International Workshop on Internet and Netwrok Economics (WINE), pp.
531–538.
Osborne, M., & Rubenstein, A. (1994). A Course in Game Theory. MIT Press.
Shoham, Y., Powers, R., & Grenager, T. (2003). Multi-agent reinforcement learning: a
critical survey. Tech. rep., Stanford.
Tesauro, G., & Kephart, J. O. (2002). Pricing in agent economies using multi-agent Qlearning. Autonomous Agents and Multi-Agent Systems, 5 (3), 289–304.
Verbeeck, K., Nowé, A., Parent, J., & Tuyls, K. (2007). Exploring selﬁsh reinforcement
learning in repeated games with stochastic rewards. Journal of Autonomous Agents
and Multi-agent Systems, 14, 239–269.
Watkins, C. J., & Dayan, P. (1992). Technical note Q-learning. Machine Learning, 8,
279–292.
Young, H. P. (2009). Learning by trial and error. Games and Economic Behavior, 65 (2),
626–643.

598

Journal of Artificial Intelligence Research 40 (2011) 353-373

Submitted 07/10; published 01/11

Clause-Learning Algorithms with Many Restarts
and Bounded-Width Resolution
Albert Atserias

atserias@lsi.upc.edu

Universitat Politècnica de Catalunya
Barcelona, Spain

Johannes Klaus Fichte

fichte@kr.tuwien.ac.at

Vienna University of Technology
Vienna, Austria

Marc Thurley

marc.thurley@googlemail.com

University of California at Berkeley
Berkeley, USA

Abstract
We offer a new understanding of some aspects of practical SAT-solvers that are based on
DPLL with unit-clause propagation, clause-learning, and restarts. We do so by analyzing
a concrete algorithm which we claim is faithful to what practical solvers do. In particular,
before making any new decision or restart, the solver repeatedly applies the unit-resolution
rule until saturation, and leaves no component to the mercy of non-determinism except
for some internal randomness. We prove the perhaps surprising fact that, although the
solver is not explicitly designed for it, with high probability it ends up behaving as width-k
resolution after no more than O(n2k+2 ) conflicts and restarts, where n is the number of
variables. In other words, width-k resolution can be thought of as O(n2k+2 ) restarts of the
unit-resolution rule with learning.

1. Introduction
The discovery of a method to introduce practically feasible clause learning and non-chronological backtracking to DPLL-based solvers layed the foundation of what is sometimes called
“modern” SAT-solving (Silva & Sakallah, 1996; Bayardo & Schrag, 1997). These methods
set the ground for new effective implementations (Moskewicz, Madigan, Zhao, Zhang, &
Malik, 2001) that spawned tremendous gains in the efficiency of SAT-solvers with many
practical applications. Such great and somewhat unexpected success seemed to contradict
the widely assumed intractability of SAT, and at the same time uncovered the need for a
formal understanding of the capabilities and limitations underlying these methods.
Several different approaches have been suggested in the literature for developing a rigorous understanding. Among these we find the proof-complexity approach, which captures
the power of SAT-solvers in terms of propositional proof systems (Beame, Kautz, & Sabharwal, 2003, 2004; Hertel, Bacchus, Pitassi, & Gelder, 2008; Pipatsrisawat & Darwiche,
2009), and the rewriting approach, which provides a useful handle to reason about the
properties of the underlying algorithms and their correctness (Nieuwenhuis, Oliveras, &
Tinelli, 2006). In both approaches, SAT-solvers are viewed as algorithms that search for
proofs in some underlying proof system for propositional logic. With this view in mind, it
was illuminating to understand that the proof system underlying modern solvers is always
c
2011
AI Access Foundation. All rights reserved.

Atserias, Fichte, & Thurley

a subsystem of resolution (Beame et al., 2003). In particular, this means that their performance can never beat resolution lower bounds, and at the same time it provides many
explicit examples where SAT-solvers require exponential time. Complementing this is the
result that an idealized SAT-solver that relies on non-determinism to apply the techniques
in the best possible way will be able to perform as good as general resolution (weak forms
of this statement were first established in Beame et al., 2003, 2004; Hertel et al., 2008, and
in the current form in Pipatsrisawat & Darwiche, 2009). As Beame et al. (2004) put it, the
negative proof complexity results uncover examples of inherent intractability even under
perfect choice strategies, while the positive proof complexity results give hope of finding a
good choice strategy.
In this work we add a new perspective to this kind of rigorous result. We try to avoid
non-deterministic choices on all components of our abstract solver and still get positive proof
complexity results. Our main finding is that a concrete family of SAT-solvers that do not
rely on non-determinism besides mild randomness is at least as powerful as bounded-width
resolution. The precise proof-complexity result is that under the unit-propagation rule and
a standard learning scheme considered by state-of-the-art solvers, the totally random decision strategy needs no more than O(k 2 ln(kn)n2k+1 ) conflicts and deterministic restarts to
detect the unsatisfiability of any CNF formula on n variables having a width-k resolution
refutation, with probability at least 1/2. Remarkably, the analysis will provide an exact expression for this upper bound that holds for all values of n and k and in particular the bound
we get is not asymptotic. Another remarkable feature is that our analysis is insensitive to
whether the algorithm implements non-chronological backtracking or heuristic-based decisions provided it restarts often enough, and provided it performs totally random decisions
often enough. Further details about this are given in Section 2.
By itself this result has some nice theoretical consequences, which we shall sketch briefly.
First, although not explicitly designed for that purpose, SAT-solvers are able to solve instances of 2-SAT in polynomial time since every unsatisfiable 2-CNF formula has a resolution
refutation of width two. More strongly, our result can be interpreted as showing that widthk resolution can be simulated by O(k 2 ln(kn)n2k+1 ) rounds of unit-clause propagation. To
our knowledge, such a tight connection between width-k resolution and repeated application
of “width-one” methods was unknown before. Another consequence is that SAT-solvers are
able to solve formulas of bounded branch-width (and hence bounded treewidth) in polynomial time. We elaborate on this later in the paper. Finally, from the partial automatizability
results of Ben-Sasson and Wigderson (1999), it follows that SAT-solvers are able to solve
formulas having polynomial-size tree-like resolution proofs in quasipolynomial time, and
formulas having polynomial-size general resolution proofs in subexponential time.
Concerning our techniques, it is perhaps surprising that the proof of our main result
does not proceed by showing that the width-k refutation is learned by the algorithm. For
all we know the produced proof has much larger width. The only thing we show is that
every width-k clause in the refutation is absorbed by the algorithm, which means that it
behaves as if it had been learned, even though it might not. In particular, if a literal and
its complement are both absorbed, the algorithm correctly declares that the formula is
unsatisfiable. This sort of analysis is the main technical contribution of this paper.
354

Clause-Learning Algorithms

1.1 Related Work
The first attempt to compare the power of SAT-solvers with the power of resolution as
a proof system was made by Beame et al. (2003, 2004). The main positive result from
their work is that clause learning with a specific learning scheme and without restarts can
provide exponentially shorter proofs than proper refinements of resolution such as tree, or
regular, or positive resolution. Furthermore, they show that the modification of a standard
solver to allow multiple assignments on the same variable would be able to simulate general
resolution efficiently, assuming an ideal decision strategy. Following work showed that the
requirement for multiple assignments on the same variable is a technical issue that can be
avoided if the given CNF formula is pre-processed appropriately (Hertel et al., 2008). In
our work we avoid these two maneuvers by introducing the concept of clause-absorption to
help us analyze the standard algorithms directly.
Interestingly, for clauses that are logical consequences of the input formulas, our concept
of clause-absorption turns out to be dual to the concept of 1-empowerment introduced
independently by Pipatsrisawat and Darwiche (2009)1 . They used 1-empowerment to show
that SAT-solvers without any conceptual modification in their operation are able to simulate
general resolution efficiently, again assuming an ideal decision strategy. For comparison,
our goal settles for a weaker simulation result, bounded-width resolution instead of general
resolution, but does not rely on the non-determinism of ideal decision. We show that the
totally random decision strategy is good enough for this purpose, provided we restart often
enough. To complete this point, it is worth noting that the non-automatizability results of
Alekhnovich and Razborov (2008) indicate that we cannot expect an efficient simulation of
general resolution and completely avoid non-determinism at the same time.
The fact that both concepts were discovered independently adds confidence to our belief
that they will play a role in subsequent studies of the power of SAT-solvers. Indeed, our
techniques were recently extended to show that SAT-solvers with a totally random decision
strategy are able to efficiently simulate local consistency techniques for general constraint
satisfaction problems (Jeavons & Petke, 2010).
1.2 Organization
In Section 2 we introduce basic notation and we define the algorithm we analyze. We also
discuss the dependence of our results on our choice of the learning scheme, the restart policy
and the decision strategy used by the algorithm. Section 3 starts with some elementary
facts about the runs of the algorithm, continues with the key definitions of absorption and
beneficial rounds, and then with the analysis of the running time of the algorithm. Section 4
contains a discussion of the consequences, including the implications for formulas of bounded
treewidth.

2. Clause Learning Algorithms
In this section we will define the algorithm and discuss our choice of its components. We
start with some preliminary definitions.
1. Note that, originally, a weaker version of 1-empowerment was introduced by Pipatsrisawat and Darwiche
(2008).

355

Atserias, Fichte, & Thurley

2.1 Preliminaries
Let V = {v1 , . . . , vn } be a fixed set of propositional variables. A literal is a propositional
variable x or its negation x̄. We use the notation x0 for x̄ and x1 for x. Note that xa is
defined in such a way that the assignment x = a satisfies it. For a ∈ {0, 1}, we also use
ā for 1 − a, and for a literal ` = xa we use `¯ for x1−a . A clause is a set of literals, and
a formula in conjunctive normal form (CNF-formula) is a set of clauses. The width of a
clause is the number of literals in it. In the following, all formulas are over the same set of
variables V and every clause contains only literals on variables from V .
For two clauses A = {x, `1 , . . . , `r } and B = {x̄, `01 , . . . , `0s } we define the resolvent of A
and B on x by Res(A, B, x) = {`1 , . . . , `r , `01 , . . . , `0s }. If the variable we resolve on, x, is
implicit we simply write Res(A, B). A clause may contain a literal and its negation. Note
that the resolvent Res(A, B, x) of A and B on x is still well-defined in this case. A resolution
refutation of a CNF formula F is a sequence of clauses C1 , . . . , Cm such that Cm = ∅ and
each clause Ci in the sequence either belongs to F or is a resolvent of previous clauses in
the sequence. The length of a refutation is the number m of clauses in the sequence. For
a clause C, a variable x, and a truth value a ∈ {0, 1}, the restriction of C on x = a is the
constant 1 if the literal xa belongs to C, and C \ {x1−a } otherwise. We write C|x=a for the
restriction of C on x = a.
A partial assignment is a sequence of assignments (x1 = a1 , . . . , xr = ar ) with all
variables distinct. Let α be a partial assignment. We say that α satisfies a literal xa if it
contains x = a. We say that α falsifies it if it contains x = 1 − a. If C is a clause, we let
C|α be the result of applying the restrictions x1 = a1 , . . . , xr = ar to C. Clearly the order
does not matter. We say that α satisfies C if it satisfies at least one of its literals; i.e., if
C|α = 1. We say that α falsifies C if it falsifies all its literals; i.e., if C|α = ∅. If D is a set
of clauses, we let D|α denote the result of applying the restriction α to each clause in D,
and removing the resulting 1’s. We call D|α the residual set of clauses.
2.2 Definition of the Algorithm
A state is a sequence of assignments (x1 = a1 , . . . , xr = ar ) in which all variables are
d
distinct and some assignments are marked as decisions. We use the notation xi = ai
to denote that the assignment xi = ai is a decision assignment. In this case xi is called a
decision variable. The rest of assignments are called implied assignments. We use S and T to
denote states. The empty state is the one without any assignments. Define the decision level
of an assignment xi = ai as the number of decision assignments in (x1 = a1 , . . . , xi = ai ).
When convenient, we identify a state with the underlying partial assignment where all
decision marks are ignored.
2.2.1 Operation
The algorithm maintains a current state S and a current set of clauses D. There are four
modes of operation DEFAULT, CONFLICT, UNIT, and DECISION. The algorithm starts in
DEFAULT mode with the empty state as the current state and the given CNF formula as
the current set of clauses:
356

Clause-Learning Algorithms

• DEFAULT. If S sets all variables in D and satisfies all clauses in D, stop and output
SAT together with the current state S. Otherwise, if D|S contains the empty clause,
move to CONFLICT mode. Otherwise, if D|S contains a unit clause, move to UNIT
mode. Finally, if control reaches this point, move to DECISION mode.
• CONFLICT. Apply the learning scheme to add a new clause C to D. If C is the empty
clause, stop and output UNSAT. Otherwise, apply the restart policy to decide whether
to continue further or to restart in DEFAULT mode with the current D and S initialized
to the empty state. In case we continue further, repeatedly remove assignments from
the tail of S as long as C|S = ∅, and then go to UNIT mode.
• UNIT. For any unit clause {xa } in D|S , add x = a to S and go back to DEFAULT
mode.
d

• DECISION. Apply the decision strategy to determine a decision x = a to be added to
S and go back to DEFAULT mode.

To guarantee correctness and termination, the learning scheme will always add a clause C
that is a logical consequence of D, for which C|S = ∅ holds at the time it is added, and that
contains at most one variable of maximum decision level. It is not hard to see that these
properties prevent such a clause from being learned twice, and since the number of clauses
on the variables of D is finite, this implies termination. Clauses with these characteristics
always exist as they include the asserting clauses (Zhang, Madigan, Moskewicz, & Malik,
2001) that will be discussed in Section 2.3.3.
The well-known DPLL-procedure is a precursor of this algorithm where, in CONFLICT
mode, the learning scheme never adds any new clause, the restart policy does not dictate
any restart at all, and assignments are removed from the tail of S up to the latest decision
d
d
assignment, say x = a, which is replaced by x = 1 − a. We say that the DPLL-procedure
backtracks on the latest decision. In contrast, modern SAT-solvers implement learning
schemes and backtrack on a literal, as determined by the learned clause, which is not necessarily the latest decision. This is called non-chronological backtracking. Besides learning
schemes and non-chronological backtracking, modern SAT-solvers also implement restart
policies and appropriate decision strategies. We discuss our choice of these components of
the algorithm in Section 2.3.
2.2.2 Runs of the Algorithm
Consider a run of the algorithm started in DEFAULT mode with the empty state and initial
set of clauses D, until either a clause is falsified or all variables are set. Such a run is called
a complete round started with D and we represent it by the sequence of states S0 , . . . , Sm
that the algorithm goes through, where S0 is the empty state and Sm is the state where
either all variables are set, or the falsified clause is found. More generally, a round is an
initial segment S0 , . . . , Sr of a complete round up to a state where either D|Sr contains the
empty clause or D|Sr does not contain any unit clause. If D|Sr contains the empty clause
we say that the round is conclusive. If a round is not conclusive we call it inconclusive. The
357

Atserias, Fichte, & Thurley

term inconclusive means to reflect the fact that no clause can be learned from such a round.
In particular, a (complete) round that ends in a satisfying assignment is inconclusive2 .
For a round S0 , . . . , Sr , note that for i ∈ {1, . . . , r}, the state Si extends Si−1 by exactly
d
one assignment of the form xi = ai or xi = ai depending on whether UNIT or DECISION
is executed at that iteration; no other mode assigns variables. When this does not lead to
confusion, we identify a round with its last state interpreted as a partial assignment. In
particular, we say that the round satisfies a clause C if C|Sr = 1, and that it falsifies it if
C|Sr = ∅.
2.3 Restart Policy, Learning Scheme, and Decision Strategy
In the following we will discuss our choice of the learning scheme, the restart policy and
the decision strategy used by the algorithm. Our discussion will particularly focus on the
dependence of our results on this choice.
2.3.1 Restart Policy
The restart policy determines whether to restart the search after a clause is learned. The
only important characteristic that we need from the restart policy is that it should dictate
restarts often enough. In particular, our analysis will work equally well for the most aggressive of the restart policies, the one that dictates a restart after every conflict, as for
a less aggressive strategy that allows any bounded number of conflicts between restarts.
The fact that our analysis is insensitive to this will follow from a monotonicity property of
the performance of the algorithm that we will prove in Lemma 5. More precisely, it will
follow from the monotonicity lemma that if we decide to use a policy that allows c > 1
conflicts before a restart, then the upper bound on the number of required restarts can
only decrease (or stay the same). Only the upper bound on the number of conflicts would
appear multiplied by a factor of c, even though the truth might be that even those decrease
as well. For simplicity of exposition, for the rest of the paper we assume that the restart
policy dictates a restart after every conflict.
2.3.2 Decision Strategy
The decision strategy determines which variable is assigned next, and to what value. Again,
the only important characteristic that we need from the decision strategy is that it should
allow a round of totally random decisions often enough. Here, a totally random decision
is defined as follows: if the current state of the algorithm is S, we choose a variable x
uniformly at random among the variables from V that do not appear in S, and a value a in
{0, 1} also uniformly at random and independently of the choice of x. Thus, our analysis
actually applies to any decision strategy that allows any bounded number of rounds with
heuristic-based decisions between totally random ones. More precisely, if we allow say c > 1
rounds of non-random decisions between random ones, then the number of required restarts
and conflicts would appear multiplied by a factor of c. Again this will follow from the
2. Let us note that the definitions of round, conclusive round and inconclusive round differ slightly from
those given in the conference version of this paper (Atserias, Fichte, & Thurley, 2009). The current
definitions make the concepts more robust.

358

Clause-Learning Algorithms

monotonicity lemma referred to above. That said, for simplicity of exposition we assume in
the following that every decision is totally random.
2.3.3 Learning Scheme
The learning scheme determines which clause will be added to the set of clauses when a
conflict occurs. Let S0 , . . . , Sr be a conclusive round started with the set of clauses D that
d
ends up falsifying some clause of D. Let xi = ai or xi = ai be the i-th assignment of the
round. We annotate each Si by a clause Ai by reverse induction on i ∈ {1, . . . , r}:
1. Let Ar+1 be any clause in D that is falsified by Sr .
d

2. For i ≤ r for which xi = ai is a decision, let Ai = Ai+1 .
3. For i ≤ r for which xi = ai is implied, let Bi be any clause in D such that Bi |Si−1 is
the unit clause {xai i }, and let Ai = Res(Ai+1 , Bi , xi ) if these clauses are resolvable on
xi , and let Ai = Ai+1 otherwise.
It is quite clear from the construction that each Ai has a resolution proof from the clauses
in D. In fact, the resolution proof is linear and even trivial in the sense of Beame et al.
(2004). We call each clause Ai a conflict clause. If d denotes the maximum decision level
of the assignments in Sr , a conflict clause is called an asserting clause if it contains exactly
one variable of decision level d. Asserting clauses, originally defined by Zhang et al. (2001),
capture the properties of conflict clauses learned by virtually any modern SAT-solver. For
brevity, we describe only two concrete learning schemes in detail. For other schemes see the
work of Zhang et al. (2001).
The Decision learning scheme adds clause A1 to the current set of clauses after each
conflict. It is not hard to check that A1 is an asserting clause. Furthermore, every literal
in A1 is the negation of some decision literal in Sr ; this will be important later on. The
1UIP learning scheme, which stands for 1st Unique Implication Point, is the one that adds
a clause Ai such that i ≤ r is maximal subject to the condition that Ai is an asserting
clause.
In the following we will assume, tacitly, that the algorithm employs some asserting
learning scheme, that is, one whose learned clauses are always asserting, except for the
empty clause.
2.3.4 Clause Bookkeeping
It should be mentioned that our analysis relies crucially on the assumption that the learned
clauses are never removed from the current set of clauses. However, practical SAT-solvers
periodically delete some of the learned clauses to save memory and to avoid the overhead
they introduce. Thus an interesting question is whether our results can be made to work
without the assumption. In this respect, the strong proof-complexity results of Nordström
(2009) showing that not every small-width resolution refutation can be made to work in
small clause-space seems to indicate that an assumption similar to ours is indeed needed.
Another remark worth making at this point concerns the width of the learned clauses.
Since our goal is to show that the algorithm can simulate small-width resolution, it seems
natural to ask whether we can restrict the learning scheme to learn clauses of small width
359

Atserias, Fichte, & Thurley

only. As mentioned in the introduction, our analysis does not seem to allow it. Moreover,
recent results by Ben-Sasson and Johannsen (2010) show that, in general, learning short
clauses only is a provably weaker scheme than learning arbitrarily long clauses. Thus,
while the examples of Ben-Sasson and Johannsen (2010) do not have small-width resolution
refutations and therefore do not show that keeping long clauses is actually required in this
case, it is conceivable that it might.

3. Analysis of the Algorithm
In this section we will analyze the running time of the algorithm. Before we can do this,
however, we will have to introduce our key technical concepts of absorption and beneficial
rounds, and study some of their most important properties.
3.1 Runs of the Algorithm
Let R and R0 be rounds, and let C be a clause. We say that R0 subsumes R if, up to decision
marks, every assignment in R appears also in R0 . We say that R and R0 agree on C if the
restrictions of R and R0 to variables in C are equal: every variable in C is either unassigned
in both, or assigned to the same value in both. We say that R branches in C if all decision
variables of R are variables in C. Note that the properties agree on C and branches in C
depend only on the set of variables of C. We define them for clauses to simplify notation
later on.
We prove two rather technical lemmas. The goal is to show that inconclusive rounds
are robust with respect to the order in which assignments are made. For example, the first
lemma shows that any inconclusive round subsumes any other round that agrees with it on
its decisions. In fact we will need a slightly stronger claim that involves rounds from two
different sets of clauses.
Lemma 1. Let D and D0 be sets of clauses with D ⊆ D0 , let C be a clause, and let R0 be an
inconclusive round started with D0 . Then, for every round R started with D that branches
in C and agrees with R0 on C, it holds that R0 subsumes R.
Proof. Let R = (S0 , . . . , Sr ). By induction on i, we prove that for every i ∈ {0, . . . , r}, every
assignment in Si is also made in R0 . For i = 0 there is nothing to prove since S0 = ∅. Let
d
i > 0 and assume that every assignment in Si−1 is also made in R0 . Let x = a or x = a be
the last assignment in Si . Since R and R0 agree on C and R branches in C, every decision
d
assignment made in R is also made in R0 . This takes care of the case x = a. Suppose then
that the last assignment x = a in Si is implied. This means that there exists a clause A
in D such that A|Si−1 = {xa }. Since D ⊆ D0 and every assignment made in Si−1 is also
made in R0 , necessarily x = a appears in R0 because R0 is inconclusive and cannot leave
unit clauses unset.
The next lemma shows that the universal quantifier in the conclusion of the previous
lemma is not void. In addition, the round can be chosen inconclusive.
Lemma 2. Let D and D0 be sets of clauses with D ⊆ D0 , let C be a clause, and let R0 be
an inconclusive round started with D0 . Then, there exists an inconclusive round R started
with D that branches in C and agrees with R0 on C, and such that R0 subsumes R.
360

Clause-Learning Algorithms

Proof. Let R0 = (T0 , . . . , Tt ). Define I ⊆ {0, . . . , t} as the set of indices i such that the i-th
d
assignment of R0 assigns some variable in C. For i ∈ I, let xi = ai or xi = ai be the i-th
assignment in R0 .
We will construct a round R = (S0 , . . . , Ss ) started with D inductively. Associated with
each Sj is the set Ij ⊆ I of indices i such that xi is left unassigned in Sj . Recall that S0 is
the empty state by definition. Hence I0 = I. We define the following process:
1. If Sj falsifies some clause in D or it sets all variables in V then set s = j and stop.
2. Otherwise, if there is a unit clause {xa } in D|Sj then let Sj+1 be Sj plus x = a.
3. Otherwise, if Ij is non-empty, let i be the minimum element of Ij , and let Sj+1 be
d
obtained by adding the decision xi = ai to Sj .
If none of the above cases applies set s = j and stop the process.
By construction R is a valid round started with D. Let us see that R0 subsumes R: let
A be the set of literals made true by decisions in R. By construction, R and R0 agree on
A and hence R0 subsumes R by Lemma 1. Furthermore, R is inconclusive: By D ⊆ D0
and R0 being inconclusive, D|R0 does not contain the empty clause, and as R0 subsumes
R, also D|R does not contain the empty clause. Further, as every variable in C belongs
to V and R is inconclusive, the process stops with Is = ∅. Together with the fact that R0
subsumes R, this shows that R and R0 agree on C. Note finally that R branches in C by
construction.
3.2 Absorption
One key feature of the definition of a round is that if it is inconclusive, then the residual set of
clauses does not contain unit clauses and, in particular, it is closed under unit propagation.
This means that for an inconclusive round R started with D, if A is a clause in D and R
falsifies all its literals but one, then R must satisfy the remaining literal, and hence A as
well. Besides those in D, other clauses may have this property, which is important enough
to deserve a definition:
Definition 3 (Absorption). Let D be a set of clauses, let A be a non-empty clause and let
xa be a literal in A. We say that D absorbs A at xa if every inconclusive round started with
D that falsifies A \ {xa } assigns x to a. We say that D absorbs A if D absorbs A at every
literal in A.
Naturally, when D absorbs A at xa we also say that A is absorbed by D at xa .
Intuitively, one way to think of absorbed clauses is as being learned implicitly. The rest
of this section is devoted to make this intuition precise. For now, let us note that if there are
no inconclusive rounds started with D, then every clause is absorbed. This agrees with the
given intuition since the absence of inconclusive rounds means that unit-clause propagation
applied on D produces the empty clause. In this section we also show that the notion
of clause-absorption is tightly connected to the concept of 1-empowerment independently
introduced by Pipatsrisawat and Darwiche (2009).
361

Atserias, Fichte, & Thurley

3.2.1 Properties of Absorption
Before we continue, let us discuss some key properties of absorption. We argued already
that every clause in D is absorbed by D. We give an example showing that D may absorb
other clauses. Let D be the set consisting of the three clauses
a ∨ b̄

b∨c

ā ∨ b̄ ∨ d ∨ e.

In this example, the clause a ∨ c does not belong to D but is absorbed by D since every inconclusive round that sets a = 0 must set c = 1 by unit-propagation, and every inconclusive
round that sets c = 0 must set a = 1 also by unit-propagation. While D may absorb other
clauses as we just saw, we note that every non-empty clause absorbed by D is a logical
consequence of D. We write D |= C, if every satisfying assignment of D satisfies C.
Lemma 4. Let D be a set of clauses and let C be a non-empty clause. If D absorbs C,
then D |= C.
Proof. Let S be a full assignment that satisfies all clauses in D. We want to show that S
satisfies C as well. Let R = (S0 , . . . , Sr ) be a complete round of the algorithm started with
D that sets all its decision variables as they are set in S. By induction on i ∈ {0, . . . , r}, we
will show that Si ⊆ S and it will follow that R is not stopped by a conflict and therefore
Sr = S. In particular R is inconclusive, and if it falsifies all literals of C but one, it must
satisfy the remaining one because C is absorbed. Since R sets all variables in C and Sr = S,
this means that S satisfies C.
It remains to show that Si ⊆ S for every i. For i = 0 there is nothing to show since
d
S0 = ∅. Fix i > 0 and assume that Si−1 ⊆ S. Let x = a or x = a be the last assignment in
d
Si . The case x = a is taken care by the assumption that all decision variables of R are set as
in S. Suppose then that the last assignment x = a is implied. This means that there exists
a clause A in D such that A|Si−1 = {xa }. Since S satisfies D and Si−1 ⊆ S, necessarily x
is set to a in S.
Next, let us see that the converse of the above lemma does not hold; namely, we see
that not every implied clause is absorbed. In the previous example, for instance, note that
b̄∨d∨e is a consequence of D (resolve the first and the third clause on a) but is not absorbed
d
d
by D (consider the inconclusive round d = 0, e = 0).
One interesting property that is illustrated by this example is that if C is the resolvent
of two absorbed clauses A and B, and C is not absorbed at some literal `, then ` appears
in both A and B. In the example above, D does not absorb b̄ ∨ d ∨ e at b̄, and b̄ appears in
the clauses a ∨ b̄ and ā ∨ b̄ ∨ d ∨ e from D, whose resolvent is precisely b̄ ∨ d ∨ e. We will
prove this general fact in the next section where the objects of study will be non-absorbed
resolvents of absorbed clauses.
Next we show three key monotonicity properties of clause-absorption, where the first is
the one that motivated its definition.
Lemma 5. Let D and E be sets of clauses and let A and B be non-empty clauses. The
following hold:
1. if A belongs to D, then D absorbs A,
362

Clause-Learning Algorithms

2. if A ⊆ B and D absorbs A, then D absorbs B,
3. if D ⊆ E and D absorbs A, then E absorbs A.
Proof. To prove 1. assume for contradiction that there is a literal ` in A and an inconclusive
round S0 , . . . , Sr started with D which falsifies A\{`} but does not satisfy A. As the round is
inconclusive, we cannot have A|Sr = ∅, which means then that A|Sr = {`}, in contradiction
to the definition of round.
For the proof of 2. let ` be a literal of B and define B 0 = B \ {`}. We consider two
different cases. If ` ∈
/ A then A ⊆ B 0 and, as A is absorbed by D, there is no inconclusive
round which falsifies B 0 . Thus B is absorbed in this case. If ` ∈ A, let A0 = A \ {`} and let
S0 , . . . , Sr be an inconclusive round started with D which falsifies B 0 . Then it falsifies A0
and satisfies A by absorption. Thus it satisfies B, and B is absorbed in this case as well.
It remains to prove 3. Let ` be some literal in A and A0 = A \ {`}. Let R0 be an
inconclusive round started with E which falsifies A0 . By Lemma 2, there is an inconclusive
round R started with D which falsifies A0 and which is subsumed by R0 . As A is absorbed
by D, we see that R (and hence R0 ) satisfies A.
3.2.2 Absorption and Empowerment
Our next goal is to show that absorption and empowerment are dual notions. For assignments α, β we write α ⊆ β if every assignment in α is also in β. Let us reproduce the
definition of 1-empowerment in the work of Pipatsrisawat and Darwiche (2009), slightly
adapted to better suit our notation and terminology.
Definition 6 (1-Empowerment in Pipatsrisawat & Darwiche, 2009). Let D be a set of
clauses, let C be a non-empty clause and let xa be a literal in C. Let α be the assignment
that sets y = 1 − b for every literal y b in C \ {xa }. We say that C is 1-empowering via xa
with respect to D, if the following three conditions are met:
1. C is a logical consequence of D; i.e. D |= C,
2. repeated applications of unit-clause propagation on D|α do not yield the empty clause,
3. repeated applications of unit-clause propagation on D|α do not assign x to a.
We also say that xa is an empowering literal of C. We say that C is 1-empowering if it is
1-empowering via some literal in C.
A preliminary version of this definition was given by Pipatsrisawat and Darwiche (2008)
where the second of the three conditions was not required.
By the definition of absorption, we see that if some non-empty clause A is not absorbed
by a set of clauses D, then there is an inconclusive round R started with D and a literal xa in
A such that R falsifies A \ {xa } but does not satisfy {xa }. When A is a logical consequence
of D, this witnesses precisely the fact that A is 1-empowering via xa . We show that the
converse is also true:
Lemma 7. Let D be a set of clauses, let C be a non-empty clause such that D |= C, and
let xa be a literal in C. Then, C is 1-empowering via xa with respect to D if and only if D
does not absorb C at xa .
363

Atserias, Fichte, & Thurley

Proof. Let C 0 = C\{xa }. Assume first that D does not absorb C at xa . Let R = (S0 , . . . , Sr )
be an inconclusive round started with D witnessing this fact, i.e. Sr falsifies C 0 and does
not assign x = a. In particular α ⊆ Sr . Furthermore, for every unit clause {y b } in D|α
we have y = b in Sr , as R is an inconclusive round. By a straightforward induction, we
see that every β obtained from α by repeated applications of unit-clause propagation from
D|α also satisfies β ⊆ Sr . This directly implies conditions 2. and 3. in the definition of
1-empowerment. Condition 1. is met by assumption.
For the converse, assume that C is 1-empowering via xa with respect to D. We have to
show that there is an inconclusive round started with D that falsifies C 0 but does not assign
x = a. Let R = (S0 , . . . , Sr ) be a round started with D in which every decision assignment
is chosen to falsify a literal in C 0 , and that, among all rounds with this property, assigns as
many literals from C 0 as possible. Clearly such a maximal round exists since the one that
does not make any decision meets the property.
We shall show that R is the round we seek. For each i ∈ {0, . . . , r}, let αi ⊆ α be the
maximal assignment such that αi ⊆ Si , let βi be obtained from αi by repeated applications
of unit-clause propagation from D|αi , and let γi be the subset of assignments in βi that are
also in Si . In particular γi ⊆ Si . We shall prove, by induction on i, that Si ⊆ γi and hence
Si = γi .
The base case i = 0 is trivial since S0 = ∅. Assume now that i > 0 and Si−1 ⊆ γi−1 . If
the i-th assignment of Si is a decision assignment, then by construction it falsifies a literal
in C 0 and hence belongs to α. But then it also belongs to αi , βi and γi as required. If
the i-th assignment of Si is implied we distinguish two cases: whether it also belongs to
α or not. If the implied assignment is also in α, then it is in αi , βi and γi as required. If
the implied assignment is not in α, then αi = αi−1 and hence βi = βi−1 . But then, since
Si−1 ⊆ γi−1 by induction hypothesis and γi−1 ⊆ βi−1 , the unit clause responsible for the
definition of Si appears in the process of forming βi−1 and hence in the process of forming
βi . Therefore the assignment will also be in γi .
This completes the induction and shows, in particular, that Sr = γr . By point 2. in the
definition of 1-empowerment, R is inconclusive. Furthermore, by point 3. in the definition
of 1-empowerment, Sr does not assign x = a. It remains to show that Sr falsifies C 0 . First
note that, by the maximality of R and the fact that R is inconclusive, every literal in C 0
is assigned by R. Moreover, since the decision assignments of R are chosen to falsify the
literals in C 0 , it suffices to show that the implied assignments of R do not satisfy any literal
in C 0 . Thus, suppose for contradiction that y = b is an implied assigned in R and that
y b is a literal in C 0 . Let i ∈ {0, . . . , r} be such that {y b } is a unit-clause in D|Si . Since
Si ⊆ Sr ⊆ γr and y is assigned to 1 − b in α, the unit-clause {y b } in D|Si appears as the
empty clause in the closure under unit-clause propagation of D|α ; this contradicts point 2.
in the definition of 1-empowerment and completes the proof.

Let us note at this point that if condition 1. in the definition of 1-empowerment is
dropped, then the hypothesis that D |= C can also be dropped from Lemma 7. This would
make 1-empowerment and absorption literally dual of each other.
364

Clause-Learning Algorithms

3.3 Beneficial Rounds
We shall now study the key situation that explains how the algorithm can possibly simulate
resolution proofs. Consider the resolvent C = Res(A, B) of two absorbed clauses A and B
which itself, however, is not absorbed. Our goal is to study what A, B and C look like
in such a case. We start by showing that if C is not absorbed at a literal ` ∈ C, then `
appears in both A and B. This property held the key for discovering the concept of clauseabsorption and its relevance to the simulation of resolution proofs. A similar connection to
clause learning was observed by Pipatsrisawat and Darwiche (2008), where it is also pointed
out that the condition that some literal from C appears in both A and B is known as merge
resolution (Andrews, 1968).
Lemma 8. Let D be a set of clauses, let A and B be two resolvable clauses that are absorbed
by D, and let C = Res(A, B). If ` is a literal in C and D does not absorb C at `, then `
appears in both A and B.
Proof. Let y be such that C = Res(A, B, y), and let A0 = A \ {y} and B 0 = B \ {ȳ}. Let
` = xa be a literal in C and assume D does not absorb C at `. Then there exists an
inconclusive round R that falsifies C \ {xa } but does not set x to a. Since ` belongs to C
and C = A0 ∪ B 0 we have that ` belongs to A or to B, or to both. If it belongs to both,
we are done. Otherwise, assume without loss of generality that it belongs to A but not to
B. In this case R falsifies B \ {ȳ}, and since B is absorbed, y is set to 0 in R. But then R
falsifies A \ {xa }, and since A is absorbed, x is set to a in R. This contradicts the choice of
R where x was not set to a.
We continue by showing that in the situation of interest, there always exist a beneficial
round of the algorithm which predicts eventual absorption.
Definition 9 (Beneficial Round). Let D be a set of clauses, let A be a non-empty clause,
let xa be a literal of A, and let R be an inconclusive round started with D. We say that R is
beneficial for A at xa if it falsifies A \ {xa }, branches in A \ {xa }, leaves x unassigned, and
d
yields a conclusive round if extended by the decision x = a . The conclusive round obtained
d
by extending R by x = a is also called beneficial for A at xa . We say that R is beneficial
for A if it is beneficial for A at some literal in A.
In other words, a round started with D that is beneficial for A at xa is a witness that
D does not absorb A at xa , which is minimal with this property, and yet yields a conflict
when x is set to the wrong value. Thus, informally, a beneficial round is a witness that D
almost absorbs A at xa .
Lemma 10. Let D be a set of clauses, let A and B be two resolvable clauses that are
absorbed by D, and let C = Res(A, B). If C is non-empty and not absorbed by D, then
there is a round started with D that is beneficial for C.
Proof. We identify a literal xa in C for which we are able to build a beneficial round for C
at xa .
Let y be such that C = Res(A, B, y), and let A0 = A \ {y} and B 0 = B \ {ȳ}. As C is
non-empty and not absorbed by D, there is a literal xa in C and an inconclusive round R0
365

Atserias, Fichte, & Thurley

started with D which falsifies C 0 = C \ {xa } but does not set x to a. Also x is not assigned
ā in R0 since otherwise it would falsify C, and as C = A0 ∪ B 0 and D absorbs both A and
B, both y and ȳ would be satisfied by R0 . This shows that x is unassigned in R0 .
Let R be the inconclusive round started with D which is obtained by applying Lemma 2
to C 0 and the given inconclusive round R0 . We claim that R is beneficial for C at xa : The
round R falsifies C 0 , as it agrees with R0 on C 0 . Also R branches in C 0 and, as R0 subsumes
R, leaves x unassigned. Finally, note that R and R0 also agree on A \ {y} and B \ {ȳ}.
d
Hence extending the round R by a decision x = ā yields a conclusive round; otherwise both
y and ȳ would be satisfied since both A and B are absorbed by D.
3.4 Main Technical Lemma
We will now start analyzing the number of complete rounds it takes until the resolvent of
two absorbed clauses is absorbed as a function of its width. However, as this is not trivial we
first have to determine the number of complete rounds it takes until a sufficient prerequisite
of absorption occurs: a beneficial round.
Lemma 11. Let D be a set of clauses, and let A and B be two resolvable clauses that
are absorbed by D and that have a non-empty resolvent C = Res(A, B). Let n be the total
number of variables in D, and k be the width of C. For every t ≥ 1, let R0 , . . . , Rt−1 denote
the t consecutive complete rounds of the algorithm started with D, and let D0 , . . . , Dt−1
denote the intermediate sets of clauses. Then, the probability that none of the Ri is beneficial
k
for C and none of the Di absorbs C is at most e−t/(4n ) .
Proof. Let R0 , . . . , Rt−1 denote the t consecutive complete rounds of the algorithm started
with D, and let D0 , . . . , Dt−1 be the intermediate sets of clauses. In particular D0 = D and
Ri is a round started with Di . For every i ∈ {0, . . . , t − 1} let Ri be the event that Ri is
not beneficial and let Di be the event that Di does not absorb C. We want to compute an
upper bound for the joint probability of these events. Note that
"t−1
# t−1 "
# t−1 "
#
j−1
 j−1

\
Y
Y
\
 \

Pr
Ri ∩ D i =
Pr Rj ∩ Dj 
Ri ∩ D i ≤
Pr Rj  Dj ∩
Ri ∩ D i
(1)
i=0

j=0

i=0

j=0

i=0

Hence, we shall give appropriate upper bounds for the factors on the right hand side of
this inequality. To do this, let us first bound Pr R̄j | Dj , Rj−1 , Dj−1 , . . . , R0 , D0 from
below. Under the conditions Dj , Rj−1 , Dj−1 , . . . , R0 , D0 , Lemma 10 implies that there is
an inconclusive round R started with Dj which is beneficial for C at some xa ∈ C. The
probability that Rj is beneficial for C is bounded from below by the probability that Rj is
beneficial for C at xa . We will therefore bound the latter from below.
First let us compute a lower bound on the probability that the first k − 1 decisions of the
d
decision strategy are chosen to falsify C \ {xa } and the k-th choice is x = ā. The probability
that these choices are made is at least




 

k−1
k−2
1
1
(k − 1)!
1
···
≥
≥ k.
k
k
2n
2(n − 1)
2(n − k + 2)
2(n − k + 1)
2 n
4n
Note that a round started with Dj that follows these choices may not even be able to do
some of the decisions as the corresponding assignments may be implied. However, before
366

Clause-Learning Algorithms

d

the decision x = ā is made, a round following these choices will only perform decisions that
agree with R in C \ {xa } and therefore stay subsumed by R after every new decision, by
d
Lemma 1. In particular, right before the decision x = a it will be inconclusive, it will falsify
C \ {xa }, and it will leave x unset. Also by Lemma 1 it has performed the same assignments
d
as R up to order, and therefore the addition of x = a will make it conclusive. It follows
that the probability that the round will be beneficial for C at xa can only be bigger.
Consequently, the probability of Rj conditional on Dj , Rj−1 , Dj−1 , . . . , R0 , D0 is bounded
from above by 1 − 4n1k . Therefore, by equation (1) we have
Pr

"t−1
\
i=0

#
Ri ∩ D i



1 t
k
≤ 1− k
≤ e−t/(4n )
4n

where in the second inequality we used the fact that 1 + x ≤ ex for every real number x.
3.5 Bounds
With the tools given above, we are now able to prove the main result of the paper: the
simulation of width-k resolution by the algorithm. We shall first give the proof for the
algorithm employing the Decision learning scheme. Not only is the proof easier and more
instructive, but also we get slightly better bounds for this special case. Afterwards, we will
see the result for asserting learning schemes in general.
3.5.1 The Decision Scheme
The fact that makes Decision easier to analyze is that, for this learning scheme, the
occurrence of a beneficial round immediately yields absorption at the next step. Indeed, if
R is beneficial for C, then it branches in C, which means that the clause learned in this
complete round is a subset of C. In particular this means that the next set of clauses will
absorb a subset of C, and hence C as well by Lemma 5. We obtain the following result as
a direct consequence to Lemma 11.
Lemma 12. Let D be a set of clauses, and let A and B be two resolvable clauses that
are absorbed by D and that have a non-empty resolvent C = Res(A, B). Let n be the total
number of variables in D and k be the width of C. Then, for all t ≥ 1, using the Decision
learning scheme, the probability that C is not absorbed by the current set of clauses after t
k
restarts is at most e−t/(4n ) .
Proof. Let R0 , . . . , Rt−1 denote the t consecutive complete rounds of the algorithm started
with D, and let D0 , . . . , Dt be the intermediate sets of clauses. In particular D0 = D
and Ri is a round started with Di . For every i ∈ {0, . . . , t − 1} let Ri be the event that
Ri is not beneficial for C and let Di be the event that Di does not absorb C. If one of
the Ri is beneficial for C, then Di+1 absorbs C. To see this, note that as R branches in
C, the clause Ci learned from Ri satisfies Ci ⊆ C. Hence Di+1 absorbs both Ci and C by
Lemma 5. Further, Dt also absorbs C, if one of the Di absorbs it again by Lemma
T 5. Hence,
the probability that C is not absorbed by Dt is bounded from above by Pr[ t−1
i=0 Ri ∩ Di ].
k)
−t/(4n
Lemma 11 implies that this is bounded by e
.
367

Atserias, Fichte, & Thurley

Theorem 13. Let F be a set of clauses on n variables having a resolution refutation of
width k and length m. With probability at least 1/2, the algorithm started with F , using
the Decision learning scheme, learns the empty clause after at most 4m ln(4m)nk conflicts
and restarts.
Proof. The resolution refutation must terminate with an application of the resolution rule
of the form Res(x, x̄). We will show that for both ` = x and ` = x̄, the probability that
{`} is not absorbed by the current set of clauses after 4m ln(4m)nk restarts is at most 1/4.
Thus, both {x} and {x̄} will be absorbed with probability at least 1/2. If this is the case, it
is straightforward that every complete round of the algorithm is conclusive. In particular,
the round that does not make any decision is conclusive, and in such a case the empty
clause is learned.
Let C1 , C2 , . . . , Cr = {`} be the resolution proof of {`} that is included in the width-k
resolution refutation of F . In particular r ≤ m−1 and every Ci is non-empty and has width
at most k. Let D0 , D1 , . . . , Ds be the sequence of clause-sets produced by the algorithm
where s = rt and t = d4 ln(4r)nk e. For every i ∈ {0, . . . , r}, let Ei be the event that every
clause in the initial segment C1 , . . . , Ci is absorbed by Dit , and let E i be its negation. Note
that Pr[ E0 ] = 1 vacuously and hence Pr[ E 0 ] = 0. For i > 0, we bound the probability that
Ei does not hold conditional on Ei−1 by cases. Let pi = Pr[ E i | Ei−1 ] be this probability. If
Ci is a clause in F , we have pi = 0 by Lemma 5. If Ci is derived from two previous clauses,
k
we have pi ≤ e−t/(4n ) by Lemma 12, which is at most 1/(4r) by the choice of t.
The law of total probability gives
 



 

Pr E i = Pr E i | Ei−1 Pr [Ei−1 ] + Pr E i | E i−1 Pr E i−1




≤ Pr E i | Ei−1 + Pr E i−1 .
 


P
Adding up over all i ∈ {1, . . . , r}, together with Pr E 0 = 0, gives Pr E r ≤ ri=1 pi ≤
r
1
4r = 4 . Since the probability that Cr is not absorbed by Drt is bounded by Pr[ E r ], the
proof follows.
3.5.2 Asserting Learning Schemes in General
We shall now study the algorithm applying an arbitrary asserting learning scheme. The
analysis is a bit more complex than that of the Decision scheme since in general a clause
learned from a complete round R cannot be assumed to be a subset of the decisions in R.
Therefore we can only show that the resolvent is eventually absorbed by a little detour. We
note that this proof has to overcome similar difficulties as, and is inspired by3 , the proof of
Proposition 2 in the work of Pipatsrisawat and Darwiche (2009).
We need some preparation. Let C be a clause and D be a set of clauses. Let WC,D
denote the set of literals ` in C such that there exists an inconclusive round started with
D that is beneficial for C at `. Let u`,C,D denote the number of variables left unassigned
by an inconclusive round started with D which is beneficial for C at `. If no such round
exists, we define u`,C,D = 0. Note that this number is well-defined, as it follows easily from
3. We thank an anonymous reviewer for pointing out that the original proof of Proposition 2 in the work
of Pipatsrisawat and Darwiche (2009) contained an error that was corrected in the version of the paper
on their webpage. Our proof is not affected by this error.

368

Clause-Learning Algorithms

Lemma 1 that every inconclusive round started with D which is beneficial for C at ` leaves
the same number of variables unassigned. Further, define
uC,D =

X

u`,C,D .

`∈WC,D

Note that if C is absorbed by D, then WC,D = ∅. Moreover, under the hypothesis of
Lemma 10, the converse is also true. Analogously, if C is absorbed by D, then uC,D = 0
and, under the hypothesis of Lemma 10, the converse is also true.
Lemma 14. Let D and D0 be sets of clauses with D ⊆ D0 . Let A and B be two resolvable
clauses that are absorbed by D, and let C = Res(A, B). Then, WC,D0 ⊆ WC,D and u`,C,D0 ≤
u`,C,D for all ` ∈ WC,D .
Proof. If WC,D0 = ∅, nothing is to be shown. Otherwise, for xa in WC,D0 , we start by
showing that xa belongs to WC,D . Let R0 be an inconclusive round started with D0 which is
beneficial for C at `. Application of Lemma 2 to R0 and C \{xa } yields an inconclusive round
R started with D with the following properties: R0 subsumes R, both agree on C \ {xa },
and R branches in C \ {xa }. To show that R is beneficial for C at xa , it only remains to
d
prove that extending R by x = ā yields a conclusive round. Let R∗ be a round defined by
this extension. Let y be such that C = Res(A, B, y). Then R∗ falsifies B \ {ȳ} and A \ {y}.
By absorption, R∗ cannot be inconclusive, as otherwise, y and ȳ would be satisfied by R∗ .
This proves WC,D0 ⊆ WC,D .
Now, we show that u`,C,D0 ≤ u`,C,D for every ` in WC,D . If ` does not belong to WC,D0
nothing is to be shown since u`,C,D0 = 0 in that case. Otherwise, let R0 and R be inconclusive
rounds beneficial for C at ` such that R0 is started with D0 and R is started with D. By
Lemma 1, R0 subsumes R, which finishes the proof.
Lemma 15. Let D be a set of clauses, let A and B be two resolvable clauses that are
absorbed by D, and let C = Res(A, B). Let R be a conclusive round started with D and let
D0 be obtained from D by adding the asserting clause learned from R. If C is not empty
and R is beneficial for C at some ` ∈ C, then u`,C,D0 < u`,C,D and uC,D0 < uC,D .
Proof. By Lemma 14 we already know that uC,D0 ≤ uC,D and u`,C,D0 ≤ u`,C,D . Therefore,
it suffices to demonstrate that, in the presence of R, the second inequality is strict.
By hypothesis, R is beneficial for C at `. Let C 0 be the asserting clause learned by R.
Let R∗ be the unique inconclusive round contained in R which is beneficial for C at `; this
is the round which does not contain the last decision made by R. By Lemma 1, the number
of assignments made by any two rounds started with D and beneficial for C at ` are the
same. Hence, the number of variables left unassigned by R∗ equals u`,C,D , and u`,C,D ≥ 1
since at least one variable is unset.
If u`,C,D0 = 0 then already u`,C,D0 < u`,C,D . Therefore, assume that u`,C,D0 ≥ 1. In
particular, there exists an inconclusive round R0 started with D0 which is beneficial for C at
`. By Lemma 1 the round R0 subsumes R∗ . By the definition of asserting clauses, C 0 |R∗ is a
unit clause, and since C 0 belongs to D0 , it is absorbed by D0 and hence R0 satisfies C 0 . This
proves that R0 sets at least one more variable than R∗ and therefore u`,C,D0 < u`,C,D .
369

Atserias, Fichte, & Thurley

With these two technical lemmas in hand we are ready to state and prove the analogue
of Lemma 12 for arbitrary asserting learning schemes.
Lemma 16. Let D be a set of clauses, and let A and B be two resolvable clauses that
are absorbed by D and that have a non-empty resolvent C = Res(A, B). Let n be the total
number of variables in D and let k be the width of C. Then, for all t ≥ 1, using an arbitrary
asserting learning scheme, the probability that C is not absorbed by the current set of clauses
k
after kn · t restarts is at most kn · e−t/(4n ) .
Proof. Let b = uC,D , and s = bt, and let D0 , . . . , Ds be the sequence of sets of clauses
produced by the algorithm, starting with D0 = D. For every i ∈ {0, . . . , b}, let Xi = uC,Dit
and let Ei be the event that Xi ≤ b − i.
We will bound the probability that C is not absorbed by Dbt from above. Since this
event implies that Xb 6= 0, it suffices to bound Pr[ E b ]. Note that Pr[ E0 ] = 1 vacuously
and hence Pr[ E 0 ] = 0. For i > 0, we bound the probability that Ei does not hold. The law
of total probability gives
 



 

Pr E i = Pr E i | Ei−1 Pr [Ei−1 ] + Pr E i | E i−1 Pr E i−1




≤ Pr E i | Ei−1 + Pr E i−1 .
Let pi = Pr[ E i | Ei−1 ] and note that Pr[ E i | Xi−1 < b − i + 1 ] = 0. Hence we have pi ≤
Pr[ E i | Xi−1 = b − i + 1 ]. Consider the sequence D(i−1)t+1 , . . . , Dit of sets of clauses and
the corresponding complete rounds of the algorithm. Conditional on Xi−1 = b − i + 1, the
event E i implies that Xi = Xi−1 6= 0 and hence none of the above sets of clauses absorbs
C. Furthermore, by Lemma 15, none of the corresponding rounds is beneficial for C. Thus,
k
by Lemma 11, we have pi ≤ e−t/(4n ) . Adding up over all i ∈ {1, . . . , r}, together with
 


Pb
k
Pr E 0 = 0, gives Pr E b ≤ i=1 pi ≤ b · e−t/(4n ) . The Lemma follows as necessarily
b ≤ kn.
We are now able to prove the main theorem.
Theorem 17. Let F be a set of clauses on n variables having a resolution refutation of width
k and length m. With probability at least 1/2, the algorithm started with F , using an arbitrary asserting learning scheme, learns the empty clause after at most 4km ln(4knm)nk+1
conflicts and restarts.
Proof. The proof is analogous to the proof of Theorem 13 with Lemma 16 playing the role
of Lemma 12, and choosing t = d4 ln(4m · kn)nk e now.

4. Consequences

The total number of clauses of width k on n variables is bounded by 2k nk , which is at most
2nk for every n and k. Therefore, if F has n variables and a resolution refutation of width
k, we may assume that its length is at most 4nk by the following estimate
 
 k

k
k
X
X
n −1
i n
i
≤1+2
n = 1 + 2n ·
≤ 4nk .
2
i
n−1
i=0

i=1

We obtain the following consequence to Theorem 17.
370

Clause-Learning Algorithms

Corollary 18. Let F be a set of clauses on n variables having a resolution refutation of
width k. With probability at least 1/2, the algorithm started with F , using an arbitrary
asserting learning scheme, learns the empty clause after at most 16k(k + 1) ln(16kn)n2k+1
conflicts and restarts.
An application of Corollary 18 is that, even though it is not explicitly defined for the
purpose, the algorithm can be used to decide the satisfiability of CNF formulas of treewidth
at most k in time O(k 2 log(kn)n2k+3 ). This follows from the known fact that every unsatisfiable formula of treewidth at most k has a resolution refutation of width at most k + 1
(Alekhnovich & Razborov, 2002; Dalmau, Kolaitis, & Vardi, 2002; Atserias & Dalmau,
2008).
If we are interested in producing a satisfying assignment when it exists, we proceed by
self-reducibility: we assign variables one at a time, running the algorithm log2 (n) + 1 times
after each assignment to detect if the current partial assignment cannot be extended any
further, in which case we choose the complementary value for the variable. For this we use
the fact that if F has treewidth at most k, then F |x=a also has treewidth at most k. For
the analysis, note that since each run of the algorithm is correct with probability at least
1/2, each new assignment is correct with probability at least
1 − 2−(log2 (n)+1) = 1 −

1
.
2n

This means that all iterations are correct with probability at least (1 −
running time of this algorithm is O(k 2 (log(kn))2 n2k+4 ).

1 n
2n )

≥

1
2.

The

Acknowledgments
We thank Martin Grohe for suggesting the problem of comparing the power of SAT-solvers
with bounded-width resolution. We also thank Knot Pipatsrisawat and Adnan Darwiche
for pointing out the connection between 1-empowering and absorption. Thanks also to
Peter Jeavons for comments on the conference version of this paper, and to the anonymous
referees for very detailed comments.
The first author was supported in part by CYCIT TIN2007-68005-C04-03. The second
author was supported in part by the European Research Council (ERC), Grant 239962. The
third author was supported in part by a fellowship within the Postdoc-Programme of the
German Academic Exchange Service (DAAD). A preliminary version of this paper appeared
in the Proceedings of the 12th International Conference on Theory and Applications of
Satisfiability Testing, SAT’09 (Atserias et al., 2009).

References
Alekhnovich, M., & Razborov, A. A. (2002). Satisfiability, branch-width and Tseitin tautologies. In Proceedings of the 43rd Symposium on Foundations of Computer Science
(FOCS 2002), pp. 593–603. IEEE Computer Society.
Alekhnovich, M., & Razborov, A. A. (2008). Resolution is not automatizable unless W[P]
is tractable. SIAM J. Comput., 38 (4), 1347–1363.
371

Atserias, Fichte, & Thurley

Andrews, P. B. (1968). Resolution with merging. J. ACM, 15 (3), 367–381.
Atserias, A., & Dalmau, V. (2008). A combinatorial characterization of resolution width.
J. Comput. Syst. Sci., 74 (3), 323–334.
Atserias, A., Fichte, J. K., & Thurley, M. (2009). Clause-learning algorithms with many
restarts and bounded-width resolution. In Kullmann, O. (Ed.), Proceedings of the 12th
International Conference on Theory and Applications of Satisfiability Testing (SAT),
Vol. 5584 of Lecture Notes in Computer Science, pp. 114–127. Springer.
Bayardo, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques to solve real-world
SAT instances. In Proceedings of the Fourtheenth National Conference on Artificial
Intelligence (AAAI’97), pp. 203–208.
Beame, P., Kautz, H. A., & Sabharwal, A. (2003). Understanding the power of clause
learning. In Gottlob, G., & Walsh, T. (Eds.), Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), pp. 1194–1201. Morgan
Kaufmann.
Beame, P., Kautz, H. A., & Sabharwal, A. (2004). Towards understanding and harnessing
the potential of clause learning. J. Artif. Intell. Res. (JAIR), 22, 319–351.
Ben-Sasson, E., & Johannsen, J. (2010). Lower bounds for width-restricted clause learning
on small width formulas. In Strichman, O., & Szeider, S. (Eds.), Proceedings of 13th
International Conference on Theory and Applications of Satisfiability Testing (SAT),
Vol. 6175 of Lecture Notes in Computer Science, pp. 16–29. Springer.
Ben-Sasson, E., & Wigderson, A. (1999). Short proofs are narrow - resolution made simple.
In Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing
(STOC 1999), pp. 517–526.
Dalmau, V., Kolaitis, P. G., & Vardi, M. Y. (2002). Constraint satisfaction, bounded
treewidth, and finite-variable logics. In CP ’02: Proceedings of the 8th International
Conference on Principles and Practice of Constraint Programming, pp. 310–326, London, UK. Springer-Verlag.
Fox, D., & Gomes, C. P. (Eds.). (2008). Proceedings of the Twenty-Third AAAI Conference
on Artificial Intelligence, AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008. AAAI
Press.
Hertel, P., Bacchus, F., Pitassi, T., & Gelder, A. V. (2008). Clause learning can effectively
p-simulate general propositional resolution.. In Fox, & Gomes (Fox & Gomes, 2008),
pp. 283–290.
Jeavons, P., & Petke, J. (2010). Local consistency and sat-solvers. In Proceedings of the
16th International Conference on Principles and Practice of Constraint Programming
- CP 2010, Vol. 6308 of Lecture Notes in Computer Science, pp. 398–413. Springer.
Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering an efficient SAT solver. In Proceedings of the 38th Design Automation Conference
(DAC’01).
Nieuwenhuis, R., Oliveras, A., & Tinelli, C. (2006). Solving SAT and SAT Modulo Theories: From an abstract Davis–Putnam–Logemann–Loveland procedure to DPLL(T).
Journal of the ACM, 53 (6), 937–977.
372

Clause-Learning Algorithms

Nordström, J. (2009). Narrow proofs may be spacious: Separating space and width in
resolution. SIAM J. Comput., 39 (1), 59–121.
Pipatsrisawat, K., & Darwiche, A. (2008). A new clause learning scheme for efficient unsatisfiability proofs.. In Fox, & Gomes (Fox & Gomes, 2008), pp. 1481–1484.
Pipatsrisawat, K., & Darwiche, A. (2009). On the power of clause-learning SAT solvers
with restarts. In Gent, I. P. (Ed.), Proceedings of the 15th International Conference
on Principles and Practice of Constraint Programming - CP 2009, Vol. 5732 of Lecture
Notes in Computer Science, pp. 654–668. Springer.
Silva, J. P. M., & Sakallah, K. A. (1996). Grasp - a new search algorithm for satisfiability.
In Proceedings of IEEE/ACM International Conference on Computer-Aided Design,
pp. 220–227.
Zhang, L., Madigan, C. F., Moskewicz, M. W., & Malik, S. (2001). Efficient conflict driven
learning in a boolean satisfiability solver. In International Conference on ComputerAided Design (ICCAD’01), pp. 279–285.

373

Journal of Artificial Intelligence Research 40 (2011) 221-267

Submitted 06/10; published 01/11

A Probabilistic Approach for Maintaining Trust
Based on Evidence
Yonghong Wang

yhwang@andrew.cmu.edu

Robotics Institute
Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA 15213 USA

Chung-Wei Hang
Munindar P. Singh

chang@ncsu.edu
singh@ncsu.edu

Department of Computer Science
North Carolina State University
Raleigh, NC 27695-8206 USA

Abstract
Leading agent-based trust models address two important needs. First, they show how
an agent may estimate the trustworthiness of another agent based on prior interactions.
Second, they show how agents may share their knowledge in order to cooperatively assess
the trustworthiness of others. However, in real-life settings, information relevant to trust
is usually obtained piecemeal, not all at once. Unfortunately, the problem of maintaining
trust has drawn little attention. Existing approaches handle trust updates in a heuristic,
not a principled, manner.
This paper builds on a formal model that considers probability and certainty as two
dimensions of trust. It proposes a mechanism using which an agent can update the amount
of trust it places in other agents on an ongoing basis. This paper shows via simulation that
the proposed approach (a) provides accurate estimates of the trustworthiness of agents that
change behavior frequently; and (b) captures the dynamic behavior of the agents. This
paper includes an evaluation based on a real dataset drawn from Amazon Marketplace, a
leading e-commerce site.

1. Introduction
Let us consider applications in domains such as electronic commerce, social networks, collaborative games, and virtual worlds populated with multiple virtual characters. These
applications exhibit two important common features: (1) they naturally involve multiple
entities, real (humans or businesses) or fictional; and (2) these entities are—or behave as if
they are—autonomous and heterogeneous. For this reason, we view these entities or their
computational surrogates as agents. The success of an agent application, evaluated in terms
such as the quality of experience enjoyed by a user or the economic value derived by a business, depends on felicitous interactions among the agents. Since the agents are functionally
autonomous, the felicity of their interactions cannot be centrally ensured. Further, each
agent usually has limited knowledge of the others with whom it interacts. Therefore, each
agent relies upon a notion of trust to identify agents with whom to interact.
c
2011
AI Access Foundation. All rights reserved.

Wang, Hang, & Singh

Given our intended applications, we narrow our scope to agents who not only provide
and consume services, but also share information regarding the trustworthiness of other
agents. We assume each agent behaves according to a fixed type, meaning that although its
behavior could be complex, its trustworthiness is not based on the incentives or sanctions
it might receive, and its behavior is not different toward different participants. One can
imagine settings such as service encounters where a service provider does not selectively
favor some of its customers. Hence, the purpose of the trust model is to distinguish good
from bad agents, not directly to cause agents to behave in a good manner. Further, we
assume that the setting is empirical, meaning that the agents base the extent of their trust
in others upon the outcomes of prior interactions. The level of trust an agent Alice places in
an agent Bob can be viewed as Alice’s prediction of Bob providing it a good service outcome
in the future. To be empirically reliable, Alice should estimate Bob’s trustworthiness based
on its past experience with Bob. Both because (1) the parties with whom an agent deals may
alter their behavior and (2) the agent receives information about other parties incrementally,
it is important that an agent be able to update its assessments of trust.
As one would expect with such an important subject, several researchers have developed
formal ways to represent and reason with trust. Interestingly, however, existing approaches
do not concentrate on how to maintain such representations. It might seem that researchers
believe that a heuristic approach would be adequate. The typical approach is based on
exponential discounting, and requires a programmer to hand-tune parameters such as a
discount factor.
This paper contributes a model and method for updating trust ratings in light of incremental evidence. Specifically, it develops a principled, mathematical approach for maintaining trust historically (as a way to evaluate agents who provide services) and socially (as
a way to evaluate agents who provide information about other agents). Further, this paper
shows how to avoid any hand-tuned parameter.
1.1 Technical Motivation
A common way to estimate the trustworthiness of a provider is to evaluate the probability
of a future service outcome being good based on the number of good service outcomes from
the provider in the past. However, a traditional scalar representation (i.e., a probability)
cannot distinguish between getting one good outcome from two interactions, and getting
100 good outcomes from 200 interactions. But, intuitively, there is a significant difference
in terms of the confidence one would place in each of the above two scenarios. For this
reason, modern trust models define trust in terms of both the probability and the certainty
of a good outcome (Jøsang & Ismail, 2002; Wang & Singh, 2007; Gómez, Carbó, & Earle,
2007; Teacy, Patel, Jennings, & Luck, 2006; Harbers, Verbrugge, Sierra, & Debenham, 2007;
Paradesi, Doshi, & Swaika, 2009). The certainty is a measure of the confidence that an agent
may place in the trust information. Computing the certainty can help an agent filter out
those parties for whom it has insufficient information, even if nominally the probability of
a good outcome is high. In general, the certainty of a trust value should (a) increase as the
amount of information increases with a fixed probability, and (b) decrease as the number
of conflicts increases with a fixed total number of experiences (Wang & Singh, 2010).
222

A Probabilistic Approach for Maintaining Trust Based on Evidence

Open systems are dynamic and distributed. In other words, an agent often needs to
select a service provider with whom it has had no previous interaction. Referral networks
enable agents to collect trust information about service providers in a distributed manner
(Yu & Singh, 2002; Procaccia, Bachrach, & Rosenschein, 2007). In a referral network, an
agent requests other agents, called referrers, to provide trust information about a service
provider. If a referrer lacks direct experience with the service provider, it may refer to
another (prospective) referrer. Existing trust models (e.g., Barber & Kim, 2001), specify
how an agent may aggregate trust information from multiple sources (which could include
a combination of referrals and direct interactions).
We view referrals themselves as services that the referrers provide. Consequently, an
agent ought to be able to estimate a referrer’s trustworthiness based on the quality of the
referrals it provides. However, existing trust models lack a principled mechanism by which
to update the trust placed in a referrer.
Besides, to reflect the dynamism of agents over time, a discount factor is needed to
help trust models provide accurate predictions of future behavior (Zacharia & Maes, 2000;
Huynh, Jennings, & Shadbolt, 2006). With a low discount factor, past behavior is forgotten
quickly and the estimated trustworthiness reflects recent behavior. Conversely, with a
high discount factor, the estimated trustworthiness considers and emphasizes the long-term
overall behavior of the rated agent. Different discount factors can yield different accuracy
of behavior predictions. Choosing a proper discount factor for different types of agents in
varied settings involves a crucial trade-off between accuracy and evidence. This trade-off,
however, has not drawn much attention in the trust research community.
We propose a probabilistic approach for updating trust that builds on Wang and Singh’s
(2010) probability-certainty trust model. Our trust update method enriches Wang and
Singh’s trust model in two ways. First, our trust update applies in estimating the trustworthiness of referrers based on the referrals they provide. Second, our method adjusts the
discount factor dynamically by updating the dynamism of the agents without requiring any
manual tuning.
We select Wang and Singh’s trust model because it supports some features that are
crucial for our purposes. One, it defines the trustworthiness of an agent in evidence space,
representing trust using both probability and certainty. Two, it defines certainty so that the
amount of trust placed in an agent increases with the amount of evidence (if the extent of
conflict is held constant) and decreases with increasing conflict (if the amount of evidence is
held constant). Three, it supports operators for propagating trust through referrals. Wang
and Singh (2006) define mathematical operators for propagating trust. We incorporate
these operators as bases for addressing the specific technical problems of computing trust
updates and discounting referrers who provide erroneous referrals.
1.2 Contributions
This paper proposes a principled, evidence-based approach by which an agent can update the
amount of trust it places in another agent. It introduces formal definitions for updating the
trust placed and studies their mathematical properties. To achieve a self-tuning approach
for trust updates, this paper proposes the new notion of trust in history, in contrast to
the traditional notion of discounting history via a hand-tuned discount factor. This paper
223

Wang, Hang, & Singh

evaluates the proposed approach (1) conceptually via comparison with existing approaches
in terms of formal properties (2) via simulation against different agent behavior profiles;
and (3) with respect to some data from a real-life marketplace. The main outcomes are
that our approach
• Does not require the fine-tuning of parameters by hand, thereby not only reducing
the burden on a system administrator or programmer, but also expanding the range
of potential applications to include those where the behavior profiles of the agents are
not known ahead of time.
• Yields precision in estimating the probability component of trust.
• Yields a more appropriate level of the certainty component of trust than existing
approaches. In particular, it recognizes the effect of conflict in evidence and how to
compute certainty based on the certainty of the input information.
• Is robust against agents who provide wrong information.
1.3 Organization
The rest of this paper is organized as follows. Section 2 provides the essential technical
background for our approach. Section 3 introduces a general model for trust update that
uniformly handles both historical and social updates. Section 4 introduces a series of trust
update methods culminating in our proposed method. Section 5 evaluates these methods
on theoretical grounds by establishing theorems regarding the desirable and undesirable
properties. Section 6 specifies the historical and social update scenarios precisely. Section 7
conducts an extensive experimental evaluation of our methods, including both simulations
and an evaluation using real marketplace data from Amazon. Section 8 studies the literature. Section 9 concludes with a discussion and some directions for future work. Appendix A
presents proofs for all theorems.

2. Background on Probabilistic Trust Representation
This section introduces the key background on Wang and Singh’s (2007) approach that is
necessary for understanding our present contribution.
2.1 Probability-Certainty Distribution Function
Considering a binary event hr, si, where r and s represent the number of positive and
negative outcomes, respectively. Let x ∈ [0, 1] be the probability of a positive outcome.
Then the posterior probability of evidence hr, si is the conditional probability of x given
hr, si (Casella & Berger, 1990). The conditional probability of x given hr, si is
f (x|hr, si) =

R1

g(hr,si|x)f (x)
g(hr,si|x)f (x)dx

R1

xr (1−x)s
xr (1−x)s dx

0

=

0

224

A Probabilistic Approach for Maintaining Trust Based on Evidence



where g(hr, si|x) = 



r+s  r
x (1 − x)s .
r
PCDF vs r and s

6

5

r=20,s=4

certainty

4
r=8,s=2

3

r=4,s=1

2
r=0,s=0
1

0

0

0.1

0.2

0.3

0.4

0.5
0.6
probability

0.7

0.8

0.9

1.0

Figure 1: Examples of probability-certainty distribution functions, varying r and s.
Here f (x) is the probability distribution function of x, which is itself the probability
of a positive outcome. The signature
R 1 of f is given by f : [0, 1] 7→ [0, ∞). Because f
is a probability density, we have 0 f (x)dx = 1. Following Jøsang (2001), we interpret
the above as a probability of a probability or a probability-certainty distribution function
(PCDF). The probability that the probability
of a positive outcome lies in [x1 , x2 ] equals
R1
R x2
f
(x)dx
0
= 1. Figure 1 gives some examples of f (x)
1−0
x1 f (x)dx. The mean value of f is
for different numbers of positive and negative outcomes. Notice that when we have no
evidence (i.e., hr, si = h0, 0i), we obtain a uniform distribution. As the evidence mounts,
the distribution becomes more and more focused around its expected value.
As an aside, notice that although we consider integral values of r and s in the above
examples, the actual values of r and s would usually not be integral because of the effect
of discounting information received from others or remembered from past interactions. In
particular, it is possible that the total evidence is positive but less than one, i.e., 0 < r +s <
1.
2.2 Trust Representation
As in Jøsang’s approach, Wang and Singh’s model represents trust values in both the
evidence and the belief spaces. In evidence space, a trust value is in the form hr, si, where
r + s > 0. Here, r ≥ 0 is the number of positive experiences (that, say, agent Alice has with
agent Bob) and s ≥ 0 is the number of negative experiences she has with Bob. Both r and s
225

Wang, Hang, & Singh

r
are real numbers. Given hr, si, α = r+s
is the expected value of the probability of a positive
outcome when r + s > 0 and can be set as α = 0.5 when r + s = 0. In belief space, a trust
value is modeled as a triple of belief, disbelief, and uncertainty weights, hb, d, ui, where each
of b, d, and u is greater than 0 and b + d + u = 1. In intuitive terms, the certainty c = 1 − u
represents the confidence placed in the probability. Trust values can be translated between
the evidence and belief space.
Wang and Singh (2007) differ from Jøsang (2001) in their definition of certainty. Wang
and Singh’s definition is based on the following intuition. As Figure 1 shows for hr, si = h0, 0i
when we know nothing, f is a uniform distribution over probabilities x. That is, f (x) = 1 for
x ∈ [0, 1] and 0 elsewhere. This reflects the Bayesian intuition of assuming an equiprobable
prior. Intuitively, the uniform distribution has a certainty of 0. As additional knowledge is
acquired, the probability mass shifts so that f (x) is above 1 for some values of x and below
1 for other values of x. For the above reason, Wang and Singh (2007) define certainty to
be the area above the uniform distribution f (x) = 1.

Definition 1 The certainty based on evidence hr, si, is given by
c(r, s) =
=

R1

1
2
1
2

0

R1
0

|f (x) − 1|dx
r

s

| R 1(xr (1−x)s

x (1−x) dx

0

− 1|dx

Certainty

1

0

.

9

0

.

8

0

.

7

0

.

6

0

.

5

0

.

4

0

.

3

0

.

2

0

1

0

0

2

0

0

3

0

0

4

0

0

5

0

0

6

0

0

7

0

0

8

0

0

9

0

0

1

0

0

0

Total number of transactions while ratio of positive and negative is fixed

Figure 2: Certainty increases with mounting evidence provided the amount of conflict in
the evidence is held constant. The X-axis measures the total number of outcomes,
which are equally positive and negative.
Conflict in the evidence in this setting means that some evidence is positive and some
is negative. Thus conflict is maximized when r = s and is minimized when r or s is zero.
Wang and Singh (2007) prove that certainty increases when the total number of transactions
increases and the conflict is fixed, as in Figure 2. They also show that certainty decreases
when conflict increases and the total number of transactions is fixed, as in Figure 3.
226

A Probabilistic Approach for Maintaining Trust Based on Evidence

0

.

9

4

0

.

9

2

Certainty

0

.

9

0

.

8

8

0

.

8

6

0

.

8

4

0

.

8

0

2

.

8

0

.

7

8

0

.

7

6

0

.

7

4

0

1

0

2

0

3

0

4

0

5

0

6

0

7

0

8

0

9

0

1

0

0

Number of positive transactions while total number of transactions is fixed

Figure 3: Certainty decreases with increasing conflict provided the amount of evidence is
held constant. The X-axis measures the number of positive outcomes out of a
fixed total number of outcomes.

2.3 Trust Propagation
In real-life settings, an agent (a prospective client) may lack direct experience with another
agent (a prospective service provider) with whom it considers interacting. In this case,
the client can ask referrers for trust referrals. If a referrer lacks direct experience, it may
refer to other referrers, and so on. This is the essential idea behind referral networks. But
how should we calculate trust through referral networks? Many researchers have studied
trust propagation. In our chosen framework, Wang and Singh (2006) define mathematical
operators for propagating trust, which we can leverage for our present goals.
Wang and Singh (2006) provide a concatenation operator (similar to Jøsang’s, 1998,
recommendation operator) that enables a client C to compute how much trust it should
place in a service provider S based on its direct experience with a referrer R and a referral
for S provided by R. The idea is that, to compute its trust in S, C simply concatenates its
trust in R with R’s report about S. Definition 2 captures Wang and Singh’s concatenation
operator. In our setting, let MR = hrR , sR i be agent C’s trust in a referrer R. Here, cR
is the certainty determined from the above trust value. Further, let MS = hr′ , s′ i be R’s
report about its trust in a provider S. Then the amount of trust to be placed by C in S is
given by MR ⊗ MS .
Definition 2 Concatenation ⊗. Let MR = hbR , dR , uR i and MS = hb′ , d′ , u′ i be two trust
values. Then MR ⊗ MS = hbR b′ , bR d′ , 1 − bR b′ − bR d′ i.
To handle the situation where C collects trust information of S from more than one
source, we use Jøsang’s aggregation operator (Jøsang, 2001; Wang & Singh, 2007), which
simply sums the available evidence pro and con. C can use this operator to combine independent reports about the trust to place in S. Definition 3 captures the aggregation operator.
In our setting, Mi = hri , si i would be the trust that C would place in S based on exactly
227

Wang, Hang, & Singh

one path from C to S. When the paths are mutually independent, i.e., nonoverlapping, C’s
aggregate trust in S would be given M1 ⊕ . . . ⊕ Mk .
Definition 3 Aggregation ⊕. Let M1 = hr1 , s1 i and M2 = hr2 , s2 i be two trust values.
Then, M1 ⊕ M2 = hr1 + r2 , s1 + s2 i.

3. General Model for Updating Trust
As we observed above, most existing trust models do not provide a suitable trust update
method by which an agent may maintain the trust it places in another agent. In our model,
trust updates arise in two major settings, which we consolidate into a universal model for
trust update. These settings are as follows.
• Trust update for referrers, wherein an agent updates the trust it places in a referrer
based on how accurate its referrals are. This is a way for an agent to maintain its
social relationship with a referrer.
• Trust update by trust in history, wherein an agent updates the trust it places in a
service provider by tuning the relative weight (discount factor) assigned to the service
provider’s past behavior with respect to its current behavior. This is a way for an
agent to accommodate the dynamism of a service provider.
Target
Trust
!rR , sR "

R
Compare

C
Client

Estimated Trust
!r′ , s′ "

S
Actual Trust
!r, s"

Source

Figure 4: Schematic illustration of our generalized trust update approach. Throughout this
′
r
R
, α′ = r′ r+s′ , and αR = rRr+s
paper, we use α = r+s
R
Interestingly, our approach treats the above settings as variations on a common theme,
which we term our general model for updating trust. Figure 4 presents this model, which
summarizes a process consisting of the following steps based on a client C, a target R, and
a service provider S. The client C seeks to update the amount of trust hrR , sR i that C
places in a target R.
• C estimates R’s trustworthiness (before the update) as hrR , sR i.
• R reports S’s trustworthiness as hr′ , s′ i.
• S delivers an outcome from which C obtains direct information by which it can estimate the actual trustworthiness hr, si of S.
228

A Probabilistic Approach for Maintaining Trust Based on Evidence

• Using this information about the (apparent) trustworthiness of S, C determines the
accuracy of the estimated trust value it previously received from R. Based on this
′ , s′ i on empirical
measure of R’s accuracy, C updates the trust it places in R to hrR
R
grounds.
Trust update methods can be differentiated by how they compare the estimated and
actual trust values. The rest of this section discusses the general structure of trust update
and investigates some trust update methods along with their shortcomings. Section 4
introduces our preferred approach.
Let us follow the setting of Figure 4. The accuracy of the estimated trust value is defined
by the closeness of the estimated trust value hr′ , s′ i and the actual trust value hr, si. Instead
of interpreting each transaction (one estimation of trust from the target) as either good or
bad, we interpret it as q good and 1 − q bad transactions. Notice that “good” and “bad” in
the present context of a target providing estimates refers to the accuracy or otherwise of the
estimates with respect to the actual outcomes the client receives from the service provider.
Thus a target’s estimate could be good or bad independently of whether the actual service
outcome is good or bad.
Thus, q reflects how close the estimation hr′ , s′ i is to the actual trust hr, si. We require
0 ≤ q ≤ 1 ranging from a perfectly inaccurate to a perfectly accurate referral. The weight
we assign such an estimation should increase with its certainty. For example, suppose the
actual trustworthiness of S is h10, 0i. Say, one target RA estimates S’s trustworthiness
as h0, 1i and another target RB estimates it as h0, 100i. Both estimates agree that S
is not trustworthy, but RA claims much lower certainty (c(0, 1) = 0.25) than does RB
(c(0, 100) = 0.99). So RA should be punished less than RB if both estimates turn out to
be inaccurate. And, similarly, for rewarding them in case of accuracy. Therefore, instead
of treating each transaction as one transaction, we treat it as c′ transactions, where c′ < 1
is the certainty in the estimation. That is, we interpret each estimation as c′ q good and
c′ (1 − q) bad transactions.
In addition, we discount each past transaction by its age (Zacharia & Maes, 2000; ?,
?, ?, ?, ?). Now let hrR , sR i be the trust placed in R by C; hr′ , s′ i be an estimation with
′ , s′ i be the updated trust placed in R by C. Algorithm 1 presents
c′ = c(r′ , s′ ); and hrR
R
a modular specification for our generic trust update approach, highlighting its key inputs
and outputs. Let β be the temporal discount factor. Based on actual observations hr, si,
let q represent how accurate the estimation is and p represent how bad the estimation is.
The specific approaches that we consider below differ in how each computes its measure of
accuracy, q.
Note that we assume the client updates its trust in the referral after it has conducted
some transactions with the service provider, so r + s > 0 in this case. When hr, si = h0, 0i,
the client cannot update the trust according to our formulation, since we discount the update
by the certainty of hr, si and when hr, si = h0, 0i, the certainty is 0. It does not matter
if hrR , sR i = 0 in the update method. We can initialize hrR , sR i to be some predefined
number, for example, h1, 1i or h0, 0i. The initial value corresponds to the prior distribution
of the trust. The uniform distribution corresponds to initial setting of hrR , sR i at h0, 0i.
But we can also set any prior we deem fit for the system. In trust update for referrers, we
initialize hrR , sR i to h1, 1i (to suggest that the client is willing to consider a referral from
229

Wang, Hang, & Singh

Algorithm 1: generalUpdate: Abstract method to revise the trust placed in target
R.
input q, p, β, c′ , hrR , sR i;
δrR ← c′ q;
δsR ← c′ p;
′ ← δr + (1 − β)r ;
rR
R
R
′
sR ← δsR + (1 − β)sR ;
′ , s′ i;
return hrR
R
the referrer) and hr, si to h0, 0i (to suggest that the client has no prior experience with the
provider). But we perform a trust update only after the client has done some transactions
with the service provider (i.e., r+s > 0). In the trust in history setting, we initialize hrR , sR i
to h0.9, 0.1i (to suggest that the client trust its own past experience with little confidence)
and hr, si to h0, 0i (to suggest the client has no prior experience with the provider).
Now let us consider the certainty density function based on hr, si, which reflects the
actual trustworthiness of S. Here, f (x) is the probability (density) of the quality of service
provided by S being x.
xr (1 − x)s
f (x) = R 1
(1)
r (1 − x)s dx
x
0

This density maximizes at x = α meaning that S will most likely provide a service outcome
with a quality of α. Consider the probability density function based on the estimation
hr′ , s′ i, which is R’s estimate of S’s trustworthiness and reflects R’s assessment of the
quality S will provide. In other words, R expects that S will provide a service outcome
most likely with a quality of x = α′ .

4. Trust Update Based on Average Accuracy
Based on the above background, we now study the trust update problem systematically.
We analyze the shortcomings of a series of approaches, culminating in an approach that
yields the characteristics we desire.
4.1 Linear Update and Its Shortcomings
Linear is a common trust update method, which serves as a baseline for comparison. Linear
defines the accuracy as the absolute difference between the quality α′ of the estimated trust
value hr′ , s′ i and the quality α of the actual trust value hr, si. That is,
q = 1 − |α − α′ |

(2)

Using Equation 2, we construct two trust update methods, Linear-WS and Jøsang, by
inserting the q defined in Linear into the general trust update model described in Algorithm 1. Note that, Linear-WS and Jøsang use trust representations with their separate
definitions of certainty, thereby yielding different trust update methods. Linear-WS, as
shown in Algorithm 2, adopts Wang and Singh’s notion of certainty underlying trust (Definition 1), whereas Algorithm 3 depicts Jøsang trust update method, where the certainty
230

A Probabilistic Approach for Maintaining Trust Based on Evidence

′

′

′

′

c′ is defined as r′r+s+s′ +2 (Jøsang, 2001). Jøsang (1998) defines certainty as r′r+s+s′ +1 , which
yields little difference from a later work by Jøsang (2001) in trust update. Our motivation
for introducing Linear-WS is to help distinguish the benefit of our trust update methods
from the benefit of using Wang and Singh’s static trust representation. As we show below, Linear-WS (which combines Wang and Singh’s static model with a heuristic update)
performs worse than our proposed update methods, thereby establishing that the proposed
methods yield some benefits beyond the static model that they incorporate.
A shortcoming of Linear is that it does not consider certainty. Consider an agent who
reports h0.1, 0.1i about a service provider when it has little information about the provider
and reports h90, 10i later when it has gathered additional information. Suppose the service
provider’s quality of service is indeed 0.9. The the above agent should be accorded high
trust, since it reports correct trust value of the service provider with high certainty and
reports wrong trust value with low certainty. In other words, when updating the trust in
this agent, the second referral should be given more weight than the first one. However,
Linear treats both referrals as the same and thus ends up with a wrong updated trust value
of this agent.
Algorithm 2: Linear-WS: A trust update method to revise the trust placed in target
R.
input hr, si, hr′ , s′ i, β, hrR , sR i;
α←

r
r+s

α′ ←

r′
r′ + s′

q ← 1 − |α − α′ |
c′ ← c(r′ , s′ )
return generalUpdate(q, 1 − q, β, c′ , hrR , sR i);

Algorithm 3: Jøsang: A trust update method to revise trust placed in agent R.
input hr, si, hr′ , s′ i, β, hrR , sR i;
α←

r+1
r+s+2

α′ ←

r′ + 1
r′ + s′ + 2

q ← 1 − |α − α′ |
c′ ←

r′ + s′
r′ + s′ + 2

return generalUpdate(q, 1 − q, β, c′ , hrR , sR i);

231

Wang, Hang, & Singh

4.2 Update Based on Max-Certainty and Its Shortcomings
Hang, Wang, and Singh (2008) proposed the Max-Certainty trust update method, which
applies on Wang and Singh’s trust representation. Max-Certainty supports some interesting
features, but suffers from some shortcomings, which the present approach avoids. MaxCertainty follows the general update model of Section 3, and defines q as in Algorithm 4.
Algorithm 4: Max-Certainty: A trust update method to revise the trust placed in
target R.
input hr, si, hr′ , s′ i, β, hrR , sR i;
α←

r
r+s

α′ ←

r′
r′ + s′

q←

α′r (1 − α′ )s
αr (1 − α)s

c′ ← c(r′ , s′ )
return generalUpdate(q, 1 − q, β, c′ , hrR , sR i);
The intuition behind Algorithm 4’s definition of q is as follows. From Equation 1, we
′)
have that q = ff(α
(α) , since S will provide service most likely with a quality of α—and not
as likely at any other quality α′ . That is, q measures the ratio of the likelihoods that the
service provided by S has qualities α′ and α, respectively. In other words, our measure of
accuracy q is the ratio of the probability computed from the estimate from R with respect
to the probability computed from the measurement made by C itself. Figure 5 illustrates
this computation.
A malicious target (such as a referrer) in the present setting is one who is not just wrong
but is over-confident—that is, such an agent exaggerates the amount of evidence it claims
behind its report about a service provider. Although Max-Certainty tells us how likely it is
that a target R is trustworthy, it is not sensitive against malicious targets. Since combined
trust is naturally weighed by the amount of evidence, a trust report from a malicious target
may falsely dominate truthful reports based on an apparently smaller amount of evidence.
Under Max-Certainty, it takes C a long time to pinpoint a malicious target. For example, suppose the actual trustworthiness of S is h2, 1i and R reports h5, 5i. According to
Max-Certainty, hδrR , δsR i = h0.37, 0.07i. If R had reported trust in S of h1000, 1000i, then
hδrR , δSR i would equal h0.79, 0.15i. In other words, Max-Certainty treats the new evidence
as being confirmatory. Instead, we claim the report of h1000, 1000i is bad because it exaggerates the available evidence and thus has a highly misleading effect. In particular, it may
end up overriding many accurate reports (of lower certainty). Therefore, we consider the
report of h1000, 1000i to inaccurate, and treat its evidence as being disconfirmatory. This
observation leads to the following update approaches.
In the update formula based on Max-Certainty, let R’s trust of S be hr, si. When r + s
r
is large, the formula is highly sensitive, since the PCDF is maximized at x = α = r+s
, but
232

A Probabilistic Approach for Maintaining Trust Based on Evidence

3

.

5

.

5

3

c

e

r

p

2

t

a

r

i

o

n

b

t

y

a

d

b

i

l

e

i

t

n

y

s

a

i

t

y

t

o

0

.

f

8

1
t

−

y

q

i

s

2

ne

d

t

i

y

n

1

t

.

5

a

r

e

c

c

e

r

t

a

i

n

t

y

d

e

n

s

i

t

y

o

f

1

p

r

o

b

a

b

i

l

i

t

y

a

t

0

.

6

q

0

.

5

0

0

.

1

0

.

2

0

.

3

0

.

4

0

p

r

o

b

.

5

a

0

b

i

l

i

t

.

6

0

.

7

0

.

8

0

.

9

1

.

0

y

Figure 5: Illustration of the trust update method Max-Certainty with hr, si = h8, 2i and
hr′ , s′ i = h6, 4i.

decreases quickly when x deviates from α (in other words, close to x = α, the magnitude of
the slope is high). For example, let the (actual) trustworthiness of S be h800, 200i. When
a referrer reports h19, 6i, it predicts that the quality of S is 0.76. This is quite close to the
actual quality of S, namely, 0.80. However, Max-Certainty yields hδrR , δsR i = h0.06, 0.58i,
which indicates that Max-Certainty treats it to be a poor estimation.
4.3 Update Based on Sensitivity and Its Shortcomings
The foregoing leads us to another update method, which we term Sensitivity. The intuition underlying Sensitivity was identified by Teacy et al. (2006). To motivate Sensitivity,
consider that in R’s estimate, the probability that the quality of S equals p, is given by
′

l(p) = R 1
0

pr (1 − p)s

′

xr′ (1 − x)s′ dx

Clearly, l(p) maximizes at α′ , which means that R estimates the quality of the service
provided by S as most likely being α′ . If we normalize l(α′ ) to 1, then
′

q =

′

αr (1 − α)s
α′r′ (1 − α′ )s′

(3)

measures how likely in R’s assessment would the quality of the service provider be α.
Figure 6 illustrates the above formulas using hr, si = h8, 2i and hr′ , s′ i = h6, 4i. Returning to the previous example, hδrR , δsR i = h0.01, 0.93i, which means that Sensitivity
considers h1000, 1000i an inaccurate report—as it should.
Although Sensitivity improves over Max-Certainty, like Max-Certainty, it remains susceptible to being excessively sensitive when the number of transactions is large. A numerical
233

Wang, Hang, & Singh

3

2

.

5

c

c

e

r

t

a

i

n

t

y

d

e

n

s

i

t

y

a

t

0

.

e

r

t

a

i

n

t

y

d

e

n

s

i

t

y

a

t

0

.

8

6

2

t

y

i

s

1

−

q

ne

c

e

r

t

a

i

n

t

y

d

e

e

d

n

s

i

t

y

f

u

n

c

t

i

o

n

d

1

.

5

b

t

i

t

a

s

o

n

(

6

,

4

)

y

n

a

r

e

c

1

0

.

5

q

0

0

.

1

0

.

2

0

.

3

0

.

4

0

p

r

o

b

.

5

a

0

b

i

l

i

t

.

6

0

.

7

0

.

8

0

.

9

1

.

0

y

Figure 6: Illustration of the trust update method based on Sensitivity with hr, si = h8, 2i
and hr′ , s′ i = h6, 4i. Thus α = 0.8 and α′ = 0.6.

example of the susceptibility of Sensitivity is as presented in Section 4.2. Let the trustworthiness of S be h800, 200i, whereas the referrer reports h190, 60i. The above method, which
incorporates uncertainty, yields hδrR , δsR i = h0.24, 0.55i. In other words, it treats the referral as being bad. However, 190/(190 + 60) = 0.76 is quite close to 800/(800 + 200) = 0.80,
which indicates that it ought to be treated as a good referral—hence we have a discrepancy.
The problem with Sensitivity is that it treats a referral as being disconfirmed by evidence
simply because the referrer is too confident even though the referral is accurate. Theorem 3
of Section 5 demonstrates the above problem, in a general setting.
The bottom-line is that both Max-Certainty and Sensitivity produce undesirable results.
4.4 Average Accuracy Disregarding Uncertainty
The foregoing discussion leads us to our penultimate step in coming up with our desired
approach. This and the final approach (detailed in Section 4.5) both consider the average
accuracy of the estimations. The present variant is the simpler of the two because it
disregards the uncertainty inherent in the belief regarding the source S.
Suppose as an idealization that the actual trustworthiness of the source S equals hα, 1 −
α, 0i, when expressed as a belief-disbelief-uncertainty triple. This case arises when we know
for sure that the source can provide the quality of service or referral at probability α.
Therefore, the uncertainty is 0, indicating that this is an ideal case.
As Figure 7 shows, suppose the target reports a trust value about the service provider
of hr′ , s′ i. Let c′ = c(r′ , s′ ) be the certainty based on hr′ , s′ i. The PCDF for hr′ , s′ i means
the target estimates that the provider will produce a quality of x ∈ (0, 1) with certainty
f (x):
′

f (x) = R 1
0

xr (1 − x)s

′

xr′ (1 − x)s′ dx
234

A Probabilistic Approach for Maintaining Trust Based on Evidence

But the provider’s actual quality is x = α. The square of the estimation error at x is
(x − α)2 . We multiply the square of the error by its certainty f (x), and then integrate
it over x from 0 to 1 to obtain the average square of the estimation error. The square
root of this integration yields the average error. That is, we can calculate the error in the
estimation according to the following formula:
v
uR 1 ′
u xr (1 − x)s′ (x − α)2 dx
e = t 0 R1 ′
r
s′
0 x (1 − x) dx
3

certainty density

2.5

2

certainty density function
based on (6,4)
1.5

1

0.5

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

probability

Figure 7: Illustration of the average trust update method for r′ = 6 and s′ = 4, and α = 0.8
(dashed line). The error e is the average of the length of the arrows.
There are many ways to compute average errors, including the L1 and L∞ norms, and
so on. We use the L2 norm here for its simple mathematical properties. This choice is
not unique but is a common one (the same as variance) and is convenient to manipulate
mathematically.
We can now give an alternative definition of q based on e, i.e., q = 1 − e. From the
PCDF corresponding to the estimation hrR , sR i, we can see that q corresponds to the average
accuracy of the estimation. The updated estimate of trustworthiness hrR , sR i of R is based
on q in the usual manner of Algorithm 1.
4.5 Average Accuracy Incorporating Uncertainty
Let us now consider a more complex variant of the above method, which uses the same q
definition as the above, but explicitly incorporates the uncertainty inherent in the belief
regarding S. Treating the trust placed in S as a belief function with uncertainty corresponding to hr, si in evidence space, we would like to discount the updates to rR and sR by
an additional factor of the certainty of the actual observations hr, si made by C. In other
235

Wang, Hang, & Singh

Algorithm 5: Average-β : A trust update method to revise trust placed in agent R.
input hr, si, hr′ , s′ i, β, hrR , sR i;
α←

r
r+s

c ← c(r, s)
q =1−

s

(α −

r′

(r′ + 1)(s′ + 1)
r′ + 1 2
) + ′
′
+s +2
(r + s′ + 2)2 (r′ + s′ + 3)
c′ ← c(r′ , s′ )
c′ = cc′

return generalUpdate(q, 1 − q, β, c′ , hrR , sR i);

words, we would begin with the definition of q as above and discount it with the certainty
determined from hr, si. Algorithm 5 captures this intuition.
The reason we consider certainty is that when we are not certain of the actual quality
of S, we are not certain of how to evaluate the target’s estimation of S either, so we
discount the update by the additional factor c. Returning to our previous example, let the
trustworthiness of S be h800, 200i, whereas the target reports h19, 6i. The above method,
which incorporates uncertainty, yields hδrR , δsR i = h0.53, 0.06i, which indicates that the
above report is a good estimation—as it is supposed to be since 19/(19 + 6) = 0.76, which is
quite close to 800/(800 + 200) = 0.8. Therefore, when α′ is close to α, no matter how large
the total number transactions is, this method considers the target’s estimation as being
confirmative. The above holds true in general, as Theorem 4 of Section 5 shows.
4.6 Understanding the Trade Offs
The following tables illustrate the pros and cons of each update method. We compare the
accuracy measurements q used in Max-Certainty (Algorithm 4), Sensitivity (Equation 3),
and Average-β (Algorithm 5).
Table 1 summarizes the various situations of interest, especially those that Max-Certainty
and Sensitivity cannot handle well. As explained in Section 4.3, when r + s is large, MaxCertainty is highly sensitive, and thus treats a good report as bad. When r′ + s′ is large,
Sensitivity is highly sensitive, and also treats a good report as bad.
Table 2 provides numerical examples corresponding to the situations specified in Table 1.
In this table, let the actual quality of the service provider be 0.50 and the quality indicated
by a referral be 0.55.
4.7 Estimating Certainty
We now evaluate the above methods with respect to their ability to infer and track the
certainty of the incoming trust reports. Figure 8 compares the q values produced by different
trust update methods. Its Y -axis is q, as calculated by the different trust update methods
introduced above. The estimate’s quality α′ is fixed at 0.55 as r′ + s′ , the amount of
236

A Probabilistic Approach for Maintaining Trust Based on Evidence

Table 1: Comparing the effectiveness of trust update methods conceptually.
Case
r+s

r′

small

Accuracy
+

s′

Max-Certainty

Sensitivity

Linear

Average-β

small

good

good

fair

good

small

large

good

poor

good

good

large

small

poor

good

fair

good

large

large

poor

poor

good

good

Table 2: Trust update methods comparison via numerical examples.
Case
hr, si

hr′ , s′ i

h1, 1i

Accuracy (q)
Max-Certainty

Sensitivity

Linear

Average-β

h1.1, 0.9i

0.99

0.99

0.95

0.78

h1, 1i

h220, 180i

0.99

0.13

0.95

0.95

h200, 200i

h1.1, 0.9i

0.13

0.99

0.95

0.78

h200, 200i

h220, 180i

0.13

0.13

0.95

0.95

evidence in the estimate, increases. The left and right plots show the resulting q with the
actual quality hr, si = h1, 1i, and hr, si = h200, 200i, respectively. Linear is always high,
independent of the certainty of the report. Max-Certainty over-estimates q when r +s is low
and under-estimates q when r + s is high, and does not vary with certainty. Only Average’s
estimate of q reflects the certainty of the reports in both cases.
4.8 Methods Summarized
Figure 9 illustrates the trust update methods that we compare, including Jøsang, LinearWS, Max-Certainty, Sensitivity, and Average-β. We categorize these methods with respect
to their accuracy measurements and their underlying trust representation. Regarding the
accuracy measurement technique, Jøsang and Linear-WS measure accuracy based on the
linear approach. Each of Max-Certainty, Sensitivity, and Average-β defines its specific
accuracy measurement. All the approaches other than Jøsang follow Wang and Singh’s
trust representation.

5. Theoretical Evaluation of Accuracy Measurement Techniques
This section evaluates the above trust update methods in theoretical terms by consolidating
some important technical results. It may be skipped on a first reading. This section seeks
to give technical intuitions about its results.
237

Wang, Hang, & Singh

1
1

0.9
0.95

0.8

0.9

0.7
0.6

0.85

q

q

0.5

0.8

0.4
0.75

0.7

Average−β

0.3

Max−Certainty
Linear−WS

0.2

0.65

0

Average−β
Max−Certainty
Linear−WS

0.1

40

80

120

160

200

240

280

320

360

0
0

400

40

80

120

160

200

240

280

320

360

400

r′+s′

r′+s′

Figure 8: Comparison of trust update methods based on their accuracy measurements q.
These graphs use a fixed referral expected quality α′ = 0.55 and vary the amount
of reported evidence r′ + s′ . The actual quality values hr, si are low (set to h1, 1i
in the left graph) and high (set to h200, 200i in the right graph), respectively.

Linear-WS

Max-Certainty

Sensitivity

Average-β

Accuracy
Measurement

Linear

Max-Certainty

Sensitivity

Average

Trust
Representation

Jøsang

Jøsang

Wang & Singh

GeneralUpdate
Figure 9: Trust update methods specified in terms of their trust representation and their
accuracy measurement methods.

As Figure 9 shows, each trust update method has three main components. The accuracy
measurement technique is the main contribution of this paper, and the one we evaluate
theoretically.
5.1 Bounded Range
As explained above, the update q means that an estimate can be treated as q good and
1 − q bad transactions. The range being bounded merely serves as a sanity check on the
definitions. Theorem 1 establishes this for all of the accuracy measurement definitions that
we consider.
238

A Probabilistic Approach for Maintaining Trust Based on Evidence

Definition 4 Let a trust update method compute q based on the above description. We say
the trust update method is bounded if and only if, for all inputs,
0≤q≤1
Theorem 1 Each of the four definitions of accuracy q as given in Equation 2, Algorithm 4,
Algorithm 5, and Equation 3 satisfies boundedness.
Proof : The range is trivially bounded for Linear. For Max-Certainty and Sensitivity
r
methods, we show that the PCDF function, f (·) achieves its maximum at x = α = r+s
, so
(x)
is between 0 and 1. For Average, we first show that |x − α| is less than 1, and then
the ff (α)
show the rest of the integral is 1.
rR
1 r′
s′
2
0 xR (1−x) (x−α) dx
.
Specifically, all we need to show is that 0 ≤ e ≤ 1, where e = 1−q =
1 r′
s′
0

x (1−x) dx

Since (x − α) ≤ 1 when 0 ≤ x ≤ 1. Thus we obtain
R1

′

′

xr (1−x)s (x−α)2 dx
R1 ′
′
xr (1−x)s dx
R 10 r′
′
x (1−x)s dx
= 1.
≤ R01 r′
s′
0 x (1−x) dx
R 1 r′
′
x (1−x)s dx
Since R01 r′
=
s′
0 x (1−x) dx
0

1, we obtain

0 ≤ e ≤ 1.

2

5.2 Monotonicity
We now introduce an important property of trust updates, which we term monotonicity.
Monotonicity means that for a fixed trust estimate α, the farther the actual quality of the
service provider is from the quality predicted by the estimate the larger is the resulting
correction. Here, the correction corresponds inversely to q in Algorithm 1. In other words,
monotonicity means that if the error is greater, the correction due to trust update is larger
as well.
Definition 5 Let a trust update method compute q1 and q2 (corresponding to α = α1 and
α = α2 , respectively) based on the above description. We define such a method as being
monotonic if and only if when α1 < α2 < α or α < α2 < α1 , for some trust estimate
α ∈ (0, 1), we have
q 1 < q2
Theorem 2 establishes that all of the accuracy measurement definitions that we consider
here satisfy monotonicity.
Theorem 2 Each of the four definitions of accuracy q as given in Equation 2, Algorithm 4,
Equation 3, and Algorithm 5 satisfies monotonicity.
Proof : Linear is trivially seen to be monotonic. To prove that Max-Certainty and
Sensitivity are monotonic, we only need to show that the PCDF function is increasing when
r
r
) and decreasing when x ∈ ( r+s
, 1). We show this by showing that the derivative
x ∈ (0, r+s
239

Wang, Hang, & Singh

r
r
of the PCDF function is positive when x ∈ (0, r+s
) and negative when x ∈ ( r+s
, 1). To
r+1
prove that Average is monotonic, we use Theorem 5 and let c = r+s+2 .
More specifically, this theorem is equivalent to showing that q(α) is increasing when
′
′
0 < α < r′ r+s′ and decreasing when r′ r+s′ < α < 1, where q is defined in Equation 3.
′

By Definition 3, q(α) =

αr (1−α)s
d

′

′

′

′

′

, where d = ( r′ r+s′ )r ( r′ s+s′ )s . Hence,
r ′ αr

′ −1

′

′

′

(1−α)s −s′ αr (1−α)s −1
d
′
′
αr −1 (1−α)s −1 (r ′ (1−α)−s′ α)
d
′
′
αr −1 (1−α)s −1 (r ′ −α(r ′ +s′ ))
.
d

q ′ (α) =
=
=
′

′

Thus q ′ (α) > 0 when 0 < α < r′ r+s′ and q ′ (α) < 0 when r′ r+s′ < α < 1.
′
′
Hence q(α) is increasing when 0 < α < r′ r+s′ and decreasing when r′ r+s′ < α < 1.
To prove monotonicity for Max-Certainty, it is equivalent to show that q(α) is increasing
r
r
when 0 < α < r+s
and decreasing when r+s
< α < 1, where q is defined in Algorithm 4.
The rest of the proof is as above.
2

Calculated quality of estimate q

0.5
Average accuracy method
Sensitivity method
0.4

0.3

0.2

0.1

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Estimated probability α

Figure 10: Monotonicity of update methods illustrated: here, each trust update method
calculates quality of estimate q given a fixed trust estimate hr′ , s′ i = h2, 8i, and
α ∈ [0, 1.0].

Figure 10 shows that Average method and Sensitivity method satisfy this property. The
Max-Certainty method is the same as Sensitivity method except that it uses r, s instead of
r′ , s′ . Thus, the figure for Max-Certainty would be the same as Figure 10 provided we use
r = 2 and s = 8.
5.3 Sensitivity Problems
The property of sensitivity alludes to a problem that some trust update methods face.
The idea is that an overly sensitive update method creates unjustifiably large updates. As
a result, the trust being placed can oscillate rapidly, leading to near chaotic conditions.
240

A Probabilistic Approach for Maintaining Trust Based on Evidence

Following Figure 4, Definition 6 specifies what it means for an update method to be asymptotically sensitive. The intuition behind Definition 6 is that as the amount of evidence used
to assess the trustworthiness of a source S goes up, it causes potentially erratic updates
in the amount of trust placed in the target R. Notice that Definition 6 is an undesirable
property.
Definition 6 Let a trust update method compute q based on a referral hr′ , s′ i and actual
experience hr, si. We further assume that α 6= α′ . We define a trust update method as being
asymptotically sensitive if and only if when α and α′ are fixed, at least one of the following
holds:
lim
q=0
′
′
r +s →∞

lim q = 0

r+s→∞

Theorem 3 establishes that Max-Certainty and Sensitivity satisfy asymptotic sensitivity.
This means the above methods are susceptible to sensitivity problems as the amount of
evidence to judge a target increases.
Theorem 3 Algorithm 4 and Equation 3 satisfy asymptotic sensitivity.
Proof sketch: As for Max-Certainty method, let f (x) be the PCDF function. We want
r
, x 6= α. Then
to show that f (α) goes to infinity and f (x) goes to zero, where α = r+s
q=

f (α′ )
f (α)

goes to infinity when r + s goes to infinity and α′ 6= α.

1
Calculated quality of estimate

Calculated quality of estimate

0.5
0.4
0.3
0.2
0.1

0.8
0.6
0.4

50
100
150
Total number of transactions: r+s

200

(a) As r + s goes from 1 to 200, the quality
calculated by Max-Certainty falls.

Average Accuracy Method
Sensitivity method
Quality calculated by sensitivity method

0.2
0

0

Quality calculated by
average accuracy method

20
40
60
Total number of referred transactions: r’+s’

80

(b) As r′ + s′ goes from 1 to 200, the quality calculated by Sensitivity falls whereas
the quality as calculated by Average rises
slightly.

Figure 11: Evaluating update methods with respect to quality. In both graphs, α = 0.6,
α′ = 0.5.

241

Wang, Hang, & Singh

Figure 11(a) shows that Max-Certainty suffers from asymptotic sensitivity as the total
number of observations r+s becomes large. And, Figure 11(b) shows that Sensitivity suffers
asymptotic sensitivity as the total number of transactions in the estimate becomes large.
When α and α′ are fixed, both Average and Sensitivity depend on the total number of
transactions r′ +s′ . In Figure 11(b), the quality calculated by Sensitivity goes from 1 to 0 and
the quality calculated by Average goes from about 0.73 to about 0.90. This demonstrates
that Sensitivity suffers from asymptotic sensitivity since it treats a good estimate as a bad
one when r′ + s′ becomes large, whereas Average does not suffer from this problem.
Definition 7 captures the opposite intuition to sensitivity where the accuracy measurement q converges to the difference between the observed and reported probabilities. Theorem 4 shows that Average, in contrast with Max-Certainty and Sensitivity, is not susceptible
to sensitivity and is thus more robust than the above methods.
Definition 7 Let a trust update method compute q based on the above description. We
define such a method as being convergent if and only if for a fixed α, we have that
lim

r ′ +s′ →∞

q = 1 − |α − α′ |

Theorem 4 Following Figure 4, when α is fixed, the Average method is convergent.
Proof sketch: The convergence of Average follows naturally from Theorem 5, given
below.
5.4 Calculating Average Accuracy
The following formula shows how to calculate Average. The important feature of this
formula is that it is a closed form for calculating updates. It is exact and does not require
computing any integrals, which can be expensive to compute numerically. Hence, in the
computational respect too, this method is superior to Max-Certainty and Sensitivity.
Theorem 5 Let q be defined in Algorithm 5. Then
s
2
r′ + 1
(r′ + 1)(s′ + 1)
q =1−
α− ′
+ ′
′
r +s +2
(r + s′ + 2)2 (r′ + s′ + 3)
R1 ′
′
r!s!
Proof sketch: We need only to show that 0 xr (1 − x)s = (r+s+1)!
. We can accomplish this via integration by parts. The boundary terms are zeros. The details are in the
Appendix.
To summarize our technical results, we find that all the methods that we consider satisfy
the property of the accuracy measure lying within the range [0, 1]. Max-Certainty and
Sensitivity satisfy the undesirable property of asymptotic sensitivity, whereas Linear and
Average satisfy the opposite—desirable—property of converging toward the actual measure
of accuracy. The advantage of Average over Linear shows up with respect to its speed of
learning, which we demonstrate below through simulation studies.

6. Trust Update Scenarios
Now we discuss the two use case scenarios where we apply trust update.
242

A Probabilistic Approach for Maintaining Trust Based on Evidence

6.1 Trust Update for Referrers
Existing trust models lack update methods for an agent to update the extent of trust it
places in a referrer, based on the referrals the referrer gives. In general, the trustworthiness
of a referrer is best estimated based on how accurate its referrals are.
Referrer
Client’s trust in Referrer
MR = !rR , sR "

R

Referrer’s referral about S
MS = !r′ , s′ "

Compare

C
Client

S
Client’s actual experiences
Service Provider
M = !r, s"

Figure 12: Illustration of trust update for referrers.
The accuracy is determined by comparing the referrals with the observed trustworthiness
of the source, as illustrated in Figure 12. This process mirrors exactly the process described
for Figure 4, but with the target now being a referrer.
6.2 Trust Combination
Referrers
Client’s trust in Referrer

R1

Referrer’s referral about S

R2
C
Client

S
R3

Service Provider

Client’s actual experiences

Figure 13: Illustration of trust combination.
After client C has determined the amount of trust it places in the service provider S
based on referrals from each of the referrers Ri , C consolidates the trust estimates using
the propagation operators introduced in Section 2.3. Figure 13 shows a situation with one
client, three referrers, and one service provider. C predicts the trustworthiness of the service
provider based on the information received from the three referrers. C would make such
a prediction when selecting a service provider—typically, before it has obtained sufficient
direct experience with S.
• C uses the concatenation operator to discount the trust report received from each
referrer according to C’s trust in that referrer.
• C uses the aggregation operator to combine the discounted trust reports. The combined trust report yields C’s estimated trust in the service provider.
243

Wang, Hang, & Singh

6.3 Trust Update by Trust in History
To accommodate updating the trust placed in a service provider, we introduce the idea of
trust in history. We can imagine a “ghost” target reflecting the client’s previous level of
trust in a specific service provider. The ghost target, in essence, estimates the outcome to
be obtained from the service provider. Based on this estimate (and others), the client may
estimate its trust in that provider. The client evaluates this ghost target on par with any
real referrer.
History
Trust in History
MR = !rR , sR "

R

Past Behavior
MS = !r′ , s′ "

Compare

C
Client

S
Current Behavior
M = !r, s"

Service Provider

Figure 14: Illustration of trust update by trust in history.
Figure 14 illustrates this scenario as an instantiation of our general model of trust
updates, from Section 3. The essential idea is that the client tries to estimate how much
trust to place in the past information about a provider. In this manner, we can avoid hardcoding a discount factor to weigh the past information. Instead, the trust placed in history
serves as a dynamically computed discount factor that tells us how much to weigh the past.
Algorithm 6 describes a method, Average-α, which calculates the discount factor dynamically from trust in history. Average-α is similar to Average-β. The main difference between
the two is that whereas Average-β applies to the referrers setting, Average-α applies to the
trust in history setting (both of these settings are introduced in Section 3).
In Algorithm 6, first, we compare the current behavior hr′ , s′ i with the past behavior
hr, si to determine how consistent the behavior of the provider S is. If the current behavior hr′ , s′ i is close to the past behavior hr, si, the trust in history increases; otherwise,
it decreases. The closeness is measured by the method averageAccuracy, as defined in Algorithm 5. The trust placed in history, hrR , sR i, reflects how static the behavior of S is.
Thus, the probability α of trust in history can be used as the discount factor β, which is
high when the new behavior is consistent with the past but low when it is not. Here, we
initially set hrR , sR i to h0.9, 0.1i, i.e., the client trusts its past experience with small amount
of confidence. We initially set hr, si to h0, 0i, and we do trust update once the client has
done some transactions with the service provider.
A key point of distinction of our approach is that the trust placed in the history hrR , sR i
is not a fixed discount factor; it is based on how much the history matches the subsequent
transactions. If the source’s behavior changes a lot and cannot be accurately predicted from
the history, then the trust placed in the history becomes low, and the historical information
is consequently discounted to a greater extent. As a result, the net past evidence that is
brought to bear on a prediction goes down in addition to that evidence including more
conflict than it would otherwise. Thus the certainty of the resulting prediction is lower
than when the new information agrees with the past.
244

A Probabilistic Approach for Maintaining Trust Based on Evidence

Algorithm 6: Average-α: Yields a discount factor based on C’s prior experiences
with S.
input hr, si, hr′ , s′ i, hrR , sR i;
α←

r
r+s

c ← c(r, s)
c′ ← c(r′ , s′ )
v

uR 1 ′
u xr (1 − x)s′ (x − α)2 dx

q ← 1 − t 0 R 1 ′
r
s′
0 x (1 − x) dx


′
rR
← rR + cc′ (1 − q)

s′R ← sR + cc′ q
β←

′
rR
′ + s′
rR
R

rT ← r + βr′
sT ← s + βs′
return hrT , sT i;

7. Experimental Evaluation
We now evaluate our approach via simulations to supplement our theoretical analysis. We
consider the following main hypotheses in this study.
Hypothesis 1: Effectiveness Average with trust in history is no worse at prediction than
existing approaches for a variety of possible behaviors of service providers (Section 7.2,
Section 7.3, Section 7.4, and Section 7.5).
Hypothesis 2: No tuning Average with trust in history can offer accuracy similar to the
traditional approaches without requiring any tuning of parameters (Section 7.4 and
Section 7.5).
Hypothesis 3: Dynamism detection The certainty computed by Average with trust in
history reflects the dynamism of service providers. (Section 7.4).
We divide our simulations into two parts. The first part evaluates the effectiveness
of our trust update method. Section 7.2 compares our approach with three other models
in predicting behavior based on the estimated trustworthiness of referrers. Section 7.3
shows how the trustworthiness estimated by our approach identifies honest from malicious
referrers, and further yields accurate reports regarding the service providers.
The second part of our simulation shows the benefits of using trust in history. Section 7.4
compares trust update with and without trust in history in predicting behavior of different
245

Wang, Hang, & Singh

profiles. Section 7.5 shows the effectiveness of trust in history on a real dataset from Amazon
Marketplace.
We begin in Section 7.1 by introducing some behavior profiles and accuracy metric
throughout our evaluation.
7.1 Behavior Profiles and Accuracy Metrics
We conduct simulation studies to evaluate our trust update method. To this end, we
introduce some interesting behavior profiles for providers to capture a variety of situations
that can arise in practice. A profile simply means a formal characterization of the behavior
of a type of agent. We use the agent profiles to evaluate the effectiveness of the approaches
against different kinds of agents.
Table 3: Behavior tracking of different behavior profiles used in Sections 7.2 and 7.4.
Profile

Example

Probability

Amazon ratings

Periodic

Restaurant
(lunch
and
dinner)

Behavior Function Xt


1.0 90%
 0.0 10%


1.0 (⌊t/2⌋ mod 2) ≡ 1
 0.0 otherwise

Damping

Scam artist




Random

Stock market

U (0, 1)

Random Walk

Flight
price

ticket

Xt−1 + γU (−1, 1)

Momentum

Flight
price

ticket

Xt−1 + γU (−1, 1) + ψ[Xt−1 − Xt−2 ]

1.0 if t ≤ T /2
 0.0 otherwise

We include the following behavior profiles in our study. Table 3 summarizes these
profiles and Figure 15 shows the resulting behaviors schematically. To define the profiles
formally, we introduce Xt , a behavior function to represent the probability of providing a
good service at timestep t. Here, U (−1, 1) represents the uniform distribution over [−1, 1].
The parameters ψ and γ are real numbers between 0 and 1. At each timestep t, we calculate
Xt first, and next determine the quality of service based on Xt .
• Probability captures providers such as a travel agency or a seller on Amazon Marketplace. For instance, a travel agency might be able to fulfill a passenger’s request for
a pleasure trip booking with a certain probability.
• Random emulates a totally unpredictable service.
246

A Probabilistic Approach for Maintaining Trust Based on Evidence

Quality

1

Quality

1

0

0

1

2

3

4

5

6

7

8

9

10

1

2

3

4

Timestep

5

6

7

8

9

10

8

9

10

8

9

10

Timestep

(a) Probability

(b) Periodic

Quality

1

Quality

1

0

0

1

2

3

4

5

6

7

8

9

10

1

2

3

4

Timestep

5

6

7

Timestep

(c) Damping

(d) Random

Quality

1

Quality

1

0

0

1

2

3

4

5

6

7

8

9

10

1

Timestep

2

3

4

5

6

7

Timestep

(e) Random Walk

(f) Momentum

Figure 15: Behavior profiles shown schematically.
• Periodic describes a service that changes behavior regularly. For example, a restaurant
may employ experienced waiters for dinner, and novices for lunch.
• Damping models agents who turn bad after building up their reputation.
• Random Walk generalizes over providers whose current behavior depends highly on
their immediately previous behavior. For example, the quality of service provided by
a hotel would depend upon its recent investments in infrastructure and staff training;
thus the next quality of service would show a dependence on the previous quality of
service.
• Momentum is similar to Random Walk except that its current behavior depends highly
on two immediately previous steps.
247

Wang, Hang, & Singh

Among these profiles, Probability, Damping, and Random Walk yield more predictable behaviors than the others because their next outcome relates closely to the previous outcome.
Conversely, Random, Periodic, and Momentum are less predictable.
We introduce average prediction error E as a measure of the effectiveness of an update
method. The idea is that an update method makes a prediction at each timestep and we
compare this prediction with the trustworthiness as observed by the client.
Definition 8 Let hrt′ , s′t i and hrt , st i be the predicted and observed behaviors at timestep t.
Define αt′ and αt as usual. Then, the average prediction error E over a total of T timesteps
equals:
PT
|α′ − αt |
E = t=1 t
T
7.2 Predicting Referrers of Different Behavior Profiles
We conduct a simulation study to demonstrate the effectiveness of our trust update method.
As Table 4 shows, this simulation includes a study of Average-β (our approach with fixed
discount factor β) along with three other trust models: Max-Certainty, Linear-WS, and
Jøsang.
Table 4: Trust update methods compared in Section 7.2.
Update Method

Description

Static Model

Linear-WS (Algorithm 2)

Linear with fixed β

Wang and Singh

Jøsang (Algorithm 3)

Linear with fixed β

Jøsang

Max-Certainty (Algorithm 4)

Max-Certainty with fixed β

Wang and Singh

Average-β (Algorithm 5)

Average with fixed β

Wang and Singh

In this experiment, there are 100 timesteps, in each of which the client conducts 50
transactions with the service provider. For concreteness, in our simulations, we set hrR , sR i
and hr, si to h1, 1i and h0, 0i in this study. This initial value reflects the intuition that a
client might place little trust in a stranger (as a referrer), and has no knowledge of the
service provider.
The first simulation compares the Average-β trust update method with other methods.
In this simulation, there is one client C and one service provider S. At each timestep, C
obtains a referral from a referrer R about S, and itself performs 50 transactions with S.
Using the various trust update methods, C updates its estimate of the trustworthiness of
R based on comparing the referral R gave with C’s actual experience. The behavior of the
referrer R is defined using each of the above profiles. Note that using different randomness,
and γ and ψ of Random Walk and Momentum in generating behavior of each profile yields
similar results. Here we only show one example of a particular set of profile parameters.
Figure 16 shows the average prediction error of all trust update methods with discount
factors of β = 0.00, 0.01, . . . , 1.00 for all behavior profiles. For example, for the more
predictable profiles such as Probability, Damping, Random Walk, and Momentum, a high
248

A Probabilistic Approach for Maintaining Trust Based on Evidence

1

1

0.9

0.9

0.8

0.8
Average−β
Max−Certainty
Linear−WS
Josang

0.6

0.7
Prediction error

Prediction error

0.7

0.5
0.4

0.6
0.5
0.4

0.3

0.3

0.2

0.2

0.1
0
0

Average−β
Max−Certainty
Linear−WS
Josang

0.1
0.1

0.2

0.3

0.4

0.5
β

0.6

0.7

0.8

0.9

0
0

1

0.1

0.2

(a) Probability

0.8

0.7

0.8

0.9

1

0.8

0.9

1

0.8

0.9

1

0.7
Prediction error

Prediction error

0.6

Average−β
Max−Certainty
Linear−WS
Josang

0.9

0.7
0.6
0.5
0.4

0.6
0.5
0.4

0.3

0.3

0.2

0.2

0.1

0.1
0.1

0.2

0.3

0.4

0.5
β

0.6

0.7

0.8

0.9

0
0

1

0.1

0.2

(c) Damping

0.3

0.4

0.5
β

0.6

0.7

(d) Random

1

1
Average−β
Max−Certainty
Linear−WS
Josang

0.9
0.8

Average−β
Max−Certainty
Linear−WS
Josang

0.9
0.8
0.7
Prediction error

0.7
Prediction error

0.5
β

1
Average−β
Max−Certainty
Linear−WS
Josang

0.8

0.6
0.5
0.4

0.6
0.5
0.4

0.3

0.3

0.2

0.2

0.1
0
0

0.4

(b) Periodic

1
0.9

0
0

0.3

0.1
0.1

0.2

0.3

0.4

0.5
β

0.6

0.7

0.8

0.9

0
0

1

(e) Random Walk

0.1

0.2

0.3

0.4

0.5
β

0.6

0.7

(f) Momentum

Figure 16: Prediction errors with various discount factors. Although Average-β does not
dominate in all graphs, it yields competitive results, whereas most of the others
fail in at least some of the cases.

discount factor β yields better predictions because it focuses on recent results, even though
it sacrifices a large amount of evidence. Conversely, for the less predictable profiles such
as Periodic and Random, it is difficult to determine the best discount factor, because it
appears to depend on extraneous factors such as the random seed chosen.
249

Wang, Hang, & Singh

1

1

0.9

0.9

0.8

0.8

0.7

0.7
Trust in referrer

Trust in referrer

Recall that Average is the only one of the update methods that considers certainty.
However, this deficiency of Max-Certainty and Linear is overcome by multiplying certainty
in hδrR , δsR i. As a result, the difference in average prediction errors is not significant.
Section 4.6 describes some cases where Max-Certainty fails to predicate accurately. In
the context of the Probability and Random profiles, when the probability-certainty density
distribution is steep (indicating strong evidence), a small difference between α based on the
observed trustworthiness and α′ based on the referral can yield a significant punishment in
Max-Certainty.
To further highlight the advantages of Average-β, we create two new profiles: Rumor
and Honest. A Rumor referrer provides accurate reports first but exaggerates the evidence
in the second half of the simulation. Suppose the actual experience is h4, 1i. An exaggerated
referral might then be, for example, h40, 10i. An Honest referrer provides referrals whose
strength depends on its experience. It can accommodate the situation such as at the
beginning when it may not have sufficient experience with a provider: an Honest referrer
would provide neutral referrals with low certainty.
Figure 17 shows the estimated trustworthiness, respectively, of a referrer following the
Rumor and a referrer following the Honest profile. A Rumor referrer provides fair referrals
at the beginning and begins to exaggerate later in the simulation. An Honest referrer has
provides fair referrals throughout: with little certainty at the beginning and generally with
greater certainty later in the simulation. For Rumor, Average-β detects the exaggeration
and lowers the trust placed in the referrer accordingly. However, other approaches are
not sensitive to this exaggeration. For Honest, Average-β does not punish the referrer
even though the referral is inaccurate at the beginning. Once the referrer gathers enough
experience and provides good reports, the trust placed in it is built up accordingly. MaxCertainty suffers in this case, because, as we discussed in Section 4.6, it punishes Honest
for its reports turning out to be false.

0.6
0.5
0.4
0.3

0.1
0

0

10

20

30

40

50
60
Timestep

0.5
0.4
0.3

Average−β
Max−Certainty
Linear−WS
Josang

0.2

0.6

Average−β
Max−Certainty
Linear−WS
Josang

0.2
0.1
70

80

90

0

100

(a) Rumor

0

10

20

30

40

50
60
Timestep

70

80

90

100

(b) Honest

Figure 17: Trust (with discount factor β = 0.2) placed in Rumor (exaggerates after timestep
50) and Honest (has little knowledge before timestep 50) profiles over time. The
result shows our approach Average-β (a) punishes against exaggeration (later
in the simulation) and (b) stays neutral about evidence with low confidence (at
the beginning of the simulation).

250

A Probabilistic Approach for Maintaining Trust Based on Evidence

Summary
The foregoing shows that Average is effective in evaluating the trustworthiness of referrers
of various behavior profiles. Average provides competitive predictions against all behavior
profiles, whereas other approaches either suffer in some of the profiles or fail to provide
accurate predictions. Hence, we conclude that the above supports Hypothesis 1: Effectiveness.
7.3 Identifying Robust and Malicious Referrers
Referrers might not be honest or cooperative. This simulation verifies that even if some
referrers maliciously provide trust reports indicating a falsely exaggerated amount of evidence, as long as the client can access a few good referrers, it can obtain a good overall
estimate of the trustworthiness of the service provider. This experiment involves one client
agent, one service provider, and two referrers: one good throughout and one who is good
for the first 50 timesteps and then turns bad.
0.95
Actual trust of service provider: 0.9
0.9
Client’s estimated trust
of service provider

Trust

0.85

0.8

0.75

0.7
0

10

20

30

40

50 60
Time

70

80

90 100

Figure 18: Estimated versus actual trustworthiness of a service provider based on referrals
from two referrers, one good throughout and one who is compromised midway.

Figure 18 shows a case where, with just one good referrer to counterbalance one malicious referrer, the client can predict the trustworthiness of the service provider accurately.
The service provider offers a good outcome with a probability of 0.90 at all times. The
trustworthiness estimated for the service provider is the same as its actual trustworthiness
until the 50th timestep, when one of the referrers turns bad, as a result of which the estimate
drops down to about 0.71. The estimate returns to about 0.88 in about five timesteps and
increases slowly back to 0.90.
Figure 19 shows the amount of trust placed in the good and the corrupted referrers.
The trust placed in the good referrer begins at about 0.90 and then increases to nearly 1.
The trust placed in the corrupted referrer begins from about 0.90 and reaches about 0.98
at the 50th timestep, then drops quickly to 0.15 in about ten timesteps. This drop in trust
in the corrupted referrer is key to the return to accuracy of the overall assessment of the
service provider.
251

Wang, Hang, & Singh

1
Trust placed in good referrer
0.8

Trust

0.6

0.4

0.2
Trust placed in damping referrer
0

10

20

30

40

50 60
Time

70

80

90

100

Figure 19: Trust placed in the good referrer versus trust placed in the corrupted referrer.
Summary
The foregoing shows that Average is effective in identifying malicious referrers. Besides,
Average provides accurate predictions despite of erroneous reports from malicious referrers.
Hence, we conclude that the above supports Hypothesis 1: Effectiveness.
7.4 Predicting Agents of Different Behavior Profiles using Trust in History
Now we evaluate the effectiveness of trust in history in tracking different behaving agents. In
this simulation, there is one client C and one service provider S. After each 100 timesteps,
C performs 50 transactions with S. The behavior of the service provider S is defined
using the profiles in Table 3. Using three different approaches, we update the estimate of
trustworthiness of S and predict its future behavior based on that estimate. In this scenario,
we set hrR , sR i (in Definition 8) to h0.9, 0.1i in this study. This initial value reflects the
intuition that whereas a client would place a fair amount of trust in its own past experience
(as history), it might place little trust in a stranger (as a referrer in Section 7.2).
Table 5: Trust update methods compared in Section 7.4.
Update Method

Description

Static Model

Amazon

No discount

Wang and Singh

Average-β (Algorithm 5)

Discount past with fixed β

Wang and Singh

Average-α (Algorithm 6)

Discount past with trust in history

Wang and Singh

Table 5 shows the three approaches compared in this simulation. These approaches
discount past experience differently. Amazon, like marketplace web sites such as Amazon and eBay, retains all past experience; Average-β decays the past information with a
fixed discount factor β; and, Average-α decays the past information with a dynamically
computed trust in history. For example, suppose C has h40, 10i and h25, 25i in first two
timesteps. Amazon estimates the trustworthiness as h40 + 25, 10 + 25i. Average-β yields
252

A Probabilistic Approach for Maintaining Trust Based on Evidence

h40β + 25, 10β + 25i, where β is a fixed value in [0, 1]. We can see Amazon is a special case
of Average-β when β = 0. Average-α uses an adaptive discount factor based on trust in
history. Figure 20 shows the average prediction error of all methods with discount factor
β = 0.00, 0.01, . . . , 1 for all behavior profiles defined in Table 3.

1

1
Amazon
Average−β
Average−α

0.8

0.8

0.7

0.7

0.6
0.5
0.4

0.6
0.5
0.4

0.3

0.3

0.2

0.2

0.1

0.1

0
0

0.1

0.2

0.3

0.4

0.5
β

0.6

0.7

0.8

0.9

Amazon
Average−β
Average−α

0.9

Prediction error

Prediction error

0.9

0
0

1

0.1

0.2

(a) Probability
1

0.8

0.7

0.7
Prediction error

Prediction error

0.6

0.7

0.8

0.9

1

0.6
0.5
0.4

0.6
0.5
0.4

0.3

0.3

0.2

0.2

0.1

0.1
0.1

0.2

0.3

0.4

0.5
β

0.6

0.7

0.8

0.9

Amazon
Average−β
Average−α

0.9

0
0

1

0.1

0.2

(c) Damping

0.3

0.4

0.5
β

0.6

0.7

0.8

0.9

1

(d) Random

1

1
Amazon
Average−β
Average−α

0.9
0.8

0.8

0.7

0.7

0.6
0.5
0.4

0.6
0.5
0.4

0.3

0.3

0.2

0.2

0.1

0.1
0.1

0.2

0.3

0.4

0.5
β

0.6

0.7

0.8

0.9

Amazon
Average−β
Average−α

0.9

Prediction error

Prediction error

0.5
β

1
Amazon
Average−β
Average−α

0.8

0
0

0.4

(b) Periodic

0.9

0
0

0.3

0
0

1

(e) Random Walk

0.1

0.2

0.3

0.4

0.5
β

0.6

0.7

0.8

0.9

1

(f) Momentum

Figure 20: Prediction error with various discount factors (lower is better). In each graph,
Amazon and Average-α yield horizontal lines since they do not take a discount
factor as input.

253

Wang, Hang, & Singh

For Probability, Average-α dominates all Average-β with different values of β. Recall
that Probability performs poorly once in a while. When that happens, Average-α adapts
dynamically by increasing the discount factor. Subsequently, Average-α adjusts the discount
factor back to a lower value once the behavior becomes more predictable. This adjustment
yields better predictions than any fixed discount factor. Note that Probability yields behavior quite similar to real-life agents. An example of these are sellers on Amazon Marketplace,
which we discuss in Section 7.5.
As we observed above, some profiles are more predictable than others. For example, for
Damping, Random Walk, and Momentum, the next outcome significantly depends upon the
previous outcomes. In these cases, discounting old information more by using a high β value
yields predictions of improved accuracy. However, although Average-β with high discount
factors provides highly accurate predictions, the high discount factors yield low certainty
because the concomitant tendency to consider only the most recent evidence results in
reduced evidence, as shown in Figure 21.
Other profiles, especially Random and Periodic, are less predictable. For Random, we
need overall information (high β) to make the best prediction. For Periodic, higher and
lower β values yield a lower error than β values from the middle, although the error is
unacceptably high: the error exceeds 0.50. Note that Periodic changes its quality back and
forth every two timesteps. Using β = 1 yields perfect prediction when Periodic stays the
same but the worst error in the immediately following timestep (because of the alternation
of Periodic). Conversely, using β = 0 considers the overall behavior: it predicts 0.50 all
the time except during the initial several warmup timesteps. All β values and α yields an
error close to 0.50. Periodic is the only case where the behavior cannot be predicted by
all approaches. However, Periodic is not realistic in practical cases because a provider who
followed it would not gain much utility—Average-α can detect its unstable behavior and
places low trust in history thus yielding low certainty. The client C can react against it
accordingly.
Figure 21 compares the certainty of Amazon, Average-α, and Average-β with respect to
the β that yields the best trust prediction. Recall that certainty reflects two facts: (1) the
amount of evidence collected, and (2) the conflict in the evidence. In the first half of the
experiment, there is no conflict with the client’s own observations. Therefore, for example,
in Damping, the certainty of Average-α goes up as the history discount decreases (and
as the evidence increases). When the referrer turns bad in the middle of the experiment,
the certainty drops dramatically, partly because of the conflict in the evidence and partly
because, by using a high discount factor to discount the old evidence significantly, Averageα in essence reduces the amount of evidence it considers. Conversely, the certainty of
Average-β is fixed because of the fixed discount factor, except when the conflict occurs in
the middle, and its certainty falls down briefly. For profiles such as Random and Periodic,
Average-α yields lower certainty than Average-β, because the behavior is unpredictable.
The low certainty of trust prediction can guide the client C not to interact with the target
because its behavior is unpredictable.
254

1

1

0.9

0.9

0.8

0.8
Certainty of trust in provider

Certainty of trust in provider

A Probabilistic Approach for Maintaining Trust Based on Evidence

0.7
0.6
0.5
0.4
0.3
Amazon
Average−β
Average−α

0.2
0.1
0

0

10

20

30

40

50
60
Timestep

0.7
0.6
0.5
0.4
0.3

Amazon
Average−β
Average−α

0.2
0.1
70

80

90

0

100

0

10

20

1

1

0.9

0.9

0.8

0.8

0.7
0.6
0.5
Amazon
Average−β
Average−α

0.4
0.3

30

40

50
60
Timestep

70

80

90

0

100

1
0.9

0.8

0.8
Certainty of trust in provider

Certainty of trust in provider

1

0.7
0.6
0.5
0.4
0.3

0

Amazon
Average−β
Average−α

0

10

20

30

40

50
60
Timestep

70

80

90

100

80

90

100

Amazon
Average−β
Average−α

0

10

20

30

40

50
60
Timestep

(d) Random

0.9

0.1

100

0.3

(c) Damping

0.2

90

0.4

0.1
20

80

0.5

0.2

10

70

0.6

0.1
0

50
60
Timestep

0.7

0.2

0

40

(b) Periodic

Certainty of trust in provider

Certainty of trust in provider

(a) Probability

30

0.7
0.6
0.5
0.4
0.3

Amazon
Average−β
Average−α

0.2
0.1

70

80

90

0

100

(e) Random Walk

0

10

20

30

40

50
60
Timestep

70

(f) Momentum

Figure 21: Certainty of trust prediction with various discount approaches. Each graph
shows the certainty values of Amazon, Average-β (choosing the β that yields the
most accurate prediction), and Average-α approaches.

Summary
The foregoing shows that Average with trust in history is effective in tracking various
dynamic behaviors. Average with trust in history provides competitive predictions without
255

Wang, Hang, & Singh

prior knowledge of the behaviors and tuning any parameter. Besides, the certainty of
Average with trust in history can be served as an indicator of the dynamism of the behavior.
Hence, we conclude that the above supports Hypothesis 1: Effectiveness; Hypothesis 2: No
tuning; and Hypothesis 3: Dynamism detection.
7.5 Predicting Amazon Marketplace Data using Trust in History
In order to evaluate the effectiveness of our method in predicting data from the real world,
we studied manually collected feedback profiles of five sellers from Amazon. These sellers
obtained 60, 107, 180, 235, and 452 feedbacks, respectively. Each feedback is an integer
from 1 to 5. We treat each feedback as equaling the quality of service the seller provided
during the rated transaction. More precisely, we normalize the rating from {1, 2, 3, 4, 5} to
{0, 0.25, 0.5, 0.75, 1} and treat each normalized rating as the probability obtained from ten
transactions. For example, the rating 5 is translated to h10, 0i and the rating 2 is translated
to h2.5, 7.5i. At each timestep, the current feedback can be predicted by using the feedbacks
in the past. For example, consider a seller who receives feedbacks 3, 1, 2, and 4, respectively,
in the first four timesteps. We can use these feedbacks as a basis for predicting its next
feedback. In a simple approach, as supported by Amazon, we can use the average of these
feedbacks to predict the fifth feedback. In the above example, we would predict a feedback
of (3 + 1 + 2 + 4)/4 = 2.50. Suppose at the fifth timestep, the seller actually receives a
feedback of 3. Thus our prediction error would be 0.50. The above approach weighs each
feedback given in the past equally.
0

.

0

4

.

1

4

H

0

.

3

i

s

t

o

r

y

d

i

s

c

o

u

n

t

f

a

c

t

o

r

9

a

r

i

e

s

f

r

o

m

t

0

o

1

.

0

v

A

e

r

a

g

e

p

r

e

d

i

c

t

i

o

n

e

r

r

o

r

v

0

.

3

8

u

0

.

3

7

0

.

3

6

0

.

3

5

s

i

n

g

t

r

u

s

t

i

n

h

i

s

t

o

r

y

.

r

o

r

r

e

F

o

r

f

i

x

e

d

h

i

s

t

o

r

y

d

i

s

c

o

u

n

t

a

p

p

r

o

a

c

h

,

e

g

t

h

e

a

e

r

a

g

e

p

r

e

d

i

c

t

i

o

n

e

r

r

o

r

i

s

a

t

m

i

n

i

m

u

m

v

a

r

h

e

n

d

i

s

c

o

u

n

t

r

a

t

e

=

w

0

.

8

2

,

a

e

r

a

g

e

e

r

r

o

r

i

s

0

.

3

5

3

9

v

e

v

a

0

.

3

4

0

.

3

3

0

.

3

2

0

.

3

1

0

.

3

0

0

0

.

1

0

.

2

0

.

3

0

.

h

4

0

i

s

t

o

r

y

d

i

s

.

c

5

o

0

u

n

t

f

a

c

t

o

.

6

0

.

7

0

.

8

0

.

9

1

.

0

r

Figure 22: Prediction error of feedback of Amazon seller. Trust in history versus different
history discount factors.

Alternatively, we may discount the history by a factor of β ∈ [0, 1]. For example, when
β = 0.90, we have 3 × β 3 + 1 × β 2 + 2 × β 1 + 4 × β 0 )/(β 3 + β 2 + β + 1), which equals
1 × 0.93 + 2 × 0.92 + 3 × 0.9 + 4)/(0.93 + 0.92 + 0.9 + 1) = 2.56. Under this scheme, in
the above case, the prediction error would be 0.44. If we use a different discount factor
256

A Probabilistic Approach for Maintaining Trust Based on Evidence

for the history, we would in general obtain a different prediction error. In our experiment,
we normalize the feedback to a real number in [0, 1]. That is, a feedback of 1, 2, 3, 4,
or 5 corresponds to a trust value of 0, 0.25, 0.50, 0.75, or 1, respectively. We compare
the prediction error using the trust in history with the prediction error using a specified
discount factor. As Figure 22 shows, using the fixed discount in history, when the discount
factor is 0.82, the average error of prediction is the lowest, which is 0.35. In general, the
error would be higher unless we happened to correctly guess the optimal discount factor.
In contrast, using trust in history, the average prediction error is 0.34, which turns out to
be lower than using any specific fixed discount factor (Hypothesis 1 ).
An important engineering challenge facing traditional approaches is that, since they
require a fixed discount factor for the history as a parameter, we need to manually tune
such a discount factor for each application scenario. Such tuning limits the applicability of
the traditional approaches substantially. In contrast, our method that uses trust in history
automatically adapts to an agent’s changing behavior, and thus does not require any manual
tuning (Hypothesis 2 ).
Summary
The foregoing shows that Average with trust in history is effective in tracking ratings from
Amazon without tuning any parameters. Hence, we conclude that the above supports
Hypothesis 1: Effectiveness and Hypothesis 2: No tuning.
7.6 Summarizing Experimental Results
Our results are twofold. First, in Section 7.2, we show Average provides a competitive
accuracy measurement on referrals. It can deal with various behavior effectively, detect
exaggerated reports, and forgive referrers with no trust information (Hypothesis 1 ). Section 7.3 demonstrates how our trust update method identifies malicious referrer and provides
an accurate report from referrals containing false information (Hypothesis 1 ). Second, we
show the effectiveness and benefits of trust in history. Section 7.4 presents how trust in
history tracks various artificial behavior competitively without parameter tuning (Hypotheses 1 and 2 ). We show that trust in history can preserve a greater amount of evidence than
the other approaches and thereby provide additional information about the dynamism of
the service provider (Hypothesis 3 ). Section 7.5 shows it works just as well on practical
behavior from real datasets (Hypothesis 1 ).
Although Average-α is not the most accurate method in all circumstances (Hypothesis
1 ), we consider Average-α to be the best solution among these methods. Average-α requires
no tuning of a discount factor. There are two main advantages for not having to tune β by
hand.
• Tuning the discount factor can be difficult for a variety of reasons. It is nontrivial to
determine a provider’s behavior profile. It is difficult to determine the best value of
β for a specific profile and to maintain that value even as an agent changes its profile
dynamically. Using the dynamically changing α as the discount factor can adapt to
all kinds of profiles.
257

Wang, Hang, & Singh

• By dynamically tuning the discount factor, Average-α can provide dynamic certainty
information, which reflects the predictability of the referrers (Hypothesis 3 ). If a
provider changes behavior frequently, the certainty computed by the trust in history
does not build up. Knowing the certainty may affect an agent’s decision-making
strategy. Even if the probability is high, the provider may not be trusted, due to a low
certainty. Conversely, using β as the discount factor cannot provide such information
because it discounts history equally regardless of conflict (Hypothesis 3 ).

8. Literature
Trust models have been widely studied (Sabater & Sierra, 2005; Jøsang, Ismail, & Boyd,
2007). Here we focus on some well-known trust models and also the ones that study trust
update and evaluate the trustworthiness of referrers based on their referrals.
The Beta Reputation System (BRS) (Jøsang & Ismail, 2002) and SPORAS (Zacharia &
Maes, 2000) are two trust models that support the idea of the discount factor. They define
a fixed damping factor to control how much past experience should be discounted. This
is similar to β, the manually coded discount factor in our experiments. In our approach,
the discount factor can be automatically tuned based on how dynamic the agent behavior
is. Besides, BRS and SPORAS fail to provide a trust update mechanism to update the
estimated trustworthiness of agents based on the accuracy of the trust information they
provide.
FIRE (Huynh et al., 2006) and REGRET (Sabater & Sierra, 2002) are two trust models
that consider trust information from both individual and social aspects. FIRE estimates
trust from four sources: interactions, roles, witnesses, and certified reputations. The trust
information in REGRET includes individual and social dimensions. However, both FIRE
and REGRET lack a trust update mechanism. Although FIRE can cope with some dynamism, in their experiments, Huynh et al. assume the agent behavior only involves minor
changes with an extremely low probability. Our trust update approach copes with various
kinds of dynamism. We evaluate our trust update by introducing several dynamic behavior
profiles. Our experiments show our trust update provides accurate trust estimation for a
variety of natural behavior profiles.
Teacy et al. (2006) develop Travos, one of the trust models based on the beta distribution. Travos calculates trust based on both direct experience and trust information from
third parties. Travos also provides a mechanism to measure the accuracy of referrals. Given
a referral hr′ , s′ i (i.e., a beta distribution), Travos divides the probability density into several
disjoint intervals. Suppose the probability α of the actual experience hr, si lies in interval k.
Then the accuracy of the referral is defined as the probability density ratio of the interval k
to all intervals. Similar to Sensitivity, their accuracy measurement suffers when the number
of transactions is large. Besides, the number of intervals requires human tuning. Teacy et
al. suggest that a good trust model should satisfy three requirements: it should provide a
comparable trust metric with or without personal experience; it should provide a confidence
measure; and it should be able to assess the reliability of trust information sources and discount the information provided by unreliable sources. Our approach satisfies their three
requirements of a good trust model. Besides, our approach makes no assumption about
agent behavior. However, Travos assumes the agent behavior remains unchanged over time.
258

A Probabilistic Approach for Maintaining Trust Based on Evidence

Teacy et al. agree that a time-based behavioral strategy is necessary for agents to deal
with dynamic behavior. By using our automatically adjusted discount factor, our approach
provides a time-based strategy dealing with a variety of dynamic behavior profiles.
Fullam and Barber (2007) study how to choose between trust from direct experience
(experience-based) and from referrals (reputation-based). They adopt reinforcement learning to learn a parameter that controls how to aggregate information from experience-based
and reputation-based trust. Based on the reward the client gains from the transactions,
Fullam and Barber dynamically update the weights of reputation providers (referrers) in
a linear manner. Wang and Vassileva (2003) present a Bayesian network-based trust and
reputation model for peer-to-peer networks. Their model also treats trust update in a linear
manner. They predefine a fixed discount factor to discount past information. Our approach
updates trust based on the probability theory. We show how our approach performs better
than linear-based trust update approach both theoretically and experimentally.
Ries and Heinemann (2008) propose CertainTrust, which is similar to Jøsang’s approach.
They define trust in terms of the numbers of positive and negative experiences. Their
certainty does not reflect the conflict in the evidence, but reflects the amount of evidence.
Trust is propagated using two operators: consensus (our aggregation) and discounting (our
concatenation). Context dependence is supported by predefining the maximum amount of
expected evidence, which is not trivial. Ries and Heinemann update trust in two ways. To
update trust from a feedback f (a scalar between −1 and 1), they increment the number
of positive experiences by (1 + f )/2 and increment the number of negative experiences by
(1 − f )/2. An alternative is to update the trust placed in an agent based on the accuracy of
the recommendations it provides. The accuracy of the recommendations is defined as the
tendency to the actual behavior. Ries and Heinemann adopt an aging factor analogous to
a discount factor. The aging factor normalizes the trust values that exceed the predefined
maximum number of experiences. Their aging factor is defined once and used only for
normalization. As shown in Section 7, using fixed aging factors poorly deals with various
kinds of behavior profiles and these parameters require human tuning. In contrast, our
approach can track the dynamic behavior as well as adjust the discount factor based on
how dynamic the agents are.
Khosravifar, Gomrokchi, and Bentahar (2009) design a maintenance-based trust model.
They define a timely relevance factor to discount the past experience when updating trust.
The timely relevance factor reflects the time difference between the current time and the
time of last update. The amount of discounted information is determined by a domaindependent variable λ, which is similar to the discount factor β discussed in this paper.
When λ is high, past experience is forgotten faster. When λ is low, trust values tend to
consider overall experience. However, Khosravifar et al.’s timely relevance factor requires
manual tuning, which is also the common limitation of the trust update methods with the
β discount factor. The discount factor α in our Average-α requires no manual tuning.
Poyraz (Sensoy, Zhang, Yolum, & Cohen, 2009) is a trust-based service selection approach. Poyraz calculates the estimated trustworthiness of web services based on both
direct experience and referrals. Poyraz filters out referrals provided by untrustworthy advisors (i.e., referrers). It assesses the trustworthiness of advisors based on private credit and
public credit. The private credit is evaluated by comparing the consumer’s actual experience with the referral. The comparison produces either a satisfactory or an unsatisfactory
259

Wang, Hang, & Singh

assessment based on the consumer’s preferences. When the consumer lacks experience,
public credit is calculated by comparing the referral with other advisors’ referrals. If the
referral deviates from the majority, the advisor is considered untrustworthy. Poyraz also
provides a time window to discard old trust information. Our approach compares actual
experience with referrals based on probability density rather than consumer’s preferences.
We discount old trust information using a discount factor. Instead of manually adjust the
size of the time window, our discount factor is automatically tuned based on how dynamic
the agent behavior is.
Paradesi et al. (2009) incorporate trust based on certainty into their work on web service
composition. Like our approach, their definition of trust and certainty is based on Wang
and Singh’s (2007) approach. In their trust framework, Wisp, Paradesi et al. study four
types of frequently encountered web service flows in composition. They provide operators to
calculate the trust and certainty for the composed web service in each case. Paradesi et al.’s
method to update the trust is intuitive based on Wang and Singh and Jøsang’s approach: it
simply adds up the number of positive and the number of negative transactions separately.
Importantly, it does not consider history discount or the effect of aging information, as we
have here. In this manner, Paradesi et al.’s work complements our proposed approach, and
could be refined to adopt our more sophisticated definitions of trust update.
Mistry, Gürsel, and Sen (2009) estimate reputation scores of sensor nodes based on measurement accuracy in sensor networks. In their framework, a parent node receives reports
from its children nodes. Based on the aggregated (average) report, the parent evaluates the
trustworthiness of its children. The parent compares the aggregated report and sensed data
from each child, and then calculates the error based on the Wilcoxon Signed Rank Test
(Wilcoxon, 1945). The reputation of each child reflects the new evidence (the error) based
on two update schemes, β-reputation (Jøsang & Ismail, 2002) and Q-learning (Watkins &
Dayan, 1992). Both β-reputation and Q-learning schemes use a fixed parameter (discount
factor γ in β-reputation and learning rate α in Q-learning) to exponentially discount the
past evidence, which is similar to our general update (Algorithm 1). Their update schemes
require manual tuning and thus lack the ability of dealing with dynamism.
Vogiatzis, MacGillivray, and Chli (2010) build a trust framework on Hidden Markov
Models. They apply a probabilistic model to estimate the quality of the service provider
with varying behavior. There are some important differences between their approach and
ours. Vogiatzis et al. assume that the changes in the behavior of a service provider are slowly
varying and can be described as a Wiener process on the quality sequence. A Wiener process
is analogous to Brownian motion and supports properties that are not well-motivated when
talking about service providers. For example, it requires that the behavior of a service
provider have a mean of 0 and a deviation proportional to the time (that is, it should offer
the same quality on average as it does at the beginning). Such a model makes sense for a
Brownian motion but does not apply to agents who may vary their quality of service due to
environmental effects as well as investments in infrastructure, as motivated above. Further,
Vogiatzis et al. make additional unjustified assumptions such as that opinions from an
honest provider have a normal distribution with a mean of the true quality and that opinions
from a dishonest provider have a uniform distribution. Thus, their approach only models
agents with randomly decision making, and does not apply to agents who deliberately
260

A Probabilistic Approach for Maintaining Trust Based on Evidence

provide extremely high or extremely low referrals. Overall, the model of Vogiatzis et al. has
limited practical applicability in connection with services and agents.
Vogiatzis et al. (2010) evaluate their approach with respect to two types of behaviors:
static and damping. We treat dynamism more extensively by defining six dynamic behavior
profiles and show how our approach performs against these profiles. However, our definition
of reputation of the target (referrer) satisfies the required mathematical properties and it is
rigorous. We use heuristics to update the trust of referrals. We have justified our heuristics
by proving that our models satisfy important mathematical properties, for example, the
farther the referrals are from the believed actual quality of a service provider, the higher
is the update (q in Algorithm 1) in the trust placed in the referrer. Our model is computationally efficient. An excessive need for computation is a big shortcoming of Vogiatzis
et al.’s and other such traditional approaches. For example, in order to estimate the quality
of a service provider with varying behavior, based on 100 transactions, Vogiatzis et al.’s
approach would need to calculate multiple integrals with dimension 100. The efficiency
would suffer further when calculating the honesty of multiple opinion providers.
Hazard and Singh (2010) identify and axiomatize some common intuitions about trust
when viewed from the perspective of the incentives of agents. They relate the trustworthiness of an agent to how it discounts future payoffs: more trustworthy agents have a
longer time horizon, an intuition also shared by Smith and desJardins (2009). The above
approaches are complementary to ours in that they deal with agents who can strategically
alter their behavior whereas we concern ourselves with agents of a fixed type, whose provided quality of service we attempt to estimate. Also, from the incentives perspective, Jurca
and Faltings (2007) study mechanisms to ensure agents offer truthful feedback on others.
Our approach deals with how to incorporate new evidence in maintaining a trust rating.
Their approach applies in settings where one can sanction false reporters and thus promote
good behavior.

9. Conclusions and Directions
This paper proposes an approach to perform trust updates. It makes the following contributions. One, for the problem of updating the trust placed in referrers on a continuing basis,
it develops a mathematically well-justified probabilistic approach for performing updates.
Importantly, this approach works on top of a conceptually simple representation for trust
that reflects common intuitions about trust and evidence. Further, the proposed approach
although cast as a heuristic for calculating trust updates is evaluated (along with some
competing heuristics) on mathematical grounds through properties of monotonicity and
sensitivity. Two, this paper adapts its referrals approach for updating trust in a provider
by modeling trust assessments as referrals from the history of prior interactions. Three,
this paper shows that the proposed approach yields performance that compares well with
existing approaches without requiring any hand tuning of parameters common in previous
approaches.
Our investigations have opened up some interesting natural directions for future study.
First, an obvious theme is further experimental evaluation. It would be instructive to
consider additional types of agents, for instance, discriminative agents. Second, a promising
line of inquiry is relating the above to the decision-making strategies for agents to compare
261

Wang, Hang, & Singh

trust estimates for service selection. It would be interesting to examine how discounting the
past, as we showed above, relates to discounting valuations of the future. Third, in certain
settings, especially with widespread sharing of information, updating trust estimates can
have significant dynamical effects. Hazard (2010) has studied such dynamical properties of
various mechanisms but without explicitly considering referrals. It will be instructive to
combine our approach with his.
Fourth, our current definition does not accommodate multivalued events and does not
tell us if a referral overestimates or underestimates the quality of the service provider.
Multivalued events can be useful in some practical cases. Further, an underestimate might
be more desirable than an overestimate—in the former case, you get a pleasant surprise,
although it is not always ideal because you would miss out on selecting some good providers
because of the underestimation. We have begun to address a suitable representation of trust.
However, it would be nontrivial to provide appropriate updating methods.

Acknowledgments
We thank Chris Hazard and Scott Gerard for helpful comments. Chris provided his dataset
for use in our study. This work was partially supported by the U.S. Army Research Office (ARO) under grant W911NF-08-1-0105 managed by the NCSU Secure Open Systems
Initiative (SOSI) and partially sponsored by the Army Research Laboratory in its Network Sciences Collaborative Technology Alliance (NS-CTA) under Cooperative Agreement
Number W911NF-09-2-0053.

Appendix A. Proofs of Theorems
Lemma 6
Z

1

r

xr (1 − x)s dx =

0

Y
1
i
r+s+1
r+s+1−i
i=1

Proof:
integration
by parts.
R
R 1 r We use
s dx = 1 xr d( −1 (1 − x)s+1 )
x
(1
−
x)
0
0
R 1 s+1
xr (1−x)s+1 1
r
= − s+1 |0 + s+1 0 xr−1 (1 − x)s+1 dx
R 1 r−1
r
= s+1
(1 − x)s+1 dx
0 x
= ···
R1
r·(r−1)···1
r+s dx
= (r+s)·(r+s−1)···(s+1)
0 (1 − x)
r
Q
1
i
= r+s+1
r+s+1−i .

2

i=1

Lemma 7 Given r and s as above, we have that
v
u r
uY
αα
i
r
lim t
=
r→∞
αr + r + 1 − i
(1 + α)α+1
i=1

Where r is a positive integer.

262

A Probabilistic Approach for Maintaining Trust Based on Evidence

Proof: This lemma is used in the next lemma, to show that the right side of an equation
approaches a constant, where the equation has duplicated roots, and then the two roots of
the equation approach that duplicated root.
r
Q
i
limr→∞ 1r ln
αr+r+1−i
i=1

=

limr→∞ 1r

ln(

=

1
αr+r+1−i )

i=1 i=1
r
r
Q
Q

= limr→∞ 1r ln(
=

r
r
Q
Q
i

1
αr+i )

i

i=1 i=1
i=r
P
i
limr→∞ 1r
ln αr+i
i=1
i=r
i
P
limr→∞ 1r
ln α+r i
r
i=1
R1
x
0 ln α+x dx
αα
ln (1+α)
α+1

=
=
Therefore,
s
limr→∞

r

r
Q

i=1

i
αr+r+1−i

Lemma 8 Let α =

s
r

=

αα
.
(1+α)α+1

2

and α is fixed. Let A(r) and B(r) be the two values of x that satisfy
xr (1 − x)αr
=c
R1
r (1 − x)αr
x
0

Where c > 0.

lim A(r) = lim B(r) =

r→∞

r→∞

(4)
1
1+α

(5)

Where r is a positive integer.
Proof: The idea is to show that A(r) and B(r) are two roots of an equation g(x) = β(r).
If limr→∞ β(r) = β and the equation g(x) = β has duplicated roots of α, then we have
limr→∞ A(r) = limr→∞ B(r) = α
A(r) and B(r)
two roots for the equation
q are
R
1
x(1 − x)α = r c 0 xr (1 − x)αr dx
since q
R1
limr→∞ r c 0 xr (1 − x)αr dx
s
r
Q
i
1
= limr→∞ r c αr+r+1
αr+r+1−i (by Lemma 6)
i=1
αα
= (1+α)α+1 (by Lemma 7)
1 α
1
(1 − 1+α
)
= 1+α
α
since x(1 − x) achieves its

maximum at x =

equation
1
1 α
x(1 − x)α = 1+α
(1 − 1+α
)
Therefore,
limr→∞ A(r) = limr→∞ B(r) =

1
1+α .

1
1+α ,

and x =

1
1+α

is the only root for the

2
263

Wang, Hang, & Singh

Lemma 9 Let α =

r
r+s

and t = r + s. Let α be fixed and c 6= α.
lim R

t→∞

lim R

t→∞
αt

cαt (1 − c)(1−α)t
=0
xαt (1 − x)(1−α)t dx

(6)

ααt (1 − α)(1−α)t
=∞
xαt (1 − x)(1−α)t dx

(7)

(1−α)t

(1−x)
Proof: Let f (x) = R xxαt (1−x)
(1−α)t dx
Proof of Equation 6:
Without losing generality, assume 0 < c < α.
For any ǫ > 0, let A(t) and B(t) be defined in Equation 4 where c = ǫ. According to
Lemma 8, there is a T > 0 such that c < A(t) < α when t > T . Since f (c) < f (A(t)) = ǫ,

we have f (c) < ǫ. Thus for any ǫ > 0, there is a T > 0, such that
when t > T , which proves Equation 6.
Proof of Equation 7:

αt
(1−α)t
R c (1−c)
xαt (1−x)(1−α)t dx

<ǫ

For any N > 0, let A(t) and B(t) be defined in Equation 4 where c = 0.50. Since
f (x) < f (A(t)) = 0.50 when x < A(t) and f (x) < f (B(t)) = 0.50 when x > B(t). Then
R A(t)
R1
R1
f (x)dx + B(t) f (x)dx < 0 0.50dx = 0.50.
0
R1
R A(t)
R B(t)
R1
Thus A(t) f (x)dx = 0 f (x)dx − 0 f (x)dx − B(t) f (x)dx
R1
R A(t)
1 − 0 f (x)dx − B(t) f (x)dx > 0.50 Since f (x) ≤ f (α) when x ∈ (A(t), B(t)).
Thus we obtain (B(t) − A(t))f (α) > 0.50
1
According to Lemma 8, there is a T > 0 such that B(t) − A(t) < 2N
when t > T .
Thus we have f (α) > 0.50/(B(t) − A(t)) > N when t > T .
Therefore, for any N > 0, there is a T > 0 such that f (α) > N when t > T , which proves
Equation 7.
2
Proof of Theorem
3
r ′ (1−α)s′
f (α)
q = αα′r′ (1−α
=
′ ) s′
f (α′ )
By Lemma 9, we have limt→∞ f (α) = 0 and limt→∞ f (α′ ) = ∞, so we have
limt→∞ q = 0.
The case for Sensitivity is the same as the above.
Proof of Theorem 4 Let t = r′ + s′ . By Theorem 5, it is equivalent to prove that
s
r′ + 1 2
(r′ + 1)(s′ + 1)
lim (α − ′
) + ′
= |α − α′ |
′
t→∞
r +s +2
(r + s′ + 2)2 (r′ + s′ + 3)
q
′
(r ′ +1)(s′ +1)
2
limt→∞ (α − r′ r+s+1
′ +2 ) + (r ′ +s′ +2)2 (r ′ +s′ +3)
q
′
′ )+1)
′ +1
)2 + (tα +1)(t(1−α
= limt→∞ (α − tαt+2
(t+2)2 (t+3)

2

= |α − α′ |

′

+1
= α′ and limt→∞
since limt→∞ tαt+2
Proof of Theorem 5

(tα′ +1)(t(1−α′ )+1)
(t+2)2 (t+3)

264

=0

2

A Probabilistic Approach for Maintaining Trust Based on Evidence

By lemma 6, we have
R1

′

′

xr (1 − x)s (x − α)2 dx
R1 ′
′
= 0 xr (1 − x)s (x2 − 2αx + α2 )dx
R1 ′
′
′
′
′
′
= 0 xr +2 (1 − x)s − 2αxr +1 (1 − x)s + α2 xr (1 − x)s dx
0

=
So

′

(r ′ +2)!s′ !
(r ′ +s′ +3)!

rR

q =1−
s

=1−

=1−
=1−
2

q

q

′

+1)!s !
r !s !
2
− 2α (r(r′ +s
′ +2)! + α (r ′ +s′ +1)!

1
0

′

′

′

′

xr (1−x)s (x−α)2 dx
R1 ′
r
s′
0 x (1−x) dx

(r ′ +1)!s′ !
(r ′ +2)!s′ !
r ′ !s′ !
−2α (r′ +s′ +2)! +α2 (r′ +s
′ +1)!
(r ′ +s′ +3)!
r ′ !s′ !
(r ′ +s′ +1)!

(r ′ +1)(r ′ +2)
(r ′ +s′ +2)(r ′ +s′ +3)
(r ′ +1)(s′ +1)
(r ′ +s′ +2)2 (r ′ +s′ +3)

α2 − 2α r′ r+s+1
′ +2 +
′

(α −

r ′ +1
2
r ′ +s′ +2 )

+

References
Barber, K. S., & Kim, J. (2001). Belief revision process based on trust: Agents evaluating
reputation of information sources. In Falcone, R., Singh, M. P., & Tan, Y.-H. (Eds.),
Trust in Cyber-Societies, Vol. 2246 of LNAI, pp. 73–82, Berlin. Springer.
Casella, G., & Berger, R. L. (1990). Statistical Inference. Duxbury Press, Pacific Grove,
CA.
Fullam, K., & Barber, K. S. (2007). Dynamically learning sources of trust information:
experience vs. reputation. In Proceedings of the 6th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 1062–1069, Honolulu, HI,
USA. IFAAMAS.
Gómez, M., Carbó, J., & Earle, C. B. (2007). Honesty and trust revisited: the advantages
of being neutral about other’s cognitive models. Autonomous Agents and Multi-Agent
Systems, 15 (3), 313–335.
Hang, C.-W., Wang, Y., & Singh, M. P. (2008). An adaptive probabilistic trust model and
its evaluation. In Proceedings of the 7th International Conference on Autonomous
Agents and MultiAgent Systems (AAMAS), pp. 1485–1488, Estoril, Portugal. IFAAMAS. Short paper.
Harbers, M., Verbrugge, R., Sierra, C., & Debenham, J. (2007). The examination of an
information-based approach to trust. In MALLOW Workshop on Coordination, Organization, Institutions and Norms in agent systems (COIN), pp. 101–112, Durham,
UK. Springer-Verlag.
Hazard, C. J. (2010). Trust and Reputation in Multiagent Systems: Strategies and Dynamics with Reference to Electronic Commerce. Ph.D. thesis, Department of Computer
Science, North Carolina State University.
265

Wang, Hang, & Singh

Hazard, C. J., & Singh, M. P. (2010). Intertemporal discount factors as a measure of
trustworthiness in electronic commerce. IEEE Transactions on Knowledge and Data
Engineering. In press.
Huynh, T. D., Jennings, N. R., & Shadbolt, N. R. (2006). An integrated trust and reputation
model for open multi-agent systems. Autonomous Agents and Multi-Agent Systems,
13 (2), 119–154.
Jøsang, A. (1998). A subjective metric of authentication. In Quisquater, J.-J., Deswarte,
Y., Meadows, C., & Gollmann, D. (Eds.), Proceedings of the 5th European Symposium
on Research in Computer Security (ESORICS), Lecture Notes in Computer Science,
pp. 329–344, Louvain-la-Neuve, Belgium. Springer.
Jøsang, A. (2001). A logic for uncertain probabilities. International Journal of Uncertainty,
Fuzziness and Knowledge-Based Systems (IJUFKS), 9 (3), 279–311.
Jøsang, A., & Ismail, R. (2002). The Beta reputation system. In Proceedings of the 15th
Bled Electronic Commerce Conference, pp. 324–337, Bled, Slovenia.
Jøsang, A., Ismail, R., & Boyd, C. (2007). A survey of trust and reputation systems for
online service provision. Decision Support Systems, 43 (2), 618–644.
Jurca, R., & Faltings, B. (2007). Obtaining reliable feedback for sanctioning reputation
mechanisms. Journal of Artificial Intelligence Research (JAIR), 29, 391–419.
Khosravifar, B., Gomrokchi, M., & Bentahar, J. (2009). Maintenance-based trust for multiagent systems. In Sierra, C., Castelfranchi, C., Decker, K. S., & Sichman, J. S. (Eds.),
Proceedings of the 8th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), Vol. 2, pp. 1017–1024, Budapest, Hungary. IFAAMAS.
Mistry, O., Gürsel, A., & Sen, S. (2009). Comparing trust mechanisms for monitoring
aggregator nodes in sensor networks. In Sierra, C., Castelfranchi, C., Decker, K. S., &
Sichman, J. S. (Eds.), Proceedings of the 8th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS), Vol. 2, pp. 985–992, Budapest, Hungary.
IFAAMAS.
Paradesi, S., Doshi, P., & Swaika, S. (2009). Integrating behavioral trust in web service compositions. In Proceedings of the 7th IEEE International Conference on Web Services
(ICWS), pp. 453–460, Los Angeles, CA, USA. IEEE Computer Society.
Procaccia, A. D., Bachrach, Y., & Rosenschein, J. S. (2007). Gossip-based aggregation of
trust in decentralized reputation systems. In Veloso, M. M. (Ed.), Proceedings of the
20th International Joint Conference on Artificial Intelligence (IJCAI), pp. 1470–1475,
Hyderabad, India. IJCAI.
Ries, S., & Heinemann, A. (2008). Analyzing the robustness of CertainTrust. In Proceedings
of the 2nd Joint iTrust and PST Conference on Privacy, Trust Management and
Security, IFIP International Federation for Information Processing, pp. 51–67, Boston,
MA, USA. Springer.
Sabater, J., & Sierra, C. (2002). Reputation and social network analysis in multi-agent
systems. In Proceedings of the 1st International Joint Conference on Autonomous
Agents and Multiagent Systems (AAMAS), pp. 475–482, Bologna, Italy. ACM Press.
266

A Probabilistic Approach for Maintaining Trust Based on Evidence

Sabater, J., & Sierra, C. (2005). Review on computational trust and reputation models.
Artificial Intelligence Review, 24 (1), 33–60.
Sensoy, M., Zhang, J., Yolum, P., & Cohen, R. (2009). Poyraz: Context-aware service
selection under deception. Computational Intelligence, 25 (4), 335–366.
Smith, M. J., & desJardins, M. (2009). Learning to trust in the competence and commitment
of agents. Autonomous Agents and Multi-Agent Systems, 18 (1), 36–82.
Teacy, W. T. L., Patel, J., Jennings, N. R., & Luck, M. (2006). TRAVOS: Trust and
reputation in the context of inaccurate information sources. Autonomous Agents and
Multi-Agent Systems, 12 (2), 183–198.
Vogiatzis, G., MacGillivray, I., & Chli, M. (2010). A probabilistic model for trust and
reputation. In van der Hoek, W., Kaminka, G. A., Lespérance, Y., Luck, M., & Sen,
S. (Eds.), Proceedings of the 9th International Conference on Autonomous Agents and
Multiagent Systems (AAMAS), Vol. 1, pp. 225–232, Toronto, Canada. IFAAMAS.
Wang, Y., & Vassileva, J. (2003). Trust and reputation model in peer-to-peer networks.
In Shahmehri, N., Graham, R. L., & Caronni, G. (Eds.), Proceedings of the 3rd International Conference on Peer-to-Peer Computing (P2P), pp. 150–157, Linköping,
Sweden. IEEE Computer Society.
Wang, Y., & Singh, M. P. (2006). Trust representation and aggregation in a distributed
agent system. In Proceedings of the 21st National Conference on Artificial Intelligence
(AAAI), pp. 1425–1430, Boston, MA, USA. AAAI Press.
Wang, Y., & Singh, M. P. (2007). Formal trust model for multiagent systems. In Veloso,
M. M. (Ed.), Proceedings of the 20th International Joint Conference on Artificial
Intelligence (IJCAI), pp. 1551–1556, Hyderabad, India. IJCAI.
Wang, Y., & Singh, M. P. (2010). Evidence-based trust: A mathematical model geared
for multiagent systems. ACM Transactions on Autonomous and Adaptive Systems
(TAAS), 5 (4), 14:1–14:28.
Watkins, C., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3), 279–292.
Wilcoxon, F. (1945). Individual comparisons by ranking methods. Biometrics Bulletin,
1 (6), 80–83.
Yu, B., & Singh, M. P. (2002). Distributed reputation management for electronic commerce.
Computational Intelligence, 18 (4), 535–549.
Zacharia, G., & Maes, P. (2000). Trust management through reputation mechanisms. Applied Artificial Intelligence, 14 (9), 881–907.

267

Journal of Artificial Intelligence Research 40 (2011) 469–521

Submitted 06/10; published 02/11

Narrowing the Modeling Gap:
A Cluster-Ranking Approach to Coreference Resolution
Altaf Rahman
Vincent Ng

altaf@hlt.utdallas.edu
vince@hlt.utdallas.edu

Human Language Technology Research Institute
University of Texas at Dallas
800 West Campbell Road; Mail Station EC31
Richardson, TX 75080-3021 U.S.A.

Abstract
Traditional learning-based coreference resolvers operate by training the mention-pair
model for determining whether two mentions are coreferent or not. Though conceptually
simple and easy to understand, the mention-pair model is linguistically rather unappealing
and lags far behind the heuristic-based coreference models proposed in the pre-statistical
NLP era in terms of sophistication. Two independent lines of recent research have attempted to improve the mention-pair model, one by acquiring the mention-ranking model to
rank preceding mentions for a given anaphor, and the other by training the entity-mention
model to determine whether a preceding cluster is coreferent with a given mention. We
propose a cluster-ranking approach to coreference resolution, which combines the strengths
of the mention-ranking model and the entity-mention model, and is therefore theoretically
more appealing than both of these models. In addition, we seek to improve cluster rankers
via two extensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity by
jointly modeling anaphoricity determination and coreference resolution. Experimental results on the ACE data sets demonstrate the superior performance of cluster rankers to
competing approaches as well as the effectiveness of our two extensions.

1. Introduction
Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions in
ACE terminology1 ) in a text or dialogue refer to the same real-world entity or concept. From
a computational perspective, coreference is a clustering task, with the goal of partitioning
a set of mentions into coreference clusters where each cluster contains all and only the
mentions that are co-referring. From a mathematical perspective, a coreference relation is
an equivalence relation defined on a pair of mentions, as it satisfies reflexivity, symmetry,
and transitivity. Following our previous work on coreference resolution, we use the term
anaphoric to describe a mention that is part of a coreference chain but is not the head of a
chain. Given an anaphoric mention mk , an antecedent of mk is a mention that is coreferent
with mk and precedes it in the associated text, and the set of candidate antecedents of mk
consists of all mentions that precede mk .2
1. More precisely, a mention is an instance of reference to an entity in the real world. In this article, we
treat the terms mention and noun phrase as synonymous and use them interchangeably.
2. Note that these definitions are somewhat overloaded. Linguistically, an anaphor is a noun phrase that
depends on its antecedent for its semantic interpretation. Hence, “Barack Obama” can be anaphoric in
our definition but not in the formal definition.
c
2011
AI Access Foundation. All rights reserved.

Rahman & Ng

The research focus of computational coreference resolution exhibited a gradual shift from
heuristic-based approaches to machine learning approaches in the past decade. The shift
can be attributed in part to the advent of the statistical natural language processing (NLP)
era, and in part to the public availability of coreference-annotated corpora produced as a
result of the MUC-6 and MUC-7 conferences and the series of ACE evaluations. One of the
most influential machine learning approaches to coreference resolution is the classificationbased approach, where coreference is recast as a binary classification task (e.g., Aone &
Bennett, 1995; McCarthy & Lehnert, 1995). Specifically, a classifier that is trained on
coreference-annotated data is used to determine whether a pair of mentions is co-referring
or not. However, the pairwise classifications produced by this classifier (which is now commonly known as the mention-pair model) may not satisfy the transitivity property inherent
in the coreference relation, since it is possible for the model to classify (A,B) as coreferent,
(B,C) as coreferent, and (A,C) as not coreferent. As a result, a separate clustering mechanism is needed to coordinate the possibly contradictory pairwise classification decisions and
construct a partition of the given mentions.
The mention-pair model has significantly influenced learning-based coreference research
in the past fifteen years. In fact, many of the recently published coreference papers are
still based on this classical learning-based coreference model (e.g., Bengtson & Roth, 2008;
Stoyanov, Gilbert, Cardie, & Riloff, 2009). Despite its popularity, the model has at least
two major weaknesses. First, since each candidate antecedent for a mention to be resolved
(henceforth an active mention) is considered independently of the others, this model only
determines how good a candidate antecedent is relative to the active mention, but not
how good a candidate antecedent is relative to other candidates. In other words, it fails
to answer the critical question of which candidate antecedent is most probable. Second,
it has limitations in its expressiveness: the information extracted from the two mentions
alone may not be sufficient for making an informed coreference decision, especially if the
candidate antecedent is a pronoun (which is semantically empty) or a mention that lacks
descriptive information such as gender (e.g., “Clinton”).
Recently, coreference researchers have investigated alternative models of coreference that
aim to address the aforementioned weaknesses of the mention-pair model. To address the
first weakness, researchers have proposed the mention-ranking model. This model determines which candidate antecedent is most probable given an active mention by imposing
a ranking on its candidate antecedents (e.g., Denis & Baldridge, 2007b, 2008; Iida, Inui,
& Matsumoto, 2009). Ranking is arguably a more natural formulation of coreference resolution than classification, as a ranker allows all candidate antecedents to be considered
simultaneously and therefore directly captures the competition among them. Another desirable consequence is that there exists a natural resolution strategy for a ranking approach:
a mention is resolved to the candidate antecedent that has the highest rank. This contrasts
with classification-based approaches, where many clustering algorithms have been employed
to co-ordinate the pairwise coreference decisions (because it is unclear which one is the best).
To address the second weakness, researchers have proposed the entity-mention coreference
model (e.g., Luo, Ittycheriah, Jing, Kambhatla, & Roukos, 2004; Yang, Su, Zhou, & Tan,
2004; Yang, Su, Lang, Tan, & Li, 2008). Unlike the mention-pair model, the entity-mention
model is trained to determine whether an active mention belongs to a preceding, possibly
partially-formed, coreference cluster. Hence, it can employ cluster-level features (i.e., fea470

A Cluster-Ranking Approach to Coreference Resolution

tures that are defined over any subset of mentions in a preceding cluster), which makes it
more expressive than the mention-pair model.
While the entity-mention model and the mention-ranking model are conceptually simple
extensions to the mention-pair model, they were born nearly ten years after the mention-pair
model was proposed, and in particular, their contributions should not be under-estimated:
they paved a new way of thinking about supervised modeling of coreference that represents
a significant departure from their mention-pair counterpart, which for many years is the
learning-based coreference model for NLP researchers. The proposal of these two models
is facilitated in part by advances in statistical modeling of natural languages: statistical
NLP models have evolved from capturing local information to global information, and
from employing classification-based models to ranking-based models. In the context of
coreference resolution, the entity-mention model enables us to compute features based on a
variable number of mentions, and the mention-ranking model enables us to rank a variable
number of candidate antecedents. Nevertheless, neither of these models addresses both
weaknesses of the mention-pair model satisfactorily: while the mention-ranking model allows
all candidate antecedents to be ranked and compared simultaneously, it does not enable the
use of cluster-level features; on the other hand, while the entity-mention model can employ
cluster-level features, it does not allow all candidates to be considered simultaneously.
Motivated in part by this observation, we propose a learning-based approach to coreference resolution that is theoretically more appealing than both the mention-ranking model
and the entity-mention model: the cluster-ranking approach. Specifically, we recast coreference as the problem of determining which of a set of preceding coreference clusters is the
best to link to an active mention using a learned cluster-ranking model. In essence, the
cluster-ranking model combines the strengths of the mention-ranking model and the entitymention model, and addresses both weaknesses associated with the mention-pair model.
While the cluster-ranking model appears to be a conceptually simple and natural extension of the entity-mention model and the mention-ranking model, we believe that such
simplicity stems primarily from our choice of a presentation of these concepts that is easiest
for the reader to understand. In particular, we note that the mental processes involved in
the design of the cluster-ranking model are by no means as simple as the way the model
is presented: it requires not only an analysis of the strengths and weaknesses of existing
approaches to learning-based coreference resolution and the connection between them, but
also our formulation of the view that the entity-mention model and the mention-ranking
model are addressing two complementary weaknesses of the mention-pair model. We believe
that the significance of our cluster-ranking model lies in bridging two rather independent
lines of learning-based coreference research that have been going on in the past few years,
one involving the entity-mention model and the other the mention-ranking model.
In addition, we seek to improve the cluster-ranking model with two sources of linguistic knowledge. First, we propose to exploit knowledge of anaphoricity (i.e., knowledge of
whether a mention is anaphoric or not). Anaphoricity determination is by no means a new
problem, and neither is the use of anaphoricity information to improve coreference resolution. Our innovation lies in the way we learn knowledge of anaphoricity. Specifically,
while previous work has typically adopted a pipeline coreference architecture, in which
anaphoricity determination is performed prior to coreference resolution and the resulting
information is used to prevent a coreference system from resolving mentions that are de471

Rahman & Ng

termined to be non-anaphoric (for an overview, see the work of Poesio, Uryupina, Vieira,
Alexandrov-Kabadjov, & Goulart, 2004), we propose a model for jointly learning anaphoricity determination and coreference resolution. Note that a major weakness of the pipeline
architecture lies in the fact that errors in anaphoricity determination could be propagated
to the coreference resolver, possibly leading to a deterioration of coreference performance
(Ng & Cardie, 2002a). Our joint model is a potential solution to this error-propagation
problem.
Second, we examine a kind of linguistic features that is not exploited by the majority
of existing supervised coreference resolvers: word pairs that are composed of the strings (or
the head nouns) of an active mention and one of its preceding mentions. Intuitively, these
word pairs contain useful information. For example, they may help improve the precision
of a model, by allowing a learner to learn that “it” only has a moderate probability of
being anaphoric, and that “the contrary” taken from the phrase “on the contrary” is never
anaphoric. They may also help improve its recall, by allowing the learner to determine,
for instance, that “airline” and “carrier” can be coreferent. Hence, they offer a convenient
means to attack one of the major problems in coreference research: identifying coreferent
common nouns that are lexically dissimilar but semantically related. Note that they are
extremely easy to compute, even more so than the so-called “cheap” features such as stringmatching and grammatical features (Yang, Zhou, Su, & Tan, 2003), but the majority of
the existing supervised coreference systems are unlexicalized and hence are not exploiting
them. Somewhat unexpectedly, however, for researchers who do lexicalize their coreference
models by employing word pairs as features (e.g., Luo et al., 2004; Daumé III & Marcu, 2005;
Bengtson & Roth, 2008), their feature analysis experiments indicate that lexical features
are at best marginally useful. For instance, Luo et al. and Daume III and Marcu report
that leaving out lexical features in their feature ablation experiments causes the ACE value
to drop only by 0.8 and 0.7, respectively. While previous attempts on lexicalization merely
append all word pairs to a conventional coreference feature set, our goal is to investigate
whether we can make better use of lexical features for learning-based coreference resolution.
To sum up, we propose a cluster-ranking approach to coreference resolution and a joint
model for exploiting anaphoricity information, and investigate the role of lexicalization in
learning-based coreference resolution. Besides empirically demonstrating that our clusterranking model significantly outperforms competing approaches on the ACE 2005 coreference
data set, and that our two extensions to the model, namely lexicalization and joint modeling,
are effective in improving its performance, we believe our work makes four contributions to
coreference resolution:
Narrowing the modeling gap. While machine learning approaches to coreference resolution have received a lot of attention since the mid-1990s, the mention-pair model has
heavily influenced learning-based coreference research for more than a decade, and yet this
model lags far behind the heuristic-based coreference models proposed in the 1980s and
1990s in terms of sophistication. In particular, the notion of ranking can be traced back to
centering algorithms (for more information, see the books by Mitkov, 2002; Walker, Joshi,
& Prince, 1998), and the idea behind ranking preceding clusters (in a heuristic manner) can
be found in Lappin and Leass’s (1994) influential paper on pronoun resolution. While our
cluster-ranking model does not completely close the gap between the simplicity of machine
learning approaches and the sophistication of heuristic approaches to coreference resolu472

A Cluster-Ranking Approach to Coreference Resolution

tion, we believe that it represents an important step towards narrowing this gap. Another
important gap that our cluster-ranking model helps to bridge is the two independent lines
of learning-based coreference research that have been going on in the past few years, one
involving the entity-mention model and the other mention-ranking model.
Promoting the use of ranking models. While the mention-ranking model has been
empirically shown to outperform the mention-pair model (Denis & Baldridge, 2007b, 2008),
the former has not received as much attention among coreference researchers as it should.
In particular, the mention-pair model continues to be more popularly used and investigated
in the past few years than the mention-ranking model. We believe the lack of excitement for
ranking-based approaches to coreference resolution can be attributed at least in part to a
lack of theoretical understanding of ranking, as previous work on ranking-based coreference
resolution has employed ranking algorithms essentially as a black box. Without opening the
black box, it could be difficult for researchers to appreciate the subtle difference between
ranking and classification. In an attempt to promote the use of ranking-based models,
we provide a brief history of the use of ranking in coreference resolution (Section 2), and
tease apart the differences between classification and ranking by showing the constrained
optimization problem a support vector machine (SVM) attempts to solve in classificationbased and ranking-based coreference models (Section 3).
Gaining a better understanding of existing learning-based coreference models.
Recall that lexicalization is one of the two linguistic knowledge sources that we propose to
use to improve the cluster-ranking model. Note that lexicalization can be applied to not only
the cluster-ranking model, but essentially any learning-based coreference models. However,
as mentioned before, the vast majority of existing coreference resolvers are unlexicalized. In
fact, the mention-ranking model has only been shown to improve the mention-pair model on
an unlexicalized feature set. In an attempt to gain additional insights into the behavior of
different learning-based coreference models, we compare their performance on a lexicalized
feature set. Furthermore, we analyze them via experiments involving feature ablation and
data source adaptability, as well as report their performance on resolving different types of
anaphoric expressions.
Providing an implementation of the cluster-ranking model. To stimulate further
research on ranking-based approaches to coreference resolution, and to facilitate the use of
coreference information in high-level NLP applications, we make our software that implements the cluster-ranking model publicly available.3
The rest of this article is organized as follows. Section 2 provides an overview of the use
of ranking in coreference resolution. Section 3 describes our baseline coreference models:
the mention-pair model, the entity-mention model, and the mention-ranking model. We
discuss our cluster-ranking approach and our joint model for anaphoricity determination
and coreference resolution in Section 4. Section 5 provides the details of how we lexicalize the
coreference models. We present evaluation results and experimental analyses of different
aspects of the coreference models in Section 6 and Section 7, respectively. Finally, we
conclude in Section 8.
3. The software is available at http://www.hlt.utdallas.edu/~ altaf/cherrypicker/.

473

Rahman & Ng

2. Ranking Approaches to Coreference Resolution: A Bit of History
While ranking is theoretically and empirically a better formulation of learning-based coreference resolution than classification, the mention-ranking model has not been as popularly
used and investigated as its mention-pair counterpart since it was proposed. To promote
ranking-based coreference models, and to set the stage for further discussion of learningbased coreference models in the next section, we provide in this section a brief history of
the use of ranking in heuristic-based and learning-based coreference resolution.
In a broader sense, many heuristic anaphora and coreference resolvers are rankingbased. For example, to find an antecedent for an anaphoric pronoun, Hobbs’s (1978) seminal syntax-based resolution algorithm considers the sentences in a given text in reverse
order, starting from the sentence in which the pronoun resides and searching for potential
antecedents in the corresponding parse trees in a left-to-right, breadth-first manner that
obeys binding and agreement constraints. Hence, if we keep searching until the beginning
of the text is reached (i.e., we do not stop even after the algorithm proposes an antecedent),
we will obtain a ranking of the candidate antecedents for the pronoun under consideration, where the rank of a candidate is determined by the order in which it is proposed by
the algorithm. In fact, the rank of an antecedent obtained via this method is commonly
known as its Hobbs’s distance, which has been used as a linguistic feature in statistical
pronoun resolvers (e.g., Ge, Hale, & Charniak, 1998; Charniak & Elsner, 2009). In general,
search-based resolution algorithms like Hobbs’s consider candidate antecedents in a particular order and (typically) propose the first candidate that satisfies all linguistic constraints
as the antecedent.
Strictly speaking, however, we may want to consider a heuristic resolution algorithm as
a ranking-based algorithm only if it considers all candidate antecedents simultaneously, for
example by assigning a rank or score to each candidate and selecting the highest-ranked
or highest-scored candidate to be the antecedent. Even under this stricter definition of
ranking, there are still many heuristic resolvers that are ranking-based. These resolvers
typically assign a rank or score to each candidate antecedent based on a number of factors,
or knowledge sources, and then propose the one that has the highest rank or score as an
antecedent (e.g., Carbonell & Brown, 1988; Cardie & Wagstaff, 1999). A factor belongs to
one of two types: constraints and preferences (Mitkov, 1998). Constraints must be satisfied
before two mentions can be posited as coreferent. Examples of constraints include gender
and number agreement, binding constraints, and semantic compatibility. Preferences indicate the likelihood that a candidate is an antecedent. Some preference factors measure the
compatibility between an anaphor and its candidate (e.g., syntactic parallelism favors candidates that have the same grammatical role as the anaphor), while other preference factors
are computed based on the candidate only, typically capturing the salience of a candidate.
Each constraint and preference is manually assigned a weight indicating its importance.
For instance, gender disagreement is typically assigned a weight of −∞, indicating that a
candidate and the anaphor must agree in gender, whereas preference factors typically have
a finite weight. The score of a candidate can then be obtained by summing the weights of
the factors associated with the candidate.
Some ranking-based resolution algorithms do not assign a score to each candidate antecedent. Rather, they simply impose a ranking on the candidates based on their salience.
474

A Cluster-Ranking Approach to Coreference Resolution

Perhaps the most representative family of algorithms that employ salience to rank candidates is centering algorithms (for descriptions of specific centering algorithms, see the
work of Grosz, Joshi, & Weinstein, 1983, 1995; Walker et al., 1998; Mitkov, 2002), where
the salience of a mention, typically estimated using its grammatical role, is used to rank
forward-looking centers.
The work most related to ours is that of Lappin and Leass (1994), whose goal is to perform pronoun resolution by assigning an anaphoric pronoun to the highest-ranked preceding
cluster, and is therefore a heuristic cluster-ranking model. Like many other heuristic-based
resolvers, Lappin and Leass’s algorithm identifies the highest-ranked preceding cluster for
an active mention by first applying a set of linguistic constraints to filter candidate antecedents that are grammatically incompatible with the active mention, and then ranking
the preceding clusters, which contain the mentions that survive the filtering process, using
salience factors. Examples of salience factors include sentence recency (whether the preceding cluster contains a mention that appears in the sentence currently being processed),
subject emphasis (whether the cluster contains a mention in the subject position), existential emphasis (whether the cluster contains a mention that is a predicate nominal in an
existential construction), and accusative emphasis (whether the cluster contains a mention
that appears in a verbal complement in accusative case). Each salience factor is associated
with a manually-assigned weight that indicates its importance relative to other factors, and
the score of a cluster is the sum of the weights of the salience factors that are applicable to
the cluster. While Lappin and Leass’s paper is a widely read paper on pronoun resolution,
the cluster ranking aspect of their algorithm has rarely been emphasized. In fact, we are
not aware of any recent work on learning-based coreference resolution that establishes the
connection between the entity-mention model and Lappin and Leass’s algorithm.
Despite the conceptual similarities, our cluster-ranking model and Lappin and Leass’s
(1994) algorithm differ in several respects. First, Lappin and Leass only tackle pronoun resolution rather than the full coreference task. Second, while they apply linguistic constraints
to filter incompatible candidate antecedents, our resolution strategy is learned without applying hand-coded constraints in a separate filtering step. Third, while they attempt to
compute the salience of a preceding cluster with respect to an active mention, we attempt
to determine the compatibility between a cluster and an active mention, using factors that
determine not only salience but also lexical and grammatical compatibility, for instance.
Finally, their algorithm is heuristic-based, where the weights associated with each salience
factor are encoded manually rather than learned, unlike our system.
The first paper on learning-based coreference resolution was written by Connolly, Burger,
and Day (1994) and was published in the same year as Lappin and Leass’s (1994) paper.
Contrary to common expectation, the coreference model this paper proposes is a rankingbased model, not the influential mention-pair model. The main idea behind Connolly et
al.’s approach is to convert a problem of ranking N candidate antecedents into a set of
pairwise ranking problems, each of which involves ranking exactly two candidates. To
rank two candidates, a classifier can be trained using a training set where each instance
corresponds to the active mention as well as two candidate antecedents and possesses a
class value that indicates which of the two candidates is better. This idea is certainly ahead
of its time, as it is embodied in many of the advanced ranking algorithms developed in
the machine learning and information retrieval communities in the past few years. It is
475

Rahman & Ng

later re-invented at almost the same time, but independently, by Yang et al. (2003) and
Iida, Inui, Takamura, and Matsumoto (2003), who refer to it as the twin-candidate model
and the tournament model, respectively. The name twin-candidate model is motivated by
the fact that the model considers two candidates at a time, whereas the name tournament
model was assigned because each ranking of two candidates can be viewed as a tournament
(with the higher-ranked candidate winning the tournament) and the candidate that wins
the largest number of tournaments is chosen as the antecedent for the active mention. This
bit of history is rarely mentioned in the literature, but it reveals three somewhat interesting
and perhaps surprising facts. First, ranking was first applied to train coreference models
much earlier than people typically think. Second, despite being the first learning-based
coreference model, Connolly et al.’s ranking-based model is theoretically more appealing
than the classification-based mention-pair model, and is later shown by Yang et al. and
Iida et al.. to be empirically better as well. Finally, despite its theoretical and empirical
superiority, Connolly et al.’s model was largely ignored by the NLP community and received
attention only when it was re-invented nearly a decade later, while during this time period
its mention-pair counterpart essentially dominated learning-based coreference research.4
We conclude this section by making the important observation that the distinction between classification and ranking applies to discriminative models but not generative models.
Generative models try to capture the true conditional probability of some event. In the context of coreference resolution, this will be the probability of a mention having a particular
antecedent or of it referring to a particular entity (i.e., preceding cluster). Since these probabilities have to normalize, this is similar to a ranking objective: the system is trying to raise
the probability that a mention refers to the correct antecedent or entity at the expense of
the probabilities that it refers to any other. Thus, the antecedent version of the generative
coreference model as proposed by Ge et al. (1998) resembles the mention-ranking model,
while the entity version as proposed by Haghighi and Klein (2010) is similar in spirit to the
cluster-ranking model.

3. Baseline Coreference Models
In this section, we describe three coreference models that will serve as our baselines: the
mention-pair model, the entity-mention model, and the mention-ranking model. For illustrative purposes, we will use the text segment shown in Figure 1. Each mention m in the
segment is annotated as [m]cid
mid , where mid is the mention id and cid is the id of the cluster
to which m belongs. As we can see, the mentions are partitioned into four sets, with Barack
Obama, his, and he in one cluster, and each of the remaining mentions in its own cluster.

4. It may not be possible (and perhaps not crucial) to determine why the mention-pair model received a
lot more attention than Connolly et al.’s model, but since those were the days when academic papers
could not be accessed easily in electronic form, we speculate that the publication venue played a role:
Connolly et al.’s work was published in the New Methods in Language Processing conference in 1994
(and later as a book chapter in 1997), whereas the mention-pair model was introduced in Aone and
Bennett’s (1995) paper and McCarthy and Lehnert’s (1995) paper, which appeared in the proceedings
of two comparatively higher-profile AI conferences: ACL 1995 and IJCAI 1995.

476

A Cluster-Ranking Approach to Coreference Resolution

[Barack Obama]11 nominated [Hillary Rodham Clinton]22 as [[his]13 secretary of state]34 on [Monday]45 .
[He]16 ...

Figure 1: An illustrative example
3.1 Mention-Pair Model
As noted before, the mention-pair model is a classifier that decides whether or not an
active mention mk is coreferent with a candidate antecedent mj . Each instance i(mj , mk )
represents mj and mk . In our implementation, an instance consists of the 39 features shown
in Table 1. These features have largely been employed by state-of-the-art learning-based
coreference systems (e.g., Soon, Ng, & Lim, 2001; Ng & Cardie, 2002b; Bengtson & Roth,
2008), and are computed automatically. As can be seen, the features are divided into four
blocks. The first two blocks consist of features that describe the properties of mj and mk ,
respectively, and the last two blocks of features describe the relationship between mj and
mk . The classification associated with a training instance is either positive or negative,
depending on whether mj and mk are coreferent.
If one training instance were created from each pair of mentions, the negative instances
would significantly outnumber the positives, yielding a skewed class distribution that will
typically have an adverse effect on model training. As a result, only a subset of mention
pairs will be generated for training. Following Soon et al. (2001), we create (1) a positive
instance for each anaphoric mention mk and its closest antecedent mj ; and (2) a negative
instance for mk paired with each of the intervening mentions, mj+1 , mj+2 , . . . , mk−1 . In
our running example shown in Figure 1, three training instances will be generated for He:
i(Monday, He), i(secretary of state, He), and i(his, He). The first two of these instances will
be labeled as negative, and the last one will be labeled as positive. To train the mention-pair
model, we use the SVM learning algorithm from the SVMlight package (Joachims, 1999).5
As mentioned in the introduction, while previous work on learning-based coreference
resolution typically treats the underlying machine learner simply as a black-box tool, we
choose to provide the reader with an overview of SVMs, the learner we are employing in our
work. Note that this is a self-contained overview, but it is by no means a comprehensive
introduction to maximum-margin learning: our goal here is to provide the reader with only
the details that we believe are needed to understand the difference between classification
and ranking and perhaps appreciate the importance of ranking.6
To begin with, assume that we are given a data set consisting of positively labeled
points, which have a class value of +1, and negatively labeled points, which have a class
5. Since SVMlight assumes real-valued features, it cannot operate on features with multiple discrete values
directly. Hence, we need to convert the features shown in Table 1 into an equivalent set of features
that can be used directly by SVMlight . For uniformity, we perform the conversion for each feature in
Table 1 (rather than just the multi-valued features) as follows: we create one binary-valued feature for
SVMlight from each feature-value pair that can be derived from the feature set in Table 1. For example,
pronoun 1 has two values, Y and N. So we will derive two binary-valued features, pronoun 1=Y and
pronoun 1=N. One of them will have a value of 1 and the other will have a value of 0 for each instance.
6. For an overview of the theory of maximum-margin learning, we refer the reader to Burges’s (1998)
tutorial.

477

Rahman & Ng

Features describing mj , a candidate antecedent
1 pronoun 1
Y if mj is a pronoun; else N
2 subject 1
Y if mj is a subject; else N
3 nested 1
Y if mj is a nested NP; else N
Features describing mk , the mention to be resolved
4 number 2
singular or plural, determined using a lexicon
male, female, neuter, or unknown, determined using a list of
5 gender 2
common first names
Y if mk is a pronoun; else N
6 pronoun 2
7 nested 2
Y if mk is a nested NP; else N
8 semclass 2
the semantic class of mk ; can be one of person, location, organization, date, time, money, percent, object, others, determined using WordNet (Fellbaum, 1998) and the Stanford NE recognizer (Finkel, Grenager, & Manning, 2005)
9 animacy 2
Y if mk is determined as human or animal by WordNet and an NE
recognizer; else N
the nominative case of mk if it is a pronoun; else NA. E.g., the
10 pro type 2
feature value for him is he
Features describing the relationship between mj , a candidate antecedent and mk ,
the mention to be resolved
11 head match
C if the mentions have the same head noun; else I
12 str match
C if the mentions are the same string; else I
13 substr match
C if one mention is a substring of the other; else I
C if both mentions are pronominal and are the same string; else I
14 pro str match
15 pn str match
C if both mentions are proper names and are the same string; else I
16 nonpro str match C if the two mentions are both non-pronominal and are the same
string; else I
17 modifier match
C if the mentions have the same modifiers; NA if one of both of them
don’t have a modifier; else I
C if both mentions are pronominal and are either the same pronoun
18 pro type match
or different only with respect to case; NA if at least one of them is
not pronominal; else I
19 number
C if the mentions agree in number; I if they disagree; NA if the
number for one or both mentions cannot be determined
20 gender
C if the mentions agree in gender; I if they disagree; NA if the gender
for one or both mentions cannot be determined
21 agreement
C if the mentions agree in both gender and number; I if they disagree
in both number and gender; else NA
22 animacy
C if the mentions match in animacy; I if they don’t; NA if the
animacy for one or both mentions cannot be determined
C if both mentions are pronouns; I if neither are pronouns; else NA
23 both pronouns
24 both proper nounsC if both mentions are proper nouns; I if neither are proper nouns;
else NA
25 maximalnp
C if the two mentions does not have the same maximial NP projection; else I
26 span
C if neither mention spans the other; else I
27 indefinite
C if mk is an indefinite NP and is not in an appositive relationship;
else I
28 appositive
C if the mentions are in an appositive relationship; else I
29 copular
C if the mentions are in a copular construction; else I

478

A Cluster-Ranking Approach to Coreference Resolution

Features describing the relationship between mj , a candidate antecedent and mk ,
the mention to be resolved (continued from the previous page)
30 semclass
C if the mentions have the same semantic class (where the set of
semantic classes considered here is enumerated in the description of
the semclass 2 feature); I if they don’t; NA if the semantic class
information for one or both mentions cannot be determined
31 alias
C if one mention is an abbreviation or an acronym of the other; else
I
32 distance
binned values for sentence distance between the mentions
Additional features describing the relationship between mj , a candidate antecedent
and mk , the mention to be resolved
33 number’
the concatenation of the number 2 feature values of mj and mk .
E.g., if mj is Clinton and mk is they, the feature value is singularplural, since mj is singular and mk is plural
34 gender’
the concatenation of the gender 2 feature values of mj and mk
35 pronoun’
the concatenation of the pronoun 2 feature values of mj and mk
36 nested’
the concatenation of the nested 2 feature values of mj and mk
37 semclass’
the concatenation of the semclass 2 feature values of mj and mk
38 animacy’
the concatenation of the animacy 2 feature values of mj and mk
the concatenation of the pro type 2 feature values of mj and mk
39 pro type’

Table 1: Feature set for coreference resolution. Non-relational features describe a mention
and in most cases take on a value of Yes or No. Relational features describe the relationship
between the two mentions and indicate whether they are Compatible, Incompatible or
Not Applicable.
value of −1. When used in classification mode, an SVM learner aims to learn a hyperplane
(i.e., a linear classifier) that separates the positive points from the negative points. If
there is more than one hyperplane that achieves zero training error, the learner will choose
the hyperplane that maximizes the margin of separation (i.e., the distance between the
hyperplane and the training example closest to it), as a larger margin can be proven to
provide better generalization on unseen data (Vapnik, 1995). More formally, a maximum
margin hyperplane is defined by w · x − b = 0, where x is a feature vector representing
an arbitrary data point, and w (a weight vector) and b (a scalar) are parameters that are
learned by solving the following constrained optimization problem:
Optimization Problem 1: Hard-Margin SVM for Classification
arg min
subject to

1
kwk2
2

yi (w · xi − b) ≥ 1,

1 ≤ i ≤ n,

where yi ∈ {+1, −1} is the class of the i-th training point xi . Note that for each data point
xi , there is exactly one linear constraint in this optimization problem that ensures xi is
correctly classified. In particular, using a value of 1 on the right side of each inequality
479

Rahman & Ng

constraint ensures a certain distance (i.e., margin) between each xi and the hyperplane. It
can be shown that the margin is inversely proportional to the length of the weight vector.
Hence, minimizing the length of the weight vector is equivalent to maximizing the margin.
The resulting SVM classifier is known as a hard-margin SVM: the margin is “hard” because
each data point has to be on the correct side of the hyperplane.
However, in cases where the data set is not linearly separable, there is no hyperplane
that can perfectly separate the positives from the negatives, and as a result, the above
constrained optimization problem does not have a solution. Instead of asking the SVM
learner to give up and return no solution, we solve a relaxed version of the problem where
we also consider hyperplanes that produce non-zero training errors as potential solutions.
In other words, we have to modify the linear constraints associated with each data point
so that training errors are allowed. However, if we only modify the linear constraints but
leave the objective function as it is, then the learner will only search for a maximum-margin
hyperplane regardless of the training error it produces. Since training error correlates
positively with generalization error, it is crucial for the objective function to also take into
consideration the training error so that a hyperplane with a large margin and a low training
error can be found. However, it is non-trivial to maximize the margin and minimize the
training error simultaneously, since training error typically increases as we maximize the
margin. As a result, we need to find a trade-off between these two criteria, resulting in
an objective function that is a linear combination of margin size and training error. More
formally, we find the optimal hyperplane by solving the following constrained optimization
problem:
Optimization Problem 2: Soft-Margin SVM for Classification
arg min

X
1
kwk2 + C
ξi
2
i

subject to
yi (w · xi − b) ≥ 1 − ξi , 1 ≤ i ≤ n.
As before, yi ∈ {+1, −1} is the class of the i-th training point xi . C is a regularization
parameter that balances training error and margin size. Finally, ξi is a non-negative slack
variable that represents the degree of misclassification of xi ; in particular, if ξi > 1, then
data point i is on the wrong side of the hyperplane. Because this SVM allows data points
to appear on the wrong side of the hyperplane, it is also known as a soft-margin SVM.
Given this optimization problem, we rely on the training algorithm employed by SVMlight
for finding the optimal hyperplane.
After training, the resulting SVM classifier is used by a clustering algorithm to identify
an antecedent for a mention in a test text. Specifically, each active mention is compared in
turn to each preceding mention. For each pair, a test instance is created as during training
and presented to the SVM classifier, which returns a value that indicates the likelihood that
the two mentions are coreferent. Mention pairs with class values above 0 are considered
coreferent; otherwise the pair is considered not coreferent. Following Soon et al. (2001), we
apply a closest-first linking regime for antecedent selection: given an active mention mk ,
480

A Cluster-Ranking Approach to Coreference Resolution

we select as its antecedent the closest preceding mention that is classified as coreferent with
mk . If mk is not classified as coreferent with any preceding mention, it will be considered
non-anaphoric (i.e., no antecedent will be selected for mk ).
3.2 Entity-Mention Model
Unlike the mention-pair model, the entity-mention model is a classifier that decides whether
or not an active mention mk belongs to a partial coreference cluster cj that precedes mk .
Each training instance, i(cj , mk ), represents cj and mk . The features for an instance can be
divided into two types: (1) features that describe mk (i.e, those shown in the second block
of Table 1), and (2) cluster-level features, which describe the relationship between cj and
mk . A cluster-level feature can be created from a feature employed by the mention-pair
model by applying a logical predicate. For example, given the number feature (i.e., feature
#19 in Table 1), which determines whether two mentions agree in number, we can apply
the all predicate to create a cluster-level feature that has the value yes if mk agrees in
number with all of the mentions in cj and no otherwise. Motivated by previous work (Luo
et al., 2004; Culotta, Wick, & McCallum, 2007; Yang et al., 2008), we create cluster-level
features from mention-pair features using four commonly-used logical predicates: none,
most-false, most-true, and all. Specifically, for each feature x shown in the last two
blocks in Table 1, we first convert x into an equivalent set of binary-valued features if it
is multi-valued. Then, for each resulting binary-valued feature xb , we create four binaryvalued cluster-level features: (1) none-xb is true when xb is false between mk and each
mention in cj ; (2) most-false-xb is true when xb is true between mk and less than half
(but at least one) of the mentions in cj ; (3) most-true-xb is true when xb is true between
mk and at least half (but not all) of the mentions in cj ; and (4) all-xb is true when xb
is true between mk and each mention in cj . Hence, for each xb , exactly one of these four
cluster-level features evaluates to true.7
Following Yang et al. (2008), we create (1) a positive instance for each anaphoric mention
mk and the preceding cluster cj to which it belongs; and (2) a negative instance for mk
paired with each preceding cluster whose last mention appears between mk and its closest
antecedent (i.e., the last mention of cj ). Consider again our running example. Three
training instances will be generated for He: i({Monday}, He), i({secretary of state}, He),
and i({Barack Obama, his}, He). The first two of these instances will be labeled as negative,
and the last one will be labeled as positive. As in the mention-pair model, we train the
entity-mention model using the SVM learner.
Since the entity-mention model is a classifier, we will again use SVMlight in classification
mode, resulting in a constrained optimization problem that is essentially the same as Optimization Problem 2, except that each training example xi represents an active mention
and one of its preceding clusters rather than two mentions.
7. Note that a cluster-level feature can also be represented as a probabilistic feature. Specifically, recall that
the four logical predicates partitions the [0,1] interval. Which predicate evaluates to true for a given
cluster-level feature depends on the probability obtained during the computation of the feature. Instead
of applying the logical predicates to convert the probability into one of the four discrete values, we can
simply use the probability as the value of the cluster-level feature. However, we choose not to employ
this probabilistic representation, as preliminary experiments indicated that using probabilistic features
yielded slightly worse results than using logical features.

481

Rahman & Ng

After training, the resulting classifier is used to identify a preceding cluster for a mention
in a test text. Specifically, the mentions are processed in a left-to-right manner. For each
active mention mk , a test instance is created between mk and each of the preceding clusters
formed so far. All the test instances are then presented to the classifier. Finally, we adopt
a closest-first clustering regime, linking mk to the closest preceding cluster that is classified
as coreferent with mk . If mk is not classified as coreferent with any preceding cluster, it
will be considered non-anaphoric. Note that all partial clusters preceding mk are formed
incrementally based on the predictions of the classifier for the first k − 1 mentions; no
gold-standard coreference information is used in their formation.
3.3 Mention-Ranking Model
As noted before, a ranking model imposes a ranking on all the candidate antecedents of an
active mention mk . To train the ranking-model, we use the SVM ranker-learning algorithm
from Joachims’s (2002) SVMlight package.
Like the mention-pair model, each training instance i(mj , mk ) represents mk and a
preceding mention mj . In fact, the features that represent an instance and the method
for creating training instances are identical to those employed by the mention-pair model.
The only difference lies in labeling the training instances. Assuming that Sk is the set of
training instances created for anaphoric mention mk , the rank value for i(mj , mk ) in Sk
is the rank of mj among competing candidate antecedents, which is 2 if mj is the closest
antecedent of mk , and 1 otherwise.8 To exemplify, consider again our running example. As
in the mention-pair model, three training instances will be generated for He: i(Monday,
He), i(secretary of state, He), i(his, He). The third instance will have a rank value of 2,
and the remaining two will have a rank value of 1.
At first glance, it seems that the training set that is generated for learning the mentionranking model, is identical to the one for learning the mention-pair model, as each instance
represents two mentions and is labeled with one of two possible values. Since previous work
on ranking-based coreference resolution does not attempt to clarify the difference between
the two, we believe that it could be difficult for the reader to appreciate the idea of using
ranking for coreference resolution.
Let us first describe the difference between classification and ranking at a high level,
beginning with the training sets employed by the mention-ranking model and the mentionpair model. The difference is that the label associated with each instance for training the
mention-ranking model is a rank value, whereas the label associated with each instance for
training the mention-pair model is a class value. More specifically, since a ranking SVM
learns to rank a set of candidate antecedents, it is the relative ranks between two candidates,
rather than the absolute rank of a candidate, that matter in the training process. In other
words, from the point of view of the ranking SVM, a training set where instance #1 has
a rank value of 2 and instance #2 has a rank value of 1 is functionally equivalent to one
where #1 has a rank value of 10 and #2 has a rank value of 5, assuming that the remaining
instances generated for the same anaphor in the two training sets are identical to each other
and do not have a rank value between 1 and 10.
8. A larger rank value implies a better rank in SVMlight .

482

A Cluster-Ranking Approach to Coreference Resolution

Next, we take a closer look at the ranker-training process. We denote the training set
that is created as described above by T . In addition, we assume that an instance in T
is denoted by (xjk , yjk ), where xjk is the feature vector created from anaphoric mention
mk and candidate antecedent mj , and yjk is its rank value. Before training a ranker, the
SVM ranker-learning algorithm derives a training set T ′ from the original training set T as
follows. Specifically, for every pair of training instances (xik , yik ) and (xjk , yjk ) in T where
yik 6= yjk , we create a new training instance (xijk , yijk ) for T ′ , where xijk = xik − xjk , and
yijk ∈ {+1, −1} is 1 if xik has a larger rank value than xjk (and −1 otherwise). In a way,
the creation of T ′ resembles Connolly et al.’s (1994) pairwise ranking approach that we saw
in Section 2, where we convert a ranking problem into a pairwise classification problem.9
The goal of the ranker-learning algorithm, then, is to find a hyperplane that minimizes the
number of misclassifications in T ′ . Note that since yijk ∈ {+1, −1}, the class value of an
instance in T ′ depends only on the relative ranks of two candidate antecedents, not their
absolute rank values.
Given the conversion from a ranking problem to a pairwise classification problem, the
constrained optimization problem that the SVM ranker-learning algorithm attempts to
solve, as described below, is similar to Optimization Problem 2:
Optimization Problem 3: Soft-Margin SVM for Ranking
X
1
ξijk
arg min kwk2 + C
2
subject to
yijk (w · (xik − xjk ) − b) ≥ 1 − ξijk ,
where ξijk is a non-negative slack variable that represents the degree of misclassification of
xijk , and C is a regularization parameter that balances training error and margin size.
Two points deserve mention. First, this optimization problem is equivalent to the one
for a classification SVM on pairwise difference feature vectors xik − xjk . As a result, the
training algorithm that was used to solve Optimization Problem 2 is also applicable
to this optimization problem. Second, while the number of linear inequality constraints
generated from document d in the optimization problems for training the mention-pair
model and the entity-mention model is quadratic in the number of mentions in d, the
number of constraints generated for a ranking SVM is cubic in the number of mentions,
since each instance now represents three (rather than two) mentions.
After training, the mention-ranking model is applied to rank the candidate antecedents
for an active mention in a test text as follows. Given an active mention mk , we follow Denis
and Baldridge (2008) and use an independently-trained classifier to determine whether mk
is non-anaphoric. If so, mk will not be resolved. Otherwise, we create test instances for mk
by pairing it with each of its preceding mentions. The test instances are then presented to
the ranker, which computes a rank value for each instance by taking the dot product of the
9. The main difference between T ′ and the training set employed by Connolly et al.’s approach is that in
T ′ , each instance is formed by taking the difference of the feature vectors of two instances in T , whereas
in Connolly et al.’s training set, each instance is formed by concatenating the feature vectors of two
instances in T .

483

Rahman & Ng

instance vector and the weight vector. The preceding mention that is assigned the largest
value by the ranker is selected as the antecedent of mk . Ties are broken by preferring the
antecedent that is closest in distance to mk .
The anaphoricity classifier used in the resolution step is trained using a publicly-available
implementation10 of maximum entropy (MaxEnt) modeling. Each instance corresponds
to a mention and is represented by 26 features that are deemed useful for distinguishing
between anaphoric and non-anaphoric mentions (see Table 2 for details). Linguistically,
these features can be broadly divided into three types: string-matching, grammatical, and
semantic. Each of them is either a relational feature, which compares a mention to one of its
preceding mentions, or a non-relational feature, which encodes certain linguistic property of
the mention whose anaphoricity is to be determined (e.g., NP type, number, definiteness).

4. Coreference as Cluster Ranking
In this section, we describe our cluster-ranking approach to NP coreference. As noted
before, our approach aims to combine the strengths of the entity-mention model and the
mention-ranking model.
4.1 Training and Applying a Cluster Ranker
For ease of exposition, we will describe in this subsection how to train and apply the clusterranking model when it is used in a pipeline architecture, where anaphoricity determination
is performed prior to coreference resolution. In the next subsection, we will show how the
two tasks can be learned jointly.
Recall that the cluster-ranking model ranks a set of preceding clusters for an active
mention mk . Since the cluster-ranking model is a hybrid of the mention-ranking model
and the entity-mention model, the way it is trained and applied is also a hybrid of the
two. In particular, the instance representation employed by the cluster-ranking model is
identical to that used by the entity-mention model, where each training instance i(cj , mk )
represents a preceding cluster cj and an anaphoric mention mk and consists of clusterlevel features formed from predicates. Unlike in the entity-mention model, however, in the
cluster-ranking model, (1) a training instance is created between each anaphoric mention mk
and each of its preceding clusters; and (2) since we are training a model for ranking clusters,
the assignment of rank values to training instances is similar to that of the mention-ranking
model. Specifically, the rank value of a training instance i(cj , mk ) created for mk is the
rank of cj among the competing clusters, which is 2 if mk belongs to cj , and 1 otherwise.
To train the cluster-ranking model, we use the SVM learner in ranking mode, resulting in
a constrained optimization problem that is essentially the same as Optimization Problem
3, except that each training example xijk represents an active mention mk and two of its
preceding clusters, ci and cj , rather than two of its preceding mentions.
Applying the learned cluster ranker to a test text is similar to applying the mentionranking model. Specifically, the mentions are processed in a left-to-right manner. For each
active mention mk , we first apply an independently-trained classifier to determine if mk is
non-anaphoric. If so, mk will not be resolved. Otherwise, we create test instances for mk by
10. See http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.

484

A Cluster-Ranking Approach to Coreference Resolution

Feature Type
Lexical

Feature
str match

head match

Grammatical
(NP type)

uppercase
definite
demonstrative
indefinite
quantified
article

Grammatical
(NP
property/
relationship

pronoun
proper noun
bare singular
bare plural
embedded
appositive
prednom
number

contains pn
Grammatical
(Syntactic
Pattern)

the n
the 2n
the pn
the pn n
the adj n
the num n
the ne
the sing n

Semantic

alias

Description
Y if there exists a mention mj preceding mk such that, after
discarding determiners, mj and mk are the same string; else
N.
Y if there exists a mention mj preceding mk such that mj and
mk have the same head; else N.
Y if mk is entirely in uppercase; else N.
Y if mk starts with “the”; else N.
Y if mk starts with a demonstrative such as “this”, “that”,
“these”, or “those”; else N.
Y if mk starts with “a” or “an”; else N.
Y if mk starts with quantifiers such as “every”, “some”, “all”,
“most”, “many”, “much”, “few”, or “none”; else N.
definite if mk is a definite NP; quantified if mk is a quantified NP; else indefinite.
Y if mk is a pronoun; else N.
Y if mk is a proper noun; else N.
Y if mk is singular and does not start with an article; else N.
Y if mk is plural and does not start with an article; else N.
Y if mk is a prenominal modifier; else N.
Y if mk is the first of the two mentions in an appositive
construction; else N.
Y if mk is the first of the two mentions in a predicate nominal
construction; else N.
singular if mk is singular in number; plural if mk is plural
in number; unknown if the number information cannot be
determined.
Y if mk is not a proper noun but contains a proper noun; else
N.
Y if mk starts with “the” followed exactly by one common
noun; else N.
Y if mk starts with “the” followed exactly by two common
nouns; else N.
Y if mk starts with “the” followed exactly by a proper noun;
else N.
Y if mk starts with “the” followed exactly by a proper noun
and a common noun; else N.
Y if mk starts with “the” followed exactly by an adjective and
a common noun; else N.
Y if mk starts with “the” followed exactly by a cardinal and a
common noun; else N.
Y if mk starts with “the” followed exactly by a named entity;
else N.
Y if mk starts with “the” followed by a singular NP not containing any proper noun; else N.
Y if there exists a mention mj preceding mk such that mj and
mk are aliases; else N.

Table 2: Feature set for anaphoricity determination. Each instance represents a single mention,
mk , characterized by 26 features.
485

Rahman & Ng

pairing it with each of its preceding clusters. The test instances are then presented to the
ranker, and mk is linked to the cluster that is assigned the highest value by the ranker. Ties
are broken by preferring the cluster whose last mention is closest in distance to mk . Note
that these partial clusters preceding mk are formed incrementally based on the predictions
of the ranker for the first k − 1 mentions.
4.2 Joint Anaphoricity Determination and Coreference Resolution
The cluster ranker described above can be used to determine which preceding cluster an
anaphoric mention should be linked to, but it cannot be used to determine whether a mention is anaphoric or not. The reason is simple: all the training instances are generated from
anaphoric mentions. Hence, to jointly learn anaphoricity determination and coreference
resolution, we must train the ranker using instances generated from both anaphoric and
non-anaphoric mentions.
Specifically, when training the ranker, we provide each active mention with the option to
start a new cluster by creating an additional instance that (1) contains features that solely
describe the active mention (i.e., the features shown in the second block of Table 1), and (2)
has the highest rank value among competing clusters (i.e., 2) if it is non-anaphoric and the
lowest rank value (i.e., 1) otherwise. The main advantage of jointly learning the two tasks
is that it allows the ranking model to evaluate all possible options for an active mention
(i.e., whether to resolve it, and if so, which preceding cluster is the best) simultaneously.
Essentially the same method can be applied to jointly learn the two tasks for the mentionranking model.
After training, the resulting cluster ranker processes the mentions in a test text in a
left-to-right manner. For each active mention mk , we create test instances for it by pairing
it with each of its preceding clusters. To allow for the possibility that mk is non-anaphoric,
we create an additional test instance that contains features that solely describe the active
mention (similar to what we did in the training step above). All these test instances are then
presented to the ranker. If the additional test instance is assigned the highest rank value
by the ranker, then mk is classified as non-anaphoric and will not be resolved. Otherwise,
mk is linked to the cluster that has the highest rank, with ties broken by preferring the
antecedent that is closest to mk . As before, all partial clusters preceding mk are formed
incrementally based on the predictions of the ranker for the first k − 1 mentions.
Finally, we note that our model for jointly learning anaphoricity determination and coreference resolution is different from recent attempts to perform joint inference for anaphoricity
determination and coreference resolution using integer linear programming (ILP), where an
anaphoricity classifier and a coreference classifier are trained independently of each other,
and then ILP is applied as a postprocessing step to jointly infer anaphoricity and coreference decisions so that they are consistent with each other (e.g., Denis & Baldridge, 2007a).
Joint inference is different from our joint-learning approach, which allows the two tasks to
be learned jointly and not independently.

5. Lexicalization for Coreference Resolution
Next, we investigate the role of lexicalization (i.e., the use of word pairs as linguistic features)
in learning-based coreference resolution. The motivation behind our investigation is two486

A Cluster-Ranking Approach to Coreference Resolution

fold. First, lexical features are very easy to compute and yet they are under-investigated
in coreference resolution. In particular, only a few attempts have been made to employ
them to train the mention-pair model (e.g., Luo et al., 2004; Daumé III & Marcu, 2005;
Bengtson & Roth, 2008). In contrast, we want to determine whether they can improve
the performance of our cluster-ranking model. Second, the mention-pair model and the
mention-ranking model have only been compared with respect to a non-lexical feature set
(Denis & Baldridge, 2007b, 2008), so it is not clear how they will perform relative to each
other when they are trained on lexical features. We desire an answer to this question,
as it will allow us to gain additional insights into the strengths and weaknesses of these
learning-based coreference models.
Recall from the introduction that previous attempts on lexicalizing the mention-pair
model show that lexical features are at best marginally useful. Hence, one of our goals
here is to determine whether we can make better use of lexical features for a learning-based
coreference resolver. In particular, unlike the aforementioned attempts on lexicalization,
which simply append all word pairs to a “conventional” coreference feature set consisting of
string-matching, grammatical, semantic, and distance (i.e., proximity-based) features (e.g.,
the feature set shown in Table 1), we investigate a model that exploits lexical features
in combination with only a small subset of these conventional coreference features. This
would allow us to have a better understanding of the significance of these conventional
features. For example, features that encode agreement on gender, number, and semantic
class between two mentions are employed by virtually all learning-based coreference resolver,
but we never question whether there are better alternatives to these features. If we could
build a lexicalized coreference model without these commonly-used features and did not
observe any performance deterioration, it would imply that these conventional features were
replaceable, and that there was no prototypical way of building a learning-based coreference
system.
The question is: what is the small subset of conventional features that we should use in
combination with the lexical features? As mentioned above, since one of the advantages of
lexical features is that they are extremely easy to compute, we desire only those conventional
features that are also easy to compute, especially those that do not require a dictionary to
compute. As we will see, we choose to use only two features, the alias feature and the
distance feature (see features 31 and 32 in Table 1), and rely on an off-the-shelf named
entity (NE) recognizer to compute NE types.
Note, however, that the usefulness of lexical features could be limited in part by data
sparseness: many word pairs that appear in the training data may not appear in the test
data. While employing some of the conventional features described above (e.g., distance)
will help alleviate this problem, we seek to further improve generalizability by introducing
two types of features: semi-lexical and unseen features. We will henceforth refer to the
feature set that comprises these two types of features, the lexical features, the alias feature,
and the distance feature as the Lexical feature set. In addition, we will refer to the feature
set shown in Table 1 as the Conventional feature set.
Below we first describe the Lexical feature set for training the mention-pair model and
the mention-ranking model (Section 5.1). After that, we show how to create cluster-level
features from this feature set for training the entity-mention model and the cluster-ranking
487

Rahman & Ng

model, as well as issues in training a joint model for anaphoricity determination and coreference resolution (Section 5.2).
5.1 Lexical Feature Set
Unlike previous work on lexicalizing learning-based coreference models, our Lexical feature
set consists of four types of features: lexical features, semi-lexical features, unseen features,
as well as two “conventional” features (namely, alias and distance).
To compute these features, we preprocess a training text by randomly replacing 10%
of its nominal mentions (i.e., common nouns) with the label unseen. If a mention mk is
replaced with unseen, all mentions that have the same string as mk will also be replaced
with unseen. A test text is preprocessed differently: we simply replace all mentions whose
strings are not seen in the training data with unseen. Hence, artificially creating unseen
labels from a training text will allow a learner to learn how to handle unseen words in a
test text, potentially improving generalizability.
After preprocessing, we can compute the features for an instance. Assuming that we are
training the mention-pair model or the mention-ranking model, each instance corresponds
to two mentions, mj and mk , where mj precedes mk in the text. The features can be
divided into four groups: unseen, lexical, semi-lexical, and conventional. Before describing
these features, two points deserve mention. First, if at least one of mj and mk is unseen,
no lexical, semi-lexical, or conventional features will be created for them, since features
involving an unseen mention are likely to be misleading for a learner in the sense that they
may yield incorrect generalizations from the training set. Second, since we use an SVM for
training and testing, each instance can contain any number of features, and unless otherwise
stated, a feature has the value 1.
Unseen feature. If both mj and mk are unseen, we determine whether they are the
same string. If so, we create an unseen-same feature; otherwise, we create an unseendiff feature. If only one of them is unseen, no feature will be created.
Lexical feature. We create a lexical feature between mj and mk , which is an ordered
pair consisting of the heads of the mentions. For a pronoun or a common noun, the head
is assumed to be the last word of the mention11 ; for a proper noun, the head is taken to be
the entire noun phrase.
Semi-lexical features. These features aim to improve generalizability. Specifically, if
exactly one of mj and mk is tagged as an NE by the Stanford NE recognizer (Finkel et al.,
2005), we create a semi-lexical feature that is identical to the lexical feature described above,
except that the NE is replaced with its NE label (i.e., person, location, organization).
If both mentions are NEs, we check whether they are the same string. If so, we create the
feature *ne*-same, where *ne* is replaced with the corresponding NE label. Otherwise,
we check whether they have the same NE tag and a word-subset match (i.e., whether all
11. As we will see in the evaluation section, our mention extractor is trained to extract base NPs. Hence,
while our heuristic for extracting head nouns is arguably overly simplistic, it will not be applied to
recursive NPs (e.g., NPs that contain prepositional phrases), which are phrases on which it is likely to
make mistakes. However, if we desire a better extraction accuracy, we can extract the head nouns from
syntactic parsers that provide head information, such as Collins’s (1999) parser.

488

A Cluster-Ranking Approach to Coreference Resolution

word tokens in one mention appear in the other’s list of tokens). If so, we create the feature
*ne*-subsame, where *ne* is replaced with their NE label. Otherwise, we create a feature
that is the concatenation of the NE labels of the two mentions.
Conventional features. To further improve generalizability, we incorporate two easy-to
compute features from the Conventional feature set: alias and distance.
5.2 Feature Generation
Now that we have a Lexical feature set for training the mention-pair model and the mentionranking model, we can describe the two extensions to this feature set that are needed to
(1) train the entity-mention model and the cluster-ranking model, and (2) perform joint
learning for anaphoricity determination and coreference resolution.
The first extension concerns the generation of cluster-level features for the entity-mention
model and the cluster-level model. Recall from Section 3.2 that to create cluster-level features given the Conventional feature set, we first convert each feature employed by the
mention-pair model into an equivalent set of binary-valued features, and then create a
cluster-level feature from each of the resulting binary-valued features. On the other hand,
given the Lexical feature set, this method of producing cluster-level features is only applicable to the two “conventional” features (i.e., alias and distance), as they also appear
in the Conventional feature set. For an unseen, lexical, or semi-lexical feature, we create a
feature between the active mention and each mention in the preceding cluster, as described
in Section 5.112 , and the value of this feature is the number of times it appears in the instance. Encoding feature values as frequency rather than binary values allows us to capture
cluster-level information in a shallow manner.
The second extension concerns the generation of features for representing the additional
instance that is created when training the joint version of the mention-ranking model and
the cluster-ranking model. Recall from Section 4.2 that when the Conventional feature set
was used, we represented this additional instance using features that were computed solely
from the active mention. On the other hand, given the Lexical feature set, we can no longer
use the same method for representing this additional instance, as there is no feature in
the Lexical feature set that is computed solely from the active mention. As a result, we
represent this additional instance using just one feature, null-x, where x is the head of the
active mention, to help the learner learn that x is likely to be non-anaphoric.

6. Evaluation
Our evaluation is driven by the following questions, focusing on (1) the comparison among
different learning-based coreference models, and (2) the effect of lexicalization on these
models. Specifically:
• How do the learning-based coreference models (namely, the mention-pair model, the
entity-mention model, the mention-ranking model, and our cluster-ranking model)
compare with each other?
12. Strictly speaking, the resulting feature is not a cluster-level feature, as it is computed between an active
mention and only one of the mentions in the preceding cluster.

489

Rahman & Ng

• Does joint modeling for anaphoricity determination and coreference resolution offer
any benefits over the pipeline architecture, where anaphoricity is performed prior to
coreference resolution?
• Do lexicalized coreference models perform better than their unlexicalized counterparts?
In the rest of this section, we will first describe the experimental setup (Section 6.1),
and then show the performance of the four models, including the effect of lexicalization and
joint modeling whenever applicable, on three different feature sets (Section 6.2).
6.1 Experimental Setup
We begin by providing the details on the data sets, our automatic mention extraction
method, and the scoring programs.
6.1.1 Corpus
We use the ACE 2005 coreference corpus as released by the LDC, which consists of the 599
training documents used in the official ACE evaluation.13 To ensure diversity, the corpus was
created by selecting documents from six different sources: Broadcast News (BN), Broadcast
Conversations (BC), Newswire (NW), Webblog (WB), Usenet (UN), and Conversational
Telephone Speech (CTS). The number of documents belonging to each source is shown in
Table 3.
Data set
# of documents

BN
226

BC
60

NW
106

WL
119

UN
49

CTS
39

Table 3: Statistics for the ACE 2005 corpus

6.1.2 Mention Extraction
We evaluate each coreference model using system mentions. To extract system mentions
from a test text, we trained a mention extractor on the training texts. Following Florian
et al. (2004), we recast mention extraction as a sequence labeling task, where we assign
to each token in a test text a label that indicates whether it begins a mention, is inside
a mention, or is outside a mention. Hence, to learn the extractor, we create one training
instance for each token in a training text and derive its class value (one of b, i, and o)
from the annotated data. Each instance represents wi , the token under consideration, and
consists of 29 linguistic features, many of which are modeled after the systems of Bikel,
Schwartz, and Weischedel (1999) and Florian et al. (2004), as described below.
Lexical (7):

Tokens in a window of 7: {wi−3 , . . . , wi+3 }.

Capitalization (4): Determine whether wi IsAllCap, IsInitCap, IsCapPeriod, and
IsAllLower.
13. Since we did not participate in ACE 2005, we do not have access to the official test set.

490

A Cluster-Ranking Approach to Coreference Resolution

Morphological (8): wi ’s prefixes and suffixes of length one, two, three, and four.
Grammatical (1): The part-of-speech (POS) tag of wi obtained using the Stanford loglinear POS tagger (Toutanova, Klein, Manning, & Singer, 2003).
Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based
NE recognizer (Finkel et al., 2005).
Dictionaries (8): We employ eight dictionary-based features that indicate the presence
or absence of wi in a particular dictionary. The eight dictionaries contain pronouns (77
entries), common words and words that are not names (399.6k), person names (83.6k),
person titles and honorifics (761), vehicle words (226), location names (1.8k), company
names (77.6k), and nouns extracted from WordNet that are hyponyms of person (6.3k).
We employ CRF++14 , a C++ implementation of conditional random fields, for training
the mention detector on the training set. Overall, the detector achieves an F-measure of
86.7 (86.1 recall, 87.2 precision) on the test set. These extracted mentions are to be used
as system mentions in our coreference experiments.
6.1.3 Scoring Programs
To score the output of a coreference model, we employ two scoring programs, B3 (Bagga
& Baldwin, 1998) and φ3 -CEAF15 (Luo, 2005), which address the inherent weaknesses of
the MUC scoring program (Vilain, Burger, Aberdeen, Connolly, & Hirschman, 1995).16
Both B3 and CEAF score a response (i.e., system-generated) partition, R, against a key
(i.e., gold-standard) partition, K, and report coreference performance in terms of recall,
precision, and F-measure. B3 first computes recall and precision for each mention, mk , as
follows:
recall(mk ) =

|Rmk ∩ Kmk |
|Rmk ∩ Kmk |
, precision(mk ) =
,
|Kmk |
|Rmk |

where Rmk is the coreference cluster containing mk in R, and Kmk is the coreference cluster
containing mk in K. Then it computes overall recall (resp. precision) by averaging the
per-mention recall (resp. precision) scores.
On the the hand, CEAF first constructs the optimal one-to-one mapping between the
clusters in the key partition and those in the response partition. Specifically, assume that
K = {K1 , K2 , . . . , Km } is the set of clusters in the key partition, and R = {R1 , R2 , . . . , Rn }
is the set of clusters in the response partition. To compute recall, CEAF first computes the
score of each cluster, Ki , in K as follows:
score(Ki ) = |Ki ∩ Rj |,
14. Available from http://crfpp.sourceforge.net
15. CEAF has two versions: φ3 -CEAF and φ4 -CEAF. The two versions differ in how the similarity of two
aligned clusters is computed. We refer the reader to Luo’s (2005) paper for details. φ3 -CEAF is chosen
here because it is the more commonly-used version of CEAF.
16. Briefly, the MUC scoring program suffers from two often-cited weaknesses. First, as a link-based measure,
it does not reward successful identification of singleton clusters, since the mentions in these clusters are
not linked to any other mentions. Second, it tends to under-penalize partitions with overly large clusters.
See the work of Bagga and Baldwin (1998), Luo (2005), and Recasens and Hovy (2011) for details.

491

Rahman & Ng

where Rj is the cluster to which Ki is mapped in the optimal one-to-one mapping, which
can be constructed efficiently using the Kuhn-Munkres algorithm (Kuhn, 1955). Note that
if Ki is not mapped to any cluster in R, then score(Ki ) = 0. CEAF then computes recall
by summing the score of each cluster in K and dividing the sum by the number of mentions
in K. Precision can be computed in the same manner, except that we reverse the roles of
K and R.
A complication arises when B3 is used to score a response partition containing system
mentions. Recall that B3 constructs a mapping between the mentions in the response and
those in the key. Hence, if the response is generated using gold-standard mentions, then
every mention in the response is mapped to some mention in the key and vice versa. In
other words, there are no twinless (i.e., unmapped) mentions (Stoyanov et al., 2009). This
is not the case when system mentions are used, but the original description of B3 does
not specify how twinless mentions should be scored (Bagga & Baldwin, 1998). To address
this problem, we set the per-mention recall and precision of a twinless mention to zero,
regardless of whether the mention appears in the key or the response. Note that CEAF can
compare partitions with twinless mentions without any modification, since it operates by
aligning clusters, not mentions.
Additionally, we apply a preprocessing step to a response partition before scoring it:
we remove all and only those twinless system mentions that are singletons. The reason is
simple: since the coreference resolver has successfully identified these mentions as singletons,
it should not be penalized, and removing them allows us to avoid such penalty. Note that
we only remove twinless (as opposed to all) system mentions that are singletons: this allows
us to reward a resolver for successful identification of singleton mentions that have twins.
On the other hand, we retain (1) twinless system mentions that are non-singletons (as the
resolver should be penalized for identifying spurious coreference relations) and (2) twinless
mentions in the key partition (as we want to ensure that the resolver makes the correct
coreference or non-coreference decisions for them).17
6.2 Results
Before showing the results of the learning-based coreference models, let us consider the “head
match” baseline, which is a commonly-used heuristic baseline for coreference resolution. It
posits two mentions as coreferent if and only if their head nouns match. Head nouns are
determined as described in Section 5.1: the head of a proper noun is the string of the entire
mention, whereas the head of a pronoun or a common noun is the last word of the mention.
Since one of our goals is to examine the effect of lexicalization on a coreference model, the
head match baseline can provide information on how well we can do with one of the simplest
kinds of string matching. Results of this baseline, shown in row 1 of Table 4, are expressed
in terms of recall (R), precision (P), and F-measure (F) obtained via B3 and CEAF. As we
can see from Table 4, this baseline achieves F-measure scores of 54.9 and 49.6 according to
B3 and CEAF, respectively.
17. In addition to the method described here, a number of methods have been proposed to address the
mapping problem. We refer the reader to the work of Enrique, Gonzalo, Artiles, and Verdejo (2009),
Stoyanov et al. (2009), and Cai and Strube (2010) for details.

492

A Cluster-Ranking Approach to Coreference Resolution

Next, we train and evaluate the learning-based coreference models using five-fold cross
validation. For each data set si shown in Table 3, we partition the documents in si into
five folds of approximately equal size, si1 , . . . , si5 . We then train each coreference model
on four folds and use it to generate coreference chains for the documents in the remaining
fold, repeating this step five times so that each fold is used as the test fold exactly once.
After that, we apply B3 and CEAF to the entire set of automatically coreference-annotated
documents to obtain the scores in Table 4. Below we discuss the results of the learningbased coreference models obtained when used in combination with three feature sets: the
Conventional feature set (Section 6.2.1), the Lexical feature set (Section 6.2.2), and the
Combined feature set, which is composed of all the features from Conventional and Lexical
(Section 6.2.3).
6.2.1 Results Using the Conventional Features
To gauge the performance of our cluster-ranking model, we employ as baselines the mentionpair model, the entity-mention model, and the mention-ranking model.
The mention-pair baseline. We train our first learning-based baseline, the mentionpair model, using the SVM learning algorithm as implemented in the SVMlight package.18
As we can see from row 2 of Table 4, the mention-pair model achieves F-measure scores of
58.6 (B3 ) and 54.4 (CEAF), which represent a statistically significant improvement of 3.7%
and 4.8% in F-measure over the corresponding results for the head match baseline.19
The entity-mention baseline. Next, we train our second learning-based baseline, the
entity-mention model, using the SVM learner. As we can see from row 3 of Table 4, this
baseline achieves F-measure scores of 58.9 (B3 ) and 54.8 (CEAF), which represent small but
statistically significant improvements over the mention-pair model. The significant performance difference is perhaps not particularly surprising given the improved expressiveness
of the entity-mention model over the mention-pair model.
The mention-ranking baseline. Our third baseline is the mention-ranking model, which
is trained using the ranker-learning algorithm in SVMlight . To identify non-anaphoric mentions, we employ two methods. In the first method, we follow Denis and Baldridge (2008)
and adopt a pipeline architecture, where we train a MaxEnt classifier for anaphoricity determination independently of the mention ranker on the training set using the 26 features
described in Section 3.3. We then apply the resulting classifier to each test text to filter nonanaphoric mentions prior to coreference resolution. Results of this pipeline mention ranker
are shown in row 4 of Table 4. As we can see, the ranker achieves F-measure scores of 57.7
(B3 ) and 53.0 (CEAF), yielding a significant performance deterioration in comparison to
the entity-mention baseline.
In the second method, we perform anaphoricity determination jointly with coreference
resolution using the method described in Section 4.2. While we discussed this joint learning
method in the context of cluster ranking, it should be easy to see that the method is
equally applicable to the mention-ranking model. Results of the mention ranker using this
18. For this and subsequent uses of the SVM learner, we set all parameters to their default values. In
particular, we employ a linear kernel to obtain all the results in this article.
19. All statistical significance results in this article are obtained using the paired t-test, with p < 0.05.

493

Rahman & Ng

R
44.1

B3
P
72.9

CEAF
P
F
60.8 49.6

1

Coreference Model
Head match

F
54.9

R
41.9

2
3
4
5
6
7

Using the Conventional feature set
Mention-pair model
49.7 71.4 58.6
Entity-mention model
49.9 71.7 58.9
Mention-ranking model (Pipeline)
48.1 72.1 57.7
Mention-ranking model (Joint)
49.1 76.1 59.7
Cluster-ranking model (Pipeline)
49.9 71.6 58.8
Cluster-ranking model (Joint)
51.1 73.3 60.2

49.5
51.0
51.7
52.9
53.4
54.1

60.5
59.2
54.4
59.2
54.6
60.2

54.4
54.8
53.0
55.9
54.0
57.0

8
9
10
11
12
13

Using the Lexical feature set
Mention-pair model
53.0 75.3 62.2
Entity-mention model
53.1 75.8 62.5
Mention-ranking model (Pipeline)
55.7 67.5 61.0
Mention-ranking model (Joint)
56.6 73.1 63.8
Cluster-ranking model (Pipeline)
51.0 67.1 58.0
Cluster-ranking model (Joint)
51.3 75.7 61.1

55.6
55.7
56.3
58.5
53.1
53.3

62.0
62.2
62.0
65.0
55.3
58.6

58.6
58.8
59.0
61.6
54.1
55.8

14
15
16
17
18
19

Using the Combined
Mention-pair model
50.4
Entity-mention model
50.5
Mention-ranking model (Pipeline)
50.6
Mention-ranking model (Joint)
49.9
Cluster-ranking model (Pipeline)
52.9
Cluster-ranking model (Joint)
54.3

53.8
54.1
54.1
54.7
57.5
57.6

61.9
62.3
61.8
61.4
62.1
64.3

57.5
57.9
57.7
57.9
59.7
60.8

feature
73.1
73.4
74.6
79.3
70.9
75.1

set
59.6
59.8
60.3
61.3
60.6
63.0

Table 4: Five-fold cross-validation coreference results obtained using B3 and CEAF. The
best F-measure achieved for each feature set/scoring program combination is boldfaced.

joint architecture are shown in row 5 of Table 4. As we can see, the ranker achieves Fmeasure scores of 59.7 (B3 ) and 55.9 (CEAF), which represent significant improvements
over the entity-mention model and its pipeline counterpart. Not only do these results
demonstrate the superiority of the joint mention-ranking model to the entity-mention model,
they substantiate the hypothesis that joint modeling offers benefits over pipeline modeling.
Our cluster-ranking model. Finally, we evaluate our cluster-ranking model. As in the
mention-ranking baselines, we employ both the pipeline architecture and the joint architecture for anaphoricity determination. Results are shown in rows 6 and 7 of Table 4,
respectively, for the two architectures. As we can see, the pipeline architecture yields Fmeasure scores of 58.8 (B3 ) and 54.0 (CEAF), which represent a significant improvement
over the mention ranker adopting the pipeline architecture. With the joint architecture,
the cluster ranker achieves F-measure scores of 60.2 (B3 ) and 57.0 (CEAF). This also rep494

A Cluster-Ranking Approach to Coreference Resolution

resents a significant improvement over the mention ranker adopting the joint architecture,
the best of the baselines. Taken together, these results demonstrate the superiority of the
cluster ranker to the mention ranker. Finally, the fact that the joint cluster ranker performs
significantly better than its pipeline counterpart provides further empirical support for the
benefits of joint modeling over pipeline modeling.
6.2.2 Results Using the Lexical Features
Next, we evaluate the learning-based coreference models using the Lexical features. Results
are shown in rows 8–13 of Table 4. In comparison to the results obtained using the Conventional features, we see a different trend: the joint mention-ranking model replaces the
cluster-ranking model as the best-performing model. Moreover, its improvement over the
second best-performing model, which is the entity-mention model according to B3 and the
pipeline mention-ranking model according to CEAF, is statistically significant regardless of
which scoring program is used. A closer examination of the results reveals that employing
Lexical rather than Conventional features substantially improves the performance of the
mention-ranking model: in comparison to the unlexicalized joint mention-ranking model
(row 5), the F-measure scores of the lexicalized joint mention-ranking model (row 11) rise
by 4.1% (B3 ) and 5.7% (CEAF). This increase in F-measure can be attributed primarily
to a substantial rise in recall, even though there is also a large increase in CEAF precision.
Besides the joint mention-ranking model, the mention-pair model and the entity-mention
model also benefit substantially when the Conventional features are replaced with the Lexical features: we see that the F-measure scores increase by 3.6% (B3 ) and 4.2% (CEAF) for
the mention-pair model, and by 3.6% (B3 ) and 4.0% (CEAF) for the entity-mention model.
The gains in F-measure for these two models can be attributed to large increases in both
recall and precision. On the other hand, the joint cluster-ranking model does not always
improve when we replace the Conventional features with the Lexical features. In fact, the
performance difference between the cluster-ranking model and the entity-mention model is
statistically indistinguishable. Finally, we see the benefits of jointly learning anaphoricity
determination and coreference resolution again: when the joint version of the mentionranking model is used rather than the pipeline version (compare rows 10 and 11), the
F-measure scores rise significantly by 2.8% (B3 ) and 2.6% (CEAF). Similarly for the clusterranking model: the joint version improves the pipeline version significantly by 3.1% (B3 )
and 1.7% (CEAF) in F-measure.
Overall, these results are somewhat unexpected: recall that the Lexical features are
very knowledge-lean, consisting of lexical, semi-lexical, and unseen features, as well as only
two Conventional features. In particular, it does not employ any conventional coreference
features that encode agreement on gender and number. This implies that many existing
implementations of the mention-pair model, the entity-mention model, and the mentionranking model, which are unlexicalized and rely heavily on the conventional features, are not
making effective use of the labeled data. Perhaps more importantly, our results indicate that
these coreference models can perform well (and in fact better) even without the conventional
coreference features. Since all of the Lexical can be computed extremely easily, they can
readily be applied to other languages, which is another advantage of this feature set. On the
other hand, it is interesting to see that both versions of the cluster-ranking model exhibit
495

Rahman & Ng

less dramatic changes in performance as we replace the Conventional features with the
Lexical features.
6.2.3 Results Using the Combined Features
Since the Conventional features and the Lexical features represent two fairly different sources
of knowledge, we examine whether we can improve the coreference models by combining
these two feature sets. Results of the coreference models using the Combined features are
shown in rows 14–19 of Table 4. These results exhibit essentially the same trend as those that
we obtained with the Conventional features, with the joint cluster-ranking model performing
the best and the mention-pair model performing the worst. In fact, the joint cluster-ranking
model yields significantly better performance when used with the Combined features than
with the Conventional features or the Lexical features alone. Similarly for the pipeline
cluster-ranking model, which achieves significantly better performance with the Combined
features than with the Conventional or Lexical features. These results seem to suggest that
the cluster-ranking model is able to exploit the potentially different sources of information
provided by the two feature sets to improve its performance. In addition, they demonstrate
the benefits of joint modeling: for the mention-ranking model, the joint version improves
the pipeline version significantly by 1.0% (B3 ) and 0.2% (CEAF) in F-measure; and for the
cluster-ranking model, the joint version improves its pipeline counterpart significantly by
2.4% (B3 ) and 1.1% (CEAF) in F-measure.
The remaining coreference models all exhibit a drop in performance when the Combined
features are used in lieu of the Lexical features. These results seem to suggest that the
cluster-ranking model offers more robust performance in the face of changes in the underlying feature set than the other coreference models, and that feature selection, an issue that is
under-explored in coreference resolution, may be crucial when we employ the other coreference models.20 Perhaps more importantly, despite the fact that the Conventional features
and the Lexical features represent two fairly different sources of information, all but the
cluster-ranking model are unable to exploit the potentially richer amount of information
contained in the Combined feature set. Hence, while virtually all the linguistic features
that are recently developed for supervised coreference resolution have been evaluated using
the mention-pair model (see, for example, the work of Strube, Rapp, & Müller, 2002; Ji,
Westbrook, & Grishman, 2005; Ponzetto & Strube, 2006), the utility of these features may
be better demonstrated using the cluster-ranking model.
A natural question is: how does our joint cluster-ranking model compare to the existing
coreference systems? Since we did not participate in the ACE evaluations, we do not have
access to the official test sets with which we can compare our model against the ACE
participating coreference systems. The comparison is further complicated by the fact that
existing coreference systems have been evaluated on different data sets, including the two
MUC data sets (MUC-6, 1995; MUC-7, 1998) and the various ACE data sets (e.g., ACE-2,
ACE 2003, ACE 2004, ACE 2005), as well as on different partitions of a given data set.
To our knowledge, the only coreference model that has been evaluated on the same test
data as ours is Haghighi and Klein’s (2010) unsupervised coreference model. Their model
20. In fact, Ng and Cardie (2002b), Strube and Müller (2003), and Ponzetto and Strube (2006) show that
the mention-pair model can be improved using feature selection.

496

A Cluster-Ranking Approach to Coreference Resolution

has recently been shown to surpass the performance of Stoyanov et al.’s (2009) system,
which is one of the best existing implementations of the mention-pair model. On our test
data, Haghighi and Klein’s model achieves a B3 F-measure of 62.7, while ours achieves a
B3 F-measure of 62.8.21 These results provide suggestive evidence that our cluster-ranking
model achieves performance that is comparable with one of the best existing coreference
models.
Nevertheless, we caution that these results do not allow one to claim anything more
than the fact that our model compares favorably to Haghighi and Klein’s (2010) model. For
instance, one cannot claim that their model is better because it achieves the same level of
performance as ours without using any labeled data. The reasons are that (1) the mentions
used by the two models in the coreference process are extracted differently and (2) the
linguistic features employed by the two models and the way these features are computed
are also different from each other. Since previous work has shown that these linguistic
preprocessing steps can have a considerable impact on the performance of a resolver (Barbu
& Mitkov, 2001; Stoyanov et al., 2009), it is possible that if one model employed the features
or the mentions that the other model is currently using, then the results would be different.
Hence, if one is to fairly compare two coreference models, they should be evaluated on the
same set of mentions (rather than just the same set of documents) and are given access to
the same set of knowledge sources, in essentially the same way as we compare the various
learning-based coreference models in this article.

7. Experimental Analyses
In an attempt to gain insights into the different aspects of our coreference models, we
conduct additional experiments and analyses. Rather than report five-fold cross-validation
results, in this section we report results on one fold (i.e., the fold we designate as the test
set) and use the remaining four folds solely for training.
7.1 Improving Classification-Based Coreference Models
Given the generally poorer performance of classification-based coreference models, a natural question is: can they be improved? To answer this question, we investigate whether
these models can be improved by employing a different clustering algorithm and a different
learning algorithm. There are reasons for our decision to focus on these two dimensions.
First, as noted in the introduction, one of the weaknesses of these models is that it is
not clear which clustering algorithm offers the best performance. Given this observation,
we will examine whether we can improve these models by replacing Soon et al.’s (2001)
“closest-first” linking regime with the “best-first” linking strategy, which has been shown
to offer better performance for the mention-pair model on the MUC data sets (Ng & Cardie,
2002b). Second, as discussed at the end of Section 2, we may be able to achieve some of the
advantage of ranking in classification-based models by employing a learning algorithm that
optimizes for conditional probabilities instead of 0/1 decisions. Motivated by this observation, we will examine whether we can improve classification-based models by training them
using MaxEnt, which employs a likelihood-based loss function. Note that MaxEnt is one
21. Note that Haghighi and Klein did not report any CEAF scores in their paper.

497

Rahman & Ng

of the most popular learning algorithms for training coreference models (see, for example,
Morton, 2000; Kehler, Appelt, Taylor, & Simma, 2004; Ponzetto & Strube, 2006; Denis &
Baldridge, 2008; Finkel & Manning, 2008; Ng, 2009).
To evaluate these two modifications, we apply them in isolation and in combination to
the two classification-based models (i.e., the mention-pair model and the entity-mention
model) when they are trained using three different feature sets (i.e., Conventional, Lexical, and Combined). We train the MaxEnt-based coreference models using YASMET22 ,
and follow Ng and Cardie’s (2002b) implementation of the best-first clustering algorithm.
Specifically, among the candidate antecedents or preceding clusters that are classified as
coreferent with active mention mk , best-first clustering links mk to the “most likely” one.
For a MaxEnt model, a pair is classified as coreferent if and only if its classification value
is above 0.5, and the most likely antecedent/preceding cluster for mk is the one that has
the highest probability of coreference with mk . For an SVMlight -trained model, a pair is
classified as coreferent if and only if its classification value is above 0, and the most likely
antecedent/preceding cluster for mk is the one that has the most positive classification
value.
Table 5 presents B3 and CEAF results of the two classification-based coreference models
when they are trained using two learning algorithms (i.e., SVM and MaxEnt) and used
in combination with two clustering algorithms (i.e., closest-first clustering and best-first
clustering). To study how the choice of the clustering algorithm impacts performance, we
should compare the results of closest-first clustering and best-first clustering in Table 5
for each combination of learning algorithm, feature set, coreference model, and scoring
program. For instance, comparing rows 1 and 2 of Table 5 enables us to examine which of
the two clustering algorithms is better for the mention-pair model when it is trained with
the Conventional feature set and each of the two learners. Overall, we see a fairly consistent
trend: best-first clustering yields results that are slightly worse than those obtained using
closest-first clustering, regardless of the choice of the clustering algorithm, the learning
algorithm, the feature set, and the scoring program. At first glance, these results seem
contradictory to those by Ng and Cardie (2002b), who demonstrate the superiority of bestfirst clustering to closest-first clustering for coreference resolution. We speculate that the
contradictory results can be attributed to two reasons. First, in our best-first clustering
experiments, we still employed Soon et al.’s (2001) training instance selection method,
where we created a positive training instance between an anaphoric mention and its closest
antecedent/preceding cluster, unlike Ng and Cardie, who claim that “for the proposed bestfirst clustering to be successful, however, a different method for training instance selection
would be needed.” In particular, they propose to use the “most confident” antecedent,
rather than the closest antecedent, to generate positive instances from an anaphoric mention.
Second, Ng and Cardie demonstrate the success of best-first clustering on the MUC data
sets, and it is possible that this success may not carry over to the ACE data sets. Additional
experiments are needed to determine the reason, however.
22. See http://www.fjoch.com/YASMET.html. The reason why YASMET is chosen is that it provides the
capability to rank, which allows us to compare the results of MaxEnt-trained classification models and
ranking models. See the work of Ravichandran, Hovy, and Och (2003) for a discussion of the differences
between the training of these two types of MaxEnt models.

498

A Cluster-Ranking Approach to Coreference Resolution

Coreference Model

R

SVM
P

F

R

MaxEnt
P
F

1
2
3
4

B3 results using the Conventional feature
Mention-pair model (Closest first)
46.2 72.0 56.2
Mention-pair model (Best first)
45.7 71.0 55.6
Entity-mention model (Closest first) 46.8 72.5 56.8
Entity-mention model (Best first)
46.3 72.1 56.3

set
59.6
59.2
59.7
59.3

55.3
54.8
55.9
55.1

57.3
56.9
57.7
57.1

5
6
7
8

B3 results using the Lexical feature set
Mention-pair model (Closest first)
52.8 73.0 61.2
Mention-pair model (Best first)
52.1 72.1 60.5
Entity-mention model (Closest first) 52.8 73.6 61.2
Entity-mention model (Best first)
52.4 72.2 60.8

52.8
52.1
52.8
52.2

64.6
64.2
64.6
64.3

58.1
57.5
58.2
57.6

Combined feature set
49.1 73.2 58.8
50.3
48.7 72.8 58.3
49.3
49.5 73.2 59.1 50.5
49.1 72.7 58.6
50.1

65.9
65.2
66.1
65.6

57.0
56.1
57.3
56.8

13
14
15
16

CEAF results using the Conventional feature set
Mention-pair model (Closest first)
48.5 55.3 51.6
51.4
Mention-pair model (Best first)
48.1 54.9 51.2
51.1
Entity-mention model (Closest first) 49.5 55.8 52.5
51.4
Entity-mention model (Best first)
49.2 55.1 51.9
51.1

56.5
56.1
56.7
56.2

53.8
53.4
53.9
53.5

17
18
19
20

CEAF results using
Mention-pair model (Closest first)
Mention-pair model (Best first)
Entity-mention model (Closest first)
Entity-mention model (Best first)

56.8
56.2
57.4
56.9

55.1
54.6
55.3
54.9

21
22
23
24

CEAF results using the Combined
Mention-pair model (Closest first)
53.8 60.0
Mention-pair model (Best first)
53.1 59.7
Entity-mention model (Closest first) 54.1 60.9
Entity-mention model (Best first)
53.7 60.3

56.3
55.8
56.8
56.2

55.5
55.0
55.9
55.3

9
10
11
12

B3 results using the
Mention-pair model (Closest first)
Mention-pair model (Best first)
Entity-mention model (Closest first)
Entity-mention model (Best first)

(a) B3 results

the Lexical feature set
54.6 61.2 57.7
53.5
54.2 60.7 57.3
53.1
54.9 61.7 58.1 53.5
54.5 61.1 57.6
53.1
feature
56.7
56.2
57.3
56.8

set
54.9
54.3
55.1
54.5

(b) CEAF3 results

Table 5: SVM vs. MaxEnt results for classification-based coreference models. These one-fold
B3 and CEAF scores are obtained by training coreference models using SVM and MaxEnt. The
best F-measure achieved for each feature set/scoring program combination is boldfaced.
499

Rahman & Ng

Next, to examine whether minimizing likelihood-based loss via MaxEnt training instead
of SVM’s classification loss would enable us to achieve some of the advantage of ranking
(and hence leads to better performance), we compare the two columns of Table 5. As we can
see, when the Conventional feature set is used, MaxEnt outperforms SVM, regardless of the
choice of the clustering algorithm, the scoring program, and the coreference model. On the
other hand, when the Lexical features or the Combined features are used, SVM outperforms
MaxEnt consistently. Overall, these mixed results seem to suggest that whether MaxEnt
offers better performance than SVM is to some extent dependent on the underlying feature
set.
7.2 Performance of Maximum-Entropy-Based Ranking Models
Some prior work suggests that MaxEnt-based ranking may provide better gains than SVMbased ranking, since it can generate reliable confidence values and can dynamically adjust
relative ranks according to baseline results (e.g., Ji, Rudin, & Grishman, 2006). To determine whether this is the case for coreference resolution, we conduct experiments in which
we train the ranking-based coreference models using the ranker-learning algorithm in YASMET.
B3 and CEAF results for the mention-ranking model and the cluster-ranking model when
they are trained using MaxEnt in combination three different feature sets (i.e., Conventional,
Lexical, and Combined) are shown in the “MaxEnt” column of Table 6. For comparison,
we also show the corresponding results obtained via SVM-based ranking in the same table
(see the “SVM” column). Comparing these two columns, we see mixed results: of the 24
experiments that involve ranking models, MaxEnt-based ranking outperforms SVM-based
ranking in six of them. In other words, our results suggest that for the coreference task,
SVM-based ranking is generally better than MaxEnt-based ranking.
7.3 Accuracy of Anaphoricity Determination
In Section 6.2, we saw that a joint ranking model always performs significantly better than
its pipeline counterpart. In other words, joint modeling for coreference and anaphoricity
improves coreference resolution. A natural question is: does joint modeling also improve
anaphoricity determination?
To answer this question, we measure the accuracy of the anaphoricity information resulting from pipeline modeling and joint modeling. Recall that for pipeline modeling, we rely
on the output of an anaphoricity classifier that is trained independently of the coreference
system that uses the anaphoricity information (see Section 3.3). The accuracy of this classifier on the test set is shown under the “Acc” column in row 1 of Table 7. In addition, we
show in the table its recall (R), precision (P), and F-measure (F) on identifying anaphoric
mentions. As we can see, the classifier achieves an accuracy of 81.1 and a F-measure score
of 83.8.
On the other hand, for joint modeling, we can compute the accuracy of anaphoricity
determination from the output of a joint coreference model. Specifically, given the output
of a joint model, we can determine which mentions are resolved to a preceding antecedent
and which are not. Assuming that a mention that is resolved is anaphoric and one that is
not resolved is non-anaphoric, we can compute the accuracy of anaphoricity determination
500

A Cluster-Ranking Approach to Coreference Resolution

Coreference Model

R

SVM
P

F

R

MaxEnt
P
F

1
2
3
4

B3 results using the Conventional feature
Mention-ranking model (Pipeline)
46.7 71.5 56.5
Mention-ranking model (Joint)
47.6 74.8 58.2
Cluster-ranking model (Pipeline)
53.6 59.5 56.4
Cluster-ranking model (Joint)
52.2 73.8 61.2

set
58.7
59.1
51.7
52.1

59.1
59.3
69.9
70.6

58.8
59.2
59.3
60.0

5
6
7
8

B3 results using the Lexical feature set
Mention-ranking model (Pipeline)
53.8 68.3 60.1
Mention-ranking model (Joint)
54.6 72.8 62.4
Cluster-ranking model (Pipeline)
51.7 68.2 58.8
Cluster-ranking model (Joint)
52.9 73.4 61.5

56.6
56.3
48.4
48.0

61.1
64.4
66.6
72.9

58.8
60.1
56.1
57.8

Combined feature set
49.8 72.6 59.1
51.4
50.5 77.6 61.2
52.5
53.8 71.2 61.3
54.1
54.4 74.8 62.8 54.5

68.3
70.3
67.5
68.3

58.7
60.1
60.1
60.6

Conventional feature set
49.4 55.7 52.4
51.5
50.5 56.3 53.2
51.8
53.6 59.5 56.4
53.1
55.2 61.6 58.2 53.2

56.6
56.9
58.7
59.3

53.9
54.2
55.8
56.1

9
10
11
12

B3 results using the
Mention-ranking model (Pipeline)
Mention-ranking model (Joint)
Cluster-ranking model (Pipeline)
Cluster-ranking model (Joint)

(a) B3 results

13
14
15
16

CEAF results using the
Mention-ranking model (Pipeline)
Mention-ranking model (Joint)
Cluster-ranking model (Pipeline)
Cluster-ranking model (Joint)

17
18
19
20

CEAF results using
Mention-ranking model (Pipeline)
Mention-ranking model (Joint)
Cluster-ranking model (Pipeline)
Cluster-ranking model (Joint)

the Lexical feature set
54.7 59.8 57.1
55.7
56.9 63.3 59.9 55.4
52.7 58.4 55.4
50.9
55.0 61.4 58.1
50.5

56.6
60.7
52.3
56.6

56.1
57.9
51.5
53.3

21
22
23
24

CEAF results the
Mention-ranking model (Pipeline)
Mention-ranking model (Joint)
Cluster-ranking model (Pipeline)
Cluster-ranking model (Joint)

Combined feature set
53.7 58.8 56.1
54.9 61.7 58.1
55.1 60.1 57.4
58.4 65.1 61.6

58.7
56.3
60.0
62.5

55.9
55.5
57.9
59.5

53.4
54.9
56.1
56.7

(b) CEAF results

Table 6: SVM vs. MaxEnt results for ranking-basd coreference models. These one-fold B3
and CEAF scores are obtained by training coreference models using SVM and MaxEnt. The best
F-measure achieved for each feature set/scoring program combination is boldfaced.
501

Rahman & Ng

1
2
3
4
5
6
7

Source of Anaphoricity Information
Anaphoricity Classifier
Mention-ranking (Conventional)
Cluster-ranking (Conventional)
Mention-ranking (Lexical)
Cluster-ranking (Lexical)
Mention-ranking (Combined)
Cluster-ranking (Combined)

Acc
81.1
78.7
81.9
84.2
83.1
79.1
83.1

R
87.6
83.1
87.8
88.3
87.9
84.3
87.4

P
80.3
79.3
80.4
82.1
81.8
79.1
82.1

F
83.8
81.2
83.9
85.1
84.7
81.6
84.6

Table 7: Anaphoricity determination results.

as well as the precision, recall, and F-measure on identifying anaphoric mentions. Since all
these performance numbers are derived from the output of a joint model, we can compute
them for each of the two joint ranking models (i.e., the mention-ranking model and the
cluster-ranking model) when used in combination with each of the three coreference feature
sets (i.e., Conventional, Lexical, and Combined). This results in six sets of performance
numbers, which are shown in rows 2–7 of Table 4. As we can see, the accuracies range from
78.7 to 84.2, and the F-measure scores range from 81.2 to 85.1.
In comparison to the results of the anaphoricity classifier shown in row 1, we can see
that joint modeling improves the performance of anaphoricity determination except for two
cases, namely, mention-ranking/Conventional and mention-ranking/Combined. In other
words, in these two cases, joint modeling benefits coreference resolution but not anaphoricity determination. While it seems counter-intuitive that one can achieve better coreference
performance with a lower accuracy on determining anaphoricity, it should not be difficult
to see the reason: the joint model is trained to maximize the pairwise ranking accuracy,
which presumably correlates with coreference performance, whereas the anaphoricity classifier is trained to maximize the accuracy of determining the anaphoricity of a mention,
which may not always have any correlation with coreference performance. In other words,
improvements in anaphoricity accuracy generally but not necessarily imply corresponding
improvements in clustering-level coreference accuracy.
Finally, it is important to bear in mind that the conclusions we have drawn regarding
pipeline and joint modeling are based on the results of an anaphoricity classifier trained
on 26 features. It is possible that different conclusions could be drawn if we trained the
anaphoricity classifier on a different set of features. Therefore, an interesting future direction
would be to improve the anaphoricity classifier by employing additional features, such as
those proposed by Uryupina (2003). We may also be able to derive sophisticated features
by harnessing recent advances in lexical semantics research, specifically by using methods
for phrase clustering (e.g., Lin & Wu, 2009), lexical chain discovery (e.g., Morris & Hirst,
1991), and paraphrase discovery (see the survey papers by Androutsopoulos & Malakasiotis,
2010; Madnani & Dorr, 2010).
502

A Cluster-Ranking Approach to Coreference Resolution

7.4 Joint Inference Versus Joint Learning for the Mention-Pair Model
As mentioned at the end of Section 4.2, joint modeling for anaphoricity determination and
coreference resolution is fundamentally different from joint inference for these two tasks.
Recall that in joint inference using ILP, an anaphoricity classifier and a coreference classifier
are trained independently of each other, and then ILP is applied as a postprocessing step
to jointly infer anaphoricity and coreference decisions so that they are consistent with each
other (e.g., Denis & Baldridge, 2007a). In this subsection, we investigate how joint learning
compares with joint inference for anaphoricity determination and coreference resolution.
Let us begin with an overview of the ILP approach proposed by Denis and Baldridge
(2007a) for joint inference for anaphoricity determination and coreference resolution. The
ILP approach is motivated by the observation that the output of an anaphoricity model and
that of a coreference model for a given document have to satisfy certain constraints. For
instance, if the coreference model determines that a mention mk is not coreferent with any
other mentions in the associated text, then the anaphoricity model should determine that
mk is non-anaphoric. In practice, however, since the two models are trained independently
of each other, this and other constraints cannot be enforced.
Denis and Baldridge (2007a) provide an ILP framework for jointly determining anaphoricity and coreference decisions for a given set of mentions based on the probabilities provided
by the anaphoricity model PA and the mention-pair coreference model PC , such that the
resulting joint decisions satisfy the desired constraints while respecting as much as possible the probabilistic decisions made by the independently-trained PA and PC . Specifically, an ILP program is composed of an objective function to be optimized subject to
a set of linear constraints, and is created for each test text D as follows. Let M be
the set of mentions in D, and P be the set of mention pairs formed from M (i.e., P =
{(mj , mk ) | mj , mk ∈ M, j < k}). Each ILP program has a set of indicator variables. In
our case, we have one binary-valued variable for each anaphoricity decision and coreference
decision to be made by an ILP solver. Following Denis and Baldridge’s notation, we use
yk to denote the anaphoricity decision for mention mk , and xhj,ki to denote the coreference
decision involving mentions mj and mk . In addition, each variable is associated with an
assignment cost. Specifically, let cC
hj,ki = − log(PC (mj , mk )) be the cost of setting xhj,ki to
C
1, and c̄hj,ki = − log(1 − PC (mj , mk )) be the complementary cost of setting xhj,ki to 0. We
can similarly define the cost associated with each yk , letting cA
k = − log(PA (mk )) be the
=
−
log(1
−
P
(m
))
be
the
complementary
cost of setting
cost of setting yk to 1, and c̄A
A
k
k
yk to 0. Given these costs, we aim to optimize the following objective function:
min

X

C
cC
hj,ki · xhj,ki + c̄hj,ki · (1 − xhj,ki ) +

X

A
cA
k · yk + c̄k · (1 − yk )

mk ∈M

(mj ,mk )∈P

subject to a set of manually-specified linear constraints. Denis and Baldridge specify four
types of constraints: (1) each indicator variable can take on a value of 0 or 1; (2) if mj and
mk are coreferent (xhj,ki =1), then mk is anaphoric (yk =1); (3) if mk is anaphoric (yk =1),
then it must be coreferent with some preceding mention mj ; and (4) if mk is non-anaphoric,
then it cannot be coreferent with any mention.
Two points deserve mention. First, we are minimizing the objective function, since each
assignment cost is expressed as a negative logarithm value. Second, since transitivity is
503

Rahman & Ng

not guaranteed by the above constraints23 , we use the closest-link clustering algorithm to
put any two mentions that are posited as coreferent into the same cluster. Note that the
best-link clustering strategy is not applicable here, since a binary decision is assigned to
each pair of mentions by the ILP solver. We use lp solve24 , a publicly-available ILP solver,
to solve this program.
B3 and CEAF results of performing joint inference on the outputs of the anaphoricity
model and the mention-pair model using ILP are shown in the “Joint Inference” column of
Tables 8a and 8b, respectively, where the rows correspond to results obtained by training
the coreference models on different feature sets. Since one of our goals is to compare joint
inference and joint learning, we also show in the “Joint Learning” column the results of the
joint mention-ranking model, where anaphoricity determination and coreference resolution
are learned in a joint fashion. Note that the reason for using the mention-ranking model
(rather than the cluster-ranking model) as our joint model here is that we want to ensure a
fair comparison of joint learning and joint inference as much as possible: had we chosen the
cluster-ranking model as our joint model, the difference between the joint learning results
and the joint inference results could have been caused by the increased expressiveness of
the cluster-ranking model. Finally, to better understand whether the mention-pair model
benefits from joint inference using ILP, we show in the “No Inference” column the relevant
mention-pair model results from Table 4, where the output of the model is not postprocessed
with any inference mechanism.
From Table 8, we can see that the joint learning results are substantially better than the
joint inference results, except for one case (Conventional/CEAF), where the two achieve
comparable performance. Previous work by Roth (2002) and Roth and Yih (2004) has
suggested that it is often more effective to learn simple local models and use complicated
integration strategies to make sure constraints on the output are satisfied than to learn
models that satisfy the constraints directly. Our results imply that this is not true for the
coreference task.
Comparing the joint inference and “No Inference” results in Table 8, we can see that the
mention-pair model does not benefit from the application of ILP. In fact, its performance
deteriorates when ILP is used. These results are inconsistent with those reported by Denis
and Baldridge (2007a), who show that joint inference using ILP can improve the mentionpair model. We speculate that the inconsistency accures from the fact that Denis and
Baldridge evaluate the ILP approach on true mentions (i.e., gold-standard mentions), while
we evaluate it on system mentions. Additional experiments are needed to determine the
reason, however.
7.5 Data Source Adaptability
One may argue that since we train and test a model on documents from the same data
source (i.e., the model trained on the documents from BC is tested on the documents from
23. Finkel and Manning (2008) show how to formulate linear constraints so that the ILP solver outputs
coreference decisions that satisfy transitivity. However, since the number of additional constraints needed
to guarantee transitivity grows cubically with the number of mentions and previous work shows that
having these additional constraints do not yield substantial performance improvements when applied to
system mentions (Ng, 2009), we decided not to employ them in our experiments.
24. Available from http://lpsolve.sourceforge.net/

504

A Cluster-Ranking Approach to Coreference Resolution

1
2
3

Feature Set
Conventional
Lexical
Combined

Joint Learning
R
P
F
47.6 74.8 58.2
54.6 72.8 62.4
50.5 77.6 61.2

Joint Inference
R
P
F
58.2 55.9 57.0
49.1 70.1 57.8
53.2 56.9 54.9

No
R
59.6
52.8
50.3

Inference
P
F
55.3 57.3
64.6 58.1
65.9 57.0

No
R
51.4
53.5
54.9

Inference
P
F
56.5 53.8
56.8 55.1
56.3 55.5

(a) B3 results

1
2
3

Feature Set
Conventional
Lexical
Combined

Joint Learning
R
P
F
50.5 56.3 53.2
56.9 63.3 59.9
54.9 61.7 58.1

Joint Inference
R
P
F
49.7 57.5 53.3
50.6 58.6 54.3
53.2 56.9 54.9

(b) CEAF results

Table 8: Joint learning vs. joint inference results. The joint modeling results are obtained
using the mention-ranking model. The joint inference results are obtained by applying ILP to the
anaphoricity classifier and the mention-pair model. The “no inference” results are those produced
by the mention-pair model. All coreference models are trained using MaxEnt.

BC, for example), it should not be surprising that lexicalization helps, since word pairs in
a training set are more likely to be found in a test set if the training and test texts are
from the same data source. To examine whether models that employ the Lexical features
will suffer if they are trained and tested on different data sources, we perform a set of data
source adaptability experiments, where we apply a coreference model that is trained with
the Lexical features on documents from one data source to documents from all data sources.
Here, we show the results obtained using the mention-ranking model, primarily because it
yielded the best performance with the Lexical features among the learning-based coreference
models. For comparison, we also show the data source adaptability results obtained using
the mention-ranking model that is trained with the (non-lexical) Conventional feature set.
The B3 and CEAF F-measure scores of these experiments are shown in Tables 9a and
9b, where the left half and the right half of the table contain the lexicalized mention-ranking
model results and the unlexicalized mention-ranking model results, respectively. Each row
corresponds to a data source on which a model is trained, except for the last two rows,
which we will explain shortly. Each column corresponds to a test set from a particular data
source.
To answer the question of whether the performance of a coreference model that employs
the Lexical features will deteriorate if they are trained and tested on different data sources,
we can look at the diagonal entries in the left half of Tables 9a and 9b, which contain
the results obtained when the lexicalized mention-ranking model is trained and tested on
documents from the same source. If the model indeed performs worse when it is trained
and tested on documents from different sources, then a diagonal entry should contain the
highest score among the entries in the same column. As we can see from the left half of
the two tables, this is to a large extent correct: four of the six diagonal entries contain the
highest scores in their respective columns according to both scoring programs. This provides
505

Rahman & Ng

Lexical features
PP
PP Test
Train PPP
P
BC
BN
CTS
NW
UN
WL
Max−Min
Std. Dev.

Conventional features

BC

BN

CTS NW UN

WL

BC

BN

CTS NW UN

WL

56.5
57.6
55.5
55.6
56.4
56.4
2.1
0.76

61.0
63.5
61.1
62.1
62.8
62.8
2.5
1.01

58.7
60.7
62.7
56.7
60.0
58.5
6.0
2.07

66.6
67.0
65.9
59.2
67.3
68.7
9.5
3.36

52.1
51.8
51.8
51.5
52.7
51.5
1.2
0.45

55.9
59.7
58.4
55.3
57.3
56.5
4.4
1.64

55.1
58.4
59.5
55.7
59.2
55.1
4.4
2.09

63.8
62.8
64.2
60.3
64.2
64.0
3.9
1.52

64.2
63.5
61.9
65.4
62.9
63.4
3.5
1.18

57.2
55.4
54.9
56.4
56.3
55.9
2.3
0.81

59.1
58.7
59.2
58.5
59.0
59.0
0.7
0.26

52.8
52.5
53.6
52.1
53.2
52.7
1.5
0.53

(a) B3 results

Lexical features
PP
PP Test
Train PPP
P
BC
BN
CTS
NW
UN
WL
Max−Min
Std. Dev.

Conventional features

BC

BN

CTS NW UN

WL

BC

BN

CTS NW UN

WL

52.0
53.8
51.9
50.5
52.5
53.4
3.3
1.18

57.2
61.3
58.6
58.9
60.3
61.1
4.1
1.60

55.5
58.8
62.0
53.3
58.3
55.7
8.7
3.07

65.7
66.0
64.8
54.5
67.0
67.1
12.7
4.82

46.0
45.9
44.7
45.5
46.0
45.7
1.3
0.50

49.3
54.8
52.3
49.2
51.3
50.4
5.6
2.11

52.7
55.2
56.5
52.6
56.2
51.4
5.1
2.14

60.8
60.1
61.0
56.5
60.8
61.1
4.6
1.77

61.5
60.5
58.4
62.7
59.3
59.8
4.3
1.55

56.5
54.1
53.7
54.6
55.7
55.0
2.8
1.04

53.1
52.9
53.8
52.7
52.8
52.9
1.1
0.40

49.1
48.0
50.0
48.4
48.8
50.1
2.1
0.85

(b) CEAF results

Table 9: Results for data source adaptability. Each row shows the results obtained by training
the mention ranking model on the data set shown on the first column of the row, and each column
corresponds to the test set from a particular data source. The best result obtained on each test set
for each of the two coreference models is boldfaced.

suggestive evidence that the answer to our question is affirmative. Nevertheless, if we look
at right half of the two tables, where we show the results obtained using the unlexicalized
mention-ranking model, we see a similar, but perhaps weaker, trend: according to CEAF,
four of the six diagonal entries contain the highest scores in their respective columns, and
according to B3 , two of the six diagonal entries exhibit this trend. Hence, the fact that
a model performs worse when it is trained and tested on different data sources cannot be
attributed solely to lexicalization.
Perhaps a more informative question is: do lexicalized models trained on different data
sources exhibit more varied performance on a given test set (composed of documents from
the same source) than unlexicalized models trained on different data sources? An affirmative
answer to this question will provide empirical support for the hypothesis that a lexicalized
model fits the data on which it is trained more than its unlexicalized counterpart. To answer
this question, we compute for each column and each of the two models (1) the difference
between the highest and lowest scores (see the “Max−Min” row), and (2) the standard
deviation of the six scores in the corresponding column (see the “Std. Dev.” row). If we
506

A Cluster-Ranking Approach to Coreference Resolution

compare the corresponding columns of the two coreference models, we can see that except
for BN, the lexicalized model does exhibit more varied performance on a given test set than
the unlexicalized model according to both scoring programs, regardless of whether we are
measuring the variation using Max−Min or standard deviation.
7.6 Feature Analysis
In this subsection, we analyze the effects of the linguistic features on the performance of
the coreference models. Given the large number of models trained on each of the three
feature sets, it is not feasible for us to analyze the features for each model and each feature
set. Since the cluster-ranking model, when used with the Combined feature set, yields the
best performance, we will analyze its features. In addition, since the Lexical features have
yielded good performance for the mention-ranking model, it would be informative to see
which Lexical features have the greatest contribution to its performance. As a result, we
will perform feature analysis on these two model/feature set combinations.
Although we have identified two particular model/feature set combinations, we actually
have a total of 12 model/feature set combinations: recall that except for row 1, each row in
Table 4 shows the aggregated result for the six data sets, where we trained one model for
each data set. In other words, for each of the two combinations we selected above, we have
six learned models. To reduce the number of models we need to analyze and yet maximize
the insights we can gain, we choose to analyze the models that were trained on data sets
from two fairly different domains: Newswire (NW) and Broadcast News (BN).
The next question is: how can we analyze the features? We apply the backward elimination feature selection algorithm (see the survey paper by Blum & Langley, 1997), which
starts with the full feature set and removes in each iteration the feature whose removal
yields the best system performance. Despite its greedy nature, this algorithm runs in time
quadratic to the number of features, making it computationally expensive to run on our
feature sets. To reduce computational cost, we divide our features into feature types and
apply backward elimination to eliminate one feature type per iteration.
The features are grouped as follows. For the Lexical feature set, we divide the features
into five types: (1) unseen features, (2) lexical features, (3) semi-lexical features, (4) distance, and (5) alias. In other words, the division corresponds roughly to the one described
in Section 5.1, except that we put the two “conventional” features into two different groups,
since linguistically one is a positional feature and the other is a semantic feature. For the
Combined feature set, we divide the features into seven groups, the first four of which are
identical to those in the division of the Lexical features above. For the remaining features,
we divide them into string-matching features, which comprise features 11–18 in Table 1;
grammatical features, which comprise features 1–7, 9–10, 19–29, 33–36, and 38–39; and
semantic features, which comprise features 8, 30, and 31. Note that alias, the only semantic feature in the Lexical feature set, is combined with other semantic features in the
Conventional feature set to form the semantic feature type.
Results are shown in Tables 10–13. Specifically, Tables 10a and 10b show the B3 and
CEAF F-measure scores of the feature analysis experiments involving the mention-ranking
model, using the Lexical feature set on the NW data set. In each table, the first row shows
how the system would perform if each class of features were removed. We remove the least
507

60.6
60.1
59.2
60.7

62.4
53.1
59.7
60.9

63.3
60.1
61.8

64.6
63.5

ee
n
U
ns

A
lia
s

D
ist
an
ce

i-l
ex
ic
al
Se
m

Le
xi
ca
l

Rahman & Ng

65.2

58.4
44.7
53.4
54.7

56.0
55.0
53.5
55.6

60.7
55.3
58.7

61.7
59.3

ee
n
U
ns

A
lia
s

D
ist
an
ce

Le
xi
ca
l

Se
m

i-l
ex
ic
al

(a) B3 results

62.1

(b) CEAF results

Table 10: Feature analysis results (in terms of F-measure scores) for the mention-ranking
model using the Lexical features on the NW data set. When all feature types are used to train
the model, the B3 and CEAF F-measure scores are 65.4 and 62.7, respectively.

important feature class (i.e., the feature class whose removal yields the best performance),
and the next row shows how the adjusted system would perform without each remaining
class. According to both scoring programs, removing the unseen features yields the least
drop to performance (note from the caption that with the full feature set, the B3 score
is 65.4 and the CEAF score is 62.7). In fact, the two scorers agree that the lexical and
semi-lexical features are more important than the unseen, alias, and distance features.
Nevertheless, these results suggest that all five feature types are important, since the best
performance is achieved using the full feature set.
Tables 11a and 11b show the B3 and CEAF F-measure scores of the feature analysis
experiments involving the cluster-ranking model, using the Combined feature set on the NW
data set. Recall that in the Combined feature set, we have seven types of features. As we can
see, the two scorers agree completely on the order in which the features should be removed.
In particular, the most important features are the lexical and semi-lexical features,
whereas the least important features are those that are not present in the Lexical feature
set, namely, the grammatical, string-matching, and semantic features. This suggests
that the lexical features are in general more important than the non-lexical features when
they are used in combination. This is somewhat surprising, as the non-lexical features
are the commonly-used features for coreference resolution, whereas Lexical features are
comparatively much less investigated by coreference researchers. Nevertheless, unlike what
we saw in Table 10, where all feature types appear to be relevant, in Table 11a, we see that
508

57.6
58.6
59.9
63.9

at
ica
l
m

at
ch
m
g

an
tic
Se
m
59.7
60.2
62.4

G
ra
m

58.5
58.8
58.9
57.5
63.8

D
ist
an
ce

U
ns

ee
n

i-l
ex
ica
l
59.7
60.4
61.1
62.3
61.9
67.2

St
rin

54.7
56.6
58.9
58.3
63.2
63.2

Se
m

Le
xi
ca
l

A Cluster-Ranking Approach to Coreference Resolution

59.6
61.1

60.4

55.6
56.9
58.8
60.0

at
ica
l
m

at
ch
m
g

an
tic
Se
m
57.7
58.5
60.1

G
ra
m

56.9
57.6
58.2
56.7
60.0

D
ist
an
ce

U
ns

ee
n

i-l
ex
ica
l
53.3
54.3
57.1
57.2
55.3
61.9

St
rin

50.2
50.6
51.4
51.0
57.9
57.9

Se
m

Le
xi
ca
l

(a) B3 results

58.4
60.6

58.7

(b) CEAF results

Table 11: Feature analysis results (in terms of F-measure) for the cluster-ranking model
using the Combined features on the NW data set. When all feature types are used to train the
model, the B3 and CEAF F-measure scores are 64.6 and 62.3, respectively.

the best B3 F-measure score is 67.2, which is achieved using only the lexical features.
This represents a 2.6% absolute gain in F-measure over the model trained on all seven
feature types, suggesting a learning-based coreference model could be improved via feature
selection.
Next, we investigate whether similar trends can be observed when the models are trained
on a different source: Broadcast News. Specifically, we show in Tables 12a and 12b the B3
and CEAF F-measure scores of the feature analysis experiments involving the mentionranking model, using the Lexical feature set on the BN data set. As in Table 11, we
see that the two scorers agree completely on the order in which the features should be
removed. In fact, similar to what we observed in Table 10 (on the NW data set), both
scorers determine that the lexical and semi-lexical features are the most important,
whereas the distance and alias features are the least important, although all five feature
types appear to be relevant according to both scorers.
Finally, we show in Tables 13a and 13b the B3 and CEAF F-measure scores of the feature
analysis experiments involving the cluster-ranking model, using the Combined feature set
509

53.4
53.4
52.3
52.9

62.7
62.3
61.2
61.6

60.9
59.9
61.7

62.9
62.9

A
lia
s

ee
n
U
ns

D
ist
an
ce

i-l
ex
ica
l
Se
m

Le
xi
ca
l

Rahman & Ng

63.4

47.1
47.1
44.9
45.5

59.6
59.1
57.4
57.5

58.6
57.8
58.0

59.9
60.0

A
lia
s

ee
n
U
ns

D
ist
an
ce

i-l
ex
ica
l
Se
m

Le
xi
ca
l

(a) B3 results

61.2

(b) CEAF results

Table 12: Feature analysis results (in terms of F-measure) for the mention-ranking model
using the Lexical features on the BN data set. When all feature types are used to train the
model, the B3 and CEAF F-measure scores are 63.5 and 61.3, respectively.

on the BN data set. As in Tables 11 and 12, the two scorers agree completely on the order in
which the features should be removed. As far as feature contribution is concerned, these two
tables resemble Tables 11a and 11b: in both cases, the lexical, semi-lexical, and unseen
features are the most important; the string-matching and grammatical features are the
least important; and the semantic and distance features are in the middle. In this case,
however, all seven feature types seem to be relevant, as the best performance is achieved
using the full feature set according to both scorers. Perhaps most interestingly, the numbers
in each column are generally increasing as we move down the column. This means that a
feature type becomes progressively less useful as we remove more and more feature types.
This also suggests that the interactions between different feature types are non-trivial and
that a feature type may be useful only in the presence of another feature type.
In summary, results on two data sets (NW and BN) and two scoring programs demonstrate that (1) in general all feature types are crucial to overall performance, and (2)
the little-investigated Lexical features contribute more to overall performance than the
commonly-used Conventional features.
7.7 Resolution Performance
To gain additional insights into our results, we analyze the behavior of the coreference
models for different types of anaphoric expressions when they are trained with different
feature sets. Specifically, we partition the mentions into different resolution classes. While
510

at
ica
l
m

m

at
ch
51.8
53.3
56.6

g

D
ist
an
ce

an
tic
51.2
54.3
60.6
61.3

G
ra
m

52.3
53.9
56.6
59.5
61.3

Se
m

U
ns

ee
n

i-l
ex
ica
l
52.7
54.2
56.9
60.4
57.4
60.6

St
rin

54.7
55.2
55.4
55.5
56.9
56.9

Se
m

Le
xi
ca
l

A Cluster-Ranking Approach to Coreference Resolution

53.9
55.6

54.8

at
ica
l
m

m

at
ch
47.1
50.0
58.5

g

D
ist
an
ce

an
tic
44.3
51.0
54.0
57.7

G
ra
m

45.2
50.7
54.9
57.0
57.1

Se
m

U
ns

ee
n

i-l
ex
ica
l
45.7
51.3
52.9
56.7
50.5
55.4

St
rin

44.4
44.3
45.5
46.3
48.5
48.6

Se
m

Le
xi
ca
l

(a) B3 results

46.4
51.8

52.1

(b) CEAF results

Table 13: Feature analysis results (in terms of F-measure) for the cluster-ranking model
using the Combined features on the BN data set. When all feature types are used to train the
model, the B3 and CEAF F-measure scores are 63.6 and 61.3, respectively.

previous work has focused mainly on three rather coarse-grained resolution classes (namely,
pronouns, proper nouns, and common nouns), we follow Stoyanov et al. (2009) and subdivide
each class into three fine-grained classes. It is worth mentioning that none of Stoyanov et
al.’s classes corresponds to non-anaphoric expressions. Since we believe that non-anaphoric
expressions play an important role in the analysis of the performance of a coreference
model, we propose three additional classes that correspond to non-anaphoric pronouns,
non-anaphoric proper nouns, and non-anaphoric common nouns. Finally, there are certain
types of anaphoric pronouns (e.g., wh-pronouns) that do not fall into any of Stoyanov et
al.’s pronoun categories. To fill this gap, we create another category that serves as the
default category for any anaphoric pronouns not covered by Stoyanov et al.’s classes. This
results in 13 resolution classes, which are discussed below in detail.
Proper nouns. Four classes are defined for proper nouns. (1) e: a proper noun is assigned
to this exact string match class if there is a preceding mention such that the two are
coreferent and are the same string; (2) p: a proper noun is assigned to this partial string
match class if there is a preceding mention such that the two are coreferent and have some
511

Rahman & Ng

content words in common; (3) n: a proper noun is assigned to this no string match class
if there is no preceding mention such that the two are coreferent and have some content
words in common; and (4) na: a proper noun is assigned to this non-anaphor class if it is
not coreferent with any preceding mention.
Common nouns. Four analogous resolution classes are defined for mentions whose head
is a common noun: (5) e; (6) p; (7) n; and (8) na.
Pronouns. We have three pronoun classes. (9) 1+2: 1st and 2nd person pronouns; (10)
G3: gendered 3rd person pronouns (e.g., she); (11) U3: ungendered 3rd person pronouns;
(12) oa: any anaphoric pronouns that do not belong to (9), (10), and (11); and (13) na:
non-anaphoric pronouns.
Next, we score each resolution class. Unlike Stoyanov et al. (2009), who use a modified
version of the MUC scorer, we employ B3 . The reasons are that the MUC scorer (1) does
not reward singleton clusters, and (2) can inflate a system’s performance when the clusters
are overly large. To compute the score for class C, we process the mentions in a test text in
a left-to-right manner. For each mention encountered, we check whether it belongs to C. If
so, we use our coreference model to decide how to resolve it. Otherwise, we use an oracle to
make the correct resolution decision25 (so that in the end all the mistakes can be attributed
to the incorrect resolution of the mentions in C, thus allowing us to directly measure its
impact on overall performance). After all the test documents are processed, we compute
the B3 F-measure score on only the mentions that belong to C.
Performance of each resolution class, when aggregated over the test sets of the six data
sources in the same way as before, are shown in Table 14, which provides a nice diagnosis
of the strengths and weaknesses of each coreference model when used in combination with
each feature set. We also show in the table the percentage of mentions belonging to each
class below the name of each class, and abbreviate the name of each model as follows: HM
corresponds to the head match baseline, whereas MP, EM, MR, and CR denote the mentionpair model, the entity-mention model, the mention-ranking model, and the cluster-ranking
model, respectively. Each ranking model has two versions, the pipeline version (denoted by
P) and the joint version (denoted by J).
A few points deserve mention. Recall from Table 4 that when the Conventional features
are used, the joint mention-ranking model performs better than the mention-pair model
and the entity-mention model. Comparing row 5 with rows 2 and 3 of Table 14, we can
see that the improvements can be attributed primarily to its better handling of one proper
25. If the oracle determines that a mention is anaphoric and that its antecedents are not in the same cluster
(because our model has previously made a mistake), we employ the following heuristic to select which
antecedent to resolve the mention to: we try to resolve it to the closest preceding antecedent that does
not belong to class C, and if no such antecedent exists, we resolve it to the closest preceding antecedent
that belongs to class C. The reason behind the heuristic’s preference for a preceding antecedent that does
not belong to class C is simple: since we are resolving the mention using an oracle, we want to choose
the antecedent that allows us to maximize the overall score; resolving the mention to an antecedent
that does not belong to C is more likely to yield a better score than resolving it to an antecedent that
belongs to C, since the former was resolved using an oracle but the latter was not. The same heuristic
applies if we are trying to use the oracle to resolve a mention to a preceding cluster: we first attempt to
resolve it to the closest preceding cluster containing a mention that does not belong to C, and if no such
antecedent exists, we resolve it to the closest preceding cluster containing a mention that belongs to C.

512

A Cluster-Ranking Approach to Coreference Resolution

Proper nouns
p
n
na
1.6
3.2
13.9

e
6.3

Common nouns
p
n
na
0.3
4.7
19.2

Class
%

e
15.2

1

HM

68.3

33.0

34.4

63.5

48.1

2
3
4
5
6
7

MP
EM
MR-P
MR-J
CR-P
CR-J

69.6
69.9
78.3
79.4
79.9
79.9

35.4
35.9
41.1
42.5
42.5
43.9

35.6
35.9
32.7
33.4
34.2
34.4

Using
65.8
65.8
76.0
76.4
75.9
76.7

8
9
10
11
12
13

MP
EM
MR-P
MR-J
CR-P
CR-J

78.8
79.1
78.3
79.5
75.3
76.4

41.9
41.8
66.4
67.1
65.7
68.4

32.1
32.4
40.7
41.3
40.4
41.1

Using the Lexical feature set
78.6 66.5 54.2 24.1 77.6
78.4 66.5 54.7 24.1 77.9
75.8 53.2 60.7 28.3 83.3
76.3 54.4 61.1 28.6 83.5
76.6 50.2 61.4 30.3 81.9
77.2 50.8 63.2 31.1 83.1

14
15
16
17
18
19

MP
EM
MR-P
MR-J
CR-P
CR-J

73.8
73.9
76.4
77.2
78.3
79.9

40.1
40.6
50.9
52.3
61.3
62.0

38.8
39.2
33.3
34.7
41.5
42.4

Using the
67.6 55.9
68.2 56.3
80.7 53.4
82.0 54.3
78.3 60.9
79.1 62.8

Pronouns
U3
oa
5.1
4.4

1+2
15.1

G3
4.9

50.7

46.3

41.7

23.2

55.7

the Conventional feature set
56.1 54.7 24.0 70.4 53.8
57.3 55.1 24.3 70.9 54.2
48.5 58.3 27.2 78.2 54.1
48.2 59.0 27.6 78.5 54.4
64.1 58.6 27.1 80.8 57.9
65.0 59.2 27.4 82.1 58.6

55.6
56.0
57.1
57.7
61.8
62.5

46.1
46.4
44.9
45.8
49.7
50.7

24.1
24.6
22.7
23.0
25.6
26.3

51.9
51.7
61.6
62.2
58.1
59.7

55.5
55.7
62.1
62.6
60.3
61.9

57.4
57.8
60.6
62.3
61.0
62.6

44.1
44.1
47.3
47.9
50.6
51.8

24.3
24.2
29.1
31.3
36.2
37.6

61.8
62.1
61.8
62.6
66.3
67.0

58.6
58.7
58.3
59.5
62.1
62.7

60.7
61.6
56.1
59.5
65.5
66.2

49.3
49.6
44.3
45.8
51.4
52.8

27.6
26.2
25.8
26.5
34.7
35.5

58.1
58.3
66.4
67.1
62.7
64.4

55.6

24.7

68.1

Combined feature set
54.8 25.0 73.7
55.8 25.0 74.4
55.4 23.2 78.9
56.8 24.7 80.9
55.4 24.4 79.3
56.8 25.5 79.9

na
6.1

Table 14: B3 F-measure scores of different resolution classes.
noun class (e) and all three classes that correspond to non-anaphoric mentions (na). These
results indicate that it is important to take into account the non-anaphoric mentions when
analyzing the performance of a coreference model. At the same time, we can see that the
joint mention-ranking model does not resolve the type ’e’ common nouns as well as the
mention-pair model and the entity-mention model. Also, results in rows 5 and 7 indicate
that the joint cluster-ranking model is better than the joint mention-ranking model due to
its better handling of the type ’e’ common nouns, the non-anaphoric common nouns, as
well as the anaphoric pronouns.
Next, recall from Table 4 that when the Lexical features are used in lieu of the Conventional features, the mention-pair model, the entity-mention model, and the joint mentionranking model all exhibit significant improvements in performance. For the mention-pair
model and the entity-mention model, such improvements stem primarily from better handling of three proper noun classes (e,p,na), two common noun classes (e,na), and the nonanaphoric pronouns (compare rows 2 and 8 as well as rows 3 and 9 of Table 14). For the joint
mention-ranking model, on the other hand, the improvements accrue from better handling
of two proper noun classes (p,n), two common classes (e,na), and the anaphoric pronouns,
513

Rahman & Ng

as can be seen from rows 5 and 11. While the joint cluster-ranking model does not show
overall improvement as we switch from Conventional to Lexical features (compare rows 7
and 13), the resulting models behave differently. Specifically, using the Lexical features, the
model gets worse at handling one proper noun class (e) and one common noun class (e),
but better at handling another proper noun class (n), two other common noun classes (p,n),
one anaphoric pronoun class (1+2), and the non-anaphoric pronouns.
Finally, recall that when the Combined features are used in lieu of the Lexical features,
all but the cluster-ranking model show a deterioration in performance. For the mention-pair
model and the entity-mention model, the deterioration in performance can be attributed
to poorer handling of two proper noun classes (e,na), two common noun classes (e,na),
and the non-anaphoric pronouns, although they are better at handling one proper noun
class (n) and the anaphoric pronouns (compare rows 8 and 14 as well as rows 9 and 15 of
Table 14). Overall, poorer handling of anaphoricity appears to be a major factor responsible
for the performance deterioration. For the joint mention-ranking model, the reasons for the
performance deterioration are slightly different: comparing rows 11 and 17, we see its poorer
handling of two proper noun classes (p,n), three common noun classes (p,n,na), and the
anaphoric pronouns, although it is better at handling the non-anaphoric proper nouns and
pronouns. As mentioned before, the two versions of the cluster-ranking model improve
when they are trained on the Combined features. However, such improvements do not stem
from improvements for all classes (compare rows 12 and 18 as well as rows 13 and 19).
For instance, when replacing the Lexical features with the Combined features in the joint
cluster-ranking model, we see improvements for two proper noun classes (e,na), one common
noun class (e), and several pronoun classes (1+2,G3,U3), but performance drops for another
proper noun class (p), three other common noun classes (p,n,na), and two pronoun classes
(oa,na).
Overall, these results provide us with additional insights into the strengths and weaknesses of a learning-based coreference model as well as directions for future work. In particular, even if two models yield similar overall performance, they can be quite different at
the resolution class level. Since there is no single coreference model that outperforms the
others on all resolution classes, it may be beneficial to apply an ensemble approach, where
an anaphor belonging to a particular resolution class is resolved by the model that offers
the best performance for that class.

8. Conclusions
As Mitkov (2001, p. 122) puts it, coreference resolution is a “difficult, but not intractable
problem,” and researchers have been making “steady progress” on improving machine learning approaches to the problem in the past fifteen years. The progress is slow, however.
Despite its deficiencies, the mention-pair model was widely thought to be the only learningbased coreference model for almost a decade. The entity-mention model and the mentionranking model emerged only after the mention-pair model has dominated learning-based
coreference research for nearly ten years. Although these two models are conceptually simple, they represent a significant departure from the mention-pair model and a new way of
thinking about how alternative models of coreference can be designed. Our cluster-ranking
model further advances learning-based coreference research theoretically by combining the
514

A Cluster-Ranking Approach to Coreference Resolution

strengths of these two models, thereby addressing two commonly cited weaknesses of the
mention-pair model. It not only bridges the gap between two independent lines of learningbased coreference research — one concerning the entity-mention model and the other the
mention-ranking model — that has been going on for the past few years, but also narrows the modeling gap between the sophistication of rule-based coreference models and
the simplicity of learning-based coreference models. Empirically, we have shown using the
ACE 2005 coreference data set that the cluster-ranking model acquired by jointly learning
anaphoricity determination and coreference resolution surpasses the performance of several
competing approaches, including the mention-pair model, the entity-mention model, and
the mention-ranking model. Perhaps equally importantly, our cluster-ranking model is the
only model considered here that can profitably exploit the information provided by two
fairly different sources of information, the Conventional features and the Lexical features.
While ranking is a more natural formulation of coreference resolution than classification,
ranking-based coreference models have not been more popularly used than the influential
mention-pair model. One of our goals in this article is to promote the application of ranking
techniques to coreference resolution. Specifically, we attempted to clarify the difference between classification-based and ranking-based coreference models by showing the constrained
optimization problem that an SVM learner needs to solve for each type of models, hoping
that this will help the reader appreciate the importance of ranking for coreference resolution. In addition, we have provided ample empirical evidence that ranking-based models
are superior to classification-based models for coreference resolution.
Another contribution of our work lies in the empirical demonstration of the benefits of
lexicalizing learning-based coreference models. While previous work showed that lexicalization only provides marginal benefits to a coreference model, we showed that lexicalization can significantly improve the mention-pair model, the entity-mention model, and the
mention-ranking model, to the point where they approach or even surpass the performance
of the cluster-ranking model. Interestingly, we showed that these models benefit from lexicalization the most when no conventional coreference features are used. This challenges
the common belief that there is a prototypical set of linguistic features (e.g., gender and
number agreement) that must be used for constructing learning-based coreference systems.
In addition, our feature analysis experiments indicated that the conventional features contributed less to overall performance than the rarely studied lexical features for our joint
cluster-ranking coreference model when the two types of features are used in combination.
Finally, we examined the performance of each coreference model in resolving mentions
belonging to different resolution classes. We found that even if two models achieve similar
overall performance, they can be quite different at the resolution class level. Overall, these
results provide us with additional insights into the strengths and weaknesses of a learningbased coreference model as well as promising directions for future research.

Bibliographic Note
Portions of this work were previously presented in a conference publication (Rahman &
Ng, 2009). The current article extends this work in several ways, most notably: (1) an
overview of the literature on ranking approaches to coreference resolution (Section 2); (2) a
detailed explanation of the difference between classification and ranking (Section 3); (3) an
515

Rahman & Ng

investigation of the issues in lexicalizing coreference models (Section 5); and (4) an in-depth
analysis of the different aspects of our coreference system (Section 7).

Acknowledgments
The authors acknowledge the support of National Science Foundation (NSF) grant IIS0812261. We thank the three anonymous reviewers for insightful comments and for unanimously recommending this article for publication in JAIR. Any opinions, findings, conclusions or recommendations expressed in this article are those of the authors and do not
necessarily reflect the views or official policies, either expressed or implied, of NSF.

References
Androutsopoulos, I., & Malakasiotis, P. (2010). A survey of paraphrasing and textual
entailment methods. Journal of Artificial Intelligence Research, 38 , 135–187.
Aone, C., & Bennett, S. W. (1995). Evaluating automated and manual acquisition of
anaphora resolution strategies. In Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics (ACL), pp. 122–129.
Bagga, A., & Baldwin, B. (1998). Algorithms for scoring coreference chains. In Proceedings
of the Linguistic Coreference Workshop at The First International Conference on
Language Resources and Evaluation (LREC), pp. 563–566.
Barbu, C., & Mitkov, R. (2001). Evaluation tool for rule-based anaphora resolution methods. In Proceedings of the 39th Annual Meeting of the Association for Computational
Linguistics (ACL), pp. 34–41.
Bengtson, E., & Roth, D. (2008). Understanding the values of features for coreference
resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 294–303.
Berger, A. L., Della Pietra, S. A., & Della Pietra, V. J. (1996). A maximum entropy
approach to natural language processing. Computational Linguistics, 22 (1), 39–71.
Bikel, D. M., Schwartz, R., & Weischedel, R. M. (1999). An algorithm that learns what’s
in a name. Machine Learning: Special Issue on Natural Language Learning, 34 (1–3),
211–231.
Blum, A., & Langley, P. (1997). Selection of relevant features and examples in machine
learning. Artificial Intelligence, 97 (1–2), 245–271.
Burges, C. J. C. (1998). A tutorial on support vector machines for pattern recognition.
Data Mining and Knowledge Discovery, 2 (2), 121–167.
Cai, J., & Strube, M. (2010). Evaluation metrics for end-to-end coreference resolution
systems. In Proceedings of the 11th Annual SIGdial Meeting on Discourse and Dialogue
(SIGDIAL), pp. 28–36.
Carbonell, J., & Brown, R. (1988). Anaphora resolution: A multi-strategy approach. In
Proceedings of the 12th International Conference on Computational Linguistics (COLING), pp. 96–101.
516

A Cluster-Ranking Approach to Coreference Resolution

Cardie, C., & Wagstaff, K. (1999). Noun phrase coreference as clustering. In Proceedings
of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP/VLC), pp. 82–89.
Charniak, E., & Elsner, M. (2009). EM works for pronoun anaphora resolution. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pp. 148–156.
Collins, M. J. (1999). Head-Driven Statistical Models for Natural Language Parsing. Ph.D.
thesis, Department of Computer and Information Science, University of Pennsylvania,
Philadelphia, PA.
Connolly, D., Burger, J. D., & Day, D. S. (1994). A machine learning approach to anaphoric
reference. In Proceedings of International Conference on New Methods in Language
Processing, pp. 255–261.
Culotta, A., Wick, M., & McCallum, A. (2007). First-order probabilistic models for coreference resolution. In Human Language Technologies 2007: The Conference of the North
American Chapter of the Association for Computational Linguistics; Proceedings of
the Main Conference (NAACL HLT), pp. 81–88.
Daumé III, H., & Marcu, D. (2005). A large-scale exploration of effective global features
for a joint entity detection and tracking model. In Proceedings of Human Language
Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pp. 97–104.
Denis, P., & Baldridge, J. (2007a). Global, joint determination of anaphoricity and coreference resolution using integer programming. In Human Language Technologies 2007:
The Conference of the North American Chapter of the Association for Computational
Linguistics; Proceedings of the Main Conference (NAACL HLT), pp. 236–243.
Denis, P., & Baldridge, J. (2007b). A ranking approach to pronoun resolution. In Proceedings
of the Twentieth International Conference on Artificial Intelligence (IJCAI), pp. 1588–
1593.
Denis, P., & Baldridge, J. (2008). Specialized models and ranking for coreference resolution.
In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 660–669.
Enrique, A., Gonzalo, J., Artiles, J., & Verdejo, F. (2009). A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval, 12 (4),
461–486.
Fellbaum, C. (1998). WordNet: An electronic lexical database. MIT Press, Cambridge, MA.
Finkel, J. R., Grenager, T., & Manning, C. (2005). Incorporating non-local information into
information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguistics (ACL), pp. 363–370.
Finkel, J. R., & Manning, C. (2008). Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT Short Papers (Companion Volume), pp. 45–48.
Florian, R., Hassan, H., Ittycheriah, A., Jing, H., Kambhatla, N., Luo, X., Nicolov, N., &
Roukos, S. (2004). A statistical model for multilingual entity detection and tracking.
In HLT-NAACL 2004: Main Proceedings, pp. 1–8.
517

Rahman & Ng

Ge, N., Hale, J., & Charniak, E. (1998). A statistical approach to anaphora resolution.
Proceedings of the Sixth Workshop on Very Large Corpora (WVLC), pp. 161–170.
Grosz, B. J., Joshi, A. K., & Weinstein, S. (1983). Providing a unified account of definite noun phrases in discourse. In Proceedings of the 21th Annual Meeting of the
Association for Computational Linguistics (ACL), pp. 44–50.
Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995). Centering: A framework for modeling
the local coherence of discourse. Computational Linguistics, 21 (2), 203–226.
Haghighi, A., & Klein, D. (2010). Coreference resolution in a modular, entity-centered
model. In Human Language Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational Linguistics (NAACL HLT),
pp. 385–393.
Hobbs, J. (1978). Resolving pronoun references. Lingua, 44, 311–338.
Iida, R., Inui, K., & Matsumoto, Y. (2009). Capturing salience with a trainable cache
model for zero-anaphora resolution. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP (ACL-IJCNLP), pp. 647–655.
Iida, R., Inui, K., Takamura, H., & Matsumoto, Y. (2003). Incorporating contextual cues
in trainable models for coreference resolution. In Proceedings of the EACL Workshop
on The Computational Treatment of Anaphora.
Ji, H., Rudin, C., & Grishman, R. (2006). Re-Ranking algorithms for name tagging. In
Proceedings of Workshop on Computationally Hard Problems and Joint Inference in
Speech and Language Processing, pp. 49–56.
Ji, H., Westbrook, D., & Grishman, R. (2005). Using semantic relations to refine coreference
decisions. In Proceedings of Human Language Technology Conference and Conference
on Empirical Methods in Natural Language Processing (HLT/EMNLP), pp. 17–24.
Joachims, T. (1999). Making large-scale SVM learning practical. In Schölkopf, B., Burges,
C., & Smola, A. (Eds.), Advances in Kernel Methods – Support Vector Learning, pp.
44–56. MIT Press, Cambridge, MA.
Joachims, T. (2002). Optimizing search engines using clickthrough data. In Proceedings
of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD), pp. 133–142.
Kehler, A., Appelt, D., Taylor, L., & Simma, A. (2004). The (non)utility of predicateargument frequencies for pronoun interpretation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for
Computational Linguistics (HLT/NAACL), pp. 289–296.
Kuhn, H. W. (1955). The Hungarian method for the assignment problem. Naval Research
Logistics Quarterly, 2, 83–97.
Lappin, S., & Leass, H. (1994). An algorithm for pronominal anaphora resolution. Computational Linguistics, 20 (4), 535–562.
Lin, D., & Wu, X. (2009). Phrase clustering for discriminative learning. In Proceedings of
the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
518

A Cluster-Ranking Approach to Coreference Resolution

Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP), pp.
1030–1038.
Luo, X. (2005). On coreference resolution performance metrics. In Proceedings of Human
Language Technology Conference and Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP), pp. 25–32.
Luo, X., Ittycheriah, A., Jing, H., Kambhatla, N., & Roukos, S. (2004). A mentionsynchronous coreference resolution algorithm based on the Bell tree. In Proceedings
of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),
pp. 135–142.
Madnani, N., & Dorr, B. (2010). Generating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics, 36 (3), 341–387.
McCarthy, J., & Lehnert, W. (1995). Using decision trees for coreference resolution. In Proceedings of the Fourteenth International Conference on Artificial Intelligence (IJCAI),
pp. 1050–1055.
Mitkov, R. (1998). Robust pronoun resolution with limited knowledge. In Proceedings of
the 36th Annual Meeting of the Association for Computational Linguistics and 17th
International Conference on Computational Linguistics (COLING/ACL), pp. 869–
875.
Mitkov, R. (2001). Outstanding issues in anaphora resolution. In Gelbukh, A. (Ed.),
Computational Linguistics and Intelligent Text Processing, pp. 110–125. Springer.
Mitkov, R. (2002). Anaphora Resolution. Longman.
Morris, J., & Hirst, G. (1991). Lexical cohesion computed by thesaural relations as an
indicator of the struture of text. Computational Linguistics, 17 (1), 21–48.
Morton, T. (2000). Coreference for NLP applications. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguistics (ACL).
MUC-6 (1995). Proceedings of the Sixth Message Understanding Conference (MUC-6).
Morgan Kaufmann, San Francisco, CA.
MUC-7 (1998). Proceedings of the Seventh Message Understanding Conference (MUC-7).
Morgan Kaufmann, San Francisco, CA.
Ng, V. (2009). Graph-cut-based anaphoricity determination for coreference resolution. In
Proceedings of the 2009 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies (NAACL HLT), pp.
575–583.
Ng, V., & Cardie, C. (2002a). Identifying anaphoric and non-anaphoric noun phrases to
improve coreference resolution. In Proceedings of the 19th International Conference
on Computational Linguistics (COLING), pp. 730–736.
Ng, V., & Cardie, C. (2002b). Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational
Linguistics (ACL), pp. 104–111.
519

Rahman & Ng

Poesio, M., Uryupina, O., Vieira, R., Alexandrov-Kabadjov, M., & Goulart, R. (2004).
Discourse-new detectors for definite description resolution: A survey and a preliminary
proposal. In Proeedings of the ACL Workshop on Reference Resolution.
Ponzetto, S. P., & Strube, M. (2006). Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference and Conference of the North American Chapter of the Association
for Computational Linguistics (HLT/NAACL), pp. 192–199.
Rahman, A., & Ng, V. (2009). Supervised models for coreference resolution. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 968–977.
Ravichandran, D., Hovy, E., & Och, F. J. (2003). Statistical QA - classifier vs. re-ranker:
What’s the difference? In Proceedings of the ACL 2003 Workshop on Multilingual
Summarization and Question Answering, pp. 69–75.
Recasens, M., & Hovy, E. (2011). BLANC: Implementing the Rand Index for coreference
resolution. Natural Language Engineering (to appear).
Roth, D. (2002). Reasoning with classifiers.. In Proceedings of the 13th European Conference
on Machine Learning (ECML), pp. 506–510.
Roth, D., & Yih, W.-T. (2009). A linear programming formulation for global inference in
natural language tasks.. In Proceedings of the Eighth Conference on Computational
Natural Language Learning (CoNLL), pp. 1–8.
Soon, W. M., Ng, H. T., & Lim, D. C. Y. (2001). A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics, 27 (4), 521–544.
Stoyanov, V., Gilbert, N., Cardie, C., & Riloff, E. (2009). Conundrums in noun phrase
coreference resolution: Making sense of the state-of-the-art. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP), pp.
656–664.
Strube, M., & Müller, C. (2003). A machine learning approach to pronoun resolution in
spoken dialogue. In Proceedings of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 168–175.
Strube, M., Rapp, S., & Müller, C. (2002). The influence of minimum edit distance on
reference resolution. In Proceedings of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pp. 312–319.
Toutanova, K., Klein, D., Manning, C. D., & Singer, Y. (2003). Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-NAACL 2003: Proceedings of the
Main Conference, pp. 173–180.
Uryupina, O. (2003). High-precision identification of discourse new and unique noun
phrases. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics: Companion Volume, pp. 80–86.
Vapnik, V. N. (1995). The Nature of Statistical Learning. Springer, New York.
520

A Cluster-Ranking Approach to Coreference Resolution

Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). A modeltheoretic coreference scoring scheme. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pp. 45–52.
Walker, M., Joshi, A., & Prince, E. (Eds.). (1998). Centering Theory in Discourse. Oxford
University Press.
Yang, X., Su, J., Lang, J., Tan, C. L., & Li, S. (2008). An entity-mention model for
coreference resolution with inductive logic programming. In Proceedings of the 46th
Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies (ACL-08: HLT), pp. 843–851.
Yang, X., Su, J., Zhou, G., & Tan, C. L. (2004). An NP-cluster based approach to coreference
resolution. In Proceedings of the 20th International Conference on Computational
Linguistics (COLING), pages 226–232.
Yang, X., Zhou, G., Su, J., & Tan, C. L. (2003). Coreference resolution using competitive
learning approach. In Proceedings of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 176–183.

521

Journal of Artificial Intelligence Research 40 (2011) 1-24

Submitted 09/10; published 01/11

Non-Deterministic Policies in
Markovian Decision Processes
Mahdi Milani Fard
Joelle Pineau

mmilan1@cs.mcgill.ca
jpineau@cs.mcgill.ca

Reasoning and Learning Laboratory
School of Computer Science, McGill University
Montreal, QC, Canada

Abstract
Markovian processes have long been used to model stochastic environments. Reinforcement learning has emerged as a framework to solve sequential planning and decision-making
problems in such environments. In recent years, attempts were made to apply methods
from reinforcement learning to construct decision support systems for action selection in
Markovian environments. Although conventional methods in reinforcement learning have
proved to be useful in problems concerning sequential decision-making, they cannot be applied in their current form to decision support systems, such as those in medical domains,
as they suggest policies that are often highly prescriptive and leave little room for the user’s
input. Without the ability to provide flexible guidelines, it is unlikely that these methods
can gain ground with users of such systems.
This paper introduces the new concept of non-deterministic policies to allow more flexibility in the user’s decision-making process, while constraining decisions to remain near
optimal solutions. We provide two algorithms to compute non-deterministic policies in
discrete domains. We study the output and running time of these method on a set of
synthetic and real-world problems. In an experiment with human subjects, we show that
humans assisted by hints based on non-deterministic policies outperform both human-only
and computer-only agents in a web navigation task.

1. Introduction
Planning and decision-making have been well studied in the AI community. Intelligent
agents have been designed and developed to act in, and interact with, a variety of environments. This usually involves sensing the environment, making a decision using some
intelligent inference mechanism, and then performing an action on the environment (Russell & Norvig, 2003). Often times, this process involves some level of learning, along with
the decision-making process, to make the agent more efficient in performing the intended
goal.
Reinforcement Learning (RL) is a branch of AI that tries to develop a computational
approach to solving the problem of learning through interaction. RL is the process of
learning what to do—how to map situations to actions—so as to maximize a numerical
reward signal (Sutton & Barto, 1998). Many methods have been developed to solve the
RL problem with different types of environments and different types of agents. However,
most of the work in RL has focused on autonomous agents such as robots or software
agents. The RL controllers are thus designed to issue a single action at each time-step
c
2011
AI Access Foundation. All rights reserved.

Milani Fard & Pineau

which will be executed by the acting agent. In the past few years, methods developed by
the RL community have started to be used in sequential decision support systems (Murphy,
2005; Pineau, Bellemare, Rush, Ghizaru, & Murphy, 2007; Thapa, Jung, & Wang, 2005;
Hauskrecht & Fraser, 2000). In many of these systems, a human being makes the final
decision. Usability and acceptance issues thus become important in these cases. Most
RL methods therefore require some level of adaptation to be used with decision support
systems. Such adaptations are the main contribution of this paper.
Medical domains are among the cases for which RL needs further adaptation. Although
the RL framework correctly models sequential decision-making in complex medical scenarios, including long-term treatment design, standard RL methods cannot be applied to
medical settings in their current form as they lack flexibility in their suggestions. Such
requirements are, of course, not specific to medical domains and, for instance, might be
needed in an aircraft controller that provides suggestions to a pilot.
The important difference between decision support system and the classical RL problem
stems from the fact that in the decision support system, the acting agent is often a human
being, which of course has his/her own decision process. Therefore, the assumption that
the controller should only send one clear commanding signal to the acting agent is not
appropriate. It is more accurate to assume that some aspect of the decision-making process
will be influenced by the user of the system.
This view of the decision process is particularly relevant in two different situations.
First, in many practical cases, we do not have an exact model of the system. Instead, we
may have a noisy model built from a finite number of interactions with the environment.
This leads to a type of uncertainty that is usually referred to as extrinsic uncertainty. Most
RL algorithms ignore this uncertainty and assume that the model is perfect. However if we
look closely, the performance of the optimal action based on an imperfect model might not
be statistically different from the next best action. Bayesian approaches have looked at this
problem by providing confidence measure over the agent’s performance (Mannor, Simester,
Sun, & Tsitsiklis, 2007). In cases where the acting agent is a human being, we can use these
confidence measures to provide the user with a complete set of actions, each of which might
be the optimal and for which we do not have enough evidence to differentiate. The user
can then use his/her expertise to make the final decision. Such methods should guarantee
that the suggestions provided by the system are statistically meaningful and plausible.
On the other hand, even when we do have complete knowledge of the system and we can
identify the optimal action, there might still be other actions which are “roughly equal” in
performance. At this point, the decision between these near-optimal options could be left to
the acting agent—namely the human being that is using the decision support system. This
could have many advantages, ranging from better user experience, to increased robustness
and flexibly. Among the near-optimal solutions, the user can select based on further domain
knowledge, or any other preferences, that are not captured by the system. For instance, in
a medical diagnosis system that suggests treatments, providing the physician with several
options might be useful as the final decision could be made based on further knowledge of
the patient’s medical status, or preferences regarding side effects.
Throughout this paper we address the latter issue through a combination of theoretical
and empirical investigations. We introduce the new concept of non-deterministic policies
to capture a decision-making process intended for decision support systems. Such policies
2

Non-Deterministic Policies in Markovian Decision Processes

involve suggesting a set of actions, from which a non-deterministic choice is made by the
user. We apply this formulation to solve the problem of finding near-optimal policies to
provide flexible suggestions to the user.
In particular, we investigate how we can suggest several actions to the acting agent,
while providing performance guarantees with a worst-case analysis. Section 2 introduces the
necessary technical background material. Section 3 defines the concept of non-deterministic
policies and related concepts. Section 4 addresses the problem of providing choice to the
acting agent while keeping near-optimality guarantees on the performance of the worst-case
scenario. We propose two algorithms to solve these problems and provide approximation
techniques to speed up the computation for larger domains.
Methods introduced in this paper are general enough to apply to any decision support system in an observable Markovian environment. Our empirical investigations focus
primarily on sequential decision-making problems in clinical domains, where the system
should provide suggestions on the best treatment options for patients. These decisions are
provided over a sequence of treatment phases. These systems are specifically interesting
because often times, different treatment options seem to provide only slightly different results. Therefore, providing the physician with several suggestions would be beneficial in
improving the usability of the system and the performance of the final decision.

2. Definitions and Notations
This section introduces the main notions behind sequential decision-making and the mathematical formulations used in RL.
2.1 Markov Decision Processes
A Markov Decision Process (MDP) is a model of system dynamics in sequential decision
problems that involves probabilistic uncertainty about future states of the system (Bellman,
1957). MDPs are used to model the interactions between an agent and an observable
Markovian environment. The system is assumed to be in a state at any given time. The
agent observes the state and performs an action accordingly. The system then makes a
transition to the next state and the agent receives some reward.
Formally, an MDP is defined by the 5-tuple (S, A, T, R, γ):
• States: S is the set of states. The state usually captures the complete configuration
of the system. Once the state of the system is known, the future of the system is
independent from all previous system transitions. This means that the state of the
system is a sufficient statistic of the history of the system.
• Actions: A : S → 2A is the set of actions allowed in each state where A is the set
of all actions. A(s) is the set of actions the agent can choose from, while interacting
with the system in state s.
• Transition Probabilities: T : S × A × S → [0, 1] defines the transition probabilities
of the system. This function specifies how likely it is to end up at any state, given the
current state and a specific action performed by the agent. Transition probabilities
3

Milani Fard & Pineau

are specified based on the Markovian assumption. That is, if the state of the system
at time t is denoted by st and the action at that time is at , then we have:
Pr(st+1 |at , st , at−1 , at−1 , . . . , a0 , s0 ) = P r(st+1 |at , st ).

(1)

We focus on homogeneous processes in which the system dynamics are independent
of the time. Thus the transition function is stationary with respect to time:
def

T (s, a, s0 ) = P r(st+1 = s0 |at = a, st = s).

(2)

• Rewards: R : S × A × R → [0, 1] is the probabilistic reward model. Depending on
the current state of the system and the action taken, the agent will receive a reward
drawn from this model. We focus on homogeneous processes in which, again, the
reward distribution does not change over time. If the reward at time t is denoted by
rt , then we have:
rt ∼ R(st , at ).

(3)

Depending on the domain, the reward could be deterministic or stochastic. We use
the general stochastic model throughout this paper. The mean of this distribution is
denoted by R̄(s, a).
• Discount Factor: γ ∈ [0, 1) is the discount rate used to calculate the long-term
return.
The agent starts in an initial state s0 ∈ S. At each time step t, an action at ∈ A(st ) is
taken by the agent. The system then makes a transition to st+1 ∼ T (st , at ) and the agent
receives an immediate reward rt ∼ R(st , at ).
The goal of the agent is to maximize the discounted sum of rewards over the planning
horizon h (could be infinite). This is usually referred to as the return (denoted by D):
D=

h
X

γ t rt .

(4)

t=0

In the finite horizon case, this sum is taken up to the horizon limit and the discount factor
can be set to 1. However, in the infinite horizon case the discount factor should be less
than 1 so that the return has finite value. The return on the process depends on both the
stochastic transitions and rewards, as well as the actions taken by the agent.
Often times the transition structure of the MDP contains no loop with non-zero probability. Such a transition graph can be modeled by a directed acyclic graph (DAG). This
class of MDPs is interesting as it includes multi-step decision-making in finite horizons, such
as those found in medical domains.
2.2 Policy and Value Function
A policy is a way of defining the agent’s action selection with respect to the changes in the
environment. A (probabilistic) policy on an MDP is a mapping from the state space to a
distribution over the action space:
π : S × A → [0, 1].
4

(5)

Non-Deterministic Policies in Markovian Decision Processes

A deterministic policy is a policy that defines a single action per state. That is, π(s) ∈
A(s). We will later introduce the notion of non-deterministic policies on MDPs to deal with
sets of actions.
The agent interacts with the environment and takes actions according to the policy. The
value function of the policy is defined to be the expectation of the return given that the
agent acts according to that policy:
"∞
#
X
def
V π (s) = E[Dπ (s)] = E
γ t rt |s0 = s, at = π(st ) .
(6)
t=0

Using the linearity of the expectation, we can write the above expression in a recursive
form, known as the Bellman equation (Bellman, 1957):
"
#
X
X
π
0
π 0
V (s) =
π(s, a) R̄(s, a) +
T (s, a, s )V (s ) .
(7)
s0 ∈S

a∈A

The value function has been used as the primary measure of performance in much of
the RL literature. There are, however, some ideas that take the risk or the variance of the
return into account as a measure of optimality (Heger, 1994; Sato & Kobayashi, 2000). The
more common criteria, though, is to assume that the agent is trying to find a policy that
maximizes the value function. Such a policy is referred to as the optimal policy.
We can also define the value function over the state-action pairs. This is usually referred
to as the Q-function, or the Q-value, of that pair. By definition:
"∞
#
X
def
(8)
Qπ (s, a) = E[Dπ (s, a)] = E
γ t rt |s0 = s, a0 = a, t ≥ 1 : at = π(st ) .
t=0

That is, the Q-value is the expectation of the return, given that the agent starts in state
s, takes action a, and then follows policy π. The Q-function also satisfies the Bellman
equation:
X
X
Qπ (s, a) = R̄(s, a) +
T (s, a, s0 )
π(s0 , a0 )Qπ (s0 , a0 ),
(9)
s0 ∈S

a0 ∈A

which can be rewritten as:
Qπ (s, a) = R̄(s, a) +

X

T (s, a, s0 )V π (s0 ).

(10)

s0 ∈S

The Q-function is often used to compare the optimality of different actions given a fixed
subsequent policy.
2.3 Planning Algorithms and Optimality
The optimal policy, denoted by π ∗ , is defined to be the policy that maximizes the value
function at the initial state:
π ∗ = argmax V π (s0 ).
π∈Π

5

(11)

Milani Fard & Pineau

It has been shown (Bellman, 1957) that for any MDP, there exists an optimal deterministic policy that is no worse than any other policy for that MDP. The value of the optimal
policy V ∗ satisfies the Bellman optimality equation:
"
#
X
V ∗ (s) = max R̄(s, a) +
T (s, a, s0 )V ∗ (s0 ) .
(12)
a∈A

s0 ∈S

The deterministic optimal policy follows from this:
"
#
X
∗
0
∗ 0
π (s) = argmax R̄(s, a) +
T (s, a, s )V (s ) .
a∈A

(13)

s0 ∈S

Alternatively we can write these equations with the Q-function:
X
Q∗ (s, a) = R̄(s, a) +
T (s, a, s0 )V ∗ (s0 ).

(14)

s0 ∈S

Thus V ∗ (s) = maxa Q∗ (s, a) and π ∗ (s) = argmaxa Q∗ (s, a).
Much of the literature in RL has focused on finding the optimal policy. There are many
methods developed for policy optimization. One way to find the optimal policy is to solve
the Bellman optimality equation and then use Eqn 13 to choose the actions. The Bellman
optimality equation can be formulated as a simple linear program (Bertsekas, 1995):
minV µT V, subject to
P
V (s) ≥ R̄(s, a) + γ s0 T (s, a, s0 )V (s0 ) ∀s, a,

(15)

where µ represents an initial distribution over the states. The solution to the above problem is the optimal value function. Notice that V is represented in matrix form in this
equation. It is known that linear programs can be solved in polynomial time (Karmarkar,
1984). However, solving them might become impractical in large (or infinite) state spaces.
Therefore often times methods based on dynamic programming are preferred to the linear
programming solution.

3. Non-Deterministic Policies: Definition and Motivation
We begin this section by considering the problem of decision-making in sequential decision
support systems. Recently, MDPs have emerged as useful frameworks for optimizing action
choices in the context of medical decision support systems (Schaefer, Bailey, Shechter, &
Roberts, 2004; Hauskrecht & Fraser, 2000; Magni, Quaglini, Marchetti, & Barosi, 2000;
Ernst, Stan, Concalves, & Wehenkel, 2006). Given an adequate MDP model (or data
source), many methods can be used to find a good action-selection policy. This policy is
usually a deterministic or stochastic function. But policies of these types face a substantial
barrier in terms of gaining acceptance from the medical community, because they are highly
prescriptive and leave little room for the doctor’s input. These problems are, of course, not
specific to the medical domain and are present in any application where the actions are
executed by a human. In such cases, it may be preferable to provide several equivalently
6

Non-Deterministic Policies in Markovian Decision Processes

good action choices, so that the agent can pick among those according to his or her own
heuristics and preferences.
To address this problem, this work introduces the notion of a non-deterministic policy,
which is a function mapping each state to a set of actions, from which the acting agent can
choose.
Definition 1. A non-deterministic policy Π on an MDP (S, A, T, R, γ) is a function
that maps each state s ∈ S to a non-empty set of actions denoted by Π(s) ⊆ A(s). The
agent can choose to do any action a ∈ Π(s) whenever the MDP is in state s.
Definition 2. The size of a non-deterministic
policy Π, denoted by |Π|, is the sum of the
P
cardinality of the action sets in Π: |Π| = s |Π(s)|.
In the following sections we discuss two scenarios in which non-deterministic policies
can be useful. We show how they can be used to implement more robust decision support
systems with statistical guarantees of performance.
3.1 Providing Choice to the Acting Agent
Even in cases where we have complete knowledge of the dynamics of the planning problem
at hand, and when we can accurately calculate actions’ utilities, it might not be desirable to
provide the user with only the optimal choice of action at each time step. In some domains,
the difference between the utility of the top few actions may not be substantial. In medical
decision-making, for instance, this difference may not be medically significant based on the
given state variables.
In such cases, it seems natural to let the user decide between the top few actions,
using his/her own expertise in the domain. This results in a further injection of domain
knowledge in the decision-making process, thus making it more robust and practical. Such
decisions can be based on facts known to the user that are not incorporated in the automated
planning system. It can also be based on preferences that might change from case to case.
For instance, a doctor can get several recommendations as to how to treat a patient to
maximize the chance of remission, but then decide what medication to apply considering
also the patient’s medical record, preferences regarding side effects, or medical expenses.
This idea of providing choice to the user should be accompanied by reasonable guarantees on the performance of the final decision, regardless of the choice made by the user. A
notion of near-optimality should be enforced to make sure the actions are never far from
the best possible option. Such guarantees are enforced by providing a worst-case analysis
on the decision process.
3.2 Handling Model Uncertainty
In many practical cases we do not have complete knowledge of the system at hand. Instead,
we may get a set of trajectories collected from the system according to some specific policy.
In some cases, we may be given the chance to choose this policy (in on-line and active RL),
and in other cases we may have access only to data from some fixed policy. In medical
trials, in particular, data is usually collected according to a randomized policy, fixed ahead
of time through consultation with the clinical researchers.
7

Milani Fard & Pineau

Given a set of sample trajectories, we can either build a model of the domain (in modelbased approaches) or directly estimate the utility of different actions (with model-free approaches). However these models and estimates are not always accurate when we only
observe a finite amount of data. In many cases, the data may be too sparse and incomplete
to uniquely identify the best option. That is, the difference in the performance measure of
different actions is not statistically significant.
There are other cases where it might be useful to let the user decide on the final choice
between those actions for which we do not have enough evidence to differentiate. This
comes with the assumption that the user can identify the best choice among those that are
recommended. The task is therefore to provide the user with a small set of actions that will
almost surely include the optimal one.
In this paper we only focus on the problem of providing flexible policies with nearoptimal performance. Using non-deterministic policies for handling model uncertainty remains an interesting future work.

4. Near-Optimal Non-Deterministic Policies
Often times, it is beneficial to provide the user of a decision support system with a set of
near-optimal solutions. With MDPs, this would be to suggest a set of near-optimal actions
to the user and let the user make a decision among the proposed actions. The notion of
near-optimality should therefore be on the set of all possible policies that are consistent
with the proposed actions. That is, no matter which action is chosen among the proposed
options at each state, the final performance should be close to that of the optimal policy.
Such constraint suggests a worst-case analysis of the decision-making process. Therefore, we
opt to guarantee the performance of any action selection consistent with a non-deterministic
policy by putting the near-optimality constraint on the worst-case selection of actions by
the user.
Definition 3. The (worst-case) value of a state-action pair (s, a) according to a nondeterministic policy Π on an MDP M = (S, A, T, R, γ) is given by the recursive definition:

X
Π
0
Π
0 0
QM (s, a) = R̄(s, a) + γ
T (s, a, s ) min QM (s , a ) ,
(16)
a0 ∈Π(s0 )

s0 ∈S

which is the worst-case expected return under the allowed set of actions.
Definition 4. We define the (worst-case) value of a state s according to a nonΠ (s), to be:
deterministic policy Π, denoted by VM
min QΠ
M (s, a).

(17)

a∈Π(s)

To calculate the value of a non-deterministic policy, we construct an evaluation MDP,
M 0 = (S, A0 , R0 , T, γ), where A0 = Π and R0 = −R.
Theorem 1. The negated value of the non-deterministic policy Π is equal to that of the
optimal policy on the evaluation MDP:
∗
QΠ
M (s, a) = −QM 0 (s, a).

8

(18)

Non-Deterministic Policies in Markovian Decision Processes

Proof. We show that if Q∗M 0 satisfies the Bellman optimality equation on M 0 , then the
negated values satisfy Eqn 16 on M :
X
Q∗M 0 (s, a) = R̄0 (s, a) + γ
T (s, a, s0 ) max
Q∗M 0 (s0 , a0 )
(19)
0
0
a ∈A

s0 ∈S

⇒ −Q∗M 0 (s, a) = −R̄0 (s, a) − γ

X

T (s, a, s0 ) max
Q∗M 0 (s0 , a0 )
0
0

(20)

T (s, a, s0 ) min −Q∗M 0 (s0 , a0 ),

(21)

a ∈A

s0 ∈S

⇒ −Q∗M 0 (s, a) = R̄(s, a) + γ

X
s0 ∈S

a0 ∈Π(s0 )

∗
which is equivalent to Eqn 16 for QΠ
M (s, a) = −QM 0 (s, a).

This means that policy evaluation for a non-deterministic policy can be achieved by any
method that finds the optimal policy on an MDP.
Definition 5. A non-deterministic policy Π is said to be augmented with state-action pair
(s, a), denoted by Π0 = Π + (s, a), if it satisfies:
(
Π(s0 ),
s0 6= s
Π0 (s0 ) =
(22)
Π(s0 ) ∪ {a}, s0 = s.
If a policy Π can be achieved by a number of augmentations from a policy Π0 , we say
that Π includes Π0 .
Definition 6. A non-deterministic policy Π is said to be non-augmentable according to
a constraint Ψ if and only if Π satisfies Ψ, and for any state-action pair (s, a), Π + (s, a)
does not satisfy Ψ.
In this paper we will be working with constraints that have this particular property:
if a policy Π does not satisfy Ψ, any policy that includes Π does not satisfy Ψ. We will
refer to such constraints as being monotonic. One such constraint is -optimality, which is
discussed in the next section.
4.1 -Optimal Non-Deterministic Policies
Definition 7. A non-deterministic policy Π on an MDP M is said to be -optimal, with
 ∈ [0, 1], if we have1 :
Π
∗
VM
(s) ≥ (1 − )VM
(s), ∀s ∈ S.

(23)

This can be thought of as a constraint Ψ on the space of non-deterministic policies, set
to ensure that the worst-case expected return is within some range of the optimal value.
Theorem 2. The -optimality constraint is monotonic.
∗
1. In some of the MDP literature, -optimality is defined as an additive constraint (QΠ
M ≥ QM − ) (Kearns
& Singh, 2002). The derivations will be analogous in that case. We chose the multiplicative constraint
as it has cleaner derivations.

9

Milani Fard & Pineau

Proof. Suppose Π is not -optimal. Then for any augmentation Π0 = Π + (s, a), we have:

X
0
0
Π0 0 0
QΠ
(s,
a)
=
R̄(s,
a)
+
γ
T
(s,
a,
s
)
min
Q
(s
,
a
)
M
M
s0 ∈S

a0 ∈Π0 (s0 )


X
0
Π0 0 0
T (s, a, s ) min QM (s , a )
≤ R̄(s, a) + γ
s0 ∈S

a0 ∈Π(s0 )

≤ QΠ
M (s, a),
which implies:
0

Π
Π
VM
(s) ≤ VM
(s).

As Π was not -optimal, this means that Π0 will not be -optimal either as the value function
can only decrease with the policy augmentation.
More intuitively, it follows from the fact that adding more options cannot increase the
minimum utility as the former worst case choice is still available after the augmentation.
Definition 8. A conservative -optimal non-deterministic policy Π on an MDP M is a
policy that is non-augmentable according to the following constraint:
X

∗
∗
R̄(s, a) + γ
T (s, a, s0 )(1 − )VM
(s0 ) ≥ (1 − )VM
(s), ∀a ∈ Π(s).
(24)
s0

This constraint indicates that we only add those actions to the policy whose reward plus
(1 − ) of the future optimal return is within the sub-optimal margin. This ensures that the
non-deterministic policy is -optimal by using the inequality:
X

∗
QΠ
T (s, a, s0 )(1 − )VM
(s0 ) ,
(25)
M (s, a) ≥ R̄(s, a) + γ
s0

instead of solving Eqn 16 and using the inequality constraint in Eqn 23. Applying Eqn 24
guarantees that the non-deterministic policy is -optimal while it may still be augmentable
according to Eqn 23, hence the name conservative.
It can also be shown that the conservative policy is unique. This is because if there were
two different conservative policies, then the union of them would be conservative, which
violates the assumption that they are non-augmentable according to Eqn 24.
Definition 9. A non-augmentable -optimal non-deterministic policy Π on an MDP
M is a policy that is non-augmentable according to the constraint in Eqn 23.
This is a non-deterministic policy for which adding more actions violates the nearoptimality constraint of the worst-case performance. In a search for -optimal policies,
a non-augmentable one has a locally maximal size. This means that although the policy
might not be the largest among the -optimal policies, we cannot add any more actions to
it without removing other actions, hence the locally maximal reference.
Any non-augmentable -optimal policy includes the conservative policy. This is because
we can always add the conservative policy to any policy and remain within the  bound.
10

Non-Deterministic Policies in Markovian Decision Processes

However, non-augmentable -optimal policies are not necessarily unique, as they have only
locally maximal size.
In the remainder of this section, we focus on the problem of searching over the space
of non-augmentable -optimal policies, such as to maximize some criteria. Specifically, we
aim to find non-deterministic policies that give the acting agent more options while staying
within an acceptable sub-optimal margin.
We now present an example that clarifies the concepts introduced so far. To simplify the
presentation of the example, we assume deterministic transitions. However, the concepts
apply as well to any probabilistic MDP. Figure 1 shows an example MDP. The labels on
the arcs show action names and the corresponding rewards are shown in the parentheses.
We assume γ ' 1 and  = 0.05. Figure 2 shows the optimal policy of this MDP. The
conservative -optimal non-deterministic policy of this MDP is shown in Figure 3.

Figure 1: Example MDP

Figure 2: Optimal policy

Figure 3: Conservative -optimal policy

Figure 4: Two non-augmentable -optimal policies

Figure 4 includes two possible non-augmentable -optimal policies. Although both policies in Figure 4 are -optimal, the union of these is not -optimal. This is due to the fact
that adding an option to one of the states removes the possibility of adding options to other
11

Milani Fard & Pineau

states, which illustrates why local changes to the policy are not always appropriate when
searching in the space of -optimal policies.
4.2 Optimization Criteria
We formalize the problem of finding an -optimal non-deterministic policy in terms of an
optimization problem. There are several optimization criteria that can be formulated, while
still complying with the -optimality constraint.
• Maximizing the size of the policy: According to this criterion, we seek nonaugmentable -optimal policies that have the biggest overall size (Def 2). This provides
more options to the agent while still keeping the -optimal guarantees. The algorithms
proposed in later sections use this optimization criterion. Notice that the solution to
this optimization problem is non-augmentable according to the -optimal constraint,
because it maximizes the overall size of the policy.
As a variant of this, we can try to maximize the sum of the log of the size of the action
sets:
X
log |Π(s)|.
(26)
s∈S

This enforces a more even distribution of choice on the action set. However, we will be
using the basic case of maximizing the overall size as it will be an easier optimization
problem.
• Maximizing the margin: We can aim to maximize the margin of a non-deterministic
policy Π:
max ΦM (Π),

(27)

Π

where:

ΦM (Π) = min
s∈S

min



Q(s, a) − Q(s, a ) .
0

a∈Π(s),a0 ∈Π(s)
/

(28)

This optimization criterion is useful when one wants to find a clear separation between
the good and bad actions in each state.
• Minimizing the uncertainty: If we learn the models from data we will have some
uncertainty about the optimal action in each state. We can use some variance estimation on the value function (Mannor, Simester, Sun, & Tsitsiklis, 2004) along with
a Z-Test to get some confidence level on our comparisons and find the probability of
having the wrong order when comparing actions according to their values. Let Q be
the value of the true model and Q̂ be our empirical estimate based on some dataset
D. We aim to minimize the uncertainty of a non-deterministic policy Π:
min ΦM (Π),
Π

(29)

where:

ΦM (Π) = max
s∈S

max

a∈Π(s),a0 ∈Π(s)
/

12



	
P r Q(s, a) < Q(s, a0 )|D
.

(30)

Non-Deterministic Policies in Markovian Decision Processes

Notice that the last two criteria can be defined both in the space of all -optimal policies,
or only the non-augmentable ones.
In the following sections we provide algorithms to solve the first optimization problem
mentioned above, which aims to maximize the size of the policy. We focus on this criterion
as it seems most appropriate for medical decision support systems, where it is desirable
for the acceptability of the system to find policies that provide as much choice as possible
for the acting agent. Developing algorithms to address the other two optimization criteria
remains an interesting open problem.
4.3 Maximal -Optimal Policy
The exact computational complexity of finding a maximal -optimal policy is not yet known.
The problem is certainly NP, as one can find the value of the non-deterministic policy in
polynomial time by solving the evaluation MDP with a linear program. We suspect that the
problem is NP-complete, but we have yet to find a reduction from a known NP-complete
problem.
In order to find the largest -optimal policy, we present two algorithms. We first present
a Mixed Integer Program (MIP) formulation of the problem, and then present a search algorithm that uses the monotonic property of the -optimal constraint. While the MIP method
is useful as a general and theoretical formulation of the problem, the search algorithm has
potential for further extensions with heuristics.
4.3.1 Mixed Integer Programming Solution
Recall that we can formulate the problem of finding the optimal deterministic policy of an
MDP as a simple linear program (Bertsekas, 1995):
minV µT V, subject to
P
V (s) ≥ R̄(s, a) + γ s0 T (s, a, s0 )V (s0 ) ∀s, a,

(31)

where µ can be thought of as the initial distribution over states. The solution to the
above problem is the optimal value function (V ∗ ). Similarly, having computed V ∗ using
Eqn 31, the problem of searching for an optimal non-deterministic policy according to the
size criterion can be rewritten as a Mixed Integer Program:2
maxV,Π (µT V + (Vmax − Vmin )eTs Πea ), subject to
V (s) ≥ (1 − )V ∗ (s)
∀s
P
Π(s, a) > 0
∀s
P a
0
0
V (s) ≤ R̄(s, a) + γ s0 T (s, a, s )V (s ) + Vmax (1 − Π(s, a)) ∀s, a.

(32)

Here we are overloading the notation Π to define a binary matrix representing the policy,
where Π(s, a) is 1 if a ∈ Π(s), and 0 otherwise. We define Vmax = Rmax /(1 − γ) and
Vmin = Rmin /(1 − γ). The e’s are column vectors of 1 with the appropriate dimensions.
The first set of constraints makes sure that we stay within  of the optimal return. The
2. Note that in this MIP, unlike the standard LP for MDPs, the choice of µ can affect the solution in cases
where there is a tie in the size of Π.

13

Milani Fard & Pineau

second set of constraints ensures that at least one action is selected per state. The third
set ensures that for those state-action pairs that are chosen in any policy, the Bellman
constraint holds, and otherwise, the constant Vmax makes the constraint trivial. Notice
that the solution to the above problem maximizes |Π| and the result is non-augmentable.
Theorem 3. The solution to the mixed integer program of Eqn 32 is non-augmentable
according to -optimality constraint.
Proof. First, notice that the solution is -optimal, due to the first set of constraints on the
(worst-case) value function. To show that it is non-augmentable, as a counter argument,
suppose that we could add a state-action pair to the solution Π, while still staying in  suboptimal margin. By adding that pair, the objective function is increased by (Vmax − Vmin ),
which is bigger than any possible decrease in the µT V term, and thus the objective is
improved, which conflicts with Π being the solution.
We can use any MIP solver to solve the above problem. Note however that we do not
make use of the monotonic nature of the constraints. A general purpose MIP solver could
end up searching in the space of all the possible non-deterministic policies, which would
require a running time exponential in the number of state-action pairs (O(2|S||A|+δ )).
4.3.2 Heuristic Search
Alternatively, we develop a heuristic search algorithm to find a maximal -optimal policy.
We can make use of the monotonic property of the -optimal policies to narrow down the
search. We start by computing the conservative policy. We then augment it until we arrive
at a non-augmentable policy. We also make use of the fact that if a policy is not -optimal,
neither is any other policy that includes it, and thus we can cut the search tree at this
point.
Table 1: Heuristic search algorithm to find -optimal policies with maximum size
Function getOptimal(Π, startIndex, )
Πo ← Π
for i ← startIndex to |S||A| do
(s, a) ← pi
if a ∈
/ Π(s) & V (Π + (s, a)) ≥ (1 − )V ∗ then
Π0 ← getOptimal (Π + (s, a), i + 1, )
if g(Π0 ) > g(Πo ) then
Πo ← Π0
end
end
end
return Πo
The algorithm presented in Table 1 is a one-sided recursive depth-first-search algorithm
that searches in the space of plausible non-deterministic policies to maximize a function
g(Π). Here we assume that there is an ordering on the set of state-action pairs {pi } =
14

Non-Deterministic Policies in Markovian Decision Processes

{(sj , ak )}. This ordering can be chosen according to some heuristic along with a mechanism
to cut down some parts of the search space. V ∗ is the optimal value function and the function
V returns the value of the non-deterministic policy that can be calculated by solving the
corresponding evaluation MDP.
We should make a call to the above function passing in the conservative policy Πm and
starting from the first state-action pair: getOptimal(Πm , 0, ).
The asymptotic running time of the above algorithm is O((|S||A|)d (tm + tg )), where d is
the maximum size of an -optimal policy minus the size of the conservative policy, tm is the
time to solve the original MDP (polynomial in the relevant parameters), and tg is the time
to calculate the function g. Although the worst-case running time is still exponential in the
number of state-action pairs, the run-time is much less when the search space is sufficiently
small. The |A| term is due to the fact that we check all possible augmentations for each
state. Note that this algorithm searches in the space of all -optimal policies rather than
only the non-augmentable ones. If we set the function g(Π) = |Π|, then the algorithm will
return the biggest non-augmentable -optimal policy.
This search can be further improved by using heuristics to order the state-action pairs
and prune the search. One can also start the search from any other policy rather than the
conservative policy. This can be potentially useful if we have further constraints on the
problem.
4.3.3 Directed Acyclic Transition Graphs
One way to narrow down the search is to only add the action that has the maximum value
for any state s, and ignore the rest of actions if adding the top action will result in values
out of the -optimality bound:
!
Π0 = Π +

s, argmax QΠ (s, a) .
a∈Π(s)
/

The modified algorithm will be as follows:
Table 2: Modified heuristic search algorithm with augmentation rule of Eqn 33.
Function getOptimal(Π, )
Πo ← Π
for s ∈ S where Π(s) 6= A(s) do
a ← argmaxa∈Π(s)
QΠ (s, a)
/
if V (Π + (s, a)) ≥ (1 − )V ∗ then
Π0 ← getOptimal (Π + (s, a), )
if g(Π0 ) > g(Πo ) then
Πo ← Π0
end
end
end
return Πo

15

(33)

Milani Fard & Pineau

The algorithm in Table 2 leads to a running time of O(|S|d (tm + tg )). However this does
not guarantee that we see all non-augmentable policies. This is due to the fact that after
adding an action, the order of values might change. If the transition structure of the MDP
contains no loop with non-zero probability (transition graph is directed acyclic, i.e. DAG),
then this heuristic will produce the optimal result while cutting down the search time.
Theorem 4. For MDPs with DAG transition structure, the algorithm of Table 2 will generate all non-augmentable -optimal policies that would be generated with a full search.
Proof. To prove this, first notice that we can sort the DAG with a topological sort. Therefore, we can arrange the states in levels, having each state only make transitions to states
at a future level. It is easy to see that adding actions to a state for a non-deterministic
policy can only change the worst-case value of past levels. It will not have any effect on the
Q-values at the current level or any future level.
Now given any non-augmentable -optimal policy generated with a full search, there is
a sequence of augmentations that generated that policy. Any permutation of that sequence
would create the same policy and all the intermediate polices are -optimal. Now we rearrange that sequence such that we add actions in the reverse order of the level. By the
point mentioned above, the Q-value of actions at the point where they are being added will
not change until the target policy is realized. Therefore all the actions with Q-values above
the minimum value must be in the policy, or otherwise we can add them, which conflicts
with the target policy being non-augmentable. Since all the actions above a certain Q-value
must be added, we can add them in order. Therefore the target policy can be realized with
the rule of Eqn 33.
When the transition structure is not a DAG, one might do a partial evaluation of the
augmented policy to approximate the value after adding the actions, possibly by doing a few
backups rather than using the original Q-values. This offers the possibility of trading-off
computation time for better solutions.

5. Empirical Results
To evaluate our framework and proposed algorithms, we first test both the MIP and search
formulations on MDPs created randomly, and then test the search algorithm on a real-world
treatment design scenario. Finally, we conduct an experiment on a computer-aided web
navigation task with human subjects to assess the usefulness of non-deterministic policies
in assisting human decision-making.
5.1 Random MDPs
In the first experiment, we aim to study how non-deterministic policies change with the
value of  and how the two algorithms compare in terms of running time. To begin, we
generated random MDPs with 5 states and 4 actions. The transitions are deterministic
(chosen uniformly at random) and the rewards are random values between 0 and 1, except
for one of the states with reward 10 for one of its actions; γ was set to 0.95. The MIP
method was implemented with MATLAB and CPLEX.
16

Non-Deterministic Policies in Markovian Decision Processes

=0

 = 0.01

 = 0.02

 = 0.03

Figure 5: MIP solution for different values of  ∈ {0, 0.01, 0.02, 0.03}. The labels on the
edges are action indices, followed by the corresponding immediate rewards.
Figure 5 shows the solution to the MIP defined in Eqn 32 for a particular randomly
generated MDP. We see that the size of the non-deterministic policy increases as the performance threshold is relaxed. We can see that even with small values for  there are several
actions included in the policy for each state. This is of course a result of the Q-values being
close to each other. Such property is typical in many medical scenarios where different
treatments provide only slightly different results.
To compare the running time of the MIP solver and the search algorithm, we constructed
random MDPs as described above with more state-action pairs. Figure 6 shows the running
time averaged over 20 different random MDPs with 5 states, assuming  = 0.01 (which
allows several solutions). As expected, both algorithms have a running time exponential
in the number of state-action pairs (note the exponential scale on the time axis). The
running time of the search algorithm has a bigger constant factor (possibly due to our naive
implementation), but has a smaller exponent base, which results in a faster asymptotic
running time. Even with the exponential running time, one can still use the search algorithm
to solve problems with a few hundred state-action pairs. This is more than sufficient for
many practical domains, including real-world medical decision scenarios as shown in the
next section.
To observe the effect of the choice of  on the running time our algorithms, we fix the
size of random MDPs to have 7 states and 5 actions at each state, and then change the
17

Milani Fard & Pineau

Figure 6: Running time of MIP and the search algorithm as a function of the number of
state-action pairs with  = 0.01.
value of  and measure the running time of the algorithms over 100 trials. Figure 7 shows
the average running time of both algorithms with different values for . As expected, the
search algorithm will go deeper in the search tree as the optimality threshold is relaxed and
its running time will thus increase. The running time of the MIP method, on the other
hand, remains relativity constant as it exhaustively searches in the space of all possible
non-deterministic policies. These results are representative of the relative behaviour of the
two approaches over a range of problems.

Figure 7: Running time of MIP and the search algorithm as a function of , with 7 states
and 5 actions. Many of the actions are included in the policy with  = 0.02.

5.2 Medical Decision-making
To demonstrate how non-deterministic policies can be used and presented in a medical
domain, we tested the full search algorithm on an MDP constructed for a medical decisionmaking task involving real patient data. The data was collected as part of a large (4000+
patients) multi-step randomized clinical trial, designed to investigate the comparative effectiveness of different treatments provided sequentially for patients suffering from depression
(Fava et al., 2003). The goal is to find a treatment plan that maximizes the chance of
18

Non-Deterministic Policies in Markovian Decision Processes

remission. The dataset includes a large number of measured outcomes. For the current
experiment, we focus on a numerical score called the Quick Inventory of Depressive Symptomatology (QIDS), which was used in the study to assess levels of depression (including
when patients achieved remission). For the purposes of our experiment, we discretize the
QIDS scores (which range from 5 to 27) uniformly into quartiles, and assume that this,
along with the treatment step (up to 4 steps were allowed), completely describe the patient’s state. Note that the underlying transition graph can be treated as a DAG, as the
study is limited to four steps of treatment and action choices change between these steps.
There are 19 actions (treatments) in total. A reward of 1 is given if the patient achieves
remission (at any step) and a reward of 0 is given otherwise. The transition and reward
models were estimated empirically from the medical database using a frequentist approach.
Table 3: Policy and running time of the full search algorithm on the medical problem.
 = 0.02

 = 0.015

 = 0.01

=0

118.7

12.3

3.5

1.4

CT
SER
BUP
CIT+BUS

CT
SER

CT

CT

9 ≤ QIDS < 12

CIT+BUP
CIT+CT

CIT+BUP
CIT+CT

CIT+BUP

CIT+BUP

VEN
CIT+BUS
CT

VEN
CIT+BUS

VEN

VEN

12 ≤ QIDS < 16

16 ≤ QIDS ≤ 27

CT
CIT+CT

CT
CIT+CT

CT
CIT+CT

CT

Time (seconds)
5 < QIDS < 9

Table 3 shows the non-deterministic policy obtained for each state during the second
step of the trial (each acronym refers to a specific treatment). This is computed using the
search algorithm, assuming different values of . Although this problem is not tractable with
the MIP formulation (304 state-action pairs), a full search in the space of -optimal policies
is still possible. Table 3 also shows the running time of the algorithm, which as expected,
increases as we relax the threshold . Here, we did not use any heuristics. However, as the
underlying transition graph is a DAG, we could use the heuristic discussed in the previous
section (Eqn 33) to get the same policies even faster.
An interesting question is how to set  a priori. In practice, a doctor may use the
full table as a guideline, using smaller values of  when he/she wants to rely more on the
decision support system, and larger values when relying more on his/her own assessments.
We believe this particular presentation of non-deterministic policies could be used and
accepted by clinicians, as it is not excessively prescriptive and keeps the physician and the
patient in the decision cycle. This is in contrast with the traditional notion of policies in
reinforcement learning, which often leaves no place for the physician’s intervention.
19

Milani Fard & Pineau

5.3 Human Subject Interaction
Finally, we conduct an experiment to assess the usefulness of non-deterministic policies with
human subjects. Ideally, we would like to conduct such experiments in medical settings and
with physicians, but such studies are costly and difficult to conduct given that they require
participation of many medical professionals. We therefore study non-deterministic policies
in an easier domain by constructing a web-based game that can be played by any computer
and human (either jointly or separately).
The game is defined as follows. A user is given a target word and is asked to navigate
around the pages of Wikipedia and visit pages that contain that target word. The user can
click on any word in a page. The system then uses a Google search on the Wiki website
with the clicked word and a keyword in the current page (the choice of this keyword is
discussed later). It then randomly chooses one of the top eight search results and moves
to that page. This process mimics the hyperlink structure of the web (extending over
the hyperlink structure of the Wiki to make target words more easily reachable). The
user is given ten attempts and is asked to reach as many pages with the target word as
possible. A similar game was used in another work to infer the semantic distances between
concepts (West, Pineau, & Precup, 2009). Our game, however, is designed in such a way
that a computer model can provide results similar to a human player and thus enable us to
assess the effectiveness of computer-aided decisions with non-deterministic policies.
We construct this task on the CD version of Wikipedia (Schools-Wikipedia, 2009), which
is a structured and manageable version of Wikipedia intended for use in schools. To test our
approach we also need to build an MDP model of the task. This is done using empirical data
as follows. First, we use Latent Dirichlet Allocation (LDA) using Gibbs sampling (Griffiths
& Steyvers, 2004) to divide the pages of Wikipedia into 20 topics. Each topic corresponds
to a state in the MDP. The LDA algorithm identifies each topic by a set of keywords that
occur more often in pages with that topic. We define each of these sets of keywords to be an
action (20 actions totals, each corresponding to 20 keywords). We then randomly navigate
around the Wiki using the protocol described above (with a computer player that only
clicks LDA keywords) and collect 200,000 transitions. We use the observed data to build
the transition and reward model of our MDP (the reward is 1 for each hit and 0 otherwise).
The specific choices for the LDA parameter and the number of states and actions in the
MDP are made in such a way that the best policy provided by the model has comparable
performance with that of the human player.
Using Amazon Mechanical Turk (MTurk, 2010), we consider three experimental conditions with this task. In one experiment, given a target, the computer chooses a word
(uniformly at random) from the set of keywords (the action) that comes from the optimal
policy on the MDP model. In another experiment, human subjects choose and click the
word themselves without any help. Finally, we test the domain with human users while the
computer highlights, as hints, the words that come from the non-deterministic policy with
 = 0.1. We record the time taken during the process and number of times the target word
is observed (number of hits). Table 4 summarizes the average outcomes of this experiment
for four of the target words (we used seven target words, but could not collect enough data
for all of them). We also include the p-value for the t-test comparing the results for human
agents with and without the hints. The computer score is averaged over 1000 runs.
20

Non-Deterministic Policies in Markovian Decision Processes

Table 4: Comparison of different agents in the web navigation task. The t-test is between
the number of hits for a human player that uses hints and one that does not.
Target

Computer

Human

Human with hint

t-Test

Marriage

1.88 hits

1.94 hits
103 seconds
(86 subjects)

2.63 hits
93 seconds
(86 subjects)

0.012

4.86 hits
91 seconds
(67 subjects)

5.61 hits
84 seconds
(97 subjects)

0.049

3.67 hits
85 seconds
(98 subjects)

4.39 hits
89 seconds
(83 subjects)

0.014

3.18 hits
96 seconds
(92 subjects)

3.42 hits
85 seconds
(123 subjects)

0.46

(1000 runs)
Military

4.72 hits
(1000 runs)

Book

3.77 hits
(1000 runs)

Animal

2.50 hits
(1000 runs)

For the first three target words, where the performance of the computer agent is close to
a human user, we observe that providing hints to the user results in a statistically significant
increase in the number of hits. In fact we see that the computer-aided human outperforms
both the computer and human agents. This shows that non-deterministic policies can
provide the means to inject human domain knowledge to computer models in such a way
that the final outcome is superior to the decision-making solely performed by one party.
For the last word, the computer model is working poorly, judging by its low hit rate. Thus,
it is not surprising to see that the hints do not provide much help to the human agent
in this case (as seen by the non-significant p-value). We also observe a general speedup
(for three of the targets) in the time taken by the agent to choose and click the words,
which further shows the usefulness of non-deterministic policies in accelerating the human
subjects’ decision-making process.

6. Discussion
This paper introduces the new concept of non-deterministic policies and their potential use
in decision support systems based on Markovian processes. In this context, we investigate
how the assumption that a decision-making system should return a single optimal action
can be relaxed, to instead return a set of near-optimal actions.
Non-deterministic policies are inherently different from stochastic policies. Stochastic
policies assume a randomized action selection strategy with some specific probabilities,
whereas non-deterministic policies do not impose such constraint. We can thus use bestcase and worst-case analysis with non-deterministic policies to highlight different scenarios
with the human user.
21

Milani Fard & Pineau

The benefits of non-deterministic policies for sequential decision-making are two-fold.
First, when we have several actions for which the difference in performance are negligible,
we can report all those actions as near-optimal options. For instance, in a medical setting,
the difference between the outcome of two treatment options might not be “medically
significant”. In that case, it may be beneficial to provide all the near-optimal options.
This makes the system more robust and user-friendly. In the medical decision-making
process, for instance, the physician can make the final decision among the near-optimal
options based on side effects burden, patient’s preferences, expense, or any other criteria
that is not captured by the model used in the decision support system. The key constraint,
however, is to make sure that regardless of the final choice of actions, the performance of
the executed policy is always bounded near the optimal. In our framework, this property
is maintained by an -optimality guarantee on the worst-case scenario.
Another potential use of the non-deterministic action sets in Markovian decision processes is to capture uncertainties in the optimality of actions. Often times, the amount of
data from which models are constructed is not sufficient to clearly identify a single optimal
action. If we are forced to chose only one action as the optimal one, we might have a high
chance of making the wrong decision. However, if we are given the chance to provide a set
of possibly-optimal actions, then we can ensure we include all the promising options while
cutting off the obviously bad ones. In this setting, the task is to trim the action set as much
as possible while providing the guarantee that the optimal action is still among the top few
possible options.
To solve the first problem, this paper introduces two algorithms to find flexible nearoptimal policies. First we derive an exact solution with a MIP formulation to find a maximal
-optimal policy. The MIP solution is, however, computationally expensive and does not
scale to large domains. We then describe a search algorithm to solve the same problem with
less computational cost. This algorithm is fast enough to be applied to real world medical
domains. We also show how to use heuristics in the search algorithm to find the solution for
DAG structures even faster. The heuristic search can also provide approximate solutions in
the general case.
Another way to scale the problem to larger domains is to approximate the solution to
the MIP program by relaxing some of the constraints. One can relax the constraints to
allow non-integral solutions and penalize the objective for values away from 0 and 1. The
study of such approximation methods remains an interesting direction of future work.
The idea of non-deterministic policies introduces a wide range of new problems and
research topics. In Section 4, we discuss the idea of near optimal non-deterministic policies
and address the problem of finding the one with the largest action set. As mentioned, there
are other optimization criteria that might be useful with decision support systems. These
include maximizing the decision margin (the margin between the worst selected action and
the best one not selected), or alternatively minimizing the uncertainty of a wrong selection.
Formalizing these problems into a MIP formulation, or incorporating them into a heuristic
search, might prove to be useful.
As evidenced by our human interaction experiments, non-deterministic policies can substantially improve the outcome of planning and decision-making tasks in which a human
user is assisted by a robust computer-generated plan. Allowing several suggestions at each
step provides an effective way of incorporating domain knowledge from the human side of
22

Non-Deterministic Policies in Markovian Decision Processes

the decision-making process. In medical domains where the physician’s domain knowledge
is often hard to capture in a computer model, a collaborative model of decision-making such
as non-deterministic policies could offer a powerful framework for selecting effective, and
clinically acceptable, treatment strategies.

Acknowledgments
The authors wish to thank A. John Rush (Duke-NUS Graduate Medical School), Susan
A. Murphy (University of Michigan), and Doina Precup (McGill University) for helpful
discussions regarding this work. Funding was provided by the National Institutes of Health
(grant R21 DA019800) and the NSERC Discovery Grant program.

References
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. (1995). Dynamic Programming and Optimal Control, Vol 2. Athena Scientific.
Ernst, D., Stan, G. B., Concalves, J., & Wehenkel, L. (2006). Clinical data based optimal
STI strategies for HIV: a reinforcement learning approach. In Proceedings of the
Fifteenth Machine Learning conference of Belgium and The Netherlands (Benelearn),
pp. 65–72.
Fava, M., Rush, A., Trivedi, M., Nierenberg, A., Thase, M., Sackeim, H., Quitkin, F., Wisniewski, S., Lavori, P., Rosenbaum, J., & Kupfer, D. (2003). Background and rationale
for the sequenced treatment alternatives to relieve depression (STAR* D) study. Psychiatric Clinics of North America, 26 (2), 457–494.
Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings of the National
Academy of Sciences, 101 (Suppl. 1), 5228–5235.
Hauskrecht, M., & Fraser, H. (2000). Planning treatment of ischemic heart disease with
partially observable Markov decision processes. Artificial Intelligence in Medicine,
18 (3), 221–244.
Heger, M. (1994). Consideration of risk in reinforcement learning. In Proceedings of the
Eleventh International Conference on Machine Learning (ICML), pp. 105–111.
Karmarkar, N. (1984). A new polynomial-time algorithm for linear programming. Combinatorica, 4 (4), 373–395.
Kearns, M., & Singh, S. (2002). Near-optimal reinforcement learning in polynomial time.
Machine Learning, 49.
Magni, P., Quaglini, S., Marchetti, M., & Barosi, G. (2000). Deciding when to intervene:
a Markov decision process approach. International Journal of Medical Informatics,
60 (3), 237–253.
Mannor, S., Simester, D., Sun, P., & Tsitsiklis, J. N. (2004). Bias and variance in value
function estimation. In Proceedings of the Twenty-First International Conference on
Machine Learning (ICML), pp. 308–322.
23

Milani Fard & Pineau

Mannor, S., Simester, D., Sun, P., & Tsitsiklis, J. N. (2007). Bias and variance approximation in value function estimates. Management Science, 53 (2), 308–322.
MTurk (2010). Amazon mechanical turk. In http://www.mturk.com/.
Murphy, S. A. (2005). An experimental design for the development of adaptive treatment
strategies. Statistics in Medicine, 24 (10), 1455–1481.
Pineau, J., Bellemare, M. G., Rush, A. J., Ghizaru, A., & Murphy, S. A. (2007). Constructing evidence-based treatment strategies using methods from computer science. Drug
and Alcohol Dependence, 88 (Supplement 2), S52 – S60.
Russell, S. J., & Norvig, P. (2003). Artificial Intelligence: A Modern Approach (Second
Edition). Prentice Hall.
Sato, M., & Kobayashi, S. (2000). Variance-penalized reinforcement learning for risk-averse
asset allocation. In Proceedings of the Second International Conference on Intelligent
Data Engineering and Automated Learning, Data Mining, Financial Engineering, and
Intelligent Agents, pp. 244–249. Springer-Verlag.
Schaefer, A., Bailey, M., Shechter, S., & Roberts, M. (2004). Handbook of Operations
Research / Management Science Applications in Health Care, chap. Medical decisions
using Markov decision processes. Kluwer Academic Publishers.
Schools-Wikipedia (2009).
wikipedia.org/.

2008/9 wikipedia selection for schools.

In http://schools-

Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction (Adaptive
Computation and Machine Learning). The MIT Press.
Thapa, D., Jung, I., & Wang, G. (2005). Agent based decision support system using reinforcement learning under emergency circumstances. Lecture Notes in Computer
Science, 3610, 888.
West, R., Pineau, J., & Precup, D. (2009). Wikispeedia: an online game for inferring
semantic distances between concepts. In Proceedings of the Twenty-First International
Jont Conference on Artifical Intelligence (IJCAI), pp. 1598–1603, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.

24

Journal of Artificial Intelligence Research 40 (2011) 701-728

Submitted 10/10; published 4/11

Computing Small Unsatisfiable Cores
in Satisfiability Modulo Theories
Alessandro Cimatti

cimatti@fbk.eu

FBK-IRST,
Via Sommarive 18, 38123 Povo, Trento, Italy

Alberto Griggio

griggio@fbk.eu

FBK-IRST,
Via Sommarive 18, 38123 Povo, Trento, Italy

Roberto Sebastiani

rseba@disi.unitn.it

DISI, Università di Trento,
Via Sommarive 14, 38123 Povo, Trento, Italy

Abstract
The problem of finding small unsatisfiable cores for SAT formulas has recently received
a lot of interest, mostly for its applications in formal verification. However, propositional
logic is often not expressive enough for representing many interesting verification problems,
which can be more naturally addressed in the framework of Satisfiability Modulo Theories,
SMT. Surprisingly, the problem of finding unsatisfiable cores in SMT has received very
little attention in the literature.
In this paper we present a novel approach to this problem, called the Lemma-Lifting
approach. The main idea is to combine an SMT solver with an external propositional
core extractor. The SMT solver produces the theory lemmas found during the search,
dynamically lifting the suitable amount of theory information to the Boolean level. The
core extractor is then called on the Boolean abstraction of the original SMT problem and
of the theory lemmas. This results in an unsatisfiable core for the original SMT problem,
once the remaining theory lemmas are removed.
The approach is conceptually interesting, and has several advantages in practice. In
fact, it is extremely simple to implement and to update, and it can be interfaced with
every propositional core extractor in a plug-and-play manner, so as to benefit for free of
all unsat-core reduction techniques which have been or will be made available.
We have evaluated our algorithm with a very extensive empirical test on SMT-LIB
benchmarks, which confirms the validity and potential of this approach.

1. Motivations and Goals
In the last decade we have witnessed an impressive advance in the efficiency of SAT techniques, which has brought large and previously-intractable problems at the reach of stateof-the-art SAT solvers. As a consequence, SAT solvers are now a fundamental tool in many
industrial-strength applications, including most formal verification design flows for hardware
systems, for equivalence, property checking, and ATPG. In particular, one of the most relevant problems in this context, thanks to its many important applications, is that of finding
small unsatisfiable cores, that is, small unsatisfiable subsets of unsatisfiable sets of clauses.
c
2011
AI Access Foundation. All rights reserved.

Cimatti, Griggio, & Sebastiani

Examples of such applications include use of SAT instead of BDDs for unbounded symbolic
model checking (McMillan, 2002), automatic predicate discovery in abstraction refinement
frameworks (McMillan & Amla, 2003; Wang, Kim, & Gupta, 2007), decision procedures
(Bryant, Kroening, Ouaknine, Seshia, Strichman, & Brady, 2009), under-approximation and
refinement in the context of bounded model checking of multi-threaded systems (Grumberg,
Lerda, Strichman, & Theobald, 2005), debugging of design errors in circuits (Suelflow, Fey,
Bloem, & Drechsler, 2008). For this reason, the problem of finding small unsat cores in
SAT has been addressed by many authors in the recent years (Zhang & Malik, 2003; Goldberg & Novikov, 2003; Lynce & Marques-Silva, 2004; Oh, Mneimneh, Andraus, Sakallah,
& Markov, 2004; Mneimneh, Lynce, Andraus, Marques-Silva, & Sakallah, 2005; Huang,
2005; Dershowitz, Hanna, & Nadel, 2006; Zhang, Li, & Shen, 2006; Biere, 2008; Gershman,
Koifman, & Strichman, 2008; van Maaren & Wieringa, 2008; Ası́n, Nieuwenhuis, Oliveras,
& Rodrı́guez Carbonell, 2008; Nadel, 2010).
The formalism of plain propositional logic, however, is often not suitable or expressive
enough for representing many other real-world problems, including the verification of RTL
designs, of real-time and hybrid control systems, and the analysis of proof obligations in
software verification. Such problems are more naturally expressible as satisfiability problems in decidable first-order theories —Satisfiability Modulo Theories, SMT. Efficient SMT
solvers have been developed in the last five years, called lazy SMT solvers, which combine a
Conflict-Driven Clause Learning (CDCL) SAT solver based on the DPLL algorithm (Davis
& Putnam, 1960; Davis, Logemann, & Loveland, 1962; Marques-Silva & Sakallah, 1996;
Zhang & Malik, 2002) — hereafter simply “DPLL” — with ad-hoc decision procedures
for many theories of interest (see, e.g., Nieuwenhuis, Oliveras, & Tinelli, 2006; Barrett
& Tinelli, 2007; Bruttomesso, Cimatti, Franzén, Griggio, & Sebastiani, 2008; Dutertre &
de Moura, 2006; de Moura & Bjørner, 2008).
Surprisingly, the problem of finding unsatisfiable cores in SMT has received virtually no
attention in the literature. Although some SMT tools do compute unsat cores, this is done
either as a byproduct of the more general task of producing proofs, or by modifying the
embedded DPLL solver so that to apply basic propositional techniques to produce an unsat
core. In particular, we are not aware of any work aiming at producing small unsatisfiable
cores in SMT.
In this paper we present a novel approach addressing this problem, which we call the
Lemma-Lifting approach. The main idea is to combine an SMT solver with an external
propositional core extractor. The SMT solver stores and returns the theory lemmas it had
to prove in order to refute the input formula; the external core extractor is then called
on the Boolean abstraction of the original SMT problem and of the theory lemmas. Our
algorithm is based on the following two key observations: i) the theory lemmas discovered
by the SMT solver during search are valid clauses in the theory T under consideration,
and therefore they do not affect the satisfiability of a formula in T ; and ii) the conjunction
of the original SMT formula with all the theory lemmas is propositionally unsatisfiable.
Therefore, the external (Boolean) core extractor finds an unsatisfiable core for (the Boolean
abstraction of) the conjunction of the original formula and the theory lemmas, which can
then be refined back into a subset of the original clauses by simply removing from it (the
Boolean abstractions of) all theory lemmas. The result is an unsatisfiable core of the original
SMT problem.
702

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

Although simple in principle, the approach is conceptually interesting: basically, the
SMT solver is used to dynamically lift the suitable amount of theory information to the
Boolean level. Furthermore, the approach has several advantages in practice: first, it is
extremely simple to implement and to update; second, it is effective in finding small cores;
third, the core extraction is not prone to complex SMT reasoning; finally, it can be interfaced
with every propositional core extractor in a plug-and-play manner, so as to benefit for free
of all unsat-core reduction techniques which have been or will be made available.
We have evaluated our approach by a very extensive empirical test on SMT-LIB benchmarks, in terms of both effectiveness (reduction in size of the cores) and efficiency (execution
time). The results confirm the validity and versatility of this approach.
As a byproduct, we have also produced an extensive and insightful evaluation of the
main Boolean unsat-core-generation tools currently available.
Content. The paper is organized as follows. In §2 and §3 we provide some background
knowledge on techniques for SAT and SMT (§2), and for the extraction of unsatisfiable cores
in SAT and in SMT (§3). In §4 we present and discuss our new approach and algorithm.
In §5 we present and comment on the empirical tests. In §6 we conclude, suggesting some
future developments.

2. SAT and SMT
Our setting is standard first order logic. A 0-ary function symbol is called a constant. A
term is a first-order term built out of function symbols and variables. If t1 , . . . , tn are terms
and p is a predicate symbol, then p(t1 , . . . , tn ) is an atom. A formula φ is built in the
usual way out of the universal and existential quantifiers, Boolean connectives, and atoms.
A literal is either an atom or its negation. We call a formula quantifier-free if it does not
contain quantifiers, and ground if it does not contain free variables. A clause is a disjunction
of literals. A formula is said to be in conjunctive normal form (CNF) if it is a conjunction of
clauses. For every non-CNF formula ϕ, an equisatisfiable CNF formula ψ can be generated
in polynomial time (Tseitin, 1983).
We also assume the usual first-order notions of interpretation, satisfiability, validity,
logical consequence, and theory, as given, e.g., by Enderton (1972). We write Γ |= φ to
denote that the formula φ is a logical consequence of the (possibly infinite) set Γ of formulas.
A first-order theory, T , is a set of first-order sentences. A structure A is a model of a theory
T if A satisfies every sentence in T . A formula is satisfiable in T (or T -satisfiable) if it is
satisfiable in a model of T . (We sometimes use the word “T -formula” for a ground formula
when we are interested in determining its T -satisfiability.)
In what follows, with a little abuse of notation, we might sometimes denote conjunctions
of literals l1 ∧ . . . ∧ ln as sets {l1 , . . . , ln } and vice versa. If η ≡ {l1 , . . . , ln }, we might write
¬η to mean ¬l1 ∨ . . . ∨ ¬ln . Moreover, following the terminology of the SAT and SMT
communities, we shall refer to predicates of arity zero as propositional variables, and to
uninterpreted constants as theory variables.
Given a first-order theory T for which the (ground) satisfiability problem is decidable,
we call a theory solver for T , T -solver, any tool able to decide the satisfiability in T of
sets/conjunctions of ground atomic formulas and their negations — theory literals or T literals — in the language of T . If the input set of T -literals µ is T -unsatisfiable, then a
703

Cimatti, Griggio, & Sebastiani

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.

SatValue DPLL (formula ϕ, assignment µ) {
while (1) {
decide next branch(ϕ, µ);
while (1) {
status = deduce(ϕ, µ);
if (status == sat)
return sat;
else if (status == conflict) {
hblevel, ηi = analyze conflict(ϕ, µ);
if (blevel < 0) return unsat;
else backtrack(blevel, ϕ, µ, η);
}
else break;
}}}
Figure 1: Schema of a modern DPLL engine.

typical T -solver not only returns unsat, but it also returns the subset η of T -literals in µ
which was found T -unsatisfiable. (η is hereafter called a theory conflict set, and ¬η a theory
conflict clause.) If µ is T -satisfiable, then T -solver not only returns sat, but it may also be
able to discover one (or more) deductions in the form
Wn{l1 , . . . , ln } |=T l, s.t. {l1 , . . . , ln } ⊆ µ
and l is an unassigned T -literal. If so, we call ( i=1 ¬li ∨ l) a theory-deduction clause.
Importantly, notice that both theory-conflict clauses and theory-deduction clauses are valid
in T . We call them theory lemmas or T -lemmas.
Satisfiability Modulo (the) Theory T — SMT (T ) — is the problem of deciding the
satisfiability of Boolean combinations of propositional atoms and theory atoms. Examples
of useful theories are equality and uninterpreted functions (EU F), difference logic (DL) and
linear arithmetic (LA), either over the reals (LA(Q)) or the integers (LA(Z)), the theory
of arrays (AR), that of bit vectors (BV), and their combinations. We call an SMT (T ) tool
any tool able to decide SMT (T ). Notice that, unlike a T -solver, an SMT (T ) tool must
handle also Boolean connectives.
Hereafter we adopt the following terminology and notation. The symbols ϕ, ψ denote
T -formulas, and µ, η denote sets of T -literals; ϕp , ψ p denote propositional formulas, µp ,
η p denote sets of propositional literals, which can be interpreted as truth assignments to
variables.
2.1 Propositional Satisfiability with the DPLL Algorithm
Most state-of-the-art SAT procedures are evolutions of the Davis-Putnam-Longeman-Loveland
(DPLL) procedure (Davis & Putnam, 1960; Davis et al., 1962). A high-level schema of
a modern DPLL engine, adapted from the description given by Zhang and Malik (2002),
704

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

1.
2.
3.
4.
5.
6.
7.
8.
9.

SatValue Lazy SMT Solver (T -formula ϕ) {
ϕp = T 2P(ϕ);
while (DPLL(ϕp , µp ) == sat) {
hρ, ηi = T -solver(P2T (µp ))
if (ρ == sat) then return sat;
ϕp = ϕp ∧ T 2P(¬η);
};
return unsat;
};

Figure 2: A simplified schema for lazy SMT (T ) procedures.
is reported in Figure 1.1 The Boolean formula ϕ is in CNF; the assignment µ is initially
empty, and it is updated in a stack-based manner.
In the main loop, decide next branch(ϕ, µ) chooses an unassigned literal l from ϕ
according to some heuristic criterion, and adds it to µ. (This operation is called decision, l
is called decision literal end the number of decision literals in µ after this operation is called
the decision level of l.) In the inner loop, deduce(ϕ, µ) iteratively deduces literals l deriving
from the current assignment and updates µ accordingly; this step is repeated until either
µ satisfies ϕ, or µ falsifies ϕ, or no more literals can be deduced, returning sat, conflict and
unknown respectively. (The iterative application of Boolean deduction steps in deduce is
also called Boolean Constraint Propagation, BCP.) In the first case, DPLL returns sat. In
the second case, analyze conflict(ϕ, µ) detects the subset η of µ which caused the conflict
(conflict set) and the decision level blevel to backtrack. If blevel < 0, then a conflict exists
even without branching, and DPLL returns unsat. Otherwise, backtrack(blevel, ϕ, µ)
adds the clause ¬η to ϕ (learning) and backtracks up to blevel (backjumping), updating µ
accordingly. (E.g., with the popular 1st-UIP schema, it backtracks to the smallest blevel
where all but one literal in η are assigned, and hence it deduces the negation of the remaining
literal applying BCP on the learned clause ¬η; see Zhang, Madigan, Moskewicz, & Malik,
2001.) In the third case, DPLL exits the inner loop, looking for the next decision.
For a much deeper description of modern DPLL-based SAT solvers, we refer the reader,
e.g., to the work of Zhang and Malik (2002).
2.2 Lazy Techniques for SMT
The idea underlying every lazy SMT (T ) procedure is that (a complete set of) the truth
assignments for the propositional abstraction of ϕ are enumerated and checked for satisfiability in T ; the procedure either returns sat if one T -satisfiable truth assignment is found,
or returns unsat otherwise.
We introduce the following notation. T 2P is a bijective function (“theory to propositional”), called Boolean (or propositional) abstraction, which maps propositional variables
into themselves, ground T -atoms into fresh propositional variables, and is homomorphic
1. We remark that many of the details provided here are not critical for understanding the rest of the paper,
but are mentioned only for the sake of completeness.

705

Cimatti, Griggio, & Sebastiani

w.r.t. Boolean operators and set inclusion. The function P2T (“propositional to theory”), called refinement, is the inverse of T 2P. (E.g., T 2P({((x − y ≤ 3) ∨ A3 ), (A2 →
(x = z))}) = {(B1 ∨ A3 ), (A2 → B2 )}, B1 and B2 being fresh propositional variables, and
P2T ({A1 , ¬A2 , ¬B1 , B2 }) = {A1 , ¬A2 , ¬(x − y ≤ 3), (x = z)}.) In what follows, we shall
use the “p ” superscript for denoting the Boolean abstraction of a formula/truth assignment
(e.g., ϕp denotes T 2P(ϕ), µ denotes P2T (µp )). Given a T -formula ϕ, we say that ϕ is
propositionally unsatisfiable when T 2P(ϕ) |= ⊥. .
Figure 2 presents a simplified schema of a lazy SMT (T ) procedure, called the off-line
schema. The propositional abstraction ϕp of the input formula ϕ is given as input to
a SAT solver based on the DPLL algorithm (Davis et al., 1962; Zhang & Malik, 2002),
which either decides that ϕp is unsatisfiable, and hence ϕ is T -unsatisfiable, or returns
a satisfying assignment µp ; in the latter case, P2T (µp ) is given as input to T -solver. If
P2T (µp ) is found T -consistent, then ϕ is T -consistent. If not, T -solver returns the conflict
set η which caused the T -inconsistency of P2T (µp ); the abstraction of the T -lemma ¬η,
T 2P(¬η), is then added as a clause to ϕp . Then the DPLL solver is restarted from scratch
on the resulting formula.
Practical implementations follow a more elaborated schema, called the on-line schema
(see Barrett, Dill, & Stump, 2002; Audemard, Bertoli, Cimatti, Kornilowicz, & Sebastiani,
2002; Flanagan, Joshi, Ou, & Saxe, 2003). As before, ϕp is given as input to a modified
version of DPLL, and when a satisfying assignment µp is found, the refinement µ of µp is
fed to the T -solver; if µ is found T -consistent, then ϕ is T -consistent; otherwise, T -solver
returns the conflict set η which caused the T -inconsistency of P2T (µp ). Then the clause
¬η p is added in conjunction to ϕp , either temporarily or permanently (T -learning), and,
rather than starting DPLL from scratch, the algorithm backtracks up to the highest point
in the search where one of the literals in ¬η p is unassigned (T -backjumping), and therefore
its value is (propositionally) implied by the others in ¬η p .
An important variant of this schema (Nieuwenhuis et al., 2006) is that of building a
“mixed Boolean+theory conflict clause”, starting from ¬η p and applying the backwardtraversal of the implication graph built by DPLL (Zhang et al., 2001), until one of the
standard conditions (e.g., 1st UIP – Zhang et al., 2001) is achieved.
Other important optimizations are early pruning and theory propagation: the T -solver
is invoked also on (the refinement of) an intermediate assignment µ: if it is found T unsatisfiable, then the procedure can backtrack, since no extension of µ can be T -satisfiable;
if not, and if the T -solver performs a deduction {l1 , . . . , ln } |=T l s.t. {l1 , . . . , lnW
} ⊆ µ, then
T 2P(l) can be unit-propagated, and the Boolean abstraction of the T -lemma ( ni=1 ¬li ∨ l)
can be learned.
The on-line lazy SMT (T ) schema is a coarse description of the procedures underlying all
the state-of-the-art lazy SMT (T ) tools like, e.g., BarceLogic, CVC3, MathSAT, Yices,
Z3. The interested reader is pointed to, e.g., the work of Nieuwenhuis et al. (2006), Barrett
and Tinelli (2007), Bruttomesso et al. (2008), Dutertre and de Moura (2006), and de Moura
and Bjørner (2008), for details and further references, or to the work of Sebastiani (2007)
and Barrett, Sebastiani, Seshia, and Tinelli (2009) for a survey.
706

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

3. Extracting Unsatisfiable Cores
Without loss of generality, in the following we consider only formulas in CNF. Given an
unsatisfiable CNF formula ϕ, we say that an unsatisfiable CNF formula ψ is an unsatisfiable
core of ϕ iff ϕ = ψ ∧ ψ 0 for some (possibly empty) CNF formula ψ 0 . Intuitively, ψ is a subset
of the clauses in ϕ causing the unsatisfiability of ϕ. An unsatisfiable core ψ is minimal iff
the formula obtained by removing any of the clauses of ψ is satisfiable. A minimum unsat
core is a minimal unsat core with the smallest possible cardinality.
3.1 Techniques for Unsatisfiable-Core Extraction in SAT
In the last few years, several algorithms for computing small, minimal or minimum unsatisfiable cores of propositional formulas have been proposed. In the approach of Zhang and
Malik (2003) and Goldberg and Novikov (2003), they are computed as a byproduct of a
DPLL-based proof-generation procedure. The computed unsat core is simply the collection
of all the original clauses that the DPLL solver used to derive the empty clause by resolution. The returned core is not minimal in general, but it can be reduced by iterating
the algorithm until a fixpoint, using as input of each iteration the core computed at the
previous one. The algorithm of Gershman et al. (2008), instead, manipulates the resolution
proof so as to shrink the size of the core, using also a fixpoint iteration as Zhang and Malik
(2003) to further enhance the quality of the results. Oh et al. (2004) present an algorithm to
compute minimal unsat cores. The technique is based on modifications of a standard DPLL
engine, and works by adding some extra variables (selectors) to the original clauses, and
then performing a branch-and-bound algorithm on the modified formula. The procedure
presented by Huang (2005) extracts minimal cores using BDD manipulation techniques,
removing one clause at a time until the remaining core is minimal. The construction of a
minimal core by Dershowitz et al. (2006) also uses resolution proofs, and it works by iteratively removing from the proof one input clause at a time, until it is no longer possible to
prove inconsistency. When a clause is removed, the resolution proof is modified to prevent
future use of that clause.
As far as the the computation of minimum unsatisfiable cores is concerned, the algorithm of Lynce and Marques-Silva (2004) searches all the unsat cores of the input problem;
this is done by introducing selector variables for the original clauses, and by increasing the
search space of the DPLL solver to include also such variables; then, (one of) the unsatisfiable subformulas with the smallest number of selectors assigned to true is returned. The
approach described by Mneimneh et al. (2005) instead is based on a branch-and-bound
algorithm that exploits the relation between maximal satisfiability and minimum unsatisfiability. The same relation is used also by the procedure of Zhang et al. (2006), which is
instead based on a genetic algorithm.
3.2 Techniques for Unsatisfiable-Core Extraction in SMT
To the best of our knowledge, there is no literature explicitly addressing the problem of
computing unsatisfiable cores in SMT 2 . However, four SMT solvers (i.e. CVC3, Barrett &
Tinelli, 2007, MathSAT, Bruttomesso et al., 2008, Yices, Dutertre & de Moura, 2006 and
2. Except for a previous short version of the present paper (Cimatti, Griggio, & Sebastiani, 2007).

707

Cimatti, Griggio, & Sebastiani

(¬(x = 0) ∨ ¬(x = 1))LA(Z)

(¬(x = 0) ∨ (x = 1) ∨ A2 )

((x = 0) ∨ ¬(x = 1) ∨ A1 )

((x = 0) ∨ (x = 1) ∨ A2 )

((x = 0) ∨ A1 ∨ A2 )

(¬(x = 0) ∨ A2 )
(¬A1 ∨ (y = 2))

(A1 ∨ A2 )

((y = 2) ∨ A2 )

(¬(y = 2) ∨ ¬(y < 0))LA(Z)

(A2 ∨ ¬(y < 0))
(¬(y = 1) ∨ ¬(y < 0))LA(Z)

(y < 0)

(¬A2 ∨ (y = 1))

(¬(y < 0) ∨ (y = 1))

(¬(y < 0))
⊥

Figure 3: Resolution proof for the SMT formula (1) found by MathSAT. Boxed clauses
correspond to the unsatisfiable core.

Z3, de Moura & Bjørner, 2008) support unsat core generation3 . In the following, we describe
the underlying approaches, that generalize techniques for propositional UC extraction. We
preliminarily remark that none of these solvers aims at producing minimal or minimum
unsat cores, nor does anything to reduce their size.
Strictly related with this work, Liffiton and Sakallah (2008) presented a general technique
for enumerating all minimal unsatisfiable subsets of a given inconsistent set of constraints,
which they implemented in the tool CAMUS. Although the description of the properties
and algorithms focuses on pure SAT, the authors remark that the approach extends easily
to SMT, and that they have implemented inside CAMUS a SMT version of the procedure.
Therefore in the following we briefly describe also their approach.
3.2.1 Proof-Based UC Extraction.
CVC3 and MathSAT can run in proof-producing mode, and compute unsatisfiable cores
as a byproduct of the generation of proofs. Similarly to the approach of Zhang and Malik
(2003), the idea is to analyze the proof of unsatisfiability backwards, and to return an
unsatisfiable core that is a collection of the assumptions (i.e. the clauses of the original
problem) that are used in the proof to derive contradiction.

3. The information reported here on the computation of unsat cores in CVC3, Yices and Z3 comes from
private communications from the authors and from the user manual of CVC3.

708

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

Example 1 In order to show how the described approaches work, consider this small unsatisfiable SMT (T ) formula, where T is LA(Z):
((x = 0) ∨ ¬(x = 1) ∨ A1 ) ∧ ((x = 0) ∨ (x = 1) ∨ A2 ) ∧ (¬(x = 0) ∨ (x = 1) ∨ A2 )∧
(¬A2 ∨ (y = 1)) ∧ (¬A1 ∨ (x + y > 3)) ∧ (y < 0) ∧ (A2 ∨ (x − y = 4))∧
((y = 2) ∨ ¬A1 ) ∧ (x ≥ 0), (1)
where x and y are real variables and A1 and A2 are Booleans.
In the proof-based approach, a resolution proof of unsatisfiability is built during the
search. E.g., Figure 3 shows the proof tree found by MathSAT. The leaves of the tree are
either original clauses (boxed in the Figure) or LA(Z)-lemmas (denoted with the LA(Z)
suffix). The unsatisfiable core is built by collecting all the original clauses appearing as
leaves in the proof. In this case, this is:
{((x = 0) ∨ ¬(x = 1) ∨ A1 ), ((x = 0) ∨ (x = 1) ∨ A2 ), (¬(x = 0) ∨ (x = 1) ∨ A2 ),
(¬A2 ∨ (y = 1)), (y < 0), ((y = 2) ∨ ¬A1 )}. (2)
In this case, the unsat core is minimal.
3.2.2 Assumption-Based UC Extraction
The approach used by Yices (Dutertre & de Moura, 2006) and Z3 (de Moura & Bjørner,
2008) is an adaptation of the method by Lynce and Marques-Silva (2004): for each clause
Ci in the problem, a new Boolean “selector” variable Si is created; then, each Ci is replaced
by (Si → Ci ); finally, before starting the search each Si is forced to true. In this way, when
a conflict at decision level zero is found by the DPLL solver the conflict clause contains only
selector variables, and the unsat core returned is the union of the clauses whose selectors
appear in such conflict clause.
Example 2 Consider again the formula (1) of Example 1. In the assumption-based approach, each of the 9 input clauses is augmented with an extra variable Si , which is asserted
to true at the beginning of the search. The formula therefore becomes:
^

Si ∧

i

(S1 → ((x = 0) ∨ ¬(x = 1) ∨ A1 )) ∧ (S2 → ((x = 0) ∨ (x = 1) ∨ A2 )) ∧
(S3 → (¬(x = 0) ∨ (x = 1) ∨ A2 )) ∧ (S4 → (¬A2 ∨ (y = 1))) ∧

(3)

(S5 → (¬A1 ∨ (x + y > 3))) ∧ (S6 → (y < 0)) ∧
(S7 → (A2 ∨ (x − y = 4))) ∧ (S8 → ((y = 2) ∨ ¬A1 )) ∧ (S9 → (x ≥ 0))
The final conflict clause generated by conflict analysis (Zhang et al., 2001) is:
¬S1 ∨ ¬S2 ∨ ¬S3 ∨ ¬S4 ∨ ¬S6 ∨ ¬S7 ∨ ¬S8 ,
4. using Yices.

709

4

(4)

Cimatti, Griggio, & Sebastiani

corresponding to the following unsat core:
{((x = 0) ∨ ¬(x = 1) ∨ A1 ), ((x = 0) ∨ (x = 1) ∨ A2 ), (¬(x = 0) ∨ (x = 1) ∨ A2 ),
(¬A2 ∨ (y = 1)), (y < 0), (A2 ∨ (x − y = 4)), ((y = 2) ∨ ¬A1 )}. (5)
Notice that this is not minimal, because of the presence of the redundant clause (A2 ∨(x−y =
4)), corresponding to ¬S7 in the final conflict clause (4).
Remark 1 The idea behind the two techniques just illustrated is essentially the same. Both
exploit the implication graph built by DPLL during conflict analysis to detect the subset of
the input clauses that were used to decide unsatisfiability. The main difference is that in
the proof-based approach this is done by explicitly constructing the proof tree, while in the
activation-based one this can be done “implicitly” by “labeling” each of the original clauses.
For a deeper comparison between these two approaches (and some variants of them), we
refer the reader to the work of Ası́n et al. (2008) and Nadel (2010).
3.2.3 The CAMUS Approach for Extracting All Minimal UC’s.
A completely different approach, aiming at generating all minimal UC’s of some given
inconsistent set of propositional clauses Φ, is presented by Liffiton and Sakallah (2008) and
implemented in the tool CAMUS. In a nutshell, the approach works in two distinct phases:
(a) enumerate the set M of all Minimal Correction Subsets (MCS’s) of Φ. 5 This is performed by a specialized algorithm, using as backend engine an incremental SAT solver
able to handle also AtMost constraints;
(b) enumerate the set U of all the minimal UC’s of Φ as minimal hitting sets of the set M .
This is also performed by a specialized algorithm. Alternatively, another algorithm can
produce from M only one minimal UC with much less effort.
It is important to notice that both sets M and U returned can be exponentially big wrt. the
size of Φ. Thus, the procedure may produce an exponential amount of MCS’s during phase
(a) before producing one UC. To this extent, the authors provide also some modified and
more efficient version of the technique, which sacrifice the completeness of the approach. We
refer the reader to the work of Liffiton and Sakallah (2008) for a more detailed explanation
of this technique and of its features.
As mentioned above, although the description of the algorithms focuses on pure SAT, the
authors remark that the approach extends easily to SMT, and that they have implemented
inside CAMUS a version of the algorithm working also for SMT, using Yices as backend
SMT solver. Unfortunately, they provide no details of such an extension. 6
5. A MCS Ψ of an unsatisfiable set of constraint Φ is the complement set of a maximal consistent subset
of Φ: Φ \ Ψ is consistent and, for every Ci ∈ Ψ, Φ \ (Ψ \ Ci ) is inconsistent (Liffiton & Sakallah, 2008).
6. See §10 “Conclusions and Future Work.” of the article by Liffiton and Sakallah (2008).

710

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

Example 3 Consider again the LA(Z)-formula (1) of Example 1 in form of clause set

def

Φ=





























c1
c2
c3
c4
c5
c6
c7
c8
c9

:
:
:
:
:
:
:
:
:

(x = 0) ∨ ¬(x = 1) ∨ A1 ,
(x = 0) ∨ (x = 1) ∨ A2 ,
¬(x = 0) ∨ (x = 1) ∨ A2 ,
¬A2 ∨ (y = 1),
¬A1 ∨ (x + y > 3),
(y < 0),
A2 ∨ (x − y = 4),
(y = 2) ∨ ¬A1 ,
(x ≥ 0)
















.

(6)















When run on (6), CAMUS returns the following two minimal UC’s:
def

uc
1 =
c1 :




c2 :



c3 :
c4 :





c
:

 5
c6 :

def


(x = 0) ∨ ¬(x = 1) ∨ A1 , 


(x = 0) ∨ (x = 1) ∨ A2 , 



¬(x = 0) ∨ (x = 1) ∨ A2 ,
,
¬A2 ∨ (y = 1),





¬A1 ∨ (x + y > 3),


(y < 0)

uc
2 =
c1 :




c2 :



c3 :
c4 :





c :

 6
c8 :

(x = 0) ∨ ¬(x = 1) ∨ A1 ,
(x = 0) ∨ (x = 1) ∨ A2 ,
¬(x = 0) ∨ (x = 1) ∨ A2 ,
¬A2 ∨ (y = 1),
(y < 0),
(y = 2) ∨ ¬A1










.

(7)









(Notice that uc2 is identical to the UC found in Example 1.)
We understand from Liffiton and Sakallah (2008) that, in order to produce uc1 and uc2 ,
CAMUS enumerates first (not necessarily in this order) the following set of MCS’s:
{{c1 }, {c2 }, {c3 }, {c4 }, {c6 }, {c5 , c8 }}

(8)

and then computes uc1 and uc2 as minimal hitting sets of (8).
Notice that (8) is a set of MCS’s because Φ, Φ \ {c5 } and Φ \ {c8 } are LA(Z)-inconsistent,
and
{A1 = ⊥, A2 = ⊥, x = 1, y = −3} |=LA(Z) Φ \ {c1 },
{A1 = ⊥, A2 = ⊥, x = 2, y = −6} |=LA(Z) Φ \ {c2 },
{A1 = ⊥, A2 = ⊥, x = 0, y = −4} |=LA(Z) Φ \ {c3 },
{A1 = ⊥, A2 = >, x = 0, y = −1} |=LA(Z) Φ \ {c4 },
{A1 = ⊥, A2 = >, x = 3, y = 1}
|=LA(Z) Φ \ {c6 },
{A1 = >, A2 = ⊥, x = 1, y = −1} |=LA(Z) Φ \ {c5 , c8 }.
Moreover, it contains all MCS’s of Φ because also Φ \ {c9 }, Φ \ {c5 , c9 } and Φ \ {c8 , c9 } are
LA(Z)-inconsistent.

4. A Novel Approach to Building Unsatisfiable Cores in SMT
We present a novel approach, called the Lemma-Lifting approach, in which the unsatisfiable
core is computed a posteriori w.r.t. the execution of the SMT solver, and only if the formula has been found T -unsatisfiable. This is done by means of an external (and possibly
optimized) propositional unsat core extractor.
711

Cimatti, Griggio, & Sebastiani

4.1 The Main Ideas
In the following, we assume that a lazy SMT (T ) procedure has been run over a T unsatisfiable set of SMT (T ) clauses ϕ =def {C1 , . . . , Cn }, and that D1 , . . . , Dk denote
all the T -lemmas, both theory-conflict and theory-deduction clauses, which have been returned by the T -solver during the run. (Notice that, by definition, T -lemmas are T -valid
clauses.) In case of mixed Boolean+theory-conflict clauses (Nieuwenhuis et al., 2006) (see
§ 2.2), the T -lemmas are those returned by the T -solver that have been used to compute
the mixed Boolean+theory-conflict clause, including the initial theory-conflict clause and
the theory-deduction clauses corresponding to the theory-propagation steps performed. 7
Under the above assumptions, two simple facts hold.
(i) Since the T -lemmas Di are valid in T , they do not affect the T -satisfiability of a formula:
(ψ ∧ Di ) |=T ⊥ ⇐⇒ ψ |=T ⊥.
(ii) The conjunction
of ϕ with all the T -lemmas D1 , . . . , Dk is propositionally unsatisfiable:
Vn
T 2P(ϕ ∧ i=1 Di ) |= ⊥.
Fact (i) is self-evident. Fact (ii) is the termination condition of all lazy SMT tools when
the input formula is T -unsatisfiable. InVthe off-line schema of Figure 2, the procedure ends
when DPLL establishes that T 2P(ϕ ∧ ni=1 Di ) is unsatisfiable, each Di being the negation
of the theory-conflict set ηi returned by the i-th call to the T -solver. Fact (ii) generalizes
to the on-line schema, noticing that T -backjumping on a theory-conflict clause Di produces
an analogous effect as re-invoking DPLL on ϕp ∧ T 2P(Di ), whilst theory propagation on
a deduction {l1 , . . . , lk }W|=T l can be seen as a form on unit propagation on the theorydeduction clause T 2P ( i ¬li ∨ l).
Example 4 Consider again formula (1) of Example 1. In order to decide its unsatisfiability, MathSAT generates the following set of LA(Z)-lemmas:
{(¬(x = 1) ∨ ¬(x = 0)), (¬(y = 2) ∨ ¬(y < 0)), (¬(y = 1) ∨ ¬(y < 0))}. (9)
Notice that they are all LA(Z)-valid (fact (i)). Then, the Boolean abstraction of (1) is
conjoined with the Boolean abstraction of these LA(Z)-lemmas, resulting in the following
propositional formula:
(B1 ∨ ¬B2 ∨ A1 ) ∧ (B1 ∨ B2 ∨ A2 ) ∧ (¬B1 ∨ B2 ∨ A2 ) ∧ (¬A2 ∨ B3 )∧
(¬A1 ∨ B4 ) ∧ B5 ∧ (A2 ∨ B6 ) ∧ (B7 ∨ ¬A1 ) ∧ B8 ∧
(¬B2 ∨ ¬B1 ) ∧ (¬B7 ∨ ¬B5 ) ∧ (¬B3 ∨ ¬B5 ), (10)
where:
B1
B2
B3
B4

def

= T 2P(x = 0)
def
= T 2P(x = 1)
def
= T 2P(y = 1)
def
= T 2P(x + y > 3)

B5
B6
B7
B8

def

= T 2P(y < 0)
= T 2P(x − y = 4)
def
= T 2P(y = 2)
def
= T 2P(x ≥ 0).
def

7. In this case, if the SMT solver did not provide the original T -lemmas when the feature of using mixed
Boolean+theory-conflict clauses is active, then the latter feature should be disabled.

712

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

The propositional formula (10) is unsatisfiable (fact (ii)), as demonstrated by the following
resolution proof.
(¬B2 ∨ ¬B1 )

(B1 ∨ ¬B2 ∨ A1 )

(¬B1 ∨ B2 ∨ A2 )

(B1 ∨ B2 ∨ A2 )

(B1 ∨ A1 ∨ A2 )

(¬B1 ∨ A2 )
(B7 ∨ ¬A1 )

(A1 ∨ A2 )

(B7 ∨ A2 )

(¬B7 ∨ ¬B5 )
(A2 ∨ ¬B5 )

(¬B3 ∨ ¬B5 )

(¬A2 ∨ B3 )

(¬B5 ∨ B3 )
¬B5

B5
⊥

Fact (ii) holds also for those SMT tools which learn mixed Boolean+theory-clauses
F1 , . . . , Fn (instead of T -lemmas), obtained from the T -lemmasVD1 , . . . , Dn by backward
traversal
implication graph. In fact, in this case, T 2P(ϕ ∧ ni=1 Fi ) |= ⊥ holds. Since
Vn of the V
ϕ ∧ i=1 Di |= ni=1 Fi , because of the way the Fi ’s are built, 8 (ii) holds.
Some SMT tools implement theory-propagation in a slightly different way (e.g. BarceLogic, Nieuwenhuis et al., 2006). If l1 , . . . , ln |=T l, instead of learning the T -lemma
¬l1 ∨ . . . ∨ ¬ln ∨ l and unit-propagating l on it, they simply propagate the value of l, without
learning any clause. Only if such propagation leads to a conflict later in the search, the
theory-deduction clause is learned and used for conflict-analysis. The validity of fact (ii) is
not affected by this optimization, because only the T -lemmas used during conflict analysis
are needed for it to hold (Nieuwenhuis et al., 2006).
Overall, in all variants of the on-line schema, the embedded DPLL engine builds –either
explicitly or implicitly– a resolution refutation of the Boolean abstraction of the conjunction
of the original clauses and the T -lemmas returned by the T -solver. Thus fact (ii) holds.
4.2 Extracting SMT Cores by Lifting Theory Lemmas
Facts (i) and (ii) discussed in §4.1 suggest a new approach to the generation of unsatisfiable
cores for SMT. The main idea is that if the theory lemmas used during the SMT search are
lifted into Boolean clauses, then the unsat core can be extracted by a purely propositional
core extractor. Therefore, we call this technique the Lemma-Lifting approach.
The algorithm is presented in Figure 4. The procedure T -Unsat Core receives as
input a set of clauses ϕ =def {C1 , . . . , Cn } and it invokes on it a lazy SMT (T ) tool
Lazy SMT Solver, which is instructed to store somewhere the T -lemmas returned by the
Vi−1
8. Each clause T 2P(Fi ) is obtained by resolving the clause T 2P(Di ) with clauses in T 2P(ϕ ∧ j=1
Fj ), so
Vi−1
Vn
Vn
that T 2P(ϕ
∧
F
∧
D
)
|=
T
2P(F
).
Thus,
by
induction,
T
2P(ϕ
∧
D
)
|=
T
2P(
i
i
i
j=1 Vj
i=1
i=1 Fi ), so
V
n
that ϕ ∧ n
D
|=
F
.
i
i
i=1
i=1

713

Cimatti, Griggio, & Sebastiani

Input clauses:

Result:

{C1 , . . . , Cn }

sat/unsat

T -unsat core:
0 }
{C10 , . . . , Cm

Lazy SMT Solver

T -valid clauses:
{D10 , . . . , Dj0 }

Stored T -Lemmas:
{D1 , . . . , Dk }
Boolean abstraction:

Refinement:

T 2P

P2T

Boolean unsat−core:

T 2P({C1 , . . . , Cn , D1 , . . . , Dk })

0 , D 0 , . . . , D 0 })
T 2P({C10 , . . . , Cm
1
j

Boolean Unsat Core Extractor

hSatValue,Clause seti T -Unsat Core(Clause set ϕ) {
// ϕ is {C1 , . . . , Cn }
if (Lazy SMT Solver(ϕ) == sat)
then return hsat,∅i;
// D1 , . . . , Dk are the T -lemmas stored by Lazy SMT Solver
ψ p =Boolean Unsat Core Extractor(T 2P({C1 , . . . , Cn , D1 , . . . , Dk }));
0 , D 0 , . . . , D 0 }));
// ψ p is T 2P({C10 , . . . , Cm
1
j
0 }i;
return hunsat,{C10 , . . . , Cm
}

Figure 4: Schema of the T -Unsat Core procedure: architecture (above) and algorithm (below).

T -solver, namely D1 , . . . , Dk . If Lazy SMT Solver returns sat, then the whole procedure
returns sat. Otherwise, the Boolean abstraction of {C1 , . . . , Cn , D1 , . . . , Dk }, which is inconsistent because of (ii), is fed to an external tool Boolean Unsat Core, which is able to
return the Boolean unsat core ψ p of the input. By construction, ψ p is the Boolean ab0 , D 0 , . . . , D 0 } s.t. {C 0 , . . . , C 0 } ⊆ {C , . . . , C } and
straction of a clause set {C10 , . . . , Cm
1
n
m
1
1
j
0
0
0 , D 0 , . . . , D 0 } is T {D1 , . . . , Dj } ⊆ {D1 , . . . , Dk }. As ψ p is unsatisfiable, then {C10 , . . . , Cm
1
j
unsatisfiable. By (i), the T -valid clauses D10 , . . . , Dj0 have no role in the T -unsatisfiability
0 , D 0 , . . . , D 0 }, so that they can be thrown away, and the procedure returns
of {C10 , . . . , Cm
1
j
0 }.
unsat and the T -unsatisfiable core {C10 , . . . , Cm
Notice that the resulting T -unsatisfiable core is not guaranteed to be minimal, even if
Boolean Unsat Core returns minimal Boolean unsatisfiable cores. In fact, it might be the
0 }\{C 0 } is T -unsatisfiable for some C 0 even though T 2P({C 0 , . . . , C 0 }\
case that {C10 , . . . , Cm
m
1
i
i
{Ci0 }) is satisfiable, because all truth assignments µp satisfying the latter are such that
P2T (µp ) is T -unsatisfiable.
714

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

Example 5 Consider the unsatisfiable SMT formula ϕ on LA(Z):
ϕ ≡ ((x = 0) ∨ (x = 1)) ∧ (¬(x = 0) ∨ (x = 1)) ∧ ((x = 0) ∨ ¬(x = 1))∧
(¬(x = 0) ∨ ¬(x = 1))
and its propositional abstraction T 2P(ϕ):
T 2P(ϕ) ≡ (B1 ∨ B2 ) ∧ (¬B1 ∨ B2 ) ∧ (B1 ∨ ¬B2 ) ∧ (¬B1 ∨ ¬B2 ).
Then, T 2P(ϕ) is a minimal Boolean unsatisfiable core of itself, but ϕ is not a minimal core
in LA(Z), since the last clause is valid in this theory, and hence it can be safely dropped.
The procedure can be implemented very simply by modifying the SMT solver so that
to store the T -lemmas and by interfacing it with some state-of-the-art Boolean unsat core
extractor used as an external black-box device. Moreover, if the SMT solver can provide
the set of all T -lemmas as output, then the whole procedure may reduce to a control device
interfacing with both the SMT solver and the Boolean core extractor as black-box external
devices.
Remark 2 Notice that here storing the T -lemmas does not mean learning them, that is,
the SMT solver is not required to add the T -lemmas to the formula during the search. Instead, it is for instance sufficient to store them in some ad-hoc data structure, or even
to dump them to a file. This causes no overhead to the Boolean search in the SMT
solver, and imposes no constraint on the lazy strategy adopted (e.g., offline/online, permanent/temporary learning, usage of mixed Boolean+theory conflict clauses, etc.).
Example 6 Once again, consider formula (1) of Example 1, and the corresponding formula
(10) of Example 4, which is the Boolean abstraction of (1) and the LA(Z)-lemmas (9) found
by MathSAT during search. In the Lemma-Lifting approach, (10) is given as input to an
external Boolean unsat core device. The resulting propositional unsatisfiable core is:
{(B1 ∨ ¬B2 ∨ A1 ), (B1 ∨ B2 ∨ A2 ), (¬B1 ∨ B2 ∨ A2 ), (¬A2 ∨ B3 ), B5 ,
(B7 ∨ ¬A1 ), (¬B2 ∨ ¬B1 ), (¬B7 ∨ ¬B5 ), (¬B3 ∨ ¬B5 )},
which corresponds (via P2T ) to:
{((x = 0) ∨ ¬(x = 1) ∨ A1 ), ((x = 0) ∨ (x = 1) ∨ A2 ), (¬(x = 0) ∨ (x = 1) ∨ A2 ),
(¬A2 ∨ (y = 1)), B5 , ((y = 2) ∨ ¬A1 ),
(¬(x = 1) ∨ ¬(x = 0)), (¬(y = 2) ∨ ¬(y < 0)), (¬(y = 1) ∨ ¬(y < 0))}.
Since the last three clauses are included in the LA(Z)-lemmas, and thus are LA(Z)-valid,
they are eliminated. The resulting core consists of only the first 6 clauses. In this case,
the core turns out to be minimal, and is identical modulo reordering to that computed by
MathSAT with proof-tracing (see Example 1).
As observed at the end of the previous section, our technique works also if the SMT tool
learns mixed Boolean+theory clauses (provided that the original T -lemmas are stored), or
715

Cimatti, Griggio, & Sebastiani

uses the lazy theory deduction of Nieuwenhuis et al. (2006). Moreover, it works also if
T -lemmas contain new atoms (i.e. atoms that do not appear in ϕ), as in the approaches of
Flanagan et al. (2003), and Barrett, Nieuwenhuis, Oliveras, and Tinelli (2006), since both
Facts (ii) and (i) hold also in that case.
As a side observation, we remark that the technique works also for the per-constraintencoding eager SMT approach of Goel, Sajid, Zhou, Aziz, and Singhal (1998), and Strichman, Seshia, and Bryant (2002). In the eager SMT approach, the input T -formula ϕ
is translated into an equi-satisfiable Boolean formula, and a SAT solver is used to check
its satisfiability. With per-constraint-encoding of Goel et al. (1998) and Strichman et al.
(2002), the resulting Boolean formula is the conjunction of the propositional abstraction ϕp
of ϕ and a formula ϕT which is the propositional abstraction of the conjunction of some
T -valid clauses. Therefore, ϕT plays the role of the T -lemmas of the lazy approach, and
our approach still works. This idea falls out of the scope of this work, and is not expanded
further.
4.3 Discussion
Despite its simplicity, the proposed approach is appealing for several reasons.
First, it is extremely simple to implement. The building of unsat cores is delegated
to an external device, which is fully decoupled from the internal DPLL-based enumerator.
Therefore, there is no need of implementing any internal unsat core constructor nor to
modify the embedded Boolean device. Every possible external device can be interfaced in
a plug-and-play manner by simply exchanging a couple of DIMACS files9 .
Second, the approach is fully compatible with optimizations carried out by the core
extractor at the Boolean level: every original clause which the Boolean unsat core device
is able to drop, is also dropped in the final formula. Notably, this involves also Boolean
unsat-core techniques which could be very difficult to adapt to the SMT setting (and to
implement within an SMT solver), such as the ones based on genetic algorithms (Zhang
et al., 2006).
Third, it benefits for free from the research on propositional unsat-core extraction, since
it is trivial to update: once some novel, more efficient or more effective Boolean unsat core
device is available, it can be used in a plug-and-play way. This does not require modifying
the DPLL engine embedded in the SMT solver.
One may remark that, in principle, if the number of T -lemmas generated by the T solver were huge, the storing of all T -lemmas might cause memory-exhaustion problems or
the generation of Boolean formulas which are too big to be handled by the Boolean unsatcore extractor. In practice, however, this is not a real problem. In fact, even the hardest
SMT formulas at the reach of current lazy SMT solvers rarely need generating more than
105 T -lemmas, whereas current Boolean unsat core extractors can handle formulas in the
order of 106 − 107 clauses. In fact, notice that the default choice in MathSAT is to learn
all T -lemmas permanently anyway, and we have never encountered problems due to this
fact. Intuitively, unlike with plain SAT, in lazy SMT the computational effort is typically
dominated by the search in the theory T , so that the number of clauses that can be stored
with a reasonable amount of memory, or which can be fed to a SAT solver, is typically much
9. DIMACS is a standard format for representing Boolean CNF formulas.

716

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

bigger than the number of calls to the T -solver which can overall be accomplished within a
reasonable amount of time.
Like with the other SMT unsat-core techniques adopted by current SMT solvers, also
with our novel approach the resulting T -unsatisfiable core is not guaranteed to be minimal,
even if Boolean Unsat Core returns minimal Boolean unsatisfiable cores. However, with
the Lemma-Lifting technique it is possible to perform all the reductions that can be done
by considering only the Boolean skeleton of the formula. Although this is in general not
enough to guarantee minimality, it is still a very significant gain, as we shall show in the
next section. Moreover, notice that it is also possible to obtain minimal UC’s by iteratively
calling one SMT core extractor, each time dropping one (or more) clause(s) from the current
UC and checking for the T -inconsistency. This minimization technique is orthogonal wrt.
the SMT core-extractor adopted, and as such it is not investigated here.

5. Empirical Evaluation
We carried out an extensive experimental evaluation of the the Lemma-Lifting approach.
We implemented the approach within the MathSAT (Bruttomesso et al., 2008) system.
MathSAT has been extended with an interface for external Boolean unsatisfiable core extractors (UCE) to exchange Boolean formulas and relative cores in form of files in DIMACS
format. (No modification was needed for the storage of T -lemmas, because MathSAT
already can learn permanently all of them.)
We have tried eight different external UCEs, namely Amuse (Oh et al., 2004), PicoSAT
(Biere, 2008), Eureka (Dershowitz et al., 2006), MiniUnsat (van Maaren & Wieringa,
2008), MUP (Huang, 2005), Trimmer (Gershman et al., 2008), ZChaff (Zhang & Malik,
2003), and the tool proposed by Zhang et al. (2006) (called Genetic here). All these
tools explicitly target core size reduction (or minimality), with the exception of PicoSAT,
which was conceived for speeding up core generation, with no claims of minimality. In fact,
PicoSAT turned out to be both the fastest and the least effective in reducing the size of the
cores. For these reasons, we adopted it as our baseline choice, as it is the ideal starting point
for evaluating the trade-off between efficiency (in execution time) and effectiveness (in core
size reduction). Thus, we start evaluating our approach by using PicoSAT as external
UCE (§5.1) and then we investigate the usage of more effective though more expensive
UCE’s (§ 5.2).
All the experiments have been performed on a subset of the SMT-LIB (Ranise & Tinelli,
2006) benchmarks. We used a total of 561 T -unsatisfiable problems, taken from the QF UF
(126), QF IDL (89), QF RDL (91), QF LIA (135) and QF LRA (120) divisions, selected
using the same criteria used in the annual SMT competition. In particular, the benchmarks
are selected randomly from the available instances in the SMT-LIB, but giving a higher
probability to real-world instances, as opposed to randomly generated or handcrafted ones.
(See http://www.smtcomp.org/ for additional details.)
We used a preprocessor to convert the instances into CNF (when required), and in some
cases we had to translate them from the SMT language to the native language of a particular
SMT solver. 10
10. In particular, CVC3 and Yices can compute unsatisfiable cores only if the problems are given in their
own native format.

717

Cimatti, Griggio, & Sebastiani

100

100

PicoSAT time

1000

PicoSAT time

1000

10

1

0.1

10

1

0.1
0.1

1

10

100

1000

Total time

0.1

1

10

100

1000

MathSAT time

Figure 5: Overhead of PicoSAT wrt. the total execution time of MathSAT+PicoSAT
(left) and wrt. the execution time of MathSAT (right).

All the tests were performed on 2.66 GHz Intel Xeon machines with 16 GB of RAM
running Linux. For each tested instance (unless explicitly stated otherwise) the timeout
was set to 600 seconds, and the memory limit to 2 GB. For all the Boolean UCEs, we have
used the default configurations.
5.1 Costs and Effectiveness of Unsat-Core Extraction Using PicoSAT
The two scatter plots in Figure 5 give a first insight on the price that the Lemma-Lifting
approach has to pay for running the external UCE. The plot on the left compares the
execution time of PicoSAT with the total time of MathSAT+PicoSAT, whilst the plot
on the right shows the comparison of the time of PicoSAT against that of MathSAT
solving time only. From the two figures, it can be clearly seen that, except for few cases,
the time required by PicoSAT is much lower or even negligible wrt. MathSAT solving
time. Notice also that this price is payed only in the case of unsatisfiable benchmarks.
We now analyze our LL approach with respect to the size of the unsat cores returned. We
compare the baseline implementation of our LL approach, MathSAT+PicoSAT, against
MathSAT+ProofBasedUC (i.e. MathSAT with proof tracing), CVCLite (Barrett &
Tinelli, 2007), 11 and Yices. 12 We have also performed a comparison with (the SMT version
of) CAMUS (Liffiton & Sakallah, 2008), running it in “SingleMUS” mode (generate only
one minimal UC, “CAMUS-one” hereafter). We also tried to run CAMUS in “AllMUS”
mode (generate all minimal UC’s), but we encountered some unexpected results (in some

11. We tried to use the newer CVC3, but we had some difficulties in the extraction of unsatisfiable cores
with it. Therefore, we reverted to the older CVCLite for the experiments.
12. CVCLite version 20061231 and Yices version 1.0.19.

718

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

Core/Problem size ratio

MathSAT+PicoSAT

MathSAT+ProofBasedUC

1

1

1/2

1/2

1/5

1/5

1/10

1/10

1/100

1/100

1/1000

1/1000
10

100

1000

10000

100000

10

Core/Problem size ratio

Yices

100

1000

10000

100000

10000

100000

CVCLite

1

1

1/2

1/2

1/5

1/5

1/10

1/10

1/100

1/100

1/1000

1/1000
10

100

1000

10000

100000

10

100

1000

CAMUS-one
1

Core/Problem size ratio

1/2

1/5
1/10

1/100

1/1000
10

100

1000

10000

100000

Size of the problem (# of clauses)

Size of the problem (# of clauses)

Figure 6: Ratio between the size of the original formula and that of the unsat core computed
by the various solvers.

719

Cimatti, Griggio, & Sebastiani

CVCLite w.u.c.

MathSAT+ProofBasedUC

MathSAT+PicoSAT

MathSAT+PicoSAT

3

3

2

2

3/2

3/2

1

1

2/3

2/3

1/2

1/2

1/3

1/3
10

100

1000

10000

100000

10

100

Yices w.u.c.

1000

10000

100000

CAMUS-one

MathSAT+PicoSAT

MathSAT+PicoSAT

3

3

2

2

3/2

3/2

1

1

2/3

2/3

1/2

1/2

1/3

1/3
10

100

1000

10000

core size ratio
CVCLite w.u.c.
MathSAT+PicoSAT
MathSAT+ProofBasedUC
MathSAT+PicoSAT
Yices w.u.c.
MathSAT+PicoSAT
CAMUS-one
MathSAT+PicoSAT

100000

10

100

1000

10000

100000

1st quartile

median

mean

3rd quartile

1.00

1.16

1.33

1.36

1.00

1.03

1.09

1.10

0.97

1.03

1.08

1.09

0.88

1.02

1.32

1.18

Figure 7: Comparison of the size of the unsat cores computed by MathSAT+PicoSAT
against those of CVCLite, MathSAT+ProofBasedUC, Yices with unsat
cores and CAMUS-one, with statistics on unsat core ratios.
Points above the middle line and values greater than 1.00 mean better core quality
for MathSAT+PicoSAT, and vice versa.

executions all the generated MUSes were larger than the unsat cores found by the other
tools13 ), and so we had to exclude it from the experiments.
13. This is very surprising because, by definition, the output produced by CAMUS in “AllMUS” mode
should should always contain UC’s of minimum size, and thus smaller than those found by the other
tools. Therefore, we have no explanation for such results, apart from conjecturing the presence of some
bug in CAMUS, or some incorrect use from our side (although we followed the indications of the authors),

720

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

In order to allow CAMUS-one terminate for a significant amount of samples, we have
run it with an increased timeout of 1800 seconds. Even so, CAMUS-one was able to produce one UC within the timeout only for 144 formulas out of 561. For the record, MathSAT+PicoSAT, MathSAT+ProofBasedUC, CVCLite, and Yices solved within the
timeout 474, 503, 253 and 494 problems out of 561 respectively.
Notice that we do not present any comparison in time between the different tools because
it is not significant for determining the relative cost of unsat-core computation, since (i)
for all the former four tools the time is completely dominated by the solving time, which
varies a lot from solver to solver (even within MathSAT, proof production requires setting
ad-hoc options, which may result into significantly-different solving times since a different
search space is explored); (ii) a comparison with CAMUS in terms of speed would not be
fair, since the ultimate goal of CAMUS is to enumerate all mimimal UC’s, and as such it
first runs the very-expensive step of enumerating all MCS’s (see §3.2).
Figure 6 shows the absolute reduction in size performed by the different solvers: the
x-axis displays the size (number of clauses) of the problem, whilst the y-axis displays the
ratio between the size of the unsat core and the size of the problem. For instance, a point
with y value of 1/10 means that the unsatisfiability is due to only 10% of the problem
clauses.
Figure 7(top) shows relative comparisons of the data of Figure 6. Each plot compares
MathSAT+PicoSAT with each of the other solvers. Such plots, which we shall call
“core-ratio” plots, have the following meaning: the x-axis displays the size (number of
clauses) of the problem, whilst the y-axis displays the ratio between the size of the unsat
core computed by CVCLite, MathSAT+ProofBasedUC, Yices or CAMUS-one and
that computed by MathSAT+PicoSAT. For instance, a point with y value of 1/2 means
that the unsat core computed by the current solver is half the size of that computed by
MathSAT+PicoSAT; values above 1 mean a smaller core for MathSAT+PicoSAT. In
core-ratio plots, we only consider the instances for which both solvers terminated successfully, since here we are only interested in the size of the cores computed, and not in the
execution times. Figure 7(bottom) reports statistics about the ratios of the unsat core sizes
computed by two different solvers.
A comment is in order. The results reported for CAMUS-one are quite surprising
wrt. our expectations, since CAMUS-one is supposed to return a minimal UC, so that
we would expect greater reductions in core sizes. This can be explained by the fact that
the minimal UC produced by CAMUS-one is not necessarily minimum. In fact, we have
manually verified for the samples with the biggest core-size ratio that the UC’s returned by
CAMUS-one are actually minimal, although significantly bigger than those returned by
MathSAT+PicoSAT.
Overall, the results presented show that, even when using as Boolean UCE PicoSAT,
which is the least effective in reducing the size of the cores, the effectiveness of the baseline
version of our LL approach is slightly better than those of the other tools.

or the activation by default of some of the incomplete heuristics CAMUS can use in order to cope with
the combinatorial explosion in the number of MCS’s UC’s generated (see §3.2.)

721

Cimatti, Griggio, & Sebastiani

further reduction

core size

execution time

wrt. baseline

1
3

1000

1/2

2

Amuse

1/5

100
3/2

1/10

1

1/100

10

2/3
1

1/2

1/3
1/1000

0.1
10

100

1000

10000

100000

10

100

1000

10000

100000

0.1

1

10

100

1000

0.1

1

10

100

1000

0.1

1

10

100

1000

1
3

1000

1/2

2

1/5

100
3/2

Genetic

1/10

1

1/100

10

2/3
1

1/2

1/3
1/1000

0.1
10

100

1000

10000

100000

10

100

1000

10000

100000

1
3

1000

Eureka

1/2

2

1/5

100
3/2

1/10

1

1/100

10

2/3
1

1/2

1/3
1/1000

0.1
10

100

1000

10000

100000

10

100

1000

10000

100000

Figure 8: Comparison of the core sizes (left), core ratios (middle) and run times (right)
using different propositional unsat core extractors. In the core-ratio plots (2nd
column), the X-axis represents the size of the problem, and the Y-axis represents
the ratio between the size of the cores computed by the two systems: a point
above the middle line means better quality for the baseline system. In the scatter
plots (3rd column), the baseline system (MathSAT+PicoSAT) is always on the
X-axis.

5.2 Impact on Costs and Effectiveness Using Different Boolean Unsat Core
Extractors
In this second part of our experimental evaluation we compare the results obtained using
different UCE’s in terms of costs and effectiveness in reducing the size of the core. We show
that, depending on the UCE used, it is possible to reduce significantly the size of cores,
and to trade core quality for speed of execution (and vice versa), with no implementation
722

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

further reduction

core size

execution time

wrt. baseline

1
3

1000

MiniUnsat

1/2

2

1/5

100
3/2

1/10

1

1/100

10

2/3
1

1/2

1/3
1/1000

0.1
10

100

1000

10000

100000

10

100

1000

10000

100000

0.1

1

10

100

1000

0.1

1

10

100

1000

0.1

1

10

100

1000

1
3

1000

Trimmer

1/2

2

1/5

100
3/2

1/10

1

1/100

10

2/3
1

1/2

1/3
1/1000

0.1
10

100

1000

10000

100000

10

100

1000

10000

100000

1
3

1000

ZChaff

1/2

2

1/5

100
3/2

1/10

1

1/100

10

2/3
1

1/2

1/3
1/1000

0.1
10

100

1000

10000

100000

10

100

1000

10000

100000

Figure 9: Comparison of the core sizes (left), core ratios (middle) and run times (right)
using different propositional unsat core extractors (continued).

effort. We compare our baseline configuration MathSAT+PicoSAT, against six other
configurations, each calling a different propositional UCE.
The results are collected in Figures 8-9. The first column shows the absolute reduction
in size performed by each tool (as in Figure 6). The second column shows core-ratio plots
comparing each configuration against the baseline one using PicoSAT (as in Figure 7, with
points below 1.00 meaning a better performance of the current configuration). Finally, the
scatter plots in the third column compare the execution times (with PicoSAT always on
the X-axis). We evaluated the six configurations which use, respectively, Amuse (Oh et al.,
2004), Genetic (Zhang et al., 2006), Eureka (Dershowitz et al., 2006), MiniUnsat (van
Maaren & Wieringa, 2008), Trimmer (Gershman et al., 2008), and ZChaff (Zhang &
Malik, 2003), against the baseline configuration, using PicoSAT. We also compared with
MUP (Huang, 2005), but we had to stop the experiments because of memory exhaustion
723

Cimatti, Griggio, & Sebastiani

CVCLite w.u.c.

MathSAT+ProofBasedUC

MathSAT+Eureka

MathSAT+Eureka

3

3

2

2

3/2

3/2

1

1

2/3

2/3

1/2

1/2

1/3

1/3
10

100

1000

10000

100000

10

Yices w.u.c.

100

1000

10000

100000

CAMUS-one

MathSAT+Eureka

MathSAT+Eureka

3

3

2

2

3/2

3/2

1

1

2/3

2/3

1/2

1/2

1/3

1/3
10

100

1000

10000

core size ratio
CVCLite w.u.c.
MathSAT+Eureka
MathSAT+ProofBasedUC
MathSAT+Eureka
Yices w.u.c.
MathSAT+Eureka
CAMUS-one
MathSAT+Eureka

100000

10

100

1000

10000

100000

1st quartile

median

mean

3rd quartile

1.03

1.32

1.55

1.73

1.03

1.17

1.27

1.35

1.00

1.16

1.28

1.34

0.98

1.05

1.41

1.26

Figure 10: Ratios of the unsat-core sizes computed by MathSAT+Eureka against those
of CVCLite, MathSAT+ProofBasedUC, Yices and CAMUS-one.
Points above the middle line and values greater than 1.00 mean better core
quality for MathSAT+Eureka, and vice versa.

problems. Looking at the second column, we notice that Eureka, followed by MiniUnsat
and ZChaff, seems to be the most effective in reducing the size of the final unsat cores,
up to 1/3 the size of those obtained with plain PicoSAT. Looking at the third column, we
notice that with Genetic, Amuse, MiniUnsat and ZChaff, and in part with Eureka,
efficiency degrades drastically, and many problems cannot be solved within the timeout.
With Trimmer the performance gap is not that dramatic, but still up to an order magnitude
slower than the baseline version.
724

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

Finally, in Figure 10 we compare the effectiveness of MathSAT+Eureka, the most
effective extractor in Figures 8-9, directly with that of the other three solvers, CVCLite,
MathSAT+ProofBasedUC and Yices, and with that of CAMUS. (Also compare the
results with those in Figure 7.) The gain in core reduction wrt. previous state-of-the-art
SMT core-reduction techniques is evident.
It is important to notice that, due to our limited know-how, we used the Boolean UCE’s
in their default configurations. Therefore, we believe that even better results, in terms of
both effectiveness and efficiency, could be obtained by means of a more accurate tuning of
the parameters of the core extractors.
As a side remark, we notice that the results in Figures 8-9 have produced as a byproduct
an insightful evaluation of the main Boolean unsat-core-generation tools currently available.
To this extent, we notice that the performances of MUP (Huang, 2005) and Genetic (Zhang
et al., 2006) seem rather poor; PicoSAT (Biere, 2008) is definitely the fastest tool, though
the least effective in reducing the size of the final core; on the opposite side, Eureka
(Dershowitz et al., 2006) is the most effective in this task, but pays a fee in terms of CPU
time; Trimmer (Gershman et al., 2008) represents a good compromise between effectiveness
and efficiency.

6. Conclusions
We have presented a novel approach to generating small unsatisfiable cores in SMT, that
computes them a posteriori, relying on an external propositional unsat core extractor. The
technique is very simple in concept, and straightforward to implement and update. Moreover, it benefits for free of all the advancements in propositional unsat core computation.
Our experimental results have shown that, by using different core extractors, it is possible
to reduce significantly the size of cores and to trade core quality for speed of execution (and
vice versa), with no implementation effort.
As a byproduct, we have also produced an insightful evaluation of the main Boolean
unsat-core-generation tools currently available.

Acknowledgments
We wish to thank Mark Liffiton for his help with the CAMUS tool. We also thank the
anonymous referees for their helpful suggestions.
A. Griggio is supported in part by the European Community’s FP7/2007-2013 under grant
agreement Marie Curie FP7 - PCOFUND-GA-2008-226070 “progetto Trentino”, project
Adaptation.
R. Sebastiani is supported in part by SRC under GRC Custom Research Project 2009-TJ1880 WOLFLING.

References
Ası́n, R., Nieuwenhuis, R., Oliveras, A., & Rodrı́guez Carbonell, E. (2008). Efficient Generation of Unsatisfiability Proofs and Cores in SAT. In Cervesato, I., Veith, H., &
Voronkov, A. (Eds.), Proceedings of LPAR’08, Vol. 5330 of LNCS, pp. 16–30. Springer.
725

Cimatti, Griggio, & Sebastiani

Audemard, G., Bertoli, P., Cimatti, A., Kornilowicz, A., & Sebastiani, R. (2002). A SAT
Based Approach for Solving Formulas over Boolean and Linear Mathematical Propositions. In Proc. CADE’2002., Vol. 2392 of LNAI. Springer.
Barrett, C., Nieuwenhuis, R., Oliveras, A., & Tinelli, C. (2006). Splitting on Demand in
SAT Modulo Theories.. In Hermann, M., & Voronkov, A. (Eds.), LPAR, Vol. 4246 of
LNCS, pp. 512–526. Springer.
Barrett, C., & Tinelli, C. (2007). CVC3. In Damm, W., & Hermanns, H. (Eds.), CAV, Vol.
4590 of LNCS, pp. 298–302. Springer.
Barrett, C. W., Dill, D. L., & Stump, A. (2002). Checking Satisfiability of First-Order
Formulas by Incremental Translation to SAT. In Brinksma, E., & Larsen, K. G. (Eds.),
Computer Aided Verification, 14th International Conference, CAV 2002, Copenhagen,
Denmark, July 27-31, 2002, Proceedings, Vol. 2404 of LNCS, pp. 236–249. Springer.
Barrett, C. W., Sebastiani, R., Seshia, S. A., & Tinelli, C. (2009). Satisfiability modulo
theories. In Biere, A., Heule, M., & van Maaren, H. (Eds.), Handbook of Satisfiability.
IOS Press.
Biere, A. (2008). Picosat essentials. Journal on Satisfiability, Boolean Modeling and Computation (JSAT), 4, 75–97.
Bruttomesso, R., Cimatti, A., Franzén, A., Griggio, A., & Sebastiani, R. (2008). The
MathSAT 4 SMT Solver. In Gupta, A., & Malik, S. (Eds.), CAV, Vol. 5123 of LNCS,
pp. 299–303. Springer.
Bryant, R. E., Kroening, D., Ouaknine, J., Seshia, S. A., Strichman, O., & Brady, B. (2009).
An abstraction-based decision procedure for bit-vector arithmetic. Int. J. Softw. Tools
Technol. Transf., 11 (2), 95–104.
Cimatti, A., Griggio, A., & Sebastiani, R. (2007). A Simple and Flexible Way of Computing
Small Unsatisfiable Cores in SAT Modulo Theories.. In Marques-Silva, J., & Sakallah,
K. A. (Eds.), SAT, Vol. 4501 of LNCS, pp. 334–339. Springer.
Davis, M., & Putnam, H. (1960). A computing procedure for quantification theory. Journal
of the ACM, 7, 201–215.
Davis, M., Logemann, G., & Loveland, D. W. (1962). A machine program for theoremproving.. Commun. ACM, 5 (7), 394–397.
de Moura, L., & Bjørner, N. (2008). Z3: An Efficient SMT Solver. In Ramakrishnan, C. R.,
& Rehof, J. (Eds.), TACAS, Vol. 4963 of LNCS, pp. 337–340. Springer.
Dershowitz, N., Hanna, Z., & Nadel, A. (2006). A Scalable Algorithm for Minimal Unsatisfiable Core Extraction.. In Proceedings of SAT’06, Vol. 4121 of LNCS. Springer.
Dutertre, B., & de Moura, L. (2006). A Fast Linear-Arithmetic Solver for DPLL(T). In
Proc. CAV’06, Vol. 4144 of LNCS. Springer.
Enderton, H. (1972). A Mathematical Introduction to Logic. Academic Pr.
Flanagan, C., Joshi, R., Ou, X., & Saxe, J. B. (2003). Theorem Proving Using Lazy Proof
Explication.. In Jr., W. A. H., & Somenzi, F. (Eds.), CAV, Vol. 2725 of LNCS, pp.
355–367. Springer.
726

Computing Small Unsatisfiable Cores in Satisfiability Modulo Theories

Gershman, R., Koifman, M., & Strichman, O. (2008). An approach for extracting a small
unsatisfiable core. Formal Methods in System Design, 33 (1-3), 1–27.
Goel, A., Sajid, K., Zhou, H., Aziz, A., & Singhal, V. (1998). BDD Based Procedures for
a Theory of Equality with Uninterpreted Functions.. In Hu, A. J., & Vardi, M. Y.
(Eds.), CAV, Vol. 1427 of LNCS, pp. 244–255. Springer.
Goldberg, E. I., & Novikov, Y. (2003). Verification of Proofs of Unsatisfiability for CNF
Formulas. In Proceedings of 2003 Design, Automation and Test in Europe Conference
and Exposition (DATE 2003), pp. 886–891. IEEE Computer Society.
Grumberg, O., Lerda, F., Strichman, O., & Theobald, M. (2005).
Proof-guided
underapproximation-widening for multi-process systems. SIGPLAN Not., 40 (1), 122–
131.
Huang, J. (2005). MUP: a minimal unsatisfiability prover. In Proceedings of ASP-DAC ’05.
ACM Press.
Liffiton, M., & Sakallah, K. (2008). Algortithms for Computing Minimal Unsatisfiable
Subsets of Constraints. Journal of Automated Reasoning, 40 (1).
Lynce, I., & Marques-Silva, J. P. (2004). On Computing Minimum Unsatisfiable Cores.
In SAT 2004 - The Seventh International Conference on Theory and Applications of
Satisfiability Testing, 10-13 May 2004, Vancouver, BC, Canada, Online Proceedings.
Marques-Silva, J. P., & Sakallah, K. A. (1996). GRASP - A new Search Algorithm for
Satisfiability. In Proc. ICCAD’96.
McMillan, K. L. (2002). Applying SAT Methods in Unbounded Symbolic Model Checking.
In Brinksma, E., & Larsen, K. G. (Eds.), Proceedings of CAV’02, Vol. 2404 of LNCS,
pp. 250–264. Springer.
McMillan, K. L., & Amla, N. (2003). Automatic abstraction without counterexamples. In
Garavel, H., & Hatcliff, J. (Eds.), Proceedings of TACAS’03, Vol. 2619 of LNCS, pp.
2–17. Springer.
Mneimneh, M. N., Lynce, I., Andraus, Z. S., Marques-Silva, J. P., & Sakallah, K. A. (2005).
A Branch-and-Bound Algorithm for Extracting Smallest Minimal Unsatisfiable Formulas.. In Proc. SAT’05, Vol. 3569 of LNCS. Springer.
Nadel, A. (2010). Boosting Minimal Unsatisfiable Core Extraction. In Bloem, R., & Sharygina, N. (Eds.), Proceedings of the 10th International Conference on Formal Methods
in Computer-Aided Design (FMCAD2010), pp. 221–229.
Nieuwenhuis, R., Oliveras, A., & Tinelli, C. (2006). Solving SAT and SAT Modulo Theories:
From an abstract Davis–Putnam–Logemann–Loveland procedure to DPLL(T). J.
ACM, 53 (6), 937–977.
Oh, Y., Mneimneh, M. N., Andraus, Z. S., Sakallah, K. A., & Markov, I. L. (2004).
Amuse: A Minimally-Unsatisfiable Subformula Extractor. In Proceedings of DAC’04.
ACM/IEEE.
Ranise, S., & Tinelli, C. (2006). The Satisfiability Modulo Theories Library (SMT-LIB).
www.SMT-LIB.org.
727

Cimatti, Griggio, & Sebastiani

Sebastiani, R. (2007). Lazy Satisfiability Modulo Theories. Journal on Satisfiability, Boolean
Modeling and Computation, JSAT, Volume 3.
Strichman, O., Seshia, S. A., & Bryant, R. E. (2002). Deciding Separation Formulas with
SAT. In Brinksma, E., & Larsen, K. G. (Eds.), CAV, Vol. 2404 of LNCS, pp. 209–222.
Springer.
Suelflow, A., Fey, G., Bloem, R., & Drechsler, R. (2008). Using unsatisfiable cores to debug
multiple design errors. In Proceedings of GLSVLSI’08, pp. 77–82, New York, NY,
USA. ACM.
Tseitin, G. S. (1983). On the complexity of derivation in propositional calculus. Automation
of Reasoning: Classical Papers in Computational Logic 1967-1970Studies in Constructive Mathematics and Mathematical Logic, Part 2, 2. Originally published 1970.
van Maaren, H., & Wieringa, S. (2008). Finding Guaranteed MUSes Fast. In SAT, Vol.
4996 of LNCS, pp. 291–304. Springer.
Wang, C., Kim, H., & Gupta, A. (2007). Hybrid CEGAR: combining variable hiding and
predicate abstraction. In Proceedings of ICCAD’07, pp. 310–317, Piscataway, NJ,
USA. IEEE Press.
Zhang, J., Li, S., & Shen, S. (2006). Extracting Minimum Unsatisfiable Cores with a Greedy
Genetic Algorithm.. In Proceedings of ACAI, Vol. 4304 of LNCS. Springer.
Zhang, L., Madigan, C. F., Moskewicz, M. H., & Malik, S. (2001). Efficient conflict driven
learning in a boolean satisfiability solver. In Proceedings of ICCAD ’01. IEEE Press.
Zhang, L., & Malik, S. (2002). The quest for efficient boolean satisfiability solvers.. In
Voronkov, A. (Ed.), CADE, Vol. 2392 of LNCS, pp. 295–313. Springer.
Zhang, L., & Malik, S. (2003). Extracting Small Unsatisfiable Cores from Unsatisfiable
Boolean Formulas. In Proceedings of 6th International Conference on Theory and
Applications of Satisfiability Testing (SAT2003).

728

Journal of Artiﬁcial Intelligence Research 40 (2011) 599-656

Submitted 11/10; published 03/11

Decidability and Undecidability Results
for Propositional Schemata
Vincent Aravantinos
Ricardo Caferra
Nicolas Peltier

Vincent.Aravantinos@imag.fr
Ricardo.Caferra@imag.fr
Nicolas.Peltier@imag.fr

Université de Grenoble (LIG/CNRS)
Bât. IMAG C, 220, rue de la Chimie
38400 Saint Martin d’Hères, France

Abstract
We deﬁne a logic of propositional formula schemata adding to the
∨ syntax
∧ of propositional logic indexed propositions (e.g., pi ) and iterated∧connectives
or
ranging over
n
intervals parameterized by arithmetic variables (e.g., i=1 pi , where n is a parameter ).
The satisﬁability problem is shown to be undecidable for this new logic, but we introduce a
very general class of schemata, called bound-linear, for which this problem becomes decidable. This result is obtained by reduction to a particular class of schemata called regular,
for which we provide a sound and complete terminating proof procedure. This schemata
calculus (called stab) allows one to capture proof patterns corresponding to a large class
of problems speciﬁed in propositional logic. We also show that the satisﬁability problem becomes again undecidable for slight extensions of this class, thus demonstrating that
bound-linear schemata represent a good compromise between expressivity and decidability.

1. Introduction
Being able to solve classes of problems – possibly eﬃciently and elegantly – strongly depends
on the language in which they are speciﬁed. This is decisive in a lot of applications of
Artiﬁcial Intelligence. One language long used by humans is that of schemata. As very
general characterizations of the notion of schema would be useless, we have focused on
a particular class of schemata arising naturally in practice, quite expressive and (as will
be shown) with “good” computational properties. These schemata are those generated by
unbounded repetitions of patterns, we call them ‘iterated schemata’.
We motivate our approach via an example, frequently used and well-known by the
AI community: circuit veriﬁcation. Circuit veriﬁcation problems are often modeled as
sequences of propositional problems parameterized by a natural number n that encodes the
size of the data (e.g., the number of bits, number of layers in the circuit, etc.). We call these
sequences iterated schemata, or simply schemata. A typical example is an n-bit sequential
adder circuit i.e. a circuit which computes the sum of two bit-vectors of length n. Such a
circuit is built by composing n 1-bit adders. The ith bits of each operand are written pi
and qi . ri is the ith bit of the result and ci+1 is carried over to the next bit (thus c1 = 0).
We set the notations (⊕ denotes exclusive or):
Sumi (p, q, c, r) = ri ⇔ (pi ⊕ qi ) ⊕ ci
def

c
⃝2011
AI Access Foundation. All rights reserved.

Aravantinos, Caferra & Peltier

and
Carryi (p, q, c) = ci+1 ⇔ (pi ∧ qi ) ∨ (ci ∧ pi ) ∨ (ci ∧ qi ).
def

Then the formula:
def

Adder(p, q, c, r) =

n
∧

Sumi (p, q, c, r) ∧

i=1

n
∧

Carryi (p, q, c) ∧ ¬c1

i=1

with the constraint n ≥ 1, schematises the adder circuit (it states that r encodes the sum of
p and q). Adder contains iterations ranging on intervals depending on n. If n is instantiated
by a natural number then the expression reduces to a propositional formula. Therefore
each instance of this schema can be solved in propositional logic. However, proving that
the schema is unsatisﬁable (or satisﬁable) for every instance of n is much harder. This
problem cannot be speciﬁed in propositional logic and, as we shall see, this is even out of
the scope of ﬁrst-order logic. It can be expressed in higher order logics but it is well-known
that such languages are less suitable for automation (see Section 3 for details).
Such iteration schemata are ubiquitous in formalized reasoning. Problems over ﬁnite
domains can be speciﬁed as generic propositional formulae ﬁtting the same pattern, the
parameter being the (ﬁnite but unbounded) size of the domain. Among these patterns,
those corresponding to the pigeonhole principle, Ramsey theory, coloring graphs problems or
constraint programming speciﬁcations such as the n-queens problem (Marriott, Nethercote,
Rafeh, Stuckey, Garcı́a de la Banda, & Wallace, 2008) should be mentioned. Iterated
schemata are also extremely useful for the formalization of mathematical proofs, because
they allow one to express inﬁnite proof sequences, which can avoid, for instance, explicit
use of the induction principle. This idea has been used, e.g., in the work of Hetzl, Leitsch,
Weller, and Woltzenlogel Paleo (2008).
In this paper we present the ﬁrst (to the best of our knowledge) thorough analysis of
propositional iterated schemata. We deﬁne a logic handling arithmetic variables, indexed
propositions and iterated connectives. The satisﬁability problem is obviously semi-decidable
in the sense that a (straightforward) algorithm exists to enumerate all satisﬁable schemata
(i.e. all schemata with a satisﬁable instance). However the set of (unrestricted) unsatisﬁable
schemata is not recursively enumerable. Thus we restrict ourselves to a particular class of
schemata, called bound-linear and we provide a decision procedure for this class. This
procedure is based on a reduction to a very simple class of schemata, called regular, for
which a tableaux-based proof procedure is presented. Then we provide some undecidability
results for (rather natural) extensions of this class.
The rest of the paper is structured as follows.
• In Section 2 we introduce a logic (syntax and semantics) for handling propositional
schemata and we establish some of its basic properties. The propositional symbols are
indexed by arithmetic expressions (e.g., pn+1 ) containing arithmetic variables. These
variables can be either parameters (i.e.∨free variables),
or bound variables introduced
∧
by generalized connectives of the form bi=a or bi=a . These connectives can be read as
∃i ∈ [a, b] or ∀i ∈ [a, b], where a, b are arithmetic expressions possibly containing (free
or bound) variables. We restrict ourselves to monadic schemata (i.e. the propositions
600

Decidability and Undecidability Results for Propositional Schemata

are indexed by at most one expression) and to linear arithmetic expressions1 . We then
introduce a particular subclass of schemata, called bound-linear. Intuitively, a schema
is bound-linear if every arithmetic expression occurring in it contains at most one
bound variable. Furthermore, the coeﬃcient of this variable in the expression should
be ±1 (or 0). Thus expressions such as 1, n, 2n − i or i + 2 are allowed (where n is
the parameter and i a bound variable), but 2i or i + j (where i, j are both bound) are
not. The coeﬃcient of the parameter n is not constrained.
• Section 3 contains a brief survey of existing work on propositional schemata as well
as (informal) comparisons with related logics.
• In Section 4 we introduce a simpler class of schemata, called regular, and we provide
an algorithm to transform every bound-linear schema into a (sat-)equivalent regular
schema.
• In Section 5 a tableaux-based proof procedure, called stab (standing for schemata
tableaux), is introduced for reasoning with propositional schemata. This proof procedure is sound and complete (w.r.t. satisﬁability) and terminates on every regular
schema. Together with the results in Section 4 this implies that the class of boundlinear schemata is decidable.
• Section 6 shows that relaxing very slightly the conditions on bound-linear schemata
makes the satisﬁability problem undecidable. Thus this class can be seen as “canonical”, with a good trade-oﬀ between expressivity, simplicity of the deﬁnition and
decidability.
• Finally, Section 7 summarizes the results and provides some lines of future work.

2. Schemata of Propositional Formulae
In this section, we introduce the syntax and semantics of propositional schemata.
2.1 Syntax
The set of linear arithmetic expressions (denoted by N ) is built as usual on the signature
0, s, +, − and on a ﬁxed and countably inﬁnite set of arithmetic variables V, quotiented
by the usual properties of the arithmetic symbols (e.g., n + s(0) + n + s(s(s(0))) and
n + n + s(s(s(s(0)))) are assumed to be equivalent). As usual, sκ (0) is denoted by κ and
i + . . . + i (κ times) is κ.i. If n is an arithmetic variable we denote by N×n the set of
arithmetic expressions of the form α.n + β where α, β ∈ Z (with possibly α = 0) and by
Nn the set of expressions of the form n + β where β ∈ Z. Obviously Nn ⊂ N×n ⊂ N . If
n + α, n + β ∈ Nn we write n + α ≤ n + β iﬀ α ≤ β.
1. If one of these two conditions does not hold then the satisﬁability problem is trivially undecidable.
For instance, the Post correspondence problem can be easily encoded into schemata with non monadic
variables (Aravantinos, Caferra, & Peltier, 2009b). Similarly, if non linear arithmetic expressions are
considered then the 10th Hilbert’s problem can be encoded.

601

Aravantinos, Caferra & Peltier

For the sake of readability, we adopt the following conventions. Integers are denoted
by Greek letters α, β, γ, δ 2 , natural numbers by κ or ι, arithmetic variables by i, j, k, n,
propositional variables by p, q, r (with indices). Arithmetic expressions are denoted
∨ by
a,
b,
c,
d.
Schemata
are
denoted
by
ϕ,
ψ.
Π
and
Γ
denote
generic
iteration
connectives
or
∧
.
Definition 2.1 (Indexed propositions)
Let P be a ﬁxed and countably inﬁnite set of propositional symbols. An indexed proposition
is an expression of the form pa where p ∈ P and a is a linear arithmetic expression (the
index ). An indexed proposition pa s.t. a ∈ Z is called a propositional variable. A literal is
an indexed proposition or its negation.
In contrast to our previous work (Aravantinos et al., 2009b) we only consider monadic
propositions, i.e. every proposition has only one index.
Definition 2.2 (Schemata)
The set of formula schemata is the smallest set satisfying the following properties.
• ⊤, ⊥ are formula schemata.
• If a, b are integer expressions then a < b is a formula schema.
• Each indexed proposition is a formula schema.
• If ϕ1 , ϕ2 are schemata then ϕ1 ∨ ϕ2 , ϕ1 ∧ ϕ2 and ¬ϕ1 are formula schemata.
• If ϕ is a formula
∨b containing <, and if a, b ∈ N , and i is an arithmetic
∧b schema not
variable, then i=a ϕ and i=a ϕ are formula schemata.
Notice that, by deﬁnition, every schema must∧be ﬁnite. Schemata
of the form a < b, pa
∨
or ⊤, ⊥ are called atoms. Schemata of the form bi=a ϕ and bi=a ϕ are called iterations, a
and b are the bounds of the iteration and b − a is its length (notice that b − a may contain
variables). A schema is an arithmetic formula iﬀ it contains no iteration and if every atom
occurring in it is of the form ⊤, ⊥ or a < b. In particular, every boolean combination of
arithmetic atoms is a schema. a ≤ b (or b ≥ a) and a = b are used as abbreviations for
¬(b < a) and ¬(b < a) ∧ ¬(a < b) respectively. As for arithmetic expressions, arithmetic
formulae are taken up to arithmetic equivalence, e.g., n = 1 and n < 2∧n > 0 are considered
identical. The usual priority rules apply to disambiguate the reading of formula schemata.
Analogously
to ﬁrst-order logic quantiﬁers, the
∧n iteration operators have the highest priority
∧n
(e.g., i=1 pi ∨ pn ∧ ¬p1 should be read as ( i=1 pi ) ∨ (pn ∧ ¬p1 )).
Example 2.3

ϕ = q1 ∧

n
∧
i=1


pi+2n ∧



2n+1
∨

(¬qn−j ∨ qj+1 ) ∧ n ≥ 0

is a formula schema.

j=n

)
∧n (
∨2n+1
q1 , pi , qj and qj+1 are indexed propositions.
p
∧
(¬q
∨
q
)
and
i+2n
n−j
j+1
i=1
j=n
∨2n+1
j=n (¬qn−j ∨ qj+1 ) are the only iterations occurring in S.
2. This slightly unusual convention is used to avoid confusion between arithmetic variables and integers.

602

Decidability and Undecidability Results for Propositional Schemata

Remark 2.4
Notice that the
∨narithmetic atoms of the form∨an < b can only occur
∨noutside the iterations,
i.e. n ≥ 1 ⇒ i=1 pi is allowed, but neither i=1 (i ≤ 3 ∨ pi ) nor i=1 (n ≥ 1 ⇒ pi ). This
restriction is only used to simplify technicalities. As we shall see in Deﬁnition 2.5 ∨
(semantics
of schemata), an arithmetic atom of the form a < b is equivalent to the schema bi=a+1 ⊤.

∨ ∧
A variable i is bound in ϕ if ϕ contains an iteration of the form Πbi=a ψ (Π ∈ { , }),
it is free (or is a parameter of ϕ) if it has an occurrence in ϕ which is not in the scope of
an iteration Πbi=a ψ. From now on, we assume that∨no variable is simultaneously free and
bound in a schema ϕ (thus schemata∨such
as pn ∧ 10
n=1 ¬pn are not well-formed) and that
∧
b
d
′
if Πi=a ψ and Γj=c ψ (where Π, Γ ∈ { , }) are two distinct iterations occurring in ϕ then
i and j are distinct.
A substitution is a function mapping every arithmetic variable to a linear arithmetic
expression. We write [a1 /i1 , . . . , aκ /iκ ] for the substitution mapping respectively i1 , . . . , iκ
to a1 , . . . , aκ . The application of a substitution σ to a schema (or arithmetic expression) ϕ
is deﬁned as usual and denoted by ϕσ. Notice that if a is an arithmetic expression and σ
a substitution mapping every variable in a to a ground term (i.e. a term with no variable)
then aσ is an integer (since we identify, e.g., 2 − 1 and 1).
The previous notation is also used to denote the replacement of subexpressions: If ϕ is
a schema, ψ is an expression (schema or arithmetic expression) occurring in ϕ and ψ ′ is an
expression of the same type as ψ, then ϕ[ψ ′ /ψ] denotes the formula obtained by replacing
all the occurrences of ψ in ϕ by ψ ′ .

2.2 Semantics
An interpretation of the schemata language is a function mapping every integer variable to
an integer and every propositional variable to a truth value T or F. If I is an interpretation
and σ a substitution, we denote by Iσ the interpretation deﬁned as follows: Iσ and I
def
coincide on every propositional variable and for every variable n, Iσ(n) = I(nσ). Consider
for instance the following interpretation I:

n 7→ 5
m 7→ 2
p1 7→ T
p2 7→ F
p3 7→ F
p4 7→ F
603

Aravantinos, Caferra & Peltier

and whose deﬁnition is unsigniﬁcant for other (integer or propositional) variables. Let also
be σ the substitution {n 7→ n − 1, m 7→ m − 2}. Then Iσ is:
n 7→ 4
m 7→ 0
p1 7→ T
p2 7→ F
p3 7→ F
p4 7→ F
If I is an interpretation, we denote by σI the restriction of I to V, i.e. the substitution
mapping every variable n to I(n). If a is an arithmetic expression, we denote by JaKI the
expression aσI . Since aσI is ground, it is (equivalent to) an integer.
Definition 2.5 (Semantics)
The truth value JϕKI of a propositional schema in an interpretation I is inductively deﬁned
as:
• J⊤KI = T, J⊥KI = F
• Ja < bKI = T iﬀ JaKI < JbKI .
• Jpa KI = I(pJaKI ) for p ∈ P.
• J¬ΦKI = T iﬀ JΦKI = F.
• JΦ ∨ Φ′ KI = T iﬀ JΦKI = T or JΦ′ KI = T.
• JΦ ∧ Φ′ KI = T iﬀ JΦKI = T and JΦ′ KI = T.
∨
• J bi=a ϕKI = T iﬀ there is an integer α s.t. JaKI ≤ α ≤ JbKI and JϕKI[α/i] = T.
• J

∧b

i=a ϕKI

= T iﬀ for every integer α s.t. JaKI ≤ α ≤ JbKI : JϕKI[α/i] = T.

A schema ϕ is satisﬁable iﬀ there is an interpretation I s.t. JϕKI = T. I is called a model of
ϕ (written I |= ϕ). Two schemata ϕ, ψ are equivalent (written ϕ ≡ ψ) iﬀ I |= ϕ ⇔ I |= ψ.
ϕ and ψ are sat-equivalent (written ϕ ≡S ψ) iﬀ ϕ and ψ are both satisﬁable or both
unsatisﬁable.
In the following, we assume that for every free variable n in ϕ and for every model I of
ϕ, I(n) ∈ N. This can be ensured by explicitly adding the arithmetic atom n ≥ 0 to ϕ3 .
Let S be the following system of rewrite rules:
3. Thus we assume that parameters are mapped to natural numbers. This convention is convenient because
it allows one to use mathematical induction on the parameters (see Section 5.2). It is not restrictive since
a schema ϕ where n ∈ Z could be replaced by the (equivalent) disjunction of the schemata ϕ ∧ n ≥ 0
and ϕ[−m/n] ∧ m ≥ 0 (i.e. in the case in which n is negative, every occurrence of n is simply replaced
by −m).

604

Decidability and Undecidability Results for Propositional Schemata

 ∨β


i=α ϕ

 ∧β
ϕ
∨βi=α
S=


i=α ϕ

 ∧β
i=α ϕ

→
→
→
→

⊥
⊤
∨
( β−1
i=α ϕ) ∨ ϕ[β/i]
∧β−1
( i=α ϕ) ∧ ϕ[β/i]

if
if
if
if

α, β
α, β
α, β
α, β

∈ Z, β
∈ Z, β
∈ Z, β
∈ Z, β

<α
<α
≥α
≥α

For instance the following formula:
¬p1 ∧

3
∧

(pi ⇒ pi+1 )

i=1

is rewritten into:
¬p1 ∧ (p1 ⇒ p2 ) ∧ (p2 ⇒ p3 ) ∧ (p3 ⇒ p4 )
∧
Notice that no rule of S applies on ¬p1 ∧ ni=1 (pi ⇒ pi+1 ) as the upper bound of the iteration
contains a parameter. S is actually designed to be used only on schemas whose parameters
have been instantiated by a number.
Proposition 2.6
S is convergent and preserves equivalence.
Proof
Termination is immediate since the length of an iteration strictly decreases at each step.
Conﬂuence is obvious since the critical pairs are trivially joinable. The fact that the obtained
schema is equivalent to the original one is a straightforward consequence of Deﬁnition 2.5. 
We denote by ϕ↓S the (unique) normal form of ϕ. If σ is a substitution mapping every
free variable in ϕ to a natural number, ϕσ↓S is called a propositional realization of ϕ.
It is trivially semi-decidable to know if a schema is satisﬁable:
Proposition 2.7
The set of satisﬁable schemata is recursively enumerable.
Proof
By Deﬁnition 2.5, for every interpretation I and for every schema ϕ, we have (I |= ϕ) ⇔
(I |= ϕσ), where σ = σI . Thus ϕ is satisﬁable iﬀ there exists a substitution σ such that
ϕσ is satisﬁable. We now prove that there exists an algorithm for checking the satisﬁability
of ϕσ. By Proposition 2.6, we have ϕσ ≡ ϕσ ↓S . By deﬁnition of σ, ϕσ contains no free
variable. Let Πbi=a ϕ be an outermost iteration in ϕ. By deﬁnition a and b must be ground,
thus one of the rules in S applies which is impossible. Thus ϕσ ↓S contains no iteration
hence ϕσ↓S is a propositional formula (in the usual sense) built on the set of propositional
variables. Consequently, there exists an algorithm to check whether the formula ϕσ↓S ≡ ϕσ
is satisﬁable or not. Since the set of ground substitutions is recursively enumerable, and
since ϕ is satisﬁable iﬀ ϕσ is satisﬁable for at least one substitution σ, this implies that it
is semi-decidable to check whether ϕ is satisﬁable or not.

605

Aravantinos, Caferra & Peltier

For every schema ϕ and for every substitution σ we denote by [ϕ]σ the formula ϕσ↓S .
For every arithmetic expression a (possibly containing bound variables) in a schema ϕ,
we compute an interval [minϕ (a), maxϕ (a)] where minϕ (a), maxϕ (a) are arithmetic expressions only containing variables that are free in ϕ. The intuition is that a always “belongs”
to this interval. Lemma 2.8 formalizes this property.
• If a is an integer or a variable that is free in ϕ then minϕ (a) = maxϕ (a) = a.
def

def

• If a is of the form b + c then minϕ (a) = minϕ (b) + minϕ (c) and maxϕ (a) = maxϕ (b) +
maxϕ (c).
def

def

• If a is of the form −b then maxϕ (a) = − minϕ (b) and minϕ (a) = − maxϕ (b).
def

def

• If i is a bound variable, occurring in an iteration of the form Πbi=a ϕ then minϕ (i) =
def
minϕ (a) and maxϕ (i) = maxϕ (b).

def

A ground substitution σ ′ is a ϕ-expansion of another ground substitution σ for a subschema ψ in ϕ iﬀ for every variable i that is bound in ψ, σ ′ (i) ∈ [σ(minϕ (i)), σ(maxϕ (i))]
(since σ, σ ′ are ground, the expressions σ ′ (i), σ(minϕ (i)), σ(maxϕ (i)) are considered as integers). The intuition behind ϕ-expansions is the following: A substitution σ does not aﬀect
the bound variables of a schema; so the values given by σ to such bound variables are
unsigniﬁcant; on the contrary, the deﬁnition of a ϕ-expansion σ ′ imposes that:
1. the value given to a variable i bound in ϕ indeed falls in the set of values that i can
take in the context of ϕ ;
2. the value given by σ ′ to a variable free in ϕ is the same as the one given by σ.
W.r.t. substitution application, there is no diﬀerence between σ and σ ′ . The next lemma
shows the importance of ϕ-expansions.
Lemma 2.8
Let ϕ be a schema and let i be a variable (possibly bound) occurring in ϕ. The expressions
minϕ (i) and maxϕ (i) are well-deﬁned. Moreover, for every ground substitution σ and for
all atoms pα occurring in [ϕ]σ there exist an atom pa occurring in ϕ and a ϕ-expansion σ ′
of σ for pa s.t. σ ′ (a) = α.
Proof
This is an immediate consequence of Deﬁnition 2.5 (by a straightforward induction on the
depth of the schema).

We write IC (ϕ) (standing for “Interval Constraints”) for the conjunction of arithmetic
constraints of the form minϕ (i) ≤ i ∧ i ≤ maxϕ (i) where i is a variable that is bound in ϕ.
IC (ϕ) can be extended to sets
∧ of schemata by handling them as conjunctions.
Consider, e.g., ϕ = p0 ∧ n−1
i=1 (pi+1 ∧ ¬qi ). We have: minϕ (i) = 1 and maxϕ (i) = n − 1.
Consider furthermore σ = {n 7→ 4} and pα = p3 . Then we can take pa = pi+1 (which indeed
occurs in ϕ) and σ ′ = {n 7→ 4, i 7→ 2}.
We see informally the use of ϕ-expansions: they allow, in some sense, to make the
connection between a propositional variable occurring in the instance of a schema and the
indexed proposition where it “comes from”.
606

Decidability and Undecidability Results for Propositional Schemata

2.3 The Class of Bound-Linear Schemata
As we shall see (in, e.g., Theorem 6.2) the satisﬁability problem is undecidable for schemata.
In order to characterize a decidable subclass, we introduce the following deﬁnition:
Definition 2.9
A schema ϕ is bound-linear iﬀ the following conditions hold:
1. ϕ contains at most one free arithmetic variable n (called the parameter of ϕ).
2. Every non arithmetic atom in ϕ is of the form pα.n+β.i+γ where p ∈ P and i is a bound
variable, α, γ ∈ Z and β ∈ {−1, 0, 1}.
∨ ∧
3. If Πbi=a ψ is an iteration in ϕ (where Π ∈ { , }) then a, b are respectively of the form
α.n + β and γ.n + δ + ϵ.j where α, β, γ, δ ∈ Z, ϵ ∈ {−1, 0, 1} and j is a bound variable.
This class is comprehensive enough with respect to decidable satisﬁability. The key
point is that all the indices and iteration bounds contain at most one bound variable.
Furthermore, the coeﬃcient of this variable must be 1 (or 0).
2.4 Expressiveness of Bound-Linear Schemata
In order to show evidence that the class of bound-linear schemata is not an artiﬁcial or too
narrow one, we provide in this section some examples of problems that can be naturally
encoded into bound-linear schemata.
It is easy to check that the schema Adder(p, q, c, r) deﬁned in the Introduction (formalizing a sequential adder) is bound-linear. Various properties of this circuit can be encoded.
For instance, the following schema checks that 0 is a (left) neutral element:
(Adder(p, q, c, r) ∧

n
∧

n
∧

¬pi ) ⇒

i=1

(ri ⇔ qi )

i=1

The schema below checks that the adder is a function i.e. that the sum of two operands is
unique.
n
∧
(Adder(p, q, c, r) ∧ Adder(p, q, c′ , r′ )) ⇒
(ri ⇔ ri′ )
i=1

The next one checks that it is commutative:
(Adder(p, q, c, r) ∧ Adder(q, p, c′ , r′ )) ⇒

n
∧

(ri ⇔ ri′ )

i=1

Many similar circuits can be formalized in a similar way, such as a carry look-ahead
adder (a faster version of the n-bit adder that reduces the amount of time required to
compute carry bits):

def

CLA-Adder(p, q, c) =

n
∧

(ri ⇔ ((pi ⊕ qi ) ⊕ ci )) ∧

i=1

n
∧

(ci+1 ⇔ (pi ∧ qi ) ∨ (ci ∧ (pi ∨ qi )))

i=1

607

Aravantinos, Caferra & Peltier

The equivalence of the two deﬁnitions is encoded as follows:
′

′

(Adder(p, q, c, r) ∧ CLA-Adder(p, q, c , r )) ⇒

n
∧

(ri ⇔ ri′ )

i=1

Comparison between two natural numbers can easily be formalized, e.g. rn holds iﬀ p ≥ q:
r0 ∧

n
∧

(ri ⇔ (ri−1 ∧ (pi ⇔ qi ) ∨ pi ∧ ¬qi ))

i=1

By composing the previous schemata, any (quantiﬁer-free) formula of Presburger arithmetic
can be encoded.
More generally, one can formalize every circuit composed by serially putting together n
layers of the same basic circuit. These circuits are usually deﬁned inductively, which can
be easily encoded into our formalism with a formula of the form:
(p0 ⇔ ϕbase ) ∧

n−1
∧

(pi+1 ⇔ ϕind ),

i=0

where ϕbase and ϕind are the formulae corresponding to the base case and inductive case,
respectively. ϕind contains some occurrences of pi and encodes the basic circuit to be
composed in sequence. Of course, for most complex circuits, pi may be replaced by a vector
of bits pi , qi , ri deﬁned inductively from the pi−1 , qi−1 , ri−1 ,. . . . Such inductively-deﬁned
circuits appear very frequently in practice (Gupta & Fisher, 1993).
If the index of the proposition denotes the time, then various ﬁnite state sequential
systems can be encoded. The state of the system is described by a set of propositional
variables, and pi encodes the value of p at step i. The parameter n denotes the number of
steps in the transformation (which is assumed to be ﬁnite but unbounded). The transition
function from state i to i + 1 can easily be formalized by a bound-linear schema. For
instance, the inclusion of two automata can be encoded (the parameter being the length
of the run). We provide another example. Consider a register with three cells p, q, r and
assume that there are two possible actions rl and rr that rotate the values of the cells to the
left and to the right respectively. The behavior of this system is modeled by the following
schema (the propositions rli and rri indicate which action is applied at step i). First L(i)
expresses the state of the registers at time i depending on their state at time i − 1, when
rli has been applied to it:
L(i) ≡ rli ⇒ ((pi ⇔ qi−1 ) ∧ (qi ⇔ ri−1 ) ∧ (ri ⇔ pi−1 ))
Then R(i) is similar for rr:
R(i) ≡ rri ⇒ ((pi ⇔ ri−1 ) ∧ (qi ⇔ pi−1 ) ∧ (ri ⇔ qi−1 ))
Finally, we state that this holds at any time:
ϕn ≡

n
∧

L(i) ∧

i=1

n
∧
i=1

608

R(i)

Decidability and Undecidability Results for Propositional Schemata

We can then express properties on such registers. For instance, the following formula states
that n rotations to the right followed by n rotations to the left are equivalent to identity:
(ϕ2n ∧

n
∧
i=1

rri ∧

2n
∧

rli ) ⇒ (p0 ⇔ p2n ) ∧ (q0 ⇔ q2n ) ∧ (r0 ⇔ r2n )

i=n+1

3. Related Work
Diﬀerent forms of schemata have been used by several authors, either in propositional logic
(Baaz & Zach, 1994) or in ﬁrst-order logic to obtain results in proof theory, in particular related to the number of proof lines (Parikh, 1973; Baaz, 1999; Krajicek & Pudlak,
1988; Orevkov, 1991). Parikh (1973) presents a notion of schematic systems, Baaz (1999)
uses the concept of uniﬁcation, Krajicek and Pudlak (1988) introduce the notion of ‘proof
skeleton’, very similar to that of schema, Orevkov (1991) studies schemata in ﬁrst-order
Hilbert-type system. Pragmatically, schemata have been successfully used, e.g., in solving
open questions in equivalential calculus (i.e. the ﬁeld of formal logic concerned with the
notion of equivalence) with the theorem-prover Otter (Wos, Overbeek, Lush, & Boyle,
1992). However, to the best of our knowledge, the formal handling of such schemata at the
object level has never been considered. Although the notion of ‘schema’ is recognized as
an important one, it deserves more applied works in our opinion. Sometimes schemata are
not suﬃciently emphasized, e.g., in the work of Barendregt and Wiedijk (2005) a nice and
deep analysis about the challenge of computer mathematics is given. The authors overview
the state of the art (by describing and comparing most powerful existing systems in use)
but structuring proofs is not explicitly mentioned (maybe this feature can be included in
what they call “mathematical style” or “support reasoning with gaps”). In our approach to
schemata it is clear that they are a way of structuring proofs and can also help to overcome
one of the obstacles to the automation of reasoning pointed out by Wos (1988), i.e. the size
of deduction steps.
There exist term languages expressive enough to denote iteration schemata as those
introduced in Deﬁnition 2.2: In particular, term schematisation languages can be used
to denote inﬁnite sequences of structurally similar terms or formulae. For instance the
ˆ
ˆ
ˆ
primal grammar
∨n (Hermann & Galbavý, 1997) f (n) → (p(n) ∨ f (n − 1)), f (0) → ⊥ denotes
the iteration i=1 pi . It is worth mentioning that this iteration cannot be denoted by other
term schematisation languages (Chen, Hsiang, & Kong, 1990; Comon, 1995) because the
inductive context is not constant. However, term schematisation languages do not allow to
reason on such iterations (they are only useful to represent them).
Encoding schemata into first-order logic is a very natural idea, interpreting iterated
connectives as bounded quantiﬁers. Additional axioms
can be∧added to express arithmetic
∨
properties if needed. For instance the schema ( ni=1 pi ) ∧ ( ni=1 ¬pi ) can be encoded by
∃i.(1 ≤ i ∧ i ≤ n ∧ p(i)) ∧ ∀i.(1 ≤ i ∧ i ≤ n ⇒ ¬p(i)) which is obviously unsatisﬁable.
However, since inductive domains cannot be deﬁned in ﬁrst-order logic, such a translation
necessarily introduces some unintended interpretations hence does not yield a complete
procedure (satisﬁability is not always preserved, although the unsatisﬁability of the obtained
formula
∧ necessarily entails the unsatisﬁability of the original one). For instance, the schema
p0 ∧ ni=1 (pi−1 ⇒ pi )∧¬pn is translated into p(0)∧∀i.(1 ≤ i∧i ≤ n∧p(i−1) ⇒ p(i)∧¬p(n)),
609

Aravantinos, Caferra & Peltier

which is actually satisﬁable (we do not know that n ∈ N and there is no way to express
this property). In order to obtain an unsatisﬁable formula, some inductive axioms must
be added to allow (necessarily restricted) applications of the induction principle. In this
particular case, the proof can be obtained by a simple induction on i using the inductive
lemma ∀i.(i ≤ n ⇒ p(i)), thus we could add the axiom: [q(0)∧∀i.(q(i) ⇒ q(i+1))] ⇒ ∀i.q(i)
where q(i) ≡ i ≤ n ⇒ p(i). With this axiom, it is easy to check that the previous formula
becomes unsatisﬁable. However, in the general case it is hard to determine a priori the right
axiom (if there is one). Actually the termination proof in Section 5 implicitly provides a way
to determine candidate axioms (for the particular class of regular schemata): every looping
node in the tableaux constructed by the proof procedure stab (see Section 5) corresponds
to an application of the induction principle, hence to an induction axiom. The termination
proof precisely shows that the size of these inductive lemmata is bounded, thus the whole
set of potential induction axioms could be in principle computed and added to the formula
before the beginning of the search. But the practical interest of this transformation is
obviously highly questionable.
Several procedures have been designed for proving inductive theorems, (Boyer &
Moore, 1979; Bouhoula, Kounalis, & Rusinowitch, 1992; Comon, 2001; Bundy, van Harmelen, Horn, & Smaill, 1990; Bundy, 2001). Since schemata can be seen as an “explicit way”
of handling mathematical induction, using such proof procedures for proving them is a very
natural idea. In general, induction is used to deﬁne terms (e.g., recursive functions operating on inductive data structures), whereas in our case the formulae themselves are deﬁned
inductively. Obviously this problem could be solved by using an appropriate encoding of
the formulae. However there are very few decidability results in inductive theorem proving
and known classes (Giesl & Kapur, 2001) are not expressive enough to encode propositional
schemata. Notice that most systems concentrate on universal quantiﬁcations, where we have
to handle both iterated conjunctions (which can be interpreted as universal quantiﬁcation
on a ﬁnite domain) and iterated disjunctions (i.e. the analogous of existential quantiﬁcations). Adding existential quantiﬁcation in inductive theorem proving is known to be a
diﬃcult problem. Most inductive theorem provers are designed to prove universal theorems
of the form ∀⃗x.ψ where ψ is a quantiﬁer-free formula (usually a clause) and the variables in
⃗x range over the set of (ﬁnite) terms. In our context, ψ would contain ﬁnite quantiﬁcation
(over intervals constrained by n), corresponding to the iterated connectives. In particular,
schemata may have several models, thus implicit induction (Comon, 2001) (which explicitly
requires that the underlying Herbrand model is unique) cannot be (directly) used.
Of course, these problems can be overcome by encoding interpretations as terms (for
instance by vectors or ordered lists of truth values) and schemata as functions mapping
every interpretation to a truth value. Then inductive theorem provers may be used to prove
inductive properties of these functions (showing for instance that their value is ⊥ for every
interpretation). However these provers are not complete (due to well-known theoretical
limitations) thus the practical interest of this encoding is unclear. For instance, we have
tried to use the theorem prover acl2 to prove the validity of some of the benchmarks
considered in Section 5, but it fails on all non trivial examples. We conjecture that this
is not only due to eﬃciency problems, but that additional inductive lemmata are needed,
which are very hard to determine in advance.
610

Decidability and Undecidability Results for Propositional Schemata

The above deﬁnitions should also remind the reader of fixed point logics. ∧
Indeed
iterated schemata are obviously particular cases of ﬁxed points, e.g., the schema ni=1 pi
might be represented as (µX(i).i ≤ 0∨(p(i)∧X(i−1)))(n). The “standard” ﬁxed point logic
is the (propositional) modal µ-calculus (Bradﬁeld & Stirling, 2007) in which many temporal
logics can be encoded, e.g., LTL or CTL. However the involved logic is very diﬀerent from
ours and actually simpler from a theoretical point of view. Indeed modal µ-calculus is
decidable (and thus complete) whereas – as we shall see in Section 6 – iterated schemata
are not (nor are they complete). Furthermore, our language allows one to use complex
(though carefully restricted) arithmetic operations in the deﬁnition of the iterations, both
in the indices and in the bounds. For instance we may relate the truth values of two
propositions whose index are arbitrary far from each other (such as pi and pn−i ). As far as
we are aware, these operations cannot be directly encoded into propositional µ-calculus.
Actually iterated schemata share much more with least fixpoint logic, or LFP (Immerman, 1982), studied in ﬁnite model theory (Fagin, 1993; Ebbinghaus & Flum, 1999):
LFP is a logic allowing to iterate ﬁrst-order formulae maintaining constant the number of
their variables. However we do no know of any calculus for deciding the satisﬁability in
LFP. We see two reasons for this: ﬁrst, LFP is undecidable and not complete, second the
purposes of this logic are mainly theoretical, hence the fact that research in this ﬁeld has
not focussed on decision procedures for some subclasses. In contrast with propositional µcalculus, first-order µ-calculus (Park, 1976) clearly
embeds iterated schemata (allowing
∧
for instance the above ﬁxed-point expression of ni=1 pi ), but no published research seems
to be focused on the identiﬁcation of complete subclasses. With a similar expressive power
one also ﬁnds logics with inductive deﬁnitions (Aczel, 1977) which are quite widespread in
proof assistants (Paulin-Mohring, 1993), but again out of the range of automated theorem
provers. As far as we know the only study of a complete subclass in such ﬁxed point logics
is in the work of Baelde (2009), and iterated schemata deﬁnitely do not lie in this class nor
can be reduced to it.
As we shall see in Section 5.2, completeness of bound-linear schemata (or more precisely
regular schemata) lies in the detection of cycles during the proof search. This idea is not
new, it is used, e.g., in tableaux methods dealing with modal logics in transitive frames
(Goré, 1999), or µ-calculi (Cleaveland, 1990; Bradﬁeld & Stirling, 1992). However cycle
detection in our work is quite diﬀerent because we use it to prove by induction. Notice in
particular that we cannot in general ensure termination (contrarily to the above methods).
It is more relevant to consider our method as a particular instance of cyclic proofs, which
are studied in proof theory precisely in the context of proofs by induction. In the works
of Brotherston (2005), and Sprenger and Dam (2003), it is shown that cyclic proofs seem
as powerful as systems dealing classically with induction. A particular advantage of cyclic
proofs is that ﬁnding an invariant is not needed, making them particularly suited to automation. However, once again those studies are essentially theoretical and there are no
completeness results for particular subclasses.
To summarize, known decidable logics (such as propositional µ-calculus) or even semidecidable ones such as ﬁrst-order logic are not expressive enough to directly embed iterated
schemata, whereas those that are suﬃciently expressive (such as ﬁxpoint or higher order
logics) are not suitable for automation. Together with the potential applications mentioned
in Section 2.4, this justiﬁes to our opinion the interest of the considered language.
611

Aravantinos, Caferra & Peltier

4. Reduction to Regular Schemata
In this section we reduce the satisﬁability problem for bound-linear schemata (see Deﬁnition
2.9) to a much simpler class of schemata, called regular. This class is deﬁned as follows:
Definition 4.1
A schema ϕ is:
• ﬂat if for every iteration Πbi=a ψ occurring in ϕ, ψ does not contain any iteration (i.e.
iterations cannot be nested in ϕ).
• of bounded propagation if every atom that occurs in an iteration Πbi=a ψ in ϕ is of the
form pi+γ for some γ ∈ Z. Since the number of atoms is ﬁnite, there exist α, β ∈ Z
s.t. for every atom pi+γ occurring in an iteration we have γ ∈ [α, β]. α, β are called
the propagation limits.
• aligned on [c, d] if all iterations occurring in ϕ are of the form Πdi=c ψ (i.e. all iterations
must have the same bounds).
• regular if it has a unique parameter n and if it is ﬂat, of bounded propagation and
aligned on [α, n − β] for some α, β ∈ Z.
As an example, the schema Adder deﬁned in the Introduction is regular, but the last
example in Section 2.4 (three cells register with shift) is not. Obviously, every regular schema
is also bound-linear (see Deﬁnition 2.9). We now deﬁne an algorithm that transforms every
bound-linear schema into a sat-equivalent regular one. This result is somewhat surprising
because the class of regular schemata seems much simpler than bound-linear schemata. In
some sense, it points at regular schemata as a canonical decidable class of schemata.
4.1 Overview of the Transformation Algorithm
We ﬁrst give an informal overview of the algorithm reducing every bound-linear schema
into a regular one, together with examples illustrating each transformation steps. This very
high level description is intended to help the reader to grasp the intuitive ideas behind
the formal deﬁnitions and more technical explanations provided in the next section. The
transformation is divided into several steps.
• The ﬁrst step is the elimination of iterations
∨n occurring
∧n inside an iteration. Consider
for instance the following schema ϕ : i=1 (pi ⇒ j=1 qj ). The reader can check
that ϕ is bound-linear∧but non regular. It is easy to transform ϕ into a sat-equivalent
regular schema: since nj=1 qj does not depend on the counter i, one can simply ∧
replace
n
this formula by a new propositional variable r ∨
and add the equivalence
r
⇔
j=1 qj
∧n
n
outside the iteration. This yields the schema: i=1 (pi ⇒ r) ∧ (r ⇔ j=1 qj ), which
is clearly regular and sat-equivalent (but not equivalent) to ϕ. This process can be
generalized; however, replacing an iteration by a proposition is only possible if the
iteration contains
∨n no
∧nvariable that is bound
∧n in the original schema. Consider the
′
schema: ϕ : i=1 j=1 (pi ⇒ qj ). Here j=1 (pi ⇒ qj ) cannot be replaced by a
variable r, since it depends on i. The solution is to get the variable pi containing i
612

Decidability and Undecidability Results for Propositional Schemata

∧
out of the∧iteration nj=1 (pi ⇒ qj ): ∧
as pi does not involve j, it is easily seen that we
can turn nj=1 (pi ⇒ qj ) into pi ⇒ nj=1 qj . This transformation can be generalized
by using case-splitting: indeed, it is well-known that every formula ψ is equivalent
to (r ∧ ψ[⊤/r]) ∨ (¬r ∧ ψ[⊥/r]),
for every propositional variable
r. Applying this
∧n
∧n
decomposition
scheme to∧ j=1 (pi ⇒ qj ) and pi we get:
) ≡ (pi ∧
j=1 (pi ⇒ qj∧
∧n
n
n
(⊤
⇒
q
))
∨
(¬p
∧
(⊥
⇒
q
)),
i.e.
(by
usual
transformations):
j
i
j
j=1
j=1
j=1 (pi ⇒
∧n
∧n
qj ) ≡ (pi ∧ j=1 qj )∨¬pi . Afterwards, the remaining iteration j=1 qj can be replaced
by a new variable r.
The decomposition scheme just explained can be applied on every variable occurring
in an iteration, but not containing the counter of this iteration. By deﬁnition of
bound-linear schemata, the propositional symbols have only one index and this index contains at most one bound variable, thus this technique actually removes every
atom containing a counter variable distinct from the one of the considered iteration.
However, it does not remove the variables that occur
in∧the bound of the iteration.
def ∨
Consider for instance the following formula: ϕ′′ = ni=1 ij=1 qj . Here i occurs in the
bound of the iteration and thus cannot
∧i be removed by the previous technique. The
idea is then to encode the formula j=1 qj by a new variable ri , that can be deﬁned
inductively
as follows: r0 is ⊤ and ri+1 is ri ∧ qi+1 . This is expressed by the schema:
∧
(r
r0 ∧ n−1
i=0 i+1 ⇔ (ri ∧ qi+1 )).
Notice that ri needs only to be deﬁned for i = 0, . . . , n because i ranges over the
interval [1, n] in ϕ′′ .
• In order to get a regular schema one has to guarantee that every iteration ranges over
the same interval of the form [α, n − β] (where β ∈ Z). This is actually
simple to
∨2n
ensure by unfolding ∨
and shifting
the
iterations.
For
instance
a
schema
i=1 pi can
∨2n
∨n
∨n
n
be transformed into i=1 pi ∨ i=n+1 pi and then into i=1 pi ∨ i=1 pi+n . Similarly
∨n−1
∨n−1
∨n−1
∨n
i=2 pi ∨pn ∨q1 ∨ j=2 qj to get iterations deﬁned
i=2 pi ∨ j=1 qj can be reduced to
on the same interval.
• A major diﬀerence between regular schemata and bound-linear ones is that, in a
regular schema, the indexed variables∨occurring inside an iteration cannot contain
parameters (e.g., an iteration such as ni=1 pi+n is forbidden). Therefore we have to
replace every variable of the form pα.n+β±i by a new variable qi , depending only on i.
The problem is that in order to preserve sat-equivalence, one also has to encode the
relation between these variables. For instance, assume that pn+i is replaced by qi and
that p2n−j is replaced by rj . Then obviously, we must have qi ≡ rj if n+i = 2n−j, i.e.
qi ≡ rn−i . This step may be problematic because in general there are inﬁnitely many
such axioms. However, by deﬁning the translation carefully, we will show that actually
only ﬁnitely many equivalences are required. To this aim, we have to assume that the
initial coeﬃcient of the parameter is even in every index (see Deﬁnition 4.2), which is
easy to ensure by case splitting. Then the maximal number of overlaps between the
newly deﬁned variable is actually bounded (this is shown by the crucial lemma 4.6).
∨
∨
For instance, a formula ni=0 (¬pi ∨ p2n−i ) is replaced by ni=0 (¬pi ∨ qi ) ∧ (pn ⇔ qn ).
qi denotes the atom p2n−i and the equivalence encodes the fact that qn ≡ p2n−n = pn .
613

Aravantinos, Caferra & Peltier

Since i ranges over the interval [0..n] this is the only equation which is relevant w.r.t.
ϕ (e.g. p0 ⇔ q2n is useless).
The algorithm for transforming every bound-linear schema ϕ into a sat-equivalent regular schema ψ is speciﬁed as a sequence of rewriting rules, operating on schemata and
preserving sat-equivalence. The rules are depicted in Figure 1. They must be applied in the
order of their presentation. As we shall see in Section 4.3, the rewrite system terminates (in
exponential time). Moreover satisﬁability is preserved and irreducible schemata are regular
(see Section 4.4).
4.2 Formal Definition of the Algorithm
We now give a more detailed and precise description of the transformation algorithm (readers not interested in technical details can skip this section). We assume that the initial
schema satisﬁes the following condition:
Definition 4.2
A bound-linear schema is normalized if the coeﬃcient of the parameter n is even in any
expression occurring in the formula (either as the index of a symbol in P or as the bound
of an iteration).
Considering exclusively normalized schemata is not restrictive because a schema ϕ not
satisfying this property can be replaced by ϕ[2n/n] ∨ ϕ[2n + 1/n] (e.g. p3n is turned into
p6n ∨ p6n+3 ). The obtained schema is obviously sat-equivalent to ϕ and normalized4 . The
use of normalized schemata will be explained later (see Remark 4.7).
Remark 4.3
The property of being normalized is only useful for the algorithm of Figure 1 to be welldeﬁned. But the schema obtained after application of this algorithm is actually not normalized in general.
We now explain in more details the diﬀerent steps of the transformation.
4.2.1 Elimination of Nested Iterations
As explained in Section 4.1, the ﬁrst step is to remove the iterations Πbi=a ϕ occurring inside
another iteration Γdj=c ψ. This is done by the rules τ1 , τ2 , τ3 , τ4 . τ2 moves Πbi=a ϕ out by
introducing a new variable p as explained before. This is possible only if ϕ does not contain
any free variable except i and the parameter n. Removing all other variables is precisely
the role of τ1 :
τ1

Πbi=a ϕ → (pc ∧ Πbi=a ϕ[⊤/pc ]) ∨ (¬pc ∧ Πbi=a ϕ[⊥/pc ])
If the variables in c are free in Πbi=a ϕ, pc occurs in ϕ
and if for every iteration Γdj=c ϕ′ containing Πbi=a ϕ, pc contains either j or a
variable bound in Γdj=c ϕ′ .

4. But the two formulae are not equivalent in general. For instance, if ϕ = pn , then the interpretation
def
def
deﬁned by I(n) = 1 and I(pκ ) = T iﬀ κ = 1 validates pn but obviously not p2n ∨ p2n+1 .

614

Decidability and Undecidability Results for Propositional Schemata

τ1

τ2

τ3

τ3′

τ4

τ4′

τ5
τ6

τ7

τ8
τ9

τ10

Πbi=a ϕ
→ (pc ∧ Πbi=a ϕ[⊤/pc ]) ∨ (¬pc ∧ Πbi=a ϕ[⊥/pc ])
If the variables in c are free in Πbi=a ϕ, pc occurs in ϕ
and if for every iteration Γdj=c ϕ′ containing Πbi=a ϕ, pc contains either j or a
variable bound in Γdj=c ϕ′ .
ψ
→ (p ⇔ Πbi=a ϕ) ∧ ψ[p/Πbi=a ϕ]
If p is a fresh symbol, ψ is the global schema, Πbi=a ϕ occurs in an iteration in ψ
and contains no free variable except n.
∧a−b−1
ϕ
→
j=minϕ (j) ¬pj
∧maxϕ (j)
∨b+j
∧ j=a−b
(pj ⇔ (pj−1 ∨ ψ[b + j/i])) ∧ (ϕ[pj / i=a ψ])
∨b+j
If p is a fresh symbol, i=a ψ occurs in an iteration of ϕ, j is bound in ϕ,
a, b and ψ contain no free variable except n, ϕ is the global schema.
∧a−b−1
ϕ
→
j=minϕ (j) ¬pj
∧maxϕ (j)
∧b+j
∧ j=a−b
(pj ⇔ (pj−1 ∧ ψ[b + j/i])) ∧ (ϕ[pj / i=a ψ])
∧b+j
If p is a fresh symbol, i=a ψ occurs in an iteration of ϕ, j is bound in ϕ,
a, b and ψ contain no free variable except n, ϕ is the global schema.
∧maxϕ (j)
ϕ
→
j=b−a+1 ¬pj
∧b−a
∨b−j
∧ j=minϕ (j) (pj ⇔ (pj+1 ∨ ψ[b − j/i])) ∧ (ϕ[pj / i=a ψ])
∨b−j
If p is a fresh symbol, i=a ψ occurs in an iteration of ϕ, j is bound in ϕ,
a, b and ψ contain no free variable except n, ϕ is the global schema.
∧maxϕ (j)
ϕ
→
j=b−a+1 pj
∧b−a
∧b−j
∧ j=minϕ (j) (pj ⇔ (pj+1 ∧ ψ[)/b − j]) ∧ (ϕ[pj / i=a ψ])
∧b−j
If p is a fresh symbol, i=a ψ occurs in an iteration of ϕ, j is bound in ϕ,
a, b and ψ contain no free variable except n, ϕ is the global schema.
(γ−α).n−δ
Πγ.n−δ
→ Πi=β
ϕ[i + α.n/i]
i=α.n+β ϕ
If α ̸= 0, β ∈ Z.
ψ
→ [ψ]n7→0 ∨ . . . ∨ [ψ]n7→κ ∨ (n > κ ∧ ψ[⋄/Πα.n−β
ϕ])
∧i=γ
∨
If ψ contains Πα.n−β
ϕ,
with
α,
β,
γ
∈
Z,
α
<
0
and
Π
∈
{
,
},
i=γ
∨
∧
then ⋄ = ⊥ and if Π = then ⋄ = ⊤.
where κ = ⌈ γ−β
α ⌉ and Π is
ψ
→ ((α − 1).n − β ≥ γ ∧ ψ[ψ ′ /Πα.n−β
ϕ]) ∨ ([ψ]n7→0 ∨ . . . ∨ [ψ]n7→κ )
i=γ
α.n−β
where ψ contains an iteration Πi=γ ϕ with α > 1,
∧ ∨
(α−1).n−β
ψ ′ is Πi=γ
ϕ ⋆ Πni=1 ϕ[i + (α − 1).n − β/i], with Π ∈ { , },
∧
∨
then ⋆ = ∧, if Π = then ⋆ = ∨.
where κ = ⌊ γ−β
α−1 ⌋, Π =
Πn−β
→ Πn−γ
i=γ ϕ
i=β ϕ[n − i/i]
If the indices of the variables in ϕ are of the form (2α + 1).n + c, where c ∈ Ni .
∧
ϕ
→ ϕ ∧ ψ∈Ψ(ϕ) ψ
If ϕ contains a variable p not occurring in V − ∪ V + ,
and where Ψ(ϕ) is deﬁned by Deﬁnitions 4.4, 4.5 and Lemma 4.6.
Πn−β
→ (n < α + β ∧ ⋄) ∨
i=α ϕ
′
−α
(n ≥ α + β ∧ Πn−β−1+α
ϕ[i − α′ + α/i] ⋆ ϕ[n − β/i])
i=α′
′
where α is the maximal lower bound of an iteration occurring in the
′
whole formula
α ̸= α′ or β ̸= β ′ ,
∨ and β is the minimal upper bound,
∧
and if Π is then ⋄ = ⊥, ⋆ = ∨ and if Π = then ⋄ = ⊤, ⋆ = ∧.

Figure 1: Transformation Into Regular Schemata

615

Aravantinos, Caferra & Peltier

This rule aims at eliminating, in the body of an iteration Πbi=a ϕ, every variable distinct
from the iteration counter i and from the (unique) parameter n. This is feasible because no
index can contain two variables distinct from n (by deﬁnition of bound-linear schemata).
This implies that the indexed variables containing an arithmetic variable distinct from i
and n cannot contain i thus they can be taken out of the iteration Πbi=a ϕ by case splitting.
Notice that the rule τ1 can increase exponentially the size of the formula.
Once ϕ contains no free variable except n and i, Πbi=a ϕ may be taken out of the global
iteration Γdj=c ψ by renaming. This is very easy if the bounds of the iteration only depend
on n, because in this case Πbi=a ϕ contains no free variable except n, thus it may be replaced
by a fresh variable p and the equivalence p ⇔ Πbi=a ϕ may be added as an axiom. This is
done by the rule τ2 :
τ2

ψ → (p ⇔ Πbi=a ϕ) ∧ ψ[p/Πbi=a ϕ]
If p is a fresh symbol, ψ is the global schema, Πbi=a ϕ occurs in an iteration in ψ
and contains no free variable except n.

Things get more
if the bounds of the iteration contain a bound variable j (e.g.,
∨
∨ complicated
the schema nj=1 (qi ⇒ ji=1 ri )) because in this case the iteration cannot be taken out and
j cannot be eliminated by τ1 . Notice that, in this case, the lower bound a cannot contain
j and the coeﬃcient of j in the upper bound b must be ±1. In this case, Πbi=a ϕ can be
replaced by a new variable pj that can be deﬁned inductively. For instance in the previous
∨
∧
example, ji=1 ri is replaced by a variable pj deﬁned as follows: ¬p0 ∧ nj=1 [pj ⇔ (rj ∨pj−1 )].
The transformation is formally speciﬁed by the rules τ3 (if the coeﬃcient of j is 1) and τ4
(if the coeﬃcient of j is −1). Notice that if ψ denotes the global schema, then pj must be
deﬁned for every j ∈ [minψ (j), maxψ (j)].
τ3

ϕ

→

∧a−b−1

j=minϕ (j) ¬pj
∧maxϕ (j)
∧ j=a−b
(pj

∨
⇔ (pj−1 ∨ ψ[b + j/i])) ∧ (ϕ[pj / b+j
i=a ψ])
∨b+j
If p is a fresh symbol, i=a ψ occurs in an iteration of ϕ, j is bound in ϕ,
a, b and ψ contain no free variable except n, ϕ is the global schema.

∧maxϕ (j)
j=b−a+1 ¬pj
∧
∨b−j
∧ b−a
j=minϕ (j) (pj ⇔ (pj+1 ∨ ψ[b − j/i])) ∧ (ϕ[pj / i=a ψ])
∨
If p is a fresh symbol, b−j
i=a ψ occurs in an iteration of ϕ, j is bound in ϕ,
a, b and ψ contain no free variable except n, ϕ is the global schema.
∧
The rules τ3′ and τ4′ for
are deﬁned in a similar way (see Figure 1).
τ4

ϕ

→

4.2.2 Transforming every Iteration into Iterations over Intervals of the
Form [α, n − β]
The next step is to ensure that for every iteration Πbi=a ϕ, a is an integer α and that b is of
the form n − β, where β is a constant (initially both a and b must be of the form 2.δ.n + γ
(since the initial schema is normalized and no iteration is contained inside another one so
no bound variable occurs in the upper bound). The ﬁrst point is easily performed by an
616

Decidability and Undecidability Results for Propositional Schemata

appropriate translation of the iteration counter (rule τ5 ):
τ5

Πγ.n−δ
i=α.n+β ϕ →
If α ̸= 0, β ∈ Z.

(γ−α).n−δ

Πi=β

ϕ[i + α.n/i]

Then we ensure that the coeﬃcient of n in b is positive. Fortunately, if this coeﬃcient is
negative then there is κ ∈ N s.t. for every interpretation I s.t. I(n) > κ, the interval
[I(a), I(b)] is empty, in which case Πbi=a ϕ is either ⊤ or ⊥ (depending on Π). Since the
value of n is positive, there exist ﬁnitely many values for n s.t. the iteration is non empty.
One can eliminate the iteration by considering these cases separately. This is done by the
rule τ6 :
τ6

ψ → [ψ]n7→0 ∨ . . . ∨ [ψ]n7→κ ∨ (n > κ ∧ ψ[⋄/Πα.n−β
i=γ ϕ])
∧ ∨
α.n−β
If ψ contains Πi=γ ϕ, with α, β, γ ∈ Z, α < 0 and Π ∈ { , },
∨
∧
then ⋄ = ⊥ and if Π = then ⋄ = ⊤.
where κ = ⌈ γ−β
α ⌉ and Π is

Finally, we obtain the desired result by (recursively) decomposing an iteration interval
of the form [γ, α.n + β] (where α > 1) into two smaller intervals [γ, (α − 1).n + β] and
[(α − 1).n + β + 1, α.n + β]. Obviously, this is possible only if (α − 1).n + β ≥ γ, thus the
case where (α − 1).n + β < γ must be considered separately. This is easy to achieve, since
in this case there are only ﬁnitely many possible values of n, namely 0, 1, . . . , ⌊ γ−β
α−1 ⌋.
τ7

ψ → ((α − 1).n − β ≥ γ ∧ ψ[ψ ′ /Πα.n−β
i=γ ϕ]) ∨ ([ψ]n7→0 ∨ . . . ∨ [ψ]n7→κ )
α.n−β
where ψ contains an iteration Πi=γ ϕ with α > 1,
∧ ∨
(α−1).n−β
ψ ′ is Πi=γ
ϕ ⋆ Πni=1 ϕ[i + (α − 1).n − β/i], with Π ∈ { , },
∧
∨
where κ = ⌊ γ−β
then ⋆ = ∧, if Π = then ⋆ = ∨.
α−1 ⌋, Π =

4.2.3 Removing the Parameter from the Indices in the Iterations
The next phase consists in removing the indexed variables of the form pα.n+ϵ.i+β where
β ∈ Z and either α ̸= 0 or ϵ = −1 (to get variables indexed by expressions of the form
i + β only). We ﬁrst ensure that α is even. Although initially the coeﬃcient of every
occurrence of n is even, this property does not hold anymore at this point because of the
rule τ7 . Suppose a variable p(2γ+1).n+c , where c does not contain n, occurs in an iteration
Πbi=a ϕ. Then (since the schema is normalized) this variable must have been introduced by
the rule τ7 and i has been shifted by (α − κ).n for some κ (by deﬁnition of τ7 ). This shift
is applied to every index containing i (by deﬁnition of τ7 ), i.e. to every index of a variable
occurring in Πbi=a ϕ (otherwise the iteration would be reducible by τ1 ). As a consequence
every index in this iteration has an odd coeﬃcient for n. Hence if we add n to each index
we retrieve even coeﬃcients in all the iteration. Fortunately by commutativity of ∨ and ∧,
any iteration Πbi=a ϕ is equivalent to Πb−a
i=0 ϕ[b − i/i]. In our case b is of the form n − β for
some β ∈ Z so applying this transformation
precisely adds n to each index
∨
∨ (and substracts
a β). For instance, the iteration ni=1 (pn+i ∨ pn−i ) can be replaced by n−1
i=0 (p2n−i ∨ pi) .
This idea is formalized by the rule τ8 :
τ8

Πn−β
Πn−γ
i=γ ϕ →
i=β ϕ[n − i/i]
If the indices of the variables in ϕ are of the form (2α + 1).n + c, where c ∈ Ni .
617

Aravantinos, Caferra & Peltier

Once the coeﬃcient of n in every indexed variable is even, we introduce, for every variable
+
−
+
−
p and for every integer κ, two new (fresh) variables pκ and pκ s.t. pκa and pκa denote
respectively p2.κ.n+a and p2.κ.n−a where a ∈ Ni ∪ Z i.e. a is of the form β.i + γ where
+
β ∈ {0, 1}, γ ∈ Z (rule τ9 ). Then the index of pκa does not contain n anymore. Furthermore,
−
the index of pκa now contains +i instead of −i. Thus this transformation indeed achieves
our goal however it does not preserve sat-equivalence because two variables p2α.n+a and
p2β.n−b (respectively p2α.n+a and p2β.n+b , p2α.n−a and p2β.n−b ) s.t. 2α.n + a = 2β.n − b
(respectively 2α.n + a = 2β.n + b and 2α.n − a = 2β.n − b) may be replaced by distinct
−
+
−
+
+
−
variables pαa and pβb (respectively pαa and pβb , pαa and pβb ). Notice that it is important
to distinguish the sign + or − in front of a and b, as both are not integers but expressions of
Ni ∪ Z. In order to preserve sat-equivalence one would have to explicitly add the following
axioms to the schema:
−
+
2α.n + γ = 2β.n − δ ⇒ (pαγ ⇔ pβδ )
and
+

+

−

−

2α.n + γ = 2β.n + δ ⇒ (pαγ ⇔ pβδ )
and

2α.n − γ = 2β.n − δ ⇒ (pαγ ⇔ pβδ )
for every tuple (α, β, γ, δ) ∈ Z4 .
This transformation is problematic, because there exist inﬁnitely many such formulae.
Fortunately, we do not have to add all these equivalences, but only those concerning propositional variables that occur in a propositional realization of the schema. As we shall see,
this set (denoted by Ψ(ϕ)) is ﬁnite, because each expression γ, δ ranges over a set of the
form [−ι, ι] ∪ [n − ι, n + ι], where ι ∈ N.
More formally, let V + and V − be two disjoint subsets of P, distinct from the symbols
already occurring in the considered formula. We assume that every pair (p, α) where p is
+
a variable occurring in the formula and α an integer is mapped to two variables pα ∈ V +
−
+
−
and pα ∈ V − . pαi and pαi will denote the atoms p2α.n+i and p2α.n−i respectively. We
denote by ϕ the schema obtained from ϕ by replacing every variable of the form p2α.n+a
+
(where a ∈ Ni ∪ N for some bound variable i) by pαa and each variable of the form p2α.n−a
−
by pαa (in both cases we may have α = 0, moreover, if a = 0 then the replacement may
+
−
+
−
be done arbitrarily by pα0 or pα0 ). Notice that all atoms in ϕ are of the form pαa or pαa ,
where a ∈ Ni ∪ N for some bound variable i. τ9 is deﬁned as follows:
∧
τ9 ϕ → ϕ ∧ ψ∈Ψ(ϕ) ψ
If ϕ contains a variable p not occurring in V − ∪ V + ,
and where Ψ(ϕ) is deﬁned by Deﬁnitions 4.4, 4.5 and Lemma 4.6.
4.2.4 Aligning Iterations
Finally, it remains to ensure that all the iterations have the same bounds. At this point
′
′
every iteration is of the form Πn−β
i=α ϕ where α, β ∈ Z. Let α , β be the greatest integers α, β.
n−β−1
If we have α ̸= α′ or β ̸= β ′ , then we unfold the iteration once, yielding Πi=α
ϕ⋆ϕ[n−β/i].
n−β−1
n−β−1+α′ −α
By translation of the iteration counter, Πi=α
is equivalent to Πi=α′
ϕ[i − α′ + α/i].
′
The lower bound of the obtained iteration is now identical to α and its length has been
618

Decidability and Undecidability Results for Propositional Schemata

decreased. This is repeated until we obtain an iteration on the interval [α′ , β ′ ]. The rule
τ10 formalizes this transformation:
τ10

Πn−β
i=α ϕ

→

(n < α + β ∧ ⋄) ∨
′ −α
(n ≥ α + β ∧ Πn−β−1+α
ϕ[i − α′ + α/i] ⋆ ϕ[n − β/i])
i=α′
where α′ is the maximal lower bound of an iteration occurring in the
′
whole formula
α ̸= α′ or β ̸= β ′ ,
∨ and β is the minimal upper bound,
∧
and if Π is
then ⋄ = ⊥, ⋆ = ∨ and if Π = then ⋄ = ⊤, ⋆ = ∧.

4.2.5 Definition of Ψ(ϕ)
The most diﬃcult part of the transformation is the removal of the variable n in the index
performed by the rule τ9 , and more precisely the deﬁnition of Ψ(ϕ). We now establish the
results ensuring the feasability of this transformation.
Definition 4.4
We denote by Ψ the set of schemata of the form:
+

−

+

+

−

−

2α.n + a = 2β.n − b ⇒ (pαa ⇔ pβb )
or

2α.n + a = 2β.n + b ⇒ (pαa ⇔ pβb )
or

2α.n − a = 2β.n − b ⇒ (pαa ⇔ pβb )
where α, β ∈ Z, a, b ∈ Nn ∪ Z.
The set Ψ is inﬁnite. Thus we add a further restriction:
Definition 4.5
Let ϕ be a schema containing a unique parameter n. A schema ψ ⇒ (p ⇔ q) occurring in
Ψ is said to be relevant w.r.t. ϕ iﬀ the following conditions hold:
• p and q are not syntactically identical.
• There exists a natural number κ s.t. ψ[κ/n] is true and ϕ[κ/n] contains both p[κ/n]
and q[κ/n]
occur in ϕ itself. For instance, take ϕ =
∧n Notice that p and q do not
∧n necessarily
−
+
2
0
2−
0+
i=1 (p2n−i ∨ ¬pi ). So ϕ =
i=1 (pi ∨ ¬pi ). Then 2n − n = 4 ⇒ (pn ⇔ p4 ) is
+
−
easily seen to be relevant, however both p2n and p04 do not occur in ϕ.
The next lemma provides a very simple necessary condition on relevant equivalences in
Ψ. It also shows that for every schema ϕ the number of relevant equivalences in Ψ is ﬁnite
(up to equivalence).
Lemma 4.6
Let ϕ be a schema containing a unique parameter n. Assume that the coeﬃcient of n is
n+ζ
even in every index in ϕ and that every iteration in ϕ is of the form Πi=ϵ
ψ, where ϵ, ζ ∈ Z
619

Aravantinos, Caferra & Peltier

(ϵ, ζ may depend on the iteration). Let ι be the greatest natural number occurring in ϕ
(possibly as a coeﬃcient of n or in an expression of the form −ι).
−
+
For every relevant formula of the form 2α.n + a = 2β.n − b ⇒ (pαa ⇔ pβb ), 2α.n + a =
−

+

+

−

2β.n + b ⇒ (pαa ⇔ pβb ) or 2α.n − a = 2β.n − b ⇒ (pαa ⇔ pβb ) in Ψ, we have, for every
κ ∈ N: α, β ∈ [−ι, ι] and a[κ/n], b[κ/n] ∈ [−2ι, 6ι] ∪ [κ − 2ι, κ + 2ι].
Proof
Let σ stand for the substitution [κ/n]. By deﬁnition of a relevant formula, there must exist
+

−

+

+

κ ∈ N such that pαa σ and pβb σ (respectively pβb σ) occur in [ϕ]σ (but notice that pαa , pβb

−

+

and pβb do not necessarily occur in ϕ). Furthermore we must have 2α.κ + aσ = 2β.κ − bσ
(resp. 2α.κ + aσ = 2β.κ + bσ).
Since the coeﬃcient of n is even in every index in ϕ and since a, b ∈ Nn ∪ Z, 2α, 2β
necessarily occur in ϕ. Thus α, β ∈ [−ι/2, ι/2] ⊆ [−ι, ι].
−
+
+
Moreover, by Lemma 2.8, there exist two atoms pαa′ and pβb′ (respectively pβb′ ) which
−

+

occur in ϕ and two ϕ-expansions σ ′ and σ ′′ of σ for pαa′ and pβb′ (respectively pβb′ ) s.t.
we have aσ = a′ σ ′ and bσ = b′ σ ′′ . By deﬁnition, a′ , b′ come from the replacement of
+
−
+
some proposition p2α.n+a′ (resp. p2β.n−b′ and p2β.n+b′ ) by pka′ (resp. pkb′ and pkb′ ). Thus
a′ and b′ do not contain n. Thus a′ and b′ are either in Z (and in this case we must
have aσ, bσ ∈ [−ι, ι] ⊆ [−2ι, κ + 2ι]) or respectively of the form i + γ and i + δ where
i is a bound variable and γ, δ ∈ Z. Then since σ ′ , σ ′′ are ϕ-expansions of σ we have
iσ ′ , iσ ′′ ∈ [minϕ (i)σ, maxϕ (i)σ]. We have minϕ (i) = ϵ ≥ −ι and maxϕ (i) = n + ζ ≤ n + ι.
Thus aσ, bσ ∈ [−2ι, κ + 2ι].
Assume that we have 2α.κ + aσ = 2β.κ − bσ. Then aσ + bσ = 2.(β − α).κ.
+

• If β ≤ α then aσ + bσ ≤ 0. Since aσ, bσ ≥ −2ι, we deduce aσ, bσ ≤ 2ι. Thus
aσ, bσ ∈ [−2ι, 6ι].
• If β > α then aσ + bσ ≥ 2κ. Since aσ ≤ κ + 2ι and bσ ≤ κ + 2ι we must have
aσ ≥ κ − 2ι and bσ ≥ κ − 2ι. Thus aσ, bσ ∈ [κ − 2ι, κ + 2ι].
Now, assume that 2α.κ + aσ = 2β.κ + bσ. Then aσ − bσ = 2.(β − α).κ.
• If α = β then we must have aσ = bσ. This contradicts the ﬁrst condition in Deﬁnition
4.5 (the indexed variables cannot be syntactically identical).
• If α < β then aσ − bσ > 2κ. This is possible only if aσ > 2κ + bσ > 2κ − 2ι, hence
κ + 2ι > 2κ − 2ι, i.e. 4ι > κ. Then since we must have aσ, bσ ∈ [−2ι, κ + 2ι] we deduce
aσ, bσ ∈ [−2ι, 6ι].
• The proof is symmetric if α > β.
Finally if 2α.κ − aσ = 2β.κ − bσ then aσ − bσ = 2.(α − β).κ and the proof follows exactly
as in the previous case.

Lemma 4.6 implies that the set of relevant formulae is ﬁnite (up to equivalence). Indeed,
it suﬃces to instantiate α, β by every integer in [−ι, ι] and a, b either by elements of [−ι, 6ι]
620

Decidability and Undecidability Results for Propositional Schemata

or by expressions of the form n + γ, where γ is an integer in [−2ι, 2ι]. Thus we denote by
Ψ(ϕ) a ﬁnite subset of Ψ containing all relevant formulae (up to equivalence). Such a set
can be easily computed by applying Lemma 4.6, but using reﬁned criteria is possible, thus
we opt for a generic deﬁnition.
Remark 4.7
The fact that the coeﬃcient of n is even (see Deﬁnition 4.2 of normalized schemata) is
essential at this point. If arbitrary coeﬃcients are allowed for n, then the coeﬃcients 2α
and 2β must be replaced by α and β respectively. Then in the second item in the proof
of Lemma 4.6 we obtain ασ + bσ ≥ κ (instead of aσ + bσ ≥ 2κ). Thus we get eventually
ασ, bσ > −2ι (instead of aσ ≥ κ − 2ι). This means that aσ, bσ range over the interval
[−2ι, κ + 2ι] instead of [−ι, 6ι] ∪ [κ − 2ι, κ + 2ι]. But this interval is unbounded, thus Ψ(ϕ)
is inﬁnite (even up to equivalence).
For instance, suppose that we allow any ∨
coeﬃcient for n (i.e. odd or∨even) and that
+
+
−
1
pα.n+β is turned into pβ . Consider then ϕ = ni=1 (pi ∨ pn−i ). We get: ϕ = ni=0 (p0i ∨ p1i ).
−

+

But the equivalence p0i ⇔ p1n−i is obviously needed for every i ∈ [1, n], which cannot be
expressed by a ﬁnite number of equivalences.
On the other hand, if we only allow normalized
schemata, i.e. even coeﬃcients
for
∨n
∨2n
τ7 ) ψ = i=1 (pi ∨
n, then we
∨(pi ∨ p2n−i ) hence∨(by
∨ ﬁrst have to turn ϕ into ψ = i=1
n−1
(p2n−i ∨ pi ). Then
p2n−i ) ∨ ni=1 (pn+i ∨ pn−i ), and (by τ8 ) ψ = ni=1 (pi ∨ p2n−i ) ∨ i=0
∨
∨
+
−
1−
0+
ψ = ni=1 (p0i ∨ p1i ) ∨ n−1
i=0 (pi ∨ pi ). No equivalence is needed in this simple case.
Lemma 4.8
Let ϕ be a schema containing a unique parameter n s.t. every iteration in ϕ is of the form
Πn+β
i=α ψ, where α, β ∈ Z. ϕ is satisﬁable iﬀ ϕ ∪ Ψ(ϕ) is satisﬁable.
Proof
Let I be an interpretation satisfying ϕ. Let κ = I(n). We deﬁne an interpretation J as
+

follows: J (n) = κ and for every pair of integers (α, β): J (pαβ ) = ⊤ iﬀ I(p2α.κ+β ) = ⊤
def

−

def

and J (pαβ ) = ⊤ iﬀ I(p2α.κ−β ) = ⊤. By deﬁnition for all ψ ∈ Ψ, J |= ψ. ϕ is obtained
from ϕ by replacing every atom of the form p2α.n+a (respectively p2α.n−a ) where a ∈ Ni ∪ Z
+
−
+
(for some bound variable i) by pαa (respectively pαa ). By deﬁnition of J , J |= pαβ iﬀ
def

−

I |= p2α.n+β and J |= pαβ iﬀ I |= p2α.n−β . Since I |= ϕ it is clear that we have J |= ϕ.
Thus J |= ϕ ∪ Ψ(ϕ).
Conversely, let I |= ϕ ∪ Ψ(ϕ). Let κ = I(n). Let J be the interpretation deﬁned as
+
+
−
def
follows. J (n) = κ, J (p2α.κ+β ) = I(pαβ ) if pαβ occurs in [ϕ]I , and J (p2α.κ−β ) = I(pαβ )
−

if pαβ occurs in [ϕ]I . It is easy to check that J is well-deﬁned since I |= Ψ(ϕ) and Ψ(ϕ)
+

−

contains all the necessary equivalences. By deﬁnition, pαa (respectively pαa ) occurs in ϕ iﬀ
p2α.n+a (respectively p2α.n+a ) occurs in ϕ. Thus, since I |= ϕ we have J |= ϕ.

4.3 Termination and Complexity
In this section, we investigate the complexity of the transformation algorithm and show
that it is exponential. For every schema ϕ, we denote by |ϕ| the size of ϕ, i.e. the number
of symbols occurring in ϕ. τ denotes the system of rewrite rules of Figure 1.
621

Aravantinos, Caferra & Peltier

Theorem 4.9
Let ϕ be a normalized bound-linear schema. A normal form ψ of ϕ w.r.t. τ can be computed
in O(2|ϕ| ) rewriting steps. Moreover, |ψ| = O(2|ϕ| ).
Proof
We ﬁrst notice that the rules are always applied sequentially: it is easy to check that a rule
cannot introduce a formula on which a previous rule applies. Thus we consider each rule in
sequence.
First, we consider the rule τ1 . We call τ1 -atoms the atoms pc on which the rule possibly
applies, i.e. the atom occurring in an iteration Πbi=a ψ but not containing the iteration
counter i. This rule removes an atom occurring in an iteration but not containing the
iteration counter. Due to the control (i.e. the application conditions of the rules), no atom
satisfying this condition can be introduced into the formula (indeed, if the atom pc occurs
in an iteration then, because of the second application condition of the rule, it must contain
the corresponding iteration counter of this iteration). Therefore, the number of applications
of this rule on an iteration is bounded by the number of τ1 -atoms it contains. Since the rule
duplicates the considered iteration the total number of applications of the rule is bounded
by 2m , where m is the total number of τ1 -atoms. Obviously m ≤ |ϕ|.
This is not suﬃcient to prove the second result, i.e. that the size of the formula is
O(2|ϕ| ), since each application of the rule can double the size of the formula (which would
yield a double exponential blow-up since there are 2m rule applications). Consider the set
of leaf positions of the considered formula. For each position p in this set, we denote by |p|
the length of p and by rp the number of possible applications of the rule τ1 along p. Each
application of the rule τ1 removes some positions p from this set (those corresponding to the
leaves of the subformula on which the rule is applied) and replaces them by new positions
p′1 , . . . , p′κ . Both the number of these positions and their length possibly increase. However,
we remark that the rule can only increase the length of these positions by 2 (by adding a
disjunction of conjunctions), i.e. we have ∀ι ∈ [1, κ], |p′ι | ≤ |p| + 2. Furthermore, the number
rp necessarily decreases: ∀ι ∈ [1, κ], rp′ι < rp . Consequently, the value |p| + 2 × rp cannot
increase (i.e. we have ∀ι ∈ [1, κ], |p′ι | + 2 × rp′ι ≤ |p| + 2 × rp ), which implies that the length
of the ﬁnal positions (when rp′ι = 0) are lower than |pmax | + 2 × rmax , where rmax denotes
the maximal number of possible applications of the rule τ1 along some position in the initial
formula (i.e. the max of the rp in the initial formula) and pmax is the position of maximal
length in the initial formula. Both |pmax | and rmax are O(|ϕ|), thus the depth of the ﬁnal
formula is O(|ϕ|), which implies that it size is O(2|ϕ| ).
We now consider the other rules. First we analyze the transformation due to a single
application of each of those rules (then we will analyze the number of such applications).
Since the proofs for the diﬀerent cases are actual very similar, we do not consider each rule
separately, but we rather factorize some part of the analysis.
• Each application of the rule τ2 only increases the size of the formula by a constant
number of symbols, since a ﬁxed number of new connectives is added and no part of
the formula is duplicated.
• The application of the rules τ3 , τ3′ , τ4 , τ4′ , τ5 , τ8 and τ10 adds a constant number of
new connectives in the formula and replaces each occurrence of the counter i in the
622

Decidability and Undecidability Results for Propositional Schemata

formula ϕ by an expression of the form b + j, b − j, i + α.n or n − i. The size of these
expressions is bounded by the size of the original formula, thus the size of the formula
increases quadratically (since the number of occurrences of i is also bound by the size
of the formula).
• Now consider the rules τ6 and τ7 . These rules introduce a constant number of new
connectives and occurrences of atoms and duplicate κ times a subformula ψ. The
value of κ is bounded by the natural number γ that occurs in ϕ, thus the size of the
formula increases polynomially (since natural numbers are encoded as unary terms
s(. . . (s(0)) . . .) is our setting, hence κ is bounded by the size of the formula – notice
that this would not be the case if the numbers were encoded as sequences of digits5 ).
Thus we only have to show that the number of applications of each of these rules is
polynomially bounded by the size of the initial formula. Once again, we distinguish several
cases:
• The rules τ2 , τ3 , τ3′ , τ4 , τ4′ only apply on iterations occurring inside another iteration.
During the application of the rule, this iteration is replaced by an atom, hence removed
from the outermost iteration. The rule introduces new iterations, however they only
occur at the root level, outside the scope of any iteration. Thus the total number of
possible applications of these rules is bounded by the number of iterations initially
occurring inside another iteration, hence by |ϕ|.
• The rules τ5 , τ6 and τ8 apply at most once on each iteration: τ5 applies on an iteration
in which the lower bound contain n and gets rid of any occurrence of n in the lower
bound. τ6 applies on iterations in which the upper bound contains −n and replaces
these iterations by purely propositional formulae. τ8 applies if the coeﬃcient of n in
every index is odd. Since the rule adds n to each index, after the application of the
rule, the coeﬃcient of n must be even and the rule cannot apply again on the same
iteration.
• The rule τ7 decreases the value of the coeﬃcient α of n in the upper bound by 1. Thus
the number of applications of the rule τ7 on each iteration is lower than the initial
value of α (which is bound by the size of the formulae since integers are encoded as
terms). Similarly, since τ10 unfolds an iteration until an iteration of length n − β ′ − α′
is obtained, the number of applications of the rule τ10 on each iteration is bound by
the value of −β + α + β ′ − α′ .
• Finally, the rule τ9 applies only once on the whole schema. The rule adds a conjunction of equivalence to the schema, but by Lemma 4.6, the size of the conjunction
is polynomially bounded by the greatest natural number ι occurring in the schema,
hence by the size of the formula.

For every schema ϕ, we denote by ϕ ↓τ a normal form of ϕ w.r.t. the rules in τ .
5. Actually the translation is doubly exponential in this case.

623

Aravantinos, Caferra & Peltier

4.4 Soundness and Completeness
We prove that the rules in τ preserve sat-equivalence and that every irreducible formula is
regular. We need the two propositions below:
Lemma 4.10
Let ψ, ϕ and ϕ′ be schemata. Let I be an interpretation such that for every ground
substitution σ of the parameters of ψ and for every ψ-expansion θ of σ for ϕ, ϕ′ , we have:
JϕθKI = Jϕ′ θKI . Then JψKI = Jψ[ϕ′ /ϕ]KI .
Proof
The proof is by induction on ψ. If ψ does not contain ϕ the proof is trivial. If ψ = ϕ
then ψ[ϕ′ /ϕ] = ϕ′ . By deﬁnition JϕKI = JϕσI KI and Jϕ′ KI = Jϕ′ σI KI . But σI is a ground
substitution of the parameters of ψ = ϕ and thus is of course a ψ-expansion of itself for ϕ
and ϕ′ . Thus JϕσI KI = Jϕ′ σI KI hence JψKI = Jψ ′ KI .
Assume that ψ = ¬ψ ′ . We have Jψ[ϕ′ /ϕ]KI = ¬Jψ ′ [ϕ′ /ϕ]KI = ¬Jψ ′ KI (by induction).
Thus Jψ[ϕ′ /ϕ]KI = JψKI . The proof is similar if ψ = (ψ1 ∨ ψ2 ) or if ψ = (ψ1 ∧ ψ2 ).
∧
Now assume that ψ = bi=a ψ ′ . I |= ψ iﬀ for every integer κ ∈ [JaKI , JbKI ] we have
def
I[κ/i] |= ψ ′ . Let σ ′ be the substitution such that σ ′ (i) = κ and σ ′ (x) = σ(x) if x ̸= i.
Let θ be a ψ-expansion of σ ′ for ψ ′ . By deﬁnition κ ∈ [Jminψ (i)KI , Jmaxψ (i)KI ], thus θ is
also a ψ-expansion of σ. Therefore we have JϕθKI = Jϕ′ θKI , hence JϕθKI[κ/i] = Jϕ′ θKI[κ/i]
(since ϕθ and ϕ′ θ do not contain i). Consequently, by the induction hypothesis, we have
Jψ ′ KI[κ/i] = Jψ ′ [ϕ′ /ϕ]KI[κ/i] . Hence I |= ψ iﬀ for every integer κ ∈ [JaKI , JbKI ] we have
∨

I[κ/i] |= ψ ′ [ϕ′ /ϕ] i.e. iﬀ I |= ψ[ϕ′ /ϕ]. The proof is similar if ψ = bi=a ψ ′ .
Lemma 4.11
For every schema ϕ and for every indexed proposition p that does not contain any variable
bound in ϕ:
ϕ ≡ (p ∧ ϕ[⊤/p]) ∨ (¬p ∧ ϕ[⊥/p])
Proof
We have p ∨ ¬p ≡ ⊤ hence by distributivity ϕ ≡ (p ∧ ϕ) ∨ (¬p ∧ ϕ). We now show that for
every interpretation I, Jp ∧ ϕKI = Jp ∧ ϕ[⊤/p]KI . If JpKI = F then both p ∧ ϕ and p ∧ ϕ[⊤/p]
are false in I. Otherwise, by Lemma 4.10, we have JϕKI = Jϕ[⊤/p]KI . Similarly, we have
J¬p ∧ ϕKI = J¬p ∧ ϕ[⊥/p]KI . Hence ϕ ≡ (p ∧ ϕ[⊤/p]) ∨ (¬p ∧ ϕ[⊥/p]).

Theorem 4.12
Let ϕ be a normalized bound-linear schema. ϕ is satisﬁable iﬀ ϕ ↓τ is satisﬁable.
Proof
The proof is by inspection of the diﬀerent rules (see the deﬁnition of the rules for the
notations):
• τ1 . The proof is a direct application of Lemma 4.11.
• τ2 . For every model I of ψ, one can construct an interpretation J of (p ⇔ Πbi=a ϕ) ∧
ψ[p/Πbi=a ϕ] by interpreting p as JΠbi=a ϕKI . By deﬁnition we have J |= (p ⇔ Πbi=a ϕ).
624

Decidability and Undecidability Results for Propositional Schemata

Since I |= ψ we have J |= ψ. By Lemma 4.10 we deduce that I |= ψ[p/Πbi=a ϕ]. Hence
J |= (p ⇔ Πbi=a ϕ) ∧ ψ[p/Πbi=a ϕ].
Conversely, if I is a model of (p ⇔ Πbi=a ϕ)∧ψ[p/Πbi=a ϕ], then due to the ﬁrst conjunct
Πbi=a ϕ and p have the same truth value in I hence since I |= ψ[p/Πbi=a ϕ], we deduce
I |= ψ, by Lemma 4.10.
• τ3 . Assume that I |= ϕ. Let J be the extension of I obtained by interpreting pκ as
∨b+κ
∨
J i=a
ψKI . By Lemma 4.10 we have J |= (ϕ[pj / b+j
i=a ψ]). Furthermore by deﬁnition
∨b+κ
of the semantics, we have J i=a ψKI = F if Jb+κ−aKI < 0 hence J |= ¬pκ if κ < a−b.
∧
Thus J |= ¬pa−b−1 ∧ a−b−1
j=minϕ (j) (pj ⇔ pa−b−1 ). Furthermore, for every ι ≥ Ja−bKI , we
∨b+ι
∨
have J i=a ψKI = T iﬀ either J b+ι−1
ψKI = T or Jψ[b + ι/j]KI = T. Hence Jpι KI = T
i=a
∧maxϕ (j)
iﬀ either Jpι−1 KI = T or Jψ[b + ι/j]KI = T. Therefore I |= j=a−b
(pj ⇔ (pj−1 ∨ ψ)).
∧a−b−1
∧maxϕ (j)
Conversely, let I be a model of ¬pa−b−1 ∧ j=min
(pj ⇔ pa−b−1 ) ∧ j=a−b
(pj ⇔
ϕ (j)
∨b+j
(pj−1 ∨ ψ[b + j/i])) ∧ (ϕ[pj / i=a ψ]). We show by induction on ι that I |= (pι ⇔
∨b+ι
i=a ψ) for every ι ∈ [Jminϕ (j)KI , Jmaxϕ (j)KI ]:
∨b+ι
– If ι < Ja − bKI then by deﬁnition J i=a
ψKI = F. Moreover by the ﬁrst two
conjuncts in the previous formula we must have Jpι KI = F.
∨
∨
ψ ∨ψ[b+ι/i]KI . Hence by the induction
ψKI = J b+ι−1
– Otherwise, we have J b+ι
i=a
i=a
∨b+ι
hypothesis: J i=a ψKI = Jpι−1 KI ∨ ψ[b + ι/i], and by the third conjunct in the
∨
formula above, we get: J b+ι
i=a ψKI = Jpι KI .
Then by Lemma 4.10 we deduce that I |= ψ. The proofs for the rules τ3′ , τ4 and τ4′
are similar.
∨
∧
∨γ.n+ϵ
ϕ iﬀ
• τ5 . Assume that Π = (the case Π = is similar). By deﬁnition I |= i=α.n+β
there exists κ ∈ [Jα.n + βKI , Jγ.n + ϵKI ] such that I |= ϕ[κ/i], i.e. iﬀ there exists κ ∈
∨(γ−α).n+ϵ
[JβKI , J(γ−α).n+ϵKI ] such that I |= ϕ[κ+Jα.nKI /i], i.e. iﬀ I |= i=β
ϕ[i+α.n/i].
• τ6 . We assume that Π = ∨ and ⋄ = ⊥ (the case Π = ∧, ⋄ = ⊤ is similar). Since we
assume that I(n) ≥ 0 for every parameter n, we have I |= (n = 0∨. . .∨n = κ∨n > κ)
hence ψ is equivalent to: (n = 0 ∨ . . . ∨ n = κ ∨ n > κ) ∧ ψ. By distributivity we get
∨
ψ ≡ (n = 0∧ψ)∨. . . (n = κ∧ψ)∨(n > κ∧ψ). But α.n+β
ϕ is empty (thus equivalent
i=γ
γ−β
to ⊥) if I(n) > κ ≥ α , hence, by Lemma 4.10, we have ψ ≡ (n = 0 ∧ ψ) ∨ . . . (n =
∨
κ ∧ ψ) ∨ (n > κ ∧ ψ[⊥/ α.n+β
ϕ]). For every ι ∈ [0, κ], we have n = ι ∧ ψ |= [ψ]n7→ι ,
i=γ
∨
hence ψ |= [ψ]n7→0 ∨ . . . ∨ [ψ]n7→κ ∨ (n > κ ∧ ψ[⊥/ α.n+β
ϕ]).
i=γ
Conversely, if I |= [ψ]n7→ι holds, then I can be straightforwardly extended into a model
of n = ι ∧ ψ by interpreting n as ι. Thus for any model of [ψ]n7→0 ∨ . . . ∨ [ψ]n7→κ ∨ (n >
∨
κ ∧ ψ[⊥/ α.n+β
ϕ]) there exists a model of ψ, and τ6 preserves satisﬁability.
i=γ
∨
• τ7 . Again, we assume that Π = and ⋄ = ⊥. We have ((α −1).n+β < γ ∨(α −1).n +
β ≥ γ) ≡ ⊤ hence ψ ≡ ((α − 1).n + β < γ ∨ ((α − 1).n + β ≥ γ) ∧ ψ ≡ ((α − 1).n + β ≥
γ ∧ ψ) ∨ ((α − 1).n + β < γ ∧ ψ). Since the parameters are interpreted as natural
625

Aravantinos, Caferra & Peltier

numbers, we have I |= (α − 1).n + β < γ iﬀ I(n) ∈ [0, ⌈ γ−β
α−1 ⌉]. Then by deﬁnition
∨
JψKI = J[ψ]n7→I(n) KI . If I |= (α − 1).n + β ≥ γ then, by unfolding, J α.n+β
ϕKI =
i=γ
∨(α−1).n+β
∨(α−1).n+β
∨α.n+β
∨n
J i=γ
ϕ ∨ i=(α−1).n+β+1 ϕKI = J i=γ
ϕ ∨ i=1 ϕ[i + (α − 1).n + β/i]KI .
Hence τ7 preserves satisﬁability.
• τ8 : the proof is similar to the one of τ6 .
• The soundness of the rule τ9 is a direct consequence of Lemma 4.8.
∨
∨n−β
∨
• τ10 . We assume that Π = and ⋄ = ⊥. We have i=α
ϕ ≡ (n < α+β∧ n−β
i=α ϕ)∨(n ≥
∨n−β
∨n−β
α + β ∧ i=α ϕ). For every interpretation I, if I(n) < α + β then J i=α ϕKI = F
∨
∨
thus n < α + β ∧ n−β
ϕ ≡ (n < α + β ∧ ⋄). If I(n) ≥ α + β, then J n−β
i=α
i=α ϕKI ≡
∨n−β
Jϕ[α/i] ∨ i=α+1 ϕKI . Furthermore by translation of the iteration counter we have
∨n−β
∨n−β ′
′

i=α+1 ϕ ≡
i=α+1−β ′ +β ϕ[i + β − β/i]. Hence τ10 preserves equivalence.

Theorem 4.13
Let ϕ be a normalized bound-linear schema. ϕ ↓τ is regular.
Proof
Firstly, we remark that the application of the rules in τ on a bound-linear schema generates a
schema that is still bound-linear. Notice however that the obtained schema is not normalized
in general.
Let ϕ be a bound-linear formula, irreducible by τ . Assume that ϕ has been obtained
from a normalized schema by application of the rules in τ . We need to prove that ϕ is
regular.
We ﬁrst prove that ϕ contains no nested iteration. Let ψ = Πbi=a χ be an iteration
occurring in ϕ. Assume that χ contains an iteration Γdj=c γ. W.l.o.g. we assume that
γ contains no iteration (otherwise we could simply take ψ = χ). By irreducibility w.r.t.
the rule τ1 , all the indices in γ must contain j. By deﬁnition of the class of bound-linear
schemata, this implies that these indices cannot contain i. If j occurs in d then one of the
rule τ3 ,τ3′ , τ4 or τ4′ applies. Consequently the only free variable in Γdj=c γ is n. Thus the rule
τ2 applies which is impossible by irreducibility.
Then we remark that for all iterations Πbi=a ψ in ϕ, a ∈ Z and b is of the form n + α
where α ∈ Z. Indeed, if a contains n then the rule τ5 applies and if the coeﬃcient of n in b
is diﬀerent from 1 then the rule τ6 or τ7 applies.
The rule τ8 eliminates all indexed propositions in which the coeﬃcient of n is odd (since
the initial schema is normalized, these indexed variables have been necessarily introduced
by the rule τ7 , thus they must occur in an iteration and all the indices in the iteration must
have an odd coeﬃcient in front of n).
τ9 eliminates all the variables of the form p2α.n±a , where α ∈ Z and a ∈ Ni ∪ N, for some
bound variable i, and replaces them by variables indexed only by a.
Finally τ10 ensures that all the iterations have the same bounds.

626

Decidability and Undecidability Results for Propositional Schemata

5. STAB: A Decision Procedure for Regular Schemata
Now that we have shown how to transform a bound linear schema into a regular one, we show
that the satisﬁability problem is decidable for regular schemata. This is done by providing
a set of block tableaux rules (Smullyan, 1968) that are complete w.r.t. satisﬁability. Those
rules are concise and natural, and, compared to the naive procedure described in the proof
of Proposition 2.7, they are much more eﬃcient and terminate more often (see the end of
Section 5.1). The procedure is called stab (standing for schemata tableaux). Notice that
it applies on any schema (not only on regular ones). We assume (w.l.o.g) that schemata are
in negative normal form.
5.1 Inference Rules
Definition 5.1 (Tableau)
A tableau is a tree T s.t. each node N occurring in T is labeled by a set of schemata written
ΦT (N ).
As usual a tableau is generated from another tableau by applying some extension rules.
P
Let r =
be a rule where P denotes a set of schemata (the premises), and
C1 . . . Cκ
C1 , . . . , Cκ denote the conclusions. Let N be a leaf of a tree T . If a subset S of ΦT (N )
matches P then we can extend the tableau by adding κ children to N , each of them labeled
with Cι σ ∪ (ΦT (N ) \ S) where ι = 1, . . . , κ and σ is the matching substitution. A leaf N
is closed iﬀ the set of arithmetic formulae (i.e. schemata containing only atoms of the form
. . . < . . . and no iteration) in ΦT (N ) is unsatisﬁable. This can be detected using decision
procedures for arithmetic without multiplication (Cooper, 1972).
Definition 5.2 (Extension rules)
The extension rules of stab are deﬁned as follows.
• The usual rules of propositional tableaux:

(∧):

ϕ∨ψ

ϕ∧ψ

(∨):

ϕ ψ

ϕ

ψ

• Rules proper to schemata (“iteration rules”)6 :
∧b

∨b

i=a ϕ

(Iterated ∧):

b≥a
∧b−1
i=a ϕ ∧ ϕ[b/i]

i=a ϕ

b<a

(Iterated ∨):

b≥a
∨b−1
i=a ϕ ∨ ϕ[b/i]

6. The right branch in the conclusion of the Iterated ∧ rule is required, e.g., to detect that
satisﬁable with n = 0.

627

∧n
i=1

⊥ is

Aravantinos, Caferra & Peltier

• The closure rule adds the constraints needed for the branch not to be closed. The rule
is applied only if a ̸= b does not already occur in the branch.
pa
(Closure):

¬pb

pa , ¬pb , a ̸= b

stab without the loop detection rule described in the next section is already better than
the straightforward procedure introduced in the proof of Proposition 2.7. First, it terminates in some cases where the schema is unsatisﬁable (whereas the naive procedure never
terminates in such a case, unless the schema
∧nis just an unsatisﬁable propositional formula).
This is trivially the case for any schema i=1 ϕ with n ≥ 1, where ϕ is propositionally
unsatisﬁable.
Second, it can ﬁnd a model much faster than the naive procedure. Consider,
∧
e.g., ( 10000
p)
∧ (¬p ∨ ϕ) where ϕ is an unsatisﬁable formula. In this case stab immediately
i=n
ﬁnds a model where n > 10000 and p is interpreted as F.
Remark 5.3
Using a tableaux-based system for deciding regular schemata may seem surprising, since
DPLL procedures (Davis, Logemann, & Loveland, 1962) are usually more eﬃcient in propositional logic. However, extending such procedures to schemata is not straightforward. The
main problem is that evaluating an atom in a schema is not immediate, since this atom may
well appear in some realization of the schema without appearing in the schema itself. Thus,
in contrast to the propositional case, it is not suﬃcient to replace syntactically ∨
the atom
n
by its truth value. For instance, the atom p2 (implicitly) appears in the schema
i=1 pi if
∨n
p
)
∧
n≤1
n > 1. Thus
evaluating
p
to,
say,
F
would
yield
two
distinct
branches:
(
2
i=1 i
∨n
and (p1 ∨ i=3 pi ) ∧ n > 1. Thus one would have to deﬁne rules operating at deep positions
in the schema in order to unfold the iterations and instantiate the counter variables when
needed. In contrast, the tableaux method operates only on formulae occurring at root level
and compares literals only after they have been instantiated (using unfolding). This makes
the procedure much easier to deﬁne and reason with (in particular the termination behavior
is easier to control). Actually a DPLL procedure for schemata is presented in our previous
work (Aravantinos, Caferra, & Peltier, 2009a, 2010), but it is much more complicated than
the calculus presented here.
Of course, one could combine the iteration rules of the tableaux procedure with a SATsolver used as a “black box” that could be in charge of the purely propositional part.
However this is also not straightforward, mainly due to the fact that a partial evaluation is
needed to propagate the values of the propositional variables into the iterations.
5.2 Discarding Infinite Derivations: the Looping Rule
stab does not terminate in general. The reason is that an iteration is, in general, inﬁnitely
unfolded by the iteration rules.
∨n Assume for instance that ϕ is a propositional unsatisﬁable
formula.
∨ Then starting
∨n−κ from i=1 ϕ one could derive an inﬁnite sequence of formulae of the
form n−1
ϕ,
.
.
.
,
i=1
i=1 ϕ, for every κ ∈ N. We now introduce a loop detection rule that
aims at improving the termination behavior of stab. Detecting looping is the most natural
way to avoid this divergence: if, while extending the tableau, we ﬁnd a schema that has
already been seen, possibly up to a shift of arithmetic variables, then there is no need to
628

Decidability and Undecidability Results for Propositional Schemata

consider it again and we can stop the procedure. Such loopings can also be interpreted as
well-foundedness arguments in an inductive proof.
Definition 5.4 (Looping)
A shift is a substitution mapping every variable n to an expression of the form n − ι, where
ι ∈ N s.t. there is at least one variable n s.t. nσ < n (which is not always the case since we
may have ι = 0).
If I, J are two interpretations, we write I < J iﬀ there exists a shift σ s.t. J = Iσ.
Let ϕ, ψ be two schemata (or sets of schemata). We write ϕ |=s ψ iﬀ for every model I
of ϕ, there exists J < I s.t. J |= ψ.
Let N, N ′ be two nodes of a tableau T . Then N ′ loops on N iﬀ ΦT (N ′ ) |=s ΦT (N ).
In existing work on cyclic proofs, N ′ is sometimes called a bud node and N is the
companion node of N ′ (Brotherston, 2005). When a leaf loops, it is treated as a closed
leaf (though it is not necessarily unsatisﬁable). To distinguish this particular case of closed
leaf from the usual one, we say that it is blocked (blocked leaves are closed). Notice that
N and N ′ may be on diﬀerent branches, thus looping may occur more often, allowing more
simpliﬁcations.
Example ∨
5.5
∨
Let Φ = { ni=1 pi } and Ψ = { ni=2 qi }. Intuitively, Φ and Ψ have the same “structure”:
stab will behave similarly on both formulae. The relation |=s is supposed to formalize
this notion. We show on this example that it is the case, as expected, i.e. that we have
Ψ |=s Φ. Indeed, consider a model I of Ψ. We construct an interpretation J as follows:
def
def
J (n) = I(n) − 1 and for every κ ∈ [1, J (n)], J (pκ ) = I(qκ+1 ). Since I |= Ψ there exists
κ ∈ [2, I(n)] such that I(qκ ) = T. Thus there exists κ ∈ [1, I(n)−1] such that I(qκ+1 ) = T,
i.e. there exists κ ∈ [1, J (n)] such that J (pκ ) = T. Therefore J |= Φ.
Proposition 5.6
Let ϕ be a schema. If ϕ is satisﬁable then ϕ has a model I that is minimal w.r.t. < (i.e.
for every interpretation J , if J < I then J ̸|= ϕ).
Proof
Let V be the set of parameters of ϕ. Notice that V is ﬁnite. For every interpretation I we
denote by I(V ) the integer: I(V ) = Σn∈V I(n). Since we assumed that I(n) ∈ N for every
variable n, we deduce that I(V ) ≥ 0.
Let I be a model of ϕ such that I(V ) is minimal. Since the truth value of ϕ does not
depend on the values of the variables that are not in V , we may assume that ∀n ̸∈ V, I(n) =
0. Let J be a model of ϕ such that J < I. By deﬁnition there exists a shift σ such that
J = Iσ. For every arithmetic variable n, we have nσ = n − ιn , where ιn ∈ N; furthermore,
there exists at least one variable m such that ιm > 0. Thus J (n) = I(σ(n)) ≤ I(n) and
J (m) < I(m). Consequently we must have J (V ) ≤ I(V ), thus J (V ) = I(V ) (since I(V )
is minimal). By deﬁnition, this entails that ιn = 0 for every n ∈ V . Thus m ̸∈ V , but in
this case I(m) = 0 hence J (m) < 0 which is impossible (since we assume that parameters
are interpreted by natural numbers).

def

To apply the looping rule in practice one has to ﬁnd a shift and check that the implication
holds. Unfortunately, the relation |=s is obviously undecidable (for instance if ψ = ⊥, then
629

Aravantinos, Caferra & Peltier

it can be easily checked that ϕ |=s ψ iﬀ ϕ is unsatisﬁable, and as we shall see in Section 6 the
satisﬁability problem is undecidable for propositional schemata). Thus, in the following, we
shall use a much stronger criterion that is suﬃcient for our purpose. An obvious solution
would be to use set inclusion: indeed, ϕ |=s ψ if there exists a shift σ s.t. ϕ ⊇ ψσ. However,
this criterion is too strong, as the following example shows.
Example 5.7
∧
The schema ϕ = pn ∧ (pn ⇒ qn ) ∧ ¬q0 ∧ ni=1 (qi ⇒ qi−1 ) is obviously unsatisﬁable. The
reader can easily check that stab generates an inﬁnite sequence of sets of schemata of the
form:
n−κ
∧
{pn , qn , ¬q0 , qn−1 , . . . , qn−κ ,
(qi ⇒ qi−1 )}, where κ ∈ N
i=1

None of these sets contains a previous one up to a shift on n because of the indexed
proposition pn that must occur in every set.
Thus we introduce a reﬁnement of set inclusion based on the purity principle. The pure
literal rule is standard in propositional theorem proving. It consists in evaluating a literal
L to ⊤ in a formula ϕ (in NNF) if the complement of L does not occur in ϕ. Such a literal
is called pure. It is well-known that this operation preserves satisﬁability and may allow
many simpliﬁcations.
We show how to extend the pure literal rule to schemata. The conditions on L have
to be strengthened
in order to take iterations into account. For instance, if L = pn and ϕ
∨2n
contains i=1 ¬pi then L is not pure in ϕ, since ¬pi is the complement of L for i = n (and
since 1 ≤ n ≤ 2n). On the other hand p2n+1 may be pure in ϕ (since 2n + 1 ̸∈ [1, 2n]).
For every set of schemata
Φ we denote by ΦN the conjunction of purely arithmetic
def ∧
formulae in Φ: ΦN = ϕ∈Φ,ϕ is arithmetic ϕ.7
Definition 5.8 (Pure literal)
A literal pa (respectively ¬pa ) is pure in a set of schemata Φ iﬀ for every occurrence of a
literal ¬pb (respectively pb ) in Φ, the arithmetic formula ΦN ∧IC (Φ)∧a = b is unsatisﬁable8 .
Definition 5.9
Let Φ, Ψ be two sets of schemata. We write Φ ⊇s Ψ iﬀ there exists a shift σ for the set of
parameters in Φ and Ψ s.t. for every ψ ∈ Ψ:
• Either ψ is an arithmetic formula and ΦN |= ψσ.
• Or ψ is a pure literal in Ψ.
• Or ψσ ∈ Φ.
The ﬁrst and third items correspond roughly to set inclusion (up to arithmetic properties). The second item only deals with Ψ and not with Φ. It corresponds to the informal
idea that a pure literal can be removed. Of course it is the most important one.
7. A possible improvement would be to add in ΦN formulae that are obvious logical consequences of Φ.
For instance, if Φ = {pn ∧ (n > 1), ¬p1 } then ΦN would contain n > 1. This would make the notion of
‘pure literal’ slightly more general, e.g., pn would be pure in Φ, which is not the case with our current
deﬁnition.
8. See page 606 for the deﬁnition of IC (Φ).

630

Decidability and Undecidability Results for Propositional Schemata

Example 5.10
∧
∧n−1
Let Ψ = {n ≥ 0, pn+1 , pn , ni=1 (¬pi ∨ pi−1 ), ¬p0 } and Φ = {n ≥ 1, pn−1 , i=1
(¬pi ∨
pi−1 ), ¬p0 }. We have Φ ⊇s Ψ. Indeed, consider the shift σ = {n 7→ n − 1}. By deﬁnition ΦN = {n ≥ 1}. We have (n ≥ 0)σ = n − 1 ≥ 0 ≡ n ≥ 1, thus ΦN |= (n ≥ 0)σ. Since
n ≥ 0 and i ∈ [1, n], p∧
to pn+1 , thus pn+1 is pure in Ψ. Finally, we have
i cannot be identical ∧
pn σ = pn−1 ∈ Φ and ni=1 (¬pi ∨ pi−1 )σ = n−1
i=1 (¬pi ∨ pi−1 ) ∈ Φ.
We now show that ⊇s is decidable. First of all, it is trivial that syntactic equality is
decidable as shown by the following deﬁnition and proposition:
Definition 5.11
Let U(ϕ, ψ) be the arithmetic formula deﬁned as follows:
• If ϕ = pa and ψ = pb then U(ϕ, ψ) = (a = b).
def

• If ϕ = (a ▹ b) and ψ = (c ▹ d) (with ▹ ∈ {≤, <}) then U(ϕ, ψ) = (a = c) ∧ (b = d).
def

• If ϕ = ¬ϕ′ and ψ = ¬ψ ′ then U(ϕ, ψ) = U(ϕ′ , ψ ′ ).
• If ϕ = (ϕ1 πϕ2 ) (with π ∈ {∨, ∧}) and ψ = (ψ1 πψ2 ) then U(ϕ, ψ) = U (ϕ1 , ψ1 ) ∧
U(ϕ2 , ψ2 ).
• If ϕ = Πbi=a ϕ′ and ψ = Πdi=c ψ ′ then U(ϕ, ψ) = (a = c) ∧ (b = d) ∧ U(ϕ′ , ψ ′ ).
def

• Otherwise U(ϕ, ψ) = ⊥.
def

Proposition 5.12
Let ϕ, ψ be two schemata. For every substitution σ, U(ϕ, ψ)σ is valid iﬀ ϕσ and ψσ are
syntactically identical.
Proof
By a straightforward induction on the formulae.



We can prove the decidability of ⊇s :
Proposition 5.13
⊇s is decidable.
Proof
Since linear arithmetic is decidable, it is possible to check whether a literal is pure or not
in a set of formulae Ψ. Then these pure literals can be simply removed from Ψ (since they
satisfy the second condition in Deﬁnition 5.9). One now has to ﬁnd a shift σ such that every
remaining formula in Ψ satisﬁes the ﬁrst or third condition. Let n1 , . . . , nκ be the variables
in Φ, Ψ. Let σ be a substitution mapping every parameter nι (1 ≤ ι ≤ κ) to nι − lι , where
the lι are distinct variables not occurring in Φ, Ψ. One has to check that there exists a
substitution θ mapping every variable lι to an integer such that:
• ∀ι ∈ [1, κ], θ(lι ) ≥ 0 and ∃ι ∈ [1, κ], θ(lι ) > 0. Since κ is ﬁxed, this condition can be
stated as an arithmetic formula.
631

Aravantinos, Caferra & Peltier

• For every formula ψ ∈ Ψ, one of the following conditions holds:
– ψ is an arithmetic formula and ΦN |= ψσθ, i.e. the formula ∀n1 , . . . , nκ .ΦN ⇒
ψσθ is valid.
– ψσθ occurs in Φ. This holds iﬀ Φ contains a formula ϕ, such that ψσθ and
ϕ are identical for every value of the parameters, i.e., by Proposition 5.12, iﬀ
∀n1 , . . . , nk .U(ϕ, ψσθ) is valid.
Since every condition above is equivalent to an arithmetic formula, the whole condition can
be expressed as an arithmetic formula (taking the conjunction of the formulae corresponding
to each ψ ∈ Ψ and ϕ ∈ Φ). This formula is satisﬁable iﬀ there exists a substitution θ satisfying the desired property. Then the proof follows straightforwardly from the decidability
of linear arithmetic.

Now we prove that ⊇s is stronger than the relation |=s .
Proposition 5.14
Let Φ, Ψ be two sets of schemata. If Φ ⊇s Ψ then Φ |=s Ψ.
Proof
Let σ be the shift satisfying the conditions of Deﬁnition 5.9. Let I be an interpretation
satisfying Φ. Let θ = σI . We have to show that there exists J < I s.t. J |= ψ, i.e. that
there exists a shift σ ′ s.t. J = Iσ ′ and J |= ψ. Equivalently, we can show that there exists
a model J of ψσ, i.e. that σ ′ = σ is convenient. Let J be an interpretation s.t. J (L) = T
def
if L is a literal that is pure in Ψσ and J (L) = I(L) otherwise. Let ψ ∈ Ψ. We have to
show that J |= ψσ. We distinguish three cases, according to the three items in Deﬁnition
5.9.
• If ΦN |= ψσ, then since I |= Φ and since J and I coincide on every arithmetic
variable we must have J |= ψσ.
• If ψ is a literal pure in Ψ then ψσ is pure in Ψσ, thus we have J |= ψσ by deﬁnition.
• If ψσ ∈ Φ, then I |= ψσ. Thus every literal that is pure in Φ must be pure in ψσ.
The complementary of these literals cannot occur in [ψσ]θ . Since I and J coincide
on all other literals and since ψ is in negative normal form, we must have J |= ψσ.
Consequently J |= ψσ, hence J σ |= ψ.



⊇s is strictly less general than |=s as evidenced by the following:
Example ∨
5.15
∨
5.5).
Let Φ = { ni=1 pi } and Ψ = { ni=2 pi }. We have shown that Ψ
∨n|=s Φ (see ∨Example
n
However, we have Ψ ̸⊇s Φ, since there is no shift σ such that ( i=1 pi )σ = i=2 pi (this is
obvious since 1σ cannot be equal to 2 whatever is σ).
5.3 Examples
Before proving the soundness, completeness and termination of stab, we provide some
examples of tableaux.
632

Decidability and Undecidability Results for Propositional Schemata

(n ≥ 0) ∧ p0 ∧
n ≥ 0, p0 ,

∧n

(1)

i=1 (¬pi−1

∧n

i=1 (¬pi−1

∨ pi ) ∧ ¬pn

∧ pi ), ¬pn

n ̸= 0

∧n−1
i=1

n≥1
(¬pi−1 ∨ pi )

¬pn−1 ∨ pn
¬pn−1

n<1

×

pn

	(1)
n ̸= n

×
Figure 2: A Simple Example of Closed Tableau

5.3.1 A Simple Example

∧
Let ϕ be the following formula: (n ≥ 0) ∧ p0 ∧ ni=1 (¬pi−1 ∨ pi ) ∧ ¬pn .
We construct a tableau for ϕ. First the ∧-rule applies to transform the conjunction into
a set of schemata. The closure rule applies on
∧ pn and p0 , yielding the constraint n ̸= 0.
Then the iteration rule applies on the schema ni=1 (¬pi−1 ∨ pi ), yielding two branches. The
ﬁrst one ∧
corresponds to the case in which the iteration is non empty and can be unfolded,
yielding n−1
i=1 (¬pi−1 ∨ pi ) and ¬pn−1 ∨ pn and the second one corresponds to the case where
the iteration is empty (hence true), yielding the constraint n < 1. The latter branch can
be closed immediately due to the constraints n ≥ 0 and n ̸= 0. In the former branch,
the ∨-rule applies on the formula ¬pn−1 ∨ pn , yielding two branches with ¬pn−1 and pn
respectively. The closure rule applies on the latter one, yielding the unsatisﬁable constraint
n ̸= n hence the branch can be closed. The last remaining branch loops on the initial one,
with the shift n 7→ n − 1. The obtained tableau is depicted in Figure 2. Closed leaves (resp.
blocked leaves looping on α) are marked by × (resp. 	(α)). Only new (w.r.t. the previous
block) formulae are presented in the blocks.
5.3.2 n-Bit Adder

In this section we provide a slightly more complicated example. We use stab to prove a
simple property of the n-bit Adder deﬁned in the Introduction. We aim at proving that
A + 0 = A. A SAT-solver can easily refute this formula for a ﬁxed n (say n = 10). We
prove it for all n ∈ N. This simple example has been chosen for the sake of readability and
conciseness, notice that commutativity or associativity of the n-bit adder could be proven
too (see Section 5.7).
∧
We express the fact that the∧ second operand is null: ∨ni=1 ¬qi , and the fact that the
result equals the ﬁrst operand: ni=1 (pi ⇔ ri ), which gives ni=1 (pi ⊕ ri ) by refutation. So
633

Aravantinos, Caferra & Peltier

∧n

n≥1

∧n

¬c1

n≥1
∧n−1
i=1 Sumi

(1)

i=1 Sumi

i=1 Carryi

∨n
i=1

∧n

i=1

¬c1
Sumn

i=1 Carryi
∧n−1
i=1 ¬qi

¬qi

n≥1
∧n−1
i=1 Sumi
∨n−1
i=1 pi ⊕ ri
∧n−1
i=1 Carryi
∧n−1
i=1 ¬qi

pn ⊕ rn

∧n−1

pi ⊕ ri

Carryn
¬qn

¬c1
Sumn
Carryn
¬qn

	 (1)
¬rn

cn

pn

¬qn

rn

¬pn

cn

(2)

¬qn

(2’)

(2)

n−1≥1
∧n−2
i=1 Carryi
Carryn−1

n≥1

n−1<1

cn

¬c1

×

n≥2
(pn−1 ∧ qn−1 ) ∨ (cn−1 ∧ pn−1 ) ∨ (cn−1 ∧ qn−1 )

pn−1 ∧ qn−1

cn−1 ∧ pn−1

×
cn−1

pn−1

cn−1 ∧ qn−1

×

¬rn−1

	 (2)

Figure 3: A Closed Tableau for A + 0 = A

we want to prove that Adder ∧
schema is regular.

∧n

i=1 ¬qi

∧

∨n

i=1 (pi

⊕ ri ) is unsatisﬁable. Notice that this

The corresponding tableau is sketched in Figure 3. Sequences of propositional extension
rules are not detailed.
634

Decidability and Undecidability Results for Propositional Schemata

Explanations.
The ﬁrst big step decomposes
∨
∨ all the iterations. The branching is due
to ni=1 pi ⊕ ri : ﬁrst we have pn ⊕ rn ,∧then n−1
i . The right branch loops after a
i=1 pi ⊕ r∧
n
n−1
few steps as all iterated conjunctions i=1 . . . contain i=1
. . . The left one is extended
by propositional rules (the reader can easily check that Sumn , Carryn , pn ⊕ rn and ¬qn
indeed lead to the presented branches, notice that cn must hold, otherwise we would have
pn ⇔ rn ).
In (2) we start by decomposing all iterations a second time. Iterations are aligned on
[1, n − 1] so they all introduce the same constraints i.e. either n − 1 ≥ 1 (ﬁrst branch)
or n − 1 < 1 (second branch). In the second case, the introduced constraint implies that
n = 1, thus cn = c1 which closes the branch. In the ﬁrst case we decompose Carryn−1 and
consider the various cases. Two of them are
∧ trivially discarded as they imply qn−1 , whereas
we easily obtain ¬qn−1 by an unfolding of ni=1 ¬qi . It only remains one case which is easily
seen to loop on (2). The branch (2′ ) is very similar to (2).
5.4 Soundness and Completeness
A leaf is irreducible if no extension rule applies to it. A derivation is a (possibly inﬁnite)
sequence of tableaux (Tι )ι∈I s.t. I is either [0, κ] for some κ ≥ 0, or N and s.t. for all
ι ∈ I \ {0}, Tι is obtained from Tι−1 by applying one of the extension rules. A derivation is
fair if either there is ι ∈ I s.t. Tι contains an irreducible not closed leaf or if for all ι ∈ I
and every not closed and not blocked leaf N in Tι there is λ ≥ ι s.t. a rule is applied on N
in Tλ (i.e. no leaf can be “freezed”).
Definition 5.16 (Tableau Semantics)
For every node N in a tableau T , ΦT (N ) is interpreted as the conjunction of its elements.
T is satisﬁed in an interpretation I iﬀ there exists a leaf N in T s.t. I |= ΦT (N ).
Lemma 5.17
If T ′ is a tableau obtained by applying one of the extension rules on a leaf N of a tableau
T then I |= ΦT (N ) iﬀ there exists a leaf N ′ of T ′ s.t. N ′ is a child of N in T ′ and
I |= ΦT ′ (N ′ ) (i.e. the rules are sound and invertible).
Proof
Obvious, by inspection of the extension rules.



Lemma 5.18
If a leaf N in T is irreducible and not closed then T is satisﬁable.
Proof
def
Let Ψ be the set of arithmetic formulae in ΦT (N ) and Φ = ΦT (N ) \ Ψ. As N is not closed
Ψ is satisﬁable (by deﬁnition), so let σ be a solution of Ψ. If Φ contains a formula ϕ that
is not a literal, one of the extension rules applies and deletes ϕ, which is impossible. Let
cT (N ) be the number of pairs pa , ¬pb ∈ ΦT (N ) s.t. there is an interpretation I validating
Ψ s.t. JaKI = JbKI . If cT (N ) ̸= 0, then the closure rule applies on pa , pb which is impossible.
Hence cT (N ) = 0 and in particular this implies that Φσ is propositionally satisﬁable (i.e.
contains no pair of complementary literals). Thus ΦT (N )σ is satisﬁable and by deﬁnition
T is satisﬁable.

635

Aravantinos, Caferra & Peltier

Theorem 5.19 (Soundness and Completeness w.r.t. Satisfiability)
Let (Tκ )κ∈I be a derivation.
• If there exists ι ∈ I s.t. Tι contains an irreducible, not closed leaf then T0 is satisﬁable.
• If the derivation is fair and if T0 is satisﬁable then there exist ι ∈ I and a leaf in Tι
that is irreducible and neither closed nor blocked.
Proof
The ﬁrst item (i.e. soundness) follows from Lemmata 5.17 and 5.18.
We now prove that the procedure is complete w.r.t. satisﬁability (the second item). Let
I be an interpretation and ϕ a schema. We deﬁne mI (ϕ) as follows:
• mI (ϕ) = 0 if ϕ is an arithmetic atom (i.e. an atom of the form . . . < . . .).
def

• mI (ϕ) = 1 if ϕ is an indexed proposition or its negation, or ϕ is ⊤ or ⊥.
def

• mI (ϕ1 ⋆ ϕ2 ) = mI (ϕ1 ) + mI (ϕ2 ) if ⋆ ∈ {∨, ∧}.
def

• mI (Πbi=a ϕ) = 2 if JbKI < JaKI
def

∧ ∨
def
• mI (Πbi=a ϕ) = β − α + 2 + Σβι=α mI[ι/ı] (ϕ) where Π ∈ { , }, α = JaKI , β = JbKI , and
β ≥ α.
If Φ is a set, then mI (Φ) = {mI (ϕ) | ϕ ∈ Φ}. If T is a tableau and N is a leaf in T then
def
mI (N, T ) = (mI (ΦT (N )), cT (N )) where cT (N ) is deﬁned in the proof of Lemma 5.18. This
measure is ordered using the multiset and lexicographic extensions of the usual ordering on
natural numbers. Thus, it is obviously well-founded. We need the following:
def

Lemma 5.20
Let I be an interpretation. Let T be a tableau. If T ′ is deduced from T by applying
an extension rule on a leaf N s.t. I |= ΦT (N ), then for every child N ′ of N in T ′ s.t.
I |= ΦT ′ (N ′ ), we have mI (N ′ , T ′ ) < mI (N, T ).
Proof
All the rules except the iteration rule and the closure rule replace a formula by simpler ones,
hence it is easy to see that mI (ΦT (N )) decreases. The iteration rules replace an iteration of
length ι either by ⊤ or by a disjunction/conjunction of an iterated disjunction/conjunction
of length ι − 1, and a smaller formula. Since ι > ι − 1, mI (ΦT (N )) decreases. The closure
rule does not aﬀect mI (ΦT (N )) but obviously decreases cT (N ).

Let I be a model of T0 . By Proposition 5.6, we can assume that I is minimal w.r.t the
ordering < introduced in Deﬁnition 5.4.
By Lemma 5.17, for all ι ∈ I, Tι contains a leaf Nι s.t. I |= ΦTι (Nι ). Let κ ∈ I s.t.
mI (Nκ , Tκ ) is minimal (κ exists since mI (Ni , Ti ) is well-founded). Assume a rule is applied
on Nκ in the derivation, on some tableau Tλ . By Lemma 5.17 there is a child N ′ of Nκ
s.t. I |= ΦTλ (N ′ ). By Lemma 5.20 we have mI (N ′ , Tλ ) < mI (Nk , Tκ ) which is impossible.
Thus no rule is applied on Nκ . Assume that Nκ is blocked. Then there exists a node N ′
s.t. Nκ loops on N ′ . By Deﬁnition 5.4 there exists an interpretation J s.t. J |= N ′ and
636

Decidability and Undecidability Results for Propositional Schemata

J <V I. But then by Lemma 5.17 (“only if” implication), J |= T0 , which contradicts the
minimality of I. Since the derivation is fair, Nκ is irreducible (or there is another leaf that
is irreducible). Furthermore, Nκ cannot be closed since it is satisﬁable (I |= ΦTκ (Nκ )).
It is worth emphasizing that stab is sound and complete (w.r.t. satisﬁability) for any
schema, not only for bound-linear or regular ones. But the termination result in the next
section only holds for regular schemata.
5.5 Termination on Regular Schemata
We consider the following strategy ST for applying the extension rules:
• The propositional extension rules, the looping and closure rules are applied as soon
as possible on all leaves, with the highest priority. These rules obviously terminate on
any schema.
• The iteration rules are applied only on iterations of maximal length (w.r.t. the natural
ordering on arithmetic expressions). For instance if we have the ∧
schema
∧n partial
∨n−1
n
p
∨
q
then
the
iteration
rules
will
only
apply
on
the
ﬁrst
iteration
i
j
i=1
j=1
i=1 pi .
• The relation ⊇s introduced in Section 5.2 is used to block looping nodes.
Theorem 5.21
ST terminates on every regular schema.
Proof
Let α, β, γ, δ ∈ Z and ϕ be a regular schema aligned on [α, n − β], of propagation limits γ, δ.
Assume that an inﬁnite branch is constructed. By deﬁnition of the strategy, after some
time, the κ last ranks of every iteration have been unfolded by the iteration rules. Thus all
the remaining iterations are of the form Πn−β−κ
ϕ′ and we have the arithmetic constraint
i=α
n − β − κ − α + 1 ≥ 0, i.e. n ≥ β + κ + α − 1.
From now on, we only consider nodes that are irreducible w.r.t. propositional rules.
We show that a ﬁnite set of formulae are generated by stab, up to a shift on n. As a
consequence the looping rule must apply, at worst when all possible formulae have been
generated.
The arithmetic formulae occurring in the initial formula must be of the form µ.n > ν
or µ.n < ν. After the last κ ranks have been unfolded, the constraint n ≥ β + κ + α − 1
must have been added. Thus if κ is suﬃciently big, µ.n > ν is equivalent to ⊤ and µ.n < ν
is equivalent to ⊥. Thus every arithmetic formula occurring in the initial formula is either
false or redundant w.r.t. n ≥ β + κ + α − 1. The remaining arithmetic formulae must
have been introduced by the closure rule (since the iterations contain no occurrence of <).
They are necessarily of the form a ̸= b where a, b are arithmetic expressions (appearing
as indices in some formula of the derivation). If a, b both contain n, or if a, b ∈ Z then
a ̸= b is equivalent either to ⊥ or to ⊤. Thus we only consider the case in which a contains
n and b ∈ Z. If a occurs in the initial formula then it must be of the form µ.n + ν for
µ, ν ∈ Z. Since n ≥ β + κ + α − 1, if κ is suﬃciently big, the disequation µ.n + ν ̸= b
must be false. If a did not occur in the initial formula then it must come from the (κ − ι)th
unfolding of some iteration, for some ι ∈ [0, κ − 1]. Since (by deﬁnition of a regular schema)
637

Aravantinos, Caferra & Peltier

the indices are of the form i + λ, where λ ∈ [γ, δ], the disequation is actually of the form
n − β − κ + ι + λ ̸= b, where λ ∈ [γ, δ], ι ∈ [0, κ − 1] (since the iteration counter i may be
replaced by n − β, n − β − 1, . . . , n − β − κ + 1) and b occurs in the initial formula. If the
previous equation is not equivalent to ⊤, then, since we have the constraint n ≥ β +κ+α−1,
we must have ι ∈ [0, b − α + 1 − λ]. Hence there are ﬁnitely many such formulae, up to the
translation n 7→ n − κ.
Now, consider the non arithmetic formulae occurring in the branch. These schemata
must be either iterations or literals (by irreducibility w.r.t. the propositional extension
rules).
′
All the iterations are of the form Πn−β−κ
ϕ′ , where Πn−β
i=α
i=α ϕ is an iteration occurring in
the initial formula. Obviously, the number of such iterations is ﬁnite up to the translation
n 7→ n − κ.
The literals occurring in the branch (but not in the scope of an iteration) are either
literals of the initial schema or literals introduced by previous applications of the iteration
rules. The former are indexed by expressions of the form µ × n + ν for some µ, ν ∈ Z and
the latter by n − β − κ + ϵ, where ϵ ∈ [γ + 1, δ + κ].
If a literal is indexed by an expression µ × n + ν that is outside [α + γ, n − β − κ + δ], then
it must be pure in every iteration, hence (by irreducibility w.r.t. the closure rule) must be
pure in the node. Actually, if κ is large enough then, by the above arithmetic constraints,
µ × n + ν cannot be in [α + γ, n − β − κ + δ] if µ ̸= 0. Indeed, if µ is negative, then it
suﬃces to take κ > α+γ−ν
− β − α + 1 to ensure µ × n + ν < α + γ, otherwise κ ≥ δ − β − ν
µ
is enough to have µ × n + ν > n − β − κ + δ (as µ, n ≥ 1). Thus every literal indexed by
integer terms of this form are pure, since by deﬁnition its index cannot be uniﬁable with an
index occurring in an iteration (after unfolding).
Similarly literals indexed by expressions of the form n − β − κ + ϵ where ϵ > δ are pure,
thus we may assume that ϵ ∈ [γ, δ]. Consequently there are ﬁnitely many such literals up
to the shift n 7→ n − κ.
This implies that the number of possible schemata obtained after κ unfolding steps is
ﬁnite, up to a translation of n. By the pigeonhole principle, the looping rule necessarily
applies at some point in the branch, which contradicts our initial assumption that an inﬁnite
branch is constructed.

Termination of the strategy also ensures fairness:
Lemma 5.22
Any derivation constructed by ST (applied until irreducibility) is fair.
Proof
Let (Tι )ι∈I be a derivation constructed by ST. Since ST terminates, there cannot be any
inﬁnite derivation, thus I is necessarily of the form [0, κ] for some κ ∈ N. By deﬁnition,
every node in Tκ is either blocked or closed or irreducible (the strategy is applied until
irreducibility). If Tκ contains a not closed irreducible leaf then the proof is completed (by
deﬁnition of the notion of fairness). Otherwise, consider Tι with ι ≤ κ. Let then N be a not
irreducible, not closed and not blocked leaf occurring in Tι . Assume that there is no λ ≥ ι
s.t. a rule is applied on N in Tλ (which would contradict our deﬁnition of fairness). This
means that no extension can possibly aﬀect N , thus N must also occur in the ﬁnal tableau
638

Decidability and Undecidability Results for Propositional Schemata

Tκ (and is labeled by the same set of schemata than in Tι ). Thus N must be not closed and
not irreducible. Moreover it cannot be blocked in Tκ , since no rule can aﬀect the nodes on
the branch behind N . But this is impossible since the nodes in Tκ must be either blocked
or closed or irreducible.

As an immediate corollary, we have the following:
Theorem 5.23
The satisﬁability problem is decidable for bound-linear schemata.
Proof
By Theorems 4.12 and 4.13, every bound-linear schema can be transformed into a satequivalent regular one. Theorem 5.21 shows that stab terminates on every regular schema,
hence by Theorem 5.19 and Lemma 5.22, stab can be used to decide the satisﬁability
problem for regular schemata.

A ﬁne analysis of the previous termination proof ensures that we can solve the satisﬁability problem for regular schemata in exponential time (if natural numbers are written in
unary notation). As we have seen furthermore (Theorem 4.9) that the translation of boundlinear schemata into regular ones was exponential, we can conclude that the satisﬁability
problem for bound-linear schemata can be solved in double exponential time.
5.6 Model Building
The existence of a non closed irreducible branch ensures that the root schema is satisﬁable,
as shown in Theorem 4.12. The arithmetic constraints in the branch specify the possible
values of the parameter. The remaining formulae must be literals, since the extension rules
apply on any complex formula (in particular, there can be no iteration schema). These
literals specify the truth value of propositional variables exactly as in the usual case of
propositional logic (the value of the propositional variables that do not appear in the branch
may be chosen arbitrarily). Since the branch is not closed, it cannot contain any pair of
complementary literals.
We illustrate this construction by a simple example. We consider the following tableau:
pn , ¬q2 , ¬r1 ,

n ≥ 1,

∨n−1
i=1

∨n

i=1 (¬pi

∧ qi ∧ ri )

n ≥ 1, ¬pn ∧ qn ∧ rn

(¬pi ∧ qi ∧ ri )

¬pn , qn , rn
n ≥ 1,

∨n−2
i=1

¬pi ∧ qi ∧ ri

...

n − 1 ≥ 1, ¬pn−1 ∧ qn−1 ∧ rn−1
¬pn−1 , qn−1 , rn−1
n − 1 ̸= n, n − 1 ̸= 2, n − 1 ̸= 1

(1)
639

n ̸= n

×

Aravantinos, Caferra & Peltier

The branch (1) is irreducible. It contains the following formulae: pn , ¬q2 , ¬r1 , n−1 ≥ 1,
¬pn−1 , qn−1 , rn−1 , n − 1 ̸= n, n − 1 ̸= 2, n − 1 ̸= 1. The value of n can be determined by
ﬁnding a solution to the above arithmetic constraints. We choose for instance the solution
n = 4. After instantiation we get the remaining formulae: {p4 , ¬q2 , ¬r1 , ¬p3 , q3 , r3 }, which
gives for instance the following interpretation of p, q and r: pκ is true iﬀ κ = 4 and qκ , rκ
are true iﬀ κ = 3. It is easy to check that the obtained interpretation satisﬁes the initial
schema.
A possible extension of this simple algorithm would be, from a given tableau, to compute
a symbolic representation of the whole set of models of the root schema. This set is inﬁnite
and must be deﬁned by induction. The closed irreducible branches correspond to concrete
models, or base cases, whereas the loops correspond to inductive construction rules. These
rules take a model I and construct a new model J of a strictly greater cardinality (the
values of the parameters increase strictly). This would require to deﬁne a formal language
for denoting sets of interpretations (one could use, e.g., automata recognizing sequences of
tuples of Boolean values).
5.7 The System
The decision procedure has been implemented and the program (called RegStab) is freely
available on the web page http://regstab.forge.ocamlcore.org/. It is written in OCaml
and was successfully tested on MacOSX (10.5), Win32 (Windows XP SP3) and GNU Linux
(Ubuntu 9.04) x86 platforms. The system comes with a manual including installation and
usage instructions and a description of the input syntax. Functions can be deﬁned to make
the input ﬁle more readable (see Sum(i) and Carry(i) below). Here is an input ﬁle for the
adder example in Section 5.3.2.
// A+0=A
let Sum(i)
let Carry(i)
let Adder
let NullB
let Conclusion

:=
:=
:=
:=
:=

S_i <-> (A_i (+) B_i (+) C_i) in
C_i+1 <-> (A_i /\ B_i \/ C_i /\ A_i \/ C_i /\ B_i) in
/\i=1..n (Sum(i) /\ Carry(i)) /\ ~C_1 in
/\i=1..n ~B_i in
\/i=1..n (A_i (+) S_i) in

Adder() /\ NullB() /\ Conclusion()
The software simply prints the status of the schema (satisﬁable or unsatisﬁable). Options
are provided to get more information about the search space (number of inference rules,
depth of unfolding etc.), see the manual for details. An additional tool is oﬀered to expand
the schema into a propositional formula in DIMACS format (by ﬁxing the value of n).
Figure 4 gives some examples of problems that can be solved by RegStab and the
corresponding running times (please refer to the distribution for input ﬁles and additional
information).
Here is an example of output, proving that 0 is a neutral element for the carry-propagate
adder. We ran the system in verbose mode, in which it prints some useful information about
the search: number of application of extension rules, number of closed and looping leaves,
unfolding depth and set of lemmata (companion nodes).
640

Decidability and Undecidability Results for Propositional Schemata

Ripple-carry adder
x+0=x
commutativity
associativity
3+4=7
x + y = z1 ∧ x + y = z2 ⇒ z1 = z2
Carry-propagate adder
x+0=x
commutativity
associativity
equivalence between two diﬀerent deﬁnitions of the same adder
equivalence with the ripple-carry adder
Comparisons between bit-vectors
x≥0
Symmetry of ≤ (i.e. x ≤ y ∧ x ≥ y ⇒ x = y)
Totality of ≤ (i.e. x > y ∨ x ≤ y)
Transitivity of ≤
1≤2
Presburger arithmetic with bit vectors
x+y ≥x
x1 ≤ x2 ≤ x3 ⇒ x1 + y ≤ x2 + y ≤ x3 + y
x1 ≤ x2 ∧ y1 ≤ y2 ⇒ x1 + y1 ≤ x2 + y2
x1 ≤ x2 ≤ x3 ∧ y1 ≤ y2 ≤ y3 ⇒ x1 + y1 ≤ x2 + y2 ≤ x3 + y3
1≤x+y ≤5∧x≥3∧y ≥4
same but with iterations factorized
Other
automata
inclusion
∨n
∧
Pi ∧ ni=1 ¬Pi
i=1 ∧
P1 ∧ ni=1 (Pi ⇒ Pi + 1) ∧ ¬Pn+1 |n ≥ 0
model checking of some safety property
Figure 4: Some Experimental Results

641

0.017s
0.267s
28.902s
2.719s
0.490s
0.016s
0.165s
8.522s
0.164s
0.194s
0.004s
0.009s
0.006s
0.011s
0.010s
0.026s
1m42s
2.949s
46m57s
7m9s
2m14s
2.324s
0.001s
0.001s
5.251s

Aravantinos, Caferra & Peltier

Conjecture:
(((/\i=1..n ((S_i <-> ((A_i (+) B_i) (+) C_i)) /\
(C_i+1 <-> (((A_i /\ B_i) \/ (C_i /\ A_i))
\/ (C_i /\ B_i))))) /\ ~C_1) /\ (/\i=1..n ~B_i)) /\
(\/i=1..n (A_i (+) S_i))

Applications of tableau rules:
/\: 67
\/: 84
(+): 38
<->: 32
->: 0
Iterated /\: 12
Iterated \/: 3
------Total propositional rules: 221
Total iterated rules:
15
Number of closed leaves: 137
Number of looping leaves: 30
Number of lemmas:
4
Maximum number of unfoldings: 3
(if this number is surprising, notice that the tableau is
constructed depth-first)
Lemmas:
[\/i=1..n (A_i (+) S_i) ; /\i=1..n ((S_i <-> ((A_i (+) B_i) (+) C_i))
/\ (C_i+1 <-> (((A_i /\ B_i) \/ (C_i /\ A_i)) \/ (C_i /\ B_i)))) ;
/\i=1..n ~B_i ; ~C_1]
[\/i=1..n-1 (A_i (+) S_i) ; /\i=1..n-1 ((S_i <-> ((A_i (+) B_i) (+) C_i))
/\ (C_i+1 <-> (((A_i /\ B_i) \/ (C_i /\ A_i)) \/ (C_i /\ B_i)))) ;
/\i=1..n-1 ~B_i ; ~C_n ; ~C_1] (n > 0)
[/\i=1..n-2 ((S_i <-> ((A_i (+) B_i) (+) C_i)) /\ (C_i+1 <-> (((A_i /\ B_i)
\/ (C_i /\ A_i)) \/ (C_i /\ B_i)))) ;
/\i=1..n-2 ~B_i ; C_n-1 ; ~C_1] (n > 1)
[\/i=1..n-2 (A_i (+) S_i) ; /\i=1..n-2 ((S_i <-> ((A_i (+) B_i) (+) C_i))
/\ (C_i+1 <-> (((A_i /\ B_i) \/ (C_i /\ A_i)) \/ (C_i /\ B_i)))) ;
/\i=1..n-2 ~B_i ; C_n-1 ; ~C_1] (n > 1)
UNSATISFIABLE

642

Decidability and Undecidability Results for Propositional Schemata

6. Undecidability Results
We provide some undecidability results for two natural extensions of the class of regular
schemata.
6.1 Homothetic Transformations on the Iteration Counters
We consider the class of schemata Ch deﬁned as follows.
Definition 6.1
Ch (h stands for “homothetic”) is the set of schemata ϕ satisfying the following properties:
• ϕ contains at most one parameter n.
∧
∨
• Every iteration in ϕ is of the form ni=1 ϕ or ni=1 ϕ, where:
– ϕ contains no iteration.
– Every atomic formula in ϕ belongs to {pi , p2i , pi±1 , p2i±1 } where p is a variable.
• The atomic formulae occurring in ϕ but not in the scope of an iteration are of the
form p0 or pn where p is a variable9 .
Ch is rather simple and very close to the class of regular schemata. There is only one
parameter n, all the iterations have the same bounds 1 and n, there is no nested iteration
and the indices of the symbol in P must be aﬃne images of the iteration counter. The only
diﬀerence with the regular class is that, in Ch the coeﬃcient of the iteration counter in the
indexed variables may be equal to 2 whereas it must be equal to 0 or 1 in regular schemata.
Thus regular schemata only contain translations of the iteration counter, whereas Ch may
involve (very simple) homothetic transformations.
Due to this closeness, one could expect that the satisﬁability problem is decidable for
Ch , but the next theorem shows that this is not the case.
Theorem 6.2
The set of unsatisﬁable formulae in Ch is not recursively enumerable.
The proof of Theorem 6.2 is diﬃcult and the remaining part of this section is devoted
to it. More precisely, we shall prove that the Post correspondence problem can be encoded
into Ch . Notice that this problem is easily encoded with general schemata (Aravantinos
et al., 2009b), whereas, here, the whole diﬃculty of the proof lies in the strong restrictions
imposed by Ch . Observe that the diﬃcult proof is really worth it as one would easily believe
that just allowing multiplication by a constant is an unsigniﬁcant change.
6.1.1 Notations
We ﬁrst recall some basic deﬁnitions and introduce some useful notations. Let A be an
alphabet. Let κ be a natural number. Let a = (a1 , . . . , aκ ) and b = (b1 , . . . , bκ ) be two
sequences of words in A∗ . If w ∈ {a, b} and ι ∈ [1, κ], |wι | denotes the length of wι and wιλ
denotes the λ-th character of the word wι (1 ≤ λ ≤ |wι |).
9. Notice that p0 and pn can occur in the scope of a negation.

643

Aravantinos, Caferra & Peltier

If ∆ = (∆1 , . . . , ∆ι ) is a sequence of indices in [1, κ] and if w = (w1 , . . . , wκ ) is a κ-tuple
of words in A∗ (where w ∈ {a, b}) we denote by w∆ the word w∆1 . · · · .w∆ι (where “.”
denotes the concatenation operator). A solution of the Post correspondence problem is a
sequence ∆ s.t. a∆ = b∆ . The witness of this solution is the word a∆ .
For technical convenience, we assume (this is obviously not restrictive) that κ > 1,
∆ι = κ, ∆λ ̸= κ if λ < ι and that aκ = bκ = ⊥ where ⊥ is a special character (not occurring
in a1 , . . . , aι−1 , b1 , . . . , bι−1 ) denoting the end of the sequence.
6.1.2 Overview of the Encoding
The intuition behind the encoding is the following. We show how to encode any instance
of the problem into a schema ϕ so that ϕ is satisﬁable iﬀ this instance has a solution. More
precisely, we construct ϕ of parameter n s.t. for all κ ∈ N , ϕ[κ/n] is satisﬁable iﬀ there is
a solution of length κ.
We ﬁrst present the encoding used to represent the potential solutions a∆ and b∆ ; then
we will see how to check that those are really solutions. We represent the potential solution
w∆ (where w = a, b) by a one-dimensional array of length n. More precisely, we do not store
the characters themselves but rather, for each character, a pair containing the index ∆ν of
the word w∆ν in which it occurs and its position in this word (as we shall see this is useful to
ﬁnd the next character in w∆ ). For instance the ﬁrst index should contain the pair (∆1 , 1)
(ﬁrst word, ﬁrst character). Then the next index contains either (∆1 , 2) (if |w∆1 | > 1, ﬁrst
word, second character) or (∆2 , 1) (if |w∆1 | = 1, second word, ﬁrst character).
For example, if A = {∗, ◦, ⋆}, a = (∗◦, ⋆) and ∆ = (1, 2), then the obtained array would
be the following one:
Values
Indices

(1, 1)
1

(1, 2)
2

(2, 1)
3

However, the word w∆ is not stored into consecutive indices in the array. Indeed,
as we shall see, we also need to store, for each character w∆λ of the witness, the indices
∆λ+1 , . . . , ∆ι of the remaining words, occurring after w∆λ in w∆ . This sequence is called the
tail of the potential solution. Since the length of this sequence is unbounded, it cannot be
encoded simply by indexed propositions: it must be stored into the array and the simplest
solution is to store these indices just after the character itself. Notice that only the indices
of words are stored in the tail i.e. there is no character position. Thus we get:
Values
Indices

(1, 1)
1

2
2

(1, 2)
3

2
4

(2, 1)
5

The easiest way to proceed would be to store the ﬁrst character of the witness at position
0, the indices of the remaining words at position 1, 2, . . . , ι, then the second character of
the witness at position ι + 1 etc. That way, the λ-th character of the witness would be
stored at position (λ − 1) × (ι + 1) and the following characters in the sequence at positions
(λ − 1) × (ι + 1) + 1, . . . , (λ − 1) × (ι + 1) + ι. For any character stored in an index λ, the next
character would be stored at the index λ + ι + 1. But this simple solution is not suitable
because it is outside the considered class. Indeed, it requires the use of another parameter
ι (the ﬁrst parameter being n: the length of the array) and also the use of this parameter
644

Decidability and Undecidability Results for Propositional Schemata

in the indices (to relate the character stored in index λ to the one at index λ + ι), which is
forbidden in the class Ch .
Thus we need to ﬁnd another encoding of the previous array. The idea is to store the ﬁrst
character at some index µ (where µ is assumed to be greater than ι), the second character
at the index 2 × µ, . . . and more generally the λ-th character at the index µ × 2λ−1 . The
tail of the sequence is then stored at the indices (µ + 1) × 2λ−1 , . . . , (µ + ι) × 2λ−1 . This
encoding ensures that the index of the next character after the one at index i is simply 2.i,
and such homethetic transformations are precisely those allowed for the indices in Ch .
Finally, the array corresponding to our recurrent example is the following one (with
µ = 2):
Values
Indices

1

(1, 1)
2

2
3

(1, 2)
4

2
5

6

7

(2, 1)
8

The witness is obtained by considering the characters stored at the indices 2,4 (= 2 × 2)
and 8 (= 2×22 ), namely ∗ (ﬁrst character of the ﬁrst word), ◦ (ﬁrst word, second character),
and ⋆ (second word, ﬁrst character). Obviously there are “holes” in the array, they are
simply ignored.
6.1.3 The Signature
The array is encoded by two indexed propositions: car(w, ν, λ) and t(w, ν) (t stands for
“tail”) where w ∈ {a, b}, 1 ≤ ν ≤ κ, 1 ≤ λ ≤ |wν |. The intuition behind car(w, ν, λ)l is that
it holds iﬀ the index l in the array corresponding to w∆ contains the pair (ν, λ) (representing
the character wνλ ). t(w, ν)l states that the index l of the array corresponding to w∆ contains
ν.
6.1.4 Formal Definition of the Encoding
Let n be a variable (intended to denote the unique parameter of the schema).
As explained in the previous section, we store the characters in an array, at the indices
µ, 2µ, 4µ, etc. Intuitively, µ should be encoded as another parameter, but only one parameter n is allowed. However, we can encode µ with a new proposition symbol in P. We ﬁrst
deﬁne two symbols p, q s.t. pν holds iﬀ ν = µ and s.t. qν holds iﬀ ν ∈ [0, µ − 1]. The ﬁrst
schema deﬁnes q in such a way that it holds exactly on an interval of the form [0, µ − 1]:
q0 ∧ ¬qn ∧

n
∧

(qi+1 ⇒ qi )

i=1

The last formula obviously implies that if qν holds for some ν ∈ [1..n] then it must also
hold for every λ ∈ [1..ν]. Then µ is simply the ﬁrst index ν such that qν does not hold (this
element necessarily exists, since qn does not hold).
The second schema deﬁnes p such that it holds exactly on the successor of the maximal
element of the interval (i.e. µ). Notice that due to the previous formula we must have µ ̸= 0
and µ ≤ n:
n
∧

[pi ⇔ (qi−1 ∧ ¬qi )]

i=1

645

Aravantinos, Caferra & Peltier

For the sake of clarity, we shall denote by (λ = µ) the atom pλ and by (λ < µ) the atom
qλ (this makes the formulae much more readable).
We then deﬁne a variable wt s.t. wtν holds iﬀ there exists λ ∈ N s.t. ν = µ.2λ : wt stands
for “witness”, because wtν holds iﬀ ν is the index of a character in the witness of a solution,
as explained before:
∧n

i=1 [((i

= µ) ⇒ wti ) ∧ ((i < µ) ⇒ ¬wti )

(1)

∧(¬(2i + 1 = µ) ⇒ ¬wt2i+1 ) ∧ (¬(2i < µ) ∧ ¬(2i = µ)) ⇒ (wti ⇔ wt2i )]
The ﬁrst line states that wtµ holds and that wti is false if i < µ. The second line deﬁnes
that value of wti for i > µ: wt2i+1 is always false (except if 2i + 1 = µ) and wt2i is equivalent
to wti if 2i > µ. By an easy induction on the set of natural numbers, these properties imply
that wtν holds iﬀ ∃λ.ν = µ.2λ . Notice the crucial use of the homothetic transformation
here.
The following formula states that an index cannot represent two distinct characters
(pairs) in the same sequence:
n
∧

(¬car(w, ν, λ)i ∨ ¬car(w, ν ′ , λ′ )i )

i=1

for every w ∈ {a, b}, (ν, ν ′ ) ∈ [1, κ]2 , λ ∈ [1, |wν |], λ′ ∈ [1, |wν ′ |] s.t. (ν, λ) ̸= (ν ′ , λ′ )
Similarly, we state that every index contains at most one word in each sequence:
n
∧

(¬t(w, ν)i ∨ ¬t(w, ν ′ )i ) for every w ∈ {a, b}, ν, ν ′ ∈ [1, κ]2 , ν ̸= ν ′

i=1

Both initial elements of the sequences corresponding to a and b must be of the form
(ν, 1) (ν is the same in both sequences and is distinct from κ, since the word |wκ | marks
the end of the sequence):
n
∧

((i = µ) ⇒ ∃ν ∈ [1, κ − 1](car(a, ν, 1)i ∧ car(b, ν, 1)i ))

i=1

We use existential quantiﬁcation over intervals of natural numbers for the sake of clarity,
but these quantiﬁers can be easily eliminated and transformed into ﬁnite (not iterated )
disjunctions.
The next formula deﬁnes e(w) to mark the end of the sequence corresponding to w.
e(w)l should hold iﬀ l is of the form µ.2λ for some λ > 0 and if the character stored at the
index l is the ﬁrst character of the word κ (remember that by convention aκ = bκ = ⊤ where
⊤ marks the end of the witness). Besides, we must ensure that the end of the sequence is
eventually reached i.e. that there exists an index l such that e(a)l and e(b)l both hold:
n
∨

(e(a)i ∧ e(b)i ) ∧

i=1

n
∧

((wti ∧ car(w, κ, 1)i ) ⇔ e(w)i )

i=1

646

(⋆)

Decidability and Undecidability Results for Propositional Schemata

for every w ∈ {a, b}
We also have to ensure that the two sequences (i.e. the words a∆ and b∆ ) are identical.
It suﬃces to check that for every index l s.t. wtl holds (i.e. for every index l of the form
µ × 2λ ), the character stored in l is the same in the sequences of a and b:
n
∧

(wti ⇒ (¬car(a, ν, λ)i ∨ ¬car(b, ν ′ , λ′ )i ))

(⋆)

i=1
′

for every ν, ν ′ ∈ [1, κ]2 , λ ∈ [1, |aν |], λ′ ∈ [1, |bν ′ |] s.t. aλν ̸= bλν ′

So far, we have ensured that at most one character and word index can be stored in
every index. We have deﬁned the starting point and the end of the two sequences and
ensured that the two represented words are identical. The next (and most diﬃcult) step is
to ensure that these sequences really encode two words of the form a∆ and b∆ respectively.
To this aim, we shall relate the value of the character stored in every index µ.2λ+1 to the
one stored in µ.2λ , to ensure that the former is really the successor of the latter in the
witness. Since each character c is represented by a pair (ν, ι) where ν denotes the index of
a word in w and ι is the position of c in wν , it is easy to ﬁnd the next character: if ι < |wν |
(i.e. if c is not the last character in wν ) then the next character is simply (ν, ι + 1) (same
word wν , next position ι + 1). If ι = |wν | (i.e. if c is the last character in wν ) then the next
character is (ν ′ , 1) where ν ′ denotes the next word index in the solution sequence (word wν ′ ,
ﬁrst position).
In order to determine the index word ν ′ we use the fact that (as explained in the
informal overview above) the remaining indices in the solution are stored in the index
µ.2λ + 1, µ.2λ + 2, . . .. Thus, we simply need to pick up the ﬁrst element of this sequence.
After checking that the character stored at µ.2λ+1 is the successor of the one in µ.2λ
it remains to ensure that the indices stored at µ.2λ+1 + 1, µ.2λ+1 + 2,. . . correspond to the
remaining part of the solution. If ι < |wν | then the sequence must actually be identical to
the one stored at µ.2λ + 1, µ.2λ + 2,. . . If ι = |wν | then the ﬁrst element of the sequence
must be deleted (since we have entered into a new word).
The next formula states that if an index l of the form µ.2λ (i.e. an index s.t. wtl holds)
contains a pair (ν, ι) and if wν contains more that ι characters then µ.2λ+1 should encode
the next character in the word wν , namely (ν, ι + 1). Moreover the tail of the sequence does
not change, which is expressed using the variable c(w)l (c stands for “copy”) that will be
speciﬁed thereafter:
n
∧

[(wti ∧ car(w, ν, λ)i ) ⇒ (car(w, ν, λ + 1)2i ∧ c(w)i+1 )]

(2)

i=1

for every w ∈ {a, b}, ν ∈ [1, κ], λ ∈ [1, |wν | − 1].
Now we deﬁne the formula encoding the copy of the tail. The most simple way to proceed
would be to copy the values stored into the indices l, l+1, . . . , l+µ−1 into 2l+1, . . . , 2l+µ−1.
Unfortunately this cannot be done in this simple way because expressions of the form l + j
647

Aravantinos, Caferra & Peltier

would be required in the indices, which is forbidden in our class (only ±1 can be added).
As explained before, we overcome this problem by copying the indices l + 1, . . . , l + µ − 1
into 2l + 2, 2l + 4, . . . , 2l + 2µ − 2, which can be done by doubling the iteration counter.
The indices 2l + 1, 2l + 3, . . . , 2l + 2µ − 1 are left empty (holes). This is not disturbing since
such empty indices will simply be ignored. An important consequence is that the length of
the sequence is doubled each time it is copied (we assume that the value of the parameter
n and the natural number µ are suﬃciently large to ensure that there is enough “space” in
the array).
This is expressed by the following formula:
n
∧

(c(w)i ⇒ [¬t(w, ν)2i−1 ∧ (t(w, ν)i ⇔ t(w, ν)2i ) ∧ (¬wti+1 ⇒ c(w)i+1 )])

(3)

i=1

for every ν ∈ [1, κ], w ∈ {a, b}
We illustrate this construction by an example. Let A = {∗, ◦, ⋆, ⋄}, a = (∗◦, ⋆, ⋄◦) and
∆ = (1, 2, 3). In the second line, we provide for every index l the pair (ν, ι) such that
car(a, ν, ι)l holds (if any). The third line gives the represented character (∗,◦,⋆ or ⋄). In the
fourth line we provide the integer ν such that t(a, ν)l holds. The ﬁfth line gives the value
of c(a). The indices between µ + 2 and 2µ are empty (we assume that µ = 3).
i
car
character
t
c(a)

µ
(1, 1)
∗

µ+1

µ+2

2
T

3
T

2µ
(1, 2)
◦

2µ + 1

2µ + 2

2

2µ + 3

2µ + 4

3

By formula (2) we must have cµ+1 . By formula (3), the value of c(a)µ+1 is propagated
to c(a)µ+2 ,. . . , c(a)2µ−1 (it is not propagated to c(a)2µ since wt2µ holds). Still by (2), if
c(a)l holds then we have t(a, ν)l ⇔ t(a, ν)2l , and the cells corresponding to odd indices are
left empty. Thus we get the array above.
If an index µ.2λ contains a pair (ν, ι) where |wν | = ι (such as 2µ in the previous example),
then one must proceed to the next word. To this aim, we need to know what is the ﬁrst
character of the next word (after the current one). Because of the holes introduced by the
special copying mechanism, the next word is not necessarily at index l+1. A simple solution
is to change the contents of the tail so that each element contains not only the index of a
word but also its ﬁrst character. This is stated by the following formula:
n
∧

[¬wti ⇒ (t(w, ν)i ⇒ car(w, ν, 1)i )]

(4)

i=1

Furthermore, we copy this character into all the holes preceding the element. As a
particular case we get what we wanted for the ﬁrst non-empty word.10 This is stated by
10. Notice that we could have as well copied the word’s index instead of its ﬁrst character, since the index
contains all the information we need to retrieve the corresponding character. However it will be useful
in the following to know that there is no word index stored in a particular cell, so we store only the
information that is useful for the problem we want to solve at this point, i.e. the ﬁrst character of the
word.

648

Decidability and Undecidability Results for Propositional Schemata

the following formula:
n
∧

[(¬wti−1 ∧ ¬wti ∧ ∀λ ∈ [1, κ] ¬t(w, λ)i−1 ) ⇒ (car(w, ν, 1)i ⇔ car(w, ν, 1)i−1 )]

(5)

i=1

for every ν ∈ [1, κ], w ∈ {a, b}
Now, if the pair stored in ι is (ν, |wν |) and if this word is not the ﬁnal word in the
sequence (i.e. e(w)ι does not hold) then one has to store into 2ι the ﬁrst character of the
next word, which is, due to the two previous formulae, the character represented by ι + 1.
The previous picture must thus be completed as follows:
i
car
character
t

µ
(1, 1)
∗

µ+1
(2, 1)
⋆
2

µ+2
(3, 1)
⋄
3

2µ
(1, 2)
◦

2µ + 1
(2, 1)
⋆

2µ + 2
(2, 1)
⋆
2

2µ + 3
(3, 1)
⋄

2µ + 4
(3, 1)
⋄
3

By the formula (4) above, if t(a, ν)i holds then car(a, ν, 1)i also holds. Then by the
formula (5), the value of car(a, ν, 1)l is recursively propagated to car(a, ν, 1)l−1 until we
have l − 1 = µ.2λ or t(a, ν)l−1 holds for some ν. Notice that a character is now stored in
every index l but only the characters in the indices µ.2λ form the witness.
Thanks to this trick, ﬁnding the next character after the one stored in µ.2λ is now
trivial: this is simply the one stored in µ.2λ + 1, which, by the previous formula, actually
corresponds to the ﬁrst position of the word stored in (µ + 1).2λ (of course, we also need to
check that the character is not ﬁnal). This is expressed by the following formula:
n
∧

[(wtl ∧ ¬e(w)l ∧ car(w, ν, |wν |)l ) ⇒ (car(w, λ, 1)2l ⇔ car(w, λ, 1)l+1 ) ∧ s(w)l+1 ]

l=1

for every ν, λ ∈ [1, κ], w ∈ {a, b}
The propositional variable s(w)l+1 (s stands for “shift”) indicates that the tail at 2l is
obtained by removing the ﬁrst word in the tail at l. This is done as follows: the indices
2l + 2, . . . , 2l + 2µ − 1 are obtained by copying the indices l + 1, . . . , l + µ − 1, except the
ﬁrst one, that is left empty. As for c(w), the indices 2l − 1, . . . , 2l + 2µ − 3 are empty. s(w)
is deﬁned by the three following formulae.
s(w) actually erases everything until it ﬁnds a non-empty index, which is expressed by
the ﬁrst formula: if s(w)l holds then the indices stored at 2l and 2l − 1 must be empty
(furthermore, we also check that the end of the tail has not been reached):
n
∧

(s(w)l ⇒ ¬wtl ∧ ¬t(w, ν)2l ∧ ¬t(w, ν)2l−1 ) for every ν ∈ [1, κ], w ∈ {a, b}

l=1

The second one propagates the erasure if the current index is empty:
649

(6)

Aravantinos, Caferra & Peltier

n
∧

[(s(w)l ∧ ¬wtl+1 ∧ ∀ν ∈ [1, κ] ¬t(w, ν)l ) ⇒ s(w)l+1 ] for every w ∈ {a, b}

(7)

l=1

The third one states that once we have reached a non-empty index then we go on by
copying everything (which is done by using the previous variable c(w)):
n
∧

(s(w)l ∧ ∃λ ∈ [1, κ] t(w, λ)l ⇒ c(w)l+1 ) for every w ∈ {a, b}

(8)

l=1

We illustrate this construction by showing how the erasure works on the previous example:

i
car
character
t

4µ
(2, 1)
⋆

i
car
character
t
c(a)
s(a)

2µ
(1, 2)
◦

4µ + 1
(3, 1)
⋄

4µ + 2
(3, 1)
⋄

2µ + 1
(2, 1)
⋆

2µ + 2
(2, 1)
⋆
2

T

T

4µ + 3
(3, 1)
⋄

4µ + 4
(3, 1)
⋄

2µ + 3
(3, 1)
⋄
T

2µ + 4
(3, 1)
⋄
3
T

4µ + 5
(3, 1)
⋄

4µ + 6
(3, 1)
⋄

4µ + 7
(3, 1)
⋄

4µ + 8
(3,1)
⋄
3

The character stored in 2µ is the last one of the ﬁrst word thus we have to remove the ﬁrst
word in the tail of the solution. As explained before, the character stored in 4µ is the same as
the one stored in 2µ+1, namely (2, 1), i.e. ⋆ (since we have car(a, ν, 1)4µ ⇔ car(a, ν, 1)2µ+1 ).
Furthermore, s(a)2µ+1 holds. This implies by (6) that the indices 4µ + 2 and 4µ + 1 of t
must be empty. Since t is empty for ι = 2µ + 1 (i.e. there is no ν such that t(a, ν)2µ+1
holds), the value of s(a)2µ+1 is propagated to s(a)2µ+2 , by (7). Thus by (6), the indices
4µ + 4 and 4µ + 3 of t must also be empty. This time, however, t(a, 2)2µ+2 holds. Thus
the value of s(a) is not propagated and c(a)2µ+3 must hold (by (8)). As before, this implies
that the remaining part of the sequence (i.e. the cells 2µ + 3, 2µ + 4 of t) is copied (in the
cells 4µ + 6, 4µ + 8, leaving the cells 4µ + 5, 4µ + 7 empty) until 4µ is reached. This implies
in particular that t(a, 3)4µ+8 holds (since t(a, 3)2µ+4 holds). Hence we have car(a, 3, 1)4µ+8 .
Since t is empty for l ∈ [4µ + 1, . . . , 4µ + 7], this value of car(a, 3, 1) is propagated to the
indices 4µ + 7, . . . , 4µ + 1 as explained before. We obtain the desired result, i.e. the ﬁrst
word in the sequence (namely 2) have been erased and the ﬁrst character of the next word
is stored into 4µ + 1.
Finally, in order to ensure that the obtained sequence is really a solution to the Post
correspondence problem, it only remains to check that the two sequences are identical, i.e.
that the words contained in µ + 1, · · · , 2µ − 1 are the same for both sequences a and b. To
this purpose we deﬁne a variable rl that is true iﬀ l < 2µ.
650

Decidability and Undecidability Results for Propositional Schemata

r0 ∧ ¬rn ∧

n
∧

[(rl ⇒ rl−1 ) ∧ (l = µ) ⇒ (r2l−1 ∧ ¬r2l )]

(⋆)

l=1
n
∧

[(rl ∧ ¬(l < µ)) ⇒ (t(a, ν)l ⇔ t(b, ν)l )]

(⋆)

l=1

for every ν ∈ [1, κ]
It is straightforward to check that the obtained formula is in Ch . The reader acquainted
with Post’s correspondence problem shall now be convinced that the obtained formula is
satisﬁable iﬀ there exists a solution to the above Post problem, and can thus skip the end
of this section. Otherwise we give in the following a sketch of the formal steps to this proof.
We denote by ϕ the conjunction of the above formulae, except the formulae marked (⋆).
We ﬁrst notice that ϕ is satisﬁable (for every value of n). Indeed, as explained before, the
formulae above impose that:
• There exists a unique natural number µ such that pν holds iﬀ ν = µ and qν holds iﬀ
ν ∈ [0, µ − 1].
• car(w, ν, λ) and t(w, ν) encode (partial) functions fw , gw mapping every index in [1, n]
to a pair (ν, λ) (where ν ∈ [1, κ], λ ∈ [1, |wν |]) and to a word index in [1, κ] respectively.
Moreover we must have fa (µ) = (ν, 1) and fb (µ) = (ν, 1) for some ν ∈ [1..κ − 1].
• wtν holds iﬀ there exists λ ∈ N s.t. ν = µ.2λ .
This obviously deﬁnes a partial interpretation. Then the remaining formulae in ϕ simply give the values of car(w, ν, λ)ι , t(w, ν)ι , c(w)ι , s(w)ι for ι ≥ 2µ. It is easy to check
that distinct formulae cannot give distinct values to the same propositional variable, hence
satisﬁability is guaranteed.
Let I be an interpretation of ϕ. Let ι ∈ [1..n]. We deﬁne the following sequences.
• hw (ι) is a sequence of word indices deﬁned as follows: If wtι+1 holds then hw (ι) is
def
empty. Otherwise, if gw (ι) = ν then hw (ι) = ν.hw (ι + 1) and if gw (ι) is undeﬁned
def
then hw (ι) = hw (ι + 1). Intuitively, hw (ι) is the sequence of word indices stored juste
after ι (i.e. the tail) ignoring empty cells.
• jw (ι) is a word deﬁned as follows: if ι > n then jw (ι) is empty. Otherwise, jw (ι) =
def
wνλ .jw (2ι) if fw (ι) is a pair (ν, λ) distinct from (κ, 1), jw (ι) = ⊤ if fw (ι) = (κ, 1) and
def
jw (ι) = jw (2ι) if fw (ι) is undeﬁned. jw (ι) denotes the word stored at the cells ι, 2ι . . .
in the array corresponding to w ((κ, 1) marks the end of the word).

def

• If fw (ι) = (ν, ν ′ ) then kw (ι) denotes the suﬃx of length |wν | − ν ′ + 1 of the word wν
(notice that by construction we must have ν ′ ≤ |ν|).
By deﬁnition of the copying/erasing mechanism above, if fw (µ.2λ ) is of the form (ν, |wν |)
(i.e. we are at the end of the word ν) then hw (µ.2λ ) = ν ′ .hw (µ.2λ+1 ), where fw (µ.2λ+1 ) =
651

Aravantinos, Caferra & Peltier

(ν ′ , 1) (i.e. the tail is equal to the next word followed by the next tail). Otherwise (i.e. if we
are in the middle of a word) we have hw (µ.2λ ) = hw (µ.2λ+1 ) and fw (µ.2λ+1 ) = (ν, ν ′ + 1)
where fw (µ.2λ ) = (ν, ν ′ ) (ν ′ ̸= |wν |)). By an easy induction on the length of jw (µ.2λ ), we
deduce that jw (µ.2λ ) is a preﬁx of kw (µ.2λ ).whw (µ.2λ ) : kw (µ.2λ ) represents the end of the
word considered at the character λ, and whw (µ.2λ ) is the concatenation of all words in the
tail.
For λ = 0 we get in particular that jw (µ) is a preﬁx of kw (µ).whw (µ) . But by deﬁnition
kw (µ) = wν for some ν (not depending on w). Thus jw (µ) is a preﬁx of wν.hw (µ) .
The formulae occurring in the conjunction but not in ϕ check that ha (µ) = hb (µ) (same
sequence of word indices for a and b), that ja (µ) = jb (µ) and that ja (µ) ends with a
character ⊤ (marking the end of the witness).
If I is a model of the whole formula, then jw (µ) is a preﬁx of wν.hw (µ) , ending with ⊤,
thus must be of the form wν.∆ where ∆ is a preﬁx of hw (µ). Hence ν.∆ is a solution to the
Post’s correspondence problem.
Conversely, if such a solution ν.∆ exists, then we simply consider a model I of ϕ such
that ha (µ) = hb (µ) = ∆ (this implies that µ > |∆|, notice that the values of fw (l) and gw (l)
can be ﬁxed arbitrarily for l < 2µ) and I(n) > µ.2λ , where λ = |aν.∆ |. jw (µ) is a preﬁx of
wν.hw (µ) . Since the length jw (µ) cannot be greater than the one of wν.∆ , jw (µ) must end
with ⊤. Thus we must have jw (µ) = wν.∆ (since ⊤ is the last character in wν.∆ ). Moreover
since ν.∆ is a solution we have ja (µ) = jb (µ). Thus I validates all the formulae above.
6.2 Unbounded Translation
One can wonder whether the decidability of the class of regular schemata still holds when
unbounded translations are allowed in the indices, i.e. translations of the form i + m where
i denotes the iteration counter and m a parameter (the case m ∈ Z is covered by the regular
class). The following deﬁnition and theorem show that the answer is negative.
Definition 6.3
Ct (t stands for “translation”) is the set of schemata S satisfying the following properties.
• S contains at most two parameters n, m.
∧
∨
• Every iteration in S is of the form ni=1 ϕ or ni=1 ϕ, where:
– ϕ contains no iteration.
– Every atomic formula in ϕ is of the form pα.i+β+γ.m , where p is a variable,
α, γ ∈ {0, 1} and β ∈ {−1, 0, 1}.
• The atomic propositions occurring in ϕ but not in the scope of an iteration are of the
form p0 or pn where p is a variable.
Theorem 6.4
The set of unsatisﬁable formulae in Ct is not recursively enumerable.
Proof
(Sketch) We do not detail the proof since it is very similar to the previous one. We reuse
the same encoding as in the proof of Theorem 6.2, except that the pairs (ν, λ) in the array
652

Decidability and Undecidability Results for Propositional Schemata

are stored in indices of the form µ + m × ι instead of µ.2ι . Formally, the formulae (1), (2),
(3) and (6) are replaced by the following ones, respectively:
n
∧

[((l = µ) ⇒ wtl ) ∧ ((l < µ) ⇒ ¬wtl ) ∧ (¬(l < µ) ∧ ¬(l = µ)) ⇒ (wtl ⇔ wtl+m )]

l=1

(i.e. wtl holds now iﬀ there exists ι s.t. l = µ + mι).
n
∧

[wtl ∧ car(w, ν, λ)l ⇒ (car(w, ν, λ + 1)l+m ∧ c(w)l+1 )]

l=1

(i.e. the index 2l is now replaced by l + m).
n
∧

(c(w)l ⇒ [(t(w, ν)l ⇔ t(w, ν)l+m ) ∧ (¬wti+1 ⇒ c(w)l+1 )])

l=1

n
∧

(s(w)l ⇒ ¬wtl ∧ ¬t(w, ν)l+m )



l=1

7. Conclusion
We introduced the ﬁrst (to the best of our knowledge) logic for reasoning with iterated
propositional schemata. We deﬁned a class of schemata called bound-linear for which the
satisﬁability problem is decidable. The decidability proof is constructive and divided into
two parts: ﬁrst we show how to transform every bound-linear schema into a sat-equivalent
schema of a simpler form, called regular. Then a proof procedure is deﬁned to decide
the satisﬁability of regular schemata. This proof procedure is sound and complete w.r.t.
satisﬁability for every schema (even if it is not regular or not bound-linear) and terminates
on every regular schema. Termination relies on a special looping detection rule. This
procedure has been implemented in the software RegStab.
The class of bound-linear schemata is expressive enough to capture speciﬁcations of
many important problems in AI, especially in automated (or interactive) theorem proving (e.g., parameterized circuit veriﬁcation problems). We proved that even a very slight
relaxation of the conditions on bound-linear schemata makes the satisﬁability problem undecidable (this is shown by a tricky reduction to the Post correspondence problem). As
a consequence, bound-linear schemata can be considered as a “canonical” decidable class,
providing a good compromise between expressivity and tractability.
As for future work, two ways are the most promising. Firstly, the extension of the
previous results to particular classes
non-monadic schemata (i.e. schemata containing
∨nof ∧
symbols with several indices, e.g., i=1 nj=1 pi,j ) would enlarge considerably applications
of propositional schemata. Secondly, extending our approach to more expressive logics,
653

Aravantinos, Caferra & Peltier

such as ﬁrst-order logic, description logics or modal logics, also deserves to be considered.
The presented results should extend straightforwardly to many-valued propositional logic
(provided the number of truth values is ﬁxed and ﬁnite). This would allow to capture
inﬁnite constraint satisfaction languages.

Acknowledgments
This work has been partly funded by the project ASAP of the French Agence Nationale de
la Recherche (ANR-09-BLAN-0407-01). The authors wish to thank the anonymous referees
for their insightful comments which helped improve an earlier version of this paper.

References
Aczel, P. (1977). An Introduction to Inductive Deﬁnitions. In Barwise, K. J. (Ed.), Handbook
of Mathematical Logic, pp. 739–782. North-Holland, Amsterdam.
Aravantinos, V., Caferra, R., & Peltier, N. (2009a). A DPLL proof procedure for propositional iterated schemata. In Workshop “Structures and Deduction 2009” (ESSLI),
pp. 24–38.
Aravantinos, V., Caferra, R., & Peltier, N. (2009b). A schemata calculus for propositional
logic. In TABLEAUX 09 (International Conference on Automated Reasoning with
Analytic Tableaux and Related Methods), Vol. 5607 of LNCS, pp. 32–46. Springer.
Aravantinos, V., Caferra, R., & Peltier, N. (2010). A Decidable Class of Nested Iterated
Schemata. In IJCAR 2010 (International Joint Conference on Automated Reasoning),
LNCS, pp. 293–308. Springer.
Baaz, M. (1999). Note on the generalization of calculations. Theoretical Computer Science,
224, 3–11.
Baaz, M., & Zach, R. (1994). Short proofs of tautologies using the schema of equivalence.
In Computer Science Logic (CSL’93), Vol. 832 of LNCS, pp. 33–35. Springer-Verlag.
Baelde, D. (2009). On the proof theory of regular ﬁxed points. In Proceedings of the 18th
International Conference on Automated Reasoning with Analytic Tableaux and Related
Methods (TABLEAUX 2009), Vol. 5607 of LNCS, pp. 93–107. Springer.
Barendregt, H., & Wiedijk, F. (2005). The challenge of computer mathematics. Philosophical
Transactions of the Royal Society A, 363, 2351–2375.
Bouhoula, A., Kounalis, E., & Rusinowitch, M. (1992). SPIKE, an automatic theorem
prover. In Proceedings of the International Conference on Logic Programming and
Automated Reasoning (LPAR’92), Vol. 624, pp. 460–462. Springer-Verlag.
Boyer, R. S., & Moore, J. S. (1979). A computational logic. Academic Press.
Bradﬁeld, J., & Stirling, C. (1992). Local model checking for inﬁnite state spaces. In Selected
papers of the Second Workshop on Concurrency and compositionality, pp. 157–174,
Essex, UK. Elsevier Science Publishers Ltd.
654

Decidability and Undecidability Results for Propositional Schemata

Bradﬁeld, J., & Stirling, C. (2007). Modal Mu-Calculi. In Blackburn, P., Benthem, J. F.
A. K. v., & Wolter, F. (Eds.), Handbook of Modal Logic, Volume 3 (Studies in Logic
and Practical Reasoning), pp. 721–756. Elsevier Science Inc., New York, NY, USA.
Brotherston, J. (2005). Cyclic Proofs for First-Order Logic with Inductive Deﬁnitions. In
Beckert, B. (Ed.), Automated Reasoning with Analytic Tableaux and Related Methods:
Proceedings of TABLEAUX 2005, Vol. 3702 of LNAI, pp. 78–92. Springer-Verlag.
Bundy, A. (2001). The automation of proof by mathematical induction. In Robinson, J. A.,
& Voronkov, A. (Eds.), Handbook of Automated Reasoning, pp. 845–911. Elsevier and
MIT Press.
Bundy, A., van Harmelen, F., Horn, C., & Smaill, A. (1990). The Oyster-Clam system.
In Proceedings of the 10th International Conference on Automated Deduction, pp.
647–648, London, UK. Springer-Verlag.
Chen, H., Hsiang, J., & Kong, H. (1990). On ﬁnite representations of inﬁnite sequences
of terms. In Conditional and Typed Rewriting Systems, 2nd International Workshop,
pp. 100–114. Springer, LNCS 516.
Cleaveland, R. (1990). Tableau-based model checking in the propositional mu-calculus.
Acta Inf., 27 (9), 725–747.
Comon, H. (2001). Inductionless induction. In Robinson, A., & Voronkov, A. (Eds.),
Handbook of Automated Reasoning, chap. 14, pp. 913–962. North-Holland.
Comon, H. (1995). On uniﬁcation of terms with integer exponents. Mathematical System
Theory, 28, 67–88.
Cooper, D. (1972). Theorem proving in arithmetic without multiplication. In Meltzer, B.,
& Michie, D. (Eds.), Machine Intelligence 7, chap. 5, pp. 91–99. Edinburgh University
Press.
Davis, M., Logemann, G., & Loveland, D. (1962). A Machine Program for Theorem Proving.
Communication of the ACM, 5, 394–397.
Ebbinghaus, H.-D., & Flum, J. (1999). Finite Model Theory. Perspectives in Mathematical
Logic. Springer. Second Revised and Enlarged Edition.
Fagin, R. (1993). Finite-Model Theory - A Personal Perspective. Theoretical Computer
Science, 116, 3–31.
Giesl, J., & Kapur, D. (2001). Decidable classes of inductive theorems. In Goré, R., Leitsch,
A., & Nipkow, T. (Eds.), IJCAR, Vol. 2083 of Lecture Notes in Computer Science,
pp. 469–484. Springer.
Goré, R. (1999). Chapter 6: Tableau Methods for Modal and Temporal Logics. In M
D’Agostino, D Gabbay, R Hähnle, J Posegga (Ed.), Handbook of Tableau Methods,
pp. 297–396. Kluwer Academic Publishers. http://arp.anu.edu.au/~ rpg (draft).
Gupta, A., & Fisher, A. L. (1993). Representation and symbolic manipulation of linearly
inductive boolean functions. In Lightner, M. R., & Jess, J. A. G. (Eds.), ICCAD, pp.
192–199. IEEE Computer Society.
Hermann, M., & Galbavý, R. (1997). Uniﬁcation of Inﬁnite Sets of Terms schematized by
Primal Grammars. Theoretical Computer Science, 176 (1–2), 111–158.
655

Aravantinos, Caferra & Peltier

Hetzl, S., Leitsch, A., Weller, D., & Woltzenlogel Paleo, B. (2008). Proof analysis with
HLK, CERES and ProofTool: Current status and future directions. In Sutcliﬀe G.,
Colton S., S. S. (Ed.), Workshop on Empirically Successful Automated Reasoning for
Mathematics (ESARM), pp. 21–41.
Immerman, N. (1982). Relational queries computable in polynomial time (Extended Abstract). In STOC ’82: Proceedings of the fourteenth annual ACM symposium on
Theory of computing, pp. 147–152, New York, NY, USA. ACM.
Krajicek, J., & Pudlak, P. (1988). The number of proof lines and the size of proofs in
ﬁrst-order logic. Archive for Mathematical Logic, pp. 69–94.
Marriott, K., Nethercote, N., Rafeh, R., Stuckey, P. J., Garcı́a de la Banda, M., & Wallace,
M. (2008). The design of the Zinc modelling language. Constraints, 13 (3), 229–267.
Orevkov, V. P. (1991). Proof schemata in Hilbert-type axiomatic theories. Journal of
Mathematical Sciences, 55 (2), 1610–1620.
Parikh, R. J. (1973). Some results on the length of proofs. Transactions of the American
Mathematical Society, 177, 29–36.
Park, D. M. (1976). Finiteness is Mu-ineﬀable. Theoretical Computer Science, 3, 173–181.
Paulin-Mohring, C. (1993). Inductive Deﬁnitions in the system Coq - Rules and Properties.
In TLCA ’93: Proceedings of the International Conference on Typed Lambda Calculi
and Applications, pp. 328–345, London, UK. Springer-Verlag.
Smullyan, R. M. (1968). First-Order Logic. Springer.
Sprenger, C., & Dam, M. (2003). On the Structure of Inductive Reasoning: Circular and
Tree-shaped Proofs in the mu-Calculus. In Proc. FOSSACS’03, Springer LNCS, pp.
425–440.
Wos, L. (1988). Automated Reasoning: 33 Basic Research Problems. Prentice Hall.
Wos, L., Overbeek, R., Lush, E., & Boyle, J. (1992). Automated Reasoning: Introduction
and Applications (Second edition). McGraw-Hill.

656

Journal of Artificial Intelligence Research 40 (2011) 729-765

Submitted 11/10; published 04/11

Exploiting Structure in Weighted Model Counting
Approaches to Probabilistic Inference
Wei Li

wei.li@autodesk.com

Autodesk Canada
Toronto, Ontario M5A 1J7 Canada

Pascal Poupart
Peter van Beek

ppoupart@cs.uwaterloo.ca
vanbeek@cs.uwaterloo.ca

Cheriton School of Computer Science
University of Waterloo
Waterloo, Ontario N2L 3G1 Canada

Abstract
Previous studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a backtracking search algorithm
can be an eﬀective method for exact inference. In this paper, we present techniques for
improving this approach for Bayesian networks with noisy-OR and noisy-MAX relations—
two relations that are widely used in practice as they can dramatically reduce the number
of probabilities one needs to specify. In particular, we present two SAT encodings for
noisy-OR and two encodings for noisy-MAX that exploit the structure or semantics of the
relations to improve both time and space eﬃciency, and we prove the correctness of the
encodings. We experimentally evaluated our techniques on large-scale real and randomly
generated Bayesian networks. On these benchmarks, our techniques gave speedups of up
to two orders of magnitude over the best previous approaches for networks with noisyOR/MAX relations and scaled up to larger networks. As well, our techniques extend the
weighted model counting approach for exact inference to networks that were previously
intractable for the approach.

1. Introduction
Bayesian networks are a fundamental building block of many AI applications. A Bayesian
network consists of a directed acyclic graph where the nodes represent the random variables
and each node is labeled with a conditional probability table (CPT) that represents the
strengths of the inﬂuences of the parent nodes on the child node (Pearl, 1988). In general,
assuming random variables with domain size d, the CPT of a child node with n parents
requires one to specify dn+1 probabilities. This presents a practical diﬃculty and has led to
the introduction of patterns for CPTs that require one to specify many fewer parameters
(e.g., Good, 1961; Pearl, 1988; Dı́ez & Druzdzel, 2006).
Perhaps the most widely used patterns in practice are the noisy-OR relation and its
generalization, the noisy-MAX relation (Good, 1961; Pearl, 1988). These relations assume
a form of causal independence and allow one to specify a CPT with just n parameters in
the case of the noisy-OR and (d − 1)2 n parameters in the case of the noisy-MAX, where n is
the number of parents of the node and d is the size of the domains of the random variables.
The noisy-OR/MAX relations have been successfully applied in the knowledge engineering of

c
2011
AI Access Foundation. All rights reserved.

Li, Poupart, & van Beek

large real-world Bayesian networks, such as the Quick Medical Reference-Decision Theoretic
(QMR-DT) project (Miller, Masarie, & Myers, 1986) and the Computer-based Patient Case
Simulation system (Parker & Miller, 1987). As well, Zagorecki and Druzdzel (1992) show
that in three real-world Bayesian networks, noisy-OR/MAX relations were a good ﬁt for up
to 50% of the CPTs in these networks and that converting some CPTs to noisy-OR/MAX
relations gave good approximations when answering probabilistic queries. This is surprising,
as the CPTs in these networks were not speciﬁed using the noisy-OR/MAX assumptions
and were speciﬁed as full CPTs. Their results provide additional evidence for the usefulness
of noisy-OR/MAX relations.
We consider here the problem of exact inference in Bayesian networks that contain
noisy-OR/MAX relations. One method for solving such networks is to replace each noisyOR/MAX by its full CPT representation and then use any of the well-known algorithms
for answering probabilistic queries such as variable elimination or tree clustering/jointree.
However, in general—and in particular, for the networks that we use in our experimental
evaluation—this method is impractical. A more fruitful approach for solving such networks
is to take advantage of the semantics of the noisy-OR/MAX relations to improve both
time and space eﬃciency (e.g., Heckerman, 1989; Olesen, Kjaerulﬀ, Jensen, Jensen, Falck,
Andreassen, & Andersen, 1989; D’Ambrosio, 1994; Heckerman & Breese, 1996; Zhang &
Poole, 1996; Takikawa & D’Ambrosio, 1999; Dı́ez & Galán, 2003; Chavira, Allen, & Darwiche, 2005).
Previous studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a DPLL-based algorithm can be
an eﬀective method for exact inference, where DPLL is a backtracking algorithm specialized
for SAT that includes unit propagation, conﬂict recording, backjumping, and component
caching (Sang, Beame, & Kautz, 2005a). In this paper, we present techniques for improving this weighted model counting approach for Bayesian networks with noisy-OR and
noisy-MAX relations. In particular, we present two CNF encodings for noisy-OR and two
CNF encodings for noisy-MAX that exploit their semantics to improve both the time and
space eﬃciency of probabilistic inference. In our encodings, we pay particular attention to
reducing the treewidth of the CNF formula. We also explore alternative search ordering
heuristics for the DPLL-based backtracking algorithm.
We experimentally evaluated our encodings on large-scale real and randomly generated
Bayesian networks using the Cachet weighted model counting solver (Sang, Bacchus, Beame,
Kautz, & Pitassi, 2004). While our experimental results must be interpreted with some
care as we are comparing not only our encodings but also implementations of systems
with conﬂicting design goals, on these benchmarks our techniques gave speedups of up to
three orders of magnitude over the best previous approaches for networks with noisy-OR
and noisy-MAX. As well, on these benchmarks there were many networks that could not
be solved at all by previous approaches within resource limits, but could be solved quite
quickly by Cachet using our encodings. Thus, our noisy-OR and noisy-MAX encodings
extend the model counting approach for exact inference to networks that were previously
intractable for the approach.

730

Exploiting Structure in Probabilistic Inference

X1

…

X2

Xn

Y

Figure 1: General causal structure for a Bayesian network with a noisy-OR/MAX relation,
where causes X1 , . . . , Xn lead to eﬀect Y and there is a noisy-OR/MAX relation
at node Y .

2. Background
In this section, we review noisy-OR/MAX relations and the needed background on weighted
model counting approaches to exact inference in Bayesian networks (for more on these topics
see, for example, Koller & Friedman, 2009; Darwiche, 2009; Chavira & Darwiche, 2008).
2.1 Patterns for CPTs: Noisy-OR and Noisy-MAX
With the noisy-OR relation one assumes that there are diﬀerent causes X1 , . . . , Xn leading
to an eﬀect Y (see Figure 1), where all random variables are assumed to have Booleanvalued domains. Each cause Xi is either present or absent, and each Xi in isolation is likely
to cause Y and the likelihood is not diminished if more than one cause is present. Further,
one assumes that all possible causes are given and when all causes are absent, the eﬀect is
absent. Finally, one assumes that the mechanism or reason that inhibits a Xi from causing
Y is independent of the mechanism or reason that inhibits a Xj , j = i, from causing Y .
A noisy-OR relation speciﬁes a CPT using n parameters, q1 , . . . , qn , one for each parent,
where qi is the probability that Y is false given that Xi is true and all of the other parents
are false,
(1)
P (Y = 0 | Xi = 1, Xj = 0[∀j,j=i]) = qi .
From these parameters, the full CPT representation of size 2n+1 can be generated using,

qi
(2)
P (Y = 0 | X1 , . . . , Xn ) =
i∈Tx

and
P (Y = 1 | X1 , . . . , Xn ) = 1 −



qi

(3)

i∈Tx

where Tx = {i | Xi = 1} and P (Y = 0 | X1 , . . . , Xn ) = 1 if Tx is empty. The last condition
(when Tx is empty) corresponds to the assumptions that all possible causes are given and
that when all causes are absent, the eﬀect is absent; i.e., P (Y = 0 | X1 = 0, . . . , Xn = 0) = 1.
These assumptions are not as restrictive as may ﬁrst appear. One can always introduce an

731

Li, Poupart, & van Beek

Cold

Malaria

Flu

Nausea

Headache

Figure 2: Example of a causal Bayesian network with causes (diseases) Cold, Flu, and
Malaria and eﬀects (symptoms) Nausea and Headache.

additional random variable X0 that is a parent of Y but itself has no parents. The variable
X0 represents all of the other reasons that could cause Y to occur. The node X0 and the
prior probability P (X0 ) are referred to as a leak node and the leak probability, respectively.
In what follows, we continue to refer to all possible causes as X1 , . . . , Xn where it is
understood that one of these causes could be a leak node.
Example 1. Consider the Bayesian network shown in Figure 2. Suppose that the random
variables are Boolean representing the presence or the absence of the disease or the symptom,
that there is a noisy-OR at node Nausea and at node Headache, and that the parameters
for the noisy-ORs are as given in Table 1. The full CPT for the node Nausea is given by,
C
0
0
0
0
1
1
1
1

F
0
0
1
1
0
0
1
1

M
0
1
0
1
0
1
0
1

P (N ausea = 0 | C, F, M )
1.00
0.40
0.50
0.20 = 0.5 × 0.4
0.60
0.24 = 0.6 × 0.4
0.30 = 0.6 × 0.5
0.12 = 0.6 × 0.5 × 0.4

P (N ausea = 1 | C, F, M )
0.00
0.60
0.50
0.80
0.40
0.76
0.70
0.88

where C, F , and M are short for Cold , Flu, and Malaria, respectively.
An alternative way to view a noisy-OR relation is as a decomposed probabilistic model.
In the decomposed model shown in Figure 3, one only has to specify a small conditional
probability table at each node Yi given by P (Yi | Xi ), instead of an exponentially large
CPT given by P (Y | X1 , . . . , Xn ). In the decomposed model, P (Yi = 0 | Xi = 0) = 1,
P (Yi = 0 | Xi = 1) = qi , and the CPT at node Y is now deterministic and is given by the
OR logical relation. The OR operator can be converted into a full CPT as follows,

1, if Y = Y1 ∨ · · · ∨ Yn ,
P (Y | Y1 , . . . , Yn ) =
0, otherwise.

732

Exploiting Structure in Probabilistic Inference

X1

X2

Y1

Y2

…

Xn

Yn

Y

OR/MAX

Figure 3: Decomposed form for a Bayesian network with a noisy-OR/MAX relation, where
causes X1 , . . . , Xn lead to eﬀect Y and there is a noisy-OR/MAX relation at node
Y . The node with a double border is a deterministic node with the designated
logical relationship (OR) or arithmetic relationship (MAX).

Table 1: Parameters for the noisy-ORs at node Nausea and at node Headache for the
Bayesian network shown in Figure 2, assuming all of the random variables are
Boolean.
P (Nausea = 0 | Cold = 1, Flu = 0, Malaria = 0)
P (Nausea = 0 | Cold = 0, Flu = 1, Malaria = 0)
P (Nausea = 0 | Cold = 0, Flu = 0, Malaria = 1)

=
=
=

0.6
0.5
0.4

P (Headache = 0 | Cold = 1, Flu = 0, Malaria = 0)
P (Headache = 0 | Cold = 0, Flu = 1, Malaria = 0)
P (Headache = 0 | Cold = 0, Flu = 0, Malaria = 1)

=
=
=

0.3
0.2
0.1

The probability distribution of an eﬀect variable Y is given by,

n


P (Yi | Xi ) ,
P (Y | X1 , . . . , Xn ) =
Y =Y1 ∨···∨Yn

i=1

where the sum is over all conﬁgurations or possible values for Y1 , . . . , Yn such that the OR
of these Boolean values is equal to the value for Y . Similarly, in Pearl’s (1988) decomposed
model, one only has to specify n probabilities to fully specify the model (see Figure 4); i.e.,
one speciﬁes the prior probabilities P (Ii ), 1 ≤ i ≤ n. In this model, causes always lead to
eﬀects unless they are prevented or inhibited from doing so. The random variables Ii model
this prevention or inhibition.
These two decomposed probabilistic models (Figure 3, and Figure 4) can be shown to
be equivalent in the sense that the conditional probability distribution P (Y | X1 , . . . , Xn )
induced by both of these networks is just the original distribution for the network shown in
Figure 1. It is important to note that both of these models would still have an exponentially
large CPT associated with the eﬀect node Y if the deterministic OR node were to be replaced
733

Li, Poupart, & van Beek

X1

AND

I1

Y1

…

…

Y

In

Xn

Yn

AND

OR

Figure 4: Pearl’s (1988) decomposed form of the noisy-OR relation. Nodes with double
borders are deterministic nodes with the designated logical relationship.

by its full CPT representation. In other words, these decomposed models address ease of
modeling or representation issues, but do not address eﬃciency of reasoning issues.
The noisy-MAX relation (see Pearl, 1988; Good, 1961; Henrion, 1987; Dı́ez, 1993) is
a generalization of the noisy-OR to non-Boolean domains. With the noisy-MAX relation,
one again assumes that there are diﬀerent causes X1 ,. . . , Xn leading to an eﬀect Y (see
Figure 1), but now the random variables may have multi-valued (non-Boolean) domains.
The domains of the variables are assumed to be ordered and the values are referred to as
the degree or the severity of the variable. Each domain has a distinguished lowest degree
0 representing the fact that a cause or eﬀect is absent. As with the noisy-OR relation, one
assumes that all possible causes are given and when all causes are absent, the eﬀect is absent.
Again, these assumptions are not as restrictive as ﬁrst appears, as one can incorporate a leak
node. As well, one assumes that the mechanism or reason that inhibits a Xi from causing
Y is independent of the mechanism or reason that inhibits a Xj , j = i, from causing Y .
Let dX be the number of values in the domain of some random variable X. For simplicity
of notation and without loss of generality, we assume that the domain of a variable X is
given by the set of integers {0, 1, . . . , dX − 1}. A noisy-MAX relation with causes X1 , . . . ,
Xn and eﬀect Y speciﬁes a CPT using the parameters,
xi
P (Y = y | Xi = xi , Xj = 0[∀j,j=i]) = qi,y

i = 1, . . . , n,

(4)

y = 0, . . . , dY − 1,
xi = 1, . . . , dXi − 1.
If all of the domain sizes are equal to d, a total of (d − 1)2 n non-redundant probabilities
must be speciﬁed. From these parameters, the full CPT representation of size dn+1 can be
generated using,
y
n 

xi
qi,y
(5)
P (Y ≤ y | X) =

i=1 y  =0
xi =0

and


P (Y ≤ 0 | X)
if y = 0,
P (Y = y | X) =
P (Y ≤ y | X) − P (Y ≤ y − 1 | X) if y > 0.
734

(6)

Exploiting Structure in Probabilistic Inference

where X represents a certain conﬁguration of the parents of Y , X = x1 , . . . , xn , and
P (Y = 0 | X1 = 0, . . . , Xn = 0) = 1; i.e., if all causes are absent, the eﬀect is absent.
Table 2: Parameters for the noisy-MAX at node Nausea for the Bayesian network shown in
Figure 2, assuming the diseases are Boolean random variables and the symptom
Nausea has domain {absent = 0, mild = 1, severe = 2}.
P (Nausea = absent | Cold = 1, Flu = 0, Malaria = 0)
P (Nausea = mild | Cold = 1, Flu = 0, Malaria = 0)
P (Nausea = severe | Cold = 1, Flu = 0, Malaria = 0)

=
=
=

0.7
0.2
0.1

P (Nausea = absent | Cold = 0, Flu = 1, Malaria = 0)
P (Nausea = mild | Cold = 0, Flu = 1, Malaria = 0)
P (Nausea = severe | Cold = 0, Flu = 1, Malaria = 0)

=
=
=

0.5
0.2
0.3

P (Nausea = absent | Cold = 0, Flu = 0, Malaria = 1)
P (Nausea = mild | Cold = 0, Flu = 0, Malaria = 1)
P (Nausea = severe | Cold = 0, Flu = 0, Malaria = 1)

=
=
=

0.1
0.4
0.5

Example 2. Consider once again the Bayesian network shown in Figure 2. Suppose that
the diseases are Boolean random variables and the symptoms Nausea and Headache have
domains {absent = 0, mild = 1, severe = 2}, there is a noisy-MAX at node Nausea and
at node Headache, and the parameters for the noisy-MAX at node Nausea are as given in
Table 2. The full CPT for the node Nausea is given by,
C
0
0
0
0
1
1
1
1

F
0
0
1
1
0
0
1
1

M
0
1
0
1
0
1
0
1

P (N = a | C, F, M )
1.000
0.100
0.500
0.050 = 0.5 × 0.1
0.700
0.070 = 0.7 × 0.1
0.350 = 0.7 × 0.5
0.035 = 0.7 × 0.5 × 0.1

P (N = m | C, F, M )
0.000
0.400
0.200
0.300
0.200
0.380
0.280
0.280

P (N = s | C, F, M )
0.000
0.500
0.300
0.650
0.100
0.550
0.370
0.685

where C, F , M , and N are short for the variables Cold , Flu, Malaria, and Nausea,
respectively, and a, m, and s are short for the values absent, mild, and severe, respectively. As an example calculation, P (Nausea = mild | Cold = 0, Flu = 1, Malaria = 1) =
((0.5 + 0.2) × (0.1 + 0.4)) − (0.05) = 0.3 As a second example, P (Nausea = mild | Cold =
1, Flu = 1, Malaria = 1) = ((0.7 + 0.2) × (0.5 + 0.2) × (0.1 + 0.4)) − (0.035) = 0.28
As with the noisy-OR relation, an alternative view of a noisy-MAX relation is as a
decomposed probabilistic model (see Figure 3). In the decomposed model, one only has to
specify a small conditional probability table at each node Yi given by P (Yi | Xi ), where
x . Each Y models the eﬀect of the
P (Yi = 0 | Xi = 0) = 1 and P (Yi = y | Xi = x) = qi,y
i
735

Li, Poupart, & van Beek

cause Xi on the eﬀect Y in isolation; i.e., the degree or the severity of the eﬀect in the case
where only the cause Xi is not absent and all other causes are absent. The CPT at node Y
is now deterministic and is given by the MAX arithmetic relation. This corresponds to the
assumption that the severity or the degree reached by the eﬀect Y is the maximum of the
degrees produced by each cause if they were acting independently; i.e., the maximum of the
Yi ’s. This assumption is only valid if the eﬀects do not accumulate. The MAX operator
can be converted into a full CPT as follows,

1, if Y = max{Y1 , . . . , Yn },
P (Y | Y1 , . . . , Yn ) =
0, otherwise.
The probability distribution of an eﬀect variable Y is given by,
 n



P (Yi | Xi ) ,
P (Y | X1 , . . . , Xn ) =
Y =max{Y1 ,...,Yn }

i=1

where the sum is over all conﬁgurations or possible values for Y1 , . . . , Yn such that the
maximum of these values is equal to the value for Y . In both cases, however, making the
CPTs explicit is often not possible in practice, as their size is exponential in the number of
causes and the number of values in the domains of the random variables.
2.2 Weighted Model Counting for Probabilistic Inference
In what follows, we consider propositional formulas in conjunctive normal form (CNF). A
literal is a Boolean variable (also called a proposition) or its negation and a clause is a
disjunction of literals. A clause with one literal is called a unit clause and the literal in the
unit clause is called a unit literal. A propositional formula F is in conjunctive normal form
if it is a conjunction of clauses.
Example 3. For example, (x ∨ ¬y) is a clause, and the formula,
F = (x ∨ ¬y) ∧ (x ∨ y ∨ z) ∧ (y ∨ ¬z ∨ w) ∧ (¬w ∨ ¬z ∨ v) ∧ (¬v ∨ u),
is in CNF, where u, v, w, x, y, and z are propositions.
Given a propositional formula in conjunctive normal form, the problem of determining
whether there exists a variable assignment that makes the formula evaluate to true is called
the Boolean satisfiability problem or SAT. A variable assignment that makes a formula
evaluate to true is also called a model. The problem of counting the number of models of a
formula is called model counting.
Let F denote a propositional formula. We use the value 0 interchangeably with the
Boolean value false and the value 1 interchangeably with the Boolean value true. The notation F |v=false represents a new formula, called the residual formula, obtained by removing
all clauses that contain the literal ¬v (as these clauses evaluate to true) and deleting the
literal v from all clauses. Similarly, the notation F |v=true represents the residual formula
obtained by removing all clauses that contain the literal v and deleting the literal ¬v from all

736

Exploiting Structure in Probabilistic Inference

clauses. Let s be a set of instantiated variables in F . The residual formula F |s is obtained
by cumulatively reducing F by each of the variables in s.
Example 4. Consider once again the propositional formula F given in Example 3. Suppose
x is assigned false. The residual formula is given by,
F |x=0 = (¬y) ∧ (y ∨ z) ∧ (y ∨ ¬z ∨ w) ∧ (¬w ∨ ¬z ∨ v) ∧ (¬v ∨ u).
As is clear, a CNF formula is satisﬁed if and only if each of its clauses is satisﬁed and a
clause is satisﬁed if and only if at least one of its literals is equivalent to 1. In a unit clause,
there is no choice and the value of the literal is said to be forced or implied. The process of
unit propagation assigns all unit literals to the value 1. As well, the formula is simpliﬁed by
removing the variables of the unit literals from the remaining clauses and removing clauses
that evaluate to true (i.e., the residual formula is obtained) and the process continues
looking for new unit clauses and updating the formula until no unit clause remains.
Example 5. Consider again the propositional formula F |x=0 given in Example 4, where x
has been assigned false. The unit clause (¬y) forces y to be assigned false. The residual
formula is given by,
F |x=0,y=0 = (z) ∧ (¬z ∨ w) ∧ (¬w ∨ ¬z ∨ v) ∧ (¬v ∨ u).
In turn, the unit clause (z) forces z to be assigned true. Similarly, the assignments w = 1,
v = 1, and u = 1 are forced.
There are natural polynomial-time reductions between the Bayesian inference problem
and model counting problems (Bacchus, Dalmao, & Pitassi, 2003). In particular, exact
inference in Bayesian networks can be reduced to the weighted model counting of CNFs
(Darwiche, 2002; Littman, 1999; Sang et al., 2005a). Weighted model counting is a generalization of model counting.
A weighted model counting problem consists of a CNF formula F and for each variable
v in F , a weight for each literal: weight(v) and weight(¬v). Let s be an assignment of a
value to every variable in the formula F that satisﬁes the formula; i.e., s is a model of the
formula. The weight of s is the product of the weights of the literals in s. The solution of
a weighted model counting problem is the sum of the weights of all satisfying assignments;
i.e.,
 
weight(l),
weight(F ) =
s

l∈s

where the sum is over all possible models and the product is over the literals in that model.
Chavira and Darwiche (2002, 2008) proposed an encoding of a Bayesian network into
weighted model counting of a propositional formula in conjunctive normal form. Chavira
and Darwiche’s encoding proceeds as follows. At each step, we illustrate the encoding using
the Bayesian network shown in Figure 2. For simplicity, we assume the random variables
are all Boolean and we omit the node Headache. To improve clarity, we refer to the random
variables in the Bayesian network as “nodes” and reserve the word “variables” for the
Boolean variables in the resulting propositional formula.
• For each value of each node in the Bayesian network, an indicator variable is created,
737

Li, Poupart, & van Beek

C
F

:
:

IC0 , IC1 ,
IF0 , IF1 ,

M
N

:
:

IM0 , IM1 ,
IN0 , IN1 .

• For each node, indicator clauses are generated which ensure that in each model exactly
one of the corresponding indicator variables for each node is true,
C
F

:
:

(IC0 ∨ IC1 ) ∧ (¬IC0 ∨ ¬IC1 ),
(IF0 ∨ IF1 ) ∧ (¬IF0 ∨ ¬IF1 ),

M
N

:
:

(IM0 ∨ IM1 ) ∧ (¬IM0 ∨ ¬IM1 ),
(IN0 ∨ IN1 ) ∧ (¬IN0 ∨ ¬IN1 ).

• For each conditional probability table (CPT) and for each parameter (probability)
value in the CPT, a parameter variable is created,
C
F
M

PC0 ,
PF0 ,
PM0 ,

:
:
:

PC1 ,
PF1 ,
PM1 ,

N

:

PN0 |C0 ,F0 ,M0 ,
. . .,
PN0 |C1 ,F1 ,M1 ,

PN1 |C0 ,F0 ,M0 ,
. . .,
PN1 |C1 ,F1 ,M1 .

• For each parameter variable, a parameter clause is generated. A parameter clause
asserts that the conjunction of the corresponding indicator variables implies the parameter variable and vice-versa,

C
F
M
N

:
:
:
:

IC0 ⇔ PC0 ,
IF0 ⇔ PF0 ,
IM0 ⇔ PM0 ,
IC0 ∧ IF0 ∧ IM0 ∧ IN0 ⇔ PN0 |C0 ,F0 ,M0 ,
. . .,
IC1 ∧ IF1 ∧ IM1 ∧ IN0 ⇔ PN0 |C1 ,F1 ,M1 ,

IC1 ⇔ PC1 ,
IF1 ⇔ PF1 ,
IM1 ⇔ PM1 ,
IC0 ∧ IF0 ∧ IM0 ∧ IN1 ⇔ PN1 |C0 ,F0 ,M0 ,
. . .,
IC1 ∧ IF1 ∧ IM1 ∧ IN1 ⇔ PN1 |C1 ,F1 ,M1 .

• A weight is assigned to each literal in the propositional formula. Each positive literal
of a parameter variable is assigned a weight equal to the corresponding probability
entry in the CPT table,
C

:

F

:

M

:

N

:

weight(PC0 ) = P (C = 0),
weight(PC1 ) = P (C = 1),
weight(PF0 ) = P (F = 0),
weight(PF1 ) = P (F = 1),
weight(PM0 ) = P (M = 0),
weight(PM1 ) = P (M = 1),
weight(PN0 |C0 ,F0 ,M0 ) = P (N
weight(PN1 |C0 ,F0 ,M0 ) = P (N
...,
weight(PN0 |C1 ,F1 ,M1 ) = P (N
weight(PN1 |C1 ,F1 ,M1 ) = P (N

= 0 | C = 0, F = 0, M = 0),
= 1 | C = 0, F = 0, M = 0),
= 0 | C = 1, F = 1, M = 1),
= 1 | C = 1, F = 1, M = 1).

All other literals (both positive and negative) are assigned a weight of 1; i.e., weight(IC0 )
= weight(¬IC0 ) = · · · = weight(IN1 ) = weight(¬IN1 ) = 1 and weight(¬PC0 ) = · · · =
738

Exploiting Structure in Probabilistic Inference

weight(¬PN1 |C1 ,F1 ,M1 ) = 1. The basic idea is that the indicator variables specify the
state of the world—i.e., a value for each random variable in the Bayesian network—
and then the weights of the literals multiplied together give the probability of that
state of the world.
Sang, Beame, and Kautz (2005a) introduced an alternative encoding of a Bayesian
network into weighted model counting of a CNF formula. Sang et al.’s encoding creates
fewer variables and clauses, but the size of generated clauses of multi-valued variables can
be larger. As with Chavira and Darwiche’s encoding presented above, we illustrate Sang
et al.’s encoding using the Bayesian network shown in Figure 2, once again assuming the
random variables are all Boolean and omitting the node Headache.
• As in Chavira and Darwiche’s encoding, for each node, indicator variables are created
and indicator clauses are generated which ensure that in each model exactly one of
the corresponding indicator variables for each node is true.
• Let the values of the nodes be linearly ordered. For each CPT entry P (Y = y | X)
such that y is not the last value in the domain of Y , a parameter variable Py|X is
created; e.g.,
C
F
M

:
:
:

PC0 ,
PF0 ,
PM0 ,

N

:

PN0 |C0 ,F0 ,M0 ,
PN0 |C0 ,F1 ,M0 ,
PN0 |C1 ,F0 ,M0 ,
PN0 |C1 ,F1 ,M0 ,

PN0 |C0 ,F0 ,M1 ,
PN0 |C0 ,F1 ,M1 ,
PN0 |C1 ,F0 ,M1 ,
PN0 |C1 ,F1 ,M1 .

• For each CPT entry P (Y = yi | X), a parameter clause is generated. Let the ordered
domain of Y be {y1 , . . . , yk } and let X = x1 , . . . , xl . If yi is not the last value in the
domain of Y , the clause is given by,
Ix1 ∧ · · · ∧ Ixl ∧ ¬Py1 |X ∧ · · · ∧ ¬Pyi−1 |X ∧ Pyi |X ⇒ Iyi .
If yi is the last value in the domain of Y , the clause is given by,
Ix1 ∧ · · · ∧ Ixl ∧ ¬Py1 |X ∧ · · · ∧ ¬Pyk−1 |X ⇒ Iyk .
For our running example, the following parameter clauses would be generated,

C
F
M
N

:
:
:
:

PC0 ⇒ IC0
PF0 ⇒ IF0
PM0 ⇒ IM0
IC0 ∧ IF0 ∧ IM0 ∧ PN0 |C0 ,F0 ,M0 ⇒ IN0 ,
. . .,
IC1 ∧ IF1 ∧ IM1 ∧ PN0 |C1 ,F1 ,M1 ⇒ IN0 ,

¬PC0 ⇒ IC1
¬PF0 ⇒ IF1
¬PM0 ⇒ IM1
IC0 ∧ IF0 ∧ IM0 ∧ ¬PN0 |C0 ,F0 ,M0 ⇒ IN1 ,
. . .,
IC1 ∧ IF1 ∧ IM1 ∧ ¬PN0 |C1 ,F1 ,M1 ⇒ IN1 .

• A weight is assigned to each literal in the propositional formula. As in Chavira and
Darwiche’s encoding, the weight of literals for indicator variables is always 1. The
739

Li, Poupart, & van Beek

weight of literals for each parameter variable Py|X is given by,
weight(Py|X ) = P (y | X),
weight(¬Py|X ) = 1 − P (y | X).
Let F be the CNF encoding of a Bayesian network (either Chavira and Darwiche’s
encoding or Sang et al.’s encoding). A general query P (Q | E) on the network can be
answered by,
weight(F ∧ Q ∧ E)
,
(7)
weight(F ∧ E)
where Q and E are propositional formulas which enforce the appropriate values for the
indicator variables that correspond to the known values of the random variables.
A backtracking algorithm used to enumerate the (weighted) models of a CNF formula
is often referred to as DPLL or DPLL-based (in honor of Davis, Putnam, Logemann, and
Loveland, the authors of some of the earliest work in the ﬁeld: Davis & Putnam, 1960; Davis,
Logemann, & Loveland, 1962), and usually includes such techniques as unit propagation,
conﬂict recording, backjumping, and component caching.

3. Related Work
In this section, we relate our work to previously proposed methods for exact inference in
Bayesian networks that contain noisy-OR/MAX relations.
One method for solving such networks is to replace each noisy-OR/MAX by its full CPT
representation and then use any of the well-known algorithms for answering probabilistic
queries such as variable elimination or tree clustering/jointree. However, in general—and
in particular, for the networks that we use in our experimental evaluation—this method
is impractical. A more fruitful approach for solving such networks is to take advantage of
the structure or the semantics of the noisy-OR/MAX relations to improve both time and
space eﬃciency (e.g., Heckerman, 1989; Olesen et al., 1989; D’Ambrosio, 1994; Heckerman
& Breese, 1996; Zhang & Poole, 1996; Takikawa & D’Ambrosio, 1999; Dı́ez & Galán, 2003;
Chavira et al., 2005).
Quickscore (Heckerman, 1989) was the ﬁrst eﬃcient exact inference algorithm for Booleanvalued two-layer noisy-OR networks. Chavira, Allen and Darwiche (2005) present a method
for multi-layer noisy-OR networks and show that their approach is signiﬁcantly faster than
Quickscore on randomly generated two-layer networks. Their approach proceeds as follows:
(i) transform the noisy-OR network into a Bayesian network with full CPTs using Pearl’s
decomposition (see Figure 4), (ii) translate the network with full CPTs into CNF using
a general encoding (see Section 2), (iii) simplify the resulting CNF by taking advantage
of determinism (zero parameters and one parameters), and (iv) compile the CNF into an
arithmetic circuit. One of our encodings for the noisy-OR (called WMC1) is similar to their
more indirect (but also more general) proposal for encoding noisy-ORs (steps (i)–(iii)). We
perform a detailed comparison in Section 4.1. In our experiments, we perform a detailed
empirical comparison of their approach using compilation (steps (i)–(iv)) against Cachet
using our encodings on large Bayesian networks.

740

Exploiting Structure in Probabilistic Inference

Many alternative methods have been proposed to decompose a noisy-OR/MAX by
adding hidden or auxiliary nodes and then solving using adaptations of variable elimination or tree clustering (e.g., Olesen et al., 1989; D’Ambrosio, 1994; Heckerman & Breese,
1996; Takikawa & D’Ambrosio, 1999; Dı́ez & Galán, 2003).
Olesen et al. (1989) proposed to reduce the size of the distribution for the OR/MAX
operator by decomposing a deterministic OR/MAX node with n parents into a set of binary OR/MAX operators. The method, called parent divorcing, constructs a binary tree
by adding auxiliary nodes Zi such that Y and each of the auxiliary nodes has exactly two
parents. Heckerman (1993) presented a sequential decomposition method again based on
adding auxiliary nodes Zi and decomposing into binary MAX operators. Here one constructs a linear decomposition tree. Both methods require similar numbers of auxiliary
nodes and similarly sized CPTs. However, as Takikawa and D’Ambrosio (1999) note, using
either parent divorcing or sequential decomposition, many decomposition trees can be constructed from the same original network—depending on how the causes are ordered—and
the eﬃciency of query answering can vary exponentially when using variable elimination or
tree clustering, depending on the particular query and the choice of ordering.
To take advantage of causal independence models, Dı́ez (1993) proposed an algorithm
for the noisy-MAX/OR. By introducing one auxiliary variable Y  , Dı́ez’s method leads to a
complexity of O(nd2 ) for singly connected networks, where n is the number of causes and d is
the size of the domains of the random variables. However, for networks with loops it needs to
be integrated with local conditioning. Takikawa and D’Ambrosio (1999) proposed a similar
multiplicative factorization approach. The complexity of their approach is O(max(2d , nd2 )).
However, Takikawa and D’Ambrosio’s approach allows more eﬃcient elimination orderings
in the variable elimination algorithm, while Dı́ez’s method enforces more restrictions on
the orderings. More recently, Dı́ez and Galán (2003) proposed a multiplicative factorization
that improves on this previous work, as it has the advantages of both methods. We use their
auxiliary graph as the starting point for the remaining three of our CNF encodings (WMC2,
MAX1, and MAX2). In our experiments, we perform a detailed empirical comparison of
their approach using variable elimination against our proposals on large Bayesian networks.
In our work, we build upon the DPLL-based weighted model counting approach of
Sang, Beame, and Kautz (2005a). Their general encoding assumes full CPTs and yields
a parameter clause for each CPT parameter. However, this approach is impractical for
large-scale noisy-OR/MAX networks. Our special-purpose encodings extend the weighted
model counting approach for exact inference to networks that were previously intractable
for the approach.

4. Eﬃcient Encodings of Noisy-OR into CNF
In this section, we present techniques for improving the weighted model counting approach
for Bayesian networks with noisy-OR relations. In particular, we present two CNF encodings
for noisy-OR relations that exploit their structure or semantics. For the noisy-OR relation
we take advantage of the Boolean domains to simplify the encodings. We use as a running
example the Bayesian network shown in Figure 2. In the subsequent section, we generalize
to the noisy-MAX relation.

741

Li, Poupart, & van Beek

4.1 Weighted CNF Encoding 1: An Additive Encoding
Let there be causes X1 , . . . , Xn leading to an eﬀect Y and let there be a noisy-OR relation
at node Y (see Figure 1), where all random variables are assumed to have Boolean-valued
domains.
In our ﬁrst weighted model encoding method (WMC1), we introduce an indicator variable IY for Y and an indicator variable IXi for each parent of Y . We also introduce a
parameter variable Pqi for each parameter qi , 1 ≤ i ≤ n in the noisy-OR (see Equation 1).
The weights of these variables are as follows,
weight(IXi ) = weight(¬IXi ) = 1,
weight(IY ) = weight(¬IY ) = 1,
weight(Pqi ) = 1 − qi ,
weight(¬Pqi ) = qi .
The noisy-OR relation can then be encoded as the formula,
(IX1 ∧ Pq1 ) ∨ (IX2 ∧ Pq2 ) ∨ · · · ∨ (IXn ∧ Pqn ) ⇔ IY .

(8)

The formula can be seen to be an encoding of Pearl’s well-known decomposition for noisyOR (see Figure 4).
Example 6. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-ORs shown in Table 1. The WMC1 encoding introduces the five
Boolean indicator variables IC , IF , IM , IN , and IH , each with weight 1; and the six parameter variables P0.6 , P0.5 , P0.4 , P0.3 , P0.2 , and P0.1 , each with weight(Pqi ) = 1 − qi and
weight(¬Pqi ) = qi . Using Equation 8, the noisy-OR at node Nausea can be encoded as,
(IC ∧ P0.6 ) ∨ (IF ∧ P0.5 ) ∨ (IM ∧ P0.4 ) ⇔ IN .
To illustrate the weighted model counting of the formula, suppose that nausea and malaria
are absent and cold and flu are present (i.e., Nausea = 0, Malaria = 0, Cold = 1, and
Flu = 1; and for the corresponding indicator variables IN and IM are false and IC and IF
are true). The formula can be simplified to,
(P0.6 ) ∨ (P0.5 ) ⇔ 0.
There is just one model for this formula, the model that sets P0.6 to false and P0.5 to false.
Hence, the weighted model count of this formula is weight(¬P0.6 )×weight(¬P0.5 ) = 0.6×0.5
= 0.3, which is just the entry in the penultimate row of the full CPT shown in Example 2.
Towards converting Equation 8 into CNF, we also introduce an auxiliary indicator variable wi for each conjunction such that wi ⇔ IXi ∧Pqi . This dramatically reduces the number

742

Exploiting Structure in Probabilistic Inference

of clauses generated. Equation 8 is then transformed into,
(¬IY

∨ ((w1 ∨ · · · ∨ wn ) ∧
(¬IX1 ∨ ¬Pq1 ∨ w1 ) ∧
(IX1 ∨ ¬w1 ) ∧
(Pq1 ∨ ¬w1 )
∧ ··· ∧
(¬IXn ∨ ¬Pqn ∨ wn ) ∧
(IXn ∨ ¬wn ) ∧
(Pqn ∨ ¬wn ))) ∧

(IY

∨ ((¬IX1 ∨ ¬Pq1 )
∧ ··· ∧
(¬IXn ∨ ¬Pqn ))).

The formula is not in CNF, but can be easily transformed into CNF using the distributive
law. It can be seen that the WMC1 encoding can also easily encode evidence—i.e, if IY = 0
or IY = 1, the formula can be further simpliﬁed—before the ﬁnal translation into CNF. Note
that we have made the deﬁnitions of the auxiliary variables (i.e., wi ⇔ IXi ∧ Pqi ) conditional
on IY being true, rather than just introducing separate clauses to deﬁne each auxiliary
variable. This allows the formula to be further simpliﬁed in the presence of evidence and
only introduces the wi if they are actually needed. In particular, if we know that IY is false,
all of the clauses involving the auxiliary variables wi , including the deﬁnitions of the wi ,
disappear when the formula is simpliﬁed.
Example 7. Consider once again the Bayesian network shown in Figure 2. To illustrate
the encoding of evidence, suppose that nausea is present (i.e., Nausea = 1) and headache
is not present (i.e., Headache = 0). The corresponding constraints for the evidence are as
follows.
(9)
(IC ∧ P0.6 ) ∨ (IF ∧ P0.5 ) ∨ (IM ∧ P0.4 ) ⇔ 1
(IC ∧ P0.3 ) ∨ (IF ∧ P0.2 ) ∨ (IM ∧ P0.1 ) ⇔ 0

(10)

The above constraints can be converted into CNF clauses. Constraint Equation 9 gives the
clauses,
(w1 ∨ w2 ∨ w3 )
∧ (¬IC ∨ ¬P0.6 ∨ w1 ) ∧ (IC ∨ ¬w1 ) ∧ (P0.6 ∨ ¬w1 )
∧ (¬IF ∨ ¬P0.5 ∨ w2 ) ∧ (IF ∨ ¬w2 ) ∧ (P0.5 ∨ ¬w2 )
∧ (¬IM ∨ ¬P0.4 ∨ w3 ) ∧ (IM ∨ ¬w3 ) ∧ (P0.4 ∨ ¬w3 )
and constraint Equation 10 gives the clauses,
(¬IC ∨ ¬P0.3 ) ∧ (¬IF ∨ ¬P0.2 ) ∧ (¬IM ∨ ¬P0.1 ).

743

Li, Poupart, & van Beek

To show the correctness of encoding WMC1 of a noisy-OR, we ﬁrst show that each entry
in the full CPT representation of a noisy-OR relation can be determined using the weighted
model count of the encoding. As always, let there be causes X1 , . . . , Xn leading to an
eﬀect Y and let there be a noisy-OR relation at node Y , where all random variables have
Boolean-valued domains.
Lemma 1. Each entry in the full CPT representation of a noisy-OR at a node Y , P (Y =
y | X1 = x1 , . . . , Xn = xn ), can be determined using the weighted model count of Equation 8
created using the encoding WMC1.
Proof. Let FY be the encoding of the noisy-OR at node Y using WMC1 and let s be the
set of assignments to the indicator variables IY , IX1 , . . . , IXn corresponding to the desired
entry in the CPT (e.g., if Y = 0, IY is instantiated to false; otherwise it is instantiated to
true). For each Xi = 0, the disjunct (IXi ∧ Pqi ) in Equation 8 is false and would be removed
in the residual formula FY |s ; and for each Xi = 1, the disjunct reduces to (Pqi ). If IY = 0,
each of the disjuncts in Equation 8 must be false and there is only a single model of the
formula. Hence,


weight(¬Pqi ) =
qi = P (Y = 0 | X),
weight(FY |s ) =
i∈Tx

i∈Tx

where Tx = {i | Xi = 1} and P (Y = 0 | X) = 1 if Tx is empty. If IY = 1, at least one of
the disjuncts in Equation 8 must be true and there are, therefore, 2|Tx | − 1 models. It can
be seen that if we sum over all 2|Tx | possible assignments, the weight of the formula is 1.
Hence, subtracting oﬀ the one possible assignment that is not a model gives,


weight(¬Pqi ) = 1 −
qi = P (Y = 1 | X).
weight(FY |s ) = 1 −
i∈Tx

i∈Tx

A noisy-OR Bayesian network over a set of random variables Z1 , . . . , Zn is a Bayesian
network where there are noisy-OR relations at one or more of the Zi and full CPTs at
the remaining nodes. The next step in the proof of correctness is to show that each entry
in the joint probability distribution represented by a noisy-OR Bayesian network can be
determined using weighted model counting. In what follows, we assume that noisy-OR
nodes are encoded using WMC1 and the remaining nodes are encoded using Sang et al.’s
general encoding discussed in Section 2.2. Similar results can be stated using Chavira and
Darwiche’s general encoding.
Lemma 2. Each entry in the joint probability distribution, P (Z1 = z1 , . . . Zn = zn ), represented by a noisy-OR Bayesian network can be determined using weighted model counting
and encoding WMC1.
Proof. Let F be the encoding of the Bayesian network using WMC1 for the noisy-OR nodes
and let s be the set of assignments to the indicator variables IZ1 , . . . , IZn corresponding
to the desired entry in the joint probability distribution. Any entry in the joint probability

744

Exploiting Structure in Probabilistic Inference

distribution can be expressed as the product,
P (X1 , . . . , Xn ) =

n


P (Xi | parents (Xi )),

i=1

where n is the size of the Bayesian network and parents (Xi ) is the set of parents of Xi
in the directed graph; i.e., the entry in the joint probability distribution is determined by
multiplying the corresponding CPT entries. For those nodes with full CPTs, s determines
the correct entry in each CPT by Lemma 2 in Sang et al. (2005a) and for those nodes with
noisy-ORs, s determines the correct probability by Lemma 1 above. Thus, weight(F ∧ s) is
the multiplication of the corresponding CPT entries; i.e., the entry in the joint probability
distribution.
The ﬁnal step in the proof of correctness is to show that queries of interest can be
correctly answered.
Theorem 1. Given a noisy-OR Bayesian network, general queries of the form P (Q | E)
can be determined using weighted model counting and encoding WMC1.
Proof. Let F be the CNF encoding of a noisy-OR Bayesian network. A general query
P (Q | E) on the network can be answered by,
weight(F ∧ Q ∧ E)
P (Q ∧ E)
=
,
P (E)
weight(F ∧ E)
where Q and E are propositional formulas that enforce the appropriate values for the indicator variables that correspond to the known values of the random variables. By deﬁnition, the
function weight computes the weighted sum of the solutions of its argument. By Lemma 2,
this is equal to the sum of the probabilities of those sets of assignments that satisfy the
restrictions Q ∧ E and E, respectively, which in turn is equal to the sum of the entries in
the joint probability distribution that are consistent with Q ∧ E and E, respectively.
As Sang et al. (2005a) note, the weighted model counting approach supports queries
and evidence in arbitrary propositional form and such queries are not supported by any
other exact inference method.
Our WMC1 encoding for noisy-OR is essentially similar to a more indirect but also more
general proposal by Chavira and Darwiche (2005) (see Darwiche, 2009, pp. 313-323 for a
detailed exposition of their proposal). Their approach proceeds as follows: (i) transform
the noisy-OR network into a Bayesian network with full CPTs using Pearl’s decomposition
(see Figure 4), (ii) translate the network with full CPTs into CNF using a general encoding
(see Section 2), and (iii) simplify the resulting CNF by taking advantage of determinism.
Simplifying the resulting CNF proceeds as follows. Suppose we have in an encoding the
sentence (Ia ∧ I¬b ) ⇔ P¬b|a . If the parameter corresponding to P¬b|a is zero, the sentence
can be replaced by ¬(Ia ∧ I¬b ) and P¬b|a can be removed from the encoding. If the parameter corresponding to P¬b|a is one, the entire sentence can be removed from the encoding.
Applying their method to a noisy-OR (see Figure 1) results in the following,

745

Li, Poupart, & van Beek

(¬IX1 ∨ ¬Pq1 ∨ w1 ) ∧
(¬IX1 ∨ Pq1 ∨ ¬w1 ) ∧
(IX1 ∨ ¬Pq1 ∨ ¬w1 ) ∧
(IX1 ∨ Pq1 ∨ ¬w1 ) ∧
...
(¬IXn ∨ ¬Pqn ∨ wn ) ∧
(¬IXn ∨ Pqn ∨ ¬wn ) ∧
(IXn ∨ ¬Pqn ∨ ¬wn ) ∧
(IXn ∨ Pqn ∨ ¬wn ) ∧
(¬IY

∨

(w1 ∨ · · · ∨ wn )) ∧

(IY

∨

((¬w1 ∨ · · · ∨ ¬wn−1 ∨ ¬wn ) ∧
(¬w1 ∨ · · · ∨ ¬wn−1 ∨ wn ) ∧
...
(w1 ∨ · · · ∨ wn−1 ∨ ¬wn ))),

where we have simpliﬁed the expression by substituting equivalent literals and by using the
fact that the random variables are Boolean (e.g., we use IX1 and ¬IX1 rather than IX1 =0
and IX1 =1 ). Three diﬀerences can be noted. First, in our encoding the deﬁnitions of the wi
are conditional on IY being true, rather than being introduced as separate clauses. Second,
our deﬁnitions of the wi are more succinct. Third, in our encoding there are a linear number
of clauses conditioned on IY whereas in the Chavira et al. encoding there are 2n − 1 clauses.
We note, however, that Chavira, Allen, and Darwiche (2005) discuss a direct translation of
a noisy-OR to CNF based on Pearl’s decomposition that is said to compactly represent the
noisy-OR (i.e., not an exponential number of clauses), but the speciﬁc details of the CNF
formula are not given.
4.2 Weighted CNF Encoding 2: A Multiplicative Encoding
Again, let there be causes X1 , . . . , Xn leading to an eﬀect Y and let there be a noisyOR relation at node Y (see Figure 1), where all random variables are assumed to have
Boolean-valued domains.
Our second weighted model encoding method (WMC2) takes as its starting point Dı́ez
and Galán’s (2003) directed auxiliary graph transformation of a Bayesian network with a
noisy-OR/MAX relation. Dı́ez and Galán note that for the noisy-OR relation, Equation (6)
can be represented as a product of matrices,

 


P (Y = 0 | X)
1 0
P (Y ≤ 0 | X)
=
.
P (Y = 1 | X)
−1 1
P (Y ≤ 1 | X)
Based on this factorization, one can integrate a noisy-OR node into a regular Bayesian
network by introducing a hidden node Y  for each noisy-OR node Y . The transformation
ﬁrst creates a graph with the same set of nodes and arcs as the original network. Then,
for each node Y with a noisy-OR relation, we add a hidden node Y  with the same domain
as Y , add an arc Y  → Y , redirect each arc Xi → Y to Xi → Y  , and associate with Y a
factorization table,

746

Exploiting Structure in Probabilistic Inference

Y =0
1
−1

Y =0
Y =1

Y =1
0
1.

This auxiliary graph is not a Bayesian network as it contains parameters that are less than
0. So the CNF encoding methods for general Bayesian networks (see Section 2) cannot be
applied here.
We introduce indicator variables IY  and IY for Y  and Y , and an indicator variable IXi
for each parent of Y  . The weights of these variables are as follows,
weight(IY  ) = weight(¬IY  ) = 1,
weight(IY ) = weight(¬IY ) = 1,
weight(IXi ) = weight(¬IXi ) = 1.
For each arc Xi → Y  , 1 ≤ i ≤ n, we create two parameter variables PX0 i ,Y  and PX1 i ,Y  . The
weights of these variables are as follows,
weight(PX0 i ,Y  )
weight(¬PX0 i ,Y  )

=
=

1,
0,

weight(PX1 i ,Y  )
weight(¬PX1 i ,Y  )

=
=

qi ,
1 − qi .

For each factorization table, we introduce two variables, uY and wY , where the weights of
these variables are given by,
weight(uY )
weight(wY )

=
=

1,
−1,

weight(¬uY )
weight(¬wY )

=
=

0,
2.

For the ﬁrst row of a factorization table, we generate the clause,
(¬IY  ∨ IY ),

(11)

and for the second row, we generate the clauses,
(¬IY  ∨ ¬IY ∨ uY ) ∧ (IY  ∨ ¬IY ∨ wY ).

(12)

Finally, for every parent Xi of Y  , we generate the clauses,
(IY  ∨ IXi ∨ PX0 i ,Y  ) ∧ (IY  ∨ ¬IXi ∨ PX1 i ,Y  ).

(13)

We now have a conjunction of clauses; i.e., CNF.
Example 8. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-ORs shown in Table 1. The auxiliary graph transformation is shown in
Figure 5. The WMC2 encoding introduces the seven Boolean indicator variables IC , IF ,
 , I , I  , and I ; the twelve parameter variables,
IM , IN
N
H
H
0
PC,N

0
PF,N

0
PM,N


1
PC,N

1
PF,N

1
PM,N


0
PC,H

0
PF,H

0
PM,H


747

1
PC,H

1
PF,H

1
PM,H
;

Li, Poupart, & van Beek

Cold

Flu

Malaria

N’

H’

Nausea

Headache

Figure 5: Dı́ez and Galán’s (2003) transformation of a noisy-OR relation applied to the
Bayesian network shown in Figure 2.

and the four factorization variables uN , wN , uH , and wH . The noisy-OR at node Nausea
can be encoded as the set of clauses,
¬IN  ∨ IN
¬IN  ∨ ¬IN ∨ uN
IN  ∨ ¬IN ∨ wN

0
IN  ∨ IC ∨ PC,N

0
IN  ∨ IF ∨ PF,N 
0
IN  ∨ IM ∨ PM,N


1
IN  ∨ ¬IC ∨ PC,N

1
IN  ∨ ¬IF ∨ PF,N

1
IN  ∨ ¬IM ∨ PM,N


To illustrate the weighted model counting of the formula, suppose that nausea and malaria
are absent and cold and flu are present (i.e., Nausea = 0, Malaria = 0, Cold = 1, and
Flu = 1; and for the corresponding indicator variables IN and IM are false and IC and IF
are true). The formula can be simplified to,
1
1
0
PC,N
 ∧ PF,N  ∧ PM,N  .

(To see this, note that clauses that evaluate to true are removed and literals that evaluate to
false are removed from a clause. As a result of simplifying the first clause, IN  is forced to
be false and is removed from the other clauses.) There is just one model for this formula,
the model that sets each of the conjuncts to true. Hence, the weighted model count of this
1
1
0
formula is weight(PC,N
 ) × weight(PF,N  ) × weight(PM,N  ) = 0.6 × 0.5 × 1.0 = 0.3, which
is just the entry in the penultimate row of the full CPT shown in Example 2.
Once again, it can be seen that WMC2 can also easily encode evidence into the CNF
formula; i.e., if IY = 0 or IY = 1, the formula can be further simpliﬁed.
Example 9. Consider once again the Bayesian network shown in Figure 2. To illustrate
the encoding of evidence, suppose that nausea is present (i.e., Nausea = 1) and headache
is not present (i.e., Headache = 0). The WMC2 encoding results in the following set of
clauses,

748

Exploiting Structure in Probabilistic Inference

¬IN  ∨ uN
IN  ∨ wN
¬IH 

0
IN  ∨ IC ∨ PC,N

0
IN  ∨ IF ∨ PF,N 
0
IN  ∨ IM ∨ PM,N


1
IN  ∨ ¬IC ∨ PC,N

1
IN  ∨ ¬IF ∨ PF,N 
1
IN  ∨ ¬IM ∨ PM,N


0
IC ∨ PC,H

0
IF ∨ PF,H 
0
IM ∨ PM,H


1
¬IC ∨ PC,H

1
¬IF ∨ PF,H 
1
¬IM ∨ PM,H


To show the correctness of encoding WMC2 of a noisy-OR, we ﬁrst show that each entry
in the full CPT representation of a noisy-OR relation can be determined using the weighted
model count of the encoding. As always, let there be causes X1 , . . . , Xn leading to an
eﬀect Y and let there be a noisy-OR relation at node Y , where all random variables have
Boolean-valued domains.
Lemma 3. Each entry in the full CPT representation of a noisy-OR at a node Y , P (Y =
y | X1 = x1 , . . . , Xn = xn ), can be determined using the weighted model count of Equations 11−13 created using the encoding WMC2.
Proof. Let FY be the encoding of the noisy-OR at node Y using WMC2 and let s be the
set of assignments to the indicator variables IY , IX1 , . . . , IXn corresponding to the desired
entry in the CPT. For each Xi = 0, the clauses in Equation 13 reduce to (IY  ∨ PX0 i ,Y  ), and
for each Xi = 1, the clauses reduce to (IY  ∨ PX1 i ,Y  ). If IY = 0, the clauses in Equations 11
& 12 reduce to (¬IY  ). Hence,


weight(PX0 i ,Y  ))
weight(PX1 i ,Y  ))
weight(FY |s ) = weight(¬IY  )
=



i∈Tx

i∈Tx

qi

i∈Tx

= P (Y = 0 | X),
where Tx = {i | Xi = 1} and P (Y = 0 | X) = 1 if Tx is empty. If IY = 1, the clauses in
Equations 11 & 12 reduce to (¬IY  ∨ uY ) ∧ (IY  ∨ wY ). Hence,

qi +
weight(FY |s ) = weight(¬IY  )weight(¬uY )weight(wY )
i∈Tx

weight(¬IY  )weight(uY )weight(wY )



i∈Tx

weight(IY  )weight(uY )weight(¬wY ) +
weight(IY  )weight(uY )weight(wY )

qi
= 1−
i∈Tx

= P (Y = 1 | X).

749

qi +

Li, Poupart, & van Beek

The remainder of the proof of correctness for encoding WMC2 is similar to that of
encoding WMC1.
Lemma 4. Each entry in the joint probability distribution, P (Z1 = z1 , . . . Zn = zn ), represented by a noisy-OR Bayesian network can be determined using weighted model counting
and encoding WMC2.
Theorem 2. Given a noisy-OR Bayesian network, general queries of the form P (Q | E)
can be determined using weighted model counting and encoding WMC2.

5. Eﬃcient Encodings of Noisy-MAX into CNF
In this section, we present techniques for improving the weighted model counting approach
for Bayesian networks with noisy-MAX relations. In particular, we present two CNF encodings for noisy-MAX relations that exploit their structure or semantics. We again use as
a running example the Bayesian network shown in Figure 2.
Let there be causes X1 , . . . , Xn leading to an eﬀect Y and let there be a noisy-MAX
relation at node Y (see Figure 1), where the random variables may have multi-valued (nonBoolean) domains. Let dX be the number of values in the domain of some random variable
X.
The WMC2 multiplicative encoding above can be extended to noisy-MAX by introducing more indicator variables to represent variables with multiple values. In this section,
we explain the extension and present two noisy-MAX encodings based on two diﬀerent
weight deﬁnitions of the parameter variables. The two noisy-MAX encodings are denoted
MAX1 and MAX2, respectively. We begin by presenting those parts of the encodings that
MAX1 and MAX2 have in common. As with WMC2, these two noisy-MAX encodings take
as their starting point Dı́ez and Galán’s (2003) directed auxiliary graph transformation of
a Bayesian network with noisy-OR/MAX. Dı́ez and Galán show that for the noisy-MAX
relation, Equation (6) can be factorized as a product of matrices,
P (Y = y | X) =

y


MY (y, y  ) · P (Y ≤ y  | X)

(14)

y  =0

where MY is a dY × dY matrix given by,
⎧
⎪
if y  = y,
⎨1,

MY (y, y ) = −1, if y  = y − 1,
⎪
⎩
0,
otherwise.
For each noisy-MAX node Y , we introduce dY indicator variables IY0 ... IYdY −1 , to
 
represent each value in the domain of Y , and d2Y + 1 clauses to ensure that exactly one of
these variables is true. As in WMC2, we introduce a hidden node Y  with the same domain
as Y , corresponding indicator variables to represent each value in the domain of Y  , and
clauses to ensure that exactly one domain value is selected in each model. For each parent
Xi , 1 ≤ i ≤ n, of Y , we deﬁne indicator variables Ii,x , where x = 0, . . . , dXi − 1, and add

750

Exploiting Structure in Probabilistic Inference

clauses that ensure that exactly one of the indicator variables corresponding to each Xi is
true. Each indicator variable and each negation of an indicator variable has weight 1.
Example 10. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-MAX shown in Table 2. As the node Nausea has domain {absent =
0, mild = 1, severe = 2} and the parents Cold, Flu, and Malaria are Boolean valued, both
the MAX1 and MAX2 encodings introduce the Boolean indicator variables INa , INm , INs ,
 , IN  , IC , IC , IF , IF , IM , and IM . The weights of these variables and their
INa , INm
0
1
0
1
0
1
s
negations are 1. Four clauses are added over the indicator variables for Nausea,
(INa ∨ INm ∨ INs )

∧
∧
∧

(¬INa ∨ ¬INm )
(¬INa ∨ ¬INs )
(¬INm ∨ ¬INs ).

Similar clauses are added over the indicator variables for the hidden node N  and over the
indicator variables for the parents Cold, Flu, and Malaria, respectively.
For each factorization table, we introduce two auxiliary variables, uY and wY , where
the weights of these variables are given by,
weight(uY )
weight(wY )

=
=

1,
−1,

weight(¬uY )
weight(¬wY )

For each factorization table, a clause is added for each entry
⎧
⎪
add (¬Iy ∨ ¬Iy ∨ uY )
⎨1,

MY (y, y ) = −1, add (¬Iy ∨ ¬Iy ∨ wY )
⎪
⎩
0,
add (¬Iy ∨ ¬Iy ∨ ¬uY )

=
=

0,
2.

in the matrix,
if y  = y,
if y  = y − 1,
otherwise.

Example 11. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-MAX shown in Table 2. As Nausea has domain {absent = 0, mild
= 1, severe = 2}, the factorization table MN is given by,
N = absent
N = mild
N = severe

N  = absent
1
−1
0

N  = mild
0
1
−1

N  = severe
0
0
1.

Auxiliary variables uN and wN are introduced and the following clauses, shown in row order,
would be added for the factorization table MN ,
¬INa ∨ ¬INa ∨ uN
¬INm ∨ ¬INa ∨ wN
¬INs ∨ ¬INa ∨ ¬uN

 ∨ ¬uN
¬INa ∨ ¬INm
 ∨ uN
¬INm ∨ ¬INm
 ∨ wN
¬INs ∨ ¬INm

¬INa ∨ ¬INs ∨ ¬uN
¬INm ∨ ¬INs ∨ ¬uN
¬INs ∨ ¬INs ∨ uN .

That completes the description of those parts of the encodings that are common to both
MAX1 and MAX2.

751

Li, Poupart, & van Beek

5.1 Weighted CNF Encoding 1 for Noisy-MAX
Our ﬁrst weighted model counting encoding for noisy-MAX relations (MAX1) is based on an
additive deﬁnition of noisy-MAX. Recall the decomposed probabilistic model for the noisyMAX relation discussed at the end of Section 2.1. It can be shown that for the noisy-MAX,
P (Y ≤ y | X1 , . . . , Xn ) can be determined using,
P (Y ≤ y | X1 , . . . , Xn ) =

n


P (Yi | Xi ) =

Yi ≤y i=1

n
 
Yi ≤y i=1
Xi =0

Xi
qi,Y
i

(15)

Xi
are the parameters to the noisy-MAX, and the sum is over all the conﬁguwhere the qi,Y
i
rations or possible values for Y1 , . . . , Yn , such that each of these values is less than or equal
to the value y. Note that the outer operator is summation; hence, we refer to MAX1 as an
additive encoding. Substituting the above into Equation 14 gives,
⎛
⎞

P (Y = y | X1 , . . . , Xn ) =

y

y  =0

n
⎜ 
⎟
Xi ⎟
⎜
MY (y, y ) · ⎝
qi,Y
.
i⎠


(16)

Yi ≤y  i=1
Xi =0

It is this equation that we encode into CNF. The encoding of the factorization table MY
is common to both encodings and has been explained above. It remains to encode the
computation for P (Y ≤ y | X1 , . . . , Xn ).
For each parent Xi , 1 ≤ i ≤ n, of Y we introduce dY indicator variables, Ii,y , to represent
the eﬀect of Xi on Y , where 0 ≤ y ≤ dY − 1, and add clauses that ensure that exactly one of
the indicator variables correspond to each Xi is true. Note that these indicators variables
are in addition to the indicator variables common to both encodings and explained above.
As always with indicator variables, the weights of Ii,y and ¬Ii,y are both 1.
x to the noisy-MAX, we introduce a corresponding parameter
For each parameter qi,y
x
variable Pi,y . The weight of each parameter variable is given by,
x
x
) = qi,y
weight(Pi,y

x
weight(¬Pi,y
)=1

where 1 ≤ i ≤ n, 0 ≤ y ≤ dY − 1, and 1 ≤ x ≤ dXi − 1. The relation between Xi and Y is
represented by the parameter clauses1 ,
x
(Ii,x ∧ Ii,y ) ⇔ Pi,y

where 1 ≤ i ≤ n, 0 ≤ y ≤ dY − 1, and 1 ≤ x ≤ dXi − 1.
Example 12. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-MAX shown in Table 2. For the noisy-MAX at node Nausea, the
encoding introduces the indicator variables IC,Na , IC,Nm , IC,Ns , IF,Na , IF,Nm , IF,Ns , IM,Na ,
IM,Nm , and IM,Ns , all with weight 1, and the clauses,
1. To improve readability, in this section the propositional formulas are sometimes written in a more natural
but non-clausal form. We continue to refer to them as clauses when the translation to clause form is
straightforward.

752

Exploiting Structure in Probabilistic Inference

IC,Na ∨ IC,Nm ∨ IC,Ns
¬IC,Na ∨ ¬IC,Nm
¬IC,Na ∨ ¬IC,Ns
¬IC,Nm ∨ ¬IC,Ns

IF,Na ∨ IF,Nm ∨ IF,Ns
¬IF,Na ∨ ¬IF,Nm
¬IF,Na ∨ ¬IF,Ns
¬IF,Nm ∨ ¬IF,Ns

IM,Na ∨ IM,Nm ∨ IM,Ns
¬IM,Na ∨ ¬IM,Nm
¬IM,Na ∨ ¬IM,Ns
¬IM,Nm ∨ ¬IM,Ns

As well, the following parameter variables and associated weights would be introduced,
1
) = 0.7
weight(PC,N
a
1
weight(PC,N
) = 0.2
m
1
weight(PC,Ns ) = 0.1

1
weight(PF,N
) = 0.5
a
1
weight(PF,N
) = 0.2
m
1
weight(PF,Ns ) = 0.3

1
weight(PM,N
) = 0.1
a
1
weight(PM,N
) = 0.4
m
1
weight(PM,Ns ) = 0.5,

along with the following parameter clauses,
1
(IC1 ∧ IC,Na ) ⇔ PC,N
a
1
(IC1 ∧ IC,Nm ) ⇔ PC,N
m
1
(IC1 ∧ IC,Ns ) ⇔ PC,N
s

1
(IF1 ∧ IF,Na ) ⇔ PF,N
a
1
(IF1 ∧ IF,Nm ) ⇔ PF,N
m
1
(IF1 ∧ IF,Ns ) ⇔ PF,N
s

1
(IM1 ∧ IM,Na ) ⇔ PM,N
a
1
(IM1 ∧ IM,Nm ) ⇔ PM,N
m
1
(IM1 ∧ IM,Ns ) ⇔ PM,N
s

It remains to relate (i) the indicator variables, Ii,x , which represent the value of the
parent variable Xi , where x = 0, . . . , dXi −1; (ii) the indicator variables, Ii,y , which represent
the eﬀect of Xi on Y , where y = 0, . . . , dY − 1; and (iii) the indicator variables, IY  , which
y

represent the value of the hidden variable Y  , where y  = 0, . . . , dY − 1. Causal independent
clauses deﬁne the relation between (i) and (ii) and assert that if the cause Xi is absent
(Xi = 0), then Xi has no eﬀect on Y ; i.e.,
Ii,x0 ⇒ Ii,y0
where 1 ≤ i ≤ n. Value constraint clauses deﬁne the relation between (ii) and (iii) and
assert that if the hidden variable Y  takes on a value y  , then the eﬀect of Xi on Y cannot
be that Y takes on a higher degree or more severe value y; i.e.,
IY  ⇒ ¬Ii,Yy
y

where 1 ≤ i ≤ n, 0 ≤ y  ≤ dY − 1, and y  < y ≤ dY − 1.
Example 13. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-MAX shown in Table 2. For the noisy-MAX at node Nausea, the
encoding introduces the causal independence clauses,
IC0 ⇒ IC,Na

IF0 ⇒ IF,Na

IM0 ⇒ IM,Na

the value constraint clauses for N  = absent,
INa ⇒ ¬IC,Nm
INa ⇒ ¬IC,Ns

INa ⇒ ¬IF,Nm
INa ⇒ ¬IF,Ns

INa ⇒ ¬IM,Nm
INa ⇒ ¬IM,Ns

and the value constraint clauses for N  = mild,
 ⇒ ¬IC,N
IN m
s

 ⇒ ¬IF,N
INm
s

753

 ⇒ ¬IM,N
INm
s

Li, Poupart, & van Beek

5.2 Weighted CNF Encoding 2 for Noisy-MAX
Our second weighted model counting encoding for noisy-MAX relations (MAX2) is based
on a multiplicative deﬁnition of noisy-MAX. Equation 5 states that P (Y ≤ y | X1 , . . . , Xn )
can be determined using,
y
n 

xi
qi,y
(17)
P (Y ≤ y | X) =
.
i=1 y  =0
xi =0

Note that the outer operator is multiplication; hence we refer to MAX2 as a multiplicative
encoding. Substituting the above into Equation 14 gives,
⎛
⎞
P (Y = y | X1 , . . . , Xn ) =

y

y  =0



y
n 
⎜
⎟
xi ⎟
MY (y, y ) · ⎜
qi,y
 ⎠ .
⎝


(18)

i=1 y  =0
xi =0

It is this equation that we encode into CNF. The encoding of the factorization table MY
is common to both encodings and has been explained above. It remains to encode the
computation for P (Y ≤ y | X1 , . . . , Xn ).
x to the noisy-MAX, we introduce a corresponding parameter
For each parameter qi,y
x . The weight of each parameter variable pre-computes the summation in Equavariable Pi,y
tion 17,
y

x
x
x
)=
qi,y
weight(¬Pi,y
)=1
weight(Pi,y

y  =0

where 1 ≤ i ≤ n, 0 ≤ y ≤ dY − 1, and 1 ≤ x ≤ dXi − 1. The relation between Xi and Y  is
represented by the parameter clauses,
x
,
(Ii,x ∧ Iy ) ⇔ Pi,y

where 0 ≤ y ≤ dY − 1 and 0 ≤ x ≤ dXi − 1.
Example 14. Consider once again the Bayesian network shown in Figure 2 and the parameters for the noisy-MAX shown in Table 2. For the noisy-MAX at node Nausea, the
following parameter variables and associated weights would be introduced,
1
) = 0.7
weight(PC,N
a
1
) = 0.9
weight(PC,N
m
1
weight(PC,Ns ) = 1

1
weight(PF,N
) = 0.5
a
1
weight(PF,N
) = 0.7
m
1
weight(PF,Ns ) = 1

1
weight(PM,N
) = 0.1
a
1
weight(PM,N
) = 0.5
m
1
weight(PM,Ns ) = 1,

along with the following parameter clauses,
1
(IC1 ∧ INa ) ⇔ PC,N
a
1
 ) ⇔ P
(IC1 ∧ INm
C,Nm
1
(IC1 ∧ INs ) ⇔ PC,N
s

1
(IF1 ∧ INa ) ⇔ PF,N
a
1
 ) ⇔ P
(IF1 ∧ INm
F,Nm
1
(IF1 ∧ INs ) ⇔ PF,N
s

1
(IM1 ∧ INa ) ⇔ PM,N
a
1
 ) ⇔ P
(IM1 ∧ INm
M,Nm
1
(IM1 ∧ INs ) ⇔ PM,N
s

As stated so far, the encoding is suﬃcient for correctly determining each entry in the full
CPT representation of a noisy-MAX relation using weighted model counting. However, to
754

Exploiting Structure in Probabilistic Inference

improve the eﬃciency of the encoding, we add redundant clauses. The redundant clauses do
not change the set of solutions to the encoding, and thus do not change the weighted model
count. They do, however, increase propagation and thus the overall speed of computation
in the special case where all of the causes are absent. To this end, for each noisy-MAX node
Y , we introduce an auxiliary variable IvY with weights given by,
weight(IvY )

=

1,

weight(¬IvY )

and we introduce the clauses,

n

Ii,0 ⇒ (IY0 ⇒ IvY ),

n


i

=

0,


⇒ (IY0 ⇒ IvY ),

Ii,0

i

and the clauses,


n


n



Ii,0

⇒ (Iy ⇒ ¬IvY ),

i


Ii,0

⇒ (Iy ⇒ ¬IvY ),

i

where 1 ≤ y  ≤ dY − 1 and 1 ≤ y ≤ dY − 1.
Example 15. Consider once again the Bayesian network shown in Figure 2. For the
noisy-MAX at node Nausea, an auxiliary variable IvN is introduced with weight(IvN ) = 1
and weight(¬IvN ) = 0 along with the following redundant clauses,
(IC0 ∧ IF0 ∧ IM0 ) ⇒ (IN  ⇒ IvN )
a
(IC0 ∧ IF0 ∧ IM0 ) ⇒ (IN  ⇒ ¬IvN )
m
(IC0 ∧ IF0 ∧ IM0 ) ⇒ (IN  ⇒ ¬IvN )

(IC0 ∧ IF0 ∧ IM0 ) ⇒ (INa ⇒ IvN )
(IC0 ∧ IF0 ∧ IM0 ) ⇒ (INm ⇒ ¬IvN )
(IC0 ∧ IF0 ∧ IM0 ) ⇒ (INs ⇒ ¬IvN ).

s

6. Experimental Evaluation
In this section, we empirically evaluate the eﬀectiveness of our encodings. We use the
Cachet solver2 in our experiments as it is one of the fastest weighted model counting solvers.
We compare against Ace (version 2) (Chavira et al., 2005) and Dı́ez and Galán’s (2003)
approach using variable elimination.
We chose to compare against Ace for two reasons. First, Ace did well in the 2008 exact
inference competition (no winner was declared, but Ace performed better on more classes of
problems than all other entries). Second, other methods that are publicly available or that
did well at the competition, such as Smile/GeNIe (Druzdzel, 2005) or Cachet using a general
encoding on the full CPT representation, currently do not take any computational advantage
of noisy-OR and noisy-MAX and thus would be “straw” algorithms. A strength of Ace
is that it does take advantage of local structure and determinism and it speciﬁcally takes
advantage of the semantics of the noisy-OR and noisy-MAX to speed up computation. The
comparison to Ace, while revealing, is not without its methodological diﬃculties however
(see Section 6.4 for a discussion).
2. http://www.cs.rochester.edu/u/kautz/Cachet/index.htm

755

Li, Poupart, & van Beek

We chose to compare against Dı́ez and Galán’s (2003) approach, which consists of variable elimination applied to an auxiliary network that permits exploitation of causal independence, as they show that the approach is more eﬃcient than previous proposals for
noisy-MAX. To our knowledge, this work has not been subsequently superseded; i.e., it is
still the state-of-the-art on improving variable elimination for noisy-MAX for exact inference. No implementation of Dı́ez and Galán’s approach is publicly available, and so we
implemented it ourselves. Our implementation uses algebraic decision diagrams (ADDs)
(Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993) as the base data structure
to represent conditional probability tables. Algebraic decision diagrams permit a compact
representation by aggregating identical probability values and speed up computation by exploiting context-speciﬁc independence (Boutilier, Friedman, Goldszmidt, & Koller, 1996),
taking advantage of determinism and caching intermediate results to avoid duplicate computation. While ADDs are more complicated than table based representations, their ability
to exploit structure often yields a speed up that is greater than the incurred overhead. In
fact, ADDs are currently the preferred data structure for inference in factored partially
observable Markov decision processes (Shani, Brafman, Shimony, & Poupart, 2008). The
variable elimination heuristic that we used is a greedy one that ﬁrst eliminates all variables
that appear in deterministic potentials of one variable (this is equivalent to unit propagation) and then eliminates the variable that creates the smallest algebraic decision diagram
with respect to the eliminated algebraic decision diagrams. In order to avoid creating an
algebraic decision diagram for each variable when searching for the next variable to eliminate, the size of a new algebraic decision diagram is estimated by the smallest of two upper
bounds: (i) the cross product of the domain size of the variables of the new algebraic decision diagram and (ii) the product of the sizes (e.g., the number of nodes) of the eliminated
algebraic decision diagrams.
Good variable ordering heuristics play an important role in the success of modern DPLLbased model counting solvers. Here, we evaluate two heuristics: Variable State Aware
Decaying Sum (VSADS) and Tree Decomposition Variable Group Ordering (DTree). The
VSADS heuristic is one of the current best performing dynamic heuristics designed for
DPLL-based model counting engines (Sang, Beame, & Kautz, 2005b). It can be viewed as a
scoring system that attempts to satisfy the most recent conﬂict clauses and also considers the
number of occurrences of a variable at the same time. Compared with the VSADS heuristic,
the DTree heuristic (Huang & Darwiche, 2003) can be described as a mixed variable ordering
heuristic. DTree ﬁrst uses a binary tree decomposition to generate ordered variable groups.
The decomposition is done prior to search. The order of the variables within a group is
then decided dynamically during the backtracking search using a dynamic heuristic.
All of the experiments were performed on a Pentium workstation with 3GHz hyperthreading CPU and 2GB RAM.
6.1 Experiment 1: Random Two-Layer Networks
In our ﬁrst set of experiments, we used randomly generated two-layer networks to compare
the time and space eﬃciency of the WMC1 and WMC2 encodings.
Both the WMC1 and WMC2 encodings can answer probabilistic queries using Equation 7. Both encodings lead to quick factorization given evidence during the encoding. The

756

Exploiting Structure in Probabilistic Inference

clauses from negative evidence can be represented compactly in the resulting CNF, even
with a large number of parents. In the WMC2 encoding, positive evidence can be represented by just three Boolean variables (see Example 9 for an illustration of which variables
are deleted and which are kept for the case of positive evidence), whereas the WMC1 encoding requires n Boolean variables, one for each parent (see Example 7). In the WMC2
encoding, we use two parameter variables (PX0 i ,Y  and PX1 i ,Y  ) to represent every arc, while
the WMC1 encoding only needs one.
Table 3: Binary, two layer, noisy-OR networks with 500 diseases and 500 symptoms. Eﬀect
of increasing amount of positive evidence (P+ ) on number of variables in encoding
(n), treewidth of the encoding (width), average time to solve (sec.), and number
of instances solved within a cutoﬀ of one hour (solv.), where the test set contained
a total of 30 instances. The results for P+ = 5, . . . , 25 are similar to the results
for P+ = 30 and are omitted.

P+
30
35
40
45
50
55
60

n
3686
3716
3746
3776
3806
3836
3916

WMC1
width sec.
10
0.2
11
0.6
13
21.4
14
38.8
19
75.3
22
175.2
24

solv.
30
30
30
30
30
30
17

n
6590
6605
6620
6635
6650
6665
6680

WMC2
width sec.
11
0.1
11
0.2
11
0.5
13
2.0
13
6.1
16
71.0
16

solv.
30
30
30
30
30
30
27

Ace
sec. solv.
31.7
30
32.5
30
32.7
30
35.7
30
40.9
30
166.0
30
21

Each random network contains 500 diseases and 500 symptoms. Each symptom has six
possible diseases uniformly distributed in the disease set. Table 3 shows the treewidth of the
encoded CNF for the WMC1 and WMC2 encodings. The ﬁrst column shows the amount
of positive evidence in the symptom variables. The remainder of the evidence variables
are negative symptoms. It can be seen that although the WMC1 encoding generates fewer
variables than the WMC2 encoding, the CNF created by the WMC2 encoding has smaller
width. The probability of evidence (PE) is computed using the tree decomposition guided
variable ordering (Huang & Darwiche, 2003) and the results are compared against Ace3 (a
more detailed experimental analysis is given in the next experiments).
6.2 Experiment 2: QMR-DT
In our second set of experiments, we used a Bayesian network called QMR-DT. In comparison to randomly generated problems, QMR-DT presents a real-world inference task with
various structural and sparsity properties. For example, in the empirical distribution of
diseases, a small proportion of the symptoms are connected with a large number of diseases
(see Figure 6).
3. http://reasoning.cs.ucla.edu/ace/

757

Li, Poupart, & van Beek

Number of symptoms

1000

100

10

1
50

100

150

200

250

300

350

400

450

500

550

Number of diseases

Figure 6: Empirical distribution of diseases in the QMR-DT Bayesian network. Approximately 80% of the symptoms are connected with less than 50 diseases.

The network we used was aQMR-DT, an anonymized version of QMR-DT4 . Symptom
vectors with k positive symptoms were generated for each experiment. For each evidence
vector, the symptom variables were sorted into ascending order by their parent (disease)
number, the ﬁrst k variables were chosen as positive symptoms, and the remaining symptom
variables were set to negative. The goal of the method is to generate instances of increasing
diﬃculty.
We report the runtime to answer the probability of evidence (PE) queries. We also
experimented with an implementation of Quickscore5 , but found that it could not solve any
of the test cases shown in Figure 7. The approach based on weighted model counting also
outperforms variable elimination on QMR-DT. The model counting time for 2560 positive
symptoms, when using the WMC1 encoding and the VSADS dynamic variable ordering
heuristic, is 25 seconds. This same instance could not be solved within one hour by variable
elimination.
We tested two diﬀerent heuristics on each encoding: the VSADS dynamic variable order
heuristic and DTree (Huang & Darwiche, 2003), the semi-static tree decomposition-based
heuristic. The runtime using an encoding and the DTree heuristic is the sum of two parts:
the preprocessing time by DTree and the runtime of model counting on the encoding. In
this experiment, DTree had a faster runtime than VSADS in the model counting process.
However, the overhead of preprocessing for large size networks is too high to achieve better
overall performance.
The WMC2 encoding generates twice as many variables as the WMC1 encoding. Although the WMC2 encoding is more promising than the WMC1 encoding on smaller size
4. http://www.utoronto.ca/morrislab/aQMR.html
5. http://www.cs.ubc.ca/∼ murphyk/Software/BNT/bnt.html

758

Exploiting Structure in Probabilistic Inference

10000

Runtime (seconds)

1000

VE
WMC2 + DTree
WMC1 + DTree
WMC1 + VSADS

100

10

1

0.1
2000

2100

2200
2300
2400
Number of positive symptoms

2500

2600

Figure 7: The QMR-DT Bayesian network with 4075 symptoms and 570 diseases. Eﬀect
of amount of positive symptoms on the time to answer probability of evidence
queries, for the WMC1 encoding and the DTree variable ordering heuristic, the
WMC1 encoding and the VSADS variable ordering heuristic, the WMC2 encoding
and the DTree variable ordering heuristic, and Dı́ez and Galán’s (2003) approach
using variable elimination. Ace could not solve instances with more than 500
positive symptoms within a one hour limit on runtime.

networks (see Table 3), here the WMC2 encoding is less eﬃcient than the WMC1 encoding.
The overhead of the tree decomposition ordering on the WMC2 encoding is also higher
than on the WMC1 encoding. Our results also show that dynamic variable ordering does
not work well on the WMC2 encoding. Model counting using the WMC2 encoding and the
VSADS heuristic cannot solve networks when the amount of positive evidence is greater
than 1500 symptoms.
The experimental results also show that our approach is more eﬃcient than Ace. For
example, using Ace, a CNF of QMR-DT with 30 positive symptoms creates 2.8 × 105
variables, 2.8 × 105 clauses and 3.8 × 105 literals. Also, it often requires more than 1GB of
memory to ﬁnish the compilation process. With the WMC1 encoding, the same network
and the same evidence create only 4.6× 104 variables, 4.6× 104 clauses and 1.1× 105 literals.
Cachet, the weighted model counting engine, needs less than 250MB of memory in most
cases to solve these instances. And in our experiments, Ace could not solve QMR-DT with
more than 500 positive symptoms within an hour.

759

Li, Poupart, & van Beek

6.3 Experiment 3: Random Multi-Layer Networks
In our third set of experiments, we used randomly generated multi-layer Bayesian networks.
To test randomly generated multi-layer networks, we constructed a set of acyclic Bayesian
networks using the same method as Dı́ez and Galán (2003): create n binary variables;
randomly select m pairs of nodes and add arcs between them, where an arc is added from
Xi to Xj if i < j; and assign a noisy-OR distribution or a noisy-MAX distribution to each
node with parents.
160
140

VE
WMC1 + DTree
WMC1 + VSADS

Runtime (seconds)

120
100
80
60
40
20
0
300

310

320

330 340 350 360 370
Number of hidden variables

380

390

400

Figure 8: Random noisy-OR Bayesian networks with 3000 random variables. Eﬀect of
number of hidden variables on average time to answer probability of evidence
queries, for the WMC1 encoding and the VSADS variable ordering heuristic,
the WMC1 encoding and the DTree variable ordering heuristic, and Dı́ez and
Galán’s (2003) approach using variable elimination.

Figure 8 shows the eﬀect of the number of hidden variables on the average time to
answer probability of evidence (PE) queries for random noisy-OR Bayesian networks. Each
data point is an average over 30 randomly generated instances, where each instance had
3000 nodes in total.
The results from the two layer QMR-DT and the multiple layer random noisy-OR show
that on average, the approach based on weighted model counting performed signiﬁcantly
better than variable elimination and signiﬁcantly better than Ace. All the approaches
beneﬁt from the large amount of evidence, but the weighted model counting approach
explores the determinism more eﬃciently with dynamic decomposition and unit propagation. In comparison to variable elimination, the weighted model counting approach encodes
the local dependencies among parameters and the evidence into clauses/constraints. The

760

Exploiting Structure in Probabilistic Inference

topological or structural features of the CNF, such as connectivity, can then be explored
dynamically during DPLL’s simpliﬁcation process.
Heuristics based primarily on conﬂict analysis have been successfully applied in modern
SAT solvers. However, Sang, Beame, and Kautz (2005b) note that for model counting it is
often the case that there are few conﬂicts in those parts of the search tree where there are
large numbers of solutions and in these parts a heuristic based purely on conﬂict analysis
will make nearly random decisions. Sang et al.’s (2005b) VSADS heuristic, which combines
both conﬂict analysis and literal counting, avoids this pitfall and can be seen to work very
well on these large Bayesian networks with large amounts of evidence. DTree is also a good
choice due to its divide-and-conquer nature. However, when we use DTree to decompose
the CNF generated from QMR-DT, usually the ﬁrst variable group contains more than 500
disease variables. As well, the overhead of preprocessing aﬀects the overall eﬃciency of this
approach.
1000

ACE
MAX1
MAX2

Runtime (seconds)

100

10

1

0.1

0.01
200

250

300

350

400

450

500

550

600

650

Number of arcs

Figure 9: Random noisy-MAX Bayesian networks with 100 ﬁve-valued random variables.
Eﬀect of number of arcs on average time to answer probability of evidence queries,
for the MAX1 encoding for noisy-MAX, the MAX2 encoding for noisy-MAX, and
Chavira, Allen, and Darwiche’s Ace (2005).

Similarly, we performed an experiment with 100 ﬁve-valued random variables. Figure 9
shows the eﬀect of the number of arcs on the average time to answer probability of evidence
(PE) queries for random noisy-MAX Bayesian networks. Each data point is an average
over 50 randomly generated instances. It can be seen that on these instances our CNF
encoding MAX2 out performs our encoding MAX1 and signiﬁcantly outperforms Chavira,
Allen, and Darwiche’s Ace (2005). It has been recognized that for noisy-MAX relations,
the multiplicative factorization has signiﬁcant advantages over the additive factorization
(Takikawa & D’Ambrosio, 1999; Dı́ez & Galán, 2003). Hence, one would expect that the
761

Li, Poupart, & van Beek

CNF encoding based on the multiplicative factorization (encoding MAX2) would perform
better than the CNF encoding based on the additive factorization (encoding MAX1). The
primary disadvantage of encoding MAX1 is that it must encode in the CNF summing over
all conﬁgurations. As a result, MAX1 generates much larger CNFs than MAX2, including
more variables and more clauses. In encoding MAX2, the weight of a parameter variable
represents the maximum eﬀect of each cause and hence minimizes the add computations.
6.4 Discussion
We experimentally evaluated our four SAT encodings—WMC1 and WMC2 for noisy-OR
and MAX1 and MAX2 for noisy-MAX—on a variety of Bayesian networks using the Cachet
weighted modeling counting solver. The WMC1 and MAX1 encodings can be characterized
as additive encodings and the WMC2 and MAX2 encodings as multiplicative encodings. In
our experiments, the multiplicative encodings gave SAT instances with smaller treewidth.
For noisy-OR, the additive encoding (WMC1) gave smaller SAT instances than the multiplicative encoding (WMC2). For noisy-MAX, it was the reverse and the additive encoding
(MAX1) gave larger SAT instances than the multiplicative encoding (MAX2). With regards to speedups, in the experiments for the noisy-OR, the results were mixed as to which
encoding is better; sometimes it was WMC1 and other times WMC2. In the experiments
for the noisy-MAX, the results suggest that the multiplicative encoding (MAX2) is better.
Here the reduced treewidth and the reduced size of the MAX2 encoding were important,
and WMC2 was able to solve many more instances.
We also compared against Dı́ez and Galán’s (2003) approach using variable elimination
(hereafter, D&G) and against Ace (Chavira et al., 2005). In our experiments, our approach
dominated D&G and Ace with speedups of up to three orders of magnitude. As well, our
approach could solve many instances which D&G and Ace could not solve within the
resource limits. However, our results should be interpreted with some care for at least three
reasons. First, it is well known that the eﬃciency of variable elimination is sensitive to the
variable elimination heuristic that is used and to how it is implemented. While we were
careful to optimize our implementation and to use a high-quality heuristic, there is still the
possibility that a diﬀerent implementation or a diﬀerent heuristic would lead to diﬀerent
results. Second, Cachet, which is based on search, is designed to answer a single query and
our experiments are based on answering a single query. However, Ace uses a compilation
strategy which is designed to answer multiple queries eﬃciently. The compilation step can
take a considerable number of resources (both time and space) which does not payoﬀ in our
experimental design. Third, although Ace can be viewed as a weighted model counting
solver, we are not comparing just encodings in our experiments. As Chavira and Darwiche
(2008) note, Cachet and Ace diﬀer in many ways including using diﬀerent methods for
decomposition, variable splitting, and caching. As well, Ace uses other optimizations that
Cachet does not, including encoding equal parameters, eclauses (a succinct way of encoding
that only one literal is true in a disjunction), and structured resolution. (We refer the reader
to Chavira & Darwiche, 2008 for an experimental comparison of search and compilation,
and an extensive discussion of the diﬃculty of comparing the two approaches and their
advantages and disadvantages.) Nevertheless, in our experiments we demonstrated instances
of noisy-OR networks (see Figure 7) and noisy-MAX networks (see Figure 9) that could not

762

Exploiting Structure in Probabilistic Inference

be solved at all by D&G and by Ace within the resource limits, but could be solved quite
quickly by Cachet using our encodings.

7. Conclusions and Future Work
Large graphical models, such as QMR-DT, are often intractable for exact inference when
there is a large amount of positive evidence. We presented time and space eﬃcient CNF
encodings for noisy-OR/MAX relations. We also explored alternative search ordering heuristics for the DPLL-based backtracking algorithm on these encodings. In our experiments,
we showed that together our techniques extend the model counting approach for exact inference to networks that were previously intractable for the approach. As well, while our
experimental results must be interpreted with some care as we are comparing not only
our encodings but also implementations of systems with conﬂicting design goals, on these
benchmarks our techniques gave speedups of up to three orders of magnitude over the best
previous approaches and scaled up to larger instances. Future work could include developing speciﬁc CNF encodings of other causal independence relations (see Koller & Friedman,
2009, pp. 175–185).

Acknowledgments
A preliminary version of this paper appeared as: Wei Li, Pascal Poupart, and Peter van
Beek. Exploiting Causal Independence Using Weighted Model Counting. In Proceedings
of the 23rd AAAI Conference on Artificial Intelligence, pages 337–343, 2008. The authors
wish to thank the anonymous referees for their helpful comments.

References
Bacchus, F., Dalmao, S., & Pitassi, T. (2003). DPLL with caching: A new algorithm for
#SAT and Bayesian inference. Electronic Colloquium on Computational Complexity,
10 (3).
Bahar, R. I., Frohm, E. A., Gaona, C. M., Hachtel, G. D., Macii, E., Pardo, A., & Somenzi,
F. (1993). Algebraic decision diagrams and their applications. In Proceedings of the
1993 IEEE/ACM International Conference on Computer-Aided Design (ICCAD-93),
pp. 188–191.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-speciﬁc independence in Bayesian networks. In Proceedings of the Twelfth Conference on Uncertainty
in Artificial Intelligence (UAI-96), pp. 115–123.
Chavira, M., Allen, D., & Darwiche, A. (2005). Exploiting evidence in probabilistic inference. In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence
(UAI-05), pp. 112–119.
Chavira, M., & Darwiche, A. (2005). Compiling Bayesian networks with local structure. In
Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence
(IJCAI-05), pp. 1306–1312.

763

Li, Poupart, & van Beek

Chavira, M., & Darwiche, A. (2008). On probabilistic inference by weighted model counting.
Artificial Intelligence, 172 (6-7), 772–799.
D’Ambrosio, B. (1994). Symbolic probabilistic inference in large BN2O networks. In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI-94),
pp. 128–135.
Darwiche, A. (2009). Modeling and Reasoning with Bayesian Networks. Cambridge.
Darwiche, A. (2002). A logical approach to factoring belief networks. In Proceedings of
the Eighth International Conference on Principles of Knowledge Representation and
Reasoning (KR-02), pp. 409–420.
Davis, M., Logemann, G., & Loveland, D. (1962). A machine program for theorem proving.
Communications of the ACM, 5 (7), 394–397.
Davis, M., & Putnam, H. (1960). A computing procedure for quantiﬁcation theory. J.
ACM, 7 (3), 201–215.
Dı́ez, F. J. (1993). Parameter adjustement in Bayes networks. The generalized noisy ORgate. In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence
(UAI-93), pp. 99–105.
Dı́ez, F. J., & Druzdzel, M. J. (2006). Canonical probabilistic models for knowledge engineering. Tech. rep. CISIAD-06-01, UNED, Madrid.
Dı́ez, F. J., & Galán, S. F. (2003). Eﬃcient computation for the noisy MAX. International
J. of Intelligent Systems, 18, 165–177.
Druzdzel, M. J. (2005). Intelligent decision support systems based on SMILE. Software 2.0,
2, 12–33.
Good, I. J. (1961). A causal calculus. The British Journal for the Philosophy of Science,
12 (45), 43–51.
Heckerman, D. (1989). A tractable inference algorithm for diagnosing multiple diseases. In
Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence (UAI-89),
pp. 163–172.
Heckerman, D., & Breese, J. (1996). Causal independence for probability assessment and
inference using Bayesian networks. IEEE, Systems, Man, and Cyber., 26, 826–831.
Heckerman, D. (1993). Causal independence for knowledge acquisition and inference. In
Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence (UAI93).
Henrion, M. (1987). Some practical issues in constructing belief networks. In Proceedings of
the Third Conference on Uncertainty in Artificial Intelligence (UAI-87), pp. 132–139.
Huang, J., & Darwiche, A. (2003). A structure-based variable ordering heuristic for SAT. In
Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence
(IJCAI-03), pp. 1167–1172.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques. The MIT Press.

764

Exploiting Structure in Probabilistic Inference

Littman, M. L. (1999). Initial experiments in stochastic satisﬁability. In Proceedings of the
Sixteenth National Conference on Artificial Intelligence (AAAI-99) (Orlando, Florida,
United States edition)., pp. 667–672.
Miller, R. A., Masarie, F. E., & Myers, J. D. (1986). Quick medical reference for diagnostic
assistance. Medical Computing, 3, 34–48.
Olesen, K. G., Kjaerulﬀ, U., Jensen, F., Jensen, F. V., Falck, B., Andreassen, S., & Andersen,
S. K. (1989). A MUNIN network for the median nerve: A case study on loops. Appl.
Artificial Intelligence, 3 (2-3), 385–403.
Parker, R., & Miller, R. (1987). Using causal knowledge to create simulated patient cases:
The CPCS project as an extension of INTERNIST-1. In The 11th Symposium Computer Applications in Medical Care, pp. 473–480.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann.
Sang, T., Bacchus, F., Beame, P., Kautz, H., & Pitassi, T. (2004). Combining component
caching and clause learning for eﬀective model counting. In Proceedings of the 7th
International Conference on Theory and Applications of Satisfiability Testing (SAT04).
Sang, T., Beame, P., & Kautz, H. (2005a). Solving Bayesian networks by weighted model
counting. In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI-05), pp. 1–10.
Sang, T., Beame, P., & Kautz, H. A. (2005b). Heuristics for fast exact model counting..
In Proceedings of the 8th International Conference on Theory and Applications of
Satisfiability Testing (SAT-05), pp. 226–240.
Shani, G., Brafman, R. I., Shimony, S. E., & Poupart, P. (2008). Eﬃcient ADD operations
for point-based algorithms. In Proceedings of the Eighteenth International Conference
on Automated Planning and Scheduling (ICAPS-08), pp. 330–337.
Takikawa, M., & D’Ambrosio, B. (1999). Multiplicative factorization of noisy-MAX. In
Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI99), pp. 622–630.
Zagorecki, A., & Druzdzel, M. J. (1992). Knowledge engineering for Bayesian networks:
How common are noisy-MAX distributions in practice?. In Proceedings of the 10th
European Conference on Artificial Intelligence (ECAI-92), pp. 482–489.
Zhang, N. L., & Poole, D. (1996). Exploiting causal independence in Bayesian network
inference. J. of Artificial Intelligence Research, 5, 301–328.

765

Journal of Artificial Intelligence Research 40 (2011) 677-700

Submitted 09/10; published 03/11

Identifying Aspects for Web-Search Queries
Fei Wu
Jayant Madhavan
Alon Halevy

wufei@google.com
jayant@google.com
halevy@google.com

Google Inc, 1600 Amphitheatre Pkwy,
Mountain View, CA 94043 USA

Abstract
Many web-search queries serve as the beginning of an exploration of an unknown space
of information, rather than looking for a speciﬁc web page. To answer such queries eﬀectively, the search engine should attempt to organize the space of relevant information in a
way that facilitates exploration.
We describe the Aspector system that computes aspects for a given query. Each
aspect is a set of search queries that together represent a distinct information need relevant
to the original search query. To serve as an eﬀective means to explore the space, Aspector
computes aspects that are orthogonal to each other and to have high combined coverage.
Aspector combines two sources of information to compute aspects. We discover
candidate aspects by analyzing query logs, and cluster them to eliminate redundancies.
We then use a mass-collaboration knowledge base (e.g., Wikipedia) to compute candidate
aspects for queries that occur less frequently and to group together aspects that are likely
to be “semantically” related. We present a user study that indicates that the aspects
we compute are rated favorably against three competing alternatives – related searches
proposed by Google, cluster labels assigned by the Clusty search engine, and navigational
searches proposed by Bing.

1. Introduction
Web-search engines today predominantly answer queries with a simple ranked list of results.
While this method has been successful, it relies on the assumption that the user’s information need can be satisﬁed with a single page on the Web. However, several studies (Broder,
2002; Rose & Levinson, 2004) have alluded to the fact that many user queries are merely
the beginning of an exploration of an unknown space of information. Such queries are likely
to be better served if users were provided with a summary of the relevant information space
and the means for conveniently exploring it. To paraphrase, rather than ﬁnding the needle
in the haystack, some queries would beneﬁt from summarizing the haystack (Rajaraman,
2008). Several commercial attempts have recently been made to provide better answers to
such queries, including systmes like Carrot2, Clusty, Kosmix, and Yahoo!Glue.
This paper describes the Aspector system that addresses the following problem: given
an exploratory search query q, compute a set of aspects that enables convenient exploration
of all the Web content that is relevant to q. We deﬁne each aspect as a set of search
queries that together represent a distinct information need relevant to the original search
query, similar to Wang’s deﬁnition of Latent Query Aspect (Wang, Chakrabarti, & Punera,
2009). For example, consider the queries in Table 1 and their potential aspects. Each
aspect covers a diﬀerent kind of information and together they span a large amount of the
c
⃝2011
AI Access Foundation. All rights reserved.

Wu, Madhavan, & Halevy

vietnam travel
travel guides
packages / agencies
visa
blogs / forums
travel advisories
weather
cities (Hanoi / Saigon /...)

kobe bryant
statistics
pictures / photos
videos / youtube
shoes
injury reports
girlfriend
trade rumors

Table 1: Potential aspects for the queries vietnam travel and kobe bryant.

relevant information that search engine users might be interested in. Two simple ways a
search engine can utilize aspects is to oﬀer them as related searches or to categorize search
results by the aspects they are most relevant to. Aspects can also form the basis for various
mashup-like interfaces, e.g., the aspect pictures can trigger the inclusion of images, while
weather can trigger a weather report gadget. Computing aspects for queries can be seen as a
ﬁrst step towards mining a knowledge base, called the database of user intentions (Battelle,
2005). This knowledge base is a timely and culture-sensitive expression of peoples’ interests.
Inferring such a knowledge base for entities can serve as a basis for an eﬀective presentation
of information, and can therefore have signiﬁcant ramiﬁcations on search, advertising and
information dissemination.
Aspector computes aspects for a query q using a search-engine query log, augmented
with information from a knowledge base created by mass-collaboration (Wikipedia). Given
a query q, related queries are extracted from the query log as candidate aspects. While the
logs are an excellent mirror of users’ interests, they can also result in noisy and redundant
aspects, e.g., top related queries for vietnam travel include vietnam visa and vietnam travel
visa. Furthermore, query logs are of limited utility for generating aspects for less popular
queries, e.g., there are much fewer related queries for laos travel than vietnam travel. We
describe the following algorithmic innovations that address these challenges. First, we show
how redundant candidate aspects can be removed using search results. Second, we apply
class-based label propagation in a bipartite graph to compute high-quality aspects even for
a long tail of less popular queries. Finally, we show that knowledge bases can be used to
group candidate aspects into categories that represent a single information need. We believe
that our solution demonstrates an interesting interplay between query logs and knowledge
bases that has not as yet been investigated in the research literature.
We describe a detailed experimental evaluation of Aspector. We compare the aspects
generated by Aspector against three possible competing approaches – related searches
proposed by Google.com, cluster labels proposed by Clusty.com, and navigational searches
proposed by Bing.com. Related searches and navigational searches are typically also generated by the analysis of query logs. Cluster labels are generated by grouping the search
results of the original query and extracting labels from the documents within each cluster.
We show that our aspects are more diverse than all other three systems. We also show that
our aspects span a larger space of information – not only do they expose more results than
the original query, but the additional results are considered highly relevant by users. Our
678

Identifying Aspects for Web-Search Queries

user study ﬁnds that the results of Aspector are preferred over related searches, cluster
labels and navigational searches as a means for further exploration.
Section 2 deﬁnes the problem of computing aspects, Section 3 considers potential alternative approaches. Section 4 describes the generation of candidate aspects, and Section 5
describes how Aspector selects aspects from the candidates. Section 6 describes our experimental evaluation,and Section 7 describes related work. Section 8 concludes.

2. Problem Definition
We begin by deﬁning the scope of the problem we address.
Queries: We assume that queries are a sequence of keywords, as is typical for searchengine interfaces. Our techniques are not meant to apply to arbitrary queries. We focus on
exploratory queries, and speciﬁcally, we assume that they either are entity names (e.g., the
country Vietnam) or have an entity and a property name (e.g., Vietnam travel). Thus, we
are interested in computing aspects for entities in general or in the context of a particular
property.
In this paper we do not handle the problem of segmenting entity and property names in
queries (previous work, such as Bergsma & Wang, 2007; Tan & Peng, 2008, have addressed
the problem). The question of identifying exploratory queries in a query stream is also
beyond the scope of this paper.
Aspects: An aspect of a query q is meant to describe a particular sense of q corresponding to an information need. Speciﬁcally, each aspect is represented by a collection of
search queries related to q. Given q, we compute a set of aspects a1 , . . . , an , along with
scores p(ai |q) that can then be used to rank them.
Since aspects are collections of search queries, we compare aspects based on the search
results retrieved by the queries that constitute them. Aspects are meant to capture diverse
dimensions along which we can organize the exploration of the entire space of information
relevant to query. Hence, the set of aspects computed for each query should have the
following properties:
Orthogonality: given two aspects, a1 and a2 , the search results for a1 and a2 should
be very diﬀerent from each other.
Coverage: the search results provided for the aspects should oﬀer a good overview of
the relevant space of information.
Thus, two sets of aspects computed for the same query can be compared based on the
pairwise orthogonality of their constituent aspects and the combined coverage of all their
aspects. The evaluation of aspects is inherently subjective, as is typical in the area of web
search. Hence, we present user studies where the aspects computed by diﬀerent approaches
are qualitatively rated by a large number of independent users. We note that while we
compare diﬀerent approaches to computing aspects, we do not focus on the diﬀerent ways
they can be presented to users.

3. Alternative Approaches
Before we describe how Aspector generates aspects, we brieﬂy mention two strawman
approaches to the problem and explain why they are insuﬃcient for our needs.
679

Wu, Madhavan, & Halevy

Class

NBA Player

University

Wikipedia
birth date
position
birth place
college
nationality height(ft)
draft year
draft
career start height(in)
name
city
established
website
country
type
campus
state
undergrad
motto

Query Log
injury
pictures
nba
wallpaper
bio
salary
shoes
girlfriend
stats
biography
library basketball
football
athletics
alumni admissions
tuition
baseball
jobs
bookstore

Table 2: Two classes that have attributes in Wikipedia that are very diﬀerent from classlevel aspects computed from the query log.

3.1 Community-Created Knowledge Bases
Knowledge bases, especially those created by a large community of contributors, are a
rich source of information about popular entities. They cover a wide spectrum of user
interests and can potentially be used to organize information relevant to search queries.
In particular, the properties in Infoboxes of Wikipedia articles can potentially be used as
candidate aspects. Wikipedia contains more than 3,500 classes with over 1 million entities
and each class on average has about 10 attributes. The Wikipedia column in Table 2 shows
the attributes for two example classes. Freebase is another community-created KB with
over 1,500 classes.
Binary relationships recorded in a knowledge base fall signiﬁcantly short of providing
a good set of aspects for a query. For example, consider the properties associated with
Cambodia in the Wikipedia Infobox – capital, flag, population, GDP, etc. None of these
words appear in the top-10 frequent queries that contain the word cambodia. In addition, a
knowledge base is limited to describing well deﬁned entities. For example, Cambodia is an
entity in a knowledge base, but Cambodia Travel is not. However, queries on the Web cover
much more than well deﬁned entities.
The underlying reason that knowledge bases fall short is that their constructors choose
attributes based on traditional design principles, but good aspects do not follow from these
principles. For example, it turns out that cambodia travel is a good aspect for vietnam travel,
because many people consider a side trip to Cambodia when visiting Vietnam. However,
when designing a knowledge base, Cambodia would never be an attribute of Vietnam.
Instead, the knowledge base would assert that Vietnam and Cambodia are neighbors, and
include a rule that states that if X is next to Y, and then X travel may be an aspect of Y
travel. Unfortunately, coming up with such rules and specifying their precise preconditions
is a formidable task and highly dependent on the instances it applies to. For example,
pakistan travel is not an aspect of india travel, even though the two countries are neighbors.
680

Identifying Aspects for Web-Search Queries

3.2 Web Documents
Another approach to ﬁnding aspects is to cluster the documents on the Web that are relevant
to a query q, and assign or extract labels for each cluster (Blei, Ng, & Jordan, 2003; Zeng,
He, Chen, Ma, & Ma, 2004). As we show in our experiments, the main disadvantage is
that the coverage of the resulting aspects may be low because this approach only considers
documents that were returned in response to the original query. In practice, users conduct
data exploration in sessions of queries, and the other queries in those sessions can also lead
to interesting aspects which might not be found among the results of the original query.
Furthermore, it can be very challenging to generate succinct names for aspects from each
cluster.

4. Generating Candidate Aspects
Aspector generates candidate aspects from query logs. Query logs are very reﬂective of a
broad range of user interests, but they are less eﬀective in generating aspects for infrequent
queries. We ﬁrst describe how we generate instance-level aspects, and then how we augment
them with class-based aspect propagation using a knowledge base.
4.1 Instance-Level Candidate Aspects
Given query q, we start by considering each of its query reﬁnements and super-strings as a
candidate aspect.
4.1.1 Query Refinements
A query qj is a reﬁnement of q, if a user poses qj after q while performing a single search
task. Query logs can be mined to identify popular reﬁnements for individual queries. Search
engines typically use popular reﬁnements as a basis for proposing related searches.
We process reﬁnements as follows: ﬁrst, the query log is segmented into sessions representing sequences of queries issued by a user for a single search task. Suppose fs (q, qj ) is
the number of sessions in which the query qj occurs after q, we then estimate the reﬁnement
score pr for each qj by normalizing fs (q, qj ) over all possible reﬁnements, i.e.,
fs (q, qj )
pr (qj |q) = ∑
i fs (q, qi )
Observe that proposing related searches based on query reﬁnements is, in principle, only
optimized towards the goal of helping users ﬁnd a single page containing a speciﬁc answer
(rather than helping the user explore the space). For example, the top 10 reﬁnements for
the query on the NBA player yao ming includes 6 other NBA players such as kobe bryant
and michael jordan. Though related, these reﬁnements are not necessarily the best aspects
for the query.
4.1.2 Query Super-Strings
The query qj is a super-string of q if it includes q as a sub-string. For example, vietnam
travel package is a super-string of vietnam travel. Unlike a reﬁnement, a super-string qj need
681

Wu, Madhavan, & Halevy

not belong to the same session as q. In fact, for a random selection of 10 popular queries,
we found that on average there is only an overlap of 1.7 between the top 10 reﬁnements
and the top 10 super-strings. In a sense, super-strings are explicitly related queries while
reﬁnements are more implicitly related.
Super-strings can be assigned scores similar to pr above, by mimicking each super-string
as a pseudo-reﬁnement, i.e., we assume an imaginary session in which q preceded superstring qj . Suppose f (qj ) was the number of occurrences of qj in the query logs, we estimate
the super-string score pss (qj |q) as below 1 :
pss (qj |q) =

f (qj )
∑
f (q) + i f (qi )

Aspector considers all the reﬁnements and super-strings of q as candidate aspects and
assigns them a single instance-level aspect score. For each candidate aspect qj , we assign
the score pinst as follows:
pinst (qj |q) = max(pr (qj |q), pss (qj |q))
For given q, we normalize all pinst (qj |q)s to add up to 1.
4.2 Class-Based Aspect Propagation
Query-log analysis is ineﬀective in generating instance-level candidate aspects for less frequent queries. For example, we generate good candidate aspects for vietnam travel, but
not for laos travel. However, we can recommend aspects that are common to travel to
many countries for Laos. We use a variation of the label-propagation algorithm named
Adsorption (Baluja, Seth, Sivakumar, Jing, Yagnik, Kumar, Ravichandran, & Aly, 2008).
We ﬁrst apply query segmentation to extract the entity e (laos in our example) and
the property p (travel) from the query q. Next, we use a knowledge base (e.g., Wikipedia
Infobox) to identify the class, or classes, C of e (e.g., country and south-east asian country
for laos). Then we construct a directed bipartite graph G = (V, E, ω) as shown in Figure 1.
The nodes on the left are instance-level query nodes such as “laos travel”, and on the right
are class-level nodes like “country travel”. E denotes the set of edges, and ω : E → R
denotes the nonnegative weight function. We set the weights of edges from instance nodes
to class nodes as 1, and the weights of edges from class nodes to instance nodes as K,
a design parameter controlling the relative-importance of the two factors. Our goal is to
−−−−→
compute p(qj |q), which is the aspect distribution on node q.
Following the work of Baluja et al. (2008), each node’s aspect distribution is iteratively
updated as the linear combination of its neighbors, until we converge (this algorithm is
shown to be equivalent to performing a random walk in the graph). Since we use Wikipedia
Infobox as our knowledge base, where each instance belongs to a single class, two iterations
are guaranteed to achieve convergence. The ﬁrst iteration computes the class-level aspects
as follows:
1 ∑
pinst (qj |q)
pclass (qj |q) =
|C| q∈C
1. We use a conservative lower bound estimate. The corresponding upper bound pss (qj |q) =
exceed 1.

682

f (qj )
f (q)

can

Identifying Aspects for Web-Search Queries

Instances

Classes

vietnam travel
southeast asia travel
laos travel
country travel
canada travel

...

...

Figure 1: The bipartite graph for class-based aspect propagation.
The second iteration smoothes the aspect distribution on instance node q with pclass (qj |q)
as follows,
pinst (qj |q) + K × pclass (qj |q)
(1)
1+K
In Section 6 we tested the eﬀect of K on the performance of Aspector. In our experiments we found that the following variation on computing class-level aspects leads to
slightly better results:
p(qj |q) =

pclass (qj |q) =

1 ∑
I(pinst (qj |q) > 0))
|C| q∈C

where, I(pinst (qj |q) > 0)) = 1 when pinst (qj |q) > 0, and 0 otherwise.
Table 2 shows examples of top class-level aspects derived for two classes and compares
them with their corresponding top attributes in the Wikipedia infobox. We can see that
the two sets of aspects have little overlap, which illustrates again that community created
schemata fall signiﬁcantly short of providing a good set of aspects for search queries.

5. Selecting Aspects
This section describes how Aspector prunes the set of candidate aspects, groups them,
and eventually ranks them in an ordered list from which a subset can be selected.
5.1 Eliminating Duplicate Aspects
It is often the case that the generated aspect list contains very similar candidates that may
be considered redundant. For example, top candidate aspects for the query vietnam travel
include vietnam travel package, vietnam travel packages and vietnam travel deal, each of which
represent either identical or very similar user intents. In particular, note that the set of
web documents returned for each of the aspects from any search engine are likely to be very
similar.
To remove redundant aspects, we compute a similarity matrix, {sim(ai , aj )}, between
every pair of candidate aspects and then cluster them based on their similarity.
5.1.1 Computing Aspect Similarity
Since most aspects only contain few words, estimating similarity based on a simple comparison of words is unlikely to be accurate. Therefore, we enrich the representation of each
683

Wu, Madhavan, & Halevy

aspect by considering the top m2 search results returned when posing the aspect as a search
query. This is consistent with our goal of enabling orthogonal exploration – aspects with
similar top results are unlikely to be orthogonal.
Let Di be the top web pages retrieved for the aspect ai . We estimate the similarity of ai
and aj to be the similarity of the corresponding sets Di and Dj . To compute sim(Di , Dj ),
we ﬁrst compute the similarity dsim for any given pair of web pages {di ∈ Di , dj ∈ Dj }.
For this we use the standard cosine distance between the TF/IDF word-vectors for the two
documents. For computational eﬃciency, we only consider the head and snippet for each
web page instead of their entire text contents3 .
While sim(Di , Dj ) can potentially be estimated by averaging similarities dsim(di , dj ) for
all pairs of web pages, on our experiment dataset, we found it better instead to compute the
average of the highest similarity for each web page. For each di ∈ Di , we assign the score:
sim(di , Dj ) = maxk dsim(di , dk ). Likewise, we assign sim(Di , dj ) = maxk dsim(dk , dj ).
The ﬁnal aspect similarity is computed as:
∑

∑

sim(ai , aj ) = sim(Di , Dj ) =

i sim(di , Dj )

2|Di |

+

j

sim(dj , Di )
2|Dj |

We could alternatively treat each Di as one single document by concatenating all {di ∈
Di } and estimate sim(qi , qj ) to be the corresponding dsim(Di , Dj ). While computationally
more eﬃcient, the quality of aspects was poorer.
5.1.2 Clustering Aspects
In principle, we can apply any clustering algorithm, such as K-means or spectral clustering,
to the resulting aspect similarity matrix. However, these algorithms often require pre-setting
the number of desired clusters, which is diﬃcult in our context. In addition, the number
of clusters also varies signiﬁcantly from one query to another. Note that the appropriate
number of clusters is not necessarily the number of resulting aspects that we will show the
user.
We instead apply a graph-partition algorithm for clustering. The algorithm proceeds by
creating a graph where the nodes are aspects, ai , and there is an edge connecting the nodes
ai and aj if sim(ai , aj ) > σ, where σ is a pre-deﬁned threshold. Each of the connected sub
graphs is treated as a cluster. We choose the label of the cluster to be the aspect ak with
the highest p(ak |q) in the cluster (Formula 1).
The only design parameter σ is easier to set and pretty stable for diﬀerent queries, as
shown in our experiments. We note that similar algorithms such as star-clustering (Aslam,
Pelekov, & Rus, 2004) can also be used.
5.2 Grouping Aspects by Vertical Category
In many cases, even after eliminating redundant aspects, we ﬁnd that we are left with
aspects that are seemingly diﬀerent, but can be semantically grouped into a single category.
For example, for the query vietnam travel, some of the top non-redundant candidate aspects
2. We use m = 8 in our experiments which performs well. Larger m might achieve slightly better performance at the cost of heavier computation.
3. We also tired using the whole document for each web page, which has only slightly better performance.

684

Identifying Aspects for Web-Search Queries

as ho chi minh city, hanoi and da nang. While these are diﬀerent cities, in principle they are
likely to represent a single information need – that of ﬁnding more information about cities
in Vietnam. Further, given a budget of a ﬁxed number of aspects that can be presented to
a user, it might not make sense to overwhelm them with a list of aspects all denoting cities
in Vietnam. Instead, a single aspect named Cities can be presented.
Here is where the community-created knowledge bases can be leveraged – Aspector
tries to identify sets of related aspects by consulting the Wikipedia Infobox system4 . If it
ﬁnds that multiple aspects contain diﬀerent entities that belong to a class in Wikipedia, it
creates an aggregate aspect (with the label of the class) and groups them together.
We encounter two challenges while looking up Wikipedia for the classes of entities.
First, the same entity can appear as diﬀerent synonymous tokens. For example, nyu is the
common acronym for new york university. Currently we use the redirect pages on Wikipedia
to infer synonyms. Redirect pages in Wikipedia point synonym terms to the same principal
article. As a result, the aspect nyu for the query yale university is grouped with harvard
university and oxford university5 . Second, the same token can refer to multiple entities that
belong to diﬀerent classes and it can lead to bad grouping decisions. For example, HIStory
and FOOD are the names of music albums in Wikipedia, but history and food are also
aspects for the query vietnam. A simple lookup of the tokens in Wikipedia might lead to
erroneously grouping them into a single album group. Aspector uses the disambiguation
pages in Wikipedia to identify tokens that are likely to have multiple senses. The Infobox
class is only retrieved for entities that do not have disambiguation pages. This conservative
method can be further improved via collaborative classiﬁcation (Meesookho, Narayanan, &
Raghavendra, 2002). For example, earth, moon and venus are all aspects for mars. Since all
of them are ambiguous based on Wikipedia, our current Aspector would treat them as
individual aspects. However, it is possible to group them together as a single planet aspect,
given all three candidates have planet as one possible type.
5.3 Selecting Aspects
The ﬁnal step in Aspector is selecting the aspects. We note that absolute ranking of
aspects is not so important in our context, because we expect that search results from
aspects will be spread out on the screen rather than being presented as a single list. However,
we still need to select the top-k aspects to present. Our selection of top-k aspects is based
on our original goals of increasing coverage and guaranteeing orthogonality.
Aspector uses the score of an aspect, p(ai |q), as a measure of coverage. To achieve
a balance between coverage and orthogonality, Aspector uses a greedy algorithm that
selects aspects in the ratio of their score p(ai |q) and the similarity of the aspects to already
selected aspects. The algorithm below produces a ranked list of aspects, G.

4. Other ontologies like Freebase and Yago can also be used.
5. This trick is used when constructing the bipartite graph in Section 4.2 as well.

685

Wu, Madhavan, & Halevy

Input: Set S = {ai }
Output: Set G

// Label aspects of clusters after de-duplication.
// Ranked list of aspects.

Initialization: G = ϕ;
a0 = argmaxai ∈S p(ai |q);
move a0 from S to G;
while (S ̸= ϕ) do
for ai ∈ S do
set sim(ai , G) = maxaj ∈G sim(ai , aj );
p(ai |q)
;
anext = argmaxai ∈S Sim(a
i ,G)
move anext from S to G;
Algorithm 1: Aspector selects top-k aspects by balancing coverage and orthogonality.
Observe we set the similarity sim(ai , G) to be the maximum similarity of ai to the
aspects already in G. On termination, Aspector returns the top n aspects in ranked order
(in our experiments we used n = 8). Our experiments indicate that balancing coverage and
orthoganality leads to better selection of aspects than simply using coverage.

6. Experiments
In this section we evaluate our system Aspector and in particular, answer the following
questions.
Quality of aspects: We compare the results of Aspector against three potential
competing systems – related searches proposed by Google (henceforth Grs), cluster labels
assigned by the Clusty search engine (Ccl), and navigational searches proposed by Bing
(Bns). To better support exploration of diﬀerent parts of the space of relevant information,
the aspects of a query have to be orthogonal to each other. Aspects should also increase the
coverage, i.e., reveal information that is not already available through the original query,
but is still very relevant to it. Using a combination of search result analysis and a user
study, we show that our aspects are less similar to each other (and hence more orthogonal)
(Section 6.3), that aspects are able to increase coverage (Section 6.4), and that aspects are
overall rated more favorably than Grs, Ccl, and Bns (Section 6.5).
Contributions of the diﬀerent components: Aspector generates instance-level
aspects and performs class-based aspect propagation, eliminates duplicates, and groups the
remaining ones using a knowledge base. We show that instance-level and class-level aspects
tend to be very diﬀerent, and that the best results are obtained by judiciously combining
them (Section 6.6). We also show that our clustering algorithm is able to stably eliminate
duplicate aspects crossing diﬀerent domains, and the grouping of aspects has a positive
impact on the quality of aspects (Section 6.7).
6.1 Experimental Setting
To compute candidate aspects from query logs, we used three months worth of anonymized
search logs from Google.com. We used a snapshot of the English version (2008.07.24) of the
Wikipedia Infobox to serve as our knowledge base. Unless otherwise mentioned, we used
686

Identifying Aspects for Web-Search Queries

K = 0.1 for class-based aspect propagation (Equation 1). We now describe our test suite
and our user study.
Test Queries: We focus on queries that are entity names or have an entity name and
a property name. We construct a test suite that contains 6 sets of queries: ﬁve with entity
names from the Wikipedia classes Country, NBA player, Company, Mountain, and University,
and one with entity-property queries of the form Country travel. To construct a mix of
popular and rare queries, in each of the six sets we select 5 queries that occur frequently in
the query stream, 5 that are relatively uncommon, and 5 are chosen randomly for the class
(as long as they appear in the query logs). Thus, in total we have 90 test queries. For each
experiment we used a random subset of these test queries.
User Study: As part of our experimental analysis, we performed user studies using the
Amazon Mechanical Turk (Amt) system. On Amt, requesters (like us) post tasks and pay
for anonymous registered workers to respond to them. Tasks are structured as a sequence
of questions that workers are expected to respond as per the instructions provided by the
requester. For example, to compare two algorithms that compute aspects, we can design
a sequence of tasks such that in each a query and two lists of aspects (computed by each
algorithm) are shown. The worker has to rate whether one list is better than the other
or they are very similar. Amt ensures that each worker can only respond to a task once.
Since, the workers in the user study are completely unknown to the requester, there is less
of chance of bias. Amt has been shown to be an eﬀective and eﬃcient way to collect data
for various research purposes (Snow, O’Connor, Jurafsky, & Ng, 2008; Su, Pavlov, Chow, &
Baker, 2007). In our experiments, we used the default qualiﬁcation requirement for workers
that requires each worker to have a HIT approval rate (%) greater than or equal to 95.
6.2 Points of Comparison
Grs can be considered as a representative of current approaches that are based on mining
reﬁnements and super-strings from query logs. It is likely that Grs only performs an
instance-level analysis and it does not attempt to identify distinct user information needs.
Ccl clusters result pages and assigns human-understandable labels to each cluster. Most
notably, the clusters are determined purely from results of the original query, and there is
no attempt to enable exploration of results that were not retrieved by the query. Further,
it is likely that cluster labels are extracted by an analysis of the contents of the result pages
(web documents). We note that while the clustering is hierarchical, for our experiments we
only considered the top-level labels.
Bns provides navigation searches (right above the “Related searches” in the result pages)
to help users better explore the information space. Bing’s goal is closest to ours in spirit,
but their technique only applies in a narrow set of domains. We note that Bns sometimes
provides generic aspects (e.g., videos, images), but we do not consider those.
We note that neither Grs nor Ccl were designed with the explicit goal of computing
aspects that help explore the information space relevant to a query. However, they can be
viewed as close alternatives in terms of the results they may produce, and therefore oﬀer
two points of comparison.
Table 3 shows the aspects, related searches, cluster labels, and navigational searches
obtained by the four systems on some example queries. In the rest of this section, we will
687

Wu, Madhavan, & Halevy

Query

Mount Shasta

Yale University

Grs
volcano
national park
climbing
vortex
camping
hotels
attractions
lodging
harvard university
athletics
press
brown university
stanford university
columbia university
cornell university
duke university

Ccl
photos
hotels
real estate
weed
wilderness, california
climbing
weather, forecast
ski
school
department
library
images
publications
admissions
laboratory
alumni

Bns
image
weather
real estate
hotels
lodging
rentals
reference/wikipedia
admissions
jobs
bookstore
alumni
library
reference/wikipedia
images

Aspector
resort
weather
high school
real estate
hiking
pictures (photos)
map
ski area
press
art gallery
athletics
harvard (oxford, stanford,...)
jobs
bookstore
admissions
tuition

Table 3: Sample output from Grs, Ccl, Bns, and Aspector.
ﬁrst show that aspects from Aspector are on average more orthogonal, increase coverage,
and are rated better overall than Grs, Ccl and Bns.
6.3 Orthogonality of Aspects
To establish the orthogonality of aspects, we measure the inter-aspect similarity – the less
similar the aspects are, the more orthogonal they are. We ﬁrst describe how we compute
inter-aspect similarity, and then report its values over the query set for Aspector, Grs,
Ccl, and Bns.
In Section 5, we used TF/IDF-based word vectors to estimate aspect similarity. Using
the same measure to establish orthogonality will bias the evaluation in favor of Aspector.
Hence, we use an alternate measure for aspect similarity that employs a topic model (Blei
et al., 2003). Brieﬂy, topic models are built by learning a probability distribution between
words in documents and the topics that might underlie a document. Given a text fragment,
a topic model can be used to predict the probability distribution of the topics relevant to
the fragment. For example, the text on the company page for Google Inc., might result
in the topic distribution ⟨⟨search engine, 0.15⟩, ⟨online business, 0.08⟩, . . .⟩. We use a topic
model developed internally at Google (henceforth TMG). Given two text fragments t1 and
t2 , we can compute their topic similarity tsim(t1 , t2 ) as the cosine distance between their
topic distribution vectors T⃗1 and T⃗2 .
Since aspects contain only a few words, we extend augmenting each aspect with its
corresponding top search results (as in Section 5). Given aspects a1 and a2 , let D1 and
D2 be their respective top m web search results. We compare D1 and D2 using TMG to
estimate aspect similarity. Speciﬁcally, we compute the average inter-document similarity.
sim(a1 , a2 ) =

1
k2

∑
di ∈D1 ,dj ∈D2

688

tsim(di , dj )

(2)

Identifying Aspects for Web-Search Queries

Normalized Inter-aspect Similarity

Aspect Similarity Comparison
0.06

0.04

0.02

0

Aspector

BNS

CCL

GRS

Figure 2: The results of Aspector are more orthogonal than those of Grs, Ccl, and Bns.

Given A, a set of n aspects, we determine its inter-aspect similarity (asim) as the average
pair-wise aspect similarity.
asim(A) =

∑
2
sim(ai , aj )
n(n − 1) a ,a ∈A
i

j

In order to make sense of the magnitude of asim, we normalize it using the average intraaspect similarity isim(A) obtained by comparing each aspect against itself.
isim(A) =

1 ∑
sim(ai , ai )
|A| a ∈A
i

Note, sim(ai , ai ) is ususally not equal to 1 based on equation 2. The result is the normalized
inter-aspect similarity nsim.
nsim(A) =

asim(A)
isim(A)

Thus, if all the aspects in A are identical, nsim(A) = 1, and if they are entirely orthogonal
nsim(A) = 0.
For each query, we retrieved the same number of aspects (at most 8) from each system,
and Figure 2 shows the average normalized inter-aspect similarity for the results output by
each system.
As can be clearly seen, Aspector has the least normalized inter-aspect similarity and
hence the most orthogonal aspects. The improvement over Bns (45%) is most likely due to
the grouping of related aspects by vertical category. The improvement over Grs (90%) is
most likely due to inclusion of class-based aspects and the grouping of related aspects by
vertical category. The improvement over Ccl (60%) is likely because their space of labels
is restricted to only the results returned by the original query.
689

Wu, Madhavan, & Halevy

Urls from Aspector Covered by Google.com
Top1_All

Top1_Popular

Top8_All

Top8_Popular

0.5

d
e
r
e
vo
c 0.4
Ls
R
U
r 0.3
o
ct
e
p
s
A
f 0.2
o
n
o
tic
0.1
ra
F

0

0

100

200

300

400

500

# of top urls from Google.com

Figure 3: Fraction of top web pages retrieved by aspects that are also in the top 500 pages
retrieved by the original search query.

6.4 Increase in Coverage
To validate the increase in coverage we are interested in answering two questions: (1) do
the aspects enable users to reach more information than the original query? (2) is the
additional information relevant to the user query?
6.4.1 More Information
To show that aspects can reach more information, we compare the web pages retrieved
using the aspects computed by Aspector against those retrieved by the original search
query. Given, a query q and its computed aspects A, let DN be the set of top N web pages
retrieved by Google for the query q. Let Dki be the collection of top k web pages retrieved
by Google for an aspect ai in A, and let Dka be the union of all the Dki s. We measure the
fractional overlap between DN and Dka , i.e.,

|Dka ∩DN |
|Dka | .

Figure 3 shows the average fractional overlap between Dka and DN for k = 1 and k = 8
against diﬀerent values of N (x-axis). The results are averaged over two sets of queries:
(1) all 90 queries, and (2) a subset of 30 popular queries, with 10 aspects being computed
for each query. As the results clearly indicate, even when considering the top 500 search
engine results, for k = 1, only about 45% of the web pages in D1a are retrieved. In other
words, 55% web pages retrieved using the aspects are not even in the top 500 (note that
|D1a | is only 10). The overlap is an even lower 33% when considering k = 8. This shows
that aspects are clearly able to retrieve more new information.
In order to isolate the potential eﬀects due to rare queries for which search engines do
not typically propose related searches, we separately consider the subset of popular queries.
Interestingly, here we ﬁnd that the overlaps are even smaller and hence aspects are able
to retrieve even more information. This is likely because there is potentially more diverse
information on the Web about popular entities.
690

Identifying Aspects for Web-Search Queries

Domain
country
country travel
nba player
company
university
mountain
Total

Cumulative Resp.
Not Cov. Covered
Y
N
Y
N
274 26
274 26
238 8
258 8
269 53
223 43
280 48
228 40
309 31
235 25
242 38
282 38
1612 204
1500 180

Not
Y
28
25
33
31
34
26
177

Url Ratings
Cov. Covered
N
Y
N
2
28 2
0
27 0
0
27 0
2
26 1
0
26 0
2
30 2
6
164 5

Table 4: User responses indicating whether the pages retrieved by aspects are relevant to
the query.

6.4.2 Relevant Information
To establish that the more information retrieved by aspects are in fact relevant to the user’s
information needs, we conducted an Amt user study. For each query q, we considered the
top 10 aspects and for each aspect we consider the top retrieved web page, i.e., D1a . We
constructed a list of aspect-based results LA which contained 4 results (selected at random
from D1a ), such that 2 overlapped with D500 and 2 that did not overlap with D500 . Users
were asked to evaluate if each of the results were (a) relevant, or (b) irrelevant to the original
query q. In order to place the results in context, we also showed LA alongside LG , the top
5 regular search engine results (these were not to be rated, but only for context).
We considered all 90 test queries with responses from 10 users in each case. The detailed
results are shown in Table 4. The columns Y and N indicate whether the web pages were
deemed relevant or not. The Covered and Not Covered columns separately consider the
web pages in LA that were covered in D500 and those that were not. The Cumulative
Responses columns aggregate the responses for all users, while the Url Ratings columns
aggregate the ratings of all users separately for each web page in LA . As can be seen,
in total there were 177 web pages that were not covered, but were deemed relevant by a
majority of users.
The results indicate that overall, the vast majority of additional web pages retrieved
by aspects were deemed relevant by a majority of users. In addition, the ratio of relevant
to not-relevant results is about the same for the covered and not-covered web pages. This
indicates that not only is the additional information relevant, but it is likely to be as relevant
as the covered information.
Note that our coverage results also establish that our aspects are likely to span much
more of an information space than alternate schemes that rely only on analyzing the results
of the original query, e.g., the cluster labels in Clusty.
6.5 Comprehensive Performance Comparison
We compare the overall performance of Aspector to Grs, Ccl, and Bns by conducting
a user study using Amt. We separately compared Aspector against each of the other
systems. In each case, we selected a random subset of around 30 queries from our original
691

Wu, Madhavan, & Halevy

set of 90 queries. We ﬁltered the queries which don’t return aspects from both systems.
For each query, two lists of (at most) 8 aspects were generated, one using Aspector and
the other using Grs, Ccl or Bns, which were then presented to an Amt rater with the
following instructions:
Each query represents the start of a session to explore more information on some topic
(presumably related to the query). For each user query, we display two lists of other related
queries and/or properties. We want you to compare the two lists for each query and identify
which of the two lists enables a better subsequent exploration of information.
The lists for each query were presented side-by-side, and the user could rate one to be better
than the other, or simply rate them to be about the same. The raters were not informed of
the source of each list and the side-by-side positioning of the lists was randomly selected.
We collected responses from 15 raters for each query.
Tables 5, 6 and 7 summarize the results of the user study. The Cumulative Responses
columns aggregate the responses for all raters for all queries. The F, E, and A columns
indicate ratings in favor of Aspector, even ratings, and against Aspector (and in favor
of Grs or Ccl) respectively. The Query Ratings columns aggregate the ratings of all the
raters for each query, with an F indicating that more raters rated Aspector in favor of
the other systems (respectively E and A).
As can be seen in Table 5, Aspector clearly outperforms Grs. The improvements
are most likely due to its increased orthogonality and the grouping of aspects by vertical
category. As is also clear in Table 6, Aspector also signiﬁcantly outperforms Ccl, most
likely due to its increased coverage.
To ascertain the statistical signiﬁcance of the evaluation, for each comparison, we performed the standard paired t-test. For each individual query, we considered the total number
of F responses against the A responses. For the comparison against Grs, the mean perquery diﬀerence (F-A) was 10.7, i.e., on average 10.7 out of the total 15 evaluators rated
Aspector to be better. The diﬀerence was statistically signiﬁcant with a two-tailed pvalue less than 0.0001. For the comparison with Ccl, the mean diﬀerence was 13.1 and
again signiﬁcant with a p-value less than 0.0001.
Bns produces aspects for a small number of domains. To set the context for our comparison we measured the breadth of Bns. We chose the top 100 Wikipedia Infobox classes,
and selected 15 entities (5 popular, 5 less common, and 5 randomly) from each class as in
section 5.3. Bns provided aspects for 17.6% of the entities. In particular, Bns provided
aspects for 29.4% of the popular entities and 9.8% for those less common entities. Bns
provided no aspects for entities of 48 of the classes, including “scientist”, “magazine” and
“airport”. The second limitation of Bns is that it only provides aspects for entity queries.
Hence, Bns does not provide aspects for queries such as “vietnam travel”, “seattle coﬀee”
or “boston rentals”. The third limitation of Bns is that it provides only class-level aspects, though the aspects may diﬀer slightly from one instance to another. For example,
Bns misses the aspect “starbucks” for “seattle”, “turkey slap” for “turkey”, and “number
change” for “kobe bryant”.
Of our 90 queries, only 41 of them obtain aspects from Bns. Table 7 shows that
Aspector and Bns are rated comparably w.r.t. this limited set of queries. The advantages
of Aspector come from the fact that it judiciously balances instance-level and class-level
aspects. It is interesting to point out that when raters are not familiar with a particular
692

Identifying Aspects for Web-Search Queries

Domain
country
country travel
nba player
company
university
mountain
Total

Cumulative
F
E
60
7
54
17
68
5
51
14
59
11
58
15
350 69

Resp.
A
8
3
2
10
5
1
29

Query Ratings
F
E
A
4
0
1
5
0
0
5
0
0
4
0
1
5
0
0
5
0
0
28 0
2

Table 5: User responses comparing Aspector against Grs.
Domain
country
country travel
nba player
company
university
mountain
Total

Cumulative
F
E
57
8
62
5
69
3
56
1
62
3
53
7
359 27

Resp.
A
9
0
2
12
8
7
38

Query Ratings
F
E
A
4
1
0
5
0
0
5
0
0
5
0
0
5
0
0
5
0
0
29 1
0

Table 6: User responses comparing Aspector against Ccl.
instance, they tend to prefer the class-level aspects. In our experiment, this observation
sometimes gives Bns an advantage.
6.6 Instance-Level Versus Class-Level Aspects
Recall that Aspector balances between class-level and instance-level aspects for a given
query. Consider, for example, the class of NBA players. There are 1365 players identiﬁed
on Wikipedia. We were able to identify 8 or more candidate instance-level aspects for only
126 of them (9.2%). For 953 (69.8%) players, we were unable to infer any instance-level
aspects. However there are 54 class-level aspects that appear with at least 5 instances,
Domain
country
nba player
company
university
mountain
Total

Cumulative
F
E
65
29
52
12
20
2
92
9
6
6
235 58

Resp.
A
56
71
53
19
18
217

Query Ratings
F
E
A
5
1
4
3
0
6
0
0
5
8
0
0
1
0
1
17 1
16

Table 7: User responses comparing Aspector against Bns. Note this is the result after
ﬁltering 54% testing queries when Bns provides no aspects, in which case users
always rate Aspector better.

693

Wu, Madhavan, & Halevy

Domain
country
country travel
nba player
company
university
mountain
Total

Cumulative
F
E
12
17
12
11
10
14
12
13
17
9
10
14
73
78

Resp.
A
46
52
51
50
49
51
299

Query Ratings
F
E
A
0
0
5
0
0
5
0
0
5
0
0
5
1
0
4
0
0
5
1
0
29

Table 8: User responses when comparing Aspector with K = 0 and K = 1.

thus giving us a potentially larger pool of good candidate aspects. By balancing these two
sources of aspects, Aspector is able to successfully compute reasonable aspects even for
less frequent queries.
We compared the extent to which class-based aspect propagation contributes to the
quality of aspects generated. For this, we again performed an Amt user study. We considered diﬀerent values for the parameter K in Formula 1: 0, 0.1, 1, 10, and 100, each
indicating progressively higher contribution of class-level aspects. Aspect lists were generated for a subset of 30 queries (5 from each set) for each value of K. We compared two
aspect lists at a time, and performed three sets of experiments comparing (1) K = 0 with
K = 1 (Table 8), (2) K = 0.1 with K = 10 (Table 9), and K = 1 with K = 100 (Table 10).
Each experiment used the same set of 30 queries and users were asked to pick which of the
two sets of aspects they preferred for each query (same task description as in Section 5).
Responses were collected from 15 users in each case. Note that to ensure signiﬁcant diﬀerences in the aspect lists being compared, our experiments did not consider consecutive K
values (e.g., 0 and 0.1). In an earlier experiment where consecutive K values were used, we
found many queries have only a subtle diﬀerence and hence large numbers of users rated the
lists to be comparable. The number of queries are fewer in Table 9 and Table 10, since the
remaining ones resulted in the same aspect lists for both K values – this is not surprising
since, larger K values result in the increased inﬂuence of the same set of class-based aspects.
We ﬁnd that the aspect lists for K = 1 are rated signiﬁcantly better than for K = 0
(the mean per-query diﬀerence (F-A) was 7.53 and signiﬁcant with a two-sided p-value less
than 1E-9). The lists for K = 10 were preferred to those with K = 0.1 (though by a
smaller (F-A) of 2.4 with p value about 0.008), while the lists for K = 100 and K = 1 were
rated about the same (the mean (F-A) was 0.3 and insigniﬁcant with p value about 0.8).
Each of these seem to indicate clearly that class-based aspects are helpful in improving user
experience.
However, we note that our results might marginally over-state the importance of classbased aspects. This is because a user’s perception of aspects is dependent upon the user’s
interest and familiarity with the entity in question – if an entity, though popular, is not
too familiar to a participant in the study, they are likely to select the class-based aspects.
On the other hand, we found that for universally well known entities, such as the company
microsoft, the lists with more instance-based aspects were always preferred.
694

Identifying Aspects for Web-Search Queries

Domain
country
country travel
nba player
company
university
mountain
Total

Cumulative
F
E
11
15
21
23
19
21
17
24
22
26
13
11
103
120

Resp.
A
34
16
35
34
27
21
167

Query Ratings
F
E
A
0
0
4
2
1
1
1
0
4
1
0
4
2
0
3
1
0
2
7
1
18

Table 9: User responses when comparing Aspector with K = 0.1 and K = 10.
Domain
country
country travel
nba player
company
university
mountain
Total

Cumulative
F
E
3
8
16
18
5
12
33
29
14
20
21
23
92
110

Resp.
A
4
11
28
13
11
31
98

Query Ratings
F
E
A
0
0
1
2
0
1
0
0
3
4
0
1
1
1
1
2
0
3
9
1
10

Table 10: User responses when comparing Aspector with K = 1 and K = 100.
6.7 Eliminating and Grouping Aspects
We now consider the impact of the content-based clustering that is used to identify duplicate
aspects, and the vertical-category-based clustering that groups aspects belonging to the
same category.
6.7.1 Duplicate Elimination
When computing candidate aspects from query logs, it is possible to ﬁnd multiple aspects
that have diﬀerent names, but are semantically the same. Such aspects have to be eliminated in order for the summary to cover more distinct axes. As explained in Section 5.1,
Aspector applies a graph partitioning algorithm that only has a single parameter, the
similarity threshold σ. We conjecture that this similarity threshold is more intuitive to set
and is stable across diﬀerent domains.
To test our hypothesis, we randomly selected 5 queries each from 5 domains. For each
query, we took the top 30 aspects candidates and manually created a gold-standard with
the correct aspect clustering results. We computed aspect lists for the queries with diﬀerent
values for the threshold σ and compared the results against the gold-standard.
We use the F-Measure (F ) to evaluate the clustering results. In particular, we view
clustering as a series of decisions, one for each of the N (N − 1)/2 pairs of aspects.
Figure 4 plots the F values for diﬀerent values of the threshold σ for the ﬁve test
domains. We found that in each case the best performance is between threshold values 0.25
and 0.4. The results indicate that clustering performance with respect to σ is pretty stable
across domains. Hence, in all our experiments, we set a single value σ = 0.35.
695

Wu, Madhavan, & Halevy

Mountain

NBA_Player

Country

University

Company

0.9
0.8

F-Measure

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Similarity threshold

Figure 4: F-Measure of aspect clustering for diﬀerent values of the similarity threshold σ.
In each domain, the best performance is around 0.35.

Domain
company
university
mountain
Total

Cumulative
F
E
25
14
18
7
6
3
49
24

Resp.
A
6
5
6
17

Query Ratings
F
E
A
3
0
0
2
0
0
0
1
0
5
1
0

Table 11: User responses comparing Aspector with vertical-category grouping and without. F, E, and A are responses in favor, even, and against grouping.

6.7.2 Vertical-Category Based Grouping
In addition to duplicates, we observed that often multiple aspects might belong to the same
vertical category. Rather than represent each as a separate aspect, we can summarize the
aspects by presenting them as a single group. Note that grouping does not eliminate the
aspects, but simply lists all of them as a single aspect. For 6 out of the 90 queries in our
dataset, Aspector was able to group aspects by vertical category.
As before, we deployed an Amt user study for the 6 test queries and the results are
shown in Table 11, with F indicating the number of responses in favor of vertical grouping
(with E and A deﬁned accordingly). As can be seen, the aspect lists with grouping were
favored in comparison to the ones without grouping.
Currently, Aspector groups aspects of the same vertical category only if there is no
corresponding disambiguation pages in Wikipedia. This conservative solution avoids potential errors when ambiguity exists, but also misses opportunities. For example, for the
query mount bachelor, mount hood and mount baker appear as separate aspects since there
are disambiguation pages for both entities. Reﬁning the grouping condition is a rich topic
for future work.
696

Identifying Aspects for Web-Search Queries

7. Related Work
We discuss work related to ours in the area of search-result organization and of query-log
mining.
7.1 Search Result Organization
Several works have considered how to better organize search results. Agrawal, Gollapudi,
Halverson, and Ieong (2009) classify queries and documents into categories and return search
results by considering both document relevance and diversity of results. In contrast, Aspector computes more ﬁne grained aspects instead of abstract categories for exploratory
queries which are not necessary to be ambiguous. Some commercial systems like Kosmix and Yahoo!Glue categorize information based on type or format (e.g. photo, video,
news and map) and retrieve top results for each category. Though the diﬀerent types often
approximate aspects, they do not represent a rich set of semantically diﬀerent groups of
information and are not sensitive to instance-speciﬁc aspects. The Carrot2 search engine
applies text clustering techniques over returned search pages and extracts keywords to summarize each cluster. Similar works were done by Bekkerman, Zilberstein, and Allan (2007),
Blei et al. (2003), Crabtree, Andreae, and Gao (2006), and Wang, Blei, and Heckerman
(2008). Multi-faceted search (Yee, Swearingen, Li, & Hearst, 2003) organizes collections
based on a set of category hierarchies each of which corresponds to a diﬀerent facet. However the category hierarchies requires heavy human eﬀort for construction and maintenance.
The Correlator system from Yahoo! performs semantic tagging of documents to enable
mining of related entities for a query. These algorithms don’t necessary discover clusters
which correspond to Web users’ search interests, and it is diﬃcult to generate informative
cluster labels from documents. Our use of query logs complements such document-based
approaches, but reﬂects searchers intentions rather the intentions of the publishers.
Wang and Zhai (2007) proposed to organize search results based on query logs. They
represent each query as a pseudo-document enriched with clickthrough information and
pick the top-k that are similar to the current query, and cluster them into aspects. Then,
they classify each resulting page into corresponding aspect by similarity. In contrast, we
generate aspects based on the idea of query reﬁnements which don’t require the aspect and
current query to have similar clickthrough. For example, the query vietnam travel visa is an
important aspect for vietnam travel, but won’t have the same click-through properties.
7.2 Query-Log Mining
There have been several eﬀorts to mine query logs for interesting artifacts. Pasca and Durme
(2007) extract relevant attributes for classes of entities from query logs rather than from
Web documents as done by Bellare et al. (2006). The main goal of these works is to create
a knowledge base of entities, and hence their results are most appropriately compared with
Wikipedia or Freebase.
Query reﬁnement and suggestion analyze query logs to predict the next most probable
query following the current query (Cucerzan & White, 2007; Jones, Rey, Madani, & Greiner,
2006; Kraft & Zien, 2004; Velez, Wiess, Sheldon, & Giﬀord, 1997). Hence, their goal
is to help users ﬁnd a single result page rather than help navigating a body of relevant
697

Wu, Madhavan, & Halevy

information. Bonchi, Castillo, Donato, and Gionis (2008) proposed to decompose a query
into a small set of queries whose union corresponds approximately to that of the original
query. However, as our experiments illustrated, the constraint that the union of the resulting
pages correspond approximately to that of the original query signiﬁcantly limits the available
body of information we expose to the user.
Wang et al. (2009) mine a set of global latent query aspects, and dynamically select
top k aspects for a given query q to help better navigate the information space. While in
some ways this is similar to Aspector, there are two key diﬀerences. First, they discover
the set of global latent query aspect via maximizing a target function, where the aspect
set aims to apply to many classes of (important) queries. In contrast, Aspector applies
class-based label propagation to identify the aspects. Therefore, the aspects tend to be
more ﬁne-grained and more query(class)-speciﬁc. Second, when selecting the k aspects for
a query q, Wang et al. apply another optimization function which tries to cover as many
original (frequent) query reﬁnements of q. This works ﬁne for popular queries but not for
less popular queries which have few query reﬁnements. Our experiments show that for most
classes, there is a long tail of such less popular queries.

8. Conclusions
We described the Aspector system for computing aspects of web-search queries. Aspects
are intended to oﬀer axes along which the space of information relevant to a query can be
organized, and therefore enable search engines to assist the user in exploring the space.
Aspector generates candidate aspects from query logs and balances aspects that are
common to classes of entities vs. those that are speciﬁc to particular instances. Aspector
also eliminates duplicate aspects and groups related aspects using a reference ontology. In
contrast with a purely knowledge-based approach, Aspector’s results are much broader
and include aspects of interest to speciﬁc instances. In contrast with an approach based
solely on clustering the results of the query, Aspector can include aspects that are not
represented directly in the query’s answer.
We set the weights of all edges from instances to classes uniformly when computing
class-based aspects. A future direction is to compute better informed weighting functions
based on available temporal, spatial and contextual constraints. Another future work is to
allow multi-class memberships based on other ontologies besides Wikipedia Infobox.
To incorporate aspects into a mainstream search engine we need to address two challenges. First, we need to reliably identify from the query stream which queries beneﬁt from
a summarization approach. Some works (Miwa & Kando, 2007; White & Roth, 2009) have
been conducted in this area, but much more needs to be investigated. Second, as done in
Kosmix, we need to dynamically generate eﬀective visualizations for aspects.

References
Agrawal, R., Gollapudi, S., Halverson, A., & Ieong, S. (2009). Diversifying Search Results.
In WSDM.
Aslam, J. A., Pelekov, E., & Rus, D. (2004). The star clustering algorithm for static and
dynamic information organization. Journal of Graph Algorithms and Applicatins.
698

Identifying Aspects for Web-Search Queries

Baluja, S., Seth, R., Sivakumar, D., Jing, Y., Yagnik, J., Kumar, S., Ravichandran, D., &
Aly, M. (2008). Video suggestion and discovery for youtube: Taking random walks
through the view graph. In WWW.
Battelle, J. (2005). The Search: How Google and Its Rivals Rewrote the Rules of Business
and Transformed Our Culture. Portfolio Hardcover.
Bekkerman, R., Zilberstein, S., & Allan, J. (2007). Web Page Clustering using Heuristic
Search in the Web Graph. In IJCAI.
Bellare, K., Talukdar, P. P., Kumaran, G., Pereira, F., Liberman, M., McCallum, A., &
Dredze, M. (2006). Lightly-Supervised Attribute Extraction. In NIPS.
Bergsma, S., & Wang, Q. I. (2007). Learning Noun Phrase Query Segmentation. In EMNLPCoNLL.
Blei, D., Ng, A., & Jordan, M. (2003). Latent Dirichlet allocation. Journal of Machine
Learning Research.
Bonchi, F., Castillo, C., Donato, D., & Gionis, A. (2008). Topical query decomposition. In
KDD.
Broder, A. (2002). A taxonomy of web search. SIGIR Forum, 36 (2).
Crabtree, D., Andreae, P., & Gao, X. (2006). Query Directed Web Page Clustering. In WI.
Cucerzan, S., & White, R. W. (2007). Query Suggestion based on User Landing Pages. In
SIGIR.
Jones, R., Rey, B., Madani, O., & Greiner, W. (2006). Generating query substitutions. In
WWW.
Kraft, R., & Zien, J. (2004). Mining anchor text for query reﬁnement. In WWW.
Meesookho, C., Narayanan, S., & Raghavendra, C. S. (2002). Collaborative classiﬁcation
applications in sensor networks. In SAMSP-Workshop.
Miwa, M., & Kando, N. (2007). Methodology for capturing exploratory search processes.
In CHI-Workshop.
Pasca, M., & Durme, B. V. (2007). What You Seek is What You Get: Extraction of Class
Attributes from Query Logs. In IJCAI.
Rajaraman, A. (2008).
Searching for a needle or exploring the haystack?.
http://anand.typepad.com/datawocky/2008/06/.

In

Rose, D. E., & Levinson, D. (2004). Understanding User Goals in Web Search. In WWW.
Snow, R., O’Connor, B., Jurafsky, D., & Ng, A. Y. (2008). Cheap and fast - but is it good?
evaluating non-expert annotations for natural language tasks. In EMNLP.
Su, Q., Pavlov, D., Chow, J.-H., & Baker, W. C. (2007). Internet-scale collection of humanreviewed data. In WWW.
Tan, B., & Peng, F. (2008). Unsupervised query segmentation using generative language
models and wikipedia. In WWW.
Velez, B., Wiess, R., Sheldon, M., & Giﬀord, D. (1997). Fast and eﬀective query reﬁnement.
In SIGIR.
699

Wu, Madhavan, & Halevy

Wang, C., Blei, D., & Heckerman, D. (2008). Continuous time dynamic topic models. In
UAI.
Wang, X., Chakrabarti, D., & Punera, K. (2009). Mining Broad Latent Query Aspects from
Search Sessions. In KDD.
Wang, X., & Zhai, C. (2007). Learn from Web Search Logs to Organize Search Results. In
SIGIR.
White, R. W., & Roth, R. A. (2009). Exploratory Search: Beyond the Query-Response
Paradigm. Morgan and Claypool Publishers.
Yee, K.-P., Swearingen, K., Li, K., & Hearst, M. (2003). Faceted metadata for image search
and browsing. In CHI.
Zeng, H.-J., He, Q.-C., Chen, Z., Ma, W.-Y., & Ma, J. (2004). Learning to cluster web
search results. In SIGIR.

700

Journal of Artificial Intelligence Research 40 (2011) 815-840

Submitted 10/10; published 04/11

Regression Conformal Prediction with Nearest Neighbours
Harris Papadopoulos

h.papadopoulos@frederick.ac.cy

Computer Science and Engineering Department
Frederick University
7 Y. Frederickou St., Palouriotisa
Nicosia 1036, Cyprus

Vladimir Vovk
Alex Gammerman

vovk@cs.rhul.ac.uk
alex@cs.rhul.ac.uk

Computer Learning Research Centre
Department of Computer Science
Royal Holloway, University of London
Egham, Surrey TW20 0EX, UK

Abstract
In this paper we apply Conformal Prediction (CP) to the k -Nearest Neighbours Regression (k -NNR) algorithm and propose ways of extending the typical nonconformity measure
used for regression so far. Unlike traditional regression methods which produce point predictions, Conformal Predictors output predictive regions that satisfy a given confidence
level. The regions produced by any Conformal Predictor are automatically valid, however
their tightness and therefore usefulness depends on the nonconformity measure used by
each CP. In effect a nonconformity measure evaluates how strange a given example is compared to a set of other examples based on some traditional machine learning algorithm. We
define six novel nonconformity measures based on the k -Nearest Neighbours Regression algorithm and develop the corresponding CPs following both the original (transductive) and
the inductive CP approaches. A comparison of the predictive regions produced by our
measures with those of the typical regression measure suggests that a major improvement
in terms of predictive region tightness is achieved by the new measures.

1. Introduction
A drawback of traditional machine learning algorithms is that they do not associate their
predictions with confidence information, instead they only output simple predictions. However, some kind of confidence information about predictions is of paramount importance in
many risk-sensitive applications such as those used for medical diagnosis (Holst, Ohlsson,
Peterson, & Edenbrandt, 1998).
Of course some machine learning theories that produce confidence information do exist.
One can apply the theory of Probably Approximately Correct learning (PAC theory, Valiant,
1984) to an algorithm in order to obtain upper bounds on the probability of its error with
respect to some confidence level. The bounds produced by PAC theory though, will be very
weak unless the data set to which the algorithm is being applied is particularly clean, which
is rarely the case. Nouretdinov, Vovk, Vyugin, and Gammerman (2001b) demonstrated the
crudeness of PAC bounds by applying one of the best bounds, by Littlestone and Warmuth
(Cristianini & Shawe-Taylor, 2000, Thm. 4.25, 6.8), to the USPS data set.
c
2011
AI Access Foundation. All rights reserved.

Papadopoulos, Vovk, & Gammerman

Another way of obtaining confidence information is by using the Bayesian framework for
producing algorithms that complement individual predictions with probabilistic measures
of their quality. In order to apply the Bayesian framework however, one is required to have
some prior knowledge about the distribution generating the data. When the correct prior is
known, Bayesian methods provide optimal decisions. For real world data sets though, as the
required knowledge is not available, one has to assume the existence of an arbitrarily chosen
prior. In this case, since the assumed prior may be incorrect, the resulting confidence levels
may also be “incorrect”; for example the predictive regions output for the 95% confidence
level may contain the true label in much less than 95% of the cases. This signifies a major
failure as we would expect confidence levels to bound the percentage of expected errors.
An experimental demonstration of this negative aspect of Bayesian methods in the case
of regression is given in Section 8, while a detailed experimental examination for both
classification and regression was performed by Melluish, Saunders, Nouretdinov, and Vovk
(2001).
A different approach to confidence prediction was suggested by Gammerman, Vapnik,
and Vovk (1998) (and later greatly improved by Saunders, Gammerman, & Vovk, 1999),
who proposed what we call in this paper “Conformal Prediction” (CP). A thorough analysis
of CP was given by Vovk, Gammerman, and Shafer (2005), while an overview was presented
by Gammerman and Vovk (2007). Conformal Predictors are built on top of traditional machine learning algorithms and accompany each of their predictions with valid measures of
confidence. Unlike Bayesian methods, CPs do not require any further assumptions about
the distribution of the data, other than that the data are independently and identically distributed (i.i.d.); although this is still a strong assumption, it is almost universally accepted
in machine learning. Even if the traditional algorithm on which a CP is based makes some
extra assumptions that are not true for a particular data set, the validity of the predictive
regions produced by the CP will not be affected. The resulting predictive regions might be
uninteresting, but they will still be valid, as opposed to the misleading regions produced
by Bayesian methods. Furthermore, in contrast to PAC methods, the confidence measures
they produce are useful in practice. Different variants of CPs have been developed based on
Support Vector Machines (Saunders et al., 1999; Saunders, Gammerman, & Vovk, 2000),
Ridge Regression (Nouretdinov, Melluish, & Vovk, 2001a; Papadopoulos, Proedrou, Vovk,
& Gammerman, 2002a), k-Nearest Neighbours for classification (Proedrou, Nouretdinov,
Vovk, & Gammerman, 2002; Papadopoulos, Vovk, & Gammerman, 2002b) and Neural Networks (Papadopoulos, Vovk, & Gammerman, 2007), all of which have been shown to give
reliable and high quality confidence measures. Moreover, CP has been applied successfully
to many problems such as the early detection of ovarian cancer (Gammerman et al., 2009),
the classification of leukaemia subtypes (Bellotti, Luo, Gammerman, Delft, & Saha, 2005),
the diagnosis of acute abdominal pain (Papadopoulos, Gammerman, & Vovk, 2009a), the
prediction of plant promoters (Shahmuradov, Solovyev, & Gammerman, 2005), the recognition of hypoxia electroencephalograms (EEGs) (Zhang, Li, Hu, Li, & Luo, 2008), the
prediction of network traffic demand (Dashevskiy & Luo, 2008) and the estimation of effort
for software projects (Papadopoulos, Papatheocharous, & Andreou, 2009b).
The only drawback of the original CP approach is its relative computational inefficiency.
This is due to the transductive nature of the approach, which entails that all computations
have to start from scratch for every test example. This renders it unsuitable for application
816

Regression Conformal Prediction with Nearest Neighbours

to large data sets. For this reason a modification of the original CP approach, called
Inductive Conformal Prediction (ICP), was proposed by Papadopoulos et al. (2002a) for
regression and by Papadopoulos et al. (2002b) for classification. As suggested by its name,
ICP replaces the transductive inference followed in the original approach with inductive
inference. Consequently, ICPs are almost as computationally efficient as their underlying
algorithms. This is achieved at the cost of some loss in the quality of the produced confidence
measures, but this loss is negligible, especially when the data set in question is large, whereas
the improvement in computational efficiency is significant. A computational complexity
comparison between the original CP and ICP approaches was performed by Papadopoulos
(2008). From now on, in order to differentiate clearly between the original CP and ICP
approaches the former will be called Transductive Conformal Prediction (TCP).
In order to apply CP (either TCP or ICP) to a traditional algorithm one has to develop
a nonconformity measure based on that algorithm. This measure evaluates the difference
of a new example from a set (actually a multiset or bag) of old examples. Nonconformity
measures are constructed using as basis the traditional algorithm to which CP is being
applied, called the underlying algorithm of the resulting Conformal Predictor. In effect
nonconformity measures assess the degree to which the new example disagrees with the
attribute-label relationship of the old examples, according to the underlying algorithm of
the CP. It is worth to note that many different nonconformity measures can be constructed
for each traditional algorithm and each of those measures defines a different CP. This
difference, as we will show in the next section, does not affect the validity of the results
produced by the CPs, it only affects their efficiency.
In this paper we are only interested in the problem of regression and we focus on k Nearest Neighbours Regression (k-NNR) as underlying algorithm, which is one of the most
popular machine learning techniques. The first regression CPs were proposed by Nouretdinov et al. (2001a) following the TCP approach and by Papadopoulos et al. (2002a) following
the ICP approach, both based on the Ridge Regression algorithm. As opposed to the conventional point predictions, the output of regression CPs is a predictive region that satisfies
a given confidence level.
The typical nonconformity measure used so far in the case of regression is the absolute
difference |yi − ŷi |, between the actual label yi of the example i and the predicted label ŷi of
the underlying algorithm for that example, given the old examples as training set. Here we
propose six extensions to this nonconformity measure for k -Nearest Neighbours Regression
and develop the corresponding Inductive and Transductive CPs; unfortunately although all
six new measures can be used with the ICP approach, only two of them can be used with
TCP. Our definitions normalize the standard measure based on the expected accuracy of the
underlying algorithm for each example, which makes the width of the resulting predictive
regions vary accordingly. As a result, the predictive regions produced by our measures are
in general much tighter than those produced by the standard regression measure. This
paper extends our previous work (Papadopoulos, Gammerman, & Vovk, 2008) where the
k -Nearest Neighbours Regression TCP was developed using two normalized nonconformity
measures. It is also worth mentioning that one other such nonconformity measure definition
was presented by Papadopoulos et al. (2002a) for the Ridge Regression ICP.
The rest of this paper is structured as follows. In the next section we discuss the general
idea on which CPs are based. Then in Sections 3 and 4 we describe the k -Nearest Neigh817

Papadopoulos, Vovk, & Gammerman

bours Regression TCP and ICP respectively using the typical regression nonconformity
measure. In Section 5 we give our new nonconformity measure definitions and explain the
rationale behind them. Section 6 analyses further one of our new nonconformity measures
and demonstrates that under specific assumptions it gives asymptotically optimal predictive
regions. Section 7 details our experimental results with the 3 TCPs and 7 ICPs developed
based on the different measures, while Section 8 compares our methods with Gaussian Process Regression (Rasmussen & Williams, 2006), which is one of the most popular Bayesian
approaches. Finally, Section 9 gives our conclusions and discusses some possible future
directions of this work.

2. Conformal Prediction
In this section we briefly describe the idea behind Conformal Prediction; for a more detailed
description the interested reader is referred to the book by Vovk et al. (2005). We are given
a training set {z1 , . . . , zl } of examples, where each zi ∈ Z is a pair (xi , yi ); xi ∈ Rd is the
vector of attributes for example i and yi ∈ R is the label of that example. We are also given
a new unlabeled example xl+1 and our task is to state something about our confidence in
different values ỹ for the label yl+1 of this example. As mentioned in Section 1 our only
assumption is that all (xi , yi ), i = 1, 2, . . . , are generated independently from the same
probability distribution.
First let us define the concept of a nonconformity measure. Formally, a nonconformity
measure is a family of functions An : Z (n−1) × Z → R, n = 1, 2, . . . (where Z (n−1) is the set
of all multisets of size n − 1), which assign a numerical score
αi = An ({z1 , . . . , zi−1 , zi+1 , . . . , zn }, zi )

(1)

to each example zi , indicating how different it is from the examples in the multiset
{z1 , . . . , zi−1 , zi+1 , . . . , zn }.
As mentioned in Section 1 each nonconformity measure is based on some traditional
machine learning method, which is called the underlying algorithm of the corresponding
CP. Given a training set of examples {z1 , . . . , zl+1 }, each such method creates a prediction
rule
D{z1 ,...,zl+1 } ,
which maps any unlabeled example x to a label ŷ. As this prediction rule is based on the
examples in the training set, the nonconformity score of an example zi ∈ {z1 , . . . , zl+1 } is
measured as the disagreement between the predicted label
ŷi = D{z1 ,...,zl+1 } (xi )

(2)

and the actual label yi of zi . Alternatively, we can create the prediction rule
D{z1 ,...,zi−1 ,zi+1 ,...,zl+1 }
using all the examples in the set except zi , and measure the disagreement between
ŷi = D{z1 ,...,zi−1 ,zi+1 ,...,zl+1 } (xi )
and yi .
818

(3)

Regression Conformal Prediction with Nearest Neighbours

Now suppose we are interested in some particular guess ỹ for the label of xl+1 . Adding
this new example (xl+1 , ỹ) to our known data set {(x1 , y1 ), . . . , (xl , yl )} gives the extended
set
{z1 , . . . , zl+1 } = {(x1 , y1 ), . . . , (xl+1 , ỹ)};
(4)
notice that the only unknown component of this set is the label ỹ. We can now use a
nonconformity measure Al+1 to compute the nonconformity score
αi = Al+1 ({z1 , . . . , zi−1 , zi+1 , . . . , zl+1 }, zi )
of each example zi , i = 1, . . . , l + 1 in (4). The nonconformity score αl+1 on its own does
not really give us any information, it is just a numeric value. However, we can find out how
unusual zl+1 is according to Al+1 by comparing αl+1 with all other nonconformity scores.
This comparison can be performed with the function
#{i = 1, . . . , l + 1 : αi ≥ αl+1 }
(5)
l+1
(we leave the dependence of the left-hand side on z1 , . . . , zl , xl+1 implicit, but it should be
1
and 1,
always kept in mind). We call the output of this function, which lies between l+1
the p-value of ỹ, as that is the only part of (4) we were not given. An important property
of (5) is that ∀δ ∈ [0, 1] and for all probability distributions P on Z,
p(ỹ) =

P {{z1 , . . . , zl+1 } : p(yl+1 ) ≤ δ} ≤ δ;

(6)

a proof was given by Nouretdinov et al. (2001b). As a result, if the p-value of a given label
is below some very low threshold, say 0.05, this would mean that this label is highly unlikely
as such sets will only be generated at most 5% of the time by any i.i.d. process.
Assuming we could calculate the p-value of every possible label ỹ, as described above,
we would be able to exclude all labels that have a p-value under some very low threshold
(or significance level ) δ and have at most δ chance of being wrong. Consequently, given a
confidence level 1 − δ a regression conformal predictor outputs the set
{ỹ : p(ỹ) > δ},

(7)

i.e. the set of all labels that have a p-value greater than δ. Of course it would be impossible
to explicitly calculate the p-value of every possible label ỹ ∈ R. In the next section we
describe how one can compute the predictive region (7) efficiently for k -Nearest Neighbours
Regression.
It should be noted that the p-values computed by any CP will always be valid in the sense
of satisfying (6) regardless of the particular algorithm or nonconformity measure definition
it uses. The choice of algorithm, nonconformity measure definition and any parameters
only affects the tightness of the predictive regions output by the CP, and consequently their
usefulness. To demonstrate the influence of an inadequate nonconformity measure definition
on the results of a CP, let us consider the case of a trivial definition that always returns
the same value αi for any given example (xi , yi ). This will make all p-values equal to 1 and
will result in the predictive region R regardless of the required confidence level. Although
this region is useless since it does not provide us with any information, it is still valid as it
will always contain the true label of the example. Therefore even in the worst case of using
some totally wrong nonconformity measure definition or algorithm, the regions produced
by the corresponding CP will be useless, but they will never be misleading.
819

Papadopoulos, Vovk, & Gammerman

3. k -Nearest Neighbours Regression TCP
The k -Nearest Neighbours algorithms base their predictions on the k training examples
that are nearest to the unlabeled example in question according to some distance measure,
such as the Euclidean distance. More specifically, for an input vector xl+1 the k -Nearest
Neighbours Regression (k -NNR) algorithm finds the k nearest training examples to xl+1 and
outputs the average (in some cases the median is also used) of their labels as its prediction.
A refined form of the method assigns a weight to each one of the k examples depending
on their distance from xl+1 , these weights determine the contribution of each label to the
calculation of its prediction; in other words it predicts the weighted average of their labels.
It is also worth to mention that the performance of the Nearest Neighbours method can be
enhanced by the use of a suitable distance measure or kernel for a specific data set.
Here we will consider the version of the k -NNR method which predicts the weighted
average of the k nearest examples. In our experiments we used the Euclidean distance,
which is the most commonly used distance measure. It will be easy to see that the use
of a kernel function or of a different distance measure will not require any changes to our
method.
As mentioned in Section 1 in order to create any CP we need to define a nonconformity measure based on the underlying algorithm in question. First let us consider the
nonconformity measure
αi = |yi − ŷi |,
(8)
where ŷi is the prediction of k -NNR for xi based on the examples
{(x1 , y1 ), . . . , (xi−1 , yi−1 ), (xi+1 , yi+1 ), . . . , (xl+1 , ỹ)};
recall from Section 2 that ỹ is the assumed label for the new example xl+1 .
Following Nouretdinov et al. (2001a) and Vovk et al. (2005) we express the nonconformity score αi of each example i = 1, . . . , l + 1 as a piecewise-linear function of ỹ
αi = αi (ỹ) = |ai + bi ỹ|.
To do this we define ai and bi as follows:
• al+1 is minus the weighted average of the labels of the k nearest neighbours of xl+1
and bl+1 = 1;
• if i ≤ l and xl+1 is one of the k nearest neighbours of xi , ai is yi minus the labels of
the k − 1 nearest neighbours of xi from {x1 , . . . , xi−1 , xi+1 , . . . , xl } multiplied by their
corresponding weights, and bi is minus the weight of xl+1 ;
• if i ≤ l and xl+1 is not one of the k nearest neighbours of xi , ai is yi minus the
weighted average of the labels of the k nearest neighbours of xi , and bi = 0.
As a result the p-value p(ỹ) (defined by (5)) corresponding to ỹ can only change at the
points where αi (ỹ) − αl+1 (ỹ) changes sign for some i = 1, . . . , l. This means that instead of
having to calculate the p-value of every possible ỹ, we can calculate the set of points ỹ on
the real line that have a p-value p(ỹ) greater than the given significance level δ, leading to
a feasible prediction algorithm.
820

Regression Conformal Prediction with Nearest Neighbours

Algorithm 1: k-NNR TCP
Input: training set {(x1 , y1 ), . . . , (xl , yl )}, new example xl+1 , number of nearest
neighbours k and significance level δ.
P := {};
for i = 1 to l + 1 do
Calculate ai and bi for example zi = (xi , yi );
if bi < 0 then ai := −ai and bi := −bi ;
if bi 6= bl+1 then add (10) to P ;
if bi = bl+1 6= 0 and ai 6= al+1 then add (11) to P ;
end
Sort P in ascending order obtaining y(1) , . . . , y(u) ;
Add y(0) := −∞ and y(u+1) := ∞ to P ;
N (j) := 0, j = 0, . . . , u;
M (j) := 0, j = 1, . . . , u;
for i = 1 to l + 1 do
if Si = {} (see (9)) then Do nothing;
else if Si contains only one point, Si = {y(j) } then
M (j) := M (j) + 1;
else if Si is an interval [y(j1 ) , y(j2 ) ], j1 < j2 then
M (z) := M (z) + 1, z = j1 , . . . , j2 ;
N (z) := N (z) + 1, z = j1 , . . . , j2 − 1;
else if Si is a ray (−∞, y(j) ] then
M (z) := M (z) + 1, z = 1, . . . , j;
N (z) := N (z) + 1, z = 0, . . . , j − 1;
else if Si is a ray [y(j) , ∞) then
M (z) := M (z) + 1, z = j, . . . , u;
N (z) := N (z) + 1, z = j, . . . , u;
else if Si is the union (−∞, y(j1 ) ] ∪ [y(j2 ) , ∞) of two rays, j1 < j2 then
M (z) := M (z) + 1, z = 1, . . . , j1 , j2 , . . . , u;
N (z) := N (z) + 1, z = 0, . . . , j1 − 1, j2 , . . . , u;
else if Si is the real line (−∞, ∞) then
M (z) := M (z) + 1, z = 1, . . . , u;
N (z) := N (z) + 1, z = 0, . . . , u;
end
Output: the predictive region

(j)
∪j: N (j) >δ (y(j) , y(j+1) ) ∪ {y(j) : Ml+1
> δ}.
l+1

821

Papadopoulos, Vovk, & Gammerman

For each i = 1, . . . , l + 1, let
Si = {ỹ : αi (ỹ) ≥ αl+1 (ỹ)}
= {ỹ : |ai + bi ỹ| ≥ |al+1 + bl+1 ỹ|}.

(9)

Each set Si (always closed) will either be an interval, a ray, the union of two rays, the real
line, or empty; it can also be a point, which is a special case of an interval. As we are
interested in |ai + bi ỹ| we can assume that bi ≥ 0 for i = 1, . . . , l + 1 (if not we multiply
both ai and bi by −1). If bi 6= bl+1 , then αi (ỹ) and αl+1 (ỹ) are equal at two points (which
may coincide):
ai − al+1
ai + al+1
−
and
−
;
(10)
bi − bl+1
bi + bl+1
in this case Si is an interval (maybe a point) or the union of two rays. If bi = bl+1 6= 0,
then αi (ỹ) = αl+1 (ỹ) at just one point:
−

ai + al+1
,
2bi

(11)

and Si is a ray, unless ai = al+1 in which case Si is the real line. If bi = bl+1 = 0, then Si
is either empty or the real line.
To calculate the p-value p(ỹ) for any potential label ỹ of the new example xl+1 , we count
how many Si include ỹ and divide by l + 1,
p(ỹ) =

#{i = 1, . . . , l + 1 : ỹ ∈ Si }
.
l+1

(12)

As ỹ increases p(ỹ) can only change at the points (10) and (11), so for any significance level
δ we can find the set of ỹ for which p(ỹ) > δ as the union of finitely many intervals and
rays. Algorithm 1 implements a slightly modified version of this idea. It creates a list of the
points (10) and (11), sorts it in ascending order obtaining y(1) , . . . , y(u) , adds y(0) = −∞ to
the beginning and y(u+1) = ∞ to the end of this list, and then computes N (j), the number
of Si which contain the interval (y(j) , y(j+1) ), for j = 0, . . . , u, and M (j) the number of Si
which contain the point y(j) , for j = 1, . . . , u.

4. k -Nearest Neighbours Regression ICP
As the TCP technique follows a transductive approach, most of its computations are repeated for every test example. The reason for this is that the test example is included in
the training set of the underlying algorithm of the TCP in order to calculate the required
nonconformity measures. This means that the underlying algorithm is retrained for every
test example, which renders TCP quite computationally inefficient for application to large
data sets.
Inductive Conformal Predictors (ICP) are based on the same general idea described in
Section 2, but follow an inductive approach, which allows them to train their underlying
algorithm just once. This is achieved by splitting the training set (of size l) into two smaller
sets, the calibration set with q < l examples and the proper training set with m := l − q
examples. The proper training set is used for creating the prediction rule D{z1 ,...,zm } and only
822

Regression Conformal Prediction with Nearest Neighbours

Algorithm 2: k-NNR ICP
Input: training set {(x1 , y1 ), . . . , (xl , yl )}, test set {xl+1 , . . . , xl+r }, number of
nearest neighbours k, number of calibration examples q, and significance
level δ.
m := l − q;
P := {};
for i = 1 to q do
Calculate ŷm+i using {(x1 , y1 ), . . . , (xm , ym )} as training set;
Calculate αm+i for the pair zm+i = (xm+i , ym+i );
Add αm+i to P ;
end
Sort P in descending order obtaining α(m+1) , . . . , α(m+q) ;
s := bδ(q + 1)c;
for g = 1 to r do
Calculate ŷl+g using {(x1 , y1 ), . . . , (xm , ym )} as training set;
Output: the predictive region (ŷl+g − α(m+s) , ŷl+g + α(m+s) ).
end

the examples in the calibration set are used for calculating the p-value of each possible label
of the new test example. More specifically, the non-conformity score αm+i of each example
zm+i in the calibration set {zm+1 , . . . , zm+q } is calculated as the degree of disagreement
between the prediction
ŷm+i = D{z1 ,...,zm } (xm+i )
(13)
and the true label ym+i . In the same way, the non-conformity score αl+g (ỹ) for the assumed
label ỹ of the new test example xl+g is calculated as the degree of disagreement between
ŷl+g = D{z1 ,...,zm } (xl+g )

(14)

and ỹ. Notice that the nonconformity scores of the examples in the calibration set only
need to be computed once. Using these non-conformity scores the p-value of each possible
label ỹ of xl+g can be calculated as
p(ỹ) =

#{i = m + 1, . . . , m + q, l + g : αi ≥ αl+g }
.
q+1

(15)

As with the original CP approach it is impossible to explicitly consider every possible
label ỹ ∈ R of a new example xl+g and calculate its p-value. However, now both the nonconformity scores of the calibration set examples αm+1 , . . . , αm+q and the k-NNR prediction
ŷl+g remain fixed for each test example xl+g , and the only thing that changes for different
values of the assumed label ỹ is the nonconformity score αl+g . Therefore p(ỹ) changes only
at the points where αl+g (ỹ) = αi for some i = m + 1, . . . , m + q. As a result, for a confidence
level 1 − δ we only need to find the biggest αi such that when αl+g (ỹ) = αi then p(ỹ) > δ,
which will give us the maximum and minimum ỹ that have a p-value bigger than δ and consequently the beginning and end of the corresponding predictive region. More specifically,
823

Papadopoulos, Vovk, & Gammerman

we sort the nonconformity scores of the calibration examples in descending order obtaining
the sequence
α(m+1) , . . . , α(m+q) ,
(16)
and output the predictive region
(ŷl+g − α(m+s) , ŷl+g + α(m+s) ),

(17)

s = bδ(q + 1)c.

(18)

where

The whole process is detailed in Algorithm 2. Notice that as opposed to Algorithm 1 where
all computations have to be repeated for every test example, here only the part inside the
second for loop is repeated.
The parameter q given as input to Algorithm 2 determines the number of training
examples that will be allocated to the calibration set and the nonconformity scores of which
will be used by the ICP to generate its predictive regions. These examples should only take
up a small portion of the training set, so that their removal will not dramatically reduce the
predictive ability of the underlying algorithm. As we are mainly interested in the confidence
levels of 99% and 95%, the calibration sizes we use are of the form q = 100n − 1, where n
is a positive integer (see (18)).

5. Normalized Nonconformity Measures
The main aim of this work was to improve the typical regression nonconformity measure (8)
by normalizing it with the expected accuracy of the underlying method. The intuition
behind this is that if two examples have the same nonconformity score as defined by (8)
and the prediction ŷ for one of them was expected to be more accurate than the other,
then the former is actually stranger than the latter. This leads to predictive regions that
are larger for the examples which are more difficult to predict and smaller for the examples
which are easier to predict.
The first measure of expected accuracy we use is based on the distance of the example
from its k nearest neighbours. Since the k nearest training examples are the ones actually
used to derive the prediction of our underlying method for an example, the nearer these are
to the example, the more accurate we expect this prediction to be.
For each example zi , let us denote by Ti the training set used for generating the prediction
ŷi . This will be the set
Ti = {z1 , . . . , zi−1 , zi+1 , . . . , zl+1 }
(19)
in the case of the TCP and the set
Ti = {z1 , . . . , zm }

(20)

in the case of the ICP. Furthermore, we denote the k nearest neighbours of xi in Ti as
(xi1 , yi1 ), . . . , (xik , yik ).
824

(21)

Regression Conformal Prediction with Nearest Neighbours

and the sum of the distances between xi and its k nearest neighbours as
dki =

k
X

distance(xi , xij ).

(22)

j=1

We could use dki as a measure of accuracy, in fact it was used successfully in our previous
work (Papadopoulos et al., 2008). However, here in order to make this measure more
consistent across different data sets we use
λki =

dki
,
median({dkj : zj ∈ Ti })

(23)

which compares the distance of the example from its k nearest neighbours with the median
of the distances of all training examples from their k nearest neighbours. Using λki we
defined the nonconformity measures:


 yi − ŷi 

,
(24)
αi = 
γ + λki 
and



 yi − ŷi 
,
αi = 
exp(γλk ) 

(25)

i

where the parameter γ ≥ 0 controls the sensitivity of each measure to changes of λki ; in the
first case increasing γ results in a less sensitive nonconformity measure, while in the second
increasing γ results in a more sensitive measure. The exponential function in definition (25)
was chosen because it has a minimum value of 1, since λki will always be positive, and grows
quickly as λki increases. As a result, this measure is more sensitive to changes when λki is
big, which indicates that an example is unusually far from the training examples.
The second measure of accuracy we use is based on how different the labels of the
example’s k nearest neighbours are, which is measured as their standard deviation. The
more these labels agree with each other, the more accurate we expect the prediction of
the k-nearest neighbours algorithm to be. For an example xi , we measure the standard
deviation of the labels of its k neighbours as
v
u k
u1 X
k
si = t
(yij − yi1,...,k )2 ,
(26)
k
j=1

where
yi1,...,k

k
1X
=
yij .
k

(27)

j=1

Again to make this measure consistent across data sets we divide it with the median standard
deviation of the k nearest neighbour labels of all training examples
ξik =

ski
.
median({skj : zj ∈ Ti })
825

(28)

Papadopoulos, Vovk, & Gammerman

In the same fashion as (24) and (25) we defined the nonconformity measures:


 yi − ŷi 

,
αi = 
γ + ξik 
and



 yi − ŷi 

,
αi = 
exp(γξ k ) 

(29)

(30)

i

where again the parameter γ controls the sensitivity of each measure to changes of ξik .
Finally by combining λki and ξik we defined the nonconformity measures:


 yi − ŷi 
,

(31)
αi = 
γ + λki + ξik 
and





yi − ŷi

,
αi = 
k
k
exp(γλ ) + exp(ρξ ) 
i

(32)

i

where in (31) the parameter γ controls the sensitivity of the measure to changes of both λki
and ξik , whereas in (32) there are two parameters γ and ρ, which control the sensitivity to
changes of λki and ξik respectively.
In order to use these nonconformity measures with the k-Nearest Neighbours Regression
TCP we need to calculate their nonconformity scores as αi = |ai +bi ỹ|. We can easily do this
for (24) and (25) by computing ai and bi as defined in Section 3 and then dividing both by
(γ + λki ) for nonconformity measure (24) and by exp(γλki ) for nonconformity measure (25).
Unfortunately however, the same cannot be applied for all other nonconformity measures
we defined since ξik depends on the labels of the k nearest examples, which change for the
k nearest neighbours of xl+1 as we change ỹ. For this reason TCP is limited to using only
nonconformity measures (24) and (25).
In the case of the ICP we calculate the nonconformity scores αm+1 , . . . , αm+q of the
calibration examples using (24), (25), (29), (30), (31) or (32) and instead of the predictive
region (17) we output
(ŷl+g − α(m+s) (γ + λki ), ŷl+g + α(m+s) (γ + λki )),

(33)

(ŷl+g − α(m+s) exp(γλki ), ŷl+g + α(m+s) exp(γλki )),

(34)

(ŷl+g − α(m+s) (γ + ξik ), ŷl+g + α(m+s) (γ + ξik )),

(35)

(ŷl+g − α(m+s) exp(γξik ), ŷl+g + α(m+s) exp(γξik )),

(36)

(ŷl+g − α(m+s) (γ + λki + ξik ), ŷl+g + α(m+s) (γ + λki + ξik )),

(37)

for (24),
for (25),
for (29),
for (30),
for (31) and
(ŷl+g − α(m+s) (exp(γλki ) + exp(ρξik )), ŷl+g + α(m+s) (exp(γλki ) + exp(ρξik ))),
for (32).
826

(38)

Regression Conformal Prediction with Nearest Neighbours

6. Theoretical Analysis of Nonconformity Measure (29)
In this section we examine k-NNR ICP with nonconformity measure (29) under some specific
assumptions and show that, under these assumptions, the predictive regions produced are
asymptotically optimal; it is important to note that these assumptions are not required for
the validity of the resulting predictive regions. We chose not to formalize all the conditions
needed for our conclusions, as this would have made our statement far too complicated.
Assume that each label yi is generated by a normal distribution N (µxi , σx2i ), where µx
and σx are smooth functions of x, that each xi is generated by a probability distribution that
is concentrated on a compact set and whose density is always greater than some constant
 > 0, and that k  1, m  k and q  k. In this case nonconformity measure (29) with
γ = 0 will be

 

 yi − ŷi   yi − µxi 



,
αi = 
≈
(39)
ski   σxi 
where the division of ski by median({skj : zj ∈ Ti }) is ignored since the latter does not change
within the same data set. The values
ym+q − µxm+q
ym+1 − µxm+1
,...,
σxm+1
σxm+q
will follow an approximately standard normal distribution, and for a new example xl+g with
probability close to 1 − δ we have
yl+g − µxl+g
∈ [−α(m+bδqc) , α(m+bδqc) ],
σxl+g
where α(m+1) , . . . , α(m+q) are the nonconformity scores αm+1 , . . . , αm+q sorted in descending
order. As a result we obtain the region
yl+g ∈ [µxl+g − α(m+bδqc) σxl+g , µxl+g + α(m+bδqc) σxl+g ],
which, on one hand, is close to the standard (and optimal in various senses) prediction
interval for the normal model and, on the other hand, is almost identical to the region (35)
of k-NNR ICP (recall that we set γ = 0 and ξik = ski ).

7. Experimental Results
Our methods were tested on six benchmark data sets from the UCI (Frank & Asuncion,
2010) and DELVE (Rasmussen et al., 1996) repositories:
• Boston Housing, which lists the median house prices for 506 different areas of Boston
MA in $1000s. Each area is described by 13 attributes such as pollution and crime
rate.
• Abalone, which concerns the prediction of the age of abalone from physical measurements. The data set consists of 4177 examples described by 8 attributes such as
diameter, height and shell weight.
827

Papadopoulos, Vovk, & Gammerman

• Computer Activity, which is a collection of a computer systems activity measures
from a Sun SPARCstation 20/712 with 128 Mbytes of memory running in a multiuser university department. It consists of 8192 examples of 12 measured values, such
as the number of system buffer reads per second and the number of system call writes
per second, at random points in time. The task is to predict the portion of time that
the cpus run in user mode, ranging from 0 to 100. We used the small variant of the
data set which contains only 12 of the 21 attributes.
• Kin, which was generated from a realistic simulation of the forward dynamics of an
8 link all-revolute robot arm. The task is to predict the distance of the end-effector
from a target. The data set consists of 8192 examples described by attributes like joint
positions and twist angles. We used the 8nm variant of the data set which contains 8
of the 32 attributes, and is highly non-linear with moderate noise.
• Bank, which was generated from simplistic simulator of the queues in a series of banks.
The task is to predict the rate of rejections, i.e. the fraction of customers that are
turned away from the bank because all the open tellers have full queues. The data
set consists of 8192 examples described by 8 attributes like area population size and
maximum possible length of queues. The 8nm variant of the data set was used with
the same characteristics given in the description of the Kin data set.
• Pumadyn, which was generated from a realistic simulation of the dynamics of a Unimation Puma 560 robot arm. It consists of 8192 examples and the task is to predict
the angular acceleration of one of the robot arm’s links. Each example is described
by 8 attributes, which include angular positions, velocities and torques of the robot
arm. The 8nm variant of the data set was used with the same characteristics given
in the description of the Kin data set.
Before conducting our experiments the attributes of all data sets were normalized to a
minimum value of 0 and a maximum value of 1. Our experiments consisted of 10 random
runs of a fold cross-validation process. Based on their sizes the Boston Housing and Abalone
data sets were split into 10 and 4 folds respectively, while the other four were splint into 2
folds. For determining the number k of nearest neighbours that was used for each data set,
one third of the training set of its first fold was held-out as a validation set and the base
algorithm was tested on that set with different k, using the other two thirds for training.
The number of neighbours k that gave the smallest mean absolute error was selected. Note
that, as explained in Section 2, the choice of k and any other parameter does not affect
the validity of the results produced by the corresponding CP, it only affects their efficiency.
The calibration set sizes were set to q = 100n − 1 (see Section 4), where n was chosen so
that q was approximately 1/10th of each data set’s training size; in the case of the Boston
Housing data set the smallest value n = 1 was used. Table 1 gives the number of folds,
number of nearest neighbours k, and calibration set size q used in our experiments for each
data set, together with the number of examples and attributes it consists of and the width
of its range of labels.
The parameters γ and ρ of our nonconformity measures were set in all cases to 0.5, which
seems to give very good results with all data sets and measures. It is worth to note however,
that somewhat tighter predictive regions can be obtained by adjusting the corresponding
828

Regression Conformal Prediction with Nearest Neighbours

Examples
Attributes
Label range
Folds
k
Calibration size

Boston
Housing

Abalone

Computer
Activity

Kin

Bank

Pumadyn

506
13
45
10
4
99

4177
8
28
4
16
299

8192
12
99
2
8
399

8192
8
1.42
2
7
399

8192
8
0.48
2
4
399

8192
8
21.17
2
6
399

Table 1: Main characteristics and experimental setup for each data set.
parameter(s) of each measure for each data set. We chose to fix these parameters to 0.5 here,
so as to show that the remarkable improvement in the predictive region widths resulting
from the use of the new nonconformity measures does not depend on fine tuning these
parameters.
Since our methods output predictive regions instead of point predictions, the main aim
of our experiments was to check the tightness of these regions. The first two parts of
Tables 2-7 report the median and interdecile mean widths of the regions produced for every
data set by each nonconformity measure of the k-NNR TCP and ICP for the 99%, 95% and
90% confidence levels. We chose to report the median and interdecile mean values instead
of the mean so as to avoid the strong impact of a few extremely large or extremely small
regions.
In the third and last parts of Tables 2-7 we check the reliability of the obtained predictive
regions for each data set. This is done by reporting the percentage of examples for which
the true label is not inside the region output by the corresponding method. In effect this
checks empirically the validity of the predictive regions. The percentages reported here are
very close to the required significance levels and do not change by much for the different
nonconformity measures.
Figures 1-6 complement the information detailed in Tables 2-7 by displaying boxplots
which show the median, upper and lower quartiles, and upper and lower deciles of the
predictive region widths produced for each data set. Each chart is divided into three parts,
separating the three confidence levels we consider, and each part contains 10 boxplots of
which the first three are for the k-NNR TCP with the nonconformity measures (8), (24)
and (25), and the remaining seven are for the k-NNR ICP with all nonconformity measures.
A transformation of the width values reported in Tables 2-7 to the percentage of the
range of possible labels they represent shows that in general the predictive regions produced
by all our methods are relatively tight. The median width percentages of all nonconformity
measures and all data sets are between 17% and 86% for the 99% confidence level and
between 11% and 47% for the 95% confidence level. If we now consider the best performing
nonconformity measure for each data set, the worst median width percentage for the 99%
confidence level is 61% and for the 95% confidence level it is 43% (both for the pumadyn
data set).
By comparing the predictive region tightness of the different nonconformity measures
for each method both in Tables 2-7 and in Figures 1-6, one can see the relatively big
improvement that our new nonconformity measures achieve as compared to the standard
829

Papadopoulos, Vovk, & Gammerman

Method/
Measure
TCP

ICP

(8)
(24)
(25)
(8)
(24)
(25)
(29)
(30)
(31)
(32)

Median Width
90%
95%
99%
12.143 17.842 33.205
11.172 14.862 24.453
10.897 14.468 24.585
13.710 19.442 38.808
11.623 16.480 30.427
11.531 16.702 30.912
11.149 15.233 36.661
10.211 14.228 34.679
10.712 14.723 28.859
10.227 13.897 29.068

Interdecile
Mean Width
90%
95%
99%
12.054 17.870 33.565
11.483 15.289 25.113
11.258 14.956 25.368
13.693 19.417 41.581
11.985 16.991 33.459
11.916 17.116 34.477
12.165 16.645 41.310
11.347 15.820 39.211
11.343 15.612 31.120
10.876 14.832 31.810

Percentage outside
predictive regions
90%
95%
99%
10.24
5.06
0.97
9.94
4.80
0.89
9.92
4.92
0.91
9.47
4.88
0.79
10.59
4.92
0.71
10.08
4.72
0.69
9.55
4.82
0.59
9.68
4.60
0.65
9.88
4.76
0.61
9.31
4.88
0.57

Table 2: The tightness and reliability results of our methods on the Boston Housing data
set.
Method/
Measure
(8)
(24)
(25)
(8)
(24)
(25)
(29)
(30)
(31)
(32)

TCP

ICP

Median Width
90%
95%
99%
6.685 9.267 16.109
6.213 8.487 14.211
6.034 8.278 13.949
6.705 9.486 16.628
6.200 8.305 14.012
6.057 8.205 13.922
5.837 7.987 14.394
5.731 7.926 14.631
5.936 7.999 13.999
5.838 7.962 14.028

Interdecile
Mean Width
90%
95%
99%
6.712 9.259 16.124
6.376 8.708 14.581
6.230 8.545 14.393
6.671 9.388 16.580
6.359 8.513 14.520
6.229 8.434 14.457
6.004 8.229 14.895
5.931 8.200 15.173
6.070 8.174 14.406
5.994 8.178 14.506

Percentage outside
predictive regions
90%
95%
99%
9.94
4.94
0.95
10.03
4.98
0.98
10.03
4.95
0.99
10.32
5.09
1.01
10.57
5.54
1.20
10.50
5.46
1.19
10.47
5.05
1.02
10.37
5.00
0.93
10.57
5.35
1.10
10.54
5.23
1.09

Table 3: The tightness and reliability results of our methods on the Abalone data set.
Method/
Measure
TCP

ICP

(8)
(24)
(25)
(8)
(24)
(25)
(29)
(30)
(31)
(32)

Median Width
90%
95%
99%
10.005 13.161 21.732
8.788 11.427 18.433
8.370 10.856 17.084
10.149 13.588 22.705
9.024 11.725 18.948
8.646 11.340 17.817
8.837 11.877 19.595
8.702 11.618 18.859
8.653 11.301 18.179
8.517 11.114 17.468

Interdecile
Mean Width
90%
95%
99%
10.009 13.113 21.679
9.238 12.017 19.372
8.924 11.572 18.207
10.245 13.467 22.577
9.483 12.333 19.918
9.206 12.067 18.953
9.031 12.145 20.114
9.013 12.031 19.522
9.020 11.789 18.983
8.914 11.627 18.264

Percentage outside
predictive regions
90%
95%
99%
9.98
4.99
0.97
9.95
4.92
0.95
9.92
4.91
0.97
9.71
4.79
0.95
9.51
4.72
0.90
9.40
4.51
0.92
9.75
4.59
0.91
9.49
4.58
0.95
9.70
4.55
0.89
9.46
4.51
0.96

Table 4: The tightness and reliability results of our methods on the Computer Activity data
set.

830

Regression Conformal Prediction with Nearest Neighbours

Method/
Measure
(8)
(24)
(25)
(8)
(24)
(25)
(29)
(30)
(31)
(32)

TCP

ICP

Median Width
90%
95%
99%
0.402 0.491 0.675
0.395 0.480 0.649
0.396 0.481 0.653
0.413 0.508 0.705
0.408 0.498 0.680
0.408 0.501 0.681
0.412 0.497 0.730
0.401 0.482 0.695
0.403 0.486 0.677
0.399 0.487 0.670

Interdecile
Mean Width
90%
95%
99%
0.402 0.491 0.675
0.396 0.481 0.651
0.397 0.482 0.655
0.414 0.515 0.702
0.409 0.499 0.682
0.408 0.502 0.682
0.418 0.504 0.741
0.408 0.491 0.707
0.406 0.489 0.682
0.402 0.490 0.676

Percentage outside
predictive regions
90%
95%
99%
10.13
4.99
0.96
10.18
5.01
0.95
10.16
5.00
0.96
9.86
4.52
0.81
9.73
4.61
0.77
9.84
4.59
0.80
9.74
5.21
0.91
9.74
5.08
0.84
9.81
4.83
0.87
10.05
4.75
0.85

Table 5: The tightness and reliability results of our methods on the Kin data set.

Method/
Measure
(8)
(24)
(25)
(8)
(24)
(25)
(29)
(30)
(31)
(32)

TCP

ICP

Median Width
90%
95%
99%
0.135 0.201 0.342
0.121 0.170 0.265
0.123 0.173 0.270
0.142 0.209 0.361
0.122 0.178 0.274
0.125 0.182 0.282
0.096 0.151 0.306
0.084 0.137 0.272
0.096 0.146 0.247
0.092 0.143 0.249

Interdecile
Mean Width
90%
95%
99%
0.135 0.201 0.341
0.122 0.171 0.267
0.124 0.175 0.273
0.139 0.209 0.362
0.123 0.179 0.277
0.126 0.183 0.284
0.110 0.174 0.352
0.104 0.169 0.334
0.105 0.160 0.270
0.102 0.159 0.274

Percentage outside
predictive regions
90%
95%
99%
10.05
4.99
0.95
10.02
4.94
0.90
10.03
4.95
0.91
9.95
4.71
0.83
10.16
4.71
0.87
10.08
4.72
0.87
9.97
4.51
0.80
10.21
4.61
0.79
10.14
4.46
0.85
10.31
4.58
0.88

Table 6: The tightness and reliability results of our methods on the Bank data set.

Method/
Measure
TCP

ICP

(8)
(24)
(25)
(8)
(24)
(25)
(29)
(30)
(31)
(32)

Median Width
90%
95%
99%
7.909
9.694 13.908
7.761
9.417 13.153
7.774
9.463 13.239
8.008 10.005 13.924
7.869
9.627 13.193
7.892
9.713 13.335
7.745
9.563 14.342
7.460
9.199 13.398
7.549
9.185 13.040
7.579
9.237 12.808

Interdecile
Mean Width
90%
95%
99%
7.927 9.726 13.827
7.781 9.439 13.183
7.793 9.486 13.268
8.097 9.993 14.007
7.894 9.655 13.233
7.919 9.731 13.377
7.842 9.698 14.588
7.634 9.410 13.723
7.610 9.263 13.180
7.660 9.336 12.955

Percentage outside
predictive regions
90%
95%
99%
9.92
4.87
0.99
9.88
4.98
1.04
9.92
4.94
1.02
9.72
4.75
1.06
9.86
4.81
1.09
9.89
4.75
1.10
9.96
4.87
1.04
9.97
4.82
1.07
10.02
4.89
1.04
9.90
4.83
1.15

Table 7: The tightness and reliability results of our methods on the Pumadyn data set.

831

Papadopoulos, Vovk, & Gammerman

100
90

Region Width

80
70
60
50
40
30
20
10
0
90%

95%

99%

Confidence Level
TCP - (8)
ICP - (29)

TCP - (24)
ICP - (30)

TCP - (25)
ICP - (31)

ICP - (8)
ICP - (32)

ICP - (24)

ICP - (25)

Figure 1: Predictive region width distribution for the Boston housing data set.

25

Region Width

20
15
10
5
0
90%

95%

99%

Confidence Level
TCP - (8)
ICP - (29)

TCP - (24)
ICP - (30)

TCP - (25)
ICP - (31)

ICP - (8)
ICP - (32)

ICP - (24)

ICP - (25)

Figure 2: Predictive region width distribution for the Abalone data set.

35

Region Width

30
25
20
15
10
5
0
90%

95%

99%

Confidence Level
TCP - (8)
ICP - (29)

TCP - (24)
ICP - (30)

TCP - (25)
ICP - (31)

ICP - (8)
ICP - (32)

ICP - (24)

ICP - (25)

Figure 3: Predictive region width distribution for the Computer Activity data set.

832

Regression Conformal Prediction with Nearest Neighbours

1.2

Region Width

1
0.8
0.6
0.4
0.2
0
90%

95%

99%

Confidence Level
TCP - (8)
ICP - (29)

TCP - (24)
ICP - (30)

TCP - (25)
ICP - (31)

ICP - (8)
ICP - (32)

ICP - (24)

ICP - (25)

Figure 4: Predictive region width distribution for the Kin data set.

1
0.9

Region Width

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
90%

95%

99%

Confidence Level
TCP - (8)
ICP - (29)

TCP - (24)
ICP - (30)

TCP - (25)
ICP - (31)

ICP - (8)
ICP - (32)

ICP - (24)

ICP - (25)

Figure 5: Predictive region width distribution for the Bank data set.

25

Region Width

20
15
10
5
0
90%

95%

99%

Confidence Level
TCP - (8)
ICP - (29)

TCP - (24)
ICP - (30)

TCP - (25)
ICP - (31)

ICP - (8)
ICP - (32)

ICP - (24)

ICP - (25)

Figure 6: Predictive region width distribution for the Pumadyn data set.

833

Papadopoulos, Vovk, & Gammerman

Method

B. Housing

Abalone

All Other

TCP with all measures
ICP with (8)
ICP with new measures

20 sec
0.6 sec
1.4 sec

51 - 52 min
8 sec
29 sec

2.5 - 2.7 hrs
17 - 18 sec
37 - 39 sec

Table 8: The processing times of k-NNR TCP and ICP.

regression measure (8). In almost all cases the new measures give smaller median and
interdecile mean widths, while in the majority of cases the difference is quite significant.
The degree of improvement is more evident in Figures 1-6 where we see that in many cases
the median widths of the predictive regions obtained with the new measures are even below
the smallest widths obtained with measure (8).
A comparison between the nonconformity measures that are based on the distances of
the k nearest neighbours, (24) and (25), and those that are based on the standard deviation
of their labels, (29) and (30), reveals that the regions produced by the latter cover a much
bigger range of widths. Furthermore, the widths of (24) and (25) seem to be in most cases
tighter on average than those of (29) and (30) for the highest confidence level 99%, whereas
the opposite is true for the 90% and 95% confidence levels. It is also worth mentioning
that measures (29) and (30) are the only ones that produced predictive regions with bigger
median and interdecile mean widths than those of measure (8) in some cases.
The last two measures (31) and (32), which combine the others, seem to give the tightest
predictive region widths of ICP overall. In 11 out of 18 cases one of the two has the smallest
median predictive region width and in all but 1 cases one of the two has the smallest
interdecile mean width. Their superiority is also evident in the figures and especially in
Figures 1, 5 and 6.
One other important comparison is between the widths of the regions produced by the
TCP and ICP approaches. For the standard regression nonconformity measure (8) the
regions of the TCP are in all cases tighter than those of the ICP. This is due to the much
richer set of examples that TCP uses for calculating its predictive regions; TCP uses the
whole training set as opposed to just the calibration examples used by ICP. The difference in
predictive region tightness is much bigger in the results of the Boston housing data set, since
the calibration set in this case consisted of only 99 examples. It is also worth to note that
the widths of the regions produced by ICP with (8) vary much more than the corresponding
widths of the TCP, this is natural since the composition of the calibration set changes to a
much bigger degree from one run to another than the composition of the whole training set.
If we now compare the region widths of the two approaches with nonconformity measures
(24) and (25) we see that, with the exception of the Boston housing set, they are very
similar in terms of distribution. This shows that the new measures are not so dependant on
the composition of the examples used for producing the predictive regions. Furthermore, if
we compare the region widths of the TCP with nonconformity measures (24) and (25) with
those of the ICP with nonconformity measures (31) and (32), we see that in many cases the
regions of the ICP are somewhat tighter, despite the much smaller set of examples it uses
to compute them. This is due to the superiority of the measures (31) and (32).
834

Regression Conformal Prediction with Nearest Neighbours

Finally, in Table 8 we report the processing times of the two approaches. We use two
rows for reporting the times of the ICP, one for measure (8) and one for all new measures,
since the new measures require some extra computations; i.e. finding the distances of all
training examples from their k nearest neighbours and/or the standard deviation of their
k nearest neighbour labels. We also group the times of the Computer Activity, Kin, Bank
and Pumadyn data sets, which were almost identical as they consist of the same number
of examples, into one column. This table demonstrates the huge computational efficiency
improvement of ICP over TCP. It also shows that there is some computational overhead
involved in using the new nonconformity measures with ICP, but it is not so important,
especially baring in mind the degree of improvement they bring in terms of predictive region
tightness.

8. Comparison with Gaussian Process Regression
In this section we compare the predictive regions produced by our methods with those
produced by Gaussian Processes (GPs, Rasmussen & Williams, 2006), which is one of the
most popular Bayesian machine learning approaches. We first compare Gaussian Process
Regression (GPR) and the k-NNR CPs on artificially generated data that satisfy the GP
prior and then check the results of GPR on three of the data sets described in Section 7,
namely Boston Housing, Abalone and Computer Activity. Our implementation of GPR was
based on the Matlab code that accompanies the work of Rasmussen and Williams.
For our first set of experiments we generated 100 artificial data sets consisting of
1000 training and 1000 test examples with inputs drawn from a uniform distribution over
[−10, 10]5 . The labels of each data set were generated from a Gaussian Process with a
covariance function defined as the sum of a squared exponential (SE) covariance and independent noise and hyperparameters (l, σf , σn ) = (1, 1, 0.1); i.e. a unit length scale, a unit
signal magnitude and a noise standard deviation of 0.1. We then applied GPR on these
data sets using exactly the same covariance function and hyperparameters and compared
the results with those of the k-NNR CPs (which do not take into account any information
about how the data were generated). Table 9 reports the results over all 100 data sets obtained by GPR and our methods in the same manner as Tables 2-7. In this case, since the
data meets the GPR prior, the percentage of labels outside the predictive regions produced
by GPR are more or less equal to the required significance level as we would expect. The
same is true for the regions produced by all our methods. Also, although the predictive
regions produced by GPR are tighter than those produced by our methods, the difference
between the two is very small.
Our experiments on the three benchmark data sets were performed following exactly the
same setting with our experiments on the two CPs, including the use of the same seed for
each fold-cross validation run. In terms of data preprocessing, we normalised the attributes
of each data set setting the mean of each attribute to 0 and its standard deviation to 1, while
we also centred the labels of each data set so as to have a zero mean; these preprocessing
steps are advisable for GPR. We tried out a SE covariance and a Matern covariance with
smoothness set to v = 3/2 and v = 5/2. In the case of the SE covariance, we used
the automatic relevance determination version of the function, which allows for a separate
length-scale for each attribute determined by a corresponding hyperparameter. In all cases
835

Papadopoulos, Vovk, & Gammerman

Method/
Measure
GPR
TCP

ICP

(8)
(24)
(25)
(8)
(24)
(25)
(29)
(30)
(31)
(32)

Median Width
90%
95%
99%
3.306 3.940 5.178
3.332 3.999 5.277
3.322 3.982 5.306
3.322 3.975 5.290
3.344 3.978 5.264
3.346 4.016 5.329
3.337 3.994 5.302
3.333 3.989 5.292
3.331 3.993 5.280
3.340 3.995 5.308
3.332 3.990 5.279

Interdecile
Mean Width
90%
95%
99%
3.306 3.939 5.177
3.332 3.989 5.274
3.339 4.001 5.332
3.335 3.990 5.309
3.337 4.001 5.283
3.358 4.029 5.349
3.348 4.005 5.320
3.339 3.994 5.305
3.338 3.997 5.295
3.348 4.002 5.324
3.339 3.994 5.294

Percentage outside
predictive regions
90%
95%
99%
10.01
4.95
0.99
9.93
4.81
0.95
9.90
4.82
0.98
9.88
4.82
0.95
9.90
4.80
0.95
9.83
4.76
1.01
9.86
4.78
0.98
9.97
4.88
0.96
9.90
4.84
0.95
9.85
4.79
0.97
9.88
4.81
0.97

Table 9: Comparison of our methods with Gaussian Process Regression on artificial data
generated by a Gaussian Process.

Covariance
Function
SE
Matern v = 3/2
Matern v = 5/2

Median Width
90%
95%
99%
7.168
8.540 11.225
8.369
9.972 13.106
8.476 10.100 13.274

Interdecile
Mean Width
90%
95%
99%
7.419
8.840 11.618
8.636 10.290 13.524
8.748 10.423 13.699

Percentage outside
predictive regions
90%
95%
99%
10.28
6.70
2.98
8.02
5.04
2.65
7.71
4.88
2.61

Table 10: The tightness and reliability results of Gaussian Process Regression on the Boston
Housing data set.

Covariance
Function
SE
Matern v = 3/2
Matern v = 5/2

Median Width
90%
95%
99%
6.705 7.988 10.499
6.740 8.031 10.555
6.750 8.042 10.570

Interdecile
Mean Width
90%
95%
99%
6.711 7.996 10.509
7.046 8.396 11.034
6.755 8.048 10.577

Percentage outside
predictive regions
90%
95%
99%
9.43
6.47
2.92
9.02
6.11
2.77
9.15
6.23
2.82

Table 11: The tightness and reliability results of Gaussian Process Regression on the
Abalone data set.

836

Regression Conformal Prediction with Nearest Neighbours

Covariance
Function
SE
Matern v = 3/2
Matern v = 5/2

Median Width
90%
95%
99%
8.115 9.669 12.708
8.120 9.675 12.715
8.340 9.937 13.060

Interdecile
Mean Width
90%
95%
99%
8.198
9.768 12.838
8.471 10.093 13.265
8.617 10.267 13.494

Percentage outside
predictive regions
90%
95%
99%
10.15
6.47
2.38
9.63
5.91
2.09
9.63
5.81
2.09

Table 12: The tightness and reliability results of Gaussian Process Regression on the Computer Activity data set.

the actual covariance function was defined as the sum of the corresponding covariance and
independent noise. All hyperparameters were adapted by maximizing marginal likelihood
on each training set as suggested by Rasmussen and Williams (2006); the adaptation of
hyperparameters using leave-one-out cross-validation produces more or less the same results.
Tables 10-12 report the results obtained by GRP on the three data sets with each
covariance function. By comparing the values reported in the first two parts of this tables
with those in Tables 2-4 one can see that the regions produced by GPR are tighter in almost
all cases. However, the percentage of predictive regions that do not include the true label of
the example is much higher than the required for the 95% and 99% confidence levels. This
shows that the predictive regions produced by GPR are not valid and therefore they are
misleading if the correct prior is not known. On the contrary, as demonstrated in Section 2,
CPs produce valid predictive regions even if the parameters or underlying algorithm used
are totally wrong.

9. Conclusions
We presented the Transductive and Inductive Conformal Predictors based on the k-Nearest
Neighbours Regression algorithm. In addition to the typical regression nonconformity measure, we developed six novel definitions which take into account the expected accuracy of
the k-NNR algorithm on the example in question. Our definitions assess the expected accuracy of k-NNR on the example based on its distances from its k nearest examples (24)
and (25), on the standard deviation of their labels (29) and (30), or on a combination of
the two (31) and (32).
The experimental results obtained by applying our methods to various data sets show
that in all cases they produce reliable predictive intervals that are tight enough to be useful
in practice. Additionally, they illustrate the great extent to which our new nonconformity
measure definitions improve the performance of both the transductive and inductive method
in terms of predictive region tightness. In the case of the ICP, with which all new measures
were evaluated, definitions (31) and (32) appear to be superior to all other measures, giving
the overall tightest predictive regions. Moreover, a comparison between the TCP and ICP
methods suggests that, when dealing with relatively large data sets the use of nonconformity
measures (31) and (32) makes ICP perform equally well with TCP in terms of predictive
region tightness, whereas it has a vast advantage when it comes to computational efficiency.
Finally, a comparison with Gaussian Process Regression (GPR) demonstrated that our
837

Papadopoulos, Vovk, & Gammerman

methods produce almost as tight predictive regions as those of GPR when the correct prior
is known, while GPR may produce misleading regions on real world data on which the
required prior knowledge is not available.
The main future direction of this work is the development of normalized nonconformity
measures like the ones presented in this paper based on other popular regression techniques,
such as Ridge Regression and Support Vector Regression. Although in the case of Ridge
Regression one such measure was already defined for ICP (Papadopoulos et al., 2002a), it
unfortunately cannot be used with the TCP approach; thus there is potentially a considerable performance gain to be achieved from a definition of this kind for TCP. Moreover, an
equally important future aim is the application of our methods to medical or other problems
where the provision of predictive regions is desirable, and the evaluation of their results by
experts in the corresponding field.

Acknowledgments
We would like to thank Savvas Pericleous and Haris Haralambous for useful discussions.
We would also like to thank the anonymous reviewers for their insightful and constructive
comments. This work was supported in part by the Cyprus Research Promotion Foundation
through research contract PLHRO/0506/22 (“Development of New Conformal Prediction
Methods with Applications in Medical Diagnosis”).

References
Bellotti, T., Luo, Z., Gammerman, A., Delft, F. W. V., & Saha, V. (2005). Qualified predictions for microarray and proteomics pattern diagnostics with confidence machines.
International Journal of Neural Systems, 15 (4), 247–258.
Cristianini, N., & Shawe-Taylor, J. (2000). An Introduction to Support Vector Machines
and Other Kernel-based Methods. Cambridge University Press, Cambridge.
Dashevskiy, M., & Luo, Z. (2008). Network traffic demand prediction with confidence. In
Proceedings of the IEEE Global Telecommunications Conference 2008 (GLOBECOM
2008), pp. 1453–1457. IEEE.
Frank, A., & Asuncion, A. (2010).
http://archive.ics.uci.edu/ml.

UCI machine learning repository.

URL

Gammerman, A., Vapnik, V., & Vovk, V. (1998). Learning by transduction. In Proceedings
of the Fourteenth Conference on Uncertainty in Artificial Intelligence, pp. 148–156,
San Francisco, CA. Morgan Kaufmann.
Gammerman, A., Vovk, V., Burford, B., Nouretdinov, I., Luo, Z., Chervonenkis, A., Waterfield, M., Cramer, R., Tempst, P., Villanueva, J., Kabir, M., Camuzeaux, S., Timms,
J., Menon, U., & Jacobs, I. (2009). Serum proteomic abnormality predating screen
detection of ovarian cancer. The Computer Journal, 52 (3), 326–333.
Gammerman, A., & Vovk, V. (2007). Hedging predictions in machine learning: The second
computer journal lecture. The Computer Journal, 50 (2), 151–163.
838

Regression Conformal Prediction with Nearest Neighbours

Holst, H., Ohlsson, M., Peterson, C., & Edenbrandt, L. (1998). Intelligent computer reporting ‘lack of experience’: a confidence measure for decision support systems. Clinical
Physiology, 18 (2), 139–147.
Melluish, T., Saunders, C., Nouretdinov, I., & Vovk, V. (2001). Comparing the Bayes and
Typicalness frameworks. In Proceedings of the 12th European Conference on Machine
Learning (ECML’01), Vol. 2167 of Lecture Notes in Computer Science, pp. 360–371.
Springer.
Nouretdinov, I., Melluish, T., & Vovk, V. (2001a). Ridge regression confidence machine. In
Proceedings of the 18th International Conference on Machine Learning (ICML’01),
pp. 385–392, San Francisco, CA. Morgan Kaufmann.
Nouretdinov, I., Vovk, V., Vyugin, M. V., & Gammerman, A. (2001b). Pattern recognition
and density estimation under the general i.i.d. assumption. In Proceedings of the 14th
Annual Conference on Computational Learning Theory and 5th European Conference
on Computational Learning Theory, Vol. 2111 of Lecture Notes in Computer Science,
pp. 337–353. Springer.
Papadopoulos, H. (2008).
Inductive Conformal Prediction: Theory and application to neural networks.
In Fritzsche, P. (Ed.), Tools in Artificial Intelligence, chap. 18, pp. 315–330. InTech, Vienna, Austria.
URL
http://www.intechopen.com/download/pdf/pdfs id/5294.
Papadopoulos, H., Gammerman, A., & Vovk, V. (2008). Normalized nonconformity measures for regression conformal prediction. In Proceedings of the IASTED International
Conference on Artificial Intelligence and Applications (AIA 2008), pp. 64–69. ACTA
Press.
Papadopoulos, H., Gammerman, A., & Vovk, V. (2009a). Confidence predictions for the
diagnosis of acute abdominal pain. In Iliadis, L., Vlahavas, I., & Bramer, M. (Eds.),
Artificial Intelligence Applications & Innovations III, Vol. 296 of IFIP International
Federation for Information Processing, pp. 175–184. Springer.
Papadopoulos, H., Papatheocharous, E., & Andreou, A. S. (2009b). Reliable confidence intervals for software effort estimation. In Proceedings of the 2nd Workshop
on Artificial Intelligence Techniques in Software Engineering (AISEW 2009), Vol.
475 of CEUR Workshop Proceedings. CEUR-WS.org. URL http://ceur-ws.org/Vol475/AISEW2009/22-pp-211-220-208.pdf.
Papadopoulos, H., Proedrou, K., Vovk, V., & Gammerman, A. (2002a). Inductive confidence
machines for regression. In Proceedings of the 13th European Conference on Machine
Learning (ECML’02), Vol. 2430 of Lecture Notes in Computer Science, pp. 345–356.
Springer.
Papadopoulos, H., Vovk, V., & Gammerman, A. (2002b). Qualified predictions for large
data sets in the case of pattern recognition. In Proceedings of the 2002 International
Conference on Machine Learning and Applications (ICMLA’02), pp. 159–163. CSREA
Press.
839

Papadopoulos, Vovk, & Gammerman

Papadopoulos, H., Vovk, V., & Gammerman, A. (2007). Conformal prediction with neural
networks. In Proceedings of the 19th IEEE International Conference on Tools with
Artificial Intelligence (ICTAI’07), Vol. 2, pp. 388–395. IEEE Computer Society.
Proedrou, K., Nouretdinov, I., Vovk, V., & Gammerman, A. (2002). Transductive confidence
machines for pattern recognition. In Proceedings of the 13th European Conference on
Machine Learning (ECML’02), Vol. 2430 of Lecture Notes in Computer Science, pp.
381–390. Springer.
Rasmussen, C. E., Neal, R. M., Hinton, G. E., Van Camp, D., Revow, M., Ghahramani, Z.,
Kustra, R., & Tibshirani, R. (1996). DELVE: Data for evaluating learning in valid
experiments. URL http://www.cs.toronto.edu/∼delve/.
Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes for Machine Learning.
MIT Press.
Saunders, C., Gammerman, A., & Vovk, V. (1999). Transduction with confidence and
credibility. In Proceedings of the 16th International Joint Conference on Artificial
Intelligence, Vol. 2, pp. 722–726, Los Altos, CA. Morgan Kaufmann.
Saunders, C., Gammerman, A., & Vovk, V. (2000). Computationally efficient transductive
machines. In Proceedings of the Eleventh International Conference on Algorithmic
Learning Theory (ALT’00), Vol. 1968 of Lecture Notes in Artificial Intelligence, pp.
325–333, Berlin. Springer.
Shahmuradov, I. A., Solovyev, V. V., & Gammerman, A. J. (2005). Plant promoter prediction with confidence estimation. Nucleic Acids Research, 33 (3), 1069–1076.
Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27 (11),
1134–1142.
Vovk, V., Gammerman, A., & Shafer, G. (2005). Algorithmic Learning in a Random World.
Springer, New York.
Zhang, J., Li, G., Hu, M., Li, J., & Luo, Z. (2008). Recognition of hypoxia EEG with a preset
confidence level based on EEG analysis. In Proceedings of the International Joint
Conference on Neural Networks (IJCNN 2008), part of the IEEE World Congress on
Computational Intelligence (WCCI 2008), pp. 3005–3008. IEEE.

840

Journal of Artificial Intelligence Research 40 (2011) 767-813

Submitted 11/10; published 04/11

Scaling up Heuristic Planning with Relational Decision Trees
Tomás de la Rosa
Sergio Jiménez
Raquel Fuentetaja
Daniel Borrajo

TROSA @ INF. UC 3 M . ES
SJIMENEZ @ INF. UC 3 M . ES
RFUENTET @ INF. UC 3 M . ES
DBORRAJO @ IA . UC 3 M . ES

Departamento de Informática
Universidad Carlos III de Madrid
Av. Universidad 30, Leganés, Madrid, Spain

Abstract
Current evaluation functions for heuristic planning are expensive to compute. In numerous
planning problems these functions provide good guidance to the solution, so they are worth the
expense. However, when evaluation functions are misguiding or when planning problems are large
enough, lots of node evaluations must be computed, which severely limits the scalability of heuristic planners. In this paper, we present a novel solution for reducing node evaluations in heuristic
planning based on machine learning. Particularly, we define the task of learning search control for
heuristic planning as a relational classification task, and we use an off-the-shelf relational classification tool to address this learning task. Our relational classification task captures the preferred action
to select in the different planning contexts of a specific planning domain. These planning contexts
are defined by the set of helpful actions of the current state, the goals remaining to be achieved, and
the static predicates of the planning task. This paper shows two methods for guiding the search of
a heuristic planner with the learned classifiers. The first one consists of using the resulting classifier as an action policy. The second one consists of applying the classifier to generate lookahead
states within a Best First Search algorithm. Experiments over a variety of domains reveal that our
heuristic planner using the learned classifiers solves larger problems than state-of-the-art planners.

1. Introduction
During the last few years, state-space heuristic search planning has achieved significant results and
has become one of the most popular paradigms for automated planning. However, heuristic search
planners suffer from strong scalability limitations. Even well-studied domains like Blocksworld
become challenging for these planners when the number of blocks is relatively large. Usually, statespace heuristic search planners are based on action grounding, which makes the state-space to be
explored very large when the number of objects and/or action parameters is large enough. Moreover,
domain-independent heuristics are expensive to compute. In domains where these heuristics are
more misleading, heuristic planners spend most of their planning time computing useless node
evaluations. Even with the best current domain-independent heuristic functions in the literature,
forward chaining heuristic planners currently have to visit too many nodes, which takes considerable
time, especially due to the time required to compute those heuristic functions.
These problems entail strong limitations on the application of heuristic planners to real problems. For instance, logistics applications need to handle hundreds of objects together with hundreds
of vehicles and locations (Flórez, Garcı́a, Torralba, Linares, Garcı́a-Olaya, & Borrajo, 2010). Current heuristic search planners exhaust the computational resources before solving a problem in a
real logistics application.
c
2011
AI Access Foundation. All rights reserved.

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

A classic approach for dealing with planning scalability issues is assisting the search engines
of planners with Domain-specific Control Knowledge (DCK). Examples of planning systems that
benefit from this knowledge are TLP LAN (Bacchus & Kabanza, 2000), TALP LANNER (Doherty
& Kvarnström, 2001) and SHOP2 (Nau, Au, Ilghami, Kuter, Murdock, Wu, & Yaman, 2003).
Nevertheless, hand-coding DCK is a complex task because it implies expertise in both, the planning domain and the search algorithm of the planning system. In recent years there has been a
renewed interest in using Machine Learning (ML) to automatically extract DCK. Zimmerman and
Kambhampati (2003) made a comprehensive survey of ML for defining DCK. As shown in the first
learning for planning competition held in 2008 (Learning Track), this renewed interest is specially
targeted at heuristic planners.
This paper presents an approach for learning DCK for planning by building domain-dependent
relational decision trees from examples of good quality solutions of a forward-chaining heuristic
planner. These decision trees are built with an off-the-shelf relational classification tool and capture which is the best action to take for each possible decision of the planner in a given domain.
The resulting decision trees can be used either as a policy to solve planning problems directly or
to generate lookahead states within a Best First Search (BFS) algorithm. Both techniques allow
the planner to avoid state evaluations, which helps in the objective of improving scalability. The
approach has been implemented in a system we have called ROLLER. This work is an improvement
of a previous one (De la Rosa, Jiménez, & Borrajo, 2008). Alternatively, a ROLLER version for
repairing relaxed plans (De la Rosa, Jiménez, Garcı́a-Durán, Fernández, Garcı́a-Olaya, & Borrajo,
2009) competed in the Learning Track of the 6th International Planning Competition (IPC) held in
2008. ROLLER improvements presented in this article are mainly a result of lessons learned from
the competition, which will be discussed later.
The paper is organized as follows. Section 2 introduces the issues that need to be considered
when designing a learning system for heuristic planning. They will help us to clarify which decisions we made in the development of our approach. Section 3 describes the ROLLER system in
detail. Section 4 presents the experimental results obtained in a variety of benchmarks. Section 5
discusses the improvements of the ROLLER system compared to the previous version of the system.
Section 6 revises the related work on learning DCK for heuristic planning. Finally, the last section
discusses some conclusions and future work.

2. Common Issues in Learning Domain-specific Control Knowledge
When designing an ML process for the automatic acquisition of DCK, one must consider some
common issues, among others:
1. The representation of the learned knowledge. Predicate logic is a common language to
represent planning DCK because planning tasks are usually defined in this language. However, other representation languages have been used aiming to make the learning of DCK
more effective. For instance, languages for describing object classes such as the Concept
Language (Martin & Geffner, 2000) or Taxonomic Syntax (Mcallester & Givan, 1989) have
been shown to provide a useful learning bias for different domains.
Another representation issue is the selection of the feature space (i.e., the set of instance
features used for representing the learned knowledge and for training the system.). The feature
space should be able to capture the key knowledge of the domain. Traditionally, the feature
768

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

space consisted only of predicates for describing the current state and the goals of the planning
task. The feature space can be enriched with extra predicates, called metapredicates, which
capture extra useful information of the planning context such as the applicable operators or
the pending goals (Veloso, Carbonell, Pérez, Borrajo, Fink, & Blythe, 1995). Recently, works
on learning DCK for heuristic planners define metapredicates to capture information about
the planning context of a heuristic planner, including for example, predicates which capture
the actions in the relaxed plan of a given state (Yoon, Fern, & Givan, 2008).
2. The learning algorithms. Inductive Logic Programming (ILP) (Muggleton & De Raedt,
1994) deals with the development of inductive techniques which learn a given target concept
from examples described in predicate logic. Because planning tasks are normally represented
in predicate logic, ILP algorithms are quite suitable for DCK learning. Moreover, in recent
years, ILP has broadened its scope to cover the whole spectrum of ML tasks such as regression, clustering and association analysis, extending the classical propositional ML algorithms
to the relational framework. Consequently, ILP algorithms have been used by heuristic planners to capture DCK in different forms such as decision rules to select actions in the different
planning context or regression rules to obtain better node evaluations (Yoon et al., 2008).
3. The generation of training examples. The success of ML algorithms depends directly on
the quality of the training examples used. When learning planning DCK, these examples
are extracted from the experience collected from solving training problems, which should
be representative of different tasks across the domain. Therefore, the quality of the training
examples will depend on the variety of the problems used for training and the quality of the
solutions to these problems. Traditionally, these training problems are obtained by random
generators provided with some parameters to tune problems difficulty. In this way, one has
to find, for each domain, which kind of problems makes the learning algorithm generalize
useful DCK.
4. Use of the learned DCK. Decisions made for each of these three issues affect the quality of
the learned DCK. Some representation schemes may not be expressive enough to capture effective DCK for a given domain, the learning algorithm may not be able to acquire the useful
DCK within reasonable time and memory requirements, or the set of training problems may
lack significant examples of the key knowledge. In all these situations, a direct use of the
learned DCK will not improve the scalability of the planner, and could even decrease its performance. An effective way of dealing with this problem in heuristic planners is integrating
the learned DCK within robust strategies such as a Best-First Search (Yoon et al., 2008) or
combining it with domain-independent heuristic functions (Röger & Helmert, 2010).

3. The ROLLER System
This section describes how the general scheme for learning DCK is instantiated in the ROLLER
system. First, it describes the DCK representation followed by ROLLER. Second, it explains the
learning algorithm used by ROLLER. Third, it depicts how ROLLER collects good quality training
examples and finally, it shows different approaches for scaling up heuristic planning algorithms with
the learned DCK.
769

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

3.1 The Representation of the Learned Knowledge: Helpful Contexts in Heuristic Planning
We present our approach following the notation specified by the Planning Domain Definition Language (PDDL) for typed STRIPS tasks. Accordingly, the definition of a planning domain D comprises the definition of:
• A hierarchy of types.
• A set of typed constants, CD , representing the objects present in all tasks for the domain. This
set can be empty.
• A set of predicate symbols, P, each one with its corresponding arity and the type of its
arguments.
• A set of operators O, whose arguments are typed variables.
Variables are declared directly when defining each operator argument, so they are local to the
operator definition. We will call Po the set of atomic formulas that can be generated using the
defined predicates P, the variables defined as arguments of the operator o ∈ O, and the general
constants CD . Then, each operator o ∈ O is defined by three sets: pre(o) ⊆ Po , the operator
preconditions; add(o) ⊆ Po , the positive effects; and del(o) ⊆ Po , the negative effects of the
operator.
A planning task Π for the domain D is a tuple < CΠ , s0 , G > where CΠ is a set of typed
constants representing the objects which are particular to the task, s0 is the set of ground atomic
formulas describing the initial state and G is the set of ground atomic formulas describing the goals.
Given the total set of constants C = CΠ ∪ CD , the task Π defines a finite state space S and a finite set
A of instantiated operators over O. A state s ∈ S is a set of ground atomic formulas representing the
facts that are true in s. States are described following the closed world assumption. An instantiated
operator or action a ∈ A is an operator where each variable has been replaced by a constant in C of
the same type. Thus, A is the set of all actions a that can be generated using the set of constants C
and the set of operators O. Under this definition, solving a planning task Π implies finding a plan π
as the sequence of actions (a1 , . . . , an ), ai ∈ A that transforms the initial state into a state in which
the goals are achieved.
The planning contexts defined by ROLLER rely on the concepts of relaxed plan heuristic and
helpful actions, both introduced by the FF planner (Hoffmann & Nebel, 2001). The relaxed plan
heuristic returns an integer for each evaluated node, which is the number of actions in a solution to
the relaxed planning task Π+ from that node. Π+ is a simplification of the original task in which the
deletes of actions are ignored. The idea of delete-relaxation for computing heuristics in planning
was first introduced by McDermott (1996) and by Bonet, Loerincs and Geffner (1997).
The relaxed plan is extracted from a relaxed planning graph, which is a sequence of facts and
actions layers (F0 , A0 , . . . , At , Ft ). The first fact layer contains all facts in the initial state. Then
each action layer contains the set of all applicable actions given the previous fact layer. Each fact
layer contains the set of all positive effects of all actions appearing in the previous layers. The
process finishes when all the goals are in a fact layer, or when two consecutive facts layers have the
same facts. In the last case, the relaxed problems have no solution and the relaxed plan heuristic
returns infinity.
770

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

Once the relaxed planning graph is built, the solution is extracted in a backwards process. Each
goal appearing for the first time in fact layer i is assigned to the set of goals of that layer, Gi . Then,
from the last set of goals, Gt , to the second set of goals, G1 , and for each goal in each goals set,
an action is selected which generates the goal and whose layer index is minimal. Afterwards, each
precondition of that action (i.e. a subgoal) is included in the goals set corresponding to the first
layer where this fact appears. When the process is finished, the set of selected actions comprises the
relaxed plan.
According to the extraction process, the FF planner marks as helpful actions the set of actions
in the first layer A0 of the relaxed planning graph which can achieve any of the subgoals of the next
fact layer, i.e. in the goals set G1 . In other words, helpful actions are those applicable actions which
generate facts that are top-level goals of the problems or required by any action of the relaxed plan.
Formally, the set of helpful actions of a given state s is defined as:
helpful (s) = {a ∈ A0 | add(a) ∩ G1 6= ∅}
The FF planner uses helpful actions in the search as a pruning technique, because they are considered as the only candidates for being selected during the search. Given that each state generates
its own particular set of helpful actions, we claim that the helpful actions, together with the remaining goals and the static literals of the planning task, encode a helpful context related to each state.
The helpful actions and the remaining target goals relate actions that are more likely to be applied
with the goals that need to be achieved. These relations arise because helpful actions and target
goals often share some arguments (problem objects). Additionally, the static predicates express
facts that characterize objects of the planning task. Identifying these objects is also relevant since
they may be shared arguments for helpful actions and/or target goals.
Definition 1 The helpful context for a state s is defined as
H(s) = {helpful (s), target(s), static(s)}
where target(s) ⊆ G describes the set of goals not achieved in the state s, target(s) = G − s
and static(s) is the set of literals that always hold in the planning task. They are defined in the
initial state and are present at every state given that they can not be changed by any action. Thus,
static(s) = {p ∈ s | @a ∈ A : p ∈ add(a) ∨ p ∈ del(a)}.
The helpful context is an alternative representation to the tuple <state, goals, applied action>,
traditionally used when learning DCK for planning. Helpful contexts present some advantages for
improving the scalability of heuristic planners:
• In most domains, the set of helpful actions contains the actions most likely to be applied and
focusing reasoning on them has been shown to be a good strategy.
• The set of helpful actions is normally smaller than the set of non-static literals in the state
(i.e., s − static(s)). Thus, the process of matching learned DCK within the search obtains
the benefits of using a more compact representation.
• The number of helpful actions normally decreases when the search has fewer goals left.
Therefore, the matching process will become faster when the search is advancing towards
the goals.
771

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

3.2 The Learning Algorithm: Learning Generalized Policies with Relational Decision Trees
ROLLER implements a two-step learning process for building DCK from a collection of examples
from different helpful contexts:

1. Learning the operator classifier. ROLLER builds a classifier to choose the best operator in the
different helpful contexts.
2. Learning the binding classifiers. For each operator in the domain, ROLLER builds a classifier
to choose the best binding (instantiation of the operator) in the different helpful contexts.
The learning process is split into these two steps to build DCK with off-the-shelf learning
tools. Each planning action may have different number of arguments and arguments of different types (e.g. actions switch on(instrument,satellite) and turn to(satellite,
direction,direction) from the Satellite domain) which hinders the definition of the target
classes. This two-step decision process is also clearer from the decision-making point of view. It
helps users to understand the generated DCK better by focusing on either the decision of which operator to apply or which bindings to use for a given selected operator. Both the learning algorithm
and the set of learning examples are the same for the two learning steps. Figure 1 shows an overview
of the learning process of the ROLLER system.

Roller Learner
Training
Problems

1. operator classifier
Example
Generator

PDDL
Domain

op. examples

Relational
Classification

bind. examples

Tool

2. binding classifiers

...

language bias

Figure 1: Overview of the ROLLER learning process.

3.2.1 L EARNING R ELATIONAL D ECISION T RESS
A classic approach to assist decision making consists of gathering a significant set of previous decisions and building a decision tree that generalizes them. The leaves of the resulting tree contain
the classes (decisions to make), and the internal nodes contain the conditions that lead to those decisions. The most common way to build these trees is following the Top-Down Induction of Decision
Trees (TDIDT) algorithm (Quinlan, 1986). This algorithm builds the tree by repeatedly splitting
the set of training examples according to the conditions that minimize the entropy in the examples. Traditionally, training examples are described in an attribute-value representation. Therefore,
conditions of the decision trees represent tests over the value of a given attribute of the examples.
Nevertheless, this attribute-value approach is not suitable for representing decisions if we want to
keep the predicate logic representation. A better approach is to represent decisions relationally, for
instance, a given action is chosen to reach certain goals in a given context if they share some arguments. Recently, new algorithms for building relational decision trees from examples described as
772

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

predicate logic facts have been developed. These new relational learning algorithms are similar to
the propositional ones, except that (1) condition nodes in the tree do not refer to attribute values, but
to logic queries about relational facts holding in the training examples and (2), these logic queries
can share variables with condition nodes placed above in the decision tree. The learning algorithm
is a greedy search process. Since the space of potential relational decision trees is usually huge, this
search is normally biased according to a specification of syntactic restrictions called language bias.
This specification contains the target concept, the predicates that can appear in the condition nodes
of the trees and some learning-specific knowledge such as type information, or input and output
variables of predicates.
In this paper we use the tool TILDE (Blockeel & De Raedt, 1998) for learning the operator
and binding classifiers. This tool implements a relational version of the TDIDT algorithm, although
any other off-the-shelf tool for learning relational classifiers could have been used, such as PRO GOL (Muggleton, 1995) or RIBL (Emde & Wettschereck, 1996). Each of these different learning
algorithms would provide different results, since they explore the classifiers space differently. The
study of the pros and cons of the different algorithms is beyond the scope of the paper. For a
comprehensive explanation of current relational learning approaches please refer to the work by De
Raedt (2008).
3.2.2 L EARNING THE O PERATOR C LASSIFIER
The inputs to learning the operator classifier are:
• Training examples. Examples are represented in a Prolog-like syntax and consist of the
operator selected (the class) together with the helpful context (the background knowledge in
terms of relational learning) in which it was selected. In particular, an example contains:
– Class. We use the predicate of arity 3 selected to encode the operator chosen in the
context. This predicate is the target concept of this learning step. Its first argument holds
the example identifier that links the rest of the example predicates. The second argument
is the problem identifier, which links the static predicates shared by all examples coming
from the same planning problem. The third argument is the example class, i.e., the name
of the selected operator in the helpful context.
– Helpful predicates. They are predicates to express the helpful actions contained in the
helpful context. The predicate symbol of these predicates is helpful ai where ai
is the name of an instantiated action. The arguments are the example and problem
identifier together with the parameters of action ai . As it is an instantiated action, its
parameters are constants.
– Target goal predicates. They represent the predicates that appear in the goals and do
not hold in the current state. These predicates have the form target goal gi where
gi are the domain predicates. Each predicate also contains the example and problem
identifiers.
– Static predicates. They represent the static predicates of a given problem. These predicates are shared by all the training examples that belong to the same planning problem.
They have the form static fact fi where fi are the domain predicates that do not
appear in the effects of any domain action. They have as arguments the problem identifier and the corresponding arguments of each domain predicate.
773

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Figure 2 shows one learning example with id tr01 e1 consisting of a selection of the operator switch-on and its associated helpful context. This example is used for building the
operator classifier for the Satellite domain.
% Example tr01 e1 from problem tr01
selected(tr01 e1,tr01,switch on).
helpful turn to(tr01 e1,tr01,satellite0,groundstation1,star0).
helpful turn to(tr01 e1,tr01,satellite0,phenomenon2,star0).
helpful turn to(tr01 e1,tr01,satellite0,phenomenon3,star0).
helpful turn to(tr01 e1,tr01,satellite0,phenomenon4,star0).
helpful switch on(tr01 e1,tr01,instrument0,satellite0).
target goal have image(tr01 e1,tr01,phenomenon3,infrared2).
target goal have image(tr01 e1,tr01,phenomenon4,infrared2).
target goal have image(tr01 e1,tr01,phenomenon2,spectrograph1).
% Static Predicates of problem
static fact calibration target(tr01,instrument0,groundstation1).
static fact supports(tr01,instrument0,spectrograph1).
static fact supports(tr01,instrument0,infrared2).
static fact on board(tr01,instrument0,satellite0).

Figure 2: Knowledge base corresponding to an example from the Satellite domain. The example
has the id tr01 e1, which links all example predicates. It was obtained solving the
training problem tr01 which links the rest of examples for the same problem. The
selected operator in this helpful context is switch on, which corresponds to one of the
helpful actions encoded in the helpful predicates of the example.

• Language bias: This bias specifies constraints over the arguments of the predicates in the
training examples. We do not assume any domain-specific constraint, given that our learning
technique is domain-independent. So, this bias only contains restrictions over argument types
and restrictions which ensure that identifier variables are not added as new variables in the
classifier generation. This bias is automatically extracted from the PDDL domain definitions
and consists of a declaration of the predicates used in the learning example and their argument
types. Figure 3 shows the language bias specified for learning the operator classifier for the
Satellite domain.
The resulting relational decision tree represents a set of disjoint rules for action selection that
can be used to provide advice to the planner: the internal nodes of the tree contain the set of conditions related to the helpful context under which the advice can be provided. The leaf nodes contain
the corresponding advice; in this case, the operator to select and the number of examples covered
by the rule. The operator to select is the one which has been selected more often in the training
examples covered by the rule. The operator classifiers learned by ROLLER also advise on nonhelpful actions. Given a state, non-helpful actions are the subset of applicable actions in the state
that are not considered as helpful actions. Certainly, these actions are not part of the helpful contexts defined. However, the learned operator classifiers indicate the name of the operator to select
regardless of whether it was helpful or not. Figure 4 shows the operator tree learned for the Satellite
774

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

% ---- The target concept ---predict(selected(+IdExample,+IdProblem,-Operator)).
type(selected(idex,idprob,class)).
classes([turn to,switch on,switch off,calibrate,take image]).
% ---- The helpful context ---% predicates for the helpful actions
rmode(helpful turn to(+IdExample,+IdProblem,+-S1,+-D1,+-D2)).
type(helpful turn to(idex,idprob,satellite,direction,direction)).
rmode(helpful switch on(+IdExample,+IdProblem,+-I1,+-S1)).
type(helpful switch on(idex,idprob,instrument,satellite)).
rmode(helpful switch off(+IdExample,+IdProblem,+-I1,+-S1)).
type(helpful switch off(idex,idprob,instrument,satellite)).
rmode(helpful calibrate(+IdExample,+IdProblem,+-S1,+-I1,+-D1)).
type(helpful calibrate(idex,idprob,satellite,instrument,direction)).
rmode(helpful take image(+IdExample,+IdProblem,+-S1,+-D1,+-I1,+-M1)).
type(helpful take image(idex,idprob,satellite,direction,instrument,mode)).
% predicates for the target goals
rmode(target goal pointing(+IdExample,+IdProblem,+-S1,+-D1)).
type(target goal pointing(idex,idprob,satellite,direction)).
rmode(target goal have image(+IdExample,+IdProblem,+-D1,+-M1)).
type(target goal have image(idex,idprob,direction,mode)).
% predicates for the static facts
rmode(static fact on board(+IdProblem,+-I1,+-S1)).
type(static fact on board(idprob,instrument,satellite)).
rmode(static fact supports(+IdProblem,+-I1,+-M1)).
type(static fact supports(idprob,instrument,mode)).
rmode(static fact calibration target(+IdProblem,+-I1,+-D1)).
type(static fact calibration target(idprob,instrument,direction)).

Figure 3: Language bias for learning the operator classifier of the Satellite domain. It is automatically generated from the PDDL definition. rmode predicates indicate those which can
be used in the tree. type predicates indicate types for each particular rmode.

domain. In learned decision trees each branch is denoted by the symbols +--:<yes/no>, where
yes indicates the next node for positive answers to the current question and no indicates the next
node for negative answers. In the figure, the first branch states that when there is a calibrate
action in the set of helpful actions, the recommendation (in square brackets) is choosing that action
(i.e. calibrate). In addition, the branch indicates that the recommended action has occurred 44
times in the training examples. Moreover, each leaf node has information (in double square brack775

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

ets) about the number of times each type of action has been selected in the training examples covered
by the rule in the current branch. Thus, in our case, the action calibrate has been selected 44
out of a total of 44 times, and other operators have never been selected. The second branch says
that if there is no calibrate helpful action, but there is a take image one, the planner selected
to take image 110 out of 110 times. If there are no helpful calibrate or take image actions but there is a helpful switch on action, switch on is the recommendation, that has been
selected 44 out of 59 times. Other tree branches are interpreted similarly.
selected(-A,-B,-C)
helpful calibrate(A,B,-D,-E,-F) ?
+--yes:[calibrate] 44.0 [[turn to:0.0,switch on:0.0,switch off:0.0,
|
calibrate:44.0,take image:0.0]]
+--no: helpful take image(A,B,-G,-H,-I,-J) ?
+--yes:[take image] 110.0 [[turn to:0.0,switch on:0.0,switch off:0.0,
|
calibrate:0.0,take image:110.0]]
+--no: helpful switch on(A,B,-K,-L) ?
+--yes:[switch on] 59.0 [[turn to:15.0,switch on:44.0,
|
switch off:0.0,calibrate:0.0,
|
take image:0.0]]
+--no: [turn to] 149.0 [[turn to:149.0,switch on:0.0,
switch off:0.0,calibrate:0.0,
take image:0.0]]

Figure 4: Relational decision tree learned for the operator selection in the Satellite domain. Internal
nodes (with ”?” ending) have queries to helpful contexts. Leaf nodes (in brackets) have
the class and the number of observed examples for each operator.

3.2.3 L EARNING THE B INDING C LASSIFIERS
At the second learning step, a relational decision tree is built for each domain operator o ∈ O. These
trees indicate the bindings to select for o in the different helpful contexts. The inputs for learning
the binding classifier of operator o are:
• Training examples. These consist exclusively of the helpful contexts where operator o was
selected, together with the applicable instantiations of o in these contexts. Note that for a
given helpful context, the applicable instantiations of o may include both helpful and nonhelpful actions. Helpful contexts are coded exactly as in the previous learning step. The
applicable instantiations of o are represented with the selected o predicate. This predicate is the target concept of the second learning step and its arguments are the example
and problem identifiers, the instantiated arguments of the applicable action and the example class (selected or rejected). The purpose of this predicate is to distinguish between
good and bad bindings for the operator. Figure 5 shows a piece of the knowledge base
for building the binding tree corresponding to the action switch on from the Satellite domain. This example, with id tr07 e63, resulted in the selection of the action instantiation
776

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

switch on(instrument1,satellite0). The action switch on(instrument0,
satellite0) was also applicable but it was rejected by the planner.
• Language bias: The bias for learning binding trees is the same as the bias for learning the
operator tree, except that it includes the definition of the selected o predicate. As in the
previous learning step, the language bias for learning a binding tree is also automatically extracted from the PDDL domain definition. Figure 6 shows part of the language bias specified
for learning the binding tree for the action switch on from the Satellite domain.

% Example tr07 e63 from problem tr07
selected switch on(tr07 e63,tr07,instrument0,satellite0,rejected).
selected switch on(tr07 e63,tr07,instrument1,satellite0,selected).
helpful switch on(tr07 e63,tr07,instrument0,satellite0).
helpful switch on(tr07 e63,tr07,instrument1,satellite0).
helpful turn to(tr07 e63,tr07,satellite0,star1,star2).
helpful turn to(tr07 e63,tr07,satellite0,star5,star2).
helpful turn to(tr07 e63,tr07,satellite0,phenomenon7,star2).
helpful turn to(tr07 e63,tr07,satellite0,phenomenon8,star2).
target goal have image(tr07 e63,tr07,phenomenon8,spectrograph2).
target goal have image(tr07 e63,tr07,phenomenon7,spectrograph2).
target goal have image(tr07 e63,tr07,star5,image1).
% Static Predicates of problem
static fact calibration target(tr07,instrument0,star1).
static fact calibration target(tr07,instrument1,star1).
static fact supports(tr07,instrument0,image1).
static fact supports(tr07,instrument1,spectrograph2).
static fact supports(tr07,instrument1,image1).
static fact supports(tr07,instrument1,image4).
static fact on board(tr07,instrument0,satellite0).
static fact on board(tr07,instrument1,satellite0).

Figure 5: Knowledge base corresponding to the example tr07 e63 obtained by solving the training problem tr07 from the Satellite domain.
The result of this second learning step is a relational decision tree to for each uninstantiated
operator o ∈ O. to consists of the set of disjoint rules for the binding selection of o. Figure 7
shows an example of the binding tree tswitch on built for operator switch on from the Satellite
domain. According to this tree, the first branch states that when there is a helpful action which is
a switch on of instrument C in satellite D, these switch on bindings (C, D) were selected
by the planner 213 out of 249 times. Note that binding trees learned by ROLLER also advise on
non-helpful actions. Frequently, the selected o predicate matches with tree queries that refer to
helpful o predicates. In these cases, the no-branch of the query may cover bindings of non-helpful
actions for this operator.
For the other binding trees of the Satellite domain we refer the reader to the Online Appendix
of this article, where we include the learned DCK for the domains used in the experimental section.
777

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

% ---- The target concept ---predict(selected switch on(+IdExample,+IdProblem,+INST0,+SAT1,-Class)).
type(selected switch on(idex,idprob,instrument,satellite,class)).
classes([selected,rejected]).
% ---- The helpful context ----, the same as in the operator classification
...

Figure 6: Part of the language bias for learning the binding tree for the switch on action from
the Satellite domain.

selected switch on(-A,-B,-C,-D,-E)
helpful switch on(A,B,C,D) ?
+--yes: [selected] 249.0 [[selected:213.0,rejected:36.0]]
+--no: [rejected] 63.0 [[selected:2.0,rejected:61.0]]

Figure 7: Relational decision tree learned for the bindings selection of the switch on action from
the Satellite domain.

In many cases, decision trees are somewhat more complex that the one shown in Figure 7. For
instance, the turn to binding tree has 29 nodes and includes several queries about target goals (e.g.,
asking if there is a pending image at the new pointed direction) and others about static facts (e.g.,
asking if the new pointed direction is a calibration target).
3.3 Generation of Training Examples
ROLLER training examples are instances of decisions made when solving training problems. In order
to characterize a variety of good solutions, these decisions should consider different alternatives for
solving each individual problem. At a given search tree node (state), the alternatives come from the
possibility of choosing different operators or of having different bindings for a single operator, in
both cases assuming the alternative will lead to equally good solutions.

Regarding binding decisions, if actions from some alternative solutions are ignored, they are
tagged as rejected and consequently they introduce noise in the learning process. For instance,
consider the problem of Figure 8 from the Satellite domain in which a satellite, with a calibrated
instrument, must turn to directions D1,D2 and D3 in order to take images there. In this planning context, the three turn to actions are helpful actions and regarding only one solution makes
learning consider one action as selected and the other two actions as rejected. However, the learned
knowledge should always recommend a helpful turn to action towards a direction where the
satellite (with the corresponding calibrated instrument) has to take an image. To learn such kind of
knowledge, ROLLER should consider the three turn to actions as selected because the three actions correspond to selectable actions for learning the correct knowledge in this particular planning
778

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

context. If most of these actions are marked as rejected the learner will consider selecting turn to
in the described context as a bad choice.
take−image D2

S3

S4

turn−to(D2,D3)

S5

take−image D3

turn−to(D1,D2)
take−image D1
turn−to(D4,D1)

S1

turn−to(D4,D2)
S0

S1’

turn−to(D4,D3)
S1"

...

S2

turn−to(D1,D3)
S3’

...

...

Figure 8: Solution path alternatives in a simplified Satellite problem.
Regarding operator decisions, complete training with a full catalogue of different solutions can
confuse the learning process. For instance, consider the example problem of Figure 9 where the
goal is to take an image at direction D2. Before applying calibrate action in s2 , it is necessary
to switch on the instrument T and to turn the satellite to D1 (the calibration target direction). These
two actions are helpful in so and generate two different solution paths. In fact, they are commutative.
Generalizing operator selection from these kinds of helpful contexts is difficult when the training
examples contain examples of both types (i.e. examples where the switch-on action is situated
before the turn-to action and vice versa). This is caused by the fact that for the same helpful
context there are different operators to choose from and all of them are equally good choices.
S1
turn−to(D3,D1)

switch−on T

S2

S0

switch−on T

calibrate T

S3

turn−to(D1,D2)

S4

take−image D2

G

turn−to(D3,D1)
S1’

Figure 9: Solution path alternatives in a simplified Satellite problem.
ROLLER follows a commitment approach for the generation of training examples: (1) Generation of solutions. Given a training problem, ROLLER performs an exhaustive search to obtain
multiple best-cost solutions, taking into account the alternatives of different binding choices. (2)
Selection of solutions. ROLLER selects a subset of solutions from the set of best-cost solutions in
order to reproduce a particular preference for the operator alternatives. (3) Extraction of examples from solutions. ROLLER encodes the selected subset of solutions as examples for the required
learning, operator classification or binding classification. The following sections detail how ROLLER
proceeds at each of these three steps.

779

G

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

3.3.1 G ENERATION OF S OLUTIONS
ROLLER solves each training problem using a Best-First Branch and Bound (BFS-BnB) algorithm
that extracts multiple good-quality solutions. If the search space has not been explored exhaustively
within a time bound, the problem is discarded and no examples are generated from it. Therefore,
training problems need to be sufficiently small. In addition, training problems need to be representative enough to generalize the DCK which assists ROLLER when solving future problems in the
same domain.
The BFS-BnB search is completed without pruning repeated states. In practice, many repeated
states are generated by changing the order among actions of different solution paths. Thus, pruning
repeated states would involve tagging actions leading to these solutions as rejected bindings, which
is in fact not true. In addition, the BFS-BnB algorithm prunes according to the A∗ evaluation
function f (n) = g(n) + h(n), where g(n) is the node cost (in this work we use plan length as the
cost function) and h(n) is the FF heuristic. The safe way to prune the search space is by using an
admissible heuristic. However, existing admissible heuristics will not allow ROLLER to complete
an exhaustive search in problems of reasonable size. In practice, using the FF heuristic produces
few overestimations which introduces negligible noise into the learning process. At the end of the
search, the BFS-BnB algorithm returns the set of solutions with the best cost. These solutions are
used to tag the nodes in the search tree that belong to any of the solutions with the label on solution.

3.3.2 S ELECTING S OLUTIONS
From the set of best-cost solutions found, ROLLER selects the subset of solutions that will be used
for generating training examples. Since it is difficult to develop domain-independent criteria for systematically selecting solutions that reproduce the same operator selection in a particular context, we
have defined an approach which, heuristically, prefers some actions over others. These preferences
are:
• Least-commitment preference: Prefer actions that generate more alternatives of different solution paths.
• Difficulty preference: Prefer actions that reach the goals or sub-goals which are most difficult
to achieve. In the example of Figure 9 having instrument T switched on is only achievable by
one action. On the other hand, pointing to direction D1 is considered easier since it can be
achieved with actions turn to(D2,D1) and turn to(D3,D1).
Given π 0 = a1 , . . . , an , a best-cost plan for a planning task, we compute these preferences with
functions depending on each action.
ϕcommitment (ai ) =| {a0 | a0 ∈ successors(ai ) ∧ on solution(a0 )} |
where the function successors(ai ) returns all applicable actions in state si+1 and function on solution(a)
verifies whether an action is tagged as being part of a best-cost plan.
ϕdifficulty (ai ) =

min

1
| supporters(l) |

l∈add(ai )

where the function supporters(l) = {a ∈ A | l ∈ add(a)} returns the set of actions that achieve the
literal l.
780

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

Solutions are ranked according to these preferences. The ranking for each solution π 0 =
a1 , . . . , an is computed as the weighted sum of the action preferences, as follows:
ranking(π 0 , ϕ) =

X
i=0,...,n−1

(n − i)
× ϕ(ai+1 )
n

where n is the plan length and ϕ is one of ϕcommitment or ϕdifficulty . This sum is weighted to
give more importance to the preferences in the first actions of the plan. The first action preference
value is multiplied by 1, the second by (n − 1)/n, and so on. Otherwise, several alternatives (i.e.,
commutative actions in different positions within a plan) would lead to the same ranking value. We
compute the ranking for all best-cost solutions using ϕcommitment . Ties in this ranking are broken
with the ranking computed with ϕdifficulty . The subset of solutions with the best ranking values is
the subset of solutions selected for generating training examples.
3.3.3 E XTRACTING E XAMPLES FROM S OLUTIONS
takes the subset of solutions selected at the previous step and generates training examples. When generating examples for the operator classification, ROLLER takes solution plans π 0 =
{a1 , a2 , ..., an } which correspond to the sequence of state transitions {s0 , s1 , ..., sn } and generates
one learning example for each pair < si , ai+1 > consisting of H(si ) and the class (i.e., the operator
name of action ai+1 ). See learning example shown in Figure 2.
When generating examples for the binding classification of operator o, ROLLER only considers
pairs < si , ai+1 > where ai+1 matches operator o. A learning example generated from the pair
< si , ai+1 > for the binding selection of the operator o consists of H(si ) and the classes of all
applicable actions in si that match o, including ai+1 . Applicable actions with the on solution label
belong to the selected class and all other applicable actions to the rejected class. Moreover, actions
belonging to solutions not in the top ranking are still marked as selected even though they are not
nodes from which an example is generated. See learning example shown in Figure 5.
ROLLER

3.4 Use of the Learned Knowledge: Planning with Relational Decision Trees
This section details how to make heuristic planning benefit from our DCK, beginning with how
we build action orderings with the learned DCK. Then, it explains two different search strategies
to exploit these orderings: (1) the application of the DCK as a generalized action policy (DepthFirst H-Context Policy algorithm) and (2) the use of the DCK to generate lookahead states within a
Best-First Search (BFS) guided by the FF heuristic (H-Context Policy Lookahead-BFS algorithm).
3.4.1 O RDERING ACTIONS WITH R ELATIONAL D ECISION T REES
Given a state s, the expression app(s) denotes the set of actions applicable in s. The learned DCK
provides an ordering for app(s). The ordering is built by matching each action a ∈ app(s) first with
the operator classifier and then with the corresponding binding classifier. Figure 10 shows in detail
the algorithm for ordering applicable actions with relational decision trees.
The algorithm divides the set of applicable actions into two subsets: the helpful actions, and
the non-helpful actions. Then, it matches the helpful context of the state, i.e., H(s), with the tree
for the operator classification. This matching provides a leaf node that contains the list of operators
sorted by the number of examples covered by the leaf during the training phase (see the operator
781

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

DT-Filter-Sort (A,H,T):sorted list of applicable actions
A: List of actions
H: Helpful Context
T: Decision Trees

selected-actions = ∅
HA = helpful-actions(A, H)
NON-HA = A \ HA
leaf-node = classify-operators-tree(T, H)
for each a in HA do
priority(a) = leaf-node-operator-value(leaf-node, a)
if priority(a) > 0 then
(selected(a),rejected(a)) = classify-bindings-tree(T, H, a)
selected(a)
selection ratio(a)= selected(a)+rejected(a)
priority(a) = priority(a) + selection ratio(a)
selected-actions = selected-actions ∪{a}
max-HA-priority = maxa∈selected-actions priority(a)
for each a in NON-HA do
priority(a) = leaf-node-operator-value(leaf-node,a)
if priority(a) > max-HA-priority then
(selected(a),rejected(a)) = classify-bindings-tree(T, H, a)
selected(a)
selection ratio(a)= selected(a)+rejected(a)
priority(a) = priority(a) + selection ratio(a)
selected-actions = selected-actions ∪{a}
return sort(selected-actions, priority)

Figure 10: Algorithm for ordering actions using relational decision trees.
classification tree in Figure 4). The number of examples covered gives an operator ordering that
can be used to prefer actions during the search. The algorithm uses this number to initialize the
priority value for each helpful action, taking the value of the corresponding operator. The algorithm
keeps only helpful actions that have at least one matching example. For each of these actions, the
algorithm matches the action with its corresponding binding classification tree. The resulting leaf
of the binding tree returns two values: the number of times the ground action was selected, and the
number of times it was rejected in the training phase. We define the selection ratio for the ground
action as:
selected(a)
selection ratio(a) =
selected(a) + rejected(a)
This ratio represents the proportion of good bindings covered by a particular leaf of the binding
tree. When the denominator is zero, the selection ratio is assumed to be zero. The priority of the
action is updated by adding this selection ratio. Thus, the final priority for an action is higher for
782

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

actions with operators for which the operator classification tree provides higher values, i.e. they
have been selected more often in the training examples. Since the selection ratio remains between 0
and 1, adding up to this number can be considered as a method for breaking ties in the initial priority
value, using the information in the binding classification tree.
The priority for non-helpful actions is computed in a similar way except that, in this case, the
algorithm only considers actions whose initial priority (the value provided by the operator classification tree), is higher than the maximum priority of the helpful actions. In this manner, we capture
useful non-helpful actions. FF follows a heuristic criterion to classify an action as helpful. Although
this heuristic has been shown to be very useful, the case may arise in which the most useful action at
a particular moment is not classified as helpful. Decision trees capture this information, given that
they can recommend choosing a non-helpful action. The described method takes advantage of this
fact and defines a way of using such information when applying the learned knowledge. An alternative approach would be to extend the planning context with a new meta-predicate for non-helpful
actions. However, it does not pay off in a variety of problems and domains because it means significantly larger contexts, which causes more expensive matching. Finally, the selected actions are
sorted in order of decreasing priority values. The sorted list of actions is the output of the algorithm.
3.4.2 T HE H-C ONTEXT P OLICY A LGORITHM
The helpful context-action policy algorithm moves forward, applying at each state the best action
according to the DCK. The pseudo-code of the algorithm is shown in Figure 11. The algorithm
maintains an ordered open-list. The open-list contains the states to be expanded which are extracted
in order. Once extracted, each state is evaluated using the FF heuristic. Thus, we evaluate upon
extraction and not when nodes are included in the open-list. The evaluation provides the heuristic
value for the state, h, and the set of helpful actions HA, which are needed to generate the helpful
context. The heuristic value is only used for: (1) continuing the search when the state is a recognized
dead-end (h = ∞), and (2) goal checking (h = 0). Then, the helpful context is generated. Subsequently, the algorithm obtains the set AA of actions applicable in the state and sorts them using the
decision trees (as shown above in the algorithm in Figure 10). The result is AA0 ⊆ AA, a sorted list
of applicable actions. The algorithm inserts the successors generated by actions in AA0 at the beginning of the open-list preserving their ordering (function push-ordered-list-in-open).
Furthermore, to make the algorithm complete and more robust, successors generated by applicable
actions that are not in AA0 are included in a secondary list called delayed-list. The delayed list is
only used when the open-list is empty. In that case, only one node of the delayed-list is moved to
the open-list and then, the algorithm continues extracting nodes from the open-list.
In this algorithm, each node maintains a pointer to each parent in order to recover the solution
once it has been found. Also, each node maintains its g value, i.e. the length of the path from the
initial state up to the node. The function push-ordered-list-in-open only inserts in the
open list those candidates that: (1) are not repeated states, or (2) are repeated states with lower g
value than the previous one. Otherwise, repeated states are pruned. This type of pruning guarantees
that we maintain for each node the shortest solution found.
In other words, the proposed search algorithm is a depth-first search with delayed successors.
The benefit of this algorithm is that it exploits the best action selection when the policy is per783

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Depth-First H-Context Policy (I, G, T ): plan
I: Initial state
G: Goals
T : Decision Trees

open-list = {I};
delayed-list = ∅;
while open-list 6= ∅ do
n = pop(open-list)
(h, HA) = evaluate(n, G) /*compute FF heuristic*/
if h = ∞ then /*recognized dead-end*/
continue
if h = 0 then /*goal state*/
return path(I, n)
H = helpful-context(HA, G, n)
AA = applicable-actions(n)
AA’ = DT-Filter-Sort(AA, H, T )
candidates = generate-successors(n, AA’)
open-list = push-ordered-list-in-open(candidates,open-list)
delayed-candidates = generate-successors(n, AA \ AA’)
delayed-list = push-ordered-list(delayed-candidates, delayed-list)
while open-list = ∅ and delayed-list 6= ∅ do
open-list = { pop(delayed-list) }
return fail

Figure 11: A depth-first algorithm with a sorting strategy given by the DCK.
fect1 and the action ordering when is not. Particularly, perfect DCK will be directly applied in a
backtrack-free search and inaccurate DCK will force the search algorithm to backtrack.
3.4.3 T HE H-C ONTEXT P OLICY IN A L OOKAHEAD S TRATEGY
In many domains the learned DCK may contain flaws: the helpful context may not be expressive
enough to capture good decisions, the learning algorithm may not be able to generalize well or
the training examples may not be representative enough. In these cases, a direct application of the
learned DCK (without backtracking) may not allow the planner to reach the goals of the problem.
Poor quality in the learned DCK can be balanced with a guide of a different nature such as
a domain-independent heuristic. A successful example is the ObtuseWedge system (Yoon et al.,
2008) that combined a learned generalized policy with the FF heuristic. ObtuseWedge exploited
the learned policy to synthesize lookahead states within a lookahead strategy. Lookahead states
1. With perfect policy we refer to a policy that leads directly to a goal state. Our policies are not guaranteed to be perfect
given that they are generated by inductive learning.

784

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

were first applied in heuristic planning by the YAHSP planner (Vidal, 2004). They are intermediate
states that are frequently closer to a goal state than the direct descendants of the current state. These
intermediate states are added to the list of nodes to be expanded so that they can be used within
different search algorithms. When the learned policy contains flaws, lookahead states synthesized
with the policy may not provide good guidance for the search. However, if these lookahead states
are included in a complete search algorithm that also considers ordinary successors, the search
process becomes more robust. In general, the use of lookahead states in a forward state-space
search slightly increases the branching factor of the search process, but overall, as shown by the
YAHSP planner at IPC-2004 and in the experiments included in the YAHSP paper (Vidal, 2004), the
approach seems to improve the performance significantly.
Figure 12 shows a generic algorithm for using lookahead states generated from a policy during
the search. This algorithm is a weighted Best-First Search (BFS), with the only modification being that one or more lookahead states are inserted into the open list when expanding a node. As
in weighted BFS, nodes to be expanded are maintained in an open list ordered by the evaluation
function f (n) = ω × h(n) + g(n). Apart from the usual arguments of BFS, the algorithm receives
the policy (P ) and the horizon. The horizon represents the maximum number of policy steps that
are applied for generating the lookahead states. In the experiments, we will use this algorithm with
the FF heuristic as h(n).

H-Context Policy Lookahead BFS (I,G,T ,horizon): plan
I: Initial state
G: Goals
T : Decision Trees (policy)
horizon: horizon
open-list = ∅
add-to-open(I)
while open-list 6= ∅ do
n = pop(open-list)
if goal-state(n, G)
return path(I, n)
add-to-open-lookahead-successors(n, G, T, horizon)
add-to-open-standard-successors(n)
return fail

Figure 12: A Generic Lookahead BFS algorithm.
The heuristic evaluation, h(n), the g-value, g(n), and the set of helpful actions, are also saved
at each node when the node is evaluated. The function add-to-open(state) evaluates the
state and inserts it in the open-list, which is ordered in increasing values of the evaluation function,
f (n). This function also prunes repeated states, following the strategy described for the DepthFirst H-Context Policy algorithm: only repeated states with higher g(n) than the existent one are
785

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

pruned. The function add-to-open-standard-successors(n) calls add-to-open for
each successor of the node n. The function add-to-open-lookahead-successors is explained below.
We have adapted the generic Lookahead BFS algorithm to our learned DCK. Our particular
instantiation of the function add-to-open-lookahead-successors is shown in Figure 13.
In our case, lookahead states are generated by iteratively applying the first action in the action ordering provided by the DCK. The inputs to the algorithm are the current state, the problem goals, the
decision trees and the horizon. First, our algorithm generates the helpful context and the applicable
actions. The helpful actions, n.HA, are recovered from the node. Then, the algorithm sorts the applicable actions using the decision trees (as previously shown in the algorithm in Figure 10). After
that, the successor generated by the first action is inserted in the open list, and there is a recursive
call with this successor and the horizon decremented by one. The function add-to-open returns
true when its argument has been added to the open list and false otherwise. In fact, it returns
false in only two cases: (1) the state is a repeated state with g-value higher than the g-value of
the existent state2 or (2) the state is a recognized dead-end. When the ordered list becomes empty,
the lookahead state can not be generated and the initial node is returned. The same occurs when
the horizon is zero. The described implementation is similar to the lookahead strategy approach
followed by ObtuseWedge, but instead we perform the lookahead generation using helpful contexts
and relational decision trees.
On the other hand, in the described H-Context Policy Lookahead BFS algorithm the search is
perfomed over the set of applicable actions of each node. However, in many domains the use of
helpful actions has shown to be a very good heuristic. One possible way of prioritizing helpful
actions over non-helpful actions is to include in the open list only those successors given by helpful
actions, and to include the remaining successors in a secondary list. We have implemented this idea
following the same strategy used in the Depth-first H-Context Policy algorithm: when the open list
becomes empty only one node is passed from the secondary list to the open list, and the search
continues. The algorithm is still complete given that we do not prune any successor. When helpful
actions are good enough, this strategy can save many heuristic evaluations. In the experiments
we will compare this strategy with the previous one. Our intuition is that the adequacy of each
strategy depends directly on the quality of the helpful actions, the quality of the learned DCK, and
the accuracy of the heuristic for each particular domain.
Another technique for prioritizing helpful actions in BFS was implemented in YAHSP (Vidal,
2004) which inserts two consecutive instances of each node in the open list. These nodes have
equal f (n) since they represent the same state. The first one contains only the helpful actions, and
therefore, when expanded, it only generates successors resulting from these actions. The second
contains only non-helpful actions, called rescue actions. In this way, all the successors with lower
f (n) than the parent node in the sub-tree generated by helpful actions are expanded before any
successor resulting from non-helpful actions.
We have performed some preliminary experiments, obtaining similar results for the two described methods for prioritizing helpful actions in BFS: the use of a secondary list for non-helpful
actions, and the use of rescue nodes. For this reason, we only include results of the first technique
in the experimental section. We call this algorithm H-Context Policy Lookahead BFS-HA.
2. When the state is repeated but with a g-value smaller than the existent one, add-to-open does not re-evaluate but
instead takes the heuristic evaluation from the existent state.

786

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

add-to-open-lookahead-successors (n,G,T ,horizon) :state
n: Node (state)
G: Goals
T : Decision Trees (policy)
horizon: horizon
if horizon = 0 then
return n
H = helpful-context(n.HA, G, n)
AA = applicable-actions(n)
AA0 = DT-Filter-Sort(AA, H, T )
while AA0 6= ∅ do
a = pop(AA0 )
n0 = generate-successor(n, a)
added = add-to-open(n0 )
if added then
if goal-state(n0 , G)
return n0
return add-to-open-lookahead-successors(n0 , G, T , horizon − 1)
return n

Figure 13: Algorithm for generating lookahead states from decision trees.

4. Experimental Results
In this section we evaluate the performance of the ROLLER system. The evaluation is carried out
over a variety of domains belonging to diverse IPCs: Four domains come from the learning track of
IPC-2008 (Gold-miner, Matching Blocksworld, Parking and Thoughtful). The rest of the domains
(Blocksworld, Depots, Satellite, Rovers, Storage and TPP) were selected from among the domains
of the sequential tracks from IPC between 2000 and 2008 because they presented different structure
and difficulty, and because they have available random problem generators, so that we can automatically build training sets for learning DCK. For each domain, we complete a training phase in which
ROLLER learns the corresponding DCK and a testing phase in which we evaluate the scalability and
quality of the solutions found by ROLLER with the learned DCK. Next, we detail the experimental
results obtained at each of these two phases. Moreover, for each of the domains we give particular
details about training and test sets, the learned DCK and the observed ROLLER performance.
4.1 Training Phase
For each domain, we built a training set of thirty randomly generated problems. The size and
structure of these problems is further discussed in the particular details given for each domain. As
explained in section 3.2, ROLLER generates its training examples solving the problems from the
787

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

training set with a BFS-BnB search. We set a time-bound of 60 seconds to solve each training
problem, discarding those that are not exhaustively explored in this time-bound. Then, ROLLER
generates the training examples from the solutions found and builds the corresponding decision
trees with the TILDE system (Blockeel & De Raedt, 1998).
To evaluate the efficiency of ROLLER at the training phase we computed the following metrics:
the time needed for solving the training problems, the number of training examples generated in
this process, the time spent by TILDE in learning the decision trees and the number of leaves of the
operator selection tree. This last number gives a clue about the size of the learned DCK. Table 1
shows the results obtained for each domain.
Domain
Blocksworld
Depots
Gold-miner
Matching-BW
Parking
Rovers
Satellite
Storage
Thoughtful
TPP

Training
Time (s)
836.0
456.2
1156.9
865.8
105.8
528.3
19.8
136.3
883.4
995.9

Learning
Examples
2542
493
126
430
442
1011
1702
677
502
560

Learning
Time (s)
13.3
23.1
4.5
12.4
7.0
13.6
13.4
5.1
352.2
23.3

Tree
Leaves
18
13
5
23
12
24
4
6
19
6

Table 1: Experimental results of the training process. Training and learning times are shown, as
well as the number of training examples, and complexity of generated trees (number of
leaves).

achieves shorter Learning Times, fourth column in Table 1, than the state-of-the-art
systems for learning generalized policies (Martin & Geffner, 2004; Yoon et al., 2008). Particularly,
while these systems implement ad-hoc learning algorithms that sometimes require hours in order to
obtain good policies, our approach only needs seconds to learn the DCK for a given domain. This
fact makes our approach more suitable for architectures that need on-line planning and learning processes. However, these learning times are not constant for different domains, because they depend
on the number of training examples (in our work, this number is given by the amount of different
solutions for the training problems), on the size of the training examples (in our work this size is
given by the number and arity of predicates and actions in the planning domain) and how training
examples are structured, i.e., whether examples are easily separated by learning or not.
ROLLER

4.2 Testing Phase
In the testing phase ROLLER attempts to solve, for each domain, a set of thirty test problems. These
problems are taken from the evaluation set of the corresponding IPC. When this evaluation set
contains more problems, these thirty problems are the thirty hardest ones. The Depots domain is
an exception with twenty-two problems, because the evaluation set for this domain at IPC-2002
788

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

only contained those twenty-two problems. Three experiments are made for the testing phase. The
first one evaluates ROLLER’s performance when DCK is learned with all solutions of the training
problems or with the ranked solution approach. The second one evaluates the usefulness of the
learned DCK and the third one compares ROLLER with state-of-the-art planners. For each experiment we evaluate two different dimensions of the solutions found by ROLLER: the scalability and
the quality. All testing experiments were done using a 2.4 GHz processor with a time-bound of 900
seconds3 and 6Gb of memory-bound.
4.2.1 S OLUTION R ANKING E VALUATION
This experiment evaluates the effect of selecting solutions following the approach described in Section 3.3. The ROLLER configurations for this evaluation are:
• Top-Ranked Solutions: The Depth-First H-Context Policy algorithm using DCK learned
with the sub-set of the top ranked solutions. We use this search algorithm, since its performance depends more on the quality of the learned DCK than that of the other algorithms
using DCK.
• All Solutions: The Depth-First H-Context Policy algorithm using DCK learned with all solutions obtained by the BFS-BnB algorithm.
Table 2 shows the number of problems solved by each configuration, also with the time and plan
length average computed over the problems solved by both configurations. The number in brackets
in the first column is the number of problems solved in common. The Top-ranked solutions configuration solved thirty more problems than all solutions configuration, mainly due to the difference
of 21 problems in the Matching Blocksworld domain.
Domains
Blocksworld (30)
Depots (18)
Gold-miner (30)
Matching-BW (0)
Parking (30)
Rovers (27)
Satellite (28)
Storage (10)
Thoughtful (12)
TPP (30)
Total

Top-Ranked Solutions
Solved
Time Length
30
0,62
170,0
21
0,94
489,1
30
0,01
65,3
21
—
—
30
4,90
148,9
28
1,40
166,0
30
11,21
123,6
15
0,00
9,0
12
1,25
249,7
30
0,97
147,1
247
—
—

All Solutions
Solved
Time Length
30
2,39
550,7
18
0,97
607,3
30
0,01
65,3
0
—
—
30
2,20
56,2
29
31,20
355,8
28
11,47
121,6
10
0,00
9,0
12
1,28
249,2
30
0,90
132,8
217
—
—

Table 2: Problems solved and time and plan length average for the evaluation on ranking solution
heuristic.

The effect of selecting solutions varies across domains. For instance, it is quite important regarding the plan quality for Blocksworld, Depots and Rovers. In the Satellite domain the top-ranked
solutions allow ROLLER to solve two more problems while maintaining similar time and plan length
3. 900 seconds was the time-bound established at the learning track of IPC-2008.

789

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

average. In the Gold Miner domain, selecting solutions is irrelevant because there are few equally
good solutions per problem (i.e., the goal is always the single fact of “having the gold”) and fairly
most of them are top-ranked ones. The Parking domain does not benefit from selecting solutions.
Considering the overall results, we think that selecting solutions is a useful heuristic for improving
the DCK quality in many domains. In the remaining evaluations we will refer to DCK used by
ROLLER as the decision trees learned with the top-ranked solutions.
4.2.2 DCK U SEFULNESS E VALUATION
As shown in IPC Learning Track results, DCK may degrade the performance of the base planner,
when the DCK is incorrect. With this in mind, we designed this experiment to measure the performance of ROLLER algorithms comparing them with versions without DCK. We made two versions
for the non-learning algorithms. The first one is an empty configuration where there is no decision
tree given to the algorithm, thus no ordering is computed for the helpful actions, and the second one
is the systematic configuration, where the ordering is supplied by the FF heuristic instead.
The ROLLER configurations used for the comparisons are:
• ROLLER: The Depth-First H-Context Policy algorithm with the DCK learned at the training
phase.
• ROLLER-BFS: The H-Context Policy Lookahead BFS algorithm with the DCK learned at
the training phase. This configuration uses the horizon h = 100. We choose this value on the
basis of empirical evaluations.
• ROLLER-BFS-HA: A modified version of ROLLER - BFS where only helpful actions are considered as immediate successors. The lookahead states are generated as in the original version, using also the same horizon.
These three algorithms have their equivalent version for the empty configuration:
• DF-HA (Depth-first Helpful Actions): An empty DCK for ROLLER corresponds to a depthfirst algorithm over the helpful actions. As in the original algorithm, non-helpful actions are
placed in the delayed list.
• BFS: An empty DCK for ROLLER - BFS does not generate lookahead states (i.e., the algorithm add-to-open-lookahead-successors in Figure 12). Therefore, the algorithm becomes
the standard Best-first Search.
• BFS-HA: A modified version of BFS where only helpful actions are considered. Non-helpful
actions are placed in a delayed list.
Previous configurations also have a systematic version. In this case action ordering is computed
with the FF heuristic:
• GR-HA (Greedy Helpful Actions): This algorithm corresponds to a greedy search over the
helpful actions. For each node, helpful immediate successors are sorted with the FF heuristic.
Non-helpful nodes go to the delayed list.
• LH-BFS (Lookahead-BFS): A BFS with lookahead states. The function DT-Filter-Sort is
replaced by a function that computes the ordering using the FF heuristic.
790

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

• LH-BFS-HA: A modified version of LH - BFS where only helpful actions are considered. Nonhelpful actions are placed in a delayed list.
For the comparison, we computed the number of problems solved and the scores used in the
IPC-2008 learning track to evaluate planners performance in terms of CPU time and quality (plan
length). The time score is computed as follows: for each problem i the planner receives Ti∗ /Ti
points, where Ti∗ is the minimum time a participant used for solving the problem i, and Ti is the
CPU time used by the planner in question. In a 30 problem test set a planner can receive at most 30
points, the higher the score the better. The quality score is computed in the same way, just replacing
T with L, where L measures the quality in terms of plan length. In addition we compute the time
and quality averages for problems solved by all configurations. If a configuration did not solve
any problem, it is not taken into account for this measure. Average measures complement scores
since they give a direct information for commonly solved problems, while scores tend to benefit
configurations that solve problems which others do not.
Table 3 shows a summary for the results obtained in the DCK usefulness evaluation. For each
configuration we compute the number of domains where the algorithm was the top performer for
each of the evaluated criteria (i.e., numbers of solved problems, time and quality scores and averages). A top performer in a domain is an algorithm that has equal to or better measure than
the rest of the algorithms. In the table, each algorithm can have at most 10 points, the number of
evaluated domains. Global section refers to overall top performers. Relative section refers to the
number of domains where a configuration was equal or better than the other two configurations of
the same algorithm strategy (i.e., depth-first, best-first, best-first with helpful actions). All averages
of commonly solved problems were computed for configurations that solve more than one problem. Results show that ROLLER is very good in the number of solved problems and speed metrics.
Regarding quality score, ROLLER and ROLLER - BFS - HA were the best performers in three domains
each. However, BFS and BFS - HA obtained better results in quality average.
Global
Solved Problems
Time Score
Time Average
Quality Score
Quality Average
Relative
Solved Problems
Time Score
Time Average
Quality Score
Quality Average

DEPTH-FIRST
roller gr-ha df-ha
7
2
2
8
1
0
9
1
1
3
1
0
0
0
0
8
9
9
7
5

3
1
1
3
5

2
0
1
0
0

BEST-FIRST
roller-bfs lh-bfs
1
0
0
0
1
0
1
0
1
2
7
8
9
4
1

3
1
1
4
2

bfs
1
0
0
1
3
2
1
0
2
7

HELPFUL BEST-FIRST
roller-bfs-ha lh-bfs-ha bfs-ha
5
1
1
1
0
0
1
0
0
3
1
2
0
1
5
9
9
7
7
2

3
0
1
1
1

2
1
2
2
7

Table 3: Summary for DCK usefulness evaluation. Each column gives the number of domains
where each configuration was the top performer for a row item.

Table 4 shows the number of solved problems for the DCK usefulness evaluation. The Total
row shows that each ROLLER configuration solved more problems than the empty and systematic
versions. Results for time and quality scores are reported in Table 9 and Table 10 of Appendix A.
791

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Detailed results for averages were considered less interesting since in many domains there are very
few common solved problems, which are the easy problems.
Domains
Blocksworld (30)
Depots (22)
Gold-miner (30)
Matching-BW (30)
Parking (30)
Rovers (30)
Satellite (30)
Storage (30)
Thoughtful(30)
TPP (30)
Total

DEPTH-FIRST
roller gr-ha df-ha
30
1
0
21
18
18
30
0
0
21
0
0
30
25
1
28
30
30
30
23
22
15
9
10
12
15
0
30
30
30
247
151
111

BEST-FIRST
roller-bfs lh-bfs
bfs
8
0
0
20
19
13
17
17
16
14
7
14
30
11
7
26
28
11
25
22
15
19
18
20
20
14
11
16
24
9
195
160 116

HELPFUL BEST-FIRST
roller-bfs-ha lh-bfs-ha bfs-ha
8
0
0
20
20
20
30
0
0
19
10
17
30
11
9
30
30
30
30
23
23
19
10
10
23
16
12
19
26
14
228
146
135

Table 4: Problems solved for the DCK usefulness evaluation.

4.2.3 T IME P ERFORMANCE C OMPARISON
This experiment evaluates the scalability of the ROLLER system, compared to state-of-the-art planners. For the comparison, we have chosen LAMA (Richter & Westphal, 2010), the winner of the
sequential track of the past IPC, and FF, which in the last IPC has shown to be still competitive. We
used the three ROLLER configurations explained in the previous evaluation. The configuration for
other planners are:
• FF. Running the Enforced Hill-Climbing (EHC) algorithm with helpful actions together with
a complete BFS in case EHC fails 4 . Though this planner dates from 2001 we include it
in the evaluation because, as shown by the results of IPC-2008, it is still competitive with
other state-of-the-art planners. Besides, this planner is extensively used in other planning and
learning systems.
• LAMA-first. The winner of the classical track of IPC-2008. In this configuration LAMA
is modified to stop when it finds the first solution. In this way, comparison is fair because
the rest of configurations do not implement anytime behavior, i.e., the continuous solution
refinement until reaching the time-bound). The anytime behavior of LAMA is compared later
with the ROLLER performance in the next section.
Table 5 shows the number of problems solved together with the speed score. These results
give an overall view of the performance of the different planners. ROLLER solves as many or more
problems than any other configuration in 6 of 10 domains and achieves the top speed score in
seven domains. The second best score belongs to ROLLER - BFS - HA, which solves as many or more
problems than other planners in six domains. LAMA-first is fairly competitive, since it solves seven
problems less than ROLLER and 13 more problems than ROLLER - BFS - HA. In both cases LAMA-first
achieves a lower speed score.
4. This planner is actually Metric-FF running STRIPS domains. We consider this implementation an adequate baseline
for comparison because ROLLER was implemented over this code rather than over the original FF in order to extend
our approach to other planning models.

792

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

Domain (problems)
Blocksworld (30)
Depots (22)
Gold-miner (30)
Matching-BW (30)
Parking (30)
Rovers (30)
Satellite (30)
Storage (30)
Thoughtful(30)
TPP (30)
Total

ROLLER
solved
score
30
29.87
21
19.86
30
26.00
21
14.84
30
28.57
28
24.82
30
22.60
15
11.02
12
11.99
30
29.50
247
219.07

ROLLER-BFS
solved
score
8
2.47
20
11.01
17
0.03
14
1.32
30
22.72
26
13.41
25
14.61
19
12.31
20
12.38
16
14.83
195 105.09

ROLLER-BFS-HA
solved
score
8
2.40
20
11.46
30
5.35
19
1.71
30
23.60
30
16.24
30
18.33
19
16.17
23
13.09
19
13.97
228
122.32

FF
solved
0
20
27
9
24
29
22
17
14
26
188

score
0.00
8.70
0.22
0.27
0.94
5.51
5.31
10.51
8.16
6.27
45.89

LAMA-first
solved
score
17
0.17
20
3.88
29
12.24
25
20.02
23
1.69
30
18.59
28
15.66
19
9.03
20
11.29
30
9.66
241
102.23

Table 5: Problems solved and speed score of the five configurations.

Table 6 shows the average time for the five configurations when addressing the subset of problems solved by all configurations. The first column shows in parenthesis the number of commonly
solved problems. These results are closely related to those shown in Table 5. ROLLER achieves
the best average time in eight out of ten domains. We also observe that different configurations are
good in particular domains and even more so in particular problems. For instance, in the Thoughtful
domain there were only four problems solved by all the configurations.
Domain (problems)
Blocksworld (7)
Depots (18)
Gold-miner (17)
Matching-BW (6)
Parking (22)
Rovers (25)
Satellite (22)
Storage (14)
Thoughtful(4)
TPP (16)

ROLLER
0.36
0.84
0.00
1.99
1.86
1.37
1.24
11.74
1.49
0.02

ROLLER-BFS
66.31
15.53
49.82
42.25
2.91
24.38
7.08
0.01
10.84
0.02

ROLLER-BFS-r
67.99
2.54
0.02
44.53
2.78
9.83
1.87
0.03
9.52
0.02

FF
4.01
0.28
74.90
74.02
42.82
18.23
0.05
14.52
0.70

LAMA-first
139.69
61.73
0.01
1.96
108.22
1.59
1.33
0.19
3.55
0.10

Table 6: Planning time averages in the problems solved by all the configurations.

4.2.4 Q UALITY P ERFORMANCE C OMPARISON
This experiment compares the quality of the first solutions found and the solutions found by the
anytime behavior. In the anytime configuration, planners exhaust the time-bound trying to improve
incrementally the best solution found. Three ROLLER algorithms are modified to a configuration
where the best solution found so far is used as an upper-bound in order to prune all nodes that
exceed this plan length. The anytime behavior is the regular configuration for LAMA. FF does
not have anytime behavior, but it will be included in the anytime comparison as well as a base for
comparing quality improvements of other planners.
Table 7 shows the quality scores for the first solution and for the last solution found by the
anytime configurations. The anytime column for each planner shows the score variation and reveals
whether or not the planner was able to make relative improvements of the first solutions. The relative
793

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Domain
Blocksworld
Depots
Gold-miner
Matching-BW
Parking
Rovers
Satellite
Storage
Thoughtful
TPP
Total

ROLLER
first anytime
29.83
29.83
8.50
9.26
14.30
18.00
9.43
9.52
19.38
17.04
21.38
21.39
28.65
28.81
13.41
13.46
6.27
6.21
25.38
24.26
176.53
177.35

ROLLER-BFS
first anytime
8.00
8.00
12.39
17.01
11.50
17.00
13.01
12.43
24.24
23.98
21.78
21.59
23.20
23.00
15.69
18.38
15.93
15.12
14.45
15.09
160.19
171.65

ROLLER-BFS-HA
first
anytime
8.00
8.00
12.85
18.95
13.08
15.39
17.67
17.16
24.24
25.51
25.66
26.14
28.18
28.94
15.64
17.26
18.63
18.35
16.80
17.77
180.75
193.53

FF
first
0.00
19.01
27.00
8.23
21.53
28.66
21.55
16.23
13.96
23.42
179.59

relative
0.00
17.96
27.00
7.15
17.79
28.33
21.33
15.80
13.09
21.56
170.06

LAMA
first anytime
7.42
8.29
18.32
19.28
14.04
26.81
23.25
24.72
19.16
22.56
28.26
28.97
27.02
27.42
17.24
18.81
18.84
18.59
29.99
29.82
203.54
219.24

Table 7: Quality scores for the first solution and the anytime configuration of evaluated planners.

for FF shows the score of its solutions compared to the solutions given by the anytime configuration
of other planners. FF loses points in most cases because the others were able to improve their
solutions. The two LAMA configurations obtained the top score in their category. Nevertheless, no
planner dominated in all the domains. Furthermore, all configurations achieved the top quality score
for the first solution in at least one domain.
Domain
Blocksworld (7)
Depots (18)
Gold-miner (17)
Matching-BW (6)
Parking (22)
Rovers (25)
Satellite (22)
Storage (14)
Thoughtful(4)
TPP (16)

ROLLER
first
anytime
146.29
146.29
385.78
372.22
55.65
38.18
186.00
170.33
96.91
93.82
150.80
149.96
78.41
77.59
43.07
42.64
292.25
291.75
60.25
57.00

ROLLER-BFS
first
anytime
142.86
142.86
81.78
54.00
30.06
19.65
75.00
70.00
75.32
59.86
115.56
115.56
80.05
80.00
15.21
11.36
168.50
168.25
60.00
52.06

ROLLER-BFS-HA
first
anytime
142.86
142.86
76.83
43.33
47.88
39.35
76.00
69.33
75.32
54.45
114.40
112.12
80.05
77.32
15.64
13.29
168.50
164.50
60.25
49.38

FF
first
–
46.39
19.65
71.67
60.00
94.20
77.18
12.43
123.25
59.19

relative
–
46.39
19.65
71.67
60.00
94.20
77.18
12.43
123.25
59.19

LAMA
first
anytime
358.57
318.00
49.28
41.56
43.35
19.65
78.33
62.33
64.14
47.91
101.44
98.36
76.91
75.50
12.71
11.29
140.25
128.50
51.81
47.94

Table 8: Quality averages for the first solution and the anytime configuration of evaluated planners.

Table 8 shows the plan length average for the problems solved by all configurations. The first
column shows the average for the first solutions and the anytime column gives the average for
the last solutions of the anytime configuration. The commonly solved problems are the same as
those reported in Table 6. Although FF is the planner that solved fewer problems, it achieves the
best average plan length in seven domains. Plan length averages reveal that ROLLER is not able
to find first solutions of good quality for most domains. ROLLER - BFS and ROLLER - BFS - HA find
better quality solutions than ROLLER, and in several domains, their averages are competitive with
LAMA . ROLLER - BFS and ROLLER - BFS - HA show a better quality performance mainly due to the
combination of learned DCK with a domain-independent heuristic within the BFS algorithm.
In the following subsections we discuss particular details for each of the domains. We give a
very brief description of the domain together with information about training and test sets used in
the experimental evaluation. For each domain, we also analyze the learned DCK and the obtained
794

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

results in order to give a fine-grained interpretation of the observed performance. Further details on
these domains can be found at the IPC web site.5
4.2.5 B LOCKSWORLD D ETAILS
Problems in this domain are concerned with configuring towers of blocks using a robotic arm.
The training set used for the experiments consisted of: ten eight-block problems, ten nine-block
problems and ten ten-block problems. The test set consisted of the 30 largest typed problems from
IPC-2000, which have from 36 to 50 blocks.
blocksworld domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
LAMA-first

Percentage solved

80

60

40

20

0
0.1

1

10
CPU Time

100

1000

Figure 14: Percentage of solved problems when increasing time for evaluating the scalability performance in the Blocksworld domain.
Although this domain is one of the oldest benchmarks in automated planning, it is still challenging for state-of-the-art heuristic planners. Blocksworld presents strong interaction among goals
that current heuristics fail to capture. In particular, achieving a goal in this domain may undo
previously satisfied goals. Therefore, it is crucial to achieve goals in a specific order. The DCK
learned by ROLLER gives a total order of the domain actions in different contexts capturing this key
knowledge, which lets ROLLER achieve impressive scalability results while producing good quality
solution plans. ROLLER configurations are considerably better than non-learning configurations.
Particularly, ROLLER solved the thirty problems of the set while DF - HA and GR - HA did not solve
any problem. ROLLER is also quite good when compared to state-of-the-art planners. In Figure 14
we can observe that ROLLER performs two orders of magnitude faster than LAMA. The x-axis of
the figure represents the CPU time in logarithmic scale and the y-axis represents the percentage of
solved problems in a particular time. Moreover, ROLLER obtained the best quality score in the first
solution and anytime evaluations. In addition, the average plan length of common problems is fairly
close to the best average, obtained by ROLLER - BFS and ROLLER - BFS - HA. BFS algorithms do not
5. http://idm-lab.org/wiki/icaps/index.php/Main/Competitions

795

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

scale well in this domain because they are partially guided by the FF heuristic, which considerably
underestimates the distance to the goals. Similarly, lookahead states generated by the policy are
discarded because they fail to escape plateaus generated by this heuristic function.
When analyzing the learned operator tree we found explanations for the good performance of
ROLLER in the Blocksworld domain: The operator tree is clearly split in two parts. The first part contains decisions to take when the arm is holding a block. In this situation, the tree captures when to
STACK or PUT-DOWN a block. The second part contains decisions to take when the arm is empty. In
this case the tree captures when to UNSTACK or PICK-UP a block. In this second part of the tree, if
the current state of the search matches the logical query helpful unstack(Block1,Block2)6
it means that the tower of blocks under Block1 is not well arranged, i.e., Block1 or at least
one block beneath Block1 is not well placed. Therefore, the set of helpful actions compactly
encodes the useful concept of a bad tower. This kind of knowledge was manually defined in
previous works in order to learn good policies for Blocksworld. One approach consisted of including recursive definitions of new predicates, such as the support predicates above(X,Y) and
inplace(X) (Khardon, 1999). Another alternative involved changing the representation language, for instance the concept language (Martin & Geffner, 2004) or the taxonomic syntax (Yoon,
Fern, & Givan, 2007). The Kleene-star operator of the taxonomic syntax (i.e., the operator for defining recursion) was discarded in a subsequent work (Yoon et al., 2008) and the above predicate was
used instead. ROLLER’s ability to recognize bad-towers without extra predicates arises because any
misplaced block in a tower makes the UNSTACK action of the top block helpful, since it is always
part of the relaxed plan when the arm is empty.
Due to the extraordinary performance of ROLLER in this domain, we built an extra test set
to clarify whether or not the trend observed in the ROLLER configuration would hold for larger
problems. With this aim, we randomly generated 30 problems distributed in sub-sets of 50, 60, 70,
80, 90 and 100 blocks with 5 problems for each sub-set. ROLLER solved the 30 problems in this
extra test set with a time average of 20.1 seconds per problem and spending at most 175.3 to solve a
problem. Obviously, problems became more difficult for ROLLER as the number of blocks increase.
4.2.6 D EPOTS D ETAILS
This domain is a combination of a transportation domain and the Blocksworld domain, where there
are crates instead of blocks and hoists instead of the robot arm. The problems consist of trucks
transporting crates around depots and distributors. Using hoists, crates can be stacked onto pallets
or on top of other crates at their final destination. In this domain, the 30 training problems are
different combinations of 2 or 3 locations (depots and distributors), 1 or 2 trucks, 1 or 2 pallets per
location, 1 hoist per location and from 2 to 5 crates to be placed in different configurations. For
the testing phase we have used the 22 problems of the IPC-2002 set. The hardest problem has 12
locations (1 or 2 pallets and 1 or 2 hoists), 6 trucks and 20 crates.
ROLLER and ROLLER - BFS improve the performance of the non-learning strategies, but the three
configurations of BFS Helpful-Action solved the same 20 problems. ROLLER is able to solve 21
problems, achieving the best speed score. However, the high average plan length indicates that the
policy is not producing good quality plans. ROLLER - BFS - HA obtains the second best speed score
with more competitive plan lengths. Figure 15 shows the percentage of solved problems while
6. As explained in section 3.2 logic queries in ROLLER present the example and problem Ids. In this case these Ids are
ignored for simplicity given that they are not needed for matching the current helpful context.

796

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

increasing the CPU time (in logarithmic scale). In the anytime configuration, ROLLER - BFS - HA is
able to refine its solutions, achieving a quality average similar to LAMA.
depots domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
FF
LAMA-first

Percentage solved

80

60

40

20

0
0.01

0.1

1

10

100

1000

CPU Time

Figure 15: Percentage of solved problem when increasing time for evaluating the scalability performance in the Depots domain.

The DCK learned in this domain provides inaccurate advice for large planning contexts. For
instance, ROLLER makes some mistakes when deciding which crate to unload when several crates
are loaded in a truck. The reason for the inaccurate DCK is that training problems are not large
enough to gain this knowledge. In addition, adding more crates to these problems makes it unfeasible for them to be solved with BFS-BnB. Nevertheless, this limitation of the learned DCK is
not very evident. The Depots domain is undirected (i.e., all actions are reversible), so it has no
dead ends. Therefore, mistakes made by the DCK are fixed with additional actions, which leads
to worse quality plans. Besides, since first solutions are rapidly found, ROLLER configurations can
spend time refining solutions. This is the reason for the great improvement in the plan average for
ROLLER - BFS - HA .
4.2.7 G OLD -M INER D ETAILS
The objective of this domain is to navigate in a grid of cells until reaching a cell containing gold.
Some of the cells are occupied by rocks that can be cleared using bombs or a laser. In this domain
the training set consists of: 10 problems with 3 × 3 cells, 10 problems with 4 × 4 cells, and 10
problems with 5 × 5 cells. This domain was part of the learning track in IPC-2008 so we have used
the same test set used in the competition. This set has problems ranging from 5 × 5 up to 7 × 7 cells.
Problems in the Gold-Miner domain are not solvable with helpful actions alone. This explains
the difference in the number of solved problems between ROLLER, ROLLER - BFS - HA and their nonlearning counterpart. In general terms, this domain is trivial for ROLLER, ROLLER - BFS - HA (they
solved all the test problems in less than 10 seconds per problem) and LAMA. Nevertheless, FF scales797

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

up poorly. In this domain essential actions for picking up bombs are frequently not considered
helpful actions, because the relaxed problem is solvable using the laser. Consequently, FF fails
to solve most problems with EHC and it requires an additional BFS search. Figure 16 shows the
percentage of solved problems while increasing the CPU time. Regarding the anytime evaluation,
all tested configurations improved the first solution found in many problems.
gold-miner domain
100

Percentage solved

80

60

40

20
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
FF
LAMA-first
0
0.01

0.1

1

10

100

1000

CPU Time

Figure 16: Percentage of solved problems when increasing time for evaluating the scalability performance in the Gold-miner domain.
In this domain, the operator tree succeeds in capturing the key knowledge. In the initial states,
the bombs and the laser are in the same cell, so the robot needs to decide which of them to pick-up.
The operator tree for this domain matches the logical query candidate pickup laser(Cell)
with a higher ratio for operator PICKUP-BOMB than for operator PICKUP-LASER. This operator
preference allows ROLLER to avoid dead ends when the laser destroys the gold. On the other hand,
situations where the laser is required (i.e., to destroy hard rocks) are reached as a second choice
of the policy. This fact implies some backtracking for ROLLER, but the additional evaluated nodes
do not significantly affect the overall performance. The preference of the PICKUP-BOMB over the
PICKUP-LASER action is an example of selecting non-helpful actions.
4.2.8 M ATCHING B LOCKSWORLD D ETAILS
This domain is a version of Blocksworld designed to analyze limitations of the relaxed plan heuristic.
In this version blocks are polarized, either positive or negative. There are also two polarized robot
arms. Furthermore, when a block is placed (stack or put-down actions) with an arm of different
polarity, the block becomes damaged and no block can be placed on top of it. However, picking up
or unstacking a block with the wrong polarity seems to be harmless. This fact makes recognizing
dead ends a difficult task for the FF heuristic. Particularly, in the relaxed task blocks are never
damaged. Thus, both the relaxed plan (and consequently the set of helpful actions) and the heuristic
estimation are wrong. The training set used in this domain consists of fifteen 6-blocks problems and
798

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

fifteen 8-blocks problems. We used an even number of blocks to keep the problems balanced (i.e.,
half of the blocks of each polarity). For the testing phase we used the test set from the learning track
of IPC-2008. This set has problems ranging from 15 to 25 blocks.
DF - HA and GR - HA did not solve any problem, because these problems are not solvable with
helpful actions alone. The learned DCK recommended some useful non-helpful actions, thus
ROLLER was able to solve 21 problems. Policy configurations perform better than systematic strategies, but are fairly similar to not using a lookahead strategy. This fact reveals that the learned DCK
is not effective enough to pay off the effort of building lookahead states. LAMA is the planner that
solves the most problems. Figure 17 shows the percentage of solved problems while increasing the
CPU time.
matching-bw domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
FF
LAMA-first

Percentage solved

80

60

40

20

0
0.1

1

10
CPU Time

100

1000

Figure 17: Percentage of solved problems when increasing time for evaluating the scalability performance in the Matching Blocksworld domain.

ROLLER solved problems by evaluating a considerable number of nodes above the plan length,
which means that the DCK learned for this domain is not accurate. When analyzing the training
examples we find many solution plans that do not satisfy the key knowledge of the domain (robot
arms should unstack or pick-up blocks of the same polarity). Specifically, when the robot is handling
a top block, i.e., a block with no other blocks above it in the goal state, then the polarity of the robot
arm becomes meaningless. This effect is unavoidable because the shortest plans involve managing
top blocks in a more efficient way while ignoring the polarities. These examples include noise in
the learning and make generalization very complex.

4.2.9 PARKING D ETAILS
This domain involves parking cars on a street with N curb locations where cars can be double
parked, but not triple parked. The goal is to move from one configuration of parked cars to another
by driving cars from one curb location to another. In this domain the training set consists of: fifteen
799

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

problems with six cars and four curbs and fifteen problems with eight cars and five curbs. For testing
we used the test set from the learning track of IPC-2008. The hardest problem in this set has 38 cars
and 20 curbs.
The three ROLLER configurations solve all problems and perform significantly better than nonlearning strategies. In addition, the three ROLLER configurations outperform FF and LAMA with a
difference of more than one order of magnitude. This is the reason for LAMA and FF low speed
scores. ROLLER configurations are also consistently better than systematic and empty configurations. Figure 18 shows the percentage of solved problems while increasing the CPU time. On
the other hand, the three ROLLER configurations did not achieve first solutions of suficient quality. However, these solutions are refined in the anytime evaluation, especially by ROLLER - BFS - HA,
which achieves the top quality score and has a plan length average fairly similar to LAMA.
parking domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
FF
LAMA-first

Percentage solved

80

60

40

20

0
0.1

1

10
CPU Time

100

1000

Figure 18: Percentage of solved problems when increasing time for evaluating the scalability performance in the Parking domain.

The learned DCK in this domain was quite effective (ROLLER rarely backtracked). The operator tree perfectly classifies the MOVE-CAR-TO-CURB action at the first tree node, asking if it is
considered a helpful action. Besides, the binding tree for this operator selects the right car by asking
about the target goal and rejecting other candidates. These two decisions guide the planner to place
a car in the right position whenever possible. As a result, a large number of nodes are not evaluated,
which explains the scalability difference with FF and LAMA.
4.2.10 ROVERS D ETAILS
This domain is a simplification of the tasks performed by the autonomous exploration vehicles sent
to Mars. The tasks consist of navigating the rovers, collecting soils and rocks samples, and taking
images of different objectives. In this domain the training set consists of: ten problems with one
rover, four waypoints, two objectives and one camera; ten problems with an additional camera; and
800

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

ten problems with an additional rover. Problems in the test set are the thirty largest problems from
the IPC-2006 set (i.e., problems 11 to 40). The largest problem in this set has 14 rovers and 100
waypoints.
DCK strategies are faster than systematic and empty strategies, but differences are not significant since all configurations solved most of the problems. On the one hand helpful actions in the
Rovers domain are quite good but on the other hand the test set does not have problems which are
big enough to generate differences among approaches. Regarding planner comparison, ROLLER
achieves the top performance score and scales significantly better than FF, and solves two problems
less than LAMA. Figure 19 shows the percentage of solved problems when increasing the CPU time.
Regarding the anytime evaluation, all planners are able to refine their first solutions. LAMA gets the
top quality score and the best plan length after refining solutions.
rovers domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
FF
LAMA-first

Percentage solved

80

60

40

20

0
0.01

0.1

1

10

100

1000

CPU Time

Figure 19: Percentage of solved problems when increasing time for evaluating the scalability performance in the Rovers domain.

In this domain, ROLLER learned imperfect DCK, but it manages to achieve good scalability
results. DCK is imperfect partially because actions for communicating rock, soil or image analysis
can be applied in any order among them. Therefore, the preferences for ranking and selecting
solutions fail to discriminate among these actions which confuse the learning algorithm. Since
these actions could be applied in any order, the DCK mistakes seem to be harmless at planning
time.
4.2.11 S ATELLITE D ETAILS
This domain comprises a set of satellites with different instruments, which can operate in different
formats (modes). The tasks consist of managing the instruments for taking images of certain targets
in particular modes. In this domain the training set consist of thirty problems with one satellite, two
instruments, five modes and five observations. Problems in the test set are the thirty largest problems
801

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

from the IPC-2004 (i.e., problems 7 to 36). The largest problem in this set has 10 satellites, 5 modes
and 174 observations.
The three ROLLER configurations improved the number of solved problems of their non-learning
counterpart. In addition, ROLLER and ROLLER - BFS - HA solved the 30 problems in the set, two more
than LAMA and eight more than FF. Figure 20 shows the percentage of solved problems when
increasing the CPU time. ROLLER and ROLLER - BFS - HA achieve good quality solutions and are
able to refine them in the anytime evaluation, achieving plan lengths similar to LAMA.
satellite domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
FF
LAMA-first

Percentage solved

80

60

40

20

0
0.01

0.1

1

10

100

1000

CPU Time

Figure 20: Percentage of solved problems when increasing time for evaluating the scalability performance in the Satellite domain.

The learned DCK captures the key knowledge of the Satellite domain. The trees shown in
Figure 4 and Figure 7 are part of the learned DCK with fewer training examples. In this domain
both ROLLER and ROLLER - BFS - HA perform quite similarly. The reason is that the FF heuristic is
also quite accurate in the domain. Thus, the deepest lookahead state generated by the learned policy
is frequently selected by the heuristic in the BFS search.
4.2.12 S TORAGE D ETAILS
This domain is concerned with the storage of a set of crates taking into account the spatial configuration of a depot. The domain tasks comprise using hoists to move crates from containers to a
particular area in the depot. The training set consists of 30 problems with 1 depot, 1 container, 1
hoist and different combinations of 2 or 3 crates and from 2 up to 6 areas inside the depot. For the
test set we used the 30 problems from the IPC-2006 set. The largest problem in this domain has 4
depots with 8 areas each, 5 hoist and 20 crates.
The first 12 problems are trivially solved by all configurations. Then, problem difficulty increases quickly when the number of problem objects increases. The BFS solved 20 problems, one
more than any DCK strategy, meaning that DCK lookahead strategies do not pay off. This domain
802

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

is also hard for FF and LAMA. Figure 21 shows the percentage of solved problems when increasing
the CPU time.
storage domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
FF
LAMA-first

Percentage solved

80

60

40

20

0
0.01

0.1

1

10

100

1000

CPU Time

Figure 21: Percentage of solved problems when increasing time for evaluating the scalability performance in the Storage domain.

Although DCK is not effective, we found interesting properties in it. The learned operator tree
is compact and succeeds in selecting the GO-IN action which is normally not marked as a helpful
action.
4.2.13 T HOUGHTFUL D ETAILS
This domain models a version of the solitaire card game, where all cards are visible and one can
turn each card from the talon rather than 3 cards at a time. As in the original version, the goal of
the game is to place all cards in ascending order in their corresponding suit stacks (home deck).
There is no available random problem generator for this domain. Therefore, we used the bootstrap
problem distribution given in the learning track of IPC-2008. This set contains problems for the
four suits, having up to card seven for each suit. For the test phase we used the 30 problems from
the test distribution of the learning of IPC-2008. The largest problem in this domain has the full set
of a standard card game.
ROLLER only solves 12 problems, three fewer than GR - HA . However, ROLLER - BFS and ROLLER BFS - HA are better in the number of solved problems than non-learning approaches. In this domain,
the use of DCK for lookahead construction combined with the FF heuristic makes the search process
more robust against policy mistakes. ROLLER - BFS - HA solves 23, three more than LAMA. Figure 22
shows the percentage of solved problems when increasing the CPU time.
The BFS-BnB algorithm for generating training examples is only able to solve 12 out of 30
problems from the bootstrap problem distribution. We believe that a different bootstrap distribution with smaller problems would generate more accurate DCK. Additionally, even though DCK
803

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

thoughtful domain
100
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
FF
LAMA-first

Percentage solved

80

60

40

20

0
0.1

1

10
CPU Time

100

1000

Figure 22: Percentage of solved problems when increasing time for evaluating the scalability performance in the Thoughtful domain.

lookahead strategies achieve good results, learning accurate decision trees is more complex when
there are many more classes (20 operators in this particular domain) and many more arguments in
the predicates of the background knowledge (up to 6 parameters in operator col-to-home and 7
parameters in operator col-to-home-b).
4.2.14 TPP D ETAILS
TPP stands for Traveling Purchase Problem, which is a generalization of the Traveling Salesman
Problem. Tasks in the domain consist in selecting a subset of markets to satisfy the demand for
a set of goods. The selection of markets should try to optimize the routing and the purchasing
costs of the goods. In the STRIPS version, the graph that connects markets has equal costs for all
arcs. Nevertheless, the domain is still interesting because it is difficult for planners to scale when
increasing the number of goods, markets and trucks. The training set consists of thirty problems
with a number of goods, trucks and depots varying from one to three and with load levels of five
and six. The test set consists of the thirty problems used for planner evaluation at IPC-2006. The
largest problem in this set has 20 goods, 8 trucks, 8 markets with a load level of six.
ROLLER , GR - HA and DF - HA solved the 30 problems in the test set, but ROLLER performs faster
than the other two, achieving similar plan lengths. Besides, ROLLER outperforms the rest of the
planners and it is two orders of magnitude faster than FF. The main reason is the overwhelming
branching factor of the large problems together with the fact that FF heuristic falls into big plateaus
in this domain. Greedy (depth-first) approaches perform better because they avoid the effect of these
plateaus. Additionally, ROLLER achieved competitive quality scores and average plan length in the
first solution and the anytime evaluation. ROLLER - BFS and ROLLER - BFS - HA got very bad results

804

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

for this domain because of the imprecision of the FF heuristic. Figure 23 shows the percentage of
solved problems while increasing the CPU time.
tpp domain
100

Percentage solved

80

60

40

20
ROLLER
ROLLER-BFS
ROLLER-BFS-HA
FF
LAMA-first
0
0.01

0.1

1

10

100

1000

CPU Time

Figure 23: Percentage of solved problems when increasing time for evaluating the scalability performance in the TPP domain.
The learned DCK is compact and useful for reducing the number of evaluations, as shown by
ROLLER performance. For instance, the DRIVE binding tree recognizes perfectly when a truck in
market A does not need to go to a market B because there is already a truck in B handling the goods
of that market. In these situations, the state with the truck in B has a helpful action DRIVE, meaning
the truck B has something to deliver.

5. Lessons Learned from the IPC
IPC-2008 included a specific track for planning systems that benefit from learning. Thirteen systems
took part in this track including a previous version of ROLLER (De la Rosa et al., 2009) that achieved
the 7th position. This version was an upgrade of the original ROLLER system (De la Rosa et al.,
2008). The first version proposed the EHC-Sorted algorithm as an alternative to the H-Context Policy, but it was not effective in many domains. The competing version tried to recommend ordering
for applying actions from the relaxed plans. This idea, although initially appealing, was not a good
choice because its usefulness strongly depends on the fact that the relaxed plan contains the right actions. After the competition we completed an analysis of the ROLLER performance to diagnose and
strengthen its weak points. The system resulting from these improvements is the ROLLER version
described in this article. One example of the ROLLER improvements is the results obtained at the
Thoughtful and Matching Blocksworld domains. At IPC-2008, ROLLER failed to solve all the problems from the Thoughtful domain and it only solved two problems from the Matching Blocksworld.
As reported in section 4, the current version of ROLLER solves 23 and 19 problems respectively in
these domains. In addition, the current version of ROLLER outperforms LAMA and FF in the Park805

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

ing domain by one order of magnitude. The improvements of ROLLER overcome limitations of the
version submitted to IPC-2008 in three aspects:
• Robustness to wrong DCK. Issues discussed in Section 2 are all decisions that introduce biases in the learning process making learning of DCK a complex task. In fact, no competitor at
IPC-2008 was able to learn useful DCK for all the domains. Furthermore, in many domains
the learned DCK damaged the performance of the baseline planner. This was the case of
ROLLER . As we described in the paper, we have strengthened ROLLER against wrong DCK
by proposing two versions of a modified BFS algorithm that combine the learned DCK with
a numerical heuristic. The combination of DCK and the heuristic makes the planning process more robust to imperfect and/or incorrectly learned knowledge. A similar approach was
followed by the winner of the best learner award, O BTUSE W EDGE (Yoon et al., 2008).
• Efficiency of the baseline. The overall competition winner was P BP (Gerevini, Saetti, & Vallati, 2009) a portfolio of state-of-the-art planners that learns which planner and settings are
the best ones for a given planning domain. As a result, the performance of this competitor
was never worse than the performance of a state-of-the-art planner. At IPC-2008 the baseline performance of ROLLER was far from being competitive with state-of-the art planners
because ROLLER algorithms were coded in LISP. To overcome this weakness we optimized
the implementation of ROLLER using C code that outperformed our IPC-2008 results in all
domains.
• Definition of significant training sets. Training examples are extracted from the experience
collected while solving problems of a training set. Therefore, the quality of the training
examples depends on the quality of the problems used for training. At IPC-2008 the training
problems were fixed by the organizers and, in many domains, they were too large for the
ROLLER system to extract useful DCK. In this paper we have created training problems using
random generators to build useful training sets for the ROLLER system for each domain.
• Selection of training examples. Relational classifiers induce a set of rules/trees that model
regularities in the training data. For the case of forward state-space search planning not all
best-cost solutions for a problem may be used as training data, because this leads to alternatives that will confuse the learner. To avoid this, training data should be cleaned before being
used by the learning algorithm. The ranking and solution selection proposed in this article is
an option to give the learner training data with clearer regularities.
Additionally, ROLLER performed poorly in the Sokoban and N-puzzle domains. Traditionally,
useful DCK for these domains has the form of numeric functions, such as the Manhattan distance,
which provides a lower-bound for the solution length. In general, action policies are inaccurate in
these domains, because they lack knowledge about the trajectory to the goals. Currently, we are still
unable to learn useful DCK for ROLLER in these domains. A possible future direction is to introduce
not only goals but subgoals (e.g. landmarks) in the helpful context with the aim of capturing some
of this knowledge.

6. Related Work
Our approach is strongly inspired by the way Prodigy (Veloso et al., 1995) models DCK. In the
Prodigy architecture, the action selection is a two-step process: first, Prodigy selects the uninstan806

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

tiated operator to apply, and second, it selects the bindings for that operator. Both selections can
be guided by DCK in the form of control rules (Leckie & Zukerman, 1998; Minton, 1990). We
have returned to this idea of the two-step action selection because it allows us to define the learning
of planning DCK as a standard classification task and therefore to solve this learning task with an
off-the-shelf classification technique such as relational decision trees. Nevertheless, ROLLER does
not need to distinguish among different kinds of nodes as Prodigy does, because ROLLER performs
a standard forward heuristic search in the state space where all the search nodes are of the same
type.
Relational decision trees have been previously used to learn action policies in the context of
Relational Reinforcement Learning (RRL) (Dzeroski, De Raedt, & Blockeel, 1998). In comparison
with the DCK learned by ROLLER, RRL action policies present two limitations when solving planning problems. First, in RRL the learned knowledge is targeted to a given set of goals, therefore
RRL cannot directly generalize the learned knowledge for different goals within a given domain.
Second, since training examples in RRL consist of explicit representations of the states, RRL needs
to add extra background knowledge to learn effective policies in domains with recursive predicates
such as Blocksworld.
Previous works on learning generalized policies (Martin & Geffner, 2004; Yoon et al., 2008)
succeed in addressing these two limitations of RRL. First, they introduce planning goals in the
training examples. In this way the learned policy applies for any set of goals in the domain. Second,
they change the representation language of the DCK from predicate logic to concept language. This
language makes capturing decisions related to recursive concepts easier. Alternatively, ROLLER
captures effective DCK in domains like Blocksworld without varying the representation language.
ROLLER implicitly encodes states in terms of the set of helpful actions of the state. As a result,
ROLLER can benefit directly from off-the-shelf relational classifiers that work in predicate logic.
This fact makes learning times shorter and the resulting policies easier to read.
Recently, other techniques have also been developed to improve the performance of heuristic
planners:
• Learning Macro-actions (Botea, Enzenberger, Müller, & Schaeffer, 2005; Coles & Smith,
2007) are the combination of two or more operators that are considered as new domain operators in order to reduce the search tree depth. However, this benefit decreases with the number
of new macro-actions added because they enlarge the branching factor of the search tree causing the utility problem (Minton, 1990). Other approaches overcome this problem, applying
filters that decide on the applicability of the macro-actions (Newton, Levine, Fox, & Long,
2007). Two versions of this work participated in the learning track of IPC-2008, obtaining
third and fourth place. One advantage of macro-actions is that the learned knowledge can
be exploited by any planner. Thus, approaches which learn generalized policies could also
benefit from macro-actions. Nevertheless, as far as we know, this combination has not been
tried for improving heuristic planners.
• Learning domain-specific heuristic functions: In this approach (Yoon, Fern, & Givan, 2006;
Xu, Fern, & Yoon, 2007), a state-generalized heuristic function is obtained from examples
of solution plans. The main drawback of learning domain-specific heuristic functions is that
the result of the learning algorithm is difficult to understand by humans which makes the
verification of the learned knowledge difficult. On the other hand, the learned knowledge is
easy to combine with existing domain-independent heuristics. A slightly different approach
807

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

consists of learning a ranking function for greedy search algorithms (Xu, Fern, & Yoon,
2009, 2010). At each step of a greedy search, the current node is expanded and the child node
with the highest rank is selected to be the current node. In this case, the ranking function is
iteratively estimated in an attempt to cover a set of solution plans with the greedy algorithm.
• Learning task decomposition: This approach learns how to divide planning tasks of a given
domain into smaller subtasks that are easier to solve. Techniques for reachability analysis
and landmark extraction (Hoffmann, Porteous, & Sebastia, 2004) are able to compute intermediate states that must be reached before satisfying the goals. However, it is not clear
how to systematically exploit this knowledge to build good problem decompositions. Vidal et al. (2010) consider this as an optimization problem and use a specialized optimization
algorithm to discover good decompositions.
In general, any system that learns planning DCK has to deal with ambiguity in the training
examples, because a given planning state may present many good actions. Trying to learn DCK
that selects one action over other, inherently equal, is a complex learning problem. To cope with
ambiguous training data ROLLER created a function that ranks solutions with the aim of learning
from the same kind of solutions. A different approach is followed by Xu et al. (2010) who generate
training examples from partially ordered plans.

7. Conclusions and Future Work
We have presented a new technique for reducing the number of node evaluations in heuristic planning based on learning and exploiting generalized policies. Our technique defines the process of
learning generalized policies as a two-step classification and builds domain-specific relational decision trees that capture the action to be selected in the different planning contexts. In this work,
planning contexts are specified by the helpful actions of the state, the pending goals and the static
predicates of the problem. Finally, we have explained how to exploit the learned policies to solve
classical planning problems, applying them directly or combining them with a domain independent
heuristic in a lookahead strategy for the BFS algorithm. This work contributes to the state-of-the-art
of learning-based planning in three ways:
1. Representation. We propose a new encoding for generalized policies that is able to capture
efficient DCK using predicate logic. As opposed to previous works that represent generalized
policies in predicate logic (Khardon, 1999), our representation does not need extra background knowledge (support predicates) to learn efficient policies for the Blocksworld domain.
Besides, encoding states with the set of helpful actions is frequently more compact and furthermore, this set normally decreases when the search has fewer goals left. Thus, the process
of matching DCK becomes faster when the search advances towards the goals.
2. Learning. We have defined the task of learning a generalized policy as a two-step standard
classification task. Thus, we can learn the generalized policy with an off-the-shelf tool for
building relational classifiers. Results in this paper are obtained with the TILDE system (Blockeel & De Raedt, 1998), but any other tool for learning relational classifiers could have been
used. Because of this, advances in relational classification can be applied in a straightforward
manner in ROLLER to learn faster and better planning DCK.
808

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

3. Planning. We have explained how to extract an action ordering from an H-Context Policy
and we have shown how to use this ordering to reduce node evaluations: (1) in the algorithm Depth-First H-Context Policy that allows a direct application of the H-Context policies;
and (2) in the H-Context Policy Lookahead BFS, which combines the policy with a domainindependent heuristic within a BFS algorithm. In addition, we have included a modified
version of this algorithm (ROLLER - BFS - HA) that only considers helpful successors in order
to reduce the number of evaluations in domains where helpful actions are good.
Experimental results show that our approach improved the scalability of the baseline heuristic
planners FF and LAMA (winner of IPC-2008) over a variety of IPC domains. This effect is more
evident in domains where the learned DCK presents good quality, e.g. Blocksworld and Parking.
In these domains the direct application of the learned DCK saves large amounts of node evaluations
achieving impressive scalability performance. Moreover, using the learned DCK in combination
with a domain-independent heuristic in a BFS algorithm achieves good quality solutions. When
the quality of the learned DCK is poor, planning with the direct application of the policy fails to
solve many problems, mainly the largest ones which are more difficult to solve without a reasonable
guide. Unfortunately, the only current mechanism for quantifying the quality of the learned DCK
is evaluating it against a set of test problems. Therefore, a good compromise solution is combining
the learned DCK with domain-independent heuristics.
In some domains, the DCK learned by ROLLER presents poor quality because the helpful context
is not able to represent concepts that are necessary in order to discriminate between good and bad
actions. This problem frequently arises when the arguments of the good action do not correspond
to the problem goals or the static predicates. We plan to study refinements to our definition of
the helpful context to achieve good DCK in such domains. One possible direction is extending
the helpful context with subgoal information such as landmarks (Hoffmann et al., 2004) of the
relaxed plan. Moreover, the use of decision trees introduces an important bias in the learning step.
Algorithms for tree learning only insert a new query in the tree if doing so produces a significant
information gain. However, in some domains this information gain can only be obtained by the
conjunction of two or more queries. Finally, we are currently providing the learner with a fixed
distribution of training examples. In the near future, we plan to explore how the learner can generate
the most convenient distribution of training examples according to a target planning task as proposed
by Fuentetaja and Borrajo (2006).

Acknowledgments
This work has been partially supported by the Spanish MICIIN project TIN2008-06701-C03-03 and
the regional CAM-UC3M project CCG08-UC3M/TIC-4141.

809

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Appendix A. DCK Usefulness Results

Domains
Blocksworld (30)
Depots (22)
Gold-miner (30)
Matching-BW (30)
Parking (30)
Rovers (30)
Satellite (30)
Storage (30)
Thoughtful(30)
TPP (30)
Total

DEPH-FIRST
roller gr-ha df-ha
29.87
0.03
0.00
19.42
7.70
3.61
28.00
0.00
0.00
20.88
0.00
0.00
28.57
1.23
0.00
25.99 10.09
7.73
27.97
2.93
1.69
11.02
8.03
8.08
11.91 13.15
0.00
29.50 10.86 11.45
233.13 54.02 32.56

BEST-FIRST
roller-bfs lh-bfs
bfs
2.47
0.00
0.00
10.51
5.32
2.45
0.04
0.05
0.00
3.82
0.98
3.90
22.72
0.26
0.01
14.58
5.52
0.14
16.09
3.05
0.08
11.53 10.56
8.97
11.30
8.90
2.31
14.83
8.67
5.00
107.89 43.31 22.86

HELPFUL BEST-FIRST
roller-bfs-ha lh-bfs-ha bfs-ha
2.40
0.00
0.00
10.98
5.27
7.08
7.35
0.00
0.00
3.72
2.20
5.81
23.60
0.26
0.04
18.12
17.17
8.21
21.93
2.68
2.90
16.12
7.00
7.00
11.89
9.05
3.04
13.97
7.54
6.13
130.08
51.17
40.21

Table 9: Problems solved for the DCK usefulness evaluation.

Domains
Blocksworld (30)
Depots (22)
Gold-miner (30)
Matching-BW (30)
Parking (30)
Rovers (30)
Satellite (30)
Storage (30)
Thoughtful(30)
TPP (30)
Total

DEPH-FIRST
roller
gr-ha df-ha
29.83
0.06
0.00
8.82
8.21
3.02
19.78
0.00
0.00
11.53
0.00
0.00
21.53
8.20
0.01
21.54
26.34 25.51
28.03
17.36
9.58
13.43
8.02
8.00
6.89
12.40
0.00
25.46
27.92 23.75
186.84 108.51 69.87

BEST-FIRST
roller-bfs
lh-bfs
8.00
0.00
12.68
14.84
11.50
17.00
12.76
6.35
26.92
6.33
21.94
24.63
22.60
21.48
15.59
16.87
16.56
13.35
13.94
22.04
162.49 142.89

bfs
0.00
12.37
15.41
13.84
6.14
10.75
14.64
19.23
10.60
8.71
111.69

HELPFUL BEST-FIRST
roller-bfs-ha lh-bfs-ha
bfs-ha
8.00
0.00
0.00
13.20
16.06
19.97
16.67
0.00
0.00
17.21
9.39
16.54
26.92
6.33
8.18
25.71
26.32
29.63
27.60
22.31
22.42
15.66
8.59
9.31
19.48
14.91
11.75
16.20
24.36
13.88
186.65
128.27 132.35

Table 10: Quality scores for the DCK usefulness evaluation.

References
Bacchus, F., & Kabanza, F. (2000). Using temporal logics to express search control knowledge for
planning. Artificial Intelligence, 116(1-2), 123–191.
Bibaı̈, J., Savéant, P., Schoenauer, M., & Vidal, V. (2010). An evolutionary metaheuristic based on
state decomposition for domain-independent satisficing planning. In Proceedings of the 20th
International Conference on Automated Planning and Scheduling (ICAPS’10) Toronto, ON,
Canada. AAAI Press.
Blockeel, H., & De Raedt, L. (1998). Top-down induction of first-order logical decision trees.
Artificial Intelligence, 101(1-2), 285–297.
810

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

Bonet, B., Loerincs, G., & Geffner, H. (1997). A robust and fast action selection mechanism
for planning. In Proceedings of the American Association for the Advancement of Artificial
Intelligence Conference (AAAI), pp. 714–719. MIT Press.
Botea, A., Enzenberger, M., Müller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI planning
with automatically learned macro-operators. Journal of Artificial Intelligence Research, 24,
581–621.
Coles, A., & Smith, A. (2007). Marvin: A heuristic search planner with online macro-action learning. Journal of Artificial Intelligence Research, 28, 119–156.
De la Rosa, T., Jiménez, S., & Borrajo, D. (2008). Learning relational decision trees for guiding heuristic planning. In International Conference on Automated Planning and Scheduling
(ICAPS).
De la Rosa, T., Jiménez, S., Garcı́a-Durán, R., Fernández, F., Garcı́a-Olaya, A., & Borrajo, D.
(2009). Three relational learning approaches for lookahead heuristic planning. In Working
Notes of ICAPS 2009 Workshop on Planning and Learning, pp. 37–44.
De Raedt, L. (2008). Logical and Relational Learning. Springer, Berlin Heidelberg.
Doherty, P., & Kvarnström, J. (2001). Talplanner: A temporal logic based planner. AI Magazine,
22(3), 95–102.
Dzeroski, S., De Raedt, L., & Blockeel, H. (1998). Relational reinforcement learning. In International Workshop on ILP, pp. 11–22.
Emde, W., & Wettschereck, D. (1996). Relational instance-based learning. In Proceedings of the
13th Conference on Machine Learning, pp. 122–130.
Flórez, J. E., Garcı́a, J., Torralba, A., Linares, C., Garcı́a-Olaya, A., & Borrajo, D. (2010). Timiplan: An application to solve multimodal transportation problems. In Proceedings of SPARK,
Scheduling and Planning Applications workshop, ICAPS’10.
Fuentetaja, R., & Borrajo, D. (2006). Improving control-knowledge acquisition for planning by
active learning. In ECML, Berlin, Germany, Vol. 4212, pp. 138–149.
Gerevini, A., Saetti, A., & Vallati, M. (2009). An automatically configurable portfolio-based planner
with macro-actions: Pbp. In Proceedings of the 19th International Conference on Automated
Planning and Scheduling, pp. 191–199 Thessaloniki, Greece.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic
search. Journal of Artificial Intelligence Research, 14, 253–302.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in planning. Journal of
Artificial Intelligence Research, 22.
Khardon, R. (1999). Learning action strategies for planning domains. Artificial Intelligence, 113,
125–148.
811

D E LA ROSA , J IMENEZ , F UENTETAJA & B ORRAJO

Leckie, C., & Zukerman, I. (1998). Inductive learning of search control rules for planning. Artificial
Intelligence, 101(1–2), 63–98.
Martin, M., & Geffner, H. (2000). Learning generalized policies in planning using concept languages. In International Conference on Artificial Intelligence Planning Systems, AIPS00.
Martin, M., & Geffner, H. (2004). Learning generalized policies from planning examples using
concept languages. Appl. Intell, 20, 9–19.
Mcallester, D., & Givan, R. (1989). Taxonomic syntax for first order inference. Journal of the ACM,
40, 289–300.
McDermott, D. (1996). A heuristic estimator for means-ends analysis in planning. In Proceedings
of the 3rd Conference on Artificial Intelligence Planning Systems (AIPS), pp. 142–149. AAAI
Press.
Minton, S. (1990). Quantitative results concerning the utility of explanation-based learning. Artif.
Intell., 42(2-3), 363–391.
Muggleton, S. (1995). Inverse entailment and progol. New Generation Computing, 13, 245–286.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory and methods. Journal
of Logic Programming, 19, 629–679.
Nau, D., Au, T.-C., Ilghami, O., Kuter, U., Murdock, W., Wu, D., & Yaman, F. (2003). SHOP2: An
HTN planning system. Journal of Artificial Intelligence Research, 20, 379–404.
Newton, M. A. H., Levine, J., Fox, M., & Long, D. (2007). Learning macro-actions for arbitrary
planners and domains. In Proceedings of the 17th International Conference on Automated
Planning and Scheduling (ICAPS).
Quinlan, J. (1986). Induction of decision trees. Machine Learning, 1, 81–106.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime planning
with landmarks. Journal of Artificial Intelligence Research, 39, 127–177.
Röger, G., & Helmert, M. (2010). The more, the merrier: Combining heuristic estimators for satisficing planning. In Proceedings of the 20th International Conference on Automated Planning
and Scheduling (ICAPS), pp. 246–249.
Veloso, M., Carbonell, J., Pérez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integrating planning
and learning: The PRODIGY architecture. JETAI, 7(1), 81–120.
Vidal, V. (2004). A lookahead strategy for heuristic search planning. In Proceedings of the 14th
International Conference on Automated Planning and Scheduling (ICAPS 2004), Whistler,
British Columbia, Canada, pp. 150–160.
Xu, Y., Fern, A., & Yoon, S. W. (2007). Discriminative learning of beam-search heuristics for
planning. In IJCAI 2007, Proceedings of the 20th IJCAI, pp. 2041–2046.
812

S CALING UP H EURISTIC P LANNING WITH R ELATIONAL D ECISION T REES

Xu, Y., Fern, A., & Yoon, S. (2009). Learning linear ranking functions for beam search with
application to planning. Journal of Machine Learning Research, 10, 1571–1610.
Xu, Y., Fern, A., & Yoon, S. (2010). Iterative learning of weighted rule sets for greedy search.
In Proceedings of the 20th International Conference on Automated Planning and Scheduling
(ICAPS) Toronto, Canada.
Yoon, S., Fern, A., & Givan, R. (2006). Learning heuristic functions from relaxed plans. In Proceedings of the 16th International Conference on Automated Planning and Scheduling (ICAPS).
Yoon, S., Fern, A., & Givan, R. (2007). Using learned policies in heuristic-search planning. In
Proceedings of the 20th IJCAI.
Yoon, S., Fern, A., & Givan, R. (2008). Learning control knowledge for forward search planning.
J. Mach. Learn. Res., 9, 683–718.
Zimmerman, T., & Kambhampati, S. (2003). Learning-assisted automated planning: looking back,
taking stock, going forward. AI Magazine, 24, 73 – 96.

813

Journal of Artificial Intelligence Research 40 (2011) 305-351

Submitted 07/10; published 01/11

Multimode Control Attacks on Elections
Piotr Faliszewski

faliszew@agh.edu.pl

Department of Computer Science
AGH University of Science and Technology
Kraków, Poland

Edith Hemaspaandra

eh@cs.rit.edu

Department of Computer Science
Rochester Institute of Technology
Rochester, NY 14623 USA

Lane A. Hemaspaandra

lane@cs.rochester.edu

Department of Computer Science
University of Rochester
Rochester, NY 14627 USA

Abstract
In 1992, Bartholdi, Tovey, and Trick opened the study of control attacks on elections—
attempts to improve the election outcome by such actions as adding/deleting candidates or
voters. That work has led to many results on how algorithms can be used to find attacks
on elections and how complexity-theoretic hardness results can be used as shields against
attacks. However, all the work in this line has assumed that the attacker employs just a
single type of attack. In this paper, we model and study the case in which the attacker
launches a multipronged (i.e., multimode) attack. We do so to more realistically capture the
richness of real-life settings. For example, an attacker might simultaneously try to suppress
some voters, attract new voters into the election, and introduce a spoiler candidate. Our
model provides a unified framework for such varied attacks. By constructing polynomialtime multiprong attack algorithms we prove that for various election systems even such
concerted, flexible attacks can be perfectly planned in deterministic polynomial time.

1. Introduction
Elections are a central model for collective decision-making: Actors’ (voters’) preferences
among alternatives (candidates) are input to the election rule and a winner (or winners in
the case of ties) is declared by the rule. Bartholdi, Orlin, Tovey, and Trick initiated a line of
research whose goal is to protect elections from various attacking actions intended to skew
the election’s results. Bartholdi, Orlin, Tovey, and Trick’s strategy for achieving this goal
was to show that for various election systems and attacking actions, even seeing whether
for a given set of votes such an attack is possible is NP-complete. Their papers (Bartholdi,
Tovey, & Trick, 1989a; Bartholdi & Orlin, 1991; Bartholdi, Tovey, & Trick, 1992) consider
actions such as voter manipulation (i.e., situations where a voter misrepresents his or her
vote to obtain some goal) and various types of election control (i.e., situations where the
attacker is capable of modifying the structure of an election, e.g., by adding or deleting
either voters or candidates). Since then, many researchers have extended Bartholdi, Orlin,
Tovey, and Trick’s work by providing new models, new results, and new perspectives. But
c
2011
AI Access Foundation. All rights reserved.

Faliszewski, Hemaspaandra, & Hemaspaandra

to the best of our knowledge, until now no one has considered the situation in which an
attacker combines multiple standard attack types into a single attack—let us call that a
multipronged (or multimode) attack.
Studying multipronged control is a step in the direction of more realistically modeling
real-life scenarios. Certainly, in real-life settings an attacker would not voluntarily limit
himself or herself to a single type of attack but rather would use all available means of
reaching his or her goal. For example, an attacker interested in some candidate p winning
might, at the same time, intimidate p’s most dangerous competitors so that they would
withdraw from the election, and encourage voters who support p to show up to vote. In
this paper we study the complexity of such multipronged control attacks.1
Given a type of multiprong control, we seek to analyze its complexity. In particular,
we try to show either that one can compute in polynomial time an optimal attack of that
control type, or that even recognizing the existence of an attack is NP-hard. It is particularly
interesting to ask about the complexity of a multipronged attack whose components each
have efficient algorithms. We are interested in whether such a combined attack (a) becomes
computationally hard, or (b) still has a polynomial-time algorithm. Regarding the (a) case,
we give an example of a natural election system that displays this behavior. Our paper’s core
work studies the (b) case and shows that even attacks having multiple prongs can in many
cases be planned with perfect efficiency. Such results yield as immediate consequences all
the individual efficient attack algorithms for each prong, and as such allow a more compact
presentation of results and more compact proofs. But they go beyond that: They show that
the interactions between the prongs can be managed without such cost as to move beyond
polynomial time.
The paper’s organization is as follows. In Section 2 we discuss the relevant literature. In
Section 3 we present the standard model of elections and describe relevant voting systems.
In Section 4 we introduce multiprong control, provide initial results, and show how existing
immunity, vulnerability, and resistance results interact with this model. In Section 5 we
provide a complexity analysis of candidate and voter control in maximin elections, showing
how multiprong control is useful in doing so. In Section 6 we consider fixed-parameter complexity of multiprong control, using as our parameter the number of candidates. Section 7
provides conclusions and open problems. In the appendix, we show that maximin has an
interesting relation to Dodgson elections: No candidate whose Dodgson score is more than
m2 times that of the Dodgson winner(s) can be a maximin winner.

2. Related Work
Since the seminal paper of Bartholdi et al. (1992), much research has been dedicated to
studying the complexity of control in elections. Bartholdi et al. (1992) considered constructive control only, i.e., scenarios where the goal of the attacker is to ensure some candidate’s
victory. Hemaspaandra, Hemaspaandra, and Rothe (2007) extended their work to the destructive case, i.e., scenarios in which the goal is to prevent someone from winning.
A central but elusive goal of control research is finding a natural election system (with
a polynomial-time winner algorithm) that is resistant to all the standard types of con1. In fact, our framework of multiprong control includes the unpriced bribery of Faliszewski, Hemaspaandra,
and Hemaspaandra (2009a), and can be extended to include manipulation.

306

Multimode Control Attacks on Elections

trol, i.e., for which all the types of control are NP-hard. Hemaspaandra, Hemaspaandra,
and Rothe (2009) showed that there exist highly resistant artificial election systems. Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2009a) then showed that the natural
system known as Copeland voting is not too far from the goal mentioned above. And
Erdélyi, Nowak, and Rothe (2009) then showed a system with even more resistances than
Copeland, but in a slightly nonstandard voter model (see Baumeister, Erdélyi, Hemaspaandra, Hemaspaandra, & Rothe, 2010, for discussion and Erdélyi, Piras, & Rothe, 2010b,
2010a; Menton, 2010, for some related follow-up work).
Recently, researchers also started focusing on the parameterized complexity of control in
elections. Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2009a) provided several
fixed-parameter tractability results. Betzler and Uhlmann (2009) and Liu, Feng, Zhu, and
Luan (2009) showed so-called W[1]- and W[2]-hardness results for control under various voting rules. In response to the conference version (Faliszewski, Hemaspaandra, & Hemaspaandra, 2009b) of the present paper, Liu and Zhu (2010) conducted a parameterized-complexity
study of control in maximin elections.
Going in a somewhat different direction, Meir, Procaccia, Rosenschein, and Zohar (2008)
bridged the notions of constructive and destructive control by considering utility functions,
and in this model obtained control results for multiwinner elections. In multiwinner elections
the goal is to elect a whole group of people (consider, e.g., parliamentary elections) rather
than just a single person. Elkind, Faliszewski, and Slinko (2010a) and Maudet, Lang,
Chevaleyre, and Monnot (2010) considered two types of problems related to control by
adding candidates for the case where it is not known how the voters would rank the added
candidates.
Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2011) and Brandt, Brill,
Hemaspaandra, and Hemaspaandra (2010) have studied control (and manipulation and
bribery) in so-called single-peaked domains, a model of overall electorate behavior from
political science.
There is a growing body of work on manipulation that regards frequency of
(non)hardness of election problems (see, e.g., Conitzer & Sandholm, 2006; Friedgut, Kalai,
& Nisan, 2008; Dobzinski & Procaccia, 2008; Xia & Conitzer, 2008b, 2008a; Walsh, 2009;
Isaksson, Kindler, & Mossel, 2010). This work studies whether a given NP-hard election
problem (to date only manipulation/winner problems have been studied, not control problems) can be often solved in practice (assuming some distribution of votes). (One however
should keep in mind that if any polynomial-time algorithm solves any NP-hard problem extremely frequently—if it errs on only a sparse set in the formal complexity-theoretic sense
of that term—then P = NP, Schöning, 1986.) Such frequency results are of course very
relevant when one’s goal is to protect elections from manipulative actions, and so although
NP-hardness is a very important step, it is but a first step towards truly broad, satisfying
security. However, in this paper we typically take the role of an attacker and design control
algorithms that are fast on all instances.
Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2009b) and Faliszewski, Hemaspaandra, and Hemaspaandra (2010b) provide an overview of some complexity-of-election
issues.
307

Faliszewski, Hemaspaandra, & Hemaspaandra

3. Preliminaries
This section covers preliminaries about elections and computational complexity.
3.1 Elections
An election is a pair (C, V ), where C = {c1 , . . . , cm } is the set of candidates and V =
(v1 , . . . , vn ) is a collection of voters. Each voter vi is represented by his or her preference
list.2 For example, if we have three candidates, c1 , c2 , and c3 , a voter who likes c1 most,
then c2 , and then c3 would have preference list c1 > c2 > c3 .3 Given an election E = (C, V ),
by NE (ci , cj ), where ci , cj ∈ C and i 6= j, we denote the number of voters in V who prefer
ci to cj . We adopt the following convention for specifying preference lists.
Convention 3.1. Listing some set D of candidates as an item in a preference list means
listing all the members of this set in increasing lexicographic order (with respect to the
←
−
candidates’ names), and listing D means listing all the members of D but in decreasing
lexicographic order (with respect to the candidates’ names).
Example 3.2. Let us give a quick example of the convention. If C = {Bob, Carol, Ted,
Alice} and D = {Alice, Ted, Bob}, then Carol > D is shorthand for Carol > Alice > Bob >
←
−
Ted, and Carol > D is shorthand for Carol > Ted > Bob > Alice.4
Note that in the model used in this paper, we assume that the person trying to do the
attack on the election knows what the votes, V , are. This has been the standard model
in computational studies of attacks on elections ever since the seminal work of Bartholdi
et al. (1989a, 1992) and Bartholdi and Orlin (1991). However, it is worth noting that it is
an abstract model that has strains in its connection to the real world. Regarding proving
lower bounds, such as NP-hardness results, results in this model are actually stronger: One
is showing that even given full access to the votes, V , the attacker still has an NP-hard
task. On the other hand, when we build polynomial-time attack algorithms in this model,
those algorithms are benefiting from the model letting them know what the votes are. How
natural this model is will vary by the situation, and one should always keep in mind that
this is indeed an abstract model, not the real world itself. However, in many settings, it
is not unreasonable to assume that the attacker might have strong information about the
votes. That information might come from polls, or it might come from door-to-door or
telephone canvassing, or it might come from voter registration or contribution records, or
2. We also assume that each voter has a unique name. However, all the election systems we consider
here—except for the election system of Theorem 4.12—are oblivious to the particular voter names and
the order of the votes.
3. Preference lists are also called preference orders, and in this paper we will use these two terms interchangeably.
4. In the constructions where we use this convention, we are using variable names for objects—such as
{b1 , . . . , b3k } and p and so on—that are used as candidate sets in elections that are being output by the
reduction. However, since an election has as part of its input the set of candidates (which are named),
our actual reductions will be assigning name strings to each of these objects, and so the ordering we have
discussed is well-defined and can be easily carried out by the reductions. And in fact, in the context of
those reductions, actual (string) values of the bi ’s are probably already part of the input to the reduction.
(We have chosen lexicographic order simply because polynomial-time reductions can sort things into it,
and its reverse, without any problem.)

308

Multimode Control Attacks on Elections

it might come (in intimate, human elections, such as votes on whether one should have a
course-based or an exam-based M.S. degree in one’s department) from great familiarity of
the attacker with the voters, or it might come from the attacker being the vote collector.
An election system is a mapping that given an election (C, V ) outputs a set W , satisfying
W ⊆ C, called the winners of the election.5
We focus on the following five voting systems: plurality, Copeland, maximin, approval,
and Condorcet. (However, in Section 6 and the appendix we take a detour through some
other systems.) Each of plurality, Copeland, maximin, and approval assigns points to
candidates and elects those that receive the most points. Let E = (C, V ) be an election,
where C = {c1 , . . . , cm } and V = (v1 , . . . , vn ). In plurality, each candidate receives a single
point for each voter who ranks him or her first. In maximin, the score of a candidate ci in E is
defined as mincj ∈C−{ci } NE (ci , cj ). For each rational α, 0 ≤ α ≤ 1, in Copelandα candidate
ci receives 1 point for each candidate cj , j 6= i, such that NE (ci , cj ) > NE (cj , ci ) and α
points for each candidate cj , j 6= i, such that NE (ci , cj ) = NE (cj , ci ). That is, the parameter
α describes the value of ties in head-to-head majority contests. In approval, instead of
preference lists each voter’s ballot is a 0-1 vector, where each entry denotes whether the voter
approves of the corresponding candidate (gives the corresponding candidate a point). For
example, vector (1, 0, 0, 1) means that the voter approves of the first and fourth candidates,
but not the second and third. We use scoreE (ci ) to denote the score of candidate ci in
election E (the particular election system used will always be clear from context).
A candidate c is a Condorcet winner of an election E = (C, V ) if for each other candidate
∈ C it holds that NE (c, c0 ) > NE (c0 , c). Condorcet voting is the election system in which
the winner set is, by definition, exactly the set of Condorcet winners. It follows from the
definition that each election has at most one Condorcet winner. Not every election has a
Condorcet winner. However, as our notion of an election allows outcomes in which no one
wins, electing the Condorcet winner when there is one and otherwise having no winner is a
legal election system.

c0

5. Readers with a social choice background may wonder why we do not forbid the case W = ∅, as is
typically done in social choice framings of elections. Briefly put, allowing the possibility W = ∅ has
been the standard model in the computational studies of elections, starting with the seminal papers of
Bartholdi, Orlin, Tovey, and Trick. By retaining this model, our results can better be compared with the
existing computational results on attacks on elections. In fact, the recent (admittedly computationally
oriented) textbook of Shoham and Leyton-Brown (2009, Def. 9.2.2) treats the definition of a social choice
correspondence as allowing any subset of the candidates, including the empty set (in contrast, in social
choice papers, the notion of social choice correspondence routinely in its definition excludes the possibility
of having no winners). Although we follow the model of allowing the empty outcome as it is the standard
computational model and allows comparison with existing results, we mention in passing that we find this
model, on its merits, the more attractive one, although this is certainly a matter of taste, familiarity, and
comfort. This model avoids building in a special-case exception in the definition and allows one to discuss
zero-candidates elections if for some reason one wants to. But more importantly, many natural election
systems might have no winners. Examples include threshold election systems, including majority-rule
elections and the election systems often used to see whether anyone—by exceeding a certain percentage
of approval votes from a group of expert sports writers, for instance—merits induction into a sports Hall
of Fame that year. Condorcet voting (to be defined later), which the seminal control-of-elections paper
of Bartholdi et al. (1992) treated as an election system, also can have an empty winner set.

309

Faliszewski, Hemaspaandra, & Hemaspaandra

3.2 Computational Complexity
We use standard notions of complexity theory, as presented, e.g., in the textbook of Papadimitriou (1994). We assume that the reader is familiar with the complexity classes P
and NP, polynomial-time many-one reductions, and the notions of NP-hardness and NPcompleteness. N will denote {0, 1, 2, . . .}.
Most of the NP-hardness proofs in this paper follow by a reduction from the well-known
NP-complete problem exact cover by 3-sets, known for short as X3C (see, e.g., Garey &
Johnson, 1979). In X3C we are given a pair (B, S), where B = {b1 , . . . , b3k } is a set of 3k
elements and S = {S1 , . . . , Sn } is a set of 3-subsets of B, and we ask whether there is a
subset S 0 of exactly k elements of S such that their union is exactly B. We call such a set
S 0 an exact cover of B.
In Section 6, we consider the fixed-parameter complexity of multiprong control. The
idea of fixed-parameter complexity is to measure the complexity of a given decision problem
with respect to both the instance size (as in the standard complexity theory) and some
parameter of the input (in our case, the number of candidates involved). For a problem
to be said to be fixed-parameter tractable, i.e., to belong to the complexity class FPT, we
as is standard require that the problem can be solved by an algorithm running in time
f (j)nO(1) , where n is the size of the encoding of the given instance, j is the value of the
parameter for this instance, and f is some function. Note that f does not have to be
polynomially bounded or even computable. However, in all FPT claims in this paper, f is
a computable function. That is, our algorithms actually achieve so-called strongly uniform
fixed-parameter tractability. We point readers interested in parameterized complexity to,
for example, the recent book by Niedermeier (2006).

4. Control and Multiprong Control
In this section we introduce multiprong control, that is, control types that combine several
standard types of control. We first provide the definition, then proceed to analyzing general
properties of multiprong control, then consider multiprong control for election systems for
which the complexity of single-prong control has already been established, and finally give
an example of an election system for which multiprong control becomes harder than any of
its constituent prongs (assuming P 6= NP). We conclude the section with a summary of its
main contributions.
4.1 The Definition
We consider combinations of control by adding/deleting candidates/voters6 and by bribing
voters. Traditionally, bribery has not been considered a type of control but it fits the model
very naturally and strengthens our results.
In discussing control problems, we must be very clear about whether the goal of the
attacker is to make his or her preferred candidate the only winner, or is to make his or
her preferred candidate a winner. To be clear on this, we as is standard will use the
term “unique-winner model” for the model in which the goal is to make one’s preferred
6. Other control types, defined by Bartholdi et al. (1992) and refined by Hemaspaandra et al. (2007), regard
various types of partitioning candidates and voters.

310

Multimode Control Attacks on Elections

candidate the one and only winner, and we will use the term “nonunique-winner model”
for the approach in which the goal is to make one’s preferred candidate be a winner. (Note
that if exactly one person wins, he or she most certainly is considered to have satisfied the
control action in the nonunique-winner model. The “nonunique” in the model name merely
means we are not requiring that winners be unique.)
The destructive cases of each of these are, in the nonunique-winner model, blocking
one’s despised candidate from being a unique winner,7 and in the unique-winner model,
blocking one’s despised candidate from being a winner. We take the unique-winner model
as the default in this paper, as is the most common model in studies of control.
Definition 4.1. Let E be an election system. In the unique-winner,8 constructive EAC+DC+AV+DV+BV control problem we are given:
(a) two disjoint sets of candidates, C and A,
(b) two disjoint collections of voters, V and W , containing voters with preference lists
over C ∪ A,
(c) a preferred candidate p ∈ C, and
(d) five nonnegative integers, kAC , kDC , kAV , kDV , and kBV .
We ask whether it is possible to find two sets, A0 ⊆ A and C 0 ⊂ C, and two subcollections
of voters, V 0 ⊆ V and W 0 ⊆ W , such that:
(e) it is possible to ensure that p is a unique winner of E election ((C − C 0 ) ∪ A0 , (V −
V 0 ) ∪ W 0 ) via changing preference orders of (i.e., bribing) at most kBV voters in
(V − V 0 ) ∪ W 0 ,
(f ) p ∈
/ C 0 , and
(g) kA0 k ≤ kAC , kC 0 k ≤ kDC , kW 0 k ≤ kAV , and kV 0 k ≤ kDV .
In the unique-winner, destructive variant of the problem, we replace item (e) above with: “it
is possible to ensure that p is not a unique winner of E election ((C −C 0 )∪A0 , (V −V 0 )∪W 0 )
via changing preference orders of at most kBV voters in (V − V 0 ) ∪ W 0 .” (In addition, in the
destructive variant we refer to p as “the despised candidate” rather than as “the preferred
candidate,” and we often denote him or her by d.)
Table 1 summarizes in informal English the notation used in this definition, so that this
information is easily available for the reader to refer back to.
The phrase AC+DC+AV+DV+BV in the problem name corresponds to four of the
standard types of control: adding candidates (AC), deleting candidates (DC), adding voters
(AV), deleting voters (DV), and to (unpriced) bribery (BV); we will refer to these five types
of control as the basic types of control. We again remind the reader that traditionally
7. We will often use the phrase “a unique winner,” as we just did. The reason we write “a unique winner”
rather than “the unique winner” is to avoid the impression that the election necessarily has some (unique)
winner.
8. One can straightforwardly adapt the definition to the nonunique-winner model.

311

Faliszewski, Hemaspaandra, & Hemaspaandra

Notation
AC
DC
AV
DV
BV
C
A
V
W
kAC
kDC
kAV
kDV
kBV
p
d

Meaning
Control by adding candidates.
Control by deleting candidates.
Control by adding voters.
Control by deleting voters.
Control by bribing voters.
The set of initial candidates in an election.
The set of additional candidates that the control agent may introduce.
The collection of initial voters in an election.
The collection of additional voters that the control agent may introduce.
The bound on the number of candidates that can be added in AC control.
The bound on the number of candidates that can be deleted in DC control.
The bound on the number of voters that can be added in AV control.
The bound on the number of voters that can be deleted in DV control.
The bound on the number of voters that can be bribed in BV control.
The preferred candidate (the constructive control goal is to ensure that p
is a unique winner).
The despised candidate (the destructive control goal is to ensure that d is
not a unique winner).

Table 1: Notations from Definition 4.1 that are used frequently elsewhere.

bribery is not a type of control but we will call it a basic type of control for the sake of
uniformity and throughout the rest of the paper we will consider it as such.
As to why we choose these as the “basic” types, it is essentially because these are the
collection on which we focus, and so the term is a handy one to use to indicate them. But we
have focused on these particular ones largely because we find them highly attractive. The
various “partition” control types that appeared in the original paper on control, while quite
interesting, have always seemed less natural to us than adding/deleting voters/candidates.
Bribery to us is also quite compellingly natural. The attack known as manipulation is not
included by us among the “basic” types, but without a doubt is a natural and important
type of attack on elections, and in the conclusion we discuss it briefly, and commend to the
reader the issue of studying manipulation as an additional prong.
Instead of considering all of AC, DC, AV, DV, and BV, we often are interested in some
subset of them and so we consider special cases of the AC+DC+AV+DV+BV problem.
For example, we write DC+AV to refer to a variant of the AC+DC+AV+DV+BV problem
where only deleting candidates and adding voters is allowed. As part of our model we
assume that in each such variant, only the parameters relevant to the prongs are part of
the input. So, for example, DC+AV would have kDC , kAV , C, V , W , and p as the (only)
parts of its input. And the “missing” parts (e.g., for DC+AV, the missing parts are A, kAC ,
kDV , and kBV ) are treated in the obvious way in evaluating the formulas in Definition 4.1,
namely, missing sets are treated as ∅ and missing constants are treated as 0. If we name
only a single type of control, we in effect degenerate to one of the standard control problems.
312

Multimode Control Attacks on Elections

The reader may naturally wonder in what order the prongs of a multi-prong attack
occur. Note that part (e) of Definition 4.1 is quietly setting the order. However, almost all
the order interactions are uninteresting (unless the attacker is idiotic). For example, the
definition does not allow one to bribe voters that one will be deleting, but doing so would
be pointless anyway, so this is not an interesting restriction on the attacker. Similarly, the
definition does not allow one to add a voter and then immediately delete it, but again, that
does not take even one successful attack away from the attacker. Indeed, the only interesting
order interaction to consider is whether one can bribe added voters, or whether one can
only bribe voters who were originally in the election. One could argue this either way, and if
one wanted to avoid focusing on one or the other, one could analyze everything both ways.
However, our definition embraces the model in which even the added voters can be bribed.
Note that this is the model more favorable to the attacker. One referee commented that it
would be unreasonable to give attackers the flexibility of multiple attacks but deny them
the freedom to control the order of the attacks. In keeping with the spirit of that comment,
our definition resolves the order issue in the way that is most favorable to the attacker, and
so in no way is biased against the attacker. However, for completeness we should mention
that it is possible that some—perhaps highly artificial—systems might have different attack
complexities in the model where added voters cannot be bribed as compared to the model
in which added voters can be bribed.
There is at least one more way in which we could define multiprong control. The model
in the above definition can be called the separate-resource model, as the extent to which
we can use each basic type of control is bounded separately. In the shared-resource model
one pool of action allowances must be allocated among the allowed control types (so in
the definition above we would replace kAC , kDC , kAV , and kDV with a single value, k, and
require that kC 0 k + kD0 k + kV 0 k + kW 0 k + the-number -of -bribed -voters ≤ k). Although one
could make various arguments about which model is more appropriate, their computational
complexity is related.
Theorem 4.2. If there is a polynomial-time algorithm for a given variant of multiprong
control in the separate-resource model then there is one for the shared-resource model as
well.
Proof. Let E be an election system. We will describe the idea of our proof on the example
of the constructive E-AC+AV problem. The idea straightforwardly generalizes to any other
set of allowed control actions (complexity-theory savvy readers will quickly see that we, in
essence, give a disjunctive truth-table reduction).
We are given an instance I of the constructive E-AC+AV problem in the shared-resource
model, where k is the limit on the sum of the number of candidates and voters that we may
add. Given a polynomial-time algorithm for the separate-resource variant of the problem,
we solve I using the following method. (If k > kAk + kW k then set k = kAk + kW k.)
We form a sequence I0 , . . . , Ik of instances of the separate-resource variant of the problem,
where each I` , 0 ≤ ` ≤ k, is identical to I, except that we are allowed to add at most `
candidates and at most k − ` voters. We accept if at least one of I` is a “yes” instance
of the separate-resource, constructive E-AC+AV problem. It is straightforward to see that
this algorithm is correct and runs in polynomial time.
313

Faliszewski, Hemaspaandra, & Hemaspaandra

It would be interesting to consider a variant of the shared-resource model where various actions come at different costs (e.g., adding some candidate c0 might be much more
expensive—or difficult—than adding some other candidate c00 ). This approach would be
close in spirit to priced bribery of Faliszewski, Hemaspaandra, and Hemaspaandra (2009a).
Analysis of such priced control is beyond the scope of the current paper.
4.2 Susceptibility, Immunity, Vulnerability, and Resistance
As is standard in the election-control (and election-bribery) literature, we consider vulnerability, immunity, susceptibility, and resistance to control. Let E be an election system and
let C be a type of control.
We say that E is susceptible to constructive C control if there is a scenario in which
effectuating C makes someone become a unique winner of some E election E. We say that
E is susceptible to destructive C control if there is a scenario in which effectuating C makes
someone stop being a unique winner of some E election E.
E is immune to constructive (respectively, destructive) C control if E is not susceptible
to constructive (respectively, destructive) C control.
We say that E is vulnerable to constructive (respectively, destructive) C control if E is susceptible to constructive (respectively, destructive) C control and there is a polynomial-time
algorithm that decides the constructive (respectively, destructive) E-C problem. Actually,
this paper’s vulnerability algorithms/proofs will each go further and will in polynomial time
produce, or will make implicitly clear how to produce, the successful control action. So we
in each case are even achieving the so-called certifiable vulnerability of Hemaspaandra et al.
(2007).
E is resistant to constructive (respectively, destructive) C control if E is susceptible
to (respectively, destructive) C control and the constructive (respectively, destructive) E-C
problem is NP-hard.
Before we move on to our theorems, we mention two important points to put these
notions into context. Vulnerability is about (within susceptible settings) polynomial-time
recognition of those inputs on which there are successful attacks (and on those, as noted
above, we will actually in this paper produce the attacks), not about the proportion of inputs
on which attacks succeed (however, see our comments and references earlier in this paper
about work studying frequency of hardness issues). Also, for a type of multiprong control
that, for example, has one on-its-own immune prong and some on-their-own vulnerable
prongs, we are about to prove that immunity will not hold. (In some cases vulnerability
will hold, and in some cases resistance might hold.) However, for that portion of the
input universe that allows changes on the immune-on-its-own prong but not on the other
prongs, one can quickly (assuming the winner problem for the given election system itself
is a polynomial-time problem) both recognize that one is on that subspace of inputs and
determine whether one can achieve one’s goal (since due to the immunity we are on a “dead”
prong—that prong on its own cannot raise us from failure to success).
The next few theorems describe how multiprong control problems can inherit susceptibility, immunity, vulnerability, and resistance from the basic control types that they are
built from. (We will often write “(destructive)” rather than “(respectively, destructive),”
when the “respectively” is clear from context.)

314

Multimode Control Attacks on Elections

Theorem 4.3. Let E be an election system and let C1 + · · · + Ck be a variant of multiprong
control (so 1 ≤ k ≤ 5 and each Ci is a basic control type). E is susceptible to constructive
(destructive) C1 +· · ·+Ck control if and only if E is susceptible to at least one of constructive
(destructive) C1 , . . . , Ck control.
Proof. The “if” direction is trivial: The attacker can always choose to use only the type
of control to which E is susceptible. As to the “only if” direction, it is not hard to see
that if there is some input election for which by a C1 + · · · + Ck action we can achieve
our desired change (of creating or removing unique-winnerhood for p, depending on the
case), then there is some election (not necessarily our input election) for which one of those
actions alone achieves our desired change. In essence, we can view a control action A of
type C1 + · · · + Ck as a sequence of operations, each operation being of one of the C1 , . . . , Ck
types, that—when executed in order—transform our input election into an election where
our goal is satisfied. Thus there is a single operation within A—and this operation is of one
of the types C1 , . . . , Ck —that transforms some election E 0 where our goal is not satisfied to
some election E 00 where the goal is satisfied.
We immediately have the following corollary.
Corollary 4.4. Let E be an election system and let C1 + · · · + Ck be a variant of multiprong
control (so 1 ≤ k ≤ 5 and each Ci is a basic control type). E is immune to constructive
(destructive) C1 + · · · + Ck control if and only if for each i, 1 ≤ i ≤ k, E is immune to
constructive (destructive) Ci control.
In the next theorem we show that if a given election system is vulnerable to some basic
type of control and is immune to another basic type of control, then it is vulnerable to these
two types of control combined. The proof of this theorem is straightforward, but we need
to be particularly careful as vulnerabilities and immunities can behave quite unexpectedly.
For example, it might seem that we can assume that if an election system is vulnerable to
AV and DV then it should also be vulnerable to BV, because bribing a particular voter
can be viewed as first deleting this voter and then adding—in his or her place—a voter
with the preference order as required by the briber. (This assumes we have such a voter
among the voters we can add, but when arguing susceptibility/immunity we can make this
assumption.) However, there is a simple election system that is vulnerable to both AV and
DV control, but that is immune to BV control. This system just says that in an election
E = (C, V ), where C = {c1 , . . . , cm } and V = (v1 , . . . , vn ), the winner is the candidate ci
such that n ≡ i − 1 (mod m).9
Theorem 4.5. Let E be an election system and let C1 + · · · + Ck + D1 + · · · + D` be a variant
of multiprong control (so 1 ≤ k ≤ 5, 1 ≤ ` ≤ 5, and each Ci and each Di is a basic control
type) such that E is vulnerable to constructive (destructive) C1 + · · · + Ck control and for
each i, 1 ≤ i ≤ `, E is immune to constructive (destructive) Di control.10 E is vulnerable
to C1 + · · · + Ck + D1 + · · · + D` control.
9. Of course, this election system is not neutral; permuting the names of the candidates, evaluating the
election’s winners, and then running the winners through the inverse of the permutation can change the
outcome of an election.
10. So we certainly have k + ` ≤ 5, since immunity and vulnerability are mutually exclusive.

315

Faliszewski, Hemaspaandra, & Hemaspaandra

Proof. We will give a proof for the constructive case only. The proof for the destructive
case is analogous. Let E be an election system as in the statement of the theorem and let
I be an instance of constructive E-C1 + · · · + Ck + D1 + · · · + D` control, which contains
election E = (C, V ), information about the specifics of control actions we can implement,
and where the goal is to ensure that candidate p is a unique winner. Let us first consider
the case where BV is not among C1 , . . . , Ck , D1 , . . . D` .
Let us assume that there is a collection A of control actions of types
C1 , . . . Ck , D1 , . . . , D` , such that applying the actions from A to E is legal within I and
results in an election EC+D where p is the unique winner. (We take A to be empty if p is a
unique winner of E.) We split A into two parts, AC and AD , such that AC contains exactly
the actions of types C1 , . . . , Ck , and a AD contains exactly the actions of types D1 , . . . , D` .
Since BV is not among our control actions, it is straightforward to see that it is possible
to apply actions AC to election E to obtain some election EC . (To see why it is important
that we do not consider BV, assume that BV is among control types C1 , . . . , Ck and AV
is among control types D1 , . . . , D` . In this case, AC might include an action that bribes a
voter that is added by an action from AD .)
We claim that p is the unique winner of EC . For the sake of contradiction, let us assume
that this is not the case (note that this implies that p is not a unique winner of E). If we
apply control actions AD to EC , we reach exactly election EC+D , where p is the unique
winner. Yet, this is a contradiction, because by Corollary 4.4 we have that E is immune to
D1 + · · · + D` . That is, there is no scenario where control actions of type D1 + · · · + D`
make some candidate a unique winner if he or she was not a unique winner before.
Thus it is possible to ensure that p is a unique winner by actions of type C1 + · · · + Ck
alone. We chose I arbitrarily, and thus any instance of E-C1 + · · · + Ck + D1 + · · · + D`
control can be solved by an algorithm that considers control actions of type C1 + · · · + Ck
only. This proves that E is vulnerable to C1 + · · · + Ck + D1 + · · · + D` control because, as
we have assumed, it is vulnerable to C1 + · · · + Ck control.
It remains to prove the theorem for the case where BV is among our control actions. In
the case where BV is among the control actions but AV is not, or if AV and BV are in the
same group of actions (i.e., either both are among the Ci ’s or both are among the Di ’s),
it is straightforward to see that the above proof still works. Similarly, if BV is among the
Di ’s and AV is among the Ci ’s, the above proof works as well. The only remaining case is
if our allowed control types include both BV and AV, where BV is among the Ci ’s and AV
is among the Di ’s.
In this last case, the proof also follows the general structure of the previous construction,
except that we have to take care of one issue: It is possible that AC includes bribery of
voters that are to be added by actions from AD . (We use the same notation as in the main
construction.) Let VBV be the collection of voters that AC requires to bribe, but that are
added in AD . We form a collection A0C of control actions that is identical to AC , except
that it includes adding the voters from VBV , and we let A0D be identical to AD , except that
it no longer includes adding the voters from VBV . Using A0C and A0D instead of AC and
AD , it is straightforward to show the following: If it is possible to ensure that p is a unique
winner in instance I by a legal action of type C1 + · · · + Ck + D1 + · · · + D` , then it is also
possible to do so by a legal action of type C1 + · · · + Ck + AV, where each added voter is
also bribed. Thus given an instance I of E-C1 + · · · + Ck + D1 + · · · + D` we can solve it
316

Multimode Control Attacks on Elections

using the following algorithm. Let W be the collection of voters that can be added within
I and let kAV be the limit on the number of voters that we can add.
1. Let t be min(kAV , kW k).
2. For each i in {0, 1, . . . , t} execute the next two substeps.
(a) Form instance I 0 that is identical to I, except i (arbitrarily chosen) voters from
W are added to the election.
(b) Run the E-C1 + · · · + Ck algorithm on instance I 0 and accept if it does.
3. If the algorithm has not accepted yet, reject.
It is straightforward to see that this algorithm is correct and, since E is vulnerable to
C1 + · · · + Ck , works in polynomial time. This completes the proof of the theorem.
Theorem 4.6. Let E be an election system and let C1 + · · · + Ck be a variant of multiprong
control (so 1 ≤ k ≤ 5 and each Ci is a basic control type). If for some i, 1 ≤ i ≤ k,
E is resistant to constructive (destructive) Ci control, then E is resistant to constructive
(destructive) C1 + · · · + Ck control.
Proof. Let Ci be the control type to which E is resistant. Since E is susceptible to constructive (destructive) Ci control, it follows by Theorem 4.3 that E is susceptible to constructive
(destructive) C1 + · · · + Ck control. And since the E-Ci constructive (destructive) control
problem is essentially (give or take syntax) an embedded subproblem of the E-C1 + · · · + Ck
control problem, it follows that E is resistant to C1 + · · · + Ck control.
By combining the results obtained so far in Section 4.2, we obtain a simple tool that
allows us to classify a large number of multiprong control problems based on the properties
of their prongs. This theorem—along with the forthcoming “Classification Rule A,” which
shows how to apply this result to well-behaved election systems—is the central result of
this paper.
Theorem 4.7. Let E be an election system and let C1 + · · · + Ck be a variant of multiprong
control (so 1 ≤ k ≤ 5 and each Ci is a basic control type), such that for each Ci , 1 ≤ i ≤ k,
E is resistant, vulnerable, or immune to constructive (destructive) Ci control. If there is
an i, 1 ≤ i ≤ k, such that E is resistant to constructive (destructive) Ci control then E
is resistant to constructive (destructive) C1 + · · · + Ck control. Otherwise, if for each i,
1 ≤ i ≤ k, E is immune to constructive (destructive) Ci control (1 ≤ i ≤ k), then it is
immune to constructive (destructive) C1 + · · · + Ck control. Otherwise, if E is vulnerable
to the constructive (destructive) multiprong control consisting of all the individual prongs
among the Ci for which E is vulnerable to constructive (destructive) control, then E is
vulnerable to constructive (destructive) C1 + · · · + Ck control.
To avoid any confusion, we stress that throughout one’s reading of the above corollary,
one must either always use the “constructive” case or must always use the “destructive”
case—one cannot mix and match. Also, our third “otherwise” really is an otherwise; its
claim assumes that the first case (that there is at least one resistance) did not hold.
317

Faliszewski, Hemaspaandra, & Hemaspaandra

So does the vulnerability/resistance/immunity information on the five individual prongs
completely determine which of vulnerability/resistance/immunity holds for each of the 25 −1
multiprong settings? It would be very satisfying if this were so. However, later results in
this paper show that this is not the case, since we will prove that two vulnerable prongs can
combine to yield resistance (Theorem 4.12) but also can combine to yield vulnerability (e.g.,
Theorem 4.10). (It is even plausible that there may exist cases where two vulnerable prongs
combine to yield a multiprong case that, while certainly susceptible due to Theorem 4.3 (i.e.,
immunity is impossible in this case), is neither in P nor NP-hard, i.e., is neither vulnerable
nor resistant.)
However, for every voting system that has a certain common, nice property, we can from
the 5 individual prongs’ vulnerability/resistance/immunity status mechanically read off all
25 − 1 multiprong results. That nice property is the following.
Definition 4.8. We say that an election system E is constructive (destructive)
vulnerability-combining if for each variant C1 +· · ·+Ck of multiprong control (so 1 ≤ k ≤ 5
and each Ci is a basic control type) it holds that if for all i, 1 ≤ i ≤ k, E is vulnerable
to constructive (destructive) Ci control, then E is vulnerable to constructive (destructive)
C1 + · · · + Ck control.
For systems that are constructive (destructive) vulnerability-combining, it is straightforward to see that Theorem 4.7 can be used to read off all 25 − 1 multiprong cases, given
just the status of the five underlying prongs.
But how can one in practice establish that a system is constructive (destructive)
vulnerability-combining? One could try it by brute force, looking at each collection of
vulnerable prongs. However, there is a better path to follow. One can look at all the
vulnerable prongs together, and prove (if it happens to be the case) that they yield vulnerability. Note that doing so successfully implies immediately that vulnerability holds for
every nonempty subset of those prongs. That this is true follows from the following claim.
Proposition 4.9. Let E be an election system and let C1 , . . . , Ck , 1 ≤ k ≤ 5, be a collection
of basic control types. If for each i, 1 ≤ i ≤ k, E is susceptible to constructive (destructive)
Ci control, and E is vulnerable to constructive (destructive) C1 + · · · + Ck control, then for
any nonempty subset K of {C1 , . . . , Ck }, E is vulnerable to the constructive (destructive)
multiprong control involving exactly the prongs from K.
Proof. Let the notation be as in the statement of the proposition, and let K be some
nonempty subset of {C1 , . . . , Ck }. By Theorem 4.3, E is susceptible to constructive (destructive) multiprong control by the constructive (destructive) control type consisting of
exactly the prongs from K. By E’s vulnerability to constructive (destructive) C1 + · · · + Ck
control, we have a polynomial-time algorithm for constructive (destructive) multiprong control involving exactly the prongs from K: It suffices to use the algorithm for constructive
(destructive) C1 + · · · + Ck multiprong control, with the bounds on the extent to which
nonoccurring prongs can be used set to 0.
Motivated by the above discussion and by Proposition 4.9, in Sections 4.3 and 5 we
will show that plurality, Condorcet, Copelandα (for each rational α, 0 ≤ α ≤ 1), approval,
318

Multimode Control Attacks on Elections

and maximin are indeed constructive vulnerability-combining and destructive vulnerabilitycombining. Thus for these systems we will have analyzed all 25 − 1 constructive cases and
all 25 − 1 destructive cases of multiprong control. Namely, for a constructive (destructive)
vulnerability-combining election system E and for any variant C1 + · · · + Ck , 1 ≤ k ≤ 5, of
constructive (destructive) multiprong control such that for each i, 1 ≤ i ≤ k, E is either
resistant, vulnerable, or immune to constructive (destructive) Ci control, we can use the
following simple rule (which we will refer back to as Classification Rule A) to classify
constructive (destructive) C1 + · · · + Ck multiprong control (note: the correctness of the
third part of this classification is where the vulnerability-combining property of E is being
relied on):
1. If E is resistant to at least one of the control prongs, then it is resistant to the whole
attack.
2. If E is immune to each of the prongs, then it is immune to the whole attack.
3. If neither of the above holds, then E is vulnerable to the whole attack.
In general, we do not consider partition cases of control in this paper. However, we make
an exception for the next example, which shows how even types of control to which a given
election system is immune may prove useful in multiprong control. In constructive control
by partition of candidates (reminder: this is not a basic control type) in the ties-eliminate
model (PC-TE control type), we are given an election E = (C, V ) and a preferred candidate
p ∈ C, and we ask whether it is possible to find a partition (C1 , C2 ) of C (i.e., C1 ∪ C2 = C
and C1 ∩ C2 = ∅) such that p is a winner of the following two-round election: We first find
the winner sets, W1 and W2 , of elections (C1 , V ) and (C2 , V ). If W1 (W2 ) contains more
than one candidate, we set W1 = ∅ (W2 = ∅), since we are in the “ties eliminate” model.
The candidates who win election (W1 ∪ W2 , V ) are the winners of the overall two-stage
election.
Now, let us look at constructive approval-AC+PC-TE control, where (by definition, let
us say) we first add new candidates and then perform the partition action. We consider an
approval election with two candidates, p and c, where p has 50 approvals and c has 100.
We are also allowed to add candidate c0 , who has 100 approvals. Note that it is impossible
to make p a unique winner by adding c0 . Exercising the partition action alone does not
ensure p’s victory either. However, combining both AC and PC-TE does the job. If we first
add c0 to the election and then partition candidates into {p} and {c, c0 } then, due to the
ties-eliminate rule, p becomes the unique winner. It is rather interesting that even though
approval is immune to constructive AC control, there are cases where one has to apply AC
control to open the possibility of effectively using other types of control.
The above example is perhaps surprising in light of Theorem 4.5. In essence, in the proof
of that theorem we argue that if an election system is vulnerable to some basic control type
C but is immune to some other basic control type D, then it is also vulnerable to control
type C + D. We proved the theorem by showing that we can safely disregard the actions
of type D (assuming C does not include BV control type). The above example shows that
this proof approach would not work if we considered PC-TE in addition to the basic control
types.
319

Faliszewski, Hemaspaandra, & Hemaspaandra

4.3 Combining Vulnerabilities
In the previous section we considered the case where separate prongs of a multiprong control
problem have different computational properties, e.g., some are resistant, some are vulnerable, and some are immune. In this section we consider the case where an election system is
vulnerable to each prong separately, and we show how such vulnerabilities combine within
election systems for which control results were obtained in previous papers (see Table 5).
In particular, in the next theorem we show that for all the election systems considered
by Bartholdi et al. (1992), Hemaspaandra et al. (2007), and Faliszewski, Hemaspaandra,
Hemaspaandra, and Rothe (2009a), all constructive vulnerabilities to AC, DC, AV, DV,
and BV combine to vulnerabilities, and all destructive vulnerabilities to AC, DC, AV, DV,
BV combine to vulnerabilities.11 Using this (and some additional discussion to correctly
handle the possibility that P = NP), we will soon conclude that each election system
studied in these three papers is both constructive vulnerability-combining and destructive
vulnerability-combining.
Theorem 4.10. (a) Plurality is vulnerable to both constructive AV+DV+BV control and
destructive AV+DV+BV control. (b) Both Condorcet and approval are vulnerable to
AC+AV+DV+BV destructive control. (c) For each rational α, 0 ≤ α ≤ 1, Copeland α
is vulnerable to destructive AC+DC control.12
Proof. (a) Let us consider an instance I of constructive plurality-AV+DV+BV control
where we want to ensure candidate p’s victory: It is enough to add all the voters who vote
for p (or as many as we are allowed) and then, in a loop, keep deleting voters who vote
for a candidate other than p with the highest score, until p is the only candidate with the
highest score or we have exceeded our limit of voters to delete. Finally, in a loop, keep
bribing voters who vote for a candidate other than p with the highest score to vote for p,
until p is the only candidate with the highest score or we have exceeded our limit of voters
to bribe. If p becomes a unique winner via this procedure, then accept. Otherwise reject.
We omit the straightforward proof for the destructive case.
(b) Let I be an instance of destructive Condorcet-AC+AV+DV+BV, where our goal is
to prevent candidate p from being a Condorcet winner (we assume that p is a Condorcet
winner before any control action is performed). It is enough to ensure that some candidate
c wins a head-to-head contest with p. Our algorithm works as follows.
Let C be the set of candidates originally in the election and let A be the set of candidates
that we can add (we take A = ∅ if we are not allowed to add any candidates). For each
c ∈ (C ∪ A) − {p} we do the following:
1. Add as many voters who prefer c to p as possible.
11. Constructive bribery for plurality and constructive bribery for approval have been considered by Faliszewski, Hemaspaandra, and Hemaspaandra (2009a) and constructive and destructive bribery for Copeland
has been studied by Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2009a). In Theorem 4.10
we—in effect—give polynomial-time algorithms for destructive bribery in plurality, approval, and
Condorcet. Constructive Condorcet-BV is NP-complete and this is implicitly shown by Faliszewski,
Hemaspaandra, Hemaspaandra, and Rothe (2009a, Theorem 3.2).
12. Regarding the types among AC, DC, AV, DV, and BV not mentioned in each part of the theorem,
plurality is resistant to constructive and destructive AC and DC, Condorcet and approval are immune
to constructive AC and destructive DC, and for each rational α, 0 ≤ α ≤ 1, Copelandα is resistant to all
five constructive basic types of control and to destructive AV, DV, and BV (see Table 5 for references).

320

Multimode Control Attacks on Elections

2. Delete as many voters who prefer p to c as possible.
3. Among the remaining voters who prefer p to c, bribe as many as possible to rank c
first.
If after these actions c wins his or her head-to-head contest with p then we accept. If no
c ∈ (C ∪ A) − {p} leads to acceptance, then we reject. It is straightforward to see that this
algorithm is correct and runs in polynomial time. (We point out that it is enough to add
only a single candidate, the candidate c that prevents p from winning, if he or she happens
to be a member of A).
For the case of approval, our algorithm works similarly, except the following differences:
We add voters who approve of c but not of p. We delete voters who approve of p but not of
c. For each remaining voter vi , if we still have not exceeded our bribing limit, if vi approves
of p but not of c, we bribe vi to reverse approvals on p and c. (Note that if we do not
exceed our bribing limit by this procedure, this means that each voter that approves of p
also approves of c and thus p is not a unique winner.) If these actions lead to p not being
a unique winner, we accept. If we do not accept for any c ∈ (C ∪ A) − {p}, we reject.
(c) The idea is to combine Copelandα destructive-AC and destructive-DC algorithms (Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009a). We give the full
proof for the sake of completeness.
Let us fix a rational value α, 0 ≤ α ≤ 1. Given an election E and a candidate c in
this election, we write scoreαE (c) to denote Copelandα score of c. Let I be an instance
of destructive Copelandα -AC+DC control, with an election E = (C, V ), where we can
add at most kAC spoiler candidates from the set A, and where we can delete at most kDC
candidates. Our goal is to ensure that some despised candidate d ∈ C is not a unique winner.
Our algorithm is based on the following simple observation of Faliszewski, Hemaspaandra,
Hemaspaandra, and Rothe (2009a). For each candidate c ∈ C:
scoreα(C,V ) (c) =

X

scoreα({c,c0 },V ) (c).

c0 ∈C−{c}

Our goal is to prevent candidate d from being a unique winner. If d is not a unique winner,
we immediately accept. Otherwise, we seek a candidate c ∈ C ∪ A such that we can ensure
that c’s score is at least as high as that of d. Thus for each c ∈ C ∪ A we do the following.
1. If c ∈ A, and kAC > 0, we add c to the election (and if c ∈ A but kAC = 0, we proceed
to the next c).
2. As long as we can still add more candidates, we keep executing the following operation:
If there is a candidate c0 ∈ A such that value a(c0 ) = scoreα({c,c0 },V ) (c)−scoreα({d,c0 },V ) (d)
is positive, we add a candidate c00 ∈ A, for whom a(c00 ) is highest.
3. As long as we can still delete candidates, we keep executing the following operation: If
there is a candidate c0 ∈ C such that value r(c0 ) = scoreα({d,c0 },V ) (d) − scoreα({c,c0 },V ) (c)
is positive, we delete a candidate c00 ∈ C, for whom r(c00 ) is highest.
4. If after these steps d is not a unique winner, we accept.
321

Faliszewski, Hemaspaandra, & Hemaspaandra

If we do not accept for any c ∈ C ∪ A, we reject.
It is straightforward to see that we never delete a candidate that we have added. Also, it
is straightforward to see that the algorithm works in polynomial time, and that it is correct.
Correctness follows from the fact that (a) in the main loop of the algorithm, when dealing
with candidate c ∈ C ∪ A, each addition of a candidate and each deletion of a candidate
increases the difference between the score of c and the score of d as much as is possible, and
(b) the order of adding/deleting candidates is irrelevant.
From the above theorem, the comments preceding it, and a bit of care regarding the
possibility that P = NP, we obtain the following claim.
Theorem 4.11. Plurality, Condorcet, Copeland α (for each rational α, 0 ≤ α ≤ 1), and approval are both constructive vulnerability-combining and destructive vulnerability-combining.
Proof. Immune prongs are never vulnerable. If P 6= NP then resistant prongs cannot be
vulnerable, and we are already done by the previous theorem. If P = NP, then it is possible
that resistant prongs are also vulnerable. Indeed, for all the systems under discussion in
this proof, all their resistant prongs under basic control types happen to be in NP and are
vulnerable if P = NP. However, it is straightforward to see that if P = NP, then for these
particular election systems it holds that all of their vulnerable constructive (destructive)
basic prongs—which for these will in that case be all their nonimmune basic constructive
(destructive) prongs—when combined yield a multiprong constructive (destructive) control
type for which vulnerability holds.
As established by Theorem 4.11 and the (soon-to-come) results of Section 5, all natural
election systems that we have discussed so far in this paper are vulnerability-combining both
in the constructive setting and in the destructive setting. It is natural to wonder whether
this is a necessary consequence of our model of multiprong control or whether in fact there is
an election system for which combining two control types to which the system is vulnerable
yields a multipronged control problem to which the system is resistant. Theorem 4.12 shows
that the latter is the case, even for a natural election system.
In the thirteenth century, Ramon Llull proposed an election system that could be used
to choose popes and leaders of monastic orders (see Hägele & Pukelsheim, 2001; McLean
& Lorrey, 2006). In his system, voters choose the winner from among themselves (so, the
candidates are the same as the voters). Apart from that, Llull’s voting system is basically
Copeland1 , the version of Copeland that most richly rewards ties. Formally, we define the
voting system OriginalLlull as follows: For an election E = (C, V ), if the set of names
of V , which we will denote by names(V ), is not equal to C, then there are no winners.
Otherwise, a candidate c ∈ C is a winner if and only if it is a Copeland1 winner. Note that
single-prong AC and AV control for OriginalLlull do not make all that much sense, and so it
should come as no surprise that OriginalLlull is vulnerable to both constructive AC control
and constructive AV control. In addition, we will show (by renaming and padding) that
Copeland1 -AV can be be reduced to OriginalLlull1 -AC+AV. Since Copeland1 is resistant
to constructive control by adding voters (Faliszewski, Hemaspaandra, Hemaspaandra, &
Rothe, 2009a), this then leads to the following theorem.
Theorem 4.12. OriginalLlull is vulnerable to both constructive AC control and constructive
AV control but is resistant to constructive AC+AV control.
322

Multimode Control Attacks on Elections

Proof. It is immediate that OriginalLlull is susceptible to constructive AC, AV, and
(by Theorem 4.3) AC+AV control. It is also straightforward to see that constructive
OriginalLlull-AC (AV) control is in P: If possible add candidates (voters) such that the
set of voter names is equal to the set of candidates, and then check if the preferred candidate is a unique Copeland1 winner. If this is not possible, reject.
We will now show, via a reduction from constructive Copeland1 -AV control (which is
NP-hard by Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009a) that constructive
OriginalLlull-AC+AV control is NP-hard. Let C be a set of candidates, let V and W be
two disjoint collections of voters with preference lists over C, let p ∈ C be the preferred
candidate, and k be an element of N. The question is whether there exists a subcollection
W 0 ⊆ W of size at most k such that p is a unique Copeland1 winner of (C, V ∪W 0 ). Without
loss of generality, we assume that V is not empty.
We will now show how to pad this election. For an OriginalLlull election to be nontrivial,
we certainly need to have the same number of candidates as voters (later, we will also rename
the voters so that they are the same as the candidates). If kV k < kCk, we want to add a
collection of new dummy voters V 0 such that kV k + kV 0 k = kCk and such that adding V 0
to an election does not change the relative Copeland1 scores of the candidates. This can be
accomplished by letting half of the voters in V 0 vote C (recall Convention 3.1) and half of
←
−
the voters in V 0 vote C . Of course, this can only be done if kV 0 k is even.
So, we will do the following. If kV k < kCk, we add a collection of new voters V 0 such
that kV 0 k = kCk − kV k if kCk − kV k is even, and kV 0 k = kCk − kV k + 1 if kCk − kV k
is odd. If kV k ≥ kCk, we let V 0 = ∅. Half of the voters in V 0 vote C and half of
←
−
the voters in V 0 vote C . In addition, we introduce a set A of new candidates such that
kCk + kAk = kV k + kV 0 k + kW k. Note that this is always possible, since kV k + kV 0 k ≥ kCk.
We extend the votes of the voters (in V , V 0 , and W ) to C ∪ A by taking their preference
order on C and following this by the candidates in A in some fixed, arbitrary order. Note
that this will have the effect that candidates in A will never be winners.
Let W 0 ⊆ W , A0 ⊆ A, E = (C, V ∪ W 0 ), E 0 = (C ∪ A0 , V ∪ V 0 ∪ W 0 ). It is straightforward
to see that the following hold (recall that V is not empty).
1. For all d ∈ A0 , score1E 0 (d) ≤ kA0 k − 1.
2. For all c ∈ C, score1E 0 (c) = score1E (c) + kA0 k.
3. For all c, c0 ∈ C, c 6= c0 , score1E (c) − score1E (c0 ) = score1E 0 (c) − score1E 0 (c0 ).
4. p is a unique Copeland1 winner of E if and only if p is a unique Copeland1 winner of
E0.
We are now ready to define the reduction. Name the voters such that names(V ∪V 0 ) ⊇ C
and names(V ∪ V 0 ∪ W ) = C ∪ A. Then map (C, V, W, p, k) to (C, A, V ∪ V 0 , W, p, kAk, k).
We claim that p can be made a unique Copeland1 winner of (C, V ) by adding at most k
voters from W if and only if p can be made a unique OriginalLlull winner of (C, V ∪ V 0 ) by
adding (an unlimited number of) candidates from A and at most k voters from W .
First suppose that W 0 is a subcollection of W of size at most k such that p is the
unique Copeland1 winner of (C, V ∪ W 0 ). Let A0 ⊆ A be the set of candidates such that
323

Faliszewski, Hemaspaandra, & Hemaspaandra

C ∪ A0 = names(V ∪ V 0 ∪ W 0 ). By item 4 above, p is the unique Copeland1 winner of
(C ∪ A0 , V ∪ V 0 ∪ W 0 ), and thus p is the unique OriginalLlull winner of (C ∪ A0 , V ∪ V 0 ∪ W 0 ).
For the converse, suppose that there exist A0 ⊆ A and W 0 ⊆ W such that kW 0 k ≤ k,
and p is the unique OriginalLlull winner of (C ∪ A0 , V ∪ V 0 ∪ W 0 ). Then p is the unique
Copeland1 winner of (C ∪A0 , V ∪V 0 ∪W 0 ), and, by item 4, p is the unique Copeland1 winner
of (C, V ∪ W 0 ).
Thus our reduction is correct and, since it can be computed in polynomial time, the
proof is complete.
We have the following corollary, since OriginalLlull is neutral (permuting the names of
the candidates, evaluating the election’s winners, and then running the winners through
the inverse of the permutation does not affect the outcome of the election) and anonymous
(permuting the names of the voters does not affect the outcome of the election).13
Corollary 4.13. There exists a neutral and anonymous election system E such that E is
vulnerable to both constructive AC control and constructive AV control but is resistant to
constructive AC+AV control.
Something might seem a bit strange about Theorem 4.12. After all, it says that a
powerful chair—one who can both add candidates and add voters—faces a harder task than
would be faced by a weaker chair—say, one who can just add candidates. The important
thing to keep in mind, to understand why this is not strange, is that the different chairs
are facing (correspondingly) different problems. The powerful type of chair is being asked
to determine whether by specified amounts of adding candidates and voters a goal can be
met, and the weaker type of chair is being asked whether by a specified amount of adding
candidates a goal can be met. Thus it is not at all paradoxical for the former problem
to have higher complexity than the latter. (After all, a “powerful” solution-finder—one
allowed to use any assignment—seeking to find a satisfying assignment to input Boolean
formulas is facing an NP-hard task, but a “weak” solution-finder—one only allowed to find
solutions with at most 2011 variables assigned “False”—seeking to find for input Boolean
formulas a satisfying assignment, when such exists, that has at most 2011 variables assigned
“False” faces just a polynomial-time task, due to the natural brute-force approach.)
4.4 Summary
We now summarize the main contributions of Section 4. We will not try here to motivate
or interpret our results, but rather will summarize our results “as they are,” so the reader
can easily jump back to this section for reference.
13. The notion of anonymity just stated is the standard one in the literature. To avoid any confusion, we
mention that an earlier version (Faliszewski, Hemaspaandra, & Hemaspaandra, 2010a) of this paper used
a much stronger definition of anonymity—one that required one to be able to not just permute the voter
names but to one-to-one map them to any set of names and yet have the outcome not change. Let us
call that notion voter-superanonymity. OriginalLlull does not satisfy that stronger notion. However, the
earlier version of this paper—by sneakily building the preference orders of the voters into the names of
the candidates—constructed a highly artificial system that was neutral, anonymous (in the sense of the
present paper), and voter-superanonymous, and that had two vulnerable prongs that when combined
yielded resistance (Faliszewski et al., 2010a). In contrast, Theorem 4.12/Corollary 4.13 provides such
a jump from vulnerability to resistance for a preexisting, natural voting system that is neutral and
anonymous.

324

Multimode Control Attacks on Elections

C1
R
R
R
I
I
V

C2
R
V
I
I
V
V

C1 + C2
R
R
R
I
V
susceptible (i.e., not I)

Table 2: An example of applying Theorem 4.7. Let E be some election system. We consider
two basic types of constructive (destructive) control, C1 and C2 , to which E is
either resistant (R), immune (I), or vulnerable (V). The table shows how C1 + C2
control for E inherits properties from its prongs. Note that Theorem 4.7 does
not on its own give results for the case when E is vulnerable to both C1 and C2
(although from Theorem 4.3 susceptibility must hold). To see this, note that for
vulnerability-combining systems putting together two vulnerable prongs leads to
a two-pronged control problem to which the system is vulnerable. But there are
examples of voting systems where combining two vulnerable prongs leads to a
resistant two-pronged control problem (see Theorem 4.12).

In Section 4.1, we introduced our model of multiprong control, which allows the attacker
to use several types of control jointly, either to try to make a given candidate a unique winner
(constructive control), or to try to prevent a given candidate from being a unique winner
(destructive control).
In Section 4.2 we focused on the following issue: Let E be an election system. Let
C1 + · · · + Ck be a variant of multiprong control (so 1 ≤ k ≤ 5 and each Ci is a basic
control type). Suppose we know, for each Ci , 1 ≤ i ≤ k, whether E is resistant, immune,
or vulnerable to constructive (destructive) control type Ci . To what extent can we from
this alone tell whether E is resistant, immune, or vulnerable to constructive (destructive)
C1 + · · · + Ck multiprong control? Theorem 4.7 and the paragraphs following it provide a
detailed answer. As an example of applying Theorem 4.7, in Table 2 we show how properties
of two control prongs, C1 and C2 , combine into properties of two-pronged C1 + C2 control.
Simply stated, Theorem 4.7 (for those systems where each prong happens to be immune
or vulnerable or resistant—and that should include essentially all reasonable, natural election systems having polynomial-time winner problems) gives a read-off-the-answer classification for all the multiprong cases, except if the system has multiple vulnerable constructive
prongs or has multiple vulnerable destructive prongs. And to handle that case, the case
of systems (where each prong happens to be immune or vulnerable or resistant) having
multiple vulnerable constructive (destructive) prongs, we defined the notion of constructive (destructive) vulnerability-combining election systems, and provided as Classification
Rule A a simple rule that classifies all multiprong control cases. We also provided a handy
tool, Proposition 4.9, for proving that an election system is vulnerability-combining. We
conjecture that almost all natural systems are vulnerability-combining.
Finally, in Section 4.3 we considered several natural election systems for which control
has already been studied (namely, plurality, Condorcet, Copelandα (for each rational α,
325

Faliszewski, Hemaspaandra, & Hemaspaandra

Election system

Plurality
Condorcet
Copelandα , for each 0 ≤ α ≤ 1
Approval
Maximin

Our results that combine into multiprong vulnerability all immune and all vulnerable entries
in each column of Table 5
Constructive
Destructive
AV+DV+BV
AV+DV+BV
AC+DC
AC+DC+AV+DV+BV
(none)
AC+DC
AC+DC
AC+DC+AV+DV+BV
DC
AC+DC

Table 3: Summary of our vulnerability results for multiprong control for plurality, Condorcet, Copelandα , and approval voting systems. Regarding the five basic control
types, these summarize that for each column of Table 5 that has at least one
vulnerable entry, we have established that the multiprong combination of all the
immunity and vulnerability entries in that column remains vulnerable. These results are obtained by combining Theorem 4.10 and Theorem 4.7 (with Theorem 4.7
letting us add in basic control types to which a given system is immune). For the
sake of completeness, we have also included the analogous results regarding maximin, from Section 5.

0 ≤ α ≤ 1), and approval), and for each of these systems we proved that the system
is both constructive vulnerability-combining and destructive vulnerability-combining (see
Theorem 4.11 for the formal result; but Table 3 summarizes the actual combinations that are
in play here unless P = NP). Nonetheless, in Theorem 4.12 we have shown that the ancient
election system OriginalLlull has two vulnerable prongs that combine to yield resistance;
unless P = NP, OriginalLlull is not vulnerability-combining.

5. Control in Maximin
In this section we initiate the study of control in the maximin election system. Maximin
is loosely related to Copelandα voting in the sense that both are defined in terms of the
pairwise head-to-head contests. In addition, the unweighted coalitional manipulation problem for maximin and Copelandα (α 6= 0.5) exhibits the same behavior: It is in P for one
manipulator and NP-complete for two or more manipulators (Xia, Zuckerman, Procaccia,
Conitzer, & Rosenschein, 2009; Faliszewski, Hemaspaandra, & Schnoor, 2008, 2010). Thus
one might wonder whether both systems will be similar with regard to their resistances to
control. In fact, there are very interesting differences.
It is straightforward to see that maximin is susceptible to all basic types of constructive
and destructive control. And so, by Theorem 4.3, to show vulnerability to constructive
(destructive) C control it suffices to give a polynomial-time algorithm that decides the
constructive (destructive) E-C problem, and to show resistance to constructive (destructive)
C control it suffices to show that the constructive (destructive) E-C problem is NP-hard.
As a consequence of the analysis given in the following subsections, we have that maximin
is both constructive vulnerability-combining and destructive vulnerability-combining.
326

Multimode Control Attacks on Elections

Theorem 5.1. Maximin is both constructive vulnerability-combining and destructive
vulnerability-combining.
Proof. The same discussion and analysis provided in the proof of Theorem 4.11 apply here,
except relying on the underpinning work provided later in this section. The constructive
case here is degenerate, as among basic prongs there is only one that is of interest (although
we will briefly discuss a special “extra” prong later). Even for the destructive case, there
are only two basic prongs that are not resistant.
5.1 Candidate Control in Maximin
Let us now focus on candidate control in maximin, that is, on the AC and the DC control
types, both in the constructive and in the destructive setting. As is the case for Copelandα ,
0 ≤ α ≤ 1, maximin is resistant to control by adding candidates.
Theorem 5.2. Maximin is resistant to constructive AC control.
Proof. We give a reduction from X3C. Let (B, S), where B = {b1 , . . . , b3k } is a set of
3k elements and S = {S1 , . . . , Sn } is a set of 3-subsets of B, be our input X3C instance.
We form an election E = (C ∪ A, V ), where C = B ∪ {p}, A = {a1 , . . . , an }, and V =
(v1 , . . . , v2n+2 ). (Candidates in A are the spoiler candidates, which the attacker has the
ability to add to election (C, V ).)
Voters in V have the following preferences. For each Si ∈ S, voter vi reports preference
←−−−−−−
list p > B − Si > ai > Si > A − {ai } and voter vn+i reports preference list A − {ai } > ai >
←
− ←−−−−
←
−
←
−
Si > B − Si > p. Voter v2n+1 reports p > A > B and voter v2n+2 reports B > p > A .
We claim that there is a set A0 ⊆ A such that kA0 k ≤ k and p is a unique winner of
(C ∪ A0 , V ) if and only if (B, S) is a “yes” instance of X3C.
To show the claim, let E 0 = (C, V ). For each pair of distinct elements bi , bj ∈ B, we have
that NE 0 (bi , bj ) = n+1, NE 0 (p, bi ) = n+1, and NE 0 (bi , p) = n+1. That is, all candidates in
E 0 tie. Now consider some set A00 ⊆ A, kA00 k ≤ k, and an election E 00 = (C ∪ A00 , V ). Values
of NE 00 and NE 0 are the same for each pair of candidates in {p}∪B. For each pair of distinct
elements ai , aj ∈ A00 , we have NE 00 (p, ai ) = n + 2, NE 00 (ai , p) = n, and NE 00 (ai , aj ) = n + 1.
For each bi ∈ B and each aj ∈ A00 we have that

NE 00 (bi , aj ) =

n
n+1

if bi ∈ Sj ,
if bi ∈
/ Sj ,

and, of course, NE 00 (aj , bi ) = 2n + 2 − NE 00 (bi , aj ). Thus, by definition of maximin, we have
the following scores in E 00 : (a) scoreE 00 (p) = n + 1, (b) for each aj ∈ A00 , scoreE 00 (aj ) = n,
and (c) for each bi ∈ B,

n
if (∃aj ∈ A00 )[bi ∈ Sj ],
scoreE 00 (bi ) =
n + 1 otherwise.
A00 corresponds to a family S 00 of 3-sets from S such that for each j, 1 ≤ j ≤ n, S 00
contains set Sj if and only if A00 contains aj . Since kA00 k ≤ k, it is straightforward to see
that p is a unique winner of E 00 if and only if S 00 is an exact cover of B.
327

Faliszewski, Hemaspaandra, & Hemaspaandra

Copelandα , 0 ≤ α ≤ 1, is resistant to constructive AC control, but for α ∈ {0, 1},
Copelandα is vulnerable to constructive control by adding an unlimited number of candidates. It turns out that so is maximin. However, interestingly, in contrast to Copeland,
maximin is also vulnerable to DC control.
Rather than just proving that, we will prove a bit more: We will just for this moment
discuss a different control type, ACu , that we will not consider to be one of the five basic
types. ACu control (called control by adding an unlimited number of candidates) is just
like control by adding candidates, except there (by definition) is no limit on the number
of candidates to add—in effect, one requires kAC = kAk.14 We will show that maximin is
vulnerable not just to DC control but in fact even to ACu +DC control. Intuitively, in constructive ACu +DC control we should add as many candidates as possible (because adding
a candidate generally decreases other candidates’ scores, making our preferred candidate’s
way to victory easier) and then delete those candidates who stand in our candidate’s way
(i.e., those whose existence blocks the preferred candidate’s score from increasing). Studying
constructive ACu +DC control for maximin jointly leads to a compact, coherent algorithm.
If we were to consider both control types separately, we would have to give two fairly similar
algorithms while obtaining a weaker result.
Theorem 5.3. Maximin is vulnerable to constructive ACu +DC control.
Proof. We give a polynomial-time algorithm for constructive maximin-ACu +DC control.
The input contains an election E = (C, V ), a set of spoiler candidates A, a preferred
candidate p ∈ C, and a nonnegative integer kDC . Voters in V have preference lists over the
candidates in C ∪ A. We ask whether there exist sets A0 ⊆ A and C 0 ⊆ C such that (a)
kC 0 k ≤ kDC and (b) p is a unique winner of election ((C − C 0 ) ∪ A0 , V ). If kDC ≥ kCk − 1,
we accept immediately because we can delete all candidates but p. Otherwise, we use the
following algorithm.
Preparation. We rename the candidates in C and A so that C = {p, c1 , . . . , cm } and A =
{cm+1 , . . . , cm+m0 }. Let E 0 = (C ∪ A, V ) and let P = {NE 0 (p, ci ) | ci ∈ C ∪ A}. That
is, P contains all the values that candidate p may obtain as scores upon deleting some
candidates from E 0 . For each k ∈ P , let Q(k) = {ci | ci ∈ C ∪A−{p}∧NE 0 (p, ci ) < k}.
Intuitively, Q(k) is the set of candidates in E 0 that prevent p from having at least k
points.
14. The control type ACu is of some historical interest as it was used as the control by adding candidates
notion in the seminal paper on control. However, following a suggestion of Faliszewski, Hemaspaandra,
Hemaspaandra, and Rothe (2007), AC has been used in most work in recent years; it is the more natural
choice since it is analogous to the other three add/delete voter/candidate control types. In addition to
Theorem 5.3’s result about the constructive case, we mention in passing that maximin’s vulnerability to
destructive AC (see Theorem 5.4 in light of Proposition 4.9) implies that it is also vulnerable to ACu .
Readers wishing to know what results hold for the prong ACu for each of the election systems covered
in this paper can find that summarized in a table of Faliszewski et al. (2010a)—except for the α 6= 0.5
cases of Copelandα , and for those cases Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2009a)
can be referred to. Our entire set of tools in Section 4.2 is framed around the five basic control types, so
we do not here try to weave in ACu into the framework. We mention that tools apply well to this type
also, and so it would prove no special hurdle to incorporate it into the framework. Still, we feel that
AC (not ACu ) is by far the more attractive way to frame control by adding candidates and so such an
addition is not compelling.

328

Multimode Control Attacks on Elections

Main loop. For each k ∈ P , our algorithm tests whether by deleting at most kDC candidates from C and any number of candidates from A it is possible to ensure that p
obtains exactly k points and becomes a unique winner of E 0 . Let us fix some value
k ∈ P . We build a set D of candidates to delete. Initially, we set D = Q(k). It is
straightforward to see that deleting candidates in Q(k) is a necessary and sufficient
condition for p to have score k. However, deleting candidates in Q(k) is not necessarily
sufficient to ensure that p is a unique winner because candidates with scores greater
or equal to k may exist. We execute the following loop (which we will call the fixing
loop):
1. Set E 00 = ((C ∪ A) − D, V ).
2. Pick a candidate d ∈ (C ∪ A) − D such that scoreE 00 (d) ≥ k (break from the loop
if no such candidate exists).
3. Add d to D and jump back to Step 1.
We accept if C ∩ D ≤ kDC and we proceed to the next value of k otherwise.15 If none
of the values k ∈ P leads to acceptance then we reject.
Let us now briefly explain why the above algorithm is correct. It is straightforward to see
that in maximin adding some candidate c to an election does not increase other candidates’
scores, and deleting some candidate d from an election does not decrease other candidates’
scores. Thus, if after deleting candidates in Q(k) there still are candidates other than p with
k points or more, the only way to ensure p’s victory—without explicitly trying to increase
p’s score—is by deleting those candidates. Also, note that the only way to ensure that p
has exactly k points is by deleting candidates Q(k).
Note that during the execution of the fixing loop, the score of p might increase to some
value k 0 > k. If that happens, it means that it is impossible to ensure p’s victory while
keeping his or her score equal to k. However, we do not need to change k to k 0 in that
iteration of the main loop as we will consider k 0 in a different iteration.
Maximin is also vulnerable to destructive AC+DC control. The proof relies on the fact
that (a) if there is a way to prevent a despised candidate from winning a maximin election
via adding some spoiler candidates then there is a way to do so by adding at most two
candidates, (b) adding a candidate cannot increase the score of any candidate other than
the added one, and (c) deleting a candidate cannot decrease the score of any candidate
other than the deleted one. In essence, the algorithm performs a brute-force search for
the candidates to add and then uses the constructive maximin-DC control algorithm from
Theorem 5.3.
Theorem 5.4. Maximin is vulnerable to destructive AC+DC control.
Proof. We remind the reader that part of the definition of destructive control by deleting
candidates is that one cannot simply delete one’s despised candidate.
15. If we accept, D implicitly describes the control action that ensures p’s victory: We should delete from
C the candidates in C ∩ D and add from A the candidates in A − D.

329

Faliszewski, Hemaspaandra, & Hemaspaandra

We will first give an algorithm for destructive maximin-AC and then argue how it can
be combined with the algorithm from Theorem 5.3 to solve destructive maximin-AC+DC
in polynomial time.
Let us first focus on the destructive AC problem. Our input is an election E =
(C, V ), where C = {d, c1 , . . . , cm } and V = (v1 , . . . , vn ), a spoiler candidate set A =
{cm+1 , . . . , cm0 }, and a nonnegative integer kAC . The voters have preference orders over
C ∪ A. The goal is to ensure that d is not a unique winner of E via adding at most kAC
candidates from A.
Let us assume that there exists a set A0 ⊆ A such that d is not a unique winner of
election E 0 = (C ∪ A0 , V ). Since d is not a unique winner of E 0 , there exists some candidate
c0 ∈ C ∪ A0 such that scoreE 0 (c0 ) ≥ scoreE 0 (d). Also, by definition of maximin, there is
some candidate d0 ∈ C ∪ A0 such that scoreE 0 (d) = NE 0 (d, d0 ). As a consequence, d is not a
unique winner of election E 00 = (C ∪ {c0 , d0 }, V ). The reason is that scoreE 00 (d) = scoreE 0 (d)
(because both E 0 and E 00 contain d0 ) and scoreE 00 (c0 ) ≥ scoreE 0 (c0 ) (because adding the
remaining A0 − {c0 , d0 } candidates to E 00 does not increase c0 ’s score). Thus, to test whether
it is possible to ensure that d is not a unique winner of E, it suffices to test whether there
is a set A00 ⊆ A such that kA00 k ≤ min(2, kAC ) and d is not a unique winner of (C ∪ A00 , V ).
Note that this test can be carried out in polynomial time.
Let us now consider the AC+DC case. The input and the goal are the same as before,
except that now we are also given a nonnegative integer kDC and we are allowed to delete
up to kDC candidates. We now describe our algorithm. For each set {c0 , d0 } of up to two
candidates, {c0 , d0 } ⊆ (C ∪ A) − {d} we execute the following steps.
1. We check if kA ∩ {c0 , d0 }k ≤ kAC (and we proceed to the next {c0 , d0 } if this is not the
case).
2. We compute a set D ⊆ C − {d, c0 , d0 }, kDk ≤ kDC , that maximizes scoreE 0 (c0 ), where
E 0 = ((C ∪ {c0 , d0 }) − D, V ).
3. If d is not a unique winner of E 0 = ((C ∪ {c0 , d0 }) − D, V ), we accept.
We reject if we do not accept for any {c0 , d0 } ⊆ (C ∪ A) − {d}.
The intended role of d0 is to lower the score of d and keep it at a fixed level, while, of
course, the intended role of c0 is to defeat d. By reasoning analogous to that for the AC
case, we can see that there is no need to add more than two candidates. Thus, given {c0 , d0 },
it remains to compute the appropriate set D. In essence, we can do so in the same manner
as in the constructive AC+DC case.
Let k be some positive integer. We set D(k) = {ci ∈ C − {c0 , d0 , d} | NE (c0 , ci ) < k} and
we pick D = D(i), where i is as large as possible (but no larger than kV k) and kDk ≤ kDC .
Deleting candidates in D maximizes the score of c0 , given that we cannot delete d and d0 (we
cannot delete d by definition of control by deleting candidates, and we cannot—or, more
precisely, do not want to—delete d0 because the role of d0 in our algorithm is to keep the
score of d in check). It is straightforward to see that this D can be computed in polynomial
time.
330

Multimode Control Attacks on Elections

5.2 Control by Adding and Deleting Voters in Maximin
In this section we consider the complexity of constructive and destructive AV and DV control types. (We will consider bribery, BV, in the next section; recall that in this paper,
bribery is a basic control type, though it is usually treated separately in the literature.) In
the previous section we have seen that maximin is vulnerable to all basic types of constructive and destructive candidate control except for constructive control by adding candidates
(constructive AC control). The situation regarding voter control is quite different: As
shown in the next three theorems, maximin is resistant to all basic types of constructive
and destructive voter control.
Theorem 5.5. Maximin is resistant to constructive and destructive AV control.
Proof. We will first give an NP-hardness proof for the constructive case and then we will
describe how to modify it for the destructive case.
We now give a reduction of the X3C problem to the constructive maximin-AV problem.
Our input X3C instance is (B, S), where B = {b1 , . . . , b3k } is a set of 3k distinct elements
and S = {S1 , . . . , Sn } is a family of n 3-element subsets of B. Without loss of generality,
we assume k ≥ 1. Our reduction outputs the following instance. We have an election
E = (C, V ), where C = B ∪ {p, d} and V = (v1 , . . . , v4k ). There are 2k voters with
preference order d > B > p, k voters with preference order p > B > d, and k voters with
preference order p > d > B. In addition, we have a collection W = (w1 , . . . , wn ) of voters
who can be added, where the i’th voter, 1 ≤ i ≤ n, has preference order
B − Si > p > Si > d.
We claim that there is a subcollection W 0 ⊆ W such that kW 0 k ≤ k and p is a unique
winner of election (C, V ∪ W 0 ) if and only if (B, S) is a “yes” instance of X3C.
It is straightforward to verify that for each bi ∈ B it holds that NE (p, bi ) = 2k, and
that NE (p, d) = 2k. Thus scoreE (p) = 2k. Similarly, it is straightforward to verify that
scoreE (d) = 2k, and that for each bi ∈ B, scoreE (bi ) ≤ k. Let W 00 be a subcollection of W
such that kW 00 k ≤ k and let E 00 = (C, V ∪ W 00 ). For each bi ∈ B it holds that scoreE 00 (bi ) ≤
2k. Since each voter in W ranks d as the least desirable candidate, scoreE 00 (d) = 2k. What
is p’s score in election E 00 ? If there exists a candidate bi ∈ B such that there is no voter
wj in W 00 that prefers p to bi , then scoreE 00 (p) = 2k (because NE 00 (p, bi ) = 2k). Otherwise,
scoreE 00 (p) ≥ 2k + 1. Thus p is a unique winner of E 00 if and only if W 00 corresponds to an
exact cover of B. This proves our claim and, as the reduction is straightforwardly seen to
be computable in polynomial time, concludes the proof for the constructive maximin-AC
case.
To show that destructive maximin-AC is NP-hard, we use the same reduction, except
that we remove from V a single voter with preference list p > B > d, and we set the task
to preventing d from being a unique winner. Removing a p > B > d voter from V ensures
that before we start adding candidates, d has score 2k (and this score cannot be changed),
p has score 2k − 1 (and p needs to get one point extra over each other candidate to increase
his or her score and prevent d from being a unique winner), and each bi ∈ B has score
k − 1 (thus no candidate in B can obtain score higher than 2k − 1 via adding no more than
k candidates from W ). The same reasoning as for the constructive case proves that the
reduction correctly reduces X3C to destructive maximin-AV.
331

Faliszewski, Hemaspaandra, & Hemaspaandra

Theorem 5.6. Maximin is resistant to constructive and destructive DV control.
Proof. We will first show NP-hardness for constructive maximin-DV control and then we
will argue how to modify the construction to obtain the result for the destructive case.
Our reduction is from X3C. Let (B, S) be our input X3C instance, where B =
{b1 , . . . , b3k }, S = {S1 , . . . , Sn }, and for each i, 1 ≤ i ≤ n, kSi k = 3. Without loss of
generality, we assume that n ≥ k ≥ 3 (if n < k then S does not contain a cover of B, and
if k ≤ 2 we can solve the problem by brute force). We form an election E = (C, V ), where
0 ), V 00 = (v 00 , . . . , v 00
C = B ∪ {p, d} and where V = V 0 ∪ V 00 , V 0 = (v10 , . . . , v2n
1
2n−k+2 ). For
0
each i, 1 ≤ i ≤ n, voter vi has preference order
d > B − Si > p > Si
0
and voter vn+i
has preference order

←
−
←−−−−
d > Si > p > B − Si .
Among the voters in V 00 we have: 2 voters with preference order p > d > B, n − k voters
with preference order p > B > d, and n voters with preference order B > p > d. We claim
that it is possible to ensure that p is a unique winner of election E via deleting at most k
voters if and only if (B, S) is a “yes” instance of X3C.
Via routine calculation we see that candidates in election E have the following scores:
1. scoreE (d) = 2n (because NE (d, p) = 2n and for each bi ∈ B, NE (d, bi ) = 2n + 2),
2. scoreE (p) = 2n−k +2 (because NE (p, d) = 2n−k +2 and for each bi ∈ B, NE (p, bi ) =
2n − k + 2), and
3. for each bi ∈ B, scoreE (bi ) ≤ 2n − k (because NE (bi , d) = 2n − k).
Before any voters are deleted, d is the unique winner with k − 2 more points than p. Via
deleting at most k voters it is possible to decrease d’s score at most by k points. Let W be
a collection of voters such that p is the unique winner of E 0 = (C, V − W ). We partition
W into W 0 ∪ W 00 , where W 0 contains those members of W that belong to V 0 and W 00
contains those members of W that belong to V 00 . We claim that W 00 is empty. For the
sake of contradiction let us assume that W 00 6= ∅. Let E 00 = (C, V − W 00 ). Since every
voter in V 00 prefers p to d, we have that NE 00 (p, d) = NE (p, d) − kW 00 k and, as a result,
scoreE 00 (p) ≤ scoreE (p)−kW 00 k. In addition, assuming W 00 is not empty, it is straightforward
to observe that scoreE 00 (d) ≥ scoreE (d) − kW 00 k + 1 (the reason for this is that deleting any
single member of V 00 does not decrease d’s score). That is, we have that:
scoreE 00 (p) ≤ 2n − k + 2 − kW 00 k,
scoreE 00 (d) ≥ 2n + 1 − kW 00 k.
So in E 00 , d has at least k − 1 more points than p. Since kW 00 k ≥ 1, we can delete at most
k − 1 voters W 0 from election E 00 . But then p will not be a unique winner of E 0 , which is a
contradiction.
Thus W contains members of V 0 only. Since d is ranked first in every vote in V 0 , deleting
voters from W decreases d’s score by exactly kW k. Further, deleting voters W certainly
decreases p’s score by at least one point. Thus after deleting voters W we have:
332

Multimode Control Attacks on Elections

1. scoreE 0 (d) = 2n − kW k,
2. scoreE 0 (p) ≤ 2n − k + 2 − 1 = 2n − k + 1.
In consequence, the only possibility that p is a unique winner after deleting voters W is
that kW k = k and we have equality in item 2 above. It is straightforward to verify that
this equality holds if and only if W contains k voters among v10 , . . . , vn0 that correspond to
an exact cover of B via sets from S (recall that k ≥ 3). This proves that our reduction is
correct, and since the reduction is straightforwardly seen to be computable in polynomial
time, completes the proof of NP-hardness of constructive maximin-DV control.
Let us now consider the destructive case. Let (B, S) be our input X3C instance (with B
and S as in the constructive case). We form election E = (C, V ) which is identical to the one
00
created in the constructive case, except that V 00 = (v100 , . . . , v2n−k
) and we set these voters’
preference orders as follows: There is one voter with preference order p > d > B, n − k
voters with preference order p > B > d, and n − 1 voters with preference order B > p > d.
(That is, compared to the constructive case, we remove one voter with preference order
p > d > B and one with preference order B > p > d.) It is straightforward to see that d is
the unique winner of election E and we claim that he or she can be prevented from being
a unique winner via deleting at most k voters if and only if there is an exact cover of B by
k sets from S.
Via routine calculation, it is straightforward to verify that scoreE (d) = 2n, and that
scoreE (p) = 2n − k. The former holds because NE (d, p) = 2n and NE (d, bi ) = 2n + 1
and the latter holds because NE (p, d) = 2n − k and for each candidate bi ∈ B we have
NE (p, bi ) = 2n − k + 1. In addition, each candidate bi ∈ B has score at most 2n − k − 1.
Thus it is possible to ensure that d is not a unique winner via deleting at most k voters if
and only if there are exactly k voters the deletion of which would decrease the score of d by
k points and would not decrease p’s score. Let us assume that such a collection of voters
exists and let W be such a collection. Since every voter in V 00 prefers p to d, note that W
does not contain any voter in V 00 . Thus W contains exactly k voters from V 0 . Since for
each bi ∈ B we have NE (p, bi ) = 2n − k + 1, for each bi ∈ B W contains at most one voter
who prefers p to bi . Since kBk = 3k and k ≥ 3, this implies that W contains exactly a
collection of voters corresponding to some exact cover of B by sets in S. This completes
the proof for the destructive case.
5.3 Bribery in Maximin
We now move on to bribery in maximin. Given the previous results, it is not surprising that
maximin is resistant both to constructive bribery and to destructive bribery. Our proof is
an application of the “UV technique” of Faliszewski, Hemaspaandra, Hemaspaandra, and
Rothe (2009a). Very informally, the idea is to build an election in a way that ensures that
the briber is limited to bribing only those voters who rank two special candidates ahead of
the preferred one.
Theorem 5.7. Maximin is resistant to constructive and destructive BV control.
Proof. Our proofs follow via reductions from X3C. The reduction for the constructive case
is almost identical the one for the constructive case and thus we will consider both cases in
parallel.
333

Faliszewski, Hemaspaandra, & Hemaspaandra

Our reductions work as follows. Let (B, S) be an instance of X3C, where B =
{b1 , . . . , b3k } is a set of 3k distinct elements, and S = {S1 , . . . , Sn } is a family of 3-element
subsets of B. (Without loss of generality, we assume that n > k > 1. If this is not the case,
it is trivial to verify if (B, S) is a “yes” instance of X3C.) We construct a set of candidates
C = {p, d, s} ∪ B, where p is our preferred candidate (the goal in the constructive setting is
to ensure p is a unique winner) and d is our despised candidate (the goal in the destructive
setting is to prevent d from being a unique winner). We construct six collections of voters,
V 1 , V 2 , V 3 , V 4 , V 5 , V 6 , as follows:
1 . For each i, 1 ≤ i ≤ n, voters v 1 and v 1
1. V 1 contains 2n voters, v11 , . . . , v2n
i
i+n have the
following preference orders:

vi1 : d > s > Si > p > B − Si
←−−−−
←
−
1
vn+i
: B − Si > p > Si > d > s.
2 . For each i, 1 ≤ i ≤ k, voters v 2 and v 2
2. V 2 contains 2k voters, v12 , . . . , v2k
i
i+k have the
following preference orders:

vi2 : s > d > p > B
←
−
2
vk+i
: B > d > p > s.
3 . For each i, 1 ≤ i ≤ k, voters v 3 and v 3
3. V 3 contains 2k voters, v13 , . . . , v2k
i
i+k have the
following preference orders:

vi3 : d > s > p > B
←
−
3
: B > s > p > d.
vk+i
4 . For each i, 1 ≤ i ≤ 2k, voters v 4 and v 4
4. V 4 contains 4k voters, v14 , . . . , v4k
i
i+2k have
the following preference orders:

vi4 : d > B > p > s
←
−
4
v2k+i
: s > p > d > B.
5. V 5 contains 2 voters, v15 , v25 with the following preference orders
v15 : s > B > p > d
←
−
v25 : d > B > p > s.
6. V 6 contains a single voter, v16 , with preference order p > d > s > B.
We form two elections, Ec and Ed , where Ec = (C, V 1 ∪ · · · ∪ V 6 ) and Ed = (C, V 1 ∪
· · · ∪ V 5 ); that is, Ec and Ed are identical except Ed does not contain the single voter from
V 6 . Ec contains 2n + 8k + 3 voters and Ed contains 2n + 8k + 2 voters. Values of NEc and
NEd for each pair of candidates are given in Table 4.
334

Multimode Control Attacks on Elections

(a) Values of NEc (·, ·).

p
d
s
B

p
–
n + 5k + 1
n + 5k + 1
n + 4k + 2

d
n + 3k + 2
–
4k + 1
n + 2k + 1

s
n + 3k + 2
2n + 4k + 2
–
n + 4k + 1

B
n + 4k + 1
n + 6k + 2
n + 4k + 2
≤ n + 4k + 2

(b) Values of NEd (·, ·).

p
d
s
B

p
–
n + 5k + 1
n + 5k + 1
n + 4k + 2

d
n + 3k + 1
–
4k + 1
n + 2k + 1

s
n + 3k + 1
2n + 4k + 1
–
n + 4k + 1

B
n + 4k
n + 6k + 1
n + 4k + 1
n + 4k + 1

Table 4: Values of NEc (·, ·) and NEd (·, ·) for each pair of candidates. Let E be one of Ec , Ed .
An entry in row c0 ∈ {p, d, s} and column c00 ∈ {p, d, s}, c0 6= c00 , of the appropriate
table above gives value NE (c0 , c00 ). For row B and for column B we adopt the
following convention. For each c ∈ {p, d, s} and for each bi ∈ B, an entry in row B
and column c is equal to NE (bi , c). For each c ∈ {p, d, s} and for each bi ∈ B, an
entry in row c and column B is equal to NE (c, bi ). For each two distinct bi , bj ∈ B,
the entry in row B and column B is the upper bound on NE (bi , bj ). (For Ed this
entry is, in fact, exact.)

For the constructive case, we claim that it is possible to ensure that p is a unique winner
of election Ec by bribing at most k voters if and only if (B, S) is a “yes” instance of X3C.
Let us now prove this claim. By inspecting Table 4, and recalling that n > k > 1, we see
that scoreEc (p) = n + 3k + 2, scoreEc (d) = n + 5k + 1, scoreEc (s) = 4k + 1, and for each
bi ∈ B, scoreEc (bi ) ≤ n + 2k + 1. That is, prior to any bribing, d is the unique winner and
p has the second highest score.
It is straightforward to see that by bribing t ≤ k voters, the briber can change each
candidate’s score by at most t points. Thus, for the bribery to be successful, the briber has
to bribe exactly k voters in such a way that d’s score decreases to n + 4k + 1 and p’s score
increases to n + 4k + 2. To achieve this, the briber has to find a collection V 0 of voters such
that kV 0 k = k, and
1. each voter in V 0 ranks p below both d and s, and
2. for each bi ∈ B, there is a voter in V 0 who ranks p below bi .
The only voters that satisfy the first condition are v11 , . . . , vn1 , v12 , . . . , vk2 , v13 , . . . , vk3 . Further,
among these voters only v11 , . . . , vn1 rank p below some member of B and, in fact, for each
i, 1 ≤ i ≤ n, vi1 ranks p below exactly three members of B. Thus it is straightforward to
see that each k voters from v11 , . . . , vn1 , v12 , . . . , vk2 , v13 , . . . , vk3 that satisfy the second condition
correspond naturally to a cover of B by sets from S. (Note that it suffices that the briber
bribes voters in V 0 to rank p first without changing the votes in any other way, and that
335

Faliszewski, Hemaspaandra, & Hemaspaandra

changing the votes in any other way than ranking p first is not necessary.) As a result, if it
is possible to ensure that p is a winner of Ec by bribing at most k voters then (B, S) is a
“yes” instance of X3C. For the other direction, it is straightforward to verify that if (B, S)
is a “yes” instance of X3C then bribing k voters from v11 , . . . , vn1 that correspond to a cover
of B to rank p first suffices to ensure that p is a unique winner. This completes the proof
for the constructive case.
For the destructive case, we claim that it is possible to ensure that d is not a unique
winner of Ed if and only if (B, S) is a “yes” instance of X3C. The proof is analogous to the
constructive case: It suffices to note that p is the only candidate that can possibly tie for
victory with d. The rest of the proof proceeds as for the constructive case.

6. Fixed-Parameter Tractability
In this section we consider the parameterized complexity of multipronged control, in particular, the case where we can assume that the number of candidates is a small constant.
Elections with few candidates are very natural. For example, in many countries presidential
elections involve only a handful of candidates.
The main result of this section is that for many natural election systems E (formally,
for all election systems whose winner determination problem can be expressed via an integer linear program of a certain form), it holds that the E-AC+DC+AV+DV+BV control
problem is fixed-parameter tractable (is in the complexity class FPT) for the parameter
“number of candidates,” both in the constructive setting and in the destructive setting.
This result combines and significantly enhances FPT results from the literature, in particular, from the papers of Faliszewski, Hemaspaandra, and Hemaspaandra (2009a) and
Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2009a), which are the model for
and inspiration of this section. We also make explicit an “automatic” path to such results
that is implicit in the work of the two papers cited in the previous sentence. This path
should be helpful in letting many future analyses be done as tool-application exercises,
rather than being case-by-case challenges.
The present paper is not the only paper to pick up on the type of pattern in the earlier
work just mentioned, and to present a definition and results building on that. Independently
of the present paper, Dorn and Schlotter (2010) have done this in a different, bribery-related
context.
In this section we focus exclusively on the number of candidates as our parameter. That
is, our parameter is the number of candidates initially in the election plus the number of
candidates (if any) in the set of potential additional candidates. That is, in terms of the
variables we have been using to describe multiprong control the parameter is kCk + kAk.
We mention that researchers sometimes analyze other parameterizations. For example,
Liu et al. (2009), Liu and Zhu (2010), and Betzler and Uhlmann (2009) consider as the
parameter the amount of change that one is allowed to use (e.g., the number of candidates one can add), Bartholdi, Tovey, and Trick (1989b), Betzler and Uhlmann (2009), and
Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2009a) study as the parameter the
number of voters (and also sometimes the number of candidates). And other parameters are
sometimes used when considering the so-called possible winner problem (see, e.g., Betzler
& Dorn, 2009; Betzler, Hemmann, & Niedermeier, 2009). However, we view the parameter
336

Multimode Control Attacks on Elections

“number of candidates” as the most essential and the most natural one. We now proceed
with our discussion of fixed-parameter tractability, with the number of candidates as the
parameter.
Let us consider an election system E and a set C = {c1 , . . . , cm } of candidates. There are
exactly m! preference orders over the candidates in C and we will refer to them as o1 , . . . , om! .
Let us assume that E is anonymous (i.e., the winners of each E election do not depend on
the order of votes or the names of the voters, but only—for each preference order oi —on
the number of votes with that preference order). We define predicate win E (cj , n1 , . . . , nm! )
to be true if and only if ci is a unique winner of E elections with C = {c1 , . . . , cm }, where
for each i, 1 ≤ i ≤ m!, there are exactly ni voters with preference order oi . For the rest
of this section, our inequalities always use one of the four operators “>,” “≥,” “<,” and
“≤.”16
Definition 6.1. We say that an anonymous election system E is unique-winner (nonuniquewinner) integer-linear-program implementable if for each set of candidates C = {c1 , . . . , cm }
and each candidate cj ∈ C there exists a set S of linear inequalities with variables n1 , . . . , nm!
such that:
1. If the integer assignment n1 = n̂1 , . . ., nm! = n̂m! satisfies S, then each n̂i belongs to
N,17
2. S can be computed (i.e., obtained) in time polynomial in m!,18 and
3. for each (n̂1 , . . . , n̂m! ) ∈ Nm! , we have that (a) holds if and only if (b) holds, where (a)
and (b) are as follows:
(a) S is satisfied by the assignment n1 = n̂1 , . . ., nm! = n̂m! .
(b) cj is a unique winner (is a winner) of an E election in which for each i, 1 ≤
i ≤ m!, there are exactly n̂i voters with preference order oi , where oi is the i’th
preference order over the set C.
16. We allow both strict and nonstrict inequalities. Since we allow only integer solutions, it is straightforward
to simulate strict inequalities with nonstrict ones and to simulate nonstrict inequalities with strict ones,
in both cases simply by adding a “1” to the appropriate side of the inequality. So we could equally well
have allowed just strict, or just nonstrict, inequalities.
17. It is straightforward to to put m! inequalities into S enforcing this condition. And this condition will
help us make the electoral part of our definition meaningful, i.e., it will avoid having problems from the
restriction in the final part of this definition that lets us avoid discussing negative numbers of voters.
18. We mention in passing that if the m! in this part of the definition were changed to any other computable
mm
function of m, e.g., mm
, we would still obtain FPT results, and still would have them hold even
in the strengthened version of FPT in which the f of “f (parameter) · InputsizeO(1) ” is required to be
computable. However, due to m! being the number of preference orders over m candidates, having S be
obtainable in time polynomial in m! will in practice be a particularly common case.
We also mention in passing that the FPT-establishing framework in this section and the results it
yields, similarly to the case in our work mentioned earlier (Faliszewski, Hemaspaandra, & Hemaspaandra, 2009a; Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009a), not only will apply in the
model where votes are input as a list of ballots, one per person, but also will hold in the so-called
“succinct” model (see Faliszewski, Hemaspaandra, & Hemaspaandra, 2009a; Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009a), in which we are given the votes not as individual ballots but as
binary numbers providing the number of voters having each preference order (or having each occurring
preference order).

337

Faliszewski, Hemaspaandra, & Hemaspaandra

In a slight abuse of notation, for integer-linear-program implementable election systems E we will simply refer to the set S of linear inequalities from Definition 6.1 as
win E (cj , n1 , . . . , nm! ). The particular set of candidates will always be clear from context.
Naturally, it is straightforward to adapt Definition 6.1 to apply to approval voting, but for
the sake of brevity we will not do so.
We are not aware of any natural systems that are integer-linear-program unique-winner
implementable yet not integer-linear-program nonunique-winner implementable, or vice
versa. In this paper we focus on the unique winner model so the reader may wonder
why we defined the nonunique winner variant of integer-linear-program implementability.
The answer is that, as we will see later in this section, it is a useful notion when dealing
with destructive control.
The class of election systems that are integer-linear-program implementable is remarkably broad. For example, it is variously implicit in or a consequence of results of Faliszewski,
Hemaspaandra, and Hemaspaandra (2009a) that plurality, veto, Borda, Dodgson, and each
polynomial-time computable (in the number of candidates) family of scoring protocols are
integer-linear-program implementable.19 For many other election systems—e.g., Kemeny
voting (Kemeny, 1959; Young & Levenglick, 1978) and Copeland voting—it is not clear
whether they are integer-linear-program implementable, but there are similar approaches
that will be as useful for us. We will return to this issue at the end of this section.
Theorem 6.2. Let E be an integer-linear-program unique-winner implementable election
system. For number of candidates as the parameter, constructive E-AC+DC+AV+DV+BV
is in FPT.
Proof. Let (C, A, V, W, p, kAC , kDC , kAV , kDV , kBV ) be our input instance of the constructive E-AC+DC+AV+DV+BV control problem, as described in Definition 4.1. Let C =
{p, c1 , . . . , cm0 } and A = {a1 , . . . , am00 }. Our parameter, the total number of candidates, is
K
m = m0 +m00 +1. For each subset K of C ∪A we let oK
1 , . . . , okKk! mean the kKk! preference
orders over K.
The idea of our algorithm is to perform an exhaustive search through all the subsets of
candidates K, K ⊆ C ∪ A, and for each K check whether (a) it is possible to obtain K from
C by deleting at most kDC candidates and adding at most kAC candidates from A, and (b)
it is possible to ensure that p is a unique winner of election (K, V ) by deleting at most kDV
voters, adding at most kAV voters from W , and bribing at most kBV voters. Given K, step
(a) can straightforwardly be implemented in polynomial time. To implement step (b), we
introduce a linear integer program P (K), which is satisfiable if and only if step (b) holds.
Let us now fix K ⊆ C ∪ A and describe the integer linear program P (K).
We assume that p ∈ K as it is not legal to delete p (and it would be pointless, given
that we want to ensure his or her victory). We interpret preference orders of voters in V
and W as limited to the candidate set K. We use the following constants in our program.
19. Let m be the number of candidates. A scoring protocol is a vector of m nonnegative integers satisfying
α1 ≥ α2 ≥ · · · ≥ αm . Each candidate receives αi points for each vote that ranks him or her in the i’th
position, and the candidate(s) with most points win. Many election systems can be viewed as families
of scoring protocols. For example, plurality is defined by scoring protocols of the form (1, 0, . . . , 0), veto
is defined by scoring protocols of the form (1, . . . , 1, 0), and Borda is defined by scoring protocols of the
form (m − 1, m − 2, . . . , 0), where m is the number of candidates.

338

Multimode Control Attacks on Elections

For each i, 1 ≤ i ≤ kKk!, we let nVi be the number of voters in V with preference order oK
i ,
K . P (K) contains the
and we let nW
be
the
number
of
voters
in
W
with
preference
order
o
i
i
following variables (described together with their intended interpretation):
Variables av1 , . . . , avkKk! . For each i, 1 ≤ i ≤ kKk!, we interpret avi as the number of
voters with preference oK
i that we add from W .
Variables dv1 , . . . , dvkKk! . For each i, 1 ≤ i ≤ kKk!, we interpret dvi as the number of
voters with preference oK
i that we delete from V .
Variables bv1,1 , bv1,2 , . . . , bv1,kKk! , bv2,1 , . . . , bvkKk!,kKk! . For each i, j, 1 ≤ i, j ≤
kKk!, we interpret bvi,j as the number of voters with preference oK
i that, in case
i 6= j, we bribe to switch to preference order oK
,
or,
in
case
i
=
j,
we
leave unbribed.
j
P (K) contains the following constraints.
1. All the variables have nonnegative values.
2. For each variable avi , 1 ≤ i ≤ kKk!, there are enough voters in W with preference
order oK
i to be added. That is, for each i, 1 ≤ i ≤ kKk!, we have a constraint
avi ≤ nW
. Altogether, we can add at most kAV voters so we have a constraint
PkKk! i
i=1 avi ≤ kAV .
3. For each variable dvi , 1 ≤ i ≤ kKk!, there are enough voters in V with preference
order oK
i to be deleted. That is, for each i, 1 ≤ i ≤ kKk!, we have a constraint
dvi ≤ nVi . Altogether, we can delete at most kDV voters so we have a constraint
PkKk!
i=1 dvi ≤ kDV .
4. For each variable bvi,j , 1 ≤ i, j ≤ kKk!, there are enough voters with preference oK
i
PkKk!
to be bribed. That is, for each i, 1 ≤ i ≤ kKk!, we have a constraint j=1 bvi,j =
nVi + avi − dvi (the equality comes from the fact that for each i, 1 ≤ i ≤ kKk!, bvi,i
is the number of voters with preference oK
i that we do not bribe). Altogether, we can
bribe at most kBV voters so we also have a constraint


kKk! kKk!
kKk!
X X
X

bvi,j  −
bvi,i ≤ kBV .
i=1 j=1

i=1

5. Candidate p is the unique winner of the election after we have executed all the adding,
deleting, and bribing of voters. Using the fact that E is integer-linear-program uniquewinner implementable, we can express this as win E (p, `1 , . . . , `kKk! ), where we subPkKk!
stitute each `j , 1 ≤ j ≤ kKk!, by i=1 bvi,j (note that, by previous constraints,
variables describing bribery already take into account adding and deleting voters).
This is a legal integer-linear-program constraint as win E (p, `1 , . . . , `kKk! ) is simply a
conjunction of linear inequalities over `1 , . . . , `kKk! .
The number of variables and the number of inequalities in P (K) are each polynomially
bounded in m!. Keeping in mind Definitions 4.1 and 6.1, it is straightforward to see that program P (K) does exactly what we expect it to. And testing whether P (K) is satisfiable (i.e.,
339

Faliszewski, Hemaspaandra, & Hemaspaandra

has an integer solution, as we are in the framework of an integer linear program) is in FPT,
with respect to the number of candidates being our parametrization, by using Lenstra’s
(1983) algorithm. Thus our complete FPT algorithm for the E-AC+DC+AV+DV+BV
problem works as follows. For each subset K of C ∪ A that includes p we execute the
following two steps:
1. Check whether it is possible to obtain K from C by deleting at most kDC candidates
and by adding at most kAC candidates from A.
2. Form linear program P (K) and check whether it has any integral solutions using the
algorithm of Lenstra (1983). Accept if so.
If after trying all sets K we have not accepted, then reject.
From the previous discussion, this algorithm is correct. Also, since (a) there are exactly
m−1
2
sets K to try, (b) executing the first step above can be done in time polynomial
in m, and (c) the second step is in FPT (given that m is the parameter), constructive
E-AC+DC+AV+DV+BV is in FPT for parameter m.
Theorem 6.2 deals with constructive control only. However, using its proof, it is straightforward to prove a destructive variant of the result. We say that an election system is
strongly voiced (Hemaspaandra et al., 2007) if it holds that whenever there is at least one
candidate, there is at least one winner.20
Corollary 6.3. Let E be a strongly voiced, integer-linear-program nonunique-winner implementable election system. Destructive E-AC+DC+AV+DV+BV is in FPT for the parameter number of candidates.
To see that the corollary holds, it is enough to note that for strongly voiced election
systems a candidate can be prevented from being a unique winner if and only if some other
candidate can be made a (possibly nonunique) winner (see, e.g., Footnote 5 of Hemaspaandra et al., 2007, for a relevant discussion). Thus to prove Corollary 6.3, we can simply
use an algorithm that for each candidate other than the despised one sees whether that
candidate can be made a (perhaps nonunique) winner, and if any can be made a (perhaps
nonunique) winner, declares destructive control achievable. (And the precise integer linear
programming feasibility problem solution given by Lenstra’s algorithm will reveal what action achieves the control.) This can be done in FPT using the algorithm from the proof of
Theorem 6.2, adapted to work for the nonunique-winner problem (this is trivial given that
Corollary 6.3 assumes that E is integer-linear-program nonunique-winner implementable).
Let us now go back to the issue that some election systems may not be integer-linearprogram implementable. As an example, let us consider maximin. Let E = (C, V ) be
an election, where C = {c1 , . . . , cm } and V = (v1 , . . . , vn ). As before, by o1 , . . . , om! we
mean the m! possible preference orders over C, and for each i, 1 ≤ i ≤ m!, by ni we
20. Please recall Footnote 5. In this paper’s model of elections, which is the one that matches that of
the previous papers studying control, the notion of election does allow an election system to have on
some inputs no winner. However, we mention that in the social choice world, formalizations of elections
typically build into their definition an exclusion of the possibility of having no winners, and so in that
world, “strongly voiced” would seem a strange concept to explicitly define and require, as it is built into
the general definition from the start.

340

Multimode Control Attacks on Elections

mean the number of voters in V that report preference order oi . For each ci and cj in C,
ci 6= cj , we let O(ci , cj ) be the set of preference orders over C where ci is preferred to cj . Let
k = (k1 , . . . , km ) be a vector of nonnegative integers such that for each i, 1 ≤ i ≤ m, it holds
that 1 ≤ ki ≤ m. For such a vector k and a candidate c` ∈ C we define M (c` , k1 , . . . , km )
to be the following set of linear integer inequalities:
1. For each candidate ci , his or her maximin score P
is equal to NE (ci ,P
cki ). That is, for
each i, j, 1 ≤ i, j ≤ m, i 6= j, we have constraint ok ∈O(ci ,ck ) nk ≤ ok ∈O(ci ,cj ) nk
i

2. c` has the highest maximin score in election E and thus is the P
unique winner of
E. That is, for each i, 1 ≤ i ≤ m, i 6= `, we have constraint
ok ∈O(c` ,ck` ) nk >
P
ok ∈O(ci ,ck ) nk .
i

It is straightforward to see that c` is a unique maximin winner of E if and only if there is a
vector k = (k1 , . . . , km ) such that all inequalities of M (c` , k1 , . . . , km ) are satisfied. It is also
straightforward how to modify the above construction to handle the nonunique winner case.
Since there are only O(mm ) vectors k to try and each M (c` , k1 , . . . , km ) contains O(m2 )
inequalities, it is straightforward to modify the proof of Theorem 6.2 to work for maximin:
Assuming that one is interested in ensuring candidate c` ’s victory, one simply has to replace
program P (K) in the proof of Theorem 6.2 with a family of programs that each include a
different M (c` , k1 , . . . , km ) for testing if c` had won. And one would accept if any of these
were satisfiable. Thus we have the following result.
Corollary
6.4. Constructive AC+DC+AV+DV+BV control and destructive
AC+DC+AV+DV+BV control are both in FPT for maximin for the parameter number of
candidates.
The above construction for the winner problem in maximin can be viewed as, in effect, a disjunction of a set of integer linear programs. Such constructions for the winner
problem have already been obtained for Kemeny elections21 by Faliszewski, Hemaspaandra, and Hemaspaandra (2009a) and for Copeland elections by Faliszewski, Hemaspaandra,
Hemaspaandra, and Rothe (2009a). Thus we have the following theorem.
21. Our definition is one that is for the notion of a Kemeny voting system where voting is by preference
orders (so no ties within a voter’s preferences are allowed) and the allowed values for a Kemeny consensus
(soon to be defined) are also limited to preference orders. This is not the notion from the original
papers (Kemeny, 1959; Young & Levenglick, 1978), but rather is the notion of Kemeny elections used
by Faliszewski, Hemaspaandra, and Hemaspaandra (2009a), who themselves mention they are following
the Saari and Merlin (2000) notion of Kemeny elections; interestingly, Hemaspaandra, Spakowski, and
Vogel (2005) ensure that their own main result, which is about the complexity of the winner problem
for Kemeny elections, holds for both cases, but they have to use real care to make that hold. So, all
that being said, Kemeny elections are (as used in this paper) defined as follows. Let E = (C, V ) be an
election, where C = {c1 , . . . , cm } and V = (v1 , . . . , vn ). For each preference order r and each voter vi ,
1 ≤ i ≤ n, let d(r, vi ) be the number of inversions between r and the preference order of vi (i.e., the
number of pairs of candidates {ci ,P
cj } ranked by vi and r in the opposite order). The Kemeny score of
a preference order r is defined as n
i=1 d(r, vi ). A preference order r is said to be a Kemeny consensus
if its Kemeny score is lowest (tied for lowest is fine—the lowest does not have to be unique) among
all preference orders over C. A candidate ci is a Kemeny winner if ci is ranked first in some Kemeny
consensus.

341

Faliszewski, Hemaspaandra, & Hemaspaandra

Corollary 6.5. With number of candidates as the parameter, constructive
AC+DC+AV+DV+BV control and destructive AC+DC+AV+DV+BV control are in
FPT for Kemeny and, for each rational α, 0 ≤ α ≤ 1, for Copeland α .
We conclude with an important caveat. The FPT algorithms of this section are very
broad in their coverage, but in practice they would be difficult to use as their running
time depends on (the fixed-value parameter) m in a very fast-growing way and as Lenstra’s
algorithm has a large multiplicative constant in its polynomial running time. Thus the
results of this section should best be interpreted as indicating that, for multipronged control
in our setting, it is impossible to prove non-FPT-ness (and so it is impossible to prove fixedparameter hardness in terms of the levels of the so-called “W” hierarchy of fixed-parameter
complexity, unless that hierarchy collapses to FPT). If one is interested in truly practically
implementing a multipronged control attack, one should probably devise a problem-specific
algorithm rather than using our very generally applicable FPT construction.

7. Conclusions
This paper was motivated by the desire to move the study of control a step in the direction
of better capturing real-life scenarios. In particular, attackers will not tend to artificially
limit themselves to one type of attack, but rather may well pull out and employ every
trick in their playbook. And so we have studied control attacks (including even (unpriced)
bribery, for the first time, within the framework of control) in which multiple attack prongs
can be pursued by the attacker.
This paper has shown that that approach—combining various types of control into multiprong control attacks—is useful. For example, it allows us to express control vulnerability
results and proofs in a compact way, and to obtain vulnerability results that are stronger
than would be obtained for single prongs alone (and that immediately imply prior results,
which were just for single prongs). The central contribution of this paper is that it provides
a broad set of tools to allow one, from the immunity/vulnerability/resistance of the basic
control prongs, to almost always determine the complexity of combinations of those prongs.
For systems where each basic prong is immune, vulnerable, or resistant, the only blind spot
in our machinery has to do with cases where there are multiple vulnerable prongs, as there
we showed that the underlying prongs simply do not on their own determine the complexity
of their multiprong combination. Even for that case, for all the systems discussed in the
paper, and indeed for a very broad class of systems (“vulnerability-combining” systems)
that we expect will include nearly all natural systems, we show how to classify even in the
face of this difficulty. We provide a useful tool to help researchers prove that additional
systems are vulnerability-combining. Section 4.4 summarizes all that. And Table 5 summarizes our results regarding the five election systems we have focused on in this paper; see
also Tables 2 and 3.
However, we have also seen that there exists a natural election system, OriginalLlull,
that unless P = NP is not constructive vulnerability-combining. That system is vulnerable
to both constructive AC control and constructive AV control yet is resistant to constructive
AC+AV control.
342

Multimode Control Attacks on Elections

Control type

AC
DC
AV
DV
BV

plurality
Con.
R
R
V
V
V

Des.
R
R
V
V
V

Condorcet
Con.
I
V
R
R
R

Des.
V
I
V
V
V

Copelandα ,
for each 0 ≤ α ≤ 1
Con.
Des.
R
V
R
V
R
R
R
R
R
R

approval
Con.
I
V
R
R
R

Des.
V
I
V
V
V

maximin
Con.
R
V
R
R
R

Des.
V
V
R
R
R

Table 5: Resistance to basic control types for the five main election systems studied in this
paper. In the table, I means the system is immune to the given control type, R
means resistance, and V means vulnerability. Constructive results for AC, DC,
AV, and DV for plurality and Condorcet are due to Bartholdi et al. (1992) and
the corresponding destructive results are due to Hemaspaandra et al. (2007). All
results for AC, DC, AV, and DV for approval are due to Hemaspaandra et al.
(2007). All results regarding Copeland are due to Faliszewski, Hemaspaandra,
Hemaspaandra, and Rothe (2009a). Constructive bribery results for plurality and
approval are due to Faliszewski, Hemaspaandra, and Hemaspaandra (2009a), and
the constructive bribery result for Condorcet is implicit in the work of Faliszewski,
Hemaspaandra, Hemaspaandra, and Rothe (2009a). All the remaining entries
(i.e., all results regarding maximin, and destructive bribery results for plurality,
approval, and Condorcet) are due to this paper (and are bold-italic in the table).
The main contribution of this paper, however, is the analysis of multiprong control
types. In particular, all constructive (or destructive) collections of prongs for the
systems here combine as specified by Classification Rule A of Section 4.2. This
holds due to Theorem 4.7, the paragraphs immediately following it, and the fact
that we prove all five systems in this table to be both constructive vulnerabilitycombining and destructive vulnerability-combining (Theorems 4.11 and 5.1).

We have also shown that as far as fixed-parameter tractability goes, at least with respect
to the parameter “number of candidates,” a very broad class of election systems is vulnerable
to the most full attack over basic prongs, namely, a AC+DC+AV+DV+BV control attack.
Finally, in the appendix, we prove that no candidate whose Dodgson score is more than
kCk2 times the Dodgson winner’s score can be a maximin winner.
This paper studies multipronged control where the prongs may include various standard
types of control or bribery. However, it is straightforward to see that our framework can
be naturally extended to include manipulation, and we commend that direction to any
interested reader (and mention in passing that Section 4 of Faliszewski, Hemaspaandra, &
Hemaspaandra, 2009a, does find a connection between bribery and manipulation). To do
so, one would have to allow some of the voters—the manipulators—to have blank preference
orders and, if such voters were to be included in the election, the controlling agent would
have to decide on how to fill them in. (That is, the controlling agent in this model would
control both the control aspects and the manipulation aspects.) It is interesting that in
this model the controlling agent might be able to add manipulative voters (if there were
manipulators among the voters that can be added) or even choose to delete them (it may
343

Faliszewski, Hemaspaandra, & Hemaspaandra

seem that deleting manipulators is never useful but Zuckerman, Procaccia, & Rosenschein,
2009, give an example where deleting a manipulator is necessary to make one’s favorite
candidate a winner of a Copeland election).
We mention as a natural but involved open direction the study of multipronged control
in the setting where there are multiple controlling agents, each with a different goal, each
controlling a different prong. In such a setting, it is interesting to consider game-theoretic
scenarios as well as situations in which, for example, one of the controlling agents is seeking
an action that will succeed regardless of the action of the other attacker. Another important
direction is the following. Section 6 for the very specific (namely, fixed-parameter) case it
studies sets up a quite flexible framework for classifying a broad range of control-attack
problems as being in polynomial time. It would be interesting to see what highly flexible
schemes and matching results one can find to broadly classify the control complexity of
elections in the general case (for motivation, see for example the work broadly classifying
the manipulation complexity of scoring protocols, Hemaspaandra & Hemaspaandra, 2007;
Conitzer, Sandholm, & Lang, 2007; Procaccia & Rosenschein, 2007).

Acknowledgments
Supported in part by NSF grants CCF-0426761, IIS-0713061, and CCF-0915792, Polish
Ministry of Science and Higher Education grant N-N206-378637, the Foundation for Polish Science’s Homing/Powroty program, AGH University of Science and Technology grant
11.11.120.865, the ESF’s EUROCORES program LogICCC, and Friedrich Wilhelm Bessel
Research Awards to Edith Hemaspaandra and Lane A. Hemaspaandra. A preliminary version (Faliszewski, Hemaspaandra, & Hemaspaandra, 2009b) of this paper appeared in the
proceedings of the 21st International Joint Conference on Artificial Intelligence, July 2009.
For their very helpful comments and suggestions, we are deeply indebted to JAIR handling
editor Vincent Conitzer, Edith Elkind, and the anonymous IJCAI and JAIR referees.

Appendix A. A Connection of Maximin Voting to Dodgson Voting
Section 5 focused on control in maximin voting. In this appendix, we focus on a different
facet of maximin voting, namely, we show a connection between maximin and the famous
voting rule (i.e., election system) of Dodgson.
Dodgson voting, proposed in the 19th century by Charles Lutwidge Dodgson (1876),22
works as follows. Let E = (C, V ) be an election, where C = {c1 , . . . , cm } and V =
(v1 , . . . , vn ). For a candidate ci ∈ C, the Dodgson score of ci , denoted scoreD
E (ci ), is the
smallest number of sequential swaps of adjacent candidates on the preference lists of voters
in V needed to make ci become the Condorcet winner. The candidates with the lowest
score are the Dodgson election’s winners. That is, Dodgson defined his system to elect
those candidates that are closest to being Condorcet winners in the sense of adjacent-swaps
distance. Although Dodgson’s eighteenth-century election system was directly defined in
terms of distance, there remains ongoing interest in understanding the classes of voting rules
that can be captured in various distance-based frameworks (see, e.g., Meskanen & Nurmi,
2008; Elkind, Faliszewski, & Slinko, 2009, 2010c, 2010b).
22. Dodgson is better known as Lewis Carroll, the renowned author of “Alice’s Adventures in Wonderland.”

344

Multimode Control Attacks on Elections

Unfortunately, it is known that deciding whether a given candidate is a winner according to Dodgson’s rule is quite complex. In fact, Hemaspaandra, Hemaspaandra, and Rothe
(1997), strengthening an NP-hardness result of Bartholdi et al. (1989b), showed that this
problem is complete for parallelized access to NP. That is, it is complete for the Θp2 level
of the polynomial hierarchy. Nonetheless, many researchers have sought efficient ways of
computing Dodgson winners, for example by using frequently correct heuristics (Homan
& Hemaspaandra, 2009; McCabe-Dansted, Pritchard, & Slinko, 2008), fixed-parameter
tractability (see Bartholdi et al., 1989b; Faliszewski, Hemaspaandra, & Hemaspaandra,
2009a; Betzler, Guo, & Niedermeier, 2010, and the discussion in Footnote 17 of Faliszewski,
Hemaspaandra, Hemaspaandra, & Rothe, 2009a), and approximation algorithms for Dodgson scores (Caragiannis, Covey, Feldman, Homan, Kaklamanis, Karanikolas, Procaccia, &
Rosenschein, 2009).
In addition to its high computational cost in determining winners, Dodgson’s rule is
often criticized for not having basic properties one would expect a good voting rule to
have. For example, Dodgson’s rule is not “weak-Condorcet consistent” (Brandt et al.,
2010)—equivalently, it does not satisfy Fishburn’s “strict Condorcet principle”—and does
not satisfy homogeneity and monotonicity (see Brandt, 2009, which surveys a number of
defects of Dodgson’s rule). We provide definitions for the latter two notions (in the former
case just for the case we need here, namely, anonymous rules), as they will be relevant to
this section.
Homogeneity. We say that an anonymous voting rule R is homogeneous if for each positive
integer k and each election E = (C, V ), where C = {c1 , . . . , cm } and V = (v1 , . . . , vn ),
it holds that R has the same winner set on E as on E 0 = (C, V 0 ), where V 0 =
(v1 , . . . , v1 , v2 , . . . , v2 , . . . , vn , . . . , vn ).
| {z } | {z }
| {z }
k

k

k

Monotonicity. We say that a voting rule R is monotone if for each election E = (C, V ),
where C = {c1 , . . . , cm } and V = (v1 , . . . , vn ), it holds that if some candidate ci ∈ C
is a winner of E then ci is also a winner of an election E 0 that is identical to E
except that some voters rank ci higher (without changing the relative order of all the
remaining candidates).
Continuing the Caragiannis et al. (2009) line of research on approximately computing Dodgson scores, Caragiannis, Kaklamanis, Karanikolas, and Procaccia (2010) devised an approximation algorithm for computing Dodgson scores that, given an election
E = (C, V ), where C = {c1 , . . . , cm } and V = (v1 , . . . , vn ) and a candidate ci in C, computes in polynomial time a nonnegative integer scE (ci ) such that scoreD
E (ci ) ≤ scE (ci ) and
D
scE (ci ) = O(m log m) · scoreE (ci ). That is, the algorithm given by Caragiannis et al. (2010)
is, in a natural sense, an O(m log m)-approximation of the Dodgson score.23 This algorithm
23. Throughout this section, we use the notion “f (m)-approximation of g” in the sense it is typically used
when dealing with minimization problems. That is, we mean that the approximation outputs a value that
is at least g and at most f (m) · g. We are slightly informal (i.e., sloppy) regarding the interplay between
this notation and Big-Oh notation; however, the sloppiness is of a quite standard type that will not
cause confusion. We assume that the type of the input to g and type of the input to the approximation
is clear from context; in this paper, in both cases, the arguments are an election E and a candidate ci .

345

Faliszewski, Hemaspaandra, & Hemaspaandra

has additional properties: If one defines a voting rule to elect those candidates that have
lowest scores according to the algorithm, then that voting rule is Condorcet consistent (i.e.,
when a Condorcet winner exists, he or she is the one and only winner under the voting
rule), homogeneous, and monotone.
The mentioned result of Caragiannis et al. (2010) is very interesting. In our next theorem
we show that maximin—which like the Caragiannis et al. rule is Condorcet-consistent,
homogeneous, and monotone—also elects candidates that are, in a certain different yet
precise sense, “close” to being Dodgson winners. This is interesting as maximin is often
considered to be quite different from Dodgson’s rule. Our proof is inspired by that of
Caragiannis et al. (2010).
Theorem A.1. Let E = (C, V ) be an election and let W ⊆ C be a set of candidates that
win in E according to the maximin rule. Let m = kCk and let s = minci ∈C scoreD
E (ci ). For
D
2
each ci ∈ W it holds that s ≤ scoreE (ci ) ≤ m s.
Proof. Let us fix an election E = (C, V ) with C = {c1 , . . . cm } and V = (v1 , . . . , vn ). For
each two candidates ci , cj ∈ C we define df E (ci , cj ) to be the smallest number k such that
if k voters in V changed their preference order to rank ci ahead of cj , then ci would be
preferred to cj by more than half of the voters. Note that if for some ci , cj ∈ C we have
df E (ci , cj ) > 0 then
jnk
NE (ci , cj ) + df E (ci , cj ) =
+ 1.
2
For each candidate ci ∈ C we define sc0E (ci ) to be
sc0E (ci ) = m2 max{df E (ci , cj ) | cj ∈ C − {ci }}.
We now prove that sc0 is an m2 -approximation of the Dodgson score.
0
2
D
Lemma A.2. For each ci ∈ C it holds that scoreD
E (ci ) ≤ scE (ci ) ≤ m scoreE (ci ).

Proof. Let us fix some ci ∈ C. To see that the secondP
inequality in the lemma statement
D
holds, note that max{df E (ci , cj ) | cj ∈ C − {ci }} ≤
cj ∈C−{ci } df E (ci , cj ) ≤ scoreE (ci )
because for each candidate ck we, at least, have to perform df E (ci , ck ) swaps to ensure that
ci defeats ck in their majority head-to-head contest. Thus, after multiplying by m2 , we have
m2 max{df E (ci , cj ) | cj ∈ C − {ci }} ≤ m2 scoreD
E (ci ).
Let us now consider the first inequality. Let ck be some candidate in C − {ci }. To make
sure that ci is ranked higher than ck by more than half of the voters, we can shift ci to
the first position in the preference lists of max{df E (ci , cj ) | cj ∈ C − {ci }} ≥ df E (ci , ck )
voters (or, all the remaining voters if less than max{df E (ci , cj ) | cj ∈ C − {ci }} voters do
not rank ci as their top choice). This requires at most m adjacent swaps per voter. Since
there are m − 1 candidates in C − {ci }, m2 max{df E (ci , cj ) | cj ∈ C − {ci }} adjacent swaps
are certainly sufficient to make ci a Condorcet winner.
(Lemma A.2)
It remains to show that if some candidate ci is a maximin winner in E then sc0E (ci ) is
minimal. Fortunately, this is straightforward to see. If some candidate ci is a Condorcet
winner of E then he or she is the unique maximin winner and he or she is the unique
346

Multimode Control Attacks on Elections

candidate ci with sc0E (ci ) = 0. Let us assume that there is no Condorcet winner of E. Let
us fix some candidate ci ∈ C and let ck ∈ C − {ci } be a candidate such that sc0E (ci ) =
m2 df E (ci , ck ). That is, df E (ci , ck ) = max{df E (ci , cj ) | cj ∈ C − {ci }} and df E (ci , ck ) > 0.
Due to this last fact and our choice of ck , we have df E (ci , ck ) = n2 + 1 − NE (ci , ck ) and so
jnk
NE (ci , ck ) =
+ 1 − df E (ci , ck ) = min NE (ci , cj ) = scoreE (ci ),
2
cj ∈C−{ci }
where scoreE (ci ) is the maximin score of ci in E. Thus each candidate ci with the lowest
value sc0E (ci ) also has the highest maximin score.
Theorem A.1 says that every maximin winner’s Dodgson score is no less than the Dodgson score of the Dodgson winner(s) (that fact of course holds trivially), and is no more than
m2 times the Dodgson score of the Dodgson winner(s). That is, we have proven that no
candidate whose Dodgson score is more than m2 times that of the Dodgson winner(s) can
be a maximin winner.

References
Bartholdi, III, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social
Choice and Welfare, 8 (4), 341–354.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989a). The computational difficulty of manipulating an election. Social Choice and Welfare, 6 (3), 227–241.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989b). Voting schemes for which it can be
difficult to tell who won the election. Social Choice and Welfare, 6 (2), 157–165.
Bartholdi, III, J., Tovey, C., & Trick, M. (1992). How hard is it to control an election?
Mathematical and Computer Modeling, 16 (8/9), 27–40.
Baumeister, D., Erdélyi, G., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2010).
Computational aspects of approval voting. In Laslier, J., & Sanver, M. (Eds.), Handbook on Approval Voting, pp. 199–251. Springer.
Betzler, N., & Dorn, B. (2009). Towards a dichotomy of finding possible winners in elections based on scoring rules. In Proceedings of the 34th International Symposium on
Mathematical Foundations of Computer Science, pp. 124–136. Springer-Verlag Lecture
Notes in Computer Science #5734.
Betzler, N., Guo, J., & Niedermeier, R. (2010). Parameterized computational complexity
of Dodgson and Young elections. Information and Computation, 208 (2), 165–177.
Betzler, N., Hemmann, S., & Niedermeier, R. (2009). A multivariate complexity analysis
of determining possible winners given incomplete votes. In Proceedings of the 21st
International Joint Conference on Artificial Intelligence, pp. 53–58. AAAI Press.
Betzler, N., & Uhlmann, J. (2009). Parameterized complexity of candidate control in elections and related digraph problems. Theoretical Computer Science, 410 (52), 43–53.
Black, D. (1958). The Theory of Committees and Elections. Cambridge University Press.
Brandt, F. (2009). Some remarks on Dodgson’s voting rule. Mathematical Logic Quarterly,
55 (4), 460–463.
347

Faliszewski, Hemaspaandra, & Hemaspaandra

Brandt, F., Brill, M., Hemaspaandra, E., & Hemaspaandra, L. (2010). Bypassing combinatorial protections: Polynomial-time algorithms for single-peaked electorates. In Proceedings of the 24th AAAI Conference on Artificial Intelligence, pp. 715–722. AAAI
Press.
Caragiannis, I., Covey, J., Feldman, M., Homan, C., Kaklamanis, C., Karanikolas, N., Procaccia, A., & Rosenschein, J. (2009). On the approximability of Dodgson and Young
elections. In Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete
Algorithms, pp. 1058–1067. Society for Industrial and Applied Mathematics.
Caragiannis, I., Kaklamanis, C., Karanikolas, N., & Procaccia, A. (2010). Socially desirable
approximations for Dodgson’s voting rule. In Proceedings of the 11th ACM Conference
on Electronic Commerce, pp. 253–262. ACM Press.
Conitzer, V., & Sandholm, T. (2006). Nonexistence of voting rules that are usually hard to
manipulate. In Proceedings of the 21st National Conference on Artificial Intelligence,
pp. 627–634. AAAI Press.
Conitzer, V., Sandholm, T., & Lang, J. (2007). When are elections with few candidates
hard to manipulate? Journal of the ACM, 54 (3), Article 14.
Dobzinski, S., & Procaccia, A. (2008). Frequent manipulability of elections: The case of two
voters. In Proceedings of the 4th International Workshop On Internet And Network
Economics, pp. 653–664. Springer-Verlag Lecture Notes in Computer Science #5385.
Dodgson, C. (1876). A method of taking votes on more than two issues. Pamphlet printed
by the Clarendon Press, Oxford, and headed “not yet published” (see the discussions
in McLean & Urken, 1995, and Black, 1958, both of which reprint this paper).
Dorn, B., & Schlotter, I. (2010). Multivariate complexity analysis of swap bribery. In
Proceedings of the 5th International Symposium on Parameterized and Exact Computation, pp. 107–122. Springer-Verlag Lecture Notes in Computer Science #6478.
Elkind, E., Faliszewski, P., & Slinko, A. (2009). On distance rationalizability of some voting
rules. In Proceedings of the 12th Conference on Theoretical Aspects of Rationality and
Knowledge, pp. 108–117. ACM Press.
Elkind, E., Faliszewski, P., & Slinko, A. (2010a). Cloning in elections. In Proceedings of the
24th AAAI Conference on Artificial Intelligence, pp. 768–773. AAAI Press.
Elkind, E., Faliszewski, P., & Slinko, A. (2010b). Good rationalizations of voting rules.
In Proceedings of the 24th AAAI Conference on Artificial Intelligence, pp. 774–779.
AAAI Press.
Elkind, E., Faliszewski, P., & Slinko, A. (2010c). On the role of distances in defining voting
rules. In Proceedings of the 9th International Conference on Autonomous Agents and
Multiagent Systems, pp. 375–382. International Foundation for Autonomous Agents
and Multiagent Systems.
Erdélyi, G., Nowak, M., & Rothe, J. (2009). Sincere-strategy preference-based approval
voting fully resists constructive control and broadly resists destructive control. Mathematical Logic Quarterly, 55 (4), 425–443.
348

Multimode Control Attacks on Elections

Erdélyi, G., Piras, L., & Rothe, J. (2010a). Bucklin voting is broadly resistant to control.
Tech. rep. arXiv:1005.4115 [cs.GT], arXiv.org.
Erdélyi, G., Piras, L., & Rothe, J. (2010b). Control complexity in fallback voting. Tech.
rep. arXiv:1004.3398 [cs.GT], arXiv.org.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2009a). How hard is bribery in
elections? Journal of Artificial Intelligence Research, 35, 485–532.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2009b). Multimode attacks on
elections. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, pp. 128–133. AAAI Press.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2010a). Multimode control attacks
on elections. Tech. rep. arXiv:1007.1800 [cs.GT], Computing Research Repository,
http://arXiv.org/corr/.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2010b). Using complexity to
protect elections. Communications of the ACM, 53 (11), 74–82.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Llull and
Copeland voting broadly resist bribery and control. In Proceedings of the 22nd AAAI
Conference on Artificial Intelligence, pp. 724–730. AAAI Press.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009a). Llull and
Copeland voting computationally resist bribery and constructive control. Journal of
Artificial Intelligence Research, 35, 275–341.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009b). A richer understanding of the complexity of election systems. In Ravi, S., & Shukla, S. (Eds.), Fundamental Problems in Computing: Essays in Honor of Professor Daniel J. Rosenkrantz,
pp. 375–406. Springer.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2011). The shield that
never was: Societies with single-peaked preferences are more open to manipulation
and control. Information and Computation, 209 (2), 89–107.
Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2008). Copeland voting: Ties matter.
In Proceedings of the 7th International Conference on Autonomous Agents and Multiagent Systems, pp. 983–990. International Foundation for Autonomous Agents and
Multiagent Systems.
Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2010). Manipulation of Copeland elections. In Proceedings of the 9th International Conference on Autonomous Agents and
Multiagent Systems, pp. 367–374. International Foundation for Autonomous Agents
and Multiagent Systems.
Friedgut, E., Kalai, G., & Nisan, N. (2008). Elections can be manipulated often. In Proceedings of the 49th IEEE Symposium on Foundations of Computer Science, pp. 243–249.
IEEE Computer Society.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the Theory of
NP-Completeness. W. H. Freeman and Company.
349

Faliszewski, Hemaspaandra, & Hemaspaandra

Hägele, G., & Pukelsheim, F. (2001). The electoral writings of Ramon Llull. Studia Lulliana,
41 (97), 3–38.
Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy for voting systems. Journal of
Computer and System Sciences, 73 (1), 73–83.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (1997). Exact analysis of Dodgson
elections: Lewis Carroll’s 1876 voting system is complete for parallel access to NP.
Journal of the ACM, 44 (6), 806–825.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Anyone but him: The complexity
of precluding an alternative. Artificial Intelligence, 171 (5–6), 255–285.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). Hybrid elections broaden
complexity-theoretic resistance to control. Mathematical Logic Quarterly, 55 (4), 397–
424.
Hemaspaandra, E., Spakowski, H., & Vogel, J. (2005). The complexity of Kemeny elections.
Theoretical Computer Science, 349 (3), 382–391.
Homan, C., & Hemaspaandra, L. (2009). Guarantees for the success frequency of an algorithm for finding Dodgson-election winners. Journal of Heuristics, 15 (4), 403–423.
Isaksson, M., Kindler, G., & Mossel, E. (2010). The geometry of manipulation—A quantitative proof of the Gibbard–Satterthwaite Theorem. In Proceedings of the 51st IEEE
Symposium on Foundations of Computer Science, pp. 319–328. IEEE Computer Society Press.
Kemeny, J. (1959). Mathematics without numbers. Daedalus, 88, 577–591.
Lenstra, Jr., H. (1983). Integer programming with a fixed number of variables. Mathematics
of Operations Research, 8 (4), 538–548.
Liu, H., Feng, H., Zhu, D., & Luan, J. (2009). Parameterized computational complexity
of control problems in voting systems. Theoretical Computer Science, 410 (27–29),
2746–2753.
Liu, H., & Zhu, D. (2010). Parameterized complexity of control problems in maximin
election. Information Processing Letters, 110 (10), 383–388.
Maudet, N., Lang, J., Chevaleyre, Y., & Monnot, J. (2010). Possible winners when new
candidates are added: The case of scoring rules. In Proceedings of the 24th AAAI
Conference on Artificial Intelligence, pp. 762–767. AAAI Press.
McCabe-Dansted, J., Pritchard, G., & Slinko, A. (2008). Approximability of Dodgson’s
rule. Social Choice and Welfare, 31 (2), 311–330.
McLean, I., & Lorrey, H. (2006). Voting in the medieval papacy and religious orders. Report
2006-W12, Nuffield College Working Papers in Politics, Oxford, Great Britain.
McLean, I., & Urken, A. (1995). Classics of Social Choice. University of Michigan Press.
Meir, R., Procaccia, A., Rosenschein, J., & Zohar, A. (2008). The complexity of strategic
behavior in multi-winner elections. Journal of Artificial Intelligence Research, 33,
149–178.
350

Multimode Control Attacks on Elections

Menton, C. (2010).
Normalized range voting broadly resists control.
arXiv:1005.5698 [cs.GT], arXiv.org.

Tech. rep.

Meskanen, T., & Nurmi, H. (2008). Closeness counts in social choice. In Braham, M., &
Steffen, F. (Eds.), Power, Freedom, and Voting. Springer-Verlag.
Niedermeier, R. (2006). Invitation to Fixed-Parameter Algorithms. Oxford University Press.
Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.
Procaccia, A., & Rosenschein, J. (2007). Junta distributions and the average-case complexity
of manipulating elections. Journal of Artificial Intelligence Research, 28, 157–181.
Saari, D., & Merlin, V. (2000). A geometric examination of Kemeny’s rule. Social Choice
and Welfare, 17 (3), 403–438.
Schöning, U. (1986). Complete sets and closeness to complexity classes. Mathematical
Systems Theory, 19 (1), 29–42.
Shoham, Y., & Leyton-Brown, K. (2009). Multiagent Systems: Algorithmic, GameTheoretic, and Logical Foundations. Cambridge University Press.
Walsh, T. (2009). Where are the really hard manipulation problems? The phase transition in
manipulating the veto rule. In Proceedings of the 21st International Joint Conference
on Artificial Intelligence, pp. 324–329. AAAI Press.
Xia, L., & Conitzer, V. (2008a). Generalized scoring rules and the frequency of coalitional
manipulability. In Proceedings of the 9th ACM Conference on Electronic Commerce,
pp. 109–118. ACM Press.
Xia, L., & Conitzer, V. (2008b). A sufficient condition for voting rules to be frequently
manipulable. In Proceedings of the 9th ACM Conference on Electronic Commerce,
pp. 99–108. ACM Press.
Xia, L., Zuckerman, M., Procaccia, A., Conitzer, V., & Rosenschein, J. (2009). Complexity of
unweighted manipulation under some common voting rules. In Proceedings of the 21st
International Joint Conference on Artificial Intelligence, pp. 348–353. AAAI Press.
Young, H., & Levenglick, A. (1978). A consistent extension of Condorcet’s election principle.
SIAM Journal on Applied Mathematics, 35 (2), 285–300.
Zuckerman, M., Procaccia, A., & Rosenschein, J. (2009). Algorithms for the coalitional
manipulation problem. Artificial Intelligence, 173 (2), 392–412.

351

Journal of Artiﬁcial Intelligence Research 40 (2011) 57-93

Submitted 08/10; published 01/11

False-Name Manipulations in Weighted Voting Games
Haris Aziz

aziz@in.tum.de

Institut für Informatik
Technische Universität München, Germany

Yoram Bachrach

yorambac@gmail.com

Microsoft Research
Cambridge, UK

Edith Elkind

eelkind@ntu.edu.sg

School of Physical and Mathematical Sciences
Nanyang Technological University, Singapore

Mike Paterson

msp@dcs.warwick.ac.uk

Department of Computer Science
University of Warwick, UK

Abstract
Weighted voting is a classic model of cooperation among agents in decision-making
domains. In such games, each player has a weight, and a coalition of players wins the
game if its total weight meets or exceeds a given quota. A player’s power in such games
is usually not directly proportional to his weight, and is measured by a power index, the
most prominent among which are the Shapley–Shubik index and the Banzhaf index.
In this paper, we investigate by how much a player can change his power, as measured
by the Shapley–Shubik index or the Banzhaf index, by means of a false-name manipulation,
i.e., splitting his weight among two or more identities. For both indices, we provide upper
and lower bounds on the eﬀect of weight-splitting. We then show that checking whether
a beneﬁcial split exists is NP-hard, and discuss eﬃcient algorithms for restricted cases of
this problem, as well as randomized algorithms for the general case. We also provide an
experimental evaluation of these algorithms.
Finally, we examine related forms of manipulative behavior, such as annexation, where
a player subsumes other players, or merging, where several players unite into one. We
characterize the computational complexity of such manipulations and provide limits on
their eﬀects. For the Banzhaf index, we describe a new paradox, which we term the
Annexation Non-monotonicity Paradox.

1. Introduction
Collaboration and cooperative decision-making are important issues in many types of interactions among self-interested agents (Ephrati & Rosenschein, 1997). In many situations,
agents must take a joint decision leading to a certain outcome, which may have a diﬀerent
impact on each of the agents. A standard and well-studied way of doing so is by means
of voting, and in recent years, there has been a lot of research on applications of voting to
multiagent systems as well as on computational aspects of various voting procedures (see
Faliszewski & Procaccia, 2010). One of the key issues in this domain is how to measure
the power of each voter, i.e., his impact on the ﬁnal outcome. In particular, this question
becomes important when the agents have to decide how to distribute the payoﬀs resulting
c
2011
AI Access Foundation. All rights reserved.

Aziz, Bachrach, Elkind, & Paterson

from their joint action: a natural approach would be to pay each agent according to his
contribution, i.e., his voting power.
This issue is traditionally studied within the framework of weighted voting games (WVGs)
(Taylor & Zwicker, 1999), which provide a model of decision-making in many political and
legislative bodies (Leech, 2002; Laruelle & Widgren, 1998; Algaba, Bilbao, & Fernández,
2007), and have also been investigated in the context of multiagent systems (Elkind, Goldberg, Goldberg, & Wooldridge, 2008b, 2007). In such a game, each of the agents has a
weight, and a coalition of agents wins the game if the sum of the weights of its participants
meets or exceeds a certain quota. There are numerous examples of multiagent systems that
can be captured by weighted voting games. For example, the agents’ weights may correspond to the amount of resources (time, money, or battery power) that they contribute, and
the quota may indicate the amount of resources needed to complete a given task. Alternatively, the weight may be an indicator of an agent’s experience or seniority, and a voting
procedure may be designed to take into account these characteristics.
Clearly, having a larger weight makes it easier for a player to aﬀect the outcome. However, the player’s power is not always proportional to his weight. For example, if the quota
is so high that the only winning coalition is the one that includes all players, intuitively,
all players have equal power, irrespective of their weight. This idea is formalized using the
concept of a power index, which is a systematic way of measuring a player’s inﬂuence in a
weighted voting game. There are several ways to deﬁne power indices. One of the most popular approaches relies on the fact that weighted voting games form a subclass of coalitional
games, and therefore one can use the terminology and solution concepts that have been
developed in the context of general coalitional games. In particular, an important notion
in coalitional games is that of the Shapley value (Shapley, 1953), which is a classic method
of distributing the gains of the grand coalition in general coalitional games. The Shapley
value has a natural interpretation in the context of weighted voting, where it is known as
the Shapley–Shubik power index (Shapley & Shubik, 1954). Another well-known power
index, which has been introduced speciﬁcally in the context of weighted voting games, is
the Banzhaf index (Banzhaf, 1965). While several other power indices have been proposed
(e.g., see Johnston, 1978; Deegan & Packel, 1978; Holler & Packel, 1983), the Shapley–
Shubik power index and the Banzhaf power index are usually viewed as the two standard
approaches to measuring the players’ power in weighted voting games, and have been widely
studied from both normative and computational perspective.
As suggested above, power indices measure the players’ power and can be used to determine their payoﬀs. However, to be applicable in real-world scenarios, this approach to
payoﬀ division has to be resistant to dishonest behavior, or manipulation, by the participating players. In this paper we study the eﬀects of a particular form of manipulation in
weighted voting games, namely, false-name voting. Under this manipulation, a player splits
his weight between himself and a “fake” agent who enters the game. Such manipulations are
virtually impossible to detect in open anonymous environments such as the internet. They
can also occur in legislative bodies, where political parties vote on bills. In such bodies,
elections are held every several years to determine the weight each party has when voting
on a bill. Before such elections are held, a party may split up into two smaller parties. It is
likely that the supporters of that party would somehow split between the two new parties,
so the total weight of the new parties will be equal to that of the original party. By choosing
58

False-Name Manipulations in WVGs

a suitable platform, the original party can decide how the weight would be split between
the two new parties.
While a weight-splitting manipulation does not change the total weight of all identities
of the cheating agent, his power (as measured by the Shapley–Shubik power index or the
Banzhaf index) may change. Therefore, this behavior presents a challenge to the designers
of multiagent systems that rely on weighted voting. The main goal of this paper is to
measure the eﬀects of false-name voting and analyze its computational feasibility. We also
examine the related scenarios of players merging in order to increase their joint power, or
one player annexing another one.
Our main results are as follows:
• We precisely quantify the worst-case eﬀect of false-name voting on agents’ payoﬀs.
Namely, we show that in an n-player game splitting into two false identities can
increase an agent’s payoﬀ by at most a factor of 2 both for the Shapley–Shubik index
and the Banzhaf index. Moreover, this bound is asymptotically tight. On the other
hand, we show that false-name manipulation can decrease an agent’s payoﬀ by at
most a factor of Θ(n) for both indices.
• We demonstrate that ﬁnding a successful manipulation is not a trivial task by proving
that for both indices it is NP-hard to verify if a beneﬁcial split exists. However, we
show that if all weights are polynomially bounded, the problem can be solved in
polynomial time, and discuss eﬃcient randomized algorithms for this problem.
• We present similar NP-hardness results for the case of players merging into a single
new player. Interestingly, in the case of a player annexing one or more players, there
is a contrast between the Shapley–Shubik index and the Banzhaf index. Whereas for
the Shapley–Shubik index, annexing is always beneﬁcial, checking whether annexing
is beneﬁcial in the case of the Banzhaf index is NP-hard. However it is beneﬁcial if a
player annexes a player with a bigger weight. We also present a new paradox called
The Annexation Non-monotonicity Paradox, which shows that annexing a “small”
player can be more useful than annexing a “big” player.
• We complement our theoretical results by experiments which indicate the expected
fractions of positive and negative false-name manipulations in weighted voting games
with randomly selected weights.
1.1 Related Work
Weighted voting games date back at least to John von Neumann and Oskar Morgenstern,
who developed their theory in their monumental book Theory of Games and Economic
Behavior (von Neumann & Morgenstern, 1944). Subsequently, WVGs have been analyzed
extensively in the game theory literature (see, for instance, Taylor & Zwicker, 1999).
In his seminal paper, Shapley (1953) considered coalitional games and the question of fair
allocation of the utility gained by the grand coalition. The solution concept introduced in
this paper became known as the Shapley value of the game. The subsequent paper (Shapley
& Shubik, 1954) studies the Shapley value in the context of simple coalitional games, where
it is usually referred to as the Shapley–Shubik power index. The Banzhaf power index was
59

Aziz, Bachrach, Elkind, & Paterson

originally introduced by Banzhaf (1965); a somewhat diﬀerent deﬁnition was later proposed
by Dubey and Shapley (1979). In this paper, we make use of Banzhaf’s original deﬁnition,
as it is more appropriate in the context of payoﬀ division.
Both of these power indices have been well studied. Straﬃn (1977) shows that each index
reﬂects certain conditions in a voting body. Laruelle (1999) describes certain axioms that
characterize these two indices, as well as several others. These indices were used to analyze
the voting structures of the European Union Council of Ministers and the IMF (Machover
& Felsenthal, 2001; Leech, 2002).
The applicability of the power indices to measuring political power in various domains
has raised the question of ﬁnding tractable ways to compute them. However, this problem
appears to be computationally hard. Indeed, the naive algorithm for calculating the Shapley
value (or the Shapley–Shubik power index) considers all permutations of the players and
hence runs in exponential time. Moreover, Papadimitriou and Yannakakis (1994) show
that computing the Shapley value in weighted voting games is #P-complete. This result
is extended by Matsui and Matsui (2001), who show that calculating the Banzhaf index
in weighted voting games is also NP-hard. Furthermore, Faliszewski and Hemaspaandra
(2009) show that comparing the player’s power in two diﬀerent weighted voting games is
PP-complete for both indices.
Despite these hardness results, several papers show how to compute these power indices
in some restricted domains, or discuss ways to approximate them. These include a generating
functions approach (Mann & Shapley, 1962), which trades required storage for running time,
Owen’s multilinear extension (MLE) approach (Owen, 1975) and Monte Carlo simulation
approaches (Mann & Shapley, 1960; Fatima, Wooldridge, & Jennings, 2007; Bachrach,
Markakis, Resnick, Procaccia, Rosenschein, & Saberi, 2010). Matsui and Matsui (2000)
provide a good survey of algorithms for calculating power indices in weighted voting games.
Many of these approaches work well in practice, which justiﬁes the use of these indices as
payoﬀ distribution schemes in multiagent domains.
As a useful and succinct model for coalitional voting games, WVGs have attracted
a lot of interest from the multiagent community. A number of papers have considered
the problem of designing WVGs with desirable properties (Aziz, Paterson, & Leech, 2007;
Fatima, Wooldridge, & Jennings, 2008; de Keijzer, Klos, & Zhang, 2010). Simple games
that can be obtained by combining multiple weighted voting games have been examined by
Elkind et al. (2008b) and Faliszewski, Elkind, and Wooldridge (2009). Another well-studied
topic is computing various stability-related solution concepts in WVGs and their extensions
(Elkind et al., 2007; Elkind & Pasechnik, 2009; Elkind, Chalkiadakis, & Jennings, 2008a).
False-name manipulations in open anonymous environments have been examined in
diﬀerent domains such as auctions (Yokoo, 2007; Iwasaki, Kempe, Saito, Salek, & Yokoo,
2007) and coalitional games (Yokoo, Conitzer, Sandholm, Ohta, & Iwasaki, 2005; Ohta,
Iwasaki, Yokoo, Maruono, Conitzer, & Sandholm, 2006; Ohta, Conitzer, Satoh, Iwasaki, &
Yokoo, 2008). In the latter domain, the characteristic function by itself does not provide
enough information to analyze false-name manipulation. To deal with this issue, Yokoo
et al. (2005) introduced a framework where each player has a subset of skills, and the
characteristic function assigns values to the subset of skills. Our model can be seen as a
special case of this framework; however, due to special properties of weighted voting games
we are able to obtain much stronger results than in the general case.
60

False-Name Manipulations in WVGs

The phenomenon considered in the paper has been studied by political scientists and
economists under the name of “the paradox of size” (Shapley, 1973; Brams, 1975; Felsenthal
& Machover, 1998); however, neither its quantitative nor its computational aspects have
been considered. Felsenthal and Machover also discuss a number of other paradoxes in
weighted voting games; Laruelle and Valenciano (2005) give an overview of more recent
work on paradoxes of weighted voting. Occurrences of these paradoxes in voting bodies are
considered by Kilgour and Levesque (1984),van Deemen and Rusinowska (2003), and Leech
and Leech (2005). Another form of manipulation in WVGs has been recently studied by
Zuckerman, Faliszewski, Bachrach, and Elkind (2008), who analyze how the center might
change the players’ power by modifying the quota even if the weights are ﬁxed.
1.2 Follow-up Work
Many results that appear in this paper have been previously presented at the AAMAS conference (Bachrach & Elkind, 2008; Aziz & Paterson, 2009). Inspired by this work, Lasisi
and Allan (2010) have recently undertaken an experimental analysis of false-name manipulations in weighted voting games. They have also considered less popular power indices,
such as the Deegan–Packel index. In another follow-up paper, Rey and Rothe (2010) investigate false-name manipulations in weighted voting games with respect to the probabilistic
Banzhaf index, i.e., the one suggested by Dubey and Shapley (1979). Although the probabilistic Banzhaf index is more useful for measuring the actual probability of inﬂuencing a
decision, it does not ﬁt the framework of using power indices to share resources or power,
because the probabilistic Banzhaf index is not normalized.

2. Preliminaries and Notation
We start by introducing the notions that will be used throughout this paper.
2.1 Coalitional Games
A coalitional game G = (N, v) is given by a set of players N = {1, . . . , n}, and a characteristic function v : 2N → R, which maps any subset, or coalition, of players to a real value. This
value is the total utility these players can guarantee to themselves when working together.
A coalitional game G = (N, v) is called monotone if v(S) ≤ v(T ) for any S ⊆ T . Further,
G is called simple if it is monotone and v can only take values 0 and 1, i.e., v : 2N → {0, 1}.
In such games, we say that a coalition S ⊆ N wins if v(S) = 1, and loses if v(S) = 0. A
player i is critical, or pivotal, for a coalition S if adding this player to S turns it from a
losing coalition into a winning coalition: v(S) = 0, v(S ∪ {i}) = 1. A player i is a veto
player if he is necessary for forming a winning coalition, i.e., v(S) = 0 for any S ⊆ N \ {i}
(for monotone games, this is equivalent to requiring v(N \ {i}) = 0).
2.2 Weighted Voting Games
A weighted voting game G is a simple game that is described by a vector of players’ weights
w = (w1 , . . . , wn ) ∈ (R+ )n and a quota q ∈ R+ . We write G = [q; w1 , . . . , wn ], or G = [q; w].
In these games, a coalition is winning if
its total weight meets or exceeds the quota. Formally,
for any S ⊆ N we have v(S) = 1 if i∈S wi ≥ q and v(S) = 0 otherwise. We will often
61

Aziz, Bachrach, Elkind, & Paterson


write w(S) to denote the total weight of a coalition S, i.e., w(S) = i∈S wi . Also, we set
wmax = maxi=1,...,n wi . We will make the standard assumption that w(N ) ≥ q, i.e., the
grand coalition is winning. Note that if q = w(N ), then any player i ∈ N is a veto player.
2.3 Power Indices
For each player, both her Shapley–Shubik index and her Banzhaf index are determined
by this player’s expected marginal contribution to all possible coalitions; however, the two
indices make use of diﬀerent probabilistic models.
The Shapley–Shubik index is a specialization of Shapley value—a classic solution concept
for coalitional games—to simple games. In more detail, let Πn be the set of all possible
permutations (orderings) of n players. Each π ∈ Πn is a one-to-one mapping from {1, . . . , n}
to {1, . . . , n}. Denote by Sπ (i) the set of predecessors of player i in π, i.e., Sπ (i) = {j |
π(j) < π(i)}. The Shapley value of the i-th player in a game G = (N, v) is denoted by
ϕi (G) and is given by the following expression:
1 
[v(Sπ (i) ∪ {i}) − v(Sπ (i))].
(1)
ϕi (G) =
n!
π∈Πn

We will occasionally abuse notation and say that a player i is pivotal for a permutation π
if it is pivotal for the coalition Sπ (i).
The Shapley–Shubik power index is simply the Shapley value in a simple coalitional
game (and therefore in the rest of the paper we will use these terms interchangeably). In
such games the value of each coalition is either 0 or 1, so formula (1) simply counts the
fraction of all orderings of the players in which player i is critical for the coalition formed
by his predecessors. The Shapley–Shubik power index thus reﬂects the assumption that
when forming a coalition, any ordering of the players entering the coalition has an equal
probability of occurring, and expresses the probability that player i is critical.
In contrast, the Banzhaf index computes the probability that the player is critical under
the assumption that all coalitions of the players are equally likely. Formally, given a game
G = (N, v), for each i ∈ N we denote by ηi (G) the number of coalitions for which i is
critical in a game G. The Banzhaf index of a player i in a WVG G = (N, v) is
ηi (G)
.
j∈N ηj (G)

βi (G) = 

While there exist several other approaches to determining the players’ inﬂuence in a
game, the Shapley–Shubik index and the Banzhaf index have many useful properties that
make them very convenient to work with. We will make use of three of these properties,
namely, the normalization property, the symmetry property, and the dummy player property. The normalization property simply states that the sum of Shapley–Shubik indices (or
Banzhaf indices) of all players is equal to 1. The symmetry property says that two players
i, j that make the same contribution to any coalition, i.e., such that v(S ∪ {i}) = v(S ∪ {j})
for any S ⊆ N \ {i, j}, have equal values of the index. The dummy player property claims
that for a dummy player both indices equal 0, where a player i is called a dummy if he
contributes nothing to any coalition, i.e., for any S ⊆ N we have v(S ∪ {i}) = v(S). It
is easy to verify from the deﬁnitions that both the Shapley–Shubik index and the Banzhaf
index have these properties.
62

False-Name Manipulations in WVGs

3. Weight-Splitting: Examples
In real-world situations modeled by weighted voting games, players may be able to split,
dividing their resources (weight) arbitrarily among the new identities. The payoﬀ would
then be distributed among the agents according to their power in the resulting game. Intuitively, the total payment obtained by the new identities should be equal to the payoﬀ
of the original player before the split. However, we will now demonstrate that this is not
the case if the payoﬀ is distributed according to either the Shapley–Shubik index or the
Banzhaf index.
We ﬁrst show that players can use weight-splitting to increase their power.
Example 1. [Advantageous splitting] Consider the WVG [6; 2, 2, 2]. By symmetry,
each player has a Banzhaf index of 1/3, and a Shapley–Shubik index of 1/3. If the last
player splits up into two players, the new game is [6; 2, 2, 1, 1]. In this game, just as in the
original game, the only winning coalition is the grand coalition, and hence all players are
equally powerful. Thus, the split-up players have a Banzhaf index of 1/4 each, as well as a
Shapley–Shubik index of 1/4 each, i.e., weight-splitting increases the manipulator’s power
by a factor of (2 · 1/4)/(1/3) = 3/2 according to both indices.
However, weight-splitting may also be harmful.
Example 2. [Disadvantageous splitting] Consider the WVG [5; 2, 2, 2]. Again, by symmetry, each player has a Banzhaf index of 1/3, and a Shapley–Shubik index of 1/3. If
the last player splits up into two players, the new game is [5; 2, 2, 1, 1]. Each of the new
players is pivotal for exactly one coalition, while each of the players of weight 2 is pivotal
for three coalitions. Thus, the new players have a Banzhaf index of 1/8 each. Similarly,
each of the new players is pivotal for a permutation if and only if it appears in the third
position, followed by the other new player, i.e., the new players have Shapley–Shubik index
of 2/24 = 1/12. Thus, weight splitting decreases the player’s power by a factor of 4/3
according to the Banzhaf index and by a factor of 2 according to the Shapley–Shubik index.
Finally, weight-splitting may have no eﬀect on the player’s power.
Example 3. [Neutral splitting] Consider the WVG [4; 2, 2, 2]. As in the previous examples, by symmetry, each player has a Banzhaf index of 1/3, and a Shapley–Shubik index
of 1/3. If the last player splits up into two players, the new game is [4; 2, 2, 1, 1]. In this
game, each of the new players is pivotal for 2 coalitions, while each of the players of weight
2 is pivotal for 4 coalitions. Thus, the split-up players have a Banzhaf index of 1/6 each.
Similarly, each of the new players is pivotal for a permutation if and only if it appears in
the third position, followed by one of the players of weight 2. There are exactly 4 such
permutations, so the Shapley–Shubik index of each of the new players is 1/6. We have
2 · 1/6 = 1/3, i.e., according to both indices, the player’s total power did not change.
In all examples presented so far, weight-splitting had the same eﬀect on the Shapley–
Shubik index and the Banzhaf index of the manipulator. We will now show that this is not
always the case.
Example 4. Consider the WVG [5; 2, 1, 1, 1, 1]. In this game, the ﬁrst player is pivotal for a
permutation if he appears in the last or second-to-last position, but not in earlier positions.
63

Aziz, Bachrach, Elkind, & Paterson

Thus, his Shapley–Shubik index is 2/5. Further, this player is pivotal for any coalition that
contains three or four players of weight 1, i.e., for 5 coalitions. On the other hand, any
player of weight 1 is pivotal for any coalition that contains the player of weight 2 as well as
any two other players of weight 1, i.e., for 3 coalitions. Thus the Banzhaf index of the ﬁrst
player is given by 5/(5 + 4 · 3) = 5/17.
Now, if the ﬁrst player splits into two players of weight one, in the resulting game all
players have the same weight. Therefore, for each of them the value of both indices is 1/6,
and hence the total power of the manipulator is 1/3.
It remains to observe that 2/5 > 1/3, but 5/17 < 1/3, i.e., weight-splitting hurts the
manipulator if the payoﬀ is distributed according to the Shapley–Shubik index, but helps
him if the Banzhaf index is used. Further, this example can be generalized to any weighted
voting game of the form [n; 2, 1, . . . , 1], where there are n − 1 players of weight 1 and n ≥ 5:
in any such game, weight-splitting lowers the payoﬀ of the ﬁrst player according to the
2
, but increases his payoﬀ according to the Banzhaf
Shapley–Shubik index from n2 to n+1
n
2
index from (n−1)2 +1 to n+1 .

4. Splitting: Bounds of Manipulation
We have seen that a player can both increase and decrease his total payoﬀ by splitting his
weight. In this subsection, we provide upper and lower bounds on how much he can change
his payoﬀ by doing so. We restrict our attention to the case of splitting into two identities;
the general case is brieﬂy discussed in Section 9.
To simplify notation, in the rest of this section we assume that in the original game
G = [q; w1 , . . . , wn ] the manipulator is player n, and he splits into two new identities n and
n , resulting in a new game G . We ﬁrst consider the case of the Shapley–Shubik index,
followed by the analysis for the Banzhaf index.
4.1 Shapley–Shubik Index
We start by providing a tight upper bound on the beneﬁts of manipulation.
Theorem 5. For any game G = [q; w1 , . . . , wn ] and any split of n into n and n , we have
2n
ϕn (G ) + ϕn (G ) ≤ n+1
ϕn (G), i.e., the manipulator cannot gain more than a factor of
2n/(n + 1) < 2 by splitting his weight between two identities. Moreover, this bound is tight,
i.e., there exists a game in which player n increases his payoﬀ by a factor of 2n/(n + 1) by
splitting into two identities.
Proof. Fix a split of n into n and n . Let Πn−1 be the set of all permutations of the ﬁrst
n − 1 players. Consider any π ∈ Πn−1 . Let P (π) be the set of all permutations of the
players in G that can be obtained by inserting n and n into π. Let Π∗n+1 be the set of all
permutations π ∗ of players in G such that n or n is pivotal for π ∗ . Finally, let P ∗ (π, k)
be the subset of P (π) ∩ Π∗n+1 that consists of all permutations π  ∈ P (π) in which at least
one of the players n and n appears between the k-th and the (k + 1)-st element of π  and
is pivotal for π  . Every permutation in Π∗n+1 appears in one of the sets P ∗ (π, k) for some
64

False-Name Manipulations in WVGs

π, k, so we have
ϕn (G ) + ϕn (G ) =


|Π∗n+1 |
1
|P ∗ (π, k)|.
≤
(n + 1)!
(n + 1)!
π,k

On the other hand, it is not hard to see that |P ∗ (π, k)| ≤ 2n for any π, k: there are two
ways to place n and n between the k-th and the (k +1)-st element of π, n−1 permutations
in P ∗ (π, k) in which n appears after the k-th element of π, but n is not adjacent to it,
and n − 1 permutations in P ∗ (π, k) in which n appears after the k-th element of π, but
n is not adjacent to it. Moreover, if P ∗ (π, k) is not empty, then n is pivotal for the
permutation f (π, k) obtained from π by inserting n after the k-th element of π. Further, if
(π1 , k1 ) 	= (π2 , k2 ) then f (π1 , k1 ) 	= f (π2 , k2 ). Hence,
ϕn (G) ≥

1
n!


π,k:P ∗ (π,k)=∅

1≥

1  ∗
n+1
|P (π, k)| ≥
(ϕn (G ) + ϕn (G )).
n! · 2n
2n
π,k

2n
ϕn (G) ≤ 2ϕn (G), i.e., the manipulator cannot
We conclude that ϕn (G ) + ϕn (G ) ≤ n+1
gain more than a factor of 2n/(n + 1) < 2 by splitting his weight between two identities.
To see that this bound is tight, consider the game G = [2n; 2, 2, . . . , 2] and suppose that
one of the players (say, n) decides to split into two identities n and n resulting in the game
G = [2n; 2, . . . , 2, 1, 1]. In both games the only winning coalition consists of all players, so we
2n
ϕn (G).
have ϕn (G) = 1/n, ϕn (G ) = ϕn (G ) = 1/(n + 1), i.e., ϕn (G ) + ϕn (G ) = n+1

We have seen that no player can increase his payoﬀ by more than a factor of 2 by
splitting his weight between two identities. In contrast, we will now show that a player can
decrease his payoﬀ by a factor of Θ(n) by doing so. This shows that a would-be manipulator
has to be careful when deciding whether to split his weight, and motivates the algorithmic
questions studied in the next two sections.
Theorem 6. For any game G = [q; w1 , . . . , wn ] and any split of n into n and n , we have
ϕn (G ) + ϕn (G ) ≥ n+1
2 ϕn (G), i.e., the manipulator cannot lose more than a factor of
(n + 1)/2 by splitting his weight between two identities. Moreover, this bound is tight,
Proof. To prove the ﬁrst part of the theorem, ﬁx a split of n into n and n and consider
any permutation π of players in G such that n is pivotal for π. It is easy to see that at
least one of n and n is pivotal for the permutation f (π) obtained from π by replacing
n with n and n (in this order). Similarly, at least one of n and n is pivotal for the
permutation g(π) obtained from π by replacing n with n and n (in this order). Moreover,
all such permutations of players in G are distinct, i.e., for any π, π  we have g(π) 	= f (π  ),
and π 	= π  implies f (π) 	= f (π  ), g(π) 	= g(π  ). Hence, if Π∗n is the set of all permutations
π of the players in G such that n is pivotal for π, and Π∗n+1 is the set of all permutations π
of the players in G such that n or n is pivotal for π, we have |Π∗n+1 | ≥ 2|Π∗n | and
ϕn (G ) + ϕn (G ) =

|Π∗n+1 |
2|Π∗n |
2
≥
=
ϕn (G).
(n + 1)!
(n + 1)!
n+1

To see that this bound is tight, consider the game G = [2n − 1; 2, 2, . . . , 2] and suppose
that one of the players (say, n) decides to split into two identities n and n resulting in
65

Aziz, Bachrach, Elkind, & Paterson

the game G = [2n − 1; 2, . . . , 2, 1, 1]. In the original game G, the only winning coalition
consists of all players, so we have ϕn (G) = 1/n. Now, consider any permutation π of
the players in G . We claim that n is pivotal for π if and only if it appears in the n-th
position of π, followed by n . Indeed, if π(n ) = n, π(n ) = n + 1, then all players in the
ﬁrst n − 1 positions have weight 2, so w(Sπ (n )) = 2n − 2, w(Sπ (n ) ∪ {n }) = 2n − 1.
Conversely, if π(n ) = n + 1, we have w(Sπ (n )) = 2n − 1 = q, and if π(n ) ≤ n − 1,
we have w(Sπ (n ) ∪ {n }) ≤ 2(n − 1). Finally, if π(n ) = n, but π(n ) 	= n + 1, we have
w(Sπ (n ) ∪ {n }) = 2n − 2 < q. Consequently, n is pivotal for (n − 1)! permutations, and, by
the same argument, n is also pivotal for (a disjoint set of) (n − 1)! permutations. Hence,
2
we have ϕn (G ) + ϕn (G ) = 2(n−1)!
(n+1)! = n+1 ϕn (G).
4.2 Banzhaf Index
For the Banzhaf index, we can obtain similar bounds on the maximum gains and losses
from a weight-splitting manipulation.
Theorem 7. For any game G = [q; w1 , . . . , wn ] and any split of n into n and n , we have
βn (G ) + βn (G ) ≤ 2βn (G). Moreover, this bound is asymptotically tight.
Proof. Assume that player n splits up into n and n and that wn ≤ wn . Consider a losing
coalition C for which n is critical in G. Then w(C) < q ≤ w(C) + wn = w(C) + wn + wn .
We have the following possibilities:
• q − w(C) ≤ wn . In this case n and n are critical for C in G .
• wn < q − w(C) ≤ wn . In this case n is critical for C ∪ {n } and C in G .
• q − w(C) > wn . In this case n is critical for C ∪ {n } and n is critical for C ∪ {n }
in G .
Therefore we have ηn (G ) + ηn (G ) = 2ηn (G) in each case.
Now consider a player i ∈ N \ {n}. Suppose that i is critical for a coalition C in G. If
n ∈ C, then i is also critical for the coalition C  = C \ {n} ∪ {n , n } in G . On the other
hand, if n 	∈ C, then i also remains critical for C in G . Hence ηi (G) ≤ ηi (G ). Moreover,
i may also be critical for some coalitions in G that contain just one of n and n , so the
above inequality will not in general be an equality. Thus, we have
βn (G ) + βn (G ) =
≤
≤

2ηn (G) +

2ηn (G)


i∈N \{n} ηi (G

)

2ηn (G)

2ηn (G) + i∈N \{n} ηi (G)
ηn (G) +

2ηn (G)


i∈N \{n} ηi (G)

= 2βn (G).

To see that this boundis tight,
consider a WVG
n−2G = [n − 1; 1, . . . , 1, 2] with n players.

We have ηn (G) = n − 1 + n−1
(G)
=
1
+
and
η
for i 	= n. Therefore,
i
2
2
n−1
n−1+ 2
n
n−1
n−2 = 2
βn (G) =
∼ 1/n.
n − 4n + 8
n − 1 + 2 + (n − 1)(1 + 2 )
66

False-Name Manipulations in WVGs

If player n splits up into two players n and n with weights 1 each, then in the resulting
1
. Thus for large n, βn (G ) + βn (G ) =
game G the Banzhaf index of each player is n+1
2
n+1 ∼ 2βn (G).
We can also bound the damage that can be incurred by weight-splitting.
Theorem 8. For any game G = [q; w1 , . . . , wn ] and any split of n into n and n , we have
βn (G ) + βn (G ) ≥ n1 βn (G).
Proof. Suppose that player n splits into two players n and n with weights wn and wn ,
respectively. We assume without loss of generality that wn ≤ wn . Now, consider an
arbitrary player i 	= n, and let
Ti = {S ⊆ N \ {i} | w(S) < q, w(S) + wi ≥ q},
Si = {S ⊆ N \ {n, i} ∪ {n , n } | w(S) < q, w(S) + wi ≥ q}.
We have ηi (G) = |Ti |, ηi (G ) = |Si |. Further, set
Si1 = {S ∈ Si | i is pivotal for S \ {n , n }},

Si2 = {S ∈ Si | i is not pivotal for S \ {n , n } and n ∈ S, n 	∈ S},

Si3 = {S ∈ Si | i is not pivotal for S \ {n , n } and n ∈ S, n 	∈ S},
Si4 = {S ∈ Si | i is not pivotal for S \ {n , n } and n , n ∈ S}.

We claim that Si = ∪4j=1 Sij . Indeed, if S 	∈ Si1 , then i is pivotal for S, but not for S \{n , n },
and hence it must be the case that S ∩{n , n } =
	 ∅; all such sets are included in Si1 ∪Si2 ∪Si3 .


For any S ∈ Si , let f (S) = S \ {n , n }, g(S) = S \ {n , n } ∪ {n}. If S ∈ Si1 , then
f (S) ∈ Ti , and for each set T ∈ Ti there are at most 4 sets S such that f (S) = T , i.e.,
|f −1 (T )| ≤ 4. Further, S ∈ Si4 implies g(S) ∈ Ti , and |g −1 (T )| ≤ 1 for any T ∈ Ti . Finally,
we have g(S  ) 	= f (S  ) for any S  , S  ∈ Si . Taken together, these observations imply that
|Si1 | + |Si4 | ≤ 4|Ti | = 4ηi (G).
Now, consider an S ∈ Si2 . We have w(S) < q, w(S) + wi < q, w(S) + wi + wn ≥ q.
Hence, n is critical for S ∪ {i}. Similarly, if S ∈ Si3 , it follows that n is critical for S ∪ {i}.
Therefore, we have |Si2 | + |Si3 | ≤ ηn (G ) + ηn (G ). We obtain
ηi (G ) = |Si | ≤

4

j=1

|Sij | ≤ 4ηi (G) + ηn (G ) + ηn (G ) = 4ηi (G) + 2ηn (G),

where the last equality follows from the proof of Theorem 7. Thus, we obtain
βn (G ) + βn (G ) =
≥
≥

2ηn (G) +

2ηn (G)


2ηn (G) + 4
2n

i∈N \{n} ηi (G

)

2ηn (G)
i∈N \{n} ηi (G) + 2(n − 1)ηn (G)



2η (G)
βi (G)
n
=
.
η
(G)
n
i∈N i

67

Aziz, Bachrach, Elkind, & Paterson

While it is not clear if the bound given in Theorem 8 is tight, our next example shows
that splitting into two
players can decrease a player’s payoﬀ according to the Banzhaf index
n
.
by a factor of almost 2π
Example 9. Consider a WVG G = [3k; 1, . . . , 1, 4k] with n = 2k players. Let N1 be the
set of all players of weight 1, i.e., N1 = {1, . . . , n − 1}. It is easy to see that player n is
critical for any coalition, while all other players are dummies, so we have βn (G) = 1. Now
suppose that player n splits up into new identities n and n with weights wn = wn = 2k.
For player n to be critical for a coalition S in G , it has to be the case that either n 	∈ S,
k ≤ |S ∩ N1 | ≤ n − 1 or n ∈ S, 0 ≤ |S ∩ N1 | ≤ k − 1. Thus, we have




η (G ) = η (G ) =
n

n


n 

n−1
i=0

i

= 2n−1 .

Moreover, for a player i with weight 1 to be critical for a coalition in G , the coalition must
include exactly oneof n or n as well as k − 1 of the n − 2 other players in N1 \ {i}. Thus,
we have ηi (G ) = 2 2k−2
for the asymptotics of the
k−1 for i < n. Using the standard formulas
	
2k−2
2
4k−1 . We obtain
central binomial coeﬃcient, we can approximate 2 k−1 by 2 π(2k−1)


2 · 2n−1



βn (G ) + βn (G ) ≈

2n−1

+

2n−1

2

	
	 ∼
=
√
2
+ (n − 1) π(n−1)
2n−1
2 + n − 1 π2




2π
.
n

5. Complexity of Finding a Beneﬁcial Split
We now examine the problem of ﬁnding a beneﬁcial weight split in weighted voting games
from the computational perspective. Ideally, the manipulator would like to ﬁnd a payoﬀmaximizing split, i.e., a way to split his weight among two or more identities that results
in the maximal total payoﬀ. A less ambitious goal is to decide whether there exists a
manipulation that increases the manipulator’s payoﬀ. However, it turns out that even this
problem is computationally hard. In the rest of the section, we show that checking whether
there exists a payoﬀ-increasing split is NP-hard both for the Shapley–Shubik index and the
Banzhaf index; this holds even if the player is only allowed to use two identities. That is,
in the spirit of the groundbreaking papers of Bartholdi and Orlin (1991) and Bartholdi,
Tovey, and Trick (1989, 1992), we show that computational complexity acts as a barrier to
manipulative behavior.
To formally deﬁne our computational problems, we require that all weights and the
quota of both the original game and the new game are integers given in binary, i.e., we
only allow integer splits. We remark that this assumption is not entirely without loss of
generality: there are games where a player can beneﬁt from a fractional split more than
from any integer split. One such example is given by the game [3; 1, 1, 1], where there
are no non-trivial integer splits available to the players, but, similarly to Example 1, each
player can increase his power by a factor of 3/2 by splitting into two players with weight
1/2. However, in real-life settings there is usually a natural bound on the granularity of
the weights: if the weight is the number of supporters of a given party, it needs to be an
68

False-Name Manipulations in WVGs

integer, and if it is the monetary contribution of a player, it usually has to be an integer
number of dollars (or, at least, cents), i.e., our assumption reﬂects real-life constraints.
We are now ready to deﬁne our problems.
Name: Beneficial-SS-Split
Instance: (G, ) where G = [q; w1 , . . . , wn ] is a weighted voting game and  ∈ {1, . . . , n}.
Question: Is there a way for player  to split
w between sub-players 1 , . . . , m
 his weight
 ) > ϕ (G)?
so that in the new game G it holds that m
ϕ
(G

j=1 j
The deﬁnition of Beneficial-BI-Split is similar.
Name: Beneficial-BI-Split
Instance: (G, ) where G = [q; w1 , . . . , wn ] is a weighted voting game and  ∈ {1, . . . , n}.
Question: Is there a way for player  to split
w between sub-players 1 , . . . , m
m his weight


so that in the new game G it holds that j=1 βj (G ) > β (G)?
Note that we are looking for a strictly beneﬁcial manipulation, i.e., one that increases
the manipulator’s total payoﬀ, rather than one that is simply not harmful.
We will prove that Beneficial-SS-Split and Beneficial-BI-Split are NP-hard. Our
hardness results are based on reductions from the following classic NP-hard problem:
Name: Partition
Instance: A set of k integer weights A = {a1 , . . . , ak }.
Question: Is it possible
to partition
A into two subsets P1 ⊆ A, P2 ⊆ A so that P1 ∩P2 = ∅,


P1 ∪ P2 = A, and ai ∈P1 ai = ai ∈P2 ai ?
We will ﬁrst prove a simple lemma that will be used in all NP-hardness proofs in this
paper.
Lemma 10. Let A = {a1 , . . . , ak } be a “no”-instance of Partition. Then for any
 weighted
. . , wn ] such that n > k, wi = 8ai for i = 1, . . . , k, q = 4 ai ∈A ai +
voting game G = [q; w1 , . 
r, where 0 < r < 4, and ni=k+1 wi < 4, it holds that all players k + 1, . . . , n are dummies,
and hence their Shapley–Shubik and Banzhaf indices are equal to 0.
Proof. Consider a player i with k < i ≤ n and a set S ⊆ N \ {i}. We will show that i is
not pivotal for S.
Set N0 = {1, . . . , k} and let S0 = S ∩ N0 . The set N0 can be partitioned into two
equal-weight subsets if and only if A can, so either w(S0 ) < w(N0 )/2, or w(S0 ) > w(N0 )/2.
Moreover, the weights of all players in N0 are multiples of 8, so w(N0 )/2 is a multiple of 4.
Similarly, the weight of S0 is a multiple of 8. Hence, if w(S0 ) < w(N0 )/2, it follows that
w(S0 ) ≤ w(N0 )/2 − 4 and w(S ∪ {i}) < w(N0 )/2 − 4 + 4 < q. Therefore, we have v(S) = 0,
v(S ∪ {i}) = 0, i.e., i is not pivotal for S. On the other hand, if w(S0 ) > w(N0 )/2, then
w(S0 ) ≥ w(N0 )/2 + 4 > q, so S0 is a winning coalition. Therefore, i is not pivotal for S in
this case as well.
Theorem 11. Beneficial-BI-Split is NP-hard, and remains NP-hard even if the player
can only split into two players with equal weights.
Proof. Given an instance of Partition A = {a1 , . . . , ak }, we construct aweighted voting
game G = [q; w1 , . . . , wn ] with n = k + 1 players as follows. We let X = ai ∈A ai , and set
wi = 8ai for i = 1, . . . , n − 1, wn = 2, and q = 4X + 2. Also, we set  = n. Since wn = 2,
69

Aziz, Bachrach, Elkind, & Paterson

the only integer split available to player n is into two identities n and n with weight 1
each. Let G = [q; w1 , . . . , wn−1 , 1, 1] be the resulting game.
If A is a “no”-instance of Partition, then Lemma 10 implies that player n is a dummy,
and, moreover, if he splits into sub-players, these sub-players are also dummies. Therefore
(G, ) is a “no”-instance of Beneficial-BI-Split.
Now let us assume that A is a “yes”-instance of Partition. Let x denote the number
of coalitions in N \ {n} of weight 4X. Then ηn (G) = x. For i = 1, . . . , n − 1, let
Si = {S ⊆ N \ {n, i} | w(S) < 4X, w(S) + wi ≥ q},

and set yi = |Si |. Also, set y = n−1
i=1 yi .
Consider a player i < n. Observe that exactly half of the x subsets of {1, . . . , n − 1} of
weight 4X contain i. For any such subset T , player i is pivotal for (T \ {i}) ∪ {n}. Further,
for any coalition S ∈ Si , player i is pivotal for both S and S ∪ {n}. Therefore for i < n we
have ηi (G) = x2 + 2yi . We obtain
βn (G) =

x
.
x + (n − 1) x2 + 2y

On the other hand, in the new game G we have ηn (G ) = ηn (G ) = x. Moreover, for
i < n we have ηi (G ) = x2 + 4yi , since each coalition in Si corresponds to 4 coalitions for
which i is pivotal, namely, S, S ∪ {n }, S ∪ {n }, and S ∪ {n , n }. Thus,
βn (G ) + βn (G ) =

2x
> βn (G),
2x + (n − 1) x2 + 4y

where the last inequality holds since x > 0. Thus, a “yes”-instance of Partition corresponds to a “yes”-instance of Beneficial-BI-Split.
We now consider the problem of ﬁnding a beneﬁcial split for the Shapley–Shubik index.
Theorem 12. Beneficial-SS-Split is NP-hard, and remains NP-hard even if the player
can only split into two players with equal weights.

Proof. Given an instance A = {a1 , . . . , ak } of Partition, we set X = ai ∈A ai , and create
a weighted voting game G = [4X + 3; 8a1 , . . . , 8ak , 1, 2] with n = k + 2 players. Also, we set
N0 = {1, . . . , n − 2}.
If A is a “no”-instance of Partition, then Lemma 10 implies that player n is a dummy,
and if he splits into several players, all of them will be dummies, too. Thus, we have
constructed a “no”-instance of Beneficial SS-Split.
Now, suppose that A is a “yes”-instance of Partition. Let P1 , P2  be a partition of
A, so w(P1 ) = w(P2 ). It corresponds to a partition S, N0 \ S of N0 , where i ∈ S if and
only if ai ∈ P1 ; observe that w(S) = w(N0 \ S). Set s = |S|, so |N0 \ S| = n − s − 2.
It is easy to see that n is critical for S ∪ {n − 1} as well as for (N0 \ S) ∪ {n − 1}. There
are (s + 1)!(n − 2 − s)! permutations of 1, . . . , n that put n directly after some permutation
of S ∪ {n − 1}. Similarly, there are s!(n − 1 − s)! permutations putting n directly after some
permutation of (N0 \ S) ∪ {n − 1}. Thus, for each partition P i = P1i , P2i , where |P1i | = s,
70

False-Name Manipulations in WVGs

we have at least (s + 1)!(n − 2 − s)! + s!(n − 1 − s)! distinct permutations where n is critical.
On the other hand, as argued above, if S is a subset of N0 such that w(S) 	= w(N0 )/2,
then n is not critical for S or S ∪ {n − 1}, since either w(S) ≤ w(N0 )/2 − 4 < q − 3 or
w(S) ≥ w(N0 )/2 + 4 > q.
Let P be the set of all partitions of A, where each partition is counted only once, i.e.,
P contains exactly one of the P1 , P2  and P2 , P1 . For each P i = P1i , P2i  ∈ P, we denote
|P1i | = si . There is a total of n! permutations of players in G. Thus, the Shapley–Shubik
index of n in G is
 (si + 1)!(n − 2 − si )! + si !(n − 1 − si )!
=
ϕn (G) =
n!
i
P ∈P

 si !(n − 2 − si )!(si + 1 + n − 1 − si )
=
n!
i

P ∈P

 nsi !(n − 2 − si )!
 si !(n − 2 − si )!
=
.
n!
(n − 1)!
i
i

P ∈P

P ∈P

We now consider what happens when n splits into two players, n and n with wn =
wn = 1, resulting in a game G = [4X + 3; 8a1 , . . . , 8ak , 1, 1, 1].
Again, let P1 , P2 , |P1 | = si , |P2 | = n − si , be a partition of A such that w(P1 ) = w(P2 ),
and let S, N0 \ S be the corresponding partition of N0 . There are (si + 2)!(n − 2 − si )!
permutations that place n directly after some permutation of S ∪ {n − 1, n }, and n is
critical for each of them. Similarly, n is critical for the si !(n − si )! permutations that place
n directly after some permutation of (N0 \ S) ∪ {n − 1, n }.
Thus, each partition P i = P1i , P2i  with |P1i | = si , corresponds to (si + 2)!(n − 2 −
si )! + si !(n − si )! distinct permutations where n is critical. By symmetry, there are (si +
2)!(n − 2 − si )! + si !(n − si )! distinct permutations where n is critical. There are n + 1
players in G , so there is a total of (n + 1)! permutations of the players. Thus each partition
i )!
to the Shapley–Shubik index of n in G, and
P i = P1i , P2i , |P1i | = si , contributes si !(n−2−s
(n−1)!
i )!+si !(n−si )!
2 (si +2)!(n−2−s
to the sum of the Shapley–Shubik indices of n and n in G . We
(n+1)!
will now show that for any partition P i

2

(si + 2)!(n − 2 − si )! + si !(n − si )!
si !(n − 2 − si )!
>
.
(n + 1)!
(n − 1)!

(2)

Summing these inequalities over all partitions P i implies ϕn (G ) + ϕn (G ) > ϕn (G), as
desired. To prove inequality (2), note that it can be simpliﬁed to
2

(s + 1)(s + 2) + (n − 1 − s)(n − s)
> 1,
n(n + 1)

where we use s instead of si to simplify notation, or, equivalently,
2(s + 1)(s + 2) + 2(n − 1 − s)(n − s) − n(n + 1) > 0.
Now, observe that
2(s + 1)(s + 2) + 2(n − 1 − s)(n − s) − n(n + 1) = (n − 2 − 2s)2 + n > 0
71

Aziz, Bachrach, Elkind, & Paterson

for any n > 0. This proves inequality (2) for any n > 0. It follows that if A is a “yes”instance of Partition, player n always gains by splitting into two players of weight 1, i.e.,
(G, n) is a “yes”-instance of Beneficial-SS-Split.
Remark 13. It can be veriﬁed that both of our proofs go through even if we allow noninteger splits, i.e., our hardness results are independent of the integrality assumption. Further, note that we have not shown that Beneficial-BI-Split and Beneficial-SS-Split
are in NP, i.e., we have not proved that these problems are NP-complete. There are two
reasons for this. First, if we allow splits into an arbitrary number of identities, some of the
candidate solutions may have exponentially many new players (e.g., a player with weight
wi can split into wi players of weight 1). Second, even if we circumvent this issue by only
considering splits into a polynomial number of identities, it is not clear how to verify in
polynomial time whether a particular split is beneﬁcial. In fact, since computing both power
indices in weighted voting games is #P-hard, it is quite possible that our problems are not
in NP.

6. Computing Beneﬁcial Splits
In Section 5, we have shown that it is hard even to test if any beneﬁcial split exists, let alone
to ﬁnd the optimal split. This can be seen as a positive result, since complexity of ﬁnding
beneﬁcial splits serves as a barrier for this kind of manipulative behavior. However, it turns
out that in many cases manipulators can overcome the problem. More precisely, in what
follows we show that in certain restricted domains manipulators can ﬁnd beneﬁcial splits
into two identities in polynomial time. We then consider manipulation algorithms that work
by approximating the Shapley–Shubik index (rather than calculating it precisely).
6.1 Examples
In this subsection, we describe two scenarios in which one of the players can always increase
his payoﬀ by splitting. While both of our examples rely on rather severe constraints on the
players’ weights and the threshold, there are practical weighted voting scenarios that satisfy
these constraints.
Example 14. It is not hard to see that Example 1 can be generalized to any weighted
voting game with w(N ) = q; such games are sometimes called unanimity games. Even
more generally, a player can always increase his payoﬀ by weight-splitting if the threshold
is set so high that any winning coalition must include all players; this holds both for the
Shapley–Shubik index and the Banzhaf index. Indeed, consider the class of weighted voting
games G = [q; w] that is characterized by the following condition: w(N ) − s < q ≤ w(N ),
where s = min{mini wi , wmax /2}. The condition s ≤ mini wi ensures that all players are
present in all winning coalitions, so the index value of each player is 1/n. Now, suppose that
the player i with the largest weight wi = wmax splits his weight (almost) equally between
two identities, i.e., sets wi = wi /2, wi = wi /2. As we also have s ≤ wmax /2, any
winning coalition still has to include all players. Therefore, in the new game the payoﬀ of
each player is 1/(n + 1), and hence the split increases the total payoﬀ of the manipulator
by a factor of 2n/(n + 1).
72

False-Name Manipulations in WVGs

Example 15. Our second example is speciﬁc to the Shapley–Shubik index. In this example,
a “small” player can beneﬁt from manipulation in the presence of “large” players, as long
as the threshold is suﬃciently high. Formally, consider the class of weighted voting games
of the form G = [q; w], where all wi , i = 1, . . . , n − 1, are multiples of some integer A, the
threshold q is of the form AT + b, b < A, and b < wn < min{2b − 1, A}. Suppose that all
winning coalitions have size at least n/2 + 1. This 
condition holds if we renumber the
players so that w1 ≥ w2 ≥ · · · ≥ wn−1 and require q > i=1,...,n/2	 wi .
Now, suppose that player n is pivotal for at least one coalition in this game (if all weights
are small multiples of A, this condition can be checked easily). Consider any permutation
π such that n is pivotal for π. We have w(Sπ (n)) = AT . Indeed, if w(Sπ (n)) > AT , then
w(Sπ (n)) ≥ AT + A > q, and the coalition Sπ (n) does not need player n to win. On the
other hand, if w(Sπ (n)) < AT , then w(Sπ (n)) ≤ AT − A, so w(Sπ (n)) + wn < AT + b = q.
|
Let P be the set of all such permutations; we have ϕn (G) = |P
n! .
Now suppose that n decides to split its weight between two new identities n and n
by setting wn = b − 1, wn = wn − b + 1; note that wn , wn < b. Consider any permutation
π. Suppose that n occurs in the k-th position in this permutation. By our assumption,
k ≥ n/2 + 1. We will construct 2k permutations πj , πj , j = 1, . . . , k, as follows. In each
of these permutations, players 1, . . . , n − 1 appear in the same order as in π. Moreover, in
πj , player n occurs in the j-th position, and player n occurs in the (k + 1)-st position.
Similarly, in πj , player n occurs in the j-th position, and player n occurs in the (k + 1)-st
position.
Observe that n is pivotal for any πj , j = 1, . . . , k. Indeed, the total weight of all players
that precede n in πj is AT +wn < q, while w(Sπj (n )∪{n }) > q. Similarly, w(Sπj (n )) < q,
so n is pivotal for any πj , j = 1, . . . , k. Hence, the total number of permutations of
1, . . . , n − 1, n , n for which either n or n is pivotal is at least 2k|P | ≥ (n + 2)|P |, and the
|
|P |
total Shapley–Shubik index of these players is at least (n+2)|P
(n+1)! > n! = ϕn (G). Hence, any
such split is strictly beneﬁcial for player n.
In addition to the scenarios discussed above, Fatima et al. (2007) describe several classes
of voting games where the Shapley–Shubik indices of all players can be computed in polynomial time; Aziz and Paterson (2008) prove similar results for the Banzhaf index. Clearly,
if the manipulator’s weight is polynomially bounded, and the original game as well as all
games that result from the manipulator splitting into two identities are “easy”, i.e., belong
to one the classes considered by Fatima et al. or Aziz and Paterson, the problem of ﬁnding
the beneﬁcial two-way split can be solved in polynomial time. However, our examples illustrate that a player may be able to decide whether it is beneﬁcial to split even if he cannot
compute his payoﬀ prior to the manipulation.
6.2 Pseudopolynomial and Approximation Algorithms
The hardness reductions in Section 5 are from Partition. While this problem in known
to be NP-hard, its hardness relies crucially on the fact that the weights of the elements
are represented in binary. Indeed, if the weights are given in unary, there is a dynamic
programming-based algorithm for this problem that runs in time polynomial in the size of
the input (such algorithms are usually referred to as pseudopolynomial ). In particular, if
all weights are polynomial in n, the running time of this algorithm is polynomial in n. In
73

Aziz, Bachrach, Elkind, & Paterson

many natural voting domains the weights of all players are not too large, so this scenario
is quite realistic. It is therefore natural to ask if there exists a pseudopolynomial algorithm
for the problem of ﬁnding a beneﬁcial split.
It turns out that the answer to this question is indeed positive as long as there is a
constant upper bound K on the number of identities that the manipulator can use and
all weights are required to be integers. To see this, recall that there is a pseudopolynomial algorithm for computing the Shapley–Shubik index of any player in a weighted voting
game (Matsui & Matsui, 2000). This algorithm is based on dynamic programming: for any
weight W and any 1 ≤ k ≤ n, it calculates the number of coalitions of size k that have
weight W . Thus, it can be easily adapted to work for the Banzhaf index as well.
One can use the algorithm of Matsui and Matsui (2000) to ﬁnd a beneﬁcial split for a
(1)
player i with weight wi in a game G as follows. Consider all possible splits wi = wi +
(K)
(j)
· · · + wi , where wi ∈ N for j = 1, . . . , K. The number of such splits is at most (wi )K ,
which is polynomial in n for constant K. Evaluate the Shapley–Shubik indices (respectively,
Banzhaf indices) of all new players in any such split and return “yes” if and only if at least
one of these splits results in an increased total payoﬀ. Let A(G) be the running time of
the algorithm of Matsui and Matsui on instance G. The running time of our algorithm is
O((wi )K K · A(G)), which is clearly pseudopolynomial.
We will now consider a more general setting, where only the weight of the manipulator
is polynomially bounded, while the weights of other players can be large. To simplify the
presentation, we limit ourselves to the case of two-way splits and the Shapley–Shubik index;
however, our approach also applies to splits into any constant number of identities and to
the Banzhaf index. We can use the same high-level approach as in the previous case, i.e.,
considering all possible splits (because of the weight restriction, there are only polynomially
many of them), and computing the indices of both new players for each split. However, if
we were to implement the latter step exactly, it would take exponential time. Therefore,
in this version of our algorithm, we replace the algorithm of Matsui and Matsui with an
approximation algorithm for computing the Shapley–Shubik index. Several such algorithms
are known; see, e.g., the work of Mann and Shapley (1960), Fatima et al. (2007), Bachrach
et al. (2010). We will use these algorithms in a black-box fashion. Namely, we assume
that we are given a procedure Shapley(G, i, δ, ) that for any given values of  > 0 and
δ > 0 outputs a number v that with probability 1 − δ satisﬁes |v − ϕi (G)| ≤  and runs in
time poly(n log wmax , 1/, 1/δ). We will now show how to use this procedure to design an
algorithm for ﬁnding a beneﬁcial split and relate the performance of our algorithm to that
of Shapley(G, i, δ, ).
Our algorithm is given in Figure 1. It takes parameters δ and  as inputs, and uses the
procedure Shapley(G, i, δ, ) as a subroutine. The algorithm outputs “yes” if it ﬁnds a split
whose total estimated payoﬀ exceeds the payoﬀ of the manipulator in the original game by
at least 3. It can easily be modiﬁed to output the (approximately) optimal split.
Proposition 16. With probability 1 − 3δ, the output of our algorithm satisﬁes the following: (i) If the algorithm outputs “yes”, then (G, i) admits a beneﬁcial integer split; (ii)
Conversely, if there is an integer split that increases the payoﬀ to the manipulator by more
than 6, our algorithm outputs “yes”. Moreover, the running time of our algorithm is
polynomial in nwi , 1/, and 1/δ.
74

False-Name Manipulations in WVGs

FindSplit(G = [q; w], i, δ, );
v ∗ =Shapley(G, i, δ, );
for j = 0, . . . , wi
wi = j, wi = wi − j;
G = [q; w1 , . . . , wi−1 , wi , wi , wi+1 , . . . , wn ];
v  = Shapley(G , i , δ, ), v  =Shapley(G , i , δ, );
v = v  + v  ;
if v > v ∗ + 3 then return yes;
return no;
Figure 1: Algorithm FindSplit(G = [q; w], i, δ, )
Proof. Suppose that the algorithm outputs “yes”. Consider the quantities v ∗ , v  and v 
computed by our algorithm. We have P rob[v ∗ < ϕi (G) − ] < δ, P rob[v  > ϕi (G ) + ] < δ,
P rob[v  > ϕi (G ) + ] < δ. Hence, with probability at least 1 − 3δ, if v  + v  > v ∗ + 3,
then ϕi (G) + ϕi (G ) + 2 > ϕi (G) −  + 3, or, equivalently, ϕi (G ) + ϕi (G ) > ϕi (G).
Conversely, suppose that there is a beneﬁcial split of the form (wi , wi ) that improves
player i’s payoﬀ by at least 6. As before, with probability at least 1 − 3δ we have that
v ∗ ≤ ϕi (G) +  and at the step j = wi it holds that v  ≥ ϕi (G ) − , v  ≥ ϕi (G ) − . Then
v = v  + v  ≥ ϕi (G ) + ϕi (G ) − 2 > ϕi (G) + 6 − 2 ≥ v ∗ + 3, so the algorithm will
output “yes”.
While our algorithm does not guarantee ﬁnding a successful manipulation, it is possible
to control the approximation quality (at the cost of increasing the running time), so that a
successful manipulation is found with high probability.
Thus we can see that manipulators have several ways to overcome the computational
diﬃculty of ﬁnding the optimal manipulation. Hence, other measures are required to avoid
such manipulations.

7. Merging and Annexation
Instead of a player splitting into smaller players, some players may merge into a single
entity. However, this situation is very diﬀerent from the game-theoretic perspective, as it
involves coordinated actions by several would-be manipulators who then have to decide how
to split the (increased) total payoﬀ. For the case of players merging to gain advantage, we
examine two cases. One is annexation where one player takes the voting weight of other
players. The annexation is advantageous if the payoﬀ of the new merged coalition in the
new game is greater than the payoﬀ of the annexer in the original game. The other case is
voluntary merging where players merge to become a bloc so that their new payoﬀ exceeds
the sum of their individual payoﬀs.
For any weighted voting game G, we denote the game that results from merging of the
players in a coalition S by G&S ; the set of players in the new game is (N \S)∪{&S}, and its
characteristic function is denoted by v&S . We will now deﬁne the computational problems
75

Aziz, Bachrach, Elkind, & Paterson

of checking whether there exist a beneﬁcial voluntary merge or annexation with respect to
the Shapley–Shubik index.
Name: Beneficial-SS-Merge
Instance: (G, S) where G = [q; w1 , . . . , wn ] is a weighted voting game and S⊂ {1, . . . , n}.
Question: If coalition S merges to form a new game G&S , is ϕ&S (G&S ) > i∈S ϕi (G)?
Name: Beneficial-SS-Annexation
Instance: (G, S, i) where G = [q; w1 , . . . , wn ] is a weighted voting game, 1 ≤ i ≤ n, and
S ⊂ {1, . . . , n} \ {i}.
Question: If i annexes S to form a new game G&(S∪{i}) , is ϕ&(S∪{i}) (G&(S∪{i}) ) > ϕi (G)?
We can easily adapt these deﬁnitions for the Banzhaf index; we will refer to the resulting problems as Beneficial-BI-Merge and Beneficial-BI-Annexation. We will ﬁrst
consider the issues related to annexation, followed by the analysis for merging.
7.1 Merging
As in the case of splitting, we expect it to be hard to ﬁnd a beneﬁcial merge. The following
theorem conﬁrms this intuition.
Theorem 17. Beneficial-SS-Merge and Beneficial-BI-Merge are NP-hard.
voting
Proof. Given an instance of Partition A = {a1 , . . . , ak }, we construct a weighted

game G = [q; w1 , . . . , wn ] with n = k + 3 players as follows. We set X = 4 ki=1 ai and let
wi = 8ai for i = 1, . . . , n − 3, wn−2 = wn−1 = wn = 1, and q = X + 2. We will now argue
that A is a “yes”-instance of Partition if and only if (G, {n − 1, n}) is a “yes”-instance of
both Beneficial-SS-Merge and Beneficial-BI-Merge.
If A is a “no”-instance of Partition, then by Lemma 10 players n and n − 1 are
dummies, and even if they merge together, the new player &{n − 1, n} remains a dummy
in the new game G&{n−1,n} . Thus, in this case (G, {n − 1, n}) is a “no”-instance for both
of our problems.
Now let us assume that A is a “yes”-instance of Partition. We will ﬁrst consider the
case of the Shapley–Shubik index, followed by the analysis for the Banzhaf index.
Set N0 = {1, . . . , n − 3}. Let P1 , P2  be a partition of A, and let S, N0 \ S be the
corresponding partition of N0 . Set s = |S|, so |N0 \ S| = n − s − 3. Player n is critical for
S ∪ {n − 2} and S ∪ {n − 1}, as well as for (N0 \ S) ∪ {n − 2} and (N0 \ S) ∪ {n − 1}. Thus,
for each partition P = P1 , P2 , where |P1 | = s, we have exactly 4(s + 1)!(n − 2 − s)! distinct
permutations where n is critical. Further, it is easy to see that n is not critical for any other
permutation. By symmetry, the same is true for n − 1. Thus, each partition P1 , P2  of A
to the sum of the Shapley–Shubik indices of n − 1
with |P1 | = s contributes 8 (s+1)!(n−2−s)!
n!
and n.
Now, consider the game G&{n−1,n} . Consider a partition S, N0 \ S of N0 with |S0 | = s
that corresponds to a partition P1 , P2  of A. Player &{n − 1, n} is critical for S and
S ∪ {n − 2}, as well as for N0 \ S and (N0 \ S) ∪ {n − 2}. Thus, each partition P1 , P2 
+ 2 (s+1)!(n−3−s)!
to the Shapley–Shubik index of
of A with |P1 | = s contributes 2 s!(n−2−s)!
(n−1)!
(n−1)!
76

False-Name Manipulations in WVGs

&{n − 1, n}. It remains to show that
8

(s + 1)!(n − 2 − s)!
s!(n − 2 − s)!
(s + 1)!(n − 3 − s)!
<2
+2
.
n!
(n − 1)!
(n − 1)!

This inequality can be simpliﬁed to 4(s + 1)(n − 2 − s) < n(n − 2 − s + s + 1) = n(n − 1),
which is equivalent to 0 < n(n − 1) − 4(s + 1)(n − 2 − s) = (n − 2s − 3)2 + n − 1. This
inequality clearly holds for n ≥ 1. Hence, ϕn−1 (G) + ϕn (G) < ϕ&{n−1,n} (G&{n−1,n} ), i.e.,
(G, {n − 1, n}) is a “yes”-instance of Beneficial-SS-Merge if and only if we started with
a “yes”-instance of Partition.
We will now show that the same is true for the Banzhaf index. Let x denote the number
of coalitions in N0 of weight 4X. Then ηn−2 (G) = ηn−1 (G) = ηn (G) = 2x. For i = 1, . . . , k,
let
Si = {S ⊆ N0 \ {i} | w(S) < 4X, w(S) + wi ≥ q},

and set yi = |Si |. Also, set y = ki=1 yi .
Consider a player i ∈ N0 . Observe that exactly half of the x subsets of N0 of weight 4X
contain i. For any such set T , player i is pivotal for (T \ {i}) ∪ {n − 2, n − 1}, (T \ {i}) ∪ {n −
2, n}, (T \ {i}) ∪ {n − 1, n}, and (T \ {i}) ∪ {n − 2, n − 1, n}. Further, for any coalition S ∈ Si ,
player i is pivotal for any coalition of the form S ∪S  , where S  ⊆ {n−2, n−1, n}. Therefore
2x
for any i ∈ N0 we have ηi (G) = 4x
2 + 8yi . Thus, we have βn−1 (G) = βn (G) = 6x+2kx+8y .
In the new game G&{n−1,n} , we have η&{n−1,n} (G&{n−1,n} ) = 2x, but ηn−2 (G&{n−1,n} ) =
0. Now, consider a player i ∈ N0 and a coalition T ⊂ N0 of weight 4X that contains i.
Player i is pivotal for (T \ {i}) ∪ {&{n − 1, n}} and for (T \ {i}) ∪ {n − 2, &{n − 1, n}}. as
well as for any coalition of the form S ∪ S  , where S ∈ Si and S  ⊆ {n − 2, &{n − 1, n}}.
Hence, ηi (G&{n−1,n} ) = 2x
2 + 4yi .
We obtain
2x
4x
β&{n−1,n} (G&{n−1,n} ) =
>
= βn−1 (G) + βn (G).
2x + kx + 4y
6x + 2kx + 8y
Thus, (G, {n − 1, n}) is a “yes”-instance of Beneficial-BI-Merge if and only if we started
with a “yes”-instance of Partition.
7.2 Annexation
Felsenthal and Machover (1998) prove that annexation is never disadvantageous with respect
to the Shapley–Shubik index. For completeness, we give a simple proof of this fact.
Proposition 18. For any weighted voting game G with the set of players N , any i ∈ N ,
and any S ⊆ N \ {i} we have ϕi (G) ≤ ϕ&(S∪{i}) (G&(S∪{i}) ).
Proof. We give a proof for the case |S| = 1, i.e., S = {j} for some j ∈ N \ {i}; the general
case follows easily by induction. Let Πi be the set of all permutations π of N such that i
is critical for π in G; we have ϕi (G) = |Πi |/n!. For each π ∈ Πi , let f (π) be a permutation
of the players in N&{i,j} obtained from π by deleting j and replacing i with a new player
&{i, j}. For any π ∈ Πi the player &{i, j} is pivotal for f (π). Moreover, for any permutation
σ of N&{i,j} we have |f −1 (σ)| = n. Hence, we have
ϕ&{i,j} (G&{i,j} ) ≥

|{f (π) | π ∈ Π}|
|Π|/n
=
= ϕi (G).
(n − 1)!
(n − 1)!
77

Aziz, Bachrach, Elkind, & Paterson

However Felsenthal and Machover (1998) show that, for the case of the Banzhaf index,
annexation could be disadvantageous; they refer to this phenomenon as the Bloc Paradox.
They provide a 13-player WVG for which this is the case, which is the simplest example they
could ﬁnd. We improve on their result by describing a 7-player WVG where annexation is
disadvantageous.
Example 19. Consider a weighted voting game [11; 6, 5, 1, 1, 1, 1, 1]. In this game, player 1
is pivotal for any coalition that involves player 2 and any subset of the remaining players, as
well as for the coalition {3, . . . , 7}, i.e., for 33 coalitions. Player 2 is pivotal for any coalition
that involves player 1 and at most 4 of the remaining players, i.e., 31 coalitions. Finally, each
of the players of weight 1 is only pivotal for the coalition that includes player 1 and the rest
33
≈ 0.47826.
of the players of weight 1. Thus, the Banzhaf index of player 1 equals 33+31+5
If player 1 annexes one of the players with weight 1, the new game is [11; 7, 5, 1, 1, 1, 1].
Applying the same reasoning as above, we obtain that player 1 is pivotal for 17 coalitions,
player 2 is pivotal for 15 coalition, and each of the remaining players is pivotal for exactly one
17
coalition, so the Banzhaf index of player 1 in the new game is 17+15+4
≈ 0.47222 < 0.47826.
We have shown that annexation can be disadvantageous in the case of the Banzhaf
index. One would at least expect that the Banzhaf index payoﬀ after annexing another
player is monotone in the power of the annexed player. Surprisingly, this is not the case.
That is, we will now show that there exists a weighted voting game G = [q; w1 , . . . , wn ]
and i, j, k ∈ {1, . . . , n} such that wj > wk , but β&{i,j} (G&{i,j} ) < β&{i,k} (G&{i,k} ). We will
refer to this phenomenon as the Annexation Non-monotonicity Paradox. Observe that it is
distinct from the Bloc Paradox: the former has to do with choosing which of the two given
players to annex, while the latter has to do with choosing between annexing a given player
and not annexing any player at all.
Example 20. Consider the weighted voting game [9; 3, 3, 2, 1, 1, 1]. Suppose ﬁrst that player
1 annexes player 2 to form the game [9; 6, 2, 1, 1, 1]. In this game, player 1 is pivotal for
1 coalition that does not include player 2, and 7 coalitions that include player 2, i.e., 8
coalitions. Further, player 2 is pivotal for 6 coalitions, and each of the remaining players is
8
pivotal for 2 coalitions. Thus, the Banzhaf index of player 1 is 8+6+6
= 0.4.
Now, suppose that player 1 annexes player 3 to form the game [9; 5, 3, 1, 1, 1]. In this
game, player 1 is pivotal for 7 coalitions, player 2 is pivotal for 7 coalitions, and each of
the remaining players is pivotal for 1 coalition. Thus, the Banzhaf index of player 1 in this
7
≈ 0.411765 > 0.4.
game is 7+7+3
In contrast, the Shapley–Shubik index is monotone with respect to annexation.
Proposition 21. For any weighted voting game G = [q; w1 , . . . , wn ] and any i, j, k ∈
{1, . . . , n} such that wj ≥ wk we have ϕ&{i,j} (G&{i,j} ) ≥ ϕ&{i,k} (G&{i,k} ).
Proof. Consider any permutation π of N&{i,k} for which &{i, k} is pivotal. Let π  be a
permutation of N&{i,j} obtained by replacing &{i, k} with &{i, j} and j with k. Since
w&{i,j} ≥ w&{i,k} , if in π player j appears after player &{i, k}, then player &{i, j} is
pivotal for π  . On the other hand, if in π player j appears before player &{i, k}, we
78

False-Name Manipulations in WVGs

have w(Sπ (&{i, j})) ≤ w(Sπ (&{i, k})) < q, w(Sπ (&{i, j}) ∪ {&{i, j}}) = w(Sπ (&{i, k}) ∪
{&{i, k}}) ≥ q, so &{i, j} is pivotal for π  in this case as well. Hence, each permutation for
which &{i, k} is pivotal corresponds to a distinct permutation for which &{i, j} is pivotal,
i.e., we have ϕ&{i,j} (G&{i,j} ) ≥ ϕ&{i,k} (G&{i,k} ).
To bound the gains and losses from annexation, we observe that a player can increase
his payoﬀ (with respect to both indices) by as much as 1. This happens if a dummy
player annexes a suﬃciently large player or a coalition of players. On the other hand,
Theorem 7 immediately implies that, when a player i annexes a player j in a game G, we
have β&{i,j} (G&{i,j} ) ≥ 12 (βi (G) + βj (G)). This has the following useful corollary.
Corollary 22. For any weighted voting game G with the set of players N and any i, j ∈ N
we have β&{i,j} (G&{i,j} ) ≥ 12 βi (G), i.e., no player can decrease his payoﬀ by more than a
factor of 2 by annexing another player. Moreover, if wi ≤ wj , then β&{i,j} (G&{i,j} ) ≥ βi (G),
We will now show that determining whether a player can beneﬁt from annexing a given
coalition (with respect to the Banzhaf index) is NP-hard.
Theorem 23. Beneficial-BI-Annexation is NP-hard.
Proof. The proof is similar to that of Theorem 11. Given an instance of Partition A =
{a1 , . . . , ak }, we construct
 a weighted voting game G = [q; w1 , . . . , wn ] with n = k+2 players
as follows. We let X = ai ∈A ai , and set wi = 8ai for i = 1, . . . , n − 2, wn−1 = wn = 1 and
q = 4X + 2.
By Lemma 10, if A is a “no”-instance of Partition, then players n − 1 and n are
dummies, and n remains a dummy even if it annexes n − 1. Now, suppose that A is a
“yes”-instance of Partition. Let x denote the number of coalitions in N \ {n − 1, n} of
weight 4X. We have ηn−1 = ηn (G) = x. For i = 1, . . . , n − 2, let
Si = {S ⊆ N \ {n − 1, n, i} | w(S) < 4X, w(S) + wi ≥ q},

and set yi = |Si |. Also, set y = n−2
i=1 yi .
The calculations in the proof of Theorem 11 show that
x
2x+(n−2) x2 +4y ,
x
= x+(n−2)
x
+2y .
2

βn (G) =
β&{n,n−1} (G&{n,n−1} )

Since x > 0, this implies βn (G) < β&{n,n−1} (G&{n,n−1} ). Hence, (G, {n − 1}, n) is a
“yes”-instance of Beneficial-BI-Annexation if and only if we have started with a “yes”instance of Partition.
We conclude this section by analyzing the beneﬁts of merging and annexation in unanimity games, i.e., games where q = w(N ).
Proposition 24. In any unanimity game, it is advantageous for a player to annex an
arbitrary coalition, with respect to both the Shapley–Shubik index and the Banzhaf index.
However, no group of players can increase its total payoﬀ (as measured by either of the
indices) by merging.
79

Aziz, Bachrach, Elkind, & Paterson

Proof. In any game all players have equal value of the index both before and after annexa1
. However,
tion. Hence, if i annexes a coalition of size s, his power increases from n1 to n−s
s
1
merging reduces the total power of all players in a coalition of size s from n to n−s
.
We remark that Proposition 24 generalizes to any game in which each player is a veto
player.

8. Empirical Analysis
We now analyze false-name splitting manipulations empirically. We have constructed a
system for randomly constructing weighted voting games and examined the changes in both
the Shapley–Shubik index and the Banzhaf index that occur when agents split their weights
between false identities. We brieﬂy describe our simulation system, the game construction
and the power index calculations, and then present the empirical evidence obtained.
8.1 Simulation System and Settings
The weighted voting games were constructed by ﬁrst randomly choosing the number of
players in the game. Then, the weights of the players were each drawn from N (μ, σ 2 ),
the normal distribution with mean μ and variance σ 2 . The weights were then rounded to
the nearest integer, to make sure that the game has integer weights. The threshold for
the game was chosen uniformly at random between 0 and the sum of the players’ weights
w = w(N ), and again rounded to the nearest integer. In our experiments, we have used a
mean of μ = 200 for the weights, and several values for the standard deviation σ from the
set {5, 10, 15, . . . , 50}. The number of players n was chosen uniformly at random from the
set {5, 6, 7, . . . , 24}.
Power indices are computationally hard to compute exactly (Papadimitriou & Yannakakis, 1994; Matsui & Matsui, 2001), but can be tractably approximated using several
methods. We have used the approximation method of Bachrach et al. (2010). This algorithm estimates the power indices and returns a result which is probably approximately
correct, as discussed in Section 6. Given a game in which a player’s true power index is
ψ, and given a target accuracy level  and conﬁdence level δ, the algorithm returns an
approximation ψ̂ such that with probability at least 1 − δ we have |ψ − ψ̂| ≤  (i.e. the
result is approximately correct, and is within a distance  of the correct value). This algorithm works by drawing a sample of k permutations (or coalitions), and testing whether
the target player is critical for them. Such a test runs in time linear in the number of
agents, so the total running time is O(kn log W ). Bachrach et al. show that to achieve
a conﬁdence level δ and accuracy level , it suﬃces to take k = ln(2/δ)/(22 ). Thus the
total running time is logarithmic in the conﬁdence and quadratic in the accuracy, so the
approach is tractable even for high accuracy and conﬁdence. We have used δ = 0.00001
and  = 0.001, so the power was estimated very accurately. Our system was implemented
in C#, and the results of the experiments were stored in an SQL database. Since power
indices were approximated accurately, a single experiment can take several seconds. Our
tests required tens of thousands of experiments, so we have used a compute cluster with
250 cores for our experiments.
80

False-Name Manipulations in WVGs

Our theoretical results show that testing for a beneﬁcial split is hard, which might create
the impression that ﬁnding a beneﬁcial manipulation is hard in practice. Our empirical
experiments were designed to see whether this is indeed the case. A very naive method
that a manipulator can use is to try many possible splits into two identities, in constant
intervals. In other words a manipulator whose weight is w can try 2s splits by splitting his
2w
3w
3w
weight as ( ws , w − ws ), ( 2w
s , w − s ), ( s , w − s ) and so on. Although this is certainly
not a complete coverage of the space of possible manipulations, in our experiments we have
tried a very simple algorithm that is based on this idea. Since all the weights were integers,
we have only tried splitting weights between two false identities, and examined all integer
splits. For example, if an agent had a weight of wi = 10, we attempted splitting into weights
of w1 = 9, w1 = 1, w1 = 8, w1 = 2, w1 = 7, w1 = 3 and so on. In each experiment we
have recorded the details of the game, the number of beneﬁcial splits (power increase) and
harmful splits (power decrease). For a split to be considered beneﬁcial, it had to increase the
power by more than twice the accuracy level. Thus, the results presented here understate
the number of positive splits. In our results we examine the proportion of experiments
where we have found at least one beneﬁcial manipulation, as well as the proportion of the
splits that were beneﬁcial (out of all the integer splits).
8.2 Empirical Results
We ﬁrst present the results regarding the Shapley–Shubik index. First and foremost, our
results indicate that the weighted voting domain is very manipulable, at least for our method
of generating random weighted voting games. Under all values we have tried for the variance
in the player weights and number of players in the game, in over 95.5% of the experiments
even the very naive manipulation algorithm managed to uncover at least one beneﬁcial
manipulation. This indicates that in most games it is enough to try all integer splits (or
splits in uniform intervals) and use the tractable method for approximating power indices to
uncover beneﬁcial manipulations. Figures 2 and 3 indicate the proportion of the experiments
in which this algorithm succeeds in ﬁnding a beneﬁcial split, as a function of the variance
in players’ weights and the number of players, respectively. It appears that the success rate
of our algorithm slightly increases as the variance increases. No such obvious trend appears
for the number of players.
One might be tempted to think that beneﬁcial splits are quite common, as most experiments had at least one beneﬁcial split. However, it turns out that most splits are harmful
splits. In all tested settings, less than 40% of all splits were beneﬁcial splits. In most
settings, harmful splits accounted for over 70% of all splits. In Figure 4 and Figure 5, we
indicate the proportion of beneﬁcial splits, as a function of the variance in players’ weights
and the number of players, respectively.
We have also examined the distribution of the proportion of beneﬁcial splits across
experiments. In some generated games, the beneﬁcial splits were quite rare, and less than a
single percent of the splits were beneﬁcial. In other generated games, beneﬁcial splits were
the common case, and over 99% of the splits were beneﬁcial. Figure 6 shows the distribution
(histogram) of the proportion of beneﬁcial splits, across games. To create the ﬁgure, the
games were partitioned into 200 bins, according to the proportion of beneﬁcial splits in the
game. Each bin was of size 0.5% (e.g., the proportion of beneﬁcial manipulations in the
81

Aziz, Bachrach, Elkind, & Paterson

Figure 2: Proportion of experiments where the naive algorithm ﬁnds a beneﬁcial split for
diﬀerent variances of players’ weights (Shapley–Shubik index)

Figure 3: Proportion of experiments where the naive algorithm ﬁnds a beneﬁcial split for
diﬀerent numbers of players (Shapley–Shubik index)

82

False-Name Manipulations in WVGs

Figure 4: Proportion of beneﬁcial splits for diﬀerent variances of players’ weights (Shapley–
Shubik index)

Figure 5: Proportion of beneﬁcial splits for diﬀerent numbers of players (Shapley–Shubik
index)

83

Aziz, Bachrach, Elkind, & Paterson

Figure 6: Distribution (histogram) of the proportion of positive splits across games
(Shapley–Shubik index)

100-th bin was between 0.5 and 0.55). The value on the X axis of Figure 6 is the proportion
of beneﬁcial splits (the bin), and the Y axis is the number of experiments falling in that
category. Figure 6 shows that most games are ones where beneﬁcial splits are more rare
than harmful splits, but the distribution has a long tail, so even games where almost all the
splits are beneﬁcial are not uncommon.
We now turn to examine the Banzhaf index. As in the case of the Shapley–Shubik index,
for the Banzhaf index the weighted voting domain is very susceptible to manipulation.
Under all tested settings, in over 92.5% of the experiments our manipulation algorithm
managed to uncover at least one beneﬁcial manipulation (slightly less than the 95.5% for
the Shapley–Shubik index). The proportion of experiments where our algorithm ﬁnds a
beneﬁcial split with respect to the Banzhaf index is shown in Figure 7 (for diﬀerent values
of variance) and Figure 8 (for diﬀerent number of players).
Similarly to the case of the Shapley–Shubik index, for the Banzhaf index beneﬁcial splits
were the less common case, and most splits are harmful splits, with less than 45% of all
splits being beneﬁcial, and typically about 40% being beneﬁcial splits (slightly higher than
for the Shapley–Shubik index). Unlike for the Shapley–Shubik index, for the Banzhaf index,
the proportion of beneﬁcial splits among all splits increases with the variance (Figure 9).
However, this proportion does not have a clear trend with regard to the number of players
(Figure 10).
The distribution of the proportion of beneﬁcial splits across games for the Banzhaf index
seems quite similar to that for the Shapley–Shubik index (see Figure 11). Again, for most
games, the majority of splits are harmful, but the distribution has a long tail, and many
84

False-Name Manipulations in WVGs

Figure 7: Proportion of experiments where the naive algorithm ﬁnds a beneﬁcial split for
diﬀerent variances of players’ weights (Banzhaf index)

Figure 8: Proportion of experiments where the naive algorithm ﬁnds a beneﬁcial split for
diﬀerent numbers of players (Banzhaf index)

85

Aziz, Bachrach, Elkind, & Paterson

Figure 9: Proportion of beneﬁcial splits for diﬀerent variances of players’ weights (Banzhaf
index)

Figure 10: Proportion of beneﬁcial splits for diﬀerent numbers of players (Banzhaf index)

86

False-Name Manipulations in WVGs

Figure 11: Distribution (histogram) of the proportion of positive splits across games
(Banzhaf index)

games have mostly beneﬁcial splits. Although the distribution seems similar to that for the
Shapley–Shubik index, the tail of the distribution seems slightly “fatter” for the Banzhaf
index.
To conclude, experiments for both indices present a similar picture. For most games
generated in our model, we have mostly harmful splits. However, many experiments have
many positive splits, and in some of them even almost all the splits are beneﬁcial. Games
where trying all the integer splits does not yield a successful manipulation are very rare,
although they do exist. Thus, for most games generated in our model, even the extremely
simple manipulation algorithm ﬁnds beneﬁcial splits. We conclude that despite the hardness
results in this paper, in practice we believe it is quite easy to ﬁnd such splits, and thus we
believe such attacks pose a real problem in many settings.

9. Splitting into More Than Two Identities
So far, we have mostly discussed the gain (or loss) that a player can achieve by splitting
into two identities. However, it is also possible for a player to use three or more false names.
Potentially, the number of identities a player can use can be as large as his weight (and if the
weights are not required to be integers, it can even be inﬁnite). It would be interesting to see
which of our results hold in this more general setting. For example, while our computational
hardness result holds for splits into any number of identities, the algorithmic results of the
previous section only apply to splits into a constant number of new identities. An obvious
open problem here is to design a pseudopolynomial algorithm for ﬁnding a beneﬁcial integer
87

Aziz, Bachrach, Elkind, & Paterson

split into any number of identities, or to prove that this problem is NP-hard even for small
weights (i.e., weights that are polynomial in n). Another question of interest here is to
extend the upper and lower bounds of Section 4 for this setting.
One might think that ﬁnding a beneﬁcial split into k ≥ 2 identities is easier than ﬁnding
one that uses exactly two identities: after all, any two-way split can be transformed into a
split into three or more players in which only two players have non-zero weight. However,
it turns out that if we restrict our attention to non-trivial splits, i.e., one in which all of the
new players have a non-zero weight, this is no longer the case.
Example 25. Consider a game G = [6; 5, 5]. In this game, the only winning coalition
includes both players, so their Shapley–Shubik indices are given by ϕ1 (G) = ϕ2 (G) = 1/2.
Suppose that player 2 splits into two identities 2 and 2 . For any selection of integer
weights w2 > 0, w2 > 0 that satisfy w2 + w2 = 5, in the new game G = [6; 5, w2 , w2 ] we
have ϕ2 (G ) = ϕ2 (G ) = 1/3. Indeed, in this game each player is pivotal in a permutation
π if and only if it occurs in the second position, which happens with probability 1/3. Hence,
any non-trivial split into two identities increases the payoﬀ of the second player by a factor
of (2/3)/(1/2) = 4/3.
Now, suppose that the second player splits into 5 new players of weight 1 each. In the
new game, player 1 is pivotal for a permutation π if and only if he does not occur in the
ﬁrst position in that permutation, so his Shapley–Shubik index is 5/6. Consequently, the
sum of Shapley–Shubik indices of all remaining players (i.e., the new identities of player 2)
is 1/6. Therefore, this split decreases the payoﬀ of player 2 by a factor of 3. To summarize,
while any non-trivial integer split into 2 identities is beneﬁcial for player 2, any integer split
into 5 identities with positive weight is harmful for him.
Remark 26. Example 25 can be generalized to games of the form GN = [N + 1; N, N ] for
an arbitrary integer N > 0. The reasoning above shows that if one of the players decides
to split into N new players of weight 1 each, this increases the Shapley–Shubik index of the
other player to N/(N + 1) and hence decreases the total payoﬀ of the splitting player by
a factor of (N + 1)/2. As the representation size of this game is polynomial in log N , this
decrease is exponential in description size.

10. Conclusions
We have considered false-name manipulations in weighted voting games with respect to the
payoﬀ schemes based on the Shapley–Shubik index and the Banzhaf index. We have also
considered manipulation via annexation and voluntary merging with respect to such payoﬀ
schemes. We have examined both the limits of manipulation (Table 1) and the complexity
of manipulation (Table 2), and complemented the theoretical investigation by empirical
analysis.
We have shown that, in most scenarios considered in this paper, testing whether a
beneﬁcial manipulation exists is NP-hard. One may ask whether these hardness results
provide an adequate barrier to manipulation, given that the power indices themselves are
hard to compute. In other words, don’t we simultaneously assume that the weights are small
(and hence computing the indices is easy) and large (and hence manipulation is hard)? To
resolve this apparent contradiction, note that the power indices considered in this paper
88

False-Name Manipulations in WVGs

2
n+1 ϕi (G)

Bounds
≤ ϕi (G ) + ϕi (G ) ≤

2n
n+1 ϕi (G)

ϕi (G) ≤ ϕi (G&({i}∪S) ) ≤ 1.
1
n+1 βi (G)

≤ βi (G ) + βi (G ) ≤ 2βi (G)

βi (G)
2

≤ βi (G&({i,j}) ) ≤ 1.

Reference
Theorems 5 and 6
Proposition 18
Theorems 7 and 8
Corollary 22

Table 1: Bounds on eﬀects of false-name manipulations in WVGs

Splitting
Merging
Annexation
Splitting in unanimity game
Merging in unanimity game
Annexation in unanimity game
∗ (Felsenthal

Banzhaf index
NP-hard
NP-hard
NP-hard
advantageous
disadvantageous
advantageous

Shapley–Shubik index
NP-hard
NP-hard
advantageous∗
advantageous
disadvantageous
advantageous

& Machover, 1998)

Table 2: Complexity of false-name manipulations in WVGs
correspond to the voting power, and the players may try to increase their voting power by
weight-splitting manipulation even if they cannot compute it. Also, when a power index is
used to compute payments, the center, which performs this computation, may have more
computational power than individual players.
Our experimental results show that, for moderately large weights, weight-splitting manipulation is easy in practice. However, our algorithm relies on considering all integer splits,
i.e., its running time is at least linear in the manipulator’s weight. An interesting open question is whether it is the case that if a beneﬁcial split exists, it can be found by testing a
number of splits that is logarithmic in the manipulator’s weight.
Our results indicate that the Shapley–Shubik index and the Banzhaf index behave similarly with respect to false-name manipulation; however, the Shapley–Shubik index appears
to be a more desirable solution concept because annexation does not decrease the payoﬀ of
a player. Exploring other solution concepts and their behavior with respect to false-name
manipulation is a natural next step; a particularly suitable solution to consider could be
the nucleolus, which not only always exists but is also unique.
The study of weighted voting has many applications, both in political science and in
multiagent systems. There are several possible interpretations for identity-splitting in these
contexts, such as obtaining a higher share of the grand coalition’s gains when these are
distributed according to the Shapley–Shubik index or the Banzhaf index, or obtaining more
political power by splitting a political party into several parties with similar political platforms. In the ﬁrst case, a false-name manipulation is hard to detect in open anonymous
89

Aziz, Bachrach, Elkind, & Paterson

environments, and can thus be very eﬀective. In the second case, the manipulation is done
using legitimate tools of political conduct. Therefore, we conjecture that false-name manipulation is widespread in the real world and may become a serious issue in multiagent
systems. It is therefore important to develop a better understanding of the eﬀects of this
behavior and/or design methods of preventing it.

Acknowledgments
Haris Aziz and Mike Paterson were partially supported by DIMAP (the Centre for Discrete
Mathematics and its Applications). DIMAP is funded by the UK EPSRC under grant
EP/D063191/1. Partial support for Aziz’s research was also provided by the Deutsche
Forschungsgemeinschaft under grants BR-2312/6-1 (within the European Science Foundation’s EUROCORES program LogICCC) and BR 2312/3-2. Edith Elkind was partially
supported by ESRC under grant ES/F035845/1 and by Singapore NRF Research Fellowship 2009-08.

References
Algaba, E., Bilbao, J. M., & Fernández, J. R. (2007). The distribution of power in the
European Constitution. European Journal of Operational Research, 176 (3), 1752–
1755.
Aziz, H., & Paterson, M. (2008). Computing voting power in easy weighted voting games.
CoRR, abs/0811.2497.
Aziz, H., & Paterson, M. (2009). False name manipulations in weighted voting games:
splitting, merging and annexation. In Proceedings of the 8th International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 409–416.
Aziz, H., Paterson, M., & Leech, D. (2007). Eﬃcient algorithm for designing weighted
voting games. In Proceedings of the 11th IEEE International Multitopic Conference,
pp. 1–6. IEEE Computer Society.
Bachrach, Y., & Elkind, E. (2008). Divide and conquer: False-name manipulations in
weighted voting games. In Proceedings of the 7th International Joint Conference on
Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 975–982.
Bachrach, Y., Markakis, E., Resnick, E., Procaccia, A. D., Rosenschein, J. S., & Saberi, A.
(2010). Approximating power indices: theoretical and empirical analysis. Autonomous
Agents and Multi-Agent Systems, 20 (2), 105–122.
Banzhaf, J. F. (1965). Weighted voting doesn’t work. Rutgers Law Review, 19, 317–343.
Bartholdi, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social
Choice and Welfare, 8 (4), 341–354.
Bartholdi, J., Tovey, C., & Trick, M. (1989). The computational diﬃculty of manipulating
an election. Social Choice and Welfare, 6 (3), 227–241.
Bartholdi, J., Tovey, C., & Trick, M. (1992). How hard is it to control an election?. Mathematical and Computer Modeling, 16 (8/9), 27–40.
90

False-Name Manipulations in WVGs

Brams, S. (1975). Game Theory and Politics. Free Press, New York.
de Keijzer, B., Klos, T., & Zhang, Y. (2010). Enumeration and exact design of weighted
voting games. In Proceedings of the 9th International Joint Conference on Autonomous
Agents and Multi-Agent Systems (AAMAS), pp. 391–398.
Deegan, J., & Packel, E. W. (1978). A new index of power for simple n-person games.
International Journal of Game Theory, 7, 113–123.
Dubey, P., & Shapley, L. S. (1979). Mathematical properties of the Banzhaf power index.
Mathematics of Operations Research, 4 (2), 99–131.
Elkind, E., Chalkiadakis, G., & Jennings, N. R. (2008a). Coalition structures in weighted
voting games. In Proceedings of the 18th European Conference on Artiﬁcial Intelligence
(ECAI), pp. 393–397.
Elkind, E., Goldberg, L. A., Goldberg, P., & Wooldridge, M. (2008b). On the dimensionality
of voting games. In Proceedings of the 23rd AAAI Conference on Artiﬁcial Intelligence
(AAAI), pp. 69–74. AAAI Press.
Elkind, E., Goldberg, L. A., Goldberg, P. W., & Wooldridge, M. J. (2007). Computational
complexity of weighted threshold games. In Proceedings of the 22nd AAAI Conference
on Artiﬁcial Intelligence (AAAI), pp. 718–723. AAAI Press.
Elkind, E., & Pasechnik, D. (2009). Computing the nucleolus of weighted voting games.
In Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), pp. 327–335.
Ephrati, E., & Rosenschein, J. (1997). A heuristic technique for multi-agent planning.
Annals of Mathematics and Artiﬁcial Intelligence, 20 (1–4), 13–67.
Faliszewski, P., Elkind, E., & Wooldridge, M. (2009). Boolean combinations of weighted
voting games. In Proceedings of the 8th International Joint Conference on Autonomous
Agents and Multi-Agent Systems (AAMAS), pp. 185–192.
Faliszewski, P., & Hemaspaandra, L. A. (2009). The complexity of power-index comparison.
Theoretical Computer Science, 410 (1), 222–245.
Faliszewski, P., & Procaccia, A. (2010). AI’s war on manipulation: Are we winning?. AI
Magazine, 31 (4).
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2007). A randomized method for the
Shapley value for the voting game. In Proceedings of the 6th International Joint
Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 1–8.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2008). An anytime approximation method
for the inverse Shapley value problem. In Proceedings of the 7th International Joint
Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 935–942.
Felsenthal, D., & Machover, M. (1998). The Measurement of Voting Power. Edward Elgar
Publishing, Cheltenham, UK.
Holler, M. J., & Packel, E. W. (1983). Power, luck and the right index. Journal of Economics,
43, 21–29.
91

Aziz, Bachrach, Elkind, & Paterson

Iwasaki, A., Kempe, D., Saito, Y., Salek, M., & Yokoo, M. (2007). False-name-proof mechanisms for hiring a team. In Proceedings of the 3rd International Workshop on Internet
and Network Economics (WINE), pp. 245–256.
Johnston, R. J. (1978). On the measurement of power: Some reactions to Laver. Environment and Planning A, 10, 907–914.
Kilgour, D. M., & Levesque, T. J. (1984). The Canadian constitutional amending formula:
Bargaining in the past and the future. Public Choice, 44 (3), 457–480.
Laruelle, A. (1999). On the choice of a power index. Working papers, serie AD 1999-10,
Instituto Valenciano de Investigaciones Económicas.
Laruelle, A., & Valenciano, F. (2005). A critical reappraisal of some voting power paradoxes.
Public Choice, 125, 17–41.
Laruelle, A., & Widgren, M. (1998). Is the allocation of voting power among EU states
fair?. Public Choice, 94 (3-4), 317–339.
Lasisi, R., & Allan, V. (2010). False name manipulations in weighted voting games: Susceptibility of power indices. In The Thirteenth International Workshop on Trust in
Agent Societies (TRUST), pp. 139–150.
Leech, D. (2002). Voting power in the governance of the International Monetary Fund.
Annals of Operations Research, 109 (1), 375–397.
Leech, D., & Leech, R. (2005). Power vs weight in IMF governance: the possible beneﬁcial implications of a united European bloc vote. In Buira, A. (Ed.), Reforming the
Governance of the IMF and the World Bank, pp. 251–282. Anthem Press.
Machover, M., & Felsenthal, D. S. (2001). The Treaty of Nice and qualiﬁed majority voting.
Social Choice and Welfare, 18 (3), 431–464.
Mann, I., & Shapley, L. S. (1960). Values of large games, IV: Evaluating the electoral college
by Montecarlo techniques. The Rand Corporation, RM-2651.
Mann, I., & Shapley, L. S. (1962). Values of large games, VI: Evaluating the electoral college
exactly. The Rand Corporation, RM-3158.
Matsui, T., & Matsui, Y. (2000). A survey of algorithms for calculating power indices of
weighted majority games. Journal of the Operations Research Society of Japan, 43 (1),
71–86.
Matsui, Y., & Matsui, T. (2001). NP-completeness for calculating power indices of weighted
majority games. Theoretical Computer Science, 263 (1-2), 305–310.
Ohta, N., Conitzer, V., Satoh, Y., Iwasaki, A., & Yokoo, M. (2008). Anonymity-proof
Shapley value: extending Shapley value for coalitional games in open environments.
In Proceedings of the 7th International Joint Conference on Autonomous Agents and
Multi-Agent Systems (AAMAS), pp. 927–934.
Ohta, N., Iwasaki, A., Yokoo, M., Maruono, K., Conitzer, V., & Sandholm, T. (2006).
A compact representation scheme for coalitional games in open anonymous environments. In Proceedings of the 21st AAAI Conference on Artiﬁcial Intelligence (AAAI),
pp. 697–702.
92

False-Name Manipulations in WVGs

Owen, G. (1975). Multilinear extensions and the Banzhaf value. Naval Research Logistics
Quarterly, 22 (4), 741–750.
Papadimitriou, C. H., & Yannakakis, M. (1994). On complexity as bounded rationality.
In Proceedings of the 26th Annual ACM Symposium on the Theory of Computing
(STOC), pp. 726–733. ACM.
Rey, A., & Rothe, J. (2010). Complexity of merging and splitting for the probabilistic
banzhaf power index in weighted voting games. In 19th European Conference on
Artiﬁcial Intelligence (ECAI 2010), pp. 1021–1022.
Shapley, L. S. (1953). A value for n-person games. In Kuhn, H. W., & Tucker, A. W. (Eds.),
Contributions to the Theory of Games, II, pp. 307–317. Princeton University Press.
Shapley, L. S. (1973). Political science: Voting and bargaining games. In Selby, H. A. (Ed.),
Notes of Lectures on Mathematics in the Behavioral Science, pp. 37–92. Mathematical
Association of America.
Shapley, L. S., & Shubik, M. (1954). A method for evaluating the distribution of power in
a committee system. The American Political Science Review, 48 (3), 787–792.
Straﬃn, P. D. (1977). Homogeneity, independence and power indices. Public Choice, 30,
107–118.
Taylor, A., & Zwicker, W. (1999). Simple Games: Desirability Relations, Trading, Pseudoweightings. Princeton University Press, New Jersey.
van Deemen, A., & Rusinowska, A. (2003). Paradoxes of voting power in dutch politics.
Public Choice, 115 (1–3), 109–137.
von Neumann, J., & Morgenstern, O. (1944). Theory of Games and Economic Behavior.
Princeton University Press.
Yokoo, M. (2007). False-name bids in combinatorial auctions. SIGecom Exchanges, 7 (1),
48–51.
Yokoo, M., Conitzer, V., Sandholm, T., Ohta, N., & Iwasaki, A. (2005). Coalitional games
in open anonymous environments. In Proceedings of the 20th AAAI Conference on
Artiﬁcial Intelligence (AAAI), pp. 509–515.
Zuckerman, M., Faliszewski, P., Bachrach, Y., & Elkind, E. (2008). Manipulating the quota
in weighted voting games. In Proceedings of the 23rd AAAI Conference on Artiﬁcial
Intelligence (AAAI), pp. 215–220.

93

Journal of Artificial Intelligence Research 40 (2011) 175-219

Submitted 09/10; published 01/11

Second-Order Consistencies
Christophe Lecoutre
Stéphane Cardon

lecoutre@cril.fr
cardon@cril.fr

CRIL-CNRS UMR 8188
Université Lille-Nord de France, Artois
rue de l’université
SP 16, F-62307 Lens, France

Julien Vion

julien.vion@univ-valenciennes.fr

LAMIH-CNRS FRE 3304
Université Lille-Nord de France, UVHC
F-59313 Valenciennes Cedex 9, France

Abstract
In this paper, we propose a comprehensive study of second-order consistencies (i.e.,
consistencies identifying inconsistent pairs of values) for constraint satisfaction. We build
a full picture of the relationships existing between four basic second-order consistencies,
namely path consistency (PC), 3-consistency (3C), dual consistency (DC) and 2-singleton
arc consistency (2SAC), as well as their conservative and strong variants. Interestingly, dual
consistency is an original property that can be established by using the outcome of the enforcement of generalized arc consistency (GAC), which makes it rather easy to obtain since
constraint solvers typically maintain GAC during search. On binary constraint networks,
DC is equivalent to PC, but its restriction to existing constraints, called conservative dual
consistency (CDC), is strictly stronger than traditional conservative consistencies derived
from path consistency, namely partial path consistency (PPC) and conservative path consistency (CPC). After introducing a general algorithm to enforce strong (C)DC, we present
the results of an experimentation over a wide range of benchmarks that demonstrate the
interest of (conservative) dual consistency. In particular, we show that enforcing (C)DC
before search clearly improves the performance of MAC (the algorithm that maintains GAC
during search) on several binary and non-binary structured problems.

1. Introduction
Many decision problems are combinatorial by nature, and can be modelled using finite
domain variables connected with constraints. Such models are formally represented as constraint networks (CNs) and finding a solution to a model is an instance of the NP-complete
constraint satisfaction problem (CSP). The CSP is usually solved through systematic backtrack search, a fundamental technique in artificial intelligence. There have been considerable
efforts during the last three decades to improve the practical efficiency of backtrack search.
Consistencies are properties of constraint networks that can be exploited (enforced), before or during search, to filter the search space of problem instances by inference. Currently,
the most successful consistencies are domain filtering consistencies (Debruyne & Bessiere,
2001; Bessiere, Stergiou, & Walsh, 2008). Common consistencies for binary CNs are arc
consistency (AC, Mackworth, 1977) and singleton arc consistency (SAC, Bessiere & Dec
2011
AI Access Foundation. All rights reserved.

Lecoutre, Cardon, & Vion

bruyne, 2005). Example consistencies for non-binary CNs are generalized arc consistency
(GAC, Mohr & Masini, 1988) and pairwise inverse consistency (PWIC, Stergiou & Walsh,
2006). Consistencies typically allow the identification of nogoods. A nogood is an instantiation of some variables that cannot lead to any solution. Identifying relevant nogoods as
soon as possible when exploring the search space of an instance is recognized as an essential
component of a backtracking search algorithm. A domain-filtering consistency is also called
a first-order consistency, meaning that it only detects inconsistent values (1-sized nogoods):
these values can be safely removed from the domains of the variables.
This paper is concerned with second-order consistencies that locally identify inconsistent
pairs of values. The most studied second-order consistency is path consistency (PC, Montanari, 1974; Mackworth, 1977). For now, path consistency, and more generally higher order
consistencies, are rather neglected by designers and developers of general constraint solvers.
This is somewhat surprising since, for many tractable classes, strong path consistency (path
consistency combined with arc consistency) is a sufficient condition to determine satisfiability (e.g., see Dechter, 1992; van Beek, 1992; Cooper, Cohen, & Jeavons, 1994; Zhang
& Yap, 2006). Neglecting higher order consistencies may partly be due to the somewhat
limited scope of these classes: exciting progress in this area has only been very recent (e.g.,
see Green & Cohen, 2008). However, path consistency has an important role in temporal reasoning. Indeed, for some classes of interval algebra, path consistency – adapted to
temporal constraint networks (Allen, 1983) – is sufficient to decide satisfiability. Another
possible reason for the low practical interest for path consistency, in the discrete constraint
satisfaction field, is that path consistency enforcement modifies constraint relations, and
more importantly, modifies the structure of the constraint graph. When a pair of values
(a, b) for the variables (x, y) is found to be path-inconsistent, this information is recorded
within the constraint network; if there is no constraint binding x with y, a new one is inserted, thus changing the constraint graph. For example, the instance scen-11 of the radio
link frequency assignment problem (Cabon, de Givry, Lobjois, Schiex, & Warners, 1999)
involves 680 variables and 4,103 constraints.
Enforcing a second-order consistency on this

network could at worst create 680
−
4,103
=
226,757 new constraints, which would be
2
really counter-productive both in time and in space. The main apparent drawback of path
consistency can be avoided by adopting a conservative approach, in which the search for inconsistent pairs of values is restricted to existing constraints. This is called conservative path
consistency (CPC, Debruyne, 1999) when restricted to paths of length 2 in the constraint
graph, and partial path consistency (PPC, Bliek & Sam-Haroud, 1999) when restricted to
paths of arbitrary length in the constraint graph; CPC and PPC are equivalent when the
constraint graph is triangulated.
In this paper, we study path consistency as well as three other basic second-order consistencies that are 3-consistency (3C, Freuder, 1978), dual consistency (DC, Lecoutre, Cardon,
& Vion, 2007a) and 2-singleton arc consistency (2SAC, Bessiere, Coletta, & Petit, 2005). On
binary constraint networks, DC is equivalent to PC – McGregor (1979) proposed an DC-like
algorithm to establish (strong) path consistency – but when considering weaker conservative
variants, we show that conservative dual consistency (CDC) is strictly stronger than PPC
and CPC: CDC can filter out more inconsistent pairs of values (from existing constraints)
than PPC or CPC. We build a full picture of the qualitative relationships existing between
all those second-order consistencies (including the stronger 2SAC property, the conserva176

Second-Order Consistencies

tive restrictions and strong variants of all studied consistencies) for both binary CNs and
non-binary CNs.
Interestingly enough, (conservative) dual consistency benefits from some nice features:
(1) as (C)DC is built on top of GAC, implementing a filtering algorithm to enforce it is
rather easy, (2) for the same reason, all optimizations achieved on GAC algorithms these
last years come for free, (3) we have the guarantee that GAC enforced on a CN that verifies
the property (C)DC leaves the property unchanged. This is why our theoretical study
is followed by the presentation of a general algorithm to enforce strong (C)DC and an
experimental study to show the practical interest of using (C)DC (during a preprocessing
step) when solving binary and non-binary problem instances with a search algorithm such
as MAC (Sabin & Freuder, 1994).
The paper is organized as follows. Section 2 introduces technical background about
constraint networks, nogoods and consistencies. In Section 3, we introduce (basic, conservative and strong) second-order consistencies, with a focus on path consistency and a
possible misunderstanding about it. A qualitative study about second-order consistencies
is conducted in Section 4. An algorithm to enforce (C)DC is proposed in Section 5, and
experimental results are presented in Section 6. Finally, we conclude.

2. Technical Background
This section provides technical background about constraint networks and consistencies.
2.1 Constraint Networks
A (finite) constraint network (CN) P is composed of a finite set of n variables, denoted
by vars(P ), and a finite set of e constraints, denoted by cons(P ). Each variable x has an
associated domain, denoted by dom(x), that contains the finite set of values that can be
assigned to x. Each constraint c involves an ordered set of variables, called the scope of c
and denoted by scp(c). It is defined by a relation, denoted by rel(c), which contains the set
of tuples allowed for the variables involved in c. The arity of a constraint c is the size of
scp(c). The maximum domain size and the maximum arity for a given CN will be denoted
by d and r, respectively. A binary constraint involves exactly 2 variables, and a non-binary
constraint strictly more than 2 variables. A binary CN only contains binary constraints
whereas a non-binary CN contains at least one non-binary constraint.
The initial domain of a variable x is denoted by dominit (x) whereas the current domain
of x in the CN P is denoted by domP (x) or more simply dom(x) when the context is
unambiguous. The initial relation of a constraint c is denoted by relinit (c) whereas the
current relation is denoted by relP (c) or more simply rel(c). A constraint c is universal iff
relinit (c) = Πx∈scp(c) dominit (x); a universal constraint imposes no restriction. We consider
that for any variable x, we always have dom(x) ⊆ dominit (x), and for any constraint c, we
always have rel(c) ⊆ relinit (c). To simplify, a pair (x, a) with x ∈ vars(P ) and a ∈ dom(x)
is called a (current) value of P . Without any loss of generality, we only consider CNs
that involve neither unary constraints (i.e., constraints involving a unique variable) nor
177

Lecoutre, Cardon, & Vion

constraints of similar scope – CNs are normalized (Apt, 2003; Bessiere, 2006). The set of
normalized CNs with neither unary constraints nor universal constraints1 is denoted by P.
An instantiation I of a set X = {x1 , . . . , xk } of variables is a set {(x1 , a1 ), . . . , (xk , ak )}
such that ∀i ∈ 1..k, ai ∈ dominit (xi ) ; the set X of variables occurring in I is denoted
by vars(I) and each value ai is denoted by I[xi ]. An instantiation I on a CN P is an
instantiation of a set X ⊆ vars(P ); it is complete iff vars(I) = vars(P ), partial otherwise.
I is valid on P iff ∀(x, a) ∈ I, a ∈ domP (x). An instantiation I covers a constraint c iff
scp(c) ⊆ vars(I), and satisfies a constraint c with scp(c) = {x1 , . . . , xr } iff (1) I covers c and
(2) the tuple (a1 , . . . , ar ) is allowed by c, i.e., (a1 , . . . , ar ) ∈ rel(c), where ∀i ∈ 1..r, ai = I[xi ].
A support (resp., a conflict) on a constraint c is a valid instantiation of scp(c) that satisfies
(resp., does not satisfy) c. An instantiation I on a CN P is locally consistent iff (1) I is valid
on P and (2) every constraint of P covered by I is satisfied by I. It is locally inconsistent
otherwise. A solution of P is a complete instantiation on P that is locally consistent. An
instantiation I on a CN P is globally inconsistent, or a nogood, iff it cannot be extended to
a solution of P . It is globally consistent otherwise. We refer here to standard nogoods (e.g.,
see Dechter, 2003); they differ from nogoods coming with justifications (Schiex & Verfaillie,
1994) and from generalized ones (Katsirelos & Bacchus, 2003). Two CNs P and P 0 defined
on the same variables are equivalent iff they have the same solutions.
A CN is said to be satisfiable iff it admits at least one solution. The Constraint Satisfaction Problem (CSP) is the NP-complete task of determining whether a given CN is
satisfiable or not. Thus, a CSP instance is defined by a CN which is solved either by finding
a solution or by proving unsatisfiability. In many cases, a CSP instance can be solved by
using a combination of search and inferential simplification (Dechter, 2003; Lecoutre, 2009).
To solve a CSP instance, a depth-first search algorithm with backtracking can be applied,
where at each step of the search, a variable assignment is performed followed by a filtering process called constraint propagation. Constraint propagation algorithms enforce some
consistency property, they identify and record explicit nogoods in CNs. When identified
nogoods are of size 1, they correspond to inconsistent values.
It is usual to refer to some properties of the (hyper)graphs that can be associated
with any CN. On the one hand, the constraint (hyper)graph, also called macro-structure,
associated with a (normalized) CN P consists of n vertices corresponding to the variables of
P and also e (hyper)edges corresponding to the constraints of P : an (hyper)edge connects
vertices corresponding to the variables in the scope of the constraint it represents. On
the other hand, the compatibility (hyper)graph, also called micro-structure (Jégou, 1993),
associated with a normalized CN P contains one vertex per value of P and one (hyper)edge
per constraint support. It corresponds to a n-partite hypergraph with one part for each
variable. Sometimes, incompatibility (hyper)graphs are used by authors where (hyper)edges
correspond to conflicts instead of supports. In this paper, (hyper)edges for supports (resp.,
conflicts) will be drawn using solid (resp., dashed) lines.
It is sometimes helpful to use a homogeneous representation of a CN, wherein domains
and also constraints are replaced by nogoods. The nogood representation of a CN is a set
of nogoods, one for every value removed from the initial domain of a variable and one
1. For our theoretical study, universal constraints can be safely ignored. In practice, any universal constraint
c in a CN P may artificially be considered as non-universal: choose a variable x ∈ scp(c) and consider a
dummy value ν such that ν ∈ dominit (x) \ domP (x) and relinit (c) forbids one tuple involving (x, ν).

178

Second-Order Consistencies

e of
for every tuple forbidden by a constraint.
More precisely, the nogood representation
x

	
a variable x is the set of instantiations {(x, a)} | a ∈ dominit (x) \ dom(x) . The nogood
representation
ce of a constraint c, withscp(c) = {x1 , . . . , xr}, is theoset of instantiations
n
Q
init (x) \ rel(c) . The nogood repre{(x1 , a1 ) , . . . , (xr , ar )} | (a1 , . . . , ar ) ∈
x∈scp(c) dom

sentation Pe of a CN P is the set of instantiations

S



e ∪
x∈vars(P ) x

S

e .
c∈cons(P ) c


Instantiations in Pe are explicit nogoods of P (recorded through domains and constraints).
Notice that when a nogood is a superset of another one, it is subsumed. Intuitively, a nogood
that is subsumed is not relevant as it is less general than at least another one, and two
CNs are nogood-equivalent – a related definition is Definition 3.11 in the work of Bessiere
(2006) – when they have the same canonical form, i.e., represent exactly the same set of
“unsubsumed” nogoods. To relate CNs, we introduce a general partial order.2 Let P and
P 0 be two CNs defined on the same variables (i.e., such that vars(P ) = vars(P 0 )), P 0  P
f0 ⊇ Pe and P 0 ≺ P iff P
f0 ) Pe . (P, ) is a partially ordered set (poset) because 
iff P
is reflexive, antisymmetric (remember that no CN in P can involve universal constraints)
and transitive. As CNs are normalized and no unary or universal constraint is present,
there is therefore only one manner to discard (or remove) an instantiation from a given
CN, or equivalently to “record” a new explicit nogood in a CN. Given a CN P in P, and
an instantiation I on P , P \ I denotes the CN P 0 in P such that vars(P 0 ) = vars(P ),
f0 = Pe ∪ {I}. P \ I is an operation that retracts I from P and builds a new CN,
and P
not necessarily with the same set of constraints. Let us show how P 0 is built. If I ∈ Pe ,
of course we have P 0 = P \ I = P : this means that the instantiation I was already an
explicit nogood of P . The interesting case is when I ∈
/ Pe . If I corresponds to a value a for
a variable x, i.e., I = {(x, a)}, it suffices to remove a from dom(x). If I corresponds to a
tuple allowed by a constraint c of P , it suffices to remove this tuple from rel(c). Otherwise,
we must introduce a new constraint whose associated relation contains all possible tuples
(built from initial domains) except the one that corresponds to the instantiation I. Note
that removing a tuple from a relation rel(c) can be a problem in practice if the constraint
c is defined in intension (i.e., by a predicate). However, for binary nogoods (our concern),
this is not a real problem because, except when variables have very large domains, it is
always possible to translate (efficiently) an intensional constraint in extension.
2.2 Consistencies
A consistency is a general property of a CN. When a consistency φ holds on a CN P , we
say that P is φ-consistent. If φ and ψ are two consistencies, a CN P is said to be φ+ψconsistent iff P is both φ-consistent and ψ-consistent. A consistency φ is nogood-identifying
iff the reason why a CN P is not φ-consistent is that some instantiations, which are not
in Pe , are identified as globally inconsistent by φ. Such instantiations correspond to (new
identified) nogoods and are said to be φ-inconsistent (on P ). A kth-order consistency is
a nogood-identifying consistency that allows the identification of nogoods of size k, where
k ≥ 1 is an integer. kth-order consistency should not be confused with k-consistency
(Freuder, 1978, 1982): k-consistency holds iff every locally consistent instantiation of a
2. This partial order is general enough for our purpose, but note that more sophisticated partial orders (or
preorders) exist (e.g., by taking account of subsumed nogoods).

179

Lecoutre, Cardon, & Vion

set of k − 1 variables can be extended to a locally consistent instantiation involving any
additional variable. In our terminology, this is a (k − 1)th-order consistency.
A domain-filtering consistency is a first-order consistency. A conservative consistency φ
is a nogood-identifying consistency such that, for every given CN P , every φ-inconsistent
instantiation on P corresponds to a tuple currently allowed by an explicit constraint of P .
To compare the pruning capability of consistencies, we introduce a preorder (see Debruyne
& Bessiere, 2001). A consistency φ is stronger than (or equal to) ψ iff whenever φ holds
on a CN P , ψ also holds on P . φ is strictly stronger than ψ iff φ is stronger than ψ and
there exists at least one CN P such that ψ holds on P but not φ. When some consistencies
cannot be ordered (none is stronger that another), we say that they are incomparable.
We now briefly introduce a formal characterization of constraint propagation, based
on the concept of stability (following Lecoutre, 2009). This formalism is also related to
previous works about local consistencies and rules iteration (e.g., see Montanari & Rossi,
1991; Apt, 1999, 2003; Bessiere, 2006). It is usually possible to enforce φ on a CN P by
computing the greatest φ-consistent CN smaller than or equal to P , while preserving the set
of solutions. A consistency is well-behaved when for any CN P ∈ P, the set {P 0 ∈ P | P 0
is φ-consistent and P 0  P } admits a greatest element, denoted by φ(P ), that is equivalent
to P and called the φ-closure of P . Enforcing φ on a CN P means computing φ(P ), and
an algorithm that enforces φ is called a φ-algorithm. The property of stability is useful for
proving that a nogood-identifying consistency is well-behaved.
A nogood-identifying consistency φ is stable iff for every CN P ∈ P, every CN P 0 ∈ P
f0 or
such that P 0  P and every φ-inconsistent instantiation I on P , we have either I ∈ P
0
I is φ-inconsistent on P ; the second condition for stability given by Lecoutre (2009) holds
f0 or
necessarily because of the choice of the poset in this paper. The fact that either I ∈ P
0
I is φ-inconsistent on P guarantees that no φ-inconsistent instantiation on a CN can be
missed when the CN is made tighter: either it is discarded (has become an explicit nogood
of P 0 ) or it remains φ-inconsistent.
Theorem 1. (Lecoutre, 2009) Any stable nogood-identifying consistency is well-behaved.
The stability of a nogood-identifying consistency φ provides a general procedure for
computing the φ-closure of any CN: iteratively discard (in any order) φ-inconsistent instantiations until a fixed point is reached. Provided that the procedure is sound (each removal
corresponds to a φ-inconsistent instantiation) and complete (each φ-inconsistent instantiation is removed), the procedure is guaranteed to compute φ-closures. More generally, when
different reduction rules are used, each must be shown to be correct, monotonic and inflationary. We can then benefit from a generic iteration algorithm (Apt, 2003, Lemmas 7.5,
7.8 and Theorem 7.11). Stability under union can also be proved for a domain-filtering
consistency, thus guaranteeing a fixed point (Bessiere, 2006). An interesting result follows:
Proposition 1. Let φ and ψ be two well-behaved (nogood-identifying) consistencies. φ is
stronger than ψ iff for every CN P ∈ P, we have φ(P )  ψ(P ).
We conclude this section with some well-known domain-filtering consistencies. First, let
us introduce generalized arc consistency (GAC). A support (resp., a conflict) for a value
(x, a) of P on a constraint c involving x is a support (resp., a conflict) I on c such that
I[x] = a. A value (x, a) of P is GAC-consistent iff there exists a support for (x, a) on every
180

Second-Order Consistencies

constraint of P involving x. P is GAC-consistent iff every value of P is GAC-consistent.
We also say that a constraint c is GAC-consistent iff for every variable x in scp(c) and every
value a in dom(x), (x, a) is GAC-consistent. For binary CNs, GAC is referred to as AC (Arc
Consistency). Second, we introduce some “singleton” consistencies (Debruyne & Bessiere,
1997b; Prosser, Stergiou, & Walsh, 2000). When the domain of a variable of P is empty, P
is clearly unsatisfiable, which is denoted by P = ⊥. The CN P |x=a is obtained from P by
removing every value b 6= a from dom(x). A value (x, a) of P is SAC-consistent (SAC stands
for Singleton Arc Consistent3 ) iff GAC (P |x=a ) 6= ⊥ (this is called a singleton check). A
value (x, a) of P is BiSAC-consistent iff GAC (P ia |x=a ) 6= ⊥, where P ia is the CN obtained
after removing every value (y, b) of P such that y 6= x and (x, a) ∈
/ GAC(P |y=b ) (Bessiere
& Debruyne, 2008). P is SAC-consistent (respectively, BiSAC-consistent) iff every value
of P is SAC-consistent (respectively, BiSAC-consistent). BiSAC is strictly stronger than
SAC, which is itself strictly stronger than GAC; BiSAC is also strictly weaker than strong
path consistency (Bessiere & Debruyne, 2008). GAC, SAC and BiSAC are well-behaved;
for example, SAC (P ) denotes the SAC-closure of the CN P .

3. Second-Order Consistencies
In this section, we introduce second-order consistencies. First, we start with the most
famous one: path consistency. Then, we clarify some aspects of path consistency that
are sometimes misrepresented in the literature, and introduce its known restricted forms.
Finally, we introduce 3-consistency, dual consistency, and 2-singleton arc consistency as well
as their conservative and strong variants.
3.1 Path Consistency
Among the consistencies that allow us to identify inconsistent pairs of values, path consistency plays a central role. Introduced by Montanari (1974), its definition has sometimes
been misinterpreted. The problem arises around the definition of a “path”, which must be
understood as any sequence of variables, and not as a sequence of variables that corresponds
to a path in the constraint graph. This ambiguity probably comes from Montanari’s original
paper, in which reasoning from path consistency is achieved with respect to complete (or
completion of) constraint graphs, although a footnote in the original paper indicates:
“A path in a network is any sequence of vertices. A vertex can occur more than
once in a path even in consecutive positions.”
A precise definition of path is thus required. The definition of path below is used by
Montanari (1974), Mackworth (1977) and Debruyne (1998), while the definition of graphpath is used, for example, by Tsang (1993) or Bessiere (2006). A path is an arbitrary
sequence of variables, and a graph-path is defined to be a sequence of variables such that a
binary constraint exists between any two variables adjacent in the sequence. For a binary
CN P , a graph-path is thus a path in the constraint graph of P . For a non-binary CN P ,
if all non-binary constraints are discarded (ignored) then a path in the resulting constraint
graph is a graph-path. It is important to note that any given variable may occur several
times in a path (and so, in a graph-path). Figure 1 gives an illustration.
3. To limit the number of acronyms, we use SAC for both binary and non-binary CNs.

181

Lecoutre, Cardon, & Vion

Definition 1 (Path). Let P be a CN.
A path of P is a sequence hx1 , . . . , xk i of variables of P such that x1 6= xk and k ≥ 2; the
path is from variable x1 to variable xk , and k − 1 is the length of the path.
A graph-path of P is a path hx1 , . . . , xk i of P such that ∀i ∈ 1..k − 1, ∃c ∈ cons(P ) |
scp(c) = {xi , xi+1 }.
A closed (graph-)path of P is a (graph-)path hx1 , . . . , xk i of P such that ∃c ∈ cons(P ) |
scp(c) = {x1 , xk }.

v

z

x
w

y

Figure 1: The constraint graph of a binary CN P . hv, z, xi and hv, y, w, yi are paths of P .
hv, z, y, wi is a closed path of P . hv, x, yi is a graph-path of P . hv, x, wi and
hz, x, v, w, x, yi are two closed graph-paths of P .
The central concept of consistent paths is defined as follows:
Definition 2 (Consistent Path). Let P be a CN.
• An instantiation {(x1 , a1 ), (xk , ak )} on P is consistent on a path hx1 , . . . , xk i of P iff
there exists a tuple τ ∈ Πki=1 dom(xi ) such that τ [x1 ] = a1 , τ [xk ] = ak and ∀i ∈ 1..k−1,
{(xi , τ [xi ]), (xi+1 , τ [xi+1 ])} is a locally consistent instantiation4 on P . The tuple τ is
said to be a support for {(x1 , a1 ), (xk , ak )} on hx1 , . . . , xk i (in P ).
• A path hx1 , . . . , xk i of P is consistent iff every locally consistent instantiation of
{x1 , xk } on P is consistent on hx1 , . . . , xk i.
In the example in Figure 2, hv, z, xi is a consistent path of P since for the locally
consistent instantiation {(v, a), (x, b)} we can find b in dom(z) such that {(v, a), (z, b)} is
locally consistent (this is trivial since there is an implicit universal binary constraint between
v and z) and {(x, b), (z, b)} is locally consistent. Similarly, the second locally consistent
instantiation {(v, b), (x, a)} can be extended to z. The closed graph-path hv, x, wi is not
4. If xi = xi+1 , then necessarily τ [xi ] = τ [xi+1 ] because an instantiation cannot contain two distinct pairs
involving the same variable.

182

Second-Order Consistencies

consistent; the locally consistent instantiation {(v, b), (w, a)} cannot be extended to x. One
might be surprised that hz, x, v, w, x, yi is consistent. It is important to note that we are
free to select different values for x along the path. For example, for the locally consistent
instantiation {(z, b), (y, a)}, we can find the support τ = (b, b, a, b, a, a) on hz, x, v, w, x, yi.
This tuple belongs to dom(z) × dom(x) × dom(v) × dom(w) × dom(x) × dom(y), satisfies
τ [z] = b, τ [y] = a and all encountered binary constraints along the path. Along this path
we have first (x, b) and subsequently (x, a).
a

v

a

b

z
b

b
a
b
w

b

x
a

a

y

Figure 2: The compatibility graph of a binary CN P (whose constraint graph is given by
Figure 1). hv, z, xi is a consistent path of P . The closed graph-path hv, x, wi is
not consistent contrary to hz, x, v, w, x, yi.
We can now introduce the historical definition of path consistency (PC, Montanari, 1974;
Mackworth, 1977).
Definition 3 (Path Consistency). A CN P is path-consistent, denoted PC-consistent, iff
every path of P is consistent.
This definition is valid for non-binary CNs. Simply, non-binary constraints are ignored,
as Dechter (2003) or Bessiere (2006) do, since in Definition 2, only pairs of variables are
considered. Montanari has shown that it is sufficient to consider paths of length two (i.e.,
sequences of three variables) only. Note that it is not necessary for the constraint graph
to be complete (but, when path consistency is enforced, the resulting CN may become
complete).
Theorem 2. (Montanari, 1974) A CN P is path-consistent iff every 2-length path of P
(i.e., every sequence of three variables) is consistent.
This leads to the following classical definition:
Definition 4 (Path Consistency). Let P be a CN.
• An instantiation5 {(x, a), (y, b)} on P is path-consistent, denoted PC-consistent, iff
it is 2-length path-consistent, that is to say, iff there exists a value c in the domain of
every third variable z of P such that {(x, a), (z, c)} and {(y, b), (z, c)} are both locally
5. In this paper, when we refer to an instantiation of the form {(x, a), (y, b)}, we assume that x 6= y.

183

Lecoutre, Cardon, & Vion

consistent; if {(x, a), (y, b)} is not path-consistent, it is said to be path-inconsistent or
PC-inconsistent.
• P is path-consistent iff every locally consistent instantiation {(x, a), (y, b)} on P is
path-consistent.
3.2 Deep in Path Consistency
Now, we show that path consistency may be easily misinterpreted, and we introduce consistency forms related to PC. A first natural question is: can we restrict our attention to
graph-paths (see Definition 1)? The answer is given by the following observation.
Observation 1. For some CNs, the following properties are not equivalent:
(a) every path is consistent
(b) every graph-path is consistent
Proof. Consider, for example, the CN depicted in Figure 3. This CN is not path-consistent
since the locally consistent instantiation {(x, b), (z, a)} is not consistent on the path hx, y, zi.
If we now limit our attention to graph-paths, these consist only of the variables x and y
(for example, hx, yi, hx, y, x, yi, . . . ) and there is no local inconsistency.
x
a

b

y

z
a

a

Figure 3: A CN P with three variables and only one constraint (between x and y). P is
not path-consistent. However, any graph-path that can be built is consistent.
However, for a binary CN that has a connected constraint graph (i.e., a constraint graph
composed of a single connected component), the restriction to graph-paths is valid.
Proposition 2. Let P be a binary CN such that P 6= ⊥ and the constraint graph of P is
connected. P is path-consistent iff every graph-path of P is consistent.
Proof. For one direction (⇒), this is immediate. If P is path-consistent, then by definition
every path of P is consistent, including graph-paths.
For the other direction (⇐), we show that if every graph-path of P is consistent, then
every 2-length path of P is consistent (and thus P is path-consistent using Theorem 2).
In practical terms, we consider a locally consistent instantiation {(x, a), (y, b)} and show
that for each third variable z of P , the following property P r(z) holds: ∃c ∈ dom(z) such
that {(x, a), (z, c)} and {(y, b), (z, c)} are both locally consistent instantiations. For each
variable z three cases must be considered, depending of the existence of the constraints cxz ,
between x and z (i.e., such that scp(cxz ) = {x, z}), and cyz , between y and z.
184

Second-Order Consistencies

(a) Both constraints exist: thus there exists a graph-path hx, z, yi and as this path is
consistent by hypothesis, the property P r(z) holds.
(b) Neither constraint exist: P 6= ⊥ implies dom(z) 6= ∅, thus P r(z) holds because cxz
and cyz are implicit and universal.
(c) Only the constraint cxz exists (similarly, only the constraint cyz exists): as the constraint graph is connected, there exists at least one graph-path from z to y, and
consequently a graph-path from x to y of the form hx, z, . . . , yi. This means that
there is a value in dom(z) which is compatible with (x, a), by using the hypothesis
(every graph-path is consistent). This value is also compatible with (y, b) because
there is an implicit universal constraint between z and y. Hence, P r(z) holds.
Unsurprisingly (because of Observation 1), a binary CN P such that every 2-length
graph-path of P is consistent, is not necessarily path-consistent. Of course, in the special
case where the constraint graph is complete, the CN is path-consistent because every path
of P is also a graph-path of P .
Observation 2. For some CNs, the following properties are not equivalent:
(a) every graph-path is consistent
(b) every 2-length graph-path is consistent
Proof. See Figure 4.

a

v

a

b

z
b

b
a
b
w

b

x
a

a

y

Figure 4: Every 2-length graph-path of this CN (whose constraint graph is given by Figure
1) is consistent. The graph-path hx, w, v, x, zi is not consistent for {(x, a), (z, a)}.

We have the following proposition for 2-length graph-paths.
Proposition 3. Let P be a binary CN such that P 6= ⊥ and P is arc-consistent. P is
path-consistent iff every 2-length graph-path of P is consistent.
Proof. The proof is similar to the proof of Proposition 2 by considering 2-length graph-paths
instead of graph-paths. Only case (c) in the demonstration differs.
185

Lecoutre, Cardon, & Vion

(c) Only the constraint cxz exists (similarly, only the constraint cyz exists): as P is arcconsistent, there exists a value in dom(z) that is compatible with (x, a). Because there
is an implicit universal constraint between z and y, this value is also compatible with
(y, b). Hence, P r(z) holds.
Theorem 2 and Propositions 2 and 3 suggest that the historical definition of path consistency is appropriate since it corresponds to the strongest form of detection of local inconsistencies using the concept of path (even if considering graph-paths may seem more natural
than considering paths). Unfortunately, path consistency has sometimes been misinterpreted. For example, Definition 3-11 in a work of Tsang (1993) links PC to graph-paths,
and Proposition 3.39 in a work of Bessiere (2006) links PC to 2-length graph-paths, but
Observations 1 and 2 (with Figure 4) show that this is not equivalent to Montanari’s definition. However, it is true that to check path consistency in practice, we only need to consider
2-length graph-paths, provided that binary constraints are arc-consistent.
Two different relation-filtering consistencies related to path consistency have been defined in terms of closed graph-paths. The first is partial path consistency (partial PC or
PPC, Bliek & Sam-Haroud, 1999) and the second is conservative path consistency (conservative PC or CPC, Debruyne, 1999).
Definition 5 (Partial Path Consistency). A CN P is partially path-consistent, denoted
PPC-consistent, iff every closed graph-path of P is consistent.
Definition 6 (Conservative Path Consistency). A CN P is conservative path-consistent,
denoted CPC-consistent, iff every closed 2-length graph-path of P is consistent.
For binary constraints, PPC and CPC are equivalent when the constraint graph is
triangulated: PPC was initially introduced to build a filtering algorithm that operates on
triangulated graphs. A graph is triangulated (or chordal) iff every cycle composed of four
or more vertices has a chord, which is an edge joining two vertices that are not adjacent in
the cycle.
Proposition 4. (Bliek & Sam-Haroud, 1999) Let P be a binary CN P with a triangulated
constraint graph. P is PPC-consistent iff P is CPC-consistent.
Enforcing path consistency simply means discarding path-inconsistent instantiations
(i.e., recording new explicit nogoods of size two) since we know that the P C-closure, denoted
PC (P ), of any CN P exists (path consistency is well-behaved). To enforce path consistency
it may be necessary to introduce new binary constraints, thus path consistency is not a
conservative consistency. PPC and CPC differ in that only existing constraints are altered.
As additional weak forms of path consistency, we find directional path consistency (Dechter
& Pearl, 1988; Tsang, 1993) and pivot consistency (David, 1995), but they will not be
discussed in this paper. Although these consistencies are attractive for controlling the
practical inference effort in some situations, both consistencies require the introduction of
a variable ordering, which restricts their applicability.
Finally, two domain-filtering consistencies related to path consistency have also been
defined: restricted path consistency (RPC, Berlandier, 1995) and max-restricted path consistency (MaxRPC, Debruyne & Bessiere, 1997a). MaxRPC is strictly stronger than RPC
186

Second-Order Consistencies

and is defined as follows: a value (x, a) of a CN P is max-restricted path consistent, denoted MaxRPC-consistent, iff for every binary constraint cxy of P involving x, there exists
a locally consistent instantiation {(x, a), (y, b)} of scp(cxy ) = {x, y} such that for every
additional variable z of P , there exists a value c ∈ dom(z) such that {(x, a), (z, c)} and
{(y, b), (z, c)} are both locally consistent. A CN P is MaxRPC-consistent iff every value of
P is MaxRPC-consistent.
3.3 Additional Second-Order Consistencies
In this section, we introduce second-order consistencies that are defined independently of
path consistency. First, we recall 3-consistency (3C, Freuder, 1978).
Definition 7 (3-consistency). Let P be a CN.
• An instantiation {(x, a), (y, b)} on P is 3-consistent, denoted 3C-consistent, iff there
exists a value c in the domain of every third variable z of P such that {(x, a), (y, b), (z, c)}
is locally consistent.
• P is 3-consistent iff every locally consistent instantiation {(x, a), (y, b)} on P is 3consistent.
Note the difference with path consistency (see Definition 4): here, an instantiation of
size 3 must be locally consistent (instead of two instantiations of size 2). It is known that
3C is equivalent to PC when no ternary constraint is present.
We now introduce dual consistency (Lecoutre et al., 2007a). Dual consistency, whose
idea has initially been used by McGregor (1979), records inconsistent pairs of values identified by successive singleton checks. Just like singleton arc consistency, dual consistency
is built on top of generalized arc consistency. Informally, a CN is dual-consistent iff each
pair of values that is locally consistent is not detected inconsistent after assigning either of
those two values and enforcing GAC. To simplify, we write (x, a) ∈ P iff (x, a) is a value of
P , i.e., x ∈ vars(P ) ∧ a ∈ domP (x); when P = ⊥, we consider that for every pair (x, a), we
have (x, a) ∈
/ P.
Definition 8 (Dual Consistency). Let P be a CN.
• An instantiation {(x, a), (y, b)} on P is dual-consistent, denoted DC-consistent, iff
(y, b) ∈ GAC (P |x=a ) and (x, a) ∈ GAC (P |y=b ).
• P is DC-consistent iff every locally consistent instantiation {(x, a), (y, b)} on P is
DC-consistent.
We may be interested by checks based on two simultaneous decisions (variable assignments): we obtain 2-singleton arc consistency (2SAC, Bessiere et al., 2005).
Definition 9 (2-Singleton Arc Consistency). Let P be a CN.
• An instantiation {(x, a), (y, b)} on P is 2-singleton arc-consistent, denoted 2SACconsistent, iff GAC (P |{x=a,y=b} ) 6= ⊥.
187

Lecoutre, Cardon, & Vion

• P is 2SAC-consistent iff every locally consistent instantiation {(x, a), (y, b)} on P is
2SAC-consistent.
3-consistency, dual consistency and 2-singleton arc consistency are second-order consistencies, from which conservative restrictions can be naturally derived as follows.
Definition 10 (Conservative Second-Order Consistency). Let P be a CN, and φ be a
consistency in {3C, DC, 2SAC}.
• An instantiation {(x, a), (y, b)} on P is conservative φ-consistent, denoted Cφ-consistent,
iff either @c ∈ cons(P ) | scp(c) = {x, y} or {(x, a), (y, b)} is φ-consistent.
• P is Cφ-consistent iff every locally consistent instantiation {(x, a), (y, b)} on P is
Cφ-consistent.
Thus, we obtain three new second-order consistencies called conservative 3-consistency
(C3C), conservative dual consistency (CDC, Lecoutre et al., 2007a) and conservative 2singleton arc consistency (C2SAC). To illustrate the difference between a consistency φ and
its conservative restriction Cφ, let us consider a CN P such that vars(P ) = {w, x, y, z} and
cons(P ) = {cwx , cwz , cxyz }, where subscripts indicate constraint scopes. φ reviews (locally
consistent instantiations of) all of the six possible distinct pairs of variables whereas Cφ
reviews only the two pairs (w, x) and (w, z).
We shall also be interested in strong variants of second-order consistencies that additionally guarantee generalized arc consistency. For example, a binary CN is strong pathconsistent, denoted sPC-consistent, iff it is both arc-consistent and path-consistent; a CN
is strong dual-consistent, denoted sDC-consistent iff it is both GAC-consistent and DCconsistent. s3C, s2SAC, sPPC, sCPC, sCDC, sC3C, sC2SAC are defined similarly.
Definition 11 (Strong Second-Order Consistency). Let φ be a second-order consistency. A
CN P is strong φ-consistent, denoted sφ-consistent, iff P is GAC+φ-consistent, i.e., both
GAC-consistent and φ-consistent.
A strong second-order consistency φ identifies both φ-inconsistent values (nogoods of
size one) and φ-inconsistent pairs of values (nogoods of size two). Strictly speaking, this is
not a second-order consistency but a “first+second” order consistency.
It is important to note that the closure of a CN can be computed for all second-order
consistencies mentioned so far. All such consistencies can be proved to be stable, and
consequently well-behaved.
Proposition 5. PC, 3C, DC, 2SAC, PPC, CPC, C3C, CDC, C2SAC, as well as their
strong variants, are consistencies that are well-behaved.
Sketch of proof. Following Theorem 1, it is sufficient to show that all mentioned consistencies are stable (see Page 180). For each consistency φ among those mentioned in the
proposition, when an instantiation I is φ-inconsistent on a CN P , necessarily if I is not
an explicit nogood in a CN P 0 smaller than or equal to P , I is φ-inconsistent on P 0 .
For example, suppose that I = {(x, a), (y, b)} is DC-inconsistent on P . This means that
f0 , we must then show that I is
(y, b) ∈
/ GAC (P |x=a ) or (x, a) ∈
/ GAC (P |y=b ). If I ∈
/ P
0
DC-inconsistent on P . But we necessarily have (y, b) ∈
/ GAC (P 0 |x=a )  GAC (P |x=a ) or
(x, a) ∈
/ GAC (P 0 |y=b )  GAC (P |y=b ) because P 0  P . Hence, I is DC-inconsistent on
0
P.
188

Second-Order Consistencies

4. Relationships between Second-Order Consistencies
This section studies the qualitative relationships between the second-order consistencies that
we have presented, namely path consistency, 3-consistency, dual consistency, 2-singleton arc
consistency, and their conservative and strong variants. This section is composed of three
parts (subsections). We start with relationships between basic second-order consistencies
(PC, 3C, DC, 2SAC). Then, we focus on relationships including conservative restrictions
(PPC, CPC, C3C, CDC, C2SAC). Finally, we finish with strong second-order consistencies
(sPC, s3C, sDC, s2SAC, sPPC, sCPC, sC3C, sCDC, sC2SAC).
In our previous works (Lecoutre et al., 2007a, 2007b), our study was limited to binary
CNs. In this paper, we generalize our results to CNs of any arity, although some results
are given specifically for binary CNs or non-binary CNs. When no precision is given, the
results hold for the set of all possible binary and non-binary CNs (i.e., CNs with constraints
of arbitrary arity).
4.1 Results on Basic Second-Order Consistencies
We start with the strongest (basic) second-order consistency of this paper, namely 2SAC.
Proposition 6. 2SAC is strictly stronger than DC, and strictly stronger than 3C.
Proof. Let P be a CN and I = {(x, a), (y, b)} be a locally consistent instantiation on P .
On the one hand, if I is DC-inconsistent then either (y, b) ∈
/ GAC (P |x=a ) or (x, a) ∈
/
GAC (P |y=b ), which necessarily entails GAC (P |{x=a,y=b} ) = ⊥. Consequently, I is 2SACinconsistent, and it follows that 2SAC is stronger than DC. On the other hand, if I is 3Cinconsistent then ∃z ∈ vars(P ) | ∀c ∈ dom(z), {(x, a), (y, b), (z, c)} is not locally consistent.
In the CN P 0 = GAC (P |{x=a,y=b} ), we necessarily have dom(z) = ∅ (because x and y are
assigned and GAC is enforced), and thus P 0 = ⊥. This means that I is 2SAC-inconsistent,
and it follows that 2SAC is stronger than 3C. Strictness is proved by Figure 5 that shows
a binary CN that is DC-consistent, 3C-consistent but not 2SAC-consistent.6
On binary CNs, DC is equivalent to PC. This could be predicted since McGregor proposed an AC-based algorithm to establish sPC (1979). We show this in 2 steps.
Proposition 7. DC is strictly stronger than PC.
Proof. Let P be a CN and I = {(x, a), (y, b)} be a locally consistent instantiation on P . If I
is path-inconsistent then ∃z ∈ vars(P ) | ∀c ∈ dom(z), {(x, a), (z, c)} or {(y, b), (z, c)} is not
locally consistent (see Definition 4). In this case, we know that (y, b) ∈
/ GAC (P|x=a ) since
after enforcing GAC on P|x=a , every value c remaining in dom(z) is such that {(x, a), (z, c)}
is consistent. Necessarily, by hypothesis, all these remaining values are incompatible with
(y, b), thus b is removed from dom(y) when enforcing GAC. Hence I is dual-inconsistent,
and it follows that DC is stronger than PC. Strictness is proved by Figure 6 that shows a
non-binary CN that is PC-consistent but not DC-consistent.
Proposition 8. On binary CNs, DC is equivalent to PC.
6. This result, as well as those of Figures 9 and 12, has been computer-checked.

189

Lecoutre, Cardon, & Vion

w

c

w

x

b
b

a

c

x

b
b

a
a

a

a

a
b
z

a

b
c

b
y

c

z

(a) The compatibility graph of P .

w

a

c

w

b

a

c

y

c

(b) The incompatibility graph of P .

x

b

b

c

x

b
b

a
a

a

a

a
b
z

a

a

b
c

c

b
y

z

(c) {(w, b), (x, a)} is DC-consistent because
(x, a) ∈ GAC(P |w=b ) (and similarly,
(w, b) ∈ GAC(P |x=a )).

b
c

c

y

(d) {(w, b), (x, a)} is 2SAC-inconsistent because GAC(P |{w=b,x=a} ) = ⊥.

Figure 5: A CN P that is sDC-consistent (and s3C-consistent) but not C2SAC-consistent.
Dotted circles and lines correspond to deleted values and tuples.

Proof. From Proposition 7, we know that DC is stronger than PC. Now, we show that,
on binary CNs, PC is stronger than DC, therefore we can conclude that DC and PC are
equivalent. Let P be a binary CN and I = {(x, a), (y, b)} be a locally consistent instantiation
on P . If I is dual-inconsistent then (y, b) ∈
/ AC (P|x=a ), or symmetrically (x, a) ∈
/ AC (P|y=b ).
We consider the first case.
Let us consider a filtering procedure F that iteratively removes (in any order) the values of P|x=a that are successively found to be arc-inconsistent up to a fixpoint. Such a
procedure is guaranteed to compute AC (P|x=a ) (cf. Apt, 2003; Lecoutre, 2009). Let H(k)
be the following induction hypothesis: if (z, c) is one of the k first values removed by F,
190

Second-Order Consistencies

w

b

b

x

a

a

a

a

z

b

b

y

Figure 6: A CN P with two ternary constraints cwxy and cwxz such that rel(cwxy ) =
{(a, a, a), (b, b, b)} and rel(cwxz ) = {(a, b, a), (b, a, b)}. P also involves a binary constraint cyz . P is sPC-consistent but not CDC-consistent. Indeed,
{(y, a), (z, a)} is DC-inconsistent since GAC (P |y=a ) = ⊥.

f0 with P 0 = PC (P ), i.e., {(x, a), (z, c)} is either initially locally
then {(x, a), (z, c)} ∈ P
inconsistent or identified as path-inconsistent (possibly after propagation).
We show that H(1) holds. If (z, c) is the first value removed by F, this means that (z, c)
has no support on a binary constraint involving z and a second variable w. If {(x, a), (z, c)}
is locally inconsistent, then H(1) holds trivially. Otherwise, necessarily w 6= x (because
this would mean that (z, c) is not compatible with (x, a) since a has been assigned to x,
so {(x, a), (z, c)} is initially locally inconsistent). Therefore {(x, a), (z, c)} clearly has no
support on the path hx, w, zi and is thus path-inconsistent.
We now assume that H(k) is true and show that H(k + 1) holds. If (z, c) is the k + 1th
value removed by F, this means that this removal involves a constraint binding z with
another variable w. The value (z, c) has no support on this constraint, thus every value
in dom(w) initially supporting (z, c), if any, is one of the k first values removed by F. By
f0 with P 0 = PC (P ). In
hypothesis this means that for any such value b, {(x, a), (w, b)} ∈ P
f
0
any case, we can now deduce that {(x, a), (z, c)} ∈ P and, as a special case, we can identify
I as path-inconsistent. Consequently, every locally consistent instantiation on P that is in
f0 with P 0 = PC (P ). We deduce that PC (P )  DC(P )
Pf00 with P 00 = DC(P ) is also in P
and also from Proposition 1 that PC is stronger than DC on binary CNs.

Now, we consider 3-consistency. On binary CNs, it is well-known that 3-consistency is
equivalent to path consistency. So, 3C is also equivalent to DC (since DC is equivalent to
PC). On non-binary CNs, we have the following relationships with PC and DC.
Proposition 9. On non-binary CNs, 3C is incomparable with DC, and strictly stronger
than PC.
Proof. On non-binary CNs, 3C is strictly stronger than PC (e.g., see Dechter, 2003, p. 69)
because 3C also checks ternary constraints. When comparing 3C with DC, it appears that
3C cannot be stronger than DC because a CN composed of only one quaternary constraint
191

Lecoutre, Cardon, & Vion

that is not GAC-consistent is necessarily 3-consistent (since there are no binary and ternary
constraints) and not DC-consistent. On the other hand, DC cannot be stronger than 3C
because Figure 7 shows a non-binary CN that is DC-consistent but not 3-consistent (there
is no way of extending {(x, a), (y, a)} on z). Hence, 3C and DC are incomparable.

x

a

a

b

y
b

a

b

c

z

Figure 7: A CN P that is sDC-consistent but not C3C-consistent (because {(x, a), (y, a)} is
not 3-consistent). The dashed hyperedge corresponds to the nogood {(x, a), (y, a),
(z, b)}, i.e., a ternary constraint cxyz only forbidding the tuple (a, a, b).

4.2 Results on Conservative Second-Order Consistencies
Now, we consider conservative variants of the basic second-order consistencies. A first
immediate result is that conservative consistencies are made strictly weaker than their
unrestricted forms. However, it is worthwhile to mention that (strong) PPC was shown
equivalent to (strong) PC on binary convex CNs with triangulated constraint graphs (Bliek
& Sam-Haroud, 1999).
Proposition 10. 2SAC, DC, 3C and PC are respectively strictly stronger than C2SAC,
CDC, C3C and PPC+CPC.
Proof. By definition, conservative consistencies are weaker. Strictness is proved by Figure 8
that shows a binary CN which is C2SAC-consistent (and CDC-consistent, C3C-consistent,
PPC-consistent, CPC-consistent) but not PC-consistent (and not 3C-consistent, not DCconsistent and not 2SAC-consistent).
Proposition 11. PC, DC and 3C are incomparable with C2SAC.
Proof. On the one hand, PC cannot be stronger than C2SAC since Figure 5 shows a binary
CN that is PC-consistent but not C2SAC-consistent. On the other hand, C2SAC cannot
be stronger than PC since Figure 8 shows a binary CN that is C2SAC-consistent but not
PC-consistent. We conclude that PC and C2SAC are incomparable. This is also valid for
DC and 3C since only binary CNs are mentioned in this proof (and PC=DC=3C on binary
CNs).
Proposition 12. C2SAC is strictly stronger than CDC, and strictly stronger than C3C.
192

Second-Order Consistencies

Proof. The proof is similar to that of Proposition 6, considering initially a locally consistent
instantiation I = {(x, a), (y, b)} which is CDC-inconsistent (and next C3C-inconsistent) and
such that x is linked to y by a constraint.
w

b

b

x

a

a

a

a

z

b

b

y

Figure 8: A CN (no constraint binds w with y and x with z) that is sC2SAC-consistent
(and CDC+C3C+PPC+CPC-consistent), but not PC-consistent (and not 2SACconsistent). For example, {(x, a), (z, b)} is not path-consistent.

Proposition 13. CDC is strictly stronger than PPC.
Proof. Assume that a CN P is CDC-consistent and consider a closed graph-path hx1 , . . . , xp i
of P . For every locally consistent instantiation {(x1 , a1 ), (xp , ap )} on P , (xp , ap ) ∈ P 0 with
P 0 = GAC (P |x1 =a1 ) since P is CDC-consistent. It also implies P 0 6= ⊥. Therefore, in the
context of P 0 , there exists at least one value in each domain and since P 0 is generalized
arc-consistent, there is clearly a value (xp−1 , ap−1 ) of P 0 compatible with (xp , ap ), a value
(xp−2 , ap−2 ) of P 0 compatible with (xp−1 , ap−1 ), . . . , and a value (x1 , a01 ) of P 0 compatible
0
with (x2 , a2 ). Because domP (x1 ) = {a1 }, we have a01 = a1 , thus the locally consistent
instantiation {(x1 , a1 ), (xp , ap )} is consistent on the closed graph-path hx1 , . . . , xp i of P .
Hence P is PPC-consistent, thus CDC is stronger than PPC.
The fact that CDC is strictly stronger than PPC is shown by the CN P depicted in
Figure 9. In Figure 9(c), P is shown to be CDC-inconsistent because the locally consistent
instantiation {(x, a), (y, b)} is dual-inconsistent: (y, b) ∈
/ AC (P |x=a ). In Figure 9(d), P
is shown to be CPC-consistent because, for example, the locally consistent instantiation
{(x, a), (y, b)} is consistent on all 2-length graph-paths linking x to y, namely, hx, z, yi and
hx, w, yi. Here, the constraint graph is triangulated, which means that CPC is equivalent
to PPC. Hence we can deduce our result.
Proposition 14. On non-binary CNs, CDC is incomparable with C3C.
Proof. CDC cannot be stronger than C3C since Figure 7 shows a non-binary CN that is
CDC-consistent but not C3C-consistent. Now, consider the CN of Figure 9 extended with a
single ternary constraint involving new variables. This CN remains C3C-consistent (because
193

Lecoutre, Cardon, & Vion

y
a
x

y
a

b

a

z

a

b

x

b

a
w

a

w

v

c

b

a

b

a
b

b

(b) The incompatibility graph of P .

y
a
x

y
a

b

a

z

a

b

x

b

a
w

v

c

(a) The compatibility graph of P (no constraint
binds x with v).

z

a

b

a
b

b

b

w

v

a
b

b
c

(c) P is not CDC-consistent. We can see that
(y, b) ∈
/ AC (P |x=a ). Thus, the locally consistent
instantiation {(x, a), (y, b)} is dual-inconsistent.

z

a

a

b
c

a
b

a
b

b

v

(d) P is sCPC-consistent (and hence sPPCconsistent since P is triangulated). Any (closed)
2-length graph-path of P linking x to y is consistent. This is shown here for {(x, a), (y, b)}.

Figure 9: Example of a binary CN P that is sPPC-consistent (and sC3C-consistent) but
not CDC-consistent.

194

Second-Order Consistencies

there is no binary constraint between these new variables), but is still not CDC-consistent.
Hence, C3C cannot be stronger than CDC, and CDC and C3C are incomparable.
Proposition 15. On binary CNs, PPC is strictly stronger than C3C. On non-binary CNs,
PPC is incomparable with C3C.
Proof. 1) Let P be a binary CN that is PPC-consistent (and different from ⊥ – this
is a very weak restriction). We consider a locally consistent instantiation {(x, a), (y, b)}
on P (such that there is a binary constraint involving x and y) and show that for every third variable z of P , the following property P r(z) holds: ∃c ∈ dom(z) such that
{(x, a), (z, c)} and {(y, b), (z, c)} are both locally consistent instantiations, which is equivalent to “{(x, a), (y, b), (z, c)} is locally consistent” since P is binary. If P r holds, then
{(x, a), (y, b)} is C3C-consistent. For each variable z, 3 cases must be considered, depending of the existence of the constraints cxz , between x and z, and cyz , between y and z.
(a) Both constraints exist: thus, there exists a graph-path hx, z, yi and as this path is
consistent by hypothesis, necessarily the property P r(z) holds.
(b) Neither constraint exist: P 6= ⊥ implies dom(z) 6= ∅, thus P r(z) holds because cxz
and cyz are implicit and universal.
(c) Only the constraint cxz exists (similarly, only the constraint cyz exists). Consider the
graph-path hx, z, x, yi. By hypothesis, this graph-path is consistent. Hence, there
exists a value c in dom(z) such that {(x, a), (z, c)} is locally consistent. We also know
that{(y, b), (z, c)} is locally consistent because there is no constraint between y and
z. We conclude that P r(z) holds and PPC is stronger than C3C on binary CNs.
Figure 10 proves strictness by showing a CN that is C3C-consistent but not PPC-consistent.
2) For non-binary CNs, Figure 7 shows that PPC cannot be stronger than C3C: the CN is
PPC-consistent but not C3C-consistent. Now, consider the CN of Figure 10 extended with a
single ternary constraint involving new variables. This CN remains C3C-consistent (because
there is no binary constraint between these new variables), but is still not PPC-consistent.
Hence, C3C cannot be stronger than PPC, and PPC and C3C are incomparable.
Proposition 16. C3C is strictly stronger than CPC.
Proof. Let P be a CN that is C3C-consistent. Let hx, z, yi be a closed 2-length graph-path
of P and {(x, a), (y, b)} be a locally consistent instantiation on P . Because P is C3Cconsistent, we know that there exists a value c in dom(z) such that {(x, a), (y, b), (z, c)} is
locally consistent. Consequently, there exists a value c in dom(z) such that {(x, a), (z, c)}
and {(y, b), (z, c)} are both locally consistent. We deduce that the path hx, z, yi is consistent,
thus P is CPC-consistent and C3C is stronger than CPC. Figure 11 proves strictness by
showing a binary CN that is CPC-consistent (there is no 3-clique) but not C3C-consistent.

Proposition 17. PPC is strictly stronger than CPC.
195

Lecoutre, Cardon, & Vion

w

b

x

b

a

a

a

a
b

z

b

y

Figure 10: A CN (no constraint binds w with y and x with z) that is sC3C-consistent (and
sCPC-consistent, and BiSAC-consistent) but not PPC-consistent. For example,
{(x, a), (w, a)} is not PPC-consistent.
x
a

b

y

z
a

a

Figure 11: A binary CN P with two constraints (no constraint exists between y and z). P
is CPC-consistent but not C3C-consistent.

Proof. PPC is stronger than CPC by definition. Moreover, the binary CN in Figure 10
is CPC-consistent but not PPC-consistent. Because there is no 3-clique in its constraint
graph, this CN is trivially CPC-consistent.
Proposition 18. On non-binary CNs, PC is incomparable with CDC.
Proof. On the one hand, consider the CN of Figure 8 extended with a single ternary GACconsistent constraint involving new variables. Because the additionnal constraint is GACconsistent, the CN remains CDC-consistent. However, it is not PC-consistent. We deduce
that CDC cannot be stronger than PC (on non-binary CNS). On the other hand, Figure
6 proves that PC cannot be stronger than CDC: P is PC-consistent (because there is only
one binary constraint) but not CDC-consistent ({(y, a), (z, a)} is CDC-inconsistent). We
conclude that on non-binary CNs, PC and CDC are incomparable.
4.3 Results on Strong Second-Order Consistencies
Before studying the relationships existing between strong variants of second-order consistencies, we observe that, in the binary case, enforcing AC on a path-consistent CN is sufficient
196

Second-Order Consistencies

to obtain a strong path-consistent CN. This well-known fact is also true in the general case
for DC, CDC, 2SAC and C2SAC. We define φ ◦ ψ(P ) as being φ(ψ(P )).
Proposition 19. For any binary CN P , we have AC ◦ PC (P ) = sPC (P ).
Proof. In PC (P ), every locally consistent instantiation {(x, a), (y, b)} has a support on
every third variable z. Hence, every value of PC (P ) in a locally consistent instantiation is
arc-consistent. Consequently, when AC is enforced on PC (P ), no value present in a locally
consistent instantiation (of size 2) can be removed, and PC is preserved.
Proposition 20. For any CN P , we have GAC ◦ DC (P ) = sDC (P ), GAC ◦ CDC (P ) =
sCDC (P ), GAC ◦ 2SAC (P ) = s2SAC (P ) and GAC ◦ C2SAC (P ) = sC2SAC (P ).
Proof. Let P 0 = DC (P ) and P 00 = GAC (P 0 ). For any singleton check GAC (P 00 |x=a ) on P 00 ,
we have GAC (P 00 |x=a ) = GAC (GAC (P 0 )|x=a ) = GAC (P 0 |x=a ). This means that the result
of the singleton check for (x, a) on P 00 is the same as the result of the singleton check for
(x, a) on P 0 . Since P 0 is DC-consistent, we can deduce that DC(P 00 ) = P 00 . P 00 being both
GAC-consistent and DC-consistent, we have P 00 = GAC ◦ DC(P ) = sDC(P ). A similar
proof holds for CDC , 2SAC and C2SAC .
It has been shown that the schema of previous propositions does not hold for CPC and
PPC (Lecoutre, 2009). For example, for some binary CNs P , we have AC ◦ CPC (P ) 6=
sCPC (P ). Unsurprisingly, some relationships are preserved when strong variants are considered.
Proposition 21. Let φ and ψ be two second-order consistencies. If φ is stronger than ψ
then sφ is stronger than sψ.
Proposition 22. We have:
(a) s2SAC is strictly stronger than sDC, and strictly stronger than s3C.
(b) sDC is strictly stronger than sPC.
(c) s3C is strictly stronger than sPC.
(d) s2SAC, sDC, s3C, and sPC are respectively strictly stronger than sC2SAC, sCDC,
sC3C, and sPPC+sCPC.
(e) sC2SAC is strictly stronger than sCDC, and strictly stronger than sC3C.
(f) sCDC is strictly stronger than sPPC.
(g) sPPC is strictly stronger than sCPC.
Proof. All illustrative CNs introduced previously are GAC-consistent (except for Figure 11),
thus using Proposition 21, it suffices to consider: (a) Proposition 6 with Figure 5 for strictness, (b) Proposition 7 with Figure 6 for strictness, (c) Proposition 9 with Figure 7 for
strictness, (d) Proposition 10 with Figure 8 for strictness, (e) Proposition 12 with Figure 5
for strictness, (f) Proposition 13 with Figure 9 for strictness, (g) Proposition 17 with Figure
10 for strictness.
197

Lecoutre, Cardon, & Vion

The following result indicates that C3C and CPC are quite close properties. For arcconsistent CNs, they are equivalent.
Proposition 23. On binary CNs, sC3C is equivalent to sCPC.
Proof. From Propositions 16 and 21, we know that sC3C is stronger than sCPC. Now, we
show that, on binary CNs, sCPC is stronger than sC3C. The proof is similar to that of
Proposition 15 by considering a binary CN that is initially sCPC-consistent. Only case (c)
in the demonstration differs:
(c) Only the constraint cxz exists (similarly, only the constraint cyz exists): as P is arcconsistent, there exists a value in dom(z) that is compatible with (x, a). Because there
is an implicit universal constraint between z and y, this value is also compatible with
(y, b). Hence, P r(z) holds.
Proposition 24. sPC, sDC and s3C are incomparable with sC2SAC.
Proof. In the proof of Proposition 11, the CNs of Figures 5 and 8 are GAC-consistent.
Proposition 25. On non-binary CNs, sPC is incomparable with sCDC.
Proof. In the proof of Proposition 18, the CNs of Figures 8 and 6 are GAC-consistent.
Finally, we conclude this section by establishing some connections with SAC.
Proposition 26. sDC is strictly stronger than SAC+CDC
Proof. Let P be a CN that is sDC-consistent. Assume that a value (x, a) of P is SACinconsistent. This means that P 0 = GAC (P |x=a ) = ⊥, and for every value (y, b), we have
(y, b) ∈
/ P 0 (recall that no value belongs to ⊥). As P is DC-consistent by hypothesis, these
nogoods are recorded in P meaning that for every variable y, we have a binary constraint
cxy forbidding any tuple involving (x, a). We deduce that (x, a) is GAC-inconsistent, which
contradicts our hypothesis (P sDC-consistent), and shows that sDC is stronger than SAC.
As we know that DC is strictly stronger than CDC, we deduce that sDC is stronger than
SAC+CDC. To prove strictness, it suffices to build a CN that is SAC-consistent, CDCconsistent but not DC-consistent (e.g., see Figure 8).
Proposition 27. On binary CNs, sCDC is strictly stronger than SAC.
Proof. Let P be a binary CN that is sCDC-consistent. Assume that a value (x, a) of P is
SAC-inconsistent. This means that AC (P |x=a ) = ⊥. As P is AC-consistent (since P is
sCDC-consistent by hypothesis), necessarily x is involved in (at least) a binary constraint
c (otherwise no propagation is possible to deduce AC (P |x=a ) = ⊥). Consequently, there
is no tuple allowed by c involving (x, a) since P is CDC-consistent (because when P 0 = ⊥,
for every value (y, b), we consider (y, b) ∈
/ P 0 ). We deduce that (x, a) is AC-inconsistent.
This contradiction shows that sCDC is stronger than SAC. To prove strictness, it suffices
to observe that sCDC reasons with both inconsistent values and pairs of values.
Proposition 28. On binary CNs, SAC+CDC is equivalent to sCDC. On non-binary CNs,
SAC+CDC is strictly stronger than sCDC.
198

Second-Order Consistencies

t
a

b

c

e

d

f

g

u
a
b
c
a
d

c
b

v

w

c

b
c

b

a

x

d

a

e
d
c
b
y

a

a

b

c

d

e

z

Figure 12: A CN that is sCDC-consistent but not BiSAC-consistent: (z, c) is not BiSACconsistent, as this value appears in none of the singleton tests AC (P |v=a ),
AC (P |v=b ) and AC (P |v=c ).

Proof. Clearly, SAC+CDC is stronger than sCDC since SAC is stronger than GAC (and
sCDC is GAC+CDC). On the other hand, sCDC is trivially stronger than CDC and we
know from Proposition 27 that sCDC is stronger than SAC on binary CNs. We deduce that,
on binary CNs, sCDC is stronger than SAC+CDC, and that SAC+CDC is equivalent to
sCDC. For non-binary CNs, to show strictness, let us consider the non-binary CN depicted
in Figure 6, but with the binary constraint cyz eliminated. The new obtained CN is GACconsistent, CDC-consistent (since there are no more binary constraints), and thus sCDCconsistent, but not SAC-consistent because GAC (P |y=a ) = ⊥.
Proposition 29. On binary CNs, sCDC is incomparable with BiSAC.
Proof. On the one hand, BiSAC cannot be stronger than sCDC since Figure 9 shows a binary
CN that is BiSAC-consistent (note that every value belongs to at least one solution) but not
CDC-consistent. On the other hand, sCDC cannot be stronger than BiSAC since Figure 12
199

Lecoutre, Cardon, & Vion

C2SAC

2SAC

s2SAC

sC2SAC

3C=DC=PC

s3C=sDC=sPC

BiSAC

CDC

sCDC=SAC+CDC

SAC

PPC

sPPC

MaxRPC

C3C

sCPC=sC3C

AC=2C

CPC
(a) Consistencies restricted to binary CNs.

C2SAC

2SAC

s2SAC

3C

DC

sDC

SAC+CDC

PC

CDC

sCDC

sPC

C3C

PPC

sPPC

CPC

sCPC

SAC
GAC

(b) Consistencies for CNs with constraints of arbitrary arity.

strictly stronger

incomparable

= equivalent

Figure 13: Summary of the relationships between consistencies.
shows a binary CN that is sCDC-consistent but not BiSAC-consistent. We conclude that
sCDC and BiSAC are incomparable.

Some results given in the general case (i.e., for CNs with constraints of arbitrary arity)
also hold when only binary CNs are considered. This is the case for Propositions 6, 10, 11,
12, 13, 16, 17, 22 (except for cases (b) and (c)), 24 and 26 because binary CNs are used in
their proofs. Figure 13 shows the relationships between (strong) second-order consistencies
introduced in this paper, with a focus on binary CNs in Figure 13(a). In Figure 13(b), for
the sake of clarity, s3C, sC3C, and sC2SAC are not inserted.
200

Second-Order Consistencies

5. An Algorithm to Enforce s(C)DC
In this section, we present a general algorithm to enforce strong (conservative) dual consistency. This algorithm is valid for both binary and non-binary CNs. This algorithm is called
sCDC1 when it is used to enforce sCDC, and sDC1 when it is used to enforce sDC. Actually,
on non-binary CNs, our sCDC1 algorithm enforces SAC+CDC, which is strictly stronger
than sCDC. This extra strength comes for free from the exploitation of the singleton checks
used to enforce CDC.
Algorithm 1: sCDC1/sDC1(P ): Boolean
Input/Output: a CN P ; sCDC (SAC+CDC) or sDC is enforced on P
Result: true iff P is strong (conservative) dual-consistent
1
2
3
4
5
6
7
8
9
10
11
12

P ← GAC (P )
if P = ⊥ then return false
x ← first(vars(P ))
marker ← x
repeat
if reviseVariable(P, x) then
P ← GAC (P )
if P = ⊥ then return false
marker ← x

// GAC is initially enforced

// GAC is maintained

x ← nextCircular(x, vars(P ))
until x = marker
return true

Algorithm 1 establishes strong (conservative) dual consistency on a given CN P . The
learnxxx function called at Line 9 of Algorithm 2, which is used by Algorithm 1, is specialized
either to learnpart (Algorithm 3) to enforce sCDC (SAC+CDC on non-binary CNs – we omit
this precision from now on) or to learnf ull (Algorithm 4) to enforce sDC on P . Basically,
Algorithm 1 performs successive singleton checks until a fixed point is reached, and returns
true iff P is strong (conservative) dual-consistent, i.e., iff sCDC (P ) 6= ⊥ (with learnpart ) or
sDC (P ) 6= ⊥ (with learnf ull ). GAC is enforced at line 1, and then a variable is considered
at each turn of the main loop to establish the consistency. first(vars(P )) is the first variable
of P in the lexicographical order, and nextCircular(x, vars(P )) is the variable of P right after
x if any, or first(vars(P )) otherwise. These two functions allow circular iteration over the
variables of P . For example, if vars(P ) = {x, y, z}, then the iteration has the form x, y, z,
x, y, z. . . Of course, it is possible to control the order of variables from one iteration to the
next using some heuristic.
The reviseVariable function (Algorithm 2) revises the given variable x by means of strong
(conservative) dual consistency, i.e., explores all possible inferences with respect to x by
performing singleton checks on values in dom(x). To achieve this, GAC is enforced on
P |x=a for each value a in the domain of x (Line 3). If a is SAC-inconsistent, then a is
removed from the domain of x (Line 5). Otherwise (Lines 7 to 10), for every variable y 6= x
of P with at least one value deleted by constraint propagation, we try to learn nogoods by
201

Lecoutre, Cardon, & Vion

Algorithm 2: reviseVariable(P ,x): Boolean

10

effective ← false
foreach value a ∈ domP (x) do
P 0 ← GAC (P |x=a )
// singleton check on (x, a)
0
if P = ⊥ then
remove a from domP (x)
// SAC-inconsistent value
effective ← true
else
0
foreach variable y in vars(P ) | y 6= x ∧ domP (y) 6= domP (y) do
0
if learnxxx (P, (x, a), y, domP (y) \ domP (y)) then
effective ← true

11

return effective

1
2
3
4
5
6
7
8
9

Algorithm 3: learnpart (P , (x, a), y, Deleted): Boolean
1
2
3
4
5
6
7
8

if ∃cxy ∈ cons(P ) | scp(cxy ) = {x, y} then
conflicts ← ∅
foreach b ∈ Deleted | (a, b) ∈ relP (cxy ) do
conflicts ← conflicts ∪ {(a, b)}

// CDC-inconsistent pair

if conflicts 6= ∅ then
relP (cxy ) ← relP (cxy ) \ conflicts
return true
return false

Algorithm 4: learnf ull (P , (x, a), y, Deleted): Boolean
1
2
3
4
5
6
7

if ∃cxy ∈ cons(P ) | scp(cxy ) = {x, y} then
conflicts ← ∅
foreach b ∈ Deleted | (a, b) ∈ relP (cxy ) do
conflicts ← conflicts ∪ {(a, b)}

// CDC-inconsistent pair

if conflicts 6= ∅ then
relP (cxy ) ← relP (cxy ) \ conflicts
return true

12

else
conflicts ← {(a, b) | b ∈ Deleted}
// DC-inconsistent pairs
Let cxy be a new constraint such that:
• scp(cxy ) = {x, y}
• rel(cxy ) = (dominit (x) × dominit (y)) \ conflicts
cons(P ) ← cons(P ) ∪ {cxy }
return true

13

return false

8
9
10

11

202

Second-Order Consistencies

means of functions learnpart or learnf ull ; the set of values of y deleted by propagation is passed
as last parameter (in practice, using a stack to handle domains and a trailing mechanism,
there is no need to explicitly compute the set of deleted values). The revision is effective for
x if a value or a tuple is deleted (possibly after inserting a new constraint). The Boolean
variable effective is introduced to track revision effectiveness. When the revision of x is
effective, reviseVariable returns true at Line 6 of Algorithm 1, and GAC is re-established
(Line 7). Any domain or relation wipe-out is detected at Line 8. A marker, initialized
with the first variable of vars(P ) (Line 4) and updated whenever inferences are performed
(Line 9), manages termination.
Both Algorithms 3 and 4 discard identified binary nogoods that correspond to CDCinconsistent pairs of values if the constraint cxy exists: every tuple (a, b) such that b is a
deleted value (i.e., b for y is present in P but not in P 0 ) and (a, b) present in relP (cxy ) is
removed from relP (cxy ). When enforcing sDC (with learnf ull ) and when no binary constraint
exists between x and y, a new constraint is created. This constraint enforces the set of
nogoods corresponding to DC-inconsistent pairs of values involving (x, a) and variable y.
The new constraint accepts every pair of values except those that have been identified as
0
conflicts. Notice that we know that there is at least one conflict since domP (y) 6= domP (y)
at line 8 of Algorithm 2.
Proposition 30. Algorithm sCDC1 enforces sCDC on binary CNs and SAC+CDC on
non-binary CNs; Algorithm sDC1 enforces sDC.
Proof. First, any inference performed by Algorithm 2 at Line 5 or by learnpart /learnf ull is
correct: such inferences correspond to clearly identified SAC-inconsistent values and (C)DCinconsistent pairs of values. Any inference directly performed by Algorithm 1 at Lines 1
and 7 is also safe because it corresponds to removing GAC-inconsistent values; remember
that the overall algorithm enforces strong (C)DC, or SAC+CDC (and SAC is stronger than
GAC). The fact that all possible inferences are performed is guaranteed by the fact that
each time a revision is effective, the marker used in Algorithm 1 is updated; see Line 9.
Also, any inference performed with respect to a pair (x, a) has no effect on GAC (P |x=b ),
where b is any other value in the domain of the variable x. Indeed, when b is assigned to x,
all other values in the current domain of x are automatically removed. Combined with the
enforcement of GAC, the assignment of a new value to x makes previous inferences related
to other values of x without any effect. This is the reason why we can iterate over all values
of x at Line 2 of Algorithm 2 in a unique pass.
One pass of Algorithm 1 means calling reviseVariable exactly once per variable.
Proposition 31. One pass of Algorithm 1 has a worst-case time complexity in O(enrdr+1 )
with learnpart (i.e., for sCDC1) and in O(enrdr+1 + n3 d3 ) with learnf ull (i.e., for sDC1).
Proof. The optimal worst-case time complexity of enforcing GAC is O(erdr ) (Mohr &
Masini, 1988). The worst-case time complexity of Lines 8–10 of Algorithm 2 is O(nd)
with both learn methods. Besides,
• with learnpart , no new constraint is inserted in P , the worst-case time complexity of
one pass of Algorithm 1 is O(nd · (erdr + nd)). This reduces to O(enrdr+1 ) with the
203

Lecoutre, Cardon, & Vion

very weak assumption that n ≤ er (otherwise, some variables would not be involved
in any constraint);
• with learnf ull , we have to consider that O(n2 ) additional binary constraints may be
added by the algorithm. So, we obtain O(enrdr+1 + n3 d3 ).
For binary constraints (r = 2, which entails e < n2 ), we obtain:
Corollary 1. For binary CNs, one pass of Algorithm 1 admits a worst-case time complexity
in O(end3 ) with learnpart and in O(n3 d3 ) with learnf ull .
When P is not already s(C)DC-consistent, several passes of Algorithm 1 are necessary.
Thus,
• with learnpart , the number of passes is bounded by O(ed2 ); one tuple being removed at
each new pass. The worst-case time complexity of Algorithm 1 is then in O(e2 nrdr+3 )
(O(e2 nd5 ) for binary CNs);
• with learnf ull , the number of passes is bounded by O(n2 d2 ); one tuple being removed
at each new pass. The resulting worst-case time complexity is then O(en3 rdr+3 +n5 d5 )
(O(n5 d5 ) for binary CNs).
The overall time complexity of Algorithm 1 seems to be rather high but we have observed
that very often a fixed point is quickly reached in practice (i.e., the number of passes
empirically tends to be constant). We can also note that for sDC1 (i.e., Algorithm 1 with
learnf ull ), it is possible to limit the cost of enforcing GAC at each singleton check. Indeed,
when a singleton check on a pair (x, a) is performed, every nogood of size 2 including (x, a)
is identified and recorded in the CN P . This means that just after the last instruction
of each iteration of the foreach loop starting at Line 2 of Algorithm 2, P is such that
after assigning again a to x, applying forward checking (any non-binary version, Bessiere,
Meseguer, Freuder, & Larrosa, 2002) is enough to enforce GAC: we only need to consider
binary constraints involving x and delete values that are not consistent with (x, a). This is
studied in a paper by Lecoutre et al. (2007b).
Finally, it is worthwhile to mention that Algorithm 1 does not require specific data
structure. The only data structures required are those of the underlying (G)AC algorithm(s)
and those for the representation of CNs. If eb denotes the number of binary constraints
in a given CN, then sCDC1 may require O(eb d2 ) additional space to store new nogoods
(if binary constraints were initially given in intension for example). sDC1 require O(n2 d2 )
additional space, which may be a serious drawback for solving certain problems.

6. Experimental Results
To show the practical interest of strong second-order consistencies, and in particular s(C)DC,
we conducted several extensive experiments on Oracle Java 6 VMs running on a cluster of
Intel Xeon 3.0 GHz with 1 GiB of RAM under Linux. To do this, we implemented sCPC,
sCDC and sDC algorithms in our constraint solver AbsCon. These implementations are
sCPC8, directly derived from PC8 (Chmeiss & Jégou, 1998), sCDC1 and sDC1 that correspond to Algorithm 1 with learnpart and learnf ull , respectively. Some refinements of the
204

Second-Order Consistencies

algorithm sDC1, as proposed by Lecoutre et al. (2007b), are also possible but will not be
considered in this paper, mainly because such optimizations are rather marginal (a speedup of 10 % on some problems) with respect to our main concern: showing that enforcing
s(C)DC before search may pay off.
The AbsCon solver also features SAC preprocessing algorithms. We used SAC1 (Bessiere
& Debruyne, 2005) and SAC3 (Lecoutre & Cardon, 2005) algorithms in some experiments.
In AbsCon, the algorithms used to enforce (G)AC are AC3bit+rm (Lecoutre & Vion, 2008)
for binary constraints, GAC3rm (Lecoutre & Hemery, 2007) for non-binary constraints
defined in intension, and STR2 (Lecoutre, 2008) for non-binary constraints in extension.
Besides, when possible, an optimization based on reasoning from the cardinality of conflict
sets (Boussemart, Hemery, Lecoutre, & Sais, 2004b) was used. Of course, SAC1, SAC3,
sCDC1 and sDC1 also benefit from the efficiency of these underlying (G)AC algorithms.
6.1 Preprocessing Performance on Random Problems
We first evaluate the performance of various consistencies and algorithms on random instances. Random CNs are generated using Model B (Gent, MacIntyre, Prosser, Smith,
& Walsh, 2001) to comply to five parameters: the number of variables n, the size of the
domains d, the arity of the constraints r (r = 2 in our experiment), the density δ,7 and the
tightness t (proportion of tuples forbidden by each constraint).
100
AC
sCPC
SAC
sCDC
sDC
Inverse

Density δ [%]

80
60
40
20

20

40

60

80

20

Tightness t [%]

40

60

80

100

Tightness t [%]

(a) n = 50, d = 10

(b) n = 50, d = 50

Figure 14: Phase transition of various consistencies for random binary CNs.
Figure 14 gives some quantitative information about the relative strength of AC, sCPC,
SAC, sCDC and sDC (= sPC) enforced on random binary CNs with n = 50 variables and
d ∈ {10, 50} values per domain. Each plot in the figures represents the position of the
phase transition of a given consistency for the generated instances; there are 50 instances
generated for each (δ, t) pair, and we recall that the phase transition of a consistency φ
occurs when 50 % of the generated instances are detected as unsatisfiable when enforcing
7. The density determines the number of constraints e = δ ·

205

n
r



CPU time [s]

Lecoutre, Cardon, & Vion

sPC8
sDC1
sCPC8
sCDC1

100

50

80
60
40

0

20

10

0
55

5
0
60

65
70
Tightness t

60
65
Tightness t

70

75

(a) d = 50, δ = 50 %

(b) d = 50, δ = 100 %

CPU time [s]

400
300
200

300

100

200

0
40
30
20
10
0
70

100
0
68
72

74
76
Tightness t

78

70
72
Tightness t

74

80

(c) d = 90, δ = 50 %

(d) d = 90, δ = 100 %

Figure 15: Mean CPU time (in seconds) for enforcing various second-order consistencies on
binary random CNs (n = 50).

206

Second-Order Consistencies

φ. Phase transition for inverse consistency is also plotted as a baseline: a CN is inverseconsistent, or (1, n)-consistent, iff each value belongs to at least one solution. The obtained
results show that sCPC is rather close to sCDC on such random instances, except when the
tightness t ranges between 40% and 70%. Surprisingly, the difference between sCDC and
sDC is so weak that it is not even visible on the graphs. Also, we can see that second-order
consistencies become closer to inverse consistency when the size of the domains is lower.
Figure 15 compares the CPU times required to establish sPC, sDC, sCPC and sCDC,
on binary random CNs, using the algorithms sPC8, sDC1, sCPC8 and sCDC1, respectively.
For each value of t, 50 instances have been generated, with either 50 values (topmost figures)
or 90 values (bottommost figures) per variable. Leftmost pictures represent performances
on CNs with density δ = 50 % and rightmost pictures on CNs with complete constraint
graphs. In the latter case, all consistencies are equivalent; this is why only plots for sDC1
and sPC8 are given. The results are quite informative. On the one hand, they illustrate
the very high performance of s(C)DC algorithms with respect to state-of-the-art s(C)PC
algorithms, with a performance gap often over one order of magnitude on dense problems.
On the other hand, the obtained results show that establishing sDC is not much harder
than enforcing sCDC.
6.2 Impact of Second-Order Consistency Preprocessing on MAC
Now, we turn to complete search algorithms. One of the most popular (systematic) search
algorithms to solve CNs is called MAC (Sabin & Freuder, 1994). MAC interleaves inference
and search since at each step of a depth-first exploration with backtracking, (generalized)
arc consistency is maintained.8 At each step of search, MAC uses a variable ordering
heuristic to select the next variable to be instantiated. Two representative variable ordering
heuristics are the dynamic heuristic dom/ddeg (Bessiere & Régin, 1996) and the adaptive heuristic dom/wdeg (Boussemart, Hemery, Lecoutre, & Sais, 2004a). Today, enforcing
(strong) second-order consistencies during search (basically, after each variable assignment)
seems to be unrealistic. However, enforcing such consistencies in a preprocessing stage and
before running MAC is an issue that deserves to be addressed; in this section, we attempt to
provide some insight in this regard. If Φ denotes the consistency enforcing algorithm applied
at preprocessing, this is denoted by Φ-MAC. The experimentation that we have conducted
consists in comparing MAC, SAC1-MAC (and also SAC3-MAC), sCPC8-MAC, sCDC1MAC and sDC1-MAC; from now on called variants of MAC. It allows us to assess the
practical impact of enforcing (strong) second-order consistencies at preprocessing on many
series of structured (i.e., not random) binary and non-binary instances. These series as well
as their description can be found at http://www.cril.fr/~lecoutre/benchmarks.html.
We briefly introduce the main features of these series (listed in alphabetical order).
aim: series of instances with Boolean variables and ternary constraints. Initially, they were
generated as Boolean formulas in DIMACS CNF (Conjunctive Normal Form) format.
bqwh: series of satisfiable balanced quasi-group instances with holes (Gomes & Shmoys,
2002); domains of variables are small (usually, a few values) and constraints are binary.
8. For simplicity, we shall use the acronym MAC whatever the arity of the constraints is.

207

Lecoutre, Cardon, & Vion

composed: series of instances randomly composed of a main (under-constrained) fragment
and some auxiliary fragments (Lecoutre, Boussemart, & Hemery, 2004); there are 10
values per domain and constraints are binary.
driver: series of planning instances with costs converted from WCSP (Weighted CSP) to
CSP; domains of variables are small (usually, a few values) and constraints are binary.
ehi: series of SAT instances converted into CSP using the dual method as described by
Bacchus (2000); there are 7 values per domain and constraints are binary.
langford: series from the (generalized version of the) Langford problem; domains of variables are usually not small (up to 150 values) and constraints are binary.
os-taillard: series of Open-Shop scheduling instances generated by J. Vion from Taillard’s
paper (1993); domains are large and constraints are binary.
primes: series of instances involving prime numbers generated by M. van Dongen; domains
of variables are not small and some constraints are non-binary.
qcp / qwh: series from the Quasi-group Completion Problem (QCP) and the Quasi-group
With Holes problem (QWH); domains are not very large and constraints are binary.
queensKnights: series of academic instances with queens and knights to be put on a chessboard (Boussemart et al., 2004a); some domains are large and constraints are binary.
radar: realistic radar surveillance problems generated following the model of the Swedish
institute of computer science (SICS); domains are small and constraints non-binary.
renault-mod: series of instances generated by K. Stergiou from a Renault Megane configuration problem; domains are diverse and constraints are non-binary.
sadeh: series containing the five sets of job-shop scheduling instances studied by Sadeh &
Fox (1996); domains are large and constraints are binary.
scens11: series containing hard variants of the original scen11 instance from the Radio Link
Frequency Assignment Problem (RLFAP, Cabon et al., 1999); domains are not small
and constraints are binary.
series: series from the all-interval series problem; domains are diverse and constraints are
binary or ternary.
ruler: series from the Golomb Ruler problem; domains are not small and constraints are
binary, ternary or quaternary.
tsp: series generated by R. Szymanek from the Travelling Salesperson problem; domains
are large and constraints are binary or ternary.
Tables 1 and 3 show the results that we have obtained with the different variants of
MAC (the third column is for MAC alone) equipped with the variable ordering heuristics
dom/ddeg and dom/wdeg, respectively. For each series, the number nbs of instances solved
by the five variants of MAC within the alloted time (20 minutes) as well as the total
number nb of instances are given under the form nbs /nb in column #Inst. The mean CPU
times displayed in these two tables is computed from the nbs instances identified for each
series; when a variant solves some additional instances, this is indicated between brackets
preceded by the + symbol. It is interesting to note that the SAC3 algorithm (Lecoutre &
Cardon, 2005) sometimes permits to discover “lucky” solutions during preprocessing due to
its greedy nature. When this happens, we show the increased number of solved instances
behind /. For example, in Table 1, the first row indicates that 12 instances out of a set
208

Second-Order Consistencies

MAC with at preprocessing Φ =
Series

#Inst

–

SAC1/3

sCPC8

sCDC1

aim-100
aim-200
bqwh-15
bqwh-18
composed-25
composed-75
driver
ehi-85
ehi-90
langford-2
langford-3
langford-4
os-taillard-4
os-taillard-5
os-taillard-7
primes-10
primes-15
primes-20
qcp-10
qcp-15
qcp-20
queensKnights
qwh-10
qwh-15
qwh-20
radar-8-24
radar-8-30
radar-9-28
renault-mod
sadeh
scens11
series
ruler
tsp-20
tsp-25

12/24
6/24
100/100
99/100
15/50
3/40
7/7
99/100
94/100
16/24
16/24
14/24
29/30
10/30
6/30
25/32
20/32
20/32
14/15
5/15
0/15
6/18
10/10
10/10
0/15
34/50
35/50
12/50
46/50
16/46
0/11
10/25
17/28
15/15
13/15

100
262
1.77
55.9
121
0.6
44.0
197
251
1.44
49.6
(+1) 25.0
(+1) 13.6
(+5) 13.1
(+2) 64.5
45.3
(+2) 3.01
3.60
(+1) 18.1
(+4) 212
–
97.9
0.77
5.95
–
(+4) 12.9
(+1) 3.5
90.7
(+1) 36.4
(+11) 4.52
–
44.5
(+1) 77.3
6.63
(+2) 84.8

(+2) 65.6
(+7) 263

1.63
54.6
(+35) 0.89
(+37) 0.73
11.4
(+1) 1.47
(+6) 1.46
1.73
47.4
29.0
(+1) 1.85
(+9/10) 6.58
(+2/3) 75.2
92.4
12.6
(+2) 76.6
(+1) 18.6
(+4) 214
–
(+11) 0.74
0.76
11.7
–
(+5) 20.9
(+4) 35.1
(+5) 1.58
(+2) 4.05
(+11/22) 5.71
–
45.9
(+1) 70.8
8.27
(+2) 72.2

106
273
1.37
(+1) 33.3
(+35) 0.97
(+37) 0.8
9.74
(+1) 1.84
(+6) 1.88
36.9
38.9
(+1) 38.0
(+1) 5.79
(+9) 103
247
45.0
(+2) 3.09
3.77
(+1) 19.8
(+3) 172
–
(+6) 0.82
0.93
8.56
(+3) –
(+4) 13.7
(+1) 3.75
93.5
(+1) 36.8
(+11) 10.1
(+3) –
47.6
(+1) 70.1
8.08
(+2) 97.1

(+2) 68.1
(+7) 265

1.19
29.3
(+35) 1.1
(+37) 0.8
8.48
(+1) 1.32
(+6) 1.3
3.77
25.7
(+1) 21.8
(+1) 2.15
(+8) 65.2
77.0
91.1
15.8
(+2) 87.9
(+1) 18.4
(+3) 168
–
(+11) 0.64
0.82
7.2
(+4) –
(+5) 14.2
(+3) 3.6
(+5) 1.35
(+2) 9.30
(+12) 4.10
(+3) –
43.6
88.9
19.6
(+2) 63.9

54.2
16.4
3.5
(+1) 26.3
(+35) 1.2
(+37) 1.1
53.9
(+1) 1.41
(+6) 1.4
3.29
26.0
(+1) 21.7
3.10
(+10) 94.8
(+3) 118
(+1) 86.2
(+2) 13.9
(+2) 73.4
18.4
(+1) 129
–
(+11) 0.69
1.10
3.34
(+8) –
(+4) 3.09
(+4) 20.2
(+5) 1.33
(+1) 20.6
(+13) 78.8
(+3) –
(+15) 0.83
(+1) 62.4
63.7
223

+36

+148/161

+129

+152

+178

(+1)

sDC1
(+3)
(+10)

Table 1: Mean cpu time (in seconds) to solve instances of different series (time-out of 1,200 s
per instance) with Φ-MAC-dom/ddeg.

209

Lecoutre, Cardon, & Vion

MAC with at preprocessing Φ =
Instances
aim-100-1-6-sat

bqwh-18-141-30

driver-02c-sat

driver-09-sat

langford-4-15

os-taillard-5-95-2

primes-15-20-2-3

primes-20-20-3-3

scen11-f12

renault-mod-8

series-12

–
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t

sCPC8

50.9 (0)
25 M
655 K
0-0
180 (0.01)
31 M
1,682 K
6-0
2.34 (0.03)
35 M
5,278
64 - 0
222 (0.05)
57 M
215 K
162 - 0
254 (0.03)
32 M
1,056 K
1,620 - 0
> 1,200

46.6 (0.01)
25 M
655 K
0-0
66.2 (0.11)
31 M
643 K
6 - 182
3.68 (1.23)
43 M
3,277
64 - 9,439
52.7 (20.9)
135 M
14,284
162 - 63,163
287 (79.2)
52 M
197 K
1,620 - 517 K
> 1,200

1.81 (0.32)
29 M
122
657 - 0
> 1,200

1.77 (0.27)
29 M
122
657 - 0
> 1,200

>1200

15.3 (11.5)
53 M
240
6,324 - 419 K
24.5 (0.04)
67 M
179 K
22 - 0
12.9 (0.10)
29 M
106 K
0-0

24.0 (0.05)
67 M
179 K
22 - 0
12.2 (0.01)
29 M
106 K
0-0

sCDC1
0.38 (0.03)
25 M
100
100 - 0
92.2 (0.20)
31 M
845 K
9 - 207
3.66 (2.33)
39 M
988
343 - 5,336
10.3 (8.2)
73 M
650
2,175 - 22,590
215 (7.61)
40 M
197 K
1,620 - 517 K
9.06 (6.56)
32 M
3,204
570 - 384 K
16.5 (15.4)
29 M
104
766 - 0
188 (178)
29 M
175
602 - 0
7.95 (4.84)
49 M
18
6,768 - 306 K
8.05 (6.07)
67 M
0
95 - 0
12.0 (0.26)
29 M
106 K
0-0

sDC1
0.47 (0.12)
25 M
100
100 - 231 (200)
49.5 (0.31)
35 M
443 K
9 - 585 (296)
15.0 (13.3)
66 M
505
343 - 31 K (7 K)
61.1 (58.6)
217 M
650
2 K - 117 K (19 K)
215 (7.45)
40 M
197 K
1,620 - 517 K (0)
15.1 (12.1)
40 M
2,562
570 - 1 M (175)
22.1 (21.4)
29 M
101
766 - 2,491 (46)
153 (141)
29 M
181
653 - 18,804 (96)
13.7 (10.4)
96 M
18
7 K - 351 K (3 K)
7.84 (6.8)
101 M
0
72 - 11 K (1,719)
0.81 (0.33)
29 M
23
0 - 310 (110)

Table 2: Detailed results on various instances (time-out of 1,200 s per instance) with ΦMAC-dom/ddeg.
210

Second-Order Consistencies

MAC with at preprocessing Φ =
Series
aim-100
aim-200
bqwh-15
bqwh-18
composed-25
composed-75
driver
ehi-85
ehi-90
langford-2
langford-3
langford-4
os-taillard-4
os-taillard-5
os-taillard-7
primes-10
primes-15
primes-20
qcp-10
qcp-15
qcp-20
queensKnight
qwh-10
qwh-15
qwh-20
radar-8-24
radar-8-30
radar-9-28
renault-mod
sadeh
scens11
series
ruler
tsp-20
tsp-25

#Inst

–

SAC1/3

24/24
24/24
100/100
100/100
50/50
40/40
7/7
100/100
100/100
16/24
16/24
14/24
30/30
28/30
11/30
26/32
22/32
21/32
15/15
15/15
0/15
7/18
10/10
10/10
10/10
50/50
50/50
27/50
50/50
37/46
9/12
10/25
19/28
15/15
13/15

0.97
6.86
0.99
5.37
0.70
1.02
3.14
1.96
1.97
1.47
56.2
26.5
1.08
40.5
(+3) 70.0
71.1
(+1) 2.43
(+2) 13.4
0.71
43.1
(+3) –
(+1) 2.57
0.69
1.67
152
26.6
1.43
(+1) 90.2
1.65
(+1) 13.1
95.4
(+1) 12.6
(+2) 16.2
3.91
(+2) 20.9
+17

sCPC8

sCDC1

sDC1

0.90
1.94
1.12
3.87
0.69
0.86
10.7
1.44
1.57
1.75
57.6
(+0/1) 29.1
1.74
(+1) 65.0
(+3/9) 76.5
(+0/2) 73.6
(+2/1) 27.2
(+1/2) 82.9
0.84
16.8
(+3/4) –
(+10) 0.83
0.81
2.27
125
27.6
37.3
(+2/10) 37.9
2.72
(+1/7) 25.1
(+0/1) 54.6
(+0/1) 1.76
(+1/0) 46.3
6.09
(+2) 28.4

1.05
7.13
1.05
4.04
0.82
0.94
8.43
1.87
2.00
39.8
46.4
44.0
5.66
52.0
(+1) 266
71.9
(+1) 2.34
(+2) 13.8
0.88
62.8
(+3) –
(+5) 0.88
1.41
2.54
96.0
26.5
1.55
(+1) 88.9
1.88
(+1) 18.8
109
(+1) 13.5
(+2) 24.0
4.48
(+2) 25.4

1.01
3.33
0.98
3.93
0.84
1.02
8.2
1.30
1.40
3.36
34.6
27.5
1.89
(+2) 45.5
(+1) 103
78.5
(+2) 16.2
(+1) 91.0
0.77
60.8
(+4) –
(+10) 0.64
0.76
1.72
73.2
27.9
2.11
(+2) 71.5
7.94
(+3) 33.2
81.6
16.3
(+1) 66.0
18.1
(+2) 59.0

0.84
1.25
1.39
3.71
0.82
0.96
55.7
1.31
1.36
3.32
32.9
28.3
2.57
(+2) 48.4
(+4) 145
97.6
(+2) 14.5
(+2) 75.9
0.91
31.1
(+3) –
(+10) 0.66
1.01
1.64
41.4
26.4
2.10
(+2) 67.5
11.8
(+6) 48.8
172
(+15) 0.79
(+1) 41.6
69.1
271

+26/51

+19

+28

+47

Table 3: Mean cpu time (in seconds) to solve instances of different series (time-out of 1,200 s
per instance) with Φ-MAC-dom/wdeg.

211

Lecoutre, Cardon, & Vion

MAC with preprocessing Φ =
Instances
e0ddr1-4

e0ddr1-5

qcp-15-120-2

qcp-20-187-11

scen11-f8

scen11-f6

series-14

series-25

tsp-20-75

os-taill-7-100-8

os-taill-7-105-9

cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t
cpu (pcpu)
mem
nodes
del v - del t

–
1.35 (0.01)
32 M
50
0-0
125 (0.01)
32 M
1,245 K
0-0
93 (0.04)
32 M
955 K
1,276 - 0
2.33 (0.15)
37 M
4,898
2,913 - 0
6.93 (0.06)
37 M
15,640
4,992 - 0
37.2 (0.05)
37 M
204 K
3,660 - 0
93.2 (0.01)
29 M
689 K
0-0
> 1,200

sCPC8
5.25 (4.0)
32 M
50
0-0
121 (4.17)
32 M
1,245 K
0-0
34.8 (0.36)
36 M
308 K
1,276 - 302
> 1,200

31.2 (26.5)
57 M
2,214
4,992 - 365 K
74.8 (33.9)
57 M
167 K
3,660 - 415 K
101 (0.16)
29 M
689 K
0-0
> 1,200

sCDC1
3.05 (1.8)
32 M
50
0 - 8,546
3.19 (1.94)
32 M
75
0 - 7,006
36.0 (0.33)
36 M
333 K
1,282 - 177
2.4 (0.58)
45 M
3,435
2,930 - 325
8.39 (4.6)
53 M
2,214
4,992 - 366 K
45.9 (4.47)
53 M
167 K
3,660 - 416 K
125 (0.2)
29 M
914 K
0-0
> 1,200

3.32 (0.13)
35 M
6,926
21 K - 0
165 (0.01)
49 M
667 K
0-0
9.43 (0.01)
46 M
4,210
0-0

4.04 (0.36)
51 M
6,926
21 K - 0
378 (361)
53 M
42,499
0 - 58
336 (256)
50 M
179 K
0 - 4,328

26.9 (25.7)
47 M
1,631
22 K - 27 K
24.8 (8.4)
49 M
42,499
0 - 58
86.2 (8.4)
50 M
179 K
0 - 4,328

sDC1
8.57 (7.3)
60 M
50
0 - 531 K (927)
9.08 (7.7)
60 M
56
0 - 529 K (922)
2.41 (0.45)
40 M
706
1,282 - 418 (189)
2.66 (0.98)
81 M
2,099
2,930 - 1,091 (583)
12.3 (6.85)
89 M
6,106
4,992 - 389 K (757)
74.4 (6.46)
89 M
260 K
3,660 - 443 K (757)
0.91 (0.41)
29 M
27
0 - 444 (156)
3.16 (2.34)
41 M
49
0 - 1,610 (552)
90.0 (81.3)
197 M
2,064
22 K - 895 K (1.4 K)
31.2 (8.8)
53 M
51,403
0 - 60 (1)
16.2 (9.4)
50 M
49
0 - 11,072 (20)

Table 4: Detailed results on various instances (time-out of 1,200 s per instance) with ΦMAC-dom/wdeg.
212

Second-Order Consistencies

of 24 instances from series aim-200 have been solved by each of the five variants of MAC.
sDC1-MAC solves these 12 instances with a mean CPU time computed at 54.2 seconds,
and additionally solves 3 other instances of the series (within 20 minutes). Note that for
each series, the mean CPU time of the best variant of MAC is printed in bold face: the
best variant is the one that solves the highest number of instances within 20 minutes and
in case of equality, the one that has the smallest mean CPU time.
Tables 2 and 4 provide details on solving various instances when enforcing second-order
consistencies at preprocessing. For each problem instance, the total CPU time to solve it is
given (this is > 1,200 when the instance cannot be solved within 20 minutes) as well as the
time taken to enforce the consistency at preprocessing (pcpu between brackets). Additional
information concern the memory requirement (expressed in MiB), the number of explored
nodes and the number of values and tuples deleted at preprocessing (del v – del t). For sDC1,
we also put the number of new binary constraints between brackets. For example, in Table
2, for solving the instance aim-100-1-6-sat, sDC1-MAC needs 0.47 s (0.12 s at preprocessing),
25 MiB of memory, explores 100 nodes, deletes 100 values and 231 tuples at preprocessing
(while adding 200 new binary constraints).
Table 1 clearly shows that when the heuristic dom/ddeg is used, there is a real interest of
making a strong propagation effort during preprocessing on many problems. Although MAC
remains the most efficient approach on a few series (e.g., langford-2 and tsp-20), sCDC1-MAC
and sDC1-MAC are proved here to be more robust than MAC. For example, on series aim200, radar-9-28 and series, sDC1-MAC largely outperforms MAC. Overall, sDC1-MAC solves
178 − 36 = 142 more instances than MAC. sCDC1-MAC and SAC1/3-MAC are comparable
in terms of the number of solved instances. However, on some series (e.g., langford-3 and
bqwh-18), sCDC1-MAC is clearly better than SAC1/3-MAC, even if the reverse is true for
some other series (e.g., renault-mod and sadeh). It is also interesting to note that sCPC8MAC is almost always outperformed by sCDC1-MAC and sDC1-MAC. This is an expected
result, as sCPC is weaker than sCDC (and a fortiori sDC), and s(C)DC algorithms benefit
of underlying highly optimized (G)AC algorithms. On some series, adding new binary
constraints in order to collect all identified nogoods of size 2 may be counterproductive.
The conservative consistency sCDC is then a better option than sDC. For example, this is
the case for driver and tsp-25 series.
The results given in Table 2 show the dramatic effect of a strong consistency at preprocessing on some instances. For example, after enforcing sCDC or sDC, MAC is able to
directly find a solution to aim-100-1-6-sat. This is also the case for sDC1-MAC on series-12.
However, sDC1 may require a significant amount of additional memory (to record new constraints). This may explain the relative inefficiency of sDC1-MAC on instances driver-02csat, driver-09-sat and scen11-f12, compared to sCDC1-MAC. Note that for renault-mod-8,
sCDC1 and sDC1 alone are sufficient to detect unsatisfiability (the number of visited nodes
is 0). However, the unsatisfiability of this instance is proved differently (i.e., following different propagation paths), which can explain why 95 and 72 values are respectively deleted
when sCDC and sDC are enforced.
Table 3 confirms that MAC equipped with the heuristic dom/wdeg is far more robust
than dom/ddeg, as initially claimed by Boussemart et al. (2004a). As a result, on some series, enforcing a strong consistency (i.e. a consistency stronger than GAC) at preprocessing
has a more limited impact. Nevertheless, overall, sDC1-MAC still increases the robustness
213

Lecoutre, Cardon, & Vion

of the solver. Concerning SAC3-MAC, it is important to note that its good behaviour is due
to its opportunistic mechanism of finding solutions, and not to its inference capability as
shown by the results of SAC1-MAC: SAC1-MAC solves 26 additional instances against 51
for SAC3-MAC. If sCDC1-MAC is outperformed by sDC1-MAC, note that on some series,
it remains a good option because it does not perturbate the heuristic by adding new constraints (e.g., compare the number of nodes for solving instances scen11-f6 and scen11-f8 in
Table 4) and its overhead at preprocessing is limited (compare for example the preprocessing
time of sCDC1 and sDC1 for e0ddr1-4 and tsp-20-75 in Table 4).
Which lessons can be learned from this experimental study? A first one is that enforcing s(C)DC is most of the time far more efficient than enforcing s(C)PC. As our first
experimental attempts (not presented in this paper) to enforce s(C)2SAC show that this is
a very expensive approach, we do believe that s(C)DC is, for now, the best second-order
consistency to be enforced at preprocessing.
A second unsurprising lesson is that there are problems for which enforcing s(C)DC is not
cost-effective. Basically, the cost of enforcing s(C)DC mainly depends on the total number
of values to be tested9 as well as on the time complexity of the underlying GAC algorithm(s).
For non-binary constraints, the time complexity of enforcing GAC usually increases with
the arity of the constraints and for extensional constraints, the time complexity of enforcing
GAC usually depends on the size of the tables. As a consequence, when a problem involves
constraints of large arity and/or large tables, enforcing s(C)DC may become penalizing.
This is the case for the renault-mod, tsp-20 and tsp-25 series in Table 3.
A third lesson is that enforcing sDC at preprocessing tends to make the MAC algorithm
more robust. To confirm this, Figure 16 shows two cactus-shaped plots that give some
insight on the relative performance of the variants of MAC on the whole range of tested
problem instances. These plots show that establishing sDC before searching for a solution
using MAC enhances the robustness of the solver, especially when the variable ordering
heuristic fails. With the dom/ddeg variable ordering heuristic, plain MAC is almost always
worse than MAC with some second-order consistency established during the preprocessing.
With the better dom/wdeg variable ordering heuristic, enforcing a second-order consistency
is interesting for the hardest problems, those that require more than 50 seconds to be solved.
A zoom in Figure 16(b) on MAC versus sDC1-MAC shows how the trend is reversed: after
50 seconds, sDC1-MAC solves more instances than MAC.
We conclude this section with an experiment on the hardest instances from the RLFAP
series scens11 (see e.g., the results at http://www.cril.fr/{CPAI06,CPAI08,CPAI09}).
Using the variable ordering heuristic dom/wdeg, we ran plain MAC, MAC with the symmetrybreaking method described by Lecoutre & Tabary (2009, denoted MACSB here), s(C)DCMAC, and finally MAC with both inference mechanisms. Enforcing sCDC permits to reduce
the size of the search tree developed by MAC on these instances, but for sDC, this is less
obvious (the added constraints perturbate the heuristic), as shown by Table 5. Interestingly, the joint use of the symmetry-breaking method reveals to be quite efficient here.
Using both inference mechanisms, the unsatisfiability of all instances can be proved without any search effort (0 nodes in the sCDC1-MACSB and sDC1-MACSB columns). Note
that SAC-MACSB builds a search tree for these problems, and it is not even possible to solve
9. For example, we discarded the fapp series of problem instances, for which consistencies based on singleton
checks are clearly not adapted because of the huge number of values.

214

Second-Order Consistencies

1,200
MAC
sCPC8-MAC
SAC1-MAC
sCDC1-MAC
SAC3-MAC
sDC1-MAC

CPU time [s]

1,000
800
600
400
200
0
500

550

600

650

700

750

800

850

900

950

1,000 1,050 1,100

950

1,000 1,050 1,100

Number of solved instances
(a) with dom/ddeg

1,200

CPU time [s]

1,000
800
600

100
80
60
40
20

400

0
900

950

1,000

1,050

200
0
500

550

600

650

700

750

800

850

900

Number of solved instances
(b) with dom/wdeg

Figure 16: Number of instances (out of the full set of 1,237 instances) that can be solved
within a given amount of CPU time with variants of MAC (e.g., 870 instances
can be solved within 1,200 s by MAC-dom/ddeg).

215

Lecoutre, Cardon, & Vion

the instance scen11-f1 with SAC1-MACSB (or SAC3-MACSB ) equipped with the heuristic
dom/ddeg, within 20 minutes.
sCDC1Instance
scen11-f1
scen11-f2
scen11-f3
scen11-f4
scen11-f6
scen11-f8

MAC
cpu
nodes
cpu
nodes
cpu
nodes
cpu
nodes
cpu
nodes
cpu
nodes

> 1,200
> 1,200
> 1,200
542
3,381 K
60.1
348 K
6.0
14,077

MACSB

MAC

52.8
267 K
24.3
108 K
11.0
35,979
7.9
11,246
3.8
2,226
3.8
1,847

> 1,200
> 1,200
> 1,200
438
2,095 K
41.9
163 K
9.94
5,021

sDC1-

MACSB
10.0
0
7.6
0
5.8
0
6.0
0
4.3
0
5.3
0

MAC
> 1,200
> 1,200
> 1,200
808
3,362 K
65.1
215 K
11.1
4,518

MACSB
12.3
0
9.7
0
6.8
0
5.9
0
4.6
0
4.3
0

Table 5: Cost of running (Φ-)MAC-dom/wdeg on the hardest instances of the RLFAP series
scens11. MACSB is MAC with an automatic global symmetry-breaking method.

7. Conclusion
This paper is intended to give a better picture of second-order consistencies. For this purpose, we have studied the theoretical relationships existing between four basic second-order
consistencies (and their variants), and we have shown that some of them can be reasonably enforced before search. However, in the next generation of constraint solvers, tractable
classes of CSP instances will certainly have to be identified and exploited during search,
in order to close, for example, certain nodes of the search tree in polynomial time. Because several theoretical results relate global consistency to second-order consistencies (e.g.,
strong 3-consistency), this should increase the importance of second-order consistencies.
In practical terms, there are some advantages of using (conservative) dual consistency.
Algorithms to enforce strong (C)DC are rather easy to implement, and made efficient because of highly optimized underlying GAC algorithms. Used at preprocessing, they reveal to
improve the robustness of a constraint solver on several hard structured problems. (C)2SAC
is stronger than (C)DC, but a naive approach for establishing it requires several passes of
O(n2 d2 ) enforcements of GAC, which makes it ineffective. A perspective of this work is to
devise efficient algorithms for (C)2SAC that could be competitive with (C)DC ones.
Finally, as multi-core processors become increasingly common, parallel constraint solving should become more and more useful. In the near future, we may imagine that strong
second-order consistencies could be used as basic components (with strong inference capabilities) of parallel solvers.
216

Second-Order Consistencies

Acknowledgments
This paper is an extended revised version of earlier works (Lecoutre et al., 2007a, 2007b).
We would like to thank the anonymous reviewers for their constructive remarks.

References
Allen, J. (1983). Maintaining knowledge about temporal intervals. Communications of the
ACM, 26 (11), 832–843.
Apt, K. (1999). The essence of constraint propagation. Theoretical Computer Science,
221 (1-2), 179–210.
Apt, K. (2003). Principles of Constraint Programming. Cambridge University Press.
Bacchus, F. (2000). Extending Forward Checking. In Proceedings of CP’00, pp. 35–51.
Berlandier, P. (1995). Improving domain filtering using restricted path consistency. In
Proceedings of IEEE-CAIA’95.
Bessiere, C. (2006). Constraint propagation. In Handbook of Constraint Programming,
chap. 3. Elsevier.
Bessiere, C., Coletta, R., & Petit, T. (2005). Apprentissage de contraintes globales implicites. In Proceedings of JFPC’05, pp. 249–258.
Bessiere, C., & Debruyne, R. (2005). Optimal and suboptimal singleton arc consistency
algorithms. In Proceedings of IJCAI’05, pp. 54–59.
Bessiere, C., & Debruyne, R. (2008). Theoretical analysis of singleton arc consistency and
its extensions. Artificial Intelligence, 172 (1), 29–41.
Bessiere, C., Meseguer, P., Freuder, E., & Larrosa, J. (2002). On Forward Checking for
non-binary constraint satisfaction. Artificial Intelligence, 141, 205–224.
Bessiere, C., & Régin, J. (1996). MAC and combined heuristics: two reasons to forsake FC
(and CBJ?) on hard problems. In Proceedings of CP’96, pp. 61–75.
Bessiere, C., Stergiou, K., & Walsh, T. (2008). Domain filtering consistencies for non-binary
constraints. Artificial Intelligence, 72 (6-7), 800–822.
Bliek, C., & Sam-Haroud, D. (1999). Path consistency on triangulated constraint graphs.
In Proceedings of IJCAI’99, pp. 456–461.
Boussemart, F., Hemery, F., Lecoutre, C., & Sais, L. (2004a). Boosting systematic search
by weighting constraints. In Proceedings of ECAI’04, pp. 146–150.
Boussemart, F., Hemery, F., Lecoutre, C., & Sais, L. (2004b). Support inference for generic
filtering. In Proceedings of CP’04, pp. 721–725.
Cabon, B., de Givry, S., Lobjois, L., Schiex, T., & Warners, J. (1999). Radio Link Frequency
Assignment. Constraints, 4 (1), 79–89.
Chmeiss, A., & Jégou, P. (1998). Efficient path-consistency propagation. International
Journal on Artificial Intelligence Tools, 7 (2), 121–142.
Cooper, M., Cohen, D., & Jeavons, P. (1994). Characterising tractable constraints. Artificial
Intelligence, 65, 347–361.
217

Lecoutre, Cardon, & Vion

David, P. (1995). Using pivot consistency to decompose and solve functional CSPs. Journal
of Artificial Intelligence Research, 2, 447–474.
Debruyne, R. (1998). Consistances locales pour les problÃĺmes de satisfaction de contraintes
de grande taille. Ph.D. thesis, Universite Montpellier II.
Debruyne, R. (1999). A strong local consistency for constraint satisfaction. In Proceedings
of ICTAI’99, pp. 202–209.
Debruyne, R., & Bessiere, C. (1997a). From restricted path consistency to max-restricted
path consistency. In Proceedings of CP’97, pp. 312–326.
Debruyne, R., & Bessiere, C. (1997b). Some practical filtering techniques for the constraint
satisfaction problem. In Proceedings of IJCAI’97, pp. 412–417.
Debruyne, R., & Bessiere, C. (2001). Domain filtering consistencies. Journal of Artificial
Intelligence Research, 14, 205–230.
Dechter, R. (1992). From local to global consistency. Artificial Intelligence, 55 (1), 87–108.
Dechter, R. (2003). Constraint processing. Morgan Kaufmann.
Dechter, R., & Pearl, J. (1988). Network-based heuristics for constraint satisfaction problems. Artificial Intelligence, 34 (1), 1–38.
Freuder, E. (1978). Synthesizing constraint expressions. Communication of the ACM,
21 (11), 958–965.
Freuder, E. (1982). A sufficient condition for backtrack-free search. Journal of the ACM,
29 (1), 24–32.
Gent, I., MacIntyre, E., Prosser, P., Smith, B., & Walsh, T. (2001). Random constraint
satisfaction: flaws and structure. Constraints, 6 (4), 345–372.
Gomes, C., & Shmoys, D. (2002). Completing quasigroups or latin squares: a structured
graph coloring problem. In Proceedings of Computational Symposium on Graph Coloring and Generalization.
Green, M., & Cohen, D. (2008). Domain permutation reduction for constraint satisfaction
problems. Artificial Intelligence, 172, 1094–1118.
Jégou, P. (1993). Decomposition of domains based on the micro-structure of finite
constraint-satisfaction problems. In Proceedings of AAAI’93, pp. 731–736.
Katsirelos, G., & Bacchus, F. (2003). Unrestricted nogood recording in CSP search. In
Proceedings of CP’03, pp. 873–877.
Lecoutre, C. (2008). Optimization of simple tabular reduction for table constraints. In
Proceedings of CP’08, pp. 128–143.
Lecoutre, C. (2009). Constraint networks: techniques and algorithms. ISTE/Wiley.
Lecoutre, C., Boussemart, F., & Hemery, F. (2004). Backjump-based techniques versus
conflict-directed heuristics. In Proceedings of ICTAI’04, pp. 549–557.
Lecoutre, C., & Cardon, S. (2005). A greedy approach to establish singleton arc consistency.
In Proceedings of IJCAI’05, pp. 199–204.
218

Second-Order Consistencies

Lecoutre, C., Cardon, S., & Vion, J. (2007a). Conservative dual consistency. In Proceedings
of AAAI’07, pp. 237–242.
Lecoutre, C., Cardon, S., & Vion, J. (2007b). Path consistency by dual consistency. In
Proceedings of CP’07, pp. 438–452.
Lecoutre, C., & Hemery, F. (2007). A study of residual supports in arc consistency. In
Proceedings of IJCAI’07, pp. 125–130.
Lecoutre, C., & Tabary, S. (2009). Lightweight detection of variable symmetries for constraint satisfaction. In Proceedings of ICTAI’09, pp. 193–197.
Lecoutre, C., & Vion, J. (2008). Enforcing arc consistency using bitwise operations. Constraint Programming Letters, 2, 21–35.
Mackworth, A. (1977). Consistency in networks of relations. Artificial Intelligence, 8 (1),
99–118.
McGregor, J. (1979). Relational consistency algorithms and their application in finding
subgraph and graph isomorphisms. Information Sciences, 19, 229–250.
Mohr, R., & Masini, G. (1988). Good old discrete relaxation. In Proceedings of ECAI’88,
pp. 651–656.
Montanari, U. (1974). Network of constraints : Fundamental properties and applications to
picture processing. Information Science, 7, 95–132.
Montanari, U., & Rossi, F. (1991). Constraint relaxation may be perfect. Artificial Intelligence, 48 (2), 143–170.
Prosser, P., Stergiou, K., & Walsh, T. (2000). Singleton consistencies. In Proceedings of
CP’00, pp. 353–368.
Sabin, D., & Freuder, E. (1994). Contradicting conventional wisdom in constraint satisfaction. In Proceedings of CP’94, pp. 10–20.
Sadeh, N., & Fox, M. (1996). Variable and value ordering heuristics for the job shop
scheduling constraint satisfaction problem. Artificial Intelligence, 86, 1–41.
Schiex, T., & Verfaillie, G. (1994). Nogood recording for static and dynamic constraint
satisfaction problems. International Journal of Artificial Intelligence Tools, 3 (2), 187–
207.
Stergiou, K., & Walsh, T. (2006). Inverse consistencies for non-binary constraints. In
Proceedings of ECAI’06, pp. 153–157.
Taillard, E. (1993). Benchmarks for basic scheduling problems. European journal of operations research, 64, 278–295.
Tsang, E. (1993). Foundations of constraint satisfaction. Academic Press.
van Beek, P. (1992). On the minimality and decomposability of constraint networks. In
Proceedings of AAAI’92, pp. 447–452.
Zhang, Y., & Yap, R. (2006). Set intersection and consistency in constraint networks.
Journal of Artificial Intelligence Research, 27, 441–464.

219

Journal of Artificial Intelligence Research 40 (2011) 415–468

Submitted 9/10; published 2/11

On-line Planning and Scheduling:
An Application to Controlling Modular Printers
Wheeler Ruml

ruml at cs.unh.edu

Department of Computer Science
University of New Hampshire
33 Academic Way
Durham, NH 03824 USA

Minh Binh Do
Rong Zhou
Markus P. J. Fromherz

minhdo at parc.com
rzhou at parc.com
fromherz at parc.com

Palo Alto Research Center
3333 Coyote Hill Road
Palo Alto, CA 94304 USA

Abstract
We present a case study of artificial intelligence techniques applied to the control of
production printing equipment. Like many other real-world applications, this complex domain requires high-speed autonomous decision-making and robust continual operation. To
our knowledge, this work represents the first successful industrial application of embedded
domain-independent temporal planning. Our system handles execution failures and multiobjective preferences. At its heart is an on-line algorithm that combines techniques from
state-space planning and partial-order scheduling. We suggest that this general architecture may prove useful in other applications as more intelligent systems operate in continual,
on-line settings. Our system has been used to drive several commercial prototypes and
has enabled a new product architecture for our industrial partner. When compared with
state-of-the-art off-line planners, our system is hundreds of times faster and often finds
better plans. Our experience demonstrates that domain-independent AI planning based on
heuristic search can flexibly handle time, resources, replanning, and multiple objectives in
a high-speed practical application without requiring hand-coded control knowledge.

1. Introduction
It is a sustaining goal of artificial intelligence to develop techniques enabling autonomous
agents to robustly achieve multiple interacting goals in a dynamic environment. This goal
is not just intellectually attractive. It also happens to align perfectly with the needs of
many commercial manufacturing plants. In this paper, we focus on one particular manufacturing setting: high-speed digital production printing systems. These large machines use
xerography to print the requested images on individual sheets of paper. Unlike traditional
continuous-feed offset presses, digital printers can treat each sheet differently: feeding different types and sizes of media, printing different kinds of images, and performing different
preparatory and finishing operations. Often, a single integrated machine can transform
blank sheets into a complete document, such as a bound book or a folded bill in a sealed
envelope. It is sometimes even possible to process different kinds of jobs simultaneously on
c
2011
AI Access Foundation. All rights reserved.

Ruml, Do, Zhou, & Fromherz

the same equipment. A printer controller must plan quickly and reliably; otherwise expensive human intervention will be required. Designing a high-performance yet cost-effective
controller for such machines is made more difficult by the current trend towards increased
modularity, in which each customer’s system is unique and includes only those components
that are most appropriate for their needs. We have been working closely with the Xerox
Corporation to explore architectures in which printing systems can be composed of literally hundreds of modules, possibly including multiple specialized printing modules, working
together at high speed.
In this paper, we demonstrate how techniques from artificial intelligence can be used to
control such machines. Requests for print jobs become goals for the system to achieve, the
various actuators and mechanisms in the machine become actions and resources to be used
in achieving these goals, and sensors provide feedback on action execution and the state
of the system. To provide high productivity (and thus high return on investment for the
equipment owner), the planning and control techniques must be fast and produce optimal
or near-optimal plans. To reduce the need for operator oversight and to allow the use of
very complex mechanisms, the system must be as autonomous and autonomic as possible.
Because operators can make mistakes and even highly-engineered system modules can fail,
the system must cope with execution failure and unexpected events. And because the
system must work with legacy modules in order to be commercially viable, its architecture
must tolerate components that are out of its direct control.
To meet these requirements, we present a novel architecture for on-line planning, execution, and replanning that synthesizes techniques from state-space planning (Ghallab, Nau,
& Traverso, 2004) and partial-order scheduling (Smith & Cheng, 1993). We develop new
heuristic evaluation functions for temporal planning that incorporate some of the effects of
resource constraints. Although domain-independent AI planning is often regarded as too
expensive for use in a soft real-time setting, our system achieves good performance without
any hand-coded control rules, despite the additional requirements of reasoning about temporal actions and resources. By avoiding domain-dependent search control knowledge, it
becomes possible to use the same planner to run very different printing systems at full productivity. The success of our system has enabled a new modular product architecture that
can span multiple markets. Much as previous work brought constraint-based scheduling
into daily use in print shops and offices world-wide (Fromherz, Saraswat, & Bobrow, 1999;
Fromherz, Bobrow, & de Kleer, 2003), our work can bring domain-independent temporal
planning into continual widespread use by everyday people. Our approach is practical and
efficient, and it showcases the flexibility inherent in viewing planning as heuristic search.
After discussing the application context in more detail, we will present an overview of
our system, followed by detailed discussion of its major aspects: nominal planning, exception handling, and multiple objectives. As we go, we will present empirical measurements
demonstrating that large printing systems can be controlled by our system while meeting
our real-time requirements. In particular, Section 4.4.1 describes a comparison with stateof-the-art generic off-line planners that demonstrates that our planner finds plans hundreds
of times faster that are often of higher quality, and an on-line appendix provides videos
of our planner controlling our hardware prototype. Our integrated approach to on-line
planning and scheduling allows us to achieve high throughput even for complex systems.
416

On-line Planning and Scheduling for Modular Printers

Figure 1: A prototype modular printer built at PARC. The system is composed of approximately 170 individually controlled modules, including four print engines.

We conclude the paper with a summary of general lessons we derived from building this
application.

2. Application Context
In analogy to other parallel systems such as RAID storage, our approach to modular printing
systems is called Rack Mounted Printing (RMP). An RMP system can be seen as a network
of transports linking multiple printing engines. These transports are known as the media
path. Figure 1 shows a four-engine prototype printer built at the Palo Alto Research Center
(PARC) from over 170 independently controlled modules. Figure 2 provides a schematic
side view, showing the many possible paper paths linking the paper feeders to the possible
output trays. (Video 1 in the on-line appendix, ‘nominal simulation,’ presents an animation
of Figure 2.) Multiple feeders allow blank sheets to enter the printer at a high rate and
multiple finishers allow several print jobs to run simultaneously. Having redundant paths
through the machine enables graceful degradation of performance when modules fail. By
building the system out of relatively small modules, we enable easy reconfiguration of the
417

Ruml, Do, Zhou, & Fromherz

Figure 2: A schematic side view of the modular printer indicating the feeders, paper path,
and output trays.

components to add new modules and functionality. Achieving these benefits, however, poses
a considerable control challenge.
The modular printing domain is reminiscent of ‘mass customization,’ in which massproduced products are closely tailored and personalized to individual customers’ needs.
It is also similar to package routing or logistics problems. From a control perspective, it
involves planning and scheduling a series of sheet requests that arrive asynchronously over
time from the front-end print-job submission and rendering engine. The system runs at
high speed, with several sheet requests arriving per second, possibly for many hours. Each
sheet request completely describes the attributes of a desired final product. There may be
several different sequences of actions that can be used to print a given sheet. For example,
in Figure 2, a blank sheet may be fed from either of the two feeders, then routed to any
one of the four print engines (or through any combination of two of the four engines in the
case of duplex printing) and then to either finisher (unless the sheet is part of an on-going
print job).
This on-line planning problem is complicated by the fact that many sheets are in-flight
simultaneously and the plan for the new sheet must not interfere with those sheets. Most
actions require the use of physical printer components, so planning for later sheets must take
into account the resource commitments in plans already released for production. Because
modern printers are highly configurable, can execute an large variety of jobs potentially
simultaneously, and have a large variety of constraints on feasible plans, hard-coded or
locally-reactive plans do not suffice (Fromherz et al., 1999). In fact, printer engineers at
Xerox delight in uncovering situations in which products from competing manufacturers,
who do not use model-based planning, attempt to execute infeasible plans.
418

printer
model

Translator

On-line Planning and Scheduling for Modular Printers

domain
description

Planner

failures

Translator

problem
description
sheet
description

constraints

STN

plans
time info
itineraries

goals

Plan Manager

rejections,
failures,
updates

Machine
Controller

Figure 3: The system architecture, with the planning system indicated by the dashed box.

The planning system must decide how to print all requested sheets as quickly as possible
and thus it must determine a plan and schedule for each sheet such that the end time T of
the plan that finishes last is minimized. In other words, the planner attempts to minimize
the makespan of the combined global plan for all sheets, in essence optimizing the system’s
overall throughput. Typically there are many feasible plans for any given sheet request;
the problem is to quickly find one that minimizes T . The optimal plan for a sheet depends
not only on the sheet request, but also on the resource commitments present in previouslyplanned sheets. Any legal series of actions can always be easily scheduled by pushing it
far into the future, when the entire printer has become completely idle, but of course this
is not desirable. This is an on-line task because the set of sheets grows as time passes
and plan execution (i.e., printing) interleaves with plan creation. In fact, because it is the
real-world wall clock end time that we want to minimize and because the production of a
sheet cannot start until it is planned, the speed of the planner itself affects the value of a
plan! However, the system often runs at full capacity, and thus the planner usually need
only plan at the rate at which sheets are completed, which again may be several per second.
While challenging, the domain is also forgiving: feasible schedules can be found quickly,
sub-optimal plans are acceptable, and plan execution is relatively reliable.
A printer controller works in an on-line real-time and continual planning environment
with three on-going processes: 1) on-line arrival of new goals; 2) planning for known goals;
and 3) execution of previously synthesized plans. Figure 3 shows the inputs and outputs
of the planning system, with the domain model and sheet requests entering from the left
and communication with the low-level control system on the right. The plan manager is
responsible for tracking the status of each goal and invoking the planner when necessary.
While planning and execution occur sequentially for any given sheet, these processes will
usually be interleaved between different sheets. Figure 4 sketches the different steps in the
sheet-plan life cycle managed by the plan manager. Specifically, upon receiving, sheets are
put in the unplanned first-in-first-out queue (sheets 6 and 7). The sheet planner then picks
one sheet at the time from the unplanned queue and tries to find a route-plan for that
sheet (sheet 5). Any plan when found is put in the queue of plans that haven’t yet been
sent to the printer controller (sheets 3 and 4). Another plan manager process regularly
checks the planned queue to decide if the earliest starting time of any plan in that queue is
419

Ruml, Do, Zhou, & Fromherz

sheet 1
sheet 2
sheet 5

sheet 3

sheet 6

start time

sheet 7

sheet 4

sheet
ddescriptions
i ti

not yet
planned

being
planned

planned,
unsent

sent to
printer

Figure 4: Stages in the life of a sheet in the planning system.
close enough to the current wall-clock time and send those plans to the printer controller
for execution (sheets 1 and 2). Note that in the figure, time advances downward so plans
starting earlier are higher in the figure. Sheets 1, 2, and 3 finish in order; sheets 4 and 5
belongs to a different job and can be scheduled to run concurrently.
In our application, there is an additional negotiation step after a plan is issued by the
planning system and before the plan is committed. First, each plan step is proposed by the
machine controller to the modules involved. If the individual hardware modules from all
steps accept their proposed actions, then the plan is committed. As we discuss below, this
commitment means that modules become responsible for notifying the controller if they fail
to complete an action or realize that they will not be able to perform a planned action in
the future. After a plan is confirmed, the planner cannot modify it. There is thus some
benefit in releasing plans to the machine controller only when their start times approach. If
not all modules confirm, then the machine controller notifies the planning system that the
proposed plan has been rejected, and the system must produce a new plan. This negotiation
process is one reason that we must find a complete plan before starting execution.
Each module has a limited number of discrete actions it can perform, each transforming
a sheet in a known deterministic way. For many of these actions, the planner is allowed
to control its duration within a range spanning three orders of magnitude (milliseconds
to seconds). For example, the planner may choose to transport a sheet faster or slower
through a module in order to avoid collisions. Actions may not split a sheet into two pieces
or join multiple sheets from different paths in the printer together. This means that a single
printed sheet must be created from a single blank sheet of the same size, thereby conflating
sheets with material and allowing plans to be a linear sequence of actions. In our domain,
adjacent actions must meet in time; sheets cannot be left lingering inside a printer after an
action has completed but must immediately begin being transported to its next location.
Sheets are grouped into print jobs. A job is an ordered set of sheets, all of which must
eventually arrive at the same destination in the same order in which they were submitted.
Multiple jobs may be in production simultaneously, although because sheets from different
420

On-line Planning and Scheduling for Modular Printers

jobs are not allowed to interleave at a single destination, the number of concurrent jobs is
limited by the number of destinations (i.e., finisher trays).
Currently, Xerox uses a constraint-based scheduler to control its high-end and midrange printers (Fromherz et al., 1999). The scheduler enumerates all possible plans when
the machine starts up and stores them in a database. When printing requests arrive on-line,
the scheduler picks the first feasible plan from the database and uses temporal constraint
processing to schedule its actions. This decoupling of planning and scheduling is insufficient
for complex machines for two reasons. First, the number of possible plans is too large to
generate ahead of time, and indeed becomes infinite if loops are present, as in the printer
shown in Figure 2. Second, the precompiled plans can be poor choices given the existing
sheets in the system. For example, sheets should be fed from different feeders depending
on when the previous sheets were fed, how large they are, and how long they will dwell
in the print engines (which can be a function of sheet thickness and material). For high
performance, we must integrate planning and scheduling in an on-line fashion.
Occasionally a module will break down, failing to perform its committed action. Modules
can also take themselves off-line intentionally, for example to perform internal re-calibration
or diagnosis. Modules may be added or subtracted from the system and this information is
passed from the machine controller to the planning system at the right side of Figure 3. The
vision of RMP is that the system should provide the highest possible level of productivity
that is safe, including running for long periods with degraded capabilities.1 Meeting this
mandate in the context of highly modular systems means that precomputing a limited set
of canonical plans and limiting on-line computation to scheduling only is not desirable. For
a large system of 200 modules, there are infeasibly many possible degraded configurations
to consider. Depending on the capabilities of the machines, the number of possible sheet
requests may also make plan precomputation infeasible. Furthermore, even the best precomputed plan for a given sheet may be suboptimal given the current resource commitments
in the printing machine.
To summarize, our domain is finite-state, fully-observable, and specifies classical goals
of achievement. However, planning is on-line with additional goals arriving asynchronously.
Actions have real-valued durations and use resources. Plans for new goals must respect
resource allocations of previous plans. Execution failures and domain model changes can
occur on-line, but are rare.

3. System Overview
A complete printing system encompasses many components, including print-job submission,
print-job management and planning, sheet management and planning, image rendering and
distribution, low-level module control, media handling hardware, and exception handling.
This paper focuses on planning issues at the sheet level, including exception handling.
Before discussing any one issue in great detail, this section provides an overview of those
topics that involve sheet planning most directly, including hardware control and exception
handling.
1. For example, for the safety of the operator, the system should not continue to use a module whose access
cover has been opened, even if it were hypothetically possible to repair one portion of the module while
another is in use.

421

Ruml, Do, Zhou, & Fromherz

Time Step
1

2

3

4

5

6

Feed 1 2
Print

1

Loop
Finish

2

2

2

2

1
2

2
1

1

3

4

1

2

5

2
2

1

2

Figure 5: Two different schedules for printing a duplex sheet (2) after a simplex sheet (1):
launching the sheets out of order improves throughput.

Figure 3 shows the basic architecture of the planning system and how it communicates
with the machine controller. The overall objective is to minimize the makespan of the
combined global plan for all sheets, in essence optimizing the system’s throughput. We
approximate this by planning one sheet at a time, with the objective of having that sheet
finish as quickly as possible while respecting any ordering constraints it may have with other
sheets. Sheets are optimally planned on an individual basis, in order of arrival, without
reconsidering the plans selected for previous sheets. In the figure, the plan manager calls
the planner for each sheet and records the resulting plan. To mitigate the restrictiveness of
this greedy scheme, we represent action times using temporal constraints instead of absolute
times. These constraints are stored in a simple temporal network (Dechter, Meiri, & Pearl,
1991), marked STN in the figure. By maintaining temporal flexibility as long as possible,
we can shift plans for older sheets later in time to make room for starting a new sheet earlier
if that improves overall machine throughput. While this may sound like a rare case, it can
be quite common. Figure 5 illustrates how, for a simplex (single-sided) cover sheet followed
by a duplex (double-sided) sheet, it can be faster overall to launch the second sheet first.
Although this basic architecture is specifically adapted to our on-line setting, the planner uses no domain-dependent search control knowledge. Furthermore, this mix of goaldecomposable planning with cross-goal resource constraints is quite common, and we believe
our framework can be useful in any AI system that needs to interleave real-time decision
making, planning, and execution, such as robot operations.
3.1 Planning
We have implemented our own temporal planner using an architecture that is adapted to
this on-line domain. As we will see below, the large number of potential plans for a given
sheet and the close interaction between plans and their schedules means that it is much
better to process scheduling constraints during the planning process and allow them to focus
planning on actions that can be executed soon. The planner uses state-space regression,
with temporal information stored in the STN. The STN records a feasible interval for each
time point in each plan. Time points are restricted to occur at specific single times only
when the posted constraints demand it. Because the planner maintains the partial orders
between different actions in plans for different sheets through the STN while conducting
422

On-line Planning and Scheduling for Modular Printers

On-linePlanner
1. plan the next sheet
2. if an unsent plan starts soon, then
3.
foreach plan, from the oldest through the imminent one
4.
clamp its time points to the earliest possible times
5.
release the plan to the machine controller
PlanSheet
6. search queue ← {final state}
7. loop:
8.
dequeue the most promising node
9.
if it is the initial state, then return
10. foreach applicable action
11.
apply the action
12.
add temporal constraints
13.
foreach potential resource conflict
14.
generate all orderings of the conflicting actions
15
enqueue any feasible child nodes
Figure 6: Outline of the hybrid planner
the backward state-space search, it can be seen as a hybrid between state-space search
and partial-order planning. A sketch of the planner is given in Figure 6. The outer loop
corresponds to the plan manager in Figure 3.
After planning a new sheet, the outer loop checks the queue of planned sheets to see
if any of them begin soon (step 2). It is imperative to recheck this queue on a periodic
basis, so ‘soon’ is defined to be before some constant amount after the current time and
we assume that the time to plan the next sheet will be smaller than this constant. The
value of this constant depends on the domain specifics such as communication delay or
module preparation time and is currently selected manually. If this assumption is violated,
we can interrupt planning the next sheet and start over later. As plans are released and
executed, resource contention will only decrease, so the time to plan the new sheet should
decrease as well. It is important that new temporal constraints are added by the outer loop
only between the planning of individual sheets, as propagation can affect feasible sheet end
times and thus could invalidate previously computed search node evaluations if planning
were underway.
While maintaining partial orderings between actions seems necessary to mitigate our
one-sheet-at-a-time greedy strategy, the planning for individual sheets need not necessarily
take the form of state-space regression. We have considered a forward search strategy, such
as employed by many modern planners such as FF (Hoffmann & Nebel, 2001) or LAMA
(Richter, Helmert, & Westphal, 2008). Initial investigation and preliminary empirical comparisons showed that while a progression planner is easier to implement and easier to extend
to handle additional domain complexities, the performance of the regression planner (using
the same heuristic) is significantly better in many problems in this domain. This seems
423

Ruml, Do, Zhou, & Fromherz

to be due mainly to the temporal constraint enforcing that a given sheet should end after
the end time of all the previous sheets in the same batch. This constraint interacts well
with searching backward from the goal, immediately constraining the end time of the plan.
Together with the constraint that actions must abut in time, many possible orderings for
resolving resource contention are immediately ruled out. For example, the current sheet
cannot be transported to its destination before the previous sheet in the same batch. In
addition, some orderings may immediately push the end time of the plan even later, further
informing the node evaluation function.
A planner that searches in the forward direction benefits slightly from avoiding logical
states that are unreachable from the initial state. However, without a similar temporal
constraint for the first action in the plan, few resource allocation orderings can be pruned
and the branching due to resource contention increases in direct proportion to the number
of plans for previous sheets maintained in the plan manager. Furthermore, the end time
of the plan rarely changes until far into the planning processes, making the heuristic less
useful. In short, for the first sheet, the performance of forward or backward planners are
similar, while as the number of plans managed by the plan manager increases, the backward
planner seems to perform better.
Due to details of the machine controller software, the planner must release plans to
the machine controller in the same order in which the sheet requests were submitted. This
means that sheets submitted before any imminent sheet must be released along with it (step
3). Only at this stage are the allowable intervals of the sheet’s time points forcibly reduced
to specific absolute times (step 4). Sensibly enough, we ask that each point occur exactly
at the earliest possible time. Because the temporal network uses a complete algorithm to
maintain the allowable window for each time point (a variation on Cervoni, Cesta, & Oddi,
1994), we are guaranteed that the propagation caused by this temporal clamping process
will not introduce any inconsistencies. The clamping happens before plans are issued; thus
we do not face the on-line dispatchability problem of Muscettola, Morris, and Tsamardinos
(1998).
In our current on-line setting, even though we plan for multiple sheets belonging to
different jobs, we build plans for a single sheet at the time. Even if there are many submitted
sheets waiting to be planned, this strategy is reasonable given that sheets arrived in sequence
and, until the arrival of the last sheet, we do not know how many sheets are in each job and
when the planner will receive the individual sheet specifications. Waiting until all sheets
are known is impractical as many production jobs, such as billing and payroll, involve jobs
with many thousands of sheets that can run for multiple days.
3.2 Control
As shown in Figure 2, our system consists of two feeders on the left, two finishing trays
on the right, and four print engines with one in each of the four ‘quadrants’ of the printer.
There are three high-speed sheet ‘highways’ that connect the feeders with the finisher trays.
Sheets traveling on the top and bottom highways can be routed to and from the print engines
through the ‘on-ramps’ and ‘off-ramps.’ For increased modularity, the highways and the
on- and off-ramps are made up of only two types of modules: ‘straight-through’ modules
and ‘three-way’ modules. Each module has its own processor: a Texas Instruments F2811
424

On-line Planning and Scheduling for Modular Printers

Figure 7: The control system architecture.

DSP. All modules run the same distributed algorithms for state estimation and control
and communicate with each other via five controller-area network (CAN) buses, plus a
dedicated data-logging bus for debugging purposes. Modules from the same ‘quadrant’
of the printer reside on the same CAN bus, except for the four print engines, which are
on a separate bus. Sheets are moved by roller actuators, called nips, that are driven by
independently-controlled stepper motors. For sensory feedback, each nip is equipped with
an edge-detection sensor on both sides of the nip. In each three-way module, there are three
solenoids that drive flipper actuators to direct the sheet along different paths.
Figure 7 shows the control system architecture, which implements a hierarchical approach to distributed plan execution in which a sheet controller manages those module
controllers that are currently, or will soon be, in contact with the sheet. Thus the sheet
controller group membership is dynamic over the life cycle of a sheet, starting from the
feeder all the way to the finisher tray. As soon as a new sheet is sent to the machine controller, a corresponding sheet controller is created that resides on a centralized processor,
even though all the module controllers it manages reside locally on the modules themselves.
Note that a module controller may be processing commands from multiple sheet controllers,
as is the case of the module controller in the middle of Figure 7. While still in contact with
the first sheet, it will soon be in contact with the second sheet.
Because a sheet can span multiple modules in our printer, different nips acting on the
same sheet must be tightly synchronized in order to avoid damaging or jamming the sheet.
However, achieving exact synchronization over a network with uncertain communication
delays and stringent bandwidth can be challenging. Moreover, one must consider the limited
computation power of a module in the design of such a synchronization scheme. For example,
our controller sample time is set at 2 milliseconds. Thus, anything that takes longer than 2
milliseconds to compute within a sampling period will not work. To eliminate the effects of
uncertain network delays, the control system uses a delay equalizer, which buffers all sensory
feedback messages until an ‘apply’ time, to make sure that any sensor information is used at
the same time by all members of a group of module controllers for the same sheet. To save
bandwidth needed for synchronization, each controller uses internal models (or estimators)
to keep track of the states of the other controllers on the network in order to limit the need
425

Ruml, Do, Zhou, & Fromherz

for communications (Crawford, Hindi, Zhou, & Larner, 2009; Hindi, Crawford, & Fromherz,
2005).
The limited network bandwidth has fundamental impact on our choice of the control algorithm. Initially, a linear-quadratic-gaussian (LQG) (Franklin, Powell, & Workman, 1997)
controller was used, which has the nice property that its solution constitutes a linear dynamic feedback control law that is easily computed. However, the bandwidth requirements
of an LQG controller, which necessitates that more than a dozen way points per sheet be
sent over the network, has prompted the adoption of a different kind of controller based
on proximate time optimal servo (PTOS) (Hindi, Crawford, Zhou, & Eldershaw, 2008;
Franklin et al., 1997), which consumes much less bandwidth. For comparison, a PTOS
controller reduces the number of intermediate way points from more than a dozen to two
per sheet. Since PTOS is based on time optimal control that uses either the maximum acceleration or deceleration to reach the target control state, this also maximizes the temporal
flexibility of the planning actions that our planner can use, thus improving on the overall
throughput of the printer.
3.3 Previous Work
There has been much interest in the last 15 years in the integration of planning and scheduling techniques. HSTS (Muscettola, 1994) and IxTeT (Ghallab & Laruelle, 1994) are examples of systems that not only select and order the actions necessary to reach a goal, but also
specify precise execution times for the actions. The Visopt ShopFloor system of Barták
(2002) uses a constraint logic programming approach to incorporate aspects of planning
into scheduling. And the Europa system of Frank and Jónsson (2003) uses an novel representation based on attributes and intervals. All of these system use domain representations
quite different from the mainstream PDDL language (Fox & Long, 2003) used in planning
research and all of them were designed for off-line use, rather than controlling a system
during continual execution.
There is currently great interest in extending planning and scheduling techniques to handle more of the complexities found in real industrial applications. For example, PDDL has
been extended to handle continuous quantities and durative actions. There are additional
dimensions to planning complexity besides expressivity, however. Our work complements
the trend in current planning research to extend expressiveness by focusing on the middle ground between planning and scheduling. The domain semantics for printing are more
complex than in job shop scheduling but simpler in many ways than PDDL2.1. Choice of
actions to perform is important in our domain, but managing resource conflicts is equally
important. As in classical scheduling, resource constraints are essential because the printer
modules often cannot perform multiple actions at once. But action selection and sequencing are also required because a given sheet can usually be achieved using several different
sequences of actions.
Our domain formalization lies between partial-order scheduling and temporal PDDL.
Because the optimal actions needed to fulfill any given print-job request may vary depending on the other sheets in the machine, the sequence of actions is not predetermined
and classical scheduling formulations such as job-shop scheduling or resource-constrained
project scheduling are not expressive enough. This domain clearly subsumes job-shop and
426

On-line Planning and Scheduling for Modular Printers

flow-shop scheduling: precedence constraints can be encoded by unique preconditions and
effects. Open shop scheduling, in which one can choose the order of a predetermined set
of actions for each job, does not capture the notion of alternative sequences of actions and
is thus also too limited. The positive planning theories of Palacios and Geffner (2002) allow actions to have real-valued durations and to allocate resources, but they cannot delete
atoms. This means that they cannot capture even simple transformations like movement
that are fundamental in our domain. In fact, optimal plans in our domain may even involve executing the same action multiple times, something that is always unnecessary in a
purely positive domain. However, the numeric effects and full durative action generality of
PDDL2.1 are not necessary.
Because of the on-line nature of the task and the unambiguous objective function, there
is an additional trade-off in this domain between planning time and execution time that is
absent from much prior work in planning and scheduling. In our setting the set of sheets is
only revealed incrementally over time, unlike in classical temporal planning where the entire
problem instance is available at once. And in contrast to much work on continual planning
(desJardins, Durfee, Ortiz, & Wolverton, 1999), the tight constraints of our domain require
that we produce a complete plan for each sheet before its execution can begin. Our domain
emphasizes on-line decision making, which has received only limited attention to date. Our
objective is to complete the known print jobs as soon as possible, so taking too long to
synthesize a slightly shorter plan is worse than quickly finding a near-optimal solution.
This is especially true when rerouting in-flight sheets during exception handling.
Although we present our system as a temporal planner, it fits easily into the tradition
of constraint-based scheduling (Smith & Cheng, 1993; Policella, Cesta, Oddi, & Smith,
2007). The main difference is that actions’ time points and resource allocations are added
incrementally rather than all being present at the start of the search process. The central
process of identifying temporal conflicts, posting constraints to resolve them, and computing
bounds to guide the search remains the same. In our approach, we attempt to maintain
a conflict-free schedule rather than allowing contention to accumulate and then carefully
choosing which conflicts to resolve first. Our approach is perhaps similar in spirit to that
taken by the IxTeT system (Ghallab & Laruelle, 1994).
Our basic approach of coordinating separate state-space searches via temporal constraints may well be suitable for other on-line planning domains. By planning for individual
print jobs and managing multiple plans at the same time, our strategy is similar in spirit to
planners that partition goals into subgoals and later merge plans for individual goals (Wah
& Chen, 2003; Koehler & Hoffmann, 2000). In our framework, even though each print job
is planned locally, the plan manager along with the global temporal database ensures that
there are no temporal or resource inconsistencies at any step of the search. It would be
interesting to see if the same strategy could be used to solve partitionable STRIPS planning
problems effectively.

4. Nominal Sheet Planning
The sheet planner builds a plan for each sheet of a job using a combination of regression
state-space planning and partial-order scheduling. It plans by adding one module action
at a time, starting from a finisher until the sequence of actions reaches a feeder. Adding
427

Ruml, Do, Zhou, & Fromherz

an action to a sheet’s itinerary (i.e., plan) causes resource allocations to be made on any
resources required for the execution of that action. Given the media path redundancies in
RMP, the planner usually faces multiple choices about which action to add at each planning
step. To organize this search, the planner uses best-first A* search with a planning-graph
heuristic, adjusted with resource conflicts, that estimates how promising each plan suffix
is. Unlike traditional regression planners, to maintain maximum flexibility, all action times
such as the start and end of each action and each resource allocation are represented as
flexible time points instead of absolute times. Temporal constraints are used to represent the
durations of actions and to resolve resource contention by imposing orderings among actions.
The planner attempts to minimize the makespan of the combined global plan for all sheets,
in essence optimizing the system’s throughput. The planner uses no domain-dependent
search control knowledge, allowing us to use the same planner to run very different printing
systems at full productivity.
4.1 Domain Language
We used a two-tiered approach to represent the RMP domain. At the highest level, we
use a specialized language that makes it easier for Xerox engineers to model their printers.
This language specifies printer configurations as components that are connected to each
other. Basic components can have different capabilities and components can be grouped in
a hierarchical structure. The model files in this format are then automatically translated
into a variation of PDDL2.1, which is then fed into our planner. The automatic translation
process instantiates the primitive modules and then converts each module’s capabilities
into durative actions. The movement of a sheet and the marking actions can be directly
translated from the printer model into traditional logical preconditions and effects that test
and modify attributes of the sheet. Following the spirit of compositionality of earlier work
(Fromherz et al., 1999), the model of the system can be automatically synthesized from
models of the individual components.
As in PDDL, we distinguish between two types of input to the planner. Before planning
begins, a domain description containing predicate and action templates is provided. Then
the problem descriptions arrive on-line, containing initial and goal states, which are sets
of literals describing the starting and desired configurations. Our action representation
is similar to the durative actions in PDDL2.1, with the notable difference that we use
explicit representation of resources. Actions can specify the exclusive use of different types
of resources for time intervals specified relative to the action’s start or end time. Executing
one action may involve allocating multiple resources of various types such as: unit-capacity,
multi-capacity, cyclic, and state resources. Our actions also specify real-valued duration
bounds. That is, one can specify upper and lower bounds and then let the planner choose
the desired duration of the action. This is critical to modeling controllable-speed paper
paths, which can be very useful in practice. While PDDL allows the specification of duration
ranges, we are not aware of any IPC benchmark that does so or any general-purpose planner
that supports it.
To summarize, the core part of a domain file is a set of actions, each of which corresponds
to a capability of some component in a printer and is a 4-tuple a = hPre, Eff, dur, Alloci,
where:
428

On-line Planning and Scheduling for Modular Printers

PrintSimplexAndInvert(?sheet, ?side, ?color, ?image)
preconditions: Location(?sheet, Printer1-Input)
Blank(?sheet)
SideUp(?sheet,?side)
Opposite(?side, ?other-side)
CanPrint(MarkingEngine, ?color)
effects: Location(?sheet, Printer1-Output)
¬Location(?sheet, Printer1-Input)
HasImage(?sheet,?side,?image)
¬Blank(?sheet)
¬SideUp(?sheet, ?side)
SideUp(?sheet,?other-side)
duration: [13.2 seconds, 15.0 seconds]
set-up time: 0.1 second
allocations: MarkingEngine at ?start + 5.9 for 3.7 seconds
Figure 8: A simple action specification.
• Pre and Eff are sets of literals representing the action’s preconditions and effects.
• dur is a pair hlower, upperi of scalars representing the upper and lower bounds on
action duration.
• Alloc is a set of triplets hres, offset, duri indicating that action a starting at time sa
uses resource res during an interval [sa + offset, sa + offset + dur]. The constraints on
different types of resources are:
– Unit-capacity: this type of resource is non-sharable and thus all allocations for
a given resource of this type should not overlap. This provides a good model of
physical space and is the most common type of resource used in our planner.
– Cyclic: cyclic resource is one special type of unit-capacity resources for which
there are repeated durations during which the resources are unavailable for the
actions selected by the planner. For example, the unavailable durations may
represent routine automatic maintenance of some modules.
– Multi-capacity: there is an upper-bound on the maximum number of allocations
for a given resource of this type that can overlap. Moreover, allocations follow
a first-in-first-out order. Thus, if there are two allocations A1 = [sA1 , eA1 ] and
A2 = [sA2 , eA2 ] then sA1 ≺ sA2 implies eA1 ≺ eA2 .
– State resource: The resource can be labeled using one of a set of ‘states’. Allocations for a resource of this type can overlap if and only if they require the
resource to be in the same ‘state’.
A simple example is given in Figure 8. Set-up time refers to the required time between
when an action is committed and its execution begins—certain actions can require extensive
preparation on the part of the module before the sheet arrives and the action is really
429

Ruml, Do, Zhou, & Fromherz

background:
initial:

goal:

print job ID:

Sheet-23
Location(Sheet-23, Some-Feeder)
Blank(Sheet-23)
SideUp(Sheet-23,Side 1)
Location(Sheet-23, Upper-Finisher)
HasImage(Sheet-23, Side 1, Image 1)
HasImage(Sheet-23, Side 2, Image 2)
Color(Sheet-23, Side 1, Color)
Color(Sheet-23, Side 2, Black & White)
5

Figure 9: A sample sheet specification.
‘performed’. For resource usage, the PrintSimplexAndInvert action in Figure 8 specifies
exclusive use of the MarkingEngine from 5.9 seconds after the start of the action until 3.7
seconds later. Printer modules with multiple independent resources or with actions that
have short allocation durations relative to the overall action duration can work on multiple
sheets simultaneously. In PDDL, arbitrary predicates can be made to hold at the start,
end, or over the duration of an action. This expressivity is not needed in our domain and
thus we can assume a simple semantics similar to that using in the TGP planner of Smith
and Weld (1999) in which: (1) delete effects happen ‘at start’; (2) add effects happen ‘at
end’; (3) preconditions that are deleted are ‘at start’; and (4) preconditions that are not
deleted are ‘over all’. In addition to sheet-dependent literals, sometimes it is convenient
to specify actions using preconditions that refer to literals that are independent of the
particular goals being sought. This ‘background knowledge’ about the domain is supplied
separately in the machine specification, although it could also be compiled into the action
specifications. In our example, the possible colors that engines can put on a sheet of paper
(e.g., Black&White, Color, Custom Color) or default sides of papers (e.g., Front, Back) are
specified in this way. They are represented similarly to the ‘constant’ concept in PDDL.
In addition to the static domain description, the on-line sheet requests are modeled by
initial and goal state pairs describing the starting and desired sheet configurations. Each
new initial/goal pair defines a new object (the sheet) and the associated literals for the
planner to track. Specifically, a problem description for a particular sheet is a 4-tuple of
hJob, Initial, Goal, Backgroundi, where Job is the id of the print job that that sheet belongs
to and Initial, Goal, and Background are sets of literals.
A simple example sheet specification is given in Figure 9. In this example, Some-Feeder
is a virtual location where all sheet sources are placed and Upper-Finisher is one particular
finisher where all sheets that belong to print job 5 need to be routed to. In terms of Figure 3,
the feeder location literal is added to the goal by the plan manager, which maintains a
table of active jobs and the finisher assigned to each. Finisher assignment is handled by
extracting the finisher selected by the planner in its plan for the first sheet of the job.
Because finishing requires actions in the plan and actions are never reconsidered (only
rescheduled), the planner can never reconsider a job’s finisher assignment, even if it hasn’t
begun production yet.
430

On-line Planning and Scheduling for Modular Printers

Given a domain description (top left of Figure 3) and a low-level delay constant tdelay
capturing the latency of the machine controller software, the planner then accepts a stream
of sheets arriving asynchronously over time. Note that sheets may belong to different print
jobs being printed in parallel; within their print job, sheets need to be routed to the same
finisher (among multiple finishers) and finish in order. This stream corresponds to the
standard notion of a PDDL problem instance. For each sheet, the planner must eventually
return a plan: a sequence of actions labeled with start times (in absolute wall clock time)
that will transform the initial state into the goal state. Any allocations made on the same
unit-capacity resource by multiple actions must not overlap in time (state and multi-capacity
resources have different constraints as described earlier). Happily, plans for individual sheets
are independent except for these interactions through resources. Additional constraints on
the planner include:
• plans for sheets with the same print job id must finish at the same destination,
• plans for sheets with the same print job id must finish in the same order in which the
jobs were submitted,
• the first action in each plan must not begin sooner than tdelay seconds after it is issued
by the planner (with tdelay represents the delays in communication and negotiation
with the printer module controller),
• subsequent actions must begin at times that obey the duration constraints specified
for the previous action (thus it is assumed that the previous action ends just as the
next action starts).
4.2 Temporal Reasoning
Printer control is a rich temporal domain with real-time constraints: (i) between wall-clock
time and the plans for individual sheets, (2) between plans for different sheets, and (3) between the planner and the machine controller. Thus, fast temporal constraint propagation,
consistency checking, and querying are extremely important in our planner. We maintain
the temporal constraints using a Simple Temporal Network (STN) (Dechter et al., 1991),
represented by the box named STN in Figure 3. Essentially, the network contains a set of
temporal time points ti and constraints between them of the form lb ≤ ti − tj ≤ ub . The
time points managed by the STN include action start and end times and resource allocation
start and end times. Temporal constraints maintained in the STN are:
• constraints on wall-clock action start time;
• action start and end times should be within the action duration range;
• constraints between action start time and resource allocation by that action; and
• conflicts for various types of resources.
Because we use an A* search strategy that maintains multiple open search nodes, there
is a separate STN for each node. Temporal constraints are added to the appropriate STN
when a search node is expanded. Whenever a new constraint is added, propagation tightens
431

Ruml, Do, Zhou, & Fromherz

Planning Time (in seconds)

IDPC
AC-3

4

2

0
20

40

60

80

100

Sheet Number

Figure 10: Simple arc consistency is faster than incremental directed path consistency for
maintaining our STNs.

the upper and lower bounds on the domain of each affected time point. While this can lead
to more memory usage and extra overhead, it allows us not having to deal with temporal
constraint retraction, which is needed if a single STN is used for multiple search nodes.
Retracting temporal constraints from an STN is a complicated and time consuming process.
Because the planner must run indefinitely, we perform garbage collection on time points in
the STN between sheet planning episodes, harvesting those that lie in the past.
All time points are flexible until the plans they belong to are sent to the machine
controller. After planning a new sheet, the plan manager checks the queue of planned
sheets to see if there are any that could begin soon. If there are, those plans are released
to the machine controller to execute. New temporal constraints are added that freeze the
start and end times of actions belonging to plans sent to the controller. Those time points
are frozen at the earliest possible wall-clock time as indicated by the STN based on its
constraint set. Those constraints can cause significant propagation and in turn (1) freeze
the start and end times of resource allocations related to actions in the frozen plans; and
(2) tighten the starting times of actions in the remaining plans.
The original representation of an STN as a complete matrix of time relations updated
by all-pairs shortest paths (Dechter et al., 1991) is much too inefficient for our purposes.
We have implemented two versions of the STN. One uses an incremental directed path
consistency (IDPC) algorithm (Chleq, 1995), which may change the values on edges in
the constraint graph as well as introduce new edges but requires only linear time to find
the minimum and maximum interval between any two time points in the database. The
other uses arc consistency (Cervoni et al., 1994) and maintains for each point its minimum
432

On-line Planning and Scheduling for Modular Printers

and maximum times from t0 , the reference time point. In this latter method, one cannot
easily obtain the exact relations between arbitrary time points, only their relations with t0 .
However, as long as inconsistency can be efficiently detected when constraints are added,
we do not need to query the relations of arbitrary pairs of points, and the efficiency gains
are welcome. New arcs are never added to the network during propagation and existing
ones are not modified, which means that copying the network for a new search node does
not entail copying all the arcs. As the Figure 10 attests, this results in dramatic time
savings and this technique is used in our current implementation. We further improved our
implementation by (1) using change flags to facilitate faster cycle detection for temporal
consistency checking and (2) converting all times and durations to integer values (with user
defined precision) to eliminate rounding effects and increase speed.
4.3 Planning a Sheet
When planning individual sheets, the regressed state representation contains the state of
the sheet, which may be only partially specified. A* search is used to find the optimal plan
for the current sheet, in the context of all previous sheets. After the optimal plan for a
sheet is found, the resource allocations and STN used for the plan are passed back to the
plan manager and become the basis for planning the next sheet.
One unusual feature of our planning approach is that we seamlessly integrate planning
and scheduling. Starting times of actions are not fixed but merely constrained by temporal
ordering constraints in the STN. We insist that any potential overlaps in allocations for
the same resource be resolved immediately, resulting in potentially multiple children for a
single action choice. This allows temporal propagation to update the action time bounds
and guide plan search. While the plan for a single sheet is a totally-ordered sequence of
actions, there are partial orders between actions that belong to plans of different sheets to
represent the resource conflict resolutions.
4.3.1 State Representation
Because the plan must be feasible in the context of previous plans, the state contains
information both about the current sheet and previous plans. More specifically, the state
is a 3-tuple hLiterals, Tdb, Rsrcsi in which:
Literals describes the regressed logical state of the current sheet. We distinguish between
literals whose status is true, false, or unknown (Le, Baral, Zhang, & Tran, 2004).
The distinction between false and unknown literals is important in our domain because there may be fine-grained restrictions on the acceptable values for unspecified
attributes of the sheet. For example, if a sheet S is the first of a given print job, then
the finisher representing the final location of the sheet is unknown because it can be
any finisher that is not allocated to another print job when the plan for that sheet S
is executed. As we discuss below, we allow regression to match unknown literals with
both true and false effects of actions; in this sense ‘unknown’ can function like ‘don’t
care’. In our implementation, we represent explicitly those literals that are currently
true and those whose value is unknown, with false literals being represented implicitly.
433

Ruml, Do, Zhou, & Fromherz

Tdb is the temporal database represented as a simple temporal network (STN) containing
all known time points and the current constraints between them. This includes constraints between different plans, between actions in the same plan, as well as against
the wall-clock time. Examples of time points include the start/end times of actions
or resource allocations. As soon as a plan for a given sheet is sent to the machine
(sheets 1 and 2 in Figure 4), time points associated with that plan in the Tdb are no
longer allowed to float but are clamped at their lower bounds. All other time points
are flexible.
Rsrcs is the set of current resource allocations, representing the commitments made to
plans of previous sheets and the partial plan of the current sheet. Each resource
allocation is of the form hres, tp , tp i with res is a particular resource and tp1 , tp2 are
two time points in the Tdb representing the duration res is allocated to some action.
Note that there are multiple resources in the domain and each resource can have
multiple (overlapping or non-overlapping depending on the resource type) resource
allocations. In our implementation, we maintain an ordered list of the allocations on
each resource, most recent to oldest.
In essence, the state contains information reflecting the strategy of our planner: hybrid between state-space sequential temporal regression search and partial order scheduling. The
Literals and the action start and end time-points represent the temporal-planning regressed
state and the Rsrcs and the temporal orderings between competing resource allocations represent partial-order scheduling constraints between actions in the plans of different sheets.
4.3.2 Branching on Applicable Actions
Each regressed logical state in our planner is a 3-tuple L = hLt , Lf , Lu i where Lt , Lf , and
Lu are the disjoint sets of literals that are true, false, and unknown, respectively. If Pre+ (a)
and Pre− (a) are the sets of positive and negative preconditions and Add(a) and Del(a)
are the sets of positive and negative effects of action a, then the regression rules used to
determine action applicability and update the state literals are:
Applicability Action a is applicable to the literal set L if (1) none of its effects are inconsistent with L and (2) any preconditions not modified by the effects of a are
T
T
consistent with L. More formally, (1) (Add(a) Lf = ∅) ∧ (Del(a) Lt = ∅), and (2)
T
T
(Pre+ (a) Lf ⊂ Del(a)) ∧ (Pre− (a) Lt ⊂ Add(a)).

In many planning settings, an additional criterion for applicability can be added withT
out destroying completeness: at least one effect of a must match L ((Add(a) Lt 6=
T
∅) ∨ (Del(a) Lf 6= ∅)). This is not necessarily valid in our setting because adding
a ‘no-op’ action a may give more time for an existing resource allocation to run out,
enabling other actions to be used which might lead to a shorter plan.

Update The regression of L = hLt , Lf , Lu i over an applicable action a is derived by undoing the effects of a and unioning the result with a’s preconditions. For a given literal
l modified by an effect of a, its status will be unknown in the regressed state unless it
is also specified by a corresponding precondition of a (e.g., ¬l is a precondition of a).
434

On-line Planning and Scheduling for Modular Printers

More formally, (1) Lt = (Lt \ Add(a)) Pre+ (a); (2) Lf = (Lf \ Del(a))
S
S
S
and 3) Lu = (Lu (Add(a) Del(a))) \ (Pre+ (a) Pre− (a))
S

S

Pre− (A);

Given that |Lf | is usually much larger than |Lt | and |Lu | in our domain, we explicitly store
Lt and Lu in our current implementation and use the closed-world assumption to imply
that all other literals belong to Lf . The modeling translator we provide to Xerox engineers
for modeling printers encourages all effects to be mentioned in preconditions, reducing the
growth of the number of unknown literals. For example, if x ∈ Add(a) then ¬x ∈ P re− (a).
Although it is not usually the case in our domain, we should note that if the goal
state were always fully specified (with no unknown literals) and every action’s effects had
corresponding preconditions, all regressed states would be fully specified. One could then
simplify the logical state representation to L = hLt , Lf i and simplify the regression rules to
Applicability Action a is applicable iff all of its effects match in L: Add(a) ⊆ Lt and
Del(a) ⊆ Lf .
Update Regressing hLt , Lf i through a gives h(Lt \Add(a))

S

Del(A), (Lf \Del(a))

S

Add(a)i

A plan is considered complete if its literals unify with the desired initial state (step 9
in Figure 6). After the optimal plan for a sheet is found, the temporal database used for
the plan is passed back to the outer loop in Figure 6 and becomes the basis for planning
the next sheet. Because feasible windows are maintained around the time points in a plan
until the plan is released to the machine controller, subsequent plans are allowed to make
earlier allocations on the same resources and push actions in earlier plans later. If such an
ordering leads to an earlier end time for the newer goal, it will be selected. This provides a
way for a complex job j2 that is submitted after a simple job j1 to start its execution in the
printer earlier than j1 . Out of order starts are allowed as long as all sheets in each print
job finish in the correct order. This can often provide important productivity gains.
4.3.3 Branching on Resource Allocation Orderings
While the first step in creating regressed states is to branch over the actions applicable
in L, applying each candidate action a can in fact result in multiple child nodes due to
resource contention. Some scheduling algorithms use complex reasoning over disjunctive
constraints to avoid premature branching on ordering decisions that might well be resolved
by propagation (Baptiste & Pape, 1995). We take a different approach, insisting that any
potential overlaps in allocations for the same resource be resolved immediately. Temporal
constraints are posted to order any potentially overlapping allocations and these changes
propagate to the action times. Because many action durations are relatively rigid in typical
printers, this aggressive commitment can propagate to cause changes in the potential end
times of a plan, immediately helping to guide the search process. Because multiple orderings
may be possible, there may be many resulting child search nodes.
For example, in Figure 4, assume that a is the current candidate action when searching
for a plan for sheet 5 and that a uses resource r for a duration [s, e]. We also assume
that there are n actions in the plans for sheets 1–4 that also use r, implying n existing
non-overlapping resource allocations [s1 , e1 ]....[sn , en ] and corresponding time points in the
temporal database. When trying to allocate r for a, one obvious and consistent choice is
435

Ruml, Do, Zhou, & Fromherz

end time of
new plan

earliest
start time
next actio
on

planning
start time

estimated length
of plan to come
(PG + Res. Conflict)

predicted
planning
time

end time of
prev. plan
in same job

end time of
all plans

length of
plan so far

t1

t2

t3

t4

STN: plan starting time constraint

t5

t6

t7

STN: sheet ordering constraint

Branching on actions, resource conflicts +
STN: resource contention constraints

Figure 11: Important time points for constructing and evaluating a plan.
putting it after all other previous allocations by adding the temporal constraint en ≺ s.
However, there can also be gaps between the existing allocations [si , ei ], allowing us to post
constraints such as ei ≺ s ≺ e ≺ si+1 . Each such possible allocation for r generates a
distinct child node in the search space. Because action a can use several different resources
r, the number of branches is potentially quite large. However, immediately resolving any
potential overlaps in allocations for the same resource avoids the introduction of disjunctions
in the temporal network, maintaining the tractability of temporal constraint propagation.
In summary, at every branch in the planner’s search space, we modify the logical state by
branching over relevant actions and potentially introduce different temporal constraints in
order to resolve resource contention. Because each branch results in a different irrevocable
choice that is reflected in the final plan, the state at each node in the planner’s search tree
is unique. Therefore, we do not need to consider the problem of duplicating search effort
due to reaching the same state by two different search paths.
4.3.4 Heuristic Estimation
For each potential plan suffix, a lower bound is computed on the remaining makespan, in
order to guide the planner’s A* search. Figure 11 illustrates how this heuristic estimate
is used. In the figure, planning start time (t1 ) refers to the wall-clock time at which the
planning process started and earliest-start-time = current wall-clock time + predicted planning time (t2 ) is the estimated time at which we will find a plan for the current sheet and
thus is the earliest time that any action can be scheduled to begin. Note that in practice,
the machine controller communication and negotiation time is also added to the predicted
planning time. The hypothetical start time of the plan when found (t3 ) is constrained to
happen after this earliest possible wall-clock plan execution time (t2 ≺ t3 ). A plan is constrained to end after those for previous sheets in the same print job (t5 ≺ t6 ), but is not
necessarily constrained to start after or before plans for previous sheets. The start time of
the next action added to the regressed partial plan (t4 ) is constrained to occur at least D
436

On-line Planning and Scheduling for Modular Printers

after the hypothetical plan starting time (t3 + D ≺ t4 ) where D is the heuristic value on
the makespan of the remaining plan to complete the current regressed partial-plan.
Our overall objective is to minimize the earliest possible end time of all plans, including
the sheet that we are planning for. This is indicated by the lower-bound on the floating
time point t7 in Figure 11. This time point is constrained to be after the end time points of
all the sheets that have been planned and the one currently being planned. For the current
sheet, this is represented by the constraints t6 ≺ t7 as shown in Figure 11. Because t6 is
constrained to end after the completion time of all the other planned sheets in the same
print job, the constraint essentially pushes t7 to be after all the sheets in the current print
job end. To support this objective function, the primary criterion evaluating the promise
of a partial plan (step 8 in Figure 6) is the estimate of the earliest possible happening time
for t7 , indicated by the STN embedded in this search node, after all constraints shown in
Figure 11 are added in the current branch.
The key duration that affects t7 is the heuristic estimate of the lower bound on the
additional makespan required to complete the current regressed plan. This heuristic value
is indicated in Figure 11 by estimated remaining makespan between t3 and t4 . By adding the
constraint t2 ≺ t3 , the insertion may thus change the earliest time of all the subsequent time
points t4 , t5 , t6 and t7 . It may also introduce an inconsistency in the temporal database, in
which case we can safely abandon the plan. Given that the current plan should end after
the end time of all previous sheets in the same print job (t5 ≺ t6 ), our objective function is
to minimize t7 without causing any inconsistency in the temporal database. We break ties
in favor of:
• smaller t6 (e.g., end time of the current print job)
• smaller predicted makespan (t6 − t3 )
• larger currently realized makespan (t6 − t4 ). This is analogous to breaking ties on
f (n) in A* search with larger g(n), and thus encourages further extension of plans
nearer to a goal.
The performance of our search-based planner heavily depends on the quality of the
heuristic estimating the makespan-to-go. We estimate D by building the temporal planning
graph with adjustment for both logical mutex and resource contentions. For the rest of this
section, we will discuss the details of how D is computed. Overall, we want an effective
planning heuristic that is:
• Admissible: because maintaining high plan quality (high productivity of the printer)
is an important criterion for our customer.
• Informed and easy to compute: because in most cases, we are only allowed a fraction
of a second to find a feasible plan. Any delay in finding a plan will delay plan start
execution time and thus reduce the overall productivity.
To derive an admissible estimate of the duration required to achieve a given set of goals
G from the initial state, we perform dynamic programming over the explicit representation
of the bi-level temporal planning graph, which was described in the TGP system (Smith
& Weld, 1999). In TGP, the planning graph is represented by a fact level and an action
437

Ruml, Do, Zhou, & Fromherz

level. Starting with the initial state at time t = 0, the graph is grown forward in time with
actions being activated when all of their preconditions are satisfied and non-mutex. There
are three types of mutual exclusion relations (fact-fact, fact-action, action-action) that are
propagated during the graph building process. The graph expansion phase alternates with
the plan extraction phase starting from the time point at which all the goals appear nonmutex in the graph.
In our graph expansion algorithm, for each action a and fact f , we store the first times
ta and tf at which a can optimistically occur or f can optimistically be achieved. They
correspond to the first times at which a and f appear in the temporal planning graph. For
mutex propagation, we also store the first time point at which each pair of facts hf1 , f2 i can
be achieved together and each pair of actions ha1 , a2 i can execute together. In the planning
graph, those are the first time points that hf1 , f2 i and ha1 , a2 i become non-mutex. In our
implementation, a fact-action mutex between fact f and action a is converted into action
mutex hnoopf , ai, as we will discuss later.
1. To begin: ∀f, a, f1 , f2 , a1 , a2 : ta = tf = thf1 ,f2 i = tha1 ,a2 i = ∞.
2. Let I be the initial state: ∀f, f1 , f2 ∈ I : tf = 0, thf1 ,f2 i = 0.
3. Dynamically update the values of ta , tf , thf1 ,f2 i , tha1 ,a2 i starting from the initial state
I and time t = 0 as follows:
ta = max (setup time(a),

max

f ∈P rec(a)

tf ,

max

f1 ,f2 ∈P rec(a)

thf1 ,f2 i )

tf = min (ta + dur(a))

(2)

f ∈Eff(a)

thf1 ,f2 i = min (tha1 ,a2 i +

max

(dur(a1 ), dur(a2 )))

f1 ∈Eff(a1 ),f2 ∈Eff(a2 )

tha1 ,a2 i = max (ta1 , ta2 ,

max

f1 ∈P rec(a1 ),f2 ∈P rec(a2 )

(1)

thf1 ,f2 i )

(3)

(4)

The updates are done in the increasing order of time, as usual for planning-graph
building algorithms.
4. Stop when ∀g ∈ G : tg < ∞ and ∀g1 , g2 ∈ G : thg1 ,g2i < ∞ or we reach a fixed point.
In the equations (1)-(4) as shown above, the actions include the ‘noop’ actions as in
the normal planning graph. Those actions start from the time point at which a fact is first
achieved. The mutex relation between a noop and an action is equivalent to a fact-action
mutex as described by Smith and Weld (1999). While the overall plan for all sheets is
highly parallel, the plan for a single sheet is sequential. Therefore, we currently use the
serial version of the temporal planning graph, which is also faster to build and consumes
less memory. In this version, two non-noop actions are always mutex with each other.
Therefore, we do not need to store and reason about action mutexes and thus the value of
tha1 ,a2 i in eq.(4) is only applicable to mutexes between a noop and a real action. In our
438

On-line Planning and Scheduling for Modular Printers

implementation, we build the graph starting from t = 0 by putting in events of (1) activating
an action (updating ta ); (2) activating a fact (updating tf ); and (3) removing a fact mutex
(updating thf1 ,f2 i ), ordered by the time they occur. Each event will trigger new events to
happen at a later time. For example, adding a new fact f or removing a fact mutex hf1 , f2 i
can activate actions supported by f or by both f1 and f2 , and activating action a will add
events of activating facts in Effect(a) and/or removing fact mutexes between Effect(a) and
‘noop’ (facts) that are not mutex with P recond(a). We also only explicitly store the factfact mutex timing values thf1 ,f2 i but none of the action mutexes tha1 ,a2 i , instead reasoning
about them on-the-fly.
The time at which all the goals are achieved pair-wise non-mutex is the heuristic value
estimating the remaining makespan to achieve the goal state (see Figure 11). While most
regression planners (Haslum & Geffner, 2001; Nguyen, Kambhampati, & Nigenda, 2002)
compute their heuristic once (until a fixed point is reached) before the planning process
begins, in our case, the planning graph expansion process may be revisited if goals representing a regressed state do not appear non-mutex in the graph and a fixed point was not
reached in the previous round of expansion. Because only pair-wise mutexes are taken into
account while building the graph, the estimated value is an underestimate of the makespan
of any plan that can achieve the goal. Therefore, the returned value by the planning graph
will lead to a underestimate (admissible heuristic) for both our objective function (overall
end time t7 ) and tie breakers (current sheet end time t6 and current estimated makespan
t6 − t3 ) as described above. Therefore, using this estimate, the planner will return plan p
with an optimal end time (minimum t7 ) and p also has a minimum makespan among all
plans with the same end time.
Incorporating resource mutexes The planning graph discussed until now assumes
that the printer is empty. Thus, we create the planning graph similar to the procedure
used in an off-line planner in which we assume the interference relations only occur between
actions related to a given sheet that we are planning for. If the machine is empty, the
heuristic is generally correct for simple sheets such as simplex printing and nearly correct
for complicated sheets such as duplex printing.
However, most of the time, the printer is not empty and there are plans for sheets that
are either (1) executing; or (2) found by the planner but haven’t been sent to the machine
controller yet. Those plans involve resource allocations, either at fixed time points (for (1))
or at still flexible ones (for (2)). To find a more effective heuristic in those scenarios where
the machine is busy, we take into account resource mutexes, thus incorporating scheduling
resource contention constraints into the temporal planning graph. Figure 12 shows the
pseudo-code of the algorithm. The key observation is that, to find the earliest time ta at
which an action a can possibly execute, a necessary condition is not only that all of a’s
preconditions appear non-mutex in the planning graph but also that there is no resource
conflict between any resource r used by a and all current allocations of r (given to previous
plans and by external processes.)
As shown in the example action description in Figure 8, each resource allocation of
action a is represented as a triple hr, o, di. If a starts at sa , this means that resource
r is used from sa + o for a duration d, which is normally different from the duration
da of a. For example, the lone resource usage for action PrintSimplexAndInvert in Fig439

Ruml, Do, Zhou, & Fromherz

1. Resource types: r1 , r2 , ....rn
2. All resource allocations: {R1 , R2 , ....Rn }
with Ri = {[si1 , ei1 ], [si2 , ei2 ], ...[sim , eim ]} the ordered list of allocations for ri
Function CheckEarliest(r, t, d)
3.
r: resource
4.
t: earliest time intend to use r
5.
d: duration intend to use r
6.
R = {[s1 , e1 ], [s2 , e2 ], ...[sm , em ]}: current allocations for r.
7.
tmin : earliest time of a non-conflict allocation for r; initialize to: tmin ← t
8.
for each allocation l = [sk , ek ] ∈ R check
9.
if we can reserve r for a duration of d before l starts: Latest(sk ) > tmin + d
10.
then go to line 14
11.
else move forward to the next possible opening at the end of allocation a
12.
tmin ← Earliest(ek )
13. end for;
14. return(tmin )
end function;
Building the temporal planning graph
15. when consider adding action a to the planning graph
16. Initialize ta to the earliest time at which P rec(a) achieved non-mutex (eq.1)
17. Resource allocations of a: Ra = {hr1 , o1 , d1 i, hr2 , o2 , d2 i, ..., hrn , on , dn i}
18. for each allocation l = hrk , ok , dk i check the earliest non-conflict time for l
19.
ta ← CheckEarliest(ri , ta + oi , di ) − oi
20. end for;
21. add action a to the temporal planning graph at ta until all
goals appear non pairwise mutex;
Figure 12: Building the temporal planning graph with adjustments for resource conflicts.
ure 8 is (M arkingEngine, 5.9, 3.7). Lines 17-19 in Figure 12 show that when building the
graph, for each action a that has all of its preconditions satisfied at ta , the algorithm goes
through all resources used by a. For each resource allocation hr, o, di, it calls the function
CheckEarliest(r, ta +o, d) to update ta , the earliest executable time that a can start without
overlapping with any of the previous resource allocations of r. The pseudo-code of function
CheckEarliest(r, t, d) is self-explanatory in that we try to find the earliest time point after
t at which we can ‘slot’ the allocation for r with duration d in without overlapping with the
previous allocations of r.
Figure 13 shows one example to demonstrate this algorithm. In this example, we try
to find the starting time for action a, which needs two unit-capacity resource allocations
hr1 , o1 , d1 i and hr2 , o2 , d2 i as shown on the top-left corner. Assume that when building the
graph, all of A’s preconditions can first be achieved non-mutex at time t1 . Referring to
Figure 13, the fixed allocations for r1 are allocations (1), (2), and(3) and for r2 are (1) and
440

On-line Planning and Scheduling for Modular Printers

sA

A:
r1

dA

o1

d1

r2

o2
(1)

r1

eA

d2
(2)

(3)

(1)

r2

t3

d2

o2

o1

t2

(4)

(3)

(2)

d1

t1

(4)

t4

t5

Figure 13: Example of action starting time adjustment using resource contentions
(2). The flexible allocations, shown with their upper/lower bound constraints are (4) for r1
and (3) and (4) for r2 . Starting from t1 , the first time point at which we can allocate r1 for
a duration d1 without overlapping with previous allocations, both fixed and flexible, is t3 .
Thus, we adjust the new earliest possible starting time for a to t2 = t3 − o1 ≻ t1 . Given the
new earliest possible starting time ta = t2 , we find that the earliest time point from t2 at
which we can allocate resource for r2 is t5 . Given that t4 = t5 − o2 ≻ t2 , we will then take
t4 as the final earliest starting time for a and activate action a at t4 in the graph (instead
of the original value t1 )2 .
With resource mutexes, the starting times of actions are adjusted to higher than the time
points at which their preconditions can be achieved, and thus the time point tG at which
all the goals appear non-mutex in the graph is not an underestimation on the makespan of
the remaining plan (value t4 − t3 in Figure 11). Thus, tG can be higher than the summation
of the durations of actions in the optimal (serial) plan. However, tG still underestimates
the first time that we can achieve the goals and thus is still an admissible heuristic for our
main objective function of minimizing the end time of current printing sheets (minimizing
t7 in Figure 11). However, if we do not use the resource mutexes, then both the heuristic
estimate for end time (t7 ) and the tie-breaker on plan makespan (t6 − t3 ) are admissible
while with resource mutexes, only the estimate of t7 is admissible and the tie-breaker t6 − t3
2. Note that we only go through all resource allocations of a given action one time (lines 18-20 in Figure 12).
Therefore, even when action A in Figure 13 is added at t4 , there is still a potential conflict for resource
r1 consumed by A (with the existing allocation (3) of r1 ). However, by not repeating the procedure
(lines 18-20 in Figure 12) multiple times until a fixed point is reached (and potentially returning better
heuristic estimate), we seek the balance between heuristic quality and heuristic computation time.

441

Ruml, Do, Zhou, & Fromherz

time (sec)

2
no-mutex
logical mutex
log + res mutex
productivity level

1.5

1

0.5

0
1

4

7 10 13 16 19 22 25 28 31 34 37 40 43 46 49

Figure 14: Performance for the prototype built by our industrial partner.

can be inadmissible. Thus, the A* algorithm is still guaranteed to find an optimal solution,
minimizing the plan end-time, but the final solution will not be guaranteed to have a shortest
duration among all plans that finish earliest.
4.4 Evaluation of Nominal Planning
In collaboration with Xerox, we have deployed the planner to control three physical prototype multi-engine printers. These deployments have been successful and the planner has
also been used in simulation to control hundreds of hypothetical printer configurations.
The planner is written in Objective Caml, a dialect of ML, and communicates with the job
submitter and the machine controller using ASCII text over sockets. The planner can also
communicate with a plan visualizer to graphically display the plans. The first two videos
in the on-line appendix show the planner controlling the prototype depicted in Figures 1
and 2 at full productivity, both using the visualizer (video 1, ‘nominal simulation’) and the
hardware (video 2, ‘nominal hardware’). A full description of the videos can be found in
the textual appendix of the paper. The shortest single plan for the machine has 25 actions.
Given that there are many sheets in the printer at any given time and the planner can
plan ahead, the plan manager consistently manages dozens of plans and hundreds of actions. During planning, the planner needs to do temporal reasoning regarding the conflict
between actions in the current plans and hundreds of actions in previous plans. Even so,
the planner consistently on average produces plans within the 0.27 seconds required to keep
the printer running at full productivity (220 pages/minute). For one of the most complex
current Xerox commercial products, the planner can regularly find the optimal plan within
0.01 seconds and can plan ahead hundreds of sheets. The ability to use domain-independent
planning techniques allows us to use the same planner for very different configurations, without needing any hand-tuned control rules. For the rest of this section, we will elaborate on
these results.
442

On-line Planning and Scheduling for Modular Printers

time (sec)

10
nomutex
logical mutex
log + res mutex
productivity level

1
1

6

11

16

21

26

31

36

41

46

0.1

0.01

jobs
Figure 15: Performance for the current prototype built by us and shown in Figure 1. Note
that time is plotted logarithmically.

Figure 14 and 15 show the performance of our planner in two of the most complex
parallel printer prototypes built by Xerox and by us. Their productivity levels are higher
than any other printer of the same class currently on the market. In each figure, we show
the CPU time consumed per-sheet for the basic test of using the planner for a print job
of 50 sheets: (1) without mutexes; (2) with serial temporal planning graph with mutexes;
(3) combination of logical and resource mutexes; and (4) the baseline requirement for the
planner’s performance to match with the printer’s full productivity. The other prototypes
investigated using our planner are either simpler or more complicated but used for theoretical investigations only. For the rest of this section, we will refer to the first printer
(results shown in Figure 14) as Configuration 1 and the other (results shown in Figure 15)
as Configuration 2.
The Configuration 1 printer in Figure 14 is the simpler one, with 25 main components
(including four print engines), 35 action schemata, and a nominal productivity of 170 sheets
per minute, leading to a timing requirement on the planner of around 60/170 = 0.353 sec
for planning each sheet. The shortest possible plan for a simplest simplex sheet contains 8
actions. There are normally more than 10 sheets in flight at a time and thus the planner
needs to handle the interaction of around 100 actions. However, the planner is typically
intended to plan many sheets ahead so the number of interactions is often much higher.
This printer is built and run by our industrial partner. In the figure, we show that without
mutex, the planner starts taking more than the base time of 0.353 second around sheet 20
and consistently takes higher than the limit around sheet 40. However, with logical and
resource mutexes, the planner consistently returns plans within a much shorter time than
required. On average, our planner with logical mutex takes an average of 0.0732 second
to find a plan and the combination of logical and resource mutex helps reduce the average
planning time to 0.0458 second (1.6x improvement). Without mutexes, the planner takes
an average of 0.4191 seconds and can not keep up with the full productivity of the printer.
443

Ruml, Do, Zhou, & Fromherz

The Configuration 2 printer tested in Figure 15 is the more complicated one. There
are 212 action schemata and the shortest possible plan contains 16 actions. The printer
generally handles more than 20 sheets at a time so the planner needs to regularly reason
about the interactions between more than 300-400 actions. The productivity level of this
printer is 220 pages-per-minute, which leads to the base running time for the planner of
60/220 = 0.27 second for planning a single sheet. Because of the wider gap in performance
between different versions of the planners, we show timing results for this printer in log scale.
Without using mutexes, the planner quickly overruns the time limit after a few sheets and
grows to more than 10 seconds around sheet 35, when we stopped the experiment. With
mutexes (logical, resource) the planner generally takes less than 0.3 second to find a plan.
However, occasionally the planner takes longer. But because it usually plans ahead around
10 sheets before releasing plans to the lower level controller, occasional jumps in planning
time don’t prevent the planner from achieving the full productivity of the printer in practice.
The planner averages 0.1336 second with only logical mutexes and 0.0928 second (1.44x
improvement) if used in conjunction with resource mutexes.
The results in Figure 14 and 15 indicate that the average planning time for individual
sheets increases with the number of previous sheets. This is due to the fact that the planner
generally plans faster than the speed at which the printer can print. Thus, as the number
of print requests received increases, the number of plans in the unsent queue (i.e., planned
for, but not sent to the machine yet) increases. This increases the resource contention and
branching factor when searching for a new plan, which leads to the increment in planning
time. Eventually, the number of lookahead sheets reaches a point where the planning time
equals the planner’s productivity and a dynamic equilibrium is reached. The planning time
does not strictly increase linearly in accordance with the number of sheets planned, but
rather shows an oscillating pattern. This is due to the complex interaction between the
on-line processes of planning, freezing time points in the found plan, and plan execution.
This can lead to easier planning problems when there are more sheets, depending on how
those sheets interact with the sheet that is currently being planned.
While it was noted by Smith and Weld (1999) and other work based on building the
planning graph that mutex propagation is costly, this was not our experience. In fact,
when the printer is rather empty, the total planning time, which subsumes the graph with
mutex building time is less than 0.01 second. We believe that this is due to a simpler mutex
propagation rule in our planner and the fact that the sequential plan of each sheet makes all
actions mutex at each step. Our resource mutex reasoning time is not as optimized as the
logical mutex implementation and can be improved, but it does not seem to be a significant
impediment in our intended application.
While the results we have presented indicate that our ‘optimal-per-sheet’ strategy seems
efficient enough, further work is needed to assess the drop in quality that would be experienced by a more greedy strategy, such as always placing the current sheet’s resource
allocations after those of any previous sheet. Similarly, during a lull in sheet submissions,
it might be beneficial to plan multiple sheets together, backtracking through the possible
plans of the first in order to find an overall faster plan for the pair together. Sheets that have
been planned but whose plans have not been released to the printer represent opportunities
for reconsideration in light of the newer sheets submitted more recently.
444

On-line Planning and Scheduling for Modular Printers

#
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

LPG
Span
Time
9.3
0.01
13.3
0.02
26.6
0.08
15.2
0.07
21.3
0.12
22.4
0.23
30.3
8.73
19.6
52.55
24.2
16.69
23.0
20.02
29.7
40.14
18.3 138.53
42.6
29.09
34.9 427.41
35.3
18.95

SGPlan
Span
Time
8.3
0.45
9.3 308.46
-

Hybrid
Span
Time
8.3 < 0.001
9.4 < 0.001
9.9
0.02
10.6
0.02
11.1
0.03
11.8
0.03
12.3
0.04
13.0
0.06
13.5
0.07
14.2
0.07
14.7
0.08
15.4
0.09
15.9
0.18
16.6
0.21
17.1
0.28

Table 1: Comparison of LPG, SGPlan, and our hybrid planner, showing the makespan of
the plans found (‘Span’) and planning times (‘Time’) in seconds for problems with
various numbers of sheets (‘#’).

4.4.1 Scaling Against Generic Planners
Although our planner has certain features, such as controllable action durations, that are
beyond the capabilities of existing planners, it is still interesting to compare against offline systems to validate our new approach. If existing generic systems could solve basic
printing control problems well, it might be possible to extend them, rather than developing
the more specialized planner architecture described above. Therefore, we built a tool to
automatically convert our custom domain language into the PDDL2.1 temporal planning
language, allowing us to test current state-of-the-art planners.
While our domain must be simplified to fit the limitations of PDDL, we observe that
even these simplified problems are not easy to solve by state-of-the-art academic planners
such as SGPlan (Chen, Hsu, & Wah, 2006) and LPG (Gerevini, Saetti, & Serina, 2003),
winners of the 2004 and 2006 International Planning Competitions. Since both planners
cannot solve any problem for the Configuration 2 machine from Figure 2, we tested them on
the much simpler Configuration 1 machine. While we only tested a monochrome job with
up to 15 simplex sheets, this already stretched the limits of LPG and SGPlan. Our planner
can plan ahead hundreds of sheets for this machine. As can be seen in Table 1, SGPlan
took more than 5 minutes to find a two-sheet plan that only took our planner less than
0.001 second to find. Compared to SGPlan, LPG is much faster, although the quality of the
plan LPG finds is much worse. On average, LPG returns plans with 86% longer makespan
and is about 400 times slower than our planner. For the objective function of minimizing
wall-clock finishing time (which combines planning time and plan makespan), our planner
is more than 1000x better than both planners for this small printer configuration.
445

Ruml, Do, Zhou, & Fromherz

In addition to being faster, our hybrid planner is also more predictable. LPG’s planning
time has much higher variance and it sometimes takes longer to plan for a smaller job than
a bigger one. For example, it took LPG 22 times longer to plan for the 14-sheet job in
Table 1 than it did for the 15-sheet job. This makes it unsuitable for real-time on-line
planning, which depends on accurate estimation of planning times for efficient temporal
event management.
4.4.2 The 2008 International Planning Competition
A version of our printing domain was used in the 6th International Planning Competition
(IPC6), held in 2008 and the results were presented at the ICAPS-08 conference. This
allowed us to evaluate our planner against many state-of-the-art systems. The deterministic
part of the competition had three tracks:
1. sequential with objective function of minimizing total cost of actions in the plan
2. temporal with objective function of minimizing the plan makespan.
3. net benefit with objective function of minimizing the trade-off between total goal
utility and action cost.
For all three tracks, the emphasis was on finding good solution quality. Thus, planner
running time is not part of the overall scoring metric. Specifically, each planner was given
30 minutes to run on a particular planning instance. The cost of the plan returned within
the time limit is used to calculate the score for that particular planner in that particular
instance. The score for a given instance is cost of best known solution / cost of generated
plan, where the cost of generating no plan is infinite. There are a total of 30 instances in
each domain and thus the maximum score any competitor can achieve is 30 (if all solutions
returned have the best quality among all competitors, or equal to the best known solution
generated by a specialized solver).
Real-world planners are often demonstrated on complex domains such as spacecraft or
mobile robot control which can be difficult to simulate and thus make awkward benchmarks.
Most popular temporal planning benchmark domains are off-line in the sense that the
planner’s speed does not affect solution quality. There remains a need for a simple yet
realistic benchmark domain that combines elements of planning and scheduling, especially
in an on-line setting. As a step toward bridging this gap, the organizers of IPC6 elected to
use the PARC printer domain in two tracks: sequential and temporal. The temporal track
was the most natural fit due to the default objective function of maximizing the printer’s
productivity, which equals to minimizing the makespan of the plan finishing all print-job
requests. For the sequential track, minimizing the total printing cost was used. Each action
has a certain cost value and using a more expensive color print engine to print a black&white
page costs more than using a monochrome print engine. However, the cost trade-off may
not be clear-cut if the feeder, where the blank sheets originally reside, is closer to the color
print engine than to the monochrome engine.
Even though the internal representation of our planner, which was used as a starting
point for the competition domain description, is not too far from the PDDL representation,
there were some difficulties in creating the competition domain file and the problem set.
446

On-line Planning and Scheduling for Modular Printers

C3
18.00

DAE1
14.45

DAE2
5.80

DTGPlan
16.44

F Fah
16.00

h
F Fsa
23.00

LAMA
20.93

PlanA
0.00

SGP 6
24.39

Upwards
26.91

baseline
26.53

Table 2: Scores from the IPC6 Sequential Satisficing Track
Because of the on-line continual nature of the domain and the fact that constraints such as
multiple resource allocations by each action and the sequential finishing order of sheets in
the same job caused a blowup in the problem size when using pure PDDL, the organizers
had to: 1) remove or approximate certain constraints in the original domain; and 2) model
less complex machines. Overall, three different printers were modeled. The first one is the
simpler four-engine (two color and two mono) Configuration 1 machine that we described
in Section 4.4. The second one is a stripped down version by using only half (one color and
one mono) of the first printer. The third one is another variation with only one mono and
one color printer. The first two printers have a rather symmetric design and a third one is
asymmetric. All three are significantly simpler than the fixture built at PARC. We helped
the IPC organizers model the actions as accurately as possible, and thus even though the
printer configurations are hypothetical, they reflect the characteristics of real hardware.
For the problem files, to reduce complexity, only print requests of a single job with
multiple sheets were used. The sheets are randomly set to be either simplex (one-sided print)
or duplex (two-sided print) and each image is also randomly selected to be either mono or
color. The number of sheets varies from 1 to 20. Given this print-job request and a particular
printer configuration, the competing planner needed to find a plan with lowest total printing
cost in the sequential track (matching effectively between image requirement and print
engine capabilities) and smallest makespan in the temporal track (synchronizing effectively
between different print engines). In the actual competition, only problems ranging from
1-10 sheets were used for each of the three modeled printers and only simplex sheets were
used for the biggest printer (4-engine version) to make the problems not too difficult for a
majority of participants.
For all problems used in the two tracks (more details below), we used the planner
described in this paper to provide the best known solutions to score competing planners.
For the temporal planning track, we ran our planner with the default objective function of
maximizing the machine’s productivity. For the sequential track, we ran our planner with
the objective function of minimizing printing cost, as described later in Section 6.1. Given
that the plan representation is different between our planner and the standard format used
in the IPC, a post-processing step was needed to convert from one format to another. Note
that in the plans returned by our planner, there are temporal buffers between related time
points, such as inter-sheet gaps. The post-processing step does not remove those small
temporal buffers, which are not needed for PDDL plans to be valid. Therefore, competing
temporal planners could theoretically return shorter makespans than our planner. However,
the results as described below show that our planner is still superior to all competing
planners in terms of plan quality. The organizers did not officially reveal the plan running
time but unofficial results showed that our planner was also much faster than all competing
planners in solving most problems.
Tables 2, 3, and 4 show the IPC results in the three sub-tracks in which the PARC printer
domain was used: sequential satisficing, sequential optimal, and temporal satisficing. Our
447

Ruml, Do, Zhou, & Fromherz

CFDP
3

CO-Plan
5

CPT3
17

Gamer
0

HSP0∗
14

HSPF∗
16

MIPS-XXL
7

PlanA
0

Upwards
0

baseline
10

Table 3: Scores from the IPC6 Sequential Optimal Track
CPT3
17.38

DAE1
11.93

DAE2
6.81

SGPlan 6
11.04

TFD
5.67

TLP-GP
1.73

baseline
13.87

Table 4: Scores from the IPC6 Temporal Satisficing Track

planner’s score would be 30 for all tracks given that it was used to provide the ”best known
solutions” for all tracks. The baseline planner for the sequential optimizing track was based
on blind search for optimal cost, in the sequential satisficing track was the FF planner, and
in temporal satisficing was the Metric-FF planner followed by a greedy scheduler. While the
sequential satisficing planners performed well, mostly due to the fact that most problems
in the sequential tracks are easy to solve, the competitors did not perform well in the other
tracks. The reason that sequential optimal planners did not perform well because they could
not solve many problems among those 30 selected. For the temporal satisficing tracks, most
planners could solve a large number of instances, but the quality of the plans returned by
those plans was not high, thus leading to the low overall scores. In short, the results of the
2008 International Planning Competition reinforced our early study indicating that generic
off-line planners are not competitive with our on-line hybrid system in this application.
Together, they provide evidence that the demands of our setting warrant a more specialized
approach.

5. Exception Handling
While maintaining high productivity, and thus high return on investment, is the most
common and important objective, it is by no means the only thing that equipment owners
care about. To reduce the need for operator oversight and expertise and to allow the use of
very complex mechanisms, the system must be as autonomic as possible. Because operators
can make mistakes and even highly-engineered system modules can fail, the system must
cope with execution failure. This is a crucial part of the RMP value proposition. For
example, imagine a printer or copier that never seems to jam, but just runs a little slower
as the month goes on. Once a month, someone opens the covers, removes some jammed
sheets, and the system is back at full productivity. The RMP systems that our planner is
used to control are designed to fulfill this vision of partial productivity when a subset of the
modules are down. To make this transition transparent to the user (and thus increase the
perceived reliability of the system), we have been concentrating on developing exception
handling techniques that minimize user interventions without stopping or slowing down the
machine. Current products perform exception handling using rules hard-coded into each
machine module. This technique works well for simple straight-line systems, but would be
limited to a small predefined subset of failures in more complex topologies. In our modular
RMP systems, there are an astronomical number of different printer configurations and
failure possibilities, so we require a more general exception handling approach.
448

On-line Planning and Scheduling for Modular Printers

In addition, because the system must work with legacy modules in order to be commercially viable, its architecture must tolerate components that are out of its direct control
and will give rise to unexpected events. We handle several different exception types such
as plan rejection (by the machine controller), model updates (i.e., module’s capabilities go
on or off-line), and sheet jams.
Since most plans in our system tightly interact through various scheduling and temporal
constraints, whether or not they belong to the same print job, an exception affecting any
single plan can affect the executability of other plans and the final job integrity. Plans in
different stages of their life cycle need to be analyzed and treated differently (see Figure 16).
Simple exceptions such as plan rejection and model updates can be handled by discarding
recently made plans and rolling back the state of the planner to before those sheets were
planned. Our implementation uses non-destructive data structures to make this efficient.
Execution failures such as sheet jams require more elaborate handling. While unsent plans
can be canceled, we need new plans for the sheets that are already in-flight at the time an
exception occurs. While this replanning can reuse much of the nominal planning system, it
requires some special modifications that we discuss in detail below. In this section, we first
provide an overview of the various types of exceptions that we handle and how the plan
manager reacts to them; we then concentrate on the hardest part of the exception handling
framework: finding a new set of consistent plans for in-flight sheets.
5.1 Related Work
There are several previously-proposed frameworks for handling exceptions and uncertainty
in plan execution. Markov decision processes (Boutilier, Dean, & Hanks, 1999) and contingency planning (Pryor & Collins, 1996) build plans and policies robust to uncertain environment. Planners built on those techniques are normally slow, especially in a real-time
dynamic environment with complex temporal constraints like ours. They are not suitable
for our domain where exceptions do not happen frequently, but need to be responded to very
quickly. Fox, Gerevini, Long, and Serina (2006) discuss the trade-off between replanning
and plan-repair strategies for handling execution failure. Their algorithms work off-line,
instead of in an on-line real-time environment such as ours, and they target a different
objective function (in their case, plan stability). CASPER system at JPL (Chien, Knight,
Stechert, Sherwood, & Rabideau, 1999) uses iterative repairs to continuously modify and
update plans to adjust to the dynamic environment. Unlike our system, CASPER uses
domain control-rules and thus is less flexible and the replanning decision is also not needed
as quickly as in our domain (in our case, sub-second).
5.2 Basic Exception Handling
Our planner can handle several types of exceptions. Figure 16 extends the system architecture diagram from Figure 3 and shows in solid lines the possible steps of the replanning
process. In general, when an exception occurs, the machine controller sends the planner
a message in real time detailing the exception. The planner then cancels plans that have
been created but have not been sent to the printer controller to execute. The corresponding
goals are rolled back into the unplanned queue. The planner at the same time also tries
to find the new plans for sheets that are moving in the printer to avoid further exceptions.
449

Ruml, Do, Zhou, & Fromherz

printer model (off!line
p
(
)

Planner
STN
plans
fl ibl
flexible
start time

sheet
descriptions
(on!line)

recreate goals

new plans
l
unplanned

planning

planned/unsent

Plan Manager

sent

fixed!time plans

exceptions
p

Figure 16: System architecture, showing the steps involved in nominal planning (dashed
lines) and replanning (solid lines).

The new plans when found are sent to the machine controller to replace the ones that are
executing.
Next, we discuss in detail each of the different exception types.
Plan Rejection: When a plan is sent to the machine controller to execute, the controller
may reject the plan if one of the relevant modules cannot commit to executing its requested
action at the time defined by the planner. While such rejections are rare, they can be caused
by module constraints that are outside the scope of the planner’s model. For example, a
printer engine may need time to bring the toner to the proper temperature—a state variable
and constraint not currently modeled in our system. When a plan is rejected, the planner
will cancel all plans in the unsent queue, in addition to the recently sent and rejected plan.
All goals corresponding to those plans will be rolled back to the unplanned queue. Even
plans that are not directly affected by the error message also need to be canceled and rolled
back because those plans were made after the commitments had been made for the rejected
plan.
Module Update: Machine modules can go off -line due to a hardware failure, such as a
sheet jam, a benign event, such as running out of paper in a feed tray, or an unmodeled
process, such as print engine self-adjustment. Similarly, they can come on-line when they
are repaired, adjusted, or otherwise made ready. When this happens, the module controller
will send a message to the planner indicating which of the module’s capabilities is now
on/off. If a given capability is turned off, then the planner will remove the corresponding
action from consideration in future planning episodes. If a given capability is turned on,
then the planner will add it to the action set for future planning episodes.
Break-in-Future: When a module changes the status of some of its capabilities from on
to off, currently executing or unsent plans using that module may become invalid. In this
case, the module controller will send messages to the planner indicating which plans are
450

On-line Planning and Scheduling for Modular Printers

affected. The planner will cancel the affected unsent plans and subsequent plans and move
the goals back to the unplanned queue. For plans that are executing and thus correspond to
sheets that have already been fed into the machine, the planner needs to find new plans for
the affected sheets so that they can get to the correct finisher tray without going through
the affected modules. The next section describes in detail how to reroute those in-flight
sheets.
Broken: This type of exception happens when one or more sheets are jammed in the system.
The broken messages sent to the planner include the ids of all sheets that are jammed and
thus cannot be reused or rerouted because of the failure. When some sheets jam, they
normally also disable some modules and thus a broken message is normally accompanied
by several module update messages, which are described above. The handling of the broken
exception is similar to the handling of the break-in-future exception in many respects: it
involves canceling of unsent plans and finding new plans for the in-flight sheets. However,
the main differences are: (1) in-flight sheets that were jammed cannot be rerouted; and (2)
more critically, the jammed sheets break print job integrity. We discuss this in detail next.
5.3 In-flight Sheet Replanning
In this section, we discuss the problem of finding a new set of plans for in-flight sheets when
a sheet is jammed or a module to be used by some plans is broken. The constraints that
make replanning more challenging than nominal planning are:
• Sheets cannot stop or slow down while the planner searches for new plans for all inflight sheets. Thus, if the planner takes too much time to find new plans, the jams
and/or module failures will cascade.
• All newly found plans do not have flexible starting times as in the nominal planning
case, but should all start from the location where the sheets are projected to be when
the plans are found. The new locations depend on the actual replanning time of the
planner.
• Any in-flight sheets occurring later in the same print job as a jammed sheet should
be rerouted to a purge tray. The sheets from jobs without jammed sheets still need
to finish in the correct finisher tray and in order.
Replanning involves four main steps: (1) create new goals for the in-flight sheets; (2)
predict (an upper bound on) the replanning time; (3) project the sheets according to the
original trajectory and the predicted planning time to find their future locations, which will
form the new initial state of the replanning problem; (4) find plans for all sheets that are
salvageable (those for which it is possible to avoid broken modules and jammed sheets in
time), satisfying the constraints listed above.
5.3.1 Example
Here we provide a concrete example illustrating our replanning procedure. Figures 17 & 18
show a scenario in which there are three in-flight sheets: S1.1 and S1.2 belong to the same
print job and were planned to go to finisher 2 (in the middle); sheet S2.1 belongs to a
451

Ruml, Do, Zhou, & Fromherz

1.2
1.1

Finisher 1
Finisher 2
P
Purge
tray

2.1

Figure 17: Replanning Example (before jam): sheet 1.1 and 1.2 are planned to enter finisher
2, and sheet 2.1 to finisher 1.

1.2
1.1

Finisher 1
Finisher 2
Purge Tray

2.1

Figure 18: Replanning Example (after jam): sheet 1.1 is jammed, which requires the planner to reroute sheet 1.2 to the purge tray and reroute sheet 2.1 to circumvent
the jammed sheet before going to finisher 1.

different print job and is scheduled to go to finisher 1. The third finisher is the purge tray.
The original routes are indicated by the dashed lines in Figure 17. Assume that S1.1 is
jammed. According to the original routes, we have: (1) S1.2 will arrive in the finisher tray
out-of-order (because S1.1 did not arrive before it); (2) S2.1 will crash into the module where
S1.1 jammed. Therefore, we need to find new plans for those two sheets so that S1.2 will
instead go to the purge tray and S2.1 goes around S1.1 . Finding those plans takes time and
given that we cannot stop or slow down S1.2 and S1.2 while finding the new plans for them,
those two sheets will continue their original trajectories to the new locations, which are
452

On-line Planning and Scheduling for Modular Printers

sheet 1
plan 2
plan 1

...
plan 3

sheet 2

plan 4

...
plan 1

plan 2

plan 3

Figure 19: Chaining many searches together gives a search tree with potentially infinite
branching factor.
circled in Figure 18. From there, the machine controller will apply the new plans, indicated
in the figure by solid lines, in order to guarantee print-job integrity while avoiding further
cascading failures. After the replanning is done, the planner will generate fresh plans to
re-create S1.1 and S1.2 .
The example above shows one replanning strategy where the new goal for out-of-order
sheet S1.2 is set to go to the purge tray. This is the default strategy in our replanner that tries
to clear out the machine and finish the replanning process as quickly as possible to return
to normal operation. However, there are scenarios where the printing media is expensive
or the content is confidential and purging sheets is not desirable. In those scenarios, we
have also experimented with a different strategy that does not purge S1.2 but keeps it in
the machine (for example, by looping it in a holding pattern) while waiting for S1.1 to be
reprinted, then S1.2 is routed to the original finisher. The only modification necessary to
implement this strategy in our system is to change the way replanning goals and end-time
constraints are generated. We have tested this strategy successfully for a small number of
sheets, although more sheets could be saved if one were allowed to slow down the transports.
5.3.2 Chained BFS
For normal operation, the planner uses A* to find the plan for a given sheet that can end
soonest, given the (temporally flexible) plans for the previous sheets. A plan always exists
if scheduled sufficiently far in the future. For rerouting, the problem is different. We must
find jointly feasible plans for as many in-flight sheets as possible. We cannot greedily plan
one sheet at a time, committing irrevocably to the plans for all previous sheets, because the
plan selected for one sheet might render subsequent sheets infeasible. This cannot happen
during nominal planning, as later sheets are always feasible when scheduled sufficiently far
in the future. When replanning, however, we are forced to confront a true multi-body
planning problem.
We considered two strategies to solve this problem. The first was to simply plan in the
joint action space of all sheets. This would result in a large branching factor and it was not
clear to us how to design an effective heuristic evaluation function. We chose a different
approach, in which we can retain the view of planning for each sheet individually using
heuristic search. However, we overlay an additional search on top of this, as depicted in
Figure 19. In the high-level search, a branching node represents the situation in which we
have selected certain specific plans for all previous sheets and it is time to select a plan for an
453

Ruml, Do, Zhou, & Fromherz

ChainedBFS (problems)
1. if problems is empty, return success
2. p ← remove first problem from problems
3. initialize openlist for p
4. repeat until openlist is empty or node limit is reached:
5.
n ← best node on openlist
6.
if n is a goal, call ChainedBFS with remaining problems
7.
expand n, adding any children to openlist
Figure 20: Sketch of Chained Best-First Search with a depth-first strategy.
additional sheet. The children of that node represent commitments to the different possible
plans for that additional sheet. By considering different paths in the high-level search
tree, we can consider different combinations of plans for the different sheets. We call this
approach chained best-first search. In our current implementation, sheets are replanned in
their original order, as an approximation of increasing ‘distance from exit,’ which correlates
with increasing flexibility. An alternative approach is to replan in the order of ‘urgency’
defined as the time left to reroute a sheet before it becomes unsalvageable.
Because the children of a node represent the possible plans returned by a best-first
search, the children are not available all at once. Instead, an individual sheet-level planning
search will encounter goal nodes one at a time. We cannot terminate the search when we
find the first goal node for single-sheet planning because we have no guarantee that the
sheet-plan reaching that first goal will make the subsequent sheets feasible. Finding a plan
for a single sheet merely results in a new branch in the high-level space, and to retain
completeness we must retain the ability to continue our search and uncover additional
possible plans. In fact, in printers such as ours that contain loops in the paper path, there
may be an infinite number of possible plans for a given sheet. Fundamentally, the highlevel search must explore a tree where nodes are expanded incrementally and the branching
factor is potentially infinite.
We identified three possible strategies for searching a tree with infinite branching factor.
The first is a best-first approach, in which one formulates a traditional heuristic evaluation
function for the high-level nodes. These nodes represent commitments to complete plans for
a subset of the in-flight sheets, so the heuristic function needs to estimate the probability
that those plans will allow feasible plans for the remaining sheets to be found. The infinite
branching factor could be handled using Partial-Expansion A* (Yoshizumi, Miura, & Ishida,
2000), although this would require a non-trivial lower bound on the heuristic value of the
plans that have not yet been found. It was not clear to us how this might be done. The
second possible strategy we considered was limited discrepancy search (Korf, 1996). Unlike
depth-first search, limited discrepancy search doesn’t necessarily visit all the children of
a node, which are potentially infinite for us. The disadvantage to this method is that,
because we revisit each node many times with different discrepancy bounds, we will suffer
considerable node regeneration overhead.
The third strategy, and the one we used in our implementation, is perhaps the simplest:
depth-first search. Figure 20 shows a pseudo-code sketch. Because we have a fixed number of
sheets to replan, the high-level search tree has bounded depth. To cope with the potentially
454

On-line Planning and Scheduling for Modular Printers

infinite branching factor, we impose a limit on the number of nodes each low-level sheet
planning search may expand. This avoids the danger of searching forever at one high-level
node without finding another goal, and is reminiscent of iterative broadening (Ginsberg
& Harvey, 1992). To guide the sheet-level planning, we use a heuristic that minimizes
plan duration. This attempts to minimize resource use in the machine and maximize the
probability that other sheets will have feasible plans.
5.4 Evaluation
Until now, the exception handling strategies in current production printers have been to:
(1) stop the production and ask the operator to remove all sheets or (2) use machine-specific
customized local rules to purge sheets in the system. Our work is the first to demonstrate
automatic exception handling that does not rely on machine-specific control rules.
The planner can handle the two easiest types of exception: Plan Reject and Module
Update without any difficulties. For the Break-In-Future and Broken exceptions, we can
currently reroute on the fly up to five sheets for the machine shown in Figure 1. This
number may seem low, but recall that replanning is harder than nominal planning by a
factor exponential in the number of in-flight sheets. For the simpler prototype systems at
Xerox with fewer (but larger) modules, four print engines, and an aggregate throughput of
180 pages-per-minute, our planner has been able to successfully reroute all reroutable sheets
when different jams happen. We have demonstrated our replanning technology in real-time
by allowing people come up and either turn on/off modules, or jam sheets intentionally,
sometimes right before sheets hit the broken module. Upon receiving the error messages
from the machine controller, the planner is fast enough to reroute the sheets around the
failed modules or jammed sheets to the correct locations. In addition to experimenting
with the physical hardware built at PARC and by Xerox, we have also tested replanning in
simulation, by connecting the planner to a visualizer instead of the machine controller. The
on-line appendix contains two videos of the planner performing in-flight rerouting on the
PARC prototype, both in simulation (Video 3, ‘replanning simulation’) and in hardware
(Video 4, ‘replanning hardware’).
In addition to testing our replanning framework on different hypothetical printer configurations and different fault modes, we have also investigated different exception handling
strategies. For example, when the printing media is expensive and the replanning objective
function is switched from the default objective function of finish replanning as quickly as
possible (which can lead to many purged sheets) to saving as many sheets at possible (which
can lead to longer replanning time) then the planner has been able to successfully route
up to 2 out-of-order sheets in long routes (that may contain loops) in the system waiting
for the jammed sheet to be printed before being routed to the correct finisher tray. While
achievement of replanning for up to five sheets in a large RMP machine may not seem very
impressive, we want to point out that: (1) our planner can reroute all reroutable sheets in
simpler machines (which is still much more complex than the biggest multi-engine printer
Xerox currently has on the market); (2) the large machine is very complex for automated
planning—the last two IPC winners SGPLan and LPG cannot even find plan for a single
sheet in nominal planning using the PDDL2.1 version of our printer domain.
455

Ruml, Do, Zhou, & Fromherz

6. Handling Multiple Objectives
Our second major extension to nominal planning is aimed at better meeting shop owner’s
needs in the nominal case. Up to this point, the planner’s objective has been to run
multi-engine reconfigurable printers at full productivity, optimizing for machine throughput.
Productivity, while very important, is only one of the many optimization criteria that
naturally exist in real-world planning and scheduling applications like the printer control
domain. In this section, we will describe several additional objective functions that were
pointed out as important by our industrial partner, and discuss how we extended our
planning framework to handle them.
In a modular system with multiple print engines, one might want to optimize the cost of
printing by choosing to print black-only pages only on monochrome engines and avoid using
more expensive color engines. Also, one might want to optimize image quality by choosing to
print pages from the same document only on print engines whose current marking gamuts
are similar. The printer controller needs to give operators the ability to trade off these
conflicting objectives while maintaining robust operation. We meet these challenges using
(1) an optimization objective that combines separate estimates of productivity and printing
cost, and (2) multiple heuristic look-ups to efficiently handle image quality consistency
constraints. In contrast to an explicit multi-objective optimization, in which a planner
would return an selection of non-dominated solutions on the Pareto frontier, presumably
for a human to choose from, our planner needs to select a single solution for execution, so
we need to combine the multiple criteria into single objective. Because our planner is built
atop generic state-space heuristic search, we need only design a new comparison function
to order search nodes. In addition to linear combinations of objectives, it is relatively easy
for us to handle tiered criteria using tie-breaking strategies.
There are several academic domain-independent planners such as GRT (Refanidis &
Vlahavas, 2003) and LPG (Gerevini, Saetti, & Serina, 2008) that can optimize for multiple objectives or trade-off between planning time and plan quality. Standard planning
languages, especially PDDL3 (Gerevini & Long, 2006), allow specifying complex objective
functions in the weighted-sum format (as in our framework). While our planner is also based
on domain-independent planning technology and uses an extension of PDDL, it works in
a dynamic on-line continual environment and interacts with a physical machine, not in an
off-line abstracted environment like previous planners.
6.1 Optimizing for Printing Cost
For systems with heterogeneous print engines, the cost of printing a given page depends on
which of the engines is used. For example, it is generally costlier to print a black-and-white
page on a color engine than a monochrome one. Thus, to minimize the overall printing cost,
one should use the engines with the lowest printing cost that still satisfy the image type
and quality requirements of a given print job. By doing so, only a subset of all the available
engines will be used for printing a job and thus the overall productivity may be reduced.
To strike a balance between machine productivity and printing cost, we have implemented an objective that can trade off productivity for cost and vice versa. We show that
by combining different performance criteria into a single objective, the same optimization
framework that works so well for single-objective planning can be efficiently applied to
456

On-line Planning and Scheduling for Modular Printers

the multi-objective case. Below are the main steps required to extend the planner from
supporting a single objective to multiple objectives.
Step 1: Extend the planner’s representation of machine capabilities to model action cost.
Specifically, we added a cost field representing the cost of executing each capability. In
addition, there is an overall objective field with user-supplied weights for each of the two
objectives: obj = min w1 ∗ t + w2 ∗ c, where t is the end time and c is the accumulated total
cost of printing all sheets.
Step 2: Create one heuristic estimation function for each of the objectives. To find the
best route for a given sheet, we estimate how good a potential route is according to each
of the objective functions. Finishing time is estimated using a temporal planning graph
adjusted with resource conflicts, as described in Section 4.3.4. To estimate the total plan
execution cost, we use dynamic programming starting from the initial state (i.e., sheet in
the feeder) to compute the total cost to reach different reachable states. The computation is similar to cost propagation on the planning graph as in the Sapa planner (Do &
Kambhampati, 2002).
Step 3: Extend the search algorithm to considering multiple objectives simultaneously.
The estimations on total time and cost are combined using the user-supplied weights (as
described in Step 1) to compare nodes in the best-first A* search algorithm. Given that both
heuristics for time and cost are admissible, like the single objective planner, our planner is
guaranteed to find an optimal solution for any given sheet. If the weights are not given, the
planner chooses to prioritize the objectives. For example, the planner can first find the plan
that has the lowest cost, and then break ties favoring plans with higher productivity, then
favoring one with lower wear and tear, and so on. This mechanism has been implemented
and fully integrated into our planner. The default option when no weights are specified is
to optimize for productivity and break ties on total cost.
6.2 Planning for Image Quality Consistency
Maintaining image consistency across a set of heterogeneous print engines is especially
important for a multi-engine printing system. The planner achieves this by enforcing additional image-consistency constraints while searching for an optimal plan. In color science,
the (in)consistency of two colors is measured by a function, often denoted ∆E, that calculates the distance between the two in some device-independent color space. While there
exist a variety of such functions (the most popular of which is called ∆E2000; see Green,
2002), for our planning purpose it suffices to assume that given any two engines, a ∆E
function returns a non-negative real-valued scalar, called ∆E distance, that measures the
discrepancy in perceived color as a result of printing the same image on these two engines.
Because facing pages (i.e., pages that face each other in a bound book or magazine) are
most sensitive to image-consistency issues, we thus consider the following constraints in our
planner:
1. facing-page constraints that require the facing pages of a job be printed by the same
print engine
2. ∆E constraints that allow only engines within some maximum ∆E distance to print
facing pages
457

Ruml, Do, Zhou, & Fromherz

Given that in reality no two engines can have a ∆E distance of zero, the facing-page
constraints can be viewed as a special case of the ∆E constraints with the maximum ∆E
distance set to zero. Thus, we only need to focus on the latter, which is more general.
To enforce ∆E constraints, the planner keeps track of the set of print capabilities that
can be used to print the front side of a sheet, which is constrained by the print action
applied to the back side of its previous sheet. Since the first sheet of a job does not have a
previous sheet, the set of print capabilities eligible for printing its front side is unconstrained
(i.e., equal to the entire set of print capabilities). For subsequent sheets of the same job,
however, only a subset of print capabilities is allowed. Such a subset is computed based
on the ∆E constraints by including only capabilities of those engines whose ∆E distance
to the print engine that printed the back side of the previous sheet is less than or equal
to some maximum distance. In most cases, this has to be determined on-line, because the
∆E distance between a pair of engines can drift over time. Thus, our planner maintains an
on-line version of a pairwise ∆E-distance matrix for all the engines in a printer.
While adding extra image-consistency constraints can reduce the brute-force search
space (if the constraints make the set of reachable states smaller), in practice we found
this often makes the search problem harder, because the heuristic computed for the unconstrained problem, while still admissible, is no longer informative. To improve the accuracy
of the heuristic, the planner computes the temporal planning-graph heuristic for all legal
combinations of print capabilities that can be used to print one side of a sheet, and then
stores them in multiple lookup tables, one for each combination. When a heuristic estimate
for a search node is needed, the planner calculates an index into the lookup table based
on the state description (e.g., sheet location, monochrome or color printing), in much the
same way as lookups are done in pattern databases (Culberson & Schaeffer, 1998). In our
implementation, a hash table of hash tables is used to store multiple lookup tables, but for
any given sheet only the relevant hash table(s) is loaded before the sheet is being planned,
because the set of eligible print actions is known and fixed at that time.
Since there are only a limited number of ways of printing a single face of a sheet, this
approach to improving heuristic accuracy has little overhead yet can significantly reduce
the time it takes to find an itinerary. Interestingly, the same approach can also be used
to improve the accuracy of the heuristic for handling exceptions in which jammed sheets
block the paper paths to some engines, because then only unblocked engines are eligible for
printing sheets, creating planning problems that are similar to enforcing the ∆E constraints.
For example, one can set the ∆E distance to any blocked engine as infinity, which effectively
forces sheets to go through only unblocked engines, and the computational savings comes
from the use of a more accurate heuristic that is built specifically for a particular set of
unblocked engines, instead of a nominal-case heuristic that assumes no engine is blocked.
6.2.1 Planning with a Constrained Action Set
From an algorithmic perspective, our approach to planning for image-quality consistency
corresponds to solving a constrained planning problem with a reduced set of actions (compared to its unconstrained version). Given a planning problem with k actions, one can create O(2k ) different versions of the constrained problem. Thus, pre-computing the temporal
planning-graph heuristic for all possible subsets of actions can quickly become infeasible as
458

On-line Planning and Scheduling for Modular Printers

k increases. Here we describe a general solution that strikes a balance between heuristic
accuracy and the space overhead for storing multiple lookup tables, one for each subset of
the actions. The idea is to limit m, the maximum number of actions that are removed from
the unconstrained problem, and compute heuristic lookup tables only for those constrained
problems. For example, it is usually feasible to enumerate those constrained problems in
which only one or two actions are removed from the action set. To compute the heuristic
value of a state in a constrained problem that is not included in this pre-computed set, the
algorithm consults all the lookup tables whose removed actions form a subset of the actions removed in the constrained problem, and returns the maximum value as the heuristic
estimate of the state, since the value returned by any of the lookup tables is admissible.
More formally, let h(s|P ) be an admissible heuristic estimate for state s in the constrained problem with the set of actions P ⊆ A removed from the original action set A, and
let m be the maximum number of actions removed in any constrained problems for which
the heuristic is pre-computed. The heuristic estimate h(s|P ) can be calculated as follows,
h(s|P ) =

(

h(s|P )
maxQ⊂P

∧ |Q|=m

if |P | ≤ m
h(s|Q) otherwise

The new heuristic resembles the hm family of admissible heuristics (Haslum & Geffner,
2000), where m limits the maximum cardinality of the set of atoms considered in the
construction of the heuristic. The difference is that our heuristic considers the set of removed
actions, whereas the hm heuristic considers the set of satisfied atoms. Our heuristic can also
be seen as a kind of multiple pattern database (Holte, Felner, Newton, Meshulam, & Furcy,
2006) in which one can take the maximum over a set of heuristic estimates without losing
admissibility, although ours is based on action-space abstraction and (multiple) pattern
databases are based on state-space abstraction.
6.3 Evaluation
To test the ability of our planner to trade off between machine productivity and printing
cost, we have tested on the model of a four-engine prototype printer built at Xerox. This is a
better test bed for this trade-off investigation because that printer has a mixed set of printer
engines (two color and two black-and-white engines) instead of four identical black engines
such as in the PARC prototype system. Moreover, the engines are aligned asymmetrically
and thus the paths leading to different engines are slightly different. We have modeled the
costs for all different components in consultation with Xerox engineers. We are especially
interested in modeling the cost to print black pages on different engines: printing them
on more expensive color engines costs more than on cheaper monochrome engines. By
varying the weights between the two objective functions, we have been able to show that:
(1) increasing the weight given to productivity results in more printer utilization of all four
engines; (2) increasing the weight on saving printing cost leads to reductions in the number
of unnecessary costly printing, thus fewer black sheets are printed on color engines. We can
observe the trade-off between modules with similar functionality as well, such as between
different feeders, finishers, or paper paths. For example, increasing the weight for saving
costs lowers the number of sheets fed from a more expensive but faster feeder. We have
also tested our search on other hypothetical printers with mixed components and similar
459

Ruml, Do, Zhou, & Fromherz

results were observed. We also observed that moving from single to multiple objectives did
not slow down our planner and thus did not affect the overall productivity.
We also tested the performance of our planner on image-consistency planning. The
model of the printer used has four monochrome engines, two of which are faster but lowquality engines, and the remaining two are slower but high-quality engines. All four engines
are connected through asymmetric paper paths. We ran the simulation with a 20-sheet
job that requires using the two high-quality engines for double-sided printing. This can be
done with certain ∆E constraints, which can prevent the planner from choosing the two
low-quality engines. Since we are particularly interested in the effect of the heuristic on the
search performance, we tested the planner with and without using multiple lookup tables,
which made a significant difference in the number of node expansions in A* search and
planning times. On average, when the multiple lookup table heuristic is used, the planner
expands only 1783 nodes per sheet; whereas using the heuristic computed for the unconstrained problem, which grossly underestimates the remaining makespan for constrained
problems, needs 6458 node expansions to find a plan. In terms of running time, the one
that uses multiple lookup tables is 60% faster than using the naive heuristic.
One future direction is to investigate a different objective entirely: wear and tear. Under
this objective, one would like the different machines in the plant to be used the same amount
over the long term. However, because machines are often cycled down when idle for a long
period and cycling them up introduces wear, one would like recently-used machines to be
selected again soon in the short term. Although our implementation currently only supports
throughput and cost, it should be easily extensible to support additional objectives.

7. Deployment
In the process of building and deploying the planner, we utilized many off-the-shelf techniques from academic research in planning, extending, and integrating them to form a fast
on-line planner/scheduler. In this section, we list the most important lessons we learned
and describe the ancillary tools that were necessary to develop and deploy the planner. We
hope that they can be useful for both application developers and academic researchers in
planning.
7.1 Lessons Learned
Modeling is important. We mean this in two respects. First, it was important to our
end-users that printers were modeled with a specialized representation in which machine
modules and the connections between them are the main themes of the language. As
we discussed in Section 4.1, this representation is then compiled into the planner input
language, taking the capabilities of different modules along with their inter-connections and
producing action schemata. In this process, the set of machine capabilities are compiled
into a higher number of action schemata that ground some of the parameters. Through
discussion with our users and industrial partners, we feel that the machine-centric language
involving modules, machine instances, and inter-connections is easier for them to understand
and accept, while the compiled-down representation makes it much easier for us to adopt
STRIPS planning techniques.
460

On-line Planning and Scheduling for Modular Printers

Second, we also found that, because we understood the search algorithm (regression with
three-value state representation) and the heuristics (planning graph with mutexes) used by
the planner, we could manipulate the modeling of the actions, goals, and initial states to
produce quite different computational results. Consider a simple example of the action a12 =
move(l1 , l2 ) that moves an object from location l1 to l2 . The most common form of STRIPS
representation of this action is P re(a12 ) = { at(l1 )} and Effect(a12 ) = {¬at(l1 ), at(l2 )}.
Recall that we use the three-value representation in which a literal can have values true,
false, or unknown. Regressing the (partial) state s1 = {at(l2 )} using a12 will get us to state
s2 = {at(l1 ), U nknown(at(l2 )) which is regressable through both actions move(l3 , l1 ) and
move(l3 , l2 ). While the normal regression rules may not consider move(l3 , l2 ) because it
will not lead to an optimal length plan, regressing s2 through move(l3 , l2 ) will not cause
inconsistency. In fact, in a domain like ours, we do branch over all regressable actions
because there are scenarios in which we need to buy time to free up resource allocations.
Note that in this example, sophisticated techniques to discover invariants such as TIM (Fox
& Long, 1998) or DISCOPLAN (Gerevini & Schuert, 1998) can discover that the object
can only be at a single location at any time and thus that s2 is not regressable through
action move(l3 , l2 ). However, we can eliminate that branch simply by adding a precondition
¬at(l2 ) to the action description of a12 and make sure that in the goal state, any location
other than the goal is false. Generating an extra child node and propagating all constraints
in our domain is expensive because, in addition to a logical part, our state representation
includes temporal and resource databases. So cutting down on the number of generated
nodes is important, and this can be partially accomplished by careful modeling.
Similarly, adding or removing predicates from the domain description can have a great
effects on the heuristic estimation derived by the planning graph through mutex propagation. We experienced a scenario in which adding two extra predicates representing subgoal
completion and modeling the domain slightly differently achieved a speedup of nearly 10x
for some printer configurations.
These manipulations are reminiscent of the work of Rintanen (2000), who showed how
domain advice expressed in linear temporal logic, such as ‘don’t move a package that is at
its destination’, could be compiled into the planning operators of a domain using conditional
effects, leading to great gains in planning efficiency. However, we want to emphasize that in
a highly configurable systems like ours, it can be dangerous to encode explicit action choices
or pruning into the domain. It can be hard to guarantee that completeness or optimality will
be maintained under all possible job mixes or failure combinations. (For example, looping
a sheet may free up a resource that will allow the job to complete earlier.) Our approach
is to encode the domain ‘physics’, that is, things that are universally true in the domain
and help keep the search within reachable states, but not any control rules in the sense of
heuristic action selections, such as ‘when condition, choose action.’ Our point is that the
same physics can be represented differently, even if limited to STRIPS, and finding the right
match with the chosen search strategy can dramatically affect the planner’s performance. As
application developers, not having to work with a fixed benchmark domain representation
allows us to exploit another dimension in modeling to improve our planner’s performance.
The most suitable planning algorithm depends on the application specifications.
Even after formulating our domain using an extension of STRIPS, we went through several
461

Ruml, Do, Zhou, & Fromherz

implementations of different planning algorithms before settling on the current one. Our first
version was a lifted partial-order planner, which we still think is the more elegant algorithm.
We then implemented a grounded forward state space planner, because that approach has
dominated the planning competitions. However, as discussed in Section 3.1, we realized
that a combination of the constraint that sheets in a same print job should be finished in
order and our objective function of minimizing the finishing time is not suitable for forward
state-space search. We finally settled on a backward state-space framework, which is much
faster in our domain. The lesson we drew from this is that just because some approach works
best in a wide range of benchmark domains in the competition does not mean that it is the
best choice for a given application; and if it doesn’t work, it does not mean that other less
popular approaches cannot do significantly better. Therefore, understanding your domain,
the important constraints involved, your objective function, and how different planning
algorithms work can help in selecting the most suitable strategy. Looking up competition
results is not a replacement for understanding the variety of applicable planning algorithms.
Having a fast and robust temporal reasoner is very important. In our planner,
even though the source code for the Simple Temporal Network (STN) totals less than 200
lines of code, it is critical in handling all temporal relations between actions and resource allocations within a single plan and between different plans. In a real-world application where
there are various temporal constraints and delays to take into account, such as communication, setup time, machine controller coordination, and time synchronization delays between
the planner and the other components in the overall control architecture, ensuring temporal
consistency is one of the most important tasks necessary for keeping the planner running
without interruption for a long period of time. Having an explicit temporal reasoner also
helped us to uniformly represent and manage start and end times of actions and different
types of resource allocations. It also allowed us to smoothly extend from handling fixed
duration actions to action with variable durations, and extend from regular resource allocations to resource allocations caused by external events such as cyclic resources allocation by
uncontrollable processes. In our domain, variable action durations are context-independent
and are different from actions such as refuel in the Logistics domain used in the competition.
We haven’t noticed many planners in the competition having an explicit general-purpose
temporal reasoner, except IxTeT(Ghallab & Laruelle, 1994). However, we would like to
emphasize that in a real-world setting in which the planner needs to coordinate with other
software and expects to face various time constraints and delays, this is critical.
There are many uses for a planner. Besides its main job of controlling different
printers, the planner has also been used extensively for system analysis purposes. Thus,
the planner is tested against (1) different printer designs to help decide the better ones;
(2) printers with various broken modules to test the reliability of each printer. Those
analyses can help the product group to decide which printer to built for a given purpose.
For example, our customer ran an extensive test consisting of 11,760 different planner runs
for variations of a single printer configuration. Among those runs, they used the planner to
test different combinations of possible broken points, different print-job mixes, and changed
speeds of different modules. Another use has been to test the performance of the upstream
job submission and sequencing methods. The most direct and accurate way to evaluate a job
sequencer is to run a long print-job mix (thousands of sheets or more) through the planner
462

On-line Planning and Scheduling for Modular Printers

and measure the total makespan. Recently, we completed a print-job mix of 50,000 sheets
without any break, which is more intensive than the regular real-life printer operation.
Through these experiences, we learned that there are many potential applications of a
planner beyond direct machine control.
Exception handling. Given that the planner interacts with other parts that are either
higher or lower in the control hierarchy, exceptions can come in many forms. We believe
that similar exceptions would occur in most applications where the planner interacts with
physical world. While robust exception handling (such as replanning) is important, we
found that there is much less research on this topic compared to other branches of domainindependent planning.
We hope that these observations can help researchers to develop planning techniques
that are closer to those needed in real-world applications and that they are also useful for
those considering deploying AI planning in their applications.
7.2 Ancillary Tools
In the course of building our system, we developed a number of ancillary tools around our
core planning and scheduling software. Among these tools, the most notable piece is the
visualizer, which simulates the movement of each sheet inside the printer in real time. Like
the planner, the visualizer adopts the same model-based principle to make it as machineindependent as possible. Because an itinerary is given as a discrete sequence of actions, each
having a single time stamp that prescribes the start time of the action, linear interpolation
is used to compute the position of a sheet when the current simulation time is somewhere
in between the start times of two consecutive actions. The visualizer works in one of the
following two modes: the on-line mode that accepts live itineraries sent by the planner over
sockets, and the off-line mode that reads in previously recorded itineraries from a file stored
on disk.
To separate the visualization engine from the specific designs of a printer, we developed
a simple module definition language for describing the dimensions of each module type,
the locations of input and output ports within a module’s local coordinate system, the
travel distance between a pair of input and output ports, and optionally a customized
drawing function that can be used to render the type of modules on the screen. Besides the
definition of module types, the visualizer needs to know the location as well as the orientation
of each module in a machine-wide coordinate system. While it is possible to specify all
these information manually, we developed another ancillary tool called the visualizer preprocessor that can be used to automate this laborious yet error-prone task. With this tool,
the user only needs to specify the location and orientation for one module, called a seed
module, from which the locations and orientations of all the other (directly or indirectly
connected) modules are deduced based on the connectivity graph of the modules. For
machines with more than one feasible configuration, our tool can find all possible solutions
and store them in multiple files that can be used later by the visualizer. Besides the nominal
case, the visualizer can also simulate various exceptions such as paper jams and break-infuture scenarios. Our long-term vision is for the visualizer to become a design, debug, and
verification tool for the manufacturer, as well as a GUI console for the end user who operates
the printer.
463

Ruml, Do, Zhou, & Fromherz

To make it easier to run tests on our modular printers, we also developed a wrapper
program that ‘glues’ together the planner and the controller (or the visualizer). It takes
as input a set of pre-defined test scenarios specified with succinct syntax (e.g., 10sc means
print 10 single-sided color sheets). To support the simulation of pre-fabricated exceptions,
it sends special messages to the visualizer that contains information about when or where
a sheet jam should occur. It also supports simultaneous printing jobs for printers with
multiple finishers, and uses a round robin algorithm to draw sheets from jobs at the same
rate to maintain fairness. To facilitate remote testing and debugging, the wrapper program
uses sockets to communicate with the machine controller (or the visualizer).

8. Conclusion
We described a real-world domain that requires a novel on-line integration of planning and
scheduling and we formalized it using a temporal extension of STRIPS that falls between
partial-order scheduling and temporal PDDL. We presented a hybrid planner that uses
state-space regression on a per-sheet basis, while using a temporal constraint network to
maintain flexibility through partial orderings representing resource conflicts between plans
for different sheets. Our system has successfully controlled three hardware prototypes and
outperforms state-of-the-art planners in this domain. No domain-dependent search control
heuristics are necessary to control a printer composed of 170 modules in real time. We
described extensions to handle two critical issues: (1) real-time execution failures; and
(2) objective functions beyond productivity. We have successfully demonstrated our fast
replanning and multiple objective handling on three physical prototype printers and many
other potential printer configurations in simulation.
Our work provides an example of how AI planning and scheduling can find real-world
application not just in exotic domains such as spacecraft or mobile robot control, but also
for common down-to-earth problems such as manufacturing process control. The modular
printer domain is representative of a wider class of AI applications that require continual
on-line decision-making. Through a novel combination of fast continual temporal planning
techniques, we have shown how artificial intelligence can successfully enable robust, highperformance, autonomous operation without hand-coded control knowledge.

Acknowledgments
Much of this work was done while the first author was with the Palo Alto Research Center.
Preliminary results from this project were published by Ruml, Do, and Fromherz (2005),
Do and Ruml (2006), and Do, Ruml, and Zhou (2008) and summarized by Do, Ruml, and
Zhou (2008). The authors would like to thank the members of the Embedded Reasoning
Area at PARC, especially Lara Crawford, Haitham Hindi, Johan de Kleer, and Lukas Kuhn,
as well as Danny Bobrow, David Biegelsen, Craig Eldershaw, and Dave Duff for their help
and contributions to the project. Our industrial collaborators not only provided domain
expertise but were invaluable in helping us to simplify and frame the application in a useful
way. We’d like to especially thank Bob Lofthus and Ron Root for their enthusiasm and
perseverance and Steve Hoover for supporting the project.
464

On-line Planning and Scheduling for Modular Printers

Appendix A: Video
The on-line appendix on the JAIR website contains four movies of the system in action:
1. nominal-simulation.mp4: shows one simplex job of 200 sheets being run in a simulation of the PARC prototype printer shown in Figure 1. The planner keeps all four
print engines busy, achieving full productivity of the system.
2. nominal-hardware.wmv: shows two simplex jobs being run simultaneously using all
four engines of the PARC hardware prototype. The two feeders are on the left and
two simple finishing trays are on the right. Red lights on the machine modules show
the position of sheets. (Background time synchronization is indicated by the periodic
blinking.) In the lower left corner, a schematic visualization shows how sheets are
moving through the machine, with one job colored blue and the other red.
3. replanning-simulation.mp4: show a simple exceptions handling scenario in simulation. Blue sheets and red sheets belong to different jobs. The second sheet of the blue
job jams. The third sheet, already in-flight, is rerouted to the middle ‘purge’ tray and
fresh plans are initiated to recreate both sheets. The red job continues uninterrupted.
4. replanning-hardware.wmv: demonstrates two exception handling scenarios. The
first shows simple on-line replanning. After a sheet has launched, a button is pushed
on a module that the sheet is headed toward, to mark the module as ‘broken.’ This
initiates replanning, and the sheet is routed around the ‘failed’ module. A second
module’s button is pushed, marking it failed and thereby blocking the finishing tray
that the sheet was headed toward. The sheet is rerouting again and emerges at the
remaining finishing tray.
In the second scenario, the module that is ‘broken’ already contains the first sheet of
a two-sheet job. The replanner is fast enough to reroute the second sheet around the
jammed first sheet to a purge tray. The original two sheets are then planned again
from scratch and arrive at the lower finishing tray.

References
Baptiste, P., & Pape, C. L. (1995). A theoretical and experimental comparison of constraint
propagation techniques for disjunctive scheduling. In Proceedings of IJCAI-95, pp.
600–606.
Barták, R. (2002). Visopt shopfloor: On the edge of planning and scheduling. In Proceedings
of the Conference on Principles and Practice of Constraint Programming (CP-02), pp.
587–602.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research,
11, 1–91.
Cervoni, R., Cesta, A., & Oddi, A. (1994). Managing dynamic temporal constraint networks.
In Proceedings of AIPS-94, pp. 13–18.
465

Ruml, Do, Zhou, & Fromherz

Chen, Y., Hsu, C.-W., & Wah, B. (2006). Temporal planning using subgoal partitioning
and resolution in sgplan. Journal of Artificial Intelligence Research, 26, 323–369.
Chien, S. A., Knight, R., Stechert, A., Sherwood, R., & Rabideau, G. (1999). Using iterative repair to improve the responsiveness of planning and scheduling for autonomous
spacecraft. In Proc. of IJCAI.
Chleq, N. (1995). Efficient algorithms for networks of quantitative temporal constraints. In
Proceedings of Constraints-95, pp. 40–45.
Crawford, L., Hindi, H., Zhou, R., & Larner, D. (2009). Synchronized control in a large-scale
networked distributed printing system. In Proceedings of 2009 IEEE International
Conference on Robotics and Automation (ICRA-06).
Culberson, J., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence, 14 (3),
318–334.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49, 61–95.
desJardins, M. E., Durfee, E. H., Ortiz, Jr., C. L., & Wolverton, M. J. (1999). A survey of
research in distributed, continual planning. AI Magazine, 20 (4), 13–22.
Do, M., Ruml, W., & Zhou, R. (2008). On-line planning and scheduling: An application to
controlling modular printers. In Proceedings of AAAI-08.
Do, M. B., & Kambhampati, S. (2002). Sapa: a multi-objective metric temporal planer.
Journal of Artificial Intelligence Research, 20, 155–194.
Do, M. B., & Ruml, W. (2006). Lessons learned in applying domain-independent planning
to high-speed manufacturing. In Proceedings of ICAPS-06, pp. 370–373.
Do, M. B., Ruml, W., & Zhou, R. (2008). Planning for modular printers: Beyond productivity. In Proceedings of the Eighteenth International Conference on Automated
Planning and Scheduling (ICAPS).
Fox, M., Gerevini, A., Long, D., & Serina, I. (2006). Plan stability: Replanning versus plan
repair. In Proc. of ICAPS-06, pp. 212–221.
Fox, M., & Long, D. (1998). The automatic inference of state invariants in TIM. Journal
of Artificial Intelligence Research, 9, 367–421.
Fox, M., & Long, D. (2003). PDDL2.1: An extension to PDDL for expressing temporal
planning domains. Journal of Artificial Intelligence Research, 20, 61–124.
Frank, J., & Jónsson, A. (2003). Constraint-based attribute and interval planning. Constraints, 8, 339–364.
Franklin, G., Powell, J., & Workman, M. (1997). Digital Control of Dynamic Systems.
Prentice Hall.
Fromherz, M. P. J., Bobrow, D. G., & de Kleer, J. (2003). Model-based computing for
design and control of reconfigurable systems. AI Magazine, 24 (4), 120–130.
Fromherz, M. P. J., Saraswat, V. A., & Bobrow, D. G. (1999). Model-based computing:
Developing flexible machine control software. Artificial Intelligence, 114 (1–2), 157–
202.
466

On-line Planning and Scheduling for Modular Printers

Gerevini, A., & Long, D. (2006). Preferences and soft constraints in pddl3. In Workshop
on Preferences and Soft Constraints in Planning, ICAPS06.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning through stochastic local search and
temporal action graphs in lpg. Journal of Artificial Intelligence Research, 20, 239–290.
Gerevini, A., Saetti, A., & Serina, I. (2008). An approach to efficient planning with numerical
fluents and multi-criteria plan quality. Artificial Intelligence, 172, 899–944.
Gerevini, A., & Schuert, L. (1998). Inferring state constraints for domain-independent
planning. In Proceedings of the Fifteenth National Conference on Artificial Intelligence
(AAAI).
Ghallab, M., & Laruelle, H. (1994). Representation and control in IxTeT, a temporal
planner. In Proceedings of AIPS-94, pp. 61–67.
Ghallab, M., Nau, D., & Traverso, P. (2004). Automated Planning Theory and Practice.
Morgan Kaufmann, San Francisco.
Ginsberg, M. L., & Harvey, W. D. (1992). Iterative broadening. Artificial Intelligence, 55,
367–383.
Green, P. (2002). Colorimetry and colour difference. In Green, P., & MacDonald, L. (Eds.),
Color Engineering, pp. 49–77. Wiley.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Proceedings
of AIPS, pp. 140–149.
Haslum, P., & Geffner, H. (2001). Heuristic planning with time and resources. In Proceedings
of ECP-01.
Hindi, H., Crawford, L., & Fromherz, M. (2005). Synchronization of state based control
processes with delayed and asynchronous measurements. In Proc. of Decision and
Control, 2005 and 2005 European Control Conference. CDC-ECC ’05, pp. 6370–6375.
Hindi, H., Crawford, L., Zhou, R., & Eldershaw, C. (2008). Efficient waypoint tracking
hybrid controllers for double integrators using classical time optimal control. In Proc.
of 47th IEEE Conference on Decision and Control, 2008 (CDC 2008).
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253–302.
Holte, R., Felner, A., Newton, J., Meshulam, R., & Furcy, D. (2006). Maximizing over
multiple pattern databases speeds up heuristic search. Artificial Intelligence, 170 (16
- 17), 1123–1136.
Koehler, J., & Hoffmann, J. (2000). On reasonable and forced goal orderings and their use
in an agenda-driven planning algorithm. Journal of Artificial Intelligence Research,
12, 338–386.
Korf, R. E. (1996). Improved limited discrepancy search. In Proceedings of AAAI-96, pp.
286–291. MIT Press.
Le, T. C., Baral, C., Zhang, X., & Tran, S. (2004). Regression with respect to sensing
actions and partial states. In Proceedings of AAAI-04.
467

Ruml, Do, Zhou, & Fromherz

Muscettola, N. (1994). HSTS: Integrating planning and scheduling. In Zweben, M., & Fox,
M. S. (Eds.), Intelligent Scheduling, chap. 6, pp. 169–212. Morgan Kaufmann.
Muscettola, N., Morris, P., & Tsamardinos, I. (1998). Reformulating temporal plans for
efficient execution. In Proceedings of the Conference on Principles of Knowledge Representation and Reasoning (KR-98).
Nguyen, X., Kambhampati, S., & Nigenda, R. S. (2002). Planning graph as the basis to
derive heuristics for plan synthesis by state space and csp search. Artificial Intelligence,
135 (1-2), 73–124.
Palacios, H., & Geffner, H. (2002). Planning as branch and bound: A constraint programming implementation. In Proceedings of CLEI-02.
Policella, N., Cesta, A., Oddi, A., & Smith, S. F. (2007). From precedence constraint posting
to partial order schedules. AI Communications, 20 (3), 163–180.
Pryor, L., & Collins, G. (1996). Planning for contingencies: A decision-based approach.
Journal of Artificial Intelligence Research, 4, 287–339.
Refanidis, I., & Vlahavas, I. (2003). Multiobjective heuristic state-space planning. Artificial
Intelligence, 145, 1–32.
Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks revisited. In Proceedings of
AAAI-08, pp. 975–982.
Rintanen, J. (2000). Incorporation of temporal logic control into plan operators. In Proceedings of the Fourteenth European Conference on Artificial Intelligence (ECAI-2000),
pp. 526–530.
Ruml, W., Do, M. B., & Fromherz, M. P. J. (2005). On-line planning and scheduling for
high-speed manufacturing. In Proceedings of ICAPS-05, pp. 30–39.
Smith, D. E., & Weld, D. S. (1999). Temporal planning with mutual exclusion reasoning.
In Proceedings of IJCAI-99, pp. 326–333.
Smith, S. F., & Cheng, C.-C. (1993). Slack-based heuristics for constraint satisfaction
scheduling. In Proceedings of AAAI-93, pp. 139–144.
Wah, B. W., & Chen, Y. (2003). Partitioning of temporal planning problems in mixed
space using the theory of extended saddle points. In IEEE International Conference
on Tools with Artificial Intelligence.
Yoshizumi, T., Miura, T., & Ishida, T. (2000). A* with partial expansion for large branching
factor problems. In Proceedings of AAAI-2000, pp. 923–929.

468

Journal of Artificial Intelligence Research 40 (2011) 269-304

Submitted 7/10; published 1/11

Iterated Belief Change Due to Actions and Observations
Aaron Hunter

hunter@cs.sfu.ca

British Columbia Institute of Technology
Burnaby, BC, Canada

James P. Delgrande

jim@cs.sfu.ca

Simon Fraser University
Burnaby, BC, Canada

Abstract
In action domains where agents may have erroneous beliefs, reasoning about the effects of actions involves reasoning about belief change. In this paper, we use a transition
system approach to reason about the evolution of an agent’s beliefs as actions are executed. Some actions cause an agent to perform belief revision while others cause an agent
to perform belief update, but the interaction between revision and update can be nonelementary. We present a set of rationality properties describing the interaction between
revision and update, and we introduce a new class of belief change operators for reasoning
about alternating sequences of revisions and updates. Our belief change operators can be
characterized in terms of a natural shifting operation on total pre-orderings over interpretations. We compare our approach with related work on iterated belief change due to action,
and we conclude with some directions for future research.

1. Introduction
We are interested in modeling the belief change that is caused by executing an alternating
sequence of actions and observations. Roughly, agents perform belief update following actions and agents perform belief revision following observations. However, to date there has
been little explicit consideration of the iterated belief change that results when an observation follows a sequence of actions. In this paper, we address belief change in the context of
sequences of the following form.
(InitialBeliefs) · (Action) · (Observation) · · · (Action) · (Observation)

(1)

We assume that the effects of actions are completely specified and infallible. We illustrate
that the belief change caused by this kind of alternating sequence can not simply be determined through a straightforward iteration of updates and revisions. The main issue is
that there are plausible examples where an observation should lead an agent to revise the
initial belief state, rather than the current belief state. Our goal is to provide a general
methodology for computing iterated belief change due to alternating sequences of actions
and observations.
1.1 Contributions to Existing Research
This paper is an extension of our previous work (Hunter & Delgrande, 2005). Our focus is
on action domains involving a single agent that can execute actions and make observations.
It is assumed that every change in the state of the world is due to the execution of an action,
c
2011
AI Access Foundation. All rights reserved.

Hunter & Delgrande

and it is assumed that the set of available actions is given. In this context, there are two
possible explanations for an erroneous belief. First, an erroneous belief can be due to an
incorrect initial belief. Second, an erroneous belief can be due to the execution of a hidden
exogenous action. We focus primarily on the first case, since we are explicitly concerned
with a single agent scenario. We briefly describe the contributions that we make in the area
of belief change caused by actions.
One contribution is that we explicitly specify precise properties that should hold whenever an action is followed by an observation. We state the properties in the style of the
AGM postulates for belief revision (Alchourrón, Gärdenfors, & Makinson, 1985), and we
argue that the properties should hold in any action domain involving a single agent with
perfect knowledge of the actions executed. The properties of iterated belief change that
we specify are a natural generalization of the AGM postulates; as such, they can easily
be justified for action domains involving a given AGM operator. We show that there are
simple examples where it is clear that the action history plays a role in the interpretation
of an observation. Therefore, it is necessary to formalize the role of the action history in
determining the appropriate belief change. However, this problem has not been explicitly
addressed in related work. To the best of our knowledge, our work is the first attempt
at formally specifying any high-level interaction between belief update and belief revision
caused by actions.
As a second contribution, we give a specific methodology for combining a belief update
operator and a belief revision operator in a single formalism. In particular, we define a new
class of belief change operators, called belief evolution operators. A belief evolution operator
takes two arguments: a set of states and an alternating sequence of actions and observations.
Each belief evolution operator ◦ is defined with respect to a fixed update operator  and a
fixed AGM revision operator ∗. Informally, we have the following correspondence
κ ◦ hA1 , α1 , . . . , An , αn i ≈ κ  A1 ∗ α1  · · ·  An ∗ αn .
The basic idea is simply to translate all observations into conditions on the initial beliefs. In
this manner, we can define an iterated belief change operator that respects our interaction
properties and handles example problems appropriately.
Formally, we demonstrate that our belief evolution operators can be characterized by
a natural “shifting” on the underlying AGM revision operator. In this sense, we can view
iterated belief change as a modified form of revision. From a more general perspective, the
belief evolution methodology is useful for combining any action formalism with an AGM
revision operator. Hence, we can view belief evolution as an improved methodology for
adding a revision operator to an action formalism.
The third contribution of this work is that we provide a mechanism for evaluating the
performance of existing epistemic action formalisms with regards to iterated belief change.
It has been common in past work to extend an existing action formalism by simply adding a
formal representation of knowledge or belief. This has been done, for example, in the action
language A (Lobo, Mendez, & Taylor, 2001; Son & Baral, 2001) as well as the Situation
Calculus (SitCalc) (Shapiro, Pagnucco, Lesperance, & Levesque, 2000). It is easy to see that
the extensions of A fail to satisfy our interaction properties, and the sensing actions in the
extended languages do not provide an appropriate model of iterated belief change. On the
other hand, we suggest that the SitCalc does consider the action history appropriately. In
270

Iterated Belief Change Due to Actions and Observations

general, we argue that simply extending an action formalism with a belief revision operator
is not sufficient. We show that such approaches either lack the formal machinery required for
reasoning about iterated belief change, or they make substantive implicit assumptions. Our
work illustrates the role that an action formalism plays in reasoning about belief change.
It also becomes clear that additional assumptions must be made in order to reason about
iterated belief change due to actions and observations. By making the role of the action
formalism salient, we can better evaluate the suitability of existing formalisms for particular
applications.

2. Preliminaries
In this section, we introduce preliminary notation and definitions related to reasoning about
action and reasoning about belief change. We also introduce a motivating example that will
be used throughout the paper.
2.1 Motivating Example
We introduce Moore’s litmus paper problem (Moore, 1985) as a motivating example. In
this problem, there is a beaker containing either an acid or a base, and there is an agent
holding a piece of white litmus paper that can be dipped into the beaker to determine the
contents. The litmus paper will turn red if it is placed in an acid and it will turn blue if
it is placed in a base. The problem is to provide a formal model of the belief change that
occurs when an agent uses the litmus paper to test the contents of the beaker.
Intuitively, the litmus paper problem seems to require an agent to revise the initial
beliefs in response to an observation at a later point in time. For example, suppose that
the agent dips the paper and then sees that the paper turns red. This observation not only
causes the agent to believe the beaker contains an acid now, but it also causes the agent
to believe that the beaker contained an acid before dipping. We refer to this process as
a prior revision, since the agent revises their beliefs at a prior point in time. This kind
of phenomenon is not explicitly discussed in many formalisms for reasoning about belief
change caused by action. We will return to this problem periodically as we introduce our
formal approach to belief change.
2.2 Basic Notation and Terminology
We assume a propositional signature composed of a finite set of atomic propositional symbols.
We use the primitive propositional connectives {¬, →}, where ¬ denotes classical negation
and → denotes implication. Conjunction, disjunction and equivalence are defined in the
usual manner, and they are denoted by ∧, ∨ and ≡, respectively. A formula is a propositional
combination of atomic symbols. A literal is either an atomic propositional symbol, or an
atomic propositional symbol preceded by the negation symbol. Let Lits denote the set of
all literals over the fixed signature.
An interpretation of a propositional signature P is a function that assigns every atomic
symbol a truth value. The set of all interpretations over P is denoted by 2P . The satisfaction
relation I |= φ is defined for formula φ by the usual recursive definition. For any formula
271

Hunter & Delgrande

φ, we define |φ| to be the set of all interpretations I such that I |= φ, and we say that φ is
satisfiable if and only if |φ| 6= ∅.
A belief state is a subset of 2P . We can think of a belief state as expressing a proposition.
Informally, an agent with belief state κ believes that the actual world is represented by one
of the interpretations in κ. An observation is also a set of interpretations. The intuition is
that the observation α provides evidence that the actual world is in the set α. In order to
maintain a superficial distinction, we will use the Greek letter α will range over observations
and the Greek letter κ to range over belief states, with possible subscripts in each case.
2.3 Transition Systems
An action signature is a pair hA, Fi where A, F are non-empty sets of symbols. We call
A the set of action symbols, and we call F the set of fluent symbols. Formally, the fluent
symbols in F are propositional variables. The action symbols in A denote the actions that
an agent may perform. The effects of actions can be specified by a transition system.
Definition 1 A transition system T for an action signature σ = hA, Fi is a pair hS, Ri
where
1. S is a set of propositional interpretations of F,
2. R ⊆ S × A × S.
The set S is called the set of states and R is the transition relation. If (s, A, s0 ) ∈ R, then we
think of the state s0 as a possible resulting state that could occur if the action A is executed
in state s. If there is exactly one possible resulting state s0 when A is executed in s for all
s ∈ S, then we say T is deterministic. We refer to fluent symbols as being true or false
if they are assigned the values t or f , respectively. Transition systems can be visualized
as directed graphs, where each node is labeled with a state and each edge is labeled with
an element of A. In terms of notation, the uppercase letter A, possibly subscripted, will
range over actions. We use the notation Ā to denote a finite sequence of action symbols
of indeterminate length. Also, given any sequence of actions Ā = hA1 , . . . , An i, we write
s ;Ā s0 to indicate that there is a path from s to s0 that follows the edges labeled by the
actions A1 , . . . , An .
It is useful to introduce the symbol λ to denote the null action that never changes the
state of the world. This will be used periodically in formal results. Also, for technical
reasons, we assume throughout this paper that every action is executable in every state. If
the transition system does not specify the effects of a particular action in a particular state,
we assume that the state does not change when that action is executed. This is tantamount
to adding self loops for every action at every state where no transition is given.
Example The litmus paper problem can be represented with the action signature
h{dip}, {Red , Blue, Acid }i.
Intuitively, the fluent symbols Red and Blue represent the colour of the litmus paper, and
the fluent symbol Acid indicates whether the beaker contains an acid or not. The only
action available is to dip the litmus paper in the beaker
272

Iterated Belief Change Due to Actions and Observations

{Acid }

∅

dip

dip

?

?

{Blue}

{Red , Acid }

Figure 1: Litmus Test
In the interest of readability, we will adopt a notational convention in our discussion of
the litmus paper problem. In particular, we will let a set V of propositional fluent symbols
stand for the interpretation where where the symbols in V are assigned the value true and
all other symbols are assigned the value false. Hence, the set {Red} will be used to denote
the interpretation I where I(Red) = true, I(Blue) = f alse and I(Acid) = f alse. This
is a standard representation in the literature on reasoning about action effects. We stress,
however, that we are not defining states in this manner; we are simply using this convention
in the litmus paper example because it facilitates the specification of an interpretation over
a small set of fluent symbols. To be clear, throughout this paper, states are actually defined
in terms of interpretations over sets of propositional variables.
The effects of dipping in the litmus paper problem are given by the transition system in
Figure 1. Note that we have not included all possible states in the figure; we have included
only those states that change when an action is executed. We assume that the state remains
unchanged when a dipping action is performed in any of the states omitted from the figure.

2.4 Belief Update
Belief update is the belief change that occurs when new information is acquired regarding a
change in the state of the world. One standard approach to belief update is the Katsuno and
Mendelzon approach (1991), which describes belief update in terms of a set of rationality
postulates. These postulates are typically referred to as the KM postulates, and they have
been analyzed, reformulated, and criticized in several subsequent papers (Boutilier, 1995;
Peppas, Nayak, Pagnucco, Foo, Kwok, & Prokopenko, 1996; Lang, 2006). Much of this
discussion has focused on the distinction between belief update and belief revision, which
we introduce in the next section.
In this paper, we adopt an approach to belief update in which beliefs are updated by
an action rather than a formula. Intuitively, after executing an action A, an agent updates
its belief state by projecting every state s to the state s0 that would result if the action A
was executed in the state s.
273

Hunter & Delgrande

Definition 2 Let T = hS, Ri be a transition system. The update function  : 2S × A → 2S
is defined as follows
κ  A = {s0 | (s, A, s0 ) ∈ R for some s ∈ κ}.
This operation is actually a form of action progression; it has been argued elsewhere that
the standard account of belief update can be understood to be a special case of this kind
of progression (Lang, 2006). The advantage of our approach is that it provides a simple
representation of the belief change that occurs following an action with conditional effects.
Example In the litmus paper problem, the agent believes the litmus paper is white, but
it is not known whether the beaker contains an acid or a base. Hence, the initial belief state
κ consists of two interpretations that can be specified as follows:
κ = {∅, {Acid }}.
After executing the dip action, the new belief state κ  dip consists of all possible outcomes
of the dip action. Hence, the new belief state contains two interpretations. In the first,
Red is true while the rest of the fluents are false. In the second interpretation, Blue and
Acid are true, while the rest of the fluents are false. Thus, following a dip action, the agent
believes that either the liquid is a base and the litmus paper is red or the liquid is an acid
and the litmus paper is blue. To determine which outcome has occurred, the agent must
observe the actual color of the paper.

2.5 Belief Revision
The term belief revision refers to the process in which an agent incorporates new information
with some prior beliefs. In this section, we briefly sketch the most influential approach to
belief revision: the AGM approach of Alchourrón, Gärdenfors and Makinson (1985). The
AGM approach to belief revision does not provide a specific recipe for revision. Instead,
a set of rationality postulates is given and any belief change operator that satisfies the
postulates is called an AGM belief revision operator.
Let F be a propositional signature. A belief set is a deductively closed set of formulas
over F. Let + denote the so-called belief expansion operator, which is defined by setting
K + φ to be the deductive closure of K ∪ {φ}. Let ∗ be a function that maps a belief set
and a formula to a new belief set. We say that ∗ is an AGM belief revision operator if it
satisfies the following postulates for every K and φ.
[AGM1]
[AGM2]
[AGM3]
[AGM4]
[AGM5]
[AGM6]
[AGM7]
[AGM8]

K ∗ φ is deductively closed
φ∈K ∗φ
K ∗φ⊆K +φ
If ¬φ 6∈ K, then K + φ ⊆ K ∗ φ
K ∗ φ = L iff |= ¬φ
If |= φ ≡ ψ, then K ∗ φ = K ∗ ψ
K ∗ (φ ∧ ψ) ⊆ (K ∗ φ) + ψ
If ¬ψ 6∈ K ∗ φ, then (K ∗ φ) + ψ ⊆ K ∗ (φ ∧ ψ)

274

Iterated Belief Change Due to Actions and Observations

The main intuition behind the AGM postulates is that the new information given by
φ must be incorporated, along with “as much of K” as consistently possible. The AGM
postulates provide a simple set of conditions that are intuitively plausible as restrictions on
belief revision operators. Moreover, the postulates completely determine a specific semantics
for revision in terms of pre-orderings over interpretations (Katsuno & Mendelzon, 1992).
We defer discussion of this semantics to §5, where we describe it in the context of a new
belief change operator.
Note that we have presented the AGM postulates in the traditional setting, where beliefs
and observations are given as sets of propositional formulae. In contrast, we represent beliefs
and observations as sets of interpretations. However, since we work with a finite language,
it is easy to translate between the two approaches and we will provide such translations
when required.
Example In the litmus paper problem, suppose that the paper turns red after dipping.
We need to represent this observation in a suitable manner for revision. We stated in §2.2
that an observation is a set of interpretations. Informally, an observation is the set of
interepretations that should be considered the most plausible after the observation. In the
litmus paper example, the observation that the paper is red is represented by the set of all
interpretations where Red is true.
We need to revise the current beliefs by an observation that represents the information.
Recall that the current belief state is
κ  dip = {{Blue}, {Red , Acid }}.
Since we also represent observations by sets of interpretations, the observation that the
paper is red is given by the set α defined as follows
α = {{Red , Acid }, {Red }, {Red , Blue, Acid }, {Red , Blue}}.
Note that this observation is consistent with the current belief state, because the intersection
is non-empty. Therefore, by [AGM3] and [AGM4], it follows that any AGM revision operator
will define the revised belief state to be this intersection. Hence, for any AGM revision
operator ∗, the final belief state is
{{Red , Acid }}.
So, after dipping the litmus paper and observing the paper is red, we correctly believe that
the beaker contains an acid.

3. Belief Update Preceding Belief Revision
As stated previously, we are interested in the belief change due to an alternating sequence
of actions and observations. The easiest example consists of a single action followed by a
single observation. However, throughout this paper, we assume that actions are infallible,
and that actions are infallible and the effects of actions are completely specified. Under this
275

Hunter & Delgrande

{Litmus}
dip
?

{Litmus, Blue}

{Litmus, Acid }

{Acid }

∅

dip

dip

?

?

{Litmus, Red , Acid }

∅

dip
?

{Acid }

Figure 2: Extended Litmus Test
assumption, a sequence of actions is no more difficult to handle than a single action. As
such, the simplest interesting case to consider is given by an expression of the form
κ  A1  · · ·  An ∗ α.

(2)

In this case, since there is only a single observation, we can focus entirely on the interaction
between revision and update. The general case involving several observations is complicated
by the fact that it implicitly requires some form of iterated revision. Our formalism handles
the general case, but our initial focus will be on problems of the form in (2). In the next
section, we consider an example of such a problem and we illustrate that the most natural
solution requires the initial state to be revised at a later point in time.
We use the phrase iterated belief change to refer to any scenario where the beliefs of an
agent change as a result of multiple sequential events. This is a somewhat non-standard
use of the term, as most of the literature on iterated belief change is concerned with the
(difficult) problem of iterated belief revision. By constrast, we consider problems of the
form 2 to be instances of iterated belief change because a sequence of events is leading to a
change in beliefs.
3.1 The Extended Litmus Paper Problem
We extend the litmus paper problem. The extended problem is just like the original, except
that we allow for the possibility that the paper is not litmus paper; it might simply be a
piece of plain white paper. In order to provide a formal representation of this problem, we
need to extend the transition system used to represent the original problem by introducing
a new fluent symbol Litmus. Informally, Litmus is true just in case the paper is actually
litmus paper.
The action signature hA, Fi consists of A = {dip} and F = {Red , Blue, Acid , Litmus}.
We assume the transition system for action effects given in Figure 2, and we assume a given
AGM revision operator ∗.
We describe a sequence of events informally. Initially, the agent believes that the paper
is a piece of litmus paper, but the agent is unsure about the contents of the beaker. To test
the contents, the agent dips the paper in the beaker. After dipping, the agent looks at the
paper and observes that it is still white. We are interested in determining a plausible final
belief state.
276

Iterated Belief Change Due to Actions and Observations

We now give a more formal representation of the problem. The initial belief state is
κ = {{Litmus}, {Litmus, Acid }}.
After dipping the paper in the beaker, we update the belief state as follows:
κ  dip = {{Litmus, Blue}, {Litmus, Red , Acid }}.
At this point, the agent looks at the paper and sees that it is neither blue nor red. This
observation is represented by the following set of worlds:
α = {∅, {Litmus}, {Acid }, {Litmus, Acid }}.
The naive suggestion is to simply revise κ  dip by α. However, this is not guaranteed to
give the correct result. It is possible, for example, to define an AGM operator which gives
the following final belief state:
κ0 = {{Litmus}, {Litmus, Acid }}.
In this case, the agent believes that the piece of paper is white litmus paper. This is clearly
not a plausible final belief state.
Informally, if the paper is litmus paper, then it must be either red or blue after a dipping
action is performed. Hence, neither {Litmus} nor {Litmus, Acid } is a plausible state after
dipping; simply revising by the observation may give a belief state that is incorrect because
the revision operator does not encode this constraint. The final belief state should consist
entirely of states that are possible consequences of dipping. Even if some particular AGM
operator does give a plausible final belief state for this example, the process through which
the final beliefs is obtained is not sound. In this kind of example, an observation after
dipping is actually giving information about the initial belief state. As such, our intuition
is that a rational agent should revise the initial belief state in any case.
We suggest that a rational agent should reason as follows. After dipping the paper and
seeing that it does not change colour, an agent should conclude that the paper was never
litmus paper to begin with. The initial belief state should be modified to reflect this new
belief before calculating the effects of the dipping action. This approach ensures that we will
have a final belief state that is a possible outcome of dipping. At the end of the experiment,
the agent should believe that the paper is not litmus paper and the agent should have
no definite beliefs regarding the contents of the beaker. Hence, we propose that the most
plausible final belief state is the set
{∅, {Acid }}.
This simple example serves to illustrate the fact that it is sometimes useful for an agent to
revise prior belief states in the face of new knowledge. In order to formalize this intuition
in greater generality, we need to introduce some new formal machinery.
277

Hunter & Delgrande

3.2 Interaction Between Revision and Update
In this section, we give a set of formal properties that we expect to hold when an update is
followed by a revision. The properties are not overly restrictive and they do not provide a
basis for a categorical semantics; they simply provide a point for discussion and comparison.
Our underlying assumption is that action histories are infallible. The most recent observation is always incorporated, provided that it is consistent with the history of actions that
have been executed. Hence, the properties we discuss are only expected to hold in action
domains in which there are no failed actions and no exogenous actions.
We briefly present some of our underlying intuitions. Let κ be a belief state, let Ā be a
sequence of actions, and let α be an observation. We are interested in the situation where
an agent has initial belief state κ, then Ā is executed, and then it is observed that the
actual state must be in α. We adopt the shorthand notation κ  Ā as an abbreviation for
the sequential update of κ by each element of Ā. There are three distinct cases to consider.
1. There are some α-states in κ  Ā.
2. There are no α-states in κ  Ā, but some α-states in 2F  Ā.
3. No α-states are possible after executing Ā.
Case (1) is the situation in which the observation α allows the agent to refine their knowledge
of the world. After the observation α, the agent should believe that the most plausible states
are the states in κ  Ā that are also in α. In other words, the agent should adopt the belief
state (κ  Ā) ∩ α.
In case (2), the agent should conclude that the actual state was not initially in κ. This
conclusion is based on our underlying assumption that the action sequence Ā cannot fail,
and the additional assumption that a new observation should be incorporated whenever
possible. Both of these assumptions can be satisfied by modifying the initial belief state
before performing the update. Informally, we would like to modify the initial belief state
minimally in a manner that ensures that α will be true after executing Ā. This is the case
that occurs in the extended litmus paper problem.
Case (3) is problematic, because it suggests that the agent has some incorrect information: either the observation α is incorrect or the sequence Ā is incorrect. We are assuming
that action histories are infallible. As such, the observation α must weakened in some manner in order to remain consistent with Ā. In cases where there is no natural weakening, it
may be necessary to abandon the observation completely. For a single observation, this is
the approach that we take. We view a single observation as a disjunctive constraint on the
possible states of world, and we assume that an observation has no meaning with respect to
the non-member states. As such, if an agent discovers that the actual state of the world is
not included in an observation, then that observation does not offer any information at all.
When we consider multiple observations, we taken a more flexible approach which allows
an agent to select a minimal repair of the sequence of observations.
Let κ and α be sets of states, let Ā be a sequence of actions, let  be an update operator
as in Definition 2, and let ∗ be an AGM revision operator. We formalize our intuitions with
the following conditions that should hold when an update is followed by a revision.

278

Iterated Belief Change Due to Actions and Observations

Interaction Properties
P1. If (2F  Ā) ∩ α 6= ∅, then κ  Ā ∗ α ⊆ α
P2. If (2F  Ā) ∩ α = ∅, then κ  Ā ∗ α = κ  Ā
P3. (κ  Ā) ∩ α ⊆ κ  Ā ∗ α
P4. If (κ  Ā) ∩ α 6= ∅, then κ  Ā ∗ α ⊆ (κ  Ā) ∩ α
P5. κ  Ā ∗ α ⊆ 2F  Ā
We give some motivation for each property. P1 is a straightforward AGM-type assertion
that α must hold after revising by α, provided α is possible after executing Ā. P2 handles
the situation where it is impossible to be in an α-world after executing Ā. In this case, we
simply discard the observation α. Together, P1 and P2 formalize the underlying assumption
that there are no failed actions.
P3 and P4 assert that revising by α is equivalent to taking the intersection with α,
provided the intersection is non-empty. These are similar to the AGM postulates asserting
that revisions correspond to expansions, provided the observation is consistent with the
knowledge base.
P5 provides the justification for revising prior belief states in the face of new knowledge.
It asserts that, after revising by α, we must still have a belief state that is a possible
consequence of executing Ā. In some cases, the only way to ensure that α holds after
executing Ā is to modify the initial belief state. We remark that P5 does not indicate how
the initial belief state should be modified.
It is worth noting that the interaction properties make no mention of minimal change
with respect to the belief state κ. There is a notion of minimal change that is implicit in
the revision operator ∗, but this is a notion change with respect to a measure plausibility
that is completely independent of the stated properties.
3.3 Representing Histories
Transition systems are only suitable for representing Markovian action effects; that is, action
effects that depend only on the action executed and the current state of the world. However,
in the extended litmus paper problem, we saw that the outcome of an observation may
depend on prior belief states. Even if action effects are Markovian, it does not follow that
changes in belief are Markovian. As such, we need to introduce some formal machinery for
representing histories. We will be interested in the historical evolution of an agent’s beliefs,
along with all of the actions executed. In order to do so, we need to introduce trajectories
of belief states, observations, and actions. For a given action signature hA, F i, we will use
the following terminology.
1. A belief trajectory is an n-tuple hκ0 , . . . , κn−1 i of belief states.
2. An observation trajectory is an n-tuple ᾱ = hα1 , . . . , αn i where each αi ∈ 2S .
3. An action trajectory is an n-tuple Ā = hA1 , . . . , An i where each Ai ∈ A.
Note that, as a matter of convention, we start the indices at 0 for belief trajectories and
we start the indices at 1 for observation and action trajectories. The rationale for this
convention will be clear later. We also adopt the convention hinted at in the definitions,
279

Hunter & Delgrande

whereby the ith component of an observation trajectory ᾱ will be denoted by αi , and the
ith component of an action trajectory Ā will be denoted by Ai .
We remark that a belief trajectory is an agent’s subjective view of how the world has
changed. Hence, a belief trajectory represents the agent’s current beliefs about the world
history, not a historical account of what an agent believed at each point in time. For
example, in the extended litmus paper problem, at the end of the experiment the agent
believes that they were never holding a piece of litmus paper. The fact that the agent once
believed that they were holding litmus paper is a different issue, one that is not represented
in our formal conception of a belief trajectory.
We define a notion of consistency between observation trajectories and action trajectories. The intuition is that an observation trajectory ᾱ is consistent with an action trajectory
Ā if and only if each observation αi is possible, given that the actions (Aj )j≤i have been
executed.
Definition 3 Let ᾱ = hα1 , . . . , αn i be an observation trajectory and let Ā = hA1 , . . . , An i
be an action trajectory. We say that Ā is consistent with ᾱ if and only if there is a belief
trajectory hκ0 , . . . , κn i such that, for all i with 1 ≤ i ≤ n,
1. κi ⊆ αi
2. κi = κi−1  Ai .
If Ā is consistent with ᾱ, we write Ā||ᾱ.
A pair consisting of an action trajectory and an observation trajectory gives a complete
picture of an agent’s view of the history of the world. As such, it is useful to introduce the
following terminology.
Definition 4 A world view of length n is a pair W = hĀ, ᾱi, where Ā is an action trajectory
and ᾱ is an observation trajectory, each of length n. We say W is consistent if Ā||ᾱ.

4. Belief Evolution
We are interested in providing a formal treatment of alternating action/observation sequences of the form
κ  A1 ∗ α1  · · ·  An ∗ αn .
(3)
Note that there is an implicit tendency to associate the operators in an expression of this
form from left to right, which gives the following expression:
(. . . ((κ  A1 ) ∗ α1 )  · · ·  An ) ∗ αn .

(4)

The extended litmus paper problem illustrates that this association can lead to unsatisfactory results. As such, we would like to propose an alternative method for evaluating
expressions of the form (3). However, discussing expressions of this form directly can be
somewhat misleading, since we can not preclude the interpretation in (4). In order to make
it explicit that we are not computing successive updates and revisions, we introduce a new
belief evolution operator ◦. Intuitively, ◦ simply allows us to associate the information to
be incorporated in a manner closer to the following informal expression:
κ ◦ (A1 , α1 , . . . , An , αn ).
280

(5)

Iterated Belief Change Due to Actions and Observations

Note that the update and revision operators have disappeared; we are left with an expression
that groups all actions and observations as a single sequence, suggesting that all information
should be incorporated simultaneously. However, the order of observations and actions is
still significant. Moreover, it is important to keep in mind that ◦ is defined with respect to
given update and revision operators. The new operator is introduced primarily to give us
a formal tool to make it explicit that we are not interpreting expressions of the form (3) by
the default interpretation in (4).
The formal definition of ◦ is presented in the following sections, and it does not use
the exact syntax in the informal expression (5). In the actual definition, a belief evolution
operator takes a belief state and a world view as arguments. Also, the value returned is not
a single belief state; it is a belief trajectory. However, (5) provides the important underlying
intuition.
4.1 Infallible Observations
In this section, we define ◦ under the assumption that observations are always correct.
Formally, this amounts to a restriction on the world views that are considered. In particular,
we need not consider inconsistent world views. It is easy to see that an inconsistent world
view is not possible under the assumption that action histories and observations are both
infallible.
Let s−1 (A) denote the set of all states s0 such that (s0 , A, s) ∈ R. We call s−1 (A) the
pre-image of s with respect to A. The following definition generalizes this idea to give the
pre-image of a set of states with respect to a sequence of actions.
Definition 5 Let T be a deterministic transition system, let Ā = hA1 , . . . , An i and let α
be an observation. Define α−1 (Ā) = {s | s ;Ā s0 for some s0 ∈ α}.
Hence, if the actual world is an element of α following the action sequence Ā, then the
initial state of the world must be in α−1 (Ā).
For illustrative purposes, it is useful to consider world views of length 1. Suppose we
have an initial belief state κ, an action A and an observation α. Without formally defining
the belief evolution operator ◦, we can give an intuitive interpretation of an expression of
the form
κ ◦ hhAi, hαii = hκ0 , κ1 i.
The agent knows that the actual world is in α at the final point in time, so we must have
κ1 ⊆ α. Moreover, the agent should believe that κ1 is a possible result of executing A from
κ0 . In other words, we must have κ0 ⊆ α−1 (A). All other things being equal, the agent
would like to keep as much of κ as possible. In order to incorporate α−1 (A) while keeping
as much of κ as possible, the agent should revise κ by α−1 (A). This suggests the following
solution.
1. κ0 = κ ∗ α−1 (A),
2. κ1 = κ0  A.
This reasoning can be applied to world views of length greater than 1. The idea is to
trace every observation back to a precondition on the initial belief state. After revising the
281

Hunter & Delgrande

initial belief state by all preconditions, each subsequent belief state can be determined by
a standard update operation.
We have the following formal definition for ◦. In the definition, for i ≤ n we let Āi
denote the subsequence of actions hA1 , . . . , Ai i.
Definition 6 Let κ be a belief state, let  be an update operator, let ∗ be an AGM revision
operator, let Ā be an action trajectory of length n and let ᾱ be an observation trajectory of
length n such that Ā||ᾱ. Define
κ ◦ hĀ, ᾱi = hκ0 , . . . , κn i
where
1. κ0 = κ ∗

T

−1
i αi (Āi )

2. for i ≥ 1, κi = κ0  A1  · · ·  Ai .
We remark that the intersection of observation preconditions in the definition of κ0 is nonempty, because Ā||ᾱ.
The following propositions are immediate, and they demonstrate that for some action sequences of length 1, the operator ◦ reduces to either revision or update. In each proposition,
we assume that Ā||ᾱ.
Proposition 1 Let κ be a belief state, let Ā = hAi and let ᾱ = h2F i. Then
κ ◦ hĀ, ᾱi = hκ, κ  Ai.
Proof Recall that we assume every action is executable in every state. It follows that
(2F )−1 (A) = 2F . Therefore
κ ◦ hĀ, ᾱi = hκ ∗ 2F , (κ ∗ 2F )  Ai
= hκ, κ  Ai.
2
In the next result, recall that λ is a null action that never changes the state of the world.
Proposition 2 Let κ be a belief state, let Ā = hλi and let ᾱ = hαi. Then
κ ◦ hĀ, ᾱi = hκ ∗ α, κ ∗ αi.
Proof

Since λ does not change the state, it follows that α−1 (λ) = α. Therefore
κ ◦ hĀ, ᾱi = hκ ∗ α, (κ ∗ α)  λi
= hκ ∗ α, κ ∗ αi.

2

282

Iterated Belief Change Due to Actions and Observations

Hence, the original revision and update operators can be retrieved from the ◦ operator. As
such, it is reasonable to compute the iterated belief change due to action in terms of belief
evolution.
We will demonstrate that belief evolution provides a reasonable approach for computing
the outcome of a sequence of actions and observations. We stress that computing updates
and revisions in succession does not provide a reasonable solution in many cases. As such,
we want to define the outcome of a sequence of updates and revisions in terms of a belief
evolution operator. Given ∗ and , we define the iterated belief change
κA∗α
to be the final belief state in the belief trajectory
κ ◦ hhAi, hαii.
We adopt this somewhat confusing convention temporarily in order to prove that belief
evolution provides a semantics for iterated belief change that satisfies our interaction properties.
More generally, consider a sequence Ā of n actions followed by a single observation α.
In this case, define ᾱn to be the observation trajectory consisting of n − 1 instances of 2F
followed by the final observation α. We define the iterated belief change
κ  Ā ∗ α
to be the final belief state in the belief trajectory
κ ◦ hĀ, ᾱn i.
Proposition 3 Let Ā be an action trajectory and let α be an observation. If Ā||ᾱn , the
iterated belief change κ  Ā ∗ α defined as above satisfies the interaction properties P1-P5.
Proof

Let κ be a belief state. By the convention outlined above,
κ  Ā ∗ α = (κ ∗ α−1 (Ā))  Ā.

We demonstrate that this definition satisfies P1-P5.
P1. If (2F  Ā) ∩ α 6= ∅, then (κ  Ā) ∗ α ⊆ α.
Note that the antecedent is true because Ā and ᾱn are consistent. We have the following
inclusions:
(κ ∗ α−1 (Ā))  Ā ⊆ α−1 (Ā)  Ā ⊆ α.
The first inclusion holds by [AGM2] plus the fact that update satisfies (X ∩ Y )  Ā ⊆ X  Ā.
The second inclusion holds by definition of the pre-image. Hence, the consequent is true.
P2. If (2F  Ā) ∩ α = ∅, then (κ  Ā) ∗ α = κ  Ā
The antecedent is false, since Ā and ᾱn are consistent.

283

Hunter & Delgrande

P3. (κ  Ā) ∩ α ⊆ (κ  Ā) ∗ α
Suppose s ∈ (κ  Ā) ∩ α. So s ∈ α and there is some s0 ∈ κ such that Ā maps s0 to s. Hence,
s0 ∈ α−1 (Ā). It follows from [AGM2] that s0 ∈ κ ∗ α−1 (Ā). Since Ā maps s0 to s, we have
s ∈ (κ ∗ α−1 (Ā))  Ā.
P4. If (κ  Ā) ∩ α 6= ∅, then (κ  Ā) ∗ α ⊆ (κ  Ā) ∩ α
Suppose that (κ Ā)∩α 6= ∅. So there is a state in κ that is mapped to α by the sequence Ā.
Hence κ ∩ α−1 (Ā) 6= ∅. By [AGM3] and [AGM4], it follows that κ ∗ α−1 (Ā) = κ ∩ α−1 (Ā).
Now suppose that s ∈ (κ ∗ α−1 (Ā))  Ā. So there exists s0 ∈ κ ∗ α−1 (Ā) such that Ā maps
s0 to s. But then s0 ∈ κ ∩ α−1 (Ā). But this implies that s ∈ κ  Ā and s ∈ α.
P5. (κ  Ā) ∗ α ⊆ 2F  Ā
This is immediate, because (κ ∗ α−1 (Ā)) ⊆ 2F and (X ∪ Y )  α = (X  α) ∪ (Y  α).
2
The three preceding propositions demonstrate the suitability of ◦ as a natural operator
for reasoning about the interaction between revision and update.
We can use a belief evolution operator to give an appropriate treatment of the litmus
paper problem.
Example Consider the extended litmus paper problem, and let
α = {∅, {Litmus}, {Acid }, {Litmus, Acid }}.
Hence, the world view W = hhdipi, hαii represents a dipping action followed by the observation that the paper is still white. If ◦ is obtained from the metric transition system defined
by the Hamming distance and the transitions in Figure 2, the final belief state in κ ◦ W is
given by
κ ∗ α−1 (dip)  dip = κ ∗ {∅, {Acid }}  dip
= {∅, {Acid }}.
This calculation is consistent with our original intuitions, in that the agent revises the initial
belief state before updating by the dip action. This ensures that we will have a final belief
state that is a possible outcome of dipping. Moreover, the initial belief state is revised by
the pre-image of the final observation, which means it is modified as little as possible while
still guaranteeing that the final observation will be feasible. Note also that the final belief
state given by this calculation is intuitively reasonable. It simply indicates that the contents
of the beaker are still unknown, but the agent now believes the paper is not litmus paper.
Hence, a belief evolution operator employs a plausible procedure and returns a desirable
result.

4.2 Fallible Observations
In this section, we consider belief evolution in the case where observations may be incorrect.
Formally, this means that we are interested in determining the outcome of belief evolution
284

Iterated Belief Change Due to Actions and Observations

for inconsistent world views. In this case, we cannot simply take the intersection of all
observation pre-images, because this intersection may be empty. The basic idea behind our
approach is to define belief evolution with respect to some external notion of the reliability
of an observation.
We start by defining belief evolution in the most general case, with respect to a total
pre-order over the elements of an observation trajectory ᾱ. In order to define the pre-order,
we assume we are given a function r that maps each element of ᾱ to an integer. The actual
value of r(αi ) is not particularly important; the function r is just used to impose an ordering
on the observations. We interpret
r(αi ) < r(αj )
to mean that αi is more reliable than αj . In this case, if consistency can be restored by
discarding either αi or αj , then αj is discarded. The ranking function on observations
may be obtained in several ways. For example, it may be induced from an ordering over
all possible observations, indicating the reliability of sensing information. On the other
hand, this pre-order might simply encode a general convention for dealing with sequential
observations. For example, in some cases it may be reasonable to prefer a more recent
observation over an earlier observation.
Our basic approach is the following. Given an observation trajectory ᾱ that is not
consistent with Ā, we discard observations in a manner that gives us a consistent world
view. To be precise, “discarding” an observation in this context means that we replace it
with 2F . We discard observations rather than weaken them, because we view the content
of an observation as an atomic proposition. We are guided by two basic principles. First,
all other things being equal, we would like to keep as much of ᾱ as consistently possible.
Second, when observations must be discarded, we try to keep those that are the most
reliable. With this informal sketch in mind, we define w(ᾱ), the set of all trajectories
obtained by discarding some of the observations in ᾱ.
Definition 7 Let ᾱ be an observation trajectory of length n, and let On be the set of all
observation trajectories of length n. Then define:
w(ᾱ) = {ᾱ0 | ᾱ0 ∈ On and for 1 ≤ i ≤ n, αi0 = αi or αi0 = 2F }.
We are interested in finding those trajectories in w(ᾱ) that are consistent with Ā, while
differing minimally from ᾱ. In the following definition, for observation trajectories of equal
length n, we write ᾱ ⊆ ᾱ0 as a shorthand notation to indicate that αi ⊆ αi0 for every
1 ≤ i ≤ n.
Definition 8 For a world view hĀ, ᾱi:
hĀ, ᾱi ↓⊥ = {ᾱ0 ∈ w(ᾱ) | Ā||ᾱ0 and for all ᾱ00 ∈ w(ᾱ) such that ᾱ00 ⊆ ᾱ0 , not Ā||ᾱ00 }.
So ᾱ0 ∈ hĀ, ᾱi ↓⊥ if and only if ᾱ0 is consistent with Ā, but it becomes inconsistent if any
of the discarded observations are re-introduced. Therefore the elements of ᾱ0 ∈ hĀ, ᾱi ↓⊥
differ minimally from ᾱ, where “minimal” is defined in terms of set-containment.1
We can use the reliability ordering over observations to define a reliability ordering over
observation trajectories.
1. Note that this is not the only reasonable notion of minimality that could be employed here. One natural
alternative would be to consider minimal change in terms of cardinality. In this case, a trajectory differs

285

Hunter & Delgrande

Definition 9 For ᾱ0 , ᾱ00 ∈ hĀ, ᾱi ↓⊥ , we write ᾱ00 < ᾱ0 if and only if there is a j such that
1. For all k < j and all i < n, if r(αi ) = k then αi0 = αi00 .
2. There exists some αi such that r(αi ) = j and αi00 ⊂ αi0 .
3. There does not exist any αi such that r(αi ) = j and αi0 ⊂ αi00 .
Informally, ᾱ00 < ᾱ0 if ᾱ00 retains more reliable observations than ᾱ0 . The minimal trajectories in this ordering are, therefore, those that retain the most plausible observations.
Definition 10 The set of repairs of a world view hĀ, ᾱi with respect to a reliability function
r is given by:
Rep(Ā, ᾱ) = {ᾱ0 ∈ hĀ, ᾱi ↓⊥ | there is no ᾱ00 ∈ hĀ, ᾱi ↓⊥ such that ᾱ00 < ᾱ0 }.
Note that Rep(Ā, ᾱ) may contain several observation trajectories. Moreover, each trajectory
in Rep(Ā, ᾱ) is consistent with Ā, while minimally discarding observations and keeping the
most reliable observations possible.
In Definition 6, we defined ◦ for consistent world views. For inconsistent world views,
we use the following definition.
Definition 11 Let κ be a belief state, let Ā be an action history, let ᾱ be an observation
trajectory, and let r be a reliability function. If Ā is not consistent with ᾱ, then:
κ ◦ hĀ, ᾱi = {κ ◦ hĀ, ᾱ0 i | ᾱ0 ∈ Rep(Ā, ᾱ)}.
This definition is well-formed, because ᾱ0 ∈ Rep(Ā, ᾱ) implies Ā||ᾱ0 . Note that the outcome
of belief evolution in this case is a set of belief trajectories.
We adopt the following convention. If hĀ, ᾱi = {ᾱ0 }, then we write hĀ, ᾱi = ᾱ0 . If
|Rep(Ā, ᾱ)| = 1, then belief evolution yields a unique belief trajectory. There are natural
examples when this is the case.
Proposition 4 Let hĀ, ᾱi be a world view of length n. Let r be a reliability function such
that αi 6= αj implies r(αi ) 6= r(αj ) for all 1 ≤ i, j ≤ n. Then |Rep(Ā, ᾱ)| = 1.
Proof Note that hĀ, ᾱi ↓⊥ 6= ∅, because it is always possible to find a trajectory consistent
with Ā by discarding some of the observations in ᾱ. It follows immediately that Rep(Ā, ᾱ)
is non-empty. Hence |Rep(Ā, ᾱ)| ≥ 1.
Now suppose that there exist ᾱ0 and ᾱ00 such that ᾱ0 ∈ Rep(Ā, ᾱ), ᾱ00 ∈ Rep(Ā, ᾱ) and
0
ᾱ 6= ᾱ00 . Thus
{αi | αi0 6= αi00 } =
6 ∅.
Let αj ∈ {αi | αi0 6= αi00 } be such that r(αj ) is minimal. By assumption, αj is unique. If
αj0 = αj , then ᾱ0 < ᾱ00 which contradicts ᾱ00 ∈ Rep(Ā, ᾱ). If αj0 = 2F , then ᾱ00 < ᾱ0 which
minimally from ᾱ just in case a minimal number of observations is discarded. There are reasonable
arguments for both the containment approach and the cardinality approach, depending on the context.
Neither approach has a clear theoretical advantage, and the development of each is virtually identical.
We determined that this paper is better served by presenting the containment approach in detail, rather
than presenting a series of duplicate results for different conceptions of minimality.

286

Iterated Belief Change Due to Actions and Observations

contradicts ᾱ0 ∈ Rep(Ā, ᾱ). Therefore, ᾱ0 = ᾱ00 . It follows that |Rep(Ā, ᾱ)| ≤ 1, because no
two elements of Rep(Ā, ᾱ) can be distinct. 2
Thus if r assigns a unique value to each observation, then belief evolution yields a unique
outcome. We return to this fact in the next section.
In cases where belief evolution does not yield a unique result, a skeptical approach can
be defined by taking a union on initial belief states. Recall that κ0 is the first element in
the trajectory κ. If we define
[
κ0 = {κ00 | κ0 ∈ κ ◦ Rep(Ā, ᾱ)}.
then a unique belief trajectory can then be defined by computing the effects of actions. This
trajectory is general enough to include the outcome of every minimal repair. This kind of
skeptical approach is appropriate in some situations.
4.3 Recency
One well-known approach to dealing with sequences of observations is to give precedence
to recent information (Nayak, 1994; Papini, 2001). Given an observation trajectory ᾱ, a
preference for recent information can be represented in our framework by defining r such
that r(αi ) = −i. For our purposes, recency provides a concrete reliability ordering over
observations, which facilitates the presentation of examples and comparison with related
formalisms. As such, throughout the remainder of this paper, we use ◦ to denote the belief
evolution operator ◦ defined with respect to this function r.
We stress that this preference for recent information is just a convention that we adopt
because it simplifies the exposition, and we note that this convention has been the subject of
criticism (Delgrande, Dubois, & Lang, 2006). Note that, by Proposition 4, belief evolution
under the recency convention defines a unique belief trajectory as an outcome. As such,
belief evolution under the recency convention also defines a specific approach to iterated
revision. A sequence of several observations interspersed with null actions is no longer
computed by simply applying a single shot revision operator several times. We will explore
the approach to iterated revision that is implicit in our belief evolution operators in §6.2.
We conclude this section with a useful result. Thus far, applying the ◦ operator requires
tracing action preconditions back to the initial state for revision, then applying action effects
to get a complete history. If we are only concerned with the final belief state, then there are
many cases in which we do not need to go to so much effort. In the following proposition,
it is helpful to think of 2F as a “null observation” that provides no new information.
Proposition 5 Let κ be a belief state, let Ā be an action trajectory of length n and let α be
a belief state such that α ⊆ κ  Ā. If ᾱ is the observation trajectory with n − 1 observations
of 2F followed by a single observation α, then the final belief state in κ ◦ hĀ, ᾱi is (κ  Ā) ∗ α.
Proof

By definition, the final belief state of κ ◦ hĀ, ᾱi is
(κ ∗ α−1 (Ā))  Ā.

Since α ⊆ κ  Ā, the intersection κ ∩ α−1 (Ā) is non-empty. By [AGM3] and [AGM4], it
follows that
κ ∗ α−1 (Ā) = κ ∩ α−1 (Ā)
287

Hunter & Delgrande

and therefore
(κ ∗ α−1 (Ā))  Ā = (κ ∩ α−1 (Ā))  Ā.
Clearly, the right hand side of this equality is equal to (κ  Ā) ∩ α. Again, since α ⊆ κ  Ā,
it follows from [AGM3] and [AGM4] that this is (κ  Ā) ∗ α. 2
The proposition indicates that, given a single observation that is consistent with the
actions that have been executed, we can simply revise the outcome of the actions and we
get the correct final belief state.

5. Defining Belief Evolution Through Orderings on Interpretations
In the next two sections, we provide a characterization of belief evolution operators terms
of total pre-orders on interpretations. We restrict attention to the case involving one action
followed by one observation. This result extends easily to the case involving n actions
followed by one observation, because action sequences of length n simply define a new set of
transitions over states. Since we prove our characterization for an arbitrary action signature,
allowing n actions prior to a single observation is no more difficult than allowing only one
action. We present the case with a single action, as it simplifies the exposition by allowing
us to avoid introducing sequences of null observations interspersed with the actions. We
remark that our result does not extend directly to the case involving several observations,
as we do not introduce an axiomatic account of the reliability of an observation.
First, we need to delineate a general class of belief change functions.
Definition 12 A combined belief change operator is a function
◦˙ : 2S × hA, 2S i → 2S .
Hence, a combined belief change operator takes a belief state and an ordered pair hA, αi as
input, and it returns a new belief state.
For a fixed update operator  and a fixed revision operator ∗, consider the following
postulates.
˙
I1 If (2F  A) ∩ α 6= ∅, then κ◦hA,
αi = κ ∗ α−1 (A)  A.
˙
I2 If (2F  A) ∩ α = ∅, then κ◦hA,
αi = κ  A.
If we abuse our notation by letting κ◦hA, αi denote the final belief state in the corresponding
belief trajectory, then we get the following result. In the proposition, we we refer to a the
belief evolution operator defined by an update operator and a revision operator. To be clear,
this refers to the belief evolution operator obtained through Definitions 6 and 11.
Proposition 6 Let  be a belief update operator and let ∗ be a belief revision operator.
Then ◦˙ is the belief evolution operator defined by , ∗ if and only if ◦˙ satisfies I1 and I2.
Proof Let ◦˙ be the belief evolution operator corresponding to  and ∗. If (2F  A) ∩ α 6= ∅,
˙
then κ◦hA,
αi = κ ∗ α−1 (A)  A by definition. Hence ◦˙ satisfies I1. Suppose, on the other
˙
hand, that (2F A)∩α = ∅. In this case α−1 (A) = ∅. Therefore, κ◦hA,
αi = κ∗2F A = κA.
So ◦˙ satisfies I2.
288

Iterated Belief Change Due to Actions and Observations

To prove the converse, suppose that ◦˙ satisfies I1 and I2. Let ◦ be the belief evolution
operator defined by  and ∗. Suppose that (2F  A) ∩ α 6= ∅. If follows that
κ ◦ hA, αi = κ ∗ α−1  A
˙
= κ◦hA,
αi

(since A and α are consistent)
(by I1)

Now suppose that (2F  A) ∩ α = ∅.
κ ◦ hA, αi = κ ∗ 2F  A
˙
= κ◦hA,
αi

(since A and α are not consistent)
(by I2)

This completes the proof. 2
Hence, the postulates I1 and I2 provide a complete syntactic description of belief evolution.
This characterization will make it easier to state the representation result in the next section.
5.1 Translations on Orderings
For a fixed transition system T , we would like to provide a characterization of all combined
belief change functions that represent belief evolution operators for T . Our characterization will be defined in terms of total pre-orderings over interpretations. First we need to
introduce the basic characterization of AGM revision operators in terms of total pre-orders,
due to Katsuno and Mendelzon (1992). Our presentation differs slightly from the original
because we define revision operators on sets of states rather than formulas.
Definition 13 (Katsuno & Mendelzon, 1992) Given a belief state κ, a total pre-order ≤κ
over interpretations is called a faithful ranking with respect to κ just in case the following
conditions hold:
• If s1 , s2 ∈ κ, then s1 =κ s2 .
• If s1 ∈ κ and s2 6∈ κ, then s1 <κ s2 .
Hence, a faithful ranking is simply a total pre-order where the minimal elements are the
members of κ. In order to simplify the discussion, we introduce the following notation. If
α is a set of states and ≤ is an ordering on a superset of α:
min(α, ≤) = {s | s ∈ α and s is ≤ -minimal among interpretations in α)}.
The following definition characterizes AGM revision operators in terms of pre-orders on
interpretations.
Proposition 7 (Katsuno & Mendelzon, 1992) Given a belief state κ, a revision operator
∗ satisfies the AGM postulates if and only if there exists a faithful ranking ≤κ with respect
to κ such that for any set of interpretations α:
κ ∗ α = min(α, ≤κ ).
In the remainder of this section, we extend this result to characterize belief evolution operators.
Assume we are given a fixed transition system T = hS, Ri defining an update operator
. The following definition gives a natural progression operation on pre-orderings.
289

Hunter & Delgrande

Definition 14 If ≤κ is a faithful ranking with respect to κ and A is an action, then define
≤κA such that s1 ≤κA s2 if and only if
• There exist t1 , t2 such that (t1 , A, s1 ) ∈ R and (t2 , A, s2 ) ∈ R).
• t1 ≤ κ t2 .
Note that ≤κA is not generally a total pre-order because it may be the case that some
states are not possible outcomes of the action A. Hence ≤κA is a partial pre-order. We
can think of ≤κA as a “shifted” ordering, with minimal elements κ  A.
The following definition associates a combined revision operator with a faithful ranking.
Definition 15 Let ≤κ be a faithful ranking with respect to κ. The combined belief change
operator associated with ≤κ is the following:

min(α, ≤κA ) if (2F  A) ∩ α 6= ∅
˙
κ◦hA,
αi =
κA
otherwise.
Note that the operator ◦˙ takes an observation and an action as inputs, and it returns a new
belief state. We will prove that the class of functions definable in this manner coincides
exactly with the class of belief evolution operators.
We first prove that every faithful ranking defines a belief evolution operator.
Proposition 8 Let ≤κ be a faithful ranking with respect to κ and let ◦˙ be the combined
belief change operator defined by ≤κ . Then ◦˙ satisfies I1 and I2.
Proof Let A be an action, let α be an observation, and let ∗ be the AGM revision operator
defined by ≤κ by Proposition 7. We will prove that ◦˙ satisfies I1 and I2 with respect to ∗.
Suppose that (2F  A) ∩ α 6= ∅, so
˙
κ◦hA,
αi = min(α, ≤κA ).
We remark that, by definition,
α = α−1 (A)  A.
So:
˙
κ◦hA,
αi = min(α−1 (A)  A, ≤κA )
= min(α−1 (A), ≤κ )  A
By definition of ∗:
κ ∗ α−1 (A) = min(α−1 (A), ≤κ )
It follows that
˙
κ◦hA,
αi = (κ ∗ α−1 (A))  A
which proves that I1 holds.
If (2F  A) ∩ α = ∅, then by definition:
˙
κ◦hA,
αi = κ  A.
Hence I2 is satisfied. 2
We now prove the converse.
290

Iterated Belief Change Due to Actions and Observations

Proposition 9 Let ◦˙ be an operator satisfying I1 and I2 for some AGM revision function
∗. Given a belief state κ, there is a faithful ranking ≤κ such that ◦˙ is the combined belief
change operator defined by ≤κ .
Proof

By Proposition 7, there is a a faithful ranking ≤κ with respect to κ such that
κ ∗ α = min(α, ≤κ )

for all α. Fix a particular α and let A be an action symbol. Suppose that (2F  A) ∩ α 6= ∅.
So, by I1:
˙
κ◦hA,
αi = κ ∗ α−1 (A)  A.
By definition of ∗, this is equal to
min(α−1 (A), ≤κ )  A
This is equivalent to:
min(α−1 (A)  A, ≤κA )
Simplifying the first argument, we get:
min(α, ≤κA )
which is what we wanted to show.
Now suppose that α (2F  A) ∩ α = ∅ then,
˙
κ◦hA,
αi = κ  A

(by I2)

This completes the proof. 2
Hence, the class of belief evolution operators can be characterized by simply shifting
the total pre-order that defines the revision operator. This characterization is essentially
a corollary of Katsuno and Mendelzon’s representation result for AGM revision. However,
this approach does represent a significant departure from the iterative approach to applying
update and revision operators. What our result demonstrates is that the progression of
beliefs through actions should be applied at the level of the same pre-order that is used for
revision. In this manner, the relative likelihood of all states is shifted appropriately when
an action is executed. This ensures that later revisions use the same ordering that would
be used initially, which captures our intuition that the execution of actions does not change
any a priori likelihood of initial states of the world.

6. Comparison with Related Work
Many action formalisms define action effects to be Markovian. This is the case, for example,
in action languages like A (Gelfond & Lifschitz, 1998). When action formalisms of this kind
are supplemented with sensing actions, there is a natural tendency to compute epistemic
change by computing the effects of ontic actions and sensing actions in succession. This is the
implicit approach to iterated belief change caused by actions in the epistemic extensions of
A (Lobo et al., 2001; Son & Baral, 2001). We have seen that this strategy is not appropriate
291

Hunter & Delgrande

for litmus-type problems. However, this does not mean that such formalisms can not be
used for reasoning about iterated belief change due to action. It simply means that some
care must be taken to define the iterated change correctly.
Belief evolution should not be seen as a formalism in competition with Markovian formalisms; it should be seen as a methodology for extending Markovian formalisms to address
iterated belief change. If the revision and update operators are given explicitly, the definition of the corresponding belief evolution operator is straightforward. This is true even in
formalisms where the basic operators are relatively sophisticated, such as those defined in
the multi-agent belief structures of Herzig, Lang and Marquis (2004).
6.1 The Situation Calculus
In this section, we compare our work with an extended version of the SitCalc that explicitly
represents the beliefs of agents. We assume the reader is familiar with the SitCalc (Levesque,
Pirri, & Reiter, 1998), and we provide only a brief introduction of the notation we will use.
The terms in a SitCalc action theory range over a domain that includes situations and
actions. A fluent is a predicate that takes a situation as the final argument. A SitCalc
action theory includes an action precondition axiom for each action symbol, a successor
state axiom for each fluent symbol, as well as the foundational axioms of the SitCalc.
Informally, a situation represents the state of the world, along with a complete history of
actions that have been executed. There is a distinguished constant S0 that represents the
initial situation, and a distinguished function symbol do that represents the execution of an
action. Every situation term can be written as follows:
do(An , do(An−1 , . . . , do(A1 , S0 ) . . . ).
To simplify the notation, we will abbreviate this situation as do([A1 , . . . , An ], S0 ).
To clarify the results that follow, we adopt the following convention. Expressions such
as s and do(A, s) will be used as syntactic variables ranging over situation terms. We will
also need to refer explicitly to the semantic objects denoted by terms in a given first-order
interpretation M of a situation calculus theory. Hence, we let sM denote the situation
denoted by the situation term s in the first-order interpretation M. We adopt the same
convention to denote the extensions of predicate symbols in an interpretation.
The SitCalc has been extended to include a representation of belief (Shapiro et al., 2000).
The extension includes a distinguished fluent symbol B that represents an accessibility
relation on situations, similar to those used in modal logics of belief. The extension also
includes a distinguished function symbol pl that assigns a numeric value to each situation.
The function pl is a plausibility function, and the intended interpretation of pl(s1 ) < pl(s2 )
is that s1 is more plausible than s2 . The formula Bel(φ, s) which expresses the fact that φ
is believed in situation s, is defined as follows:
Bel(φ, s) ⇐⇒ ∀s0 [B(s0 , s) ∧ (∀s00 B(s00 , s) → pl(s0 ) ≤ pl(s00 ))] → φ[s0 ].
This formula states that φ is believed in s if and only if φ is true in every B-accessible
situation that is assigned a minimal pl-value. In other words, φ is believed if it is true in
all of the most plausible worlds that are considered possible.
292

Iterated Belief Change Due to Actions and Observations

Note that the accessibility relation B can be used to define a formula init(s) that defines
a set of initial situations:
init(s) ⇔ B(s, S0 ).
The set of situations initially believed possible are the pl-minimal elements of init. Since
init is a formula with one free variable, it defines a set of situations in a given first-order
theory. We will let initM denote the set of situations that satisfy init in the interpretation
M. The successor state axiom for pl is straightforward, and it guarantees the following
condition:
pl(do(a, s)) = pl(s).
In order to express the successor state axiom for B, it is convenient to distinguish between
ontic actions that change the state of the world, and sensing actions that simply give an
agent information about the world. For ontic actions, the successor state axiom for B
guarantees the following:
B(s0 , do(A, s)) ⇐⇒ ∃s00 (B(s00 , s)) ∧ s0 = do(A, s00 ).
This axiom states that s is accessible after executing A if and only if s is accessible from
some world that results from executing A in a state that is considered possible. The effects of binary sensing actions are given through a special sensing predicate SF (Levesque,
1996). For our purposes, it will be sufficient to restrict attention to sensing actions that
simply determine the truth value of a single fluent symbol. For sensing actions, the successor state axiom for B says that s0 is B-related to do(O, s) just in case s0 agrees with s on
the value of the sensed fluent symbol. The effects of sensing actions in the SitCalc define
an approach to belief revision that satisfies five of the AGM postulates (Shapiro et al., 2000).
In order to compare belief change in the SitCalc with belief evolution, we need to express
situations in terms of states. In order to simplify the discussion, we will restrict attention
to SitCalc action theories where every fluent symbol is unary, with the exception of the
distinguished accessibility fluent B. We say that a SitCalc theory T is elementary if it
satisfies the following conditions:
1. The set of fluent symbols is F ∪ {B}, where each F ∈ F is unary.
2. T is complete.
3. Every interpretation with M |= T has the following properties:
(a) M |= init(S0 ).
(b) |{x | initM (x)}| = 2|F| .
(c) If M |= init(s1 ) ∧ init(s2 ) ∧ s1 6= s2 , then there is some fluent symbol F ∈ F
such that M |= F (si ) for exactly one of s1 and s2 .
Elementary SitCalc theories are essentially categorical on the set of initial situations.
Given a model M of an elementary SitCalc theory, each situation sM defines a propositional
293

Hunter & Delgrande

interpretation IsM over the set F of unary fluent symbols. Specifically, for each situation
sM , we define IsM as follows:
IsM |= F ⇐⇒ F M (sM ).
We can also use this idea to associate a belief state with every situation sM . For ease of
readability, in the following definition we omit the superscript M on all situation terms and
fluent symbols on the right hand side:
[
κ(s,M) = {Is0 | B(s0 , s) and (∀s00 B(s00 , s) → pl(s0 ) ≤ pl(s00 ))}.
Hence κ(s,M) is the set of states with minimal plausibility among the situations that are
are B M -accessible from sM .
For a model M of an elementary SitCalc theory T , we define belief update, belief revision
and belief evolution. For any ontic action symbol A, define M as follows.
κ(s,M) M A = κ(do(A,s),M) .
Note that the plausibility function plM defines a total pre-order over the initial situations.
Since the initial situations are in 1-1 correspondence with interpretations of F, it follows
that plM defines a total pre-order over interpretations of F. Let ∗M denote the AGM
revision operator corresponding to this ordering. Finally, let ◦M denote the belief evolution
operator obtained from M and ∗M . For ease of readability, we will omit the subscript
when the theory M is clear.
In order to state our main result concisely, we introduce some simplifying notation. Let
O be a sensing action symbol for the fluent symbol FO , and let M be a suitable first-order
interpretation. We define a set of propositional interpretations over F as follows:

{I | I |= F0 }, if FOM (sM )
M
Os =
{I | I |= ¬F0 }, otherwise
Inuitively OsM is the set of interpretations that agree with IsM on the value of the fluent
FO . In the following result, let κ(s,M) ◦ hA, αi stand for the final belief state given by this
belief evolution operation.
Proposition 10 Let T be an elementary SitCalc theory, let M |= T and let ◦ be the
evolution operator induced by M. If O is a sensing action and A is an ontic action, then:
κ(do([A,O],S0 ),M) = κ(S0 ,M) ◦ hA, OSM0 )i.
Proof Without loss of generality, assume that M |= FO (do(A, S0 )). In other words,
assume that FO holds in the situation resulting from executing the action A. Under this
assumption, we must prove
κ(do([A,O],S0 ),M) = κ(S0 ,M) ∗ |FO |−1 (A)  A.
If FO happens to be false after executing A, the changes required in the proof are obvious.
Note that M |= B(s, do([A, O], S0 )) just in case:
294

Iterated Belief Change Due to Actions and Observations

M
1. s = do(A, s1 ) for some s1 with B M (sM
1 , S0 ), and

2. FOM (s).
Suppose that I ∈ κ(do([A,O],S0 ),M) . It follows that I = IsM for some situation term s that
satisfies conditions (1) and (2), and also has the property that sM is plM -minimal among
situations that are B M accessible from do([A, O], S0 )M . Consider the situation term s1 in
condition (1). Clearly IsM
∈ |FO |−1 (A), because M |= FO (do(A, s0 )). Now suppose there
1
exists s2 with the following properties:
1. IsM
∈ |FO |−1 (A)
2
2. M |= B(do([A, O], s2 ), do([A, O]), S0 ))
M M
3. plM (sM
2 ) < pl (s1 ).

By the successor state axiom for pl, it follows that
plM (do(A, s2 )M ) < plM (do(A, s1 )M ) = plM (sM ).
This contradicts the plM -minimality of sM among situations that are B M accessible from
M
do([A, O], S0 )M . Therefore, there is no such s2 . So sM
1 is pl -minimal among situations
that satisfy the first two properties above defining s2 .
Recall that M is elementary, so every propositional interpretation I ∈ |FO |−1 (A) is
equal to ItM for some initial situation t. Note that, if I ∈ |FO |−1 (A), then I = ItM for some
t that satisfies points (1) and (2) in the specification of s2 above. If I ∈ |FO |−1 (A) is less
than IsM
in the total pre-order on interpretations defined by plM , then this situation t also
1
satisfies the third condition. We have seen that this is not possible. So, if ≤M
pl is the total
M
pre-order on interpretations defined by pl , then we have:
IsM
1

∈ min(|FO |−1 (A), ≤M
pl )
= κ(S0 ,M) ∗ |FO |−1 (A)

(by definition)

But then, since sM = do(A, s1 )M :
IsM ∈ κ(S0 ,M) ∗ |FO |−1 (A)  A.
So κ(do([A,O],S0 ),M) ⊆ κ(S0 ,M) ∗ |FO |−1 (A)  A.
For the other direction, suppose that I ∈ κ(S0 ,M) ∗ |FO |−1 (A)  A. So there is some
I 0 ∈ κ(S0 ,M) ∗ |FO |−1 (A) such that I 0  A = I. But then I 0 = IsM for some plM -minimal
situation sM ∈ initM such that M |= FO (do(A, s)). Since sM ∈ initM , it follows that
M |= B(do(A, s), do(A, S0 )).
Moreover, since M |= FO (do(A, s)), it follows that
M |= B(do([A, O], s), do([A, O], S0 )).
Now suppose that there is a situation term s1 such that M |= B(s1 , do([A, O], S0 )) and
M
M
M ∈ initM
plM (sM
1 ) < pl (do(A, s)) . It follows immediately that there is some s2
295

Hunter & Delgrande

such that do([A, O], s2 )M = sM
1 . Moreover, by the successor state axiom for pl, we have
M (sM ). However, since sM ∈ initM , this contradicts the plM minimality
plM (sM
)
<
pl
2
2
of sM among initial situations. So there is no such sM
1 . Therefore Ido(A,s) ∈ κdo([A,O],S0 ) .
M
Recall that Ido(A,s)
= IsM  A = I. Therefore, I ∈ κ(do([A,O],S0 ),M) . 2
Skimming over the details, the preceding proof simply relies on the condition that M |=
pl(do(a, s)) = pl(s) in every epistemic SitCalc model. The fact that plausibility values
persist following the execution of actions is equivalent to restricting belief change by always
revising the initial belief state, then determining the final belief state by simply computing
ontic action effects. Hence, the semantics of revision actions in the SitCalc can be framed
as an instance of belief evolution. We suggest that this is an important point in discussing
belief change in the SitCalc. In the original description of the approach, the belief change
due to a sensing action is identified with belief revision (Shapiro et al., 2000). On this
view, the fact that sensing actions do not satisfy all of the AGM postulates can be seen
as a problem with the approach. However, when we are explicit about the fact that belief
change in the SitCalc is a form of belief evolution, this is no longer a problem. We do not
expect the AGM postulates to be satisfied, as we need to be concerned with the interaction
between ontic actions and sensing actions.
We conclude this section by remarking that belief evolution operators do have one
expressive advantage over the epistemic extension of the SitCalc. In the epistemic extension
that we have considered, the information obtained from sensing actions is always correct.
As a result, it is not sensible to consider revising by F followed by ¬F . By contrast, in
belief evolution, this is simply handled by keeping the more reliable observation. Hence,
belief evolution is able to deal with unreliable perception in a straightforward manner that
is not possible in the SitCalc. We remark however, that inconsistent observations have been
treated in a later extension of the SitCalc postulating exogenous actions to account for the
inconsistent sensing information (Shapiro & Pagnucco, 2004).
6.2 Iterated Belief Revision
Recall that λ is a null action that does not change the state of the world. If we consider
action domains where agents can perform λ, then sequential belief revision is a special case
of belief evolution.
Observation 1 For any κ and ᾱ, there is a unique belief state κ0 such that
κ ◦ hλ̄, ᾱi = hκ0 , . . . , κ0 i.
Since every action is null, the unique belief state κ0 is the belief state that results after the
sequence α of observations. In this section, we consider belief evolution operators from the
perspective of the well-known Darwiche-Pearl postulates for iterated revision (Darwiche &
Pearl, 1997).
First, it is important to note that belief evolution does not define iterated revision as
a simple sequence of AGM revision operations. According to Definition 11, inconsistencies
between observations are resolved by keeping the more reliable observations. By default,
we take recency as our measure of reliability. To illustrate, consider a simple example. In
296

Iterated Belief Change Due to Actions and Observations

the following expression, let α̃ denote the complement of α
κ ◦ hhλ, λi, hα̃, αii.
In this example, the observation α̃ is followed by the observation α. The final belief state
is not obtained by performing two single-shot revisions, however. According to Definition
11 with the recency ordering, the first observation is discarded because it is inconsistent
with a more recent observation. As such, the final belief state after this operation is κ ∗ α.
Hence, the approach to iterated revision that is implicit in belief evolution is not a naive
iteration. It is therefore reasonable to ask if the implicit iterated revision operator satisfies
existing rationality postulates.
We state the Darwiche-Pearl postulates in terms of possible worlds. Let κ, α, and β be
sets of possible worlds. The Darwiche-Pearl postulates are as follows.
Darwiche-Pearl Postulates
[DP1] If α ⊆ β, then (κ ∗ β) ∗ α = κ ∗ α.
[DP2] If α ⊆ β̃, then (κ ∗ β) ∗ α = κ ∗ α.
[DP3] If κ ∗ α ⊆ β, then (κ ∗ β) ∗ α ⊆ β.
[DP4] If κ ∗ α 6⊆ β̃, then (κ ∗ β) ∗ α 6⊆ β̃.
We would like to determine if these postulates hold when we perform belief evolution with
null actions.
We define the iterated revision operator obtained from ◦ as follows:
κ ∗ β ∗ α =def κ ◦ hλ̄, hβ, αii.

(6)

Of course this is not true if we apply the AGM operator on the left successively. But we
adopt this convention as a definition of iterated revision under ◦ because it allows us to ask
if the Darwiche-Pearl postulates hold. It is critical to remark, however, that we are using a
notational convention that defines iterated revision in terms of belief evolution.
Given that we have to introduce a new notational convention to even ask if the DarwichePearl postulates hold, one might question why we are concerned with these postulates.
In other words, why is it important to check that belief evolution operators satisfy key
properties of iterated belief revision? Our stance is that these postulates matter for belief
evolution because we can en up using belief evolution operators for iterated revision “by
accident.” Although our focus is on iterated sequences of actions and observations, there
will be cases where the actions are all null and we are clearly looking at a case of iterated
revision. It would be problematic if belief evolution handled such cases poorly, so we would
like to ensure that these instances of iterated revision are handled appropriately. One way
to assess “appropriateness” is by checking if the Darwiche-Pearl postulates hold.
The next result relies on the following crucial observation. If α 6= ∅, then

κ ∗ (β ∩ α) if β ∩ α 6= ∅
κ∗β∗α=
κ∗α
otherwise
This observation follows immediately from the definition of belief evolution. By using this
expression, we can prove the following result.
297

Hunter & Delgrande

Proposition 11 Let ◦ be a belief evolution operator and let α 6= ∅. Then the iterated
revision operator as given in (6) satisfies the Darwiche-Pearl postulates.
Proof Note that α−1 (λ) = α and β −1 (λ) = β. Since α 6= ∅, we need to show that the DP
postulates are satisfied by κ ∗ β ∗ α, as defined in the observation above.
For [DP1], suppose that α ⊆ β. Since α 6= ∅, it follows that β ∩ α 6= ∅ and hence
κ ∗ β ∗ α = κ ∗ (β ∩ α). But β ∩ α = α, so the right hand side is equal to κ ∗ α.
For [DP2], suppose that α ⊆ β̃. Hence, α ∩ β = ∅ and the desired conclusion follows
immediately.
For [DP3], suppose that κ ∗ α ⊆ β. Since α 6= ∅, it follows from [AGM5] that κ ∗ α 6= ∅.
So there exists some s ∈ κ ∗ α. But then s ∈ α since κ ∗ α ⊆ α, and s ∈ β since κ ∗ α ⊆ β.
Hence α ∩ β 6= ∅, and therefore
κ ∗ β ∗ α = κ ∗ (β ∩ α) ⊆ β ∩ α ⊆ β.
For [DP4], suppose that κ ∗ α 6⊆ β̃. So there exists some s ∈ κ ∗ α such that s ∈ β. It
follows that s ∈ α ∩ β, so α ∩ β 6= ∅. Translating to possible worlds, postulate [AGM7] says
the following:
if κ ∗ α 6⊆ β̃, then (κ ∗ α) ∩ β ⊆ κ ∗ (α ∩ β).
Since s ∈ (κ ∗ α) ∩ β, this implies that s ∈ κ ∗ (α ∩ β). But then, since α ∩ β 6= ∅ it follows
by definition that s ∈ κ ∗ β ∗ α. Hence s ∈ β and s ∈ κ ∗ β ∗ α. Therefore s ∈ κ ∗ β ∗ α 6⊆ β̃.
2
It is well known that many AGM revision operators do not satisfy the Darwiche-Pearl
postulates when applied in succession. Proposition 11 shows that, even if we start with a
single-shot revision operator, the approach to iterated revision induced by belief evolution
is always a Darwiche-Pearl operator.
It is easy to demonstrate that belief evolution also satisfies the so-called recalcitrance
postulate introduced by Nayak, Pagnucco and Peppas (2003). Rephrased in terms of possible worlds and belief evolution, recalcitrance is the following property:
(Recalcitrance) If β ∩ α 6= ∅, then (κ ◦ hλ̄, hβ, αi) ⊆ β.
It is known that DP1, DP2, and (Recalcitrance) characterize Nayak’s lexicographic iterated revision operator on epistemic states (Booth & Meyer, 2006). It follows from this
that our approach to iterated revision also satisfies the independence postulate introduced
independently by Jin and Thielscher (2007) as well as Booth and Meyer (2006). This
gives a complete characterization of the iterated revision operator that is implicit in belief
evolution, from the perspective of the Darwiche-Pearl tradition.
6.3 Lehmann Postulates
In this section, we consider belief evolution from the perspective of Lehmann’s postulates
(1995). Given observation trajectories O and O0 , let O · O0 denote the concatenation of the
two sequences. If α is an observation, we write O · α as a shorthand for O · hαi. Finally, for
the remainder of this section we write κ ◦ O as an abbreviation for the final belief state in
κ ◦ hλ̄, Oi. Translated into our notation, the Lehmann postulates are as follows.
298

Iterated Belief Change Due to Actions and Observations

Lehmann Postulates
[L2] κ ◦ (O · α) ⊆ α.
[L3] If κ ◦ (O · α) ⊆ β and κ ◦ O ⊆ α, then κ ◦ O ⊆ β.
[L4] If κ ◦ O ⊆ α, then κ ◦ (O · O0 ) = κ ◦ (O · α · O0 ).
[L5] If β ⊆ α, then κ ◦ (O · α · β · O0 ) = κ ◦ (O · β · O0 ).
[L6] If κ ◦ (O · α) 6⊆ β̃, then κ ◦ (O · α · β · O0 ) = κ ◦ (O · α · α ∩ β · O0 ).
[L7] κ ◦ (O · α) ⊆ κ ◦ (O · α̃ · α).
We start with postulate [L2] to remain consistent with the original numbering. However,
we omit Lehmann’s first postulate as it states that sequences of observed formulas define a
consistent theory. Since we work directly with sets of states rather than formulas, this kind
of postulate is not necessary.
From the perspective of iterated belief change, a more interesting distinction between our
approach and Lehmann’s approach can be seen by looking at postulates [L4]-[L6]. Lehmann
views [L4]-[L6] as dealing with “superfluous revisions” (Lehmann, 1995). For example, in
postulate [L4], the observation α is superfluous because revising by the observations in O
already leads an agent to believe that the actual state is in α. As such, observing α after O
does not provide any new information. The postulate [L4] suggests that such observations
may be discarded. This kind of reasoning is not supported in belief evolution, because the
observation α may take on new meaning following future observations. Postulates [L5] and
[L6] are problematic for similar reasons.
We present a counterexample that illustrates that postulates [L4]-[L6] all fail for belief
evolution.
Example
follows:

Let s1 , s2 , s3 be states over some action signature. Define κ, α, O and O0 as
κ = {s1 }
α = {s2 , s3 }
β = {s3 }
O = h{s3 }i
O0 = h{s1 , s2 }i

We demonstrate that [L4]-[L6] all fail for this example.
Let ◦ be a belief evolution operator obtained from some update operator  and some
AGM revision operator ∗. Note that
κ ◦ O = {s1 } ∗ {s3 } = {s3 } ⊆ α.
However,
κ ◦ (O · O0 ) = {s1 } ∗ {s1 , s2 } = {s1 }
κ ◦ (O · α · O0 ) = {s1 } ∗ {s2 } = {s2 }.
Hence κ ◦ (O · O0 ) 6= κ ◦ (O · α · O0 ), which violates [L4]. Since β = κ ◦ O, this also violates
[L5].
299

Hunter & Delgrande

Let γ = {s1 , s3 }. The following equalities refute [L6].
κ ◦ (O · α) = {s3 } 6⊆ γ̃
κ ◦ (O · α · γ · O0 ) = {s1 }
κ ◦ (O · α · α ∩ γ · O0 ) = {s2 }.

The preceding example demonstrates that [L4]-[L6] do not hold for belief evolution;
however, it is perhaps too abstract to illustrate the intuitive problem. At the level of
commonsense reasoning, it is instructive to imagine a situation involving a certain bird that
is either red, yellow or black. Postulate [L4] says that the following informal sequences lead
to the same belief state:
Believe(bird is red) + Observe(black, red OR yellow)
Believe(bird is red) + Observe(black, yellow OR black, red OR yellow)
However, we can see that such sequences are not the same from the perspective of our belief
evolution operators. In both sequences, the first observation is discarded2 . In the first case,
the agent is left to keep the initial belief that the bird is red because that is consistent
with the final observation. By contrast, in the second case, the final two observations are
combined to suggest that the bird is yellow. Following the intuitions of AGM revision,
the agent then believes the bird is yellow. Similar “bird colour” arguments can be used to
demonstrate whey [L5] and [L6] fail for belief evolution.
Although [L4]-[L6] do not hold, we can construct weaker versions that do hold. We
have claimed that the reason these postulates fail is because future observations may affect
the interpretation of observations that are initially superfluous. To avoid this problem, we
modify the postulates by removing the observations that follow a superfluous observation.
Weakening gives the following postulates:
Weak Lehmann Postulates
[L4∗ ] If κ ◦ O ⊆ α and O 6= ¯
∅, then κ ◦ O = κ ◦ (O · α).
∗
[L5 ] If β ⊆ α, then κ ◦ (O · α · β) = κ ◦ (O · β).
[L6∗ ] If κ ◦ (O · α) 6⊆ β̃, then κ ◦ (O · α · β) = κ ◦ (O · α · α ∩ β).
If [L4]-[L6] are replaced with [L4∗ ]-[L6∗ ], then belief evolution satisfies the resulting set of
postulates.
Proposition 12 Let ◦ be a belief evolution operator, let α 6= ∅ and let O be an observation
trajectory where each component is non-empty. Then ◦ satisfies [L2], [L3], [L4∗ ], [L5∗ ],
[L6∗ ], and [L7].
2. The first observation is only discarded under the assumption that recent information takes precedence.
As discussed previously, we are not committed to this assumption, but it is useful for the purpose of
comparison with iterated belief revision.

300

Iterated Belief Change Due to Actions and Observations

Proof Since α 6= ∅, κ ◦ (O · α) = κ ∗ γ for some γ ⊆ α. Hence κ ◦ (O · α) ⊆ γ ⊆ α, which
proves [L2].
For [L3], suppose that κ ◦ (O ·Tα) ⊆ β and κ ◦ O ⊆ α. Now suppose that T
s ∈ κ ◦ O. By
definition,
this
means
that
s
∈
κ∗
τ
(O).
By
postulate
[AGM7],
we
have
(κ∗
τ (O))∩α ⊆
T
T
κ ∗ ( τ (O) ∩ α). But κ ∗ ( τ (O) ∩ α) = κ ◦ (O · α), so it follows that s ∈ β. Therefore
κ ◦ O ⊆ β.
T
Suppose that κ ◦ O ⊆ α and T
O 6= ¯∅. By definition, κ ◦ O = κ ∗ τ (O). Since O 6= ¯
∅,
it follows that κ ◦ (O · α) = κ ∗ ( τ (O) ∩ α). With these equalities in mind, the following
results prove that [L4∗ ] holds:
\
T
κ ∗ τ (O) ⊆ (κ ∗ τ (O)) ∩ α
(since κ ◦ O ⊆ α)
T
⊆ κ ∗ ( τ (O) ∩ α)
(by [AGM7])
T
⊆
κ ∗ τ (O)
(by [AGM8])
It is clear that [L5∗ ] holds, simply because the assumption that β ⊆ α implies that
β = α ∩ β.
For [L6∗ ], suppose that κ ◦ (O · α) 6⊆ β̃. It follows that α ∩ β 6= ∅. By definition, this
means that κ ◦ (O · α · β) = κ ◦ (O · α · α ∩ β), which is the desired result.
Since α̃ ∩ α = ∅, it follows by definition that κ ◦ (O · α) = κ ◦ (O · α̃ · α). 2

Hence, if we do not consider the influence of observations that will occur in the future,
then belief evolution defines an approach to iterated revision that satisfies all of the Lehmann
postulates that do not deal with empty belief states.
We conclude with a brief remark about the assumption that α and β are non-empty in
Propositions 11 and 12. In our framework, action histories take precedence over observations. As such, empty observations are discarded because they are inconsistent with every
sequence of actions. This is true even in the case when Ā is a sequence of null actions.
We accept this treatment of inconsistent observations, because it allows inconsistency to be
treated in a uniform manner.

7. Discussion
We have presented a transition system framework for reasoning about belief change due to
actions and observations. We suggested that agents should perform belief update following
an action, and they should perform belief revision following an observation. We showed
that the interaction between update and revision can be non-elementary. Consequently we
specified how an agent should consider the history of actions when incorporating a new
observation. In contrast, existing formalisms for reasoning about epistemic action effects
either ignore the interaction between revision and update or they deal with it implicitly.
Hence, we have provided an explicit treatment of a phenomenon that has not always been
recognised in related formalisms.
The fundamental idea motivating this work is that the interpretation of an observation
may depend on the preceding sequence of actions. Our treatment of iterated belief change
includes two main components. First, we introduce a set of postulates that capture our
intuitions about the appropriate treatment of an observation following a sequence of actions.
301

Hunter & Delgrande

Informally, our postulates capture the intuitions of AGM revision for domains where actions
may occur. Second, we introduce belief evolution operators to give a concrete recipe for
combining a given update operator with a given revision operator. Belief evolution operators
satisfy our postulates, along with some standard postulates for iterated revision.
The class of problems that are appropriate for belief evolution can be described by an
ordering over action histories. Let A1 , α1 , . . . , An , αn be an alternating sequence of actions
and observations. Let  denote a total pre-order over the elements of this sequence. Belief
evolution is suitable for problems in which the underlying ordering is given as follows, for
some permutation p1 , . . . , pn of 1, . . . , n.

A1 

..
.   αp 1  αp 2  · · ·  αp n

An

Hence belief evolution is appropriate when we have any total pre-order over observations,
including the case where all observations are considered equally reliable. However, we have
not addressed the case where action histories may be incorrect.
We have framed our results in a transition system framework, primarily because it
provides a simple representation of action effects. However, our results can be translated
into related action formalisms as well. For example, in our comparison with the SitCalc, we
illustrated that the revision actions of the SitCalc are implicitly defined in terms of belief
evolution. Our results provide a general account of the interaction between actions and
observations, grounded in the context of transition systems.
It is worth noting that belief evolution operators model a specific reasoning problem
that has been addressed in more general formalisms. For example, in our own work, we
have explored the use of arbitrary plausibility rankings over both actions and observations
to reason about iterated belief change (Hunter & Delgrande, 2006). Moreover, even the
distinction between revision and update can be understood as a pragmatic distinction for
modelling certain kinds of reasoning. A single generic notion of “belief change” has been
defined such that belief revision and belief update are both special cases (Kern-Isberner,
2008). From this perspective, iterated belief change due to action is simply one particular
instance; it is an instance where the belief change operations are subject to certain constraints that encode the effects of actions. We believe this is an important instance that
is worthy of detailed study, but it is important to be aware that it can be framed and
understood at a more general level.
There are several directions for future research. One direction deals with the implementation of a belief evolution solver. We have previously explored the use of Answer Set
Planning to develop a solver for iterated belief change (Hunter, Delgrande, & Faber, 2007),
and we believe this approach could be further developed. Another important direction for
future research involves relaxing the assumption that action histories are correct. In realistic action domains, agents may be incorrect about the actions that have occurred. In
such domains, it is not always reasonable to discard an observation that is inconsistent
with the perceived action history. Instead, an agent should consider the likelihood that the
observation is correct as well as the likelihood that the action history is correct. Hence, the
most plausible histories can be determined by considering the relative plausibility of each
302

Iterated Belief Change Due to Actions and Observations

action and observation. Belief evolution operators can be seen as a specific case of this kind
of reasoning, in which action occurrences are always more plausible than observations.

References
Alchourrón, C., Gärdenfors, P., & Makinson, D. (1985). On the logic of theory change:
Partial meet functions for contraction and revision. Journal of Symbolic Logic, 50 (2),
510–530.
Booth, R., & Meyer, T. (2006). Admissible and restrained revision. Journal of Artificial
Intelligence Research, 26, 127–151.
Boutilier, C. (1995). Generalized update: Belief change in dynamic settings. In Proceedings
of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI
1995), pp. 1550–1556.
Darwiche, A., & Pearl, J. (1997). On the logic of iterated belief revision. Artificial Intelligence, 89 (1-2), 1–29.
Delgrande, J., Dubois, D., & Lang, J. (2006). Iterated revision as prioritized merging. In
Proceedings of the 10th International Conference on Principles of Knowledge Representation and Reasoning (KR2006).
Gelfond, M., & Lifschitz, V. (1998). Action languages. Linköping Electronic Articles in
Computer and Information Science, 3 (16), 1–16.
Herzig, A., Lang, J., & Marquis, P. (2004). Revision and update in multi-agent belief
structures. In Proceedings of LOFT 6.
Hunter, A., & Delgrande, J. (2005). Iterated belief change: A transition system approach. In
Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI05),
pp. 460–465.
Hunter, A., & Delgrande, J. (2006). Belief change in the context of fallible actions and observations. In Proceedings of the National Conference on Artificial Intelligence(AAAI06).
Hunter, A., Delgrande, J., & Faber, J. (2007). Using answer sets to solve belief change
problems. In Proceedings of the 9th International Conference on Logic Programming
and Non Monotonic Reasoning (LPNMR 2007).
Jin, Y., & Thielscher, M. (2007). Iterated belief revision, revised. Artificial Intelligence,
171 (1), 1–18.
Katsuno, H., & Mendelzon, A. (1991). On the difference between updating a knowledge base
and revising it. In Proceedings of the Second International Conference on Principles
of Knowledge Representation and Reasoning (KR 1991), pp. 387–394.
Katsuno, H., & Mendelzon, A. (1992). Propositional knowledge base revision and minimal
change. Artificial Intelligence, 52 (2), 263–294.
Kern-Isberner, G. (2008). Linking iterated belief change operations to nonmonotonic reasoning. In Proceedings of the 11th International Conference on Principles of Knowledge
Representation and Reasoning (KR2008).
303

Hunter & Delgrande

Lang, J. (2006). About time, revision, and update. In Proceedings of the 11th International
Workshop on Non-Monotonic Reasoning (NMR 2006).
Lehmann, D. (1995). Belief revision, revised. In Proceedings of the Fourteenth International
Joint Conference on Artificial Intelligence (IJCAI95), pp. 1534–1541.
Levesque, H. (1996). What is planning in the presence of sensing?. In Proceedings of the
Thirteenth National Conference on Artificial Intelligence (AAAI96), pp. 1139–1146.
Levesque, H., Pirri, F., & Reiter, R. (1998). Foundations for the situation calculus.
Linköping Electronic Articles in Computer and Information Science, 3 (18), 1–18.
Lobo, J., Mendez, G., & Taylor, S. (2001). Knowledge and the action description language
A. Theory and Practice of Logic Programming, 1 (2), 129–184.
Moore, R. (1985). A formal theory of knowledge and action. In Hobbs, J., & Moore, R.
(Eds.), Formal Theories of the Commonsense World, pp. 319–358. Ablex Publishing.
Nayak, A. (1994). Iterated belief change based on epistemic entrenchment. Erkenntnis, 41,
353–390.
Nayak, A., Pagnucco, M., & Peppas, P. (2003). Dynamic belief change operators. Artificial
Intelligence, 146, 193–228.
Papini, O. (2001). Iterated revision operations stemming from the history of an agent’s
observations. In Rott, H., & Williams, M. (Eds.), Frontiers in Belief Revision, pp.
279–301. Kluwer Academic Publishers.
Peppas, P., Nayak, A., Pagnucco, M., Foo, N., Kwok, R., & Prokopenko, M. (1996). Revision
vs. update: Taking a closer look. In Proceedings of the Twelfth European Conference
on Artificial Intelligence (ECAI96), pp. 95–99.
Shapiro, S., & Pagnucco, M. (2004). Iterated belief change and exogenous actions in the
situation calculus. In Proceedings of the Sixteenth European Conference on Artificial
Intelligence (ECAI’04), pp. 878–882.
Shapiro, S., Pagnucco, M., Lesperance, Y., & Levesque, H. (2000). Iterated belief change
in the situation calculus. In Proceedings of the Seventh International Conference
on Principles of Knowledge Representation and Reasoning (KR 2000), pp. 527–538.
Morgan Kaufmann Publishers.
Son, T., & Baral, C. (2001). Formalizing sensing actions: A transition function based
approach. Artificial Intelligence, 125 (1-2), 19–91.

304

Journal of Artificial Intelligence Research 40 (2011) 95-142

Submitted 07/10; published 01/11

A Monte-Carlo AIXI Approximation
Joel Veness

joelv@cse.unsw.edu.au

University of New South Wales and National ICT Australia

Kee Siong Ng

keesiong.ng@gmail.com

The Australian National University

Marcus Hutter

marcus.hutter@anu.edu.au

The Australian National University and National ICT Australia

William Uther

william.uther@nicta.com.au

National ICT Australia and University of New South Wales

David Silver

davidstarsilver@googlemail.com

Massachusetts Institute of Technology

Abstract
This paper introduces a principled approach for the design of a scalable general reinforcement
learning agent. Our approach is based on a direct approximation of AIXI, a Bayesian optimality
notion for general reinforcement learning agents. Previously, it has been unclear whether the theory
of AIXI could motivate the design of practical algorithms. We answer this hitherto open question
in the aﬃrmative, by providing the first computationally feasible approximation to the AIXI agent.
To develop our approximation, we introduce a new Monte-Carlo Tree Search algorithm along with
an agent-specific extension to the Context Tree Weighting algorithm. Empirically, we present a set
of encouraging results on a variety of stochastic and partially observable domains. We conclude by
proposing a number of directions for future research.

1. Introduction
Reinforcement Learning (Sutton & Barto, 1998) is a popular and influential paradigm for agents that
learn from experience. AIXI (Hutter, 2005) is a Bayesian optimality notion for reinforcement learning agents in unknown environments. This paper introduces and evaluates a practical reinforcement
learning agent that is directly inspired by the AIXI theory.
1.1 The General Reinforcement Learning Problem
Consider an agent that exists within some unknown environment. The agent interacts with the
environment in cycles. In each cycle, the agent executes an action and in turn receives an observation
and a reward. The only information available to the agent is its history of previous interactions. The
general reinforcement learning problem is to construct an agent that, over time, collects as much
reward as possible from the (unknown) environment.
1.2 The AIXI Agent
The AIXI agent is a mathematical solution to the general reinforcement learning problem. To
achieve generality, the environment is assumed to be an unknown but computable function; i.e.
c
⃝2011
AI Access Foundation. All rights reserved.

Veness, Ng, Hutter, Uther, & Silver

the observations and rewards received by the agent, given its past actions, can be computed by some
program running on a Turing machine. The AIXI agent results from a synthesis of two ideas:
1. the use of a finite-horizon expectimax operation from sequential decision theory for action
selection; and
2. an extension of Solomonoﬀ’s universal induction scheme (Solomonoﬀ, 1964) for future prediction in the agent context.
More formally, let U(q, a1 a2 . . . an ) denote the output of a universal Turing machine U supplied
with program q and input a1 a2 . . . an , m ∈ N a finite lookahead horizon, and ℓ(q) the length in bits
of program q. The action picked by AIXI at time t, having executed actions a1 a2 . . . at−1 and having
received the sequence of observation-reward pairs o1 r1 o2 r2 . . . ot−1 rt−1 from the environment, is
given by:
∑
∑
∑
a∗t = arg max
. . . max
[rt + · · · + rt+m ]
2−ℓ(q) .
(1)
at

ot rt

at+m

q:U(q,a1 ...at+m )=o1 r1 ...ot+m rt+m

ot+m rt+m

Intuitively, the agent considers the sum of the total reward over all possible futures up to m steps
ahead, weighs each of them by the complexity of programs consistent with the agent’s past that can
generate that future, and then picks the action that maximises expected future rewards. Equation (1)
embodies in one line the major ideas of Bayes, Ockham, Epicurus, Turing, von Neumann, Bellman,
Kolmogorov, and Solomonoﬀ. The AIXI agent is rigorously shown by Hutter (2005) to be optimal
in many diﬀerent senses of the word. In particular, the AIXI agent will rapidly learn an accurate
model of the environment and proceed to act optimally to achieve its goal.
Accessible overviews of the AIXI agent have been given by both Legg (2008) and Hutter (2007).
A complete description of the agent can be found in the work of Hutter (2005).
1.3 AIXI as a Principle
As the AIXI agent is only asymptotically computable, it is by no means an algorithmic solution to
the general reinforcement learning problem. Rather it is best understood as a Bayesian optimality
notion for decision making in general unknown environments. As such, its role in general AI research should be viewed in, for example, the same way the minimax and empirical risk minimisation
principles are viewed in decision theory and statistical machine learning research. These principles
define what is optimal behaviour if computational complexity is not an issue, and can provide important theoretical guidance in the design of practical algorithms. This paper demonstrates, for the
first time, how a practical agent can be built from the AIXI theory.
1.4 Approximating AIXI
As can be seen in Equation (1), there are two parts to AIXI. The first is the expectimax search
into the future which we will call planning. The second is the use of a Bayesian mixture over
Turing machines to predict future observations and rewards based on past experience; we will call
that learning. Both parts need to be approximated for computational tractability. There are many
diﬀerent approaches one can try. In this paper, we opted to use a generalised version of the UCT
algorithm (Kocsis & Szepesvári, 2006) for planning and a generalised version of the Context Tree
Weighting algorithm (Willems, Shtarkov, & Tjalkens, 1995) for learning. This combination of ideas,
together with the attendant theoretical and experimental results, form the main contribution of this
paper.
96

A Monte-Carlo AIXI Approximation

1.5 Paper Organisation
The paper is organised as follows. Section 2 introduces the notation and definitions we use to
describe environments and accumulated agent experience, including the familiar notions of reward,
policy and value functions for our setting. Section 3 describes a general Bayesian approach for
learning a model of the environment. Section 4 then presents a Monte-Carlo Tree Search procedure
that we will use to approximate the expectimax operation in AIXI. This is followed by a description
of the Context Tree Weighting algorithm and how it can be generalised for use in the agent setting
in Section 5. We put the two ideas together in Section 6 to form our AIXI approximation algorithm.
Experimental results are then presented in Sections 7. Section 8 provides a discussion of related
work and the limitations of our current approach. Section 9 highlights a number of areas for future
investigation.

2. The Agent Setting
This section introduces the notation and terminology we will use to describe strings of agent experience, the true underlying environment and the agent’s model of the true environment.
Notation. A string x1 x2 . . . xn of length n is denoted by x1:n . The prefix x1: j of x1:n , j ≤ n,
is denoted by x≤ j or x< j+1 . The notation generalises for blocks of symbols: e.g. ax1:n denotes
a1 x1 a2 x2 . . . an xn and ax< j denotes a1 x1 a2 x2 . . . a j−1 x j−1 . The empty string is denoted by ϵ. The
concatenation of two strings s and r is denoted by sr.
2.1 Agent Setting
The (finite) action, observation, and reward spaces are denoted by A, O, and R respectively. Also,
X denotes the joint perception space O × R.
Definition 1. A history h is an element of (A × X)∗ ∪ (A × X)∗ × A.
The following definition states that the environment takes the form of a probability distribution
over possible observation-reward sequences conditioned on actions taken by the agent.
Definition 2. An environment ρ is a sequence of conditional probability functions {ρ0 , ρ1 , ρ2 , . . . },
where ρn : An → Density (Xn ), that satisfies
∑
∀a1:n ∀x<n : ρn−1 (x<n | a<n ) =
ρn (x1:n | a1:n ).
(2)
xn ∈X

In the base case, we have ρ0 (ϵ | ϵ) = 1.
Equation (2), called the chronological condition in (Hutter, 2005), captures the natural constraint
that action an has no eﬀect on earlier perceptions x<n . For convenience, we drop the index n in ρn
from here onwards.
Given an environment ρ, we define the predictive probability
ρ(xn | ax<n an ) :=

ρ(x1:n | a1:n )
ρ(x<n | a<n )

(3)

∀a1:n ∀x1:n such that ρ(x<n | a<n ) > 0. It now follows that
ρ(x1:n | a1:n ) = ρ(x1 | a1 )ρ(x2 | ax1 a2 ) · · · ρ(xn | ax<n an ).
97

(4)

Veness, Ng, Hutter, Uther, & Silver

Definition 2 is used in two distinct ways. The first is a means of describing the true underlying
environment. This may be unknown to the agent. Alternatively, we can use Definition 2 to describe
an agent’s subjective model of the environment. This model is typically learnt, and will often only
be an approximation to the true environment. To make the distinction clear, we will refer to an
agent’s environment model when talking about the agent’s model of the environment.
Notice that ρ(· | h) can be an arbitrary function of the agent’s previous history h. Our definition of
environment is suﬃciently general to encapsulate a wide variety of environments, including standard
reinforcement learning setups such as MDPs or POMDPs.
2.2 Reward, Policy and Value Functions
We now cast the familiar notions of reward, policy and value (Sutton & Barto, 1998) into our setup.
The agent’s goal is to accumulate as much reward as it can during its lifetime. More precisely, the
agent seeks a policy that will allow it to maximise its expected future reward up to a fixed, finite,
but arbitrarily large horizon m ∈ N. The instantaneous reward values are assumed to be bounded.
Formally, a policy is a function that maps a history to an action. If we define Rk (aor≤t ) := rk for
1 ≤ k ≤ t, then we have the following definition for the expected future value of an agent acting
under a particular policy:
Definition 3. Given history ax1:t , the m-horizon expected future reward of an agent acting under
policy π : (A × X)∗ → A with respect to an environment ρ is:
 t+m


 ∑


m

(5)
vρ (π, ax1:t ) := Eρ 
Ri (ax≤t+m )  x1:t  ,
i=t+1

where for t < k ≤ t + m, ak := π(ax<k ). The quantity vm
ρ (π, ax1:t at+1 ) is defined similarly, except that
at+1 is now no longer defined by π.
The optimal policy π∗ is the policy that maximises the expected future reward. The maximal
achievable expected future reward of an agent with history h in environment ρ looking m steps ahead
∗
t
is Vρm (h) := vm
ρ (π , h). It is easy to see that if h ∈ (A × X) , then
Vρm (h)

= max
at+1

∑
xt+1

ρ(xt+1 | hat+1 ) · · · max
at+m

∑
xt+m

 t+m
 ∑
ρ(xt+m | haxt+1:t+m−1 at+m ) 



ri  .

(6)

i=t+1

For convenience, we will often refer to Equation (6) as the expectimax operation. Furthermore,
the m-horizon optimal action a∗t+1 at time t + 1 is related to the expectimax operation by
a∗t+1 = arg max Vρm (ax1:t at+1 ).
at+1

(7)

Equations (5) and (6) can be modified to handle discounted reward, however we focus on the
finite-horizon case since it both aligns with AIXI and allows for a simplified presentation.

3. Bayesian Agents
As mentioned earlier, Definition 2 can be used to describe the agent’s subjective model of the true
environment. Since we are assuming that the agent does not initially know the true environment,
98

A Monte-Carlo AIXI Approximation

we desire subjective models whose predictive performance improves as the agent gains experience.
One way to provide such a model is to take a Bayesian perspective. Instead of committing to any
single fixed environment model, the agent uses a mixture of environment models. This requires
committing to a class of possible environments (the model class), assigning an initial weight to each
possible environment (the prior), and subsequently updating the weight for each model using Bayes
rule (computing the posterior) whenever more experience is obtained. The process of learning is
thus implicit within a Bayesian setup.
The mechanics of this procedure are reminiscent of Bayesian methods to predict sequences
of (single typed) observations. The key diﬀerence in the agent setup is that each prediction may
now also depend on previous agent actions. We incorporate this by using the action conditional
definitions and identities of Section 2.
ρ

Definition 4. Given a countable model class M := {ρ1 , ρ2 , . . . } and a prior weight w0 > 0 for each
∑
∑ ρ
ρ
ρ ∈ M such that ρ∈M w0 = 1, the mixture environment model is ξ(x1:n | a1:n ) :=
w0 ρ(x1:n | a1:n ).
ρ∈M

The next proposition allows us to use a mixture environment model whenever we can use an
environment model.
Proposition 1. A mixture environment model is an environment model.
Proof. ∀a1:n ∈ An and ∀x<n ∈ Xn−1 we have that
∑
∑ ∑
∑
∑
ρ
ρ
ξ(x1:n | a1:n ) =
w0 ρ(x1:n | a1:n ) =
w0
ρ(x1:n | a1:n ) = ξ(x<n | a<n )
xn ∈X

xn ∈X ρ∈M

ρ∈M

xn ∈X

where the final step follows from application of Equation (2) and Definition 4.



The importance of Proposition 1 will become clear in the context of planning with environment
models, described in Section 4.
3.1 Prediction with a Mixture Environment Model
As a mixture environment model is an environment model, we can simply use:
ξ(x1:n | a1:n )
(8)
ξ(xn | ax<n an ) =
ξ(x<n | a<n )
to predict the next observation reward pair. Equation (8) can also be expressed in terms of a convex
combination of model predictions, with each model weighted by its posterior, from
∑ ρ
w0 ρ(x1:n | a1:n )
∑
ρ∈M
ρ
=
w ρ(xn | ax<n an ),
ξ(xn | ax<n an ) = ∑ ρ
w0 ρ(x<n | a<n ) ρ∈M n−1
ρ∈M

where the posterior weight

ρ
wn−1

for environment model ρ is given by
ρ

w ρ(x<n | a<n )
ρ
wn−1 := ∑ 0 ν
= Pr(ρ | ax<n )
w0 ν(x<n | a<n )

(9)

ν∈M

If |M| is finite, Equations (8) and (3.1) can be maintained online in O(|M|) time by using the
fact that
ρ(x1:n | a1:n ) = ρ(x<n | a<n )ρ(xn | ax<n a),
which follows from Equation (4), to incrementally maintain the likelihood term for each model.
99

Veness, Ng, Hutter, Uther, & Silver

3.2 Theoretical Properties
We now show that if there is a good model of the (unknown) environment in M, an agent using the
mixture environment model
∑
ρ
ξ(x1:n | a1:n ) :=
w0 ρ(x1:n | a1:n )
(10)
ρ∈M

will predict well. Our proof is an adaptation from the work of Hutter (2005). We present the full
proof here as it is both instructive and directly relevant to many diﬀerent kinds of practical Bayesian
agents.
First we state a useful entropy inequality.
Lemma 1 (Hutter, 2005). Let {yi } and {zi } be two probability distributions, i.e. yi ≥ 0, zi ≥ 0, and
∑
∑
i yi = i zi = 1. Then we have
∑
∑
yi
(yi − zi )2 ≤
yi ln .
zi
i
i
Theorem 1. Let µ be the true environment. The µ-expected squared diﬀerence of µ and ξ is bounded
as follows. For all n ∈ N, for all a1:n ,
n ∑
∑

(
)2
{
}
ρ
µ(x<k | a<k ) µ(xk | ax<k ak ) − ξ(xk | ax<k ak ) ≤ min − ln w0 + D1:n (µ ∥ ρ) ,
ρ∈M

k=1 x1:k

where D1:n (µ ∥ ρ) :=

∑
x1:n

1:n | a1:n )
µ(x1:n | a1:n ) ln µ(x
ρ(x1:n | a1:n ) is the KL divergence of µ(· | a1:n ) and ρ(· | a1:n ).

Proof. Combining Sections 3.2.8 and 5.1.3 from the work of Hutter (2005) we get
n ∑
∑

(
)2
µ(x<k | a<k ) µ(xk | ax<k ak ) − ξ(xk | ax<k ak )

k=1 x1:k

=
≤

n ∑
∑
k=1 x<k
n ∑
∑

µ(x<k | a<k )

∑(
xk

µ(x<k | a<k )

∑

k=1 x<k

n ∑
∑

µ(xk | ax<k ak ) − ξ(xk | ax<k ak )

µ(xk | ax<k ak ) ln

xk

µ(xk | ax<k ak )
ξ(xk | ax<k ak )

µ(xk | ax<k ak )
ξ(xk | ax<k ak )
k=1 x1:k
n ∑( ∑
) µ(x | ax a )
∑
k
<k k
=
µ(x1:n | a1:n ) ln
ξ(x
|
ax
ak )
k
<k
x
k=1 x
=

1:k

=

n ∑
∑

µ(x1:k | a1:k ) ln

=

x1:n

=

∑
x1:n

[Lemma 1]
[Equation (3)]
[Equation (2)]

k+1:n

µ(x1:n | a1:n ) ln

k=1 x1:n

∑

)2

µ(x1:n | a1:n )

n
∑
k=1

µ(x1:n | a1:n ) ln

ln

µ(xk | ax<k ak )
ξ(xk | ax<k ak )
µ(xk | ax<k ak )
ξ(xk | ax<k ak )

µ(x1:n | a1:n )
ξ(x1:n | a1:n )

[Equation (4)]

100

A Monte-Carlo AIXI Approximation

[

]
µ(x1:n | a1:n ) ρ(x1:n | a1:n )
=
µ(x1:n | a1:n ) ln
[arbitrary ρ ∈ M]
ρ(x1:n | a1:n ) ξ(x1:n | a1:n )
x1:n
∑
µ(x1:n | a1:n ) ∑
ρ(x1:n | a1:n )
=
µ(x1:n | a1:n ) ln
+
µ(x1:n | a1:n ) ln
ρ(x1:n | a1:n ) x
ξ(x1:n | a1:n )
x1:n
1:n
∑
ρ(x1:n | a1:n )
≤ D1:n (µ ∥ ρ) +
µ(x1:n | a1:n ) ln ρ
[Definition 4]
w
x1:n
0 ρ(x1:n | a1:n )
∑

ρ

= D1:n (µ ∥ ρ) − ln w0 .
Since the inequality holds for arbitrary ρ ∈ M, it holds for the minimising ρ.



In Theorem 1, take the supremum over n in the r.h.s and then the limit n → ∞ on the l.h.s.
If supn D1:n (µ ∥ ρ) < ∞ for the minimising ρ, the infinite sum on the l.h.s can only be finite if
ξ(xk | ax<k ak ) converges suﬃciently fast to µ(xk | ax<k ak ) for k → ∞ with probability 1, hence ξ
predicts µ with rapid convergence. As long as D1:n (µ ∥ ρ) = o(n), ξ still converges to µ but in
a weaker Cesàro sense. The contrapositive of the statement tells us that if ξ fails to predict the
environment well, then there is no good model in M.
3.3 AIXI: The Universal Bayesian Agent
Theorem 1 motivates the construction of Bayesian agents that use rich model classes. The AIXI
agent can be seen as the limiting case of this viewpoint, by using the largest model class expressible
on a Turing machine.
Note that AIXI can handle stochastic environments since Equation (1) can be shown to be formally equivalent to
∑
∑
∑
a∗t = arg max
. . . max
[rt + · · · + rt+m ]
2−K(ρ) ρ(x1:t+m | a1:t+m ),
(11)
at

ot rt

at+m

ρ∈MU

ot+m rt+m

where ρ(x1:t+m | a1 . . . at+m ) is the probability of observing x1 x2 . . . xt+m given actions a1 a2 . . . at+m ,
class MU consists of all enumerable chronological semimeasures (Hutter, 2005), which includes all
computable ρ, and K(ρ) denotes the Kolmogorov complexity (Li & Vitányi, 2008) of ρ with respect
to U. In the case where the environment is a computable function and
∑
ξU (x1:t | a1:t ) :=
2−K(ρ) ρ(x1:t | a1:t ),
(12)
ρ∈MU

Theorem 1 shows for all n ∈ N and for all a1:n ,
n ∑
∑

(
)2
µ(x<k | a<k ) µ(xk | ax<k ak ) − ξU (xk | ax<k ak ) ≤ K(µ) ln 2.

(13)

k=1 x1:k

3.4 Direct AIXI Approximation
We are now in a position to describe our approach to AIXI approximation. For prediction, we seek
a computationally eﬃcient mixture environment model ξ as a replacement for ξU . Ideally, ξ will
retain ξU ’s bias towards simplicity and some of its generality. This will be achieved by placing a
suitable Ockham prior over a set of candidate environment models.
101

Veness, Ng, Hutter, Uther, & Silver

For planning, we seek a scalable algorithm that can, given a limited set of resources, compute
an approximation to the expectimax action given by
a∗t+1 = arg max VξmU (ax1:t at+1 ).
at+1

The main diﬃculties are of course computational. The next two sections introduce two algorithms that can be used to (partially) fulfill these criteria. Their subsequent combination will
constitute our AIXI approximation.

4. Expectimax Approximation with Monte-Carlo Tree Search
Naı̈ve computation of the expectimax operation (Equation 6) takes O(|A × X|m ) time, unacceptable
for all but tiny values of m. This section introduces ρUCT, a generalisation of the popular MonteCarlo Tree Search algorithm UCT (Kocsis & Szepesvári, 2006), that can be used to approximate
a finite horizon expectimax operation given an environment model ρ. As an environment model
subsumes both MDPs and POMDPs, ρUCT eﬀectively extends the UCT algorithm to a wider class
of problem domains.
4.1 Background
UCT has proven particularly eﬀective in dealing with diﬃcult problems containing large state
spaces. It requires a generative model that when given a state-action pair (s, a) produces a subsequent state-reward pair (s′ , r) distributed according to Pr(s′ , r | s, a). By successively sampling
trajectories through the state space, the UCT algorithm incrementally constructs a search tree, with
each node containing an estimate of the value of each state. Given enough time, these estimates
converge to their true values.
The ρUCT algorithm can be realised by replacing the notion of state in UCT by an agent history
h (which is always a suﬃcient statistic) and using an environment model ρ to predict the next
percept. The main subtlety with this extension is that now the history condition of the percept
probability ρ(or | h) needs to be updated during the search. This is to reflect the extra information an
agent will have at a hypothetical future point in time. Furthermore, Proposition 1 allows ρUCT to be
instantiated with a mixture environment model, which directly incorporates the model uncertainty of
the agent into the planning process. This gives (in principle, provided that the model class contains
the true environment and ignoring issues of limited computation) the well known Bayesian solution
to the exploration/exploitation dilemma; namely, if a reduction in model uncertainty would lead to
higher expected future reward, ρUCT would recommend an information gathering action.
4.2 Overview
ρUCT is a best-first Monte-Carlo Tree Search technique that iteratively constructs a search tree in
memory. The tree is composed of two interleaved types of nodes: decision nodes and chance nodes.
These correspond to the alternating max and sum operations in the expectimax operation. Each
node in the tree corresponds to a history h. If h ends with an action, it is a chance node; if h ends
with an observation-reward pair, it is a decision node. Each node contains a statistical estimate of
the future reward.
Initially, the tree starts with a single decision node containing |A| children. Much like existing
MCTS methods (Chaslot, Winands, Uiterwijk, van den Herik, & Bouzy, 2008a), there are four
102

A Monte-Carlo AIXI Approximation

a1

o1

o2

a2

a3

o3

o4

future reward estimate

Figure 1: A ρUCT search tree
conceptual phases to a single iteration of ρUCT. The first is the selection phase, where the search
tree is traversed from the root node to an existing leaf chance node n. The second is the expansion
phase, where a new decision node is added as a child to n. The third is the simulation phase, where
a rollout policy in conjunction with the environment model ρ is used to sample a possible future
path from n until a fixed distance from the root is reached. Finally, the backpropagation phase
updates the value estimates for each node on the reverse trajectory leading back to the root. Whilst
time remains, these four conceptual operations are repeated. Once the time limit is reached, an
approximate best action can be selected by looking at the value estimates of the children of the root
node.
During the selection phase, action selection at decision nodes is done using a policy that balances
exploration and exploitation. This policy has two main eﬀects:
• to gradually move the estimates of the future reward towards the maximum attainable future
reward if the agent acted optimally.
• to cause asymmetric growth of the search tree towards areas that have high predicted reward,
implicitly pruning large parts of the search space.
The future reward at leaf nodes is estimated by choosing actions according to a heuristic policy
until a total of m actions have been made by the agent, where m is the search horizon. This heuristic
estimate helps the agent to focus its exploration on useful parts of the search tree, and in practice
allows for a much larger horizon than a brute-force expectimax search.
ρUCT builds a sparse search tree in the sense that observations are only added to chance nodes
once they have been generated along some sample path. A full-width expectimax search tree would
not be sparse; each possible stochastic outcome would be represented by a distinct node in the search
tree. For expectimax, the branching factor at chance nodes is thus |O|, which means that searching
to even moderate sized m is intractable.
Figure 1 shows an example ρUCT tree. Chance nodes are denoted with stars. Decision nodes
are denoted by circles. The dashed lines from a star node indicate that not all of the children have
been expanded. The squiggly line at the base of the leftmost leaf denotes the execution of a rollout
policy. The arrows proceeding up from this node indicate the flow of information back up the tree;
this is defined in more detail below.
103

Veness, Ng, Hutter, Uther, & Silver

4.3 Action Selection at Decision Nodes
A decision node will always contain |A| distinct children, all of whom are chance nodes. Associated
with each decision node representing a particular history h will be a value function estimate, V̂(h).
During the selection phase, a child will need to be picked for further exploration. Action selection in
MCTS poses a classic exploration/exploitation dilemma. On one hand we need to allocate enough
visits to all children to ensure that we have accurate estimates for them, but on the other hand we
need to allocate enough visits to the maximal action to ensure convergence of the node to the value
of the maximal child node.
Like UCT, ρUCT recursively uses the UCB policy (Auer, 2002) from the n-armed bandit setting
at each decision node to determine which action needs further exploration. Although the uniform
logarithmic regret bound no longer carries across from the bandit setting, the UCB policy has been
shown to work well in practice in complex domains such as computer Go (Gelly & Wang, 2006) and
General Game Playing (Finnsson & Björnsson, 2008). This policy has the advantage of ensuring
that at each decision node, every action eventually gets explored an infinite number of times, with
the best action being selected exponentially more often than actions of lesser utility.
Definition 5. The visit count T (h) of a decision node h is the number of times h has been sampled
by the ρUCT algorithm. The visit count of the chance node found by taking action a at h is defined
similarly, and is denoted by T (ha).
Definition 6. Suppose m is the remaining search horizon and each instantaneous reward is bounded
in the interval [α, β]. Given a node representing a history h in the search tree, the action picked by
the UCB action selection policy is:

√

(h))

1

V̂(ha) + C log(T
if T (ha) > 0;
 m(β−α)
T (ha)
(14)
aUCB (h) := arg max 

a∈A 
∞
otherwise,
where C ∈ R is a positive parameter that controls the ratio of exploration to exploitation. If there
are multiple maximal actions, one is chosen uniformly at random.
Note that we need a linear scaling of V̂(ha) in Definition 6 because the UCB policy is only
applicable for rewards confined to the [0, 1] interval.
4.4 Chance Nodes
Chance nodes follow immediately after an action is selected from a decision node. Each chance
node ha following a decision node h contains an estimate of the future utility denoted by V̂(ha).
Also associated with the chance node ha is a density ρ(· | ha) over observation-reward pairs.
After an action a is performed at node h, ρ(· | ha) is sampled once to generate the next
observation-reward pair or. If or has not been seen before, the node haor is added as a child of
ha.
4.5 Estimating Future Reward at Leaf Nodes
If a leaf decision node is encountered at depth k < m in the tree, a means of estimating the future
reward for the remaining m − k time steps is required. MCTS methods use a heuristic rollout policy
∑
Π to estimate the sum of future rewards m
i=k ri . This involves sampling an action a from Π(h),
104

A Monte-Carlo AIXI Approximation

sampling a percept or from ρ(· | ha), appending aor to the current history h and then repeating this
process until the horizon is reached. This procedure is described in Algorithm 4. A natural baseline
policy is Πrandom , which chooses an action uniformly at random at each time step.
As the number of simulations tends to infinity, the structure of the ρUCT search tree converges
to the full depth m expectimax tree. Once this occurs, the rollout policy is no longer used by ρUCT.
This implies that the asymptotic value function estimates of ρUCT are invariant to the choice of
Π. In practice, when time is limited, not enough simulations will be performed to grow the full
expectimax tree. Therefore, the choice of rollout policy plays an important role in determining
the overall performance of ρUCT. Methods for learning Π online are discussed as future work in
Section 9. Unless otherwise stated, all of our subsequent results will use Πrandom .
4.6 Reward Backup
After the selection phase is completed, a path of nodes n1 n2 . . . nk , k ≤ m, will have been traversed
from the root of the search tree n1 to some leaf nk . For each 1 ≤ j ≤ k, the statistics maintained for
history hn j associated with node n j will be updated as follows:
V̂(hn j ) ←

T (hn j )
T (hn j ) + 1

∑
1
ri
T (hn j ) + 1 i= j
m

V̂(hn j ) +

T (hn j ) ← T (hn j ) + 1

(15)
(16)

Equation (15) computes the mean return. Equation (16) increments the visit counter. Note that the
same backup operation is applied to both decision and chance nodes.
4.7 Pseudocode
The pseudocode of the ρUCT algorithm is now given.
After a percept has been received, Algorithm 1 is invoked to determine an approximate best
action. A simulation corresponds to a single call to Sample from Algorithm 1. By performing
a number of simulations, a search tree Ψ whose root corresponds to the current history h is constructed. This tree will contain estimates V̂ρm (ha) for each a ∈ A. Once the available thinking time
is exceeded, a maximising action â∗h := arg maxa∈A V̂ρm (ha) is retrieved by BestAction. Importantly,
Algorithm 1 is anytime, meaning that an approximate best action is always available. This allows
the agent to eﬀectively utilise all available computational resources for each decision.
Algorithm 1 ρUCT(h, m)
Require: A history h
Require: A search horizon m ∈ N
Initialise(Ψ)
repeat
3:
Sample(Ψ, h, m)
4: until out of time
5: return BestAction(Ψ, h)
1:

2:

For simplicity of exposition, Initialise can be understood to simply clear the entire search tree
Ψ. In practice, it is possible to carry across information from one time step to another. If Ψt is the
105

Veness, Ng, Hutter, Uther, & Silver

search tree obtained at the end of time t, and aor is the agent’s actual action and experience at time
t, then we can keep the subtree rooted at node Ψt (hao) in Ψt and make that the search tree Ψt+1 for
use at the beginning of the next time step. The remainder of the nodes in Ψt can then be deleted.
Algorithm 2 describes the recursive routine used to sample a single future trajectory. It uses
the SelectAction routine to choose moves at decision nodes, and invokes the Rollout routine at
unexplored leaf nodes. The Rollout routine picks actions according to the rollout policy Π until
the (remaining) horizon is reached, returning the accumulated reward. After a complete trajectory
of length m is simulated, the value estimates are updated for each node traversed as per Section 4.6.
Notice that the recursive calls on Lines 6 and 11 append the most recent percept or action to the
history argument.
Algorithm 2 Sample(Ψ, h, m)
Require: A search tree Ψ
Require: A history h
Require: A remaining search horizon m ∈ N
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

if m = 0 then
return 0
else if Ψ(h) is a chance node then
Generate (o, r) from ρ(or | h)
Create node Ψ(hor) if T (hor) = 0
reward ← r + Sample(Ψ, hor, m − 1)
else if T (h) = 0 then
reward ← Rollout(h, m)
else
a ← SelectAction(Ψ, h)
reward ← Sample(Ψ, ha, m)
end if
1
[reward + T (h)V̂(h)]
V̂(h) ← T (h)+1
T (h) ← T (h) + 1
return reward

The action chosen by SelectAction is specified by the UCB policy described in Definition 6. If
the selected child has not been explored before, a new node is added to the search tree. The constant
C is a parameter that is used to control the shape of the search tree; lower values of C create deep,
selective search trees, whilst higher values lead to shorter, bushier trees. UCB automatically focuses
attention on the best looking action in such a way that the sample estimate V̂ρ (h) converges to Vρ (h),
whilst still exploring alternate actions suﬃciently often to guarantee that the best action will be
eventually found.
4.8 Consistency of ρUCT
Let µ be the true underlying environment. We now establish the link between the expectimax value
Vµm (h) and its estimate V̂µm (h) computed by the ρUCT algorithm.
Kocsis and Szepesvári (2006) show that with an appropriate choice of C, the UCT algorithm
is consistent in finite horizon MDPs. By interpreting histories as Markov states, our general agent
106

A Monte-Carlo AIXI Approximation

Algorithm 3 SelectAction(Ψ, h)
Require: A search tree Ψ
Require: A history h
Require: An exploration/exploitation constant C

7:

U = {a ∈ A : T (ha) = 0}
if U , {} then
Pick a ∈ U uniformly at random
Create node Ψ(ha)
return a
else
√
{
}
log(T (h))
1
return arg max m(β−α) V̂(ha) + C
T (ha)

8:

end if

1:
2:
3:
4:
5:
6:

a∈A

Algorithm 4 Rollout(h, m)
Require: A history h
Require: A remaining search horizon m ∈ N
Require: A rollout function Π
1:
2:
3:
4:
5:
6:
7:
8:

reward ← 0
for i = 1 to m do
Generate a from Π(h)
Generate (o, r) from ρ(or | ha)
reward ← reward + r
h ← haor
end for
return reward

problem reduces to a finite horizon MDP. This means that the results of Kocsis and Szepesvári
(2006) are now directly applicable. Restating the main consistency result in our notation, we have
(
)
∀ϵ∀h lim Pr |Vµm (h) − V̂µm (h)| ≤ ϵ = 1,
T (h)→∞

(17)

that is, V̂µm (h) → Vµm (h) with probability 1. Furthermore, the probability that a suboptimal action
(with respect to Vµm (·)) is picked by ρUCT goes to zero in the limit. Details of this analysis can be
found in the work of Kocsis and Szepesvári (2006).
4.9 Parallel Implementation of ρUCT
As a Monte-Carlo Tree Search routine, Algorithm 1 can be easily parallelised. The main idea is
to concurrently invoke the Sample routine whilst providing appropriate locking mechanisms for the
interior nodes of the search tree. A highly scalable parallel implementation is beyond the scope of
the paper, but it is worth noting that ideas applicable to high performance Monte-Carlo Go programs
(Chaslot, Winands, & Herik, 2008b) can be easily transferred to our setting.
107

Veness, Ng, Hutter, Uther, & Silver

5. Model Class Approximation using Context Tree Weighting
We now turn our attention to the construction of an eﬃcient mixture environment model suitable for
the general reinforcement learning problem. If computation were not an issue, it would be suﬃcient
to first specify a large model class M, and then use Equations (8) or (3.1) for online prediction.
The problem with this approach is that at least O(|M|) time is required to process each new piece
of experience. This is simply too slow for the enormous model classes required by general agents.
Instead, this section will describe how to predict in O(log log |M|) time, using a mixture environment
model constructed from an adaptation of the Context Tree Weighting algorithm.
5.1 Context Tree Weighting
Context Tree Weighting (CTW) (Willems et al., 1995; Willems, Shtarkov, & Tjalkens, 1997) is
an eﬃcient and theoretically well-studied binary sequence prediction algorithm that works well in
practice (Begleiter, El-Yaniv, & Yona, 2004). It is an online Bayesian model averaging algorithm
that computes, at each time point t, the probability
∑
Pr(y1:t ) =
Pr(M) Pr(y1:t | M),
(18)
M

where y1:t is the binary sequence seen so far, M is a prediction suﬃx tree (Rissanen, 1983; Ron,
Singer, & Tishby, 1996), Pr(M) is the prior probability of M, and the summation is over all prediction suﬃx trees of bounded depth D. This is a huge class, covering all D-order Markov processes. A
D
naı̈ve computation of (18) takes time O(22 ); using CTW, this computation requires only O(D) time.
In this section, we outline two ways in which CTW can be generalised to compute probabilities of
the form
∑
Pr(x1:t | a1:t ) =
Pr(M) Pr(x1:t | M, a1:t ),
(19)
M

where x1:t is a percept sequence, a1:t is an action sequence, and M is a prediction suﬃx tree as in
(18). These generalisations will allow CTW to be used as a mixture environment model.
5.2 Krichevsky-Trofimov Estimator
We start with a brief review of the KT estimator (Krichevsky & Trofimov, 1981) for Bernoulli
distributions. Given a binary string y1:t with a zeros and b ones, the KT estimate of the probability
of the next symbol is as follows:
b + 1/2
a+b+1
= 0 | y1:t ) := 1 − Prkt (Yt+1 = 1 | y1:t ).

Prkt (Yt+1 = 1 | y1:t ) :=
Prkt (Yt+1

(20)
(21)

The KT estimator is obtained via a Bayesian analysis by putting an uninformative (Jeﬀreys
Beta(1/2,1/2)) prior Pr(θ) ∝ θ−1/2 (1 − θ)−1/2 on the parameter θ ∈ [0, 1] of the Bernoulli distribution. From (20)-(21), we obtain the following expression for the block probability of a string:
Prkt (y1:t ) = Prkt (y1 | ϵ)Prkt (y2 | y1 ) · · · Prkt (yt | y<t )
∫
= θb (1 − θ)a Pr(θ) dθ.
108

A Monte-Carlo AIXI Approximation

◦?
 ???0


??
 

◦?
θ1 = 0.1
?? 0
1 
??

?


1

θ01 = 0.3

θ00 = 0.5

Figure 2: An example prediction suﬃx tree
Since Prkt (s) depends only on the number of zeros a s and ones b s in a string s, if we let 0a 1b denote
a string with a zeroes and b ones, then we have
Prkt (s) = Prkt (0as 1bs ) =

1/2(1 + 1/2) · · · (a s − 1/2)1/2(1 + 1/2) · · · (b s − 1/2)
.
(a s + b s )!

(22)

We write Prkt (a, b) to denote Prkt (0a 1b ) in the following. The quantity Prkt (a, b) can be updated
incrementally (Willems et al., 1995) as follows:
a + 1/2
Prkt (a, b)
a+b+1
b + 1/2
Prkt (a, b),
Prkt (a, b + 1) =
a+b+1

Prkt (a + 1, b) =

(23)
(24)

with the base case being Prkt (0, 0) = 1.
5.3 Prediction Suﬃx Trees
We next describe prediction suﬃx trees, which are a form of variable-order Markov models.
In the following, we work with binary trees where all the left edges are labeled 1 and all the right
edges are labeled 0. Each node in such a binary tree M can be identified by a string in {0, 1}∗ as
follows: ϵ represents the root node of M; and if n ∈ {0, 1}∗ is a node in M, then n1 and n0 represent
the left and right child of node n respectively. The set of M’s leaf nodes L(M) ⊂ {0, 1}∗ form a
complete prefix-free set of strings. Given a binary string y1:t such that t ≥ the depth of M, we define
M(y1:t ) := yt yt−1 . . . yt′ , where t′ ≤ t is the (unique) positive integer such that yt yt−1 . . . yt′ ∈ L(M).
In other words, M(y1:t ) represents the suﬃx of y1:t that occurs in tree M.
Definition 7. A prediction suﬃx tree (PST) is a pair (M, Θ), where M is a binary tree and associated
with each leaf node l in M is a probability distribution over {0, 1} parametrised by θl ∈ Θ. We call
M the model of the PST and Θ the parameter of the PST, in accordance with the terminology of
Willems et al. (1995).
A prediction suﬃx tree (M, Θ) maps each binary string y1:t , where t ≥ the depth of M, to
the probability distribution θ M(y1:t ) ; the intended meaning is that θ M(y1:t ) is the probability that the
next bit following y1:t is 1. For example, the PST shown in Figure 2 maps the string 1110 to
θ M(1110) = θ01 = 0.3, which means the next bit after 1110 is 1 with probability 0.3.
In practice, to use prediction suﬃx trees for binary sequence prediction, we need to learn both
the model and parameter of a prediction suﬃx tree from data. We will deal with the model-learning
part later. Assuming the model of a PST is known/given, the parameter of the PST can be learnt
using the KT estimator as follows. We start with θl := Prkt (1 | ϵ) = 1/2 at each leaf node l of M. If
109

Veness, Ng, Hutter, Uther, & Silver

d is the depth of M, then the first d bits y1:d of the input sequence are set aside for use as an initial
context and the variable h denoting the bit sequence seen so far is set to y1:d . We then repeat the
following steps as long as needed:
1. predict the next bit using the distribution θ M(h) ;
2. observe the next bit y, update θ M(h) using Formula (20) by incrementing either a or b according
to the value of y, and then set h := hy.
5.4 Action-Conditional PST
The above describes how a PST is used for binary sequence prediction. In the agent setting, we
reduce the problem of predicting history sequences with general non-binary alphabets to that of
predicting the bit representations of those sequences. Furthermore, we only ever condition on actions. This is achieved by appending bit representations of actions to the input sequence without a
corresponding update of the KT estimators. These ideas are now formalised.
For convenience, we will assume without loss of generality that |A| = 2lA and |X| = 2lX for
some lA , lX > 0. Given a ∈ A, we denote by ⟦a⟧ = a[1, lA ] = a[1]a[2] . . . a[lA ] ∈ {0, 1}lA
the bit representation of a. Observation and reward symbols are treated similarly. Further, the bit
representation of a symbol sequence x1:t is denoted by ⟦x1:t ⟧ = ⟦x1 ⟧⟦x2 ⟧ . . . ⟦xt ⟧.
To do action-conditional sequence prediction using a PST with a given model M, we again start
with θl := Prkt (1 | ϵ) = 1/2 at each leaf node l of M. We also set aside a suﬃciently long initial
portion of the binary history sequence corresponding to the first few cycles to initialise the variable
h as usual. The following steps are then repeated as long as needed:
1. set h := h⟦a⟧, where a is the current selected action;
2. for i := 1 to lX do
(a) predict the next bit using the distribution θ M(h) ;
(b) observe the next bit x[i], update θ M(h) using Formula (20) according to the value of x[i],
and then set h := hx[i].
Let M be the model of a prediction suﬃx tree, a1:t ∈ At an action sequence, x1:t ∈ Xt an
observation-reward sequence, and h := ⟦ax1:t ⟧. For each node n in M, define h M,n by
h M,n := hi1 hi2 · · · hik

(25)

where 1 ≤ i1 < i2 < · · · < ik ≤ t and, for each i, i ∈ {i1 , i2 , . . . ik } iﬀ hi is an observation-reward bit
and n is a prefix of M(h1:i−1 ). In other words, h M,n consists of all the observation-reward bits with
context n. Thus we have the following expression for the probability of x1:t given M and a1:t :
Pr(x1:t | M, a1:t ) =
=

t
∏

Pr(xi | M, ax<i ai )

i=1
lX
t ∏
∏

Pr(xi [ j] | M, ⟦ax<i ai ⟧xi [1, j − 1])

i=1 j=1

=

∏

Prkt (h M,n ).

n∈L(M)

110

(26)

A Monte-Carlo AIXI Approximation

The last step follows by grouping the individual probability terms according to the node
n ∈ L(M) in which each bit falls and then observing Equation (22). The above deals with actionconditional prediction using a single PST. We now show how we can perform eﬃcient actionconditional prediction using a Bayesian mixture of PSTs. First we specify a prior over PST models.
5.5 A Prior on Models of PSTs
Our prior Pr(M) := 2−ΓD (M) is derived from a natural prefix coding of the tree structure of a PST.
The coding scheme works as follows: given a model of a PST of maximum depth D, a pre-order
traversal of the tree is performed. Each time an internal node is encountered, we write down 1. Each
time a leaf node is encountered, we write a 0 if the depth of the leaf node is less than D; otherwise
we write nothing. For example, if D = 3, the code for the model shown in Figure 2 is 10100; if
D = 2, the code for the same model is 101. The cost ΓD (M) of a model M is the length of its code,
which is given by the number of nodes in M minus the number of leaf nodes in M of depth D. One
can show that
∑
2−ΓD (M) = 1,
M∈C D

where C D is the set of all models of prediction suﬃx trees with depth at most D; i.e. the prefix code
is complete. We remark that the above is another way of describing the coding scheme in Willems
et al. (1995). Note that this choice of prior imposes an Ockham-like penalty on large PST structures.
5.6 Context Trees
The following data structure is a key ingredient of the Action-Conditional CTW algorithm.
Definition 8. A context tree of depth D is a perfect binary tree of depth D such that attached to each
node (both internal and leaf) is a probability on {0, 1}∗ .
The node probabilities in a context tree are estimated from data by using a KT estimator at each
node. The process to update a context tree with a history sequence is similar to a PST, except that:
1. the probabilities at each node in the path from the root to a leaf traversed by an observed bit
are updated; and
2. we maintain block probabilities using Equations (22) to (24) instead of conditional probabilities.
This process can be best understood with an example. Figure 3 (left) shows a context tree of depth
two. For expositional reasons, we show binary sequences at the nodes; the node probabilities are
computed from these. Initially, the binary sequence at each node is empty. Suppose 1001 is the
history sequence. Setting aside the first two bits 10 as an initial context, the tree in the middle of
Figure 3 shows what we have after processing the third bit 0. The tree on the right is the tree we
have after processing the fourth bit 1. In practice, we of course only have to store the counts of
zeros and ones instead of complete subsequences at each node because, as we saw earlier in (22),
Prkt (s) = Prkt (a s , b s ). Since the node probabilities are completely determined by the input sequence,
we shall henceforth speak unambiguously about the context tree after seeing a sequence.
The context tree of depth D after seeing a sequence h has the following important properties:
1. the model of every PST of depth at most D can be obtained from the context tree by pruning
oﬀ appropriate subtrees and treating them as leaf nodes;
111

Veness, Ng, Hutter, Uther, & Silver

ϵ?
 ???0

?


ϵ?
ϵ
??0 1  ???0
1 
?
??


? 



1 

1

ϵ

ϵ ϵ

ϵ

ϵ



ϵ?
?? 0
1 
??





01 ?

0?

?? 0
??
?

1 



ϵ 0

?? 0
??


1 

0?

?? 0
??
?

ϵ

ϵ



ϵ?
?? 0
1 
??





01 ?

1 



ϵ 0

?? 0
??
?

1

Figure 3: A depth-2 context tree (left); trees after processing two bits (middle and right)
2. the block probability of h as computed by each PST of depth at most D can be obtained from
the node probabilities of the context tree via Equation (26).
These properties, together with an application of the distributive law, form the basis of the highly
eﬃcient Action Conditional CTW algorithm. We now formalise these insights.
5.7 Weighted Probabilities
The weighted probability Pnw of each node n in the context tree T after seeing h := ⟦ax1:t ⟧ is defined
inductively as follows:



if n is a leaf node;
Prkt (hT,n )
Pnw := 
(27)

1
1
n0
n1
 Prkt (hT,n ) + Pw × Pw otherwise,
2
2
where hT,n is as defined in (25).
Lemma 2 (Willems et al., 1995). Let T be the depth-D context tree after seeing h := ⟦ax1:t ⟧. For
each node n in T at depth d, we have
∑
∏
Pnw =
2−ΓD−d (M)
Prkt (hT,nn′ ).
(28)
n′ ∈L(M)

M∈C D−d

Proof. The proof proceeds by induction on d. The statement is clearly true for the leaf nodes at
depth D. Assume now the statement is true for all nodes at depth d + 1, where 0 ≤ d < D. Consider
a node n at depth d. Letting d = D − d, we have
1
1
Pnw = Prkt (hT,n ) + Pn0
Pn1
2
2 w w


 ∑
  ∑

∏
∏
1
1 



−Γd+1 (M)
−Γ
(M)


d+1



= Prkt (hT,n ) + 
2
Prkt (hT,n0n′ ) 
2
Prkt (hT,n1n′ )




2
2 M∈C
M∈Cd+1
n′ ∈L(M)
n′ ∈L(M)
d+1



∑
∑
 ∏
  ∏

1
−(Γd+1 (M1 )+Γd+1 (M2 )+1) 
2
Prkt (hT,n0n′ ) 
Prkt (hT,n1n′ )
= Prkt (hT,n ) +


2
M1 ∈Cd+1 M2 ∈Cd+1
n′ ∈L(M1 )
n′ ∈L(M2 )
∑
∏
1
[
= Prkt (hT,n ) +
2−Γd ( M1 M2 )
Prkt (hT,nn′ )
2
′
[
[
n ∈L( M1 M2 )
M1 M2 ∈Cd
∑
∏
−ΓD−d (M)
=
2
Prkt (hT,nn′ ),
M∈C D−d

n′ ∈L(M)

where M[
1 M2 denotes the tree in C d whose left and right subtrees are M1 and M2 respectively.
112



A Monte-Carlo AIXI Approximation

5.8 Action Conditional CTW as a Mixture Environment Model
A corollary of Lemma 2 is that at the root node ϵ of the context tree T after seeing h := ⟦ax1:t ⟧, we
have
∑
∏
Prkt (hT,l )
(29)
Pϵw =
2−ΓD (M)
M∈C D

=

∑

l∈L(M)
−ΓD (M)

(30)

2−ΓD (M) Pr(x1:t | M, a1:t ),

(31)

M∈C D

=

∑

∏

Prkt (h M,l )

2

l∈L(M)

M∈C D

where the last step follows from Equation (26). Equation (31) shows that the quantity computed
by the Action-Conditional CTW algorithm is exactly a mixture environment model. Note that the
conditional probability is always defined, as CTW assigns a non-zero probability to any sequence.
To sample from this conditional probability, we simply sample the individual bits of xt one by one.
In summary, to do prediction using Action-Conditional CTW, we set aside a suﬃciently long
initial portion of the binary history sequence corresponding to the first few cycles to initialise the
variable h and then repeat the following steps as long as needed:
1. set h := h⟦a⟧, where a is the current selected action;
2. for i := 1 to lX do
(a) predict the next bit using the weighted probability Pϵw ;
(b) observe the next bit x[i], update the context tree using h and x[i], calculate the new
weighted probability Pϵw , and then set h := hx[i].
5.9 Incorporating Type Information
One drawback of the Action-Conditional CTW algorithm is the potential loss of type information
when mapping a history string to its binary encoding. This type information may be needed for
predicting well in some domains. Although it is always possible to choose a binary encoding scheme
so that the type information can be inferred by a depth limited context tree, it would be desirable to
remove this restriction so that our agent can work with arbitrary encodings of the percept space.
One option would be to define an action-conditional version of multi-alphabet CTW (Tjalkens,
Shtarkov, & Willems, 1993), with the alphabet consisting of the entire percept space. The downside
of this approach is that we then lose the ability to exploit the structure within each percept. This
can be critical when dealing with large observation spaces, as noted by McCallum (1996). The key
diﬀerence between his U-Tree and USM algorithms is that the former could discriminate between
individual components within an observation, whereas the latter worked only at the symbol level.
As we shall see in Section 7, this property can be helpful when dealing with larger problems.
Fortunately, it is possible to get the best of both worlds. We now describe a technique that
incorporates type information whilst still working at the bit level. The trick is to chain together k :=
lX action conditional PSTs, one for each bit of the percept space, with appropriately overlapping
binary contexts. More precisely, given a history h, the context for the ith PST is the most recent
D + i − 1 bits of the bit-level history string ⟦h⟧x[1, i − 1]. To ensure that each percept bit is dependent
on the same portion of h, D + i − 1 (instead of only D) bits are used. Thus if we denote the PST
113

Veness, Ng, Hutter, Uther, & Silver

model for the ith bit in a percept x by Mi , and the joint model by M, we now have:
Pr(x1:t | M, a1:t ) =
=

t
∏

Pr(xi | M, ax<i ai )

i=1
t ∏
k
∏

Pr(xi [ j] | M j , ⟦ax<i ai ⟧xi [1, j − 1])

(32)

i=1 j=1

=

k
∏

Pr(x1:t [ j] | M j , x1:t [− j], a1:t )

j=1

where x1:t [i] denotes x1 [i]x2 [i] . . . xt [i], x1:t [−i] denotes x1 [−i]x2 [−i] . . . xt [−i], with xt [− j] denoting
xt [1] . . . xt [ j − 1]xt [ j + 1] . . . xt [k]. The last step follows by swapping the two products in (32) and
using the above notation to refer to the product of probabilities of the jth bit in each percept xi , for
1 ≤ i ≤ t.
We next place a prior on the space of factored PST models M ∈ C D × · · · × C D+k−1 by assuming
that each factor is independent, giving
k
∏

Pr(M) = Pr(M1 , . . . , Mk ) =

2

−ΓDi (Mi )

=2

−

k
∑
i=1

ΓDi (Mi )

,

i=1

where Di := D + i − 1. This induces the following mixture environment model
∑

ξ(x1:t | a1:t ) :=

2

−

k
∑
i=1

ΓDi (Mi )

Pr(x1:t | M, a1:t ).

(33)

M∈C D1 ×···×C Dk

This can now be rearranged into a product of eﬃciently computable mixtures, since
ξ(x1:t | a1:t ) =

∑

∑

···

M1 ∈C D1

2

−

k
∑
i=1

k
ΓDi (Mi ) ∏

Mk ∈C Dk

Pr(x1:t [ j] | M j , x1:t [− j], a1:t )

j=1




k 
∏
 ∑

−ΓD j (M j )

=
2
Pr(x
[
j]
|
M
,
x
[−
j],
a
)
 .
1:t
j 1:t
1:t 


(34)

j=1 M j ∈C D j

Note that for each factor within Equation (34), a result analogous to Lemma 2 can be established by
appropriately modifying Lemma 2’s proof to take into account that now only one bit per percept is
being predicted. This leads to the following scheme for incrementally maintaining Equation (33):
1. Initialise h ← ϵ, t ← 1. Create k context trees.
2. Determine action at . Set h ← hat .
3. Receive xt . For each bit xt [i] of xt , update the ith context tree with xt [i] using history
hx[1, i − 1] and recompute Pϵw using Equation (27).
4. Set h ← hxt , t ← t + 1. Goto 2.
We will refer to this technique as Factored Action-Conditional CTW, or the FAC-CTW algorithm
for short.
114

A Monte-Carlo AIXI Approximation

5.10 Convergence to the True Environment
We now show that FAC-CTW performs well in the class of stationary n-Markov environments. Importantly, this includes the class of Markov environments used in state-based reinforcement learning,
where the most recent action/observation pair (at , xt−1 ) is a suﬃcient statistic for the prediction of
xt .
Definition 9. Given n ∈ N, an environment µ is said to be n-Markov if for all t > n, for all a1:t ∈ At ,
for all x1:t ∈ Xt and for all h ∈ (A × X)t−n−1 × A
µ(xt | ax<t at ) = µ(xt | hxt−n axt−n+1:t−1 at ).

(35)

Furthermore, an n-Markov environment is said to be stationary if for all ax1:n an+1 ∈ (A × X)n × A,
for all h, h′ ∈ (A × X)∗ ,
µ(· | hax1:n an+1 ) = µ(· | h′ ax1:n an+1 ).
(36)
It is easy to see that any stationary n-Markov environment can be represented as a product of
suﬃciently large, fixed parameter PSTs. Theorem 1 states that the predictions made by a mixture
environment model only converge to those of the true environment when the model class contains
a model suﬃciently close to the true environment. However, no stationary n-Markov environment
model is contained within the model class of FAC-CTW, since each model updates the parameters
for its KT-estimators as more data is seen. Fortunately, this is not a problem, since this updating
produces models that are suﬃciently close to any stationary n-Markov environment for Theorem 1
to be meaningful.
Lemma 3. If M is the model class used by FAC-CTW with a context depth D, µ is an environment
expressible as a product of k := lX fixed parameter PSTs (M1 , Θ1 ), . . . , (Mk , Θk ) of maximum depth
D and ρ(· | a1:n ) ≡ Pr(· | (M1 , . . . , Mk ), a1:n ) ∈ M then for all n ∈ N, for all a1:n ∈ An ,
(

k
∑

n
D1:n (µ || ρ) ≤
|L(M j )| γ
|L(M j )|
j=1
{

where
γ(z) :=

z
1
2

log z + 1

for
for

)

0≤z<1
z ≥ 1.

Proof. For all n ∈ N, for all a1:n ∈ An ,
∑

µ(x1:n | a1:n )
ρ(x1:n | a1:n )
x1:n
∏k
∑
j=1 Pr(x1:n [ j] | M j , Θ j , x1:n [− j], a1:n )
=
µ(x1:n | a1:n ) ln ∏k
x1:n
j=1 Pr(x1:n [ j] | M j , x1:n [− j], a1:n )

D1:n (µ || ρ) =

∑

µ(x1:n | a1:n ) ln

k
∑

Pr(x1:n [ j] | M j , Θ j , x1:n [− j], a1:n )
Pr(x1:n [ j] | M j , x1:n [− j], a1:n )
x1:n
j=1
)
(
k
∑
∑
n
≤
µ(x1:n | a1:n )
|L(M j )|γ
|L(M j )|
x
j=1
=

µ(x1:n | a1:n )

ln

1:n

115

(37)

Veness, Ng, Hutter, Uther, & Silver

(

k
∑

n
=
|L(M j )| γ
|L(M j )|
j=1

)

where Pr(x1:n [ j] | M j , Θ j , x1:n [− j], a1:n ) denotes the probability of a fixed parameter PST (M j , Θ j )
generating the sequence x1:n [ j] and the bound introduced in (37) is from the work of Willems et al.
(1995).

If the unknown environment µ is stationary and n-Markov, Lemma 3 and Theorem 1 can be
applied to the FAC-CTW mixture environment model ξ. Together they imply that the cumulative µexpected squared diﬀerence between µ and ξ is bounded by O(log n). Also, the per cycle µ-expected
squared diﬀerence between µ and ξ goes to zero at the rapid rate of O(log n/n). This allows us to
conclude that FAC-CTW (with a suﬃciently large context depth) will perform well on the class of
stationary n-Markov environments.
5.11 Summary
We have described two diﬀerent ways in which CTW can be extended to define a large and
eﬃciently computable mixture environment model. The first is a complete derivation of the
Action-Conditional CTW algorithm first presented in the work of Veness, Ng, Hutter, and Silver
(2010). The second is the introduction of the FAC-CTW algorithm, which improves upon ActionConditional CTW by automatically exploiting the type information available within the agent setting.
As the rest of the paper will make extensive use of the FAC-CTW algorithm, for clarity we
define
∑ − ∑k ΓDi (Mi )
Υ(x1:t | a1:t ) :=
2 i=1
Pr(x1:t | M, a1:t ).
(38)
M∈C D1 ×···×C Dk

Also recall that using Υ as a mixture environment model, the conditional probability of xt given
ax<t at is
Υ(x1:t | a1:t )
Υ(xt | ax<t at ) =
,
Υ(x<t | a<t )
which follows directly from Equation (3). To generate a percept from this conditional probability
distribution, we simply sample lX bits, one by one, from Υ.
5.12 Relationship to AIXI
Before moving on, we examine the relationship between AIXI and our model class approximation.
Using Υ in place of ρ in Equation (6), the optimal action for an agent at time t, having experienced
ax1:t−1 , is given by
 t+m 
∑ Υ(x1:t | a1:t )
∑ Υ(x1:t+m | a1:t+m ) ∑
 r 
∗
at = arg max
· · · max
i
at
at+m
Υ(x<t | a<t )
Υ(x<t+m | a<t+m )  i=t 
xt
xt+m
 t+m  t+m
∑
∏ Υ(x1:i | a1:i )
∑ ∑
 r 
= arg max
· · · max
i


at
at+m
Υ(x<i | a<i )
xt
xt+m i=t
i=t
 t+m 
∑
∑ ∑  Υ(x1:t+m | a1:t+m )
 r 
= arg max
· · · max
i

at
at+m
Υ(x<t | a<t )
x
x
i=t
t

t+m

116

A Monte-Carlo AIXI Approximation

= arg max
at

∑
xt

 t+m 
∑ ∑
 r  Υ(x
· · · max
i
1:t+m | a1:t+m )

at+m
xt+m

i=t

 t+m 
∑
∑ ∑
 r 
= arg max
· · · max
i

at

at+m

xt

xt+m

i=t

∑

2

−

k
∑
i=1

ΓDi (Mi )

Pr(x1:t+m | M, a1:t+m ).

M∈C D1 ×···×C Dk

Contrast (39) now with Equation (11) which we reproduce here:
 t+m 
∑
∑ ∑
∑
 r 
. . . max
2−K(ρ) ρ(x1:t+m | a1:t+m ),
a∗t = arg max
i

at

xt

at+m

xt+m

(39)

i=t

(40)

ρ∈M

where M is the class of all enumerable chronological semimeasures, and K(ρ) denotes the Kolmogorov complexity of ρ. The two expressions share a prior that enforces a bias towards simpler
models. The main diﬀerence is in the subexpression describing the mixture over the model class.
AIXI uses a mixture over all enumerable chronological semimeasures. This is scaled down to a
(factored) mixture of prediction suﬃx trees in our setting. Although the model class used in AIXI
is completely general, it is also incomputable. Our approximation has restricted the model class to
gain the desirable computational properties of FAC-CTW.

6. Putting it All Together
Our approximate AIXI agent, MC-AIXI(fac-ctw), is realised by instantiating the ρUCT algorithm
with ρ = Υ. Some additional properties of this combination are now discussed.
6.1 Convergence of Value
We now show that using Υ in place of the true environment µ in the expectimax operation leads to
good behaviour when µ is both stationary and n-Markov. This result combines Lemma 3 with an
adaptation of the work of Hutter (2005, Thm. 5.36). For this analysis, we assume that the instantaneous rewards are non-negative (with no loss of generality), FAC-CTW is used with a suﬃciently
large context depth, the maximum life of the agent b ∈ N is fixed and that a bounded planning
horizon mt := min(H, b − t + 1) is used at each time t, with H ∈ N specifying the maximum planning
horizon.
Theorem 2. Using the FAC-CTW algorithm, for every policy π, if the true environment µ is expressible as a product of k PSTs (M1 , Θ1 ), . . . , (Mk , Θk ), for all b ∈ N, we have

(
)
k
k
b
[(
∑
∑
∑
)2 ]

b
mt
mt
3 2 

E x<t ∼µ vΥ (π, ax<t ) − vµ (π, ax<t ) ≤ 2H rmax  ΓDi (Mi ) +
|L(M j )| γ

|L(M
)|
j
i=1
j=1
t=1
t
where rmax is the maximum instantaneous reward, γ is as defined in Lemma 3 and vm
µ (π, ax<t ) is the
value of policy π as defined in Definition 3.

Proof. First define ρ(xi: j | a1: j , x<i ) := ρ(x1: j | a1: j )/ρ(x<i | a<i ) for i < j , for any environment model
ρ and let at:mt be the actions chosen by π at times t to mt . Now



∑
 m


t
 (rt + · · · + rm ) [Υ(xt:m | a1:m , x<t ) − µ(xt:m | a1:m , x<t )]
vΥt (π, ax<t ) − vm
µ (π, ax<t ) = 
t
t
t
t
t

 xt:mt
117

Veness, Ng, Hutter, Uther, & Silver

≤

∑



(rt + · · · + rmt ) Υ(xt:mt | a1:mt , x<t ) − µ(xt:mt | a1:mt , x<t )

xt:mt

≤

mt rmax

∑
Υ(x | a , x ) − µ(x | a , x )
t:mt
1:mt <t
t:mt
1:mt <t
xt:mt

=: mt rmax At:mt (µ || Υ).
Applying this bound, a property of absolute distance (Hutter, 2005, Lemma 3.11) and the chain rule
for KL-divergence (Cover & Thomas, 1991, p. 24) gives
b
∑

b
[(
∑
)2 ]
]
[
mt
2 2
t
(π,
ax
)
(π,
ax
)
−
v
≤
m
r
E x<t ∼µ At:mt (µ || Υ)2
E x<t ∼µ vm
<t
<t
µ
t max
Υ
t=1

t=1
2
≤ 2H 2 rmax

b
∑

]
[
2
E x<t ∼µ Dt:mt (µ || Υ) = 2H 2 rmax

∑

[
]
E x<i ∼µ Di:i (µ || Υ)

t=1 i=t

t=1
2
≤ 2H 3 rmax

mt
b ∑
∑

b
∑

[
]
2
E x<t ∼µ Dt:t (µ || Υ) = 2H 3 rmax
D1:b (µ || Υ),

t=1

where Di: j (µ || Υ) := xi: j µ(xi: j | a1: j , x<i ) ln(Υ(xi: j | a1: j , x<i )/µ(xi: j | a1: j , x<i )). The final inequality
uses the fact that any particular Di:i (µ || Υ) term appears at most H times in the preceding double
sum. Now define ρ M (· | a1:b ) := Pr(· | (M1 , . . . , Mk ), a1:b ) and we have
[
]
∑
µ(x1:b | a1:b ) ρ M (x1:b | a1:b )
D1:b (µ || Υ) =
µ(x1:b | a1:b ) ln
ρ M (x1:b | a1:b ) Υ(x1:b | a1:b )
x1:b
∑
∑
µ(x1:b | a1:b )
ρ M (x1:b | a1:b )
=
µ(x1:b | a1:b ) ln
+
µ(x1:b | a1:b ) ln
ρ M (x1:b | a1:b ) x
Υ(x1:b | a1:b )
x1:b
1:b
∑
ρ M (x1:b | a1:b )
≤ D1:b (µ ∥ ρ M ) +
µ(x1:b | a1:b ) ln ρM
w0 ρ M (x1:b | a1:b )
x1:b
= D1:b (µ ∥ ρ M ) +

k
∑

ΓDi (Mi )

i=1
ρ
w0 M

−

k
∑

ΓDi (Mi )

where
:= 2
and the final inequality follows by dropping all but ρ M ’s contribution to
Equation (38). Using Lemma 3 to bound D1:b (µ ∥ ρ M ) now gives the desired result.

i=1

For any fixed H, Theorem 2 shows that the cumulative expected squared diﬀerence of the true
and Υ values is bounded by a term that grows at the rate of O(log b). The average expected squared
log b
diﬀerence of the two values then goes down to zero at the rate of O( b ). This implies that for
suﬃciently large b, the value estimates using Υ in place of µ converge for any fixed policy π.
Importantly, this includes the fixed horizon expectimax policy with respect to Υ.
6.2 Convergence to Optimal Policy
This section presents a result for n-Markov environments that are both ergodic and stationary. Intuitively, this class of environments never allow the agent to make a mistake from which it can
no longer recover. Thus in these environments an agent that learns from its mistakes can hope to
achieve a long-term average reward that will approach optimality.
118

A Monte-Carlo AIXI Approximation

Definition 10. An n-Markov environment µ is said to be ergodic if there exists a policy π such that
every sub-history s ∈ (A × X)n possible in µ occurs infinitely often (with probability 1) in the history
generated by an agent/environment pair (π, µ).
Definition 11. A sequence of policies {π1 , π2 , . . . } is said to be self optimising with respect to model
class M if
1 m
1
vρ (πm , ϵ) − Vρm (ϵ) → 0
as m → ∞ for all ρ ∈ M.
(41)
m
m
A self optimising policy has the same long-term average expected future reward as the optimal
policy for any environment in M. In general, such policies cannot exist for all model classes.
We restrict our attention to the set of stationary, ergodic n-Markov environments since these are
what can be modeled eﬀectively by FAC-CTW. The ergodicity property ensures that no possible
percepts are precluded due to earlier actions by the agent. The stationarity property ensures that the
environment is suﬃciently well behaved for a PST to learn a fixed set of parameters.
We now prove a lemma in preparation for our main result.
Lemma 4. Any stationary, ergodic n-Markov environment can be modeled by a finite, ergodic MDP.
Proof. Given an ergodic n-Markov environment µ, with associated action space A and percept space
X, an equivalent, finite MDP (S , A, T, R) can be constructed from µ by defining the state space as
S := (A × X)n , the action space as A := A, the transition probability as T a (s, s′ ) := µ(o′ r′ | hsa)
and the reward function as Ra (s, s′ ) := r′ , where s′ is the suﬃx formed by deleting the leftmost
action/percept pair from sao′ r′ and h is an arbitrary history from (A × X)∗ . T a (s, s′ ) is well defined
for arbitrary h since µ is stationary, therefore Eq. (36) applies. Definition 10 implies that the derived
MDP is ergodic.

Theorem 3. Given a mixture environment model ξ over a model class M consisting
{ ξ ξ of} a countable
set of stationary, ergodic n-Markov environments, the sequence of policies π1 , π2 , . . . where
ξ

πb (ax<t ) := arg max Vξb−t+1 (ax<t at )
at ∈A

(42)

for 1 ≤ t ≤ b, is self-optimising with respect to model class M.
Proof. By applying Lemma 4 to each ρ ∈ M, an equivalent model class N of finite, ergodic MDPs
can be produced. We know from Hutter (2005, Thm. 5.38) that a sequence of policies for N that
is self-optimising exists. This implies the existence of a corresponding sequence of policies for M
that is self-optimising.
{ ξ ξ
} Using the work of Hutter (2005, Thm. 5.29), this implies that the sequence
of policies π1 , π2 , . . . is self optimising.

Theorem 3 says that by choosing a suﬃciently large lifespan b, the average reward for an agent
ξ
following policy πb can be made arbitrarily close to the optimal average reward with respect to the
true environment.
Theorem 3 and the consistency of the ρUCT algorithm (17) give support to the claim that
the MC-AIXI(fac-ctw) agent is self-optimising with respect to the class of stationary, ergodic, nMarkov environments. The argument isn’t completely rigorous, since the usage of the KT-estimator
implies that the model class of FAC-CTW contains an uncountable number of models. Our conclusion is not entirely unreasonable however. The justification is that a countable mixture of PSTs
119

Veness, Ng, Hutter, Uther, & Silver

behaving similarly to the FAC-CTW mixture can be formed by replacing each PST leaf node KTestimator with a finely grained, discrete Bayesian mixture predictor. Under this interpretation, a
floating point implementation of the KT-estimator would correspond to a computationally feasible
approximation of the above.
The results used in the proof of Theorem 3 can be found in the works of Hutter (2002b) and
Legg and Hutter (2004). An interesting area for future research would be to investigate whether a
self-optimising result similar to the work of Hutter (2005, Thm. 5.29) holds for continuous mixtures.
6.3 Computational Properties
The FAC-CTW algorithm grows each context tree data structure dynamically. With a context depth
D, there are at most O(tD log(|O||R|)) nodes in the set of context trees after t cycles. In practice,
this is considerably less than log(|O||R|)2D , which is the number of nodes in a fully grown set of
context trees. The time complexity of FAC-CTW is also impressive; O(Dm log(|O||R|)) to generate
the m percepts needed to perform a single ρUCT simulation and O(D log(|O||R|)) to process each
new piece of experience. Importantly, these quantities are not dependent on t, which means that the
performance of our agent does not degrade with time. Thus it is reasonable to run our agent in an
online setting for millions of cycles. Furthermore, as FAC-CTW is an exact algorithm, we do not
suﬀer from approximation issues that plague sample based approaches to Bayesian learning.
6.4 Eﬃcient Combination of FAC-CTW with ρUCT
Earlier, we showed how FAC-CTW can be used in an online setting. An additional property however
is needed for eﬃcient use within ρUCT. Before Sample is invoked, FAC-CTW will have computed
a set of context trees for a history of length t. After a complete trajectory is sampled, FAC-CTW
will now contain a set of context trees for a history of length t + m. The original set of context
trees now needs to be restored. Saving and copying the original context trees is unsatisfactory, as is
rebuilding them from scratch in O(tD log(|O||R|)) time. Luckily, the original set of context trees can
be recovered eﬃciently by traversing the history at time t + m in reverse, and performing an inverse
update operation on each of the D aﬀected nodes in the relevant context tree, for each bit in the
sample trajectory. This takes O(Dm log(|O||R|)) time. Alternatively, a copy on write implementation
can be used to modify the context trees during the simulation phase, with the modified copies of
each context node discarded before Sample is invoked again.
6.5 Exploration/Exploitation in Practice
Bayesian belief updating combines well with expectimax based planning. Agents using this combination, such as AIXI and MC-AIXI(fac-ctw), will automatically perform information gathering
actions if the expected reduction in uncertainty would lead to higher expected future reward. Since
AIXI is a mathematical notion, it can simply take a large initial planning horizon b, e.g. its maximal
lifespan, and then at each cycle t choose greedily with respect to Equation (1) using a remaining
horizon of b − t + 1. Unfortunately in the case of MC-AIXI(fac-ctw), the situation is complicated
by issues of limited computation.
In theory, the MC-AIXI(fac-ctw) agent could always perform the action recommended by
ρUCT. In practice however, performing an expectimax operation with a remaining horizon of b−t+1
is not feasible, even using Monte-Carlo approximation. Instead we use as large a fixed search hori120

A Monte-Carlo AIXI Approximation

Environment

Perform action in real world

Record new sensor information
... Past Observation/Reward Action

... Past

Record Action

Observation/Reward

MC-AIXI
An approximate AIXI agent

Refine environment model

a1

o1

o2

a2

a3

o3

o4

Update Bayesian Mixture of Models

+

-

-

-

+

-

future reward estimate

.......
Simple

Complex

Large Prior

Small Prior

Determine best action

Figure 4: The MC-AIXI agent loop

zon as we can aﬀord computationally, and occasionally force exploration according to some heuristic policy. The intuition behind this choice is that in many domains, good behaviour can be achieved
by using a small amount of planning if the dynamics of the domain are known. Note that it is still
possible for ρUCT to recommend an exploratory action, but only if the benefits of this information
can be realised within its limited planning horizon. Thus, a limited amount of exploration can help
the agent avoid local optima with respect to its present set of beliefs about the underlying environment. Other online reinforcement learning algorithms such as SARSA(λ) (Sutton & Barto, 1998),
U-Tree (McCallum, 1996) or Active-LZ (Farias, Moallemi, Van Roy, & Weissman, 2010) employ
similar such strategies.

6.6 Top-level Algorithm
At each time step, MC-AIXI(fac-ctw) first invokes the ρUCT routine with a fixed horizon to estimate the value of each candidate action. An action is then chosen according to some policy that
balances exploration with exploitation, such as ϵ-Greedy or Softmax (Sutton & Barto, 1998). This
action is communicated to the environment, which responds with an observation-reward pair. The
agent then incorporates this information into Υ using the FAC-CTW algorithm and the cycle repeats.
Figure 4 gives an overview of the agent/environment interaction loop.
121

Veness, Ng, Hutter, Uther, & Silver

Domain
1d-maze
Cheese Maze
Tiger
Extended Tiger
4 × 4 Grid
TicTacToe
Biased Rock-Paper-Scissor
Kuhn Poker
Partially Observable Pacman

|A|
2
4
3
4
4
9
3
2
4

|O|
1
16
3
3
1
19683
3
6
216

Aliasing
yes
yes
yes
yes
yes
no
no
yes
yes

Noisy O
no
no
yes
yes
no
no
yes
yes
no

Uninformative O
yes
no
no
no
yes
no
no
no
no

Table 1: Domain characteristics

7. Experimental Results
We now measure our agent’s performance across a number of diﬀerent domains. In particular, we
focused on learning and solving some well-known benchmark problems from the POMDP literature.
Given the full POMDP model, computation of the optimal policy for each of these POMDPs is
not diﬃcult. However, our requirement of having to both learn a model of the environment, as
well as find a good policy online, significantly increases the diﬃculty of these problems. From the
agent’s perspective, our domains contain perceptual aliasing, noise, partial information, and inherent
stochastic elements.
7.1 Domains
Our test domains are now described. Their characteristics are summarized in Table 1.
1d-maze. The 1d-maze is a simple problem from the work of Cassandra, Kaelbling, and Littman
(1994). The agent begins at a random, non-goal location within a 1 × 4 maze. There is a choice of
two actions: left or right. Each action transfers the agent to the adjacent cell if it exists, otherwise
it has no eﬀect. If the agent reaches the third cell from the left, it receives a reward of 1. Otherwise
it receives a reward of 0. The distinguishing feature of this problem is that the observations are
uninformative; every observation is the same regardless of the agent’s actual location.
Cheese Maze. This well known problem is due to McCallum (1996). The agent is a mouse inside
a two dimensional maze seeking a piece of cheese. The agent has to choose one of four actions:
move up, down, left or right. If the agent bumps into a wall, it receives a penalty of −10. If the
agent finds the cheese, it receives a reward of 10. Each movement into a free cell gives a penalty
of −1. The problem is depicted graphically in Figure 5. The number in each cell represents the
decimal equivalent of the four bit binary observation (0 for a free neighbouring cell, 1 for a wall)
the mouse receives in each cell. The problem exhibits perceptual aliasing in that a single observation
is potentially ambiguous.
Tiger. This is another familiar domain from the work of Kaelbling, Littman, and Cassandra
(1995). The environment dynamics are as follows: a tiger and a pot of gold are hidden behind
one of two doors. Initially the agent starts facing both doors. The agent has a choice of one of three
actions: listen, open the left door, or open the right door. If the agent opens the door hiding the
122

A Monte-Carlo AIXI Approximation

Figure 5: The cheese maze
tiger, it suﬀers a -100 penalty. If it opens the door with the pot of gold, it receives a reward of 10.
If the agent performs the listen action, it receives a penalty of −1 and an observation that correctly
describes where the tiger is with 0.85 probability.
Extended Tiger. The problem setting is similar to Tiger, except that now the agent begins sitting
down on a chair. The actions available to the agent are: stand, listen, open the left door, and open the
right door. Before an agent can successfully open one of the two doors, it must stand up. However,
the listen action only provides information about the tiger’s whereabouts when the agent is sitting
down. Thus it is necessary for the agent to plan a more intricate series of actions before it sees the
optimal solution. The reward structure is slightly modified from the simple Tiger problem, as now
the agent gets a reward of 30 when finding the pot of gold.
4 × 4 Grid. The agent is restricted to a 4 × 4 grid world. It can move either up, down, right or
left. If the agent moves into the bottom right corner, it receives a reward of 1, and it is randomly
teleported to one of the remaining 15 cells. If it moves into any cell other than the bottom right
corner cell, it receives a reward of 0. If the agent attempts to move into a non-existent cell, it
remains in the same location. Like the 1d-maze, this problem is also uninformative but on a much
larger scale. Although this domain is simple, it does require some subtlety on the part of the agent.
The correct action depends on what the agent has tried before at previous time steps. For example,
if the agent has repeatedly moved right and not received a positive reward, then the chances of it
receiving a positive reward by moving down are increased.
TicTacToe. In this domain, the agent plays repeated games of TicTacToe against an opponent who
moves randomly. If the agent wins the game, it receives a reward of 2. If there is a draw, the agent
receives a reward of 1. A loss penalises the agent by −2. If the agent makes an illegal move, by
moving on top of an already filled square, then it receives a reward of −3. A legal move that does
not end the game earns no reward.
Biased Rock-Paper-Scissors. This domain is taken from the work of Farias et al. (2010). The
agent repeatedly plays Rock-Paper-Scissor against an opponent that has a slight, predictable bias in
its strategy. If the opponent has won a round by playing rock on the previous cycle, it will always
play rock at the next cycle; otherwise it will pick an action uniformly at random. The agent’s
observation is the most recently chosen action of the opponent. It receives a reward of 1 for a win,
0 for a draw and −1 for a loss.
123

Veness, Ng, Hutter, Uther, & Silver

Kuhn Poker. Our next domain involves playing Kuhn Poker (Kuhn, 1950; Hoehn, Southey, Holte,
& Bulitko, 2005) against an opponent playing a Nash strategy. Kuhn Poker is a simplified, zerosum, two player poker variant that uses a deck of three cards: a King, Queen and Jack. Whilst
considerably less sophisticated than popular poker variants such as Texas Hold’em, well-known
strategic concepts such as bluﬃng and slow-playing remain characteristic of strong play.
In our setup, the agent acts second in a series of rounds. Two actions, pass or bet, are available
to each player. A bet action requires the player to put an extra chip into play. At the beginning of
each round, each player puts a chip into play. The opponent then decides whether to pass or bet;
betting will win the round if the agent subsequently passes, otherwise a showdown will occur. In a
showdown, the player with the highest card wins the round. If the opponent passes, the agent can
either bet or pass; passing leads immediately to a showdown, whilst betting requires the opponent to
either bet to force a showdown, or to pass and let the agent win the round uncontested. The winner
of the round gains a reward equal to the total chips in play, the loser receives a penalty equal to the
number of chips they put into play this round. At the end of the round, all chips are removed from
play and another round begins.
Kuhn Poker has a known optimal solution. Against a first player playing a Nash strategy, the
1
second player can obtain at most an average reward of 18
per round.

Partially Observable Pacman. This domain is a partially observable version of the classic Pacman game. The agent must navigate a 17 × 17 maze and eat the pills that are distributed across
the maze. Four ghosts roam the maze. They move initially at random, until there is a Manhattan
distance of 5 between them and Pacman, whereupon they will aggressively pursue Pacman for a
short duration. The maze structure and game are the same as the original arcade game, however
the Pacman agent is hampered by partial observability. Pacman is unaware of the maze structure
and only receives a 4-bit observation describing the wall configuration at its current location. It also
does not know the exact location of the ghosts, receiving only 4-bit observations indicating whether
a ghost is visible (via direct line of sight) in each of the four cardinal directions. In addition, the
locations of the food pellets are unknown except for a 3-bit observation that indicates whether food
can be smelt within a Manhattan distance of 2, 3 or 4 from Pacman’s location, and another 4-bit
observation indicating whether there is food in its direct line of sight. A final single bit indicates
whether Pacman is under the eﬀects of a power pill. At the start of each episode, a food pellet is
placed down with probability 0.5 at every empty location on the grid. The agent receives a penalty
of 1 for each movement action, a penalty of 10 for running into a wall, a reward of 10 for each food
pellet eaten, a penalty of 50 if it is caught by a ghost, and a reward of 100 for collecting all the food.
If multiple such events occur, then the total reward is cumulative, i.e. running into a wall and being
caught would give a penalty of 60. The episode resets if the agent is caught or if it collects all the
food.
Figure 6 shows a graphical representation of the partially observable Pacman domain. This
problem is the largest domain we consider, with an unknown optimal policy. The main purpose
of this domain is to show the scaling properties of our agent on a challenging problem. Note that
this domain is fundamentally diﬀerent to the Pacman domain used in (Silver & Veness, 2010). In
addition to using a diﬀerent observation space, we also do not assume that the true environment is
known a-priori.
124

A Monte-Carlo AIXI Approximation

Figure 6: A screenshot (converted to black and white) of the PacMan domain
7.2 Experimental Setup
We now evaluate the performance of the MC-AIXI(fac-ctw) agent. To help put our results into
perspective, we implemented and directly compared against two competing algorithms from the
model-based general reinforcement learning literature: U-Tree (McCallum, 1996) and Active-LZ
(Farias et al., 2010). The two algorithms are described on page 133 in Section 8. As FAC-CTW
subsumes Action Conditional CTW, we do not evaluate it in this paper; results using Action Conditional CTW can be found in our previous work (Veness et al., 2010). The performance of the agent
using FAC-CTW is no worse and in some cases slightly better than the previous results.
Each agent communicates with the environment over a binary channel. A cycle begins with the
agent sending an action a to the environment, which then responds with a percept x. This cycle is
then repeated. A fixed number of bits are used to encode the action, observation and reward spaces
for each domain. These are specified in Table 2. No constraint is placed on how the agent interprets
the observation component; e.g., this could be done at either the bit or symbol level. The rewards
are encoded naively, i.e. the bits corresponding to the reward are interpreted as unsigned integers.
Negative rewards are handled (without loss of generality) by oﬀsetting all of the rewards so that
they are guaranteed to be non-negative. These oﬀsets are removed from the reported results.
The process of gathering results for each of the three agents is broken into two phases: model
learning and model evaluation. The model learning phase involves running each agent with an
exploratory policy to build a model of the environment. This learnt model is then evaluated at
various points in time by running the agent without exploration for 5000 cycles and reporting the
average reward per cycle. More precisely, at time t the average reward per cycle is defined as
1 ∑t+5000
5000 i=t+1 ri , where ri is the reward received at cycle i. Having two separate phases reduces
the influence of the agent’s earlier exploratory actions on the reported performance. All of our
experiments were performed on a dual quad-core Intel 2.53Ghz Xeon with 24 gigabytes of memory.
Table 3 outlines the parameters used by MC-AIXI(fac-ctw) during the model learning phase.
The context depth parameter D specifies the maximal number of recent bits used by FAC-CTW.
The ρUCT search horizon is specified by the parameter m. Larger D and m increase the capabilities of our agent, at the expense of linearly increasing computation time; our values represent
125

Veness, Ng, Hutter, Uther, & Silver

Domain
1d-maze
Cheese Maze
Tiger
Extended Tiger
4 × 4 Grid
TicTacToe
Biased Rock-Paper-Scissor
Kuhn Poker
Partially Observable Pacman

A bits
1
2
2
2
2
4
2
1
2

O bits
1
4
2
3
1
18
2
4
16

R bits
1
5
7
8
1
3
2
3
8

Table 2: Binary encoding of the domains

Domain
1d-maze
Cheese Maze
Tiger
Extended Tiger
4 × 4 Grid
TicTacToe
Biased Rock-Paper-Scissor
Kuhn Poker
Partial Observable Pacman

D
32
96
96
96
96
64
32
42
96

m
10
8
5
4
12
9
4
2
4

ϵ
0.9
0.999
0.99
0.99
0.9
0.9999
0.999
0.99
0.9999

γ
0.99
0.9999
0.9999
0.99999
0.9999
0.999999
0.99999
0.9999
0.99999

ρUCT Simulations
500
500
500
500
500
500
500
500
500

Table 3: MC-AIXI(fac-ctw) model learning configuration

an appropriate compromise between these two competing dimensions for each problem domain.
Exploration during the model learning phase is controlled by the ϵ and γ parameters. At time t,
MC-AIXI(fac-ctw) explores a random action with probability γt ϵ. During the model evaluation
phase, exploration is disabled, with results being recorded for varying amounts of experience and
search eﬀort.
The Active-LZ algorithm is fully specified in the work of Farias et al. (2010). It contains only
two parameters, a discount rate and a policy that balances between exploration and exploitation.
During the model learning phase, a discount rate of 0.99 and ϵ-Greedy exploration (with ϵ = 0.95)
were used. Smaller exploration values (such as 0.05, 0.2, 0.5) were tried, as well as policies that
decayed ϵ over time, but these surprisingly gave slightly worse performance during testing. As
a sanity check, we confirmed that our implementation could reproduce the experimental results
reported in the work of Farias et al. (2010). During the model evaluation phase, exploration is
disabled.
The situation is somewhat more complicated for U-Tree, as it is more of a general agent framework than a completely specified algorithm. Due to the absence of a publicly available reference
implementation, a number of implementation-specific decisions were made. These included the
choice of splitting criteria, how far back in time these criteria could be applied, the frequency of
126

A Monte-Carlo AIXI Approximation

Domain
1d-maze
Cheese Maze
Tiger
Extended Tiger
4 × 4 Grid
TicTacToe
Biased Rock-Paper-Scissor
Kuhn Poker

ϵ
0.05
0.2
0.1
0.05
0.05
0.05
0.05
0.05

Test Fringe
100
100
100
200
100
1000
100
200

α
0.05
0.05
0.05
0.01
0.05
0.01
0.05
0.05

Table 4: U-Tree model learning configuration
fringe tests, the choice of p-value for the Kolmogorov-Smirnov test, the exploration/exploitation
policy and the learning rate. The main design decisions are listed below:
• A split could be made on any action, or on the status of any single bit of an observation.
• The maximum number of steps backwards in time for which a utile distinction could be made
was set to 5.
• The frequency of fringe tests was maximised given realistic resource constraints. Our choices
allowed for 5 × 104 cycles of interaction to be completed on each domain within 2 days of
training time.
• Splits were tried in order from the most temporally recent to the most temporally distant.
• ϵ-Greedy exploration strategy was used, with ϵ tuned separately for each domain.
• The learning rate α was tuned for each domain.
To help make the comparison as fair as possible, an eﬀort was made to tune U-Tree’s parameters for
each domain. The final choices for the model learning phase are summarised in Table 4. During the
model evaluation phase, both exploration and testing of the fringe are disabled.
Source Code. The code for our U-Tree, Active-LZ and MC-AIXI(fac-ctw) implementations can
be found at: http://jveness.info/software/mcaixi_jair_2010.zip.
7.3 Results
Figure 7 presents our main set of results. Each graph shows the performance of each agent as it
accumulates more experience. The performance of MC-AIXI(fac-ctw) matches or exceeds U-Tree
and Active-LZ on all of our test domains. Active-LZ steadily improved with more experience, however it learnt significantly more slowly than both U-Tree and MC-AIXI(fac-ctw). U-Tree performed
well in most domains, however the overhead of testing for splits limited its ability to be run for long
periods of time. This is the reason why some data points for U-Tree are missing from the graphs
in Figure 7. This highlights the advantage of algorithms that take constant time per cycle, such
as MC-AIXI(fac-ctw) and Active-LZ. Constant time isn’t enough however, especially when large
observation spaces are involved. Active-LZ works at the symbol level, with the algorithm given by
Farias et al. (2010) requiring an exhaustive enumeration of the percept space on each cycle. This is
not possible in reasonable time for the larger TicTacToe domain, which is why no Active-LZ result
127

Veness, Ng, Hutter, Uther, & Silver

Domain
1d Maze
Cheese Maze
Tiger
Extended Tiger
4 × 4 Grid
TicTacToe
Biased RPS
Kuhn Poker

Experience
5 × 103
2.5 × 103
2.5 × 104
5 × 104
2.5 × 104
5 × 105
1 × 104
5 × 106

ρUCT Simulations
250
500
25000
25000
500
2500
5000
250

Search Time per Cycle
0.1s
0.5s
10.6s
12.6s
0.3s
4.1s
2.5s
0.1s

Table 5: Resources required for (near) optimal performance by MC-AIXI(fac-ctw)
is presented. This illustrates an important advantage of MC-AIXI(fac-ctw) and U-Tree, which have
the ability to exploit structure within a single observation.
Figure 8 shows the performance of MC-AIXI(fac-ctw) as the number of ρUCT simulations
varies. The results for each domain were based on a model learnt from 5 × 104 cycles of experience,
except in the case of TicTacToe where 5 × 105 cycles were used. So that results could be compared
across domains, the average reward per cycle was normalised to the interval [0, 1]. As expected,
domains that included a significant planning component (such as Tiger or Extended Tiger) required
more search eﬀort. Good performance on most domains was obtained using only 1000 simulations.
Given a suﬃcient number of ρUCT simulations and cycles of interaction, the performance of
the MC-AIXI(fac-ctw) agent approaches optimality on our test domains. The amount of resources
needed for near optimal performance on each domain during the model evaluation phase is listed
in Table 5. Search times are also reported. This shows that the MC-AIXI(fac-ctw) agent can be
realistically used on a present day workstation.
7.4 Discussion
The small state space induced by U-Tree has the benefit of limiting the number of parameters that
need to be estimated from data. This can dramatically speed up the model-learning process. In
contrast, both Active-LZ and our approach require a number of parameters proportional to the number of distinct contexts. This is one of the reasons why Active-LZ exhibits slow convergence in
practice. This problem is much less pronounced in our approach for two reasons. First, the Ockham prior in CTW ensures that future predictions are dominated by PST structures that have seen
enough data to be trustworthy. Secondly, value function estimation is decoupled from the process
of context estimation. Thus it is reasonable to expect ρUCT to make good local decisions provided
FAC-CTW can predict well. The downside however is that our approach requires search for action
selection. Although ρUCT is an anytime algorithm, in practice more computation (at least on small
domains) is required per cycle compared to approaches like Active-LZ and U-Tree that act greedily
with respect to an estimated global value function.
The U-Tree algorithm is well motivated, but unlike Active-LZ and our approach, it lacks theoretical performance guarantees. It is possible for U-Tree to prematurely converge to a locally optimal
state representation from which the heuristic splitting criterion can never recover. Furthermore,
the splitting heuristic contains a number of configuration options that can dramatically influence
its performance (McCallum, 1996). This parameter sensitivity somewhat limits the algorithm’s
128

A Monte-Carlo AIXI Approximation

Learning Scalability - 1d Maze
MC-AIXI

U-Tree

Active-LZ

Learning Scalability - Cheese Maze
Optimal

MC-AIXI

U-Tree

0.5

0.4
0.3

0.2
0.1

100

0

-2
-4

-6
-8

1000

10000

100

100000

1000

U-Tree

Active-LZ

Optimal

MC-AIXI

U-Tree

Active-LZ

Optimal

10

5

0

Average Reward per Cycle

Average Reward per Cycle

100000

Learning Scalability - Extended Tiger

Learning Scalability - Tiger
MC-AIXI

10000

Experience (cycles)

Experience (cycles)

-5

-10
-15

-20
-25

-30

0

-10
-20

-30
-40
-50

100

1000

10000

100

100000

1000

Experience (cycles)

U-Tree

10000

100000

Experience (cycles)

Learning Scalability - TicTacToe

Learning Scalability - 4x4 Grid
MC-AIXI

Active-LZ

MC-AIXI

Optimal

U-Tree

Optimal

1
Average Reward per Cycle

0.3
Average Reward per Cycle

Optimal

-10

0

0.25
0.2
0.15

0.1
0.05

0.5

0
-0.5

-1
-1.5
-2

0
100

1000

10000

100

100000

1000

10000

Learning Scalability - Kuhn Poker
MC-AIXI

U-Tree

100000

1000000

Experience (cycles)

Experience (cycles)

Active-LZ

Learning Scalability - Rock-Paper-Scissors
Optimal

MC-AIXI

0.1

0.3

0.05

0.25

Average Reward per Cycle

Average Reward per Cycle

Active-LZ

2
Average Reward per Cycle

Average Reward per Cycle

0.6

0
-0.05
-0.1
-0.15
-0.2

U-Tree

Active-LZ

Optimal

0.2
0.15

0.1
0.05

0
-0.05

100

1000

10000

100000

1000000

100

Experience (cycles)

1000

10000
Experience (cycles)

Figure 7: Average Reward per Cycle vs Experience
129

100000

1000000

Veness, Ng, Hutter, Uther, & Silver

Normalised Average Reward per Cycle

Search Scalability
1
0.9
0.8
0.7

Optimal
Tiger
4x4 Grid
1d Maze
Extended Tiger
TicTacToe
Cheese Maze
Biased RPS
Kuhn Poker

0.6
0.5
0.4
0.3
0.2
0.1
0

25

250

2500

25000

Simulations

Figure 8: Performance versus ρUCT search eﬀort

applicability to the general reinforcement learning problem. Still, our results suggest that further
investigation of frameworks motivated along the same lines as U-Tree is warranted.
7.5 Comparison to 1-ply Rollout Planning
We now investigate the performance of ρUCT in comparison to an adaptation of the well-known
1-ply rollout-based planning technique of Bertsekas and Castanon (1999). In our setting, this works
as follows: given a history h, an estimate V̂(ha) is constructed for each action a ∈ A, by averaging
the returns of many length m simulations initiated from ha. The first action of each simulation
is sampled uniformly at random from A, whilst the remaining actions are selected according to
some heuristic rollout policy. Once a suﬃcient number of simulations have been completed, the
action with the highest estimated value is selected. Unlike ρUCT, this procedure doesn’t build a
tree, nor is it guaranteed to converge to the depth m expectimax solution. In practice however,
especially in noisy and highly stochastic domains, rollout-based planning can significantly improve
the performance of an existing heuristic rollout policy (Bertsekas & Castanon, 1999).
Table 6 shows how the performance (given by average reward per cycle) diﬀers when ρUCT is
replaced by the 1-ply rollout planner. The amount of experience collected by the agent, as well as
the total number of rollout simulations, is the same as in Table 5. Both ρUCT and the 1-ply planner
use the same search horizon, heuristic rollout policy (each action is chosen uniformly at random)
and total number of simulations for each decision. This is reasonable, since although ρUCT has a
slightly higher overhead compared to the 1-ply rollout planner, this diﬀerence is negligible when
taking into account the cost of simulating future trajectories using FAC-CTW. Also, similar to
previous experiments, 5000 cycles of greedy action selection were used to evaluate the performance
of the FAC-CTW + 1-ply rollout planning combination.
130

A Monte-Carlo AIXI Approximation

Domain
1d Maze
Cheese Maze
Tiger
Extended Tiger
4x4 Grid
TicTacToe
Biased RPS
Kuhn Poker

MC-AIXI(fac-ctw)
0.50
1.28
1.12
3.97
0.24
0.60
0.25
0.06

FAC-CTW + 1-ply MC
0.50
1.25
1.11
-0.97
0.24
0.59
0.20
0.06

Table 6: Average reward per cycle: ρUCT versus 1-ply rollout planning
Importantly, ρUCT never gives worse performance than the 1-ply rollout planner, and on some
domains (shown in bold) performs better. The ρUCT algorithm provides a way of performing multistep planning whilst retaining the considerable computational advantages of rollout based methods.
In particular, ρUCT will be able to construct deep plans in regions of the search space where most
of the probability mass is concentrated on a small set of the possible percepts. When such structure
exists, ρUCT will automatically exploit it. In the worst case where the environment is highly noisy
or stochastic, the performance will be similar to that of rollout based planning. Interestingly, on
many domains the empirical performance of 1-ply rollout planning matched that of ρUCT. We
believe this to be a byproduct of our modest set of test domains, where multi-step planning is less
important than learning an accurate model of the environment.
7.6 Performance on a Challenging Domain
The performance of MC-AIXI(fac-ctw) was also evaluated on the challenging Partially Observable
Pacman domain. This is an enormous problem. Even if the true environment were known, planning
would still be diﬃcult due to the 1060 distinct underlying states.
We first evaluated the performance of MC-AIXI(fac-ctw) online. A discounted ϵ-Greedy policy, which chose a random action at time t with probability ϵγt was used. These parameters were
instantiated with ϵ := 0.9999 and γ := 0.99999. When not exploring, each action was determined
by ρUCT using 500 simulations. Figure 10 shows both the average reward per cycle and the average
reward across the most recent 5000 cycles.
The performance of this learnt model was then evaluated by performing 5000 steps of greedy
action selection, at various time points, whilst varying the number of simulations used by ρUCT.
Figure 9 shows obtained results. The agent’s performance scales with both the number of cycles of
interaction and the amount of search eﬀort. The results in Figure 9 using 500 simulations are higher
than in Figure 10 since the performance is no longer aﬀected by the exploration policy or earlier
behavior based on an inferior learnt model.
Visual inspection1 of Pacman shows that the agent, whilst not playing perfectly, has already
learnt a number of important concepts. It knows not to run into walls. It knows how to seek out
food from the limited information provided by its sensors. It knows how to run away and avoid
chasing ghosts. The main subtlety that it hasn’t learnt yet is to aggressively chase down ghosts
when it has eaten a red power pill. Also, its behaviour can sometimes become temporarily erratic
1. See http://jveness.info/publications/pacman_jair_2010.wmv for a graphical demonstration

131

Veness, Ng, Hutter, Uther, & Silver

Scaling Properties - Partially Observable Pacman
500 simulations

1000 simulations

2000 simulations

5000 simulations

Average Reward per Cycle

2
1
0
-1

-2
-3
-4

2500

25000

250000

Experience (cycles)

Figure 9: Scaling properties on a challenging domain
when stuck in a long corridor with no nearby food or visible ghosts. Still, the ability to perform
reasonably on a large domain and exhibit consistent improvements makes us optimistic about the
ability of the MC-AIXI(fac-ctw) agent to scale with extra computational resources.

8. Discussion
We now discuss related work and some limitations of our current approach.
8.1 Related Work
There have been several attempts at studying the computational properties of AIXI. In the work of
Hutter (2002a), an asymptotically optimal algorithm is proposed that, in parallel, picks and runs the
fastest program from an enumeration of provably correct programs for any given well-defined problem. A similar construction that runs all programs of length less than l and time less than t per cycle
and picks the best output (in the sense of maximising a provable lower bound for the true value)
results in the optimal time bounded AIXItl agent (Hutter, 2005, Chp.7). Like Levin search (Levin,
1973), such algorithms are not practical in general but can in some cases be applied successfully
(e.g., see Schmidhuber, 1997; Schmidhuber, Zhao, & Wiering, 1997; Schmidhuber, 2003, 2004).
In tiny domains, universal learning is computationally feasible with brute-force search. In the work
of Poland and Hutter (2006), the behaviour of AIXI is compared with a universal predicting-withexpert-advice algorithm (Poland & Hutter, 2005) in repeated 2 × 2 matrix games and is shown to
exhibit diﬀerent behaviour. A Monte-Carlo algorithm is proposed by Pankov (2008) that samples
programs according to their algorithmic probability as a way of approximating Solomonoﬀ’s universal prior. A closely related algorithm is that of speed prior sampling (Schmidhuber, 2002).
We now move on to a discussion of the model-based general reinforcement learning literature.
An early and influential work is the Utile Suﬃx Memory (USM) algorithm described by McCallum
132

A Monte-Carlo AIXI Approximation

Online Performance - Partially Observable Pacman
Running Average

5k Rolling Average

2
0
-2
-4
-6
-8
-10
-12
-14
0

50000

100000

150000

200000

250000

Experience (Cycles)

Figure 10: Online performance on a challenging domain

(1996). USM uses a suﬃx tree to partition the agent’s history space into distinct states, one for each
leaf in the suﬃx tree. Associated with each state/leaf is a Q-value, which is updated incrementally
from experience like in Q-learning (Watkins & Dayan, 1992). The history-partitioning suﬃx tree
is grown in an incremental fashion, starting from a single leaf node in the beginning. A leaf in the
suﬃx tree is split when the history sequences that fall into the leaf are shown to exhibit statistically
diﬀerent Q-values. The USM algorithm works well for a number of tasks but could not deal effectively with noisy environments. Several extensions of USM to deal with noisy environments are
investigated in the work of Shani and Brafman (2004) and Shani (2007).
U-Tree (McCallum, 1996) is an online agent algorithm that attempts to discover a compact
state representation from a raw stream of experience. The main diﬀerence between U-Tree and
USM is that U-Tree can discriminate between individual components within an observation. This
allows U-Tree to more eﬀectively handle larger observation spaces and ignore potentially irrelevant
components of the observation vector. Each state is represented as the leaf of a suﬃx tree that
maps history sequences to states. As more experience is gathered, the state representation is refined
according to a heuristic built around the Kolmogorov-Smirnov test. This heuristic tries to limit the
growth of the suﬃx tree to places that would allow for better prediction of future reward. Value
Iteration is used at each time step to update the value function for the learnt state representation,
which is then used by the agent for action selection.
Active-LZ (Farias et al., 2010) combines a Lempel-Ziv based prediction scheme with dynamic
programming for control to produce an agent that is provably asymptotically optimal if the environment is n-Markov. The algorithm builds a context tree (distinct from the context tree built by CTW),
with each node containing accumulated transition statistics and a value function estimate. These estimates are refined over time, allowing for the Active-LZ agent to steadily increase its performance.
In Section 7, we showed that our agent compared favourably to Active-LZ.
The BLHT algorithm (Suematsu, Hayashi, & Li, 1997; Suematsu & Hayashi, 1999) uses symbol
level PSTs for learning and an (unspecified) dynamic programming based algorithm for control.
BLHT uses the most probable model for prediction, whereas we use a mixture model, which admits
133

Veness, Ng, Hutter, Uther, & Silver

a much stronger convergence result. A further distinction is our usage of an Ockham prior instead
of a uniform prior over PST models.
Predictive state representations (PSRs) (Littman, Sutton, & Singh, 2002; Singh, James, &
Rudary, 2004; Rosencrantz, Gordon, & Thrun, 2004) maintain predictions of future experience.
Formally, a PSR is a probability distribution over the agent’s future experience, given its past experience. A subset of these predictions, the core tests, provide a suﬃcient statistic for all future
experience. PSRs provide a Markov state representation, can represent and track the agent’s state in
partially observable environments, and provide a complete model of the world’s dynamics. Unfortunately, exact representations of state are impractical in large domains, and some form of approximation is typically required. Topics such as improved learning or discovery algorithms for PSRs are
currently active areas of research. The recent results of Boots, Siddiqi, and Gordon (2010) appear
particularly promising.
Temporal-diﬀerence networks (Sutton & Tanner, 2004) are a form of predictive state representation in which the agent’s state is approximated by abstract predictions. These can be predictions
about future observations, but also predictions about future predictions. This set of interconnected
predictions is known as the question network. Temporal-diﬀerence networks learn an approximate
model of the world’s dynamics: given the current predictions, the agent’s action, and an observation
vector, they provide new predictions for the next time-step. The parameters of the model, known
as the answer network, are updated after each time-step by temporal-diﬀerence learning. Some
promising recent results applying TD-Networks for prediction (but not control) to small POMDPs
are given in (Makino, 2009).
In model-based Bayesian Reinforcement Learning (Strens, 2000; Poupart, Vlassis, Hoey, &
Regan, 2006; Ross, Chaib-draa, & Pineau, 2008; Poupart & Vlassis, 2008), a distribution over
(PO)MDP parameters is maintained. In contrast, we maintain an exact Bayesian mixture of PSTs,
which are variable-order Markov models. The ρUCT algorithm shares similarities with Bayesian
Sparse Sampling (Wang, Lizotte, Bowling, & Schuurmans, 2005). The main diﬀerences are estimating the leaf node values with a rollout function and using the UCB policy to direct the search.
8.2 Limitations
Our current AIXI approximation has two main limitations.
The first limitation is the restricted model class used for learning and prediction. Our agent will
perform poorly if the underlying environment cannot be predicted well by a PST of bounded depth.
Prohibitive amounts of experience will be required if a large PST model is needed for accurate
prediction. For example, it would be unrealistic to think that our current AIXI approximation could
cope with real-world image or audio data.
The second limitation is that unless the planning horizon is unrealistically small, our full
Bayesian solution (using ρUCT and a mixture environment model) to the exploration/exploitation
dilemma is computationally intractable. This is why our agent needs to be augmented by a heuristic
exploration/exploitation policy in practice. Although this did not prevent our agent from obtaining
optimal performance on our test domains, a better solution may be required for more challenging
problems. In the MDP setting, considerable progress has been made towards resolving the exploration/exploitation issue. In particular, powerful PAC-MDP approaches exist for both model-based
and model-free reinforcement learning agents (Brafman & Tennenholtz, 2003; Strehl, Li, Wiewiora,
134

A Monte-Carlo AIXI Approximation

Impact of Learnt Rollouts - Cheese Maze
1

Average Reward per Cycle

0
-1
-2
-3
500 simulations - Learnt
-4

500 simulations - Uniform
100 simulations - Learnt

-5

100 simulations - Uniform
-6
100

1000

10000

100000

Experience (cycles)

Figure 11: Online performance when using a learnt rollout policy on the Cheese Maze

Langford, & Littman, 2006; Strehl, Li, & Littman, 2009). It remains to be seen whether similar such
principled approaches exist for history-based Bayesian agents.

9. Future Scalability
We now list some ideas that make us optimistic about the future scalability of our approach.
9.1 Online Learning of Rollout Policies for ρUCT
An important parameter to ρUCT is the choice of rollout policy. In MCTS methods for Computer
Go, it is well known that search performance can be improved by using knowledge-based rollout
policies (Gelly, Wang, Munos, & Teytaud, 2006). In the general agent setting, it would thus be
desirable to gain some of the benefits of expert design through online learning.
We have conducted some preliminary experiments in this area. A CTW-based method was
used to predict the high-level actions chosen online by ρUCT. This learnt distribution replaced our
previous uniformly random rollout policy. Figure 11 shows the results of using this learnt rollout
policy on the cheese maze. The other domains we tested exhibited similar behaviour. Although more
work remains, it is clear that even our current simple learning scheme can significantly improve the
performance of ρUCT.
Although our first attempts have been promising, a more thorough investigation is required. It
is likely that rollout policy learning methods for adversarial games, such as those investigated by
Silver and Tesauro (2009), can be adapted to our setting. It would also be interesting to try to apply
some form of search bootstrapping (Veness, Silver, Uther, & Blair, 2009) online. In addition, one
could also look at ways to modify the UCB policy used in ρUCT to automatically take advantage of
learnt rollout knowledge, similar to the heuristic techniques used in computer Go (Gelly & Silver,
2007).
135

Veness, Ng, Hutter, Uther, & Silver

9.2 Combining Mixture Environment Models
A key property of mixture environment models is that they can be composed. Given two mixture
environment models ξ1 and ξ2 , over model classes M1 and M2 respectively, it is easy to show that
the convex combination
ξ(x1:n | a1:n ) := αξ1 (x1:n | a1:n ) + (1 − α)ξ2 (x1:n | a1:n )
is a mixture environment model over the union of M1 and M2 . Thus there is a principled way for
expanding the general predictive power of agents that use our kind of direct AIXI approximation.
9.3 Richer Notions of Context for FAC-CTW
Instead of using the most recent D bits of the current history h, the FAC-CTW algorithm can be
generalised to use a set of D boolean functions on h to define the current context. We now formalise
this notion, and give some examples of how this might help in agent applications.
Definition 12. Let P = {p0 , p1 , . . . , pm } be a set of predicates (boolean functions) on histories
h ∈ (A×X)n , n ≥ 0. A P-model is a binary tree where each internal node is labeled with a predicate
in P and the left and right outgoing edges at the node are labeled True and False respectively. A
P-tree is a pair (MP , Θ) where MP is a P-model and associated with each leaf node l in MP is a
probability distribution over {0, 1} parametrised by θl ∈ Θ.
A P-tree (MP , Θ) represents a function g from histories to probability distributions on {0, 1} in
the usual way. For each history h, g(h) = θlh , where lh is the leaf node reached by pushing h down
the model MP according to whether it satisfies the predicates at the internal nodes and θlh ∈ Θ is
the distribution at lh . The notion of a P-context tree can now be specified, leading to a natural
generalisation of Definition 8.
Both the Action-Conditional CTW and FAC-CTW algorithms can be generalised to work with
P-context trees in a natural way. Importantly, a result analogous to Lemma 2 can be established,
which means that the desirable computational properties of CTW are retained. This provides a
powerful way of extending the notion of context for agent applications. For example, with a suitable choice of predicate class P, both prediction suﬃx trees (Definition 7) and looping suﬃx trees
(Holmes & Jr, 2006) can be represented as P-trees. It also opens up the possibility of using rich
logical tree models (Blockeel & De Raedt, 1998; Kramer & Widmer, 2001; Lloyd, 2003; Ng, 2005;
Lloyd & Ng, 2007) in place of prediction suﬃx trees.
9.4 Incorporating CTW Extensions
There are several noteworthy ways the original CTW algorithm can be extended. The finite depth
limit on the context tree can be removed (Willems, 1998), without increasing the asymptotic space
overhead of the algorithm. Although this increases the worst-case time complexity of generating a
symbol from O(D) to linear in the length of the history, the average-case performance may still be
suﬃcient for good performance in the agent setting. Furthermore, three additional model classes,
each significantly larger than the one used by CTW, are presented in the work of Willems, Shtarkov,
and Tjalkens (1996). These could be made action conditional along the same lines as our FAC-CTW
derivation. Unfortunately, online prediction with these more general classes is now exponential in
the context depth D. Investigating whether these ideas can be applied in a more restricted sense
would be an interesting direction for future research.
136

A Monte-Carlo AIXI Approximation

9.5 Parallelization of ρUCT
The performance of our agent is dependent on the amount of thinking time allowed at each time
step. An important property of ρUCT is that it is naturally parallel. We have completed a prototype
parallel implementation of ρUCT with promising scaling results using between 4 and 8 processing
cores. We are confident that further improvements to our implementation will allow us to solve
problems where our agent’s planning ability is the main limitation.
9.6 Predicting at Multiple Levels of Abstraction
The FAC-CTW algorithm reduces the task of predicting a single percept to the prediction of its
binary representation. Whilst this is reasonable for a first attempt at AIXI approximation, it’s worth
emphasising that subsequent attempts need not work exclusively at such a low level.
For example, recall that the FAC-CTW algorithm was obtained by chaining together lX actionconditional binary predictors. It would be straightforward to apply a similar technique to chain
together multiple k-bit action-conditional predictors, for k > 1. These k bits could be interpreted in
many ways: e.g. integers, floating point numbers, ASCII characters or even pixels. This observation, along with the convenient property that mixture environment models can be composed, opens
up the possibility of constructing more sophisticated, hierarchical mixture environment models.

10. Conclusion
This paper presents the first computationally feasible general reinforcement learning agent that directly and scalably approximates the AIXI ideal. Although well established theoretically, it has
previously been unclear whether the AIXI theory could inspire the design of practical agent algorithms. Our work answers this question in the aﬃrmative: empirically, our approximation achieves
strong performance and theoretically, we can characterise the range of environments in which our
agent is expected to perform well.
To develop our approximation, we introduced two new algorithms: ρUCT, a Monte-Carlo expectimax approximation technique that can be used with any online Bayesian approach to the general reinforcement learning problem and FAC-CTW, a generalisation of the powerful CTW algorithm to the agent setting. In addition, we highlighted a number of interesting research directions
that could improve the performance of our current agent; in particular, model class expansion and
the online learning of heuristic rollout policies for ρUCT.
We hope that this work generates further interest from the broader artificial intelligence community in both the AIXI theory and general reinforcement learning agents.

Acknowledgments
The authors thank Alan Blair, Thomas Degris-Dard, Evan Greensmith, Bernhard Hengst, Ramana
Kumar, John Lloyd, Hassan Mahmud, Malcolm Ryan, Scott Sanner, Rich Sutton, Eric Wiewiora,
Frans Willems and the anonymous reviewers for helpful comments and feedback. This work received support from the Australian Research Council under grant DP0988049. NICTA is funded
by the Australian Government’s Department of Communications, Information Technology, and the
Arts and the Australian Research Council through Backing Australia’s Ability and the ICT Research
Centre of Excellence programs.
137

Veness, Ng, Hutter, Uther, & Silver

References
Auer, P. (2002). Using confidence bounds for exploitation-exploration trade-oﬀs. Journal of Machine Learning Research, 3, 397–422.
Begleiter, R., El-Yaniv, R., & Yona, G. (2004). On prediction using variable order Markov models.
Journal of Artificial Intelligence Research, 22, 385–421.
Bertsekas, D. P., & Castanon, D. A. (1999). Rollout algorithms for stochastic scheduling problems.
Journal of Heuristics, 5(1), 89–108.
Blockeel, H., & De Raedt, L. (1998). Top-down induction of first-order logical decision trees.
Artificial Intelligence, 101(1-2), 285–297.
Boots, B., Siddiqi, S. M., & Gordon, G. J. (2010). Closing the learning-planning loop with predictive state representations. In Proceedings of the 9th International Conference on Autonomous
Agents and Multiagent Systems: volume 1 - Volume 1, AAMAS ’10, pp. 1369–1370 Richland,
SC. International Foundation for Autonomous Agents and Multiagent Systems.
Brafman, R. I., & Tennenholtz, M. (2003). R-max - a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research, 3, 213–231.
Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally in partially observable
stochastic domains. In AAAI, pp. 1023–1028.
Chaslot, G.-B., Winands, M., Uiterwijk, J., van den Herik, H., & Bouzy, B. (2008a). Progressive
strategies for Monte-Carlo Tree Search. New Mathematics and Natural Computation, 4(3),
343–357.
Chaslot, G. M., Winands, M. H., & Herik, H. J. (2008b). Parallel monte-carlo tree search. In
Proceedings of the 6th International Conference on Computers and Games, pp. 60–71 Berlin,
Heidelberg. Springer-Verlag.
Cover, T. M., & Thomas, J. A. (1991). Elements of information theory. Wiley-Interscience, New
York, NY, USA.
Farias, V., Moallemi, C., Van Roy, B., & Weissman, T. (2010). Universal reinforcement learning.
Information Theory, IEEE Transactions on, 56(5), 2441 –2454.
Finnsson, H., & Björnsson, Y. (2008). Simulation-based approach to general game playing. In
AAAI, pp. 259–264.
Gelly, S., & Silver, D. (2007). Combining online and oﬄine learning in UCT. In Proceedings of
the 17th International Conference on Machine Learning, pp. 273–280.
Gelly, S., & Wang, Y. (2006). Exploration exploitation in Go: UCT for Monte-Carlo Go. In NIPS
Workshop on On-line trading of Exploration and Exploitation.
Gelly, S., Wang, Y., Munos, R., & Teytaud, O. (2006). Modification of UCT with patterns in
Monte-Carlo Go. Tech. rep. 6062, INRIA, France.
138

A Monte-Carlo AIXI Approximation

Hoehn, B., Southey, F., Holte, R. C., & Bulitko, V. (2005). Eﬀective short-term opponent exploitation in simplified poker. In AAAI, pp. 783–788.
Holmes, M. P., & Jr, C. L. I. (2006). Looping suﬃx tree-based inference of partially observable
hidden state. In ICML, pp. 409–416.
Hutter, M. (2002a). The fastest and shortest algorithm for all well-defined problems. International
Journal of Foundations of Computer Science., 13(3), 431–443.
Hutter, M. (2002b). Self-optimizing and Pareto-optimal policies in general environments based on
Bayes-mixtures. In Proceedings of the 15th Annual Conference on Computational Learning
Theory (COLT 2002), Lecture Notes in Artificial Intelligence. Springer.
Hutter, M. (2005). Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic
Probability. Springer.
Hutter, M. (2007). Universal algorithmic intelligence: A mathematical top→down approach. In
Artificial General Intelligence, pp. 227–290. Springer, Berlin.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1995). Planning and acting in partially
observable stochastic domains. Artificial Intelligence, 101, 99–134.
Kocsis, L., & Szepesvári, C. (2006). Bandit based Monte-Carlo planning. In ECML, pp. 282–293.
Kramer, S., & Widmer, G. (2001). Inducing classification and regression trees in first order logic.
In Džeroski, S., & Lavrač, N. (Eds.), Relational Data Mining, chap. 6. Springer.
Krichevsky, R., & Trofimov, V. (1981). The performance of universal coding. IEEE Transactions
on Information Theory, IT-27, 199–207.
Kuhn, H. W. (1950). A simplified two-person poker. In Contributions to the Theory of Games, pp.
97–103.
Legg, S., & Hutter, M. (2004). Ergodic MDPs admit self-optimising policies. Tech. rep. IDSIA-2104, Dalle Molle Institute for Artificial Intelligence (IDSIA).
Legg, S. (2008). Machine Super Intelligence. Ph.D. thesis, Department of Informatics, University
of Lugano.
Levin, L. A. (1973). Universal sequential search problems. Problems of Information Transmission,
9, 265–266.
Li, M., & Vitányi, P. (2008). An Introduction to Kolmogorov Complexity and Its Applications (Third
edition). Springer.
Littman, M., Sutton, R., & Singh, S. (2002). Predictive representations of state. In NIPS, pp.
1555–1561.
Lloyd, J. W. (2003). Logic for Learning: Learning Comprehensible Theories from Structured Data.
Springer.
139

Veness, Ng, Hutter, Uther, & Silver

Lloyd, J. W., & Ng, K. S. (2007). Learning modal theories. In Proceedings of the 16th International
Conference on Inductive Logic Programming, LNAI 4455, pp. 320–334.
Makino, T. (2009). Proto-predictive representation of states with simple recurrent temporaldiﬀerence networks. In ICML, pp. 697–704.
McCallum, A. K. (1996). Reinforcement Learning with Selective Perception and Hidden State.
Ph.D. thesis, University of Rochester.
Ng, K. S. (2005). Learning Comprehensible Theories from Structured Data. Ph.D. thesis, The
Australian National University.
Pankov, S. (2008). A computational approximation to the AIXI model. In AGI, pp. 256–267.
Poland, J., & Hutter, M. (2005). Defensive universal learning with experts. In Proc. 16th International Conf. on Algorithmic Learning Theory, Vol. LNAI 3734, pp. 356–370. Springer.
Poland, J., & Hutter, M. (2006). Universal learning of repeated matrix games. Tech. rep. 18-05,
IDSIA.
Poupart, P., & Vlassis, N. (2008). Model-based bayesian reinforcement learning in partially observable domains. In ISAIM.
Poupart, P., Vlassis, N., Hoey, J., & Regan, K. (2006). An analytic solution to discrete bayesian
reinforcement learning. In ICML ’06: Proceedings of the 23rd international conference on
Machine learning, pp. 697–704 New York, NY, USA. ACM.
Rissanen, J. (1983). A universal data compression system. IEEE Transactions on Information
Theory, 29(5), 656–663.
Ron, D., Singer, Y., & Tishby, N. (1996). The power of amnesia: Learning probabilistic automata
with variable memory length. Machine Learning, 25(2), 117–150.
Rosencrantz, M., Gordon, G., & Thrun, S. (2004). Learning low dimensional predictive representations. In Proceedings of the twenty-first International Conference on Machine Learning,
p. 88 New York, NY, USA. ACM.
Ross, S., Chaib-draa, B., & Pineau, J. (2008). Bayes-adaptive POMDPs. In Platt, J., Koller, D.,
Singer, Y., & Roweis, S. (Eds.), Advances in Neural Information Processing Systems 20, pp.
1225–1232. MIT Press, Cambridge, MA.
Schmidhuber, J., Zhao, J., & Wiering, M. A. (1997). Shifting inductive bias with success-story
algorithm, adaptive Levin search, and incremental self-improvement. Machine Learning, 28,
105–130.
Schmidhuber, J. (1997). Discovering neural nets with low Kolmogorov complexity and high generalization capability. Neural Networks, 10(5), 857–873.
Schmidhuber, J. (2002). The speed prior: A new simplicity measure yielding near-optimal computable predictions. In Proc. 15th Annual Conf. on Computational Learning Theory, pp.
216–228.
140

A Monte-Carlo AIXI Approximation

Schmidhuber, J. (2003). Bias-optimal incremental problem solving. In Advances in Neural Information Processing Systems 15, pp. 1571–1578. MIT Press.
Schmidhuber, J. (2004). Optimal ordered problem solver. Machine Learning, 54, 211–254.
Shani, G. (2007). Learning and Solving Partially Observable Markov Decision Processes. Ph.D.
thesis, Ben-Gurion University of the Negev.
Shani, G., & Brafman, R. (2004). Resolving perceptual aliasing in the presence of noisy sensors. In
NIPS.
Silver, D., & Tesauro, G. (2009). Monte-carlo simulation balancing. In ICML ’09: Proceedings
of the 26th Annual International Conference on Machine Learning, pp. 945–952 New York,
NY, USA. ACM.
Silver, D., & Veness, J. (2010). Monte-Carlo Planning in Large POMDPs. In Advances in Neural
Information Processing Systems (NIPS). To appear.
Singh, S., James, M., & Rudary, M. (2004). Predictive state representations: A new theory for
modeling dynamical systems. In UAI, pp. 512–519.
Solomonoﬀ, R. J. (1964). A formal theory of inductive inference: Parts 1 and 2. Information and
Control, 7, 1–22 and 224–254.
Strehl, A. L., Li, L., & Littman, M. L. (2009). Reinforcement learning in finite MDPs: PAC analysis.
Journal of Machine Learning Research, 10, 2413–2444.
Strehl, A. L., Li, L., Wiewiora, E., Langford, J., & Littman, M. L. (2006). PAC model-free reinforcement learning. In ICML ’06: Proceedings of the 23rd international conference on
Machine learning, pp. 881–888 New York, NY, USA. ACM.
Strens, M. (2000). A Bayesian framework for reinforcement learning. In ICML, pp. 943–950.
Suematsu, N., & Hayashi, A. (1999). A reinforcement learning algorithm in partially observable
environments using short-term memory. In NIPS, pp. 1059–1065.
Suematsu, N., Hayashi, A., & Li, S. (1997). A Bayesian approach to model learning in nonMarkovian environment. In ICML, pp. 349–357.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Sutton, R. S., & Tanner, B. (2004). Temporal-diﬀerence networks. In NIPS.
Tjalkens, T. J., Shtarkov, Y. M., & Willems, F. M. J. (1993). Context tree weighting: Multi-alphabet
sources. In Proceedings of the 14th Symposium on Information Theory Benelux.
Veness, J., Ng, K. S., Hutter, M., & Silver, D. (2010). Reinforcement Learning via AIXI Approximation. In Proceedings of the Conference for the Association for the Advancement of
Artificial Intelligence (AAAI).
Veness, J., Silver, D., Uther, W., & Blair, A. (2009). Bootstrapping from Game Tree Search. In
Neural Information Processing Systems (NIPS).
141

Veness, Ng, Hutter, Uther, & Silver

Wang, T., Lizotte, D. J., Bowling, M. H., & Schuurmans, D. (2005). Bayesian sparse sampling for
on-line reward optimization. In ICML, pp. 956–963.
Watkins, C., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279–292.
Willems, F., Shtarkov, Y., & Tjalkens, T. (1997). Reflections on “The Context Tree Weighting
Method: Basic properties”. Newsletter of the IEEE Information Theory Society, 47(1).
Willems, F. M. J. (1998). The context-tree weighting method: Extensions. IEEE Transactions on
Information Theory, 44, 792–798.
Willems, F. M. J., Shtarkov, Y. M., & Tjalkens, T. J. (1996). Context weighting for general finitecontext sources. IEEE Trans. Inform. Theory, 42, 42–1514.
Willems, F. M., Shtarkov, Y. M., & Tjalkens, T. J. (1995). The context tree weighting method: Basic
properties. IEEE Transactions on Information Theory, 41, 653–664.

142

Journal of Artificial Intelligence Research 40 (2011) 25-56

Submitted 06/10; published 01/11

A Logical Study of Partial Entailment
Yi Zhou
Yan Zhang

yzhou@scm.uws.edu.au
yan@scm.uws.edu.au

Intelligent Systems Laboratory
School of Computing and Mathematics
University of Western Sydney, NSW, Australia

Abstract
We introduce a novel logical notion–partial entailment–to propositional logic. In contrast with classical entailment, that a formula P partially entails another formula Q with
respect to a background formula set Γ intuitively means that under the circumstance of Γ,
if P is true then some “part” of Q will also be true. We distinguish three different kinds
of partial entailments and formalize them by using an extended notion of prime implicant.
We study their semantic properties, which show that, surprisingly, partial entailments fail
for many simple inference rules. Then, we study the related computational properties,
which indicate that partial entailments are relatively difficult to be computed. Finally, we
consider a potential application of partial entailments in reasoning about rational agents.

1. Introduction
In standard propositional logic, classical entailment does not distinguish among the formulas
that do not entail another formula. For example, let x, y and z be three atoms. We have
that neither x nor z classically entails x ∧ y. However, x seems intuitively “closer” to x ∧ y
than z. The reason is that, x ∧ y can be considered as the union of two “parts” x and y,
and x is exactly one of them. On the other hand, z is completely irrelevant to x ∧ y.
This example motivates us to consider the notion of “partial entailment”. In comparison with classical entailment, partial entailment intends to capture the “partial satisfaction”
relationship between two formulas with respect to a background theory. In standard propositional logic, that a formula P entails another formula Q with respect to a formula set Γ
intuitively means that under the circumstance of Γ, if P is true then Q will also be true. In
contrast, that a formula P partially entails another formula Q with respect to a formula set
Γ intuitively means that, under the circumstance of Γ, if P is true then some “part” of Q
will also be true. Partial entailment is widely used in everyday life. For example, suppose
that a student Laura wants to get score “HD” on the subjects mathematics and physics in
the final examination. If she cannot achieve both of them, she could also be satisfied to
achieve just one of them. At least, it is better than achieving none.
Let us consider a further example. Suppose that the background formula set is empty
and the objective formula is x ∧ y. Consider the following four formulas: x, x ∧ z, x ∧ ¬y
and z. Clearly, none of these formulas classically entails x ∧ y. However, x, x ∧ z and x ∧ ¬y
entail x, which can be regarded as a part of x ∧ y. In contrast, z is irrelevant to x ∧ y. Thus,
one can conclude that, to some extent, x, x ∧ z and x ∧ ¬y “partially satisfy” x ∧ y while z
does not.
c
2011
AI Access Foundation. All rights reserved.

Zhou & Zhang

One may observe that there exists some difference between x and x ∧ z in partially
satisfying x ∧ y. That is, x ∧ z contains a new atom z, which is irrelevant to x ∧ y. On
the other hand, x is exactly a part of x ∧ y. In other words, x ∧ z contains “irrelevancy”
(namely z) in partially satisfying x ∧ y while x does not. Another observation is about
x ∧ ¬y. Although it entails x (a part of x ∧ y), it also entails ¬y, which contradicts to y
(also a part of x ∧ y). In other words, x ∧ ¬y has some “side effect” (namely ¬y) besides
partially satisfying x ∧ y.
The background theory is crucial for partial entailment. As an example, consider z and
x ∧ y again. Suppose that the background formula set is empty. In this case, z does not
partially satisfy x ∧ y. If the background formula set turns into {z → x}, then z should
partially satisfy x ∧ y since under the background theory, if z holds, then x holds as well.
Based on the above observations and intuitions, in this paper, we formalize the notion of partial entailment between two formulas with respect to a background theory in
a propositional language. Moreover, we distinguish three different kinds of partial entailments, namely weak partial entailment, partial entailment and strong partial entailment.
The intuitions of them are summarized in Table 1. Of course, all of them require partial
satisfaction between the two formulas. However, weak partial entailment may allow both
irrelevancies and side effects, whilst partial entailment prohibits side effects but still may
allow irrelevancies, and strong partial entailment is the strongest one that further prohibits
irrelevancies.
Table 1: Intuitions of partial entailments
weak partial entailment
partial entailment
strong partial entailment

partial satisfaction
yes
yes
yes

irrelevancy
allowed
allowed
no

side effect
allowed
no
no

This paper is organized as follows. In the next section, we formalize three different kinds
of partial entailments by using an extended notion of prime implicant, and discuss their
semantic properties. In Section 3, we focus on the computational complexities of decision
problems in relation to prime implicant and partial entailments, ranging from some special
cases to the most general case. We then compare the notion of partial entailments with
related notions in the AI literature in Section 4. In Section 5, we show how the notions of
partial entailments can be applied to formalize partial goal satisfaction in reasoning about
rational agents. Finally, we draw our conclusions in Section 6.

2. Partial Entailment
We restrict our discussions within a propositional language, denoted by L. Formulas in L
are composed recursively by a finite set Atom of atoms (also called variables) with {>, ⊥}
and standard connectives ¬ and →. The connectives ∧, ∨, ↔ are defined as usual. Literals
are atoms and their negations. We use lower case letters to denote atoms and literals, upper
case letters to denote formulas, lower Greek letters to denote literal sets, and upper Greek
26

A Logical Study of Partial Entailment

letters to denote formula sets respectively. We write −l to denote the complementary literal
of a literal l, −π to denote the set of complementary literals of all literals in π. We write
Atom(l), Atom(P ), Atom(π) and Atom(Γ) to denote the sets of atoms occurring in literal
l, formula P , literal set π and formula set Γ respectively.
We say that a literal set π is an assignment over a set A of atoms (assignment for
short if A = Atom) if for each atom x ∈ A, exactly one of x and ¬x is in π. Notice that
both a literal l and an assignment π can also be considered as formulas. For convenience,
henceforth, both l and π also denote their corresponding formulas if it is clear from the
context. The entailment relation |= and the notion of model are defined in the standard
way. A set of formulas is said to be consistent if it has at least one model, otherwise, it
is said to be inconsistent. A theory is a set of formulas closed under |=. Let Γ be a set of
formulas. The deductive closure of Γ, denoted by T h(Γ), is the minimal theory (in the sense
of set inclusion) containing Γ. For convenience, we also use Γ itself to denote the theory
T h(Γ) if it is clear from the context.
We write P |l to denote the formula obtained from P by simultaneously replacing every
occurrence of x by > (⊥) if l is of the form x (¬x). If π is a consistent literal set (i.e.
6 ∃x ∈ Atom s.t. x ∈ π and ¬x ∈ π), and π = {l1 , l2 , ..., lk }, we write P |π to denote the
formula (((P |l1 )|l2 )|...)|lk .
2.1 A Simple Case
We begin to define the notions of partial entailments between two formulas with respect
to a background formula set by considering a rather simple case. That is, both the two
formulas are consistent conjunctions of literals and the background formula set is assumed
to be empty.
Definition 1 Let π and π 0 be two consistent sets of literals.
• We say that π weakly partially entails π 0 if π ∩ π 0 6= ∅.
• We say that π partially entails π 0 if π ∩ π 0 6= ∅ and π ∩ −π 0 = ∅.
• We say that π strongly partially entails π 0 if ∅ ⊂ π ⊆ π 0 .
Definition 1 simply follows the intuitions presented in the introduction section (see
Table 1). In this simple case, π 0 is a consistent set of literals. Thus, the “parts” of π 0
can be regarded as all the elements (or subsets) of π 0 . Recall our intuitive sense of partial
satisfaction. That is, if π holds, then some parts of π 0 hold as well. This means that there
exists some part of π 0 , which is a subset of π. Clearly, this can be precisely captured by
π ∩ π 0 6= ∅, which is exactly the definition of weak partial entailment in this simple case.
For partial entailment, we forbid side effects based on partial satisfaction. Again, since π 0
is a consistent set of literals, the “side effects” of π 0 can be regarded as all the complementary
literal of the elements (or subsets) of π 0 . Hence, that π has no side effects to π 0 means that π
does not mention the complementary literal of any elements (or subsets) of π 0 . Clearly, this
can be precisely captured by π ∩ −π 0 = ∅, where −π 0 is the set of complementary literals of
all elements in π 0 . Together with π ∩ π 0 6= ∅, this forms the definition of partial entailment
in this simple case.
27

Zhou & Zhang

Finally, consider strong partial entailment. We require additionally that there is no
irrelevancy. The irrelevancies of π 0 can be considered as all those atoms (or literals) not
mentioned in π 0 . Hence, that π has no irrelevancy to π 0 means that there is no atom
(or literal) mentioned in π but not in π 0 . Formally, this can be precisely captured by
Atom(π) ⊆ Atom(π 0 ) (or π ⊆ π 0 ). Whatever the case is, together with the two conditions
π ∩ π 0 6= ∅ and π ∩ −π 0 = ∅, this is equivalent to π ⊆ π 0 and π 6= ∅, which is the definition
of strong partial entailment in this simple case.
Example 1 Recall the example proposed in the introduction section. According to Definition 1, it is easy to check that x, x ∧ ¬y and x ∧ z weakly partially entail x ∧ y but z does
not; x and x ∧ z partially entail x ∧ y but x ∧ ¬y and z do not; only x strongly partially
entails x ∧ y while x ∧ ¬y, x ∧ z and z do not. The results coincide with our intuitions and
observations discussed in the introduction section.
Comparing weak partial entailment and partial entailment, one may observe that x ∧ ¬y
weakly partially entails x∧y but it does not partially entail x∧y. As discussed, this is because
¬y is a “side effect” to x ∧ y. Comparing partial entailment and strong partial entailment,
one may observe that x ∧ z partially entails x ∧ y but it does not strongly partially entail
x ∧ y. Again, this coincides with our discussions since z is an “irrelevancy” to x ∧ y. 2
An interesting phenomenon is about the relationships between partial entailments and
classical entailment. In this simple case, it is easy to see that if π classically entails π 0 ,
then π partially entails π 0 . Also, π weakly partially entails π 0 . This means that classical
entailment is a special case of (weak) partial entailment to some extent. However, this does
not hold for strong partial entailment. For instance, x ∧ y classically entails x, but it does
not strongly partially entail x since y is an “irrelevancy” with respect to x.
At first glance, it seems that this result is strange in the sense that the term of partial
entailment should be a generalization of classical entailment. However, this does not mean
that classical entailment also prohibits side effects and irrelevancies. Thus, from the intuitive
sense, one can only conclude that classical entailment should be a special case of weak
partial entailment, but may be mutually different from partial entailment and strong partial
entailment. Interestingly, as we will show later, classical entailment indeed prohibits side
effects. This means that classical entailment is a special case of partial entailment as well.
However, classical entailment may allow irrelevancy (e.g., x ∧ y classically entails x but y is
an irrelevancy to x). Hence, roughly speaking, classical entailment is a special case of both
partial entailment and weak partial entailment, but classical entailment and strong partial
entailment are mutually different.
Definition 1 gives a basic impression on the notions of partial entailments. In the
following, we consider to extend the three notions of partial entailments in a general sense,
in which the two formulas are arbitrary formulas and the background is an arbitrary set of
formulas.
2.2 Prime Implicant
In order to define partial entailments in general case, we use Quine’s notion of prime implicant (Quine, 1952), which has been widely used in many areas in logic and computer
28

A Logical Study of Partial Entailment

science. An excellent survey of prime implicant and its dual prime implicate is due to
Marquis (2000).
Roughly speaking, a prime implicant of a formula is a minimal set (in the sense of
set inclusion) of literals that entails this formula. The notion of prime implicant can be
relativized with respect to a background formula set as follows.
Definition 2 (Relativized prime implicant) A literal set π is a prime implicant of a
formula P with respect to a formula set Γ if:
1. Γ ∪ π is consistent.
2. Γ ∪ π |= P .
3. There does not exist a literal set π 0 ⊂ π that satisfies the above two conditions.
The set of all prime implicants of P with respect to Γ is denoted by P I(Γ, P ). For convenience, we omit Γ when it is empty.
Intuitively, a prime implicant of P w.r.t. Γ is a minimal set of information needed in
order to satisfy P under the circumstance of Γ. Condition 1 requires that the set of information is “feasible”, i.e., it must be consistent with the background formula set. Condition
2 means that the set of information is an “implicant”, i.e., it is indeed powerful enough to
satisfy P w.r.t. Γ. Finally, condition 3 requires that the set of information is “prime”, i.e.,
it is a minimal set of information for satisfying P w.r.t. Γ.
Example 2 According to Definition 2, the only prime implicant of x ∧ y is {x, y}, while
x ∨ y has two prime implicants, namely {x} and {y}. The background formula set indeed
plays an important role. Clearly, x has a unique prime implicant {x}. However, {y} is also
a prime implicant of x with respect to the background formula set {y → x}. 2
It is well known that the relativized notion of prime implicant is similar to the notion of
logic-based abduction (Eiter & Gottlob, 1995; Selman & Levesque, 1990; Eiter & Makino,
2007). There are different characterizations of abduction in the literature. Here, we consider
one of the most well-studied forms (Selman & Levesque, 1990; Eiter & Makino, 2007).
Definition 3 (Selman & Levesque, 1990) Let Γ be a theory, F a formula and H a set
of literals. An (abductive) explanation of F with respect to H from Γ is a minimal set
π ⊆ H such that:
1. Γ ∪ π is consistent,
2. Γ ∪ π |= F .
Following the above definitions, it can be observed that the relativized notion of prime
implicant and abduction can be simply transformed each other. More precisely, suppose
that Γ is a theory, F a formula and H a set of literals. Let Lit be the set of all literals in
the language. Then, π is a prime implicant of F w.r.t. Γ iff π is an abductive explanation of
F from Γ with respect to Lit. Conversely, π is an abductive explanation of F with respect
to H from Γ iff π is a prime implicant of F w.r.t. Γ and mentions literals only from H.
29

Zhou & Zhang

Another closely related notion to prime implicant is its dual, called prime implicate,
which is also of interests in many areas as well. Roughly speaking, a prime implicate of a
propositional formula P is a minimal clause (i.e., disjunction of literals) entailed by P . It
is easy to see that the conjunction of a set π of literals is a prime implicant of a formula
P if and only if the disjunction of all the complementary literals of the elements in π is a
prime implicate of ¬P . Prime implicate can also be relativized to a background formula
set as well (Marquis, 2000). We omit this definition since we are mainly focused on prime
implicant in this paper.
Many properties in relation to prime implicate were discussed and summarized by Marquis (2000). Not surprisingly, similar results hold for prime implicant as well. In the
following, we recall some of the important properties in relation to the relativized notion of
prime implicant, which will be used later in this paper.
Proposition 1 Let Γ be a finite set of formulas, P a formula and π a set of literals. π is a
V
prime implicant of P w.r.t. Γ iff Γ ∪ π is consistent and π is a prime implicant of Γ → P ,
V
where Γ is the conjunction of all formulas in Γ.
Proposition 2 Let P be a formula, Γ a set of formulas and π a literal set such that Γ ∪ π
is consistent and Γ ∪ π |= P . Then there exists π 0 ⊆ π such that π 0 is a prime implicant of
P with respect to Γ.
Proposition 3 Let P be a formula, Γ a formula set and π a literal set. If π is a prime
implicant of P w.r.t. Γ, then there exists an assignment π 0 such that π ⊆ π 0 and π 0 is a
model of both Γ and P .
Together with Proposition 2, Proposition 3 indicates the correspondence relationship
between all the models of P w.r.t. Γ and all the prime implicants of P w.r.t. Γ. Suppose
that π is a model of P and is consistent with Γ. Then, according to Proposition 2, there
exists a subset of π, which is a prime implicant of P w.r.t. Γ. Conversely, according to
Proposition 3, every prime implicant of P w.r.t. Γ can be extended to a model of both Γ
and P .
Proposition 4 Let P be a formula and Γ a set of formulas. Γ |= ¬P if and only if
P I(Γ, P ) = ∅; Γ |= P if and only if P I(Γ, P ) = {∅}.
Proposition 5 Let P and Q be two formulas and Γ a set of formulas. Γ |= P ↔ Q iff
P I(Γ, P ) = P I(Γ, Q).
To conclude, the prime implicants of a formula P w.r.t. a formula set Γ play two roles.
Cases: On one hand, if there exists an assignment π satisfying both Γ and P , then according
to Proposition 2, there exists a subset of π which is a prime implicant of P w.r.t. Γ. On
the other hand, if π is a prime implicant of P w.r.t. Γ, then according to Proposition
3, it can be extended to an assignment satisfying both Γ and P . This means that the
prime implicants of P w.r.t. Γ are corresponding to the possible worlds (assignments)
satisfying both Γ and P . Intuitively, they are corresponding to all the cases which
make P true w.r.t. Γ, i.e., all the ways to achieve P under Γ.
30

A Logical Study of Partial Entailment

Parts: Suppose that π is a prime implicant of P w.r.t. Γ and l is a literal in π. Then,
Γ ∪ π |= P and Γ ∪ π\{l} 6|= P . As mentioned above, π can be considered as a way
(case) to achieve P w.r.t. Γ. Then, intuitively, l plays an essential role for achieving
P w.r.t. Γ via π. Without l, π is no longer a way to achieve P under Γ. This shows
that all the elements in π are essential. In other words, π is a minimal way to achieve
P under Γ, and none of the elements in π can be thrown away. Thus, these literals
can be considered as parts of P w.r.t. Γ.
Both roles of prime implicants will be exploited by the notions of partial entailments
introduced in the following.
2.3 Definitions of Partial Entailments
Based on the notion of prime implicant, we formalize three kinds of partial entailments.
From the weakest to the strongest, they are weak partial entailment, partial entailment and
strong partial entailment respectively.
Definition 4 (Weak partial entailment) A formula P weakly partially entails a formula Q with respect to a formula set Γ, denoted by P W
Γ Q, if:
1. P I(Γ, P ) is not empty.
2. For each π ∈ P I(Γ, P ), there exists π 0 ∈ P I(Γ, Q), such that π ∩ π 0 6= ∅.
Definition 5 (Partial entailment) A formula P partially entails a formula Q with respect to a formula set Γ, denoted by P Γ Q, if:
1. P I(Γ, P ) is not empty.
2. For each π ∈ P I(Γ, P ), there exists π 0 ∈ P I(Γ, Q), such that π ∩ π 0 6= ∅ and π ∩ −π 0 =
∅.
Definition 6 (Strong partial entailment) A formula P strongly partially entails a formula Q with respect to a formula set Γ, denoted by P SΓ Q, if:
1. P I(Γ, P ) is not empty.
2. For each π ∈ P I(Γ, P ), there exists π 0 ∈ P I(Γ, Q), such that ∅ ⊂ π ⊆ π 0 .
W
We write P 6W
Γ Q if it is not the case that P Γ Q, similar for partial entailment and
strong partial entailment. For convenience, we omit Γ when it is empty.
Clearly, these definitions are generalizations of Definition 1 proposed in Section 2.1 since
the unique prime implicant of a literal set is itself. Note that the only difference among
these definitions of partial entailments is indeed presented in Section 2.1 in the simple case.
Definitions 4-6 all require that for any literal set in P I(Γ, P ), there exists some literal
set in P I(Γ, Q) such that the two literal sets satisfy the same corresponding relationships
presented in Definition 1 respectively.
Let us take a closer look at condition 2 in Definition 4. Recall our intuitive sense of
partial satisfaction again, that is, if P is true under Γ, then some parts of Q hold as well.

31

Zhou & Zhang

As we discussed in Section 2.2, the prime implicants of P w.r.t. Γ represent all the cases
that make P true under Γ. Also, the prime implicants of Q w.r.t. Γ can capture the idea
of “part” of Q under Γ. Hence, condition 2 means that, intuitively, for all the cases that
P is true under Γ (captured by all the prime implicants of P w.r.t. Γ), there exists a
way to achieve Q under Γ (captured by a prime implicant of Q w.r.t. Γ), such that the
former partially satisfies the latter (reduced to the simple case in Definition 1). In addition,
condition 1 in Definition 4 ensures that there exists at least one such situation. The intuitive
senses of Definitions 5 and 6 can be explained in a similar way.
Example 3 Let the background theory Γ be {x ∨ y, z → y}. Consider two formulas P =
(x ∧ r) ∨ (y ∧ s) and Q = (x ∧ z) ∨ (¬x ∧ y ∧ s). The prime implicants of P w.r.t. Γ are
as follows: {x, r}, {y, s}, {r, s}, {z, s}, {¬y, r} and {¬x, s}. Notice that {¬y, r} is a prime
implicant of P w.r.t. Γ although the literal ¬y even does not occur in P . On the other
hand, the prime implicants of Q w.r.t. Γ are as follows: {x, z}, {¬x, s} and {z, s}. Notice
that {¬x, y, s} is not a prime implicant of Q w.r.t. Γ since {¬x, s} is a subset of it and also
satisfies Q under Γ.
According to the definitions, we have that P does not weakly partially entail Q w.r.t.
Γ since there is a prime implicant {¬y, r} of P w.r.t. Γ such that it has no intersection
with any of the prime implicants of Q w.r.t. Γ. Also, P does not (strongly) partially entail
Q w.r.t. Γ either. Conversely, Q (weakly) partially entails P w.r.t. Γ, whilst it does not
strongly partially entail P w.r.t. Γ. 2
Note that there are other possible relations between two formulas w.r.t. a background
formula set by using the extended notion of prime implicant. In this paper, for defining
partial entailments, we use a ∀∃ style definition in the sense that we require that “for all”
literal sets in P I(Γ, P ), “there exists” a literal set in P I(Γ, Q) such that the two literal
sets satisfy the conditions in Definition 1 respectively. Recall again the intuition of partial
entailments, which requires that in any cases, if P is true under the context of Γ, then some
part of Q is true under the context of Γ. This naturally suggests the ∀∃ style definition for
the definitions of partial entailments (i.e. Definitions 4-6), where the “∀”-part captures all
the cases of achieving P under the context of Γ, and the “∃”-part associates a possibility
of achieving Q under the context of Γ to the above case for P . In addition, the condition
item 2 in Definitions 4-6 ensures that, in this case, P partially achieves Q w.r.t. Γ via the
two literal sets.
The reason why we choose the ∀∃ style definition for partial entailments can also be
explained analogously to classical entailment. Note that a formula P classically entails
another formula Q w.r.t. a background theory Γ iff for all models of P w.r.t. Γ, it is also a
model of Q w.r.t. Γ.
Certainly, there are other possible styles, including ∀∀, ∃∃ and ∃∀. For instance, the ∃∀
style definition in the weakest case should be: “there exists” π ∈ P I(Γ, P ) such that “for
all” π 0 ∈ P I(Γ, Q), π ∩ π 0 6= ∅. Although these definitions might be interesting and useful
elsewhere, they fail to capture the basic idea of “partial satisfaction”.
Example 4 Consider two formulas x ∨ z and x ∧ y. The former has two prime implicants,
namely {x} and {z}, while the latter has a unique prime implicant, namely {x, y}. If we
use the ∃∃ style definition, we have x ∨ z (weakly, strongly) partially entails x ∧ y. However,
32

A Logical Study of Partial Entailment

this is not intuitive because {z} achieves x ∨ z true but it is irrelevant to x ∧ y. In other
words, the assignment {¬x, ¬y, z} satisfies x ∨ z but not x ∧ y. Hence, there exists a case
that x ∨ z is true but none of the parts of x ∧ y is true.
Consider the formula x ∨ y, which has two prime implicants, {x} and {y}. If we take the
∀∀ style definition or the ∃∀ one, this formula does not (weakly, strongly) partially entail
itself. This is obviously not intuitive. 2
Another feasible direction of definitions is to switch the formulas P and Q. For instance,
the switched-∀∃ style definition in the weakest case is: “for all” π ∈ P I(Γ, Q), “there exists”
some π 0 ∈ P I(Γ, P ) such that π∩π 0 6= ∅. This definition, and the corresponding switched-∃∀,
∃∃ and ∀∀ style definitions, fail to capture the basic idea of partial satisfaction either. Again,
recall the intuition of partial entailments. That is, if P is true (under the context of Γ), then
some part of Q is true as well (under the context of Γ). Similar to classical entailment, this
naturally suggests the order of P and Q for the definitions of partial entailments. Also, the
switched-∀∀ (switched-∃∃) style definition is the same as the ∀∀ (∃∃) style definition. As
shown in Example 4, they are not suitable for defining the notions of partial entailments. To
understand why the switched-∀∃ style and the switched-∃∀ style definitions fail for capturing
partial entailments, let us consider the following example.
Example 5 Consider two formulas x ↔ y and x. The former has two prime implicants,
namely {x, y} and {¬x, ¬y}, while the latter has a unique prime implicant, namely {x}. If
we take the switched-∀∃ style definition, we have x ↔ y (weakly, strongly) partially entails
x. However, this is not intuitive because {¬x, ¬y} achieves x ↔ y but it also achieves ¬x.
Hence, there exists a case that x ↔ y is true but x is false.
Again, consider the formula x ∨ y. It has two prime implicants, namely {x} and {y}.
Intuitively, this formula should (weakly, strongly) partially entail itself. However, this is not
the case if we apply the switched-∃∀ style definition. 2
One of the fundamental properties is the relationships among three notions of partial
entailments. The following proposition shows that weak partial entailment is weaker than
partial entailment, which is further weaker than strong partial entailment. However, by
observations from Example 1, the converses do not hold in general.
Proposition 6 (Basic relationships among partial entailments) Let P , Q be two formulas and Γ a formula set. If P SΓ Q, then P Γ Q; if P Γ Q, then P W
Γ Q.
It is worth mentioning that, in the definitions of partial entailments, P I(Γ, P ) is required
not to be empty to exclude the case when P is inconsistent with the background theory
Γ. The underlying intuition is that partial entailments require “real” connections between
the two formulas w.r.t. the background theory. Let us take a closer look at item 2 in the
definitions. The basic idea of partial entailments is that for each prime implicant of P w.r.t.
Γ, there exists a prime implicant of Q w.r.t. Γ such that the two literal sets satisfy that,
for instance, their intersection are not empty. In other words, P partially entails Q w.r.t.
Γ via these two literal sets. However, item 2 does not exclude the case where there is no
prime implicant of P w.r.t. Γ. In this case, item 2 still holds. However, we cannot say that
P partially entails Q w.r.t. Γ “via the two literal sets” now because the two literals sets do
33

Zhou & Zhang

not exist at all. Hence, we require an additional item 1 to make sure that P I(Γ, P ) is not
empty.
Then, for any formulas P inconsistent with Γ, P does not (weakly, strongly) partially
entail any formula Q. Also, for any formulas P entailed by Γ, P does not (weakly, strongly)
partially entail any formula Q. However, the reasons of these two cases are different. For
the former, the reason is that P I(Γ, P ) is empty (see Proposition 4), thus P is excluded by
item 1 in the definitions. For the latter, the reason is that P I(Γ, P ) = {∅} (see Proposition
4 again), thus the prime implicant of P w.r.t. Γ has no intersections with other prime
implicants. We say that a formula P is trivial with respect to a background formula set Γ if
Γ |= P or Γ |= ¬P . Otherwise, we say that P is nontrivial with respect to Γ. The following
proposition shows that a trivial formula can neither (weakly, strongly) partially entail any
formula, nor they can be (weakly, strongly) partially entailed by any formulas.
Proposition 7 (Non-Triviality) Let Γ be a formula set and P and Q two formulas. If
P is trivial w.r.t. Γ, then P 6Γ Q and Q 6Γ P . This assertion holds for weak partial
entailment and strong partial entailment as well.
Proposition 8 (Extension of Classical Entailment) Let Γ be a formula set and P and
Q two formulas nontrivial w.r.t. Γ. If Γ |= P → Q, then P Γ Q. Also, P W
Γ Q.
Proposition 8 shows that partial entailment is an extension of classical entailment if only
considering nontrivial formulas, so is weak partial entailment. The converses do not hold.
As a simple example, x (weakly) partially entails x ∧ y but x does not classically entail x ∧ y.
Note that strong partial entailment and classical entailment are mutually different. For
example, x ∧ y classically entails x but x ∧ y does not strongly partially entail x. Conversely,
x strongly partially entails x∧y but x does not classically entail x∧y. As we demonstrated in
Section 2.1, the reason is that strong partial entailment prohibits irrelevancy whilst classical
entailment does not.
In addition, strong partial entailment captures the notion of “part” in a broader sense.
In other words, if a formula P strongly partially entails a formula Q with respect to a
background formula set Γ, then P can be considered as a “part” of Q w.r.t. Γ. In this
sense, it also explains why classical entailment does not necessarily imply strong partial
entailment. However, this explanation seems to suggest the other way around. That is, if
P strongly partially entails Q w.r.t. Γ (i.e., P is a part of Q w.r.t. Γ), then Q classically
entails P w.r.t. Γ. In fact, this is not the case either. For example, x strongly partially
entails (x ∧ y) ∨ (y ∧ z), but (x ∧ y) ∨ (y ∧ z) does not classically entails x. The reason here
is that a formula may contain disjunctive information. If restricted into the simple case
discussed in Section 2.1, this is indeed the case. That is, given two consistent literal sets π
and π 0 , π strongly partially entails π 0 if and only if π 0 classically entails π.
2.4 More Semantic Properties
In this subsection, we extensively study some semantic properties in relation to all three
kinds of partial entailments. The reasons why we are interested in these properties are
twofold. Firstly, they provide deeper understandings of how partial entailments work. As
we will see later, surprisingly, many simple inference rules fail for partial entailments. Secondly, these properties illustrate some similarities/differences among three kinds of partial
34

A Logical Study of Partial Entailment

entailments, as well as the similarities/differences between partial entailments and classical
entailment.
We consider a collection of inference rules. Some of them are considered to be important
in many kinds of philosophical logics and knowledge representation logics, others are likely
to hold for partial entailments. Let Γ and Γ0 be two sets of formulas, P , Q and R formulas
and x and y atoms. In this subsection, we assume that all formulas are nontrivial w.r.t.
the background theory.1 Here, we use >Γ to denote any kinds of partial entailments with
respect to Γ. The inferences rules we considered are listed as follows:
Ref: (Reflexivity) P >Γ P , meaning that a formula partially entails itself w.r.t. any background theory.
LE: (Left Equivalence) If Γ |= P ↔ R and P >Γ Q, then R >Γ Q, meaning that if two
formulas are equivalent under the background theory, then they play the same role in
partial entailments on the left side.
RE: (Right Equivalence) If Γ |= Q ↔ R and P >Γ Q, then P >Γ R, meaning that if two
formulas are equivalent under the background theory, then they play the same role in
partial entailments on the right side.
BE: (Background theory Equivalence) If Γ |= Γ0 and Γ0 |= Γ, then P >Γ Q iff P >Γ0 Q,
meaning that if two background theories are equivalent, then they play the same role
in partial entailments.
Rev: (Relevancy) If P > Q, then Atom(P ) ∩ Atom(Q) 6= ∅, meaning that if one formula
partially entails another w.r.t. the empty background theory, then the atom sets used
in the two formulas respectively must not be disjoint.
Tran: (Transitivity) If P >Γ Q and Q >Γ R, then P >Γ R, meaning that partial entailment
relation among formulas is an ordering on the propositional language.
AS: (Atom Substitution) If P >Γ Q, then P (x/y) >Γ Q(x/y),2 meaning that any atoms
in a partial entailment relation can be replaced by other atoms.
LO: (Left Or) If P >Γ Q and R >Γ Q, then P ∨R >Γ Q, meaning that if both two formulas
partially entail another one, then their disjunction should partially entails the formula
as well.
LS: (Left Strengthening) If Γ |= P → R and R >Γ Q, then P >Γ Q, meaning that if a
formula partially entails another, then a formula strengthened by the former should
partially entail the latter as well.
RA: (Right And) If P >Γ Q and P >Γ R, then P >Γ R ∧ Q, meaning that if two
formulas are both partially entailed by another formula, then their conjunction should
be partially entailed by the formula as well.
1. This is because trivial formulas are not interesting for partial entailments (see Proposition 7).
2. Here, P (x/y) is the formula obtained from P by simultaneously replacing every occurrence of atom x by
y, similar for Q(x/y).

35

Zhou & Zhang

Table 2: Properties of partial entailments
Ref
LE
RE
BE
Rev
* Tran
As
LO
* LS
RA
RO
Mono
LN
RN

weak partial entailment
yes
yes
yes
yes
yes
no
no
no
yes
no
no
no
no
no

partial entailment
yes
yes
yes
yes
yes
no
no
no
no
no
no
no
no
no

strong partial entailment
yes
yes
yes
yes
yes
yes
no
no
no
no
no
no
no
no

RO: (Right Or) If P >Γ Q, then P >Γ Q ∨ R, meaning that if a formula partially entails
another, then it should partially entail the disjunction of the latter formula with any
formulas.
Mono: (Monotonicity) If Γ0 |= Γ and P >Γ Q, then P >Γ0 Q, meaning that adding more
information into the background theory preserves partial entailment relations.
LN: (Left Negation) If P >Γ Q, then ¬P 6>Γ Q, meaning that it is impossible that both a
formula and its negation partially entail another formula.
RN: (Right Negation) If P >Γ Q, then P 6>Γ ¬Q, meaning that it is impossible that both
a formula and its negation can be partially entailed by another formula.
Suppose that we define the relation >Γ between P and Q as classical entailment, i.e.,
Γ |= P → Q. Then, it satisfies all the inference rules proposed above. However, the following
proposition shows that partial entailments fail for many of them.
Proposition 9 Table 2 summarizes whether all three kinds of partial entailments satisfy
the inference rules considered above.
Table 2 illustrates some similarities and differences among all three kinds of partial
entailments. They have the same results for most of these inference rules. However, Transitivity (Tran) and Left Strengthening (LS) (highlighted by the ∗ symbol in Table 2) are
two exceptions. Transitivity (Tran) holds for strong partial entailment but not for partial
entailment and weak partial entailment. Left Strengthening (LS) holds for weak partial
entailment but not for partial entailment and strong partial entailment. Finally, both weak
36

A Logical Study of Partial Entailment

partial entailment and partial entailment are extensions of classical entailment in the sense
considered in Proposition 8, while strong partial entailment is not.
Also, Table 2 illustrates some similarities and differences between partial entailments
and classical entailment. As we mentioned above, all these inference rules hold for classical
entailment. However, this is not the case for partial entailments. This means that partial
entailments actually behave quite different from classical entailment. Among these inference
rules, Transitivity is an important one. It does not hold for partial entailment and weak
partial entailment since they allow irrelevancies. Nevertheless, Transitivity holds for strong
partial entailment. However, strong partial entailment is even more dissimilar to classical
entailment since they are mutually different.
Table 2 also indicates that it is not easy to capture the properties of the notions of
partial entailments due to the fact that they fail most of the simple inference rules (see
Table 2). Meanwhile, some properties, (e.g. Tran, LN and RN), may seem to be intuitive
for partial entailments. However, it turns out that they do not hold in general according to
the formal definitions.
Certainly, one can consider other inference rules, for instance, Contraposition. That is,
if P >Γ Q, then ¬Q Γ ¬P . One can check that Contraposition does not hold for all three
kinds of partial entailments either.
2.5 Discussions
One may doubt whether partial entailments can be directly captured within classical propositional logic. For example, one may define partial entailment as follows. A formula P
partially entails a formula Q w.r.t. a formula set Γ iff there exists a formula R such that
Γ ∪ {R} 6|= Q but Γ ∪ {P, R} |= Q. However, this definition cannot capture partial satisfaction. In fact, for every formula P such that Γ 6|= P → Q, there always exists such a formula
V
R (Let R be ¬ Γ∨¬P ∨Q). Even if we restrict R with consistent literal sets, this definition
does not capture partial satisfaction either. For instance, x ∨ ¬y should partially entail x ∧ y
according to this definition since (x ∨ ¬y) ∧ y |= x ∧ y. However, this conclusion is counterintuitive. As an alternative possibility, since “P partially entails Q” intuitively means that
P entails some parts of Q, one may define partial entailment as: a formula P partially
entails a formula Q iff there exists Q0 and Q00 such that |= Q ↔ Q0 ∧ Q00 , Q00 6|= Q0 and
P |= Q0 . However, this definition fails to capture the essence of partial entailment either.
For instance, according to this definition, x partially entails y since |= y ↔ (x∨y)∧(¬x∨y),
¬x ∨ y 6|= x ∨ y, and x |= x ∨ y.
According to the definitions of partial entailments, x (weakly, strongly) partially entails
both x ∧ y and x ∨ y. One may conclude that this result indicates that conjunction and
disjunction play a similar role in partial entailments. However, this is not the case. In this
example, the reasons why x partially entails x ∧ y and why x partially entails x ∨ y are
completely different. In the former case, the reason is that x is a part of x ∧ y, while in the
latter case, the reason is that x classically entails x ∨ y. In fact, conjunction and disjunction
play totally different roles in partial entailments. For instance, x ∧ y partially entails x,
while x ∨ y does not. More examples can be easily found.
37

Zhou & Zhang

Another evidence to illustrate the differences between conjunction and disjunction is to
consider another special case of partial entailments by restricting the formulas to clauses,
i.e., disjunction of literals, and assuming the background theory to be empty.
Proposition 10 Let δ and δ 0 be two non-valid clauses (i.e., disjunctions of two consistent
sets of literals). The following statements are equivalent.
1. δ is a subset of δ 0 if they are represented as two sets of literals.
2. δ classically entails δ 0 .
3. δ weakly partially entails δ 0 .
4. δ partially entails δ 0 .
5. δ strongly partially entails δ 0 .
Proposition 10 states that under this setting, classical entailment and all three kinds of
partial entailments coincide. Compared Proposition 10 to Definition 1, disjunction and
conjunction are quite different in partial entailments.
As we mentioned earlier, the Substitution principle does not hold for all three kinds of
partial entailments in general. This means that, in partial entailments, the atoms indeed
play a different role with formulas. For instance, x (weakly, strongly) partially entails x ∧ y.
However, it is not necessary the case that P partially entails P ∧ Q. This shows a difference
between partial entailments and classical entailment due to the fact that the Substitution
principle holds for standard propositional logic.

3. Complexity Analysis
In this section, we analyze the complexity issues in relation to the extended notion of prime
implicant and all three kinds of partial entailments. We assume that readers are familiar
with some basic notions of computational complexity. More details can be found in the
textbook by Papadimitriou (1994).
Here, we briefly recall some complexity classes. DP is a widely used complexity class,
which contains all languages L such that L = L1 ∩ L2 , where L1 is in NP and L2 is in coNP.
The polynomial hierarchy is defined recursively as follows:
∆P0 := ΣP0 := ΠP0 := P,
P

∆Pi+1 := P Σi , i ≥ 0,
P

ΣPi+1 := N P Σi , i ≥ 0,
P

ΠPi+1 := coN P Σi , i ≥ 0.
P

For instance, ∆P3 = P Σ2 is the complexity class of all languages that are recognizable in
polynomial time by a deterministic Turing Machine equipped with a ΣP2 oracle. In particular, ∆P3 [O(log n)] is a subset of the ∆P3 by restricting the uses of the oracle within logarithmical times. Gottlob (1995) showed that the complexity classes ∆P3 and ∆P3 [O(log n)]
38

A Logical Study of Partial Entailment

coincide with so-called ΣP2 -dag and ΣP2 -tree respectively. Intuitively, a ΣP2 -dag is an acyclic
directed graph of dependent queries to ΣP2 oracles. The nodes are queries whilst the edges
represent the dependency relationships among nodes. A ΣP2 -tree is a ΣP2 -dag that is a tree.
We mainly investigate the following decision problems, both in the most general case
and in some restricted cases:
PRIC(Γ, P, π) : (Prime Implicant Checking) to determine whether a literal set π is a prime
implicant of a formula P w.r.t. a formula set Γ.
LEPR(Γ, P, l) : (Literal Existence in Prime Implicants) to determine whether a literal l is in
at least one prime implicant of a formula P w.r.t. a formula set Γ.
LAPR(Γ, P, l) : (Literal in All Prime Implicants) to determine whether a literal l is in all
prime implicants of a formula P w.r.t. a formula set Γ.
WPE(Γ, P, Q) : (Weak Partial Entailment) to determine whether P weakly partially entails Q
w.r.t. Γ.
PE(Γ, P, Q) : (Partial Entailment) to determine whether P partially entails Q w.r.t. Γ.
SPE(Γ, P, Q) : (Strong Partial Entailment) to determine whether P strongly partially entails
Q w.r.t. Γ.
For convenience, we omit Γ when it is empty.
The first three decision problems are concerned with the relativized notion of prime
implicant, while the others are concerned with the notions of partial entailments. The main
focuses in this paper are of course the latter ones, while the former ones are needed as the
intermediate steps.
Note that some of the complexity results related to prime implicant can be found in or
followed by existing results in the literature (see more details in Appendix). For instance, as
shown by Marquis (2000) (Propositions 3.27 and 3.32), checking whether a literal set (with
or without background theory) is a prime implicate of a formula is DP complete. Another
result comes from Proposition 10 by Lang et al. (2003) that checking whether a literal is
in at least one prime implicant of a formula is NP complete. Also, as a consequence of the
correspondence between prime implicant and abduction, many complexity results related
to abduction can be borrowed for studying prime implicant. For instance, as shown in the
Appendix, checking whether a literal l is in one of the prime implicants of F with respect
to Γ is exactly the task of relevance checking in abduction.
However, as the main focus of this paper, partial entailments are defined upon prime
implicant (i.e. abduction). Although existing results conclude some of the computational
complexity results related to prime implicant, they do not reveal too much information
about the complexities of partial entailments.
3.1 Empty Background Theory
We start our complexity analysis when the background formula set Γ is empty. The following
proposition shows the complexity results in relation to prime implicant when the background
theory is empty. As mentioned above, the first two results follow by existing results in the
literature.
39

Zhou & Zhang

Proposition 11 The computational complexities in relation to prime implicant with empty
background theory are summarized in Table 3.

Table 3: Complexity results for prime implicant: empty background theory
PRIC(P, π)
LEPR(P, l)
LAPR(P, l)

DP complete
NP complete
DP complete

Based on the computational analysis on prime implicant, the following proposition
presents a complete result for the complexity results in relation to all three kinds of partial
entailments when the background theory is empty.3
Proposition 12 The computational complexities in relation to partial entailments with
empty background theory are summarized in Table 4. Here, l is a literal, π is a set of
literals and P and Q are formulas.

Table 4: Complexity results for partial entailments: empty background theory
(l, P )
(P, l)
(π, P )
(P, π)
(P, Q)

NP
DP
NP
DP
ΠP2

WPE
complete
complete
complete
complete
complete

NP
DP
NP
DP
ΠP2

PE
complete
complete
complete
complete
complete

SPE
NP complete
coNP complete
ΣP2 complete
coNP complete
ΠP3 complete

Let us take a closer look at Table 4. First of all, it shows that it is not an easy task
to compute the notions of partial entailments, even when the background theory is empty.
For instance, to check whether a formula (weakly) partially entails another formula is at
the second level of polynomial hierarchy. More surprisingly, this task for strong partial
entailment is even more difficult, which is at the third level of polynomial hierarchy.
Also, from Table 4, one can conclude that the computational complexities increase when
the forms of formulas become more complicated. For instance, checking whether a literal
strongly partially entails a formula is NP complete. When extending the literal to a literal
set, this becomes ΣP2 complete. The task further turns into ΠP3 complete when extending the
antecedent to an arbitrary formula. Interestingly, for weak partial entailment and partial
entailment, no matter considering partially entailing a formula or partially entailed by a
formula, turning a literal to a literal set does not increase the complexity. However, for
3. Clearly, if both the antecedent and the consequent are restricted to literal sets, then the corresponding
decision problems in relation to all three kinds of partial entailments can be solved in linear time.

40

A Logical Study of Partial Entailment

strong partial entailment, this is the case for partially entailed by a formula but not for
partially entailing a formula.
Finally, it can be observed that, when the background theory is empty, weak partial
entailment and partial entailment have the same computational complexities, while strong
partial entailment acts differently. Interestingly, based on some basic assumptions in the
complexity theory, checking strong partial entailment relationship is sometimes easier than
the rest two (e.g. compare SPE(P, π) and PE(P, π)), but sometimes more difficult (e.g.
compare SPE(P, Q) and PE(P, Q)).
In some application scenarios, the background theory is simply a set of facts, which can
be represented as a set of literals. Hence, we are interested in the computational complexities
for this special case. As we shall see later, this is much simpler than the general case when
the background theory is an arbitrary set of formulas.
The following proposition indicates that, fortunately, turning the background formula
set into a literal set does not increase the computational complexities for all three kinds of
partial entailments.
Proposition 13 All the complexity results in Tables 3 and 4 remain the same if the background theory is a set of literals.
3.2 General Background Theory
Finally, we face to the general case, in which the background theory is an arbitrary formula
set. We only consider the general cases of all the six decision problems. One reason is that
the background theory is a general setting already. Another reason is that, even for a single
literal, there might exist many different prime implicants of it with respect to an arbitrary
background theory.
Proposition 14 The computational complexities in relation to partial entailments in general case are summarized in Table 5.4

Table 5: Complexity results: general case
PRIC(Γ, P, π)
LEPR(Γ, P, l)
LAPR(Γ, P, l)
WPE(Γ, P, Q)
PE(Γ, P, Q)
SPE(Γ, P, Q)

empty Γ
DP complete
NP complete
DP complete
ΠP2 complete
ΠP2 complete
ΠP3 complete

arbitrary Γ
DP complete
ΣP2 complete
ΠP2 complete
in ∆P3 [O(log n)]/ ΠP2 hard/ ΣP2 hard
in ΠP3 / ΠP2 hard/ ΣP2 hard
ΠP3 complete

According to Table 5, the complexities of prime implicant checking and strong partial
entailment checking remain the same when allowing arbitrary background theory, while the
4. Here, we also present the corresponding computational complexity results when the background theory
is empty in order to compare them directly.

41

Zhou & Zhang

complexities increase for the other decision problems. For instance, checking whether a
literal is in one (or all) prime implicant(s) of a formula increases from NP complete (DP
complete, resp.) to ΣP2 complete (ΠP2 complete, resp.) when the background theory turns
into an arbitrary one. Also, the complexities of checking weak partial entailment and partial
entailment in general case increase a little because these two problems are both ΣP2 and
ΠP2 hard. However, the corresponding complexity for strong partial entailment remains the
same since it is already ΠP3 hard in the special case when the background theory is empty.
As shown in Table 5, WPE(Γ, P, Q) and PE(Γ, P, Q) are both ΠP2 hard and ΣP2 hard. This
shows that they are neither ΠP2 complete nor ΣP2 complete based on some basic assumptions
in the complexity theory. We conjecture that WPE(Γ, P, Q) is ∆P3 [O(log n)] complete and
PE(Γ, P, Q) is ΠP3 complete. However, to verify this, more advanced techniques, e.g. the
raising technique proposed by Liberatore (2007), are needed.

4. Related Work
In section 2, we discussed the relationships between partial entailments and classical entailment. Here, we further consider the relationships between partial entailments and other
related notions in the literature, including the family of notions of relevance and their relatives, such as formula-variable independence (Boutilier, 1994; Lang et al., 2003), relevance
between formulas (Lakemeyer, 1995, 1997), novelty (Marquis, 1991) and probabilistic positive relevance (Zhou & Chen, 2006), and partial satisfaction of a formula (Lieberherr &
Specker, 1981; Käppeli & Scheder, 2007).
4.1 Formula-Variable Independence
Lang et al. (2003) defined a notion of formula-variable independence between a formula
and a set of atoms. Roughly speaking, a formula is independent to a set of atoms if the
formula can be rewritten to another one that mentions no atoms from the set of atoms.
Definition 7 (Lang et al., 2003) A formula F is variable-independent to a set V of
variables iff there exists a formula G such that |= F ↔ G and Atom(G) ∩ V = ∅.
Also, Lang et al. proved that variable-independence coincides with the notions of influenceability introduced by Boutilier (1994) and relevance of a formula to a set of atoms
introduced by Lakemeyer (1997).
As shown by Lang et al. (2003), the notion of formula-variable independence can be
reformulated by prime implicant. Here, we show that it can be reformulated by weak partial
entailment as well.
Proposition 15 Let F be a formula and V a set of atoms. The following statements are
equivalent:
1. F is variable-independent to V .
2. For all literals l in V ∪ −V , l is not in any prime implicants of F .
3. For all literals l in V ∪ −V , l does not weakly partially entail F .
42

A Logical Study of Partial Entailment

4.2 Other Forms of Relevance
Lakemeyer also introduced other forms of relevance as well, including strict relevance, explanatory relevance and relevance between two subject matters (Lakemeyer, 1997).
Definition 8 (Lakemeyer, 1997) A formula F is strictly relevant to a set V of atoms
iff every prime implicate of F contains at least one atom from V .
The following proposition shows that strict relevance can be reformulated by prime
implicant and weak partial entailment as well.
Proposition 16 Let F be a formula and V = {x1 , . . . , xn } a set of atoms. The following
statements are equivalent.
1. F is strictly relevant to V .
2. ¬F weakly partially entails (x1 ∧ . . . ∧ xn ) ∨ (¬x1 ∧ . . . ∧ ¬xn ).
Definition 9 (Lakemeyer, 1997) A formula F is explanatory relevant to a set V of
atoms w.r.t. a formula set Γ iff there exists a minimal abductive explanation of F w.r.t. Γ
that mentions a variable from V .
The definition of explanatory relevance can be reformulated by prime implicant. That
is, F is explanatory relevant to V w.r.t. Γ iff there exists l ∈ V ∪ −V such that l is in at
least one of the prime implicants of F w.r.t. Γ.
4.3 Relevance between Formulas
As pointed out by Lakemeyer (1997), it is interesting to consider relevance between formulas
with respect to a background theory. Here, we propose a definition of relevance between
two formulas with respect to a background theory by using prime implicant.
Definition 10 A formula P is relevant to a formula Q with respect to a formula set Γ iff
there exists a prime implicant π of P w.r.t. Γ and a prime implicant π 0 of Q w.r.t. Γ such
that π ∩ π 0 6= ∅ .
Definition 10 looks very similar to the definition of weak partial entailment (See Definition 4). The major difference between these two definitions is that weak partial entailment
is defined by a ∀∃ style, whilst relevance is defined by an ∃∃ style. More precisely, in the
former, we require that “for all” prime implicants of P w.r.t. Γ, “there exists” a prime
implicant of Q w.r.t. Γ such that their intersection is not empty. However, in the latter,
we require that “there exists” a prime implicant of P w.r.t. Γ and “there exists” a prime
implicant of Q w.r.t. Γ satisfying the same condition.
However, weak partial entailment can serve as a strict notion of relevance between two
formulas with respect to a background theory. That is, a formula P is strictly relevant to
another formula Q with respect to a formula set Γ if and only if P weakly partially entails
Q w.r.t. Γ. Clearly, this strict version of relevance between formulas w.r.t. to a background
theory based on weak partial entailment implies the normal one defined in Definition 10,
but the converse does not hold in general.
43

Zhou & Zhang

4.4 Novelty and Novelty-Based Independence
The notion of novelty (Marquis, 1991) is defined between two formulas with respect to a
formula set as well.
Definition 11 (Marquis, 1991) A formula P is new positive (new negative) to another
formula Q with respect to a formula set Γ iff there exists a prime implicant π of Q (¬Q)
with respect to Γ ∪ {P }, which is not a prime implicant of Q (¬Q) with respect to Γ.
In addition, a notion of independence, namely novelty-based independence (also known
as separability, see Levesque, 1998), between two formulas is defined based on novelty (Lang,
Liberatore, & Marquis, 2002).
Definition 12 (Lang et al., 2002) Two formulas P and Q are (novelty-based) independent iff P is not new negative to Q w.r.t. >.
Intuitively, that P is new to Q w.r.t. Γ means that adding new information P to
the background theory Γ has some influences on Q or ¬Q. Although novelty and partial
entailments are both defined by using the notion of prime implicant, they are essentially
different. For instance, (non)novelty-based independence satisfies Symmetry. That is, if
P is (non)novelty-based independent to Q, then Q is (non)novelty-based independent to
P . However, (weak, strong) partial entailment does not satisfy Symmetry. For instance,
x (weakly, strongly) partially satisfies x ∨ y, but the converse does not hold. As another
example, x ∨ y (weakly, strongly) partially entails x ∧ y. However, x ∨ y is not new positive
(negative) to x ∧ y. Also, x ↔ y is new positive (negative) to x. However, x ↔ y does not
(weakly, strongly) partially entail x.
4.5 Probabilistic Positive Relevance
Another approach to formalize a certain kind of usefulness is probabilistic positive relevance
(Zhou & Chen, 2006), which is based on probability distributions. The basic idea of positive relevance is: a formula P is positive relevant to another formula Q with respect to a
background formula set Γ iff for all probability distributions P r, P r(Q|Γ∪{P }) ≥ P r(Q|Γ).
Although positive relevance looks similar to partial entailments, and probability distribution is highly related to prime implicant (Lang et al., 2002), the underlying intuition
and the semantic properties of positive relevance and partial entailments are quite different.
For instance, according to the definition of positive relevance by Zhou and Chen (2006),
x ∨ y is positive relevant to x. However, x ∨ y does not (weakly, strongly) partially entail
x. Generally speaking, in positive relevance (defined based on probability distributions),
Symmetry holds. That is, P is positive relevant to Q w.r.t. Γ if and only if Q is positive
relevant to P w.r.t. Γ. However, this does not hold for any kinds of partial entailments.
4.6 Partial Satisfaction of a CNF
The term “partial satisfaction” is also used for satisfaction of a subset of a set of clauses
(Lieberherr & Specker, 1981; Käppeli & Scheder, 2007). A CNF formula (i.e. a conjunction
of clauses) is said to be k-satisfiable if every subformula of it containing at most k clauses
is satisfiable.
44

A Logical Study of Partial Entailment

Although the term “partial satisfaction” is used for both k-satisfaction and partial entailments, it represents different intuitive meanings. For the former, ‘part” means a subset
of a formula (i.e. the set of clauses), while for the latter, it means a subset of a prime implicant of a formula. Hence, partial entailments and k-satisfaction are basically irrelevant.
Moreover, k-satisfaction is only concerned with a particular (CNF) formula, while partial
entailments are concerned with the relationship between two formulas (w.r.t. a background
theory).

5. Partial Goal Satisfaction
In traditional logic based approach of rational agency, the agents always try to find actions
to “completely” achieve their goals. If not successful, the agents would choose to wait and
do nothing. This idea is usually formalized by using classical entailment. However, it is not
always the case that the agents can find out such a perfect action (think about the real world
that we live in). Under the situation, it is sometimes useful for the agents to do something
“towards” their goals rather than just waiting, in which those actions “partially” satisfying
the agents’ goal should be rational candidates. Here, partial entailments may serve as a
logic foundation.
Example 6 Let us consider an example. Suppose that Laura wants to have milk and cereal
as her breakfast. However, there are only three choices available on the breakfast menu.
Choice1 Only milk is offered.
Choice2 Milk and bread are offered.
Choice3 Only bread is offered.
Due to the current information, none of these choices completely satisfies Laura’s goal.
Then, what should she do? 2
Here, we argue that, under this circumstance, it is also rational for the agents to choose
those actions partially achieving their goal according to their belief, as there is no action
completely achieving the goal.
It is important to allow those actions partially achieving the agents’ goal to be rational
candidates. First of all, the rationality of performing those actions is solid since they
make the goal closer for the agents and there is nothing better to do. Hence, choosing
those actions useful to the agents’ goal is more reasonable than just waiting. Also, the
environment is dynamic in nature. In some cases, if the agents do not choose those actions
partially achieving their goal, it might lose the chance to achieve the goal forever. This
frequently happens in everyday life when we cannot seize the opportunities.
We formalize partial achievement between actions and goals under the agents’ belief
by using the notion of partial entailments proposed in Section 2. This is natural since
partial entailments precisely capture the partial satisfaction relations between two formulas
with respect to a background theory. To apply partial entailments to formalize partial goal
satisfaction, we treat the background theory as the agents’ belief, the consequent as the
agents’ goal and the antecedent as the consequences of the action. One obstacle is how to
45

Zhou & Zhang

represent actions and their consequences. Here, to address this issue, we simply use triples
in Hoare logic to represent actions.5
Again, we restrict our discussions within a propositional language. The goal and the
belief of agents are represented as a propositional formula and a set of propositional formulas
respectively. An action α is a triple hP re(α), α, P ost(α)i, where α is a label called the body of
the action, and P re(α) and P ost(α) are two propositional formulas called the precondition
and postcondition of the action respectively. Next, we show how the traditional idea of
complete goal satisfaction and our idea of partial goal satisfaction can be formalized under
this setting.
Definition 13 (Complete goal satisfaction) Let Γ be an agent’s belief and G the agent’s
goal. Let A be a set of candidate actions. An action α ∈ A completely achieves the agent’s
goal G according to its belief Γ iff:
1. Γ |= P re(α).
2. Γ ∪ {P ost(α)} is consistent.
3. Γ |= P ost(α) → G.
Definition 13 means that if the agent’s belief satisfies the precondition of an action and
it believes that the postcondition of the action entails its goal, then this action completely
achieves its goal under the belief. Partial goal satisfaction is formalized in a similar way
except that partial entailments are used instead of classical entailment.
Definition 14 (Partial goal satisfaction) Let Γ be an agent’s belief and G the agent’s
goal. Let A be a set of candidate actions. An action α ∈ A (weakly, strongly) partially
achieves the agent’s goal G according to its belief Γ iff:
1. Γ |= P re(α).
S
2. P ost(α)(W
Γ , Γ ) Γ G.

Definition 14 means that if the agent’s belief satisfies the precondition of an action and
it believes that the postcondition of the action partially entails its goal, then this action
partially achieves its goal under the belief.
Example 7 Consider Example 6 proposed previously. To formalize this example, we use x,
y and z to represent “having milk”, “having cereal” and “having bread” respectively. Then,
Laura’s belief is empty since there is no background knowledge; her goal can be represented
as x ∧ y; the preconditions of the three actions to take the three choices respectively are
empty as well and can be represented as >, while the postcondition of the three cases can
be represented as x, x ∧ z and z respectively. According to Definition 13, none of the three
actions completely achieves the goal x ∧ y. However, according to Definition 14, to take
Choice1 and Choice2 (weakly) partially achieve the goal. In particular, to take Choice1
strongly partially achieves the goal, whilst to take Choice3 does not partially achieve the
goal in any sense. 2
5. Here, we only use a simple action theory to demonstrate how the notion of partial entailments can be
used for formalizing partial goal satisfaction. The problems of how to represent actions for agents in
general and how to deal with the frame problem is out of the scope of this paper.

46

A Logical Study of Partial Entailment

A problem arises which kind of partial goal satisfaction is rational for the agents. At first
glance, it seems that partial entailment is more appropriate for this purpose. Weak partial
entailment may contain side effects that are not acceptable, whilst strong partial entailment
are too strict that some powerful actions are excluded. However, we believe it is better to
leave this to the agent designers. One may choose different kinds of partial goal satisfaction
in different application domains, e.g., strong partial entailment if both irrelevancies and
side effects are crucial issues, or partial entailment if side effects are unacceptable but
irrelevancies are not, or weak partial entailment in looser cases.
It is worth mentioning that partial goal satisfaction only proposes one possible solution
for dealing with the situations that the agents cannot find a plan to completely achieve
their goal. It is neither the only solution nor necessarily perfectly rational. In some such
situations, the actions achieving some parts of the agents’ goal may lose its chances to
achieve other parts, which might be unacceptable for the agents.
Also, the actions partially achieving the agents’ goal are only rational candidates but
not compulsory. It is quite different between the fact that an action is a rational candidate
and that the agents really choose to perform it. Partial goal satisfaction only explains the
rationality of those actions partially achieving the goals. The problem of which particular
action should be chosen to perform among all the actions partially achieving the agents’
goal is another research topic and is beyond the scope of this paper.
Another approach for handling these situations is to explicitly represent the agents’ full
preferences among all combinations of goals. For instance, in Example 6, Laura may have
the following full preferences among all candidate goals:
{M ilk, Cereal} > {M ilk} > {Cereal} > {},
where > represents the preference relation. If Laura’s original goal (i.e. M ilk ∧ Cereal)
cannot be achieved, then she intends to achieve the second best candidate goal (i.e. M ilk)
according to her preference. It can be observed that partial goal satisfaction is not always
consistent with the full preference approach, for instance, if the above preference of Laura
turns into {M ilk, Cereal} > {} > {M ilk} > {Cereal}. Although sharing some similarities,
these two approaches are essentially different. First of all, full preference requires additional
information, while partial goal satisfaction does not. Moreover, it is expensive to represent
the full preference of agents. Since the number of possible combinations of goals are exponential with respect to the size of goals, to represent a full preference requires double
exponential number of new information. Last but not least, it is usually difficult to obtain
the full preferences of agents. Hence, we believe that both the full preference approach
and the partial goal satisfaction approach have their own advantages and disadvantages for
dealing with the situations when there is no perfect actions to achieve the agents’ goal.
The idea of partial satisfaction of goals has been discussed elsewhere in the AI literature
(Haddawy & Hanks, 1992; Smith, 2004; Do, Benton, van den Briel, & Kambhampati, 2007).
One approach is so called partial satisfaction planning (Smith, 2004; Do et al., 2007). In
partial satisfaction planning, agents find plans, instead of completely achieving their goals,
that partially achieve their goals. Since the agents’ goals in AI planning, particularly in
STRIPS-like planning, are usually represented as conjunctions of small pieces of subgoals,
it can be formalized as finding plans that achieve subsets of those subgoals. In fact, this
is a special case of partial entailments in the sense that partial entailment is dealing with
47

Zhou & Zhang

arbitrary propositional formulas. Another approach is due to Haddawy and Hanks (1992).
In their approach, the agents’ goals are represented as groups of candidate goals and each
of them is associated with a real number between [0, 1] to represent the degree of chances
of satisfying a goal. A plan satisfying a candidate goal with number 1 is full satisfaction
while a plan satisfying a candidate goal with a number between (0, 1) (e.g., 0.5) is partial
satisfaction. There are several differences between this approach and partial entailments.
Firstly, in partial entailments, the objective formula is represented by a single formula
instead of a set of candidate variations. Secondly, numerical degree of satisfaction is not
introduced in partial entailments. Finally, in partial entailments, the partial satisfaction
relationships come from the internal structures of formulas.
Finally, we would like to mention that the application scenario discussed here for partial
entailments is reasoning about rational agents but not classical AI planning, e.g. STRIPS.
The main reason is that the agents’ goal and beliefs are formalized by literal sets rather than
arbitrary formulas in STRIPS planning, but the main focus of this paper is to introduce the
notions of partial entailments to propositional logic in general. Nevertheless, since checking
partial entailments when restricted the antecedent, the consequent and the background
theory all to be literal sets can be done in linear time, it is interesting to apply partial
entailments in AI planning. We would like to leave this as our future investigations.

6. Concluding Remarks
In this paper, we introduced a new logical notion–partial entailment–to propositional logic.
We distinguished three different kinds of partial entailments (See Table 1) based on the
notion of prime implicant. From the weakest to strongest, they are weak partial entailment,
partial entailment and strong partial entailment respectively. We then investigated the
semantic properties of partial entailments (See Table 2). The results demonstrate that the
properties of partial entailments are difficult to be captured since many simple inference
rules do not hold. We also investigated the computational complexity of partial entailments
(See Tables 4 and 5). The complexity results are surprising and interesting. For instance,
checking strong partial entailment is ΠP3 complete, even when the background theory is
empty. This indicates that although the definitions of partial entailments look simple, it is
not easy to compute them.
We showed that the notions of partial entailments can serve as a foundation of formalizing partial goal satisfaction in reasoning about rational agents. Another potential application scenario, mentioned in our previous work (Zhou, van der Torre, & Zhang, 2008),
is goal weakening by using strong partial entailment. When an agent needs to modify its
goal, it can choose a weakened one that strongly partially entails the original goal with
respect to the agent’s belief. This is because strong partial entailment preserves some of
the important parts of the original goal. However, more sophisticated work are needed to
develop such a goal weakening framework. Also, as mentioned previously, it is interesting
to apply partial entailments in AI planning, where all formulas are syntactically restricted
so that the computation task of partial entailment may become easier.
Another direction of future work lies in computing partial entailments. Of course, an
important task is to develop an algorithm directly for this purpose. However, an alternative
48

A Logical Study of Partial Entailment

approach is to identify some tractable subclasses for checking partial entailments since the
general complexities are relatively high.

Acknowledgments
Some preliminary results of this paper were published (Zhou & Chen, 2004; Zhou et al.,
2008). We are grateful to Xiaoping Chen and Leon van der Torre for their inspirations
and contributions on these works. We are also grateful to Jerome Lang for his valuable
comments on an earlier draft of this paper, and we would like to thank the anonymous
reviewers for their valuable comments as well. The authors were partially supported by an
Australian Research Council (ARC) Discovery Projects grant (DP0988396).

Appendix A. Selected Proofs6
Proposition 8 (Extension of Classical Entailment) Let Γ be a formula set and P and
Q two formulas nontrivial w.r.t. Γ. If Γ |= P → Q, then P Γ Q. Also, P W
Γ Q.
Proof: We first prove that P Γ Q. Let π be a prime implicant of P w.r.t. Γ. We have
that Γ ∪ π |= P . Since Γ |= P → Q, we have that Γ ∪ π |= Q. By Proposition 2, there
is a subset π 0 of π such that π 0 is a prime implicant of Q w.r.t. Γ. Thus π 0 ⊆ π. Due to
non-triviality of P and Q, π ∩ π 0 6= ∅ and π ∩ −π 0 = ∅. This shows that P Γ Q.
According to Proposition 6, P W
Γ Q. 2

Proposition 9 Table 2 summarizes whether all three kinds of partial entailments satisfy
the inference rules considered above.
Proof: Here, we only give the proofs or counterexamples of some of the results.
For Relevancy, according to Definition 5, there exists π ∈ P I(P ) and π 0 ∈ P I(Q) such
that π∩π 0 6= ∅. Therefore, Atom(π)∩Atom(π 0 ) 6= ∅. It follows that Atom(P )∩Atom(Q) 6= ∅
since Atom(π) ⊆ Atom(P ) and Atom(π 0 ) ⊆ Atom(Q). Similar for weak partial entailment
and strong partial entailment.
For Transitivity of strong partial entailment, let π be a prime implicant of P w.r.t. Γ.
Since P SΓ Q, there exists a literal set π1 consistent with Γ such that π1 ∈ P I(Γ, Q) and
π ⊆ π1 . Moreover, since Q SΓ R, there exists a literal set π2 consistent with Γ such that
π2 ∈ P I(Γ, R) and π1 ⊆ π2 . Hence, π ⊆ π2 . This shows that P SΓ R.
Note that Transitivity does not hold for either partial entailment or weak partial entailment. For instance, x  x ∧ y and x ∧ y  y, but x 6 y. Similarly, x W x ∧ y and
x ∧ y W y, but x 6W y. The reason why partial entailment and weak partial entailment
fail transitivity is that they allow irrelevancies. In the above example, although x ∧ y partially entails y, it contains x, which is irrelevant to y, and it is exactly the reason why x
partially entails x ∧ y.
6. We only present some of the proofs here. The others are relatively simple, and can be found in our full
version, available at http://www.scm.uws.edu.au/~yzhou/papers/partial-entailment-full-version.pdf .

49

Zhou & Zhang

For Left Strengthening of weak partial entailment, let π be a prime implicant of P w.r.t.
Γ. Then, Γ ∪ π |= P . It follows that Γ ∪ π |= R since Γ |= P → R. By Proposition 2, there
is a subset π1 of π, which is a prime implicant of R w.r.t. Γ. Moreover, R W
Γ Q. Then,
there exists a prime implicant π2 of Q w.r.t. Γ such that π1 ∩ π2 6= ∅. Thus, π ∩ π2 6= ∅.
This shows that P W
Γ Q.
Note that Left Strengthening does not hold for either partial entailment or strong partial
entailment. For example, we have that x S x ∧ y and x  x ∧ y, but x ∧ ¬y 6S x ∧ y and
x ∧ ¬y 6 x ∧ y. The reason why strong partial entailment and partial entailment fail Left
Strengthening is that they prohibit side effects since the strengthening part on the left side
could be side effect to the right side, e.g., ¬y in the above example.
Both Left Negation and Right Negation do not hold for all three kinds of partial entailments. As an example, both x and ¬x (weakly, strongly) partially entail x ↔ y. Meanwhile,
x (weakly, strongly) partially entails both x ↔ y and ¬(x ↔ y). 2

Proposition 11 The computational complexities in relation to prime implicant are summarized in Table 3.
Proof: The first result can be proved similarly to Proposition 3.27 in the work of Marquis
(2000), and the second one follows directly from Proposition 10 in the work of Lang et al.
(2003). For the last result, it is easy to prove that l occurs in all prime implicants of P iff
P can be satisfied and |= P → l. It immediately proves the membership of this assertion.
Hardness follows from the fact that P is satisfiable and Q is unsatisfiable iff x is in all prime
implicants of (x ∧ P ) ∨ (¬x ∧ Q), where x is a new atom. 2

Proposition 12 The computational complexities in relation to partial entailments with
empty background theory are summarized in Table 4. Here, l is a literal, Π is a set of
literals and P and Q are formulas.
Proof: For the membership of WPE(π, P), let π = {l1 , ..., lk }. Then π weakly partially entails
P if and only if there exists li , 1 ≤ i ≤ k such that li is in one of the prime implicant of P .
By Proposition 11, this problem is in NP.
For the membership of PE(π, P), we first prove that a literal set π partially entails a
formula P iff there is an assignment π1 over Atom\Atom(π) and an assignment π2 over
Atom(π) such that π ∪ π1 |= P and π1 ∪ π2 |= ¬P . “⇒:” By Definition 5, there is a
prime implicant π 0 of P such that π ∩ π 0 6= ∅ and π ∩ −π 0 = ∅. Let l ∈ π 0 ∩ π. Then
π 0 \{l} ∪ {−l} 6|= P . It can be extended to an assignment π0 over Atom, which satisfies
¬P . Let π1 ⊆ π0 and Atom(π1 ) = Atom\Atom(π); let π2 ⊆ π0 and Atom(π2 ) = Atom(π).
Clearly, π ∪ π1 |= P and π1 ∪ π2 |= ¬P . “⇐:” By Proposition 2, there is a prime implicant
π 0 of P such that π 0 ⊆ π ∪ π1 . It follows that π 0 ∩ π 6= ∅ and π 0 ∩ −π = ∅. Hence, π
partially entails P . Hence, the following algorithm determines whether π partially entails
P : 1. simultaneously guess two literal sets π1 and π2 ; 2. check whether π1 and π2 together
with π satisfy the above conditions. Step 2 can be done in polynomial time. Therefore,
PE(π, P) in NP.
50

A Logical Study of Partial Entailment

For SPE(π, P), membership is easily shown by the following algorithm: 1. guess a consistent literal set π 0 ; 2. check whether π 0 is a prime implicant of P ; 3. if yes, check whether
π is a subset of π 0 . By Proposition 11, step 2 requires an N P oracle. Hence, this problem is
in ΣP2 . For hardness, we construct a reduction from 2 − QBF . Let X and Y be two disjoint
sets of atoms and P a formula such that Atom(P ) ⊆ X ∪ Y . Let Y = {y1 , y2 , ..., yk }; T1 be
y1 ∨ ... ∨ yk ; T2 be ¬y1 ∨ ... ∨ ¬yk ; x and y be two new atoms different with X ∪ Y ; Q be
(x∧y ∧P )∨(x∧¬y ∧T1 )∨(¬x∧y ∧T2 ). We now prove that ∃X∀Y P holds if and only if x∧y
strongly partially entails Q. Suppose that ∃X∀Y P holds. Then, there exists an assignment
π0 over X such that π0 |= P . By Proposition 2, there exists π1 ⊆ π0 such that π1 is a
prime implicant of P . Therefore, {x, y} ∪ π1 |= Q. In addition, {x} ∪ π1 6|= Q. Otherwise,
{x} ∪ π1 |= (x ∧ y ∧ P ) ∨ (x ∧ ¬y ∧ T1 ) ∨ (¬x ∧ y ∧ T2 ). It follows that {x} ∪ π1 |= y ∨ T1 , a
contradiction. Symmetrically, {y} ∪ π1 6|= Q. Moreover, for all l ∈ π1 , {x, y} ∪ π1 \{l} 6|= Q
since π1 is a prime implicant of P . This shows that {x, y} ∪ π1 is a prime implicant of Q.
Hence, x ∧ y strongly partially entails Q. On the other hand, suppose that x ∧ y strongly
partially entails Q. Then, there exists a prime implicant of Q including both x and y.
Let it be {x, y} ∪ π1 . Therefore π1 |= P . We have π1 6|= T1 . Otherwise, {x} ∪ π1 |= Q.
This shows that {x, y} ∪ π1 is not a prime implicant of Q, a contradiction. Symmetrically,
π1 6|= T2 . This shows that Atom(π1 ) ⊆ X. Moreover, {x, y} ∪ π1 |= Q. It follows that
{x, y} ∪ π1 |= x ∧ y ∧ P . Therefore π1 |= P . Let π0 be an assignment over X such that
π1 ⊆ π0 . Then, π0 |= P . This shows that ∃X∀Y P holds.
For membership of WPE(P, π), P weakly partially entails π iff a) P is satisfiable, and b)
for all assignments π1 over Atom\Atom(π), π1 ∪ −π 6|= P , which is equivalent to −π |= ¬P .
To prove this, suppose that P weakly partially entails π. Then, P is satisfiable. Now assume
that there exists π1 such that π1 ∪ −π |= P . By Proposition 2, there exists π2 ⊆ π1 ∪ −π
such that π2 is a prime implicant of P . However, π2 ∩ π = ∅, a contradiction. On the other
hand, suppose that the above three conditions hold. Then, for all prime implicants π1 of
P , π1 ∩ π 6= ∅. Otherwise, π1 ∪ −π can be extended to an assignment π2 over Atom such
that π2 |= P . However, −π ⊆ π2 , a contradiction. Hence, WPE(P, π) is in DP.
For membership of PE(P, π), let π = {l1 , . . . , ln }. Then, P partially entails π iff for all
prime implicant π 0 of P , a) π 0 ∩ −π = ∅, and b) π 0 ∩ π 6= ∅ iff a) for all i, (1 ≤ i ≤ n), −li
is not in any of the prime implicants of P , and b) P weakly partially entails π. Hence, by
Proposition 11 and the above result for weak partial entailment, this problem is in DP .
For membership of SPE(P, π), let π = {l1 , . . . , ln }, Atom(P )\Atom(π) = {x1 , . . ., xm }.
P strongly partially entails π iff all the prime implicants of P are subsets of π iff for all
i, (1 ≤ i ≤ n), −li is not in any prime implicants of P and for all j, (1 ≤ j ≤ m), xj (¬xj )
is not in any prime implicants of P . By Proposition 11, this problem is in coNP.
For WPE(P, Q), membership can be shown by the following algorithm, which determines
whether P does not weakly partially entail Q: 1. guess a consistent literal set π; 2. check
whether π is a prime implicant of P ; 3. if yes, check whether π does not weakly partially
entail Q. By Proposition 11 and the above case for WPE(π, P), steps 2 and 3 require an N P
oracle. Hence, this problem is in ΠP2 . For hardness, we construct a reduction from 2−QBF .
Let X and Y be two disjoint sets of atoms and P a formula such that Atom(P ) ⊆ X ∪ Y .
Let Y = {y1 , y2 , ..., yk }. Formula T is (y1 ∨ ... ∨ yk ) ∧ (¬y1 ∨ ... ∨ ¬yk ). We prove that
∃X∀Y P holds if and only if P does not weakly partially entail T . Suppose that ∃X∀Y P
holds. Then, there exists an assignment π1 over X such that π1 |= P . By Proposition
51

Zhou & Zhang

2, there exists a prime implicant π2 of P such that π2 ⊆ π1 . Clearly, there is no prime
implicant π3 of T such that π2 ∩ π3 6= ∅. On the other hand, suppose that P does not
weakly partially entail T . Then, there exists a prime implicant π1 of P such that for all
prime implicants π2 of T , π1 ∩ π2 = ∅. It follows that Atom(π1 ) ⊆ X. Then, π1 can be
extended to an assignment π3 over X such that π3 |= P . Hence, ∃X∀Y P holds.
For PE(P, Q), membership can be shown by the following algorithm, which determines
whether P does not partially entail Q: 1. guess a consistent literal set π; 2. check whether
π is a prime implicant of P ; 3. if yes, check whether π does not partially entail Q. By
Proposition 11 and the above case for PE(π, P), steps 2 and 3 require an N P oracle. Hence,
this problem is in ΠP2 . For hardness, we construct a reduction from 2 − QBF . Let X
and Y be two disjoint sets of atoms and P a formula such that Atom(P ) ⊆ X ∪ Y . Let
X = {x1 , x2 , ..., xk }; X 0 = {x01 , x02 , ..., x0k } be k new atoms different with Atom. Formula K
is (x1 ↔ x01 ) ∧ (x2 ↔ x02 ) ∧ ... ∧ (xk ↔ x0k ). We prove that ∀X∃Y P holds if and only if x ∧ K
partially entails x ∧ P , where x is a new atom. Notice that all the prime implicants of K
have the form π ∪ π 0 , where π (π 0 ) is an assignment over X (X 0 ) and for all i, (1 ≤ i ≤ k),
xi ∈ π iff x0i ∈ π 0 (¬xi ∈ π iff ¬x0i ∈ π 0 ). Now suppose that x ∧ K partially entails x ∧ P .
Then, for all assignments π0 over X, {x} ∪ π0 ∪ π00 partially entails x ∧ P . Therefore, there
exists an assignment π1 over Y such that {x} ∪ π0 ∪ π00 ∪ π1 |= x ∧ P . It follows that
π0 ∪ π1 |= P . This shows that ∀X∃Y P holds. On the other hand, suppose that ∀X∃Y P
holds. Then, for all assignments π0 over X, there exists an assignment π1 over Y , such
that π0 ∪ π1 |= P . Therefore, for all prime implicants {x} ∪ π0 ∪ π00 of x ∧ K, there exists
an assignment π1 over Y such that {x} ∪ π0 ∪ π00 ∪ π1 |= x ∧ P . Moreover, there exists an
assignment π2 = {¬x} ∪ π0 ∪ π00 over {x} ∪ X ∪ X 0 , such that π2 ∪ π1 |= ¬(x ∧ P ). Hence,
x ∧ K partially entails x ∧ P .
Finally for SPE(P, Q), membership can be shown by the following algorithm, which determines whether P does not strongly partially entail Q: 1. guess a consistent literal set
π; 2. check whether π is a prime implicant of P ; 3. if yes, check whether π doesn’t
strongly partially entails Q. By Proposition 11, step 2 requires an N P oracle; by the
above case for SPE(π, P), step 3 requires a ΣP2 oracle. Hence, this problem is in ΠP3 .
For hardness, we construct a reduction from 3 − QBF . Let X, Y and Z be three disjoint sets of atoms and P a formula such that Atom(P ) ⊆ X ∪ Y ∪ Z. Suppose that
X = {x1 , x2 , ..., xk }. Let X 0 = {x01 , x02 , ..., x0k } be k new atoms. Let K be the formula
(x1 ↔ x01 ) ∧ (x2 ↔ x02 ) ∧ ... ∧ (xk ↔ x0k ). Suppose that Z = {z1 , z2 , ..., zk }. Let T1 be
z1 ∨ ... ∨ zk and T2 be ¬z1 ∨ ... ∨ ¬zk respectively. Let R be the formula x ∧ y ∧ K and Q be
the formula (x ∧ y ∧ P ∧ K) ∨ (x ∧ ¬y ∧ T1 ∧ K) ∨ (¬x ∧ y ∧ T2 ∧ K) respectively, where x and
y are two new atoms. Next, we will prove that ∀X∃Y ∀ZP holds if and only if R strongly
partially entails Q. The proof is tedious. On the one hand, suppose that ∀X∃Y ∀ZP holds.
Given a prime implicant of R of the form {x, y} ∪ π ∪ π 0 , where π is an assignment over
X and π 0 is a corresponding assignment over X 0 . Then, there exists an assignment π1 over
Y such that π ∪ π1 |= P . Therefore, {x, y} ∪ π ∪ π 0 ∪ π1 |= x ∧ y ∧ P ∧ K. It follows that
{x, y} ∪ π ∪ π 0 ∪ π1 |= Q. By Proposition 2, there exists a subset π2 of {x, y} ∪ π ∪ π 0 ∪ π1 ,
which is a prime implicant of Q. We have that x ∈ π2 . Otherwise, {y} ∪ π ∪ π 0 ∪ π1 |= Q.
Therefore {y} ∪ π ∪ π 0 ∪ π1 |= x ∨ T2 , a contradiction. Symmetrically, y ∈ π2 . Moreover,
for each atom l ∈ π ∪ π 0 , we have that l ∈ π2 since π2 |= K. Therefore {x, y} ∪ π ∪ π 0 ⊆ π2 .
This shows that for all prime implicants of R, there exists a prime implicant of Q such that
52

A Logical Study of Partial Entailment

the former is a subset of the latter. Hence, R strongly partially entails Q. On the other
hand, suppose that R strongly partially entails Q. Notice that for all assignments π over
X, {x, y} ∪ π ∪ π 0 is a prime implicant of R. Therefore, there is a prime implicant of Q
that contains {x, y} ∪ π ∪ π 0 . Let it be {x, y} ∪ π ∪ π 0 ∪ π1 , where Atom(π1 ) ⊆ Y ∪ Z.
Therefore, π ∪ π1 |= P . We have that π1 6|= T1 . Otherwise, {x} ∪ π ∪ π 0 ∪ π1 |= Q, a
contradiction. Symmetrically, π1 6|= T2 . This shows that Atom(π1 ) ⊆ Y . Thus, π1 can be
extended to an assignment π2 over Y . We have that {x, y} ∪ π ∪ π 0 ∪ π2 |= Q. Therefore
{x, y} ∪ π ∪ π 0 ∪ π2 |= x ∧ y ∧ P ∧ K. Hence, π ∪ π2 |= P . This shows that for all assignments
π over X, there exists an assignment π2 over Y , such that π ∪ π2 |= P . That is, ∀X∃Y ∀ZP
holds. 2
Proposition 13 All the complexity results in Tables 3 and 4 remain the same if the background theory is a set of literals.
Proof: This assertion follows directly from the following fact:
Suppose that P is a formula and π is a consistent set of literals. Then, P I(π, P ) =
P I(P |π).
On one hand, suppose that π1 ∈ P I(π, P ). Then, according to the definition, π1 ∪ π is
consistent and π1 ∪ π |= P . We have that Atom(π1 ) ∩ Atom(π) = ∅. Otherwise, suppose
that l ∈ Atom(π1 ) ∩ Atom(π), then π1 ∪ π\{l} |= P . Also, π1 ∪ π\{l} is consistent. This
shows that π1 is not a prime implicant of P w.r.t. π, a contradiction. Moreover, π1 ∪π |= P .
It follows that π1 |= P |π. In addition, there does not exist π2 ⊂ π1 such that π2 |= P |π.
Otherwise, π2 ∪ π |= P and π2 ∪ π is consistent. This shows that π1 is not a prime implicant
of P w.r.t. π, a contradiction. Hence, π1 is a prime implicant of P |π. On the other hand,
suppose that π1 is a prime implicant of P |π. Then, Atom(π1 ) ∩ Atom(π) is empty since
Atom(P |π) ∩ Atom(π) is empty. In addition, π1 ∪ π |= P since π1 |= P |π. Thus, π1 ∪ π
is consistent. Moreover, there does not exist π2 ⊂ π1 such that π2 ∪ π |= P . Otherwise,
π2 |= P |π. This shows that π1 is not a prime implicant of P |π, a contradiction. Hence, π1
is a prime implicant of P w.r.t. π. 2
Proposition 14 The computational complexities in relation to partial entailment in general
case are summarized in Table 5.
Proof: The DP completeness for PRIC(Γ, P, π) follows directly from Proposition 1 and the
DP completeness for PRIC(P, π). This can also be proved in a similar way to the techniques
introduced by Marquis (2000) for proving the DP completeness for the corresponding decision problem of prime implicate (see Marquis, 2000, Proposition 3.36).
For LEPR(Γ, P, l), the membership is easy by guessing a literal set π and checking if l ∈ π
and π ∈ P I(Γ, P ). For hardness, it can be shown that ∃X∀Y P iff x is in one of the elements
in P I(Γ, F ), where Γ = {¬(x ∧ P ∧ (y1 ∨ · · · ∨ yk ))}, F = x ∧ P ∧ ¬(y1 ∨ · · · ∨ yk ) ∨ ¬x ∧
(¬y1 ∨ · · · ∨ ¬yk ), where x is a new atom. The proof is tedious. We only outline the basic
ideas as follows.
x is in one of the elements of P I(Γ, F )
53

Zhou & Zhang

iff
V
V
V
∃ π, x ∈ π, π 6|= ¬ Γ, π |= Γ → F and ∀π 0 ⊂ π, π 0 6|= Γ → F .
iff
V
V
V
∃ π1 , π1 ∪ {x} 6|= ¬ Γ, π1 ∪ {x} |= ¬ Γ ∨ F and π1 6|= ¬ Γ ∨ F . (Notice that π1 ∪ {x}
is not necessarily the same as π mentioned above.)
iff
V
V
V
∃ π1 , π1 6|= (¬ Γ)|x, π1 |= (¬ Γ)|x ∨ F |x and π1 6|= (¬ Γ)|¬x ∨ F |¬x.
iff
∃ π1 , π1 6|= P ∧ (y1 ∨ · · · ∨ yk ), π1 |= P and π1 6|= ¬y1 ∨ · · · ∨ ¬yk .
iff
∃X∀Y P holds. In fact, the hardness result also follows from the ΣP2 completeness of checking
relevance in abductive reasoning (see Eiter & Gottlob, 1995, Thm. 4.11).
For LAPR(Γ, P, l), the membership can be shown by the following algorithm, which determines whether l is not in all prime implicants of P w.r.t. Γ: 1. guess a literal set π;
2. check if π is a prime implicant of P w.r.t. Γ; 3. if yes, check if l is not in π. According to the above result for prime implicant checking, this problem is in ΣP2 . Thus,
the original problem is in ΠP2 . For harness, we can prove it in a similar way to the
above proof for the ΣP2 hardness of LEPR(Γ, P, l). Indeed, one can prove that ∃X∀Y P
iff x is in one of the elements of P I(Γ, F ), where Γ = {¬(x ∧ z ∧ P ∧ (y1 ∨ · · · ∨ yk ))},
F = x ∧ z ∧ P ∧ ¬(y1 ∨ · · · ∨ yk ) ∨ ¬x ∧ ¬z ∧ (¬y1 ∨ · · · ∨ ¬yk ), where x and z are two new
atoms. Moreover, x is in one of the elements of P I(Γ, F ) iff ¬x is not in all of the elements
in P I(Γ, F ) since every element in P I(Γ, F ) contains either x or ¬x. This shows that determining whether a literal is not in all prime implicants of a formula w.r.t. a formula set
is ΣP2 hard. It follows that LAPR(Γ, P, l) is ΠP2 hard.
For WPE(Γ, P, Q), the ΠP2 hardness follows directly from Proposition 12. The ΣP2 hardness
can be implied from the hardness proof of the ΣP2 hardness of LEPR(Γ, P, l) by noticing that
the only prime implicant of x w.r.t. Γ is {x} itself. For the membership, let us first consider
the following algorithm, which determines whether P does not weakly partially entail Q
w.r.t. Γ: 1. compute all the literals which occur at least in one of the prime implicants of Q
w.r.t. Γ; 2. check whether there is a prime implicant of P w.r.t. Γ, which does not contain
any of the literals computed in step 1. According to above result for the ΣP2 hardness of
LEPR(Γ, P, l), step 1 requires linear calls to a ΣP2 oracle. In addition, step 2 requires only
one recall to a ΣP2 oracle based on the results obtained in step 1. To do this, we just need
to guess a consistent literal set, and to check if it is a prime implicant of P w.r.t. Γ and
contains no literals computed from step 1. This algorithm can be converted to a ΣP2 -tree, in
which the root is corresponding to the ΣP2 call for step 2, and its children are corresponding
to the linear ΣP2 calls in step 1 for computing those literals. Thus, according the ΣP2 -tree
techniques introduced by Gottlob (1995), WPE(Γ, P, Q) is in ∆P3 [O(log n)].
For PE(Γ, P, Q), both the ΠP2 hardness and the ΣP2 hardness can be shown in a similar
way to the corresponding tasks for weak partial entailment. For membership, the following
algorithm determines whether there exists a prime implicant π 0 of Q w.r.t. Γ, such that
π ∩ π 0 = ∅ or π ∩ −π 0 6= ∅: 1. guess such a literal set π 0 ; 2. check if π 0 is a prime implicant
of Q w.r.t. Γ; 3. check if π and π 0 satisfy the conditions. According to the above result for
prime implicant checking, step 2 requires calls to an NP oracle. Thus, the above decision
problem is in ΣP2 . Based on this result, the following algorithm determines whether P does
54

A Logical Study of Partial Entailment

not partially entail Q w.r.t. Γ: 1. guess a literal set π; 2. check if π is a prime implicant of
P w.r.t. Γ; 3. check if there exists a prime implicant π 0 of Q w.r.t. Γ such that π ∩ π 0 = ∅
or π ∩ −π 0 6= ∅. It is easy to see that this problem is in ΣP3 . Thus, the original problem is
in ΠP3 .
Finally, for SPE(Γ, P, Q), hardness follows directly from Proposition 12, while membership
can be shown in a similar way to the membership of the same task for partial entailment.
2

References
Boutilier, C. (1994). Toward a logic for qualitative decision theory. In Proceedings of KR’94,
pp. 75–86.
Do, M. B., Benton, J., van den Briel, M., & Kambhampati, S. (2007). Planning with goal
utility dependencies.. In Proceedings of IJCAI’07, pp. 1872–1878.
Eiter, T., & Gottlob, G. (1995). The complexity of logic-based abduction. Journal of the
ACM, 42 (1), 3–42.
Eiter, T., & Makino, K. (2007). On computing all abductive explanations from a propositional horn theory. Journal of the ACM, 54 (5).
Gottlob, G. (1995). NP trees and Carnap’s modal logic. Journal of the ACM, 42 (2),
421–457.
Haddawy, P., & Hanks, S. (1992). Representations for decision-theoretic planning: Utility
functions for deadline goals. In Proceedings of KR’92, pp. 71–82.
Käppeli, C., & Scheder, D. (2007). Partial satisfaction of k-satisfiable formulas. Electronic
Notes in Discrete Mathematics, 29, 497–501.
Lakemeyer, G. (1995). A logical account of relevance.. In Proceedings of IJCAI’95, pp.
853–861.
Lakemeyer, G. (1997). Relevance from an epistemic perspective. Artificial Intelligence,
97 (1-2), 137–167.
Lang, J., Liberatore, P., & Marquis, P. (2002). Conditional independence in propositional
logic. Artificial Intelligence, 141 (1/2), 79–121.
Lang, J., Liberatore, P., & Marquis, P. (2003). Propositional independence: Formulavariable independence and forgetting. Journal of Artificial Intelligence Research, 18,
391–443.
Levesque, H. J. (1998). A completeness result for reasoning with incomplete first-order
knowledge bases. In Proceedings of KR’98, pp. 14–23.
Liberatore, P. (2007). Raising a hardness result. CoRR, abs/0708.4170.
Lieberherr, K. J., & Specker, E. (1981). Complexity of partial satisfaction. Journal of the
ACM, 28 (2), 411–421.
Marquis, P. (1991). Novelty revisited. In Proceedings of ISMIS’91, pp. 550–559.
55

Zhou & Zhang

Marquis, P. (2000). Consequence finding algorithms. In Kohlas, J., & Moral, S. (Eds.),
Handbook of Defeasible Reasoning and Uncertainty Management Systems, Volume 5:
Algorithms for Uncertainty and Defeasible Reasoning, pp. 41–145. Kluwer, Dordrecht.
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.
Quine, W. (1952). The problem of simplifying truth functions. American Mathematical
Monthly, 59 (8), 521–531.
Selman, B., & Levesque, H. J. (1990). Abductive and default reasoning: A computational
core. In AAAI, pp. 343–348.
Smith, D. E. (2004). Choosing objectives in over-subscription planning.. In Proceedings of
ICAPS’04, pp. 393–401.
Zhou, Y., & Chen, X. (2004). Partial implication semantics for desirable propositions.. In
Proceedings of KR’04, pp. 606–612.
Zhou, Y., & Chen, X. (2006). Toward formalizing usefulness in propositional language.. In
Proceedings of KSEM’06, LNAI 4092, pp. 650–661.
Zhou, Y., van der Torre, L., & Zhang, Y. (2008). Partial goal satisfaction and goal change:
weak and strong partial implication, logical properties, complexity. In Proceedings of
AAMAS’08, pp. 413–420.

56

