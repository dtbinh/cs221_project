Journal of Articial Intelligence Research 12 (2000) 105-147

Submitted 10/99; published 3/00

Robust Agent Teams via Socially-Attentive Monitoring

Gal A. Kaminka
Milind Tambe

galk@isi.edu
tambe@isi.edu

Information Sciences Institute and Computer Science Department
University of Southern California
4676 Admiralty Way
Los Angeles, CA 90292, USA

Abstract
Agents in dynamic multi-agent environments must monitor their peers to execute individual and group plans. A key open question is how much monitoring of other agents'
states is required to be eective: The Monitoring Selectivity Problem. We investigate this
question in the context of detecting failures in teams of cooperating agents, via SociallyAttentive Monitoring, which focuses on monitoring for failures in the social relationships
between the agents. We empirically and analytically explore a family of socially-attentive
teamwork monitoring algorithms in two dynamic, complex, multi-agent domains, under
varying conditions of task distribution and uncertainty. We show that a centralized scheme
using a complex algorithm trades correctness for completeness and requires monitoring all
teammates. In contrast, a simple distributed teamwork monitoring algorithm results in
correct and complete detection of teamwork failures, despite relying on limited, uncertain
knowledge, and monitoring only key agents in a team. In addition, we report on the design
of a socially-attentive monitoring system and demonstrate its generality in monitoring several coordination relationships, diagnosing detected failures, and both on-line and o-line
applications.

1. Introduction
Agents in complex, dynamic, multi-agent environments must be able to detect, diagnose,
and recover from failures at run-time (Toyama & Hager, 1997).

For instance, a robot's

grip may be slippery, opponents' behavior may be intentionally dicult to predict, communications may fail, etc. Examples of such environments include virtual environments for
training (Johnson & Rickel, 1997; Calder, Smith, Courtemanche, Mar, & Ceranowicz, 1993),
high-delity distributed simulations (Tambe, Johnson, Jones, Koss, Laird, Rosenbloom, &
Schwamb, 1995; Kitano, Tambe, Stone, Veloso, Coradeschi, Osawa, Matsubara, Noda, &
Asada, 1997), and multi-agent robotics (Parker, 1993; Balch, 1998). The rst key step in
this process is execution-monitoring (Doyle, Atkinson, & Doshi, 1986; Ambros-Ingerson &
Steel, 1988; Cohen, Amant, & Hart, 1992; Reece & Tate, 1994; Atkins, Durfee, & Shin,
1997; Veloso, Pollack, & Cox, 1998).
Monitoring execution in multi-agent settings requires an agent to monitor its peers,
since its own correct execution depends also on the state of its peers (Cohen & Levesque,
1991; Jennings, 1993; Parker, 1993; Jennings, 1995; Grosz & Kraus, 1996; Tambe, 1997).
Monitoring peers is of particular importance in teams, since team-members rely on each
other and work closely together on related tasks:

c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Kaminka and Tambe

 Monitoring allows team-members to coordinate their actions and plans with team-

mates, to help teammates and cooperate without interference. For example, drivers
of cars in a convoy cannot drive without monitoring other cars in the convoy, so as to
not disband the convoy, and to help other drivers if cars break down.
 Monitoring allows team-members to use peers as dynamic information sources, for

learning new information. For instance, if a driver in a convoy sees that the other cars
in front of it suddenly turn to the left, she can infer the existence of an obstacle or
milestone despite not directly seeing it herself.
Previous work has investigated dierent ways of monitoring in the context of teams of cooperating agents. For example, theoretical work on SharedPlans (Grosz & Kraus, 1999) has

passive monitoring, in which an agent is notied when a proposition
changes (e.g., via communications), and active monitoring, in which an agent actively seeks
distinguished between

to nd out when a proposition changes (e.g., via observations and inference of unobservable
attributes). Practical implementations have investigated the use of passive monitoring via
communications (Jennings, 1995), active monitoring via plan-recognition (Huber & Durfee, 1995), active implicit monitoring via the environment (Fenster, Kraus, & Rosenschein,
1995), and dierent combinations of these methods (Parker, 1993; Jennings, 1993; Tambe,
1997; Lesh, Rich, & Sidner, 1999).

No approach is clearly superior to another:

Passive

monitoring is generally perceived as being less costly than active monitoring, but also less
reliable (Grosz & Kraus, 1999; Huber & Durfee, 1995; Kaminka & Tambe, 1998).
Regardless of the monitoring method, bandwidth and computational limitations prohibit
a monitoring agent from monitoring all other agents to full extent, all the time (Jennings,
1995; Durfee, 1995; Grosz & Kraus, 1996). Thus a key open question is how much monitoring of other agents is required to be eective (in teams) (Jennings, 1993; Grosz & Kraus,
1996, 1999). We call this challenging problem the

Monitoring Selectivity Problem,

i.e., the

problem of selectivity in observing others and inferring their state (based on the observations) for monitoring.

Although it has been raised in the past, only a framework and

minimal constraints for answers were provided (Jennings, 1993; Grosz & Kraus, 1996). For
instance, the theory of SharedPlans requires agents to verify that their intentions do not
conict with those of teammates (Grosz & Kraus, 1996). However, the methods by which
such verication can take place are left for further investigation (Grosz & Kraus, 1996, p.
308). Section 8 provides more details on related work.
This paper begins to address the monitoring selectivity problem in teams, by investigating monitoring requirements for eective failure detection.

We focus our investigation

on detecting failures in the social relationships that ideally hold between agents in a monitored team. We call such monitoring of social relationships

socially-attentive monitoring,

to dierentiate it from other types of monitoring, such as monitoring for failures in the
progress of agents towards their goals. Here, the term social relationship is used to denote a
relation on attributes of multiple agents' states. Socially-attentive monitoring in the convoy
example involves verifying that agents have common destination and heading, that their
beliefs in driving as a convoy are mutual, etc. For instance, if the agents are observed to
head in dierent directions, they clearly do not have a common heading. This is dierent
than monitoring whether their chosen (common) heading leads towards their (agreed upon)
destination.

106

Robust Agent Teams via Socially-Attentive Monitoring

Monitoring relationships in a team (socially-attentive monitoring) is a critical task in
monitoring team-members.

Failures to maintain the team's relationships can often lead

to catastrophic failures on the part of the team, lack of cooperative behavior and lack of
coordination. Such failures are often the result of individual agent failures, such as failures
in an agent's sensors and actuators. Thus socially-attentive monitoring covers a large class
of failures, and promotes robust individual operation.
We explore socially-attentive monitoring algorithms for detecting teamwork failures under various conditions of uncertainty.

We analytically show that despite the presence of

uncertainty about the actual state of monitored agents, a centralized

active

monitoring

scheme can guarantee failure detection that is either sound and incomplete, or complete
and unsound. However, this requires reasoning about multiple hypotheses as to the actual
state of monitored agents, and monitoring all agents in the team.

We show that active

distributed teamwork monitoring results in both sound and complete detection capabilities,
despite using a much simpler algorithm. This distributed algorithm: (a) uses only a single,
possibly incorrect hypothesis of the actual state of monitored agents, and (b) involves monitoring only key agents in a team, not necessarily all team-members. Using a transformation
on the analytical constructs, we show analogous results for centralized failure-detection in
mutual-exclusion coordination relationships.
We also conduct an empirical investigation of socially-attentive monitoring in teams.
We present an implemented general socially-attentive monitoring framework in which the
expected ideal social relationships that are to be maintained by the agents are compared
to the actual social relationships. Discrepancies are detected as possible failures and diagnosed. We apply this framework to two dierent complex, dynamic, multi-agent domains,
in service of monitoring various social relationships, both on-line and o-line. Both of these
domains involve multiple interacting agents in collaborative and adversarial settings, with
uncertainties in both perception and action.

In one domain, we provide empirical results

for active monitoring which conrm our analytical results. In another domain we show how
o-line socially-attentive monitoring can provide quantitative teamwork quality feedback to
a designer. We also provide initial diagnosis procedures for detected failures.
Our focus in these explorations is on practical algorithms that have guarantees on performance in real-world applications. The algorithms we present seek to complement the use of
passive communications-based monitoring (which is unreliable in many domains) and explore
the use of unintrusive key-hole plan-recognition as an alternative. However, we do not rule
out the use of communicationswe simply seek to provide techniques that can work even
when communications fail.

Our analytical guarantees of failure-detection soundness and

completeness hold whether monitoring is done through communications or plan-recognition.
This paper is organized as follows: Section 2 presents motivating examples and background. Section 3 presents the socially-attentive monitoring framework. Section 4 explores
monitoring selectivity in centralized teamwork monitoring.

Section 5 explores monitoring

selectivity in distributed teamwork monitoring. Section 6 demonstrates the generality of our
framework by applying it in an o-line conguration. Section 7 presents investigations of additional relationship models. Section 8 presents related work, and Section 9 concludes. The
two appendices contain the proofs for theorems presented (Appendix A), and pseudo-code
for the socially-attentive monitoring algorithms (Appendix B).

107

Kaminka and Tambe

2. Motivation and Background
The monitoring selectivity problem this paper addresseshow much monitoring is required
for failure-detection in teamsrose out of growing frustration with the signicant software
maintenance eorts in two of our application domains.

In the ModSAF domain, a high-

delity battleeld virtual environment (Calder et al., 1993), we have been involved in the
development of synthetic helicopter pilots (Tambe et al., 1995).

In the RoboCup soccer

simulation domain (Kitano et al., 1997) we have been involved in developing synthetic soccer players (Marsella, Adibi, Al-Onaizan, Kaminka, Muslea, Tallis, & Tambe, 1999). The
environments in both domains are dynamic and complex, and have many uncertainties:
the behavior of other agents (some of it adversarial, some cooperative), unreliable communications and sensors, actions which may not execute as intended, etc.

Agents in these

environments are therefore presented with countless opportunities for failure despite the
designers' best eorts.
Some examples may serve to illustrate. The following two examples are actual failures
that occurred in the ModSAF domain.

We will use these two to illustrate and explore

socially-attentive monitoring throughout this paper:

Example 1.

Here, a team of three helicopter pilot agents were to y to a specied way-

point (a given position), where one of the team-members, the

attackers )

towards the enemy, while its teammates (

scout,

was to y forward

land and wait for its signal. All of the

agents monitored for the way-point. However, due to an unexpected sensor failure, one of
the attackers failed to sense the way-point.

So while the other attacker correctly landed,

the failing attacker continued to y forward with the scout (see Figure 1 for a screen shot
illustrating this failure).

Example 2.

In a dierent run, after all three agents reached the way-point and detected it,

the scout has gone forward and identied the enemy. It then sent a message to the waiting
attackers to join it and attack the enemy. One of the attackers did not receive the message,
and so it remained behind indenitely while the scout and the other attacker continued the
mission alone.
We have collected dozens of similar reports in both the ModSAF and RoboCup domain.
In general, such failures are dicult to anticipate in design time, due to the huge number of
possible states. The agents therefore easily nd themselves in novel states which have not
been foreseen by the developer, and the monitoring conditions and communications in place
proved insucient: In none of the failure cases reported did the agents involved detect, let
alone correct, their erroneous behavior. Each agent believed the other agents to be acting in
coordination with it, since no communication was received from the other agents to indicate
otherwise. However, the agents were violating the collaboration relationships between them,
as the agents came to disagree on what plan is being executeda collaboration relationship
failure had occurred.

Preliminary empirical results show that upwards of 30% of failures

reported involved relationship violations (relationship failures).
Human observers, however, were typically quick to notice these failures, because of the
clear social misbehavior of the agents in these cases. They were able to infer that a failure
has occurred despite not knowing what exactly happened. For instance, seeing an attacker
continuing to y ahead despite its teammates' switching to a dierent plan (which the human

108

Robust Agent Teams via Socially-Attentive Monitoring

Enemy

Scout (ahead) and failing attacker (trailing)

Landing attacker

Figure 1: A plan-view display (the ModSAF domain) illustrating the failure in Example 1.
The thick wavy lines are contour lines.

observers inferred from the fact that one of the teammates, the other attacker, has landed)
is sucient for an observer to detect that something has gone amisswithout knowing what
the dierent plan was.
Our analysis showed that the agents were not monitoring each other suciently. However, a naive solution of continuous communications between the agents was clearly impractical since: (i) the agents are operating in a hostile environment; (ii) the communications
overheads would have been prohibitive; and (iii) in fact, it was the communications equipment itself that broke down in some cases. We therefore sought practical ways to achieve
quick detection of failure, based on the limited ambiguous knowledge that was available to
a monitoring agent.

3. Socially-Attentive Monitoring
We begin with an overview of the general structure of a socially-attentive monitoring system, shown in Figure 2. It consists of: (1) a social relationship knowledge-base containing
models of the relationships that should hold among the monitored agents, enabling generation of

expected ideal behavior

in terms of relationships (Section 3.1); (2) an agent and

team modeling component, responsible for collecting and representing knowledge about the
monitored agents'

actual behavior

(Section 3.2); (3) a relationship failure-detection compo-

nent that monitors for violations of relationships among monitored agents by contrasting
the expected and actual behavior (Section 3.3); and (4) a relationship diagnosis component
that veries the failures, and provides an explanation for them (Section 3.4). The resulting

109

Kaminka and Tambe

explanation (diagnosis) is then used for recovery, e.g., by a negotiations system (Kraus,
Sycara, & Evenchik, 1998), or a general (re)planner (Ambros-Ingerson & Steel, 1988).

Expected
Attribute
Values

What agents to monitor,
What agent attributes

Social Relationships Knowledge-Base
Expected Behavior
Relationship
Diagnosis

Relationship
Failure
Detections

Detected
Failure

Observations,
Communications

Actual
Behavior

Actual Values

Monitored Agent

Agent/Team Modeling Component

Diagnosis

Socially-Attentive Monitoring System

Monitored Agent

Figure 2: The general structure of a socially-attentive monitoring system.

3.1 A Knowledge-Base of Relationship Models
We take a relationship among agents to be a relation on their state attributes. A relationship
model thus species how dierent attributes of an agent's state are related to those of
other agents in a multi-agent system. These attributes can include the beliefs held by the
agents, their goals, plans, actions, etc. For example, many teamwork relationship models
require that team-members have mutual belief in a joint goal (Cohen & Levesque, 1991;
Jennings, 1995).

A spatial formation relationship (Parker, 1993; Balch, 1998) species

relative distances, and velocities that are to be maintained by a group of agents (in our
domain, helicopter pilots).

Coordination relationships may specify temporal relationships

that are to hold among the actions of two agents, e.g., business contractors (Malone &
Crowston, 1991).

All such relationships are

social they

explicitly specify how multiple

agents are to act and what they are to believe if they are to maintain the relationships
between them.
The relationship knowledge-base contains models of the relationships that are supposed
to hold in the system, and species the agents that are participating in the relationships. The
knowledge-base guides the agent-modeling component in selecting agents to be monitored,
and what attributes of their state need be represented (for detection and diagnosis). It is
used by the failure detection component to generate expectations which are contrasted with
actual relationships maintained by the agents. And it provides the diagnosis component with
detailed information about how agents' states' attributes are related, to drive the diagnosis
process.

Our implementation of socially-attentive monitoring in teams uses four types of

relationships:
For

formations, role-similarity, mutual exclusion, and teamwork.

teamwork

monitoring

we

use

the

STEAM

(Tambe,

1997)

general

domain-

independent model of teamwork, which is based on Cohen and Levesque's Joint Intentions
Framework (Levesque, Cohen, & Nunes, 1990; Cohen & Levesque, 1991) and Grosz, Sidner,
and Kraus's SharedPlans (Grosz & Sidner, 1990; Grosz & Kraus, 1996, 1999).

110

However,

Robust Agent Teams via Socially-Attentive Monitoring

other teamwork models may be used instead of STEAM. Although STEAM is used by our
pilot and soccer agents to generate collaborative behavior, it is reused here independently
in service of monitoring, i.e., monitored agents are assumed to be a team, and STEAM is
used in monitoring their teamwork. STEAM and other teamwork models (e.g.,

Cohen &

Levesque, 1991; Jennings, 1995; Rich & Sidner, 1997) require mutual belief by team members in their joint goals and plans. This characteristic is used to monitor teamwork in our
system. The other relationship models are used only in a secondary monitoring role. They
will be discussed in greater length in Section 7.

3.2 Knowledge of Monitored Agents and Team
The agent modeling component is responsible for acquiring and maintaining knowledge

actual relations that exist
ideal expected relations. In this

about monitored agents. This knowledge is used to construct the
between agents' states' attributes, which are compared to the

section, we describe the plan-recognition capabilities of the agent-modeling component in our
implementation and experiments, i.e., the extent of the knowledge that could be maintained
about monitored agents' plans if necessary. Later sections show that in fact limited, possibly
inaccurate, knowledge is sucient for eective failure detection. Thus implementations may
use optimized agent-modeling algorithms rather than these full capabilities. Section 3.4 will
discuss additional agent-modeling capabilities, necessary for diagnosis.

3.2.1 Representation

For monitoring teamwork relationships, we have found that representing agents in terms of
their selected hierarchical reactive plans enables quick monitoring of their state, and also
facilitates further inference of the monitored agents' beliefs, goals, and unobservable actions,
since they capture the agents' decision processes.
In this representation, reactive plans (Firby, 1987; Newell, 1990) form a single decomposition hierarchy (a tree) that represents the alternative controlling processes of each agent.
Each reactive plan in the hierarchy (hereafter referred to simply as a plan) has selection
conditions (also referred to as preconditions) for when it is applicable, and termination conditions which are used to terminate or suspend plans. At each given moment, the agent is
executing a single path (root to a leaf ) through the hierarchy.

This path is composed of

plans at dierent levels.
Figure 3 presents a small portion of such a hierarchy, created for the ModSAF domain. In
the case of Example 1, prior to the way-point,

each of the agents

was executing the path be-

execute-mission as highest-level plan, through fly-flight-plan, fly-route,
traveling and low-level. Upon reaching the way-point, they were all supposed to switch
from fly-flight-plan and its descendents to wait-at-point. The attackers would then
select just-wait as a child of wait-at-point, while the scout would select scout-forward
ginning with

and its descendents. Of course, the failing attacker did not detect the way-point and so the

fly-flight-plan and the selection conditions for wait-at-point
the failing attacker continued to execute fly-flight-plan and its

termination conditions for
were not satised and
descendents.

111

Kaminka and Tambe

Execute-Mission
Fly Flight Plan (F)
Fly Route

Wait at Point (W)

Join Scout (J)

Ordered Halt (H)

Low Level

Just Wait

Scout Forward
Traveling

Nap of the Earth

Contour

Figure 3: Portion of Hierarchical Reactive Plan Library for ModSAF Domain (Team plans
are boxed. These are explained in Section 3.3).

3.2.2 Acquisition

From a practical perspective, while the agents may cooperatively report to the monitoring
agent on their own state using communications, it requires communication channels to be
suciently fast, reliable and secure.

This is unfortunately not possible in many realistic

domains, as our examples demonstrate (Section 2).
Alternatively, a monitor may use plan-recognition to infer the agents' unobservable state
from their observable behavior. This approach is unintrusive and robust in face of communication failures. Of course, the monitor may still benet from focused communications with
the other agents, but would not be critically dependent on them.
To enable plan-recognition using reactive plans (our chosen representation), we have
employed a reactive plan-recognition algorithm called RESL (REal-time Situated Leastcommitments). The key capability required is to allow explicit maintenance of hierarchical
plan hypotheses matching each agent's observed behavior, while pruning of hypotheses which
are deemed incorrect or useless for monitoring purposes.

RESL works by expanding the

entire plan-library hierarchy for each modeled agent, and tagging all paths matching the
observed behavior of the agent being modeled (see Appendix B for pseudo-code for the
algorithm). Heuristics and external knowledge may be used to eliminate paths (hypotheses)
which are deemed inappropriateindeed such heuristics will be explored shortly. RESL's
basic approach is very similar to previous work in reactive plan recognition (Rao, 1994) and
team-tracking (Tambe, 1996), which have been used successfully in the ModSAF domain,
and share many of RESL's properties.

However, RESL adds belief-inference capabilities

which are used in the diagnosis process, discussed below (Section 3.4).
Figure 4 gives a simplied presentation of the plan hierarchies for a variation of Example
1, in which all the agents correctly detected the way-point, i.e., no failure has occurred (note
that some plans at intermediate levels have been abstracted out in the gure). The scout
(Figure 4a) and the two attackers (Figures 4b, 4c) switched from the
(denoted by

fly-flight-plan plan

F) to the wait-at-point plan (denoted by W). An outside observer using RESL

infers explanations for each agent's behavior by observing the agents. The scout continues

112

Robust Agent Teams via Socially-Attentive Monitoring

low-level, one of the possible ight-methods
wait-at-point (W) plans. Thus they are both

to y ahead, its speed and altitude matching
under both the

fly-flight-plan (F)

and

tagged as possible hypotheses for the scout's executing plan hierarchy.

Similarly, as the

just-wait
ordered-halt (H)

attackers land, RESL recognizes that they are executing the
this plan can be used in service of either the

W

or the

plan.

plana plan in

which the helicopters are ordered by their headquarters to land immediately.

H

and

W

Thus both

are tagged as explanations for each of the attackers' states (at the second level of

the hierarchies). For all agents, RESL identies the plan
plan.

However,

execute-mission

as the top-level

In this illustration, the actual executing paths of the agents are marked with lled

arrows. Other

individual modeling

hypotheses that match the observed behavior are marked

using dashed arrows. An outside observer, of course, has no way of knowing which of the
possible hypotheses is correct.

Execute Mission

Wait-at-Point (W)

Execute Mission

Execute Mission

Fly-Flight-Plan (F)

Wait-at-Point (W)

Ordered-Halt (H)

Wait-at-Point (W)

Low-Level

Just-Wait

Just-Wait

Just-Wait

Low-Level

(b)

(a)

Ordered-Halt (H)

Just-Wait

(c)

Figure 4: Scout (a) and Attackers' (b, c) actual and recognized

abbreviated

reactive plan

hierarchies.

Once individual modeling hypotheses are acquired for each individual agent (using planrecognition in our implementation, but potentially also by communications), the monitoring
agent must combine them to create team-modeling hypotheses as to the state of the team
as a whole. The monitoring agent selects a single individual modeling hypothesis for each
individual agent and combines them into a single team-modeling hypothesis. Several such
team-modeling hypotheses are possible given multiple hypotheses for individual agents. For
instance, in Figure 4, while all team-hypotheses will have

execution-mission

as the top-

level plan, there are eight dierent team-hypotheses which can be dierentiated by their
second-level plan:

(W,W,W), (W,W,H), (W,H,W), (W,H,H), (F,W,W), (F,W,H), (F,H,W), (F,H,H). If the

observer is a member of the team, it knows what it is executing itself, but would still have
multiple-hypotheses about its teammates' states. For instance, if the attacker in Figure 4b
is monitoring its teammates, its hypotheses at the second level would be (W,W,W), (W,W,H),

(F,W,W), (F,W,H).

To avoid explicitly representing a combinatorial number of hypotheses, RESL explicitly
maintains all candidate hypotheses for each agent individually, but not all combinations
of individual models as team hypotheses. Instead, these combinations are implicitly represented. Thus the number of hypotheses explicitly maintained grows linearly in the number
of agents.

113

Kaminka and Tambe

3.3 Relationship Violation Detection
The failure-detection component detects violations of the social relationships that should
hold among agents.

This is done by comparing the ideal expected relationships to their

actual maintenance by the agents. For teamwork specically, the relationship model requires
team-members to always agree on which

team plan is jointly executed by the team, similarly

to Joint Responsibility (Jennings, 1995), and SharedPlans (Grosz & Kraus, 1996). If this
requirement fails in actuality (i.e., the agents are executing dierent team plans) then a
teamwork failure has occurred.
The basic teamwork failure detection algorithm is as follows.
plan-hierarchies are processed in a top-down manner.

The monitored agents'

The detection component uses the

teamwork model to tag specic plans as team plans, explicitly representing joint activity by
the team (these plans are boxed in Figures 3, 5 and 4).

The team-plans in equal depths

of the hierarchies are used to create team-modeling hypotheses. For each hypothesis, the
plans of dierent agents are compared to detect disagreements. Any dierence found is an
indication of failure.

If no dierences are found, or if the comparison reaches individual

plans (non-team, therefore non-boxed in the gures) no failure is detected. Individual plans,
which may be chosen by an agent individually in service of team plans are not boxed in
these gures, and are handled using other relationships as discussed in Section 7
For instance, suppose the failing attacker from Example 1 is monitoring the other attacker. Figure 5 shows its view of its own hierarchical plan on the left. The path on the
right represents the state of the other attacker (who has landed). This state has been inferred in this example from observations made by the monitoring attacker (here, we are
assuming that the plan-recognition process has resulted in one correct hypothesis for each
agent. We will discuss more realistic settings below). In Figure 5, the dierence that would
be detected is marked by the arrow between the two plans at the second level from the top.

fly-flight-plan team-plan (on
wait-at-point team-plan (on the right). The

While the failing attacker is executing the

the left), the

other attacker is executing the

disagreement

on which team-plan is to be executed is a failure of teamwork.

Execute Mission

Execute Mission

Fly-Flight-Plan

Wait-at-Point

Fly-Route
Traveling
Low-Level

Just-Wait

Figure 5: Comparing two hierarchical plans. The top-most dierence is at level 2.

114

Robust Agent Teams via Socially-Attentive Monitoring

Detecting disagreements is dicult with multiple team-modeling hypotheses, since they
may imply contradictory results with respect to failure detection: Some hypotheses may imply that a failure had occurred in the team, while others may not. Unfortunately, this is to
be expected in realistic applications. For instance, Figure 4 (Section 3.2) shows several hypotheses that are possible based on the same observations. However, one of the hypotheses,
(W,W,W), implies no failure has occurredall the agents are in agreement on which team-plan

is executingwhile another hypothesis, (F,W,H), implies failures have occurred.

To limit reasoning to only a small number of team hypotheses, while not restricting
failure-detection capabilities, we use a disambiguation heuristic that ranks team-modeling
hypotheses by the level of

coherence

they represent. This heuristic is provided as an initial

solution. Later sections will examine additional heuristics.

Denition 1.

The

coherence

level of a multi-agent modeling hypothesis is dened as the

ratio of the number of agents modeled to the number of plans contained in the hypothesis.
This denition results in a partial ordering of the hypotheses set, from the least coherent
hypothesis (one that assigns each agent a dierent plan than its team-mates), to the most
coherent hypothesis (that assigns the same plan to all team members).

For instance, the

hypothesis (F,W,H) would have the lowest level of coherence, 1, since it implies complete

breakdown of teamworkevery agent is executing a dierent plan. The hypothesis (W,W,W)
would have a coherence level of 3, the highest level of coherence for the group of three agents,
since they are all assigned the same plan. Ranked between them would be the hypothesis
(W,W,H), with a single teamwork failure (disagreement on W and H) and a coherence level of
3/2.
The detection component selects a single maximally-coherent team-modeling hypothesis
(ties broken randomly).

The intuition for using coherence is that failures to agree occur

despite the agents' attempts at teamwork. Thus we expect more agreements than disagreements in the team.

The coherence level of a team-hypothesis is inversely related to the

number of teamwork failures implied by the hypothesis. Selecting a maximally-coherent hypothesis therefore corresponds to the minimum-number-of-failures heuristic commonly used
in diagnosis (Hamscher, Console, & de Kleer, 1992).
For the case depicted in Figure 4, the complete detection process may be conceptu-

1

alized as follows .

Suppose that one of the attackers, whose hierarchy is described in

Figure 4b, is monitoring the team.

First, it collects the plan hypotheses at the top of

the hierarchy for each agent (including itself ). In this case, they are {execute-mission},

{execute-mission}, {execute-mission}. Only one team-modeling hypothesis can be built
from these: (execute-mission,

execute-mission, execute-mission).

Since this hypoth-

esis shows no disagreement occurs at this level, the process continues to the second level.
Here, the hypotheses for the rst agent on the left are {F,W}, for the monitoring second agent
(since it knows its own state) there is only one possibility {W}, and for the third agent {W,H}.

As we saw above, the maximally team-coherent hypothesis is (W,W,W) which is selected. Since
it does not indicate failure, the process continues to the third level.

Here the agents are

executing individual plans, and so the comparison process stops. Algorithm 2 in Appendix
B provides greater details about this process.
1. Other implementations may make use of optimized algorithms in which the heuristics are integrated into
the agent-modeling algorithm.

115

Kaminka and Tambe

When sub-teams are introduced, a dierence between team-plans may be explained by
the agents in question being a part of dierent sub-teams.

Sub-team members still have

to agree between themselves on the joint sub-team plans, but these may dier from one
sub-team to the next. For now, let us assume that the teams under consideration are

teams, as dened in Denition 2.

simple

We make this denition in service of later analytical results

in which it will appear as a condition. We return to the issue of sub-teams in Section 7.1.

Denition 2.

We say that a team T is

simple,

if its plan-hierarchy involves no dierent

team plans which are to be executed by dierent sub-teams.
Intuitively, the idea is that in a simple team, all members of the team jointly execute
each of the team plans in the hierarchy. This denition is somewhat similar to the denition
of a

ground team

in (Kinny, Ljungberg, Rao, Sonenberg, Tidhar, & Werner, 1992), but it

does not allow sub-team members of a team to have a joint plan which is dierent than that
of other members.

3.4 Relationship Diagnosis
The diagnosis component constructs an explanation for the detected failure, identifying the
failure state and facilitating recovery.

The diagnosis is given in terms of a set of agent

belief dierences (inconsistencies) that explains the failure to maintain the relationship.
The starting point for this process is the detected failure (e.g., the dierence in team-plans).
The diagnosis process then compares the beliefs of the agents involved to produce a set of
inconsistent beliefs that explain the failure.
Two problems exist in practical applications of this procedure.

First, the monitoring

agent is not likely to have access to all of the beliefs held by the monitored agents, since it
is not feasible in practice to communicate all the agents' beliefs to each other. Second, each
agent in a real-world domain may have many beliefs, and many of them will vary among the
agents, though most of them will be irrelevant to the diagnosis. Thus relevant knowledge
may be simply not be accessible, or may be hidden in mountains of irrelevant facts.
To gain knowledge of the beliefs of monitored agents without relying on communications,
the diagnosis process uses a process of belief ascription. The agent-modeling component (using RESL in our implementation) maintains knowledge about the selection and termination
conditions of recognized plans (hypotheses). For each recognized plan hypothesis, the modeling component infers that any termination conditions for the plan are believed to be false
by the monitored agent (since it has not terminated the plan). We have also found it useful
to use an additional heuristic, and infer that the selection conditions (preconditions) for
any plan which

has just begun execution

are true. The idea is that when a plan is selected

for execution, its preconditions are likely to hold, at least for a short period of time. This
heuristic involves an explicit assumption on the part of our system that the new plan is
recognized as soon as it begins execution.

Designers in other domains will need to verify

that this assumption holds.
For each agent i, the inferred termination and selection conditions make up a set of beliefs
Bi for the agent. For instance, suppose an agent is hypothesized to have just switched from

executing

fly-flight-plan

to

wait-at-point.

RESL infers that the agent believes that

the way-point was just detected (a selection condition for

116

wait-at-point).

In addition,

Robust Agent Teams via Socially-Attentive Monitoring

RESL infers that the agent believes that an enemy was not seen, and that no order was

wait-at-point).

received from base to halt the mission (negated termination conditions of

To determine the facts that are relevant to the failure, the diagnosis component uses
the teamwork model. The teamwork model dictates which beliefs the agents hold must be
mutually believed by all the agents in the team.

Any dierence that is detected in those

beliefs is a certain failure, as the team members do not agree on issues on which agreement
is mandatory to participation in the team.

The teamwork model thus species that the

beliefs contained in the Bi sets should be mutual, and should therefore be consistent:

[

Bi 6`?

i

If an inconsistency is detected, the diagnosis procedure looks for contradictions (disagreements) that would cause the dierence in team-plan selection. A dierence in beliefs serves
as the diagnosis, allowing the monitoring agent to initiate a process of recovery, e.g., by
negotiating about the conicting beliefs (Kraus et al., 1998).
For example, as shown in Section 3.3, the two attackers in Example 1 (Section 2) dier
in their choice of a team-plan: One attacker is continuing execution of the

fly-flight-plan

plan, in which the helicopters y in formation. The other attacker has detected the waypoint, terminated

fly-flight-plan

wait-at-point,

and has switched to

landing immedi-

ately (Figure 5). When the failing attacker monitors its team-mate, it detects a dierence in
the team-plans (Section 3.3), and the detected dierence is passed to diagnosis. The failing
attacker makes the following inferences:
1.

Fly-flight-plan has three termination conditions:

(a) seeing the enemy, (b) detecting

the way-point, or (c) receiving an order to halt. The failing attacker (left hierarchy in
Figure 5) knows its own belief that none of these conditions hold, and thus
B1

2.

Wait-at-point

=

f:W ayP oint; :E nemy; :H altOrderg

has one selection condition:

the way-point has been detected.

Its

termination condition is that the scout has sent a message to join it, having identied
the enemy's position. The diagnosis component in this case therefore infers that for
the other attacker (right hierarchy in Figure 5)
B2

=

fW ayP oint; :S coutM essageReceivedg

Then,
B1 [ B2

which

is

=

f:W ayP oint; W ayP oint; :E nemy; :S coutM essageReceived; :H altOrderg

inconsistent.

The

inconsistency

(disagreement

between

the

attackers)

is

f:W ayP oint; W ayP ointg, i.e., contradictory beliefs about W aypoint. Thus now the failing

attacker knows that its team-mate has seen the way-point. It can choose to quietly adapt
this belief, thereby terminating its own

fly-flight-plan

117

and selecting

wait-at-point,

or

Kaminka and Tambe

it may choose other recovery actions, such as negotiating with the other attacker on whether
the way-point has been reached.
We have found these diagnosis procedures to be useful in many of the failures detected
by socially-attentive monitoring (see Section 4 for evaluation and discussion).
since this paper focuses on the monitoring selectivity problem in

However,

detection, we leave further

investigation of the diagnosis procedures to future work.

4. Monitoring Selectivity in Centralized Teamwork Monitoring
Using the socially-attentive framework of Section 3 we systematically examine all failure
permutations of Examples 1 and 2 (Section 2) under a centralized teamwork monitoring
conguration, where a single team-member is monitoring the team.

We vary the agents

failing (attacker, attacker and scout, etc.) and the role of the monitoring agent (attacker or
scout). We report on the empirical results of detecting and diagnosing failures in all cases.
Using these empirical results as a guide, we explore centralized teamwork monitoring analytically. We show that even under monitoring uncertainty, centralized teamwork monitoring
can provide either sound or complete detection results (but not both).
As a starting point for our exploration, the monitoring agent uses a single maximallycoherent team-modeling hypothesis as discussed in Section 3.3. We begin with Example 2.
The normal order of execution is

W,

the execution of
enemy's position.

wait-at-point (W),

followed by

join-scout (J).

During

the two attackers land and wait for the scout to visually identify the

Upon identication, the scout sends them a message to join it, which

triggers the selection of the

J

plan, and the termination of the

W

plan. When executing

J,

the scout hovers at low altitude, waiting for the attackers to join it. Any failures here are
on the part of the attackers (they cannot receive the message) or on the part of the scout (it
cannot send it). These failures arise, for instance, if the radio is broken or team-members
are out of range. When an agent fails, it continues to execute

W

instead of switching to

J.

Table 1 summarizes the permutations of Example 2. The permutation number appear
in the rst column.

The next three columns show the actual plans selected by the three

agents A1, A2 and A3 in each permutation.

The second-to-last column shows whether a

relationship failure has occurred in each case, i.e., whether disagreement exists between the
agents. Finally, the last column details the physical conditions in each case. There are ve
possible failure permutations: In case 1, none of the agents failed.
attacker failed.

In cases 2 and 3 one

In case 4 the scout failed to send a message or both attackers failed to

receive it. In case 5 the scout does not identify the enemy's position (so no message is sent,
and all three agents continue to execute the
since no attacker can switch to the

J

W

plan). Other permutations are not possible,

plan without the scout.

For instance, case 2 in Table 1 corresponds to Example 2. The scout (A3) has detected
the enemy, switched to plan

J,

and sent a message to the attackers to join it. One attacker

(A2) received the message, switched to plan

J, and began ying towards the scout.

However,

the remaining attacker (A1) failed to receive the message, and so it maintains its position,
continuing to execute

W and failing to switch to J. Since the agents are no longer in agreement

on which team plan should be jointly executed, a teamwork failure has occurred. Condition
monitors were used in the original failure case to monitor for the scout's message. However
failures in communications resulted in these monitoring conditions to be rendered useless.

118

Robust Agent Teams via Socially-Attentive Monitoring

Case

#

Actual Executing Plans

Relationship Failure

Physical

?

Attacker A1

Attacker A2

Scout A3

Occurred

Failure

1

J

J

J

-

-

2

W

J

J

+

A1 fails to receive

3

J

W

J

+

A2 fails to receive

4

W

W

J

+

A3's message lost

5

W

W

W

-

Enemy not identied

Table 1: All possible failure permutations of the broken radio-link scenario (Example 2).

One key issue is raised by case 5 in Table 1. Here, due to the scout's inability to identify
the enemy's position (perhaps due to failure on the scout's part, perhaps because the enemy
is simply not there), the three helicopter pilots remain in agreement that the enemy has not
been identied. Here, even though clearly the pilots are failing to make progress towards the
task goals (the scout continues to search for the enemy indenitely), no relationship failure
is taking place, since the agents are maintaining the teamwork relationship while failing to
make progress. This clearly demonstrates that not all failures are necessarily relationship
failures.
Table 2 presents the results of the scout monitoring its teammates in Example 2, using a
maximally team-coherent hypothesis as the basis for detection. The rst column again shows
the case number, for reference into Table 1. The next three columns show the scout's (A3's)
hypothesis about what plan each agent is executing according to the maximal coherence
heuristic. The next two columns show whether a failure was detected, and whether it was
diagnosed correctly. The last column shows the detection class (discussed below).

Case

#

A3's Hypothesized Executing Plans

Relationship Failure

Diagnosis

Detection

Detected?

Success?

Class

J

-

n/a

True Negative

J

+

+

True Positive

W

J

+

+

True Positive

H

J

+

-

True Positive

W

W

-

n/a

True Negative

Attacker A1

Attacker A2

Scout A3

1

J

J

2

W

J

3

J

4

H

5

W

Table 2: Scout's (A3) monitoring results in all permutations of Example 2.
For example, case 2 in Table 2 shows the results of the scout monitoring in the original
failure in Example 2 (Section 2). Using RESL, and selecting a maximally-coherent hypothesis, the scout hypothesizes that the non-moving attacker is executing
while the moving attacker is executing
its own selected plan

J

J (case 2, column 3).

W (case

2, column 2),

The scout of course knows that

(case 2, column 4). A violation of the teamwork relationship is thus

detected (case 2, column 5), since A1's

W

is not in agreement with the rest of the team's

J.

Furthermore, the diagnosis was successful in identifying the cause for the failure, i.e., the
fact that the enemy's position has been identied by the scout, but no knowledge of this
was passed on to the failing attacker (case 2, column 6).

119

Kaminka and Tambe

The last column of Table 2 shows the detection class of each failure. The detection class
of a case can be one of: true positive, true negative, false positive, and false negative. These
correspond to the following possible monitoring outcomes: A true positive is an outcome
where a relationship failure has actually occurred, and has been detected. A true negative
is where no failure has occurred, and the system correctly reports none is being detected.
A false positive is where no failure has occurred, but the system nevertheless incorrectly
detects one, and a false negative is where a failure has occurred, but the system fails to
detect it. Table 2 shows that in all permutations of Example 2 the teamwork monitoring
techniques did not encounter the problematic false positive or false negative cases.
A closer look at these results hints at a key contribution of this paper in addressing the
monitoring selectivity problem: Eective failure detection can take place despite the use of
uncertain, limited, knowledge about monitored agents. In case 4 of Table 2, the monitoring agent was able to detect the failure

involved.

despite being wrong about the state of the agents

The scout believes that the two attackers are executing the

W. H

plan, but they are actually executing,

H (ordered-halt)

is selected when a command is received from

headquarters to halt execution and hover in place.

From the scout's perspective, a hov-

ering attacker can therefore be inferred to be executing

H

or

W.

Thus two equally-ranked

maximally-coherent hypotheses exist: the two attackers are either both executing
executing

H.

W or

both

A random selection was made, and in this case resulted in the wrong hypothe-

sis being selected. Nevertheless, a violation of the teamwork relationships was detected, as
neither

H

or

W

agrees with the scout's

J.

However, as the last column of case 4 shows (in Table 2), the diagnosis procedures are
sensitive to the selection of the team-modeling hypothesis. The hypothesis used in this case
does not correctly reect the true state of the agents, and so despite the scout's success to
detect a failure in this case, the diagnosis procedures fail to provide correct diagnosis (the
diagnosis was successful in the two other failure cases). This phenomenon repeats in other
empirical results we provide below:

diagnosis failed whenever the hypothesis chosen was

incorrect, although it was sucient for detection.

We intend to explore ways to improve

the accuracy of the hypotheses in future work, and therefore will not address this issue
in this paper anymore. The failure detection capabilities are a signicant improvement in
themselves, since the agents know with certainty that a failure has occurred, even if their
diagnosis of it is incorrect.
Many social and physical failures are successfully captured using the team-coherence
heuristic for monitoring selectivity. In fact, in our permutations of Example 2, no matter
which one of the agents was the monitor, all failures to maintain relationship (i.e., all physical
failures except for one in which the team remains in agreement) were detected reliably,
although sometimes diagnosis failed.

This result is especially surprising considering that

only a single agent did the monitoring. Previous monitoring methods (condition monitors
and communications) were unable to detect the failures, despite being used by all three
agents .
Tables 3 and 4 present further empirical results, on the basis of Example 1.

Table 3

presents all failure permutations of Example 1 in the same format as Table 1. The normal or-

fly-flight-plan
wait-at-point (W) plan,

der of execution of these plans is as follows: All agents jointly execute the
(F) plan until they detect the way-point. They then switch to the

in which the two attackers land while the scout continues to y ahead to identify the enemy.

120

Robust Agent Teams via Socially-Attentive Monitoring

The failures here are all on the part of the agents to detect the way-point, and thus to switch
to the

W

plan.

Case

#

Actual Executing Plans

Relationship Failure

Physical

?

Attacker A1

Attacker A2

Scout A3

Occurred

Failure

1

W

W

W

-

-

2

F

W

W

+

A1 vision fails

3

W

F

W

+

A2 vision fails

4

F

F

W

+

A1, A2 vision fails

5

W

W

F

+

A3 vision fails

6

F

W

F

+

A1, A3 vision fails

7

W

F

F

+

A2, A3 vision fails

8

F

F

F

-

A1,2,3 vision fails

Table 3: All failure permutations of the undetected way-point scenario (Example 1).

Case

#

A1's Hypothesized Executing Plans

Relationship Failure

Detection

Attacker A1

Attacker A2

Scout A3

Detected?

Class

W

W

W

-

True Negative

2

F

W

F

+

True Positive

3

W

F

W

+

-

True Positive

False Negative
False Negative

1

4
5

F
W

F
W

F
W

6

F

W

F

+

True Positive

7

W

F

F

+

True Positive

8

F

F

F

-

True Negative

Table 4: Attacker's (A1) monitoring results in all permutations of Example 1.

Table 4 present the monitoring results for all permutations of Example 1.

Here the

attacker A1 is monitoring the team using again a maximally team-coherent hypothesis in
detecting failures. The results show that A1 is successful in detecting all teamwork failures
but two (cases 4-5, highlighted in bold face).
These two false outcomes are both false negatives. In both these cases, the monitoring
attacker A1 picked an incorrect hypothesis for the scout, since the scout's actions lead to
ambiguous interpretations. The scout is to y forward (to scout the enemy) if it detected the
way-point (plan

W), but also if it did not (then it would be ying in formationplan F). The

use of the maximal team-coherence heuristic causes A1 to prefer a hypothesis in which the
scout is in agreement with the attackers when in fact it is not. For example, in case 4, the two
attackers have failed to detect the way-point and are executing

F. Observing the scout, the
F or W. However, believing

monitoring attacker A1 is not sure whether the scout is executing
that the scout is executing

F results in a maximally-coherent

team-modeling hypothesis (all

the agents are in agreement), while believing that the scout is executing

121

W

results in a less

Kaminka and Tambe

coherent hypothesis. Thus A1 selects a wrong hypothesis, which in this case fails to detect
the teamwork failure.
The maximal team-coherence heuristic can detect failures despite using incorrect hypotheses. Unfortunately, such hypotheses can also lead to false-negatives as we have seen in
Table 4. However, none of our experiments resulted in a false-positive result, i.e., a result
in which the system detected a failure but in reality none had occurred. Thus the heuristic
provided sound results in these cases. We are able to formally prove this property holds in
general when the maximal team-coherence heuristic is used.
First, we address a matter of notation. Let an agent A monitor an agent B , which is
executing some plan P .

We denote by M (A; B =P ) the set of agent-modeling hypotheses

that A's agent-modeling component constructs based on B 's observable behavior during the
execution of P . In other words, M (A; B =P ) is A's set of all plans that match B 's observable
behavior.

Note that when A monitors itself, it has direct access to its own state and so

M (A; A=P )

=fP g. Using the modeling notation, we make the following denitions which

ground our assumptions about the underlying knowledge used in monitoring:

Denition 3.

agent-modeling

Given a monitoring agent A, and a monitored agent B , we say that A's
of agent B is

complete

if for any plan P that may be executed by B, P 2

M (A; B=P ).

The set M (A; B=P ) will typically include other matching hypotheses besides the correct
hypothesis P, but is guaranteed to include P. Following this denition of

individual

agent-

modeling completeness, we can dene group-wide team-modeling completeness:

Denition 4.
A's

Let A be an agent monitoring a team T of agents B1 ;    ; Bn . We say that

team-modeling

of the team T is

complete

if A's agent-modeling of each of B1 ;    ; Bn is

complete.
Denition 4 is critical to guarantee the capabilities we will explore analytically in this
section and the next. It generally holds in our use of RESL in the ModSAF and RoboCup
domains, and we make it explicit here in service of applications of the techniques in other
domains.
Armed with these denitions, we now formalize the failure detection capabilities suggested by the empirical evidence in Theorem 1.

Theorem 1. Let a monitoring agent

A monitor a simple team T . If A's team-modeling
of T is complete, and A uses a maximally team-coherent hypothesis for detection, then the
teamwork failure detection results are sound.

Proof.

We will show that if no failure has occurred, none will be detected, and thus that any

failure that is detected is in fact a failure. Let a1 ; : : : ; an be the agent members of T . Each
agent ai is executing some plan Pi (1  i  n). Thus collectively, the group is executing

(P1 ; : : : ; Pn ). If no failure has occurred, then all the agents are executing the same plan

P0 ,

i.e., 8i; Pi = P0 . Since A's team-modeling is complete, the correct hypothesis (P0 ; : : : ; P0 )
is going to be in the set of team-modeling hypotheses H .

Since it is a maximally team-

coherent hypothesis, either it will be selected, or that a dierent hypothesis

coherence level

of the same

will be selected. Any hypothesis with the same coherence level as the correct

one implies no failure is detected. Thus the detection procedure is sound.

122

Robust Agent Teams via Socially-Attentive Monitoring

Despite uncertainty in the knowledge used, sound failure-detection can be guaranteed using
the maximal team-coherence heuristic.

This is one answer to the monitoring selectivity

problem. However, as we have seen in Table 4, some failures may pass undetected using this
heuristic (i.e., it may result in false-negatives).
may therefore unfortunately be

complete guaranteed

incomplete.

Detection using maximal team-coherence

We may prefer our monitoring system to be

to detect all teamwork failures.

incoherence

We therefore experimented with the maximal team-

heuristic, the inverse of

the maximal team-coherence heuristic. This heuristic prefers hypotheses that suggest
failures, rather than less.

more

Table 5 gives the monitoring attacker A1's view of the team,

incoherent

similar to Table 4, but using a maximally team-

hypothesis. It shows that indeed

using a maximally team-incoherent hypothesis will not lead to the false-negative detections
in cases 4 and 5 (in contrast to these cases in Table 4).
Case

Relationship Failure

Detection

Attacker A1

Scout A3

Detected?

+

Class

1

W

Attacker A2

False Positive

2

F

H

W

+

True Positive

3

W

F

F

+

True Positive

F

F

W

+

True Positive

#

A1's Hypothesized Executing Plans

4
5

H

F

W

H

F

+

True Positive

6

F

H

W

+

True Positive

7

W

8

F

F

F

F

+

W

+

True Positive

False Positive

Table 5: Attacker's (A1) monitoring results in all permutations of Example 1, using team-

incoherence.

Guided by these results, we formally show that the team-incoherence heuristic leads to
a detection procedure that is

complete.

Theorem 2. Let a monitoring agent A monitor a simple team T . If the A's team-modeling
of T is complete, and A uses a maximally team-incoherent hypothesis for detection, then the
teamwork failure detection results are complete.
Proof. Analogous to that of Theorem 1, the proof is provided in appendix A.
However, these successes are oset by false positive outcomes in cases 1 and 8 of Table 5. In
these cases, no failures have occurred, but the monitoring system falsely reported detected
failures. In practice, this may lead to costly processing of many false alarms.
Ideally, the detection capabilities should be sound

and

complete. Unfortunately, we can

show that no coherence-based disambiguation scheme exists that results in both sound and
complete detection. We show in Theorem 3 that to provide sound and complete detection, a
disambiguation method will have to be inconsistent: Given the same set of possible matching
hypotheses, it will have to sometimes rank one hypothesis on top, and sometimes another.

Theorem 3. Let

H be a complete team-modeling hypotheses set, modeling a simple team.
There does not exists a disambiguation scheme S that (1) uses coherence alone as the basis

123

Kaminka and Tambe

for disambiguation of H , and (2) is deterministic in its selection, and (3) results in sound
and complete failure detection.
Proof.

Let S be a disambiguation scheme that leads to complete and sound detection and

uses only knowledge of the coherence of the hypotheses in selecting a disambiguated hypothesis. Suppose for contradiction that it is deterministic, and thus consistent, in its selection
of an hypothesis out of H , i.e., given H , a set of candidate hypotheses, it applies some
deterministic procedure to choose one hypothesis based on its coherence.

Since it does

not use any other knowledge outside of the coherence of the candidate hypotheses, given
the same set of candidates, it will always choose the same hypothesis.

Let Am be the

monitoring agent using S . Let B be a monitored agent, whose actions are identical when
executing team plans P1 ; P2 .
P2 , M (Am ; B =P1 )

=

Thus, Am cannot determine whether B is executing P1 or

M (Am ; B =P2 )

=

fP1 ; P2 g. If Am and B are both executing P1 , Am 's

H

=

f(P1 ; P1 );

hypotheses set is

(P1 ; P2 )g

Since S leads to complete and sound detection, it will choose (P1 ; P1 ). Now, when Am and
B are executing P1 and P2 , respectively, the matching hypothesis set is again H as dened

above. But now S must select (P1 ; P2 ). Since the same set of candidate hypothesis H was
used in each case, and no other information was supplied, S must be non-deterministic in
its selection of a disambiguated hypothesis, contradicting the assumption.
The empirical and analytical results show that our use of a single disambiguated hypothesis leads to improved, but imperfect, failure-detection results, compared to the monitoring
conditions and communications previously used. The empirical results in Tables 2, 4, and
5 establish the benets of the teamwork monitoring technique: Most physical failures were
detected. However, the analytical results (Theorems 1, 2, 3) show that the results are less
than perfect. The algorithms are either sound or complete, but not both. For complete monitoring, we would require additional procedures that can dierentiate the true positives from
the false ones, e.g., by focused communication. These procedures are often very expensive.
We can reduce the need for costly verication by letting go of our insistence on a single
hypothesis, focusing instead of maintaining two hypotheses: a maximally-coherent hypothesis and a maximally-incoherent hypothesis.

Table 6 shows a portion of the full set of

team-hypotheses available when the attacker A1 is monitoring the team. The total number
of hypotheses presented in the table is 24, with as many as 4 co-existing in a single case,
and thus maintaining a full set of hypotheses would be expensive. However, the two inverse
heuristicsteam-coherence and incoherencerepresent two extremes of the space of these
hypotheses.

If they agree that a failure exists, then a failure actually occurred, since the

team-coherent hypothesis guarantees soundness (Theorem 1). If they agree that no failure
exists, then no failure took place, since the team-incoherent hypothesis guarantees completeness (Theorem 2). If they disagree (i.e., the team-coherent hypothesis does not imply
a failure, but the team-incoherent hypothesis does), the monitoring system cannot be sure
either way, and must revert back to verication.
This revised detection algorithm oers signicant computational savings compared to the
single team-incoherent hypothesis approach. It is complete and unsound, but signicantly

124

Robust Agent Teams via Socially-Attentive Monitoring

Case

#
1

2

3
4
5

6

7
8

A1's Hypothesized Executing Plans

Relationship Failure

Detection

Scout A3

Detected?

Class

H

F

+

False Positive

H

W

+

False Positive

W

W

F

+

False Positive

W

W

W

-

True Negative

F

H

F

+

True Positive

F

H

W

+

True Positive

F

W

F

+

True Positive

F

W

W

+

True Positive

W

F

F

+

True Positive

W

F

W

+

True Positive

F

F

W

+

True Positive

F

F

F

-

False Negative

W

H

F

+

True Positive

W

H

W

+

True Positive

W

W

F

+

True Positive

W

W

W

-

False Negative

F

H

W

+

True Positive

Attacker A1

Attacker A2

W
W

F

H

F

+

True Positive

F

W

W

+

True Positive

F

W

F

+

True Positive

W

F

F

+

True Positive

W

F

W

+

True Positive

F

F

W

+

False Positive

F

F

F

-

True Negative

Table 6: A portion of the attacker's (A1) monitoring hypotheses and implied results when
no ranking is used to select a single hypothesis for each case.

reduces the need for verication, since at least when the team-coherent hypothesis implies
failures, verication is not necessary. It requires representing only two hypotheses, and is
thus still computationally cheaper than maintaining an exponential number of hypotheses.
For example, using a maximally team-incoherent hypothesis on permutations of Example
1 results in a need to verify in all eight cases as we have seen (5). However, when we combine
such an hypothesis with a maximally team-coherent hypothesis (e.g., as in Table 4), we only
need to verify four (50% ) of the cases. In cases 2, 3, 6, 7 there is agreement between the
two hypotheses that a failure has occurred, and thus no verication is required.
A monitoring agent can therefore address the monitoring selectivity problem by balancing
its resource usage against the guaranteed performance of the monitoring algorithm used.
Either of the simpler single-hypothesis algorithms would utilize only one hypothesis in each
case, with detection capabilities that are guaranteed to be sound or complete, but not both.
In the more complex algorithm, two hypotheses would be reasoned about in each case, and

125

Kaminka and Tambe

the algorithm would be complete and require verication in fewer cases compared to the
simple-hypothesis complete algorithm.

5. Monitoring Selectivity in Distributed Teamwork Monitoring
This section focuses on monitoring selectivity when exploiting a key opportunity for execution monitoring in multi-agent environmentsit is not only the monitored agents that
are distributed, but the monitoring agents can be distributed as well. We begin with the
simple scheme of selecting a single maximally team-coherent hypothesis. Since centralized
teamwork monitoring was successful in addressing all permutations of Example 2, we focus
here on the permutations of Example 1 (Table 3), in which centralized teamwork monitoring
by the attacker resulted in false-negative detections (cases 4-5 in Table 4).
In a distributed teamwork monitoring scheme, not only will a single attacker monitor
its teammates, but the scout (and the other attacker) will also engage in monitoring. Table
7 presents the monitoring results of the same failure permutations, with the scout as the
monitoring agent.

We nd that the scout successfully detects the two failure cases that

the attacker failed to detect, compensating for the attackers' monitoring mistakes. Furthermore, since the scout used the the maximal-coherence heuristic, detection is sound and no
verication is required. The reason for the scout's success is that the attackers' actions in
this case, although ambiguous, do not support any hypothesis that can be matched to the
scout's plan. In other words, regardless of what plan the attackers are executing in these
two cases, it is dierent that the plan executed by the scout.
Case

Relationship Failure

Detection

Detected?

Class

W

-

True Negative

F

+

True Positive

F

W

+

True Positive

F

W

+

True Positive

H

H

F

+

True Positive

F

H

F

+

True Positive

7

H

F

F

+

True Positive

8

F

F

F

-

True Negative

#

A3's Hypothesized Executing Plans
Attacker A1

Attacker A2

Scout A3

1

W

W

2

F

W

3

W

4

F

5
6

Table 7: Scout's (A3) monitoring results in all permutations of Example 1, using teamcoherence.

Thus if all agents engaged in monitoring in permutations of Example 1, detection would
be sound and complete. In all actual failure cases (and only in those) there would at least one
team-member who detects the failure. We attempt to formally dene the general conditions
under which this phenomenon holds.

Denition 5.

We say that two team-plans P1 ; P2 , have

observably-dierent roles

R1 ; R2 if

given an agent B who fullls the roles R1 ; R2 in the two plans, resp., any monitoring agent
A (dierent than B ) will have M (A; B =P1 ) \ M (A; B =P2 )

observably-dierent roles in P1 and P2 , and call B a

126

=

key agent.

;.

We then say that B has

Robust Agent Teams via Socially-Attentive Monitoring

Intuitively, B is a key agent that has observably dierent roles in the two plans if a
monitoring agent can dierentiate between B 's behavior in executing P1 and in executing
P2 . For instance, both attackers have observably dierent roles in

W

(in which they land).

F

(in which they y) and

However, they do not have observably dierent roles in

both of which require them to land. The scout has observably dierent roles in
and

H

W and H,
W (ying)

(landing).

The key-agent is the basis for the conditions under which a self-monitoring team will
detect a failure with each agent using only team-coherence. We rst prove a lemma on the
conditions in which a single given agent will detect a failure. We then use this lemma to
prove the conditions under which at least one agent in a given team will detect a failure.

Lemma 1. Suppose a simple team

T is self-monitoring (all members of the team monitor
each other) using the maximally team-coherent heuristic (and under the assumption that for
each agent, team-modeling is complete). Let A1 ; A2 be monitoring agents who are members of
T and are executing P1 ; P2 , respectively. A1 would detect a failure in maintaining teamwork
relationships with an agent A2 , if A2 is a key-agent in P1 ; P2 .

Proof.

See appendix A.

A1 knows that it is executing P1 .

If A2 is executing P2 , and is a key-agent in P1 and P2 ,

then A1 is guaranteed to notice that a dierence exists between itself and A2 , since A2 is
acting observably dierent than it would if it had been executing P1 . Note, however, that
A2 may or may not detect this dierence, since from A2 's perspective, A1 's behavior may

or may not be explained by P2 . A2 will detect a dierence only if A1 's roles in P1 and P2
are also observably-dierent. However, since A1 has detected the failure, it can now alert
its teammates, diagnose the failure, or choose corrective action.
If we want to guarantee that a teamwork failure will always be detected by at least
one agent, we must make sure that in each possible combination of plans, there has to be
at least one key-agent whose roles are observably dierent.

The lemma shows that other

agents monitoring this agent will notice a failure if one occurs. To this aim, we dene an
observably-partitioned set of plans employed by a team.

Denition 6.

A set P of team-plans is said to be

observably-partitioned

if for any two plans

Pi ; Pj 2 P there exists a key-agent Aij . The set of these Aij agents is called the

set of P .

key agents

For instance, the set of team-plans our helicopter pilots team has been using in the
examples (Fly-Flight-Plan (F),

Wait-at-Point (W), Ordered-Halt (H), and Join-Scout
(J)) is observably-partitioned. The attackers land in W and H, but y in F and J. The scout
lands in J and H, but ies in W and F. Table 8 shows which agents have observably dierent
roles in any two plans in the set. For instance, by nding the cell at the intersection of the

H

row and the

W

column, we nd that the scout has observably dierent roles in these two

plans. Indeed, the scout lands when a command is received to halt execution (H), but ies
out to scout the enemy's position when executing

W.

Here, since all agents have observably-

dierent roles in at least two plans, the key agents set of {
of the teamattackers and scout.

127

W , F , H, J }

includes all members

Kaminka and Tambe

Fly-Flight-Plan (F)

Wait-at-Point (W)

Ordered-Halt (H)

-

Attackers

Attackers

Scout

Attackers

-

Scout

Scout and Attackers

F
W
H
J

Join-Scout (J)

Attackers

Scout

-

Attackers

Scout

Scout and Attackers

Attackers

-

Table 8: Observable partitioning of the helicopter pilot team in ModSAF.

Theorem 4. If a simple team (1) employs an observably-partitioned set of team-plans O,

and all team-members monitor members of the key agents set of O, (2) using complete teammodeling and (3) maximally team-coherent hypotheses, then the teamwork failure detection
results are sound and complete.
Proof.

From theorem 1 we know that detection would be sound. To show that it is complete,

we will prove at least one agent will detect a dierence between itself and others whenever
team-members are not all executing the same plan (i.e., a failure is occurring). Suppose the
team is currently divided on the team-plans that must be executed, i.e., there are agents
ai ; aj in the team that are executing team plans Pi ; Pj , respectively, such that Pi 6= Pj .

Thus a failure has occurred. Let K be the key agents set of O . Since the team is observablypartitioned, for Pi ; Pj there exists at least one key agent a1 2 K . There are three cases:

case (i).

a1 is executing Pi . In this case any agent executing Pj would detect a dierence

with a1 and would therefore detect the failure (lemma 1).

case (ii).

a1 is executing Pj . In this case any agent executing Pi would detect a dierence

with a1 and would therefore detect the failure (lemma 1).

case (iii).

a1 is executing some other plan Q. Its roles must be observably dierent in Q

and Pi , or in Q and Pj (or in both), and thus any agent executing Pj and/or Pi would detect
the failure. The case that a1 's roles are not observably dierent in Q and Pi

and

in Q and

Pj is impossible, since then for a monitoring agent Am
M (Am ; a1 =Pi ) \ M (Am ; a1 =P j )  fQg 6= ;

Contradicting a1 being a key agent for Pi ; Pj .
Since in all three cases, at least one agent would detect a failure where one occurred.
Therefore, failure detection is complete. Since it is also sound as we have seen, detection is
sound and complete.
The theorem shows that distributed teamwork monitoring can result in

sound and complete
only the key

failure-detection, while using a simple algorithm. Each team-member monitors

agents 2 , using a maximally team-coherent

hypothesis. If it detects a failure, then certainly

one has occurred. If no agent detects a failure, then indeed no failure has occurred.
This simple distributed algorithm, with its attention-focusing features and guaranteed
soundness and completeness contrasts with the more complex centralized algorithm which
we discussed in the previous section (Section 4). The algorithm's eectiveness relies on the
2. If the monitoring team-member does not know who the key agents are, but knows they exist, it can
monitor all other team-members. This increases monitoring, but sound and complete failure detection
is still guaranteed.

128

Robust Agent Teams via Socially-Attentive Monitoring

condition of an observably-partitioned set of plans, and on the distribution of the monitoring.
A corollary of Theorems 3 and 4 is that if key agents are not available in the distributed
case, failure detection is either sound or complete, but not both. And even when key agents
are available, centralized teamwork monitoring is still not complete and sound.
Fortunately, observable-partitioning is not a dicult property to design: Teams are very
often composed such that not all agents have the same role in the same plan, and in general,
roles do have observable dierences between them. For instance, our helicopter pilot team
in the ModSAF domain typically executes a set of plans with this property, as Table 8
demonstrates.
If the team, however, is not observably-partitioned, there may be a case where two agents
are each executing a dierent plan, but no agent will be able to detect it using the teamcoherence heuristic. The minimal case where this occurs is when two agents, A1 and A2 are
executing plans P1 and P2 , respectively, and P1 and P2 are not observably dierent, such
that
M (A2 ; A1 =P1 ) \ M (A1 ; A2 =P 2)

=

fP1 ; P2 g

This will result in A1 and A2 each believing that the other is in agreement with them. A
check for such a situation can be made a part of the plan design process, marking

points

risky

in the execution in which detection is either sound or complete (Theorem 3), and

verication (e.g., by communications) can be prescribed pro-actively. Or, the check could
be inserted into the protocol for run-time analysisthe agent would simulate the other's
hypotheses matching their own actions, and detect risky points dynamically.

6. Using Socially-Attentive Monitoring in an O-Line Conguration
To further demonstrate the generality of our socially-attentive monitoring framework, this
section examines re-use of teamwork monitoring in domains in which diagnosis and recovery
from every failure are infeasible during execution. Examples of such domains include team
sports, military human team training (Volpe, Cannon-Bowers, & Salas, 1996), and other
multi-agent domains.

The dynamic nature of the domain, hard real-time deadlines, and

complexity of the agents involved (e.g., human team members) make diagnosis and recovery
dicult.

Even if a failure can be diagnosed, it is often too late for eective recovery.

In

such environments, the monitoring agent is often concerned with trends of performance.
This information is important for long-term design evaluation and analysis, and need not
necessarily be calculated on-line. The results of the analysis are meant as feedback to the
agents' designer (coach or supervisor, for humans).
To this end, we are developing an o-line socially-attentive monitoring system called
Teamore (TEAmwork MOnitoring REview). Teamore currently uses execution traces of

the monitored agents to perform the monitoring, rather than using plan recognition. Thus it
does not need to worry about the uncertainty in plan-recognition, nor about real-time performance. Instead, it knows with certainty each agent's plans during execution. Teamore
accumulates several quantitative measures related to teamwork, including the Average-Timeto-Agreement measure (ATA, for short), and a measure of the level of agreement in a team.
These build on the failure detection algorithm, but aggregate failures in quantitatively. We
focus here on the ATA measure.

129

Kaminka and Tambe

Teamore denes a

switch

as the time interval beginning at the point where any team-

member (at least one) selects a new team plan for execution by the team, and ending
at the point where the team is again in agreement on the team-plan being executed.

In

perfect teamwork, all team-members select a new team-plan jointly, and so always remain
in agreement.

In more realistic scenario, some agents will take longer to switch, and so

initially a teamwork failure will occur. The rst team-member to select a new plan will be
in disagreement with some of its teammates, until either it rejoins them in executing the
original plan, or they join it in selecting the new plan. Such a switch begins with a detected
failure and ends when no more failures are detected.
Figure 6 shows an illustration of a switch. The three agents begin in an initial state of
agreement on joint execution of Plan 1 (lled line). Agent 1 is the rst agent to switch to
Plan 2 (dotted line), and is followed by Agent 3, and nally Agent 2.

The switch is the

interval which begins at the instance Agents 1 selected Plan 2, to the time all three agents
regained their agreement (but this time on Plan 2).

A Switch
Legend:
Plan 1

Agent 3

Plan 2
Agent 2
Agent 1
Time
Figure 6: An illustration of a switch. The three agents switch from plan 1 to plan 2.
Teamore keeps track of the lengths of time in which failures are detected until they

are resolved. The ATA measure is the average switch length (in time ticks) per a complete
team run (e.g., a mission in ModSAF, a game in RoboCup). A perfect team would have
all switches of length zero, and therefore an ATA of 0. The worst team would be one that
from the very beginning of their task execution to the very end of it, would not agree on
the team plan being executed. For instance, each RoboCup game lasts for 6000 ticks. The
worst possible team would have only one switch during the game, of length 6000. Thus the
ATA scale in RoboCup goes from 0 (perfect) to 6000 (worst).
We used the ATA measure to analyze a series of games of our two RoboCup simulationleague teams, ISIS'97 and ISIS'98 (Marsella et al., 1999) against a xed opponent, Andhill'97
(Andou, 1998).

In these games, we varied the use of communications by our teams to

evaluate design decisions on the use of communications. In approximately half of the games,
players were allowed to use communications in service of teamwork. In the other half, all
communications between agents were disabled. ISIS'97 played approximately 15 games in
each settings, and ISIS'98 played 30 games in each communication settings.
Table 9 shows the mean ATA values over these games, for two sub-teams (each having
three members) of ISIS'97 and ISIS'98 (ATA values are calculated separately for each subteam). The rst column shows which sub-team the results refer to in each row. The second

130

Robust Agent Teams via Socially-Attentive Monitoring

columns shows the mean ATA for each sub-team, when no communications were used. The
third column shows the mean ATA when communications were used. The next column shows
the size of the ATA reductionthe drop in the mean ATA values when communications are
introduced.

The last column shows the probability of the null hypothesis in a two-tailed

t-test of the dierence in the ATA means. This is the probability that the dierence is due
to chance, thus smaller numbers indicate greater signicance.

ISIS

Mean ATA

Mean ATA

ATA

t-test prob.

sub-team

No comm.

Comm.

Reduction

null-hypothesis

32.80

5.79

27.01

7.13e-13

'97 Goalies
'97 Defenders

57.5

6.81

50.69

.45e-10

'98 Goalies

13.28

3.65

9.63

9.26e-16

'98 Defenders

12.99

3.98

9.01

7.13e-5

Table 9: Average-Time-to-Agreement (ATA) for games against Andhill'97.

Clearly, a very signicant dierence emerges between the communicating and noncommunicating versions of each sub-team.

The ATA values indicate that sharing infor-

mation by way of communications signicantly decreases the time it takes team-members
to come to agreement on a selected plan. This result agrees with our intuitions about the
role of communications, and in that sense, may not be surprising.
However, the ATA reduction magnitudes indicate that ISIS'98 may be much less sensitive to loss of communications than ISIS'97. The dierences in ATA values for ISIS'97 are
approximately triple, nearly four times, as great as for ISIS'98.

Our explanation for this

phenomenon is that ISIS'98 is composed of players with improved capabilities for monitoring the environment (such that they have better knowledge of the environment).

ISIS'98

is therefore not as dependent on communications as are teams, such as ISIS'97, composed
of players with lesser environment monitoring capabilities. ISIS'98 players are better able
to select the correct plan without relying on their teammates.

Thus, they would be able

to maintain the same level of performance when communications are not used. In contrast,
ISIS'97 players rely on passing information to and from each other (monitoring each other)
through communications, and so took much longer to establish agreement when communications were not available.
We can validate the hypothesis suggested by ATA measurements by looking at the overall
team-performance in the games, measured by the score dierence at the end of the game.
Table 10 shows the mean score dierence from the same series of games against Andhill'97.
The rst column lists the communications settings (with or without).
third columns show the

mean

The second and

score-dierence in the games for ISIS'97 and ISIS'98.

The

bottom row summarizes the results of t-tests run on each set of games, to determine the
signicance level of the dierence between the mean score-dierences. The score-dierence
results corroborate the ATA results. While the dierence in mean score-dierence is indeed
statistically signicant in ISIS'97 games, it is not signicant in ISIS'98 games. This supports
our explanation that the more situationally aware ISIS'98 is indeed better able to handle
loss of communications than ISIS'97.

131

Kaminka and Tambe

ISIS'97

ISIS'98

Communication Used

-3.38

-1.53

Communication Not Used

-4.36

-2.13

t-test p/null hypothesis

p=0.032

p=0.13

Table 10: ISIS'97 and ISIS'98 mean score dierence against Andhill'97, with changing communications settings

The general lesson emerging from these experiments is that a trade-o exists in addressing the monitoring selectivity problem. The knowledge that is maintained about teammates
(here, via communications) can be traded, to an extent, with knowledge maintained about
the environment.

A designer therefore has a range of alternative capabilities that it can

choose for its agents. Dierent domains may better facilitate implicit coordination by monitoring the environment, while others require agents to rely on communications or explicit
knowledge of team-members to handle the coordination.
The ATA results support additional conclusions, especially when combined with a general
performance measure such as the score-dierence.

To illustrate, consider the plots of the

actual data from these games. Figure 7 plots all the ATA values for all four variants, for
the Goalies sub-team.

The graph plots approximately 60 data-points.

We see in Figure

7 that when communications are used, ISIS'97's ATA values are still generally better than
ISIS'98's ATA without communications. Thus, despite its importance, individual situational
awareness is not able to fully compensate for lack of communications.

Average Time to Agreement (ATA)

90
80
70
60
50

40
30
20
10
0
ISIS98/Comm.

ISIS97/Comm.

ISIS98/No-Comm. ISIS97/No-Comm.

Goalies Sub-Team
(a) ATA Values for Goalies subteam

Figure 7: ATA values for the Goalies sub-teams in games against Andhill'97.

132

Robust Agent Teams via Socially-Attentive Monitoring

Teamore demonstrates the reuse of the teamwork monitoring techniques developed in

earlier sections in an o-line conguration. The designer of ISIS'97 should set its agents to
use communications, since those will have signicant improvement on the score-dierence.
In contrast, with or without communications, ISIS'98 players are able to maintain their
collaboration. Thus if communications takes precious resources, it can be relatively safely
eliminated from the ISIS'98 agents' design, and the development eorts can be directed at
some other components of the agents.

7. Beyond Teamwork
We have presented a general socially-attentive monitoring framework to detect failures in
maintaining agreement on joint team plans.

However, eective operation in teams often

relies on additional relationships, which we briey address in this section.

7.1 A Richer Agreement Model: Agreeing to Disagree
The teamwork model requires joint execution of team plans. In service of such agreed-upon
joint plans, agents may sometimes agree to execute dierent sub-plans individually, or split
into sub-teams to execute dierent sub-team plans. Two examples may serve to illustrate.

Example 3.

In the ModSAF domain, helicopters engage the enemy by repeatedly following

masking ), then popping up (unmask-

the following three steps: hiding behind a hill or trees (

ing ),

then shooting missiles at the enemy, and back to hiding. In some variations of this

plan, they are required to make sure that no two helicopters are shooting at the same time.
Of course, due to limits of communications, helicopters do fail and unmask at the same time.

Example 4.

In the RoboCup domain, our 11 players in both ISIS'97 and ISIS'98 (Marsella

et al., 1999) are divided into four sub-teams: mid-elders, attackers, defenders, and goalies
(the goalie and two close defenders). This division into sub-teams is modeled by the agents

play team plan (see Figure 8). Mid-elders
must select the midfield plan, goalies must select the defend-goal plan, etc. Again, ideally
an attacker would never select any other plan but attack, a defender would select no other
plan but defend, etc. However, due to communication failures, players may sometimes
selecting one of four team plans in service of the

accidently abandon their intended sub-team, and execute a team-plan of another sub-team.
[Win−Game]
[Play]

[Attack]
[Simple
Advance]
.....
Score−goal

[Flank
Attack]
...
Pass

[Interrupt]
...

[Defend]
......

[Midfield]
...........

[Defend−Goal]

[Careful−
defense]
Intercept

kick−out

[Simple−goal
defense]
.....
Reposition

Figure 8: A Portion of the plan-hierarchy used by ISIS RoboCup agents.

In both of these examples, certain dierences between agents are agreed upon and are a
sign of correct execution, not of failure. Indeed, it is the lack of dierence in selected plans

133

Kaminka and Tambe

that would indicate failure in these cases. We use the term

mutual-exclusion coordination

refer to these relationships. In Example 3, ideally no two pilots are executing the
plan at the same time.

to

shooting

In Example 4, no two members of dierent sub-teams (e.g., an

attacker and a defender) are executing the same plan in service of

play (e.g., defend).

As the

examples demonstrate, there is a clear need for monitoring mutual-exclusion coordination.
Our results of previous sections are re-used in service of socially-attentive monitoring of
mutual-exclusion relationships. They require a transformation both in implementation and
theory. The hierarchies are compared in the usual manner, except that failures are signied
by equalities, rather than dierences. For instance, if an attacker is staying in the team's
own half of the eld, its teammates may come to suspect that it mistakenly defected the
attackers' sub-team and believes itself to be a defender.
The analytical results are inverted as well. The maximal team-coherence heuristic will
now lead to completeness, since it prefers hypotheses that contain equalities among agents,
which are failures in mutual-exclusion coordination. The maximal team-incoherence heuristic will now lead to sound detection, as it prefers hypotheses that imply no equalities have
occurred. These properties can be proven formally.

Theorem 5. Let a monitoring agent

A monitor mutual-exclusion relationships in a group
of agents G. If A's modeling of G is complete, and A uses a maximally team-incoherent
hypothesis for detection, then the failure detection results are sound.

Proof.

Provided in appendix A.

Theorem 6. Let a monitoring agent

monitor mutual-exclusion relationships in a group
of agents G. If A's modeling of G is complete, and A uses a maximally team-coherent
hypothesis for detection, then the failure detection results are complete.
Proof.

A

Provided in appendix A.

Thus in mutual-exclusion relationships, as in teamwork relationships, guaranteed failuredetection results may still be provided despite the use of limited, uncertain knowledge
about monitored agents. The centralized teamwork monitoring algorithms can now be easily transformed for monitoring mutual-exclusion relationships. Unfortunately, the results in
the distributed case (Theorem 4) cannot be so easily transformed, since they rely on the
property of observable-partitioning, which is associated with dierences, not with equalities.
We leave this issue for future work.

7.2 Monitoring Using Role-Similarity Relationships
This section applies socially-attentive monitoring to role-similarity relationships, for monitoring individual performance within teams. In particular, in service of team-plans agents
may select individual sub-plans, which do not necessitate agreement by team-members, but
are constrained by the agents' roles.

fly-flight-plan

For instance, in service of executing the team-plan

(Figure 3) pilots individually select their own individual plans which set

the velocity and heading within the constraints of the formation and ight method specied
in the mission.
Role similarity relationships specify the ways in which given individual plans are similar,
and to what extent.

Two agents of the same role who are executing dissimilar plans can

134

Robust Agent Teams via Socially-Attentive Monitoring

be considered to be in violation of the role-similarity relationships. This enables a sociallyattentive monitoring system to detect failure in role-execution. To monitor individual plans
the agent is executing, it compares its selection with that of other agents of

the same role,

similarly to the method we used for teamwork. If the plans are considered similar by the
role-similarity relationship model, no failure is detected.

Otherwise, a failure may have

occurred, and the diagnosis component is called to verify it and provide an explanation.
Let us illustrate with a failure from the ModSAF domain which our system was able to
detect using the role-similarity relationship:

Example 5.

A team of three helicopters was to take o from the base and head out on a

mission. However, one of the pilot agents failed to correctly process the mission statement.
It therefore kept its helicopter hovering above the base, while its teammates left to execute
the mission by themselves.
This failures was detected using role-similarity relationship monitoring. The agreed-upon
team-plan was selected by all the agents, and so no problem with teamwork relationship was
detected. This team-plan involved each agent then selecting individual methods of ight,
which determine altitude and velocity.

Here the agents diered.

The failing helicopter

remained hovering, while its teammates moved forward. Using a role similarity relationship,
the failing helicopter compared its own selected plan to that of its teammate (who shared its
role of a subordinate in the formation), and realized that their plans were dissimilar enough
to announce a possible failure.
Unfortunately, the actual similarity metrics seem to be domain- and task-specic, and
thus are not as easy to re-use across domains. Furthermore, detected failures are not necessarily real failures, nor do all detected failures have the same weight.

We are currently

investigating ways to address these challenging issues.

8. Related Work
Our investigation of socially-attentive monitoring, and the relationship between knowledge
maintained of agents' states and monitoring eectiveness builds on research in dierent subelds of multi-agent systems. We address these sub-elds in this section, and explain how
our investigation is related to existing literature.

8.1 Related Work on Teamwork
Previous work in teamwork has recognized that monitoring other agents is critical to teams.
Past investigations have raised the monitoring selectivity problem, but have not addressed
it in depth. Building upon these investigations, this paper begins to provide some in-depth
answers to this problem.
The theory of SharedPlans (Grosz & Kraus, 1996, 1999) touches on the teamwork monitoring selectivity problem in several ways, but provides only some initial answers. First, the
theory requires agents to know that their teammates are capable of carrying out their tasks
in the team.

The authors note that agents must communicate enough about their plans

to convince their teammates of their ability to carry out actions (Grosz & Kraus, 1996,
p. 314). Second, the theory requires agents to have mutual-belief in the shared recipe, a

135

Kaminka and Tambe

state that requires agents to reason to innite recursion about other agent's beliefs.

Un-

fortunately, attainment of mutual belief is undecidable in theory (Halpern & Moses, 1990)
and hence must be approximated in practice (Jennings, 1995; Rich & Sidner, 1997). Such
approximations may still impose strong monitoring requirements. Third, theory introduces
the intention-that construct in service of coordination and helpful behavior, implying monitoring of others' progress to assess the the need for such behavior (Grosz & Kraus, 1996,
Axiom A5-A7).

Fourth, SharedPlans requires that intentions of an agent must not con-

ict (Grosz & Kraus, 1996, Axiom A1), and since some of these intentions (in particular,
intentions-that) may involve the attitudes of other agents, some monitoring of others to
detect and avoid conicts is implied. The authors point out that while theoretically all such
conicts can be detected, this is infeasible in practice (Grosz & Kraus, 1996, p. 307). They
suggest that conict detection and prevention be investigated in a problem-specic manner
within the minimal constraints (i.e., monitoring for capabilities, mutual-belief, progress, lack
of conicts) provided by the SharedPlans framework (p. 308 and 314).
Joint-Intentions (Levesque et al., 1990; Cohen & Levesque, 1991) requires an agent who
privately comes to believe that a joint-goal is either achieved, unachievable, or irrelevant,
must commit to having the entire team mutually believe it to be the case. As in the theory
of SharedPlans, Joint-Intentions' use of mutual belief can only be approximated in practice,
and imposes strong monitoring requirements. Thus, the monitoring selectivity problem is
raised for practical implementations of Joint-Intentions.
Jennings has hypothesized that two central constructs in cooperative multi-agent coordination are

commitments

made by the agents, and

conventions, rules used to monitor these

commitments (Jennings, 1993). Such conventions are used to decide what information needs
to be monitored about agents, and how it is to be monitored. For instance, a convention may
require an agent to report to its teammates any changes it privately detects with respect
to the attainability of the team goal.

Jennings raises the monitoring selectivity problem

and provides an example of specic conventions for high- and low-bandwidth situations in
which some knowledge is not communicated to all agents if the bandwidth is not available.
However, Jennings does not explore in-depth the question of how such conventions are selected, and what are the trade-os and guarantees associated with the selection of particular
conventions. For instance, there are no guarantees on the eects of using the low-bandwidth
convention in the example.
The theoretical investigations described above all raise the monitoring selectivity problem (implicitly or explicitly). Our work builds upon these to address this problem in depth,
in the context of socially-attentive monitoring in teams. This paper reports on soundness
and/or completeness properties of teamwork relationship failure-detection that can be analytically guaranteed, despite uncertainty in knowledge acquired about monitored agents.
The analytical guarantees are applicable to plan-recognition and communications, and are
corroborated by empirical results.
Building on theoretical work, practical teamwork systems include (Jennings, 1995; Rich
& Sidner, 1997) and (Tambe, 1997).

Jennings' investigation of the Joint-Responsibility

teamwork model in GRATE* (Jennings, 1995) builds on Joint-Intentions, and similarly to
our own implementation, requires agents to agree on the team-plans which are to execute.
However, GRATE* is used in industrial settings in which foolproof communications can
be assumed (Jennings, 1995, p.

211), and thus only passive monitoring (via communica-

136

Robust Agent Teams via Socially-Attentive Monitoring

tions) is used. Although Jennings provides an evaluation of GRATE*'s performance with
respect to communication delays, no guarantees are provided with respect to failure detection. GRATE* maintains knowledge about other agents through acquaintances models,
which are used to keep track of what team-members' capabilities are (in service of forming
teams). However, the question of how much knowledge should be used in these models is
left unaddressed.
Rich and Sidner investigate COLLAGEN in a collaborative user-interface system, in
which communications are reliable (Rich & Sidner, 1997). However, from a human-usability
perspective, limiting the amount of communications is still desirable.

To address this is-

sue, recent empirical work by Lesh, Sidner and Rich (1999) utilizes plan recognition in
COLLAGEN; the focus of that work is on using the collaborative settings to make the
plan-recognition tractable.

For instance, ambiguities in plan-recognition may be resolved

by asking the user for clarication. Work on COLLAGEN does not investigate how much
knowledge is to be maintained for eective collaborative dialogue with the user. In contrast,
we are able to provide guarantees on the failure-detection results of our algorithms. Also,
analysizing the dialogue plans for

risky points

may allow systems such as COLLAGEN to

decide whether to use communications for clarication regardless of plan-recognition ambiguity.
STEAM (Tambe, 1997) maintains limited information about the ability of team-members
to carry out their roles. STEAM also allows team-members to reason explicitly about the
cost of communication in deciding whether to communicate or not. Our work signicantly
extends these capabilities via plan-recognition, and provides analytically-guaranteed faultdetection results. Furthermore, our teamwork failure-detection capabilities can be useful to
trigger STEAM's re-planning capabilities.

8.2 Related Work on Coordination
Huber (1995) investigated the use of probabilistic plan-recognition in service of active teamwork monitoring, motivated by the unreliability and costs of passive communications-based
monitoring in military applications. Washington explores observation-based coordination using Markov Models (Washington, 1998), focusing on making the computations tractable. In
contrast to Huber and Washington, our work focuses on the monitoring selectivity problem.
We showed strengths and limitations of centralized and distributed approaches that guaranteed failure-detection results using coherence-based disambiguation of plan-recognition
hypotheses.
Durfee (1995) discusses various methods of reducing the amount of knowledge that agents
need to consider in coordinating with others. The methods discussed involve pruning parts
of the nested models, using communications, using hierarchies and abstractions, etc. While
the focus of this work is on methods by which modeling can be limited, the focus of our
work is on the question of how much modeling is required for guaranteed performancethe
monitoring selectivity problem. We provide analytical guarantees on trade-os involved in
using limited knowledge of agents for failure-detection purposes.
Sugawara and Lesser (1998) report on the use of comparative reasoning/analysis techniques in service of learning and specializing coordination rules for a system in which distributed agents coordinate in diagnosing a faulty network. The investigation is focused on

137

Kaminka and Tambe

optimizing coordination rules to minimize ineciency and redundancy in the agent's coordinating messages. Upon detecting sub-optimal coordination (via a fault model), the agents
exchange information on their local views of the system and the problem solving activity,
and construct a global view. They then compare the local view to the global view to nd
critical values/attributes which were missing from the local view and therefore gave rise to
the sub-optimal performance problem. These values and attributes are used in constructing
situation-specic rules that optimize coordination in particular situations.

For example,

network diagnosis agents may learn a rule that guides them to choose a coordination strategy in which only one agent performs the diagnosis and shares its result with the rest of
the diagnosis agents. Our work on socially-attentive monitoring similarly uses comparison
between agents views to drive the monitoring process. However, our use of comparison is
a product of the relationship we are monitoring.

While Sugawara and Lesser's work can

be viewed as letting the agents incrementally optimize their monitoring requirements, our
results analytically explore the level of monitoring required for eective failure-detection, in
dierent congurations. Our teamwork monitoring technique addresses uncertainty in the
acquired information, and does not construct a global view of all attributes the systemas
that would be extremely expensive. Instead, our technique focuses on triggering failure detection via contrasting of plans, then incrementally expanding the search for dierences in
the diagnosis process.
Robotics literature has also raised the monitoring selectivity problem.

Parker (1993)

investigated the monitoring selectivity problem from a dierent perspective, for a formationmaintenance task. She empirically examined the eects of combining socially-attentive information (which she referred to as local) and knowledge of the team's goals, and concludes
that the most fault-tolerant strategy is one where the agents monitor each other as well as
progress towards the goals.

Kuniyoshi et al.

(Kuniyoshi, Rougeaux, Ishii, Kita, Sakane,

& Kakikura, 1994) present a framework for cooperation by observations, in which robots
visually attend to others as a prerequisite to coordination. The framework presents several
standard attentional templates, i.e., who monitors whom. They dene a team attentional
structure as one in which all agents monitor each other.

Our work focuses on the mon-

itoring selectivity problem within socially-attentive monitoring of teamwork relationships,
and provides analytical as well as empirical results. We treat the attentional templates as
a product of the relationships that hold in the system. Our results show that monitoring in
teams may not necessarily require monitoring all team-members.

8.3 Other Related Work
Horling et al.

(Horling, Lesser, Vincent, Bazzan, & Xuan, 1999) present a distributed

diagnosis system for a multi-agent intelligent home environment.The system uses faultmodels to identify failures and ineciencies in components, and to guide recovery. Schroeder
and Wagner (1997) proposed a distributed diagnosis technique in which cooperating agents
receive requests for tests and diagnoses, and send responses to other agents.

They each

construct a global diagnosis based on the local ones they produce and receivewith the
assumption that no conicts will occur.

Frohlich and Nejdl (1996) investigates a scheme

in which multiple diagnosis agents cooperate via a blackboard architecture in diagnosing a
physical system. The agents may use dierent diagnosis models or systems, but a centralized

138

Robust Agent Teams via Socially-Attentive Monitoring

conict-resolution agent is employed to handle any conicts in diagnoses found. All three
approaches do not address the monitoring selectivity problem.
There are a few social measures related to the ATA. Goldberg and Mataric (1997) in-

interference the amount of time robots
social entropy (Bailey, 1990) to measure be-

vestigate a multi-robot foraging task and measure
spend avoiding each other. Balch (1998) uses

havioral diversity

in multi-agent tasks of soccer, foraging, and formation-maintenance. Both

investigations focus on characterizing heterogeneity in multi-agent systems and its relation
to performance. In contrast, the focus of our work is on providing useful feedback to the
designer.

Possible correlation between task performance and ATA values remains to be

investigated.

9. Conclusions and Future Work
The work presented in this paper is motivated by practical concerns. We have begun our
investigation of the monitoring selectivity problem as a result of our observation that failures
continue to occur despite our agents' use of monitoring conditions and communications.
Analysis of the failures revealed that agents were not suciently informed about each other's
state. While the need to monitor one's teammates has been recognized repeatedly in the past
(Jennings, 1993; Grosz & Kraus, 1996; Tambe, 1997), the monitoring selectivity problem
the question of how much monitoring is requiredremained largely unaddressed (Jennings,
1993; Grosz & Kraus, 1996).
We provide key answers to the monitoring selectivity problem.

Within the context of

socially-attentive monitoring in teams, we demonstrate that teamwork relationship failures
can be detected eectively even with uncertain, limited, knowledge of team-members' states.
We show analytically that centralized active teamwork monitoring provides failure-detection
that is either complete and unsound, or sound and incomplete. However, centralized teamwork monitoring requires multiple hypotheses and monitoring of all team-members.

In

contrast, distributed active teamwork monitoring results in complete and sound failuredetection, despite using a simpler algorithm and monitoring only key agents in a team.
Using an implemented general framework for socially-attentive monitoring, we empirically validate these results in the ModSAF domain. We also provide initial results in monitoring mutual-exclusion and role-similarity relationships, and initial diagnosis procedures.
We further demonstrate the generality of the framework by applying it in the RoboCup
domain, in which we show how useful quantitative analysis can be generated o-line. Both
ModSAF and RoboCup are dynamic, complex, multi-agent domains that involve many uncertainties in perception and action.
We attempted to demonstrate how the results and techniques can be applied in other
domains.

We have explicitly pointed out necessary conditions for the theorems to hold,

such as observable-partitioning and team-modeling completeness. The presented diagnosis
algorithm is sensitive to the accuracy of the knowledge used, and may require assuming that
plans can be recognized as soon as they are selected. These conditions should be veried by
the designer in the target application domain. Reactive plans (our chosen representation) are
commonly used in many dynamic multi-agent domains. Our focus on monitoring agreements
on joint plans stems from the centrality of similar notions of agreement in agent and human
teamwork literature (Jennings, 1995; Grosz & Kraus, 1996; Volpe et al., 1996; Tambe, 1997).

139

Kaminka and Tambe

We made several references to additional areas in which we would like to conduct further
investigations.

One important topic which we plan to investigate in depth is the strong

requirements of the distributed teamwork monitoring algorithm in terms of observability. In
order to provide its soundness and completeness guarantees, the distributed algorithm relies
on the ability of all team-members to monitor the key agents. We are investigating ways
to relax this requirement while still providing guaranteed results. In addition, the diagnosis
procedures should be extended and formalized, and we would like to investigate ways to
alleviate the sensitivity of these procedures to the choice of team-modeling hypothesis.

Acknowledgments
This article is partially based on an AAAI-98 paper (Kaminka & Tambe, 1998), and an
Agents-99 paper (Kaminka & Tambe, 1999) by the same authors. This research was supported in part by NSF Grant ISI-9711665, and in part by AFOSR contract #F49620-97-10501. We thank Je Rickel, George Bekey, Victor Lesser, Dan O'Leary, and David Pynadath
for many useful comments. The anonymous reviewers have our thanks for helping us to crystallize our ideas and contributions in revisions of this paper.

Appendix A. Proofs
Theorem. (# 2, page 123). Let a monitoring agent A monitor a simple team T . If A's
team-modeling of T is complete, and A uses a maximally team-coherent hypothesis for detection, then the teamwork failure detection results are sound.
Proof.

We will show that any failure that occurs is detected, and thus that all failures will

be detected. Let a1 ; : : : ; an be the agent members of T . Each agent ai is executing some
plan Pi (1  i  n).

Thus collectively, the group is executing (P1 ; : : : ; Pn ).

If a failure

has occurred, then there are two agents ak ; aj ; 1  j; k  n such that aj is executing plan
Pj and ak is executing plan Pk and Pj 6= Pk .

Since A's team-modeling is complete, the

correct hypothesis (P1 ; : : : ; Pj ; : : : ; Pk ; : : : Pn ) will in the set of team-modeling hypotheses.
Since A will choose a maximally team-incoherent hypothesis, either it will choose the correct
hypothesis, which is more incoherent than a hypothesis implying no failure has occurred, or
that it will select a hypothesis with greater incoherence hypothesis (or equivalent level). In
any case, a failure would be detected, and the detection procedure is complete.

Lemma. (#

1, page 127). Suppose a simple team T is self-monitoring (all members of
the team monitor each other) using the maximally team-coherent heuristic (and under the
assumption that for each agent, team-modeling is complete). A monitoring agent A1 who is a
member of T and is executing P1 would detect a failure in maintaining teamwork relationships
with an agent A2 (also a member of T ) executing a dierent plan P2 , if A2 has an observably
dierent role in P1 and P2 .

Proof.

A1 knows that it is executing P1 .

Since all members of T monitor each other and

themselves, A1 is monitoring A2 , who has an observably dierent role in P1 and P2 . Since A2
is executing P2 , and following the observably dierent role, P1 2
= M (A1 ; A2 =P2 ). Therefore
from the perspective of A1 , it cannot be the case that it assigns P1 in any
hypothesis, and therefore any

team-modeling

agent-modeling

hypothesis that A1 has will have A1 executing

140

Robust Agent Teams via Socially-Attentive Monitoring

P1 , and A2 executing some plan other than P1 . In other words, from A1 's perspective there is

no team-coherent hypothesis, and so a dierence would be detected between A1 and A2 .

Theorem. (# 5, page 134). Let a monitoring agent A monitor mutual-exclusion relation-

ships in a group of agents G. If A's modeling of G is complete, and A uses a maximally
team-incoherent hypothesis for detection, then the failure detection results are sound.
Proof.

We will show that if no failure has occurred, none will be detected, and thus that

any failure that is detected is in fact a failure.
G.

Let a1 ; : : : ; an be the agent members of

Each agent ai is executing some plan Pi (1  i  n). Thus collectively, the group is

executing (P1 ; : : : ; Pn ). If no failure has occurred, then each agent is executing a dierent
plan (i 6= j ) Pi 6= Pj ). Since A's group-modeling is complete, the correct hypothesis is
going to be in the set of group-modeling hypotheses H . Since it is a maximally incoherent
hypothesis, either it will be selected, or that a dierent hypothesis

level

will be selected.

of the same coherence

Any hypothesis with the same coherence level as the correct one

implies no failure is detected. Thus the detection procedure is sound.

Theorem. (# 6, page 134). Let a monitoring agent A monitor mutual-exclusion relation-

ships in a group of agents G. If A's modeling of G is complete, and A uses a maximally
team-coherent hypothesis for detection, then the failure detection results are complete.
Proof.

We will show that any failure that occurs is detected, and thus that the procedure

is complete. Let a1 ; : : : ; an be the agent members of G. Each agent ai is executing some
plan Pi (1  i  n).

Thus collectively, the group is executing (P1 ; : : : ; Pn ).

If a failure

has occurred, then there are two agents ak ; aj ; 1  j; k  n such that aj is executing plan
Pj and ak is executing plan Pk and Pj

=

Pk .

Since A's group-modeling is complete, the

correct hypothesis (P1 ; : : : ; Pj ; : : : ; Pk ; : : : Pn ) will in the set of group-modeling hypotheses.
Since A will choose a maximally team-coherent hypothesis, either it will choose the correct
hypothesis, which is more coherent than a hypothesis implying no failure has occurred, or
that it will select a hypothesis with greater coherence hypothesis (or equivalent level). In
any case, a failure would be detected. Therefore, the detection procedure is complete.

Appendix B. Socially-Attentive Monitoring Algorithms
We bring here the algorithms (in pseudo-code) for the RESL plan-recognition algorithm,
the comparison test supporting detection in both simple and non-simple teams, and the
monitoring algorithms for the centralized and distributed cases.

B.1 RESL
RESL works by rst expanding the complete operator hierarchy for the agents being modeled, tagging all plans as non-matching. All plans' preconditions and termination conditions
are agged as non-matching as well. All plans' actions are set to be used as expectations
on behavior. After initializing the plan-recognition hierarchy for each monitored agent, observations of an agent are continuously matched against the actions expected by the plans.
Plans whose expectations match observations are tagged as matching, and these ags are
propagated along the hierarchy, up and down, so that complete paths through the hierarchy

141

Kaminka and Tambe

are agged as matching or not. These paths specify the possible matching interpretations of
the observations. In addition, precondition and termination conditions are agged as true
or not, signifying the inferred appropriate belief by the modeled agents.

This process is

described in Algorithm 1.

Algorithm 1 RESL's main loop, matching

observation and making inferences for a given

plan-recognition hierarchy (a single agent).
1.

Get observations about agent

2.

For each plan that has a set of expected observations:
(a) Compare observations to expectations
(b) If succeed, ag plan as matching successfully, otherwise ag plan as failing to match

3.

For each plan that is agged as matching successfully
(a) Flag its parents as matching successfully // propagate matching

4.

For each plan whose children (all of them) are agged as failing to match
(a) Flag it as failing to match // propagate non-matching

B.2 Detection of Failure, Centralized and Distributed Teamwork Monitoring
Algorithm 2 shows how comparison of hierarchical plans is carried out. We limit ourselves
here to simple-teams. The algorithm accepts as input two sets of hierarchical plan hypotheses, and their two associated agents (for clarity, the algorithms assume only two agents.
The generalization to n agents is straightforward). The algorithm also accepts a policy ag,

Policy.

An

OPTIMISTIC

policy causes the algorithm to use maximal team-coherence to

provide sound, but incomplete detection. A

PESSIMISTIC policy causes the algorithm to use

maximal team-incoherence to provide complete, but unsound detection.

hierarchy_1 and hierarchy_2. The two agents
agent_2. The algorithm makes use of the predicate Sub-team,
which is true if the two agents (Agent1, Agent2) belong to dierent sub-teams at the given
level of the hierarchy (Depth).
The set of hierarchical plans are marked

are marked

agent_1

and

With the aid of Algorithm 2, we can now dene the centralized and distributed failure detection algorithms.

The centralized teamwork monitoring algorithm (Algorithm 3)

utilizes Algorithm 2 twice, checking for failures with both

PESSIMISTIC

and

OPTIMISTIC

policies. If the results of both policies agree, they are certain. If the results do not agree,
(i.e., the

PESSIMISTIC

policy causes a failure to be detected, while the

OPTIMISTIC

policy

causes no failure to be detected), then the monitoring agent cannot be certain that a failure
has taken place, and therefore needs to verify the failure.

Algorithm 3 therefore returns

FAILURE, NO_FAILURE, POSSIBLE_FAILURE.
The distributed monitoring algorithm is not given in pseudo-code form, because it is
nothing more than a call to Algorithm 2 with an

142

OPTIMISTIC

policy parameter. Its power

Robust Agent Teams via Socially-Attentive Monitoring

Algorithm 2 Hierarchical comparison of two agents, allowing for sub-teams.
1.

Set Depth to 0 //

2.

While both plans at depth Depth are team-plans Do
(a)

look for top-most dierence rst

if Policy == OPTIMISTIC
i. then Let Plan_1, Plan_2 be maximally team-coherent plans at level Depth
of hierarchy_1 and hierarchy_2, respectively.
ii. else Let Plan_1, Plan_2 be maximally team-incoherent plans at level Depth
of hierarchy_1 and hierarchy_2, respectively.

(b)

If Plan_1 is not equal to Plan_2
i. then return FAILURE
ii. else if bottom of hierarchies reached, return NO_FAILURE, otherwise increase Depth and go to 2.

3.

If only one plan is a team-plan, return FAILURE, else return NO_FAILURE.

Algorithm 3 Centralized Teamwork Monitoring, applying both optimistic and pessimistic
views.
1.

2.

Let Optimistic_Result = Detect(agent_1, agent_2, hierarchies_1,
hierarchies_2, OPTIMISTIC)
/* algorithm 2 */
Let Pessimistic_Result = Detect(agent_1, agent_2, hierarchies_1,
hierarchies_2, PESSIMISTIC)
/* algorithm 2 */

3.

if Optimistic_Result == Pessimistic_Result

4.

then return Optimistic_Result /*

5.

else return POSSIBLE_FAILURE

either

143

FAILURE,

or

NO_FAILURE */

Kaminka and Tambe

is derived from the fact that all members of the team are using it to monitor the key agents
of the team.

References
Ambros-Ingerson, J. A., & Steel, S. (1988). Intergrating planning, execution and monitoring.
In

Proceedings of the Seventh National Conference on Articial Intelligence (AAAI-88)

Minneapolis/St. Paul, MN. AAAI Press.
Andou, T. (1998). Renement of soccer agents' positions using reinforcement learning. In
Kitano, H. (Ed.),

RoboCup-97: Robot soccer world cup 1, Vol. LNAI 1395, pp. 373388.

Springer-verlag.
Atkins, E. M., Durfee, E. H., & Shin, K. G. (1997). Detecting and reacting to unplanned-

Proceedings of the Fourteenth National Conference on Articial
Intelligence (AAAI-97), pp. 571576 Providence, RI. AAAI Press.
for world states. In

Bailey, K. D. (1990).
Balch, T. (1998).

Social Entropy Theory.

State University of New York Press.

Behavioral Diversity in Learning Robot Teams.

Ph.D. thesis, Georgia

Institute of Technology.
Calder, R. B., Smith, J. E., Courtemanche, A. J., Mar, J. M. F., & Ceranowicz, A. Z. (1993).

Modsaf behavior simulation and control. In Proceedings of the Third Conference on
Computer Generated Forces and Behavioral Reresentation Orlando, Florida. Institute
for Simulation and Training, University of Central Florida.

Cohen, P. R., Amant, R. S., & Hart, D. M. (1992).

Early warnings of plan failure, false

positives, and envelopes: Experiments and a model.

Tech. rep. CMPSCI Technical

Report 92-20, University of Massachusetts.
Cohen, P. R., & Levesque, H. J. (1991). Teamwork.

Nous, 35.

Doyle, R. J., Atkinson, D. J., & Doshi, R. S. (1986). Generating perception requests and
expectations to verify the execution of plans.

In

Conference on Articial Intelligence (AAAI-86).
Durfee, E. H. (1995).

Blissful ignorance:

Proceedings of the Fifth National

Knowing just enough to coordinate well.

In

Proceedings of the First International Conference on Multiagent Systems (ICMAS-95),
pp. 406413.

Fenster, M., Kraus, S., & Rosenschein, J. S. (1995).

Coordination without communica-

In Proceedings of the First
International Conference on Multiagent Systems (ICMAS-95), pp. 102108 California,
tion: Experimental validation of focal point techniques.
USA.

Firby, R. J. (1987). An investigation into reactive planning in complex domains. In

ceedings of the Sixth National Conference on Articial Intelligence (AAAI-87).
144

Pro-

Robust Agent Teams via Socially-Attentive Monitoring

Frohlich, P., & Nejdl, W. (1996). Resolving conicts in distributed diagnosis. In Wahlster,
W. (Ed.),

the 12th Europeach Conference on Articial Intelligence (ECAI-96).

John

Wiley & Sons, Inc.
Goldberg, D., & Mataric, M. J. (1997).

Interference as a tool for designing and evaluat-

Proceedings of the Fourteenth National Conference on
Articial Intelligence (AAAI-97), pp. 637642 Providence, RI. AAAI Press.
ing multi-robot controllers. In

Grosz, B. J., & Kraus, S. (1999). The evolution of sharedplans. In Wooldridge, M., & Rao,
A. (Eds.),

Foundations and Theories of Rational Agency, pp. 227262.

Grosz, B. J., & Kraus, S. (1996). Collaborative plans for complex group actions.

Intelligence, 86, 269358.

Articial

Grosz, B. J., & Sidner, C. L. (1990). Plans for discourse. In Cohen, P. R., Morgan, J., &
Pollack, M. (Eds.),

Intentions in Communication, pp. 417445. MIT Press, Cambridge,

MA.
Halpern, J. Y., & Moses, Y. (1990). Knowledge and common knowledge in a distributed
environment.

distributed computing, 37 (3), 549587.

Hamscher, W., Console, L., & de Kleer, J. (Eds.). (1992).

nosis.

Readings in Model-Based Diag-

Morgan Kaufmann Publishers, San Mateo, CA.

Horling, B., Lesser, V. R., Vincent, R., Bazzan, A., & Xuan, P. (1999).

Diagnosis as an

integral part of multi-agent adaptability. Tech. rep. CMPSCI Technical Report 199903, University of Massachusetts/Amherst.
Huber, M. J., & Durfee, E. H. (1995).

On acting together: Without communication.

In

Working Notes of the AAAI Spring Symposium on Representing Mental States and
Mechanisms, pp. 6071 Stanford, CA.

Jennings, N. R. (1993). Commitments and conventions: the foundations of coordination in
multi-agent systems.

Knowledge Engineering Review, 8 (3), 223250.

Jennings, N. R. (1995). Controlling cooperative problem solving in industrial multi-agent
systems using joint intentions.

Articial Intelligence, 75 (2), 195240.

Johnson, W. L., & Rickel, J. (1997). STEVE: An animated pedagogical agent for procedural
training in virtual environments.

SIGART Bulletin, 8 (1-4), 1621.

Kaminka, G. A., & Tambe, M. (1998). What's wrong with us? Improving robustness through

In Proceedings of the Fifteenth National Conference on Articial
Intelligence (AAAI-98), pp. 97104 Madison, WI. AAAI Press.
social diagnosis.

Kaminka, G. A., & Tambe, M. (1999).

I'm OK, You're OK, We're OK: Experiments in

distributed and centralized social monitoring and diagnosis.

In

Proceedings of the

Third International Conference on Autonomous Agents (Agents-99) Seattle, WA. ACM
Press.

145

Kaminka and Tambe

Kinny, D., Ljungberg, M., Rao, A., Sonenberg, E., Tidhar, G., & Werner, E. (1992). Planned
team activity.

In Castelfranchi, C., & Werner, E. (Eds.),

Articial Social Systems,

Lecture notes in AI 830, pp. 227256. Springer Verlag, New York.

Kitano, H., Tambe, M., Stone, P., Veloso, M., Coradeschi, S., Osawa, E., Matsubara, H.,

Proceedings of the International Joint Conference on Articial Intelligence (IJCAI-97)

Noda, I., & Asada, M. (1997). The RoboCup synthetic agent challenge '97. In
Nagoya, Japan.

Kraus, S., Sycara, K., & Evenchik, A. (1998). Reacing agreements through negotiations: a
logical model and implementation.

articial intelligence, 104 (1-2), 169.

Kuniyoshi, Y., Rougeaux, S., Ishii, M., Kita, N., Sakane, S., & Kakikura, M. (1994). Cooperation by observation  the framework and the basic task patterns. In

International Conference on Robotics and Automation,

the IEEE

pp. 767773 San-Diego, CA.

IEEE Computer Society Press.
Lesh, N., Rich, C., & Sidner, C. L. (1999). Using plan recognition in human-computer col-

Proceedings of the Seventh International Conference on User Modelling
(UM-99) Ban, Canada.
laboration. In

Levesque, H. J., Cohen, P. R., & Nunes, J. H. T. (1990). On acting together. In

of the Eigth National Conference on Articial Intelligence (AAAI-90)

Proceedings

Menlo-Park,

CA. AAAI Press.
Malone, T. W., & Crowston, K. (1991).
tion.

Toward an interdisciplinary theory of coordina-

Tech. rep. CCS TR#120 SS WP# 3294-91-MSA, Massachusetts Institute of

Technology.
Marsella, S. C., Adibi, J., Al-Onaizan, Y., Kaminka, G. A., Muslea, I., Tallis, M., & Tambe,
M. (1999).

On being a teammate:

Experiences acquired in the design of robocup

Proceedings of the Third International Conference on Autonomous Agents
(Agents-99) Seattle, WA. ACM Press.

teams.. In

Newell, A. (1990).

Unied Theories of Cognition.

Harvard University Press, Cambridge,

Massachusetts.

the Proceedings
of the IEEE Robotics and Automation Conference, pp. 582587 Atlanta, GA.

Parker, L. E. (1993). Designing control laws for cooperative agent teams. In

Rao, A. S. (1994). Means-end plan recognition  towards a theory of reactive recognition.

In Proceedings of the International Conference on Knowledge Representation and Reasoning (KR-94), pp. 497508.

Reece, G. A., & Tate, A. (1994). Synthesizing protection monitors from causal structure.
In

Proceedings of Articial Intelligence Planning Systems (AIPS-94) Chicago, IL.

Rich, C., & Sidner, C. L. (1997).

COLLAGEN: When agents collaborate with people.

In Johnson, W. L. (Ed.), Proceedings of the First International Conference on Autonomous Agents (Agents-97), pp. 284291 Marina del Rey, CA. ACM Press.

146

Robust Agent Teams via Socially-Attentive Monitoring

Proceedings
of the First International Conference on Autonomous Agents (Agents-97), pp. 268275

Schroeder, M., & Wagner, G. (1997). Distributed diagnosis by vivid agents. In
Marina del Rey, CA. ACM Press.

Sugawara, T., & Lesser, V. R. (1998). Learning to improve coordinated actions in cooperative
distributed problem-solving environments.

Machine Learning, 33 (2/3), 129153.

Tambe, M. (1996). Tracking dynamic team activity. In

ence on Articial Intelligence (AAAI).

Tambe, M. (1997). Towards exible teamwork.

7, 83124.

Proceedings of the National Confer-

Journal of Articial Intelligence Research,

Tambe, M., Johnson, W. L., Jones, R., Koss, F., Laird, J. E., Rosenbloom, P. S., & Schwamb,
K. (1995).

16 (1).

Intelligent agents for interactive simulation environments.

AI Magazine,

Proceedings of the
Fourteenth National Conference on Articial Intelligence (AAAI-97), pp. 39 Provi-

Toyama, K., & Hager, G. D. (1997). If at rst you don't succeed.... In
dence, RI.

Veloso, M., Pollack, M. E., & Cox, M. T. (1998). Rationale-based monitoring for planning
in dynamic environments. In

(AIPS-98) Pittsburgh, PA.

Proceedings of Articial Intelligence Planning Systems

Volpe, C. E., Cannon-Bowers, J. A., & Salas, E. (1996). The impact of cross-training on
team functioning: An empirical investigation.

human factors, 38 (1), 87100.

Markov tracking for agent coordination. In Proceedings of the
Second International Conference on Autonomous Agents (Agents-98), pp. 7077 Min-

Washington, R. (1998).

neapolis/St. Paul, MN. ACM Press.

147

Journal of Artificial Intelligence Research 12 (2000) 317–337

Submitted 6/99; published 5/00

Axiomatizing Causal Reasoning
Joseph Y. Halpern

halpern@cs.cornell.edu

Cornell University, Computer Science Department
Ithaca, NY 14853
http://www.cs.cornell.edu/home/halpern

Abstract
Causal models defined in terms of a collection of equations, as defined by Pearl, are
axiomatized here. Axiomatizations are provided for three successively more general classes
of causal models: (1) the class of recursive theories (those without feedback), (2) the class
of theories where the solutions to the equations are unique, (3) arbitrary theories (where
the equations may not have solutions and, if they do, they are not necessarily unique). It
is shown that to reason about causality in the most general third class, we must extend
the language used by Galles and Pearl (1997, 1998). In addition, the complexity of the
decision procedures is characterized for all the languages and classes of models considered.

1. Introduction
The important role of causal reasoning—in prediction, explanation, and counterfactual
reasoning—has been argued eloquently in a number of recent papers and books (Chajewska
& Halpern, 1997; Heckerman & Shachter, 1995; Henrion & Druzdzel, 1990; Druzdzel &
Simon, 1993; Pearl, 1995; Pearl & Verma, 1991; Spirtes, Glymour, & Scheines, 1993). If
we are to reason about causality, then it is certainly useful to find axioms that characterize
such reasoning. The way we go about axiomatizing causal reasoning depends on two critical
factors:
• how we model causality, and
• the language that we use to reason about it.
In this paper, I consider one approach to modeling causality, using structural equations.
The use of structural equations as a model for causality is standard in the social sciences,
and seems to go back to the work of Sewall Wright in the 1920s (see (Goldberger, 1972) for a
discussion); the particular framework that I use here is due to Pearl (1995). Galles and Pearl
(1997) introduce some axioms for causal reasoning in this framework; they also provide a
complete axiomatic characterization of reasoning about causality in this framework, under
the strong assumption that there is a fixed, given causal ordering ≺ of the equations (Galles
& Pearl, 1998). Roughly speaking, this means there is a way of ordering the variables that
appear in the equations and we have explicit axioms that say Xj has no influence of Xi if
Xi ≺ Xj in this causal ordering.
In this paper, I extend the results of Galles and Pearl by providing a complete axiomatic
characterization for three increasingly general classes of causal models (defined by structural
equations):
c
2000
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Halpern

1. the class of recursive theories (those without feedback—this generalizes the situation
considered by Galles and Pearl (1998), since every fixed causal ordering of the variables
gives rise to a recursive theory),
2. the class of theories where the solutions to the equations are unique,
3. arbitrary theories (where the equations may not have solutions and, if they do, they
are not necessarily unique).
In the process, I clarify some problems in the Galles-Pearl completeness proof that arise
from the lack of propositional connectives (particularly disjunction) in the language they
consider and, more generally, highlight the role of the language in reasoning about causality.
I also characterize the complexity of the decision problem for all these languages and classes
of models.
The rest of the paper is organized as follows. In Section 2, I give the syntax and semantics
of the languages I will be considering and review the definition of modifiable causal models.
In Section 3, I present the complete axiomatizations. In Section 4 I consider the complexity
of the decision procedure. I conclude in Section 5.

2. Syntax and Semantics
An axiomatization is given with respect to a particular language and a class of models, so
we need to make both precise. Both the language and models I use are based on those
considered by Galles and Pearl (1997, 1998). To make comparisons easier, I use their
notation as much as possible. I start with the semantic model, since it motivates some of
the choices in the syntax, then give the syntax, and finally define the semantics of formulas.
2.1 Causal Models
The basic picture here is that we are interested in the values of random variables, some of
which have a causal effect on others. This effect is modeled by a set of structural equations.
In practice, it seems useful to split the random variables into two sets, the exogenous
variables, whose values are determined by factors outside the model, and the endogenous
variables. It is these endogenous variables whose values are described by the structural
equations.
More formally, a signature S is a tuple (U, V, R}, where U is a finite set of exogenous
variables, V is a finite set of endogenous variables, and R associates with every variable
Y ∈ U ∪ V a nonempty set R(Y ) of possible values for Y (the range of possible values of Y ).
Unless explicitly noted otherwise, I assume that R(Y ) is a finite set for each Y ∈ U ∪ V and
|R(Y )| ≥ 2. The assumption that U and V are finite is relatively innocuous; as we shall see,
the assumption that R(Y ) is finite has more of an impact on the axioms. The assumption
that |R(Y )| ≥ 2 allows us to ignore the trivial situation where |R(Y )| = 1. If |R(Y )| = 1,
we can just remove the variable Y from the signature without loss of expressiveness.
A causal model over signature S is a tuple T = (S, F ) where F associates with each
variable X ∈ V a function denoted FX such that FX : (×U ∈U R(U )) × (×Y ∈V−{X}R(Y )) →
R(X). FX tells us the value of X given the values of all the other variables in U ∪ V. We
think of the functions FX as defining a set of (modifiable) structural equations, relating the
318

Axiomatizing Causal Structures

values of the variables. Because FX is a function, there is a unique value of X once we have
set all the other variables. Notice we have such functions only for the endogenous variables.
The exogenous variables are taken as given; it is their effect on the endogenous variables
(and the effect of the endogenous variables on each other) that we are modeling with the
structural equations.
~ of
Given a causal model T = (S, F ) over signature S, a (possibly empty) vector X
~ and U, respectively, we
variables in V, and vectors ~x and ~u of values for the variables in X
~ R| ~ ).1
u) over the signature SX~ = (∅, V−X,
can define a new causal model denoted TX←~
~ x (~
V−X
~ are set to ~x and the
Intuitively, this is the causal model that results when the variables in X
~
~
u) = (SX~ , F X←~x,~u }), where FYX←~x,~u is obtained
variables in U are set to ~u. Formally, TX←~
~ x (~
~ to ~x and the values of the variables in U
from FY by setting the values of the variables in X
u) is called a submodel of T by Pearl (1999). It can describe a
to ~u. The causal model TX←~
~ x (~
possible counterfactual situation; that is, even though, under normal circumstances, setting
~ having values ~
the exogenous variables to ~u may result in the variables X
x0 6= ~x, this
submodel describes what happens if they are set to ~
x due to some “external action”, the
cause of which is not modeled explicitly. For example, to determine if the manufacturer
is at fault in an accident that involved a poorly maintained car, we may want to consider
what would have happened had the car been well maintained. If there is a random variable
in the signature that describes how well maintained the car is, then this means examining
the submodel where that random variable is set to 1 (the car is well maintained). It is this
ability to examine counterfactual situations that makes causal structures a useful tool for
reasoning about causality.
Notice that, in general, there may not be a unique vector of values that simultaneously
satisfies the equations in TX←~
u); indeed, there may not be a solution at all. One special
~ x (~
case where there is guaranteed to be such a unique solution is if there is some total ordering
≺ of the variables in V such that if X ≺ Y , then FX is independent of the value of Y ;
i.e., FX (. . . , y, . . .) = FX (. . . , y 0 , . . .) for all y, y 0 ∈ R(Y ). In this case, the causal model is
said to be recursive or acyclic. Intuitively, if the theory is recursive, there is no feedback.
If X ≺ Y , then the value of X may affect the value of Y , but the value of Y has no effect
on the value of X.
It should be clear that if T is a recursive theory, then there is always a unique solution
~ ~
to the equations in TX←~
u), for all X,
x, and ~
u. (We simply solve for the variables in the
~ x (~
order given by ≺.) On the other hand, as the following example shows, it is not hard to
construct nonrecursive theories for which there is always a unique solution to the equations
that arise.
Example 2.1: Let S = (∅, {X, Y }, R}), where R(X) = R(Y ) = {−1, 0, 1}, and let T =
(S, F ), where FX is characterized by the equation X = Y and FY is characterized by the
equation Y = −X (that is, FX (y) = y and FY (x) = −x). Clearly T is not recursive; the
value of X depends on the value of Y and the value of Y depends on that of X. Nevertheless,
it is easy to see that T has the unique solution X = 0, Y = 0, TX←x has the unique solution
Y = −x, and TY ←y has the unique solution X = y.
~ with the subset of V consisting of the variables in X.
~ I do this
1. I am implicitly identifying the vector X
~
is
the
restriction
of
R
to
the
variables
in
V
−
X.
throughout the paper. R|V−X
~

319

Halpern

In this paper, I consider three successively more general classes of causal models for a
given signature S = (U, V, R).
1. Trec(S): the class of recursive causal models over signature S,
~ ⊆ V, ~x, and ~u, the
2. Tuniq(S): the class of causal models T over S where for all X
u) have a unique solution,
equations in TX←~
~ x (~
3. T (S): the class of all causal models over S.
I often omit the signature S when it is clear from context or irrelevant, but the reader
should bear in mind its important role.
Why should we be interested in causal models that do not possess unique solutions?
Are there real causal systems that do not possess unique solutions? The issue of whether
nonrecursive system can be given a causal interpretation is discussed at some length by
Strotz and Wold (1960). They argue that there are reasonable ways of interpreting causal
interpretations where the answer is yes. Under these interpretations, there may well be
more than one solution to the equations. Perhaps the best way to view such equations is to
think of the variables in V as being mutually interdependent; changing any one of them may
cause a change in the others. (Think of demand and supply in economics or populations of
rabbits and wolves.) The solutions to the equations then represent equilibrium situations.
If there is more than one equilibrium, there will be more than one solution to the equations.
Of course, if there are no equilibria, then there will be no solutions to the equations.
A related way of thinking about these equations is that they represent atemporal versions
of temporal causal equations. That is, suppose that we replace every variable Y ∈ U ∪ V by
a family of variables Y0 , Y1 , Y2, . . ., where, intuitively, Yt represents the value of Y at time t.
Each equation fX ∈ F is then replaced by a family of equations fXt , where fXt depends only
on exogenous variables Ut0 with t0 ≤ t and endogenous variables Yt0 with t0 < t. This gives
us a recursive system. The values of Xt under some setting of the variables with subscript 0
represents the evolution of X under that setting of the variables. If Xt eventually stabilizes,
then we might expect the equilibrium value to be the value of X in some solution to the
original set of equations. If Xt stabilizes, then there would not be a solution to the original
set of equations.
2.2 Syntax
I focus here on two languages. Both languages are parameterized by a signature S. The
first language, L+ (S), borrows ideas from dynamic logic (Harel, 1979). Again, I often write
L+ rather than L+ (S) (and similarly for the other languages defined below) to simplify the
notation. A basic causal formula is one of the form [Y1 ← y1 , . . . , Yk ← yk ]ϕ, where ϕ is
a Boolean combination of formulas of the form X(~u) = x, Y1 , . . . , Yk , X are variables in V,
Y1 , . . . , Yk are distinct, x ∈ R(X), and ~u is a vector of values for all the variables in U. I
~ ←~
typically abbreviate such a formula as [Y
y ]ϕ. The special case where k = 0 (which is
~
allowed) is abbreviated as [true]ϕ. [Y ← ~
y ]X(~u) = x can be interpreted as “in all possible
solutions to the structural equations obtained after setting Yi to yi , i = 1, . . ., k, and the
exogenous variables to ~u, random variable X has value x”. As we shall see, this formula
is true in a causal model T if in all solutions to the equations in TY~ ←~y (~u), the random
320

Axiomatizing Causal Structures

variable X has value x. Note that this formula is trivially true if there are no solutions
to the structural equations. A causal formula is a Boolean combination of basic causal
formulas.
~ ← ~y i(X(~u) = x) to be an
Just as with dynamic logic, we can also define the formula hY
~ ← ~y]¬(X(~u) = x). hY
~ ←~
~ ←~
abbreviation of ¬[Y
y i(X(~u) = x) is the dual of [Y
y ](X(~u) =
x); it is true if, in some solution to the structural equations obtained after setting Yi to
yi , i = 1, . . ., k, and the exogenous variables to ~u, random variable X has value x. Taking
true(~u) to be an abbreviation for X(~u) = x ∨ X(~u) 6= x for some variable X and x ∈ R(X),
~ ←~
and taking false(~u) to be an abbreviation for ¬true(~u), we have that hY
y itrue(~u) is true
if there is some solution to the equations obtained by setting Yi to yi , i = 1, . . . , k, and
~ ←~
the variables in U to ~u (since [Y
y ]false(~u) says that in every solution to the equations
obtained by setting Yi to yi and U to ~
u, the formula false(~u) is true, and thus holds exactly
if the equations have no solution).
Let Luniq(S) be the sublanguage of L+ (S) which consists of Boolean combinations of
~ ← ~y]X(~u) = x. Thus, the difference between Luniq and L+ is
formulas of the form [Y
~ ← ~
y ], while in L+ , arbitrary Boolean
that in Luniq, only X(~u) = x is allowed after [Y
combinations of formulas of the form X(~u) = x are allowed. As we shall see, for reasoning
about causality in Tuniq, the language Luniq is adequate, since it is equivalent in expressive
power to L+ . However, this is no longer the case when reasoning about causality in T .
~ ← ~y ]X(~u) = x as X ~ (~u) = x.
Following Galles and Pearl’s notation, I often write [Y
Y ←~
y
~ is clear from context or irrelevant, I further abbreviate this as X~y (~u) = x. (This is
If Y
actually the notation used by Galles and Pearl.) Let LGP (S) be the sublanguage of Luniq(S)
consisting of just conjunctions of formulas of the form X~y (~u) = x. In particular, it does not
contain disjunctions or negations of such formulas. Although Galles and Pearl (1998) are
not explicit about the language they are using, it seems to be LGP .2
2.3 Semantics
A formula in L+ (S) is true or false in a causal model in T (S). As usual, we write T |= ϕ
if the causal formula ϕ is true in causal model T . For a basic causal formula, we have
~ ← ~y](X(~u) = x) if in all solutions to T ~ (~u) (i.e., in all vectors of values for the
T |= [Y
Y ←~
y
~

~ that simultaneously satisfy all the equations F Y ←~y , for Z ∈ V − Y~ ), the
variables in V − Y
Z
variable X has value x. We define the truth value of arbitrary causal formulas, which are
just Boolean combinations of basic causal formulas, in the obvious way:
• T |= ϕ1 ∧ ϕ2 if T |= ϕ1 and T |= ϕ2
• T |= ¬ϕ if T 6|= ϕ.
As usual, we say that a formula ϕ is valid with respect to a class T 0 of causal models if
T |= ϕ for all T ∈ T 0 .
I can now make precise the earlier claim that in Tuniq (and hence Trec), the language
Luniq is just as expressive as the full language L+ .

Lemma 2.2: The following formulas are valid in Tuniq:
2. This was confirmed by Judea Pearl [private communication, 1997].

321

Halpern

~ ← ~y](ϕ ∨ ψ) ⇔ [Y
~ ←~
~ ←~
(a) Tuniq |= [Y
y ]ϕ ∨ [Y
y ]ψ,
~ ← ~y](ϕ ∧ ψ) ⇔ [Y
~ ←~
~ ←~
(b) Tuniq |= [Y
y ]ϕ ∧ [Y
y ]ψ,
~ ← ~y]¬ϕ ⇔ ¬[Y
~ ←~
(c) Tuniq |= [Y
y ]ϕ.
Hence, in Tuniq, every formula in L+ is equivalent to a formula in Luniq.
Proof: Straightforward; left to the reader.
~ ← ~
Note that it follows from these equivalences that in Tuniq, [Y
y ]ϕ is equivalent to
~
hY ← ~yiϕ. It is also worth noting that Lemma 2.2(b) holds in arbitrary causal models in
T , not just in Tuniq. However, parts (a) and (c) do not, as the following example shows.
Example 2.3: Let S = (∅, {X, Y }, R), where R(X) = R(Y ) = {0, 1}; let T = (S, F ),
where FX is characterized by the equation X = Y and FY is characterized by the equation
Y = X. Clearly T ∈
/ Tuniq; both (0, 0) and (1, 1) are solutions to T . It is easy to see that
T |= [true](X = 0 ∨ X = 1) ∧ ¬[true](X = 0) ∧ ¬[true](X = 1), showing that part (a) of
Lemma 2.2 does not hold in T , and that T |= ¬[true](X = 1) ∧ ¬[true]¬(X = 1), showing
that part (c) does not hold either.

3. Complete Axiomatizations
I briefly recall some standard definitions from logic. An axiom system AX consists of a
collection of axioms and inference rules. An axiom is a formula (in some predetermined language L), and an inference rule has the form “from ϕ1, . . . , ϕk infer ψ,” where ϕ1, . . . , ϕk , ψ
are formulas in L. A proof in AX consists of a sequence of formulas in L, each of which is
either an axiom in AX or follows by an application of an inference rule. A proof is said to
be a proof of the formula ϕ if the last formula in the proof is ϕ. We say ϕ is provable in AX,
and write AX ` ϕ, if there is a proof of ϕ in AX; similarly, we say that ϕ is consistent with
AX if ¬ϕ is not provable in AX.
An axiom system AX is said to be sound for a language L with respect to a class T 0
of causal models if every formula in L provable in AX is valid with respect to T 0 . AX is
complete for L with respect to T 0 if every formula in L that is valid with respect to T 0 is
provable in AX.
We now want to find axioms that characterize the classes of causal models in which we
are interested, namely Trec, Tuniq, and T . To deal with Trec, it is helpful to define Y
Z,
read “Y affects Z”, as an abbreviation for the formula

;

∨X⊆V,~
~
x∈×

u∈×U ∈U R(U ),z6=z
X∈V R(X),y∈R(y),~

0 ∈R(Z)

(Z~xy (~u) = z 0 ∧ Z~x (~u) = z).

Thus, Y affects Z if there is some setting of the exogenous variables and some other endogenous variables for which changing the value of Y changes the value of Z. This definition is
used in axiom C6 below, which characterizes recursiveness.
Consider the following axioms:
C0. All instances of propositional tautologies.
C1. X~y (~u) = x ⇒ X~y (~u) 6= x0 if x, x0 ∈ R(X), x 6= x0
322

(equality)

Axiomatizing Causal Structures

C2. ∨x∈R(X)X~y (~u) = x

(definiteness)

C3. (W~x(~u) = w ∧ Y~x (~u) = y) ⇒ Y~xw (~u) = y

(composition)

C4. Xxw~ (~u) = x

(effectiveness)

C5. (Y~xw (~u) = y ∧ W~xy (~u) = w) ⇒ Y~x (~u) = y

(reversibility)

C6. (X0

;X

1

∧ . . . ∧ Xk−1

; X ) ⇒ ¬(X ; X )
k

k

(recursiveness)

0

We have one rule of inference:
MP. From ϕ and ϕ ⇒ ψ, infer ψ

(modus ponens)

C1 just states an obvious property of equality: if X = x for every solution of the
equations in T~x (~u), then we cannot have X = x0 , if x0 6= x.3 In a richer language, this could
have been expressed as (X~y (~u) = x ∧ X~y(~u) = x0 ) ⇒ (x = x0 ), but this formula is not in L+
(since L+ does not include expressions such as x0 = x). C2 states that there is some value
x ∈ R(X) which is the value of X in all solutions to the equations in T~x(~u). C2 is not valid
in T , but it is valid in Tuniq. Note that in stating C2, I am making use of the fact that
R(X) is finite (otherwise C2 would involve an infinite disjunction, and would no longer be a
formula in Luniq). In fact, it can be shown that if we allow signatures where the sets R(X)
are infinite, we include C2 only for those random variables X such that R(X) is finite.4
C3–C5 were introduced by Galles and Pearl (1997, 1998), as were their names. Roughly
speaking, C3 says that if the value of W is w in all solutions to the equations T~x(~u), then
all solutions to the equations in T~xw (~u) are the same as the solutions to the equations in
T~x (~u). C3 is valid in T as well as Tuniq. As we shall see, a variant of C3 (obtained by
replacing “all” by “some”) is also valid in T . C4 simply says that in all solutions obtained
after setting X to x, the value of X is x. C5 is perhaps the least obvious of these axioms;
~ to ~x and W
the proof of its soundness is not at all straightforward. It says that if setting X
~
to w results in Y having value y and setting X to ~
x and Y to y results in W having value
~ to x (and W must already have value
w, then Y must already have value when we set X
w).
Finally, it is easy to see that C6 holds in recursive models. For if Y
Z, then Y must
X1 ∧ . . . ∧ Xk−1
Xk , then X0 must
precede Z in the causal ordering. Thus, if X0
precede Xk in the causal ordering, so Xk cannot affect X0. Thus, ¬(Xk
X0 ) holds. As
we shall see, in a precise sense, C6 characterizes recursive models.
C6 can be viewed as a collection of axioms (actually, axiom schemes), one for each k.
The case k = 1 already gives us ¬(Y
Z) ∨ ¬(Z
Y ) for all variables Y and Z. That

;

;

;

;
;

;

3. In an earlier draft of this paper, where C1 and C2 were introduced, C1 was called “uniqueness”. Galles
and Pearl (1998) then adopted this name as well. In retrospect, this axiom really does not say anything
about uniqueness. The axiom which does is D10, which will be discussed later.
Y used in C6 to
4. The assumption that R(X) and V are finite is also necessary for the abbreviation X
be in Luniq ; however, we can replace C6 by the axiom scheme

;

ui ) = zi ∧ (Xi+1 )~yi = zi0 ) ∧ (X0 )~yk xk (~
uk ) = zk ∧ (X0 )~yk = zk0 ),
¬(∧k−1
yi xi (~
i=0 (Xi+1 )~
where xi ∈ R(Xi) for i = 1, . . . , k. That is, we essentially replace C6 by all its instances. This axiom is
equivalent to C6 (although not as transparent) and can be expressed even if |V| is infinite or |R(X)| is
infinite for some X ∈ V.

323

Halpern

is, it tells us that, for any pair of variables, at most one affects the other. However, just
restricting C6 to the case of k = 1 does not suffice to characterize Trec, as the following
example shows.
Example 3.1: Let S = (∅, {X0, X1, X2}, R), where R(X0) = R(X1) = R(X2) = {0, 1, 2},
and let T = (S, F ), where FXi is characterized by the equation
(

Xi =

2 if Xi⊕1 = 1
0 otherwise

and ⊕ is addition mod 3. It is easy to see that T ∈ Tuniq: If any of the variables are set, the
equations completely determine the values of all the other variables. On the other hand, if
none of the variables are set, it is easy to see that (0, 0, 0) is the only solution that satisfies all
the equations. Moreover, in TX←~
~ x , the variable Xi is 0 unless it is set to a value other than
0 or Xi⊕1 is set to 1. It easily follows that Xi is affected only by Xi⊕1 . A straightforward
verification (or an appeal to Theorem 3.2 below) shows that T satisfies all the axioms other
than C6. C6 does not hold in T , since T |= X0
X1 ∧ X1
X2 ∧ X2
X0. This also
shows that T is not recursive. However, the restricted version of C6 (where k = 1) does
hold in T . A generalization of this example (with k random variables rather than just 2)
can be used to show that we cannot bound k at all in C6; we need C6 to hold for all finite
values of k.

;

;

;

Let AXuniq(S) consist of C0–C5 and MP; let AXrec (S) consist of C0–C4, C6, and MP.
We could include C5 in AXrec(S); I did not do so because, as Galles and Pearl (1998) point
out, it follows from C3 and C6. Note that the signature S is a parameter of the axiom
system, just as it is for the language and the set of models. This is because, for example,
the set R(X) (which is determined by S) appears explicitly in C1 and C2.
Theorem 3.2: AXuniq(S) (resp., AXrec(S)) is a sound and complete axiomatization for
Luniq(S) with respect to Tuniq(S) (resp., Trec(S)).
Proof: See the appendix.
As I said in the introduction, Galles and Pearl (1998) prove a similar completeness result
for causal models whose variables satisfy a fixed causal ordering. Given a total ordering ≺
on the variables in V, consider the following axiom:
Ord. Y~xw (~u) = Y~x (~u) if Y ≺ W

;

Since ~x, w, and ~u are implicitly universally quantified in Ord, this axiom says that ¬(W
Y ) holds if Y ≺ W . It follows that if W
Y , then W ≺ Y . From this and the fact that ≺
is a total order, it is easy to see that Ord implies C6.
Galles and Pearl show that C1–C4 and Ord is a sound and complete axiomatization
with respect to the class of causal models satisfying Ord for LGP . More precisely, Galles
and Pearl take AC to consist of the axioms C1–C4 and Ord (but not C0 or MP), and show,
in their notation, that S |= σ implies S `AC σ, where S ∪ {σ} is a set of formulas in LGP .
There is an important subtle point worth stressing about their result: C1 and C2, which

;

324

Axiomatizing Causal Structures

are axioms in AC , are not expressible in LGP (since their statement involves disjunction
and negation).
So what exactly is Galles and Pearl’s result saying? They interpret S |= σ, as usual,
as meaning that in all causal models satisfying S, σ is true.5 They interpret S `AC σ as
meaning that σ is provable from S and the axioms in the axioms of AC “together with the
rules of logic”, which presumably means C0 and MP. It follows easily from Theorem 3.2
that their result is correct (see below), but it is unlike typical soundness and completeness
proofs, since the proof of σ from S will in general involve formulas in Luniq that are not in
LGP . (In particular, this will happen whenever C1–C3 are used in the proof.)
To see that Galles and Pearl’s result follows from Theorem 3.2, define S ∗ to be the
formula in Luniq(S) which is the conjunction of the formulas in S (there can only be finitely
many, since LGP (S) itself has only finitely many distinct formulas), together with the conjunction of all the instances of the axiom Ord (again, there are only finitely many). Note
that S |= σ holds iff Tuniq(S) |= S ∗ ⇒ σ (since the formulas in Ord guarantee that the only
causal models that satisfy S ∗ are recursive, and hence are in Tuniq(S)). Thus, by Theorem 3.2, S |= σ iff AXuniq(S) ` S ∗ ⇒ σ. The latter statement is equivalent to S `AC σ,
as defined by Galles and Pearl. In fact, Theorem 3.2 shows that AXuniq(S) + Ord gives
a sound and complete axiomatization with respect to causal models satisfying Ord for the
language Luniq(S), which allows Boolean connectives. (Of course, Theorem 3.2 shows more,
since it extends Galles and Pearl’s result to Trec(S) and Tuniq(S).) This suggests that Luniq
is a more appropriate language for reasoning about causality than LGP , at least for causal
models in Tuniq. LGP cannot express a number of properties of causal reasoning of interest
(for example, the ones captured by axioms C1–C3). When we use Luniq , not only is every
formula in Luniq valid in Tuniq provable from the axioms in AXuniq, but the proof involves
only formulas in Luniq.
What about T ? I have not been able to find a complete axiomatization for the language
Luniq with respect to T . However, I do not think that finding a complete axiomatization
for Luniq with respect to T is of great interest, because Luniq is simply not a language
appropriate for reasoning about causality in T . Because there is not necessarily a unique
solution to the equations that arise in a causal model T ∈ T , it is useful to be able to say
both that there exists a solution with certain properties and that all solutions have certain
properties. This is precisely what the language L+ lets us do.6 As I now show, there is in
fact an elegant sound and complete axiomatization for L+ with respect to T .
Consider the following axioms:
D0. All instances of propositional tautologies.
~ ← ~y](X(~u) = x ⇒ X(~u) 6= x0 ) if x, x0 ∈ R(X), x 6= x0
D1. [Y

(functionality)

~ ← ~y](∨x∈R(X)X(~u) = x)
D2. [Y

(definiteness)

~ ← ~xi(W (~u) = w ∧ Y
~ (~u) = ~y) ⇒ hX
~ ←~
~ (~u) = ~
D3. hX
x; W ← wi(Y
y)

(composition)

5. Although they do not say this explicitly, it is clear that they intend to further restrict to casual models
satisfying S and Ord, for the fixed order ≺. Without this restriction, their result is not true.
6. Note that L+ allows us to say that there is a unique solution for a random variable X after setting some
~ ← ~yitrue(~
~ ←~
other variables. For example, hY
u) ∧ [Y
y ](X(~
u) = x) says that there are solutions to the
~ is set to ~y and U is set to ~
equations when Y
u and, in all of them, X is uniquely determined to be x.

325

Halpern

~ ← w;
D4. [W
~ X ← x](X(~u) = x)

(effectiveness)

~ ← ~x; Y ← yi(W (~u) = w ∧ Z(~
~ u) = ~
~ ←~
~ u) = ~z))
D5. (hX
z ) ∧ hX
x; W ← wi(Y (~u) = y ∧ Z(~
~
~
~
~
⇒ hX ← ~xiW (~u) = w ∧ Y (~u) = y ∧ Z(~u) = ~z)), where Z = V − (X ∪ {W, Y })
(reversibility)
D6. (X0

;X

1

∧ . . . ∧ Xk−1

; X ) ⇒ ¬(X ; X )
k

k

0

~ ← ~x]ϕ ∧ [X
~ ← ~x](ϕ ⇒ ψ)) ⇒ [X
~ ← ~x]ψ
D7. ([X
~ ← ~x]ϕ if ϕ is a propositional tautology
D8. [X

(recursiveness)
(distribution)
(generalization)

~ ← ~yitrue(~u) ∧ ∨x∈R(X)[Y
~ ←~
D9. hY
y ](X(~u) = x) if Y = V − {X}
(unique solutions for V − {X})
~ ← ~yitrue(~u) ∧ ∨x∈R(X)[Y
~ ←~
D10. hY
y ](X(~u) = x)

(unique solutions)

~ ←~
~ ← ~y iϕk (~uk ), if ϕi (~ui)
~ ← ~yi(ϕ1(~u1 ) ∧ . . . ∧ ϕk (~uk )) ⇔ (hY
y iϕ1(~u1) ∧ . . . ∧ hY
D11. hY
is a Boolean combination of formulas of the form X(~ui) = x and ~ui 6= ~
uj for i 6= j
(separability)
D1–D6 are the analogues of C1–C6 in L+ . D4 and D6 are just C4 and C6, with no
changes at all. The other axioms are not quite the same though. For example, C1 is
~ ← ~y](X(~u) = x) ⇒ ¬[Y
~ ←~
actually [Y
y ](X(~u) = x0 ) if x 6= x0 . By Lemma 2.2, this is
equivalent to D1 in Tuniq; however, the two formulas are not equivalent in general. Similarly,
~ ← ~y ](X(~u) = x), which is closer to D10 than D2 (since the disjunction
C2 is ∨x∈R(X)[Y
~ ← ~y]). Again, D10 and D2 are equivalent in Tuniq (both are
is outside the scope of the [Y
equivalent to C2 in this case) but, in general, D10 is stronger than D2. Only D2 and D9,
both weaker than D10, hold in T . The exact analogue of C3 would use [ ] instead of h i and
~ (~u) = ~
say Y (~u) = y instead of Y
y . For completeness, it is necessary to have a vector of
variables here. Using [ ] instead of h i also results in a valid formula (and would not require a
~ ). While the two variants are equivalent in Tuniq, they are different in general, and
vector Y
the one given here is the more useful. (More precisely, with it we get completeness, while
the version with [ ] does not suffice for completeness.) Similarly, in D5, we use h i instead of
~ u) = ~z. Both turn out to be necessary for soundness. In some
[ ], and add the extra clause Z(~
sense, we can think of D1–D6 as capturing the “true content” of C1–C6, once we drop the
assumption that the structural equations have a unique solution. D7 and D8 are standard
properties of modal operators. D10 is what we need to capture the fact that the structural
equations have unique solutions. D11 essentially says that the solutions to the equations
that arise when the exogenous variables are set to ~u are independent of the solutions that
arise when the exogenous variables are set to ~
u0 6= ~u.
+
Let AX consist of D0–D5, D7–D9, D11, and MP (modus ponens); let AX+
uniq be the
+
+
+
result of adding D10 to AX ; let AXrec be the result of adding D6 to AXuniq.
+
Theorem 3.3: AX+ (S) (resp., AX+
uniq(S), AXrec (S)) is a sound and complete axiomati+
zation for L (S) with respect to T (S) (resp., Tuniq(S), Trec(S)).

Proof: See the appendix.

326

Axiomatizing Causal Structures

4. Decision Procedures
In this section I consider the complexity of deciding if a formula is satisfiable (or valid).
This, of course, depends on the language (L+ , Luniq, or LGP ) and the class of models (Trec,
Tuniq, T ) we consider. It also depends on how we formulate the problem.
One version of the problem is to consider a fixed signature S = (U, V, R), and ask how
hard it is to decide if a formula ϕ ∈ L+ (S) (resp., Luniq(S), LGP (S)) is satisfiable in Trec(S)
(resp., Tuniq(S), T (S)). If S is finite (that is, if V and U are finite and R(Y ) is finite for
each Y ∈ U ∈ V), this turns out to be quite easy, for trivial reasons.
Theorem 4.1: If S is a fixed finite signature, the problem of deciding if a formula ϕ ∈
L+ (S) (resp., Luniq(S), LGP (S)) is satisfiable in Trec(S) (resp., Tuniq(S), T (S)) can be
solved in time linear in |ϕ| (the length of ϕ viewed as a string of symbols).
Proof: If S is finite, there are only finitely many causal models in T (S), independent of
ϕ. Given ϕ, we can explicitly check if ϕ is satisfied in any (or all) of them. This can be
done in time linear in |ϕ|. Since S is not a parameter to the problem, the huge number of
possible causal models that we have to check affects only the constant.
We can do even better than Theorem 4.1 suggests if S is a fixed finite signature. Suppose
that V consists of 100 variables and ϕ mentions only 3 of them. A causal model must specify
the equations for all 100 variables. Is it really necessary to consider what happens to the
97 variables not mentioned in ϕ to decide if ϕ is satisfiable or valid? As the following result
shows, if we restrict to models in Tuniq, then we need to check only the variables that appear
in S. Given a signature S = (U, V, R), let Sϕ = ({U ∗}, Vϕ, Rϕ), where Vϕ consists of the
variables in V that appear in ϕ, U ∗ is a fresh exogenous variable, not mentioned in V or U,
Rϕ (X) = R(X) for X ∈ Vϕ , and Rϕ(U ∗ ) consists of all those tuples in ×U ∈U R(U ) that are
mentioned in ϕ.
Theorem 4.2: A formula ϕ ∈ L+ (S) is satisfiable in Trec(S) (resp., Tuniq(S)) iff it is
satisfiable in Trec(Sϕ) (resp., Tuniq(Sϕ)).
Proof: See the appendix.
The analogue to Theorem 4.2 does not hold for T . For example, suppose that S =
(∅, {X, Y, Z}, R), where R(X) = R(Y ) = R(Z) = {0, 1}, and ϕ is the formula hX ←
0i(Y = 0) ∧ hX ← 0i(Y = 1). It is easy to see that there is a causal model in T (S)
satisfying ϕ. For example, if T = (S, F ), where FX (y, z) = y ⊕ z, FY (x, z) = x ⊕ z and
FZ (x, y) = x ⊕ y, and ⊕ represents addition mod 2, then it is easy to check that T |= ϕ.
On the other hand, there is no causal model T 0 ∈ T (Sϕ) such that T 0 |= ϕ. For suppose
T 0 |= ϕ and T 0 = (Sϕ, F 0). Since T 0 |= hX ← 0i(Y = 0), we must have FY0 (0) = 0; since
T 0 |= hX ← 0i(Y = 1), we must have FY0 (0) = 1. But we cannot have both FY0 (0) = 0 and
FY0 (1) = 1, since FY0 is a function.
There is a variant of Theorem 4.2 that does hold for T that does give us a bound
on the number of variables we need to consider. Given a signature S = (U, V, R), define
||S|| = ×X∈V |R(X)| (where we take ||S|| = ∞ if either V is infinite or |R(X)| = ∞
+
for some X ∈ V). If ||S|| > ||Sϕ||2 + ||Sϕ||, let Sϕ+ = ({U ∗}, Vϕ+, R+
ϕ ), where Vϕ is Vϕ as
327

Halpern

∗
defined above together with one fresh endogenous variable X ∗, R+
ϕ (X ) = ×X∈Vϕ R(X), and
+
∗
∗
2
+
∗
0
Rϕ (U ) = Rϕ(U ). If ||S|| ≤ ||Sϕ|| + ||Sϕ||, let Sϕ = ({U }, V, R ), where R0 (X) = R(X)
for X ∈ V and R0 (U ∗) = Rϕ (U ∗).

Theorem 4.3: A formula ϕ ∈ L+ (S) is satisfiable in T (S) iff ϕ is satisfiable in T (Sϕ+ ).
Proof: See the appendix.
Note that if ||S|| ≤ ||Sϕ||2 + ||Sϕ||, then, since we have assumed (without loss of generality) that |R(X)| ≥ 2 for each variable X, it must be the case that there are at most
2 log2 (||Sϕ||) + 1 variables in signature S.
Since Theorems 4.2 and 4.3 apply to all formulas in L+ (S), they apply a fortiori to
formulas in Luniq(S) and LGP (S). Although stated only in terms of satisfiability, it is
immediate that they also hold for validity. Thus, they tell us that, without loss of generality,
when considering satisfiability or validity, we need to consider only finitely many variables
(essentially, the ones that appear in ϕ, and perhaps a few more). In this sense, we can
restrict to signatures with only finitely many variables without loss of generality. Note that
these results do not tell us that we can restrict to finite sets of values for these variables
without loss of generality.
Returning to the complexity of the decision problem, note that Theorem 4.1 is the analogue of the observation that for propositional logic, the satisfiability problem is in linear
time if we restrict to a fixed set of primitive propositions. The proof that the satisfiability problem for propositional logic is NP-complete implicitly assumes that we have an
unbounded number of primitive propositions at our disposal.
There are two ways to get an analogous result here. The first is to allow the signature
S to be infinite and the second is to make the signature part of the input to the problem.
The results in both cases are similar, so I just consider the case where the signature is part
of the input here.
Theorem 4.4: Given as input a pair (ϕ, S), where ϕ ∈ L+ (S) (resp., Luniq(S)) and S is a
finite signature, the problem of deciding if ϕ is satisfiable in Trec(S) (resp., Tuniq(S), T (S))
is NP-complete (resp., NP-hard) in |ϕ|; if ϕ ∈ LGP (S), then the problem of deciding if ϕ is
satisfiable in Trec(S) (resp., Tuniq(S)) is NP-complete (resp., NP-hard).
Proof: See the appendix.
I believe that the problem of deciding if a formula ϕ in Luniq(S) or L+ (S) is satisfiable in
Tuniq(S) and T (S) is NP-complete, as is the case of deciding if ϕ ∈ LGP (S) is satisfiable in
Tuniq(S). However, I have not been able to show this. What about the satisfiability problem
for formulas in LGP in T (S)? This may well be in constant time! Indeed, if S is an infinite
signature (that is, if S = (U, V, R) and |V| = ∞), then it is provably in constant time. The
point is that a formula in LGP (S) is trivially satisfiable in a structure T ∈ LGP (S) where
~ ← ~x, the equations in T ~
for all settings X
X←~
x have no solutions, and there always is such
model structure if S has infinitely many variables. If S has only finitely many variables, we
do not have such trivial models, but it may still be possible to show that a “trivial enough”
model exists that satisfies the formula. This just emphasizes that LGP (S) is simply too
weak a language to reason about models in T (S).
328

Axiomatizing Causal Structures

5. Conclusion
I have provided complete axiomatizations and decision procedures for propositional languages for reasoning about causality. I have tried to stress the important role of the choice
of language (and the signature) in both the axiomatizations and, more generally, in the
reasoning process.
Both the models and the languages considered here are somewhat limited. For example,
a more general approach to modeling causality would allow there to be more than one value
of X once we have set all the other variables. This would be appropriate if we model things
at a somewhat coarser level of granularity, where the values of all the variables other than
X do not suffice to completely determine the value of X. I believe the results of this paper
can be extended in a straightforward way to deal with this generalization, although I have
not checked the details. For general causal reasoning, I believe we need a richer language,
which includes some first-order features. I hope to return to the issue of finding appropriate
richer languages for causal reasoning in future work.

Acknowledgments
I’d like to thank Judea Pearl for his comments on a previous version of this paper, as well as
his generous help in providing pointers to the literature. This work was supported in part
by NSF under grant IRI-96-25901 and by the Air Force Office of Scientific Research under
grant F49620-96-1-0323. A preliminary version of this paper appears in Proc. Fourteenth
Conference on Uncertainty in AI, pp. 202–210, 1998.

Appendix A. Proofs
Theorem 3.2: AXuniq (resp., AXrec) is a sound and complete axiomatization for Luniq(S)
with respect to Tuniq(S) (resp., Trec(S)).
Proof: Soundness is proved by Galles and Pearl. To make the paper self-contained, I
reprove the only non-obvious case—the validity of C5 in Tuniq.
Let T ∈ Tuniq and suppose that T |= Y~xw (~u) = y ∧ W~xy (~u) = w. We want to show
that T |= Y~x (~u) = y. Since we are in Tuniq, there is a unique vector ~v1 that satisfies the
equations in T~xw (~u) and a unique vector ~v2 that satisfies the equations in T~xy (~u). I claim
~ Y , and W components of these vectors are the same
that ~v1 = ~v2 . By assumption, the X,
(~x, y, and w, respectively). Now consider the T~xyw (~u). I claim that ~v1 and ~v2 are both
solutions to the equations in that causal theory. Note that for any variable Z other than
~ ∪ {W, Y }, the equation F ~xw,~u for Z in T~xw (~u) is the same as the equations F ~xy,~u
those in X
Z
Z
and FZ~xyw,~u for Z in T~xy (~u) and T~xyw (~u), respectively, except that in the first case, w has
been plugged in as the value of W , in the second case y has been plugged in as the value of
Y , and in the third case, both w and y have been plugged in. However, since w and y are
the values of W and Y , respectively, in both ~v1 and ~v2, and since these vectors satisfy both
equation FZ~xw and FZ~xy , they must also satisfy FZ~xwy . Since the equations for T~xyw (~u) have
a unique solution, we have that ~v1 = ~v2 , as desired.
329

Halpern

Next, I claim that ~v1 satisfies the equations in T~x (~u). Again, as above, it is clear that
~ ∪ {W, Y }. A similar argument shows that it satisfies the
it satisfies the equation for Z ∈
/X
equation for Y in T~x (~u), since ~v1 satisfies the equation for Y in T~xw (~u). Finally, a similar
argument shows that it satisfies the equation for W in T~x (~u), since ~v2 = ~v1 satisfies the
equation for W in T~xy (~u). Since the Y component of ~v1 is y, it follows that Y~x (~u) = y.
So much for soundness. For completeness, as usual, it suffices to prove that if a formula
in Luniq is consistent with AXuniq (resp., AXrec), then it is satisfied in a causal model in
Tuniq (resp., Trec). (Here’s the argument: We want to show that every valid formula is
provable. Suppose that we have shown that every consistent formula is satisfiable and that
ϕ is valid. If ϕ is not provable, then ¬ϕ is consistent. By assumption, this means that ¬ϕ
is satisfiable, contradicting the assumption that ϕ is valid.)
I now give the argument in the case of AXuniq.
Suppose that a formula ϕ ∈ Luniq(S), with S = (U, V, V ), is consistent with AXuniq.
Consider a maximal consistent set C of formulas that includes ϕ. (A maximal consistent set
is a set of formulas whose conjunction is consistent such that any larger set of formulas would
be inconsistent.) It follows easily from standard propositional reasoning (i.e., using C0 and
MP only) that such a maximal consistent set exists. Moreover, from C1 and C2, it follows
that for each random variable X ∈ V and vector ~
y of values, there exists exactly one element
x ∈ R(X) such that X~y = x ∈ C. I now construct a causal model T = (S, F ) ∈ Tuniq(S)
that satisfies every formula in C (and, in particular, satisfies ϕ).
~ consists of all the variables in V − {X}. Thus,
A term XY~ ←~y (~u) is complete (for X) if Y
XY~ ←~y (~u) is a complete term if every random variable other than X is determined. We use
the complete terms to define the structural equations. For each variable in X ∈ V, define
FX (~u, ~y) = x if X~y (~u) = x, where X~y (~u) is a complete term. This gives us a causal model
T . Now we have to show that this model is in Tuniq and that all the formulas in C are
satisfied by T .
~ |. The
I show that XY~ ←~y (~u) = x is in C iff T |= XY~ ←~y (~u) = x by induction on |V| − |Y
~ | = 0 follows immediately from C4, since then X is in Y
~ . If |V| −|Y
~| =
case where |V| −|Y
6 0,
we can assume without loss of generality that X is not in Y~ , for otherwise the result again
~ | = 1, the result follows by definition of
follows from C4. Given this assumption, if |V| − |Y
the equations FX .
~ | = k > 1. We want to show that there is a
For the general case, suppose that |V| − |Y
unique solution to the equations in TY~ ←~y (~u) and that, in this solution, X has value x. To
see that there is a solution, we define a vector ~v and show that it is in fact a solution. If
W ∈ Y~ and W ← w is the assignment to W in Y~ ← ~
y , then we set the W component of ~v
to w. If W is not in Y~ , then set the W component of ~v to the unique value w∗ such that
WY~ ←~y (~u) = w∗ is in C. (By C1 and C2 there is such a unique value w.) I claim that ~v is a
solution to the equations in TY~ ←~y (~u).
~ W . By C3 and C4, for every
~0 =Y
To see this, let W be a variable in V not in Y~ . Let Y
0
∗
~
~
variable Z ∈ V − Y , we have Z~yw∗ (~u) = z . Since |V| − |Y 0 | = k − 1, by the inductive
~ 0 , the
hypothesis, ~v is in fact the unique solution for T~yw∗ (~u). For every variable Z in V − Y
y w∗ ,~
~
u
~
y,~
u
equation FZ
for Z in T~yw∗ (~u) is the same as the equation FZ for Z in T~y (~u), except
y ,~
~
u
is
that W is set to w∗ . Thus, every equation in T~y (~u) except possibly the equation FW
~
y,~
u
satisfied by ~v . To see that FW is also satisfied by ~v , simply repeat this argument above
330

Axiomatizing Causal Structures

~ . (Such a variable must exist since |V| − |Y
~ | was
starting with another variable W 0 in V − Y
assumed to be at least 2.)
It remains to show that ~v is the unique solution to the equations in T~y (~u). Suppose
there were another solution, say ~v 0 , to the equations. Suppose that for each variable W
in V − Y~ , the W component of ~v 0 is w∗∗. For some variable Z, we must have z ∗∗ 6= z ∗.
Since Z~y (~u) = z ∗ , by assumption, it follows from C1 that Z~y (~u) 6= z ∗∗ is in C (since C is
a maximal consistent set). It is also easy to see that for each W in V − Y~ , the vector ~v 0 is
~.
also a solution to the equations in T~yw∗∗ (~u). Let W be a variable other than Z in V − Y
∗∗
∗∗
By the induction hypothesis, it follows that W~yz∗∗ (~u) = w and Z~yw∗∗ (~u) = z are both
in C. By C5 (reversibility), Z~y (~u) = z ∗∗ is in C. But this contradicts the consistency of C.
This completes the proof in the case of Tuniq(S). Essentially the same proof works for
Trec. We just need to observe that C6 guarantees that the theory we construct can be taken
to be recursive. To see this, given a formula ϕ consistent with Trec, consider a maximal set
C of formulas consistent with Trec that contains ϕ. Let TC be the causal model determined
by C, as above. The set C also determines a relation ≺ on the exogenous variables: define
Y ≺ Z if Y
Z ∈ C. It easily follows from C6 that the transitive closure ≺∗ of ≺ is
a partial order: if X ≺∗ Y and Y ≺∗ X, then X = Y . Any total order on the variables
consistent ≺∗ gives an ordering for which TC is recursive.

;

+
Theorem 3.3: AX+ (resp., AX+
uniq, AXrec ) is a sound and complete axiomatization for
L+ (S) with respect to T (S) (resp., Tuniq(S), Trec(S)).

Proof: Soundness proceeds much as that of Theorem 3.2; I leave details to the reader. For
completeness, we again proceed much as in the proof of Theorem 3.2. Because the proofs
are so similar in spirit, I just sketch the proof for AX+ ; the modifications for AX+
uniq and
are
left
to
the
reader.
AX+
rec
Again, given a formula ϕ consist with AX+ , we consider a maximal consistent set of
formulas containing ϕ that is consistent with AX+ , and use it to construct a causal model
T . Note that D9 suffices for this, because in defining FX (~u, ~y), we needed to know only the
~ ← ~y ](X(~u) = x) for Y~ = V − X, and D9 (together with D1) assures
unique x such that [Y
us that there is a unique such x. Again, we want to show that all the formulas in C are
satisfied by T .
~ ← ~y ϕ,
To do this, it clearly suffices to show that for every formula ψ of the form hY
we have ψ in C iff T |= ψ. We can reduce to considering even simpler formulas, namely,
~ u) = ~x, by applying some of the axioms. To see this, first
ones where ϕ has the form X(~
observe that standard arguments of modal logic (using D0, D7, D8, and MP) show that
~ ← ~y i(ϕ1 ∨ ϕ2) is provably equivalent to hY
~ ←~
~ ←~
hY
y iϕ1 ∨ hY
y iϕ2. That means we can
assume without loss of generality that ϕ is a conjunction of formulas of the form X(~u) ← x
~ ← ~yi(ϕ ∧ X(~u) 6= x) is equivalent to
and their negations. From D2 it follows that hY
0
~
hY ← ~y i(ϕ ∧ (∨x0∈R(X)−{x}X(~u) = x ). Thus, we can assume without loss of generality that
ϕ has no negations. By applying D11, we can assume without loss of generality that the
same setting ~u of the exogenous variables is used in all the conjuncts. Thus, it suffices to
~ ← ~yi(X(~
~ u) = ~x) ∈ C iff T |= hY
~ ←~
~ u) = ~x) for X
~ = V −Y
~.
show that hY
y i(X(~
~ | again. The base case is dealt with using
To do this, we proceed by induction on |V| −|Y
~
~ ←~
~ u) =
D4, as before. So assume that k ≥ 1 and |V| − |Y | = k + 1. Suppose that hY
y i(X(~
~ Suppose that X1 ← x1 and X2 ← x2 are the assignments to
~x) ∈ C. Let X1 , X2 ∈ X.
331

Halpern

~ ← ~x. Let X
~ 0 ← ~x0 and X
~ 00 ← ~x00 be the result of removing X1 ← x1
X1 and X2 in X
~ ←~
~ ←~
~ 00(~u) = ~
and X2 ← x2, respectively, from X
x. By D3, both hY
y ; X1 ← x1 i(X
x00 )
~ 0(~u) = ~x0 ) are in C. By the induction hypothesis, both of these
~ ← ~y; X2 ← x2 i(X
and hY
~ ←~
~ u) = ~x0 ),
formulas are true in T . By the soundness of D5, it follows that T |= hY
y i(X(~
as desired.
~ ←~
~ u) = ~x0 ). Then, since D3 is sound, we have
Conversely, suppose that T |= hY
y i(X(~
~ 00(~u) = ~
~ ← ~y ; X2 ← x2 i(X
~ 0(~u) = ~x0 ).
~ ← ~y; X1 ← x1i(X
x00 ) and T |= hY
that T |= hY
00
~
~
By the induction hypothesis, we have that both hY ← ~
y ; X1 ← x1i(X (~u) = ~x00 ) and
0
0
~ (~u) = ~x ) are in C. We now apply D5 to complete the proof.
~ ← ~y ; X2 ← x2 i(X
hY
Theorem 4.2: A formula ϕ ∈ L+ (S) is satisfiable in Trec(S) (resp., Tuniq(S)) iff it is
satisfiable in Trec(Sϕ) (resp., Tuniq(Sϕ)).
Proof: Clearly, if a formula is satisfiable in Trec(Sϕ) (resp., Tuniq(Sϕ )), then it is satisfiable
in Trec(S) (resp., Tuniq(S)). We can easily convert a causal model T = (Sϕ, F ) ∈ Trec(Sϕ)
satisfying ϕ to a causal model T 0 = (S, F 0) ∈ Trec(S) satisfying ϕ by simply defining
0
FX
to be a constant, independent of its arguments, for X ∈ V − Vϕ ; if X ∈ Vϕ , define
0
x ∈ ×Y ∈Vϕ −{X}R(Y ) and ~
y ∈ ×Y ∈V−Vϕ R(Y );
FX (~u, ~x, ~y) = FX (~u, ~x), where ~u ∈ R(U ∗), ~
0
if ~u ∈
/ R(U ∗), define FX
(~u, ~x, ~y) to be an arbitrary constant. An identical transformation
works for T ∈ Tuniq(Sϕ).
For the converse, suppose that ϕ is satisfiable in a causal model T = (S, F ) ∈ Trec(S).
Thus, there is an ordering ≺ on the variables in V such that if X ≺ Y , then FX is independent of the value of Y . This means we can view FX as a function of the exogenous variables
in U and the variables Y ∈ V such that Y ≺ X. Let Pre(X) = {Y ∈ V : Y ≺ X}. For
convenience, I allow FX to take as arguments the values of only the variables in U ∪Pre(X),
rather than requiring its arguments to include the values of all the variables in U ∪ V −{X}.
0
Now define functions FX
: (×U ∈U R(U )) × (×Y ∈Vϕ−{X} R(Y )) → R(X) for all X ∈ V by
induction on ≺ (that is, start with the ≺-minimal element, whose value is independent of
that of all the other variables, and work up the ≺ chains). Suppose X ∈ Vϕ and ~x is a vector
0 (~
u, ~x) = FX (~u).
of values for the variables in Vϕ − {X}. If X is ≺-minimal, then define FX
0
In general, define FX (~u, ~x) = FX (~u, ~z), where ~z is a vector of values for the variables in
Pre(X) defined as follows. If Y ∈ Pre(X) ∩ Vϕ , then the value of the Y component in ~z is
the value of the Y component in ~y ; if Y ∈ Pre(X) − Vϕ, then the value of the Y component
in ~z is FY0 (~u, ~x). (By the induction hypothesis, FY0 (~u, ~x) has already been defined.) Now
define a causal model T 0 = (Sϕ , F 0). It is easy to check that T 0 ∈ Trec(Sϕ ) (the ordering
of the variables is just ≺ restricted to Vϕ ). Moreover, the construction guarantees that if
~ ⊆ Vϕ , then the solutions to the equations T 0
u) and TX←~
u) are the same, when
X
~ x (~
~ x (~
X←~
0
restricted to the variables in Vϕ . It follows that T satisfies ϕ.
The argument in the case that T ∈ Tuniq(S) is similar in spirit. For X ∈ Vϕ , ~u ∈
0
(~u, ~x) to be the value of X in the
(×U ∈U R(U )), and ~x ∈ (×Y ∈Vϕ −{X}R(Y )), define FX
7
unique solution to the equations in TVϕ−{X}←~x (~u). It is again straightforward to check
that now T 0 = (Sϕ , F 0) ∈ Tuniq(Sϕ ) and satisfies ϕ.
0
7. This definition is easily seen to agree with the earlier definition of FX
if T ∈ Trec .

332

Axiomatizing Causal Structures

Theorem 4.3: A formula ϕ ∈ L+ (S) is satisfiable in T (S) iff ϕ is satisfiable in T (Sϕ+ ).
Proof: If ||S|| ≤ ||Sϕ||2 +||Sϕ|| then the proof is immediate, so suppose that ||S|| > ||Sϕ||2 +
||Sϕ|| and ϕ is satisfied in a causal model T = (S, F ) ∈ T (S). Before going on with the
proof, it is useful to define some notation. Let V = {X1, . . . , Xm}, where Vϕ = {X1, . . . , Xk }
and V − Vϕ = {Xk+1 , . . . , Xm}. Given a vector ~x ∈ R(X ∗) = ×X∈Vϕ R(X) and Xi ∈ Vϕ ,
let ~x−i denote the vector excluding the value for Xi . For each Xi ∈ Vϕ , choose two values
0
xi0 and xi1 in R(Xi). Define T 0 = (Sϕ , F 0) by defining FX
(~u, ~x−i , ~y−i , yi) = x, where
• x = yi if ~x−i = ~y−i and X = yi in some solution to the equations in TVϕ −{Xi }←~x−i (~u);
• x = xi0 if yi 6= xi0 and either ~x−i 6= ~
y−i or there is no solution to the equations in
TVϕ−{X}←~x−i (~u) in which X = yi ;
• x = xi1 otherwise.
Finally, define FX ∗ (~u, ~x) = ~x.
~ ⊆ Vϕ, then the solutions
I now show that the construction again guarantees that if X
0
to the equations TX←~
u) and TX←~
u) are the same, when restricted to the variables in
~ x (~
~ x (~
u), where ~
y ∈ R(X ∗)
Vϕ . First suppose that (~y, ~z) is a solution to the equations in TX←~
~ x (~
~
x and ~y agree on the variables in X,
and ~z ∈ ×Y ∈V−Vϕ R(Y ). It must be the case that ~
~
so (~y, ~z) is also a solution of the equations in TVϕ −{Xi }←~y−i (~u) if Xi ∈ Vϕ − X. Thus,
0
0
FX
(~u, ~y−i , ~y) = yi . It follows that (~
y, ~y ) is a solution to the equations in TX←~
u).
~ x (~
i
0
0
u). Then the
Conversely, suppose that (~y, ~y ) is a solution to the equations in TX←~
~ x (~
0
~
y . Moreover, since ~x and ~
y agree on the variables in X,
definition of FX ∗ guarantees that ~y = ~
0
0
(~y, ~y) must also be a solution to the equations in TVϕ−{X1 }←~y−1 (~u). Thus, FX1 (~u, ~y−1, ~y) =
z of values for the variables in V − Vϕ such
y1 , which means that there must be some vector ~
that (~y, ~z) is a solution to the equations in TVϕ −{X1 }←~y−1 (~u). But then it is easy to check
that (~y, ~z) must in fact be a solution to the equations in TVϕ −{Xi }←~y−i (~u) for all i = 1, . . ., k.
u), as desired. This suffices to
It follows that (~y, ~z) is a solution to the equations in TX←~
~ x (~
prove this direction of the theorem.
Now suppose that ϕ is satisfied in a causal model T = (Sϕ+ , F ) ∈ T (Sϕ+ ). Since ||S|| >
||Sϕ||2 + ||Sϕ||, there must be an injective function f : R(X ∗) → ×Y ∈V−Vϕ R(Y ) and two
distinct vectors ~y0 = (y01, . . . , y0k ), ~y1 = (y11, . . . , y1k ) that are not in the range of f .
Choose two distinct vectors ~x0 = (x10, . . . , xk0), ~x1 = (x11, . . . , xk1) ∈ R(X ∗). Define
T 0 = (S, F 0) ∈ T (S) as follows. If Xi ∈ Vϕ, ~x−i ∈ ×Y ∈Vϕ −{Xi } R(Y ), ~
z ∈ R(X ∗), and
~y ×Y ∈V−Vϕ R(Y ), let
0
FX
(~x−i , ~y) =
i



x−i , ~z) if f (~z) = ~
y,
 FXi (~

x

0i

 x
1i

if ~y is not in the range of f , ~
y 6= ~y1 ,
otherwise.

If Xj ∈ V − Vϕ , ~x ∈ R(X ∗) and ~y−j ∈ ×Y ∈V−Vϕ−{Xj } R(Y ), then let
0
(~x, ~y−j )
FX
j



 y

=

y

0j

 y
1j

if f (FX ∗ (~x)) = (~
y−j , y),
if f (FX ∗ (~x)) 6= (~
y−j , y 0) for all y 0 ∈ R(Xj ), ~x 6= x~0 ,
otherwise.
333

Halpern

~ ⊆ Vϕ , then the solutions to the
Again, I show that the construction guarantees that if X
0
equations TX←~
u) and TX←~
u) are the same, when restricted to the variables in Vϕ . First
~ x (~
~ x (~
u), where ~
y , ~z ∈ R(X ∗). It is easy
suppose that (~y, ~z) is a solution to the equations in TX←~
~ x (~
0
to check that (~y, f (~z)) is a solution to the equations in TX←~
u). Conversely, suppose that
~ x (~
0
u), where ~
y ∈ R(X ∗) and ~z ∈ ×Y ∈V−Vϕ R(Y ).
(~y, ~z) is a solution to the equations in TX←~
~ x (~
I claim that we must have ~z = f (FX ∗ (~
y )). If, in fact, this is the case, then it is easy to
∗
u). On the other hand, if
check that (~y, FX (~y) is a solution to the equations in TX←~
~ x (~
0 for X ∈ V − V guarantees that ~
z=~
y0 unless
~z 6= f (FX ∗ (~y)), then the definition of FX
j
ϕ
j
~y = ~x0; if ~y = ~x0 , then ~z = ~y1 . But the definition of FXi for Xi ∈ Vϕ guarantees that if
y , ~z) is a solution iff ~z = f (FX ∗ (~
y)). This
~z = ~y0 , then ~y = ~x0: otherwise, ~y = ~x1 . Thus, (~
suffices to prove the result.
Theorem 4.4: Given as input a pair (ϕ, S), where ϕ ∈ L+ (S) (resp., Luniq(S)) and S is
a finite signature, the problem of deciding if ϕ is satisfiable with respect to Trec(S) (resp.,
Tuniq(S), T (S)) is NP-complete (resp., NP-hard) in |ϕ|; if ϕ ∈ LGP (S), then the problem
of deciding if ϕ is satisfiable in Trec(S) (resp., Tuniq(S)) is NP-complete (resp., NP-hard).
Proof: The NP-lower bound is easy for L+ (S) and Luniq(S), since there is an obvious way
to encode the satisfiability problem for propositional logic into the satisfiability problem for
L+ and Luniq. Given a propositional formula ϕ with primitive propositions p1, . . . , pk , let
S = (∅, {X1, . . . , Xk }, R), where R(Xi) = {0, 1} for i = 1, . . . , k. Replace each occurrence
of the primitive proposition pi in ϕ with the formula Xi = 1. This gives us a formula ϕ0 in
Luniq(S). It is easy to see that if ϕ0 is satisfiable in a causal model T ∈ T (S) (and, a fortiori
if ϕ0 is satisfiable in a causal model T in either Trec(S) or Tuniq(S)) then the solution to the
equations in T defines a satisfying assignment for ϕ. Conversely, if ϕ is satisfiable, say by
some truth assignment v, then we can trivially construct a causal model T ∈ Trec(S) such
that FXi = v(pi). (For simplicity, I assume that valuations assign values 0 and 1 rather
than false and true.)
This trivial construction of ϕ0 will not work for LGP (S), since we do not have disjunctions
or negations available. The lack of negations does not cause a problem. We can assume
without loss of generality that the negations occur only in front of primitive propositions,
and we can capture ¬pi by the formula Xi = 0. The idea for dealing with disjunctions is that
a formula such as p1 ∨ ¬p2 ∨ p3 is translated to [X1 ← 0; X2 ← 1; X3 ← 1](Y = 0), where Y
is a fresh variable. Essentially, we are viewing p1 ∨¬p2 ∨p3 as (¬p1 ∧p2 ∧¬p3 ) ⇒ false, which
is why we write, for example, X1 ← 0 even though p1 appears positively in the disjunction.
To make matters simpler, assume that ϕ is a formula in 3-CNF. This suffices for NPhardness, since the satisfiability problem for 3-CNF formulas is also NP-hard (Garey &
Johnson, 1979). Suppose ϕ is of the form c1 ∧. . .∧cm , where each cl is a clause consisting of a
disjunction of three primitive propositions and their negations. Suppose that the primitive
propositions that appear in ϕ are p1, . . . , pk . Let S = (∅, {X1, . . . , Xk , Y1 , . . ., Ym }, R),
where R(Xi) = R(Yj ) = {0, 1} for all i, j. Suppose that cj , the jth clause of ϕ, is of the
form qj1 ∨ qj2 ∨ qj3 , where qji is either pji or ¬pji for some ji . Let ctj be the LGP formula
[Xj1 ← xj1 ; Xj2 ← xj2 ; Xj3 ← xj3 ](Yj = 0),
334

Axiomatizing Causal Structures

where xjh is 0 if qjh is pjh and xjh is 1 if qjh is ¬pjh for h = 1, 2, 3. Let ϕ0 be
[true](Y1 = 1 ∧ . . . ∧ Ym = 1) ∧ ct1 ∧ . . . ∧ ctm.
I claim that ϕ is a satisfiable propositional formula iff the LGP formula ϕ0 is satisfiable in
Trec(S) (resp. Tuniq(S)). First suppose that ϕ0 is satisfiable, say in some model T ∈ Tuniq(S).
(If this direction holds for T ∈ Tuniq(S), it clearly holds a fortiori for T ∈ Trec(S).) Let ~z be
the unique solution to the equations in T . By construction, the Yj component of ~z is 1 for
j = 1, . . ., m. Let x∗i be the value of the Xi component in ~z. Consider the valuation v such
that v(pi) = x∗i . I claim that v(ϕ) = 1. To see this, suppose that clause cj is qj1 ∨ qj2 ∨ qj3 .
If v makes qj1 , qj2 , and qj3 false, then we must have xjh = x∗jh for h = 1, 2, 3. Since
T |= [Xj1 ← xj1 ; Xj2 ← xj2 ; Xj3 = xj3 )](Yj = 0) and the value of the Xjh component of ~z is
xjh for h = 1, 2, 3, it follows that ~z is a solution to the equations in TXj1 ←xj1 ;Xj2 ←xj2 ;Xj3 ←xj3 .
But this contradicts the fact that T |= [Xj1 ← xj1; Xj2 ← xj2 ; Xj3 ← xj3](Yj = 0) (since
the Yj component of ~z is 1). It follows that v(cj ) = v(qj1 ∨ qj2 ∨ qj3 ) = 1. Since this is true
for all clauses cj , we must have that v(ϕ) = 1.
For the converse, suppose that ϕ is satisfiable, say by valuation v. I show that ϕ0 is
satisfiable in T ∈ Trec(S). Order the variables so that Xj1 , Xj2 , Xj3 ≺ Yj . (There are many
orderings of the variables that satisfy these constraints; any one will do.) Define FXi = v(pi)
(so that FXi is a constant, independent of its arguments); define FYj (xj1 , xj2 , xj3 ) = 1 if
(xj1 , xj2 , xj3 ) = (v(pj1 ), v(pj2 ), v(pj3 )) and 0 otherwise. It is easy to check that T |= ϕ0 , as
desired.
For the NP upper bound in the case of Trec(S), it clearly suffices to deal with ϕ ∈ L+ .
Suppose we are given (ϕ, S) with ϕ ∈ L+ . We want to check if ϕ is satisfiable in Trec(S).
The basic idea in to guess a causal model T and verify that it indeed satisfies ϕ. There
is a problem with this though. To completely describe a model T , we need to describe
the functions FX . However, there may be many variables X in S and they can have many
possible inputs. Just describing these functions may take time much longer than polynomial
in ϕ. Part of the solution to this problem is provided by Theorem 4.2, which tells us that
it suffices to check whether ϕ is satisfiable in Trec(Sϕ ). In light of this, for the remainder
of this part of the proof, I assume without loss of generality that S = Sϕ . This limits the
number of variables that we must consider to O(|ϕ|). But even this does not solve our
problem completely. Since we are not given any bounds on |R(Y )| for variables Y in Sϕ,
even describing the functions FY for the variables Y that appear in ϕ on all their possible
input vectors could take time much more than polynomial in ϕ. The solution is to give only
a short partial description of a model T and show that this suffices.
~ ← ~y , ~u) such that there is a subformula of ϕ of the form [Y
~ ←~
Consider all pairs (Y
y ]ψ
2
and ~u appears in ψ. Let R be the set of all such pairs. Note that |R| < |ϕ| . We say that
~ ← ~
y , ~u) ∈ R, the
two causal models T and T 0 in Trec(S) agree on R if, for all pairs (Y
0
(unique) solutions to the equations in TY~ ←~y (~u) and T ~ (~u) are the same. It is easy to see
Y ←~
y
that if T and T 0 agree on R, then either both T and T 0 satisfy ϕ or neither do. That is, all
we need to know about a causal model is how it deals with the relevant equations—those
corresponding to pairs in R.
~ ← ~y , ~u) ∈ R, guess a vector ~v (Y
~ ←~
For each pair (Y
y , ~u) of values for the endogenous
variables; intuitively, these are the unique solutions to the relevant equations in a model
satisfying T . Given these guesses, it is easy to check if ϕ is satisfied in a model where these
335

Halpern

guesses do indeed represent the solutions to the relevant equations. It remains to show that
there exists a causal model in Trec(S) where the relevant equations have these solutions.
To do this, first guess an ordering ≺ on the variables. We can then verify, for each
~ ←~
fixed ~u that appears in ϕ, whether the solution vectors ~v (Y
y , ~u) guessed for the relevant
equations are compatible with ≺, in the sense that it is not the case that there are two
solutions (~u, ~x) and (~u, ~x0 ) such that some variable X takes on different values in ~x and ~x0 ,
but all variables Y such that Y ≺ X take on the same values in ~x and ~
x0 . It is easy to
see that if the solutions are compatible with ≺, we can define the functions FX for X ∈ V
such that all the equations hold and FX is independent of the values of Y if X ≺ Y for all
X, Y ∈ V. (Note we never actually have to write out the functions FX , which may take
too long; we just have to know they exist.) To summarize, as long as we can guess some
solutions to the relevant equations such that a causal model that has these solutions satisfies
ϕ, and an ordering ≺ such that these solutions are compatible with ≺, then ϕ is satisfiable
in Trec(S). Conversely, if ϕ is satisfiable in T ∈ Trec(S), then there clearly are solutions
to the relevant equations that satisfy ϕ and an ordering ≺ such that these solutions are
compatible with ≺. (We just take the solutions and the ordering ≺ from T .) This shows
that the satisfiability problem for Trec is in NP, as desired.

References
Chajewska, U., & Halpern, J. Y. (1997). Defining explanation in probabilistic systems. In
Proc. Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI ’97), pp.
62–71.
Druzdzel, M. J., & Simon, H. A. (1993). Causality in bayesian belief networks. In Uncertainty in Artificial Intelligence 9, pp. 3–11.
Galles, D., & Pearl, J. (1997). Axioms of causal relevance. Artificial Intelligence, 97 (1–2),
9–43.
Galles, D., & Pearl, J. (1998). An axiomatic characterization of causal counterfactuals.
Foundation of Science, 3 (1), 151–182.
Garey, M., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory
of NP-completeness. W. Freeman and Co., San Francisco, Calif.
Goldberger, A. S. (1972). Structural equation methods in the social sciences. Econometrica,
40 (6), 979–1001.
Harel, D. (1979). First-Order Dynamic Logic. Lecture Notes in Computer Science, Vol. 68.
Springer-Verlag, Berlin/New York.
Heckerman, D., & Shachter, R. (1995). Decision-theoretic foundations for causal reasoning.
Journal of Artificial Intelligence Research, 3, 405–430.
Henrion, M., & Druzdzel, M. J. (1990). Qualitative propagation and scenario-based approaches to explanation of probabilistic reasoning. In Uncertainty in Artificial Intelligence 6, pp. 17–32.
336

Axiomatizing Causal Structures

Pearl, J. (1995). Causal diagrams for empirical research. Biometrika, 82 (4), 669–710.
Pearl, J. (1999). Causality. Cambridge University Press, New York. Forthcoming.
Pearl, J., & Verma, T. (1991). A theory of inferred causation. In Principles of Knowledge
Representation and Reasoning: Proc. Second International Conference (KR ’91), pp.
441–452.
Spirtes, P., Glymour, C., & Scheines, R. (1993).
Springer-Verlag, New York.

Causation, Prediction, and Search.

Strotz, R. H., & Wold, H. O. A. (1960). Recursive vs. nonrecursive systems: an attempt at
synthesis. Econometrica, 28 (2), 417–427.

337

Journal of Artificial Intelligence Research 12 (2000) 271-315

Submitted 2/00; published 5/00

On the Compilability and Expressive Power
of Propositional Planning Formalisms
Bernhard Nebel

NEBEL @ INFORMATIK . UNI - FREIBURG . DE

Institut für Informatik, Albert-Ludwigs-Universität, Georges-Köhler-Allee, D-79110 Freiburg, Germany

Abstract
The recent approaches of extending the GRAPHPLAN algorithm to handle more expressive
planning formalisms raise the question of what the formal meaning of “expressive power” is. We
formalize the intuition that expressive power is a measure of how concisely planning domains
and plans can be expressed in a particular formalism by introducing the notion of “compilation
schemes” between planning formalisms. Using this notion, we analyze the expressiveness of a
large family of propositional planning formalisms, ranging from basic STRIPS to a formalism with
conditional effects, partial state specifications, and propositional formulae in the preconditions.
One of the results is that conditional effects cannot be compiled away if plan size should grow
only linearly but can be compiled away if we allow for polynomial growth of the resulting plans.
This result confirms that the recently proposed extensions to the GRAPHPLAN algorithm concerning
conditional effects are optimal with respect to the “compilability” framework. Another result is that
general propositional formulae cannot be compiled into conditional effects if the plan size should
be preserved linearly. This implies that allowing general propositional formulae in preconditions
and effect conditions adds another level of difficulty in generating a plan.

1. Introduction
G RAPHPLAN (Blum & Furst, 1997) and SATPLAN (Kautz & Selman, 1996) are among the most
efficient planning systems nowadays. However, it is generally felt that the planning formalism
supported by these systems, namely, propositional basic STRIPS (Fikes & Nilsson, 1971), is not
expressive enough. For this reason, much research effort (Anderson, Smith, & Weld, 1998; Gazen
& Knoblock, 1997; Kambhampati, Parker, & Lambrecht, 1997; Koehler, Nebel, Hoffmann, & Dimopoulos, 1997) has been devoted in extending GRAPHPLAN in order to handle more powerful
planning formalisms such as ADL (Pednault, 1989).
There appears to be a consensus on how much expressive power is added by a particular language feature. For example, everybody seems to agree that adding negative preconditions does not
add very much to the expressive power of basic STRIPS, whereas conditional effects are considered
as a significant increase in expressive power (Anderson et al., 1998; Gazen & Knoblock, 1997;
Kambhampati et al., 1997; Koehler et al., 1997). However, it is unclear how to measure the expressive power in a more formal way. Related to this problem is the question of whether “compilation”
approaches to extend the expressiveness of a planning formalism are optimal. For example, Gazen
and Knoblock (1997) propose a particular method of compiling operators with conditional effects
into basic STRIPS operators. This method, however, results in exponentially larger operator sets.
While most people (Anderson et al., 1998; Kambhampati et al., 1997; Koehler et al., 1997) agree
that we cannot do better than that, nobody has proven yet that a more space-efficient method is
impossible.
c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

N EBEL

In order to address the problem of measuring the relative expressive power of planning formalisms, we start with the intuition that a formalism  is at least as expressive as another formalism  if planning domains and the corresponding plans in formalism  can be concisely expressed
in the formalism  . This, at least, seems to be the underlying intuition when expressive power is
discussed in the planning literature.
Bäckström (1995) proposed to measure the expressiveness of planning formalisms using his
ESP-reductions. These reductions are, roughly speaking, polynomial many-one reductions on
planning instances that do not change the plan length. Using this notion, he showed that all of
the propositional variants of basic STRIPS not containing conditional effects or arbitrary logical
formulae can be considered as expressively equivalent. However, taking our point of view, ESPreductions are too restrictive for two reasons. Firstly, plans must have identical size, while we might
want to allow a moderate growth. Secondly, requiring that the transformation can be computed
in polynomial time is overly restrictive. If we ask for how concisely something can be expressed,
this does not necessarily imply that there exists a polynomial-time transformation. In fact, one
formalism might be as expressive as another one, but the mapping between the formalisms might
not be computable at all. This, at least, seems to be the usual assumption made when the term
expressive power is discussed (Baader, 1990; Cadoli, Donini, Liberatore, & Schaerf, 1996; Erol,
Hendler, & Nau, 1996; Gogic, Kautz, Papadimitriou, & Selman, 1995).
Inspired by recent approaches to measure the expressiveness of knowledge representation formalisms (Cadoli et al., 1996; Gogic et al., 1995), we propose to address the questions of how
expressive a planning formalism is by using the notion of compiling one planning formalism into
another one. A compilation scheme from one planning formalism to another differs from a polynomial many-one reduction in that it is not required that the compilation is carried out in polynomial
time. However, the result should be expressible in polynomial space. Furthermore, it is required
that the operators of the planning instance can be translated without considering the initial state
and the goal. While this restriction might sound unnecessarily restrictive, it turns out that existing
practical approaches to compilation (Gazen & Knoblock, 1997) as well as theoretical approaches
(Bäckström, 1995) consider only structured transformations where the operators can be transformed
independently from the initial state and the goal description. From a technical point of view this
restriction guarantees that compilations are non-trivial. If the entire instance could be transformed,
a compilation scheme could decide the existence of a plan for the source instance and then generate
a small solution-preserving instance in the target formalism, which would lead to the unintuitive
conclusion that all planning formalisms have the same expressive power.
As mentioned in the beginning, not only the space taken up by the domain structure is important,
but also the space used by the plans. For this reason, we distinguish between compilation schemes
in whether they preserve plan size exactly, linearly, or polynomially.
Using the notion of compilability, we analyze a wide range of propositional planning formalisms, ranging from basic STRIPS to a planning formalism containing conditional effects, arbitrary boolean formulae, and partial state specifications. As one of the results, we identify two
equivalence classes of planning formalisms with respect to polynomial-time compilability preserving plan size exactly. This means that adding a language feature to a formalism without leaving
the class does not increase the expressive power and should not affect the principal efficiency of
1. We assume that the reader has a basic knowledge of complexity theory (Garey & Johnson, 1979; Papadimitriou,
1994), and is familiar with the notion of polynomial many-one reductions and the complexity classes P, NP, coNP,
and PSPACE. All other notions will be introduced in the paper when needed.

272

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

the planning method. However, we also provide results that separate planning formalisms using
results from computational complexity theory on circuit complexity and non-uniform complexity
classes. Such separation results indicate that adding a particular language feature adds to the expressive power and to the difficulty of integrating the feature into an existing planning algorithm.
For example, we prove that conditional effects cannot be compiled away and that boolean formulae
cannot be compiled into conditional effects—provided the plans in the target formalism are allowed
to grow only linearly.
This answers the question posed in the beginning. The compilation approach proposed by Gazen
and Knoblock (1997) cannot be more space efficient, even if we allow for linear growth of the plans
in the target formalism. Allowing for polynomial growth of the plans, however, the compilation
scheme can be more space efficient. Interestingly, it seems to be the case that a compilation scheme
that allows for polynomially larger plans is similar to the implementation of conditional effects in
the IPP system (Koehler et al., 1997), Kambhampati and colleagues' (1997) planning system, and
Anderson and colleagues' (1998) planning system.
The rest of the paper is structured as follows. In Section 2, we introduce the range of propositional planning formalisms analyzed in this paper together with general terminology and definitions.
Based on that, we introduce the notion of compilability between planning formalisms in Section 3.
In Section 4 we present polynomial-time compilation schemes between different formalisms that
preserve the plan size exactly, demonstrating that these formalisms are of identical expressiveness.
For all of the remaining cases, we prove in Section 5 that there cannot be any compilation scheme
preserving plan size linearly, even if there are no bounds on the computational resources of the
compilation process. In Section 6 we reconsider the question of identical expressiveness by using compilation schemes that allow for polynomial growth of the plans. Finally, in Section 7 we
summarize and discuss the results.

2. Propositional Planning Formalisms
First, we will define a very general propositional planning formalism, which appears almost as
expressive as the propositional variant of ADL (Pednault, 1989). This formalism allows for arbitrary
boolean formulae as preconditions, conditional effects and partial state specifications. Subsequently,
we will specialize this formalism by imposing different syntactic restrictions.
2.1 A General Propositional Planning Formalism
Let  be the countably infinite set of propositional atoms or propositional variables. Finite
subsets of  are denoted by  . Further,   is defined to be the set consisting of the constants 
(denoting truth) and 	 (denoting falsity) as well as atoms and negated atoms, i.e., the literals,
over  . The language of propositional logic over the logical connectives 
 , and  and the
propositional atoms  is denoted by  . A clause is a disjunction of literals. Further, we say that
a formula   is in conjunctive normal form (CNF) if it is a conjunction of clauses. It is in
disjunctive normal form (DNF) if it is a disjunction of conjunctions of literals.
Given a set of literals  , by  we refer to the positive literals in  , by "!$#%& we refer to
the negative literals in  , and by '(& to the atoms used in  , i.e., '($*),+.-//102-33 or 4-5
2. Note that Gazen and Knoblock's (1997) translation scheme also generates planning operators that depend on the
initial state and the goal description. However, these operators simply code the initial state and the goal description
and do nothing else. For this reason, we can ignore them here.

273

N EBEL

76

. Further, we define 8 to be the element-wise negation of  , i.e.,

8),+.-90:4-35;6&<=+>4-?0@-=376BA
A state C is a truth-assignment for the atoms in  . In the following, we also identify a state with
the set of atoms that are true in this state. A state specification D is a subset of   , i.e., it is a logical
theory consisting of literals only. It is called consistent iff it does not contain complementary literals
or 	 . In general, a state specification describes many states, namely all those that satisfy D , which
are denoted by EF:GH$D( . Only in case that D is complete, i.e., for each -3/ we have either -=/D
or 4-IJD , D has precisely one model, namely K>$D( . By abusing notation, we will refer to the
inconsistent state specification by 	 , which is the “illegal” state specification.
Operators are pairs LM)ON pre  post P . We use the notation pre LB and post LQ to refer to the first
and second part of an operator L , respectively. The precondition pre is an element of RKSKT , i.e.,
it is a set of propositional formulae. The set post, which is the set of postconditions, consists of
conditional effects, each having the form

UWV X
UZY

Y

  are called effectV conditions and the elements of 
  are called
where the elements of
U
effects. If or  are singleton sets, e.g., +.-H6
+\[]6 , we often omit the curly brackets and write
- V [.
Example 1 In order to illustrate the various notions, we will use as a running example planning
problems connected with the production of camera-ready manuscripts from LATEX source files—
somewhat simplified, of course. As the set of atoms  , we choose the following set:

^),+`_baced]fgcihQj]klgm no4gp\qggrsklrsgr]rsmlgrsm o4tklu>hvtk hQctkwm o
hQj]k klu>h nxQihBjsk y>k _za nx6BA
These propositional atoms have the following intended meaning. The atoms in the first line represent
the presence of the corresponding files, and the atoms in the second line signify that the index and
citations are correct in the dvi-file. Based on that, we define the following operators: rBkwr\_bac , m d{_zac ,
| d]xea>kwu{h]ac . The first of these operators is very simple. The precondition for its execution is that
a rBklr - and an d]fgc -file exist. After the successful execution, a r]rsm - and a r]m o -file will have been
produced:

rBklr{_zacM)~}`d]fgcgrBkwr4{v V +Brsrsmlgrsm ov6"vA

The

| d]xea>kwu{hsac

operator is similar:

| d]xea>kwu{h]acM)}4`k hc`4{v V +Qkwu{h`tklm ov6"vA
Finally, the m d{_zac operator is a bit more complicated. As a precondition it needs the presence of the
_bac -file and it produces as its effect d]fgc -, k hc -, hBjsk , and m no -files unconditionally. In addition, we
know that the citations will be correct if a rsr]m -file is present and that the index will be correct if an
274

C OMPILABILITY

klu>h

-file is present:

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

m d{_bac)~}_zac`4
V
  V
rsr]m
r]rsV m
klu>h

+{d]f:ctk hQcihBjsklgm no`6B
V hBjs8k hQy>j]k _bk a y>k n_zax nxQ
hBjsk klu>h nxQ
V
kwu{h 8hQj]k kwu{h nx 4 A

The semantics of operators is given by state-transition functions, i.e., mappings from states to
states. Given a state C and a set of postconditions post, CB post  denotes the active effects in C :

MCs post()+\0 UWV &

post eC0 )

U 6BA

The state-transition function { induced by the operator L is defined as follows:

>\
{C>)

R  3 R 
 C7"!$#%$Cs post LB..<$MCs post LB.. if C0 ) pre LB and

 CB post LQ.0)	

undefined

otherwise

In words, if the precondition of the operator is satisfied in state C and the active effects are consistent,
then state C is mapped to the state C: which differs from C in that the truth values of active effects
are forced to become true for positive effects and forced to become false for negative effects. If the
precondition is not satisfied or the set of active effects is inconsistent, the result of the function is
undefined.
In the planning formalism itself, we do not work on states but on state specifications. In general,
this can lead to semantic problems. By restricting ourselves to state specifications that are sets of
literals, however, the syntactic manipulations of the state specifications can be defined in a way such
that they are sound in Lifschitz' (1986) sense.
Similarly to the active effects with respect to states, we define a corresponding function with
respect to state specifications:

$D post *)+\I0 UWV 

post D^0 )

U 6BA

Further, we define the potentially active effects as follows:

 $D post () CB postA
e  ¡%¢


$D post ,¥
If for a state specification D and an operator L5)£N pre  post P , we have $D post ¤) 
it means that the state specification resulting from the application of the state-transition functions
might not be representable as a theory consisting of literals only. For this reason, we consider such
an operator application as illegal, resulting in the illegal state specification 	 . We could be more
liberal at this point and consider an operator application to a state specification only as illegal if the
set of states resulting from applying the state-transition functions could definitely not be represented
3. Note that this can only happen if the state specification is incomplete.

275

N EBEL

as a theory consisting of literals only. Alternatively, we could consider all atoms mentioned in
 $D post ¦,M$D8 post  as “unsafe” after the application of the operator and delete the literals
7  $D8 post  $D post . from the state specification, but consider the resulting state specification
still as “legal” if $D post  is consistent. Since there does not seem to exist a standard model for
the execution of conditional effects in the presence of partial state specifications, we adopt the first
alternative as one arbitrary choice. It should be noted, however, that this decision influences some
of the results we present below.
 $D post  leads to an illegal state specification, we
Similarly to the rule that $D post §) 
require that if the precondition is not satisfied by all states in EF:GH$D( or if the state specification
is already inconsistent, the result of applying L to D results in 	 . This leads to the definition of the
function ¨ , which defines the outcome of applying an operator L from the set of operators © to a
state specification:

R  1ª ©  R 
 D/($D post LQ.<5M$D8 post LB. if D0)	 and

D 0 ) pre LB and
J

 $D8 post LB.¬0)
M
 	 $Dand
¨«$D8iLQ«) 

M
$

8
D



B
L
.



)
 post LB.
post

¨

	

otherwise

Example 2 Using the propositional atoms and operators from Example 1, we assume the following
two state specifications ­
 )®+:_zacKtklu>hK6 , and ­  )¯+:_bacKtklu{h`grsr]m°grsm ov6 . If we try to apply the
operator m d{_bac to ­ , we notice that this results in 	 because



­  post zm d\_bacB.) +{d]fgctk hcihQj]klgm no4ihQj]k klu{h nx6B
  ­   post zm d\_bacB.) ­  post zm d{_bacB.H<=+\hBjsk y>k _za nxQ8hQj]k y>k _ba nx6B



­   post zm d\_bacB.A On the other hand, we can apply rBkwr\_bac
i.e., we have M­  post zm d{_bacB.) 

successfully to ­ : ¨«­ grBkwr\_bacB()±­ A



It is easily verified that the syntactic operation on a state specification using the function ¨
corresponds to state transitions on the states described by the specification.
Proposition 1 Let D be a state specification, L be an operator, and { be the induced state-transition
function. If ¨«$D8iLQ²0)	 , then

EF:GH$¨«$DiLB.),+{C  0gC  )±{C>eC«0 )³D6BA
If ¨«$DiLB;0 )³	 , then either
1.

EF:GH$D(´)±µ

, or

2. there are two states C
3.

 MC   post LB. , or
 eC  /E¶:G$D( such that MC   post LB.²)³
there exists a state CWE¶:GH$D´ such that {>C> is undefined.
276

C OMPILABILITY

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

AND

In other words, whenever ¨«$DiLB results in a “legal” specification, this specification describes the
states that result from the application of the state-transition function > to the states that satisfy the
original state specification D . Further, if ¨«$D8iLQ is illegal, there are good reasons for it.
·
A planning instance is a tuple

)~N¸&i¹tº5P

where
»

»

¸§)~N$¬t©¼P is the domain structure consisting of a finite set of propositional atoms  and a
finite set of operators © ,

Y
» ¹   is the initial state specification, and
º Y   is the goal specification.½

·

When we talk about the size of an instance, symbolically 0¾0 0¾0 , in the following, we mean the size
of a (reasonable) encoding of the instance.
In the following, we use the notation ©¿ to refer to the set of finite sequences of operators.
Elements À of ©¿ are called plans. Then 0¾0 À30¾0 denotes the size of the plan, i.e., the number of
operators in À . We say that À is a Á -step plan if 0¾0 À30¾08ÂÃÁ . The result of applying À to a state
specification D is recursively defined as follows:

¨ !ÄB
;
R  ª © ¿  R 
¨;!t$D\N$P.) D
¨;!t]$D\NL  i L  gAgAgA:iL>ÅKP.Æ) ¨;!Ä]$¨«$DiL  \NL  gAgAgA{iL{ÅKP.
·
·
A sequence of operators À is said to be a plan for or a solution of iff
1. ¨X!Ä]¹eÀÇ¬0)³	 and
2. ¨X!Ä]¹eÀÇ;0 )Èº .
Example 3 Let  and © be the propositional
· atoms and operators introduced in Example 1 and
consider the following planning instance:
)¯N.N$²t©ÇPt+:_bacKgrBklr]klu>hK6Bt+\hBjskÉihQjsk y>k _za nx6>PA In
words, given a latex source file (_bac ) and a bibliography database ( rBklr ), we want to generate a dvi
file (hBjsk ) such that the citations in this file are correct (hBjsk y>k _ba nx ). Furthermore, we do not know
· but we know that there is no index file yet
anything about the existence of a bbl-file or aux-file etc.,
( kwu{h ). The plan ÀÊ)~Nzrsklr\_bacgm d{_zacsP is a solution of because the plan does not result in an illegal
state specification and the resulting state specification entails hQj]k and hBjsk y>k _ba nx .
Plans satisfying (1) and (2) above are “sound.” In order to state this more precisely, we extend
the notion of state transition functions for operators to state transition functions for plans. Let {Ë be
the state transition function corresponding to the composition of primitive state-transition functions
induced by the operators in ÀÌ)~NL gAgAgA\iL>ÅP , i.e.,



BÍ eÎ.ÏÑÐÑÐÑÐ Ï bÒ>Ó ) >eÎÕÔ¦AgAgAQÔ{bÒv
4. We could have been more liberal requiring that ÖØ×Ù T . We have not done that in order to allow for a “fair”
comparison with restricted planning formalisms.

277

N EBEL

·
such that  Í eÎ.ÏÑÐÑÐÑÐ Ï  Ò Ó C> is defined iff  Í eÎ.ÏÑÐÑÐÑÐ Ï bÚÛÓ C is defined for every Ü , Ý§ÂÞÜ9Âàß . Using this
notion, one can easily prove—using induction over the plan length—that any plan for an instance
is sound in Lifschitz' (1986) sense, i.e., corresponds to the application of state transition functions
to the initial states.
·

Proposition 2 Let )àN¸&i¹tº5P be a planning instance and
©¿ . If ¨X!Ä]¹eÀÇ is consistent, then

Àá)àNL  g AgAgA\iL>ÅP

be an element of

E¶:GH$¨X!Ä¹eÀÇ.´),+{C  0tC  )±{Ë²C>eC«0 )¹>6BA
If ¨;!Ä]¹eÀÇ is inconsistent, then either
1.

EF:GH¹\()µ

, or

2. there exists a (possibly empty) prefix
¨X!Ä]¹\NL gAgAgA:iL â P. and either



(a) there are two states C
or

NL  g AgAgA\iL>â$P (
ã ÂäÜ3Âäß§~Ý ) of À

 e C  WEF:GH$D(

such that MC

(b) there exists a state CWE¶:GH$D´ such that {bÚwæ

Î C>

such that

DØ)

 C   post L>â¾å  . ,
  post L>â2å  .²)³
is undefined.

2.2 A Family of Propositional Planning Formalisms
The propositional variant of standard STRIPS (Fikes & Nilsson, 1971), which we will also call ç
in what follows, is a planning formalism that requires complete state specifications, unconditional
effects, and propositional atoms as formulae in the precondition lists. Less restrictive planning
formalisms can have the following additional features:
Incomplete state specifications (è ): The state specifications may not be complete.
Conditional Effects (é ): Effects can be conditional.
Literals as formulae ( ): The formulae in preconditions and effect conditions can be literals.
Boolean formulae (ê ): The formulae in preconditions and effect conditions can be arbitrary
boolean formulae.
These extensions can also be combined. We will use combinations of letters to refer to such multiple
extensions. For instance, ç refers to the formalism ç extended by literals in the precondition lists,
S
ç`ësì refers to the formalism allowing for incomplete state specifications and conditional effects, and
ç%íQëBì , finally, refers to the general planning formalism introduced in Section 2.1.

·

Example 4 When we consider the planning instance from Example 3, it becomes quickly obvious
that this instance has been expressed using ç ësì . The initial state specification is incomplete, the
S
operator m d{_zac contains conditional effects and negative literals in some effect conditions. However,
we do not need general Boolean formulae to express the instance.
278

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

î`ï{ðñ

îð>ñ

î ò>ð>ñ

î ïñ

î ï{ð

î`òQñ

î`òð

î`ï

î ñ

îð

î ò

î
Figure 1: Planning formalisms partially ordered by syntactic restrictions

Figure 1 displays the partial order on propositional planning formalisms defined in this way. In
the sequel we say that  is a specialization of  , written £ó , iff  is identical to  or below
 in the diagram depicting the partial order.
Comparing this set of planning formalisms with the one Bäckström (1995) analyzed,ô one notices» that despite small differences in the presentation of the planning formalisms:

» ç

is the same as common propositional strips (CPS),

» ç S is the same as propositional strips with negative goals (PSN), and
ç S ë is the same as ground Tweak (GT).
2.3 The Computational Complexity of Planning in the ç -Family

While one would expect that planning in ç is much easier than planning in ç%íësì , it turns out that
this is not the case, provided one takes a computational complexity perspective.
In analyzing the computational complexity of planning in different formalisms, we consider, as
usual, the problem of deciding whether there exists a plan for a given instance—the plan existence
problem (PLANEX). We will use a prefix referring to the planning formalism if we consider the
existence problem in a particular planning formalism.
Theorem 3



-PLANEX is PSPACE-complete for all 

æ

with çóõó§ç%íQëBì .

5. We do not consider planning formalisms identical to the SAS formalism (Bäckström & Nebel, 1995), since we do
not allow for multi-valued state variables.

279

N EBEL

Proof. PSPACE-hardness of ç -PLANEX follows from a result by Bylander (1994, Corollary 3.2).
Membership of ç%íQësì -PLANEX in PSPACE follows because we could, step by step, guess a
sequence of operators, verifying at each step that the operator application leads to a legal follow up
state specification and that the last operator application leads to a state specification that entails the
goal specification. For each step, this verification can be carried out in polynomial space. The reason
for this is that all the conditions in the definition of ¨ are verified by polynomially many calls to an
NP-oracle. Therefore, ç%íQësì can be decided on a non-deterministic machine in polynomial space,
hence it is a member of PSPACE.
From that it follows that the plan existence problem for all formalisms that are in expressiveness
between ç and ç%íësì —including both formalisms—is PSPACE-complete.

3. Expressiveness and Compilability between Planning Formalisms
Although there is no difference in the computational complexity between the formalisms in the
ç íQëBì -family, there might nevertheless be a difference in how concisely planning domains and plans
can be expressed. In order to investigate this question, we introduce the notion of compiling planning formalisms.
3.1 Compiling Planning Formalisms
As mentioned in the Introduction, we will consider a planning formalism  as expressive as another
formalism  if planning domains and plans formulated in formalism  are concisely expressible
in  . We formalize this intuition by making use of what we call compilation schemes, which
are solution preserving mappings with polynomially sized results from  domain structures to 
domain structures. While we restrict the size of the result of a compilation scheme, we do not
require any bounds on the computational resources for the compilation. In fact, for measuring the
expressibility, it is irrelevant whether the mapping is polynomial-time computable, exponential-time
computable, or even non-recursive. At least, this seems to be the idea when the notion of expressive
power is discussed in similar contexts (Baader, 1990; Erol et al., 1996; Gogic et al., 1995; Cadoli
et al., 1996). If we want to use such compilation schemes in practice, they should be reasonably
efficient, of course. However, if we want to prove that one formalism is strictly more expressive
than another one, we have to prove that there is no compilation scheme regardless of how many
computational resources such a compilation scheme might use.
So far, compilation schemes restrict only the size of domain structures. However, when measuring expressive power, the size of the generated plans should also play a role. In Bäckström's
ESP-reductions (1995), the plan size must be identical. Similarly, the translation from ç ì to ç
S
S
proposed by Gazen and Knoblock (1997) seems to have as an implicit prerequisite that the plan
length in the target formalism should be almost the same. When comparing the expressiveness of
different planning formalisms, we might, however, be prepared to accept some growth of the plans
in the target formalism. For instance, we may accept an additional constant number of operators, or
we may even be satisfied if the plan in the target formalism is linearly or polynomially larger. This
leads to the schematic picture of compilation schemes as displayed in Figure 2.
Although Figure 2 gives a good picture of the compilation framework, it is not completely
accurate. First of all, a compilation scheme may introduce some auxiliary propositional atoms that
are used to control the execution of newly introduced operators. These atoms should most likely
have an initial value and may appear in the goal specification of planning instances in the target
280

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

ö
ø

Planning

compilation

I

Ë

G

÷

øBù

Planning

Ë ù

Figure 2: The compilation framework

formalism. We will assume that the compilation scheme takes care of this and adds some literals to
the initial state and goal specifications.
Additionally, some translations of the initial state and goal specifications may be necessary. If
we want to compile a formalism that permits for literals in preconditions and goals to one that requires atoms, some trivial translations are necessary. Similarly, if we want to compile a formalism
that permits us to use partial state specification to a formalism that requires complete state specifications, a translation of the initial state specification is necessary. However, such state translation
functions should be very limited. They should depend only on the set of symbols in the source
formalism, should be “context-independent,” i.e., the translation of a literal in a state specification
should not depend on the whole specification, and they should be efficiently computable.
While the compilation framework is a theoretical tool to measure expressiveness, it has, of
course, practical relevance. Let us assume that we have a reasonably fast planning system for a
planning formalism  and we want to add a new feature to  resulting in formalism  . If we
can come up with an efficient compilation scheme from  to  , this means we can easily integrate
the new feature—either by using the compilation scheme or by modifying the planning algorithm
minimally. If no compilation scheme exists, we probably would have problems integrating this feature. Finally, if only computationally expensive compilation schemes exist, we have an interesting
situation. In this case, the off-line compilation costs may be high. However, since the compiled
domain structure can be used for different initial and goal state specifications, the high off-line costs
may be compensated by the efficiency gain resulting from using the  planning algorithm.ú As
it turns, however, this situation does not arise in analyzing compilability between the ç%íQësì formalisms. Either we can identify a polynomial-time compilation scheme or we are able to prove that
no compilation scheme exists.

6. This means that compilation schemes between planning formalisms are similar to knowledge compilations (Cadoli &
Donini, 1997), where the fixed part of a computational problem is the domain structure and the variable part consists
of the initial state and goal specifications. The main difference to the knowledge compilation framework is that we
also take the (size of the) result into account. In other words we compile function problems instead of decision
problems.

281

N EBEL

·

3.2 Compilation Schemes
Assume a tuple of functions· ûM)üNý>þ>eý â eý>ÿQ
N¸&i¹tº5P to  -instances Ç  as follows:

â zÿ>P

that induce a function 

from



-instances

)

·

Ç ()N@ý>þ>¸´eý â ¸´H< â $¬i¹\eý>ÿs¸´%<zÿs$¬tº9.PgA


If the following three conditions are satisfied, we call û a compilation scheme from

·

iff there exists a plan for ¼

1. there exists a plan for

·

;

2. the state-translation functions zâ and ÿ are modular, i.e., for
D±0)	 , the functions  (for ?)Üe	 ) satisfy






to  :

~)Ã  </  , D Y  

, and



$¬D(()
  $   D   H<  $   D   

and they are polynomial-time computable;
3. and the size of the results of ý>þ\eý â , and ý>ÿ is polynomial in the size of the arguments.
Condition (1) states that the function  induced by the compilation scheme û is solutionpreserving. Condition (2) states requirements on the on-line state-translation functions. The result
of these functions should be computable element-wise, provided the state specification is consistent. Considering the fact that these functions depend only on the original set of symbols and the
state specification, this requirement does not seem to be very restrictive. Since the state-translation
functions are on-line functions, we also require that the result should be efficiently computable.
Finally, condition (3) formalizes the idea that û is a compilation. For a compilation it is much more
important that the result can be concisely represented, i.e., in polynomial space, than that the compilation process is fast. Nevertheless, we are also interested in efficient compilation schemes. We say
that û is a polynomial-time compilation scheme if ý>þ\eý â , and ý>ÿ are polynomial-time computable
functions.
In addition to the resource requirements on the compilation process, we will distinguish between different compilation schemes according to the effects on the size of the plans solving the
· û has the property that for every plan À
instance in the target· formalism. If a compilation scheme
solving an instance there exists a plan À« solving ¼  such that 0¾0 À«@0¾0Âá0¾0 À30¾0
 for some
positive integer constant  , û is a compilation scheme preserving plan size exactly (up to additive
ª 0¾0 À30¾0 for positive integer constants
· Á and  , then û is a compilation
constants). If 0¾0 À«@0¾0KÂ±Á
scheme preserving plan size linearly, and if 0¾0 À  0¾0Â¶-i0¾0 À30¾0l{0¾0 0¾0Ñ for some polynomial - , then û
is a compilation scheme preserving plan size polynomially. More generally, we say that a planning formalism  is compilable to formalism  (in polynomial time, preserving plan size exactly,
linearly, or polynomially), if there exists a compilation scheme with the appropriate properties. We

write 
 in case  is compilable to  or     if the compilation can be done in polynomial time. The super-script  can be Ý , Á , or - depending on whether the scheme preserves plan size
exactly, linearly plan, or polynomially, respectively.
As is easy to see, all the notions of compilability introduced above are reflexive and transitive.
7. Although it is hard to imagine a modular state-translation function that is not polynomial time computable, some
pathological function could, e.g., output translations that have exponential size in the encoding of the symbols.

282

C OMPILABILITY

Proposition 4 The relations



AND



E XPRESSIVE P OWER OF P LANNING F ORMALISMS

and

 



are transitive and reflexive.

Furthermore, it is obvious that when moving upwards in the diagram displayed in Figure 1,
there is always a polynomial-time compilation scheme preserving plan size exactly. If vâ denotes
the projection to the Ü -th argument and µ the function that returns always the empty set, the generic
compilation scheme for moving upwards in the partial order is û¬)ON eµeµ  P .
Proposition 5 If õóJ , then  









.

4. Compilability Preserving Plan Size Exactly
Proposition 5 leads to the question of whether there exist other compilation schemes than those
implied by the specialization relation. Because of Proposition 5 and Proposition 4, we do not have
to find compilation schemes for every pair of formalisms. It suffices to prove that  is compilable
to  , in order to arrive at the conclusion that all formalisms that are below  are compilable to 
and formalisms above  .
A preview of the results of this section is given in Figure 3. We will establish two equivalence
classes such that all members of each class are compilable to each other preserving plan size exactly.
These two equivalence classes will be called ç ë - and ç ësì -class, in symbols  ç ë and  ç ësì  ,
S
S
S
S
naming them after their respective largest elements.

î ï{ðñ

î ð>ñ

î òð>ñ

î ïñ

î ï{ð

î òQñ

î ò>ð

î ï

î ñ

îð

î`ò

î
Figure 3: Equivalence classes of planning formalisms created by polynomial-time compilation
schemes preserving plan size exactly

283

N EBEL

4.1 Planning Formalisms without Conditional Effects and Boolean Formulae
First, we will show that the formalisms analyzed by Bäckström (1995), namely, ç ë , ç , and ç are
S
S
polynomial-time compilable into each other preserving plan size exactly. In fact, a fourth class can
be added to this set, namely, ç`ë , which lies between ç ë and ç .
S
In other words, using the notion of compilability, we get the same equivalence class as with
Bäckström's ESP-reductions. Having a closer look at the proofs in Bäckström's (1995) paper reveals that this is not surprising at all because the ESP-reductions he used could be reformulated as
compilation schemes. Since he used a quite different notation, we will nevertheless prove this claim
from first principles.
The key idea in compiling planning formalisms with literals to formalisms that allow for atoms
only
is to consider - and 4- as different atoms in the new formalism. For this purpose, we introduce
!
!
Y   , then "? is a set where each negative
copy of  . Further, if 
J),+ -Ç0 -=/¦6 , i.e., a disjoint
!
literal 4- in  is replaced by - , i.e.,
#

?)
"

	

!

!
+.-//I0 -3=76<=+ -3
 J0:4-53;6

if ³0)	
otherwise.

!

Using <  as the new set of atoms, one can translate state specifications and preconditions easily. In the postconditions we have to make sure that the intended semantics is taken care of, i.e.,
!
whenever - is added, - must be deleted and vice versa.
Finally, we have to deal with the problem of partial state specifications. However, this not
a problem when all effects are unconditional and the preconditions contain only atoms. In this
case, we can safely assume that all atoms with unknown truth-value are false without changing the
outcome of the application of an operator. Let $&%¦* denote the completion of  with respect
to  , i.e.,

  *),+>4-90 -3/¬-§3
 76<?XA

$&%

Using this function, we can transform a partial state specification into a complete specification
without changing the outcome, i.e., we get the same plans.
Theorem 6
exactly.

ç S ë , `ç ë , ç S

, and ç are polynomial-time compilable to each other preserving plan size

Proof. Since ç^óç`ëHóç ë and çZó§ç
S
S óç S ë , it follows from Propositions 4 and 5 that we only
have to show
·
that ç ë  ç in order to prove the claim.
S
Let )ÊN¸&i¹tº5P be a ç ë -instance with ¸)üN$¬t©¼P . We translate each operator L© into
S
the operator
!
L)~N'" pre LB(" post LQ<5)"5 post LQ.PA
The set of all such operators is denoted by
Ný>þ>eý â eý>ÿQ â zÿ{P as follows:

ý>þ¸´Æ)
ý â ¸´Æ)
ý>ÿs¸´Æ)
â $¬i¹\Æ)
zÿs$¬tº9)

©

!

. Now we can define the compilation scheme
!

!

$N ¶< ¦ ¼
© P
µ
µ
$&% +* ! ' "?¹\
"=º=A
284

ûW)

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

· satisfies conditions (2) and (3), all the functions can be computed in polyThe scheme û obviously
nomial time, and Ç  is a ç -instance.
Y   . Then it is obvious that
Let D
!

â $²¨«$DiLB.()¨«, â $²D( LBA
!

À ) .N -L  gAgAgA:/L- Å P denote a sequence of operators corresponding to a sequence of operators
¯
ÀÊ)~NL  gAgAgA:i L>ÅvP . Using induction on plan · length, it is easy to show· that
!
À is a plan for iff À is a plan for Ç  ,
i.e., condition (1) on compilation schemes is also satisfied. This means, û is in fact a compilation
Let

scheme. Further, since the plan size does not change, the compilation scheme preserves plan size
exactly. Finally, because all functions in û can be computed in time polynomial in their arguments,
û is a polynomial-time compilation scheme.
One view on this result is that it does not matter whether, from an expressivity point of view, we
allow for atoms only or for literals and it does not matter whether we have complete or partial state
specification—provided propositional formulae and conditional effects are not allowed.
4.2 Planning Formalisms with Conditional Effects but without Boolean Formulae
Interestingly, the view spelled out above generalizes to the case where conditional effects are allowed. Also in this case it does not matter whether only atoms or also literals are allowed and
whether we have partial or complete state specifications. In proving that, however, there are two
additional complications. Firstly, one must compile conditional effects over partial state specifications to conditional effects over complete state specifications. This is a problem because the
 $D8 post LB. in the definition of the function ¨ must be tested. Seccondition $D post LQ.)
ondly, when compiling a formalism with literals into a formalism that allows for atoms only, the
condition M$D post LB.0) 	 in the definition of ¨ must be taken care of. For this reason, we will
prove this result in two steps.
As a first step, we show that ç ësì can be compiled to ç ì . The problem in specifying such a
S
S
compilation scheme is that the execution of an operator L on a partial state specification leads to the

$D post LB. .
illegal state if $D post LB.¬) 
When considering our running example (Ex. 1), things are quite obvious. When a state specification does not contain the literal or the negation of the literal that is mentioned in the effect
condition, then the illegal state specification results. For example, if a state specification does neither contain r]rsm nor &rsrsm , then the result of executing m d{_bac is 	 . In the general case, however,
things are less straightforward because effect literals can be produced by more than one conditional
rule and an effect condition can consist of more than one literal.
Assuming without loss of generality (using a polynomial transformation) that the effects are
all singleton sets, we have to check the following condition. Either one of the conditional effects
with the same effect literal is activated—i.e., the effect condition is entailed by the partial state—
or all of the conditional effects with the same effect literal are blocked, i.e., each effect condition
contains a literal that is inconsistent with the state specification. If this is true, the original operator
 $D post LQ. , otherwise the resulting state specification is inconsistent.
satisfies $D post LB.)
For example, consider the following ç ësì operator:

Lâ )

S
N$t+Q+.-Õ8[]6 V +>4-H6Bt+10Õ2v6 V +>4-H6Q6>PA
285

N EBEL

The application of this operator satisfies $D post LB.´)
1.
2.

-

0

and 8[ are true in the state specification, or

 $D post LB.

iff either

and 2 are true in the state specification, or

3. one of - and *[ is false and one of 0 and 2 is false.



In all other cases, we get M$D8 post LB.) 
$D post LB. and the result is the illegal state. In order
to test for this condition in a formalism with complete states we introduce four new sets of atoms:

  )
 å )

+.-  0 -3=¦6B
.+ - å 0@-3/¬6B
43 ) .+ -53=0@-3/¬6B
6
) +1 â Ï 7X0 for the 8 th conditional effect of L â 6BA
The atom -  is true if either - or 4- is part of the original partial state specification. The atom
- å is set true by an operator if one of the conditional effects adds - or if - does not appear as an
effect in the operator. The atom 5
- 3 is set true by an operator if one of the conditional effects deletes
- or if 4- does not appear as an effect in the operator. Finally, atoms of the form  â Ï 7 are added by
an action if the 8 th conditional effect in the Ü th operator is blocked by some effect condition. Using
these new atoms, we could translate the above operator to

N$8+H+.-`i[>-8[]6 V V +.-4 -53*4-6B
+ 0   2   0 V 2`6 +.-  - 3 4-H6B
1
+.-`4-6 V +1 â Ï  6B
+\[  i[]6 V 1+  â Ï  6B
+ 049 0H6 V 1+  â Ï  6B
1
+ 2]V 9 2v6 1+ `â Ï  6B
1
 V  å <5 3 6 I+.- 3 6B

+ ;:XÏ 7 (0 < )Z
1
 Ü.6Q6>PA
Let = Üe>8] be a function that returns - å or 5
- 3 , if - or 4- , respectively, is the effect of the 8 th
conditional effect in the Ü th operator. Assuming now that the atoms from  are set according to
6
their intended semantics and that the previous operator deleted all atoms from 7å=<W4
 35< , the
>L ! âä)

following test operator checks whether the original operator would have led to an inconsistent result:
test

)

ÕA@+>9 â Ï 79=8 Üi>8]e6 V 	0( â Ï 7
?

6BDC

A

Whenever we have 9 â Ï 7 , it means the 8 th conditional effect in the Ü th operator (which must be the
previously executed operator) was not blocked. If in addition to that the effect of this conditional
 $D post LB. in the
effect was not activated, i.e., 9=8 Üi>8] is true, we would have $D post LB.) 
original formalism. For this reason, we force the illegal state. Conversely, if either  â Ï 7 is true for all
Ü and 8 or if it is false for one 8 , but = Üe>8] is true, we would have $D post LQ.¦)  $D8 post LB.
in the original formalism and do not need to force the illegal state.
!
We now could force, by using some extra literals, that after each operator L{â the test operator is
applied. This would result in a compilation scheme that preserves plan size only linearly. However,
it is possible to do better than that. The key idea is to merge the test operator for the Ü th step into the
operator of step ÜEÝ .
286

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

ç S së ì is polynomial-time compilable to ç S ì preserving plan size exactly.
·
)äN¸&i¹tº9P be a ç S ësì -instance with ¸~)äN$¬t©¼P . Without loss of generality, we
Proof. Let
assume that the postconditions of operators L â 1© have the following form:
V F Ï gAgAgA:i Ï :Ú H
G
V F Ï :Úz6B
post L â ´),+\ â Ï
â
â
â

Y
F
with  â Ï 7
  and â Ï 7²I  .
Lemma 7



First, we introduce a number of new sets of symbols that are pairwise disjoint and disjoint from
:

  )
JIå )
 å )


)

I3

 3
6

6

+.- 
.+ -KIå
+.- å
+.- I3
+.- 3

I

)
 )

)
Y

0@-=/¦6B
0z-3/¦6B
0z-3/¦6B
0z-3/¦6B
0z-3/¦6B
+1KâI Ï 7 0 for the 8
+1 â  Ï 7 0 for the 8

th conditional effect of L

â 6B
th conditional effect of L â 6BA

  ,   denotes the set of primed literals, i.e.,   )à+.-  0i-^Z76¦<
For a given set of literals 
2, i.e., C]Mv=)
+>4-`*0"$4- Wü76 and the function CLl denotes the successor function modulo
6
MNÝ\DOQPSRUT . Further, the functions =WV for )ã:Ý shall be functions from V to JVå <9JV3 such
that
#
V [B post L â 
V / Vå
[
if  â Ï 7
å
= V  Üe>8]´)
[ V3 / V3 if (â Ï 7 V 8[B post L>âA
Now let postV

L â 

postV
let block V

L â 

for Ç)ã:Ý be defined as follows

L â *)±+\/<?'(´2 VV .+ --`- Vå ²
6 0@-3/²\ V V %-  post L â e6&<
+\/<?'(´2 >+ 4--`- V3 ¬
6 0 -3W¬\
" -   post L>â$e6B

for )ã:Ý be defined as
block V

L â ()+Q+\[i[{ 6 V V  âV Ï 7 0Q â Ï 7 VXVXF â Ï F 7{ post L â \$*[Q75 â Ï 7>6&<
+Q+>8[i[  6 WâV Ï 7 0 â Ï 7
â Ï 7\& post L â i[«3 â Ï 7>6B

and let testV be defined as

Z
Z
Z
6 Z
V\[ 6BA
),+Q+>D â Ï 7 V\[ 9= V\[  Üi>8]e6 V 	0] â Ï 7 V\[ 
6
V . Now we can
Further, let Á  , Á  , and  be fresh symbols not appearing in ¶<3  <=JVå <3JV 3 <
define the pair of compiled operators L âV ( Ç)ã:Ý ) corresponding to the original operator L â W© :
L âV )ON pre L â H<=+\Á V 6B postV V L â %< blockV ZL â %< testV <
+> V +>*/Á V]^ `iÁ V\[b6Q67<
+> V  Vå <9 V3 6 J+1= V  Üi>8]e6Q6X<
+>
+ K:;
1
V Ï7 
 Ü.6Q6&<
Z V\[ V¬]0 < Z V\[ )
6
V
+>
´ å <5( 3 <5  Z V\[ 6BA
{C

	Y 1 V

287

N EBEL

This pair of compiled operators achieves the intended effects and keeps track of fully known
atoms using postV , checks which conditional effects are blocked using block V , tests whether the
 $D post LB. using
execution of the previous operator satisfied the condition $D post LB.¶)
testV , and setup the bookkeeping atoms for the next step. Using the atoms Á/V , it is enforced that
executing and testing is merged by parallelizing the test on step Ü and execution of step Ü9ÈÝ . In
order to check the execution of the last step, we need an extra checking step:

L ÿV )ON +\Á V B6  test V <=+> V +\`8Á V Q6 6>PA
Now we can specify a compilation scheme û from ç

ý>þ¸´ )

S ësì to ç S ì as follows:
6
6
N\§
9
<


s

9
<

<=+\`iÁ I iÁ{g6B
Iå <5¦å  <5 I 3 <9¦3 <
I <
_
+\L âI iL â  0tL â 1©?6<=+\L ÿI iL ÿ  6>P
6
6
+>^4iÁ/IB8Á  6&<9(JIå 5
< ( å <5´JI3 <3( 3 <5 I(<9  
+\v6B
$&%  ¹\%
< $&%  ù  +.-  0 -3=¬t+.-"-H6J?¹«)±
 µ]6>
º/A

ý â ¸´ )
ý>ÿB¸´ )
â $¬i¹\ )
ÿ $¬tº5Æ)

The scheme û obviously satisfies conditions (2), i.e., that the state-translation functions are modular,
· polynomially sized results. Further, all the functions
and (3), i.e., that the compilation functions have
can be computed in polynomial time, and Ç  is a ç ì -instance.
S
Y   . Then it is obvious that
Assume D




 <   *)¨X!Ä],zâ.$²D(H<9ý>âb¸´\NL âI P.51 ¶
 <   
â.$¬¨;!t]$D\NL>â P..51 ¶
provided ¨;!t]$D\NL â P.0) 	 . In case ¨;!t]$D\NL â P.F0 ) 	 , either ¨;!t], â $¬D(&<ý â ¸\NL âI P.¶0 )
	 or M$D post L â .Ê)   $D post L â . . In the latter case, the application of any operator to
¨;!t], â $¬D(\NL âI P. leads to an inconsistent state because of the conditional effects in test  , which
is part of all postconditions of operators applicable in this state. Additionally, the same is true for
the relation between zâb$¬¨X!Ä]$D\NL>â.iL 7 . and ¨;!Ä],zâi$¬D(H<?ý>â.¸´\NL âI iL]7  P. .
Let À  )~NL  gAgAgA\iL Å P denote a sequence of operators corresponding to a sequence of operators
ÀÊ)~NL gAgAgA:iL>Åv P . Using induction on the plan length, it can be easily shown that



À

·

iff À

is a plan for

Further, since any plan solving the instance Ç

·

there exists a plan for

·

a` LAÿV


is a plan for Ç

·

.

must have LAÿV as the last operator, it follows that

iff there exists a plan for Ç

·

.

From that it follows immediately that û is a polynomial-time compilation scheme from ç
preserving plan size exactly, which proves the claim.

S ësì

to ç

Sì

Having proved that ç ësì can be compiled to ç ì preserving plan size exactly, it seems worth
S
S
noting that this result depends on the semantics chosen for executing conditional operators on partial state specifications. For example, if we use an alternative semantics that deletes all the literals


in 7 $D post LB.ZM$D post LB. provided $D post LB. is consistent, then there exists probably only a compilation scheme that preserves plan size linearly. If we use a semantics where the
288

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

resulting state specification is legal when the application of all state-transformation functions leads
to a theory that can be represented as a set of literals, it seems unlikely that there exists a scheme
that preserves plan size polynomially. The reason for this pessimistic conjecture is that under this
semantics it appears to be coNP-hard to determine whether the state specification resulting from
applying a ç ëBì -operator is legal.
S
As a second step in showing that partial state specifications and literals can be compiled away,
we show that we can compile ç ì to ç ì . The key idea in the proof is the same as in the proof of
S
!
Theorem 6. We replace each negative literal 4- by a new atom - . In order to detect inconsistencies
introduced by conditional effects, we add to each postcondition conditional effects of the form
!
+.- -%6 V 	 . Further, to check that the last operator in a plan does not introduce any inconsistencies,
we force the application of a “checking” operator that contains the same conditional effects.
Lemma 8

ç Sì
·

is polynomial-time compilable to ç

ì

preserving plan size exactly.

Proof. Let
) N¸&i¹tº9P be a ç S ì -instance with ¸Ì) N$¬t©¼P . Since
postconditions of all operators L1© have the following form:

b

post LB()È+\

Y

VXb g AgAgA\i




:

VXb

:

·

is a

ç Sì

-instance, the

6B

with ^7B 7
 .
!
As in the proof of Theorem 6,  shall be a disjoint copy of  , and "9 is the set of atoms where
!
c LQ be the following set
each negative literal 4- is replaced by the atom - . Now let post
post
c LB()È+

?^7 V '"

b

"

<5)"5
7

b

\(097 VXb {7 
7

post LBe6BA

Further, let cons be the set of conditional effects
cons

),+Q+.- -H! 6 V 	0@-3/¬6B
!

let  be an atom not appearing in  , let L be

L! )~N'"

let

!

©O)È+ M
L! 0tL1©?6

pre LB
post
c LBH< cons <=+>

V 9v6>P

, and let the operator L>ÿ be

L>ÿ¬)~N$ cons <=+> V ` 6>PA

Then we can specify a compilation scheme û from ç

ý>þQ¸´ )
ý â ¸´ )
ý>ÿs¸´ )
â $¬i¹\ )
zÿs$¬tº5Æ)

S ì to ç ì as follows:
!
!
N$¶< ¶<3+\v6B ©=
< +\L>ÿQ6>P
+>^ `6B
+\v6B
"?¹(<5d
 "58¹
"=º/A

· satisfies conditions (2) and (3), all the functions can be computed in polyThe scheme û obviously
nomial time, and Ç  is a ç ì -instance.
289

N EBEL

Assume D

Y 

. Then it is obvious that
!

â $¬¨«$DiLB.´)¨«, â $¬D( LB

provided ¨«$DiLB¬0)³	«A

!

!

Y

!

In case ¨«$DiLB;0 )	 , either ¨«, â $²D( LB;0 )	 or +.- -%6
¨«, â $¬D( LB for some -=/ . In the
!
latter case, the application of any operator to ¨«, â $¬D´ LB leads to an inconsistent state because
of the conditional
effects in cons, which is part of all postconditions.
!
Let Àü)ÊN.-L gAgAgA:/L- Å P denote a sequence of operators corresponding to a sequence of operators

ÀÊ)~NL gAgAgA:iL>ÅvP . Using induction on the plan length, it can be easily shown that



À

·

iff À

is a plan for

Further, since any plan solving the instance Ç

·

there exists a plan for

·

!

`



L>ÿ

is a plan for ¼

·

.

must have L>ÿ as the last operator, it follows that

iff there exists a plan for Ç

It follows that û is polynomial-time compilation scheme from ç
which proves the claim.

Sì

to ç

ì

·

.

preserving plan size exactly,

This result is, of course, not dependent on the semantics because both formalisms deal only with
 $D post LB. .
complete state specifications, and hence we always have M$D post LB.()
Theorem 9
size exactly.

ç S së ì , ç S ì , `ç ësì

, and ç

ì

are polynomial-time compilable to each other preserving plan

Proof. ç ëBì   ç ì follows from Lemma 8, Lemma 7 and Proposition 4. Using Propositions 4
S
and 5 and the fact that ç ìó§ç ìó§ç ësì and ç ìó§ç`ëBì`ó§ç ësì , the claim follows.

S

S

S

5. The Limits of Compilation when Preserving Plan Size Linearly
The interesting question is, of course, whether there are other compilation schemes preserving plan
size exactly than those we have identified so far. As it turns out, this is not the case. We will prove
that for all pairs of formalisms for which we have not identified a compilation scheme preserving
plan size exactly, such a compilation scheme is impossible even if we allow for a linear increase of
the plan size. For some pairs of formalisms we are even able to prove that a polynomial increase
of the plan size would not help in establishing a compilation scheme. These results are, however,
 fhe assumption. A preview
conditional based on an assumption that is slightly stronger than the eF)g
of the results of this section is given in Table 1. The symbol ó means that there exists a compilation
scheme because the first formalism is a specialization of the second one. In all the other cases, we
specify the separation and give the theorem number for this result.
5.1 Conditional Effects Cannot be Compiled Away
First of all, we will prove that conditional effects cannot be compiled away. The deeper reason
for this is that with conditional effects, one can independently do a number of things in parallel,
which is impossible in formalisms without conditional effects. If we consider, for example, the
operator m d{_zac from Example 1, it is clear that it ”`propagates”' the truth value of rsr]m and klu{h to
hQj]k y>k _ba nx and hBjsk klu>h nx , respectively—provided the state specification satisfies the precondition.
290

C OMPILABILITY



ç%íësì


ç íQësì


ç%ísì

ó



ç%í] ì


ç%íQë
j
 i

ç%í 


Cor. 15

Cor. 12

Cor. 15

Cor. 15

ji

ji

4i

Theo. 11

Cor. 12

Cor. 12

ji

ji

4i

Cor. 12

Cor. 12

Cor. 19

)

ó



ji

ó

ç%í

ó







Cor. 15









Theo. 14

 i

ó

)












ó
ó

ó

Cor. 15

ó

Theo. 18

ó




)

Cor. 19

ç%íQë



ç S së  ì


Cor. 15

ó

ç Së



)

ç S së ìk



E XPRESSIVE P OWER OF P LANNING F ORMALISMS

AND

ç S ë












Cor. 15



 i

)

Cor. 19

ó

)

Table 1: Separation Results

It is obviously possible to come up with a set of exponentially many operators that can do the same
thing in one step. However, it is unclear how to do that with less than exponentially many operators.
In fact, we will show that this is impossible.
In order to illustrate this point, let us generalize the above example. We start with a set of ß
propositional atoms &Å) +.- gAgAgA:-`Å`6 and a disjoint copy of this set: mÅl ) +.- âl 0- â ,Åv6 .
Further, if


D Y  Å



, then Dnl shall denote the corresponding set of literals over

D
Consider now the following ç

 Å
© Å
¸ Å

)

 Å

o l

, i.e.,

)È+.- âl 0@- â =D6&<=+>4- âl 0g4- â /D6BA
l

S ësì

domain structure:

Å²<5 Ål 
B
) @*N$t+.- â V - âl "- â V 4- âl 0@- â WÅv6>P 
) N$  Å 8©  Å PA
From the construction it follows that· for all pairs ¹tº5 such that ¹ is a consistent and complete set
Y ¹1l , the instance )õN¸ Å`i¹tº5P has a one-step plan. Conversely, for all pairs
over Å and º

¹tº9 with ºg o l Y  1¹ l , there does not exist a solution.
Trying to define a ç íë domain structure polynomially sized in 0¾0 ¸ Å 0¾0 with the same property

seems to be impossible, even if we allow for Á -step plans. However, in trying to prove this, it turns
out that an additional condition on the state-translation function is needed.
We say that the state-translation functions are local iff for all state specifications
 5 )±µ we have







â $   Dp   q ÿs$  D   ()±µA
291

D

and for

N EBEL

With locality as an additional condition on state-translation functions we could easily prove that
conditional effects cannot be compiled away. Instead of doing so we will show, however, that it
is possible to derive a weaker condition from the definition of compilation schemes that will be
enough to prove the impossibility result. This weaker condition is quasi-locality of state-translation
functions relative to a given set of symbols  , which in turn is based on the notion of universal
F
literals. A literal is called a universal literal for given state-translation functions on  iff one of
the following conditions is satisfied:
1. for all -3= :

F

2. for all -3= :

F

3. for all -3= :

F

4. for all -3= :

F

5. for all -3= :

F

6. for all -3= :

F

p â  +.-6Bt+.-H6> , or
p â  +.-6Bt+>4-H6> , or
p â  +.-6BeµB , or
pzÿs +.-H6Bt+.-H6> , or
pzÿs +.-H6Bt+>4-H6> , or
pzÿs +.-H6BeµB .

Let r denote the set of universal literals. Now we define quasi-locality of state-translation functions relative to a set of propositional atoms  and the induced set of universal literals r as follows.
Y   such that D0)³	 and for all pairs   Y  with  3 )±µ , we have
For each D







  Y W
r A

â $   D   szÿs$  D







In words, the only non-local literals in quasi-local state-translation functions are the universal literals.
Lemma 10 For a given compilation scheme ûF)ÆNý>þ{eý â
Y  such that 0Ñ0ut^ß and â and
exists a set of atoms 



eý>ÿQ â zÿ>P and natural number ß
ÿ are quasi-local on  .

, there

R;v be a function that has as the result the union of all results for all possible
Proof. Let Ät
translations of a literal returned by the state-translation functions, i.e.,
gl- *) â  +.-H6Bt+.-H6>< â  +.-H6Bt+>"-H6>< â  +.-H6BeµB.<
ÿs +.-H6Bt+.-6>Õ
< zÿB +.-H6Bt+>4-H6>
< zÿs +.-H6BeµBA
Set

5)È
w

and rØ)±µ . Now we choose an infinite subset
w



of

w

such that either

1. for all -3xw  , there are only finitely many other atoms [xw" such that ,glor if such an infinite subset of w does not exist,
2.

w



has a universal literal

F


 r

and we set r?`)
rÌ<=+

F

6

A4g[Q.sUr )±
 µ

,

.
F

 r must
Note that such an infinite subset w  must exist. The reason is that some literal y
occur for infinitely many atoms in over w because we could not find an infinite subset
satisfying condition (1). Because for a single atom there are only six possible ways to generate
F
, there must exist an infinite subset such that this literal occurs in all of either v +.-H6Bt+.-H6> ,
F
v +.-H6Bt+>4-H6> , or zv +.-H6BeµB (for ?)Üi	 ) and in this subset is a universal literal.
292

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

If we can pick a subset satisfying the first condition, we can choose from it a finite subset  with
any desired cardinality such that the state-translation functions are quasi-local with respect to  and
r .
Otherwise we repeat the selection process with w  and r  until condition (1) is satisfied. This
selection process can only be repeated finitely often because otherwise there are some atoms such that tl-  has an infinite result, which is impossible because the state-translation functions are
polynomial-time computable and can therefore have only finite results.
This demonstrates that there always exists a set of propositional atoms such that the statetranslation functions are quasi-local. However, we might not be able to effectively determine this
set.
Using this result, we are finally able to prove the non-existence of compilation schemes for
compiling conditional effects away when preserving plan size linearly.
Theorem 11

ç S së ì

cannot be compiled to ç%íë preserving plan size linearly.

Proof. Assume for contradiction that there exists a compilation scheme û from ç ësì to ç%íQë preS
serving plan size linearly, which compiles the domain structure ¸ Å defined above into the ç%íë

domain structure

ý>þ¸  Å ´)±¸  Å )ON$  Å t ©  Å PA
Because of Lemma 10 we can assume that the set of atoms  Å is chosen such that the translation

functions are quasi-local on this set.
Let us now consider all initial state specifications ¹ that are consistent and complete over
and do not contain only positive or only negative literals:

Obviously, there are
following form

T

Å {T

 Å



K
¹²/R Ò I
 +>Å4(ÅK6BA

such state specifications. By assumption, each

ç%íQë

instance of the

N¸  Å 5 â.$  Å i¹\H<?ý>âb¸  Å 5 ÿ $ Å i¹ l H<9ý ÿ ¸  Å .P
has a Á -step plan. Since there are only |i0°© Å 0 i  different Á -step plans, which is a number polyno
mial in the size of ¸ Å , the same plan À is used for different initial states—provided ß is sufficiently

large.
Suppose that the plan À is used for the pairs ¹t tº¤ \¹t tº¼  , which result from ¹ and ¹ :


   
¹   ) â $Å4i¹  H<9ý â ¸  ÅK
º   ) ÿ $ Å i¹ l %<9ý ÿ ¸  Å 
¹  ) â $Å4i¹  H<9ý â ¸  ÅK
º  ) zÿs$Å`i¹ l %<9ý>ÿs¸  ÅK

Since ¹ )à
  ¹  , ¹  and ¹  must differ on at least one atom, say - . Without loss of generality we
assume -33¹ and 4-35¹ . Since À is a successful plan from ¹  to º  and because ÿ is modular,




it follows that

¨X!Ä]¹   eÀÇh}±º  

zÿs +.- l B6 t+.- l >6 A

}~

293

N EBEL

Some of the literals in zÿs +.-Kl;6Bt+.-KlX6> may be added by operators in À but none of the literals
in ÿs +.- l 6Bt+.- l 6> can be deleted by an operator in À without reestablishing this literal by another
operator after its deletion. Because À contains only operators with unconditional effects, it adds
and deletes the same literals regardless of the initial state.
F
Let us now assume that there exists a literal zÿs +.-Kl;6Bt+.-KlX6> that is not added by À . This
F
implies that 3¹t and we have to distinguish three cases:


=ý â ¸  ÅK , from which we conclude that F 3¹t .
F
2. p â  +.K
- lX6BeµB Y ¹t  , which also implies that F 3¹t .
F
3. 
that the state zâ. +\[s6Bi& with [^)  K- l and Ã,+Q+\[s6BtF +>*[s6Beµ]6 . Because we assumed
F
translation functions are quasi-local on  Å , must be a universal literal. If is universal for

F
contain positive and negative
â , then we will have I¹t because the possible initial states
F
literals as well as no literal for some elements from  Å . If is universal for ÿ , it is present
 F
in º¼ and in º¼ for the same reason. Further, because is not added by À and À is a valid


plan from ¹t to º¤ , it must also be part of of ¹t .



F
In other words, all literals 
 ÿs +.K- lX6Bt+.K- lX6> that are not added by À are already in ¹t  and ¹t .
From that we conclude that
¨;!Ä]¹  eÀÇ }y ÿs +.- l 6Bt+.- l 6>A
1.

F

Now let

º   )
º    )

ÿ $   Å +.- l 6Bi¹ l J+>4- l 6>H<9ý ÿ ¸  Å 
º   
< ÿs +.- l 6Bt+.- l 6>
) zÿs$  Å4i¹ l +>"- l 67<=+.- l 6>H<9ý>ÿB¸  ÅvA
Because zÿ is modular, it is clear that º¼ } º¼  and therefore ¨;!t]¹t eÀÇ} º¼  . Because À




achieves º¤  as well as ÿ  +.-KlX6Bt+.-KlX6> , it follows that (again because ÿ is modular), À achieves

also º    .

Since N¸ Å i¹ i¹ l Z+>4K
- l;67</+.E- l76>P does not have any plan, there should not be any plan for
 
N¸  Å i¹  tº    P . The  fact that À is a plan for this instance implies that û cannot be a compilation
scheme, which is the desired contradiction.
Using Propositions 4 and 5 as well as Theorem 9, this result can be generalized as follows (see
also Table 1).
Corollary 12 ç%íQësì , ç%í]ì , and  ç
preserving plan size linearly.

S ësì


cannot be compiled to ç%íQë or any formalism specializing ç%íë

This answers the question of whether more space efficient compilation schemes from ç ì to
S
ç than the one proposed by Gazen and Knoblock (1997) are possible. Even assuming unbounded
computational resources for the compilation process, a more space efficient compilation scheme is
impossible—provided that the compilation should preserve plan size linearly. If we allow polynomially larger plans, then efficient compilation schemes are possible (see Section 6).
8. This result demonstrates that the choice of the semantics can be very important. If we interpret conditional effects
sequentially as Brewka and Hertzberg (1993) do, then there exists an straightforward compilation scheme preserving
plan size exactly.

294

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

5.2 Non-Uniform Complexity Classes
In the next section we make use of so-called non-uniform complexity classes, which are defined
using advice-taking machines, in order to prove the impossibility of a compilation scheme. An
advice-taking Turing machine is a Turing machine with an advice oracle, which is a (not necessarily recursive) function  from positive integers to bit strings. On input  , the machine loads the
bit string  i0¾0 "0¾0Ñ and then continues as usual. Note that the oracle derives its bit string only from the
length of the input and not from the contents of the input. An advice is said to be polynomial if the
oracle string is polynomially bounded by the instance size. Further, if  is a complexity class defined in terms of resource-bounded machines, e.g., P or NP, then J]pBnm  (also called non-uniform
X) is the class of problems that can be decided on machines with the same resource bounds and
polynomial advice.
Because of the advice oracle, the class P/poly appears to be much more powerful than P. HowY es]pBnm 
ever, it seems unlikely that P/poly contains all of NP. In fact, one can prove that fJe
implies certain relationships between uniform complexity classes that are believed to be very unlikely. For stating this result, we first have to introduce the polynomial hierarchy.
Let X be a class of decision problems. Then e+ denotes the class of decision problems 
that can be decided in polynomial time by a deterministic Turing machine that is allowed to use a
procedure—a so-called oracle—for deciding a problem Ø , whereby executing the procedure
does only cost constant time. Similarly, fJe  denotes the class of decision problems  such that
· of
 in polynomial time using
there is a nondeterministic Turing-machine that solves all
instances



are defined as follows:
an oracle for ~p . Based on these notions, the sets À ,  , and


·





À

I

)

À Vå  )

· V å  )

å )
V 



·





V

V

V



)
)eÕ
e I   I

 
fJe
ygn fJe   A

Thus,  )gfhe and

 )ygnfJe . The set of all classes defined in this way is called the polynomial
hierarchy, denoted by PH. Note that

F)  À
·  V I

e^


Y 




V


)  
· V Y I


V

) 
V I

·

Y


V

­

"A

e sekD

Further we have, À

and  
À V å  . As with other classes, it is unknown whether
V
V
V
V
V
the inclusions between the classes are proper. However, it is strongly believed that this is the case,
i.e., that the hierarchy is truly infinite.
Based on the firm belief that the polynomial hierarchy is proper, the above mentioned question
Y eE]pBnm  can be answered. It has been shown that fJe Y es]pBnm  would imply
·  that
of whether fJe

the polynomial hierarchy collapses on the second level (Karp & Lipton, 1982), i.e.,  )
Y yg nfhes] pB.nThis,
m  or
however, is considered to be quite unlikely. Further, it has been shown that fJe
Y
ygnfJe  fJ· es ]pBnm  implies that the polynomial hierarchy collapses at the third level (Yap, 1983),
i.e., 
) ¥ , which again is considered to be very unlikely. We will use these result for proving
¥
that for some pairs of formalisms it is very unlikely that one formalism can be compiled into the
other one.
9. The super-script  is only used to distinguish these sets from the analogous sets in the Kleene hierarchy.

295

N EBEL

5.3 On the Expressive Power of Partial State Specifications and Boolean Formulae
In all the cases considered so far, operators over partial state specifications could be compiled to
operators over complete state specifications, i.e., partial state specifications did not add any expressiveness. This is no longer true, however, if we also allow for arbitrary boolean formulae in
preconditions and effect conditions. In this case, we can decide the coNP-complete problem of
whether a formula is a tautology by deciding whether a one-step plan exists. Asking, for example,
if the ç íQë -instance N$²t+]N´	KPe6Beµt+\v6>P has a plan is equivalent to asking whether  is a tautology.
Let the one-step plan existence problem (1-PLANEX) be the PLANEX problem restricted to
plans of size one. From the above it is evident that ç%íQëBì -1-PLANEX and ç%íQë -1-PLANEX are
coNP-hard. Let - be some fixed polynomial, then the polynomial step plan-existence problem
(- -PLANEX) is the PLANEX problem restricted to plans that have length bounded by - ßÕ , if ß is
the size of the planning instance. As is easy to see, this problem is in NP for all formalisms except
ç%íQëBì and ç%íQë . The reason is that after guessing a sequence of operators and state specifications of
polynomial size, one can verify for each step in polynomial time that the precondition is satisfied
by the current state specification and produces the next state specification. Since there are only
polynomially many steps, the overall verification takes only polynomial time.
Proposition 13  -- -PLANEX can be solved in polynomial time on a nondeterministic Turing machine for all formalisms different from ç%íësì and ç%íQë .
From the fact that ç%íë -1-PLANEX is coNP-hard and, e.g., ç%ísì -p-PLANEX is in NP, it follows
almost immediately that there is no polynomial-time compilation scheme from ç íQë to ç í]ì that
 ygnfJe ). However, even if we allow for unbounded
preserves plan length polynomially (if fheÊ)à
computational resources of the compilation process, a proof technique first used by Kautz and· Sel
).
man (1992) can be used to show that such a compilation scheme cannot exist (provided  ) 

¥

Theorem 14

ç%íQë

cannot be compiled to ç%í]ì preserving plan size polynomially, unless 



¥ )

· ¥

¥.

Proof. Let  be a propositional formula of size ß in conjunctive normal form with three literals per
clause. As a first step, we construct for each ß a ç%íë domain structure ¸*Å with size polynomial in ß
and the following properties. Unsatisfiability of an arbitrary 3CNF formula  of size ß is equivalent
to Ý -step plan existence for the ç%íë - Ý -PLANEX instance N¸8Å`i¹1 t+\`6>P , where ¹1 can be computed
in polynomial time from  .
Given a set of ß atoms, denoted by ÕÅ , we define the set of clauses ¼Å to be the set containing
all clauses with three literals that can be built using these atoms. The size of  Å is | ß ¥  , i.e.,
polynomial in ß . Let ?Å be a set of new atoms -E Î.Ï   Ï ¢¡£ corresponding one-to-one to the clauses
in ÇÅ . Further, let ¤
B

Å)¦¥§@9 F   F   F ¥ M-  .Î Ï   Ï ¢¡¨£ (0\+ F   F   F ¥ 6M¼Å A
We now construct a ç%íë domain structure ¸8Å)~N$Åvt©ÅKP for all formulae of size ß as follows:

 Å
&
©Å

¤

)

)

ÕÅ²<?Å²<=+\v6B
+]N +> Åv6Bt+\v6>Pe6BA


296

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

Let © be a function that determines for all 3CNF formulae  , which atoms in ?Å correspond to the
clauses in the formula , i.e.,

¤*()È+.-  .Î Ï   Ï ¢¡£ 0\+ F   F   F ¥ 63(6BA
Now, the initial state for any particular formula  of size ß is computed as follows:
¹ )h ©¤*H<1 ?ÅM~©¤*.H<=+>^v6BA
1
From the construction, it follows that there exists a one-step plan for N$&Å`t©Å`i¹1"t+\v6>P
©

iff



is

unsatisfiable.
Let us now assume that there exists a compilation scheme û from ç%íQë to ç%í]ì preserving plan
size polynomially. Further, let us assume that the ç%íë domain structure ¸8Å is compiled to the ç%í]ì
domain structure ¸*Å )N$Å t©Å P . Using this compiled domain structure, we can construct the
following advice-taking Turing machine.
On input of a formula  of size ß , we load the advice N¸8Å eý â $&Å`t©Åeý>ÿ]$&Å`t©Å.P . This
advice is polynomial because ¸8Å is polynomial in the size of  and a compilation scheme generates
only polynomially larger domain structures. Because â is a polynomial-time function and ¹1 can
be computed from  in polynomial time, we can compute

¹  )
 â.$ Å i¹  % <9ý>âb$ Å t© Å 
in polynomial time. Also the goal specification

º  )ª ÿ $ Å t+\v6>Õ<9ý ÿ $ Å t© Å 
can be computed in polynomial time. Finally, we decide the - -PLANEX problem on the resulting
ç%í]ì -instance N¸ Å i¹  tº  P . From Proposition 13 we know that this can be done in polynomial time

on a nondeterministic Turing machine.
Because deciding - -PLANEX for N¸ Å i¹  tº  P is equivalent to deciding Ý -PLANEX for
N¸8Å4i¹1"t+\v6>P , which is in turn equivalent to deciding unsatisfiability of  , it follows that we can
decide a coNP-complete problem on a nondeterministic, polynomial advice-taking Turing machine
Y fhe5]pBnm  . Using Yap's (1983) result, the
in polynomial time. From that it follows that ygnfJe
claim follows.
Using Proposition 4 and Proposition 5, the above result generalizes as follows (see also Table 1).

Corollary 15 ç%íQësì and ç%íë cannot
be
·  compiled to any of the other planning formalisms preserving

.
plan size polynomially, unless  )

¥

¥

If we restrict the form of the formulae, however, we may be able to devise compilation schemes
from ç%íQë to, e.g., ç%í . Reconsidering the proof of the last theorem, it turns out that it is essential
to use the negation of a CNF formula as a precondition. If we restrict ourselves to CNF formulae
in preconditions, it seems possible to move from partial to complete state descriptions using ideas
similar to the ones used in the proof of Lemma 7.
However, no such compilation scheme will work for ç%íësì . The reason is the condition
$D post LB.Ç)  $D post LB. in the definition of the function ¨ . If this condition is not satisfied, the result of the operator is inconsistent. This condition could be easily employed to reduce
unsatisfiability of CNF formulae to 1-step plan existence, which enables us to use the same technique as in the proof of the above theorem.
297

N EBEL

5.4 Circuit Complexity
For the next impossibility result we need the notions of boolean circuits and families of circuits.
A boolean circuit is a directed, acyclic graph «á)ä'¬*­« , where the nodes ¬ are called gates.
Each gate 2§
¬ has a type ®¯K!,2K«+>
&:Ýiã6<F+1  gAgAgA26 . The gates with ®¯K!,2K
 
+sÝiã    gAgAgA26 have in-degree zero, the gates with ®¯K!,2K«³+>6 have in-degree one, and the
gates with ®¯K!,2K+>
6 have in-degree two. All gates except one have at least one outgoing
edge. The gate with no outgoing edge is called the output gate. The gates with no incoming edges
are called the input gates. The depth of a circuit is the length of the longest path from an input gate
to the output gate. The size of a circuit is the number of gates in the circuit.
Given a value assignment to the variables +1
output gate in the obvious way. For example, for 
gate of the circuit shown in Figure 4.

    gAgAgA¾6 , the circuit computes the value of the
 )~Ý and   )ã we get the value 1 at the output

°

±

Î

 



Figure 4: Example of a boolean circuit

Instead of using circuits for computing boolean functions, we can also use them for accepting
Å
words of length ß in +\ã:Ý>6 ¿ . A word ² )H AgAgA¨4ÅÈõ+\ã:Ý>6 is now interpreted as a value

assignment to the ß input variables  gAgAgA:4Å of a circuit. The word is accepted iff the output gate

has value 1 for this word. In order to deal with words of different length, we need one circuit for
each possible length. A family of circuits is an infinite sequence © )O'« (« gAgAgAw , where «Å has

ß input variables. The language accepted by such a family of circuits is theI set of words ² such that
«  Ñ  ³8 Ñ  accepts ² .
Usually, one considers so-called uniform families of circuits, i.e., circuits that can be generated
on a Turing machine with a ´µPk¶(ß -space bound. Sometimes, however, also non-uniform families are
interesting. For example, the class of languages accepted by non-uniform families of polynomiallysized circuits is just the class P/poly introduced in Section 5.2.
Using restrictions on the size and depth of the circuits, we can now define new complexity
classes, which in their uniform variants are all subsets of P. One class that is important in the
following is the class of languages accepted by uniform families of circuits with polynomial size
and logarithmic depth, named NC  . Another class which proves to be important for us is defined
in terms of non-standard circuits, namely circuits with gates that have unbounded fan-in. Instead of
restricting the in-degree of each gate to be two at maximum, we now allow an unbounded in-degree.
The class of languages accepted by families of polynomially sized circuits with unbounded fan-in
and constant depth is called ACI .
298

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

Y

From the definition, it follows almost immediately that AC I
NC  . Moreover, it has been
shown that there are some languages in NC  that are not in the non-uniform variant of ACI , which
implies that AC I )  NC  (Furst, Saxe, & Sipser, 1984).
5.5 Boolean Formulae Cannot be Compiled to Conditional Effects
As we have seen in Section 5.3, Boolean formulae are quite expressive if they are used in combination with partial state specifications. However, what if all state specifications are complete? In
this case, it seems to be possible to simulate the evaluation of CNF formulae by using conditional
effects. In fact, it is possible to compile in polynomial-time, for example, ç%í to ç ì preserving plan
S
size linearly, provided all formulae are in conjunctive normal form. Each operator would have to
be split into two operators, one that evaluates the clauses of all the formulae in the original operator
and one that combines these evaluations and takes the appropriate actions, e.g., asserting 	 if the
precondition is not satisfied. Sequencing of these pairs of operators can be achieved by introducing
some extra literals.
What can we say about the general case, however? When trying to simulate the evaluation of
an arbitrary logical formula using conditional effects, it seems to be the case that we need as many
operators as the nesting depth of the formula, which means that we would need plans that cannot be
bounded to be only linearly longer than the original plans.
We will use the results sketched in Section 5.4 to separate ç%í and ç ì . In order to do so, let us
S
view domain structures with fixed size plans as “machines” that accept languages. For all words ²
consisting of ß bits, let

¸8Å«)ON$Å²<=+\`6Bt©«ÅvPA
Assume that the atoms in Å are numbered from 1 to ß . Then a word ²

consisting of ß bits could

be encoded by the set of literals

¹³ ,
) +.- â 0

if the Ü th bit of ² is

Ý 6<=+>4- â 0 if the Ü th bit of ² is ãK6BA

Conversely, for a consistent state specification D Å , let ² ¢ be a word such that the Ü th bit is 1 iff
- â /D .
We now say that the ß -bit word ² is accepted with a one-step or Á -step plan by ¸8Å iff there
exists a one-step or Á -step plan, respectively, for the instance
·
Å)~N.N$Å<=\+ v6Bt©ÅKPi¹ ³ <=+>^ v6Bt\+ `6>PA
Similarly to families of circuits, we also define families of domain structures, ·à)£¸ e¸ gAgAgAÉ .

I
The language accepted by such a family with a one-step (or Á -step) plan is the set of words accepted
using the domain structure ¸8Å for words of length ß . Borrowing the notion of uniformity as well,
we say that a family of domain structures is uniform if it can be generated by a ´µPk¶´ß -space Turing
machine.
Papadimitriou has pointed out that the languages accepted by uniform polynomially-sized
boolean expressions is identical to NC  (Papadimitriou, 1994, p. 386). As is easy to see, a family of ç%í domain structures is nothing more than a family of boolean expressions, provided we use
one-step plans for acceptance.
Proposition 16 The class of languages accepted by uniform families of ç%í domain structures using
one-step plan acceptance is identical to NC  .
299

N EBEL

If we now have a closer look at what the power of Á -step plan acceptance for families of ç ì
S
domain structures is, it turns out that it is less powerful than NC  . In order to show that, we will first
prove the following lemma that relates Á -step ç ì plans to circuits with gates of unbounded fan-in.

S

Y

Lemma 17 Let ¸F)~N$¬t©¼P be a ç ì domain structure, let º
  , and let À be a Á -step plan over
S
¸ . Then there exists a polynomially sized boolean circuit « with unbounded fan-in and depth ¸>Á;T
such that À is a plan for N¸&i¹tº5P iff the circuit « has value 1 for the input ²º¹ .
Proof. The general structure of a circuit for a Á -step
±
±

.
.
.

¼

plan is displayed in Figure 5. For each

°

»

Î

1½

1½¡

.....

Î

.
.
.

.
.
.

¼

.
.
.

1½

ç Sì

¼¡

Ò

¾

1½

.
.
.

.....

Î

. . . ¾s¿

Ò

¼

Figure 5: Circuit structure and goal testing for a Á -step ç

Sì

plan

7

plan step (or level) 8 and each atom - â , there is a connection - â . The connections on level ã are
the input gates, i.e., - âI )¦ â . The goal test is performed by an 
 -gate that checks that all the goals
are true on level Á , in our case º£)Ì+.- 4- - Å 6 . Further, using the  -gate, it is checked that no
 
inconsistency was generated when executing the plan.
For each plan step 8 , it must be computed whether the precondition is satisfied and what the
result of the conditional effects are. Figure 6 (a) displays the precondition test for the precondition
+.-  -  4- ¥ 6 . If the conjunction of the precondition literals is not true, 	 V becomes true, which is
connected to the  -gate in Figure 5.
Without loss of generality (using a polynomial transformation), we assume that all conditional
VGF . Whether the effect F is activated on level 8 is computed by a circuit as
effects have the form 
V 4- â .
displayed in Figure 6 (b), which shows the circuit for +.- 4- 6
 ¥
Finally, all activated effects are combined by the circuit shown in Figure 6 (c). For all atoms - â ,
we check whether both - â and 4- â have been activated, which would set 	ÁÀ true. This is again one
of the inputs of the  -gate in Figure 5. If neither - â nor 4- â have been activated, the value of - â on
level 8N±Ý is determined by the value of - â on level 8 . Otherwise the value of - â on level 8Â³Ý is
Ï7
determined by the value of -WâÃ , i.e., the activation value of the positive effect - â on level 8 .
The depths of the circuits in Figure 6 (b) and (c) dominate the depth of the circuit necessary to
represent one plan step leading to the conclusion that a plan step can be represented using a circuit
of depth 7. Adding the depth of the goal testing circuit, the claim follows.
The lemma implies that ç ì¬Á -step plan acceptance is indeed less powerful than ç%í 1-step plan
S
acceptance, which means that a compilation scheme from ç%í to ç ì preserving plan size linearly is
S
impossible.
300

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

±
 Ä

Ú

¾ 
±

1ÆzÇ Ä
»

»
Úæ Î

± °
°

°

...

°

±

»

¾sÅ
±

°
±
¨Ä

Î

(Ä 

(Ä ¡

¨Ä

(a)

Î

¨Ä 

Ú

¨Ä ¡

 ÆzÇ Ä

(b)

Ú

 ÆzÇ Ä

(Ä

Ú

(c)

Figure 6: Circuit structure for precondition testing (a), conditional effects (b), and the computation
of effects (c) for ç ì operators

S

Theorem 18

ç í  i 

, for all members



S ësì

of the ç

-class.

Proof. We show that ç%í1  i ç ì , from which by Theorem 9 and Proposition 4 the claim follows.
S
Assume for contradiction that ç í  i ç ì . Let ·Ø) ¸ e¸ gAgAgAÉ be a uniform family of ç í
S

I
domain structures and ·¬`)Ê¸8 e¸8 gAgAgAw be the ç ì domain structures generated by a compilation
S

I
scheme û that preserves plan size linearly. By Lemma 17 we know that for each ç ì domain
S
structure ¸ Å )~N$ Å t© Å P and given goal º  we can generate a polynomially sized, unbounded fanin circuit with depth ¸>ÁT that tests whether a particular Á -step plan achieves the goal. In order to
decide Á -step plan existence, we must test |i0°© Å 0 ie different plans, which is polynomial in the size
of ¸8Å because û is a compilation scheme. For each plan, we can generate one test circuit, and by
adding another  -gate we can decide Á -step plan existence using a circuit with depth ¸>ÁWÈ and size
polynomial in the size of ¸8Å . Further, since the state-translation functions are modular, the results
of â for fixed  can be computed using an additional level of gates. Since by Proposition 16 all
languages in NC  are accepted by uniform families of ç%í domain structures using one-step plan
acceptance, our assumption ç%íÉ i ç ì implies that we can accept all language in NC  by (possibly
S
non-uniform) ACI circuits, which is impossible by the result of Furst and colleagues (1984).
Using the Propositions 4 and 5 again, we can generalize the above theorem as follows.
Corollary 19

ç í]ì

and ç

í

cannot be compiled to



ç S Bë ìA

or



ç S ë 

preserving plan size linearly.

6. Compilability Preserving Plan Size Polynomially
As has been shown in the previous section, only the compilation schemes induced by Propositions 4
and 5 and the ones identified in Section 4 allow for compilation schemes preserving plan size exactly. For all other pairs of formalisms we were able to rule out such compilation schemes—even
301

N EBEL

if we allow linear growth of the resulting plans. Nevertheless, there might still be a chance for
compilation schemes preserving plan size polynomially. Having shown that ç%íQësì and ç%íQë cannot
be compiled to the other formalisms even if the plan can grow polynomially, we may still be able
to find compilation schemes preserving plan size polynomially for the ç%íQësì /ç%íë pair and for the
remaining formalisms.
A preview of the results of this section is given in Figure 7. As it can be seen, we are able

î`ï{ð>ñ

î ð>ñ

î ò>ðñ

î ïñ

î ï{ð

î òBñ

î ò>ð

î ï

î ñ

îð

î ò

î
Figure 7: Equivalence classes of planning formalisms created by polynomial-time compilation
schemes preserving plan size polynomially. Compilation schemes constructed in this
section are indicated by dashed lines

to establish compilation schemes preserving plan size polynomially for all pairs of formalisms for
which we have not proved the impossibility of such compilation schemes.
6.1 Compiling Conditional Effects Away for Partial State Specifications
The first compilation scheme we will develop is one from ç íQësì to ç íQë . As before, we assume that
the conditional effects have only singleton effect sets. Further, since we can use arbitrary boolean
formulae in the effect conditions in ç íQësì , we assume that there is only one rule for each effect literal.
Using a simple polynomial transformation, arbitrary sets of operators can be brought into this form.
 $D post LB. considerably, because now
This simplifies checking the condition $D post LB.)
only one rule can activate a particular literal.
302

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

In order to simulate the parallel behavior of conditional effects, we have to break them up into
individual operators that are executed sequentially. This means that for each conditional effect of
an operator we introduce two new operators. One simulates the successful application of the rule,
the other one simulates the “blocking” situation of the rule. At least one of these operators must
be executed for each conditional effect in the original operator. This is something we can force
by additional literals that are added to control the execution of operators. All in all this leads to a
sequence of operators that has length bounded by the number of conditional effects in the original
operator.
If we want to simulate the parallel behavior by a sequence of unconditional operators, the effects
of the unconditional operators should not directly influence the state description, but the effect
should be deferred until all operators corresponding to the set of conditional effects have been
executed. For this reason, we will use a sequence of “copying operators” which copy the activated
effects to the state description after all “conditional operators” have been executed. These “copying
operators” can also be used to check that the set of activated effects is consistent.
Theorem 20

ç íQësì

can be compiled to ç

íQë

in polynomial time preserving plan size polynomially.

Proof. Assume that ¸) N$¬t©¼P is the ç%íQëBì source domain structure and assume further, without
loss of generality (using a polynomial transformation), that all operators have the form

L â )~N pre L â t+{ â Ï  VXF â Ï  gAgAgA\e â Ï :Ú VGF â Ï :Úz6>P
F
F
F
with  â Ï 73  , â Ï 7  , and â Ï 7«)  â Ï for ÜX¦
)  .
V
Let  å and  3 be disjoint copies of  , which are used to record the active effects of conditional
be another disjoint copy, which is used to record that an active effect has not
effects, and let 
l
been copied yet. Further, let m
 ÊJ),+.-4*0zL«W©?6 be a new set of atoms corresponding one-to-one to
6
the operators in © and let be a set of symbols corresponding one-to-one to all conditional effects
in © , i.e.,
6
V F â Ï 7 & post L>â@iL{â81©?6BA
),+14â Ï 7 0Éâ Ï 7 X
Finally, let Á be a fresh atom not appearing in <= å </ 3 </
<=ºÊ that signals that copying
l
the active effects to the state specification is in progress. The set of symbols   for the compiled
domain structure is then

  )F<9 å <5
3

<5
l

<9mÊ¶<

6

</+\Á\6BA

For each operator L â © , the compilation scheme introduces a number of new operators. The
first operator we introduce is one which checks whether the conditional effects of the previous
operators have all been executed, no copying is in progress and the precondition is satisfied. If this
is the case, the execution of the conditional effects for this operator is started:

L pre
â )~N pre L{â$H<5´
Ê

6

<=+>8Á{6B(+.- bÚ 6&<3(7å?<5(43¼<5

<3(
l

PA

This operator enables all the “conditional effect operators.” For the activated effects, we introduce
the following operators:

LkâË Ï â Ï 7 ~
) N +.-4bÚ4
5 â Ï 7Q6B*+1 â Ï 7>6&</+.- å l

303

0z-?) F â Ï 7>67<=+.- 3  l

0:4-¤) F â Ï 76>PA

N EBEL

In words, if the effect condition is entailed, then the activated positive or negative effect as well as
the fact that the rule has been tried is recorded.
Since there is at most one effect literal for each conditional effect, a conditional effect is
“blocked” if the negation of the ± effect condition is entailed by the state specification. For all
“blocked conditional effects” we introduce the following operators:

L â Ïâ Ï7 ~
) N +.- bÚ 
5(â Ï 7 6B(+1`â Ï 7 6>PA

In order to check that all conditional effects have been tried (activating the corresponding effect
or not activating it because the conditional effect is blocked), the following operator is used:

L âÃ O
) N +.-4bÚb6<=+1 â Ï 7

6

0 â Ï 7 VGF â Ï 7\

post L

â e6B(+\Á{6&<=+>"-4bÚb6>PA

This operator enables copying of the activated effects to the state specification, which is achieved
with the following set of operators for each atom -3/ :


Lå

L
L



)
3
¾

)

)

N +\Á>- å 4- 3 - l B6 t+.-"- l >6 P
N +\Á>4-"å-53(- l B6 t+>4-Õ4- l >6 P
N +\Á>- å - 3 - l 6B
	 PA

Finally, we need an operator that checks that all possible effects have been copied. This operator
also starts the “execution cycle” again by enabling the execution of another “precondition operator:”

Li ~
) N +\Á{67<3(
l

8+>*Á\6>PA

Using these definitions, we can now specify the set of compiled operators:
±
pre

©  )+\L â iL âÃ 0ÄL â 1©?6&< VHF
+\L Ë Ï â Ï 7 0ÄL â ¶©?\ â Ï 7 VXF â Ï 7\ post L â e67<
+\L  Ï â Ï 7  0ÄL â  1©9\ â Ï 7
â Ï 7\& post L â e6&<
+\L å iL 3 iL ¾ 0 -3W¦6<
+\LAit6BA
Based on that, we specify a compilation scheme û¬)ONý>þ{eý>âzeý ÿ  âz ÿ P as follows:
ý>þ>¸ ) N$  t©  P
6
ý>âb¸ ) (7å?<5(Á
 3Ç<9´ l <3( Ê <5 </+>8Á{6B
ý>ÿs¸ ) (m
 Ê¶<3+>*Á\6B
â $²i¹\ ) ¹
zÿs$¬tº9) º=A
The scheme û obviously satisfies conditions (2) and (3)
· for compilation schemes
· and all the functions can be computed in polynomial time. Further, ¼  is a ç íQë -instance if is a ç ± íësì -instance.
Let now Dü   be a legal ç%íësì state specification and let D  ) ¨«$DiL â  for some operator
it is clear that if D¦0)Ã	 , then there exists a sequence À« of
L>âX© . From the above discussion,
Ï â Ï 7 and L Ï â Ï 7 followed
pre
operators from ©  consisting of L â , followed by operators of the form L âË
â

by the operator L âÃ , followed in turn by operators L  , followed finally by the operator L i , such that
D  )¨;!t]$D5<9ý â ¸´eÀ  s F¬ A
304

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

Conversely, if D8%0 )	 , then there does not exist any plan that transforms

¨«$D5<9ý>âb¸´iL pre
â 
into a legal state specification that contains 8Á and 4-4bÚ .
Using
· induction on the plan length, it· follows from the arguments above that there exists a plan
À for iff there exists a plan À« for Ç  and for every such plan we have 0¾0 À«@0¾0vÂÊ0¾0 À30¾0 ª M ÈÁ
T <= , with <
being the maximum number of conditional effects in operators of © . Hence û is a
polynomial-time compilation scheme preserving plan size polynomially.
An immediate consequence of this theorem is that ç%íësì and ç%íQë form an equivalence class with
respect to compilability preserving plan size polynomially.
Corollary 21
polynomially.

ç%íQësì

and

ç%íë

are polynomial-time compilable to each other preserving plan size

Further, we know from Corollary 15 that this class cannot become larger.
As in the case of compiling ç ësì to ç ì , however, the result depends on the semantics chosen
S
S
for executing conditional effects over partial state specifications. If we use the alternative semantics where the resulting state specification is legal when the application of all state-transformation
functions leads to a theory that can be represented as a set of literals, it seems likely that there exists
another scheme that preserves plan size polynomially. However, if we use the alternative semantics


that deletes all the literals in ; $D post LB.(I$D post LB. if $D post LB is consistent, then it
appears to be very unlikely that we are able to identify a compilation scheme that preserves plan
size polynomially.
6.2 Compiling Conditional Effects Away for Complete State Specifications
The next compilation scheme compiles ç%í]ì to ç%í and ç ì to ç . Since we deal with complete state
S
S
 $D8 post LB. , which
specification, we do not have to take care of the condition M$D8 post LB.M)
is always true for complete states. This makes the compilation scheme somewhat simpler. Since
ç S does not allow for general boolean formulae, the scheme becomes a little bit more difficult. In
general, however, the compilation scheme we will specify is very similar to the one given in the
proof of Theorem 20.
Theorem 22 ç%í]ì can be compiled to
serving plan size polynomially.

ç%í

and ç

Sì

can be compiled to ç

S

in polynomial time pre-

Proof. As in the proof of Theorem 20, we assume that ¸Ê) N$¬t©¼P is the (ç%í]ì or
domain structure. Further, we assume that all operators have the form

V F â Ï :Úb6>P
L â )~N pre L â t+ U â Ï  VXF â Ï  gAgAgA\ U â Ï :Ú X
F
U Y   if ¸ is a ç%í]ì structure or U â Ï 7 Y   if ¸ is a ç ì
with â Ï 7M   and â Ï 7
S

ç Sì

) source

structure. This means
that we do not assume the effects to be unique for each conditional effect.
In addition, we assume the same set symbols for the compiled domain structure as in the proof
of Theorem 20:

  )F<9 å <5

3

<5

305

l

<9mÊ¶<

6

</+\Á\6BA

N EBEL



pre
For each operator L â ¶© , we introduce the operators L â , L âÃ , L
Theorem 20.
In addition, the following operators are needed:
±

LkâË Ï â Ï 7 )
L â Ï â Ï 7eÏ : )

å ,L


3

,L


¾

N +.-4bÚb6&< U â Ï 7>*+1 â Ï 7Q6<=+.- å  - l 0@-?) F â Ï 76&<=+.- 3  N +.-4bÚb6&</+>* â Ï 7eÏ :0t â Ï 7eÏ : U â Ï 76B*+1 â Ï 7>6>PA

, and L

l

i

as in the proof of

0:4-¤) F â Ï 76>P

The
compiled set of operators © contains all of the above operators and the compilation scheme is
±
identical to the scheme presented in the proof of Theorem 20. This means that the only significant
difference to the compilation scheme presented in the proof of Theorem 20 is the operator scheme
L â Ï â Ï 7eÏ : which tests for each rule whether it contains an effect condition that blocks the rule. Since
we have complete state specifications, every conditional effect is either activated or blocked, and
the  â Ï 7 's are used to record that the execution of each conditional effect has been tried.
Using now similar arguments as in the proof of Theorem 20, it follows that this compilation
scheme is indeed a scheme that leads to the claim made in the theorem.


It follows that ç ísì and ç í are equivalent with respect to   and all formalisms in  ç ësì  and
S

 ç ë  are equivalent with respect to   . These two sets could be merged into one equivalence class,
S
provided we are able to prove that, e.g., ç%í can be compiled to ç .

S

6.3 Compiling Boolean Formulae Away
In Section 5.5 we showed that it is impossible to compile boolean formulae to conditional effects if
plans are only allowed to grow linearly. However, we also sketched already the idea of a compilation
scheme that preserves plan size polynomially. Here we will now show that we can compile boolean
formulae to ç , which is expressively equivalent to basic STRIPS, i.e., we can compile boolean
S
formulae away completely.
Theorem 23

ç%í

is polynomial-time compilable to ç

S

preserving plan size polynomially.

Proof. Assume that ¸F)ON$¬t©¼P is a ç í domain structure. Further assume without loss of generality
Y   and  â =  (i.e., we have just
that all operators L â 1© are of the form L â )ON â i â P , with  â
one formula as the precondition instead of a set of formulae).
Let mÌ and  Ì be two new sets of atoms corresponding one-to-one to all sub-formulae occurring in preconditions of operators in © . These new atoms are denoted by [\Í and [>Í  for the
sub-formula Î . Atoms of the form [{Í are used to record that the truth-value of the sub-formula Î
has been computed and the atoms of the form [\Í are used to store the computed truth-value.
For each operator L â )~N â i â P , we will have in the target operator set the following operator:

L â )ON +\[QÚ.i[ B Ú 6Bi â <5( Ì PA
The set of all operators generated in this way is denoted by ©  .
Further, for each atom -3/ , we introduce the following two operators:

L å
L



3

)

)

N +.-6B*+\[   i[  6>P
N +>"-H6B*+\[   8[  6>PA

The set of operators generated in this way is denoted by
306

© 

.

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

For each sub-formula occurring in preconditions of
operators are introduced:

L Íå

LÍ  )
L Í3  )
For sub-formulae ÎJ)ªÎ

 Î
 
L Íå 
L Íå 
L Í3

of the form Î)§Î

 
pÎ 

the following

N +\[ Í Î i[ Í  i[\Í Î i[\Í  B6 t+\[ Í i[\ÍÕ6>P
N +\[ Í Î *[\Í Î 6Bt+\[ Í 8\[ ÍH6>P
N +\[ Í   *[\Í  6Bt+\[ Í 8\[ ÍH6>PA
)

3

©

 , the following operators are introduced:
) N +\[ Í Î i\[ Í Î 6Bt+\[ Í i\[ ÍÕ6>P
) N +\[ Í  i\[ Í  6Bt+\[ Í i\[ ÍÕ6>P
) N +\[ Í Î i[ Í  8\[ Í Î 81[ Í  6Bt+\[ Í *\[ ÍH6>PA

Finally, for ÎJ)^Ï , we have the following operators:

L Íå
L

Í

3

)

)

N +\[ Ð   8[ Ð 6Bt+\[ Í i [\Í6>P
N +\[ Ð  i [ Ð 6Bt+\[ Í 8\[ Í6>PA

The set of operators generated by sub-formulae is denoted by
Now we can specify the compilation scheme û :

ý>þ¸´)
ý â ¸´)
ý>ÿB¸´)
â $¬i¹\)
ÿs$²tº9)

©ÉÌ

.

$N §<9mÌ9<5 Ì 8©  </©  </©UÌ*P
´ Ì 
´ Ì 
¹
º/A

From the construction it is obvious that all the functions are polynomial-time computable, that
·
· the induced function  is a reduction,
and that for
the state-translation functions are modular, that
every plan À for a source planning instance there exists a plan À« for Ç  such that 0¾0 À«@0¾0Â
0¾0 À30¾0 ª ,<Ñ^Ý\ , with < being the maximum number of sub-formulae of preconditions in © . From
that, the claim follows.
There might be the question whether compiling boolean formulae away could be done more
efficiently. Using the result that boolean expressions can be evaluated by circuits with logarithmic
depth, this should be indeed possible. However, we are satisfied here with the result that there is
a compilation scheme preserving plan size polynomially at all. This result together with Theorem 22 settles the question for compilation schemes preserving plan size polynomially for all pairs
of formalisms.
Corollary 24 All formalisms  with  óç
S
each other preserving plan size polynomially.

ësì

or

307

 óç%í]ì

are polynomial-time compilable to

N EBEL

6.4 Parallel Execution Models and the Feasibility of Compilation Schemes Preserving Plan
Size Polynomially
While compilation schemes that preserve plan size exactly or linearly seem to be of immediate use,
a polynomial growth of the plan appears to be of little practical interest. Considering the practical
experience that planning algorithms can roughly be characterized by their property of how many
steps they can plan without getting caught by the combinatorial explosion and the fact that this
number is significantly smaller than 100, polynomial growth does not seem to make much sense.
If we take GRAPHPLAN (Blum & Furst, 1997) into consideration again—the planning system
that motivated our investigation in the first place—it turns out that this system allows for the parallel
execution of actions. Although parallel execution might seem to add to the power of the planning
system considerably, it does not affect our results at all. If a sequential plan can solve a planning
instance with ß steps, a parallel plan will also need at least ß actions. Nevertheless, although the size
of a plan (measured in the number of operations) might be the same, the number of time steps may
be considerably smaller—which might allow for a more efficient generation of the plan. Having a
look at the compilation scheme that compiles conditional effects away, it seems to be the case that
a large number of generated actions could be executed in parallel—in particular those actions that
simulate the conditional effects.
However, the semantics of parallel execution in GRAPHPLAN is quite restrictive. If one action
adds or deletes an atom that a second action adds or deletes or if one action deletes an atom that
a second action has in its precondition, then these two actions cannot be executed in parallel in
GRAPHPLAN . With this restriction, it seems to be impossible to compile conditional effects away
preserving the number of time steps in a plan. However, a compilation scheme that preserves the
number of time steps linearly seems to be possible. Instead of such a compilation scheme, the
approaches so far either used an exponential translation (Gazen & Knoblock, 1997) or modified the
GRAPHPLAN -algorithm in order to handle conditional effects (Anderson et al., 1998; Koehler et al.,
1997; Kambhampati et al., 1997). These modifications involve changes in the semantics of parallel
execution as well as changes in the search procedure. While all these implementations are compared
with the straightforward translation Gazen and Knoblock (1997) used, it would also be interesting
to compare them with a compilation scheme based on the ideas spelled out in Theorem 22 as the
base line.

7. Summary and Discussion
Motivated by the recent approaches to extend the GRAPHPLAN algorithm (Blum & Furst, 1997) to
deal with more expressive planning formalisms (Anderson et al., 1998; Gazen & Knoblock, 1997;
Kambhampati et al., 1997; Koehler et al., 1997), we asked what the term expressive power could
mean in this context. One reasonable intuition seems to be that the term expressive power refers
to how concisely domain structures and the corresponding plans can be expressed. Based on this
intuition and inspired by recent approaches in the area of knowledge compilation (Gogic et al., 1995;
Cadoli et al., 1996; Cadoli & Donini, 1997), we introduced the notion of compilability in order to
measure the relative expressiveness of planning formalisms. The basic idea is that a compilation
scheme can only transform the domain structure, i.e., the symbol set and the operators, while the
initial state and the goal specification are not transformed—modulo some small changes necessary
for technical reasons. Further, we distinguish compilation schemes according to whether the plan
in the target formalism has the same size (up to an additive constant), a size bounded linearly by the
308

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

size of the plan in the source formalism, or a size bounded polynomially by the original planning
instance and the original plan.
Although the compilability framework appears to be a straightforward and intuitive tool for
measuring the expressiveness of planning formalisms, it is possible to come up with alternative
measures. Bäckström (1995), for instance, proposed to use ESP-reductions, which are polynomial
many-one reductions on planning problems that preserve the plan size exactly. However, requiring
that the transformation should be polynomial-time computable seems to be overly restrictive. In
particular, if we want to prove that one formalism is not as expressive as another one, we had better
proven that there exists no compilation scheme regardless of how much computational resources
the compilation process may need. Furthermore, there appear to be severe technical problems to
using Bäckström's (1995) framework for proving negative results. On the other hand, all of the
positive results reported by Bäckström are achievable in the compilation framework because the
transformations he used are in fact compilation schemes. Taking all this together, it appears to be
the case that the compilation framework is superior from an intuitive and technical point of view.
Another approach to judging the expressiveness of planning formalisms has been proposed by
Erol and colleagues (1994, 1996). They measure the expressiveness of planning formalisms according to the set of plans a planning instance can have. While this approach contrasts hierarchical task
network planning nicely with STRIPS-planning, it does not help us in making distinctions between
the formalisms in the ç -family.
The compilability framework is mainly a theoretical tool to measure how concisely domain
structures and plans can be expressed. However, it also appears to be a good measure of how
difficult planning becomes when a new language feature is added. Polynomial-time compilation
schemes that preserve the plan size linearly indicate that it is easy to integrate the feature that is
compiled away. One can either use the compilation scheme as is or mimic the compilation scheme
by extending the planning algorithm. If only a polynomial-time compilation scheme leading to a
polynomial growth of the plan is possible, then this is an indication that adding the new feature
requires most probably a significant extension of the planning algorithm. If even a compilation
scheme preserving plan size polynomially can be ruled out, then there is most probably a serious
problem integrating the new feature.
Using this framework, we analyzed a large family of planning formalisms ranging from basic
STRIPS to formalisms with conditional effects, boolean formulae, and incomplete state specifications. The most surprising result of this analysis is that we are able to come up with a complete
classification. For each pair of formalisms, we were either able to construct a polynomial-time
compilation scheme with the required size bound on the resulting plans or we could prove that compilation schemes are impossible—even if the computational resources for the compilation process
are unbounded.
In particular, we showed for the formalisms considered in this paper:
»

»

»

incomplete state specifications and literals in preconditions can be compiled to basic STRIPS
preserving plan size exactly,
incomplete state specifications and literals in preconditions and effect conditions can be compiled away preserving plan size exactly, if we have already conditional effects,
and there are no other compilation schemes preserving plan size linearly except those implied
by the specialization relationship and those described above.
309

N EBEL

If we allow for polynomial growth of the plans in the target formalism, then all formalisms not
containing incomplete state specifications and boolean formulae are compilable to each other. Incomplete state specifications together with boolean formulae, however, seem to add significantly
to the expressiveness of a planning formalism, since these cannot be compiled away even when
allowing for polynomial growth of the plan and unbounded resources in the compilation process.
It should be noted, however, that some of these results hold only if we use the semantics for
conditional effects over partial state specifications as spelled out in Section 2.1. For other semantics,
we may get slightly different results concerning the compilability of conditional effects over partial
states.
One question one may ask is what happens if we consider formalisms with boolean formulae
that are syntactically restricted. As indicated at various places in the paper, restricted formulae,
such as CNF or DNF formulae, can sometimes be easily compiled away. However, there are also
cases when this is impossible. For example, it can be shown that CNF formulae cannot be compiled
to basic STRIPS preserving plan size linearly (Nebel, 1999), which confirms Bäckström's (1995)
conjecture that CNF-formulae in preconditions add to the expressive power of basic STRIPS.
Another question is how reasonable our restrictions on a compilation scheme are. In particular,
one may want to know whether non-modular state-translation functions could lead to more powerful
compilation schemes. First of all, requiring that the state-translation functions are modular seems
to be quite weak considering the fact that a compilation scheme should only be concerned with
the domain structure and that the initial state and goal specification should not be transformed at
all. Secondly, considering the fact that the state-translation functions do not depend on the operator
set, more complicated functions seem to be useless. From a more technical point of view, we need
modularity in order to prove that conditional effects and boolean formulae cannot be compiled away
preserving plan size linearly. For the conditional effects, modularity or a similar condition seems
to be crucial. For the case of boolean formulae, we could weaken the condition to the point that
we require only that state-translation functions are computable by circuits of constant depth—or
something similar. In any case, the additional freedom one gets from non-modular state-translation
functions does not seem to be of any help because these functions do not take the operators into
account. Nevertheless, it seems to be an interesting theoretical problem to prove that more powerful
state-translation functions do not add to the power of compilation schemes.
Although the paper is mainly theoretical, it was inspired by the recent approaches to extend
the GRAPHPLAN algorithm to handle more powerful planning formalisms containing conditional
effects. So, what are the answers we can give to open problems in the field of planning algorithm
design? First of all, Gazen and Knoblock's (1997) approach to compiling conditional effects away
is optimal if we do not want to allow plan growth more than by a constant factor. Secondly, all of
the other approaches (Anderson et al., 1998; Kambhampati et al., 1997; Koehler et al., 1997) that
modify the GRAPHPLAN algorithm are using a strategy similar to a polynomial-time compilation
scheme preserving plan size polynomially. For this reason, these approaches should be compared
to a “pure compilation approach” using the ideas from the compilation scheme developed in the
proof of Theorem 22 as the base line. Thirdly, allowing for unrestricted boolean formulae adds
again a level of expressivity because they cannot be compiled away with linear growth of the plan
size. In fact, approaches such as the one by Anderson and colleagues (1998) simply expand the
formulae to DNF accepting an exponential blow-up. Again, we cannot do better than that if plan
size should be preserved linearly. Fourthly, if we want to add partial state specifications on top of
general boolean formulae, this would amount to an increase of expressivity that is much larger than
310

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

adding conditional effects or general formulae to basic STRIPS, because in this case there is no way
to compile this away even if we allow for polynomial plan growth.
Finally, one may wonder how our results apply to planning approaches that are based on translating (bounded) planning problems to propositional logic such as SATPLAN (Kautz & Selman, 1996)
or BLACKBOX (Kautz & Selman, 1998). Since the entire analysis of the relative expressiveness of
planning formalisms uses the assumption that we compile from one planning formalism to another
planning formalism, the results do not tell us anything about the size of representations if we switch
to another formalism. In particular, it seems possible to find an encoding of (bounded) planning
problems with conditional operators in propositional logic which is as concise as an encoding of
unconditional operators. The only advice our results give is that such a concise encoding will not
be found by first translating conditional actions to unconditional actions and then using the “standard” encoding for unconditional actions (Kautz, McAllester, & Selman, 1996) to generate boolean
formulae. However, addressing the problem of determining the conciseness of representation in this
context appears to be an interesting and relevant topic for future research.

Acknowledgments
The research reported in this paper was started and partly carried out while the author enjoyed being
a visitor at the AI department of the University of New South Wales. Many thanks go to Norman
Foo, Maurice Pagnucco, and Abhaya Nayak and the rest of the AI department for the discussions
and cappuccinos. I would also like to thank Birgitt Jenner and Jacobo Toran for some clarifications
concerning circuit complexity.

Appendix A: Symbol Index
Symbol

Explanation
cardinality of a set
size of an instance
symbol used in conditional effects
ó
syntactic specialization relation
Ò
compilability relation with restriction  and Ó

boolean constant denoting falsity, also denoting
	
the illegal state specification

273 boolean constant denoting truth
 Ll
295 advice function
L¾\Ll
275, 275
active effects of an operator in a state or state specification
AC I
298 complexity class
«
298 boolean circuit
298 family of boolean circuits
©
coNP
272 complexity class
coNP/poly 295 non-uniform coNP
closing a set of literals w.r.t. 
$&%;(Ll 284
À 
277 plan, i.e., sequence of operators
295 complexity class in the polynomial hierarchy
À â

295 instance of a problem

0L0
V 0¾0 L]0¾0

Page
292
277
274
279
282
273

311

N EBEL

¹
û

ÇLl
ý>þ\eý â eý>ÿ
Uº


ÕF eÎ
Ï

; b
 
E¶\GHLl
NC 
"!@#% Ll
NP
NP/poly

L

©

©¿
- i[024
 L¾\
 Ll
P
P/poly
PH

277
282
282
282
277
274
273
287
273
273
274
298
273
272
295
274
276
277
274
275

272
295
295
273
>Ll
PLANEX
279
post
274
pre
274
·
PSPACE
272
· 
277
295
â
276
¨«L¾\Ll
¨;!t]L¾\Ll 277
C
274
D
274
ç
278
278
ç S
ç%í
278
278
ç`ë
ç ì
278
ç;ÐÑÐÑÐ
278
283
 ç ësì 
S
ç ë 
283
S
'(	Ll
273

273

initial state description
compilation scheme ( )~Ný>þ{eý â eý>ÿQ â zÿ>P )
transformation induced by compilation scheme
components of a compilation scheme
goal of a planning task
set of boolean formulae
boolean formulae
literal
sets of literals
all boolean formulae that use atoms from 
set of all models of a theory
complexity class
negative literals in a set of literals
complexity class
non-uniform NP
operator ( )~N pre  post P )
set of operators
set of finite sequences of operators
propositional atoms
potentially active effects of an operator
for a given state specification
complexity class
non-uniform P
the polynomial hierarchy
positive literals in a set of literals
plan existence problem
postconditions of an operator
preconditions of an operator
complexity class
planning instance ( )ON¸&i¹tº9P )
complexity class in the polynomial hierarchy
maps a state specification and an operator to a new state
extension of ¨«L¾\Ll to plans
state (or truth assignment)
state specification
the STRIPS planning formalism
STRIPS with literals in preconditions
STRIPS with boolean formulae in preconditions
STRIPS with incomplete state descriptions
STRIPS with conditional effects
STRIPS with combinations of the above extensions
equivalence classes induced by  
equivalence classes induced by   
all propositional atoms used in a set of literals
countably infinite set of propositional atoms
312

C OMPILABILITY







r

 â
â zÿ

²

X

 ,

¸
·

273
273
295
282
292
298
295
272
277
299

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

finite subset of 
set of literals overs 
complexity class in the polynomial hierarchy
state-translation functions in a compilation scheme
universal literals
a word over +\ã:Ý>6t¿
some complexity class
some planning formalisms
domain structure ( )ON$¬t©¼P )
family of domain structures

References
Anderson, C. R., Smith, D. E., & Weld, D. S. (1998). Conditional effects in Graphplan. In Proceedings of the 4th International Conference on Artificial Intelligence Planning Systems (AIPS98), pp. 44–53. AAAI Press, Menlo Park.
Baader, F. (1990). A formal definition for expressive power of knowledge representation languages.
In Proceedings of the 9th European Conference on Artificial Intelligence (ECAI-90) Stockholm, Sweden. Pitman.
Bäckström, C. (1995). Expressive equivalence of planning formalisms. Artificial Intelligence, 76(1–
2), 17–34.
Bäckström, C., & Nebel, B. (1995). Complexity results for SAS Ô planning. Computational Intelligence, 11(4), 625–655.
Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90(1-2), 279–298.
Brewka, G., & Hertzberg, J. (1993). How to do things with worlds: On formalizing actions and
plans.. Journal Logic and Computation, 3(5), 517–532.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning. Artificial
Intelligence, 69(1–2), 165–204.
Cadoli, M., & Donini, F. M. (1997). A survey on knowledge compilation. AI Communications,
10(3,4), 137–150.
Cadoli, M., Donini, F. M., Liberatore, P., & Schaerf, M. (1996). Comparing space efficiency of
propositional knowledge representation formalism. In Aiello, L. C., Doyle, J., & Shapiro,
S. (Eds.), Principles of Knowledge Representation and Reasoning: Proceedings of the 5th
International Conference (KR-96), pp. 364–373 Cambridge, MA. Morgan Kaufmann.
Erol, K., Hendler, J. A., & Nau, D. S. (1994). HTN planning: Complexity and expressivity. In
Proceedings of the 12th National Conference of the American Association for Artificial Intelligence (AAAI-94), pp. 1123–1129 Seattle, WA. MIT Press.
313

N EBEL

Erol, K., Hendler, J. A., & Nau, D. S. (1996). Complexity results for hierarchical task-network
planning. Annals of Mathematics and Artificial Intelligence, 18, 69–93.
Fikes, R. E., & Nilsson, N. (1971). STRIPS: A new approach to the application of theorem proving
to problem solving. Artificial Intelligence, 2, 189–208.
Furst, M., Saxe, J. B., & Sipser, M. (1984). Parity, circuits, and the polynomial-time hierarchy.
Mathematical Systems Theory, 17(1), 13–27.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability—A Guide to the Theory of
NP-Completeness. Freeman, San Francisco, CA.
Gazen, B. C., & Knoblock, C. (1997). Combining the expressiveness of UCPOP with the efficiency
of Graphplan. In Steel, S., & Alami, R. (Eds.), Recent Advances in AI Planning. 4th European
Conference on Planning (ECP'97), Vol. 1348 of Lecture Notes in Artificial Intelligence, pp.
221–233 Toulouse, France. Springer-Verlag.
Gogic, G., Kautz, H. A., Papadimitriou, C. H., & Selman, B. (1995). The comparative linguistics
of knowledge representation. In Proceedings of the 14th International Joint Conference on
Artificial Intelligence (IJCAI-95), pp. 862–869 Montreal, Canada. Morgan Kaufmann.
Kambhampati, S., Parker, E., & Lambrecht, E. (1997). Understanding and extending Graphplan.
In Steel, S., & Alami, R. (Eds.), Recent Advances in AI Planning. 4th European Conference
on Planning (ECP'97), Vol. 1348 of Lecture Notes in Artificial Intelligence, pp. 260–272
Toulouse, France. Springer-Verlag.
Karp, R. M., & Lipton, R. J. (1982).
Mathématique, 28, 191–210.

Turing machines that take advice.

L' Ensignement

Kautz, H. A., McAllester, D. A., & Selman, B. (1996). Encoding plans in propositional logic. In
Aiello, L. C., Doyle, J., & Shapiro, S. (Eds.), Principles of Knowledge Representation and
Reasoning: Proceedings of the 5th International Conference (KR-96), pp. 374–385 Cambridge, MA. Morgan Kaufmann.
Kautz, H. A., & Selman, B. (1992). Forming concepts for fast inference.. In Proceedings of the
10th National Conference of the American Association for Artificial Intelligence (AAAI-92),
pp. 786–793 San Jose, CA. MIT Press.
Kautz, H. A., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic, and
stochastic search. In Proceedings of the 13th National Conference of the American Association for Artificial Intelligence (AAAI-96), pp. 1194–1201. MIT Press.
Kautz, H. A., & Selman, B. (1998). BLACKBOX: A new approach to the application of theorem
proving to problem solving. In Working notes of the AIPS'98 Workshop on Planning as
Combinatorial Search Pittsburgh, PA.
Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphs to an
ADL subset. In Steel, S., & Alami, R. (Eds.), Recent Advances in AI Planning. 4th European
Conference on Planning (ECP'97), Vol. 1348 of Lecture Notes in Artificial Intelligence, pp.
273–285 Toulouse, France. Springer-Verlag.
314

C OMPILABILITY

AND

E XPRESSIVE P OWER OF P LANNING F ORMALISMS

Lifschitz, V. (1986). On the semantics of STRIPS. In Georgeff, M. P., & Lansky, A. (Eds.), Reasoning about Actions and Plans: Proceedings of the 1986 Workshop, pp. 1–9 Timberline, OR.
Morgan Kaufmann.
Nebel, B. (1999). What is the expressive power of disjunctive preconditions?. In Biundo, S., & Fox,
M. (Eds.), Recent Advances in AI Planning. 5th European Conference on Planning (ECP'99)
Durham, UK. Springer-Verlag. To appear.
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley, Reading, MA.
Pednault, E. P. (1989). ADL: Exploring the middle ground between STRIPS and the situation
calculus. In Brachman, R., Levesque, H. J., & Reiter, R. (Eds.), Principles of Knowledge
Representation and Reasoning: Proceedings of the 1st International Conference (KR-89),
pp. 324–331 Toronto, ON. Morgan Kaufmann.
Yap, C. K. (1983). Some consequences of non-uniform conditions on uniform classes. Theoretical
Computer Science, 26, 287–300.

315

Journal of Articial Intelligence Research 12 (2000) 199-217

Submitted 12/1999; published 5/2000

The Complexity of Reasoning with Cardinality Restrictions
and Nominals in Expressive Description Logics
Stephan Tobies

tobies@informatik.rwth-aachen.de

LuFG Theoretical Computer Science, RWTH Aachen
Ahornstr. 55, 52074 Aachen, Germany

Abstract

We study the complexity of the combination of the Description Logics ALCQ and ALCQI
with a terminological formalism based on cardinality restrictions on concepts. These combinations can naturally be embedded into C 2 , the two variable fragment of predicate logic
with counting quantiers, which yields decidability in NExpTime. We show that this approach leads to an optimal solution for ALCQI , as ALCQI with cardinality restrictions has
the same complexity as C 2 (NExpTime-complete). In contrast, we show that for ALCQ,
the problem can be solved in ExpTime. This result is obtained by a reduction of reasoning with cardinality restrictions to reasoning with the (in general weaker) terminological
formalism of general axioms for ALCQ extended with nominals . Using the same reduction,
we show that, for the extension of ALCQI with nominals, reasoning with general axioms
is a NExpTime-complete problem. Finally, we sharpen this result and show that pure
concept satisability for ALCQI with nominals is NExpTime-complete. Without nominals,
this problem is known to be PSpace-complete.
1. Introduction

Description Logics (DLs) can be used in knowledge based systems to represent and reason about taxonomical knowledge of the application domain in a semantically well-dened
manner (Woods & Schmolze, 1992). They allow the denition of complex concepts (i.e.,
classes, unary predicates) and roles (binary predicates) to be built from atomic ones by
the application of a given set of constructors. For example, the following concept describes
those parents having at least two daughters:
Human u (Male t Female) u (> 2 hasChild Female) u 8hasChild:Human
This concept is an example for the DL ALCQ. ALCQ extends the \standard" DL ALC
(Schmidt-Schau & Smolka, 1991) by qualifying number restrictions, i.e., concepts restricting the number of individuals that are related via a given role (here hasChild), instead of
allowing only for existential or universal restrictions like ALC. ALCQ is a syntactic variant of
the (multi-)modal logic K with graded modalities (Fine, 1972). In this paper we will study
problems for the DLs ALCQ and ALCQI . The latter extends ALCQ with the possibility to
refer to the inverse of role relations. Additionally, in this paper we will encounter nominals,
i.e., concepts referring to single elements of the domain. The extensions of ALCQ and ALCQI
with nominals are denoted by ALCQO and ALCQIO. An example concept of ALCQIO that
describes the common children of the individuals ALICE and BOB living with ALICE or BOB
is
9hasChild 1 :ALICE u 9hasChild 1:BOB u 9livesWith:(ALICE t BOB):
c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Tobies

Concept Satisability
GCIs
Cardinality Restr.

ALCQ

-c.
-c.
ExpTime-c.
PSpace

ExpTime

ALCQO

open

-c.
ExpTime-c.
ExpTime

ALCQI

-c.
-c.
NExpTime-c.
PSpace

ExpTime

ALCQIO

NExpTime-c.
NExpTime-c.
NExpTime-c.

Figure 1: Complexity results established in this paper (shown in bold face)
Here, the parent relationship is expressed as the inverse of the hasChild relationship.
A terminological component (TBox) allows for the organisation of dened concepts and
roles and forms the knowledge base of a DL system. TBoxes studied in DLs range from weak
ones allowing only for the acyclic introduction of abbreviations for complex concepts, over
TBoxes capable of expressing various forms of general axioms, to cardinality restrictions
that can express restrictions on the number of elements the extension of a concept may
have. In the following, we give examples of these three types of assertions.
The following TBox introduces parent as an abbreviation for a human having at least
one child and whose children are all human, toddler for very young human, and busy parent
for a parent having at least two children that are toddlers.
Parent = Human u (> 1 hasChild) u 8hasChild:Human
Toddler = Human u VeryYoung
BusyParent = Parent u (> 2 hasChild Toddler)
The next expressions are general axioms stating that males and females are disjoint (?
denotes the empty concept) and that males or females coincide with those humans having
exactly two human parents.
Female u Male = ?
Female t Male = Human u (= 2 hasChild 1 Human)
Finally, the following expression is a cardinality restriction expressing that there are at most
two earliest ancestors:
( 2 (Human u (6 0 hasChild 1 Human)))
Cardinality restriction were rst introduced by Baader et al. (1996) as a terminological
formalism for the DL ALCQ; as we will see, they can express general axioms and hence are
the most expressive of the terminological formalisms considered in this paper.
A key component of a DL system is a reasoning component that provides services like
subsumption or consistency tests for the knowledge stored in the TBox. A subsumption
test, for example, could infer from the previous denitions that both Male and Female are
subsumed by Human and that BusyParent is subsumed by Parent as each busy parent must
have at least one child. There exist sound and complete algorithms for reasoning in a large
number of DLs and dierent TBox formalisms that are optimal with respect to the known
worst-case complexity of these problems (see Donini et al., 1996, for an overview).
200

The Complexity of Cardinality Restrictions and Nominals

In this paper we establish a number of new complexity results for DLs with cardinality
restrictions or nominals. Figure 1 summarises the new complexity bounds established in
this paper. All problems are complete for their respective complexity class. This paper is
organised as follows.
After giving some basic denitions in Section 2, we show that consistency of TBoxes
with cardinality restrictions for ALCQI is a NExpTime-complete problem (Section 3). Membership in NExpTime is shown by a translation to the satisability problem of C 2 (Pacholski et al., 1997)1 , the two variable fragment of rst order predicate logic augmented
with counting quantiers. The matching lower bound is established by a reduction from a
NExpTime-complete bounded domino problem.
In Section 4, we show that reasoning with cardinality restrictions can be reduced to
reasoning with the (weaker) formalism of general axioms in the presence of nominals. This
yields interesting complexity results both for reasoning with cardinality restrictions and
with nominals. Using a result from (De Giacomo, 1995), the reduction shows that consistency of TBoxes with cardinality restrictions for ALCQ is in ExpTime. This improves the
result from (Baader et al., 1996), where it was shown that the problem can be solved in
NExpTime. Moreover, we show that for a DL with number restrictions, inverse roles, and
nominals reasoning problems become NExpTime-hard, which solves an open problem from
(De Giacomo, 1995). This combination is of particular interest for the application of DLs
in the area of reasoning with database schemata (Calvanese et al., 1998a, 1998b).
2. The Logic

ALCQI

Denition 2.1 Let NC be a set of atomic concept names and NR be a set of atomic role
names. Concepts in ALCQI are built inductively from these using the following rules: all
A 2 NC are concepts, and, if C , C1 , and C2 are concepts, then also

:C; C1 u C2;

and

(> n S C );

are concepts, where n 2 N and S = R or S = R 1 for some R 2 NR .
A cardinality restriction of ALCQI is an expression of the form (> n C ) or (6 n C )
where C is a concept and n 2 N ; an ALCQI -TCBox 2 is a nite set of cardinality restrictions.
The semantics of concepts is dened relative to an interpretation I = (I ; I ), which
consists of a domain I and a valuation (I ) that maps each concept name A to a subset
AI of I and each role name R to a subset RI of I  I . This valuation is inductively
extended to arbitrary concepts using the following rules, where ]M denotes the cardinality
of a set M :
(:C )I := I n C I ;
(C1 u C2 )I := C1I \ C2I ;
(> n R C )I := fa 2 I j ]fb 2 I j (a; b) 2 RI ^ b 2 C I g  ng;
(> n R 1 C )I := fa 2 I j ]fb 2 I j (b; a) 2 RI ^ b 2 C I g  ng:
1. The NExpTime-result is valid only if we assume unary coding of numbers in the counting quantiers.
This is the standard assumption made by most results concerning the complexity of DLs.
2. The subscripted \C" indicates that the TBox consists of cardinality restrictions

201

Tobies

	x(A)
:= Ax
for A 2 NC
	x(:C )
:= :	x(C )
	x(C1 u C2) := 	x(C1 ) ^ 	x(C2 )
	x(> n R C ) := 9ny:(Rxy ^ 	y (C ))
	x(> n R 1 C ) := 9ny:(Ryx ^ 	y (C ))
	y (C )
:= 	x(C )[xny; ynx]
	(./ n C )
:= 9./nx:	x(C ) for ./ 2 f>; 6g
	(T )
:= Vf	(./ n C ) j (./ n C ) 2 T g
Figure 2: The translation from ALCQI into C 2
An interpretation I satises a cardinality restriction (> n C ) i ](C I )  n, and it satises (6 n C ) i ](C I )  n. It satises a TCBox T i it satises all cardinality restrictions
in T ; in this case, I is called a model of T and we will denote this fact by I j= T . A TCBox
that has a model is called consistent.
With ALCQ we denote the fragment of ALCQI that does not contain any inverse roles
R 1.

Using the constructors from Denition 2.1, we use (8 C ) as an abbreviation for the
cardinality restriction (6 0 :C ) and introduce the following abbreviations for concepts:
C1 t C2 = :(:C1 u :C2 )
(6 n S C ) = :(> (n + 1) S C )
C1 ! C2 = :C1 t C2
(= n S C ) = (6 n S C ) u (> n S C )
9S:C = (> 1 S C )
> = A t :A for some A 2 NC
8S:C = (6 0 S :C )
TBoxes consisting of cardinality restrictions have rst been studied in (Baader et al.,
1996) for the DL ALCQ. Obviously, two concepts C; D have the same extension in an interpretation I i I satises the cardinality restriction (6 0 (C u :D) t (:C u D)). Hence,
cardinality restrictions can express terminological axioms of the form C = D, which are
satised by an interpretation I i C I = DI . General axioms are the most expressive TBox
formalisms usually studied in the DL context (De Giacomo & Lenzerini, 1996). One standard inference service for DL systems is satisability of a concept C with respect to a TCBox
T , i.e., is there an interpretation I such that I j= T and C I 6= ;. For a TBox formalism
based on cardinality restrictions this is easily reduced to TBox consistency, because obviously C is satisable with respect to T i T [ f(> 1 C )g is a consistent TCBox. For this
the reason, we will restrict our attention to TCBox consistency; other standard inferences
such as concept subsumption can be reduced to consistency as well.
Until now there does not exist a direct decision procedure for ALCQI TCBox consistency.
Nevertheless this problem can be decided with the help of a well-known translation of
ALCQI -TCBoxes to C 2 (Borgida, 1996), given in Figure 2. The logic C 2 is the fragment of
predicate logic in which formulae may contain at most two variables, but which is enriched
with counting quantiers of the form 9`. The translation 	 yields a satisable sentence of
C 2 if and only if the translated TCBox is consistent. Since the translation from ALCQI to C 2
202

The Complexity of Cardinality Restrictions and Nominals

can be performed in linear time, the NExpTime upper bound (Gradel et al., 1997; Pacholski
et al., 1997) for satisability of C 2 directly carries over to ALCQI -TCBox consistency:
Lemma 2.2 Consistency of an ALCQI -TCBox T can be decided in NExpTime.
Please note that the NExpTime-completeness result from (Pacholski et al., 1997) is only
valid if we assume unary coding of numbers in the input; this implies that a number n may
not be stored in logarithmic space in some k-ary representation but consumes n units of
storage. This is the standard assumption made by most results concerning the complexity
of DLs. We will come back to this issue in Section 3.3.
3. Consistency of

ALCQI -TCBoxes is NExpTime-complete

To show that NExpTime is also the lower bound for the complexity of TCBox consistency,
we use a bounded version of the domino problem. Domino problems (Wang, 1963; Berger,
1966) have successfully been employed to establish undecidability and complexity results
for various description and modal logics (Spaan, 1993; Baader & Sattler, 1999).
3.1 Domino Systems
Denition 3.1 For n 2 N , let Zn denote the set f0; : : : ; n 1g and n denote the addition
modulo n. A domino system is a triple D = (D; H; V ), where D is a nite set (of tiles)
and H; V  D  D are relations expressing horizontal and vertical compatibility constraints
between the tiles. For s; t 2 N , let U (s; t) be the torus Zs  Zt, and let w = w0 : : : wn 1 be
a word over D of length n (with n  s). We say that D tiles U (s; t) with initial condition
w i there exists a mapping  : U (s; t) ! D such that, for all (x; y ) 2 U (s; t),

 if  (x; y) = d and  (x s 1; y) = d0, then (d; d0 ) 2 H (horizontal constraint);
 if  (x; y) = d and  (x; y t 1) = d0 , then (d; d0 ) 2 V (vertical constraint);
  (i; 0) = wi for 0  i < n (initial condition).

Bounded domino systems are capable of expressing the computational behaviour of
restricted, so-called simple, Turing Machines (TM). This restriction is non-essential in the
following sense: Every language accepted in time T (n) and space S (n) by some one-tape TM
is accepted within the same time and space bounds by a simple TM, as long as S (n); T (n) 
2n (Borger et al., 1997).
Theorem 3.2 ((Borger et al., 1997), Theorem 6.1.2)

Let M be a simple TM with input alphabet . Then there exists a domino system D =
(D; H; V ) and a linear time reduction which takes any input x 2  to a word w 2 D with
jxj = jwj such that
 If M accepts x in time t0 with space s0, then D tiles U (s; t) with initial condition w
for all s  s0 + 2; t  t0 + 2;
 if M does not accept x, then D does not tile U (s; t) with initial condition w for any
s; t  2.
203

Tobies

Corollary 3.3

There is a domino system D such that the following is a NExpTime-hard problem:
Given an initial condition w = w0 : : : wn 1 of length n. Does D tile the torus
U (2n+1 ; 2n+1 ) with initial condition w?
Proof. Let M be a (w.l.o.g. simple) non-deterministic TM with time- (and hence space-)
bound 2n deciding an arbitrary NExpTime-complete language L(M ) over the alphabet .
Let D be the according domino system and trans the reduction from Theorem 3.2.
The function trans is a linear reduction from L(M ) to the problem above: For v 2 
with jvj = n, it holds that v 2 L(M ) i M accepts v in time and space 2jvj i D tiles
U (2n+1 ; 2n+1 ) with initial condition trans(v ).
3.2 Dening a Torus of Exponential Size

Similar to proving undecidability by reduction of unbounded domino problems, where dening innite grids is the key problem, dening a torus of exponential size is the key to
obtaining a NExpTime-completeness proof by reduction of bounded domino problems.
To be able to apply Corollary 3.3 to TCBox consistency for ALCQI , we must characterise
the torus Z2  Z2 with a TCBox of polynomial size. To characterise this torus, we use
2n concepts X0 ; : : : ; Xn 1 and Y0 ; : : : ; Yn 1 , where Xi (resp., Yi) codes the ith bit of the
binary representation of the X-coordinate (resp., Y-coordinate) of an element a.
For an interpretation I and an element a 2 I , we dene pos(a) by
n

n

pos(a) := (xpos(a); ypos(a)) :=
(

nX1

xi

1

n
X

2 ;
i

=0 (

i

=0

yi

i

2

i



;

where

a 62 XiI
0; if a 62 YiI
= 01;; ifotherwise
yi =
1; otherwise :
We use a well-known characterisation of binary addition (e.g. (Borger et al., 1997)) to
relate the positions of the elements in the torus:
Lemma 3.4 Let x; x0 be natural numbers with binary representations
xi

x

=

Then
x0  x + 1

1

n
X

=0

i

1

xi

 2i and
1

n^ k^

(mod 2 ) i
n

k

^

(

=0 j =0
1 1

n^ k_
k

(

=0 j =0

x0

=

1

n
X

=0

i

x0i

 2i :

xj

= 1) ! (xk = 1 $ x0k = 0)

xj

= 0) ! (xk = x0k ) ;

where the empty conjunction and disjunction are interpreted as true and false, respectively.
204

The Complexity of Cardinality Restrictions and Nominals

Deast

=

Xk

u

n

n

(

=0 j =0
1 kG1

k
n

Xj

1

=0
1

k

=0
k=0
1k 1

k
n

(8 9north:>);
(8 (= 1 north 1 >));
(> 1 C(2 1;2 1) );	
(8 Deast u Dnorth )
n

n

:Xk u

G G

=

n

n

=0
1

k
n

G

1;2 1)

1

G

C(2n

=

n

G

C(0;0)

=  (8 9east:>);
(8 (= 1 east 1 >));
(> 1 C(0;0) );
(6 1 C(2 1;2 1) );
G

Tn

n

:Yk

Yk

) ! ((Xk ! 8east::Xk ) u (:Xk ! 8east:Xk )) u

( :Xj ) ! ((Xk ! 8east:Xk ) u (:Xk ! 8east::Xk )) u

=0 j =0
1

G

k
n

=

=0

G

Dnorth

k

((Yk ! 8east:Yk ) u (:Yk ! 8east::Yk ))

:::

Figure 3: A TCBox dening a torus of exponential size
The TCBox Tn is dened in Figure 3. The concept C(0;0) is satised by all elements
a of the domain for which pos(a) = (0; 0) holds. C(2 1;2 1) is a similar concept, whose
instances a satisfy pos(a) = (2n 1; 2n 1).
The concept Dnorth is similar to Deast where the role north has been substituted for east
and variables Xi and Yi have been swapped. The concept Deast (resp. Dnorth ) enforces that,
along the role east (resp. north), the value of xpos (resp. ypos) increases by one while the
value of ypos (resp. xpos) is unchanged. They are analogous to the formula in Lemma 3.4.
The following lemma is a consequence of the denition of pos and Lemma 3.4.
Lemma 3.5 Let I = (I ; I ) be an interpretation, Deast ; Dnorth dened as in Figure 3,
and a; b 2 I .
n

I implies:
(a; b) 2 eastI and a 2 Deast
I implies:
(a; b) 2 northI and a 2 Dnorth

n

xpos(b)  xpos(a) + 1
ypos(b) = ypos(a)
xpos(b) = xpos(a)
ypos(b)  ypos(a) + 1

(mod 2n)

(mod 2n)
The TCBox Tn denes a torus of exponential size in the following sense:
Lemma 3.6 Let Tn be the TCBox as dened in Figure 3. Let I = (I ; I ) be a model of
Tn . Then
(I ; eastI ; northI ) = (U (2n ; 2n ); S1 ; S2 ) ;
205

Tobies

where U (2n; 2n ) is the torus Z2  Z2 and S1; S2 are the horizontal and vertical successor
relations on this torus.
Proof. We show that the function pos is an isomorphism from I to U (2n ; 2n ). Injectivity
of pos is shown by induction on the \Manhattan distance" d(a) of the pos-value of an element
a to the pos-value of the upper right corner.
For an element a 2 I we dene d(a) by
d(a) = (2n 1 xpos(a)) + (2n 1 ypos(a)):
Note that pos(a) = pos(b) implies d(a) = d(b). Since I j= (6 1 C(2 1;2 1) ), there is
at most one element a 2 I such that d(a) = 0. Hence, there is exactly one element a
such that pos(a) = (2n 1; 2n 1). Now assume there are elements a; b 2 I such that
pos(a) = pos(b) and d(a) = d(b) > 0. Then either xpos(a) < 2n 1 or ypos(a) < 2n 1.
W.l.o.g., we assume xpos(a) < 2n 1. From I j= Tn, it follows that a; b 2 (9east:>)I . Let
a1 ; b1 be elements such that (a; a1 ) 2 eastI and (b; b1 ) 2 eastI . Since d(a1 ) = d(b1 ) < d(a)
and pos(a1 ) = pos(b1), the induction hypothesis yields a1 = b1 . From Lemma 3.5 it follows
that
xpos(a1 )  xpos(b1 )  xpos(a) + 1 (mod 2n )
ypos(a1 ) = ypos(b1 ) = ypos(a)
This also implies a = b because a1 2 (= 1 east 1:>)I and f(a; a1 ); (b; a1 )g  eastI . Hence
pos is injective.
To prove that pos is also surjective we use a similar technique. This time, we use an
induction on the distance from the lower left corner. For each element (x; y) 2 U (2n ; 2n),
we dene:
d0 (x; y ) = x + y:
We show by induction that, for each (x; y) 2 U (2n ; 2n ), there is an element a 2 I
such that pos(a) = (x; y). If d0 (x; y) = 0, then x = y = 0. Since I j= (> 1 C(0;0) ), there
is an element a 2 I such that pos(a) = (0; 0). Now consider (x; y) 2 U (2n ; 2n ) with
d0 (x; y ) > 0. Without loss of generality we assume x > 0 (if x = 0 then y > 0 must hold).
Hence (x 1; y) 2 U (2n ; 2n ) and d0 (x 1; y) < d0 (x; y). From the induction hypothesis, it
follows that there is an element a 2 I such that pos(a) = (x 1; y). Then there must be
an element a1 such that (a; a1 ) 2 eastI and Lemma 3.5 implies that pos(a1 ) = (x; y). Hence
pos is also surjective.
Finally, pos is indeed a homomorphism as an immediate consequence of Lemma 3.5.
n

n

n

n

It is interesting to note that we need inverse roles only to guarantee that pos is injective. The same can be achieved by adding the cardinality restriction (6 (2n  2n) >) to
Tn , from which the injectivity of pos follows from its surjectivity and simple cardinality
considerations. Of course the size of this cardinality restriction would only be polynomial
in n if we assume binary coding of numbers. Also note that we have made explicit use of
the special expressive power of cardinality restrictions by stating that, in any model of Tn,
the extension of C(2 1;2 1) must have at most one element. This cannot be expressed
with a ALCQI -TBox consisting of terminological axioms.
n

n

206

The Complexity of Cardinality Restrictions and Nominals

3.3 Reducing Domino Problems to TCBox Consistency

Once Lemma 3.6 has been proved, it is easy to reduce the bounded domino problem to
TCBox consistency. We use the standard reduction that has been applied in the DL context,
e.g., in (Baader & Sattler, 1999).
Lemma 3.7 Let D = (D; V; H ) be a domino system. Let w = w0 : : : wn 1 2 D . There is
a TCBox T (n; D; w) such that:
 T (n; D; w) is consistent i D tiles U (2n; 2n ) with initial condition w.
 T (n; D; w) can be computed in time polynomial in n.
Proof. We dene T (n; D; w) := Tn [ TD [ Tw , where Tn is dened in Figure 3, TD captures
the vertical and horizontal compatibility constraints of the domino system D, and Tw enforces the initial condition. We use an atomic concept Cd for each tile d 2 D. TD consists
of the following cardinality restrictions:
G
:(Cd u Cd0 ));
(8 Cd ); (8
d2D d0 2Dnfdg

Cd0

))); (8

G

(d;d0 )2H

G

G

d2D

(Cd ! (8east:

G

G

(8

d2D

d2D

G

(Cd ! (8north:

(d;d0 )2V

Cd0

))):

Tw consists of the cardinality restrictions
(8 (C(0;0) ! Cw0 )); : : : ; (8 (C(n 1;0) ! Cw 1 );
n

where, for each x; y, C(x;y) is a concept that is satised by an element a i pos(a) = (x; y),
dened similarly to C(0;0) and C(2 1;2 1) .
From the denition of T (n; D; w) and Lemma 3.6, it follows that each model of T (n; D; w)
immediately induces a tiling of U (2n ; 2n ) and vice versa. Also, for a xed domino system
D, T (n; D; w) is obviously polynomially computable.
The main result of this section is now an immediate consequence of Lemma 2.2, Lemma 3.7, and Corollary 3.3:
n

n

Theorem 3.8

Consistency of ALCQI -TCBoxes is NExpTime-complete, even if unary coding of numbers is
used in the input.
Recalling the note below the proof of Lemma 3.6, we see that the same argument also
applies to ALCQ if we allow binary coding of numbers.
Corollary 3.9

Consistency of ALCQ-TCBoxes is NExpTime-hard, if binary coding is used to represent
numbers in cardinality restrictions.
It should be noted that it is open if the problem can be decided in NExpTime, if binary
coding of numbers is used, since the reduction of C 2 only yields decidability in 2-NExpTime.
207

Tobies

In the following section, we will see that, for unary coding of numbers, deciding consistency of ALCQ-TCBoxes can be done in ExpTime (Corollary 4.8). This shows that the
coding of numbers indeed has an inuence on the complexity of the reasoning problem. It
is worth noting that the complexity of pure concept satisability for ALCQ does not depend on the coding; the problem is PSpace-complete both for binary and unary coding of
numbers (Tobies, 2000).
For unary coding, we needed both inverse roles and cardinality restrictions for the
reduction. This is consistent with the fact that satisability for ALCQI concepts with respect
to TBoxes consisting of terminological axioms is still in ExpTime, which can be shown by
a reduction to the ExpTime-complete logics CIN (De Giacomo, 1995) or CPDL (Pratt,
1979). This shows that cardinality restrictions on concepts are an additional source of
complexity. One reason for this might be that ALCQI with cardinality restrictions no longer
has the tree-model property.
4. Reasoning with Nominals

Nominals, i.e., atomic concepts referring to single individualsof the domain, are studied both
in the area of DLs (Borgida & Patel-Schneider, 1994; Donini et al., 1996) and modal logics
(Gargov & Goranko, 1993; Blackburn & Seligman, 1996; Areces et al., 1999). In this section
we show how, in the presence of nominals, consistency for TCBoxes can be polynomially
reduced to consistency of TBoxes consisting of general inclusion axioms, which, in general,
is an easier problem. This correspondence is used to obtain two novel results: (i) we show
that, for unary coding, consistency of ALCQ-TBoxes consisting of cardinality restrictions
can be decided in ExpTime; (ii) we show that, in the presence of both inverse roles and
number restrictions, reasoning with nominals is strictly harder than without nominals: the
complexity of determining consistency of TBoxes with general axioms rises from ExpTime
to NExpTime, and the complexity of concept satisability rises from PSpace to NExpTime.
Denition 4.1 Let NI be a set of individual names (also called nominals) disjoint from
NC and NR . Concepts in ALCQIO are dened as ALCQI -concepts with the additional rule
that, for every o 2 NI , o is an ALCQIO-concept.
A general concept inclusion axiom for ALCQIO is an expression of the from C v D,
where C and D are ALCQIO-concepts. A TIBox for ALCQIO is a nite set of general
inclusion axioms for ALCQIO, where the subscript \I" stands for \Inclusion".
The semantics of ALCQIO concepts is dened similar as for ALCQI , with the additional
requirement that every interpretation maps a nominal o 2 NI to a singleton set oI  I ;
o can be seen as a name for the element in oI . Please note that we do not have a unique
name assumption, i.e., we do not assume that o1 6= o2 implies oI1 6= oI2 .
An interpretation I satises an axiom C v D i C I  DI . It satises a TIBox Tgci i
it satises all axioms in Tgci ; in this case I is called a model of Tgci, and we will denote
this fact by I j= Tgci . A TIBox that has a model is called consistent.
Cardinality restrictions, TCBoxes, and their interpretation for ALCQIO are dened analogously to ALCQI .
208

The Complexity of Cardinality Restrictions and Nominals

R

With ALCQO we denote the fragment of ALCQIO that does not contain any inverse roles

1.

Consistency of TCBoxes or TIBoxes both for ALCQO and ALCQIO is Exp-hard and can be decided in NExpTime, if unary coding of numbers is used.
Proof. Consistency of TIBoxes (and hence of TCBoxes) is ExpTime-hard already for (a
syntactical variant of) ALC (Halpern & Moses, 1992). Assuming unary coding of numbers,
we can reduce the problems to satisability of C 2, which yields the NExpTime upper
bound.
Lemma 4.2
Time

4.1 Expressing Cardinality Restrictions Using Nominals

In the following we show how, under the assumption of unary coding of numbers, consistency
of ALCQI -TCBoxes can be polynomially reduced to consistency of ALCQIO-TIBoxes. It
should be noted that, conversely, it is also possible to polynomially reduce consistency
of ALCQIO-TIBoxes to consistency of ALCQI -TCBoxes: for an arbitrary concept C , the
cardinality restrictions f(6 1 C ); (> 1 C )g force the interpretation of C to be a singleton.
Since we do not gain any further insight from this reduction, we do not formally prove this
result.
Denition 4.3 Let T = f(./1 n1 C1 ); : : : (./k nk Ck )g be an ALCQI -TCBox. W.l.o.g., we
assume that T contains no cardinality restriction of the form (> 0 C ) as these are trivially
satised by any interpretation. The translation of T , denoted by (T ), is the ALCQIO-TIBox
dened as follows:

[

(T ) = f(./i ni Ci) j 1  i  kg;
where (./i ni Ci ) is dened depending on whether ./i =6 or ./i =>.
(

n
1
(./i ni Ci ) = ffoCji vv Coi tj 1  tj oi ng g [ foj v :o` j 1  j < `  n g
i
i
i
i
i
i
i

if
if

=6 ;
./i =>
./i

where o1i ; : : : ; oni are fresh and distinct nominals and we use the convention that the empty
disjunction is interpreted as :> to deal with the case ni = 0.
i

Assuming unary coding of numbers, the translation of a TCBox T is obviously computable in polynomial time.
Lemma 4.4 Let T be an ALCQI -TCBox. T is consistent i (T ) is consistent.
Proof. Let T = f(./1 n1 C1 ); : : : (./k nk Ck )g be a consistent TCBox. Hence, there is a
model I of T , and I j= (./i ni Ci ) for each 1  i  k. We show how to construct a model
I 0 of (T ) fromj I . I 0 will be identical to I in every respect except for the interpretation of
the nominals oi (which do not appear in T ).
If ./i =6, then I j= T implies ]CiI  ni. If ni = 0, then we have0 not introduced
new nominals, and (T ) contains Ci v :>. Otherwise, we dene (oji )I such that CiI 
209

Tobies

f(oji )I 0 j 1  j  nig. This implies CiI 0  (o1i )I 0 [    [ (oni )I 0 and hence, in either case,
I 0 j= (6 ni Ci).
If ./i =>, then ni > 0 must hold, and I j= T implies ]CiI  0ni. Let x1; : : : xn be ni
distinct elements from I with fx1 ; : : : ; xn g  CiI . We set (oji )I = fxj g. Since we have
chosen distinct individuals to interpret dierent nominals, we have I 0 j= oji v :o`i for every
1  i < `  ni. Moreover, xj 2 CiI implies I 0 j= oji v Ci and hence I 0 j= (> ni Ci).
i

i

i

We have chosen distinct nominals for every cardinality restrictions, hence the previous
construction is well-dened and, since I 0 satises (./i ni Ci ) for every i, I 0 j= (T ).
For the converse direction, let I be a models of (T ). The fact that I j= T (and hence
the consistency of T ) can be shown as follows: let (./i ni Ci) be an arbitrary cardinality
restriction in T . If ./i =6 and ni = 0, then we have (6 0 Ci) = fCi v :>g and,
since I j= (T ),nwe have CiI = ; and hence I j= (6 0 Ci). If ./i =6 and
ni > 0, we have
n I
1
I
1
fCi v oi t  t oi g  (T ). From I j= (T ) follows ]Ci  ](oi t  t oi )  ni. If ./i =>,
then we have foji v Ci j 1  j  nig[foji v :o`i j 1  j < `  nig  (T ). From the rst set
of axioms we get f(oji )I j 1  j  nig  CiI . From the secondSset of axioms we get that, for
every 1  j < `  ni, (oji )I 6= (o`i )I . This implies that ni = ] f(oji )I j 1  j  nig  ]CiI .
i

i

Theorem 4.5

Assuming unary coding of numbers, consistency of ALCQI -TCBoxes can be polynomially
reduced to consistency of ALCQIO-TIBoxes. Similarly, consistency of ALCQ-TCBoxes can be
polynomially reduced to consistency of ALCQO-TIBoxes.
Proof. The rst proposition follows from the fact that (T ) is polynomially computable
from T if we assume unary coding of numbers and from Lemma 4.4. The second proposition
follows from the fact that the translation does not introduce additional inverse roles. If T
does not contain inverse roles, then neither does (T ), and hence the result of translating
an ALCQ-TCBox is an ALCQO-TIBox.
4.2 Complexity Results

We will now use Theorem 4.5 to obtain new complexity results both for DLs with cardinality
restrictions and with nominals.
4.2.1

ALCQ and ALCQO

De Giacomo (1995) obtains complexity results for various DLs by sophisticated polynomial
reduction to a propositional dynamic logic. The author establishes many complexity results,
one of which is of special interest for our purposes.
Theorem 4.6 ((De Giacomo, 1995), Section 7.3)

Satisability and logical implication for CNO knowledge bases (TBox and ABox) are Exp-complete.
The DL CNO studied by the author is a strict extension of ALCQO; TBoxes in his thesis
correspond to what we call TIBoxes in this paper. Unary coding of numbers is assumed
Time

210

The Complexity of Cardinality Restrictions and Nominals

throughout his thesis. Although a unique name assumption is made, it is not inherent
to the utilised reduction since is explicitly enforced. It is thus possible to eliminate the
propositions that require a unique interpretation of individuals from the reduction. Hence,
together with Lemma 4.2, we get the following corollary.
Corollary 4.7

Consistency of ALCQO-TIBoxes is ExpTime-complete if unary coding of number is assumed.
Together with Theorem 4.5, this solves the open problem concerning the lower bound
for the complexity of ALCQ with cardinality restrictions; moreover, it shows that the NExpTime-algorithm presented in (Baader et al., 1996) is not optimal with respect to worst case
complexity.
Corollary 4.8

Consistency of ALCQ-TCBoxes is ExpTime-complete, if unary coding of numbers in cardinality and number restrictions is used.
4.2.2

ALCQIO

Conversely, using Theorem 4.5 enables us to transfer the NExpTime-completeness result
from Theorem 3.8 to ALCQIO.
Corollary 4.9

Consistency of ALCQIO-TIBoxes or TCBoxes is NExpTime-complete.
Proof. For TIBoxes, this is an immediate corollary of Theorem 4.5 and Theorem 3.8.
Reasoning with TCBoxes is as hard as for TIBoxes in the presences of nominals.
This results explains a gap in (De Giacomo, 1995). There the author establishes the
complexity of satisability of knowledge bases consisting of TIBoxes and ABoxes both for
CNO, which allows for qualifying number restrictions, and for CIO, which allows for inverse
roles, by reduction to an ExpTime-complete PDL. No results are given for the combination CINO, which is a strict extension of ALCQIO. Corollary 4.8 shows that, assuming
ExpTime 6= NExpTime, there cannot be a polynomial reduction from the satisability
problem of CINO knowledge bases to an ExpTime-complete PDL. Again, a possible explanation for this leap in complexity is the loss of the tree model property. While for CIO and
CNO, consistency is decided by searching for a tree-like pseudo-models even in the presence
of nominals, this seems no longer to be possible in the case of knowledge bases for CINO.
Unique Name Assumption It should be noted that our denition of nominals is nonstandard for Description Logics in the sense that we do not impose the unique name assumption that is widely made, i.e., for any two individual names o1 ; o2 2 NI , oI1 6= oI2 is
required. Even without a unique name assumption, it is possible to enforce distinct interpretation of nominals by adding axioms of the form o1 v :o2. Moreover, imposing a unique
name assumption in the presence of inverse roles and number restriction leads to peculiar
eects. Consider the following TIBox:
T = fo v (6 k R >); > v 9R :og
211

Tobies

Under the unique name assumption, T is consistent i NI contains at most k individual
names, because each individual name must be interpreted by a unique element of the domain, every element of the domain must be reachable from oI via the role R, and oI may
have at most k R-successors. We believe that this dependency of the consistency of a TIBox
on constraints that are not explicit in the TIBox is counter-intuitive and hence have not
imposed the unique name assumption.
Nevertheless, it is possible to obtain a tight complexity bound for consistency of ALCQIOTIBoxes with the unique name assumption without using Theorem 4.5, but by an immediate
adaption of the proof of Theorem 3.8.
Corollary 4.10

Consistency of ALCQIO-TIBoxes with the unique name assumption is NExpTime-complete
if unary coding of numbers assumed.
Proof. A simple inspection of the reduction used to prove Theorem 3.8, and especially
of the proof of Lemma 3.6 shows that only a single nominal, which marks the upper right
corner of the torus, is necessary to perform the reduction. If o is an individual name and
create is a role name, then the following TIBox denes a torus of exponential size:

Tn = > v 9east:>;
> v 9north:>;
> v (= 1 east 1 >); > v (= 1 north 1 >);
> v 9create:C(0;0) ; o v C(2 1;2 1) ; 	
C(2 1;2 1) v o;
> v Deast u Dnorth
n

n

n

n

Since this reduction uses only a single individual name, the unique name assumption is
irrelevant.
Internalisation of Axioms In the presence of inverse roles and nominals, it is possible
to internalise general inclusion axioms into concepts using the spy-point technique used,
e.g., in (Blackburn & Seligman, 1996; Areces et al., 1999). The main idea of this technique
is to enforce that all elements in the model of a concept are reachable from a distinct point
(the spy-point) marked by an individual name in a single step.
Denition 4.11 Let T be an ALCQIO-TIBox. W.l.o.g., we assume that T is of the form
f> v C1 ; : : : ; > v Ck g. Let spy denote a fresh role name and i a fresh individual name.
We dene the function spy inductively on the structure of concepts by setting Aspy = A for
all A 2 NC , ospy = o for all o 2 NI , (:C )spy = :C spy, (C1 u C2 )spy = C1spy u C2spy , and
(> n R C )spy = (> n R (9spy :i) u C spy).
The internalisation CT of T is dened as follows:
G

=iu

>vC 2T

C spy

u

G

CT

>vC 2T

8spy:C spy

Lemma 4.12 Let T be an ALCQIO-TIBox. T is consistent i CT is satisable.
Proof. For the if -direction let I be a model of CT with a 2 (CT )I . This implies iI = fag.

Let I 0 be dened by

I 0 = fag [ fx 2 I j (a; x) 2 spyI g
212

The Complexity of Cardinality Restrictions and Nominals

and I 0 = I jI0 .
0
0
Claim 1: For every x 2 I and every ALCQIO-concept C , we have x 2 (C spy )I i x 2 C I .
We proof this claim by induction on the structure of C . The only interesting case is
C = (> n R D ). In this case C spy = (> n R (9spy :i) u D spy ). We have
x 2 (> n R (9spy :i) u D spy )I
i ]fy 2 I j (x; y) 2 RI and y 2 (9spy :i)I \ (Dspy)I g > n
() i ]fy 2 I 0 j (x; y) 2 RI 0 and y 2 DI 0 g > n
i x 2 (> n R D)I 0 ;
where the equivalence () holds because of set equality and 0 the denition of I 0.
By construction,0 for every > v C 2 T and every x 2 I , x 2 (C spy )I . Due to Claim 1,
this implies x 2 C I and hence I 0 j= > v C .
For the only-if -direction, let I be an interpretation with I0 j= T . We pick an
arbitrary
0
I
0
I
I
element a 2  and dene an extension I of I by setting i = fag and spy = f(a; x) j
x 2 I . Since i and spy do not occur in T , we still have that I 0 j= T .
0
Claim 2: For every x 2 I and every ALCQIO-concept C that does not contain i or spy,
0
0
x 2 C I i x 2 (C spy )I .
Again, this claim is proved by induction on the structure of concepts and the only
interesting case is C = (> n R D).
0
x 2 (> n R D )I
i ]fy 2 I 0 j (x; y) 2 RI 0 and y 2 DI 0 g > n
() i ]fy 2 I 0 j (x; y) 2 RI 0 ; (a; y) 2 spyI 0 ; and y 2 (Dspy)I 0 g > n
i x 2 (> n R (9spy :i) u Dspy)I 0 :
Again, the equivalence () holds due to set equality and the denition of I 0.
Since, for every > v C0 2 T , we have I 0 j= > v C , Claim 2 yields that ( >vC 2T C spy)I 0 =
0
I and hence a 2 (CT )I
As a consequence, we obtain the sharper result that already pure concept satisability
for ALCQIO is a NExpTime-complete problem.
F

Corollary 4.13

Concept satisability for ALCQIO is NExpTime-complete.
Proof. From Lemma 4.12, we get that the function mapping a ALCQIO-TIBox T to CT
is a reduction from consistency of ALCQIO-TIBoxes to ALCQIO-concept satisability. From
Corollary 4.9 we know that the former problem is NExpTime-complete. Obviously, CT can
be computed from T in polynomial time. Hence, the lower complexity bound transfers.
213

Tobies

Concept Satisability
GCIs
Cardinality Restr.

ALCQ

-c.
-c.
ExpTime-c.
PSpace

ExpTime

ALCQO

open

-c.
ExpTime-c.
ExpTime

ALCQI

-c.
-c.
NExpTime-c.

ALCQIO

-c.
-c.
NExpTime-c.

PSpace

NExpTime

ExpTime

NExpTime

Figure 4: Complexity of the reasoning problems
5. Conclusion

Combining the results from (De Giacomo, 1995) and (Tobies, 2000) with the results from
this paper, we obtain the classication of the complexity of concept satisability and TBoxconsistency for various DLs and for TBoxes consisting either of cardinality restrictions or
of general concept inclusion axioms shown in Figure 4, where we assume unary coding of
numbers.
The result for ALCQIO shows that the current eorts of extending very expressive DLs
as the logic SHIQ (Horrocks et al., 1999) and DLR(Calvanese et al., 1998c) or propositional
dynamic logics as CPDLg (De Giacomo & Lenzerini, 1996) with nominals are diÆcult tasks,
if one wants to obtain a practical decision procedure, since already concept satisability for
these logics is a NExpTime-hard problem.
We have shown that, while having the same complexity as C 2, ALCQI does not reach its
expressive power (Tobies, 1999). Cardinality restrictions, although interesting for knowledge
representation, are inherently hard to handle algorithmically. The same applies to nominals
in the presence of inverse roles and number restrictions. As an explanation for this we oer
the lack of a tree model property, which was identied by Vardi (1997) as an explanation
for good algorithmic behaviour of many modal logics.
At a rst glance, our results make ALCQI with cardinality restrictions on concepts or
ALCQIO with general axioms obsolete for knowledge representation because C 2 delivers
more expressive power at the same computational price. Yet, is is likely that a dedicated
algorithm for ALCQI may have better average complexity than the C 2 algorithm; such an
algorithm has yet to be developed. This is highly desirable as it would also be applicable to
reasoning problems for expressive DLs with nominals, which have applications in the area
of reasoning with database schemata (Calvanese et al., 1998a, 1998b).
An interesting question lies in the coding of numbers: If we allow binary coding of
numbers, the translation approach together with the result from (Pacholski et al., 1997)
leads to a 2-NExpTime algorithm. As for C 2, it is an open question whether this additional
exponential blow-up is necessary. A positive answer would settle the same question for C 2
while a proof of the negative answer might give hints how the result for C 2 might be
improved. For ALCQ with cardinality restrictions, we have partially solved this problem:
with unary coding, the problem is ExpTime-complete whereas, for binary coding, it is
NExpTime-hard.

214

The Complexity of Cardinality Restrictions and Nominals

Acknowledgments

I would like to thank Franz Baader, Ulrike Sattler, and the anonymous referees for valuable
comments and suggestions. This work was supported by the DFG, Project No. GR 1324/3{
1.
References

Aiello, L. C., Doyle, J., & Shapiro, S. C. (Eds.)., Proceedings of KR'96 (1996). Principles
of Knowledge Representation and Reasoning: Proceedings of the Fifth International
Conference (KR'96). Morgan Kaufmann Publishers, San Francisco, California.
Areces, C., Blackburn, P., & Marx, M. (1999). A road-map on complexity for hybrid logics.
In Proceedings of the Annual Conference of the European Association for Computer
Science Logic (CSL-99), LNCS 1683, pp. 307{321. Springer-Verlag.
Baader, F., Buchheit, M., & Hollunder, B. (1996). Cardinality restrictions on concepts.
Articial Intelligence, 88 (1{2), 195{213.
Baader, F., & Sattler, U. (1999). Expressive number restrictions in description logics.
Journal of Logic and Computation, 9 (3), 319{350.
Berger, R. (1966). The undecidability of the dominoe problem. Memoirs of the American
Mathematical Society, 66.
Blackburn, P., & Seligman, J. (1996). Hybrid languages. Journal of Logic, Language and
Information, 3 (4), 251{272.
Borger, E., Gradel, E., & Gurevich, Y. (1997). The Classical Decision Problem. Perspectives
in Mathematical Logic. Springer-Verlag, Berlin.
Borgida, A. (1996). On the relative expressiveness of description logics and rst order logics.
Articial Intelligence, 82, 353{367.
Borgida, A., & Patel-Schneider, P. F. (1994). A semantic and complete algorithm for
subsumption in the classic description logic. Journal of Articial Intelligence Research,
1, 277{308.
Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998a). Source integration in data warehousing. In Proceedings of the Ninth International Workshop on
Database and Expert Systems Applications (DEXA-98), pp. 192{197. IEEE Computer
Society Press.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998b). On the decidability of query
containment under constraints. In Proceedings of the Seventeenth ACM SIGACT
SIGMOD SIGART Symposium on Principles of Database Systems (PODS-98).
Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998c). Description
logic framework for information integration. In Proc. of the 6th Int. Conf. on the
Principles of Knowledge Representation and Reasoning (KR'98), pp. 2{13.
215

Tobies

De Giacomo, G. (1995). Decidability of Class-Based Knowledge Representation Formalisms. Ph.D. thesis, Dipartimento di Informatica e Sistemistica, Univ. di Roma
\La Sapienza".
De Giacomo, G., & Lenzerini, M. (1996). TBox and ABox reasoning in expressive description
logics.. In Aiello et al. (Aiello et al., 1996), pp. 316{327.
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1996). Reasoning in description
logics. In Brewka, G. (Ed.), Foundation of Knowledge Representation, pp. 191{236.
CSLI-Publications.
Fine, K. (1972). In so many possible worlds. Notre Dame Journal of Formal Logic, 13,
516{520.
Gargov, G., & Goranko, V. (1993). Modal logic with names. J. of Philosophical Logic, 22,
607{636.
Gradel, E., Otto, M., & Rosen, E. (1997). Two-variable logic with counting is decidable.
In Proceedings, Twelth Annual IEEE Symposium on Logic in Computer Science, pp.
306{317 Warsaw, Poland. IEEE Computer Society Press.
Halpern, J. Y., & Moses, Y. (1992). A guide to completeness and complexity for model
logics of knowledge and belief. Articial Intelligence, 54 (3), 319{379.
Horrocks, I., Sattler, U., & Tobies, S. (1999). Practical reasoning for expressive description
logics. In Proceedings of the 6th International Conference on Logic for Programming
and Automated Reasoning (LPAR'99), pp. 161{180.
Pacholski, L., Szwast, W., & Tendera, L. (1997). Complexity of two-variable logic with
counting. In Proceedings, Twelth Annual IEEE Symposium on Logic in Computer
Science, pp. 318{327 Warsaw, Poland. IEEE Computer Society Press.
Pratt, V. R. (1979). Models of program logics. In Proceedings of the Twentieth IEEE
Symposium on Foundations of Computer Science, pp. 115{122. IEEE.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions with complements. Articial Intelligence, 48, 1{26.
Spaan, E. (1993). Complexity of Modal Logics. Ph.D. thesis, University of Amsterdam.
Tobies, S. (1999). A NExpTime-complete description logic strictly contained in C 2. In
Flum, J., & Rodriguez-Artalejo, M. (Eds.), Proceedings of the Annual Conference
of the European Association for Computer Science Logic (CSL-99), LNCS 1683, pp.
292{306. Springer-Verlag.
Tobies, S. (2000). PSPACE reasoning for graded modal logics. Journal of Logic and Computation. To appear.
216

The Complexity of Cardinality Restrictions and Nominals

Vardi, M. Y. (1997). Why is modal logic so robustly decidable?. In Descriptive Complexity
and Finite Models: Proceedings of a DIMACS Workshop, January 14-17, 1996, No. 31
in DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pp.
149{184. American Math. Society.
Wang, H. (1963). Dominoes and the AEA case of the Decision Problem. Bell Syst. Tech.
J., 40, 1{41.
Woods, W. A., & Schmolze, J. G. (1992). The Kl-One family. Computers and Mathematics
with Applications { Special Issue on Articial Intelligence, 23 (2{5), 133{177.

217

Journal of Articial Intelligence Research 12 (2000) 387-416

Submitted 12/99; published 6/00

An Application of Reinforcement Learning to Dialogue
Strategy Selection in a Spoken Dialogue System for Email
Marilyn A. Walker

walker@research.att.com

AT&T Shannon Laboratory
180 Park Ave., Bldg 103, Room E103
Florham Park, NJ 07932

Abstract

This paper describes a novel method by which a spoken dialogue system can learn
to choose an optimal dialogue strategy from its experience interacting with human users.
The method is based on a combination of reinforcement learning and performance modeling of spoken dialogue systems. The reinforcement learning component applies Q-learning
(Watkins, 1989), while the performance modeling component applies the PARADISE evaluation framework (Walker et al., 1997) to learn the performance function (reward) used
in reinforcement learning. We illustrate the method with a spoken dialogue system named
elvis (EmaiL Voice Interactive System), that supports access to email over the phone. We
conduct a set of experiments for training an optimal dialogue strategy on a corpus of 219
dialogues in which human users interact with elvis over the phone. We then test that
strategy on a corpus of 18 dialogues. We show that elvis can learn to optimize its strategy
selection for agent initiative, for reading messages, and for summarizing email folders.

1. Introduction
In the past several years, it has become possible to build spoken dialogue systems that can
communicate with humans over the telephone in real time. Systems exist for tasks such as
nding a good restaurant nearby, reading your email, perusing the classied advertisements
about cars for sale, or making travel arrangements (Sene, Zue, Polifroni, Pao, Hetherington, Goddeau, & Glass, 1995; Baggia, Castagneri, & Danieli, 1998; Sanderman, Sturm,
den Os, Boves, & Cremers, 1998; Walker, Fromer, & Narayanan, 1998). These systems are
some of the few realized examples of real time, goal-oriented interactions between humans
and computers. Yet in spite of 30 years of research on algorithms for dialogue management in task-oriented dialogue systems, (Carbonell, 1971; Winograd, 1972; Simmons &
Slocum, 1975; Bruce, 1975; Power, 1974; Walker, 1978; Allen, 1979; Cohen, 1978; Pollack,
Hirschberg, & Webber, 1982; Grosz, 1983; Woods, 1984; Finin, Joshi, & Webber, 1986;
Carberry, 1989; Moore & Paris, 1989; Smith & Hipp, 1994; Kamm, 1995) inter alia, the
design of the dialogue manager in real-time, implemented systems is still more of an art
than a science (Sparck-Jones & Galliers, 1996). This paper describes a novel method, and
experiments that validate the method, by which a spoken dialogue system can learn from
its experience with human users to optimize its choice of dialogue strategy.
The dialogue manager of a spoken dialogue system processes the user's utterance and
then chooses in real time what information to communicate to the human user and how to
communicate it. The choice it makes is called its strategy. The dialogue manager can be
naturally formulated as a state machine, where the state of the dialogue is dened by a set
c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Walker

of state variables representing observations of the user's conversational behavior, the results
of accessing various information databases, and aspects of the dialogue history. Transitions
between states are driven by the system's dialogue strategy. In a typical system, there are
a large number of potential strategy choices at each state of a dialogue.
For example, consider one choice faced by elvis (EmaiL Voice Interactive System) a
spoken dialogue system that supports access to a user's email by phone. elvis provides
verbal summaries of a user's email folders, but there are many ways to summarize a folder
(Sparck-Jones, 1993, 1999). A summary could consist of a simple statement about the
number of messages in dierent folders, e.g., You have 5 new messages, or it could provide
much more detail about the messages in a particular folder, e.g., In your messages from
Kim, you have one message about a meeting, and a second about interviewing Antonio.
elvis must decide which of many properties of a message to mention, such as the message's
status, its sender, or the subject of the message.1
Decision theoretic planning can be applied to the problem of choosing among dialogue
strategies, by associating a utility U with each strategy (action) choice and by positing
that spoken dialogue systems should adhere to the Maximum Expected Utility Principle
(Keeney & Raia, 1976; Russell & Norvig, 1995),

Maximum Expected Utility Principle: An optimal action is one that maximizes the expected utility of outcome states.

Thus, elvis can act optimally by choosing a strategy a in state Si that maximizes U (Si ).
This formulation however simply leaves us with the problem of how to derive the utility
values U (Si ) for each dialogue state Si . Several reinforcement learning algorithms based on
dynamic programming specify a way to calculate U (Si ) in terms of the utility of a successor
state Sj (Bellman, 1957; Watkins, 1989; Sutton, 1991; Barto, Bradtke, & Singh, 1995), so
if the utility for the nal state of the dialogue were known, it would be possible to calculate
the utilities for all the earlier states, and thus determine a policy which selects only optimal
dialogue strategies.
Previous work suggested that it should be possible to treat dialogue strategy selection as
a stochastic optimization problem in this way (Walker, 1993; Biermann & Long, 1996; Levin,
Pieraccini, & Eckert, 1997; Mellish, Knott, Oberlander, & O'Donnell, 1998). However in
(Walker, 1993), we argued that the lack of a performance function for assigning a utility
to the nal state of a dialogue was a critical methodological limitation. There seemed to
be three main possibilities for a simple reward function: user satisfaction, task completion,
or some measure of user eort such as elapsed time for the dialogue or the number of
user turns. But it appeared that any of these simple reward functions on their own fail
to capture essential aspects of the system's performance. For example, the level of user
eort to complete a dialogue task is system, domain and task dependent. Moreover, high
levels of eort, e.g., the requirement that users conrm the system's understanding of each
utterance, do not necessarily lead to concomitant increases in task completion, but do
1. All of the strategies implemented in elvis are summarized in Figure 1. Note that due to practical
constraints, we have only implemented strategy choices in a subset of states, and that elvis uses a xed
strategy in other states. In Section 2, we describe in detail the strategy choices that elvis explores in
addition to choices about summarization, namely choices among strategies for controlling the dialogue
initiative and for reading multiple messages.

388

Reinforcement Learning in the ELVIS System

lead to signicant decreases in user satisfaction (Shriberg, Wade, & Price, 1992; Danieli &
Gerbino, 1995; Kamm, 1995; Baggia et al., 1998). Furthermore, user satisfaction alone fails
to reect the fact that the system will not be successful unless it helps the user complete
a task. We concluded that the relationship between these measures is both interesting
and complex and that a method for deriving an appropriate performance function was
a necessary precursor to applying stochastic optimization algorithms to spoken dialogue
systems. In (Walker, Litman, Kamm, & Abella, 1997a), we proposed the paradise method
for learning a performance function from a corpus of human-computer dialogues.
In this work, we apply the paradise model to learn a performance function for elvis,
which we then use for calculating the utility of the nal state of a dialogue in experiments
applying reinforcement learning to elvis's selection of dialogue strategies. Section 2 describes the implementation of a version of elvis that randomly explores alternate strategies
for initiative, for reading messages, and for summarizing email folders. Section 3 describes
the experimental design in which we rst use this exploratory version of elvis to collect
a training corpus of conversations with 73 human users carrying out a set of three email
tasks. Section 4 describes how we apply reinforcement learning to the corpus of 219 dialogues to optimize elvis's dialogue strategy decisions. We then test the optimized policy
in an experiment in which six new users interact with elvis to complete the same set of
tasks, and show that the learned policy performs signicantly better than the exploratory
policy used during the training phase.

2. ELVIS Spoken Dialogue System
We started the process of designing elvis by conducting a Wizard-of-Oz experiment in
which we recorded dialogues with six people accessing their email remotely by talking to
a human who was playing the part of the spoken dialogue system. The purpose of this
experiment was to identify the basic functionality that should be implemented in elvis.
The analysis of the resulting dialogues suggested that elvis needed to support contentbased access to email messages by specication of the subject or the sender eld, verbal
summaries of email folders, reading the body of an email message, and requests for help
and repetition of messages (Walker et al., 1997b, 1998).
Given these requirements, we then implemented elvis using a general-purpose platform
for spoken dialogue systems (Kamm et al., 1997). The platform consists of a dialogue
manager (described in detail in Section 2.2), a speech recognizer, an audio server for both
voice recordings and text-to-speech (TTS), an interface between the computer running
Elvis and the telephone network, and modules for specifying the rules for spoken language
understanding and application specic functions.
The speech recognizer is a speaker-independent Hidden Markov Model (HMM) system,
with context-dependent phone models for telephone speech, and constrained grammars
dening the vocabulary that is permitted at any point in a dialogue (Rabiner, Juang, &
Lee, 1996). The platform supports barge-in, so that the user can interrupt the system;
barge-in is very important for this application so that the user can interrupt the system
when it is reading a long email message.
The audio server can switch between voice recordings and text-to-speech (TTS) and
integrate voice recordings with TTS. The TTS technology is concatenative diphone synthe389

Walker

sis (Sproat & Olive, 1995). Elvis uses only TTS since it would not be possible to pre-record,
and then concatenate, all the words necessary for realizing the content of email messages.
The spoken language understanding (SLU) module consists of a set of rules for specifying
the vocabulary and allowable utterances, and an associated set of rules for translating the
user's utterance into a domain-specic semantic representation of its meaning. The syntactic
rules are converted into an FSM network that is used directly by the speech recognizer
(Mohri, Pereira, & Riley, 1998). The semantic rule that is associated with each syntactic
rule maps the user's utterance directly to an application specic template consisting of an
application function name and its arguments. These templates are then converted directly to
application specic function calls specied in the application module. The understanding
module also supports dynamic grammar generation and loading because the recognizer
vocabulary must change during the interaction, e.g., to support selection of email messages
by content elds such as sender and subject.
The application module provides application specic functions, e.g., functions for accessing message attributes such as subject and sender, and functions for making these realizable
as speech so that they can be used to instantiate variables in spoken language generation.

2.1 ELVIS's Dialogue Manager and Strategies

's dialogue manager is based on a state machine where one or more dialogue strategies
can be explored in each state. The state of the dialogue manager is dened by a set of
state variables representing various items of information that the dialogue manager uses
in deciding what to do next. The state variables encode various observations of the user's
conversational behavior, such as the results of processing the user's speech with the spoken
language understanding (SLU) module, and results from accessing information databases
relevant to the application, as well as certain aspects of the dialogue history. A dialogue
strategy is a specication of what the system should say; in Elvis this is represented as a
template with variables that must be instantiated by the current context. In some states the
system always executes the same dialogue strategy and in other states alternate strategies
are explored. All of the strategies implemented in Elvis are summarized in Figure 1. A
complete specication of which dialogue strategy should be executed in each state is called
a policy for a dialogue system.
To develop a version of Elvis that supported exploring a number of possible policies,
we implemented several dierent choices in particular states of the system. Our goal was to
implement strategy choices in states where the optimal strategy was not obvious a priori.
For the purpose of illustrating the dialogue strategies we explored, consider a situation in
which the user is attempting to execute the following task (one of the tasks used in the
experimental data collection described in Section 3):
Elvis

 Task 1.1: You are working at home in the morning and plan to go directly to a meeting

when you go into work. Kim said she would send you a message telling you where
and when the meeting is. Find out the Meeting Time and the Meeting Place.

To complete this task, the user needs to nd a message from Kim about a meeting in
her inbox and listen to it. There are many possible strategies that Elvis could use to help
the user accomplish this task. Below, we rst describe the dialogue strategies from Figure 1
390

Reinforcement Learning in the ELVIS System

Strategy Type
Initiative
Summarization

Choices
Explored?
yes
yes

Reading

yes

Request-Info

no

Provide-Info
Help

no
no

Timeout

no

Rejection

no

Strategy Choices
System-Initiative (SI), Mixed-Initiative (MI)
SummarizeBoth (SB), SummarizeSystem (SS),
SummarizeChoicePrompt (SCP)
Read-First (RF), Read-Summary-Only (RSO),
Read-Choice-Prompt (RCP)
AskUserName, Ask-Which-Selection (AskWS),
Ask-Selection-Criteria (AskSC),
Read-Message
AskUserName-Help, SI-Top-Help, MI-Top-Help,
Read-Message-Help, AskWS-Help, AskSC-Help
AskUserName-Timeout,
Read-Timeout,
SI-Top-Timeout, MI-Top-Timeout, Read-MessageTimeout, AskWS-Timeout, AskSC-Timeout
AskUserName-Reject,
SITop-Reject, MI-Top-Reject, AskWS-Reject, AskSCReject, Read-Message-Reject

Figure 1: ELVIS's Dialogue Strategies. ELVIS explores choices in Initiative, Summarization
and Read Strategies and uses xed strategies elsewhere.
that Elvis makes choices among, then describe in detail the complete state machine, the
dialogue strategies from Figure 1 that are used in states where there is no choice among
dialogue strategies, and the space of policies that Elvis can execute. We provide several
detailed examples of dialogues that can be generated by Elvis's dialogue manager's state
machine.
2.1.1 Strategy Choices in ELVIS

One strategy choice in our experiments involved the exploration of two dierent strategies
for managing the dialogue initiative: system-initiative and mixed-initiative. In the implementation of Elvis tested here, the choice of initiative was made early in the dialogue and
the same style of initiative was then used throughout the dialogue whenever there could
have been a choice between the two in order to avoid confusing the user.
Previous research suggested that the system-initiative strategy in Figure 2 in which
Elvis is directive with the user might be the best strategy, given the current state of the
art in speech recognition (Kamm, 1995). In Figure 2, each of Elvis's utterances is labelled
with the name of the dialogue strategy that Elvis is using to produce the utterance, e.g.,
utterance E1 is labelled SI-Top, the name for the system-initiative top level strategy. The
choice of the system-initiative strategy also aects utterances E2 and E3; in these utterances
Elvis continues the dialogue by requesting particular items of information. In general, the
system-initiative strategy constrains what the user can say at each point in the dialogue,
by requesting particular items of information one at a time, and the user is constrained to
only providing the information that has been asked for. The reason why this strategy is
recommended given the current state of the art in speech recognition is that it guides the
391

Walker

E1: Hi, ELVIS here...You have 5 new or unread messages in your
inbox. Say Read, Summarize, or say Help for more options.
U1: Read.
E2: Select by Sender, Subject or in order of arrival?
U2: Sender.
E3: Which Sender?
U3: Kim.
E4: In the messages from Kim, there's 1 message about \Interviewing
Antonio" and 1 message about \Meeting Today." The rst message is
titled, \Interviewing Antonio." It says, \I'd like to interview him. I
could also go along to lunch. Kim."
U4: Next.
E5: This is a message about meeting today. It says the meeting this
morning is at 10:00 in 2d516. See you there, Kim.
U5: I'm done here.

SI-Top
Ask-Which-Selection
Ask-Selection-Criterion
Read-First (RF)

Read-Message

Figure 2: System-Initiative (SI) Dialogue Strategy: ELVIS explicitly prompts the user with
what to say at each point of the dialogue. ELVIS's utterances are labelled with
the strategy names.
user into saying things that the system is more likely to understand, and the recognizer
vocabulary at each state is constrained to responses to the question that is asked. This
strategy might be dispreferred by users however, because it could be tedious for the user to
be constrained to providing one item of information at a time. In the dialogue in Figure 2,
it takes four user utterances to access the message from Kim needed to complete Task 1.1.
Other research suggested that a better strategy is a mixed-initiative strategy that allows
the user to take the initiative except when the user fails to say anything or the system is
having trouble understanding the user (Smith & Hipp, 1994; Levin, Pieraccini, Eckert,
Fabbrizio, & Narayanan, 1999; Webber & Joshi, 1982; Walker & Whittaker, 1990). The
dialogue in Figure 3 illustrates the mixed-initiative dialogue strategy. Each of Elvis's
utterances in Figure 3 is labelled with the name of the dialogue strategy that Elvis is using
to produce the utterance, e.g., utterance E1 is labelled MI-Top, the name for the mixedinitiative top level strategy. Note that unlike the SI-Top strategy, the MI-Top strategy
does not volunteer information and does not tell the user what she can say. Instead, the
mixed-initiative strategy allows the user to take control of the dialogue and specify exactly
what she wants to do in a single utterance, as in utterance U1 in Figure 3. However,
if there is evidence that the user is having trouble or is confused, as when the user says
nothing in utterance U2, the mixed-initiative strategy will take the initiative to ask the
user for information or tell the user what she can say. In utterance E3 in Figure 3 the
Read-Timeout strategy is triggered by the fact that the user's response was delayed longer
than a pre-set timeout threshhold, and the system tells the user what actions are available
at this point in the dialogue. In general, the mixed-initiative strategy can support more
ecient dialogues; in the dialogue in Figure 2, it takes two user utterances to access the
message from Kim needed to complete Task 1.1. However this eciency gain may be oset
by the increased likelihood of misunderstandings and user confusion.
392

Reinforcement Learning in the ELVIS System

E1: Hi, ELVIS here. I've got your mail.
U1: Read me my messages from Kim.
E2: In the messages from Kim, there's 1 message about \Interviewing
Antonio" and 1 message about \Meeting Today." The rst message is
titled, \Interviewing Antonio." It says, \I'd like to interview him. I could
also go along to lunch. Kim."
U2: (User says nothing within 5 seconds so the system takes the initiative)
E3: You can say repeat, to repeat the last message, or you can say next,
to read the next message in this folder. You can say, I'm done here to
exit this folder.
U3: Next message.
E4: This is a message about meeting today. It says the meeting this
morning is at 10:00 in 2d516. See you there, Kim.
U4: I'm done here.

MI-Top
Read-First (RF)

Read-Timeout
ReadMessage

Figure 3: Mixed-Initiative (MI) Dialogue Strategy: ELVIS leaves it up to the user to take
the initiative, unless the user seems to be having trouble with the system
Summarize Strategy
Summarize-Both (SB)

Summarize-System (SS)
Summarize-Choice-Prompt
(SCP)

Example Prompt
In your top level inbox, from Kim, there's 1 message about
\Lunch." From Michael, there's 1 message about \Evaluation
group meeting." From Noah, there's 1 message about \Call Me
Tomorrow" and 1 message about \Interviewing Antonio." And
from Owen, there's 1 message about \Agent Personality."
In your top level inbox, there's 1 message from Kim, 2 messages
from Noah, 1 message from Michael, and 1 message from Owen.
E: Summarize by subject, by sender, or both?
U: Subject.
E: In your top level inbox, there's 1 message about \Lunch," 1
message about \Interviewing Antonio," 1 message about \Call
Me Tomorrow," 1 message about \Evaluation Group Meeting,"
and 1 message about \Agent Personality."

Figure 4: Alternate Summarization Strategies in response to a request to \Summarize my
messages"
A dierent type of strategy choice involves Elvis's decisions about how to present information to the user. We mentioned above that there are many dierent ways to summarize a
set of items that the user wants information about. Elvis explores the set of alternate summarization strategies illustrated in Figure 4; these strategies vary the message attributes
that are included in a summary of the messages in the current folder. The Summarize-Both
strategy (SB) uses both the sender and the subject attributes in the summary. When employing the Summarize-System strategy (SS), Elvis summarizes by subject or by sender
based on the current context. For instance, if the user is in the top level inbox, Elvis will
summarize by sender, but if the user is situated in a folder containing messages from a par393

Walker

ticular sender, Elvis will summarize by subject, as a summary by sender would provide no
new information. The Summarize-Choice-Prompt (SCP) strategy asks the user to specify
which of the relevant attributes to summarize by. See Figure 4.
Another type of information presentation choice occurs when a request from the user
to read some subset of messages, e.g., Read my messages from Kim, results in multiple
matching messages. The strategies explored in Elvis are summarized in Figure 5. One
choice is the Read-First strategy (RF) which involves summarizing all the messages from
Kim, and then taking the initiative to read the rst one. Elvis used this read strategy
in the dialogues in Figures 2 and 3. An alternate strategy for reading multiple matching
messages is the Read-Summary-Only (RSO) strategy, where Elvis provides information
that allows users to rene their selection criteria. Another strategy for reading multiple
messages is the Read-Choice-Prompt (RCP) strategy, where Elvis explicitly tells the user
what to say in order to rene the message selection criteria. See Figure 5.
Read Strategy
Read-First (RF)
Read-Summary-Only
(RSO)
Read-Choice-Prompt
(RCP)

Example Prompt
In the messages from Kim, there's 1 message about \Interviewing Antonio" and 1 message about \Meeting Today." The rst message is
titled, \Interviewing Antonio." It says, \I'd like to interview him. I
could also go along to lunch. Kim."
In the messages from Kim, there's 1 message about \Interviewing Antonio" and 1 message about \Meeting Today."
In the messages from Kim, there's 1 message about \Interviewing Antonio" and 1 message about \Meeting Today." To hear the messages,
say, \Interviewing Antonio" or \Meeting."

Figure 5: Alternate Read Strategies in response to a request to \Read my messages from
Kim"
The remainder of Elvis's dialogue strategies, as summarized in Figure 1, are xed, i.e.
multiple versions of these strategies are not explored in the experiments presented here.
2.1.2 ELVIS's Dialogue State Machine

As mentioned above, a dialogue strategy is a choice the system makes, in a particular
state, about what to say and how to say it. A policy for a dialogue system is a complete
specication of which strategy to execute in each system state. A state is dened by a set
of state variables. Ideally, the state representation corresponds to a dialogue model that
summarizes the dialogue history compactly, but retains all the relevant information about
the dialogue interaction so far. The notion of a dialogue model retaining all the relevant
information is more formally known in reinforcement learning as a state representation that
satises the Markov Property. A state representation satisfying the Markov Property is one
in which the probability of being in a particular state s with a particular reward r after
doing some action a in a prior state can be estimated as a function of the action and the
prior state, and not as a function of the complete dialogue history (Sutton & Barto, 1998).
More precisely,
Pr(st+1 = s0; rt+1 = rjst ; at ) = Pr(st+1 = s0 ; rt+1 = rjst; at ; rt ; st,1 ; at,1 ; rt,1 ; : : : R1; s0 ; a0 )
394

Reinforcement Learning in the ELVIS System

for all s0 ; r; st and at .
The Markov Property is guaranteed if the state representation encodes everything that
the system has been able to observe about everything that happened in the dialogue so far.
However, this representation would be too complex to estimate a model of the probability
of various state transitions, and systems as complex as spoken dialogue system must in
general utilize state representations which are as compact as possible.2 However if the state
representation is too impoverished, the system will lose too much relevant information to
work well.
Operations Variable
KnowUserName
InitStrat
SummStrat
ReadStrat
TaskProgress
CurrentUserGoal
NumMatches
WhichSelection
KnowSelectionCriteria
Condence
Timeout
Help
Cancel

Abbrev
(U)
(I)
(S)
(R)
(P)
(G)
(M)
(W)
(SC)
(C)
(T)
(H)
(L)

Possible Values
0,1
0,SI,MI
0,SS,SCP,SB
0,RF,RSO,RCP
0,1,2
0,Read,Summarize
0,1,N>1
0,Sender (Snd),Subject (Sub),InOrder (InO)
0,1
0,1
0,1
0,1
0,1

Figure 6: Operations variables and possible values that dene the operations vector for
controlling all aspects of ELVIS's behavior. The abbreviations for the variable
names and values are used as column headers for the Operations Variables in
Figures 7, 8 and 9.
's state space representation must obviously discriminate among states in which
various strategy choices are explored, but in addition, there must be state variables to
capture distinctions between a number of states in which Elvis always executes the same
strategy. The state variables that Elvis keeps track of and their possible values are given in
Figure 6. The KnowUserName (U) variable keeps track of whether Elvis knows the user's
name or not. The InitStrat (I), SummStrat (S) and ReadStrat (R) variables keep track of
whether Elvis has already employed a particular initiative strategy, summarize strategy
or a reading strategy in the current dialogue, and if so, which strategy it has selected.
This variable is needed because once Elvis employs one of these strategies, that strategy is
used consistently throughout the rest of the dialogue in order to avoid confusing the user.
The TaskProgress (P) variable tracks how much progress the user has made completing the
experimental task. The CurrentUserGoal (G) variable corresponds to the system's belief
Elvis

2. In some respects this is driven by implementation requirements since system development and maintenance is impossible without compact state representations.

395

Walker

U
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

Operations Variables
Action Choices
I S
R P G
M W SC C T H L
0 0
0 0 0
0
0 0 0 0 0 0 AskUserName
0 0
0 0 0
0
0 0 1 0 0 0 SI-Top, MI-Top
SI 0
0 0 0
0
0 0 1 0 1 0 SI-Top-Help
SI 0
0 0 0
0
0 0 0 0 0 0 SI-Top-Reject
SI 0
0 0 S
0
0 0 1 0 0 0 SS,SB,SCP
SI 0
0 0 R
0
0 0 1 0 0 0 AskWS
SI 0
0 0 R
0
0 0 0 0 0 0 AskWS-Reject
SI 0
0 0 R
0 Snd 0 1 0 0 0 AskSC
SI 0
0 0 R
0 Snd 0 1 1 0 0 AskSC-TimeOut
SI 0
0 0 R N>1 Snd 1 1 0 0 0 RF,RSO,RCP
SI 0 RCP 0 R
1 Snd 1 1 0 0 0 ReadMessage
SI 0 RCP 1 0
0
0 1 0 0 0 0 SI-Top
MI 0
0 0 0
0
0 0 1 0 1 0 MI-Top-Help
MI 0
0 0 0
0
0 0 0 0 0 0 MI-Top-Reject
MI 0
0 0 S
0
0 0 1 0 0 0 SS,SB,SCP
MI SS
0 0 R N>1 Snd 1 1 0 0 0 RF,RSO,RCP
MI SS RF 0 R
1 Snd 1 1 0 0 0 ReadMessage

Figure 7: A portion of ELVIS's operations state machine using the full operations vector
to control ELVIS's behavior
about what the user's current goal is. The WhichSelection (W) variable tracks whether
the system knows what type of selection criteria the user would like to use to read her
messages. The KnowSelectionCriteria (SC) variable tracks whether the system believes
it understood either a sender name or a subject name to use to select messages. The
NumMatches (M) variable keeps track of how many messages match the user's selection
criteria. The Condence (C) variable is a threshholded variable indicating whether the
speech recognizer's condence that it understood what the user said was above a pre-set
threshhold. The Timeout (T) variable represents the system's belief that the user didn't
say anything in the allotted time. The Help (H) variable represents the system's belief that
the user said Help, and leads to the system providing context-specic help messages. The
Cancel (L) variable represents the system's belief that the user said Cancel, which leads
to the system resetting the state to the state before the last user utterance was processed.
Thus there are 110,592 possible states used to control the operation of the system, although
not all of the states occur.3
In order for the reader to achieve a better understanding of the range of Elvis's capabilities and the way the operations vector is used, Figure 7 shows a portion of Elvis's state
machine that can generate the sample system and mixed-initiative dialogue interactions
in Figures 8 and 9. Each of these gures provides the state representation and the strategy choices made in each state of the sample dialogues. For example, row two of Figure
7 shows that after the system acquires the user's name (KnowUserName (U) = 1) with
high condence (Condence (C) = 1), that it can explore the system-initiative (SI-Top) or
3. For example until the system knows the user name, none of the other variable values change from their
initial value.

396

Reinforcement Learning in the ELVIS System

mixed-initiative (MI-Top) strategies. Figure 8 illustrates a dialogue in which the SI strategy
was chosen while Figure 9 illustrates a dialogue in which the MI-Top strategy was chosen.
Here we discuss in detail how the dialogue in Figure 8 was generated by the state machine
in Figure 7.
In Figure 8, the rst row shows that Elvis's strategy AskUserName is executed in
the initial state of the dialogue where all the operations variables are set to 0. Elvis's
utterance E1 is the surface realization of this strategy's execution. Note that according to
the state machine in Figure 7, there are no other strategy choices for the initial state of
the dialogue. The user responds with her name and the SLU module returns the user's
name to the dialogue manager with high condence (Condence (C) = 1). The dialogue
manager updates the operations variables with KnowUserName(U) = 1 and Condence (C)
= 1, as shown in row two of Figure 8. Now, according to the state machine in Figure 7,
there are two choices of strategy, the system-initiative strategy whose initial action is SI-Top
and the mixed-initiative strategy whose initial action is MI-Top. Figure 8 illustrates one
potential path when the SI-Top strategy is chosen; Elvis's utterance E2 is the realization
of the SI-Top strategy. The user responds with the utterance Help which is processed by
SLU, and the dialogue manager receives as input the information that SLU believes that
the user said Help (Help (H) = 1) with high condence (Condence (C) = 1). The dialogue
manager updates the operations variables to reect the information from SLU as well as
the fact that it executed the system-initiative strategy (InitStrat (I) = SI). This results in
the operations vector shown adjacent to Elvis's utterance E3. The third row of the state
machine in Figure 7 shows that in this state, Elvis has no choice of strategies, so Elvis
simply executes the SI-Top-Help strategy, which is realized as utterance E3. The user
responds by saying Read (utterance U3) and the dialogue manager updates the operations
variables with the results of the SLU module saying that it believes that the user said Read
(Goal (G) = R) with high condence (Condence (C) = 1). The state machine in Figure 7
species that in this state Elvis should execute the AskWhichSelection (AskWS) strategy,
which corresponds to Elvis's utterance E4 in Figure 8. This time, however, when the user
responds to the system's query with the word Sender (utterance U4), the SLU module
is not condent of its understanding (Condence (C) = 0) and the operations variable is
updated with this condence value. According to the state machine in Figure 7, the strategy
that Elvis executes in this state is the AskWS-Reject strategy, a specic rejection strategy
designed to be executed in the context of a goal to acquire the value of the WhichSelection
operations variable (see the list of operations variables in Figure 6). The realization of
the AskWS-Reject strategy is utterance E5. The user responds by saying Sender again
(utterance U5) and this time the SLU module understands the user's utterance with high
condence. The dialogue manager updates the WhichSelection (W) variable with Snd and
the condence variable, resulting in the operations vector shown adjacent to utterance
E6. The state machine in Figure 7 species that in this state Elvis should execute the
AskSelectionCriterion (AskSC) strategy which is realized as utterance E6. The user says
nothing (utterance U6) and the SLU module detects a silence above a pre-set timeout
threshhold with high condence. The dialogue manager updates the operations variables
to reect this observation (Timeout (T) = 1, Condence (C) = 1). The state machine in
Figure 7 species that the AskSC-Timeout strategy should be executed in this state; this is a
context specic strategy that re-attempts the system's initial goal by providing information
397

Walker

Operations Variables

Utterance

U I
0 0

S R
0 0

P
0

G
0

M W
0 0

SC C T
0
0 0

1

0

0

0

0

0

0

0

0

1

0

1

SI

0

0

0

0

0

0

0

1

0

1

SI

0

0

0

R

0

0

0

1

0

1

SI

0

0

0

R

0

0

0

0

0

1

SI

0

0

0

R

0

Snd

0

1

0

1

SI

0

0

0

R

0

Snd

0

1

1

1

SI

0

0

0

R

N

Snd

1

1

0

1

SI

0

RCP

0

R

1

Snd

1

1

0

1

SI

0

RCP

1

0

0

0

0

1

0

H L
0 0 E1: Hi, ELVIS here, Who is
that?
U1: Susan.
0 0 E2: Hi Susan, Please wait.
I'll get your mail (10 seconds).
Okay Susan, You have 5 new
or unread messages in your inbox. Say Read, Summarize or
say Help for more options.
U2: Help.
1 0 E3: To access your messages,
you can say read or summarize.
To switch the active messages,
say new messages, or all messages. If you want me to repeat
a message or a summary, say repeat. You can always say cancel
or help. Say Goodbye Elvis, to
hang up.
U3: Read.
0 0 E4: Select by Sender, by Subject
or In order of Arrival?
U4: Sender
0 0 E5: I didn't understand you. Do
you want to select messages by
sender, subject or in order of
arrival?
U5: Sender
0 0 E6: Which Sender?
U6: (user says nothing)
0 0 E7: The senders are, Kim,
Noah, Michael, and Owen.
Which sender do you want?
U7: Kim.
0 0 E8: In the messages from Kim,
there's 1 message about \Interviewing Antonio" and 1 message about \Meeting Today." To
hear the messages, say, \Interviewing Antonio" or \Meeting."
U8: Meeting.
0 0 E9: This is a message about
meeting today. It says the meeting this morning is at 10:00 in
2d516. See you there, Kim.
U9: I'm done here.
0 0 E10: You are back to your
toplevel inbox. Say Read, Summarize or say Help for more
options.

Strategy
Name
AskUserName
SI-Top

SI-Top-Help

AskWS
AskWSReject
AskSC
AskSCTimeOut
RCP

ReadMessage

SI-Top

Figure 8: A System-Initiative Dialogue, completing Task 1.1 in Figure 11, illustrating
ELVIS's ability to provide help, and use timeout and condence information
that is intended to help the user and then re-asking the original query, as realized by
utterance E7. The user responds with the name of the sender (utterance U7) which is
understood by SLU with high condence (KnowSelectionCriteria (SC) = 1, Condence =
398

Reinforcement Learning in the ELVIS System

1). When Elvis retrieves messages from the mail server matching this selection criteria,
multiple matches are found (NumMatches = N, as per the list of operations variables in
Figure 6). This time row ten of the state machine in Figure 7 species that this state
has a choice of dialogue strategies, namely a choice between the Read-First (RF), ReadSummary-Only (RSO) and Read-Choice-Prompt (RCP) strategies illustrated in Figure 5.
Elvis randomly chooses to explore the RCP strategy, which is realized as utterance E8.
The information the user needs to complete Task 1.1 is then provided by utterance E9 after
the user responds in utterance U8 by saying Meeting (and SLU understands this with high
condence). The row with utterance E9 in Figure 8 shows the updated operations vector
reecting the fact that the system executed the RCP strategy; the ReadStrat (R) variable
is used to enforce the fact that in this implementation of Elvis, once a particular reading,
strategy is selected, it is then used consistently throughout the dialogue to avoid confusing
the user. In the last exchange of Figure 8, the SLU module's condent understanding of
the user's utterance in U9, I'm done here, results in resetting the G,M,W, and SC variables
and the dialogue manager also updates the variable TaskProg (P) to 1 to reect progress on
the experimental task. Figure 7 shows that, in this state, the system has only one strategy;
since the InitStrat variable has been set to SI, the system executes the SI-Top strategy, as
realized in this context by utterance E10.
The dialogue in Figure 9 illustrates a potential dialogue with Elvis when the MI-Top
strategy is selected rather than the SI-Top strategy after the user name is acquired. The
reader may also track the path of this dialogue by utilizing the state machine in Figure 7.
Note that the operations vector that Elvis utilizes is needed to make Elvis a fully operational system that provides all the functionality equired to support users. The dialogues
in Figures 8 and 9 also show how Elvis provides:

 Context-Specic Help strategies: illustrated by the strategies SI-Top-Help and MITop-Help, and supported by the Help variable.

 Timeout strategies: that the system uses for taking the initiative in each context, trig-

gered by the delay for the user's response going above a time threshhold, as illustrated
by the strategy AskSC-TimeOut, and supported by the Timeout variable.

 Rejection strategies: backo dialogue actions that become more directive to the user
when the ASR returns a condence value lower than its condence threshhold, as
illustrated by strategies AskWS-Reject and MI-Top-Reject, and supported by the
Condence variable.

However, the operations vector state representation needed to control the operation of
the system is not necessarily required or even optimal for applying reinforcement learning
(Barto et al., 1995; Sutton & Barto, 1998). Sometimes it may be advantageous to aggregrate states for the purpose of applying reinforcement learning, even if it is not possible
to guarantee that the state representation obeys the Markov property (Sutton & Barto,
1998). Note that in many of the states dened in Figure 7 alternate strategies are not
explored. For example, the Condence variable is used by the system to determine whether
it should apply a backo rejection strategy, but we do not vary the rejection strategies. The
incentive for aggregrating states by ignoring state variables that are irrelevant to learning
399

Walker

Operations Variables

Utterance

U I
0 0

S
0

R
0

P G
0 0

M W
0 0

SC C
0
0

T
0

H L
0 0

1

0

0

0

0

0

0

0

0

0

0

1

0

E1: Hi, ELVIS here, Who is AskUserName
that?
U1: Susan.
E2: Hi Susan, Please wait. I'll
get your mail (delay of about 10
seconds).

1

MI 0

0

0

0

0

0

0

1

0

1

0

1

MI 0

0

0

0

0

0

0

0

0

0

0

1

MI 0

0

0

S

0

0

0

1

0

0

0

1

MI SS

0

0

R

N

Snd

1

1

0

0

0

1

MI SS

RF

1

R

1

Snd

1

1

0

0

0

Strategy
Name

Okay Susan, I've got your mail.
U2: Help.
E3: You can read or summarize messages using values from
the sender or the subject eld.
For example, you can say, read
the message from Lee about the
meeting, or, read my messages
from Kim, or, summarize my
messages. To access messages in
the current folder, you can also
say next, previous, rst, or last.
If you want me to repeat a message or a summary, say repeat.
You can always say cancel or
help. To exit the current folder
say, I'm done here. To hangup,
say, Goodbye Elvis .
U3: Summarize my messages.
E4: I didn't understand you.
What do you want to do with
your messages?
U4: Summarize them.
E5: In your top level inbox,
there's 2 messages from Kim,
1 message from Noah, 1 message from Michael, and 1 message from Owen.
U5: Read my messages from
Kim
E6: In the messages from Kim,
there's one message about Interviewing Antonio and one message about Meeting Today. The
rst message says (user barges
in)
U6: Next message
E7: Next message. This is a
message about meeting today. It
says the meeting this morning is
at 10:00 in 2d516. See you there,
Kim.

MI-Top
MI-Top-Help

MI-TopReject
SS

RF

ReadMessage

Figure 9: A Mixed Initiative Dialogue completing Task 1.1 in Figure 11, illustrating ELVIS's
ability to provide help, and use timeout and condence information
is a reduction in the state space size; this means that fewer dialogue samples are needed to
collect a large enough sample of state/action pairs for the purpose of applying reinforcement
learning. From this perspective, our goal is to aggregrate the state space in such a way as
to only distinguish states where dierent dialogue strategies are explored.
400

Reinforcement Learning in the ELVIS System

However, there is an additional constraint on state aggregration. Reinforcement learning4
backs up rewards received in the nal states of the dialogue sf to earlier states si where
strategy choices were explored. However the algorithm can only distinguish strategy choices
when the trajectory between si and sf are distinct for each strategy choice. In other words,
if two actions at some point lead to the same state, then without local reward, the Q-values
of these two actions will be equal.
UserName (U) Init (I) TaskProg (P) UserGoal (G)
0,1
0,SI,MI 0,1,2,
0,R,S
Figure 10: Reinforcement Learning State Variables and Values
Figure 10 species the subset of the state variables given in Figure 6 that we developed to represent the state space for the purpose of applying reinforcement learning. The
combination of these state variables is very compact, but provides distinct trajectories for
dierent strategy choices. The reduced state space has only 18 states, but supports dialogue
optimization over a policy space of 2  312 = 1062882 dierent policies. All of the policies
are prima facie candidates for optimal policies in that they can all support human users in
completing a set of experimental email tasks.

3. Experimental Design
Experimental dialogues for both the training and testing phase were collected via experiments in which human users interacted with Elvis to complete three representative application tasks that required them to access email messages in three dierent email inboxes.
We collected data from 73 users performing three tasks (219 dialogues) for training Elvis,
and then tested the learned policy against a corpus from six users performing the same
three tasks (18 dialogues).
Instructions to the users were given on a set of web pages, with one page for each experimental dialogue. The web page for each dialogue also contained a brief general description
of the functionality of the system, a list of hints for talking to the system, a description of
the tasks that the user was supposed to complete, and information on how to call Elvis.
Each page also contained a form for specifying information acquired from Elvis during the
dialogue, and a survey, to be lled out after task completion, designed to probe the user's
satisfaction with Elvis. Users read the instructions in their oces before calling Elvis
from their oce phone.
Each of the three calls to Elvis was made in sequence, and each conversation consisted
of two task scenarios where the system and the user exchanged information about criteria
for selecting messages and information within the message. The tasks are given in Figure
11, where, e.g., Task 1.1 and Task 1.2 were done in the same conversation with Elvis. The
motivation for asking the caller to complete multiple tasks in a call was to create subdialogue
structure in the experimental dialogues (Litman, 1985; Grosz & Sidner, 1986).
4. When applied without local rewards.

401

Walker











Task 1.1: You are working at home in the morning and plan to go directly to a meeting when
you go into work. Kim said she would send you a message telling you where and when the
meeting is. Find out the Meeting Time and the Meeting Place.
Task 1.2: The second task involves nding information in a dierent message. Yesterday
evening, you had told Lee you might want to call him this morning. Lee said he would send
you a message telling you where to reach him. Find out Lee's Phone Number.
Task 2.1: When you got into work, you went directly to a meeting. Since some people were
late, you've decided to call ELVIS to check your mail to see what other meetings may have
been scheduled. Find out the day, place and time of any scheduled meetings.
Task 2.2: The second task involves nding information in a dierent message. Find out if you
need to call anyone. If so, nd out the number to call.
Task 3.1: You are expecting a message telling you when the Discourse Discussion Group can
meet. Find out the place and time of the meeting.
Task 3.2: The second task involves nding information in a dierent message. Your secretary
has taken a phone call for you and left you a message. Find out who called and where you
can reach them.

Figure 11: Sample Task Scenarios





Dialogue Eciency Metrics: elapsed time, system turns, user turns
Dialogue Quality Metrics mean recognition score, timeouts, rejections, helps, cancels,

bargeins, timeout%, rejection%, help%, cancel%, bargein%
Task Success Metrics: task completion as per survey
User Satisfaction: the sum of TTS Performance, ASR Performance, Task Ease, Interaction
Pace, User Expertise, System Response, Expected Behavior, Comparable Interface, Future
Use.

Figure 12: Metrics collected for spoken dialogues.
We collect a number of of dierent measures for each dialogue via four dierent methods:
(1) All of the dialogues are recorded; (2) The dialogue manager logs each state that the
system enters and the dialogue strategy that Elvis selects in that state; (3) The dialogue
manager logs information for calculating a number of dialogue quality and dialogue eciency
metrics summarized in Figure 12 and described in more detail below; and (4) At the end
of the dialogue, users ll out web page forms to support the calculation of task success and
user satisfaction measures. We explain below how we use these measures in the paradise
framework and in reinforcement learning.
402

Reinforcement Learning in the ELVIS System

The dialogue eciency metrics were calculated from the dialogue recordings and the
system logs. The length of the recording was used to calculate the elapsed time in seconds
(ET) from the beginning to the end of the interaction. Measures for the number of System
Turns, and the number of User Turns, were calculated on the basis of the system logging
everything it said and everything it heard the user say.
The dialogue quality measures were derived from the recordings, the system logs and
hand-labeling. A number of system behaviors that aect the quality of the resulting dialogue
were automatically logged. These included the number of timeout prompts (timeouts)
played when the user didn't respond as quickly as expected, the number of recognizer
rejections (rejects) where the system's condence in its understanding was low and it said
something like I'm sorry I didn't understand you. User behaviors that the system perceived
that might aect the dialogue quality were also logged: these included the number of times
the system played one of its context specic help messages because it believed that the
user had said Help (helps), and the number of times the system reset the context and
returned to an earlier state because it believed that the user had said Cancel (cancels).
The recordings were used to check whether users barged in on system utterances, and these
were labeled on a per-state basis (bargeins).
Another measure of dialogue quality was recognizer performance over the whole dialogue,
calculated in terms of concept accuracy. The recording of the user's utterance was compared
with the logged recognition result to calculate a concept accuracy measure for each utterance
by hand. Concept accuracy is a measure of semantic understanding by the system, rather
than word for word understanding. For example, the utterance Read my messages from
Kim contains two concepts, the read function, and the sender:kim selection criterion. If the
system understood only that the user said Read, then concept accuracy would be 0.5. Mean
concept accuracy was then calculated over the whole dialogue and used, in conjunction with
ASR rejections, to compute a Mean Recognition Score (MRS) for the dialogue.
Because our goal is to generate models of performance that will generalize across systems
and tasks, we also thought it important to introduce metrics that are likely to generalize.
All of the eciency metrics seemed unlikely to generalize since, e.g., the elapsed time to
complete a task depends on how dicult the task is. Other research suggested that the
dialogue quality metrics were more likely to generalize (Litman, Walker, & Kearns, 1999),
but we thought that the raw counts were likely to be task specic. Thus we normalized the
dialogue quality metrics by dividing the raw counts by the total number of utterances in the
dialogue. This resulted in the timeout%, rejection%, help%, cancel%, and bargein%
metrics.
The web page forms are the basis for calculating Task Success and User Satisfaction
measures. Users reported their perceptions as to whether they had completed the task
(Comp).5 They also had to provide objective evidence that they had in fact completed the
task by lling in a form with the information that they had acquired from Elvis.6
5. Yes,No responses are converted to 1,0.
6. This supports an alternative way of calculating Task Success objectively by using the Kappa statistic
to compare the information that the users lled in with a key for the task (Walker et al., 1997a). However
some of our earlier results indicated that the user's perception of task success was a better predictor of
overall satisfaction, so here we simply use perceived task success as measured by Comp.

403

Walker











Was ELVIS easy to understand in this conversation? (TTS Performance)
In this conversation, did ELVIS understand what you said? (ASR Performance)
In this conversation, was it easy to nd the message you wanted? (Task Ease)
Was the pace of interaction with ELVIS appropriate in this conversation? (Interaction Pace)
In this conversation, did you know what you could say at each point of the dialogue? (User
Expertise)
How often was ELVIS sluggish and slow to reply to you in this conversation? (System
Response)
Did ELVIS work the way you expected him to in this conversation? (Expected Behavior)
In this conversation, how did ELVIS's voice interface compare to the touch-tone interface to
voice mail? (Comparable Interface)
From your current experience with using ELVIS to get your email, do you think you'd use
ELVIS regularly to access your mail when you are away from your desk? (Future Use)

Figure 13: User Satisfaction Survey
In order to calculate User Satisfaction, users were asked to evaluate the system's performance with the user satisfaction survey in Figure 13. Some of the question responses were
on a ve point Likert scale and some simply required yes, no or yes, no, maybe responses.
The survey questions probed a number of dierent aspects of the users' perceptions of their
interaction with Elvis in order to focus the user on the task of rating the system, as in
(Shriberg et al., 1992; Jack, Foster, & Stentiford, 1992; Love, Dutton, Foster, Jack, &
Stentiford, 1994). Each multiple choice survey response was mapped into the range of 1 to
5. Then the values for all the responses were summed, resulting in a User Satisfaction
measure for each dialogue with a possible range of 8 to 40.

4. Training and Testing an Optimized Dialogue Strategy
Given the experimental training data, we rst apply paradise to estimate a performance
function for Elvis as a linear combination of the metrics described above. We apply the
performance function to each dialogue in the training corpus to estimate a utility for the
nal state of the dialogue and then apply Q-learning using this utility. Finally we test the
learned policy against a new population of users.

4.1

paradise

Performance Modeling

The rst step in developing a performance model for spoken dialogue systems was the specication of the causal model of performance illustrated in Figure 14 (Walker et al., 1997a).
According to this model, the system's primary objective is to maximize user satisfaction.
404

Reinforcement Learning in the ELVIS System

MAXIMIZE USER SATISFACTION

MAXIMIZE TASK
SUCCESS

MINIMIZE COSTS

EFFICIENCY
MEASURES

Figure 14:

QUALITATIVE
MEASURES

's structure of objectives for spoken dialogue performance.

paradise

Task success and various costs that can be associated with the interaction are both contributors to user satisfaction. Task success can be measured quantitatively in a number of
ways: it could be represented by a continuous variable representing quality of solution or
by a boolean variable representing binary task completion. Dialogue costs are of two types:
dialogue eciency and quality. Eciency costs are measures of the system's eciency in
helping the user complete the task, such as the number of utterances to completion of the
dialogue. Dialogue quality costs are intended to capture other aspects of the system that
may have strong eects on user's perception of the system, such as the number of times the
user had to repeat an utterance in order to make the system understand the utterance.
Given this model, a performance metric for a dialogue system can be estimated from
experimental data by applying multivariate linear regression with user satisfaction as the
dependent variable and task success, dialogue quality, and dialogue eciency measures as
independent variables.7 A stepwise linear regression on the training data over the measures
discussed above, showed that Comp, MRS, BargeIn% and Rejection% were signicant
contributors to User Satisfaction, accounting for 39% of the variance in R-Squared (F
(4,192)=30.7, p <.0001).8
Performance = :27  Comp + :54  MRS , :09  BargeIn% + :15  Rejection%
We tested how well this performance function should generalize to unseen test dialogues
with tenfold cross-validation, by randomly sampling 90% of the training dialogues and
testing the goodness of t of the performance model on the remaining 10% of the dialogues
7. One advantage of this approach is that once the performance function is derived, it is no longer necessary
to collect user satisfaction reports from users, which opens up the possibility of estimating the reward
function from fully automatic measures. This latter possibility might also be useful for online calculation
of the reward function or for calculating a local reward.
8. We normalize the metrics before doing the regression so that the magnitude of the coecients directly
indicates the contribution of that factor to User Satisfaction (Cohen, 1995; Walker et al., 1997a).

405

Walker

in the training set. The average R2 for the training set was 37% with a standard error of
.005, while the average R2 for the held-out 10% of the dialogues was 38% with a standard
error of .06. Since the average R2 for the test set is statistically indistinguishable from the
training set, we assume that the performance model will generalize to new Elvis dialogues.

4.2 Training an Optimized Policy

Given the learned performance function described above, we apply this function to the
measures logged for each dialogue Di , thereby replacing a range of measures with a single performance value Pi , which is used as the utility (reward) for the nal state of each
dialogue.9 We then apply reinforcement learning with Pi as the utility of the nal state
of the dialogue Di (Bellman, 1957; Sutton, 1991; Tesauro, 1992; Russell & Norvig, 1995;
Watkins, 1989). The utility of doing action a in state Si , U (a; Si ) (its Q-value), can be
calculated in terms of the utility of a successor state Sj , by obeying the recursive equation:
U (a; Si ) = R(a; Si ) + Mija max
U (a0 ; Sj )
a

X

0

j

where R(a; Si ) is the immediate reward received for doing action a in Si , a is a strategy
from a nite set of strategies A that are admissable in state Si, and Mija is the probability
of reaching state Sj if strategy a is selected in state Si . In the experiments reported here,
the reward associated with each state, R(Si ), is zero. In addition, since reliable a priori
prediction of a user action in a particular state is not possible (for example the user may say
Help or the speech recognizer may fail to understand the user), the state transition model
Mija is estimated from the logged state-strategy history for the dialogue.
The utility values can be estimated to within a desired threshold using value iteration,
which updates the estimate of U (a; Si ), based on updated utility estimates for neighboring
states, so that the equation above becomes:

Un+1 (a; Si ) = R(Si) +

XM
j

a
ij

max
Un(a0 ; Sj )
a
0

where Un (a; Si ) is the utility estimate for doing a in state Si after n iterations (Sutton &
Barto, 1998) pp. 101. Value iteration stops when the dierence between Un (a; Si ) and
Un+1 (a; Si ) is below a threshold, and utility values have been associated with states where
strategy selections were made.10 Once value iteration is completed the optimal policy is
obtained by selecting the action with the maximal Q-value in each dialogue state.
Figure 15 enumerates the subset of the states in the aggregrated state space used for
reinforcement learning and potential actions dening the policy space. The strategy with
the greatest Q-value in each state after training is indicated by boldface in Figure 15.
This optimized policy will then be tested as a xed policy in the operation of Elvis. In
all the states of the task, the System-Initiative strategy in Figure 2 is predicted to be the
optimal initiative strategy, and the Read-First strategy in Figure 5 is predicted to have
the best performance of the Read strategies. As Figure 15 shows, the learned strategy
9. Each dialogue is treated as having a unique nal state.
10. After experimenting with various threshholds, we used a threshold of 5% of the performance range of
the dialogues.

406

Reinforcement Learning in the ELVIS System

State Variables
U I P G
0 0 0 0
1 0 0 0
1 SI 0 S
1 SI 0 R
1 SI 1 0
1 SI 1 S
1 SI 1 R
1 SI 2 0
1 SI 2 S
1 SI 2 R
1 MI 0 S
1 MI 0 R
1 MI 1 0
1 MI 1 S
1 MI 1 R
1 MI 2 0
1 MI 2 S
1 MI 2 R

Strategy Choices
AskUserName
SI-Top, MI-Top
SS,SB,SCP
RF,RSO,RCP
SI-Top
SS,SB,SCP
RF,RSO,RCP
SI-Top
SS,SB,SCP
RF,RSO,RCP
SS,SB,SCP
RF,RSO,RCP
MI-Top
SS,SB,SCP
RF,RSO,RCP
MI-Top
SS,SB,SCP
RF,RSO,RCP

Figure 15: The subset of the state space that denes the policy class explored in our experiments. The learned policy is indicated by boldface.
for summarization varies according to the state of the task. The dierent summarization
strategies were illustrated in Figure 4. The policy that is learned is to use the SummarizeBoth strategy at the beginning of the dialogue (when TaskProg = 0), and then to switch to
using the Summarize-System strategy at later phases of the dialogue. This strategy makes
sense in terms of giving the user complete information about all the messages in her inbox
at the beginning of the dialogue.

4.3 Testing an Optimized Policy

We rst constructed a deterministic version of Elvis that implemented the learned policy as
discussed above, with one variation. The variation was based on the fact that the decision
on whether to use the Summarize-Both or Summarize-System summarization strategy was
conditioned on the value of the TaskProg variable. However, we intended to utilize the optimized version of the system in situations where we would not have access to the TaskProg
variable, namely situations where the task that the user was attempting to perform were not
under the control of the experimenter. When we examined the Q-values for the summarization strategies over the course of the dialogue, we found that the Summarize-System strategy
had the greatest average Q-value, being strongly preferred to the Summarize-Both strategy
except in the initial phase of the dialogue, where the Q-value for the Summarize-Both was
407

Walker

only slightly greater. Thus we implemented the learned policy (see Figure 15), with the
exception that the Summarize-System strategy was used throughout the dialogue.11
In terms of the operations state machine in Figure 7, implementation of the learned
policy means that the choices between the SI-Top and MI-Top strategies are replaced by
the SI-Top strategy, choices between the dierent read strategies in dierent states are
replaced by the Read-First (RF) strategy and choices between the dierent summarization
strategies in dierent states are replaced by the Summarize-System (SS) strategy.
We then tested this policy on six new users who had never used Elvis before. These
users conversed with Elvis to perform the same set of six email tasks that were used
in the training phase, as described in Figure 10 above. In addition, identical performance
measures were collected for each testing dialogue and training dialogue. Overall performance
measures for the training and test dialogues are given in Table 1, with the training data split
in terms of System-Initiative, Mixed-Initiative and overall means. The table shows that all
versions of Elvis have high levels of task completion, which is important for testing the
utility of reinforcement learning. Statistical analysis of these results indicated a statistically
signicant increase in User Satisfaction from training to test (F= = 4.07 p = .047).

5. Discussion and Future Work

This paper proposes a novel method by which a dialogue system can learn to choose an
optimal dialogue strategy and tests it in experiments with Elvis, a dialogue system that
supports access to email by phone, with strategies for initiative, and for reading and summarizing messages. We reported experiments in which Elvis learned that the System-Initiative
strategy has higher utility than the Mixed-Initiative strategy, that Read-First is the best
read strategy, and that Summarize-System is generally the best summary strategy. We
then tested the policy that Elvis learned on a new set of users performing the same set of
tasks and showed that the learned policy resulted in a statistically signicant increase in
user satisfaction in the test set of dialogues.
Previous work has also treated a system's choice of dialogue strategy as a stochastic
optimization problem (Walker, 1993; Biermann & Long, 1996; Levin & Pieraccini, 1997;
Levin et al., 1997). To our knowledge, Walker (1993) rst proposed that reinforcement
learning algorithms could be applied to dialogue strategy selection. In simulation experiments reported by Walker (1993, 1996), dialogues between two agents in an articial world
were used to test which dialogue strategies were optimal under various conditions. These
experiments varied: (1) the dialogue agent's resource bounds; and (2) the performance function used to assess the agent's performance. The experiments showed that strategies that
were not optimal under one set of assumptions about the performance function could be
highly ecacious when the performance function reected the fact that the dialogue agent
was resource bounded. Walker (1993) suggested that the optimal dialogue strategy could be
11. Obviously this choice of the strategy to test risked testing a non-optimal policy. An alternative that we
would like to try in future work is to utilize only the SummStrat state variable from the operations vector
in the state representation for reinforcement learning and simply distinguish states where no summarize
strategy has been selected (no summary has been produced) and states where at least one summary has
been produced. If the analysis about dialogue phase carries through, then the policy that should be
learned is to use the Summarize-Both strategy for the rst summary in a dialogue and then afterwards
use the Summarize-System strategy.

408

Reinforcement Learning in the ELVIS System

Measure Train SI Train MI Overall Train Test
Comp
.87
.80
.85
.94
User Turns
21.5
17.0
20.0 25.8
System Turns
24.2
21.2
23.1 29.2
Elapsed time (sec) 339.14
296.18
311.56 368.5
Mean recognition score
.88
.72
.82
.81
TimeOuts
2.7
4.2
3.0
3.3
TimeOut%
.11
.19
.13
.11
Cancs
.34
.02
.26
.00
Canc%
.02
.00
.01
.00
Help Requests
.67
.92
0.66 1.11
Help%
.03
.05
.03
.04
BargeIns
3.6
3.6
3.7
7.8
BargeIn%
.08
.09
.18
.30
Rejects
.9
1.6
1.1
1.4
Reject%
.04
.08
.05
.05
User satisfaction
28.9
25.0
27.5 31.7
Table 1: Performance measure means per dialogue for Training and Testing Dialogues. SI
= System-Initiative, MI = Mixed-Initiative
learned via reinforcement learning, if an appropriate performance function could be determined, and described an experiment using genetic algorithms to learn an optimal dialogue
strategy. In subsequent work, utilized here, the paradise model was proposed as a way to
learn an appropriate performance function (Walker et al., 1997a). In addition, related work
utilizing Elvis, that varied the reward function, and applied other reinforcement learning
algorithms, was carried out by Fromer (Fromer, 1998).
Biermann and Long (1996), proposed the use of similar techniques in the context of
learning optimal dialogue strategies for a multi-modal dialogue tutor. The goal of the tutor
was to instruct students taking their rst programming class and the tutor interacted with
the students by highlighting parts of their code and printing text on the screen telling them
what was wrong with their program. Biermann and Long describe a planned experiment in
which the system would vary its instructional style, and the system's reward would be the
amount of time between the system's instructions and the student's response. This reward
function was based on the assumption that a delayed response suggested a greater cognitive
load for the student, and that cognitive load should be minimized in an instructional setting.
Levin and colleagues also proposed treating dialogue systems as Markov Decision Processes and suggested that system designers could determine what an appropriate objective
function might be (Levin et al., 1997; Levin & Pieraccini, 1997). They carried out a series
of experiments in which a simulated user interacted with an implemented spoken dialogue
system for travel planning by exchanging messages at the semantic meaning level. They
showed that the system could learn strategy choices at the level of database interaction,
409

Walker

e.g., that the system should not query the database until it had determined many of the
constraints necessary in order to nd ights that matched the user's goals.
Stochastic optimization techniques have also been applied to similar problems in textbased dialogue interaction and graphical user interfaces. Mellish and colleagues applied
stochastic optimization to the problem of determining the content and structure of the
system's utterances in the ILEX system, an interactive museum tour guide (Mellish et al.,
1998). This work was not tested against a user population and the performance (reward)
measure was based on heuristics about good text plans formulated by experts. Christensen
and colleagues applied genetic algorithms to the design of a graphical user interface for an
automated teller machine. The goal was to automatically learn the best layout of a sequence
of interaction screens for intracting with a user (Christensen, Marks, & Shieber, 1994). In
this work, as in that of Levin and colleagues, the user population was simulated.
Here, the method for optimizing dialogue strategy selection was illustrated by evaluating strategies for managing initiative and for information presentation by interaction with
human callers. We applied the paradise performance model to derive an empirically motivated performance function, that combines both subjective user preferences and objective
system performance measures into a single function. It would have been impossible to predict a priori which dialogue factors inuence the usability of a dialogue system, and to what
degree. Our performance equation shows that task success and dialogue quality measures
are the primary contributors to system performance. Furthermore, in contrast to assuming an a priori model, we use the dialogues from real user-system interactions to provide
realistic estimates of Mija , the state transition model used by the learning algorithm. It
is impossible to predict a priori the transition frequencies, given the imperfect nature of
spoken language understanding, and the unpredictability of user behavior.
The use of this method introduces several open issues and possible areas for future
work. First, the results of the learning algorithm are dependent on the representation of
the state space. In spoken dialogue systems, the system designers construct the state space
and decide what state variables the system needs to monitor, whereas in other applications of reinforcement learning (e.g. backgammon), the state space is pre-dened. In the
experiments reported here, we xed the state representation and carried out experiments
on a particular state representation. However in future work we hope to be able to learn
which aspects of the state history should be represented using similar techniques to those
described in (Langkilde, Walker, Wright, Gorin, & Litman, 1999). For example, it may be
benecial for the system to represent additional state variables representing more of the
dialogue history, in order for Elvis to be able to learn dialogue strategies that reect those
aspects of the dialogue history.
Second, in advance of actually running experiments, it is not clear how much experience
a system will need to determine which strategy is better. In the experiments reported here,
we were able to show an improvement for a policy that had converged on the initiative and
read strategies but had not yet converged on the appropriate summarization strategy. It
is possible that if our local rewards had been nonzero that the optimal policy could have
been learned from less training data. In future work, we hope to explore the interaction of
training set size and the use of a local reward.
Third, our experimental data is based on xing particular experimental parameters. All
of the experiments are based on short-term interactions with novice users, but we might
410

Reinforcement Learning in the ELVIS System

expect that users of an email system would engage in many interactions with the same
system, and that preferences for system interaction strategies could change over time with
user expertise. This means that the performance function might change over time. We also
used a xed set of tasks that were representative of the domain, but it is possible that some
aspects of the policies we learned might be sensitive to our experimental tasks. Another
limitation is that the experiments were carried out in a scenario where each email folder
only had a small number of messages: the strategies tested here might not be optimal when
an email folder contains hundreds of messages.
Fourth, the optimal strategy is potentially dependent on various system parameters. For
example, the ReadFirst strategy takes the initiative to read a message, which might result
in messages being read that the user wasn't interested in, but since the user can barge-in
on system utterances, there is little overhead with taking this decision. If the system did
not support barge-in, our results might have been dierent.
Fifth, the learned policy depends on the reward function. For example, since Elvis
is a fully functional system, users can complete the experimental task with any version of
the system using any of the strategies that we explored. This means that if we had used
task completion as the reward function, reinforcement learning would have predicted that
there were no dierences between the dierent strategies. On the other hand, by using the
paradise performance function, we utilized a reward function that was t to our data and
Elvis's performance, and we have some evidence that this reward function may generalize
to other systems (Walker, Kamm, & Litman, 2000).
Sixth, the experiments that we report here are limited in the way that they demonstrate
the utility of reinforcement learning for dialogue strategy optimization. A more traditional
way of selecting the best dialogue strategies would be with experiments which treated
dialogue strategy selection as a factor, and standard statistical hypothesis testing would be
used to compare the performance of dierent strategies. The scale of the experiment here
is small enough that it is imaginable that the space of policies could possibly have been
tested in the more traditional way. However, the primary goal of the experiments reported
here was simply to test the feasibility of these methods, which required working out in
detail many of the issues of state and strategy representation discussed above. Now that
many of these details have been worked out, the methods presented here can be applied to
much more complex dialogue strategy optimization problems, such as varying the initiative
depending on the dialogue state (Chu-Carroll & Brown, 1997; Webber & Joshi, 1982), or
exploring combinations of strategies for information presentation, summarization (SparckJones, 1999), error recovery (Hirschman & Pao, 1993), database query (Levin et al., 1997),
cooperative responses (Joshi, Webber, & Weischedel, 1986; Finin et al., 1986; Chu-Carroll &
Carberry, 1994), and content selection for generation (McKeown, 1985; Kittredge, Korelsky,
& Rambow, 1991), inter alia.
Finally, the learning algorithm that we report here is an o-line algorithm, i.e. Elvis
collects a set of dialogues and then decides on an optimal strategy as a result. In contrast, it
should be possible for Elvis to learn on-line, during the course of a dialogue, once methods
are developed for the performance function to be automatically calculated or approximated.
Our primary goal with the experiments reported here was to explore the application
of reinforcement learning to spoken dialogue systems and to identify open issues such as
those discussed above. In current work, we are exploring these issues in several ways. We
411

Walker

have codied the notion of a state estimator so that we can systematically vary the state
representation in order to explore the eect of the state representation on the value function
and the optimal policy (Singh, Kearns, Litman, & Walker, 1999). We are also in the process
of using reinforcement learning to conduct a set of experiments on a spoken dialogue system
for accessing information about activities in New Jersey. In these experiments we explore
a number of dierent reward functions and also explore a much broader range of strategies
for user initiative, for reprompting the user, and for conrming the system's understanding.

6. Acknowledgements
I received many useful questions and comments on this research when I presented some
initial results at an invited talk given at AAAI 1997 in Providence, R.I. The design and
implementation of the basic functionality of Elvis was done in collaboration with J. Fromer,
G. DiFabbrizio, D. Hindle and C. Mestel. Initial experiments on reinforcement learning with
Elvis were done in collaboration with J. Fromer and S. Narayanan. This work has also
beneted from discussions with W. Eckert, C. Kamm, M. Kearns, E. Levin, D. Litman, D.
McAllester, R. Pieraccini, R. Sutton, and S. Singh. Special thanks are to S. Whittaker, J.
Wiebe and four reviewers for detailed comments on earlier versions of this manuscript.
v

References
Allen, J. F. (1979). A Plan-Based Approach to Speech Act Recognition. Tech. rep., University of Toronto.
Baggia, P., Castagneri, G., & Danieli, M. (1998). Field trials of the italian arise train
timetable system. In Interactive Voice Technology for Telecommunications Applications, IVTTA, pp. 97{102.
Barto, A., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic
programming. Articial Intelligence Journal, 72(1-2), 81{138.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton, N.J.
Biermann, A. W., & Long, P. M. (1996). The composition of messages in speech-graphics
interactive systems. In Proceedings of the 1996 International Symposium on Spoken
Dialogue, pp. 97{100.
Bruce, B. (1975). Belief systems and language understanding. Tech. rep. AI-21, Bolt,
Berenak and Newman.
Carberry, S. (1989). Plan recognition and its use in understanding dialogue. In Kobsa,
A., & Wahlster, W. (Eds.), User Models in Dialogue Systems, pp. 133{162. Springer
Verlag, Berlin.
Carbonell, J. R. (1971). Mixed-initiative man-computer dialogues. Tech. rep. 1970, Bolt
Beranek and Newman, Cambridge, MA.
412

Reinforcement Learning in the ELVIS System

Christensen, J., Marks, J., & Shieber, S. (1994). Placing text labels on maps and diagrams.
In Heckbert, P. (Ed.), Graphics Gems IV. Academic Press.
Chu-Carroll, J., & Brown, M. K. (1997). Tracking initiative in collaborative dialogue interactions. In Proceedings of the 35th Annual Meeting of the Association for Computational
Linguistics, pp. 262{270.
Chu-Carroll, J., & Carberry, S. (1994). A plan-based model for response generation in
collaborative task-oriented dialogue. In AAAI 94, pp. 799{805.
Cohen, P. R. (1995). Empirical Methods for Articial Intelligence. MIT Press, Boston.
Cohen, P. R. (1978). On knowing what to say: Planning speech acts. Tech. rep. 118,
University of Toronto; Department of Computer Science.
Danieli, M., & Gerbino, E. (1995). Metrics for evaluating dialogue strategies in a spoken
language system. In Proceedings of the 1995 AAAI Spring Symposium on Empirical
Methods in Discourse Interpretation and Generation, pp. 34{39.
Finin, T. W., Joshi, A. K., & Webber, B. L. (1986). Natural language interactions with
articial experts. Proceedings of the IEEE, 74(7), 921{938.
Fromer, J. C. (1998). Learning optimal discourse strategies in a spoken dialogue system.
Tech. rep., MIT AI Lab M.S. Thesis.
Grosz, B. J. (1983). Team: A transportable natural language interface system. In Proc. 1st
Applied ACL, Association of Computational Linguistics, Santa Monica, Ca.
Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions and the structure of discourse.
Computational Linguistics, 12, 175{204.
Hirschman, L., & Pao, C. (1993). The cost of errors in a spoken language system. In Proceedings of the Third European Conference on Speech Communication and Technology,
pp. 1419{1422.
Jack, M., Foster, J. C., & Stentiford, F. W. (1992). Intelligent dialogues in automated telephone services. In International Conference on Spoken Language Processing, ICSLP,
pp. 715 { 718.
Joshi, A. K., Webber, B., & Weischedel, R. M. (1986). Some aspects of default reasoning
in interactive discourse. Tech. rep. MS-CIS-86-27, University of Pennsylvania.
Kamm, C., Narayanan, S., Dutton, D., & Ritenour, R. (1997). Evaluating spoken dialog systems for telecommunication services. In 5th European Conference on Speech
Technology and Communication, EUROSPEECH 97, pp. 2203{2206.
Kamm, C. (1995). User interfaces for voice applications. In Roe, D., & Wilpon, J.
(Eds.), Voice Communication between Humans and Machines, pp. 422{442. National
Academy Press.
413

Walker

Keeney, R., & Raia, H. (1976). Decisions with Multiple Objectives: Preferences and Value
Tradeos. John Wiley and Sons.
Kittredge, R., Korelsky, T., & Rambow, O. (1991). On the need for domain communication
knowledge. Computational Intelligence, 7 (4), 305{314.
Langkilde, I., Walker, M., Wright, J., Gorin, A., & Litman, D. (1999). Automatic prediction
of problematic human-computer dialogues in How May I Help You?. In Proceedings of
the IEEE Workshop on Automatic Speech Recognition and Understanding, ASRUU99.
Levin, E., & Pieraccini, R. (1997). A stochastic model of computer-human interaction for
learning dialogue strategies. In EUROSPEECH 97.
Levin, E., Pieraccini, R., & Eckert, W. (1997). Learning dialogue strategies within the
Markov Decision Process framework. In Proc. IEEE Workshop on Automatic Speech
Recognition and Understanding.
Levin, E., Pieraccini, R., Eckert, W., Fabbrizio, G. D., & Narayanan, S. (1999). Spoken
language dialogue: From theory to practice. In Proc. IEEE Workshop on Automatic
Speech Recognition and Understanding, ASRUU99.
Litman, D. (1985). Plan recognition and discourse analysis: An integrated approach for
understanding dialogues. Tech. rep. 170, University of Rochester.
Litman, D. J., Walker, M. A., & Kearns, M. J. (1999). Automatic detection of poor speech
recognition at the dialogue level. In Proceedings of the Thirty Seventh Annual Meeting
of the Association of Computational Linguistics, pp. 309{316.
Love, S., Dutton, R. T., Foster, J. C., Jack, M. A., & Stentiford, F. W. M. (1994). Identifying salient usability attributes for automated telephone services. In International
Conference on Spoken Language Processing, ICSLP, pp. 1307{1310.
McKeown, K. R. (1985). Discourse strategies for generating natural language text. Articial
Intelligence, 27 (1), 1{42.
Mellish, C., Knott, A., Oberlander, J., & O'Donnell, M. (1998). Experiments using stochastic search for text planning. In Proceedings of International Conference on Natural
Language Generation, pp. 97{108.
Mohri, M., Pereira, F. C. N., & Riley, M. D. (1998). Fsm library { general purpose nitestate machine software tools..
Moore, J. D., & Paris, C. L. (1989). Planning text for advisory dialogues. In Proc. 27th
Annual Meeting of the Association of Computational Linguistics.
Pollack, M., Hirschberg, J., & Webber, B. (1982). User participation in the reasoning process
of expert systems. In Proceedings First National Conference on Articial Intelligence,
pp. pp. 358{361.
Power, R. (1974). A Computer Model of Conversation. Ph.D. thesis, University of Edinburgh.
414

Reinforcement Learning in the ELVIS System

Rabiner, L. R., Juang, B. H., & Lee, C. H. (1996). An overview of automatic speech
recognition. In Lee, C. H., Soong, F. K., & Paliwal, K. K. (Eds.), Automatic Speech
and Speaker Recognition, Advanced Topics, pp. 1{30. Kluwer Academic Publishers.
Russell, S., & Norvig, P. (1995). Articial Intelligence: A Modern Approach. Prentiss Hall,
Englewood Clis, N.J.
Sanderman, A., Sturm, J., den Os, E., Boves, L., & Cremers, A. (1998). Evaluation of
the dutchtrain timetable information system developed in the ARISE project. In
Interactive Voice Technology for Telecommunications Applications, IVTTA, pp. 91{
96.
Sene, S., Zue, V., Polifroni, J., Pao, C., Hetherington, L., Goddeau, D., & Glass, J. (1995).
The preliminary development of a displayless PEGASUS system. In ARPA Spoken
Language Technology Workshop.
Shriberg, E., Wade, E., & Price, P. (1992). Human-machine problem solving using spoken language systems (SLS): Factors aecting performance and user satisfaction. In
Proceedings of the DARPA Speech and NL Workshop, pp. 49{54.
Simmons, R., & Slocum, J. (1975). Generating english discourse from semantic networks.
CACM, 15 (10), 891{905.
Singh, S., Kearns, M. S., Litman, D. J., & Walker, M. A. (1999). Reinforcement learning
for spoken dialogue systems. In Proc. NIPS99.
Smith, R. W., & Hipp, D. R. (1994). Spoken Natural Language Dialog Systems: A Practical
Approach. Oxford University Press.
Sparck-Jones, K. (1993). What might be in a summary?. In Proceedings of Information
Retrieval 93: Von der Modellierung zur Anwendung, pp. 9{26 Universitatsverlag Knstanz.
Sparck-Jones, K. (1999). Automatic summarizing; factors and directions. In Mani, I., &
Maybury, M. (Eds.), Advances in Automatic Text Summarization. MIT Press.
Sparck-Jones, K., & Galliers, J. R. (1996). Evaluating Natural Language Processing Systems.
Springer.
Sproat, R., & Olive, J. (1995). An approach to text-to-speech synthesis. In Kleijn, W. B.,
& Paliwal, K. K. (Eds.), Speech Coding and Synthesis, pp. 611{633. Elsevier.
Sutton, R. S. (1991). Planning by incremental dynamic programming. In Proceedings Ninth
Conference on Machine Learning, pp. 353{357. Morgan-Kaufmann.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning. MIT Press.
Tesauro, G. (1992). Practical Issues in Temporal Dierence Learning. Machine Learning,
8 (3{4), 257{277.
Walker, D. (1978). Understanding Spoken Language. Elsevier, North-Holland, New York.
415

Walker

Walker, M., Fromer, J., Fabbrizio, G. D., Mestel, C., & Hindle, D. (1998). What can I say:
Evaluating a spoken language interface to email. In Proceedings of the Conference on
Computer Human Interaction (CHI 98), pp. 582{589.
Walker, M. A., Litman, D., Kamm, C. A., & Abella, A. (1997a). PARADISE: A general
framework for evaluating spoken dialogue agents. In Proceedings of the 35th Annual
Meeting of the Association of Computational Linguistics, ACL/EACL 97, pp. 271{
280.
Walker, M., Hindle, D., Fromer, J., Fabbrizio, G. D., & Mestel, C. (1997b). Evaluating
competing agent strategies for a voice email agent. In Proceedings of the European
Conference on Speech Communication and Technology, EUROSPEECH97.
Walker, M. A. (1993). Informational Redundancy and Resource Bounds in Dialogue. Ph.D.
thesis, University of Pennsylvania.
Walker, M. A. (1996). The Eect of Resource Limits and Task Complexity on Collaborative
Planning in Dialogue. Articial Intelligence Journal, 85 (1{2), 181{243.
Walker, M. A., Fromer, J. C., & Narayanan, S. (1998). Learning optimal dialogue strategies: A case study of a spoken dialogue agent for email. In Proceedings of the 36th
Annual Meeting of the Association of Computational Linguistics, COLING/ACL 98,
pp. 1345{1352.
Walker, M. A., Kamm, C. A., & Litman, D. J. (2000). Towards developing general models
of usability with PARADISE. Natural Language Engineering: Special Issue on Best
Practice in Spoken Dialogue Systems.
Walker, M. A., & Whittaker, S. (1990). Mixed initiative in dialogue: An investigation into
discourse segmentation. In Proc. 28th Annual Meeting of the ACL, pp. 70{79.
Watkins, C. J. (1989). Models of Delayed Reinforcement Learning. Ph.D. thesis, Cambridge
University.
Webber, B., & Joshi, A. (1982). Taking the initiative in natural language database interaction: Justifying why. In Coling 82, pp. 413{419.
Winograd, T. (1972). Understanding Natural Language. Academic Press, New York, N.Y.
Woods, W. A. (1984). Natural language communication with machines: An ongoing goal.
In Reitman, W. (Ed.), Articial Intelligence Applications for Business, pp. 195{209.
Ablex Publishing Corp, Norwood, N.J.

416

Journal of Artificial Intelligence Research 12 (2000) 1–34

Submitted 7/99; published 2/00

Planning Graph as a (Dynamic) CSP:
Exploiting EBL, DDB and other CSP Search Techniques in Graphplan
Subbarao Kambhampati

RAO @ ASU . EDU

Department of Computer Science and Engineering
Arizona State University, Tempe AZ 85287-5406

Abstract
This paper reviews the connections between Graphplan’s planning-graph and the dynamic
constraint satisfaction problem and motivates the need for adapting CSP search techniques to the
Graphplan algorithm. It then describes how explanation based learning, dependency directed backtracking, dynamic variable ordering, forward checking, sticky values and random-restart search
strategies can be adapted to Graphplan. Empirical results are provided to demonstrate that these
augmentations improve Graphplan’s performance significantly (up to 1000x speedups)on several
benchmark problems. Special attention is paid to the explanation-based learning and dependency
directed backtracking techniques as they are empirically found to be most useful in improving the
performance of Graphplan.

1. Introduction
Graphplan (Blum & Furst, 1997) is currently one of the more efficient algorithms for solving classical planning problems. Four of the five competing systems in the recent AIPS-98 planning competition were based on the Graphplan algorithm (McDermott, 1998). Extending the efficiency of
the Graphplan algorithm thus seems to be a worth-while activity. In (Kambhampati, Parker, &
Lambrecht, 1997), we provided a reconstruction of Graphplan algorithm to explicate its links to
previous work in classical planning and constraint satisfaction. One specific link that was discussed
is the connection between the process of searching Graphplan’s planning graph, and solving a “dynamic constraint satisfaction problem” (DCSP) (Mittal & Falkenhainer, 1990). Seen from the DCSP
perspective, the standard backward search proposed by Blum and Furst (1997) lacks a variety of ingredients that are thought to make up efficient CSP search mechanisms (Frost & Dechter, 1994;
Bayardo & Schrag, 1997). These include forward checking, dynamic variable ordering, dependency directed backtracking and explanation-based learning (Tsang, 1993; Kambhampati, 1998).
In (Kambhampati et al., 1997), I have suggested that it would be beneficial to study the impact of
these extensions on the effectiveness of Graphplan’s backward search.
In this paper, I describe my experiences with adding a variety of CSP search techniques to improve Graphplan backward search–including explanation-based learning (EBL) and dependencydirected backtracking capabilities (DDB), Dynamic variable ordering, Forward checking, sticky
values, and random-restart search strategies. Of these, the addition of EBL and DDB capabilities
turned out to be empirically the most useful. Both EBL and DDB are based on explaining failures
at the leaf-nodes of a search tree, and propagating those explanations upwards through the search
tree (Kambhampati, 1998). DDB involves using the propagation of failure explanations to support
intelligent backtracking, while EBL involves storing interior-node failure explanations, for pruning
future search nodes. Graphplan does use a weak form of failure-driven learning that it calls “mem-

c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

K AMBHAMPATI

oization.” As we shall see in this paper, Graphplan’s brand of learning is quite limited as there is
no explicit analysis of the reasons for failure. Instead the explanation of failure of a search node is
taken to be all the constraints in that search node. As explained in (Kambhampati, 1998), this not
only eliminates the opportunities for dependency directed backtracking, it also adversely effects the
utility of the stored memos.
Adding full-fledged EBL and DDB capabilities in effect gives Graphplan both the ability to
do intelligent backtracking, and the ability to learn generalized memos that are more likely to be
applicable in other situations. Technically, this involves generalizing conflict-directed backjumping
(Prosser, 1993), a specialized version of EBL/DDB strategy applicable for binary CSP problems1
to work in the context of dynamic constraint satisfaction problems (as discussed in (Kambhampati, 1998)). Empirically, the EBL/DDB capabilities improve Graphplan’s search efficiency quite
dramatically–giving rise to up to 1000x speedups, and allowing Graphplan to easily solve several
problems that have hither-to been hard or unsolvable. In particular, I will report on my experiments
with the bench-mark problems described by Kautz and Selman (1996), as well as 4 other domains,
some of which were used in the recent AIPS planning competition (McDermott, 1998).
I discuss the utility issues involved in storing and using memos, and point out that the Graphplan
memoization strategy can be seen as a very conservative form of CSP no-good learning. While this
conservative strategy keeps the storage and retrieval costs of no-goods –the usual bane of no-good
learning strategies–under control, it also loses some learning opportunities. I then present the use
of “sticky values” as a way of recouping some of these losses. Empirical studies show that sticky
values lead to a further 2-4x improvement over EBL.
In addition to EBL and DDB, I also investigated the utility of forward checking and dynamic
variable ordering, both in isolation and in concert with EBL and DDB. My empirical studies show
that these capabilities typically lead to an additional 2-4x speedup over EBL/DDB, but are not by
themselves competitive with EBL/DDB.
Finally, I consider the utility of the EBL/DDB strategies in the context of random-restart search
strategies (Gomes, Selman, & Kautz, 1998) that have recently been shown to be good at solving hard combinatorial problems, including planning problems. My results show that EBL/DDB
strategies retain their advantages even in the context of such random-restart strategies. Specifically,
EBL/DDB strategies enable Graphplan to use the backtrack limits more effectively–allowing it to
achieve higher solvability rates, and more optimal plans with significantly smaller backtrack and
restart limits.
This paper is organized as follows. In the next section, I provide some background on viewing
Graphplan’s backward search as a (dynamic) constraint satisfaction problem, and review some of
the opportunities this view presents. In Section 3, I discuss some inefficiencies of the backtracking
and learning methods used in normal Graphplan that motivate the need for EBL/DDB capabilities.
Section 4 describes how EBL and DDB are added to Graphplan. Section 5 presents empirical studies
demonstrating the usefulness of these augmentations. Section 7 investigates the utility of forward
checking and dynamic variable ordering strategies for Graphplan. Section 8 investigates the utility
of EBL/DDB strategies in the context of random-restart search. Section 9 discusses related work
and Section 10 presents conclusions and some directions for further work.
1. Binary CSP problems are those problems where all initial constraints are between pairs of variables.

2

P LANNING G RAPH

Action list
Level k-1

Proposition list
Level k-1

Action list
Level k

Proposition List
Level k

G1 ;    ; G4 ; P1    P6
Domains: G1 : fA1 g; G2 : fA2 gG3 : fA3 gG4 : fA4 g
P1 : fA5 gP2 : fA6 ; A11 gP3 : fA7 gP4 : fA8 ; A9 g
P5 : fA10 gP6 : fA10 g
Constraints (normal):P1 = A5 ) P4 6= A9
P2 = A6 ) P4 6= A8
P2 = A11 ) P3 6= A7
Constraints (Activity): G1 = A1 ) ActivefP1 ; P2 ; P3 g
G2 = A2 ) ActivefP4 g
G3 = A3 ) ActivefP5 g
G4 = A4 ) ActivefP1 ; P6 g
Init State: ActivefG1 ; G2 ; G3 ; G4 g

P1
A6

A1

X
A7
A8

P3

A 11

A2

G2
G3

P4
P5

A 10

G1

P2

A9

X

CSP

Variables:

A5

X

AS A

A3

G4

P6
A4

(a) Planning Graph

(b) DCSP

Figure 1: A planning graph and the DCSP corresponding to it

2. Review of Graphplan Algorithm and its Connections to DCSP
2.1 Review of Graphplan Algorithm
Graphplan algorithm (Blum & Furst, 1997) can be seen as a “disjunctive” version of the forward
state space planners (Kambhampati et al., 1997; Kambhampati, 1997). It consists of two interleaved
phases – a forward phase, where a data structure called “planning-graph” is incrementally extended,
and a backward phase where the planning-graph is searched to extract a valid plan. The planninggraph consists of two alternating structures, called proposition lists and action lists. Figure 1 shows
a partial planning-graph structure. We start with the initial state as the zeroth level proposition list.
Given a k level planning graph, the extension of structure to level k + 1 involves introducing all
actions whose preconditions are present in the k th level proposition list. In addition to the actions
given in the domain model, we consider a set of dummy “persist” actions, one for each condition
in the k th level proposition list. A “persist-C” action has C as its precondition and C as its effect.
Once the actions are introduced, the proposition list at level k + 1 is constructed as just the union of
the effects of all the introduced actions. Planning-graph maintains the dependency links between the
actions at level k + 1 and their preconditions in level k proposition list and their effects in level k + 1
proposition list. The planning-graph construction also involves computation and propagation of
“mutex” constraints. The propagation starts at level 1, with the actions that are statically interfering
with each other (i.e., their preconditions and effects are inconsistent) labeled mutex. Mutexes are
then propagated from this level forward by using a two simple rules: two propositions at level k are
marked mutex if all actions at level k that support one proposition are mutex with all actions that
support the second proposition. Two actions at level k + 1 are mutex if they are statically interfering
or if one of the propositions (preconditions) supporting the first action is mutually exclusive with
one of the propositions supporting the second action.
The search phase on a k level planning-graph involves checking to see if there is a sub-graph
of the planning-graph that corresponds to a valid solution to the problem. This involves starting
with the propositions corresponding to goals at level k (if all the goals are not present, or if they are
present but a pair of them are marked mutually exclusive, the search is abandoned right away, and
planning-grap is grown another level). For each of the goal propositions, we then select an action
3

K AMBHAMPATI

G1 ;    ; G4 ; P1    P6
G1 : fA1 g; G2 : fA2 gG3 : fA3 gG4 : fA4 g
P1 : fA5 gP2 : fA6 ; A11 gP3 : fA7 gP4 : fA8 ; A9 g
P5 : fA10 gP6 : fA10 g
Constraints (normal):P1 = A5 ) P4 6= A9
P2 = A6 ) P4 6= A8
P2 = A11 ) P3 6= A7
Constraints (Activity): G1 = A1 ) ActivefP1 ; P2 ; P3 g
G2 = A2 ) ActivefP4 g
G3 = A3 ) ActivefP5 g
G4 = A4 ) ActivefP1 ; P6 g
Init State: ActivefG1 ; G2 ; G3 ; G4 g

G1 ;    ; G4 ; P1    P6
G1 : fA1 ; ?g; G2 : fA2 ; ?gG3 : fA3 ; ?gG4 : fA4 ; ?g
P1 : fA5 ; ?gP2 : fA6 ; A11 ; ?gP3 : fA7 ; ?gP4 : fA8 ; A9 ; ?g
P5 : fA10 ; ?gP6 : fA10 ; ?g
Constraints (normal):P1 = A5 ) P4 6= A9
P2 = A6 ) P4 6= A8
P2 = A11 ) P3 6= A7
Constraints (Activity): G1 = A1 ) P1 6=? ^P2 6=? ^P3 6=?
G2 = A2 ) P4 6=?
G3 = A3 ) P5 6=?
G4 = A4 ) P1 6=? ^P6 6=?
Init State: G1 6=? ^G2 6=? ^G3 6=? ^G4 6=?

Variables:

Variables:

Domains:

Domains:

(a) DCSP

(b) CSP

Figure 2: Compiling a DCSP to a standard CSP
from the level k action list that supports it, such that no two actions selected for supporting two
different goals are mutually exclusive (if they are, we backtrack and try to change the selection of
actions). At this point, we recursively call the same search process on the k , 1 level planning-graph,
with the preconditions of the actions selected at level k as the goals for the k , 1 level search. The
search succeeds when we reach level 0 (corresponding to the initial state).
Consider the (partial) planning graph shown in Figure 3 that Graphplan may have generated
and is about to search for a solution. G1    G4 are the top level goals that we want to satisfy,
and A1    A4 are the actions that support these goals in the planning graph. The specific actionprecondition dependencies are shown by the straight line connections. The actions A5    A11 at the
left-most level support the conditions P1    P6 in the planning-graph. Notice that the conditions P2
and P4 at level k , 1 are supported by two actions each. The x-marked connections between the
actions A5 ; A9 , A6 ; A8 and A7 ; A11 denote that those action pairs are mutually exclusive. (Notice
that given these mutually exclusive relations alone, Graphplan cannot derive any mutual exclusion
relations at the proposition level P1    P6 .)
2.2 Connections Between Graphplan and CSP
The Graphplan algorithm as described above bears little resemblance to previous classical planning
algorithms. In (Kambhampati et al., 1997), we explicate a number of important links between
the Graphplan algorithm and previous work in planning and constraint satisfaction communities.
Specifically, I show that a planning-graph of length k can be thought of (to a first approximation) as a
disjunctive (unioned) version of a k -level search tree generated by a forward state-space refinement,
with the action lists corresponding to the union of all actions appearing at k th level, and proposition
lists corresponding to the union of all states appearing at the k th level. The mutex constraints
can be seen as providing (partial) information about which subsets of a proposition list actually
correspond to legal states in the corresponding forward state-space search. The process of searching
the planning graph to extract a valid plan from it can be seen as a dynamic constraint satisfaction
problem. Since this last link is most relevant to the work described in this paper, I will review it
further below.
The dynamic constraint satisfaction problem (DCSP) (Mittal & Falkenhainer, 1990) is a generalization of the constraint satisfaction problem (Tsang, 1993), that is specified by a set of variables,
4

P LANNING G RAPH

AS A

CSP

activity flags for the variables, the domains of the variables, and the constraints on the legal variablevalue combinations. In a DCSP, initially only a subset of the variables is active, and the objective is
to find assignments for all active variables that is consistent with the constraints among those variables. In addition, the DCSP specification also contains a set of “activity constraints.” An activity
constraint is of the form: “if variable x takes on the value vx , then the variables y; z; w::: become
active.”
The correspondence between the planning-graph and the DCSP should now be clear. Specifically, the propositions at various levels correspond to the DCSP variables2 , and the actions supporting them correspond to the variable domains. There are three types of constraints: action mutex
constraints, fact (proposition) mutex constraints and subgoal activation constraints.
Since actions are modeled as values rather than variables, action mutex constraints have to be
modeled indirectly as constraints between propositions. If two actions a1 and a2 are marked mutex
with each other in the planning graph, then for every pair of propositions p11 and p12 where a1 is
one of the possible supporting actions for p11 and a2 is one of the possible supporting actions for
p12 , we have the constraint:

: (p11 = a1 ^ p12 = a2 )
Fact mutex constraints are modeled as constraints that prohibit the simultaneous activation of
the two facts. Specifically, if two propositions p11 and p12 are marked mutex in the planning graph,
we have the constraint:

: (Active(p11 ) ^ Active(p12 ))

Subgoal activation constraints are implicitly specified by action preconditions: supporting an
active proposition p with an action a makes all the propositions in the previous level corresponding
to the preconditions of a active.
Finally, only the propositions corresponding to the goals of the problem are “active” in the beginning. Figure 1 shows the dynamic constraint satisfaction problem corresponding to the example
planning-graph that we discussed.
2.2.1 S OLVING

A

DCSP

There are two ways of solving a DCSP problem. The first, direct, approach (Mittal & Falkenhainer,
1990) involves starting with the initially active variables, and finding a satisfying assignment for
them. This assignment may activate some new variables, and these newly activated variables are
assigned in the second epoch. This process continues until we reach an epoch where no more new
variables are activated (which implies success), or we are unable to give a satisfying assignment to
the activated variables at a given epoch. In this latter case, we backtrack to the previous epoch and
try to find an alternative satisfying assignment to those variables (backtracking further, if no other
assignment is possible). The backward search process used by the Graphplan algorithm (Blum &
Furst, 1997) can be seen as solving the DCSP corresponding to the planning graph in this direct
fashion.
The second approach for solving a DCSP is to first compile it into a standard CSP, and use
the standard CSP algorithms. This compilation process is quite straightforward and is illustrated in
2. Note that the same literal appearing in different levels corresponds to different DCSP variables. Thus, strictly speaking, a literal p in the proposition list at level i is converted into a DCSP variable pi . To keep matters simple, the
example in Figure 1 contains syntactically different literals in different levels of the graph.

5

K AMBHAMPATI

Figure 2. The main idea is to introduce a new “null” value (denoted by “?”) into the domains of
each of the DCSP variables. We then model an inactive DCSP variable as a CSP variable which
takes the value ?. The constraint that a particular variable P be active is modeled as P 6=?. Thus,
activity constraint of the form

G1 = A1 ) ActivefP1 ; P2 ; P3 g
is compiled to the standard CSP constraint

G1 = A1 ) P1 6=? ^P2 6=? ^P3 6=?
It is worth noting here that the activation constraints above are only concerned about ensuring
that propositions that are preconditions of a selected action do take non-? values. They thus allow
for the possibility that propositions can become active (take non-? values) even though they are
strictly not supporting preconditions of any selected action. Although this can lead to inoptimal
plans, the mutex constraints ensure that no unsound plans will be produced (Kautz & Selman,
1999). To avoid unnecessary activation of variables, we need to add constraints to the effect that
unless one of the actions needing that variable as a precondition has been selected as the value for
some variable in the earlier (higher) level, the variable must have ? value. Such constraints are
typically going to have very high arity (as they wind up mentioning a large number of variables in
the previous level), and may thus be harder to handle during search.
Finally, a mutex constraint between two propositions

: (Active(p11 ) ^ Active(p12 ))
is compiled into

: (p11 6=? ^p12 6=?) :

Since action mutex constraints are already in the standard CSP form, with this compilation, all
the activity constraints are converted into standard constraints and thus the entire CSP is now a
standard CSP. It can now be solved by any of the standard CSP search techniques (Tsang, 1993).3
The direct method has the advantage that it closely mirrors the Graphplan’s planning graph
structure and its backward search. Because of this, it is possible to implement the approach on the
plan graph structure without explicitly representing all the constraints. Furthermore, as I will discuss in Section 6, there are some distinct advantages for adopting the DCSP view in implementing
EBL/DDB on Graphplan. The compilation to CSP requires that plan graph be first converted into
an extensional CSP. It does however allow the use of standard algorithms, as well as supports nondirectional search (in that one does not have to follow the epoch-by-epoch approach in assigning
variables).4 Since my main aim is to illustrate the utility of CSP search techniques in the context of
the Graphplan algorithm, I will adopt the direct solution method for the DCSP. For a study of the
tradeoffs offered by the technique of compiling the planning graph into a CSP, the reader is referred
to (Do & Kambhampati, 2000).
3. It is also possible to compile any CSP problem to a propositional satisfiability problem (i.e., a CSP problem with
boolean variables). This is accomplished by compiling every CSP variable P that has the domain fv1 ; v2 ;    ; vn g
into n boolean variables of the form P-is-v1   P-is-vn . Every constraint of the form P
vj ^    )    is compiled
to P-is-vj ^    )   . This is essentially what is done by the BLACKBOX system (Kautz & Selman, 1999).
4. Compilation to CSP is not a strict requirement for doing non-directional search. In (Zimmerman & Kambhampati,
1999), we describe a technique that allows the backward search of Graphplan to be non-directional, see the discussion
in Section 10.

=

6

P LANNING G RAPH

AS A

CSP

2.3 Interpreting Mutex Propagation from the CSP View
Viewing the planning graph as a constraint satisfaction problem helps put the mutex propagation
in a clearer perspective (see (Kambhampati et al., 1997)). Specifically, the way Graphplan constructs its planning graph, it winds up enforcing partial directed 1-consistency and partial directed
2-consistency (Tsang, 1993). The partial 1-consistency is ensured by the graph building procedure
which introduces an action at level l only if the actions preconditions are present in the proposition
list at level l , 1 and are not mutually exclusive. Partial 2-consistency is ensured by the mutual
exclusion propagation procedure.
In particular, the Graphplan planning graph construction implicitly derives “no-good”5 constraints of the form:

:Active(Pmi ) (or Pmi 6=?)

i will be simply removed from (or will not be put into) the level i, and the mutex
In which case Pm
constraints of the form:




: Active(Pmi ) ^ Active(Pni )

or Pmi 6=? ^Pni 6=?)

(

i and P i are marked mutually exclusive.
in which case Pm
n
Both procedures are “directed” in that they only use “reachability” analysis in enforcing the consistency, and are “partial” in that they do not enforce either full 1-consistency or full 2-consistency.
Lack of full 1-consistency is verified by the fact that the appearance of a goal at level k does not
necessarily mean that the goal is actually achievable by level k (i.e., there is a solution for the CSP
that assigns a non- ? value to that goal at that level). Similarly, lack of full 2-consistency is verified by the fact that appearance of a pair of goals at level k does not imply that there is a plan for
achieving both goals by that level.
There is another, somewhat less obvious, way in which the consistency enforcement used in
Graphplan is partial (and very conservative)–it concentrates only on whether a single goal variable
or a pair of goal variables can simultaneously have non- ? values (be active) in a solution. It may
be that a goal can have a non- ? value, but not all non- ? values are feasible. Similarly, it may be
that a pair of goals are achievable, but not necessarily achievable with every possible pair of actions
in their respective domains.
This interpretation of mutex propagation procedure in Graphplan brings to fore several possible
extensions worth considering for Graphplan:
1. Explore the utility of directional consistency enforcement procedures that are not based solely
on reachability analysis. Kambhampati et. al. (1997) argue for extending this analysis using
relevance information, and Do et. al. (2000) provide an empirical analysis of the effectiveness
of consistency enforcement through relevance information.
2. Explore the utility of enforcing higher level consistency. As pointed out in (Kambhampati
et al., 1997; Kambhampati, 1998), the memoization strategies can be seen as failure-driven
procedures that incrementally enforce partial higher level consistency.
5. Normally, in the CSP literature, a no-good is seen as a compound assignment that can not be part of any feasible
i 6 ? ^Pni 6 ? correspond to a conjunction of nogoods of
solution. With this view, mutex constraints of the form Pm
i
i
i and Pni .
the the form Pm au ^ Pn av where au and av are values in the domains of Pm

=

=

=

7

=

K AMBHAMPATI

3. Consider relaxing the focus on non- ? values alone, and allow derivation of no-goods of the
form

Pmi = au ^ Pni = av

This is not guaranteed to be a winning idea as the number of derived no-goods can increase
quite dramatically. In particular, assuming that there are l levels in the planning graph, and an
average of m goals per level, and an average of d actions supporting each goal, the maximum
number of Graphplan style pair-wise mutexes will be O (l  m2 ) while the 2-size no-goods of
type discussed here will be O (l  (m  (d + 1))2 ). We consider a similar issue in the context
of Graphplan memoization strategy in Section 6.

3. Some Inefficiencies of Graphplan’s Backward Search
To motivate the need for EBL and DDB, we shall first review the details of Graphplan’s backward
search, and pinpoint some of its inefficiencies. We shall base our discussion on the example planning
graph from Figure 3 (which is reproduced for convenience from Figure 1). Assuming that G1    G4
are the top level goals of the problem we are interested in solving, we start at level k , and select
actions to support the goals G1    G4 . To keep matters simple, we shall assume that the search
assigns the conditions (variables) at each level from top to bottom (i.e., G1 first, then G2 and so
on). Further, when there is a choice in the actions (values) that can support a condition, we will
consider the top actions first. Since there is only one choice for each of the conditions at this level,
and none of the actions are mutually exclusive with each other, we select the actions A1 ; A2 ; A3
and A4 for supporting the conditions at level k . We now have to make sure that the preconditions
of A1 ; A2 ; A3 ; A4 are satisfied at level k , 1. We thus subgoal on the conditions P1    P6 at level
k , 1, and recursively start the action selection for them. We select the action A5 for P1 . For P2 ,
we have two supporting actions, and using our convention, we select A6 first. For P3 , A7 is the
only choice. When we get down to selecting a support for P4 , we again have a choice. Suppose
we select A8 first. We find that this choice is infeasible as A8 is mutually exclusive with A6 that is
already chosen. So, we backtrack and choose A9 , and find that it too is mutually exclusive with a
previously selected action, A5 . We now are stymied as there are no other choices for P4 . So, we
have to backtrack and undo choices for the previous conditions. Graphplan uses a chronological
backtracking approach, whereby, it first tries to see if P3 can be re-assigned, and then P2 and so on.
Notice the first indication of inefficiency here – the failure to assign P4 had nothing to do with the
assignment for P3 , and yet, chronological backtracking will try to re-assign P3 in the vain hope of
averting the failure. This can lead to a large amount of wasted effort had it been the case that P3 did
indeed have other choices.
As it turns out, we find that P3 has no other choices and backtrack over it. P2 does have another
choice – A11 . We try to continue the search forward with this value for P2 , but hit an impasse at P3 –
since the only value of P3 , A7 is mutex with A11 . At this point, we backtrack over P3 , and continue
backtracking over P2 and P1 , as they too have no other remaining choices. When we backtrack over
P1 , we need to go back to level k and try to re-assign the goals at that level. Before this is done, the
Graphplan search algorithm makes a “memo” signifying the fact that it failed to satisfy the goals
P1    P6 at this level, with the hope that if the search ever subgoals on these same set of goals in
future, we can scuttle it right away with the help of the remembered memo. Here is the second
indication of inefficiency – we are remembering all the subgoals P1    P6 even though we can see
that the problem lies in trying to assign P1 ; P2 ; P3 and P4 simultaneously, and has nothing to do
8

P LANNING G RAPH

Action list
Level k-1

Proposition list
Level k-1

AS A

CSP

Action list
Level k

Proposition List
Level k

A5
P1
A6

X

A1

X
A7
A8

P2
A2

P3

G3
A3

P5
A 10

G2

P4

A9

X

G1

G4

P6
A4

A 11

Figure 3: The running example used to illustrate EBL/DDB in Graphplan
with the other subgoals. If we remember fP1 ; P2 ; P3 ; P4 g as the memo as against fP1    P6 g, the
remembered memo would be more general, and would have a much better chance of being useful
in the future.
After the memo is stored, the backtracking continues into level k – once again in a chronological
fashion, trying to reassign G4 ; G3 ; G2 and G1 in that order. Here we see the third indication of inefficiency caused by chronological backtracking – G3 really has no role in the failure we encountered
in assigning P3 and P4 – since it only spawns the condition P5 at level k , 1. Yet, the backtracking
scheme of Graphplan considers reassigning G3 . A somewhat more subtle point is that reassigning
G4 is not going to avert the failure either. Although G4 requires P1 one of the conditions taking
part in the failure, P1 is also required by G1 and unless G1 gets reassigned, considering further
assignments to G4 is not going to avert the failure.
For this example, we continue backtracking over G2 and G1 too, since they too have no alternative supports, and finally memoize fG1 ; G2 ; G3 ; G4 g at this level. At this point the backward search
fails, and Graphplan extends the planning graph by another level before re-initiating the backward
search on the extended graph.

4. Improving Backward Search with EBL and DDB
I will now describe how Graphplan’s backward search can be augmented with full fledged EBL
and DDB capabilities to eliminate the inefficiencies pointed out in the previous section. Informally,
EBL/DDB strategies involve explanation of failures at leaf nodes, and regression and propagation
of leaf node failure explanations to compute interior node failure explanations, along the lines described in (Kambhampati, 1998). The specific extensions I propose to the backward search can

9

K AMBHAMPATI

essentially be seen as adapting conflict-directed backjumping strategy (Prosser, 1993), and generalizing it to work with dynamic constraint satisfaction problems.
The algorithm is shown in pseudo-code form in Figure 4. It contains two mutually recursive
procedures find-plan and assign-goals. The former is called once for each level of the
planning-graph. It then calls assign-goals to assign values to all the required conditions at that
level. assign-goals picks a condition, selects a value for it, and recursively calls itself with
the remaining conditions. When it is invoked with empty set of conditions to be assigned, it calls
find-plan to initiate the search at the next (previous) level.
In order to illustrate how EBL/DDB capabilities are added, let’s retrace the previous example,
and pick up at the point where we are about to assign P4 at level k , 1, having assigned P1 ; P2 and
P3 . When we try to assign the value A8 to P4, we violate the mutex constraint between A6 and A8.
An explanation of failure for a search node is a set of constraints from which False can be derived.
The complete explanation for this failure can thus be stated as:

P2 = A6 ^ P4 = A8 ^ (P2 = A6 ) P4 6= A8 )
Of this, the part P2 = A6 ) P4 6= A8 can be stripped from the explanation since the mutual
exclusion relation will hold as long as we are solving this particular problem with these particular
actions. Further, we can take a cue from the conflict directed backjumping algorithm (Prosser,
1993), and represent the remaining explanation compactly in terms of “conflict sets.” Specifically,
whenever the search reaches a condition c (and is about to find an assignment for it), its conflict
set is initialized as fcg. Whenever one of the possible assignments to c is inconsistent (mutually
exclusive) with the current assignment of a previous variable c0 , we add c0 to the conflict set of c. In
the current example, we start with fP4 g as the conflict set of P4 , and expand it by adding P2 after
we find that A8 cannot be assigned to P4 because of the choice of A6 to support P2 . Informally,
the conflict set representation can be seen as an incrementally maintained (partial) explanation of
failure, indicating that there is a conflict between the current value of P2 and one of the possible
values of P4 (Kambhampati, 1998).
We now consider the second possible value of P4 , viz., A9 , and find that it is mutually exclusive
with A5 which is currently supporting P1 . Following our practice, we add P1 to the conflict set of
P4 . At this point, there are no further choices for P4 , and so we backtrack from P4 , passing the
conflict set of P4 , viz., fP1 ; P2 ; P4 g as the reason for its failure. In essence, the conflict set is a
shorthand notation for the following complete failure explanation (Kambhampati, 1998):6
[(

P4 = A8 ) _ (P4 = A9 )] ^ (P1 = A5 ) P4 6= A9 ) ^ (P2 = A6 ) P4 6= A8 ) ^ P1 = A5 ^ P2 = A6

It is worth noting at this point that when P4 is revisited in the future with different assignments
to the preceding variables, its conflict set will be re-initialized to fP4 g before considering any assignments to it.
The first advantage of the conflict set is that it allows a transparent way of supporting dependency directed backtracking (Kambhampati, 1998). In the current example, having failed to assign
P4 , we have to start backtracking. We do not need to do this in a chronological fashion however.
6. We strip the first (disjunctive) clause since it is present in the graph structure, and the next two implicative clauses
since they are part of the mutual exclusion relations that will not change for this problem. The conflict set representation just keeps the condition (variable) names of the last two clauses – denoting, in essence, that it is the current
assignments of the variables P1 and P2 that are causing the failure to assign P4 .

10

P LANNING G RAPH

AS A

CSP

Find-Plan(G:goals, pg : plan graph, k : level)
If k = 0, Return an empty subplan P with success.
If there is a memo M such that M  G,
Fail, and return M as the conflict set
Call Assign-goals(G; pg; k; ;).
If Assign-goals fails and returns a conflict set M ,
Store M as a memo
Regress M over actions selected at level k + 1 to get R
Fail and return R as the conflict set
If Assign-goals succeeds, and returns a k -level subplan P ,
Return P with success
Assign-goals(G:goals, pg : plan graph, k : level, A: actions)
If G = ;
Let U be the union of preconditions of the actions in A
Call Find-plan(U; pg; k , 1)
If Find-plan fails and returns a conflict set R,
Fail and return R
If Find-plan succeeds and returns a subplan P of length k , 1
Succeed and return a k length subplan P  A
Else ;;(G 6= ;)
Select a goal g 2 G
Let cs
fgg, and Ag be the set of actions from level k in pg that support g
L1:
If Ag = ;, Fail and return cs as the conflict set
Ag , a
Else, pick an action a 2 Ag , and set Ag
If a is mutually exclusive with some action b 2 A
Let l be the goal that b was selected to support
cs [ flg
Set cs
Goto L1
Else (a is not mutually exclusive with any action in A)
Call Assign-goals(G , fg g; pg; k; A [ fag)
If the call fails and returns a conflict set C
If g 2 C
Set cs = cs [ C ;conflict set absorption
Goto L1
Else ;(g 62 C )
Fail and return C as the conflict set
;dependency directed backjumping

Figure 4: A pseudo-code description of Graphplan backward search enhanced with EBL/DDB capabilities. The backward search at level k in a planning-graph pg is initiated with the call
Find-Plan(G; pg; k), where G is the set of top level goals of the problem.
11

K AMBHAMPATI

Instead, we jump back to the most recent variable (condition) taking part in the conflict set of P4 –
in this case P2 . By doing so, we are avoiding considering other alternatives at P3 , and thus avoiding
one of the inefficiencies of the standard backward search. It is easy to see that such backjumping is
sound since P3 is not causing the failure at P4 and thus re-assigning it won’t avert the failure.
Continuing along, whenever the search backtracks to a condition c, the backtrack conflict is
absorbed into the current conflict set of c. In our example, we absorb fP1 ; P2 ; P4 g into the conflict
set of P2 , which is currently fP2 g (making fP1 ; P2 ; P4 g the new conflict set of P2 ). We now assign
A11 , the only remaining value, to P2 . Next we try to assign P3 and find that its only value A7 is
mutex with A11 . Thus, we set conflict set of P3 to be fP3 ; P2 g and backtrack with this conflict
set. When the backtracking reaches P2 , this conflict set is absorbed into the current conflict set of
P2 (as described earlier), giving rise to fP1 ; P2 ; P3 ; P4 g as the current combined failure reason for
P2 . This step illustrates how the conflict set of a condition is incrementally expanded to collect the
reasons for failure of the various possible values of the condition.
At this point, P2 has no further choices, so we backtrack over P2 with its current conflict set,
fP1 ; P2 ; P3 ; P4 g. At P1 , we first absorb the conflict set fP1 ; P2 ; P3 ; P4 g into P1 ’s current conflict
set, and then re-initiate backtracking since P1 has no further choices.
Now, we have reached the end of the current level (k , 1). Any backtracking over P1 must
involve undoing assignments of the conditions at the k th level. Before we do that however, we do
two steps: memoization and regression.
4.1 Memoization
Before we backtrack over the first assigned variable at a given level, we store the conflict set of that
variable as a memo at that level. We store the conflict set fP1 ; P2 ; P3 ; P4 g of P1 as a memo at this
level. Notice that the memo we store is shorter (and thus more general) than the one stored by the
normal Graphplan, as we do not include P5 and P6 , which did not have anything to do with the
failure7
4.2 Regression
Before we backtrack out of level k , 1 to level k , we need to convert the conflict set of (the first
assigned variable in) level k , 1 so that it refers to the conditions in level k . This conversion
process involves regressing the conflict set over the actions selected at the k th level (Kambhampati,
1998). In essence, the regression step computes the (smallest) set of conditions (variables) at the
kth level whose supporting actions spawned (activated, in DCSP terms) the conditions (variables)
in the conflict set at level k , 1. In the current case, our conflict set is fP1 ; P2 ; P3 ; P4 g. We can
see that P2 , P3 are required because of the condition G1 at level k , and the condition P4 is required
because of the condition G2 .
In the case of condition P1 , both G1 and G4 are responsible for it, as both their supporting
actions needed P1 . In such cases we have two heuristics for computing the regression: (1) Prefer
choices that help the conflict set to regress to a smaller set of conditions (2) If we still have a choice
between multiple conditions at level k , we pick the one that has been assigned earlier. The motivation for the first rule is to keep the failure explanations as compact (and thus as general) as possible,
7. While in the current example, the memo includes all the conditions up to P4 (which is the farthest we have gone in
this level), even this is not always necessary. We can verify that P3 would not have been in the memo set if A11 were
not one of the supporters of P2 .

12

P LANNING G RAPH

AS A

CSP

and the motivation for the second rule is to support deeper dependency directed backtracking. It
is important to note that these heuristics are aimed at improving the performance of the EBL/DDB
and do not affect the soundness and completeness of the approach.
In the current example, the first of these rules applies, since P1 is already required by G1 , which
is also requiring P2 and P3 . Even if this was not the case (i.e., G1 only required P1 ), we still would
have selected G1 over G4 as the regression of P1 , since G1 was assigned earlier in the search.
The result of regressing fP1 ; P2 ; P3 ; P4 g over the actions at k th level is thus fG1 ; G2 g. We start
backtracking at level k with this as the conflict set. We jump back to G2 right away, since it is the
most recent variable named in the conflict set. This avoids the inefficiency of re-considering the
choices at G3 and G4 , as done by the normal backward search. At G2 , the backtrack conflict set
is absorbed, and the backtracking continues since there are no other choices. Same procedure is
repeated at G1 . At this point, we are once again at the end of a level–and we memoize fG1 ; G2 g
as the memo at level k . Since there are no other levels to backtrack to, Graphplan is called on to
extend the planning-graph by one more level.
Notice that the memos based on EBL analysis capture failures that may require a significant
amount of search to rediscover. In our example, we are able to discover that fG1 ; G2 g is a failing
goal set despite the fact that there are no mutex relations between the choices of the goals G1 and
G2 .
4.3 Using the Memos
Before we end this section, there are a couple of observations regarding the use of the stored memos.
In the standard Graphplan, memos at each level are stored in a level-specific hash table. Whenever
backward search reaches a level k with a set of conditions to be satisfied, it consults the hash table
to see if this exact set of conditions is stored as a memo. Search is terminated only if an exact hit
occurs. Since EBL analysis allows us to store compact memos, it is not likely that a complete goal
set at some level k is going to exactly match a stored memo. What is more likely is that a stored
memo is a subset of the goal set at level k (which is sufficient to declare that goal set a failure).
In other words, the memo checking routine in Graphplan needs to be modified so that it checks to
see if some subset of the current goal set is stored as a memo. The naive way of doing it – which
involves enumerating all the subsets of the current goal set and checking if any of them are in the
hash table, turns out to be very costly. One needs more efficient data structures, such as the setenumeration trees (Rymon, 1992). Indeed, Koehler and her co-workers (Koehler, Nebel, Hoffman,
& Dimopoulos, 1997) have developed a data structure called UB-Trees for storing the memos. The
UB-Tree structures can be seen as a specialized version of the “set-enumeration trees,” and they can
efficiently check if any subset of the current goal set has been stored as a memo.
The second observation regarding memos is that they can often serve as a failure explanation
in themselves. Suppose we are at some level k , and find that the goal set at this level subsumes
some stored memo M . We can then use M as the failure explanation for this level, and regress it
back to the previous level. Such a process can provide us with valuable opportunities for further
back jumping at levels above k . It also allows us to learn new compact memos at those levels. Note
that none of this would have been possible with normal memos stored by Graphplan, as the only
way a memo can declare a goal set at level k as failing is if the memo is exactly equal to the goal
set. In such a case regression will just get us all the goals at level k + 1, and does not buy us any
backjumping or learning power (Kambhampati, 1998).

13

K AMBHAMPATI

5. Empirical Evaluation of the Effectiveness of EBL/DDB
We have now seen the way EBL and DDB capabilities are added to the backward search by maintaining and updating conflict-sets. We also noted that EBL and DDB capabilities avoid a variety
of inefficiencies in the standard Graphplan backward search. That these augmentations are soundness and completeness preserving follows from the corresponding properties of conflict-directed
backjumping (Kambhampati, 1998). The remaining (million-dollar) question is whether these capabilities make a difference in practice. I now present a set of empirical results to answer this
question.
I implemented the EBL/DDB approach described in the previous section on top of a Graphplan
implementation in Lisp.8 The changes needed to the code to add EBL/DDB capability were relatively minor – only two functions needed non-trivial changes9 . I also added the UB-Tree subset
memo checking code described in (Koehler et al., 1997). I then ran several comparative experiments
on the “benchmark” problems from (Kautz & Selman, 1996), as well as from four other domains.
The specific domains included blocks world, rocket world, logistics domain, gripper domain, ferry
domain, traveling salesperson domain, and towers of hanoi. Some of these domains, including the
blocks world, the logistics domain and the gripper domain were used in the recent AI Planning
Systems competition. The specifications of the problems as well as domains are publicly available.
Table 1 shows the statistics on the times taken and number of backtracks made by normal Graphplan, and Graphplan with EBL/DDB capabilities.10
5.1 Run-Time Improvement
The first thing we note is that EBL/DDB techniques can offer quite dramatic speedups – from 1.6x
in blocks world all the way to 120x in the logistics domain (the Att-log-a problem is unsolvable by
normal Graphplan after over 40 hours of cpu time!). We also note that the number of backtracks
reduces significantly and consistently with EBL/DDB. Given the lengh of some of the runs, the time
Lisp spends doing garbage collection becomes an important issue. I thus report the cumulative time
(including cpu time and garbage collection time) for Graphplan with EBL/DDB, while I separate
the cpu time from cumulative time for the plain Graphplan (in cases where the total time spent
was large enough that garbage collection time is a significant fraction). Specifically, there are two
entrys in the column corresponding to total time for the normal Graphplan. The first entry is the
cpu time spent, while the second entry in parenthesis is the cumulative time (cpu time and garbage
collection time) spent. The speedup is computed with respect to the cumulative time of Graphplan
with EBL/DDB and cpu time of plain Graphplan. 11 The reported speedups should thus be seen as
conservative estimates.
8. The original lisp implementation of Graphplan was done by Mark Peot. The implementation was subsequently
improved by David Smith.
9. Assign-goals and find-plan
10. In the earlier versions of this paper, including the paper presented at IJCAI (Kambhampati, 1999) I have reported
experiments on a Sun SPARC Ultra 1 running Allegro Common Lisp 4.3. The Linux machine run-time statistics
seem to be approximately 2.7x faster than those from the Sparc machine.
11. It is interesting to note that the percentage of time spent doing garbage collection is highly problem dependent. For
example, in the case of Att-log-a, only 30 minutes out of 41 hours (or about 1% of the cumulative time) was spent
doing garbage collection, while in the case of Tower-6, 3.1 hours out of 4.8 hours (or about 65% of the cumulative
time) was spent on garbage collection!

14

Problem

Speedup
1.7x
1.8x
24x
17x
>1215x
11x
90x
>10x
42x
>40x
50x
37x
>25x
90x
>58x

Table 1: Empirical performance of EBL/DDB. Unless otherwise noted, times are in cpu minutes on a Pentium-III 500 MHZ machine with
256meg RA running Linux and allegro common lisp 5, compiled for speed. “Tt” is total time, “Mt” is the time used in checking
memos and “Btks” is the number of backtracks done during search. The times for Graphplan with EBL/DDB include both the cpu
and garbage collection time, while the cpu time is separated from the total time in the case of normal Graphplan. The numbers in
parentheses next to the problem names list the number of time steps and number of actions respectively in the solution. AvLn and
AvFM denote the average memo length and average number of failures detected per stored memo respectively.

CSP

AvFM
1.26
1.13
3.2
3.22
4.9
2.2
2.3
2.4
5
-

AS A

Normal Graphplan
Tt. Mt.
# Btks AvLn
5.3 0.22
5181K
11.3
4.15 0.05
2823K 11.83
19.43 11.7
8128K
23.9
14.1
7.7 10434K
23.8
>40.5hr (>41hr)
32
1.1
.39
2802K
14.9
215(272)
17.8
>8.2hr(>16hr)
7.23 1.27 19070K
20.9
>1.7hr (>4.8hr)
22.3
22(29)
11 33357K
24.5
42(144)
24 53233K
25
>5hr(>18.4hr)
89(93) 56.7 68648K
13
>12hr (>14.5hr)
-

P LANNING G RAPH

15

Huge-Fact (18/18)
BW-Large-B (18/18)
Rocket-ext-a (7/36)
Rocket-ext-b (7/36)
Att-log-a(11/79)
Gripper-6 (11/17)
Gripper-8 (15/23)
Gripper-10(19/29)
Tower-5 (31/31)
Tower-6 (63/63)
Ferry-41 (27/27)
Ferry-5 (31/31)
Ferry-6(39/39)
Tsp-10 (10/10)
Tsp-12(12/12)

Graphplan with EBL/DDB
Tt
Mt
# Btks AvLn AvFM
3.08 0.28
2004K
9.52
2.52
2.27 0.11
798K 10.15
3.32
.8
.34
764K
8.5
82
.8
.43
569K
7.5
101
1.97
.89
2186K
8.21 46.18
0.1 0.03
201K
6.9
6.2
2.4
.93
4426K
9
7.64
47.9 18.2 61373K 11.05
8.3
.17 0.02
277K
6.7
2.7
2.53 0.22
4098K
7.9
2.8
.44 0.13
723K
7.9
2.54
1.13
.41
1993K
8.8
2.53
11.62
5.3 18318K
10.9
2.6
.99 0.23
2232K
6.9
12
12.4 2.65 21482K
7.9
15.2

K AMBHAMPATI

5.2 Reduction in Memo Length
The results also highlight the fact that the speedups offered by EBL/DDB are problem/domain
dependent – they are quite meager in blocks world problems, and are quite dramatic in many other
domains including the rocket world, logistics, ferry, gripper, TSP and Hanoi domains. The statistics
on the memos, shown in Table 1 shed light on the reasons for this variation. Of particular interest
is the average length of the stored memos (given in the columns labeled “AvLn”). In general, we
expect that the EBL analysis reduces the length of stored memos, as conditions that are not part of
the failure explanation are not stored in the memo. However, the advantage of this depends on the
likelihood that only a small subset of the goals at a given level are actually taking part in the failure.
This likelihood in turn depends on the amount of inter-dependencies between the goals at a given
level. From the table, we note that the average length reduces quite dramatically in the rocket world
and logistics12 , while the reduction is much less pronounced in the blocks world. This variation can
be traced back to a larger degree of inter-dependency between goals at a given level in the blocks
world problems.
The reduction in average memo length is correlated perfectly with the speedups offered by EBL
on the corresponding problems. Let me put this in perspective. The fact that the average length
of memos for Rocket-ext-a problem is 8.5 with EBL and 24 without
EBL, shows in essence that
, 
normal Graphplan is re-discovering an 8-sized failure embedded in 24
8 possible ways in the worst
case in a 24 sized goal set – storing a new memo each time (incurring both increased backtracking
and matching costs)! It is thus no wonder that normal Graphplan performs badly compared to
Graphplan with EBL/DDB.
5.3 Utility of Stored Memos
The statistics in Table 1 also show the increased utility of the memos stored by Graphplan with
EBL/DDB. Since EBL/DDB store more general (smaller) memos than normal Graphplan, they
should, in theory, generate fewer memos and use them more often. The columns labeled “AvFM”
give the ratio of the number of failures discovered through the use of memos to the number of memos
generated in the first place. This can be seen as a measure of the average “utility” of the stored
memos. We note that the utility is consistently higher with EBL/DDB. As an example, in Rocketext-b, we see that on the average an EBL/DDB generated memo was used to discover failures 101
times, while the number was only 3.2 for the memos generated by the normal Graphplan.13
5.4 Relative Utility of EBL vs. DDB
From the statistics in Table 1, we see that even though EBL can make significant improvements in
run-time, a significant fraction of the run time with EBL (as well as normal Graphplan) is spent in
memo checking. This raises the possibility that the overall savings are mostly from the DDB part
and that the EBL part (i.e, the part involving storing and checking memos) is in fact a net drain
(Kambhampati, Katukam, & Qu, 1997). To see if this is true, I ran some problems with EBL (i.e.,
memo-checking) disabled. The DDB capability as well as the standard Graphplan memoization
12. For the case of Att-log-a, I took the memo statistics by interrupting the search after about 6 hours
13. The statistics for Att-log-aseem to suggest that memo usage was not as bad for normal Graphplan. However, it should
be noted that Att-log-a was not solved by normal Graphplan to begin with. The improved usage factor may be due
mostly to the fact that the search went for a considerably longer time, giving Graphplan more opportunity to use its
memos.

16

P LANNING G RAPH

Problem
Att-log-a
Tower-6
Rocket-ext-a
Gripper-8
TSP-10
Huge-Fct

EBL+DDB
Btks
Time
2186K 1.95
4098K 2.37
764K
.83
4426K 2.43
2238K
1.1
2004K 3.21

AS A

CSP

DDB
Btks
Time
115421K 235
97395K
121
3764K
17.18
5426K
4.71
4308K
2.3
2465K
3.83

Speedup
120x
51x
21x
1.94x
2.09x
1.19x

Table 2: Utility of storing and using EBL memos over just doing DDB
strategies were left in.14 The results are shown in Table 2, and demonstrate that the ability to store
smaller memos (as afforded by EBL) is quite helpful–giving rise to 120x speedup over DDB alone
in the Att-log-a problem, and 50x speedup in Tower-6 problem. Of course, the results also show that
DDB is an important capability in itself. Indeed, Att-log-aand tower-6 could not even be solved by
the standard Graphplan, while with DDB, these problems become solvable. In summary, the results
show that both EBL and DDB can have a net positive utility.
5.5 Utility of Memoization
Another minor, but not well-recognized, point brought out by the statistics in Table 1 is that the
memo checking can sometimes be a significant fraction of the run-time of standard Graphplan. For
example, in the case of Rocket-ext-a, standard Graphplan takes 19.4 minutes of which 11.7 minutes,
or over half the time, is spent in memo checking (in hash tables)! This raises the possibility that if
we just disable the memoization, perhaps we can do just as well as the version with EBL/DDB. To
see if this is the case, I ran some of the problems with memoization disabled. The results show that
in general disabling memo-checking leads to worsened performance. While I came across some
cases where the disablement reduces the overall run-time, the run-time is still much higher than
what you get with EBL/DDB. As an example, in the case of Rocket-ext-a, if we disable the memo
checking completely, Graphplan takes 16.5 minutes, which while lower than the 19.4 minutes taken
by standard Graphplan, is still much higher than the .8 minutes taken by the version of Graphplan
with EBL/DDB capabilities added. If we add DDB capability, while still disabling the memochecking, the run time becomes 2.4 minutes, which is still 3 times higher than that afforded with
EBL capability.
5.6 The C vs. Lisp Question
Given that most existing implementations of Graphplan are done in C with many optimizations,
one nagging doubt is whether the dramatic speedups due to EBL/DDB are somehow dependent
on the moderately optimized Lisp implementation I have used in my experiments. Thankfully, the
EBL/DDB techniques described in this paper have also been (re)implemented by Maria Fox and
Derek Long on their STAN system. STAN is a highly optimized implementation of Graphplan
that fared well in the recent AIPS planning competition. They have found that EBL/DDB resulted
in similar dramatic speedups on their system too (Fox, 1998; Fox & Long, 1999). For example,
14. I also considered removing the memoization completely, but the results were even poorer.

17

K AMBHAMPATI

they were unable to solve Att-log-a with plain Graphplan, but could solve it easily with EBL/DDB
added.
Finally, it is worth pointing out that even with EBL/DDB capabilities, I was unable to solve
some larger problems in the AT&T benchmarks, such as bw-large-c and Att-log-b. This is however
not an indictment against EBL/DDB since to my knowledge the only planners that solved these
problems have all used either local search strategies such as GSAT, randomized re-start strategies,
or have used additional domain-specific knowledge and pre-processing. At the very least, I am not
aware of any existing implementations of Graphplan that solve these problems.

6. On the Utility of Graphplan Memos
One important issue in using EBL is managing the costs of storage and matching. Indeed, as discussed in (Kambhampati, 1998), naive implementations of EBL/DDB are known to lose the gains
made in pruning power in the matching and storage costs. Consequently, several techniques have
been invented to reduce these costs through selective learning as well as selective forgetting. It is
interesting to see why these costs have not been as prominent an issue for EBL/DDB on Graphplan.
I think this is mostly because of two characteristics of Graphplan memoization strategy:
1. Graphplan’s memoization strategy provides a very compact representation for no-goods, as
well as a very selective strategy for remembering no-goods. Seen as DCSP, it only remembers
subsets of activated variables that do not have a satisfying assignment. Seen as a CSP (c.f.
Figure 2), Graphplan only remembers no-goods of the form

P1i 6=? ^P2i 6=?    Pmi 6=?
(where the superscripts correspond to the level of the planning graph to which the proposition
belongs), while normal EBL implementations learn no-goods of the form

P1i = a1 ^ P2j = a2    Pmk = am
Suppose a planning graph contains n propositions divided into l levels, and each proposition
P at level j has at most d actions supporting it. A CSP compilation of the planning graph will
have n variables, each with d + 1 values (the extra one for ?). A normal EBL implementation
for such a CSP can learn, in the worst case, (d + 2)n no-goods.15 In contrast, Graphplan
n
remembers only l  2 l memos16 –a very dramatic reduction. This reduction is a result of two
factors:
(a) Each individual memo stored by Graphplan corresponds to an exponentially large set of
normal no-goods (the memo

P1i 6=? ^P2i 6=?    Pmi 6=?
is a shorthand notation for the conjunction of dm no-goods corresponding to all possible
i)
non- ? assignments to P1i    Pm

15. Each variable v may either not be present in a no-good, or be present with one of d
d possibilities for each of n variables.
16. At each level, each of nl propositions either occurs in a memo or does not occur

+1

18

+ 1 possible assignments–giving

P LANNING G RAPH

AS A

CSP

(b) Memos only subsume no-goods made up of proposition variables from the same planning graph level.
2. The matching cost is reduced by both the fact that considerably fewer no-goods are ever
learned, and the fact that Graphplan stores no-goods (memos) separately for each level, and
only consults the memos stored at level j , while doing backwards search at a level j ,
The above discussion throws some light on why the so-called “EBL utility” problem is not as
critical for Graphplan as it is for EBL done on normal CSPs.
6.1 Scenarios Where Memoization is too Conservative to Avoid Rediscovery of the Same
Failures
The discussion above also raises the possibility that Graphplan (even with EBL/DDB) memoization
is too conservative and may be losing some useful learning opportunities only because they are not
in the required syntactic form. Specifically, before Graphplan can learn a memo of the form

P1i 6=? ^P2i 6=?    Pmi 6=?;
it must be the case that each of the dm possible assignments to the m propositional variables must
be a no-good. Even if one of them is not a no-good, Graphplan avoids learning the memo, thus
potentially repeating the failing searches at a later time (although the loss is made up to some extent
by learning several memos at a lower level).
i    P i at some
Consider for example the following scenario: we have a set of variables P1i    Pm
n
level i that are being assigned by backward search. Suppose the search has found a legal partial asi , and the domain of P i contains the k values fv1    vk g. In
signment for the variables P1i    Pm
m
,1
i
trying to assign the variables Pm    Pni , suppose we repeatedly fail and backtrack up to the variable
Pmi , re-assigning it and eventually settling at the value v7. At this point once again backtracking
i to higher level variables (P i    P i ) and re-assigning
occurs, but this time we backtrack over Pm
m
1
them. At this point, it would have been useful to remember some no-goods to the effect that none
i are going to work so all that backtracking does not have to be repeated.
of the first 6 values of Pm
Such no-goods will take the form:

Pmi = vj ^ Pmi +1 6=? ^Pmi +2 6=?    Pni 6=?
i that were tried and found to lead to failure while
where j ranges over 1    6, for all the values of Pm

assigning the later variables. Unfortunately, such no-goods are not in the syntactic form of memos
and so the memoization procedure cannot remember them. The search is thus forced to rediscover
the same failures.
6.2 Sticky Values as a Partial Antidote
One way of staying with the standard memoization, but avoiding rediscovery of the failing search
paths, such as those in the case of the example above, is to use the “sticky values” heuristic (Frost
& Dechter, 1994; Kambhampati, 1998). This involves remembering the current value of a variable
while skipping over it during DDB, and trying that value first when the search comes back to that
variable. The heuristic is motivated by the fact that when we skip over a variable during DDB, it
means that the variable and its current assignment have not contributed to the failure that caused
19

K AMBHAMPATI

the backtracking–so it makes sense to restore this value upon re-visit. In the example above, this
i when we backtracked over it, and tries
heuristic will remember that v7 was the current value of Pm
that as the first value when it is re-visited. A variation on this technique is to re-arrange or fold the
domain of the variable such that all the values that precede the current value are sent to the back
of the domain, so that these values will be tried only if other previously untried values are found to
fail. This makes the assumption that the values that led to failure once are likely to do so again. In
i so it becomes fv7 ; v8    vk ; v1 ; v2    v6 g.
the example above, this heuristic folds the domain of Pm
Notice that both these heuristics make sense only if we employ DDB, as otherwise we will never
skip over any variable during backtracking.
I implemented both sticky value heuristics on top of EBL/DDB for Graphplan. The statistics
in Table 3 show the results of experiments with this extension. As can be seen, the sticky values
approach is able to give up to 4.6x additional speedup over EBL/DDB depending on the problem.
Further, while the folding heuristic dominates the simple version in terms of number of backtracks,
the difference is quite small in terms of run-time.

7. Forward Checking & Dynamic Variable Ordering
DDB and EBL are considered “look-back” techniques in that they analyze the failures by looking
back at the past variables that may have played a part in those failures. There is a different class
of techniques known as “look-forward” techniques for improving search. Prominent among these
latter are forward checking and dynamic variable ordering. Supporting forward checking involves
filtering out the conflicting actions from the domains of the remaining goals, as soon as a particular
goal is assigned. In the example in Figure 1, forward checking will filter A9 from the domain of P4
as soon as P1 is assigned A5 . Dynamic variable ordering (DVO) involves selecting for assignment
the goal that has the least number of remaining establishers.17 When DVO is combined with forward checking, the variables are ordered according to their “live” domain sizes (where live domain
is comprised of values from the domain that are not yet pruned by forward checking). Our experiments18 show that these techniques can bring about reasonable, albeit non-dramatic, improvements
in Graphplan’s performance. Table 4 shows the statistics for some benchmark problems, with dynamic variable ordering alone, and with forward checking and dynamic variable ordering. We note
that while the backtracks reduce by up to 3.6x in the case of dynamic variable ordering, and 5x in the
case of dynamic variable ordering and forward checking, the speedups in time are somewhat smaller,
ranging only from 1.1x to 4.8x. Times can perhaps be improved further with a more efficient implementation of forward checking.19 The results also seem to suggest that no amount of optimization
is going to make dynamic variable ordering and forward checking competitive with EBL/DDB on
other problems. For one thing, there are several problems, including Att-log-a, Tsp-12, Ferry-6 etc.
which just could not be solved even with forward checking and dynamic variable ordering. Second,
even on the problems that could be solved, the reduction in backtracks provided by EBL/DDB is far
greater than that provided by FC/DVO strategies. For example, on Tsp-10, the FC/DVO strategies
17. I have also experimented with a variation of this heuristic, known as the Brelaz heuristic (Gomes et al., 1998), where
the ties among variables with the same sized live-domains are broken by picking variables that take part in most
number of constraints. This variation did not however lead to any appreciable improvement in performance.
18. The study of forward checking and dynamic variable ordering was initiated with Dan Weld.
19. My current implementation physically removes the pruned values of a variable during forward checking phase, and
restores values on backtracks. There are better implementations, including use of in/out flags on the values as well as
use of indexed arrays (c.f. (Bacchus & van Run, 1995))

20

21

EBL/DDB+Sticky
Btks
Speedup
372K
2.2x(2.05x)
172K
4.6x(3.3x)
56212K 1.29x(1.09x)
18151K .99x(1.01x)
20948K 1.26x(1.02x)
1144K
2x(1.91x)

EBL/DDB+Sticky+Fold
Time
Btks
Speedup
.33
347K
2.4x (2.2x)
.177
169K
4.5x(3.36x)
40.8 54975K 1.17x(1.12x)
11.87 18151K .97x(1.01x)
10.18 20948K 1.22x(1.02x)
.67
781K
2.9x(2.8x)

Table 3: Utility of using sticky values along with EBL/DDB.

CSP

Time
.37
.18
36.9
11.75
9.86
.95

AS A

Rocket-ext-a(7/36)
Rocket-ext-b(7/36)
Gripper-10(39/39)
Ferry-6
TSP-12(12/12)
Att-log-a(11/79)

Plain EBL/DDB
Time
Btks
.8
764K
.8
569K
47.95 61373K
11.62 18318K
12.44 21482K
1.95
2186K

P LANNING G RAPH

Problem

K AMBHAMPATI

Problem
Huge-fact (18/18)
BW-Large-B (18/18)
Rocket-ext-a (7/36)
Rocket-ext-b (7/36)
Att-log-a(11/79)
Gripper-6(11/17)
Tsp-10(10/10)
Tower-6(63/63)

GP
5.3(5181K)
4.15(2823K)
19.43(8128K)
14.1(10434K)
>10hr
1.1(2802K)
89(69974K)
>10hr

GP+DVO
2.26 (1411K)
3.14(1416K)
14.9(5252K)
7.91(4382K)
>10hr
.65(1107K)
78(37654K)
>10hr

Speedup
2.3x(3.6x)
1.3x(2x)
1.3x(1.5x)
1.8x(2.4x)
1.7x(2.5x)
1.14x(1.9x)
-

GP+DVO+FC
3.59 (1024K)
4.78(949K)
14.5(1877K)
6(1490K)
>10hr.
.73 (740K)
81(14697K)
>10hr.

Speedup
1.47x(5x)
.86(3x)
1.3x(4.3x)
2.4x(7x)
1.5x(3.7x)
1.09x(4.8x)

Table 4: Impact of forward checking and dynamic variable ordering routines on Graphplan. Times
are in cpu minutes as measured on a 500 MHZ Pentium-III running Linux and Franz
Allegro Common Lisp 5. The numbers in parentheses next to times are the number of
backtracks. The speedup columns report two factors–the first is the speedup in time, and
the second is the speedup in terms of number of backtracks. While FC and DVO tend to
reduce the number of backtracks, the reduction does not always seem to show up in the
time savings.

reduce number of backtracks from 69974K to 14697K, a 4.8x improvement. However, this pales
in comparison to 2232K backtracks (or 31x improvement) given by by EBL/DDB (see the entry in
Table 1). Notice that these results only say that variable ordering strategies do not make a dramatic
difference for Graphplan’s backward search (or a DCSP compilation of the planning graph); they do
not make any claims about the utility of FC and DVO for a CSP compilation of the planning graph.
7.1 Complementing EBL/DDB with Forward Checking and Dynamic Variable Ordering
Although forward checking and dynamic variable ordering approaches were not found to be particularly effective in isolation for Graphplan’s backward search, I thought that it would be interesting
to revisit them in the context of a Graphplan enhanced with EBL/DDB strategies. Part of the original reasoning underlying the expectation that goal (variable) ordering will not have a significant
effect on Graphplan performance is based on the fact that all the failing goal sets are stored in-toto
as memos (Blum & Furst, 1997, pp. 290). This reason no longer holds when we use EBL/DDB.
Further more, there exists some difference of opinion as to whether or not forward checking and
DDB can fruitfully co-exist. The results of (Prosser, 1993) suggest that domain-filtering–such as
the one afforded by forward checking, degrades intelligent backtracking. The more recent work
(Frost & Dechter, 1994; Bayardo & Schrag, 1997) however seems to suggest however that best CSP
algorithms should have both capabilities.
While adding plain DVO capability on top of EBL/DDB presents no difficulties, adding forward
checking does require some changes to the algorithm in Figure 4. The difficulty arises because a
failure may have occurred as a combined effect of the forward checking and backtracking. For
example, suppose we have four variables v1    v4 that are being considered for assignment in that
order. Suppose v3 has the domain f1; 2; 3g, and v3 cannot be 1 if v1 is a, and cannot be 2 if v2 is
b. Suppose further that v4’s domain only contains d, and there is a constraint saying that v4 can’t
22

P LANNING G RAPH

Problem
Huge-fct
BW-Large-B
Rocket-ext-a
Rocket-ext-b
Att-log-a
Tower-6
TSP-10

EBL
Time(btks)
3.08(2004K)
2.27(798K)
.8(764K)
.8(569K)
1.97(2186K)
2.53(4098K)
.99(2232K)

AS A

CSP

EBL+DVO
Time(btks)
Speedup
1.51(745K)
2x(2.68x)
1.81(514K)
1.25x(1.6x)
.4(242K)
2x(3.2x)
.29(151K)
2.75x(3.76x)
2.59(1109K) .76x(1.97x)
3.78(3396K)
.67x(1.2x)
1.27(1793K) .77x(1.24x)

EBL+FC+DVO
Time(Btks)
Speedup
2.57(404K)
1.2x(5x)
2.98(333K) .76x(2.39x)
.73(273K)
1.09x(2.8x)
.72(195K)
1.1x(2.9x)
3.98(1134K) .5x(1.92x)
2.09(636K)
1.2x(6.4x)
1.34(828K)
.73x(2.7x)

Table 5: Effect of complementing EBL/DDB with dynamic variable ordering and forward checking
strategies. The speedup columns report two factors–the first is the speedup in time, and
the second is the speedup in terms of number of backtracks. While FC and DVO tend to
reduce the number of backtracks, the reduction does not always seem to show up in the
time savings.

be d if v1 is a and v3 is 3. Suppose we are using forward checking, and have assigned v1 ; v2 the
values a and b. Forward checking prunes 1 and 2 from v3 ’s domain, leaving only the value 3. At
this point, we try to assign v4 and fail. If we use the algorithm in Figure 4, the conflict set for v4
would be fv4 ; v3 ; v1 g, as the constraint that is violated is v1 = a ^ v3 = 3 ^ v4 = d. However this
is not sufficient since the failure at v4 may not have occurred if forward checking had not stripped
the value 2 from the domain of v3 . This problem can be handled by pushing v1 and v2 , the variables
whose assignment stripped some values from v3 , into v3 ’s conflict set.20 Specifically, the conflict
set of every variable v is initialized to fv g to begin with, and whenever v loses a value during
forward checking with respect to the assignment of v 0 , v 0 is added to the conflict set of v . Whenever
a future variable (such as v4 ) conflicts with v3 , we add the conflict set of v3 (rather than just v3 ) to
the conflict set of v4 . Specifically the line
“Set cs = cs [ f l g”
in the procedure in Figure 4 is replaced with the line
“Set cs = cs [ Conflict-set(l)”
I have incorporated the above changes into my implementation, so it can support support forward checking, dynamic variable ordering as well as EBL on Graphplan. Table 5 shows the performance of this version on the experimental test suite. As can be seen from the numbers, the number
of backtracks are reduced by up to 3.7x in the case of EBL+DVO, and up to 5x in the case of
EBL+FC+DVO. The cpu time improvements are somewhat lower. While we got up to 2.7x speedup
20. Notice that it is possible that the values that were stripped off from v3 ’s domain may not have had any impact on the
failure to assign v4 . For example, perhaps there is another constraint that says that v4 can’t be d if v3 is b, and in
that case, strictly speaking, the assignment of v2 cannot really be blamed for the failure at v4 . While this leads to
non-minimal explanations, there is no reason to expect that strict minimization of explanations is a pre-requisite for
the effectiveness of EBL/DDB; see (Kambhampati, 1998)

23

K AMBHAMPATI

with EBL+DVO, and up to 1.2x speedup with EBL+FC+DVO, in several cases, the cpu times increase with FC and DVO. Once again, I attribute this to the overheads of forward checking (and to
a lesser extent, of dynamic variable ordering). Most importantly, by comparing the results in the
Tables 4 and 5, we can see that EBL/DDB capabilities are able to bring about significant speedups
even over a Graphplan implementation using FC and DVO.

8. EBL/DDB & Randomized Search
Recent years have seen increased use of randomized search strategies in planning. These include
both purely local search strategies (Gerevini, 1999; Selman, Levesque, & Mitchell, 1992) as well
as hybrid strategies that introduce a random restart scheme on top of a systematic search strategy
(Gomes et al., 1998). The BLACKBOX planning system (Kautz & Selman, 1999) supports a variety
of random restart strategies on top of a SAT compilation of the planning graph, and empirical
studies show that these strategies can, probabilistically speaking, scale up much better than purely
systematic search strategies.
I wanted to investigate if (and by how much) EBL & DDB techniques will help Graphplan
even in the presence of these newer search strategies. While EBL and DDB techniques have little
applicability to purely local search strategies, they could in theory help random restart systematic
search strategies. Random restart strategies are motivated by an attempt to exploit the “heavytail” distribution (Gomes et al., 1998) of the solution nodes in the search trees of many problems.
Intuitively, in problems where there are a non-trivial percentage of very easy to find solutions as
well as very hard to find solutions, it makes sense to restart the search when we find that we are
spending too much effort for a solution. By restarting this way, we hope to (probabilistically) hit on
the easier-to-find solutions.
I implemented a random-restart strategy on top of Graphplan by making the following simple
modifications to the backward search:
1. We keep track of the number of times the backward search backtracks from one level of the
plan graph to a previous level (a level closer to the goal state), and whenever this number
exceeds a given limit (called backtrack limit), the search is restarted (by going back to the last
level of the plan graph), assuming that the number of restarts has not also exceeded the given
limit. The search process between any two restarts is referred to as an epoch.
2. The supporting actions (values) for a proposition variable are considered in a randomized
order. It is this randomization that ensures that when the search is restarted, we will look at
the values of each variable in a different order.21
Notice that random-restart strategy still allows the application of EBL and DDB strategies, since
during any given epoch, the behavior of the search is identical to that of the standard backward
search algorithm. Indeed, as the backtrack limit and the number of restarts are made larger and
larger, the whole search becomes identical to standard backward search.
21. Reordering values of a variable doesn’t make a whole lot of sense in BLACKBOX which is based on SAT encodings
and thus has only boolean variables. Thus, the randomization in BLACKBOX is done on the order in which the goals
are considered for assignment. This typically tends to clash with the built-in goal ordering strategies (such as DVO
and SAT-Z (Li & Anbulagan, 1997)), and they get around this conflict by breaking ties among variables randomly.
To avoid such clashes, I decided to randomize Graphplan by reordering values of a variable. I also picked inter-level
backtracks as a more natural parameter characterizing the difficulty of a problem for Graphplan’s backward search.

24

Problem

%sol
2%
11%
54%
13%
94%
0%
0%
3%
2%
2%
58%
90%
100%

Normal Graphplan
Length
Time Av. MFSL
19(103)
.21
.3K(3.7K)
17.6(100.5) 1.29
3.7K(41K)
25.6(136)
3
4K(78K)
18(97.5)
3
31K(361K)
22.1(119.3)
31
33K(489K)
.2K(4K)
2.6K(53K)
28(156)
4
5K(111K)
26.5(135)
.75
.4K(8K)
29(152)
4
3.7K(111K)
21.24(87.3)
2
.2K(4K)
21.3(85)
8.1
2.3K(43K)
15.3(62.5)
45
35K(403K)

Table 6: Effect of EBL/DDB on random-restart Graphplan. Time is measured in cpu minutes on Allegro Common Lisp 5.0 running on a
Linux 500MHZ Pentium machine. The numbers next to the problem names are the number of steps and actions in the shortest
plans reported for those problems in the literature. The R/B/L parameters in the second column refer to the limits on the number
of restarts, number of backtracks and the number levels to which the plan graph is expanded. All the statistics are averaged over
multiple runs (typically 100 or 50). The “MFSL” column gives the average number of memo-based failures per searched level of
the plan graph. The numbers in parentheses are the total number of memo-based failures averaged over all runs. Plan lengths were
averaged only over the successful runs.

CSP

Graphplan with EBL/DDB
Length
Time Av. MFSL
14(82)
.41
4.6K(28K)
11.3(69.5)
.72
17.8K(59K)
11.3(69.5)
.72
17.8K(59K)
11(68.5)
2.38 73K(220K)
11(68.5)
2.38 73K(220K)
18.1(101)
1.62
8K(93K)
17.3(98)
11.4 69K(717K)
20.1(109)
15.3 74K(896K)
22.85(124) 2.77
8K(145K)
19.9(110)
14
71K(848K)
7.76(35.8)
1.3
29K(109K)
7(34.1)
1.32 38K(115K)
7(34.2)
1.21 35K(105K)

AS A

%sol
99%
100%
100%
100%
100%
17%
60%
100%
55%
100%
100%
100%
100%

P LANNING G RAPH

25

Att-log-a(11/54)
Att-log-a(11/54)
Att-log-a(11/54)
Att-log-a(11/54)
Att-log-a(11/54)
Att-log-b(13/47)
Att-log-b(13/47)
Att-log-b(13/47)
Att-log-c(13/65)
Att-log-c(13/65)
Rocket-ext-a(7/34)
Rocket-ext-a(7/34)
Rocket-ext-a(7/34)

Parameters
R/B/L
5/50/20
10/100/20
10/100/30
20/200/20
20/200/30
5/50/20
10/100/20
10/100/30
5/50/30
10/100/30
10/100/30
20/200/30
40/400/30

K AMBHAMPATI

To check if my intuitions about the effectiveness of EBL/DDB in randomized search were indeed correct, I conducted an empirical investigation comparing the performance of random search
on standard Graphplan as well as Graphplan with EBL/DDB capabilities. Since the search is randomized, each problem is solved multiple number of times (100 times in most cases), and the runtime, plan length and other statistics were averaged over all the runs. The experiments are conducted
with a given backtrack limit, a given restart limit, as well as a limit on the number of levels to which
the planning graph is extended. This last one is needed as in randomized search, a solution may be
missed at the first level it appears, leading to a prolonged extension of the planning graph until a
(inoptimal) solution is found at a later level. When the limit on the number of levels is expanded,
the probability of finding solution increases, but at the same time, the cpu time spent searching the
graph also increases.
Having implemented this random restart search, the first thing I noticed is an improvement in
the solvability horizon (as expected, given the results in (Gomes et al., 1998)). Table 6 shows these
results. One important point to note is that the results in the table above talk about average plan
lengths and cpu times. This is needed as due to randomization potentially each run can produce
a different outcome (plan). Secondly, while Graphplan with systematic search guarantees shortest
plans (measured in the number of steps), the randomized search will not have such a guarantee.
In particular, the randomized version might consider a particular planning graph to be barren of
solutions, based simply on the fact that no solution could be found within the confines of the given
backtrack limit and number of restarts.
Graphplan, with or without EBL/DDB, is more likely to solve larger problems with randomized
search strategies. For example, in the logistics domain, only the Att-log-a problem was solvable
(within 24 hours real time) with EBL and systematic search. With the randomization added, my
implementation was able to solve both Att-log-b and Att-log-c quite frequently. As the limits on the
number of restarts, backtracks and levels is increased, the likelihood of finding a solution as well as
the average length of the solution found improves. For example, Graphplan with EBL/DDB is able
to solve Att-log-b in every trial for 10 restarts, 100 backtracks and 30 levels as the limits (although
the plans are quite inoptimal).
The next, and perhaps more interesting, question I wanted to investigate is whether EBL and
DDB will continue to be useful for Graphplan when it uses randomized search. At first blush,
it seems as if they will not be as important–after all even Graphplan with standard search may
luck out and be able to find solutions quickly in the presence of randomization. Further thought
however suggests that EBL and DDB may still be able to help Graphplan. Specifically, they can
help Graphplan in using the given backtrack limit in a more judicious fashion. To elaborate, suppose
the random restart search is being conducted with 100 backtracks and 10 restarts. With EBL and
DDB, Graphplan is able to pinpoint the cause of the failure more accurately than without EBL and
DDB. This means that when the search backtracks, the chance that it will have to backtrack again
for the same (or similar) reasons is reduced. This in turn gives the search more of a chance on
catching a success during one of the number of epochs allowed. All this is in addition to the more
direct benefit of being able to use the stored memos across epochs to cut down search.
As can be seen from the data in Table 6, for a given set of limits on number of restarts, number
of backtracks, and number of levels expanded, Graphplan with EBL/DDB is able to get a higher
percentage of solvability as well as significantly shorter length solutions (both in terms of levels and
in terms of actions). To get comparable results on the standard Graphplan, I had to significantly
increase the input parameters (restarts, backtracks and levels expanded), which in turn led to dra26

P LANNING G RAPH

AS A

CSP

matic increases in the average run time. For example, for the Att-log-a problem, with 5 restarts and
50 backtracks, and 20 levels limit, Graphplan was able to solve the problem 99% of the time, with
an average plan length of 14 steps and 82 actions. In contrast, without EBL/DDB, Graphplan was
able to solve the problem in only 2% of the cases, with an average plan length of 19 steps and 103
actions. If we double the restarts and backtracks, the EBL/DDB version goes to 100% solvability
with an average plan length of 11.33 steps and 69.53 actions. The standard Graphplan goes to 11%
solvability and a plan length of 17.6 steps and 100 actions. If we increase the number of levels to 30,
then the standard Graphplan solves 54% of the problems with an average plan length of 25.6 steps
and 136 actions. It takes 20 restarts and 200 backtracks, as well as a 30-level limit before standard
Graphplan is able to cross 90% solvability. By this time, the average run time is 31 minutes, and
the average plan length is 22 steps and 119 actions. The contrast between this and the 99% solvability in 0.4 minutes with 14 step 82 action plans provided by Graphplan with EBL and 5 restarts
and 50 backtracks is significant! Similar results were observed in other problems, both in logistics
(Att-log-b, Att-log-c) and other domains (Rocket-ext-a, Rocket-ext-b).
The results also show that Graphplan with EBL/DDB is able to generate and reuse memos effectively across different restart epochs. Specifically, the numbers in the columns titled “Av. MFSL”
give the average number of memo-based failures per search level.22 We note that in all cases, the
average number of memo-based failures are significantly higher for Graphplan with EBL than for
normal Graphplan. This shows that EBL/DDB analysis is helping Graphplan reduce wasted effort
significantly, and thus reap better benefits out of the given backtrack and restart limits.

9. Related Work
In their original implementation of Graphplan, Blum and Furst experimented with a variation of the
memoization strategy called “subset memoization”. In this strategy, they keep the memo generation
techniques the same, but change the way memos are used, declaring a failure when a stored memo
is found to be a subset of the current goal set. Since complete subset checking is costly, they
experimented with a “partial” subset memoization where only subsets of length n and n , 1 are
considered for an n sized goal set.
As we mentioned earlier, Koehler and her co-workers (Koehler et al., 1997) have re-visited the
subset memoization strategy, and developed a more effective solution to complete subset checking
that involves storing the memos in a data structure called UB-Tree, instead of in hash tables. The
results from their experiments with subset memoization are mixed, indicating that subset memoization does not seem to improve the cpu time performance significantly. The reason for this is quite
easy to understand – while they improved the memo checking time with the UB-Tree data structure,
they are still generating and storing the same old long memos. In contrast, the EBL/DDB extension
described here supports dependency directed backtracking, and by reducing the average length of
stored memos, increases their utility significantly, thus offering dramatic speedups.
To verify that the main source of power in the EBL/DDB-Graphplan is in the EBL/DDB part
and not in the UB-Tree based memo checking, I re-ran my experiments with EBL/DDB turned off,
22. Notice that the number of search levels may be different from (and smaller than) the number of planning graph levels,
because Graphplan initiates a search only when none of the goals are pair-wise mutex with each other. In Att-log-a,
Att-log-b and Att-log-c, this happens starting at level 9. For Rocket-ext-a it happens starting at level 5. The numbers
in parentheses are the total number of memo based failures. We divide this number by the average number of levels
in which search was conducted to get the “Av. MFSL” statistic.

27

K AMBHAMPATI

Problem
Huge-Fact
BW-Large-b
Rocket-ext-a
Rocket-ext-b
Att-log-a

Tt
3.20
2.74
19.2
7.36
> 12hrs

Mt
1
0.18
16.7
4.77
-

#Btks
2497K
1309K
6188K
7546K
-

EBL x"
1.04x
1.21x
24x
9.2x
>120x

#Gen
24243
11708
62419
61666
-

#Fail
33628
15011
269499
265579
-

AvFM
1.38
1.28
4.3
4.3
-

AvLn
11.07
11.48
24.32
24.28
-

Table 7: Performance of subset memoization with UB-Tree data structure (without EBL/DDB). The
“Tt” is the total cpu time and “Mt” is the time taken for checking memos. “#Btks” is the
number of backtracks. “EBLx” is the amount of speedup offered by EBL/DDB over subset
memoization “#Gen” lists the number of memos generated (and stored), “#Fail” lists the
number of memo-based failures, “AvFM” is the average number of failures identified per
generated memo and “AvLn” is the average length of stored memos.

but with subset memo checking with UB-Tree data structure still enabled. The results are shown in
in Table 7. The columns labeled “AvFM” show that as expected subset memoization does improve
the utility of stored memos over normal Graphplan (since it uses a memo in more scenarios than
normal Graphplan can). However, we also note that subset memoization by itself does not have any
dramatic impact on the performance of Graphplan, and that EBL/DDB capability can significantly
enhance the savings offered by subset memoization.
In (Kambhampati, 1998), I describe the general principles underlying the EBL/DDB techniques
and sketch how they can be extended to dynamic constraint satisfaction problems. The development
in this paper can be seen as an application of the ideas there. Readers needing more background
on EBL/DDB are thus encouraged to review that paper. Other related work includes previous attempts at applying EBL/DDB to planning algorithms, such as the work on UCPOP+EBL system
(Kambhampati et al., 1997). One interesting contrast is the ease with which EBL/DDB can be added
to Graphplan as compared to UCPOP system. Part of the difference comes from the fact that the
search in Graphplan is ultimately on a propositional dynamic CSP, while in UCPOP’s search is a
variablized problem-solving search.
As I mentioned in Section 2, Graphplan planning graph can also be compiled into a normal CSP
representation, rather than the dynamic CSP representation. I used the dynamic CSP representation as it corresponds quite directly to the backward search used by Graphplan. We saw that the
model provides a clearer picture of the mutex propagation and memoization strategies, and helps us
unearth some of the sources of strength in the Graphplan memoization strategy–including the fact
that memos are a very conservative form of no-good learning that obviate the need for the no-good
management strategies to a large extent.
The dynamic CSP model may also account for some of the peculiarities of the results of my
empirical studies. For example, it is widely believed in the CSP literature that forward checking and
dynamic variable ordering are either as critical as, or perhaps even more critical than, the EBL/DDB
strategies (Bacchus & van Run, 1995; Frost & Dechter, 1994). Our results however show that for
Graphplan, which uses the dynamic CSP model of search, DVO and FC are largely ineffective
compared to EBL/DDB on the standard Graphplan. To some extent, this may be due to the fact that

28

P LANNING G RAPH

AS A

CSP

Graphplan already has a primitive form of EBL built into its memoization strategy. In fact, Blum
& Furst (1997) argue that with memoization and a minimal action set selection (an action set is
considered minimal if it is not possible to remove an action from the set and still support all the
goals for which the actions were selected), the ordering of goals will have little effect (especially in
the earlier levels that do not contain a solution).
Another reason for the ineffectiveness of the dynamic variable ordering heuristic may have to
do with the differences between the CSP and DCSP problems. In DCSP, the main aim is not just to
quickly find an assignment for the the current level variables, but rather to find an assignment for
the current level which is likely to activate fewer and easier to assign variables, whose assignment
in turn leads to fewer and easier to assign variables and so on. The general heuristic of picking the
variable with the smallest (live) domain does not necessarily make sense in DCSP, since a variable
with two actions supporting it may actually be much harder to handle than another with many
actions supporting it, if each of the actions supporting the first one eventually lead to activation of
many more and harder to assign new variables. It may thus be worth considering ordering strategies
that are more customized to the dynamic CSP models–e.g. orderings that are based on the number
(and difficulty) of variables that get activated by a given variable (or value) choice.
We have recently experimented with a value-ordering heuristic that picks the value to be assigned to a variable using the distance estimates of the variables that will be activated by that choice
(Kambhampati & Nigenda, 2000). The planning graph provides a variety of ways of obtaining these
distance estimates. The simplest idea would be to say that the distance of a proposition p is the level
at which p enters the planning graph for the first time. This distance estimate can then be used
to rank variables and their values. Variables can be ranked simply in terms of their distances–the
variables that have the highest distance are chosen first (akin to fail-first principle). Value ordering
is a bit trickier–for a given variable, we need to pick an action whose precondition set has the lowest
distance. The distance of the precondition set can be computed from the distance of the individual
preconditions in several ways:





Maximum of the distances of the individual propositions making up the preconditions.
Sum of the distances of the individual propositions making up the preconditions.
The first level at which the set of propositions making up the preconditions are present and
are non-mutex.

In (Kambhampati & Nigenda, 2000), we evaluate goal and value ordering strategies based on
these ideas, and show that they can lead to quite impressive (upto 4 orders of magnitude in our
tests) speedups in solution-bearing planning graphs. We also relate the distances computed through
planning graph to the distance transforms computed by planners like HSP (Bonet, Loerincs, &
Geffner, 1999) and UNPOP (McDermott, 1999). This idea of using the planning graph as a basis
for computing heuristic distance metrics is further investigated in the context of state-space search
in (Nguyen & Kambhampati, 2000). An interesting finding in that paper is that even when one is
using state-space instead of CSP-style solution extraction, EBL can still be useful as a lazy demanddriven approach for discovering n-ary mutexes that can improve the informedness of the heuristic.
Specifically, Long & Kambhampati describe a method where a limited run of Graphplan’s backward search, armed with EBL/DDB is used as a pre-processing stage to explicate memos (“n-ary
mutexes”) which are then used to significantly improve the effectiveness of the heuristic on the
state-search.
29

K AMBHAMPATI

The general importance of EBL & DDB for CSP and SAT problems is well recognized. Indeed,
one of the best systematic solvers for propositional satisfiability problems is RELSAT (Bayardo &
Schrag, 1997), which uses EBL, DDB, and forward checking. A randomized version of RELSAT is
one of the solvers supported by the BLACKBOX system (Kautz & Selman, 1999), which compiles
the planning graph into a SAT encoding, and ships it to various solvers. BLACKBOX thus offers
a way of indirectly comparing the Dynamic CSP and static CSP models for solving the planning
graph. As discussed in Section 2.2, the main differences are that BLACKBOX needs to compile
the planning graph into an extensional SAT representation. This makes it harder for BLACKBOX
to exploit the results of searches in previous levels (as Graphplan does with its stored memos),
and also leads to memory blowups. The latter is particularly problematic as the techniques for
condensing planning graphs, such as the bi-level representation discussed in (Fox & Long, 1999;
Smith & Weld, 1999) will not be effective when we compile the planning graph to SAT. On the
flip side, BLACKBOX allows non-directional search, and the opportunity to exploit existing SAT
solvers, rather than develop customized solvers for the planning graph. At present, it is not clear
whether either of these approaches dominates the other. In my own informal experiments, I found
that certain problems, such as Att-log-x, are easier to solve with non-directional search offered by
BLACKBOX, while others, such as Gripper-x, are easier to solve with the Graphplan backward
search. The results of the recent AIPS planning competition are also inconclusive in this respect
(McDermott, 1998).
While my main rationale for focusing on dynamic CSP model of the planning graph is due to
its closeness to Graphplan’s backward search, Gelle (1998) argues that keeping activity constraints
distinct from value constraints has several advantages in terms of modularity of the representation.
In Graphplan, this advantage becomes apparent when not all activation constraints are known a
priori, but are posted dynamically during search,. This is the case in several extensions of the
Graphplan algorithm that handle conditional effects (Kambhampati et al., 1997; Anderson, Smith,
& Weld, 1998; Koehler et al., 1997), and incomplete initial states (Weld, Anderson, & Smith, 1998).
Although EBL and DDB strategies try to exploit the symmetry in the search space to improve the
search performance, they do not go far enough in many cases. For example, in the Gripper domain,
the real difficulty is that search gets lost in the combinatorics of deciding which hand should be used
to pick which ball for transfer into the next room–a decision which is completely irrelevant for the
quality of the solution (or the search failures, for that matter). While EBL/DDB allow Graphplan
to cut the search down a bit, allowing transfer of up to 10 balls from one room to another, they
are over come beyond 10 balls. There are two possible ways of scaling further. The first is to
“variablize” memos, and realize that certain types of failures would have occurred irrespective of
the actual identity of the hand that is used. Variablization, also called “generalization” is a part
of EBL methods (Kambhampati, 1998; Kambhampati et al., 1997). Another way of scaling up
in such situations would be to recognize the symmetry inherent in the problem and abstract the
resources from the search. In (Srivastava & Kambhampati, 1999), we describe this type of resource
abstraction approach for Graphplan.

10. Conclusion and Future work
In this paper, I traced the connections between the Graphplan planning graph and CSP, and motivated the need for exploiting CSP techniques to improve the performance of Graphplan backward search. I then adapted and evaluated several CSP search techniques in the contest of Graph-

30

P LANNING G RAPH

AS A

CSP

plan. These included EBL, DDB, forward checking, dynamic variable ordering, sticky values, and
random-restart search. My empirical studies show the EBL/DDB is particularly useful in dramatically speeding up Graphplan’s backward search (by up tp 1000x in some instances). The speedups
can be improved further (by up to 8x) with the addition of forward checking, dynamic variable ordering and sticky values on top of EBL/DDB. I also showed that EBL/DDB techniques are equally
effective in helping Graphplan, even if random-restart search strategies are used.
A secondary contribution of this paper is a clear description of the connections between the
Graphplan planning graph, and the (dynamic) constraint satisfaction problem. These connections
help us understand some unique properties of the Graphplan memoization strategy, when viewed
from CSP standpoint (see Section 9).
There are several possible ways of extending this work. The first would be to support the
use of learned memos across problems (or when the specification of a problem changes, as is the
case during replanning). Blum & Furst (1997) suggest this as a promising future direction, and
the EBL framework described here makes the extension feasible. As discussed in (Kambhampati,
1998; Schiex & Verfaillie, 1993), supporting such inter-problem usage involves “contextualizing”
the learned no-goods. In particular, since the soundness of memos depends only on the initial state
of the problem (given that operators do not change from problem to problem), inter-problem usage
of memos can be supported by tagging each learned memo with the specific initial state literals that
supported that memo. Memos can then be used at the corresponding level of a new problem as
long as their initial state justification holds in the new problem. The initial state justification for
the memos can be computed incrementally by a procedure that first justifies the propagated mutex
relations in terms of the initial state, and then justifies individual memos in terms of the justifications
of the mutexes and other memos from which they are derived.
The success of EBL/DDB approaches in Graphplan is in part due to the high degree of redundancy in the planning graph structure. For example, the propositions (actions) at level l in a
planning graph are a superset of the propositions (actions) at level l , 1, the mutexes (memos) at
level l are a subset of the mutexes (memos) at level l , 1). While the EBL/DDB techniques help
Graphplan exploit some of this redundancy by avoiding previous failures, the exploitation of redundancy can be pushed further. Indeed, the search that Graphplan does on a planning graph of size l
is almost a re-play of the search it did on the planning graph of size l , 1 (with a few additional
choices). In (Zimmerman & Kambhampati, 1999), we present a complementary technique called
“explanation-guided backward search” that attempts to exploit this deja vu property of the Graphplan’s backward search. Our technique involves keeping track of an elaborate trace of the search at
a level l (along with the failure information), termed the “pilot explanation” for level l, and using the
pilot explanation to guide the search at level l , 1. The way EBL/DDB help in this process is that
they significantly reduce the size of the pilot explanations that need to be maintained. Preliminary
results with this technique shows that it complements EBL/DDB and provides significant further
savings in search.
Acknowledgements
This research is supported in part by NSF young investigator award (NYI) IRI-9457634, ARPA/Rome
Laboratory planning initiative grant F30602-95-C-0247, Army AASERT grant DAAH04-96-10247, AFOSR grant F20602-98-1-0182 and NSF grant IRI-9801676. I thank Maria Fox and Derek
Long for taking the time to implement and experiment with EBL/DDB on their STAN system. I

31

K AMBHAMPATI

would also like to thank them, as well as Terry Zimmerman, Biplav Srivastava, Dan Weld, Avrim
Blum and Steve Minton for comments on previous drafts of this paper. Special thanks are due to
Dan Weld, who hosted me at University of Washington in Summer 1997, and spent time discussing
the connections between CSP and Graphplan. Finally, I thank Mark Peot and David Smith for their
clean Lisp implementation of the Graphplan algorithm, which served as the basis for my extensions.

References
Anderson, C., Smith, D., & Weld, D. (1998). Conditional effects in graphplan. In Proc. AI Planning
Systems Conference.
Bacchus, F., & van Run, P. (1995). Dynamic variable ordering in CSPs. In Proc. Principles and
Practice of Constraint Programming (CP-95). Published as Lecture Notes in Artificial Intelligence, No. 976. Springer Verlag.
Bayardo, R., & Schrag, R. (1997). Using CSP look-back techniques to solve real-world sat instances. In Proc. AAAI-97.
Blum, A., & Furst, M. (1997). Fast planning through planning graph analysis. Artificial Intelligence,
90(1-2).
Bonet, B., Loerincs, G., & Geffner, H. (1999). A robust and fast action selection mechanism for
planning. In In Proc. AAAI-97.
Do, B., & Kambhampati, S. (2000). Solving planning graph by compiling it into CSP. In Proc. 5th
International Conference on AI Planning and Scheduling.
Do, B., Srivastava, B., & Kambhampati, S. (2000). Investigating the effect of relevance and reachability constraints on the sat encodings of planning. In Proc. 5th International Conference on
AI Planning and Scheduling.
Fox, M. (1998). Private correspondence..
Fox, M., & Long, D. (1999). Efficient implementation of plan graph. Journal of Artificial Intelligence Research, 10.
Frost, D., & Dechter, R. (1994). In search of the best constraint satisfactions earch. In Proc. AAAI94.
Gelle, E. (1998). On the generation of locally consistent solution spaces in mixed dynamic constraint problems. Ph.D. thesis, Ingenieure informaticienne EPFL de nationalite Suisse, Lausanne, Switzerland.
Gerevini, A. (1999). Fast planning through greedy planning graphs. In Proc. AAAI-99.
Gomes, C., Selman, B., & Kautz, H. (1998). Boosting combinatorial search through randomization.
In Proc. AAAI-98, pp. 431–437.
Kambhampati, S. (1997). Challenges in bridging plan synthesis paradigms. In Proc. IJCAI-97.

32

P LANNING G RAPH

AS A

CSP

Kambhampati, S. (1998). On the relations between intelligent backtracking and explanation-based
learning in planning in constraint satisfaction. Artifical Intelligence, 105(1-2).
Kambhampati, S. (1999). Improving graphplan’s search with ebl & ddb techniques. In Proc. IJCAI99.
Kambhampati, S., Katukam, S., & Qu, Y. (1997). Failure driven dynamic search control for partial
order planners: An explanation-based approach. Artificial Intelligence, 88(1-2), 253–215.
Kambhampati, S., & Nigenda, R. (2000). Distance-based goal ordering heuristics for graphplan. In
Proc. 5th International Conference on AI Planning and Scheduling.
Kambhampati, S., Parker, E., & Lambrecht, E. (1997). Understanding and extending graphplan.
In Proceedings of 4th European Conference on Planning. URL: rakaposhi.eas.asu.edu/ewspgraphplan.ps.
Kautz, H., & Selman, B. (1996). Pushing the envelope: Plannng, propositional logic and stochastic
search. In Proc. AAAI-96.
Kautz, H., & Selman, B. (1999). Blackbox: Unifying sat-based and graph-based planning. In Proc.
IJCAI-99.
Koehler, J., Nebel, B., Hoffman, J., & Dimopoulos, Y. (1997). Extending planning graphs to an adl
subset. Tech. rep. 88, Albert Ludwigs University.
Li, C., & Anbulagan (1997). Heuristics based on unit propagation for satisfiability problems. In
Proc. IJCAI-97.
McDermott,
D.
(1998).
Aips-98
planning
ftp.cs.yale.edu/pub/mcdermott/aipscomp-results.html.

competition

results.

McDermott, D. (1999). Using regression graphs to control search in planning. Aritificial Intelligence, 109(1-2), 111–160.
Mittal, S., & Falkenhainer, B. (1990). Dynamic constraint satisfaction problems. In Proc. AAAI-90.
Nguyen, X., & Kambhampati, S. (2000). Extracting effective and admissible state-space heuristics
from the planning graph. Tech. rep. ASU CSE TR 00-03, Arizona State University.
Prosser, P. (1993). Domain filtering can degrade intelligent backtracking search. In Proc. IJCAI-93.
Rymon, R. (1992). Set enumeration trees. In Proc. KRR-92.
Schiex, T., & Verfaillie, G. (1993). Nogood recording for static and dynamic constraint satisfaction
problems. In Proc. 5th intl. conference on tools with artificial intelligence.
Selman, B., Levesque, H., & Mitchell, D. (1992). GSAT: a new method for solving hard satisfiability
problems. In In Proc. AAAI-92.
Smith, D., & Weld, D. (1999). Temporal planning with mutual exclusion reasoning. In Proc.
IJCAI-99.
33

K AMBHAMPATI

Srivastava, B., & Kambhampati, S. (1999). Scaling up planning by teasing out resource scheduling.
In Proc. European Conference on Planning.
Tsang, E. (1993). Foundations of Constraint Satisfaction. Academic Press, San Diego, California.
Weld, D., Anderson, C., & Smith, D. (1998). Extending graphplan to handle uncertainty & sensing
actions. In Proc. AAAI-98.
Zimmerman, T., & Kambhampati, S. (1999). Exploiting symmetry in the plan-graph via
explanation-guided search. In Proc. AAAI-99.

34

Journal of Artificial Intelligence Research 12 (2000) 235-270

Submitted 12/99; published 5/00

Backbone Fragility and the Local Search Cost Peak
Josh Singer

joshuas@dai.ed.ac.uk

Division of Informatics, University of Edinburgh
80 South Bridge, Edinburgh EH1 1HN, United Kingdom

Ian P. Gent

ipg@dcs.st-and.ac.uk

School of Computer Science, University of St. Andrews
North Haugh, St. Andrews, Fife KY16 9SS, United Kingdom

Alan Smaill

A.Smaill@ed.ac.uk

Division of Informatics, University of Edinburgh
80 South Bridge, Edinburgh EH1 1HN, United Kingdom

Abstract
The local search algorithm WSat is one of the most successful algorithms for solving
the satisfiability (SAT) problem. It is notably effective at solving hard Random 3-SAT
instances near the so-called ‘satisfiability threshold’, but still shows a peak in search cost
near the threshold and large variations in cost over different instances. We make a number
of significant contributions to the analysis of WSat on high-cost random instances, using
the recently-introduced concept of the backbone of a SAT instance. The backbone is the set
of literals which are entailed by an instance. We find that the number of solutions predicts
the cost well for small-backbone instances but is much less relevant for the large-backbone
instances which appear near the threshold and dominate in the overconstrained region.
We show a very strong correlation between search cost and the Hamming distance to the
nearest solution early in WSat’s search. This pattern leads us to introduce a measure of the
backbone fragility of an instance, which indicates how persistent the backbone is as clauses
are removed. We propose that high-cost random instances for local search are those with
very large backbones which are also backbone-fragile. We suggest that the decay in cost
beyond the satisfiability threshold is due to increasing backbone robustness (the opposite
of backbone fragility). Our hypothesis makes three correct predictions. First, that the
backbone robustness of an instance is negatively correlated with the local search cost when
other factors are controlled for. Second, that backbone-minimal instances (which are 3-SAT
instances altered so as to be more backbone-fragile) are unusually hard for WSat. Third,
that the clauses most often unsatisfied during search are those whose deletion has the most
effect on the backbone. In understanding the pathologies of local search methods, we hope
to contribute to the development of new and better techniques.

1. Introduction
Why do some problem instances require such a high computational cost for algorithms to
solve? Answering this question will help us to understand the interaction between search
algorithms and problem instance structure and can potentially suggest principled improvements, for example the Minimise-Kappa heuristic (Gent, MacIntyre, Prosser, & Walsh,
1996; Walsh, 1998).
In this paper we study the propositional satisfiability problem (SAT). SAT is important
as it was the first and is perhaps the archetypal NP-complete problem. Furthermore, many
c
2000
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Singer, Gent & Smaill

AI tasks of practical interest such as constraint satisfaction, planning and timetabling can
be naturally encoded as SAT instances.
A SAT instance C is a propositional formula in conjunctive normal form. C is a bag
of m clauses which represents their conjunction. A clause is a disjunction of literals, which
are Boolean variables or their negations. The variables constitute a set of n symbols V . An
assignment is a mapping from V to {true, false}. The decision question for SAT asks whether
there exists an assignment which makes C true under the standard logical interpretation
of the connectives. Such an assignment is a solution of the instance. If there is a solution,
the SAT instance is said to be satisfiable. In this study, assignments where a few clauses
are unsatisfied are also important. We refer to these as quasi-solutions. k-SAT is the SAT
problem restricted to clauses containing k literals. Notably, the k-SAT decision problem
is NP-hard for k ≥ 3 (Cook, 1971). In several NP-hard decision problems, such as 3SAT, certain probabilistic distributions of instances parameterised by a “control parameter”
exhibit a sharp threshold or “phase transition” in the probability of there being a solution
(Cheeseman, Kanefsky, & Taylor, 1991; Gent et al., 1996; Mitchell, Selman, & Levesque,
1992). There is a critical value of the control parameter such that instances generated
with the parameter in the region lower than the critical value (the underconstrained region)
almost always have solutions. Those generated from the overconstrained region where the
control parameter is higher than the critical value almost always have no solutions.
In many problem distributions, this threshold is associated with a peak in search cost
for a wide range of algorithms. Instances generated from the distribution with the control
parameter near the critical value are hardest and cost decays as we move from this value
to lower or higher values. This behaviour is of interest to basic AI research. Being devoid
of any regularities, random instances represent the challenge faced by an algorithm in the
absence of any assumptions about the problem domain, or once all knowledge about it has
been exploited in the design of the algorithm or transformation of the problem instance.
Random k-SAT is a parameterised distribution of k-SAT instances. In Random k-SAT,
n is fixed and the control parameter is m/n. Varying m/n produces a sharp threshold in the
probability of satisfiability and an associated cost peak for a range of complete algorithms
(Crawford & Auton, 1996; Larrabee & Tsuji, 1992). The cost peak pattern in Random
k-SAT has been conjectured to extend to all reasonable complete methods by Cook and
Mitchell, (1997) who also give an overview of analytic and experimental results on the
average-case hardness of SAT distributions.
In this paper we study the behaviour of local search on Random k-SAT. The term local
search encompasses a class of procedures in which a series of assignments are examined with
the objective of finding a solution. The first assignment is typically selected at random.
Local search then proceeds by moving from one assignment to another by “flipping” (i.e.
inverting) the truth value of a single variable. The variable to flip is chosen using a heuristic
which may include randomness, an element of hill-climbing (for example on the number of
satisfied clauses) and memory. Usually, local search is incomplete for the SAT decision
problem: there is no guarantee that if a solution exists, it will be found within any time
bound. Unlike complete procedures, local search cannot usually prove for certain that no
solution exists.
It is a relatively recent discovery (e.g. Selman, Levesque and Mitchell, 1992) that the
average cost for local search procedures scales much better than that of the best complete
236

Backbone Fragility and the Local Search Cost Peak

procedures at the critical value of m/n in Random 3-SAT. More recent studies, (e.g. Parkes
and Walser, 1996) have confirmed this in detail. Therefore in any system where completeness
may be sacrificed, local search procedures have an important role to play, and this is why
they have generated so much interest in recent years.
If we restrict ourselves to those instances of the distribution which are satisfiable and
increase the control parameter, there is a peak in the cost for local search procedures to
solve the instances near the critical value in several constraint-like problems (Clark, Frank,
Gent, MacIntyre, Tomov, & Walsh, 1996; Hogg & Williams, 1994). In the underconstrained
region, the average cost increases with m/n due to the decreasing number of solutions per
instance (Clark et al., 1996). However, in the overconstrained region, the cost decreases with
m/n although the number of solutions per instance continues to fall. Several researchers
have noted this fact with surprise (Clark et al., 1996; Parkes, 1997; Yokoo, 1997) since the
number of solutions does not change in any special way near the critical value. Why, then,
should the cost of satisfiable instances peak near the critical value, and then decay?
Parkes (1997) provided an appealing answer to the first part of this question in his study
of the backbone of satisfiable Random 3-SAT instances. For satisfiable SAT instances, the
backbone is the set of literals which are logically entailed by the clauses of the instance1 .
Variables which appear in these entailed literals are each forced to take a particular truth
value in all solutions. Parkes’ study demonstrated that in instances from the underconstrained region, only a small fraction of the variables, if any, appear in the backbone.
However, as the control parameter is increased towards the critical value, a subclass of
instances which have large backbones, mentioning around 75-95% of the variables, rapidly
emerges. Soon after the control parameter is increased into the overconstrained region these
large-backbone instances account for all but a few of the satisfiable instances. Parkes also
showed that for a fixed value of the control parameter, the cost for the local search procedure WSat is strongly influenced by the size of the backbone. This suggests that the peak in
average WSat cost near the critical value as the control parameter is increased may be due
to the emergence of large-backbone instances at this point. Parkes noted that for any given
size of backbone, the cost is actually higher for instances from the underconstrained region
and falls as the control parameter is increased. He also identified this fall as indicative of
another factor which produces the overall peak in cost. The main aim of this paper is to
identify the factor responsible for this pattern; why are some instances with a certain size
of backbone more costly to solve than others?
The remainder of the paper is organised as follows. In Section 2 we review the details
of the WSat algorithm and the Random k-SAT distribution and discuss the experimental
conditions which were used. We also elucidate the patterns in cost which we intend to
explain and show how the number of solutions and the backbone size interact. In Section
3 we identify a remarkable pattern in WSat’s search behaviour which clearly distinguishes
high cost from lower cost instances of a certain backbone size. WSat is usually drawn
early on in the search to quasi-solutions where a few clauses are unsatisfied. On high cost
instances, these quasi-solutions are distant from the nearest solution, while on lower cost
instances of equal backbone size, they are less distant. In Section 4 we develop a causal
hypothesis, postulating a structural property of instances which induces a search space
1. Here, our use of the term “backbone” follows Monasson, Zecchina, Kirkpatrick, Selman and Troyansky
(1999a, 1999b) whose definition of the backbone is equivalent to ours for satisfiable instances.

237

Singer, Gent & Smaill

structure which in turn causes the observed search behaviour and thus the cost pattern. We
suggest that instances of a certain backbone size are of high cost when they are backbonefragile, i.e. when the removal of a few clauses at random results in an instance with a
greatly reduced backbone size. We discuss how this property may be measured and show
how as the control parameter is increased, instances of a certain backbone size become less
backbone-fragile.
A hypothesis is only of true scientific merit if it makes correct predictions. Our hypothesis made three correct predictions for which we provide experimental evidence. In
Section 5 we show that the degree to which an instance is backbone-fragile accounts for
some of the variance in cost when the control parameter and the backbone-size are fixed.
In Section 6 we consider the generation of instances which are very backbone-fragile. If
clauses are removed such that the backbone is unaffected, we found that the resulting instances became progressively more backbone-fragile. Eventually, no more clauses can be
removed without affecting the backbone and the instance is said to be backbone minimal.
Our hypothesis correctly predicts that as clauses are removed in this way from Random
3-SAT instances, the cost becomes considerably higher. In Section 7 we show that the hypothesis makes a correct prediction relating to the search behaviour: the clauses which are
most often unsatisfied during search are those whose removal most affects the backbone. In
Section 8 we relate this study to previous research and give suggestions for further work.
Finally, Section 9 concludes.

2. Background
In this section we discuss the local search algorithm WSat, the measurement of computational cost for it and its representativeness of local search algorithms in general. We also
review the Random k-SAT distribution and the overall cost pattern for WSat on Random
k-SAT. Finally we look at how backbone size and the number of solutions interact to affect
the cost.
2.1 The WSat Algorithm
The term WSat was first introduced by Selman et al. (1994). It refers to a local search
architecture which has also been the subject of a number of subsequent empirical studies
(Hoos, 1999a; McAllester, Selman, & Kautz, 1997; Parkes & Walser, 1996; Parkes, 1997).
A pseudocode outline of the WSat algorithm is given in Figure 1. An important feature
of WSat is that, unlike earlier local search algorithms, it chooses an unsatisfied clause
and then flips a variable appearing in that clause: Select-variable-from-clause must
return a variable mentioned in clause. This architecture was first seen in the “random walk
algorithm” due to Papadimitriou (1991). WSat may use different strategies for Selectvariable-from-clause. In this study, we used the “SKC” strategy introduced by Selman,
Kautz and Cohen (1994); we refer to this combination simply as WSat. Pseudocode for
the SKC strategy is given in Figure 2.
We follow Hoos (1998) in our approach to measuring the computational cost of SAT
instances for our local search algorithm WSat. Rather than run-times, we measure runlengths : the number of flips taken to find a solution. We set the noise level p to 0.55, which
Hoos found to be approximately optimal on Random 3-SAT. Hoos and Stützle (1998) showed
238

Backbone Fragility and the Local Search Cost Peak

WSat(C, Max-tries, Max-flips, p)
for i = 1 to Max-tries
T := a random assignment
for j = 1 to Max-flips
clause := an unsatisfied clause of C, selected at random
v := Select-variable-from-clause(clause, C, p)
T := T with v’s value “flipped”
if T is satisfying
return T
end if
end for
end for
return “no satisfying assignment found”

Figure 1: The WSat local search algorithm

Select-variable-from-clause(clause, C, p)
for each variable x mentioned in clause
breaks[x] := the number of clauses in C which would
become unsatisfied if x were flipped
end for
if there is some variable y from clause such that breaks[y] = 0
return such a variable, breaking ties randomly
else
with probability 1 − p
return a variable z from clause
which minimises breaks[z], breaking ties randomly
with probability p
return a variable z from clause
chosen randomly
end if

Figure 2: The SKC variable selection strategy

that run lengths on all but the easiest instances are exponentially distributed for many local
search variants. This implies that the random “restart” mechanism (the re-randomisation
of T after Max-flips flips) is not significantly worthwhile.
239

Singer, Gent & Smaill

It is not known to date whether, without using restart, WSat will almost surely (i.e. with
probability approaching 1) find a solution on satisfiable 3-SAT instances if given unlimited
flips. If a local search algorithm will eventually find a solution under these conditions, it is
said to be probabilistically approximately complete (PAC). Hoos (1999a) proved whether
several local search algorithms were PAC and Culberson and Gent (1999a) proved that
WSat is PAC for the 2-SAT case. Hoos (1998) observed that his data suggested WSat
could be PAC. We set Max-tries to 1 and Max-flips infinite on all runs reported in this
paper. A solution was found in every run, which is further evidence that WSat may be
PAC.
Another implication of the exponential distribution of run lengths is that a large number
of samples must be taken to obtain a good estimate of the mean. Following Hoos, we use
the median of 1000 WSat runs on each instance as our descriptive statistic representing
WSat’s search cost on that instance. This appears to give a stable estimate of the cost
(as it is less sensitive to the long tail than the mean) with only a moderate amount of
computational effort.
One objection to studying a single algorithm from the local search class is that it may not
be representative: results obtained for the algorithm may not generalise to other members
of the class. While we accept this objection, there is evidence that under certain conditions,
one local search algorithm is actually to a large extent representative of the whole class.
For example Hoos (1998) found a very high correlation between the computational costs of
random instances of pairs of different local search algorithms, including WSat. This also
suggests that there is some algorithm-independent property of these instances which results
in high cost for this class of algorithms.
2.2 Random k-SAT
We use the well-studied Random k-SAT distribution (Franco & Paull, 1983; Mitchell et al.,
1992) with k = 3. Random k-SAT is a distribution of k-SAT instances, parameterised by
the ratio of clauses to variables m/n. Let V be the fixed set of Boolean variable symbols of
size n. To generate an instance from Random k-SAT with m clauses and n variables, each
clause in C is independently chosen by randomly selecting as its literals k distinct variables
from V and independently negating each with probability 12 . There is no guarantee that all
variables are mentioned in the instance or that it will not contain duplicate clauses.
As local search cannot solve unsatisfiable instances, we filter these out using a complete
SAT procedure. In order to control for the effects of the backbone size, we will also need
to isolate the portion of the satisfiable part of the distribution for which the backbone size
is of a certain value. This is obtained by calculating the backbone size of each satisfiable
instance and discarding those whose backbone is not of the required size. We will term this
controlling the backbone size. Satisfiable instances with certain backbone sizes are rare at
certain values of m/n. For example when m/n is 4.49, we found that only 1 in about 20,000
generated instances is satisfiable with a backbone size of 10. Hence generation of instances
in this way can be somewhat costly in computational terms. This was therefore one of the
limits on the value of n for which data could be collected.
240

Backbone Fragility and the Local Search Cost Peak

We were primarily interested in the threshold region of the control parameter, where the
cost peak occurs: the region near the point at which 50% of the instances are satisfiable.
We looked at the region between 90% and 20% satisfiability.
2.3 A Pattern in WSat Cost for Random 3-SAT
In Figure 3 we show the peak in WSat cost which has been mentioned e.g. by Parkes
(1997). The peak is slightly above the 50% point (4.29) for the median but appears to shift
down for higher percentiles. A similar pattern was noticed by Hogg and Williams (1994) in
local search cost for graph colouring.

9000

8000
95th

7000

cost

6000

5000

90th

4000

3000
75th
2000
50th
1000

0

25th

4

4.1

4.2

4.3
m/n

4.4

4.5

Figure 3: The cost peak for WSat as m/n is varied. At each level of m/n, we generated
5000 satisfiable instances. We measured per-instance WSat cost for each of these.
Each line in the plot gives a different point in the cost distribution, e.g. the 90th
percentile is the difficulty of the 500th most costly instance for WSat.

Both Parkes (1997) and Yokoo (1997) suggest that the local search cost peak shown for
WSat in Figure 3 is a result of two competing factors. As m/n is increased the number
of solutions per instance falls and this causes the onset of high cost. However, the number
of solutions continues to fall in the overconstrained region but the cost decreases. There
must therefore be a second factor whose effect outweighs that of the number of solutions in
the overconstrained region so as to cause the fall in cost. The main aim of this paper is to
identify this factor. A pattern in WSat cost on Random 3-SAT identified by Parkes (1997)
241

Singer, Gent & Smaill

is our starting point. Parkes observed that for a given backbone size and n, the average
cost falls as m/n is increased.
Figure 4 shows the fall in WSat cost for n = 100 Random 3-SAT instances. Each
point in the plot is the median cost of 1000 instances2 and the length of the bars is the
interquartile range of instance cost. The fall in cost is an approximately exponential decay
for a range of m/n near the threshold and for a range of backbone sizes. The rate of decay
is affected by the backbone size, with the cost of large-backbone instances decaying fastest.
The length of the error bars in Figure 4 along with the log scale of the cost axis indicates
that the distribution of per-instance cost is also positively skewed even once backbone size
is controlled. For example at the point where m/n is 4.11 and backbone size is 0.9n the
difference between the 75th percentile and the median is about 4000 whereas between the
median and the 25th percentile it is about half that. The spread of cost is large, particularly
relative to the effect of the control parameter. We do not think that a significant portion
of this variance in the cost among instances is due to errors in our estimates of the cost for
each instance.

backbone size = 0.9 n
backbone size = 0.5 n
backbone size = 0.1 n

4

cost

10

3

10

4

4.05

4.1

4.15

4.2

4.25
m/n

4.3

4.35

4.4

4.45

4.5

Figure 4: The effect of varying m/n on cost while backbone size is controlled.

2. The cost of each instance is defined as the median run length of 1000 runs so each point in Figure 4 is a
median of medians.

242

Backbone Fragility and the Local Search Cost Peak

2.4 The Number of Solutions when Backbone Size is Controlled
We studied the effect of the number of solutions on WSat cost. The number of solutions
was determined using a modified complete procedure. For small-backbone instances, there
was some evidence that the number of solutions actually increases with m/n, at least in the
overconstrained region. Figure 5 shows a plot of the number of solutions, with backbone
size controlled at 0.1n. Each point is the median of 1000 instances and the bars show the
interquartile range. This possible increase in the number of solutions may help to explain
the fall in cost for small-backbone instances, but it appears to be too weak an effect to
account for it in full.
6

10

number of solutions

backbone size = 0.1 n

5

10

4

10

4

4.05

4.1

4.15

4.2

4.25
m/n

4.3

4.35

4.4

4.45

4.5

Figure 5: Number of solutions with n = 100, m/n varied, and backbone size controlled at
0.1n.

We studied the relationship between the number of solutions and the WSat cost with
backbone size controlled at different values. Figure 6 shows a log-log plot of the number of
solutions against cost, where m/n is 4.29 and backbone size is 0.1n. A linear least squares
regression (lsr) fit is superimposed. Table 1 gives summary data on the log-log scatter plot
for different backbone sizes through the transition : the gradient and intercept of lsr fits,
the product-moment correlation r and the rank correlation.
The number of solutions is strongly and negatively related to the cost for smaller backbone sizes through the transition and the strength of the relationship is fairly constant as
m/n is varied. We speculate that the strong relationship on these instances arises because
243

Singer, Gent & Smaill

m/n
4.03

4.11

4.18

4.23

4.29

4.35

4.41

4.49

Backbone
size
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n

Intercept
of lsr fit
3.8993
4.1410
4.2070
3.8727
4.1551
4.1387
3.7867
4.0533
4.0202
3.7771
3.9890
3.9891
3.7309
3.9169
3.7836
3.6981
3.8933
3.8173
3.6083
3.8445
3.7772
3.5483
3.7577
3.6228

Gradient
of lsr fit
−0.1967
−0.2123
−0.1372
−0.1989
−0.2304
−0.1336
−0.1911
−0.2180
−0.1146
−0.1932
−0.2140
−0.1270
−0.1910
−0.2076
−0.0610
−0.1896
−0.2133
−0.1018
−0.1782
−0.2094
−0.1120
−0.1748
−0.2043
−0.0842

r

Rank corr.

−0.7808
−0.6761
−0.1307
−0.7696
−0.6834
−0.1275
−0.7664
−0.6932
−0.1159
−0.7829
−0.6729
−0.1317
−0.7787
−0.6921
−0.0612
−0.8007
−0.6872
−0.1044
−0.7784
−0.7024
−0.1179
−0.7972
−0.6954
−0.0992

-0.7731
-0.6699
-0.1365
-0.7669
-0.6855
-0.1291
-0.7760
-0.6974
-0.1217
-0.7873
-0.6867
-0.1329
-0.7844
-0.6941
-0.0534
-0.7994
-0.6967
-0.0903
-0.7628
-0.7085
-0.1045
-0.7932
-0.6991
-0.0783

Table 1: Data on log-log correlations between number of solutions and cost with n = 100,
m/n varied and backbone size fixed at different values.

244

Backbone Fragility and the Local Search Cost Peak

m/n = 4.29, backbone size = 0.1 n

4

cost

10

3

10

2

10
2
10

3

10

4

10

5

10
number of solutions

6

10

7

10

8

10

Figure 6: Scatter plot of number of solutions and cost with n = 100, m/n = 4.29 and
backbone size fixed at 0.1n.

finding the backbone is straightforward and the main difficulty is encountering a solution
once the backbone has been satisfied. The density of solutions in the region satisfying the
backbone is then important. For larger backbone sizes, the number of solutions is less
relevant to the cost. No significant change in the number of solutions for large backbone
instances was observed as m/n was varied. That the number of solutions and the cost are
not strongly related for these instances is unsurprising, as the large backbone size implies
that the solutions lie in a compact cluster and local search’s main difficulty is finding this
cluster (i.e. satisfying the backbone). Therefore we expect that the density of solutions
within the cluster is not so important. Hoos (1998) observed that the correlation between
number of solutions and local search cost becomes small in the overconstrained region. This
can now be explained simply by the fact that the large-backbone instances dominate in this
region.

3. Search Behaviour: the Hamming Distance to the Nearest Solution
In order to suggest the cause of the cost decay for large-backbone instances which was
observed in Section 2.3, we made a detailed study of WSat’s search behaviour, i.e. the
assignments visited during search. We report on this exploratory part of the research in
245

Singer, Gent & Smaill

this section. We explain the somewhat novel search behaviour metrics which were used
before giving results and our discussion of them.
3.1 Definitions and Methods
Assuming a local search algorithm is PAC, in any given run of unlimited length, fb , the
number of flips taken to find the first assignment where at most b clauses are unsatisfied, is
well-defined for b ≥ 0. f0 is then equal to the run length.
A particular run of a local search algorithm then consists of a series of assignments
T0 , T1 , ..., Tf0 , where Ti is the assignment visited after i flips have been made. We found that
on Random 3-SAT with n = 100, an assignment satisfying all but a few clauses was quickly
found and that during the remainder of the search, few clauses (1 - 10) were unsatisfied.
As shown by Gent and Walsh (1993) in GSat, there is a rapid hill-climbing phase, which is
also suggested by Hoos (1998), followed by a long plateau-like phase in which the number
of unsatisfied clauses is low but constantly changing. In our experiments we used f5 as an
arbitrary indicator of the length of the hill-climbing phase. Unlike in GSat, in WSat there
is no well-defined end point for the hill-climbing phase, since short bursts of hill-climbing
continue to occur for the rest of the search. We think that using fb as the indicator with
any value of b between 1 and 10 would give similar results.
Local search proceeds by flipping variable values and so we might expect that the Hamming distance between the current assignment and the nearest solution may also be of
interest. The Hamming distance between two assignments hd(T1 , T2 ) is simply the number
of variables which T1 and T2 assign differently. We studied the Hamming distance between
the current assignment T and the solution Tsol of C for which hd(T, Tsol ) is minimised. We
abbreviate this hdns(T, C) (Hamming distance to nearest solution). For any assignment
T , hdns(T, C) may be calculated by using a complete SAT procedure which is modified so
that every solution to C is visited and its Hamming distance from T calculated.
3.2 Results
In this section, data is based on Random 3-SAT instances with n = 100 and backbone size
controlled at various values between 0.1n and 0.9n. Recall that to control backbone at
a certain value, we generate satisfiable Random 3-SAT instances as usual and discard all
those whose backbone is not of the required size. We varied m/n from the point of 90%
satisfiability (4.03) to the point of 20% satisfiability (4.49). hdns(Tf5 , C) is the Hamming
distance between the first assignment at which no more than 5 clauses are unsatisfied and
the nearest solution. For each instance we calculated the median value for f5 and the mean
value for hdns(Tf5 , C) based on 1000 runs of WSat. In the plots in Figures 7 and 8, each
point is the median of 1000 instances.
Figure 7 shows the effect of varying m/n on f5 when backbone size is controlled. The
values for f5 are low compared to the cost and the range is very small. So although the
cost to find a solution varies considerably from instance to instance, quasi-solutions are
quickly found no matter what the overall cost. However, there are some notable effects of
backbone size and m/n on f5 . As might be expected, on the larger backbone instances, for
which overall cost is generally higher, WSat takes slightly longer to find a quasi-solution.
The effect of m/n is unexpected. If backbone size is controlled at 0.5n or more, as m/n is
246

Backbone Fragility and the Local Search Cost Peak

increased WSat takes slightly longer to find a quasi-solution, although simultaneously cost
is decreasing as we have seen in Figure 4.

170

backbone size = 0.9 n
backbone size = 0.7 n
backbone size = 0.5 n
backbone size = 0.3 n
backbone size = 0.1 n

160

150

140

f5

130

120

110

100

90

80

4

4.05

4.1

4.15

4.2

4.25
m/n

4.3

4.35

4.4

4.45

4.5

Figure 7: The effect of varying m/n on f5 while backbone size is controlled.
Figure 8 shows the effect of varying m/n on hdns(Tf5 , C) when the effects of backbone
size are controlled for. In this plot, the bars give the interquartile range. The spread of
values for mean hdns(Tf5 , C) at each point is also small relative to the effect of varying
m/n. Again the positive effect of backbone size on hdns(Tf5 , C) is as one might expect
since backbone size affects cost.
With backbone size controlled, as m/n is increased through the satisfiability threshold,
mean hdns(Tf5 , C) decreases linearly for a wide range of backbone values. Hence, although
a quasi-solution (Tf5 ) is usually quickly found, on the instances of lower m/n this quasisolution is considerably Hamming-distant from the nearest solution. As m/n is increased,
while the backbone size is controlled, this effect is gradually lessened.
We also looked at the relationship between the search behaviour and the cost when
m/n was fixed and the backbone size was controlled. We found that in this case variance
in hdns(Tf5 , C) accounts for most of the cost variance. Figure 9 shows a plot of the mean
hdns(Tf5 , C) against the cost with backbone size controlled at 0.5n and m/n fixed at 4.29.
An lsr fit is superimposed. The plot suggests hdns(Tf5 , C) is linearly related to log of cost.
Table 2 gives the intercept and gradient for lsr fits and r values with backbone size
controlled at three values and m/n varied. Variance in hdns(Tf5 , C) accounts for most of
the variance in cost at three different backbone sizes and this is consistent through the
247

Singer, Gent & Smaill

45

backbone size = 0.9 n
backbone size = 0.5 n
backbone size = 0.1 n
40

5

hdns(Tf ,C)

35

30

25

20

15

4

4.05

4.1

4.15

4.2

4.25
m/n

4.3

4.35

4.4

4.45

4.5

Figure 8: The effect of varying m/n on hdns(Tf5 , C) while backbone size is controlled.

threshold. The scatter plots (not shown) and linear lsr fits to the data were similar in
shape to that of Figure 9 and so are consistent with a linear relationship. The r values are
greatest for small-backbone instances but the reasons for this are unclear. Possibly, since
the search is shorter on the small-backbone instances, success follows quickly after f5 and
so hdns(Tf5 , C) is a better indicator of the likelihood of finding a solution.
Figure 8 showed that while backbone size is controlled, hdns(Tf5 , C) falls linearly as m/n
is increased. The gradient of the fall is about −14. Table 2 showed that if backbone size is
controlled and m/n fixed, hdns(Tf5 , C) is linearly related to log of cost, with the gradient
of the fit being around 0.08. Given that this linear relationship continues to hold with a
constant gradient as m/n is varied (in fact the gradient decreases slightly) and assuming
that increasing m/n is not affecting the cost by other means, we would expect a linear
decrease in log of mean cost with gradient −1.12, which is only slightly steeper than the
observed decrease in log of median cost shown in Figure 4.
So the results are consistent with the idea that whatever factor causes the cost to decay
exponentially as m/n is varied does so largely by causing hdns(Tf5 , C) to fall linearly.
3.3 Discussion
We have identified a pattern in search behaviour which is strongly related to the pattern
in cost discussed in Section 2.3. Our interpretation of this pattern is as follows. In each
248

Backbone Fragility and the Local Search Cost Peak

m/n

Backbone size

4.03

0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n

4.11

4.18

4.23

4.29

4.35

4.41

4.49

Intercept
of lsr fit
1.0528
0.6928
0.7065
1.0166
0.6315
0.8158
1.0858
0.8090
0.8109
1.1290
0.8343
0.7480
1.1289
1.0032
0.8382
1.1664
0.9835
0.9835
1.2029
1.0274
1.1070
1.2458
1.1472
1.1930

Gradient
of lsr fit
0.0844
0.0925
0.0895
0.0868
0.0955
0.0867
0.0839
0.0895
0.0864
0.0821
0.0887
0.0878
0.0826
0.0828
0.0856
0.0811
0.0842
0.0808
0.0795
0.0830
0.0768
0.0777
0.0787
0.0742

r
0.9445
0.8769
0.7308
0.9511
0.8852
0.7196
0.9556
0.8799
0.7195
0.9581
0.8974
0.7691
0.9550
0.8935
0.7579
0.9628
0.8996
0.7728
0.9565
0.9135
0.7816
0.9661
0.9197
0.8086

Table 2: Data on correlations between hdns(Tf5 , C) and log10 cost with n = 100 and m/n
and backbone size fixed at different values.

249

Singer, Gent & Smaill

5

10

4

cost

10

3

10

2

10

16

18

20

22

24

26
hdns(Tf ,C)

28

30

32

34

36

5

Figure 9: The relationship of hdns(Tf5 , C) to log of cost when backbone size is controlled
at 0.5n and m/n is fixed at 4.29.

instance the quasi-solutions which WSat visits form interconnected areas of the search space
such that local search can always move to a solution from them, without often moving to an
assignment where many clauses are unsatisfied. The evidence for this is simply that WSat
runs are apparently always successful but visit the assignments where more clauses are
unsatisfied very infrequently. Frank, Cheeseman and Stutz (1997) also mentioned in their
analysis of GSat search spaces that in Random 3-SAT, local minima where few clauses
were unsatisfied can usually be escaped by unsatisfying just one clause.
We believe that in instances of higher cost this quasi-solution area extends to parts of the
search space which are Hamming-distant from solutions, whereas in instances of lower cost
the area is less extensive. The mean Hamming distance between the early quasi-solution
Tf5 and the nearest solution is an accurate indicator of how extensive the quasi-solution
area is. This interpretation suggests why hdns(Tf5 , C) is so strongly correlated with cost:
the extensiveness of the quasi-solution area determines how costly it is to search. It also
suggests why, on instances of higher cost, quasi-solutions are found slightly more quickly:
when the quasi-solution area is extensive, from a random starting point a shorter series of
hill-climbing flips is required to find a quasi-solution.
250

Backbone Fragility and the Local Search Cost Peak

The mean hdns(Tf5 , C) decreases linearly as m/n is increased while backbone size is
controlled. At the same time, cost decays exponentially. We think this is because as m/n
is increased, the quasi-solution area becomes progressively less extensive.

4. A Causal Hypothesis
The pattern in search behaviour from Section 3 and our interpretation of it suggested a
causal hypothesis to account for the decay in cost discussed in Section 2.3 and hence the
overall peak. The key to this hypothesis is a property of SAT instances: backbone fragility.
This property is qualitatively consistent with the above observations. Most importantly,
although backbone fragility has implications for an instance’s search space topology, it is
a property based on the logical structure of the SAT instance. In this section we motivate
and define backbone fragility, discuss how it may be measured and show how it relates to
the patterns reported in Sections 2.3 and 3.
4.1 Backbone Fragility : Motivation
Suppose B is a small sub-bag of the clauses of a satisfiable SAT instance C, such that
there exists a set of quasi-solutions QB where at most the clauses B are unsatisfied. What
structural property of C would cause the quasi-solutions QB to be attractive to WSat?
We already know that if the backbone of a Random 3-SAT instance is small, its solutions
are found with little search (Parkes, 1997). The solutions to C − B (C − B denotes C with
one copy of each member of B removed) are either solutions to C or members of QB . If
we assume that the assignments which are attractive to WSat on C are approximately
the same assignments which are attractive on C − B, then the members of QB (which are
solutions of C −B) will be attractive in C when the backbone of C −B is small, particularly
if C’s backbone is large. Furthermore for any TB ∈ QB , the number of variables which do
not appear in the backbone of C −B is an upper bound on hdns(TB , C), so a large reduction
in the backbone size allows for high hdns(TB , C). To summarise, if the removal of a certain
small sub-bag of clauses causes the backbone size to be greatly reduced, we can expect that
quasi-solutions where only these clauses are unsatisfied will be attractive to WSat and
possibly Hamming-distant from the nearest solution.
We are interested in quasi-solutions in general rather than those in QB . If removing a
random small set of clauses on average causes a large reduction in the backbone size, we
say that the instance is backbone-fragile. Where the effect on the backbone is smaller on
average, the instance is backbone-robust. If a large-backbone instance is backbone-fragile, by
extension of the above argument we expect that in general quasi-solutions will be attractive
and they may be Hamming-distant from the nearest solution. Hence this idea is consistent
with our observations and interpretation in Section 3: backbone fragility approximately
corresponds to how extensive the quasi-solution area is.
The idea that backbone fragility is the underlying factor causing the search behaviour
pattern is appealing for other reasons. For each entailed literal l of C, there must be a
sub-bag of clauses in C whose conjunction entails l. For any given backbone size, as clauses
are added, for any given entailed literal l we expect that the extra clauses allow alternative
combinations of clauses which entail l. Hence after adding clauses whilst controlling the
backbone size, the random removal of clauses will have less effect on the backbone since
251

Singer, Gent & Smaill

the fact that a literal is entailed depends less on the presence of particular sub-bags. As
clauses are added, we expect that instances will become less backbone-fragile. Given the
hypothetical relationship between backbone fragility and the search behaviour, this would
then explain qualitatively why the search behaviour changes as it does when m/n is varied.
We think that because backbone fragility is a property of the instance’s logical structure, its
study may also lead to further results about complexity issues, but we postpone discussion
of this until Section 8.
4.2 The Measurement of Backbone Robustness
We now define a measure of the backbone robustness of an instance which will allow us to
test predictions of the hypothesis. We take the instance C and delete clauses at random,
halting the process when the backbone size is reduced by at least half. At this point we
record as the result the number of deleted clauses. This constitutes one robustness trial.
Our metric for backbone robustness is the mean result of all such possible trials, i.e. the
average number of random deletions of clauses which must be made so as to reduce the
backbone size by half.
It is infeasible to compute the results of all possible robustness trials. Therefore, when
measuring backbone robustness of an instance we estimated it by computing the average of
a random sample of trials. We used at least 100 robustness trials in each case and in order
to ensure a reasonably accurate estimate, we continued to sample more robustness trials
until the standard error was less than 0.05 × the sample mean (in which case our estimate
of the mean was accurate to within about 10% at the 95% confidence level). With n = 100,
using satisfiable instances from near the satisfiability threshold whose backbone size was
controlled at 50, usually less than 250 robustness trials were required for the estimate to
converge in this way. Even then, backbone robustness was costly to compute.
There were different possible metrics for backbone fragility/robustness, but we found
that the metric described above gave the clearest results for our purposes without an unnecessarily complicated definition. Other metrics, such as the reduction in backbone size when
a random fixed fraction of clauses are removed, may be more suitable in other contexts.
4.3 The Change in Backbone Robustness as the Control Parameter is Varied
As discussed in Section 4.1 we expect that if backbone size is controlled, backbone robustness
increases as m/n is increased. Since our measure of backbone robustness is defined in terms
of the size of the backbone, it is most useful when comparing instances of equal backbone
size.
We found that increasing the control parameter made instances more backbone-robust,
as expected. Figure 10 shows the effect on backbone robustness of increasing m/n through
the satisfiability threshold while n = 100 and backbone size is controlled. Each point is the
median of 1000 instances.
We note that backbone robustness as defined by our measure is generally higher for
instances with larger backbones. We think that this is because on the large-backbone
instances, the backbone must be reduced by a larger number of literals in each fragility trial
and that this requires more clauses to be removed.
252

Backbone Fragility and the Local Search Cost Peak

22

20

18

backbone robustness

16

14

12

10

8

backbone size = 0.9 n
backbone size = 0.5 n
backbone size = 0.1 n

6

4

4

4.05

4.1

4.15

4.2

4.25
m/n

4.3

4.35

4.4

4.45

4.5

Figure 10: Backbone robustness through the satisfiability transition, with backbone size
fixed at 0.1n 0.5n and 0.9n.

5. A Correct Prediction about Cost Variance
We may assert that the fall of cost observed with the increase in the control parameter
is due to the change in some other factor F , as for example Yokoo (1997) has. Such an
assertion makes an important and testable prediction: that any variation in F when the
control parameter is fixed accounts for some of the variation in cost. However there may
be other factors whose influence on the cost is so great as to obscure the effect of F when
the control parameter is fixed. To best reveal the effect of F , if there is any, the effects of
some other factors may have to be controlled for.
Backbone robustness is our proposed factor F . The backbone size is another factor
which strongly influences the cost. Our result in this section is that when the effects of
m/n and backbone size are controlled for, i.e. when they are fixed, the effects of backbone
robustness can be seen quite clearly for large-backbone instances.
5.1 Correlation Data
Figure 11 shows a plot of the log cost against the measure of backbone robustness for
Random 3-SAT instances with n = 100, m/n 4.29 and backbone size controlled at 0.1n,
0.5n and 0.9n. A linear lsr fit is superimposed in each case. Table 3 gives the intercept,
253

Singer, Gent & Smaill

m/n

Backbone size

4.03

0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n
0.1n
0.5n
0.9n

4.11

4.18

4.23

4.29

4.35

4.41

4.49

Intercept
of lsr fit
3.0338
3.7075
4.2846
2.9639
3.6675
4.2287
2.9365
3.6067
4.1811
2.9257
3.5142
4.1313
2.8766
3.4903
4.0934
2.8261
3.4325
3.9939
2.7925
3.3772
3.9284
2.7164
3.3506
3.8720

Gradient
of lsr fit
−0.0204
−0.0370
−0.0419
−0.0134
−0.0351
−0.0370
−0.0146
−0.0302
−0.0338
−0.0155
−0.0239
−0.0312
−0.0136
−0.0225
−0.0290
−0.0109
−0.0199
−0.0237
−0.0100
−0.0172
−0.0211
−0.0073
−0.0170
−0.0198

r

r −95%

r +95%

−0.1928
−0.3730
−0.4711
−0.1490
−0.3891
−0.4535
−0.1745
−0.3840
−0.5306
−0.2107
−0.3643
−0.5253
−0.1894
−0.3863
−0.5325
−0.1671
−0.3734
−0.4984
−0.1763
−0.3452
−0.5152
−0.1392
−0.4034
−0.5549

−0.2506
−0.4191
−0.5165
−0.2088
−0.4355
−0.5001
−0.2356
−0.4272
−0.5687
−0.2659
−0.4105
−0.5645
−0.2483
−0.4350
−0.5721
−0.2250
−0.4222
−0.5394
−0.2321
−0.3954
−0.5582
−0.1926
−0.4585
−0.5949

−0.1400
−0.3235
−0.4251
−0.0873
−0.3417
−0.4065
−0.1149
−0.3389
−0.4921
−0.1553
−0.3154
−0.4818
−0.1300
−0.3395
−0.4931
−0.1105
−0.3244
−0.4555
−0.1175
−0.2919
−0.4692
−0.0841
−0.3516
−0.5125

Rank corr.
coefficient
−0.1934
−0.3713
−0.4699
−0.1402
−0.3770
−0.4662
−0.1663
−0.3738
−0.5466
−0.2116
−0.3436
−0.5457
−0.2053
−0.3996
−0.5467
−0.1724
−0.3782
−0.5243
−0.1683
−0.3582
−0.5270
−0.1355
−0.4027
−0.5604

Table 3: Data on the correlation between backbone robustness and log10 cost with n = 100
and m/n and backbone size fixed at different values.

gradient and r values for lsr fits with backbone size controlled at the same three values and
with m/n varied through the threshold.
The r values suggest an effect of backbone robustness on cost, particularly for large
backbone sizes. For smaller backbone sizes, we imagine that finding the backbone is less of
an issue and so backbone fragility, which hinders this, has less of an effect. For the larger
backbone sizes, we think the main difficulty for WSat is satisfying the backbone; backbone
fragility is then important. However, given the somewhat unclear shape of the scatter plots,
there are several concerns as to the significance of the correlation, which we now address
using some simple statistical methods.
254

Backbone Fragility and the Local Search Cost Peak

m/n = 4.29, backbone size = 0.1 n

3

cost

10

2

10

0

5

10

15
20
25
backbone robustness

30

35

40

35

40

35

40

m/n = 4.29, backbone size = 0.5 n

4

cost

10

3

10

2

10

0

5

10

15
20
25
backbone robustness

30

m/n = 4.29, backbone size = 0.9 n
5

10

cost

4

10

3

10

2

10

0

5

10

15
20
25
backbone robustness

30

Figure 11: Scatter plot of backbone robustness and cost with n = 100, m/n = 4.29 and
backbone size fixed at 0.1n, 0.5n and 0.9n.

255

Singer, Gent & Smaill

5.2 An Artifact of the Distributions of the Variables?
One concern is that the observed r could also have arisen simply because of the distributions
of the two variables rather than because of any relationship between them. This is a serious
concern here as the distributions are unknown.
The null hypothesis, H0 is that the value of r which results from the distributions of the
two variables is equal to the observed r. A randomisation method can be used to test H0 .
See Appendix A for details of this method. For each data set presented, 1000 randomised
pairings of the data were constructed. In each case, we found that the observed r does
not fall within the range of the sampling distribution of r for randomised pairings. H0 can
therefore be rejected at the 99.9% confidence level.
The r coefficient, given above, can be greatly affected by outliers. Therefore the rank
correlation coefficient, which is less affected, was also calculated. The rank correlation is
also given in Table 3. We found that in each case the rank correlation coefficient is not
considerably different from the r coefficient. This demonstrates that the observed r was not
greatly affected by outliers.
5.3 Confidence Intervals for the Correlation
Given that there is a relationship between the two variables which is not merely an artifact
of the distributions or of outliers, how accurate is our measurement of r? A bootstrap
method can be used to obtain bounds on a confidence interval for this statistic. Again, the
reader is referred to Appendix A for details of this method. Using this method with 1000
pseudo-samples we obtained lower and upper bounds on the 95% confidence interval for r,
which are also given in Table 3 as r −95% and r +95% respectively. The data implies with 95%
confidence, the upper bounds on the amount of error in our estimates of r 2 are between
about 0.02 and 0.05.

6. A Correct Prediction about Very Backbone-Fragile Instances
Our hypothesis proposes that high backbone fragility of instances quite accurately represents
the factor which (via the search behaviour patterns uncovered in Section 3) causes high
WSat cost for those instances. However, it is plausible that the high backbone fragility is
a by-product of some unmeasured latent factor and that it is not causally related to the
cost.
To help establish the causal link between backbone fragility and cost, we therefore created sets of random SAT instances which had higher backbone fragility than usual Random
3-SAT instances. This is to some degree following the methodological precedent of Bayardo
and Schrag (1996), who created random instances which contained small unsatisfiable subinstances but which had few constraints overall. These were often found to be exceptionally
hard for the complete procedure Ntab. Their experiments thereby helped establish that
this feature of instance structure was the cause of exceptionally high cost for complete
procedures.
We cannot easily set backbone fragility directly, since it is not a generation parameter.
One manipulation experiment which is possible is the use of an instance generation procedure which results in instances with a higher backbone fragility. Our hypothesis predicts that
256

Backbone Fragility and the Local Search Cost Peak

instances generated using such a procedure will be harder than Random 3-SAT instances.
In this section we define such a procedure and test the prediction. It may be that our procedure is also manipulating the latent factor. However, since the procedure is specifically
designed to increase backbone fragility, a correct prediction here still lends credibility to
our hypothesis.
6.1 Backbone-minimal Sub-instances
Suppose we have a SAT instance C and we remove a clause such that the backbone is not
affected by the removal of the clause. If such clauses are repeatedly removed, eventually
the instance will be such that no clause can be removed without disturbing the backbone.
In this case we have a backbone-minimal sub-instance (BMS) of C. More formally, we have
the following definition:
Definition A SAT instance C 0 is a BMS of C iff
• C 0 is a sub-instance of C (i.e. a sub-bag of the clauses) such that C 0 has the same
backbone as C.
• for each clause c of C 0 there exists a literal l such that:
1. C 0 → l
2. (C 0 − {c}) ∧ ¬l is satisfiable
i.e. every strict sub-instance of C 0 has a strictly smaller backbone than the backbone of C 0 2
BMSs can be seen as satisfiable analogues of the minimal unsatisfiable sub-instances
(MUSs) of unsatisfiable instances studied by amongst others Culberson and Gent (1999b)
in the context of graph colouring and Gent and Walsh (1996) and Bayardo and Schrag
(1996) in satisfiability. An MUS of an instance C is a sub-instance which is unsatisfiable,
but such that the removal of any one clause from the sub-instance renders it satisfiable.
Just as all unsatisfiable instances must have an MUS, all satisfiable SAT instances must
have a BMS. Having a BMS does not depend on having a non-empty backbone – if the
backbone of the instance is empty, its BMS is the empty sub-instance. An instance can
have more than one BMS. Different BMSs of an instance may share clauses. One BMS of
an instance cannot be a strict sub-instance of another.
Suppose the backbone of a satisfiable instance C is the set of literals {l1 , l2 , . . . , lk }. Let
d be the clause ¬l1 ∨ ¬l2 ∨ . . . ∨ ¬lk . Then we have the following useful fact:
Theorem C 0 is a BMS of C iff C 0 ∧ d is an MUS of C ∧ d 2
A simple proof of the above is given in Appendix B. Due to this fact, methods for studying
MUSs can be applied to the study of BMSs. We can study the BMSs of a satisfiable
instance C by finding the backbone of C and then studying the MUSs of C ∧ d: each of
these corresponds to a BMS of C since d must be present in every MUS of C ∧ d.
To find a BMS of C we determine the backbone, then find a random MUS of C ∧ d using
the same MUS-finding method as Gent and Walsh (1996) and remove d from the result.
257

Singer, Gent & Smaill

Instances
Preserve-backbone(C, 0, C 0 )
Preserve-backbone(C, 5, C 0 )
Preserve-backbone(C, 10, C 0 )
Preserve-backbone(C, 20, C 0 )
Preserve-backbone(C, 40, C 0 )
Preserve-backbone(C, 80, C 0 )
BMS

Backbone robustness
10th percentile Median 90th percentile
8.5845 12.9700
20.6300
7.7374 12.0317
19.1700
7.1977 11.0851
17.3500
6.0690
9.3913
14.5351
4.2622
6.4899
9.9900
2.0745
2.8661
3.9851
1.0200
1.0600
1.1600

Table 4: The effect of Preserve-backbone on backbone robustness.

6.2 Interpolating Between an Instance and one of its BMSs
Once a BMS C 0 has been established, we can also study the effects of interpolation between
C and C 0 by removing at random from C some of the clauses which do not appear in
C 0 . This is equivalent to removing clauses at random such that the backbone is preserved.
Preserve-backbone(C, mr , C 0 ) will denote C with mr clauses, which do not appear in
the BMS C 0 , removed at random. The resulting instance will have the same backbone as
C.
Just as increasing m/n while controlling the backbone size causes backbone robustness
to increase, we have found that deleting clauses such that the backbone is unaffected causes
backbone robustness (as measured above) to decrease, as one might expect.
We used 500 Random 3-SAT instances with n = 100 and m/n = 4.29. For each instance
we found one BMS. We then used Preserve-backbone to interpolate with mr set at
various values. Table 4 shows the effect of increasing mr on backbone robustness. The
BMSs of the threshold instances are so backbone-fragile that the removal of just one clause
from them is likely to reduce the backbone by a half or more.
Our hypothesis predicts that as this interpolation from C to C 0 proceeds, the cost
for local search increases because the backbone robustness decreases. It is conceivable,
although it would be very surprising, that removing any clauses from random instances
near the threshold generally makes their cost for local search increase. If this were the
case, any increase in cost during interpolation towards a BMS could merely be due to the
removal of clauses per se rather than the removal of clauses whilst preserving the backbone.
To control for this possibility we also removed clauses according to two other procedures.
The procedure Random(C, mr ) removes mr clauses from C at random. The procedure
Reduce-backbone(C, mr ) removes mr clauses such that each time a clause is removed,
the size of the backbone is reduced. The clause to be removed is chosen randomly from all
such clauses. This procedure therefore uses the opposite removal criterion to Preservebackbone. If the backbone becomes empty, no further clauses are removed.
Figure 12 shows the effect on per-instance cost of applying the three clause removal
procedures to the same set of 500 Random 3-SAT threshold instances. Each plot is the
median per-instance cost.
258

Backbone Fragility and the Local Search Cost Peak

4

10

cost

Preserve−backbone
Reduce−backbone
Random

3

10

2

10

0

10

20

30

40
mr

50

60

70

80

Figure 12: The effect of the three clause removal procedures on median per-instance cost.

We observe that removing clauses randomly or such that the backbone is strictly reduced, causes cost to be reduced, so the removal of clauses does not in itself cause higher
cost. The Reduce-backbone procedure causes a greater initial fall in cost, as the backbone size is reduced more quickly than with Random. However, the cost then stabilises for
Reduce-backbone because the backbone becomes empty and thereafter no more clauses
are removed.
Removing clauses according to Preserve-backbone causes the local search cost to
increase by an amount approximately exponential in the number of clauses removed. Table
5 gives more data on this effect and also cost data for BMSs. The interpolation shifts the
whole distribution up, not just the median. The median cost of the BMSs, which are the
most backbone-fragile of all the instances, is more than three times that of the 90th cost
percentile of Random 3-SAT instances.
The BMSs of these instances had between 254 and 318 clauses. The above results
therefore demonstrate the existence of instances in the underconstrained region which are
much harder than the typical instances from near the satisfiability threshold. However since
these were not obtained by sampling from Random 3-SAT directly, we do not know how
often they occur. As far as we know, they are vanishingly rare and therefore, in contrast
to exceptionally hard instances for complete algorithms, it seems unlikely that they affect
the mean cost. Also, while Gent and Walsh (1996) showed that the exceptionally hard
259

Singer, Gent & Smaill

Instances
Preserve-backbone(C, 0, C 0 )
Preserve-backbone(C, 5, C 0 )
Preserve-backbone(C, 10, C 0 )
Preserve-backbone(C, 20, C 0 )
Preserve-backbone(C, 40, C 0 )
Preserve-backbone(C, 80, C 0 )
BMS

Per-instance cost
10th percentile Median 90th percentile
517
1450
5175
537
1515
5657
557
1608
6009
570
1803
7037
643
2295
10683
816
4154
24313
1556
16945
135883

Table 5: The effect of Preserve-backbone on per-instance cost.

instances for complete algorithms are hard for a different reason from that of threshold
instances, BMSs are apparently hard for the same reason – because they are backbonefragile.
One useful by-product of this section is a means of generating harder test instances for
local search variants without increasing n. However these instances do require O(m + n)
complete searches to generate: O(n) to determine satisfiability and the backbone and O(m)
to reduce to a BMS.

7. A Correct Prediction about Search Behaviour
Recall that in the motivating discussion of Section 4.1 it was suggested that the quasisolutions in QB would be attractive if the backbone of C − B was small. That is to say that
the clauses of B are more likely to be the set of unsatisfied clauses if the removal of the
clauses of B has a large effect on the backbone. This part of the hypothesis also makes a
prediction about search behaviour – that clauses most often unsatisfied by WSat should be
those whose removal reduces the backbone size most. In this section we show this prediction
to be correct.
We looked at individual instances which were cost percentiles from a set of 5000 Random
k-SAT instances with n = 100 and m/n = 4.29. Per-instance cost was determined as in
previous sections. For each clause in the instance, we calculated the number of backbone
literals which were no longer entailed if the clause was removed. This is a simple measure of
the backbone contribution (bc) of the clause – how much the backbone size depends on the
presence of the clauses. If a clause’s backbone contribution is high, it is termed a backbonecritical clause. We made 1000 runs of WSat on each instance under the same conditions as
in previous sections. During search, each time the current assignment changed we recorded
whether each clause was unsatisfied. The result of averaging the number of times the clause
was unsatisfied over all runs gives the unsatisfaction frequency (uf ) of that clause.
Figure 13 shows a plot of these two quantities for the clauses of the instance whose cost
was median of 5000 threshold instances. We note from this figure that the clauses whose
presence contributes the most to the backbone are more often unsatisfied than average
during WSat search.
260

Backbone Fragility and the Local Search Cost Peak

3

10

unsatisfaction frequency

2

10

1

10

0

10

0

5

10
15
backbone contribution

20

25

Figure 13: Scatter plot of unsatisfaction frequency against backbone contribution for the
clauses of the cost median of 5000 instances, m/n = 4.29, n = 100.

Table 6 confirms this pattern. Each row of the table gives data for one instance. We
selected cost percentiles; individual instances of varying degrees of difficulty. For example
the row labelled ‘30th’ corresponds to the instance whose cost is the 1500th in rank from
the easiest to the most difficult of the 5000 instances, while the 50th percentile instance
is the one used to produce Figure 13. The third and fourth columns give the mean and
standard deviation of the unsatisfaction frequency over all clauses in the instance and the
last two columns give the same statistics for the sub-bag of the clauses which were most
backbone-critical (their backbone contribution was in the top 10%).
Table 7 shows that the converse effect is also present: the clauses which are most often
unsatisfied (their unsatisfaction frequency is in the top 10%) are more backbone-critical than
average. Although an effect is quite clear from the means, there are sometimes particularly
large standard deviations in bc values for the most frequently unsatisfied clauses. This
is because, as can be seen from Figure 13, some clauses are very often unsatisfied even
though removing them on their own does not affect the backbone size at all. We have
found in other experiments that the removal of these clauses along with other small random
bags of clauses does on average reduce the backbone size considerably. The large standard
deviations therefore arise because the true backbone contribution of these clauses is not
apparent when using this simple measure.
261

Singer, Gent & Smaill

Cost
Percentile

Backbone
size

10th
20th
30th
40th
50th
60th
70th
80th
90th

16
11
13
36
48
25
63
70
93

All clauses
uf mean
11.3430
13.1079
21.0207
22.9825
29.5615
36.2940
52.4198
92.2623
108.3124

uf std. dev.
8.0704
10.9596
16.3680
21.3118
25.9275
35.6327
48.1078
87.7827
127.1968

Most backbonecritical clauses
uf mean uf std. dev.
20.8703
8.8730
30.1817
16.8730
41.4660
21.1142
56.6841
27.4660
72.0704
38.7779
96.1664
54.3081
119.7691
66.8187
167.3428
149.8058
306.7200
198.6933

Table 6: Unsatisfaction frequencies of clauses in different cost percentile instances.

Cost
Percentile
10th
20th
30th
40th
50th
60th
70th
80th
90th

Backbone
size
16
11
13
36
48
25
63
70
93

All clauses
bc mean
0.5921
0.4848
0.3963
1.8089
1.0629
1.3800
3.3916
0.6946
3.0653

bc std. dev.
1.2358
1.0380
1.2405
4.2411
3.2781
3.4920
8.8630
3.4577
10.0376

Most often
unsatisfied clauses
bc mean bc std. dev.
2.0909
1.8529
1.7727
1.9632
1.8409
2.4490
8.3182
6.4620
6.3182
6.6043
7.7500
5.7794
14.9091
15.8126
2.5909
7.8602
16.9318
20.6436

Table 7: Backbone contributions of clauses in different cost percentile instances.

262

Backbone Fragility and the Local Search Cost Peak

For instances of different costs at the satisfiability threshold, the clauses which are most
likely to be unsatisfied during search have a higher backbone contribution than average.
Conversely, the clauses which have the largest backbone contribution are more likely to be
unsatisfied during search. This section therefore demonstrates that as well as explaining
differences in cost between instances, the backbone fragility hypothesis can also explain
differences in the difficulty of satisfying particular clauses during search.

8. Related and Further Work
Clark et al. (1996) showed that the number of solutions is correlated with search cost for
a number of local search algorithms on random instances of different constraint problems,
including Random 3-SAT. The pattern was confirmed by Hoos (1998) using an improved
methodology. Clark et al.’s work was the first step towards understanding the variance in
cost when the number of constraints is fixed. We have followed their approach both by
looking at the number of solutions and by using linear regression to estimate strengths of
relationships between factors.
Schrag and Crawford (1996) made an early empirical study of the clauses (including
literals) which were entailed by Random 3-SAT instances. Parkes (1997), whose study is
also discussed in Section 1, looked in detail at backbone size in Random 3-SAT and its effect
on local search cost. He also linked the position of the cost peak to that of the satisfiability
threshold by the emergence of large-backbone instances which occurs at that point. Parkes
also identified the fall in WSat cost for instances of a given backbone size. This was
therefore the basis for our study. Parkes conjectured that the presence of a “failed cluster”
may be the cause of high WSat cost for some large-backbone Random 3-SAT instances.
According to this hypothesis, the addition of a single clause could remove a group of solutions
which is Hamming distant from the remaining solutions, reducing the size of the backbone
dramatically. Such a clause would then have a large backbone contribution. Therefore our
explanation for the general high cost of the threshold region has certain features in common
with Parkes’ conjecture. In particular we agree that it is the presence of clauses with a
large backbone contribution which causes high cost. This is especially demonstrated by our
results from Section 7.
Frank et al. (1997) studied in detail the topology of the GSat search space induced by
different classes of random SAT instances. Their study discussed the implications of search
space structure for future algorithms, as well as the effects of these structures on algorithms
such as GSat. They also noted that some local search algorithms such as WSat may be
blind to the structures they studied because they search in different ways to GSat.
Yokoo (1997) also addressed the question of why there is a cost peak for local search
as m/n is increased. The approach was to analyse the whole search space of small satisfiable random instances. While in this study, we have only examined SAT, Yokoo also
showed his results generalised to the colourability problem. Yokoo used a deterministic
hill-climbing algorithm. He studied the number of assignments from which a solution is
reachable (solution-reachable assignments) via the algorithm’s deterministic moves, which
largely determines the cost for the algorithm.
We followed Yokoo in looking for a factor competing with the number of solutions whose
effect on cost changes as m/n is increased. The factor which Yokoo proposed as the cause
263

Singer, Gent & Smaill

of the overall fall in cost was the decrease in the number of local minima – assignments
from which no local move decreased the number of unsatisfied clauses. The decrease in
this number was demonstrated as m/n is increased. The decrease was attributed to the
decreasing size of “basins” (interconnected regions of local minima with the same number
of unsatisfied clauses). Yokoo claimed (p. 363) that:
“adding constraints [...] makes the [instance] easier by decreasing the number
of local minima”.
However, we do not think it is clear a priori what the relationship between the number
of local minima and the cost is in a given instance and Yokoo did not study it sufficiently
to convince us of his explanation. In contrast with Yokoo, we have studied in detail the
relationship between the backbone fragility of instances and WSat’s cost on these instances
and confirmed it by testing predictions of our hypothesis. Also, we studied instance properties that related to the logical structure of the clauses rather than the search space topology
which was induced as we think this has more potential to generalise across algorithms and
even to address complexity issues, as we explain towards the end of this section.
Hoos (1998) also analysed the search spaces of SAT instances in relation to local search
cost by looking at two new measures of the induced objective function which he defined,
including one based on local minima. Although via these measures, Hoos was not able to
account for the Random 3-SAT cost peak, he found that the features were correlated with
cost for some SAT encodings of other problems and has also shown (Hoos, 1999b) that his
measures can help distinguish between alternative encodings of the same problem.
How does the pattern we have uncovered fit in to other work on what makes instances
require a high cost to solve? Gent and Walsh (1996) looked at the probability that an unsatisfiable SAT instance became satisfiable if a fixed number of clauses are removed at random.
The unsatisfiable instances which had the highest computational cost for a complete procedure were found to be those which were unsatisfiability-fragile – their unsatisfiability was
sensitive to the random removal of clauses. It may therefore be that the fragility of an instance’s unsatisfiability or backbone size is the cause of high computational cost both in the
context of complete procedures and incomplete local search, which would be an interesting
link between the two algorithm classes. This link may form the basis of a possible explanation of the reasons why threshold Random 3-SAT instances may be universally hard in
the average case, as opposed to merely costly for some class of algorithms. Recent work by
Monasson et al. (1999a, 1999b) has suggested that parameterised distributions of instances
which are hard in the average case, e.g. Random 3-SAT, exhibit a discontinuity in the
backbone size3 as the control parameter is varied, whereas in polynomial time average-case
distributions, such as Random 2-SAT, the backbone size changes smoothly. They propose
that the complexity of the distribution is linked to the presence of this discontinuity. We
conjecture that this may be because in the asymptotic limit, instances which are backboneor unsatisfiability-fragile only persist as n is increased where there is such a discontinuity.
This line of research may therefore establish a testable causal mechanism for this pattern,
showing how the properties of the instance distributions affect algorithm performance.
It would be interesting to compare backbone fragility in different random distributions
of 3-SAT instances, such as those introduced by Bayardo and Schrag (1996) and by Iwama,
3. Monasson et al.’s definition of the backbone also extends to unsatisfiable instances.

264

Backbone Fragility and the Local Search Cost Peak

Miyano and Asahiro (1996) to see whether differences in local search cost could be explained.
A method which generates satisfiable instances which are quickly solved by local search is
analysed by Koutsoupias and Papadimitriou (1992) and Gent (1998). Random clauses are
added to the formula as in Random 3-SAT but only if they do not conflict with a certain
solution which is set in advance. We conjecture that overconstrained examples of these are
quickly solved by local search because they are very backbone-robust.
An interesting possibility mentioned by Hoos and Stützle (1998) suggested by the exponential run length distribution, was that local search is equivalent to random generate-andtest in a drastically reduced search space. We conjecture that this reduced search space
corresponds to the quasi-solution area. Measurements of hdns(TB , C) for quasi-solutions
TB may therefore be indicative of the extensiveness of this reduced search space, especially
since this metric is linearly correlated with log cost. Further experimentation in this vein
may therefore reveal more about the topology of the reduced search space which could in
turn lead to better local search algorithms designed to exploit this knowledge.
Finally, we should emphasise that the notions of backbone and backbone-fragility are
equally applicable to non-random SAT instances. In future we may be able to confirm that
the results we have shown for random SAT instances apply equally to benchmark and realworld SAT instances. However, one caveat here is that entailed literals may be uncommon
in these instances and we may need to study the fragility of other sets of entailed formulas.

9. Conclusion
We have reconsidered the question of why cost for local search peaks near the Random
3-SAT satisfiability threshold. The overall pattern is one of two competing factors. The
cause of the onset of high cost as the control parameter is increased has been previously
established as the decreasing number of solutions. We have proposed that the cause of the
subsequent fall in cost is falling backbone fragility.
We found a striking pattern in the search behaviour of the local search algorithm WSat.
For instances of a given backbone size, in the underconstrained region of the control parameter, WSat is attracted early on to quasi-solutions which are Hamming-distant from the
nearest solution. This distance is also very strongly related to search cost. As the control
parameter is increased, the distance decreases. We suggested backbone fragility was the
cause of this pattern.
We defined a measure of backbone robustness. Backbone-fragile instances have low
robustness. We were then able to test predictions of the hypothesis that the fall in backbone
fragility is the cause of the overall decay in cost as the control parameter is increased. We
found that the hypothesis made three correct predictions. Firstly that the degree to which
an instance is backbone-fragile is correlated with the cost when the effects of other factors
are controlled for. Secondly, that when Random 3-SAT instances are altered so as to be
more backbone-fragile (by removing clauses without disrupting the backbone) their cost
increases. Thirdly, that the clauses most often unsatisfied during search are those whose
deletion has most effect on the backbone.
We now summarise our interpretation of the evidence. In the underconstrained region,
instances with small backbones are predominant. In this region, the rapid hill-climbing
phase typically results in an assignment which is close to the nearest solution (and probably
265

Singer, Gent & Smaill

satisfies the backbone). Since finding the small backbone is largely accomplished by hillclimbing, typical cost for WSat is low in this region and variance in cost is due to variance
in the density of solutions in the region of the search space where the backbone is satisfied.
In the threshold region, large-backbone instances quickly appear in large quantities.
For large-backbone instances, the main difficulty for local search is to identify the backbone
rather than to find a solution once the backbone has been identified. The identification of a
large backbone may be accomplished by the rapid hill-climbing phase to a greater or lesser
extent. We think that this extent is determined by the backbone fragility of the instance.
If a large-backbone instance is backbone-fragile the hill-climbing phase is ineffective and
results in an assignment which is Hamming-distant from the nearest solution (probably
implying that much of the backbone has not been identified). Then a costly plateau search
is required to find a solution. Hence when the rare large-backbone instances do occur in the
underconstrained region, they are extremely costly to solve because of their high backbone
fragility.
If a large-backbone instance is more backbone robust, the rapid hill-climbing phase is
more effective in determining the backbone and the plateau phase is shorter. So overall
the instance is less costly for WSat to solve. Hence for large-backbone instances, since
backbone fragility increases as we add clauses, cost decreases. In the overconstrained region,
large backbone instances are dominant and so backbone fragility becomes the main factor
determining cost. Hence cost decreases in this region. Our hypothesis proposes the following
explanation for the cost peak: Typical cost peaks in the threshold region because of the
appearance of many large-backbone instances which are still moderately backbone-fragile,
followed by the increasing backbone robustness of these instances.

Acknowledgments
This research was supported by UK Engineering and Physical Sciences Research Council
studentship 97305799 to the first author. The first two authors are members of the crossuniversity Apes research group (http://www.cs.strath.ac.uk/~apes/). We would like
to thank the other members of the Apes group, the anonymous reviewers of this and an
earlier paper and Andrew Tuson for invaluable comments and discussions.

266

Backbone Fragility and the Local Search Cost Peak

Appendix A: Randomisation and Bootstrap Tests
We summarise the methods as used in this context. Further explanation of these methods
is given in Cohen (1995).
A.1 Randomisation for Estimating the Correlation Coefficient due to the
Distributions of the Variables
Randomisation can be used to estimate the correlation coefficient between the two variables
which results simply from their distributions rather than from any relationship. We start
with the two vectors of data x̄ = hx1 , x2 , . . . , xN i and ȳ = hy1 , y2 , . . . , yN i. If the correlation
coefficient is merely due to the distributions of x and y, then it is not dependent on any
particular xi being paired with yi . Therefore to calculate the correlation coefficient resulting
merely from the distributions we pair the x and y data randomly.
We construct K randomisations. Each randomisation consists of a vector ȳ 0 , which is
simply a random permutation of ȳ. For each randomisation, we calculate the correlation
coefficient between x̄ and ȳ 0 – note that each value xi is now paired with a random value
from ȳ. These randomised correlation coefficients give us an estimate of the correlation
coefficients resulting from the distributions of the variables. If K is large enough, we will
have an accurate estimate which can be compared with the correlation coefficient in the
observed data.
A.2 Bootstrap Estimation of Confidence Intervals for Correlation Coefficients
We have an original sample h(x1 , y1 ), (x2 , y2 ), . . . (xN , yN )i of N pairs. A pseudo-sample from
the original also consists of N pairs. The j th pair in the pseudo-sample (xbj , yjb ) = (xq , yq )
where q is a random number between 1 and N . Each pair in the pseudo-sample is chosen
independently i.e. pairs are sampled from the original with replacement. We assume that
our original sample of pairs of data is representative of the whole population of such pairs.
Given this, composing pseudo-samples is just like sampling from the whole population.
Therefore by measuring the correlation coefficient of many pseudo-samples, we can study
what the correlation coefficient would have looked like had we taken many sets of samples
from the whole population. From the distribution of the correlation coefficient among many
pseudo-samples (the bootstrap sampling distribution) we can infer bounds on the confidence
interval for the observed correlation coefficients.
Many pseudo-samples are taken, and the correlation coefficient is calculated for each of
the pseudo-samples. This gives the bootstrap sampling distribution of the correlation coefficient. The 97.5th percentile of this distribution is an upper bound on the 95% confidence
interval for the correlation coefficient, and the 2.5th percentile is a lower bound.

Appendix B: The Relationship Between BMSs and MUSs
Let C be a satisfiable SAT instance and {l1 , l2 , . . . , lk } be the set of all literals entailed by
C. Let d be the clause ¬l1 ∨ ¬l2 ∨ . . . ∨ ¬lk .
Theorem C 0 is a BMS of C iff C 0 ∧ d is an MUS of C ∧ d 2
267

Singer, Gent & Smaill

Proof Suppose C 0 is a BMS of C. Then C 0 ∧ d, which is a sub-instance of C ∧ d, must
be unsatisfiable, as d violates every literal in the backbone of C 0 . If d is removed from C 0 ∧d,
the result C 0 is satisfiable. If any other clause c is removed from C 0 ∧ d, there must be some
literal from the backbone of C 0 , li say, such that (C 0 − {c}) ∧ ¬li is satisfiable. Therefore,
since ¬li is also a literal of d, (C 0 − {c}) ∧ d is satisfiable. Therefore C 0 ∧ d is an MUS of
C ∧ d.
Conversely, suppose C 0 ∧ d is an MUS of C ∧ d. Since C 0 ∧ d is minimally unsatisfiable,
0
C is satisfiable. Since C 0 is a sub-instance of C, the backbone of C 0 must be a subset of the
backbone of C. Suppose there were some literal lj which was in the backbone of C but not
in the backbone of C 0 . Then there would be a solution to C 0 ∧ ¬lj . This would then also be
a solution to C 0 ∧ d, since ¬lj is one literal of d. This contradicts C 0 ∧ d being unsatisfiable
and so there can be no lj i.e. C 0 and C must have the same backbone.
C 0 ∧ d is minimally unsatisfiable. Therefore for any clause c of C 0 , (C 0 − {c}) ∧ d is
satisfiable. Any solution to (C 0 − {c}) ∧ d must make some literal ¬lk of d true, and must
therefore also be a solution to (C 0 − {c}) ∧ ¬lk . Therefore lk , which is in the backbone of
C 0 , is not in the backbone of (C 0 − {c}). Hence C 0 is a BMS of C 2

References
Bayardo, R. J., & Schrag, R. (1996). Using CSP Look-Back Techniques to Solve Exceptionally Hard SAT Instances. In Proceedings of the Second International Conference
on the Principles and Practice of Constraint Programming, pp. 46–60. Springer.
Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). Where the Really Hard Problems Are.
In Proceedings of IJCAI-91, pp. 331–340. Morgan Kaufmann.
Clark, D., Frank, J., Gent, I. P., MacIntyre, E., Tomov, N., & Walsh, T. (1996). Local Search
and the Number of Solutions. In Proceedings of the Second International Conference
on the Principles and Practice of Constraint Programming, pp. 119–133. Springer.
Cohen, P. (1995). Empirical Methods for Artificial Intelligence. The MIT Press.
Cook, S. (1971). The Complexity of Theorem-Proving Procedures. In Proc. 3rd Ann. ACM
Symp. on Theory of Computing, pp. 151–158.
Cook, S., & Mitchell, D. (1997). Finding Hard Instances of the Satisfiability Problem: A
Survey. In Satisfiability Problem: Theory and Applications, Vol. 35 of DIMACS Series
in Discrete Mathematics and Theoretical Computer Science, pp. 1 – 18. American
Mathematical Society.
Crawford, J. M., & Auton, L. D. (1996). Experimental Results on the Crossover Point in
Random 3SAT. Artificial Intelligence, 81, 31–57.
Culberson, J., & Gent, I. P. (1999a). On the Completeness of WalkSAT for 2-SAT. Tech.
rep. APES-15-1999, APES Research Group.
Available from http://apes.cs.strath.ac.uk/apesreports.html.
268

Backbone Fragility and the Local Search Cost Peak

Culberson, J., & Gent, I. P. (1999b). Well out of reach: Why hard problems are hard. Tech.
rep. APES-13-1999, APES Research Group.
Available from http://apes.cs.strath.ac.uk/apesreports.html.
Franco, J., & Paull, M. (1983). Probabilistic analysis of the Davis Putnam procedure for
solving the satisfiability problem. Discrete Applied Math., 5, 77–87.
Frank, J., Cheeseman, P., & Stutz, J. (1997). When Gravity Fails: Local Search Topology.
J. Artificial Intelligence Research, 7, 249–281.
Gent, I. P. (1998). On the Stupid Algorithm for Satisfiability. Tech. rep. APES-02-1998,
APES Research Group.
Available from http://apes.cs.strath.ac.uk/apesreports.html.
Gent, I. P., MacIntyre, E., Prosser, P., & Walsh, T. (1996). The Constrainedness of Search.
In Proceedings of AAAI-96, pp. 246–252. AAAI Press / The MIT Press.
Gent, I. P., & Walsh, T. (1993). An Empirical Analysis of Search in GSAT. J. Artificial
Intelligence Research, 1, 47–59.
Gent, I. P., & Walsh, T. (1996). The satisfiability constraint gap. Artificial Intelligence,
81, 59–80.
Hogg, T., & Williams, C. P. (1994). The hardest constraint problems: a double phase
transition. Artificial Intelligence, 69, 359–377.
Hoos, H. (1998). Stochastic Local Search - Methods, Models, Applications. Ph.D. thesis,
Darmstadt University of Technology.
Hoos, H. (1999a). On the Run-time Behaviour of Stochastic Local Search Algorithms for
SAT. In Proceedings of AAAI-99, pp. 661–666. AAAI Press / The MIT Press.
Hoos, H. (1999b). SAT-Encodings, Search Space Structure, and Local Search Performance.
In Proceedings of IJCAI-99, pp. 296–302. Morgan Kaufmann.
Hoos, H., & Stützle, T. (1998). Characterising the Run-time Behaviour of Stochastic Local
Search. Tech. rep. AIDA-98-01, Darmstadt University of Technology.
Iwama, K., Miyano, E., & Asahiro, Y. (1996). Random generation of test instances with controlled attributes. In Cliques, Coloring, and Satisfiability, Vol. 26 of DIMACS Series
in Discrete Mathematics and Theoretical Computer Science, pp. 377–394. American
Mathematical Society.
Koutsoupias, E., & Papadimitriou, C. H. (1992). On the greedy algorithm for satisfiability.
Information Processing Letters, 43 (1), 53 – 55.
Larrabee, T., & Tsuji, Y. (1992). Evidence for a Satisfiability Threshold for Random 3CNF
Formulas. Tech. rep. UCSC-CRL-92-42, Jack Baskin School of Engineering, University
of California, Santa Cruz.
269

Singer, Gent & Smaill

McAllester, D., Selman, B., & Kautz, H. (1997). Evidence for Invariants in Local Search.
In Proceedings of AAAI-97, pp. 321–326. AAAI Press / The MIT Press.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and Easy Distributions of SAT
Problems. In Proceedings of AAAI-92, pp. 459–465. AAAI Press / The MIT Press.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999a). 2+PSAT: Relation of Typical-Case Complexity to the Nature of the Phase Transition.
Random Structures and Algorithms, 15, 414 – 440.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999b). Determining computational complexity from characteristic ‘phase transitions’. Nature, 400,
133–137.
Papadimitriou, C. H. (1991). On selecting a satisfying truth assignment. In Proc. 32nd
IEEE Symp. on the Foundations of Comp. Sci., pp. 163–169.
Parkes, A. (1997). Clustering at the Phase Transition. In Proceedings of AAAI-97, pp.
340–345. AAAI Press / The MIT Press.
Parkes, A., & Walser, J. (1996). Tuning Local Search for Satisfiability Testing. In Proceedings of AAAI-96, pp. 356–362. AAAI Press / The MIT Press.
Schrag, R., & Crawford, J. (1996). Implicates and prime implicates in Random 3-SAT.
Artificial Intelligence, 81, 199–222.
Selman, B., Kautz, H., & Cohen, B. (1994). Noise Strategies for Improving Local Search.
In Proceedings of AAAI-94, pp. 337–343. AAAI Press / The MIT Press.
Walsh, T. (1998). The Constrainedness Knife-Edge. In Proceedings of AAAI-98, pp. 406–
411. AAAI Press / The MIT Press.
Yokoo, M. (1997). Why Adding More Constraints Makes a Problem Easier for Hill-Climbing
Algorithms: Analysing Landscapes of CSPs. In Proceedings of the Third International
Conference on the Principles and Practice of Constraint Programming, pp. 356–370.
Springer.

270


	
 
			 ! #"$ % 
'&)( *,+-*...0/21035476!(

89:;<  =)( .0>
66!?A@:	%&=CB0>
..

DFE<GHEJILKJM<NPORQLSTE
UWVYX[Z]\_^a`]b9VacedgfhVa\_b
ijb0fkValnmpo$Va\qZHrsZHVa\qZ,Va`<ZutAvjfkw
xzy${|ZHral}vjl~ijb0fkVbVdgln^alnmvjlnrqaWb
iZHZ]\_al}`

s$#52F<2

 2¡2¢£2T¡j¤h¥§¦¨ª©$«T£¬ £2­T«$£#¨®¥

¯h°'±³²´µ¶L°·³µ¹¸ºC»§µ'²Aµ¼-½µ¼¾½n²· ¿k¯h°À±²A´µ¶Á°0·µz¸ ºLÂj¸A¶]± ÃÄµ'°´C» ¾¼°·§¾
°
Å¬·¼Æ°´
½¼µÇ|¸ ºÉÈ¸´5¸A·³µÀ¸Ê~È¸´5¸A·³µÀ¸ÊHË¹·³µÀ²´¼¸Ê]Â²·§²¿Ì²
ÍÄÎÎÏ¹Ð5ÑÑAÒÒÒ¹Ó
ÔÕÓ'ÖÄÎÄ×ÌØ×AÙÄÎ×JÓÚÔAÛÄÑÜ,ØÛÝÞ×ÌØÝÑ

ßáà~âÌãÄäåjæ§ã
çè!éAêÚë$éAì³íqî,è9ï
ðñÚèêkò ó9ôôÌõözïë÷éÌø®ùLè9íñÚð³éñHñ5ðè)úÌûÀü è9ý³éAê
éñÚø®þìaïê5ø®ñ5è9ê5ø÷þÌìÉÿ7þÌê~ï0þì³íÄø®ñ5ø÷þÌì³éÌë$ø®ìíÄè0û
ý èì³íèì³ïèHø÷ì|éÌïïë®ø-ï<ïé³üÚéAë³ì³è0ñzþÌêüjéÌë÷ü5þ)éÌýýë÷ø®è!üjñ5þ)ìè0ñzþÌêÄüjþAÿ$íÄø-ü5ïê5èñ5è	éÌê5ø-é
ë÷è9üjñ5ð³éAñ#ð³éè
§
ÿ7è9è9í
³éïÉïïë®è!üÄý³ê5þø-íÄè9í|ñ5ð³éAñ]ñ5ð³èéÌê5ø-é
³ë®è!ü¹þAÿñ5ðènüÄüñÚèù éÌê5èì³øè9ëaíÄè0ñÚèêÚùLø®ìè!í
|ñ5ðè
ê
éAì³íþÌù íø÷ü ñê
³éÌì³ï0è!ü<ü ðþ
ÉèéAùLýë÷è}ñ5ð³éAñ]ñ5ð³ø÷üHø÷üHìþAñ]ñÚê³ènø®ì èìè9êÚéÌë!#"þùhènïþÌì³íøªñÚø®þì
ü ñ5êÚþÌì$èê#ñ5ð³éÌì%ì³øè9ìè9üÚüzø÷üzìè9è9íÄè!í&ü'ï
ðÉéÌü#ñÚðè,èÄø÷ü ñ5è9ì³ï0è~þAÿékï9é³üÚéAë íì³éAùLø-ïüéAê
éAìñ5è9è9í
ñÚþÁë®è!éÌí|ñ5þhñ5ðè³ìøènü5þÌëÄñÚø®þì(

)*+-,.*/10&234657.89,;:<*/=,>5?890&54@0A*,CB*D2,.E=*0F0&23'4G57.89,C57@HI2/JEK2LM0&2346578N,POQ*7.2%*RL!57SR*/=E=,.S
LT57U7.2V-7.2,>20W3.E=0&XA3.Y(2[Z>5$E=0\3](E=,>3>7PEKH-+&3.EK5$0^5LQ*F_5$/=/K2_3.E=5$0`5LQ7*0-]&5$Sba*7E=*H-/=2,%E=0c3>27S?,U5LQ3.Y&2
_5$0(](E=3.EK5$0(*/[](E=,.3>7EKH-+&3.E=5$0(,?LT57d2*_Yea*7E=*H-/K2^X$EKa20fa*/=+(2,gL!57hEK3.,jiV-*7.20W3.kla*7E=*H-/=2,monQY&2
,>3>7+-_3.+&7.2	5L93.Y&2p]-E=,>3>7EKHq+&3.EK5$0rE=,s7.2V-7.2,>20W3>2]tX7*V-Y-E=_*/=/KD@HWD[*C0(23'4G57.8tE=0[4CY-E=_Yt0(5N]&2,17.2V(7.2,>20W3
a*7E=*H-/K2,r*0(]^*7.754C,[*7.2

]&7P*4C0^LT7.5$SuV-*7.20W3[0&5N](2,@3>5d_PY(E=/=]^0&59]&2,mtnCY&2,>2

*7.7.5v4C,r3'DNV-E=_*/=/=D

_57.7.2,.VI5$0-]R3>5;_*+(,.*/I7.2/=*3.E=5$0(,.Y(EKVq,mxw0R3.Y(2y,>3.*0-](*7]zL!57S+(/=*3.E=5$0s{\3.Y&2r0&234657.8|E=,G0&53G*/J/K54G2]
3>5RY(*a2U](E=7.2_3>2]g_DN_/=2,m
Y&20R*%](E=,>3>7E=H-+&3.EK5$0E=,#,>V~2_E-2];HND
}

3>5g]&23>27S?EJ0&2%3.Y(*3r5$0&2

,.+(_PY?*t0&234657.8I{N3.Y&2C',.2V-*7*3.EK5$0z_7EK3>27EK5$0R*/=/K54@,#5$0&2

,>23r5L	7P*0(]&5$Sua*7PE=*H-/K2,{~U{~EJ,@_5$0(]-EK3.EK5$0(*/=/=DEJ0(]&2V~20(]&20W3Q5L6*0(53.Y&27

,>23#5Lq7*0(]&5$Sa*7E=*H-/K2,{|{X$EKa20;a*/=+&2,LT57	*y3.Y(EK7],>23#5L-7P*0(]&5$Sa*7PE=*H-/K2,	;mnQY(EJ,M_7E=3>27EK5$0
E=0Wa5$/Ka2,@5$0(/KD|3.Y&2%V(72,>20(_2t57r*H-,>20(_2U5L#*7.7.54@,@E=0d3.Y&2U0&2346578q{I0&53C3.Y&2]&23.*E=/K2]A0W+(S?27E=_*/
,>V~2_Eq_*3.E=5$05L3.Y(2t_5$0(](EK3.EK5$0-*/s](E=,>3>7E=H-+&3.EK5$0(,mN22%2*7/6:$OLT57C*?]&23.*EJ/K2]g](E=,._+-,.,.EK5$0sm
2*7/*0(]@2_PY\3>27F:$OY(*a2g*3>3>2S

V(3>2]3>5F2N3>20-]3.Y-E=,tLT7*S

24G57.8`3>5^0&23'4G57.89,3.Y(*3

S?*DF_5$0W3.*E=0c]-EK7.2_3>2]_D9_/K2,{14CY-E=_Y_57.72,>V~5$0(]F3>5dL!22](H-*_.8^7.2/=*3.E=5$0(,.Y(EKVq,@*S

5$0(Xga*7EJ*H-/K2,m

Y&20`_D9_/K2,t2&E=,>3{M3.Y&2rZ>5$E=0\3t]-E=,>3>7EKHq+&3.EK5$0FE=,[0&5A/K5$0(X27%,>V~2_E-2]^E=03>27PS?,[5L3.Y&2
}

V(7.59](+(_3[5L

_5$0(](E=3.EK5$0(*/~](E=,>3>7PEKH-+&3.EK5$0-,	LT57C_Y(EJ/=]&7.20|X$EKa20gVq*7.20\3.,{&H-+&37*3.Y&27CHWD|,.*D9E=0&X?Y&5v4f3.Y&2[a*/=+&2,5L
3.Y&2U5H-,>27.a*H-/K2ta*7E=*Hq/K2,{(

(  I{q*72U]&23>27SRE=0&2]dHND3.Y&2%a*/=+&2,CL!57r*R,.23C5L	+(0&5Hq,>27.a2]
7*0(](5$S](EJ,>3.+&7.H-*0-_2,{s ( MI{q4CY-E=_YF*7.2;*,.,.+(S 2]A3>5HI2E=0(]&2V~20(](20\35L	2*_PY^53.Y&27[*0(]

3>5UY-*a2@,.VI2_EK-2]?](E=,.3>7EKH-+&3.E=5$0(,mx&57G2*_Yza*7E=*H-/K2{\
E=,2N+(*/s3>5|,>5$S

{N*0?2N+(*3.EK5$0zE=,#X$EKa20z,>V~2_EKL!D9E=0&X[3.Y-*3pEK3

2tL<+(0(_3.E=5$0g5L3.Y&2%_57.7.2,.VI5$0-](E=0&X|  *0(]d5L,.5$S

2%,>23@5LVq*7.20\3Ca*7EJ*H-/K2,LT7.5$S

*S 5$0&XR3.Y&2[%[4CE=3.Y?d¡
   mp¢@,QHI2LT57.2{&V-*720\3'_PY(E=/=]g72/=*3.EK5$0(,.Y-EKV-,G*7.2t7.2V(72,>20\3>2]dX7*V-Y(E=_*/J/KD
HND|]&7P*4CE=0(X2](X2,Q4CEK3.Yg*77.54C,CL!75$SoV-*7.20W3Q0&59]&2,3>5R_PY(E=/=]g0&59]&2,m
w0?57](27	3>5US?*82y3.Y(E=,p,._Y&2S?2C4G2/=/']&2q0(2]s{$2*7/-*0(]zy2_YW3>27G72W+(E=7.23.Y(*3pLT576*0WD

a*/=+&2,

5LM 
* /=/93.Y&22N+(*3.EK5$0-,	*7.2
(  U3.Y&272QE=,29*_3./=D5$0(2Q,>23	5LIa*/=+(2,L!57	 (  ULT57	4CY(EJ_Y 
,.*3.E=,>-2]smpwL3.Y(E=,Q+-0(E=N+&20&2,.,_5$0(]-EK3.EK5$0gE=,@,.*3.E=,-2]1{(*R](E=,.3>7EKH-+&3.E=5$0R5va27U 
(  M|4CE=/J/s]&2q0&2
£  *...z§ %%q¤!=Ún
=¥C0
G¦j
!®;]
p§§:!	%&%'¨³		Ì&0%2%J©0 =v¨

ªy« -¬

*t](EJ,>3>7EKH-+(3.EK5$0t5va276 (  Imp­r0&2_*0?3.Y(20?*,>84CY(*3	_5$0(](E=3.EK5$0(*/&E=0(](2VI20-]&20(_2GV(7.5V~27.3.EK2,
3.Y(E=,]-E=,>3>7EKHq+&3.EK5$0|S?EKX$YW3VI5$,,>2,.,m
¢y__57](E=0(X3>5cnQY&257.2S¯®^5LrM2*7P/C*0(]ly2_YW3>27`:$OP{CEKL@3.Y&2d

[*7.2d*/=/Q]-E=,._7.23>2{G3.Y&2

a*7E=*H-/K2,Uo*7.2z_5$0-](EK3.EK5$0(*/J/KDhE=0-]&2V~20(]&20W3[5L3.Y&2Ra*7E=*H-/=2,%°X$EKa203.Y&2Ra*7E=*Hq/K2,±EKL3.Y&2
a*7E=*H-/K2,C²',>2Vq*7*3>2%3.Y&2[a*7E=*Hq/K2,³*0(]g|m	nQY&2t',.2V-*7*3.EK5$0h_7EK3>27EK5$0g_*0AH~2y29V(7.2,,>2]
E=0^3>27SR,[5L3.Y&2

L!5$/J/K54CEJ0&XgS?*0(EKV-+-/=*3.EK5$0(,@5L3.Y&2

X7*V-Y4@EK3.Y0&59]&2,t_57.7.2,.VI5$0-](E=0&X|3>5d3.Y&2?



*0(]g4@EK3.Yg*7.7.5v4C,LT7.5$SoV-*7.20W3.,Q3>5z_Y(E=/J]&7.20s´
Oy2/K23>2*/=/s0&59]&2,LT7.5$So3.Y&2[X7P*V-Yg2&_2V(3Q3.Y&5$,>2UE=0g%{qz{&57rµ*0(]g3.Y(2EK7Q*0(_2,>3>57,m
®$Od)5$0(0&2_3QHND|*0d2]&X2t2a27.DgV-*E=75L0&59]&2,3.Y-*3C,.Y(*7.2t*R_5$S?S

5$0d_PY(E=/=]sm

¶ O·C2S 5a2t*77.54C,GLT7.5$S¸*/=/-3.Y(2@2]&X2,p¹ºE»m¼2mK{97.2V-/J*_2C2*_Yg]-EK7.2_3>2]|2]&X2yHWDR*0+(0-](EK7.2_3>2]
2](X2m
wL{~E=0h3.Y&2U72,.+(/K3.E=0(X?X7*V-Ys{~*/=/MV-*3.Y(,@L!75$Su*|0&59]&2UEJ0hµ3>5*0&59]&2UE=0h½V-*,,C3.Y&75$+&X$Yh*0&59]&2
E=0A

{&3.Y&20hµ$',>2V-*7*3>2,yeL!7.5$S±zm
EKX$+&72^,.Y&5v4C,*0¡29*S

_5$+(0W3>27.2&*S

Vq/K2h5L*]-E=,>3>7EKHq+&3.EK5$0j]&2I0&2]¾EJ0¿3.Y(EJ,z4*D{y4CY(EJ_Y¡,>27.a2,g*,g*

V-/K2d3>5c3.Y&2A_/=*E=SÀ3.Y-*3z',.2V-*7*3.EK5$0jEJS

V-/=EK2,;_5$0(]-EK3.EK5$0(*/E=0-]&2V~20(]&20(_2|L!57|*0\D

0&234657.8F,.*3.E=,>LTD9E=0&X?3.Y&2;+(0(EJW+&20(2,.,Q_5$0(](E=3.EK5$0sm@nCY&2%a*7E=*H-/=2,CE=0h3.Y(E=,C29*S
5L6Á|57

m%nQY(2zp*7.2

Vq/K2*/J/13.*82

E=0(](2VI20-]&20\3y*0(]^*7.2;2W+-*/=/KDd/=EK82/=Dg3>5gHI2;Á|57?mtnQY&2;

a*/=+&2,

6,*3.E=,>LTDd3.Y&2

2N+(*3.EK5$0(,	,Y&54C01{\E=04CY(E=_PY

*](](EK3.E=5$0*0-]?SU+-/K3.EKV-/=EJ_*3.EK5$0U*72Q]&5$0&2QS 59](+(/K5t® :<E»m¼2mK{NE=0RÂ * OPm	Ã@53>2
3.Y(*3r * {~ B {~Ä{(*0(] 3 ]&5?0&53Q*V(V~2*7QE=0|3.Y&2%2W+(*3.E=5$0(,{&*0(]dY&20-_2[V-/=*D|0&5 7.5$/K2[E=0g](2q0(E=0&X
(
3.Y&2%]-E=,>3>7EKHq+&3.EK5$0?LT57Q ( 
 v  3 m
nQY(2%0&23'4G57.8d*0(]d3.Y&2%2N+(*3.EK5$0(,@_/K2*7/KDY(*a2%3.Y&2U7.2W+-EK7.2]g,>D90\3.*_3.EJ_tLT57Sgmnx5R,Y&54e3.Y(*3
3.Y(E=,yE=,y*Ra*/=E=]h2&*S


LT57G3.Y(2@

V-/K2{~EK3rE=,@*/J,>5|0(2_2,.,.*7.Dd3>5g,.Y(54³3.Y(*3r3.Y&2z  +(0-E=W+(2/KD|](23>27S?E=0&2a*/=+&2,

m6­r0&2y_*02*,.E=/KD?_5$0&-7S¸3.Y(*3GL!57*0WDRa*/=+&2,65Lx3.Y&2UM13.Y(2yLT5$/=/K5v4CE=0&X%a*/=+(2,6LT573.Y&2

4CE=/=/~,.*3.EJ,>L!D|*/=/s3.Y&2[2N+(*3.EK5$0-,´


 (
Å6Æ¾Ç


(


*




B



Å


?Ç


Ç

?Ä


Á


Á



Å6Æ¾ÇpÆ¡ (
Å


3

nM5R,>22[3.Y(*3Q3.Y-E=,E=,3.Y&2[5$0-/KDz,>23C5LMa*/=+&2,LT57Q3.Y&2[

3.Y(*3@,.*3.E=,>LTDz*/=/~3.Y(2t2W+-*3.EK5$0(,{(0&53>2t-7,>3

* Æ¾ Å Æ¾ Ç SU+(,.3tH~2RÁ9{#,.E=0(_2RE=LGEK3UE=,%E=0-,>3>2*]¿{3.Y&20 Ä 
4CY(EJ_Y^E=,rE=S V~5$,.,.E=H-/K2m[ÈC20-_2 ?Ä   3  Á9m;&E=0(_2; Å  Å *0(]F?Ç

3.Y(*3U





Æ R*0(]c 3   Ä {
3 
Ç{s462 */=,.5d,>22;3.Y(*3

ÉËÊqÌ~ÍÎpÎÏvÐPÑÒvÓÔÎpÕÖË×ÓÙØ[ÍÐ>ÚÎ	ÛÎÎÜ%Ý»ÞÔÑÒvÓÙÞ¼ßÎ>Ø[ÐCÛvÞ¼àÛárÖPÑÞÔà»àÞÔÜvâ@ã~äÐPÜØ[åCä#ÐPÝxæ~ÎÓÔÓKç\Û×àè1ÍÐ>ÚÎ	éÎÒvàxàÍÎÑ¿ÞÔÜ
ÖPê'ØÎê1àÖÝ»ÍÖ>æAàÍÐà1àÍvÎpÕÖË×ÜàÎêÎÏÐÑÒÓÔÎ#ØÖÎÝMÜvÖPàØvÎÒÎÜØ@ÖPÜ[×Ý»Î#ÖëqÝ»×vÕÍrÐØvÎâPÎÜÎê'Ð.àÎ	ÜÎà!æ~ÖPêéÊsìÜÎ#Õ>ÐÜ
Î>ÐÝ»ÞÙÓ¼áyÑQÐPéÎxàÍÎ#ÎÏvÐPÑÒvÓÔÎÓÔÎÝ»ÝØÎâËÎÜÎê'Ð.àÎpÝ<àÞÔÓÔÓNÛáQêÎßÜÞÔÜâGàÍvÎ#Ý<à'ÐàÎ#ÚPÐêÞÙÐPÛvÓÔÎÝçÝ»ÒvÓÙÞ¼à»àÞÔÜâÝ<à'Ð.àÎpíGÞÔÜàÖGÝ<à'ÐàÎÝ
íÐÜØyíî&ÐPÜØyÝ<à'ÐàÎÉ	ÞÔÜàÖÝ<à'Ð.àÎÝ#ÉpÐPÜØtÉîJÊIÌ~ÍvÎ	Õ×vê»êÎÜàÓ¼áC×Ü×Ý»ÎØ[ã&ï-Õ>ÐÜyàÍÎÜrÛÎ#ÐPÓÔÓÔÖ>æ~Î>ØràÖÞÔÜð×ÎÜÕÎàÍvÎ
Õ'ÍÖËÞÔÕÎ#ÛÎàTæ~ÎÎÜríÐÜØríî9ÐÜØ@ÛÎàTæ~ÎÎÜÉ	ÐÜØ%Éî!çæ1Þ¼àÍyàÍÞÔÝ1ÕÍvÖËÞÔÕÎ#ÍÐ.ÚÞÔÜâ6ÜÖÎñ$ÎÕàxÖËÜCàÍÎ#ÖPàÍvÎê1ÜÖØÎÝÊ
òò

ó ­Fô « ¡2©2¥Nõ­-ö`÷~£¬­$¡-õ-«&õ7£¬­T-¬;ø
­$¡ «$ù\« ­$¡ « ­2¥ «

U1





B Æ
(





* Æ
(

Å


MÅ

?Ç


Ç

Ä


:T

* Æ

Å Æ



:T

* Æ

ÅpÆ?ÇOú?Ä

X3



X4



B

U5
X2


*

X1
U4

 (


(

3

Ç Oú\:T

3 ÆûO

X5
X6

X7

EKX$+&7.2?´rüy7*V-Y(EJ_*/C,>3>7+-_3.+&7.2A*0(]¾2N+(*3.EK5$0(,RL!57|3.Y&2h_5$+-0\3>27.2&*S

V-/=2mý¢@/J/Qa*7E=*H-/K2,?3.*82

a*/=+(2,tE=0ÿþÁ  $m|nQY(2g  *7.2|E=0(]&2V~20(]&20W3{s4CEK3.Y`2W+(*/pV(7.5H-*HqE=/=EK3.EK2,@LT57UÁA*0-]jm
¢@](]-EK3.EK5$0R*0-]zS+(/K3.EKVq/=E=_*3.EK5$0?EJ,6]&5$0(2@S 59](+(/K5;®Nm	nQY&2%Mx*0(]z3.Y&2r]&53>3>2]g*7.7.5v4C,*7.2
0&53QLT57S?*/=/KDRV-*7.3Q5L3.Y&2tX7*VqYs{&H-+&3Q*7.2t,Y&54C0dL!57C_/J*7EK3'Dm

* Æd Å Æd Ç  Á%E=S Vq/=EK2,3.Y(*3p *   Å Æ Ç {\LT7.5$Sµ4@Y(E=_YREK3	LT5$/=/K54@,#3.Y(*3	 B   Å Æ Ç Æ ( {
,.E=0-_2[ (   ( m
 23+(,U0&54 _5$0(,.EJ]&27%4CY(23.Y&27%57;0&53 Åz=E ,U_5$0(](EK3.EK5$0-*/=/KDhEJ0(]&2V~20(]&20W3[5L?Ç?X$EKa20 * m
} 2 _*0^,>22U3.Y-*3@ Å;*0(]h?Ç%*7.2;',>2Vq*7*3>2]^HWDg * {~,.E=0-_2UE=0F,>3>2Vl:OC*H~5va2{s4624CE=/=/x]&2/K23>2



?ÄC*0-]?

3 *0-]z*/=/-3.Y&2C2](X2,G_5$0-0&2_3.E=0&XU3>5U3.Y&2Sd{N/K2*aNEJ0&XU0&5UV-*3.YzL!7.5$S  ÅQ3>5?Çm	ÈC5v462a27{
X$EKa20l*0WDca*/=+&2|L!57; * {	3.Y(2za*7E=*H-/=2,U Å *0(] Ç *7.2gE=0cL<*_3?]&2V~20(]&20W3m¢@, ,.220*H~5va2{

*0WD,.23Q5La*/=+(2,L!57C3.Y&2t

#,.*3.E=,>LTD9E=0&X;3.Y&2t2N+(*3.EK5$0(,CSU+(,.3QH~2t,.+(_Yd3.Y(*3Q

È@20(_2{xEKL6

Æ  ÅGÆÿ?Ç  Á9m
* 
{s3.Y&20cEK3yS+(,>3[H~2

3.Y(*3t

Å?E=,[]&23>27S?EJ0&2]^HWD

*  Á9{x3.Y&20EK3[SU+-,>3rHI2;3.Y(*3r Å   Ç {x*0(]E=LpE=0(,>3>2*]F * 
?

ÇÆemz)G5$0-](EK3.EK5$0(*/5$0`*a*/=+&2 LT57[ * {x4G2 3.YN+(,t,>22?3.Y-*3[

?Ç{(,.Y&5v4CE=0&X 3.Y-*3Q3.Y&2D*7.2%0&53C_5$0-](EK3.EK5$0(*/J/KDRE=0(](2VI20-]&20\3m

Å

nQY(2V(7.5H-/K2Sº*V(V~2*7,g3>5¿*7EJ,>2`H~2_*+(,>2`2a20ý3.Y&5$+&X$Ye,>V~2_EKL!D9E=0&Xa*/=+&2,gLT57h*/=/t3.Y&2lM
{,.VI2_E=L!D9E=0&Xg*ha*/=+(2?L!57R ( */=5$0&2z/K2*a2,^,>23.,
5La*/J+&2,[LT57z:T (   *   B O@3.Y(*3,.*3.E=,>LTDh3.Y(2?2W+-*3.EK5$0(,t*,.,.5N_E=*3>2]c4CEK3.Y^3.Y&2,.2?a*7PE=*H-/K2,{x2a20
3.Y&5$+&X$Y¿5$0(/KDÿ5$0&2A,.23R5L[a*/=+&2, 4CEJ/=/,.*3.E=,>LTDÿ3.Y&2d20W3.EK7.2F,>23R5Lr2N+(*3.EK5$0(,m } Y(E=_PYla*/J+&2,?LT57
+(0(EJW+&2/=Dh]&23>27SRE=0&2,ta*/=+&2,%L!57*/=/	3.Y(2R

:T

(   *  B O *72dV-*7.3?5Lr3.Y&2A5a27*/=/@,>5$/=+&3.E=5$0l]&2V~20(](, 5$0l3.Y(2da*/J+&2d5LMÅ[Æ³Çv{*0(]j3.Y(E=,

E=0(]-+(_2, *`]&2V~20(]&20-_2|H~2346220¿ Å 
MÅd*0(]ÿ?Ç 
Ç4CY&20ÿ3.Y&2da*/=+&2g5Lr * E=,;890&5v4C0sm

nQY&2[72S

5a*/s5LV-*7.35L3.Y&2[X7*V-YAE=0,>3>2V:OG5L3.Y&2tV-7.5N_2]-+&7.2rL!57Q](23>27S?E=0(EJ0&X

',>2V-*7P*3.EK5$0

2/=E=SRE=0(*3>2,*0WD|V~5$,.,EKH-E=/=E=3'D;5L#*__5$+(0\3.EJ0&X?LT57Q3.Y(E=,]&2V~20(](20(_2m
nQY(2[V(7.5H-/K2S

4@EK3.Yg2*7/1*0(]dy2_PY\3>27
	Ô,@V(7.5N5L*V(V~2*7,Q_5$0(0&2_3>2]A4CEK3.Y3.Y(EJ,m	nQY&2D,.*D{

¢3C3.Y(E=,V~5$E=0W3G4G2%E=0Wa582t3.Y&2[L<*_3Q3.Y(*3C3.Y&2t_5$0(,>3>7*EJ0\3.,C5L#)û*72t0&53Q*7.H-EK3>7P*7.DRH-+(3
*72

LT+(0-_3.EK5$0(*/»{10(*S?2/KD{sL!57%2a27.Dha*/=+&2,r5LI3.Y&2
ò ô

V-*720\3.,[5L-p*0(]-G3.Y&27.2?EJ,

ªy« -¬

*A,>5$/=+&3.EK5$0^LT57qmRnCY(E=,rE=S

V-/JEK2,y3.Y(*3{xLT57%*0WD^,.23

*,,>5N_EJ*3>2]l4CEK3.Yÿ0&5$09'*0-_2,>3>57,;5L



5La*7E=*H-/=2,{s3.Y&2?2N+(*3.EK5$0(,

]&50&53?_5$0(,.3>7*E=03.Y&2V~27S?EK3>3>2]a*/=+&2,5L

mmm

nQY&2U29*S?V-/K2UY(27.2U,.Y(54C,@3.Y(*3C3.Y(2U2N+(*3.EK5$0(,yE=0Wa5$/Ka9E=0&X?0&5$09'*0-_2,>3>57,y5L
,>3>7*EJ03.Y&2tV~27S?EK3>3>2]a*/=+&2,QL!57

_*0^E=0(]&22]A_5$09

m

 +(]&2*M2*7P/;:TV~27,.5$0(*/@_5$SRSU+(0-E=_*3.EK5$0qORY(*,,.+&XX2,>3>2]û3.Y-*3z3.Y&2$',>2V-*7*3.EK5$0û_7E=3>27EK5$0
_*0dH~2%,*/Ka*X2]AHWD7.2N+(EK7PE=0&X

0&53C5$0-/KDz3.Y-*3t 
( v M+(0(E=N+&2/KD|]&23>27S?E=0(2[ (  q{qH-+&3

g

_*0hH~2t5H(3.*E=0&2]dHWDd* V-7.5N_2]-+&7.2%E=0d4CY(E=_PYg3.Y&2
(

*/=,>5R3.Y(*3C3.Y(EJ,C+(0(EJW+&2[,>5$/J+&3.EK5$0gLT57C


*7.2?+&Vs](*3>2]E=0*__57P](*0(_2R4@EK3.YF3.Y&2R_*+(,.*/p,>3>7+(_3.+&72
]&D90(*S?E=_*/qV(7.59_2](+&72{W2*_PYg
_57.7.2,.VI5$0-](E=0&Xg6*0(]F3.Y&2

5L63.Y&2?0(23'4G57.8qmRw0`,+(_Y`*g_*,+(*/

ME=,67.2V~2*3>2](/KDR7.2V-/=*_2]|HND?3.Y&2ra*/J+&2y_5$S?V-+&3>2]|L!57QE=36LT7.5$S¸3.Y&2

_+&77.20\3ya*/=+&2,y5LpEK3.,yV-*7.20W3.,{1*__57]-E=0&X|3>5|3.Y(22N+(*3.EK5$0FLT57r3.Y(*3

{-+(0W3.E=/s*?,>3.*H-/=2t,>3.*3>2%E=,72*_Y&2]sm6nQY&2-5v4ý5LE=0&LT57S?*3.EK5$0gE=0g,+(_Yd*?V(7.59_2](+&7.2rLT5$/=/K54@,3.Y&2


](EK72_3.EK5$0A5Lp3.Y&2*77.54C,rE=0A3.Y&2;0&23'4G57.8Imt)5$0(,>2N+&20\3./=D{~0(5N]&2,@3.Y(*3r*7.2

0&53r*0(_2,>3>7*/3>5g*0\D

0&59]&2;5LE=0\3>272,>3t_*0`Y(*a2R0&5dE=0I+&20(_2;5$03.Y&2,>2?0(5N]&2,{9Z>+-,>3.EKLTDNE=0(Xz3.Y&2E=7[2/=E=S?E=0-*3.EK5$0hEJ0,>3>2V
:OG5L3.Y&2tV-7.5N_2]-+&7.2rL!57@]&23>27S?E=0-E=0&X;',>2V-*7P*3.EK5$0sm
w0|X20&27*/»{&4CY(23.Y&2757Q0&53,.+(_PYg*](DN0(*SRE=_*/IV(7.59_2](+&7.2y2a20\3.+-*/=/KD?I0(](,63.Y&2t,>5$/=+&3.E=5$0RLT57


(    S?*DR]&2V~20(];5$0z4CY&23.Y&2763.Y&2Q 1*7.2y+&Vs](*3>2]?,.E=S+(/K3.*0(25$+(,./KD576,>2N+&20W3.E=*/=/KD{N*0(]

EKL3.Y&2Dg*7.2+&Vs](*3>2]d,>2N+&20W3.E=*/=/KD{q5$0d3.Y&2U57]&27C5L3.Y&2,.2U+&Vs](*3>2,m&57yV(7.2,.20\3@V-+&7.V~5$,>2,{(E=3CE=,

,.+|_EK20\363.Y(*3!t+&Vs](*3.E=0&X%,_Y&2S

2r2&E=,>363.Y(*3E=,pX$+-*7*0\3>22]1{NLT57*0\DRa*/=+(2,p5L# 
(  MI{
3>5l/K2*]¾3>53.Y(2F+(0-E=W+(2A,>5$/=+&3.E=5$0s{Q,>3.*7.3.E=0(Xc4CEK3.Y¡*0WD¿E=0(EK3.EJ*/Qa*/=+&2,zL!57| (  ImewLt,.+(_PY
*0+&Vs](*3>257]&27r2&E=,>3.,{1$',>2V-*7*3.EK5$0^4@E=/=/xEJS
*H~5a2{s*0WDA+&Vs](*3.E=0&X
a*/=+&2,?5L[?Äh*0(]j
_5$0(](E=3.EK5$0sm

57P]&27@4@E=/=/sLT57y,.5$S
3

V-/KDg_5$0(](E=3.EK5$0(*/E=0(]&2V~20(]&20-_2myw0F3.Y&2;29*S?V-/K2

2EJ0(EK3.E=*/s,>3.*3>2

/K2*]h3>5_DN_/=EJ_tH~2Y(*a9EK5$+&7yE=0d4@Y(E=_YA3.Y&2

qEKVjHq*_.8j*0(]¾L!573.Ys{,>5c3.Y&2h29*S?V-/K2h](5W2,|0&53z,*3.E=,>LTD3.Y-E=,R,>3>75$0&X27

­r0(2%,.Y&5$+(/J]g0&53>2%3.Y(*3@3.Y&2%2&*S

V-/=2%E=0g3.Y(EJ,Q0&53>2U](5W2,@0&53CE=0Wa*/JE=](*3>2[3.Y&2U7.2,.+(/K35L69V-EK7.3>2,

:#"$O@3.Y(*3y',>2V-*7P*3.EK5$0^_*0AH~2U+-,>2]d3>5]&23>27S?EJ0&2%_5$0(](E=3.EK5$0(*/1E=0-]&2V~20(]&20(_2tE=0h/=E=0&2*7@0&23
4G57.8N,R5Ly0(57S?*/=/KD](E=,>3>7PEKH-+&3>2]a*7E=*H-/K2,;2a20¡EKL@3.Y&2Dl_5$0\3.*E=0¾_DN_/=2,m¿w0l3.Y&2A,*S
NV-E=7.3>2,[*/=,>5gX$*a2z*g_5$+(0W3>27.2&*S

V-/K2R,Y&54CEJ0&X3.Y(*3%',.2V-*7*3.EK5$00&22]0&53%E=S?V-/KDA_5$0(]-EK3.EK5$0(*/

E=0(](2VI20-]&20(_2RE=00&5$09'/JE=0&2*7U0(23'4G57.8N,;5L@_5$0\3.E=0N+&5$+(,U7*0(](5$S
nQY(EJ,

V(7.5H-/=2S

2dV-*V~27{

_*0(0&53zHI2h*a5$E=]&2]¾HNDÿ,E=S

a*7E=*Hq/K2,t3.Y(*3

_5$0W3.*E=0ÿ_DN_/=2,m

V-/KD](E=,._723.E%$E=0&X3.Y&2F_5$0\3.EJ0W+&5$+-,?a*7E=*H-/K2,{G*,z3.Y&2

V(7.5Hq/K2S±72*V(V~2*7,rE=0A3.Y(2LT57Su5L60(5$0929EJ,>3>20(_257t0&5$09'+(0(EJW+&20(2,.,Q5LG,>5$/=+&3.EK5$0-,m[nQY(E=,@0&53>2
,.Y&5v4C,#3.Y(*36E=0

0&5$09'/=EJ0&2*7	0&23'4G57.89,	5L~](E=,._723>2Qa*7PE=*H-/K2,#*[,>3>75$0&X276_5$0(]-EK3.EK5$0;3.Y(*0?+-0(E=N+&20&2,.,

E=,7.2N+(EK72];L!57p$',>2V-*7*3.E=5$0?3>5tH~2a*/JE=]sm¢@/K3.Y(5$+&X$Y

,.+(_PY?*t,>3>7.5$0(X27p_5$0(](EK3.E=5$0

]&D90(*S?E=_,;_*0lH~2,>220j*,?0-*3.+&7*/»{	3.Y(20(22]ÿ3>5a27EKLTDc3.Y(E=,
3.Y&2*3>3>7*_3.EKa20&2,,

E=0\a5$/=aNE=0(Xr_*+-,.*/

,.3>7.5$0&X27?_5$0(](E=3.EK5$0ÿ]&5W2,;72](+(_2

5LC0&234657.89,;4CEK3.Y_D9_/K2,;*,*F4*D5LQL!57PS?*/=E%$E=0(XA_*+(,.*/,.EK3.+-*3.EK5$0(,U4CEK3.Y

LT22]&H-*_.8Im

ß

æ'&)(*,+.-0/,132546/7(Jãâ

wC3.Y-*0&8h23>27NVqEK7.3>2,r*0(]^nQY&5$S?*,[·@E=_Y-*7](,>5$0FLT57tY&2/KV(L<+(/x_5$SRS

20\3.,m;nCY(E=,@4657.8h4G*,%,.+&V&

V~57.3>2]HND^3.Y&2zÃy*3.+&7*/9_EK20(_2,U*0(]98p0(X$E=0&227E=0(X·C2,>2*7_PYl)5$+(0(_E=/5LC)*0(*]-*d*0(]`HND^3.Y&2
w0(,>3.EK3.+&3>2[LT57C·C5HI53.EJ_,Q*0(]gw0\3>2/J/=EKX20W3Q9DN,>3>2SR,m

ô;:

ó ­Fô « ¡2©2¥Nõ­-ö`÷~£¬­$¡-õ-«&õ7£¬­T-¬;ø
­$¡ «$ù\« ­$¡ « ­2¥ «

< />=?/$ä'/,(,æ/2â
2*7/»{  mt:$OPmA@CBDFEHGFEJILKMILNIPOQRHGJS>ITSVU6ILSXWYS,0KTKZI%UVJS,\[,]0^`_a
Bcbde?fg@CK%GhiYIjEKk

WYS
f
BHS7ONmG&*0mlh*3>25&{s)*/=EKLT570(EJ*9´nlh57.X$*0.ot*+&L<S?*0(0sm

2*7/»{  m*0-]y2_PY\3>27{N·tm(:$OPm	w]&20W3.EKL!D9E=0&X@E=0(]&2V~20(]&20-_EK2,xE=0_*+(,.*/9X7P*V-Y(,4CEK3.YUL!22](H-*_.8Im
w0!8QmNÈC57aNEK3D$r*0(]z6m  20-,>20h:T2](E=3>57,PO\pqS7ONBjGILS7]rILStsBI uOJIPGK>WYS,0KTKZI%UVJS7OY;^@vB?;OYccwITSVU

?fxTyz|{5K fNTye})S
f
BHS~OYmlA57X$*0got*+&L<S?*0(01{NV-V1mz\®Áj\®Nm

NV-E=7.3>2,{mW:#"$OPm	rEK7.2_3>2]%_DN_/=EJ_X7*V-Y(E=_*/72V(7.2,>20W3.*3.EK5$0(,s5L9LT22]&H-*_8rS?5N]&2/J,m1w0[MmvB62,0(*7]
ImvÈy*0&89,:T2](EK3>57,POp5S~OYBjGITS,]ITSsBI uOJIPGKFWYS,0KTKZI%UVJS7OY;^@CBD
ONcHw#ILSVU#R?fvTyvK%JJS,Ty
})S
f
BHS~OYmlA57X$*0got*+&L<S?*0(01{NV-V1m\9Ëj\Nm

*0(]

ôó

Journal of Artificial Intelligence Research 12 (2000) 149–198

Submitted 11/99; published 3/00

A Model of Inductive Bias Learning
Jonathan Baxter

J ONATHAN .BAXTER @ ANU . EDU . AU

Research School of Information Sciences and Engineering
Australian National University, Canberra 0200, Australia

Abstract
A major problem in machine learning is that of inductive bias: how to choose a learner’s hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small
enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is
supplied by hand through the skill and insights of experts. In this paper a model for automatically
learning bias is investigated. The central assumption of the model is that the learner is embedded
within an environment of related learning tasks. Within such an environment the learner can sample
from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to
many of the problems in the environment. Under certain restrictions on the set of all hypothesis
spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently
large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an
environment of related tasks can potentially give much better generalization than learning a single
task.

1. Introduction
Often the hardest problem in any machine learning task is the initial choice of hypothesis space;
it has to be large enough to contain a solution to the problem at hand, yet small enough to ensure
good generalization from a small number of examples (Mitchell, 1991). Once a suitable bias has
been found, the actual learning task is often straightforward. Existing methods of bias generally
require the input of a human expert in the form of heuristics and domain knowledge (for example,
through the selection of an appropriate set of features). Despite their successes, such methods are
clearly limited by the accuracy and reliability of the expert’s knowledge and also by the extent to
which that knowledge can be transferred to the learner. Thus it is natural to search for methods for
automatically learning the bias.
In this paper we introduce and analyze a formal model of bias learning that builds upon
the PAC model of machine learning and its variants (Vapnik, 1982; Valiant, 1984; Blumer,
Ehrenfeucht, Haussler, & Warmuth, 1989; Haussler, 1992). These models typically take the
and training data
following general form: the learner is supplied with a hypothesis space
drawn independently according to some underlying distribution
on
. Based on the information contained in , the learner’s goal is to select a hypothesis
from minimizing some measure
of expected loss with respect to (for example, in the case of squared loss
). In such models the learner’s
bias is represented by the choice of ; if does not contain a good solution to the problem, then,
regardless of how much data the learner receives, it cannot learn.
Of course, the best way to bias the learner is to supply it with an containing just a single optimal hypothesis. But finding such a hypothesis is precisely the original learning problem, so in the

	

	
!"
#%$ '
&("

@



# 

)

*
+
#
$
 ,/.1032 46587 +  # 	 9;:<=?>
) *+   -

c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.





BAXTER

PAC model there is no distinction between bias learning and ordinary learning. Or put differently,
the PAC model does not model the process of inductive bias, it simply takes the hypothesis space
as given and proceeds from there. To overcome this problem, in this paper we assume that instead
of being faced with just a single learning task, the learner is embedded within an environment of
related learning tasks. The learner is supplied with a family of hypothesis spaces
, and its
) that is appropriate for the entire environment.
goal is to find a bias (i.e. hypothesis space
A simple example is the problem of handwritten character recognition. A preprocessing stage that
identifies and removes any (small) rotations, dilations and translations of an image of a character
will be advantageous for recognizing all characters. If the set of all individual character recognition
problems is viewed as an environment of learning problems (that is, the set of all problems of the
form “distinguish ‘A’ from all other characters”, “distinguish ‘B’ from all other characters”, and
so on), this preprocessor represents a bias that is appropriate for all problems in the environment.
It is likely that there are many other currently unknown biases that are also appropriate for this
environment. We would like to be able to learn these automatically.

DC A

A B 

There are many other examples of learning problems that can be viewed as belonging to environments of related problems. For example, each individual face recognition problem belongs to an
(essentially infinite) set of related learning problems (all the other individual face recognition problems); the set of all individual spoken word recognition problems forms another large environment,
as does the set of all fingerprint recognition problems, printed Chinese and Japanese character recognition problems, stock price prediction problems and so on. Even medical diagnostic and prognostic
problems, where a multitude of diseases are predicted from the same pathology tests, constitute an
environment of related learning problems.
In many cases these “environments” are not normally modeled as such; instead they are treated
as single, multiple category learning problems. For example, recognizing a group of faces would
normally be viewed as a single learning problem with multiple class labels (one for each face in
the group), not as multiple individual learning problems. However, if a reliable classifier for each
individual face in the group can be constructed then they can easily be combined to produce a
classifier for the whole group. Furthermore, by viewing the faces as an environment of related
learning problems, the results presented here show that bias can be learnt that will be good for
learning novel faces, a claim that cannot be made for the traditional approach.
This point goes to the heart of our model: we are not not concerned with adjusting a learner’s
bias so it performs better on some fixed set of learning problems. Such a process is in fact just
ordinary learning but with a richer hypothesis space in which some components labelled “bias” are
also able to be varied. Instead, we suppose the learner is faced with a (potentially infinite) stream of
tasks, and that by adjusting its bias on some subset of the tasks it improves its learning performance
on future, as yet unseen tasks.
Bias that is appropriate for all problems in an environment must be learnt by sampling from
many tasks. If only a single task is learnt then the bias extracted is likely to be specific to that
task. In the rest of this paper, a general theory of bias learning is developed based upon the idea of
learning multiple related tasks. Loosely speaking (formal results are stated in Section 2), there are
two main conclusions of the theory presented here:

E

Learning multiple related tasks reduces the sampling burden required for good generalization,
at least on a number-of-examples-required-per-task basis.
150

A M ODEL OF I NDUCTIVE B IAS L EARNING

E

Bias that is learnt on sufficiently many training tasks is likely to be good for learning novel
tasks drawn from the same environment.

The second point shows that a form of meta-generalization is possible in bias learning. Ordinarily, we say a learner generalizes well if, after seeing sufficiently many training examples, it
produces a hypothesis that with high probability will perform well on future examples of the same
task. However, a bias learner generalizes well if, after seeing sufficiently many training tasks it produces a hypothesis space that with high probability contains good solutions to novel tasks. Another
term that has been used for this process is Learning to Learn (Thrun & Pratt, 1997).
Our main theorems are stated in an agnostic setting (that is,
does not necessarily contain a
hypothesis space with solutions to all the problems in the environment), but we also give improved
bounds in the realizable case. The sample complexity bounds appearing in these results are stated
in terms of combinatorial parameters related to the complexity of the set of all hypothesis spaces
available to the bias learner. For Boolean learning problems (pattern classification) these parameters
are the bias learning analogue of the Vapnik-Chervonenkis dimension (Vapnik, 1982; Blumer et al.,
1989).
As an application of the general theory, the problem of learning an appropriate set of neuralnetwork features for an environment of related tasks is formulated as a bias learning problem. In
the case of continuous neural-network features we are able to prove upper bounds on the number
of training tasks and number of examples of each training task required to ensure a set of features
that works well for the training tasks will, with high probability, work well on novel tasks drawn
where is
from the same environment. The upper bound on the number of tasks scales as
a measure of the complexity of the possible feature sets available to the learner, while the upper
where
is the number
bound on the number of examples of each task scales as
of examples required to learn a task if the “true” set of features (that is, the correct bias) is already
known, and is the number of tasks. Thus, in this case we see that as the number of related tasks
learnt increases, the number of examples required of each task for good generalization decays to
the minimum possible. For Boolean neural-network feature maps we are able to show a matching
lower bound on the number of examples required per task of the same form.

A

A

F JILKMGONPQ

P

F HG
F JIR

G

1.1 Related Work
There is a large body of previous algorithmic and experimental work in the machine learning and
statistics literature addressing the problems of inductive bias learning and improving generalization
through multiple task learning. Some of these approaches can be seen as special cases of, or at least
closely aligned with, the model described here, while others are more orthogonal. Without being
completely exhaustive, in this section we present an overview of the main contributions. See Thrun
and Pratt (1997, chapter 1) for a more comprehensive treatment.

E

Hierarchical Bayes. The earliest approaches to bias learning come from Hierarchical Bayesian
methods in statistics (Berger, 1985; Good, 1980; Gelman, Carlin, Stern, & Rubim, 1995).
In contrast to the Bayesian methodology, the present paper takes an essentially empirical
process approach to modeling the problem of bias learning. However, a model using a mixture
of hierarchical Bayesian and information-theoretic ideas was presented in Baxter (1997a),
with similar conclusions to those found here. An empirical study showing the utility of the
hierarchical Bayes approach in a domain containing a large number of related tasks was given
in Heskes (1998).
151

E

BAXTER

E

Early machine learning work. In Rendell, Seshu, and Tcheng (1987) “VBMS” or Variable Bias
Management System was introduced as a mechanism for selecting amongst different learning
algorithms when tackling a new learning problem. “STABB” or Shift To a Better Bias (Utgoff, 1986) was another early scheme for adjusting bias, but unlike VBMS, STABB was not
primarily focussed on searching for bias applicable to large problem domains. Our use of an
“environment of related tasks” in this paper may also be interpreted as an “environment of
analogous tasks” in the sense that conclusions about one task can be arrived at by analogy
with (sufficiently many of) the other tasks. For an early discussion of analogy in this context, see Russell (1989, S4.3), in particular the observation that for analogous problems the
sampling burden per task can be reduced.
Metric-based approaches. The metric used in nearest-neighbour classification, and in vector
quantization to determine the nearest code-book vector, represents a form of inductive bias.
Using the model of the present paper, and under some extra assumptions on the tasks in
the environment (specifically, that their marginal input-space distributions are identical and
they only differ in the conditional probabilities they assign to class labels), it can be shown
that there is an optimal metric or distance measure to use for vector quantization and onenearest-neighbour classification (Baxter, 1995a, 1997b; Baxter & Bartlett, 1998). This metric
can be learnt by sampling from a subset of tasks from the environment, and then used as a
distance measure when learning novel tasks drawn from the same environment. Bounds on
the number of tasks and examples of each task required to ensure good performance on novel
tasks were given in Baxter and Bartlett (1998), along with an experiment in which a metric
was successfully trained on examples of a subset of 400 Japanese characters and then used as
a fixed distance measure when learning 2600 as yet unseen characters.
A similar approach is described in Thrun and Mitchell (1995), Thrun (1996), in which a
neural network’s output was trained to match labels on a novel task, while simultaneously
being forced to match its gradient to derivative information generated from a distance metric
trained on previous, related tasks. Performance on the novel tasks improved substantially
with the use of the derivative information.

E

E

Note that there are many other adaptive metric techniques used in machine learning, but these
all focus exclusively on adjusting the metric for a fixed set of problems rather than learning a
metric suitable for learning novel, related tasks (bias learning).
Feature learning or learning internal representations. As with adaptive metric techniques,
there are many approaches to feature learning that focus on adapting features for a fixed task
rather than learning features to be used in novel tasks. One of the few cases where features
have been learnt on a subset of tasks with the explicit aim of using them on novel tasks was
Intrator and Edelman (1996) in which a low-dimensional representation was learnt for a set
of multiple related image-recognition tasks and then used to successfully learn novel tasks of
the same kind. The experiments reported in Baxter (1995a, chapter 4) and Baxter (1995b),
Baxter and Bartlett (1998) are also of this nature.
Bias learning in Inductive Logic Programming (ILP). Predicate invention refers to the process in ILP whereby new predicates thought to be useful for the classification task at hand
are added to the learner’s domain knowledge. By using the new predicates as background domain knowledge when learning novel tasks, predicate invention may be viewed as a form of
152

A M ODEL OF I NDUCTIVE B IAS L EARNING

inductive bias learning. Preliminary results with this approach on a chess domain are reported
in Khan, Muggleton, and Parson (1998).

E

E

Improving performance on a fixed reference task. “Multi-task learning” (Caruana, 1997)
trains extra neural network outputs to match related tasks in order to improve generalization
performance on a fixed reference task. Although this approach does not explicitly identify the
extra bias generated by the related tasks in a way that can be used to learn novel tasks, it is
an example of exploiting the bias provided by a set of related tasks to improve generalization
performance. Other similar approaches include Suddarth and Kergosien (1990), Suddarth and
Holden (1991), Abu-Mostafa (1993).

E

Bias as computational complexity. In this paper we consider inductive bias from a samplecomplexity perspective: how does the learnt bias decrease the number of examples required of
novel tasks for good generalization? A natural alternative line of enquiry is how the runningtime or computational complexity of a learning algorithm may be improved by training on
related tasks. Some early algorithms for neural networks in this vein are contained in Sharkey
and Sharkey (1993), Pratt (1992).
Reinforcement Learning. Many control tasks can appropriately be viewed as elements of sets
of related tasks, such as learning to navigate to different goal states, or learning a set of
complex motor control tasks. A number of papers in the reinforcement learning literature
have proposed algorithms for both sharing the information in related tasks to improve average
generalization performance across those tasks Singh (1992), Ring (1995), or learning bias
from a set of tasks to improve performance on future tasks Sutton (1992), Thrun and Schwartz
(1995).

1.2 Overview of the Paper
In Section 2 the bias learning model is formally defined, and the main sample complexity results
are given showing the utility of learning multiple related tasks and the feasibility of bias learning.
These results show that the sample complexity is controlled by the size of certain covering numbers
associated with the set of all hypothesis spaces available to the bias learner, in much the same way
as the sample complexity in learning Boolean functions is controlled by the Vapnik-Chervonenkis
dimension (Vapnik, 1982; Blumer et al., 1989). The results of Section 2 are upper bounds on
the sample complexity required for good generalization when learning multiple tasks and learning
inductive bias.
The general results of Section 2 are specialized to the case of feature learning with neural networks in Section 3, where an algorithm for training features by gradient descent is also presented.
For this special case we are able to show matching lower bounds for the sample complexity of
multiple task learning. In Section 4 we present some concluding remarks and directions for future
research. Many of the proofs are quite lengthy and have been moved to the appendices so as not to
interrupt the flow of the main text.
The following tables contain a glossary of the mathematical symbols used in the paper.
153

BAXTER

Symbol



U

"

Description
Input Space
Output Space
Distribution on
(learning task)
Loss function
Hypothesis Space
Hypothesis
Error of hypothesis on distribution
Training set
Learning Algorithm
Empirical error of on training set
Set of all learning tasks
Distribution over learning tasks
Family of hypothesis spaces
Loss of hypothesis space on environment
-sample
Empirical loss of on
Bias learning algorithm
Function induced by and
Set of
Average of
Same as
Set of
Set of
Function on probability distributions
Set of
Pseudo-metric on
Pseudo-metric on
Covering number of
Capacity of
Covering number of
Capacity of
Sequence of hypotheses
Sequence of distributions
Average loss of on
Average loss of on
Set of feature maps
Output class composed with feature maps
Hypothesis space associated with
Loss function class associated with
Covering number of
Capacity of
Pseudo-metric on feature maps
Covering number of

ST"

#
#
) * +  # 
V
#
)Y W *X  # 

Z
A
)\ *[  
	P]^_
W)*a`  
\
V
#b
#
b
#b
# 
 2 b  # c 2 b
 # 
  #Rc  b
# 
  #Rc  b
dQb c
bc
 # c 
  #=c  b
b
b
TeA
e
e
cb
fA g
f[
e
A
h Jij A e  f [ 
A e
k Jij A e c 
eA
cb
h Jij A c b  flg 
A
c
m Jij b 
AP b
d A
n
P
g)*  d 
d n
d \
W)* `  d 
o
p
psr q
pb
h Jij p b  f + 
pb
k Hij p b 
pb
ft + 2 uvxw  q  qzy 
h Jij o  f=t + 2 uv{w 
o





U

 # 
  #=c 
  
   c 

154

qp

q  qzy

q

Z

First Referenced
155
155
155
155
155
155
156
156
156
156
157
157
157
158
158
158
159
159
159
159
159
159
159
160
160
160
160
160
160
160
160
163
163
164
164
166
166
166
166
166
166
166
166

A M ODEL OF I NDUCTIVE B IAS L EARNING

hSymbol
o
Jij o  f t + 2 uv{w8 Description
Covering number of
k uv Jij o 
o
Capacity of
}|
Neural network hypothesis space
~ 0
restricted to vector 
 	^_
Growth function of
  
Vapnik-Chervonenkis dimension of
~ 
restricted to matrix 
A ~  	P]^_
A restricted to matrix 
Growth function of A
f  	PQ
Dimension function of A
f A 
Upper dimension function of A
f A  c
Lower dimension function cof A
n
= g  A 
Optimal performance of A
on
f
on 
# 
/ #=c Metric
#
#Rc
Average of 
 ,  ,
c
#
=
#
c

3
Set of 
/
 . >  2c 5
Permutations on integer pairs
\j
Permuted \
f ` d  d y
U
d
Empirical 
 metric on functions
W)* g  
n
Optimal average error of on

First Referenced
166
166
167
172
172
172
173
173
173
173
173
173
175
179
179
180
182
182
182
185

2. The Bias Learning Model
In this section the bias learning model is formally introduced. To motivate the definitions, we first
describe the main features of ordinary (single-task) supervised learning models.
2.1 Single-Task Learning
Computational learning theory models of supervised learning usually include the following ingredients:

E

An input space

E



 on Ss"
E a loss function U$ "s"&D , and
a probability distribution

E

a hypothesis space

"

and an output space

,

,

which is a set of hypotheses or functions

#$  &¡"

.

As an example, if the problem is to learn to recognize images of Mary’s face using a neural network,
then would be the set of all images (typically represented as a subset of
where each component
is a pixel intensity), would be the set
, and the distribution would be peaked over images
of different faces and the correct class labels. The learner’s hypothesis space would be a class of
neural networks mapping the input space
to
. The loss in this case would be discrete loss:



"

£¤

 ¢  £¤
§ 
U 	z y  $ ¦¥ £¤ if L
 y



if

155

y

]¢

(1)

BAXTER

£¤ U
U 	¨ y ]©	L:ª y  >

" 

"  

Using the loss function allows us to present a unified treatment of both pattern recognition (
, as above), and real-valued function learning (e.g. regression) in which
and usually
.
The goal of the learner is to select a hypothesis
with minimum expected loss:

# CT

(2)
)* +  #  $ B«¬®­°¯ U  # 	±= f  	QR
#
for an minimizing
Of course, the learner does not know  and so it cannot search through
)* +  #  . In practice, the learner samples repeatedly from ²³" according to the distribution  to
generate a training set
 $ 	 
  
 	    j
(3)
# CT . Hence, in general
Based on the information contained in  the learner produces a hypothesis
V
a learner is simply a map from the set of all training samples to the hypothesis space :
V $´  DT"   &
µ¶
V
(stochastic learner’s can be treated by assuming a distribution-valued .)
#
Many algorithms seek to minimize the empirical loss of on  , where this is defined by:
¸ ¸

W)* X  #  $  ^ ¤ · ¸¹ U  # 	  
(4)


Of course, there are more intelligent things to do with the data than simply minimizing empirical
error—for example one can add regularisation terms to avoid over-fitting.
However the learner chooses its hypothesis , if we have a uniform bound (over all
) on
the probability of large deviation between
and
, then we can bound the learner’s genas a function of its empirical loss on the training set
. Whether such
eralization error
a bound holds depends upon the “richness” of . The conditions ensuring convergence between
and
are by now well understood; for Boolean function learning (
, discrete
loss), convergence is controlled by the VC-dimension1 of :

) W * X  # 

)*+  # 
) * +  # 

#
W) *X  # 

# C

) *+  # 

) W *X  # 
" B£¤

£¤ and suppose  
probability distribution on 
 	

6	 be isanygenerated
^
times from º £¤ according to  . Let
f»$  =   . Then with probabilityby atsampling
least ¤®:½¼ (over the choice of the training set  ), all
# C¾ will satisfy

ÍÌ >
^
#
#
Å
f
Ä
Ä
W
(5)
)* +  ¿ )* X  K ÀOÁ^ Â Ã Æ ÂÇ f K ÆÉÈ¼±ÊË

Theorem 1. Let

Proofs of this result may be found in Vapnik (1982), Blumer et al. (1989), and will not be
reproduced here.

ÓaÔÕ×Ö×Ø×Ø×ØÖJÔ3ÙÚÅÛÝÜ

Î Ð

Î

Þ

Ù

Ï

Ð

1. The VC dimension of a class of Boolean functions
is the largest integer such that there exists a subset
such that the restriction of to contains all Boolean functions on .

156

Ð»Ñ Ò

A M ODEL OF I NDUCTIVE B IAS L EARNING

)*+  # 

) *+  # 

) W *X  # 

Theorem 1 only provides conditions under which the deviation between
and
is
will actually be small. This is
likely to be small, it does not guarantee that the true error
governed by the choice of . If contains a solution with small error and the learner minimizes
error on the training set, then with high probability
will be small. However, a bad choice of
will mean there is no hope of achieving small error. Thus, the bias of the learner in this model2
is represented by the choice of hypothesis space .

)*+  # 

2.2 The Bias Learning Model
The main extra assumption of the bias learning model introduced here is that the learner is embedded in an environment of related tasks, and can sample from the environment to generate multiple
training sets belonging to multiple different tasks. In the above model of ordinary (single-task)
on
. So in the bias learning
learning, a learning task is represented by a distribution
model, an environment of learning problems is represented by a pair
where is the set of
(i.e., is the set of all possible learning problems), and is a
all probability distributions on
distribution on . controls which learning problems the learner is likely to see3 . For example, if
the learner is in a face recognition environment, will be highly peaked over face-recognition-type
problems, whereas if the learner is in a character recognition environment will be peaked over
character-recognition-type problems (here, as in the introduction, we view these environments as
sets of individual classification problems, rather than single, multiple class classification problems).
Recall from the last paragraph of the previous section that the learner’s bias is represented by its
choice of hypothesis space . So to enable the learner to learn the bias, we supply it with a family
.
or set of hypothesis spaces
Putting all this together, formally a learning to learn or bias learning problem consists of:

Y

á®"

Y Z



ßà" Y  Z 

Z

Y

Z

Z

A $ â 

E

an input space

E

a loss function



and an output space

U$ "s"&D ,
E an environment  Y  Z  where Y
Y
a distribution on ,
E

a hypothesis space family

"

(both of which are separable metric spaces),

is the set of all probability distributions on

A â 

where each

U

U

ãC A

From now on we will assume the loss function has range
we assume that is bounded.

Î

D³"

is a set of functions

and

#%$  &²"

Z

is

.

ä £¤6å , or equivalently, with rescaling,

2. The bias is also governed by how the learner uses the hypothesis space. For example, under some circumstances the
learner may choose not to use the full power of (a neural network example is early-stopping). For simplicity in
this paper we abstract away from such features of the algorithm and assume that it uses the entire hypothesis space
.
3. ’s domain is a -algebra of subsets of . A suitable one for our purposes is the Borel -algebra
generated
by the topology of weak convergence on . If we assume that and are separable metric spaces, then is also
a separable metric space in the Prohorov metric (which metrizes the topology of weak convergence) (Parthasarathy,
1967), so there is no problem with the existence of measures on
. See Appendix D for further discussion,
particularly the proof of part 5 in Lemma 32.

Î

ç

è

é

Ü

é

157

æ

î
êëìéÅí

è

êQëìéÅí

é

BAXTER

We define the goal of a bias learner to be to find a hypothesis space
following loss:

ßC A

minimizing the

)*6[   $  « ï ò ð=ó  ñ )*+  #  f Z   
(6)
 « ï ò=ðó  ñ «¬É­°¯ U  # 	±= f  	= f Z   
Z
#
The only way )*[   can be small is if, with high -probability, contains a good solution to
Z
any problem  drawn at random according to . In this sense )*[   measures how appropriate
Y Z
the bias embodied by is for the environment    .
Z
In general the learner will not know , so it will not be able to find an minimizing )*[  

Z
Y
times from according to to yield:
c
¸
 
   .
¸
¸
¸
¸
¸
E Sample ^ times from S³" according to each  to yield:
 B	 
  
 ¨	     .
E The resulting P training sets—henceforth called an 	P]^_ -sample if they are generated by the
above process—are supplied to the learner. In the sequel, an 	P]^_ -sample will be denoted
by \ and written as a matrix:
	



  	
ô
ôõ j

..
..
..
..
\ $
(7)
.
.
.
	 c 
  c 
   	 c   c  !. c
c
An 	P]^_ -sample is simply P training sets  
 a sampled from P different learning tasks
c
 
  , where each task is selected according to the environmental probability distribution Z .
The size of each training set is kept the same primarily to facilitate the analysis.
'C A .
Based on the information contained in \ , the learner must choose a hypothesis space
\
directly. However, the learner can sample from the environment in the following way:

E

Sample

P

One way to do this would be for the learner to find an
this is defined by:

minimizing the empirical loss on , where

) W *a ` ¸  
 
[

)
*
c
 
  

c
¤
·
ì
¸
¹
) W * `   $  P 
 ò3 óðR ñ )W * X?ö  # 

(8)

)*6[  

Note that
is simply the average of the best possible empirical error achievable on each
training set , using a function from . It is a biased estimate of
. An unbiased estimate of
would require choosing an with minimal average error over the distributions
, where this is defined by
.
As with ordinary learning, it is likely there are more intelligent things to do with the training data
than minimizing (8). Denoting the set of all
-samples by
, a general “bias
learner” is a map that takes
-samples as input and produces hypothesis spaces
as
output:

\

V

	P]^_


Éc ÷ ¸c ¹ 
  ðRñ 3ò ó   # 
) *+ö
	P]^_

. c 2 5 
V $ ´ D

 s"
& A
'
c µµ¶ ¶
158

 Ss"  . c 2  5

P

SC A

(9)

A M ODEL OF I NDUCTIVE B IAS L EARNING

V

(as stated, is a deterministic bias learner, however it is trivial to extend our results to stochastic
learners).
Note that in this paper we are concerned only with the sample complexity properties of a bias
learner ; we do not discuss issues of the computability of .
Since is searching for entire hypothesis spaces within a family of such hypothesis spaces
, there is an extra representational question in our model of bias learning that is not present in
is represented and searched by . We defer this
ordinary learning, and that is how the family
discussion until Section 2.5, after the main sample complexity results for this model of bias learning
have been introduced. For the specific case of learning a set of features suitable for an environment
of related learning problems, see Section 3.
Regardless of how the learner chooses its hypothesis space , if we have a uniform bound (over
all
) on the probability of large deviation between
and
, and we can compute
an upper bound on
, then we can bound the bias learner’s “generalization error”
.
With this view, the question of generalization within our bias learning model becomes: how many
tasks ( ) and how many examples of each task ( ) are required to ensure that
and
are close with high probability, uniformly over all
? Or, informally, how many tasks and how
many examples of each task are required to ensure that a hypothesis space with good solutions to
all the training tasks will contain good solutions to novel tasks drawn from the same environment?
It turns out that this kind of uniform convergence for bias learning is controlled by the “size”
of certain function classes derived from the hypothesis space family , in much the same way as
the VC-dimension of a hypothesis space
controls uniform convergence in the case of Boolean
function learning (Theorem 1). These “size” measures and other auxiliary definitions needed to
state the main theorem are introduced in the following subsection.

V

A

V

V

V

A

ãC A
P

) W *a`  

) W *`  

^

)*6[  

) W *`  

 C A

)*[  
)*[  

A

#%$ ø&D" , define #=b$ ù³"&(ä £¤6å by
#b 	= $  U  # 	9R
(10)
For any hypothesis space in the hypothesis space family A , define
b$ B #=b$# CT j
(11)
c
#
# c  , define  # 
 # c  b$  DT"  &(ä £¤6å by
For any sequence of P hypotheses  

c ¸ ¸ ¸
¤
 # 
  #Rc  b 	 
  
  c  c  $  P · ¸ì¹ U  # 	  
(12)


db
#
#=c  b . For any in the hypothesis space family A , define
We will also use to denote  
 
cb $ B # 
 # c  b$¨# 
 # c CT j
(13)
2.3 Covering Numbers

Definition 1. For any hypothesis

Define

cb $  ´ cb 
A
 ó
159

(14)

BAXTER

#$  & "
b
b
P

øÝ"©&(ä £¤6å

#b

In the first part of the definition above, hypotheses
are turned into functions
mapping
by composition with the loss function.
is then just the collection of all
such functions where the original hypotheses come from .
is often called a loss-function class.
In our case we are interested in the average loss across tasks, where each of the hypotheses
is chosen from a fixed hypothesis space . This motivates the definition of
and
. Finally,
is the collection of all
, with the restriction that all
belong to a single
hypothesis space
.

c
A b

 C A

Definition 2. For each

 # 
 # c  b

ãC A

, define

For the hypothesis space family

A

Te $ Y (
& ä £¤6å by
e    $  ò3 óðR ñ )* +  # 

, define

db
# 
 # c

P c
b

(15)

e $B
  e $  C A j
A
(16)
cb
e that controls how large the 	P]^_ -sample \ must be to ensure
It is the “size” of A
and A
)W *a`   and )*6[   are close uniformly over all úC A . Their size will be defined in terms of
certain covering
cb numbers, and for this we neede to define how to measure the distance between
elements of A
and also between elements of A .
n ©  
   c  be any sequence of P probability distributions on DT" . For
Definition 3. Let
c
db d b C A b , define
any  y
flg  dQb  d yb  $ M« . ¬®­°¯ 5ûõü db 	 
  
  c  c ;: d yb 	 
  
  c  c  ü
(17)
f  
 	 
  
 ¨ f  c 	 c  c 
Z Y
e e C A e , define
Similarly, for any distribution on and any 
  >
f [  e
  e  $  « ï ü e
   ;: e    ü f Z   
(18)
>
>
fg and f [ are pseudo-metrics on A cb and A e respectively.
It is easily verified that
e f
Te Te
¾eDC A e ,
¸
¸
Definition 4. An i -cover of  A  [  is a set  
  ý  such that for all
f [  Te  ¾e T¿þi for some ÿ ²¤; . Note that we do not require the Te to be contained in
h
Y
e f
A e , just that they be measurable functions
on . Let Jij A  [  denote the size of the smallest
e
such cover. Define the capacity of A
by
k Jil A e  $ 
   h Jil A e  f [ 
(19)
[
Y . h Jij A cb  f g  is defined in a similar
where the supremum is over all probability measures on
c
fg in place of f [ . Define the capacity of A b by:
way, using
k Jij A cb  $ g  h Jil A cb  fg 
(20)
where now the supremum is over all sequences of P probability measures on S³" .
Ô°Ö  íR	Ò 
 Ô Ò  .
4. A pseudo-metric Ï is a metric without the condition that Ïë
4

160

A M ODEL OF I NDUCTIVE B IAS L EARNING

2.4 Uniform Convergence for Bias Learners
Now we have enough machinery to state the main theorem. In the theorem the hypothesis space
family is required to be permissible. Permissibility is discussed in detail in Appendix D, but note
that it is a weak measure-theoretic condition satisfied by almost all “real-world” hypothesis space
families. All logarithms are to base .

Y



"

Ç

Z

\
	P]^_

»"
¸
¸
¸
¸
Z
c
Y
P
^
 
  ¤3P




	

















	


















A
P ÿ

k  e 






P
Âi >  Ä Æ  >¼  A  i È>  
and the number of examples ^ of each task satisfies
k     A cb 
Ä





^!
¥ÉPÂi  > Æ  > ¼ "i È$> # 
 C A will satisfy
then with probability at least ¤:¼ (over the 	P]^_ -sample \ ), all
)*[  %¿ )W *a`  9K i

Theorem 2. Suppose
and are separable metric spaces and let be any probability distribution on , the set of all distributions on
. Suppose is an
-sample generated by
sampling times from according to to give
, and then sampling times from each
to generate
,
. Let
be any permissible
hypothesis space family. If the number of tasks satisfies

¸

¸

(21)

(22)

(23)

Proof. See Appendix A.

k Jij A cb 
	P]^_ A

There are several important points to note about Theorem 2:

k J ij A e 

)* [  
# Cs
)*O[  

1. Provided the capacities
and
are finite, the theorem shows that any bias
can bound its generalisation error
in
learner that selects hypothesis spaces from
terms of
for sufficiently large
-samples . Most bias learner’s will not find the
exact value of
because it involves finding the smallest error of any hypothesis
on each of the training sets in . But any upper bound on
(found, for example
by gradient descent on some error function) will still give an upper bound on
. See
Section 3.3.1 for a brief discussion on how this can be achieved in a feature learning setting.

) W *a`  W  
)*aP `

) W *`  

\

 

)

*
[
P

C A

\

) W *a`  

^

and
are close uniformly over all
2. In order to learn bias (in the sense that
), both the number of tasks and the number of examples of each task
must
be sufficiently large. This is intuitively reasonable because the bias learner must see both
sufficiently many tasks to be confident of the nature of the environment, and sufficiently
many examples of each task to be confident of the nature of each task.

úC A
Z
^

) W * `  

3. Once the learner has found an
with a small value of
, it can then use
to
learn novel tasks drawn according to . One then has the following theorem bounding the
sample complexity required for good generalisation when learning with (the proof is very
similar to the proof of the bound on in Theorem 2).



161

BAXTER

â	 
  
 	    
¡

il¼ " £ ij¼ M¤
^

k ' b 
^    i È> Ä Æ È 
)¼ (   ¤i > 
!
(24)
# CT will satisfy
then with probability at least ¤%: ¼ , all
)*+  # %¿ )W *X  # K ij
k
The capacity Jil  appearing in equation (24) is defined in an analogous
¯ #b 	to=the:
f #b # b $  * ¬®­°fashion
capacities in Definition 4 (we just use the pseudo-metric +   y  +
ü
# yb 	QR ü f  	= ). The important thing to note about Theorem 3 is that the number
of ex-

Theorem 3. Let
be a training set generated by sampling from
according to some distribution . Let be a permissible hypothesis space. For all
&%
with %
, if the number of training examples satisfies

amples required for good generalisation when learning novel tasks is proportional to the logarithm of the capacity of the learnt hypothesis space . In contrast, if the learner does not
do any bias learning, it will have no reason to select one hypothesis space
over any
other and consequently it would have to view as a candidate solution any hypothesis in any
of the hypothesis spaces
. Thus, its sample complexity will be proportional to the
capacity of ,
, which in general will be considerably larger than the capacity
. So by learning the learner has learnt to learn in the environment
of any individual
in the sense that it needs far smaller training sets to learn novel tasks.

Y  Z 

¡C A

C
 ó   b ® A b
 A
ãC A

¤ :_¼
W) *a`   K i
)*[   £
0
* ¥  $ ò3 ðRó  ñ ) * +  # 1

W *a`  
)

ð=ñ ò ó  )*+  # 

4. Having learnt a hypothesis space
with a small value of
, Theorem 2 tells us that
with probability at least
, the expected value of
on a novel task will be
less than
. Of course, this does not rule out really bad performance on some tasks
. However, the probability of generating such “bad” tasks can be bounded. In particular,
note that
is just the expected value of the function
over , and so by Markov’s
inequality, for -/. ,



e

- #

 0 * 
¿ 2[
3

)
6
*
[
 
W
¿ )*` 

e

$ e  

-


9 Kài


-



Y



(with probability

¤%:¼ ).

ij¼

5. Keeping the accuracy and confidence parameters
fixed, note that the number of examples
required of each task for good generalisation obeys

^  F Ã P ¤ Ä Æ k ô ij A cb  Ê 
(25)
c
Ä k Jil A b  increases sublinearly with P , the upper bound on the number of
So provided  Æ

examples required of each task will decrease as the number of tasks increases. This shows
that for suitably constructed hypothesis space families it is possible to share information
between tasks. This is discussed further after Theorem 4 below.
162

A M ODEL OF I NDUCTIVE B IAS L EARNING

2.5 Choosing the Hypothesis Space Family

) *[  
	P]^_
\

A

.

) W * `  

)* [  
A  C A
P A ^

Theorem 2 only provides conditions under which
and
are close, it does not guarantee that
is actually small. This is governed by the choice of . If contains a hypothesis
space with a small value of
and the learner is able to find an
minimizing error on
sample (i.e., minimizing
), then, for sufficiently large and , Theorem 2 enthe
sures that with high probability
will be small. However, a bad choice of will mean there
is no hope of finding an with small error. In this sense the choice of represents the hyper-bias
of the learner.
Note that from a sample complexity point of view, the optimal hypothesis space family to choose
that contains good solutions to all of the
is one containing a single, minimal hypothesis space
problems in the environment (or at least a set of problems with high -probability), and no more.
For then there is no bias learning to do (because there is no choice to be made between hypothesis
spaces), the output of the bias learning algorithm is guaranteed to be a good hypothesis space for
the environment, and since the hypothesis space is minimal, learning any problem within the environment using will require the smallest possible number of examples. However, this scenario
is analagous to the trivial scenario in ordinary learning in which the learning algorithm contains a
single, optimal hypothesis for the problem being learnt. In that case there is no learning to be done,
just as there is no bias learning to be done if the correct hypothesis space is already known.
At the other extreme, if contains a single hypothesis space consisting of all possible functions from
then bias learning is impossible because the bias learner cannot produce a
restricted hypothesis space as output, and hence cannot produce a hypothesis space with improved
sample complexity requirements on as yet unseen tasks.
Focussing on these two extremes highlights the minimal requirements on for successful bias
must be strictly smaller than the space of all
learning to occur: the hypothesis spaces
functions
, but not so small or so “skewed” that none of them contain good solutions to a
large majority of the problems in the environment.
It may seem that we have simply replaced the problem of selecting the right bias (i.e., selecting
the right hypothesis space ) with the equally difficult problem of selecting the right hyper-bias (i.e.,
the right hypothesis space family ). However, in many cases selecting the right hyper-bias is far
easier than selecting the right bias. For example, in Section 3 we will see how the feature selection
problem may be viewed as a bias selection problem. Selecting the right features can be extremely
difficult if one knows little about the environment, with intelligent trial-and-error typically the best
one can do. However, in a bias learning scenario, one only has to specify that a set of features should
exist, find a loosely parameterised set of features (for example neural networks), and then learn the
features by sampling from multiple related tasks.

)*[ 
)*[



W *a`  
)

 

A

A

Z

 & "

A

A

C A

'&ß"

A

2.6 Learning Multiple Tasks

P

Y  Z 
A

It may be that the learner is not interested in learning to learn, but just wants to learn a fixed set
. As in the previous section, we assume the learner starts
of tasks from the environment
out with a hypothesis space family , and also that it receives an
-sample generated from
the distributions
. This time, however, the learner is simply looking for hypotheses
, all contained in the same hypothesis space , such that the average generalization
error of the hypotheses is minimal. Denoting
by and writing
,

P
# 
  #Rc 

P

 
   c

	P]^_

 # 
  #=c 

163

d

\

P

n    
   c 

BAXTER

c
¸
¤
·
ì
¸
¹
) * g  d  $  P 
 ) *+ö  # 
(26)
c
¸
¸
 P ¤ · ¸ì¹ «¬É­°¯ U  # 	±= f  	 =


d
and the empirical loss of on \ is
c
¸
¤
W)*a`  d  $  P · ¸¹ )W *X?ö  # 
(27)

c  ¸ ¸ ¸
 P ¤ · ¸¹ ^ ¤ · ¹ U  # 	 4  4 

 4 

#
#=c  , if we can prove a uniform bound on
As before, regardless of how the learner chooses  
 
g
d
d
#
# c  that perform
W
the probability of large deviation between )* `   and )*   then any  

well on the training sets \ will with high probability perform well on future examples of the same

this error is given by:

tasks.

n ©
   
  c  P
^

" ¸


\

A   

	P]^_

Theorem 4. Let
be probability distributions on
and let be an
according to each . Let
be any
sample generated by sampling times from
permissible hypothesis space family. If the number of examples of each task satisfies

ºª"

^

kÈ  
) (  A cb  ¤
Ä





¥ P  i È > Æ
5
^!
(28)
¼  i > #
d C A c will satisfy
then with probability at least ¤:¼ (over the choice of \ ), any
(29)
)* g  d %¿ )W *a`  d K i
c
k b
(recall Definition 4 for the meaning of Jil A  ).
Proof. Omitted (follow the proof of the bound on ^ in Theorem 2).
The bound on ^ in Theorem 4 is virtually identical to the bound on ^ in Theorem 2, and note
again that it depends inversely on the number of tasks P (assuming that the first part of the “max”
k  cb
expression is the dominate one). Whether this helps depends on the rate of growth of  )
 (  A  as
a function of P . The following Lemma shows that this growth is always small enough to ensure that
we never do worse by learning multiple tasks (at least in terms of the upper bound on the number of
examples required per task).

A ,


k	 li  A b  ¿ k H il A cb ¿ 	k  li  A b
  c 

Lemma 5. For any hypothesis space family

164

(30)

A M ODEL OF I NDUCTIVE B IAS L EARNING

¸
#
#
R
#
c
b
Proof. Let 6 denote the set of all functions  
  c  where each can be
k Hij A cb  a¿ member
k Jij 6 of . any
!C A (recall Definition 1). Then
87
b
c
A
6
and
so
By
hypothesis space


k
k
	
 il

b
H

j
i



¿
Lemma 29 in Appendix B,
6
A and so the right hand inequality follows.
n be the meaFor the first inequality,
let  be any probability measure on ß " and let
c
sure on  !<"  obtained by using  on the first copy of  <"
cb flg in the product,#and
b C ignoring
b
 and
all other elements of the product. Let
be an i -cover for  A 
. Pick any
A
c b C be such that fg  #  #  #  b :9 
 ;9 c  b ¿ i . But by construction,
let :9 
 ;9 
flg  #  # 3 #  b : 9 
 ; 9 c  b  f +  # : 9 
  b  , which establishes the first inequality.
Ä Æ k  ji  A b
  ¿ Ä Æ k Jij A cb ¿àP Ä Æ k  ij A b
  
(31)
So keeping the accuracy parameters i and ¼ fixed, and plugging (31) into (28), we see that the upper
bound on the number of examples required of each task never increases with the number of tasks,
and at best decreases as F ×¤NPQ . Although only an upper bound, this provides a strong hint that
By Lemma 5

learning multiple related tasks should be advantageous on a “number of examples required per task”
basis. In Section 3 it will be shown that for feature learning all types of behavior are possible, from
decrease.
no advantage at all to
2.7 Dependence on

i

F ×¤NPQ

¤Ni >

¤Ni

In Theorems 2, 3 and 4 the bounds on sample complexity all scale as
. This behavior can be
improved to
if the empirical loss is always guaranteed to be zero (i.e., we are in the realizable
case). The same behavior results if we are interested in relative deviation between empirical and
true loss, rather than absolute deviation. Formal theorems along these lines are stated in Appendix
A.3.

3. Feature Learning
The use of restricted feature sets is nearly ubiquitous as a method of encoding bias in many areas of
machine learning and statistics, including classification, regression and density estimation.
In this section we show how the problem of choosing a set of features for an environment of
related tasks can be recast as a bias learning problem. Explicit bounds on
and
are calculated for general feature classes in Section 3.2. These bounds are applied to the problem of
learning a neural network feature set in Section 3.3.

k  A e ai3

k  A cb ai3

3.1 The Feature Learning Model
Consider the following quote from Vapnik (1996):
The classical approach to estimating multidimensional functional dependencies is
based on the following belief:
Real-life problems are such that there exists a small number of “strong features,” simple
functions of which (say linear combinations) approximate well the unknown function.
Therefore, it is necessary to carefully choose a low-dimensional feature space and then
to use regular statistical techniques to construct an approximation.
165

BAXTER

qþ$ ø &
o

q














q
q
q

q
p
"
q C_o
pTr q $ B r q $ C p 
A
s
p
r
$
$
¾
C
o
(32)
A  q q j
Now the problem of “carefully choosing” the right features q is equivalent to the bias learning
 C A ”. Hence, provided the
problem “find the right hypothesis space
k e
k cb learner is embedded within
an environment of related tasks, and the capacities  A ai and  A ai are finite, Theorem 2 tells
us that the feature set q can be learnt rather than carefully chosen. This represents an important
simplification, as choosing a set of features is often the most difficult part of any machine learning
problem.
k e
k cb
In Section 3.2 we give a theorem bounding  A ai and  A ai3 for general feature classes.
The theorem is specialized to neural network classes in Section 3.3.
p
Note that we have forced the function class to be the same for all feature maps q , although
p
this is not necessary. Indeed variants of the results to follow can be obtained if is allowed to vary
with q .

In general a set of “strong features” may be viewed as a function
+< mapping the input
into some (typically lower) dimensional space < . Let
be a set of such feature
space
=
maps (each may be viewed as a set of features
). It is the that must be
>= if <
“carefully chosen” in the above quote. In general, the “simple functions of the features” may be
represented as a class of functions mapping < to . If for each
we define the hypothesis
?9
9
, then we have the hypothesis space family
space

3.2 Capacity Bounds for General Feature Classes

	= &
q
ù

s"
á
s"
p
U
C p
 q 	±=
U
ä £¤6å  ¨R &  ± = U
bâ "
pb
b
p b r o $ ¦ r q $ C p b  q Cªo 
pb
k Jil p b  $ D
 E  h Jil p b  f + 
+
f
$ ­°¯ ü 9zCB¨R:
where the supremum is over all probability measures on <á³" , and + :9;9 y   *GF
9 CB= f
y ü  CB= . To
 define the capacity p ofb o we first define a pseudo-metric f t + 2 uvxw on o by
“pulling back” the H metric on  through as follows:
ft + 2 uv w  q  q y  $  «°¬É­°¯  u v 9 r q 	=;/: 9 r q y 	QR f  	=
(33)
ü
ü
I ó
f=t
ft
It is easily verified that + 2 uvxw is a pseudo-metric. Note that for + 2 uv w to be well defined the suprep
b
mum over in the integrand must be measurable. This is guaranteed if theh hypothesis space family
o ft
A   p ib r q $ q C o  is permissible (Lemma
32, part 4). Now define Jil  + 2 uv{w8 to be the
t
pb
f
o
o
smallest -cover of the pseudo-metric space   + 2 u v w  and the i -capacity of (with respect to )
as
k uv Jil o  $ D
   h Jil o  f t + 2 uv{w 
+
where the supremum is over all probability measures on -õ" . Now we can state the main theorem
A@
to <
by
Notationally it is easier to view the feature maps as mapping from
, and also to absorb the loss function into the definition of by viewing each 9
as a
A@
:9 CB
into
via CB
. Previously this latter function would have been
map from <
denoted 9 but in what follows we will drop the subscript where this does not cause confusion. The
class to which 9 belongs will still be denoted by .
?9
9
With the above definitions let
. Define the capacity of in
the usual way,

of this section.

166

A M ODEL OF I NDUCTIVE B IAS L EARNING

i®i 
 K i >

Theorem 6. Let
,

A

be a hypothesis space family as in equation (32). Then for all

k J il A cb ¿
k Jij A e  ¿

k H i 
  p b  c k u v Ji >  o 
k uv Hil o 

ijai 
 ai > . £

with
(34)
(35)

Proof. See Appendix B.
3.3 Learning Neural Network Features

f

In general, a set of features may be viewed as a map from the (typically high-dimensional) input
=
to a much smaller dimensional space
( JLK
). In this section we consider approximatspace
ing such a feature map by a one-hidden-layer neural network with input nodes and J output nodes
QP R
ON
N
=
(Figure 1). We denote the set of all such feature maps by M
where
R
is a bounded subset of TS ( U is the number of weights (parameters) in the first two layers).
This set is the of the previous section.
Each feature N
,
J is defined by

¢

o





¸

f
|
 © | 2 
  | 2  $ C 

| 2 $  ¢ & ä £¤6å ÿ ©¤3
b ¸
¸
¸
|N 2 	9 $ VWX · ¹ B 4 # 4 	 ±KYB b 
[Z
(36)
 ¸
4 

¸
#
b 
  are the output
#
where 4 	± is the output
of the \^] node in the first hidden layer, CB 
 _B
¸
$
node parameters for the ÿ th feature and V is a “sigmoid” squashing function V á& ä £¤6å . Each
# $  ¢ &S , ÿ ¤3 U , computes
first layer hidden node
¸
¸
¸
·
¹
¢
# 	9 $ V WX

 Z[
4  4 K
(37)

¢



a
`
`
4
¸
¸

  are the hidden node’s parameters. We assume V is Lipschitz. The weight
where  

`
` ¢
vector for the entire feature map is thus
P © 

 


  b 
  b ¢ 
 _ B

_ B 
 b  
 _ B = 
 _ B = b  
 

¢

`
`
`
`
Uf
U
and the total number of feature parameters U   K¤K J  K ¤ .
p
For argument’s sake, assume the “simple functions” of the features (the class of the previous
5

section) are squashed affine maps using the same sigmoid function V above (in keeping with the
P
“neural network” flavor of the features). Thus, each setting of the feature weights generates a
hypothesis space:

Ý| $ 



· ¸¹ =

¸ ¸
N | 2 K


 f $  


  	C R y  
=
=

ed
d 
d
d 


R
=
where y is a bounded subset of   . The set of all such hypothesis spaces,
A $ B | Q$ P C R 
Ô í ièë >Ô k ;í h?lmgnh Ô i >Ô k h for all Ô°ÖHÔkporq .
5. è is Lipschitz if there exists a constant g such that h è¨ë j
Vcb

167

(38)

(39)

BAXTER

Multiple Output Classes
n

k

l

Feature
Map

d
Input

P

P

	P]^_

Figure 1: Neural network for feature learning. The feature map is implemented by the first two
hidden layers. The output nodes correspond to the different tasks in the
sample . Each node in the network computes a squashed linear function of the nodes in
the previous layer.

\

 
  
 


=
and feature
is a hypothesis space family. The restrictions on the output layer weights
P
d
d
weights , and the restriction to a Lipschitz squashing function are needed to obtain finite upper
bounds on the covering numbers in Theorem 2.
Finding a good set of features for the environment
is equivalent to finding a good hyP
, which in turn means finding a good set of feature map parameters .
pothesis space
As in Theorem 2, the correct set of features may be learnt by finding a hypothesis space with
small error on a sufficiently large
-sample . Specializing to squared loss, in the present
framework the empirical loss of
on (equation (8)) is given by

Y  Z 

| C A

	P]^_
\
\
c
¸
¸ >
¹
=
¤
¤
·
·
·

¸
¹
¹
b
b
(40)
)W *`  | Å P 
 .tsu62 s Õ 2w vwðRvwv 2ñ s^x65 ó>y k ^ 4 
{z VYb b 
 d N | 2 	 4 9K d ¶ f :< 4}|
Since our sigmoid function V only has range ä £¤6å , we also restrict the outputs " to this range.
|

3.3.1 A LGORITHMS

FOR

F INDING

A

G OOD S ET

OF

F EATURES

Provided the squashing function V is differentiable, gradient descent (with a small variation on
P
backpropagation to compute the derivatives) can be used to find feature weights minimizing (40)
(or at least a local minimum of (40)). The only extra difficulty over and above ordinary gradient
descent is the appearance of “ ” in the definition of
. The solution is to perform gradient
P
= for each node and the feature weights . For
descent over both the output parameters
d
d
more details see Baxter (1995b) and Baxter (1995a, chapter 4), where empirical results supporting
the theoretical results presented here are also given.

ðRñ

W * `  Ý| 
)

 ¶  
168

A M ODEL OF I NDUCTIVE B IAS L EARNING

3.3.2 S AMPLE C OMPLEXITY B OUNDS

\
k J ij A cb 

FOR

N EURAL -N ETWORK F EATURE L EARNING

The size of ensuring that the resulting features will be good for learning novel tasks from the same
environment is given by Theorem 2. All we have to do is compute the logarithm of the covering
numbers
and
.

k J ij A e 
Ý| $aP C  S5 be a hypothesis space family where each }| is of the form
Theorem 7. Let A 3~
¸ ¸

=
·

¸
¹
Ý| $  V b
N | 2   K
¶f $  
 =  C  =  

d

d
d
d
|  O N | 2 
  N | 2 =  is a neural network with U weights mapping from  ¢ to  = . If the
where M
P
feature weights and the output weights ¶3 
 = are bounded, the squashing function V is
U
d
d
d
Lipschitz, is squared loss, and the output space "  ä £¤6å (any bounded subset of  will do), then
there exist constants   =y (independent of il U and J ) such that for all i . £ ,
Ä Æ k Jij A cb  ¿ Â  J K¤ÍP}K U  Ä Æ 
(41)
i
Ä Æ k Jij A e  ¿ Â U Ä Æ Ri y
(42)
(recall that we have specialized to squared loss here).
Proof. See Appendix B.
Noting that our neural network hypothesis space family
into Theorem 2 gives the following theorem.

A

is permissible, plugging (41) and (42)

A   | 

|

Theorem 8. Let
be a hypothesis space family where each hypothesis space
is a
set of squashed linear maps composed with a neural network feature map, as above. Suppose the
number of features is J , and the total number of feature weights is W. Assume all feature weights and
-sample
output weights are bounded, and the squashing function V is Lipschitz. Let be an
generated from the environment
. If

\

Y  Z 

P F Ã i ¤ > À U Ä Æ i¤ K Ä Æ =¼¤ ËRÊ 

	P]^_

(43)

and

^! F Ã i ¤ > À Ã J K¤ K U P Ê Ä Æ i¤ K P ¤ Ä  Æ ¼¤ RË Ê
| C A will satisfy
then with probability at least ¤:¼ any
)* [  }| ¿ )W * `  }| 9 K il
169

(44)

(45)

BAXTER

3.3.3 D ISCUSSION

i

F  K

¼

NPQ

1. Keeping the accuracy and confidence parameters and fixed, the upper bound on the number
of examples required of each task behaves like J
U
. If the learner is simply learning
fixed tasks (rather than learning to learn), then the same upper bound also applies (recall
Theorem 4).

P
^

F  

P

£
¼

and the upper bound on
2. Note that if we do away with the feature map altogether then U
becomes J , independent of (apart from the less important term). So in terms of the
upper bound, learning tasks becomes just as hard as learning one task. At the other extreme,
if we fix the output weights then effectively J
and the number of examples required of
each task decreases as U
. Thus a range of behavior in the number of examples required
decrease as the number of
of each task is possible: from no improvement at all to an
tasks increases (recall the discussion at the end of Section 2.6).

P

¦£

F  NPQ

P

F ×¤NPQ

3. Once the feature map is learnt (which can be achieved using the techniques outlined in Baxter,
1995b; Baxter & Bartlett, 1998; Baxter, 1995a, chapter 4), only the output weights have to be
estimated to learn a novel task. Again keeping the accuracy parameters fixed, this requires no
more that J examples. Thus, as the number of tasks learnt increases, the upper bound on
the number of examples required of each task decays to the minimum possible, J .

F  

F  

4. If the “small number of strong features” assumption is correct, then J will be small. However,
typically we will have very little idea of what the features are, so to be confident that the neural
network is capable of implementing a good feature set it will need to be very large, implying
UJ .
J
U
decreases most rapidly with increasing when UJ , so at least in
terms of the upper bound on the number of examples required per task, learning small feature
sets is an ideal application for bias learning. However, the upper bound on the number of
tasks does not fare so well as it scales as U .

F  K

NPQ

P

F  

¸
A special¬ case of this multi-task framework is one in which the marginal distribution on the input
~ is the same for each task ÿ ¤3P , and all that varies between tasks is the conditional
space 
distribution over the output space " . An example would be a multi-class problem such as face
l¤3P; where P is the number of¸ faces to be recognized and the
recognition, in which " S
¸
marginal distribution on  is simply the “natural” distribution over images of those faces. In that
case, if for every example  4 we have—in addition to the sample  4 from the ÿ th task’s conditional
distribution on " —samples from the remaining P: ¤ conditional distributions on " , then we can
view the P training sets containing ^ examples each as one large training set for the multi-class
problem with ^TP examples altogether. The bound on ^ in Theorem 8 states that ^TP should be
F 	P J K U  , or proportional to the total number of parameters in the network, a result we would
3.3.4 C OMPARISON

WITH

T RADITIONAL M ULTIPLE -C LASS C LASSIFICATION

expect from6 (Haussler, 1992).
So when specialized to the traditional multiple-class, single task framework, Theorem 8 is consistent with the bounds already known. However, as we have already argued, problems such as face
recognition are not really single-task, multiple-class problems. They are more appropriately viewed
6. If each example can be classified with a “large margin” then naive parameter counting can be improved upon (Bartlett,
1998).

170

A M ODEL OF I NDUCTIVE B IAS L EARNING

P

P

as a (potentially infinite) collection of distinct binary classification problems. In that case, the goal
of bias learning is not to find a single -output network that can classify some subset of faces
well. It is to learn a set of features that can reliably be used as a fixed preprocessing for distinguishing any single face from other faces. This is the new thing provided by Theorem 8: it tells us that
provided we have trained our -output neural network on sufficiently many examples of sufficiently
many tasks, we can be confident that the common feature map learnt for those tasks will be good
for learning any new, as yet unseen task, provided the new task is drawn from the same distribution
that generated the training tasks. In addition, learning the new task only requires estimating the J
output node parameters for that task, a vastly easier problem than estimating the parameters of the
entire network, from both a sample and computational complexity perspective. Also, since we have
high confidence that the learnt features will be good for learning novel tasks drawn from the same
environment, those features are themselves a candidate for further study to learn more about the
nature of the environment. The same claim could not be made if the features had been learnt on too
small a set of tasks to guarantee generalization to novel tasks, for then it is likely that the features
would implement idiosyncrasies specific to those tasks, rather than “invariances” that apply across
all tasks.

P

P

^

P

When viewed from a bias (or feature) learning perspective, rather than a traditional -class
classification perspective, the bound on the number of examples required of each task takes on
a somewhat different meaning. It tells us that provided is large (i.e., we are collecting examples
of a large number tasks), then we really only need to collect a few more examples than we would
examples vs. J examples).
otherwise have to collect if the feature map was already known ( J U
So it tells us that the burden imposed by feature learning can be made negligibly small, at least when
viewed from the perspective of the sampling burden required of each task.

P

K NP

3.4 Learning Multiple Tasks with Boolean Feature Maps

i

P

¼

Ignoring the accuracy and confidence parameters and , Theorem 8 shows that the number of
examples required of each task when learning tasks with a common neural-network feature map
J
U
is bounded above by
, where J is the number of features and U is the number of
adjustable parameters in the feature map. Since
J examples are required to learn a single task
once the true features are known, this shows that the upper bound on the number of examples
required of each task decays (in order) to the minimum possible as the number of tasks increases.
This suggests that learning multiple tasks is advantageous, but to be truly convincing we need to
)
prove a lower bound of the same form. Proving lower bounds in a real-valued setting (
is complicated by the fact that a single example can convey an infinite amount of information, so
one typically has to make extra assumptions, such as that the targets
are corrupted by a
noise process. Rather than concern ourselves with such complications, in this section we restrict
our attention to Boolean hypothesis space families (meaning each hypothesis
maps to

and we measure error by discrete loss
if
and
otherwise).

F  K

NPQ

F  

P

 C "

"  

# C A 

U  # 	9R®þ¤ # 	±}§  U  # 	 9Rõ¦£
"   ¤
We show that the sample complexity for learning P tasks with a Boolean hypothesis space family
f= 	PQ (that is, we give nearly matching upper
type parameter
A is controlled by a “VC dimension”
f  	PQ ). We then derive bounds on f  	PQ for the hypothesis space
and lower bounds involving

family considered in the previous section with the Lipschitz sigmoid function V replaced by a hard
threshold (linear threshold networks).
171

BAXTER

F  

As well as the bound on the number of examples required per task for good generalization across
those tasks, Theorem 8 also shows that features performing well on U
tasks will generalize well
to novel tasks, where U is the number of parameters in the feature map. Given that for many feature
learning problems U is likely to be quite large (recall Note 4 in Section 3.3.3), it would be useful
to know that
U
tasks are in fact necessary without further restrictions on the environmental
distributions generating the tasks. Unfortunately, we have not yet been able to show such a lower
bound.
There is some empirical evidence suggesting that in practice the upper bound on the number of
tasks may be very weak. For example, in Baxter and Bartlett (1998) we reported experiments in
which a set of neural network features learnt on a subset of only 400 Japanese characters turned out
to be good enough for classifying some 2600 unseen characters, even though the features contained
several hundred thousand parameters. Similar results may be found in Intrator and Edelman (1996)
and in the experiments reported in Thrun (1996) and Thrun and Pratt (1997, chapter 8). While
this gap between experiment and theory may be just another example of the looseness inherent in
general bounds, it may also be that the analysis can be tightened. In particular, the bound on the
number of tasks is insensitive to the size of the class of output functions (the class in Section 3.1),
which may be where the looseness has arisen.

ZF

 

p

3.4.1 U PPER AND L OWER B OUNDS
S PACE FAMILIES

FOR

L EARNING  TASKS

WITH

B OOLEAN H YPOTHESIS

T©	 
    C   ~ 0

~ 0 $ B # 	 
  # 	   $¨# CT j
~ 
~ 
Clearly ü 0 ü ¿ Â . If ü 0 ü  Â we say shatters  . The growth function of is defined by
  	^_ $  0 L ó ¬  / ~ 0  
   is the size of the largest set shattered by :
The Vapnik-Chervonenkis dimension
=   $    ^ $  	^_ Â  j

First we recall some concepts from the theory of Boolean function learning. Let
be a class of
.
is the set of all binary vectors obtainable
Boolean functions on and
by applying functions in to :



An important result in the theory of learning Boolean functions is Sauer’s Lemma (Sauer, 1972), of
which we will also make use.
Lemma 9 (Sauer’s Lemma). For a Boolean function class

for all positive integers

^

with

  Å f ,

^ ¢
  	^_¿ · ¸¹ ¢ Ã ^ Ê ¿ Ç f

¶ ÿ
.

We now generalize these concepts to learning

P

172

tasks with a Boolean hypothesis space family.

A M ODEL OF I NDUCTIVE B IAS L EARNING

Definition 5. Let
input space by
matrices,



^ matrices over the
Denote the P
A . bec 2  a5 Boolean hypothesis
. c 2  space
5 family.
C
¡C
~



A , define to be the set of (binary)
. For each 
and
 # 
 	 

 
 # 
 	 
ô 

$z# 
 # c CT  
_~  $  
..
..
..

.
.
.
#=c 	 c 
   #Rc 	 c   t




A ~ }$   ´ ó  ~  
  	P]^_ by
Now for each P . £^ . £ , define
 	P]^_ $   L¬ û     A ~   
ó
c   ~   c 


	

]
P


_
^
%

¿
Note that
matrix 
Â . If  A  Â we say A shatters the
c

f  	PQ $    ^ $   	P]^_ Â j
Define

Define

Lemma 10.

. For each

P
.

£

let

f  A  $  =   A 
  and
f  A  $  ó   =   
f A   f A 
f 	PQ   ¢¥ ¡ f  P A  £  f  A 
#

f A  K f  
Â Ã P £ A Ê
Proof. The first inequality is trivial from the definitions. To get the second term in the maximum
C A with =  ª f  A  and construct a matrix
in the second
inequality, choose an
c
.
2
5

f
 C 
whose rows are of length  A  and are shattered by . Then clearly A 
 shatters  . For

the first term in the maximum take a sequence T©	 
  . 5  shattered by A (the hypothesis
¢
space consisting of the union over all hypothesis spaces from A ), and distribute its elements equally
among the rows of  (throw away any leftovers). The set of matrices
 # 	

O
 # 	
ô®


   

$
#
C
..
..

..
A

.
.
.
#
c
#
c  ¤ t

	



	











c

f
~  and has size Â .
where ^ ¦¥  A NP § is a subset of A
c .c 5
Lemma 11.
^
 	P]^_¿ À f= Ç 	PQ Ë ¢?¨
173



¤

¡

BAXTER

P  	P]^_    	 P9^_
 
  c 
#

#>
^
f  	 PQ =  Å P f  	QP 

P

# 
  #Rc

Proof. Observe that for each ,
where is the collection of all Boolean
obtained by first choosing functions
from some
functions on sequences
, and then applying
to the first examples,
to the second examples and so on. By
the definition of
,
, hence the result follows from Lemma 9 applied to
.

C A

^

£ k  A cb ai3

 	 P] Â ^_


If one follows the proof of Theorem 4 (in particular the proof of Theorem 18 in Appendix
A) then it is clear that for all ©.
,
may be replaced by
in the Boolean
Eª
case. Making this replacement in Theorem 18, and using the choices of
from the discussion
d
following Theorem 26, we obtain the following bound on the probability of large deviation between
empirical and true performance in this Boolean setting.

n    
   c  P
 ¤ ¸ and let \ be an
º

	P]^_
^
  ¤£  ¿ ¤

 . Let A B 
0
* \ $¬«=d C A c $ )* g  d   )W a* `  d K ij ¿ È  	P] Â ^_ )   ×: © > P9^_N  È 

(46)
Corollary 13. Under the conditions of Theorem 12, if the number of examples ^ of each task


be probability distributions on
Theorem 12. Let

-sample generated by sampling times from
according to each
be any permissible Boolean hypothesis space family. For all % ©
,

satisfies

^  i > À Â f  	PQ Ä Æ Âi Â K P ¤ Ä Æ
!
d
then with probability at least ¤:¼ (over the choice of \ ), any
)* g  d %¿ )W *a`  d K i

È¼Ë

C A c

(47)
will satisfy
(48)

Proof. Applying Theorem 12, we require

È   	 P] Â ^_ )   ×: © > P9^_N  È ¿ ¼

which is satisfied if

^!  È> À f  	 PQ Ä Æãf Â Ç 	^ PQ K P ¤ Ä Æ È¼ Ë 
©
I M¤ , if
where we have used Lemma 11. Now, for all m
^  Ã ¤ K ¤ Ê I Ä Æ Ã ¤ÅK ¤ Ê I
Ç
Ç
Ä
f  	PQNi > , (49) is satisfied if
then ^!½I Æ ^ . So setting IL  È
^! i > À Â f  	PQ Ä Æ Âi Â K P ¤ Ä Æ È¼ Ë 
174

(49)

A M ODEL OF I NDUCTIVE B IAS L EARNING

Corollary 13 shows that any algorithm learning
requires no more than

P

tasks using the hypothesis space family

^  F Ã i ¤ > À =f  	 PQ Ä Æ i¤ K P ¤ Ä  Æ ¼¤ ËRÊ

c

i

A

(50)

\

P

examples of each task to ensure that with high probability the average true error of any hypotheses
it selects from
is within of their average empirical error on the sample . We now give a
theorem showing that if the learning algorithm is required to produce hypotheses whose average
true error is within of the best possible error (achievable using
) for an arbitrary sequence of
distributions
, then within a
 factor the number of examples in equation (50) is also
necessary.

For any sequence
of probability distributions on
, define
by

A

i
 
  c

= g  A c 

A
Ä Æ 

n    
   c  P
 g  A c  $  ­ó ðR ñ û )* g  d 

c P



 ¤


 contains at least two
A P³be¤3a Boolean
hypothesis space family such that A
 let V c be any learning algorithm taking as input 	P]^_c  -samples
c
.
2
5
Â

C
\  (  ¤ and producing as output P hypotheses d S # 
  #=c  C A . For all
£%½i%M¤N  È and £ % ¤¼ %M¤N  È , if
^ % i ¤ > À f  ¤ 	PQ KM×¤:»i >  P ¤ Ä Æ Ã ¼°×¤ ¤ : ¼3ÊË
!
Â
 

c
n
   
   such that with probability at least ¼ (over the
then there exist distributions
\
random choice of ),
g)*  V c J\° . = g  A c K i

Theorem 14. Let
functions. For each

Proof. See Appendix C

Ä Æ × ¤Ni3
f  	PQ A

3.4.2 L INEAR T HRESHOLD N ETWORKS

P
f  	PQ

factor, the sample complexity of
Theorems 13 and 14 show that within constants and a
learning tasks using the Boolean hypothesis space family is controlled by the complexity parameter
. In this section we derive bounds on
for hypothesis space families constructed
as thresholded linear combinations of Boolean feature maps. Specifically, we assume is of the
form given by (39), (38), (37) and (36), where now the squashing function V is replaced with a hard
threshold:

if ®
V
otherwise

	± $  ¤É: ¤ 

£R

A



 Ry

and we don’t restrict the range of the feature and output layer weights. Note that in this case the
proof of Theorem 8 does not carry through because the constants   in Theorem 7 depend on the
Lipschitz bound on V .

A

f U

Theorem 15. Let be a hypothesis space family of the form given in (39), (38), (37) and (36), with
a hard threshold sigmoid function V . Recall that the parameters , and J are the input dimension,
number of hidden nodes in the feature map and number of features (output nodes in the feature map)
175

BAXTER

$  U  f Kâ¤]K J  U Kâ¤ (the number of adjustable parameters in the feature
f  	PQ ¿ Â Ã U K J K¤ Ê Ä Æ  ÂÇ  J K U K¤z
P
>
Proof. Recall that
. c 2  for5 eachM | P ~  C TS , M | $  ¢ &( = denotes the feature map with parameters P .
C

For each 
, let
denote the matrix
 M | 	

6
 M | 	
ôõ
.
.

.
|M 	.. c 
  . . M | 	 .. c  t 
~  is the set of all binary P ^ matrices obtainable by composing thresholded linear
Note that A
M | ~

respectively. Let U
map). Then,

functions with the elements of
, with the restriction that the same function must be applied to
each element in a row (but the functions may differ between rows). With a slight abuse of notation,
define

°¯ 	P]^_ $   ¬  û   
ó

~ M


| ~  $aP C 

S



 



. c 2 5
C
Fix 
. By Sauer’s
 Lemma, each node in the first hidden layer of the feature map computes

f 5 ¤ ¢ functions on the P9^ input vectors in  . Thus, there can be at most
at most  Ç ^TPQNb  . K
 Ç ^TPQN f K¤ ¢ 
 distinct functions from the input to the output of the first hidden layer on
the P9^ points in  . Fixing the first hidden layer
U b 
 parameters, each node in the second layer of. b the5
feature map computes at most  Ç ^TPQN K¤  functions on the image of  produced at the output
U = 

of the first hidden layer. Thus the second hidden layer computes no more than  Ç ^TPQN K¤ 
functions on the output of the first hidden layer on the P9^ points in  . So, in total,
b . 
 5 ^TP = . b 
 5
T
^
P
 ¯ 	P]^_ ¿ Ã f Ç K¤RÊ ¢ Ã U Ç K¤Ê  
°
| ~  , the number of functions computable on each row of M | ~  by a
Now, for each possible matrix M


=
thresholded linear combination of the output of the feature map is at most  Ç ^_N J K¤  . Hence,
c . = 
 5 obtainable by applying linear threshold functions to all the
the number of binary sign assignments
 . Thus,
rows is at most  Ç ^_N J K ¤
b. 
5
c . 
5
.b 
 5
  	P]^_¿ Ã f Ç ^TKP ¤ Ê ¢ Ã UÇ ^TKP ¤ Ê =  Ã PÅ Ç ^TKP ¤ Ê =  
J
$
Ä
q 	±  Æ  is a convex function, hence for all IG ± . £ ,
IÉK U GY
K ±
¤
J
q Ã J K U K¤ Ê ¿ J K U K¤  JRq JIK U q HGK q ² ±
b¶µ
b¹µ ¤
U
=´
=}´
K

K
¤
¤
¤


¸
·
·
J
³

Ã J IõK U G]Kc± Ê
ÃIÊ ÃGÊ Ã±Ê 
U K¤ , G f K¤ and ± PÅ J K¤ shows that
Substituting I 
c .= 
 5
U
T
^
Å
P

K

K

¤


 
S
 	P]^_¿ Ã Ç K J PÅ K¤ Ê
(51)
U

J

176

A M ODEL OF I NDUCTIVE B IAS L EARNING

Hence, if

^TPÅ J K U K ¤
Ä


Æ
Ç
K
K
¤
(52)
. Ã P
J
Ê > Ã U K PÅ J K¤ Ê
 	P]^_º% Â c  and so by definition f  	PQ¿^ . For all I . ¤ , observe that  . I Ä Æ > 
then
Ä
U K¤N U K PÅ J K¤ and I  Ç  J K U K¤ shows that
if T Â I Æ > Â I . Setting T Ç ^TPÅ J K
Ä
U K¤ .
(52) is satisfied if ^  Â  U NP}K J K¤ Æ >  ÂÇ  J K
f
U
Theorem 16. Let A be as in Theorem 15 with the following extra restrictions:  Á ,  J and
f
¿ . Then
J
f  	PQ  ¤ Ã ¡ U P £ K J K¤ Ê
Â Â


f
f
Proof. We bound  A  and  A  and then apply Lemma 10. In the present setting A contains all
f
U
three-layer linear-threshold networks with input nodes, hidden nodes in the first hidden layer, J
^

U

hidden nodes in the second hidden layer and one output node. From Theorem 13 in Bartlett (1993),
we have

=   A 
  lf U K U  J :½¤ K¤3
Â
f
which under the restrictions stated above is greater than U N Â . Hence  A 
¿ f U

U

NÂ.

f :

As J
and
J we can choose a feature weight assignment so that the feature map is the
J
identity on J components of the input vector and insensitive to the setting of the reminaing
components. Hence we can generate J
points in
whose image under the feature map is
J
shattered by the linear threshold output node, and so
.

Ká¤

f  A  ]  K¤

Combining Theorem 15 with Corrolary 13 shows that

^! F Ã i ¤ > À Ã U P K J K¤ Ê Ä Æ i¤ K P ¤ Ä Æ ¼¤ ËÊ
examples of each task suffice when learning P tasks using a linear threshold hypothesis space family,
while combining Theorem 16 with Theorem 14 shows that if

^ ¿¼» Ã i ¤ > À Ã U P K J K ¤ Ê K P ¤ Ä  Æ ¼¤ ËÊ
then any learning algorithm will fail on some set of P tasks.
4. Conclusion
The problem of inductive bias is one that has broad significance in machine learning. In this paper
we have introduced a formal model of inductive bias learning that applies when the learner is able
to sample from multiple related tasks. We proved that provided certain covering numbers computed
from the set of all hypothesis spaces available to the bias learner are finite, any hypothesis space
that contains good solutions to sufficiently many training tasks is likely to contain good solutions to
novel tasks drawn from the same environment.
In the specific case of learning a set of features, we showed that the number of examples
J
U
required of each task in an -task training set obeys
, where J is the number of

P

^ F  K

177

NPQ

^

BAXTER

features and U is a measure of the complexity of the feature class. We showed that this bound is
essentially tight for Boolean feature maps constructed from linear threshold networks. In addition,
we proved that the number of tasks required to ensure good performance from the features on novel
tasks is no more than U . We also showed how a good set of features may be found by gradient
descent.
The model of this paper represents a first step towards a formal model of hierarchical approaches
to learning. By modelling a learner’s uncertainty concerning its environment in probabilistic terms,
we have shown how learning can occur simultaneously at both the base level—learn the tasks at
hand—and at the meta-level—learn bias that can be transferred to novel tasks. From a technical
perspective, it is the assumption that tasks are distributed probabilstically that allows the performance guarantees to be proved. From a practical perspective, there are many problem domains that
can be viewed as probabilistically distributed sets of related tasks. For example, speech recognition
may be decomposed along many different axes: words, speakers, accents, etc. Face recognition
represents a potentially infinite domain of related tasks. Medical diagnosis and prognosis problems
using the same pathology tests are yet another example. All of these domains should benefit from
being tackled with a bias learning approach.
Natural avenues for further enquiry include:

E

F  

A

Alternative constructions for . Although widely applicable, the specific example on feature
learning via gradient descent represents just one possible way of generating and searching
the hypothesis space family . It would be interesting to investigate alternative methods,
including decision tree approaches, approaches from Inductive Logic Programming (Khan
et al., 1998), and whether more general learning techniques such as boosting can be applied
in a bias learning setting.

E

A

A

Algorithms for automatically determining the hypothesis space family . In our model the
structure of
is fixed apriori and represents the hyper-bias of the bias learner. It would
be interesting to see to what extent this structure can also be learnt.

E

E

A

Algorithms for automatically determining task relatedness. In ordinary learning there is usually little doubt whether an individual example belongs to the same learning task or not.
The analogous question in bias learning is whether an individual learning task belongs to a
given set of related tasks, which in contrast to ordinary learning, does not always have such
a clear-cut answer. For most of the examples we have discussed here, such as speech and
face recognition, the task-relatedness is not in question, but in other cases such as medical
problems it is not so clear. Grouping too large a subset of tasks together as related tasks could
clearly have a detrimental impact on bias-learning or multi-task learning, and there is emprical evidence to support this (Caruana, 1997). Thus, algorithms for automatically determining
task-relatedness are a potentially useful avenue for further research. In this context, see Silver
and Mercer (1996), Thrun and O’Sullivan (1996). Note that the question of task relatedness
is clearly only meaningful relative to a particular hypothesis space family (for example, all
possible collections of tasks are related if contains every possible hypothesis space).

A

A

Extended hierarchies. For an extension of our two-level approach to arbitrarily deep hierarchies,
see Langford (1999). An interesting further question is to what extent the hierarchy can
be inferred from data. This is somewhat related to the question of automatic induction of
structure in graphical models.
178

A M ODEL OF I NDUCTIVE B IAS L EARNING

Acknowledgements
This work was supported at various times by an Australian Postgraduate Award, a Shell Australia Postgraduate Fellowship, U.K Engineering and Physical Sciences Research Council grants
K70366 and K70373, and an Australian Postdoctoral Fellowship. Along the way, many people
have contributed helpful comments and suggestions for improvement including Martin Anthony,
Peter Bartlett, Rich Caruana, John Langford, Stuart Russell, John Shawe-Taylor, Sebastian Thrun
and several anonymous referees.

Appendix A. Uniform Convergence Results
Theorem 2 provides a bound (uniform over all ½¿¾ÁÀ ) on the probability of large deviation between
Â?ÃÄ1Å
Â?ÃÈpÅ
½	Æ and Ç
½	Æ . To obtain a more general result, we follow Haussler (1992) and introduce the
following parameterized class of metrics on ÉAÊ :
ÎmÖ×Ñ
Õ
Õ Ï
Î Ø8ÑÙØcÚ


Ë"Ì Í ÎÐÏ_Ñ"ÒeÓ¹Ô

ÚÜÛÞÝ
. Our main theorem will be a uniform bound on the probability of large values
Ë"Ì1Í Â?ÃÄ1Å
Ï Â?ÃÈpÅ
Ò
Ö Â?ÃEÈ"Å
Â?ÃÄ1Å
½	Æ
Ç
½ßÆ Õ . Theorem 2 will then follow as a corollary,
½	Æ
½	Æ , rather than Õ
ÔDÝ
Â?ÃEÈ"Å
will better bounds for the realizable case Ç ½ßÆ
(Appendix A.3).
ËpÌ

where

Lemma 17. The following three properties of

3. For

are easily established:

ÏEáÙâ¼Ý

1. For all à
2. For all

of
as

Ýnã

ÝLã
à

ÝãËpÌÍ EÏ áäÒÐãæå
,
à
ã¼áãèç Ë"Ì1Í ÏEá?ÒÐã¼Ë"Ì1Í Ï_çéÒ
à
à
,

à

ÏEá¤ãæå

,

ãË"Ì Í EÏ á?ÒÐã
Ì
ë ì}íïîë
à
Ê¸ð

and

Ë"Ì Íêá^Ï_çéÒãËpÌ Í _Ï çéÒ
à

.

Ì
ë ìíïîë

For ease of exposition we have up until now been dealing explicitly with hypothesis spaces ½
ÓQò!óô
ò÷ö	ô3óøÍ ÝjÏäå}Ò
containing functions ñ
, and then constructing loss functions ñQõ mapping
ÎÅ ÐÏ_Ñ
Ó¹Ôù Å Å Î Ï_Ñ
ùÓúôÁöAô3óûÍ ÝjÏäå}Ò
by ñQõ
. However, in general we can view
Æ
ñ
Æ
Æ for some loss function
ò+ö®ô
Í ÝjÏäå}Ò
ñQõ just as a function from an abstract set ü (
) to
and ignore its particular construction
ù
in terms of the loss function . So for the remainder of this section, unless otherwise stated, all
Í ÝjÏäå}Ò
. It will also be considerably more
hypothesis spaces ½ will be sets of functions mapping ü to
_
Ï
þ
ý
convenient to transpose our notation for ÅCý
Æ -samples, writing the
training sets as columns
instead of rows:
ÿ

 ...  . .. ...
 	    
 (Equation 9 and prior discussion),
where each 

. Recalling the definition
of





with this transposition lives in
. The following definition now generalizes quantities


like
,
and so on to this new setting.

Definition 6. Let  
of functions mapping
into
. For any 
    , let  be  orsetssimply
the map
 denote

    
   
 

Ô

ÿ

Å ò÷öô

¾¼ü

½

Â?Ã È Å
Ç
½	Æ

Ï

Ï

Â?Ã

Å

Åò

½ßÆ

ñ

½

¾ß½

Ï

ñ

Ï

ößô

Æ

Æ

ý

½

ü

ñ

Å

Æ

Ô

å

ý

179

ñ

Å

Æ

Í ÝjÏäå}Ò

ñ

¾

BAXTER

      . Let !"#   denote the set of all such functions. Given
 $ %     &  and elements
of
,      (or equivalently an element of
by writing the  
 as rows), define
 



'   

(recall equation
 , define(8)). Similarly, for any product probability measure ( *)   +)  on
  -,#.0/   ( 

(recall equation (26)). For any  21
(not necessarily of the form 345  ),
define
   1 %, . /    1  ( 
 to , define
(recall equation (17)). For any class of functions mapping
6 57 98;:=?< > @7 
 and 7  is the
where the supremum is over all product probability measures on
>
size of the smallest 7 -cover of under  (recall Definition 4).
Ô

for all

5¾ ½
ö	ô
Åò

Å

Ï

ÿ

Ï

Æ

½

¾Üü
þ

½

Åò

½

ö	ô

Ï

Å

Æ

Ï

Æ

Æ

å

Ó¹Ô
Â?ÃÈ"Å
Ç
Æ

Å

þ

Æ

Ô

Åò

ö	ô

Æ

Â?Ã

Ï

Ë

Å

Ó Åò

Ï

Å

Ó¹Ô

Æ

ö	ô

Å

Å

Å

Õ

ñ

Ö

Å

Æ

Ï

Å

Å

Ï

ÏË

½

Í ÝjÏäå}Ò

Æ

Æ

Åò

Ë

½

ñ

Æ

Å ò÷öô

Ó¹Ô

½	Æ

Ë

Æ Õ

½

Å

ö

Æ

óûÍ ÝjÏäå}Ò

Æ

Ó¹Ô

Æ

Ë

Æ

ö

ö	ô

Å

Æ

Ï

½

ÏË

Æ

The following theorem is the main result from which the rest of the uniform convergence results
in this paper are derived.

BA   C    

ÿ




EDF @G
JI G I
( 9)   H) 
   GQP
+R 6 @G ST VU < G W T  (53)
ò÷ößô

be a permissible class of functions mapping Å
Theorem 18. Let ½ è½
Æ into
½
Í ÝjÏäå}Ò
òÅ ÷ö	ô
þâ
Ú
ö	ô
Å
Åò
Æ
Æ
. Let ¾
be generated by
ð Æ independent trials from
Ô
ö
ö
ÚßÛ¼Ý Ý
æå
according
to
some
product
probability
measure
.
For
all
,
,
ÿ

K ML
Ã

¾

Åò

ößô

     N8;:#<
O
Ó

Æ

ËpÌÍ Â?ÃÈ Å
Ï Â?Ã
Ç
'Æ

Å

'Æ

ÒÐÛ

ã

Ú

Å

Ï

½	Æ

Å Ö

Â

ð

Ú ý þ

Æ

The following immediate corollary will also be of use later.

YX[Z U]\ G T H^ _&` R 6bac gd fe G D ih

Corollary 19. Under the same conditions as Theorem 18, if
Ì

þ!â

ð

then

K L
Ã

ÿ
¾

Å ò÷ö	ô
Æ

Ú ý

     j8;:#<
O
Ó

Ï

½

Ï

ð

Ú

  

ËpÌÍ Â?ÃÈ"Å
Ï Â?Ã
Ç
'Æ

Å

'Æ

Ò'Û

Ï

(54)

GP g
ã

(55)

A.1 Proof of Theorem 18
The proof is via a double symmetrization argument of the kind given in chapter 2 of Pollard (1984).
I have also borrowed some ideas from the proof of Theorem 3 in Haussler (1992).
180

A M ODEL OF I NDUCTIVE B IAS L EARNING

A.1.1 F IRST S YMMETRIZATION

Ô

ÿ

     , let

ÿ
Å å

Æ ð

    
..
..
..
.
.
 k  . 

ÿ
Æ

Å ò÷öô
¾

An extra piece of notation: for all
bottom half, viz:
Å å

ÿ

ÿ
Å

D

D

and Å Æ be the

be the top half of

  l	   
..
..
..
.
.
 l   .  
Ê

Ô
Æ

Æ

ÿ

Ê

ð

ð

The following lemma is the first “symmetrization trick.” We relate the probability of large deviation
between an empirical estimate of the loss and the true loss to the probability of large deviation
between two independent empirical estimates of the loss.



mI G I



ò÷ö	ô

Lemma 20. Let ½ be a permissible set of functions from Å
Æ
ò÷öô
Ú	ÛÝjÏÝ
æå
þ!â
Æ . For all
probability measure on Å
and

K L

ÿ

Ã

Åò
¾

ößô
Æ

     N8;:#<
O
Ó

po

GP
     N8;:#< rq   
O

cFn

into
Ì
ð ,

ËpÌÍ Â?ÃÈ Å
Ï Â?Ã 1Å
Ò'Û
Ç
ñÿÆ
ï
ñ Æ
¬

YD K L
ã

Ã

Ó

ð

¾®ü

ËpÌ

Â?Ã È
Ç

Å

ñ¬Æ

Ï Â?Ã È
Ç

Í ÝjÏäå}Ò

and let

)

be a

  ts G D P 
ð

Å

Û

ñ¬Æ

(56)

q
rq      ts G D    uo s G
q      s G zD y
     0w
q    uo s G and
q   uo s I G D y 

Proof. Note first that permissibility of ½ guarantees the measurability of suprema over ½
ËpÌ
Ë"Ì Â?Ã È
Ï Â?Ã 1Å
Û
Å
Ç
ñïÆ
ñ Æ
ï
(Lemma 32 part 5). By the triangle inequality for
, if
and
Ë"Ì Â?Ã È
Ï
"
Ë
Ì
Ï
Û
Å
Â?Ã Å
Â?Ã È
Å
Â?Ã
Å
, then
. Thus,
Ç
ñïÆ
ñïÆ
Ç
ñïÆ Ç È
ñïÆ

]q  
K &v

Vo ts I G  D
     xw
K v

ÿð

Ã

¾

Å ò÷öô

Ó

Æ ð

â

ð

ÿ ñ¾ß½

Ã

ÓQË"Ì

¾

Å ò÷ö	ô

Â?Ã È
Ç

Å

ñïÆ
Ó

Æ ð

Ï Â?Ã È
Ç

Å

ð

ñ¾¢½

Û

ñïÆ

Ó"Ë"Ì

Â?Ã È
Ç

Å

Ë"Ì

Â?Ã È
Ç

Å

ð

ñïÆ
ñïÆ

Ï Â?Ã 1Å
Ï Â?Ã 1Å

Û

ñïÆ

(57)

ñïÆ

By Chebyshev’s
inequality, for any fixed ñ ,
ÿ

K v
Ã

¾

Åò

ö	ô
Æ

   

ÓQË Ì Í Â?Ã È Å
Ï Â?Ã
Ç
ñïÆ

o {I G D y
K L
Å

ñïÆ

Ò

â

Ã

âæåÖ
â

-DF @G

o

þâ
Ú
ã
Å
Â?Ã 1Å
as
ð Æ and
ñ Æ
¬
gives the result.

å

ÿ

D
å

Â?Ã

o

Åò
¾
Å

ö	ô

Å årÖ
ñïÆ
þÚ

G

   
|R o
Æ

ð

Â?Ã

Å

Ö
Â?ÃÈ"Å
Ó Õ Ç
ñïÆ

o

Â?Ã 1Å
Ú

ñïÆ Õ

I GD P

ñïÆ_Æ

. Substituting this last expression into the right hand side of (57)

181

BAXTER

A.1.2 S ECOND S YMMETRIZATION
The second symmetrization trick bounds the probability of large deviation between two empirical
estimates of the loss (i.e. the right hand side of (56)) by computing the probability of large deviation when elements are randomly permuted between the first and second sample. The following
definition introduces the appropriate permutation group for this purpose.

}       D  D
~





 



~     ~     ~  
  ~  

     and any ~  H}      , let 
For any
F  l      
 .. . . .  ..
 . l    .    
 into
"  be a permissible set of  functions
Lemma 21. Let
mapping
   and let    3W be
(as in the statement of Theorem 18). Fix 
ST -cover for , where   1 + 
'     
  1  
 where the  
 are the rows
an G
of . Then,
K L ~ H}      j8:#O < ]q ;     ;    ts G D P
  K v ~ H}      q      
     
 ts GR y (58)

' 
where each ~ H}      is chosen uniformly at random.
q ;     ;    ts G D (if there is
Proof. Fix ~ }      and let 
be such that
G ST . Without loss
no such  for any ~ we are already done). Choose 
such that





 3  . Now,
of generality we can assume  is of the form 
D

'    u    
    
  



 
 
'     u     
        
     
  
'    p    q   
       
    s 
  
'   u  q   
       
    s
  
'     p    q   
       
    ts 
  
'     u  q   
       
    ts
q       s q         s 
þ×Ï

â

å

ý
Definition 7. For all integers
, let
denote the set of all permutations of the
åÅ Ïäå Ï
>
Ï Å >
å Ï ý Ï ð
Ï Å þ×Ïäå Ï
Ï Å þ×Ï ý
å ã
&
sequence of pairs of integers
Æ
Æ
Æ
Æ such that for all ,
Ï
Ô
þ
Ø
Ï

þ
Ø
Ï
Ô
Ï
Ï
Ô
Ï

þ
Ø
Ï
Ô
ãþ
Å
Å
Å
Å
Å
Å
Æ
Æ and
Æ
Æ or
Æ
Æ and
Æ
, either Å
Ï
Åþ Ø
Æ . ÿ
Å ò÷öô

¾

Æ ð

×¾

ð

ÿ

Ó¹Ô

½

Í ÝjÏäå}Ò
ÿ

Ú

Ã

ð

Ô

Å

½

½

ÏË È

½

Ë ÈpÅ

Æ

Ó

×¾

Ë"Ì

Â?Ã È
Ç

ð

Ï

Å

Æ

ã

×¾

ð

ÿ

Å ò÷ößô

¾

Ó¹Ô

Æ

ð

ð

Ï Â?Ã È
Ç

Å

Ã

ð

ÓQË Ì

¾

ËpÌ

Ú

Ë È Å

Ï

ð

>Æ

ð

Ô

â

Å

ñ
Ú þ
j

Ô

Ø

ÔË"Ì

Úaþ

Â?Ã È
Ç

Ø

Ï ?
Â Ã È
Ç
Ë È Å

Ïð

ð

Å

>Æ

Å

Æ

Û

Æ
ã

Ú

Æ

Å

Å

Æ

182

Å

Å

Å

ñ
ØcËpÌ

Æ

Ø

Æ

ñ

Ê

Ö

Æ

Ê

ð
Ï
Å
Â?Ã
Æ Ç È

Å

Å

ñ

ý

'Æ

Ï Â?Ã È
Ç

ý

Ø

ð

Å

Æ

Æ

Ö

Æ

Újþ

Æ

Ï

Æ Õ

Å

Â?Ã È
Ç

Å

Å

ñ

ý

Ï

ý

ñ

Újþ

Ö

Æ

Ó¹Ô

½ Ç

Å

½ Ç

¤¾

Ô

Ö

Æ

Â?Ã È
Ç

ð

¾8½

ð

Å

Õ

Æ ð

ö	ô

Û

'Æ

ð

¼¾

Åò

Æ

Æ

Ö

Å

Å

Æ

Ø

Â?Ã È
Ç

ð

Å

Æ

Å

Ï Â?Ã
Æ Ç È

ð

Æ

Å

Æ

Û

Ï

A M ODEL OF I NDUCTIVE B IAS L EARNING

D

ËpÌ

Hence, by the triangle inequality for

,

q        s
q         ts
q ;     ;    s 
]q     
But
and
  q G  | R   by construction
     s G |R . Thus,
(59) implies
rq          ts
v ~ H}      xw 
A v ~ H}      xw 
Ú

Ì Ë p
È Å
ð

Ï

>Æ
Ë"Ì

 

Ë È Å

Ï

>Æ

ã

Â?Ã È
Ç
Ó

×¾

âË Ì Â?Ã È
Ï Â?Ã
Â?Ã È
Å Ï Â?Ã È
Å
Å
Ç
>Æ Ç
Æ
Ç
Æ Ç È
ð
ØcË Ì ?
Ï Â?Ã
ØcË Ì Â?Ã È
Â Ã È
Å
Å
Å Ï ?
Â Ã
Ç
'Æ Ç È
Æ
Ç
Æ Ç È
ð
ð
âËpÌ Â?Ã È
Ï
Å
Â Ã
?
Å
Ç
'Æ Ç È
Æ
ð
Ë"Ì Â?Ã È
Ï ?
Û
Å
Â Ã
Å
Ç
Æ Ç È
Æ
Û
ð
Å Ï Â?Ã È
Å
>Æ Ç
>Æ
ð
ÓúË"Ì

¾¢½

ð

q       s
q          ts
(59)
    ts G D by assumption, so
GD y
q          s GRy

Ø Ë Ì

Â?Ã È
Ç

Å

Ï Â?Ã È
Ç

Æ

×¾

Å

ð

Û

'Æ

Ó

Ù¾

ð

ÓïË"Ì

½ Ç

Â?Ã È
Ç

Å

Å

Æ

Å

ð

Ï Â?Ã È
Ç

Æ

Æ

Å

ð

Æ Æ

Û

Ï

which gives (58).



     ,be any function that can be written in the form 
 {3  
K Mv ~ b}      ]q          ts GR y YD VU < G T 
(60)
where each ~ H}      is chosen uniformly at random.


Proof. For any ~ f}      ,
   q   
        
    ts 





p




q          ts 
 

(61)







 
    u 

  ,  , let
To simplify the notation denote  
 by  
 . For each pair  ,
D and

l be an independent random variable









such that 

with probability



l   
    
 with probability D . From (61),
K Mv ~ H}      q          s GR y




l



     
£¤Y¥l¦
K  ~ H}         q   
        
    ts  GR¡ ¢
 
'  p 


'  p  §


     
 £¤+¥ ¦
K        
  GR¡ ¢
 
'  u  

   u  §


0¨ with bounded ranges © 
 
 «ª 
 , HoFor zero-mean independent random variables  
effding’s inequality (Devroye, Györfi, & Lugosi, 1996) is
¨

K \  
  ¬ h YD VU <®­ ¨ D3ª ¬


 '  
 
   
 © 
 #¯
Now we bound the probability of each term in the right hand side of (58).
ÿ
Å ò÷öô

Ó

Lemma 22. Let
. For any
Ã

ó

Æ

Å

ð

Å

Û

>Æ

ã

Ö

Â

Újþ

ð

ý

Ï

ð

Å Ï Â?Ã È
Æ Ç

Â?Ã È
Ç

Å

ð

Å

Ô

Æ

Újþ

Æ

Ö

Ø

ý

Ö

Æ

Å

Å

ð

ô

Ô

Æ

Ê

Æ

åYã

Å

ô

Ô

Ï Â?Ã È
Ç

>Æ

ð

¾

ô

Ô

Æ ð

Â?Ã È
Ç

ð

×¾

Í ÝjÏäå}Ò

ö	ô

ÓQË"Ì

¾

Ë Ì

¾

Åò

ã

þ

åYã

Dã

Ö

å

Ê

å

Ê

Ã

ÓïË"Ì

×¾

Ô

Ô

Â?Ã È
Ç

ð

Ã

Ã

¾

Å

Ï Â?Ã È
Ç

Æ

ð

Ó

Újþ

ý

Û

Æ

Å

ð

ô

Å

Ö

Æ

Å

ô

Ê

Úaþ

ý

Ø

ð

ð

Ø

ô

Ã

Û

Æ

â

ã

Ï

Â

183

Ïô

'Ö

ã

ð

Å

Ö

Æ ð

ô

ã

ý

BAXTER


 is  
   
     
  
    , we have
     
£¤+¥ ¦ YD VU <  ± µ G D ³²    
     u    
V´ £¤ ¶
K l      
  GR° ¢
¢  
    p   
  
   
 
'  p  

   u  §
  
Let ·
 
    p   
 . As  
 ,  
    u    
   
 · . Hence,

D VU <  ±¢ µ G D ² 
' u  
'  
l  u 
   
l  ´ £¤ ¶ YD VU <  G µ D ·  
·
    
R . Hence
giving a value of
·  · is minimized by setting ·
K v ~ f}      q          ts GR y YD VU <  G T 
ô

Noting that the range of each
ô

Ã

Ô

Û

ý

Ø

Újþ

ð

Ö

Å

¾

ð

Â?Ã È
Ç

Å

ð

ð

Ö

ã

Æ ð

Ê

Ö

Â

Æ ð

Ê

ð

Å Újþ

Ø

ý

eÆ ð

Æ ð

Ê

ÔDÚjþ

ÓúË Ì

Ø

ý

Å

Ö

ã

Ò

Újþ

ð

Ö

Â

ð

ð

Æ Õ

Ê

Å

eÆ ð

Ã

Ö

ãæå

Ø

ý

Ï
Æ Õ Õ

Ê

ã

Ýã

Ö

Ö

Õ

ð

Ø

ý

ð

Â

Å Újþ

Újþ

ÍÖ

"Újþ

ý

Ï Â?Ã
Æ Ç È

Å

ð

Û

Æ

ã

ý

Ö

Â

ð

Úaþ

ý

Ï

as required.
ÿ

   
21 and 22 give:
K L ~ H}      j8:#O < ]q ;     ;    ts G D P
YD > @G ST
VU <W G T  
)   ;)  and each ) 
 is the empirical distribution that
Note that
is simply  where (




 D (recall Definition 3). Hence,
puts point mass
on each 

     N8;:=< q          s G D P
K L ~ H}     
O
YD 6 @G ST VU <W G T  
Now, for a random choice of , each 
 in is independently (but not identically) distributed and ~
)
only ever swaps 
 and 
    (so that ~ swaps a 
 drawn according to  with another component
drawn according to the same distribution). Thus we can integrate out with respect to the choice of
~ and write
     j8;:#< q        s G D P
K L
O
YD 6 @G ST VU <W G T  
A.1.3 P UTTING
¾

For fixed
Ã

Åò

IT T OGETHER
ö	ô
Æ ð
, Lemmas

Ó

×¾

Ë"Ì

Â?Ã È
Ç

ð

Å

Ï Â?Ã È
Ç

Æ

Å

ð

Û

'Æ

ã

Ë È

Ë

Ô

å Gþ

Ï ¤Ô

å>Ï

Ï

Å

Ï

Å

Ú

Ï

ÏË È

½

Â

eÖ

Â

eÖ

ð

Â

eÖ

ð

Æ_Æ

ð

Újþ

ý

Æ

Ï þ

ÿ

Ã

Ï

×¾

¾

ð

Åò

öô

Ó

Æ ð

ÿ

Ë"Ì

Â?Ã È
Ç

Å

Ï Â?Ã È
Ç

Æ

ã

Å

ã

Å

ÿ

Å

ð

Ú

Û

Æ

Ï

½	Æ

Újþ

ý

Ê

ÿ

Ã

¾

Åò

ößô

Æ ð

Ó

ËpÌ

Â?Ã È
Ç

Å

Æ

Ï Â?Ã È
Ç

ð

Å

Æ

Applying Lemma 20 to this expression gives Theorem 18.
184

Û

Ú

Ï

½	Æ

Újþ

ý

A M ODEL OF I NDUCTIVE B IAS L EARNING

A.2 Proof of Theorem 2

)   ;) 

(

Another piece of notation is required for the proof. For any hypothesis space ½
Ô Å
Ï
Ï
measures
Æ on ü , let



Â?Ã
Ç

Å

½	Æ

  »&¼ O o&½ 

' ]¸'¹=º

å

Ó¹Ô

Â?Ã

ý

Å

ñ¬Æ


)   ;) 
K  (
(



and any probability



Â?Ã Å
Ç
½ßÆ is another empirical
Note that we have used Â?Ç Ã Å ½	Æ rather than Â?Ã Å ½ßÆ to indicate that
ÿ
Â?ÃÄ1Å
estimate of
½	Æ .
Ï_þ
With the ÅCý
there is also generated a seÆ -sampling process, in addition to the
ÿ sample
Ô Å
Ï
Ï
Æ although these are not supplied to the learner.
quence of probability measures,
Ï
ò
ößô
ö
Ó
Æ
means
This notion is used
in the following Lemma, where Ã Å Æ¾ Å
ÿ
Å nÏ
“the probability of generating a sequence of measures from the environment
Æ and then an
ÅCý Ï_þ
Æ -sample according to
such that A holds”.

(

(

Lemma 23. If

ÿ

K L (
Ã

and

Å

Ï

Åò

Æ1¾

ö	ô
Æ

 ¾ ! f¿  N8:#<
Ã
ö

Ó

K L ( ¿  Ä8;:#Ã < 
 ¾ ! Ä8;:#<
K L
Ã
Ã

Ó

¾

Ë"Ì Í Â?Ã
Ç

Å

 ¾ ! À¿  xÁ 
¿ pÂ

Ë"Ì1Í Â?ÃÈ"Å
Ï Â?Ã
Ç
½	Æ Ç

½	Æ

Ï Â?Ã}Ä1Å

½	Æ


Å

G D P Dg

ÒÛ

ã

ÿ

then

Ã

Åò

¾

ö	ô

Ó

Æ

Ë"Ì1Í Â?ÃÈpÅ
Ï Â?ÃÄ1Å
ÒÐÛ
Ç
½	Æ
½	Æ

Proof. Follows directly from the triangle inequality for

Ë Ì

½	Æ

G D P Dg

ÒÐÛ

ã

Ï

(62)

Ï

(63)

G P g
ã

.

We treat the two inequalities in Lemma 23 separately.

A.2.1 I NEQUALITY (62)





In the following Lemma we replace the supremum over ½¿¾ÁÀ
over 8¾ÁÀ .
Lemma 24.
ÿ

K ML (
Ã

Å

Ï

Æ ¾

Å ò÷öô
ã

in inequality (62) with a supremum

   f¿  j8;:#<
QG P

Ã
    f¿  N8;:#/Å <
K \ (
  
Ã
ö

Æ

Ó

Ã

ÿ

Å

Ï

Æ¾

ËpÌÍ Â?ÃÈ"Å
Ï Â?Ã
Ç
½ßÆ Ç

Å ò÷öô

ö

Æ

185

Ó

Å

½ßÆ

ÒÐÛ

ËpÌ Í Â?ÃEÈpÅ
Ï Â?Ã
Ç
'Æ

Å

'Æ

ÒÛ

Gh

(64)

BAXTER
ÿ

(
-&
Ï



8;:#< Ã

ËpÌ Í



Ï

G
jI 7

ÒnÛ

7

Â?ÃEÈ"Å
Â?Ã Å
Proof. Suppose that Å Æ are such that
. Let ½ satisfy this inÇ
½ßÆ Ç
½ßÆ
ã Â?Ã Å
Û Ý
Â?ÃÈ"Å
Â?ÃÈ"Å
Ç
½ßÆ . By the definition of Ç
½ßÆ , for all
there
equality. Suppose first that Ç ½ßÆ
Ó¹Ô
Ø
Ë"Ì
Â?ÃÈ"Å
Â?ÃEÈ"Å
exists 5¾ ½
. Hence by property (3) of the
½
½ such that Ç
Æ
Ç
½ßÆ
Û Ý
Ë"Ì Í Â?ÃÈ"Å
Ï Â?ÃEÈ"Å
Ò
Ç
Æ Ç
½	Æ
metric, for all
, there exists æ¾Y½
such that
. Pick an arbitrary
ã Â?Ã Å
ã Â?Ã Å
ã Â?Ã Å
Â?Ã Å
Â?ÃEÈ"Å
satisfying this inequality. By definition, Ç
½	Æ
Æ , and so Ç
½ßÆ
Ç
½ßÆ
'Æ .
Ë Ì Í Â?Ã È Å
Ï Â?Ã Å
ÒÛ
Ë Ì
(by assumption), by the compatibility of
with the ordering on the
As
Ç
½ßÆ Ç
½ßÆ
ËpÌ1Í Â?ÃÈ"Å
Ï Â?Ã Å
Ò'Û
Ô
Ø
ËpÌ
Ç
½ßÆ
'Æ
reals,
, say. By the triangle inequality for ,





7

u



G

 

7
  I

  

G G g

 



  G g
  

Thus
can be
 7  g G g 7 and for any 7 an  satisfying this inequality
G


found. Choosing
shows that there exists 
such that
.
I
If instead, 
, then an identical argument can be run with the role of and (
interchanged. Thus in both cases,
8:#Ã <
G®Æ w  

   G
ËpÌ1Í Â?ÃÈpÅ
Ï Â?Ã
Ç
Æ

ËpÌ1Í Â?ÃÈ"Å
Ï Â?Ã
Ç
'Æ
Â?Ã
Ç

Å

Ô

Å

'Æ

Å

'Æ

ÒQØcË"Ì Í Â?ÃÈpÅ
Ï Â?ÃEÈ"Å
ÒÐâËpÌ1Í Â?ÃÈ"Å
Ï Â?Ã
Ç
Æ Ç
½	Æ
Ç
½ßÆ

Ò¤Û

Ø

Ö

Û

Ë"Ì1Í Â?ÃÈpÅ
Ï ?
Â Ã
Ç
½	Æ Ç

Å

½	Æ

ÒÐÛ

Ò Ô

'Æ

Ø

Ý

Ë"Ì1Í Â?ÃÈ"Å
Ï Â?Ã
Ç
Æ

¾	½

Â?ÃÈ"Å
Ç
½	Æ

½ßÆ

Å

8¾ÁÀ

ÓQËpÌ1Í Â?ÃÈ"Å
Ï Â?Ã
Ç
'Æ

õ

Å

'Æ

Å

ÿ

Ò'Û

Æ

ÒÛ

Ï

which completes the proof of the Lemma.
Ï_þ
By the nature of the ÅCý
Æ sampling process,
ÿ

   Ç¿  ;8 :#/Å <

Ã
    j8;:=/Å <
, / K \
Ã
 ¼3È
 AYÊ Ë Ê where Ê 

K \ (
Ã °Å

Ï

Åò

Æ ¾

öô

ö

Æ

 

ÓúËpÌÍ Â?ÃÈ"Å
Ï Â?Ã
Ç
'Æ

ÿ

Å

'Æ

Gh

Ò'Û

 p  G h ÉÂ  (  (65)
H  and  is permissible by the assumed
Now
permissibility of (Lemma 32, Appendix D). Hence
satisfies the conditions of Corollary 19
D for G and g D for g in Corollary
and so combining Lemma 24, Equation (65) and substituting G
Ô

À

Ã

Å ò÷öô

¾

Ó¹Ô

õ

Ó

GñQõ

Ë Ì Í Â?Ã È Å
Ï Â?Ã
Ç
Æ

Ó

Æ

ñ¾¢½

Ó

½¿¾ÁÀ

À

À

À

Å

Æ

ÒÐÛ

Ë

Å

Æ

õ

õ

19 gives the following Lemma on the sample size required to ensure (62) holds.

+X[Z U L G µ D f^ _&` T 6 @G S g Ì  G T P
   f¿  j8;:#<

Ã

Lemma 25. If

then

K L (
Ã

) 

Ï

Å

Æ ¾

Åò

öô

Ï

Æ

ö

À

õ Æ Ï

Ú

ð

Ó

ËpÌÍ Â?ÃÈ"Å
Ï ?
Â Ã
Ç
½ßÆ Ç

   '
   ÇÍ ) Â 


Å

½ßÆ

ÒÐÛ

ÏÎ oÑÐ ÇÍ )
g
D
G
G
4
Í
¿
Í
+X[Z U L G µ D ^ _&` T 6 @G S g Ì 4Í G T P

A.2.2 I NEQUALITY (63)
Ô

å

Ú ý

ð

ÿ

Ú

Å

þ!â

Ô

G D P Dg 
ã

fÍ )

 Ä
Å
Â?Ã Ä Å
Å
Å
Note that Â?Ã Å ½	Æ
½
Æ and
½	Æ
½
Æ , i.e the expectation of ½
Æ
where is distributed according to . So to bound the left-hand-side of (63) we can apply Corollary
Ô å þ
19 with ý
, replaced by ý , ½ replaced by À , and replaced by
and
respectively,
replaced by and ü replaced by . Note that À
is permissible whenever À is (Lemma 32).
Thus, if

)

Â

ý

Å

â

ð

Ú

Ú

å

Ï

À

?Æ Ï

ð

186

Ú

g D

(66)

A M ODEL OF I NDUCTIVE B IAS L EARNING

then inequality (63) is satisfied.
Now, putting together Lemma 23, Lemma 25 and Equation 66, we have proved the following
ÿ
more general version of Theorem 2.

[I G g I
YX[Z U L G µ D ^ _&` T 6 @G S g Ì Í G T P
µ D T 6 @G 0 Ì  T P
Y
[
X
Z
U
L
g
and
G ^ _&`
G
then
 ¾ ! j8;:#<
K L
GP g
Ã
7mÆ
To get Theorem 2, observe that

D
Ò
D
7
7 |R
Setting G
and maximizing G
gives
. Substituting G
¿ pÂ

Ï_þ
Æ -sample genTheorem 26. Let À be a permissible hypothesis space family and let be an ÅCý
n
Ï
Ý
Ï
æ
å
ß
Ú

Û
Ý
erated from the environment Å
Æ . For all
and
, if

Ô

Å ò÷ö	ô

Â?Ã Ä Å

Å ØÚ

ð

Ï

Ú

õ Æ Ï

À

Ï

Ú

ËpÌ1Í Â?ÃÈ"Å
Ï Â?ÃÄ1Å
Ò'Û
Ç
½ßÆ
½ßÆ

Û

½ßÆ

Æ

å

ð

Ó

Æ

Æ Ï

À

Ú ý

ð

¾

Ï

Ú

Å

þ!â

ÿ

å

Ú

ð

Ã

Ú

Å

â

ý

ð

Ë Ì Í Â?Ã È Å
Ï Â?Ã Ä Å
Ò1Û
Ç
½	Æ
½	Æ
Ô

Ø
Â?Ã È Å
Ç
½ßÆ
ÚÔ

Ú

ã

and

Ú

7  ED D
Ô

Å {Ø5Ú

Æ

.
into

Theorem 26 gives Theorem 2.

 7
G 

A.3 The Realizable Case

 7

Ó
7
S

G G
G Æ
G
G 0 G 7
G
JI G g I , if
and
Corollary 27. Under the same conditions as Theorem 26, for all 7
X[Z U L G µ D G 7 ^ _&` T 6 G g 7  Ì ÔÍ G T G 7 P
T 6 G 7  Ì  T P
µD

[
X
Z
U
L
g
and
G G 7 ^ _&`
G G 7
then
G
 ¾ ! N8;:#<
K L
7P g
G
Ã
D
These bounds are particularly useful if we know that
, for then we can set G
G ).
(which maximizes G
þ

å

7

In Theorem 2 the sample complexity for both
and ý scales as
ð . This can be improved to
å
ã Â?ÃÈ"Å
Ø
ã
Ø
Â?ÃÄÅ
Â?ÃEÈpÅ
½	Æ
Ç
½	Æ
Ç
½ßÆ
if instead of requiring
, we require only that Â?ÃÄ Å ½	Æ
¼Û å
Û

å
Ø

å
Ö
Ø
Ú
å
Ö
Â?Ã È Å
Å
Å
Å
for some
. To see this, observe that Â?Ã Ä Å ß
½ Æ
Ç
½	Æ
Æ
Æ
Æ
Ë"Ì1Í Â?ÃÈ"Å
Ï Â?Ã}Ä1Å
ÒÛ
Ú Å åºÖ
Ô
Ç
½	Æ
½	Æ
Æ
, so setting
in Theorem 26 and treating as a constant
gives:

Ó

ÛÝ

Å_Å åÖ

â

ý

þ

$Å å Ö

â

Æ

$Å å Ö

Ã

Å ò÷ö	ô

¾

Å_Å åÖ

Æ

ÿ

Â?ÃÄ1Å

Ï

À

Å åÖ

õ Æ Ï

Ï

Æ

ã

Ô



 Õ Ö  3 Õ zÖ  Õ   Õ  b× and  ÇØÇ 
 as a composition of two function classes note that if for each 
 Ù  by
 Ú            

To write À õ
rÓ ò
ö	ô

õ

of the form given in (32), À õ can be written

Ô

rÓ

Ï

Ï

¾

¬õ

¾

Æ

ºÓúò

ó

Å

å

Æ

Recalling Definition 6, for À
À

æå

Æ

$Å åÖ

Ø
Â?ÃÈpÅ
Ç
½	Æ

å Ö

Appendix B. Proof of Theorem 6

Å

äÆ Ï

À

Ï

ÔÜÝ
Â?ÃÈpÅ
Ç
½	Æ

Å å Ö

Ú

å

å$Ø

â

½	Æ

Ï

Æ

ý

Ó

Æ

å

Æ

Ý

ö	ô

Æ

ÅÎ

Ï_Ñ

Ï

Ï_Î

Ï_Ñ

Æ

Ó¹Ô

Å

187

ÅÎ

Æ

Ï_Ñ

Ï

Ï

ÅÎ

Æ

Ï_Ñ

Æ

ó

Ù

we define

BAXTER

Õ Ö  bt Õ iÖ  Õ  Õ zÖ  Ú . Thus, setting ×  × b × and Ø   Ú 0
ØÇ
 ×ÖØ 
(67)
6 .
The following two Lemmas will enable us to bound 57
Å
Û Ù
Ü
Ö
×
Ø
Lemma 28. Let
be of the form
where
. For all 7  7
,
6 57  7
6 Ü Å 57  Ø 6 57 × 
Åß
=
Þ
)
7
o
Å


Proof. Fix a measure
on
and let Ý be a minimum size -cover for Ø
. By
Ü
6

$
)
à
Ù
$
)
à
5
7

á

definition
each bÝ let
be the measure on
defined by

)   á Ý for anyÜ set á Ø in. For
Ù


á is6 measurable).
the ~ -algebra on
( is measurable so
à
à
7
F
o
ã
57 × . Let
Å
Let â
for ×
. By definition again, â
6
6
å 9Ý andsizeÕ -cover
àâ  . Note
ä
ä ÕbeÖ a minimum
57  Ø 57 × so the Lemma
that
be
Ü
 will
ä
7
7
o
Å
ß

can be shown to be an
-cover for
. So, given any Õ Ö
choose
proved if
 1 HÝ such that Þ o  Ü   1 7  and Õ¾1 Àâ àVæ such that o ã æ Õ Õ¾1 7 . Now,
o Õ Ö  Õ 1 Ö  1 o Õ Åçß Ö  Õ Ö  1 o Õ Ö  1 Õ 1 Ö  1
Þ o  Ü   1 o ã æ Õ Õ 1
7 7 
Å ß   line followsä from
where the first line follows from the triangle inequality for o and the second
Þ




o
F
o
ã
o
o
Ö
Ö
Ö
Ö

the facts:
1
Õ Õ¾1 and Õ Õ 1
Ü 1 . Thus is an
7  7 -cover forÕ 1 Õ1 o and
so the result follows.
 (Definition 6), we have the following Lemma.
Recalling the definition of {
Lemma 29.
6 57 M3  è  6 57 


 
 . Let ä   ä  be
)   )  on
Proof. Fix a product probability measure
(
/
7 -covers of  oé   o . andä let ä ä  % ä  . Given  % 
{&  , choose Õ 93 Õ  such that o&½ 
 Õ 
 7 for each   . Now,





/
.
,
   Õ 93 Õ 
 
'  
 
 
'  Õ 
 
  (   

 

'  o&½ 
 Õ 

7
ä
 and as ä -ê 
'  ä 
 the result follows.
Thus is an 7 -cover for 93
ßÔ

then
,

À

Ô

õ

Ï

Í ÝjÏäå}Ò

Óïò÷ö	ô÷ó

Í ÝjÏäå}Ò

Ï

ò

Å

Å

í

Õ

Õ

Ø

Å

ã

Ï

Å

Ï

ð

ö	ô

ºÓ

¼¾

¸¾

Å ¬Ï

Æ

Ë 1Å

Ï

Å

LÆ

ö	ô

ÏË

ïõ

Ö¶ó

Õ

¸¾

ã

Õ

Å

ð

ãË

Å ¬Ï

ã

ØcË

Æ

Ø

Ï

Å
Ï Ë

½

'Ï

Å

ãË

¾

ØYË

Æ

Ï

Å

Ï

Å

mÆ

Æ

Ë

ð

Å

Õ

ÖOó

Æ

Ï

Å

ð

¬õCÆ

Å

Æ

Ô

ïõ²Æ

¾¼½

ð

Ï

Ï

Æ

ã

Õ

ã

CÆ

Å

EÅ

ö	ô

ÏË

Å

í

Æ

Ø

ã

ºÓ

¬õ:Æ

ð

ö×ô

ð

Æ

ò

¬õ

ö	ô

Å

ÐÏ

Ó¹Ô

ïõ

õ Æ

Ô

Ï

Å

¾

mÆ

¾

Ë

ã

½ßÆ

Æ_Æ

Ó¹Ô

À

½

ÛÝ

ð

¬õ

õ

Å

½

Ó¹Ô

õ

Æ

Æ

ð

Ë

Ë 1Å

Ø

Ï

Å

ð

Æ

ÏË

½

Ô÷Ë

Ï

jÅ

Ë 1Å

:Æ

Ï

½

½

ã

Æ

Ô

½

½

Ï Å

Æ

½

ÏË

Å

ñ

ö

½

Æ

Æ

ö

Æ

Ë

¾

ñ

Ï

Å

Å ò÷ö	ô

Ô

½

Ë

Å ¬Ï

½

Å

Å

ã!Ë

Æ

Æ

½

ÏË

'Ï

Ï

Æ

Å

å

Ô

å

ã

Å

ñ

ý

Ë

ý

EÅ

ñ

Ï

ñ

Ï

Æ

Æ

Å

ã

½

½

Õ

188

Õ

Ô

Õ

Ô

ñ

ã

Ö

Æ

Ï

Æ

Õ

Æ

ñ

Ë

Ô

Å

å>Ï

Ï

Ï

ñ

Ï

Ï ý

Æ

¾

A M ODEL OF I NDUCTIVE B IAS L EARNING

B.1 Bounding

6 @7 Çëì
Ï

Å

À

jÆ

6 a 7  7 ×  Ö Ø e 6 @7  ×  6 Ü Å / a 7 Ø e
and from Lemma 29,
6 @7  ×  6 57  ×  
Å / 57
6
Ø
Using similar techniques to those used to prove Lemmas 28 and 29, Ü
satisfy
6 Ü Å / 57 Ø 6 Ü Å 7 Ø 
From Lemma 28,

Ø

Ï

ã

¬õ

ð

Ï

Å

¬õ

Ï

Å

ã

Æ

ïõ
Ï

Å

Ï

Æ

¬õ:Æ

Ï

Å

Ï

Å

ã

Æ

ð

(68)

ð

Ï

Å

(69)
¢Æ

can be shown to

mÆ

ð

(70)

Equations (67), (68), (69) and (70) together imply inequality (34).

íîïSð Ç6 ñò
Å
6
@7 Ø when is a hypothesis space
We wish to prove that 57 ÔÍ
Ü

x

 family of the form
Ö
|×
fØ4 . Note that each fÍ ÔÍ corresponds
to some × Ö , and that
Í ) ó ¸'¼¹#Ü º Å o Õ Ö  
Â ¿
Âiôõö on , defined by
Any probability measure on induces a probability measure
Âzôzõö á -, È ) á ÉÂ )
for any á in the ~ -algebra on
. Note also that if
1 are bounded, positive functions on an
Á
arbitrary set , then
 ø ¼3ù © ø ¼3ù 1 ©  +8ø :#¼3ù<  © 1 ©  
(71)




÷
¸
#
¹
º
÷
¸
#
¹
º


Â
¿
. Let fÍ ÇÍ
Let be any probability measure on the space of probability measures on


Ö

Ö
× . Then,
be two elements of Í with corresponding hypothesis spaces ×
Å
Å
Í Í -, È  ó ¸'¼¹=Ü º o Õ Ö   ó ¸'¼¹#Ü º o Õ Ö   ÉÂ )
, È 8; ¼:#< Å o Õ Ö   o Õ Ö  ÉÂ  ) (by (71) above)
óÜ

¾) Â )
, È , ôõö 8¼:#< Å Õ Ö  
Ö
Õ
ó
ÞÅ ûúýüþ  Ü Åçß  Ü   
8;:#< Ü Õ Ö  is guaranteed
Å ß  by the permissibility of (Lemma 32 part 4, ApThe measurability of
Þ
M
ú
|
ü
þ
 Ü  we have, Å ß
pendix D). From
ÇÍ fÍ
(72)
> 57 Í
> a 7 Ø =Þ úMü|þ  Ü e
^À

B.2 Bounding
À

Ô

¬õ

Ï

Å

Ó

À

ã

äÆ

Ï

Å

¾

mÆ
º¾/À
Ô
Å
Æ

½

½

À

¬õ

Â?Ã 1Å

Æ

ò

Å

ò

Ô

Æ

Å

ößô

Å

ñ

ñ

Ö

jÆ

Å

ñ

ã

aÆ

Ë

Æ

Å

Ï

ößô

Æ

ñ

Å

ñ

Ö

aÆ

Å

ñ

aÆ

ò

À

Ë Ä1Å

½

Ï

½

Ï

¬õ

ð

Ô

Æ

Â?Ã 1Å

ã

Õ

Â?Ã 1Å

Æ

ã

Ë Ä1Å

½

Ï

½

Å

ð

Ï

ð

Å

À

Â?Ã 1Å

ð

Æ Õ

Ö

Æ

ð

Ë

Æ

Ë

Å ÎÏ_Ñ

Å

Å

Æ

Æ

Ë

Æ Õ

Å ÎÏ_Ñ

Æ

À

Ï

Å

ÏË Ä

Ö

ð

ã

ð

Æ

which gives inequality (35).
189

Ï

ÏË Ä

½

ð

Æ

ãË Ä
Æ
Ï

Â?Ã 1Å

Å ÎÐÏ_Ñ

Õ

ÔË Ä

Ö

Æ

¬õ

ö	ô

Ï

Æ

Ë

Å

Æ

Ï

½

ð

BAXTER

B.3 Proof of Theorem 7
In order to prove the bounds in Theorem 7 we have to apply Theorem 6 to the neural network
hypothesis space family of equation (39). In this case the structure is

Û ¨ Ü
ý
ÿ
 ¨ G 
 
 G  @G G   G ¨
0
¨






 ¨   ~  
' 
³ for some bounded¨
where ×
subset  of
and some Lipschitz squashing function ~ . The feature class Ø
ÿ
is the set of all one hidden layer neural networks with inputs, hidden nodes, 	 outputs, ~ as
the squashing function and weights 
  where  is a bounded subset of  . The Lipschitz
restriction on ~ and the bounded restrictions on the weights ensure that Ø and × are Lipschitz
ªb
I 

for all Ø and
classes. Hence there exists
1 ÿ ,      1  I
ª  1  and for all Õ W× and 1 such¨ that
Y
I
ª
Õ 1  1  where   is the  norm
, Õ
in each case. The loss function is squared loss.
, hence for all Õ Õ1 × and all probability measures
Õ
) onNow,¨ Õ (recallÕ that we assumed
the output space was
),
o Õ Õ 1 -,  õ Þ  l ß  Õ 
Õ 1 
 ) 
YDý,    Õ  Õ 1   ¾)   
(73)
 ¨ 
) 
)
  1 -Ø and
where
is the marginal distribution on
derived from . Similarly, for all
)
probability measures on ÿ
,
Å
ß
Þ o  Ü   1 YD&ªå,     1  )   
(74)
Define
6 a 7 ×   e 98;:#o < > a 7 ×   ) e
¨
where the  supremum is over all probability measures on (the Borel subsets of)
, and
a
)
)
7
7
e
×

×

is
the
size
of
the
smallest
-cover
of
under
the
metric.
Similarly
set,
>
6 a 7 Ø   e 98;:#o < > a 7 Ø   ) e
where now the supremum is over all probability measures on ÿ . Equations (73) and (74) imply
6 57 × 6  7D ×   
(75)
Å6 @7 6  7  
D&ª Ø 
(76)
Ü Ø
Applying Theorem 11 from Haussler (1992), we find
¨
D



ª

7

6 D ×    7!
6  D&7 ª Ø     D 7 ª " 
Öwó

É

Ô

ÅÎ

É

Ï

Ï_Î

ó

Æ

Ö óûÍ ÝjÏäå}Ò

É

Î

Ø

Ó Å

Ï

Ê

Ï

Ï

Ë

Æ	¾

Ó

ù

¾

ÎmÖ/Î

ÎÏ_Î

m¾

É

ö

õ

Î Ï_Ñ
Ôæù Å
ÅÐ
Æ
Í ÝjÏäå}Ò
Ë

Å

ÅÎ

>õ

Ï_Ñ

Ï

Æ

õÆ

Ô

Æ

Ö

É

Ë

Å

Ï

Å

Æ

Æ

ÎÏ_Î

¾

ÅÎ

Æ Õ

Ï

Ö

Æ

Æ

Ö×Ñ
Å

Ö

Æ ð

Å

Å

Ë

Æ

Ö×Ñ

Ë

Æ ð

ÅÎ

Ï

Ö

Æ

ÅÎ

Ó¹Ô

Ï

Ï_Ñ

Ï

Ó¹Ô

Ë

Æ

Ï

Ï

ÅÎ

Æ

Æ

¬Ï

ã

Ö

Ï

Æ

Å

ÅÎ

Ï

Å

¾

Æ

Ï

Æ

Å

Ï

Å

É

Æ

Æ

Ð¾
Í ÝjÏäå}Ò

Æ

Å

ÅÎ

¾É

ÎmÖ/Î

ô

Å

Í ÝjÏäå}Ò

Å ¬Ï

Ö

Æ ð

Å

Ï

Ï

Ñ

Ô

ö

ÅÎ

Õ

Æ

ã

É

É

¾	É

ÅÎ

Å

ó

É

Æ

Ï

É

Æ

É

Ï

Å

Ï

Å

Ï

mÆ

Ï

ïõ

Ï

ã

Ï

¬õ²Æ

Ï

ã

Ï

Ï

Ï

ð Ê¸ð

ã

ã

ð

ð

Substituting these two expressions into (75) and (76) and applying Theorem 6 yields Theorem
7.
190

A M ODEL OF I NDUCTIVE B IAS L EARNING

Appendix C. Proof of Theorem 14
This proof follows a similar argument to the one presented in Anthony and Bartlett (1999) for
ordinary Boolean function learning.
First we need a technical Lemma.

D  D D  D  , with

G for all
#    # 
   D D D D K # 


   
 
K #   #  0 #    #  %$ G  'R & '( ) +/é *-. , n 1n 0 
,
ä # denote the number of occurences of in the random sequence # #   #  .
Proof. Let


The function can be viewed as a decision rule, i.e. based on the observations # , tries to guess



D


D



D


D
whether the probability of
the Bayes

Dis  D if ä # or WD , and. The
 #  optimal
D rule isD otherwise.
 #  decision
estimator: #   # 
Hence,
K  # 2$ G D K  ä # D  G D  D 


D K ä # I D  G D  Dý
KD  ä # D  G  D  D 
 D



D


which is half the probability that a binomial
 random variable is at least WD . By
Slud’s inequality (Slud, 1977),
K  # 3$ G D K ­

 ¯
G

ÇI  I

å

ÙØ

Ïäå

Lemma 30. Let be a random variable uniformly distributed on
Ý
å
Ï
Ï
å>ÏäÖÙå
. Let
be i.i.d.
-valued random variables with ÃGÅ
å>ÏäÖ{å
ó
å ºØ
Ïäå Ö
. For any function mapping
,
Ï

Ã

Å

Ï

Ó

Ï

Å

Ï

Ô

Æ

åÖ

å1Ö

Ø&å

Ï

Ï

Ô

Æ

å

å

ÃÅ

Å

ºØ

å

Ø

Å

Ô

^Æ

å

â

Æ

Å

å

å

Û

Å

"Æ

Ô

å

Û

Æ

Ï

Æ

þ

å

ü

å

Ô

Ô

Æ

å

$Ö

Ö

Ô

&Ö

Ã

å

Ô

"Æ

â

^Æ

Ï

þ

Å

Å þ×Ïäå

ÃÐÅ

þ

â

^Æ

Å

Ï

Å

Ã

Ã

Ï

Å

ÙÖ

Ã

Ø

Ô

Æ

Ô

âèþ

"Æ

å

í

Ø&å

"Æ

Å

å

Û

nÖ

Ô

Ø

Ö

þ

Æ

þ

â

ð

åÖ

ð

ÝjÏäå
Î®âÝ
Æ . Tate’s inequality (Tate, 1953) states that for all
where ü is normal Å
,

K

ÃÅ

âèÎ
ü

'4

D²
å

â
Æ

å Ö

65 n ´

)

å1Ö



í

 ¾ ! be shattered by , with Ã . For each row  in 7 let ¿ 
 be the set of
Let 7
D
)
) «)
all ÿ distributions on
such that
if is not contained in the
9

8


)

D
Ã
Ã and ) 










 th;: row  ofD 7 ,Ã and for each¿ ¿   f¿  ,
8

. Let
.
)
;
)
¿
<<  is achieved by any sequence







Note that for (
, the optimal error _ 
 Í Í  Í such that 
Í 
 if and only if ) 
 

  D Ã , and
always contains such a sequence because shatters 7 . The optimal error is then

<_ 6<     Í   ) 
  
Í 3$    ÿ =   D Ã  D 

' 

'  u 
Combining the last two inequalities completes the proof.
¾

Å å

Ô

Æ
Å

ñ

ò

ò+ö
ÁÔ

Å >Ë

ÅCý

Ï

Ï

Æ_Æ

ñ

å>Ï

3Ó¹Ô

Ô

Å

ÏË

ö

Ï

Æ

Ë
ÅCý
Æ
ÎÅ ÐÏäå Ô
Æ
Ïäå
Ô
ÅÎ
Æ

Æ1¾
ÔÞå
Æ

ÅÎ

ñ

ÅCý

ö

Ï

Æ

þ÷Ô

À

å

Î ÏÝ
Ô Ý
3
ÅÐ
Æ
Å å
Å >Ë
'Æ

ÅÎ

Å
À
Ïäå

Æ

Ô

Æ

Î

ÅCý

Å å Ø

ÅÎ

Æ_Æ

Å >Ë

Æ

ÅCý

ÏäÖÙå

Æ_Æ

À

À

Å

À

Æ

Ô

Â?Ã

Å

Æ

Ô

ý

å

Gñ

ÅÎ

Æ

Ô5Ñ

191

Ô

ý

å

å1Ö

>Ë

ÅCý

Æ

Ô

å1Ö

Ô

Æ

Ï

BAXTER

and for any



Ô
Å

    ,
p  _ 6< <  
Ï

ñ

Ï

ñ

Æ ¾ÁÀ

(77)
Ã     
 
 3$ 
Í 
  
For any
-sample , let each element 
 in the array
   ÿ?=   
..
..
>
..
.
.
	  ÿ.=   
equal the number of occurrences of 
 in .
)   ;)  uniformly at random from ¿ , and generate an Now, if we select (
A
@  (the output of the learning algorithm) we have:
sample using ( , then for 
Î    
 
 2$ 
Í 
l  C B ) /> Î    
 
 2$ 
Í 
l  >
 B ) />   ÿ  =    ) 
 2$ Í 
 
l

'  u 
) is the probability of generating a configuration > of the 
 under the -sampling
where />
process and the sum is over all possible configurations. From Lemma 30,
) 
l 2$ Í 
 
 ER DF 'G ) * é/.½ H , nn JI
,
Â?Ã

ÅCý Ï_þ

Å

Ô

'ÿ Æ

Å

À

Ø

Æ

ý Ë

Ô

Å Ï

Æ

Ó

ÅÎ

ñ

ý Ë

Ã

Æ

Ô

ÿ

ÅÎ

ñ

þ

ÅCý Ï_þ

Æ

Å

Ô

ÿ þ

Ï

Õ

ÅÎ

ñ

Ô

Å

Ô

Å

ÕÆ

Æ

Ô

Æ

ÅÎ

ñ

Ó

Æ

ÅÎ

ñ

Ô

Æ

ñ

ÅÎ

þ

Æ Õ

   
 
 3$ 
Í 
  
Å Ï

K  Ã
Ã

ý Ë

ÅCý

Å

Æ

Å Ï

Õ

Ó

Æ

ÅÎ

ñ

Å

Æ

Ô

Æ

ÅÎ

ñ

where

Æ

Å Ï

Ó

Æ

ñ

å

Ó¹Ô

Í ÝjÏäå}Ò

Í ÝjÏäå}Ò

å1Ö

åÖ

B )
Ã
å

Û
Õ

Æ

å

Û

Æ

Æ

ÅÎ

ñ

Ô

ý Ë

ÅCý

R'&
å

/>
Å

Æ

(

å Ö

) í

å1Ö



Æ

Æ Õ

å Ö

Æ

Ô

ÅÎ

ñ

å Ö

Õ

Æ

Ã

Å

Ï
Æ

Ó Â?Ã

 @
Å

Å

Æ_Æ

Û

  ÿ? =    R

'  p 
M/ L é/n . L
n
å

K

Û¼Î

Û

Û

À

Æ

Ø

Æ

Æ

Æ

YÎ
â

½H
) í * é/. , n n IJ
,

åÖ

ü

(78)
ÖÎ

, (78)

· G

Å å Ö

 Æ

í

Å

192

DF

G

å1Ö

=K *-K, , 0

_ <6<   · G M

þ

Ï

(79)

ÿ
. Plugging thisÿ into (77) shows
that

K (

Æ

ÅCý Ï_þ

-valued random variable ü , GÃ Å ü
ÅÎ

ÕaÕ

Æ

ÅÎ

ñ

í

   
 
l 3$ 
Í 
   · G 
 O/ L é/n . L
(
)

G NR &
= K *-K, , n 0
Õ

Æ

Æ

by Jensen’s inequality. Since for any
implies:
å

·

Õ

Æ

Î

å
ÅCý

ÅÎ

ñ

þ

â

and Á¾

Ô

Æ

Æ

hence



Ï

Å

Æ

Å

Î

ÅÎ

ñ

Ó¹Ô

Æ

Î

ÿ

Å

Æ

Ó

Æ

þ

5Å

Õ

Å Ï

Õ

þ

Æ

ÿ

Å

ÅCý

Û

Å å Ö

· G
 Æ

A M ODEL OF I NDUCTIVE B IAS L EARNING

(

(



@

(

Since the inequality holds over the random choice of , it must also hold for some specific choice
ÿ
algorithmÿ
there is some sequence of distributions such that
of . Hence for any learning

K 

Ó Â?Ã

Ã

 @
Å

Å

Setting

Assuming equality in (80), we get

Ï

Å

Å

Ø

Æ

·G 7
â

Û

· G

Å å Ö

eÆ

Ï

(80)

_ <6<   7  g 
Û

Å

Æ_Æ

g

À

and

ÿ

 @

Ó Â?Ã

Ã

â

eÆ

ÿ

K 

Å

· G g

Å å Ö

ensures

_ <<   · G M

Û

Æ_Æ

Ø

À

Û

Æ

(81)

7g · 

·
·
Solving (79) for , and substituting the above expressions for G and  shows that (81) is satisfied
provided
Ã &  g7   · ·  0 ^ _&` T g · · D g
(82)
g
«R (© ¡R since G I |R and G g  · ), and assuming
Setting ·
g7  	É© for © somefor	 some
YD , © (82) becomes
Ã  D T © D 
(83)
©
	 ^'_&` ©
ÒR the right hand side of (83) is approximately maximized at ©
Subject to the constraint ©
T QPSR Ì&Ì , at which point the value, exceeds
CDF 	  D&D 7 . Thus, for all 	 , if 7 g
Ã
 R 	 and
Ã D&D a ¨ e
(84)
7
G

Ô

åÖ

Ï

å Ö

Ô

þ

þ!ãË

Ô

Ï

ãÜå

Å

ÅCý

ð

Æ

å Ö

å1Ö

Û

Å å Ö
Å åÖ

ð Öå

Û

Þå

eÆ ð
Ö

Æ

Ô

Å åÙÖ

Û

jÆ

þ!ã

Ë

ÅCý

å1Ö

Æ

Ö

Å

ð

 Æ

ð

Æ

Û

Ô

Ë

ÅCý

å

ÿ

Æ

Ë

þ!ã

Å å°Ö

ÅCý

Æ

Å

QÆ

å Ö

>Ý

>Ý

ð

â

ð^Æ

å

Ï

ã

Ï

ð

ÿ

<6<   7  g 
_
 contains at least two
g
To obtain the -dependence in Theorem 14 observe that by assumption
)UT be two distributions
functions 
, hence there exists an
such that  3$
. Let
)


D
)T
T
7

8
concentrated on
and
such that
and
;: 7 D . Let ( 9)
T  b) and ( -)  H) be the product distributions on

V
)
98   generated by ,T and       @   . Note that   and 
are both in
. If ( is one of (
and the learning algorithm
chooses the wrong hypothesis  ,
then
  _ 6< <   7 
K 

then

Ã

Ï

ñ

Å å
Åò

ö

Æ

å

ñ

Ó Â?Ã

 @
Å

ð

Î

Å ÎÐÏäå
Æ
Ó¹Ô
Ê

Ê

Å ÎÐÏäÖ{å
Æ
ö
ö

Ó¹Ô

Æ
À

Â?Ã

Å

Û

Æ_Æ

Å

ò

¾

Ê

Å

'Æ

Å

ñ

í

Ó¹Ô

Ï

À

Æ

Ø

Û

Ô
ÅÎ
ÅÎ
ñ
Æ
ñ
Æ
ÎÅ Ï
ÎÅ
Ô ðÅ å
ñ
Æ_Æ
ö
ö
í
í
Ï
Ï
Ó¹Ô Å
Ï
ñ Æ
ñ
ð
ð

Ö

Å

193

À

Æ

Ô

À

Å ÎÏ

Æ

Ï

ñ

ð

Æ

ñ

ð

ÅÎ

Æ_Æ

Ô

ð

BAXTER

(

(

( (  and generate an
/ n
_ <<   7  NR & ( ) é/*X. W W n 0
Ï

Now, if we choose uniformly at random from ÙÊ
cording to , Lemma 30 shows that

K (
g
which is at least if
Ã

Å

ÿ

Ï
Æ

Ó Â?Ã

ÿ

 @
Å

Å

â

Æ_Æ

þ

Å

À

Ø

Æ

Î

åÖ

å

ð

â

ÅÎ

ð

ØcÎ

ð

ð

å1Ö

åÖ

Æ

-sample

ac-

Ï

å

Å årÖ

ý

æå

Ï_Î

å

Û

JI g I  |R . Combining the two constraints on
X[Z U     finishes the proof.
Ý

ÅCý Ï_þ

í

I 7 7 f^ _&` T g D g
ð

provided

ÿ

þ

(85)
Æ

: (84) (with

Æ

	

P ) and (85), and using
Ô

Appendix D. Measurability
In order for Theorems 2 and 18 to hold in full generality we had to impose a constraint called
“permissibility” on the hypothesis space family À . Permissibility was introduced by Pollard (1984)
for ordinary hypothesis classes ½ . His definition is very similar to Dudley’s “image admissible
Suslin” (Dudley, 1984). We will be extending this definition to cover hypothesis space families.
Throughout this section we assume all functions ñ map from (the complete separable metric
Í ÝjÏäå}Ò
. Let Å °Æ denote the Borel -algebra of any topological space . As in Section
space) ü into
2.2, we view , the set of all probability measures on ü , as a topological space by equipping it
with the topology of weak convergence. Å Æ is then the -algebra generated by this topology. The
following two definitions are taken (with minor modifications) from Pollard (1984).

¿



Y 

~

¿

Y

Í ÝjÏäå}Ò



~

-valued functions on ü is indexed by the set
Definition 8. A set ½ of
rÓ
ö
óûÍ ÝjÏäå}Ò
such that
ü



Definition 9. The set ½
1.





 

Ô

½

Ï_ç

Å

Z

ÓQç

Æ

¾



is permissible if it can be indexed by a set







if there exists a function

such that

is an analytic subset of a Polish7 space , and
ºÓ

~

ü
2. the function
-algebra Å ü°Æ

Y

Å

ö

ó

Í ÝjÏäå}Ò

indexing ½

by

\[]Y  Æ .
An analytic subset  of a Polish space 
ò



is measurable with respect to the product
ò

is simply the continuous image of a Borel subset
of another Polish space . The analytic subsets of a Polish space include the Borel sets. They
are important because projections of analytic sets are analytic, and can be measured in a complete
measure space whereas projections of Borel sets are not necessarily Borel, and hence cannot be
measured with a Borel measure. For more details see Dudley (1989), section 13.2.
Lemma 31. ½

2| 
½

Ó Å ò÷öô
Æ



ó

Í ÝjÏäå}Ò

is permissible if ½

  
Ï

Ï

½

are all permissible.

Proof. Omitted.
We now define permissibility of hypothesis space families.
7. A topological space is called Polish if it is metrizable such that it is a complete separable metric space.

194

A M ODEL OF I NDUCTIVE B IAS L EARNING

 W

Ô

á



Definition 10. A hypothesis space family À
is permissible if there exist sets
ä½
ºÓ
ö
ö
ü
are analytic subsets of Polish spaces and respectively, and a function
Å
Å
measurable with respect to
°Æ
Æ , such that

^_[`Y  \[`Y
Ôba  Å
À


fe hg



á 

Ï_ç}ÏEá

Óïç
Æ



Z
¾

Óïá
¾



ádc 

á

á and 

that
,

ó

Í ÝjÏäå}Ò

@

òÏ ºÏ
ò
Let Å
be an analytic subset of a Polish space. Let Å Æ
Æ be a measure space and
ò
denote the analytic subsets of . The following three facts about analytic sets are taken from
Pollard (1984), appendix C.

fe hg

ò®Ï ºÏ
(a) If Å

@
(b)

Åò

ö

Æ

°Æ

is complete then

@

Åò

e

A
Æ

.

e ]
[ Y Å ° Æ .
 Æ , the projection i ô ô of ô onto ò

~

contains the product -algebra

(c) For any set

ô

in

@

Å ò÷ö

4Í

@

is in

)

Åò
Æ

.

¿ Y ¿

Ï

Y

Å
Recall Definition 2 for the definition of À . In the following Lemma we assume that Å ü
ü°Æ_Æ
Å nÏ Å
Æ_Æ is complete
has been completed with respect to any probability measure , and also that
with respect to the environmental measure .

Â



Lemma 32. For any permissible hypothesis space family À ,
1. À õ is permissible.


f is permissible.
3.
is permissible for all
.

8
#
:
<
O and ¸'¹#º O are measurable for all .
4.
5. Í is measurable for all
.
6. ÔÍ is permissible.
 is simply the set of all
Proof. As we have absorbed the loss function into the hypotheses ,
" such that
-fold products
. Thus (1) follows from Lemma 31. (2) and (3)
2. Gñ®¾ß½

Ó

½Þ¾×À

½

½¿¾/À

½¿¾/À

½

½Þ¾/À

À

ñ

ý

½

½

½

¾

À

õ

À

are immediate from the definitions. As ½ is permissible for all ½ ¾À , (4) can be proved by an
identical argument to that used in the “Measurable Suprema” section of Pollard (1984), appendix
C.
Ó
ó
Í ÝjÏäå}Ò
Ó
ó
Í ÝjÏäå}Ò
, the function ñ
defined
For (5), note that for any Borel-measurable ñ ü
¹
Ó
Ô
Ë
Å
Å
ñ
Æ
Æ is Borel measurable Kechris (1995, chapter 17). Now, permissibility of
by ñ Å Æ
Ó¹Ô
Ó
Ô
, and ½
so ½
is measurable
½ automatically implies permissibility of ½
ñ
ñ/¾	½
by (4).
rÓ
ö
ö
ó
Í ÝjÏäå}Ò
in the appropriate way. To prove (6),
Now let À be indexed by
ü
Ó Þö
ö
ó
Í ÝjÏäå}Ò
1
_
Ï
}
ç
E
Ï
á
¹
Ó
Ô
Å Ï_çÏEá Ë
Å
Æ
Æ
Æ . By Fubini’s theorem is a
define
by Å
Ó
ö
ó
Í ÝjÏäå}Ò
ÏEá
Ó¹Ô
Å
Å
Å
Æ
°Æ
Æ -measurable function. Let
be defined by Å
Æ
Å 1Ï_ç}ÏEá
indexes À
in the appropriate way for À
to be permissible, provided it can
Æ .
Å
Æ -measurable. This is where analyticity becomes important. Let
be shown that is Å Æ
Ó¹Ô
Û
ö
ö
Å 1Ï_ç}ÏEá Ó Å Ï_çÏEá
. By property (b) of analytic sets, Å
.
Æ
Æ
Æ contains
Ó¹Ô
1
E
Ï
á
Ó
1
E
Ï
á
Û
ö
Å
Å
Æ
Æ
The set
is the projection of
onto
, which by property (c) is
nÏ Å
Ï
also analytic. As Å
Æ
Æ is assumed complete,
is measurable, by property (a). Thus is
a measurable function and the permissibility of À
follows.

)

kj .

0¿

)


û
¿
¿Y ;[mÕ Y  ;[ Y áá
qp Õ )
¼
¸'¹#ºon ) â â Y ) ¿ r[Y ÔÍ á
G
Õ c  c )Õ
)
â  ¿ ¿ â pÂ
Y

Õ)

G





W

á lj . 
)
û
¿
á
â
ÔÍ
4Í

195

âc

Õc

fÍ ¸'¹#º O fÍ
@

¿ á

¿



á

Õ
â )

â

Õc

BAXTER

References
Abu-Mostafa, Y. (1993). A method for learning from hints. In Hanson, S. J., Cowan, J. D., & Giles,
C. L. (Eds.), Advances in Neural Information Processing Systems 5, pp. 73–80 San Mateo,
CA. Morgan Kaufmann.
Anthony, M., & Bartlett, P. L. (1999). Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, UK.
Bartlett, P. L. (1993). Lower bounds on the VC-dimension of multi-layer threshold networks. In
Proccedings of the Sixth ACM Conference on Computational Learning Theory, pp. 44–150
New York. ACM Press. Summary appeared in Neural Computation, 5, no. 3.
Bartlett, P. L. (1998). The sample complexity of pattern classification with neural networks: the
size of the weights is more important than the size of the network. IEEE Transactions on
Information Theory, 44(2), 525–536.
Baxter, J. (1995a). Learning Internal Representations. Ph.D. thesis, Department of Mathematics and Statistics, The Flinders University of South Australia. Copy available from
http://wwwsyseng.anu.edu.au/ jon/papers/thesis.ps.gz.

s

Baxter, J. (1995b). Learning internal representations. In Proceedings of the Eighth International
Conference on Computational Learning Theory, pp. 311–320. ACM Press. Copy available
from http://wwwsyseng.anu.edu.au/ jon/papers/colt95.ps.gz.

s

Baxter, J. (1997a). A Bayesian/information theoretic model of learning to learn via multiple task
sampling. Machine Learning, 28, 7–40.
Baxter, J. (1997b). The canonical distortion measure for vector quantization and function approximation. In Proceedings of the Fourteenth International Conference on Machine Learning,
pp. 39–47. Morgan Kaufmann.
Baxter, J., & Bartlett, P. L. (1998). The canonical distortion measure in feature space and 1-NN
classification. In Advances in Neural Information Processing Systems 10, pp. 245–251. MIT
Press.
Berger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis. Springer-Verlag, New
York.
Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1989). Learnability and the vapnikchervonenkis dimension. Journal of the ACM, 36, 929–965.
Caruana, R. (1997). Multitask learning. Machine Learning, 28, 41–70.
Devroye, L., Györfi, L., & Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition.
Springer, New York.
Dudley, R. M. (1984). A Course on Empirical Processes, Vol. 1097 of Lecture Notes in Mathematics, pp. 2–142. Springer-Verlag.
Dudley, R. M. (1989). Real Analysis and Probability. Wadsworth & Brooks/Cole, California.
196

A M ODEL OF I NDUCTIVE B IAS L EARNING

Gelman, A., Carlin, J. B., Stern, H. S., & Rubim, D. B. (Eds.). (1995). Bayesian Data Analysis.
Chapman and Hall.
Good, I. J. (1980). Some history of the hierarchical Bayesian methodology. In Bernardo, J. M.,
Groot, M. H. D., Lindley, D. V., & Smith, A. F. M. (Eds.), Bayesian Statistics II. University
Press, Valencia.
Haussler, D. (1992). Decision theoretic generalizations of the pac model for neural net and other
learning applications. Information and Computation, 100, 78–150.
Heskes, T. (1998). Solving a huge number of similar tasks: a combination of multi-task learning and
a hierarchical Bayesian approach. In Shavlik, J. (Ed.), Proceedings of the 15th International
Conference on Machine Learning (ICML ’98), pp. 233–241. Morgan Kaufmann.
Intrator, N., & Edelman, S. (1996). How to make a low-dimensional representation suitable for
diverse tasks. Connection Science, 8.
Kechris, A. S. (1995). Classical Descriptive Set Theory. Springer-Verlag, New York.
Khan, K., Muggleton, S., & Parson, R. (1998). Repeat learning using predicate invention. In Page,
C. D. (Ed.), Proceedings of the 8th International Workshop on Inductive Logic Programming
(ILP-98), LNAI 1446, pp. 65–174. Springer-Verlag.
Langford, J. C. (1999). Staged learning. Tech. rep., CMU, School of Computer Science.
http://www.cs.cmu.edu/ jcl/research/ltol/staged latest.ps.

s

Mitchell, T. M. (1991). The need for biases in learning generalisations. In Dietterich, T. G., &
Shavlik, J. (Eds.), Readings in Machine Learning. Morgan Kaufmann.
Parthasarathy, K. R. (1967). Probabiliity Measures on Metric Spaces. Academic Press, London.
Pollard, D. (1984). Convergence of Stochastic Processes. Springer-Verlag, New York.
Pratt, L. Y. (1992). Discriminability-based transfer between neural networks. In Hanson, S. J.,
Cowan, J. D., & Giles, C. L. (Eds.), Advances in Neural Information Processing Systems 5,
pp. 204–211. Morgan Kaufmann.
Rendell, L., Seshu, R., & Tcheng, D. (1987). Layered concept learning and dynamically-variable
bias management. In Proceedings of the Tenth International Joint Conference on Artificial
Intelligence (IJCAI ’87), pp. 308–314. IJCAI , Inc.
Ring, M. B. (1995). Continual Learning in Reinforcement Environments. R. Oldenbourg Verlag.
Russell, S. (1989). The Use of Knowledge in Analogy and Induction. Morgan Kaufmann.
Sauer, N. (1972). On the density of families of sets. Journal of Combinatorial Theory A, 13,
145–168.
Sharkey, N. E., & Sharkey, A. J. C. (1993). Adaptive generalisation and the transfer of knowledge.
Artificial Intelligence Review, 7, 313–328.
197

BAXTER

Silver, D. L., & Mercer, R. E. (1996). The parallel transfer of task knowledge using dynamic
learning rates based on a measure of relatedness. Connection Science, 8, 277–294.
Singh, S. (1992). Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8, 323–339.
Slud, E. (1977). Distribution inequalities for the binomial law. Annals of Probability, 4, 404–412.
Suddarth, S. C., & Holden, A. D. C. (1991). Symolic-neural systems and the use of hints in developing complex systems. International Journal of Man-Machine Studies, 35, 291–311.
Suddarth, S. C., & Kergosien, Y. L. (1990). Rule-injection hints as a means of improving network performance and learning time. In Proceedings of the EURASIP Workshop on Neural
Networks Portugal. EURASIP.
Sutton, R. (1992). Adapting bias by gradient descent: An incremental version of delta-bar-delta. In
Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 171–176. MIT
Press.
Tate, R. F. (1953). On a double inequality of the normal distribution. Annals of Mathematical
Statistics, 24, 132–134.
Thrun, S. (1996). Is learning the n-th thing any easier than learning the first?. In Advances in Neural
Information Processing Systems 8, pp. 640–646. MIT Press.
Thrun, S., & Mitchell, T. M. (1995). Learning one more thing. In Proceedings of the International
Joint Conference on Artificial Intelligence, pp. 1217–1223. Morgan Kaufmann.
Thrun, S., & O’Sullivan, J. (1996). Discovering structure in multiple learning tasks: The TC algorithm. In Saitta, L. (Ed.), Proceedings of the 13th International Conference on Machine
Learning (ICML ’96), pp. 489–497. Morgen Kaufmann.
Thrun, S., & Pratt, L. (Eds.). (1997). Learning to Learn. Kluwer Academic.
Thrun, S., & Schwartz, A. (1995). Finding structure in reinforcement learning. In Tesauro, G.,
Touretzky, D., & Leen, T. (Eds.), Advances in Neural Information Processing Systems, Vol. 7,
pp. 385–392. MIT Press.
Utgoff, P. E. (1986). Shift of bias for inductive concept learning. In Machine Learning: An Artificial
Intelligence Approach, pp. 107–147. Morgan Kaufmann.
Valiant, L. G. (1984). A theory of the learnable. Comm. ACM, 27, 1134–1142.
Vapnik, V. N. (1982). Estimation of Dependences Based on Empirical Data. Springer-Verlag, New
York.
Vapnik, V. N. (1996). The Nature of Statistical Learning Theory. Springer Verlag, New York.

198

Journal of Articial Intelligence Research 12 (2000) 339{386

Submitted 12/99; published 6/00

On Reasonable and Forced Goal Orderings and their Use in
an Agenda-Driven Planning Algorithm

jana koehler@ch.schindler.com

Jana Koehler
Schindler Lifts, Ltd.
R & D Technology Management
6031 Ebikon, Switzerland
Jorg Homann
Institute for Computer Science
Albert Ludwigs University
Georges-Kohler-Allee, Geb. 52
79110 Freiburg, Germany

hoffmann@informatik.uni-freiburg.de

Abstract

The paper addresses the problem of computing goal orderings, which is one of the
longstanding issues in AI planning. It makes two new contributions. First, it formally
denes and discusses two dierent goal orderings, which are called the reasonable and the
forced ordering. Both orderings are dened for simple STRIPS operators as well as for
more complex ADL operators supporting negation and conditional eects. The complexity
of these orderings is investigated and their practical relevance is discussed. Secondly, two
dierent methods to compute reasonable goal orderings are developed. One of them is
based on planning graphs, while the other investigates the set of actions directly. Finally,
it is shown how the ordering relations, which have been derived for a given set of goals
G , can be used to compute a so-called goal agenda that divides G into an ordered set of
subgoals. Any planner can then, in principle, use the goal agenda to plan for increasing
sets of subgoals. This can lead to an exponential complexity reduction, as the solution to a
complex planning problem is found by solving easier subproblems. Since only a polynomial
overhead is caused by the goal agenda computation, a potential exists to dramatically speed
up planning algorithms as we demonstrate in the empirical evaluation, where we use this
method in the IPP planner.
1. Introduction

How to eectively plan for interdependent subgoals has been in the focus of AI planning
research for a very long time. Starting with the early work on ABSTRIPS (Sacerdoti, 1974)
or on conjunctive-goal planning problems (Chapman, 1987), quite a number of approaches
have been presented and the complexity of the problems has been studied. But until today,
planners have made only some progress in solving bigger planning instances and scalability
of classical planning systems is still a problem.
In this paper, we focus on the following problem: Given a set of conjunctive goals, can
we dene and detect an ordering relation over subsets from the original goal set? To arrive
at such an ordering relation over subsets, we rst focus on the atomic facts contained in the
goal set. We formally dene two closely related ordering relations over such atomic goals,
c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Koehler & Hoffmann
which we call reasonable and forced ordering, and study their complexity. It turns out that
both are very hard to decide.
Consequently, we introduce two eÆcient methods that can both be used to approximate
reasonable goal orderings. The denitions are rst given for simple STRIPS domains, where
the desired theoretical properties can be easily proven. Afterwards, we extend our denitions
to ADL operators (Pednault, 1989) handling conditional eects and negative preconditions,
and discuss why we do not further invest any eort in trying to nd forced orderings.
We show how a set of ordering relations between atomic goals can be used to divide the
goal set into disjunct subsets, and how these subsets can be ordered with respect to each
other. The resulting sequence of subsets comprises the so-called goal agenda, which can
then be used to control an agenda-driven planning algorithm.
The method, called Goal Agenda Manager, is implemented in the context of the IPP
planning system, where we show its potential of exponentially reducing computation times
on certain planning domains.
The paper is organized as follows: Section 2 introduces and motivates reasonable and
forced goal orderings. Starting with simple STRIPS operators, they are formally dened,
and their complexity is investigated. In Section 3, we present two methods, which compute an approximation of the reasonable ordering and discuss both orderings from a more
practical point of view. The section concludes with an extension of our denitions to ADL
operators having conditional eects. Section 4 shows how a planning system can benet
from ordering information by computing a goal agenda that guides the planner. We dene
how subsets of goals can be ordered with respect to each other and discuss how a goal
agenda can aect the theoretical properties, in particular the completeness of a planning
algorithm. Section 5 contains the empirical evaluation of our work, showing results that we
obtained using the goal agenda in IPP. In Section 6 we summarize our approach in the light
of related work. The paper concludes with an outlook on possible future research directions
in Section 7.
2. Ordering Relations between Atomic Goals

For a start, we only investigate simple STRIPS domains just allowing sets of atoms to
describe states, the preconditions, and the add and delete lists of operators.
Denition 1 (State) The set of all ground atoms is denoted with P . A state s 2 2 is a
P

subset of ground atoms.

Note that all states are assumed to be complete, i.e., we always know for an atom p whether
p 2 s or p 62 s holds. We also assume that all operator schemata are ground, i.e., we only
talk about actions.
Denition 2 (Strips Action) A STRIPS action o has the usual form
pre(o) ! ADD add(o) DEL del(o)
where pre(o) are the preconditions of o, add(o) is the Add list of o and del(o) is the Delete
list of the action, each being a set of ground atoms. We also assume that del(o) \ add(o) = ;.
The result of applying a STRIPS action to a state is dened as usual:
340

On Reasonable and Forced Goal Orderings

Result(s; o) :=



(s [ add(o)) n del(o) if pre(o)  s
otherwise

s

If pre(o)  s holds, the action is said to be applicable in s. The result of applying a
sequence of more than one action to a state is recursively dened as
Result(s; ho1 ; : : : ; o

i) := Result(Result(s; ho1 ; : : : ; o 1 i); o ):
Denition 3 (Planning Problem) A planning problem (O; I ; G ) is a triple where O is
the set of actions, and I (the initial state) and G (the goals) are sets of ground atoms. A
plan P is an ordered sequence of actions. If all actions in a plan are taken out of a certain
action set O, we denote this by writing P O .
n

n

n

Note that we dene a plan to be a sequence of actions, not a sequence of parallel steps,
as it is done for graphplan (Blum & Furst, 1997), for example. This makes the subsequent
theoretical investigation more readable. The results directly carry over to parallel plans.
Given two atomic goals A and B , various ways to dene an ordering relation over
them can be imagined. First, one can distinguish between domain-specic and domainindependent goal ordering relations. But although domain-specic orderings can be very
eective, they need to be redeveloped for each single domain. Therefore, one is in particular
interested in domain-independent ordering relations having a broader range of applicability.
Secondly, following Hullem et al. (1999), one can distinguish the goal selection and the goal
achievement order. The rst ordering determines in which order a planner works on the
various atomic goals, while the second one determines the order, in which the solution
plan achieves the goals. In this paper, we compute an ordering of the latter type. In
the agenda-driven planning approach that we propose later in the paper, both orderings
coincide anyway. The goals that are achieved rst in the plan are those that the planner
works on rst.
The following scenario motivates how an achievement order for goals can be possibly
dened. Given two atomic goals A and B , for which a solution plan exists, let us assume
the planner has just achieved the goal A, i.e., it has arrived at a state s( : ) , in which A
holds, but B does not hold yet. Now, if there exists a plan that is executable in s( : )
and achieves B without ever deleting A, a solution has been found. If no such plan can be
found, then two possible reasons exist:
1. The problem is unsolvable|achieving A rst leads the planner into a deadlock situation. Thus, the planner is forced to achieve B before or simultaneously with A.
2. The only existing solution plans have to destroy A temporarily in order to achieve B .
But then, A should not be achieved rst. Instead, it seems to be reasonable to achieve
B before or simultaneously with A for the sake of shorter solution plans.
In the rst situation, the ordering \B before or simultaneously with A" is forced by inherent properties of the planning domain. In the second situation, the ordering \B before or
simultaneously with A" appears to be reasonable in order to avoid non-optimal plans. Consequently, we will dene two goal orderings, called the forced and the reasonable ordering.
For the sake of clarity, we rst give some more basic denitions.
A;

B

A;

341

B

Koehler & Hoffmann
Denition 4 (Reachable State) Let (O; I ; G ) be a planning problem and let P be the

set of ground atoms that occur in the problem. We say that a state s  P is reachable, i
there exists a sequence ho1 ; : : : ; o i out of actions in O for which s = Result(I ; ho1 ; : : : ; o i)
holds.
n

n

Denition 5 (Generic State s( : ) ) Let (O; I ; G ) be a planning problem. By s( :
A;

B

A;

B

)

we denote any reachable state in which A has just been achieved, but B is false,
i.e., B 62 s( : ) and there is a sequence of actions ho1 ; : : : ; o i such that s( : ) =
Result(I ; ho1 ; : : : ; o i), with A 2 add(o ).
A;

n

B

n

A;

B

n

One can imagine s( : ) as a state about which we only have incomplete information.
All the states s it represents satisfy s j= A; :B , but the other atoms p 2 P with p 6= A; B
can adopt arbitrary truth values.
Denition 6 (Reduced Action Set O ) Let (O; I ; G ) be a planning problem, and let
A;

B

A

A 2 G be an atomic goal. By O we denote the set of all actions that do not delete A,
i.e., O = fo 2 O j A 62 del(o)g.
A

A

We are now prepared to dene what we exactly mean by forced and reasonable goal orderings.
Denition 7 (Forced Ordering  ) Let (O; I ; G ) be a planning problem, and let A; B 2
G be two atomic goals. We say that there is a forced ordering between B and A, written
f

B  A, if and only if
f

: :9 P O : B 2 Result(s( : ) ; P O )
If Denition 7 is satised, then each plan achieving A and B must achieve B before
or simultaneously with A, because otherwise it will encounter a deadlock, rendering the
problem unsolvable.
Denition 8 (Reasonable Ordering  ) Let (O; I ; G ) be a planning problem, and let
A; B 2 G be two atomic goals. We say that there is a reasonable ordering between B and

8 s(

:

A;

B

)

A;

B

r

A, written B  A, if and only if
r

8 s( : ) : :9 P OA : B 2 Result(s( : ) ; P OA )
Denition 8 gives B  A the meaning that if, after the goal A has been achieved, there
A;

B

A;

B

r

is no plan anymore that achieves B without|at least temporarily|destroying A, then B
is a goal prior to A.
We remark that obviously B  A implies B  A, but not vice versa. We also make
a slightly less obvious observation at this point: The formulae in Denitions 7 and 8 use
a universal quantication over states s( : ) . If in a planning problem there is no such
state at all, the formulae are satised and the goals A and B get ordered, i.e., B  A and
B  A follow, respectively. In this case, however, there is not much information gained
by a goal ordering between A and B , because any sequence of actions will achieve B prior
or simultaneously with A|A cannot be achieved with B still being false. Thus in this
case, the ordering relations B  A and B  A are trivial in the sense that no reasonable
planner would invest much eort in considering the goals A and B ordered the other way
round anyway.
r

f

A;

B

f

r

f

r

342

On Reasonable and Forced Goal Orderings
Denition 9 (Trivial Ordering Relation) Let (O; I ; G ) be a planning problem, and let
A; B 2 G be two atomic goals. An ordering relation B
there is no state s( : ) .
A;



f

A or B  A is called
r

trivial i

B

In this paper, we will usually consider forced and reasonable goal orderings as non-trivial
orderings and make the distinction explicit only if we have to do so.
Denitions 7 and 8 seem to deliver promising candidates for an achievement order.
Unfortunately, both are very hard to test: it turns out that their corresponding decision
problems are PSPACE hard.
Theorem 1 Let F ORDER denote the following problem:

Given two atomic facts A and B , as well as an action set O and an initial state I , does
B  A hold ?
f

Deciding F ORDER is PSPACE-hard.

Proof: The proof proceeds by polynomially reducing

PLANSAT (Bylander, 1994)|the
decision problem of whether there exists a solution plan for a given arbitrary STRIPS
planning instance|to the problem of deciding F ORDER.

Let I , G , and O denote the initial state, the goal state, and the action set in an arbitrary
STRIPS instance. Let A, B , and C be new atomic facts not contained in the instance so
far. We build a new action set and initial state for our F ORDER instance by setting

8
<
O0 := O [ :

o1
o2
o
I
I

G

9
! ADD fAg DEL fC g; =
! ADD I DEL fAg; ;
! ADD fB g DEL ;

= fC g
= fAg
=G

and

I 0 := fC g
With these denitions, reaching B from A is equivalent to solving the original problem. The
other way round, unreachability of B from A|forced ordering B  A|is equivalent to
the unsolvability of the original problem. In order to prove this, we consider the following:
The only way of achieving A is by applying o 1 to I 0 . Consequently, the only state s( : )
is fAg, cf. Denition 5. Thus starting with the assumption that B  A is valid, we apply
the following equivalences:
f

I

A;

B

f

B A
f

,
,
,
,

8 s( : ) : :9 P O0 : B 2 Result(s( : ) ; P O0 )
cf. Denition 7
0
0
:9 P O : B 2 Result(fAg; P O )
fAg is the only reachable state s( : )
:9 P O : G  Result(I ; P O )
with the denition of O 0
no solution plan exists for I ; G and given O
A;

B

A;

B

A;

343

B

Koehler & Hoffmann
Thus, the complement of PLANSAT can be polynomially reduced to F ORDER. As PSPACE
= co-PSPACE, we are done.
Theorem 2 Let R ORDER denote the following problem:

Given two atomic facts A and B , as well as an action set O and an initial state I , does
B  A hold ?
Deciding R ORDER is PSPACE-hard.
r

Proof: The proof proceeds by polynomially reducing PLANSAT to R ORDER.

Let I , G , and O be the initial state, the goal state, and the action set in an arbitrary
STRIPS planning instance. Let A, B , C , and D be new atomic facts not contained in the
instance so far. We dene the new action set O0 by setting
8
9
o 1 = fC g
! ADD fA; Dg DEL fC g; =
<
O0 := O [ : o 2 = fA; Dg ! ADD I DEL fDg;
;
o =G
! ADD fB g DEL ;
I
I

G

and the new initial state by

I 0 := fC g

As in the proof of Theorem 1, the intention behind these denitions is to make solvability
of the original problem equivalent to reachability of B from A. For reasonable orderings,
reachability is concerned with actions that do not delete A, which is why we need the safety
condition D.
Precisely, the only way to achieve A is by applying o 1 to I 0, i.e., per Denition 5 the
only state s( : ) is fA; Dg. As no action in the new operator set O0 deletes A, we have
the following sequence of equivalences.
I

A;

B

B A
8 s( : ) :9 P OA0 : B 2 Result(s( : ); P OA0 )
:9 P OA0 : B 2 Result(fA; Dg; P OA0 )
r

,
,
, :9 P O0 : B 2 Result(fA; Dg; P O0 )
, :9 P O such that G  Result(I ; P O )
, no solution plan exists for I ; G ; O
A;

B

A;

B

cf. Denition

fA; Dg is the only reachable state s( : )
no action in O 0 deletes A
with the denition of O 0
A;

B

Thus, the complement of PLANSAT can be polynomially reduced to R ORDER. With
PSPACE = co-PSPACE, we are done.
Consequently, nding reasonable and forced ordering relations between atomic goals is
already as hard as the original planning problem and it appears unlikely that a planner will
gain any advantage from doing that. A possible way out of the dilemma is to dene new
ordering relations, which can be decided in polynomial time and which are, ideally, suÆcient
for the existence of reasonable or forced goal orderings. In the following, we introduce two
such orderings.
344

8

On Reasonable and Forced Goal Orderings
3. The Computation of Goal Orderings

In this section, we will
1. dene a goal ordering  , which can be computed using graphplan's exclusivity
information about facts. We prove that this ordering is suÆcient for  and that it
can be decided in polynomial time (the subscript \e" stands for \eÆcient").
2. dene a goal ordering  , which is computed based on a heuristic method that is
much faster than the computation based on graphplan, and also delivers powerful
goal ordering information (the subscript \h" stands for \heuristic").
3. discuss that most of the currently available benchmark planning domains do not contain forced orderings, i.e.,  will fail in providing a problem decomposition for them.
4. show how our orderings can be extended to handle more expressive ADL operators.
e

r

h

f

3.1 Reasonable Goal Orderings based on graphplan

A goal ordering is always computed for a specic planning problem involving an initial
state I , a goal set G  fA; B g, and the set O of all ground actions. In order to develop an
eÆcient computational method, we proceed in two steps now:
1. We compute more knowledge about the generic state s( : ) .
2. We dene the relation  and investigate its theoretical properties. In particular, we
prove that  implies  .
A;

B

e

e

r

The state s( : ) represents states that are reachable from I , and in which A has
been achieved, but B does not hold. Given this information about s( : ) , one can derive
additional knowledge about it. In particular, it is possible to determine a subset of atoms F,
of which one denitely knows that F \ s( : ) = ; must hold. One method to determine F is
obtained via the computation of invariants, i.e., logical formulae that hold in all reachable
states, cf. (Fox & Long, 1998). After having determined the invariants, one assumes that A
holds, but B does not, and then computes the logical implications. Another possibility is to
simply use graphplan (Blum & Furst, 1997). Starting from I with O, the planning graph
is built until the graph has leveled o at some time step. The proposition level at this time
step represents a set of states, which is a superset of all states that are reachable from I
when applying actions from O. All atoms, which are marked as mutually exclusive (Blum
& Furst, 1997) of A in this level can never hold in a state satisfying A. Thus, they cannot
hold in s( : ) . We denote this set with F |the False set with respect to A returned by
1
graphplan.
F
:= fp j p is exclusive of A when the graph has leveled og
(1)
Note that the planning graph is only grown once for a given I and O, but can be used to
determine the F sets for all atomic goals A 2 G .
A;

B

A;

A;

A;

B

B

A
GP

B

A
GP

A

GP

1. We assume the reader to be familiar with graphplan, because this planning system is very well known in
the planning research community. Otherwise, (Blum & Furst, 1997) provide the necessary background.

345

Koehler & Hoffmann

\ s = ; holds for all states s
I using actions from O.
Lemma 1

A

FGP

A

A

satisfying A 2 s that are reachable from
A

The proof follows immediately from the denitions of \level-o" and \two propositions
being mutual exclusive" given in (Blum & Furst, 1997).
We now provide a simple test which is suÆcient for the existence of a reasonable ordering
B  A between two atomic goals A and B .
Denition 10 (EÆcient Ordering  ) Let (O; I ; G  fA; B g) be a planning problem.
r

e

Let F

A

GP

be the False set for A. The ordering B  A holds if and only if
e

8 o 2 O : B 2 add(o) ) pre(o) \ F 6= ;
A
GP

A

This means, B is ordered before A if the reduced action set only contains actions, which
either do not have B in their add lists or if they do, then they require a precondition which
is contained in the False set. Such preconditions can never hold in a state satisfying A and
thus, these actions will never be applicable.
Theorem 3
B A)B A
e

r

Proof: Assume that B 6 A, i.e., B 2 Result(s( : ) ; P OA ) for a reachable state s( :
r

A;

B

A;

B

)

with A 2 s( : ) , B 62 s( : ) , and a Plan P OA = ho1 ; : : : ; o i where o 2 O for 1  i  n.
As A 62 del(o ) for all i (Denition 6), we have
A 2 Result(s( : ) ; ho1 ; : : : ; o i) for 0  i  n
and, with Lemma 1,
F
\ Result(s( : ) ; ho1 ; : : : ; o i) = ; for 0  i  n
(2)
Furthermore, as B 62 s( : ) , but B 2 Result(s( : ) ; ho1 ; : : : ; o i), there must be a
step which makes B true, i.e.,
91  k  n : B 62 Result(s( : ); ho1 ; : : : ; o 1i) ^ B 2 Result(s( : ); ho1 ; : : : ; o i)
For this step, we obviously have B 2 add(o ) and consequently, with the denition
of B  A, pre(o ) \ F 6= ;. Now, as o must be applicable in the state where it
is executed (otherwise it would not add anything to this state), the preconditions of o
must hold, i.e., pre(o )  Result(s( : ) ; ho1 ; : : : ; o 1 i). This immediately leads to F \
Result(s( : ) ; ho1 ; : : : ; o 1 i) 6= ;, which is a contradiction to Equation (2).
Quite obviously, the ordering  can be decided in polynomial time.
A;

B

A;

n

B

i

A

i

A;

i

B

A

A;

GP

A;

i

B

B

A;

A;

n

B

k

B

A;

B

k

k

e

A
GP

k

k

k

k

A;

B

A;

A
GP

k

B

k

e

Theorem 4 Let E ORDER denote the following problem:

Given two atomic facts A and B , as well as an initial state I and an action set O, does

B  A hold ?
e

Then, E ORDER can be decided in polynomial time:
346

E ORDER

2 P.

On Reasonable and Forced Goal Orderings
Proof: To begin with, we need to show that computing F

takes only polynomial time.
From the results in (Blum & Furst, 1997), it follows directly that building a planning graph
is polynomial in jIj, jOj, l and t, where l is the maximal length of any precondition, add
or delete list of an action, and t is the number of time steps built. Taking l as a parameter
of the input size, it remains to show that a planning graph levels o after a polynomial
number t of time steps. Now, a planning graph has leveled o if between some time steps
t and t + 1 neither the set of facts nor the number of exclusion relations change. Between
two subsequent time steps, the set of facts can only increase|facts already occuring in the
graph remain there|and the number of exclusions can only decrease|non-exclusive facts
will be non-exclusive in all subsequent layers. Thus, the maximal number of time steps to
be built until the graph has leveled o is dominated by the maximal number of changes
that can occur between two subsequent layers, which is dominated by the maximal number
of facts plus the maximal number of exclusion relations. The maximal number of facts is
O(jIj + jOj  l), and the maximal number of exclusions is O((jIj + jOj  l)2 ), the square of
the maximal number of facts.
Having computed F in polynomial time, testing B  A involves looking at all actions
in O, and rejecting them if they either
 delete A, which is decidable in time O(l), or
 have a precondition, which is an element of F , decidable in time O(l  (jIj + jOj l)).
Thus we have an additional runtime for the test, which is O(jOj  l  (jIj + jOj  l)).
A
GP

A
GP

e

A
GP

Let us consider the following example, which illustrates the computation of  using
a common representational variant of the blocks world with actions to stack, unstack,
pickup, and putdown blocks:
e

pickup(?ob)

clear(?ob) on-table(?ob) arm-empty()

! ADD holding(?ob)

DEL clear(?ob) on-table(?ob) arm-empty().

putdown(?ob)
holding(?ob)

! ADD clear(?ob) arm-empty() on-table(?ob)
DEL holding(?ob).

stack(?ob,?underob)

clear(?underob) holding(?ob)

unstack(?ob,?underob)

! ADD arm-empty() clear(?ob) on(?ob,?underob)
DEL clear(?underob) holding(?ob).

on(?ob,?underob) clear(?ob) arm-empty()

! ADD holding(?ob) clear(?underob)

DEL on(?ob,?underob) clear(?ob) arm-empty().

Given the simple task of stacking three blocks:
initial state: on-table(a) on-table(b) on-table(c)
goal state: on(a,b) on(b,c)
347

Koehler & Hoffmann
is there a reasonable ordering between the two atomic goals? Intuitively, the blocks world
domain possesses a very natural goal ordering, namely that the planner should start building
each tower from the bottom to the top and not the other way round.2
Let us rst investigate whether the relation on(a; b)  on(b; c) holds. Vividly speaking,
it asks whether it is still possible to stack the block a on b after on(b; c) has been achieved.
As a rst step, we run graphplan to nd out which atoms are exclusive of on(b; c) when
the planning graph, which corresponds to this problem, has leveled o. The result is
e

(

)

on b;c

FGP

= fclear(c), on-table(b), holding(c), holding(b), on(a,c), on(c,b), on(b,a)g

One observes immediately that these atoms can never be true in a state that satises

on(b; c).

Secondly, we remove all ground actions which delete on(b; c) (in this case, only the action
( ).
Now we are ready to test if on(a; b)  on(b; c) holds. The only action, which can add
on(a; b) is stack(a,b). It has the preconditions holding(a) and clear(b), neither of which
is a member of F ( ) . The test fails and we get on(a; b) 6 on(b; c).
As a next step, we test whether on(b; c)  on(a; b) holds. graphplan returns the
following False set:

unstack(b,c) satises this condition) and obtain the reduced action set O

on b;c

e

on b;c

e

GP

e

(

)

on a;b

FGP

= fclear(b), on-table(a), holding(b), holding(a), on(a,c), on(c,b), on(b,a)g

The action unstack(a,b) is not contained in O ( ) because it deletes on(a; b). The
only action which adds on(b; c) is stack(b,c). It needs the preconditions clear(c) and
holding(b). The second precondition holding(b) is contained in the set of false facts,
i.e., holding(b) 2 F ( ) and thus, we conclude on(b; c)  on(a; b). Altogether, we have
on(a; b) 6 on(b; c) and on(b; c)  on(a; b), which correctly reects the intuition that b
needs to be stacked onto c before a can be stacked onto b.
Although  appears to impose very strict conditions on a domain in order to derive a
reasonable goal ordering, it succeeds in nding reasonable goal orderings in all available test
domains in which such orderings exists. For example, in the tyreworld, in bulldozer problems,
in the shopping problem (Russel & Norvig, 1995), the fridgeworld, the glass domain, the
tower of hanoi domain, the link-world, and the woodshop. Its only disadvantage are the
computational resources it requires, since building planning graphs, while being theoretically
polynomial, is a quite time- and memory-consuming thing to do.3
Therefore, the next section presents a fast heuristic computation of goal orderings, which
analyzes the domain actions directly and does not need to build planning graphs anymore.
on a;b

on a;b

e

GP

e

e

e

2. Note that the goals do not specify where the block c has to go, but leave this to the planner.
3. More recent implementations of planning graphs, which are for example developed for STAN (Fox &
Long, 1999) and IPP 4.0 (Koehler, 1999) do not build the graphs explicitly anymore and are orders of
magnitude faster than the original graphplan implementation, but still the computation of the planning
graph takes almost all the time that is needed to determine the e relations.

348

On Reasonable and Forced Goal Orderings
3.2 Reasonable Goal Orderings derived by a Fast Heuristic Method

One can analyze the available actions directly using a method we will call Direct Analysis
(DA). It determines an initial value for F by computing the intersection of all delete lists of
all actions which contain A in their add list, as dened in the following equation.
A

FDA

\

:=
o

2O 2
; A

( )

del(o)

(3)

add o

The atoms in this set are all false in a state where A has just been achieved: they are
deleted from the state description independently of the action that is used to add A. As a
short example, let us consider the two actions

! ADD fAg DEL fC; Dg
! ADD fA; C g DEL fDg
Only the atom D is deleted by both actions, and thus D is the only element initially
contained in F .
However, Equation (3) only says that when A is added then the atoms from F will be
deleted. It does not say anything about whether it might be possible to reestablish atoms
in F . One can easily imagine that actions exist, which leave A true, and at the same
time add such atoms. If this is the case, there are reachable states in which A and atoms
from F hold.
Now, our goal is to derive an ordering relation that can be easily computed, and that
ideally, like the  relation, is suÆcient for the  relation. Therefore, we want to make
sure that the atoms in F are really false in any state after A has been achieved. We
arrive at an approximation of atoms that remain false by performing a xpoint reduction
on the F set, removing those atoms that are achievable in the following sense.
A
DA

A
DA

A
DA

A
DA

e

r

A

DA

A

DA

Denition 11 (Achievable Atoms) An atom p is achievable from a state s given an
action set O (written A(s; p; O)) if and only if
p2s

_ 9 o 2 O : p 2 add(o) ^ 8 p0 2 pre(o) : A(s; p0; O)

The denition says that an atom p is achievable from a state s if it holds in s, or if there
exists an action in the domain, which adds p and whose preconditions are all achievable
from s. This is a necessary condition for the existence of a plan P O from s to a state where
p holds.
Lemma 2 9 P O : p 2 Result(s; P O ) ) A(s; p; O)
Proof: The atom p must either already be contained in the state s, or it has to be added

by a step o out of P O . In the second case, all preconditions of o need to be established by
P O in the same way. Thus p and all preconditions of the step, which adds it, are achievable
in the sense of Denition 11.
349

Koehler & Hoffmann
There are two obvious diÆculties with Denition 11: First, p 2 s must be tested. With
complete knowledge about the state s, this should not cause any problems. In our case,
however, we only have the generic state s( : ) and cannot decide whether an arbitrary
atom is contained in it or not. Secondly, we observe an innite regression over preconditions,
which must be tested for achievability.
As for the rst problem, it turns out that it is a good heuristic to simply assume p 62 s,
i.e., no test is performed at all. As for the second problem, in order to avoid innite
looping of the \achievable"-test, one needs to terminate the regression over preconditions
at a particular level. The point in question is how far to regress? A quick approximation
simply decides \achievable" after the rst recursive call.
A;

B

Denition 12 (Possibly Achievable Atoms) An atom p is possibly achievable given
an action set O (written pA(p; O)) if and only if
9 o 2 O : p 2 add(o) ^ 8 p0 2 pre(o) :

9 o0 2 O : p0 2 add(o0 )

holds, i.e., there is an action that adds p and all of its preconditions are add eects of other
actions in O.

If the assumption is justied that none of the atoms p is contained in the state s, then
being possibly achievable is a necessary condition for being achievable.
Lemma 3 Let s be a state for which p 62 s and also 8o 2 O : p 2 add(o) ) pre(o) \ s = ;

holds. Then we have

A(s; p; O) ) pA(p; O)
Proof: From A(s; p; O) and p 62 s, we know that there is a step o 2 O, p 2 add(o), with

8 p0 2 pre(o) A(s; p0; O). We also know that pre(o) \ s = ;, so for each p0 2 pre(o) there
must be an achiever o0 2 O : p0 2 add(o0 ).

The condition that all of the facts p must not be contained in the state s seems to be
rather rigid. Nevertheless, the condition of being possibly achievable delivers good results
on all of the benchmark domains and it is easy to decide. We can now use this test to both

 perform a xpoint reduction on the set F and
 decide whether an atomic goal B should be ordered before A.
A
DA

The xpoint reduction, as depicted in Figure 1 below, uses the approximative test pA(f; O )
to remove facts from F that can be achieved. It nds all these facts under certain
restrictions, see below. As a side eect of the xpoint algorithm, we obtain the set O of
actions that our method assumes to be applicable after a state s( : ) . We then order B
before A i it cannot possibly be achieved using these actions.
A
DA

A;

350

B

On Reasonable and Forced Goal Orderings
 := F
O := O n fo j F \ pre(o) 6= ;g
fixpoint reached := false
while :fixpoint reached
fixpoint reached := true
for f 2 F
if pA(f; O ) then


F := F n ff g
O := O n fo j F \ pre(o) 6= ;g
fixpoint reached := false
F

A

DA
A

A

endif
endfor
endwhile
return F , O

Figure 1: Quick, heuristic xpoint reduction of the set F .
A
DA

The computation checks whether atoms of F , which is initially set to F , are possibly
achievable using only those actions, which do not delete A and which do not require atoms
from F as a precondition. Achievable atoms are removed from F , and O gets updated
accordingly. If in one iteration, F does not change, the xpoint is reached, i.e., F will not
further decrease and O will not further increase|the nal sets F of false facts and O of
applicable actions are returned.
Let us illustrate the xpoint computation with a short example consisting of the empty
initial state, the goals fA; B g, and the following set of actions
A

DA

op1:
op2:
op3: f C g
op4: f D g

!
!
!
!

ADD f A g
ADD fA, C g
ADD f D g
ADD f B g

DEL f C, D g
DEL f D g

When assuming that A has been achieved, we obtain F = F = fDg as the initial
value of the False set, since D is the only atom that op1 and op2 delete when adding A.
Figure 2 illustrates a hypothetical planning process. Starting in the empty initial state
and trying to achieve A rst, we get two dierent states s( : ) in which A holds. The
atom D does not hold in any of them and thus in both states, no action is applicable that
requires D as a precondition. This excludes op4 from O , yielding the initial action set
O = fop1; op2; op3g. Now, op4 is the only action that can add B . Therefore, if we used
this action set to see if B can still be achieved, we would nd that this is not the case.
Consequently, without performing the xpoint computation, we would order B before A.
But as can be seen in Figure 2, this would not be a reasonable ordering: there is the plan
hop3 ;op4i that achieves B from the state s( : ) = Result(I ;op2) without destroying A.
The xpoint computation works us around this problem as follows: There is the action op3, which can add the precondition D of op4 without deleting A. When checking
pA(D; O ) in the rst iteration, the xpoint procedure nds this action. It then checks
A

DA

A;

A

A;

351

B

B

Koehler & Hoffmann
whether the preconditions of op3 are achievable in the sense that they are added by another action. This is the case since the only precondition C is added by op2. Thus, D is
removed from F , which becomes empty now. The action op4 is put back into the set O ,
which now becomes identical with the action set O . This set, in turn, is identical with the
original action set O as no action deletes A. The xpoint process terminates and B will
not be ordered before A as it can be achieved using the action op4. This correctly reects
the fact that there exists a plan from the state s( : ) = Result(I ; hop2i) = fC; Ag to a
state that satises B without destroying A.
A

A;

B

0/

Deadlock

op1

op2

A

C, A
op3
C, A, D

D holds in a state satisfying A

op4
C, A, D, B

There is a plan from A to B

Figure 2: An example illustrating why we need the xpoint computation.
As already pointed out, the intention behind the xpoint procedure is the following:
Starting from a state s( : ) , we want to know which facts can become true without
destroying A, and consequently, which actions can become applicable. In the rst step,
only actions that do not use any of the facts in F are applicable, as all those facts are
deleted from the state description when A is added. However, such actions may make facts
in F true, so we want to remove those facts from F . If we manage to nd all the facts
that can be made true without destroying A, then the nal set F will contain only those
facts that do not hold in a state reachable from s( : ) without destroying A. In this case,
the nal action set O will contain all the actions that can be applied after s( : ) , and we
can safely use this action set to determine whether another goal B can still be achieved or
not.
However, as we only use the approximative test pA(f; O) with f 2 F to nd out if
a fact in the current F set is achievable, there may be facts which are achievable without
destroying A, but which remain in the set F . This could exclude actions from the set
O which can be safely applied after s( : ). Under certain restrictions, however, we can
prove that this will not happen. In order to do so, we need to impose a restriction on the
particular state s( : ) , in which we achieved the goal A: If none of the preconditions of
actions, which add facts contained in F , occur in the state s( : ) , then the xpoint
procedure will remove all facts from F that are achievable without destroying A. We will
use this property of the xpoint procedure later to show that our heuristic ordering relation
approximates reasonable orderings.
A;

B

A

DA

A
DA

A
DA

A;

B

A;

A;

A;

B

B

A
DA

A;

A

DA

352

B

B

On Reasonable and Forced Goal Orderings
Lemma 4 Let (O; I ; G ) be a planning problem, and let A 2 G be an atomic goal. Let
s( : ) be a reachable state where A has just been achieved. Let P OA = ho1 ; : : : ; o i be
a sequence of actions not destroying A. Let F be the set of facts that is returned by the
xpoint computation depicted in Figure 1. If we have
A;

n

B

8f 2 F

A
DA

: 8o 2 O : f 2 add(o) ) pre(o) \ s( : ) = ;
A

A;

()

B

then no fact in F holds in the state that is reached by applying P OA , i.e.,
Result(s( : ) ; P OA ) \ F = ;
A;

B

Proof:

Let F and O denote the state of the fact and action sets, respectively, after j iterations
of the algorithm depicted in Figure 1. As F only decreases during the computation, we have


F  F for all j . Let s0 ; : : : ; s denote the sequence of states that are encountered when
executing P OA = ho1 ; : : : ; o i in s( : ) , i.e., s0 = s( : ) and s = Result(s 1 ; ho i) for
0  i  n. We can assume that each action o is applicable in state s 1 , i.e., pre(o )  s 1 .
Otherwise, o does not cause any state transition, and we can skip it from P OA . Obviously,
we have s = Result(s( : ) ; P OA ), so we need to show that s \ F = ;. The proof proceeds
by induction over the length n of P OA .
n = 0: P OA = hi and s = s0 = s( : ) . All facts in F are deleted from the state
description when A is added, so we have s \ F = ;. As F = F0 and F  F0 , the
proposition follows immediately.
n ! n + 1: P OA = ho1 ; : : : ; o ; o +1 i. From the induction hypothesis, we know that
s \ F = ; for 0  i  n. What we need to show is s +1 \SF = ;.
Let j be the step in the xpoint iteration where F \ =0 s becomes empty, i.e., j
denotes the iteration in which the intersection of all the states s ; i  n with F is empty
for the rst time. Such an iteration exists, because all the intersections s \ F with i  n
are empty.
Now each action o ; 1  i  n + 1 is applicable in state s 1 , i.e., pre(o )  s 1 , and
thus pre(o ) \ F = ; for all the actions o in P OA . Therefore, all these actions are contained
in O , as this set contains all the actions out of O whose intersection with F is empty.
Let us focus on the facts in the state s +1 . All these facts are achieved by executing P OA in
s( : ) . In other words, there is a plan from s( : ) to each of these facts. As we have just
seen, this plan consists out of actions in O . Applying Lemma 2 to all the facts p 2 s +1
using s( : ) and P OA (= P Oj ), we know that all facts p are achievable using actions from
O .
j

j

n

j

n

A;

B

A;

i

B

i

i

i

i

i

i

i

n

A;

n

B

n

A;

A
DA

B

n

n

A

A

DA

DA

n

i

n

j

i

;:::;n

i

i

j

i

i

i

i

i

i

i

j

A

j

j

n

A;

B

A;

B

n

j

A;

B

j

: A(s( : ) ; p; O )
We will now show that those facts f 2 s +1 we are interested in, namely the F facts that are
added by o +1 and that are still contained in F , are also possibly achievable using actions
from O . Let f be a fact f 2 s +1 , f 2 F . We apply Lemma 3 using s( : ) , f , and

8p 2 s

+1

n

A;

B

j

n

n

j

j

n

j

353

A;

B

Koehler & Hoffmann

O. We can apply Lemma 3 as obviously f 62 s( : ) , and as 8o 2 O : f 2 add(o) )
pre(o) \ s( : ) = ; by prerequisite (). With A(s( : ) ; p; O ), we arrive at
8f 2 s +1 \ F : pA(f; O)
A;

j

A;

B

A;

n

B

j

B

j

j

j

What remains to be proven is that all these facts f will be removed from F during the
xpoint computation. With the argumentation above, it is suÆcient to show that all the
facts f 2 s +1 \ F will get tested for pA(f; O ) in iteration j +1 of the xpoint computation.
These tests will succeed and lead to s +1 \ F+1 = ;, yielding, as desired, s +1 \ F = ;.
Remember that F+1  F . There are two cases, which we need to consider:
1. j = 0: all intersections s \ F0 are initially empty, i.e., s \ F = ; for 0  i  n. In
this case, all facts f 2 s +1 \ F are tested for pA(f; O0 ) in iteration j + 1 = 1 of
the xpoint computation.
2. j > 0: in this case, at least one of the intersections s \ F became empty in iteration
j by denition of j , i.e., at least one fact was removed from F in this iteration.
Therefore, the xpoint has not been reached yet, and the computation performs at
least one more iteration, namely iteration j + 1. All facts in F will be tested in this
iteration, in particular all facts f 2 s +1 \ F .
With these observations, the induction is complete and the proposition is proven.
As has already been said, we now simply order B before A, if it is not possibly achievable
using the action set that resulted from the xpoint computation. The ordering relation 
(where h stands for \heuristic") obtained in this way approximates the reasonable goal
ordering  .
Denition 13 (Heuristic Ordering  ) Let (O; I ; G  fA; B g) be a planning problem.
n

j

j

n

n

j

j

i

n

A

i

DA

A
DA

i

j

j

n

j

h

r

Let O be the set of actions that is obtained from O by performing the xpoint computation
shown in Figure 1.
The ordering B  A holds if and only if
:pA(B; O)
h

h

If A has been reached in a particular state s( : ) where the assumptions made by
the xpoint computation and by the test for pA(B; O ) are justied, then being not possibly achievable is a suÆcient condition for the non-existence of a plan to B that does not
temporarily destroy A.
Theorem 5 Let (O; I ; G ) be a planning problem, and let A; B 2 G be two atomic goals. Let
A;

B

s( : ) be a reachable state where A has just been achieved, but B is still false, i.e., B 62
s( : ) . Let F and O be the sets of facts and actions, respectively, that are derived by the
xpoint computation shown in Figure 1. If we have
A;

B

A;

B

8f 2 F [ fB g : 8o 2 O : f 2 add(o) ) pre(o) \ s(
A

DA

then we have

A

:pA(B; O) ) :9P OA :

:

A;

B

B 2 Result(s( : ) ; P OA )

354

A;

B

)

=;

()

On Reasonable and Forced Goal Orderings
Proof: Assume that there is a plan P OA = ho1 ; : : : ; o i that does not destroy A, but
n

achieves B , i.e., B 2 Result(s( : ) ; ho1 ; : : : ; o i). With the restriction of () to the
facts in F , Lemma 4 can be applied to each action sequence ho1 ; : : : ; o 1 i yielding
Result(s( : ) ; ho1 ; : : : ; o 1 i) \ F = ;. Consequently, each o is either
 not applicable in Result(s( : ) ; ho1 ; : : : ; o 1i),
 or its preconditions are contained in Result(s( : ); ho1 ; : : : ; o 1 i), yielding pre(o ) \

F = ;.
In the rst case, we simply skip o as it does not have any eects. In the second case,
o 2 O follows. Thus, we have a plan constructed out of actions in O that achieves B
from s( : ) . Applying Lemma 2 leads us to A(s( : ) ; B; O ). We have B 62 s( : ) .
We also know, from () with respect to B , as O  O , that 8o 2 O : B 2 add(o) )
pre(o) \ s( : ) = ; holds. Therefore, we can now apply Lemma 3 and arrive at pA(B; O ),
which is a contradiction.
We return to the blocks world example and show how the computation of  proceeds.
Let us rst investigate whether on(a; b)  on(b; c) holds. The initial value for F ( ) is
obtained from the delete list of the stack(b,c) action, which is the only one that adds this
goal.
A;

n

B

A

i

DA

A;

B

i

i

A;

i

B

A;

i

B

i

i

i

A;

B

A;

B

A;

B

A

A;

B

h

on b;c

h

DA

= fclear(c); holding(b)g
Intuitively, it is immediately clear that neither of these facts can ever hold in a state
where on(b; c) is true: if b is on c, then c is not clear and the gripper cannot hold b. It
turns out that the xpoint computation respects this intuition and leaves the set F ( )
unchanged, yielding F = fclear(c); holding(b)g. We do not repeat the xpoint process in
detail here, because it can be reconstructed from Figure 1 and the details are not necessary
for understanding how the correct ordering relations are derived. In short, for both facts
there are achievers in the reduced action set, but all of them need preconditions for which
no achiever is available. For example, holding(b) can be achieved by either an unstack or
a pickup action. Both either need b to stand on another block or to stand on the table.
All actions that can achieve these facts need holding(b) to be true and are thus excluded
from the reduced action set.
After nishing the xpoint computation, the planner tests pA(on(a; b); O ), where O
contains all actions except those that delete on(b; c) and those that use clear(c) or holding(b)
as a precondition. It nds that the action stack(a,b) adds on(a; b). The preconditions
of this action are holding(a) and clear(b). These conditions are added by the actions
pickup(a) and unstack(a,b), respectively, which are both contained in O : neither of
them needs c to be clear or b to be in the gripper. Thus, the test nds that in fact, on(a; b)
is possibly achievable using the actions in O , and no ordering is derived, i.e., on(a; b) 6
on(b; c) follows.
Now, the other way round, on(b; c)  on(a; b) is tested. The initial value for F ( ) is
obtained from the single action stack(a,b) as
( )
= fclear(b); holding(a)g
F
(

)

on b;c

FDA

on b;c
DA

h

on a;b

h

DA

on a;b
DA

355

Koehler & Hoffmann
Again, the xpoint computation does not cause any changes, resulting in F = fclear(b);
holding(a)g. The process now tests whether pA(on(b; c); O ) holds, where O contains
all actions except those that delete on(a; b) and those that use clear(b) or holding(a) as
a precondition. The only action that can add on(b; c) is stack(b,c). This action needs
as preconditions the facts holding(b) and clear(c). The process now nds that a crucial
condition for achieving the rst fact is violated: Each action that can achieve holding(b)
has clear(b) as a precondition, because b must be clear rst before the gripper can hold it.
Since clear(b) is an element of F , none of the actions achieving holding(b) is contained in
O. Consequently, the test for pA(on(b; c); O ) fails and we obtain the ordering on(b; c) 
on(a; b). This makes sense as the gripper cannot grasp b and stack it onto c anymore, once
on(a; b) is achieved.
h

3.3 On Forced Goal Orderings and Invertible Planning Problems

So far, we have introduced two easily computable ordering relations  and  that both
approximate the reasonable goal ordering  . One might wonder why we do not invest any
eort in trying to nd forced goal orderings. There are two reasons for that:
h

e

r

1. As we have already seen in Section 2, any forced goal ordering is also a reasonable
goal ordering, i.e., a method that approximates the latter can also be used as a crude
approximation to the former.
2. Many benchmark planning problems are invertible in a certain sense. Those problems
do not contain forced orderings anyway.
In this section, we elaborate in detail the second argument. The results are a bit more
general than necessary at this point. We want to make use of them later when we show that
the Agenda-Driven planning algorithm we propose is complete with respect to a certain class
of planning problems. We proceed by formally dening this class of planning problems, show
that these problems do not contain forced orderings, and identify a suÆcient criterion for
the membership of a problem in this class. Finally, we demonstrate that many benchmark
planning problems do in fact satisfy this criterion. For a start, we introduce the notion of
a deadlock in a planning problem.
Denition 14 (Deadlock) Let (O; I ; G ) be a planning problem. A reachable state s is
called a deadlock i there is no sequence of actions that leads from s to the goal, i.e., i
0
0
s = Result(I ; P O ) and :9 P O : G  Result(s; P O ).

The class of planning problems we are interested in is the class of problems that are

deadlock-free. Naturally, a problem is called deadlock-free if none of its reachable states is

a deadlock in the sense of Denition 14.
Non-trivial forced goal orderings imply the existence of deadlocks (remember that an
ordering B  A or B  A is called trivial i there is no state s( : ) at all).
f

r

A;

B

Lemma 5 Let (O; I ; G ) be a planning problem, and let A; B 2 G be two atomic goals. If
there is a non-trivial forced ordering B  A between A and B , then there exists a deadlock
state s in the problem.
f

356

On Reasonable and Forced Goal Orderings
Proof: Recalling Denition 9 and assuming non-triviality of  , we know that there is
at least one state s( : ) where A is made true, but B is still false. From Denition 7,
we know that there is no plan in any such state that achieves B . In particular, it is not
possible to achieve all goals starting out from s( : ) . Thus, the state s := s( : ) must be
a deadlock.
f

A;

B

A;

B

A;

B

We will now investigate deadlocks in more detail and discuss that most of the commonly
used benchmark problems do not contain them, i.e., they are deadlock-free. With Lemma 5,
we then also know that such domains do not contain non-trivial forced goal orderings
either|so there is not much point in trying to nd them. We do not care about trivial goal
orderings. Such orderings force any reasonable planning algorithm to consider the goals in
the correct order.
The existence of deadlocks depends on structural properties of a planning problem:
There must be action sequences, which, once executed, lead into states from which the goals
cannot be reached anymore. These sequences must have undesired eects, which cannot be
inverted by any other sequence of actions in O. Changing perspective, one obtains a hint
on how a suÆcient condition for the non-existence of deadlocks might be dened. Assume
we have a planning problem where the eects of each action sequence in the domain can
be inverted by executing a certain other sequence of actions. In such an invertible planning
problem, it is in particular possible to get back to the initial state from each reachable state.
Therefore, if such a problem is solvable, then it does not contain deadlocks: From any state,
one can reach all goals by going back to the initial state rst, and then execute an arbitrary
solution thereafter. We will now formally dene the notion of invertible planning problems,
and turn the above argumentation into a proof.
Denition 15 (Invertible Planning Problem) Let (O; I ; G ) be a planning problem, and
let s denote the states that are reachable from I with actions from O. The problem is called

invertible if and only if

8 s : 8 PO : 9 PO :

Result(Result(s; P O ); P

O) = s

Theorem 6 Let (O; I ; G ) be an invertible planning problem, for which a solution exists.
Then (O; I ; G ) does not contain any deadlocks.

Proof: Let s = Result(I ; P O ) be an arbitrary reachable state. As the problem is invert-

ible, we know that there is a sequence of actions P O for which Result(s; P O ) = I holds.
As the problem is solvable, we have a solution plan P O starting from I and achieving
G  Result(I ; P O ). Together, we obtain G  Result(Result(s; P O ); P O ). Therefore, the
concatenation of P O and P O is a solution plan executable in s and consequently, s is no
deadlock.
s

s

s

s

s

We now know that invertible planning problems, if solvable, do not contain deadlocks and
consequently, they do not contain (non-trivial) forced goal orderings. What we will see next
is that, as a matter of fact, most benchmark planning problems are invertible. We arrive
at a suÆcient condition for invertibility through the notion of inverse actions.
357

Koehler & Hoffmann
Denition 16 (Inverse Action) Given an action set O containing an action o of the
form pre(o) ! add(o) del(o). An action o 2 O is called inverse to o if and only if o has
the form pre(o) ! add(o) del(o) and satises the following conditions
1. pre(o)  pre(o) [ add(o) n del(o)
2. add(o) = del(o)
3. del(o) = add(o)

Under certain conditions, applying an inverse action leads back to the state one started
from.
Lemma 6 Let s be a state and o be an action, which is applicable in s. If del(o)  pre(o)

and s \ add(o) = ; hold, then an action o that is inverse to o in the sense of Denition 16
is applicable in Result(s; hoi) and Result(Result(s; hoi); hoi) = s follows.

Proof: As o is applicable in s, we have pre(o)  s. The atoms in add(o) are added, and

the atoms in del(o) are removed from s, so altogether we have

Result(s; hoi)  (pre(o) [ add(o)) n del(o)  pre(o)

Thus, o is applicable in Result(s; hoi).
Furthermore, we have Result(s; hoi) = s [ add(o) n del(o) and with that
Result(Result(s; hoi); hoi)
= Result(s [ add(o) n del(o); hoi)
= (s [ add(o) n del(o)) [ add(o) n del(o)
= (s [ add(o) n del(o)) [ del(o) n add(o)
(cf. Denition 16)
= s [ add(o) n add(o)
(because del(o)  pre(o)  s)
= s
(because s \ add(o) = ;)
Lemma 6 states two prerequisites: (1) inclusion of the operator's delete list in its preconditions and (2) an empty intersection of the operator's add list with the state where it is
applicable. A planning problem is called invertible if it meets both prerequisites and if there
is an inverse to each action.
Theorem 7 Given a planning problem (O; I ; G ) with the set of ground actions O satisfying

del(o)  pre(o) and pre(o)  s ) add(o) \ s = ; for all actions and reachable states s. If
there is an inverse action o 2 O for each action o 2 O, then the problem is invertible.

Proof: Let s be a reachable state, and let P O = ho1 ; : : : o i be a sequence of actions. We

need to show the existence of a sequence P O

for which

n

Result(Result(s; P O ); P O ) = s
358

(  )

On Reasonable and Forced Goal Orderings
holds. We dene P O := ho ; : : : ; o1 i, and prove (  ) by induction over n.
O
n = 0: Here, we have P O = P = hi, and Result(Result(s; hi); hi) = s is obvious.
n ! n + 1: Now P O = ho1 ; : : : ; o ; o +1 i. From the induction hypothesis we know that
Result(Result(s; ho1 ; : : : ; o i); ho ; : : : ; o1 i) = s. To make the following a bit more readable,
let s0 denote s0 := Result(s; ho1 ; : : : ; o i). We have
n

n

n

n

n

n

Result(Result(s; ho1 ; : : : ; o +1 i); ho +1 ; : : : ; o1 i)
Result(Result(s0 ; ho +1 i); ho +1 ; : : : ; o1 i)
Result(Result(Result(s0 ; ho +1 i); ho +1 i); ho ; : : : ; o1 i)
Result(s0 ; ho ; : : : ; o1 i)
s
n

n

=
=
=
=

n

n

n

n

n

n

(cf. Lemma 6 on s0 and o +1 )
(per induction)
n

Altogether, we know now that invertible problems, if solvable, do not contain forced
orderings. We also know that problems, where there is an inverse action to each action in
O, are invertible following Theorem 7. Theorem 7 requires del(o)  pre(o) to hold for each
action o, and pre(o)  s ) add(o) \ s = ; to hold for all actions and reachable states s.
We will see that all conditions, (a) inclusion of the delete list in the precondition list, (b)
empty intersection of an action's add list with reachable states where it is applicable, and
(c) existence of inverse actions, hold in most currently used benchmark domains.4
Concerning the condition (a) that actions only delete facts they require as preconditions, one nds this phenomenon in all domains that are commonly used by the planning
community, at least in those that are known to the authors. It is just something that seems
to hold in any reasonable logical problem formulation. Some authors even postulate it as
an assumption for their algorithms to work, cf. (Fox & Long, 1998).
Similarly in the case of conditions (b) and (c): One usually nds inverse actions in
benchmark domains. Also, an action's preconditions usually imply|by state invariants|
that its add eects are all false. For example in the blocks world, stack and unstack
actions invert each other, and an action's add eects are exclusive of its preconditions|
the former are contained in the union of the False constructed for the preconditions, see
Section 3.1. Similarly in domains that deal with logistics problems, for example logistics,
trains, ferry, gripper etc., one can often nd inverse pairs of actions with their preconditions
always excluding the add eects. Sometimes, two dierent ground instances of the same
operator schema yield an inverse pair. For example, in gripper, the two ground instances
move(roomA, roomB)
at-robby(roomA)

! ADD at-robby(roomB) DEL at-robby(roomA).

and
4. In order to avoid reasoning about reachable states in condition (b), one could also postulate that an
action has all of its add eects as negative preconditions, cf. (Jonsson, Haslum, & Backstrom, 2000).
This is, however, not commonly used in the typical planning benchmark problems.

359

Koehler & Hoffmann
move(roomB, roomA)
at-robby(roomB)

! ADD at-robby(roomA) DEL at-robby(roomB).

of the move(?from,?to) operator schema invert each other. Similarly, in towers of hanoi,
where there is only the single move operator schema, an inverse instance can be found
for each ground instance of the schema, and the add eects are always false when the
preconditions are true.
Only very rarely, non-invertible actions can be found in benchmark domains. If they
occur, their role in the domain is often quite limited as for example the operators cuss and
inate in Russel's Tyreworld.
cuss

! DEL annoyed().

inate(?x:wheel)

have(pump) not-inated(?x) intact(?x)

! ADD inated(?x) DEL not-inated(?x).

Obviously, there is not much point in dening something like a decuss or a deate
operator. More formally speaking, none of the ground actions to these operators destroys
a goal or a precondition of any other action in the domain. Therefore, it does not matter
that their eects cannot be inverted. In particular, no forced goal ordering can be derived
wrt. these actions. 5
The importance of inverse actions in real-world domains has also been discussed by
Nayak and Williams (1997), who describe the planner BURTON controlling the Cassini
spacecraft. In contrast to these domains, problems such as those for example used by
Barrett et al. in (1994) almost never contain inverse actions. Consequently, in these domains
plenty of forced goal orderings could be discovered and used by a planner to avoid deadlock
situations. The widespread, although perhaps unconscious use of invertible problems for
benchmarking is a current phenomenon related to STRIPS descending planning systems. As
one of the anonymous reviewers pointed out to us, quite a number of non-invertible planning
problems have also been proposed in the planning literature, e.g., the register assignment
problem (Nilsson, 1980), the robot crossing a road problem (Sanborn & Hendler, 1988), some
instances of manufacturing problems (Regli, Gupta, & Nau, 1995), and the Yale Shooting
problem (McDermott & Hanks, 1987). For these problems, i.e., for problems that are not
invertible, one could|in the spirit of argument 1 at the very beginning of this section|
simply use  and  to approximate forced orderings if one is interested in nding at least
those. More precisely,  and  are methods that might detect forced orderings|as those
are also reasonable|but that might also nd more, not necessarily forced, orderings. If
one is not interested in nding only the forced orderings, this is a possible way to go. For
example, in a simple blocks world modication where blocks cannot be unstacked anymore
once they are stacked|which forces the planner to build the stacks bottom up|both 
and  are still capable of nding the correct goal orderings.
e

h

e

h

e

h

5. The cuss operator, by the way, is the only one known to the authors that deletes a fact it is not using
as a precondition. It is also the only one we know that could be removed from the domain description
without changing anything.

360

On Reasonable and Forced Goal Orderings
3.4 An Extension of Goal Orderings to ADL Actions

The orderings, which have been introduced so far, can be easily extended to deal with
ground ADL actions having conditional eects and using negation instead of delete lists.
Such actions have the following syntactic structure:
o : 0 (o) = pre0 (o) ! e+
0 (o); e0 (o)
1 (o) = pre1 (o) ! e+
1 (o); e1 (o)
..
.
 (o) = pre (o) ! e+ (o); e (o)
All unconditional elements of the action are summarized in 0 (o): The precondition
of the action is denoted with pre0 (o), and its unconditional positive and negative eects
with e+0 (o) and e0 (o), respectively. Each conditional eect  (o) consists of an eect
condition (antecedent) pre (o), and the positive and negative eects e+ (o) and e (o).
Additionally, we denote with (o) the set of all unconditional and conditional eects,
i.e., (o) = f0 (o); 1 (o); : : : ;  (o)g.
The computation of  immediately carries over to ADL actions when an extension of
planning graphs is used, which can handle conditional eects, e.g., IPP (Koehler, Nebel,
Homann, & Dimopoulos, 1997) or SGP (Anderson & Weld, 1998). One simply takes the
set of exclusive facts that is returned by these systems to determine the set F . The test
from Denition 10, which decides whether there is an ordering B  A of two atomic goals
A and B , is extended to ADL as follows.
n

n

n

n

i

i

i

i

n

e

A

GP

e

Denition 17 (Ordering  for ADL) Let (O; I ; G  fA; B g) be a planning problem.
e

be the False set for A. The ordering B  A holds if and only if

Let F

A

e

GP

8 o 2 O;  (o) 2 (o) : B 2 e+(o) ^ A 62 D (o) ) (pre (o) [ pre0(o)) \ F 6= ;
i

i

i

A
GP

i

Here, D (o) denotes all negative eects that are implied by the conditions of  (o).
i

D (o) :=
i



e0 (o) [
e0 (o)

i

S

pre

j (o)  prei (o) ej (o) i 6= 0

i=0

Thus, B is ordered before A if all (unconditional or conditional) eects that add B either
imply an eect that deletes A, or need conditions that cannot be made true together with
A. Note that an eect  requires all the conditions in pre (o) [ pre0 (o) to be satised,
which is impossible in any state where A holds because of the non-empty intersection with
F
.
The computation of  requires a little more adaptation eort. In order to obtain the
set F , we now need to investigate the conditional eects as well. For each action that
has A as a conditional or unconditional eect, we determine which atoms are negated by
it, no matter which eect is used to achieve A. We obtain these atoms by intersecting the
appropriate sets D (o).
\
D (o)
D(o) :=
i

i

A
GP

h

A
DA

i

+
A 2 ei (o)

361

i

Koehler & Hoffmann
These are exactly the facts that are always deleted by o when achieving A, no matter which
eect we use.
The intersection of the sets D(o) for all actions o yields the desired set F . Let us
consider the following small example to clarify the computation.
0 (o) = fU g
! fW g f:X g;
1 (o) = fV; W g ! fAg f:X g;
2 (o) = fW g
! fU g f:Y g
A
DA

We obtain D1 (o) = f:X g [ f:Y g = f:X; :Y g, because the precondition of 2 (o) is
implied by the rst conditional eect 1 (o). As 1 (o) is the only eect that can achieve A,
we get D(o) = D1 (o) = f:X; :Y g.
We obtain a smaller set D(o), if we add A as an unconditional positive eect of the
action.
0 (o) = fU g
! fW; Ag f:X g;
1 (o) = fV; W g ! fAg f:X g;
2 (o) = fW g
! fU g f:Y g
In this case, we need to intersect the sets D0 (o) = f:X g and D1 (o) = f:X; :Y g,
yielding D(o) = f:X g. This reects the fact that, when achieving A via the unconditional
eect of o, only X gets removed from the state.
The xpoint computation requires to adapt the computation of O . First, we repeat the
same steps as in the case of simple STRIPS actions and consider the unconditional negative
eects and the intersection of the preconditions with the False set:

O := O n fo j A 2 e0 (o) _ F \ pre0(o) 6= ;g
A
DA

Then, we additionally remove from each action the conditional eects that either imply the
deletion of A or have an impossible eect condition.

O := red(O ) = fred(o)jo 2 Og
Here, red is a function red(o) : o 7! o0 such that
(o0 ) = (o) n f (o) j A 2 D (o) _ pre (o) \ F
k

k

A
DA

k

6= ;g

Finally, we need to redene Denition 12, which expresses the conditions under which a
fact is believed to be possibly achievable given a certain set of operators O.
Denition 18 (Possibly Achievable Atoms for ADL) An atom p is possibly achievable given an action set O (written pA(p; O)) if and only if

9 o 2 O;  2 (o) : p 2 e+(o) ^
8 p0 2 (pre (o) [ pre0(o)) : 9 o0 2 O;  0 2 (o0) : p0 2 e+0 (o0 )
i

i

i

i

i

holds, i.e., there is a positive eect for p and all of its conditions and preconditions can be
made true by other eects in the reduced action set.
362

On Reasonable and Forced Goal Orderings
The process, which decides whether an atomic goal B is heuristically ordered before another
goal A (i.e., whether B  A holds) proceeds in exactly the same way as described in
Section 3.2: The False set F for A is reduced by the xpoint computation, which remains
unchanged, but employs the updated routines for computing O and for deciding pA(f; O).
As a result, B is ordered before A (B  A) if and only if it is not possibly achievable
pA(B; O ) using the action set that results from the xpoint.
h

A
DA

h

4. The Use of Goal Orderings During Planning

After having determined the ordering relations that hold between pairs of atomic goals
from a given goal set, the question is how to make use of them during planning. Several
proposals have been made in the literature, see Section 6 for a detailed discussion. In this
paper, we propose a novel approach that extracts an explicit ordering between subsets of
the goal set|called the goal agenda. The planner, in our case IPP, is then run successively
on the planning subproblems represented in the agenda.
4.1 The Goal Agenda

The rst step one has to take for computing the goal agenda is to perform a so-called goal
2 G of atomic goals must be examined in
order to nd out whether an ordering relation A  B , or B  A, or both, or none holds
between them. For the ordering relation , an arbitrary denition can be used. In our
experiments, the relation  was always either  or  .
After having determined all ordering relations that hold between atomic goals, we want
to split the goal set into smaller sets based on these relations, and we want to order the
smaller sets, also based on these relations. More precisely, our goal is to have a sequence of
goal sets G1 ; : : : ; G with
[
G =G
analysis. During goal analysis, each pair A; B

e

h

n

n

i

and

i

=1

G

i

\G =;
j

for i 6= j; 1  i; j  n. We also want the sequence of goal sets to respect the ordering
relations that have been derived between atomic goals. To make this explicit, we rst
introduce a simple representation for the detected atomic orderings: the goal graph G.
G := (V; E )
where
V := G
and
E := f(A; B ) 2 G  G j A  B g
Now, the desired properties, which the sequence of goal sets should possess, can be easily
stated:
363

Koehler & Hoffmann

 Goals A; B that lie on a cycle in G belong to the same set, i.e., A; B 2 G .
 If G contains a path from a goal A to a goal B , but not vice versa, then A is ordered
before B , i.e., A 2 G and B 2 G with i < j .
i

i

j

These are the only properties that appear to be reasonable for a goal-set sequence respecting
the atomic orderings. We will now introduce a simple algorithmic method that does produce
a sequence of goal sets which meets these requirements.
First of all, the transitive closure of G is computed. This can be done in at most cubic
time in the size of the goal set (Warshall, 1962). Then, for each node A in the transitive
closure, the ingoing edges A and outgoing edges A are counted. All disconnected nodes
with A = A = 0 are moved into a separate set of goals G-sep containing now those
atomic goals, which do not participate in a  relation. For all other nodes A, their degree
d(A) = A
A is determined as the dierence between the number of ingoing edges and
the number of outgoing edges. Nodes with identical degree are merged into one set. The
sets are then ordered by increasing degree and yield our desired sequence of goal sets. The
only problem remaining is the set G-sep. If it is non-empty, it is not clear in which place
to put it.
Let us consider a small example of the process. Figure 3 depicts on the left the goal
graph, which results from the goal set G = fA; B; C; D; E g and the ordering relations
A  B; B  C and B  D, and its transitive closure on the right.
in

in

out

out

in

out

B

B
C

C
A

A
D

D
E

E

Figure 3: On the left, the goal graph depicting the  relations between the atomic subgoals.
On the right, the transitive closure of this graph.
In Figure 4, the number of in- and outgoing edges of each goal, the corresponding degrees,
and resulting goal-set sequence are shown.
0
0

E

A

0
3

1

2

B2

0

2
0

-3
{A}

C

-1
{B}

2
{C,D}

G-sep
E

D

Figure 4: On the left, the number of in- and outgoing edges for each node. On the right,
the degree of the nodes and the merged sets of goals having same degree. The
node E becomes a member of the G-sep set and remains unordered.
It is not diÆcult to verify that the resulting goal sequence respects the atomic goal orderings:
364

On Reasonable and Forced Goal Orderings

 Nodes occurring on a cycle in a graph have isomorphic in- and outgoing edges in the

transitive closure of that graph. In particular, they have the same degree and get
merged into the same set G .
 Say we have a graph, where there is a path from A to B , but not vice versa. Then,
in the transitive closure of that graph, we will have an edge from A to each node
that B has a path to, and additionally the edge from A to B , i.e., A > B
follows. Similarly, we have an ingoing edge to B for each node that has a path to
A, and additionally, the edge from A to B , which gives us B > A . Altogether,
d(A) = A
A <B
A <B
B = d(B ) and thus, the degree of A is
smaller than the degree of B and as required, A gets ordered before B .
Note that nothing is said in this argumentation about the set of unordered goals, Gsep. This set could, in principle, be inserted anywhere in the sequence with the resulting
sequence still respecting the atomic orderings. A possible heuristic may use this goal set as
the rst in the sequence, because apparently there is no problem to reach all other goals
after the goals in this set have been achieved. Another heuristic could put this set at the end
as there is neither a problem to reach this goal set from all other goals. We have decided to
deal with the problem in a more sophisticated way by trying to derive an ordering relation
between G-sep and the other goal sets G that have already been derived. In order to do
so, we need to extend our denitions of goal orderings to sets of goals.
i

out

in

in

out

in

out

in

out

in

out

i

4.2 Extension of Goal Orderings to Goal Sets

Given a set of atomic goals, it has always been a problem which of the exponentially many
subsets should be compared with each other in order to derive a reasonable goal ordering
between goal sets. A consideration of all possible subsets is out of question, because it will
result in an exponential overhead. The partial goal agenda that we have obtained so far
oers one possible answer. It suggests taking the set G-sep and trying to order it with
respect to the goal sets emerging from the goal graph.
Given a planning problem (O; I; G ) and two subsets of atomic goals fA1 ; : : : ; A g  G
and fB1 ; : : : ; B g  G , the denition of  and  for sets of atomic goals is straightforward.
For the sake of simplicity, we consider only STRIPS actions here. The denitions can be
directly extended to ADL.
To dene an ordering  , which extends  to sets, we begin by dening a set F f 1 n g
of all atoms, which are exclusive of at least one atomic goal A in the planning graph
generated for (O; I; G ):
F f 1 n g := fp j p is exclusive of at least one A when the graph has leveled o g
The set Of 1 n g is obtained accordingly by removing from O all actions that delete
at least one of the A , i.e., Of 1 n g = fo 2 O j 8 i 2 f1; : : : ; ng : A 62 del(o)g.
Denition 19 (Ordering  over Goal Sets) Let (O; I; G ) be a planning problem with
n

e

k

h

A ;:::;A

E

e

GP

i

A ;:::;A

i

GP

A ;:::;A

i

i

A ;:::;A

fA1 ; : : : ; A g  G and fB1 ; : : : ; B g  G . Let Ff 1 ng be the False set for fA1 ; : : : ; A g.
The ordering fB1 ; : : : ; B g  fA1 ; : : : A g holds if and only if
9 j 2 f1; : : : ; kg : 8 o 2 Of 1 ng : B 2 add(o) ) pre(o) \ Ff 1 ng 6= ;:
E

A ;:::;A

n

k

k

E

n

GP

n

A ;:::;A

A ;:::;A

j

365

GP

Koehler & Hoffmann
In a similar way,  can be extended to  . For each A , the sets F i are determined
based on Equation (3). The set Ff 1 n g is simply the union over the individual sets:
f 1 n g := [ F i
F
(4)
A

H

h

i

DA

A ;:::;A

DA

A ;:::;A

A

DA

DA

i

Then the xpoint computation is entered with

O := O n fo 2 O j 9 i 2 f1; : : : ; ng : A 2 del(o) _ Ff 1 ng \ pre(o) 6= ;g (5)
The recomputation of O in each iteration of the xpoint algorithm from Figure 1 is done
A ;:::;A

i

DA

accordingly. Apart from this, the algorithm remains unchanged.

Denition 20 (Ordering  ) Let (O; I; G ) be a planning problem with fA1 ; : : : ; A g 

and fB1 ; : : : ; B g  G . Let O be the set of actions that is obtained by performing
the xpoint computation shown in Figure 1, modied to handle sets of facts as dened in
Equations (4) and (5). The ordering fB1 ; : : : ; B g  fA1 ; : : : ; A g holds if and only if
9 j 2 f1; : : : ; kg : :pA(B ; O)

G

H

n

k

H

k

n

j

All given goal sets then undergo goal analysis, i.e., each pair of sets is checked for an
ordering relation  or  . Each derived relation denes an edge in a graph with the
subgoal sets as nodes. The transitive closure is determined as before, and the degree of
each node is computed. If the graph contains no disconnected nodes, then a total ordering
over subsets of goals results by ordering the nodes based on their degree. This ordering
denes the goal agenda. In the case of disconnected nodes, we default to the heuristic of
adding the corresponding goals to the last goal set in the agenda.
E

H

4.3 The Agenda-Driven Planning Algorithm

Given a planning problem (O; I ; G ), let us assume that a goal agenda G1 ; G2 ; : : : ; G with
k entries has been returned by the analysis. Each entry contains a subset G  G . The
basic idea for the agenda-driven planning algorithm is now to rst feed the planner with
the original initial state I1 := I and the goals G1 := G1 , then execute the solution plan P
in I , yielding the new initial state I2 = Result(I1; P ). Then, a new planning problem is
initialized as (O; I2 ; G2 ). After solving this problem, we want the goals in G2 to be true,
but we also want the goals in G1 to remain true, so we set G2 := G1 [ G2 . The continuous
merging of successive entries from the agenda yields a sequence of incrementally growing
goal sets for the planner, namely
[
G := G
k

i

i

i

j

j

=1

In a little more detail, the agenda-driven planning algorithm we implemented for IPP works
as follows. First, IPP is called on the problem (O; I ; G1 ) and returns the plan P1 , which
achieves the subgoal set G1 . P1 is a sequence of parallel sets of actions, which is returned
by IPP similarly to graphplan. Given this plan, the resulting state R(I ; P1 ) = I2 is
366

On Reasonable and Forced Goal Orderings
computed based on the operational semantics of the planning actions.6 In the case of a set
of STRIPS actions, one simply adds all ADD eects to and deletes all DEL eects from
a state description in order to obtain the resulting state, following the Result function in
Denition 2. For STRIPS, the Result function coincides directly with the R function. In
the case of a set of parallel ADL actions, one needs to consider all possible linearizations
of the parallel action set and has to deal with the conditional eects separately. For each
linearization, a dierent resulting state can be obtained, but each of them will satisfy the
goals. To obtain the new initial state I2 , one takes the intersection of the resulting states
for each possible linearization of the actions in a parallel set. This means to compute n!
linearizations for a parallel action set of n actions in each time step. Since n is usually
small (more than 5 or 6 ADL actions per time step are very rare), the practical costs for
this computation are neglectible.
This way, given a solution to a subproblem (O; I ; G ), one calculates the new initial
state I +1 and runs the planner on the subsequent planning problem (O; I +1 ; G +1 ) until
the planning problem (O; I ; G ) is solved.
The plan solving the original planning problem (O; I ; G ) is obtained by taking the
sequence of subplans P1 ; P2 ; : : : ; P . One could argue that planning for increasing goal
sets can lead to highly non-optimal plans. But IPP still uses the \no-ops rst" strategy to
achieve goals, which was originally introduced in the graphplan system (Blum & Furst,
1997). Employing this strategy, the graphplan algorithm, in short, rst tries to achieve
goals by simply keeping them true, if possible. Since all goals G1 ; G2 ; : : : ; G are already
satised in the initial state I +1 , starting from which the planner tries to achieve G +1 , this
strategy ensures that these goals are only destroyed and re-established if no solution can
be found otherwise. The no-ops rst strategy is merely a graphplan feature, but any
reasonable planning strategy should preserve goals that are already true in the initial state
whenever possible.
The soundness of the agenda-driven planning algorithm is obvious because G = G and
we have a sequence of sound subplans yielding a state transition from the initial state I to
a state satisfying G .
The completeness of the approach is less obvious and holds only if the planner cannot
make wrong decisions before nally reaching the goals. More precisely, the approach is
complete on problems that do not contain deadlocks as they were introduced in Denition 14.
i

i

i

i

k

i

k

k

i

i

i

k

Theorem 8 Given a solvable planning problem (O; I; G ), and a goal agenda G1 ; G2 ; : : : G

k

with G  G +1 and G = G . Running any complete planner in the agenda-driven manner
described above will yield a solution if the problem is deadlock-free.
i

i

k

Proof: Let us assume the planner does not nd a solution in step i of the agenda-driven

algorithm, i.e., no solution is found for the subproblem (O; I ; G ). As the planner is assumed
to be complete on each subproblem, this implies unsolvability of (O; I ; G ). If this problem
is not solvable, then neither is the problem (O; I ; G ) solvable, since G  G holds. Therefore,
the goals cannot be reached from I . Furthermore, I is a reachable state|it was reached
by executing the partial solution plans P1 ; : : : ; P 1 in the initial state. Consequently, I
must be a deadlock state in the sense of Denition 14, which is a contradiction.
i

i

i

i

i

i

i

i

i

6. See (Koehler et al., 1997) for the exact denition of R, which we do not want to repeat here.

367

i

Koehler & Hoffmann
This result states the feasibility of our approach: As we have shown, most benchmark
problems that are currently investigated do contain inverse actions, are therefore invertible
(Theorem 7), and are with that also deadlock-free (Theorem 6). Thus, with Theorem 8,
our approach preserves completeness in these domains.
However in the general case, completeness cannot be guaranteed. The following example
illustrates a situation where the assumption s( : ) 6j= p (assuming that preconditions of
achieving actions are not contained in the state where A is reached, cf. the derivation of the
ordering  in Section 3) is wrong and yields a goal ordering under which no plan can be
found anymore although the problem is solvable.
Given the initial state fC; Dg and the goals fA; B g, the planner has the following set
of ground STRIPS actions :
A;

B

h

op1:
op2:
op3:
op4:

fC g
fDg
fE g
fF g

!
!
!
!

ADD fB g DEL fDg
ADD fE g
ADD fF g
ADD fAg

The analysis will return an ordering B  A because B is only added by op1, but its
precondition C is not an eect of any of the other actions. Thus it concludes that C is
not reachable from a state in which A holds. But in this example, C holds in all reachable
states. The assumption s( : ) 6j= C as made by the test pA(B; O ) is wrong. Thus, B
can be reached after A. On the other hand, A  B holds, we even have a forced ordering
A  B . But when testing for A  B , this ordering remains undetected, because our
method does not discover that the precondition F of op4 is not achievable from the state
in which B holds: we obtain F = fDg, which excludes op2 from O , but op3 and op4
remain in the set of usable actions. Thus, op4 is considered a legal achiever of A, and op3
is considered a legal achiever for its precondition F . We could only detect the right ordering
if we regressed over the action chain op4, op3, op2 and found out that, with D being in
the F set of B , all these actions must be excluded from O .
Consequently, the goal agenda fB g; fAg is fed into the planner, which solves the rst
subproblem using op1, but then fails in achieving A from the state fB; C g since there is
no inverse action to op1 and D cannot be re-established in any other way.
h

A;

B

r

f

h

B
DA

5. Empirical Results

We implemented both methods to approximate  as a so-called Goal Agenda Manager
(GAM) for the IPP planning system (Koehler et al., 1997). GAM is activated after the
set of ground actions has been determined and either uses  or  to approximate the
reasonable goal ordering. Then it calls the IPP planning algorithm on each entry from the
goal agenda and outputs the solution plan as the concatenation of the solution plans that
have been found for each entry in the agenda.7
r

e

h

7. The source code of GAM, which is based on IPP 3.3, and the collection of domains from which
we draw the subsequent examples can be downloaded from http://www.informatik.uni-freiburg.de/~
koehler/ipp/gam.html. All experiments have been performed on a SPARC 1/170.

368

On Reasonable and Forced Goal Orderings
The empirical evaluation that we performed uses the IPP domain collection, which contains 48 domains with more than 500 planning problems. Out of these domains, we were
able to derive goal ordering information in 10 domains. These domains indeed pose constraints on the ordering in which a planner has to a achieve a set of goals. In all other
domains, where no goal orderings could be derived, we found that either only a single goal
has to be achieved, for example in the manhattan, movie, molgen, and montlake domains
or the goals can be achieved in any order, as for example in the logistics, gripper, and ferry
domains. We found no benchmark domain, in which a natural goal ordering existed, but
our method failed to detect it. As a matter of fact, looking at a goal ordering that seems to
be natural, one usually nds that the ordering is reasonable in the sense of Denition 8, see
for example the blocks world, woodshop, and tyreworld domains. Our method nds almost
all of the reasonable orderings, which indicates that both approximation techniques  and
 are appropriate for detecting ordering information.
e

h

In the following, we will rst compare the  and  techniques in terms of runtime
and number of goal agenda entries generated. Then we take a closer look at the agendas
that are generated in selected domains and investigate how they inuence the performance
of the IPP planning system. The exact denition of all domains can be downloaded from
the IPP webpage, we just give the name of the domain and the name of the particular
planning problem as well as the number of (ground) actions a domain contains, because
this parameter nicely characterizes the size of a domain and with that usually the diÆculty
to handle it.
e

h

In all examples, the times shown to compute the goal agenda contain the eort to
parse and instantiate the operators, i.e., to compute the set of actions. Times for parsing
and instantiation are not listed explicitly, because they are, on the test examples used here,
usually very close to zero and do not inuence the performance of the planner in a signicant
way.
5.1 Comparison of  and 
h

e

We begin our comparison with a summary of results that we obtained in dierent representational variants of the blocks world. The bw large a to bw large d examples originate from
the SATPLAN test suite (Kautz & Selman, 1996) to which we added the larger examples
bw large e to bw large g. The parcplan example comes from (El-Kholy & Richards, 1996)
and uses multiple grippers and limited space on the table. The stack n examples use the
graphplan blocks world representation and simply require to stack n blocks on each other,
which are all on the table in the initial state.
The two methods return exactly the same ordering relations across all blocks world
problems. But as Figure 5 conrms, the computation of  based on planning graphs is
much more time-consuming. It hits the computational border when a domain contains more
than 10000 actions. The computation of  is much faster and also scales to larger action
sets.
e

h

369

Koehler & Hoffmann
problem
bw large a
bw large b
bw large c
bw large d
bw large e
bw large f
bw large g
parcplan
stack 20
stack 40
stack 60
stack 80

#actions #agenda entries CPU( ) CPU( )
162
1
0.69
0.07
242
5
1.45
0.11
450
7
4.85
0.22
722
11
14.18
0.35
722
11
12.95
0.35
1250
6
44.93
0.58
1800
9
97.11
0.88
1960
4
25.84
1.47
800
19
6.91
0.36
3200
39
160.00
1.74
7200
59
840.42
4.85
12800
79
11.38
e

h

Figure 5: Comparison of  and  on blocks world problems. #actions shows the number
of actions in the set O, from which the planner tries to construct a plan. #agenda
entries says how many goal subsets have been detected and ordered by GAM.
Column 4 and 5 display the CPU time that is required by both methods to
compute the agenda when provided with the set O. A dash will always mean
that IPP ran out of memory on a 1 Gbyte machine.
e

h

Figure 6 and Figure 7 show the results for the other domains, in which our method
is able to detect reasonable orderings. Figure 6 lists the domains, in which both methods
return the same goal agendas. The tyreworld, hanoi, and fridgeworld domains originate from
UCPOP (Penberthy & Weld, 1992), while the link-repeat domain can be found in (Veloso
& Blythe, 1994). The performance results coincide with those shown in Figure 5. Figure 7
shows the same picture in terms of runtime performance, but in these domains dierent
agendas are returned by  and  .
The woodshop and scheduling domains contain actions with conditional eects, while
the other domains only use STRIPS operators. The computation of  fails to derive goal
orderings for all scheduling world problems (of which we only display the largest problem
sched6) and for the wood1 problem. The explanation for this behavior can be found in the
dierent treatment of conditional eects by both methods. IPP does only nd a very limited
form of mutex relations between conditional eects when building the planning graph. A
goal, which is achieved with a conditional eect, will not very often be exclusive to a large
number of other facts in the graph. Thus, the F sets are very small or sometimes even empty
and consequently, only very few actions can be excluded when performing the reachability
analysis and thus, reasonable orderings may remain undetected. Direct analysis investigates
the conditional eects in more detail and is therefore able to derive much larger F sets.
The behavior of the  method in the STRIPS domains bulldozer, glassworld, and
shopping world is caused by the same phenomenon. In these domains, one can derive much
larger F sets using planning graphs and in turn these sets exclude more actions. Since direct
analysis nds smaller or empty F sets, it also nds less  relations. The woodshop domain
e

h

e

h

h

370

On Reasonable and Forced Goal Orderings
#actions #agenda entries CPU( ) CPU( )
26
6
0.05
0.01
59
6
0.20
0.03
108
6
0.45
0.06
173
6
0.84
0.10
254
6
1.56
0.15
899
6
16.29
0.64
48
3
0.05
0.02
90
4
0.10
0.04
150
5
0.19
0.08
231
6
0.35
0.12
336
7
0.63
0.19
779
2
0.77
0.55
31
2
0.19
0.01
31
2
0.21
0.01

domain
tyreworld

problem
xit1
xit2
xit3
xit4
xit5
xit10
hanoi
hanoi3
hanoi4
hanoi5
hanoi6
hanoi7
fridgeworld fridge
link-repeat link10
link30

e

h

Figure 6: Comparison of  and  on those benchmark domains, in which they return
identical agendas.
e

h

domain
bulldozer
glassworld

problem
bull
glass1
glass2
glass3
shoppingworld shop
scheduling
sched6
woodshop
wood1
wood2
wood3

#actions #agenda entries CPU( ) CPU( )
61
2/1
0.09
0.03
26
2/1
0.02
0.01
114
2/1
0.19
0.09
122
2/1
0.22
0.09
81
2/1
0.07
0.02
104
1/4
01.0
0.12
15
1/3
0.03
0.01
15
6/5
0.03
0.01
43
6/5
0.14
0.06
e

h

Figure 7: Domains in which  and  return dierent goal agendas, which we give in the
form n1 =n2 . The number before the slash says how many entries are contained
in the agenda computed by  , the number following the slash says how many
entries are contained in the agenda computed by  . #agenda entries=1 means
that the agenda contains only a single entry, namely the original goal set, and no
ordering was derived.
e

h

e

h

shows that the results can dier within the same domain, but depending on the specic
planning problem. The problem wood2 varies from the problem wood1 in the sense that one
goal is slightly dierent|an object needs to be put into a dierent shape|and that two
more goals are present. While there are no goal orderings derived between pairs of the old
371

Koehler & Hoffmann
goals from wood1, lots of  relations are derived between mixed pairs of old and new goals
in wood2, yielding a detailed goal agenda. The problem wood3 contains additional objects
and many more goals, which can also be successfully ordered.
In the subsequent experiments, we decided to solely use the heuristic ordering  because
the computation of  is less costly than the computation of  in all cases, yielding
comparable agendas in most cases. In the three domains that we investigate more closely,
namely the blocks world, tyreworld and hanoi domains, the agendas derived by both methods
are, in fact, exactly the same.
e

h

e

h

5.2 Inuence of Goal Orderings on the Performance of IPP and Interaction
with RIFO

In this section, we analyze the inuence of the goal agenda on the performance of IPP
and combine it with another domain analysis method, called RIFO (Nebel, Dimopoulos, &
Koehler, 1997). RIFO is a family of heuristics that enables IPP to exclude irrelevant actions
and initial facts from a planning problem. It can be very eectively combined with GAM,
because if IPP plans for only a subset of goals from the original goal set, it is very likely
that also only a subset of the relevant actions is needed to nd a plan. More precisely, we
obtain one subproblem for each entry in the agenda, and, for each such subproblem, we
use RIFO for preprocessing before planning with IPP. In this conguration, GAM reduces
the search space for IPP by decreasing the number of subgoals the planner has to achieve
at each moment, while RIFO reduces the search space dramatically by selecting only those
actions that are relevant for this goal subset.
5.2.1 The Blocks World

Figure 8 illustrates the parcplan problem (El-Kholy & Richards, 1996) in detail. Seven
robot arms can be used to order 10 blocks into 3 stacks on 5 possible positions on the table.

1

11

14

24

23

13

23

12

22

32

11

21

31

1

2

3

32
12

31

22

24

14

13
21

2

3

4

5

Figure 8: The parcplan problem with limited space on the table, seven robot arms, and
several stacks.
The goal agenda derived by IPP orders the blocks into horizontal layers:
1:
2:
3:
4:

on-table(21, t2) ^ on-table(11, t1)
on-table(31, t3) ^ on(22, 21) ^ on(12, 11)
on(32, 31) ^ on(13, 12) ^ on(23, 22)
on(14, 13) ^ on(24, 23)
372

On Reasonable and Forced Goal Orderings
The optimal plan of 20 actions solving the problem is found by IPP using GAM in 14 s,
where it spends one second on computing the goal agenda, almost 13 seconds to build the
planning graphs, but only 0.01 second to search for a plan. Only 70 actions have to be tried
to nd the solution. Without the goal analysis, IPP needs approx. 47 s and searches 52893
actions in more than 26 seconds.
RIFO (Nebel et al., 1997) fails in detecting a subset of relevant actions when the original
goal set has to be considered, but it succeeds in selecting relevant actions for the subproblems
stated in the agenda. It reduces runtime down to less than 8 s with 1 s again spent on the
goal agenda, almost 6 s spent on the removal of irrelevant actions and initial facts, less
than 1 s spent on building the planning graphs. As previously, almost no time is spent on
planning.
Figure 9 shows IPP on the SATPLAN blocks world examples from (Kautz & Selman,
1996), the bw large.e example taken from (Dimopoulos, Nebel, & Koehler, 1997), and two
very large examples bw large.f (containing 25 blocks and requiring to build 6 stacks in the
goal state) and bw large.g with 30 blocks/8 stacks.
SATPLAN
bw large.a
bw large.b
bw large.c
bw large.d
bw large.e
bw large.f
bw large.g

# actions plan length IPP +G +G+R +G+R+L
162
12 (12)
0.70 0.74
0.58
0.34
242
22 (18) 26.71 0.86
0.55
0.52
450
48
- 7.34
2.42
2.58
722
54
- 11.62
3.74
3.81
722
52
- 11.14
3.99
3.97
1250
90
16.01
1800
84
- 117.56
28.71

Figure 9: Performance on the extended SATPLAN blocks world test suite. The second
column shows the number of ground actions in this domain, the third column
shows the plan length, i.e., the number of actions contained in the plan, generated
by GAM and in parentheses the plan length generated by IPP without GAM given
that IPP without GAM is able to solve the corresponding problem. +G means
that IPP is using GAM, +G+R means IPP uses GAM and RIFO, +G+R+L
means that subgoals from the same set in the agenda are arbitrarily linearized.
All runtimes cover the whole planning process starting with parsing the operator
and domain le, performing the GAM and RIFO analysis (if active), and then
searching the graph until a plan is found.
IPP 3.3 without GAM can only solve the bw large.a and bw large.b problems. Using a
goal agenda, some plans become slightly longer, but performance is increasing dramatically.
Plan length is growing because blocks are accidentally put in positions where they cut o
goals that are still ahead in the agenda and thus, additional actions need to be added to
the plan to remove these blocks from wrong positions. A further speed-up is possible when
RIFO is additionally used, because it reduces the size of planning graphs dramatically.
Finally, goals that belong to the same subset in the agenda can be linearized based on the

373

Koehler & Hoffmann
heuristic assumption that if the analysis found no reasonable goal orderings, then the goals
are achievable in any order. With this option, the problems are solved almost instantly.
The reader may wonder at this point why we use linearization of agenda entries only
as an extra option and do not investigate it further. There are two reasons for that. First,
linearization does have negative side eects in most domains that we investigated. For
example, it yields much longer plans in the logistics domain and all its variants. When
linearizing the single entry that the agenda for a logistics problem contains, all packages get
transported to their goal position one by one. Of course, this takes much more planning
steps than simultaneously transporting packages with coinciding destinations.
Secondly, the eects of linearization are somewhat unpredictible, even in domains where
it usually tends to yield good results. This is because GAM does not recognise all interactions between goals. Consider a blocks world problem with four blocks A, B , C and D.
Say B is positioned on C initially, the other blocks being each on the table, and the goal is
to have on(A; B ) and on(C; D). The agenda for this problem will comprise a single entry
containing both goals. In fact, there is no reasonable goal ordering here. Nevertheless,
stacking A onto B immedeatly is a bad idea, as the planner needs to move C to achieve
on(C; D). Being not aware of this, GAM might linearize the single agenda entry to have
on(A; B ) up front, which makes the problem harder than it actually is. Thus, the runtime
advantages that linearization sometimes yields on the blocks world can be more or less seen
as cases of \good luck".
Figure 10 shows IPP on the stack n problems. IPP without any domain analysis can
handle up to 12 blocks in less than 5 minutes, but for 13 blocks more than 15 minutes are
needed. Using GAM, 40 blocks can be stacked in less than 5 minutes. Using GAM and
RIFO, the 5 minutes limit is extended to 80 blocks, while stack100 is solved in 11.5 min
where 11.3 min are spent for both analysis methods and only 0.2 min are needed for building
the planning graphs and extracting a plan.
time
in s
600
450

IPP

IPP+G
IPP+G+R

300
150

10

20

Figure 10:

IPP

30

40

50

60

70

80

90

100 blocks

3.3 on a simple, but huge stacking problem.

Figure 11 shows the sharing of the overall problem-solving time between GAM, RIFO
and the IPP search algorithm on blocks world problems. Similar results are obtained in the
tyreworld. GAM takes between 3 and 16 %, RIFO takes between 75 and 96 %, and the
search eort is reduced down to approx. 1 %. The overall problem solving time is clearly
determined by RIFO, while the search eort becomes a marginal factor in the determination
of performance. This indicates that a further speed-up is possible when improving the
374

On Reasonable and Forced Goal Orderings
performance of GAM and RIFO. It also indicates that even the hardest planning problems
can become easy if they are structured and decomposed in the right way.
problem
stack 20
stack 40
stack 60
stack 80
parcplan

# actions
800
3200
7200
12800
1960

GAM
RIFO
0.31 = 16 % 1.44 = 75 %
1.57 = 7 % 18.77 = 90 %
4.40 = 4 % 93.10 = 94 %
9.60 = 3 % 283.60 = 96 %
0.86 = 12 % 5.52 = 76 %

search algorithm
0.13 = 7 %
0.51 = 2 %
1.15 = 1 %
2.33 = 1 %
0.83 = 11 %

Figure 11: Distribution of problem-solving time on blocks world examples between GAM,
RIFO, and the search algorithm, which comprises the time to build and search
the planning graph. The remaining fraction of total problem-solving time, which
is not shown in the table, is spent on parsing and instantiating the operators.

5.2.2 The Tyreworld

The tyreworld problem, originally formulated by Stuart Russell, asks a planner to nd out
how to replace a at tire. It is easily solved by IPP within a few milliseconds. The problem
becomes much harder if the number of at tires is increasing, cf. Figure 12.
Tires
1
2
3
4
5
6
7
8
9
10
Figure 12:

# actions
IPP
+G+R
+G+R+L
Search Space
26
0.10 (12/19) 0.15 (14/19) 0.16 (17/19)
1298/88
59
17.47 (18/30) 0.41 (24/32) 0.32 (30/34) 1290182/210
108
2.87 (32/44) 0.63 (41/46)
-/366
173
1.12 (52/60)
-/565
254
1.93 (63/73)
-/807
353
3.42 (73/85)
-/1092
464
4.81 (84/98)
-/1420
593
8.07 (95/121)
-/1791
738
11.27 (106/124)
-/ 2205
899
16.89 (118/136)
-/2662
in the Tyreworld. The numbers in parentheses show the time steps, followed
by the number of actions in the generated plan. The last column compares the
search spaces. The number before the slash shows the \number of actions tried"
parameter for the plain IPP planning algorithm, while the number following
the slash shows the \number of actions tried" for IPP using GAM, RIFO, and
the linearization of entries in the agenda. A dash means that the \number
of actions tried" is unknown because IPP failed in solving the corresponding
planning problem.

IPP

375

Koehler & Hoffmann
IPP is only able to solve the problem for 1 and 2 tires. Using GAM and RIFO, 3
tires can be handled. Solution length under GAM is slightly increasing, which is caused
by superuous jack-up and jack-down actions. In short, this is explained as follows. Each
wheel needs to be mounted on its hub, which is expressed by an on(?r, ?h) goal. To mount
a wheel, its hub must be jacked up. After mounting, the nuts are done up. Then, the hub
needs to be jacked down again, in order to tighten the nuts achieving a tight(?n, ?h) goal.
Now, GAM puts all of the on goals into one entry preceeding the tight goals. Thus, solving
the entry containing the on goals, each hub is jacked up, the wheel is put on, and the hub
is immediatly jacked down again in order to replace the next wheel. Afterwards, solving
the tight goals, each hub must be jacked up|and down|one more time for doing up the
nuts. Solving the problem in this manner, the planner inserts one superuous jack-up, and
one superuous jack-down action for each wheel. More precisely, superuous actions are
inserted for all but one wheel, namely the wheel that is last mounted when solving the on
goals. After mounting this wheel, all on goals are achieved, and the planner proceeds to
the next agenda entry with this wheel still being jacked up. Then, trying to achieve the
tight goals, IPP recognizes that the shortest plan (in terms of the number of parallel steps)
results when the nuts are rst done up on the hub that is already jacked up. Thus, this hub
is only jacked up one time, achieving the corresponding on goal, and jacked down again one
time, before achieving its tight goal.
In the case of 3 tires, the following goal subsets are identied and ordered:

1:
2:
3:
4:
5:
6:
7:

inated(r3), inated(r2), inated(r1)
on(r3, hub3), on(r1, hub1), on(r2, hub2)
tight(n2, hub2), tight(n3, hub3), tight(n1, hub1)
in(w3, boot), in(pump, boot), in(w1, boot), in(w2, boot)
in(jack, boot)
in(wrench, boot)
closed(boot)

The hardest subproblem in the agenda is to achieve the on(r ; hub ) goals in entry 2,
i.e., to mount inated spare wheels on the various hubs. Trying to generate a maximum parallelized plan is impossible for IPP for more than 3 tires. But since the goals are completely
independent of each other, any linearization of them will perfectly work. The resulting
plans become slightly longer due to the way that the tight goals are achieved when using
the -L option. We noticed earlier that for one wheel (the one that is last mounted when
solving the on goals) no superuous jack-up and jack-down actions need to be inserted into
the plan. Linearizing the agenda entries, superuous jack-up and jack-down actions must
most likely be inserted for all wheels, yielding plans that are two steps longer. The reason
for that is that any tight goal might be the rst in the linearization. Most likely, this is
not the tight goal corresponding to the hub that is still jacked up, so the planner needs to
insert one superuous jack-down action here. Later, it must jack up this hub again, yielding
another superuous action. Using +G+R+L in the case of 10 tires, only 2662 actions need
to be tried until a plan of 136 actions is found, which takes 0.08 s. GAM requires 0.55 s,
RIFO requires 14.42 s, 1.74 s are consumed to generate the planning graphs, and 0.08 s are
spent to compute the initial states for all subproblems. The remaining 0.02 s are consumed
for parsing and instantiating.
i

376

i

On Reasonable and Forced Goal Orderings
5.2.3 The Tower of Hanoi

A surprising result is obtained in the tower of hanoi domain. In this domain, a stack of discs
has to be moved from one peg to a third peg with an auxiliary second peg between them,
but never a larger disc can be put onto a smaller disc. In the case of three discs d1, d2, d3
of increasing size, the goals are stated as on(d3,peg3), on(d2,d3), on(d1,d2). GAM returns
the following agenda, which correctly reects the ordering that the largest disc needs to be
put in its goal position rst.
1: on(d3,peg3)
2: on(d2,d3)
3: on(d1,d2)

The goal agenda leads to a partition into subproblems that corresponds to the recursive
formulation of the problem solving algorithm, i.e., to solve the problem for n discs, the
planner rst has to solve the problem for n 1 discs, etc. For the rst entry, a plan of 4
actions (time steps 0 to 3 below) is generated, which achieves the goal on(d3,peg3).8 Then
a plan of 2 actions (time steps 4 and 5) achieves the goals on(d3,peg3) and on(d2,d3) with
on(d3,peg3) holding already in the initial state. Finally, a one-step plan (time step 6) is
generated that moves the third disc with the other two discs being already in the goal
position.
time
time
time
time

step
step
step
step

0:
1:
2:
3:

move(d1,d2,peg3)
move(d2,d3,peg2)
move(d1,peg3,d2)
move(d3,peg1,peg3)

time step 4: move(d1,d2,peg1)
time step 5: move(d2,peg2,d3)
time step 6: move(d1,peg1,d2)

Surprisingly, IPP is not able to benet from this information, but runtime of IPP using
GAM is exploding dramatically for increasing numbers of discs, see Figure 13.
discs #actions IPP IPP +G
UCPOP
UCPOP on subproblems
2
21 0.02
0.02 0.12 (27)
0.06 (17) + 0.02 (6)
3
48 0.08
0.07 8.00 (2291) 0.18 (48) + 0.06 (13) + 0.01 (6)
4
90 0.33
0.25
5
150 1.57
3.10
6
231 9.71
88.45
7
336 69.44 2339.94
Figure 13: Runtimes of IPP with and without the goal agenda on hanoi problems compared to UCPOP without agenda and UCPOP on the agenda subproblems using
ZLIFO and the ibf control strategy.
8. A move action takes as rst argument the disc to be moved, as second the disc from which it is moved,
and as third argument the disc or peg to which it is moved.

377

Koehler & Hoffmann
We are not able to provide an explanation for this phenomenon, but the division into
subproblems causes a much larger search space for the planner although the same solution
plans result. RIFO cannot improve on the situation because it selects all actions as relevant.
The tower of hanoi domain is the only one we found where IPP's performance is deteriorated by GAM. We do currently not see a way of how one can tell in advance whether IPP
will gain an advantage from using GAM or not. The overhead caused by the goal analysis
itself is very small, but an \inadequate" split of the goals into subgoal sets can lead to more
search, see also Section 6.
However in this case, the phenomenon seems to be specic to IPP. We simulated the
information that is provided by GAM in UCPOP and obtained a quite dierent picture.
The fth column in Figure 13 shows the runtime of UCPOP using ZLIFO (Pollack, Joslin,
& Paolucci, 1997) and the ibf control strategy with the number of explored partial plans
in parentheses. UCPOP can only solve the problem for 2 and 3 discs. In the last column
of the gure, we show the runtime and number of explored partial plans, which result
when UCPOP is run on the subproblems that result from the agenda. These are exactly
the same subproblems which IPP has to solve, but the performance of UCPOP improves
signicantly. Instead of taking 8 s and exploring 2291 partial plans, UCPOP only takes
0.18+0.06+0.01=0.25 s and explores only 48+13+6=67 plans. Unfortunately, any problems
or subproblems with more than 3 discs remain beyond the performance of UCPOP. The
performance improvement is independent of the search strategies used by UCPOP. For
example, if ibf control is used without ZLIFO, the number of explored partial plans is
reduced from 78606 down to 2209 in the case of the problem with 3 discs. Runtime improves
from 65 seconds down to 2 seconds. Similarly, when using bf control without ZLIFO the
number of explored partial plans reduces from 1554 down to 873.
Knoblock (1994) also reports an improvement in performance for the Prodigy planner
(Fink & Veloso, 1994) when it is using the abstraction hierarchy generated for this domain
by the alpine module, which provides in essence the same information as the goal agenda.9
6. Summary and Comparison to Related Work

Many related approaches have been developed to provide a planner with the ability to
decompose a planning problem by giving it any kind of goal ordering information. Subsequently, we discuss the most important of them and review our own work in the light of
these approaches.
Our method introduces a preprocessing approach, which derives a total ordering for
subsets of goals by performing a static, heuristic analysis of the planning problem at hand.
The approach works for domains described with STRIPS or ADL operators and is based
on polynomial-time algorithms. The purpose of this method is to provide a planner with
search control, i.e., we opt at deriving a goal achievement order and then successively call
the planner on the totally ordered subsets of goals.
The method preserves the soundness of the planning system, but the completeness
only in the case that the planning domain does not contain deadlocks. We argue that
9. However, to nd that goal ordering information, alpine requires to represent the tower of hanoi domain
involving several operators, cf. (Knoblock, 1991).

378

On Reasonable and Forced Goal Orderings
benchmark domains quite often possess this property, which is also supported by other
authors (Williams & Nayak, 1997).
The computation of  and  requires only polynomial time, but both methods are
incomplete in the sense that they will not detect all reasonable goal orderings in the general
case. The complexity of deciding on the existence of forced and reasonable goal orderings
has been proven to be PSPACE-hard in Section 2 and therefore, trading completeness for
eÆciency seems to be an acceptable solution. Our complexity results relate to those found
by Bylander (1992) who proves the PSPACE-completeness of serial decomposability (Korf,
1987). Given a set of subgoals, serial decomposability means that previously satised subgoals do not need to be violated later in the solution path, i.e., once a subgoal has been
achieved, it remains valid until the goal is reached. The purpose of our method is to derive
constraints that make those orderings explicit under which no serial decomposability of a set
of goals can be found, i.e., we consider the complementary problem, which is also reected
in our complexity proofs.
In many cases, we found that the goal agenda manager can signicantly improve the
performance of the IPP planning system, but we found at least one domain, namely the
tower of hanoi, where a dramatic decrease in performance can be observed although IPP
still generates the optimal plan when processing the ordered goals from the agenda. So
far, the complexity results of Backstrom and Jonsson (1995) predicted that planning with
abstraction hierarchies can be exponentially less eÆcient, but because exponentially longer
plans can be generated.
The idea to analyze the eects and preconditions of operators and to derive ordering
constraints based on the interaction of operators can also be found in a variety of approaches.
While we analyze harmful interactions of operators in our method by studying the delete
eects, the approaches described in (Dawsson & Siklossy, 1977; Korf, 1985; Knoblock,
1994) concentrate on the positive interactions between operators. The successful matching
of eects to preconditions forms the basis to learn macro-operators, see (Dawsson & Siklossy,
1977; Korf, 1985).
The alpine system (Knoblock, 1994) learns abstraction hierarchies for the Prodigy
planner (Fink & Veloso, 1994). The approach is based on an ordering of the preconditions
and the eects of each operator, i.e., all eects of an operator must be in the same abstraction
hierarchy and its preconditions must be placed at the same or a lower level than its eects.
This introduces an ordering between the possible subgoals in a domain, which is orthogonal
to the ordering we compute: In alpine, a subgoal A is ordered before a subgoal B if
A enables B , i.e., A must be possibly achieved rst in order to achieve B . Our method
orders A before B if A cannot be achieved without necessarily destroying B . The result of
alpine and GAM are a set of binary constraints. In the case of alpine, the constraints
are computed between all atoms in a domain, while GAM restricts the analysis to the
goals only. Both approaches represent the binary constraints in a graph structure. alpine
merges atomic goals together if they belong to a strongly connected component in the graph.
GAM merges sets of goals together if they have identical degree. Then they both compute
a topological sorting of the sets that is consistent with the constraints. The resulting goal
orderings can be quite similar as the examples by Knoblock (1994) demonstrate, but GAM
approximates reasonable goal orderings in domains where alpine fails in nding abstraction
hierarchies. Two further examples (Knoblock, 1991) are the tower of hanoi domain using
h

e

379

Koehler & Hoffmann
only one move operator and the blocks world. In both domains, alpine cannot detect
the orderings because it investigates the operator schemata, not the set of ground actions,
and therefore cannot distinguish the orderings between dierent instantiations of the same
literal. Although alpine could be modied to handle ground actions, this will signicantly
increase the amount of computation it requires. GAM on the other hand, handles large sets
of ground actions in an eÆcient way, in particular if direct analysis is used.10
An analysis, which is quite similar to alpine, but which is performed in the framework
of HTN planning, is described by Tsuneto et al. (1998). The approach analyzes the external
conditions of methods, which cannot be achieved when decomposing the method further.
This means, such conditions have to be established by the decomposition of those methods,
which precede the method using this external condition. Two strategies to determine the
decomposition order of methods are dened and empirically compared. Here lies the main
dierence to the other approaches described so far: Instead of trying to automatically
construct the decomposition orderings, they are predened and xed for all domains and
problems.
Harmful interactions among operators are studied by Smith and Peot (1993) and Etzioni
(1993). A threat of an operator o to a precondition p occurs if there is an instantiation of
o such that its eects are inconsistent with p (Smith & Peot, 1993). The knowledge about
threats is used to control a plan-space planner. In contrast to a state-space planner such as
IPP, computing an explicit ordering of goals does not prevent the presence of threats in a
partial plan because the order in which the goals are processed does not determine the order
in which actions occur in the plan. The notion of forced and reasonable goal orderings is
not comparable to that of a threat because a threat still has the potential of being resolved
by adding binding or ordering constraints to the plans. In contrast to this, a forced or
reasonable goal ordering persists under all bindings and enforces a specic ordering of the
subgoals.
Given a planning problem, static (Etzioni, 1993) computes a backchaining tree from the
goals in the form of an AND/OR graph, which it subsequently analyzes for the occurrence
of goal interactions that will necessarily occur. This analysis is much more complicated
than ours, because static has to deal with uninstantiated operators and axioms, which
describe properties of legal states. The result of the analysis are goal ordering rules, which
order goals if certain conditions are satised in a state. This is the main dierence to GAM,
which generates explicit goal orderings independently of a specic state. It does not need to
extract conditions that a specic state has to satisfy because it considers the generic state
s( : ) in the analysis, which represents all states satisfying A, but not B . As GAM, static
is incomplete in the sense that it cannot detect all existing goal interactions. The problem
for GAM is that deciding reasonable orderings is PSPACE-hard, as we have proven in this
paper. The problem for static is that it has to compute the necessary eects of an operator
in a given state. As Etzioni (1993) conjectures and Nebel and Backstrom (1994) prove, this
A;

B

10. Abstraction hierarchies are more general than the goal orderings we compute. They cannot only serve
for the purpose of providing a planner with goal ordering information, but also allow to generate plans
at dierent levels of renement, see also (Bacchus & Yang, 1994). Two other approaches generating
abstraction hierarchies based on numerical criticality values can be found in (Sacerdoti, 1974; Bundy,
Giunchiglia, Sebastiani, & Walsh, 1996).

380

On Reasonable and Forced Goal Orderings
problem is computationally intractable and therefore, any polynomial-time analysis method
must be incomplete.
Last, but not least there have been quite a number of approaches in the late Eighties,
which focused directly on subgoal orderings. These fall into two categories: The approaches
described in (Drummond & Currie, 1989; Hertzberg & Horz, 1989) focus on the detection of
conicts caused by goal interdependencies to guide a partial-order planner during search. We
do not investigate these approaches in more detail here because they do not extract explicit
goal orderings as a preprocess to planning as we do. The works described in (Irani & Cheng,
1987; Cheng & Irani, 1989; Joslin & Roach, 1990) implement preprocessing approaches,
which perform a structural analysis of the planning task to determine an appropriate goal
ordering before planning starts. Irani and Cheng (1987) compute a relation  between
pairs of goals, which|roughly speaking|orders a goal A after a goal B if B must be
achieved before A can be achieved. Their formalism is rather complicated and the theoretical
properties of the relation are not investigated. In (Cheng & Irani, 1989), the approach is
extended such that sets of goals can be ordered with respect to each other. The exact
properties of the formalism remain unclear. In (Joslin & Roach, 1990), a graph-theoretical
approach is described that generates a graph with all atoms from a given domain description
as nodes and draws an arc between a node A and a node B if an operator exists that takes
A as precondition and has B as an eect. When assuming that all operators have inverse
counterparts, identifying connected components in the graph is proposed as a means to
order goals. The approach is unlikely to scale to the size of problem spaces today's planners
consider and it is also completely outdated in terms of terminology.
Finally, one can wonder how the reasonable and forced goal orderings relate to others
dened in the literature. There is only one attempt of which we know where an ordering
relation is explicitly dened and its properties are studied, see (Hullem et al., 1999). In
this paper, the notion of necessary goal orderings is introduced, which must be true in
all minimal solution plans (Kambhampati, 1995).11 The approach extends operator graphs
(Smith & Peot, 1993) and orders a goal based on three criteria called goal subsumption, goal
clobbering, and precondition violation. Goal subsumption A < B holds if every solution plan
achieving a goal B in a state s also achieves a goal A in a state s0 preceding s, and no plan
achieving one of the goals in G n fAg deletes A. Goal clobbering holds if any solution plan
for A deletes B and thus, A < B . Precondition violation holds if any solution for B results
in a deadlock from which A cannot be reached anymore, i.e., again A < B . A composite
criterion is dened that tests all three criteria simultaneously.12 A goal A is necessarily
ordered before B if it satises the composite criterion.
We remark that precondition violation seems to be equivalent to the forced orderings we
introduced, while goal clobbering appears to be similar to our reasonable orderings. It is not
possible for us to verify this conjecture as the authors of (Hullem et al., 1999) do not give
exact formal denitions. We have nothing similar to goal subsumption and we argue that
this criterion will be rarely satised in natural problems: if a goal A is achieved by every
11. A plan is minimal if it contains no subplan that is also a solution plan. We remark that minimality does
not mean that only shortest plans having the least number of actions are considered. In fact, minimal
plans can be highly non-optimal as long as no action is truly superuous.
12. Here, the authors are not very precise about what they mean with this. We argue that this means that
two goals are ordered if they satisfy at least one of the criteria.

381

Koehler & Hoffmann
solution for a goal B anyway, then the goal A can be removed from the goal set without
changing the planning task.
The authors report that they are able to detect necessary orderings in the articial
domains D S , cf. (Barrett & Weld, 1994), but fail in typical benchmark domains such as
the blocks world or the tyreworld. The reason for this seems to be that their operator graphs
do not represent all possible instantiations of operator schemes. As the authors claim, this
makes operator graph analysis very eÆcient. However, the heuristic ordering  that we
introduced in this paper also takes almost no computation time, and succeeds in nding
the goal orderings in these domains.
i

i

h

7. Outlook

Three promising avenues for future research are the following:
First, one can imagine that goal ordering information is also used during the search
process, i.e., by not only ordering the original goal set, but also other goals that emerge
during search. The major challenge seems to balance the eort on computing the goal
ordering information with the savings that can result for the search process. One can
easily imagine that ordering all goal sets that are ever generated can become a quite costly
investment without yielding a major benet for the planner.
Secondly, the renement of the goal agenda with additional subgoals is another interesting future line of work. A rst investigation using so-called intermediate goals (these are
facts that the planner must make true before it can achieve an original goal) has been
explored inside GAM and the results are reported in (Koehler & Homann, 1998). Earlier
work addressing the task of learning intermediate goals can be found in (Ruby & Kibler,
1989), but this problem has not been in the focus of AI planning research since then.
A third line of work addresses the interaction of GAM with a forward-searching planning system. We have seen that GAM preserves the correctness of a planner, and that
it preserves the completeness at least on deadlock-free planning domains. We have also
seen, however, that solution plans using GAM can get longer, i.e., GAM does not preserve the optimality of a planner. Recently, planning systems that do not deliver plans of
guaranteed optimality have demonstrated an impressive performance in terms of runtime
and plan length, e.g., HSP, which is rst mentioned in (Bonet, Loerincs, & Gener, 1997),
GRT (Refanidis & Vlahavas, 1999), and in particular ff (Homann, 2000). These systems
are heuristic-search planners searching forward in the state space with non-admissible, but
informative heuristics.
The ff planning system developed by one of the authors has been awarded \Group A
Distinguished Performance Planning System" and has also won the Schindler Award for
the best performing planning system in the Miconic 10 Elevator domain (ADL track) at
the AIPS 2000 planning competition. The integration of goal agenda techniques into the
planner is one of the factors that enabled the excellent behavior of ff in the competition:
they were crucial for scaling to blocks world problems of 50 blocks, helped by about a factor 2
on schedule and Miconic 10, and never slowed down the algorithm.
Forward state-space search is a quite natural framework to be driven by the goal agenda:
Simply let the planner solve a subproblem, and start the next search from the state where
the last search ended. Even more appealing, heuristic forward-search planners have a deeper
382

On Reasonable and Forced Goal Orderings
kind of interaction with GAM than for example graphplan-style planners. In addition
to the smaller problems they are facing when using the goal agenda, their heuristics are
inuenced because they employ techniques for estimating the goal distance from a state.
When using the goal agenda, dierent goal sets result at each stage of the planning process
and therefore, the goal-distance estimate will be dierent, too. Currently a heuristic device
inside the ff search algorithm is being developed, which knows that it is being driven by
a goal agenda, and which has access to the complete set of goals. This information can be
used to further prune unpromising branches from the search space when it discovers that
currently achieved goals will probably have to be destroyed and reachieved later on.
References

Allen, J. (Ed.), AIPS-98 (1998). Proceedings of the 4th International Conference on Articial Intelligence Planning Systems. AAAI Press, Menlo Park.
Anderson, C., & Weld, D. (1998). Conditional eects in Graphplan. In Allen (Allen, 1998),
pp. 44{53.
Bacchus, F., & Yang, Q. (1994). Downward renement and the eÆciency of hierarchical
problem solving. Articial Intelligence, 71, 43{100.
Backstrom, C., & Jonsson, P. (1995). Planning with abstraction hierarchies can be exponentially less eÆcient. In Mellish (Mellish, 1995), pp. 1599{1604.
Barrett, A., & Weld, D. (1994). Partial-order planning: Evaluating possible eÆciency gains.
Articial Intelligence, 67, 71{112.
Blum, A., & Furst, M. (1997). Fast planning through planning graph analysis. Articial
Intelligence, 90 (1{2), 279{298.
Bonet, B., Loerincs, G., & Gener, H. (1997). A robust and fast action selection mechanism for planning. In Proceedings of the 14th National Conference of the American
Association for Articial Intelligence, pp. 714{719.
Bundy, A., Giunchiglia, F., Sebastiani, R., & Walsh, T. (1996). Computing abstraction
hierarchies by numerical simulation. In Weld, & Clancey (Weld & Clancey, 1996), pp.
523{529.
Bylander, T. (1992). Complexity results for serial decomposability. In Proceedings of the
10th National Conference of the American Association for Articial Intelligence, pp.
729{734 San Jose, CA. MIT Press.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Articial Intelligence, 69, 165{204.
Chapman, D. (1987). Planning for conjunctive goals. Articial Intelligence, 32 (3), 333{377.
Cheng, J., & Irani, K. (1989). Ordering problem subgoals. In Sridharan (Sridharan, 1989),
pp. 931{936.
383

Koehler & Hoffmann
Dawsson, C., & Siklossy, L. (1977). The role of preprocessing in problem solving systems.
In Proceedings of the 5th International Joint Conference on Articial Intelligence, pp.
465{471 Cambridge, MA.
Dimopoulos, Y., Nebel, B., & Koehler, J. (1997). Encoding planning problems in nonmonotonic logic programs. In Steel (Steel, 1997), pp. 169{181.
Drummond, M., & Currie, K. (1989). Goal ordering in partially ordered plans. In Sridharan
(Sridharan, 1989), pp. 960{965.
El-Kholy, A., & Richards, B. (1996). Temporal and resource reasoning in planning: the
parcPLAN approch. In Wahlster, W. (Ed.), Proceedings of the 12th European Conference on Articial Intelligence, pp. 614{618. John Wiley & Sons, Chichester, New
York.
Etzioni, O. (1993). Acquiring search-control knowledge via static analysis. Articial Intelligence, 62, 255{301.
Fink, E., & Veloso, M. (1994). Prodigy planning algorithm. Technical report CMU-94-123,
Carnegie Mellon University.
Fox, M., & Long, D. (1998). The automatic inference of state invariants in TIM. Journal
of Articial Intelligence Research, 9, 367{421.
Fox, M., & Long, D. (1999). EÆcient implementation of the plan graph in STAN. Journal
of Articial Intelligence Research, 10, 87{115.
Hertzberg, J., & Horz, A. (1989). Towards a theory of conict detection and resolution in
nonlinear plans. In Sridharan (Sridharan, 1989), pp. 937{942.
Homann, J. (2000). A heuristic for domain independent planning and its use in an enforced
hill-climbing algorithm. In 12th International Symposium on Methods for Intelligent
Systems.
Hullem, J., Munoz-Avila, H., & Weberskirch, F. (1999). Extracting goal orderings to
improve partial-order and Graphplan-based planning. Technical report, University of
Kaiserslautern.
Irani, K., & Cheng, J. (1987). Subgoal ordering and goal augmentation for heuristic problem solving. In McDermott, D. (Ed.), Proceedings of the 10th International Joint
Conference on Articial Intelligence, pp. 1018{1024 Milan, Italy. Morgan Kaufmann.
Jonsson, P., Haslum, P., & Backstrom, C. (2000). Towards eÆcient universal planning: A
randomized approach. Articial Intelligence, 117 (1), 1{29.
Joslin, D., & Roach, J. (1990). A theoretical analysis of conjunctive-goal problems. Articial
Intelligence, 41, 97{106.
Kambhampati, S. (1995). Admissible pruning strategies based on plan minimality for planspace planning. In Mellish (Mellish, 1995), pp. 1627{1633.
384

On Reasonable and Forced Goal Orderings
Kautz, H., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic, and
stochastic search. In Weld, & Clancey (Weld & Clancey, 1996), pp. 1194{1201.
Knoblock, C. (1991). Automatically Generating Abstractions for Problem Solving. Ph.D.
thesis, Carnegie Mellon University.
Knoblock, C. (1994). Automatically generating abstractions for planning. Articial Intelligence, 68 (2), 243{302.
Koehler, J. (1999). Handling of conditional eects and negative goals in IPP. Technical report 128, University of Freiburg, Institute of Computer Science. available at
http://www.informatik.uni-freiburg.de/~ koehler/ipp.html.
Koehler, J., & Homann, J. (1998). Planning with goal agendas. Technical report
110, University of Freiburg. available at http://www.informatik.uni-freiburg.de/~
koehler/ipp.html.
Koehler, J., Nebel, B., Homann, J., & Dimopoulos, Y. (1997). Extending planning graphs
to an ADL subset. In Steel (Steel, 1997), pp. 273{285.
Korf, R. (1985). Macro-operators: A weak method for learning. Articial Intelligence, 26,
35{77.
Korf, R. (1987). Planning as search: A quantitative approach. Articial Intelligence, 33,
65{88.
McDermott, D., & Hanks, S. (1987). Nonmonotonic logic and temporal projection. Articial
Intelligence, 33, 379{412.
Mellish, C. (Ed.), IJCAI-95 (1995). Proceedings of the 14th International Joint Conference
on Articial Intelligence. Morgan Kaufmann, San Francisco, CA.
Nebel, B., & Backstrom, C. (1994). On the computational complexity of temporal projection, planning, and plan validation. Journal of Articial Intelligence, 66 (1), 125{160.
Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts and operators in
plan generation. In Steel (Steel, 1997), pp. 338{350.
Nilsson, N. (1980). Principles of Articial Intelligence. Tioga Publishing Company, Palo
Alto.
Pednault, E. (1989). ADL: Exploring the middle ground between STRIPS and the Situation
Calculus. In Brachman, R., Levesque, H., & Reiter, R. (Eds.), Proceedings of the 1st
International Conference on Principles of Knowledge Representation and Reasoning,
pp. 324{332 Toronto, Canada. Morgan Kaufmann.
Penberthy, J., & Weld, D. (1992). UCPOP: A sound, complete, partial order planner
for ADL. In Nebel, B., Swartout, W., & Rich, C. (Eds.), Proceedings of the 3rd
International Conference on Principles of Knowledge Representation and Reasoning,
pp. 103{113. Morgan Kaufmann, San Mateo.
385

Koehler & Hoffmann
Pollack, M., Joslin, D., & Paolucci, M. (1997). Selection strategies for partial-order planning.
Journal of Articial Intelligence Research, 6, 223{262.
Refanidis, I., & Vlahavas, I. (1999). GRT: A domain independent heuristic for STRIPS
worlds based on greedy regression tables. In Proceedings of the 5th European Conference on Planning, pp. 346{358.
Regli, W., Gupta, S., & Nau, D. (1995). AI planning versus manufactoring-operation
planning: A case study. In Mellish (Mellish, 1995), pp. 1670{1676.
Ruby, D., & Kibler, D. (1989). Learning subgoal sequences for planning. In Sridharan
(Sridharan, 1989), pp. 609{615.
Russel, S., & Norvig, P. (1995). Articial Intelligence - A modern Approach. Prentice Hall.
Sacerdoti, E. (1974). Planning in a hierarchy of abstraction spaces. Articial Intelligence,
5, 115{135.
Sanborn, J., & Hendler, J. (1988). Near-term event projection through dynamic simulation
or how did the robot cross the road? In Proceedings of the 2nd Conference on AI and
Simulation.
Smith, D., & Peot, M. (1993). Postponing threats in partial-order planning. In Proceedings
of the 11th National Conference of the American Association for Articial Intelligence,
pp. 500{506. AAAI Press, MIT Press.
Sridharan, N. (Ed.), IJCAI-89 (1989). Proceedings of the 11th International Joint Conference on Articial Intelligence, Detroit, MI. Morgan Kaufmann.
Steel, S. (Ed.), ECP-97 (1997). Proceedings of the 4th European Conference on Planning,
Vol. 1348 of LNAI. Springer.
Tsuneto, R., Hendler, J., & Nau, D. S. (1998). Analyzing external conditions to improve
the eÆciency of HTN planning. In Allen (Allen, 1998), pp. 913{920.
Veloso, M., & Blythe, J. (1994). Linkability: Examining causal link commitments in partialorder planning. In Hammond, K. (Ed.), Proceedings of the 2nd International Conference on Articial Intelligence Planning Systems, pp. 170{175. AAAI Press, Menlo
Park.
Warshall, J. (1962). A theorem on boolean matrices. Journal of the ACM, 9 (1), 11{12.
Weld, D., & Clancey, B. (Eds.)., AAAI-96 (1996). Proceedings of the 14th National Conference of the American Association for Articial Intelligence. AAAI Press.
Williams, B., & Nayak, R. (1997). A reactive planner for a model-based executive. In
Proceedings of the 15th International Joint Conference on Articial Intelligence, pp.
1178{1185. Morgan Kaufmann, San Francisco, CA.

386

Journal of Articial Intelligence Research 12 (2000) 219-234

Submitted 5/99; published 5/00

Randomized Algorithms for the Loop Cutset Problem
Ann Becker
Reuven Bar-Yehuda
Dan Geiger

anyuta@cs.technion.ac.il
reuven@cs.technion.ac.il
dang@cs.technion.ac.il

Computer Science Department
Technion, Haifa, 32000, Israel

Abstract

We show how to nd a minimum weight loop cutset in a Bayesian network with high
probability. Finding such a loop cutset is the rst step in the method of conditioning for
inference. Our randomized algorithm for nding a loop cutset outputs
a minimum loop
cutset after O(c 6k kn) steps with probability at least 1 ; (1 ; 61k )c6k , where c > 1 is a
constant specied by the user, k is the minimal size of a minimum weight loop cutset, and
n is the number of vertices. We also show empirically that a variant of this algorithm often
nds a loop cutset that is closer to the minimum weight loop cutset than the ones found
by the best deterministic algorithms known.

1. Introduction
The method of conditioning is a well known inference method for the computation of posterior probabilities in general Bayesian networks (Pearl, 1986, 1988; Suermondt & Cooper,
1990; Peot & Shachter, 1991) as well as for nding MAP values and solving constraint satisfaction problems (Dechter, 1999). This method has two conceptual phases. First to nd
an optimal or close to optimal loop cutset and then to perform a likelihood computation
for each instance of the variables in the loop cutset. This method is routinely used by
geneticists via several genetic linkage programs (Ott, 1991; Lang, 1997; Becker, Geiger, &
Schaer, 1998). A variant of this method was developed by Lange and Elston (1975).
Finding a minimum weight loop cutset is NP-complete and thus heuristic methods have
often been applied to nd a reasonable loop cutset (Suermondt & Cooper, 1990). Most
methods in the past had no guarantee of performance and performed very badly when
presented with an appropriate example. Becker and Geiger (1994, 1996) oered an algorithm
that nds a loop cutset for which the logarithm of the state space is guaranteed to be at most
a constant factor o the optimal value. An adaptation of these approximation algorithms
has been included in version 4.0 of FASTLINK, a popular software for analyzing large
pedigrees with small number of genetic markers (Becker et al., 1998). Similar algorithms in
the context of undirected graphs are described by Bafna, Berman, and Fujito (1995) and
Fujito (1996).
While approximation algorithms for the loop cutset problem are quite useful, it is still
worthwhile to invest in nding a minimum loop cutset rather than an approximation because the cost of nding such a loop cutset is amortized over the many iterations of the
conditioning method. In fact, one may invest an eort of complexity exponential in the size
of the loop cutset in nding a minimum weight loop cutset because the second phase of
the conditioning algorithm, which is repeated for many iterations, uses a procedure of such

c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Becker, Bar-Yehuda, & Geiger

complexity. The same considerations apply also to constraint satisfaction problems as well
as other problems in which the method of conditioning is useful (Dechter, 1990, 1999).
In this paper we describe several randomized algorithms that compute a loop cutset. As
done by Bar-Yehuda, Geiger, Naor, and Roth (1994), our solution is based on a reduction
to the weighted feedback vertex set problem. A feedback vertex set (FVS) F is a set of
vertices of an undirected graph G = (V; E ) such that by removing F from G, along with
all the edges incident with F , a set of trees is obtained. The Weighted Feedback Vertex Set
(WFVS) problem is to nd a feedback
vertex set F of a vertex-weighted graph with a weight
P
function w : V ! IR+ , such that v2F w(v ) is minimized. When w(v )  1, this problem is
called the FVS problem. The decision version associated with the FVS problem is known
to be NP-Complete (Garey & Johnson, 1979, pp. 191{192).
Our randomized algorithm for nding a WFVS, called RepeatedWGuessI, outputs
a
k
1
k
c
6
minimum weight FVS after O(c 6 kn) steps with probability at least 1 ; (1 ; 6k ) , where
c > 1 is a constant specied by the user, k is the minimal size of a minimum weight FVS,
and n is the number of vertices. For unweighted graphs we present an algorithm that ndsk
a minimum FVS of a graph G after O(c 4kkn) steps with probability at least 1 ; (1 ; 41k )c4 .
In comparison, several deterministic algorithms for nding a minimum FVS are described
in the literature. One has a complexity O((2k + 1)k n2 ) (Downey & Fellows, 1995b) and
others have a complexity O((17k4)!n) (Bodlaender, 1990; Downey & Fellows, 1995a).
A nal variant of our randomized algorithms, called WRA, has the best performance
because it utilizes information from previous runs. This algorithm is harder to analyze
and its investigation is mostly experimental. We show empirically that the actual run time
of WRA is comparable to a Modied Greedy Algorithm (MGA), described by Becker and
Geiger (1996), which is the best available deterministic algorithm for nding close to optimal
loop cutsets, and yet, the output of WRA is often closer to the minimum weight loop cutest
than the output of MGA.
The rest of the paper is organized as follows. In Section 2 we outline the method of
conditioning, explain the related loop cutset problem and describe the reduction from the
loop cutset problem to the WFVS Problem. In Section 3 we present three randomized algorithms for the WFVS problem and their analysis. In Section 4 we compare experimentally
WRA and MGA with respect to output quality and run time.

2. Background: The Loop Cutset Problem

A short overview of the method of conditioning and denitions related to Bayesian networks
are given below. See the book by Pearl (1988) for more details. We then dene the loop
cutset problem.
Let P (u1 ; : : :; un) be a probability distribution where each variable ui has a nite set
of possible values called the domain of ui. A directed graph D with no directed cycles is
called a Bayesian network of P if there is a 1{1 mapping between fu1; : : :; ung and vertices
in D, such that ui is associated with vertex i and P can be written as follows:

P (u1; : : :; un) =

n
Y
i=1

P (ui j ui ; : : :; uij i )
1

( )

where i1 ; : : :; ij (i) are the source vertices of the incoming edges to vertex i in D.
220

(1)

Randomized Algorithms for the Loop Cutset Problem

Suppose now that some variables fv1; : : :; vl g among fu1; : : :; ung are assigned specic
values fv1; : : :; vlg respectively. The updating problem is to compute the probability P (ui j
v1 = v1; : : :; vl = vl ) for i = 1; : : :; n.
A trail in a Bayesian network is a subgraph whose underlying graph is a simple path. A
vertex b is called a sink with respect to a trail t if there exist two consecutive edges a ! b
and b c on t. A trail t is active by a set of vertices Z if (1) every sink with respect to
t either is in Z or has a descendant in Z and (2) every other vertex along t is outside Z .
Otherwise, the trail is said to be blocked (d-separated) by Z .
Verma and Pearl proved that if D is a Bayesian network of P (u1 ; : : :; un) and all trails
between a vertex in fr1; : : :; rl g and a vertex in fs1 ; : : :; sk g are blocked by ft1 ; : : :; tm g,
then the corresponding sets of variables fur ; : : :; url g and fus ; : : :; usk g are independent
conditioned on fut ; : : :; utm g (Verma & Pearl, 1988). Furthermore, Geiger and Pearl proved
that this result cannot be enhanced (Geiger & Pearl, 1990). Both results were presented
and extended by Geiger, Verma, and Pearl (1990).
Using the close relationship between blocked trails and conditional independence, Kim
and Pearl developed an algorithm update-tree that solves the updating problem on
Bayesian networks in which every two vertices are connected with at most one trail (Kim
& Pearl, 1983). Pearl then solved the updating problem on any Bayesian network as follows (Pearl, 1986). First, a set of vertices S is selected such that any two vertices in the
network are connected by at most one active trail in S [ Z , where Z is any subset of vertices. Then, update-tree is applied once for each combination of value assignments to
the variables corresponding to S , and, nally, the results are combined. This algorithm is
called the method of conditioning and its complexity grows exponentially with the size of
S . The set S is called a loop cutset. Note that when the domain size of the variables varies,
then update-tree is called a number of times equal to the product of the domain sizes
of the variables whose corresponding vertices participate in the loop cutset. If we take the
logarithm of the domain size (number of values) as the weight of a vertex, then nding a
loop cutset such that the sum of its vertices weights is minimum optimizes Pearl's updating
algorithm in the case where the domain sizes may vary.
We now give an alternative denition for a loop cutset S and then provide a probabilistic
algorithm for nding it. This denition is borrowed from a paper by Bar-Yehuda et al.
(1994). The underlying graph G of a directed graph D is the undirected graph formed
by ignoring the directions of the edges in D. A cycle in G is a path whose two terminal
vertices coincide. A loop in D is a subgraph of D whose underlying graph is a cycle. A
vertex v is a sink with respect to a loop ; if the two edges adjacent to v in ; are directed
into v . Every loop must contain at least one vertex that is not a sink with respect to that
loop. Each vertex that is not a sink with respect to a loop ; is called an allowed vertex
with respect to ;. A loop cutset of a directed graph D is a set of vertices that contains at
least one allowed vertex with respectPto each loop in D. The weight of a set of vertices X
is denoted by w(X ) and is equal to v2X w(v ) where w(x) = log(jxj) and jxj is the size of
the domain associated with vertex x. A minimum weight loop cutset of a weighted directed
graph D is a loop cutset F  of D for which w(F  ) is minimum over all loop cutsets of G.
The Loop Cutset Problem is dened as nding a minimum weight loop cutset of a given
weighted directed graph D.
1

1

221

1

Becker, Bar-Yehuda, & Geiger

The approach we take is to reduce the loop cutset problem to the weighted feedback
vertex set problem, as done by Bar-Yehuda et al. (1994). We now dene the weighted
feedback vertex set problem and then the reduction.
Let G = (V; E ) be an undirected graph, and let w : V ! IR+ be a weight function
on the vertices of G. A feedback vertex set of G is a subset of vertices F  V such that
each cycle in G passes through at least one vertex in F . In other words, a feedback vertex
set F is a set of vertices of G such that by removing F from G, along with all the edges
incident with F , we obtain a set of trees (i.e., aPforest). The weight of a set of vertices X
is denoted (as before) by w(X ) and is equal to v2X w(v ). A minimum feedback vertex set
of a weighted graph G with a weight function w is a feedback vertex set F  of G for which
w(F  ) is minimum over all feedback vertex sets of G. The Weighted Feedback Vertex Set
(WFVS) Problem is dened as nding a minimum feedback vertex set of a given weighted
graph G having a weight function w.
The reduction is as follows. Given a weighted directed graph (D; w) (e.g., a Bayesian
network), we dene the splitting weighted undirected graph Ds with a weight function ws as
follows. Split each vertex v in D into two vertices v and v in Ds such that all incoming
edges to v in D become undirected incident edges with v in Ds , and all outgoing edges
from v in D become undirected incident edges with v in Ds. In addition, connect v and
v in Ds by an undirected edge. Now set ws (v ) = 1 and ws (v ) = w(v ). For a set of
vertices X in Ds , we dene (X ) as the set obtained by replacing each vertex v or v in
X by the respective vertex v in D from which these vertices originated. Note that
if X is a
S
cycle in Ds , then (X ) is a loop in D, and if Y is a loop in D, then ;1(Y ) = v2Y ;1(v )
is a cycle in Ds where
8
>
v is a sink on Y
< v
;1(v ) = v
v is a source on Y
>
: fv ; v g otherwise
(A vertex v is a source with respect to a loop Y if the two edges adjacent to v in Y originate
from v ). This mapping between loops in D and cycles in Ds is one-to-one and onto.
Our algorithm can now be easily stated.
in

out

in

out

out

in

in

out

in

out

in

out

in

out

ALGORITHM LoopCutset
Input: A Bayesian network D
Output: A loop cutset of D

1. Construct the splitting graph Ds
with weight function ws
2. Find a feedback vertex set F for (Ds; ws)
using the Weighted Randomized Algorithm (WRA)
3. Output (F ).

It is immediately seen that if WRA (developed in later sections) outputs a feedback
vertex set F of Ds whose weight is minimum with high probability, then (F ) is a loop
cutset of D with minimum weight with the same probability. This observation holds due to
the one-to-one and onto correspondence between loops in D and cycles in Ds and because
WRA never chooses a vertex that has an innite weight.
222

Randomized Algorithms for the Loop Cutset Problem

3. Algorithms for the WFVS Problem

Recall that a feedback vertex set of G is a subset of vertices F  V such that each cycle
in G passes through at least one vertex in F . In Section 3.1 we address the problem of
nding a FVS with a minimum number of vertices and in Sections 3.2 and 3.3 we address
the problem of nding a FVS with a minimum weight. Throughout, we allow G to have
parallel edges. If two vertices u and v have parallel edges between them, then every FVS
of G includes either u, v , or both.

3.1 The Basic Algorithms

In this section we present a randomized algorithm for the FVS problem. First we introduce
some additional terminology and notation. Let G = (V; E ) be an undirected graph. The
degree of a vertex v in G, denoted by d(v ), is the number of vertices adjacent to v . A
self-loop is an edge with two endpoints at the same vertex. A leaf is a vertex with degree
less or equal 1, a linkpoint is a vertex with degree 2 and a branchpoint is a vertex with
degree strictly higher than 2. The cardinality of a set X is denoted by jX j.
A graph is called rich if every vertex is a branchpoint and it has no self-loops. Given
a graph G, by repeatedly removing all leaves, and bypassing with an edge every linkpoint,
a graph G0 is obtained such that the size of a minimum FVS in G0 and in G are equal
and every minimum FVS of G0 is a minimum FVS of G. Since every vertex involved in
a self-loop belongs to every FVS, we can transform G0 to a rich graph Gr by adding the
vertices involved in self loops to the output of the algorithm.
Our algorithm is based on the observation that if we pick an edge at random from a rich
graph there is a probability of at least 1=2 that at least one endpoint of the edge belongs
to any given FVS F . A precise formulation of this claim is given by Lemma 1 whose proof
is given implicitly by Voss (1968, Lemma 4).

Lemma 1 Let G = (V; E ) be a rich graph, F be a feedback vertex set of G and X = V n F .
Let EX denote the set of edges in E whose endpoints are all vertices in X and EF;X denote
the set of edges in G that connect vertices in F with vertices in X . Then, jEX j  jEF;X j.
Proof. The graph obtained by deleting a feedback vertex set F of a graph G(V; E ) is
a forest with vertices X = V n F . Hence, jEX j < jX j. However, each vertex in X is a
branchpoint in G, and so,
3 jX j 

X

v 2X

d(v ) = jEF;X j + 2 jEX j:

Thus, jEX j  jEF;X j. 2
Lemma 1 implies that when picking an edge at random from a rich graph, it is at least
as likely to pick an edge in EF;X than an edge in EX . Consequently, selecting a vertex
at random from a randomly selected edge has a probability of at least 1=4 to belong to a
minimum FVS. This idea yields a simple algorithm to nd a FVS.

223

Becker, Bar-Yehuda, & Geiger

ALGORITHM SingleGuess(G,j)
Input: An undirected graph G0 and an integer j > 0.
Output: A feedback vertex set F of size  j , or "Fail" otherwise.
For i = 1; : : :; j
1. Reduce Gi;1 to a rich graph Gi
while placing self loop vertices in F .
2. If Gi is the empty graph Return F
3. Pick an edge e = (u; v ) at random from Ei
4. Pick a vertex vi at random from (u; v )
5. F F [ fvig
6. V V n fvi g
Return "Fail"

Due to Lemma 1, when SingleGuess(G; j ) terminates with a FVS of size j , there is a
probability of at least 1=4j that the output is a minimum FVS.
Note that steps 3 and 4 in SingleGuess determine a vertex v by rst selecting an
arbitrary edge and then selecting an arbitrary endpoint of this edge. An equivalent way of
achieving the same selection rule is to choose a vertex with probability proportional to its
degree:
p(v) = P d(vd)(u) = 2d(jvE) j
u2V
To see the equivalence of these two selection methods, dene ;(v ) to be a set of edges whose
one endpoint is v , and note that for graphs without self-loops,
X
X
p(v) =
p(v je)  p(e) = 12
p(e) = 2d(jvE) j
e2;(v)
e2;(v)

This equivalent phrasing of the selection criterion is easier to extend to the weighted case
and will be used in the following sections.
An algorithm for nding a minimum FVS with high probability, which we call RepeatedGuess, can now be described as follows: Start with j = 1. Repeat SingleGuess c 4j
times where c > 1 is a parameter dened by the user. If in one of the iterations a FVS of
size  j is found, then output this FVS, otherwise, increase j by one and continue.

ALGORITHM RepeatedGuess(G,c)
Input: An undirected graph G

and a constant c > 1.
Output: A feedback vertex set F .
For j = 1; : : :; jV j
Repeat c 4j times
1. F SingleGuess(G; j )
2. If F is not "Fail" then Return F
End fRepeatg
End fForg
224

Randomized Algorithms for the Loop Cutset Problem

The main claims about these algorithms are given by the following theorem.

Theorem 2 Let G be an undirected graph and c  1 be a constant. Then, SingleGuess(G; k)
outputs a FVS whose expected size is no more than 4k, and RepeatedGuess(G;
c) outputs,
after O(c 4kkn) steps, a minimum FVS with probability at least 1 ; (1 ; 41k )c4k , where k is
the size of a minimum FVS and n is the number of vertices.
The claims about the probability of success and number of steps follow immediately
from the fact that the probability of success of SingleGuess(G; j ) is at least (1=4)j and
that, in case of success, O(cP4j ) iterations are performed each taking O(jn) steps. The result
follows from the fact that kj=1 j 4j is of order O(k4k ). The proof about the expected size
of a single guess is presented in the next section.
Theorem 2 shows that each guess produces a FVS which, on the average, is not too
far from the minimum, and that after enough iterations, the algorithm converges to the
minimum with high probability. In the weighted case, discussed next, we managed to
achieve each of these two guarantees in separate algorithms, but we were unable to achieve
both guarantees in a single algorithm.

3.2 The Weighted Algorithms

We now turn to the weighted FVS problem (WFVS) of size k which is to nd a feedback
vertex set F of a vertex-weighted graph (G; w), w : V ! IR+ , of size less or equal k such
that w(F ) is minimized.
Note that for the weighted FVS problem we cannot replace each linkpoint v with an
edge because if v has weight lighter than its branchpoint neighbors then v can participate
in a minimum weight FVS of size k.
A graph is called branchy if it has no endpoints, no self loops, and, in addition, each
linkpoint is connected only to branchpoints (Bar-Yehuda, Geiger, Naor, & Roth, 1994).
Given a graph G, by repeatedly removing all leaves, and bypassing with an edge every
linkpoint that has a neighbor with equal or lighter weight, a graph G0 is obtained such
that the weight of a minimum weight FVS (of size k) in G0 and in G are equal and every
minimum WFVS of G0 is a minimum WFVS of G. Since every vertex with a self-loop
belongs to every FVS, we can transform G0 to a branchy graph without self-loops by adding
the vertices involved in self loops to the output of the algorithm.
To address the WFVS problem we oer two slight modications to the algorithm SingleGuess presented in the previous section. The rst algorithm, which we call SingleWGuessI, is identical to SingleGuess except that in each iteration we make a reduction to a branchy graph instead of a reduction to a rich graph.
It chooses a vertex
P
with probability proportional to the degree using p(v ) = d(v )= u2V d(u). Note that this
probability does not take the weight of a vertex into account. A second algorithm, which
we call SingleWGuessII, chooses a vertex with probability proportional to the ratio of its
degree over its weight,
X
(2)
p(v ) = wd((vv)) = wd((uu)) :
u2V

225

Becker, Bar-Yehuda, & Geiger

ALGORITHM SingleWGuessI(G,j)
Input: An undirected weighted graph G0
and an integer j > 0.

Output: A feedback vertex set F of size  j ,
or "Fail" otherwise.

For i = 1; : : :; j

1. Reduce Gi;1 to a branchy graph Gi (Vi; Ei)
while placing self loop vertices in F .
2. If Gi is the empty graph Return F
3. Pick a vertex vi 2 Vi at random
with
P
probability pi(v ) = di(v )= u2Vi di(u)
4. F F [ fvig
5. V V n fvi g
Return "Fail"
The second algorithm uses Eq. 2 for computing p(v ) in Line 3. These two algorithms
have remarkably dierent guarantees of performance. Version I guarantees that choosing a
vertex that belongs to any given FVS is larger than 1=6, however, the expected weight of a
FVS produced by version I cannot be bounded by a constant times the weight of a minimum
WFVS. Version II guarantees that the expected weight of its output is bounded by 6 times
the weight of a minimum WFVS, however, the probability of converging to a minimum
after any xed number of iterations can be arbitrarily small. We rst demonstrate via an
example the negative claims. The positive claims are phrased more precisely in Theorem 3
and proven thereafter.
Consider the graph shown in Figure 1 with three vertices a,b and c, and corresponding
weights w(a) = 6, w(b) = 3 and w(c) = 3m, with three parallel edges between a and b,
and three parallel edges between a and c. The minimum WFVS F  with size 1 consists of
vertex a. According to Version II, the probability of choosing vertex a is (Eq. 2):

p(a) = (1 + 1=m )   + 1

So if  is arbitrarily small and m is suciently large, then the probability of choosing vertex
a is arbitrarily small. Thus, the probability of choosing a vertex from some F  by the
criterion d(v )=w(v ), as done by Version II, can be arbitrarily small. If, on the other hand,
Version I is used, then the probability of choosing a; b, or c is 1=2; 1=4; 1=4, respectively.
Thus, the expected weight of the rst vertex to be chosen is 3=4  ( + m + 4), while the
weight of a minimum WFVS is 6. Consequently, if m is suciently large, the expected
weight of a WFVS found by Version I can be arbitrarily larger than a minimum WFVS.
The algorithm for repeated guesses, which we call RepeatedWGuessI(G; c; j ) is as
follows: repeat SingleWGuessI(G; j ) c 6j times, where j is the minimal number of vertices
of a minimum weight FVS we seek. If no FVS is found of size  j , the algorithm outputs
that the size of a minimum WFVS is larger than j with high probability, otherwise, it
outputs the lightest FVS of size less or equal j among those explored. The following
theorem summarizes the main claims.
226

Randomized Algorithms for the Loop Cutset Problem

w(c) = 3
c

w(a) = 6

w(b) = 3m

a

b

Figure 1: The minimum WFVS F  = fag.

Theorem 3 Let G be a weighted undirected graph and c  1 be a constant.
a) The algorithm RepeatedWGuessI(G; c; kk ) outputs after O(c 6kkn) steps a minimum
WFVS with probability at least 1 ; (1 ; 61k )c6 , where k is the minimal size of a minimum

weight FVS of G and n is the number of vertices.
b) The algorithm SingleWGuessII(G,k) outputs a feedback vertex set whose expected
weight is no more than six times the weight of a minimum weight FVS.

The proof of each part requires a preliminary lemma.

Lemma 4 Let G = (V; E ) be a branchy graph, F be a feedback vertex set of G and X =
V n F . Let EX denote the set of edges in E whose endpoints are all vertices in X and

EF;X denote the set of edges in G that connect vertices in F with vertices in X . Then,
jEX j  2  jEF;X j.

Proof. Let X b be the set of branchpoints in X . We replace every linkpoint in X by an

edge between its neighbors, and denote the resulting set of edges between vertices in X b by
b b . The proof of Lemma 1 shows that
EXb b and between vertices in X b and F by EF;X
b b j:
jEXb b j  jEF;X

Since every linkpoint in X has both neighbors in the set X b [ F , the following holds:
b b j:
jEX j  2  jEXb b j and jEF;X j = jEF;X
Hence, jEX j  2  jEF;X j. 2

An immediate consequence of Lemma 4 is that the probability of randomly choosing an
edge that has at least one endpoint that belongs to a FVS is greater or equal 1=3. Thus,
selecting a vertex at random from a randomly selected edge has a probability of at least
1=6 to belong to a FVS. Consequently, if the algorithm terminates after
c 6k iterations, with
k
1
a WFVS of size k, there is a probability of at least 1 ; (1 ; 6k )c6 that the output is a
minimum WFVS of size at most k. This proves part (a) of Theorem 3. Note that since k
is not known in advance, we use RepeatedWGuessI(G; c; j ) with increasing values of j
until a FVS is found, say when j=J. When such a set is found it is still possible that there
exists a WFVS with more than J vertices that has a smaller weight than the one found.
This happens when k > J . However, among the WFVSs of size at most J , the algorithm
nds one with minimum weight with high probability.
The second part requires the following lemma.
227

Becker, Bar-Yehuda, & Geiger

Lemma 5 Let G be a branchy graph and F be a FVS of G. Then,
X
X
d(v )  6 d(v):
v2V

v 2F

Proof. Denote by dY (v) the number of edges between a vertex v and a set of vertices Y .
Then,

X

v2V

Due to Lemma 4,
Consequently,

d(v) =

X

X

d(v ) =

v 2F
X
X
X
dX (v) + dF (v) + d(v ):
v2X
v 2X
v2F

X

v2X

v2X

d(v ) +

dX (v) = 2jEX j  4jEF;X j = 4

X

v2V

d (v )  4
X

v2X

X

v2X

dF (v) +

X

v2X

dF (v ):

(3)

dF (v)+
X

v2F

d(v)  6

X

v 2F

d(v )

as claimed. 2
We can now prove part (b) of Theorem 3 analyzing SingleWGuessII(G,k). Recall
that Vi is the set of vertices in graph Gi in iteration i, di (v ) is the degree of vertex v in Gi,
and vi is the vertex chosen in iteration i. Furthermore, recall that pi(v ) is the probability
to choose vertex v in iteration i.
P
The expected weight Ei (w(v )) = v2Vi w(v )  pi (v ) of a chosen vertex in iteration
i is
Pk
denoted with ai . Thus, due to the linearity of the expectation operator, E (w(F )) = i=1 ai,
assuming jF j = k. We dene a normalization constant for iteration i as follows:

i =

"

#
X di (u) ;1

u2Vi

w(u)

Then, pi (v ) = i  dwi((vv)) and

ai =

X

v2Vi

X
w(v)  dwi((vv))  i = i  di(v )

v 2V i

Let F  be a minimum FVS of G and Fi be minimum weight FVS of the graph Gi . The
expected weight Ei(w(v )jv 2 Fi )) of a vertex chosen from Fi in iteration i is denoted with
bi. We have,
X
X
bi =
w(v )  pi(v ) = i  di (v)
v2Fi

v2Fi

By Lemma 5, ai =bi  6 for every i.

228

Randomized Algorithms for the Loop Cutset Problem

Recall that by denition F2 is the minimum FVS in the branchy graph G2 obtained
from G1 n fv1g. We get,

E (w(F ))  E1(w(v)jv 2 F1 )) + E (w(F2))
because the right hand side is the expected weight of the output F assuming the algorithm
nds a minimum FVS on G2 and just needs to select one additional vertex, while the left
hand side is the unrestricted expectation. By repeating this argument we get,

E (w(F ))  b1 + E (w(F2))     
Using

P

i ai =

P
i bi  maxi ai =bi  6, we

k
X
i=1

bi

obtain

E (w(F ))  6  E (w(F )):
Hence, E (w(F ))  6  w(F  ) as claimed. 2
The proof that SingleGuess(G; k) outputs a FVS whose expected size is no more
than 4k (Theorem 2) where k is the size of a minimum FVS is analogous to the proof of

Theorem 3 in the following sense. We assign a weight 1 to all vertices and replace the
reference
to Lemma P
5 by a reference to the following claim: If F is a FVS of a rich graph G,
P
then v2V d(v )  4 v2F d(v ). The proof of this claim is identical to the proof of Lemma 5
except that instead of using Lemma 4 we use Lemma 1.

3.3 The Practical Algorithm

In previous sections we presented several algorithms for nding minimum FVS with high
probability. The description of these algorithms was geared towards analysis, rather than
as a prescription to a programmer. In particular, the number of iterations used within
RepeatedWGuessI(G; c; k) is not changed as the algorithm is run with j < k. This feature
allowed us to regard each call to SingleWGuessI(G; j ) made by RepeatedWGuessI as
an independent process. Furthermore, there is a small probability for a very long run even
when the size of the minimum FVS is small.
We now slightly modify RepeatedWGuessI to obtain an algorithm, termed WRA,
which does not suer from these deciencies. The new algorithm works as follows. Repeat
SingleWGuessI(G; jV j) for min(Max; c 6w(F )) iterations, where w(F ) is the weight of the
lightest WFVS found so far and Max is some specied constant determining the maximum
number of iterations of SingleWGuessI.

ALGORITHM WRA(G; c; Max)
Input: An undirected weighted graph G(V; E ) and constants Max and c > 1
Output: A feedback vertex set F
F SingleWGuessI (G; jV j)
M

min(Max; c 6w(F )); i 1;
While i  M do
1. F 0 SingleWGuessI(G; jV j)
2. If w(F 0 )  w(F ) then

229

Becker, Bar-Yehuda, & Geiger

jV j jE j values
15 25
15 25
15 25
25 55
25 55
25 55
55 125

2{6
2{8
2{10
2{6
2{8
2{10
2{10

size MGA WRA Eq.
3{6
12
81
7
3{6
7
89
4
3{6
6
90
4
7{12
3
95
2
7{12
3
97
0
7{12
0
100 0
17{22
0
100 0
31
652 17

Figure 2: Number of graphs in which MGA or WRA yield a smaller loop cutset. The last
column records the number of graphs for which the two algorithms produced loop
cutsets of the same weight. Each line in the table is based on 100 graphs.

F F 0; M
3. i i + 1;
End fWhileg
Return F

min(Max; c 6w(F ))

Theorem 6 If Max  c6k, where k is the minimal size of a minimum WFVS of an undi-

rected weighted graph G, thenk WRA(G; c; Max) outputs a minimum WFVS of G with probability at least 1 ; (1 ; 61k )c6 .

The proof is an immediate corollary of Theorem 3.
The choice of Max and c depend on the application. A decision-theoretic approach for
selecting such values for any-time algorithms is discussed by Breese and Horvitz (1990).

4. Experimental Results

The experiments compared the outputs of WRA vis-a-vis a greedy algorithm GA and a
modied greedy algorithm MGA (Becker & Geiger, 1996) based on randomly generated
graphs and on some real graphs contributed by the Hugin group (www.hugin.com).
The random graphs are divided into three sets. Graphs with 15 vertices and 25 edges
where the number of values associated with each vertex is randomly chosen between 2 and
6, 2 and 8, and between 2 and 10. Graphs with 25 vertices and 55 edges where the number
of values associated with each vertex is randomly chosen between 2 and 6, 2 and 8, and
between 2 and 10. Graphs with 55 vertices and 125 edges where the number of values
associated with each vertex is randomly chosen between 2 and 10. Each instance of the
three classes is based on 100 random graphs generated as described by Suermondt and
Cooper (1990). The total number of random graphs we used is 700.
The results are summarized in the table of Figure 2. WRA is run with Max = 300 and
c = 1. The two algorithms, MGA and WRA, output loop cutsets of the same size in only
230

Randomized Algorithms for the Loop Cutset Problem

Name jV j
Water 32
Mildew 35
Barley 48
Munin1 189

jEj jF j

123
80
126
366

16
14
20
59

GA MGA WRA
40.7 42.7 29.5
48.1 40.5 39.3
72.1 76.3 57.3
159.4 167.5 122.6

Figure 3: Log size (base 2) of the loop cutsets found by GA, MGA, and WRA.
17 graphs and when the algorithms disagree, then in 95% of these graphs WRA performed
better than MGA.
The actual run time of WRA(G; 1; 300) is about 300 times slower than GA (or MGA)
on G. On the largest random graph we used, it took 4.5 minutes. Most of the time is spend
in the last improvement of WRA. Considerable run time can be saved by letting Max = 5.
For all 700 graphs, WRA(G,1,5) has already obtained a better loop cutset than MGA. The
largest improvement, with Max = 300, was from a weight of 58.0 (log2 scale) to a weight
of 35.9. The improvements in this case were obtained in iterations 1, 2, 36, 83, 189 with
respective weights of 46.7, 38.8, 37.5, 37.3, 35.9 and respective sizes of 22, 18, 17, 18, and
17 nodes. On the average, after 300 iterations, the improvement for the larger 100 graphs
was from a weight of 52 to 39 and from size 22 to 20. The improvement for the smaller 600
graphs was from a weight of 15 to 12.2 and from size 9 to 6.7.
The second experiment compared between GA, MGA and WRA on four real Bayesian
networks showing that WRA outperformed both GA and MGA after a single call to SingleWGuessI. The weight of the output continued to decrease logarithmically with the
number of iterations. We report the results with Max = 1000 and c = 1. Run time was
between 3 minutes for Water and 15 minutes for Munin1 on a Pentium 133 with 32M RAM.

5. Discussion

Our randomized algorithm, WRA, has been incorporated into the popular genetic software
FASTLINK 4.1 by Alejandro Schaer who develops and maintains this software at the
National Institute of Health. WRA replaced previous approximation algorithms for nding
FVS because with a small Max value it already matched or improved FASTLINK 4.0 on
most datasets examined. The datasets used for comparison are described by Becker et
al. (1998). The main characteristics of these datasets is that they were all collected by
geneticists, they have a small number of loops, and a large number of values at each node
(tens to hundreds depending on the genetic analysis). For such networks the method of
conditioning is widely used by geneticists.
The leading inference algorithm, however, for Bayesian networks is the clique-tree algorithm (Lauritzen & Spiegelhalter, 1988) which has been further developed in several papers
(Jensen, Lauritzen, & Olsen, 1990a; Jensen, Olsen, & Andersen, 1990b). For the networks
presented in Table 3 conditioning is not a feasible method while the clique tree algorithm
can and is being used to compute posterior probabilities in these networks. Furthermore,
it has been shown that the weight of the largest clique is bounded by the weight of the
loop cutset union the largest parent set of a vertex in a Bayesian network implying that the
231

Becker, Bar-Yehuda, & Geiger

clique tree algorithm is always superior in time performance over the conditioning algorithm
(Shachter, Andersen, & Szolovits, 1994). The two methods, however, can be combined to
strike a balance between time and space requirements as done within the bucket elimination
framework (Dechter, 1999).
The algorithmic ideas behind the randomized algorithms presented herein can also be
applied for constructing good clique trees and initial experiments conrm that an improvement over deterministic algorithms is often obtained. The idea is that instead of greedily
selecting the smallest clique when constructing a clique tree, one would randomly select the
next clique according to the relative weights of the candidate cliques. It remains to develop
the theory behind random choices of clique trees before a solid assessment can be presented.
Currently, there is no algorithm for nding a clique tree such that its size is guaranteed to
be close to optimal with high probability.
Horvitz et al. (1989) show that the method of conditioning can be useful for approximate
inference. In particular, they show how to rank the instances of a loop cutset according
to their prior probabilities assuming all variables in the cutset are marginally independent.
The conditioning algorithm can then be run according to this ranking and the answer to
a query be given as an interval that shrinks towards the exact solution as more instances
of the loop cutset are considered (Horvitz, Suermondt, & Cooper, 1989; Horvitz, 1990).
Applying this idea without making independence assumptions is described by Darwiche
(1994). So if the maximal clique is too large to store one can still perform approximate
inferences using the conditioning algorithm.

Acknowledgment
We thank Se Naor for fruitful discussions. Part of this work was done while the third
author was on sabbatical at Microsoft Research. A variant of this work has been presented
at the fteenth conference on uncertainty in articial intelligence, July 1999, Sweden.

References
Bafna, V., Berman, P., & Fujito, T. (1995). Constant ratio approximations of the weighted
feedback vertex set problem for undirected graphs. In Proceedings of the Sixth Annual
Symposium on Algorithms and Computation (ISAAC95), pp. 142{151.
Bar-Yehuda, R., Geiger, D., Naor, J., & Roth, R. (1994). Approximation algorithms for the
feedback vertex set problems with applications to constraint satisfaction and Bayesian
inference. In Proceedings of the 5th Annual ACM-Siam Symposium On Discrete Algorithms, pp. 344{354.
Becker, A., & Geiger, D. (1994). Approximation algorithms for the loop cutset problem. In
Proceedings of the 10th conference on Uncertainty in Articial Intelligence, pp. 60{68.
Becker, A., & Geiger, D. (1996). Optimization of pearl's method of conditioning and greedylike approximation algorithms for the feedback vertex set problem. Articial Intelligence, 83, 167{188.
232

Randomized Algorithms for the Loop Cutset Problem

Becker, A., Geiger, D., & Schaer, A. (1998). Automatic selection of loop breakers for
genetic linkage analysis. Human Heredity, 48, 47{60.
Bodlaender, H. (1990). On disjoint cycles. International Journal of Foundations of Computer Science (IJFCS), 5, 59{68.
Breese, J., & Horvitz, E. (1990). Ideal reformulation of belief netwroks. In Proceedings of
the 6th conference on Uncertainty in Articial Intelligence, pp. 64{72.
Darwiche, A. (1994). -bounded conditioning: A method for the approximate updating of
causal networks. Research note, Rockwell Science Center.
Dechter, R. (1990). Enhancement schemes for constraint processing: backjumping, learning,
and cutset decomposition. Articial Intelligence, 41, 273{312.
Dechter, R. (1999). Bucket elimination: A unifying framework for structure-driven inference. Articial Intelligence, To appear.
Downey, R., & Fellows, M. (1995a). Fixed-parameter tractability and completeness I: Basic
results. SIAM Journal on Computing, 24 (4), 873{921.
Downey, R., & Fellows, M. (1995b). Parameterized computational feasibility. In P. Clote, .
J. R. (Ed.), Feasible Mathematics II, pp. 219{244. Birkhauser, Boston.
Fujito, T. (1996). A note on approximation of the vertex cover and feedback vertex set
problems - unied approach. Information Processing Letters, 59, 59{63.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the Theory of
NP-completeness. W. H. Freeman, San Francisco, California.
Geiger, D., & Pearl, J. (1990). On the logic of causal models. In Uncertainty in Articial
Intelligence 4, pp. 3{14 New York. North-Holland.
Geiger, D., Verma, T., & Pearl, J. (1990). Identifying independence in bayesian networks.
Networks, 20, 507{534.
Horvitz, E. J. (1990). Computation and action under bounded resources. Ph.D dissertation,
Stanford university.
Horvitz, E. J., Suermondt, H. J., & Cooper, G. H. (1989). Bounded conditioning: Flexible
inference for decisions under scarce resources. In Proceedings of 5th conference on
Uncertainty in Articial Intelligence, pp. 182{193. Morgan Kaufmann.
Jensen, F., Lauritzen, S. L., & Olsen, K. (1990a). Bayesian updating in causal probabilisitic
networks by local computations. Computational Statistics Quarterly, 4, 269{282.
Jensen, F., Olsen, K., & Andersen, S. (1990b). An algebra of bayesian belief universes for
knowledge-based systems. Networks, 20, 637{659.
Kim, H., & Pearl, J. (1983). A computational model for combined causal and diagnostic reasoning in inference systems. In Proceedings of the Eighth International Joint
Conference on Articial Intelligence (IJCAI83), pp. 190{193.
233

Becker, Bar-Yehuda, & Geiger

Lang, K. (1997). Mathematical and statistical methods for genetic analysis. Springer.
Lange, K., & Elston, R. (1975). Extensions to pedigree analysis. I. likelihood calculation
for simple and complex pedigrees. Human Heredity, 25, 95{105.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations with probabilities on graphical
structures and their application to expert systems (with discussion). Journal Royal
Statistical Society, B, 50, 157{224.
Ott, J. (1991). Analysis of human genetic linkage (revised edition). The Johns Hopkins
University Press.
Pearl, J. (1986). Fusion, propagation and structuring in belief networks. Articial Intelligence, 29, 241{288.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference. Morgan Kaufmann, San Mateo, California.
Peot, M., & Shachter, R. (1991). Fusion and propagation with multiple observations in
belief networks. Articial Intelligence, 48, 299{318.
Shachter, R., Andersen, S., & Szolovits, P. (1994). Global conditioning for probabilistic
inference in belief networks. In Proceedings of the tenth conference on Uncertainty in
Articial Intelligence, pp. 514{522. Morgan Kaufmann.
Suermondt, H., & Cooper, G. (1990). Probabilistic inference in multiply connected belief
networks using loop cutsets. International Journal of Approximate Reasoning, 4, 283{
306.
Verma, T., & Pearl, J. (1988). Causal networks: Semantics and expressiveness. In Proceedings of 4th Workshop on Uncertainty in Articial Intelligence, pp. 352{359.
Voss, H. (1968). Some properties of graphs containing k independent circuits. In Proceedings
of Colloquium Tihany, pp. 321{334 New York. Academic Press.

234

Journal of Articial Intelligence Research 12 (2000) 93-103

Submitted 11/99; published 3/00

Exact Phase Transitions in
Random Constraint Satisfaction Problems
kexu@nlsde.buaa.edu.cn
liwei@nlsde.buaa.edu.cn

Ke Xu
Wei Li
National Laboratory of Software Development Environment,
Department of Computer Science and Engineering,
Beijing University of Aeronautics and Astronautics,
Beijing, 100083, P.R. China

Abstract
In this paper we propose a new type of random CSP model, called Model RB, which is
a revision to the standard Model B. It is proved that phase transitions from a region where
almost all problems are satisable to a region where almost all problems are unsatisable
do exist for Model RB as the number of variables approaches innity. Moreover, the critical
values at which the phase transitions occur are also known exactly. By relating the hardness
of Model RB to Model B, it is shown that there exist a lot of hard instances in Model RB.

1. Introduction
Since the seminal paper of Cheeseman, Kanefsky and Taylor (1991) appeared, there has
been a great amount of interest in the study of phase transitions in NP-complete problems.
However, it seems to be very diÆcult to prove the existence of this phenomenon or to obtain
the exact location of the transition points for such problems. For example, in random 3SAT, it is known from experiments that the phase transition will occur when the ratio of
clauses to variables is approximately 4:3 (Mitchell, Selman, & Levesque, 1992). Another
experimental estimate of the transition point suggested by Kirkpatrick and Selman (1994)
is 4:17. They used nite-size scaling methods from statistical physics to derive the result. In
contrast with the experimental studies, the theoretical work has only given some loose but
hard won bounds on the location of the transition point. Currently, the best known lower
bound and upper bound are 3:003 (Frieze & Suen, 1996) and 4:602 (Kirousis et al., 1998)
respectively. Recently, Friedgut (1999) made tremendous progress towards establishing the
existence of a threshold for random k-SAT by proving that the width of the transition region
narrows as the number of variables increases. But we still can not obtain the exact location
of the phase transition point from this approach.
In fact, SAT is a special case of the constraint satisfaction problem (CSP). CSP has not
only important theoretical value in articial intelligence, but also many immediate applications in areas ranging from vision, language comprehension to scheduling and diagnosis
(Dechter, 1998). In general, CSP tasks are computationally intractable (NP-hard) (Dechter,
1998). In recent years random constraint satisfaction problems have also received great attention, both from an experimental and a theoretical point of view (Achlioptas et al., 1999;
Cheeseman et al., 1991; Frost & Dechter, 1994; Gent et al., 1999; Hogg, 1996; Larrosa &
Meseguer, 1996; Prosser, 1996; Purdom, 1997; Smith & Dyer, 1996; Smith, 1999; Williams

c 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Xu & Li
& Hogg, 1994). Williams and Hogg (1994) developed a technique to predict where the
hardest problems are to be found and where the uctuations in diÆculty are greatest in a
space of problem instances. They have also shown that their predictions of the critical value
agree well with the experimental data. Smith and Dyer (1996) studied the location of the
phase transition in binary constraint satisfaction problems and discussed the accuracy of a
prediction based on the expected number of solutions. Their results show that the variance
of the number of solutions can be used to set bounds on the phase transition and to indicate
the accuracy of the prediction. Recently, a theoretical result by Achlioptas et al. (1999)
shows that many models commonly used for generating random CSP instances do not have
an asymptotic threshold due to the presence of awed variables. More recently, Gent et al.
(1999) have shown how to introduce structure into the conict matrix to eliminate aws.
In this paper we propose a new type of random CSP model, called Model RB, which
is a revision to the standard Model B (Gent et al., 1999; Smith & Dyer, 1996). It is
proved that the phase transition phenomenon does exist for Model RB as the number of
variables approaches innity. More precisely, there exist two control parameters r, p and
the corresponding critical values rcr , pcr such that for each xed value r < rcr or p < pcr , a
random CSP instance generated following Model RB is satisable with probability tending
to 1 as the number of variables approaches innity, and when r > rcr or p > pcr , unsatisable
with probability tending to 1. Moreover, the critical values rcr and pcr are also known
exactly. By relating the hardness of Model RB to Model B, it is shown that Model RB
actually has a lot of hard instances.

2. Denitions and Notations
A constraint satisfaction problem (CSP) consists of a nite set U = fu1 ;    ; un g of n
variables and a set of constraints. For each variable ui , a domain Di with di elements
is specied; a variable can only be assigned a value from its domain. For 2  k  n a
constraint Ci1;i2;;ik consists of a subset fui1 ; ui2 ;    ; uik g of U and a relation Ri1;i2;;ik 
Di1    Dik , where i1; i2;    ; ik are distinct. Ci1;i2;;ik is called a k-ary constraint which
bounds the variables ui1 ;    ; uik . Ri1;i2;;ik species all the allowed tuples of values for
the variables ui1 ;    ; uik which are compatible with each other. A solution to a CSP is an
assignment of a value to each variable from its domain such that all the constraints are
satised. A constraint Ci1;i2;;ik is satised if the tuple of values assigned to the variables
ui1 ;    ; uik is in the relation Ri1;i2;;ik . A CSP that has a solution is called satisable;
otherwise it is unsatisable. In this paper, the probability of a random CSP instance being
satisable is denoted by P r(Sat).
We assume that k  2 and all the variable domains contain the same number of values
d = n in Model RB (where  is a constant). The generation of a random CSP instance in
Model RB is done in the following two steps:
Step 1. We select with repetition t = rn ln n random constraints. Each random constraint
is formed by selecting without repetition k of n variables.
Step 2. For each constraint we uniformly select without repetition q = p  dk incompatible
tuples of values, i.e., each constraint relation contains exactly (1 p)  dk compatible tuples
of values.
94

Exact Phase Transitions in Random Constraint Satisfaction Problems
The parameter r determines how many constraints are in a CSP instance, while p
determines how restrictive the constraints are.
The following denitions will be needed in section 4 when we derive the expectation of
the second moment of the number of solutions.

Denition 1 An assignment pair is an ordered pair hti ; tj i of assignments to the variables
in U , where ti = (ai1 ; ai2 ;    ; ain ) and tj = (aj 1 ; aj 2 ;    ; ajn ) with ail ; ajl 2 Dl . An assignment pair hti ; tj i satises a CSP if and only if both ti and tj satisfy this CSP. The set that
consists of all the assignment pairs is denoted by Apair .
Denition 2 Similarity number S f : Apair 7! f0; 1; 2;   g,
f
S (ht

i ; tj i) =

n
X

( il ; ajl )

(1)

if ail = ajl
if ail 6= ajl

(2)

S am a

l=1

where the function Sam is dened as follows:


( il ; ajl ) =

S am a

1
0

The similarity number of an assignment pair is equal to the number of variables at which
the two assignments of this assignment pair take the identical values. By Denition 2 it is
easy to see that 0  S f (hti ; tj i)  n.

3. Main Results
In this paper, the following theorems are proved.

Theorem 1 Let rcr =
inequality k  1 1 p , then



ln(1 p) .

If  > k1 , 0 < p < 1 are two constants and k, p satisfy the

lim

P r S at

(

) = 1 when

r < r

lim

P r S at

(

) = 0 when

r > r

n!1
n!1

cr

(3)

cr

(4)



Theorem 2 Let pcr = 1 e r . If  > k1 , r > 0 are two constants and k,  and r satisfy

the inequality ke r  1, then
lim

P r S at

lim

P r S at

n!1
n!1

(

) = 1 when

p < p

cr

(5)

(

) = 0 when

p > p

cr

(6)

4. Proof of Theorem 1 and Theorem 2
The expected number of solutions E (N ) for model RB is given by
( ) = dn (1

E N

)rn ln n = nn (1

p

)rn ln n

p

(7)

By the Markov inequality P r(Sat)  E (N ) it is not hard to show that limn!1 P r(Sat) = 0
when r > rcr or p > pcr . Hence relations (4), (6) are proved. It is also easy to see that
95

Xu & Li
E (N ) is eqal to 1 when r = rcr or p = pcr , and E (N ) grows exponentially with n when
r < rcr or p < pcr .

The key point in the proof of relations (3), (5) is to derive the expectation of the second
moment E (N 2 ) and give an asymptotic estimate of it. Let  be a random CSP instance
generated following Model RB. P (hti ; tj i) stands for the probability of hti ; tj i satisfying .
Now we start to derive the expression of P (hti ; tj i). Since each constraint is generated
independently, we only need to consider the probability of hti ; tj i satisfying a random constraint. Assuming that the similarity number of hti ; tj i is equal to S , we have the following
two cases:
(1) Each variable of a constraint is assigned the same value in ti as that in tj . In this
 k
d 1 = dk .
case, the probability of hti ; tj i satisfying the constraint is

q

(2) Otherwise, the probability of hti ; tj i satisfying a constraint is




The probability that a random constraint falls into the rst case is

0
B

P

(hti ; tj i) = B
@

k

1

d





q

k 

d

S = n . Thus we get
k
k



the probability into the second case is 1








S
k



n

+

d



k



q

k 

d

k

q

2



 (1



S
k
n

q

dk

dk .
=
q
q
 

S = n . Hence
k
k
 

2

 1rn ln n
C
C
 )A

(8)

k

q

Let AS be the set of assignment pairs whose similarity number is equal to S . It is easy
to show that the cardinality of AS is given by

j Sj =

n

A



d

n



1)n S

(d

S

(9)

From the denition of E (N 2 ), we have

E (N 2 ) =

n
X
S =0

= dn

jAS jP (ti ; tj )



0

n (d 1)n
S

B

SB
B
@

dk

1





q
dk
q







S
k
n
k






+

dk

2





q
dk
q





 (1



S  rn ln n
k C
)C
C (10)
n A
k
1

It is very diÆcult to analyze the above expression directly. First, we give an asymptotic
estimate of P (hti ; tj i). Let s = Sn . It is obvious that 0  s  1. By asymptotic analysis, we
get




S
k
n
k



S S



(
= n n
(1

1 S
n )( n

1

n )(1

2
S
n)    (n

n )    (1
2

96

k 1)
1
k g (s)
n
k 1 ) = s + n + O ( n2 )
n

Exact Phase Transitions in Random Constraint Satisfaction Problems
where
and

k





k

1)

s

(11)



1

d

1)(sk
2

(

k k

( )=

g s

q

=

k 
d

k

d

k

q

=1

d

(12)

p

q

k







2

d

q

k
d

(dk

=



)(dk q
k k 1)
d (d

1)

q

= (1

1
)2 + O ( k )
d

(13)

p

q

Note that d = n , we have


P

(hti ; tj i) = (1

)  (sk +

p

( )

g s
n

)2  (1

) + (1

k

p

s

( )

g s
n

) + O(


1 rn ln n
)
+
O(
)
2
k
n
n

1

(14)

By use of the condition  > k1 , we get
P

(hti ; tj i) = (1

p

2rn ln n



)

1+

p

1

p

rn ln n
g (s)
1
k
(s +
)
(1 + O( ))
n

(15)

n

For every 0 < s < 1 (where s = Sn ), the asymptotic estimate of jAS j is

jAS j

1
1
en( s ln s (1 s) ln(1 s)) (1 + O( ))
n
2ns(1 s)
1
1
1 n ns 1 ns
)
( ) p
en( s ln s (1 s) ln(1 s)) (1 + O( )) (16)
n
n
n
2ns(1 s)

= nn (n

1)n ns p

= n2n (1

Notice that E (N ) = nn(1

j S j (h i j i) =
A

P

t ;t

E

2

p)rn ln n , we have



(N ) 1 +

p

1

p

g (s)
(sk +
)

rn ln n

n

(1

1 n ns 1 ns 
( )
)
n
n

n
ns



1
(1 + O( )) (17)
n

When n is suÆciently large, except the rst term E 2 (N ), jAS jP (hti ; tj i) is mainly determined by the following terms:


f (s) = 1 +
We can rewrite it as
Let



f (s) = e

1

p

p

h00 (s) =

rn ln n

1
(  )ns
n



r ln(1+ 1 p p sk ) s n ln n

h(s) = r ln(1 +

The second derivative of h(s) is

sk

1

p

p

sk ) s

rkpsk 2[(k 1)(1 p) psk ]
(1 p + psk )2
97

(18)
(19)
(20)

(21)

Xu & Li
Applying the condition k  1 1 p in Theorem 1 to the above equation we can easily prove

that h00 (s)  0 on the interval 0  s  1. For Theorem 2, from the condition ke r  1 it
follows that the inequality k  1 1 p still holds when p < pcr . It is also not hard to show that
h(0) = 0, and h(1) = r ln(1 p)  < 0 when r < rcr or p < pcr . Hence we can easily
prove that the unique maximum point of h(s) is s = 0 when r < rcr or p < pcr . Thus the
terms of 0 < s  1 are negligible when r < rcr or p < pcr . We only need to consider those
terms near s = 0 . The process can be divided into the following three cases:
Case 1:  > 1. When S = 0 (s = 0), from the denition of g(s) in Equation (11) we
have

p k g(s) rn ln n
1+
(s +
)
=1
(22)
1 p
n
Thus by Equation (17) we get

jAS jP (hti ; tj i)  E 2 (N )(1 n1 )n  E 2(N )

(23)

When S = 1 (s = n1 ), it also not hard to prove that


lim 1 +
n!1
1
Hence we obtain
Similary,

p

p

(sk +

g(s)
)
n

rn ln n

jAS jP (hti ; tj i)  E 2 (N )n1

 when

2(1 )

jAS jP (hti ; tj i)  E 2 (N ) n

2!

3(1 )

jAS jP (hti; tj i)  E 2 (N ) n
Summing the above terms together, we obtain

E (N 2 ) =

n
X
S =0

3!

= e0 = 1

S=1

(24)
(25)

when S = 2

when S = 3;   

(26)



(27)

jAS jP (hti ; tj i)  E 2 (N )en

1

 E 2 (N )

Case 2:  = 1. By use of the method in Case 1, it can be easily shown that

jAS jP (hti; tj i)  E 2 (N )(1 n1 )n  E 2(N ) 1e when S = 0
jAS jP (hti ; tj i)  E 2 (N ) 1e when S = 1
jAS jP (hti ; tj i)  E 2 (N ) e 12! when S = 2

jAS jP (hti ; tj i)  E 2 (N ) e 13! when S = 3;   
98

(28)

Exact Phase Transitions in Random Constraint Satisfaction Problems
Summing the above terms together, we obtain

E (N 2 ) =

n
X
S =0

jAS jP (hti ; tj i)  E 2 (N ) 1e  e = E 2(N )

(29)

<  < 1. Let S0 = n (where  is a constant and satises 1  <  < 1 k1 ).
1
It is not hard to show that when 0  S  S0 (0  s  n 1 < n k ), the following limit
Case 3:

1

k

holds:

lim

n!1 1

p

p

g(s)
)  n ln n = 0
n

(sk +

(30)

Thus when 0  S  S0 , the asymptotic estimate of the second term in the right of Equation
(17) is


p k g(s) rn ln n 0
1+
(s +
)
 e = 1 when n ! 1
(31)
1 p
n
So when 0  S  S0 , the asymptotic estimate of jAS jP (hti ; tj i) is

jAS jP (hti ; tj i)  E 2 (N ) Sn



(1

1 n S 1 S
) ( )
n
n

(32)

n (1 1 )n S ( 1 )S is a binomial term whose maximum point
n
n
S
is around S = n1  , and S0 = n > n1  . By asymptotic analysis, we obtain
It should be noted that
S0 
X
S =0



n (1
S

1 n S 1 S
) ( )
n
n

Thus we get

E (N 2 ) =

n
X
S =0



n 
X
S =0

n (1
S

1 n S 1 S
) ( ) = 1
n
n

jAS jP (hti ; tj i)  E 2(N )

(33)

(34)

Combining the above three cases gives

Hence

E (N 2 )  E 2 (N ) when r < rcr or when p < pcr

(35)

E 2 (N )
lim
= 1 when r < rcr or when p < pcr
n!1 E (N 2 )

(36)

By the Cauchy inequality P r(Sat)  EE (N(N2 )) (Bollobas, 1985), it can be easily proved
that limn!1 P r(Sat) = 1 when r < rcr or p < pcr . Hence Theorem 1 and Theorem 2 are
proved.
2

99

Xu & Li
5. The Relation between Model B and Model RB
In this section we will explain in detail how Model RB revises Model B and show the
hardness of Model RB by relating it to Model B. From the previous papers (Gent et al.,
1999; Smith & Dyer, 1996) we know that the generation of a random CSP instance in the
standard Model B (which is often written as hn; d; p1 ; p2 i) is done in the following two steps:
Step 1. We select with repetition t = p1 n(n2 1) random constraints. Each random constraint
is formed by selecting without repetition 2 of n variables.
Step 2. For each constraint we uniformly select without repetition q = p2  d2 incompatible
tuples of values, i.e., each constraint relation contains exactly (1 p2 )  d2 compatible tuples
of values.
Since the standard Model B is a binary CSP model, we will only consider the binary
case of Model RB in this section. In the previous papers Model B was used to test the
CSP algorithms in the following way. Given the values of n, d and p1 , the constraint
tightness p2 was varied from 0 to 1 in steps of d12 . At each setting of hn; d; p1 ; p2 i a xed
number of instances (e.g. 100) were generated. The search algorithm was then applied
to each instance. Finally numerous statistics about the search cost and the probability of
being satisable were gathered. In fact, the two steps of forming a constraint and selecting
incompatible tuples of values in Model RB is exactly the same as those in Model B. The
signicant dierence between Model B and Model RB is that Model RB restricts how fast
the domain size and the number of constraints increase with the number of variables while
Model B does not, which may lead to the result that many instances of Model B suer from
being asymptotically trivially insoluble (Achlioptas et al., 1999) while Model RB avoids this
problem. But it is easy to see that given the values of n, d and p1 , for the setting hn; d; p1 ; p2 i
of Model B there is an equivalent setting in Model RB with the same number of variables
ln d and r = p1 (n 1) (Let n = d and rn ln n = 1 p n(n 1)).
as that in hn; d; p1 ; p2 i,  = ln
n
2 ln n
2 1
Theorem 2 shows that if  > 12 and 2e r  1, then there exists an exact phase transition
in the binary case of Model RB. Given the values of n, d and p1 in Model B, for the setting
of hn; d; p1 ; p2 i, the conditions that the equivalent setting in Model RB satises Theorem 2
are
ln d 1
=
> ) d2 > n
(37)
ln n 2
ln d
2 ln n

2 ln d
2e r  1 ) 2e ln n  p1 (n 1)  1 ) p1 
(38)
(n 1) ln 2
The proof of Theorem 2 reveals that if the conditions (37), (38) are satised, then Model
RB will exhibit an exact phase transition at E (N ) = 1. It should be noted that Williams
and Hogg (Williams & Hogg, 1994), and independently Smith (1996) have already developed
a theory to predict the phase transition point in Model B on the basis of E (N ) = 1. Prosser
(1996) found that this theory is in close agreement with the empirical results, except when
p1 is small. Inequality (38) shows that in order to make the equivalent setting in Model RB
satisfy the conditions of Theorem 2, the parameter p1 in Model B should not be less than
a certain value, which is consistent with Prosser's experimental nding.
ln 10 
Now we consider a typical setting h20; 10; 0:5; p2 i of Model B. Let n = 20,  = ln
20
1)

1
:
5856
in
Model
RB.
Then
the
setting
of
Model
RB
with
such
0:7686 and r = 0:5(20
2 ln 20
values is equivalent to the setting h20; 10; 0:5; p2 i of Model B. From Inequalities (37) and
100

Exact Phase Transitions in Random Constraint Satisfaction Problems
(38) it is also not hard to show that the equivalent setting in Model RB corresponding to
the setting h20; 10; 0:5; p2 i satises the conditions of Theorem 2, i.e., 102 > 20 and p1 =
0:5  (202 ln1)10ln 2  0:35. The experiments done by Prosser (1996) show that the instances
generated at p2 = 0:38 are very hard to solve. This maximum cost point
also agrees
well with

0:7686
r
1
:
5856
the asymptotic phase transition point of Model RB that is p = 1 e  1 e
 0:38.
For some other settings of Model B in the previous work, we can also nd their equivalent
settings in Model RB using this method. Thus the hardness of solving these settings of
Model B is equal to that of solving their equivalent settings in Model RB. From many
previous studies (Gent et al., 1999; Smith & Dyer, 1996; Prosser, 1996) we know that the
instances generated at the phase transition in many settings of Model B are very hard to
solve for various kinds of CSP algorithms. So there exist a lot of hard instances to solve in
Model RB.

6. Conclusions and Future Work
A lot of experimental and theoretical studies indicate that a phase transition in solvability
is a very important feature of many decision problems in computer science. It is shown
that these problems can be characterized by a control parameter in such a way that the
space of problem instances is divided into two regions: the under-constrained region where
almost all problems have many solutions, and the over-constrained region where almost all
problems have no solutions, with a sharp transition between them. Another interesting
feature associated with the phase transition is that the peak in hardness of solving the
problem instances occurs in the transition region. Since these instances generated in the
transition region appear hardest to solve, they are commonly used as a benchmark for
algorithms for many NP-complete problems. But unfortunately, except for the Hamiltonian
cycle problem (which is NP-complete), all the decision problems that have exact results
about the existence and the location of the phase transition are in P class (Parkes, 1997),
e.g. random 2-SAT. These problems are not so interesting as the NP-complete problems
from a complexity theoretic point of view because they can be solved in polynomial time. For
the Hamiltonian cycle problem, using an improved backtrack algorithm with sophisticated
pruning techniques, Vandegriend and Culberson (1998) recently found that the problem
instances in the phase transition region are not hard to solve.
In this paper we proposed a new type of random CSP model, Model RB, which is a
revision to the standard Model B, and the asymptotic analysis of this model has also been
presented. The results are quite surprising. We can not only prove the existence of phase
transitions in this model but also know the location of transition points exactly. It was
further shown that there exist a lot of hard instances in Model RB by relating its hardness
to the standard Model B. Since there is still some lack of studies about the exact derivation
of the phase transitions in NP-complete problems, this paper may provide some new insight
into this eld. However, we did not discuss the scaling behaviour of Model RB and some
other related issues in this paper. In order to get a better understanding of Model RB,
we suggest that future work should include determining either empirically or theoretically
whether or not hard instances persist with reasonably high frequency as the number of
variables increases. 1
1. Two anonymous referees suggest this point.

101

Xu & Li
Acknowledgments
This research was supported by National 973 Project of China Grant No. G1999032701.
We would like to thank Ian P. Gent, Barbara M. Smith, Peter van Beek and the anonymous
referees for their helpful comments and suggestions.

References
Achlioptas, D., Kirousis, L., Kranakis, E., Krizanc, D., Molloy, M., & Stamatiou, Y. (1999).
Random constraint satisfaction: A more accurate picture. Constraints. submitted.
Also in Proc. Third International Conference on Principles and Practice of Constraint
Programming (CP 97), Springer-Verlag, pp. 107{120, 1997.
Bollobas, B. (1985). Random Graphs. Academic Press, New York.
Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). Where the really hard problems are. In
Proceedings of IJCAI-91, pp. 331{337.
Dechter, R. (1998). Constraint satisfaction. In MIT Encyclopedia of the Cognitive Sciences
(MITECS). Online at \ ftp://ftp.ics.uci.edu/pub/CSP-repository/papers/R68.ps".
Friedgut, E. (1999). Sharp thresholds for graph properties and the k-sat problem. Journal
of the American Mathematical Society, 12, 1017{1054.
Frieze, A., & Suen, S. (1996). Analysis of two simlpe heuristics on a random instance of
k-sat. Journal of Algorithms, 53, 469{486.
Frost, D., & Dechter, R. (1994). In search of the best constraint satisfaction search. In
Proceedings of AAAI-94, pp. 301{306.
Gent, I., MacIntyre, E., Prosser, P., Smith, B., & Walsh, T. (1999). Random constraint satisfaction: Flaws and structure. Constraints. submitted. Online at \
http://www.cs.strath.edu.uk/~apes/apesreports.html".
Hogg, T. (1996). Rening the phase transition in combinatorial search. Articial Intelligence, 81, 127{154.
Kirkpatrick, S., & Selman, B. (1994). Critical behavior in the satisability of random
boolean expressions. Science, 264, 1297{1301.
Kirousis, L., Kranakis, E., Krizanc, D., & Stamatiou, Y. (1998). Approximating the unsatisability threshold of random formulas. Random Structures and Algorithms, 12,
253{269.
Larrosa, J., & Meseguer, P. (1996). Phase transition in max-csp. In Proceedings of ECAI-96,
pp. 190{194.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of sat problems. In Proceedings of AAAI-92, pp. 459{465.
102

Exact Phase Transitions in Random Constraint Satisfaction Problems
Parkes, A. (1997). Clustering at the phase transition. In Proceedings of AAAI-97, pp.
340{345.
Prosser, P. (1996). An empirical study of phase transitions in binary constraint satisfaction
problems. Articial Intelligence, 81, 81{109.
Purdom, P. (1997). Backtracking and random constraint satisfaction. Annals of Mathematics and Articial Intelligence, 20, 393{410.
Smith, B. (1999). Constructing an Asymptotic Phase Transition in Random Binary Constraint Satisfaction Problems. Extended Abstract.
Smith, B., & Dyer, M. (1996). Locating the phase transition in binary constraint satisfaction
problems. Articial Intelligence, 81, 155{181.
Vandegriend, B., & Culberson, J. (1998). The Gn;m phase transition is not hard for the
hamiltonian cycle problem. Journal of Articial Intelligence Research, 9, 219{245.
Williams, C., & Hogg, T. (1994). Exploiting the deep structure of constraint problems.
Articial Intelligence, 70, 73{117.

103

Journal of Artificial Intelligence Research 12 (2000) 35-86

Submitted 5/99; published 2/00

Reasoning on Interval and Point-based
Disjunctive Metric Constraints in Temporal Contexts
Federico Barber

FBARBER@DSIC.UPV.ES

Dpto. de Sistemas Informáticos y Computación
Universidad Politécnica de Valencia
Camino de Vera s/n, 46022 Valencia, Spain

Abstract
We introduce a temporal model for reasoning on disjunctive metric constraints on intervals and
time points in temporal contexts. This temporal model is composed of a labeled temporal algebra
and its reasoning algorithms. The labeled temporal algebra defines labeled disjunctive metric pointbased constraints, where each disjunct in each input disjunctive constraint is univocally associated
to a label. Reasoning algorithms manage labeled constraints, associated label lists, and sets of
mutually inconsistent disjuncts. These algorithms guarantee consistency and obtain a minimal
network. Additionally, constraints can be organized in a hierarchy of alternative temporal contexts.
Therefore, we can reason on context-dependent disjunctive metric constraints on intervals and
points. Moreover, the model is able to represent non-binary constraints, such that logical
dependencies on disjuncts in constraints can be handled. The computational cost of reasoning
algorithms is exponential in accordance with the underlying problem complexity, although some
improvements are proposed.

1. Introduction
Two main lines of research are commonly recognized in the temporal reasoning area. The first
approach deals with reasoning about temporal constraints on time-dependent entities. The goal is to
determine what consequences (T) follow from a set of temporal constraints, "{TemporalConstraints}|=T?", or to determine whether a set of temporal constraints is consistent, with no
assumptions about properties of temporal facts. The second approach deals with reasoning about
change, events, actions and causality. Here, the goal is to obtain the consequent state from a set of
actions or events which are performed on an initial state: "[Si, {A1, A2, ..., An }]|= Sj?". Both these
approaches constitute active fields of research with applications in several artificial intelligence areas
such as reasoning about change, scheduling, temporal planning, knowledge-based systems, natural
language understanding, etc. In these areas, time plays a crucial role, problems have a dynamic
behavior, and it is necessary to represent and reason about the temporal dimension of information.
In this paper, we deal with the first of these approaches. Our goal is reasoning on qualitative and
quantitative constraints between intervals or time-points in temporal contexts. Moreover, special
cases of non-binary constraints are also managed. These tasks are pending issues in the temporal
reasoning area, as well as important features to facilitate modeling of relevant problems in this area
(including planning, scheduling, causal or hypothetical reasoning, etc.).
Several temporal reasoning models have been defined in the literature, with a clear trade-off
between representation expressiveness and complexity of reasoning algorithms. Qualitative Point
Algebra (PA) (Vilain, Kautz & Van Beek, 1986) is a limited subset of interval-based models. Interval

© 2000 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

Algebra (IA) introduced by Allen (1983) represents symbolic (qualitative) constraints between
intervals but metric information, such as 'interval1 starts 2 seconds before interval2 ', cannot be
included. Metric (quantitative) point-based models (Dechter, Meiri & Pearl, 1991) include the 'time
line' (metric) in their constraints, but they can only represent a limited subset of disjunctive
constraints between intervals. Thus, constraints like 'interval1 {bef, aft} interval2 ' cannot be
represented (Gerevini & Schubert, 1995).
Some efforts have been made to integrate qualitative and quantitative temporal information on
points and intervals (Kautz & Ladkin, 1991; Drakengren & Jonsson, 1997; etc.). Particularly, Meiri
(1996) introduces Qualitative Algebra (QA), where each interval is represented by three nodes (one
representing the interval and the other two representing its extreme points) such that QA can
represent qualitative and metric constraints on points and intervals. Badaloni and Berati (1996) define
the Interval Distance Sub Algebra (IDSA), where nodes are intervals. These intervals are related by
disjunctive 4-tuple-metric constraints between their ending time points {(I-i, I-j ), (I+i , I-j ), (I-i , I+j ), (I+i ,
I+j )}. Staab and Hahn (1998) propose a model for reasoning on qualitative and metric boundaries of
intervals. However, these models cannot handle constraints on interval durations, which were
identified earlier by Allen (1983). Constraints such as 'interval1 lasts 2 seconds more than interval2'
require a high-order expression (Dechter et al., 1991), or a duration primitive which should be
integrated with interval and point constraints (Allen, 1983; Barber, 1993). Particularly, Barber (1993)
proposes two orthogonal networks to relate constraints on durations and time points. Navarrete
(1997) and Wetprasit and Sattar (1998) relate disjunctive constraints on durations and time points,
but only a limited subset of interval constraints is managed. More recently, Pujari and Sattar (1999)
propose a framework for reasoning on points, intervals and durations (PIDN). Here, variables
represent points or intervals, and constraints are an ordered set of three intervals representing (Start,
End, Duration) subdomains. However, no specialized algorithms for management of PIDN
constraints are proposed.
In relation to the complexity of reasoning algorithms, the consistency problem is polynomial in
PA (Vilain, Kautz & Van Beek, 1986) and in non-disjunctive metric networks (Dechter et al., 1991).
However, Vilain, Kautz and Van Beek (1986) also showed that determining the consistency of a
general-case temporal network (i.e.: disjunctive qualitative and metric constraints between points,
intervals or durations) is NP-hard. Thus, in previous qualitative or quantitative models, the
consistency problem is tractable only under some properties on constraints, relationships between
variable domains and constraints, or by using restricted subsets of constraints (Dechter et al., 1991;
Dechter, 1992; van Beek & Detcher, 1995; Wetprasit & Sattar, 1998; Jeavons et al., 1998; etc.). For
instance, tractable subclasses of IA have been identified by Vilain, Kautz and Van Beek (1986),
Nebel and Burckert (1995), Drakengren and Jonsson (1997), etc. Moreover, some interesting results
have been obtained in identification of tractable subclasses of QA. Specifically, Jonsson et al. (1999)
identified the five maximal tractable subclasses of the qualitative point-interval algebra. However,
to my knowledge the maximal tractable subclass of PIDN model (maximal tractable subclass of
qualitative and quantitative point, interval and duration constraints) is still not identified. In any case,
these restricted tractable subclasses are not able to obtain expressiveness of full models, and the
problem of reasoning on disjunctive constraints on points and intervals remains NP-complete.
On the other hand, these qualitative and metric temporal models do not manage certain types of
non-binary constraints, which are important for modeling some problems (scheduling, causal
reasoning, etc.). For instance, disjunctive assertions like ‘(interval1 {bef, meets} interval2 ) ∨ (time36

BARBER

point3 is [10 20] from time-point4 )’, or temporal-causal relations like ‘If (interval1 {bef, meets}
interval2 ) then (time-point3 is [10 20] from time-point4 )’ should be incorporated in these models
(Meiri, 1996). Moreover, the global consistency property introduced by Dechter (1992) is an
important property in temporal networks, since it allows us to obtain solutions by backtrack-free
search (Dechter, 1992; Freuder, 1982). In particular, a global consistent network would allow us to
handle conjunctive queries like ‘does ‘(interval1 {bef, meets} interval2 ) ∧ (time-point3 is [10 20] from
time-point4 ) hold?’ without propagation of the query, as it is required in (van Beek, 1991).
Stergiou and Koubarakis (1996), Jonsson and Bäckström (1996) dealt with the representation of
temporal constraints by means of disjunctions of linear constraints (linear inequalities and
inequations) also named Disjunctive Linear Relations (DLRs). These expressions are a unifying
approach to manage disjunctive constraints on points, intervals and durations, such that these
expressions subsume most of the formalism for temporal constraint reasoning (Jonsson & Bäckström,
1998). Moreover, DLRs are able to represent disjunctions of non-disjunctive metric constraints (x1 y1 ≤c1 ∨ x2 -y2 ≤c2 ∨ ....∨ xn -yn ≤cn ), where xi and yi are time points, ci real numbers and n≥1 (Stergiou
& Koubarakis, 1998). Obviously, the satisfiability problem for an arbitrary set of disjunctions of
linear constraints is NP-complete. Interesting tractable subclasses of DLRs and conditions on
tractability are identified in (Cohen et al., 1996; Jonsson & Bäckström, 1996; and Stergiou &
Koubarakis, 1996). The two main tractable subclasses are Horn linear and Ord-Horn linear
constraints (Stergiou & Koubarakis, 1996; Jonsson & Bäckström, 1998). However, these subclasses
subsume temporal algebras whose management is also polynomial.
The management of a set of disjunctions of linear constraints is mainly based on general methods
from linear programming, although some specific methods have been defined for tractable subclasses
(Stergiou & Koubarakis, 1998; Cohen et al., 1996; etc.). As Pujari and Sattar outline (1999), the
linear programming approach, though expressive, does not take advantage of the underlying
structures (e.g., domain constraints) of temporal constraints. In addition, usual concepts in temporal
reasoning, as composition and intersection operations on constraints, minimal constraints, kconsistency (Freuder, 1982), decomposability (Montanari , 1974), globally consistency (Dechter,
1992), etc., and their consequences should be adapted to reasoning on disjunctive linear constraints,
which is not a trivial issue.
In spite of the expressive power of the previous models, some problems (including planning,
scheduling, hypothetical reasoning, etc.) also need to reason on alternative contexts (situations,
intentions or causal projections) and to know what holds in each one of them (Dousson et al., 1993;
Gerevini & Schubert, 1995; Garcia & Laborie, 1996; Srivastava & Kambhampati, 1999). This gives
rise to the need to reason on context-dependent constraints. This feature is not supported in the usual
temporal models in a general way, nor described in the usual expressive power of constraints
(Jeavons et al., 1999). Therefore, ad-hoc methods should be used when reasoning on temporal
contexts is required.
These issues will be addressed in this paper. We describe a temporal model, which integrates
qualitative and metric disjunctive constraints on time-points and intervals. The temporal model is
based on time-points as primitive, such that intervals are represented by means of their end timepoints. However, the representation of interval constraints seems to imply some kind of relation
among endpoint constraints (Gerevini & Schubert, 1995). The proposed temporal model introduces
labeled constraints, where each elemental constraint (disjunct) in a disjunctive point-based metric
constraint is associated to one unique label. In this way, point-based constraints can be related among
37

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

them without using hyper-arcs. Therefore, metric and symbolic constraints among intervals and timepoints can be fully integrated, represented and managed by means of a labeled metric point-based
Temporal Constraint Network (TCN). Particularly, the model proposed here handles constraints
proposed in QA (Meiri, 1996), IDSA (Badaloni & Berati, 1996), and Distance Constraint Arrays
model (Staab & Hahn, 1998). Moreover, several added functionalities are also provided:
• Management of alternative temporal contexts. Each input constraint can be associated to a
given context. A hierarchy of alternative temporal contexts can be defined, such that
constraints between points and intervals are dependent on each context. To my knowledge,
these features improve existing temporal models, where contexts are not managed.
• Reasoning algorithms on labeled constraints are based on a closure process. These processes
guarantee consistency and obtain a minimal disjunctive context-dependent TCN. Additionally,
a special type of globally labeled-consistent TCN is obtained. This property allows us to obtain
solutions by backtrack-free search (Freuder, 1982).
• Management of a special type of non-binary constraints. Reasoning algorithms are able to
manage disjunctions of disjunctive constraints. This supposes an extension of disjunctions of
non-disjunctive metric constraints proposed by Stergiou and Koubarakis (1998). Moreover,
given a set of disjunctive constraints, the model can handle logical relations among
disjunctions of different constraints. Thus, we can express that a set of atomic disjuncts in
disjunctive constraints are mutually disjunctive among them. Therefore, a special type of
and/or TCN can be managed as a conjunctive (and) TCN. Likewise, the model can also handle
special non-binary constraints representing implications among temporal constraints as were
identified by Meiri (1996).
With these features, the proposed temporal model is suitable for modeling problems where these
requirements appear. The computational cost of reasoning methods is non-polynomial, given the
complexity of the underlying problem. However, several improvements are also proposed.
A brief revision of the main temporal reasoning concepts is presented in Section 2. In Section 3,
a temporal algebra for labeled point-based disjunctive metric constraints is described. This temporal
algebra introduces the concept of labeled constraints and their temporal operations. Reasoning
algorithms for guaranteeing a minimal (and consistent) TCN are specified in Section 4. By using this
model, the integration of interval and point-based constraints and management of non-binary
constraints are respectively described in Sections 5 and 6. Association of constraints to temporal
contexts and management of context-dependent constraints are detailed in Section 7. Finally, Section
8 concludes.

2. Basic Temporal Concepts
Temporal reasoning deals with reasoning on temporal constraints. The syntax and semantics of
constraints are defined by an underlying temporal algebra, which is the basis for performing the
reasoning processes. A temporal algebra can be defined according to the following elements:
• Temporal primitive (or variable) 'x i ', usually time-points (ti ) or intervals (Ii ).
• Interpretation domain D for primitives xi . The interpretation domain represents the time line.
38

BARBER

Time points are instantiated on D (ti ∈D), and temporal intervals can be modelled as pairs of
ending time points that can be instantiated on D: Ii = (Ii -, Ii +), Ii ∈DxD, Ii -≤Ii +.
• Temporal constraints between primitives, where each constraint relates n primitives: c1,2..n (x1 ,
x2 , ..., xn ). As particular cases, the 'empty constraint' {∅} is named the Inconsistent-Constraint
and 'U' is the Universal-Constraint. Unary-constraints restrict the interpretation domain D for
variables. They are not usually used in symbolic algebras, where an infinite domain is
assumed. Binary-constraints are temporal constraints between two variables (xi cij xj ), and nary-constraints represent temporal constraints among n variables. By default, binary constraints
are assumed in this paper. We can also have qualitative (relative relation) or quantitative
(metric relation) constraints, as well as disjunctive (cij is a set of disjunctive basic constraints,
|cij |≥1) or non-disjunctive constraints.
• Operations between constraints. Mainly, Temporal Composition (⊗), Temporal Intersection
(⊕), Temporal Union (∪Τ), and Temporal Inclusion (⊆Τ).
A temporal problem is specified by a set of n variables X= {xi }, an interpretation domain D and
a finite set of temporal constraints between variables {(xi cij xj )}. A temporal problem gives rise to
a Temporal Constraint Network (TCN) which can be represented as a directed graph where nodes
represent temporal primitives (xi ) and labeled-directed edges represent the binary constraints between
them (cij). The Universal Constraint U is not usually represented in the graph, and each direct edge
(representing cij ) between xi and xj implies an inverse one (representing cji ) between xj and xi .
According to the underlying Temporal Algebra, we mainly have IA-TCNs based on the Interval
Algebra (Allen, 1983), PA-TCNs based on the Point Algebra (Vilain et al., 1986), or Metric-TCNs
based on the Metric Point Algebra (Dechter et al., 1991; Dean & McDermott, 1987). In this later
case, disjunctive metric point-based constraints give rise to a Temporal Constraint Satisfaction
Problem (TCSP) (Dechter et al., 1991).
Reasoning on temporal constraints can be seen as a Constraint Satisfaction Problem (CSP). An
instantiation of the variables X is a n-tuple (v1 , v2 , v3 , ...,vn ) / vi ∈D which represents the assignments
of values {vi } to variables {x i }: (x1 =v1 , x2 =v2 , ...,xn =vn). A (global) solution of a TCN is a consistent
instantiation of the variables X in their domains such that all TCN constraints are satisfied. A value
v is a consistent (or feasible) value for xi if there exists a TCN solution in which xi =v. The set of all
feasible values of a variable xi is the minimal domain for the variable. A constraint (xi cij xj ) is
consistent if there exists a solution in which (xi cij xj ) holds. A constraint cij is minimal iff it consists
only of consistent elements (or feasible values) that is, those which are satisfied by some
interpretation of TCN constraints. A TCN is minimal iff all its constraints are minimal.
A TCN is consistent (or satisfiable ) iff it has at least one solution. Freuder (1982) generalizes the
notion of consistency as: 'a network is k-consistent iff (given any instantiation of any k-1 variables
satisfying all the direct constraints among those variables) there exists at least one instantiation of any
kth variable such that the k values taken together satisfy all the constraints among the k variables'. As
consequences: (i) all (k-1)-length paths in the network are consistent, (ii) for each pair or nodes, there
exists an interpretation that satisfies each (k-1)-length path between them, and (iii) each sub-TCN of
k-nodes is consistent. As particular cases, 1-consistency, 2-consistency and 3-consistency are called
node-consistency, arc-consistency and path-consistency, respectively (Mackworth, 1977; Montanari,
1974).
39

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

Path-consistency is a common concept in constraint networks. From Montanari (1974) and
Mackworth (1977), ‘a path of k-length through nodes (x1 , x2 , ..., xk , xj ) is path-consistent iff for any
value v1 ∈d1 and vj ∈dj such that (x1=v1 c1j xj=vj ) holds, there exists a sequence of values v2 ∈d2 , v3∈d3 ,
..., vk ∈dk such that (v1 cl2 v2 ), (v2 c23 v3 ),...., and (vk ck,j vj ) hold’. A TCN is path-consistent iff all its
paths are consistent. Moreover, Montanari (1974) proves that to ensure path-consistency it suffices
to check every 2-length path. Thus, path-consistency and 3-consistency are equivalent concepts.
Alternatively, Meiri (1996) outlines a path of k-length (xi , x1 , x2 , ...,xk , xj ) is path-consistent iff cij ⊆Τ
(ci1 ⊗ c12 ⊗ ... ⊗ ckj ). However, this definition disregards domain constraints, such that it is equivalent
to the former definition if variable domains are infinite or the TCN is also node and arc-consistent,
as the usual case in symbolic algebras. In metric algebras, path-consistency usually assumes node and
arc-consistency. Therefore, taking into account that it is only necessary to test 2-length paths to
assure path-consistency, a TCN is path-consistent iff ∀cij ,cik ,ckj⊆TCN, cij ⊆Τ (cik ⊗ ckj ). This
condition gives rise to the more usual path-consistent algorithm: the Transitive Closure Algorithm
(TCA) which imposes local 3-consistency in each sub-TCN of 3 nodes, such that all 2-length paths
become consistent paths (Mackworth, 1977; Montanari , 1974). The TCA algorithm will obtain an
equivalent path-consistent TCN if it exists. Otherwise, it fails.
∀cij ,cik ,ckj ⊆TCN: cij ←cij ⊕ (cik ⊗ ckj )
A network is strong k-consistent iff the network is j-consistent for all j≤k (Freuder, 1982). An nconsistent TCN is a consistent TCN, and a strong n-consistent TCN is a minimal TCN. Alternatively,
Dechter (1992) introduces the concepts of local and global consistency: A partial instantiation of
variables (x1 =v1 , x2 =v2 , ...,xk =vk ) / 1≤k<n is locally consistent if it satisfies all the constraints among
these variables. A subTCN is globally consistent if any locally consistent instantiation of the
variables in the subTCN can be extended to a consistent instantiation of all TCN. A globally
consistent TCN is one in which all its subTCNs are globally consistent. Thus, a TCN is strong nconsistent iff it is globally consistent (Dechter, 1992).
The first reasoning task on a TCN is to determine whether the TCN is consistent. If the TCN is
consistent, we can then obtain the minimal-TCN, all TCN solutions (by assuming a discrete and finite
model of time), only one solution, a partial solution (consistent instantiation of a subset of TCN
variables, which is a part of a global solution), etc.
Deductive closure, or propagation, is one of the basic reasoning algorithms. The closure process
is a deductive process on a TCN, where new derived constraints are deduced from the explicitly
asserted ones by means of the composition (⊗) and intersection (⊕) operations. Thus, the process of
determining the consistency and the minimality of a TCN is related to a sound and complete closure
process (Vilain et al., 1986). Alternatively, CSP-based methods (with several heuristic search criteria)
are also used for guaranteeing consistency and obtaining TCN solutions. In this paper, we are mainly
interested in TCN closure processes.
Determining the consistency of a general-case TCN is NP-hard, and Minimal TCNs can be
obtained by a polynomial number of consistency processes (Vilain et al., 1986). Particularly, Dechter,
Meiri and Pearl (1991) showed that determining consistency and obtaining a minimal disjunctive
metric TCN can be achieved in O(n3 le), where ‘n’ is the number of TCN nodes, ‘e’ is the number of
explicitly asserted (input) constraints, and ‘l’ is the maximum number of intervals in an input
constraint. However, specific levels of k-consistency can guarantee consistency and obtain a minimal
TCN, depending on the TCN topology or the underlying temporal algebra. For example, path40

BARBER

consistency guarantees consistency and obtains a minimal non-disjunctive metric TCN (Dechter et
al., 1991). The path-consistency TCA Algorithm has an O(n3) cost (Allen, 1983; Vilain, Kautz & Van
Beek, 1986). However, assuring path-consistency can become a complex task in disjunctive metricTCNs if the variable domain D is large or continuous. As was stated by Dechter, Meiri and Pearl
(1991), the number of intervals in |cij ⊗ cjk | is upper bounded by |cij |x|cjk |. Thus, the total number of
disjuncts (subintervals) in a path-consistent TCN might be exponential in the number of disjuncts per
constraints in the initial (input) TCN. Schwalb and Dechter (1997) call this the fragmentation
problem, which does not appear in non-disjunctive metric TCNs. Thus, the TCA algorithm is O(n3
R3 ) in disjunctive metric-TCNs if time is not dense (Dechter et al., 1991), where the range ‘R’ is the
maximum difference between the lowest and highest number specified in any input constraints.

3. A Labeled Temporal Algebra
The main elements of the point-based disjunctive metric temporal algebra are (Dechter et al., 1991):
• Time-point (ti ) as primitive variable. A continuous variable domain (like Q or ℜ) is usually
assumed.
• Each temporal constraint cij ⊆U is a finite set of l mutually exclusive subdomains (or
subintervals) of D.
cij ≡{[d-1 d+1 ], [d-2 d+2 ], ...., [d-k d+k ], ....., [d-l d+l ]} ,

where d-k ≤d+k and d-k ,d+k ∈D,

and disjunctively restricts the temporal distance between two time-points, ti and tj :
tj - ti ∈ {[d-1 d+1 ], [d-2 d+2 ], ....., [d-l d+l ]},
meaning that (d-1 ≤tj -ti ≤ d+1 ) ∨ .... ∨ (d-l ≤ tj -ti≤ d+l ). Similar conditions can be applied to open
(d-k d+k ) and semi-open intervals (d-k d+k ], [d-k d+k ). The Universal-Constraint U is {(-∞ +∞)}.
Unary constraints restrict the associated subdomain of a time-point ti ∈{[d-1 d+1 ], [d-2 d+2], .....,
[d-l d+l ]}. A special time-point T0 is usually included, which represents 'the beginning of the
world' (usually, T0 =0). Thus, each unary constraint on ti can be represented as a binary one
between ti and T0 :
ti - T0 ∈ {[d-1 d+1 ], [d-2 d+2 ], ..... ,[d-l d+l ]} ≡ ti∈[d-1 , d+1 ] ∨ ti ∈[d-2 , d+2 ] ∨, ..., ∨ ti∈[d-l , d+l ]
and, by default: ∀ti , (T0 {[0 ∞)} ti ).
• The algebra operations, mainly ⊗, ⊕, ∪Τ and ⊆Τ. From (Meiri, 1996), given two temporal
constraints S={[dS-i , dS+i ]} and T={[dT-j , dT +j ]},
S ⊗ T = {dk / ∃di∈S ∧ ∃dj∈T / dk = di +dj }.
That is, ∀[dS-i , dS+i ]∈S, ∀[dT-j , dT +j ]∈T, ∪T{[dS-i +d T-j , dS+i +d T+j ]}. Here, resulting subdomains
in S ⊗ T may not be pairwise disjoint. Therefore, some additional processing may be required
to compute a disjoint subdomain set.
S ⊕ T = {dk / dk∈S ∧ dk∈T}. That is, the set-intersection of their subdomains.
S ∪Τ T = {dk / dk∈S ∨ dk ∈T}, as the set-union of their subdomains.
S⊆ΤT = iff ∀dk ∈S, ∃dk ∈T.
41

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

On the basis of the point-based disjunctive metric temporal algebra and its operations, we
introduce a labeled point-based disjunctive metric temporal algebra, which gives rise to a labeledTCN.
3.1 Labeled Constraints and Inconsistent Label Sets
An elemental constraint (ec) is one disjunct in a disjunctive constraint. Similar terms are atomic,
basic or canonical constraints. However, let’s use this term due to the special structure of labeled
elemental constraints which are introduced further on. Thus, a disjunctive constraint cij can be
considered as a disjunctive set of l mutually exclusive elemental constraints {ecij.k }.
ecij.k = [d-ij.k d+ij.k ] / ∀i,j,k d-ij.k ≤d+ij.k
cij ≡{ec ij.1 , ec ij.2 , ..., ec ij.l } ⊆ U / ∀k,p∈(1,..,l), k≠p, (ecij.k ⊕ ec ij.p )=∅
Definition 1 (Labeled constraints). A labeled elemental constraint lecij.k is an elemental constraint
ecij.k associated to a set of labels {labelij.k}, where each labelij.k is a symbol. A labeled constrain t lc ij
is a disjunctive set of labeled elemental constraints {lecij.k }. That is,
lc ij ≡ {lecij.1 , lecij.2 , ..., lecij.l }, where
lecij.k ≡ (ec ij.k {labelij.k }), and {labelij.k }≡{label1 , label2 , ..., labels } is a set of symbols.◊
◊
Each label in a labeled-TCN can be considered as a unique symbol. The following cases can
occur:
i)

If an input (or explicitly asserted) constraint lc ij has only one elemental constraint, that is,
only one disjunct, this elemental constraint has the label 'R0 '. The labeled UniversalConstraint is {U{R0}}. In a given TCN, the set of all elemental constraints labeled with 'R0 '
is the ‘common context’. Thus, the label R0 represents the set of elemental constraints which
have no other alternatives (disjuncts). All elemental constraints labeled only with R0 should
hold since they have no other alternative disjuncts.

ii)

If an input constraint lc ij has more than one elemental constraint, each elemental constraint
lecij.k ∈lc ij has a single and exclusive label associated to it (|{labelij.k }|=1). Thus, each label
in the TCN represents bi-univocally an elemental constraint in an explicitly asserted
constraint.

iii) Each derived elemental constraint (obtained by combining (⊗lc) or intersecting (⊕ lc) two
labeled elemental constraints) has a set of labels associated to it. This set of labels is obtained
from the label sets associated to the combined (or intersected) labeled elemental constraints.
It will be detailed in the later specification of operations (⊗lc, ⊕ lc) in Section 3.2. In
consequence, the label set associated to a derived elemental constraint represents the
conjunctive support-set of explicitly asserted elemental constraints that imply this derived
elemental constraint.
Let's see a simple example on labeled constraints, which was introduced by Dechter, Meiri and
Pearl (1991).

42

BARBER

{([60 70]{R0})}

t4
{([40 50]{R3}) ([20 30]{R4})}

{([10 20]{R0})}

t3

T0
t2

{([60 ∞){R1}) ([30 40]{R2})}

T0

{([10 20]{R0} )}

t1

Figure 1: The labeled point-based disjunctive metric TCN of the Example 1

Example 1: "John goes to work either by car [30'-40'], or by bus (at least 60'). Fred goes to work
either by car [20'-30'], or in a carpool [40'-50']. Today John left home (t1) between
7:10 and 7:20, and Fred arrived (t4) at work between 8:00 and 8:10. We also know
that John arrived (t2) at work about 10'-20' after Fred left home (t3)."
In this example, we have the disjunctive labeled constraints in Figure 1, where T0 represents the
initial time (7:00) and where the granularity is in minutes. A label 'R0 ' is associated to elemental
constraints belonging to constraints with only one disjunct. In constraints with more than one,
mutually exclusive disjuncts, each disjunct is labeled with an exclusive label Rn (n>0). Thus,
• The label R0 is associated to "John left home between 7:10 and 7:20", "Fred arrived at work
between 8:00 and 8:10", and "John arrived at work about 10'-20' after Fred left home". This
is the common context.
• The label R1 is associated to "John goes by bus", and R2 to "John goes by car".
• The label R3 is associated to "Fred goes in a carpool", and R4 to "Fred goes by car".
Definition 2 (Inconsistent-Label-Sets). An Inconsistent-Label-Set (I-L-Set) is a set of labels {labeli }
and represents a set of overall inconsistent elemental constraints. That is, they cannot all
simultaneously hold. ◊
Theorem 1. Any label set that is a superset of an I-L-Set is also an I-L-Set. The proof is obvious. If
a set of elemental constraints is inconsistent, any superset of it is also inconsistent. ◊
Definition 3. Elemental constraints {lecij.k} of an input disjunctive constraint lc ij are pairwise disjoint.
Thus, each 2-length set of labels from each pair of {lecij.k } is added to the set of I-L-Sets. That is, for
each input constraint lc ij ≡ {lecij.1 , lecij.2 , ..., lecij.l }, where lecij.k ≡(ecij.k {labelij.k }) and |{labelij.k }|=1:
∀k,p∈(1,..,l) / k≠p, I-L-Sets ← I-L-Sets ∪ ({labelij.k }∪{labelij.p }) ◊
In the example of Figure 1, {R1 R2 } and {R3 R4 } are I-L-Sets. Other I-L-Sets existing in a labeled
TCN will be detected in the reasoning processes later detailed in Section 4.

43

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

3.2 Operations on Labeled Constraints
The following points define the main operations on labeled constraints.
3.2.1 TEMPORAL INCLUSION ⊆LC
The temporal inclusion operation ⊆lc should take into account the inclusion of temporal intervals and
the inclusion of associated label sets:
lecij.k ⊆lc lecij.p = (ecij.k {labelij.k }) ⊆lc (ecij.p {labelij.p }) =def ecij.k ⊆T ecij.p ∧ {labelij.k }⊆ {labelij.p }.
3.2.2 TEMPORAL UNION ∪LC
Operation ∪lc performs the disjunctive temporal union of labeled constraints as the set-union of their
elemental constraints. However, all labeled elemental constraints whose associated labels are I-L-Sets
should be rejected.
lc ij ∪lc lc’ij =def ∀lecij.k ∈lc ij , ∪lc [{lecij.k } lc’ij] , where
∪lc [{lecij.k } lc’ij ] = (ec ij.k {labelij.k }) ∪lc lc’ij =def
Inconsistent({labelij.k }) : lc’ij
∃lecij.p ∈lc’ij / lecij.p ⊆lc lecij.k : lc’ij
Other : ({lc’ij } ∪ {lecij.k }) - ({lecij.p }, ∀lecij.p∈lc’ij ∧ lecij.k ⊆lclecij.p )

(s1 )
(s2 ).

The function Inconsistent({labelij.k}) returns true if the set {labelij.k } is an I-L-Set or a superset of
any existing I-L-Set (Theorem 1). Otherwise, it returns false:
Inconsistent({labelij.k }) =def
If ∃{labels }∈Inconsistent-Label-Sets / {labels }⊆{labelij.k } Then True Else False.
The operation ∪lc simplifies the resulting constraint. Equal or less-restricted elemental constraints
with equal or bigger associated label sets are removed. For instance:
{([10 30] {R1 R3 R5 R9}), ([40 40] {R6 R7})} ∪lc {([10 20] {R1 R3}), ([40 40] {R6 R7 R8})} =
{([10 20] {R1 R3}), ([40 40] {R6 R7})}.
In the resulting constraint, ([10 30] {R1 R3 R5 R9}) and ([40 40] {R6 R7 R8}) are eliminated, as examples
of the cases s1 and s2 , respectively. That is, ([10 20] {R1 R3}) ⊆lc ([10 30]{R1 R3 R5 R9}) and ([40 40] {R6 R7})
⊆lc ([40 40] {R6 R7 R8}). These simplifications can seem counter-intuitive. However, note that the label
set associated to each derived-labeled elemental constraint represents the support set (composed of
input elemental constraints) from which the derived-labeled elemental constraint is obtained. Thus,
only the minimal associated label set should be represented, for reason of efficiency. Moreover, the
more labels are in the associated label set {labelij.k }, the elemental constraint (ecij.k ) should be equal
or more restricted.
3.2.3 TEMPORAL COMPOSITION ⊗LC
Operation ⊗lc performs the temporal composition of labeled constraints. It is based in the operation
⊗ of the underlying disjunctive metric point-based algebra.
44

BARBER

lc ij ⊗lc lc jk =def ∀lecij.p∈lc ij , ∀lecjk.q ∈lc jk ∪lc [ (ecij.p ⊗ ecjk.q {labelij.p }∪{labeljk.q })].
For instance: {([0 10] {R1}), ([20 30] {R2})} ⊗lc {([100 200] {R3}), ([300 400] {R4})} =
{([320 430] {R4 R2}), ([300 410] {R4 R1}), ([100 210] {R3 R1}), ([120 230] {R3 R2})}.
Note that elemental constraints in a labeled derived constraint may not be pairwise disjoint.
However, these labeled derived elemental constraints cannot be simplified. This is related to the
fragmentation problem of the disjunctive metric algebra (Schwalb & Dechter, 1997). We have that
each derived-labeled elemental constraint should have its own associated label set. In the example,
(([320 430] {R4 R2}), ([300 410] {R4 R1})) cannot be simplified to ([300 430] {R4 R2 R1}) since each
subinterval depends on a different set of labels (that is, on a different support-set of elemental
constraints). If the label set {R4 R2 } becomes an I-L-Set, only ([320 430] {R4 R2}) should be removed.
On the other hand, if [300 410] becomes an inconsistent interval between the implied time points,
only {R4 R1 } should be asserted as an I-L-Set.
3.2.4 TEMPORAL INTERSECTION ⊕ LC
Operation ⊕ lc performs the temporal intersection of labeled constraints and is based on the operation
⊕.
lc ij ⊕lc lc’ij =def ∀lecij.k ∈lc ij , ∀lecij.p ∈lc’ij , ∪lc [lecij.k ⊕ lc lecij.p ]
where, lecij.k ⊕ lc lecij.p =def
If ec ij.k ⊕ ecij.p = ∅ Then {∅}
;The Inconsistent-Constraint is returned.
Else [(ecij.k ⊕ ecij.p ) ({labelij.k }∪{labelij.p })]
As example:
{([0 10] {R1}), ([20 25] {R2})} ⊕ lc {([0 30] {R3}), ([40 50] {R4})} = {([20 25] {R3 R2}), ([0 10] {R3 R1})}
In the operations ⊗lc and ⊕ lc, the label set {labelij.r} associated to each derived labeled-elemental
constraint (ecij.r) is obtained from the set-union of labels associated to combined (⊗lc) or intersected
(⊕ lc) labeled-elemental constraints. Therefore, {labelij.r} represents the support set (composed of input
elemental constraints) that implies the derived elemental constraint (ecij.r).
Definition 4. A set of I-L-Sets is complete if it represents all inconsistent sets of TCN elemental
constraints. A set of I-L-Sets is sound if each I-L-Set represents an inconsistent set of elemental
constraints. ◊
Theorem 2. Assuming a complete and sound set of I-L-Sets, a labeled elemental constraint is
consistent iff it has an associated label set which is not an I-L-Set. The proof is trivial, since the label
set associated to each labeled elemental constraint represents its support-set. ◊
Theorem 3. Assuming a complete and sound set of I-L-Sets, no inconsistent labeled elemental
constraint is obtained in operations ⊗lc and ⊕lc.
Proof: The operations ⊗lc and ⊕lc use the operation ∪lc to obtain their results. This operation ∪lc
rejects all labeled elemental constraints whose associated labels are I-L-Sets. Thus, all elemental
constraints derived in operations ⊗lc and ⊕ lc are consistent (Theorem 2). ◊
45

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

3.3 Distributive Property ⊗ lc Over ⊕ lc in Disjunctive Labeled Constraints
Operations ⊗ and ⊕ are distributive (i.e.: ⊗ distributes over ⊕) in non-disjunctive metric TCN, but
this property does not hold in disjunctive metric constraints. Dechter, Meiri and Pearl (1991) show
the following example. Given the disjunctive metric constraints:
a= {[0 1], [10 20]},
we have:

b= {[25 50]},

c= {[0 30], [40 50]},

(a ⊗ (b ⊕ c) = {[25 31], [35 70]}

(a ⊗ b) ⊕ (a ⊗ c) = {[25 70]}.

Thus, clearly (a ⊗ (b ⊕ c) ≠ (a ⊗ b) ⊕ (a ⊗ c). However, the distributive property holds for
operations ⊗lc and ⊕ lc in labeled TCN.
Theorem 4. By using labeled constraints and I-L-Sets, ⊗lc distributes over ⊕lc.
Proof: Let’s consider the labeled constraints lc i , lc j and lc k . Thus,
(lc i ⊗lc lc j ) ⊕ lc (lc i ⊗lc lc k )
can be expressed, according to the definition of operation ⊗lc, as:
(∀lecp ∈lc i , ∀lecq ∈lc j , ∪lc[(lecp ⊗lc lecq )]) ⊕ lc (∀lecr∈lc i , ∀lecs ∈lc k , ∪lc[(lecr ⊗lc lecs)]) =
∀lecp ∈lc i , ∀lecq ∈lc j , ∀lecr∈lc i , ∀lecs ∈lc k (∪lc[(lecp ⊗lc lecq)] ⊕ lc ∪lc[(lecr ⊗lc lecs )])
which, according to the definition of ⊕ lc, can be expressed as:
∀lecp ∈lc i , ∀lecq ∈lc j , ∀lecr∈lc i , ∀lecs ∈lc k (∪lc[(lecp ⊗lc lecq ) ⊕ lc (lecr ⊗lc lecs )])

(e1)

In this expression, lecp and lecr are elemental constraints of the same-labeled constraint lc i .
However, the set-union of label sets associated to each pair of elemental constraints in any (input or
derived) labeled constraint is an I-L-Set (Definition 3). That is, if lecp ≠lecr, then {labelp }∪{labelr}
is an I-L-Set. Thus, if lecp ≠lecr, the label set associated to (lecp ⊗lc lecq ) ⊕lc (lecr ⊗lc lecs ) is an I-LSet. In consequence, (lecp ⊗lc lecq ) ⊕lc (lecr ⊗lc lecs ) is rejected in operation ∪lc. That is,
∀lecp ∈lc i , ∀lecq ∈lc j , ∀lecr∈lc i , ∀lecs ∈lc k / lecp≠lecr (∪lc[(lecp ⊗lc lecq ) ⊕lc (lecr ⊗lc lecs )]) = ∅.
Thus, the above expression (e1) results:
∀lecp ∈lc i , ∀lecq ∈lc j , ∀lecs∈lc k (∪lc [(lecp ⊗lc lecq ) ⊕lc (lecp ⊗lc lecs )]).
In this expression, ⊗lc clearly distributes over ⊕ lc for elemental constraints (i.e.: non-disjunctive
constraints). Therefore:
∀lecp ∈lc i , ∀lecq ∈lc j , ∀lecs∈lc k (∪lc [(lecp ⊗lc (lecq ⊕lc lecs ))]) =
∀lecp ∈lc i , ∪lc [lecp ⊗lc (∀lecq∈lc j , ∀lecs ∈lc k , ∪lc [lecq ⊕lc lecs ])] = lc i ⊗lc (lc j ⊕lc lc k ).
That is, ⊗lc distributes over ⊕ lc for labeled constraints.◊
◊
For instance, following the previous example:
a= {[0 1] {R1}, [10 20] {R2}},

b= {[25 50] {R0}},

c= {[0 30] {R3}, [40 50] {R4}}

and {R1 R2 }, {R3 R4 } are I-L-Sets. Thus, we have:
(a ⊗lc (b ⊕ lc c) = {[0 1] {R1}, [10 20] {R2}} ⊗lc ({[25 50] {R0}} ⊕ lc {[0 30] {R3}, [40 50] {R4}}) =
{[0 1] {R1}, [10 20] {R2}}⊗lc {[25 30] {R3 R0}, [40 50] {R4 R0}} =
46

BARBER

{[25 31] {R1 R3 R0}, [40 51] {R1 R4 R0}, [35 50] {R3 R2 R0}, [50 70] {R4 R2 R0}}.
Also,
(a ⊗lc b) ⊕ lc (a ⊗lc c) =
({[0 1] {R1}, [10 20] {R2}} ⊗lc {[25 50] {R0}}) ⊕ lc
({[0 1] {R1}, [10 20] {R2}} ⊗lc {[0 30] {R3}, [40 50] {R4}}) =
{[25 51] {R1 R0}, [35 70] {R2 R0}} ⊕ lc {[0 31] {R1 R3}, [40 51] {R1 R4} [10 50] {R2 R3}, [50 70] {R2 R4}} =
∪lc ([25 31] {R1 R3 R0}, [40 51] {R1 R4 R0}, [25 50] {R1 R2 R3 R0},
[50 51] {R1 R2 R4 R0}, [40 51] {R1 R2 R4 R0}, [35 50] {R3 R2 R0}, [50 70] {R4 R2 R0}).
However, {R1 R2 }, {R3 R4 } are I-L-Sets. Thus, ([25 50] {R1 R2 R3 R0}, [50 51] {R1 R2 R4 R0}, [40 51] {R1
R2 R4 R0}) are removed in operation ∪lc. Therefore,
(a ⊗lc b) ⊕ lc (a ⊗lc c) = {[25 31] {R1 R3 R0}, [40 51] {R1 R4 R0}, [35 50] {R3 R2 R0}, [50 70] {R4 R2 R0}}.
That is, (a ⊗lc (b ⊕lc c) = (a ⊗lc b) ⊕ lc (a ⊗lc c).

4. Reasoning Algorithms on Labeled Constraints
Several algorithms for reasoning on disjunctive constraints can be applied for the management of
labeled temporal constraints, by using the ⊗lc, ⊕lc, ∪lc and ⊆lc operations. For instance, the wellknown Transitive Closure Algorithm, general closure algorithms as in (Dechter, 1992; Dechter et al.,
1991; van Beek & Dechter, 1997), CSP-based approaches, etc. However, Montanari (1974) shows
that when composition operation distributes over intersection, any path-consistent TCN is also a
minimal TCN. From Theorem 4, we have that ⊗lc distributes over ⊕lc. Thus, application of a pathconsistent algorithm on the proposed-labeled TCN will obtain a minimal TCN. Thus, the TCA
algorithm could be used as the closure process on labeled constraints, in a similar way as Allen
(1983) uses it. However, an incremental reasoning process is proposed on the basis of the incremental
path-consistent algorithm for non-disjunctive metric constraints described by Barber (1993). An
incremental reasoning process is useful when temporal constraints are not initially known but are
successively deduced from an independent process; for instance, in an integrated planning and
scheduling system (Garrido et al., 1999). The proposed reasoning algorithm is similar to the TCA
algorithm. However, updating and closure processes are performed at each new input constraint.
Thus, each new input constraint is updated and closured on a previously minimal TCN (Figure 9).
Therefore, no further propagation of modified constraints in the closure process is needed. Moreover,
the proposed reasoning algorithms will obtain a complete and sound set of I-L-Sets.
The specification of reasoning processes is described in Section 4.1. The properties of these
processes will be described later in Section 4.2.
4.1 The Updating Process
Given a previous labeled-TCN, composed by a set of nodes {ni }, a set of labeled constraints {lc ij }
among them, and a set of I-L-Sets, the updating process of each new c’ ij between nodes ni and nj
constraint is detailed in Figure 2.
47

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

Updating (ni c’ij nj)
;c’ij ≡{ec’ij., ec’ij.2 , ..., ec’ij.l }, a disjunctive metric constraint.
lc'ij ← Put-Labels (c’ij ), ;An exclusive label is associated to each elemental constraint
ec’ij.k in c’ij
If Consistency-Test (lc ij , lc'ij ) ;Consistency test of lc'ij. The previously existing
constraint between n i and nj is lcij . Moreover, new I-L-Sets
are detected.
Then (*Inconsistent Constraint*)
Return (false)
Else (*Consistent Constraint*)
lc ij ← lc ij ⊕ lc lc'ij ,
lc ji ← Inverselc (lc ij ),
Closure (ni lc ij nj ), ;Closure algorithm for the updated constraint.
Return (true)
End-If
End-Updating
Figure 2: Updating process on labeled constraints
The function Put-Labels(c’ij ) returns a labeled-constraint lc’ij ≡{lec’ij.1 , lec’ij.2 , ..., lec’ij.l },
associating an exclusive label to each elemental constraint in c’ij. If there is only one disjunct in c’ij ,
the label in the unique elemental constraint is {R0 }. Otherwise, each pair of labels in lc’ij is added to
the set of I-L-Sets, since elemental constraints in c’ij are pairwise disjoint (Definition 3). By using the
Inverse function on non-labeled constraints, the Inverse lc function is:
Inverselc ({(ec ij.k {labelij.k })}) =def {(Inverse (ec ij.k ) {labelij.k })}
The described updating process is performed each time that one new input constraint c’ij is
asserted on a previous TCN. Thus, an initial TCN with no nodes, no constraints, and no I-L-Sets is
assumed (Figure 9). At each new input constraint (c’ij ), the TCN is incrementally updated and
closured. That is, if c’ij is consistent (Consistency-Test function), the constraint c’ij is added to the
TCN, the closure process (Closure function) propagates its effects to all TCN, and the new TCN is
obtained. A new updating process can be performed on this new TCN, and so on successively.
4.1.1. THE CONSISTENCY-TEST FUNCTION
The Consistency-Test function (Figure 3) is based on the operation ⊕ lc. A new input constraint lc'ij
between nodes ni and nj is consistent if it temporally intersects with the previously existing constraint
lc ij between these nodes. Moreover, the Consistency-Test function can detect new I-L-Sets:
i)

If the new constraint lc'ij is consistent with the existing constraint lc ij , and two elemental
constraints ecij.p ∈lc'ij , ecij.k∈lc ij do not intersect (ecij.k ⊕ ecij.p =∅), then the label set
{labelij.k }∪{labelij.p } is an I-L-Set and should be added to the current set of I-L-Sets.

ii)

If an existing elemental constraint between nodes ni and nj (lecij.k∈lc ij) does not intersect with
the new constraint lc'ij , then {labelij.k } is an I-L-Set and should be added to the current set of
I-L-Sets.

48

BARBER

Consistency-Test (lc ij , lc’ij ) =
If (lc ij ⊕ lc lc’ij ) = {∅}
Then Return (False)
Else
If ∃lecij.k ∈lc ij , ∃lecij.p ∈lc'ij / lecij.k ⊕ lc lecij.p ={∅}
Then I-L-Sets ← I-L-Sets ∪ ({labelij.k }∪{labelij.p }),
If ∃lecij.k ∈lc ij / lecij.k ⊕ lc lc’ij = {∅}
Then I-L-Sets ← I-L-Sets ∪ {labelij.k },
End-If
Return (True)
End- Consistency-Test
Figure 3: Consistency-Test function
For example,
Consistency-Test ({([0 10] {R1}), ([20 25] {R2}), ([100 110] {Ra})},
{([0 30] {R3}), ([40 50] {R4}), ([-50 -40] {Rb})}) = True
since
{{([0 10] {R1}), ([20 25] {R2}), ([100 110] {Ra})} ⊕ lc {([0 30] {R3}), ([40 50] {R4}), ([-50 -40] {Rb})} =
{([20 25] {R3 R2}), ([0 10] {R3 R1})} ≠ {∅}.
In this function, the label sets {R4 R2 }, {R4 R1 } and {Ra} are detected as I-L-Sets and should be
added to the current set of I-L-Sets, since:
{[20 25] {R2}} ⊕ lc {[40 50] {R4}}={∅},
{[0 10] {R1})} ⊕lc {[40 50] {R4})}={∅},
{([100 110] {Ra})} ⊕ lc {([0 30] {R3}), ([40 50] {R4}), ([-50 -40] {Rb})}={∅}.
Note that {Rb } does not need to be detected as an I-L-Set, since the label Rb is not included in the
final constraint {([20 25] {R3 R2}), ([0 10] {R3 R1 })} to be added to the TCN.
Any superset of an I-L-Set is also an I-L-Set (Theorem 1). Moreover, note that {R4 R2}, {R4 R1 }
do not need to be added to the set of I-L-Sets, since the label R4 is not included in the final constraint.
Therefore, the following simplifications can also be performed each time a new I-L-Set is added to
the current set of I-L-Sets. These simplifications do not modify the results of reasoning processes,
but minimize the size of the set of I-L-Sets and improve its management efficiency.
i)

No new I-L-Set that is superset of an existing I-L-Set is added to the set of I-L-Sets.

ii)

If an existing I-L-Set is superset of the new I-L-Set, then the existing I-L-Set is removed.

iii) No new I-L-Set that contains a label of lc'ij , which does not appear in the labeled constraint
(lc ij ⊕ lc lc'ij ) to be added to the TCN, should be added to the set of I-L-Sets.
Let’s see an example of the updating and consistency-test processes. Let’s take the labeled-TCN
that results from Example 1 once the following constraints have been updated and closured:

49

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

Set of Inconsistent-Label-Sets: {{R1 R 2}, {R3 R 4}}
{([60 70] {R0})}

t4
{([40 50] {R3})]
([20 30] {R4})}
{([-10 20] {R3 R0})
([10 40] {R4 R0})}

t3
t1

{([-10 20] {R3 R0})
([10 40] {R4 R0})}

T0
{([40 60] {R2 R0})
([70 ∞) {R1 R0})}

{([40 ∞) {R1 R3 R0})
([20 ∞) {R1 R4 R0})
([-10 30] {R2 R4 R0})
([10 50] {R2 R3 R0})}

{([10 30] {R3 R0})
([30 50] {R4 R0})]}

t2

{((-∞ 0] {R1 R0})
([0 30] {R2 R0})}

{([60 ∞) {R1})
([30 40] {R2})}

T0
{([10 20] {R0})]}

t4
t1

Figure 4: The resulting labeled-TCN of Figure 1 before updating (t3 {[10 20]} t2 )
(t1 {[60 ∞)R1, [30 40] R2 } t2 ), (t3 {[40 50] R3 , [20 30] R4 } t4), (T0 {[10 20]R0 } t1 ), (T0 {[60 70]R0 } t4 ).
The resulting labeled-TCN is shown in Figure 4 and the set of I-L-Set is {{R1 R2 }, {R3 R4 }}.
Now, we update (t3 {[10 20]R0 } t2 ). The previously existing constraint between t3 and t2 is (Figure 4):
{([40 ∞){R1 R3 R0 }) ([20 ∞){R1 R4 R0}), ([-10 30] {R2 R4 R0}) ([10 50] {R2 R3 R0})}
The Consistency-Test function performs:
{[10 20] {R0}} ⊕ lc {([40 ∞){R1 R3 R0 }) ([20 ∞){R1 R4 R0}), ([-10 30] {R2 R4 R0}) ([10 50] {R2 R3 R0})} =
{[20 20] {R1 R4 R0}, [10 20] {R2 R0} [∅]{R1 R3 R0}} ≠ {∅}

(e1)

Thus, (t2-t3∈{[10 20] {R0}}) is consistent. Moreover, {R1 R3 R0 } is detected as an I-L-Set. The
elemental constraints associated to {R1 R3 R0 } are an inconsistent set of disjuncts that cannot hold
simultaneously. That is:
"If today John left home between 7:10 and 7:20 (R0 ), Fred arrived at work between 8:00
and 8:10 (R0 ) and John arrived at work about 10'-20' after Fred left home (R0 ), then it is
impossible for John to have gone by bus (R 1 ) and Fred to have gone in a carpool (R 3 )."
The set of I-L-Sets obtained in the reasoning process can be considered as special derived
constraints, which express the inconsistency of a set of input elemental constraints. For instance, the
I-L-Set {R0 R1 R3 } represents (Figure 1):
¬ ( (T0 [10 20] T1 ) ∧ (T3 [10 20] T2 ) ∧ (T0 [60 70] T4 ) ∧ (T3 [40 50] T4 ) ∧ (T1 [60 ∞) T2 )).
This expression is a non-binary constraint. This type of constraints could be represented as a
disjunctive linear constraint, as Jonsson and Bäckström (1996), Stergiou and Koubarakis (1996)
show. However, input elemental constraints should be represented in derived constraints to be able
to derive these inconsistent sets of input elemental constraints. In this model, this is done by means
of the label sets associated to labeled elemental constraints.

50

BARBER

4.2 The Closure Process
The closure process (Figure 5) is applied each time a new input constraint (lc'i j ) is updated, such that
the effects of lc'ij are propagated to all TCN.
Closure (ni lc ij nj)
(* First loop: Closure n i → n j → n k *)
∀nk ∈TCN / lc jk ≠ {U{R0}}:
lc ik ← lc ik ⊕ lc (lc ij ⊗lc lc jk ), lc ki ← Inverse(lc ik)
(* Second loop: Closure n j → ni → nl *)
∀nl ∈TCN / lc il ≠ {U{R0}}:
lc jl ← lc jl ⊕ lc (Inverse(lc ij ) ⊗lc lc il ), lc lj ← Inverse(lc jl )
(* Third loop: Closure nl → ni → nj → nk *)1
∀nl , nk ∈TCN / lc li ≠ {U{R0}}, lc jk ≠ {U{R0}}:
lc lk ← lc lk ⊕ lc (lc li ⊗lc lc ij ⊗lc lc jk ), lc kl ← Inverse(lc lk )
End-Closure
Figure 5: The closure process on labeled constraints

(3)
nl.1

nl.i

lc il.i

(2)

ni

nk.1
(1)

lcij

nj

nk.i
lcjk.i

nk.t

nl.s

Figure 6: Loops in the Closure Process
The closure process has three loops (Figure 6). In these loops the process obtains:

1

i)

Derived constraints lc ik between ni and any node nk , if nk is previously connected with nj
(edge 1 of Figure 6).

ii)

Derived constraints lc lj between nj and any node nl , if nl is previously connected with ni
(edge 2 of Figure 6).

This loop could be simplified as:
(*n l → n i → n k*): ∀n l, n k ∈TCN / lc li ≠ {U{R0}}, lc j k ≠ {U{R0}}:

lc lk ← lc lk ⊕lc (lc li ⊗lc lc ik), or as

(*n l → nj → n k*): ∀n l, n k ∈TCN / lc li ≠ {U{R0}}, lc j k ≠ {U{R0}}: lc lk ← lc lk ⊕lc (lc lj ⊗lc lcj k)
since lc ik (or lc lj ) has already been closured in the first (or in the second loop). Moreover, the efficiency of the third loop
can be improved if only modified constraints in the first (or in the second loop) are considered.

51

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

Set of Inconsistent-Label-Sets: {{R1 R2 }, {R3 R4}, {R1 R3 R0}}
{([60 70]{R2 R0}) ([70 70]{R4 R1 R0})}

t4
{([60 60]{R4 R1 R0})
([40 60]{R4 R2 R0})
([50 60]{R3 R2 R0})}

t1 {([10 30]{R4 R2 R0})
([10 20]{R3 R2 R0})
([40 40]{R4 R1 R0})}

{([40 50]{R3 R2 R0})
([20 30]{R4 R2 R0})
([20 20]{R4 R1 R0})}

t3

T0

{([40 50]{R2 R3 R0})
([40 60]{R2 R4 R0})
([70 70]{R1 R4 R0})}

{([10 20]{R2 R0}) ([20 20] {R4 R1 R0})}

{([50 50]{R4 R1 R0})
([20 30]{R3 R2 R0})
([30 50]{R4 R2 R0})}

t2

{([0 0] {R1 R4 R0})
([20 30]{R2 R3 R0})
([0 20]{R2 R4 R0})}

{([30 40]{R2 R0})
([60 60]{R1 R4 R0})}

T0
{([10 20]{ R2 R0}) ([10 10] {R4 R1 R0})}

t4
t1

Figure 7: The Labeled-Minimal TCN of the Example 1
iii) Derived constraints lc lk between any pair of nodes nl and nk , if nl and nk are previously
connected with ni and nj respectively (edge 3 of Figure 6).
Let’s see the previous Example 1 represented in Figure 1 and Figure 4, when the consistent
constraint (expression e1):
(t3 {[20 20] {R1 R4 R0}, [10 20] {R2 R0}} t2 )
is closured. In the first loop of the closure process, we have:
lc 30 ← lc 30 ⊕lc ({[20 20] {R1 R4 R0}, [10 20] {R2 R0}} ⊗lc lc 20 =
{[-30 -10] {R3 R0 } [-50 -30] {R4 R 0 }} ⊕ lc
({[20 20] {R1 R4 R0}, [10 20] {R2 R0}} ⊗lc {[-60 –40] {R2 R0} (-∞ -70] {R1 R0}}) =
{[-30 -10] {R3 R0}} [-50 -30] {R4 R0}} ⊕ lc
{[-40 -20] {R1 R2 R4 R0}, (-∞ -50] {R1 R4 R0} [-50 –20] {R2 R0} (-∞ -50] {R1 R2 R0}}.
However, {{R1 R2 }, {R3 R4 } {R0 R1 R3 }} are I-L-Sets. No labeled elemental constraints whose
associated label set is a superset of these I-L-Sets will be derived (Theorem 3). Thus:
lc 30 ←{[-30 -10] {R3 R0}} [-50 -30] {R4 R0}} ⊕ lc {(-∞ -50] {R1 R4 R0} [-50 –20] {R2 R0} }=
{(-30 -20] {R2 R3 R0} [-50 –50] {R4 R1 R0} [-50 -30] {R4 R2 R0}}.
Similarly,
lc 31 ← lc 31 ⊕lc ({[20 20] {R1 R4 R0} [10 20] {R2 R0}} ⊗lc lc 21 =
{[-20 -10] {R3 R2 R0 } [-40 -40] {R4 R1 R0} [-30 -10] {R4 R2 R0}}
lc 34 ← lc 34 ⊕lc ({[20 20] {R1 R4 R0}, [10 20] {R2 R0}} ⊗lc lc 24 =
{[40 50] {R3 R2 R0 } [20 30] {R4 R2 R0} [20 20] {R4 R1 R0}}.
After the second and third loops, the final labeled-TCN is obtained (Figure 7). The final set of I-LSets is {{R1 R2 }, {R3 R4 } {R0 R1 R3 }}. These sets represent all sets of mutually inconsistent inputelemental constraints that exist in the TCN of Figure 1.
52

BARBER

4.3 Properties of Reasoning Algorithms
In this section, the main properties of the proposed reasoning algorithms are described.
Theorem 5. The proposed updating and closure processes (Sections 4.1 and 4.2) guarantee a
consistent TCN if they are applied on a previous minimal (and consistent) TCN.
Proof: The updating constraint lc’ij is asserted in the TCN if it is consistent with the previous minimal
constraint lc ij (Consistency-Test function).◊
◊
Theorem 6. The proposed closure algorithm obtains a path-consistent TCN, if it is applied over a
previous minimal TCN.
Proof: This was detailed by Barber (1993) for non-disjunctive TCNs and it is applied here to labeled
TCNs. We have:
i)

No derived constraint can exist between a pair of nodes if no path between them combines
the asserted constraint lc ij .

ii)

The closure process computes a derived constraint between any pair of nodes (nl , nk ) that
become connected by a path across the closured constraint lc ij . Let’s assume an existing path
between the nodes nx1 , ny1 that includes lc ij :
nx1 , nx2 , nx3 , ........, nx, (nj lc ij nj ), ny ......, ny2 , ny1
such that a derived constraint between nx1 ny1 should be computed. However, a minimal
constraint between (nx1 , ni ) and between (nj , ny1 ) should already exist in the previous minimal
TCN. In consequence, a derived constraint between (nx1 , ny1 ) is computed in the third loop
of the process.

iii) If the previous TCN is minimal, all possible derived constraints that can exist between any
pair of nodes (nl, nk ) are already computed in the constraint lc’lk derived between these nodes
in the proposed closure process. In the third loop, this process obtains:
lc’lk = lc lk ⊕ lc (lc li ⊗lc lc ij ⊗lc lc jk ).
Let’s suppose there exists another path between (nl , nk ) across the updated lc ij constraint: (nl ,
np , ni , nj , nq , nk ). This path computes another derived constraint between (nl , nk ):
lc''lk = lc lk ⊕lc (lc lp ⊗lc lc pi ⊗lc lc ij ⊗lc lc jq ⊗lc lc qk ).
However, since the previous TCN is minimal, the previously existing minimal constraints
lc li and lc jk imply (lc lp ⊗lc lc pi ) and (lc jq ⊗lc lc qk), respectively. That is, lc li ⊆lc(lc lp ⊗lc lc pi ) and
lc jk ⊆lc(lc jq ⊗lc lc qk ) Thus, lc''lk is also implicitly implied by lc’lk (lc’lk ⊆lclc''lk ). Here, we have
assumed the associative property for ⊗lc, which is obvious from its definition.
iv) Derived constraints obtained in the closure process do not need to be closured again if the
previous TCN is minimal. That is, no constraint in the TCN would become more restricted
if derived constraints were also closured. Let suppose lc lk is modified in the third loop of
closure process:
lc’lk = lc lk ⊕ lc (lc li ⊗lc lc ij ⊗lc lc jk )
such that it should be propagated to the (nl , nk , np ) subTCN (Figure 8). Thus, the following
derived constraints should be obtained:
53

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

lc’lp = lc lp ⊕ lc (lc’lk ⊗lc lc kp )

lc’pq = lc pq ⊕lc (lc pl ⊗lc lc’lk ).

For constraint lc’lp , we have,
lc’lp = lc lp ⊕lc (lc’lk ⊗lc lc kp ) = lc lp ⊕lc ((lc lk ⊕lc (lc li ⊗lc lc ij ⊗lc lc jk )) ⊗lc lc kp ).
However, since ⊗lc distributes over ⊕ lc,
lc’lp = lc lp ⊕ lc ((lc lk ⊗lc lc kp ) ⊕ lc (lc li ⊗lc lc ij ⊗lc lc jk ⊗lc lc kp )).
Since the previous TCN is minimal, the minimal constraints lc pi and lc pj should previously
exist, such that lclp ⊆lc(lc lk ⊗lc lc kp ) and lc jp ⊆lc(lc jk ⊗lc lc kp ). Thus,
lc’lp ⊆lc lc lp ⊕ lc (lc li ⊗lc lc ij ⊗lc lc jp ).
However, in the third loop of the closure process, the following derived constraint is
computed:
lc''lp = lc lp ⊕lc (lc li ⊗lc lc ij ⊗lc lc jp ).
Thus, lc’lp is already represented in the obtained constraint lc''lp (that is, lc''lp ⊆lc lc'lp ). In a
similar way,
lc''pq = lc pq ⊕lc (lc pi ⊗lc lc ij ⊗lc lc jq )
is also obtained in the proposed closure process, such that lc''pq ⊆lc lc'pq .
Therefore, each derived constraint (any combinable path across lc ij ) between any pair of nodes
in the TCN is computed, so that the closure process obtains a path-consistent TCN. ◊

np

lclp

lcpk

lclk

nl
lclj

lc ij

ni

nk
nj

lcjk

Figure 8: lc lk is also propagated to lc lp and lc pq
Theorem 7. The proposed reasoning processes obtain a minimal TCN, if the previous TCN is a
minimal TCN.
Proof: Montanari (1974) shows that when composition distributes over intersection (i.e.: ⊗
distributes over ⊕), any path-consistent TCN is also a minimal TCN). This is the case in nondisjunctive metric TCNs (Dechter et al., 1991). In our case, ⊗lc distributes over ⊕lc (Theorem 4) and
the closure process obtains a path consistent TCN (Theorem 6). Therefore, the proposed reasoning
processes also obtain a minimal TCN. ◊

54

BARBER

New input
constraint

INITIAL
TCN
No nodes, No
constraints, No I-L-Sets

Input Constraint
( ni lcij nj )
If (ni lcij nj ) is consistent
Reasoning Process: Updating + Closure processes
Consistency-Test: Consistent TCN
Closure Process: Path-Consistent TCN.
Distributive Property ( ⊗lc over ⊕lc): Minimal
New consistent and minimal TCN
New complete and sound set of I-L-Sets

Figure 9: An incremental reasoning process
Theorem 8. At each updating process, reasoning algorithms obtain a complete and a sound new set
of I-L-Sets (Definition 4), if they are applied on a previous minimal TCN and a previous sound and
complete set of I-L-Sets.
Proof:
i) The new set of I-L-Sets is complete. The consistency test of the updated constraint lc'ij obtains
all possible new I-L-Sets that can appear when lc'ij is added to the TCN, except those I-L-Sets
which are related to the mutual exclusion of the disjuncts in lc'ij (which are determined in the
Put-Label function):
a) No new I-L-Sets can appear in which some label of lc'ij does not participate. Otherwise,
they would have been detected in a previous updating process, since the previous set of
I-L-Sets is assumed complete. Thus, some label of lc’ij should always participate in any
new I-L-Set that appears when lc’ij is updated.
b) All new I-L-Sets (in which some label of lc’ij participates) are detected in the consistency
test of lc’ij. Let's assume that a new and undetected I-L-Set exists {Rk , R1 , R2 , ....., Rp } in
which some new elemental constraint eck{Rk}∈lc'ij takes part. Thus, the elemental
constraints associated to {R1 , R2 , ....., Rp } compute a derived elemental constraint ecx
between the nodes ni and nj :
(ecx {R1, R2, ....., Rp})

/

(ecx {R1, R2, ....., Rp}) ⊕ lc (eck{Rk}) =∅

This elemental constraint ecx is already represented in the previously existing constraint lc ij

55

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

between ni and nj since the previous TCN is minimal2 . Thus, eck ⊕ecx=∅, such that the I-LSet {Rk , R1 , R2 , ....., Rp } is detected in the consistency test of lc’ij . In conclusion, all new
inconsistent sets of elemental constraints in which lc'ij participates are detected and no other
new I-L-Sets can exist. Therefore, the new set of I-L-Sets is complete if the previous set
of I-L-Sets is complete.
ii) The new set of I-L-Sets is sound. All new I-L-Sets obtained represent inconsistent sets of
elemental constraints. This is trivial, given the consistency test function. ◊
In conclusion, the proposed reasoning algorithms obtain a minimal (and consistent) TCN if they
are applied to a previous minimal-TCN (Figure 9). Therefore, the reasoning algorithms guarantee
TCN consistency and obtain a minimal TCN and a complete and sound set of I-L-Sets at each new
input assertion.
4.4 Global Labeled-Consistency
In a minimal (binary) disjunctive network, every subnetwork of size two is globally consistent
(Dechter, 1992). Therefore, any local consistent instantiation of a subset of two variables can be
extended to a full consistent instantiation. However, to assure that a local consistent instantiation of
a subset of more that two variables is overall consistent, the partial instantiation should be propagated
on the whole TCN (van Beek, 1991). Thus, assembling a TCN solution can become a costly
propagation process in disjunctive TCNs, even though a minimal TCN was used. The proposed
reasoning processes maintain a complete and sound set of I-L-Sets (Theorem 8). Thus, we can deduce
if a locally consistent set of elemental constraints is overall consistent by means of label sets
associated to labeled elemental constraints and the set of I-L-Sets. Specifically, we can deduce
whether any locally consistent instantiation of k variables (1<k<n) is overall consistent. Let’s see the
following example, which is based on a previous one proposed by Dechter, Meiri and Pearl (1991):
Example 2: "Dave goes walking to work in [25’ 50’]. John goes to work either by car
[10’ 30'], or by bus [45’ 60’]. Fred goes to work either by car [15' 20'],
or in a carpool [35' 40'], or walking [55’ 60’]. Today, they all left their
home between 6:50 and 7:50 (at t1, t2 and t3 time-points), and arrived at
work at just the same time (t4 ) before 8:00."
Here, we have the following labeled disjunctive constraints where, T0 represents the initial time
(6:50) and granularity is in minutes:
t1 - T0 ∈ {[0 60]R0 },
t4 - t 1 ∈ {[25 50]R0 },

t 2 - T0 ∈ {[0 60]R0 },

t 3 - T0 ∈ {[0 60]R0 },

t 4 – t 2 ∈ {[10 30]R1 , [45 60]R2 },

t 4 - T0 ∈ {[0 70]R0 },

t 4 – t 3 ∈ {[15 20]R3 , [35 40]R4 , [55 60]R5 }.

The minimal TCN of Example 2 is represented in Figure 10. Here, the binary constraints between
each time-point and T0 represent unary constraints and restrict interpretation domains for variables
(t1 , t2 , t3 , t4 ). Obviously, this minimal TCN is not a globally consistent TCN. For instance,

2

The elemental constraint ec x is already represented in an explicit way, or by means of another elemental constraint ecy
(ecy⊆ Tec x, {labely }⊆{R1, R2, ....., Rp}) due to the simplification process performed in the operation ∪lc . In both cases,
ec k⊕ec x=∅, ec k⊕ecy =∅.

56

BARBER

instantiations {(t1 =0), (t2=0), (t3=0)} are consistent with the existing constraints involved among (T0 ,
t1 , t2 , t3 ), but this partial solution cannot be extended to the overall TCN.

{[0 45]}

t1
{[25 50]}

{[-35 35]}
{[0 55]}

T0
{[-35 40}]

t3

{[15 20] [35 40] [55 60]}

t4

{[-50 45]}
{[10 30] [45 60]}

{[0 60]}

t2

{[25 70]}

Figure 10: Minimal TCN of Example 2
Let’s consider the TCN with labeled constraints. For reasons of simplicity, we only denote the
labeled constraints among (T0 , t1 , t2 , t3 ):
(T0 {[5 45]{R0 R5}, [0 45]{R0 R4}, [0 45]{R0 R3}} t 1 ),
(T0 {[0 25]{R2 R0}, [5 60]{R1 R0 R4}, [25 60]{R1 R0 R5}, [0 60]{R1 R0 R3}} t 2 ),
(T0 {[25 55]{R0 R2 R3}, [0 15] {R0 R5}, [0 35]{R0 R1 R4}, [5 55]{R0 R1 R3}, [5 35]{R0 R2 R4}} t 3 ),
(t 1 {([-5 35]{R0 R2}, [-40 5] {R0 R1}} t 2 ),
(t 1 {[-15 15]{R0 R4}, [-35 -5]{R0 R3}, [5 35]{R0 R5}} t 3 ),
(t 2 {[5 30] {R1 R0 R4}, [-45 -25]{R2 R0 R3}, [25 50]{R1 R0 R5}, [-15 10]{R1 R0 R3}, [-25 -5]{R2 R0 R4}, [-5 15]{R2 R0 R5}} t3 ).

The set of I-L-Sets is {{R1 R2 } {R3 R4 } {R3 R5 } {R4 R5}}. From this labeled TCN and the set of
I-L-Sets, we can deduce that instantiations {(t1 =0), (t2 =0), (t3 =0)} are not overall consistent. These
instantiations are not locally consistent with the labeled constraints in the subTCN (T0 , t1 , t2 , t3 ): All
label sets associated to possible simultaneous fulfillment of
(T0 {[0 0]} t 1 ), (T0 {[0 0]} t 2 ) and (T0 {[0 0]} t 3 )

are I-L-Sets. That is, all label sets in the Cartesian product
{{R0 R4 } {R0 R3 }} Χ {{R2 R0 } {R1 R0 R3 }} Χ {{R0 R5 } {R0 R1 R4 }}

are I-L-Sets. Thus, the set of I-L-Sets can be used to deduce consistency of a set of labeled elemental
constraints and to obtain a globally consistent labeled-TCN.
Theorem 9. Let’s assume a labeled-TCN of n nodes (and the corresponding complete and sound set
of I-L-Sets) and a local set of k (1≤k≤( n2 )) labeled elemental constraints in the TCN, each one of
which is between any pair of nodes:
{lec1 , lec2 ,....., leck } ≡ {(ec 1 {label1 }), (ec2 {label2 }), ..., (ec k {labelk })}.

57

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

The local set of labeled elemental constraints {lec1 , lec2 , ... , leck }is overall consistent iff the setunion of their associated label sets (∪i=1,k {labeli }) is not an I-L-Set.
Proof: The label set (∪i=1,k {labeli }) is the support-set of the simultaneous fulfillment of {lec1 , lec2 ,
--- , leck }. Moreover, the set of I-L-Sets is complete and sound with respect to overall TCN (Theorem
8), such that any label set not in the set of I-L-Set is overall consistent. Therefore (Theorem 2),
(∪i=1,k {labeli }) and {lec1 , lec2 , ... , leck } are overall consistent iff ∪i=1,k {labeli } is not an I-L-Set. ◊
Definition 5 (Labeled-consistency3 ): Let’s assume a labeled-TCN of n nodes (and the corresponding
complete set of I-L-Sets) and a set of k (1≤k≤(n2 )) constraints, each one of which is between any pair
of nodes in the TCN:
{c ij } / 1≤i≤n, 1≤j≤n, i≠j.
The set of constraints {c ij } is labeled-consistent with respect to the nodes involved in these
constraints, iff:
i)

For each constraint cij , there exists an elemental labeled constraint elc ij.x between (ni , nj ) in
the TCN such that elc ij.x satisfies cij . That is: ∀cij , ∃elc ij.x∈lc ij / c ij ⊕ ecij.x ≠ ∅.

ii)

The resulting set of the union of label sets associated to these elemental labeled constraints
(which satisfy {c ij }) is not an I-L-Set: U ∀cij{label ij.x} is not an I-L-Set. Note that this is the
condition of Theorem 9. ◊

Theorem 10. Let’s assume a labeled-TCN of n nodes (and the corresponding complete set of I-LSets) and a set of k (1≤k≤( n2 )) constraints, each one of which is between any pair of nodes in the
TCN:
{c ij } / 1≤i≤n, 1≤j≤n, i≠j.
The set of constraints {c ij } is overall consistent iff {c ij } is labeled-consistent with respect to the
nodes involved in constraints {c ij }.
Proof: The proof is trivial according to Definition 5 and Theorem 9. We have that the set of
constraints {c ij } is consistent iff there exists a local set of elemental constraints in the TCN {elc ij.x}
that makes {c ij} labeled-consistent (Definition 5). Thus, the local set {elc ij.x} is consistent (Theorem
9), such that {c ij } is also consistent. ◊
For instance, we can determine whether any pair of constraints c' ij and c'kl can hold simultaneously
(that is, they are overall consistent) if:
∃elc ij.x∈lc ij / c' ij ⊕ ecij.x≠∅ ∧ ∃elc kl.y∈ckl / c'kl ⊕ eckl.y≠∅ ∧ {labelij.x}∪{labelkl.y }
is not an I-L-Set.
Moreover, any local instantiation of any k-1 (1<k≤n) variables {t1 =v1 , t2 =v2 , ..., t(k-1)=v(k-1)} can
be extended to a global solution if:
∃elc 10.x∈lc 10 / v1∈ec10.x,...... , ∃elc (k-1)0.y∈lc (k-1)0 / v(k-1)∈ec10.x,
where lc i0 is the constraint between ni and T0 , and {label10.x}∪{label20.y }∪ .... ∪{label(k-1)0.y }is not
and I-L-Set.
3

We need to introduce the concept of labeled-consistency since it is a different concept from the consistency concept.

58

BARBER

For instance, in Example 2 of Figure 10, the partial instantiation {(t1 =0), (t2 =5), (t3 =5)} is
consistent. We have:
([0 45]{R0 R3})∈lc10 / 0∈[0 45],

([0 60]{R1 R0 R3})∈lc20 / 5∈[0 60],

([5 55]{R0 R1 R3})∈lc30 / 5∈[5 55],

and {R0 R3 }∪{R0 R1 R3}∪{R0 R1 R3 }={R0 R1 R3 } is not an IL-Set. Thus, this partial solution can be
extended to a global solution. For instance, {(t1 =0), (t2 =5), (t3 =5), (t4 =25)}.
Therefore, a labeled-TCN can be considered as a globally labeled-consistent TCN. That is, on the
basis of the concepts introduced by Dechter (1992):
Definition 6. (Local Labeled-consistency): A partial instantiation of variables (1≤k<n) {t1 =v1 , t2 =v2 ,
..., tk =vk } is local labeled-consistent if it is labeled-consistent with respect to (T0 , t1 , t2 , ..., tk ) nodes.
This also holds for k=n. ◊
Definition 7. (Global Labeled-consistency): A labeled sub-TCN (with the global set of I-L-Sets) is
global labeled-consistent if any partial instantiation of variables in the sub-TCN, which is local
labeled-consistent, can be extended to the overall TCN. A globally labeled-consistent TCN is one in
which all its sub-TCNs are globally labeled-consistent. ◊
Theorem 11. At each new assertion, the proposed reasoning processes obtain a globally labeledconsistent TCN, if they are applied on a previous minimal TCN and a previous sound and complete
set of I-L-Sets.
Proof: The proof is trivial according to the previous definitions (Definition 6 and Definition 7) and
to the properties of the reasoning processes (Theorem 7 and Theorem 8). Any partial instantiation
in any subTCN, which is labeled-consistent with respect to the nodes involved in the partial
instantiation, is overall consistent (Theorem 10). ◊
Similar expressions can be made for k-labeled-consistency and strong k-labeled-consistency on
the basis of the concepts provided by Freuder (1982). Therefore, the set of I-L-Sets in a labeled-TCN
provides a useful way to assure whether a local instantiation of variables can be part of a global
solution. Moreover, Freuder (1982) shows that in a strong k-consistent TCN, consistent instantiations
of variables of any subnetwork of size k can be found in a backtrack-free manner and in any variable
ordering. This is also a consequence of the decomposability (Montanari, 1974; Dechter et al., 1991)
or globally consistency (Dechter, 1992) properties. Obviously, this feature also holds for labeled
TCNs.
4.5 Analysis of Temporal Complexity
Let’s analyze the computational cost of the proposed reasoning processes. These processes are,
basically, an incremental path-consistent algorithm (Barber, 1993). At each updating process of a
new input constraint on a TCN with n nodes, the computational cost of updating and closure
processes is bounded by 'n2 (O(⊗lc) + O(⊕lc))'. In the proposed reasoning process, the path-consistent
algorithm obtains a minimal disjunctive metric TCN. This is possible due to the management of
labeled constraints, associated label sets, and I-L-Sets. Thus, the complexity of reasoning processes
is mainly due (instead of a complex closure process) to the management of complex data structures
(labeled constraints, associated label sets, and I-L-Sets). That is, the complexity of the proposed

59

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

reasoning processes is mainly due to the complexity of operations ⊗lc and ⊕ lc.
The computational cost of ⊗lc and ⊕ lc depends on the number of elemental constraints in labeled
constraints, the size of associated label sets, and the size of I-L-Sets in the previous minimal labeled
TCN. Let 'n' be the number of nodes, 'l' the maximum number of disjuncts (or labels) in input
constraints, and 'e' the number of updated input constraints in the previous TCN. The maximum
number of labels in the TCN is l*e, since each disjunct in each updated input labeled constraint has
its own, unequivocal label. Moreover, any I-L-Set can have as maximum one label from each input
labeled constraint lc ij , since: (i) elemental constraints in lc ij are pairwise disjoint, such that each pair
of labels in lc ij is added to the set of I-L-Sets, and (ii) any superset of an existing I-L-Set is also an
I-L-Set. Thus, the maximum number of labels in any I-L-Set is e. Furthermore, each label in an I-LSet should be from a different input labeled constraint. There are e input labeled constraints, and each
input labeled constraint has as maximum l labels. Thus, the maximum number of I-L-Sets of q-length
(1≤q≤e) is (( qe ) lq ).
Therefore, the number of i-length (1≤i≤e) I-L-Sets is Σi=1,e (( ei ) li ) = O(2e le). However, any
superset of an I-L-Set is already known as inconsistent, such that supersets are not stored in the set
of I-L-Sets. Thus, the number of I-L-Set is bounded by O(le). Additionally, we also have e*( l2 ) I-LSets of 2-length, since the l disjuncts in each updated constraint are mutually exclusive among them.
Similarly, the maximum number of associated label sets is also bounded by O(le), each one with a
maximum of e labels. Thus, the number of elemental constraints (or labeled subintervals) in any
labeled constraint is bound by O(le), since each elemental constraint in a labeled constraint has its
own associated label set.
According to these parameters, the computational cost of each updating process is bounded by
O(n2 l3e). The recovery process of constraints has a constant cost, since a minimal-TCN is always
maintained. The computational cost of the proposed algorithms agreed with the computational cost
inherent to the problem of the management of disjunctive metric constraints (Dechter, 1991). In fact,
the closure process could be considered as an integrated management of the le alternative nondisjunctive TCNs in which a disjunctive TCN can be split, as it is shown by Dechter, Meiri and Pearl
(1991). It should be noted that l can be bounded in some typical problems like scheduling, where
usually l≤ 2 (Garrido et al., 1999), or by restricting domain size (range or granularity) in metric
algebras. On the other hand, several improvements can be made on the described processes. For
example, an efficient management of label sets has a direct influence on the efficiency of the
reasoning processes. Thus, each label set (for instance, {R3 R5 R8 }) can be considered as a
unidimensional array of bits, which is the binary representation of an integer number (for instance
(23 +25 +28 )). Therefore, each associated label set is represented by a number and the set of I-L-Sets
becomes a set of numbers. Matching and set-union processes on label sets in operations ⊗lc and ⊕lc
can be efficiently performed by means of operations on integer numbers with a constant cost.
Therefore, the computational cost can be bounded by O(n2 l2e).
Other alternative implementations are under study. Two different approaches exist for temporal
constraint management (Brusoni et al., 1997; Yampratoom, Allen, 1993; Barber, 1993). The first
approach is to maintain a closured TCN by recomputing the TCN at each new input constraint and
making the derived constraints explicit. Here, queries are answered in constant time, although this
implies a high spatial cost. The second approach is to explicitly represent only input constraints, such
that the spatial requirements are minimum. However, further computation is needed at query time
and when consistency of each new input constraint is tested. The proposed reasoning methods hold
60

BARBER

in the first approach, which seems more appropriate for problems where queries on the TCN are more
usual tasks than updating processes.
In addition, the proposed reasoning algorithms obtain a sound and complete set of I-L-Sets and
a globally labeled-consistent TCN. Regrettably, assembling a solution in a labeled TCN, although
backtrack free, is also costly due to the exponential number of I-L-Sets. However, these features offer
the capability of representing and managing special types of non-binary disjunctive constraints (later
detailed in Section 6).
Other reasoning algorithms for query processes on a non-closured TCN, as well as CSP
approaches can be defined on the basis of the labeled temporal algebra described. Less expensive
algorithms can be applied on labeled constraints by using the specified operations ⊗lc, ⊕lc, ∪Τlc and
⊆Τlc. For instance, the TCA algorithm as is applied by Allen (1983), and the k-consistency algorithms
like those described in (Cooper, 1990; Freuder, 1978). Moreover, a minimal TCN of labeled
constraints can be obtained without enforcing global consistency; for example, by applying the naive
backtracking algorithm described by Dechter, Meiri and Pearl (1991), which is O(n3 le).

5. Interval-Based Constraints Through Labeled Point-Based Constraints
The integration of quantitative and qualitative information has been the goal of several temporal
models, as was described in Section 1. When intervals are represented by means of their ending
points Ii + Ii -, integration of constraints on intervals and points seems to require some kind of nonbinary constraints between time-points (Gerevini & Schubert, 1995; Schwalb & Dechter, 1997;
Drakengren & Jonsson, 1997). In this section, the proposed temporal model is applied in order to
integrate interval and point-based constraints. Constraints on intervals are managed by means of
constraints on ending points of intervals and I-L-Sets. Likewise, metric information can also be added
to interval constraints such that an expressive way of integrating qualitative and quantitative
constraints is obtained.
5.1 Symbolic Interval-Based Constraints
Symbolic constraints on intervals express the qualitative temporal relation between two intervals.
Each symbolic constraint is a disjunctive subset of 13 elemental constraints, which are mutually
exclusive among them (Allen, 1983). For example, the following constraint
I1 {ec 1 , ec2 } I2 ,

ec1 , ec2 ∈{b, m, o, d, s, f, e, bi, mi, oi, di, si, fi},

really means 'I1 [ (ec1 ∨ ec2 ) ∧ ¬(ec1 ∧ ec2 ) ] I2', since ec 1 and ec 2 are mutually exclusive, and one and
only one elemental constraint should hold. For reasons of simplicity, we only consider two disjuncts
in the symbolic constraint. However, these expressions can be easily extended for managing from
2 to 13 disjuncts. The above expression can be expressed as:
I1 [ (ec1 ∧ ¬ec2 ) ∨ (¬ec1 ∧ ec2 ) ] I2 ≡
I1 [ (ec1 ∨ ¬ec1 ) ∧ (ec 2 ∨ ¬ec2 ) ∧ ¬(ec1 ∧ ec2 ) ∧ ¬ (¬ec1 ∧ ¬ec2 ) ] I2
In this way, we have:

61

(e2).

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

i)

The constraints [I1 (ec1 ∨ ¬ec1 ) I2 ] and [I1 (ec2 ∨ ¬ec2 ) I 2 ] can be expressed as disjunctive
metric constraints on the same pairs of time-points,

ii)

The constraints [I1 ¬(ec 1 ∧ ec2 ) I2 ] and [I1 ¬(¬ec1 ∧ ¬ec2 ) I2 ] can be expressed as a mutual
exclusion among the associated labels of the above point-based constraints. That is, as a set
of I-L-Sets.

We present a simple example to illustrate these conclusions. For instance, (I1 {before after} I2 )
can be expressed by means of constraints among the time points I1 -, I1 +, I2 - and I2 +, as:
[I1 {b a} I2 ] ≡ (I1 + {(0 ∞){Rb1}} I2 -) ∨ (I1 - {(-∞ 0){Ra1}} I2 +).
Thus, when intervals are represented by means of their ending points Ii + Ii -, an interval-based
constraint gives rise to disjunctive constraints between different pairs of time points (i.e.: non-binary
constraints). These non-binary constraints can be represented as I-L-Sets. Thus, according to the
above expression (e2),
[I1 {b a} I2] ≡ [I1 (b ∨ ¬b) I2 ] ∧ [I1 (a ∨ ¬a) I2 ] ∧ [I1 ¬(b ∧ a) I2 ] ∧ [I1 ¬(¬b ∧ ¬a) I2 ],
we have:
I1 before I2 ⇔ I1 + {(0 ∞){Rb1}} I2 -,
I1 ¬before I2
⇔ I1 + {(-∞ 0]{Rb2}} I2 -,

I1 after I2 ⇔ I1 - {(-∞ 0){Ra1}} I2 +,
I1 ¬after I2 ⇔ I1 - {[0 ∞){Ra2}} I2 +.

Therefore, [I1 {b a} I 2 ] can be expressed as:
[I1 + {(0 ∞){Rb1} (-∞ 0]{Rb2}} I2 -] ∧ [I1 - {(-∞ 0){Ra1} [0 ∞){Ra2}} I2 +] ∧
¬ [ (I1 + {(0 ∞){Rb1}} I2 -) ∧ (I1 - {(-∞ 0){Ra1}} I2 +) ] ∧
¬ [ (I1 + {(-∞ 0]{Rb2}} I2 -) ∧ (I1 - {[0 ∞){Ra2}} I2 +) ],
which is equivalent to (by using the labels associated to each elemental constraint):
[I1 + {(0 ∞){Rb1} (-∞ 0]{Rb2}} I2 -] ∧ [I1 - {(-∞ 0){Ra1} [0 ∞){Ra2}} I2 +]
and {Rb1 Ra1},{Rb2 Ra2} are I-L-Sets, such that one and only one disjunctive symbolic constraint
holds.
Thus, symbolic constraints between intervals can be represented by means of: (i) a set of
disjunctive metric constraints between time-points, and (ii) a set of I-L-Sets. In Table 1, the
equivalent metric constraints between interval ending time points for each elemental interval-based
constraint are detailed. According to this table, the following steps allow us to represent disjunctive
symbolic constraints between intervals by means of disjunctive metric constraints between interval
ending points and I-L-Sets:
i)

Each interval I i is represented by means of its ending points Ii +, Ii-. By default, (I i - {(0, ∞){R0}}
Ii +) holds.

ii)

A symbolic constraint between two intervals (Ii cij Ij) is composed of a disjunctive set of
(from 1 to 13) elemental symbolic constraints cij ={ecij.k }⊆{b, m, o, d, s, f, e, bi, mi, oi, di,
si, fi}.

iii) Each elemental symbolic constraint ec∈{b, m, o, d, s, f, e, bi, mi, oi, di, si, fi} is represented
62

BARBER

by a conjunctive set of disjunctive point-based metric constraints (fourth column of Table
1). This conjunctive set of point-based constraints expresses the ‘fulfillment or nonfulfillment’ (ec ∨ ¬ec) of the elemental symbolic constraint ec.
iv) A disjunctive set cij ={ecij.k } of elemental symbolic constraints between Ii and Ij is represented
by:
•

A conjunctive set of disjunctive point-based metric constraints between the time-points
Ii +, Ii -, Ij+ and I-j . This conjunctive set is composed by the constraints in the fourth column
of Table 1 for each elemental constraint in {ecij.k }.

•

A set of I-L-Sets that expresses the logical relation among elemental symbolic
constraints in {ec ij.k }. That is, 'one and only one elemental symbolic constraint in {ecij.k}
should hold':
iv.a) Only one elemental constraint in {ecij.k } should hold. This condition does not
need to be represented since the different sets of point-based constraints that
correspond to fulfillment of different elemental symbolic constraints (second
column of Table 1) are already mutually exclusive.
iv.b)

One of the elemental symbolic constraints in {ecij.k } should hold. Let S be the
label sets, where each label set corresponds to the point-based constraints which
are related to the non-fulfillment of each elemental symbolic constraint in {ec ij.k }
(third column of Table 1). Thus, the Cartesian product among the label sets in S
is a set of I-L-Sets.

For instance, I1 {b m s di} I2 can be represented as:
(I1 - { (0 ∞){R0}} I1 +), (I2 - { (0 ∞) {R0}} I2 +),

I1 {b ¬b} I2 ⇒ (I1 + {(0 ∞){Rb1} (-∞ 0]{Rb2}} I2 -),
I1 {m ¬m} I2 ⇒ (I1 + {[0 0] {Rm1} (0 ∞){Rm2} (-∞ 0){Rm3}} I2 -),
I1 {s ¬s} I2 ⇒ (I1 - {[0 0] {Rs1} (0 ∞){Rs3} (-∞ 0){Rs4}} I2 -) ∧ (I1 + {(0 ∞){Rs2} (-∞ 0]{Rs5}} I2 +),
I1 {di ¬di} I2 ≡ I2 {d ¬d} I1 ⇒ (I2 - {(-∞ 0){Rd1} [0 ∞){Rd3}} I1 -) ∧ (I2 + {(0 ∞){Rd2} (-∞ 0]{Rd4}} I1 +).

Moreover, one of the symbolic constraints in {b, m, s, di} should hold. Thus (according to Point
iv.b of the method), the Cartesian product of the associated labels related to the non-fulfillment of
each elemental symbolic constraints in {b, m, s, di}. That is:
{{Rb2 }Χ{Rm2 , Rm3 }Χ{Rs3 , Rs4 , Rs5 }Χ{Rd3 , Rd4 }

should be explicitly included in the set of I-L-Sets.
By applying this method, qualitative interval-based constraints can be fully integrated in the
proposed labeled point-based constraints. In this case, the interpretation domain for time-points {Ii Ii +} can be restricted to only three values ({D}={(-∞, 0), [0 0], (0 ∞)}), such that, l=3. Therefore, the
computational cost of reasoning algorithms is bounded by O(n2 3 2e).
To illustrate the proposed method, let’s show a typical example on symbolic interval-based
constraints (Figure 11.a), which was given by Allen (1983). This example shows how interval-based
constraints can be represented and managed by means of disjunctive metric point-based constraints
and a minimal IA-TCN can be obtained.
63

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

Ii ecij.k Ij

Ii ecij.k Ij

Ii ¬ ecij.k Ij

Ii (ecij.k ∨ ¬ec
¬ ij.k) Ij

Ii before Ij

Ii + {(0 ∞){Rb1}} Ij -

Ii + {(-∞ 0]{Rb2}} Ij -

Ii + {(0 ∞){Rb1} (-∞ 0]{Rb2}} Ij -

Ii meets Ij

Ii + {[0 0]{Rm1}} Ij -

Ii + {(0 ∞){Rm2} (-∞ 0){Rm3}} Ij -

Ii + {[0 0]{Rm1} (0 ∞){Rm2} (-∞ 0){Rm3}} Ij -

Ii during Ij

Ii - {(-∞ 0){Rd1}} Ij -

(Ii - {[0 ∞){Rd3}} Ij -)

Ii - {(-∞ 0){Rd1} [0 ∞){Rd3}} Ij -

Ii + {(0 ∞){Rd2}} Ij +
Ii starts Ij

Ii + {[0 0]{Rf1}} Ij +
-

Ii {(-∞ 0){Rf2}} Ij

-

∨
∨

Ii + {(0 ∞){Rd2} (-∞ 0]{Rd4}} Ij +
Ii - {[0 0]{Rs1} (0 ∞){Rs3} (-∞ 0){Rs4}} Ij Ii + {(0 ∞){Rs2} (-∞ 0]{Rs5}} Ij +

(Ii + {(-∞ 0]{Rs5}} Ij +)

(Ii + {(0 ∞){Rf3} (-∞ 0){Rf4}} Ij +)

Ii + {[0 0]{Rf1} (0 ∞){Rf3} (-∞ 0){Rf4}} Ij +

(Ii {[0 ∞){Rf5}} Ij )
-

Ii - {(-∞ 0){Rf2} [0 ∞) {Rf5}} Ij -

-

(Ii + {[0 ∞){Ro4}} Ij -)

+
Ii overlaps Ij Ii {(-∞ 0){Ro1}} Ij

Ii equal Ij

(Ii + {(-∞ 0]{Rd4}} Ij +)

(Ii - {(0 ∞){Rs3} (-∞ 0){Rs4}} Ij -)

Ii - {[0 0]{Rs1}} Ij Ii + {(0 ∞){Rs2}} Ij +

Ii finishes Ij

∨

Ii + {(-∞ 0){Ro1} [0 ∞){Ro4}} Ij -

Ii + {(0 ∞){Ro2}} Ij +

∨

(Ii + {(-∞ 0]{Ro5}} Ij +)

Ii + {(0 ∞){Ro2} (-∞ 0]{Ro5}} Ij +

Ii - {(0 ∞){Ro3}} Ij -

∨

(Ii - {(-∞ 0]{Ro6}} Ij -)

Ii - {(0 ∞){Ro3} (-∞ 0]{Ro6}} Ij -

Ii + {[0 0]{Re1}} Ij +

(Ii + {(0 ∞){Re3} (-∞ 0){Re4}} Ij +)

Ii + {(0 ∞){Re3} [0 0]{Re1} (-∞ 0){Re4}} Ij +

Ii - {[0 0]{Re2}} Ij -

∨ (Ii - {(0 ∞){Re5} (-∞ 0){Re6}} Ij -)

Ii - {(0 ∞){Re5} [0 0]{Re2} (-∞ 0){Re6}} Ij -

Table 1: Interval-based constraints and their equivalent disjunctive metric constraints between
interval ending points (Cells in the second and fourth columns are a conjunctive set of constraints)

Symbolic
Constraint
(IA {d di} IB)

⇒

(IB {d di} IC)

⇒

(ID {m s} IA)

⇒

(ID {o} IB)

⇒

(ID {m s} IC)

⇒

Disjunctive Metric Constraint between I+ I-

Inconsistent-Label-Sets

IA - {(-∞ 0){R1} [0 ∞){R3}} IBIA + {(0 ∞){R2} (-∞ 0]{R4}} IB+
IB- {(-∞ 0){R5} [0 ∞){R7}} IA IB+ {(0 ∞) {R6} (-∞ 0]{R8}} IA +

{R4 R8 } {R3 R8 }
{R4 R7 } {R3 R7 }

IB- {(-∞ 0){R9} [0 ∞){R11}} ICIB+ {(0 ∞) {R10} (-∞ 0]{R12}} IC+
IC- {(-∞ 0){R13} [0 ∞){R15}} IBIC+ {(0 ∞) {R14} (-∞ 0]{R16}} IB+
+
ID {[0 0]{R17} (0 ∞){R18} (-∞ 0){R19}} IA ID- {[0 0]{R20} (0 ∞){R22} (-∞ 0){R23}} IA ID+ {(0 ∞){R21} (-∞ 0]{R24}} IA +
ID+ {(-∞ 0){R0}} IBID+ {(0 ∞){R0}} IB+
ID- {(0 ∞){R0}} IBID+ {[0 0]{R25} (0 ∞){R26 (-∞ 0){R27}} ICID- {[0 0]{R28} (0 ∞){R30} (-∞ 0){R31}} ICID+ {(0 ∞){R29} (-∞ 0]{R32}} IC+

{R12 R16 } {R11 R16 }
{R12 R15 } {R11 R15 }
{R19 R24 } {R18 R24 } {R19 R23 }
{R18 R23 } {R19 R22 } {R18 R22 }

{R27 R32 } {R26 R32 } {R27 R31 }
{R26 R31 } {R27 R30 } {R26 R30 }

Table 2: Symbolic constraints in Figure 11.a by means of disjunctive metric
constraints between I+, I-

64

BARBER

Figure 11.a represents a path-consistent IA-TCN, which has inconsistent values in constraints
(Allen, 1983). In Table 2, we have the interval-based symbolic constraints for this example, the
corresponding disjunctive metric constraints between their ending time -points (Ii +, Ii -) and the
corresponding set of I-L-Sets (according to Table 1). Moreover, we also have:
(IA -{(0 ∞){R0}}IA +), (IB-{(0 ∞){R0}}IB+), (IC-{(0 ∞){R0}}IC+ ) and (ID- {(0 ∞){R0}}ID+).

When all these metric constraints among the ending time-points of intervals are updated according
the proposed methods in Section 4, the labeled minimal TCN in Table 3 is obtained. The associated
labels to each elemental constraint (disjunct) in constraints are not included for reasons of brevity.

D

D
{s,m}
A

{s,m}

{o}

{d, di}

A

B

{d,di}

{d, oi, f, e, fi,
si, s, o , di}

{o}

{d, di}

B

{d,di}

{s,m}

{s,m}
{d, di, s, si, e}

C

a) Path-Consistent IA-TCN

C

b) Minimal IA-TCN

Figure 11: Path-Consistent and equivalent Minimal IA-TCN

IA+
IA+

IA-

IB+

{(-∞ 0)}

{(0 ∞),
(-∞ 0)}
{(0 ∞)}

IA-

{(0 ∞)}

IB+

{(-∞ 0), {(-∞ 0)}
(0 ∞)}

IB-

{(0 ∞)}

{(0 ∞),
(-∞ 0)}

IB-

{(0 ∞)}

{[0 0],
(0 ∞)}

{(-∞ 0),
(0 ∞)}
{(-∞ 0)}

{(0 ∞)}

{(0 ∞)}

IC-

ID+

ID-

{(-∞ 0)} {(-∞ ∞)} {(-∞ 0)} {(-∞ 0)} {(-∞ 0)}
{(0 ∞)}

{(-∞ 0), {[0 0], {(-∞ 0),
[0 0],
(0 ∞)}
[0 0]}
(0 ∞)}
{(-∞ 0), {(-∞ 0)} {(-∞ 0)} {(-∞ 0)}
(0 ∞)}
{(0 ∞)}

IC+ {(-∞ ∞)} {(-∞ 0)} {(-∞ 0), {(-∞ 0)}
(0 ∞)}
IC
{(0 ∞)} {(-∞ 0), {(0 ∞)} {(-∞ 0),
[0 0],
(0 ∞)}
(0 ∞)}
ID+ {(0 ∞)} {(-∞ 0), {(0 ∞)} {(-∞ 0)}
[0 0]}
ID-

IC+

{(0 ∞)}

{(-∞ 0),
(0 ∞)}

{(0 ∞)}

{(-∞ 0)}

{(-∞ 0)} {(-∞ 0)} {(-∞ 0)}
{(0 ∞)}

{(0 ∞),
[0 0]}

{(0 ∞)}

{(-∞ 0),
[0 0]}

{(0 ∞)}

{[0 0],
(0 ∞)}

{(-∞ 0),
[0 0]}
{(-∞ 0)}

{(0 ∞)}

Table 3: The minimal metric point-based TCN of the IA-TCN in Figure 11.a
65

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

Allen (1983) remarks that the symbolic constraint (IA {f fi} IC) cannot hold given the existing
constraints between IA, IB, IC and ID. In the labeled point-based TCN, (IA {f fi} IC) is represented
by a set of constraints among ending points of IA and IC. Moreover, the labels associated to each
labeled elemental constraint allow us to determine whether a set of elemental constraints between
different pairs of time-points can be part of a global solution (Theorem 10). Thus, we can deduce
whether (IA {f fi} IC) can hold in the point-based TCN.
The existing constraints between the ending time-points of IC and IA, with their associated labelsets are:
IC+ {(-∞ ∞){R25 R30 R29 R17 R22 R21 R0)∨{R27 R28 R29 R19 R20 R21 R0},
(-∞ 0){R27 R28 R29 R17 R22 R21 R9 R10 R15 R16 R1 R2 R7 R0 R8},
(0 ∞){R25 R30 R29 R19 R20 R21 R11 R12 R13 R14 R3 R4 R5 R0 R6}} IA+
IC-

{(0 ∞){R27 R28 R29 R17 R22 R21 R9 R10 R15 R16 R1 R2 R7 R8 R0},
[0 0] {R25 R30 R29 R17 R22 R21 R0}∨{R27 R28 R29 R19 R20 R21 R0},
(-∞ 0){R25 R30 R29 R19 R20 R21 R11 R12 R13 R14 R3 R4 R5 R6 R0}} IA-

Let's ask for each disjunct in (IA {f fi} IC):
i) The constraint (IA {f} IC) implies (IC+ {[0 0]} IA+) ∧ (IC- {(-∞ 0)} IA-). According to
Theorem 10, these constraints hold iff the set-union of the label sets associated to (IC+ [0 0]
IA+) and to (IC- (-∞ 0) IA-) is not an I-L-Set. We have two possibilities:
i.1) {R25 R30 R29 R17 R22 R21 R0 } ∪ {R25 R30 R29 R19 R20 R21 R11 R12 R13 R14 R3 R4 R5 R6 R0 } =
{R6 R5 R4 R3 R20 R19 R25 R30 R29 R17 R22 R21 R11 R12 R13 R14 R0 }, or
i.2)

{R27 R28 R29 R19 R20 R21 R0 } ∪ {R25 R30 R29 R19 R20 R21 R11 R12 R13 R14 R3 R4 R5 R6 R0 } =
{R14 R13 R12 R11 R30 R25 R27 R28 R29 R19 R20 R21 R3 R4 R5 R0 R6 }.

However, both label sets (i.1, i.2) are I-L-Sets: For instance, {R19 R22 } and {R27 R30 } are I-LSets (Table 2) and they are subsets of i.1 and i.2, respectively. Thus, (IA {f} IC) does not hold.
ii) The constraint (IA {fi} IC) implies (IC+ {[0 0]} IA+) ∧ (IC- { (0 ∞)} IA-). Similarly:
ii.1) {R25 R30 R29 R17 R22 R21 R0 } ∪ {R27 R28 R29 R17 R22 R21 R9 R10 R15 R16 R1 R2 R7 R8 R0 } =
{R16 R15 R10 R9 R28 R27 R25 R30 R29 R17 R22 R21 R1 R2 R7 R0 R8 }.

This label set is an I-L-Set. For instance, {R30 R27 } is an I-L-Set. Also,
ii.2) {R27 R28 R29 R19 R20 R21 R0 } ∪ {R27 R28 R29 R17 R22 R21 R9 R10 R15 R16 R1 R2 R7 R8 R0 } =
{R8 R7 R2 R1 R22 R17 R27 R28 R29 R19 R20 R21 R9 R10 R15 R16 R0 }.

Both these label sets (ii.1, ii.2) are also I-L-Sets. For instance, {R 30 R27} and {R19 R22 } are I-LSets. Thus, (IA {fi} IC) does not hold either.
In conclusion, the symbolic constraint (IA {f fi} IC) cannot hold on the globally labeled-consistent
point-based TCN. This conclusion could be also obtained from a minimal IA-TCN (Figure 11.b).
Additionally, we have that (IA {f fi} IC) implies (IA+ [0 0] IC+). That is, if the constraint (IA+ [0 0]
IC+) holds, we have that the associated constraints to the label sets {R25 R30 R29 R17 R22 R21 R0 } or
{R27 R28 R29 R19 R20 R21 R0 } should also hold. Each one of these label sets implies (IC - {[0 0]} IA-).
That is: (IA+ [0 0] IC+) → (IC- {[0 0]} IA-). Thus, the only way that (IA+ [0 0] IC+) can hold is if (IA
{e} IC) holds. These relations will be detailed in Section 6.
66

BARBER

5.2 Metric Constraints on Intervals
Metric constraints between intervals can also be managed in the described temporal model. From a
general point of view, metric information can be added to each elemental interval-based constraint
in a standard way (Table 4). These metric constraints on interval boundaries (Table 4) are similar to
the ones proposed by Staab and Hahn (1998).
IA Symbolic
Elemental
Constraints

IA Metric Elemental Constraints
cij ≡ {[dm1 dM1 ], [dm2 dM2 ], ..... [dmn dMn ]}
c'ij ≡ {[dm’ 1 dM’1 ], [dm’ 2 dM’2 ], ..... [dm’ n dM’n ]}
Ii

Ii before Ij

Ii (before cij ) Ij

Ii meets Ij

Ii (meets c ij ) Ij

Ii during Ij

Ii (cij during c'ij ) Ij

Ii starts Ij

Ii (starts c ij ) Ij

Ii finishes Ij

Ii (finishes cij ) I j

C

ij

Ij

Ii
Cij

Ij

Ii

C' ij

C
ij

Ij
Ii
C
ij

Ij
Ii

C
ij

Ij
Ii

Ii overlaps Ij

Ii (overlaps cij ) Ij

Cij

Ij

Ii

Ii equal Ij

Ii (cij equal c'ij ) Ij

C
ij

C'

ij

Ij

Table 4: Metric interval constraints on interval boundaries
Obviously, the metric constraints of Table 4 can be managed in the proposed model, by means
of metric constraints on interval ending points. Thus, symbolic constraints of Interval Algebra can
be extended in this way to metric domain. However, since each interval is represented by means of
its ending time-points, more flexible metric constraints on intervals can be represented by means of
metric constraints on their ending time-points. In this way, the described model also subsumes the
Interval Distance Sub Algebra model proposed by Badaloni and Berati (1996). Moreover, ending
points of intervals can also be related to the initial time-point T0 , and unary metric constraints on
interval durations can be expressed by means of metric constraints between the two ending points of
each interval:
dur (Ii ) = {[dm1 dM1 ], [dm2 dM2 ], ..... [dmn dMn ]} ⇒
(Ii - {[dm1 dM1 ], [dm2 dM2 ], ..... [dmn dMn ]} Ii +).

67

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

-

[ I1 ]

-

[ I1 ]

30
60

T0

I1

I1+

[

]

20
I2
[

50

]

{[140 150], [200 210]}

Figure 12: Metric constraints between intervals
Thus, following constraints (Figure 12):
(I1 {b, o} I2) ∧ (I1- is [[20 30], [50 60]} before I2-) ∧ (I2- is {[140 150], [200 210]} after T0 )
can be represented as (Table 1):
By default: (I1 - { (0 ∞){R0}} I1+), (I2- { (0 ∞){R0}} I2+), and
(I1 {b, o} I2)

⇒

(I1+ {(0 ∞){Rb1} (-∞ 0]{Rb2}} I2-),

(I1+ {(-∞ 0){Ro1} [0 ∞){Ro4}} I2-),

(I1+ {(0 ∞){Ro2} (-∞ 0]{Ro5}} I2+),

(I1- {(0 ∞){Ro3} (-∞ 0]{Ro6}} I2-),

⇒

(I1- is [[20 30], [50 60]} at the left of I2-)
(I2- is {[140 150], [200 210]} after T0 )

⇒

(I1- {[50 60]{R1} [20 30]{R2}} I2-),
(T0 {[140 150]{R3} [200 210]{R4}} I2- ),

and {Rb2 Ro4 }, {Rb2 Ro5 }, {Rb2 Ro6 }, {R1 R2 } and {R3 R4 } are I-L-Sets.

6. Reasoning on Logical Expressions of Constraints
In the described model, each disjunct in an input constraint is univocally associated to a label.
Moreover, the label set associated to each derived elemental constraint represents the support-set of
input elemental constraints from which the elemental constraint is derived. I-L-Sets represent
inconsistent sets of input elemental constraints. By reasoning on labeled disjunctive constraints,
associated label lists and I-L-Sets, the temporal model offers the capability of reasoning on logical
expressions of elemental constraints belonging to disjunctive constraints between different pairs of
time points. Let's assume the following labeled input constraints:
(ni lc ij nj ) ≡ (ni {(lecij.1 ){Rij.1} (lecij.2 ) {Rij.2} .....(lecij.p ) {Rij.p}} nj ),
(nk lc kl nl ) ≡ (nk {(leckl.1 ) {Rkl.1} (leckl.2 ) {Rkl.2} .....(leckll.q ) {Rkl.q}} nl )
i) To represent that two elemental constraints 4 (elc ij.x ∈lc ij , elckl.y∈lc kl ) cannot hold simultaneously
(that is ¬(elc ij.x ∧ elc kl.y )) the label set {Rij.x Rkl.y } should be added to the set of I-L-Sets.
ii) To represent a logical dependency between two elemental constraints, such as 'If lec ij.x then
leckl.y' (where lecij.x∈cij , leckl.y∈ckl ), the Cartesian product {Rij.x} Χ {{Rkl.1 , Rkl.2 , ....., Rkl.q }{Rkl.y }} should be added to the set of I-L-Sets.
iii) To represent that two elemental constraints (elc ij.x∈lcij , elckl.y ∈lc kl ) should hold simultaneously
(bi-directional logical dependency), the Cartesian products {Rij.x} Χ {{Rkl.1 , Rkl.2 , ....., Rkl.q }4

For reasons of simplicity, only two elemental constraints are shown. However, more than two disjunctions can be managed
in a similar way. Likewise, these features can be also applied to labeled derived constraints.

68

BARBER

{Rkl.y }} and {Rkl.y } Χ {{Rij.1 , Rij.2 , ....., Rij.p }-{Rij.x}} should be added to the set of I-L-Sets.
For instance, let’s see the Example 2 of Section 4.4 (Figure 10):
• To represent that ‘John goes to work by car and Fred goes to work walking’ is not possible,
{R1 R5 } should be asserted as an I-L-Set.
• To represent that ‘if John goes to work by car then Fred goes to work walking’, {R1 R3 } and
{R1 R4 } should be asserted as I-L-Sets.
• To represent that ‘if John goes to work by car then Fred goes to work walking, and vice versa’,
{R1 R3 }, {R1 R4 } and {R5 R2 } should be asserted as I-L-Sets.
In a similar way, logical relations among point-based and interval-based elemental constraints can
also be represented. For instance, the logical dependence "the duration of I1 is [5 8] if I2 is before I3
and the duration of I 1 is [12 15] if I2 is after I3" can be represented as:
(I2 {b, bi} I3 ) ⇒

(I2 + {(0 ∞){Rb9} (-∞ 0]{Rb10}} I3 -),
{Rb10 Rb12 } is an I-L-Set,

(I3 + {(0 ∞){Rb11} (-∞ 0]{Rb12}} I2 -),

(I1 - {[5 8] {R1} [12 15] {R2}} I1 +),
and {R1 Rb11 }, {R2 Rb9 } are I-L-Sets, since Rb11 is associated to ‘I2 is after I3 ’ and Rb9 is associated
to ‘I2 is before I3 ’. Likewise, "I1 starts at the same time as I 2 if t1 occurs after t2" can be represented
as (see Table 1):
I1 {s, ¬s} I2 ⇒ (I1 - {[0 0] {Rs1} (0 ∞){Rs3} (-∞ 0){Rs4}} I2 -) , (I1 + {(0 ∞){Rs2} (-∞ 0]{Rs5}} I2 +) ,
(t1 {(-∞ -1] {R1}, [0 0] {R2}, [1 ∞){R3}} t2 ),
and {R3 Rs3 }, {R3 Rs4 }, and {R3 Rs5 } are I-L-Sets, since R3 is associated to 't1 occurs after t2 ' and Rs3 ,
Rs4 and Rs5 are associated to 'I1 does not start at the same time as I2 '.
6.1 Disjunctions of Point and Interval-Based Constraints
Disjunctions of constraints between different pairs of points and intervals can be represented in the
proposed model by means of labeled constraints between points and a set of I-L-Sets. This subsumes
the related expressiveness in the subset of disjunctive linear constraints proposed by Stergiou and
Koubarakis (1998), where only disjunctions of constraints between different pairs of points are
managed.
To represent a disjunctive set of disjunctive constraints between points, we have5 :
(ni lc ij nj ) ∨ (nk lc kl nl ) can be represented as: (ni {lc ij ∨¬lc ij } nj ) ∧ (nk {lc kl ∨¬lc kl } nl ),
and some logical relation among lc ij , ¬lc ij , lc kl and ¬lc kl. Thus, the disjunctive set of constraints:
{(ni lc ij nj ) ∨ (nk lc kl nl )} ≡
{(ni {(lecij.1 ){Rij.1}, (lecij.2){Rij.2}, ...., (lecij.p ){Rij.p}} nj ) ∨
(nk {(leckl.1 ){Rkl.1}, (leckl.2 ){Rkl.2}, ...., (leckj.q ){Rkl.q}} nl )}
5

For reasons of simplicity, only two constraints are shown. However, more than two disjunctive constraints can be managed
in a similar way.

69

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

can be represented as:
i) A conjunctive set of constraints between (ni , nj ) and between (nk , nl ), where, ¬(lecx) can be
represented by means of the complementary domain of (lecx):
(ni {(lecij.1 ){Rij.1}, (lecij.2 ){Rij.2}, ...., (lecij.p ){Rij.p}, ¬{(lecij.1 ){Rij.1}, (lecij.2 ){Rij.2}, ...., (lecij.p ){Rij.p}}} nj ) ∧
(nk {(leckl.1 ){Rkl.1}, (leckl.2 ){Rkl.2}, ..., (leckj.q ){Rkl.q}, ¬{(leckl.1 ){Rkl.1}, (leckl.2 ){Rkl.2}, ..., (leckj.q ){Rkl.q}}} nl )
≡{(ni {(lecij.1 ){Rij.1}, (lecij.2 ){Rij.2}, ..., (lecij.p ){Rij.p}, (¬lecij.1 ){R'ij.1}, (¬lecij.2 ){R'ij.2}, ..., (¬lecij.p){R'ij.p}} nj ) ∧
(nk {(leckl.1){Rkl.1}, (leckl.2 ){Rkl.2}, .., (leckj.q ){Rkl.q}, (¬leckl.1 ){R'kl.1}, (¬leckl.2){R'kl.2}, ..., ( ¬leckl.q ){R'kl.q} } nl )}
ii) A set of I-L-Sets to represent the mutually exclusive disjunction of lc ij and lc kl (they cannot
simultaneously hold):
ii.a) One of the constraints lc ij or lc kl should hold: The Cartesian product of label sets from
complementary domains of lc ij and lc kl , {R'ij.1 , R'ij.2 , ...., R'ij.p }Χ{R'kl.1 , R'kl.2 , ...., R'kl.q },
are I-L-Sets.
ii.b) Only one of the constraints lc ij or lc kl should hold: The Cartesian product of label sets
from lc ij and lc kl , {Rij.1 , Rij.2 , ...., Rij.p }Χ{Rkl.1 , Rkl.2 , ...., Rkl.q } are I-L-Sets.
Thus, disjunctive and conjunctive sets of disjunctive constraints between points can be represented
and managed by means of a conjunctive set of disjunctive constraints and a set of I-L-Sets. For
example:
(ti {[5 5] {R1} [10 10] {R2}} tj ) ∨ (tk {[0 0] {R3} [20 20] {R4}} tl ) ≡
(ti {[5 5] {R1} [10 10] {R2} (-∞ 5){R5} (5 10) {R6} (10 ∞){R7}} tj ) ∧
(tk {[0 0] {R3} [20 20] {R4} (-∞ 0){R8} (0 20) {R9} (20 ∞){R10}} tl ),
and
(ii.a) since (ti {[5 5] {R1}, [10 10] {R2}} tj ] or [tk {[0 0] {R3}, [20 20] {R4}} tl ] should hold:
{R5 R6 R7 }Χ{R8 R9 R10 } are I-L-Sets,
(ii.b) since only one constraint (t i {[5 5]{R1} [10 10] {R2}} tj ) or (tk {[0 0] {R3} [20 20]{R4}} tl ) should
hold:
{R1 R2 }Χ{R3 R4 } = {R1 R3 }, {R1 R4 }, {R2 R3 }, {R2 R4 } are I-L-Sets.
Ii ¬ ecij Ij

Ii ecij Ij

Ii ecij Ij

I1 before I2

I1 {(0 ∞){Rb1}} I2

I3 before I4

I3 + {(0 ∞){Rb3}} I4 -

+

-

Ii (ecij ∨ ¬ ecij) Ij
-

I1 {(0 ∞){Rb1} (-∞ 0]{Rb2}} I2 -

I3 + {(-∞ 0]{Rb4}} I4 -

I3 + {(0 ∞){Rb3} (-∞ 0]{Rb4}} I4 -

+

I1 {(-∞ 0]{Rb2}} I2

+

Table 5: Point-based constraints for (I1 before I2 ) and (I3 before I 4 )
Similarly, disjunctions of interval-based constraints between different pairs of intervals can also
be represented. For instance, from Table 1 and Table 5, {(I1 before I2 ) ∨ (I3 before I4)} can be
represented as:
(I1 + {(0 ∞){Rb1} (-∞ 0]{Rb2}} I2 -), (I3 + {(0 ∞){Rb3} (-∞ 0]{Rb4}} I4 -),

70

BARBER

and
a)

one of the constraints (I1 before I2) or (I3 before I4 ) should hold. Thus, the Cartesian product
of label sets associated to the disjunctive constraints in (Ii ¬ecij Ij ) is a set of I-L-Sets: {Rb2 ,
Rb4 } is an I-L-Set,

b)

only one of the constraints (I1 before I2 ) or (I3 before I4 ) should hold. Thus, the label set
associated to the mutual fulfillment of constraints in (Ii ecij Ij ) is an I-L-Set: {Rb1 , Rb3 } is an
I-L-Set.

Thus:
{(I1 before I2 ) ∨ (I3 before I4 )} ≡
(I1 + {(0 ∞){Rb1} (-∞ 0]{Rb2}} I2 -), (I3 + {(0 ∞){Rb3} (-∞ 0]{Rb4}} I4 -),
and {Rb2 , Rb4 }, {Rb1 , Rb3 } are I-L-Sets.
Ii ecij Ij

Ii ecij Ij

Ii ¬ ecij Ij

Ii (ecij ∨ ¬ ecij) Ij

(I1 during I2 )

I1 - {(-∞ 0){Rd1}} I2 -

(I1 - {[0 ∞){Rd3}} I2 -)

I1 - {(-∞ 0){Rd1} [0 ∞){Rd3}} I2 -

I1 + {(0 ∞){Rd2}} I2 +
(I3 starts I4 )

I3 - {[0 0]{Rs1}} I4 I3 + {(0 ∞){Rs2}} I4 +

∨

(I1 + {(-∞ 0]{Rd4}} I2 +)

I1 + {(0 ∞){Rd2} (-∞ 0]{Rd4}} I2 +

(I3 - {(0 ∞){Rs3} (-∞ 0){Rs4}} I4 -) I3 - {[0 0]{Rs1} (0 ∞){Rs3} (-∞ 0){Rs4}} I4 ∨

(I3 + {(-∞ 0]{Rs5}} I4 +)

I3 + {(0 ∞){Rs2} (-∞ 0]{Rs5}} I4 +

Table 6: Point-based constraints for (I1 during I2 ) and (I3 starts I4 )
In a similar way (Table 6), (I1 during I2 ) ∨ (I3 starts I 4 ) ≡
(I1 - {(-∞ 0){Rd1} [0 ∞){Rd3}} I2 -),

(I1 + {(0 ∞){Rd2} (-∞ 0]{Rd4}} I2 +),

(I3 - {[0 0] {Rs1} (0 ∞){Rs3} (-∞ 0){Rs4}} I4 -), (I3 + {(0 ∞){Rs2} (-∞ 0]{Rs5}} I4 +),
and {Rd1 Rd2 Rs1 Rs2 } and the Cartesian product {Rd3 Rd4 } X {Rs3 Rs4 Rs5 } are I-L-Sets.
Therefore, logical relations on elemental constraints can be represented by a set of I-L-Sets. Thus,
a labeled TCN (and the set of I-L-Sets) can represent a special type of and/or TCN. These types of
non-binary constraints enrich the expressiveness of language and allow for the modeling of more
complex problems (Meiri, 1996). Stergiou and Koubarakis (1996) and Jonsson and Bäckström (1998)
show that Disjunctions of Linear Constraints (DLR) are also able to represent these non-binary
constraints. However, Pujari and Sattar (1999) remark that general methods from linear programming
should then be applied for DLR management, such that specific temporal concepts (like the ones
detailed in Section 2) are not considered in these general methods. In the proposed model,
management of these non-binary constraints are performed by the proposed reasoning methods
without increasing their computational complexity. The added functionality is of interest in several
temporal reasoning problems, including planning, scheduling and temporal constraint databases
(Barber et al., 1994; Gerevini & Schubert, 1995; Brusoni et al., 1997; Stergiou & Koubarakis, 1998;
etc.) where no general solutions are provided in the specific temporal reasoning area.
In addition, the proposed reasoning algorithms obtain a globally labeled-consistent TCN
(Theorem 11). This feature allows us to manage hypothetical queries, which is an important
requirement in query processes on temporal constraint databases (Brusoni et al., 1997). Thus, queries
71

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

such as Does c'ij hold, if c'kl? can be answered without any TCN propagation. The label set associated
to each derived elemental constraint represents the set of input elemental constraints that should hold
for the fulfillment of this elemental constraint. Therefore,
(xk c'kl xl)→(xi c'ij xj )
holds, if ∀elc kl.y ∈lc kl / eckl.y⊆c'kl then ∃elc ij.x∈lc ij / ec ij.x⊆c'ij and labels(elc ij.x)⊆labels(elc kl.y ) hold.
For example, from the labeled minimal TCN in Figure 7, we have:
(T1 {[40 40]} T3 ) → (T2 { [0 0] } T4 ),

(T3 { [20 20] } T2 ) → (T3 { [20 20] } T4 ).

However, (T3 {[10 20]} T2 ) does not imply (T1 {[70 70]} T4 ). Similarly, questions such as ‘Can
c'ij hold, if c'kl?’ can also be easily answered by applying Theorem 9 and Theorem 10.

7. Alternative Temporal Contexts
When we reason on temporal facts, we can simultaneously work on different alternative temporal
contexts, situations, trends, plans, intentions or possible worlds (Dousson et al., 1993; Garcia &
Laborie, 1996). This is usual in a branching (backward or forward) model of time. Here, we can have
alternative past contexts (i.e.: different lines about how facts may have occurred) or alternative future
contexts (i.e.: different lines about how facts may occur). Thus, temporal context management is also
required in hypothetical or causal reasoning. Also, having different contexts permits a partition of
the whole TCN in a set of independent chains in order to decrease the complexity problem size
(Gerevini & Schubert, 1995). In this section, we do not deal with hypothetical reasoning issues. Our
goal is temporal management of context-dependent constraints. Thus, in general, a hierarchy of
alternative temporal contexts can be established, such that constraints can be associated to different
temporal contexts. For instance, Figure 13 represents a hierarchy of alternative contexts, where W0
represents the root context and there are different disjunctive constraints between (n1 , n2 ) in each
context. Temporal reasoning algorithms detailed in this paper are able to manage these contextdependent constraints:
§ Input disjunctive constraints are asserted in different temporal contexts. To do this, the labels
associated to input elemental constraints can also be used to represent the context in which the
disjunctive is asserted. For instance (Figure 13), if the constraint:
(n1 {[0 50] {R1}, [200 210] {R2}} n2 )
is asserted in context W1 , we have the following input context-dependent labeled constraint:
(n1 {[0 25] {R1, W1}, [260 280] {R2, W1}} n2 ).
Here, each context-dependent label set associated to each elemental constraint represents both
the alternative temporal disjunct (i.e.: R1 or R2 ) and the context in which the elemental
constraint is asserted (W1 ).
§ Label sets associated to context-dependent derived elemental constraints will represent the
temporal contexts in which derived elemental constraints hold.
Definition 8. A context-dependent disjunctive constraint is a disjunctive constraint where each
elemental constraint (i.e.: disjunct) is associated to an alternative temporal context. The universal
labeled constraint is {(-∞ ∞){W0 R0}}, where W0 is the root context. ◊
72

BARBER

The proposed reasoning processes can manage context-dependent disjunctive constraints in a way
similar to previously defined labeled disjunctive constraints (Section 3). For instance, according to
the constraints and contexts in Figure 13, the following input labeled constraints between nodes n1
n2 should be updated:
(n1 {[0 100] {R1 W0}, [200 300] {R2 W0}} n2 ),

(n1 {[0 50] {R3 W1}, [200 210] {R4 W1}} n2 ),

(n1 {[60 100] {R5 W 2} , [290 300] {R6 W2}} n2 ),

(n1 {[0 25] {R7 W3}, [260 280] {R8 W3}} n2 ),

(n1 { [0 25] {R0 W11}} n2 ),

(n1 { [30 50] {R9 W12}, [200 205] {R10 W12}} n2 ),

(n1 {[0 20] {R0 W31}, [210 215] {R0 W32}} n2 ),

(n1 {[260 280] {R0 W33}} n2 ).

More restricted constraints
Context W11

n1 {[0 25]} n 2

Context W1

n 1 {[0 50], [200 210]} n 2
Context W12

n 1 {[0 100], [200 300]} n 2

Context W2

n 1{[30 50], [200 205]} n 2

n1 {[60 100], [290 300]} n 2

Root-Context W0

Context W31

n 1{[0 20]} n 2
Context W3

n 1{[0 25], [260 280]} n 2

Context W32

n1{[210 215]} n

2

Context W33

n1{[260 280]} n

2

Assertion in Context k

Downward Propagation:
Propagation to contextk
and its successor contexts

Upward Consistency:
Consistency in contextk
and its predecessor contexts

Figure 13: A hierarchy of alternative contexts
The updating process of each new constraint cij in a given context Wp should assure the
consistency of c ij in the context Wp , as well as in its predecessor contexts (Figure 13). The consistency
of cij with the successor contexts of Wp will be detailed in Section 7.2, since several options can be
identified. However, it is not necessary to assure consistency among constraints belonging to contexts
of different hierarchies. Successor contexts of a given context represent different alternatives, which
are mutually exclusive. Thus, constraints belonging to contexts of different hierarchies can be
mutually inconsistent. However, this does not imply that constraints in these contexts should
necessarily be mutually disjoint. For instance (Figure 13), the constraints (n1 {[0 50] {R3 W1}, [200
210]{R4 W1}} n2 ) in context W1 and (n1 {[0 25] {R7 W3}, [260 280] {R8 W3}} n2 ) in context W3 are not
mutually disjoint. However, W1 , W2 and W3 are assumed as three mutually exclusive alternatives of
W0 .
73

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

The closure process of each new constraint cij in context Wp should downward propagate the new
constraint cij to all its successor contexts (Figure 13). Moreover, no propagation should be performed
to the predecessor contexts of contextk , nor among contexts of different hierarchies. Elemental
constraints belonging to contexts of different hierarchies cannot be simultaneously considered, that
is, combined or intersected.
7.1 Context-Dependent Updating and Closure Processes
The update and closure processes defined in Section 4 should be adapted in order to manage contextdependent disjunctive constraints. The Context-Update process (Figure 14) asserts the constraint
c’ij ≡{ec’1 , ec’2, ..., ec’n } in the context contextk. In a way similar to the updated process described in
Section 4, Context-Update should be performed each time a new context-dependent constraint is
asserted.
Context-Update (ni c’ij nj contextk)
lc'ij ← Put-label-context (c’ij , contextk )
;Labelling and mutual inconsistency.
If Consistency-Test (get-upward (ni , nj , contextk ), lc'ij )
;Upwards Consistency test
Then (*Inconsistent Constraint*)
Return (false)
Else (*Consistent Constraint*)
;lc'ij is asserted in the contextk and in all its
lc ij ← (lc ij - get (ni , nj , contextk )) ∪lc (lc ij ⊕ lc lc'ij ),
;successor contexts.
lc ji ← Inverselc (lc ij ),
Context-Closure (ni lc ij nj contextk )
;Downwards Closure algorithm in contextk.
.
Return (true)
End-If
End-Context-Update
Figure 14: Context-Update process for context-dependent labeled constraints
Where:
• Put-label-context (c’ij , contextk ) associates an exclusive label set to each elemental constraint
ec’ij.p ∈c’ij . This label set has two labels {Rij.p contextk }. In this label set, the first label is the
label associated to each temporal disjunct. In a way similar to Put-labels function, these labels
are mutually exclusive (Definition 3). The second label represents the context in which c’ij is
updated. Moreover, each pair of labels associated to successor contexts of the parent context
of contextk is added to the I-L-Sets, since all the successor contexts of a given context are
mutually exclusive:
∀contextp / contextp ∈Succesor-Contexts(Parent-Context(Contextk )),
I-L-Sets ← I-L-Sets ∪ ({contextk }∪{contextp }).
Where Parent-Context(contextk ) and Successor-Contexts(contextk ) return the parent-context
and the set of successor-contexts of contextk , respectively. Thus, in Figure 13, {{W1 , W2 },
{W1 , W3 }, {W2 , W3 }, {W11 , W12 }, {W31 , W32 }, {W31 , W33 }, {W32 , W33 }} are I-L-Sets.
74

BARBER

• get (ni , nj , contextk ) returns the set of labeled elemental constraints between ni and nj in the
contextk (and in all its successor contexts). That is:
get (ni , nj , contextk )::= {(ecij.p {labelij.p })∈lc ij / contextk ∈{labelij.p }}.
Note that get(ni , nj, contextk ) is a subset of lc ij . Thus, (lc ij - get (ni , nj , context k )) means the setdifference between lc ij and get (ni , nj , contextk ). That is, the set of elemental constraints in the
context-dependent constraint lc ij , which are not in contextk , nor in any of its successor contexts.
§

get-upward (ni, nj , contextk ), similarly to the previous get function, it returns the existing
constraints between ni and nj in the contextk (and in all its successor contexts). However, if
there is no constraint between ni and nj in the contextk , then the function returns the
constraints between ni and nj that exist in the predecesor context of contextk:
get-upward (ni, nj , contextk ) ::=
If get (ni , nj , contextk ) ≠ ∅ Then return (get (ni , nj , contextk ))
Else
Contextk ← Parent-Context (Contextk )
Until get (ni , nj , contextk ) ≠ ∅ ∨ Contextk =W0 do
If get (ni , nj , contextk ) ≠ ∅ Then return (get (ni , nj , contextk ))
Else return({(-∞ +∞)}{W0 R0}})
End-get-upward

The context-dependent closure (Figure 15) process is similar to the closure process described in
Section 4 and it is also performed at each updating process. The closure process of each updated
constraint in contextk is downwards performed in contextk and in all its successor contexts.

Context-Closure (ni lc ij nj contextk)
(* First loop: Closure n i → n j → n k *)
∀nk ∈TCN / lc jk ≠ {U{R0 W0}}:
lc'ik ← lc ik ⊕lc (lc ij ⊗lc lc jk ),
lc ik ← (lc ik - get (ni , nk , contextk )) ∪lc lc’ij ,
lc ki ← Inverse(lc ik)
(* Second loop: Closure n j → ni → nl *)
∀nl ∈TCN / lc il ≠ {U{R0 W0}}:
lc'jl ← lc jl ⊕lc (Inverse(lc ij ) ⊗lc lc il ),
lc jl ← (lc jl - get (nj , nl , contextk )) ∪lc lc'jl ,
lc lj ← Inverse(lc jl )
(* Third loop: Closure nl → ni → nj → nk *)
∀nl , nk ∈TCN / lc lj ≠ {U{R0 W0}}, lc jk ≠ {U{R0 W0}}:
lc'lk ← lc lk ⊕lc (lc li ⊗lc lc ij ⊗lc lc jk )
lc lk ← (lc lk - get (nl , nk , contextk )) ∪lc lc'lk ,
lc kl ← Inverse(lc lk)
End-Context-Closure
Figure 15: Context-Closure process for context-dependent labeled constraints
75

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

The resulting label set associated to each context-dependent derived elemental constraint represents
the contexts where the elemental constraint holds, as well as the hierarchy of predecessor contexts
of the elemental constraint. For instance, Figure 16 shows the contextual labeling for the example in
Figure 13. Moreover, after successively performing the updating and closure processes for all
constraints in this example, we have the following constraint between nodes n1 and n2 :
(n1 lc 12 n2 ): (n1 {[0 100] {R1 W0}, [200 300] {R2 W0}, [0 50] {R3 R1 W1 W0}, [200 210] {R4 R2 W1 W0},
(e3)
[60 100] {R5 R1 W2 W0}, [290 300] {R6 R2 W2 W0}, [0 25] {R7 R1 W3 W0}, [260 280] {R8 R2 W3 W0},
[0 25] {R0 R3 R1 W11 W1 W0}, [30 50] {R9 R3 R1 W12 W1 W0}, [200 205] {R10 R2 R4 W12 W1 W0},
[0 20] {R0 R7 R1 W31 W3 W0}, [210 215]{R0 R2 R8 W32 W3 W0}, [260 280]{R0 R2 R8 W33 W3 W0}} n2 )

{W0}

{W0 W1}

{W0 W1 W11}

{W0 W3}

{W0 W2}

{W0 W1 W12}

{W0 W 3 W 31}

{W0 W3 W32}

{W0 W 3 W33}

Figure 16: Labels in contexts
No closure process is performed among constraints belonging to contexts of different hierarchies.
According to Put-label-context function, each pair of labels related to the successor contexts of each
context is an I-L-Set. Thus, these I-L-Sets prevent deriving elemental constraints from contexts of
different hierarchies. That is, each derived elemental constraint obtained (combining or intersecting)
from two elemental constraints in contexts of different hierarchy will have an inconsistent associated
label set. Therefore, these derived elemental constraints will be rejected in the operation ∪lc. For
instance, in the example of Figure 13, {{W1 , W2 }, {W1 , W3 }, {W2 , W3 }, {W11 , W12 }, {W31 , W32 },
{W31 , W33 }, {W32 , W33 }} are I-L-Sets. Thus, if a constraint is asserted in context W1 :
i) No propagation is performed using constraints in contexts W11 and W12 simultaneously,
since {W11 , W12 } is an I-L-Set.
ii) No propagation is performed in context W2 , nor in W3 , nor in their successors, since {W1 ,
W2 } and {W1 W3 } are I-L-Sets.
Let's see an example of the Context-Update and Context-Closure processes. Let’s assume that the
context-dependent constraints in Figure 13 are already updated and closured, such that the previous
constraint lc 12 (expression e3) exists between n1 and n2 . Now, we update (n1 {[20 40]} n2 ) in context
W1 . The call to Consistency-Test function in the Context-Update function is:
Consistency-Test (get-upward (n1 , n2 , W1 ), {[20 40] {R0 W1}}).
Given the previous constraint lc 12 between n1 and n2 (expression e3), the function performs:
{[0 50] {R3 R1 W1 W0}, [200 210] {R4 R2 W1 W0}, [0 25] {R0 R3 R1 W11 W1 W0},
76

BARBER

[30 50] {R9 R3 R1 W12 W1 W0}, [200 205] {R10 R2 R4 W12 W1 W0}} ⊕ lc {[20 40]{R0 W1}}=
{[20 40] {R3 R1 R0 W1 W0}, [20 25] {R0 R3 R1 W11 W1 W0}, [30 40] {R9 R3 R1 R0 W12 W1 W0}} ≠ ∅
Thus, the new constraint (n1 {[20 40]} n2 ) is consistent in context W1 . Therefore, the constraint
between n1 n2 results:
lc12 ← (lc 12 - get (n 1 , n 2 , W 1 )) ∪lc (lc 12 ⊕lc {[20 40]{R0 W1}}) =
{[0 100]{R1 W0}, [200 300] {R2 W0}, [60 100]{R5 R1 W2 W0}, [290 300]{R6 R2 W2 W0},
[0 25]{R7 R1 W3 W0}, [260 280]{R8 R2 W3 W0}, [0 20]{R0 R7 R1 W31 W3 W0},
[210 215]{R0 R2 R8 W32 W3 W0}, [260 280]{R0 R2 R8 W33 W3 W0}} ∪lc
{[20 40]{R1 R0 W1 W0}, [20 40]{R3 R1 R0 W1 W0}, [20 25]{R0 R3 R1 W11 W1 W0}, [30 40]{R9 R3 R1 R0 W12 W1 W0}}=
{[0 100]{R1 W0}, [200 300] {R2 W0}, [60 100]{R5 R1 W2 W0}, [290 300]{R6 R2 W2 W0}, [0 25]{R7 R1 W3 W0},

(e4)

[260 280]{R8 R2 W3 W0}, [0 20]{R0 R7 R1 W31 W3 W0}, [210 215]{R0 R2 R8 W32 W3 W0}, [260 280]{R0 R2 R8 W33 W3 W0},
[20 40]{R1 R0 W1 W0}, [20 25]{R0 R3 R1 W11 W1 W0}, [30 40]{R9 R3 R1 R0 W12 W1 W0}}.

Note that the new updated constraint is asserted in context W1 and propagated to all its successor
contexts (W11 and W12 ). However, the new constraint in context W1 does not affect the existing
constraints in predecessor contexts of W1 (W0) nor the constraints belonging to contexts of different
hierarchies (W2 , W3 and their successors).
In this update process, no closure process is performed, since no node is related with n1 or n2 .
Now, let’s update (n3 {[10 20]} n1 ) in context W1 . We have:
Consistency-Test (get-upward (n3 , n1 , W1 ), {[10 20] {R0 W1}}),
that performs:
{(-∞ +∞)}{W0 R0} ⊕ lc {[20 40] {R0 W1}} = {[20 40] {R0 W0 W1}} ≠ ∅,
since no previous constraint exists between (n3 n1 ) in context W1 . The constraint (n3 {[10 20]} n1 ) is
consistent, and asserted in the TCN:
lc 31 ← {(-∞ +∞)}{W0 R0}, [20 40] {R0 W0 W1}}.
(e5)
Afterwards, this constraint is closured. The call to Context-Closure process is:
Context-Closure (n3 , {(-∞ +∞)}{W0 R0}, [20 40] {R0 W0 W1}}, n1 , W1 ).
In this closure process, only the first loop is performed since no node is related to n3 . Moreover,
only the previous constraint lc 12 (expression e4) exists in the current TCN between n1 and n2 . Thus,
the first loop performs:
lc'32 ← lc 32 ⊕lc ({(-∞ +∞)}{W0 R0}, [20 40] {R0 W0 W1}} ⊗lc lc 12 ) =
{(-∞ ∞){W0 R0}} ⊕ lc ({(-∞ +∞)}{W0 R0}, [20 40] {R0 W0 W1}} ⊗lc lc 12 ) =
{(-∞ +∞)}{W0 R0}, [220 340] {R2 R0 W0 W1}, [40 80] {R1 R0 W1 W0},
[40 65] {R0 R3 R1 W11 W1 W0}, [50 80] {R9 R3 R1 R0 W12 W1 W0}},
such that,
lc 32 ← (lc 32 - get (n3 , n2 , W1 )) ∪lc lc'32 = ({(-∞ ∞){W0 R0}} - {}) ∪lc lc'32 =
{(-∞ ∞){W0 R0}, [220 340] {R2 R0 W0 W1}, [40 80] {R1 R0 W1 W0},
[40 65] {R0 R3 R1 W11 W1 W0}, [50 80] {R9 R3 R1 R0 W12 W1 W0}}.
77

(e6)

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

Thus, the asserted constraint between (n3 , n2 ) in context W1 is closured in the context W1 and in
all its successor contexts (W11 and W12 ). Likewise, the closure process does not perform any
propagation simultaneously using constraints of the contexts W11 and W 22 , nor any of the context W2 ,
W3 , nor any of their successors.
7.2 Complete Versus Incomplete Partition of Contexts
In each updating process, the consistency of each new constraint lc’ij in a given context is assured in
this context and in all its parent contexts. Let’s deal with consistency issues between a context and
its successor contexts. Here, we have that constraints in a given context Wi can be either completely
covered or only partially covered by the existing constraints in the successor contexts of Wi . That is,
the successor contexts of Wi can be either a complete partition or only a partial partition of Wi .
For instance, let's assert the constraint (n1 {[210 210] {R0 W1}} n2 ) in the context W1 of the example
in Figure 13. In the Consistency-test function, we have (where the constraint lc 12 is the previous
expression e2):
get-upward (n 1 , n 2 , W 1 ) ⊕lc {[210 210]{R0 W1}} =
{[0 50]{R3 R1 W1 W0}, [200 210]{R4 R2 W1 W0}, [0 25]{R0 R3 R1 W11 W1 W0}, [30 50]{R9 R3 R1 W12 W1 W0},
[200 205]{R10 R2 R4 W12 W1 W0}} ⊕lc {[210 210]{R0 W1}} = {[210 210]{R0 W1 R4 R2 W0}}.

That is, the asserted constraint is consistent with the existing constraints in context W1 . However,
no resulting elemental constraint is associated to context W11 nor W12 . This means that the asserted
constraint (n1 {[210 210] {R0 W1}} n2 ) is consistent in W1 , but is inconsistent in W11 and in W12 . Here,
two alternatives appear:
i) To assume that existing successor contexts are a complete partition of their parent context.
Therefore, a new constraint cij in a context Wi should be rejected, if cij is inconsistent in all
successor contexts of Wi. For instance, we can assume that W11 and W12 in Figure 13 are a
complete partition of W1 . Thus, (n1 {[210 210] {R0 W1}} n2 ) should be rejected.
ii) To assume that successor contexts are not a complete partition of their parent context. Therefore,
successor contexts become inconsistent and they should be removed. In the example, we can
assume that contexts W11 and W12 are not a complete partition of the context W1 , such that
another possible new successor context of W 1 would be able to match in the future the asserted
constraint (n1 {[210 210] {R0 W1}} n2 ). In this case, the constraint (n1 {[210 210] {R0 W1}} n2 ) is
assumed to be correct, such that it can be asserted in the TCN. Therefore, the contexts W11 and
W12 become inconsistent. {W11 } and {W12 } should be added to the set of I-L-Sets, such that
these contexts (and all their successor contexts and all their constraints) become inconsistent and
removed from the TCN. That is, all elemental constraints with an associated label set containing
{W11 } or {W12 } should be removed.
In both cases, each context will always be consistent with all its successor contexts. The option
to be adopted can depend on the problem type to solve (Garrido et al., 1999). Any of the these options
can be easily introduced in the described reasoning processes, since the function Consistency Test
can determine which successor contexts (Ws ) become inconsistent at each new constraint (lc’ij ) in
a context (Wk ):
78

BARBER

Ws ∈Successor-Contexts(Wk ) / ∃elc ij.p ∈get-upward (ni , nj , Wk ), Ws∈{labelij.p } ∧
¬∃elc ij.r∈(get-upward (ni , nj , Wk ) ⊕lc lc’ij ), Ws∈{labelij.r}.
On the other hand, when: (i) the successor contexts (Wk1 , Wk2 , ..., Wkp ) of a context Wk are a
complete partition of it, and (ii) all constraints in (Wk1 , Wk2 , ..., Wkp ) have been asserted, then
constraints in Wk can be restricted according to the final existing constraints in (W k1 , Wk2 , ..., Wkp ).
To do this, the context Wk should be constrained by the temporal union of the constraints in all its
successor contexts.
7.3 A Minimal and Consistent Context-Dependent TCN
Definition 9. A context-dependent TCN is minimal (and consistent) if the constraints in each context
are consistent (with respect to constraints in this context, in all its predecessor contexts, and all its
successor contexts) and minimal (with respect to constraints in this context and in all its predecessor
contexts). ◊
Theorem 12. At each updating process, the context-dependent reasoning processes obtain a minimal
(and consistent) context-dependent TCN if the previous context-dependent TCN is minimal.
Proof: If the previous context-dependent TCN is minimal, the Consistency-Test function guarantees
the consistency of each new context-dependent input constraint:
i)

in its context and in all its parent contexts (get-upward function and Theorem 5),

ii)

in all its successor contexts (depending of the two identified cases in Section 7.2).

The closure process of a new constraint in a given context (Wk ) propagates its effects to this
context and to all its successor contexts. Therefore (Theorem 7), the process obtains the new minimal
constraints in this context (Wk ) and in all its successor contexts. ◊
Moreover, the obtained context-dependent TCN is globally labeled-consistent. Thus, we can
deduce whether a set of elemental constraints (between different pairs of time points) is consistent
(Theorem 10). That is, this set of elemental constraints holds in some context. For instance, given the
previous constraints lc 12 , lc 31 and lc 32 (previous expressions e4, e5 and e6), we can deduce that:
(n1 {[40 40]} n2 ) ∧ (n3 {[40 40]} n1 ) ∧ (n3 {[40 40]} n2 )
is full consistent since:
∃elc 12.x∈lc 12 , ∃elc 31.y ∈lc 31 , ∃elc 32.z∈lc 32 / ({label12.x} ∪ {label12.x} ∪ {label12.x}) is not an I-L-Set.
Specifically, these instantiations hold in {R1 R0 W1 W0 } and {R1 R0 W0 }. Thus, this set of
elemental constraints holds in context W1 (and, obviously, in all its predecessor contexts).
Likewise, from a minimal context-dependent TCN, the user can retrieve the constraints that hold
in each context or the constraints that simultaneously hold in a set of given contexts. To do this, the
Context-Constraints function retrieves the constraints that hold between a pair of nodes (ni , nj ) in a
given context (contextk ). That is, the result of Get-upwards(ni , nj , contextk ) except those elemental
constraints belonging to successor contexts of contextk :

79

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

Context-Constraints (ni, nj, contextk )::= Get-upwards (ni , nj , contextk ) –
{lecij.p ∈lc ij / ∃contextq∈Succesor-Contexts(contextk ), {contextq }∩{labelij.p }≠∅}.
For instance, given the context-dependent constraint lc 12 in Figure 13 (expression e3), the
following constraint would hold between (n1 , n2 ) in both contexts W1 and W3 :
Context-Constraints(n 1 , n 2 , W1 ) ⊕lc Context-Constraint(n 1 , n 2 , W 3 ) =
{[0 50]{R3 R1 W1 W0}, [200 210]{R4 R2 W1 W0}} ⊕lc {[0 25]{R7 R1 W3 W0}, [260 280]{R8 R2 W3 W0}}=
{[0 25]{R7 R3 R1 W3 W1 W0}}6 .

In addition, we can obtain the constraints, which simultaneously hold in a context and in any of
its successor ones. For instance, in context W1 and in any of its successor contexts (W11 , W12), the
following constraint holds:
Context-Constrains(n1 , n2 , W1) ⊕lc [Context-Constraints(n1, n 2 , W11 ) ∪lc Context-Constraints(n 1 , n2 , W12 )]=
{[0 50]{R3 R1 W1 W0}, [200 210]{R4 R2 W1 W0}} ⊕lc
{[0 25]{R0 R3 R1 W11 W1 W0}}∪lc {[30 50] {R9 R3 R1 W12 W1 W0}, [200 205]{R10 R2 R4 W12 W1 W0}}=
{[200 205]{W12 R10 R4 R2 W1 W0}, [0 25]{W11 R0 R3 R1 W1 W0}, [30 50]{W12 R9 R3 R1 W1 W0}}.

On the other hand, each alternative context (Wi ) can be associated to an alternative hypothesis
(Hi ). Each hypothesis Hi gives rise to a set of constraints, which will be asserted in the associated
context Wi . Thus, the proposed reasoning processes assure minimal constraints in the hierarchy of
hypotheses. Moreover, if a hypothesis (Hi ) becomes unavailable, then the label set {Wi } should be
added to the set of I-L-Sets. Thus, all constraints in context Wi (and in all its successor contexts) will
be removed. That is, all constraints that depend on the unavailable hypothesis Hi will be removed.
7.4 Computational Complexity of Temporal Context Management
The management of temporal context does not increase the complexity of the reasoning processes
detailed in Section 4. In fact, we can consider that each label associated to a disjunct (Ri ) in labeled
disjunctive constraints is also associated to a context (Wi ). Thus, the computational cost of each
updating process is also bounded by O(n2 l2e), where 'l' is the maximum number of input disjuncts
between any pair of nodes in all contexts.
The temporal labeled algebra proposed in this paper (Section 3) has been applied on the pointbased disjunctive metric constraints (Dechter, Meiri & Pearl, 1991). However, this labeled algebra
can also be applied on other temporal constraints. In this case, the operations ⊕ lc, ⊗ lc, ∪lc and ⊆ lc
should be specified (Section 3) on the basis of the operations ⊕, ⊗, ∪T and ⊆T of the underlying
algebra. In this way, the management of temporal contexts can also be applied to other types of
constraints.
Theorem 13. The computational complexity of the proposed reasoning process applied to contextdependent non-disjunctive metric constraints is polynomial (O(n2 W2)) in the number W of managed
contexts.
6

However, note that this is an impossible situation, since W 1 and W 3 are mutually exclusive contexts. That is, {W 3, W 1}
is an I-L-Set.

80

BARBER

Proof: Disjunctions in constraints are only related to the contexts in which input constraints are
asserted, if non-disjunctive constraints are managed. That is, constraints between each pair of nodes
are in the form:
(ni {(ec ij.0 {W0 R0 }), (ec ij.1 {W1 R0 }), ...... , (ecij.k {Wk R0 })} nj ) ,

0≤k≤W / W=|{Wi }|

Thus, the maximum number of disjuncts in constraints is bounded by the maximum number of
managed contexts W. Moreover, the maximum length of associated label sets is the maximum depth
in the hierarchy of contexts, and the set of I-L-Sets has only 2-length sets (i.e.: pairs of labels
associated to each pair of successor contexts of each context). Therefore, the computational cost of
operations ⊗lc and ⊕ lc is bounded by O(W2 ). ◊
The methods proposed in Section 7.1 for management of temporal contexts can also be applied
to other temporal reasoning algorithms, instead of the reasoning methods detailed in Section 4. This
requires that these other reasoning algorithms be based on the operations of composition and
intersection of temporal constraints. Thus,
i) Each elemental constraint should only be associated to the context (W i ) in which it is asserted7 .
Thus, label sets associated to elemental constraints have only one contextual label {Wi }.
ii) The methods for management of temporal contexts described in Section 7.1 should be
integrated into the new reasoning algorithms. These algorithms should use the operations ⊕
lc, ⊗lc, get and get-upwards. The computational cost of operations ⊕ lc and ⊗lc related to
management of temporal contexts is polynomial (O(W2 )) in the number (W) of managed
contexts. Therefore, the computational cost of the reasoning algorithms is increased by a factor
W2 when temporal contexts are managed.
For instance, when interval-based constraints are managed, the TCA algorithm can be used to
obtain a path-consistent context-dependent IA-TCN, with a O(n3 W2 ) cost. Similarly, when a contextdependent reasoning is applied to PIDN networks (Pujari & Sattar, 1999), the computational cost of
specific reasoning algorithms on PIDN constraints is increased by a factor W2 . When the proposed
temporal algebra in Section 3 is applied to tractable classes of constraints, the specific reasoning
algorithms for management of these classes of constraints can also be applied. The computational
cost of these reasoning algorithms (which should be based on combination and intersection
operations on constraints) is increased by a polynomial factor W2 . For instance, when nondisjunctive metric constraints are managed, the TCA algorithm can be used as the closure algorithm
in Section 7.1. This algorithm will obtain a minimal context-dependent TCN with a computational
cost O(n 3 W2 ).

8. Conclusions
Several problems remain pending in representation and reasoning problems on temporal constraints.
In relation to this, we have dealt with reasoning on complex qualitative and quantitative constraints
between time-points and intervals, which can be organized in a hierarchy of alternative temporal
7

That is, there are not labels (Ri) associated to disjunctions in disjunctive constraints. Thus, Definition 3 is not applied in
the Put-Label-Context function. Therefore, the distributive property for ⊗lc over ⊕lc does not hold for disjunctive
constraints. However, this is not relevant since other reasoning processes will be applied.

81

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

contexts. We have described a new-labeled temporal algebra, whose main elements are labeled
disjunctive metric constraints, label sets associated to elemental constraints, and sets of inconsistent
elemental constraints (I-L-Sets). The temporal model presented is able to integrate qualitative and
metric constraints on time-points and intervals. In fact, symbolic and metric constraint between
intervals can be represented by means of disjunctive metric constraints between time points and a set
of I-L-Sets. The model is also able to manage (non-binary) logical relations among elemental
constraints. The reasoning algorithms on the described model are based on the distributive property
for composition over intersection in labeled constraints, and guarantee consistency and obtain a
minimal TCN of disjunctive metric point-based constraints. In addition, a special type of global
labeled-consistent TCN is also obtained.
Labeled constraints can be organized in a hierarchy of alternative temporal contexts, such that
temporal reasoning processes can be performed on these contexts. Reasoning algorithms guarantee
consistency in each hierarchy of contexts, maintain a minimal context-dependent TCN, and allow us
to determine what constraints hold in each context or in a set of alternative contexts. Thus, we can
reason on a hierarchy of context-dependent constraints on intervals, points and unary durations
(Figure 17).
These described features are useful functionalities for modeling important problems in the
temporal reasoning area. However, they have not been identified in previous models. Therefore, the
temporal model presented here represents a flexible framework for reasoning on complex, contextdependent, metric and qualitative constraints on time-points, intervals and unary durations.
Dur(I 1) ∈ { [ 2 0 20], [50 60]}

I1 {b} I 2
t 1 {[10 20], [100 130]} I -2

Context W 1 1
-

I 1 {b d m} I 2
Dur(I 1 )∈ {[20 30], [50 100]}

I 1 {[100 100], [200 300]} I+ 2
Dur(I 1) ∈ {[20 30], [60 100]}

Context W 1

Context W 1 2
t 1 {[10 20], [100 200]} I - 2
I- 1 {[0 100], [200 300]} I + 2
Root-Context W 0

t 1 {[10 15], [120 200]} I
I 1{ d m } I 2
Context W 2

2

I 1{d} I 2
t 1 {[10 10]} I -2
Context W 2 1
Dur(I 1 ) = 50
I 1 { m } I2
Context W 2 2

Figure 17: Context-dependent constraints on intervals, time points and unary durations
A path-consistent algorithm can be used as the closure process on labeled TCNs, like the typical
TCA algorithm as applied by Allen (1983). This path-consistent algorithm would obtain a minimal
context-dependent TCN of disjunctive metric constraints. We have proposed an incremental
reasoning process. Thus, a minimal (and consistent) context-dependent TCN is assured at each new
assertion. This incremental reasoning allows us to detect whether each new input constraint is
inconsistent with the previously existing ones. This can be useful when problem constraints are not

82

BARBER

initially known but are successively deduced from an incremental independent process (Garrido et
al., 1999).
A prototype of proposed reasoning algorithms has been implemented in Common-Lisp and is
available from the author. These reasoning algorithms are being applied to an integrated architecture
of planning and scheduling processes (Garrido et al., 1999). Here, the scheduling process should
guarantee the consistency of each alternative partial plan (i.e.: temporal constraints and availability
of resources for operations) simultaneously as the planner is generating each partial plan (Srivastava
& Kambhampati, 1999). Thus, the following main features are needed:
§ Management of disjunctive metric constraints. Particularly, in planning and scheduling
problems the number disjuncts in input constraints is generally bounded by l≤2 (i.e.: nonsimultaneous use of resources). However, temporal dependencies between constraints (i.e.:
non-binary constraints) can appear. For instance, operation durations can be dependent on the
order in which they are scheduled.
§ Incremental reasoning. The process should interactively guarantee the consistency of each new
input temporal constraint (about resources, plans, ordering, and objects) as each new step is
deduced in a partial plan.
§ Management of temporal contexts, where each context is associated to an alternative plan
(action or state). Reasoning algorithms simultaneously work over different and alternative
partial plans.
A globally labeled-consistent (and minimal) TCN allows us to determine consistent alternative
choices and to obtain optimal solutions in each plan. Additionally, the proposed model can be a
useful framework to apply on problems where these features also appear (Dousson et al., 1993;
Garcia & Laborie, 1996; Srivastava & Kambhampati, 1999; etc.).
The computational cost of reasoning algorithms is exponential, due to the inherent complexity of
the management of disjunctive constraints. However, the management of temporal contexts does not
increase the complexity of the reasoning processes on disjunctive constraints.
Some improvements to decrease the empirical cost of reasoning algorithms have been proposed
in this paper. The application of algorithms to handle only an explicit TCN (without making the
derived constraints explicit) and empirical evaluations on several test cases are under study.
Moreover, other reasoning algorithms can be applied to the temporal algebra presented, as proposed
in Section 4. On the other hand, it is interesting to identify subclasses of the labeled temporal algebra
where the size of label sets can be bounded, and to identify tractable subclasses of IA on the proposed
model. It could also be interesting to identify the expressive power of I-L-Sets (and labeled
constraints) on the basis of method described by Jeavons, Cohen and Cooper (1999). Here, each I-LSet represents a special derived constraint, which expresses the inconsistency of a set of input
elemental constraints; that is, a special type of disjunctive linear constraint (Jonsson & Bäckström,
1996; Stergiou & Koubarakis, 1996).
The proposed-labeled algebra (labeled constraints and the operations on them) can be applied to
other temporal models (i.e.: to other classes of temporal constraints, operations, and reasoning
algorithms). To do this, the operations of the labeled algebra (⊕lc, ⊗lc, ∪lc and ⊆lc) should be defined
on the basis of the respective operations (⊕, ⊗, ∪Τ and ⊆Τ) of these models, and the reasoning
algorithms should use the operations defined on labeled constraints (⊕ lc, ⊗lc, ∪lc and ⊆lc). This
83

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

requires that these reasoning algorithms be based on the composition and intersection operations.
Specifically, the application of the proposed model to tractable temporal constraints -as those
identified in Section 1 (Jonsson et al., 1999; Drakengren & Jonsson, 1997; Vilain, Kautz and Van
Beek, 1986; etc.)- allows for a tractable reasoning process on a hierarchy of temporal constraint
contexts.

Acknowledgements
This work was partially supported by the Generalitat Valenciana (Research Project #GV-1112/93)
and by the Spanish Government (Research Project #CYCIT-TAP-98-0345). The author would
sincerely like to thank the JAIR reviewers for their helpful comments and suggestions on previous
versions of this paper.

References
Allen, J. (1983). Maintaining knowledge about temporal intervals. Comm of the ACM, 26, 11, 832843.
Badaloni, S., & Berati, M. (1996). Hybrid Temporal Reasoning for Planning and Scheduling. In
Proceedings of the 3º Int. Workshop on Temporal Representation and Reasoning (TIME’96).
Barber, F. (1993). A metric time-point and duration-based temporal model. ACM Sigart Bulletin,
4 (3), 30-40.
Barber, F., Botti, V., Onaindia, E., & Crespo, A. (1994). Temporal reasoning in Reakt: An
environment for real-time knowledge-based systems. AICommunications, 7 (3), 175-202.
Brusoni, V., Console, L., & Terenziani, P. (1997). Later: Managing temporal information efficiently,
IEEE Expert, 12 (4), 56-64.
Cohen, D., Jeavons, P., & Koubarakis, M. (1996). Tractable disjunctive constraints. In Proceedings.
of the 3rd Int. Conf. on Principles and Practice of Constraint Programming (CP’96). Freuder,
E.C. (Ed.). Lecture Notes in Computer Science, 1118, 297-307.
Cooper, M.C. (1990). An optimal k-consistency algorithm. Artificial Intelligence, 41, 89-95.
Dean, T.L., & McDermott, D.V. (1987). Temporal data base management. Artificial Intelligence, 38,
1-55.
Dechter. R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49,
61-95.
Dechter, R. (1992). From local to global consistency. Artificial Intelligence, 55, 87-107.
Dousson, C., Gaborit, P., & Ghallab M. (1993). Situation Recognition: Representation and
Algorithms. In Proceedings of 13th International Joint Conference on Artificial Intelligence
(IJCAI’93).
Drakengren, T., & Jonsson, P. (1997). Eight maximal tractable subclasses of Allen's algebra with
metric time. Journal of A.I. Research, 7, 25-45.
84

BARBER

Freuder, E. C. (1978). Synthesizing constraint expressions. Comm. of the ACM, 21 (11), 958-965.
Freuder, E. C. (1982). A sufficient condition for backtrack-free search. Journal of the ACM, 29 (1),
24-32.
Garcia, F., & Laborie, P. (1996). Hierarchisation of the Seach Space in Temporal Planning. New
Directions in AI Planning, 217-232, IOS Press.
Garrido, A., Marzal, E., Sebastiá, L., & Barber F. (1999). A model for planning and scheduling
integration. In Proceedings of the 8 th. Conference of Spanish Association of A.I.
(CAEPIA’99).
Gerevini, A., & Schubert, L. (1995). Efficient algorithms for qualitative reasoning about time.
Artificial Intelligence, 74, 207-248.
Jeavons, P., Cohen, D., & Cooper M. (1998). Constraints, consistency and closure. Artificial
Intelligence, 101, 251-268.
Jeavons, P., Cohen, D., Gyssens, M. (1999). How to determine the expressive power of constraints.
Constraints: An Int. Journal, 4, 113-131.
Jonsson, P., & Bäckström, C. (1996). A linear-programming approach to temporal reasoning. In
Proceedings of the 13 th. National Conference on Artificial Intelligence (AAAI’96).AAAI
Press.
Jonsson, P., & Bäckström, C. (1998). A unifying approach to temporal constraint reasoning. Artificial
Intelligence, 102, 143-155.
Jonsson, P., Drakengren, T., & Bäckström, C. (1999). Computational complexity of relating time
points and intervals. Artificial Intelligence, 109, 273-295.
Kautz, H., & Ladkin, P. (1991). Integrating metric and qualitative temporal reasoning. In Proceedings
of the 9th. National Conference on Artificial Intelligence (AAAI’91).AAAI Press.
Mackworth, A. K. (1977). Consistency in networks of relations, Artificial Intelligence, 8, 121-118,.
Meiri, I. (1996). Combining qualitative and quantitative constraints in temporal reasoning. Artificial
Intelligence, 87, 343-385.
Montanari, U. (1974). Networks of constraints: fundamental properties and applications to picture
processing. Information Science, 7, 95-132.
Navarrete, I., & Marin, R. (1997). Qualitative temporal reasoning with points and durations. In
Proceedings of the 15 th. International Joint Conference on Artificial Intelligence (IJCAI-97).
Nebel, B., & Burckert, H.J. (1995). Reasoning about temporal relations: a maximal tractable subclass
of Allen's interval algebra. Journal of the ACM, 42 (1), 43-66.
Pujari, A., & Sattar, A. (1999). A new framework for reasoning about Points,. Intervals and
Durations. In Proceedings on the Int. Joint Conference on Artificial Intelligence (IJCAI'99).
Schwalb, E., & Dechter, R. (1997). Processing disjunctions in temporal constraints networks.
Artificial Intelligence, 93, 29-61.
85

REASONING ON INTERVAL AND POINT -BASED DISJUNCTIVE M ETRIC CONSTRAINTS IN TEMPORAL CONTEXTS

Staab, S., & Hahn, U. (1998). Distance constraint arrays: A model for reasoning on intervals with
qualitative and quantitative distances. In Proceedings of the 12th Biennial Conference of the
Canadian Society for Computational Studies of Intelligence on Advances in Artificial
Intelligence (AI-98), Lecture Notes in Artificial Intelligence, 1418, 334-348.
Srivastava, B., & Kambhampati, S. (1999). Efficient planning through separate resource scheduling.
In Proceedings of the AAAI Spring Symp. on search strategy under uncertainty and incomplete
information. AAAI Press.
Stergiou, K., & Koubarakis, M. (1996). Tractable disjunctions of Linear Constraints. In Proceedings
of the 2nd Int. Conf. on Principles and Practice of Constraints Programming (CP’96).
Freuder, E.C. (Ed.). Lecture Notes in Computer Science, 1118, 297-307.
Stergiou, K., & Koubarakis, M. (1998). Bactracking algorithms for disjunctions of temporal
constraints. In Proceedings of the 15 th. National Conference on Artificial Intelligence (AAAI98). AAAI Press.
Van Beek, P. (1991).Temporal query processing with indefinite information. Artificial Intelligence
in Medicine, 3 (6), 325-339.
Van Beek, P., & Detcher R. (1995). On the minimality and global consistency of row convex
networks. Journal of the ACM, 42 (3), 543-561.
Van Beek, P., & Dechter, R. (1997). Constraint tightness and looseness versus local and global
consistency. Journal of the ACM, 44 (4), 549-566.
Vilain, M., Kautz, H., & Van Beek P. (1986). Constraint propagation algorithm for temporal
reasoning. In Proceedings of the 5Th. National Conference on Artificial Intelligence (AAAI86).AAAI Press.
Wetprasit, R., Sattar A. (1998). Temporal representation with qualitative and quantitative information
about points and durations. In Proceedings of the 15 th. National Conference on Artificial
Intelligence (AAAI’98). AAAI Press.
Yampratoom, E., & Allen, J. (1993). Performance of temporal reasoning systems, ACM Sigart
Bulletin, 4, (3), 26-29.

86

